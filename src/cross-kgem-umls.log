============================================
--------------------------------------------
Running a TWIG experiment with tag: UMLS-all
--------------------------------------------
============================================
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_UMLS-all
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}, 'DistMult': {'UMLS': ['2.1']}, 'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=11, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.437 +- 0.005
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0, 0.0001039674

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 400
rank avg (pred): 0.341 +- 0.052
mrr vals (pred, true): 0.025, 0.119
batch losses (mrrl, rdl): 0.0, 0.0002539238

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 297
rank avg (pred): 0.114 +- 0.093
mrr vals (pred, true): 0.277, 0.529
batch losses (mrrl, rdl): 0.0, 0.0001342603

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.348 +- 0.244
mrr vals (pred, true): 0.251, 0.052
batch losses (mrrl, rdl): 0.0, 9.94268e-05

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 358
rank avg (pred): 0.359 +- 0.256
mrr vals (pred, true): 0.254, 0.096
batch losses (mrrl, rdl): 0.0, 0.0001628126

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 56
rank avg (pred): 0.085 +- 0.109
mrr vals (pred, true): 0.347, 0.523
batch losses (mrrl, rdl): 0.0, 5.53258e-05

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.404 +- 0.281
mrr vals (pred, true): 0.229, 0.045
batch losses (mrrl, rdl): 0.0, 9.1697e-06

Epoch over!
epoch time: 53.961

Epoch 2 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 417
rank avg (pred): 0.375 +- 0.271
mrr vals (pred, true): 0.245, 0.064
batch losses (mrrl, rdl): 0.0, 3.46335e-05

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 898
rank avg (pred): 0.400 +- 0.280
mrr vals (pred, true): 0.212, 0.102
batch losses (mrrl, rdl): 0.0, 0.0010781096

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1035
rank avg (pred): 0.368 +- 0.268
mrr vals (pred, true): 0.248, 0.042
batch losses (mrrl, rdl): 0.0, 8.31684e-05

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 16
rank avg (pred): 0.060 +- 0.129
mrr vals (pred, true): 0.391, 0.184
batch losses (mrrl, rdl): 0.0, 0.0003590212

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 256
rank avg (pred): 0.051 +- 0.130
mrr vals (pred, true): 0.428, 0.556
batch losses (mrrl, rdl): 0.0, 1.70719e-05

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 463
rank avg (pred): 0.353 +- 0.271
mrr vals (pred, true): 0.237, 0.042
batch losses (mrrl, rdl): 0.0, 8.84255e-05

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 170
rank avg (pred): 0.361 +- 0.274
mrr vals (pred, true): 0.256, 0.049
batch losses (mrrl, rdl): 0.0, 9.66638e-05

Epoch over!
epoch time: 53.682

Epoch 3 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 484
rank avg (pred): 0.373 +- 0.277
mrr vals (pred, true): 0.231, 0.043
batch losses (mrrl, rdl): 0.0, 5.87081e-05

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 724
rank avg (pred): 0.397 +- 0.274
mrr vals (pred, true): 0.228, 0.038
batch losses (mrrl, rdl): 0.0, 9.8976e-06

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 415
rank avg (pred): 0.363 +- 0.273
mrr vals (pred, true): 0.260, 0.052
batch losses (mrrl, rdl): 0.0, 7.19003e-05

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 428
rank avg (pred): 0.378 +- 0.283
mrr vals (pred, true): 0.240, 0.052
batch losses (mrrl, rdl): 0.0, 2.37448e-05

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 854
rank avg (pred): 0.410 +- 0.281
mrr vals (pred, true): 0.224, 0.091
batch losses (mrrl, rdl): 0.0, 0.0004490341

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 796
rank avg (pred): 0.383 +- 0.275
mrr vals (pred, true): 0.246, 0.047
batch losses (mrrl, rdl): 0.0, 2.61684e-05

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 875
rank avg (pred): 0.393 +- 0.282
mrr vals (pred, true): 0.230, 0.047
batch losses (mrrl, rdl): 0.0, 3.56436e-05

Epoch over!
epoch time: 55.389

Epoch 4 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 865
rank avg (pred): 0.408 +- 0.284
mrr vals (pred, true): 0.225, 0.049
batch losses (mrrl, rdl): 0.0, 3.07237e-05

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 815
rank avg (pred): 0.077 +- 0.140
mrr vals (pred, true): 0.360, 0.532
batch losses (mrrl, rdl): 0.0, 4.16956e-05

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 799
rank avg (pred): 0.405 +- 0.278
mrr vals (pred, true): 0.238, 0.043
batch losses (mrrl, rdl): 0.0, 4.2401e-06

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 284
rank avg (pred): 0.071 +- 0.165
mrr vals (pred, true): 0.407, 0.225
batch losses (mrrl, rdl): 0.0, 0.0002656656

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 256
rank avg (pred): 0.046 +- 0.131
mrr vals (pred, true): 0.462, 0.556
batch losses (mrrl, rdl): 0.0, 1.17734e-05

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 703
rank avg (pred): 0.431 +- 0.283
mrr vals (pred, true): 0.193, 0.050
batch losses (mrrl, rdl): 0.0, 1.9776e-06

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1032
rank avg (pred): 0.364 +- 0.273
mrr vals (pred, true): 0.258, 0.050
batch losses (mrrl, rdl): 0.0, 7.89699e-05

Epoch over!
epoch time: 53.229

Epoch 5 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 815
rank avg (pred): 0.057 +- 0.142
mrr vals (pred, true): 0.414, 0.539
batch losses (mrrl, rdl): 0.0, 4.956e-07

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 520
rank avg (pred): 0.404 +- 0.287
mrr vals (pred, true): 0.203, 0.072
batch losses (mrrl, rdl): 0.0, 9.8638e-05

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 854
rank avg (pred): 0.404 +- 0.282
mrr vals (pred, true): 0.239, 0.114
batch losses (mrrl, rdl): 0.0, 7.95258e-05

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 168
rank avg (pred): 0.401 +- 0.286
mrr vals (pred, true): 0.195, 0.050
batch losses (mrrl, rdl): 0.0, 2.05678e-05

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 316
rank avg (pred): 0.112 +- 0.194
mrr vals (pred, true): 0.313, 0.559
batch losses (mrrl, rdl): 0.0, 0.0001583951

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 601
rank avg (pred): 0.445 +- 0.284
mrr vals (pred, true): 0.140, 0.041
batch losses (mrrl, rdl): 0.0, 3.90607e-05

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 120
rank avg (pred): 0.378 +- 0.282
mrr vals (pred, true): 0.232, 0.044
batch losses (mrrl, rdl): 0.0, 7.07008e-05

Epoch over!
epoch time: 54.133

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1127
rank avg (pred): 0.380 +- 0.279
mrr vals (pred, true): 0.223, 0.052
batch losses (mrrl, rdl): 0.3008705676, 3.64743e-05

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 995
rank avg (pred): 0.132 +- 0.262
mrr vals (pred, true): 0.387, 0.611
batch losses (mrrl, rdl): 0.501401186, 0.000284781

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 929
rank avg (pred): 0.524 +- 0.194
mrr vals (pred, true): 0.027, 0.017
batch losses (mrrl, rdl): 0.0053841337, 0.0007256747

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 361
rank avg (pred): 0.413 +- 0.227
mrr vals (pred, true): 0.094, 0.050
batch losses (mrrl, rdl): 0.0194726959, 1.25642e-05

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 173
rank avg (pred): 0.398 +- 0.193
mrr vals (pred, true): 0.083, 0.043
batch losses (mrrl, rdl): 0.0106421476, 3.98532e-05

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 462
rank avg (pred): 0.373 +- 0.157
mrr vals (pred, true): 0.091, 0.041
batch losses (mrrl, rdl): 0.016544098, 0.0001328578

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1075
rank avg (pred): 0.168 +- 0.210
mrr vals (pred, true): 0.469, 0.273
batch losses (mrrl, rdl): 0.3837103844, 2.35992e-05

Epoch over!
epoch time: 53.726

Epoch 2 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 904
rank avg (pred): 0.344 +- 0.188
mrr vals (pred, true): 0.117, 0.022
batch losses (mrrl, rdl): 0.0451881513, 0.0010568363

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 593
rank avg (pred): 0.406 +- 0.120
mrr vals (pred, true): 0.054, 0.039
batch losses (mrrl, rdl): 0.0001576928, 6.33642e-05

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 926
rank avg (pred): 0.403 +- 0.114
mrr vals (pred, true): 0.054, 0.015
batch losses (mrrl, rdl): 0.0001430525, 0.002779041

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 709
rank avg (pred): 0.416 +- 0.096
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0006942581, 8.73727e-05

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 912
rank avg (pred): 0.373 +- 0.155
mrr vals (pred, true): 0.082, 0.301
batch losses (mrrl, rdl): 0.4779045582, 0.002183089

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 148
rank avg (pred): 0.404 +- 0.107
mrr vals (pred, true): 0.050, 0.111
batch losses (mrrl, rdl): 0.037723884, 0.000276726

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 213
rank avg (pred): 0.390 +- 0.126
mrr vals (pred, true): 0.068, 0.047
batch losses (mrrl, rdl): 0.0031968681, 0.0001258855

Epoch over!
epoch time: 53.824

Epoch 3 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 487
rank avg (pred): 0.352 +- 0.155
mrr vals (pred, true): 0.097, 0.024
batch losses (mrrl, rdl): 0.0216864254, 0.0005330133

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 680
rank avg (pred): 0.414 +- 0.082
mrr vals (pred, true): 0.037, 0.039
batch losses (mrrl, rdl): 0.0015725873, 0.0001256337

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 605
rank avg (pred): 0.402 +- 0.098
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 2.69238e-05, 0.0001846207

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 732
rank avg (pred): 0.243 +- 0.211
mrr vals (pred, true): 0.389, 0.524
batch losses (mrrl, rdl): 0.1840293705, 0.0008580373

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 2
rank avg (pred): 0.225 +- 0.211
mrr vals (pred, true): 0.450, 0.602
batch losses (mrrl, rdl): 0.2298070788, 0.0010740294

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 80
rank avg (pred): 0.230 +- 0.210
mrr vals (pred, true): 0.446, 0.549
batch losses (mrrl, rdl): 0.1056091413, 0.0009792492

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1193
rank avg (pred): 0.399 +- 0.088
mrr vals (pred, true): 0.044, 0.055
batch losses (mrrl, rdl): 0.0004178254, 0.0001133221

Epoch over!
epoch time: 52.202

Epoch 4 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 109
rank avg (pred): 0.392 +- 0.100
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001308951, 8.37917e-05

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 699
rank avg (pred): 0.391 +- 0.089
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 7.96148e-05, 0.000122314

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 938
rank avg (pred): 0.402 +- 0.085
mrr vals (pred, true): 0.042, 0.021
batch losses (mrrl, rdl): 0.0005832624, 0.0017054772

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1199
rank avg (pred): 0.390 +- 0.090
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 7.53121e-05, 0.0001246929

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 51
rank avg (pred): 0.237 +- 0.203
mrr vals (pred, true): 0.381, 0.506
batch losses (mrrl, rdl): 0.1577153802, 0.0010941231

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 163
rank avg (pred): 0.377 +- 0.084
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 9.967e-07, 0.0001817294

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 664
rank avg (pred): 0.384 +- 0.082
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002461402, 0.0001244622

Epoch over!
epoch time: 53.588

Epoch 5 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 512
rank avg (pred): 0.342 +- 0.126
mrr vals (pred, true): 0.091, 0.026
batch losses (mrrl, rdl): 0.0164531637, 0.0006703258

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1012
rank avg (pred): 0.365 +- 0.103
mrr vals (pred, true): 0.066, 0.136
batch losses (mrrl, rdl): 0.0492333099, 0.0003368594

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1054
rank avg (pred): 0.196 +- 0.193
mrr vals (pred, true): 0.477, 0.537
batch losses (mrrl, rdl): 0.0359251909, 0.0006767774

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 732
rank avg (pred): 0.213 +- 0.192
mrr vals (pred, true): 0.417, 0.524
batch losses (mrrl, rdl): 0.1148684323, 0.0006065603

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 265
rank avg (pred): 0.215 +- 0.197
mrr vals (pred, true): 0.437, 0.546
batch losses (mrrl, rdl): 0.1195712239, 0.0009300316

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 443
rank avg (pred): 0.370 +- 0.101
mrr vals (pred, true): 0.064, 0.045
batch losses (mrrl, rdl): 0.0020599673, 0.0001606397

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 727
rank avg (pred): 0.390 +- 0.075
mrr vals (pred, true): 0.040, 0.044
batch losses (mrrl, rdl): 0.0009286915, 0.0001204376

Epoch over!
epoch time: 53.941

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1171
rank avg (pred): 0.376 +- 0.090
mrr vals (pred, true): 0.054, 0.035
batch losses (mrrl, rdl): 0.000193124, 0.0002273565

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 530
rank avg (pred): 0.341 +- 0.131
mrr vals (pred, true): 0.098, 0.123
batch losses (mrrl, rdl): 0.005974202, 0.0001158558

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1061
rank avg (pred): 0.209 +- 0.193
mrr vals (pred, true): 0.454, 0.549
batch losses (mrrl, rdl): 0.0905495062, 0.0007775101

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 949
rank avg (pred): 0.388 +- 0.086
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 2.5259e-06, 0.0001186868

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 405
rank avg (pred): 0.376 +- 0.091
mrr vals (pred, true): 0.057, 0.046
batch losses (mrrl, rdl): 0.0005535029, 0.0001447817

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 274
rank avg (pred): 0.197 +- 0.189
mrr vals (pred, true): 0.477, 0.547
batch losses (mrrl, rdl): 0.0486757979, 0.0006810686

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 534
rank avg (pred): 0.348 +- 0.101
mrr vals (pred, true): 0.072, 0.024
batch losses (mrrl, rdl): 0.0048722015, 0.0006932766

Epoch over!
epoch time: 54.29

Epoch 7 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 646
rank avg (pred): 0.377 +- 0.079
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 7.98062e-05, 0.0001973942

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 105
rank avg (pred): 0.363 +- 0.097
mrr vals (pred, true): 0.067, 0.091
batch losses (mrrl, rdl): 0.0030478085, 0.0002473322

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1111
rank avg (pred): 0.360 +- 0.105
mrr vals (pred, true): 0.071, 0.045
batch losses (mrrl, rdl): 0.0044164774, 0.0002419608

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 137
rank avg (pred): 0.374 +- 0.095
mrr vals (pred, true): 0.058, 0.048
batch losses (mrrl, rdl): 0.0006724197, 0.0001220785

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 431
rank avg (pred): 0.353 +- 0.104
mrr vals (pred, true): 0.077, 0.041
batch losses (mrrl, rdl): 0.0073000691, 0.0002122119

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1188
rank avg (pred): 0.377 +- 0.081
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 2.645e-07, 0.000151216

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 826
rank avg (pred): 0.224 +- 0.192
mrr vals (pred, true): 0.402, 0.245
batch losses (mrrl, rdl): 0.246070832, 8.98133e-05

Epoch over!
epoch time: 53.863

Epoch 8 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 909
rank avg (pred): 0.350 +- 0.122
mrr vals (pred, true): 0.093, 0.023
batch losses (mrrl, rdl): 0.0184567347, 0.0009244005

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 788
rank avg (pred): 0.370 +- 0.081
mrr vals (pred, true): 0.058, 0.039
batch losses (mrrl, rdl): 0.0006685104, 0.0001866547

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1046
rank avg (pred): 0.358 +- 0.096
mrr vals (pred, true): 0.073, 0.047
batch losses (mrrl, rdl): 0.0054021482, 0.000240582

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 714
rank avg (pred): 0.370 +- 0.066
mrr vals (pred, true): 0.040, 0.050
batch losses (mrrl, rdl): 0.0009135408, 0.0001632693

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 500
rank avg (pred): 0.311 +- 0.146
mrr vals (pred, true): 0.138, 0.104
batch losses (mrrl, rdl): 0.0115407119, 5.15558e-05

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 102
rank avg (pred): 0.367 +- 0.100
mrr vals (pred, true): 0.068, 0.085
batch losses (mrrl, rdl): 0.0033431789, 6.58142e-05

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 328
rank avg (pred): 0.366 +- 0.088
mrr vals (pred, true): 0.058, 0.043
batch losses (mrrl, rdl): 0.0006823111, 0.0002004095

Epoch over!
epoch time: 52.72

Epoch 9 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 503
rank avg (pred): 0.335 +- 0.134
mrr vals (pred, true): 0.114, 0.026
batch losses (mrrl, rdl): 0.0411726162, 0.0006100576

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 424
rank avg (pred): 0.351 +- 0.095
mrr vals (pred, true): 0.070, 0.040
batch losses (mrrl, rdl): 0.0041327225, 0.0002507374

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 710
rank avg (pred): 0.388 +- 0.079
mrr vals (pred, true): 0.046, 0.045
batch losses (mrrl, rdl): 0.0001922553, 0.0001333592

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1195
rank avg (pred): 0.383 +- 0.083
mrr vals (pred, true): 0.052, 0.041
batch losses (mrrl, rdl): 5.32499e-05, 0.0001527198

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 889
rank avg (pred): 0.378 +- 0.087
mrr vals (pred, true): 0.056, 0.039
batch losses (mrrl, rdl): 0.0004007478, 0.0001627579

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 805
rank avg (pred): 0.395 +- 0.071
mrr vals (pred, true): 0.040, 0.042
batch losses (mrrl, rdl): 0.0010293639, 0.0001532639

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1101
rank avg (pred): 0.375 +- 0.112
mrr vals (pred, true): 0.081, 0.043
batch losses (mrrl, rdl): 0.0093765678, 0.0001094709

Epoch over!
epoch time: 52.557

Epoch 10 -- 
running batch: 0 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 603
rank avg (pred): 0.386 +- 0.076
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001948596, 0.000200026

running batch: 500 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 616
rank avg (pred): 0.381 +- 0.078
mrr vals (pred, true): 0.050, 0.038
batch losses (mrrl, rdl): 5.453e-07, 0.0001359855

running batch: 1000 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 538
rank avg (pred): 0.364 +- 0.109
mrr vals (pred, true): 0.076, 0.060
batch losses (mrrl, rdl): 0.0068129781, 0.0002237631

running batch: 1500 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 744
rank avg (pred): 0.241 +- 0.200
mrr vals (pred, true): 0.389, 0.231
batch losses (mrrl, rdl): 0.2484057248, 0.0001392603

running batch: 2000 / 3282 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1063
rank avg (pred): 0.210 +- 0.199
mrr vals (pred, true): 0.465, 0.605
batch losses (mrrl, rdl): 0.1960826665, 0.0009170292

running batch: 2500 / 3282 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1091
rank avg (pred): 0.359 +- 0.110
mrr vals (pred, true): 0.089, 0.096
batch losses (mrrl, rdl): 0.0150215989, 6.48829e-05

running batch: 3000 / 3282 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 965
rank avg (pred): 0.383 +- 0.075
mrr vals (pred, true): 0.046, 0.051
batch losses (mrrl, rdl): 0.0001826745, 0.0001424089

Epoch over!
epoch time: 53.477

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.219 +- 0.198
mrr vals (pred, true): 0.448, 0.295

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   94 	     0 	 0.12737 	 0.02393 	 MISS
   93 	     1 	 0.12558 	 0.02414 	 MISS
   82 	     2 	 0.07926 	 0.02424 	 m..s
   87 	     3 	 0.09408 	 0.02476 	 m..s
   80 	     4 	 0.07742 	 0.02481 	 m..s
   70 	     5 	 0.07336 	 0.02543 	 m..s
   89 	     6 	 0.10577 	 0.02544 	 m..s
   92 	     7 	 0.12436 	 0.02552 	 m..s
   71 	     8 	 0.07355 	 0.02570 	 m..s
   90 	     9 	 0.11912 	 0.02571 	 m..s
   91 	    10 	 0.12044 	 0.02579 	 m..s
   95 	    11 	 0.13706 	 0.02609 	 MISS
   74 	    12 	 0.07378 	 0.02623 	 m..s
   85 	    13 	 0.08421 	 0.02633 	 m..s
   38 	    14 	 0.05477 	 0.02721 	 ~...
   88 	    15 	 0.10546 	 0.02758 	 m..s
   62 	    16 	 0.07064 	 0.02767 	 m..s
   35 	    17 	 0.05412 	 0.02843 	 ~...
   31 	    18 	 0.05326 	 0.03512 	 ~...
   13 	    19 	 0.04989 	 0.03647 	 ~...
    4 	    20 	 0.04743 	 0.03695 	 ~...
    3 	    21 	 0.04742 	 0.03730 	 ~...
   21 	    22 	 0.05105 	 0.03889 	 ~...
   30 	    23 	 0.05259 	 0.03923 	 ~...
   18 	    24 	 0.05091 	 0.03937 	 ~...
   24 	    25 	 0.05145 	 0.03973 	 ~...
   28 	    26 	 0.05232 	 0.04046 	 ~...
   77 	    27 	 0.07587 	 0.04050 	 m..s
   16 	    28 	 0.05064 	 0.04084 	 ~...
   14 	    29 	 0.05004 	 0.04091 	 ~...
   33 	    30 	 0.05364 	 0.04101 	 ~...
   43 	    31 	 0.05751 	 0.04126 	 ~...
   25 	    32 	 0.05194 	 0.04129 	 ~...
   11 	    33 	 0.04971 	 0.04139 	 ~...
    7 	    34 	 0.04851 	 0.04147 	 ~...
   22 	    35 	 0.05130 	 0.04169 	 ~...
    9 	    36 	 0.04926 	 0.04209 	 ~...
   32 	    37 	 0.05332 	 0.04221 	 ~...
   12 	    38 	 0.04982 	 0.04224 	 ~...
   73 	    39 	 0.07375 	 0.04224 	 m..s
   20 	    40 	 0.05097 	 0.04224 	 ~...
   69 	    41 	 0.07254 	 0.04240 	 m..s
    8 	    42 	 0.04916 	 0.04270 	 ~...
   61 	    43 	 0.06864 	 0.04276 	 ~...
   34 	    44 	 0.05393 	 0.04279 	 ~...
    6 	    45 	 0.04831 	 0.04302 	 ~...
    1 	    46 	 0.04672 	 0.04332 	 ~...
    2 	    47 	 0.04690 	 0.04333 	 ~...
   75 	    48 	 0.07430 	 0.04369 	 m..s
   59 	    49 	 0.06787 	 0.04386 	 ~...
   83 	    50 	 0.08096 	 0.04388 	 m..s
   55 	    51 	 0.06494 	 0.04418 	 ~...
   58 	    52 	 0.06669 	 0.04444 	 ~...
   53 	    53 	 0.06425 	 0.04460 	 ~...
   50 	    54 	 0.06180 	 0.04473 	 ~...
   57 	    55 	 0.06552 	 0.04514 	 ~...
   51 	    56 	 0.06202 	 0.04549 	 ~...
   66 	    57 	 0.07123 	 0.04601 	 ~...
   26 	    58 	 0.05195 	 0.04609 	 ~...
   65 	    59 	 0.07108 	 0.04652 	 ~...
   86 	    60 	 0.08572 	 0.04658 	 m..s
   44 	    61 	 0.05780 	 0.04659 	 ~...
   17 	    62 	 0.05084 	 0.04757 	 ~...
   60 	    63 	 0.06794 	 0.04760 	 ~...
   27 	    64 	 0.05226 	 0.04777 	 ~...
   84 	    65 	 0.08399 	 0.04815 	 m..s
   40 	    66 	 0.05677 	 0.04815 	 ~...
   63 	    67 	 0.07092 	 0.04819 	 ~...
   47 	    68 	 0.05840 	 0.04843 	 ~...
    5 	    69 	 0.04805 	 0.04894 	 ~...
    0 	    70 	 0.04593 	 0.04896 	 ~...
   54 	    71 	 0.06439 	 0.04952 	 ~...
   42 	    72 	 0.05699 	 0.04953 	 ~...
   36 	    73 	 0.05414 	 0.04959 	 ~...
   41 	    74 	 0.05690 	 0.04969 	 ~...
   64 	    75 	 0.07107 	 0.04974 	 ~...
   48 	    76 	 0.06054 	 0.04978 	 ~...
   78 	    77 	 0.07646 	 0.04989 	 ~...
   76 	    78 	 0.07432 	 0.04990 	 ~...
   67 	    79 	 0.07124 	 0.05007 	 ~...
   19 	    80 	 0.05095 	 0.05012 	 ~...
   52 	    81 	 0.06388 	 0.05014 	 ~...
   37 	    82 	 0.05454 	 0.05082 	 ~...
   68 	    83 	 0.07131 	 0.05085 	 ~...
   79 	    84 	 0.07724 	 0.05086 	 ~...
   29 	    85 	 0.05234 	 0.05092 	 ~...
   56 	    86 	 0.06504 	 0.05178 	 ~...
   46 	    87 	 0.05825 	 0.05199 	 ~...
   39 	    88 	 0.05650 	 0.05259 	 ~...
   10 	    89 	 0.04961 	 0.05269 	 ~...
   45 	    90 	 0.05782 	 0.05275 	 ~...
   81 	    91 	 0.07790 	 0.05333 	 ~...
   15 	    92 	 0.05027 	 0.05340 	 ~...
   72 	    93 	 0.07362 	 0.05355 	 ~...
   49 	    94 	 0.06061 	 0.05479 	 ~...
   23 	    95 	 0.05139 	 0.05634 	 ~...
  101 	    96 	 0.43626 	 0.18803 	 MISS
  104 	    97 	 0.44017 	 0.20836 	 MISS
  107 	    98 	 0.45429 	 0.20872 	 MISS
  108 	    99 	 0.45430 	 0.21171 	 MISS
  102 	   100 	 0.43659 	 0.21637 	 MISS
  103 	   101 	 0.43971 	 0.21805 	 MISS
  109 	   102 	 0.45466 	 0.21849 	 MISS
  113 	   103 	 0.45796 	 0.23847 	 MISS
  117 	   104 	 0.46195 	 0.25215 	 MISS
  112 	   105 	 0.45790 	 0.25298 	 MISS
  111 	   106 	 0.45714 	 0.26711 	 MISS
  110 	   107 	 0.45679 	 0.27410 	 MISS
  100 	   108 	 0.43069 	 0.27441 	 MISS
   97 	   109 	 0.39612 	 0.28106 	 MISS
  105 	   110 	 0.44723 	 0.28160 	 MISS
  115 	   111 	 0.45968 	 0.28472 	 MISS
  120 	   112 	 0.46271 	 0.28533 	 MISS
  119 	   113 	 0.46252 	 0.28534 	 MISS
   96 	   114 	 0.33264 	 0.29334 	 m..s
  118 	   115 	 0.46196 	 0.29484 	 MISS
  106 	   116 	 0.44766 	 0.29550 	 MISS
  116 	   117 	 0.46135 	 0.30397 	 MISS
  114 	   118 	 0.45932 	 0.31636 	 MISS
   98 	   119 	 0.41895 	 0.37350 	 m..s
   99 	   120 	 0.41910 	 0.39525 	 ~...
==========================================
r_mrr = 0.949078381061554
r2_mrr = 0.1071510910987854
spearmanr_mrr@5 = 0.9383627772331238
spearmanr_mrr@10 = 0.7477167248725891
spearmanr_mrr@50 = 0.9686829447746277
spearmanr_mrr@100 = 0.9782069325447083
spearmanr_mrr@All = 0.9798083305358887
==========================================
test time: 0.708
Done Testing dataset UMLS
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.219 +- 0.198
mrr vals (pred, true): 0.448, 0.557

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   38 	     0 	 0.05477 	 0.01547 	 m..s
   31 	     1 	 0.05326 	 0.01573 	 m..s
   35 	     2 	 0.05412 	 0.01574 	 m..s
   22 	     3 	 0.05130 	 0.01764 	 m..s
   96 	     4 	 0.33264 	 0.01876 	 MISS
   18 	     5 	 0.05091 	 0.02025 	 m..s
    2 	     6 	 0.04690 	 0.03193 	 ~...
    3 	     7 	 0.04742 	 0.03285 	 ~...
    9 	     8 	 0.04926 	 0.03716 	 ~...
   29 	     9 	 0.05234 	 0.03907 	 ~...
    4 	    10 	 0.04743 	 0.03921 	 ~...
   43 	    11 	 0.05751 	 0.04022 	 ~...
    7 	    12 	 0.04851 	 0.04031 	 ~...
   26 	    13 	 0.05195 	 0.04101 	 ~...
   10 	    14 	 0.04961 	 0.04132 	 ~...
   32 	    15 	 0.05332 	 0.04155 	 ~...
   21 	    16 	 0.05105 	 0.04194 	 ~...
   13 	    17 	 0.04989 	 0.04203 	 ~...
   50 	    18 	 0.06180 	 0.04242 	 ~...
   39 	    19 	 0.05650 	 0.04303 	 ~...
   19 	    20 	 0.05095 	 0.04316 	 ~...
   63 	    21 	 0.07092 	 0.04374 	 ~...
   34 	    22 	 0.05393 	 0.04377 	 ~...
    0 	    23 	 0.04593 	 0.04444 	 ~...
   66 	    24 	 0.07123 	 0.04460 	 ~...
    8 	    25 	 0.04916 	 0.04465 	 ~...
    5 	    26 	 0.04805 	 0.04475 	 ~...
   59 	    27 	 0.06787 	 0.04547 	 ~...
   11 	    28 	 0.04971 	 0.04598 	 ~...
   60 	    29 	 0.06794 	 0.04604 	 ~...
   23 	    30 	 0.05139 	 0.04612 	 ~...
   27 	    31 	 0.05226 	 0.04628 	 ~...
   46 	    32 	 0.05825 	 0.04631 	 ~...
   24 	    33 	 0.05145 	 0.04634 	 ~...
   30 	    34 	 0.05259 	 0.04665 	 ~...
   25 	    35 	 0.05194 	 0.04707 	 ~...
   20 	    36 	 0.05097 	 0.04768 	 ~...
   51 	    37 	 0.06202 	 0.04779 	 ~...
   16 	    38 	 0.05064 	 0.04804 	 ~...
    1 	    39 	 0.04672 	 0.04845 	 ~...
   14 	    40 	 0.05004 	 0.04954 	 ~...
   15 	    41 	 0.05027 	 0.04966 	 ~...
   84 	    42 	 0.08399 	 0.05004 	 m..s
   70 	    43 	 0.07336 	 0.05028 	 ~...
   12 	    44 	 0.04982 	 0.05033 	 ~...
    6 	    45 	 0.04831 	 0.05069 	 ~...
   37 	    46 	 0.05454 	 0.05069 	 ~...
   17 	    47 	 0.05084 	 0.05115 	 ~...
   36 	    48 	 0.05414 	 0.05121 	 ~...
   65 	    49 	 0.07108 	 0.05133 	 ~...
   28 	    50 	 0.05232 	 0.05144 	 ~...
   79 	    51 	 0.07724 	 0.05165 	 ~...
   41 	    52 	 0.05690 	 0.05178 	 ~...
   57 	    53 	 0.06552 	 0.05197 	 ~...
   67 	    54 	 0.07124 	 0.05366 	 ~...
   64 	    55 	 0.07107 	 0.05412 	 ~...
   68 	    56 	 0.07131 	 0.05549 	 ~...
   33 	    57 	 0.05364 	 0.05645 	 ~...
   82 	    58 	 0.07926 	 0.05732 	 ~...
   71 	    59 	 0.07355 	 0.06234 	 ~...
   62 	    60 	 0.07064 	 0.06455 	 ~...
   80 	    61 	 0.07742 	 0.06948 	 ~...
   78 	    62 	 0.07646 	 0.07881 	 ~...
   61 	    63 	 0.06864 	 0.07982 	 ~...
   74 	    64 	 0.07378 	 0.08048 	 ~...
   86 	    65 	 0.08572 	 0.08200 	 ~...
   69 	    66 	 0.07254 	 0.08797 	 ~...
   76 	    67 	 0.07432 	 0.09547 	 ~...
   53 	    68 	 0.06425 	 0.09729 	 m..s
   47 	    69 	 0.05840 	 0.10013 	 m..s
   45 	    70 	 0.05782 	 0.10042 	 m..s
   56 	    71 	 0.06504 	 0.10126 	 m..s
   87 	    72 	 0.09408 	 0.10451 	 ~...
   83 	    73 	 0.08096 	 0.10545 	 ~...
   73 	    74 	 0.07375 	 0.10651 	 m..s
   81 	    75 	 0.07790 	 0.10828 	 m..s
   42 	    76 	 0.05699 	 0.10861 	 m..s
   52 	    77 	 0.06388 	 0.11219 	 m..s
   54 	    78 	 0.06439 	 0.11794 	 m..s
   40 	    79 	 0.05677 	 0.12025 	 m..s
   55 	    80 	 0.06494 	 0.12331 	 m..s
   72 	    81 	 0.07362 	 0.12428 	 m..s
   44 	    82 	 0.05780 	 0.12680 	 m..s
   49 	    83 	 0.06061 	 0.13142 	 m..s
   77 	    84 	 0.07587 	 0.13168 	 m..s
   58 	    85 	 0.06669 	 0.13489 	 m..s
   75 	    86 	 0.07430 	 0.13801 	 m..s
   88 	    87 	 0.10546 	 0.13903 	 m..s
   90 	    88 	 0.11912 	 0.13990 	 ~...
   91 	    89 	 0.12044 	 0.14093 	 ~...
   89 	    90 	 0.10577 	 0.14821 	 m..s
   48 	    91 	 0.06054 	 0.14995 	 m..s
   93 	    92 	 0.12558 	 0.16048 	 m..s
   85 	    93 	 0.08421 	 0.18150 	 m..s
   97 	    94 	 0.39612 	 0.20395 	 MISS
   95 	    95 	 0.13706 	 0.23331 	 m..s
   92 	    96 	 0.12436 	 0.24069 	 MISS
   94 	    97 	 0.12737 	 0.24936 	 MISS
   98 	    98 	 0.41895 	 0.36843 	 m..s
  100 	    99 	 0.43069 	 0.42624 	 ~...
   99 	   100 	 0.41910 	 0.43149 	 ~...
  102 	   101 	 0.43659 	 0.52120 	 m..s
  113 	   102 	 0.45796 	 0.52247 	 m..s
  111 	   103 	 0.45714 	 0.52512 	 m..s
  120 	   104 	 0.46271 	 0.53266 	 m..s
  117 	   105 	 0.46195 	 0.53628 	 m..s
  115 	   106 	 0.45968 	 0.53657 	 m..s
  109 	   107 	 0.45466 	 0.54072 	 m..s
  112 	   108 	 0.45790 	 0.54120 	 m..s
  114 	   109 	 0.45932 	 0.54489 	 m..s
  118 	   110 	 0.46196 	 0.54506 	 m..s
  110 	   111 	 0.45679 	 0.54738 	 m..s
  105 	   112 	 0.44723 	 0.54784 	 MISS
  103 	   113 	 0.43971 	 0.54894 	 MISS
  101 	   114 	 0.43626 	 0.55251 	 MISS
  104 	   115 	 0.44017 	 0.55497 	 MISS
  108 	   116 	 0.45430 	 0.55718 	 MISS
  106 	   117 	 0.44766 	 0.55731 	 MISS
  116 	   118 	 0.46135 	 0.56143 	 MISS
  107 	   119 	 0.45429 	 0.56317 	 MISS
  119 	   120 	 0.46252 	 0.56546 	 MISS
==========================================
r_mrr = 0.9576473832130432
r2_mrr = 0.8932769298553467
spearmanr_mrr@5 = 0.8946676254272461
spearmanr_mrr@10 = 0.966671347618103
spearmanr_mrr@50 = 0.9723507165908813
spearmanr_mrr@100 = 0.9816760420799255
spearmanr_mrr@All = 0.9823540449142456
==========================================
test time: 0.649
Done Testing dataset UMLS
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.219 +- 0.198
mrr vals (pred, true): 0.448, 0.623

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04926 	 0.03230 	 ~...
    3 	     1 	 0.04742 	 0.03303 	 ~...
    7 	     2 	 0.04851 	 0.03379 	 ~...
   26 	     3 	 0.05195 	 0.03491 	 ~...
   60 	     4 	 0.06794 	 0.03543 	 m..s
   13 	     5 	 0.04989 	 0.03555 	 ~...
    4 	     6 	 0.04743 	 0.03566 	 ~...
    6 	     7 	 0.04831 	 0.03572 	 ~...
   20 	     8 	 0.05097 	 0.03665 	 ~...
   84 	     9 	 0.08399 	 0.03691 	 m..s
    8 	    10 	 0.04916 	 0.03694 	 ~...
   66 	    11 	 0.07123 	 0.03722 	 m..s
   21 	    12 	 0.05105 	 0.03736 	 ~...
   64 	    13 	 0.07107 	 0.03779 	 m..s
   15 	    14 	 0.05027 	 0.03794 	 ~...
   67 	    15 	 0.07124 	 0.03801 	 m..s
   19 	    16 	 0.05095 	 0.03818 	 ~...
   11 	    17 	 0.04971 	 0.03824 	 ~...
   33 	    18 	 0.05364 	 0.03829 	 ~...
   63 	    19 	 0.07092 	 0.03841 	 m..s
   34 	    20 	 0.05393 	 0.03880 	 ~...
   79 	    21 	 0.07724 	 0.03886 	 m..s
   68 	    22 	 0.07131 	 0.03940 	 m..s
   57 	    23 	 0.06552 	 0.03945 	 ~...
   24 	    24 	 0.05145 	 0.03948 	 ~...
    5 	    25 	 0.04805 	 0.03954 	 ~...
   51 	    26 	 0.06202 	 0.03959 	 ~...
   30 	    27 	 0.05259 	 0.03977 	 ~...
   39 	    28 	 0.05650 	 0.03987 	 ~...
   36 	    29 	 0.05414 	 0.04001 	 ~...
   41 	    30 	 0.05690 	 0.04019 	 ~...
   50 	    31 	 0.06180 	 0.04050 	 ~...
   10 	    32 	 0.04961 	 0.04059 	 ~...
   46 	    33 	 0.05825 	 0.04060 	 ~...
    1 	    34 	 0.04672 	 0.04081 	 ~...
   65 	    35 	 0.07108 	 0.04126 	 ~...
   14 	    36 	 0.05004 	 0.04143 	 ~...
   16 	    37 	 0.05064 	 0.04182 	 ~...
   25 	    38 	 0.05194 	 0.04418 	 ~...
   23 	    39 	 0.05139 	 0.04435 	 ~...
   59 	    40 	 0.06787 	 0.04593 	 ~...
   43 	    41 	 0.05751 	 0.04595 	 ~...
   37 	    42 	 0.05454 	 0.04607 	 ~...
   70 	    43 	 0.07336 	 0.06426 	 ~...
   62 	    44 	 0.07064 	 0.06454 	 ~...
   85 	    45 	 0.08421 	 0.06812 	 ~...
   82 	    46 	 0.07926 	 0.07245 	 ~...
   80 	    47 	 0.07742 	 0.07306 	 ~...
   53 	    48 	 0.06425 	 0.07572 	 ~...
   74 	    49 	 0.07378 	 0.07645 	 ~...
   71 	    50 	 0.07355 	 0.08144 	 ~...
   22 	    51 	 0.05130 	 0.08159 	 m..s
    0 	    52 	 0.04593 	 0.08354 	 m..s
   42 	    53 	 0.05699 	 0.08483 	 ~...
   17 	    54 	 0.05084 	 0.08605 	 m..s
   35 	    55 	 0.05412 	 0.08625 	 m..s
   18 	    56 	 0.05091 	 0.08644 	 m..s
   54 	    57 	 0.06439 	 0.08687 	 ~...
   44 	    58 	 0.05780 	 0.08730 	 ~...
   38 	    59 	 0.05477 	 0.08739 	 m..s
   32 	    60 	 0.05332 	 0.08768 	 m..s
   40 	    61 	 0.05677 	 0.08889 	 m..s
   31 	    62 	 0.05326 	 0.08899 	 m..s
   45 	    63 	 0.05782 	 0.08924 	 m..s
   49 	    64 	 0.06061 	 0.09103 	 m..s
   61 	    65 	 0.06864 	 0.09155 	 ~...
   28 	    66 	 0.05232 	 0.09245 	 m..s
    2 	    67 	 0.04690 	 0.09376 	 m..s
   52 	    68 	 0.06388 	 0.09394 	 m..s
   47 	    69 	 0.05840 	 0.09410 	 m..s
   27 	    70 	 0.05226 	 0.09495 	 m..s
   12 	    71 	 0.04982 	 0.09496 	 m..s
   48 	    72 	 0.06054 	 0.09999 	 m..s
   29 	    73 	 0.05234 	 0.10033 	 m..s
   58 	    74 	 0.06669 	 0.10079 	 m..s
   72 	    75 	 0.07362 	 0.10238 	 ~...
   75 	    76 	 0.07430 	 0.10850 	 m..s
   78 	    77 	 0.07646 	 0.10900 	 m..s
   92 	    78 	 0.12436 	 0.11248 	 ~...
   73 	    79 	 0.07375 	 0.11368 	 m..s
   77 	    80 	 0.07587 	 0.11432 	 m..s
   76 	    81 	 0.07432 	 0.11644 	 m..s
   87 	    82 	 0.09408 	 0.11699 	 ~...
   95 	    83 	 0.13706 	 0.12359 	 ~...
   94 	    84 	 0.12737 	 0.13060 	 ~...
   86 	    85 	 0.08572 	 0.13087 	 m..s
   81 	    86 	 0.07790 	 0.13577 	 m..s
   56 	    87 	 0.06504 	 0.13662 	 m..s
   55 	    88 	 0.06494 	 0.13689 	 m..s
   91 	    89 	 0.12044 	 0.14348 	 ~...
   83 	    90 	 0.08096 	 0.14453 	 m..s
   93 	    91 	 0.12558 	 0.14620 	 ~...
   69 	    92 	 0.07254 	 0.14897 	 m..s
   88 	    93 	 0.10546 	 0.15149 	 m..s
   90 	    94 	 0.11912 	 0.15153 	 m..s
   89 	    95 	 0.10577 	 0.16156 	 m..s
   96 	    96 	 0.33264 	 0.36531 	 m..s
   99 	    97 	 0.41910 	 0.41053 	 ~...
   98 	    98 	 0.41895 	 0.42189 	 ~...
  104 	    99 	 0.44017 	 0.48792 	 m..s
  103 	   100 	 0.43971 	 0.50860 	 m..s
  100 	   101 	 0.43069 	 0.51098 	 m..s
  101 	   102 	 0.43626 	 0.56204 	 MISS
   97 	   103 	 0.39612 	 0.56964 	 MISS
  102 	   104 	 0.43659 	 0.56998 	 MISS
  118 	   105 	 0.46196 	 0.59282 	 MISS
  114 	   106 	 0.45932 	 0.59483 	 MISS
  120 	   107 	 0.46271 	 0.59591 	 MISS
  119 	   108 	 0.46252 	 0.59711 	 MISS
  113 	   109 	 0.45796 	 0.60102 	 MISS
  111 	   110 	 0.45714 	 0.60193 	 MISS
  112 	   111 	 0.45790 	 0.61400 	 MISS
  117 	   112 	 0.46195 	 0.61403 	 MISS
  105 	   113 	 0.44723 	 0.61471 	 MISS
  109 	   114 	 0.45466 	 0.61801 	 MISS
  115 	   115 	 0.45968 	 0.61930 	 MISS
  116 	   116 	 0.46135 	 0.61949 	 MISS
  106 	   117 	 0.44766 	 0.62336 	 MISS
  110 	   118 	 0.45679 	 0.62817 	 MISS
  108 	   119 	 0.45430 	 0.63164 	 MISS
  107 	   120 	 0.45429 	 0.63323 	 MISS
==========================================
r_mrr = 0.9856119751930237
r2_mrr = 0.8906580209732056
spearmanr_mrr@5 = 0.9562214612960815
spearmanr_mrr@10 = 0.9168857932090759
spearmanr_mrr@50 = 0.9905094504356384
spearmanr_mrr@100 = 0.9931580424308777
spearmanr_mrr@All = 0.9932547211647034
==========================================
test time: 0.745
Done Testing dataset UMLS
total time taken: 826.4508039951324
training time taken: 806.890389919281
TWIG out ;))
