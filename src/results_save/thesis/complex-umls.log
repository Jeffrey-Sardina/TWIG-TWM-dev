Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_complex-umls-both_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [10, 0]
rank_dist_loss_coeffs: [1, 0]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 10 and 2: 0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0109523255, 7.86493e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.092 +- 0.026
mrr vals (pred, true): 0.083, 0.026
batch losses (mrrl, rdl): 0.0110881915, 0.0032919813

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.144 +- 0.035
mrr vals (pred, true): 0.054, 0.050
batch losses (mrrl, rdl): 0.0001610359, 0.0019927383

Epoch over!
epoch time: 15.091

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.029 +- 0.007
mrr vals (pred, true): 0.217, 0.252
batch losses (mrrl, rdl): 0.0124365268, 0.00035042

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.183 +- 0.003
mrr vals (pred, true): 0.039, 0.026
batch losses (mrrl, rdl): 0.0011628623, 0.0022360631

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.029 +- 0.001
mrr vals (pred, true): 0.206, 0.192
batch losses (mrrl, rdl): 0.0021662817, 0.0005821529

Epoch over!
epoch time: 13.22

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.175 +- 0.010
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.000819315, 0.0016283033

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.023 +- 0.001
mrr vals (pred, true): 0.242, 0.258
batch losses (mrrl, rdl): 0.0024817956, 0.0006828517

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.126 +- 0.003
mrr vals (pred, true): 0.056, 0.055
batch losses (mrrl, rdl): 0.0003720744, 0.0015964459

Epoch over!
epoch time: 13.21

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.147 +- 0.005
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 2.64941e-05, 0.0018641672

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.148 +- 0.006
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 3.67864e-05, 0.001838233

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.165 +- 0.004
mrr vals (pred, true): 0.043, 0.025
batch losses (mrrl, rdl): 0.0004651216, 0.0028586343

Epoch over!
epoch time: 13.648

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.181 +- 0.001
mrr vals (pred, true): 0.040, 0.024
batch losses (mrrl, rdl): 0.0010673497, 0.0024432756

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.163 +- 0.000
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0003901173, 0.0016423363

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.131 +- 0.002
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001567899, 0.0020388893

Epoch over!
epoch time: 13.765

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.141 +- 0.000
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.721e-07, 0.0020405645

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.156 +- 0.002
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001849758, 0.0017070734

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.131 +- 0.002
mrr vals (pred, true): 0.054, 0.053
batch losses (mrrl, rdl): 0.0001624156, 0.0020979606

Epoch over!
epoch time: 12.993

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 535
rank avg (pred): 0.165 +- 0.001
mrr vals (pred, true): 0.043, 0.024
batch losses (mrrl, rdl): 0.0004532592, 0.002471955

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1134
rank avg (pred): 0.137 +- 0.000
mrr vals (pred, true): 0.052, 0.023
batch losses (mrrl, rdl): 2.87194e-05, 0.0027255677

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 161
rank avg (pred): 0.141 +- 0.000
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 2.333e-07, 0.0020750032

Epoch over!
epoch time: 12.94

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1191
rank avg (pred): 0.151 +- 0.001
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 8.00347e-05, 0.0020063408

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.151 +- 0.001
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 9.30811e-05, 0.0018150517

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 150
rank avg (pred): 0.148 +- 0.001
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 4.29301e-05, 0.0020009722

Epoch over!
epoch time: 13.428

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 447
rank avg (pred): 0.171 +- 0.001
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0006813392, 0.0015129125

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 117
rank avg (pred): 0.159 +- 0.000
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002594477, 0.0018618078

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.132 +- 0.000
mrr vals (pred, true): 0.054, 0.046
batch losses (mrrl, rdl): 0.0001281317, 0.0021589047

Epoch over!
epoch time: 12.917

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 440
rank avg (pred): 0.125 +- 0.000
mrr vals (pred, true): 0.057, 0.056
batch losses (mrrl, rdl): 0.000422694, 0.0021661792

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 321
rank avg (pred): 0.026 +- 0.000
mrr vals (pred, true): 0.223, 0.176
batch losses (mrrl, rdl): 0.0226104427, 0.0006758196

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.149 +- 0.000
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 4.64832e-05, 0.0019756418

Epoch over!
epoch time: 12.815

Saving checkpoint at [1] epoch 10
Done training phase:  0
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.140 +- 0.000
mrr vals (pred, true): 0.051, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   80 	     0 	 0.05064 	 0.01773 	 m..s
   82 	     1 	 0.05100 	 0.02009 	 m..s
   78 	     2 	 0.04982 	 0.02164 	 ~...
   87 	     3 	 0.05416 	 0.02192 	 m..s
   36 	     4 	 0.04600 	 0.02235 	 ~...
   36 	     5 	 0.04600 	 0.02398 	 ~...
   36 	     6 	 0.04600 	 0.02414 	 ~...
    5 	     7 	 0.04232 	 0.02414 	 ~...
   84 	     8 	 0.05109 	 0.02423 	 ~...
   69 	     9 	 0.04804 	 0.02467 	 ~...
    7 	    10 	 0.04271 	 0.02475 	 ~...
   70 	    11 	 0.04805 	 0.02496 	 ~...
   86 	    12 	 0.05352 	 0.02544 	 ~...
   73 	    13 	 0.04856 	 0.02586 	 ~...
    1 	    14 	 0.04140 	 0.02933 	 ~...
   16 	    15 	 0.04529 	 0.03159 	 ~...
    6 	    16 	 0.04247 	 0.03197 	 ~...
    2 	    17 	 0.04153 	 0.03458 	 ~...
   26 	    18 	 0.04578 	 0.03549 	 ~...
    4 	    19 	 0.04207 	 0.03582 	 ~...
   71 	    20 	 0.04811 	 0.03732 	 ~...
   50 	    21 	 0.04677 	 0.03811 	 ~...
   62 	    22 	 0.04763 	 0.03856 	 ~...
   28 	    23 	 0.04585 	 0.03931 	 ~...
   58 	    24 	 0.04755 	 0.03957 	 ~...
   57 	    25 	 0.04742 	 0.03980 	 ~...
   12 	    26 	 0.04468 	 0.04032 	 ~...
   11 	    27 	 0.04468 	 0.04070 	 ~...
   76 	    28 	 0.04882 	 0.04080 	 ~...
   44 	    29 	 0.04624 	 0.04099 	 ~...
   31 	    30 	 0.04592 	 0.04129 	 ~...
   68 	    31 	 0.04799 	 0.04130 	 ~...
   59 	    32 	 0.04758 	 0.04163 	 ~...
   61 	    33 	 0.04760 	 0.04165 	 ~...
   60 	    34 	 0.04758 	 0.04176 	 ~...
   83 	    35 	 0.05108 	 0.04224 	 ~...
   25 	    36 	 0.04572 	 0.04272 	 ~...
   53 	    37 	 0.04715 	 0.04276 	 ~...
   66 	    38 	 0.04768 	 0.04306 	 ~...
   45 	    39 	 0.04638 	 0.04338 	 ~...
   72 	    40 	 0.04824 	 0.04372 	 ~...
   64 	    41 	 0.04766 	 0.04373 	 ~...
   10 	    42 	 0.04462 	 0.04388 	 ~...
   52 	    43 	 0.04710 	 0.04410 	 ~...
   79 	    44 	 0.05057 	 0.04419 	 ~...
   24 	    45 	 0.04571 	 0.04420 	 ~...
   19 	    46 	 0.04552 	 0.04423 	 ~...
   32 	    47 	 0.04594 	 0.04476 	 ~...
   46 	    48 	 0.04639 	 0.04514 	 ~...
   89 	    49 	 0.05731 	 0.04551 	 ~...
   13 	    50 	 0.04475 	 0.04564 	 ~...
   48 	    51 	 0.04648 	 0.04617 	 ~...
   55 	    52 	 0.04733 	 0.04642 	 ~...
   30 	    53 	 0.04588 	 0.04687 	 ~...
    3 	    54 	 0.04164 	 0.04699 	 ~...
   23 	    55 	 0.04566 	 0.04706 	 ~...
   17 	    56 	 0.04542 	 0.04718 	 ~...
   81 	    57 	 0.05081 	 0.04727 	 ~...
   47 	    58 	 0.04641 	 0.04740 	 ~...
   29 	    59 	 0.04588 	 0.04744 	 ~...
   43 	    60 	 0.04623 	 0.04750 	 ~...
   39 	    61 	 0.04602 	 0.04758 	 ~...
   67 	    62 	 0.04769 	 0.04763 	 ~...
   15 	    63 	 0.04514 	 0.04824 	 ~...
   35 	    64 	 0.04600 	 0.04857 	 ~...
   40 	    65 	 0.04605 	 0.04879 	 ~...
   51 	    66 	 0.04683 	 0.04879 	 ~...
   88 	    67 	 0.05575 	 0.04904 	 ~...
   20 	    68 	 0.04553 	 0.04921 	 ~...
   49 	    69 	 0.04662 	 0.04928 	 ~...
   85 	    70 	 0.05138 	 0.04929 	 ~...
    0 	    71 	 0.04076 	 0.04949 	 ~...
   74 	    72 	 0.04858 	 0.04956 	 ~...
   34 	    73 	 0.04598 	 0.04987 	 ~...
   56 	    74 	 0.04734 	 0.05004 	 ~...
   27 	    75 	 0.04583 	 0.05023 	 ~...
   21 	    76 	 0.04558 	 0.05056 	 ~...
   65 	    77 	 0.04766 	 0.05108 	 ~...
   63 	    78 	 0.04766 	 0.05128 	 ~...
   75 	    79 	 0.04864 	 0.05129 	 ~...
   54 	    80 	 0.04716 	 0.05144 	 ~...
    8 	    81 	 0.04331 	 0.05146 	 ~...
   41 	    82 	 0.04610 	 0.05160 	 ~...
   18 	    83 	 0.04547 	 0.05189 	 ~...
   77 	    84 	 0.04948 	 0.05246 	 ~...
   42 	    85 	 0.04619 	 0.05275 	 ~...
   33 	    86 	 0.04597 	 0.05352 	 ~...
   22 	    87 	 0.04562 	 0.05355 	 ~...
    9 	    88 	 0.04435 	 0.05567 	 ~...
   14 	    89 	 0.04490 	 0.05977 	 ~...
   93 	    90 	 0.19706 	 0.18889 	 ~...
   91 	    91 	 0.19359 	 0.18959 	 ~...
   90 	    92 	 0.18340 	 0.19213 	 ~...
   92 	    93 	 0.19425 	 0.19872 	 ~...
   99 	    94 	 0.21919 	 0.20550 	 ~...
   98 	    95 	 0.21846 	 0.20694 	 ~...
  102 	    96 	 0.22725 	 0.20727 	 ~...
   96 	    97 	 0.21583 	 0.21693 	 ~...
  104 	    98 	 0.23485 	 0.21723 	 ~...
  110 	    99 	 0.25191 	 0.21940 	 m..s
   97 	   100 	 0.21839 	 0.21956 	 ~...
  101 	   101 	 0.22652 	 0.23290 	 ~...
  100 	   102 	 0.22097 	 0.23984 	 ~...
  105 	   103 	 0.23607 	 0.24107 	 ~...
  109 	   104 	 0.25091 	 0.24218 	 ~...
  103 	   105 	 0.22968 	 0.24307 	 ~...
  111 	   106 	 0.25422 	 0.25298 	 ~...
   94 	   107 	 0.21355 	 0.25327 	 m..s
  112 	   108 	 0.25606 	 0.27378 	 ~...
  107 	   109 	 0.24302 	 0.27425 	 m..s
  114 	   110 	 0.31004 	 0.27441 	 m..s
   95 	   111 	 0.21356 	 0.27737 	 m..s
  115 	   112 	 0.31019 	 0.28034 	 ~...
  106 	   113 	 0.23763 	 0.28353 	 m..s
  116 	   114 	 0.31713 	 0.29788 	 ~...
  108 	   115 	 0.24773 	 0.29922 	 m..s
  117 	   116 	 0.34758 	 0.31748 	 m..s
  113 	   117 	 0.26995 	 0.33485 	 m..s
  119 	   118 	 0.40880 	 0.39525 	 ~...
  120 	   119 	 0.41682 	 0.52223 	 MISS
  118 	   120 	 0.37570 	 0.53924 	 MISS
==========================================
r_mrr = 0.9777216911315918
r2_mrr = 0.9487298727035522
spearmanr_mrr@5 = 0.9644059538841248
spearmanr_mrr@10 = 0.925354540348053
spearmanr_mrr@50 = 0.9852613806724548
spearmanr_mrr@100 = 0.9909531474113464
spearmanr_mrr@All = 0.9906558990478516
==========================================
test time: 0.416
Done Testing dataset UMLS
total time taken: 141.41916966438293
training time taken: 134.50806164741516
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_complex-umls-mrr-only_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [10, 0]
rank_dist_loss_coeffs: [0, 0]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 10 and 2: 0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0109523255, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.093 +- 0.023
mrr vals (pred, true): 0.080, 0.026
batch losses (mrrl, rdl): 0.0089938752, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.148 +- 0.031
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 3.8067e-06, 0.0

Epoch over!
epoch time: 12.98

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.027 +- 0.006
mrr vals (pred, true): 0.220, 0.252
batch losses (mrrl, rdl): 0.0100570926, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.162 +- 0.009
mrr vals (pred, true): 0.044, 0.026
batch losses (mrrl, rdl): 0.0003531323, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.032 +- 0.002
mrr vals (pred, true): 0.192, 0.192
batch losses (mrrl, rdl): 1.563e-07, 0.0

Epoch over!
epoch time: 13.067

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.159 +- 0.011
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.0002550968, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.024 +- 0.001
mrr vals (pred, true): 0.240, 0.258
batch losses (mrrl, rdl): 0.0034452141, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.124 +- 0.003
mrr vals (pred, true): 0.057, 0.055
batch losses (mrrl, rdl): 0.0004867332, 0.0

Epoch over!
epoch time: 13.112

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.163 +- 0.006
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003782859, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.132 +- 0.003
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 0.000114897, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.138 +- 0.004
mrr vals (pred, true): 0.051, 0.025
batch losses (mrrl, rdl): 2.21252e-05, 0.0

Epoch over!
epoch time: 12.956

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.175 +- 0.002
mrr vals (pred, true): 0.041, 0.024
batch losses (mrrl, rdl): 0.0008360532, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.156 +- 0.002
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001846436, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.135 +- 0.002
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 4.92039e-05, 0.0

Epoch over!
epoch time: 13.321

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.139 +- 0.001
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 9.7682e-06, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.141 +- 0.001
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 7.883e-07, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.143 +- 0.002
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 6.524e-07, 0.0

Epoch over!
epoch time: 13.106

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 535
rank avg (pred): 0.127 +- 0.000
mrr vals (pred, true): 0.056, 0.024
batch losses (mrrl, rdl): 0.0003234339, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1134
rank avg (pred): 0.130 +- 0.000
mrr vals (pred, true): 0.054, 0.023
batch losses (mrrl, rdl): 0.000187051, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 161
rank avg (pred): 0.142 +- 0.001
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 3.932e-07, 0.0

Epoch over!
epoch time: 12.786

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1191
rank avg (pred): 0.168 +- 0.001
mrr vals (pred, true): 0.042, 0.052
batch losses (mrrl, rdl): 0.0005756479, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.149 +- 0.001
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 4.85754e-05, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 150
rank avg (pred): 0.146 +- 0.001
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 1.79106e-05, 0.0

Epoch over!
epoch time: 12.979

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 447
rank avg (pred): 0.152 +- 0.001
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 9.91369e-05, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 117
rank avg (pred): 0.140 +- 0.000
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 4.8786e-06, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.124 +- 0.000
mrr vals (pred, true): 0.057, 0.046
batch losses (mrrl, rdl): 0.0004301793, 0.0

Epoch over!
epoch time: 13.14

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 440
rank avg (pred): 0.135 +- 0.000
mrr vals (pred, true): 0.052, 0.056
batch losses (mrrl, rdl): 4.92184e-05, 0.0

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 321
rank avg (pred): 0.024 +- 0.000
mrr vals (pred, true): 0.236, 0.176
batch losses (mrrl, rdl): 0.0360369012, 0.0

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.131 +- 0.000
mrr vals (pred, true): 0.054, 0.039
batch losses (mrrl, rdl): 0.0001620102, 0.0

Epoch over!
epoch time: 13.454

Saving checkpoint at [1] epoch 10
Done training phase:  0
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.126 +- 0.000
mrr vals (pred, true): 0.056, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   86 	     0 	 0.05590 	 0.01773 	 m..s
   88 	     1 	 0.05773 	 0.02009 	 m..s
   54 	     2 	 0.05288 	 0.02164 	 m..s
   54 	     3 	 0.05288 	 0.02192 	 m..s
   70 	     4 	 0.05332 	 0.02235 	 m..s
   54 	     5 	 0.05288 	 0.02398 	 ~...
   65 	     6 	 0.05299 	 0.02414 	 ~...
   84 	     7 	 0.05561 	 0.02414 	 m..s
   54 	     8 	 0.05288 	 0.02423 	 ~...
   81 	     9 	 0.05455 	 0.02467 	 ~...
   89 	    10 	 0.05787 	 0.02475 	 m..s
   54 	    11 	 0.05288 	 0.02496 	 ~...
   71 	    12 	 0.05364 	 0.02544 	 ~...
   54 	    13 	 0.05288 	 0.02586 	 ~...
    5 	    14 	 0.04502 	 0.02933 	 ~...
    0 	    15 	 0.04315 	 0.03159 	 ~...
    7 	    16 	 0.04510 	 0.03197 	 ~...
   11 	    17 	 0.04595 	 0.03458 	 ~...
   85 	    18 	 0.05562 	 0.03549 	 ~...
    6 	    19 	 0.04508 	 0.03582 	 ~...
   45 	    20 	 0.04983 	 0.03732 	 ~...
   83 	    21 	 0.05551 	 0.03811 	 ~...
   75 	    22 	 0.05397 	 0.03856 	 ~...
   25 	    23 	 0.04742 	 0.03931 	 ~...
   78 	    24 	 0.05432 	 0.03957 	 ~...
   12 	    25 	 0.04599 	 0.03980 	 ~...
   64 	    26 	 0.05294 	 0.04032 	 ~...
   63 	    27 	 0.05291 	 0.04070 	 ~...
   46 	    28 	 0.05004 	 0.04080 	 ~...
    1 	    29 	 0.04362 	 0.04099 	 ~...
   54 	    30 	 0.05288 	 0.04129 	 ~...
   16 	    31 	 0.04653 	 0.04130 	 ~...
   74 	    32 	 0.05384 	 0.04163 	 ~...
   72 	    33 	 0.05380 	 0.04165 	 ~...
    8 	    34 	 0.04519 	 0.04176 	 ~...
   47 	    35 	 0.05109 	 0.04224 	 ~...
   21 	    36 	 0.04701 	 0.04272 	 ~...
   43 	    37 	 0.04977 	 0.04276 	 ~...
   77 	    38 	 0.05429 	 0.04306 	 ~...
   26 	    39 	 0.04786 	 0.04338 	 ~...
   15 	    40 	 0.04648 	 0.04372 	 ~...
   73 	    41 	 0.05381 	 0.04373 	 ~...
   17 	    42 	 0.04665 	 0.04388 	 ~...
   20 	    43 	 0.04683 	 0.04410 	 ~...
   49 	    44 	 0.05116 	 0.04419 	 ~...
   28 	    45 	 0.04829 	 0.04420 	 ~...
   52 	    46 	 0.05217 	 0.04423 	 ~...
   31 	    47 	 0.04844 	 0.04476 	 ~...
   42 	    48 	 0.04969 	 0.04514 	 ~...
   76 	    49 	 0.05414 	 0.04551 	 ~...
   87 	    50 	 0.05623 	 0.04564 	 ~...
   18 	    51 	 0.04682 	 0.04617 	 ~...
   33 	    52 	 0.04869 	 0.04642 	 ~...
   44 	    53 	 0.04983 	 0.04687 	 ~...
    9 	    54 	 0.04573 	 0.04699 	 ~...
   68 	    55 	 0.05313 	 0.04706 	 ~...
   66 	    56 	 0.05302 	 0.04718 	 ~...
   38 	    57 	 0.04935 	 0.04727 	 ~...
   54 	    58 	 0.05288 	 0.04740 	 ~...
   40 	    59 	 0.04966 	 0.04744 	 ~...
   54 	    60 	 0.05288 	 0.04750 	 ~...
   41 	    61 	 0.04967 	 0.04758 	 ~...
   36 	    62 	 0.04918 	 0.04763 	 ~...
   19 	    63 	 0.04682 	 0.04824 	 ~...
   34 	    64 	 0.04903 	 0.04857 	 ~...
   32 	    65 	 0.04845 	 0.04879 	 ~...
   37 	    66 	 0.04933 	 0.04879 	 ~...
   82 	    67 	 0.05473 	 0.04904 	 ~...
   53 	    68 	 0.05284 	 0.04921 	 ~...
   48 	    69 	 0.05111 	 0.04928 	 ~...
   30 	    70 	 0.04831 	 0.04929 	 ~...
    4 	    71 	 0.04489 	 0.04949 	 ~...
   27 	    72 	 0.04791 	 0.04956 	 ~...
   69 	    73 	 0.05316 	 0.04987 	 ~...
   22 	    74 	 0.04716 	 0.05004 	 ~...
   50 	    75 	 0.05172 	 0.05023 	 ~...
   13 	    76 	 0.04619 	 0.05056 	 ~...
   80 	    77 	 0.05433 	 0.05108 	 ~...
   79 	    78 	 0.05433 	 0.05128 	 ~...
   10 	    79 	 0.04590 	 0.05129 	 ~...
    2 	    80 	 0.04433 	 0.05144 	 ~...
    3 	    81 	 0.04449 	 0.05146 	 ~...
   29 	    82 	 0.04831 	 0.05160 	 ~...
   14 	    83 	 0.04625 	 0.05189 	 ~...
   35 	    84 	 0.04909 	 0.05246 	 ~...
   24 	    85 	 0.04727 	 0.05275 	 ~...
   51 	    86 	 0.05190 	 0.05352 	 ~...
   23 	    87 	 0.04719 	 0.05355 	 ~...
   39 	    88 	 0.04947 	 0.05567 	 ~...
   67 	    89 	 0.05308 	 0.05977 	 ~...
   93 	    90 	 0.21453 	 0.18889 	 ~...
   91 	    91 	 0.20357 	 0.18959 	 ~...
   90 	    92 	 0.20112 	 0.19213 	 ~...
   92 	    93 	 0.21448 	 0.19872 	 ~...
   95 	    94 	 0.22339 	 0.20550 	 ~...
   97 	    95 	 0.22464 	 0.20694 	 ~...
   99 	    96 	 0.23300 	 0.20727 	 ~...
   94 	    97 	 0.21963 	 0.21693 	 ~...
  100 	    98 	 0.23628 	 0.21723 	 ~...
  102 	    99 	 0.24287 	 0.21940 	 ~...
   96 	   100 	 0.22462 	 0.21956 	 ~...
  103 	   101 	 0.24378 	 0.23290 	 ~...
   98 	   102 	 0.23204 	 0.23984 	 ~...
  105 	   103 	 0.24711 	 0.24107 	 ~...
  112 	   104 	 0.28438 	 0.24218 	 m..s
  104 	   105 	 0.24507 	 0.24307 	 ~...
  106 	   106 	 0.24773 	 0.25298 	 ~...
  101 	   107 	 0.23950 	 0.25327 	 ~...
  108 	   108 	 0.25684 	 0.27378 	 ~...
  109 	   109 	 0.26056 	 0.27425 	 ~...
  114 	   110 	 0.34450 	 0.27441 	 m..s
  113 	   111 	 0.29859 	 0.27737 	 ~...
  115 	   112 	 0.34459 	 0.28034 	 m..s
  110 	   113 	 0.26920 	 0.28353 	 ~...
  116 	   114 	 0.34619 	 0.29788 	 m..s
  107 	   115 	 0.25396 	 0.29922 	 m..s
  117 	   116 	 0.36505 	 0.31748 	 m..s
  111 	   117 	 0.27405 	 0.33485 	 m..s
  120 	   118 	 0.50215 	 0.39525 	 MISS
  119 	   119 	 0.46921 	 0.52223 	 m..s
  118 	   120 	 0.46111 	 0.53924 	 m..s
==========================================
r_mrr = 0.9801581501960754
r2_mrr = 0.9542145133018494
spearmanr_mrr@5 = 0.9097946286201477
spearmanr_mrr@10 = 0.9323757290840149
spearmanr_mrr@50 = 0.9897531867027283
spearmanr_mrr@100 = 0.9937918186187744
spearmanr_mrr@All = 0.9932413101196289
==========================================
test time: 0.407
Done Testing dataset UMLS
total time taken: 137.1766710281372
training time taken: 131.37083220481873
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_complex-umls-kl-only_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 0]
rank_dist_loss_coeffs: [1, 0]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 10 and 2: 0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 7.86493e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.479 +- 0.007
mrr vals (pred, true): 0.015, 0.026
batch losses (mrrl, rdl): 0.0, 3.26328e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.479 +- 0.209
mrr vals (pred, true): 0.017, 0.050
batch losses (mrrl, rdl): 0.0, 1.93526e-05

Epoch over!
epoch time: 12.668

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.196 +- 0.274
mrr vals (pred, true): 0.084, 0.252
batch losses (mrrl, rdl): 0.0, 4.60789e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.543 +- 0.258
mrr vals (pred, true): 0.031, 0.026
batch losses (mrrl, rdl): 0.0, 2.13303e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.160 +- 0.184
mrr vals (pred, true): 0.120, 0.192
batch losses (mrrl, rdl): 0.0, 1.91073e-05

Epoch over!
epoch time: 12.889

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.441 +- 0.262
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.0, 2.3502e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.185 +- 0.205
mrr vals (pred, true): 0.132, 0.258
batch losses (mrrl, rdl): 0.0, 5.5246e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.576 +- 0.237
mrr vals (pred, true): 0.038, 0.055
batch losses (mrrl, rdl): 0.0, 0.0006423834

Epoch over!
epoch time: 12.845

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.449 +- 0.256
mrr vals (pred, true): 0.083, 0.048
batch losses (mrrl, rdl): 0.0, 2.3996e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.460 +- 0.272
mrr vals (pred, true): 0.072, 0.051
batch losses (mrrl, rdl): 0.0, 6.8821e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.508 +- 0.243
mrr vals (pred, true): 0.057, 0.025
batch losses (mrrl, rdl): 0.0, 1.08657e-05

Epoch over!
epoch time: 13.068

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.506 +- 0.244
mrr vals (pred, true): 0.059, 0.024
batch losses (mrrl, rdl): 0.0, 4.8056e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.439 +- 0.268
mrr vals (pred, true): 0.100, 0.044
batch losses (mrrl, rdl): 0.0, 7.913e-07

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.450 +- 0.267
mrr vals (pred, true): 0.105, 0.044
batch losses (mrrl, rdl): 0.0, 3.8487e-06

Epoch over!
epoch time: 12.714

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.438 +- 0.266
mrr vals (pred, true): 0.108, 0.050
batch losses (mrrl, rdl): 0.0, 1.5731e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.441 +- 0.274
mrr vals (pred, true): 0.121, 0.043
batch losses (mrrl, rdl): 0.0, 1.739e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.440 +- 0.261
mrr vals (pred, true): 0.123, 0.053
batch losses (mrrl, rdl): 0.0, 8.687e-07

Epoch over!
epoch time: 12.879

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 535
rank avg (pred): 0.528 +- 0.256
mrr vals (pred, true): 0.079, 0.024
batch losses (mrrl, rdl): 0.0, 2.2874e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1134
rank avg (pred): 0.494 +- 0.243
mrr vals (pred, true): 0.069, 0.023
batch losses (mrrl, rdl): 0.0, 3.7223e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 161
rank avg (pred): 0.438 +- 0.271
mrr vals (pred, true): 0.124, 0.042
batch losses (mrrl, rdl): 0.0, 1.6481e-06

Epoch over!
epoch time: 13.137

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1191
rank avg (pred): 0.447 +- 0.267
mrr vals (pred, true): 0.103, 0.052
batch losses (mrrl, rdl): 0.0, 1.6004e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.448 +- 0.278
mrr vals (pred, true): 0.127, 0.049
batch losses (mrrl, rdl): 0.0, 2.5541e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 150
rank avg (pred): 0.441 +- 0.275
mrr vals (pred, true): 0.121, 0.050
batch losses (mrrl, rdl): 0.0, 1.5751e-06

Epoch over!
epoch time: 13.07

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 447
rank avg (pred): 0.448 +- 0.278
mrr vals (pred, true): 0.138, 0.049
batch losses (mrrl, rdl): 0.0, 7.156e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 117
rank avg (pred): 0.437 +- 0.280
mrr vals (pred, true): 0.141, 0.050
batch losses (mrrl, rdl): 0.0, 2.3211e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.424 +- 0.256
mrr vals (pred, true): 0.124, 0.046
batch losses (mrrl, rdl): 0.0, 8.6337e-06

Epoch over!
epoch time: 12.924

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 440
rank avg (pred): 0.454 +- 0.275
mrr vals (pred, true): 0.121, 0.056
batch losses (mrrl, rdl): 0.0, 4.1142e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 321
rank avg (pred): 0.176 +- 0.180
mrr vals (pred, true): 0.159, 0.176
batch losses (mrrl, rdl): 0.0, 1.07174e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.451 +- 0.256
mrr vals (pred, true): 0.094, 0.039
batch losses (mrrl, rdl): 0.0, 4.761e-07

Epoch over!
epoch time: 12.723

Saving checkpoint at [1] epoch 10
Done training phase:  0
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.596 +- 0.219
mrr vals (pred, true): 0.048, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04822 	 0.01773 	 m..s
    0 	     1 	 0.04814 	 0.02009 	 ~...
    4 	     2 	 0.05770 	 0.02164 	 m..s
    2 	     3 	 0.05279 	 0.02192 	 m..s
   11 	     4 	 0.06235 	 0.02235 	 m..s
    8 	     5 	 0.05949 	 0.02398 	 m..s
    7 	     6 	 0.05933 	 0.02414 	 m..s
   10 	     7 	 0.06091 	 0.02414 	 m..s
    3 	     8 	 0.05754 	 0.02423 	 m..s
    9 	     9 	 0.05967 	 0.02467 	 m..s
   12 	    10 	 0.06242 	 0.02475 	 m..s
    6 	    11 	 0.05823 	 0.02496 	 m..s
   13 	    12 	 0.06274 	 0.02544 	 m..s
    5 	    13 	 0.05792 	 0.02586 	 m..s
   38 	    14 	 0.11502 	 0.02933 	 m..s
   32 	    15 	 0.10042 	 0.03159 	 m..s
   36 	    16 	 0.11397 	 0.03197 	 m..s
   33 	    17 	 0.10850 	 0.03458 	 m..s
   25 	    18 	 0.09372 	 0.03549 	 m..s
   37 	    19 	 0.11421 	 0.03582 	 m..s
   83 	    20 	 0.13319 	 0.03732 	 m..s
   30 	    21 	 0.09926 	 0.03811 	 m..s
   31 	    22 	 0.09984 	 0.03856 	 m..s
   50 	    23 	 0.13093 	 0.03931 	 m..s
   14 	    24 	 0.08105 	 0.03957 	 m..s
   69 	    25 	 0.13204 	 0.03980 	 m..s
   23 	    26 	 0.09121 	 0.04032 	 m..s
   22 	    27 	 0.09116 	 0.04070 	 m..s
   80 	    28 	 0.13280 	 0.04080 	 m..s
   87 	    29 	 0.13336 	 0.04099 	 m..s
   24 	    30 	 0.09321 	 0.04129 	 m..s
   42 	    31 	 0.13059 	 0.04130 	 m..s
   17 	    32 	 0.08751 	 0.04163 	 m..s
   19 	    33 	 0.08809 	 0.04165 	 m..s
   68 	    34 	 0.13200 	 0.04176 	 m..s
   78 	    35 	 0.13275 	 0.04224 	 m..s
   44 	    36 	 0.13063 	 0.04272 	 m..s
   77 	    37 	 0.13267 	 0.04276 	 m..s
   26 	    38 	 0.09545 	 0.04306 	 m..s
   46 	    39 	 0.13067 	 0.04338 	 m..s
   89 	    40 	 0.13357 	 0.04372 	 m..s
   20 	    41 	 0.08952 	 0.04373 	 m..s
   49 	    42 	 0.13090 	 0.04388 	 m..s
   85 	    43 	 0.13333 	 0.04410 	 m..s
   79 	    44 	 0.13278 	 0.04419 	 m..s
   51 	    45 	 0.13101 	 0.04420 	 m..s
   54 	    46 	 0.13133 	 0.04423 	 m..s
   52 	    47 	 0.13120 	 0.04476 	 m..s
   66 	    48 	 0.13194 	 0.04514 	 m..s
   62 	    49 	 0.13169 	 0.04551 	 m..s
   29 	    50 	 0.09780 	 0.04564 	 m..s
   81 	    51 	 0.13288 	 0.04617 	 m..s
   76 	    52 	 0.13251 	 0.04642 	 m..s
   61 	    53 	 0.13163 	 0.04687 	 m..s
   34 	    54 	 0.10924 	 0.04699 	 m..s
   70 	    55 	 0.13207 	 0.04706 	 m..s
   18 	    56 	 0.08784 	 0.04718 	 m..s
   58 	    57 	 0.13141 	 0.04727 	 m..s
   27 	    58 	 0.09666 	 0.04740 	 m..s
   60 	    59 	 0.13160 	 0.04744 	 m..s
   28 	    60 	 0.09763 	 0.04750 	 m..s
   56 	    61 	 0.13133 	 0.04758 	 m..s
   86 	    62 	 0.13333 	 0.04763 	 m..s
   43 	    63 	 0.13060 	 0.04824 	 m..s
   45 	    64 	 0.13063 	 0.04857 	 m..s
   55 	    65 	 0.13133 	 0.04879 	 m..s
   67 	    66 	 0.13198 	 0.04879 	 m..s
   71 	    67 	 0.13209 	 0.04904 	 m..s
   74 	    68 	 0.13227 	 0.04921 	 m..s
   73 	    69 	 0.13225 	 0.04928 	 m..s
   41 	    70 	 0.13053 	 0.04929 	 m..s
   35 	    71 	 0.11053 	 0.04949 	 m..s
   59 	    72 	 0.13158 	 0.04956 	 m..s
   84 	    73 	 0.13331 	 0.04987 	 m..s
   72 	    74 	 0.13224 	 0.05004 	 m..s
   65 	    75 	 0.13194 	 0.05023 	 m..s
   47 	    76 	 0.13073 	 0.05056 	 m..s
   16 	    77 	 0.08237 	 0.05108 	 m..s
   15 	    78 	 0.08229 	 0.05128 	 m..s
   63 	    79 	 0.13170 	 0.05129 	 m..s
   88 	    80 	 0.13345 	 0.05144 	 m..s
   64 	    81 	 0.13185 	 0.05146 	 m..s
   48 	    82 	 0.13075 	 0.05160 	 m..s
   57 	    83 	 0.13136 	 0.05189 	 m..s
   53 	    84 	 0.13125 	 0.05246 	 m..s
   40 	    85 	 0.13027 	 0.05275 	 m..s
   75 	    86 	 0.13242 	 0.05352 	 m..s
   39 	    87 	 0.13026 	 0.05355 	 m..s
   82 	    88 	 0.13291 	 0.05567 	 m..s
   21 	    89 	 0.09004 	 0.05977 	 m..s
   93 	    90 	 0.16407 	 0.18889 	 ~...
   91 	    91 	 0.15824 	 0.18959 	 m..s
   90 	    92 	 0.14976 	 0.19213 	 m..s
   92 	    93 	 0.16011 	 0.19872 	 m..s
   97 	    94 	 0.17205 	 0.20550 	 m..s
  100 	    95 	 0.17508 	 0.20694 	 m..s
  105 	    96 	 0.18375 	 0.20727 	 ~...
  101 	    97 	 0.17508 	 0.21693 	 m..s
  108 	    98 	 0.18666 	 0.21723 	 m..s
  110 	    99 	 0.19482 	 0.21940 	 ~...
   99 	   100 	 0.17504 	 0.21956 	 m..s
   95 	   101 	 0.16716 	 0.23290 	 m..s
  103 	   102 	 0.17822 	 0.23984 	 m..s
  106 	   103 	 0.18495 	 0.24107 	 m..s
  112 	   104 	 0.19806 	 0.24218 	 m..s
   96 	   105 	 0.16781 	 0.24307 	 m..s
  113 	   106 	 0.20136 	 0.25298 	 m..s
   94 	   107 	 0.16610 	 0.25327 	 m..s
  104 	   108 	 0.18157 	 0.27378 	 m..s
   98 	   109 	 0.17285 	 0.27425 	 MISS
  115 	   110 	 0.23624 	 0.27441 	 m..s
  111 	   111 	 0.19607 	 0.27737 	 m..s
  116 	   112 	 0.23679 	 0.28034 	 m..s
  107 	   113 	 0.18604 	 0.28353 	 m..s
  114 	   114 	 0.23032 	 0.29788 	 m..s
  102 	   115 	 0.17716 	 0.29922 	 MISS
  117 	   116 	 0.23762 	 0.31748 	 m..s
  109 	   117 	 0.19275 	 0.33485 	 MISS
  120 	   118 	 0.28610 	 0.39525 	 MISS
  119 	   119 	 0.26326 	 0.52223 	 MISS
  118 	   120 	 0.24149 	 0.53924 	 MISS
==========================================
r_mrr = 0.8511302471160889
r2_mrr = 0.46153193712234497
spearmanr_mrr@5 = 0.9292997121810913
spearmanr_mrr@10 = 0.8830839991569519
spearmanr_mrr@50 = 0.9659311175346375
spearmanr_mrr@100 = 0.9445057511329651
spearmanr_mrr@All = 0.88279789686203
==========================================
test time: 0.416
Done Testing dataset UMLS
total time taken: 134.78030848503113
training time taken: 129.35460376739502
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_complex-umls-both-no-coeff-change_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [10, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 10 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0109523255, 7.86493e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.092 +- 0.026
mrr vals (pred, true): 0.083, 0.026
batch losses (mrrl, rdl): 0.0110881915, 0.0032919813

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.144 +- 0.035
mrr vals (pred, true): 0.054, 0.050
batch losses (mrrl, rdl): 0.0001610359, 0.0019927383

Epoch over!
epoch time: 13.991

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.029 +- 0.007
mrr vals (pred, true): 0.217, 0.252
batch losses (mrrl, rdl): 0.0124365268, 0.00035042

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.183 +- 0.003
mrr vals (pred, true): 0.039, 0.026
batch losses (mrrl, rdl): 0.0011628623, 0.0022360631

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.029 +- 0.001
mrr vals (pred, true): 0.206, 0.192
batch losses (mrrl, rdl): 0.0021662817, 0.0005821529

Epoch over!
epoch time: 13.792

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.175 +- 0.010
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.000819315, 0.0016283033

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.023 +- 0.001
mrr vals (pred, true): 0.242, 0.258
batch losses (mrrl, rdl): 0.0024817956, 0.0006828517

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.126 +- 0.003
mrr vals (pred, true): 0.056, 0.055
batch losses (mrrl, rdl): 0.0003720744, 0.0015964459

Epoch over!
epoch time: 13.751

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.147 +- 0.005
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 2.64941e-05, 0.0018641672

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.148 +- 0.006
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 3.67864e-05, 0.001838233

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.165 +- 0.004
mrr vals (pred, true): 0.043, 0.025
batch losses (mrrl, rdl): 0.0004651216, 0.0028586343

Epoch over!
epoch time: 13.355

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.181 +- 0.001
mrr vals (pred, true): 0.040, 0.024
batch losses (mrrl, rdl): 0.0010673497, 0.0024432756

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.163 +- 0.000
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0003901173, 0.0016423363

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.131 +- 0.002
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001567899, 0.0020388893

Epoch over!
epoch time: 13.489

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.141 +- 0.000
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.721e-07, 0.0020405645

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.156 +- 0.002
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001849758, 0.0017070734

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.131 +- 0.002
mrr vals (pred, true): 0.054, 0.053
batch losses (mrrl, rdl): 0.0001624156, 0.0020979606

Epoch over!
epoch time: 13.251

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 535
rank avg (pred): 0.165 +- 0.001
mrr vals (pred, true): 0.043, 0.024
batch losses (mrrl, rdl): 0.0004532592, 0.002471955

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1134
rank avg (pred): 0.137 +- 0.000
mrr vals (pred, true): 0.052, 0.023
batch losses (mrrl, rdl): 2.87194e-05, 0.0027255677

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 161
rank avg (pred): 0.141 +- 0.000
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 2.333e-07, 0.0020750032

Epoch over!
epoch time: 13.23

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1191
rank avg (pred): 0.151 +- 0.001
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 8.00347e-05, 0.0020063408

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.151 +- 0.001
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 9.30811e-05, 0.0018150517

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 150
rank avg (pred): 0.148 +- 0.001
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 4.29301e-05, 0.0020009722

Epoch over!
epoch time: 13.343

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 447
rank avg (pred): 0.171 +- 0.001
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0006813392, 0.0015129125

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 117
rank avg (pred): 0.159 +- 0.000
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002594477, 0.0018618078

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.132 +- 0.000
mrr vals (pred, true): 0.054, 0.046
batch losses (mrrl, rdl): 0.0001281317, 0.0021589047

Epoch over!
epoch time: 13.156

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 440
rank avg (pred): 0.125 +- 0.000
mrr vals (pred, true): 0.057, 0.056
batch losses (mrrl, rdl): 0.000422694, 0.0021661792

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 321
rank avg (pred): 0.026 +- 0.000
mrr vals (pred, true): 0.223, 0.176
batch losses (mrrl, rdl): 0.0226104427, 0.0006758196

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.149 +- 0.000
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 4.64832e-05, 0.0019756418

Epoch over!
epoch time: 13.258

Saving checkpoint at [1] epoch 10
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1113
rank avg (pred): 0.161 +- 0.000
mrr vals (pred, true): 0.044, 0.054
batch losses (mrrl, rdl): 0.000335489, 0.0017275781

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 145
rank avg (pred): 0.152 +- 0.000
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 0.0001102305, 0.0019467499

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 914
rank avg (pred): 0.130 +- 0.000
mrr vals (pred, true): 0.054, 0.021
batch losses (mrrl, rdl): 0.0001884003, 0.0042762929

Epoch over!
epoch time: 13.348

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 183
rank avg (pred): 0.149 +- 0.000
mrr vals (pred, true): 0.048, 0.046
batch losses (mrrl, rdl): 5.09057e-05, 0.0018524707

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 781
rank avg (pred): 0.139 +- 0.000
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 6.9149e-06, 0.0020316851

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 879
rank avg (pred): 0.131 +- 0.000
mrr vals (pred, true): 0.054, 0.053
batch losses (mrrl, rdl): 0.0001636991, 0.0020281672

Epoch over!
epoch time: 13.235

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1139
rank avg (pred): 0.164 +- 0.000
mrr vals (pred, true): 0.043, 0.023
batch losses (mrrl, rdl): 0.0004245702, 0.0023575891

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.157 +- 0.000
mrr vals (pred, true): 0.045, 0.046
batch losses (mrrl, rdl): 0.0002136279, 0.0018598522

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 205
rank avg (pred): 0.146 +- 0.000
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 1.9699e-05, 0.0019651414

Epoch over!
epoch time: 12.925

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 67
rank avg (pred): 0.026 +- 0.000
mrr vals (pred, true): 0.223, 0.212
batch losses (mrrl, rdl): 0.001193972, 0.0004815785

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1161
rank avg (pred): 0.163 +- 0.000
mrr vals (pred, true): 0.044, 0.041
batch losses (mrrl, rdl): 0.0004026442, 0.00177796

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 726
rank avg (pred): 0.143 +- 0.000
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 2.6314e-06, 0.0018937618

Epoch over!
epoch time: 13.173

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1133
rank avg (pred): 0.142 +- 0.000
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.418e-07, 0.0020212657

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 618
rank avg (pred): 0.142 +- 0.000
mrr vals (pred, true): 0.050, 0.041
batch losses (mrrl, rdl): 2.018e-07, 0.0022233792

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 42
rank avg (pred): 0.034 +- 0.000
mrr vals (pred, true): 0.179, 0.200
batch losses (mrrl, rdl): 0.0047399197, 0.0005785887

Epoch over!
epoch time: 13.152

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 157
rank avg (pred): 0.156 +- 0.000
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001959672, 0.0020067075

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1071
rank avg (pred): 0.017 +- 0.000
mrr vals (pred, true): 0.302, 0.282
batch losses (mrrl, rdl): 0.0038252983, 0.0003391666

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 405
rank avg (pred): 0.145 +- 0.000
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 8.3325e-06, 0.0020856722

Epoch over!
epoch time: 13.221

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 398
rank avg (pred): 0.147 +- 0.000
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 2.64006e-05, 0.0020353741

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 554
rank avg (pred): 0.149 +- 0.000
mrr vals (pred, true): 0.048, 0.028
batch losses (mrrl, rdl): 5.28768e-05, 0.0026771047

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 309
rank avg (pred): 0.018 +- 0.000
mrr vals (pred, true): 0.290, 0.212
batch losses (mrrl, rdl): 0.0596280545, 0.0005888567

Epoch over!
epoch time: 13.172

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.143 +- 0.000
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.1332e-06, 0.0019149709

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 698
rank avg (pred): 0.139 +- 0.000
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 9.2635e-06, 0.0018118231

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 337
rank avg (pred): 0.151 +- 0.000
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 8.78014e-05, 0.0017980227

Epoch over!
epoch time: 13.046

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1112
rank avg (pred): 0.148 +- 0.000
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 3.44721e-05, 0.0017411367

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 761
rank avg (pred): 0.124 +- 0.000
mrr vals (pred, true): 0.057, 0.045
batch losses (mrrl, rdl): 0.0004561933, 0.0021646819

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 101
rank avg (pred): 0.139 +- 0.000
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 1.23139e-05, 0.0020493949

Epoch over!
epoch time: 13.196

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1153
rank avg (pred): 0.145 +- 0.000
mrr vals (pred, true): 0.049, 0.028
batch losses (mrrl, rdl): 1.15442e-05, 0.0024372803

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1059
rank avg (pred): 0.021 +- 0.000
mrr vals (pred, true): 0.265, 0.282
batch losses (mrrl, rdl): 0.0028424682, 0.0005428933

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 886
rank avg (pred): 0.156 +- 0.000
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001934974, 0.0018581123

Epoch over!
epoch time: 13.118

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.134 +- 0.000
mrr vals (pred, true): 0.053, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   87 	     0 	 0.05270 	 0.01773 	 m..s
   89 	     1 	 0.05290 	 0.02009 	 m..s
   50 	     2 	 0.04887 	 0.02164 	 ~...
   78 	     3 	 0.05049 	 0.02192 	 ~...
   38 	     4 	 0.04865 	 0.02235 	 ~...
   37 	     5 	 0.04865 	 0.02398 	 ~...
   39 	     6 	 0.04870 	 0.02414 	 ~...
   80 	     7 	 0.05103 	 0.02414 	 ~...
   49 	     8 	 0.04887 	 0.02423 	 ~...
   46 	     9 	 0.04879 	 0.02467 	 ~...
   66 	    10 	 0.04987 	 0.02475 	 ~...
   45 	    11 	 0.04879 	 0.02496 	 ~...
   68 	    12 	 0.04990 	 0.02544 	 ~...
   41 	    13 	 0.04874 	 0.02586 	 ~...
    0 	    14 	 0.04331 	 0.02933 	 ~...
    4 	    15 	 0.04361 	 0.03159 	 ~...
    2 	    16 	 0.04334 	 0.03197 	 ~...
    6 	    17 	 0.04388 	 0.03458 	 ~...
   11 	    18 	 0.04549 	 0.03549 	 ~...
    1 	    19 	 0.04333 	 0.03582 	 ~...
   82 	    20 	 0.05123 	 0.03732 	 ~...
   21 	    21 	 0.04620 	 0.03811 	 ~...
   22 	    22 	 0.04641 	 0.03856 	 ~...
   77 	    23 	 0.05031 	 0.03931 	 ~...
   34 	    24 	 0.04762 	 0.03957 	 ~...
   47 	    25 	 0.04880 	 0.03980 	 ~...
   16 	    26 	 0.04588 	 0.04032 	 ~...
   17 	    27 	 0.04588 	 0.04070 	 ~...
   56 	    28 	 0.04911 	 0.04080 	 ~...
   67 	    29 	 0.04988 	 0.04099 	 ~...
   15 	    30 	 0.04580 	 0.04129 	 ~...
   79 	    31 	 0.05078 	 0.04130 	 ~...
   29 	    32 	 0.04721 	 0.04163 	 ~...
   28 	    33 	 0.04703 	 0.04165 	 ~...
   31 	    34 	 0.04758 	 0.04176 	 ~...
    9 	    35 	 0.04513 	 0.04224 	 ~...
   71 	    36 	 0.05011 	 0.04272 	 ~...
   63 	    37 	 0.04971 	 0.04276 	 ~...
   24 	    38 	 0.04665 	 0.04306 	 ~...
    8 	    39 	 0.04486 	 0.04338 	 ~...
   83 	    40 	 0.05133 	 0.04372 	 ~...
   27 	    41 	 0.04698 	 0.04373 	 ~...
   65 	    42 	 0.04977 	 0.04388 	 ~...
   84 	    43 	 0.05149 	 0.04410 	 ~...
   18 	    44 	 0.04594 	 0.04419 	 ~...
   55 	    45 	 0.04902 	 0.04420 	 ~...
   40 	    46 	 0.04872 	 0.04423 	 ~...
   42 	    47 	 0.04877 	 0.04476 	 ~...
   69 	    48 	 0.05007 	 0.04514 	 ~...
   30 	    49 	 0.04728 	 0.04551 	 ~...
   23 	    50 	 0.04658 	 0.04564 	 ~...
   86 	    51 	 0.05192 	 0.04617 	 ~...
   64 	    52 	 0.04972 	 0.04642 	 ~...
   26 	    53 	 0.04691 	 0.04687 	 ~...
    5 	    54 	 0.04384 	 0.04699 	 ~...
   61 	    55 	 0.04955 	 0.04706 	 ~...
   19 	    56 	 0.04596 	 0.04718 	 ~...
   73 	    57 	 0.05023 	 0.04727 	 ~...
   12 	    58 	 0.04555 	 0.04740 	 ~...
   25 	    59 	 0.04689 	 0.04744 	 ~...
   10 	    60 	 0.04527 	 0.04750 	 ~...
   60 	    61 	 0.04945 	 0.04758 	 ~...
   52 	    62 	 0.04895 	 0.04763 	 ~...
   35 	    63 	 0.04764 	 0.04824 	 ~...
   53 	    64 	 0.04895 	 0.04857 	 ~...
   44 	    65 	 0.04879 	 0.04879 	 ~...
   76 	    66 	 0.05029 	 0.04879 	 ~...
   72 	    67 	 0.05022 	 0.04904 	 ~...
   51 	    68 	 0.04891 	 0.04921 	 ~...
   74 	    69 	 0.05028 	 0.04928 	 ~...
   81 	    70 	 0.05110 	 0.04929 	 ~...
    3 	    71 	 0.04355 	 0.04949 	 ~...
   58 	    72 	 0.04918 	 0.04956 	 ~...
   62 	    73 	 0.04970 	 0.04987 	 ~...
    7 	    74 	 0.04403 	 0.05004 	 ~...
   43 	    75 	 0.04878 	 0.05023 	 ~...
   14 	    76 	 0.04560 	 0.05056 	 ~...
   32 	    77 	 0.04760 	 0.05108 	 ~...
   33 	    78 	 0.04760 	 0.05128 	 ~...
   13 	    79 	 0.04559 	 0.05129 	 ~...
   59 	    80 	 0.04929 	 0.05144 	 ~...
   75 	    81 	 0.05028 	 0.05146 	 ~...
   54 	    82 	 0.04896 	 0.05160 	 ~...
   57 	    83 	 0.04918 	 0.05189 	 ~...
   88 	    84 	 0.05272 	 0.05246 	 ~...
   36 	    85 	 0.04860 	 0.05275 	 ~...
   48 	    86 	 0.04885 	 0.05352 	 ~...
   70 	    87 	 0.05009 	 0.05355 	 ~...
   85 	    88 	 0.05176 	 0.05567 	 ~...
   20 	    89 	 0.04617 	 0.05977 	 ~...
   91 	    90 	 0.20674 	 0.18889 	 ~...
   90 	    91 	 0.20421 	 0.18959 	 ~...
   92 	    92 	 0.20708 	 0.19213 	 ~...
   93 	    93 	 0.21314 	 0.19872 	 ~...
  102 	    94 	 0.23498 	 0.20550 	 ~...
   95 	    95 	 0.21929 	 0.20694 	 ~...
   97 	    96 	 0.22385 	 0.20727 	 ~...
   96 	    97 	 0.21994 	 0.21693 	 ~...
  100 	    98 	 0.22980 	 0.21723 	 ~...
  104 	    99 	 0.23969 	 0.21940 	 ~...
   94 	   100 	 0.21928 	 0.21956 	 ~...
  106 	   101 	 0.24611 	 0.23290 	 ~...
   99 	   102 	 0.22662 	 0.23984 	 ~...
  101 	   103 	 0.23082 	 0.24107 	 ~...
  113 	   104 	 0.29249 	 0.24218 	 m..s
  103 	   105 	 0.23899 	 0.24307 	 ~...
  107 	   106 	 0.25409 	 0.25298 	 ~...
   98 	   107 	 0.22540 	 0.25327 	 ~...
  109 	   108 	 0.26499 	 0.27378 	 ~...
  105 	   109 	 0.24133 	 0.27425 	 m..s
  117 	   110 	 0.32000 	 0.27441 	 m..s
  112 	   111 	 0.29208 	 0.27737 	 ~...
  116 	   112 	 0.31998 	 0.28034 	 m..s
  111 	   113 	 0.29047 	 0.28353 	 ~...
  114 	   114 	 0.31155 	 0.29788 	 ~...
  108 	   115 	 0.26360 	 0.29922 	 m..s
  115 	   116 	 0.31433 	 0.31748 	 ~...
  110 	   117 	 0.26688 	 0.33485 	 m..s
  120 	   118 	 0.48032 	 0.39525 	 m..s
  119 	   119 	 0.43478 	 0.52223 	 m..s
  118 	   120 	 0.41627 	 0.53924 	 MISS
==========================================
r_mrr = 0.9798290729522705
r2_mrr = 0.9571400284767151
spearmanr_mrr@5 = 0.9345096349716187
spearmanr_mrr@10 = 0.9673887491226196
spearmanr_mrr@50 = 0.9909743070602417
spearmanr_mrr@100 = 0.9944669008255005
spearmanr_mrr@All = 0.9937779903411865
==========================================
test time: 0.477
Done Testing dataset UMLS
total time taken: 272.87169885635376
training time taken: 266.71145129203796
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_complex-umls-final_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 10 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 7.86493e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.479 +- 0.007
mrr vals (pred, true): 0.015, 0.026
batch losses (mrrl, rdl): 0.0, 3.26328e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.479 +- 0.209
mrr vals (pred, true): 0.017, 0.050
batch losses (mrrl, rdl): 0.0, 1.93526e-05

Epoch over!
epoch time: 12.927

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.196 +- 0.274
mrr vals (pred, true): 0.084, 0.252
batch losses (mrrl, rdl): 0.0, 4.60789e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.543 +- 0.258
mrr vals (pred, true): 0.031, 0.026
batch losses (mrrl, rdl): 0.0, 2.13303e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.160 +- 0.184
mrr vals (pred, true): 0.120, 0.192
batch losses (mrrl, rdl): 0.0, 1.91073e-05

Epoch over!
epoch time: 12.885

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.441 +- 0.262
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.0, 2.3502e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.185 +- 0.205
mrr vals (pred, true): 0.132, 0.258
batch losses (mrrl, rdl): 0.0, 5.5246e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.576 +- 0.237
mrr vals (pred, true): 0.038, 0.055
batch losses (mrrl, rdl): 0.0, 0.0006423834

Epoch over!
epoch time: 12.882

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.449 +- 0.256
mrr vals (pred, true): 0.083, 0.048
batch losses (mrrl, rdl): 0.0, 2.3996e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.460 +- 0.272
mrr vals (pred, true): 0.072, 0.051
batch losses (mrrl, rdl): 0.0, 6.8821e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.508 +- 0.243
mrr vals (pred, true): 0.057, 0.025
batch losses (mrrl, rdl): 0.0, 1.08657e-05

Epoch over!
epoch time: 12.871

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.506 +- 0.244
mrr vals (pred, true): 0.059, 0.024
batch losses (mrrl, rdl): 0.0, 4.8056e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.439 +- 0.268
mrr vals (pred, true): 0.100, 0.044
batch losses (mrrl, rdl): 0.0, 7.913e-07

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.450 +- 0.267
mrr vals (pred, true): 0.105, 0.044
batch losses (mrrl, rdl): 0.0, 3.8487e-06

Epoch over!
epoch time: 12.84

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.438 +- 0.266
mrr vals (pred, true): 0.108, 0.050
batch losses (mrrl, rdl): 0.0, 1.5731e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.441 +- 0.274
mrr vals (pred, true): 0.121, 0.043
batch losses (mrrl, rdl): 0.0, 1.739e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.440 +- 0.261
mrr vals (pred, true): 0.123, 0.053
batch losses (mrrl, rdl): 0.0, 8.687e-07

Epoch over!
epoch time: 12.962

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 535
rank avg (pred): 0.528 +- 0.256
mrr vals (pred, true): 0.079, 0.024
batch losses (mrrl, rdl): 0.0, 2.2874e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1134
rank avg (pred): 0.494 +- 0.243
mrr vals (pred, true): 0.069, 0.023
batch losses (mrrl, rdl): 0.0, 3.7223e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 161
rank avg (pred): 0.438 +- 0.271
mrr vals (pred, true): 0.124, 0.042
batch losses (mrrl, rdl): 0.0, 1.6481e-06

Epoch over!
epoch time: 12.995

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1191
rank avg (pred): 0.447 +- 0.267
mrr vals (pred, true): 0.103, 0.052
batch losses (mrrl, rdl): 0.0, 1.6004e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.448 +- 0.278
mrr vals (pred, true): 0.127, 0.049
batch losses (mrrl, rdl): 0.0, 2.5541e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 150
rank avg (pred): 0.441 +- 0.275
mrr vals (pred, true): 0.121, 0.050
batch losses (mrrl, rdl): 0.0, 1.5751e-06

Epoch over!
epoch time: 13.128

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 447
rank avg (pred): 0.448 +- 0.278
mrr vals (pred, true): 0.138, 0.049
batch losses (mrrl, rdl): 0.0, 7.156e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 117
rank avg (pred): 0.437 +- 0.280
mrr vals (pred, true): 0.141, 0.050
batch losses (mrrl, rdl): 0.0, 2.3211e-06

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.424 +- 0.256
mrr vals (pred, true): 0.124, 0.046
batch losses (mrrl, rdl): 0.0, 8.6337e-06

Epoch over!
epoch time: 13.082

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 440
rank avg (pred): 0.454 +- 0.275
mrr vals (pred, true): 0.121, 0.056
batch losses (mrrl, rdl): 0.0, 4.1142e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 321
rank avg (pred): 0.176 +- 0.180
mrr vals (pred, true): 0.159, 0.176
batch losses (mrrl, rdl): 0.0, 1.07174e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.451 +- 0.256
mrr vals (pred, true): 0.094, 0.039
batch losses (mrrl, rdl): 0.0, 4.761e-07

Epoch over!
epoch time: 12.902

Saving checkpoint at [1] epoch 10
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1113
rank avg (pred): 0.443 +- 0.274
mrr vals (pred, true): 0.132, 0.054
batch losses (mrrl, rdl): 0.0678112507, 1.2604e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 145
rank avg (pred): 0.407 +- 0.119
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 6.1383e-05, 8.04181e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 914
rank avg (pred): 0.646 +- 0.253
mrr vals (pred, true): 0.047, 0.021
batch losses (mrrl, rdl): 7.5856e-05, 0.0001084991

Epoch over!
epoch time: 13.121

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 183
rank avg (pred): 0.415 +- 0.110
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 1.70857e-05, 5.95853e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 781
rank avg (pred): 0.413 +- 0.114
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 0.0001207228, 6.45431e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 879
rank avg (pred): 0.405 +- 0.115
mrr vals (pred, true): 0.058, 0.053
batch losses (mrrl, rdl): 0.0005911762, 7.4593e-05

Epoch over!
epoch time: 13.285

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1139
rank avg (pred): 0.556 +- 0.217
mrr vals (pred, true): 0.056, 0.023
batch losses (mrrl, rdl): 0.0003680596, 9.6934e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.426 +- 0.102
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 4.56246e-05, 6.98363e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 205
rank avg (pred): 0.425 +- 0.101
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 1e-10, 6.81502e-05

Epoch over!
epoch time: 13.031

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 67
rank avg (pred): 0.122 +- 0.074
mrr vals (pred, true): 0.226, 0.212
batch losses (mrrl, rdl): 0.0019579432, 7.51453e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1161
rank avg (pred): 0.428 +- 0.086
mrr vals (pred, true): 0.045, 0.041
batch losses (mrrl, rdl): 0.0002320788, 6.33712e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 726
rank avg (pred): 0.433 +- 0.097
mrr vals (pred, true): 0.052, 0.048
batch losses (mrrl, rdl): 4.02267e-05, 6.81701e-05

Epoch over!
epoch time: 13.162

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1133
rank avg (pred): 0.430 +- 0.097
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 8.76467e-05, 6.93331e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 618
rank avg (pred): 0.419 +- 0.119
mrr vals (pred, true): 0.063, 0.041
batch losses (mrrl, rdl): 0.0016872718, 7.90273e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 42
rank avg (pred): 0.183 +- 0.108
mrr vals (pred, true): 0.213, 0.200
batch losses (mrrl, rdl): 0.0016892863, 2.21429e-05

Epoch over!
epoch time: 13.211

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 157
rank avg (pred): 0.436 +- 0.098
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001609922, 6.95124e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1071
rank avg (pred): 0.066 +- 0.039
mrr vals (pred, true): 0.283, 0.282
batch losses (mrrl, rdl): 2.105e-07, 0.0001365167

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 405
rank avg (pred): 0.436 +- 0.095
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001035792, 7.34759e-05

Epoch over!
epoch time: 13.307

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 398
rank avg (pred): 0.434 +- 0.091
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 1.24825e-05, 6.41473e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 554
rank avg (pred): 0.501 +- 0.149
mrr vals (pred, true): 0.052, 0.028
batch losses (mrrl, rdl): 3.61083e-05, 1.20091e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 309
rank avg (pred): 0.179 +- 0.122
mrr vals (pred, true): 0.264, 0.212
batch losses (mrrl, rdl): 0.0261527151, 1.1869e-05

Epoch over!
epoch time: 13.869

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 162
rank avg (pred): 0.443 +- 0.090
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.3724e-06, 5.98922e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 698
rank avg (pred): 0.440 +- 0.095
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 6.70338e-05, 5.84498e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 337
rank avg (pred): 0.439 +- 0.086
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 2.2542e-06, 6.46524e-05

Epoch over!
epoch time: 13.567

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1112
rank avg (pred): 0.437 +- 0.086
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 8.016e-07, 6.21113e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 761
rank avg (pred): 0.437 +- 0.087
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 5.6129e-06, 5.88278e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 101
rank avg (pred): 0.450 +- 0.090
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.3864e-06, 6.75527e-05

Epoch over!
epoch time: 13.609

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1153
rank avg (pred): 0.591 +- 0.185
mrr vals (pred, true): 0.041, 0.028
batch losses (mrrl, rdl): 0.0007854118, 0.0002085591

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1059
rank avg (pred): 0.093 +- 0.060
mrr vals (pred, true): 0.289, 0.282
batch losses (mrrl, rdl): 0.0005688737, 0.0001756127

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 886
rank avg (pred): 0.437 +- 0.084
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 5.986e-06, 7.3419e-05

Epoch over!
epoch time: 14.045

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.500 +- 0.192
mrr vals (pred, true): 0.051, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   54 	     0 	 0.05146 	 0.01773 	 m..s
   77 	     1 	 0.05208 	 0.02009 	 m..s
   56 	     2 	 0.05147 	 0.02164 	 ~...
    5 	     3 	 0.05046 	 0.02192 	 ~...
    0 	     4 	 0.04867 	 0.02235 	 ~...
    1 	     5 	 0.05029 	 0.02398 	 ~...
   24 	     6 	 0.05090 	 0.02414 	 ~...
   67 	     7 	 0.05181 	 0.02414 	 ~...
   51 	     8 	 0.05140 	 0.02423 	 ~...
   10 	     9 	 0.05052 	 0.02467 	 ~...
   73 	    10 	 0.05202 	 0.02475 	 ~...
   58 	    11 	 0.05153 	 0.02496 	 ~...
   47 	    12 	 0.05130 	 0.02544 	 ~...
   31 	    13 	 0.05106 	 0.02586 	 ~...
   16 	    14 	 0.05078 	 0.02933 	 ~...
    7 	    15 	 0.05049 	 0.03159 	 ~...
    8 	    16 	 0.05050 	 0.03197 	 ~...
   15 	    17 	 0.05076 	 0.03458 	 ~...
   74 	    18 	 0.05206 	 0.03549 	 ~...
   11 	    19 	 0.05057 	 0.03582 	 ~...
   50 	    20 	 0.05140 	 0.03732 	 ~...
   68 	    21 	 0.05189 	 0.03811 	 ~...
   26 	    22 	 0.05096 	 0.03856 	 ~...
   39 	    23 	 0.05123 	 0.03931 	 ~...
   88 	    24 	 0.05255 	 0.03957 	 ~...
   78 	    25 	 0.05212 	 0.03980 	 ~...
   75 	    26 	 0.05207 	 0.04032 	 ~...
   76 	    27 	 0.05208 	 0.04070 	 ~...
   69 	    28 	 0.05189 	 0.04080 	 ~...
   46 	    29 	 0.05130 	 0.04099 	 ~...
   89 	    30 	 0.05262 	 0.04129 	 ~...
   60 	    31 	 0.05158 	 0.04130 	 ~...
   81 	    32 	 0.05218 	 0.04163 	 ~...
   86 	    33 	 0.05226 	 0.04165 	 ~...
   72 	    34 	 0.05201 	 0.04176 	 ~...
   42 	    35 	 0.05129 	 0.04224 	 ~...
   25 	    36 	 0.05090 	 0.04272 	 ~...
   55 	    37 	 0.05147 	 0.04276 	 ~...
   48 	    38 	 0.05133 	 0.04306 	 ~...
   14 	    39 	 0.05065 	 0.04338 	 ~...
   28 	    40 	 0.05099 	 0.04372 	 ~...
   70 	    41 	 0.05191 	 0.04373 	 ~...
   36 	    42 	 0.05114 	 0.04388 	 ~...
   49 	    43 	 0.05136 	 0.04410 	 ~...
   53 	    44 	 0.05143 	 0.04419 	 ~...
   13 	    45 	 0.05063 	 0.04420 	 ~...
   22 	    46 	 0.05088 	 0.04423 	 ~...
    6 	    47 	 0.05047 	 0.04476 	 ~...
   40 	    48 	 0.05124 	 0.04514 	 ~...
   61 	    49 	 0.05164 	 0.04551 	 ~...
   65 	    50 	 0.05175 	 0.04564 	 ~...
   63 	    51 	 0.05173 	 0.04617 	 ~...
   52 	    52 	 0.05141 	 0.04642 	 ~...
   18 	    53 	 0.05079 	 0.04687 	 ~...
    2 	    54 	 0.05039 	 0.04699 	 ~...
   45 	    55 	 0.05130 	 0.04706 	 ~...
   79 	    56 	 0.05215 	 0.04718 	 ~...
   62 	    57 	 0.05170 	 0.04727 	 ~...
   82 	    58 	 0.05218 	 0.04740 	 ~...
   17 	    59 	 0.05079 	 0.04744 	 ~...
   71 	    60 	 0.05191 	 0.04750 	 ~...
   35 	    61 	 0.05113 	 0.04758 	 ~...
   29 	    62 	 0.05103 	 0.04763 	 ~...
   32 	    63 	 0.05106 	 0.04824 	 ~...
   21 	    64 	 0.05088 	 0.04857 	 ~...
    4 	    65 	 0.05044 	 0.04879 	 ~...
   57 	    66 	 0.05152 	 0.04879 	 ~...
   85 	    67 	 0.05223 	 0.04904 	 ~...
   44 	    68 	 0.05129 	 0.04921 	 ~...
   59 	    69 	 0.05157 	 0.04928 	 ~...
   34 	    70 	 0.05111 	 0.04929 	 ~...
    3 	    71 	 0.05040 	 0.04949 	 ~...
   43 	    72 	 0.05129 	 0.04956 	 ~...
   38 	    73 	 0.05118 	 0.04987 	 ~...
   19 	    74 	 0.05081 	 0.05004 	 ~...
   33 	    75 	 0.05111 	 0.05023 	 ~...
    9 	    76 	 0.05052 	 0.05056 	 ~...
   80 	    77 	 0.05218 	 0.05108 	 ~...
   83 	    78 	 0.05219 	 0.05128 	 ~...
   41 	    79 	 0.05128 	 0.05129 	 ~...
   30 	    80 	 0.05105 	 0.05144 	 ~...
   66 	    81 	 0.05177 	 0.05146 	 ~...
   20 	    82 	 0.05085 	 0.05160 	 ~...
   12 	    83 	 0.05060 	 0.05189 	 ~...
   64 	    84 	 0.05174 	 0.05246 	 ~...
   27 	    85 	 0.05098 	 0.05275 	 ~...
   23 	    86 	 0.05090 	 0.05352 	 ~...
   37 	    87 	 0.05115 	 0.05355 	 ~...
   84 	    88 	 0.05219 	 0.05567 	 ~...
   87 	    89 	 0.05235 	 0.05977 	 ~...
   93 	    90 	 0.20464 	 0.18889 	 ~...
   92 	    91 	 0.20321 	 0.18959 	 ~...
   90 	    92 	 0.19819 	 0.19213 	 ~...
   91 	    93 	 0.20213 	 0.19872 	 ~...
   96 	    94 	 0.22638 	 0.20550 	 ~...
   95 	    95 	 0.21021 	 0.20694 	 ~...
  102 	    96 	 0.23587 	 0.20727 	 ~...
   98 	    97 	 0.22892 	 0.21693 	 ~...
  103 	    98 	 0.23868 	 0.21723 	 ~...
  105 	    99 	 0.24226 	 0.21940 	 ~...
   94 	   100 	 0.21019 	 0.21956 	 ~...
   97 	   101 	 0.22655 	 0.23290 	 ~...
  101 	   102 	 0.23573 	 0.23984 	 ~...
  106 	   103 	 0.24353 	 0.24107 	 ~...
  109 	   104 	 0.26149 	 0.24218 	 ~...
   99 	   105 	 0.23054 	 0.24307 	 ~...
  107 	   106 	 0.24586 	 0.25298 	 ~...
  100 	   107 	 0.23348 	 0.25327 	 ~...
  111 	   108 	 0.28343 	 0.27378 	 ~...
  104 	   109 	 0.24022 	 0.27425 	 m..s
  116 	   110 	 0.32196 	 0.27441 	 m..s
  113 	   111 	 0.29936 	 0.27737 	 ~...
  117 	   112 	 0.32281 	 0.28034 	 m..s
  108 	   113 	 0.24988 	 0.28353 	 m..s
  114 	   114 	 0.30831 	 0.29788 	 ~...
  112 	   115 	 0.29500 	 0.29922 	 ~...
  115 	   116 	 0.31180 	 0.31748 	 ~...
  110 	   117 	 0.27787 	 0.33485 	 m..s
  120 	   118 	 0.46828 	 0.39525 	 m..s
  118 	   119 	 0.41845 	 0.52223 	 MISS
  119 	   120 	 0.46606 	 0.53924 	 m..s
==========================================
r_mrr = 0.9845762848854065
r2_mrr = 0.9631768465042114
spearmanr_mrr@5 = 0.9559406638145447
spearmanr_mrr@10 = 0.9777991771697998
spearmanr_mrr@50 = 0.9927105903625488
spearmanr_mrr@100 = 0.9954217076301575
spearmanr_mrr@All = 0.9939088821411133
==========================================
test time: 0.456
Done Testing dataset UMLS
total time taken: 269.9703757762909
training time taken: 264.16821670532227
TWIG out ;))
