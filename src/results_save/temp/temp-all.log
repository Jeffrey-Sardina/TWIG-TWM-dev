Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_UMLS-Kinships-CoDExSmall-DBpedia50-OpenEA
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.427 +- 0.005
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 8.00222e-05

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 27
rank avg (pred): 0.196 +- 0.027
mrr vals (pred, true): 0.037, 0.244
batch losses (mrrl, rdl): 0.0, 4.89255e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 135
rank avg (pred): 0.465 +- 0.017
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 8.19922e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 163
rank avg (pred): 0.446 +- 0.067
mrr vals (pred, true): 0.017, 0.048
batch losses (mrrl, rdl): 0.0, 7.07636e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 192
rank avg (pred): 0.454 +- 0.262
mrr vals (pred, true): 0.097, 0.047
batch losses (mrrl, rdl): 0.0, 2.64513e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.377 +- 0.230
mrr vals (pred, true): 0.117, 0.026
batch losses (mrrl, rdl): 0.0, 0.0001985572

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 626
rank avg (pred): 0.404 +- 0.264
mrr vals (pred, true): 0.125, 0.046
batch losses (mrrl, rdl): 0.0, 1.19654e-05

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1197
rank avg (pred): 0.434 +- 0.246
mrr vals (pred, true): 0.079, 0.045
batch losses (mrrl, rdl): 0.0, 6.178e-06

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 992
rank avg (pred): 0.193 +- 0.195
mrr vals (pred, true): 0.105, 0.285
batch losses (mrrl, rdl): 0.0, 5.40593e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1199
rank avg (pred): 0.457 +- 0.260
mrr vals (pred, true): 0.093, 0.046
batch losses (mrrl, rdl): 0.0, 1.6697e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.413 +- 0.262
mrr vals (pred, true): 0.100, 0.050
batch losses (mrrl, rdl): 0.0, 9.9684e-06

Epoch over!
epoch time: 95.7

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.431 +- 0.264
mrr vals (pred, true): 0.094, 0.050
batch losses (mrrl, rdl): 0.0, 1.3103e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 901
rank avg (pred): 0.590 +- 0.215
mrr vals (pred, true): 0.059, 0.020
batch losses (mrrl, rdl): 0.0, 9.5e-06

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 417
rank avg (pred): 0.416 +- 0.242
mrr vals (pred, true): 0.098, 0.064
batch losses (mrrl, rdl): 0.0, 6.8997e-06

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 992
rank avg (pred): 0.129 +- 0.134
mrr vals (pred, true): 0.108, 0.285
batch losses (mrrl, rdl): 0.0, 9.4118e-06

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.168 +- 0.136
mrr vals (pred, true): 0.089, 0.258
batch losses (mrrl, rdl): 0.0, 3.31176e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.451 +- 0.251
mrr vals (pred, true): 0.094, 0.043
batch losses (mrrl, rdl): 0.0, 2.17771e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 476
rank avg (pred): 0.409 +- 0.262
mrr vals (pred, true): 0.109, 0.044
batch losses (mrrl, rdl): 0.0, 7.879e-06

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 121
rank avg (pred): 0.425 +- 0.271
mrr vals (pred, true): 0.098, 0.041
batch losses (mrrl, rdl): 0.0, 1.43598e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 884
rank avg (pred): 0.432 +- 0.243
mrr vals (pred, true): 0.096, 0.045
batch losses (mrrl, rdl): 0.0, 2.9698e-06

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 512
rank avg (pred): 0.492 +- 0.240
mrr vals (pred, true): 0.072, 0.026
batch losses (mrrl, rdl): 0.0, 1.58618e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.444 +- 0.266
mrr vals (pred, true): 0.088, 0.053
batch losses (mrrl, rdl): 0.0, 3.6098e-06

Epoch over!
epoch time: 95.771

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1113
rank avg (pred): 0.436 +- 0.260
mrr vals (pred, true): 0.081, 0.054
batch losses (mrrl, rdl): 0.0098804496, 1.6466e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 234
rank avg (pred): 0.417 +- 0.222
mrr vals (pred, true): 0.057, 0.044
batch losses (mrrl, rdl): 0.000486213, 2.77974e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 824
rank avg (pred): 0.141 +- 0.292
mrr vals (pred, true): 0.292, 0.302
batch losses (mrrl, rdl): 0.0010643983, 4.50164e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 477
rank avg (pred): 0.615 +- 0.263
mrr vals (pred, true): 0.055, 0.048
batch losses (mrrl, rdl): 0.0002895327, 0.000505698

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 573
rank avg (pred): 0.479 +- 0.202
mrr vals (pred, true): 0.055, 0.043
batch losses (mrrl, rdl): 0.0002531044, 1.78861e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 145
rank avg (pred): 0.558 +- 0.187
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 9.60675e-05, 0.0002507404

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 236
rank avg (pred): 0.564 +- 0.189
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 2.42674e-05, 0.000328206

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 923
rank avg (pred): 0.381 +- 0.133
mrr vals (pred, true): 0.055, 0.037
batch losses (mrrl, rdl): 0.0002471948, 0.0001858401

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1056
rank avg (pred): 0.049 +- 0.064
mrr vals (pred, true): 0.283, 0.290
batch losses (mrrl, rdl): 0.0004579034, 0.0002476727

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 421
rank avg (pred): 0.432 +- 0.156
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 9.5239e-06, 3.20368e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 914
rank avg (pred): 0.518 +- 0.278
mrr vals (pred, true): 0.053, 0.021
batch losses (mrrl, rdl): 9.68104e-05, 0.0001140128

Epoch over!
epoch time: 84.233

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 157
rank avg (pred): 0.538 +- 0.195
mrr vals (pred, true): 0.048, 0.041
batch losses (mrrl, rdl): 2.81141e-05, 0.0001417356

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 152
rank avg (pred): 0.611 +- 0.214
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 3.476e-06, 0.0006192088

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 799
rank avg (pred): 0.392 +- 0.145
mrr vals (pred, true): 0.065, 0.041
batch losses (mrrl, rdl): 0.0022140923, 0.0001318492

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.663 +- 0.236
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0001519637, 0.0009078973

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 81
rank avg (pred): 0.425 +- 0.166
mrr vals (pred, true): 0.058, 0.048
batch losses (mrrl, rdl): 0.000712905, 2.57889e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1071
rank avg (pred): 0.096 +- 0.117
mrr vals (pred, true): 0.217, 0.282
batch losses (mrrl, rdl): 0.0422975272, 4.34543e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.364 +- 0.207
mrr vals (pred, true): 0.059, 0.053
batch losses (mrrl, rdl): 0.0008293904, 0.0001706407

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 60
rank avg (pred): 0.138 +- 0.162
mrr vals (pred, true): 0.191, 0.219
batch losses (mrrl, rdl): 0.0076526259, 1.02979e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 901
rank avg (pred): 0.576 +- 0.295
mrr vals (pred, true): 0.062, 0.020
batch losses (mrrl, rdl): 0.0014916882, 4.44586e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 995
rank avg (pred): 0.111 +- 0.167
mrr vals (pred, true): 0.273, 0.289
batch losses (mrrl, rdl): 0.0023131245, 5.93218e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 405
rank avg (pred): 0.515 +- 0.209
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 8.0964e-06, 7.88726e-05

Epoch over!
epoch time: 80.569

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 110
rank avg (pred): 0.539 +- 0.232
mrr vals (pred, true): 0.054, 0.050
batch losses (mrrl, rdl): 0.0001819425, 0.0002359145

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 725
rank avg (pred): 0.457 +- 0.187
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 0.0001224017, 2.17058e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 572
rank avg (pred): 0.524 +- 0.205
mrr vals (pred, true): 0.041, 0.041
batch losses (mrrl, rdl): 0.0008790743, 0.0001394913

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 985
rank avg (pred): 0.107 +- 0.174
mrr vals (pred, true): 0.251, 0.285
batch losses (mrrl, rdl): 0.0111308275, 0.0001006482

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 727
rank avg (pred): 0.514 +- 0.233
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 5.1971e-06, 0.000108319

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 675
rank avg (pred): 0.490 +- 0.226
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 4.18795e-05, 2.79628e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.497 +- 0.229
mrr vals (pred, true): 0.052, 0.039
batch losses (mrrl, rdl): 3.85606e-05, 3.6411e-05

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 514
rank avg (pred): 0.555 +- 0.271
mrr vals (pred, true): 0.065, 0.024
batch losses (mrrl, rdl): 0.0023139529, 2.40733e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 166
rank avg (pred): 0.419 +- 0.208
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 5.143e-07, 2.67531e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 416
rank avg (pred): 0.421 +- 0.220
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 2.72907e-05, 3.45804e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.252 +- 0.327
mrr vals (pred, true): 0.185, 0.252
batch losses (mrrl, rdl): 0.0452638939, 0.0001754458

Epoch over!
epoch time: 73.22

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.583 +- 0.288
mrr vals (pred, true): 0.053, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   61 	     0 	 0.05342 	 0.01773 	 m..s
   58 	     1 	 0.05290 	 0.02009 	 m..s
   75 	     2 	 0.06213 	 0.02164 	 m..s
   75 	     3 	 0.06213 	 0.02192 	 m..s
   75 	     4 	 0.06213 	 0.02235 	 m..s
   75 	     5 	 0.06213 	 0.02398 	 m..s
   75 	     6 	 0.06213 	 0.02414 	 m..s
   75 	     7 	 0.06213 	 0.02414 	 m..s
   75 	     8 	 0.06213 	 0.02423 	 m..s
   75 	     9 	 0.06213 	 0.02467 	 m..s
   86 	    10 	 0.06445 	 0.02475 	 m..s
   75 	    11 	 0.06213 	 0.02496 	 m..s
   75 	    12 	 0.06213 	 0.02544 	 m..s
   75 	    13 	 0.06213 	 0.02586 	 m..s
   60 	    14 	 0.05324 	 0.02933 	 ~...
   18 	    15 	 0.04830 	 0.03159 	 ~...
   64 	    16 	 0.05420 	 0.03197 	 ~...
   67 	    17 	 0.05439 	 0.03458 	 ~...
   23 	    18 	 0.04857 	 0.03549 	 ~...
   62 	    19 	 0.05395 	 0.03582 	 ~...
   21 	    20 	 0.04845 	 0.03732 	 ~...
   13 	    21 	 0.04817 	 0.03811 	 ~...
    9 	    22 	 0.04801 	 0.03856 	 ~...
   30 	    23 	 0.04952 	 0.03931 	 ~...
   42 	    24 	 0.05064 	 0.03957 	 ~...
   74 	    25 	 0.06146 	 0.03980 	 ~...
   44 	    26 	 0.05072 	 0.04032 	 ~...
   45 	    27 	 0.05074 	 0.04070 	 ~...
   56 	    28 	 0.05256 	 0.04080 	 ~...
   59 	    29 	 0.05302 	 0.04099 	 ~...
   41 	    30 	 0.05058 	 0.04129 	 ~...
   72 	    31 	 0.05839 	 0.04130 	 ~...
   37 	    32 	 0.05036 	 0.04163 	 ~...
   32 	    33 	 0.05003 	 0.04165 	 ~...
   88 	    34 	 0.06503 	 0.04176 	 ~...
   54 	    35 	 0.05183 	 0.04224 	 ~...
   27 	    36 	 0.04916 	 0.04272 	 ~...
   16 	    37 	 0.04822 	 0.04276 	 ~...
   10 	    38 	 0.04804 	 0.04306 	 ~...
   39 	    39 	 0.05054 	 0.04338 	 ~...
   69 	    40 	 0.05484 	 0.04372 	 ~...
   31 	    41 	 0.04971 	 0.04373 	 ~...
   55 	    42 	 0.05219 	 0.04388 	 ~...
   63 	    43 	 0.05401 	 0.04410 	 ~...
   53 	    44 	 0.05176 	 0.04419 	 ~...
    4 	    45 	 0.04784 	 0.04420 	 ~...
    0 	    46 	 0.04766 	 0.04423 	 ~...
   29 	    47 	 0.04926 	 0.04476 	 ~...
   11 	    48 	 0.04804 	 0.04514 	 ~...
   68 	    49 	 0.05464 	 0.04551 	 ~...
   15 	    50 	 0.04819 	 0.04564 	 ~...
   49 	    51 	 0.05127 	 0.04617 	 ~...
   14 	    52 	 0.04817 	 0.04642 	 ~...
    5 	    53 	 0.04784 	 0.04687 	 ~...
   66 	    54 	 0.05433 	 0.04699 	 ~...
   22 	    55 	 0.04845 	 0.04706 	 ~...
   46 	    56 	 0.05090 	 0.04718 	 ~...
   48 	    57 	 0.05106 	 0.04727 	 ~...
   33 	    58 	 0.05007 	 0.04740 	 ~...
    2 	    59 	 0.04782 	 0.04744 	 ~...
   34 	    60 	 0.05011 	 0.04750 	 ~...
    6 	    61 	 0.04784 	 0.04758 	 ~...
   24 	    62 	 0.04858 	 0.04763 	 ~...
   47 	    63 	 0.05098 	 0.04824 	 ~...
    8 	    64 	 0.04790 	 0.04857 	 ~...
   28 	    65 	 0.04926 	 0.04879 	 ~...
   19 	    66 	 0.04834 	 0.04879 	 ~...
   70 	    67 	 0.05486 	 0.04904 	 ~...
   12 	    68 	 0.04816 	 0.04921 	 ~...
   20 	    69 	 0.04837 	 0.04928 	 ~...
   71 	    70 	 0.05639 	 0.04929 	 ~...
   65 	    71 	 0.05424 	 0.04949 	 ~...
   89 	    72 	 0.06521 	 0.04956 	 ~...
   17 	    73 	 0.04830 	 0.04987 	 ~...
   51 	    74 	 0.05159 	 0.05004 	 ~...
    3 	    75 	 0.04782 	 0.05023 	 ~...
   40 	    76 	 0.05056 	 0.05056 	 ~...
   35 	    77 	 0.05030 	 0.05108 	 ~...
   36 	    78 	 0.05032 	 0.05128 	 ~...
   87 	    79 	 0.06469 	 0.05129 	 ~...
   57 	    80 	 0.05284 	 0.05144 	 ~...
   73 	    81 	 0.06141 	 0.05146 	 ~...
    7 	    82 	 0.04786 	 0.05160 	 ~...
   52 	    83 	 0.05175 	 0.05189 	 ~...
   38 	    84 	 0.05049 	 0.05246 	 ~...
   25 	    85 	 0.04901 	 0.05275 	 ~...
    1 	    86 	 0.04776 	 0.05352 	 ~...
   26 	    87 	 0.04903 	 0.05355 	 ~...
   50 	    88 	 0.05146 	 0.05567 	 ~...
   43 	    89 	 0.05070 	 0.05977 	 ~...
  103 	    90 	 0.23250 	 0.18889 	 m..s
  100 	    91 	 0.21016 	 0.18959 	 ~...
   99 	    92 	 0.20674 	 0.19213 	 ~...
  102 	    93 	 0.23181 	 0.19872 	 m..s
   98 	    94 	 0.20633 	 0.20550 	 ~...
   95 	    95 	 0.20570 	 0.20694 	 ~...
   90 	    96 	 0.18958 	 0.20727 	 ~...
   96 	    97 	 0.20583 	 0.21693 	 ~...
   91 	    98 	 0.19590 	 0.21723 	 ~...
   93 	    99 	 0.20392 	 0.21940 	 ~...
   97 	   100 	 0.20616 	 0.21956 	 ~...
  106 	   101 	 0.23895 	 0.23290 	 ~...
   94 	   102 	 0.20491 	 0.23984 	 m..s
  101 	   103 	 0.23096 	 0.24107 	 ~...
  105 	   104 	 0.23589 	 0.24218 	 ~...
  108 	   105 	 0.23965 	 0.24307 	 ~...
   92 	   106 	 0.20132 	 0.25298 	 m..s
  104 	   107 	 0.23478 	 0.25327 	 ~...
  111 	   108 	 0.25672 	 0.27378 	 ~...
  107 	   109 	 0.23958 	 0.27425 	 m..s
  113 	   110 	 0.27858 	 0.27441 	 ~...
  115 	   111 	 0.28655 	 0.27737 	 ~...
  114 	   112 	 0.27991 	 0.28034 	 ~...
  109 	   113 	 0.24154 	 0.28353 	 m..s
  112 	   114 	 0.26845 	 0.29788 	 ~...
  116 	   115 	 0.28841 	 0.29922 	 ~...
  117 	   116 	 0.28966 	 0.31748 	 ~...
  110 	   117 	 0.25107 	 0.33485 	 m..s
  120 	   118 	 0.49712 	 0.39525 	 MISS
  118 	   119 	 0.43553 	 0.52223 	 m..s
  119 	   120 	 0.45869 	 0.53924 	 m..s
==========================================
r_mrr = 0.9785739779472351
r2_mrr = 0.9486009478569031
spearmanr_mrr@5 = 0.9226877093315125
spearmanr_mrr@10 = 0.9599168300628662
spearmanr_mrr@50 = 0.9938821196556091
spearmanr_mrr@100 = 0.9962009787559509
spearmanr_mrr@All = 0.9953951835632324
==========================================
test time: 0.541
Done Testing dataset UMLS
Testing model with dataset Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.311 +- 0.292
mrr vals (pred, true): 0.277, 0.228

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   68 	     0 	 0.03681 	 0.04713 	 ~...
   60 	     1 	 0.03637 	 0.04727 	 ~...
   57 	     2 	 0.03624 	 0.04763 	 ~...
   34 	     3 	 0.03441 	 0.04803 	 ~...
   71 	     4 	 0.03745 	 0.04830 	 ~...
   24 	     5 	 0.03341 	 0.04840 	 ~...
   45 	     6 	 0.03595 	 0.04863 	 ~...
   21 	     7 	 0.03330 	 0.04890 	 ~...
    0 	     8 	 0.02971 	 0.04927 	 ~...
   53 	     9 	 0.03618 	 0.04930 	 ~...
   28 	    10 	 0.03372 	 0.04948 	 ~...
   20 	    11 	 0.03310 	 0.04950 	 ~...
   54 	    12 	 0.03620 	 0.04954 	 ~...
   56 	    13 	 0.03623 	 0.04990 	 ~...
   43 	    14 	 0.03531 	 0.05021 	 ~...
   72 	    15 	 0.03751 	 0.05035 	 ~...
    1 	    16 	 0.03189 	 0.05050 	 ~...
   70 	    17 	 0.03698 	 0.05071 	 ~...
    9 	    18 	 0.03248 	 0.05076 	 ~...
   58 	    19 	 0.03624 	 0.05097 	 ~...
   64 	    20 	 0.03657 	 0.05105 	 ~...
   66 	    21 	 0.03664 	 0.05107 	 ~...
   63 	    22 	 0.03655 	 0.05112 	 ~...
   32 	    23 	 0.03439 	 0.05113 	 ~...
   61 	    24 	 0.03642 	 0.05127 	 ~...
   73 	    25 	 0.03780 	 0.05151 	 ~...
   31 	    26 	 0.03435 	 0.05154 	 ~...
    5 	    27 	 0.03215 	 0.05179 	 ~...
   15 	    28 	 0.03304 	 0.05186 	 ~...
   13 	    29 	 0.03293 	 0.05193 	 ~...
    2 	    30 	 0.03203 	 0.05211 	 ~...
   33 	    31 	 0.03440 	 0.05215 	 ~...
   59 	    32 	 0.03636 	 0.05219 	 ~...
   69 	    33 	 0.03689 	 0.05236 	 ~...
   74 	    34 	 0.03786 	 0.05312 	 ~...
   35 	    35 	 0.03461 	 0.05317 	 ~...
   50 	    36 	 0.03608 	 0.05317 	 ~...
   25 	    37 	 0.03346 	 0.05341 	 ~...
   40 	    38 	 0.03490 	 0.05350 	 ~...
   65 	    39 	 0.03658 	 0.05351 	 ~...
   75 	    40 	 0.03787 	 0.05358 	 ~...
   27 	    41 	 0.03362 	 0.05406 	 ~...
   44 	    42 	 0.03593 	 0.05428 	 ~...
   38 	    43 	 0.03481 	 0.05440 	 ~...
   55 	    44 	 0.03622 	 0.05481 	 ~...
   10 	    45 	 0.03266 	 0.05498 	 ~...
   48 	    46 	 0.03607 	 0.05510 	 ~...
   26 	    47 	 0.03347 	 0.05531 	 ~...
   23 	    48 	 0.03338 	 0.05575 	 ~...
   46 	    49 	 0.03600 	 0.05576 	 ~...
   49 	    50 	 0.03608 	 0.05587 	 ~...
   18 	    51 	 0.03309 	 0.05596 	 ~...
   42 	    52 	 0.03529 	 0.05638 	 ~...
    4 	    53 	 0.03211 	 0.05694 	 ~...
    3 	    54 	 0.03204 	 0.05696 	 ~...
   30 	    55 	 0.03434 	 0.05719 	 ~...
   39 	    56 	 0.03483 	 0.05723 	 ~...
   16 	    57 	 0.03305 	 0.05725 	 ~...
   19 	    58 	 0.03310 	 0.05733 	 ~...
   29 	    59 	 0.03399 	 0.05749 	 ~...
   41 	    60 	 0.03520 	 0.05787 	 ~...
   14 	    61 	 0.03300 	 0.05825 	 ~...
   12 	    62 	 0.03287 	 0.05868 	 ~...
   47 	    63 	 0.03603 	 0.05870 	 ~...
   62 	    64 	 0.03651 	 0.05874 	 ~...
   11 	    65 	 0.03286 	 0.05881 	 ~...
    6 	    66 	 0.03215 	 0.05882 	 ~...
   67 	    67 	 0.03667 	 0.05918 	 ~...
    7 	    68 	 0.03220 	 0.05969 	 ~...
   51 	    69 	 0.03609 	 0.05993 	 ~...
   36 	    70 	 0.03463 	 0.06014 	 ~...
   37 	    71 	 0.03479 	 0.06038 	 ~...
   52 	    72 	 0.03616 	 0.06049 	 ~...
    8 	    73 	 0.03237 	 0.06096 	 ~...
   17 	    74 	 0.03308 	 0.06158 	 ~...
   22 	    75 	 0.03337 	 0.06229 	 ~...
   76 	    76 	 0.17556 	 0.16198 	 ~...
   76 	    77 	 0.17556 	 0.18404 	 ~...
   76 	    78 	 0.17556 	 0.18970 	 ~...
   76 	    79 	 0.17556 	 0.19886 	 ~...
   76 	    80 	 0.17556 	 0.21213 	 m..s
   76 	    81 	 0.17556 	 0.21714 	 m..s
   76 	    82 	 0.17556 	 0.21822 	 m..s
   76 	    83 	 0.17556 	 0.21861 	 m..s
   76 	    84 	 0.17556 	 0.22784 	 m..s
   99 	    85 	 0.27748 	 0.22825 	 m..s
  100 	    86 	 0.29001 	 0.25404 	 m..s
   76 	    87 	 0.17556 	 0.26205 	 m..s
   97 	    88 	 0.25065 	 0.26840 	 ~...
   98 	    89 	 0.25482 	 0.27182 	 ~...
   94 	    90 	 0.24969 	 0.28071 	 m..s
   87 	    91 	 0.21622 	 0.28148 	 m..s
   92 	    92 	 0.24879 	 0.28774 	 m..s
   95 	    93 	 0.25003 	 0.29619 	 m..s
   89 	    94 	 0.24239 	 0.29634 	 m..s
   76 	    95 	 0.17556 	 0.29652 	 MISS
   88 	    96 	 0.23940 	 0.30510 	 m..s
  104 	    97 	 0.29745 	 0.30598 	 ~...
   96 	    98 	 0.25021 	 0.30754 	 m..s
  101 	    99 	 0.29050 	 0.30803 	 ~...
   93 	   100 	 0.24956 	 0.30884 	 m..s
  103 	   101 	 0.29331 	 0.31529 	 ~...
  102 	   102 	 0.29205 	 0.31655 	 ~...
   91 	   103 	 0.24789 	 0.32286 	 m..s
  105 	   104 	 0.29946 	 0.33122 	 m..s
   90 	   105 	 0.24581 	 0.34296 	 m..s
  109 	   106 	 0.30967 	 0.36221 	 m..s
  108 	   107 	 0.30625 	 0.36880 	 m..s
  106 	   108 	 0.30497 	 0.36975 	 m..s
  107 	   109 	 0.30613 	 0.37989 	 m..s
  114 	   110 	 0.38268 	 0.38991 	 ~...
  117 	   111 	 0.39973 	 0.40258 	 ~...
  115 	   112 	 0.38685 	 0.40967 	 ~...
  113 	   113 	 0.38034 	 0.41075 	 m..s
  112 	   114 	 0.36176 	 0.42112 	 m..s
  110 	   115 	 0.32671 	 0.47933 	 MISS
  111 	   116 	 0.33664 	 0.48000 	 MISS
  116 	   117 	 0.38985 	 0.50285 	 MISS
  118 	   118 	 0.61450 	 0.61365 	 ~...
  120 	   119 	 0.69284 	 0.61786 	 m..s
  119 	   120 	 0.64434 	 0.62288 	 ~...
==========================================
r_mrr = 0.980937659740448
r2_mrr = 0.929338276386261
spearmanr_mrr@5 = 0.9846611022949219
spearmanr_mrr@10 = 0.9501392245292664
spearmanr_mrr@50 = 0.9770702123641968
spearmanr_mrr@100 = 0.9900869727134705
spearmanr_mrr@All = 0.9909529685974121
==========================================
test time: 0.591
Done Testing dataset Kinships
Testing model with dataset CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.464 +- 0.357
mrr vals (pred, true): 0.060, 0.001

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   71 	     0 	 0.05256 	 0.00086 	 m..s
   87 	     1 	 0.06043 	 0.00089 	 m..s
   16 	     2 	 0.04199 	 0.00170 	 m..s
   88 	     3 	 0.06161 	 0.00174 	 m..s
    2 	     4 	 0.02841 	 0.00204 	 ~...
    5 	     5 	 0.02945 	 0.00240 	 ~...
    3 	     6 	 0.02910 	 0.00242 	 ~...
    4 	     7 	 0.02918 	 0.00242 	 ~...
   59 	     8 	 0.05035 	 0.00287 	 m..s
   64 	     9 	 0.05212 	 0.00289 	 m..s
   17 	    10 	 0.04374 	 0.00291 	 m..s
   52 	    11 	 0.04962 	 0.00292 	 m..s
   30 	    12 	 0.04691 	 0.00296 	 m..s
   11 	    13 	 0.03582 	 0.00310 	 m..s
    6 	    14 	 0.03204 	 0.00315 	 ~...
   70 	    15 	 0.05253 	 0.00316 	 m..s
   29 	    16 	 0.04687 	 0.00317 	 m..s
   47 	    17 	 0.04914 	 0.00325 	 m..s
   60 	    18 	 0.05076 	 0.00331 	 m..s
   50 	    19 	 0.04937 	 0.00331 	 m..s
   35 	    20 	 0.04737 	 0.00336 	 m..s
   12 	    21 	 0.03734 	 0.00337 	 m..s
   67 	    22 	 0.05234 	 0.00345 	 m..s
   44 	    23 	 0.04878 	 0.00345 	 m..s
   69 	    24 	 0.05248 	 0.00356 	 m..s
   14 	    25 	 0.04046 	 0.00360 	 m..s
   41 	    26 	 0.04783 	 0.00362 	 m..s
   58 	    27 	 0.05013 	 0.00362 	 m..s
   86 	    28 	 0.05421 	 0.00363 	 m..s
   51 	    29 	 0.04943 	 0.00363 	 m..s
    9 	    30 	 0.03319 	 0.00364 	 ~...
   19 	    31 	 0.04429 	 0.00368 	 m..s
    7 	    32 	 0.03219 	 0.00368 	 ~...
   66 	    33 	 0.05223 	 0.00374 	 m..s
   24 	    34 	 0.04551 	 0.00376 	 m..s
   25 	    35 	 0.04557 	 0.00376 	 m..s
   10 	    36 	 0.03494 	 0.00381 	 m..s
    1 	    37 	 0.02822 	 0.00381 	 ~...
    8 	    38 	 0.03299 	 0.00384 	 ~...
   31 	    39 	 0.04694 	 0.00389 	 m..s
   18 	    40 	 0.04428 	 0.00391 	 m..s
   34 	    41 	 0.04730 	 0.00394 	 m..s
   37 	    42 	 0.04745 	 0.00399 	 m..s
   26 	    43 	 0.04568 	 0.00400 	 m..s
   83 	    44 	 0.05380 	 0.00402 	 m..s
   13 	    45 	 0.03878 	 0.00404 	 m..s
   43 	    46 	 0.04835 	 0.00406 	 m..s
   68 	    47 	 0.05246 	 0.00409 	 m..s
   42 	    48 	 0.04793 	 0.00412 	 m..s
   56 	    49 	 0.05007 	 0.00413 	 m..s
   36 	    50 	 0.04737 	 0.00415 	 m..s
   49 	    51 	 0.04931 	 0.00415 	 m..s
   22 	    52 	 0.04519 	 0.00415 	 m..s
   27 	    53 	 0.04613 	 0.00415 	 m..s
   53 	    54 	 0.04963 	 0.00416 	 m..s
   15 	    55 	 0.04121 	 0.00418 	 m..s
   62 	    56 	 0.05097 	 0.00418 	 m..s
   21 	    57 	 0.04516 	 0.00420 	 m..s
   57 	    58 	 0.05008 	 0.00422 	 m..s
   63 	    59 	 0.05110 	 0.00424 	 m..s
   45 	    60 	 0.04879 	 0.00440 	 m..s
   32 	    61 	 0.04725 	 0.00454 	 m..s
   65 	    62 	 0.05217 	 0.00455 	 m..s
    0 	    63 	 0.02797 	 0.00459 	 ~...
   82 	    64 	 0.05261 	 0.00460 	 m..s
   28 	    65 	 0.04679 	 0.00461 	 m..s
   23 	    66 	 0.04550 	 0.00461 	 m..s
   39 	    67 	 0.04756 	 0.00467 	 m..s
   85 	    68 	 0.05421 	 0.00469 	 m..s
   61 	    69 	 0.05095 	 0.00470 	 m..s
   40 	    70 	 0.04762 	 0.00473 	 m..s
   48 	    71 	 0.04923 	 0.00477 	 m..s
   20 	    72 	 0.04435 	 0.00478 	 m..s
   84 	    73 	 0.05412 	 0.00485 	 m..s
   54 	    74 	 0.04966 	 0.00500 	 m..s
   33 	    75 	 0.04729 	 0.00501 	 m..s
   38 	    76 	 0.04749 	 0.00514 	 m..s
   55 	    77 	 0.04970 	 0.00551 	 m..s
   46 	    78 	 0.04890 	 0.00604 	 m..s
   71 	    79 	 0.05256 	 0.00680 	 m..s
   71 	    80 	 0.05256 	 0.00694 	 m..s
   71 	    81 	 0.05256 	 0.00739 	 m..s
   71 	    82 	 0.05256 	 0.00804 	 m..s
   71 	    83 	 0.05256 	 0.01152 	 m..s
   71 	    84 	 0.05256 	 0.01271 	 m..s
   71 	    85 	 0.05256 	 0.02050 	 m..s
   89 	    86 	 0.06295 	 0.02430 	 m..s
   71 	    87 	 0.05256 	 0.02863 	 ~...
   71 	    88 	 0.05256 	 0.02916 	 ~...
   71 	    89 	 0.05256 	 0.02920 	 ~...
   95 	    90 	 0.18507 	 0.12715 	 m..s
   91 	    91 	 0.16571 	 0.12799 	 m..s
   97 	    92 	 0.18620 	 0.13094 	 m..s
   90 	    93 	 0.15676 	 0.13140 	 ~...
   93 	    94 	 0.18092 	 0.13806 	 m..s
   92 	    95 	 0.17543 	 0.14041 	 m..s
   99 	    96 	 0.18766 	 0.17202 	 ~...
  100 	    97 	 0.19659 	 0.17271 	 ~...
   94 	    98 	 0.18319 	 0.17434 	 ~...
   96 	    99 	 0.18539 	 0.17792 	 ~...
  117 	   100 	 0.25135 	 0.18551 	 m..s
   98 	   101 	 0.18662 	 0.18924 	 ~...
  113 	   102 	 0.24619 	 0.19630 	 m..s
  112 	   103 	 0.24123 	 0.20171 	 m..s
  114 	   104 	 0.24682 	 0.21048 	 m..s
  101 	   105 	 0.22128 	 0.22857 	 ~...
  105 	   106 	 0.22376 	 0.22949 	 ~...
  102 	   107 	 0.22171 	 0.23928 	 ~...
  115 	   108 	 0.24740 	 0.24645 	 ~...
  103 	   109 	 0.22207 	 0.25269 	 m..s
  118 	   110 	 0.31128 	 0.25972 	 m..s
  104 	   111 	 0.22321 	 0.26879 	 m..s
  120 	   112 	 0.33664 	 0.27244 	 m..s
  111 	   113 	 0.23378 	 0.28454 	 m..s
  119 	   114 	 0.32057 	 0.29618 	 ~...
  107 	   115 	 0.22557 	 0.30562 	 m..s
  110 	   116 	 0.23111 	 0.31181 	 m..s
  116 	   117 	 0.24822 	 0.31362 	 m..s
  109 	   118 	 0.22652 	 0.31910 	 m..s
  108 	   119 	 0.22560 	 0.33494 	 MISS
  106 	   120 	 0.22525 	 0.34039 	 MISS
==========================================
r_mrr = 0.960224986076355
r2_mrr = 0.8028374910354614
spearmanr_mrr@5 = 0.897918164730072
spearmanr_mrr@10 = 0.8404134511947632
spearmanr_mrr@50 = 0.9816118478775024
spearmanr_mrr@100 = 0.9892449975013733
spearmanr_mrr@All = 0.9880390763282776
==========================================
test time: 0.62
Done Testing dataset CoDExSmall
Testing model with dataset DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.519 +- 0.379
mrr vals (pred, true): 0.065, 0.002

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   31 	     0 	 0.04938 	 0.00014 	 m..s
   38 	     1 	 0.05144 	 0.00014 	 m..s
   71 	     2 	 0.05395 	 0.00014 	 m..s
   79 	     3 	 0.06813 	 0.00015 	 m..s
   61 	     4 	 0.05324 	 0.00016 	 m..s
   51 	     5 	 0.05278 	 0.00016 	 m..s
   43 	     6 	 0.05213 	 0.00017 	 m..s
   28 	     7 	 0.04879 	 0.00017 	 m..s
   10 	     8 	 0.04194 	 0.00018 	 m..s
   17 	     9 	 0.04605 	 0.00018 	 m..s
    7 	    10 	 0.03963 	 0.00018 	 m..s
   62 	    11 	 0.05325 	 0.00019 	 m..s
   35 	    12 	 0.04961 	 0.00019 	 m..s
   63 	    13 	 0.05334 	 0.00021 	 m..s
   29 	    14 	 0.04916 	 0.00021 	 m..s
   40 	    15 	 0.05186 	 0.00021 	 m..s
   54 	    16 	 0.05291 	 0.00021 	 m..s
   44 	    17 	 0.05215 	 0.00021 	 m..s
   52 	    18 	 0.05280 	 0.00021 	 m..s
    3 	    19 	 0.03808 	 0.00022 	 m..s
   57 	    20 	 0.05293 	 0.00022 	 m..s
   36 	    21 	 0.04963 	 0.00023 	 m..s
   74 	    22 	 0.05447 	 0.00023 	 m..s
   58 	    23 	 0.05295 	 0.00024 	 m..s
   33 	    24 	 0.04956 	 0.00024 	 m..s
   49 	    25 	 0.05270 	 0.00024 	 m..s
   26 	    26 	 0.04876 	 0.00024 	 m..s
   18 	    27 	 0.04623 	 0.00024 	 m..s
    6 	    28 	 0.03949 	 0.00024 	 m..s
   19 	    29 	 0.04730 	 0.00025 	 m..s
   66 	    30 	 0.05383 	 0.00025 	 m..s
    8 	    31 	 0.03996 	 0.00025 	 m..s
   65 	    32 	 0.05367 	 0.00026 	 m..s
   22 	    33 	 0.04746 	 0.00026 	 m..s
   59 	    34 	 0.05308 	 0.00027 	 m..s
   46 	    35 	 0.05245 	 0.00027 	 m..s
    0 	    36 	 0.03634 	 0.00028 	 m..s
   64 	    37 	 0.05357 	 0.00028 	 m..s
   11 	    38 	 0.04379 	 0.00028 	 m..s
   68 	    39 	 0.05391 	 0.00029 	 m..s
   12 	    40 	 0.04392 	 0.00029 	 m..s
   48 	    41 	 0.05267 	 0.00029 	 m..s
   32 	    42 	 0.04940 	 0.00030 	 m..s
   39 	    43 	 0.05183 	 0.00030 	 m..s
   30 	    44 	 0.04928 	 0.00030 	 m..s
   27 	    45 	 0.04877 	 0.00031 	 m..s
   55 	    46 	 0.05291 	 0.00031 	 m..s
   53 	    47 	 0.05282 	 0.00032 	 m..s
   75 	    48 	 0.05447 	 0.00032 	 m..s
   14 	    49 	 0.04469 	 0.00033 	 m..s
   72 	    50 	 0.05435 	 0.00034 	 m..s
   21 	    51 	 0.04736 	 0.00034 	 m..s
    2 	    52 	 0.03793 	 0.00034 	 m..s
   41 	    53 	 0.05188 	 0.00035 	 m..s
    9 	    54 	 0.04149 	 0.00036 	 m..s
   70 	    55 	 0.05393 	 0.00037 	 m..s
   73 	    56 	 0.05445 	 0.00037 	 m..s
    1 	    57 	 0.03745 	 0.00038 	 m..s
   45 	    58 	 0.05245 	 0.00038 	 m..s
   24 	    59 	 0.04851 	 0.00039 	 m..s
   23 	    60 	 0.04804 	 0.00039 	 m..s
   13 	    61 	 0.04443 	 0.00039 	 m..s
   25 	    62 	 0.04866 	 0.00039 	 m..s
   56 	    63 	 0.05293 	 0.00043 	 m..s
    4 	    64 	 0.03818 	 0.00043 	 m..s
   42 	    65 	 0.05211 	 0.00046 	 m..s
   50 	    66 	 0.05278 	 0.00049 	 m..s
   16 	    67 	 0.04599 	 0.00050 	 m..s
   60 	    68 	 0.05323 	 0.00052 	 m..s
   69 	    69 	 0.05392 	 0.00053 	 m..s
   67 	    70 	 0.05385 	 0.00055 	 m..s
   37 	    71 	 0.05077 	 0.00056 	 m..s
   15 	    72 	 0.04481 	 0.00057 	 m..s
    5 	    73 	 0.03826 	 0.00076 	 m..s
   47 	    74 	 0.05262 	 0.00079 	 m..s
   34 	    75 	 0.04958 	 0.00084 	 m..s
   20 	    76 	 0.04735 	 0.00129 	 m..s
   77 	    77 	 0.06528 	 0.00171 	 m..s
   76 	    78 	 0.06525 	 0.00789 	 m..s
   95 	    79 	 0.09381 	 0.04857 	 m..s
   79 	    80 	 0.06813 	 0.06447 	 ~...
   93 	    81 	 0.09057 	 0.07961 	 ~...
   91 	    82 	 0.08170 	 0.08371 	 ~...
   79 	    83 	 0.06813 	 0.08488 	 ~...
   90 	    84 	 0.07808 	 0.08803 	 ~...
   99 	    85 	 0.09605 	 0.08860 	 ~...
   79 	    86 	 0.06813 	 0.08867 	 ~...
   97 	    87 	 0.09477 	 0.09060 	 ~...
   94 	    88 	 0.09228 	 0.09126 	 ~...
   92 	    89 	 0.08688 	 0.09149 	 ~...
   98 	    90 	 0.09514 	 0.09180 	 ~...
   79 	    91 	 0.06813 	 0.09286 	 ~...
   79 	    92 	 0.06813 	 0.09363 	 ~...
  100 	    93 	 0.10554 	 0.09460 	 ~...
   79 	    94 	 0.06813 	 0.09532 	 ~...
  101 	    95 	 0.18463 	 0.09535 	 m..s
  104 	    96 	 0.18996 	 0.09764 	 m..s
   79 	    97 	 0.06813 	 0.10902 	 m..s
  116 	    98 	 0.20597 	 0.12319 	 m..s
   79 	    99 	 0.06813 	 0.13164 	 m..s
   78 	   100 	 0.06655 	 0.13674 	 m..s
  112 	   101 	 0.19956 	 0.13680 	 m..s
   96 	   102 	 0.09408 	 0.13810 	 m..s
   79 	   103 	 0.06813 	 0.14653 	 m..s
   79 	   104 	 0.06813 	 0.15318 	 m..s
  107 	   105 	 0.19401 	 0.16378 	 m..s
  110 	   106 	 0.19828 	 0.16586 	 m..s
  115 	   107 	 0.20274 	 0.18204 	 ~...
  117 	   108 	 0.20637 	 0.18503 	 ~...
  113 	   109 	 0.19974 	 0.21574 	 ~...
  114 	   110 	 0.20029 	 0.21666 	 ~...
  111 	   111 	 0.19832 	 0.22657 	 ~...
  105 	   112 	 0.19111 	 0.26914 	 m..s
  109 	   113 	 0.19513 	 0.28706 	 m..s
  102 	   114 	 0.18602 	 0.30486 	 MISS
  103 	   115 	 0.18706 	 0.31033 	 MISS
  108 	   116 	 0.19405 	 0.31527 	 MISS
  119 	   117 	 0.24138 	 0.31960 	 m..s
  106 	   118 	 0.19359 	 0.33534 	 MISS
  120 	   119 	 0.24854 	 0.34767 	 m..s
  118 	   120 	 0.23703 	 0.38203 	 MISS
==========================================
r_mrr = 0.898872971534729
r2_mrr = 0.655248761177063
spearmanr_mrr@5 = 0.8808207511901855
spearmanr_mrr@10 = 0.8170951008796692
spearmanr_mrr@50 = 0.9045436978340149
spearmanr_mrr@100 = 0.9426950812339783
spearmanr_mrr@All = 0.9466044306755066
==========================================
test time: 0.558
Done Testing dataset DBpedia50
Testing model with dataset OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.600 +- 0.339
mrr vals (pred, true): 0.060, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   82 	     0 	 0.07941 	 0.00031 	 m..s
   63 	     1 	 0.05516 	 0.00045 	 m..s
   24 	     2 	 0.05309 	 0.00047 	 m..s
    1 	     3 	 0.03012 	 0.00051 	 ~...
   73 	     4 	 0.05620 	 0.00052 	 m..s
    7 	     5 	 0.04707 	 0.00053 	 m..s
   34 	     6 	 0.05436 	 0.00053 	 m..s
   11 	     7 	 0.05245 	 0.00055 	 m..s
   65 	     8 	 0.05522 	 0.00055 	 m..s
   59 	     9 	 0.05503 	 0.00056 	 m..s
   45 	    10 	 0.05470 	 0.00056 	 m..s
   26 	    11 	 0.05313 	 0.00057 	 m..s
   23 	    12 	 0.05308 	 0.00057 	 m..s
   15 	    13 	 0.05271 	 0.00057 	 m..s
   38 	    14 	 0.05451 	 0.00057 	 m..s
   19 	    15 	 0.05302 	 0.00058 	 m..s
   47 	    16 	 0.05474 	 0.00059 	 m..s
    2 	    17 	 0.03512 	 0.00059 	 m..s
   18 	    18 	 0.05290 	 0.00060 	 m..s
   16 	    19 	 0.05285 	 0.00060 	 m..s
    4 	    20 	 0.03731 	 0.00061 	 m..s
    5 	    21 	 0.04512 	 0.00064 	 m..s
   27 	    22 	 0.05316 	 0.00068 	 m..s
    6 	    23 	 0.04527 	 0.00069 	 m..s
   61 	    24 	 0.05508 	 0.00069 	 m..s
   67 	    25 	 0.05523 	 0.00070 	 m..s
   13 	    26 	 0.05270 	 0.00071 	 m..s
   40 	    27 	 0.05458 	 0.00072 	 m..s
   36 	    28 	 0.05438 	 0.00073 	 m..s
   31 	    29 	 0.05411 	 0.00073 	 m..s
   29 	    30 	 0.05318 	 0.00075 	 m..s
   17 	    31 	 0.05288 	 0.00076 	 m..s
   37 	    32 	 0.05450 	 0.00076 	 m..s
   71 	    33 	 0.05601 	 0.00076 	 m..s
   42 	    34 	 0.05462 	 0.00077 	 m..s
   58 	    35 	 0.05499 	 0.00078 	 m..s
   48 	    36 	 0.05474 	 0.00078 	 m..s
   41 	    37 	 0.05460 	 0.00080 	 m..s
   49 	    38 	 0.05474 	 0.00081 	 m..s
   25 	    39 	 0.05311 	 0.00081 	 m..s
   51 	    40 	 0.05477 	 0.00087 	 m..s
   60 	    41 	 0.05504 	 0.00088 	 m..s
   56 	    42 	 0.05498 	 0.00091 	 m..s
   69 	    43 	 0.05547 	 0.00092 	 m..s
   14 	    44 	 0.05271 	 0.00101 	 m..s
   22 	    45 	 0.05307 	 0.00103 	 m..s
    0 	    46 	 0.02981 	 0.00104 	 ~...
   57 	    47 	 0.05498 	 0.00106 	 m..s
   46 	    48 	 0.05472 	 0.00108 	 m..s
   44 	    49 	 0.05465 	 0.00112 	 m..s
   64 	    50 	 0.05518 	 0.00115 	 m..s
   70 	    51 	 0.05556 	 0.00116 	 m..s
   35 	    52 	 0.05436 	 0.00116 	 m..s
   66 	    53 	 0.05522 	 0.00118 	 m..s
   55 	    54 	 0.05496 	 0.00120 	 m..s
   68 	    55 	 0.05541 	 0.00122 	 m..s
   62 	    56 	 0.05509 	 0.00123 	 m..s
   12 	    57 	 0.05247 	 0.00133 	 m..s
   54 	    58 	 0.05493 	 0.00136 	 m..s
   28 	    59 	 0.05318 	 0.00137 	 m..s
   30 	    60 	 0.05332 	 0.00139 	 m..s
   33 	    61 	 0.05434 	 0.00142 	 m..s
   20 	    62 	 0.05302 	 0.00142 	 m..s
   72 	    63 	 0.05611 	 0.00143 	 m..s
    8 	    64 	 0.04750 	 0.00146 	 m..s
   50 	    65 	 0.05477 	 0.00153 	 m..s
   10 	    66 	 0.04954 	 0.00156 	 m..s
   39 	    67 	 0.05457 	 0.00161 	 m..s
   52 	    68 	 0.05485 	 0.00170 	 m..s
   53 	    69 	 0.05488 	 0.00186 	 m..s
   32 	    70 	 0.05425 	 0.00194 	 m..s
   75 	    71 	 0.05637 	 0.00195 	 m..s
    3 	    72 	 0.03553 	 0.00196 	 m..s
   21 	    73 	 0.05305 	 0.00199 	 m..s
    9 	    74 	 0.04909 	 0.00200 	 m..s
   43 	    75 	 0.05465 	 0.00200 	 m..s
   74 	    76 	 0.05634 	 0.00224 	 m..s
   77 	    77 	 0.06033 	 0.00249 	 m..s
   76 	    78 	 0.05918 	 0.00459 	 m..s
   82 	    79 	 0.07941 	 0.04959 	 ~...
   82 	    80 	 0.07941 	 0.05253 	 ~...
   94 	    81 	 0.08219 	 0.06381 	 ~...
   81 	    82 	 0.07882 	 0.07113 	 ~...
   96 	    83 	 0.08340 	 0.07360 	 ~...
   82 	    84 	 0.07941 	 0.07464 	 ~...
   82 	    85 	 0.07941 	 0.07469 	 ~...
   99 	    86 	 0.08482 	 0.07542 	 ~...
   98 	    87 	 0.08415 	 0.07636 	 ~...
   82 	    88 	 0.07941 	 0.07648 	 ~...
  100 	    89 	 0.09249 	 0.07650 	 ~...
   82 	    90 	 0.07941 	 0.07667 	 ~...
   79 	    91 	 0.07482 	 0.07719 	 ~...
   82 	    92 	 0.07941 	 0.07971 	 ~...
  104 	    93 	 0.16031 	 0.08036 	 m..s
   97 	    94 	 0.08389 	 0.08403 	 ~...
   95 	    95 	 0.08322 	 0.08523 	 ~...
   80 	    96 	 0.07601 	 0.08922 	 ~...
   93 	    97 	 0.08108 	 0.08956 	 ~...
  116 	    98 	 0.16584 	 0.09360 	 m..s
   78 	    99 	 0.07065 	 0.09838 	 ~...
  101 	   100 	 0.15687 	 0.10039 	 m..s
  114 	   101 	 0.16431 	 0.11871 	 m..s
   82 	   102 	 0.07941 	 0.12857 	 m..s
  115 	   103 	 0.16460 	 0.12860 	 m..s
  111 	   104 	 0.16257 	 0.13637 	 ~...
  110 	   105 	 0.16243 	 0.13818 	 ~...
  117 	   106 	 0.16592 	 0.17068 	 ~...
   82 	   107 	 0.07941 	 0.17901 	 m..s
  107 	   108 	 0.16177 	 0.18394 	 ~...
  108 	   109 	 0.16178 	 0.18588 	 ~...
  106 	   110 	 0.16154 	 0.18649 	 ~...
  105 	   111 	 0.16099 	 0.21233 	 m..s
  113 	   112 	 0.16312 	 0.21589 	 m..s
   82 	   113 	 0.07941 	 0.22307 	 MISS
  103 	   114 	 0.15849 	 0.27717 	 MISS
  102 	   115 	 0.15781 	 0.28122 	 MISS
  112 	   116 	 0.16259 	 0.28885 	 MISS
  109 	   117 	 0.16236 	 0.29363 	 MISS
  120 	   118 	 0.18062 	 0.31774 	 MISS
  118 	   119 	 0.17498 	 0.34668 	 MISS
  119 	   120 	 0.17693 	 0.34919 	 MISS
==========================================
r_mrr = 0.8798063397407532
r2_mrr = 0.524951696395874
spearmanr_mrr@5 = 0.9631550908088684
spearmanr_mrr@10 = 0.8822285532951355
spearmanr_mrr@50 = 0.895384669303894
spearmanr_mrr@100 = 0.9418267011642456
spearmanr_mrr@All = 0.9419822692871094
==========================================
test time: 0.637
Done Testing dataset OpenEA
total time taken: 498.42038321495056
training time taken: 432.7159597873688
TWIG out ;))
