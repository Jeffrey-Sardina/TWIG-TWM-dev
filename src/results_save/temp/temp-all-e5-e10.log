Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_UMLS-Kinships-CoDExSmall-DBpedia50-OpenEA
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.427 +- 0.005
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 8.00222e-05

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 27
rank avg (pred): 0.196 +- 0.027
mrr vals (pred, true): 0.037, 0.244
batch losses (mrrl, rdl): 0.0, 4.89255e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 135
rank avg (pred): 0.465 +- 0.017
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 8.19922e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 163
rank avg (pred): 0.446 +- 0.067
mrr vals (pred, true): 0.017, 0.048
batch losses (mrrl, rdl): 0.0, 7.07636e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 192
rank avg (pred): 0.454 +- 0.262
mrr vals (pred, true): 0.097, 0.047
batch losses (mrrl, rdl): 0.0, 2.64513e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.377 +- 0.230
mrr vals (pred, true): 0.117, 0.026
batch losses (mrrl, rdl): 0.0, 0.0001985572

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 626
rank avg (pred): 0.404 +- 0.264
mrr vals (pred, true): 0.125, 0.046
batch losses (mrrl, rdl): 0.0, 1.19654e-05

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1197
rank avg (pred): 0.434 +- 0.246
mrr vals (pred, true): 0.079, 0.045
batch losses (mrrl, rdl): 0.0, 6.178e-06

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 992
rank avg (pred): 0.193 +- 0.195
mrr vals (pred, true): 0.105, 0.285
batch losses (mrrl, rdl): 0.0, 5.40593e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1199
rank avg (pred): 0.457 +- 0.260
mrr vals (pred, true): 0.093, 0.046
batch losses (mrrl, rdl): 0.0, 1.6697e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.413 +- 0.262
mrr vals (pred, true): 0.100, 0.050
batch losses (mrrl, rdl): 0.0, 9.9684e-06

Epoch over!
epoch time: 83.024

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.431 +- 0.264
mrr vals (pred, true): 0.094, 0.050
batch losses (mrrl, rdl): 0.0, 1.3103e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 901
rank avg (pred): 0.590 +- 0.215
mrr vals (pred, true): 0.059, 0.020
batch losses (mrrl, rdl): 0.0, 9.5e-06

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 417
rank avg (pred): 0.416 +- 0.242
mrr vals (pred, true): 0.098, 0.064
batch losses (mrrl, rdl): 0.0, 6.8997e-06

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 992
rank avg (pred): 0.129 +- 0.134
mrr vals (pred, true): 0.108, 0.285
batch losses (mrrl, rdl): 0.0, 9.4118e-06

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.168 +- 0.136
mrr vals (pred, true): 0.089, 0.258
batch losses (mrrl, rdl): 0.0, 3.31176e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.451 +- 0.251
mrr vals (pred, true): 0.094, 0.043
batch losses (mrrl, rdl): 0.0, 2.17771e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 476
rank avg (pred): 0.409 +- 0.262
mrr vals (pred, true): 0.109, 0.044
batch losses (mrrl, rdl): 0.0, 7.879e-06

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 121
rank avg (pred): 0.425 +- 0.271
mrr vals (pred, true): 0.098, 0.041
batch losses (mrrl, rdl): 0.0, 1.43598e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 884
rank avg (pred): 0.432 +- 0.243
mrr vals (pred, true): 0.096, 0.045
batch losses (mrrl, rdl): 0.0, 2.9698e-06

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 512
rank avg (pred): 0.492 +- 0.240
mrr vals (pred, true): 0.072, 0.026
batch losses (mrrl, rdl): 0.0, 1.58618e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.444 +- 0.266
mrr vals (pred, true): 0.088, 0.053
batch losses (mrrl, rdl): 0.0, 3.6098e-06

Epoch over!
epoch time: 80.918

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1113
rank avg (pred): 0.436 +- 0.260
mrr vals (pred, true): 0.081, 0.054
batch losses (mrrl, rdl): 0.0, 1.6466e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 234
rank avg (pred): 0.435 +- 0.271
mrr vals (pred, true): 0.086, 0.044
batch losses (mrrl, rdl): 0.0, 4.484e-06

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 824
rank avg (pred): 0.161 +- 0.165
mrr vals (pred, true): 0.101, 0.302
batch losses (mrrl, rdl): 0.0, 1.28047e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 477
rank avg (pred): 0.464 +- 0.280
mrr vals (pred, true): 0.081, 0.048
batch losses (mrrl, rdl): 0.0, 1.48979e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 573
rank avg (pred): 0.458 +- 0.251
mrr vals (pred, true): 0.064, 0.043
batch losses (mrrl, rdl): 0.0, 1.3125e-06

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 145
rank avg (pred): 0.440 +- 0.300
mrr vals (pred, true): 0.081, 0.044
batch losses (mrrl, rdl): 0.0, 9.4982e-06

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 236
rank avg (pred): 0.414 +- 0.246
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 0.0, 1.07527e-05

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 923
rank avg (pred): 0.445 +- 0.248
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0, 8.1681e-06

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1056
rank avg (pred): 0.090 +- 0.146
mrr vals (pred, true): 0.222, 0.290
batch losses (mrrl, rdl): 0.0, 6.68188e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 421
rank avg (pred): 0.423 +- 0.252
mrr vals (pred, true): 0.059, 0.047
batch losses (mrrl, rdl): 0.0, 2.0586e-06

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 914
rank avg (pred): 0.619 +- 0.222
mrr vals (pred, true): 0.029, 0.021
batch losses (mrrl, rdl): 0.0, 2.20964e-05

Epoch over!
epoch time: 77.325

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 157
rank avg (pred): 0.422 +- 0.254
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 0.0, 1.72474e-05

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 152
rank avg (pred): 0.456 +- 0.277
mrr vals (pred, true): 0.059, 0.055
batch losses (mrrl, rdl): 0.0, 1.61558e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 799
rank avg (pred): 0.449 +- 0.237
mrr vals (pred, true): 0.037, 0.041
batch losses (mrrl, rdl): 0.0, 5.7821e-06

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 715
rank avg (pred): 0.413 +- 0.243
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 0.0, 1.66473e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 81
rank avg (pred): 0.437 +- 0.246
mrr vals (pred, true): 0.043, 0.048
batch losses (mrrl, rdl): 0.0, 5.1163e-06

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1071
rank avg (pred): 0.202 +- 0.239
mrr vals (pred, true): 0.102, 0.282
batch losses (mrrl, rdl): 0.0, 0.0001086629

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1089
rank avg (pred): 0.448 +- 0.273
mrr vals (pred, true): 0.036, 0.053
batch losses (mrrl, rdl): 0.0, 3.8614e-06

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 60
rank avg (pred): 0.171 +- 0.205
mrr vals (pred, true): 0.128, 0.219
batch losses (mrrl, rdl): 0.0, 1.36373e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 901
rank avg (pred): 0.626 +- 0.193
mrr vals (pred, true): 0.013, 0.020
batch losses (mrrl, rdl): 0.0, 3.02061e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 995
rank avg (pred): 0.177 +- 0.235
mrr vals (pred, true): 0.175, 0.289
batch losses (mrrl, rdl): 0.0, 1.96551e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 405
rank avg (pred): 0.423 +- 0.221
mrr vals (pred, true): 0.030, 0.045
batch losses (mrrl, rdl): 0.0, 2.30228e-05

Epoch over!
epoch time: 78.156

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 110
rank avg (pred): 0.456 +- 0.246
mrr vals (pred, true): 0.027, 0.050
batch losses (mrrl, rdl): 0.0, 1.14076e-05

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 725
rank avg (pred): 0.465 +- 0.246
mrr vals (pred, true): 0.025, 0.045
batch losses (mrrl, rdl): 0.0, 7.505e-06

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 572
rank avg (pred): 0.443 +- 0.255
mrr vals (pred, true): 0.034, 0.041
batch losses (mrrl, rdl): 0.0, 1.1429e-06

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 985
rank avg (pred): 0.185 +- 0.233
mrr vals (pred, true): 0.141, 0.285
batch losses (mrrl, rdl): 0.0, 1.56328e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 727
rank avg (pred): 0.462 +- 0.254
mrr vals (pred, true): 0.027, 0.044
batch losses (mrrl, rdl): 0.0, 1.49613e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 675
rank avg (pred): 0.458 +- 0.249
mrr vals (pred, true): 0.028, 0.049
batch losses (mrrl, rdl): 0.0, 2.5709e-06

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 609
rank avg (pred): 0.451 +- 0.256
mrr vals (pred, true): 0.031, 0.039
batch losses (mrrl, rdl): 0.0, 9.775e-07

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 514
rank avg (pred): 0.457 +- 0.208
mrr vals (pred, true): 0.023, 0.024
batch losses (mrrl, rdl): 0.0, 7.37685e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 166
rank avg (pred): 0.426 +- 0.230
mrr vals (pred, true): 0.027, 0.052
batch losses (mrrl, rdl): 0.0, 3.7871e-06

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 416
rank avg (pred): 0.434 +- 0.245
mrr vals (pred, true): 0.027, 0.045
batch losses (mrrl, rdl): 0.0, 1.325e-06

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.189 +- 0.245
mrr vals (pred, true): 0.154, 0.252
batch losses (mrrl, rdl): 0.0, 5.43803e-05

Epoch over!
epoch time: 79.87

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 909
rank avg (pred): 0.649 +- 0.214
mrr vals (pred, true): 0.014, 0.023
batch losses (mrrl, rdl): 0.0132862302, 0.0001018646

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1169
rank avg (pred): 0.496 +- 0.306
mrr vals (pred, true): 0.047, 0.031
batch losses (mrrl, rdl): 0.000112239, 3.97645e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 502
rank avg (pred): 0.411 +- 0.312
mrr vals (pred, true): 0.137, 0.022
batch losses (mrrl, rdl): 0.075764142, 0.000194295

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1167
rank avg (pred): 0.552 +- 0.264
mrr vals (pred, true): 0.042, 0.032
batch losses (mrrl, rdl): 0.0005669036, 0.0001984296

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1085
rank avg (pred): 0.475 +- 0.267
mrr vals (pred, true): 0.063, 0.054
batch losses (mrrl, rdl): 0.001791148, 3.26199e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 871
rank avg (pred): 0.672 +- 0.337
mrr vals (pred, true): 0.043, 0.048
batch losses (mrrl, rdl): 0.0005082756, 0.0009604325

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1136
rank avg (pred): 0.608 +- 0.360
mrr vals (pred, true): 0.109, 0.026
batch losses (mrrl, rdl): 0.0350778997, 0.00047656

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 566
rank avg (pred): 0.728 +- 0.357
mrr vals (pred, true): 0.074, 0.023
batch losses (mrrl, rdl): 0.0059065502, 0.000866062

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 965
rank avg (pred): 0.528 +- 0.267
mrr vals (pred, true): 0.058, 0.051
batch losses (mrrl, rdl): 0.0006737541, 0.0001560638

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 303
rank avg (pred): 0.605 +- 0.405
mrr vals (pred, true): 0.183, 0.236
batch losses (mrrl, rdl): 0.0273467042, 0.0034809869

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 692
rank avg (pred): 0.432 +- 0.227
mrr vals (pred, true): 0.070, 0.052
batch losses (mrrl, rdl): 0.0039318604, 6.8071e-06

Epoch over!
epoch time: 78.19

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 760
rank avg (pred): 0.593 +- 0.313
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 9.2172e-06, 0.0004013568

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 220
rank avg (pred): 0.548 +- 0.275
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 2.8449e-06, 0.0002434872

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 159
rank avg (pred): 0.556 +- 0.290
mrr vals (pred, true): 0.043, 0.048
batch losses (mrrl, rdl): 0.0004744852, 0.0002460753

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 476
rank avg (pred): 0.449 +- 0.221
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 4.94748e-05, 1.11726e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 794
rank avg (pred): 0.492 +- 0.248
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 2.66862e-05, 8.0777e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 840
rank avg (pred): 0.545 +- 0.307
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 2.5545e-06, 0.0002065336

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 835
rank avg (pred): 0.228 +- 0.198
mrr vals (pred, true): 0.313, 0.285
batch losses (mrrl, rdl): 0.0079225264, 0.0001541273

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 16
rank avg (pred): 0.231 +- 0.199
mrr vals (pred, true): 0.321, 0.184
batch losses (mrrl, rdl): 0.18975465, 6.71372e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 420
rank avg (pred): 0.535 +- 0.271
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 2.00894e-05, 0.0002513064

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 89
rank avg (pred): 0.533 +- 0.268
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0003486276, 0.0002237221

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 690
rank avg (pred): 0.402 +- 0.219
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.000398424, 2.12078e-05

Epoch over!
epoch time: 73.604

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 569
rank avg (pred): 0.443 +- 0.239
mrr vals (pred, true): 0.040, 0.038
batch losses (mrrl, rdl): 0.0009300519, 2.6976e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1088
rank avg (pred): 0.577 +- 0.317
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001283463, 0.0003141381

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 572
rank avg (pred): 0.423 +- 0.215
mrr vals (pred, true): 0.058, 0.041
batch losses (mrrl, rdl): 0.0006551244, 7.6701e-06

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 783
rank avg (pred): 0.500 +- 0.257
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 3.7853e-06, 0.0001454468

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 841
rank avg (pred): 0.523 +- 0.264
mrr vals (pred, true): 0.043, 0.047
batch losses (mrrl, rdl): 0.0005246202, 0.0001451731

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 127
rank avg (pred): 0.479 +- 0.248
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 4.51188e-05, 5.56742e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 137
rank avg (pred): 0.450 +- 0.228
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 0.0001135327, 2.4045e-05

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 266
rank avg (pred): 0.355 +- 0.251
mrr vals (pred, true): 0.273, 0.218
batch losses (mrrl, rdl): 0.0300091598, 0.0007484428

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 291
rank avg (pred): 0.395 +- 0.264
mrr vals (pred, true): 0.219, 0.209
batch losses (mrrl, rdl): 0.0010067656, 0.0009263208

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 69
rank avg (pred): 0.321 +- 0.207
mrr vals (pred, true): 0.173, 0.209
batch losses (mrrl, rdl): 0.0135316085, 0.0004134012

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 888
rank avg (pred): 0.526 +- 0.251
mrr vals (pred, true): 0.045, 0.055
batch losses (mrrl, rdl): 0.0002700641, 0.0001960275

Epoch over!
epoch time: 72.54

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 649
rank avg (pred): 0.444 +- 0.214
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 8.1991e-05, 7.9192e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 104
rank avg (pred): 0.558 +- 0.258
mrr vals (pred, true): 0.045, 0.055
batch losses (mrrl, rdl): 0.0002102774, 0.0002829542

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1138
rank avg (pred): 0.418 +- 0.211
mrr vals (pred, true): 0.050, 0.026
batch losses (mrrl, rdl): 1.95e-08, 8.81287e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1187
rank avg (pred): 0.484 +- 0.231
mrr vals (pred, true): 0.050, 0.032
batch losses (mrrl, rdl): 1.51e-08, 1.61931e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 34
rank avg (pred): 0.366 +- 0.239
mrr vals (pred, true): 0.211, 0.237
batch losses (mrrl, rdl): 0.007142006, 0.0009658779

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 531
rank avg (pred): 0.443 +- 0.195
mrr vals (pred, true): 0.040, 0.026
batch losses (mrrl, rdl): 0.0009576628, 0.0001154281

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 545
rank avg (pred): 0.338 +- 0.206
mrr vals (pred, true): 0.070, 0.025
batch losses (mrrl, rdl): 0.0038205946, 0.000600195

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 755
rank avg (pred): 0.150 +- 0.143
mrr vals (pred, true): 0.285, 0.211
batch losses (mrrl, rdl): 0.0548175126, 2.80981e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 941
rank avg (pred): 0.467 +- 0.288
mrr vals (pred, true): 0.048, 0.028
batch losses (mrrl, rdl): 3.74696e-05, 2.22787e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1056
rank avg (pred): 0.152 +- 0.145
mrr vals (pred, true): 0.262, 0.290
batch losses (mrrl, rdl): 0.0078998432, 5.4951e-06

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1086
rank avg (pred): 0.515 +- 0.327
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 4.28924e-05, 0.0001099718

Epoch over!
epoch time: 74.937

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 954
rank avg (pred): 0.566 +- 0.369
mrr vals (pred, true): 0.047, 0.058
batch losses (mrrl, rdl): 9.72565e-05, 0.0003026702

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 664
rank avg (pred): 0.600 +- 0.379
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002795161, 0.0004752658

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 23
rank avg (pred): 0.179 +- 0.149
mrr vals (pred, true): 0.242, 0.211
batch losses (mrrl, rdl): 0.0095326817, 7.2025e-06

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 549
rank avg (pred): 0.361 +- 0.181
mrr vals (pred, true): 0.046, 0.023
batch losses (mrrl, rdl): 0.0001903948, 0.0004819401

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 194
rank avg (pred): 0.547 +- 0.356
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 8.04774e-05, 0.0002250319

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.333 +- 0.234
mrr vals (pred, true): 0.069, 0.026
batch losses (mrrl, rdl): 0.0034526545, 0.0004211098

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 877
rank avg (pred): 0.546 +- 0.375
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 2.80486e-05, 0.000262068

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 863
rank avg (pred): 0.447 +- 0.314
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 2.1409e-05, 2.0658e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 513
rank avg (pred): 0.513 +- 0.227
mrr vals (pred, true): 0.038, 0.028
batch losses (mrrl, rdl): 0.0014429166, 3.0026e-06

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 432
rank avg (pred): 0.542 +- 0.388
mrr vals (pred, true): 0.059, 0.046
batch losses (mrrl, rdl): 0.0008221662, 0.0001738221

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 48
rank avg (pred): 0.203 +- 0.198
mrr vals (pred, true): 0.226, 0.208
batch losses (mrrl, rdl): 0.0035581756, 2.8628e-06

Epoch over!
epoch time: 73.151

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 452
rank avg (pred): 0.535 +- 0.376
mrr vals (pred, true): 0.050, 0.038
batch losses (mrrl, rdl): 6.4e-08, 0.0001791184

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 321
rank avg (pred): 0.203 +- 0.211
mrr vals (pred, true): 0.223, 0.176
batch losses (mrrl, rdl): 0.0225110874, 1.6669e-06

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 764
rank avg (pred): 0.475 +- 0.310
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 0.0001083259, 2.50772e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 790
rank avg (pred): 0.488 +- 0.334
mrr vals (pred, true): 0.052, 0.041
batch losses (mrrl, rdl): 2.63985e-05, 6.16377e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1126
rank avg (pred): 0.480 +- 0.323
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 1.48436e-05, 4.27887e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 940
rank avg (pred): 0.507 +- 0.326
mrr vals (pred, true): 0.074, 0.040
batch losses (mrrl, rdl): 0.0058920658, 7.20238e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 823
rank avg (pred): 0.138 +- 0.154
mrr vals (pred, true): 0.299, 0.321
batch losses (mrrl, rdl): 0.0049923677, 4.8848e-06

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1061
rank avg (pred): 0.157 +- 0.185
mrr vals (pred, true): 0.334, 0.266
batch losses (mrrl, rdl): 0.0462641865, 1.22224e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1000
rank avg (pred): 0.531 +- 0.348
mrr vals (pred, true): 0.052, 0.038
batch losses (mrrl, rdl): 3.68763e-05, 0.0001157967

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 665
rank avg (pred): 0.518 +- 0.324
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 2.428e-07, 0.0002143865

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 203
rank avg (pred): 0.494 +- 0.311
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0001994147, 6.59245e-05

Epoch over!
epoch time: 74.732

Epoch 7 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 318
rank avg (pred): 0.113 +- 0.122
mrr vals (pred, true): 0.270, 0.201
batch losses (mrrl, rdl): 0.0465594754, 0.00018993

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 283
rank avg (pred): 0.250 +- 0.237
mrr vals (pred, true): 0.216, 0.237
batch losses (mrrl, rdl): 0.0045854906, 0.0001460758

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 892
rank avg (pred): 0.383 +- 0.275
mrr vals (pred, true): 0.070, 0.020
batch losses (mrrl, rdl): 0.0039845859, 0.0008731753

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 148
rank avg (pred): 0.487 +- 0.301
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0002145756, 2.1141e-05

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 561
rank avg (pred): 0.620 +- 0.330
mrr vals (pred, true): 0.043, 0.024
batch losses (mrrl, rdl): 0.0005058303, 0.0002721859

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 866
rank avg (pred): 0.542 +- 0.334
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 6.04793e-05, 0.0001979816

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1205
rank avg (pred): 0.538 +- 0.330
mrr vals (pred, true): 0.051, 0.057
batch losses (mrrl, rdl): 7.0905e-06, 0.0002554941

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 608
rank avg (pred): 0.418 +- 0.287
mrr vals (pred, true): 0.052, 0.037
batch losses (mrrl, rdl): 2.93279e-05, 5.92517e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1131
rank avg (pred): 0.634 +- 0.380
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0002222293, 0.0006564839

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1145
rank avg (pred): 0.373 +- 0.273
mrr vals (pred, true): 0.067, 0.025
batch losses (mrrl, rdl): 0.0030372348, 0.0002826251

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 103
rank avg (pred): 0.497 +- 0.334
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 2.20286e-05, 0.0001131885

Epoch over!
epoch time: 73.662

Epoch 8 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 338
rank avg (pred): 0.483 +- 0.329
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0003301827, 6.69668e-05

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 204
rank avg (pred): 0.408 +- 0.270
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001300739, 4.07942e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 515
rank avg (pred): 0.506 +- 0.321
mrr vals (pred, true): 0.060, 0.025
batch losses (mrrl, rdl): 0.0010106815, 7.6929e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 419
rank avg (pred): 0.515 +- 0.330
mrr vals (pred, true): 0.058, 0.040
batch losses (mrrl, rdl): 0.0006951269, 0.0001028702

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 589
rank avg (pred): 0.483 +- 0.308
mrr vals (pred, true): 0.046, 0.040
batch losses (mrrl, rdl): 0.0001941416, 3.21788e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 604
rank avg (pred): 0.396 +- 0.262
mrr vals (pred, true): 0.058, 0.047
batch losses (mrrl, rdl): 0.0006970047, 5.05971e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1047
rank avg (pred): 0.616 +- 0.365
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 8.62313e-05, 0.0006838695

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 737
rank avg (pred): 0.303 +- 0.230
mrr vals (pred, true): 0.182, 0.293
batch losses (mrrl, rdl): 0.123312965, 0.0004770149

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 901
rank avg (pred): 0.457 +- 0.270
mrr vals (pred, true): 0.050, 0.020
batch losses (mrrl, rdl): 1.6307e-06, 0.0003106669

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 278
rank avg (pred): 0.233 +- 0.220
mrr vals (pred, true): 0.209, 0.227
batch losses (mrrl, rdl): 0.0032601394, 0.0001527241

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 62
rank avg (pred): 0.272 +- 0.244
mrr vals (pred, true): 0.221, 0.225
batch losses (mrrl, rdl): 0.0001792323, 0.0002810781

Epoch over!
epoch time: 72.721

Epoch 9 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.251 +- 0.237
mrr vals (pred, true): 0.222, 0.192
batch losses (mrrl, rdl): 0.0091419714, 9.06105e-05

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 583
rank avg (pred): 0.487 +- 0.299
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001287175, 2.786e-05

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 936
rank avg (pred): 0.474 +- 0.303
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 1.29034e-05, 1.37197e-05

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 917
rank avg (pred): 0.305 +- 0.216
mrr vals (pred, true): 0.073, 0.019
batch losses (mrrl, rdl): 0.0054850699, 0.0015791358

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 230
rank avg (pred): 0.441 +- 0.270
mrr vals (pred, true): 0.048, 0.047
batch losses (mrrl, rdl): 5.79067e-05, 4.9e-07

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 958
rank avg (pred): 0.476 +- 0.301
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.000130532, 2.76116e-05

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 856
rank avg (pred): 0.443 +- 0.282
mrr vals (pred, true): 0.058, 0.040
batch losses (mrrl, rdl): 0.0006050894, 5.471e-06

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 440
rank avg (pred): 0.497 +- 0.315
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 3.38168e-05, 9.39474e-05

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 932
rank avg (pred): 0.431 +- 0.276
mrr vals (pred, true): 0.053, 0.036
batch losses (mrrl, rdl): 6.72289e-05, 1.99082e-05

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 644
rank avg (pred): 0.485 +- 0.313
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002309886, 4.20831e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 98
rank avg (pred): 0.388 +- 0.254
mrr vals (pred, true): 0.051, 0.040
batch losses (mrrl, rdl): 2.03308e-05, 5.22576e-05

Epoch over!
epoch time: 75.077

Epoch 10 -- 
running batch: 0 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 588
rank avg (pred): 0.440 +- 0.273
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001857412, 4.5553e-06

running batch: 500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 527
rank avg (pred): 0.444 +- 0.259
mrr vals (pred, true): 0.050, 0.025
batch losses (mrrl, rdl): 1.1367e-06, 0.000153254

running batch: 1000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 510
rank avg (pred): 0.286 +- 0.212
mrr vals (pred, true): 0.087, 0.028
batch losses (mrrl, rdl): 0.0133620724, 0.0008523738

running batch: 1500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 128
rank avg (pred): 0.457 +- 0.303
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 3.58407e-05, 7.38e-06

running batch: 2000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 959
rank avg (pred): 0.492 +- 0.309
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 9.5245e-06, 5.95592e-05

running batch: 2500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 823
rank avg (pred): 0.126 +- 0.147
mrr vals (pred, true): 0.323, 0.321
batch losses (mrrl, rdl): 3.4977e-05, 3.0804e-06

running batch: 3000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 343
rank avg (pred): 0.445 +- 0.295
mrr vals (pred, true): 0.046, 0.055
batch losses (mrrl, rdl): 0.0001570892, 7.3696e-06

running batch: 3500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 1064
rank avg (pred): 0.314 +- 0.273
mrr vals (pred, true): 0.257, 0.289
batch losses (mrrl, rdl): 0.0102237016, 0.0007602983

running batch: 4000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 917
rank avg (pred): 0.371 +- 0.279
mrr vals (pred, true): 0.107, 0.019
batch losses (mrrl, rdl): 0.0323970877, 0.0008327946

running batch: 4500 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 596
rank avg (pred): 0.464 +- 0.314
mrr vals (pred, true): 0.051, 0.041
batch losses (mrrl, rdl): 1.21261e-05, 2.36946e-05

running batch: 5000 / 5470 and superbatch(1); data from UMLS, run 2.1, exp 293
rank avg (pred): 0.250 +- 0.241
mrr vals (pred, true): 0.225, 0.214
batch losses (mrrl, rdl): 0.0012401603, 7.98325e-05

Epoch over!
epoch time: 77.282

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.406 +- 0.289
mrr vals (pred, true): 0.052, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   67 	     0 	 0.05222 	 0.01773 	 m..s
   69 	     1 	 0.05243 	 0.02009 	 m..s
   36 	     2 	 0.05012 	 0.02164 	 ~...
   84 	     3 	 0.06318 	 0.02192 	 m..s
   88 	     4 	 0.07319 	 0.02235 	 m..s
   19 	     5 	 0.04782 	 0.02398 	 ~...
   14 	     6 	 0.04757 	 0.02414 	 ~...
   85 	     7 	 0.07167 	 0.02414 	 m..s
   44 	     8 	 0.05082 	 0.02423 	 ~...
   85 	     9 	 0.07167 	 0.02467 	 m..s
   85 	    10 	 0.07167 	 0.02475 	 m..s
   37 	    11 	 0.05014 	 0.02496 	 ~...
   89 	    12 	 0.07832 	 0.02544 	 m..s
   51 	    13 	 0.05105 	 0.02586 	 ~...
   46 	    14 	 0.05087 	 0.02933 	 ~...
   23 	    15 	 0.04823 	 0.03159 	 ~...
   26 	    16 	 0.04854 	 0.03197 	 ~...
   24 	    17 	 0.04829 	 0.03458 	 ~...
   52 	    18 	 0.05118 	 0.03549 	 ~...
   28 	    19 	 0.04907 	 0.03582 	 ~...
   31 	    20 	 0.04937 	 0.03732 	 ~...
   38 	    21 	 0.05030 	 0.03811 	 ~...
   21 	    22 	 0.04794 	 0.03856 	 ~...
   78 	    23 	 0.05309 	 0.03931 	 ~...
   54 	    24 	 0.05137 	 0.03957 	 ~...
   15 	    25 	 0.04758 	 0.03980 	 ~...
   41 	    26 	 0.05058 	 0.04032 	 ~...
   40 	    27 	 0.05055 	 0.04070 	 ~...
   82 	    28 	 0.05365 	 0.04080 	 ~...
    2 	    29 	 0.04448 	 0.04099 	 ~...
   61 	    30 	 0.05185 	 0.04129 	 ~...
   45 	    31 	 0.05084 	 0.04130 	 ~...
   68 	    32 	 0.05233 	 0.04163 	 ~...
   49 	    33 	 0.05097 	 0.04165 	 ~...
   25 	    34 	 0.04834 	 0.04176 	 ~...
   71 	    35 	 0.05257 	 0.04224 	 ~...
   74 	    36 	 0.05270 	 0.04272 	 ~...
    8 	    37 	 0.04673 	 0.04276 	 ~...
    7 	    38 	 0.04666 	 0.04306 	 ~...
   53 	    39 	 0.05123 	 0.04338 	 ~...
    1 	    40 	 0.04428 	 0.04372 	 ~...
   39 	    41 	 0.05048 	 0.04373 	 ~...
   75 	    42 	 0.05286 	 0.04388 	 ~...
    3 	    43 	 0.04585 	 0.04410 	 ~...
   72 	    44 	 0.05266 	 0.04419 	 ~...
   63 	    45 	 0.05198 	 0.04420 	 ~...
   70 	    46 	 0.05251 	 0.04423 	 ~...
   33 	    47 	 0.04963 	 0.04476 	 ~...
   11 	    48 	 0.04716 	 0.04514 	 ~...
   47 	    49 	 0.05089 	 0.04551 	 ~...
   79 	    50 	 0.05326 	 0.04564 	 ~...
    6 	    51 	 0.04665 	 0.04617 	 ~...
   27 	    52 	 0.04861 	 0.04642 	 ~...
    4 	    53 	 0.04587 	 0.04687 	 ~...
   20 	    54 	 0.04790 	 0.04699 	 ~...
   10 	    55 	 0.04689 	 0.04706 	 ~...
   34 	    56 	 0.04986 	 0.04718 	 ~...
   77 	    57 	 0.05298 	 0.04727 	 ~...
   59 	    58 	 0.05159 	 0.04740 	 ~...
    5 	    59 	 0.04591 	 0.04744 	 ~...
   42 	    60 	 0.05060 	 0.04750 	 ~...
   12 	    61 	 0.04718 	 0.04758 	 ~...
   22 	    62 	 0.04803 	 0.04763 	 ~...
   76 	    63 	 0.05298 	 0.04824 	 ~...
   62 	    64 	 0.05196 	 0.04857 	 ~...
   29 	    65 	 0.04924 	 0.04879 	 ~...
   30 	    66 	 0.04931 	 0.04879 	 ~...
   64 	    67 	 0.05206 	 0.04904 	 ~...
   80 	    68 	 0.05327 	 0.04921 	 ~...
   13 	    69 	 0.04756 	 0.04928 	 ~...
   32 	    70 	 0.04957 	 0.04929 	 ~...
   16 	    71 	 0.04767 	 0.04949 	 ~...
   18 	    72 	 0.04780 	 0.04956 	 ~...
   83 	    73 	 0.05373 	 0.04987 	 ~...
   56 	    74 	 0.05154 	 0.05004 	 ~...
   65 	    75 	 0.05206 	 0.05023 	 ~...
   55 	    76 	 0.05149 	 0.05056 	 ~...
   48 	    77 	 0.05095 	 0.05108 	 ~...
   50 	    78 	 0.05098 	 0.05128 	 ~...
    9 	    79 	 0.04677 	 0.05129 	 ~...
    0 	    80 	 0.04323 	 0.05144 	 ~...
   17 	    81 	 0.04774 	 0.05146 	 ~...
   60 	    82 	 0.05167 	 0.05160 	 ~...
   43 	    83 	 0.05079 	 0.05189 	 ~...
   81 	    84 	 0.05330 	 0.05246 	 ~...
   58 	    85 	 0.05156 	 0.05275 	 ~...
   66 	    86 	 0.05208 	 0.05352 	 ~...
   73 	    87 	 0.05269 	 0.05355 	 ~...
   35 	    88 	 0.04996 	 0.05567 	 ~...
   57 	    89 	 0.05156 	 0.05977 	 ~...
  101 	    90 	 0.23377 	 0.18889 	 m..s
  100 	    91 	 0.21903 	 0.18959 	 ~...
   99 	    92 	 0.21879 	 0.19213 	 ~...
  102 	    93 	 0.24600 	 0.19872 	 m..s
   94 	    94 	 0.21104 	 0.20550 	 ~...
   97 	    95 	 0.21793 	 0.20694 	 ~...
   90 	    96 	 0.20645 	 0.20727 	 ~...
   95 	    97 	 0.21144 	 0.21693 	 ~...
   91 	    98 	 0.20887 	 0.21723 	 ~...
   96 	    99 	 0.21267 	 0.21940 	 ~...
   98 	   100 	 0.21838 	 0.21956 	 ~...
  106 	   101 	 0.25976 	 0.23290 	 ~...
   92 	   102 	 0.20890 	 0.23984 	 m..s
  103 	   103 	 0.24874 	 0.24107 	 ~...
  108 	   104 	 0.28561 	 0.24218 	 m..s
  107 	   105 	 0.26338 	 0.24307 	 ~...
   93 	   106 	 0.20901 	 0.25298 	 m..s
  104 	   107 	 0.24978 	 0.25327 	 ~...
  116 	   108 	 0.36208 	 0.27378 	 m..s
  105 	   109 	 0.25858 	 0.27425 	 ~...
  112 	   110 	 0.31708 	 0.27441 	 m..s
  114 	   111 	 0.33396 	 0.27737 	 m..s
  113 	   112 	 0.31904 	 0.28034 	 m..s
  109 	   113 	 0.30309 	 0.28353 	 ~...
  111 	   114 	 0.31677 	 0.29788 	 ~...
  117 	   115 	 0.38126 	 0.29922 	 m..s
  110 	   116 	 0.30704 	 0.31748 	 ~...
  115 	   117 	 0.36029 	 0.33485 	 ~...
  118 	   118 	 0.41689 	 0.39525 	 ~...
  119 	   119 	 0.44549 	 0.52223 	 m..s
  120 	   120 	 0.45111 	 0.53924 	 m..s
==========================================
r_mrr = 0.9781767129898071
r2_mrr = 0.9480052590370178
spearmanr_mrr@5 = 0.9687344431877136
spearmanr_mrr@10 = 0.9462459683418274
spearmanr_mrr@50 = 0.9815124273300171
spearmanr_mrr@100 = 0.9887461066246033
spearmanr_mrr@All = 0.9887431263923645
==========================================
test time: 0.631
Done Testing dataset UMLS
Testing model with dataset Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.431 +- 0.459
mrr vals (pred, true): 0.297, 0.228

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   67 	     0 	 0.05262 	 0.04713 	 ~...
    7 	     1 	 0.04976 	 0.04727 	 ~...
   26 	     2 	 0.05100 	 0.04763 	 ~...
   58 	     3 	 0.05180 	 0.04803 	 ~...
   37 	     4 	 0.05130 	 0.04830 	 ~...
   15 	     5 	 0.05066 	 0.04840 	 ~...
   16 	     6 	 0.05071 	 0.04863 	 ~...
   13 	     7 	 0.05062 	 0.04890 	 ~...
   22 	     8 	 0.05089 	 0.04927 	 ~...
   63 	     9 	 0.05241 	 0.04930 	 ~...
   41 	    10 	 0.05133 	 0.04948 	 ~...
   32 	    11 	 0.05118 	 0.04950 	 ~...
   50 	    12 	 0.05143 	 0.04954 	 ~...
   21 	    13 	 0.05089 	 0.04990 	 ~...
   34 	    14 	 0.05124 	 0.05021 	 ~...
   24 	    15 	 0.05092 	 0.05035 	 ~...
   71 	    16 	 0.05331 	 0.05050 	 ~...
    2 	    17 	 0.04901 	 0.05071 	 ~...
   61 	    18 	 0.05191 	 0.05076 	 ~...
    1 	    19 	 0.04900 	 0.05097 	 ~...
    5 	    20 	 0.04926 	 0.05105 	 ~...
   43 	    21 	 0.05138 	 0.05107 	 ~...
   31 	    22 	 0.05114 	 0.05112 	 ~...
   72 	    23 	 0.05361 	 0.05113 	 ~...
   17 	    24 	 0.05073 	 0.05127 	 ~...
   68 	    25 	 0.05269 	 0.05151 	 ~...
   55 	    26 	 0.05169 	 0.05154 	 ~...
   11 	    27 	 0.05045 	 0.05179 	 ~...
   38 	    28 	 0.05131 	 0.05186 	 ~...
   73 	    29 	 0.05406 	 0.05193 	 ~...
   69 	    30 	 0.05308 	 0.05211 	 ~...
   64 	    31 	 0.05243 	 0.05215 	 ~...
   44 	    32 	 0.05139 	 0.05219 	 ~...
    3 	    33 	 0.04913 	 0.05236 	 ~...
   18 	    34 	 0.05081 	 0.05312 	 ~...
   65 	    35 	 0.05245 	 0.05317 	 ~...
   52 	    36 	 0.05146 	 0.05317 	 ~...
   42 	    37 	 0.05138 	 0.05341 	 ~...
   74 	    38 	 0.06019 	 0.05350 	 ~...
   33 	    39 	 0.05121 	 0.05351 	 ~...
   12 	    40 	 0.05057 	 0.05358 	 ~...
   57 	    41 	 0.05172 	 0.05406 	 ~...
    8 	    42 	 0.04983 	 0.05428 	 ~...
   27 	    43 	 0.05102 	 0.05440 	 ~...
    0 	    44 	 0.04891 	 0.05481 	 ~...
   59 	    45 	 0.05183 	 0.05498 	 ~...
    9 	    46 	 0.05013 	 0.05510 	 ~...
   51 	    47 	 0.05143 	 0.05531 	 ~...
   54 	    48 	 0.05164 	 0.05575 	 ~...
   23 	    49 	 0.05092 	 0.05576 	 ~...
   20 	    50 	 0.05089 	 0.05587 	 ~...
   66 	    51 	 0.05261 	 0.05596 	 ~...
   35 	    52 	 0.05124 	 0.05638 	 ~...
   47 	    53 	 0.05140 	 0.05694 	 ~...
   60 	    54 	 0.05184 	 0.05696 	 ~...
   48 	    55 	 0.05141 	 0.05719 	 ~...
   45 	    56 	 0.05139 	 0.05723 	 ~...
   40 	    57 	 0.05132 	 0.05725 	 ~...
   39 	    58 	 0.05131 	 0.05733 	 ~...
   36 	    59 	 0.05129 	 0.05749 	 ~...
   19 	    60 	 0.05087 	 0.05787 	 ~...
   62 	    61 	 0.05196 	 0.05825 	 ~...
   49 	    62 	 0.05143 	 0.05868 	 ~...
    4 	    63 	 0.04925 	 0.05870 	 ~...
   75 	    64 	 0.06047 	 0.05874 	 ~...
   29 	    65 	 0.05104 	 0.05881 	 ~...
   53 	    66 	 0.05154 	 0.05882 	 ~...
   30 	    67 	 0.05105 	 0.05918 	 ~...
   10 	    68 	 0.05044 	 0.05969 	 ~...
    6 	    69 	 0.04958 	 0.05993 	 ~...
   14 	    70 	 0.05065 	 0.06014 	 ~...
   28 	    71 	 0.05104 	 0.06038 	 ~...
   25 	    72 	 0.05093 	 0.06049 	 ~...
   70 	    73 	 0.05313 	 0.06096 	 ~...
   56 	    74 	 0.05169 	 0.06158 	 ~...
   46 	    75 	 0.05140 	 0.06229 	 ~...
   76 	    76 	 0.18341 	 0.16198 	 ~...
   78 	    77 	 0.26160 	 0.18404 	 m..s
   81 	    78 	 0.28135 	 0.18970 	 m..s
   77 	    79 	 0.25914 	 0.19886 	 m..s
   80 	    80 	 0.28120 	 0.21213 	 m..s
   79 	    81 	 0.27135 	 0.21714 	 m..s
   83 	    82 	 0.28853 	 0.21822 	 m..s
   96 	    83 	 0.34559 	 0.21861 	 MISS
   82 	    84 	 0.28677 	 0.22784 	 m..s
   84 	    85 	 0.29656 	 0.22825 	 m..s
   85 	    86 	 0.29789 	 0.25404 	 m..s
   96 	    87 	 0.34559 	 0.26205 	 m..s
   99 	    88 	 0.34676 	 0.26840 	 m..s
   95 	    89 	 0.34321 	 0.27182 	 m..s
   89 	    90 	 0.34048 	 0.28071 	 m..s
   96 	    91 	 0.34559 	 0.28148 	 m..s
   91 	    92 	 0.34126 	 0.28774 	 m..s
   93 	    93 	 0.34156 	 0.29619 	 m..s
   87 	    94 	 0.33718 	 0.29634 	 m..s
  100 	    95 	 0.36111 	 0.29652 	 m..s
   86 	    96 	 0.33411 	 0.30510 	 ~...
  104 	    97 	 0.41305 	 0.30598 	 MISS
   92 	    98 	 0.34137 	 0.30754 	 m..s
  103 	    99 	 0.41090 	 0.30803 	 MISS
   88 	   100 	 0.34022 	 0.30884 	 m..s
  101 	   101 	 0.37994 	 0.31529 	 m..s
  102 	   102 	 0.40342 	 0.31655 	 m..s
   90 	   103 	 0.34108 	 0.32286 	 ~...
  108 	   104 	 0.47942 	 0.33122 	 MISS
   94 	   105 	 0.34192 	 0.34296 	 ~...
  109 	   106 	 0.50286 	 0.36221 	 MISS
  107 	   107 	 0.44227 	 0.36880 	 m..s
  106 	   108 	 0.43427 	 0.36975 	 m..s
  105 	   109 	 0.43168 	 0.37989 	 m..s
  114 	   110 	 0.52699 	 0.38991 	 MISS
  111 	   111 	 0.51205 	 0.40258 	 MISS
  110 	   112 	 0.50359 	 0.40967 	 m..s
  112 	   113 	 0.52430 	 0.41075 	 MISS
  113 	   114 	 0.52584 	 0.42112 	 MISS
  115 	   115 	 0.53194 	 0.47933 	 m..s
  116 	   116 	 0.53637 	 0.48000 	 m..s
  117 	   117 	 0.56277 	 0.50285 	 m..s
  119 	   118 	 0.66362 	 0.61365 	 m..s
  118 	   119 	 0.63982 	 0.61786 	 ~...
  120 	   120 	 0.66794 	 0.62288 	 m..s
==========================================
r_mrr = 0.9893261194229126
r2_mrr = 0.9020447134971619
spearmanr_mrr@5 = 0.9898554682731628
spearmanr_mrr@10 = 0.9687694907188416
spearmanr_mrr@50 = 0.980868935585022
spearmanr_mrr@100 = 0.9920275807380676
spearmanr_mrr@All = 0.9928893446922302
==========================================
test time: 0.593
Done Testing dataset Kinships
Testing model with dataset CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.408 +- 0.381
mrr vals (pred, true): 0.049, 0.001

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   84 	     0 	 0.06684 	 0.00086 	 m..s
   20 	     1 	 0.04887 	 0.00089 	 m..s
   21 	     2 	 0.04907 	 0.00170 	 m..s
   26 	     3 	 0.04958 	 0.00174 	 m..s
   28 	     4 	 0.04980 	 0.00204 	 m..s
   47 	     5 	 0.05153 	 0.00240 	 m..s
   27 	     6 	 0.04959 	 0.00242 	 m..s
   31 	     7 	 0.05009 	 0.00242 	 m..s
   46 	     8 	 0.05153 	 0.00287 	 m..s
   37 	     9 	 0.05097 	 0.00289 	 m..s
   83 	    10 	 0.05652 	 0.00291 	 m..s
   66 	    11 	 0.05292 	 0.00292 	 m..s
   52 	    12 	 0.05185 	 0.00296 	 m..s
   15 	    13 	 0.04846 	 0.00310 	 m..s
   39 	    14 	 0.05105 	 0.00315 	 m..s
   25 	    15 	 0.04953 	 0.00316 	 m..s
   80 	    16 	 0.05497 	 0.00317 	 m..s
   41 	    17 	 0.05128 	 0.00325 	 m..s
   16 	    18 	 0.04866 	 0.00331 	 m..s
   60 	    19 	 0.05248 	 0.00331 	 m..s
   19 	    20 	 0.04873 	 0.00336 	 m..s
   40 	    21 	 0.05126 	 0.00337 	 m..s
   51 	    22 	 0.05181 	 0.00345 	 m..s
    4 	    23 	 0.04588 	 0.00345 	 m..s
   75 	    24 	 0.05395 	 0.00356 	 m..s
   64 	    25 	 0.05276 	 0.00360 	 m..s
   13 	    26 	 0.04829 	 0.00362 	 m..s
   63 	    27 	 0.05276 	 0.00362 	 m..s
   12 	    28 	 0.04820 	 0.00363 	 m..s
   69 	    29 	 0.05316 	 0.00363 	 m..s
   18 	    30 	 0.04872 	 0.00364 	 m..s
   54 	    31 	 0.05195 	 0.00368 	 m..s
   81 	    32 	 0.05556 	 0.00368 	 m..s
   33 	    33 	 0.05055 	 0.00374 	 m..s
   38 	    34 	 0.05102 	 0.00376 	 m..s
   70 	    35 	 0.05321 	 0.00376 	 m..s
   32 	    36 	 0.05019 	 0.00381 	 m..s
   24 	    37 	 0.04938 	 0.00381 	 m..s
    8 	    38 	 0.04731 	 0.00384 	 m..s
   74 	    39 	 0.05390 	 0.00389 	 m..s
   73 	    40 	 0.05384 	 0.00391 	 m..s
    7 	    41 	 0.04720 	 0.00394 	 m..s
   29 	    42 	 0.04993 	 0.00399 	 m..s
   72 	    43 	 0.05330 	 0.00400 	 m..s
   34 	    44 	 0.05058 	 0.00402 	 m..s
   11 	    45 	 0.04808 	 0.00404 	 m..s
   42 	    46 	 0.05131 	 0.00406 	 m..s
   59 	    47 	 0.05241 	 0.00409 	 m..s
   36 	    48 	 0.05080 	 0.00412 	 m..s
   49 	    49 	 0.05156 	 0.00413 	 m..s
   45 	    50 	 0.05149 	 0.00415 	 m..s
   17 	    51 	 0.04868 	 0.00415 	 m..s
   76 	    52 	 0.05423 	 0.00415 	 m..s
   71 	    53 	 0.05328 	 0.00415 	 m..s
   68 	    54 	 0.05305 	 0.00416 	 m..s
   57 	    55 	 0.05223 	 0.00418 	 m..s
   44 	    56 	 0.05142 	 0.00418 	 m..s
   77 	    57 	 0.05444 	 0.00420 	 m..s
   48 	    58 	 0.05154 	 0.00422 	 m..s
   79 	    59 	 0.05473 	 0.00424 	 m..s
   35 	    60 	 0.05076 	 0.00440 	 m..s
   50 	    61 	 0.05159 	 0.00454 	 m..s
   23 	    62 	 0.04911 	 0.00455 	 m..s
   22 	    63 	 0.04909 	 0.00459 	 m..s
   61 	    64 	 0.05253 	 0.00460 	 m..s
   62 	    65 	 0.05261 	 0.00461 	 m..s
   82 	    66 	 0.05595 	 0.00461 	 m..s
   10 	    67 	 0.04741 	 0.00467 	 m..s
   30 	    68 	 0.05004 	 0.00469 	 m..s
   43 	    69 	 0.05136 	 0.00470 	 m..s
    9 	    70 	 0.04738 	 0.00473 	 m..s
   58 	    71 	 0.05236 	 0.00477 	 m..s
   53 	    72 	 0.05188 	 0.00478 	 m..s
   78 	    73 	 0.05458 	 0.00485 	 m..s
   67 	    74 	 0.05304 	 0.00500 	 m..s
   65 	    75 	 0.05280 	 0.00501 	 m..s
   56 	    76 	 0.05220 	 0.00514 	 m..s
   55 	    77 	 0.05200 	 0.00551 	 m..s
   14 	    78 	 0.04835 	 0.00604 	 m..s
    0 	    79 	 0.04330 	 0.00680 	 m..s
    1 	    80 	 0.04352 	 0.00694 	 m..s
    2 	    81 	 0.04562 	 0.00739 	 m..s
    3 	    82 	 0.04565 	 0.00804 	 m..s
    6 	    83 	 0.04666 	 0.01152 	 m..s
    5 	    84 	 0.04642 	 0.01271 	 m..s
   85 	    85 	 0.08237 	 0.02050 	 m..s
   85 	    86 	 0.08237 	 0.02430 	 m..s
   85 	    87 	 0.08237 	 0.02863 	 m..s
   89 	    88 	 0.10869 	 0.02916 	 m..s
   88 	    89 	 0.08489 	 0.02920 	 m..s
   98 	    90 	 0.19293 	 0.12715 	 m..s
   93 	    91 	 0.18153 	 0.12799 	 m..s
   99 	    92 	 0.19365 	 0.13094 	 m..s
   90 	    93 	 0.17869 	 0.13140 	 m..s
   96 	    94 	 0.18542 	 0.13806 	 m..s
   91 	    95 	 0.18028 	 0.14041 	 m..s
   97 	    96 	 0.19159 	 0.17202 	 ~...
  100 	    97 	 0.19481 	 0.17271 	 ~...
   92 	    98 	 0.18033 	 0.17434 	 ~...
   95 	    99 	 0.18402 	 0.17792 	 ~...
  110 	   100 	 0.29107 	 0.18551 	 MISS
   94 	   101 	 0.18326 	 0.18924 	 ~...
  113 	   102 	 0.29898 	 0.19630 	 MISS
  112 	   103 	 0.29848 	 0.20171 	 m..s
  114 	   104 	 0.30038 	 0.21048 	 m..s
  103 	   105 	 0.25982 	 0.22857 	 m..s
  108 	   106 	 0.28068 	 0.22949 	 m..s
  102 	   107 	 0.25828 	 0.23928 	 ~...
  111 	   108 	 0.29734 	 0.24645 	 m..s
  101 	   109 	 0.24946 	 0.25269 	 ~...
  118 	   110 	 0.38138 	 0.25972 	 MISS
  104 	   111 	 0.26099 	 0.26879 	 ~...
  117 	   112 	 0.36574 	 0.27244 	 m..s
  116 	   113 	 0.33856 	 0.28454 	 m..s
  119 	   114 	 0.38456 	 0.29618 	 m..s
  105 	   115 	 0.26961 	 0.30562 	 m..s
  115 	   116 	 0.33397 	 0.31181 	 ~...
  120 	   117 	 0.38627 	 0.31362 	 m..s
  109 	   118 	 0.29049 	 0.31910 	 ~...
  107 	   119 	 0.27341 	 0.33494 	 m..s
  106 	   120 	 0.27059 	 0.34039 	 m..s
==========================================
r_mrr = 0.9688564538955688
r2_mrr = 0.7429779171943665
spearmanr_mrr@5 = 0.7703190445899963
spearmanr_mrr@10 = 0.9307120442390442
spearmanr_mrr@50 = 0.9920836687088013
spearmanr_mrr@100 = 0.9953837394714355
spearmanr_mrr@All = 0.9956287741661072
==========================================
test time: 0.633
Done Testing dataset CoDExSmall
Testing model with dataset DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.709 +- 0.427
mrr vals (pred, true): 0.060, 0.002

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.05285 	 0.00014 	 m..s
   53 	     1 	 0.05740 	 0.00014 	 m..s
   48 	     2 	 0.05727 	 0.00014 	 m..s
   92 	     3 	 0.08625 	 0.00015 	 m..s
   32 	     4 	 0.05686 	 0.00016 	 m..s
   55 	     5 	 0.05743 	 0.00016 	 m..s
    3 	     6 	 0.05078 	 0.00017 	 m..s
   66 	     7 	 0.05839 	 0.00017 	 m..s
    8 	     8 	 0.05333 	 0.00018 	 m..s
   21 	     9 	 0.05573 	 0.00018 	 m..s
   78 	    10 	 0.06018 	 0.00018 	 m..s
   34 	    11 	 0.05689 	 0.00019 	 m..s
   73 	    12 	 0.05949 	 0.00019 	 m..s
   10 	    13 	 0.05354 	 0.00021 	 m..s
    9 	    14 	 0.05347 	 0.00021 	 m..s
   41 	    15 	 0.05713 	 0.00021 	 m..s
   40 	    16 	 0.05713 	 0.00021 	 m..s
   25 	    17 	 0.05633 	 0.00021 	 m..s
   11 	    18 	 0.05357 	 0.00021 	 m..s
    2 	    19 	 0.05055 	 0.00022 	 m..s
   56 	    20 	 0.05744 	 0.00022 	 m..s
   36 	    21 	 0.05699 	 0.00023 	 m..s
   22 	    22 	 0.05574 	 0.00023 	 m..s
   42 	    23 	 0.05714 	 0.00024 	 m..s
   20 	    24 	 0.05543 	 0.00024 	 m..s
   61 	    25 	 0.05762 	 0.00024 	 m..s
   68 	    26 	 0.05874 	 0.00024 	 m..s
   50 	    27 	 0.05733 	 0.00024 	 m..s
   52 	    28 	 0.05735 	 0.00024 	 m..s
   58 	    29 	 0.05749 	 0.00025 	 m..s
   15 	    30 	 0.05415 	 0.00025 	 m..s
   30 	    31 	 0.05678 	 0.00025 	 m..s
   37 	    32 	 0.05708 	 0.00026 	 m..s
   69 	    33 	 0.05907 	 0.00026 	 m..s
   46 	    34 	 0.05725 	 0.00027 	 m..s
    0 	    35 	 0.04219 	 0.00027 	 m..s
   62 	    36 	 0.05784 	 0.00028 	 m..s
   33 	    37 	 0.05686 	 0.00028 	 m..s
   38 	    38 	 0.05709 	 0.00028 	 m..s
   45 	    39 	 0.05724 	 0.00029 	 m..s
   14 	    40 	 0.05410 	 0.00029 	 m..s
    7 	    41 	 0.05313 	 0.00029 	 m..s
   71 	    42 	 0.05916 	 0.00030 	 m..s
    1 	    43 	 0.04812 	 0.00030 	 m..s
   49 	    44 	 0.05732 	 0.00030 	 m..s
   64 	    45 	 0.05825 	 0.00031 	 m..s
   39 	    46 	 0.05712 	 0.00031 	 m..s
   47 	    47 	 0.05726 	 0.00032 	 m..s
    6 	    48 	 0.05292 	 0.00032 	 m..s
   24 	    49 	 0.05586 	 0.00033 	 m..s
   27 	    50 	 0.05658 	 0.00034 	 m..s
   13 	    51 	 0.05388 	 0.00034 	 m..s
   29 	    52 	 0.05676 	 0.00034 	 m..s
   57 	    53 	 0.05746 	 0.00035 	 m..s
   23 	    54 	 0.05583 	 0.00036 	 m..s
   63 	    55 	 0.05791 	 0.00037 	 m..s
   67 	    56 	 0.05852 	 0.00037 	 m..s
   44 	    57 	 0.05723 	 0.00038 	 m..s
   28 	    58 	 0.05666 	 0.00038 	 m..s
   65 	    59 	 0.05836 	 0.00039 	 m..s
   83 	    60 	 0.06409 	 0.00039 	 m..s
   16 	    61 	 0.05455 	 0.00039 	 m..s
   82 	    62 	 0.06220 	 0.00039 	 m..s
   54 	    63 	 0.05741 	 0.00043 	 m..s
   12 	    64 	 0.05371 	 0.00043 	 m..s
    4 	    65 	 0.05093 	 0.00046 	 m..s
   43 	    66 	 0.05723 	 0.00049 	 m..s
   18 	    67 	 0.05476 	 0.00050 	 m..s
   81 	    68 	 0.06210 	 0.00052 	 m..s
   17 	    69 	 0.05467 	 0.00053 	 m..s
   26 	    70 	 0.05656 	 0.00055 	 m..s
   35 	    71 	 0.05693 	 0.00056 	 m..s
   19 	    72 	 0.05529 	 0.00057 	 m..s
   60 	    73 	 0.05760 	 0.00076 	 m..s
   31 	    74 	 0.05680 	 0.00079 	 m..s
   59 	    75 	 0.05758 	 0.00084 	 m..s
   51 	    76 	 0.05734 	 0.00129 	 m..s
   79 	    77 	 0.06050 	 0.00171 	 m..s
   80 	    78 	 0.06055 	 0.00789 	 m..s
   93 	    79 	 0.08856 	 0.04857 	 m..s
   72 	    80 	 0.05924 	 0.06447 	 ~...
   90 	    81 	 0.06647 	 0.07961 	 ~...
   85 	    82 	 0.06580 	 0.08371 	 ~...
   74 	    83 	 0.05992 	 0.08488 	 ~...
   84 	    84 	 0.06569 	 0.08803 	 ~...
   91 	    85 	 0.08509 	 0.08860 	 ~...
   97 	    86 	 0.10804 	 0.08867 	 ~...
   94 	    87 	 0.08909 	 0.09060 	 ~...
   87 	    88 	 0.06597 	 0.09126 	 ~...
   89 	    89 	 0.06614 	 0.09149 	 ~...
   86 	    90 	 0.06595 	 0.09180 	 ~...
   75 	    91 	 0.05993 	 0.09286 	 m..s
   77 	    92 	 0.06016 	 0.09363 	 m..s
   95 	    93 	 0.09018 	 0.09460 	 ~...
   70 	    94 	 0.05915 	 0.09532 	 m..s
  104 	    95 	 0.16439 	 0.09535 	 m..s
  105 	    96 	 0.16582 	 0.09764 	 m..s
   76 	    97 	 0.06011 	 0.10902 	 m..s
  100 	    98 	 0.12597 	 0.12319 	 ~...
   96 	    99 	 0.10599 	 0.13164 	 ~...
   97 	   100 	 0.10804 	 0.13674 	 ~...
  103 	   101 	 0.14100 	 0.13680 	 ~...
   88 	   102 	 0.06606 	 0.13810 	 m..s
   97 	   103 	 0.10804 	 0.14653 	 m..s
  102 	   104 	 0.13507 	 0.15318 	 ~...
  110 	   105 	 0.18352 	 0.16378 	 ~...
  101 	   106 	 0.13212 	 0.16586 	 m..s
  106 	   107 	 0.17590 	 0.18204 	 ~...
  112 	   108 	 0.19086 	 0.18503 	 ~...
  107 	   109 	 0.17841 	 0.21574 	 m..s
  108 	   110 	 0.17947 	 0.21666 	 m..s
  109 	   111 	 0.18227 	 0.22657 	 m..s
  114 	   112 	 0.21916 	 0.26914 	 m..s
  117 	   113 	 0.23019 	 0.28706 	 m..s
  116 	   114 	 0.22654 	 0.30486 	 m..s
  115 	   115 	 0.22240 	 0.31033 	 m..s
  113 	   116 	 0.19655 	 0.31527 	 MISS
  120 	   117 	 0.27031 	 0.31960 	 m..s
  111 	   118 	 0.18656 	 0.33534 	 MISS
  118 	   119 	 0.26344 	 0.34767 	 m..s
  119 	   120 	 0.26929 	 0.38203 	 MISS
==========================================
r_mrr = 0.9348567128181458
r2_mrr = 0.6616705656051636
spearmanr_mrr@5 = 0.825395405292511
spearmanr_mrr@10 = 0.9245353937149048
spearmanr_mrr@50 = 0.9572122693061829
spearmanr_mrr@100 = 0.964326798915863
spearmanr_mrr@All = 0.9666028618812561
==========================================
test time: 0.597
Done Testing dataset DBpedia50
Testing model with dataset OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.600 +- 0.469
mrr vals (pred, true): 0.053, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   87 	     0 	 0.06180 	 0.00031 	 m..s
   35 	     1 	 0.05019 	 0.00045 	 m..s
   14 	     2 	 0.04805 	 0.00047 	 m..s
   56 	     3 	 0.05134 	 0.00051 	 m..s
   17 	     4 	 0.04869 	 0.00052 	 m..s
   81 	     5 	 0.05368 	 0.00053 	 m..s
   62 	     6 	 0.05171 	 0.00053 	 m..s
   46 	     7 	 0.05082 	 0.00055 	 m..s
   42 	     8 	 0.05071 	 0.00055 	 m..s
   77 	     9 	 0.05294 	 0.00056 	 m..s
   25 	    10 	 0.04958 	 0.00056 	 m..s
   20 	    11 	 0.04890 	 0.00057 	 m..s
   75 	    12 	 0.05286 	 0.00057 	 m..s
   82 	    13 	 0.05400 	 0.00057 	 m..s
    3 	    14 	 0.04608 	 0.00057 	 m..s
   29 	    15 	 0.04993 	 0.00058 	 m..s
   60 	    16 	 0.05162 	 0.00059 	 m..s
   30 	    17 	 0.05003 	 0.00059 	 m..s
   64 	    18 	 0.05186 	 0.00060 	 m..s
   10 	    19 	 0.04768 	 0.00060 	 m..s
   57 	    20 	 0.05143 	 0.00061 	 m..s
    1 	    21 	 0.04582 	 0.00064 	 m..s
   79 	    22 	 0.05326 	 0.00068 	 m..s
    8 	    23 	 0.04757 	 0.00069 	 m..s
   27 	    24 	 0.04983 	 0.00069 	 m..s
   41 	    25 	 0.05069 	 0.00070 	 m..s
   12 	    26 	 0.04796 	 0.00071 	 m..s
   61 	    27 	 0.05168 	 0.00072 	 m..s
   80 	    28 	 0.05335 	 0.00073 	 m..s
   33 	    29 	 0.05010 	 0.00073 	 m..s
   49 	    30 	 0.05107 	 0.00075 	 m..s
    5 	    31 	 0.04714 	 0.00076 	 m..s
    4 	    32 	 0.04614 	 0.00076 	 m..s
   16 	    33 	 0.04843 	 0.00076 	 m..s
   13 	    34 	 0.04798 	 0.00077 	 m..s
   15 	    35 	 0.04841 	 0.00078 	 m..s
   18 	    36 	 0.04872 	 0.00078 	 m..s
   54 	    37 	 0.05122 	 0.00080 	 m..s
   43 	    38 	 0.05072 	 0.00081 	 m..s
   74 	    39 	 0.05268 	 0.00081 	 m..s
   11 	    40 	 0.04796 	 0.00087 	 m..s
   59 	    41 	 0.05159 	 0.00088 	 m..s
   26 	    42 	 0.04982 	 0.00091 	 m..s
   40 	    43 	 0.05063 	 0.00092 	 m..s
   71 	    44 	 0.05231 	 0.00101 	 m..s
   83 	    45 	 0.05443 	 0.00103 	 m..s
   52 	    46 	 0.05115 	 0.00104 	 m..s
    7 	    47 	 0.04736 	 0.00106 	 m..s
   55 	    48 	 0.05126 	 0.00108 	 m..s
   51 	    49 	 0.05113 	 0.00112 	 m..s
   36 	    50 	 0.05025 	 0.00115 	 m..s
   34 	    51 	 0.05018 	 0.00116 	 m..s
   28 	    52 	 0.04990 	 0.00116 	 m..s
   58 	    53 	 0.05158 	 0.00118 	 m..s
   23 	    54 	 0.04927 	 0.00120 	 m..s
   39 	    55 	 0.05058 	 0.00122 	 m..s
   65 	    56 	 0.05189 	 0.00123 	 m..s
   47 	    57 	 0.05089 	 0.00133 	 m..s
   73 	    58 	 0.05245 	 0.00136 	 m..s
   72 	    59 	 0.05236 	 0.00137 	 m..s
   37 	    60 	 0.05047 	 0.00139 	 m..s
    0 	    61 	 0.04384 	 0.00142 	 m..s
   63 	    62 	 0.05179 	 0.00142 	 m..s
   24 	    63 	 0.04929 	 0.00143 	 m..s
   31 	    64 	 0.05008 	 0.00146 	 m..s
   53 	    65 	 0.05117 	 0.00153 	 m..s
    6 	    66 	 0.04727 	 0.00156 	 m..s
   38 	    67 	 0.05056 	 0.00161 	 m..s
    9 	    68 	 0.04760 	 0.00170 	 m..s
   32 	    69 	 0.05009 	 0.00186 	 m..s
    2 	    70 	 0.04586 	 0.00194 	 m..s
   19 	    71 	 0.04881 	 0.00195 	 m..s
   68 	    72 	 0.05197 	 0.00196 	 m..s
   44 	    73 	 0.05078 	 0.00199 	 m..s
   22 	    74 	 0.04922 	 0.00200 	 m..s
   45 	    75 	 0.05081 	 0.00200 	 m..s
   21 	    76 	 0.04906 	 0.00224 	 m..s
   76 	    77 	 0.05290 	 0.00249 	 m..s
   78 	    78 	 0.05307 	 0.00459 	 m..s
   48 	    79 	 0.05099 	 0.04959 	 ~...
   50 	    80 	 0.05110 	 0.05253 	 ~...
   85 	    81 	 0.06167 	 0.06381 	 ~...
   86 	    82 	 0.06168 	 0.07113 	 ~...
   90 	    83 	 0.06272 	 0.07360 	 ~...
   69 	    84 	 0.05218 	 0.07464 	 ~...
   97 	    85 	 0.08185 	 0.07469 	 ~...
   92 	    86 	 0.06747 	 0.07542 	 ~...
   89 	    87 	 0.06245 	 0.07636 	 ~...
   66 	    88 	 0.05193 	 0.07648 	 ~...
   95 	    89 	 0.06908 	 0.07650 	 ~...
   67 	    90 	 0.05194 	 0.07667 	 ~...
   84 	    91 	 0.06122 	 0.07719 	 ~...
   70 	    92 	 0.05227 	 0.07971 	 ~...
  105 	    93 	 0.15651 	 0.08036 	 m..s
   94 	    94 	 0.06860 	 0.08403 	 ~...
   93 	    95 	 0.06834 	 0.08523 	 ~...
   88 	    96 	 0.06191 	 0.08922 	 ~...
   91 	    97 	 0.06327 	 0.08956 	 ~...
  101 	    98 	 0.12509 	 0.09360 	 m..s
   97 	    99 	 0.08185 	 0.09838 	 ~...
  104 	   100 	 0.15516 	 0.10039 	 m..s
  102 	   101 	 0.13944 	 0.11871 	 ~...
   96 	   102 	 0.07694 	 0.12857 	 m..s
  103 	   103 	 0.14609 	 0.12860 	 ~...
  110 	   104 	 0.16774 	 0.13637 	 m..s
  106 	   105 	 0.15859 	 0.13818 	 ~...
  117 	   106 	 0.20781 	 0.17068 	 m..s
   97 	   107 	 0.08185 	 0.17901 	 m..s
  109 	   108 	 0.16647 	 0.18394 	 ~...
  108 	   109 	 0.16428 	 0.18588 	 ~...
  107 	   110 	 0.16290 	 0.18649 	 ~...
  114 	   111 	 0.18708 	 0.21233 	 ~...
  116 	   112 	 0.19689 	 0.21589 	 ~...
  100 	   113 	 0.12287 	 0.22307 	 MISS
  113 	   114 	 0.18246 	 0.27717 	 m..s
  115 	   115 	 0.18866 	 0.28122 	 m..s
  112 	   116 	 0.17368 	 0.28885 	 MISS
  111 	   117 	 0.16922 	 0.29363 	 MISS
  118 	   118 	 0.24342 	 0.31774 	 m..s
  119 	   119 	 0.26702 	 0.34668 	 m..s
  120 	   120 	 0.27137 	 0.34919 	 m..s
==========================================
r_mrr = 0.9191415905952454
r2_mrr = 0.660223126411438
spearmanr_mrr@5 = 0.9898920059204102
spearmanr_mrr@10 = 0.9197443127632141
spearmanr_mrr@50 = 0.9554168581962585
spearmanr_mrr@100 = 0.9618377089500427
spearmanr_mrr@All = 0.9641096591949463
==========================================
test time: 0.626
Done Testing dataset OpenEA
total time taken: 1212.336342573166
training time taken: 1148.569215297699
TWIG out ;))
