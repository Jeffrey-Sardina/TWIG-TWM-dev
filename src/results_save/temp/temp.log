Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 7.86493e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.479 +- 0.007
mrr vals (pred, true): 0.015, 0.026
batch losses (mrrl, rdl): 0.0, 3.26328e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.479 +- 0.209
mrr vals (pred, true): 0.017, 0.050
batch losses (mrrl, rdl): 0.0, 1.93526e-05

Epoch over!
epoch time: 18.035

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.196 +- 0.274
mrr vals (pred, true): 0.084, 0.252
batch losses (mrrl, rdl): 0.0, 4.60789e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.543 +- 0.258
mrr vals (pred, true): 0.031, 0.026
batch losses (mrrl, rdl): 0.0, 2.13303e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.160 +- 0.184
mrr vals (pred, true): 0.120, 0.192
batch losses (mrrl, rdl): 0.0, 1.91073e-05

Epoch over!
epoch time: 18.35

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.441 +- 0.262
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.0019426843, 2.3502e-06

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.068 +- 0.043
mrr vals (pred, true): 0.259, 0.258
batch losses (mrrl, rdl): 2.7625e-06, 0.0003999973

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.563 +- 0.191
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 9.4001e-05, 0.0006810604

Epoch over!
epoch time: 17.765

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.377 +- 0.119
mrr vals (pred, true): 0.059, 0.048
batch losses (mrrl, rdl): 0.0007931835, 0.0001215411

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.397 +- 0.111
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 1.96046e-05, 8.38615e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.534 +- 0.167
mrr vals (pred, true): 0.051, 0.025
batch losses (mrrl, rdl): 1.44852e-05, 1.06022e-05

Epoch over!
epoch time: 19.243

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.550 +- 0.165
mrr vals (pred, true): 0.047, 0.024
batch losses (mrrl, rdl): 6.27275e-05, 4.11143e-05

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.414 +- 0.104
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001400857, 6.57392e-05

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.407 +- 0.113
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002535717, 7.26076e-05

Epoch over!
epoch time: 18.293

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.515 +- 0.152
mrr vals (pred, true): 0.049, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   30 	     0 	 0.04867 	 0.01773 	 m..s
   78 	     1 	 0.05243 	 0.02009 	 m..s
   25 	     2 	 0.04842 	 0.02164 	 ~...
   73 	     3 	 0.05144 	 0.02192 	 ~...
   79 	     4 	 0.05246 	 0.02235 	 m..s
   58 	     5 	 0.05054 	 0.02398 	 ~...
   80 	     6 	 0.05263 	 0.02414 	 ~...
   47 	     7 	 0.04987 	 0.02414 	 ~...
    7 	     8 	 0.04725 	 0.02423 	 ~...
   62 	     9 	 0.05071 	 0.02467 	 ~...
   89 	    10 	 0.05628 	 0.02475 	 m..s
   34 	    11 	 0.04871 	 0.02496 	 ~...
   87 	    12 	 0.05391 	 0.02544 	 ~...
    0 	    13 	 0.04583 	 0.02586 	 ~...
   40 	    14 	 0.04917 	 0.02933 	 ~...
    2 	    15 	 0.04675 	 0.03159 	 ~...
   66 	    16 	 0.05099 	 0.03197 	 ~...
   53 	    17 	 0.05026 	 0.03458 	 ~...
   63 	    18 	 0.05072 	 0.03549 	 ~...
   61 	    19 	 0.05068 	 0.03582 	 ~...
   45 	    20 	 0.04956 	 0.03732 	 ~...
   68 	    21 	 0.05110 	 0.03811 	 ~...
   77 	    22 	 0.05208 	 0.03856 	 ~...
   37 	    23 	 0.04882 	 0.03931 	 ~...
   85 	    24 	 0.05316 	 0.03957 	 ~...
   13 	    25 	 0.04779 	 0.03980 	 ~...
   10 	    26 	 0.04747 	 0.04032 	 ~...
    9 	    27 	 0.04746 	 0.04070 	 ~...
   75 	    28 	 0.05157 	 0.04080 	 ~...
   12 	    29 	 0.04775 	 0.04099 	 ~...
   27 	    30 	 0.04843 	 0.04129 	 ~...
   50 	    31 	 0.04997 	 0.04130 	 ~...
   69 	    32 	 0.05125 	 0.04163 	 ~...
   86 	    33 	 0.05317 	 0.04165 	 ~...
    8 	    34 	 0.04745 	 0.04176 	 ~...
   55 	    35 	 0.05032 	 0.04224 	 ~...
   20 	    36 	 0.04817 	 0.04272 	 ~...
   35 	    37 	 0.04875 	 0.04276 	 ~...
   88 	    38 	 0.05424 	 0.04306 	 ~...
    3 	    39 	 0.04692 	 0.04338 	 ~...
   23 	    40 	 0.04832 	 0.04372 	 ~...
   84 	    41 	 0.05293 	 0.04373 	 ~...
   51 	    42 	 0.05006 	 0.04388 	 ~...
   31 	    43 	 0.04868 	 0.04410 	 ~...
   59 	    44 	 0.05055 	 0.04419 	 ~...
   56 	    45 	 0.05036 	 0.04420 	 ~...
   76 	    46 	 0.05202 	 0.04423 	 ~...
   33 	    47 	 0.04870 	 0.04476 	 ~...
   44 	    48 	 0.04956 	 0.04514 	 ~...
   48 	    49 	 0.04987 	 0.04551 	 ~...
   19 	    50 	 0.04803 	 0.04564 	 ~...
   41 	    51 	 0.04925 	 0.04617 	 ~...
   39 	    52 	 0.04906 	 0.04642 	 ~...
   29 	    53 	 0.04848 	 0.04687 	 ~...
   49 	    54 	 0.04988 	 0.04699 	 ~...
   52 	    55 	 0.05011 	 0.04706 	 ~...
   42 	    56 	 0.04952 	 0.04718 	 ~...
   67 	    57 	 0.05102 	 0.04727 	 ~...
   26 	    58 	 0.04843 	 0.04740 	 ~...
   28 	    59 	 0.04847 	 0.04744 	 ~...
   15 	    60 	 0.04793 	 0.04750 	 ~...
   46 	    61 	 0.04971 	 0.04758 	 ~...
   18 	    62 	 0.04800 	 0.04763 	 ~...
   32 	    63 	 0.04869 	 0.04824 	 ~...
   70 	    64 	 0.05131 	 0.04857 	 ~...
   24 	    65 	 0.04834 	 0.04879 	 ~...
   60 	    66 	 0.05065 	 0.04879 	 ~...
   71 	    67 	 0.05135 	 0.04904 	 ~...
   81 	    68 	 0.05265 	 0.04921 	 ~...
   54 	    69 	 0.05027 	 0.04928 	 ~...
   36 	    70 	 0.04876 	 0.04929 	 ~...
   17 	    71 	 0.04797 	 0.04949 	 ~...
    4 	    72 	 0.04692 	 0.04956 	 ~...
   72 	    73 	 0.05143 	 0.04987 	 ~...
    5 	    74 	 0.04698 	 0.05004 	 ~...
   74 	    75 	 0.05147 	 0.05023 	 ~...
    6 	    76 	 0.04709 	 0.05056 	 ~...
   83 	    77 	 0.05292 	 0.05108 	 ~...
   82 	    78 	 0.05291 	 0.05128 	 ~...
    1 	    79 	 0.04650 	 0.05129 	 ~...
   11 	    80 	 0.04757 	 0.05144 	 ~...
   21 	    81 	 0.04819 	 0.05146 	 ~...
   64 	    82 	 0.05088 	 0.05160 	 ~...
   22 	    83 	 0.04828 	 0.05189 	 ~...
   65 	    84 	 0.05094 	 0.05246 	 ~...
   14 	    85 	 0.04791 	 0.05275 	 ~...
   57 	    86 	 0.05051 	 0.05352 	 ~...
   38 	    87 	 0.04897 	 0.05355 	 ~...
   43 	    88 	 0.04952 	 0.05567 	 ~...
   16 	    89 	 0.04795 	 0.05977 	 ~...
   96 	    90 	 0.21291 	 0.18889 	 ~...
   93 	    91 	 0.20711 	 0.18959 	 ~...
   90 	    92 	 0.20315 	 0.19213 	 ~...
   94 	    93 	 0.20842 	 0.19872 	 ~...
   95 	    94 	 0.21032 	 0.20550 	 ~...
   91 	    95 	 0.20319 	 0.20694 	 ~...
   97 	    96 	 0.21501 	 0.20727 	 ~...
   98 	    97 	 0.21519 	 0.21693 	 ~...
   99 	    98 	 0.21523 	 0.21723 	 ~...
  100 	    99 	 0.21799 	 0.21940 	 ~...
   92 	   100 	 0.20325 	 0.21956 	 ~...
  103 	   101 	 0.22439 	 0.23290 	 ~...
  102 	   102 	 0.22004 	 0.23984 	 ~...
  106 	   103 	 0.23688 	 0.24107 	 ~...
  109 	   104 	 0.24530 	 0.24218 	 ~...
  104 	   105 	 0.22894 	 0.24307 	 ~...
  101 	   106 	 0.21864 	 0.25298 	 m..s
  105 	   107 	 0.23357 	 0.25327 	 ~...
  110 	   108 	 0.25117 	 0.27378 	 ~...
  107 	   109 	 0.23775 	 0.27425 	 m..s
  114 	   110 	 0.28840 	 0.27441 	 ~...
  113 	   111 	 0.27190 	 0.27737 	 ~...
  115 	   112 	 0.28861 	 0.28034 	 ~...
  108 	   113 	 0.24327 	 0.28353 	 m..s
  116 	   114 	 0.29581 	 0.29788 	 ~...
  112 	   115 	 0.25920 	 0.29922 	 m..s
  117 	   116 	 0.31025 	 0.31748 	 ~...
  111 	   117 	 0.25340 	 0.33485 	 m..s
  120 	   118 	 0.32537 	 0.39525 	 m..s
  119 	   119 	 0.32034 	 0.52223 	 MISS
  118 	   120 	 0.31070 	 0.53924 	 MISS
==========================================
r_mrr = 0.9665548801422119
r2_mrr = 0.9087692499160767
spearmanr_mrr@5 = 0.9109581708908081
spearmanr_mrr@10 = 0.8267022371292114
spearmanr_mrr@50 = 0.9543513059616089
spearmanr_mrr@100 = 0.9724559783935547
spearmanr_mrr@All = 0.9739253520965576
==========================================
test time: 0.662
Done Testing dataset UMLS
total time taken: 101.3926031589508
training time taken: 92.47690320014954
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_Kinships
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 702
rank avg (pred): 0.431 +- 0.011
mrr vals (pred, true): 0.022, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001069047

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 1140
rank avg (pred): 0.116 +- 0.073
mrr vals (pred, true): 0.147, 0.319
batch losses (mrrl, rdl): 0.0, 9.5271e-06

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 403
rank avg (pred): 0.470 +- 0.265
mrr vals (pred, true): 0.037, 0.050
batch losses (mrrl, rdl): 0.0, 7.6142e-06

Epoch over!
epoch time: 17.79

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 272
rank avg (pred): 0.102 +- 0.086
mrr vals (pred, true): 0.173, 0.332
batch losses (mrrl, rdl): 0.0, 2.1776e-06

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 525
rank avg (pred): 0.147 +- 0.135
mrr vals (pred, true): 0.130, 0.211
batch losses (mrrl, rdl): 0.0, 5.007e-06

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 306
rank avg (pred): 0.111 +- 0.134
mrr vals (pred, true): 0.186, 0.293
batch losses (mrrl, rdl): 0.0, 8.3684e-06

Epoch over!
epoch time: 17.863

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 460
rank avg (pred): 0.475 +- 0.271
mrr vals (pred, true): 0.037, 0.052
batch losses (mrrl, rdl): 0.0017686507, 4.1333e-06

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 1079
rank avg (pred): 0.027 +- 0.015
mrr vals (pred, true): 0.365, 0.482
batch losses (mrrl, rdl): 0.1374141872, 5.41139e-05

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 899
rank avg (pred): 0.047 +- 0.028
mrr vals (pred, true): 0.289, 0.084
batch losses (mrrl, rdl): 0.5727787018, 0.0016062419

Epoch over!
epoch time: 17.071

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 658
rank avg (pred): 0.321 +- 0.097
mrr vals (pred, true): 0.042, 0.045
batch losses (mrrl, rdl): 0.0006653246, 0.0005847432

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 233
rank avg (pred): 0.306 +- 0.118
mrr vals (pred, true): 0.066, 0.047
batch losses (mrrl, rdl): 0.0027129613, 0.0006670714

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 562
rank avg (pred): 0.154 +- 0.095
mrr vals (pred, true): 0.195, 0.202
batch losses (mrrl, rdl): 0.0004368883, 1.6245e-05

Epoch over!
epoch time: 17.025

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 501
rank avg (pred): 0.106 +- 0.064
mrr vals (pred, true): 0.223, 0.224
batch losses (mrrl, rdl): 2.29762e-05, 5.06365e-05

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 881
rank avg (pred): 0.340 +- 0.098
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 3.12273e-05, 0.0003616691

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 1045
rank avg (pred): 0.331 +- 0.113
mrr vals (pred, true): 0.061, 0.056
batch losses (mrrl, rdl): 0.0013095743, 0.000378567

Epoch over!
epoch time: 17.639

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.093 +- 0.056
mrr vals (pred, true): 0.242, 0.228

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.03692 	 0.04713 	 ~...
   42 	     1 	 0.03962 	 0.04727 	 ~...
   12 	     2 	 0.03745 	 0.04763 	 ~...
   14 	     3 	 0.03750 	 0.04803 	 ~...
   51 	     4 	 0.04024 	 0.04830 	 ~...
   35 	     5 	 0.03905 	 0.04840 	 ~...
   39 	     6 	 0.03938 	 0.04863 	 ~...
   22 	     7 	 0.03837 	 0.04890 	 ~...
   46 	     8 	 0.03993 	 0.04927 	 ~...
   71 	     9 	 0.04255 	 0.04930 	 ~...
   57 	    10 	 0.04062 	 0.04948 	 ~...
   68 	    11 	 0.04188 	 0.04950 	 ~...
   58 	    12 	 0.04081 	 0.04954 	 ~...
   21 	    13 	 0.03828 	 0.04990 	 ~...
   54 	    14 	 0.04045 	 0.05021 	 ~...
   28 	    15 	 0.03852 	 0.05035 	 ~...
   26 	    16 	 0.03844 	 0.05050 	 ~...
   52 	    17 	 0.04038 	 0.05071 	 ~...
   61 	    18 	 0.04109 	 0.05076 	 ~...
   40 	    19 	 0.03948 	 0.05097 	 ~...
   67 	    20 	 0.04181 	 0.05105 	 ~...
    5 	    21 	 0.03676 	 0.05107 	 ~...
   36 	    22 	 0.03907 	 0.05112 	 ~...
   10 	    23 	 0.03711 	 0.05113 	 ~...
    7 	    24 	 0.03681 	 0.05127 	 ~...
   60 	    25 	 0.04093 	 0.05151 	 ~...
   55 	    26 	 0.04047 	 0.05154 	 ~...
   13 	    27 	 0.03749 	 0.05179 	 ~...
   47 	    28 	 0.03993 	 0.05186 	 ~...
   20 	    29 	 0.03813 	 0.05193 	 ~...
   16 	    30 	 0.03754 	 0.05211 	 ~...
   74 	    31 	 0.04337 	 0.05215 	 ~...
   63 	    32 	 0.04121 	 0.05219 	 ~...
   66 	    33 	 0.04161 	 0.05236 	 ~...
   32 	    34 	 0.03880 	 0.05312 	 ~...
   33 	    35 	 0.03884 	 0.05317 	 ~...
   69 	    36 	 0.04197 	 0.05317 	 ~...
   43 	    37 	 0.03984 	 0.05341 	 ~...
    8 	    38 	 0.03683 	 0.05350 	 ~...
    4 	    39 	 0.03671 	 0.05351 	 ~...
   23 	    40 	 0.03838 	 0.05358 	 ~...
   70 	    41 	 0.04212 	 0.05406 	 ~...
   73 	    42 	 0.04332 	 0.05428 	 ~...
   25 	    43 	 0.03840 	 0.05440 	 ~...
   27 	    44 	 0.03847 	 0.05481 	 ~...
   19 	    45 	 0.03774 	 0.05498 	 ~...
   38 	    46 	 0.03934 	 0.05510 	 ~...
   65 	    47 	 0.04148 	 0.05531 	 ~...
   30 	    48 	 0.03869 	 0.05575 	 ~...
   44 	    49 	 0.03986 	 0.05576 	 ~...
    2 	    50 	 0.03619 	 0.05587 	 ~...
   18 	    51 	 0.03769 	 0.05596 	 ~...
   53 	    52 	 0.04043 	 0.05638 	 ~...
    1 	    53 	 0.03602 	 0.05694 	 ~...
   64 	    54 	 0.04146 	 0.05696 	 ~...
    0 	    55 	 0.03569 	 0.05719 	 ~...
   37 	    56 	 0.03908 	 0.05723 	 ~...
   48 	    57 	 0.03997 	 0.05725 	 ~...
   45 	    58 	 0.03990 	 0.05733 	 ~...
   72 	    59 	 0.04278 	 0.05749 	 ~...
   50 	    60 	 0.04016 	 0.05787 	 ~...
   62 	    61 	 0.04121 	 0.05825 	 ~...
   49 	    62 	 0.04010 	 0.05868 	 ~...
   17 	    63 	 0.03759 	 0.05870 	 ~...
   75 	    64 	 0.04617 	 0.05874 	 ~...
   41 	    65 	 0.03961 	 0.05881 	 ~...
    3 	    66 	 0.03619 	 0.05882 	 ~...
   56 	    67 	 0.04054 	 0.05918 	 ~...
   15 	    68 	 0.03750 	 0.05969 	 ~...
   29 	    69 	 0.03852 	 0.05993 	 ~...
   31 	    70 	 0.03872 	 0.06014 	 ~...
   24 	    71 	 0.03839 	 0.06038 	 ~...
    6 	    72 	 0.03681 	 0.06049 	 ~...
   11 	    73 	 0.03736 	 0.06096 	 ~...
   34 	    74 	 0.03894 	 0.06158 	 ~...
   59 	    75 	 0.04091 	 0.06229 	 ~...
   83 	    76 	 0.20413 	 0.16198 	 m..s
   76 	    77 	 0.16928 	 0.18404 	 ~...
   80 	    78 	 0.18892 	 0.18970 	 ~...
   76 	    79 	 0.16928 	 0.19886 	 ~...
   81 	    80 	 0.19227 	 0.21213 	 ~...
   78 	    81 	 0.17533 	 0.21714 	 m..s
   79 	    82 	 0.18462 	 0.21822 	 m..s
   84 	    83 	 0.20635 	 0.21861 	 ~...
   82 	    84 	 0.19709 	 0.22784 	 m..s
   88 	    85 	 0.24246 	 0.22825 	 ~...
   95 	    86 	 0.26666 	 0.25404 	 ~...
   87 	    87 	 0.23911 	 0.26205 	 ~...
   85 	    88 	 0.22278 	 0.26840 	 m..s
   86 	    89 	 0.23760 	 0.27182 	 m..s
   98 	    90 	 0.27497 	 0.28071 	 ~...
   97 	    91 	 0.27384 	 0.28148 	 ~...
  101 	    92 	 0.28527 	 0.28774 	 ~...
   91 	    93 	 0.24429 	 0.29619 	 m..s
   99 	    94 	 0.27524 	 0.29634 	 ~...
   92 	    95 	 0.24961 	 0.29652 	 m..s
   94 	    96 	 0.26644 	 0.30510 	 m..s
  105 	    97 	 0.31085 	 0.30598 	 ~...
   96 	    98 	 0.27035 	 0.30754 	 m..s
  107 	    99 	 0.31889 	 0.30803 	 ~...
   90 	   100 	 0.24412 	 0.30884 	 m..s
   93 	   101 	 0.25267 	 0.31529 	 m..s
   89 	   102 	 0.24391 	 0.31655 	 m..s
  103 	   103 	 0.29698 	 0.32286 	 ~...
  100 	   104 	 0.28087 	 0.33122 	 m..s
  104 	   105 	 0.29720 	 0.34296 	 m..s
  102 	   106 	 0.28623 	 0.36221 	 m..s
  108 	   107 	 0.32610 	 0.36880 	 m..s
  106 	   108 	 0.31429 	 0.36975 	 m..s
  109 	   109 	 0.34720 	 0.37989 	 m..s
  111 	   110 	 0.36035 	 0.38991 	 ~...
  115 	   111 	 0.39827 	 0.40258 	 ~...
  119 	   112 	 0.41907 	 0.40967 	 ~...
  110 	   113 	 0.36003 	 0.41075 	 m..s
  112 	   114 	 0.37412 	 0.42112 	 m..s
  113 	   115 	 0.38592 	 0.47933 	 m..s
  114 	   116 	 0.39325 	 0.48000 	 m..s
  120 	   117 	 0.43004 	 0.50285 	 m..s
  117 	   118 	 0.40880 	 0.61365 	 MISS
  118 	   119 	 0.41427 	 0.61786 	 MISS
  116 	   120 	 0.40154 	 0.62288 	 MISS
==========================================
r_mrr = 0.9795836806297302
r2_mrr = 0.9158247709274292
spearmanr_mrr@5 = 0.8584182858467102
spearmanr_mrr@10 = 0.8977420926094055
spearmanr_mrr@50 = 0.9655237793922424
spearmanr_mrr@100 = 0.9861077666282654
spearmanr_mrr@All = 0.9876053333282471
==========================================
test time: 0.651
Done Testing dataset Kinships
total time taken: 98.59531354904175
training time taken: 88.15583062171936
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_CoDExSmall
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 702
rank avg (pred): 0.432 +- 0.010
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001451381

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 1140
rank avg (pred): 0.184 +- 0.108
mrr vals (pred, true): 0.138, 0.035
batch losses (mrrl, rdl): 0.0, 3.37792e-05

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 403
rank avg (pred): 0.446 +- 0.273
mrr vals (pred, true): 0.116, 0.005
batch losses (mrrl, rdl): 0.0, 7.1491e-06

Epoch over!
epoch time: 17.643

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 272
rank avg (pred): 0.046 +- 0.033
mrr vals (pred, true): 0.167, 0.205
batch losses (mrrl, rdl): 0.0, 2.47302e-05

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.441 +- 0.283
mrr vals (pred, true): 0.055, 0.008
batch losses (mrrl, rdl): 0.0, 1.82142e-05

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 306
rank avg (pred): 0.065 +- 0.083
mrr vals (pred, true): 0.114, 0.182
batch losses (mrrl, rdl): 0.0, 5.8702e-06

Epoch over!
epoch time: 17.302

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 460
rank avg (pred): 0.462 +- 0.276
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.00019303, 1.9481e-06

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 1079
rank avg (pred): 0.013 +- 0.008
mrr vals (pred, true): 0.211, 0.289
batch losses (mrrl, rdl): 0.059955515, 2.36971e-05

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 899
rank avg (pred): 0.417 +- 0.198
mrr vals (pred, true): 0.075, 0.001
batch losses (mrrl, rdl): 0.006376693, 0.0003253542

Epoch over!
epoch time: 16.76

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 658
rank avg (pred): 0.508 +- 0.216
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001677549, 6.13774e-05

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 233
rank avg (pred): 0.450 +- 0.188
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 3.7773e-05, 4.05028e-05

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 562
rank avg (pred): 0.410 +- 0.181
mrr vals (pred, true): 0.052, 0.007
batch losses (mrrl, rdl): 5.48977e-05, 1.70549e-05

Epoch over!
epoch time: 17.418

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 501
rank avg (pred): 0.422 +- 0.202
mrr vals (pred, true): 0.063, 0.044
batch losses (mrrl, rdl): 0.0018002314, 0.0004027101

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 881
rank avg (pred): 0.490 +- 0.220
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 2.4872e-06, 3.15371e-05

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 1045
rank avg (pred): 0.439 +- 0.199
mrr vals (pred, true): 0.043, 0.005
batch losses (mrrl, rdl): 0.0004654987, 4.53288e-05

Epoch over!
epoch time: 17.641

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.488 +- 0.229
mrr vals (pred, true): 0.045, 0.001

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   83 	     0 	 0.04688 	 0.00086 	 m..s
   74 	     1 	 0.04459 	 0.00089 	 m..s
   44 	     2 	 0.03893 	 0.00170 	 m..s
   78 	     3 	 0.04534 	 0.00174 	 m..s
   72 	     4 	 0.04356 	 0.00204 	 m..s
   52 	     5 	 0.04072 	 0.00240 	 m..s
   64 	     6 	 0.04213 	 0.00242 	 m..s
   62 	     7 	 0.04177 	 0.00242 	 m..s
   59 	     8 	 0.04128 	 0.00287 	 m..s
   24 	     9 	 0.03730 	 0.00289 	 m..s
   45 	    10 	 0.03917 	 0.00291 	 m..s
    8 	    11 	 0.03484 	 0.00292 	 m..s
   47 	    12 	 0.03953 	 0.00296 	 m..s
   87 	    13 	 0.04740 	 0.00310 	 m..s
   69 	    14 	 0.04310 	 0.00315 	 m..s
   79 	    15 	 0.04537 	 0.00316 	 m..s
   49 	    16 	 0.04022 	 0.00317 	 m..s
   13 	    17 	 0.03554 	 0.00325 	 m..s
   20 	    18 	 0.03587 	 0.00331 	 m..s
   31 	    19 	 0.03763 	 0.00331 	 m..s
   29 	    20 	 0.03748 	 0.00336 	 m..s
    4 	    21 	 0.03424 	 0.00337 	 m..s
   33 	    22 	 0.03770 	 0.00345 	 m..s
    5 	    23 	 0.03433 	 0.00345 	 m..s
    2 	    24 	 0.03320 	 0.00356 	 ~...
   35 	    25 	 0.03792 	 0.00360 	 m..s
   36 	    26 	 0.03811 	 0.00362 	 m..s
   17 	    27 	 0.03566 	 0.00362 	 m..s
   42 	    28 	 0.03883 	 0.00363 	 m..s
   18 	    29 	 0.03580 	 0.00363 	 m..s
   82 	    30 	 0.04628 	 0.00364 	 m..s
   63 	    31 	 0.04200 	 0.00368 	 m..s
   55 	    32 	 0.04083 	 0.00368 	 m..s
   15 	    33 	 0.03559 	 0.00374 	 m..s
   77 	    34 	 0.04513 	 0.00376 	 m..s
   32 	    35 	 0.03766 	 0.00376 	 m..s
   81 	    36 	 0.04586 	 0.00381 	 m..s
   71 	    37 	 0.04354 	 0.00381 	 m..s
   88 	    38 	 0.04753 	 0.00384 	 m..s
   41 	    39 	 0.03860 	 0.00389 	 m..s
   48 	    40 	 0.03970 	 0.00391 	 m..s
    1 	    41 	 0.03264 	 0.00394 	 ~...
   30 	    42 	 0.03756 	 0.00399 	 m..s
   39 	    43 	 0.03828 	 0.00400 	 m..s
   23 	    44 	 0.03707 	 0.00402 	 m..s
   10 	    45 	 0.03523 	 0.00404 	 m..s
   40 	    46 	 0.03840 	 0.00406 	 m..s
    6 	    47 	 0.03437 	 0.00409 	 m..s
   11 	    48 	 0.03532 	 0.00412 	 m..s
   16 	    49 	 0.03566 	 0.00413 	 m..s
   22 	    50 	 0.03660 	 0.00415 	 m..s
    9 	    51 	 0.03496 	 0.00415 	 m..s
   61 	    52 	 0.04163 	 0.00415 	 m..s
   73 	    53 	 0.04400 	 0.00415 	 m..s
   28 	    54 	 0.03745 	 0.00416 	 m..s
   70 	    55 	 0.04324 	 0.00418 	 m..s
   25 	    56 	 0.03741 	 0.00418 	 m..s
   57 	    57 	 0.04109 	 0.00420 	 m..s
   14 	    58 	 0.03555 	 0.00422 	 m..s
   56 	    59 	 0.04085 	 0.00424 	 m..s
   43 	    60 	 0.03893 	 0.00440 	 m..s
   12 	    61 	 0.03546 	 0.00454 	 m..s
   21 	    62 	 0.03622 	 0.00455 	 m..s
   68 	    63 	 0.04236 	 0.00459 	 m..s
    7 	    64 	 0.03450 	 0.00460 	 ~...
   85 	    65 	 0.04737 	 0.00461 	 m..s
   37 	    66 	 0.03812 	 0.00461 	 m..s
   66 	    67 	 0.04225 	 0.00467 	 m..s
   38 	    68 	 0.03814 	 0.00469 	 m..s
   27 	    69 	 0.03745 	 0.00470 	 m..s
   67 	    70 	 0.04227 	 0.00473 	 m..s
   26 	    71 	 0.03744 	 0.00477 	 m..s
   54 	    72 	 0.04082 	 0.00478 	 m..s
    3 	    73 	 0.03382 	 0.00485 	 ~...
    0 	    74 	 0.03240 	 0.00500 	 ~...
   19 	    75 	 0.03586 	 0.00501 	 m..s
   76 	    76 	 0.04503 	 0.00514 	 m..s
   34 	    77 	 0.03788 	 0.00551 	 m..s
   65 	    78 	 0.04214 	 0.00604 	 m..s
   60 	    79 	 0.04150 	 0.00680 	 m..s
   50 	    80 	 0.04052 	 0.00694 	 m..s
   58 	    81 	 0.04119 	 0.00739 	 m..s
   51 	    82 	 0.04060 	 0.00804 	 m..s
   46 	    83 	 0.03921 	 0.01152 	 ~...
   53 	    84 	 0.04080 	 0.01271 	 ~...
   75 	    85 	 0.04469 	 0.02050 	 ~...
   89 	    86 	 0.05162 	 0.02430 	 ~...
   84 	    87 	 0.04711 	 0.02863 	 ~...
   80 	    88 	 0.04561 	 0.02916 	 ~...
   86 	    89 	 0.04739 	 0.02920 	 ~...
   94 	    90 	 0.14410 	 0.12715 	 ~...
   92 	    91 	 0.12728 	 0.12799 	 ~...
   95 	    92 	 0.14525 	 0.13094 	 ~...
   91 	    93 	 0.12433 	 0.13140 	 ~...
   93 	    94 	 0.13654 	 0.13806 	 ~...
   90 	    95 	 0.11324 	 0.14041 	 ~...
   98 	    96 	 0.17006 	 0.17202 	 ~...
  104 	    97 	 0.18697 	 0.17271 	 ~...
   97 	    98 	 0.16779 	 0.17434 	 ~...
  100 	    99 	 0.17342 	 0.17792 	 ~...
  102 	   100 	 0.18541 	 0.18551 	 ~...
   96 	   101 	 0.16104 	 0.18924 	 ~...
   99 	   102 	 0.17302 	 0.19630 	 ~...
  103 	   103 	 0.18695 	 0.20171 	 ~...
  101 	   104 	 0.17353 	 0.21048 	 m..s
  108 	   105 	 0.21496 	 0.22857 	 ~...
  109 	   106 	 0.21536 	 0.22949 	 ~...
  110 	   107 	 0.23708 	 0.23928 	 ~...
  116 	   108 	 0.26044 	 0.24645 	 ~...
  113 	   109 	 0.25626 	 0.25269 	 ~...
  105 	   110 	 0.19747 	 0.25972 	 m..s
  117 	   111 	 0.26444 	 0.26879 	 ~...
  107 	   112 	 0.20556 	 0.27244 	 m..s
  114 	   113 	 0.25696 	 0.28454 	 ~...
  106 	   114 	 0.20287 	 0.29618 	 m..s
  120 	   115 	 0.29039 	 0.30562 	 ~...
  118 	   116 	 0.27236 	 0.31181 	 m..s
  112 	   117 	 0.25582 	 0.31362 	 m..s
  111 	   118 	 0.24535 	 0.31910 	 m..s
  119 	   119 	 0.28619 	 0.33494 	 m..s
  115 	   120 	 0.25934 	 0.34039 	 m..s
==========================================
r_mrr = 0.9882873892784119
r2_mrr = 0.8789796829223633
spearmanr_mrr@5 = 0.9916630983352661
spearmanr_mrr@10 = 0.9188467860221863
spearmanr_mrr@50 = 0.9973345994949341
spearmanr_mrr@100 = 0.9983500242233276
spearmanr_mrr@All = 0.9983133673667908
==========================================
test time: 0.656
Done Testing dataset CoDExSmall
total time taken: 100.06964468955994
training time taken: 87.52861285209656
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_DBpedia50
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 702
rank avg (pred): 0.428 +- 0.010
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002126066

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 1140
rank avg (pred): 0.363 +- 0.299
mrr vals (pred, true): 0.000, 0.184
batch losses (mrrl, rdl): 0.0, 5.04566e-05

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 403
rank avg (pred): 0.529 +- 0.263
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 6.18873e-05

Epoch over!
epoch time: 18.412

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 272
rank avg (pred): 0.334 +- 0.312
mrr vals (pred, true): 0.000, 0.111
batch losses (mrrl, rdl): 0.0, 1.38707e-05

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 525
rank avg (pred): 0.451 +- 0.315
mrr vals (pred, true): 0.000, 0.098
batch losses (mrrl, rdl): 0.0, 2.05403e-05

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 306
rank avg (pred): 0.300 +- 0.308
mrr vals (pred, true): 0.000, 0.123
batch losses (mrrl, rdl): 0.0, 0.0001136469

Epoch over!
epoch time: 17.621

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 460
rank avg (pred): 0.528 +- 0.275
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249055922, 1.03275e-05

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.213 +- 0.386
mrr vals (pred, true): 0.012, 0.129
batch losses (mrrl, rdl): 0.1370741576, 0.0002699293

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 899
rank avg (pred): 0.353 +- 0.455
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.003405442, 0.0029036291

Epoch over!
epoch time: 16.486

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 658
rank avg (pred): 0.486 +- 0.414
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003468048, 0.0001193819

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 233
rank avg (pred): 0.564 +- 0.405
mrr vals (pred, true): 0.030, 0.000
batch losses (mrrl, rdl): 0.003886896, 0.0002388391

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 562
rank avg (pred): 0.338 +- 0.422
mrr vals (pred, true): 0.086, 0.081
batch losses (mrrl, rdl): 0.0126371151, 0.000246537

Epoch over!
epoch time: 16.857

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 501
rank avg (pred): 0.304 +- 0.436
mrr vals (pred, true): 0.106, 0.107
batch losses (mrrl, rdl): 5.8536e-06, 8.71199e-05

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 881
rank avg (pred): 0.493 +- 0.400
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001525742, 6.65747e-05

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 1045
rank avg (pred): 0.545 +- 0.414
mrr vals (pred, true): 0.026, 0.000
batch losses (mrrl, rdl): 0.0057556434, 0.0001337876

Epoch over!
epoch time: 16.609

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.458 +- 0.416
mrr vals (pred, true): 0.043, 0.002

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   22 	     0 	 0.03608 	 0.00014 	 m..s
    1 	     1 	 0.03200 	 0.00014 	 m..s
   27 	     2 	 0.03640 	 0.00014 	 m..s
   95 	     3 	 0.06206 	 0.00015 	 m..s
   33 	     4 	 0.03722 	 0.00016 	 m..s
   46 	     5 	 0.03829 	 0.00016 	 m..s
   29 	     6 	 0.03645 	 0.00017 	 m..s
   45 	     7 	 0.03824 	 0.00017 	 m..s
   57 	     8 	 0.03939 	 0.00018 	 m..s
    7 	     9 	 0.03428 	 0.00018 	 m..s
   70 	    10 	 0.04152 	 0.00018 	 m..s
   32 	    11 	 0.03721 	 0.00019 	 m..s
   41 	    12 	 0.03794 	 0.00019 	 m..s
   55 	    13 	 0.03898 	 0.00021 	 m..s
   24 	    14 	 0.03613 	 0.00021 	 m..s
   78 	    15 	 0.04338 	 0.00021 	 m..s
   83 	    16 	 0.04539 	 0.00021 	 m..s
   19 	    17 	 0.03597 	 0.00021 	 m..s
   51 	    18 	 0.03887 	 0.00021 	 m..s
   63 	    19 	 0.04012 	 0.00022 	 m..s
   81 	    20 	 0.04453 	 0.00022 	 m..s
   49 	    21 	 0.03844 	 0.00023 	 m..s
   17 	    22 	 0.03529 	 0.00023 	 m..s
   53 	    23 	 0.03892 	 0.00024 	 m..s
   43 	    24 	 0.03799 	 0.00024 	 m..s
   73 	    25 	 0.04265 	 0.00024 	 m..s
   44 	    26 	 0.03821 	 0.00024 	 m..s
   13 	    27 	 0.03501 	 0.00024 	 m..s
   37 	    28 	 0.03745 	 0.00024 	 m..s
    6 	    29 	 0.03416 	 0.00025 	 m..s
   56 	    30 	 0.03912 	 0.00025 	 m..s
   61 	    31 	 0.03985 	 0.00025 	 m..s
   35 	    32 	 0.03739 	 0.00026 	 m..s
   14 	    33 	 0.03508 	 0.00026 	 m..s
   58 	    34 	 0.03944 	 0.00027 	 m..s
    2 	    35 	 0.03274 	 0.00027 	 m..s
   59 	    36 	 0.03948 	 0.00028 	 m..s
   71 	    37 	 0.04227 	 0.00028 	 m..s
    4 	    38 	 0.03385 	 0.00028 	 m..s
   38 	    39 	 0.03763 	 0.00029 	 m..s
   79 	    40 	 0.04386 	 0.00029 	 m..s
   20 	    41 	 0.03597 	 0.00029 	 m..s
   21 	    42 	 0.03606 	 0.00030 	 m..s
    9 	    43 	 0.03438 	 0.00030 	 m..s
    3 	    44 	 0.03378 	 0.00030 	 m..s
   11 	    45 	 0.03454 	 0.00031 	 m..s
   82 	    46 	 0.04538 	 0.00031 	 m..s
   40 	    47 	 0.03781 	 0.00032 	 m..s
   23 	    48 	 0.03608 	 0.00032 	 m..s
    8 	    49 	 0.03436 	 0.00033 	 m..s
   50 	    50 	 0.03866 	 0.00034 	 m..s
   34 	    51 	 0.03737 	 0.00034 	 m..s
   39 	    52 	 0.03773 	 0.00034 	 m..s
   72 	    53 	 0.04250 	 0.00035 	 m..s
   60 	    54 	 0.03980 	 0.00036 	 m..s
   31 	    55 	 0.03680 	 0.00037 	 m..s
   15 	    56 	 0.03518 	 0.00037 	 m..s
   26 	    57 	 0.03626 	 0.00038 	 m..s
   30 	    58 	 0.03657 	 0.00038 	 m..s
   18 	    59 	 0.03588 	 0.00039 	 m..s
   52 	    60 	 0.03888 	 0.00039 	 m..s
   62 	    61 	 0.03997 	 0.00039 	 m..s
   42 	    62 	 0.03795 	 0.00039 	 m..s
   36 	    63 	 0.03743 	 0.00043 	 m..s
   66 	    64 	 0.04053 	 0.00043 	 m..s
   28 	    65 	 0.03643 	 0.00046 	 m..s
   54 	    66 	 0.03895 	 0.00049 	 m..s
    5 	    67 	 0.03404 	 0.00050 	 m..s
   77 	    68 	 0.04328 	 0.00052 	 m..s
   10 	    69 	 0.03438 	 0.00053 	 m..s
   48 	    70 	 0.03833 	 0.00055 	 m..s
    0 	    71 	 0.03197 	 0.00056 	 m..s
   64 	    72 	 0.04013 	 0.00057 	 m..s
   47 	    73 	 0.03830 	 0.00076 	 m..s
   25 	    74 	 0.03614 	 0.00079 	 m..s
   16 	    75 	 0.03527 	 0.00084 	 m..s
   12 	    76 	 0.03459 	 0.00129 	 m..s
   75 	    77 	 0.04303 	 0.00171 	 m..s
   80 	    78 	 0.04422 	 0.00789 	 m..s
   93 	    79 	 0.06151 	 0.04857 	 ~...
   76 	    80 	 0.04321 	 0.06447 	 ~...
   88 	    81 	 0.05579 	 0.07961 	 ~...
   87 	    82 	 0.05543 	 0.08371 	 ~...
   67 	    83 	 0.04103 	 0.08488 	 m..s
   89 	    84 	 0.05604 	 0.08803 	 m..s
   97 	    85 	 0.06772 	 0.08860 	 ~...
   74 	    86 	 0.04285 	 0.08867 	 m..s
   94 	    87 	 0.06166 	 0.09060 	 ~...
   92 	    88 	 0.06140 	 0.09126 	 ~...
   86 	    89 	 0.05301 	 0.09149 	 m..s
   90 	    90 	 0.05900 	 0.09180 	 m..s
   65 	    91 	 0.04017 	 0.09286 	 m..s
   69 	    92 	 0.04148 	 0.09363 	 m..s
   99 	    93 	 0.07082 	 0.09460 	 ~...
   84 	    94 	 0.04543 	 0.09532 	 m..s
  112 	    95 	 0.14473 	 0.09535 	 m..s
  114 	    96 	 0.16566 	 0.09764 	 m..s
   68 	    97 	 0.04119 	 0.10902 	 m..s
  107 	    98 	 0.11912 	 0.12319 	 ~...
   91 	    99 	 0.06010 	 0.13164 	 m..s
   85 	   100 	 0.05213 	 0.13674 	 m..s
  104 	   101 	 0.10935 	 0.13680 	 ~...
   96 	   102 	 0.06365 	 0.13810 	 m..s
   98 	   103 	 0.07076 	 0.14653 	 m..s
  100 	   104 	 0.08269 	 0.15318 	 m..s
  118 	   105 	 0.17875 	 0.16378 	 ~...
  101 	   106 	 0.08947 	 0.16586 	 m..s
  106 	   107 	 0.11731 	 0.18204 	 m..s
  113 	   108 	 0.16146 	 0.18503 	 ~...
  102 	   109 	 0.10918 	 0.21574 	 MISS
  103 	   110 	 0.10925 	 0.21666 	 MISS
  108 	   111 	 0.11934 	 0.22657 	 MISS
  110 	   112 	 0.13678 	 0.26914 	 MISS
  115 	   113 	 0.16638 	 0.28706 	 MISS
  117 	   114 	 0.17083 	 0.30486 	 MISS
  120 	   115 	 0.18362 	 0.31033 	 MISS
  119 	   116 	 0.18234 	 0.31527 	 MISS
  111 	   117 	 0.14205 	 0.31960 	 MISS
  116 	   118 	 0.16974 	 0.33534 	 MISS
  105 	   119 	 0.11173 	 0.34767 	 MISS
  109 	   120 	 0.12686 	 0.38203 	 MISS
==========================================
r_mrr = 0.870924711227417
r2_mrr = 0.5527284145355225
spearmanr_mrr@5 = 0.8920726180076599
spearmanr_mrr@10 = 0.9559739828109741
spearmanr_mrr@50 = 0.9665513634681702
spearmanr_mrr@100 = 0.9739082455635071
spearmanr_mrr@All = 0.9758363366127014
==========================================
test time: 0.564
Done Testing dataset DBpedia50
total time taken: 93.55488896369934
training time taken: 86.60591912269592
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_OpenEA
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 702
rank avg (pred): 0.432 +- 0.010
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001973751

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 1140
rank avg (pred): 0.291 +- 0.007
mrr vals (pred, true): 0.000, 0.183
batch losses (mrrl, rdl): 0.0, 0.0001577966

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 403
rank avg (pred): 0.499 +- 0.303
mrr vals (pred, true): 0.058, 0.002
batch losses (mrrl, rdl): 0.0, 1.2902e-06

Epoch over!
epoch time: 16.868

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 272
rank avg (pred): 0.390 +- 0.308
mrr vals (pred, true): 0.065, 0.076
batch losses (mrrl, rdl): 0.0, 1.23096e-05

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 525
rank avg (pred): 0.337 +- 0.318
mrr vals (pred, true): 0.101, 0.064
batch losses (mrrl, rdl): 0.0, 3.92365e-05

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 306
rank avg (pred): 0.369 +- 0.307
mrr vals (pred, true): 0.106, 0.077
batch losses (mrrl, rdl): 0.0, 8.4045e-06

Epoch over!
epoch time: 17.216

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 460
rank avg (pred): 0.508 +- 0.299
mrr vals (pred, true): 0.090, 0.001
batch losses (mrrl, rdl): 0.0159245431, 4.403e-07

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 1079
rank avg (pred): 0.495 +- 0.304
mrr vals (pred, true): 0.071, 0.125
batch losses (mrrl, rdl): 0.0291197468, 0.000548931

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 899
rank avg (pred): 0.526 +- 0.366
mrr vals (pred, true): 0.114, 0.000
batch losses (mrrl, rdl): 0.0413163714, 0.0014777108

Epoch over!
epoch time: 17.01

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 658
rank avg (pred): 0.504 +- 0.252
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001033163, 1.29359e-05

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 233
rank avg (pred): 0.533 +- 0.239
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0014361978, 2.73163e-05

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 562
rank avg (pred): 0.443 +- 0.254
mrr vals (pred, true): 0.066, 0.055
batch losses (mrrl, rdl): 0.0024334742, 0.0001141712

Epoch over!
epoch time: 17.951

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 501
rank avg (pred): 0.421 +- 0.331
mrr vals (pred, true): 0.137, 0.165
batch losses (mrrl, rdl): 0.0077781868, 0.0006065436

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 881
rank avg (pred): 0.565 +- 0.285
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.07943e-05, 5.12061e-05

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 1045
rank avg (pred): 0.505 +- 0.243
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.99767e-05, 1.50989e-05

Epoch over!
epoch time: 19.364

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.508 +- 0.298
mrr vals (pred, true): 0.068, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   98 	     0 	 0.09356 	 0.00031 	 m..s
   26 	     1 	 0.04859 	 0.00045 	 m..s
   45 	     2 	 0.05012 	 0.00047 	 m..s
   70 	     3 	 0.05325 	 0.00051 	 m..s
   21 	     4 	 0.04835 	 0.00052 	 m..s
   54 	     5 	 0.05129 	 0.00053 	 m..s
   60 	     6 	 0.05223 	 0.00053 	 m..s
   76 	     7 	 0.05431 	 0.00055 	 m..s
   10 	     8 	 0.04628 	 0.00055 	 m..s
   38 	     9 	 0.04963 	 0.00056 	 m..s
   18 	    10 	 0.04797 	 0.00056 	 m..s
   87 	    11 	 0.06071 	 0.00057 	 m..s
   51 	    12 	 0.05093 	 0.00057 	 m..s
   47 	    13 	 0.05054 	 0.00057 	 m..s
    5 	    14 	 0.04608 	 0.00057 	 m..s
   58 	    15 	 0.05158 	 0.00058 	 m..s
   15 	    16 	 0.04681 	 0.00059 	 m..s
   44 	    17 	 0.05008 	 0.00059 	 m..s
   65 	    18 	 0.05259 	 0.00060 	 m..s
   82 	    19 	 0.05664 	 0.00060 	 m..s
   37 	    20 	 0.04960 	 0.00061 	 m..s
   34 	    21 	 0.04944 	 0.00064 	 m..s
   48 	    22 	 0.05060 	 0.00068 	 m..s
   41 	    23 	 0.04992 	 0.00069 	 m..s
   11 	    24 	 0.04652 	 0.00069 	 m..s
    7 	    25 	 0.04613 	 0.00070 	 m..s
   50 	    26 	 0.05086 	 0.00071 	 m..s
   35 	    27 	 0.04949 	 0.00072 	 m..s
   68 	    28 	 0.05299 	 0.00073 	 m..s
   59 	    29 	 0.05214 	 0.00073 	 m..s
   78 	    30 	 0.05525 	 0.00075 	 m..s
   84 	    31 	 0.05685 	 0.00076 	 m..s
    9 	    32 	 0.04624 	 0.00076 	 m..s
   20 	    33 	 0.04802 	 0.00076 	 m..s
    3 	    34 	 0.04590 	 0.00077 	 m..s
    4 	    35 	 0.04598 	 0.00078 	 m..s
   12 	    36 	 0.04660 	 0.00078 	 m..s
   40 	    37 	 0.04975 	 0.00080 	 m..s
    6 	    38 	 0.04612 	 0.00081 	 m..s
   52 	    39 	 0.05095 	 0.00081 	 m..s
    2 	    40 	 0.04582 	 0.00087 	 m..s
   22 	    41 	 0.04846 	 0.00088 	 m..s
   14 	    42 	 0.04668 	 0.00091 	 m..s
   27 	    43 	 0.04870 	 0.00092 	 m..s
   75 	    44 	 0.05427 	 0.00101 	 m..s
   56 	    45 	 0.05151 	 0.00103 	 m..s
   46 	    46 	 0.05012 	 0.00104 	 m..s
    0 	    47 	 0.04530 	 0.00106 	 m..s
   33 	    48 	 0.04923 	 0.00108 	 m..s
   43 	    49 	 0.05003 	 0.00112 	 m..s
   23 	    50 	 0.04849 	 0.00115 	 m..s
    1 	    51 	 0.04581 	 0.00116 	 m..s
   62 	    52 	 0.05228 	 0.00116 	 m..s
   16 	    53 	 0.04688 	 0.00118 	 m..s
    8 	    54 	 0.04618 	 0.00120 	 m..s
   30 	    55 	 0.04892 	 0.00122 	 m..s
   17 	    56 	 0.04689 	 0.00123 	 m..s
   72 	    57 	 0.05346 	 0.00133 	 m..s
   39 	    58 	 0.04965 	 0.00136 	 m..s
   67 	    59 	 0.05279 	 0.00137 	 m..s
   89 	    60 	 0.06119 	 0.00139 	 m..s
   64 	    61 	 0.05247 	 0.00142 	 m..s
   61 	    62 	 0.05226 	 0.00142 	 m..s
   29 	    63 	 0.04877 	 0.00143 	 m..s
   32 	    64 	 0.04915 	 0.00146 	 m..s
   36 	    65 	 0.04953 	 0.00153 	 m..s
   25 	    66 	 0.04857 	 0.00156 	 m..s
   74 	    67 	 0.05386 	 0.00161 	 m..s
   13 	    68 	 0.04665 	 0.00170 	 m..s
   19 	    69 	 0.04798 	 0.00186 	 m..s
   57 	    70 	 0.05156 	 0.00194 	 m..s
   28 	    71 	 0.04872 	 0.00195 	 m..s
   49 	    72 	 0.05066 	 0.00196 	 m..s
   63 	    73 	 0.05235 	 0.00199 	 m..s
   31 	    74 	 0.04908 	 0.00200 	 m..s
   42 	    75 	 0.04999 	 0.00200 	 m..s
   24 	    76 	 0.04851 	 0.00224 	 m..s
   94 	    77 	 0.06782 	 0.00249 	 m..s
   95 	    78 	 0.06908 	 0.00459 	 m..s
   88 	    79 	 0.06084 	 0.04959 	 ~...
   86 	    80 	 0.05948 	 0.05253 	 ~...
   77 	    81 	 0.05523 	 0.06381 	 ~...
   66 	    82 	 0.05264 	 0.07113 	 ~...
   83 	    83 	 0.05681 	 0.07360 	 ~...
   55 	    84 	 0.05150 	 0.07464 	 ~...
   80 	    85 	 0.05570 	 0.07469 	 ~...
   93 	    86 	 0.06650 	 0.07542 	 ~...
   79 	    87 	 0.05549 	 0.07636 	 ~...
   69 	    88 	 0.05324 	 0.07648 	 ~...
   92 	    89 	 0.06538 	 0.07650 	 ~...
   71 	    90 	 0.05335 	 0.07667 	 ~...
   85 	    91 	 0.05691 	 0.07719 	 ~...
   53 	    92 	 0.05116 	 0.07971 	 ~...
  110 	    93 	 0.15738 	 0.08036 	 m..s
   91 	    94 	 0.06252 	 0.08403 	 ~...
   90 	    95 	 0.06211 	 0.08523 	 ~...
   81 	    96 	 0.05578 	 0.08922 	 m..s
   73 	    97 	 0.05381 	 0.08956 	 m..s
   97 	    98 	 0.09282 	 0.09360 	 ~...
   96 	    99 	 0.07392 	 0.09838 	 ~...
  108 	   100 	 0.14825 	 0.10039 	 m..s
  100 	   101 	 0.09986 	 0.11871 	 ~...
  101 	   102 	 0.10830 	 0.12857 	 ~...
   99 	   103 	 0.09809 	 0.12860 	 m..s
  109 	   104 	 0.15687 	 0.13637 	 ~...
  102 	   105 	 0.11032 	 0.13818 	 ~...
  105 	   106 	 0.13327 	 0.17068 	 m..s
  107 	   107 	 0.14215 	 0.17901 	 m..s
  106 	   108 	 0.13391 	 0.18394 	 m..s
  104 	   109 	 0.12197 	 0.18588 	 m..s
  103 	   110 	 0.12131 	 0.18649 	 m..s
  111 	   111 	 0.16454 	 0.21233 	 m..s
  114 	   112 	 0.17360 	 0.21589 	 m..s
  118 	   113 	 0.19866 	 0.22307 	 ~...
  116 	   114 	 0.17671 	 0.27717 	 MISS
  115 	   115 	 0.17583 	 0.28122 	 MISS
  117 	   116 	 0.18169 	 0.28885 	 MISS
  112 	   117 	 0.17197 	 0.29363 	 MISS
  113 	   118 	 0.17200 	 0.31774 	 MISS
  119 	   119 	 0.21448 	 0.34668 	 MISS
  120 	   120 	 0.22506 	 0.34919 	 MISS
==========================================
r_mrr = 0.9243084788322449
r2_mrr = 0.6148520708084106
spearmanr_mrr@5 = 0.9882981777191162
spearmanr_mrr@10 = 0.8810573816299438
spearmanr_mrr@50 = 0.9606093764305115
spearmanr_mrr@100 = 0.968792736530304
spearmanr_mrr@All = 0.971014142036438
==========================================
test time: 0.669
Done Testing dataset OpenEA
total time taken: 117.32759475708008
training time taken: 89.14075040817261
TWIG out ;))
