Using random seed: 17
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_UMLS
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: True
rescale_rank_dist_loss: True
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 702
rank avg (pred): 0.434 +- 0.011
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 0.0121843014

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.470 +- 0.006
mrr vals (pred, true): 0.016, 0.026
batch losses (mrrl, rdl): 0.0, 0.005848994

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 403
rank avg (pred): 0.464 +- 0.028
mrr vals (pred, true): 0.016, 0.050
batch losses (mrrl, rdl): 0.0, 0.0122259418

Epoch over!
epoch time: 19.111

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 272
rank avg (pred): 0.216 +- 0.199
mrr vals (pred, true): 0.046, 0.252
batch losses (mrrl, rdl): 0.0, 0.0135507351

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 525
rank avg (pred): 0.532 +- 0.255
mrr vals (pred, true): 0.023, 0.026
batch losses (mrrl, rdl): 0.0, 0.001216053

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 306
rank avg (pred): 0.167 +- 0.188
mrr vals (pred, true): 0.104, 0.192
batch losses (mrrl, rdl): 0.0, 0.0027246838

Epoch over!
epoch time: 19.391

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 460
rank avg (pred): 0.440 +- 0.258
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0282582492, 0.0006248463

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1079
rank avg (pred): 0.074 +- 0.056
mrr vals (pred, true): 0.281, 0.258
batch losses (mrrl, rdl): 1.6610075235, 0.0695999563

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 899
rank avg (pred): 0.382 +- 0.111
mrr vals (pred, true): 0.063, 0.055
batch losses (mrrl, rdl): 0.225715369, 0.0144135961

Epoch over!
epoch time: 18.494

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 658
rank avg (pred): 0.553 +- 0.148
mrr vals (pred, true): 0.059, 0.048
batch losses (mrrl, rdl): 0.0940873101, 0.0520752855

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 233
rank avg (pred): 0.433 +- 0.108
mrr vals (pred, true): 0.054, 0.051
batch losses (mrrl, rdl): 0.0184480809, 0.0085391449

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 562
rank avg (pred): 0.490 +- 0.141
mrr vals (pred, true): 0.066, 0.025
batch losses (mrrl, rdl): 0.2967570424, 0.0054144296

Epoch over!
epoch time: 18.491

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 501
rank avg (pred): 0.512 +- 0.195
mrr vals (pred, true): 0.088, 0.024
batch losses (mrrl, rdl): 1.5929969549, 0.0008253952

running batch: 500 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 881
rank avg (pred): 0.458 +- 0.106
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0167609453, 0.0108882245

running batch: 1000 / 1094 and superbatch(1); data from UMLS, run 2.1, exp 1045
rank avg (pred): 0.463 +- 0.105
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0416000821, 0.011560265

Epoch over!
epoch time: 18.785

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.498 +- 0.145
mrr vals (pred, true): 0.068, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   89 	     0 	 0.06825 	 0.01773 	 m..s
   84 	     1 	 0.06487 	 0.02009 	 m..s
   73 	     2 	 0.05680 	 0.02164 	 m..s
   80 	     3 	 0.06239 	 0.02192 	 m..s
   88 	     4 	 0.06611 	 0.02235 	 m..s
   83 	     5 	 0.06445 	 0.02398 	 m..s
   79 	     6 	 0.05939 	 0.02414 	 m..s
   86 	     7 	 0.06555 	 0.02414 	 m..s
   78 	     8 	 0.05903 	 0.02423 	 m..s
   81 	     9 	 0.06259 	 0.02467 	 m..s
   85 	    10 	 0.06546 	 0.02475 	 m..s
   77 	    11 	 0.05884 	 0.02496 	 m..s
   82 	    12 	 0.06419 	 0.02544 	 m..s
   87 	    13 	 0.06577 	 0.02586 	 m..s
   26 	    14 	 0.04729 	 0.02933 	 ~...
    9 	    15 	 0.04430 	 0.03159 	 ~...
    6 	    16 	 0.04350 	 0.03197 	 ~...
   25 	    17 	 0.04722 	 0.03458 	 ~...
   11 	    18 	 0.04444 	 0.03549 	 ~...
   10 	    19 	 0.04441 	 0.03582 	 ~...
   24 	    20 	 0.04719 	 0.03732 	 ~...
   18 	    21 	 0.04583 	 0.03811 	 ~...
   56 	    22 	 0.05221 	 0.03856 	 ~...
   46 	    23 	 0.05104 	 0.03931 	 ~...
    3 	    24 	 0.04171 	 0.03957 	 ~...
   31 	    25 	 0.04825 	 0.03980 	 ~...
   47 	    26 	 0.05107 	 0.04032 	 ~...
   48 	    27 	 0.05111 	 0.04070 	 ~...
   52 	    28 	 0.05167 	 0.04080 	 ~...
   45 	    29 	 0.05090 	 0.04099 	 ~...
   65 	    30 	 0.05401 	 0.04129 	 ~...
   29 	    31 	 0.04802 	 0.04130 	 ~...
   69 	    32 	 0.05556 	 0.04163 	 ~...
   54 	    33 	 0.05182 	 0.04165 	 ~...
   50 	    34 	 0.05146 	 0.04176 	 ~...
   75 	    35 	 0.05760 	 0.04224 	 ~...
   40 	    36 	 0.05010 	 0.04272 	 ~...
   35 	    37 	 0.04908 	 0.04276 	 ~...
   27 	    38 	 0.04738 	 0.04306 	 ~...
   32 	    39 	 0.04852 	 0.04338 	 ~...
    7 	    40 	 0.04382 	 0.04372 	 ~...
   53 	    41 	 0.05178 	 0.04373 	 ~...
    4 	    42 	 0.04282 	 0.04388 	 ~...
   19 	    43 	 0.04627 	 0.04410 	 ~...
   71 	    44 	 0.05612 	 0.04419 	 ~...
   37 	    45 	 0.04932 	 0.04420 	 ~...
   15 	    46 	 0.04536 	 0.04423 	 ~...
    5 	    47 	 0.04282 	 0.04476 	 ~...
   64 	    48 	 0.05400 	 0.04514 	 ~...
   70 	    49 	 0.05574 	 0.04551 	 ~...
   23 	    50 	 0.04707 	 0.04564 	 ~...
   58 	    51 	 0.05245 	 0.04617 	 ~...
   44 	    52 	 0.05062 	 0.04642 	 ~...
   34 	    53 	 0.04899 	 0.04687 	 ~...
   20 	    54 	 0.04627 	 0.04699 	 ~...
   21 	    55 	 0.04680 	 0.04706 	 ~...
   13 	    56 	 0.04482 	 0.04718 	 ~...
   74 	    57 	 0.05691 	 0.04727 	 ~...
   63 	    58 	 0.05377 	 0.04740 	 ~...
   33 	    59 	 0.04897 	 0.04744 	 ~...
   61 	    60 	 0.05344 	 0.04750 	 ~...
   62 	    61 	 0.05353 	 0.04758 	 ~...
   43 	    62 	 0.05038 	 0.04763 	 ~...
   14 	    63 	 0.04490 	 0.04824 	 ~...
   38 	    64 	 0.04947 	 0.04857 	 ~...
   12 	    65 	 0.04450 	 0.04879 	 ~...
   59 	    66 	 0.05336 	 0.04879 	 ~...
   55 	    67 	 0.05212 	 0.04904 	 ~...
    8 	    68 	 0.04390 	 0.04921 	 ~...
   66 	    69 	 0.05490 	 0.04928 	 ~...
   68 	    70 	 0.05543 	 0.04929 	 ~...
   39 	    71 	 0.05002 	 0.04949 	 ~...
   30 	    72 	 0.04808 	 0.04956 	 ~...
    0 	    73 	 0.03940 	 0.04987 	 ~...
   36 	    74 	 0.04913 	 0.05004 	 ~...
   42 	    75 	 0.05019 	 0.05023 	 ~...
   28 	    76 	 0.04769 	 0.05056 	 ~...
    2 	    77 	 0.04163 	 0.05108 	 ~...
    1 	    78 	 0.04158 	 0.05128 	 ~...
   57 	    79 	 0.05243 	 0.05129 	 ~...
   51 	    80 	 0.05151 	 0.05144 	 ~...
   17 	    81 	 0.04553 	 0.05146 	 ~...
   41 	    82 	 0.05018 	 0.05160 	 ~...
   16 	    83 	 0.04552 	 0.05189 	 ~...
   76 	    84 	 0.05778 	 0.05246 	 ~...
   67 	    85 	 0.05540 	 0.05275 	 ~...
   22 	    86 	 0.04692 	 0.05352 	 ~...
   60 	    87 	 0.05339 	 0.05355 	 ~...
   72 	    88 	 0.05675 	 0.05567 	 ~...
   49 	    89 	 0.05144 	 0.05977 	 ~...
   95 	    90 	 0.20972 	 0.18889 	 ~...
   93 	    91 	 0.20239 	 0.18959 	 ~...
   94 	    92 	 0.20634 	 0.19213 	 ~...
   92 	    93 	 0.19961 	 0.19872 	 ~...
   98 	    94 	 0.21820 	 0.20550 	 ~...
   90 	    95 	 0.15183 	 0.20694 	 m..s
  106 	    96 	 0.24570 	 0.20727 	 m..s
   99 	    97 	 0.22598 	 0.21693 	 ~...
  105 	    98 	 0.24069 	 0.21723 	 ~...
   97 	    99 	 0.21587 	 0.21940 	 ~...
   91 	   100 	 0.15203 	 0.21956 	 m..s
  102 	   101 	 0.23318 	 0.23290 	 ~...
  100 	   102 	 0.22951 	 0.23984 	 ~...
  112 	   103 	 0.28545 	 0.24107 	 m..s
  109 	   104 	 0.25744 	 0.24218 	 ~...
  103 	   105 	 0.23525 	 0.24307 	 ~...
   96 	   106 	 0.21522 	 0.25298 	 m..s
  110 	   107 	 0.25802 	 0.25327 	 ~...
  101 	   108 	 0.23092 	 0.27378 	 m..s
  104 	   109 	 0.24053 	 0.27425 	 m..s
  115 	   110 	 0.31781 	 0.27441 	 m..s
  113 	   111 	 0.29982 	 0.27737 	 ~...
  116 	   112 	 0.31814 	 0.28034 	 m..s
  111 	   113 	 0.26693 	 0.28353 	 ~...
  117 	   114 	 0.34127 	 0.29788 	 m..s
  108 	   115 	 0.25566 	 0.29922 	 m..s
  114 	   116 	 0.31284 	 0.31748 	 ~...
  107 	   117 	 0.25389 	 0.33485 	 m..s
  119 	   118 	 0.36799 	 0.39525 	 ~...
  120 	   119 	 0.38842 	 0.52223 	 MISS
  118 	   120 	 0.34659 	 0.53924 	 MISS
==========================================
r_mrr = 0.9637678861618042
r2_mrr = 0.9187449812889099
spearmanr_mrr@5 = 0.943871796131134
spearmanr_mrr@10 = 0.906220555305481
spearmanr_mrr@50 = 0.9723964333534241
spearmanr_mrr@100 = 0.9829525351524353
spearmanr_mrr@All = 0.9842005372047424
==========================================
test time: 0.733
Done Testing dataset UMLS
total time taken: 104.299400806427
training time taken: 95.09686207771301
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_Kinships
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: True
rescale_rank_dist_loss: True
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 702
rank avg (pred): 0.431 +- 0.011
mrr vals (pred, true): 0.022, 0.049
batch losses (mrrl, rdl): 0.0, 0.0161785409

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 1140
rank avg (pred): 0.114 +- 0.072
mrr vals (pred, true): 0.146, 0.319
batch losses (mrrl, rdl): 0.0, 0.0018185703

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 403
rank avg (pred): 0.470 +- 0.271
mrr vals (pred, true): 0.040, 0.050
batch losses (mrrl, rdl): 0.0, 0.001259911

Epoch over!
epoch time: 19.163

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 272
rank avg (pred): 0.104 +- 0.092
mrr vals (pred, true): 0.173, 0.332
batch losses (mrrl, rdl): 0.0, 0.0003860829

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 525
rank avg (pred): 0.147 +- 0.137
mrr vals (pred, true): 0.131, 0.211
batch losses (mrrl, rdl): 0.0, 0.0009529284

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 306
rank avg (pred): 0.114 +- 0.135
mrr vals (pred, true): 0.182, 0.293
batch losses (mrrl, rdl): 0.0, 0.0014605562

Epoch over!
epoch time: 19.622

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 460
rank avg (pred): 0.479 +- 0.273
mrr vals (pred, true): 0.037, 0.052
batch losses (mrrl, rdl): 0.2153293043, 0.0009915269

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 1079
rank avg (pred): 0.145 +- 0.118
mrr vals (pred, true): 0.347, 0.482
batch losses (mrrl, rdl): 253.1576843262, 0.0289660152

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 899
rank avg (pred): 0.090 +- 0.073
mrr vals (pred, true): 0.368, 0.084
batch losses (mrrl, rdl): 144.1667022705, 0.1928777695

Epoch over!
epoch time: 19.552

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 658
rank avg (pred): 0.477 +- 0.187
mrr vals (pred, true): 0.059, 0.045
batch losses (mrrl, rdl): 0.0928080603, 0.0030867874

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 233
rank avg (pred): 0.409 +- 0.123
mrr vals (pred, true): 0.048, 0.047
batch losses (mrrl, rdl): 0.0071171666, 0.0223159641

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 562
rank avg (pred): 0.281 +- 0.169
mrr vals (pred, true): 0.193, 0.202
batch losses (mrrl, rdl): 0.1956975758, 0.0583065376

Epoch over!
epoch time: 19.541

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 501
rank avg (pred): 0.218 +- 0.174
mrr vals (pred, true): 0.312, 0.224
batch losses (mrrl, rdl): 21.1009464264, 0.0293292962

running batch: 500 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 881
rank avg (pred): 0.536 +- 0.199
mrr vals (pred, true): 0.040, 0.053
batch losses (mrrl, rdl): 0.1369841397, 0.0199929792

running batch: 1000 / 1094 and superbatch(1); data from Kinships, run 2.1, exp 1045
rank avg (pred): 0.461 +- 0.182
mrr vals (pred, true): 0.062, 0.056
batch losses (mrrl, rdl): 0.1943919063, 0.0041302843

Epoch over!
epoch time: 18.846

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.297 +- 0.217
mrr vals (pred, true): 0.256, 0.228

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   54 	     0 	 0.04460 	 0.04713 	 ~...
   64 	     1 	 0.04663 	 0.04727 	 ~...
   23 	     2 	 0.04121 	 0.04763 	 ~...
   16 	     3 	 0.04065 	 0.04803 	 ~...
   17 	     4 	 0.04079 	 0.04830 	 ~...
   26 	     5 	 0.04127 	 0.04840 	 ~...
   34 	     6 	 0.04192 	 0.04863 	 ~...
   70 	     7 	 0.04733 	 0.04890 	 ~...
    8 	     8 	 0.03975 	 0.04927 	 ~...
   63 	     9 	 0.04621 	 0.04930 	 ~...
   68 	    10 	 0.04671 	 0.04948 	 ~...
   12 	    11 	 0.04003 	 0.04950 	 ~...
   43 	    12 	 0.04340 	 0.04954 	 ~...
    5 	    13 	 0.03875 	 0.04990 	 ~...
   32 	    14 	 0.04189 	 0.05021 	 ~...
   20 	    15 	 0.04092 	 0.05035 	 ~...
   56 	    16 	 0.04527 	 0.05050 	 ~...
   66 	    17 	 0.04667 	 0.05071 	 ~...
   55 	    18 	 0.04505 	 0.05076 	 ~...
   65 	    19 	 0.04665 	 0.05097 	 ~...
   72 	    20 	 0.04777 	 0.05105 	 ~...
   47 	    21 	 0.04369 	 0.05107 	 ~...
   18 	    22 	 0.04079 	 0.05112 	 ~...
   35 	    23 	 0.04194 	 0.05113 	 ~...
   39 	    24 	 0.04310 	 0.05127 	 ~...
   58 	    25 	 0.04571 	 0.05151 	 ~...
   41 	    26 	 0.04324 	 0.05154 	 ~...
   52 	    27 	 0.04408 	 0.05179 	 ~...
   31 	    28 	 0.04179 	 0.05186 	 ~...
   59 	    29 	 0.04572 	 0.05193 	 ~...
   53 	    30 	 0.04455 	 0.05211 	 ~...
   46 	    31 	 0.04367 	 0.05215 	 ~...
   37 	    32 	 0.04301 	 0.05219 	 ~...
   69 	    33 	 0.04684 	 0.05236 	 ~...
    9 	    34 	 0.03989 	 0.05312 	 ~...
   30 	    35 	 0.04165 	 0.05317 	 ~...
   44 	    36 	 0.04346 	 0.05317 	 ~...
   67 	    37 	 0.04668 	 0.05341 	 ~...
   48 	    38 	 0.04376 	 0.05350 	 ~...
   42 	    39 	 0.04340 	 0.05351 	 ~...
    6 	    40 	 0.03907 	 0.05358 	 ~...
   50 	    41 	 0.04396 	 0.05406 	 ~...
    3 	    42 	 0.03749 	 0.05428 	 ~...
   14 	    43 	 0.04057 	 0.05440 	 ~...
   62 	    44 	 0.04621 	 0.05481 	 ~...
   49 	    45 	 0.04394 	 0.05498 	 ~...
    4 	    46 	 0.03815 	 0.05510 	 ~...
   57 	    47 	 0.04558 	 0.05531 	 ~...
   38 	    48 	 0.04309 	 0.05575 	 ~...
   36 	    49 	 0.04278 	 0.05576 	 ~...
   24 	    50 	 0.04121 	 0.05587 	 ~...
   29 	    51 	 0.04144 	 0.05596 	 ~...
   33 	    52 	 0.04189 	 0.05638 	 ~...
    7 	    53 	 0.03969 	 0.05694 	 ~...
   40 	    54 	 0.04323 	 0.05696 	 ~...
    2 	    55 	 0.03744 	 0.05719 	 ~...
   27 	    56 	 0.04128 	 0.05723 	 ~...
   74 	    57 	 0.04838 	 0.05725 	 ~...
   73 	    58 	 0.04804 	 0.05733 	 ~...
   21 	    59 	 0.04112 	 0.05749 	 ~...
   61 	    60 	 0.04611 	 0.05787 	 ~...
   75 	    61 	 0.04989 	 0.05825 	 ~...
   28 	    62 	 0.04131 	 0.05868 	 ~...
    1 	    63 	 0.03743 	 0.05870 	 ~...
   60 	    64 	 0.04578 	 0.05874 	 ~...
   25 	    65 	 0.04124 	 0.05881 	 ~...
   10 	    66 	 0.04000 	 0.05882 	 ~...
   13 	    67 	 0.04012 	 0.05918 	 ~...
   51 	    68 	 0.04405 	 0.05969 	 ~...
    0 	    69 	 0.03657 	 0.05993 	 ~...
   19 	    70 	 0.04086 	 0.06014 	 ~...
   15 	    71 	 0.04060 	 0.06038 	 ~...
   11 	    72 	 0.04001 	 0.06049 	 ~...
   22 	    73 	 0.04114 	 0.06096 	 ~...
   45 	    74 	 0.04366 	 0.06158 	 ~...
   71 	    75 	 0.04737 	 0.06229 	 ~...
   82 	    76 	 0.22226 	 0.16198 	 m..s
   80 	    77 	 0.17938 	 0.18404 	 ~...
   76 	    78 	 0.16664 	 0.18970 	 ~...
   78 	    79 	 0.16788 	 0.19886 	 m..s
   77 	    80 	 0.16664 	 0.21213 	 m..s
   91 	    81 	 0.26016 	 0.21714 	 m..s
   81 	    82 	 0.18024 	 0.21822 	 m..s
   92 	    83 	 0.26474 	 0.21861 	 m..s
   79 	    84 	 0.17216 	 0.22784 	 m..s
   86 	    85 	 0.25576 	 0.22825 	 ~...
   85 	    86 	 0.25377 	 0.25404 	 ~...
   87 	    87 	 0.25672 	 0.26205 	 ~...
  100 	    88 	 0.30629 	 0.26840 	 m..s
   97 	    89 	 0.30279 	 0.27182 	 m..s
   99 	    90 	 0.30481 	 0.28071 	 ~...
  105 	    91 	 0.34296 	 0.28148 	 m..s
   98 	    92 	 0.30397 	 0.28774 	 ~...
   84 	    93 	 0.25139 	 0.29619 	 m..s
   90 	    94 	 0.25916 	 0.29634 	 m..s
   93 	    95 	 0.26591 	 0.29652 	 m..s
   89 	    96 	 0.25768 	 0.30510 	 m..s
  106 	    97 	 0.34310 	 0.30598 	 m..s
   95 	    98 	 0.28757 	 0.30754 	 ~...
  107 	    99 	 0.34785 	 0.30803 	 m..s
   83 	   100 	 0.25041 	 0.30884 	 m..s
  104 	   101 	 0.34135 	 0.31529 	 ~...
  103 	   102 	 0.33398 	 0.31655 	 ~...
   88 	   103 	 0.25701 	 0.32286 	 m..s
  108 	   104 	 0.35798 	 0.33122 	 ~...
   94 	   105 	 0.27299 	 0.34296 	 m..s
  109 	   106 	 0.36638 	 0.36221 	 ~...
  101 	   107 	 0.30846 	 0.36880 	 m..s
   96 	   108 	 0.30253 	 0.36975 	 m..s
  102 	   109 	 0.31244 	 0.37989 	 m..s
  113 	   110 	 0.41553 	 0.38991 	 ~...
  110 	   111 	 0.39159 	 0.40258 	 ~...
  116 	   112 	 0.45428 	 0.40967 	 m..s
  112 	   113 	 0.41280 	 0.41075 	 ~...
  111 	   114 	 0.39301 	 0.42112 	 ~...
  115 	   115 	 0.44721 	 0.47933 	 m..s
  114 	   116 	 0.42184 	 0.48000 	 m..s
  118 	   117 	 0.46296 	 0.50285 	 m..s
  120 	   118 	 0.51422 	 0.61365 	 m..s
  119 	   119 	 0.50416 	 0.61786 	 MISS
  117 	   120 	 0.45867 	 0.62288 	 MISS
==========================================
r_mrr = 0.9797976016998291
r2_mrr = 0.9514724612236023
spearmanr_mrr@5 = 0.757854700088501
spearmanr_mrr@10 = 0.9288999438285828
spearmanr_mrr@50 = 0.9748081564903259
spearmanr_mrr@100 = 0.9903689026832581
spearmanr_mrr@All = 0.9914066195487976
==========================================
test time: 0.698
Done Testing dataset Kinships
total time taken: 107.92628264427185
training time taken: 97.50805330276489
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_CoDExSmall
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: True
rescale_rank_dist_loss: True
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 702
rank avg (pred): 0.432 +- 0.010
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 0.021806078

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 1140
rank avg (pred): 0.172 +- 0.086
mrr vals (pred, true): 0.071, 0.035
batch losses (mrrl, rdl): 0.0, 0.0106879668

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 403
rank avg (pred): 0.431 +- 0.264
mrr vals (pred, true): 0.134, 0.005
batch losses (mrrl, rdl): 0.0, 0.001351247

Epoch over!
epoch time: 19.438

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 272
rank avg (pred): 0.055 +- 0.036
mrr vals (pred, true): 0.192, 0.205
batch losses (mrrl, rdl): 0.0, 0.0035428659

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.415 +- 0.259
mrr vals (pred, true): 0.120, 0.008
batch losses (mrrl, rdl): 0.0, 0.0016475647

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 306
rank avg (pred): 0.070 +- 0.119
mrr vals (pred, true): 0.167, 0.182
batch losses (mrrl, rdl): 0.0, 0.0008842314

Epoch over!
epoch time: 21.204

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 460
rank avg (pred): 0.449 +- 0.272
mrr vals (pred, true): 0.090, 0.004
batch losses (mrrl, rdl): 1.5969945192, 0.0008178466

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 1079
rank avg (pred): 0.016 +- 0.010
mrr vals (pred, true): 0.253, 0.289
batch losses (mrrl, rdl): 4.9779582024, 0.0047161831

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 899
rank avg (pred): 0.394 +- 0.176
mrr vals (pred, true): 0.086, 0.001
batch losses (mrrl, rdl): 1.3395498991, 0.0687654391

Epoch over!
epoch time: 19.293

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 658
rank avg (pred): 0.513 +- 0.153
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.1078709736, 0.012687481

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 233
rank avg (pred): 0.479 +- 0.158
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 2.13654e-05, 0.0072146025

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 562
rank avg (pred): 0.419 +- 0.128
mrr vals (pred, true): 0.051, 0.007
batch losses (mrrl, rdl): 0.0009671317, 0.0053249886

Epoch over!
epoch time: 19.445

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 501
rank avg (pred): 0.445 +- 0.166
mrr vals (pred, true): 0.058, 0.044
batch losses (mrrl, rdl): 0.0717222989, 0.0918032303

running batch: 500 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 881
rank avg (pred): 0.469 +- 0.164
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0202002656, 0.0068247961

running batch: 1000 / 1094 and superbatch(1); data from CoDExSmall, run 2.1, exp 1045
rank avg (pred): 0.487 +- 0.158
mrr vals (pred, true): 0.046, 0.005
batch losses (mrrl, rdl): 0.0182594042, 0.0078008049

Epoch over!
epoch time: 20.336

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.449 +- 0.152
mrr vals (pred, true): 0.051, 0.001

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   74 	     0 	 0.05074 	 0.00086 	 m..s
   79 	     1 	 0.05117 	 0.00089 	 m..s
   11 	     2 	 0.04054 	 0.00170 	 m..s
   81 	     3 	 0.05269 	 0.00174 	 m..s
   23 	     4 	 0.04252 	 0.00204 	 m..s
   73 	     5 	 0.05070 	 0.00240 	 m..s
   68 	     6 	 0.05027 	 0.00242 	 m..s
   69 	     7 	 0.05038 	 0.00242 	 m..s
   54 	     8 	 0.04651 	 0.00287 	 m..s
   15 	     9 	 0.04215 	 0.00289 	 m..s
   61 	    10 	 0.04757 	 0.00291 	 m..s
    4 	    11 	 0.03796 	 0.00292 	 m..s
   60 	    12 	 0.04741 	 0.00296 	 m..s
   55 	    13 	 0.04671 	 0.00310 	 m..s
   62 	    14 	 0.04768 	 0.00315 	 m..s
   71 	    15 	 0.05050 	 0.00316 	 m..s
   48 	    16 	 0.04540 	 0.00317 	 m..s
    1 	    17 	 0.03699 	 0.00325 	 m..s
   39 	    18 	 0.04480 	 0.00331 	 m..s
   56 	    19 	 0.04679 	 0.00331 	 m..s
   78 	    20 	 0.05107 	 0.00336 	 m..s
   57 	    21 	 0.04714 	 0.00337 	 m..s
   10 	    22 	 0.04009 	 0.00345 	 m..s
    7 	    23 	 0.03970 	 0.00345 	 m..s
   51 	    24 	 0.04620 	 0.00356 	 m..s
   52 	    25 	 0.04631 	 0.00360 	 m..s
   77 	    26 	 0.05101 	 0.00362 	 m..s
   18 	    27 	 0.04233 	 0.00362 	 m..s
   38 	    28 	 0.04473 	 0.00363 	 m..s
    6 	    29 	 0.03920 	 0.00363 	 m..s
   33 	    30 	 0.04396 	 0.00364 	 m..s
   66 	    31 	 0.05009 	 0.00368 	 m..s
   65 	    32 	 0.04981 	 0.00368 	 m..s
   46 	    33 	 0.04532 	 0.00374 	 m..s
   19 	    34 	 0.04233 	 0.00376 	 m..s
   63 	    35 	 0.04778 	 0.00376 	 m..s
   58 	    36 	 0.04733 	 0.00381 	 m..s
   16 	    37 	 0.04218 	 0.00381 	 m..s
   30 	    38 	 0.04356 	 0.00384 	 m..s
   75 	    39 	 0.05076 	 0.00389 	 m..s
   83 	    40 	 0.05319 	 0.00391 	 m..s
    5 	    41 	 0.03869 	 0.00394 	 m..s
   87 	    42 	 0.05746 	 0.00399 	 m..s
   67 	    43 	 0.05025 	 0.00400 	 m..s
   76 	    44 	 0.05084 	 0.00402 	 m..s
   28 	    45 	 0.04331 	 0.00404 	 m..s
   37 	    46 	 0.04468 	 0.00406 	 m..s
   27 	    47 	 0.04329 	 0.00409 	 m..s
    0 	    48 	 0.03689 	 0.00412 	 m..s
   21 	    49 	 0.04240 	 0.00413 	 m..s
   88 	    50 	 0.05908 	 0.00415 	 m..s
   22 	    51 	 0.04243 	 0.00415 	 m..s
   40 	    52 	 0.04484 	 0.00415 	 m..s
   29 	    53 	 0.04343 	 0.00415 	 m..s
   53 	    54 	 0.04647 	 0.00416 	 m..s
   34 	    55 	 0.04424 	 0.00418 	 m..s
    8 	    56 	 0.03984 	 0.00418 	 m..s
   44 	    57 	 0.04528 	 0.00420 	 m..s
   20 	    58 	 0.04234 	 0.00422 	 m..s
   59 	    59 	 0.04738 	 0.00424 	 m..s
   14 	    60 	 0.04197 	 0.00440 	 m..s
    2 	    61 	 0.03785 	 0.00454 	 m..s
   43 	    62 	 0.04522 	 0.00455 	 m..s
   24 	    63 	 0.04257 	 0.00459 	 m..s
   45 	    64 	 0.04531 	 0.00460 	 m..s
   50 	    65 	 0.04560 	 0.00461 	 m..s
   49 	    66 	 0.04543 	 0.00461 	 m..s
   36 	    67 	 0.04441 	 0.00467 	 m..s
   42 	    68 	 0.04495 	 0.00469 	 m..s
    9 	    69 	 0.03987 	 0.00470 	 m..s
   35 	    70 	 0.04433 	 0.00473 	 m..s
   47 	    71 	 0.04534 	 0.00477 	 m..s
   72 	    72 	 0.05064 	 0.00478 	 m..s
   80 	    73 	 0.05141 	 0.00485 	 m..s
   26 	    74 	 0.04312 	 0.00500 	 m..s
    3 	    75 	 0.03787 	 0.00501 	 m..s
   85 	    76 	 0.05506 	 0.00514 	 m..s
   41 	    77 	 0.04492 	 0.00551 	 m..s
   64 	    78 	 0.04836 	 0.00604 	 m..s
   13 	    79 	 0.04193 	 0.00680 	 m..s
   12 	    80 	 0.04115 	 0.00694 	 m..s
   31 	    81 	 0.04388 	 0.00739 	 m..s
   25 	    82 	 0.04311 	 0.00804 	 m..s
   17 	    83 	 0.04229 	 0.01152 	 m..s
   32 	    84 	 0.04389 	 0.01271 	 m..s
   86 	    85 	 0.05575 	 0.02050 	 m..s
   89 	    86 	 0.07083 	 0.02430 	 m..s
   82 	    87 	 0.05298 	 0.02863 	 ~...
   70 	    88 	 0.05039 	 0.02916 	 ~...
   84 	    89 	 0.05490 	 0.02920 	 ~...
   93 	    90 	 0.14318 	 0.12715 	 ~...
   91 	    91 	 0.12477 	 0.12799 	 ~...
   94 	    92 	 0.14408 	 0.13094 	 ~...
   90 	    93 	 0.11541 	 0.13140 	 ~...
   95 	    94 	 0.14669 	 0.13806 	 ~...
   92 	    95 	 0.13114 	 0.14041 	 ~...
   99 	    96 	 0.17177 	 0.17202 	 ~...
  100 	    97 	 0.17518 	 0.17271 	 ~...
   96 	    98 	 0.15371 	 0.17434 	 ~...
   97 	    99 	 0.15743 	 0.17792 	 ~...
  104 	   100 	 0.19917 	 0.18551 	 ~...
   98 	   101 	 0.15889 	 0.18924 	 m..s
  102 	   102 	 0.19343 	 0.19630 	 ~...
  106 	   103 	 0.20506 	 0.20171 	 ~...
  103 	   104 	 0.19349 	 0.21048 	 ~...
  109 	   105 	 0.22260 	 0.22857 	 ~...
  105 	   106 	 0.20381 	 0.22949 	 ~...
  108 	   107 	 0.21498 	 0.23928 	 ~...
  111 	   108 	 0.23756 	 0.24645 	 ~...
  110 	   109 	 0.22922 	 0.25269 	 ~...
  107 	   110 	 0.20586 	 0.25972 	 m..s
  114 	   111 	 0.26859 	 0.26879 	 ~...
  101 	   112 	 0.19124 	 0.27244 	 m..s
  112 	   113 	 0.25781 	 0.28454 	 ~...
  113 	   114 	 0.26483 	 0.29618 	 m..s
  118 	   115 	 0.28623 	 0.30562 	 ~...
  116 	   116 	 0.27569 	 0.31181 	 m..s
  120 	   117 	 0.29863 	 0.31362 	 ~...
  115 	   118 	 0.27515 	 0.31910 	 m..s
  119 	   119 	 0.28724 	 0.33494 	 m..s
  117 	   120 	 0.28439 	 0.34039 	 m..s
==========================================
r_mrr = 0.9919514656066895
r2_mrr = 0.8578981161117554
spearmanr_mrr@5 = 0.8532864451408386
spearmanr_mrr@10 = 0.9466903805732727
spearmanr_mrr@50 = 0.9961021542549133
spearmanr_mrr@100 = 0.9975823163986206
spearmanr_mrr@All = 0.9975176453590393
==========================================
test time: 0.843
Done Testing dataset CoDExSmall
total time taken: 113.06114077568054
training time taken: 100.6247034072876
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_DBpedia50
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: True
rescale_rank_dist_loss: True
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 702
rank avg (pred): 0.428 +- 0.010
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0316601321

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 1140
rank avg (pred): 0.310 +- 0.004
mrr vals (pred, true): 0.000, 0.184
batch losses (mrrl, rdl): 0.0, 0.0229378939

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 403
rank avg (pred): 0.530 +- 0.256
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0110264616

Epoch over!
epoch time: 20.466

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 272
rank avg (pred): 0.344 +- 0.305
mrr vals (pred, true): 0.000, 0.111
batch losses (mrrl, rdl): 0.0, 0.0037621434

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 525
rank avg (pred): 0.476 +- 0.296
mrr vals (pred, true): 0.000, 0.098
batch losses (mrrl, rdl): 0.0, 0.008097833

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 306
rank avg (pred): 0.325 +- 0.312
mrr vals (pred, true): 0.000, 0.123
batch losses (mrrl, rdl): 0.0, 0.0094621126

Epoch over!
epoch time: 19.972

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 460
rank avg (pred): 0.521 +- 0.277
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 2.4930546284, 0.0017807466

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.171 +- 0.337
mrr vals (pred, true): 0.002, 0.129
batch losses (mrrl, rdl): 28.1643810272, 0.072495155

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 899
rank avg (pred): 0.557 +- 0.244
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 2.4924206734, 0.1396675855

Epoch over!
epoch time: 20.064

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 658
rank avg (pred): 0.459 +- 0.243
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 2.4921703339, 0.005159229

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 233
rank avg (pred): 0.538 +- 0.248
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 2.4956762791, 0.0090179238

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 562
rank avg (pred): 0.307 +- 0.373
mrr vals (pred, true): 0.000, 0.081
batch losses (mrrl, rdl): 3.4577519894, 0.053930115

Epoch over!
epoch time: 18.916

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 501
rank avg (pred): 0.232 +- 0.411
mrr vals (pred, true): 0.003, 0.107
batch losses (mrrl, rdl): 16.9305438995, 0.0324786678

running batch: 500 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 881
rank avg (pred): 0.509 +- 0.281
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 2.4949026108, 0.0028892516

running batch: 1000 / 1094 and superbatch(1); data from DBpedia50, run 2.1, exp 1045
rank avg (pred): 0.529 +- 0.265
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 2.4937953949, 0.0022885501

Epoch over!
epoch time: 19.357

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.265 +- 0.414
mrr vals (pred, true): 0.001, 0.002

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   22 	     0 	 9e-0500 	 0.00014 	 ~...
   56 	     1 	 9e-0500 	 0.00014 	 ~...
    8 	     2 	 9e-0500 	 0.00014 	 ~...
   84 	     3 	 0.00079 	 0.00015 	 ~...
   68 	     4 	 0.00010 	 0.00016 	 ~...
   70 	     5 	 0.00010 	 0.00016 	 ~...
    5 	     6 	 9e-0500 	 0.00017 	 ~...
   26 	     7 	 9e-0500 	 0.00017 	 ~...
   33 	     8 	 9e-0500 	 0.00018 	 ~...
   16 	     9 	 9e-0500 	 0.00018 	 ~...
   62 	    10 	 9e-0500 	 0.00018 	 ~...
   69 	    11 	 0.00010 	 0.00019 	 ~...
   44 	    12 	 9e-0500 	 0.00019 	 ~...
    9 	    13 	 9e-0500 	 0.00021 	 ~...
   28 	    14 	 9e-0500 	 0.00021 	 ~...
   51 	    15 	 9e-0500 	 0.00021 	 ~...
   72 	    16 	 0.00010 	 0.00021 	 ~...
   45 	    17 	 9e-0500 	 0.00021 	 ~...
   23 	    18 	 9e-0500 	 0.00021 	 ~...
   27 	    19 	 9e-0500 	 0.00022 	 ~...
   74 	    20 	 0.00010 	 0.00022 	 ~...
   50 	    21 	 9e-0500 	 0.00023 	 ~...
    3 	    22 	 9e-0500 	 0.00023 	 ~...
   20 	    23 	 9e-0500 	 0.00024 	 ~...
   15 	    24 	 9e-0500 	 0.00024 	 ~...
   67 	    25 	 9e-0500 	 0.00024 	 ~...
   34 	    26 	 9e-0500 	 0.00024 	 ~...
   41 	    27 	 9e-0500 	 0.00024 	 ~...
   38 	    28 	 9e-0500 	 0.00024 	 ~...
   11 	    29 	 9e-0500 	 0.00025 	 ~...
   17 	    30 	 9e-0500 	 0.00025 	 ~...
   64 	    31 	 9e-0500 	 0.00025 	 ~...
   71 	    32 	 0.00010 	 0.00026 	 ~...
   46 	    33 	 9e-0500 	 0.00026 	 ~...
   47 	    34 	 9e-0500 	 0.00027 	 ~...
   18 	    35 	 9e-0500 	 0.00027 	 ~...
   65 	    36 	 9e-0500 	 0.00028 	 ~...
   63 	    37 	 9e-0500 	 0.00028 	 ~...
   55 	    38 	 9e-0500 	 0.00028 	 ~...
   31 	    39 	 9e-0500 	 0.00029 	 ~...
   75 	    40 	 0.00010 	 0.00029 	 ~...
   14 	    41 	 9e-0500 	 0.00029 	 ~...
   54 	    42 	 9e-0500 	 0.00030 	 ~...
   24 	    43 	 9e-0500 	 0.00030 	 ~...
   35 	    44 	 9e-0500 	 0.00030 	 ~...
   59 	    45 	 9e-0500 	 0.00031 	 ~...
   73 	    46 	 0.00010 	 0.00031 	 ~...
    7 	    47 	 9e-0500 	 0.00032 	 ~...
    2 	    48 	 9e-0500 	 0.00032 	 ~...
   30 	    49 	 9e-0500 	 0.00033 	 ~...
   19 	    50 	 9e-0500 	 0.00034 	 ~...
    1 	    51 	 9e-0500 	 0.00034 	 ~...
   37 	    52 	 9e-0500 	 0.00034 	 ~...
   52 	    53 	 9e-0500 	 0.00035 	 ~...
   40 	    54 	 9e-0500 	 0.00036 	 ~...
   36 	    55 	 9e-0500 	 0.00037 	 ~...
   42 	    56 	 9e-0500 	 0.00037 	 ~...
   66 	    57 	 9e-0500 	 0.00038 	 ~...
   12 	    58 	 9e-0500 	 0.00038 	 ~...
   43 	    59 	 9e-0500 	 0.00039 	 ~...
   58 	    60 	 9e-0500 	 0.00039 	 ~...
   48 	    61 	 9e-0500 	 0.00039 	 ~...
   49 	    62 	 9e-0500 	 0.00039 	 ~...
   39 	    63 	 9e-0500 	 0.00043 	 ~...
   29 	    64 	 9e-0500 	 0.00043 	 ~...
    6 	    65 	 9e-0500 	 0.00046 	 ~...
   25 	    66 	 9e-0500 	 0.00049 	 ~...
   10 	    67 	 9e-0500 	 0.00050 	 ~...
   61 	    68 	 9e-0500 	 0.00052 	 ~...
    0 	    69 	 9e-0500 	 0.00053 	 ~...
   21 	    70 	 9e-0500 	 0.00055 	 ~...
   32 	    71 	 9e-0500 	 0.00056 	 ~...
   57 	    72 	 9e-0500 	 0.00057 	 ~...
   60 	    73 	 9e-0500 	 0.00076 	 ~...
   53 	    74 	 9e-0500 	 0.00079 	 ~...
    4 	    75 	 9e-0500 	 0.00084 	 ~...
   13 	    76 	 9e-0500 	 0.00129 	 ~...
   85 	    77 	 0.00095 	 0.00171 	 ~...
   87 	    78 	 0.00111 	 0.00789 	 ~...
  106 	    79 	 0.07663 	 0.04857 	 ~...
   78 	    80 	 0.00029 	 0.06447 	 m..s
  102 	    81 	 0.05836 	 0.07961 	 ~...
   96 	    82 	 0.04964 	 0.08371 	 m..s
   80 	    83 	 0.00041 	 0.08488 	 m..s
   95 	    84 	 0.04862 	 0.08803 	 m..s
  103 	    85 	 0.07014 	 0.08860 	 ~...
   77 	    86 	 0.00028 	 0.08867 	 m..s
  107 	    87 	 0.07757 	 0.09060 	 ~...
   93 	    88 	 0.04719 	 0.09126 	 m..s
   94 	    89 	 0.04821 	 0.09149 	 m..s
   92 	    90 	 0.04376 	 0.09180 	 m..s
   82 	    91 	 0.00045 	 0.09286 	 m..s
   83 	    92 	 0.00046 	 0.09363 	 m..s
  105 	    93 	 0.07366 	 0.09460 	 ~...
   79 	    94 	 0.00031 	 0.09532 	 m..s
  114 	    95 	 0.15772 	 0.09535 	 m..s
  113 	    96 	 0.14227 	 0.09764 	 m..s
   81 	    97 	 0.00043 	 0.10902 	 MISS
  104 	    98 	 0.07333 	 0.12319 	 m..s
   76 	    99 	 0.00017 	 0.13164 	 MISS
   86 	   100 	 0.00108 	 0.13674 	 MISS
   91 	   101 	 0.04064 	 0.13680 	 m..s
   99 	   102 	 0.05395 	 0.13810 	 m..s
   89 	   103 	 0.00171 	 0.14653 	 MISS
   88 	   104 	 0.00169 	 0.15318 	 MISS
  117 	   105 	 0.17345 	 0.16378 	 ~...
   90 	   106 	 0.03895 	 0.16586 	 MISS
  100 	   107 	 0.05512 	 0.18204 	 MISS
  110 	   108 	 0.09646 	 0.18503 	 m..s
   97 	   109 	 0.05048 	 0.21574 	 MISS
   98 	   110 	 0.05092 	 0.21666 	 MISS
  101 	   111 	 0.05826 	 0.22657 	 MISS
  108 	   112 	 0.07949 	 0.26914 	 MISS
  109 	   113 	 0.08324 	 0.28706 	 MISS
  119 	   114 	 0.19573 	 0.30486 	 MISS
  120 	   115 	 0.21205 	 0.31033 	 m..s
  118 	   116 	 0.17825 	 0.31527 	 MISS
  116 	   117 	 0.17042 	 0.31960 	 MISS
  115 	   118 	 0.16470 	 0.33534 	 MISS
  111 	   119 	 0.11856 	 0.34767 	 MISS
  112 	   120 	 0.13219 	 0.38203 	 MISS
==========================================
r_mrr = 0.8231906890869141
r2_mrr = 0.46403467655181885
spearmanr_mrr@5 = 0.980778694152832
spearmanr_mrr@10 = 0.9837188720703125
spearmanr_mrr@50 = 0.9638409614562988
spearmanr_mrr@100 = 0.964859127998352
spearmanr_mrr@All = 0.9658321142196655
==========================================
test time: 0.869
Done Testing dataset DBpedia50
total time taken: 107.25589537620544
training time taken: 99.7061779499054
TWIG out ;))
Using random seed: 17
Starting TWIG!
Loading datasets
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8606280476482954
the save name prefix for this run is:  chkpt-ID_8606280476482954_tag_TWIG-job_OpenEA
running TWIG with settings:
kge_model_name: ComplEx
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: True
rescale_rank_dist_loss: True
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 702
rank avg (pred): 0.432 +- 0.010
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0293457098

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 1140
rank avg (pred): 0.308 +- 0.273
mrr vals (pred, true): 0.000, 0.183
batch losses (mrrl, rdl): 0.0, 0.0040062508

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 403
rank avg (pred): 0.498 +- 0.308
mrr vals (pred, true): 0.001, 0.002
batch losses (mrrl, rdl): 0.0, 0.000249706

Epoch over!
epoch time: 16.573

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 272
rank avg (pred): 0.381 +- 0.312
mrr vals (pred, true): 0.002, 0.076
batch losses (mrrl, rdl): 0.0, 0.0012805767

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 525
rank avg (pred): 0.380 +- 0.315
mrr vals (pred, true): 0.014, 0.064
batch losses (mrrl, rdl): 0.0, 0.0007437656

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 306
rank avg (pred): 0.358 +- 0.304
mrr vals (pred, true): 0.006, 0.077
batch losses (mrrl, rdl): 0.0, 0.0013991134

Epoch over!
epoch time: 16.272

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 460
rank avg (pred): 0.489 +- 0.298
mrr vals (pred, true): 0.002, 0.001
batch losses (mrrl, rdl): 2.3182737827, 0.0005275973

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 1079
rank avg (pred): 0.511 +- 0.360
mrr vals (pred, true): 0.072, 0.125
batch losses (mrrl, rdl): 4.6901392937, 0.1049693301

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 899
rank avg (pred): 0.443 +- 0.381
mrr vals (pred, true): 0.119, 0.000
batch losses (mrrl, rdl): 4.7496638298, 0.3243204355

Epoch over!
epoch time: 17.372

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 658
rank avg (pred): 0.451 +- 0.327
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 0.0018963784, 0.0111227445

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 233
rank avg (pred): 0.451 +- 0.324
mrr vals (pred, true): 0.035, 0.001
batch losses (mrrl, rdl): 0.2337993383, 0.0089819701

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 562
rank avg (pred): 0.488 +- 0.346
mrr vals (pred, true): 0.077, 0.055
batch losses (mrrl, rdl): 0.932926178, 0.0371015072

Epoch over!
epoch time: 16.277

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 501
rank avg (pred): 0.274 +- 0.414
mrr vals (pred, true): 0.164, 0.165
batch losses (mrrl, rdl): 0.0025434073, 0.0177474096

running batch: 500 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 881
rank avg (pred): 0.495 +- 0.342
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.2166360915, 0.001623732

running batch: 1000 / 1094 and superbatch(1); data from OpenEA, run 2.1, exp 1045
rank avg (pred): 0.485 +- 0.336
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0753641278, 0.0024802634

Epoch over!
epoch time: 16.57

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.452 +- 0.344
mrr vals (pred, true): 0.086, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   97 	     0 	 0.10641 	 0.00031 	 MISS
   40 	     1 	 0.03338 	 0.00045 	 m..s
   76 	     2 	 0.04465 	 0.00047 	 m..s
    0 	     3 	 0.01425 	 0.00051 	 ~...
   77 	     4 	 0.04504 	 0.00052 	 m..s
   19 	     5 	 0.02511 	 0.00053 	 ~...
   26 	     6 	 0.02862 	 0.00053 	 ~...
   87 	     7 	 0.05019 	 0.00055 	 m..s
   61 	     8 	 0.03886 	 0.00055 	 m..s
   56 	     9 	 0.03790 	 0.00056 	 m..s
   27 	    10 	 0.02899 	 0.00056 	 ~...
   72 	    11 	 0.04337 	 0.00057 	 m..s
   69 	    12 	 0.04261 	 0.00057 	 m..s
   58 	    13 	 0.03829 	 0.00057 	 m..s
    6 	    14 	 0.02268 	 0.00057 	 ~...
    2 	    15 	 0.01799 	 0.00058 	 ~...
   46 	    16 	 0.03417 	 0.00059 	 m..s
   18 	    17 	 0.02489 	 0.00059 	 ~...
   78 	    18 	 0.04532 	 0.00060 	 m..s
   45 	    19 	 0.03417 	 0.00060 	 m..s
   12 	    20 	 0.02391 	 0.00061 	 ~...
   48 	    21 	 0.03524 	 0.00064 	 m..s
   67 	    22 	 0.04207 	 0.00068 	 m..s
   41 	    23 	 0.03370 	 0.00069 	 m..s
   35 	    24 	 0.03152 	 0.00069 	 m..s
   59 	    25 	 0.03858 	 0.00070 	 m..s
    1 	    26 	 0.01639 	 0.00071 	 ~...
   36 	    27 	 0.03157 	 0.00072 	 m..s
   85 	    28 	 0.04888 	 0.00073 	 m..s
   34 	    29 	 0.03140 	 0.00073 	 m..s
   89 	    30 	 0.05942 	 0.00075 	 m..s
   47 	    31 	 0.03464 	 0.00076 	 m..s
    8 	    32 	 0.02289 	 0.00076 	 ~...
   74 	    33 	 0.04373 	 0.00076 	 m..s
   16 	    34 	 0.02478 	 0.00077 	 ~...
   24 	    35 	 0.02687 	 0.00078 	 ~...
   30 	    36 	 0.02984 	 0.00078 	 ~...
   37 	    37 	 0.03203 	 0.00080 	 m..s
   38 	    38 	 0.03294 	 0.00081 	 m..s
   75 	    39 	 0.04396 	 0.00081 	 m..s
   22 	    40 	 0.02651 	 0.00087 	 ~...
   49 	    41 	 0.03553 	 0.00088 	 m..s
   25 	    42 	 0.02799 	 0.00091 	 ~...
   42 	    43 	 0.03373 	 0.00092 	 m..s
   83 	    44 	 0.04696 	 0.00101 	 m..s
   55 	    45 	 0.03772 	 0.00103 	 m..s
    4 	    46 	 0.02145 	 0.00104 	 ~...
   20 	    47 	 0.02523 	 0.00106 	 ~...
   33 	    48 	 0.03105 	 0.00108 	 ~...
   32 	    49 	 0.03067 	 0.00112 	 ~...
   39 	    50 	 0.03324 	 0.00115 	 m..s
   51 	    51 	 0.03635 	 0.00116 	 m..s
   64 	    52 	 0.04046 	 0.00116 	 m..s
   63 	    53 	 0.04036 	 0.00118 	 m..s
   23 	    54 	 0.02666 	 0.00120 	 ~...
    5 	    55 	 0.02206 	 0.00122 	 ~...
   52 	    56 	 0.03661 	 0.00123 	 m..s
   88 	    57 	 0.05220 	 0.00133 	 m..s
   44 	    58 	 0.03411 	 0.00136 	 m..s
   82 	    59 	 0.04626 	 0.00137 	 m..s
   70 	    60 	 0.04272 	 0.00139 	 m..s
    9 	    61 	 0.02303 	 0.00142 	 ~...
   73 	    62 	 0.04338 	 0.00142 	 m..s
   13 	    63 	 0.02400 	 0.00143 	 ~...
   43 	    64 	 0.03397 	 0.00146 	 m..s
   29 	    65 	 0.02961 	 0.00153 	 ~...
   54 	    66 	 0.03752 	 0.00156 	 m..s
   80 	    67 	 0.04565 	 0.00161 	 m..s
   21 	    68 	 0.02575 	 0.00170 	 ~...
   28 	    69 	 0.02919 	 0.00186 	 ~...
    3 	    70 	 0.02108 	 0.00194 	 ~...
   15 	    71 	 0.02467 	 0.00195 	 ~...
   11 	    72 	 0.02313 	 0.00196 	 ~...
   84 	    73 	 0.04752 	 0.00199 	 m..s
   53 	    74 	 0.03661 	 0.00200 	 m..s
   31 	    75 	 0.03048 	 0.00200 	 ~...
   81 	    76 	 0.04572 	 0.00224 	 m..s
   95 	    77 	 0.08568 	 0.00249 	 m..s
   90 	    78 	 0.06667 	 0.00459 	 m..s
   57 	    79 	 0.03825 	 0.04959 	 ~...
   71 	    80 	 0.04288 	 0.05253 	 ~...
   50 	    81 	 0.03557 	 0.06381 	 ~...
   65 	    82 	 0.04136 	 0.07113 	 ~...
   62 	    83 	 0.04023 	 0.07360 	 m..s
   10 	    84 	 0.02310 	 0.07464 	 m..s
   86 	    85 	 0.04949 	 0.07469 	 ~...
   94 	    86 	 0.08144 	 0.07542 	 ~...
   68 	    87 	 0.04249 	 0.07636 	 m..s
   14 	    88 	 0.02426 	 0.07648 	 m..s
   91 	    89 	 0.07787 	 0.07650 	 ~...
    7 	    90 	 0.02282 	 0.07667 	 m..s
   60 	    91 	 0.03876 	 0.07719 	 m..s
   17 	    92 	 0.02484 	 0.07971 	 m..s
  111 	    93 	 0.17915 	 0.08036 	 m..s
   93 	    94 	 0.07997 	 0.08403 	 ~...
   92 	    95 	 0.07869 	 0.08523 	 ~...
   66 	    96 	 0.04151 	 0.08922 	 m..s
   79 	    97 	 0.04559 	 0.08956 	 m..s
  104 	    98 	 0.12356 	 0.09360 	 ~...
   96 	    99 	 0.10284 	 0.09838 	 ~...
  112 	   100 	 0.18440 	 0.10039 	 m..s
  101 	   101 	 0.11429 	 0.11871 	 ~...
   97 	   102 	 0.10641 	 0.12857 	 ~...
  100 	   103 	 0.10728 	 0.12860 	 ~...
  113 	   104 	 0.19857 	 0.13637 	 m..s
   99 	   105 	 0.10675 	 0.13818 	 m..s
  107 	   106 	 0.15094 	 0.17068 	 ~...
  103 	   107 	 0.12167 	 0.17901 	 m..s
  102 	   108 	 0.11990 	 0.18394 	 m..s
  106 	   109 	 0.12786 	 0.18588 	 m..s
  105 	   110 	 0.12572 	 0.18649 	 m..s
  109 	   111 	 0.16203 	 0.21233 	 m..s
  110 	   112 	 0.17296 	 0.21589 	 m..s
  108 	   113 	 0.15968 	 0.22307 	 m..s
  119 	   114 	 0.23405 	 0.27717 	 m..s
  118 	   115 	 0.23091 	 0.28122 	 m..s
  115 	   116 	 0.21550 	 0.28885 	 m..s
  114 	   117 	 0.20985 	 0.29363 	 m..s
  117 	   118 	 0.21944 	 0.31774 	 m..s
  116 	   119 	 0.21740 	 0.34668 	 MISS
  120 	   120 	 0.23472 	 0.34919 	 MISS
==========================================
r_mrr = 0.9014153480529785
r2_mrr = 0.7326350808143616
spearmanr_mrr@5 = 0.9618135094642639
spearmanr_mrr@10 = 0.9719612002372742
spearmanr_mrr@50 = 0.9665573239326477
spearmanr_mrr@100 = 0.978045642375946
spearmanr_mrr@All = 0.9778791666030884
==========================================
test time: 0.719
Done Testing dataset OpenEA
total time taken: 111.13712811470032
training time taken: 83.84265542030334
TWIG out ;))
