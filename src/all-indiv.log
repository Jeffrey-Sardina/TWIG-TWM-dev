=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.423 +- 0.005
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0, 0.0001304942

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.466 +- 0.000
mrr vals (pred, true): 0.016, 0.052
batch losses (mrrl, rdl): 0.0, 9.74896e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.432 +- 0.190
mrr vals (pred, true): 0.116, 0.045
batch losses (mrrl, rdl): 0.0, 2.26595e-05

Epoch over!
epoch time: 12.69

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 794
rank avg (pred): 0.423 +- 0.213
mrr vals (pred, true): 0.153, 0.050
batch losses (mrrl, rdl): 0.0, 1.61476e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 58
rank avg (pred): 0.172 +- 0.079
mrr vals (pred, true): 0.173, 0.226
batch losses (mrrl, rdl): 0.0, 2.62956e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 898
rank avg (pred): 0.486 +- 0.230
mrr vals (pred, true): 0.142, 0.061
batch losses (mrrl, rdl): 0.0, 0.0002752846

Epoch over!
epoch time: 12.414

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1022
rank avg (pred): 0.416 +- 0.273
mrr vals (pred, true): 0.211, 0.039
batch losses (mrrl, rdl): 0.0, 7.3905e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1206
rank avg (pred): 0.421 +- 0.242
mrr vals (pred, true): 0.153, 0.046
batch losses (mrrl, rdl): 0.0, 8.3516e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 970
rank avg (pred): 0.456 +- 0.231
mrr vals (pred, true): 0.127, 0.051
batch losses (mrrl, rdl): 0.0, 3.08665e-05

Epoch over!
epoch time: 12.108

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 417
rank avg (pred): 0.408 +- 0.271
mrr vals (pred, true): 0.185, 0.064
batch losses (mrrl, rdl): 0.0, 5.1223e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 16
rank avg (pred): 0.167 +- 0.107
mrr vals (pred, true): 0.208, 0.184
batch losses (mrrl, rdl): 0.0, 2.79066e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 170
rank avg (pred): 0.418 +- 0.263
mrr vals (pred, true): 0.135, 0.049
batch losses (mrrl, rdl): 0.0, 4.6772e-06

Epoch over!
epoch time: 11.827

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.418 +- 0.263
mrr vals (pred, true): 0.131, 0.052
batch losses (mrrl, rdl): 0.0, 3.3684e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 100
rank avg (pred): 0.420 +- 0.259
mrr vals (pred, true): 0.122, 0.040
batch losses (mrrl, rdl): 0.0, 7.0788e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 797
rank avg (pred): 0.438 +- 0.258
mrr vals (pred, true): 0.112, 0.052
batch losses (mrrl, rdl): 0.0, 1.79219e-05

Epoch over!
epoch time: 11.837

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1157
rank avg (pred): 0.483 +- 0.233
mrr vals (pred, true): 0.085, 0.027
batch losses (mrrl, rdl): 0.0121881617, 7.3078e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1130
rank avg (pred): 0.473 +- 0.243
mrr vals (pred, true): 0.080, 0.043
batch losses (mrrl, rdl): 0.0091102775, 3.38937e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.470 +- 0.208
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 8.6847e-06, 4.82942e-05

Epoch over!
epoch time: 12.133

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 484
rank avg (pred): 0.479 +- 0.230
mrr vals (pred, true): 0.057, 0.043
batch losses (mrrl, rdl): 0.0005168078, 5.49577e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 428
rank avg (pred): 0.495 +- 0.212
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001319826, 0.0001368801

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 875
rank avg (pred): 0.489 +- 0.226
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001360005, 6.19297e-05

Epoch over!
epoch time: 12.109

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 205
rank avg (pred): 0.491 +- 0.226
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 6.38744e-05, 7.3196e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1073
rank avg (pred): 0.090 +- 0.071
mrr vals (pred, true): 0.275, 0.272
batch losses (mrrl, rdl): 0.0001212214, 6.8108e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 811
rank avg (pred): 0.083 +- 0.067
mrr vals (pred, true): 0.290, 0.401
batch losses (mrrl, rdl): 0.1231471524, 1.75366e-05

Epoch over!
epoch time: 12.116

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 144
rank avg (pred): 0.460 +- 0.206
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 9.35939e-05, 3.44572e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 320
rank avg (pred): 0.202 +- 0.154
mrr vals (pred, true): 0.219, 0.197
batch losses (mrrl, rdl): 0.0047923601, 1.07733e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 866
rank avg (pred): 0.491 +- 0.222
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 4.96302e-05, 7.16055e-05

Epoch over!
epoch time: 12.495

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 865
rank avg (pred): 0.477 +- 0.238
mrr vals (pred, true): 0.059, 0.049
batch losses (mrrl, rdl): 0.0008192234, 2.38438e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 284
rank avg (pred): 0.216 +- 0.164
mrr vals (pred, true): 0.251, 0.225
batch losses (mrrl, rdl): 0.006665417, 4.77982e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1032
rank avg (pred): 0.453 +- 0.201
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 4.45187e-05, 2.44352e-05

Epoch over!
epoch time: 12.442

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 806
rank avg (pred): 0.469 +- 0.202
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 5.24824e-05, 6.02722e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1162
rank avg (pred): 0.450 +- 0.159
mrr vals (pred, true): 0.042, 0.036
batch losses (mrrl, rdl): 0.0006171972, 2.64771e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 268
rank avg (pred): 0.284 +- 0.204
mrr vals (pred, true): 0.210, 0.217
batch losses (mrrl, rdl): 0.0004812586, 0.0002585044

Epoch over!
epoch time: 12.081

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 178
rank avg (pred): 0.458 +- 0.185
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001481669, 4.12159e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 843
rank avg (pred): 0.463 +- 0.197
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 3.45957e-05, 1.63379e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 660
rank avg (pred): 0.434 +- 0.147
mrr vals (pred, true): 0.041, 0.052
batch losses (mrrl, rdl): 0.0008226391, 4.4094e-05

Epoch over!
epoch time: 12.057

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 815
rank avg (pred): 0.074 +- 0.057
mrr vals (pred, true): 0.358, 0.539
batch losses (mrrl, rdl): 0.3301436901, 1.27383e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 168
rank avg (pred): 0.447 +- 0.186
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.391e-07, 2.43428e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 120
rank avg (pred): 0.438 +- 0.189
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002708433, 2.39809e-05

Epoch over!
epoch time: 11.987

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 485
rank avg (pred): 0.457 +- 0.181
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 0.000105868, 4.48463e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 309
rank avg (pred): 0.284 +- 0.201
mrr vals (pred, true): 0.228, 0.212
batch losses (mrrl, rdl): 0.0025757032, 0.0002882885

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1187
rank avg (pred): 0.449 +- 0.175
mrr vals (pred, true): 0.047, 0.032
batch losses (mrrl, rdl): 6.65956e-05, 2.60743e-05

Epoch over!
epoch time: 12.066

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 186
rank avg (pred): 0.442 +- 0.159
mrr vals (pred, true): 0.041, 0.059
batch losses (mrrl, rdl): 0.0007513884, 4.30368e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 329
rank avg (pred): 0.462 +- 0.165
mrr vals (pred, true): 0.041, 0.047
batch losses (mrrl, rdl): 0.0008452939, 4.51264e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 769
rank avg (pred): 0.433 +- 0.174
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 4.9207e-06, 2.45556e-05

Epoch over!
epoch time: 11.916

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.154 +- 0.106
mrr vals (pred, true): 0.290, 0.295

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04651 	 0.02393 	 ~...
    7 	     1 	 0.04884 	 0.02414 	 ~...
    4 	     2 	 0.04839 	 0.02424 	 ~...
    8 	     3 	 0.04884 	 0.02476 	 ~...
    2 	     4 	 0.04712 	 0.02481 	 ~...
   14 	     5 	 0.05040 	 0.02543 	 ~...
   10 	     6 	 0.04910 	 0.02544 	 ~...
    3 	     7 	 0.04718 	 0.02552 	 ~...
   13 	     8 	 0.04954 	 0.02570 	 ~...
   15 	     9 	 0.05168 	 0.02571 	 ~...
    6 	    10 	 0.04882 	 0.02579 	 ~...
    0 	    11 	 0.04469 	 0.02609 	 ~...
    5 	    12 	 0.04854 	 0.02623 	 ~...
    9 	    13 	 0.04894 	 0.02633 	 ~...
   22 	    14 	 0.05489 	 0.02721 	 ~...
   11 	    15 	 0.04911 	 0.02758 	 ~...
   12 	    16 	 0.04954 	 0.02767 	 ~...
   21 	    17 	 0.05453 	 0.02843 	 ~...
   23 	    18 	 0.05490 	 0.03512 	 ~...
   27 	    19 	 0.05502 	 0.03647 	 ~...
   19 	    20 	 0.05450 	 0.03695 	 ~...
   20 	    21 	 0.05450 	 0.03730 	 ~...
   38 	    22 	 0.05607 	 0.03889 	 ~...
   34 	    23 	 0.05570 	 0.03923 	 ~...
   16 	    24 	 0.05204 	 0.03937 	 ~...
   36 	    25 	 0.05597 	 0.03973 	 ~...
   92 	    26 	 0.07356 	 0.04046 	 m..s
   43 	    27 	 0.05700 	 0.04050 	 ~...
   82 	    28 	 0.05934 	 0.04084 	 ~...
   29 	    29 	 0.05533 	 0.04091 	 ~...
   33 	    30 	 0.05567 	 0.04101 	 ~...
   32 	    31 	 0.05546 	 0.04126 	 ~...
   25 	    32 	 0.05493 	 0.04129 	 ~...
   26 	    33 	 0.05502 	 0.04139 	 ~...
   30 	    34 	 0.05541 	 0.04147 	 ~...
   17 	    35 	 0.05220 	 0.04169 	 ~...
   42 	    36 	 0.05618 	 0.04209 	 ~...
   87 	    37 	 0.06699 	 0.04221 	 ~...
   86 	    38 	 0.06251 	 0.04224 	 ~...
   43 	    39 	 0.05700 	 0.04224 	 ~...
   91 	    40 	 0.07307 	 0.04224 	 m..s
   81 	    41 	 0.05916 	 0.04240 	 ~...
   28 	    42 	 0.05533 	 0.04270 	 ~...
   43 	    43 	 0.05700 	 0.04276 	 ~...
   24 	    44 	 0.05492 	 0.04279 	 ~...
   41 	    45 	 0.05611 	 0.04302 	 ~...
   31 	    46 	 0.05543 	 0.04332 	 ~...
   90 	    47 	 0.07300 	 0.04333 	 ~...
   43 	    48 	 0.05700 	 0.04369 	 ~...
   43 	    49 	 0.05700 	 0.04386 	 ~...
   43 	    50 	 0.05700 	 0.04388 	 ~...
   43 	    51 	 0.05700 	 0.04418 	 ~...
   43 	    52 	 0.05700 	 0.04444 	 ~...
   43 	    53 	 0.05700 	 0.04460 	 ~...
   43 	    54 	 0.05700 	 0.04473 	 ~...
   43 	    55 	 0.05700 	 0.04514 	 ~...
   43 	    56 	 0.05700 	 0.04549 	 ~...
   43 	    57 	 0.05700 	 0.04601 	 ~...
   35 	    58 	 0.05578 	 0.04609 	 ~...
   43 	    59 	 0.05700 	 0.04652 	 ~...
   43 	    60 	 0.05700 	 0.04658 	 ~...
   43 	    61 	 0.05700 	 0.04659 	 ~...
   84 	    62 	 0.06133 	 0.04757 	 ~...
   43 	    63 	 0.05700 	 0.04760 	 ~...
   94 	    64 	 0.07507 	 0.04777 	 ~...
   43 	    65 	 0.05700 	 0.04815 	 ~...
   43 	    66 	 0.05700 	 0.04815 	 ~...
   43 	    67 	 0.05700 	 0.04819 	 ~...
   83 	    68 	 0.05936 	 0.04843 	 ~...
   95 	    69 	 0.08728 	 0.04894 	 m..s
   93 	    70 	 0.07401 	 0.04896 	 ~...
   43 	    71 	 0.05700 	 0.04952 	 ~...
   43 	    72 	 0.05700 	 0.04953 	 ~...
   40 	    73 	 0.05610 	 0.04959 	 ~...
   85 	    74 	 0.06136 	 0.04969 	 ~...
   43 	    75 	 0.05700 	 0.04974 	 ~...
   43 	    76 	 0.05700 	 0.04978 	 ~...
   43 	    77 	 0.05700 	 0.04989 	 ~...
   43 	    78 	 0.05700 	 0.04990 	 ~...
   43 	    79 	 0.05700 	 0.05007 	 ~...
   89 	    80 	 0.07191 	 0.05012 	 ~...
   43 	    81 	 0.05700 	 0.05014 	 ~...
   43 	    82 	 0.05700 	 0.05082 	 ~...
   43 	    83 	 0.05700 	 0.05085 	 ~...
   43 	    84 	 0.05700 	 0.05086 	 ~...
   88 	    85 	 0.06985 	 0.05092 	 ~...
   80 	    86 	 0.05766 	 0.05178 	 ~...
   43 	    87 	 0.05700 	 0.05199 	 ~...
   43 	    88 	 0.05700 	 0.05259 	 ~...
   39 	    89 	 0.05610 	 0.05269 	 ~...
   43 	    90 	 0.05700 	 0.05275 	 ~...
   43 	    91 	 0.05700 	 0.05333 	 ~...
   18 	    92 	 0.05257 	 0.05340 	 ~...
   43 	    93 	 0.05700 	 0.05355 	 ~...
   43 	    94 	 0.05700 	 0.05479 	 ~...
   37 	    95 	 0.05600 	 0.05634 	 ~...
   97 	    96 	 0.22454 	 0.18803 	 m..s
  101 	    97 	 0.25382 	 0.20836 	 m..s
  102 	    98 	 0.25916 	 0.20872 	 m..s
  103 	    99 	 0.25946 	 0.21171 	 m..s
   96 	   100 	 0.20273 	 0.21637 	 ~...
  104 	   101 	 0.26256 	 0.21805 	 m..s
   98 	   102 	 0.24280 	 0.21849 	 ~...
  105 	   103 	 0.28062 	 0.23847 	 m..s
  110 	   104 	 0.32024 	 0.25215 	 m..s
   99 	   105 	 0.24285 	 0.25298 	 ~...
  100 	   106 	 0.24929 	 0.26711 	 ~...
  107 	   107 	 0.30078 	 0.27410 	 ~...
  116 	   108 	 0.34305 	 0.27441 	 m..s
  113 	   109 	 0.33111 	 0.28106 	 m..s
  114 	   110 	 0.33322 	 0.28160 	 m..s
  108 	   111 	 0.31522 	 0.28472 	 m..s
  109 	   112 	 0.31609 	 0.28533 	 m..s
  118 	   113 	 0.36296 	 0.28534 	 m..s
  117 	   114 	 0.34557 	 0.29334 	 m..s
  111 	   115 	 0.32051 	 0.29484 	 ~...
  106 	   116 	 0.28989 	 0.29550 	 ~...
  115 	   117 	 0.34050 	 0.30397 	 m..s
  112 	   118 	 0.32711 	 0.31636 	 ~...
  120 	   119 	 0.39064 	 0.37350 	 ~...
  119 	   120 	 0.38618 	 0.39525 	 ~...
==========================================
r_mrr = 0.9898871183395386
r2_mrr = 0.9371352195739746
spearmanr_mrr@5 = 0.9756141304969788
spearmanr_mrr@10 = 0.9609623551368713
spearmanr_mrr@50 = 0.9960773587226868
spearmanr_mrr@100 = 0.9972832202911377
spearmanr_mrr@All = 0.9966816306114197
==========================================
test time: 0.408
Done Testing dataset UMLS
total time taken: 189.5190134048462
training time taken: 182.7802290916443
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 508425322870164
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 1122, 973, 460, 838, 863, 909, 21, 504, 464, 89, 865, 842, 266, 1184, 609, 899, 1110, 285, 455, 921, 748, 52, 550, 604, 328, 224, 633, 193, 763, 299, 990, 969, 1019, 1138, 272, 1171, 110, 497, 1146, 42, 696, 1046, 46, 645, 56, 300, 941, 77, 76, 954, 124, 6, 232, 32, 848, 326, 1153, 823, 364, 815, 225, 600, 576, 182, 914, 1173, 797, 1035, 854, 1113, 729, 454, 11, 427, 394, 896, 1126, 939, 140, 857, 416, 866, 214, 817, 279, 1069, 593, 543, 937, 793, 762, 1038, 432, 1060, 888, 711, 362, 818, 956, 1, 469, 142, 625, 377, 578, 506, 329, 839, 895, 929, 556, 101, 1005, 1196, 352, 260, 970, 1116, 322, 55]
valid_ids (0): []
train_ids (1094): [388, 230, 437, 675, 201, 383, 579, 862, 400, 280, 1054, 35, 10, 978, 1157, 652, 1158, 655, 268, 1088, 286, 1037, 367, 639, 318, 674, 515, 241, 1057, 591, 778, 940, 636, 1130, 713, 136, 715, 475, 619, 67, 430, 531, 8, 209, 632, 130, 786, 15, 755, 692, 319, 252, 664, 611, 424, 47, 943, 583, 827, 462, 1174, 294, 491, 1109, 363, 1197, 841, 1103, 790, 338, 1034, 78, 907, 1125, 386, 648, 912, 988, 852, 289, 29, 812, 45, 40, 856, 379, 723, 757, 777, 119, 301, 754, 568, 679, 700, 274, 1190, 993, 1089, 672, 980, 1042, 606, 544, 629, 349, 710, 314, 706, 760, 867, 221, 1136, 66, 61, 396, 1043, 667, 530, 484, 994, 1133, 596, 638, 135, 282, 313, 802, 642, 792, 307, 139, 1049, 137, 1008, 803, 335, 305, 1094, 433, 850, 1092, 602, 936, 378, 178, 492, 255, 1102, 534, 389, 87, 80, 60, 1191, 1093, 1023, 324, 779, 933, 100, 890, 112, 1058, 86, 871, 858, 958, 787, 191, 785, 949, 1017, 740, 668, 566, 85, 1121, 177, 357, 218, 238, 269, 479, 93, 687, 630, 561, 676, 744, 143, 846, 1118, 1135, 722, 292, 864, 188, 97, 892, 1072, 885, 25, 567, 102, 799, 820, 613, 637, 935, 1032, 254, 64, 964, 179, 747, 104, 195, 938, 242, 308, 1131, 944, 624, 985, 31, 81, 62, 804, 49, 735, 989, 107, 1096, 623, 580, 123, 371, 98, 798, 53, 183, 155, 312, 925, 1020, 945, 159, 283, 204, 75, 582, 1140, 766, 16, 950, 444, 1183, 196, 947, 855, 502, 1181, 805, 1214, 540, 281, 1167, 880, 331, 1067, 213, 1175, 273, 733, 824, 96, 3, 800, 791, 458, 1178, 343, 987, 374, 553, 983, 592, 247, 789, 971, 658, 1134, 441, 836, 1112, 1064, 886, 277, 918, 1154, 756, 222, 457, 346, 927, 720, 51, 170, 708, 320, 494, 468, 446, 495, 256, 486, 205, 752, 295, 1099, 878, 659, 702, 961, 761, 897, 948, 923, 1015, 117, 459, 765, 995, 354, 511, 1114, 194, 879, 290, 536, 407, 519, 1198, 192, 0, 125, 1106, 82, 37, 1039, 784, 471, 1193, 1009, 1036, 197, 1212, 529, 133, 1048, 439, 569, 651, 169, 877, 443, 122, 1194, 448, 239, 1172, 926, 38, 922, 1045, 816, 180, 1151, 1074, 1016, 649, 1024, 563, 910, 185, 425, 1101, 50, 677, 783, 128, 772, 418, 919, 1162, 693, 889, 1195, 808, 234, 873, 617, 120, 605, 369, 1176, 208, 1021, 429, 111, 271, 1168, 54, 361, 391, 891, 574, 333, 2, 828, 210, 1003, 344, 1029, 717, 549, 105, 245, 175, 533, 1161, 1170, 421, 240, 1090, 203, 265, 1111, 28, 481, 216, 1031, 138, 882, 874, 43, 1203, 1200, 17, 951, 588, 287, 339, 1041, 202, 235, 554, 1079, 164, 1145, 1210, 671, 466, 653, 745, 814, 1075, 310, 643, 1115, 1071, 911, 1185, 36, 199, 276, 641, 884, 870, 1001, 1107, 275, 351, 48, 616, 689, 33, 631, 325, 734, 622, 463, 115, 152, 306, 1091, 607, 837, 1123, 678, 782, 695, 423, 894, 682, 72, 90, 413, 1025, 825, 581, 560, 957, 1026, 844, 603, 1018, 399, 144, 974, 417, 1211, 171, 380, 258, 392, 725, 296, 703, 132, 24, 20, 627, 924, 1143, 573, 709, 381, 541, 806, 612, 408, 522, 771, 411, 490, 598, 634, 79, 893, 472, 1076, 236, 930, 1055, 1155, 160, 39, 226, 750, 847, 705, 1084, 434, 215, 341, 151, 595, 207, 1204, 116, 71, 304, 385, 233, 996, 704, 154, 906, 517, 438, 1177, 661, 356, 44, 728, 801, 770, 336, 1053, 575, 251, 382, 1105, 330, 1108, 982, 565, 913, 499, 449, 965, 521, 410, 1040, 902, 683, 1068, 545, 719, 572, 716, 810, 347, 1059, 1147, 172, 1010, 998, 262, 74, 966, 30, 1186, 309, 348, 167, 1202, 58, 376, 1061, 881, 165, 1165, 1002, 190, 920, 610, 718, 426, 398, 350, 153, 714, 302, 253, 900, 738, 1169, 422, 366, 131, 397, 781, 1033, 483, 477, 916, 688, 932, 586, 451, 764, 1201, 181, 860, 654, 480, 570, 503, 470, 849, 908, 1156, 833, 872, 528, 141, 1166, 963, 1137, 650, 539, 150, 547, 1150, 405, 1132, 999, 1187, 915, 1189, 514, 493, 1100, 984, 876, 246, 1117, 327, 70, 795, 577, 1004, 200, 7, 1087, 168, 261, 1022, 509, 1163, 1073, 690, 507, 134, 946, 12, 428, 524, 1097, 84, 206, 1207, 826, 903, 635, 473, 186, 518, 928, 173, 489, 1065, 13, 1160, 597, 628, 1027, 843, 157, 217, 1119, 419, 680, 291, 359, 584, 146, 587, 420, 952, 219, 9, 440, 527, 263, 1047, 1179, 1028, 303, 409, 1006, 450, 546, 829, 402, 721, 739, 775, 1014, 780, 1013, 231, 108, 1164, 293, 774, 482, 751, 1124, 220, 997, 284, 1213, 726, 942, 736, 384, 851, 737, 288, 360, 496, 753, 727, 807, 663, 788, 614, 227, 1192, 129, 883, 1141, 859, 387, 340, 749, 564, 662, 979, 834, 986, 353, 732, 1208, 478, 505, 187, 542, 311, 510, 68, 250, 59, 229, 730, 620, 212, 594, 315, 666, 644, 435, 237, 759, 372, 665, 548, 373, 794, 538, 65, 887, 41, 685, 647, 1030, 796, 1083, 121, 724, 488, 1188, 223, 114, 768, 370, 968, 1159, 508, 1152, 467, 975, 345, 163, 532, 853, 1062, 249, 758, 525, 162, 189, 684, 537, 321, 278, 904, 447, 332, 1086, 453, 355, 1056, 57, 557, 149, 337, 63, 585, 699, 516, 198, 211, 821, 742, 1104, 822, 590, 94, 1149, 166, 599, 819, 656, 618, 415, 741, 694, 589, 615, 1205, 646, 452, 868, 145, 1120, 512, 243, 1066, 669, 840, 19, 1199, 1052, 126, 1011, 316, 811, 869, 861, 358, 158, 555, 4, 334, 1078, 601, 395, 967, 776, 27, 686, 393, 640, 767, 461, 88, 436, 526, 501, 270, 500, 1128, 69, 513, 1098, 977, 1209, 562, 298, 1129, 1095, 401, 1070, 498, 1182, 342, 1080, 962, 73, 773, 670, 1012, 1139, 832, 485, 26, 976, 571, 259, 1082, 14, 551, 520, 113, 559, 106, 960, 184, 552, 845, 118, 442, 34, 691, 487, 905, 412, 174, 109, 1144, 813, 608, 707, 712, 22, 898, 465, 390, 673, 558, 746, 1081, 375, 323, 92, 267, 953, 23, 414, 228, 95, 5, 406, 698, 981, 476, 147, 403, 626, 1180, 835, 1148, 992, 18, 264, 1077, 445, 1127, 1007, 148, 523, 959, 769, 955, 1044, 809, 535, 176, 1206, 934, 456, 697, 127, 660, 156, 1000, 431, 297, 1050, 991, 875, 99, 731, 621, 83, 161, 244, 404, 317, 103, 681, 701, 368, 474, 931, 91, 365, 917, 830, 1063, 972, 901, 743, 1051, 1085, 257, 1142, 657, 248]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4810767613836557
the save name prefix for this run is:  chkpt-ID_4810767613836557_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.415 +- 0.002
mrr vals (pred, true): 0.001, 0.012
batch losses (mrrl, rdl): 0.0, 0.0001051659

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1106
rank avg (pred): 0.447 +- 0.277
mrr vals (pred, true): 0.125, 0.004
batch losses (mrrl, rdl): 0.0, 6.1311e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 720
rank avg (pred): 0.514 +- 0.292
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.0, 7.07353e-05

Epoch over!
epoch time: 12.026

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 837
rank avg (pred): 0.461 +- 0.285
mrr vals (pred, true): 0.103, 0.005
batch losses (mrrl, rdl): 0.0, 4.115e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.475 +- 0.291
mrr vals (pred, true): 0.073, 0.005
batch losses (mrrl, rdl): 0.0, 4.699e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.453 +- 0.286
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0, 2.5456e-06

Epoch over!
epoch time: 12.044

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.072 +- 0.089
mrr vals (pred, true): 0.121, 0.266
batch losses (mrrl, rdl): 0.0, 6.1898e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 305
rank avg (pred): 0.094 +- 0.127
mrr vals (pred, true): 0.104, 0.191
batch losses (mrrl, rdl): 0.0, 1.13594e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 631
rank avg (pred): 0.473 +- 0.284
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 0.0, 8.2783e-06

Epoch over!
epoch time: 11.93

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 125
rank avg (pred): 0.466 +- 0.281
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0, 3.6796e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.474 +- 0.294
mrr vals (pred, true): 0.081, 0.004
batch losses (mrrl, rdl): 0.0, 2.873e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 524
rank avg (pred): 0.438 +- 0.281
mrr vals (pred, true): 0.056, 0.009
batch losses (mrrl, rdl): 0.0, 4.5531e-06

Epoch over!
epoch time: 11.885

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 891
rank avg (pred): 0.510 +- 0.282
mrr vals (pred, true): 0.059, 0.002
batch losses (mrrl, rdl): 0.0, 1.25155e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.476 +- 0.290
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 0.0, 1.9943e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.495 +- 0.282
mrr vals (pred, true): 0.033, 0.003
batch losses (mrrl, rdl): 0.0, 2.394e-07

Epoch over!
epoch time: 11.983

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.065 +- 0.130
mrr vals (pred, true): 0.134, 0.184
batch losses (mrrl, rdl): 0.0244722124, 1.2783e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 889
rank avg (pred): 0.464 +- 0.176
mrr vals (pred, true): 0.056, 0.005
batch losses (mrrl, rdl): 0.0003880237, 3.94188e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 928
rank avg (pred): 0.486 +- 0.180
mrr vals (pred, true): 0.056, 0.003
batch losses (mrrl, rdl): 0.0003820299, 2.84687e-05

Epoch over!
epoch time: 12.332

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.028 +- 0.020
mrr vals (pred, true): 0.170, 0.117
batch losses (mrrl, rdl): 0.0280672479, 8.04889e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.446 +- 0.152
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.24858e-05, 6.37713e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.760 +- 0.365
mrr vals (pred, true): 0.036, 0.012
batch losses (mrrl, rdl): 0.0019759401, 0.0028332935

Epoch over!
epoch time: 12.114

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.484 +- 0.192
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 0.0, 2.66947e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 856
rank avg (pred): 0.489 +- 0.188
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0002890158, 3.89698e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.521 +- 0.221
mrr vals (pred, true): 0.047, 0.003
batch losses (mrrl, rdl): 8.78239e-05, 3.87629e-05

Epoch over!
epoch time: 12.25

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.666 +- 0.437
mrr vals (pred, true): 0.050, 0.012
batch losses (mrrl, rdl): 1.8089e-06, 0.0015520229

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.623 +- 0.430
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0003870228, 0.0002021035

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.540 +- 0.258
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 0.0001016175, 5.23465e-05

Epoch over!
epoch time: 12.221

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 647
rank avg (pred): 0.492 +- 0.237
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001499028, 7.3992e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 97
rank avg (pred): 0.482 +- 0.249
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.0006889871, 6.3412e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.479 +- 0.261
mrr vals (pred, true): 0.053, 0.003
batch losses (mrrl, rdl): 8.92607e-05, 9.4603e-06

Epoch over!
epoch time: 12.467

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.495 +- 0.276
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 2.021e-06, 4.7502e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 401
rank avg (pred): 0.477 +- 0.257
mrr vals (pred, true): 0.053, 0.005
batch losses (mrrl, rdl): 7.44404e-05, 7.3381e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.496 +- 0.286
mrr vals (pred, true): 0.056, 0.003
batch losses (mrrl, rdl): 0.0003897381, 3.9538e-06

Epoch over!
epoch time: 12.316

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.030 +- 0.022
mrr vals (pred, true): 0.236, 0.254
batch losses (mrrl, rdl): 0.0031932655, 6.3866e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1097
rank avg (pred): 0.485 +- 0.265
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.93969e-05, 5.4893e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 475
rank avg (pred): 0.512 +- 0.279
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0007709832, 6.8963e-06

Epoch over!
epoch time: 12.336

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.218 +- 0.257
mrr vals (pred, true): 0.106, 0.117
batch losses (mrrl, rdl): 0.0012602538, 0.0003308157

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.475 +- 0.252
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 3.84387e-05, 9.1851e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.502 +- 0.270
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 4.3255e-06, 2.1408e-06

Epoch over!
epoch time: 12.41

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.514 +- 0.291
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.37832e-05, 1.07775e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.242 +- 0.382
mrr vals (pred, true): 0.060, 0.017
batch losses (mrrl, rdl): 0.0010442949, 0.0001943653

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1177
rank avg (pred): 0.530 +- 0.271
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001696749, 4.27023e-05

Epoch over!
epoch time: 13.714

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1001
rank avg (pred): 0.504 +- 0.268
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.000118426, 8.5838e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 639
rank avg (pred): 0.479 +- 0.238
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 9.5745e-06, 8.0516e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.498 +- 0.276
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 9.046e-07, 8.9044e-06

Epoch over!
epoch time: 12.635

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.020 +- 0.015
mrr vals (pred, true): 0.270, 0.250

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   82 	     0 	 0.05754 	 0.00088 	 m..s
   83 	     1 	 0.05786 	 0.00122 	 m..s
   53 	     2 	 0.05138 	 0.00128 	 m..s
    0 	     3 	 0.04604 	 0.00145 	 m..s
   33 	     4 	 0.04988 	 0.00155 	 m..s
   25 	     5 	 0.04925 	 0.00170 	 m..s
    3 	     6 	 0.04733 	 0.00192 	 m..s
   49 	     7 	 0.05104 	 0.00194 	 m..s
   64 	     8 	 0.05169 	 0.00204 	 m..s
   80 	     9 	 0.05380 	 0.00225 	 m..s
   79 	    10 	 0.05378 	 0.00250 	 m..s
   34 	    11 	 0.05001 	 0.00262 	 m..s
   27 	    12 	 0.04940 	 0.00267 	 m..s
   21 	    13 	 0.04908 	 0.00272 	 m..s
   38 	    14 	 0.05035 	 0.00287 	 m..s
   45 	    15 	 0.05081 	 0.00290 	 m..s
   71 	    16 	 0.05257 	 0.00300 	 m..s
    1 	    17 	 0.04687 	 0.00311 	 m..s
   23 	    18 	 0.04916 	 0.00314 	 m..s
   66 	    19 	 0.05184 	 0.00316 	 m..s
   56 	    20 	 0.05142 	 0.00321 	 m..s
   41 	    21 	 0.05059 	 0.00321 	 m..s
   51 	    22 	 0.05116 	 0.00323 	 m..s
   63 	    23 	 0.05167 	 0.00333 	 m..s
   54 	    24 	 0.05140 	 0.00335 	 m..s
   60 	    25 	 0.05148 	 0.00336 	 m..s
   44 	    26 	 0.05063 	 0.00338 	 m..s
   72 	    27 	 0.05266 	 0.00342 	 m..s
   28 	    28 	 0.04950 	 0.00348 	 m..s
   76 	    29 	 0.05343 	 0.00350 	 m..s
   17 	    30 	 0.04870 	 0.00351 	 m..s
   77 	    31 	 0.05347 	 0.00352 	 m..s
   73 	    32 	 0.05284 	 0.00361 	 m..s
   43 	    33 	 0.05063 	 0.00361 	 m..s
    7 	    34 	 0.04815 	 0.00362 	 m..s
   14 	    35 	 0.04859 	 0.00376 	 m..s
   16 	    36 	 0.04862 	 0.00381 	 m..s
   74 	    37 	 0.05305 	 0.00383 	 m..s
    2 	    38 	 0.04728 	 0.00387 	 m..s
   65 	    39 	 0.05175 	 0.00389 	 m..s
    6 	    40 	 0.04801 	 0.00396 	 m..s
   78 	    41 	 0.05375 	 0.00400 	 m..s
   55 	    42 	 0.05141 	 0.00403 	 m..s
   36 	    43 	 0.05016 	 0.00404 	 m..s
   46 	    44 	 0.05084 	 0.00407 	 m..s
   70 	    45 	 0.05239 	 0.00410 	 m..s
   31 	    46 	 0.04977 	 0.00410 	 m..s
   48 	    47 	 0.05104 	 0.00411 	 m..s
   15 	    48 	 0.04860 	 0.00412 	 m..s
   52 	    49 	 0.05132 	 0.00414 	 m..s
   75 	    50 	 0.05314 	 0.00416 	 m..s
   30 	    51 	 0.04967 	 0.00417 	 m..s
   42 	    52 	 0.05061 	 0.00419 	 m..s
   37 	    53 	 0.05023 	 0.00428 	 m..s
   47 	    54 	 0.05100 	 0.00428 	 m..s
   59 	    55 	 0.05146 	 0.00430 	 m..s
   10 	    56 	 0.04841 	 0.00431 	 m..s
   24 	    57 	 0.04923 	 0.00437 	 m..s
   58 	    58 	 0.05145 	 0.00438 	 m..s
   61 	    59 	 0.05153 	 0.00441 	 m..s
   50 	    60 	 0.05110 	 0.00442 	 m..s
   12 	    61 	 0.04850 	 0.00444 	 m..s
   57 	    62 	 0.05142 	 0.00451 	 m..s
   67 	    63 	 0.05189 	 0.00452 	 m..s
   32 	    64 	 0.04983 	 0.00453 	 m..s
   29 	    65 	 0.04955 	 0.00455 	 m..s
   20 	    66 	 0.04895 	 0.00456 	 m..s
   26 	    67 	 0.04931 	 0.00457 	 m..s
   19 	    68 	 0.04889 	 0.00458 	 m..s
   39 	    69 	 0.05036 	 0.00461 	 m..s
   13 	    70 	 0.04856 	 0.00463 	 m..s
   35 	    71 	 0.05002 	 0.00485 	 m..s
   69 	    72 	 0.05237 	 0.00489 	 m..s
   11 	    73 	 0.04849 	 0.00490 	 m..s
    4 	    74 	 0.04781 	 0.00490 	 m..s
    8 	    75 	 0.04824 	 0.00492 	 m..s
   62 	    76 	 0.05157 	 0.00498 	 m..s
   68 	    77 	 0.05231 	 0.00501 	 m..s
   18 	    78 	 0.04880 	 0.00534 	 m..s
   22 	    79 	 0.04911 	 0.00558 	 m..s
   40 	    80 	 0.05044 	 0.00577 	 m..s
   85 	    81 	 0.06320 	 0.00740 	 m..s
   84 	    82 	 0.06104 	 0.00858 	 m..s
   86 	    83 	 0.06428 	 0.01402 	 m..s
   81 	    84 	 0.05632 	 0.01818 	 m..s
    5 	    85 	 0.04785 	 0.01940 	 ~...
   89 	    86 	 0.08208 	 0.02091 	 m..s
   88 	    87 	 0.07267 	 0.02854 	 m..s
   87 	    88 	 0.06816 	 0.02942 	 m..s
   98 	    89 	 0.17156 	 0.03535 	 MISS
  100 	    90 	 0.17741 	 0.03563 	 MISS
    9 	    91 	 0.04838 	 0.04671 	 ~...
   96 	    92 	 0.16472 	 0.12561 	 m..s
   97 	    93 	 0.16808 	 0.12715 	 m..s
   92 	    94 	 0.15159 	 0.12902 	 ~...
   94 	    95 	 0.16256 	 0.13437 	 ~...
   90 	    96 	 0.14684 	 0.13525 	 ~...
   91 	    97 	 0.14915 	 0.13734 	 ~...
   95 	    98 	 0.16293 	 0.14054 	 ~...
   93 	    99 	 0.15272 	 0.14848 	 ~...
  107 	   100 	 0.23410 	 0.16128 	 m..s
  105 	   101 	 0.20243 	 0.17085 	 m..s
  101 	   102 	 0.17871 	 0.17792 	 ~...
   99 	   103 	 0.17511 	 0.17992 	 ~...
  103 	   104 	 0.18405 	 0.18103 	 ~...
  106 	   105 	 0.21413 	 0.18578 	 ~...
  102 	   106 	 0.18077 	 0.18907 	 ~...
  104 	   107 	 0.18502 	 0.20543 	 ~...
  114 	   108 	 0.27386 	 0.21151 	 m..s
  108 	   109 	 0.25286 	 0.22805 	 ~...
  109 	   110 	 0.25607 	 0.22857 	 ~...
  112 	   111 	 0.27067 	 0.24456 	 ~...
  110 	   112 	 0.27001 	 0.25004 	 ~...
  113 	   113 	 0.27152 	 0.26363 	 ~...
  115 	   114 	 0.28513 	 0.26513 	 ~...
  111 	   115 	 0.27031 	 0.26886 	 ~...
  116 	   116 	 0.28672 	 0.28931 	 ~...
  118 	   117 	 0.30683 	 0.29618 	 ~...
  119 	   118 	 0.32750 	 0.31362 	 ~...
  117 	   119 	 0.30045 	 0.34039 	 m..s
  120 	   120 	 0.34339 	 0.35668 	 ~...
==========================================
r_mrr = 0.9783837795257568
r2_mrr = 0.7552728652954102
spearmanr_mrr@5 = 0.9884044528007507
spearmanr_mrr@10 = 0.9828062653541565
spearmanr_mrr@50 = 0.9862478375434875
spearmanr_mrr@100 = 0.9915627241134644
spearmanr_mrr@All = 0.99209064245224
==========================================
test time: 0.405
Done Testing dataset CoDExSmall
total time taken: 194.16871738433838
training time taken: 185.15124201774597
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 2733616111716865
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [903, 328, 1167, 1212, 540, 1022, 856, 1148, 911, 593, 904, 377, 984, 228, 305, 281, 423, 436, 545, 162, 409, 268, 1024, 958, 349, 836, 577, 65, 772, 1197, 400, 1161, 70, 846, 831, 1187, 462, 1067, 266, 499, 914, 1119, 805, 1004, 670, 1068, 589, 101, 672, 467, 827, 79, 700, 699, 418, 659, 693, 171, 1056, 717, 394, 466, 1175, 117, 718, 665, 148, 1147, 351, 439, 1177, 966, 433, 724, 707, 196, 876, 848, 775, 762, 896, 592, 1214, 123, 729, 661, 306, 238, 89, 734, 746, 668, 711, 226, 515, 310, 100, 785, 1211, 821, 907, 562, 802, 921, 617, 404, 1111, 141, 820, 71, 233, 736, 833, 694, 360, 789, 814, 615, 382, 142, 760]
valid_ids (0): []
train_ids (1094): [894, 739, 526, 732, 764, 556, 154, 1210, 580, 634, 538, 504, 383, 235, 29, 946, 458, 867, 631, 1162, 863, 222, 1114, 888, 362, 1174, 189, 850, 774, 223, 509, 293, 1133, 2, 949, 627, 242, 740, 1059, 520, 583, 187, 103, 939, 152, 883, 697, 1087, 131, 837, 1052, 1120, 1095, 389, 77, 422, 1163, 183, 744, 195, 243, 737, 380, 301, 962, 988, 1014, 150, 960, 190, 209, 352, 551, 490, 109, 120, 172, 408, 289, 261, 182, 929, 318, 658, 572, 1008, 258, 374, 844, 241, 447, 630, 1016, 376, 1077, 229, 806, 657, 719, 927, 208, 449, 86, 517, 157, 344, 868, 399, 345, 1091, 1100, 249, 1176, 60, 675, 1043, 444, 877, 265, 855, 508, 749, 484, 1060, 88, 95, 923, 287, 59, 563, 935, 616, 1019, 225, 678, 1080, 521, 933, 144, 406, 568, 1153, 192, 854, 692, 781, 481, 1013, 165, 275, 290, 129, 721, 731, 518, 198, 1191, 1116, 1122, 548, 555, 413, 31, 512, 75, 626, 849, 606, 424, 82, 671, 611, 158, 941, 928, 695, 69, 917, 1118, 149, 940, 603, 1128, 432, 1126, 346, 943, 535, 981, 197, 461, 838, 994, 255, 800, 498, 1127, 354, 456, 916, 956, 1033, 708, 893, 1018, 1020, 1149, 401, 500, 507, 1173, 777, 277, 194, 751, 472, 353, 78, 703, 825, 14, 459, 1090, 973, 15, 899, 1089, 359, 111, 106, 664, 1182, 476, 696, 793, 451, 487, 858, 188, 704, 370, 163, 272, 743, 332, 1107, 81, 145, 473, 912, 479, 337, 440, 236, 677, 442, 944, 385, 340, 167, 19, 263, 922, 1085, 1048, 529, 674, 248, 130, 652, 17, 810, 1146, 395, 1104, 201, 125, 232, 537, 35, 990, 987, 1097, 710, 480, 327, 90, 1134, 219, 974, 733, 202, 807, 861, 181, 553, 477, 783, 947, 860, 112, 56, 1179, 654, 215, 685, 558, 1155, 586, 522, 726, 1201, 778, 748, 1037, 333, 516, 794, 124, 533, 722, 115, 690, 1154, 214, 283, 673, 906, 823, 393, 87, 411, 788, 497, 121, 930, 813, 792, 1203, 1072, 396, 51, 1031, 667, 909, 895, 811, 330, 107, 637, 884, 683, 1124, 342, 495, 104, 632, 527, 879, 567, 1098, 1140, 1054, 1075, 299, 590, 768, 308, 887, 1010, 218, 1049, 460, 367, 581, 1164, 605, 742, 761, 598, 464, 254, 375, 262, 766, 1145, 386, 1115, 513, 528, 231, 1135, 392, 865, 828, 1186, 660, 96, 274, 534, 983, 1021, 321, 547, 278, 1199, 804, 1026, 93, 797, 1006, 217, 482, 1017, 246, 1045, 1032, 784, 997, 882, 795, 334, 485, 641, 253, 610, 763, 63, 1074, 878, 76, 478, 1058, 584, 607, 859, 84, 869, 1168, 791, 118, 434, 519, 44, 381, 184, 213, 453, 622, 443, 55, 206, 651, 138, 292, 315, 959, 576, 320, 506, 132, 1151, 998, 1169, 986, 28, 525, 952, 669, 613, 957, 801, 852, 969, 752, 323, 829, 1065, 716, 220, 560, 276, 224, 474, 1166, 866, 91, 738, 595, 798, 297, 625, 180, 816, 938, 350, 999, 1159, 176, 684, 635, 174, 643, 1180, 1012, 1213, 1188, 809, 566, 133, 578, 1025, 343, 357, 1200, 597, 1108, 951, 99, 1204, 273, 483, 647, 1183, 623, 339, 361, 531, 412, 552, 68, 715, 62, 1202, 614, 758, 620, 1207, 680, 417, 10, 491, 698, 1195, 1123, 1030, 720, 913, 264, 295, 296, 1005, 179, 471, 54, 1046, 812, 369, 757, 32, 853, 1142, 542, 842, 146, 11, 286, 1079, 5, 961, 559, 1205, 965, 322, 1157, 1036, 972, 116, 1064, 985, 329, 702, 16, 221, 20, 1047, 771, 937, 830, 1138, 953, 1081, 178, 366, 1156, 435, 74, 18, 80, 39, 1063, 786, 257, 834, 430, 618, 250, 1039, 514, 288, 1112, 968, 505, 587, 872, 588, 915, 964, 431, 502, 714, 730, 799, 845, 390, 139, 204, 324, 1040, 511, 1196, 437, 642, 1035, 1160, 1184, 582, 363, 410, 532, 624, 503, 463, 325, 1076, 1129, 832, 835, 1034, 12, 446, 136, 256, 92, 151, 608, 676, 628, 317, 1002, 991, 1071, 919, 1105, 1069, 979, 585, 926, 457, 862, 523, 319, 338, 892, 594, 307, 57, 1093, 689, 1178, 279, 127, 1094, 621, 452, 114, 554, 425, 205, 160, 967, 83, 1062, 314, 619, 175, 638, 1132, 741, 280, 686, 387, 488, 169, 874, 420, 186, 230, 216, 924, 1101, 97, 438, 298, 122, 1209, 415, 723, 779, 609, 826, 153, 602, 954, 7, 364, 493, 687, 22, 161, 1165, 108, 889, 137, 1103, 536, 428, 948, 128, 931, 355, 384, 403, 237, 767, 1096, 85, 126, 379, 311, 787, 1029, 113, 886, 701, 942, 147, 441, 640, 600, 1110, 1003, 421, 549, 977, 770, 429, 46, 1023, 414, 819, 891, 378, 285, 870, 890, 1001, 776, 207, 469, 170, 840, 570, 747, 24, 98, 655, 579, 544, 902, 416, 918, 191, 901, 227, 936, 43, 1027, 454, 135, 291, 1137, 755, 1057, 1131, 662, 681, 982, 27, 591, 1051, 1009, 765, 302, 539, 405, 688, 1102, 489, 571, 569, 1136, 240, 1092, 910, 419, 709, 336, 300, 1088, 140, 851, 159, 1144, 313, 155, 649, 309, 1143, 134, 4, 934, 388, 975, 843, 30, 564, 575, 372, 646, 864, 546, 365, 871, 976, 1189, 282, 36, 727, 759, 105, 166, 45, 49, 1208, 873, 326, 510, 269, 725, 808, 34, 629, 1050, 37, 199, 1171, 407, 25, 494, 244, 21, 267, 348, 782, 356, 648, 1139, 251, 398, 465, 335, 650, 177, 978, 596, 885, 880, 501, 604, 847, 745, 612, 212, 818, 682, 304, 64, 245, 1078, 73, 200, 1172, 1194, 1028, 1185, 541, 817, 550, 316, 1117, 1130, 397, 955, 1121, 119, 1158, 271, 496, 1125, 557, 193, 6, 492, 42, 284, 639, 203, 897, 402, 920, 996, 824, 644, 445, 1, 66, 1055, 992, 713, 1099, 530, 450, 756, 347, 803, 1082, 260, 211, 391, 1073, 524, 1000, 875, 185, 995, 455, 259, 633, 656, 574, 50, 40, 1193, 247, 239, 773, 769, 67, 963, 1198, 1041, 728, 303, 565, 470, 796, 753, 1113, 23, 1141, 1152, 705, 371, 58, 1044, 48, 341, 1061, 426, 102, 358, 754, 38, 294, 599, 679, 706, 573, 252, 898, 841, 1192, 168, 1038, 9, 94, 691, 61, 1011, 905, 636, 857, 822, 1042, 486, 475, 663, 3, 210, 110, 52, 815, 1181, 945, 925, 8, 1066, 1150, 989, 173, 1109, 331, 645, 950, 653, 543, 735, 26, 270, 72, 0, 234, 1206, 750, 427, 881, 143, 993, 1015, 1053, 312, 1170, 1086, 790, 971, 900, 468, 47, 712, 164, 780, 1084, 839, 53, 1007, 448, 561, 970, 373, 666, 156, 368, 33, 1083, 601, 908, 1070, 1106, 41, 932, 980, 13, 1190]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3307245663304244
the save name prefix for this run is:  chkpt-ID_3307245663304244_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 461
rank avg (pred): 0.477 +- 0.004
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.02288e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 625
rank avg (pred): 0.464 +- 0.260
mrr vals (pred, true): 0.174, 0.000
batch losses (mrrl, rdl): 0.0, 1.55561e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 120
rank avg (pred): 0.455 +- 0.292
mrr vals (pred, true): 0.186, 0.000
batch losses (mrrl, rdl): 0.0, 1.47325e-05

Epoch over!
epoch time: 12.504

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 33
rank avg (pred): 0.288 +- 0.203
mrr vals (pred, true): 0.189, 0.094
batch losses (mrrl, rdl): 0.0, 9.64085e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 784
rank avg (pred): 0.460 +- 0.304
mrr vals (pred, true): 0.187, 0.000
batch losses (mrrl, rdl): 0.0, 8.86661e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 658
rank avg (pred): 0.465 +- 0.304
mrr vals (pred, true): 0.185, 0.000
batch losses (mrrl, rdl): 0.0, 3.50798e-05

Epoch over!
epoch time: 12.765

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 36
rank avg (pred): 0.311 +- 0.254
mrr vals (pred, true): 0.192, 0.105
batch losses (mrrl, rdl): 0.0, 1.75948e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1154
rank avg (pred): 0.272 +- 0.278
mrr vals (pred, true): 0.205, 0.089
batch losses (mrrl, rdl): 0.0, 3.36518e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 172
rank avg (pred): 0.463 +- 0.284
mrr vals (pred, true): 0.181, 0.000
batch losses (mrrl, rdl): 0.0, 1.61938e-05

Epoch over!
epoch time: 13.213

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 749
rank avg (pred): 0.258 +- 0.270
mrr vals (pred, true): 0.197, 0.176
batch losses (mrrl, rdl): 0.0, 6.68248e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 219
rank avg (pred): 0.482 +- 0.281
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.0, 1.47787e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 218
rank avg (pred): 0.460 +- 0.300
mrr vals (pred, true): 0.185, 0.000
batch losses (mrrl, rdl): 0.0, 1.10169e-05

Epoch over!
epoch time: 13.415

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 761
rank avg (pred): 0.474 +- 0.290
mrr vals (pred, true): 0.180, 0.000
batch losses (mrrl, rdl): 0.0, 3.35224e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 340
rank avg (pred): 0.443 +- 0.292
mrr vals (pred, true): 0.184, 0.000
batch losses (mrrl, rdl): 0.0, 4.93137e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.196 +- 0.257
mrr vals (pred, true): 0.219, 0.229
batch losses (mrrl, rdl): 0.0, 2.45546e-05

Epoch over!
epoch time: 12.558

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 750
rank avg (pred): 0.224 +- 0.272
mrr vals (pred, true): 0.206, 0.251
batch losses (mrrl, rdl): 0.0199542213, 4.7463e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 683
rank avg (pred): 0.481 +- 0.212
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0041485718, 3.69191e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 252
rank avg (pred): 0.233 +- 0.241
mrr vals (pred, true): 0.222, 0.307
batch losses (mrrl, rdl): 0.0712807626, 0.0001361666

Epoch over!
epoch time: 14.088

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 325
rank avg (pred): 0.490 +- 0.185
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0009092062, 4.70969e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 429
rank avg (pred): 0.458 +- 0.196
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.30608e-05, 6.34881e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 571
rank avg (pred): 0.449 +- 0.212
mrr vals (pred, true): 0.073, 0.001
batch losses (mrrl, rdl): 0.0054511968, 5.21332e-05

Epoch over!
epoch time: 13.751

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 212
rank avg (pred): 0.474 +- 0.192
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001812307, 0.0001081385

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 732
rank avg (pred): 0.112 +- 0.158
mrr vals (pred, true): 0.317, 0.382
batch losses (mrrl, rdl): 0.0411757864, 2.99597e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 771
rank avg (pred): 0.466 +- 0.203
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.000610635, 2.4721e-05

Epoch over!
epoch time: 14.263

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.483 +- 0.186
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 5.40316e-05, 5.54356e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 317
rank avg (pred): 0.238 +- 0.221
mrr vals (pred, true): 0.148, 0.101
batch losses (mrrl, rdl): 0.0218469054, 0.0002259915

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1089
rank avg (pred): 0.480 +- 0.183
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 2.7637e-06, 5.54028e-05

Epoch over!
epoch time: 14.156

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1115
rank avg (pred): 0.523 +- 0.146
mrr vals (pred, true): 0.025, 0.000
batch losses (mrrl, rdl): 0.0060931914, 0.000109155

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.466 +- 0.200
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0008300514, 4.48955e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.203 +- 0.208
mrr vals (pred, true): 0.231, 0.164
batch losses (mrrl, rdl): 0.0456363112, 4.59693e-05

Epoch over!
epoch time: 13.059

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1076
rank avg (pred): 0.281 +- 0.225
mrr vals (pred, true): 0.104, 0.135
batch losses (mrrl, rdl): 0.0092922822, 4.57493e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 569
rank avg (pred): 0.449 +- 0.218
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0090398062, 2.72527e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.471 +- 0.197
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.161e-07, 4.92772e-05

Epoch over!
epoch time: 12.237

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 632
rank avg (pred): 0.469 +- 0.193
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002203426, 7.4057e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1082
rank avg (pred): 0.491 +- 0.189
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.64272e-05, 6.85734e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 115
rank avg (pred): 0.465 +- 0.196
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.67284e-05, 5.81928e-05

Epoch over!
epoch time: 12.882

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.501 +- 0.177
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0012591433, 7.07423e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 541
rank avg (pred): 0.384 +- 0.230
mrr vals (pred, true): 0.055, 0.097
batch losses (mrrl, rdl): 0.0002240139, 4.76869e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 137
rank avg (pred): 0.515 +- 0.164
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002212109, 8.01203e-05

Epoch over!
epoch time: 12.951

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 871
rank avg (pred): 0.529 +- 0.159
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0004099372, 0.0001612636

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 327
rank avg (pred): 0.487 +- 0.197
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001067366, 5.74634e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.483 +- 0.194
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.62174e-05, 5.60393e-05

Epoch over!
epoch time: 12.721

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 288
rank avg (pred): 0.288 +- 0.243
mrr vals (pred, true): 0.090, 0.099
batch losses (mrrl, rdl): 0.0163198262, 2.76906e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 338
rank avg (pred): 0.476 +- 0.209
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002061387, 3.50031e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 830
rank avg (pred): 0.241 +- 0.241
mrr vals (pred, true): 0.156, 0.184
batch losses (mrrl, rdl): 0.0080872085, 2.91377e-05

Epoch over!
epoch time: 12.799

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.446 +- 0.220
mrr vals (pred, true): 0.065, 0.001

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.04979 	 0.00012 	 m..s
   74 	     1 	 0.05731 	 0.00013 	 m..s
   80 	     2 	 0.06013 	 0.00014 	 m..s
   84 	     3 	 0.06252 	 0.00014 	 m..s
   33 	     4 	 0.05268 	 0.00014 	 m..s
   18 	     5 	 0.05089 	 0.00014 	 m..s
   23 	     6 	 0.05191 	 0.00016 	 m..s
    6 	     7 	 0.04973 	 0.00017 	 m..s
   42 	     8 	 0.05339 	 0.00017 	 m..s
   11 	     9 	 0.05025 	 0.00019 	 m..s
   71 	    10 	 0.05672 	 0.00019 	 m..s
   32 	    11 	 0.05267 	 0.00019 	 m..s
   51 	    12 	 0.05412 	 0.00019 	 m..s
   79 	    13 	 0.05993 	 0.00020 	 m..s
   17 	    14 	 0.05080 	 0.00020 	 m..s
   15 	    15 	 0.05058 	 0.00020 	 m..s
   56 	    16 	 0.05435 	 0.00021 	 m..s
   19 	    17 	 0.05107 	 0.00021 	 m..s
   60 	    18 	 0.05472 	 0.00021 	 m..s
   65 	    19 	 0.05597 	 0.00021 	 m..s
   21 	    20 	 0.05114 	 0.00021 	 m..s
   26 	    21 	 0.05210 	 0.00021 	 m..s
   89 	    22 	 0.06602 	 0.00021 	 m..s
   36 	    23 	 0.05285 	 0.00022 	 m..s
   77 	    24 	 0.05982 	 0.00022 	 m..s
   72 	    25 	 0.05687 	 0.00022 	 m..s
   64 	    26 	 0.05591 	 0.00022 	 m..s
   69 	    27 	 0.05635 	 0.00023 	 m..s
   62 	    28 	 0.05506 	 0.00024 	 m..s
   63 	    29 	 0.05546 	 0.00024 	 m..s
   48 	    30 	 0.05405 	 0.00025 	 m..s
   13 	    31 	 0.05045 	 0.00025 	 m..s
   76 	    32 	 0.05816 	 0.00026 	 m..s
   82 	    33 	 0.06065 	 0.00026 	 m..s
   20 	    34 	 0.05113 	 0.00026 	 m..s
   22 	    35 	 0.05123 	 0.00026 	 m..s
   81 	    36 	 0.06044 	 0.00026 	 m..s
   61 	    37 	 0.05481 	 0.00026 	 m..s
    3 	    38 	 0.04927 	 0.00026 	 m..s
   41 	    39 	 0.05325 	 0.00027 	 m..s
   67 	    40 	 0.05623 	 0.00027 	 m..s
   27 	    41 	 0.05211 	 0.00027 	 m..s
    0 	    42 	 0.04506 	 0.00027 	 m..s
   45 	    43 	 0.05349 	 0.00028 	 m..s
    4 	    44 	 0.04941 	 0.00028 	 m..s
   91 	    45 	 0.06676 	 0.00028 	 m..s
   50 	    46 	 0.05411 	 0.00028 	 m..s
   10 	    47 	 0.05007 	 0.00028 	 m..s
   16 	    48 	 0.05058 	 0.00029 	 m..s
   35 	    49 	 0.05285 	 0.00029 	 m..s
   54 	    50 	 0.05432 	 0.00030 	 m..s
   52 	    51 	 0.05422 	 0.00031 	 m..s
    1 	    52 	 0.04748 	 0.00031 	 m..s
   59 	    53 	 0.05444 	 0.00031 	 m..s
   53 	    54 	 0.05424 	 0.00032 	 m..s
   43 	    55 	 0.05339 	 0.00033 	 m..s
   75 	    56 	 0.05760 	 0.00033 	 m..s
   28 	    57 	 0.05216 	 0.00033 	 m..s
   46 	    58 	 0.05359 	 0.00034 	 m..s
   49 	    59 	 0.05410 	 0.00034 	 m..s
   44 	    60 	 0.05343 	 0.00035 	 m..s
    9 	    61 	 0.05004 	 0.00035 	 m..s
   66 	    62 	 0.05610 	 0.00036 	 m..s
   83 	    63 	 0.06109 	 0.00038 	 m..s
   73 	    64 	 0.05707 	 0.00038 	 m..s
   37 	    65 	 0.05285 	 0.00038 	 m..s
   34 	    66 	 0.05277 	 0.00039 	 m..s
    8 	    67 	 0.04996 	 0.00040 	 m..s
   12 	    68 	 0.05041 	 0.00040 	 m..s
   39 	    69 	 0.05313 	 0.00041 	 m..s
   58 	    70 	 0.05437 	 0.00042 	 m..s
    5 	    71 	 0.04957 	 0.00044 	 m..s
   38 	    72 	 0.05307 	 0.00044 	 m..s
   85 	    73 	 0.06463 	 0.00046 	 m..s
   57 	    74 	 0.05435 	 0.00052 	 m..s
    2 	    75 	 0.04899 	 0.00058 	 m..s
   24 	    76 	 0.05193 	 0.00066 	 m..s
   90 	    77 	 0.06654 	 0.00066 	 m..s
   88 	    78 	 0.06584 	 0.00066 	 m..s
   14 	    79 	 0.05046 	 0.00074 	 m..s
   55 	    80 	 0.05434 	 0.00076 	 m..s
   30 	    81 	 0.05245 	 0.00076 	 m..s
   87 	    82 	 0.06547 	 0.00110 	 m..s
   47 	    83 	 0.05374 	 0.00129 	 m..s
   40 	    84 	 0.05324 	 0.00129 	 m..s
   31 	    85 	 0.05264 	 0.00304 	 m..s
   86 	    86 	 0.06472 	 0.00514 	 m..s
   29 	    87 	 0.05222 	 0.00839 	 m..s
   99 	    88 	 0.08191 	 0.04062 	 m..s
   96 	    89 	 0.07919 	 0.06020 	 ~...
  100 	    90 	 0.08479 	 0.07620 	 ~...
   92 	    91 	 0.06782 	 0.07888 	 ~...
   78 	    92 	 0.05989 	 0.08070 	 ~...
   68 	    93 	 0.05633 	 0.08504 	 ~...
   94 	    94 	 0.06999 	 0.08713 	 ~...
   25 	    95 	 0.05208 	 0.08928 	 m..s
   98 	    96 	 0.07984 	 0.09224 	 ~...
   70 	    97 	 0.05635 	 0.10593 	 m..s
  103 	    98 	 0.09258 	 0.11104 	 ~...
   93 	    99 	 0.06987 	 0.11377 	 m..s
  104 	   100 	 0.10706 	 0.12240 	 ~...
   95 	   101 	 0.07831 	 0.12260 	 m..s
  101 	   102 	 0.08479 	 0.12300 	 m..s
   97 	   103 	 0.07931 	 0.13257 	 m..s
  105 	   104 	 0.11863 	 0.13589 	 ~...
  102 	   105 	 0.08946 	 0.14954 	 m..s
  107 	   106 	 0.14020 	 0.16274 	 ~...
  106 	   107 	 0.13369 	 0.16717 	 m..s
  108 	   108 	 0.17159 	 0.17724 	 ~...
  109 	   109 	 0.18381 	 0.19092 	 ~...
  110 	   110 	 0.18399 	 0.21422 	 m..s
  111 	   111 	 0.19477 	 0.22629 	 m..s
  112 	   112 	 0.21089 	 0.23966 	 ~...
  114 	   113 	 0.26043 	 0.25348 	 ~...
  115 	   114 	 0.26451 	 0.26914 	 ~...
  113 	   115 	 0.25849 	 0.28706 	 ~...
  117 	   116 	 0.30129 	 0.31455 	 ~...
  116 	   117 	 0.29271 	 0.33249 	 m..s
  120 	   118 	 0.33785 	 0.34035 	 ~...
  119 	   119 	 0.33104 	 0.35019 	 ~...
  118 	   120 	 0.32420 	 0.37989 	 m..s
==========================================
r_mrr = 0.9503870606422424
r2_mrr = 0.7133713960647583
spearmanr_mrr@5 = 0.9050779938697815
spearmanr_mrr@10 = 0.9671691060066223
spearmanr_mrr@50 = 0.9532740116119385
spearmanr_mrr@100 = 0.959264874458313
spearmanr_mrr@All = 0.9611777663230896
==========================================
test time: 0.398
Done Testing dataset DBpedia50
total time taken: 202.76113533973694
training time taken: 197.82197785377502
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 2660138701403651
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [881, 274, 983, 1157, 1150, 555, 1015, 791, 419, 719, 757, 921, 36, 364, 1188, 941, 558, 901, 198, 832, 696, 40, 61, 281, 1194, 59, 739, 378, 53, 607, 1139, 814, 675, 984, 353, 928, 1039, 255, 460, 846, 43, 362, 903, 46, 210, 375, 957, 1169, 544, 823, 310, 371, 651, 193, 27, 864, 501, 120, 751, 1115, 89, 792, 550, 882, 621, 899, 926, 620, 736, 880, 775, 594, 730, 166, 110, 496, 291, 139, 1027, 1108, 1103, 160, 799, 724, 701, 737, 654, 249, 1081, 233, 269, 156, 12, 435, 1098, 686, 441, 179, 1154, 1023, 1173, 996, 294, 908, 973, 282, 968, 1145, 333, 532, 706, 712, 655, 127, 841, 1160, 162, 184, 1, 749, 222]
valid_ids (0): []
train_ids (1094): [617, 334, 562, 1105, 1043, 844, 1159, 138, 1198, 509, 429, 24, 994, 224, 497, 627, 998, 1199, 257, 833, 802, 1085, 1165, 785, 566, 180, 860, 952, 1028, 22, 384, 403, 443, 695, 35, 810, 761, 200, 936, 77, 723, 411, 287, 347, 631, 482, 766, 100, 277, 672, 1211, 163, 355, 689, 504, 146, 1013, 48, 381, 315, 207, 949, 1017, 18, 1210, 1084, 23, 1118, 454, 769, 214, 1053, 679, 1111, 746, 389, 143, 392, 405, 913, 907, 358, 1186, 455, 918, 56, 342, 728, 506, 1022, 537, 1079, 895, 643, 1185, 244, 794, 1060, 388, 929, 866, 1051, 336, 352, 426, 1163, 106, 545, 820, 540, 524, 1208, 438, 1152, 767, 440, 58, 990, 759, 923, 155, 965, 800, 979, 126, 173, 1175, 569, 889, 1137, 112, 1004, 1126, 638, 174, 124, 644, 21, 565, 1038, 781, 424, 851, 930, 363, 39, 859, 343, 900, 571, 1046, 745, 650, 945, 665, 1029, 452, 253, 1148, 1113, 189, 959, 597, 60, 987, 1156, 290, 1026, 408, 1136, 326, 483, 891, 836, 232, 57, 539, 639, 31, 771, 1166, 503, 584, 1068, 177, 83, 659, 365, 312, 956, 523, 190, 788, 87, 19, 946, 511, 1037, 1178, 436, 932, 1020, 669, 181, 54, 783, 1000, 938, 579, 1155, 768, 196, 811, 288, 718, 185, 71, 286, 1171, 1146, 492, 714, 611, 962, 250, 808, 25, 741, 300, 1124, 670, 176, 1003, 1087, 916, 893, 47, 314, 321, 301, 734, 81, 777, 444, 434, 246, 868, 495, 848, 619, 13, 374, 450, 839, 733, 103, 20, 773, 822, 380, 711, 824, 1168, 512, 266, 637, 464, 1086, 15, 1007, 1102, 829, 385, 320, 349, 552, 653, 95, 323, 298, 105, 753, 69, 813, 704, 580, 1054, 1089, 499, 311, 1128, 152, 633, 459, 489, 169, 886, 217, 357, 1011, 747, 468, 226, 593, 279, 414, 939, 821, 570, 449, 165, 1164, 313, 517, 150, 977, 806, 26, 239, 604, 616, 234, 2, 416, 534, 585, 850, 219, 225, 588, 835, 1096, 167, 758, 583, 168, 568, 80, 855, 1106, 694, 284, 1042, 73, 919, 33, 367, 231, 656, 115, 1073, 340, 394, 1205, 433, 681, 406, 682, 360, 243, 680, 674, 911, 431, 1138, 985, 954, 151, 238, 904, 386, 1062, 710, 1200, 1101, 1070, 809, 914, 327, 272, 790, 1192, 1177, 116, 400, 215, 251, 328, 37, 415, 119, 576, 74, 410, 157, 4, 636, 687, 975, 465, 445, 1063, 1083, 1076, 1010, 149, 1024, 1197, 1190, 472, 346, 123, 974, 992, 595, 413, 471, 1080, 581, 887, 425, 778, 700, 667, 154, 872, 108, 283, 756, 892, 807, 1123, 615, 254, 1184, 264, 563, 873, 890, 303, 601, 144, 209, 285, 463, 256, 493, 995, 292, 178, 211, 396, 1117, 986, 182, 359, 270, 484, 79, 553, 705, 731, 1120, 1040, 1045, 518, 796, 235, 366, 933, 609, 755, 856, 409, 1014, 516, 369, 68, 260, 688, 332, 519, 572, 707, 319, 1181, 629, 480, 826, 828, 513, 1100, 600, 951, 420, 692, 554, 671, 625, 1092, 129, 1134, 547, 612, 344, 685, 750, 922, 10, 634, 1041, 339, 1202, 804, 118, 107, 30, 159, 702, 1201, 527, 953, 191, 469, 1167, 812, 218, 462, 838, 780, 722, 556, 86, 764, 302, 915, 248, 491, 421, 842, 652, 1064, 628, 442, 1074, 748, 430, 765, 646, 1077, 978, 970, 699, 1109, 1203, 803, 1075, 658, 894, 883, 293, 567, 502, 220, 797, 991, 7, 927, 1091, 446, 75, 795, 242, 691, 587, 976, 1093, 879, 849, 909, 11, 1110, 478, 265, 397, 370, 1135, 510, 236, 898, 542, 88, 267, 308, 716, 192, 1183, 213, 1095, 330, 819, 17, 521, 221, 131, 676, 94, 316, 278, 1125, 439, 1132, 498, 1180, 863, 546, 295, 522, 1056, 1147, 865, 66, 1078, 1055, 988, 140, 752, 657, 935, 65, 418, 677, 275, 391, 708, 531, 526, 660, 1191, 132, 473, 377, 456, 717, 51, 354, 114, 910, 982, 825, 1033, 206, 1212, 461, 479, 348, 559, 6, 1019, 268, 404, 477, 1172, 997, 96, 869, 774, 372, 194, 437, 1001, 432, 673, 98, 818, 14, 989, 111, 229, 309, 787, 241, 172, 350, 770, 128, 1129, 878, 325, 1072, 1008, 395, 1044, 683, 16, 470, 448, 548, 104, 399, 713, 1195, 912, 137, 383, 661, 62, 720, 276, 917, 195, 740, 582, 613, 3, 1193, 259, 1196, 457, 801, 1158, 1187, 1144, 549, 475, 458, 102, 34, 101, 870, 398, 109, 1067, 738, 1097, 1130, 317, 390, 122, 877, 345, 575, 201, 228, 148, 742, 133, 1149, 170, 204, 950, 1142, 862, 599, 837, 827, 525, 401, 97, 1050, 605, 205, 1127, 1006, 161, 1209, 1104, 164, 368, 42, 338, 63, 1009, 197, 341, 183, 666, 993, 817, 64, 72, 488, 533, 942, 273, 560, 754, 626, 972, 38, 32, 188, 1021, 41, 28, 980, 387, 306, 1088, 529, 793, 1030, 447, 263, 888, 958, 1107, 335, 84, 379, 136, 861, 622, 487, 937, 322, 361, 1025, 948, 896, 782, 125, 564, 1094, 481, 373, 1018, 1161, 141, 91, 29, 67, 635, 964, 1002, 649, 299, 760, 647, 1065, 476, 494, 258, 230, 684, 55, 1189, 514, 624, 763, 905, 925, 171, 1143, 591, 186, 175, 331, 981, 725, 135, 1119, 1213, 1012, 721, 417, 735, 645, 577, 535, 1153, 590, 606, 871, 407, 117, 280, 324, 93, 902, 840, 1214, 697, 1071, 955, 76, 853, 920, 474, 632, 831, 508, 969, 153, 662, 1151, 262, 45, 208, 453, 779, 1049, 1179, 515, 1058, 541, 589, 147, 451, 1057, 610, 1170, 0, 538, 726, 1052, 82, 199, 356, 145, 1112, 428, 351, 247, 121, 943, 237, 1059, 602, 1204, 252, 947, 561, 727, 158, 924, 897, 203, 1061, 1114, 52, 1162, 1016, 240, 1176, 543, 875, 640, 49, 641, 618, 729, 423, 858, 971, 467, 776, 536, 668, 815, 130, 427, 1082, 505, 642, 1034, 92, 961, 297, 1116, 608, 485, 854, 1099, 99, 9, 402, 78, 1174, 960, 5, 586, 663, 678, 134, 289, 592, 216, 847, 70, 304, 834, 296, 393, 1122, 271, 693, 551, 805, 772, 490, 212, 1047, 934, 422, 843, 1207, 857, 967, 318, 113, 466, 1005, 867, 1031, 744, 798, 614, 223, 1066, 1090, 885, 1032, 329, 376, 709, 963, 732, 245, 623, 1206, 578, 698, 966, 1131, 520, 664, 500, 816, 789, 90, 261, 142, 648, 884, 573, 762, 940, 412, 1036, 1069, 382, 598, 715, 931, 85, 703, 830, 845, 337, 307, 507, 1048, 852, 1182, 906, 530, 876, 305, 786, 1140, 8, 630, 1035, 44, 187, 743, 486, 574, 596, 690, 202, 557, 528, 1133, 603, 784, 1121, 874, 999, 50, 227, 1141, 944]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6195565893011902
the save name prefix for this run is:  chkpt-ID_6195565893011902_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 912
rank avg (pred): 0.489 +- 0.007
mrr vals (pred, true): 0.019, 0.235
batch losses (mrrl, rdl): 0.0, 0.0022975227

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 473
rank avg (pred): 0.470 +- 0.266
mrr vals (pred, true): 0.067, 0.048
batch losses (mrrl, rdl): 0.0, 2.709e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 81
rank avg (pred): 0.463 +- 0.266
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 0.0, 1.4372e-06

Epoch over!
epoch time: 12.219

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 687
rank avg (pred): 0.455 +- 0.264
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 0.0, 8.759e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 432
rank avg (pred): 0.464 +- 0.265
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0, 1.2298e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 301
rank avg (pred): 0.098 +- 0.092
mrr vals (pred, true): 0.213, 0.326
batch losses (mrrl, rdl): 0.0, 3.0397e-06

Epoch over!
epoch time: 13.448

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 704
rank avg (pred): 0.460 +- 0.271
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0, 5.528e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 230
rank avg (pred): 0.452 +- 0.275
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 0.0, 5.8514e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 865
rank avg (pred): 0.467 +- 0.258
mrr vals (pred, true): 0.040, 0.053
batch losses (mrrl, rdl): 0.0, 1.3252e-06

Epoch over!
epoch time: 12.906

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1119
rank avg (pred): 0.448 +- 0.260
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 0.0, 9.0403e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 741
rank avg (pred): 0.085 +- 0.117
mrr vals (pred, true): 0.271, 0.398
batch losses (mrrl, rdl): 0.0, 2.864e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 694
rank avg (pred): 0.451 +- 0.272
mrr vals (pred, true): 0.056, 0.054
batch losses (mrrl, rdl): 0.0, 5.056e-06

Epoch over!
epoch time: 12.689

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 471
rank avg (pred): 0.462 +- 0.270
mrr vals (pred, true): 0.052, 0.054
batch losses (mrrl, rdl): 0.0, 1.2033e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 499
rank avg (pred): 0.125 +- 0.161
mrr vals (pred, true): 0.232, 0.226
batch losses (mrrl, rdl): 0.0, 3.4131e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1174
rank avg (pred): 0.453 +- 0.269
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0, 1.565e-07

Epoch over!
epoch time: 13.052

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 324
rank avg (pred): 0.466 +- 0.273
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 9.18995e-05, 1.964e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 154
rank avg (pred): 0.412 +- 0.171
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 5.1704e-06, 5.74405e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 563
rank avg (pred): 0.324 +- 0.256
mrr vals (pred, true): 0.185, 0.195
batch losses (mrrl, rdl): 0.0009779127, 0.0005492767

Epoch over!
epoch time: 15.133

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 355
rank avg (pred): 0.432 +- 0.163
mrr vals (pred, true): 0.046, 0.053
batch losses (mrrl, rdl): 0.0001313567, 5.7749e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 405
rank avg (pred): 0.432 +- 0.177
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 1.1466e-06, 5.46023e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 192
rank avg (pred): 0.451 +- 0.167
mrr vals (pred, true): 0.040, 0.062
batch losses (mrrl, rdl): 0.0010626587, 3.40727e-05

Epoch over!
epoch time: 14.348

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 467
rank avg (pred): 0.447 +- 0.168
mrr vals (pred, true): 0.042, 0.056
batch losses (mrrl, rdl): 0.0007179283, 3.47945e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 807
rank avg (pred): 0.480 +- 0.128
mrr vals (pred, true): 0.025, 0.053
batch losses (mrrl, rdl): 0.0060871271, 5.07976e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 693
rank avg (pred): 0.438 +- 0.176
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 5.1923e-06, 4.88378e-05

Epoch over!
epoch time: 14.509

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 88
rank avg (pred): 0.428 +- 0.186
mrr vals (pred, true): 0.055, 0.055
batch losses (mrrl, rdl): 0.0002273595, 7.55129e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 18
rank avg (pred): 0.136 +- 0.216
mrr vals (pred, true): 0.313, 0.285
batch losses (mrrl, rdl): 0.0078743827, 9.9119e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 977
rank avg (pred): 0.065 +- 0.194
mrr vals (pred, true): 0.450, 0.479
batch losses (mrrl, rdl): 0.0088171009, 1.48579e-05

Epoch over!
epoch time: 13.761

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 42
rank avg (pred): 0.170 +- 0.216
mrr vals (pred, true): 0.279, 0.266
batch losses (mrrl, rdl): 0.0016590047, 2.30502e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 177
rank avg (pred): 0.445 +- 0.186
mrr vals (pred, true): 0.045, 0.059
batch losses (mrrl, rdl): 0.0002611917, 3.63601e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 114
rank avg (pred): 0.430 +- 0.202
mrr vals (pred, true): 0.058, 0.056
batch losses (mrrl, rdl): 0.0006962504, 4.88207e-05

Epoch over!
epoch time: 13.325

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 238
rank avg (pred): 0.433 +- 0.191
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 7.08291e-05, 7.16435e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 425
rank avg (pred): 0.440 +- 0.183
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002400797, 5.49997e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 798
rank avg (pred): 0.437 +- 0.190
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0003758629, 6.55e-05

Epoch over!
epoch time: 13.93

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 178
rank avg (pred): 0.433 +- 0.188
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 1.8453e-06, 5.31656e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 149
rank avg (pred): 0.436 +- 0.181
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.0002322253, 4.22743e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 483
rank avg (pred): 0.432 +- 0.186
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 1.34457e-05, 5.21598e-05

Epoch over!
epoch time: 15.139

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 579
rank avg (pred): 0.438 +- 0.181
mrr vals (pred, true): 0.044, 0.051
batch losses (mrrl, rdl): 0.0003352896, 6.93711e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 802
rank avg (pred): 0.428 +- 0.188
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 3.56899e-05, 5.97832e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1036
rank avg (pred): 0.439 +- 0.180
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 0.0001169033, 5.44193e-05

Epoch over!
epoch time: 13.475

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 237
rank avg (pred): 0.432 +- 0.188
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 1.4305e-06, 6.09877e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1071
rank avg (pred): 0.082 +- 0.226
mrr vals (pred, true): 0.424, 0.363
batch losses (mrrl, rdl): 0.03687457, 2.77368e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 947
rank avg (pred): 0.427 +- 0.188
mrr vals (pred, true): 0.049, 0.063
batch losses (mrrl, rdl): 1.31556e-05, 5.17507e-05

Epoch over!
epoch time: 13.411

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 212
rank avg (pred): 0.417 +- 0.192
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0001952844, 7.39406e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1050
rank avg (pred): 0.414 +- 0.193
mrr vals (pred, true): 0.060, 0.058
batch losses (mrrl, rdl): 0.0010288054, 6.13024e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 24
rank avg (pred): 0.168 +- 0.219
mrr vals (pred, true): 0.285, 0.284
batch losses (mrrl, rdl): 1.59914e-05, 1.69513e-05

Epoch over!
epoch time: 15.011

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.430 +- 0.183
mrr vals (pred, true): 0.053, 0.053

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   16 	     0 	 0.05471 	 0.04376 	 ~...
   19 	     1 	 0.05520 	 0.04538 	 ~...
   41 	     2 	 0.05742 	 0.04547 	 ~...
   56 	     3 	 0.05875 	 0.04668 	 ~...
   34 	     4 	 0.05693 	 0.04726 	 ~...
   49 	     5 	 0.05830 	 0.04763 	 ~...
   35 	     6 	 0.05696 	 0.04823 	 ~...
   15 	     7 	 0.05464 	 0.04840 	 ~...
   69 	     8 	 0.06152 	 0.04864 	 ~...
   52 	     9 	 0.05841 	 0.04899 	 ~...
   67 	    10 	 0.06084 	 0.04905 	 ~...
   21 	    11 	 0.05541 	 0.04922 	 ~...
   64 	    12 	 0.05956 	 0.04941 	 ~...
   51 	    13 	 0.05839 	 0.04950 	 ~...
    2 	    14 	 0.05318 	 0.04973 	 ~...
   10 	    15 	 0.05426 	 0.05022 	 ~...
   46 	    16 	 0.05813 	 0.05035 	 ~...
   58 	    17 	 0.05883 	 0.05053 	 ~...
   59 	    18 	 0.05883 	 0.05056 	 ~...
   65 	    19 	 0.06038 	 0.05092 	 ~...
   31 	    20 	 0.05665 	 0.05113 	 ~...
   66 	    21 	 0.06064 	 0.05117 	 ~...
   61 	    22 	 0.05898 	 0.05146 	 ~...
   70 	    23 	 0.06152 	 0.05161 	 ~...
    8 	    24 	 0.05408 	 0.05168 	 ~...
   42 	    25 	 0.05764 	 0.05171 	 ~...
   38 	    26 	 0.05719 	 0.05178 	 ~...
    7 	    27 	 0.05404 	 0.05179 	 ~...
   62 	    28 	 0.05899 	 0.05189 	 ~...
   23 	    29 	 0.05563 	 0.05212 	 ~...
   22 	    30 	 0.05560 	 0.05216 	 ~...
   53 	    31 	 0.05842 	 0.05290 	 ~...
   28 	    32 	 0.05601 	 0.05293 	 ~...
    1 	    33 	 0.05274 	 0.05299 	 ~...
   63 	    34 	 0.05916 	 0.05303 	 ~...
   57 	    35 	 0.05876 	 0.05315 	 ~...
   32 	    36 	 0.05665 	 0.05336 	 ~...
    4 	    37 	 0.05348 	 0.05360 	 ~...
   50 	    38 	 0.05833 	 0.05376 	 ~...
   25 	    39 	 0.05578 	 0.05385 	 ~...
    0 	    40 	 0.05246 	 0.05412 	 ~...
   30 	    41 	 0.05632 	 0.05413 	 ~...
    5 	    42 	 0.05359 	 0.05417 	 ~...
   24 	    43 	 0.05573 	 0.05425 	 ~...
   60 	    44 	 0.05890 	 0.05425 	 ~...
   20 	    45 	 0.05536 	 0.05427 	 ~...
   37 	    46 	 0.05704 	 0.05428 	 ~...
   33 	    47 	 0.05680 	 0.05449 	 ~...
   47 	    48 	 0.05822 	 0.05462 	 ~...
   14 	    49 	 0.05464 	 0.05495 	 ~...
   40 	    50 	 0.05734 	 0.05503 	 ~...
   68 	    51 	 0.06096 	 0.05521 	 ~...
   44 	    52 	 0.05792 	 0.05555 	 ~...
   71 	    53 	 0.06191 	 0.05577 	 ~...
   26 	    54 	 0.05590 	 0.05595 	 ~...
   36 	    55 	 0.05699 	 0.05596 	 ~...
   54 	    56 	 0.05872 	 0.05630 	 ~...
   13 	    57 	 0.05451 	 0.05647 	 ~...
   43 	    58 	 0.05769 	 0.05723 	 ~...
   12 	    59 	 0.05427 	 0.05733 	 ~...
   18 	    60 	 0.05507 	 0.05735 	 ~...
   27 	    61 	 0.05600 	 0.05751 	 ~...
   29 	    62 	 0.05629 	 0.05810 	 ~...
   39 	    63 	 0.05727 	 0.05814 	 ~...
   48 	    64 	 0.05825 	 0.05817 	 ~...
    6 	    65 	 0.05378 	 0.05875 	 ~...
   55 	    66 	 0.05873 	 0.05902 	 ~...
    3 	    67 	 0.05346 	 0.05910 	 ~...
   11 	    68 	 0.05426 	 0.05917 	 ~...
   45 	    69 	 0.05809 	 0.05986 	 ~...
   17 	    70 	 0.05473 	 0.06111 	 ~...
    9 	    71 	 0.05420 	 0.06158 	 ~...
   80 	    72 	 0.28135 	 0.08417 	 MISS
  109 	    73 	 0.33190 	 0.14588 	 MISS
   74 	    74 	 0.25021 	 0.15981 	 m..s
  100 	    75 	 0.32444 	 0.16239 	 MISS
   75 	    76 	 0.26430 	 0.18244 	 m..s
   72 	    77 	 0.23865 	 0.18561 	 m..s
   73 	    78 	 0.24206 	 0.19962 	 m..s
   75 	    79 	 0.26430 	 0.20491 	 m..s
   78 	    80 	 0.27619 	 0.21861 	 m..s
   75 	    81 	 0.26430 	 0.22043 	 m..s
   91 	    82 	 0.31278 	 0.22226 	 m..s
   79 	    83 	 0.27753 	 0.22414 	 m..s
  103 	    84 	 0.32589 	 0.26043 	 m..s
   84 	    85 	 0.29869 	 0.26126 	 m..s
   83 	    86 	 0.29853 	 0.26205 	 m..s
   82 	    87 	 0.29553 	 0.26840 	 ~...
   90 	    88 	 0.30979 	 0.27425 	 m..s
   81 	    89 	 0.29123 	 0.27645 	 ~...
  104 	    90 	 0.32626 	 0.27991 	 m..s
   85 	    91 	 0.29980 	 0.28022 	 ~...
   86 	    92 	 0.30347 	 0.28091 	 ~...
  107 	    93 	 0.33030 	 0.28370 	 m..s
  105 	    94 	 0.32652 	 0.28459 	 m..s
  108 	    95 	 0.33164 	 0.28579 	 m..s
   88 	    96 	 0.30784 	 0.28658 	 ~...
  102 	    97 	 0.32541 	 0.28900 	 m..s
   93 	    98 	 0.31621 	 0.29390 	 ~...
   94 	    99 	 0.31718 	 0.29634 	 ~...
   96 	   100 	 0.32139 	 0.29813 	 ~...
   92 	   101 	 0.31336 	 0.30412 	 ~...
  106 	   102 	 0.32894 	 0.30803 	 ~...
   97 	   103 	 0.32212 	 0.30931 	 ~...
   99 	   104 	 0.32408 	 0.30980 	 ~...
   87 	   105 	 0.30460 	 0.31037 	 ~...
   95 	   106 	 0.31886 	 0.31542 	 ~...
   89 	   107 	 0.30819 	 0.32032 	 ~...
   98 	   108 	 0.32354 	 0.33811 	 ~...
  101 	   109 	 0.32473 	 0.35342 	 ~...
  115 	   110 	 0.39266 	 0.37821 	 ~...
  112 	   111 	 0.37295 	 0.39820 	 ~...
  117 	   112 	 0.41726 	 0.40136 	 ~...
  116 	   113 	 0.40080 	 0.41200 	 ~...
  114 	   114 	 0.39166 	 0.41409 	 ~...
  110 	   115 	 0.36332 	 0.42648 	 m..s
  111 	   116 	 0.36463 	 0.42736 	 m..s
  113 	   117 	 0.39127 	 0.45341 	 m..s
  118 	   118 	 0.44485 	 0.51145 	 m..s
  120 	   119 	 0.61557 	 0.61055 	 ~...
  119 	   120 	 0.56466 	 0.62311 	 m..s
==========================================
r_mrr = 0.9656801223754883
r2_mrr = 0.9215700626373291
spearmanr_mrr@5 = 0.9742001295089722
spearmanr_mrr@10 = 0.9826066493988037
spearmanr_mrr@50 = 0.9509333968162537
spearmanr_mrr@100 = 0.974288821220398
spearmanr_mrr@All = 0.9773027300834656
==========================================
test time: 0.582
Done Testing dataset Kinships
total time taken: 214.2060534954071
training time taken: 207.00120449066162
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 5759413499077296
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [889, 243, 476, 458, 249, 977, 351, 1060, 1164, 17, 1128, 558, 772, 321, 1105, 596, 763, 127, 732, 289, 1029, 11, 136, 99, 780, 1062, 610, 710, 1175, 384, 232, 154, 517, 456, 980, 67, 427, 626, 203, 948, 568, 997, 223, 676, 753, 87, 313, 540, 276, 174, 387, 500, 843, 1159, 163, 861, 656, 85, 139, 350, 200, 1048, 931, 643, 1106, 20, 305, 408, 273, 646, 109, 511, 1025, 1005, 740, 425, 903, 1138, 78, 96, 1211, 797, 1061, 378, 83, 590, 1134, 149, 1142, 267, 449, 1201, 651, 1031, 112, 492, 825, 1047, 1037, 312, 1178, 735, 451, 247, 585, 399, 8, 158, 370, 573, 445, 1091, 34, 468, 1024, 159, 33, 617, 201, 840, 799]
valid_ids (0): []
train_ids (1094): [148, 81, 57, 721, 877, 452, 13, 1087, 719, 496, 360, 199, 1191, 786, 100, 157, 386, 960, 569, 895, 762, 802, 282, 1127, 416, 288, 850, 238, 853, 874, 992, 406, 114, 471, 541, 742, 463, 1143, 743, 91, 553, 811, 869, 973, 804, 867, 383, 993, 356, 1054, 302, 1093, 837, 325, 286, 3, 1194, 146, 778, 432, 911, 892, 213, 1171, 1103, 974, 1075, 1006, 228, 347, 1000, 420, 579, 366, 951, 1129, 301, 74, 947, 441, 1189, 418, 934, 1044, 1161, 619, 1072, 300, 284, 152, 145, 258, 9, 698, 989, 448, 1140, 882, 1003, 217, 536, 412, 110, 222, 757, 578, 1108, 1065, 657, 730, 981, 1056, 926, 293, 134, 696, 95, 31, 439, 51, 784, 856, 202, 263, 140, 1130, 684, 443, 264, 566, 75, 1212, 25, 188, 311, 944, 1050, 830, 922, 713, 489, 1034, 28, 41, 173, 580, 277, 212, 129, 298, 822, 1079, 252, 1176, 116, 563, 625, 779, 891, 679, 642, 333, 354, 829, 470, 1173, 589, 587, 894, 235, 1022, 402, 688, 885, 1098, 820, 1213, 475, 1137, 21, 358, 582, 921, 570, 380, 1084, 411, 707, 169, 245, 197, 985, 905, 1193, 604, 1036, 618, 120, 521, 485, 22, 90, 852, 562, 331, 118, 430, 318, 1162, 237, 1059, 652, 674, 689, 828, 549, 1168, 435, 798, 600, 268, 682, 552, 988, 530, 875, 53, 224, 107, 594, 806, 858, 673, 501, 43, 183, 1184, 663, 916, 991, 942, 628, 1070, 497, 841, 122, 680, 389, 827, 1073, 274, 352, 1170, 623, 936, 789, 369, 690, 156, 132, 1158, 906, 986, 904, 966, 662, 0, 510, 297, 1206, 72, 764, 253, 338, 121, 182, 290, 720, 564, 812, 636, 700, 736, 1192, 1163, 193, 632, 315, 65, 454, 415, 442, 979, 575, 678, 395, 631, 896, 1145, 381, 505, 444, 1039, 543, 433, 927, 294, 859, 595, 1207, 768, 349, 467, 893, 461, 39, 266, 1019, 603, 1203, 512, 401, 241, 307, 1141, 211, 106, 667, 453, 538, 1012, 561, 1081, 838, 1013, 609, 635, 715, 557, 1139, 520, 1051, 447, 382, 773, 147, 781, 949, 1132, 473, 488, 373, 932, 332, 965, 704, 125, 82, 255, 162, 1126, 259, 460, 40, 180, 920, 756, 972, 377, 654, 883, 56, 160, 1076, 925, 353, 144, 117, 170, 1041, 1069, 375, 161, 546, 345, 308, 322, 250, 613, 469, 555, 30, 355, 554, 529, 1110, 910, 758, 124, 390, 964, 372, 1104, 576, 55, 751, 1026, 835, 479, 647, 598, 952, 45, 583, 18, 419, 687, 890, 1007, 1092, 697, 128, 240, 36, 24, 1052, 655, 648, 176, 192, 509, 309, 115, 634, 365, 887, 428, 348, 1205, 519, 913, 943, 769, 607, 774, 292, 1099, 990, 659, 847, 363, 760, 206, 1004, 1096, 782, 1017, 694, 560, 574, 586, 164, 14, 438, 337, 846, 186, 898, 1038, 219, 1014, 1027, 1016, 665, 103, 50, 172, 204, 477, 998, 296, 1117, 54, 744, 391, 44, 622, 481, 866, 670, 257, 1174, 915, 248, 108, 508, 591, 620, 878, 1209, 1020, 545, 1124, 946, 319, 346, 98, 930, 593, 767, 978, 84, 788, 111, 155, 503, 1177, 606, 602, 516, 12, 417, 660, 48, 1121, 15, 1066, 870, 94, 711, 261, 410, 480, 959, 1118, 1214, 32, 171, 938, 699, 340, 971, 533, 1107, 1144, 967, 1102, 1045, 937, 907, 929, 283, 761, 462, 681, 1119, 873, 539, 166, 945, 7, 275, 1077, 398, 336, 271, 181, 69, 436, 1010, 914, 734, 547, 137, 765, 1160, 216, 1035, 709, 849, 123, 615, 190, 649, 1190, 368, 816, 731, 234, 361, 499, 330, 982, 727, 198, 969, 483, 1182, 422, 209, 324, 683, 59, 1187, 1148, 800, 577, 824, 537, 150, 737, 526, 650, 785, 722, 388, 995, 611, 374, 624, 1101, 664, 712, 227, 1064, 490, 1172, 999, 983, 823, 47, 738, 379, 272, 572, 52, 633, 280, 178, 601, 1030, 175, 1149, 919, 376, 630, 437, 334, 242, 902, 42, 269, 295, 855, 138, 246, 49, 1135, 984, 826, 194, 299, 627, 933, 876, 1115, 1009, 975, 316, 868, 306, 794, 729, 1146, 1196, 431, 1071, 105, 528, 230, 565, 961, 532, 225, 950, 86, 524, 498, 394, 571, 714, 879, 637, 534, 113, 329, 702, 518, 783, 567, 434, 621, 327, 484, 871, 776, 392, 923, 502, 770, 1136, 1085, 807, 486, 1074, 1185, 66, 371, 940, 404, 35, 705, 405, 478, 515, 897, 1113, 385, 909, 1008, 359, 792, 1042, 793, 218, 1090, 1111, 527, 834, 426, 1199, 1181, 808, 513, 1, 1088, 831, 1154, 1195, 92, 917, 1001, 1100, 1002, 455, 270, 281, 341, 2, 440, 1021, 706, 599, 326, 76, 265, 658, 278, 29, 185, 865, 364, 1198, 304, 1089, 1080, 262, 725, 1125, 1028, 941, 1015, 523, 775, 80, 1040, 968, 1208, 189, 796, 605, 215, 1067, 1116, 733, 135, 862, 424, 987, 101, 953, 935, 403, 328, 1152, 863, 771, 46, 177, 343, 491, 195, 814, 133, 1197, 1167, 357, 126, 260, 413, 857, 795, 525, 588, 396, 168, 68, 1150, 550, 612, 1023, 414, 752, 1165, 1063, 19, 556, 256, 196, 141, 726, 93, 1049, 955, 494, 832, 323, 597, 535, 747, 464, 142, 482, 717, 507, 1097, 749, 970, 746, 872, 16, 908, 836, 344, 239, 754, 695, 104, 686, 208, 1078, 912, 638, 220, 291, 880, 421, 818, 1114, 860, 787, 423, 1057, 581, 817, 207, 692, 187, 231, 884, 1083, 317, 693, 342, 1094, 833, 244, 335, 644, 976, 1153, 221, 38, 559, 639, 1131, 367, 143, 407, 888, 1055, 1043, 339, 487, 668, 1179, 544, 1169, 285, 531, 254, 958, 73, 718, 89, 810, 102, 88, 167, 493, 584, 1032, 755, 362, 645, 504, 962, 27, 750, 1053, 64, 954, 70, 899, 614, 4, 10, 629, 815, 1068, 1082, 675, 691, 821, 1018, 71, 472, 446, 803, 514, 474, 506, 640, 400, 608, 708, 466, 1109, 801, 766, 723, 1202, 233, 97, 844, 842, 994, 314, 131, 918, 63, 724, 62, 939, 6, 205, 854, 809, 845, 320, 303, 79, 393, 1204, 1046, 23, 459, 457, 1011, 839, 409, 661, 522, 957, 901, 741, 790, 677, 465, 1112, 1086, 791, 1186, 881, 813, 251, 130, 928, 151, 819, 748, 701, 191, 1133, 666, 1122, 1210, 672, 61, 184, 1166, 1120, 1058, 214, 210, 1188, 777, 685, 848, 616, 1095, 963, 956, 716, 37, 551, 805, 1183, 287, 226, 1147, 77, 279, 669, 864, 641, 739, 310, 26, 900, 153, 58, 542, 119, 1151, 759, 886, 1180, 1156, 996, 924, 592, 703, 60, 236, 653, 1157, 728, 429, 397, 1155, 5, 165, 745, 450, 548, 851, 495, 1033, 179, 229, 671, 1200, 1123]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3496520609485006
the save name prefix for this run is:  chkpt-ID_3496520609485006_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 352
rank avg (pred): 0.472 +- 0.004
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001270448

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 287
rank avg (pred): 0.321 +- 0.031
mrr vals (pred, true): 0.000, 0.088
batch losses (mrrl, rdl): 0.0, 0.0001908802

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1052
rank avg (pred): 0.476 +- 0.246
mrr vals (pred, true): 0.104, 0.001
batch losses (mrrl, rdl): 0.0, 1.91249e-05

Epoch over!
epoch time: 13.397

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 917
rank avg (pred): 0.433 +- 0.270
mrr vals (pred, true): 0.153, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002269603

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 209
rank avg (pred): 0.456 +- 0.294
mrr vals (pred, true): 0.176, 0.001
batch losses (mrrl, rdl): 0.0, 1.09253e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.339 +- 0.254
mrr vals (pred, true): 0.192, 0.138
batch losses (mrrl, rdl): 0.0, 9.64137e-05

Epoch over!
epoch time: 13.861

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 478
rank avg (pred): 0.440 +- 0.303
mrr vals (pred, true): 0.177, 0.001
batch losses (mrrl, rdl): 0.0, 1.72806e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 481
rank avg (pred): 0.457 +- 0.300
mrr vals (pred, true): 0.154, 0.001
batch losses (mrrl, rdl): 0.0, 1.03469e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 234
rank avg (pred): 0.470 +- 0.297
mrr vals (pred, true): 0.123, 0.001
batch losses (mrrl, rdl): 0.0, 7.6848e-06

Epoch over!
epoch time: 15.022

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 544
rank avg (pred): 0.341 +- 0.253
mrr vals (pred, true): 0.149, 0.077
batch losses (mrrl, rdl): 0.0, 2.03629e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 202
rank avg (pred): 0.455 +- 0.303
mrr vals (pred, true): 0.124, 0.001
batch losses (mrrl, rdl): 0.0, 8.1939e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 600
rank avg (pred): 0.465 +- 0.302
mrr vals (pred, true): 0.123, 0.002
batch losses (mrrl, rdl): 0.0, 7.5981e-06

Epoch over!
epoch time: 15.46

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 634
rank avg (pred): 0.488 +- 0.292
mrr vals (pred, true): 0.092, 0.001
batch losses (mrrl, rdl): 0.0, 1.15782e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 741
rank avg (pred): 0.292 +- 0.240
mrr vals (pred, true): 0.122, 0.176
batch losses (mrrl, rdl): 0.0, 0.0001140262

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 531
rank avg (pred): 0.300 +- 0.277
mrr vals (pred, true): 0.127, 0.057
batch losses (mrrl, rdl): 0.0, 4.21959e-05

Epoch over!
epoch time: 13.186

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 723
rank avg (pred): 0.476 +- 0.284
mrr vals (pred, true): 0.080, 0.001
batch losses (mrrl, rdl): 0.0092191072, 5.3215e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 464
rank avg (pred): 0.450 +- 0.200
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0007185969, 6.02752e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 976
rank avg (pred): 0.274 +- 0.201
mrr vals (pred, true): 0.072, 0.125
batch losses (mrrl, rdl): 0.0272973441, 6.10051e-05

Epoch over!
epoch time: 14.917

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 252
rank avg (pred): 0.009 +- 0.015
mrr vals (pred, true): 0.180, 0.259
batch losses (mrrl, rdl): 0.0621588714, 0.0003016388

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 104
rank avg (pred): 0.454 +- 0.236
mrr vals (pred, true): 0.067, 0.001
batch losses (mrrl, rdl): 0.0030293325, 2.93518e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 442
rank avg (pred): 0.461 +- 0.287
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004279851, 9.2809e-06

Epoch over!
epoch time: 13.879

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 21
rank avg (pred): 0.032 +- 0.041
mrr vals (pred, true): 0.208, 0.253
batch losses (mrrl, rdl): 0.0199757684, 0.0001296073

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 957
rank avg (pred): 0.529 +- 0.245
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0013295156, 2.75495e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 657
rank avg (pred): 0.536 +- 0.256
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.63758e-05, 4.25927e-05

Epoch over!
epoch time: 13.448

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 301
rank avg (pred): 0.355 +- 0.332
mrr vals (pred, true): 0.058, 0.064
batch losses (mrrl, rdl): 0.0006751657, 8.6951e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 653
rank avg (pred): 0.473 +- 0.276
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.000241119, 8.2095e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 841
rank avg (pred): 0.444 +- 0.320
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0007235837, 4.27281e-05

Epoch over!
epoch time: 13.377

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1
rank avg (pred): 0.197 +- 0.253
mrr vals (pred, true): 0.145, 0.100
batch losses (mrrl, rdl): 0.0199706946, 0.0001072581

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 921
rank avg (pred): 0.504 +- 0.288
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0011224656, 7.0463e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1147
rank avg (pred): 0.400 +- 0.303
mrr vals (pred, true): 0.078, 0.107
batch losses (mrrl, rdl): 0.0083013047, 0.0001517162

Epoch over!
epoch time: 13.212

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1197
rank avg (pred): 0.465 +- 0.319
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008623577, 1.48691e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 317
rank avg (pred): 0.344 +- 0.318
mrr vals (pred, true): 0.070, 0.072
batch losses (mrrl, rdl): 0.0038305358, 8.3926e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 113
rank avg (pred): 0.415 +- 0.341
mrr vals (pred, true): 0.055, 0.002
batch losses (mrrl, rdl): 0.0002668761, 6.69221e-05

Epoch over!
epoch time: 14.691

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 745
rank avg (pred): 0.305 +- 0.264
mrr vals (pred, true): 0.176, 0.218
batch losses (mrrl, rdl): 0.0180520173, 0.000260074

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 982
rank avg (pred): 0.430 +- 0.327
mrr vals (pred, true): 0.055, 0.072
batch losses (mrrl, rdl): 0.0002269653, 0.0002080465

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 939
rank avg (pred): 0.510 +- 0.269
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 9.83586e-05, 5.393e-06

Epoch over!
epoch time: 14.505

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 124
rank avg (pred): 0.413 +- 0.321
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002971695, 5.63907e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 529
rank avg (pred): 0.384 +- 0.339
mrr vals (pred, true): 0.055, 0.072
batch losses (mrrl, rdl): 0.0002032048, 6.62675e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1131
rank avg (pred): 0.473 +- 0.287
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 3.475e-06, 3.1804e-06

Epoch over!
epoch time: 13.637

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1067
rank avg (pred): 0.415 +- 0.311
mrr vals (pred, true): 0.048, 0.086
batch losses (mrrl, rdl): 2.73542e-05, 0.0001253973

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 823
rank avg (pred): 0.300 +- 0.276
mrr vals (pred, true): 0.171, 0.178
batch losses (mrrl, rdl): 0.0004331261, 8.00349e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1203
rank avg (pred): 0.412 +- 0.342
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0003013042, 8.04342e-05

Epoch over!
epoch time: 15.096

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 872
rank avg (pred): 0.434 +- 0.344
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 6.86965e-05, 5.3602e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 283
rank avg (pred): 0.419 +- 0.348
mrr vals (pred, true): 0.052, 0.087
batch losses (mrrl, rdl): 4.63604e-05, 0.0002059033

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 263
rank avg (pred): 0.031 +- 0.029
mrr vals (pred, true): 0.280, 0.299
batch losses (mrrl, rdl): 0.0033027667, 0.000247701

Epoch over!
epoch time: 14.355

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.447 +- 0.337
mrr vals (pred, true): 0.053, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   86 	     0 	 0.06714 	 0.00043 	 m..s
   61 	     1 	 0.05774 	 0.00048 	 m..s
   10 	     2 	 0.05241 	 0.00050 	 m..s
   83 	     3 	 0.06544 	 0.00050 	 m..s
   22 	     4 	 0.05308 	 0.00051 	 m..s
   35 	     5 	 0.05376 	 0.00051 	 m..s
   14 	     6 	 0.05271 	 0.00053 	 m..s
   27 	     7 	 0.05338 	 0.00053 	 m..s
   13 	     8 	 0.05259 	 0.00055 	 m..s
   62 	     9 	 0.05793 	 0.00055 	 m..s
   88 	    10 	 0.06999 	 0.00056 	 m..s
   19 	    11 	 0.05289 	 0.00056 	 m..s
   23 	    12 	 0.05310 	 0.00057 	 m..s
   47 	    13 	 0.05476 	 0.00057 	 m..s
   54 	    14 	 0.05549 	 0.00057 	 m..s
   50 	    15 	 0.05493 	 0.00058 	 m..s
   44 	    16 	 0.05447 	 0.00060 	 m..s
   73 	    17 	 0.06157 	 0.00060 	 m..s
   39 	    18 	 0.05400 	 0.00061 	 m..s
   45 	    19 	 0.05448 	 0.00062 	 m..s
   29 	    20 	 0.05339 	 0.00062 	 m..s
   49 	    21 	 0.05490 	 0.00062 	 m..s
   18 	    22 	 0.05287 	 0.00064 	 m..s
   20 	    23 	 0.05298 	 0.00064 	 m..s
   85 	    24 	 0.06670 	 0.00065 	 m..s
   77 	    25 	 0.06323 	 0.00066 	 m..s
   57 	    26 	 0.05727 	 0.00068 	 m..s
    0 	    27 	 0.04875 	 0.00070 	 m..s
    7 	    28 	 0.05175 	 0.00070 	 m..s
   74 	    29 	 0.06227 	 0.00073 	 m..s
   43 	    30 	 0.05413 	 0.00073 	 m..s
   81 	    31 	 0.06431 	 0.00073 	 m..s
   78 	    32 	 0.06349 	 0.00074 	 m..s
   51 	    33 	 0.05494 	 0.00075 	 m..s
   38 	    34 	 0.05392 	 0.00081 	 m..s
    6 	    35 	 0.05120 	 0.00081 	 m..s
   41 	    36 	 0.05412 	 0.00081 	 m..s
   90 	    37 	 0.07192 	 0.00082 	 m..s
   36 	    38 	 0.05381 	 0.00083 	 m..s
   16 	    39 	 0.05272 	 0.00085 	 m..s
   37 	    40 	 0.05385 	 0.00086 	 m..s
   30 	    41 	 0.05346 	 0.00087 	 m..s
   80 	    42 	 0.06389 	 0.00088 	 m..s
   66 	    43 	 0.05978 	 0.00089 	 m..s
   31 	    44 	 0.05359 	 0.00090 	 m..s
    8 	    45 	 0.05227 	 0.00091 	 m..s
    1 	    46 	 0.05051 	 0.00093 	 m..s
   92 	    47 	 0.07722 	 0.00094 	 m..s
   34 	    48 	 0.05376 	 0.00094 	 m..s
   75 	    49 	 0.06269 	 0.00098 	 m..s
    5 	    50 	 0.05112 	 0.00099 	 m..s
   24 	    51 	 0.05331 	 0.00106 	 m..s
   84 	    52 	 0.06547 	 0.00112 	 m..s
   28 	    53 	 0.05339 	 0.00119 	 m..s
   70 	    54 	 0.06122 	 0.00125 	 m..s
   17 	    55 	 0.05277 	 0.00126 	 m..s
   87 	    56 	 0.06714 	 0.00126 	 m..s
   56 	    57 	 0.05701 	 0.00127 	 m..s
   26 	    58 	 0.05336 	 0.00130 	 m..s
   33 	    59 	 0.05375 	 0.00135 	 m..s
    2 	    60 	 0.05085 	 0.00138 	 m..s
   89 	    61 	 0.07094 	 0.00139 	 m..s
   53 	    62 	 0.05502 	 0.00141 	 m..s
   40 	    63 	 0.05407 	 0.00141 	 m..s
   25 	    64 	 0.05333 	 0.00142 	 m..s
   76 	    65 	 0.06318 	 0.00142 	 m..s
   48 	    66 	 0.05482 	 0.00146 	 m..s
   67 	    67 	 0.05989 	 0.00150 	 m..s
   91 	    68 	 0.07447 	 0.00153 	 m..s
   32 	    69 	 0.05374 	 0.00167 	 m..s
   82 	    70 	 0.06463 	 0.00170 	 m..s
   12 	    71 	 0.05259 	 0.00170 	 m..s
    3 	    72 	 0.05100 	 0.00174 	 m..s
   69 	    73 	 0.06097 	 0.00181 	 m..s
   55 	    74 	 0.05676 	 0.00181 	 m..s
    9 	    75 	 0.05228 	 0.00195 	 m..s
   11 	    76 	 0.05250 	 0.00245 	 m..s
    4 	    77 	 0.05105 	 0.00283 	 m..s
   42 	    78 	 0.05413 	 0.00319 	 m..s
   52 	    79 	 0.05495 	 0.00319 	 m..s
   46 	    80 	 0.05450 	 0.00364 	 m..s
  103 	    81 	 0.18674 	 0.01790 	 MISS
   79 	    82 	 0.06366 	 0.04752 	 ~...
   99 	    83 	 0.14627 	 0.04963 	 m..s
   63 	    84 	 0.05833 	 0.06381 	 ~...
   65 	    85 	 0.05961 	 0.06450 	 ~...
   68 	    86 	 0.06055 	 0.06915 	 ~...
   64 	    87 	 0.05957 	 0.07080 	 ~...
   58 	    88 	 0.05755 	 0.07098 	 ~...
   21 	    89 	 0.05302 	 0.07464 	 ~...
   59 	    90 	 0.05761 	 0.07472 	 ~...
  109 	    91 	 0.22161 	 0.07483 	 MISS
   93 	    92 	 0.08044 	 0.07493 	 ~...
   15 	    93 	 0.05272 	 0.07514 	 ~...
   72 	    94 	 0.06152 	 0.07636 	 ~...
   94 	    95 	 0.08229 	 0.07650 	 ~...
   95 	    96 	 0.08944 	 0.07869 	 ~...
  113 	    97 	 0.25828 	 0.07963 	 MISS
   71 	    98 	 0.06145 	 0.07981 	 ~...
   60 	    99 	 0.05771 	 0.08511 	 ~...
   97 	   100 	 0.12395 	 0.10118 	 ~...
  111 	   101 	 0.23773 	 0.10850 	 MISS
  110 	   102 	 0.23031 	 0.12216 	 MISS
   96 	   103 	 0.09757 	 0.13256 	 m..s
  102 	   104 	 0.17754 	 0.13901 	 m..s
  100 	   105 	 0.16102 	 0.14035 	 ~...
  106 	   106 	 0.20421 	 0.14575 	 m..s
   98 	   107 	 0.14584 	 0.14792 	 ~...
  108 	   108 	 0.21728 	 0.16228 	 m..s
  107 	   109 	 0.21359 	 0.17068 	 m..s
  104 	   110 	 0.19402 	 0.17323 	 ~...
  105 	   111 	 0.20318 	 0.18384 	 ~...
  101 	   112 	 0.17172 	 0.18508 	 ~...
  112 	   113 	 0.25555 	 0.18564 	 m..s
  116 	   114 	 0.28176 	 0.21629 	 m..s
  115 	   115 	 0.27095 	 0.22307 	 m..s
  119 	   116 	 0.33880 	 0.26355 	 m..s
  114 	   117 	 0.27014 	 0.27048 	 ~...
  117 	   118 	 0.30910 	 0.27213 	 m..s
  118 	   119 	 0.33642 	 0.29051 	 m..s
  120 	   120 	 0.38513 	 0.34191 	 m..s
==========================================
r_mrr = 0.9035838842391968
r2_mrr = 0.40322649478912354
spearmanr_mrr@5 = 0.9041934609413147
spearmanr_mrr@10 = 0.9557346105575562
spearmanr_mrr@50 = 0.9623463153839111
spearmanr_mrr@100 = 0.9710453152656555
spearmanr_mrr@All = 0.972800612449646
==========================================
test time: 0.467
Done Testing dataset OpenEA
total time taken: 236.41158246994019
training time taken: 212.57691502571106
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 2420341342983926
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [679, 990, 369, 661, 553, 1146, 84, 284, 904, 747, 780, 460, 738, 26, 809, 230, 576, 354, 70, 366, 228, 1148, 482, 919, 439, 17, 473, 501, 942, 424, 529, 1018, 465, 160, 364, 1073, 438, 595, 155, 36, 678, 640, 630, 244, 798, 719, 98, 1103, 820, 539, 892, 340, 1039, 64, 229, 750, 390, 1205, 289, 1017, 1186, 884, 819, 388, 619, 607, 523, 475, 412, 433, 227, 664, 181, 535, 158, 1206, 561, 857, 73, 916, 178, 177, 66, 1072, 562, 866, 319, 1139, 832, 927, 1036, 614, 579, 941, 736, 908, 347, 620, 20, 715, 874, 1150, 127, 945, 1050, 195, 437, 694, 1159, 1091, 700, 1200, 257, 786, 1001, 954, 647, 291, 1154, 662, 898]
valid_ids (0): []
train_ids (1094): [2, 889, 274, 1211, 546, 888, 45, 303, 756, 410, 506, 573, 342, 753, 850, 211, 235, 329, 324, 1048, 762, 8, 140, 758, 191, 1096, 174, 748, 1006, 415, 852, 43, 102, 687, 1095, 720, 1214, 1174, 639, 253, 1077, 1011, 914, 1057, 759, 844, 1162, 1031, 1142, 681, 30, 906, 938, 723, 24, 785, 1136, 494, 571, 1016, 853, 701, 418, 318, 478, 778, 1168, 7, 769, 655, 180, 1010, 430, 245, 1172, 481, 685, 445, 559, 240, 721, 332, 400, 466, 952, 810, 830, 1093, 575, 602, 1165, 773, 962, 497, 770, 987, 746, 622, 48, 802, 527, 1191, 569, 761, 991, 827, 627, 254, 1177, 733, 507, 674, 1185, 368, 1104, 22, 122, 725, 1198, 760, 335, 545, 842, 416, 57, 1175, 406, 200, 1082, 262, 1084, 428, 273, 1098, 534, 966, 757, 735, 979, 896, 429, 551, 833, 40, 683, 548, 493, 584, 514, 741, 372, 714, 610, 643, 359, 212, 525, 1028, 978, 1060, 395, 787, 293, 1021, 246, 305, 742, 768, 170, 310, 1130, 440, 772, 444, 54, 1117, 864, 880, 811, 1065, 1047, 463, 642, 729, 184, 104, 456, 164, 280, 1079, 782, 248, 986, 1080, 1081, 617, 210, 929, 153, 91, 1045, 1128, 116, 1101, 763, 500, 988, 146, 74, 901, 88, 1030, 172, 931, 960, 333, 1090, 670, 969, 509, 25, 206, 781, 1199, 1135, 1023, 666, 32, 1094, 358, 1149, 265, 251, 441, 696, 790, 499, 949, 590, 600, 895, 570, 1169, 963, 824, 147, 636, 189, 691, 1078, 225, 276, 847, 815, 800, 615, 414, 812, 886, 572, 76, 495, 933, 68, 1127, 789, 498, 910, 754, 1153, 130, 269, 951, 1112, 1055, 258, 699, 1062, 612, 1190, 867, 281, 185, 11, 726, 169, 1075, 709, 261, 677, 50, 1015, 637, 110, 621, 817, 651, 601, 337, 652, 260, 672, 1120, 134, 1115, 932, 14, 542, 379, 151, 512, 123, 879, 85, 1170, 698, 1070, 788, 654, 835, 86, 304, 256, 975, 377, 1167, 518, 1107, 111, 634, 183, 771, 394, 710, 680, 313, 1033, 435, 79, 314, 344, 957, 426, 224, 1106, 1187, 31, 152, 108, 290, 90, 1194, 922, 327, 878, 505, 541, 1020, 234, 799, 450, 718, 480, 249, 1183, 80, 479, 974, 154, 1197, 1076, 383, 939, 851, 336, 722, 21, 131, 427, 792, 1019, 618, 166, 490, 1192, 483, 97, 774, 531, 401, 425, 357, 51, 209, 893, 259, 1181, 971, 688, 900, 202, 348, 1066, 1176, 46, 156, 215, 955, 667, 606, 1126, 1184, 221, 326, 382, 536, 204, 716, 1042, 1195, 83, 538, 398, 1013, 352, 838, 345, 924, 232, 380, 496, 1118, 1067, 845, 145, 135, 568, 766, 1124, 320, 1151, 608, 707, 386, 555, 1038, 985, 243, 1007, 277, 485, 133, 403, 476, 1009, 669, 373, 176, 1171, 409, 1051, 854, 549, 188, 1212, 1125, 744, 378, 141, 157, 190, 411, 1123, 375, 219, 961, 948, 998, 532, 72, 58, 457, 795, 580, 885, 784, 1049, 1041, 983, 624, 99, 384, 605, 75, 791, 558, 508, 165, 1032, 641, 448, 503, 374, 724, 905, 77, 980, 93, 663, 537, 404, 585, 238, 587, 162, 935, 1110, 717, 711, 182, 764, 12, 826, 692, 462, 848, 192, 873, 899, 363, 511, 139, 890, 689, 3, 422, 861, 1012, 806, 338, 739, 740, 37, 656, 446, 413, 323, 168, 351, 282, 1178, 855, 1054, 252, 1140, 526, 61, 1005, 126, 632, 604, 519, 350, 515, 1014, 603, 339, 517, 442, 1069, 81, 330, 150, 793, 1022, 920, 592, 921, 676, 103, 1152, 33, 829, 510, 504, 649, 78, 263, 207, 272, 217, 704, 436, 860, 1074, 299, 897, 596, 28, 887, 241, 682, 846, 658, 408, 1046, 447, 836, 956, 566, 593, 599, 469, 684, 730, 994, 316, 312, 965, 950, 63, 356, 5, 1003, 1133, 461, 1026, 959, 560, 1143, 449, 114, 19, 106, 367, 278, 918, 745, 397, 849, 1111, 673, 911, 236, 547, 1088, 361, 1155, 301, 112, 453, 1008, 266, 831, 113, 1083, 226, 977, 638, 218, 891, 626, 10, 591, 633, 1043, 1132, 311, 371, 894, 296, 1207, 431, 976, 1040, 13, 1037, 743, 841, 92, 69, 1109, 513, 34, 734, 247, 1059, 993, 392, 953, 283, 279, 486, 1163, 47, 816, 389, 87, 923, 35, 59, 370, 958, 1138, 402, 109, 628, 242, 376, 1134, 455, 1092, 161, 1102, 1158, 477, 1114, 65, 623, 233, 1085, 946, 871, 173, 343, 583, 671, 997, 749, 727, 15, 999, 365, 285, 1161, 9, 1052, 1208, 464, 972, 752, 470, 645, 1156, 533, 44, 930, 95, 737, 903, 391, 292, 875, 1193, 1, 1204, 52, 399, 909, 947, 657, 62, 1210, 474, 865, 297, 82, 144, 563, 487, 616, 41, 859, 1116, 697, 804, 288, 1027, 1113, 915, 71, 137, 564, 100, 237, 712, 471, 163, 231, 1119, 668, 353, 936, 708, 271, 216, 315, 765, 1145, 635, 322, 355, 360, 222, 925, 205, 818, 713, 796, 143, 825, 167, 629, 834, 94, 298, 1100, 89, 1188, 443, 201, 877, 522, 1201, 186, 872, 268, 926, 928, 1108, 912, 937, 4, 843, 705, 943, 1180, 970, 828, 732, 863, 693, 862, 597, 837, 489, 107, 49, 1000, 1196, 124, 574, 451, 528, 286, 115, 60, 581, 554, 309, 213, 128, 1122, 1173, 989, 16, 840, 1129, 516, 1087, 609, 302, 194, 492, 653, 944, 706, 675, 992, 940, 644, 776, 856, 1157, 419, 432, 1137, 660, 421, 270, 405, 813, 565, 755, 808, 917, 396, 148, 582, 648, 393, 530, 119, 779, 349, 613, 488, 196, 1179, 540, 132, 556, 594, 794, 346, 598, 381, 870, 702, 120, 807, 821, 417, 6, 686, 220, 27, 387, 214, 868, 287, 611, 179, 967, 934, 858, 331, 902, 588, 1189, 423, 193, 458, 117, 1035, 39, 767, 1029, 1097, 1024, 552, 121, 472, 0, 665, 198, 1121, 484, 996, 783, 175, 334, 1002, 23, 881, 321, 29, 1147, 275, 454, 203, 308, 1166, 521, 984, 1203, 1064, 328, 728, 325, 1141, 1089, 317, 208, 307, 982, 159, 1044, 839, 913, 199, 968, 407, 907, 101, 125, 1144, 1131, 187, 543, 520, 797, 129, 149, 544, 524, 577, 1058, 300, 171, 142, 589, 197, 650, 659, 690, 468, 306, 239, 491, 1099, 459, 995, 695, 805, 105, 1071, 646, 751, 56, 1004, 42, 586, 1025, 731, 67, 775, 38, 703, 1034, 420, 250, 964, 876, 1182, 294, 801, 822, 1068, 1209, 578, 467, 557, 18, 452, 1160, 823, 803, 118, 981, 1086, 362, 567, 96, 814, 53, 1105, 264, 136, 1056, 55, 502, 1053, 973, 295, 434, 267, 625, 1202, 255, 550, 1213, 1061, 1063, 777, 882, 385, 223, 341, 1164, 138, 631, 883, 869]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5994404721095290
the save name prefix for this run is:  chkpt-ID_5994404721095290_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 44
rank avg (pred): 0.546 +- 0.003
mrr vals (pred, true): 0.013, 0.527
batch losses (mrrl, rdl): 0.0, 0.0054352544

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 852
rank avg (pred): 0.404 +- 0.247
mrr vals (pred, true): 0.175, 0.104
batch losses (mrrl, rdl): 0.0, 0.0001212301

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 316
rank avg (pred): 0.050 +- 0.037
mrr vals (pred, true): 0.354, 0.553
batch losses (mrrl, rdl): 0.0, 3.3055e-06

Epoch over!
epoch time: 14.163

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1189
rank avg (pred): 0.381 +- 0.273
mrr vals (pred, true): 0.248, 0.038
batch losses (mrrl, rdl): 0.0, 4.64372e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 601
rank avg (pred): 0.467 +- 0.291
mrr vals (pred, true): 0.180, 0.041
batch losses (mrrl, rdl): 0.0, 7.4916e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 716
rank avg (pred): 0.422 +- 0.291
mrr vals (pred, true): 0.227, 0.047
batch losses (mrrl, rdl): 0.0, 1.01509e-05

Epoch over!
epoch time: 13.806

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 309
rank avg (pred): 0.024 +- 0.020
mrr vals (pred, true): 0.453, 0.543
batch losses (mrrl, rdl): 0.0, 6.8052e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 862
rank avg (pred): 0.438 +- 0.283
mrr vals (pred, true): 0.184, 0.106
batch losses (mrrl, rdl): 0.0, 1.66545e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1089
rank avg (pred): 0.323 +- 0.256
mrr vals (pred, true): 0.247, 0.129
batch losses (mrrl, rdl): 0.0, 0.0001229964

Epoch over!
epoch time: 13.445

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1111
rank avg (pred): 0.363 +- 0.257
mrr vals (pred, true): 0.195, 0.045
batch losses (mrrl, rdl): 0.0, 8.86624e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 138
rank avg (pred): 0.344 +- 0.272
mrr vals (pred, true): 0.252, 0.106
batch losses (mrrl, rdl): 0.0, 0.0001039388

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 961
rank avg (pred): 0.574 +- 0.297
mrr vals (pred, true): 0.114, 0.044
batch losses (mrrl, rdl): 0.0, 0.0004040641

Epoch over!
epoch time: 13.829

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 269
rank avg (pred): 0.043 +- 0.036
mrr vals (pred, true): 0.424, 0.559
batch losses (mrrl, rdl): 0.0, 2.2906e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1081
rank avg (pred): 0.343 +- 0.259
mrr vals (pred, true): 0.236, 0.094
batch losses (mrrl, rdl): 0.0, 2.83646e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 706
rank avg (pred): 0.440 +- 0.278
mrr vals (pred, true): 0.163, 0.049
batch losses (mrrl, rdl): 0.0, 1.57997e-05

Epoch over!
epoch time: 13.9

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 48
rank avg (pred): 0.061 +- 0.053
mrr vals (pred, true): 0.403, 0.509
batch losses (mrrl, rdl): 0.1118681207, 5.8634e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 755
rank avg (pred): 0.239 +- 0.194
mrr vals (pred, true): 0.224, 0.245
batch losses (mrrl, rdl): 0.0045440011, 8.16976e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 41
rank avg (pred): 0.011 +- 0.010
mrr vals (pred, true): 0.583, 0.530
batch losses (mrrl, rdl): 0.0273235179, 1.88266e-05

Epoch over!
epoch time: 13.893

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 410
rank avg (pred): 0.345 +- 0.174
mrr vals (pred, true): 0.095, 0.048
batch losses (mrrl, rdl): 0.0200239588, 0.0001634933

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 341
rank avg (pred): 0.368 +- 0.149
mrr vals (pred, true): 0.065, 0.178
batch losses (mrrl, rdl): 0.1278681606, 0.0004717246

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 509
rank avg (pred): 0.195 +- 0.179
mrr vals (pred, true): 0.299, 0.280
batch losses (mrrl, rdl): 0.0036566018, 3.95224e-05

Epoch over!
epoch time: 14.822

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1080
rank avg (pred): 0.344 +- 0.150
mrr vals (pred, true): 0.068, 0.080
batch losses (mrrl, rdl): 0.0031097059, 4.66421e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 546
rank avg (pred): 0.438 +- 0.131
mrr vals (pred, true): 0.052, 0.064
batch losses (mrrl, rdl): 2.6053e-05, 9.77263e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 138
rank avg (pred): 0.364 +- 0.154
mrr vals (pred, true): 0.071, 0.106
batch losses (mrrl, rdl): 0.0122749507, 0.0001205119

Epoch over!
epoch time: 13.048

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 193
rank avg (pred): 0.356 +- 0.153
mrr vals (pred, true): 0.068, 0.041
batch losses (mrrl, rdl): 0.0031585698, 0.0002052235

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1162
rank avg (pred): 0.400 +- 0.145
mrr vals (pred, true): 0.057, 0.038
batch losses (mrrl, rdl): 0.0004233797, 0.0001952936

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1094
rank avg (pred): 0.320 +- 0.172
mrr vals (pred, true): 0.085, 0.110
batch losses (mrrl, rdl): 0.006485397, 6.92949e-05

Epoch over!
epoch time: 14.027

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 288
rank avg (pred): 0.018 +- 0.018
mrr vals (pred, true): 0.554, 0.536
batch losses (mrrl, rdl): 0.0033021816, 1.08467e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1172
rank avg (pred): 0.420 +- 0.147
mrr vals (pred, true): 0.056, 0.042
batch losses (mrrl, rdl): 0.0003958285, 0.0001098489

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 654
rank avg (pred): 0.375 +- 0.176
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0003888033, 0.0001218093

Epoch over!
epoch time: 14.949

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 69
rank avg (pred): 0.030 +- 0.031
mrr vals (pred, true): 0.508, 0.510
batch losses (mrrl, rdl): 3.47989e-05, 5.3814e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 769
rank avg (pred): 0.375 +- 0.190
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0002550713, 0.0003632173

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 705
rank avg (pred): 0.361 +- 0.206
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 6.34024e-05, 0.0001159894

Epoch over!
epoch time: 14.072

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 542
rank avg (pred): 0.405 +- 0.154
mrr vals (pred, true): 0.065, 0.124
batch losses (mrrl, rdl): 0.0346066654, 0.0001951685

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 657
rank avg (pred): 0.396 +- 0.177
mrr vals (pred, true): 0.045, 0.047
batch losses (mrrl, rdl): 0.0002567013, 4.48843e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 492
rank avg (pred): 0.316 +- 0.205
mrr vals (pred, true): 0.184, 0.185
batch losses (mrrl, rdl): 4.1802e-06, 0.000179717

Epoch over!
epoch time: 13.584

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 294
rank avg (pred): 0.030 +- 0.033
mrr vals (pred, true): 0.553, 0.540
batch losses (mrrl, rdl): 0.0016737083, 3.3806e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 466
rank avg (pred): 0.390 +- 0.169
mrr vals (pred, true): 0.058, 0.054
batch losses (mrrl, rdl): 0.000594195, 4.92801e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1211
rank avg (pred): 0.439 +- 0.150
mrr vals (pred, true): 0.063, 0.059
batch losses (mrrl, rdl): 0.0016602131, 4.60799e-05

Epoch over!
epoch time: 13.46

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 267
rank avg (pred): 0.042 +- 0.045
mrr vals (pred, true): 0.525, 0.549
batch losses (mrrl, rdl): 0.0056217886, 2.0057e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 250
rank avg (pred): 0.032 +- 0.036
mrr vals (pred, true): 0.554, 0.566
batch losses (mrrl, rdl): 0.0015180693, 4.22e-08

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 485
rank avg (pred): 0.357 +- 0.194
mrr vals (pred, true): 0.071, 0.047
batch losses (mrrl, rdl): 0.0045913686, 7.11878e-05

Epoch over!
epoch time: 13.532

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 295
rank avg (pred): 0.031 +- 0.036
mrr vals (pred, true): 0.574, 0.546
batch losses (mrrl, rdl): 0.0077016586, 9.508e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 142
rank avg (pred): 0.388 +- 0.175
mrr vals (pred, true): 0.068, 0.083
batch losses (mrrl, rdl): 0.0032379094, 8.77241e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 122
rank avg (pred): 0.394 +- 0.174
mrr vals (pred, true): 0.065, 0.113
batch losses (mrrl, rdl): 0.0233556349, 0.0002215972

Epoch over!
epoch time: 13.92

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.364 +- 0.226
mrr vals (pred, true): 0.052, 0.060

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   39 	     0 	 0.05683 	 0.01574 	 m..s
   43 	     1 	 0.06169 	 0.01614 	 m..s
   37 	     2 	 0.05579 	 0.01759 	 m..s
   93 	     3 	 0.09220 	 0.01876 	 m..s
    2 	     4 	 0.04399 	 0.01876 	 ~...
   33 	     5 	 0.05362 	 0.02025 	 m..s
   24 	     6 	 0.05169 	 0.03707 	 ~...
   13 	     7 	 0.04959 	 0.03710 	 ~...
   31 	     8 	 0.05224 	 0.03747 	 ~...
   14 	     9 	 0.04963 	 0.03808 	 ~...
   21 	    10 	 0.05157 	 0.04000 	 ~...
   15 	    11 	 0.04964 	 0.04031 	 ~...
    1 	    12 	 0.03998 	 0.04075 	 ~...
   64 	    13 	 0.06666 	 0.04100 	 ~...
   32 	    14 	 0.05271 	 0.04115 	 ~...
   74 	    15 	 0.06840 	 0.04242 	 ~...
   19 	    16 	 0.05137 	 0.04279 	 ~...
   71 	    17 	 0.06735 	 0.04283 	 ~...
   38 	    18 	 0.05674 	 0.04302 	 ~...
   76 	    19 	 0.06935 	 0.04303 	 ~...
   12 	    20 	 0.04945 	 0.04304 	 ~...
   62 	    21 	 0.06623 	 0.04308 	 ~...
   10 	    22 	 0.04938 	 0.04316 	 ~...
   48 	    23 	 0.06307 	 0.04374 	 ~...
   66 	    24 	 0.06680 	 0.04377 	 ~...
   22 	    25 	 0.05160 	 0.04418 	 ~...
   23 	    26 	 0.05169 	 0.04430 	 ~...
   63 	    27 	 0.06646 	 0.04447 	 ~...
    8 	    28 	 0.04857 	 0.04460 	 ~...
   29 	    29 	 0.05192 	 0.04475 	 ~...
   11 	    30 	 0.04941 	 0.04476 	 ~...
   26 	    31 	 0.05180 	 0.04479 	 ~...
   30 	    32 	 0.05220 	 0.04507 	 ~...
   40 	    33 	 0.05783 	 0.04514 	 ~...
   25 	    34 	 0.05173 	 0.04581 	 ~...
   52 	    35 	 0.06433 	 0.04587 	 ~...
   79 	    36 	 0.07006 	 0.04619 	 ~...
   35 	    37 	 0.05464 	 0.04624 	 ~...
   73 	    38 	 0.06768 	 0.04630 	 ~...
   34 	    39 	 0.05451 	 0.04678 	 ~...
   45 	    40 	 0.06203 	 0.04682 	 ~...
   68 	    41 	 0.06688 	 0.04698 	 ~...
    5 	    42 	 0.04660 	 0.04738 	 ~...
   56 	    43 	 0.06538 	 0.04750 	 ~...
   59 	    44 	 0.06606 	 0.04754 	 ~...
   44 	    45 	 0.06197 	 0.04759 	 ~...
    4 	    46 	 0.04634 	 0.04801 	 ~...
   51 	    47 	 0.06430 	 0.04816 	 ~...
   36 	    48 	 0.05501 	 0.04851 	 ~...
    7 	    49 	 0.04854 	 0.04921 	 ~...
   27 	    50 	 0.05190 	 0.04950 	 ~...
   18 	    51 	 0.05084 	 0.04951 	 ~...
   17 	    52 	 0.05073 	 0.04954 	 ~...
   57 	    53 	 0.06543 	 0.04960 	 ~...
   84 	    54 	 0.07188 	 0.04967 	 ~...
    9 	    55 	 0.04936 	 0.05005 	 ~...
   77 	    56 	 0.06962 	 0.05054 	 ~...
    6 	    57 	 0.04681 	 0.05069 	 ~...
   60 	    58 	 0.06607 	 0.05074 	 ~...
    3 	    59 	 0.04587 	 0.05143 	 ~...
   20 	    60 	 0.05143 	 0.05240 	 ~...
   16 	    61 	 0.05016 	 0.05266 	 ~...
   72 	    62 	 0.06757 	 0.05341 	 ~...
   61 	    63 	 0.06623 	 0.05593 	 ~...
   54 	    64 	 0.06469 	 0.05604 	 ~...
   28 	    65 	 0.05191 	 0.05983 	 ~...
   55 	    66 	 0.06500 	 0.06182 	 ~...
   42 	    67 	 0.06160 	 0.06286 	 ~...
   65 	    68 	 0.06677 	 0.06704 	 ~...
   41 	    69 	 0.06056 	 0.06948 	 ~...
    0 	    70 	 0.03981 	 0.06967 	 ~...
   46 	    71 	 0.06244 	 0.07170 	 ~...
   67 	    72 	 0.06685 	 0.07317 	 ~...
   78 	    73 	 0.07002 	 0.08228 	 ~...
   49 	    74 	 0.06381 	 0.08797 	 ~...
   47 	    75 	 0.06292 	 0.09621 	 m..s
   89 	    76 	 0.07978 	 0.10029 	 ~...
   53 	    77 	 0.06454 	 0.10147 	 m..s
   69 	    78 	 0.06691 	 0.10651 	 m..s
   70 	    79 	 0.06708 	 0.10981 	 m..s
   88 	    80 	 0.07554 	 0.11408 	 m..s
   85 	    81 	 0.07214 	 0.11517 	 m..s
   91 	    82 	 0.08998 	 0.12212 	 m..s
   75 	    83 	 0.06934 	 0.12400 	 m..s
   90 	    84 	 0.08941 	 0.12660 	 m..s
   50 	    85 	 0.06401 	 0.13014 	 m..s
   87 	    86 	 0.07243 	 0.13168 	 m..s
   80 	    87 	 0.07007 	 0.13182 	 m..s
   86 	    88 	 0.07236 	 0.13310 	 m..s
   83 	    89 	 0.07109 	 0.13355 	 m..s
   58 	    90 	 0.06580 	 0.13372 	 m..s
   95 	    91 	 0.09290 	 0.14093 	 m..s
   81 	    92 	 0.07033 	 0.14099 	 m..s
   82 	    93 	 0.07092 	 0.14238 	 m..s
   92 	    94 	 0.09060 	 0.14609 	 m..s
   94 	    95 	 0.09289 	 0.16048 	 m..s
   96 	    96 	 0.13947 	 0.19126 	 m..s
   97 	    97 	 0.18549 	 0.23045 	 m..s
   98 	    98 	 0.43669 	 0.40848 	 ~...
   99 	    99 	 0.45951 	 0.40925 	 m..s
  104 	   100 	 0.53090 	 0.51200 	 ~...
  105 	   101 	 0.53168 	 0.51556 	 ~...
  112 	   102 	 0.54787 	 0.51598 	 m..s
  101 	   103 	 0.48454 	 0.51622 	 m..s
  103 	   104 	 0.50039 	 0.51763 	 ~...
  100 	   105 	 0.48284 	 0.51956 	 m..s
  106 	   106 	 0.53249 	 0.52120 	 ~...
  111 	   107 	 0.54764 	 0.52563 	 ~...
  107 	   108 	 0.53251 	 0.52858 	 ~...
  102 	   109 	 0.49609 	 0.53267 	 m..s
  108 	   110 	 0.53596 	 0.53485 	 ~...
  113 	   111 	 0.54809 	 0.53566 	 ~...
  109 	   112 	 0.53668 	 0.53922 	 ~...
  117 	   113 	 0.55450 	 0.54162 	 ~...
  110 	   114 	 0.54421 	 0.54463 	 ~...
  114 	   115 	 0.54912 	 0.55007 	 ~...
  118 	   116 	 0.55739 	 0.55053 	 ~...
  115 	   117 	 0.54996 	 0.55251 	 ~...
  119 	   118 	 0.55764 	 0.55256 	 ~...
  116 	   119 	 0.55385 	 0.55778 	 ~...
  120 	   120 	 0.56298 	 0.56122 	 ~...
==========================================
r_mrr = 0.9878528118133545
r2_mrr = 0.9755908250808716
spearmanr_mrr@5 = 0.9128615856170654
spearmanr_mrr@10 = 0.9250988364219666
spearmanr_mrr@50 = 0.9963564872741699
spearmanr_mrr@100 = 0.9911323189735413
spearmanr_mrr@All = 0.9908212423324585
==========================================
test time: 0.442
Done Testing dataset UMLS
total time taken: 214.907546043396
training time taken: 208.9590790271759
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 4674981592856998
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1094, 455, 634, 32, 805, 730, 1050, 521, 278, 548, 268, 582, 594, 1174, 282, 985, 835, 649, 437, 457, 247, 188, 16, 952, 321, 855, 839, 1056, 734, 223, 930, 957, 1128, 625, 754, 1171, 945, 253, 1061, 80, 1175, 490, 655, 688, 811, 436, 320, 769, 452, 748, 774, 23, 1083, 1199, 825, 962, 991, 1054, 210, 1036, 19, 803, 653, 54, 943, 219, 259, 164, 102, 1135, 326, 145, 135, 1091, 143, 648, 37, 737, 224, 402, 958, 609, 872, 875, 533, 194, 349, 412, 818, 876, 144, 126, 1210, 715, 520, 561, 48, 979, 44, 1071, 617, 392, 1129, 106, 1102, 687, 168, 63, 492, 611, 502, 200, 903, 67, 1108, 1185, 824, 963, 986, 852, 701]
valid_ids (0): []
train_ids (1094): [242, 505, 564, 707, 435, 186, 311, 937, 865, 211, 421, 927, 178, 571, 418, 801, 753, 451, 526, 131, 196, 886, 447, 104, 1213, 971, 1113, 1006, 516, 692, 420, 816, 1131, 263, 355, 397, 14, 81, 1153, 831, 430, 140, 553, 411, 303, 925, 458, 351, 363, 93, 929, 1138, 1205, 432, 993, 907, 842, 42, 125, 1028, 1200, 1162, 751, 236, 433, 550, 21, 1154, 951, 348, 319, 1012, 1003, 441, 731, 239, 959, 75, 1074, 1045, 1209, 1164, 938, 503, 598, 1189, 782, 644, 174, 454, 255, 1151, 820, 563, 86, 627, 25, 271, 884, 95, 1157, 328, 39, 325, 1019, 1063, 1041, 79, 777, 861, 274, 2, 926, 470, 792, 132, 877, 916, 269, 826, 409, 403, 758, 723, 1110, 1156, 551, 356, 542, 401, 335, 700, 58, 92, 755, 440, 148, 66, 983, 973, 1053, 127, 1097, 515, 17, 666, 339, 620, 994, 336, 950, 804, 786, 394, 874, 49, 797, 785, 672, 531, 491, 212, 36, 696, 918, 873, 361, 374, 385, 89, 362, 749, 84, 391, 1166, 756, 152, 1121, 765, 301, 1069, 387, 159, 915, 346, 76, 1072, 989, 449, 988, 133, 156, 752, 3, 358, 583, 848, 545, 1000, 1095, 966, 379, 60, 371, 307, 844, 225, 608, 718, 243, 373, 1105, 7, 558, 287, 750, 539, 911, 31, 171, 395, 712, 262, 713, 1120, 234, 899, 427, 389, 469, 162, 535, 249, 673, 509, 1186, 961, 96, 431, 297, 182, 1158, 261, 726, 497, 97, 784, 871, 709, 15, 475, 536, 1117, 512, 357, 1150, 773, 1109, 540, 187, 960, 575, 496, 322, 481, 460, 87, 300, 304, 645, 1192, 290, 313, 256, 474, 632, 1112, 921, 172, 283, 1184, 552, 935, 1015, 947, 114, 725, 849, 612, 997, 870, 13, 312, 549, 1084, 1143, 1134, 103, 1139, 710, 636, 1132, 1089, 791, 264, 588, 1022, 488, 352, 1170, 9, 590, 891, 1197, 1020, 589, 577, 316, 101, 484, 689, 837, 417, 879, 847, 888, 864, 614, 1191, 585, 123, 284, 1034, 828, 708, 621, 459, 1079, 508, 288, 91, 1060, 822, 821, 670, 980, 604, 694, 560, 354, 344, 579, 112, 939, 154, 191, 28, 906, 534, 965, 856, 982, 1073, 578, 1206, 808, 350, 453, 257, 1093, 142, 615, 177, 169, 942, 173, 1203, 1127, 158, 829, 424, 82, 368, 817, 833, 461, 838, 720, 221, 794, 668, 139, 525, 775, 439, 895, 1011, 1141, 55, 853, 832, 678, 912, 57, 719, 1031, 203, 867, 622, 468, 727, 429, 841, 428, 1090, 562, 1165, 665, 650, 546, 176, 630, 381, 664, 109, 823, 228, 592, 423, 796, 1013, 56, 1133, 862, 232, 308, 364, 717, 799, 1049, 43, 1194, 506, 721, 631, 359, 1195, 334, 778, 149, 170, 651, 759, 776, 64, 414, 969, 462, 372, 111, 1122, 834, 685, 98, 11, 105, 1123, 541, 595, 586, 479, 1046, 342, 519, 619, 568, 810, 52, 1145, 556, 1002, 527, 341, 944, 110, 1008, 207, 466, 1027, 624, 998, 587, 1208, 760, 296, 663, 972, 260, 956, 473, 347, 254, 258, 222, 893, 779, 332, 883, 857, 1116, 377, 854, 602, 978, 216, 107, 1004, 443, 868, 416, 0, 742, 405, 662, 370, 33, 375, 714, 680, 1037, 26, 1106, 528, 738, 465, 1100, 908, 291, 547, 1058, 1142, 641, 941, 442, 252, 46, 1052, 18, 380, 1182, 658, 62, 798, 900, 121, 1086, 669, 1196, 1099, 345, 317, 1017, 529, 314, 1029, 30, 1068, 119, 554, 74, 635, 330, 887, 286, 147, 559, 65, 1025, 472, 984, 1009, 596, 1059, 281, 613, 1026, 677, 691, 338, 1092, 600, 892, 740, 360, 426, 241, 607, 530, 728, 1124, 859, 767, 977, 309, 122, 165, 1207, 1001, 61, 1169, 995, 836, 100, 647, 1148, 659, 230, 1188, 815, 1152, 214, 298, 1146, 279, 967, 1064, 1, 894, 217, 250, 643, 711, 12, 914, 642, 686, 498, 1103, 456, 922, 936, 202, 757, 1193, 408, 819, 679, 201, 406, 783, 369, 494, 40, 59, 398, 523, 4, 761, 576, 266, 566, 276, 766, 1160, 486, 538, 1168, 806, 476, 1038, 990, 763, 681, 499, 167, 569, 273, 1030, 981, 137, 231, 999, 795, 809, 199, 1167, 1107, 869, 606, 1098, 206, 716, 628, 1062, 388, 415, 733, 1190, 160, 118, 20, 337, 1178, 108, 640, 293, 878, 964, 99, 1136, 860, 190, 919, 882, 928, 1075, 1078, 1126, 544, 968, 1048, 299, 573, 386, 504, 400, 524, 846, 146, 1085, 580, 830, 248, 514, 904, 1005, 446, 1163, 850, 574, 1070, 845, 932, 764, 204, 365, 73, 41, 699, 690, 1111, 889, 781, 34, 1080, 975, 53, 866, 557, 485, 584, 272, 205, 471, 1082, 616, 376, 445, 917, 450, 136, 5, 1101, 772, 565, 195, 1076, 949, 1179, 150, 789, 605, 1137, 88, 1014, 923, 1202, 1214, 94, 741, 410, 909, 657, 814, 323, 863, 682, 209, 555, 438, 378, 27, 702, 115, 185, 157, 275, 183, 277, 623, 141, 1155, 310, 1043, 601, 218, 396, 1119, 116, 517, 913, 1147, 654, 652, 638, 50, 639, 735, 1066, 593, 1176, 629, 464, 413, 384, 113, 744, 1055, 366, 898, 992, 47, 955, 610, 1149, 1067, 189, 1183, 480, 151, 1039, 745, 353, 77, 184, 934, 463, 896, 1010, 280, 245, 885, 793, 770, 390, 138, 38, 180, 404, 974, 467, 768, 1024, 1042, 603, 237, 10, 448, 661, 851, 697, 787, 129, 289, 425, 724, 1035, 626, 591, 1130, 567, 1087, 161, 1161, 128, 570, 705, 843, 513, 1114, 1177, 996, 208, 483, 572, 660, 324, 240, 532, 407, 1051, 1104, 698, 71, 905, 45, 294, 920, 130, 790, 1172, 646, 1187, 124, 1023, 1140, 743, 1032, 444, 477, 285, 70, 671, 193, 1096, 976, 722, 746, 970, 175, 306, 72, 198, 1040, 265, 840, 802, 500, 1088, 1057, 134, 8, 902, 910, 736, 305, 315, 890, 238, 597, 478, 85, 693, 543, 181, 1159, 729, 827, 215, 226, 762, 1144, 90, 434, 333, 518, 1212, 51, 192, 706, 1081, 329, 327, 501, 674, 940, 1016, 270, 618, 511, 489, 1118, 780, 788, 897, 179, 1033, 880, 1181, 487, 704, 771, 522, 24, 399, 419, 343, 302, 68, 1211, 220, 382, 924, 684, 1047, 695, 656, 267, 166, 1018, 675, 1007, 197, 954, 739, 948, 931, 495, 510, 295, 858, 69, 1044, 812, 229, 732, 340, 1065, 1021, 227, 244, 683, 537, 987, 953, 637, 1180, 251, 881, 29, 235, 83, 813, 393, 581, 901, 383, 1077, 482, 507, 747, 946, 676, 667, 292, 633, 120, 800, 1201, 6, 331, 22, 1125, 153, 318, 422, 933, 703, 233, 213, 493, 78, 163, 1115, 367, 1204, 246, 35, 155, 807, 599, 1173, 117, 1198]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4119288082181835
the save name prefix for this run is:  chkpt-ID_4119288082181835_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.408 +- 0.009
mrr vals (pred, true): 0.001, 0.259
batch losses (mrrl, rdl): 0.0, 0.0027966928

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1141
rank avg (pred): 0.208 +- 0.146
mrr vals (pred, true): 0.110, 0.140
batch losses (mrrl, rdl): 0.0, 0.0001567709

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.279 +- 0.244
mrr vals (pred, true): 0.250, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005583505

Epoch over!
epoch time: 13.841

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 206
rank avg (pred): 0.228 +- 0.213
mrr vals (pred, true): 0.296, 0.004
batch losses (mrrl, rdl): 0.0, 0.001081845

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.251 +- 0.249
mrr vals (pred, true): 0.329, 0.242
batch losses (mrrl, rdl): 0.0, 0.0010352815

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 272
rank avg (pred): 0.053 +- 0.052
mrr vals (pred, true): 0.323, 0.238
batch losses (mrrl, rdl): 0.0, 1.6538e-06

Epoch over!
epoch time: 12.802

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 580
rank avg (pred): 0.307 +- 0.281
mrr vals (pred, true): 0.260, 0.109
batch losses (mrrl, rdl): 0.0, 0.0001598535

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.248 +- 0.244
mrr vals (pred, true): 0.292, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007545434

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 960
rank avg (pred): 0.663 +- 0.355
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 0.0, 0.000712134

Epoch over!
epoch time: 13.366

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.196 +- 0.210
mrr vals (pred, true): 0.326, 0.201
batch losses (mrrl, rdl): 0.0, 0.0005787601

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.226 +- 0.228
mrr vals (pred, true): 0.287, 0.218
batch losses (mrrl, rdl): 0.0, 0.0008128911

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.376 +- 0.321
mrr vals (pred, true): 0.230, 0.004
batch losses (mrrl, rdl): 0.0, 8.28718e-05

Epoch over!
epoch time: 12.611

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.597 +- 0.401
mrr vals (pred, true): 0.158, 0.002
batch losses (mrrl, rdl): 0.0, 8.84262e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 332
rank avg (pred): 0.230 +- 0.231
mrr vals (pred, true): 0.272, 0.192
batch losses (mrrl, rdl): 0.0, 0.0007746666

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.222 +- 0.236
mrr vals (pred, true): 0.323, 0.230
batch losses (mrrl, rdl): 0.0, 0.0007086131

Epoch over!
epoch time: 12.539

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.059 +- 0.067
mrr vals (pred, true): 0.399, 0.141
batch losses (mrrl, rdl): 0.6660301089, 2.5234e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.016 +- 0.013
mrr vals (pred, true): 0.260, 0.256
batch losses (mrrl, rdl): 0.0001713729, 1.73983e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.350 +- 0.190
mrr vals (pred, true): 0.097, 0.156
batch losses (mrrl, rdl): 0.0340698063, 0.0007744837

Epoch over!
epoch time: 13.198

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 827
rank avg (pred): 0.059 +- 0.044
mrr vals (pred, true): 0.199, 0.256
batch losses (mrrl, rdl): 0.0321143083, 6.2971e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.278 +- 0.191
mrr vals (pred, true): 0.134, 0.004
batch losses (mrrl, rdl): 0.0711288229, 0.0007819599

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.358 +- 0.179
mrr vals (pred, true): 0.084, 0.153
batch losses (mrrl, rdl): 0.0486900508, 0.0008833436

Epoch over!
epoch time: 13.858

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.309 +- 0.193
mrr vals (pred, true): 0.118, 0.213
batch losses (mrrl, rdl): 0.0910319313, 0.0014290478

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.023 +- 0.017
mrr vals (pred, true): 0.243, 0.253
batch losses (mrrl, rdl): 0.0008889523, 3.0251e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.293 +- 0.196
mrr vals (pred, true): 0.139, 0.003
batch losses (mrrl, rdl): 0.0791023523, 0.0005791116

Epoch over!
epoch time: 13.714

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 940
rank avg (pred): 0.604 +- 0.238
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0011386875, 0.0005303197

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 659
rank avg (pred): 0.336 +- 0.181
mrr vals (pred, true): 0.109, 0.003
batch losses (mrrl, rdl): 0.0346037857, 0.0003554749

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 810
rank avg (pred): 0.326 +- 0.179
mrr vals (pred, true): 0.100, 0.151
batch losses (mrrl, rdl): 0.026173396, 0.0011629212

Epoch over!
epoch time: 13.557

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 244
rank avg (pred): 0.016 +- 0.011
mrr vals (pred, true): 0.233, 0.221
batch losses (mrrl, rdl): 0.0015226296, 2.59887e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.341 +- 0.167
mrr vals (pred, true): 0.086, 0.049
batch losses (mrrl, rdl): 0.0129755847, 0.0003110486

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.335 +- 0.166
mrr vals (pred, true): 0.092, 0.043
batch losses (mrrl, rdl): 0.0177975893, 0.0002918042

Epoch over!
epoch time: 12.934

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 35
rank avg (pred): 0.052 +- 0.037
mrr vals (pred, true): 0.201, 0.192
batch losses (mrrl, rdl): 0.000986046, 2.4488e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.342 +- 0.160
mrr vals (pred, true): 0.072, 0.160
batch losses (mrrl, rdl): 0.0766946226, 0.0006770904

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.345 +- 0.159
mrr vals (pred, true): 0.071, 0.004
batch losses (mrrl, rdl): 0.0045222542, 0.000341629

Epoch over!
epoch time: 13.407

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1152
rank avg (pred): 0.322 +- 0.165
mrr vals (pred, true): 0.097, 0.055
batch losses (mrrl, rdl): 0.0222178958, 0.0003573194

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.334 +- 0.157
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0071485965, 0.0003667921

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.273 +- 0.182
mrr vals (pred, true): 0.140, 0.004
batch losses (mrrl, rdl): 0.080492124, 0.0008009379

Epoch over!
epoch time: 13.372

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 78
rank avg (pred): 0.031 +- 0.021
mrr vals (pred, true): 0.183, 0.222
batch losses (mrrl, rdl): 0.0158580225, 1.10334e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.322 +- 0.163
mrr vals (pred, true): 0.095, 0.156
batch losses (mrrl, rdl): 0.0376144163, 0.0005183208

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 334
rank avg (pred): 0.296 +- 0.176
mrr vals (pred, true): 0.130, 0.203
batch losses (mrrl, rdl): 0.0534274764, 0.0013448852

Epoch over!
epoch time: 14.818

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.287 +- 0.181
mrr vals (pred, true): 0.136, 0.201
batch losses (mrrl, rdl): 0.0425373577, 0.0013029136

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1021
rank avg (pred): 0.275 +- 0.184
mrr vals (pred, true): 0.160, 0.207
batch losses (mrrl, rdl): 0.0223504025, 0.0011800287

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.647 +- 0.292
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 4.985e-06, 0.0002387705

Epoch over!
epoch time: 12.991

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.338 +- 0.145
mrr vals (pred, true): 0.067, 0.049
batch losses (mrrl, rdl): 0.0028811183, 0.0002908458

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.402 +- 0.172
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.000899674, 9.74146e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 619
rank avg (pred): 0.335 +- 0.147
mrr vals (pred, true): 0.070, 0.172
batch losses (mrrl, rdl): 0.1037848666, 0.00071386

Epoch over!
epoch time: 13.925

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.274 +- 0.174
mrr vals (pred, true): 0.142, 0.203

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04785 	 0.00074 	 m..s
    2 	     1 	 0.04657 	 0.00076 	 m..s
   46 	     2 	 0.11474 	 0.00104 	 MISS
    9 	     3 	 0.06168 	 0.00121 	 m..s
   41 	     4 	 0.10874 	 0.00340 	 MISS
   70 	     5 	 0.13333 	 0.00343 	 MISS
   77 	     6 	 0.13855 	 0.00344 	 MISS
   10 	     7 	 0.06322 	 0.00344 	 m..s
   59 	     8 	 0.13318 	 0.00348 	 MISS
   29 	     9 	 0.09725 	 0.00352 	 m..s
   74 	    10 	 0.13753 	 0.00356 	 MISS
    5 	    11 	 0.04810 	 0.00363 	 m..s
   25 	    12 	 0.09060 	 0.00366 	 m..s
   35 	    13 	 0.09844 	 0.00371 	 m..s
   40 	    14 	 0.10772 	 0.00376 	 MISS
   59 	    15 	 0.13318 	 0.00380 	 MISS
   53 	    16 	 0.12843 	 0.00385 	 MISS
   84 	    17 	 0.14266 	 0.00385 	 MISS
    3 	    18 	 0.04771 	 0.00393 	 m..s
   52 	    19 	 0.12031 	 0.00394 	 MISS
   57 	    20 	 0.13183 	 0.00394 	 MISS
    8 	    21 	 0.06104 	 0.00395 	 m..s
   72 	    22 	 0.13523 	 0.00398 	 MISS
   73 	    23 	 0.13663 	 0.00403 	 MISS
    7 	    24 	 0.04853 	 0.00412 	 m..s
   55 	    25 	 0.12972 	 0.00419 	 MISS
   83 	    26 	 0.14166 	 0.00422 	 MISS
   22 	    27 	 0.08082 	 0.00423 	 m..s
   59 	    28 	 0.13318 	 0.00423 	 MISS
   34 	    29 	 0.09840 	 0.00437 	 m..s
   59 	    30 	 0.13318 	 0.00438 	 MISS
   76 	    31 	 0.13824 	 0.00443 	 MISS
   13 	    32 	 0.07184 	 0.00443 	 m..s
    1 	    33 	 0.04065 	 0.00443 	 m..s
   17 	    34 	 0.07614 	 0.00445 	 m..s
   59 	    35 	 0.13318 	 0.00446 	 MISS
   15 	    36 	 0.07419 	 0.00464 	 m..s
   75 	    37 	 0.13766 	 0.00469 	 MISS
   24 	    38 	 0.08911 	 0.00478 	 m..s
   78 	    39 	 0.13888 	 0.00496 	 MISS
    6 	    40 	 0.04841 	 0.00502 	 m..s
    0 	    41 	 0.03915 	 0.00504 	 m..s
   59 	    42 	 0.13318 	 0.00521 	 MISS
   33 	    43 	 0.09796 	 0.00543 	 m..s
   26 	    44 	 0.09592 	 0.00578 	 m..s
   38 	    45 	 0.09976 	 0.00682 	 m..s
   51 	    46 	 0.11936 	 0.03246 	 m..s
   21 	    47 	 0.07984 	 0.03267 	 m..s
   14 	    48 	 0.07249 	 0.03909 	 m..s
   12 	    49 	 0.07183 	 0.04276 	 ~...
   23 	    50 	 0.08130 	 0.04382 	 m..s
   19 	    51 	 0.07886 	 0.05590 	 ~...
   20 	    52 	 0.07895 	 0.05615 	 ~...
   18 	    53 	 0.07850 	 0.05981 	 ~...
   16 	    54 	 0.07543 	 0.06300 	 ~...
   49 	    55 	 0.11824 	 0.07000 	 m..s
   11 	    56 	 0.07148 	 0.07395 	 ~...
   30 	    57 	 0.09773 	 0.10742 	 ~...
   37 	    58 	 0.09866 	 0.12053 	 ~...
   27 	    59 	 0.09680 	 0.12924 	 m..s
   42 	    60 	 0.10903 	 0.13473 	 ~...
   45 	    61 	 0.11169 	 0.13481 	 ~...
   39 	    62 	 0.10756 	 0.14437 	 m..s
   48 	    63 	 0.11615 	 0.14888 	 m..s
   43 	    64 	 0.10906 	 0.14994 	 m..s
   89 	    65 	 0.17067 	 0.15698 	 ~...
   88 	    66 	 0.16832 	 0.15803 	 ~...
   31 	    67 	 0.09776 	 0.15954 	 m..s
   32 	    68 	 0.09778 	 0.16140 	 m..s
   85 	    69 	 0.15428 	 0.16163 	 ~...
   36 	    70 	 0.09848 	 0.16450 	 m..s
   87 	    71 	 0.15544 	 0.16605 	 ~...
   44 	    72 	 0.10918 	 0.16628 	 m..s
   28 	    73 	 0.09694 	 0.16654 	 m..s
   90 	    74 	 0.17384 	 0.16862 	 ~...
   86 	    75 	 0.15441 	 0.17512 	 ~...
   47 	    76 	 0.11557 	 0.17763 	 m..s
   91 	    77 	 0.21361 	 0.18234 	 m..s
   92 	    78 	 0.21424 	 0.19366 	 ~...
   54 	    79 	 0.12922 	 0.20044 	 m..s
   81 	    80 	 0.14130 	 0.20045 	 m..s
   59 	    81 	 0.13318 	 0.20099 	 m..s
   58 	    82 	 0.13198 	 0.20127 	 m..s
   59 	    83 	 0.13318 	 0.20179 	 m..s
   82 	    84 	 0.14162 	 0.20257 	 m..s
   95 	    85 	 0.21837 	 0.20499 	 ~...
   93 	    86 	 0.21747 	 0.20718 	 ~...
  112 	    87 	 0.27709 	 0.20837 	 m..s
   59 	    88 	 0.13318 	 0.20947 	 m..s
   79 	    89 	 0.13938 	 0.21005 	 m..s
  108 	    90 	 0.26818 	 0.21035 	 m..s
   59 	    91 	 0.13318 	 0.21152 	 m..s
   94 	    92 	 0.21815 	 0.21152 	 ~...
   96 	    93 	 0.22375 	 0.21177 	 ~...
   56 	    94 	 0.13159 	 0.21270 	 m..s
  107 	    95 	 0.26744 	 0.21487 	 m..s
  102 	    96 	 0.25580 	 0.21730 	 m..s
   99 	    97 	 0.24424 	 0.21798 	 ~...
  105 	    98 	 0.26302 	 0.21933 	 m..s
  101 	    99 	 0.25474 	 0.22495 	 ~...
  110 	   100 	 0.27178 	 0.22627 	 m..s
   98 	   101 	 0.24290 	 0.22879 	 ~...
   71 	   102 	 0.13459 	 0.23172 	 m..s
   50 	   103 	 0.11934 	 0.23205 	 MISS
   59 	   104 	 0.13318 	 0.23218 	 m..s
  100 	   105 	 0.25004 	 0.23589 	 ~...
  109 	   106 	 0.26887 	 0.23636 	 m..s
   80 	   107 	 0.13942 	 0.23821 	 m..s
  104 	   108 	 0.26222 	 0.24317 	 ~...
  103 	   109 	 0.26191 	 0.24716 	 ~...
   97 	   110 	 0.23675 	 0.24858 	 ~...
  117 	   111 	 0.29553 	 0.25083 	 m..s
  111 	   112 	 0.27256 	 0.25179 	 ~...
  116 	   113 	 0.29231 	 0.25344 	 m..s
  113 	   114 	 0.28361 	 0.25578 	 ~...
  106 	   115 	 0.26599 	 0.27384 	 ~...
  120 	   116 	 0.31892 	 0.27612 	 m..s
  115 	   117 	 0.29154 	 0.28511 	 ~...
  114 	   118 	 0.29093 	 0.30443 	 ~...
  118 	   119 	 0.30856 	 0.31364 	 ~...
  119 	   120 	 0.31245 	 0.31754 	 ~...
==========================================
r_mrr = 0.7633678317070007
r2_mrr = 0.48799723386764526
spearmanr_mrr@5 = 0.989186704158783
spearmanr_mrr@10 = 0.9604138731956482
spearmanr_mrr@50 = 0.9003447890281677
spearmanr_mrr@100 = 0.8601371645927429
spearmanr_mrr@All = 0.8941505551338196
==========================================
test time: 0.497
Done Testing dataset CoDExSmall
total time taken: 211.02638959884644
training time taken: 201.498028755188
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 782163540610260
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [961, 1011, 423, 1214, 1134, 383, 731, 1088, 1033, 394, 405, 269, 288, 898, 284, 795, 57, 586, 1017, 715, 505, 957, 519, 188, 707, 363, 848, 1132, 655, 403, 1019, 964, 670, 354, 913, 679, 14, 1199, 246, 613, 75, 178, 609, 139, 53, 52, 420, 976, 607, 908, 63, 339, 28, 1168, 989, 148, 979, 1201, 1111, 1167, 1122, 765, 1094, 949, 545, 1062, 1014, 435, 897, 772, 820, 793, 769, 82, 464, 1182, 1200, 250, 201, 697, 636, 278, 554, 238, 174, 1181, 470, 382, 1048, 991, 198, 583, 921, 945, 969, 786, 350, 1086, 774, 604, 199, 813, 1056, 140, 404, 630, 1003, 313, 1096, 451, 736, 422, 168, 682, 1149, 625, 664, 1142, 584, 181, 928]
valid_ids (0): []
train_ids (1094): [778, 955, 428, 34, 706, 13, 341, 1150, 296, 657, 230, 860, 98, 388, 1117, 766, 1080, 113, 1020, 623, 321, 80, 754, 1040, 292, 656, 216, 557, 639, 177, 1126, 789, 650, 1022, 1084, 40, 1043, 783, 755, 833, 749, 1119, 771, 830, 709, 193, 906, 693, 540, 995, 610, 454, 853, 516, 775, 56, 746, 959, 315, 15, 640, 626, 575, 307, 819, 1058, 366, 926, 294, 910, 1068, 668, 1093, 1118, 1206, 184, 498, 131, 938, 161, 412, 525, 1202, 924, 834, 179, 305, 561, 94, 1187, 227, 1103, 143, 993, 396, 816, 149, 1038, 169, 548, 432, 476, 934, 1016, 565, 990, 433, 1175, 351, 543, 718, 716, 306, 888, 665, 605, 24, 727, 735, 681, 1082, 417, 68, 799, 20, 91, 779, 598, 880, 37, 580, 197, 239, 153, 1045, 488, 266, 323, 806, 458, 868, 684, 62, 588, 751, 532, 45, 962, 343, 745, 1140, 145, 67, 603, 915, 480, 978, 1095, 495, 539, 358, 517, 499, 787, 1049, 105, 47, 332, 1107, 276, 566, 494, 1197, 36, 1083, 801, 644, 634, 1037, 275, 429, 7, 667, 688, 1015, 64, 1211, 933, 975, 815, 373, 1108, 1114, 151, 570, 1030, 289, 8, 130, 245, 175, 431, 837, 265, 96, 791, 273, 418, 1063, 796, 846, 861, 182, 32, 556, 214, 948, 981, 881, 914, 1021, 4, 911, 828, 1079, 522, 643, 2, 160, 974, 364, 812, 48, 792, 564, 460, 635, 869, 1113, 1121, 591, 42, 274, 1059, 1002, 734, 61, 982, 695, 112, 425, 389, 615, 1141, 807, 1099, 106, 618, 845, 43, 1191, 484, 549, 259, 11, 980, 157, 407, 300, 89, 1067, 713, 426, 362, 1208, 9, 16, 1024, 1209, 672, 243, 1018, 21, 659, 1129, 507, 577, 205, 691, 673, 491, 46, 864, 10, 996, 967, 386, 1055, 1213, 58, 529, 568, 109, 817, 527, 559, 666, 1092, 99, 100, 633, 882, 759, 78, 1130, 704, 855, 165, 747, 919, 1210, 832, 41, 1013, 600, 946, 1073, 760, 825, 541, 1075, 122, 395, 23, 553, 249, 999, 1098, 1177, 863, 497, 866, 329, 1158, 838, 937, 973, 1179, 271, 279, 1065, 1131, 803, 741, 459, 73, 349, 1186, 352, 381, 1036, 430, 805, 263, 689, 638, 87, 1171, 652, 1029, 518, 208, 1138, 1166, 572, 137, 827, 537, 456, 224, 101, 524, 753, 714, 449, 84, 374, 965, 1164, 761, 858, 242, 879, 69, 687, 282, 737, 573, 1178, 852, 79, 406, 324, 642, 723, 1051, 653, 763, 378, 947, 475, 55, 984, 22, 490, 253, 277, 876, 1154, 597, 357, 302, 1101, 71, 171, 38, 121, 1185, 331, 361, 896, 1105, 1143, 654, 1136, 402, 411, 611, 337, 146, 486, 399, 647, 255, 267, 283, 936, 297, 1120, 878, 442, 445, 930, 211, 994, 322, 1147, 700, 508, 138, 225, 685, 206, 166, 1115, 5, 602, 1035, 671, 83, 338, 1112, 397, 25, 1031, 463, 546, 385, 1026, 851, 943, 85, 295, 1077, 788, 972, 1100, 50, 489, 621, 971, 773, 680, 1046, 849, 1133, 103, 708, 986, 455, 675, 348, 102, 187, 1041, 528, 599, 628, 1137, 86, 1123, 590, 400, 1050, 207, 1076, 234, 334, 54, 530, 578, 1102, 195, 30, 1006, 220, 501, 1146, 236, 790, 892, 493, 416, 823, 927, 387, 506, 172, 200, 309, 514, 1189, 123, 762, 562, 438, 932, 756, 164, 465, 900, 658, 439, 1023, 750, 93, 134, 632, 854, 421, 126, 478, 369, 1170, 764, 916, 931, 585, 108, 370, 677, 631, 894, 612, 446, 574, 1087, 344, 468, 739, 88, 1061, 136, 159, 152, 19, 509, 39, 1160, 441, 885, 325, 60, 147, 1151, 533, 1124, 992, 721, 162, 1116, 717, 390, 440, 641, 857, 1004, 257, 340, 701, 163, 90, 368, 535, 232, 595, 998, 794, 606, 479, 264, 76, 678, 258, 954, 436, 1057, 719, 191, 204, 808, 326, 744, 142, 1139, 784, 150, 115, 1027, 167, 536, 218, 301, 840, 346, 119, 821, 608, 738, 569, 434, 987, 550, 1091, 733, 170, 596, 49, 720, 1, 1128, 120, 1001, 155, 593, 558, 376, 538, 291, 1097, 31, 699, 237, 829, 614, 192, 674, 124, 582, 877, 1145, 551, 342, 314, 758, 29, 345, 408, 127, 547, 141, 1039, 213, 690, 781, 710, 809, 893, 144, 229, 1012, 33, 116, 917, 703, 627, 272, 859, 1205, 299, 59, 251, 355, 335, 1053, 477, 1127, 645, 1165, 1009, 377, 929, 114, 902, 310, 826, 26, 409, 871, 901, 183, 462, 443, 353, 581, 712, 474, 552, 622, 380, 414, 316, 576, 180, 542, 132, 104, 483, 1204, 240, 1194, 466, 843, 190, 447, 1081, 1078, 1109, 392, 1176, 970, 173, 359, 696, 44, 886, 905, 317, 1106, 874, 1089, 887, 95, 1044, 722, 333, 1025, 1148, 209, 1192, 17, 935, 1000, 960, 336, 944, 918, 70, 1007, 320, 770, 850, 616, 752, 776, 1005, 940, 836, 285, 814, 555, 117, 873, 268, 683, 521, 624, 379, 473, 822, 244, 217, 247, 1196, 308, 966, 0, 711, 968, 281, 1161, 619, 223, 798, 1157, 202, 452, 1174, 444, 215, 904, 424, 110, 780, 222, 286, 523, 365, 154, 1054, 496, 977, 469, 1028, 895, 694, 676, 492, 398, 811, 637, 129, 221, 865, 298, 920, 958, 629, 92, 419, 330, 1090, 1184, 18, 125, 740, 729, 824, 1085, 233, 728, 939, 504, 1042, 1069, 328, 768, 360, 1070, 1010, 617, 601, 802, 724, 1104, 472, 1032, 1212, 810, 1066, 72, 620, 692, 81, 1159, 111, 1125, 1110, 903, 841, 1173, 950, 27, 482, 312, 196, 587, 800, 923, 1153, 280, 189, 467, 579, 1047, 203, 226, 212, 1072, 77, 371, 186, 648, 883, 804, 726, 231, 662, 1064, 3, 870, 1155, 646, 1188, 907, 74, 66, 391, 453, 797, 875, 862, 515, 97, 256, 592, 777, 1183, 303, 649, 457, 210, 651, 367, 318, 393, 767, 988, 1071, 1163, 372, 481, 867, 589, 663, 531, 1207, 1172, 831, 942, 261, 241, 65, 35, 1074, 884, 563, 705, 951, 1060, 953, 660, 725, 952, 133, 912, 156, 889, 135, 6, 842, 571, 560, 375, 327, 856, 1008, 1203, 304, 311, 260, 839, 118, 743, 963, 1169, 997, 427, 520, 899, 262, 384, 526, 248, 270, 835, 235, 544, 983, 922, 485, 128, 1052, 12, 410, 941, 1162, 185, 500, 785, 502, 293, 252, 319, 219, 461, 290, 1144, 513, 1198, 487, 471, 511, 534, 698, 702, 107, 450, 872, 512, 158, 1180, 415, 1190, 818, 661, 730, 1152, 228, 356, 401, 1156, 347, 891, 985, 890, 757, 567, 254, 956, 686, 194, 503, 413, 448, 287, 844, 742, 925, 782, 1135, 51, 748, 1195, 1034, 1193, 669, 510, 847, 594, 732, 437, 176, 909]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5560990120470893
the save name prefix for this run is:  chkpt-ID_5560990120470893_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 573
rank avg (pred): 0.608 +- 0.004
mrr vals (pred, true): 0.000, 0.138
batch losses (mrrl, rdl): 0.0, 0.0013916023

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 826
rank avg (pred): 0.293 +- 0.050
mrr vals (pred, true): 0.000, 0.198
batch losses (mrrl, rdl): 0.0, 0.0003651845

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1043
rank avg (pred): 0.376 +- 0.275
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001149933

Epoch over!
epoch time: 13.368

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 424
rank avg (pred): 0.416 +- 0.330
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0, 8.7438e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 18
rank avg (pred): 0.172 +- 0.273
mrr vals (pred, true): 0.175, 0.332
batch losses (mrrl, rdl): 0.0, 2.53138e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1092
rank avg (pred): 0.408 +- 0.326
mrr vals (pred, true): 0.032, 0.167
batch losses (mrrl, rdl): 0.0, 0.0003151834

Epoch over!
epoch time: 14.264

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.389 +- 0.316
mrr vals (pred, true): 0.011, 0.000
batch losses (mrrl, rdl): 0.0, 8.63994e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.403 +- 0.334
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001139412

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 965
rank avg (pred): 0.567 +- 0.360
mrr vals (pred, true): 0.028, 0.000
batch losses (mrrl, rdl): 0.0, 7.7306e-05

Epoch over!
epoch time: 13.498

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 917
rank avg (pred): 0.702 +- 0.383
mrr vals (pred, true): 0.009, 0.016
batch losses (mrrl, rdl): 0.0, 2.11957e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 177
rank avg (pred): 0.399 +- 0.316
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001177438

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1150
rank avg (pred): 0.307 +- 0.309
mrr vals (pred, true): 0.063, 0.220
batch losses (mrrl, rdl): 0.0, 8.6064e-06

Epoch over!
epoch time: 13.571

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 177
rank avg (pred): 0.399 +- 0.321
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001198662

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 331
rank avg (pred): 0.392 +- 0.319
mrr vals (pred, true): 0.054, 0.175
batch losses (mrrl, rdl): 0.0, 0.0001418146

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 321
rank avg (pred): 0.131 +- 0.258
mrr vals (pred, true): 0.223, 0.231
batch losses (mrrl, rdl): 0.0, 9.949e-06

Epoch over!
epoch time: 13.445

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 846
rank avg (pred): 0.441 +- 0.335
mrr vals (pred, true): 0.084, 0.020
batch losses (mrrl, rdl): 0.0117132552, 9.77084e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 552
rank avg (pred): 0.273 +- 0.345
mrr vals (pred, true): 0.152, 0.178
batch losses (mrrl, rdl): 0.0066440674, 8.3173e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1002
rank avg (pred): 0.304 +- 0.314
mrr vals (pred, true): 0.112, 0.165
batch losses (mrrl, rdl): 0.0279637761, 1.50944e-05

Epoch over!
epoch time: 12.816

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.435 +- 0.344
mrr vals (pred, true): 0.085, 0.154
batch losses (mrrl, rdl): 0.0480216071, 0.0002493265

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1101
rank avg (pred): 0.429 +- 0.281
mrr vals (pred, true): 0.081, 0.135
batch losses (mrrl, rdl): 0.0289548393, 0.0005102818

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1018
rank avg (pred): 0.371 +- 0.266
mrr vals (pred, true): 0.106, 0.160
batch losses (mrrl, rdl): 0.0300063286, 7.18099e-05

Epoch over!
epoch time: 12.774

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 869
rank avg (pred): 0.717 +- 0.286
mrr vals (pred, true): 0.030, 0.000
batch losses (mrrl, rdl): 0.0038402625, 0.0006887983

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 180
rank avg (pred): 0.383 +- 0.266
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0331956558, 8.34768e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 620
rank avg (pred): 0.395 +- 0.274
mrr vals (pred, true): 0.097, 0.117
batch losses (mrrl, rdl): 0.0040656487, 5.6265e-05

Epoch over!
epoch time: 12.508

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1190
rank avg (pred): 0.375 +- 0.253
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0428482816, 0.0002744084

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1059
rank avg (pred): 0.062 +- 0.147
mrr vals (pred, true): 0.385, 0.386
batch losses (mrrl, rdl): 3.38e-07, 1.94505e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 686
rank avg (pred): 0.422 +- 0.285
mrr vals (pred, true): 0.093, 0.000
batch losses (mrrl, rdl): 0.0186012238, 4.8248e-05

Epoch over!
epoch time: 12.665

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.411 +- 0.234
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0220026635, 3.58805e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 120
rank avg (pred): 0.351 +- 0.240
mrr vals (pred, true): 0.147, 0.163
batch losses (mrrl, rdl): 0.0024315023, 7.13577e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.572 +- 0.255
mrr vals (pred, true): 0.052, 0.019
batch losses (mrrl, rdl): 4.4124e-05, 0.0005512057

Epoch over!
epoch time: 12.727

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 101
rank avg (pred): 0.373 +- 0.235
mrr vals (pred, true): 0.108, 0.176
batch losses (mrrl, rdl): 0.0467523597, 0.0001361763

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 267
rank avg (pred): 0.090 +- 0.166
mrr vals (pred, true): 0.368, 0.353
batch losses (mrrl, rdl): 0.0021823393, 1.1388e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 656
rank avg (pred): 0.407 +- 0.226
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0209814012, 0.0001172098

Epoch over!
epoch time: 12.743

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 570
rank avg (pred): 0.402 +- 0.223
mrr vals (pred, true): 0.091, 0.116
batch losses (mrrl, rdl): 0.006444179, 0.0002009059

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.318 +- 0.250
mrr vals (pred, true): 0.199, 0.176
batch losses (mrrl, rdl): 0.0051362561, 0.0001045215

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 968
rank avg (pred): 0.591 +- 0.274
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 6.8887e-06, 0.0001814077

Epoch over!
epoch time: 12.767

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 557
rank avg (pred): 0.273 +- 0.229
mrr vals (pred, true): 0.195, 0.194
batch losses (mrrl, rdl): 1.59926e-05, 6.12517e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 129
rank avg (pred): 0.378 +- 0.216
mrr vals (pred, true): 0.086, 0.169
batch losses (mrrl, rdl): 0.0682772845, 0.0001506299

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1169
rank avg (pred): 0.467 +- 0.254
mrr vals (pred, true): 0.095, 0.140
batch losses (mrrl, rdl): 0.0209232774, 0.0003760547

Epoch over!
epoch time: 12.781

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 608
rank avg (pred): 0.398 +- 0.213
mrr vals (pred, true): 0.107, 0.155
batch losses (mrrl, rdl): 0.023589341, 0.0001537251

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 900
rank avg (pred): 0.529 +- 0.265
mrr vals (pred, true): 0.079, 0.028
batch losses (mrrl, rdl): 0.0081519028, 6.53289e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.399 +- 0.204
mrr vals (pred, true): 0.092, 0.125
batch losses (mrrl, rdl): 0.0109680602, 0.0001051905

Epoch over!
epoch time: 13.092

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 103
rank avg (pred): 0.364 +- 0.209
mrr vals (pred, true): 0.104, 0.178
batch losses (mrrl, rdl): 0.0551155955, 0.0001272609

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.393 +- 0.212
mrr vals (pred, true): 0.101, 0.142
batch losses (mrrl, rdl): 0.0170099437, 0.0002156568

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 626
rank avg (pred): 0.386 +- 0.203
mrr vals (pred, true): 0.096, 0.141
batch losses (mrrl, rdl): 0.0208308436, 0.0001297265

Epoch over!
epoch time: 13.904

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.583 +- 0.275
mrr vals (pred, true): 0.049, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.04919 	 5e-0500 	 m..s
    6 	     1 	 0.04901 	 0.00011 	 m..s
    2 	     2 	 0.04884 	 0.00011 	 m..s
    9 	     3 	 0.04933 	 0.00013 	 m..s
    8 	     4 	 0.04920 	 0.00014 	 m..s
   38 	     5 	 0.10176 	 0.00014 	 MISS
   79 	     6 	 0.10648 	 0.00016 	 MISS
   92 	     7 	 0.11811 	 0.00016 	 MISS
   68 	     8 	 0.10482 	 0.00017 	 MISS
   63 	     9 	 0.10336 	 0.00017 	 MISS
   78 	    10 	 0.10630 	 0.00017 	 MISS
    1 	    11 	 0.04884 	 0.00018 	 m..s
    4 	    12 	 0.04890 	 0.00019 	 m..s
   80 	    13 	 0.10681 	 0.00020 	 MISS
   33 	    14 	 0.10066 	 0.00021 	 MISS
   42 	    15 	 0.10202 	 0.00021 	 MISS
   67 	    16 	 0.10478 	 0.00022 	 MISS
   20 	    17 	 0.08514 	 0.00022 	 m..s
   42 	    18 	 0.10202 	 0.00022 	 MISS
   82 	    19 	 0.10713 	 0.00022 	 MISS
   76 	    20 	 0.10624 	 0.00022 	 MISS
   12 	    21 	 0.04989 	 0.00023 	 m..s
   88 	    22 	 0.11249 	 0.00025 	 MISS
   28 	    23 	 0.09975 	 0.00025 	 m..s
    3 	    24 	 0.04885 	 0.00026 	 m..s
   71 	    25 	 0.10518 	 0.00026 	 MISS
   41 	    26 	 0.10200 	 0.00027 	 MISS
   66 	    27 	 0.10416 	 0.00028 	 MISS
   31 	    28 	 0.10021 	 0.00029 	 m..s
   42 	    29 	 0.10202 	 0.00030 	 MISS
   25 	    30 	 0.09837 	 0.00030 	 m..s
   27 	    31 	 0.09907 	 0.00031 	 m..s
   37 	    32 	 0.10164 	 0.00035 	 MISS
   81 	    33 	 0.10702 	 0.00038 	 MISS
   29 	    34 	 0.09980 	 0.00041 	 m..s
   14 	    35 	 0.05035 	 0.00042 	 m..s
   42 	    36 	 0.10202 	 0.00043 	 MISS
   11 	    37 	 0.04946 	 0.00044 	 m..s
   62 	    38 	 0.10332 	 0.00046 	 MISS
   42 	    39 	 0.10202 	 0.00053 	 MISS
    5 	    40 	 0.04893 	 0.00054 	 m..s
   13 	    41 	 0.05024 	 0.00068 	 m..s
   35 	    42 	 0.10122 	 0.00069 	 MISS
   42 	    43 	 0.10202 	 0.00071 	 MISS
   73 	    44 	 0.10572 	 0.00076 	 MISS
   89 	    45 	 0.11265 	 0.00112 	 MISS
   65 	    46 	 0.10412 	 0.00122 	 MISS
    0 	    47 	 0.04884 	 0.00150 	 m..s
   42 	    48 	 0.10202 	 0.00206 	 m..s
   17 	    49 	 0.05865 	 0.00336 	 m..s
   10 	    50 	 0.04938 	 0.00375 	 m..s
   19 	    51 	 0.07343 	 0.00861 	 m..s
   16 	    52 	 0.05115 	 0.01370 	 m..s
   18 	    53 	 0.06102 	 0.01885 	 m..s
   15 	    54 	 0.05085 	 0.01934 	 m..s
   24 	    55 	 0.09500 	 0.03525 	 m..s
   21 	    56 	 0.08516 	 0.05263 	 m..s
   42 	    57 	 0.10202 	 0.11249 	 ~...
   42 	    58 	 0.10202 	 0.11647 	 ~...
   42 	    59 	 0.10202 	 0.12393 	 ~...
   42 	    60 	 0.10202 	 0.12420 	 ~...
   72 	    61 	 0.10541 	 0.12699 	 ~...
   42 	    62 	 0.10202 	 0.12961 	 ~...
   74 	    63 	 0.10584 	 0.12973 	 ~...
   39 	    64 	 0.10188 	 0.13086 	 ~...
   69 	    65 	 0.10492 	 0.13197 	 ~...
   42 	    66 	 0.10202 	 0.13256 	 m..s
   32 	    67 	 0.10031 	 0.13434 	 m..s
   42 	    68 	 0.10202 	 0.13457 	 m..s
   22 	    69 	 0.09320 	 0.13509 	 m..s
   77 	    70 	 0.10625 	 0.13690 	 m..s
   23 	    71 	 0.09356 	 0.13800 	 m..s
   93 	    72 	 0.16888 	 0.14150 	 ~...
   42 	    73 	 0.10202 	 0.14170 	 m..s
   61 	    74 	 0.10246 	 0.14193 	 m..s
   75 	    75 	 0.10620 	 0.14212 	 m..s
   94 	    76 	 0.18445 	 0.14433 	 m..s
   42 	    77 	 0.10202 	 0.14604 	 m..s
   30 	    78 	 0.09982 	 0.14665 	 m..s
   42 	    79 	 0.10202 	 0.15226 	 m..s
   26 	    80 	 0.09847 	 0.15551 	 m..s
   34 	    81 	 0.10117 	 0.15555 	 m..s
   36 	    82 	 0.10137 	 0.15758 	 m..s
   97 	    83 	 0.21816 	 0.15838 	 m..s
   91 	    84 	 0.11619 	 0.15883 	 m..s
   40 	    85 	 0.10193 	 0.16143 	 m..s
   87 	    86 	 0.11121 	 0.16398 	 m..s
   90 	    87 	 0.11472 	 0.16444 	 m..s
   95 	    88 	 0.19430 	 0.16472 	 ~...
   70 	    89 	 0.10516 	 0.16703 	 m..s
   83 	    90 	 0.10730 	 0.16741 	 m..s
   60 	    91 	 0.10228 	 0.16757 	 m..s
   59 	    92 	 0.10217 	 0.16927 	 m..s
   86 	    93 	 0.10808 	 0.17491 	 m..s
   85 	    94 	 0.10754 	 0.17646 	 m..s
   64 	    95 	 0.10396 	 0.17787 	 m..s
   84 	    96 	 0.10747 	 0.18083 	 m..s
   96 	    97 	 0.19772 	 0.19467 	 ~...
   99 	    98 	 0.22423 	 0.21250 	 ~...
   98 	    99 	 0.22280 	 0.21750 	 ~...
  102 	   100 	 0.25750 	 0.22477 	 m..s
  100 	   101 	 0.23951 	 0.22776 	 ~...
  103 	   102 	 0.26087 	 0.23125 	 ~...
  101 	   103 	 0.24744 	 0.24172 	 ~...
  109 	   104 	 0.28251 	 0.24670 	 m..s
  110 	   105 	 0.29091 	 0.25835 	 m..s
  104 	   106 	 0.26187 	 0.26095 	 ~...
  106 	   107 	 0.27264 	 0.26422 	 ~...
  107 	   108 	 0.27458 	 0.26510 	 ~...
  105 	   109 	 0.26665 	 0.28166 	 ~...
  112 	   110 	 0.30397 	 0.28735 	 ~...
  113 	   111 	 0.31773 	 0.28879 	 ~...
  108 	   112 	 0.27570 	 0.28984 	 ~...
  111 	   113 	 0.29602 	 0.30876 	 ~...
  114 	   114 	 0.32960 	 0.31608 	 ~...
  116 	   115 	 0.33957 	 0.32253 	 ~...
  118 	   116 	 0.34549 	 0.34745 	 ~...
  117 	   117 	 0.34423 	 0.36096 	 ~...
  115 	   118 	 0.33450 	 0.36372 	 ~...
  120 	   119 	 0.35604 	 0.36583 	 ~...
  119 	   120 	 0.34627 	 0.37521 	 ~...
==========================================
r_mrr = 0.8525800108909607
r2_mrr = 0.6523358225822449
spearmanr_mrr@5 = 0.9395239353179932
spearmanr_mrr@10 = 0.9574659466743469
spearmanr_mrr@50 = 0.9656853675842285
spearmanr_mrr@100 = 0.8537645936012268
spearmanr_mrr@All = 0.8813353180885315
==========================================
test time: 0.413
Done Testing dataset DBpedia50
total time taken: 202.3162877559662
training time taken: 197.40825510025024
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 4865937335602121
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1055, 1161, 0, 268, 55, 615, 1147, 285, 127, 1090, 418, 22, 313, 650, 480, 701, 484, 441, 751, 174, 910, 329, 711, 217, 662, 82, 1195, 1023, 1006, 457, 599, 558, 597, 353, 1018, 808, 271, 159, 476, 1100, 979, 557, 863, 538, 1191, 459, 641, 181, 422, 88, 300, 167, 496, 410, 115, 594, 371, 477, 1002, 565, 664, 344, 519, 391, 355, 109, 897, 70, 393, 1099, 499, 833, 603, 1039, 66, 1182, 535, 868, 908, 1091, 569, 302, 735, 988, 673, 246, 261, 307, 1128, 705, 258, 166, 502, 1185, 706, 514, 913, 992, 560, 1122, 610, 479, 63, 928, 243, 583, 1125, 952, 1050, 368, 450, 1213, 57, 256, 481, 561, 663, 257, 354, 1146, 428]
valid_ids (0): []
train_ids (1094): [80, 1035, 96, 634, 85, 782, 624, 43, 1202, 58, 991, 417, 239, 1065, 99, 28, 842, 339, 1133, 629, 3, 759, 554, 687, 406, 674, 32, 856, 654, 986, 971, 377, 1109, 154, 728, 815, 215, 1017, 400, 171, 108, 501, 273, 244, 320, 145, 446, 780, 505, 537, 464, 289, 297, 439, 581, 675, 164, 125, 282, 420, 52, 325, 1079, 158, 409, 324, 331, 838, 390, 413, 970, 186, 528, 612, 685, 704, 852, 295, 846, 608, 926, 357, 267, 894, 1108, 813, 298, 596, 632, 427, 882, 646, 720, 322, 493, 890, 1, 767, 648, 434, 198, 670, 948, 475, 415, 817, 911, 684, 1036, 254, 1180, 200, 68, 693, 540, 497, 916, 488, 382, 136, 1139, 772, 671, 487, 380, 350, 1003, 627, 474, 59, 930, 1208, 630, 750, 823, 379, 990, 375, 722, 702, 1145, 791, 1151, 444, 1193, 749, 605, 492, 53, 721, 1165, 549, 114, 276, 442, 881, 809, 618, 311, 511, 64, 983, 1038, 1061, 414, 907, 33, 194, 843, 1166, 253, 423, 788, 521, 977, 151, 31, 958, 678, 438, 577, 29, 1105, 279, 1027, 572, 367, 349, 814, 1007, 452, 697, 770, 607, 252, 707, 359, 1093, 968, 310, 1075, 250, 864, 216, 448, 5, 466, 1067, 1062, 976, 133, 898, 242, 424, 939, 544, 529, 604, 730, 1110, 163, 914, 639, 614, 854, 384, 912, 161, 1088, 628, 700, 798, 471, 736, 828, 709, 978, 202, 71, 472, 765, 555, 328, 1121, 389, 361, 517, 113, 110, 531, 48, 1092, 433, 606, 436, 146, 1207, 1011, 224, 197, 314, 695, 1179, 431, 399, 1203, 1005, 786, 762, 613, 92, 682, 394, 89, 713, 338, 90, 1137, 525, 965, 666, 345, 625, 232, 533, 172, 228, 46, 1021, 262, 1031, 130, 981, 7, 333, 909, 102, 223, 959, 1184, 771, 588, 1154, 879, 160, 369, 378, 611, 238, 951, 327, 1004, 1096, 12, 787, 547, 1160, 1149, 392, 1214, 723, 1025, 539, 1186, 315, 779, 822, 1008, 1115, 1032, 845, 857, 542, 974, 373, 653, 1033, 935, 587, 1010, 901, 1177, 1117, 938, 699, 348, 1116, 647, 893, 155, 1168, 1014, 601, 347, 1152, 411, 849, 11, 342, 169, 1150, 1054, 218, 995, 245, 803, 530, 138, 876, 177, 75, 884, 1140, 1045, 123, 463, 508, 1173, 1157, 1012, 826, 582, 103, 1049, 1174, 426, 987, 874, 1169, 638, 340, 287, 211, 567, 676, 408, 906, 1101, 1175, 84, 1148, 954, 1042, 686, 147, 1124, 343, 1172, 545, 150, 24, 17, 1070, 1041, 326, 967, 729, 21, 1136, 960, 78, 1197, 226, 1053, 972, 124, 1131, 510, 39, 1094, 998, 792, 235, 1156, 407, 1126, 563, 1026, 396, 225, 626, 805, 67, 458, 689, 1028, 144, 858, 934, 1142, 73, 131, 398, 830, 72, 1013, 896, 1051, 550, 937, 278, 506, 139, 187, 461, 299, 86, 421, 260, 1196, 366, 334, 173, 552, 395, 887, 34, 251, 30, 973, 1058, 1081, 571, 850, 284, 559, 761, 292, 275, 281, 1107, 575, 230, 1123, 790, 162, 1127, 157, 305, 668, 41, 240, 220, 236, 241, 500, 209, 925, 248, 207, 490, 467, 799, 963, 853, 383, 140, 49, 920, 1144, 1106, 744, 265, 755, 45, 1016, 919, 62, 902, 860, 997, 796, 401, 129, 742, 593, 91, 435, 941, 1155, 1095, 516, 358, 824, 376, 756, 494, 93, 316, 453, 1052, 523, 1143, 365, 25, 219, 1046, 969, 1057, 306, 449, 797, 694, 922, 107, 522, 135, 1076, 97, 768, 203, 753, 231, 121, 915, 622, 763, 748, 949, 984, 374, 69, 1135, 760, 841, 955, 179, 861, 489, 994, 943, 795, 385, 222, 623, 870, 586, 579, 512, 769, 35, 156, 341, 2, 364, 54, 717, 644, 176, 964, 255, 1212, 335, 180, 1060, 1112, 65, 775, 1114, 1043, 1086, 1190, 178, 580, 867, 405, 498, 134, 50, 1059, 773, 524, 945, 989, 195, 620, 76, 381, 658, 81, 142, 503, 1069, 286, 532, 570, 272, 309, 42, 738, 1118, 437, 456, 362, 18, 921, 486, 642, 1178, 621, 892, 757, 740, 865, 903, 714, 175, 266, 589, 568, 553, 1200, 1083, 304, 683, 840, 1015, 551, 1097, 478, 877, 672, 754, 468, 889, 708, 848, 534, 19, 527, 221, 346, 541, 895, 318, 206, 677, 758, 564, 932, 982, 386, 504, 105, 1111, 847, 283, 962, 412, 462, 372, 834, 189, 880, 578, 183, 126, 337, 652, 835, 1119, 227, 878, 56, 637, 746, 141, 1211, 270, 118, 387, 691, 120, 485, 844, 811, 592, 669, 851, 430, 839, 77, 956, 293, 1198, 831, 660, 402, 111, 429, 388, 1064, 659, 696, 731, 469, 900, 233, 716, 1044, 509, 192, 301, 872, 330, 360, 741, 518, 112, 184, 100, 36, 122, 931, 1089, 715, 781, 148, 303, 432, 288, 832, 1192, 739, 15, 566, 598, 319, 927, 836, 264, 980, 404, 536, 947, 1134, 13, 263, 869, 454, 196, 645, 655, 871, 837, 886, 212, 665, 4, 810, 1205, 933, 595, 576, 923, 515, 859, 445, 204, 710, 143, 1080, 363, 40, 294, 27, 680, 513, 416, 205, 208, 1162, 918, 117, 210, 229, 866, 1153, 74, 996, 60, 460, 829, 600, 1138, 734, 789, 774, 875, 153, 26, 1085, 336, 1048, 946, 280, 1072, 419, 1063, 816, 801, 95, 317, 1171, 584, 785, 985, 312, 1030, 793, 507, 104, 820, 899, 616, 905, 827, 885, 1037, 891, 1199, 649, 1129, 784, 1001, 259, 688, 602, 732, 247, 690, 548, 352, 1077, 465, 573, 1034, 745, 888, 291, 1098, 1141, 116, 679, 719, 1047, 1159, 802, 188, 783, 1084, 633, 296, 546, 79, 1024, 590, 447, 966, 617, 37, 483, 764, 942, 777, 993, 961, 778, 370, 149, 8, 323, 1040, 1000, 1074, 440, 718, 562, 574, 1188, 1164, 403, 940, 269, 185, 98, 168, 692, 1029, 950, 609, 1181, 543, 1102, 128, 1056, 152, 165, 234, 51, 862, 491, 87, 308, 635, 1163, 455, 743, 667, 94, 1130, 924, 20, 356, 277, 1078, 1113, 1132, 1183, 636, 1167, 591, 193, 1009, 1073, 651, 332, 213, 199, 556, 495, 137, 482, 643, 725, 526, 191, 657, 703, 61, 1066, 656, 201, 776, 470, 1120, 1189, 681, 6, 443, 47, 812, 727, 1206, 904, 807, 917, 1201, 747, 16, 726, 1104, 724, 1176, 1158, 975, 936, 1071, 1022, 818, 619, 1170, 14, 9, 929, 873, 821, 794, 883, 473, 1187, 451, 38, 855, 190, 766, 640, 1020, 804, 274, 712, 1019, 132, 397, 1209, 999, 10, 119, 520, 1210, 800, 752, 290, 170, 83, 351, 698, 631, 737, 214, 1194, 237, 953, 425, 23, 106, 944, 182, 1103, 585, 1087, 733, 321, 806, 1204, 825, 957, 44, 819, 1082, 661, 249, 1068, 101]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3234694198400357
the save name prefix for this run is:  chkpt-ID_3234694198400357_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 864
rank avg (pred): 0.550 +- 0.007
mrr vals (pred, true): 0.017, 0.048
batch losses (mrrl, rdl): 0.0, 0.0001875994

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1070
rank avg (pred): 0.035 +- 0.031
mrr vals (pred, true): 0.443, 0.458
batch losses (mrrl, rdl): 0.0, 6.7737e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 443
rank avg (pred): 0.248 +- 0.214
mrr vals (pred, true): 0.286, 0.054
batch losses (mrrl, rdl): 0.0, 0.0008079847

Epoch over!
epoch time: 16.69

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 340
rank avg (pred): 0.246 +- 0.225
mrr vals (pred, true): 0.321, 0.245
batch losses (mrrl, rdl): 0.0, 0.0006327727

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 140
rank avg (pred): 0.264 +- 0.238
mrr vals (pred, true): 0.310, 0.174
batch losses (mrrl, rdl): 0.0, 0.0001996974

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 789
rank avg (pred): 0.341 +- 0.301
mrr vals (pred, true): 0.333, 0.054
batch losses (mrrl, rdl): 0.0, 0.0001690318

Epoch over!
epoch time: 14.372

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 894
rank avg (pred): 0.125 +- 0.124
mrr vals (pred, true): 0.391, 0.209
batch losses (mrrl, rdl): 0.0, 2.33497e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 107
rank avg (pred): 0.267 +- 0.246
mrr vals (pred, true): 0.331, 0.235
batch losses (mrrl, rdl): 0.0, 0.0007261099

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1123
rank avg (pred): 0.276 +- 0.240
mrr vals (pred, true): 0.296, 0.055
batch losses (mrrl, rdl): 0.0, 0.0005056523

Epoch over!
epoch time: 13.62

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1209
rank avg (pred): 0.248 +- 0.242
mrr vals (pred, true): 0.364, 0.052
batch losses (mrrl, rdl): 0.0, 0.0007487551

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 170
rank avg (pred): 0.255 +- 0.237
mrr vals (pred, true): 0.327, 0.049
batch losses (mrrl, rdl): 0.0, 0.0007059751

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 644
rank avg (pred): 0.270 +- 0.246
mrr vals (pred, true): 0.329, 0.276
batch losses (mrrl, rdl): 0.0, 0.0008154189

Epoch over!
epoch time: 13.882

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 995
rank avg (pred): 0.028 +- 0.030
mrr vals (pred, true): 0.571, 0.441
batch losses (mrrl, rdl): 0.0, 1.64751e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 468
rank avg (pred): 0.286 +- 0.248
mrr vals (pred, true): 0.300, 0.051
batch losses (mrrl, rdl): 0.0, 0.0005248179

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 48
rank avg (pred): 0.040 +- 0.040
mrr vals (pred, true): 0.509, 0.429
batch losses (mrrl, rdl): 0.0, 7.4642e-06

Epoch over!
epoch time: 13.275

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 530
rank avg (pred): 0.048 +- 0.052
mrr vals (pred, true): 0.512, 0.386
batch losses (mrrl, rdl): 0.1587918997, 3.2769e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 998
rank avg (pred): 0.045 +- 0.034
mrr vals (pred, true): 0.384, 0.459
batch losses (mrrl, rdl): 0.0571299419, 2.4026e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1092
rank avg (pred): 0.437 +- 0.221
mrr vals (pred, true): 0.133, 0.246
batch losses (mrrl, rdl): 0.1269808561, 0.0022914922

Epoch over!
epoch time: 14.661

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 394
rank avg (pred): 0.410 +- 0.225
mrr vals (pred, true): 0.159, 0.255
batch losses (mrrl, rdl): 0.0926916301, 0.0023087615

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 118
rank avg (pred): 0.436 +- 0.207
mrr vals (pred, true): 0.128, 0.267
batch losses (mrrl, rdl): 0.1935717613, 0.0025987043

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 676
rank avg (pred): 0.407 +- 0.201
mrr vals (pred, true): 0.138, 0.051
batch losses (mrrl, rdl): 0.0782881007, 4.30594e-05

Epoch over!
epoch time: 15.018

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 803
rank avg (pred): 0.580 +- 0.158
mrr vals (pred, true): 0.044, 0.050
batch losses (mrrl, rdl): 0.0003256755, 0.0002530997

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 333
rank avg (pred): 0.335 +- 0.206
mrr vals (pred, true): 0.191, 0.265
batch losses (mrrl, rdl): 0.0556601137, 0.0014239033

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 926
rank avg (pred): 0.533 +- 0.100
mrr vals (pred, true): 0.028, 0.050
batch losses (mrrl, rdl): 0.0046813004, 9.5176e-05

Epoch over!
epoch time: 14.609

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 855
rank avg (pred): 0.427 +- 0.121
mrr vals (pred, true): 0.057, 0.059
batch losses (mrrl, rdl): 0.0005513548, 5.15938e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 510
rank avg (pred): 0.073 +- 0.055
mrr vals (pred, true): 0.338, 0.395
batch losses (mrrl, rdl): 0.0321200006, 4.2386e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 996
rank avg (pred): 0.026 +- 0.021
mrr vals (pred, true): 0.483, 0.451
batch losses (mrrl, rdl): 0.0101155676, 1.63352e-05

Epoch over!
epoch time: 14.429

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 288
rank avg (pred): 0.031 +- 0.026
mrr vals (pred, true): 0.469, 0.440
batch losses (mrrl, rdl): 0.0082948869, 1.59625e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 469
rank avg (pred): 0.356 +- 0.160
mrr vals (pred, true): 0.140, 0.054
batch losses (mrrl, rdl): 0.0815355331, 0.0002268136

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 681
rank avg (pred): 0.372 +- 0.158
mrr vals (pred, true): 0.133, 0.051
batch losses (mrrl, rdl): 0.0683975294, 0.0001689678

Epoch over!
epoch time: 13.649

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 463
rank avg (pred): 0.362 +- 0.162
mrr vals (pred, true): 0.138, 0.056
batch losses (mrrl, rdl): 0.0769070387, 0.000209794

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1052
rank avg (pred): 0.370 +- 0.161
mrr vals (pred, true): 0.136, 0.055
batch losses (mrrl, rdl): 0.0732703432, 0.000169949

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 896
rank avg (pred): 0.316 +- 0.190
mrr vals (pred, true): 0.193, 0.021
batch losses (mrrl, rdl): 0.2043700218, 0.000354337

Epoch over!
epoch time: 15.732

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1011
rank avg (pred): 0.338 +- 0.139
mrr vals (pred, true): 0.134, 0.262
batch losses (mrrl, rdl): 0.1646352708, 0.0013002977

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 922
rank avg (pred): 0.395 +- 0.084
mrr vals (pred, true): 0.043, 0.050
batch losses (mrrl, rdl): 0.0004540049, 0.0003001894

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 11
rank avg (pred): 0.054 +- 0.043
mrr vals (pred, true): 0.424, 0.429
batch losses (mrrl, rdl): 0.0001867781, 5.915e-07

Epoch over!
epoch time: 15.449

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 689
rank avg (pred): 0.353 +- 0.153
mrr vals (pred, true): 0.142, 0.056
batch losses (mrrl, rdl): 0.0838618129, 0.0002840308

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 874
rank avg (pred): 0.392 +- 0.111
mrr vals (pred, true): 0.067, 0.056
batch losses (mrrl, rdl): 0.0029246309, 0.0001978325

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1150
rank avg (pred): 0.091 +- 0.075
mrr vals (pred, true): 0.414, 0.392
batch losses (mrrl, rdl): 0.0046214582, 2.18179e-05

Epoch over!
epoch time: 14.232

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 316
rank avg (pred): 0.058 +- 0.047
mrr vals (pred, true): 0.428, 0.446
batch losses (mrrl, rdl): 0.0033947248, 5.777e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 84
rank avg (pred): 0.341 +- 0.152
mrr vals (pred, true): 0.149, 0.219
batch losses (mrrl, rdl): 0.0493155457, 0.0010238441

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 517
rank avg (pred): 0.182 +- 0.148
mrr vals (pred, true): 0.370, 0.375
batch losses (mrrl, rdl): 0.0002086509, 0.0003482676

Epoch over!
epoch time: 14.523

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 429
rank avg (pred): 0.261 +- 0.112
mrr vals (pred, true): 0.149, 0.049
batch losses (mrrl, rdl): 0.0988329276, 0.0009431993

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1209
rank avg (pred): 0.340 +- 0.144
mrr vals (pred, true): 0.146, 0.052
batch losses (mrrl, rdl): 0.0927959308, 0.0003696074

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 374
rank avg (pred): 0.303 +- 0.165
mrr vals (pred, true): 0.194, 0.262
batch losses (mrrl, rdl): 0.0461380854, 0.0009951

Epoch over!
epoch time: 14.806

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.038 +- 0.032
mrr vals (pred, true): 0.477, 0.441

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.12936 	 0.02078 	 MISS
   15 	     1 	 0.14081 	 0.02078 	 MISS
   71 	     2 	 0.18370 	 0.03915 	 MISS
   75 	     3 	 0.19847 	 0.04733 	 MISS
    2 	     4 	 0.05119 	 0.04779 	 ~...
   35 	     5 	 0.14701 	 0.04888 	 m..s
   24 	     6 	 0.14348 	 0.04952 	 m..s
   73 	     7 	 0.18541 	 0.04963 	 MISS
    4 	     8 	 0.06596 	 0.05112 	 ~...
   54 	     9 	 0.15799 	 0.05138 	 MISS
   48 	    10 	 0.15486 	 0.05142 	 MISS
    6 	    11 	 0.13439 	 0.05186 	 m..s
   59 	    12 	 0.16020 	 0.05276 	 MISS
   37 	    13 	 0.14804 	 0.05288 	 m..s
   67 	    14 	 0.17568 	 0.05325 	 MISS
    0 	    15 	 0.03307 	 0.05331 	 ~...
   78 	    16 	 0.21131 	 0.05336 	 MISS
   41 	    17 	 0.15226 	 0.05337 	 m..s
   38 	    18 	 0.14883 	 0.05364 	 m..s
   10 	    19 	 0.13766 	 0.05365 	 m..s
   11 	    20 	 0.13847 	 0.05394 	 m..s
   63 	    21 	 0.16544 	 0.05463 	 MISS
   16 	    22 	 0.14124 	 0.05474 	 m..s
   72 	    23 	 0.18472 	 0.05483 	 MISS
   34 	    24 	 0.14697 	 0.05501 	 m..s
   25 	    25 	 0.14420 	 0.05519 	 m..s
   17 	    26 	 0.14131 	 0.05557 	 m..s
   66 	    27 	 0.17010 	 0.05569 	 MISS
   58 	    28 	 0.15879 	 0.05570 	 MISS
   23 	    29 	 0.14283 	 0.05625 	 m..s
   68 	    30 	 0.17896 	 0.05692 	 MISS
   44 	    31 	 0.15363 	 0.05706 	 m..s
   45 	    32 	 0.15374 	 0.05707 	 m..s
   20 	    33 	 0.14225 	 0.05715 	 m..s
    3 	    34 	 0.05638 	 0.05722 	 ~...
    7 	    35 	 0.13635 	 0.05743 	 m..s
   77 	    36 	 0.21086 	 0.05828 	 MISS
   46 	    37 	 0.15387 	 0.05838 	 m..s
   76 	    38 	 0.20480 	 0.05845 	 MISS
   13 	    39 	 0.13937 	 0.05955 	 m..s
   74 	    40 	 0.19423 	 0.05963 	 MISS
    1 	    41 	 0.04547 	 0.06028 	 ~...
   19 	    42 	 0.14222 	 0.06029 	 m..s
   28 	    43 	 0.14489 	 0.06074 	 m..s
   29 	    44 	 0.14504 	 0.20531 	 m..s
   18 	    45 	 0.14165 	 0.21624 	 m..s
   50 	    46 	 0.15581 	 0.21979 	 m..s
   62 	    47 	 0.16411 	 0.22306 	 m..s
   27 	    48 	 0.14464 	 0.22362 	 m..s
   52 	    49 	 0.15609 	 0.22636 	 m..s
   21 	    50 	 0.14231 	 0.22808 	 m..s
   51 	    51 	 0.15597 	 0.22982 	 m..s
   60 	    52 	 0.16162 	 0.23128 	 m..s
   40 	    53 	 0.15163 	 0.23134 	 m..s
   57 	    54 	 0.15872 	 0.23155 	 m..s
   14 	    55 	 0.13983 	 0.23156 	 m..s
   32 	    56 	 0.14621 	 0.23238 	 m..s
    8 	    57 	 0.13659 	 0.23322 	 m..s
   47 	    58 	 0.15427 	 0.23467 	 m..s
   49 	    59 	 0.15552 	 0.23503 	 m..s
   43 	    60 	 0.15325 	 0.23732 	 m..s
   70 	    61 	 0.18281 	 0.23787 	 m..s
   42 	    62 	 0.15262 	 0.24060 	 m..s
   33 	    63 	 0.14623 	 0.24224 	 m..s
   30 	    64 	 0.14545 	 0.24241 	 m..s
   61 	    65 	 0.16400 	 0.24434 	 m..s
   12 	    66 	 0.13860 	 0.24588 	 MISS
   53 	    67 	 0.15728 	 0.24617 	 m..s
   26 	    68 	 0.14436 	 0.24952 	 MISS
   22 	    69 	 0.14244 	 0.24969 	 MISS
   64 	    70 	 0.16855 	 0.25163 	 m..s
   69 	    71 	 0.18063 	 0.25319 	 m..s
   55 	    72 	 0.15813 	 0.25355 	 m..s
   56 	    73 	 0.15863 	 0.25377 	 m..s
   65 	    74 	 0.16925 	 0.25562 	 m..s
   31 	    75 	 0.14561 	 0.25634 	 MISS
    9 	    76 	 0.13710 	 0.26188 	 MISS
   36 	    77 	 0.14777 	 0.26367 	 MISS
   39 	    78 	 0.14942 	 0.28497 	 MISS
   79 	    79 	 0.32050 	 0.33622 	 ~...
   80 	    80 	 0.35153 	 0.35276 	 ~...
   88 	    81 	 0.38946 	 0.35294 	 m..s
   91 	    82 	 0.39367 	 0.35679 	 m..s
   83 	    83 	 0.38722 	 0.36606 	 ~...
   85 	    84 	 0.38734 	 0.36692 	 ~...
   92 	    85 	 0.39523 	 0.37045 	 ~...
   86 	    86 	 0.38928 	 0.37263 	 ~...
   93 	    87 	 0.39533 	 0.37356 	 ~...
   84 	    88 	 0.38734 	 0.37780 	 ~...
   81 	    89 	 0.38300 	 0.38333 	 ~...
   95 	    90 	 0.41319 	 0.38369 	 ~...
   90 	    91 	 0.39293 	 0.38613 	 ~...
   89 	    92 	 0.39001 	 0.39308 	 ~...
   94 	    93 	 0.39779 	 0.39875 	 ~...
   82 	    94 	 0.38685 	 0.40130 	 ~...
   87 	    95 	 0.38933 	 0.41169 	 ~...
  114 	    96 	 0.46689 	 0.41396 	 m..s
  107 	    97 	 0.45314 	 0.42847 	 ~...
  105 	    98 	 0.44976 	 0.42881 	 ~...
  118 	    99 	 0.47356 	 0.43046 	 m..s
  109 	   100 	 0.45908 	 0.43176 	 ~...
  100 	   101 	 0.44232 	 0.43199 	 ~...
   97 	   102 	 0.43298 	 0.43431 	 ~...
  101 	   103 	 0.44259 	 0.43603 	 ~...
   96 	   104 	 0.42695 	 0.43650 	 ~...
  116 	   105 	 0.46904 	 0.43746 	 m..s
  115 	   106 	 0.46840 	 0.43760 	 m..s
   98 	   107 	 0.43820 	 0.43783 	 ~...
  106 	   108 	 0.44999 	 0.43902 	 ~...
  104 	   109 	 0.44922 	 0.44003 	 ~...
  108 	   110 	 0.45663 	 0.44077 	 ~...
  120 	   111 	 0.47674 	 0.44110 	 m..s
  112 	   112 	 0.46512 	 0.44342 	 ~...
  117 	   113 	 0.47337 	 0.44401 	 ~...
   99 	   114 	 0.44060 	 0.44612 	 ~...
  102 	   115 	 0.44265 	 0.44803 	 ~...
  103 	   116 	 0.44870 	 0.44804 	 ~...
  111 	   117 	 0.46362 	 0.44904 	 ~...
  110 	   118 	 0.46099 	 0.45085 	 ~...
  119 	   119 	 0.47364 	 0.45472 	 ~...
  113 	   120 	 0.46516 	 0.47721 	 ~...
==========================================
r_mrr = 0.8625735640525818
r2_mrr = 0.7359094619750977
spearmanr_mrr@5 = 0.7841172218322754
spearmanr_mrr@10 = 0.8117420077323914
spearmanr_mrr@50 = 0.9857375621795654
spearmanr_mrr@100 = 0.9047856330871582
spearmanr_mrr@All = 0.9154921770095825
==========================================
test time: 0.62
Done Testing dataset Kinships
total time taken: 227.45882153511047
training time taken: 219.64266228675842
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 7725030105057986
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [301, 116, 545, 655, 507, 808, 279, 620, 934, 531, 306, 774, 679, 510, 275, 336, 891, 85, 1150, 501, 63, 742, 1038, 413, 985, 441, 1044, 21, 383, 331, 417, 361, 1065, 257, 1119, 897, 863, 644, 15, 781, 1204, 427, 1132, 928, 231, 22, 740, 1072, 238, 1166, 31, 323, 834, 726, 298, 590, 734, 838, 699, 70, 552, 573, 93, 661, 146, 340, 927, 233, 641, 457, 779, 804, 20, 80, 468, 1196, 990, 518, 259, 855, 443, 579, 872, 302, 307, 540, 470, 90, 1016, 71, 596, 769, 1013, 1086, 992, 577, 1155, 1170, 627, 585, 938, 1, 646, 47, 462, 162, 278, 519, 756, 780, 473, 1001, 1141, 542, 304, 2, 913, 593, 1080, 961, 1022]
valid_ids (0): []
train_ids (1094): [55, 1151, 1164, 114, 725, 1110, 23, 430, 980, 280, 270, 757, 1039, 5, 475, 1193, 294, 1210, 1199, 467, 956, 801, 944, 782, 669, 532, 74, 1056, 724, 404, 1098, 563, 662, 766, 929, 344, 589, 460, 486, 712, 647, 185, 155, 1051, 17, 795, 166, 535, 628, 262, 654, 168, 131, 741, 14, 823, 1172, 702, 154, 525, 978, 659, 521, 458, 35, 260, 1178, 365, 342, 435, 497, 949, 343, 377, 536, 952, 1078, 369, 1027, 140, 332, 1055, 480, 715, 1106, 748, 642, 1198, 713, 558, 771, 72, 201, 737, 1123, 300, 657, 1111, 937, 1108, 833, 49, 348, 445, 656, 173, 447, 223, 451, 328, 407, 1003, 33, 264, 122, 719, 1160, 1149, 842, 547, 236, 248, 1100, 1168, 682, 494, 1214, 723, 574, 706, 971, 514, 106, 743, 676, 1006, 172, 1125, 539, 50, 947, 1101, 61, 925, 123, 1159, 568, 580, 455, 720, 45, 1138, 548, 556, 819, 469, 733, 716, 1209, 511, 115, 412, 605, 625, 767, 885, 1020, 1174, 1034, 516, 993, 564, 1052, 835, 75, 89, 1126, 1112, 1103, 479, 354, 253, 826, 637, 895, 129, 700, 554, 148, 82, 705, 1181, 493, 128, 1068, 1029, 915, 936, 1036, 1095, 143, 919, 698, 452, 324, 425, 630, 889, 973, 920, 738, 488, 345, 350, 164, 822, 42, 884, 764, 603, 388, 793, 492, 672, 239, 1122, 753, 1114, 214, 965, 312, 871, 401, 687, 308, 87, 193, 418, 776, 629, 138, 689, 101, 176, 600, 466, 640, 831, 946, 349, 330, 530, 960, 1010, 792, 69, 954, 1124, 703, 1066, 261, 760, 235, 569, 91, 775, 572, 132, 1088, 81, 180, 648, 1075, 597, 945, 761, 496, 645, 426, 1041, 843, 839, 228, 18, 245, 395, 456, 1045, 896, 989, 1043, 942, 582, 650, 1211, 249, 1037, 1182, 744, 181, 205, 732, 446, 624, 751, 272, 777, 914, 890, 635, 282, 633, 1208, 9, 99, 52, 88, 608, 851, 459, 288, 192, 387, 133, 1069, 537, 364, 359, 865, 51, 1186, 297, 1202, 873, 828, 182, 1053, 7, 799, 951, 158, 394, 184, 610, 1049, 905, 113, 218, 1116, 136, 888, 832, 190, 338, 968, 156, 159, 841, 255, 611, 879, 752, 199, 322, 675, 534, 24, 296, 111, 487, 878, 500, 363, 440, 506, 1063, 187, 119, 745, 276, 40, 754, 824, 68, 416, 287, 329, 232, 202, 333, 442, 549, 759, 571, 1131, 104, 43, 478, 722, 1105, 1017, 830, 1189, 439, 1008, 805, 415, 314, 1213, 988, 763, 846, 341, 327, 912, 1176, 875, 1084, 319, 709, 1067, 1060, 1071, 1096, 73, 996, 810, 584, 224, 557, 243, 286, 449, 1000, 592, 357, 171, 433, 562, 247, 1152, 409, 677, 267, 30, 38, 615, 870, 292, 790, 718, 538, 124, 178, 815, 921, 299, 567, 303, 482, 1021, 188, 693, 665, 1093, 750, 371, 317, 546, 150, 170, 561, 1135, 550, 398, 269, 1033, 707, 880, 935, 27, 126, 410, 1062, 768, 576, 903, 34, 599, 714, 421, 898, 522, 32, 673, 681, 972, 337, 382, 1073, 786, 432, 555, 708, 477, 678, 392, 553, 614, 1154, 200, 339, 1127, 785, 213, 840, 66, 623, 334, 1167, 103, 1087, 56, 802, 1134, 4, 1032, 220, 854, 244, 1142, 861, 6, 773, 1083, 995, 836, 606, 1023, 520, 64, 789, 523, 619, 1057, 1156, 121, 450, 651, 79, 994, 1185, 1194, 77, 1205, 977, 591, 1070, 618, 882, 403, 818, 1201, 527, 739, 108, 356, 899, 551, 174, 194, 736, 886, 1207, 290, 483, 429, 1148, 1143, 526, 755, 622, 1146, 346, 969, 1091, 1175, 1158, 222, 953, 1012, 820, 437, 316, 1042, 436, 320, 62, 251, 428, 959, 1104, 175, 1019, 313, 894, 691, 643, 83, 283, 1030, 604, 144, 422, 153, 680, 503, 1117, 731, 911, 1007, 25, 293, 97, 1128, 1092, 36, 1177, 533, 237, 474, 86, 570, 701, 987, 142, 632, 67, 495, 234, 844, 671, 966, 12, 216, 370, 970, 240, 660, 710, 916, 1015, 794, 639, 504, 940, 29, 607, 226, 609, 408, 102, 274, 950, 1058, 1097, 1048, 207, 285, 225, 1031, 717, 860, 509, 814, 221, 845, 380, 310, 908, 747, 1165, 362, 1139, 943, 37, 1035, 529, 1183, 137, 683, 490, 800, 857, 812, 867, 909, 163, 694, 204, 258, 697, 784, 246, 157, 385, 1157, 1147, 1018, 1107, 1144, 979, 621, 612, 256, 1094, 758, 997, 378, 864, 360, 1187, 1133, 183, 877, 721, 165, 930, 636, 829, 368, 263, 729, 1140, 1191, 100, 809, 684, 791, 229, 352, 1180, 438, 668, 1171, 955, 664, 1005, 57, 983, 400, 465, 197, 321, 666, 981, 453, 986, 1009, 1089, 134, 727, 964, 711, 230, 289, 852, 481, 1195, 749, 8, 1184, 318, 464, 1120, 783, 1059, 375, 167, 746, 1203, 941, 271, 856, 900, 817, 3, 1173, 76, 849, 141, 1077, 663, 414, 347, 59, 517, 353, 991, 1099, 16, 1047, 396, 811, 351, 859, 39, 196, 858, 893, 638, 728, 98, 94, 0, 125, 384, 420, 367, 444, 179, 847, 389, 219, 528, 393, 513, 565, 411, 907, 118, 982, 1024, 19, 373, 594, 1061, 797, 762, 1206, 559, 881, 1074, 78, 212, 962, 1102, 117, 160, 848, 177, 787, 967, 11, 53, 151, 386, 44, 311, 887, 406, 215, 381, 120, 397, 512, 827, 1130, 670, 326, 1064, 471, 1082, 203, 999, 1054, 379, 975, 883, 335, 1136, 273, 152, 112, 423, 926, 601, 26, 617, 575, 498, 765, 1081, 1161, 46, 515, 974, 667, 1090, 1169, 472, 1200, 135, 948, 505, 92, 1026, 252, 431, 217, 524, 130, 268, 1014, 1145, 209, 544, 807, 1115, 798, 54, 543, 295, 1076, 206, 391, 692, 602, 837, 1050, 109, 1011, 189, 541, 901, 254, 695, 476, 616, 566, 191, 922, 374, 910, 1153, 735, 850, 902, 658, 1197, 147, 690, 1046, 918, 1025, 1113, 1190, 1109, 778, 399, 502, 281, 816, 1163, 366, 241, 208, 957, 613, 402, 485, 499, 1085, 923, 508, 862, 95, 931, 1137, 376, 932, 419, 315, 250, 491, 372, 355, 998, 41, 578, 1188, 704, 560, 463, 1129, 906, 586, 796, 358, 461, 581, 853, 211, 96, 60, 917, 48, 107, 813, 868, 291, 127, 730, 325, 821, 933, 424, 65, 28, 110, 186, 806, 688, 169, 1004, 984, 454, 772, 788, 242, 1121, 198, 145, 595, 284, 869, 1162, 652, 305, 434, 924, 265, 1028, 583, 976, 277, 649, 390, 825, 1192, 803, 13, 195, 405, 770, 588, 161, 139, 266, 105, 587, 1002, 876, 674, 598, 1040, 1179, 1118, 149, 686, 634, 1212, 58, 84, 484, 210, 963, 874, 653, 10, 448, 892, 489, 631, 685, 866, 904, 1079, 696, 958, 227, 626, 309, 939]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8226097263966443
the save name prefix for this run is:  chkpt-ID_8226097263966443_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1127
rank avg (pred): 0.468 +- 0.004
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001228961

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 310
rank avg (pred): 0.137 +- 0.058
mrr vals (pred, true): 0.057, 0.221
batch losses (mrrl, rdl): 0.0, 5.38843e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 852
rank avg (pred): 0.440 +- 0.248
mrr vals (pred, true): 0.141, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003689808

Epoch over!
epoch time: 15.041

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 980
rank avg (pred): 0.072 +- 0.044
mrr vals (pred, true): 0.174, 0.318
batch losses (mrrl, rdl): 0.0, 2.94882e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 268
rank avg (pred): 0.065 +- 0.047
mrr vals (pred, true): 0.233, 0.306
batch losses (mrrl, rdl): 0.0, 1.26213e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 384
rank avg (pred): 0.405 +- 0.300
mrr vals (pred, true): 0.199, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001226274

Epoch over!
epoch time: 14.107

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1169
rank avg (pred): 0.442 +- 0.294
mrr vals (pred, true): 0.177, 0.074
batch losses (mrrl, rdl): 0.0, 0.0002801928

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1195
rank avg (pred): 0.421 +- 0.305
mrr vals (pred, true): 0.190, 0.001
batch losses (mrrl, rdl): 0.0, 3.89874e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 800
rank avg (pred): 0.481 +- 0.300
mrr vals (pred, true): 0.141, 0.001
batch losses (mrrl, rdl): 0.0, 1.21107e-05

Epoch over!
epoch time: 13.97

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 313
rank avg (pred): 0.101 +- 0.090
mrr vals (pred, true): 0.193, 0.214
batch losses (mrrl, rdl): 0.0, 2.51647e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1156
rank avg (pred): 0.242 +- 0.226
mrr vals (pred, true): 0.196, 0.219
batch losses (mrrl, rdl): 0.0, 6.65203e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1067
rank avg (pred): 0.101 +- 0.094
mrr vals (pred, true): 0.149, 0.240
batch losses (mrrl, rdl): 0.0, 2.33869e-05

Epoch over!
epoch time: 15.568

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 993
rank avg (pred): 0.096 +- 0.094
mrr vals (pred, true): 0.182, 0.224
batch losses (mrrl, rdl): 0.0, 4.67375e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 4
rank avg (pred): 0.085 +- 0.084
mrr vals (pred, true): 0.192, 0.275
batch losses (mrrl, rdl): 0.0, 1.72368e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 16
rank avg (pred): 0.041 +- 0.041
mrr vals (pred, true): 0.203, 0.297
batch losses (mrrl, rdl): 0.0, 3.03555e-05

Epoch over!
epoch time: 14.142

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 145
rank avg (pred): 0.397 +- 0.319
mrr vals (pred, true): 0.121, 0.060
batch losses (mrrl, rdl): 0.0500180982, 0.0001217196

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 747
rank avg (pred): 0.201 +- 0.163
mrr vals (pred, true): 0.129, 0.116
batch losses (mrrl, rdl): 0.0017278788, 0.0001693593

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 648
rank avg (pred): 0.595 +- 0.226
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008059248, 0.0002590118

Epoch over!
epoch time: 15.426

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1202
rank avg (pred): 0.565 +- 0.245
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005677796, 0.0001478399

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 772
rank avg (pred): 0.568 +- 0.224
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002054809, 7.12144e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 494
rank avg (pred): 0.013 +- 0.011
mrr vals (pred, true): 0.229, 0.257
batch losses (mrrl, rdl): 0.0079100151, 0.0008257332

Epoch over!
epoch time: 15.282

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 68
rank avg (pred): 0.029 +- 0.025
mrr vals (pred, true): 0.210, 0.227
batch losses (mrrl, rdl): 0.0028960337, 0.000162638

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 242
rank avg (pred): 0.505 +- 0.234
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005932982, 3.66417e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.508 +- 0.217
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002082026, 4.30467e-05

Epoch over!
epoch time: 14.843

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1110
rank avg (pred): 0.489 +- 0.227
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001046844, 2.47444e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1069
rank avg (pred): 0.026 +- 0.022
mrr vals (pred, true): 0.256, 0.263
batch losses (mrrl, rdl): 0.0004532085, 0.0001853359

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 879
rank avg (pred): 0.468 +- 0.219
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.35844e-05, 2.96454e-05

Epoch over!
epoch time: 13.865

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 811
rank avg (pred): 0.426 +- 0.284
mrr vals (pred, true): 0.092, 0.055
batch losses (mrrl, rdl): 0.01782443, 0.0022163047

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 677
rank avg (pred): 0.473 +- 0.200
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.79704e-05, 3.67254e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 685
rank avg (pred): 0.456 +- 0.204
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.06719e-05, 4.84141e-05

Epoch over!
epoch time: 13.783

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1191
rank avg (pred): 0.463 +- 0.198
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 4.4941e-06, 4.23829e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 893
rank avg (pred): 0.413 +- 0.230
mrr vals (pred, true): 0.067, 0.003
batch losses (mrrl, rdl): 0.0027412823, 5.95962e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 955
rank avg (pred): 0.440 +- 0.202
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001258458, 7.06951e-05

Epoch over!
epoch time: 13.406

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 289
rank avg (pred): 0.074 +- 0.059
mrr vals (pred, true): 0.240, 0.201
batch losses (mrrl, rdl): 0.0150004718, 5.31064e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 352
rank avg (pred): 0.462 +- 0.182
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 0.0001047225, 0.0003608433

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 755
rank avg (pred): 0.220 +- 0.150
mrr vals (pred, true): 0.149, 0.147
batch losses (mrrl, rdl): 6.86688e-05, 0.000120118

Epoch over!
epoch time: 14.006

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 829
rank avg (pred): 0.176 +- 0.124
mrr vals (pred, true): 0.161, 0.132
batch losses (mrrl, rdl): 0.0085824747, 0.0001049372

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 482
rank avg (pred): 0.460 +- 0.176
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 8.13731e-05, 5.88889e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 839
rank avg (pred): 0.475 +- 0.152
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0004089322, 7.05945e-05

Epoch over!
epoch time: 13.706

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 701
rank avg (pred): 0.452 +- 0.180
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.50986e-05, 7.59321e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 123
rank avg (pred): 0.449 +- 0.180
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 3.6e-09, 0.0002316261

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 486
rank avg (pred): 0.464 +- 0.330
mrr vals (pred, true): 0.231, 0.250
batch losses (mrrl, rdl): 0.0034966394, 0.0016185755

Epoch over!
epoch time: 13.404

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 799
rank avg (pred): 0.447 +- 0.177
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.55438e-05, 8.63505e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 850
rank avg (pred): 0.421 +- 0.193
mrr vals (pred, true): 0.060, 0.003
batch losses (mrrl, rdl): 0.0010476073, 0.0001621313

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 587
rank avg (pred): 0.415 +- 0.191
mrr vals (pred, true): 0.056, 0.066
batch losses (mrrl, rdl): 0.0003672591, 0.0001868174

Epoch over!
epoch time: 14.566

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.106 +- 0.074
mrr vals (pred, true): 0.178, 0.141

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   30 	     0 	 0.05116 	 8e-0500 	 m..s
   62 	     1 	 0.05570 	 0.00010 	 m..s
   63 	     2 	 0.05572 	 0.00023 	 m..s
   60 	     3 	 0.05557 	 0.00026 	 m..s
   66 	     4 	 0.05681 	 0.00041 	 m..s
   67 	     5 	 0.05682 	 0.00047 	 m..s
   34 	     6 	 0.05157 	 0.00049 	 m..s
   40 	     7 	 0.05257 	 0.00049 	 m..s
   64 	     8 	 0.05574 	 0.00050 	 m..s
   39 	     9 	 0.05243 	 0.00052 	 m..s
   25 	    10 	 0.05086 	 0.00053 	 m..s
    5 	    11 	 0.04885 	 0.00056 	 m..s
   28 	    12 	 0.05102 	 0.00056 	 m..s
   33 	    13 	 0.05134 	 0.00057 	 m..s
   50 	    14 	 0.05359 	 0.00057 	 m..s
   56 	    15 	 0.05473 	 0.00059 	 m..s
   48 	    16 	 0.05349 	 0.00060 	 m..s
   13 	    17 	 0.05020 	 0.00063 	 m..s
   29 	    18 	 0.05108 	 0.00064 	 m..s
   58 	    19 	 0.05503 	 0.00066 	 m..s
   71 	    20 	 0.05747 	 0.00068 	 m..s
   16 	    21 	 0.05034 	 0.00068 	 m..s
   22 	    22 	 0.05079 	 0.00068 	 m..s
   14 	    23 	 0.05025 	 0.00074 	 m..s
   17 	    24 	 0.05044 	 0.00078 	 m..s
   32 	    25 	 0.05130 	 0.00078 	 m..s
    0 	    26 	 0.04179 	 0.00079 	 m..s
    4 	    27 	 0.04884 	 0.00080 	 m..s
   20 	    28 	 0.05061 	 0.00080 	 m..s
   11 	    29 	 0.05013 	 0.00082 	 m..s
    7 	    30 	 0.04938 	 0.00084 	 m..s
   68 	    31 	 0.05714 	 0.00086 	 m..s
    3 	    32 	 0.04880 	 0.00087 	 m..s
   12 	    33 	 0.05017 	 0.00099 	 m..s
   19 	    34 	 0.05050 	 0.00107 	 m..s
   59 	    35 	 0.05522 	 0.00112 	 m..s
    2 	    36 	 0.04370 	 0.00211 	 m..s
   47 	    37 	 0.05337 	 0.00344 	 m..s
   44 	    38 	 0.05333 	 0.00462 	 m..s
   46 	    39 	 0.05336 	 0.00475 	 m..s
    1 	    40 	 0.04328 	 0.00510 	 m..s
   49 	    41 	 0.05359 	 0.00545 	 m..s
   69 	    42 	 0.05716 	 0.00655 	 m..s
   73 	    43 	 0.05791 	 0.00684 	 m..s
   74 	    44 	 0.06294 	 0.00878 	 m..s
   75 	    45 	 0.06531 	 0.01348 	 m..s
    8 	    46 	 0.04976 	 0.03871 	 ~...
   36 	    47 	 0.05174 	 0.04285 	 ~...
   24 	    48 	 0.05086 	 0.04843 	 ~...
   21 	    49 	 0.05064 	 0.05026 	 ~...
   55 	    50 	 0.05471 	 0.05231 	 ~...
   57 	    51 	 0.05501 	 0.05260 	 ~...
   35 	    52 	 0.05170 	 0.05544 	 ~...
    6 	    53 	 0.04906 	 0.05545 	 ~...
   43 	    54 	 0.05303 	 0.05861 	 ~...
   15 	    55 	 0.05026 	 0.06143 	 ~...
   53 	    56 	 0.05439 	 0.06187 	 ~...
   65 	    57 	 0.05584 	 0.06248 	 ~...
   31 	    58 	 0.05122 	 0.06291 	 ~...
    9 	    59 	 0.04977 	 0.06304 	 ~...
   23 	    60 	 0.05083 	 0.06329 	 ~...
   27 	    61 	 0.05101 	 0.06358 	 ~...
   70 	    62 	 0.05742 	 0.06472 	 ~...
   10 	    63 	 0.05000 	 0.06524 	 ~...
   26 	    64 	 0.05096 	 0.06609 	 ~...
   54 	    65 	 0.05450 	 0.06711 	 ~...
   18 	    66 	 0.05044 	 0.06748 	 ~...
   37 	    67 	 0.05188 	 0.06783 	 ~...
   61 	    68 	 0.05561 	 0.07270 	 ~...
   38 	    69 	 0.05197 	 0.07304 	 ~...
   51 	    70 	 0.05414 	 0.07453 	 ~...
   52 	    71 	 0.05419 	 0.07485 	 ~...
   72 	    72 	 0.05762 	 0.07508 	 ~...
   45 	    73 	 0.05335 	 0.07569 	 ~...
   42 	    74 	 0.05294 	 0.07587 	 ~...
   41 	    75 	 0.05288 	 0.07694 	 ~...
   76 	    76 	 0.13587 	 0.12640 	 ~...
   78 	    77 	 0.14135 	 0.13046 	 ~...
   79 	    78 	 0.14187 	 0.13631 	 ~...
   90 	    79 	 0.17801 	 0.14065 	 m..s
   81 	    80 	 0.16243 	 0.14379 	 ~...
   82 	    81 	 0.16273 	 0.14674 	 ~...
   83 	    82 	 0.16418 	 0.14677 	 ~...
   88 	    83 	 0.17716 	 0.14811 	 ~...
   92 	    84 	 0.18057 	 0.15093 	 ~...
   80 	    85 	 0.14841 	 0.15094 	 ~...
   85 	    86 	 0.17001 	 0.15219 	 ~...
   77 	    87 	 0.13735 	 0.15453 	 ~...
   96 	    88 	 0.19514 	 0.15826 	 m..s
   87 	    89 	 0.17629 	 0.16306 	 ~...
   97 	    90 	 0.19566 	 0.16345 	 m..s
   99 	    91 	 0.19817 	 0.17081 	 ~...
   95 	    92 	 0.18965 	 0.17630 	 ~...
   84 	    93 	 0.16937 	 0.18394 	 ~...
   98 	    94 	 0.19620 	 0.19528 	 ~...
   91 	    95 	 0.17843 	 0.20050 	 ~...
  101 	    96 	 0.19871 	 0.20293 	 ~...
  106 	    97 	 0.24094 	 0.20387 	 m..s
   86 	    98 	 0.17065 	 0.20773 	 m..s
   93 	    99 	 0.18136 	 0.21003 	 ~...
   89 	   100 	 0.17790 	 0.21400 	 m..s
  108 	   101 	 0.24660 	 0.21567 	 m..s
   94 	   102 	 0.18650 	 0.21861 	 m..s
  103 	   103 	 0.20935 	 0.22049 	 ~...
  102 	   104 	 0.20200 	 0.22064 	 ~...
  100 	   105 	 0.19843 	 0.22261 	 ~...
  107 	   106 	 0.24163 	 0.22936 	 ~...
  105 	   107 	 0.22682 	 0.22966 	 ~...
  104 	   108 	 0.21324 	 0.24043 	 ~...
  112 	   109 	 0.28337 	 0.26915 	 ~...
  113 	   110 	 0.29160 	 0.27365 	 ~...
  109 	   111 	 0.26558 	 0.27434 	 ~...
  111 	   112 	 0.28023 	 0.27569 	 ~...
  110 	   113 	 0.27958 	 0.27724 	 ~...
  115 	   114 	 0.29894 	 0.27889 	 ~...
  114 	   115 	 0.29500 	 0.29490 	 ~...
  116 	   116 	 0.30102 	 0.29696 	 ~...
  117 	   117 	 0.30793 	 0.29863 	 ~...
  118 	   118 	 0.32264 	 0.29939 	 ~...
  119 	   119 	 0.33049 	 0.30439 	 ~...
  120 	   120 	 0.34841 	 0.30759 	 m..s
==========================================
r_mrr = 0.9575162529945374
r2_mrr = 0.8734305500984192
spearmanr_mrr@5 = 0.9603341221809387
spearmanr_mrr@10 = 0.8901419043540955
spearmanr_mrr@50 = 0.9837040901184082
spearmanr_mrr@100 = 0.971381425857544
spearmanr_mrr@All = 0.9702463150024414
==========================================
test time: 0.473
Done Testing dataset OpenEA
total time taken: 232.2829246520996
training time taken: 215.65918827056885
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 4486418957022485
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [781, 336, 1017, 486, 670, 353, 512, 880, 26, 296, 237, 813, 615, 1012, 567, 6, 1152, 41, 613, 653, 1010, 1026, 110, 927, 471, 689, 423, 298, 162, 439, 33, 765, 735, 150, 87, 273, 1071, 342, 306, 1110, 349, 231, 258, 105, 165, 27, 483, 704, 28, 370, 576, 648, 164, 404, 772, 827, 234, 1135, 601, 973, 1167, 149, 843, 1041, 818, 35, 681, 588, 1128, 718, 773, 147, 1196, 1168, 130, 756, 794, 538, 351, 767, 463, 55, 989, 1137, 388, 760, 786, 1008, 1183, 882, 184, 350, 699, 762, 895, 1164, 253, 341, 856, 1073, 684, 1134, 53, 1015, 698, 94, 708, 933, 1214, 870, 865, 69, 955, 346, 1057, 326, 1087, 811, 1132, 330, 261]
valid_ids (0): []
train_ids (1094): [858, 700, 1031, 707, 995, 706, 731, 355, 380, 469, 1066, 948, 751, 280, 91, 36, 182, 50, 76, 254, 673, 745, 730, 826, 966, 978, 289, 920, 195, 339, 1170, 92, 444, 1011, 119, 474, 1004, 901, 816, 1206, 787, 20, 1051, 835, 657, 557, 450, 1160, 558, 8, 726, 935, 961, 1146, 153, 801, 930, 623, 1081, 80, 1093, 982, 410, 1121, 859, 1068, 1104, 470, 593, 1097, 1101, 2, 9, 1193, 127, 467, 722, 112, 603, 912, 664, 286, 74, 671, 1161, 759, 1033, 386, 154, 403, 609, 190, 903, 1151, 125, 238, 1067, 727, 16, 1117, 608, 445, 392, 947, 1118, 1094, 161, 625, 1116, 244, 1016, 540, 270, 179, 146, 646, 391, 1169, 305, 301, 831, 38, 495, 304, 285, 923, 54, 579, 577, 299, 916, 95, 658, 318, 728, 227, 878, 365, 72, 778, 398, 1034, 1083, 1032, 1047, 115, 93, 451, 476, 57, 313, 390, 884, 1019, 14, 929, 1158, 1058, 846, 531, 986, 460, 1143, 458, 205, 833, 1150, 457, 120, 368, 260, 1029, 847, 823, 627, 564, 712, 491, 784, 523, 957, 186, 377, 893, 854, 62, 268, 48, 743, 331, 1042, 675, 1106, 275, 357, 199, 369, 128, 871, 586, 639, 534, 307, 433, 178, 183, 171, 1014, 716, 951, 672, 240, 714, 894, 347, 628, 266, 739, 156, 630, 425, 886, 66, 96, 409, 375, 1197, 1124, 315, 605, 747, 251, 552, 524, 60, 659, 908, 405, 416, 785, 1075, 1200, 780, 1095, 553, 1102, 49, 389, 802, 513, 544, 562, 983, 909, 230, 505, 1122, 725, 1157, 761, 245, 382, 817, 1105, 1192, 427, 1063, 809, 497, 1202, 311, 876, 533, 116, 99, 297, 1030, 434, 1089, 796, 1074, 293, 309, 791, 898, 148, 566, 1144, 1207, 848, 507, 600, 447, 272, 89, 1209, 666, 1055, 111, 1006, 257, 776, 192, 461, 1018, 489, 738, 994, 1025, 656, 713, 680, 132, 749, 873, 508, 494, 742, 555, 1085, 674, 842, 526, 988, 1082, 103, 345, 175, 967, 1153, 78, 7, 492, 201, 650, 549, 1145, 734, 34, 705, 755, 1024, 965, 741, 757, 4, 678, 883, 70, 866, 372, 560, 71, 233, 314, 117, 750, 599, 852, 1129, 188, 431, 0, 753, 770, 550, 922, 950, 721, 44, 595, 11, 321, 928, 194, 424, 931, 596, 411, 343, 1187, 1127, 122, 1096, 335, 1072, 906, 374, 527, 913, 956, 276, 793, 782, 732, 442, 294, 1115, 1022, 572, 998, 107, 621, 774, 582, 277, 319, 963, 585, 75, 815, 529, 992, 643, 212, 701, 807, 64, 200, 252, 652, 869, 626, 1023, 758, 530, 979, 1046, 185, 402, 547, 729, 918, 970, 812, 10, 262, 581, 971, 209, 1159, 422, 810, 1125, 239, 669, 337, 56, 449, 1060, 975, 287, 717, 429, 647, 622, 768, 448, 216, 136, 518, 393, 406, 151, 720, 1009, 946, 159, 620, 528, 24, 584, 651, 248, 949, 267, 302, 515, 84, 500, 464, 934, 366, 889, 269, 378, 493, 1007, 954, 490, 17, 855, 207, 271, 316, 358, 303, 635, 911, 1103, 1002, 475, 30, 215, 376, 1162, 644, 247, 1027, 872, 710, 224, 925, 1163, 535, 1155, 541, 1133, 479, 545, 839, 536, 228, 1079, 81, 754, 636, 687, 1107, 897, 959, 452, 612, 1140, 1204, 373, 462, 829, 59, 259, 875, 282, 969, 520, 210, 624, 568, 677, 556, 1005, 891, 1194, 1184, 783, 487, 1199, 438, 1111, 703, 1176, 841, 384, 142, 940, 400, 1208, 638, 395, 25, 551, 1044, 362, 225, 29, 432, 332, 481, 418, 12, 591, 1165, 173, 58, 454, 915, 1172, 1003, 22, 746, 1203, 1213, 1136, 124, 804, 850, 694, 607, 459, 1077, 740, 942, 715, 455, 1195, 408, 498, 300, 667, 999, 905, 921, 900, 141, 68, 163, 435, 1039, 691, 800, 598, 21, 1108, 39, 98, 805, 480, 436, 143, 288, 137, 851, 256, 86, 232, 795, 764, 665, 616, 1040, 594, 709, 1201, 291, 1109, 574, 61, 797, 501, 168, 695, 32, 1182, 340, 844, 597, 1064, 140, 690, 1084, 939, 77, 104, 121, 421, 1054, 611, 539, 1001, 155, 981, 1123, 13, 655, 1174, 509, 697, 619, 100, 937, 821, 219, 649, 176, 1099, 1154, 1038, 338, 1112, 868, 102, 144, 82, 881, 312, 885, 354, 532, 634, 85, 985, 663, 478, 443, 348, 1139, 437, 1088, 170, 18, 236, 792, 902, 1037, 836, 968, 246, 158, 419, 67, 990, 504, 208, 676, 1185, 468, 1119, 367, 472, 242, 1070, 222, 97, 838, 662, 1212, 1211, 641, 352, 633, 223, 977, 5, 590, 417, 1062, 1086, 363, 1198, 693, 1180, 1156, 629, 583, 737, 592, 135, 604, 860, 323, 456, 134, 679, 385, 642, 1148, 941, 1126, 537, 1113, 1028, 887, 840, 514, 265, 571, 1191, 295, 322, 221, 867, 484, 1050, 334, 640, 580, 845, 453, 1065, 1013, 814, 888, 379, 991, 1061, 565, 465, 283, 861, 711, 987, 788, 919, 775, 241, 51, 229, 145, 1171, 356, 863, 290, 1020, 779, 129, 837, 1166, 1090, 522, 736, 19, 381, 953, 668, 413, 1053, 849, 569, 958, 325, 777, 575, 570, 281, 654, 561, 879, 926, 101, 1076, 1138, 972, 213, 1059, 1120, 139, 503, 440, 399, 692, 198, 521, 15, 1181, 359, 106, 1098, 853, 748, 789, 415, 420, 324, 752, 798, 892, 1078, 719, 1147, 177, 1175, 563, 974, 573, 1210, 542, 361, 525, 226, 733, 278, 546, 292, 167, 364, 1186, 255, 686, 519, 360, 1036, 264, 45, 138, 396, 412, 1049, 83, 114, 1205, 1021, 407, 46, 499, 799, 90, 133, 661, 830, 113, 40, 63, 511, 877, 993, 214, 1100, 485, 1069, 907, 344, 310, 1091, 702, 820, 169, 160, 824, 263, 328, 73, 47, 1092, 414, 174, 506, 517, 862, 1048, 126, 790, 944, 825, 31, 473, 1177, 217, 203, 181, 88, 193, 383, 936, 187, 397, 166, 211, 960, 688, 932, 744, 803, 52, 1190, 543, 724, 284, 482, 890, 249, 202, 945, 152, 387, 682, 606, 1179, 488, 1000, 819, 118, 874, 502, 279, 317, 578, 832, 371, 206, 610, 614, 723, 1056, 632, 996, 763, 769, 980, 131, 822, 1130, 976, 172, 917, 1178, 696, 308, 1052, 1149, 997, 554, 1142, 466, 196, 65, 864, 510, 3, 618, 899, 548, 394, 37, 204, 1189, 1114, 857, 329, 645, 430, 43, 587, 631, 1188, 617, 962, 180, 683, 426, 896, 952, 984, 766, 964, 250, 943, 191, 559, 660, 1131, 637, 428, 218, 1045, 516, 157, 23, 108, 477, 924, 320, 243, 42, 1035, 1, 1080, 327, 806, 197, 401, 274, 589, 685, 1173, 1141, 446, 441, 834, 904, 828, 109, 914, 496, 808, 189, 1043, 333, 79, 938, 235, 771, 123, 602, 220, 910]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7675666536263713
the save name prefix for this run is:  chkpt-ID_7675666536263713_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 119
rank avg (pred): 0.543 +- 0.005
mrr vals (pred, true): 0.014, 0.114
batch losses (mrrl, rdl): 0.0, 0.0019268329

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 416
rank avg (pred): 0.328 +- 0.205
mrr vals (pred, true): 0.097, 0.039
batch losses (mrrl, rdl): 0.0, 0.0002783964

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 795
rank avg (pred): 0.320 +- 0.231
mrr vals (pred, true): 0.161, 0.037
batch losses (mrrl, rdl): 0.0, 0.0002283586

Epoch over!
epoch time: 13.608

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 584
rank avg (pred): 0.416 +- 0.284
mrr vals (pred, true): 0.134, 0.042
batch losses (mrrl, rdl): 0.0, 8.0526e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 740
rank avg (pred): 0.013 +- 0.011
mrr vals (pred, true): 0.527, 0.433
batch losses (mrrl, rdl): 0.0, 3.75837e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 4
rank avg (pred): 0.020 +- 0.016
mrr vals (pred, true): 0.461, 0.544
batch losses (mrrl, rdl): 0.0, 1.5764e-06

Epoch over!
epoch time: 14.109

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 376
rank avg (pred): 0.348 +- 0.244
mrr vals (pred, true): 0.149, 0.109
batch losses (mrrl, rdl): 0.0, 0.0002920449

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1121
rank avg (pred): 0.326 +- 0.244
mrr vals (pred, true): 0.187, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001115253

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 717
rank avg (pred): 0.438 +- 0.265
mrr vals (pred, true): 0.101, 0.040
batch losses (mrrl, rdl): 0.0, 1.1603e-05

Epoch over!
epoch time: 12.861

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 143
rank avg (pred): 0.352 +- 0.254
mrr vals (pred, true): 0.198, 0.101
batch losses (mrrl, rdl): 0.0, 0.0001928464

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 626
rank avg (pred): 0.409 +- 0.268
mrr vals (pred, true): 0.162, 0.047
batch losses (mrrl, rdl): 0.0, 6.4746e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1038
rank avg (pred): 0.313 +- 0.252
mrr vals (pred, true): 0.264, 0.038
batch losses (mrrl, rdl): 0.0, 0.0002255624

Epoch over!
epoch time: 12.655

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 301
rank avg (pred): 0.022 +- 0.019
mrr vals (pred, true): 0.481, 0.571
batch losses (mrrl, rdl): 0.0, 1.874e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1103
rank avg (pred): 0.298 +- 0.243
mrr vals (pred, true): 0.267, 0.149
batch losses (mrrl, rdl): 0.0, 0.0001743148

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 278
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.609, 0.617
batch losses (mrrl, rdl): 0.0, 3.6744e-06

Epoch over!
epoch time: 12.701

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 122
rank avg (pred): 0.329 +- 0.259
mrr vals (pred, true): 0.259, 0.118
batch losses (mrrl, rdl): 0.1980168968, 0.0002644825

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 883
rank avg (pred): 0.411 +- 0.203
mrr vals (pred, true): 0.066, 0.037
batch losses (mrrl, rdl): 0.0025366643, 1.54483e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 491
rank avg (pred): 0.323 +- 0.197
mrr vals (pred, true): 0.096, 0.131
batch losses (mrrl, rdl): 0.0122238565, 7.29178e-05

Epoch over!
epoch time: 12.892

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 420
rank avg (pred): 0.400 +- 0.205
mrr vals (pred, true): 0.071, 0.040
batch losses (mrrl, rdl): 0.0042265384, 3.22583e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 173
rank avg (pred): 0.384 +- 0.214
mrr vals (pred, true): 0.094, 0.043
batch losses (mrrl, rdl): 0.0189859755, 3.72986e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 453
rank avg (pred): 0.402 +- 0.197
mrr vals (pred, true): 0.068, 0.040
batch losses (mrrl, rdl): 0.0032669567, 2.55649e-05

Epoch over!
epoch time: 13.016

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 509
rank avg (pred): 0.297 +- 0.200
mrr vals (pred, true): 0.125, 0.115
batch losses (mrrl, rdl): 0.00116531, 3.19126e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 142
rank avg (pred): 0.430 +- 0.176
mrr vals (pred, true): 0.055, 0.090
batch losses (mrrl, rdl): 0.0003009714, 0.0005589011

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 10
rank avg (pred): 0.012 +- 0.010
mrr vals (pred, true): 0.529, 0.545
batch losses (mrrl, rdl): 0.00269237, 4.8596e-06

Epoch over!
epoch time: 13.195

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 596
rank avg (pred): 0.468 +- 0.132
mrr vals (pred, true): 0.041, 0.037
batch losses (mrrl, rdl): 0.0008905265, 3.99855e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 403
rank avg (pred): 0.385 +- 0.194
mrr vals (pred, true): 0.072, 0.116
batch losses (mrrl, rdl): 0.0194190275, 0.0005531152

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 998
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.608, 0.624
batch losses (mrrl, rdl): 0.002628169, 4.0617e-06

Epoch over!
epoch time: 13.369

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 284
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.597, 0.633
batch losses (mrrl, rdl): 0.0129622407, 2.2793e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 223
rank avg (pred): 0.408 +- 0.181
mrr vals (pred, true): 0.066, 0.039
batch losses (mrrl, rdl): 0.0025923424, 4.01057e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 313
rank avg (pred): 0.015 +- 0.014
mrr vals (pred, true): 0.545, 0.565
batch losses (mrrl, rdl): 0.0038494212, 2.1103e-06

Epoch over!
epoch time: 14.636

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 378
rank avg (pred): 0.402 +- 0.182
mrr vals (pred, true): 0.073, 0.080
batch losses (mrrl, rdl): 0.0051027881, 0.0002637029

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 289
rank avg (pred): 0.016 +- 0.019
mrr vals (pred, true): 0.559, 0.562
batch losses (mrrl, rdl): 8.75682e-05, 1.5731e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1162
rank avg (pred): 0.435 +- 0.140
mrr vals (pred, true): 0.052, 0.036
batch losses (mrrl, rdl): 2.86704e-05, 3.06102e-05

Epoch over!
epoch time: 15.25

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 503
rank avg (pred): 0.324 +- 0.205
mrr vals (pred, true): 0.115, 0.122
batch losses (mrrl, rdl): 0.000528501, 0.0001024981

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 967
rank avg (pred): 0.411 +- 0.162
mrr vals (pred, true): 0.062, 0.042
batch losses (mrrl, rdl): 0.0013392273, 5.2202e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 447
rank avg (pred): 0.414 +- 0.157
mrr vals (pred, true): 0.065, 0.044
batch losses (mrrl, rdl): 0.002120998, 4.00732e-05

Epoch over!
epoch time: 14.925

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 182
rank avg (pred): 0.373 +- 0.182
mrr vals (pred, true): 0.082, 0.040
batch losses (mrrl, rdl): 0.0101655778, 0.0001095064

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 67
rank avg (pred): 0.079 +- 0.135
mrr vals (pred, true): 0.533, 0.548
batch losses (mrrl, rdl): 0.0023733238, 8.25262e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 290
rank avg (pred): 0.042 +- 0.095
mrr vals (pred, true): 0.638, 0.635
batch losses (mrrl, rdl): 0.0001211508, 1.82204e-05

Epoch over!
epoch time: 15.254

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 442
rank avg (pred): 0.383 +- 0.165
mrr vals (pred, true): 0.078, 0.043
batch losses (mrrl, rdl): 0.0077123465, 7.17253e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 783
rank avg (pred): 0.441 +- 0.106
mrr vals (pred, true): 0.049, 0.039
batch losses (mrrl, rdl): 2.13257e-05, 5.84741e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 942
rank avg (pred): 0.412 +- 0.129
mrr vals (pred, true): 0.062, 0.083
batch losses (mrrl, rdl): 0.0015115696, 0.0003239674

Epoch over!
epoch time: 13.66

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 525
rank avg (pred): 0.416 +- 0.114
mrr vals (pred, true): 0.051, 0.063
batch losses (mrrl, rdl): 1.08567e-05, 0.0001183301

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1188
rank avg (pred): 0.410 +- 0.117
mrr vals (pred, true): 0.057, 0.042
batch losses (mrrl, rdl): 0.000457429, 6.44634e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 971
rank avg (pred): 0.394 +- 0.128
mrr vals (pred, true): 0.066, 0.046
batch losses (mrrl, rdl): 0.0024033056, 6.25042e-05

Epoch over!
epoch time: 13.687

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.420 +- 0.105
mrr vals (pred, true): 0.052, 0.090

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04016 	 0.02982 	 ~...
    3 	     1 	 0.03988 	 0.03370 	 ~...
   47 	     2 	 0.05469 	 0.03532 	 ~...
    7 	     3 	 0.04050 	 0.03584 	 ~...
   22 	     4 	 0.04956 	 0.03595 	 ~...
   29 	     5 	 0.05019 	 0.03621 	 ~...
   25 	     6 	 0.04984 	 0.03665 	 ~...
   55 	     7 	 0.05771 	 0.03668 	 ~...
   63 	     8 	 0.06288 	 0.03674 	 ~...
   84 	     9 	 0.08391 	 0.03686 	 m..s
   12 	    10 	 0.04313 	 0.03694 	 ~...
   41 	    11 	 0.05286 	 0.03699 	 ~...
   16 	    12 	 0.04616 	 0.03713 	 ~...
   11 	    13 	 0.04169 	 0.03723 	 ~...
   40 	    14 	 0.05234 	 0.03760 	 ~...
   50 	    15 	 0.05636 	 0.03780 	 ~...
   58 	    16 	 0.06006 	 0.03794 	 ~...
   71 	    17 	 0.07083 	 0.03810 	 m..s
   19 	    18 	 0.04773 	 0.03827 	 ~...
   38 	    19 	 0.05214 	 0.03832 	 ~...
   64 	    20 	 0.06292 	 0.03841 	 ~...
    9 	    21 	 0.04138 	 0.03866 	 ~...
   86 	    22 	 0.08623 	 0.03880 	 m..s
   70 	    23 	 0.06955 	 0.03883 	 m..s
   10 	    24 	 0.04142 	 0.03906 	 ~...
   15 	    25 	 0.04443 	 0.03908 	 ~...
   26 	    26 	 0.04987 	 0.03913 	 ~...
    1 	    27 	 0.03978 	 0.03917 	 ~...
   13 	    28 	 0.04422 	 0.03928 	 ~...
   67 	    29 	 0.06633 	 0.03931 	 ~...
   49 	    30 	 0.05483 	 0.03935 	 ~...
   62 	    31 	 0.06122 	 0.03940 	 ~...
    2 	    32 	 0.03980 	 0.03963 	 ~...
    8 	    33 	 0.04134 	 0.03977 	 ~...
   46 	    34 	 0.05453 	 0.04002 	 ~...
   36 	    35 	 0.05205 	 0.04019 	 ~...
   14 	    36 	 0.04431 	 0.04046 	 ~...
    5 	    37 	 0.03993 	 0.04070 	 ~...
   18 	    38 	 0.04764 	 0.04112 	 ~...
    0 	    39 	 0.03964 	 0.04116 	 ~...
   83 	    40 	 0.08180 	 0.04116 	 m..s
   51 	    41 	 0.05653 	 0.04126 	 ~...
   30 	    42 	 0.05030 	 0.04153 	 ~...
   42 	    43 	 0.05299 	 0.04190 	 ~...
   33 	    44 	 0.05130 	 0.04217 	 ~...
   48 	    45 	 0.05476 	 0.04220 	 ~...
    4 	    46 	 0.03992 	 0.04328 	 ~...
   61 	    47 	 0.06115 	 0.07324 	 ~...
   23 	    48 	 0.04973 	 0.07590 	 ~...
   32 	    49 	 0.05123 	 0.07720 	 ~...
   24 	    50 	 0.04980 	 0.07924 	 ~...
   44 	    51 	 0.05347 	 0.07992 	 ~...
   21 	    52 	 0.04954 	 0.08000 	 m..s
   31 	    53 	 0.05092 	 0.08483 	 m..s
   53 	    54 	 0.05739 	 0.08658 	 ~...
   34 	    55 	 0.05159 	 0.08687 	 m..s
   59 	    56 	 0.06016 	 0.08739 	 ~...
   39 	    57 	 0.05223 	 0.08873 	 m..s
   35 	    58 	 0.05203 	 0.08934 	 m..s
   45 	    59 	 0.05428 	 0.08968 	 m..s
   37 	    60 	 0.05205 	 0.09043 	 m..s
   17 	    61 	 0.04756 	 0.09072 	 m..s
   43 	    62 	 0.05339 	 0.09103 	 m..s
   27 	    63 	 0.05004 	 0.09154 	 m..s
   52 	    64 	 0.05675 	 0.09161 	 m..s
   60 	    65 	 0.06050 	 0.09213 	 m..s
   28 	    66 	 0.05007 	 0.09245 	 m..s
   20 	    67 	 0.04794 	 0.09376 	 m..s
   57 	    68 	 0.05777 	 0.09433 	 m..s
   54 	    69 	 0.05770 	 0.09493 	 m..s
   56 	    70 	 0.05775 	 0.09999 	 m..s
   78 	    71 	 0.07942 	 0.11059 	 m..s
   72 	    72 	 0.07396 	 0.11071 	 m..s
   80 	    73 	 0.08048 	 0.11099 	 m..s
   65 	    74 	 0.06440 	 0.11169 	 m..s
   69 	    75 	 0.06856 	 0.11467 	 m..s
   82 	    76 	 0.08101 	 0.11629 	 m..s
   73 	    77 	 0.07479 	 0.11998 	 m..s
   75 	    78 	 0.07508 	 0.12127 	 m..s
   79 	    79 	 0.08030 	 0.12201 	 m..s
   66 	    80 	 0.06631 	 0.12642 	 m..s
   68 	    81 	 0.06704 	 0.12672 	 m..s
   76 	    82 	 0.07534 	 0.13270 	 m..s
   81 	    83 	 0.08093 	 0.13608 	 m..s
   74 	    84 	 0.07494 	 0.13723 	 m..s
   87 	    85 	 0.09282 	 0.13797 	 m..s
   85 	    86 	 0.08551 	 0.14086 	 m..s
   77 	    87 	 0.07941 	 0.14453 	 m..s
   88 	    88 	 0.09924 	 0.14521 	 m..s
   92 	    89 	 0.12421 	 0.14963 	 ~...
   91 	    90 	 0.11502 	 0.16007 	 m..s
   89 	    91 	 0.11447 	 0.16485 	 m..s
   90 	    92 	 0.11454 	 0.16535 	 m..s
   93 	    93 	 0.15858 	 0.16790 	 ~...
   96 	    94 	 0.38765 	 0.36050 	 ~...
   97 	    95 	 0.39444 	 0.39730 	 ~...
   94 	    96 	 0.36988 	 0.43006 	 m..s
   99 	    97 	 0.48413 	 0.50176 	 ~...
  100 	    98 	 0.48844 	 0.50875 	 ~...
  104 	    99 	 0.52530 	 0.51267 	 ~...
  101 	   100 	 0.48897 	 0.51292 	 ~...
  102 	   101 	 0.51924 	 0.52169 	 ~...
   95 	   102 	 0.37582 	 0.52794 	 MISS
  107 	   103 	 0.52941 	 0.53229 	 ~...
  103 	   104 	 0.51988 	 0.54013 	 ~...
  109 	   105 	 0.55820 	 0.54185 	 ~...
  106 	   106 	 0.52870 	 0.54273 	 ~...
  108 	   107 	 0.55785 	 0.55278 	 ~...
  110 	   108 	 0.56325 	 0.55857 	 ~...
  105 	   109 	 0.52730 	 0.55974 	 m..s
   98 	   110 	 0.47817 	 0.57695 	 m..s
  116 	   111 	 0.61403 	 0.58207 	 m..s
  115 	   112 	 0.60716 	 0.61320 	 ~...
  119 	   113 	 0.62792 	 0.61400 	 ~...
  112 	   114 	 0.57074 	 0.61656 	 m..s
  113 	   115 	 0.57924 	 0.61885 	 m..s
  120 	   116 	 0.63200 	 0.61945 	 ~...
  114 	   117 	 0.59381 	 0.61970 	 ~...
  111 	   118 	 0.56660 	 0.62127 	 m..s
  118 	   119 	 0.62646 	 0.62143 	 ~...
  117 	   120 	 0.62036 	 0.62676 	 ~...
==========================================
r_mrr = 0.9878650307655334
r2_mrr = 0.9709816575050354
spearmanr_mrr@5 = 0.8183407783508301
spearmanr_mrr@10 = 0.7538130879402161
spearmanr_mrr@50 = 0.9970492124557495
spearmanr_mrr@100 = 0.9938898086547852
spearmanr_mrr@All = 0.9935452938079834
==========================================
test time: 0.468
Done Testing dataset UMLS
total time taken: 212.73726081848145
training time taken: 206.36420464515686
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 4303420726650775
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 421, 392, 1014, 319, 547, 417, 654, 855, 925, 1168, 371, 224, 714, 355, 213, 484, 955, 1196, 804, 409, 807, 673, 1089, 828, 62, 154, 341, 125, 27, 576, 1151, 121, 84, 1160, 435, 759, 304, 874, 259, 1157, 134, 635, 607, 272, 1099, 385, 32, 580, 598, 796, 459, 1106, 527, 321, 267, 1011, 143, 1052, 867, 591, 716, 1188, 914, 870, 227, 1204, 275, 300, 447, 68, 751, 130, 210, 410, 615, 443, 157, 348, 2, 972, 60, 736, 913, 104, 625, 480, 884, 1060, 781, 922, 787, 1091, 329, 299, 370, 303, 155, 382, 798, 425, 78, 1042, 1173, 406, 959, 659, 1067, 1203, 715, 1122, 767, 1087, 556, 400, 1057, 241, 9, 706, 856, 937]
valid_ids (0): []
train_ids (1094): [95, 353, 948, 509, 273, 1199, 644, 992, 1137, 1094, 703, 82, 456, 269, 776, 1023, 915, 649, 129, 719, 1181, 575, 243, 1053, 826, 908, 761, 971, 74, 483, 772, 5, 1019, 879, 753, 1070, 90, 747, 667, 562, 1013, 426, 102, 601, 144, 819, 839, 458, 434, 1071, 295, 287, 708, 848, 454, 1075, 850, 639, 73, 825, 558, 260, 1100, 599, 829, 296, 248, 424, 712, 1142, 919, 658, 460, 1171, 857, 211, 803, 450, 1051, 18, 1128, 1004, 895, 905, 617, 12, 939, 1193, 966, 133, 1135, 492, 1195, 1167, 301, 120, 1103, 1098, 1049, 502, 1180, 208, 713, 111, 627, 324, 583, 1007, 618, 445, 1048, 1205, 632, 318, 735, 372, 277, 265, 1147, 845, 545, 270, 284, 640, 528, 985, 1101, 264, 452, 574, 100, 860, 89, 597, 595, 784, 239, 399, 1086, 186, 451, 950, 779, 970, 503, 1012, 340, 317, 87, 175, 488, 739, 205, 431, 550, 361, 1090, 26, 1170, 397, 518, 793, 231, 590, 989, 298, 114, 201, 69, 778, 813, 721, 696, 105, 1113, 37, 81, 1159, 469, 866, 854, 165, 1043, 262, 810, 203, 57, 475, 977, 740, 873, 572, 365, 827, 1024, 1092, 332, 229, 338, 695, 146, 1214, 947, 209, 629, 853, 552, 729, 1114, 903, 1134, 1208, 790, 918, 336, 279, 46, 233, 702, 670, 80, 872, 1115, 660, 136, 1149, 64, 215, 268, 156, 592, 40, 762, 543, 768, 52, 325, 880, 920, 21, 886, 717, 1213, 608, 328, 620, 1136, 1130, 403, 1123, 780, 110, 996, 374, 823, 30, 1107, 147, 398, 221, 153, 407, 891, 25, 305, 569, 145, 1045, 91, 816, 108, 47, 333, 344, 515, 228, 22, 184, 169, 1001, 815, 968, 331, 805, 643, 648, 230, 573, 88, 560, 489, 487, 529, 737, 34, 167, 979, 782, 15, 1097, 1041, 1076, 77, 1082, 85, 462, 878, 974, 605, 500, 883, 168, 662, 347, 1059, 1210, 570, 491, 422, 1073, 466, 252, 137, 987, 170, 63, 923, 54, 844, 628, 159, 1035, 468, 537, 70, 687, 728, 775, 148, 432, 724, 1186, 764, 461, 470, 257, 151, 393, 376, 285, 638, 1132, 245, 464, 603, 1172, 471, 748, 909, 172, 668, 894, 732, 115, 786, 166, 244, 140, 783, 292, 549, 359, 619, 741, 306, 669, 742, 478, 904, 838, 534, 991, 1069, 678, 930, 297, 103, 954, 967, 119, 652, 362, 181, 189, 725, 927, 463, 282, 1085, 395, 630, 745, 1175, 794, 578, 593, 997, 865, 1009, 36, 342, 437, 536, 692, 1145, 770, 830, 1058, 897, 164, 283, 750, 1003, 664, 1184, 316, 349, 817, 932, 1006, 413, 636, 1072, 986, 360, 238, 936, 928, 423, 150, 681, 935, 7, 131, 963, 694, 1032, 246, 579, 49, 1064, 634, 477, 1118, 219, 777, 582, 98, 1080, 862, 193, 101, 863, 266, 28, 523, 522, 621, 797, 236, 709, 554, 13, 419, 943, 93, 861, 834, 1183, 482, 1, 812, 1112, 526, 727, 1095, 387, 841, 840, 56, 1044, 3, 1040, 187, 611, 92, 6, 94, 1126, 436, 960, 14, 1198, 564, 357, 586, 1150, 594, 66, 48, 614, 314, 1127, 1133, 637, 646, 1108, 308, 258, 738, 944, 377, 1038, 1111, 1010, 521, 1125, 44, 495, 501, 898, 23, 188, 726, 773, 1081, 984, 1000, 1102, 366, 749, 1143, 311, 831, 29, 1055, 286, 563, 892, 496, 588, 519, 179, 290, 253, 567, 524, 746, 525, 921, 38, 19, 1015, 888, 722, 900, 135, 814, 20, 1025, 97, 206, 415, 472, 138, 975, 799, 404, 1185, 765, 4, 194, 256, 851, 806, 774, 112, 0, 758, 565, 139, 1165, 697, 881, 345, 1212, 788, 1169, 952, 1020, 242, 679, 811, 938, 1031, 142, 988, 384, 320, 995, 1202, 1131, 680, 339, 671, 1152, 312, 71, 688, 197, 1034, 438, 976, 1008, 192, 467, 1189, 53, 416, 542, 657, 17, 514, 1206, 280, 731, 852, 655, 11, 907, 118, 96, 1046, 113, 538, 1084, 212, 801, 755, 1109, 202, 1088, 733, 641, 160, 367, 39, 754, 42, 1117, 1017, 465, 394, 504, 389, 789, 929, 1029, 10, 1047, 497, 1036, 689, 401, 67, 1096, 973, 1192, 288, 182, 122, 771, 541, 158, 785, 375, 994, 388, 58, 833, 962, 396, 1187, 978, 566, 255, 561, 875, 1139, 520, 220, 693, 141, 899, 51, 335, 822, 1021, 479, 1116, 546, 585, 701, 949, 998, 1074, 234, 1141, 846, 613, 276, 176, 43, 232, 677, 132, 1144, 1176, 808, 414, 356, 75, 183, 358, 1056, 281, 707, 1062, 683, 942, 906, 1154, 261, 204, 61, 983, 832, 704, 16, 405, 408, 1028, 45, 982, 964, 718, 1140, 474, 499, 559, 555, 1018, 606, 124, 1153, 596, 612, 802, 951, 383, 455, 730, 792, 278, 1093, 294, 418, 1191, 247, 1120, 577, 173, 289, 123, 1066, 651, 800, 548, 847, 1039, 161, 217, 1065, 882, 616, 493, 476, 207, 584, 511, 969, 1002, 581, 354, 274, 896, 626, 291, 330, 149, 1124, 763, 836, 35, 877, 390, 924, 931, 791, 1121, 1026, 126, 917, 642, 322, 869, 876, 216, 1110, 198, 1200, 177, 843, 363, 871, 720, 199, 532, 531, 1146, 1119, 1027, 901, 911, 185, 271, 1050, 956, 196, 128, 1201, 310, 859, 557, 958, 59, 430, 442, 744, 1174, 412, 498, 65, 544, 946, 218, 535, 1162, 505, 868, 481, 1077, 622, 86, 999, 449, 539, 824, 571, 698, 842, 600, 76, 350, 705, 448, 1030, 820, 178, 510, 485, 690, 490, 663, 1194, 551, 609, 1016, 666, 1022, 33, 195, 427, 457, 902, 858, 700, 769, 351, 240, 1079, 428, 486, 610, 337, 473, 1163, 734, 650, 981, 379, 107, 225, 116, 24, 1129, 334, 106, 8, 1177, 887, 645, 849, 661, 302, 1158, 453, 433, 1033, 254, 1104, 1164, 589, 684, 711, 961, 1182, 893, 1138, 55, 604, 1054, 364, 665, 533, 152, 127, 190, 309, 647, 293, 313, 352, 263, 623, 685, 682, 315, 378, 980, 200, 916, 674, 327, 933, 223, 953, 323, 391, 250, 1063, 699, 835, 889, 926, 766, 890, 381, 631, 1083, 587, 1037, 99, 540, 760, 993, 117, 516, 691, 513, 818, 1105, 343, 506, 174, 1190, 1179, 1068, 1197, 251, 1166, 72, 934, 743, 508, 180, 214, 568, 885, 1209, 386, 965, 402, 757, 672, 440, 171, 109, 517, 553, 653, 941, 368, 945, 420, 710, 1156, 1148, 723, 235, 864, 226, 249, 444, 624, 990, 346, 633, 530, 957, 676, 41, 1078, 446, 795, 373, 163, 1161, 940, 307, 1211, 222, 752, 686, 1207, 50, 162, 1061, 31, 494, 79, 512, 1005, 83, 191, 675, 821, 237, 910, 326, 369, 837, 411, 439, 1155, 912, 809, 756, 441, 380, 602, 429, 656, 507]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2464276625823700
the save name prefix for this run is:  chkpt-ID_2464276625823700_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.454 +- 0.005
mrr vals (pred, true): 0.001, 0.267
batch losses (mrrl, rdl): 0.0, 0.0037191631

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.291 +- 0.194
mrr vals (pred, true): 0.108, 0.138
batch losses (mrrl, rdl): 0.0, 0.0007941914

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 199
rank avg (pred): 0.260 +- 0.214
mrr vals (pred, true): 0.298, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006889647

Epoch over!
epoch time: 14.146

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.366 +- 0.283
mrr vals (pred, true): 0.255, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001087949

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.322 +- 0.277
mrr vals (pred, true): 0.336, 0.003
batch losses (mrrl, rdl): 0.0, 0.0003124118

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 457
rank avg (pred): 0.279 +- 0.247
mrr vals (pred, true): 0.354, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004391382

Epoch over!
epoch time: 15.486

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1048
rank avg (pred): 0.237 +- 0.232
mrr vals (pred, true): 0.376, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007597717

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.270 +- 0.248
mrr vals (pred, true): 0.358, 0.205
batch losses (mrrl, rdl): 0.0, 0.0008412052

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 538
rank avg (pred): 0.185 +- 0.176
mrr vals (pred, true): 0.382, 0.019
batch losses (mrrl, rdl): 0.0, 6.8183e-06

Epoch over!
epoch time: 14.83

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.046 +- 0.046
mrr vals (pred, true): 0.428, 0.243
batch losses (mrrl, rdl): 0.0, 1.1906e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.251 +- 0.249
mrr vals (pred, true): 0.387, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006166488

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 379
rank avg (pred): 0.259 +- 0.254
mrr vals (pred, true): 0.375, 0.152
batch losses (mrrl, rdl): 0.0, 0.0003937553

Epoch over!
epoch time: 14.839

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1066
rank avg (pred): 0.046 +- 0.047
mrr vals (pred, true): 0.468, 0.241
batch losses (mrrl, rdl): 0.0, 1.3561e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 6
rank avg (pred): 0.047 +- 0.048
mrr vals (pred, true): 0.461, 0.135
batch losses (mrrl, rdl): 0.0, 1.0177e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.259 +- 0.256
mrr vals (pred, true): 0.387, 0.004
batch losses (mrrl, rdl): 0.0, 0.00052477

Epoch over!
epoch time: 14.635

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 564
rank avg (pred): 0.189 +- 0.185
mrr vals (pred, true): 0.381, 0.018
batch losses (mrrl, rdl): 1.0958201885, 5.1017e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.450 +- 0.178
mrr vals (pred, true): 0.062, 0.022
batch losses (mrrl, rdl): 0.0014817704, 0.0005856929

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 766
rank avg (pred): 0.329 +- 0.201
mrr vals (pred, true): 0.112, 0.166
batch losses (mrrl, rdl): 0.0300461501, 0.0011995622

Epoch over!
epoch time: 15.403

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 567
rank avg (pred): 0.457 +- 0.138
mrr vals (pred, true): 0.048, 0.008
batch losses (mrrl, rdl): 5.82701e-05, 0.0002816469

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 642
rank avg (pred): 0.451 +- 0.128
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.000135744, 0.0003257615

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1003
rank avg (pred): 0.323 +- 0.201
mrr vals (pred, true): 0.118, 0.208
batch losses (mrrl, rdl): 0.0820150897, 0.0012821454

Epoch over!
epoch time: 14.85

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 166
rank avg (pred): 0.351 +- 0.191
mrr vals (pred, true): 0.088, 0.004
batch losses (mrrl, rdl): 0.0144673958, 0.0001664636

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1019
rank avg (pred): 0.259 +- 0.167
mrr vals (pred, true): 0.121, 0.175
batch losses (mrrl, rdl): 0.0290325433, 0.0006062402

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.247 +- 0.159
mrr vals (pred, true): 0.123, 0.204
batch losses (mrrl, rdl): 0.0644467324, 0.000576636

Epoch over!
epoch time: 13.805

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1102
rank avg (pred): 0.295 +- 0.184
mrr vals (pred, true): 0.109, 0.200
batch losses (mrrl, rdl): 0.0840359107, 0.0009377802

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.327 +- 0.178
mrr vals (pred, true): 0.098, 0.004
batch losses (mrrl, rdl): 0.0233716369, 0.0003942271

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.381 +- 0.130
mrr vals (pred, true): 0.057, 0.027
batch losses (mrrl, rdl): 0.0004512104, 0.0010559717

Epoch over!
epoch time: 15.563

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 958
rank avg (pred): 0.322 +- 0.177
mrr vals (pred, true): 0.089, 0.005
batch losses (mrrl, rdl): 0.0154365264, 0.0003335789

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.376 +- 0.123
mrr vals (pred, true): 0.054, 0.025
batch losses (mrrl, rdl): 0.0001410402, 0.0008387817

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 76
rank avg (pred): 0.048 +- 0.031
mrr vals (pred, true): 0.169, 0.203
batch losses (mrrl, rdl): 0.0116009787, 1.7498e-06

Epoch over!
epoch time: 13.747

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1061
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.267, 0.262
batch losses (mrrl, rdl): 0.0001920466, 2.13346e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1120
rank avg (pred): 0.252 +- 0.159
mrr vals (pred, true): 0.124, 0.004
batch losses (mrrl, rdl): 0.0554197058, 0.0008559729

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.292 +- 0.171
mrr vals (pred, true): 0.108, 0.005
batch losses (mrrl, rdl): 0.0337122232, 0.0004499428

Epoch over!
epoch time: 14.634

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1194
rank avg (pred): 0.367 +- 0.109
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 4.444e-07, 0.0002826162

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.360 +- 0.116
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001244333, 0.0002951391

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.281 +- 0.168
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0351374075, 0.0005167467

Epoch over!
epoch time: 14.527

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.036 +- 0.023
mrr vals (pred, true): 0.170, 0.190
batch losses (mrrl, rdl): 0.0038376246, 3.3232e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.044 +- 0.027
mrr vals (pred, true): 0.166, 0.179
batch losses (mrrl, rdl): 0.0017665944, 5.8902e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 974
rank avg (pred): 0.026 +- 0.017
mrr vals (pred, true): 0.210, 0.201
batch losses (mrrl, rdl): 0.0007051809, 1.0576e-05

Epoch over!
epoch time: 14.914

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.173 +- 0.108
mrr vals (pred, true): 0.128, 0.153
batch losses (mrrl, rdl): 0.0061741122, 0.0002614917

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.351 +- 0.105
mrr vals (pred, true): 0.052, 0.025
batch losses (mrrl, rdl): 5.96343e-05, 0.0006453604

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.295 +- 0.159
mrr vals (pred, true): 0.093, 0.004
batch losses (mrrl, rdl): 0.0182700865, 0.0005386228

Epoch over!
epoch time: 13.642

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 91
rank avg (pred): 0.313 +- 0.147
mrr vals (pred, true): 0.078, 0.140
batch losses (mrrl, rdl): 0.0373178497, 0.0009366209

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.356 +- 0.099
mrr vals (pred, true): 0.047, 0.015
batch losses (mrrl, rdl): 6.5058e-05, 0.0003419464

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.280 +- 0.160
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0344380699, 0.0006499005

Epoch over!
epoch time: 14.671

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.359 +- 0.103
mrr vals (pred, true): 0.048, 0.009

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   32 	     0 	 0.08070 	 0.00317 	 m..s
   11 	     1 	 0.04657 	 0.00334 	 m..s
   15 	     2 	 0.04676 	 0.00336 	 m..s
   37 	     3 	 0.08654 	 0.00353 	 m..s
   38 	     4 	 0.09012 	 0.00359 	 m..s
    7 	     5 	 0.04280 	 0.00360 	 m..s
   16 	     6 	 0.04681 	 0.00364 	 m..s
   10 	     7 	 0.04650 	 0.00366 	 m..s
   60 	     8 	 0.09846 	 0.00370 	 m..s
    2 	     9 	 0.04117 	 0.00373 	 m..s
   74 	    10 	 0.10114 	 0.00377 	 m..s
   91 	    11 	 0.11048 	 0.00378 	 MISS
    8 	    12 	 0.04420 	 0.00381 	 m..s
   52 	    13 	 0.09655 	 0.00384 	 m..s
   48 	    14 	 0.09569 	 0.00387 	 m..s
    0 	    15 	 0.03986 	 0.00388 	 m..s
   31 	    16 	 0.07852 	 0.00390 	 m..s
   95 	    17 	 0.11529 	 0.00392 	 MISS
   39 	    18 	 0.09090 	 0.00393 	 m..s
   45 	    19 	 0.09359 	 0.00397 	 m..s
   73 	    20 	 0.09907 	 0.00408 	 m..s
   18 	    21 	 0.04715 	 0.00409 	 m..s
    6 	    22 	 0.04245 	 0.00411 	 m..s
   78 	    23 	 0.10210 	 0.00412 	 m..s
   94 	    24 	 0.11473 	 0.00412 	 MISS
   43 	    25 	 0.09330 	 0.00414 	 m..s
   72 	    26 	 0.09904 	 0.00419 	 m..s
   82 	    27 	 0.10367 	 0.00422 	 m..s
   14 	    28 	 0.04673 	 0.00424 	 m..s
   53 	    29 	 0.09707 	 0.00437 	 m..s
   42 	    30 	 0.09319 	 0.00442 	 m..s
   60 	    31 	 0.09846 	 0.00442 	 m..s
   40 	    32 	 0.09094 	 0.00446 	 m..s
   75 	    33 	 0.10117 	 0.00450 	 m..s
   47 	    34 	 0.09561 	 0.00451 	 m..s
   41 	    35 	 0.09145 	 0.00459 	 m..s
   60 	    36 	 0.09846 	 0.00463 	 m..s
   89 	    37 	 0.10917 	 0.00468 	 MISS
   60 	    38 	 0.09846 	 0.00472 	 m..s
   70 	    39 	 0.09879 	 0.00498 	 m..s
   84 	    40 	 0.10425 	 0.00528 	 m..s
    9 	    41 	 0.04470 	 0.00540 	 m..s
   60 	    42 	 0.09846 	 0.00551 	 m..s
   20 	    43 	 0.04757 	 0.00876 	 m..s
   13 	    44 	 0.04669 	 0.00898 	 m..s
    5 	    45 	 0.04174 	 0.00916 	 m..s
    1 	    46 	 0.03997 	 0.00921 	 m..s
    3 	    47 	 0.04119 	 0.00943 	 m..s
   21 	    48 	 0.04863 	 0.00999 	 m..s
   19 	    49 	 0.04716 	 0.01044 	 m..s
    4 	    50 	 0.04140 	 0.01188 	 ~...
   17 	    51 	 0.04711 	 0.01274 	 m..s
   22 	    52 	 0.05111 	 0.01807 	 m..s
   23 	    53 	 0.05123 	 0.02133 	 ~...
   12 	    54 	 0.04658 	 0.02269 	 ~...
   25 	    55 	 0.05364 	 0.02424 	 ~...
   26 	    56 	 0.05373 	 0.02452 	 ~...
   28 	    57 	 0.06911 	 0.02530 	 m..s
   27 	    58 	 0.05507 	 0.02655 	 ~...
   24 	    59 	 0.05156 	 0.04148 	 ~...
   98 	    60 	 0.13345 	 0.05106 	 m..s
   30 	    61 	 0.07691 	 0.09733 	 ~...
   96 	    62 	 0.11705 	 0.12113 	 ~...
   46 	    63 	 0.09361 	 0.12756 	 m..s
   33 	    64 	 0.08201 	 0.13809 	 m..s
   34 	    65 	 0.08365 	 0.14112 	 m..s
   49 	    66 	 0.09584 	 0.14249 	 m..s
   35 	    67 	 0.08443 	 0.14271 	 m..s
  102 	    68 	 0.15031 	 0.14500 	 ~...
   68 	    69 	 0.09849 	 0.14642 	 m..s
   97 	    70 	 0.12338 	 0.14747 	 ~...
   58 	    71 	 0.09808 	 0.14854 	 m..s
   36 	    72 	 0.08444 	 0.14934 	 m..s
   50 	    73 	 0.09585 	 0.14963 	 m..s
  103 	    74 	 0.15181 	 0.15089 	 ~...
   51 	    75 	 0.09651 	 0.15094 	 m..s
  101 	    76 	 0.14418 	 0.15206 	 ~...
   60 	    77 	 0.09846 	 0.15261 	 m..s
   60 	    78 	 0.09846 	 0.15438 	 m..s
   60 	    79 	 0.09846 	 0.15816 	 m..s
  100 	    80 	 0.13423 	 0.15845 	 ~...
   69 	    81 	 0.09873 	 0.16538 	 m..s
   44 	    82 	 0.09347 	 0.16565 	 m..s
   29 	    83 	 0.07643 	 0.16607 	 m..s
   83 	    84 	 0.10386 	 0.16650 	 m..s
   54 	    85 	 0.09751 	 0.16854 	 m..s
   71 	    86 	 0.09893 	 0.17104 	 m..s
   57 	    87 	 0.09770 	 0.17169 	 m..s
  105 	    88 	 0.16168 	 0.17282 	 ~...
   56 	    89 	 0.09768 	 0.17295 	 m..s
   59 	    90 	 0.09818 	 0.17355 	 m..s
   99 	    91 	 0.13361 	 0.17536 	 m..s
   80 	    92 	 0.10258 	 0.17628 	 m..s
   55 	    93 	 0.09754 	 0.17681 	 m..s
   77 	    94 	 0.10206 	 0.17788 	 m..s
   79 	    95 	 0.10222 	 0.17845 	 m..s
   81 	    96 	 0.10273 	 0.17983 	 m..s
  107 	    97 	 0.16362 	 0.18173 	 ~...
   76 	    98 	 0.10127 	 0.18195 	 m..s
   86 	    99 	 0.10471 	 0.18278 	 m..s
   85 	   100 	 0.10462 	 0.18507 	 m..s
  110 	   101 	 0.17009 	 0.18788 	 ~...
  112 	   102 	 0.18140 	 0.18871 	 ~...
  113 	   103 	 0.18149 	 0.19110 	 ~...
   92 	   104 	 0.11441 	 0.19210 	 m..s
  114 	   105 	 0.18204 	 0.19682 	 ~...
  115 	   106 	 0.18326 	 0.20160 	 ~...
   88 	   107 	 0.10783 	 0.20379 	 m..s
  108 	   108 	 0.16388 	 0.20703 	 m..s
  117 	   109 	 0.18795 	 0.20774 	 ~...
  119 	   110 	 0.19007 	 0.20816 	 ~...
   93 	   111 	 0.11460 	 0.21079 	 m..s
   87 	   112 	 0.10683 	 0.21109 	 MISS
   90 	   113 	 0.11016 	 0.21182 	 MISS
  104 	   114 	 0.15857 	 0.21490 	 m..s
  120 	   115 	 0.19343 	 0.23008 	 m..s
  111 	   116 	 0.17831 	 0.23043 	 m..s
  109 	   117 	 0.16833 	 0.23807 	 m..s
  106 	   118 	 0.16191 	 0.24214 	 m..s
  118 	   119 	 0.18851 	 0.25626 	 m..s
  116 	   120 	 0.18510 	 0.26223 	 m..s
==========================================
r_mrr = 0.7005808353424072
r2_mrr = 0.42825984954833984
spearmanr_mrr@5 = 0.9594455361366272
spearmanr_mrr@10 = 0.9673031568527222
spearmanr_mrr@50 = 0.9455585479736328
spearmanr_mrr@100 = 0.8009887337684631
spearmanr_mrr@All = 0.8440355658531189
==========================================
test time: 0.614
Done Testing dataset CoDExSmall
total time taken: 230.22111749649048
training time taken: 220.379061460495
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 7939117544682113
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [217, 855, 233, 256, 765, 447, 1037, 275, 276, 620, 640, 675, 1096, 401, 691, 722, 754, 1141, 1051, 880, 772, 145, 93, 1136, 290, 658, 917, 630, 1110, 146, 1039, 934, 689, 1126, 367, 926, 969, 1069, 85, 415, 462, 1142, 122, 719, 512, 711, 965, 324, 104, 112, 237, 195, 663, 509, 731, 767, 1212, 380, 713, 966, 467, 779, 537, 431, 971, 545, 204, 437, 720, 438, 956, 466, 983, 316, 1165, 29, 590, 1073, 894, 960, 959, 309, 662, 297, 653, 1124, 73, 441, 541, 449, 247, 1055, 1095, 191, 807, 674, 1034, 633, 834, 34, 420, 1076, 234, 206, 1214, 1199, 756, 829, 845, 1100, 232, 16, 686, 1001, 858, 886, 814, 626, 933, 357, 425]
valid_ids (0): []
train_ids (1094): [870, 957, 332, 1213, 666, 1115, 141, 750, 546, 391, 874, 615, 113, 348, 681, 169, 847, 744, 660, 723, 190, 1133, 248, 273, 1102, 1166, 948, 253, 1081, 1181, 444, 579, 820, 548, 595, 1083, 883, 343, 592, 538, 598, 669, 216, 1056, 943, 564, 827, 972, 696, 843, 386, 114, 840, 241, 508, 264, 850, 761, 240, 892, 821, 193, 946, 360, 314, 902, 993, 359, 143, 27, 24, 982, 1109, 514, 6, 504, 296, 735, 510, 1200, 1030, 1091, 947, 135, 147, 1107, 261, 398, 171, 1029, 23, 1047, 1175, 453, 1108, 745, 1120, 333, 460, 501, 1103, 35, 841, 542, 924, 1122, 547, 805, 131, 138, 250, 12, 102, 995, 334, 823, 494, 417, 846, 258, 1007, 1009, 1008, 422, 281, 1044, 1066, 793, 970, 656, 382, 931, 288, 629, 1, 279, 992, 238, 635, 646, 739, 1159, 305, 1211, 543, 100, 887, 601, 1035, 428, 589, 743, 519, 1012, 223, 1157, 1087, 411, 885, 303, 539, 586, 1036, 600, 1094, 351, 710, 154, 574, 647, 746, 553, 347, 918, 106, 219, 70, 738, 904, 407, 158, 403, 523, 285, 1179, 624, 955, 607, 1038, 860, 618, 1135, 1052, 912, 962, 440, 783, 818, 188, 413, 1152, 869, 1114, 1154, 94, 628, 19, 163, 676, 1129, 617, 211, 134, 162, 1049, 474, 976, 636, 725, 139, 0, 242, 377, 341, 1143, 721, 176, 389, 803, 121, 550, 436, 1150, 571, 133, 412, 639, 62, 175, 654, 697, 1188, 1067, 84, 220, 1192, 529, 762, 96, 310, 130, 1137, 554, 490, 1176, 1074, 796, 520, 1170, 435, 246, 637, 954, 363, 419, 185, 205, 737, 632, 372, 1182, 717, 1060, 930, 361, 985, 832, 263, 809, 1146, 1050, 464, 940, 362, 1209, 1167, 459, 115, 1169, 945, 173, 74, 319, 67, 648, 395, 790, 1015, 344, 1162, 891, 485, 254, 1097, 489, 1042, 667, 393, 371, 914, 14, 493, 906, 758, 222, 312, 784, 25, 1032, 741, 838, 328, 424, 968, 798, 1197, 578, 998, 495, 157, 612, 879, 320, 819, 480, 1006, 218, 552, 919, 699, 517, 476, 161, 1040, 944, 751, 602, 534, 788, 975, 1145, 349, 680, 565, 8, 203, 32, 1180, 1138, 340, 597, 842, 142, 17, 208, 409, 764, 549, 535, 69, 257, 272, 352, 268, 473, 786, 716, 5, 198, 625, 777, 921, 148, 1084, 861, 339, 346, 862, 483, 81, 949, 265, 186, 399, 575, 318, 189, 117, 868, 621, 937, 698, 321, 1045, 43, 997, 544, 336, 1104, 749, 503, 1207, 307, 655, 661, 325, 563, 867, 83, 68, 785, 373, 151, 323, 350, 540, 82, 243, 1206, 1003, 518, 48, 77, 354, 292, 181, 406, 1088, 651, 472, 236, 950, 1183, 752, 524, 172, 329, 127, 729, 753, 266, 837, 463, 470, 1160, 613, 1031, 38, 769, 967, 802, 72, 889, 442, 980, 614, 53, 446, 1054, 913, 604, 155, 649, 1119, 1057, 212, 1064, 863, 1189, 616, 136, 402, 728, 747, 706, 566, 327, 262, 877, 202, 168, 384, 1061, 366, 1093, 907, 991, 1195, 668, 1128, 488, 815, 1011, 734, 1023, 817, 408, 455, 694, 313, 915, 928, 1098, 252, 922, 562, 194, 300, 810, 789, 561, 294, 47, 1024, 787, 140, 811, 491, 1111, 559, 671, 376, 828, 245, 797, 650, 499, 461, 677, 866, 782, 199, 851, 123, 457, 1078, 610, 1046, 576, 989, 98, 707, 1193, 603, 228, 469, 652, 804, 505, 197, 718, 1149, 1071, 445, 683, 63, 1210, 153, 1041, 701, 1184, 1168, 996, 942, 1065, 911, 414, 558, 430, 80, 315, 700, 1112, 129, 831, 726, 1090, 201, 1043, 326, 283, 452, 355, 702, 584, 99, 835, 964, 583, 778, 308, 1163, 51, 688, 196, 1132, 854, 1144, 118, 670, 330, 800, 1021, 78, 56, 594, 267, 511, 378, 383, 174, 665, 591, 977, 1186, 277, 59, 1058, 105, 156, 311, 875, 1020, 42, 760, 410, 54, 251, 231, 712, 1164, 306, 1191, 1068, 335, 580, 1082, 528, 76, 732, 21, 293, 882, 773, 836, 641, 183, 214, 434, 97, 484, 95, 150, 200, 896, 923, 1022, 65, 57, 577, 905, 45, 433, 878, 496, 890, 953, 759, 31, 60, 556, 768, 79, 342, 92, 1201, 659, 605, 90, 1148, 481, 259, 1118, 269, 400, 299, 37, 727, 780, 421, 672, 572, 1053, 755, 1086, 46, 30, 1158, 492, 551, 274, 184, 774, 187, 555, 1077, 864, 15, 26, 36, 249, 479, 515, 375, 111, 695, 390, 908, 179, 581, 984, 766, 587, 1178, 465, 392, 872, 536, 1204, 533, 833, 66, 678, 244, 405, 448, 317, 3, 478, 994, 730, 939, 852, 456, 1025, 1113, 839, 423, 1147, 159, 132, 903, 282, 1048, 925, 13, 1177, 693, 963, 39, 137, 987, 52, 812, 374, 568, 429, 1187, 387, 1208, 298, 644, 951, 859, 596, 608, 876, 881, 144, 213, 426, 830, 981, 952, 1026, 11, 623, 813, 160, 1134, 9, 271, 638, 1194, 560, 385, 125, 475, 1017, 128, 1027, 192, 941, 973, 439, 1174, 215, 149, 531, 1010, 657, 1079, 126, 525, 826, 1196, 55, 497, 388, 301, 1072, 418, 1205, 679, 381, 1028, 416, 103, 61, 532, 748, 255, 848, 853, 687, 709, 642, 1203, 808, 20, 1014, 107, 432, 41, 740, 775, 229, 1019, 1127, 365, 856, 582, 781, 1105, 44, 498, 1059, 235, 631, 89, 379, 715, 221, 888, 1002, 569, 284, 487, 353, 799, 209, 794, 801, 978, 295, 606, 593, 230, 108, 337, 116, 207, 356, 120, 506, 1153, 101, 645, 958, 736, 1130, 500, 1013, 705, 88, 1116, 109, 527, 18, 1106, 124, 521, 322, 338, 304, 1016, 451, 87, 792, 450, 1198, 643, 49, 684, 516, 7, 901, 166, 673, 152, 1092, 824, 1202, 110, 929, 28, 369, 961, 742, 1151, 884, 302, 526, 1117, 227, 522, 585, 893, 471, 622, 331, 898, 1173, 427, 692, 895, 1121, 260, 397, 619, 557, 865, 1101, 170, 86, 776, 58, 177, 627, 1123, 224, 708, 4, 979, 770, 816, 1070, 900, 71, 443, 10, 871, 988, 482, 910, 849, 1155, 920, 690, 1063, 507, 573, 370, 567, 795, 822, 724, 1080, 477, 986, 599, 2, 935, 873, 1161, 1139, 1075, 33, 178, 345, 486, 1171, 167, 1085, 791, 1033, 180, 404, 1140, 91, 1125, 1190, 936, 75, 806, 825, 226, 771, 502, 733, 844, 165, 897, 899, 703, 1089, 368, 364, 999, 685, 990, 280, 182, 291, 932, 1018, 396, 1004, 164, 974, 916, 1131, 664, 704, 1172, 609, 1005, 530, 289, 278, 714, 458, 927, 468, 287, 394, 938, 22, 611, 1156, 1062, 358, 857, 513, 634, 1000, 239, 909, 588, 454, 570, 64, 270, 50, 286, 1185, 763, 757, 210, 119, 225, 1099, 682, 40]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1561269977600664
the save name prefix for this run is:  chkpt-ID_1561269977600664_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 124
rank avg (pred): 0.491 +- 0.003
mrr vals (pred, true): 0.000, 0.017
batch losses (mrrl, rdl): 0.0, 0.0013268352

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 477
rank avg (pred): 0.350 +- 0.237
mrr vals (pred, true): 0.172, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001208099

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.077 +- 0.066
mrr vals (pred, true): 0.354, 0.185
batch losses (mrrl, rdl): 0.0, 4.6112e-05

Epoch over!
epoch time: 14.613

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.305 +- 0.264
mrr vals (pred, true): 0.331, 0.001
batch losses (mrrl, rdl): 0.0, 0.0004222773

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 677
rank avg (pred): 0.354 +- 0.307
mrr vals (pred, true): 0.343, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001007083

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 890
rank avg (pred): 0.315 +- 0.274
mrr vals (pred, true): 0.349, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004709405

Epoch over!
epoch time: 12.812

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 119
rank avg (pred): 0.333 +- 0.290
mrr vals (pred, true): 0.346, 0.087
batch losses (mrrl, rdl): 0.0, 0.0005420687

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.100 +- 0.089
mrr vals (pred, true): 0.393, 0.129
batch losses (mrrl, rdl): 0.0, 2.63746e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 68
rank avg (pred): 0.086 +- 0.075
mrr vals (pred, true): 0.392, 0.101
batch losses (mrrl, rdl): 0.0, 2.08023e-05

Epoch over!
epoch time: 14.106

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 349
rank avg (pred): 0.313 +- 0.272
mrr vals (pred, true): 0.359, 0.043
batch losses (mrrl, rdl): 0.0, 0.0003826119

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 246
rank avg (pred): 0.148 +- 0.127
mrr vals (pred, true): 0.385, 0.102
batch losses (mrrl, rdl): 0.0, 3.80598e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 490
rank avg (pred): 0.263 +- 0.226
mrr vals (pred, true): 0.380, 0.059
batch losses (mrrl, rdl): 0.0, 6.35152e-05

Epoch over!
epoch time: 15.257

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1197
rank avg (pred): 0.340 +- 0.291
mrr vals (pred, true): 0.362, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001023397

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 907
rank avg (pred): 0.111 +- 0.095
mrr vals (pred, true): 0.402, 0.130
batch losses (mrrl, rdl): 0.0, 3.70937e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 838
rank avg (pred): 0.308 +- 0.265
mrr vals (pred, true): 0.386, 0.168
batch losses (mrrl, rdl): 0.0, 0.0002801227

Epoch over!
epoch time: 12.848

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 818
rank avg (pred): 0.088 +- 0.076
mrr vals (pred, true): 0.393, 0.097
batch losses (mrrl, rdl): 1.1775232553, 5.44789e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.030 +- 0.012
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0035401515, 0.0039828685

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 472
rank avg (pred): 0.480 +- 0.250
mrr vals (pred, true): 0.056, 0.002
batch losses (mrrl, rdl): 0.0003678233, 2.35011e-05

Epoch over!
epoch time: 14.811

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.050 +- 0.029
mrr vals (pred, true): 0.114, 0.000
batch losses (mrrl, rdl): 0.0414207801, 0.0037963071

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 487
rank avg (pred): 0.429 +- 0.242
mrr vals (pred, true): 0.053, 0.057
batch losses (mrrl, rdl): 9.19494e-05, 0.0006575478

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 522
rank avg (pred): 0.487 +- 0.232
mrr vals (pred, true): 0.052, 0.038
batch losses (mrrl, rdl): 3.14482e-05, 0.000971632

Epoch over!
epoch time: 13.659

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 854
rank avg (pred): 0.018 +- 0.012
mrr vals (pred, true): 0.134, 0.162
batch losses (mrrl, rdl): 0.0081743374, 0.0009742966

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 203
rank avg (pred): 0.464 +- 0.236
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0004184912, 1.49533e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.182, 0.217
batch losses (mrrl, rdl): 0.0126501154, 0.0001713033

Epoch over!
epoch time: 12.676

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 239
rank avg (pred): 0.434 +- 0.254
mrr vals (pred, true): 0.060, 0.002
batch losses (mrrl, rdl): 0.0010912983, 2.1714e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.072 +- 0.048
mrr vals (pred, true): 0.088, 0.136
batch losses (mrrl, rdl): 0.0231766887, 1.87793e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 236
rank avg (pred): 0.418 +- 0.249
mrr vals (pred, true): 0.074, 0.001
batch losses (mrrl, rdl): 0.0056805182, 2.87426e-05

Epoch over!
epoch time: 12.887

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 188
rank avg (pred): 0.237 +- 0.153
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0219948981, 0.0009054167

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 612
rank avg (pred): 0.504 +- 0.191
mrr vals (pred, true): 0.043, 0.002
batch losses (mrrl, rdl): 0.0005576557, 0.0007504692

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 529
rank avg (pred): 0.459 +- 0.200
mrr vals (pred, true): 0.044, 0.040
batch losses (mrrl, rdl): 0.0003379611, 0.0007675621

Epoch over!
epoch time: 13.509

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 353
rank avg (pred): 0.429 +- 0.223
mrr vals (pred, true): 0.056, 0.109
batch losses (mrrl, rdl): 0.0275000036, 0.0011073598

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1173
rank avg (pred): 0.443 +- 0.220
mrr vals (pred, true): 0.058, 0.069
batch losses (mrrl, rdl): 0.0006768944, 0.0004315699

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 785
rank avg (pred): 0.217 +- 0.135
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0107963467, 0.0016233802

Epoch over!
epoch time: 15.047

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 577
rank avg (pred): 0.442 +- 0.202
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.0001749071, 0.0003421235

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1205
rank avg (pred): 0.418 +- 0.211
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0008278223, 8.35786e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 728
rank avg (pred): 0.462 +- 0.180
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002039653, 4.81534e-05

Epoch over!
epoch time: 12.718

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.400 +- 0.216
mrr vals (pred, true): 0.054, 0.116
batch losses (mrrl, rdl): 0.0386759229, 0.0009991778

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 875
rank avg (pred): 0.188 +- 0.117
mrr vals (pred, true): 0.110, 0.002
batch losses (mrrl, rdl): 0.0357581303, 0.0017146417

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.407 +- 0.195
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001806368, 0.0001271188

Epoch over!
epoch time: 13.162

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 427
rank avg (pred): 0.363 +- 0.221
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0018701318, 0.0001719269

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1173
rank avg (pred): 0.404 +- 0.201
mrr vals (pred, true): 0.063, 0.069
batch losses (mrrl, rdl): 0.0016694921, 0.0002515746

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.410 +- 0.184
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004515328, 0.0001133949

Epoch over!
epoch time: 13.192

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 739
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.160, 0.132
batch losses (mrrl, rdl): 0.0080106892, 0.0002005567

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 511
rank avg (pred): 0.390 +- 0.196
mrr vals (pred, true): 0.056, 0.056
batch losses (mrrl, rdl): 0.0004125173, 0.0003347588

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 536
rank avg (pred): 0.383 +- 0.192
mrr vals (pred, true): 0.045, 0.097
batch losses (mrrl, rdl): 0.000236067, 0.0004434921

Epoch over!
epoch time: 13.411

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.399 +- 0.185
mrr vals (pred, true): 0.052, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04634 	 0.00015 	 m..s
   90 	     1 	 0.09467 	 0.00016 	 m..s
   91 	     2 	 0.09880 	 0.00016 	 m..s
   36 	     3 	 0.05367 	 0.00018 	 m..s
    8 	     4 	 0.04718 	 0.00019 	 m..s
   88 	     5 	 0.09414 	 0.00019 	 m..s
   94 	     6 	 0.10036 	 0.00020 	 MISS
   48 	     7 	 0.05538 	 0.00020 	 m..s
   85 	     8 	 0.09274 	 0.00021 	 m..s
   97 	     9 	 0.10168 	 0.00021 	 MISS
   16 	    10 	 0.04970 	 0.00021 	 m..s
    9 	    11 	 0.04722 	 0.00022 	 m..s
  110 	    12 	 0.12870 	 0.00022 	 MISS
   15 	    13 	 0.04962 	 0.00022 	 m..s
   79 	    14 	 0.09049 	 0.00023 	 m..s
   67 	    15 	 0.07250 	 0.00024 	 m..s
  101 	    16 	 0.11000 	 0.00024 	 MISS
   42 	    17 	 0.05429 	 0.00024 	 m..s
   46 	    18 	 0.05526 	 0.00025 	 m..s
   40 	    19 	 0.05415 	 0.00025 	 m..s
   33 	    20 	 0.05359 	 0.00026 	 m..s
  104 	    21 	 0.11646 	 0.00026 	 MISS
   21 	    22 	 0.05070 	 0.00027 	 m..s
   29 	    23 	 0.05278 	 0.00028 	 m..s
   13 	    24 	 0.04782 	 0.00028 	 m..s
   10 	    25 	 0.04774 	 0.00028 	 m..s
  103 	    26 	 0.11183 	 0.00029 	 MISS
   37 	    27 	 0.05395 	 0.00030 	 m..s
   47 	    28 	 0.05536 	 0.00030 	 m..s
   35 	    29 	 0.05361 	 0.00031 	 m..s
   89 	    30 	 0.09424 	 0.00031 	 m..s
   19 	    31 	 0.05018 	 0.00032 	 m..s
    6 	    32 	 0.04701 	 0.00035 	 m..s
    5 	    33 	 0.04694 	 0.00036 	 m..s
  111 	    34 	 0.13015 	 0.00036 	 MISS
   25 	    35 	 0.05163 	 0.00039 	 m..s
   56 	    36 	 0.05636 	 0.00039 	 m..s
   45 	    37 	 0.05525 	 0.00040 	 m..s
   34 	    38 	 0.05360 	 0.00040 	 m..s
   20 	    39 	 0.05057 	 0.00041 	 m..s
   22 	    40 	 0.05150 	 0.00044 	 m..s
   43 	    41 	 0.05481 	 0.00045 	 m..s
    4 	    42 	 0.04690 	 0.00045 	 m..s
    2 	    43 	 0.04649 	 0.00047 	 m..s
   39 	    44 	 0.05409 	 0.00050 	 m..s
   80 	    45 	 0.09057 	 0.00058 	 m..s
   83 	    46 	 0.09131 	 0.00058 	 m..s
   50 	    47 	 0.05552 	 0.00061 	 m..s
   71 	    48 	 0.07457 	 0.00061 	 m..s
   57 	    49 	 0.05675 	 0.00062 	 m..s
   86 	    50 	 0.09375 	 0.00067 	 m..s
   54 	    51 	 0.05618 	 0.00074 	 m..s
  107 	    52 	 0.12123 	 0.00083 	 MISS
   49 	    53 	 0.05548 	 0.00089 	 m..s
   27 	    54 	 0.05250 	 0.00171 	 m..s
   12 	    55 	 0.04776 	 0.00195 	 m..s
   11 	    56 	 0.04776 	 0.00237 	 m..s
   41 	    57 	 0.05416 	 0.00834 	 m..s
    0 	    58 	 0.04560 	 0.01397 	 m..s
   23 	    59 	 0.05151 	 0.01837 	 m..s
   44 	    60 	 0.05507 	 0.02069 	 m..s
    7 	    61 	 0.04710 	 0.02078 	 ~...
   52 	    62 	 0.05586 	 0.02693 	 ~...
   24 	    63 	 0.05162 	 0.02804 	 ~...
   18 	    64 	 0.05009 	 0.03392 	 ~...
    3 	    65 	 0.04668 	 0.03464 	 ~...
   51 	    66 	 0.05552 	 0.03716 	 ~...
   17 	    67 	 0.04998 	 0.04655 	 ~...
   62 	    68 	 0.06789 	 0.04998 	 ~...
   61 	    69 	 0.06775 	 0.05661 	 ~...
   28 	    70 	 0.05260 	 0.06036 	 ~...
   14 	    71 	 0.04865 	 0.06098 	 ~...
   65 	    72 	 0.06983 	 0.06330 	 ~...
   26 	    73 	 0.05233 	 0.06900 	 ~...
   64 	    74 	 0.06958 	 0.06977 	 ~...
   60 	    75 	 0.06487 	 0.07300 	 ~...
   31 	    76 	 0.05350 	 0.07838 	 ~...
   55 	    77 	 0.05621 	 0.08155 	 ~...
   93 	    78 	 0.09942 	 0.08483 	 ~...
   69 	    79 	 0.07350 	 0.08491 	 ~...
   30 	    80 	 0.05296 	 0.08933 	 m..s
   70 	    81 	 0.07378 	 0.09016 	 ~...
   32 	    82 	 0.05355 	 0.09317 	 m..s
   84 	    83 	 0.09248 	 0.09745 	 ~...
   53 	    84 	 0.05606 	 0.09751 	 m..s
   63 	    85 	 0.06881 	 0.10138 	 m..s
   59 	    86 	 0.05973 	 0.10319 	 m..s
   38 	    87 	 0.05405 	 0.10673 	 m..s
   74 	    88 	 0.07935 	 0.10806 	 ~...
   75 	    89 	 0.08036 	 0.10829 	 ~...
   87 	    90 	 0.09394 	 0.10915 	 ~...
   73 	    91 	 0.07737 	 0.11003 	 m..s
   77 	    92 	 0.08899 	 0.12322 	 m..s
   76 	    93 	 0.08802 	 0.12444 	 m..s
   66 	    94 	 0.07172 	 0.12816 	 m..s
   58 	    95 	 0.05944 	 0.12885 	 m..s
   68 	    96 	 0.07257 	 0.14154 	 m..s
  113 	    97 	 0.16394 	 0.14189 	 ~...
   72 	    98 	 0.07706 	 0.14257 	 m..s
   81 	    99 	 0.09065 	 0.14422 	 m..s
   82 	   100 	 0.09086 	 0.14636 	 m..s
  105 	   101 	 0.11798 	 0.15098 	 m..s
   96 	   102 	 0.10164 	 0.15277 	 m..s
   92 	   103 	 0.09900 	 0.15411 	 m..s
  100 	   104 	 0.10265 	 0.16078 	 m..s
  106 	   105 	 0.12064 	 0.16282 	 m..s
  102 	   106 	 0.11150 	 0.16364 	 m..s
   99 	   107 	 0.10238 	 0.16627 	 m..s
   78 	   108 	 0.08949 	 0.17079 	 m..s
   95 	   109 	 0.10161 	 0.17132 	 m..s
   98 	   110 	 0.10228 	 0.17251 	 m..s
  109 	   111 	 0.12360 	 0.19371 	 m..s
  112 	   112 	 0.16110 	 0.20156 	 m..s
  117 	   113 	 0.22619 	 0.20920 	 ~...
  108 	   114 	 0.12321 	 0.21796 	 m..s
  116 	   115 	 0.22058 	 0.22383 	 ~...
  115 	   116 	 0.20205 	 0.22896 	 ~...
  114 	   117 	 0.19953 	 0.24545 	 m..s
  119 	   118 	 0.24195 	 0.25704 	 ~...
  118 	   119 	 0.22922 	 0.27102 	 m..s
  120 	   120 	 0.24743 	 0.30536 	 m..s
==========================================
r_mrr = 0.7203992009162903
r2_mrr = 0.4343562126159668
spearmanr_mrr@5 = 0.9614385962486267
spearmanr_mrr@10 = 0.8856658935546875
spearmanr_mrr@50 = 0.9414094090461731
spearmanr_mrr@100 = 0.9439330697059631
spearmanr_mrr@All = 0.9509766697883606
==========================================
test time: 0.5
Done Testing dataset DBpedia50
total time taken: 210.9320170879364
training time taken: 205.2854084968567
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 9648260179318608
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [256, 342, 294, 785, 881, 37, 637, 616, 430, 644, 182, 1120, 1108, 309, 1078, 336, 764, 676, 333, 349, 878, 316, 1043, 613, 949, 494, 400, 908, 150, 1056, 86, 912, 445, 966, 1168, 1018, 391, 310, 573, 847, 850, 468, 1027, 280, 1156, 653, 264, 816, 36, 326, 241, 433, 1014, 94, 950, 1214, 49, 985, 904, 725, 906, 515, 188, 1034, 499, 782, 504, 215, 772, 1064, 399, 22, 1097, 105, 810, 74, 665, 187, 190, 2, 582, 893, 814, 419, 939, 781, 550, 386, 868, 12, 962, 943, 214, 638, 829, 1066, 734, 864, 640, 561, 347, 1204, 965, 1000, 558, 983, 989, 129, 45, 208, 1158, 461, 1181, 451, 397, 262, 340, 223, 954, 120, 917]
valid_ids (0): []
train_ids (1094): [857, 650, 719, 923, 1058, 625, 783, 945, 351, 125, 1136, 25, 136, 516, 395, 691, 1118, 102, 1001, 594, 1209, 549, 1103, 147, 258, 581, 1199, 5, 1153, 35, 1009, 1207, 1096, 563, 952, 1161, 465, 463, 1134, 460, 514, 675, 371, 958, 293, 658, 385, 1157, 849, 790, 259, 799, 177, 79, 875, 517, 405, 976, 897, 696, 1200, 843, 126, 513, 181, 348, 570, 163, 388, 90, 235, 431, 830, 502, 1184, 584, 471, 910, 10, 553, 443, 1203, 862, 278, 702, 576, 731, 46, 1174, 84, 577, 376, 357, 931, 598, 145, 140, 1149, 413, 325, 1054, 915, 886, 1030, 447, 331, 678, 263, 444, 104, 52, 751, 792, 905, 663, 851, 381, 979, 526, 436, 1042, 439, 353, 291, 987, 1127, 17, 760, 980, 870, 1115, 483, 184, 1109, 416, 191, 272, 1023, 143, 818, 590, 968, 685, 379, 592, 523, 64, 845, 1143, 464, 510, 162, 547, 605, 901, 610, 42, 667, 1082, 168, 527, 362, 1186, 883, 528, 882, 710, 1048, 865, 884, 1090, 559, 1057, 377, 47, 647, 787, 78, 423, 1104, 179, 932, 578, 437, 543, 1016, 1035, 642, 1210, 257, 282, 890, 560, 902, 833, 396, 960, 273, 1017, 213, 947, 113, 18, 1179, 40, 364, 251, 662, 601, 101, 686, 648, 1020, 791, 564, 1197, 995, 707, 606, 836, 290, 1132, 859, 59, 1019, 1125, 977, 1061, 72, 828, 621, 587, 343, 1065, 300, 1085, 169, 874, 307, 769, 1113, 612, 71, 363, 654, 729, 61, 1122, 321, 212, 796, 283, 478, 82, 1005, 914, 271, 1183, 426, 111, 160, 715, 540, 754, 242, 1053, 860, 572, 450, 319, 721, 335, 304, 723, 387, 34, 519, 909, 733, 53, 265, 172, 422, 903, 132, 110, 414, 420, 951, 732, 1089, 824, 207, 1013, 529, 334, 750, 858, 997, 763, 596, 1160, 604, 938, 926, 580, 1032, 359, 531, 407, 56, 1111, 755, 503, 619, 77, 557, 848, 689, 512, 1102, 372, 724, 925, 1133, 13, 934, 1198, 907, 63, 861, 520, 548, 1169, 1110, 497, 737, 27, 509, 1081, 1114, 623, 652, 155, 631, 19, 330, 122, 314, 770, 1041, 81, 591, 834, 268, 953, 891, 1182, 133, 154, 449, 482, 1140, 922, 1116, 714, 575, 244, 198, 1010, 350, 673, 773, 608, 226, 195, 96, 706, 567, 446, 490, 511, 927, 885, 602, 1155, 275, 609, 894, 67, 73, 1193, 31, 197, 793, 963, 1202, 305, 853, 969, 481, 562, 76, 116, 837, 91, 1059, 742, 127, 417, 759, 368, 1038, 972, 554, 1173, 831, 852, 970, 356, 641, 473, 588, 425, 1093, 301, 1080, 38, 1025, 981, 1172, 541, 660, 1029, 877, 1105, 856, 1098, 217, 332, 778, 1100, 599, 495, 286, 297, 645, 1063, 7, 83, 618, 863, 937, 924, 1159, 919, 33, 827, 178, 205, 338, 216, 752, 615, 85, 539, 80, 532, 930, 477, 717, 21, 1185, 185, 462, 1092, 60, 803, 720, 672, 303, 626, 1175, 284, 109, 99, 780, 323, 1086, 398, 392, 786, 175, 346, 159, 961, 703, 586, 1037, 889, 209, 700, 839, 1189, 1021, 239, 669, 1124, 199, 248, 1040, 260, 747, 1167, 967, 1206, 739, 921, 899, 681, 176, 88, 620, 518, 245, 229, 118, 448, 164, 276, 643, 123, 51, 804, 730, 1144, 629, 1045, 138, 311, 1046, 639, 234, 811, 459, 896, 928, 825, 93, 249, 538, 753, 946, 43, 1131, 821, 1074, 95, 378, 551, 100, 141, 1015, 1117, 892, 498, 777, 535, 170, 880, 156, 112, 670, 20, 406, 920, 1135, 354, 1026, 992, 666, 222, 250, 26, 867, 728, 767, 708, 32, 148, 210, 956, 1073, 1212, 1055, 589, 131, 299, 408, 466, 795, 1107, 75, 651, 668, 194, 1084, 888, 775, 193, 579, 410, 913, 281, 69, 1163, 534, 617, 383, 274, 1091, 738, 54, 990, 1130, 1094, 634, 1024, 798, 30, 722, 48, 1075, 955, 911, 237, 55, 988, 1060, 530, 761, 552, 1004, 611, 964, 743, 657, 716, 664, 1147, 1171, 114, 491, 487, 756, 569, 1049, 871, 165, 1142, 533, 595, 522, 823, 900, 467, 687, 202, 1069, 228, 1011, 66, 603, 1076, 14, 784, 1067, 434, 693, 712, 704, 339, 556, 480, 699, 566, 1083, 1195, 246, 774, 986, 412, 374, 11, 916, 475, 600, 324, 835, 65, 684, 1201, 971, 402, 959, 500, 139, 382, 656, 1148, 384, 158, 812, 735, 415, 1154, 635, 1192, 501, 671, 855, 1176, 525, 1178, 994, 367, 121, 933, 749, 16, 805, 1051, 456, 380, 876, 255, 24, 661, 6, 745, 701, 1187, 130, 794, 269, 973, 329, 211, 746, 627, 1146, 457, 1180, 758, 674, 289, 369, 762, 574, 679, 630, 1208, 768, 128, 683, 1039, 1028, 9, 1126, 157, 295, 1141, 1121, 840, 524, 1031, 203, 935, 108, 243, 1044, 233, 292, 1079, 124, 427, 469, 236, 225, 998, 1165, 313, 1101, 887, 390, 315, 173, 8, 1, 705, 555, 328, 872, 1128, 841, 993, 942, 152, 428, 948, 394, 607, 44, 624, 3, 403, 401, 1003, 779, 842, 438, 545, 161, 221, 1205, 58, 489, 1008, 736, 655, 373, 219, 895, 404, 1006, 748, 1022, 151, 320, 247, 832, 393, 149, 117, 485, 429, 646, 200, 50, 298, 727, 39, 941, 166, 1139, 238, 869, 991, 296, 713, 789, 484, 936, 375, 288, 788, 218, 424, 680, 695, 593, 492, 317, 999, 632, 1213, 496, 366, 365, 41, 153, 822, 361, 1072, 726, 370, 231, 940, 508, 254, 62, 411, 106, 201, 192, 435, 694, 1070, 622, 698, 806, 421, 227, 279, 224, 1152, 776, 659, 565, 455, 807, 1190, 677, 232, 1099, 29, 1150, 302, 134, 312, 189, 28, 813, 771, 1106, 493, 978, 809, 546, 802, 355, 929, 252, 975, 797, 135, 360, 285, 1164, 476, 697, 144, 119, 89, 984, 1062, 472, 453, 741, 454, 568, 996, 103, 571, 918, 196, 167, 1211, 597, 479, 341, 506, 441, 765, 1052, 261, 137, 800, 146, 536, 866, 23, 186, 352, 389, 452, 633, 718, 1047, 766, 614, 521, 1138, 230, 1137, 544, 344, 220, 1123, 507, 1166, 879, 1071, 474, 740, 682, 838, 1033, 4, 345, 171, 1077, 1162, 174, 418, 583, 690, 898, 97, 488, 505, 1007, 70, 327, 92, 253, 358, 0, 711, 1170, 409, 826, 308, 266, 1129, 1068, 318, 944, 1088, 974, 57, 1012, 815, 628, 1177, 306, 982, 432, 844, 636, 87, 277, 442, 846, 458, 270, 287, 1087, 1145, 692, 470, 142, 107, 98, 649, 240, 542, 180, 801, 1036, 1151, 585, 1188, 1119, 808, 537, 957, 204, 1191, 183, 337, 1194, 1050, 1112, 322, 1002, 15, 709, 1196, 1095, 819, 440, 688, 115, 757, 486, 267, 820, 206, 873, 68, 854, 744, 817]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2110502777337082
the save name prefix for this run is:  chkpt-ID_2110502777337082_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 296
rank avg (pred): 0.478 +- 0.004
mrr vals (pred, true): 0.020, 0.284
batch losses (mrrl, rdl): 0.0, 0.002738724

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 905
rank avg (pred): 0.230 +- 0.019
mrr vals (pred, true): 0.041, 0.197
batch losses (mrrl, rdl): 0.0, 6.82635e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 117
rank avg (pred): 0.437 +- 0.260
mrr vals (pred, true): 0.138, 0.049
batch losses (mrrl, rdl): 0.0, 1.18114e-05

Epoch over!
epoch time: 13.418

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 267
rank avg (pred): 0.114 +- 0.090
mrr vals (pred, true): 0.244, 0.303
batch losses (mrrl, rdl): 0.0, 1.54773e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 663
rank avg (pred): 0.466 +- 0.267
mrr vals (pred, true): 0.093, 0.045
batch losses (mrrl, rdl): 0.0, 2.7247e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 402
rank avg (pred): 0.400 +- 0.271
mrr vals (pred, true): 0.130, 0.053
batch losses (mrrl, rdl): 0.0, 4.2434e-06

Epoch over!
epoch time: 12.787

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 181
rank avg (pred): 0.451 +- 0.265
mrr vals (pred, true): 0.083, 0.046
batch losses (mrrl, rdl): 0.0, 1.7576e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 15
rank avg (pred): 0.093 +- 0.125
mrr vals (pred, true): 0.265, 0.297
batch losses (mrrl, rdl): 0.0, 3.26e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 431
rank avg (pred): 0.433 +- 0.270
mrr vals (pred, true): 0.087, 0.041
batch losses (mrrl, rdl): 0.0, 2.57202e-05

Epoch over!
epoch time: 14.155

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1049
rank avg (pred): 0.445 +- 0.263
mrr vals (pred, true): 0.071, 0.042
batch losses (mrrl, rdl): 0.0, 1.45867e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1059
rank avg (pred): 0.079 +- 0.120
mrr vals (pred, true): 0.275, 0.341
batch losses (mrrl, rdl): 0.0, 5.5448e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1031
rank avg (pred): 0.453 +- 0.264
mrr vals (pred, true): 0.067, 0.047
batch losses (mrrl, rdl): 0.0, 3.646e-07

Epoch over!
epoch time: 13.532

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 795
rank avg (pred): 0.447 +- 0.272
mrr vals (pred, true): 0.079, 0.045
batch losses (mrrl, rdl): 0.0, 3.5614e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1178
rank avg (pred): 0.450 +- 0.275
mrr vals (pred, true): 0.077, 0.046
batch losses (mrrl, rdl): 0.0, 2.5695e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 678
rank avg (pred): 0.443 +- 0.273
mrr vals (pred, true): 0.081, 0.045
batch losses (mrrl, rdl): 0.0, 5.6513e-06

Epoch over!
epoch time: 14.398

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 751
rank avg (pred): 0.205 +- 0.205
mrr vals (pred, true): 0.146, 0.256
batch losses (mrrl, rdl): 0.1204064116, 1.82892e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 594
rank avg (pred): 0.445 +- 0.138
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.000781033, 4.59164e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 531
rank avg (pred): 0.202 +- 0.169
mrr vals (pred, true): 0.270, 0.228
batch losses (mrrl, rdl): 0.0184801947, 0.0002225394

Epoch over!
epoch time: 14.559

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1198
rank avg (pred): 0.451 +- 0.136
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0004646054, 4.12489e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1167
rank avg (pred): 0.441 +- 0.140
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 3.71268e-05, 3.74114e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 583
rank avg (pred): 0.452 +- 0.129
mrr vals (pred, true): 0.043, 0.047
batch losses (mrrl, rdl): 0.0004868304, 5.10069e-05

Epoch over!
epoch time: 13.797

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 255
rank avg (pred): 0.231 +- 0.191
mrr vals (pred, true): 0.253, 0.296
batch losses (mrrl, rdl): 0.0182556286, 0.0004682935

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 777
rank avg (pred): 0.442 +- 0.136
mrr vals (pred, true): 0.048, 0.044
batch losses (mrrl, rdl): 3.32464e-05, 4.84328e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 770
rank avg (pred): 0.442 +- 0.133
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 0.0001000989, 4.28182e-05

Epoch over!
epoch time: 13.691

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 5
rank avg (pred): 0.159 +- 0.118
mrr vals (pred, true): 0.282, 0.293
batch losses (mrrl, rdl): 0.0012207255, 5.31368e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 177
rank avg (pred): 0.438 +- 0.135
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 4.9535e-06, 5.95873e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 775
rank avg (pred): 0.425 +- 0.144
mrr vals (pred, true): 0.056, 0.046
batch losses (mrrl, rdl): 0.0003725736, 8.02852e-05

Epoch over!
epoch time: 14.344

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 576
rank avg (pred): 0.434 +- 0.139
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 1.22684e-05, 5.96787e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1091
rank avg (pred): 0.433 +- 0.142
mrr vals (pred, true): 0.056, 0.052
batch losses (mrrl, rdl): 0.0003623195, 4.22803e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 186
rank avg (pred): 0.445 +- 0.125
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0005439296, 5.33514e-05

Epoch over!
epoch time: 13.562

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 968
rank avg (pred): 0.431 +- 0.143
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.0001377985, 7.79167e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 15
rank avg (pred): 0.151 +- 0.102
mrr vals (pred, true): 0.285, 0.297
batch losses (mrrl, rdl): 0.0014846133, 7.62363e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 229
rank avg (pred): 0.439 +- 0.129
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 9.56094e-05, 7.02607e-05

Epoch over!
epoch time: 14.469

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1029
rank avg (pred): 0.430 +- 0.138
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 2.42302e-05, 5.74246e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 446
rank avg (pred): 0.437 +- 0.131
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 1.98084e-05, 5.90992e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 752
rank avg (pred): 0.278 +- 0.169
mrr vals (pred, true): 0.210, 0.244
batch losses (mrrl, rdl): 0.0119943824, 0.0002854491

Epoch over!
epoch time: 15.455

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 412
rank avg (pred): 0.441 +- 0.127
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 9.45753e-05, 4.70492e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 260
rank avg (pred): 0.168 +- 0.109
mrr vals (pred, true): 0.283, 0.295
batch losses (mrrl, rdl): 0.001347091, 7.17156e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 172
rank avg (pred): 0.457 +- 0.109
mrr vals (pred, true): 0.040, 0.045
batch losses (mrrl, rdl): 0.0009198589, 5.85002e-05

Epoch over!
epoch time: 15.004

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 283
rank avg (pred): 0.171 +- 0.111
mrr vals (pred, true): 0.276, 0.275
batch losses (mrrl, rdl): 2.77708e-05, 0.0001235211

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 96
rank avg (pred): 0.442 +- 0.125
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 2.6e-08, 5.0965e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 110
rank avg (pred): 0.430 +- 0.134
mrr vals (pred, true): 0.055, 0.057
batch losses (mrrl, rdl): 0.0002943582, 5.66537e-05

Epoch over!
epoch time: 15.332

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 882
rank avg (pred): 0.438 +- 0.125
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 3.5556e-06, 6.1553e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 453
rank avg (pred): 0.442 +- 0.122
mrr vals (pred, true): 0.048, 0.042
batch losses (mrrl, rdl): 3.66279e-05, 7.14762e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 981
rank avg (pred): 0.154 +- 0.100
mrr vals (pred, true): 0.312, 0.315
batch losses (mrrl, rdl): 0.0001446247, 7.0763e-05

Epoch over!
epoch time: 15.207

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.188 +- 0.118
mrr vals (pred, true): 0.277, 0.295

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   66 	     0 	 0.05330 	 0.03862 	 ~...
   45 	     1 	 0.05110 	 0.03958 	 ~...
   39 	     2 	 0.05086 	 0.03969 	 ~...
   10 	     3 	 0.04875 	 0.04079 	 ~...
   30 	     4 	 0.05030 	 0.04100 	 ~...
    8 	     5 	 0.04848 	 0.04127 	 ~...
   47 	     6 	 0.05120 	 0.04189 	 ~...
   51 	     7 	 0.05168 	 0.04209 	 ~...
   34 	     8 	 0.05055 	 0.04217 	 ~...
   76 	     9 	 0.05603 	 0.04223 	 ~...
   16 	    10 	 0.04927 	 0.04225 	 ~...
   73 	    11 	 0.05457 	 0.04257 	 ~...
   63 	    12 	 0.05279 	 0.04259 	 ~...
   14 	    13 	 0.04919 	 0.04276 	 ~...
    2 	    14 	 0.04669 	 0.04284 	 ~...
   56 	    15 	 0.05211 	 0.04289 	 ~...
   52 	    16 	 0.05171 	 0.04301 	 ~...
    6 	    17 	 0.04748 	 0.04304 	 ~...
    7 	    18 	 0.04805 	 0.04320 	 ~...
   11 	    19 	 0.04883 	 0.04323 	 ~...
    5 	    20 	 0.04730 	 0.04331 	 ~...
   12 	    21 	 0.04890 	 0.04333 	 ~...
   43 	    22 	 0.05109 	 0.04362 	 ~...
   59 	    23 	 0.05250 	 0.04367 	 ~...
   32 	    24 	 0.05038 	 0.04371 	 ~...
   65 	    25 	 0.05306 	 0.04377 	 ~...
   38 	    26 	 0.05082 	 0.04393 	 ~...
   19 	    27 	 0.04943 	 0.04400 	 ~...
    3 	    28 	 0.04680 	 0.04415 	 ~...
   55 	    29 	 0.05178 	 0.04436 	 ~...
   74 	    30 	 0.05466 	 0.04438 	 ~...
   58 	    31 	 0.05238 	 0.04450 	 ~...
    0 	    32 	 0.04525 	 0.04453 	 ~...
   27 	    33 	 0.04995 	 0.04459 	 ~...
   31 	    34 	 0.05038 	 0.04460 	 ~...
   33 	    35 	 0.05051 	 0.04466 	 ~...
   23 	    36 	 0.04955 	 0.04486 	 ~...
   75 	    37 	 0.05498 	 0.04495 	 ~...
   36 	    38 	 0.05061 	 0.04503 	 ~...
   62 	    39 	 0.05274 	 0.04507 	 ~...
    1 	    40 	 0.04560 	 0.04509 	 ~...
    9 	    41 	 0.04874 	 0.04522 	 ~...
   18 	    42 	 0.04937 	 0.04523 	 ~...
   57 	    43 	 0.05232 	 0.04528 	 ~...
   41 	    44 	 0.05103 	 0.04542 	 ~...
   77 	    45 	 0.05683 	 0.04546 	 ~...
   21 	    46 	 0.04951 	 0.04549 	 ~...
   13 	    47 	 0.04892 	 0.04560 	 ~...
   29 	    48 	 0.05017 	 0.04586 	 ~...
   15 	    49 	 0.04926 	 0.04639 	 ~...
   40 	    50 	 0.05098 	 0.04666 	 ~...
   72 	    51 	 0.05442 	 0.04688 	 ~...
   49 	    52 	 0.05123 	 0.04700 	 ~...
    4 	    53 	 0.04698 	 0.04726 	 ~...
   69 	    54 	 0.05396 	 0.04802 	 ~...
   50 	    55 	 0.05153 	 0.04874 	 ~...
   67 	    56 	 0.05384 	 0.04935 	 ~...
   37 	    57 	 0.05070 	 0.04946 	 ~...
   46 	    58 	 0.05119 	 0.04984 	 ~...
   22 	    59 	 0.04953 	 0.05012 	 ~...
   35 	    60 	 0.05060 	 0.05025 	 ~...
   44 	    61 	 0.05109 	 0.05072 	 ~...
   60 	    62 	 0.05267 	 0.05077 	 ~...
   25 	    63 	 0.04976 	 0.05129 	 ~...
   24 	    64 	 0.04969 	 0.05148 	 ~...
   70 	    65 	 0.05403 	 0.05189 	 ~...
   20 	    66 	 0.04950 	 0.05320 	 ~...
   48 	    67 	 0.05123 	 0.05375 	 ~...
   26 	    68 	 0.04984 	 0.05403 	 ~...
   54 	    69 	 0.05178 	 0.05432 	 ~...
   71 	    70 	 0.05419 	 0.05451 	 ~...
   61 	    71 	 0.05267 	 0.05566 	 ~...
   28 	    72 	 0.04998 	 0.05610 	 ~...
   42 	    73 	 0.05104 	 0.05630 	 ~...
   17 	    74 	 0.04928 	 0.05711 	 ~...
   64 	    75 	 0.05279 	 0.05711 	 ~...
   53 	    76 	 0.05172 	 0.05853 	 ~...
   68 	    77 	 0.05388 	 0.06201 	 ~...
   80 	    78 	 0.16953 	 0.10078 	 m..s
   78 	    79 	 0.14969 	 0.15474 	 ~...
   84 	    80 	 0.17428 	 0.17928 	 ~...
   81 	    81 	 0.17191 	 0.18570 	 ~...
   86 	    82 	 0.20283 	 0.19216 	 ~...
   91 	    83 	 0.21698 	 0.19716 	 ~...
   82 	    84 	 0.17210 	 0.19813 	 ~...
   85 	    85 	 0.19313 	 0.20170 	 ~...
   79 	    86 	 0.16499 	 0.20997 	 m..s
   93 	    87 	 0.23406 	 0.21544 	 ~...
   89 	    88 	 0.21540 	 0.21918 	 ~...
   87 	    89 	 0.21396 	 0.21950 	 ~...
   92 	    90 	 0.22659 	 0.22106 	 ~...
   83 	    91 	 0.17404 	 0.22189 	 m..s
   90 	    92 	 0.21642 	 0.22418 	 ~...
   94 	    93 	 0.23901 	 0.22737 	 ~...
   88 	    94 	 0.21494 	 0.22898 	 ~...
   95 	    95 	 0.24003 	 0.23013 	 ~...
  103 	    96 	 0.27310 	 0.24849 	 ~...
  100 	    97 	 0.27086 	 0.25490 	 ~...
   97 	    98 	 0.26464 	 0.25649 	 ~...
   98 	    99 	 0.26528 	 0.26140 	 ~...
   96 	   100 	 0.26396 	 0.26278 	 ~...
  108 	   101 	 0.27993 	 0.26867 	 ~...
  101 	   102 	 0.27110 	 0.26893 	 ~...
  104 	   103 	 0.27494 	 0.27108 	 ~...
  105 	   104 	 0.27626 	 0.27421 	 ~...
  102 	   105 	 0.27215 	 0.27473 	 ~...
  112 	   106 	 0.28451 	 0.27873 	 ~...
  110 	   107 	 0.28285 	 0.28228 	 ~...
  113 	   108 	 0.29547 	 0.28517 	 ~...
  111 	   109 	 0.28330 	 0.28573 	 ~...
  109 	   110 	 0.28031 	 0.29076 	 ~...
  106 	   111 	 0.27682 	 0.29520 	 ~...
  115 	   112 	 0.31088 	 0.29805 	 ~...
   99 	   113 	 0.26958 	 0.29913 	 ~...
  107 	   114 	 0.27982 	 0.30137 	 ~...
  118 	   115 	 0.31602 	 0.32126 	 ~...
  120 	   116 	 0.32567 	 0.32959 	 ~...
  116 	   117 	 0.31147 	 0.33674 	 ~...
  119 	   118 	 0.32273 	 0.34246 	 ~...
  117 	   119 	 0.31478 	 0.34698 	 m..s
  114 	   120 	 0.30959 	 0.36279 	 m..s
==========================================
r_mrr = 0.9920105338096619
r2_mrr = 0.9825987219810486
spearmanr_mrr@5 = 0.9502369165420532
spearmanr_mrr@10 = 0.8922846913337708
spearmanr_mrr@50 = 0.987931489944458
spearmanr_mrr@100 = 0.9961637258529663
spearmanr_mrr@All = 0.9965858459472656
==========================================
test time: 0.446
Done Testing dataset Kinships
total time taken: 221.9517946243286
training time taken: 214.23517990112305
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: all-comboes-at-once
-------------------------------------------------------
=======================================================
Using random seed: 4586819324266094
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [284, 831, 246, 71, 19, 361, 1166, 1134, 1201, 722, 816, 3, 731, 364, 81, 1138, 112, 931, 782, 296, 691, 935, 608, 69, 336, 923, 894, 992, 297, 38, 1068, 1186, 370, 51, 735, 42, 910, 405, 305, 1167, 554, 880, 646, 221, 308, 456, 355, 248, 399, 867, 198, 555, 602, 957, 786, 34, 23, 1038, 885, 961, 377, 228, 14, 624, 876, 1070, 408, 373, 897, 779, 1094, 217, 651, 604, 467, 1159, 161, 1132, 33, 1026, 850, 466, 956, 113, 421, 1183, 1149, 703, 349, 785, 881, 643, 639, 411, 896, 201, 844, 803, 443, 520, 548, 1205, 274, 930, 967, 92, 236, 1071, 149, 693, 671, 1187, 243, 969, 622, 630, 748, 613, 777, 383, 676]
valid_ids (0): []
train_ids (1094): [1088, 1192, 629, 1098, 823, 1120, 921, 534, 186, 718, 62, 524, 340, 57, 801, 889, 258, 828, 345, 18, 1154, 1007, 26, 440, 1180, 591, 1066, 389, 479, 1050, 714, 331, 721, 496, 679, 1073, 756, 1041, 561, 668, 323, 354, 487, 346, 556, 793, 747, 653, 356, 552, 610, 615, 887, 271, 214, 877, 379, 189, 917, 893, 859, 235, 905, 1001, 860, 819, 725, 128, 631, 821, 787, 1080, 759, 483, 315, 589, 448, 523, 1165, 337, 1122, 978, 713, 412, 1004, 338, 1097, 133, 1058, 266, 974, 299, 1000, 868, 277, 1016, 963, 663, 288, 376, 550, 1101, 352, 832, 1121, 773, 400, 115, 1191, 1012, 754, 446, 962, 543, 68, 451, 817, 66, 162, 290, 1190, 692, 1059, 110, 429, 1052, 792, 928, 80, 570, 966, 845, 410, 234, 511, 1175, 392, 253, 566, 30, 260, 362, 229, 951, 997, 197, 517, 879, 884, 557, 477, 150, 560, 233, 933, 1048, 1025, 1130, 372, 494, 763, 551, 1072, 1053, 344, 1092, 934, 999, 584, 125, 837, 899, 24, 929, 525, 460, 797, 160, 285, 690, 680, 869, 1086, 843, 132, 380, 1140, 141, 302, 397, 681, 450, 1013, 516, 1069, 914, 1185, 170, 1079, 158, 537, 533, 387, 851, 509, 12, 1179, 423, 901, 968, 965, 518, 1199, 28, 343, 497, 226, 848, 758, 637, 508, 324, 209, 946, 134, 1145, 638, 925, 611, 438, 120, 307, 49, 7, 1116, 757, 665, 1177, 402, 163, 1198, 15, 872, 1125, 598, 122, 1207, 375, 108, 1168, 334, 52, 585, 1163, 804, 4, 368, 526, 401, 640, 842, 127, 318, 1024, 1114, 712, 1084, 871, 328, 977, 1076, 737, 949, 674, 64, 454, 199, 168, 195, 1011, 357, 242, 781, 565, 1184, 240, 633, 687, 612, 41, 959, 1210, 601, 67, 683, 393, 43, 970, 507, 1206, 776, 701, 188, 1081, 1054, 1003, 280, 1214, 1027, 1146, 841, 16, 21, 652, 82, 22, 491, 634, 689, 648, 856, 529, 60, 157, 97, 603, 211, 891, 79, 427, 874, 780, 164, 461, 70, 101, 586, 490, 498, 220, 1049, 655, 771, 495, 519, 251, 169, 707, 1115, 1082, 826, 558, 109, 244, 219, 590, 532, 1144, 1010, 635, 366, 938, 8, 10, 1119, 784, 1062, 121, 861, 892, 1161, 1074, 697, 649, 762, 728, 350, 453, 87, 203, 594, 696, 1113, 814, 417, 979, 107, 1006, 1089, 265, 666, 911, 796, 1176, 314, 751, 138, 862, 213, 709, 184, 878, 1021, 670, 1141, 29, 732, 47, 353, 789, 93, 1103, 937, 906, 800, 152, 866, 650, 1112, 890, 514, 208, 1015, 886, 123, 472, 455, 846, 1102, 40, 326, 499, 176, 702, 644, 799, 404, 470, 27, 658, 73, 738, 282, 1008, 645, 147, 617, 912, 990, 175, 1031, 1211, 1139, 659, 1203, 319, 824, 247, 972, 291, 1106, 656, 1197, 950, 895, 434, 593, 310, 1036, 794, 898, 1075, 1160, 11, 863, 1034, 167, 119, 976, 452, 1174, 528, 694, 322, 873, 941, 486, 984, 54, 853, 301, 1104, 1057, 717, 420, 268, 818, 135, 306, 165, 1035, 657, 954, 58, 682, 245, 185, 753, 53, 791, 191, 447, 330, 231, 730, 916, 1150, 1032, 178, 812, 1117, 143, 1039, 329, 388, 945, 286, 813, 918, 685, 391, 1093, 182, 936, 283, 1123, 85, 578, 744, 750, 1044, 287, 430, 348, 426, 275, 17, 218, 742, 926, 259, 261, 1213, 1204, 215, 146, 562, 339, 173, 545, 567, 541, 815, 705, 729, 332, 1195, 662, 437, 413, 571, 765, 1, 398, 367, 710, 588, 641, 192, 210, 1143, 661, 991, 964, 445, 1051, 194, 327, 838, 269, 196, 1037, 900, 95, 960, 320, 289, 1046, 238, 549, 431, 1107, 313, 1009, 985, 432, 493, 1083, 227, 971, 428, 190, 772, 806, 124, 711, 471, 761, 769, 920, 475, 1078, 395, 875, 913, 607, 241, 358, 580, 363, 1042, 544, 1126, 409, 442, 1200, 542, 317, 1018, 174, 333, 86, 422, 205, 749, 1047, 888, 369, 577, 778, 1181, 232, 582, 940, 1189, 1171, 724, 870, 677, 419, 530, 106, 942, 72, 131, 56, 74, 745, 802, 156, 278, 1096, 783, 407, 767, 276, 988, 335, 1055, 994, 204, 118, 609, 351, 180, 1040, 298, 256, 396, 177, 148, 114, 84, 715, 136, 295, 484, 249, 482, 1148, 675, 788, 425, 1194, 883, 1085, 102, 104, 600, 1129, 130, 1155, 347, 907, 987, 144, 932, 250, 535, 489, 212, 193, 669, 621, 740, 1100, 839, 1182, 605, 441, 513, 1014, 78, 581, 309, 625, 538, 480, 596, 223, 465, 599, 378, 628, 614, 1109, 667, 45, 371, 374, 183, 1063, 569, 111, 1162, 385, 706, 834, 65, 833, 1188, 512, 61, 1212, 312, 515, 684, 126, 840, 947, 381, 501, 727, 695, 755, 25, 403, 958, 230, 574, 903, 943, 1169, 980, 116, 986, 636, 808, 1208, 996, 1135, 536, 416, 88, 1105, 647, 311, 207, 36, 733, 1118, 811, 583, 390, 83, 764, 632, 272, 592, 418, 865, 1172, 1017, 1170, 564, 944, 807, 500, 1147, 224, 1045, 449, 1124, 618, 187, 5, 13, 1164, 506, 222, 94, 382, 1196, 365, 1110, 664, 855, 206, 281, 678, 504, 904, 1023, 252, 836, 485, 1173, 492, 1091, 273, 1202, 98, 181, 716, 1002, 809, 955, 626, 522, 540, 909, 129, 768, 760, 1028, 468, 546, 46, 37, 736, 752, 76, 316, 539, 464, 708, 798, 620, 686, 444, 90, 386, 1033, 830, 1152, 59, 1178, 254, 654, 852, 154, 159, 563, 166, 827, 1151, 939, 321, 32, 975, 433, 1137, 835, 1099, 1136, 1193, 559, 1153, 502, 137, 510, 521, 1020, 153, 304, 6, 790, 117, 435, 673, 952, 202, 478, 1064, 257, 1061, 1056, 48, 527, 847, 948, 766, 1029, 1087, 384, 103, 595, 726, 998, 225, 1127, 457, 462, 0, 20, 1043, 770, 424, 704, 849, 100, 39, 96, 924, 267, 75, 415, 91, 294, 805, 660, 989, 476, 774, 915, 89, 142, 463, 829, 35, 179, 77, 406, 575, 1133, 1157, 739, 746, 1108, 1111, 488, 723, 9, 474, 151, 719, 439, 459, 579, 572, 50, 698, 503, 775, 239, 44, 973, 858, 810, 360, 2, 325, 993, 394, 642, 1128, 469, 155, 882, 436, 1158, 795, 1060, 619, 820, 741, 1019, 140, 927, 237, 1077, 700, 31, 293, 481, 1090, 1156, 587, 292, 341, 264, 983, 99, 473, 1209, 981, 262, 105, 919, 606, 505, 139, 743, 145, 922, 864, 255, 359, 300, 1095, 200, 672, 1022, 263, 627, 573, 279, 902, 568, 688, 63, 531, 982, 576, 995, 270, 547, 171, 822, 908, 953, 1065, 616, 458, 699, 1142, 342, 597, 1067, 1131, 553, 414, 734, 1005, 825, 720, 857, 172, 623, 854, 303, 1030, 216, 55]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9851026077322748
the save name prefix for this run is:  chkpt-ID_9851026077322748_tag_all-comboes-at-once
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 918
rank avg (pred): 0.598 +- 0.003
mrr vals (pred, true): 0.000, 0.097
batch losses (mrrl, rdl): 0.0, 0.0023074397

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 987
rank avg (pred): 0.067 +- 0.056
mrr vals (pred, true): 0.091, 0.287
batch losses (mrrl, rdl): 0.0, 2.64821e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1064
rank avg (pred): 0.042 +- 0.041
mrr vals (pred, true): 0.256, 0.213
batch losses (mrrl, rdl): 0.0, 7.8168e-06

Epoch over!
epoch time: 15.98

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 103
rank avg (pred): 0.330 +- 0.295
mrr vals (pred, true): 0.124, 0.023
batch losses (mrrl, rdl): 0.0, 0.0002570927

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 53
rank avg (pred): 0.072 +- 0.064
mrr vals (pred, true): 0.152, 0.270
batch losses (mrrl, rdl): 0.0, 1.66893e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 709
rank avg (pred): 0.327 +- 0.308
mrr vals (pred, true): 0.198, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003068653

Epoch over!
epoch time: 15.994

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 533
rank avg (pred): 0.165 +- 0.165
mrr vals (pred, true): 0.272, 0.092
batch losses (mrrl, rdl): 0.0, 4.07241e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 851
rank avg (pred): 0.322 +- 0.295
mrr vals (pred, true): 0.159, 0.097
batch losses (mrrl, rdl): 0.0, 0.0001338025

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 83
rank avg (pred): 0.330 +- 0.300
mrr vals (pred, true): 0.175, 0.036
batch losses (mrrl, rdl): 0.0, 0.0003967151

Epoch over!
epoch time: 15.992

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1189
rank avg (pred): 0.399 +- 0.318
mrr vals (pred, true): 0.102, 0.006
batch losses (mrrl, rdl): 0.0, 8.424e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 813
rank avg (pred): 0.052 +- 0.050
mrr vals (pred, true): 0.312, 0.070
batch losses (mrrl, rdl): 0.0, 3.4548e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 747
rank avg (pred): 0.049 +- 0.046
mrr vals (pred, true): 0.212, 0.088
batch losses (mrrl, rdl): 0.0, 2.7449e-06

Epoch over!
epoch time: 14.463

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1100
rank avg (pred): 0.308 +- 0.295
mrr vals (pred, true): 0.324, 0.075
batch losses (mrrl, rdl): 0.0, 0.0003879589

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1008
rank avg (pred): 0.316 +- 0.294
mrr vals (pred, true): 0.263, 0.052
batch losses (mrrl, rdl): 0.0, 0.0003521226

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 169
rank avg (pred): 0.339 +- 0.307
mrr vals (pred, true): 0.206, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002601594

Epoch over!
epoch time: 16.155

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 406
rank avg (pred): 0.343 +- 0.304
mrr vals (pred, true): 0.171, 0.006
batch losses (mrrl, rdl): 0.1463692784, 0.0001994652

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 119
rank avg (pred): 0.509 +- 0.183
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 1.12646e-05, 0.0020404912

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 744
rank avg (pred): 0.327 +- 0.192
mrr vals (pred, true): 0.107, 0.216
batch losses (mrrl, rdl): 0.1194497347, 0.0021502082

Epoch over!
epoch time: 15.514

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 939
rank avg (pred): 0.503 +- 0.166
mrr vals (pred, true): 0.043, 0.097
batch losses (mrrl, rdl): 0.0005189499, 0.0011584059

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 303
rank avg (pred): 0.359 +- 0.214
mrr vals (pred, true): 0.102, 0.057
batch losses (mrrl, rdl): 0.0266981311, 0.0013464398

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 333
rank avg (pred): 0.461 +- 0.155
mrr vals (pred, true): 0.042, 0.022
batch losses (mrrl, rdl): 0.0005921057, 0.0010485162

Epoch over!
epoch time: 15.319

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 877
rank avg (pred): 0.449 +- 0.164
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002041285, 7.78392e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 253
rank avg (pred): 0.153 +- 0.104
mrr vals (pred, true): 0.107, 0.180
batch losses (mrrl, rdl): 0.0535946302, 0.0001653103

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1200
rank avg (pred): 0.394 +- 0.175
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.0002024931, 0.0001747695

Epoch over!
epoch time: 14.838

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 698
rank avg (pred): 0.405 +- 0.162
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001632939, 0.0001651379

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 686
rank avg (pred): 0.389 +- 0.166
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 3.3973e-06, 0.0002385269

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.376 +- 0.163
mrr vals (pred, true): 0.048, 0.037
batch losses (mrrl, rdl): 2.38987e-05, 0.0001426516

Epoch over!
epoch time: 14.568

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 583
rank avg (pred): 0.398 +- 0.155
mrr vals (pred, true): 0.045, 0.015
batch losses (mrrl, rdl): 0.0002801282, 0.0001723988

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 713
rank avg (pred): 0.376 +- 0.156
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001508374, 0.0002912392

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1084
rank avg (pred): 0.326 +- 0.176
mrr vals (pred, true): 0.054, 0.103
batch losses (mrrl, rdl): 0.0239938907, 0.0005663929

Epoch over!
epoch time: 14.06

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1200
rank avg (pred): 0.356 +- 0.168
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.0001304749, 0.0003300918

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 390
rank avg (pred): 0.384 +- 0.146
mrr vals (pred, true): 0.045, 0.021
batch losses (mrrl, rdl): 0.0002398567, 0.0004956223

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1153
rank avg (pred): 0.284 +- 0.201
mrr vals (pred, true): 0.095, 0.106
batch losses (mrrl, rdl): 0.0011314561, 0.0004181599

Epoch over!
epoch time: 14.965

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.288 +- 0.179
mrr vals (pred, true): 0.080, 0.106
batch losses (mrrl, rdl): 0.0064726109, 0.0003352046

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 849
rank avg (pred): 0.354 +- 0.163
mrr vals (pred, true): 0.055, 0.096
batch losses (mrrl, rdl): 0.0002201161, 0.0002770663

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.344 +- 0.169
mrr vals (pred, true): 0.058, 0.007
batch losses (mrrl, rdl): 0.0005646031, 0.0003966299

Epoch over!
epoch time: 15.79

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 52
rank avg (pred): 0.121 +- 0.111
mrr vals (pred, true): 0.140, 0.187
batch losses (mrrl, rdl): 0.022214774, 9.90005e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 384
rank avg (pred): 0.378 +- 0.149
mrr vals (pred, true): 0.050, 0.019
batch losses (mrrl, rdl): 6.298e-07, 0.0003812256

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 430
rank avg (pred): 0.377 +- 0.148
mrr vals (pred, true): 0.047, 0.007
batch losses (mrrl, rdl): 8.14277e-05, 0.0002578311

Epoch over!
epoch time: 16.08

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 913
rank avg (pred): 0.148 +- 0.092
mrr vals (pred, true): 0.075, 0.095
batch losses (mrrl, rdl): 0.006257141, 0.0001661085

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 307
rank avg (pred): 0.087 +- 0.054
mrr vals (pred, true): 0.087, 0.128
batch losses (mrrl, rdl): 0.0166987237, 1.59506e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1164
rank avg (pred): 0.387 +- 0.139
mrr vals (pred, true): 0.044, 0.058
batch losses (mrrl, rdl): 0.0003753524, 0.0001965066

Epoch over!
epoch time: 15.338

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 277
rank avg (pred): 0.138 +- 0.083
mrr vals (pred, true): 0.059, 0.073
batch losses (mrrl, rdl): 0.0008456248, 4.85896e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 563
rank avg (pred): 0.129 +- 0.073
mrr vals (pred, true): 0.075, 0.092
batch losses (mrrl, rdl): 0.0060042497, 0.0001478292

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1129
rank avg (pred): 0.254 +- 0.143
mrr vals (pred, true): 0.059, 0.006
batch losses (mrrl, rdl): 0.0007402186, 0.0010859843

Epoch over!
epoch time: 15.837

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.149 +- 0.134
mrr vals (pred, true): 0.172, 0.167

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04684 	 0.00600 	 m..s
   27 	     1 	 0.04853 	 0.00603 	 m..s
   18 	     2 	 0.04758 	 0.00605 	 m..s
   12 	     3 	 0.04716 	 0.00611 	 m..s
    6 	     4 	 0.04676 	 0.00613 	 m..s
    3 	     5 	 0.04608 	 0.00616 	 m..s
   24 	     6 	 0.04785 	 0.00622 	 m..s
   72 	     7 	 0.05379 	 0.00623 	 m..s
   20 	     8 	 0.04765 	 0.00623 	 m..s
   70 	     9 	 0.05310 	 0.00625 	 m..s
   48 	    10 	 0.05004 	 0.00631 	 m..s
   13 	    11 	 0.04717 	 0.00634 	 m..s
   15 	    12 	 0.04729 	 0.00635 	 m..s
   44 	    13 	 0.04980 	 0.00636 	 m..s
   50 	    14 	 0.05036 	 0.00638 	 m..s
   66 	    15 	 0.05200 	 0.00638 	 m..s
   80 	    16 	 0.05581 	 0.00644 	 m..s
   82 	    17 	 0.05825 	 0.00645 	 m..s
   19 	    18 	 0.04759 	 0.00645 	 m..s
   79 	    19 	 0.05533 	 0.00646 	 m..s
   14 	    20 	 0.04722 	 0.00646 	 m..s
   81 	    21 	 0.05791 	 0.00647 	 m..s
   93 	    22 	 0.08290 	 0.00648 	 m..s
   57 	    23 	 0.05086 	 0.00649 	 m..s
   42 	    24 	 0.04931 	 0.00652 	 m..s
   63 	    25 	 0.05165 	 0.00652 	 m..s
   26 	    26 	 0.04819 	 0.00653 	 m..s
   75 	    27 	 0.05477 	 0.00653 	 m..s
   34 	    28 	 0.04882 	 0.00655 	 m..s
   55 	    29 	 0.05081 	 0.00656 	 m..s
   16 	    30 	 0.04729 	 0.00657 	 m..s
    2 	    31 	 0.04528 	 0.00659 	 m..s
    3 	    32 	 0.04608 	 0.00660 	 m..s
   33 	    33 	 0.04876 	 0.00660 	 m..s
   65 	    34 	 0.05182 	 0.00661 	 m..s
   54 	    35 	 0.05077 	 0.00663 	 m..s
   61 	    36 	 0.05155 	 0.00666 	 m..s
    1 	    37 	 0.04463 	 0.00671 	 m..s
   11 	    38 	 0.04716 	 0.00673 	 m..s
    7 	    39 	 0.04680 	 0.00801 	 m..s
   10 	    40 	 0.04711 	 0.00950 	 m..s
   36 	    41 	 0.04891 	 0.01007 	 m..s
    5 	    42 	 0.04652 	 0.01096 	 m..s
   21 	    43 	 0.04765 	 0.01171 	 m..s
   25 	    44 	 0.04819 	 0.01313 	 m..s
   23 	    45 	 0.04783 	 0.01365 	 m..s
   32 	    46 	 0.04868 	 0.01451 	 m..s
   30 	    47 	 0.04861 	 0.01482 	 m..s
   22 	    48 	 0.04777 	 0.01709 	 m..s
   17 	    49 	 0.04739 	 0.02222 	 ~...
   39 	    50 	 0.04904 	 0.02557 	 ~...
   29 	    51 	 0.04860 	 0.02570 	 ~...
   31 	    52 	 0.04868 	 0.02799 	 ~...
   40 	    53 	 0.04913 	 0.02997 	 ~...
   37 	    54 	 0.04901 	 0.03191 	 ~...
   35 	    55 	 0.04887 	 0.03243 	 ~...
   41 	    56 	 0.04918 	 0.03526 	 ~...
   43 	    57 	 0.04941 	 0.03776 	 ~...
   47 	    58 	 0.05004 	 0.03818 	 ~...
   45 	    59 	 0.04987 	 0.03866 	 ~...
   60 	    60 	 0.05146 	 0.04039 	 ~...
   46 	    61 	 0.04987 	 0.04185 	 ~...
   68 	    62 	 0.05239 	 0.04369 	 ~...
   51 	    63 	 0.05037 	 0.05450 	 ~...
   84 	    64 	 0.06419 	 0.05635 	 ~...
   28 	    65 	 0.04859 	 0.05713 	 ~...
   56 	    66 	 0.05083 	 0.05852 	 ~...
   86 	    67 	 0.06525 	 0.06170 	 ~...
   87 	    68 	 0.06748 	 0.06223 	 ~...
   71 	    69 	 0.05373 	 0.06428 	 ~...
   49 	    70 	 0.05011 	 0.06457 	 ~...
   73 	    71 	 0.05384 	 0.06528 	 ~...
   76 	    72 	 0.05493 	 0.06534 	 ~...
   88 	    73 	 0.06892 	 0.06559 	 ~...
   38 	    74 	 0.04903 	 0.06589 	 ~...
   52 	    75 	 0.05040 	 0.06673 	 ~...
   92 	    76 	 0.08202 	 0.07219 	 ~...
   94 	    77 	 0.08385 	 0.07302 	 ~...
   98 	    78 	 0.09504 	 0.07911 	 ~...
   99 	    79 	 0.09658 	 0.08285 	 ~...
   97 	    80 	 0.08999 	 0.08414 	 ~...
   58 	    81 	 0.05112 	 0.08468 	 m..s
   53 	    82 	 0.05048 	 0.08502 	 m..s
  102 	    83 	 0.11405 	 0.08610 	 ~...
    9 	    84 	 0.04685 	 0.08716 	 m..s
  105 	    85 	 0.12279 	 0.09061 	 m..s
   85 	    86 	 0.06522 	 0.09162 	 ~...
    0 	    87 	 0.04461 	 0.09260 	 m..s
   64 	    88 	 0.05182 	 0.09363 	 m..s
   69 	    89 	 0.05267 	 0.09500 	 m..s
   59 	    90 	 0.05138 	 0.09526 	 m..s
   74 	    91 	 0.05442 	 0.09585 	 m..s
   67 	    92 	 0.05209 	 0.09588 	 m..s
  103 	    93 	 0.11447 	 0.09631 	 ~...
   77 	    94 	 0.05510 	 0.09648 	 m..s
   62 	    95 	 0.05156 	 0.09700 	 m..s
   89 	    96 	 0.06910 	 0.10072 	 m..s
   83 	    97 	 0.05894 	 0.10089 	 m..s
   90 	    98 	 0.07370 	 0.10361 	 ~...
   78 	    99 	 0.05521 	 0.10387 	 m..s
   91 	   100 	 0.07398 	 0.10414 	 m..s
   95 	   101 	 0.08402 	 0.10453 	 ~...
   96 	   102 	 0.08435 	 0.10670 	 ~...
  107 	   103 	 0.14517 	 0.10675 	 m..s
  101 	   104 	 0.11331 	 0.10825 	 ~...
  104 	   105 	 0.11570 	 0.11076 	 ~...
  106 	   106 	 0.12599 	 0.13739 	 ~...
  100 	   107 	 0.10742 	 0.14016 	 m..s
  109 	   108 	 0.17409 	 0.14759 	 ~...
  115 	   109 	 0.24817 	 0.16005 	 m..s
  110 	   110 	 0.18378 	 0.16380 	 ~...
  108 	   111 	 0.17172 	 0.16702 	 ~...
  111 	   112 	 0.19159 	 0.19480 	 ~...
  112 	   113 	 0.20685 	 0.20024 	 ~...
  113 	   114 	 0.23370 	 0.20183 	 m..s
  114 	   115 	 0.24483 	 0.21616 	 ~...
  116 	   116 	 0.25002 	 0.21775 	 m..s
  117 	   117 	 0.25408 	 0.24481 	 ~...
  118 	   118 	 0.25447 	 0.28145 	 ~...
  119 	   119 	 0.27554 	 0.29087 	 ~...
  120 	   120 	 0.27657 	 0.29431 	 ~...
==========================================
r_mrr = 0.8869526982307434
r2_mrr = 0.7336885929107666
spearmanr_mrr@5 = 0.8068117499351501
spearmanr_mrr@10 = 0.8734681606292725
spearmanr_mrr@50 = 0.9708243608474731
spearmanr_mrr@100 = 0.9303492307662964
spearmanr_mrr@All = 0.9269314408302307
==========================================
test time: 0.597
Done Testing dataset OpenEA
total time taken: 247.23947381973267
training time taken: 231.5798966884613
TWIG out ;))
