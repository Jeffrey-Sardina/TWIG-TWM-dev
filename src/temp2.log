loading model settings from cache: checkpoints/chkpt-ID_2531937323629027_tag_test-kgems.pkl
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_Finetune-job
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [10, 0]
rank_dist_loss_coeffs: [1, 0]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 1 and 2: 0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 148
rank avg (pred): 0.320 +- 0.094
mrr vals (pred, true): 0.046, 0.099
batch losses (mrrl, rdl): 0.0001709873, 9.71089e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 404
rank avg (pred): 0.290 +- 0.093
mrr vals (pred, true): 0.052, 0.145
batch losses (mrrl, rdl): 0.0873600096, 0.0001489078

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1198
rank avg (pred): 0.339 +- 0.066
mrr vals (pred, true): 0.036, 0.041
batch losses (mrrl, rdl): 0.001953695, 0.0002911868

Epoch over!
epoch time: 14.486

Saving checkpoint at [1] epoch 1
Done training phase:  0
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.035 +- 0.012
mrr vals (pred, true): 0.223, 0.623

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.03675 	 0.03230 	 ~...
    1 	     1 	 0.03629 	 0.03303 	 ~...
   12 	     2 	 0.03759 	 0.03379 	 ~...
    3 	     3 	 0.03652 	 0.03491 	 ~...
   30 	     4 	 0.04178 	 0.03543 	 ~...
   16 	     5 	 0.03806 	 0.03555 	 ~...
    2 	     6 	 0.03629 	 0.03566 	 ~...
    4 	     7 	 0.03659 	 0.03572 	 ~...
   31 	     8 	 0.04179 	 0.03665 	 ~...
   78 	     9 	 0.04885 	 0.03691 	 ~...
   20 	    10 	 0.03841 	 0.03694 	 ~...
   52 	    11 	 0.04384 	 0.03722 	 ~...
   18 	    12 	 0.03807 	 0.03736 	 ~...
   47 	    13 	 0.04296 	 0.03779 	 ~...
   23 	    14 	 0.03996 	 0.03794 	 ~...
   37 	    15 	 0.04251 	 0.03801 	 ~...
   29 	    16 	 0.04164 	 0.03818 	 ~...
   15 	    17 	 0.03801 	 0.03824 	 ~...
   26 	    18 	 0.04116 	 0.03829 	 ~...
   46 	    19 	 0.04295 	 0.03841 	 ~...
   10 	    20 	 0.03732 	 0.03880 	 ~...
   73 	    21 	 0.04723 	 0.03886 	 ~...
   38 	    22 	 0.04252 	 0.03940 	 ~...
   58 	    23 	 0.04468 	 0.03945 	 ~...
   14 	    24 	 0.03796 	 0.03948 	 ~...
   71 	    25 	 0.04678 	 0.03954 	 ~...
   57 	    26 	 0.04467 	 0.03959 	 ~...
   13 	    27 	 0.03774 	 0.03977 	 ~...
   41 	    28 	 0.04268 	 0.03987 	 ~...
   21 	    29 	 0.03920 	 0.04001 	 ~...
   40 	    30 	 0.04266 	 0.04019 	 ~...
   60 	    31 	 0.04482 	 0.04050 	 ~...
    5 	    32 	 0.03675 	 0.04059 	 ~...
   64 	    33 	 0.04564 	 0.04060 	 ~...
    8 	    34 	 0.03705 	 0.04081 	 ~...
   51 	    35 	 0.04383 	 0.04126 	 ~...
   19 	    36 	 0.03837 	 0.04143 	 ~...
   79 	    37 	 0.04904 	 0.04182 	 ~...
    7 	    38 	 0.03702 	 0.04418 	 ~...
   17 	    39 	 0.03807 	 0.04435 	 ~...
   72 	    40 	 0.04703 	 0.04593 	 ~...
   25 	    41 	 0.04104 	 0.04595 	 ~...
   49 	    42 	 0.04336 	 0.04607 	 ~...
   80 	    43 	 0.04955 	 0.06426 	 ~...
   70 	    44 	 0.04652 	 0.06454 	 ~...
   74 	    45 	 0.04746 	 0.06812 	 ~...
   88 	    46 	 0.05174 	 0.07245 	 ~...
   81 	    47 	 0.04958 	 0.07306 	 ~...
   44 	    48 	 0.04293 	 0.07572 	 m..s
   82 	    49 	 0.05020 	 0.07645 	 ~...
   87 	    50 	 0.05158 	 0.08144 	 ~...
   24 	    51 	 0.04019 	 0.08159 	 m..s
   84 	    52 	 0.05105 	 0.08354 	 m..s
   48 	    53 	 0.04299 	 0.08483 	 m..s
   61 	    54 	 0.04494 	 0.08605 	 m..s
    0 	    55 	 0.03580 	 0.08625 	 m..s
   22 	    56 	 0.03975 	 0.08644 	 m..s
   42 	    57 	 0.04270 	 0.08687 	 m..s
   59 	    58 	 0.04471 	 0.08730 	 m..s
   11 	    59 	 0.03738 	 0.08739 	 m..s
   35 	    60 	 0.04206 	 0.08768 	 m..s
   62 	    61 	 0.04494 	 0.08889 	 m..s
    9 	    62 	 0.03710 	 0.08899 	 m..s
   54 	    63 	 0.04405 	 0.08924 	 m..s
   27 	    64 	 0.04117 	 0.09103 	 m..s
   65 	    65 	 0.04571 	 0.09155 	 m..s
   34 	    66 	 0.04196 	 0.09245 	 m..s
   95 	    67 	 0.05388 	 0.09376 	 m..s
   50 	    68 	 0.04342 	 0.09394 	 m..s
   45 	    69 	 0.04294 	 0.09410 	 m..s
   36 	    70 	 0.04211 	 0.09495 	 m..s
   56 	    71 	 0.04456 	 0.09496 	 m..s
   28 	    72 	 0.04161 	 0.09999 	 m..s
   33 	    73 	 0.04189 	 0.10033 	 m..s
   66 	    74 	 0.04584 	 0.10079 	 m..s
   39 	    75 	 0.04262 	 0.10238 	 m..s
   92 	    76 	 0.05183 	 0.10850 	 m..s
   88 	    77 	 0.05174 	 0.10900 	 m..s
   94 	    78 	 0.05230 	 0.11248 	 m..s
   53 	    79 	 0.04396 	 0.11368 	 m..s
   86 	    80 	 0.05137 	 0.11432 	 m..s
   63 	    81 	 0.04509 	 0.11644 	 m..s
   88 	    82 	 0.05174 	 0.11699 	 m..s
   91 	    83 	 0.05181 	 0.12359 	 m..s
   93 	    84 	 0.05218 	 0.13060 	 m..s
   83 	    85 	 0.05087 	 0.13087 	 m..s
   77 	    86 	 0.04880 	 0.13577 	 m..s
   43 	    87 	 0.04285 	 0.13662 	 m..s
   55 	    88 	 0.04449 	 0.13689 	 m..s
   68 	    89 	 0.04636 	 0.14348 	 m..s
   85 	    90 	 0.05117 	 0.14453 	 m..s
   67 	    91 	 0.04603 	 0.14620 	 MISS
   32 	    92 	 0.04180 	 0.14897 	 MISS
   76 	    93 	 0.04827 	 0.15149 	 MISS
   69 	    94 	 0.04648 	 0.15153 	 MISS
   75 	    95 	 0.04825 	 0.16156 	 MISS
  118 	    96 	 0.30956 	 0.36531 	 m..s
  119 	    97 	 0.39094 	 0.41053 	 ~...
  120 	    98 	 0.41367 	 0.42189 	 ~...
  105 	    99 	 0.20329 	 0.48792 	 MISS
  102 	   100 	 0.20094 	 0.50860 	 MISS
  116 	   101 	 0.27561 	 0.51098 	 MISS
   99 	   102 	 0.18783 	 0.56204 	 MISS
  117 	   103 	 0.27967 	 0.56964 	 MISS
   96 	   104 	 0.17531 	 0.56998 	 MISS
  110 	   105 	 0.23753 	 0.59282 	 MISS
  113 	   106 	 0.25737 	 0.59483 	 MISS
  111 	   107 	 0.24522 	 0.59591 	 MISS
  115 	   108 	 0.27208 	 0.59711 	 MISS
  101 	   109 	 0.19645 	 0.60102 	 MISS
   98 	   110 	 0.18693 	 0.60193 	 MISS
  100 	   111 	 0.19331 	 0.61400 	 MISS
  109 	   112 	 0.23741 	 0.61403 	 MISS
  112 	   113 	 0.24820 	 0.61471 	 MISS
   97 	   114 	 0.18534 	 0.61801 	 MISS
  108 	   115 	 0.23556 	 0.61930 	 MISS
  114 	   116 	 0.25837 	 0.61949 	 MISS
  107 	   117 	 0.22337 	 0.62336 	 MISS
  106 	   118 	 0.20809 	 0.62817 	 MISS
  104 	   119 	 0.20306 	 0.63164 	 MISS
  103 	   120 	 0.20296 	 0.63323 	 MISS
==========================================
r_mrr = 0.8915011286735535
r2_mrr = 0.3626289367675781
spearmanr_mrr@5 = 0.9284647107124329
spearmanr_mrr@10 = 0.9373477101325989
spearmanr_mrr@50 = 0.9506827592849731
spearmanr_mrr@100 = 0.9617201089859009
spearmanr_mrr@All = 0.9631426334381104
==========================================
test time: 0.479
Done Testing dataset UMLS
total time taken: 22.40752673149109
training time taken: 15.046483516693115
TWIG out ;))
