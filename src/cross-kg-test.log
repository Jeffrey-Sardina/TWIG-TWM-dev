===============================================
-----------------------------------------------
Running a TWIG experiment with tag: ComplEx-all
-----------------------------------------------
===============================================
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_ComplEx-all
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1'], 'CoDExSmall': ['2.1'], 'DBpedia50': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.420 +- 0.004
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0, 0.0001367971

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 309
rank avg (pred): 0.138 +- 0.040
mrr vals (pred, true): 0.054, 0.212
batch losses (mrrl, rdl): 0.0, 7.37409e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 83
rank avg (pred): 0.479 +- 0.005
mrr vals (pred, true): 0.015, 0.041
batch losses (mrrl, rdl): 0.0, 8.63267e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1034
rank avg (pred): 0.479 +- 0.000
mrr vals (pred, true): 0.015, 0.042
batch losses (mrrl, rdl): 0.0, 8.94638e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 39
rank avg (pred): 0.182 +- 0.129
mrr vals (pred, true): 0.060, 0.218
batch losses (mrrl, rdl): 0.0, 1.01651e-05

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.485 +- 0.002
mrr vals (pred, true): 0.015, 0.052
batch losses (mrrl, rdl): 0.0, 0.0001215247

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 980
rank avg (pred): 0.117 +- 0.124
mrr vals (pred, true): 0.111, 0.284
batch losses (mrrl, rdl): 0.0, 8.06352e-05

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 802
rank avg (pred): 0.484 +- 0.003
mrr vals (pred, true): 0.015, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001590843

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 397
rank avg (pred): 0.490 +- 0.006
mrr vals (pred, true): 0.015, 0.055
batch losses (mrrl, rdl): 0.0, 0.0001520028

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 917
rank avg (pred): 0.483 +- 0.044
mrr vals (pred, true): 0.015, 0.019
batch losses (mrrl, rdl): 0.0, 0.0002885625

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.451 +- 0.046
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 7.16969e-05

Epoch over!
epoch time: 66.884

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1157
rank avg (pred): 0.498 +- 0.069
mrr vals (pred, true): 0.015, 0.027
batch losses (mrrl, rdl): 0.0, 2.55922e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1099
rank avg (pred): 0.422 +- 0.177
mrr vals (pred, true): 0.021, 0.046
batch losses (mrrl, rdl): 0.0, 2.54047e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 505
rank avg (pred): 0.430 +- 0.244
mrr vals (pred, true): 0.028, 0.030
batch losses (mrrl, rdl): 0.0, 0.0001018626

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1027
rank avg (pred): 0.460 +- 0.227
mrr vals (pred, true): 0.022, 0.051
batch losses (mrrl, rdl): 0.0, 2.99459e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 54
rank avg (pred): 0.147 +- 0.222
mrr vals (pred, true): 0.256, 0.230
batch losses (mrrl, rdl): 0.0, 3.9034e-06

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1130
rank avg (pred): 0.427 +- 0.252
mrr vals (pred, true): 0.027, 0.043
batch losses (mrrl, rdl): 0.0, 6.1614e-06

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 354
rank avg (pred): 0.443 +- 0.235
mrr vals (pred, true): 0.023, 0.045
batch losses (mrrl, rdl): 0.0, 6.5349e-06

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 732
rank avg (pred): 0.105 +- 0.188
mrr vals (pred, true): 0.310, 0.524
batch losses (mrrl, rdl): 0.0, 6.46467e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 434
rank avg (pred): 0.429 +- 0.264
mrr vals (pred, true): 0.026, 0.049
batch losses (mrrl, rdl): 0.0, 7.7292e-06

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1126
rank avg (pred): 0.444 +- 0.261
mrr vals (pred, true): 0.025, 0.051
batch losses (mrrl, rdl): 0.0, 6.601e-07

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.459 +- 0.241
mrr vals (pred, true): 0.022, 0.052
batch losses (mrrl, rdl): 0.0, 6.6054e-06

Epoch over!
epoch time: 65.102

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 806
rank avg (pred): 0.442 +- 0.257
mrr vals (pred, true): 0.025, 0.049
batch losses (mrrl, rdl): 0.0, 2.4622e-06

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1094
rank avg (pred): 0.456 +- 0.250
mrr vals (pred, true): 0.022, 0.047
batch losses (mrrl, rdl): 0.0, 3.2117e-06

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 187
rank avg (pred): 0.404 +- 0.290
mrr vals (pred, true): 0.033, 0.049
batch losses (mrrl, rdl): 0.0, 1.32012e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 568
rank avg (pred): 0.419 +- 0.283
mrr vals (pred, true): 0.030, 0.034
batch losses (mrrl, rdl): 0.0, 5.45042e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 994
rank avg (pred): 0.171 +- 0.169
mrr vals (pred, true): 0.070, 0.275
batch losses (mrrl, rdl): 0.0, 2.1197e-06

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1162
rank avg (pred): 0.434 +- 0.269
mrr vals (pred, true): 0.027, 0.036
batch losses (mrrl, rdl): 0.0, 5.7409e-06

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 872
rank avg (pred): 0.444 +- 0.262
mrr vals (pred, true): 0.025, 0.043
batch losses (mrrl, rdl): 0.0, 4.4778e-06

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 953
rank avg (pred): 0.452 +- 0.254
mrr vals (pred, true): 0.023, 0.043
batch losses (mrrl, rdl): 0.0, 1.3242e-06

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 213
rank avg (pred): 0.468 +- 0.251
mrr vals (pred, true): 0.023, 0.047
batch losses (mrrl, rdl): 0.0, 2.5029e-06

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 522
rank avg (pred): 0.460 +- 0.245
mrr vals (pred, true): 0.022, 0.025
batch losses (mrrl, rdl): 0.0, 4.7872e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 268
rank avg (pred): 0.104 +- 0.182
mrr vals (pred, true): 0.198, 0.217
batch losses (mrrl, rdl): 0.0, 0.0001264798

Epoch over!
epoch time: 66.135

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1127
rank avg (pred): 0.428 +- 0.276
mrr vals (pred, true): 0.027, 0.052
batch losses (mrrl, rdl): 0.0, 3.4206e-06

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 160
rank avg (pred): 0.461 +- 0.245
mrr vals (pred, true): 0.022, 0.053
batch losses (mrrl, rdl): 0.0, 6.9944e-06

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 648
rank avg (pred): 0.439 +- 0.261
mrr vals (pred, true): 0.024, 0.047
batch losses (mrrl, rdl): 0.0, 1.8867e-06

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 286
rank avg (pred): 0.174 +- 0.189
mrr vals (pred, true): 0.085, 0.213
batch losses (mrrl, rdl): 0.0, 2.245e-07

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 914
rank avg (pred): 0.576 +- 0.154
mrr vals (pred, true): 0.014, 0.021
batch losses (mrrl, rdl): 0.0, 3.7014e-06

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 361
rank avg (pred): 0.452 +- 0.256
mrr vals (pred, true): 0.022, 0.050
batch losses (mrrl, rdl): 0.0, 4.6673e-06

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1193
rank avg (pred): 0.452 +- 0.256
mrr vals (pred, true): 0.023, 0.055
batch losses (mrrl, rdl): 0.0, 1.2403e-06

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 594
rank avg (pred): 0.446 +- 0.261
mrr vals (pred, true): 0.023, 0.037
batch losses (mrrl, rdl): 0.0, 1.47621e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 124
rank avg (pred): 0.449 +- 0.254
mrr vals (pred, true): 0.023, 0.047
batch losses (mrrl, rdl): 0.0, 6.611e-07

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 486
rank avg (pred): 0.525 +- 0.204
mrr vals (pred, true): 0.017, 0.025
batch losses (mrrl, rdl): 0.0, 8.357e-07

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1075
rank avg (pred): 0.234 +- 0.233
mrr vals (pred, true): 0.062, 0.273
batch losses (mrrl, rdl): 0.0, 0.0001458675

Epoch over!
epoch time: 67.331

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 388
rank avg (pred): 0.445 +- 0.261
mrr vals (pred, true): 0.023, 0.046
batch losses (mrrl, rdl): 0.0, 2.8302e-06

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 603
rank avg (pred): 0.436 +- 0.269
mrr vals (pred, true): 0.025, 0.044
batch losses (mrrl, rdl): 0.0, 1.90623e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 548
rank avg (pred): 0.539 +- 0.194
mrr vals (pred, true): 0.016, 0.024
batch losses (mrrl, rdl): 0.0, 4.262e-06

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 794
rank avg (pred): 0.446 +- 0.259
mrr vals (pred, true): 0.023, 0.050
batch losses (mrrl, rdl): 0.0, 2.036e-06

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 658
rank avg (pred): 0.455 +- 0.253
mrr vals (pred, true): 0.022, 0.048
batch losses (mrrl, rdl): 0.0, 3.2124e-06

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1109
rank avg (pred): 0.450 +- 0.257
mrr vals (pred, true): 0.023, 0.044
batch losses (mrrl, rdl): 0.0, 2.2568e-06

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 21
rank avg (pred): 0.088 +- 0.176
mrr vals (pred, true): 0.211, 0.207
batch losses (mrrl, rdl): 0.0, 0.0002253716

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 988
rank avg (pred): 0.155 +- 0.207
mrr vals (pred, true): 0.109, 0.335
batch losses (mrrl, rdl): 0.0, 6.671e-07

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1199
rank avg (pred): 0.448 +- 0.249
mrr vals (pred, true): 0.023, 0.046
batch losses (mrrl, rdl): 0.0, 1.3284e-06

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 448
rank avg (pred): 0.444 +- 0.258
mrr vals (pred, true): 0.024, 0.050
batch losses (mrrl, rdl): 0.0, 8.109e-07

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 263
rank avg (pred): 0.091 +- 0.185
mrr vals (pred, true): 0.333, 0.217
batch losses (mrrl, rdl): 0.0, 9.66343e-05

Epoch over!
epoch time: 68.389

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 899
rank avg (pred): 0.654 +- 0.108
mrr vals (pred, true): 0.012, 0.055
batch losses (mrrl, rdl): 0.0146999294, 0.0014221967

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1
rank avg (pred): 0.089 +- 0.064
mrr vals (pred, true): 0.175, 0.241
batch losses (mrrl, rdl): 0.044031471, 0.0001250729

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 527
rank avg (pred): 0.444 +- 0.181
mrr vals (pred, true): 0.064, 0.025
batch losses (mrrl, rdl): 0.0020014842, 0.0001083911

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 914
rank avg (pred): 0.447 +- 0.239
mrr vals (pred, true): 0.107, 0.021
batch losses (mrrl, rdl): 0.0319973305, 0.000282343

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1136
rank avg (pred): 0.500 +- 0.204
mrr vals (pred, true): 0.064, 0.026
batch losses (mrrl, rdl): 0.0020010648, 2.12458e-05

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 215
rank avg (pred): 0.524 +- 0.179
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 5.0546e-06, 0.0001594809

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 632
rank avg (pred): 0.534 +- 0.187
mrr vals (pred, true): 0.054, 0.037
batch losses (mrrl, rdl): 0.0001609106, 9.6078e-05

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 844
rank avg (pred): 0.386 +- 0.174
mrr vals (pred, true): 0.070, 0.052
batch losses (mrrl, rdl): 0.0041042604, 9.54511e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 692
rank avg (pred): 0.536 +- 0.170
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 6.4102e-06, 0.00022358

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1008
rank avg (pred): 0.494 +- 0.174
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 3.98052e-05, 9.32301e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1064
rank avg (pred): 0.061 +- 0.057
mrr vals (pred, true): 0.277, 0.289
batch losses (mrrl, rdl): 0.0014079001, 0.0001368191

Epoch over!
epoch time: 70.024

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1171
rank avg (pred): 0.526 +- 0.175
mrr vals (pred, true): 0.051, 0.035
batch losses (mrrl, rdl): 2.9739e-06, 0.0001023342

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1077
rank avg (pred): 0.097 +- 0.081
mrr vals (pred, true): 0.199, 0.273
batch losses (mrrl, rdl): 0.0538204983, 0.0001855783

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 264
rank avg (pred): 0.174 +- 0.177
mrr vals (pred, true): 0.216, 0.171
batch losses (mrrl, rdl): 0.0203775801, 3.38125e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1065
rank avg (pred): 0.111 +- 0.115
mrr vals (pred, true): 0.260, 0.277
batch losses (mrrl, rdl): 0.0026952659, 4.85559e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1060
rank avg (pred): 0.064 +- 0.062
mrr vals (pred, true): 0.303, 0.299
batch losses (mrrl, rdl): 0.0001538548, 0.000309185

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 949
rank avg (pred): 0.550 +- 0.198
mrr vals (pred, true): 0.055, 0.045
batch losses (mrrl, rdl): 0.0002331047, 0.0003163452

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 346
rank avg (pred): 0.473 +- 0.180
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 1.1985e-06, 3.48712e-05

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 546
rank avg (pred): 0.401 +- 0.262
mrr vals (pred, true): 0.116, 0.026
batch losses (mrrl, rdl): 0.0440345928, 0.0001447477

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 617
rank avg (pred): 0.509 +- 0.181
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 5.27828e-05, 7.30912e-05

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 221
rank avg (pred): 0.492 +- 0.162
mrr vals (pred, true): 0.039, 0.058
batch losses (mrrl, rdl): 0.0011244216, 8.5552e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 534
rank avg (pred): 0.463 +- 0.229
mrr vals (pred, true): 0.070, 0.024
batch losses (mrrl, rdl): 0.0041839755, 5.75021e-05

Epoch over!
epoch time: 65.56

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1014
rank avg (pred): 0.491 +- 0.191
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 1.236e-05, 4.57632e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 788
rank avg (pred): 0.359 +- 0.171
mrr vals (pred, true): 0.057, 0.045
batch losses (mrrl, rdl): 0.0005165117, 0.0002227418

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 402
rank avg (pred): 0.462 +- 0.179
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002262198, 2.47031e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 759
rank avg (pred): 0.457 +- 0.175
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0001581725, 2.37669e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 694
rank avg (pred): 0.486 +- 0.165
mrr vals (pred, true): 0.044, 0.045
batch losses (mrrl, rdl): 0.0003608719, 6.82702e-05

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1053
rank avg (pred): 0.211 +- 0.238
mrr vals (pred, true): 0.313, 0.277
batch losses (mrrl, rdl): 0.0130425161, 0.0001607074

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1191
rank avg (pred): 0.441 +- 0.164
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.0002936198, 3.31605e-05

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 224
rank avg (pred): 0.420 +- 0.153
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 4.57415e-05, 3.45004e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1065
rank avg (pred): 0.198 +- 0.225
mrr vals (pred, true): 0.213, 0.277
batch losses (mrrl, rdl): 0.039984785, 6.36131e-05

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 855
rank avg (pred): 0.472 +- 0.161
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 9.22548e-05, 7.4294e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 340
rank avg (pred): 0.379 +- 0.157
mrr vals (pred, true): 0.054, 0.048
batch losses (mrrl, rdl): 0.0001467033, 8.7443e-05

Epoch over!
epoch time: 64.467

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 222
rank avg (pred): 0.445 +- 0.164
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 6.5116e-05, 2.44287e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 367
rank avg (pred): 0.458 +- 0.172
mrr vals (pred, true): 0.056, 0.054
batch losses (mrrl, rdl): 0.0003100944, 3.59823e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 164
rank avg (pred): 0.421 +- 0.165
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 1.5822e-05, 3.66139e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 3
rank avg (pred): 0.201 +- 0.223
mrr vals (pred, true): 0.274, 0.245
batch losses (mrrl, rdl): 0.008432243, 5.29331e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 828
rank avg (pred): 0.182 +- 0.221
mrr vals (pred, true): 0.293, 0.328
batch losses (mrrl, rdl): 0.0123230591, 0.0001077453

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 159
rank avg (pred): 0.484 +- 0.150
mrr vals (pred, true): 0.045, 0.048
batch losses (mrrl, rdl): 0.0002483152, 7.22634e-05

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 987
rank avg (pred): 0.228 +- 0.228
mrr vals (pred, true): 0.244, 0.322
batch losses (mrrl, rdl): 0.0606029779, 0.0001237638

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 651
rank avg (pred): 0.408 +- 0.186
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 4.339e-07, 3.23047e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 384
rank avg (pred): 0.342 +- 0.153
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 6.3494e-06, 0.0002321181

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 716
rank avg (pred): 0.422 +- 0.164
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 3.588e-07, 3.83539e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 658
rank avg (pred): 0.458 +- 0.152
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003681803, 4.45373e-05

Epoch over!
epoch time: 62.797

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 827
rank avg (pred): 0.176 +- 0.220
mrr vals (pred, true): 0.312, 0.284
batch losses (mrrl, rdl): 0.0081932992, 2.95193e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 133
rank avg (pred): 0.399 +- 0.181
mrr vals (pred, true): 0.054, 0.049
batch losses (mrrl, rdl): 0.0001326289, 3.69499e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1204
rank avg (pred): 0.426 +- 0.158
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.384e-06, 4.2597e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 763
rank avg (pred): 0.414 +- 0.166
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 8.862e-06, 3.97893e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 157
rank avg (pred): 0.347 +- 0.152
mrr vals (pred, true): 0.037, 0.041
batch losses (mrrl, rdl): 0.0018133478, 0.0002767417

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 242
rank avg (pred): 0.406 +- 0.166
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 1.563e-06, 9.16737e-05

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 401
rank avg (pred): 0.411 +- 0.152
mrr vals (pred, true): 0.044, 0.051
batch losses (mrrl, rdl): 0.0003143458, 4.51908e-05

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1116
rank avg (pred): 0.372 +- 0.164
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 4.87202e-05, 9.70744e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 926
rank avg (pred): 0.382 +- 0.176
mrr vals (pred, true): 0.040, 0.029
batch losses (mrrl, rdl): 0.0010561296, 0.0001812401

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 295
rank avg (pred): 0.322 +- 0.256
mrr vals (pred, true): 0.199, 0.215
batch losses (mrrl, rdl): 0.0027298895, 0.0004660703

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 927
rank avg (pred): 0.476 +- 0.146
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 8.93095e-05, 3.99627e-05

Epoch over!
epoch time: 62.266

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 221
rank avg (pred): 0.405 +- 0.167
mrr vals (pred, true): 0.045, 0.058
batch losses (mrrl, rdl): 0.0002078777, 5.87155e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 85
rank avg (pred): 0.457 +- 0.137
mrr vals (pred, true): 0.045, 0.049
batch losses (mrrl, rdl): 0.0002787521, 7.27783e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 42
rank avg (pred): 0.243 +- 0.229
mrr vals (pred, true): 0.182, 0.200
batch losses (mrrl, rdl): 0.0032037285, 9.11338e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1132
rank avg (pred): 0.402 +- 0.159
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 8.75981e-05, 4.58834e-05

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 563
rank avg (pred): 0.378 +- 0.206
mrr vals (pred, true): 0.069, 0.021
batch losses (mrrl, rdl): 0.0035516054, 0.0004135753

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 975
rank avg (pred): 0.277 +- 0.241
mrr vals (pred, true): 0.274, 0.296
batch losses (mrrl, rdl): 0.0048058159, 0.0004431647

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 619
rank avg (pred): 0.427 +- 0.170
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.0001238865, 4.18214e-05

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 581
rank avg (pred): 0.429 +- 0.171
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 2.53853e-05, 2.8727e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 218
rank avg (pred): 0.436 +- 0.157
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001366911, 3.26743e-05

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 452
rank avg (pred): 0.432 +- 0.153
mrr vals (pred, true): 0.047, 0.038
batch losses (mrrl, rdl): 9.65834e-05, 3.65245e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1189
rank avg (pred): 0.435 +- 0.154
mrr vals (pred, true): 0.040, 0.054
batch losses (mrrl, rdl): 0.0010058554, 4.00384e-05

Epoch over!
epoch time: 64.275

Epoch 7 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 392
rank avg (pred): 0.430 +- 0.140
mrr vals (pred, true): 0.041, 0.043
batch losses (mrrl, rdl): 0.0008748064, 4.03263e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 327
rank avg (pred): 0.423 +- 0.161
mrr vals (pred, true): 0.049, 0.053
batch losses (mrrl, rdl): 1.57448e-05, 3.86029e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 783
rank avg (pred): 0.428 +- 0.152
mrr vals (pred, true): 0.042, 0.055
batch losses (mrrl, rdl): 0.0006870354, 3.80715e-05

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 536
rank avg (pred): 0.332 +- 0.203
mrr vals (pred, true): 0.079, 0.021
batch losses (mrrl, rdl): 0.0086535094, 0.000645429

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 756
rank avg (pred): 0.370 +- 0.182
mrr vals (pred, true): 0.059, 0.050
batch losses (mrrl, rdl): 0.0008807349, 8.04906e-05

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 443
rank avg (pred): 0.398 +- 0.178
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 1.978e-07, 3.16987e-05

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 528
rank avg (pred): 0.322 +- 0.214
mrr vals (pred, true): 0.091, 0.025
batch losses (mrrl, rdl): 0.0168943964, 0.0005167428

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1036
rank avg (pred): 0.401 +- 0.176
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.0003342977, 6.06068e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 909
rank avg (pred): 0.350 +- 0.185
mrr vals (pred, true): 0.084, 0.023
batch losses (mrrl, rdl): 0.0115755675, 0.0008278653

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 376
rank avg (pred): 0.390 +- 0.171
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.000176917, 5.7443e-05

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1007
rank avg (pred): 0.382 +- 0.188
mrr vals (pred, true): 0.052, 0.037
batch losses (mrrl, rdl): 5.4718e-05, 6.27264e-05

Epoch over!
epoch time: 69.572

Epoch 8 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 341
rank avg (pred): 0.403 +- 0.173
mrr vals (pred, true): 0.034, 0.043
batch losses (mrrl, rdl): 0.0026748118, 3.29559e-05

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 228
rank avg (pred): 0.383 +- 0.181
mrr vals (pred, true): 0.042, 0.045
batch losses (mrrl, rdl): 0.0006122937, 5.26262e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 66
rank avg (pred): 0.296 +- 0.234
mrr vals (pred, true): 0.216, 0.207
batch losses (mrrl, rdl): 0.0006928868, 0.0003178795

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 408
rank avg (pred): 0.357 +- 0.191
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 2.18522e-05, 0.0001325863

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 935
rank avg (pred): 0.387 +- 0.174
mrr vals (pred, true): 0.040, 0.027
batch losses (mrrl, rdl): 0.0009539588, 0.0002525054

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1162
rank avg (pred): 0.353 +- 0.190
mrr vals (pred, true): 0.055, 0.036
batch losses (mrrl, rdl): 0.0002462426, 0.0001402194

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1011
rank avg (pred): 0.349 +- 0.191
mrr vals (pred, true): 0.056, 0.047
batch losses (mrrl, rdl): 0.0003938617, 0.0001523895

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 132
rank avg (pred): 0.378 +- 0.172
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.000192904, 7.62689e-05

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1008
rank avg (pred): 0.364 +- 0.186
mrr vals (pred, true): 0.062, 0.044
batch losses (mrrl, rdl): 0.0013797054, 9.91379e-05

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 648
rank avg (pred): 0.374 +- 0.173
mrr vals (pred, true): 0.036, 0.047
batch losses (mrrl, rdl): 0.0020256517, 0.0001062902

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 254
rank avg (pred): 0.290 +- 0.227
mrr vals (pred, true): 0.167, 0.221
batch losses (mrrl, rdl): 0.0291258991, 0.0003194064

Epoch over!
epoch time: 70.711

Epoch 9 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 904
rank avg (pred): 0.358 +- 0.162
mrr vals (pred, true): 0.054, 0.022
batch losses (mrrl, rdl): 0.0001924246, 0.001002346

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 791
rank avg (pred): 0.368 +- 0.185
mrr vals (pred, true): 0.054, 0.049
batch losses (mrrl, rdl): 0.0001393627, 6.30235e-05

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 543
rank avg (pred): 0.295 +- 0.212
mrr vals (pred, true): 0.109, 0.023
batch losses (mrrl, rdl): 0.0348131061, 0.0007833149

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 549
rank avg (pred): 0.302 +- 0.206
mrr vals (pred, true): 0.095, 0.023
batch losses (mrrl, rdl): 0.0202021096, 0.0007289661

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 82
rank avg (pred): 0.399 +- 0.151
mrr vals (pred, true): 0.039, 0.040
batch losses (mrrl, rdl): 0.0012439226, 8.03736e-05

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 663
rank avg (pred): 0.369 +- 0.168
mrr vals (pred, true): 0.043, 0.046
batch losses (mrrl, rdl): 0.0004599056, 9.68417e-05

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 559
rank avg (pred): 0.318 +- 0.195
mrr vals (pred, true): 0.070, 0.026
batch losses (mrrl, rdl): 0.0039058928, 0.0005669721

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1075
rank avg (pred): 0.245 +- 0.220
mrr vals (pred, true): 0.304, 0.273
batch losses (mrrl, rdl): 0.0098655298, 0.0002511046

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 433
rank avg (pred): 0.367 +- 0.172
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 8.5441e-06, 0.0001362307

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 222
rank avg (pred): 0.347 +- 0.182
mrr vals (pred, true): 0.055, 0.048
batch losses (mrrl, rdl): 0.0002610981, 0.0001882417

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 310
rank avg (pred): 0.281 +- 0.225
mrr vals (pred, true): 0.198, 0.220
batch losses (mrrl, rdl): 0.0049304594, 0.000280683

Epoch over!
epoch time: 68.171

Epoch 10 -- 
running batch: 0 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 185
rank avg (pred): 0.336 +- 0.178
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 6.8591e-06, 0.0001963668

running batch: 500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.332 +- 0.179
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 5.1914e-06, 0.000202544

running batch: 1000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 788
rank avg (pred): 0.324 +- 0.182
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0004093616, 0.000337074

running batch: 1500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 807
rank avg (pred): 0.362 +- 0.173
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 8.3725e-06, 0.000110439

running batch: 2000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 680
rank avg (pred): 0.379 +- 0.150
mrr vals (pred, true): 0.050, 0.056
batch losses (mrrl, rdl): 9.615e-07, 8.30981e-05

running batch: 2500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 255
rank avg (pred): 0.249 +- 0.212
mrr vals (pred, true): 0.198, 0.205
batch losses (mrrl, rdl): 0.0005524812, 9.55068e-05

running batch: 3000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 523
rank avg (pred): 0.319 +- 0.179
mrr vals (pred, true): 0.064, 0.022
batch losses (mrrl, rdl): 0.0020292702, 0.0007415697

running batch: 3500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 54
rank avg (pred): 0.246 +- 0.212
mrr vals (pred, true): 0.230, 0.230
batch losses (mrrl, rdl): 3.7619e-06, 0.0002129567

running batch: 4000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 151
rank avg (pred): 0.353 +- 0.169
mrr vals (pred, true): 0.043, 0.049
batch losses (mrrl, rdl): 0.0004574492, 0.0001283755

running batch: 4500 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 657
rank avg (pred): 0.340 +- 0.165
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001600039, 0.0001964112

running batch: 5000 / 5470 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1127
rank avg (pred): 0.325 +- 0.182
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001281816, 0.0002056403

Epoch over!
epoch time: 69.059

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.241 +- 0.216
mrr vals (pred, true): 0.315, 0.295

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   77 	     0 	 0.05256 	 0.02393 	 ~...
   94 	     1 	 0.07906 	 0.02414 	 m..s
   84 	     2 	 0.07234 	 0.02424 	 m..s
   87 	     3 	 0.07445 	 0.02476 	 m..s
   85 	     4 	 0.07289 	 0.02481 	 m..s
   86 	     5 	 0.07428 	 0.02543 	 m..s
   91 	     6 	 0.07741 	 0.02544 	 m..s
   76 	     7 	 0.05237 	 0.02552 	 ~...
   89 	     8 	 0.07497 	 0.02570 	 m..s
   95 	     9 	 0.08002 	 0.02571 	 m..s
   93 	    10 	 0.07844 	 0.02579 	 m..s
   78 	    11 	 0.05313 	 0.02609 	 ~...
   88 	    12 	 0.07474 	 0.02623 	 m..s
   80 	    13 	 0.05522 	 0.02633 	 ~...
    6 	    14 	 0.03790 	 0.02721 	 ~...
   92 	    15 	 0.07782 	 0.02758 	 m..s
   90 	    16 	 0.07503 	 0.02767 	 m..s
    4 	    17 	 0.03426 	 0.02843 	 ~...
    0 	    18 	 0.03343 	 0.03512 	 ~...
    7 	    19 	 0.03820 	 0.03647 	 ~...
   11 	    20 	 0.03843 	 0.03695 	 ~...
   12 	    21 	 0.03854 	 0.03730 	 ~...
   14 	    22 	 0.03858 	 0.03889 	 ~...
   22 	    23 	 0.03955 	 0.03923 	 ~...
    3 	    24 	 0.03390 	 0.03937 	 ~...
   15 	    25 	 0.03861 	 0.03973 	 ~...
   70 	    26 	 0.04784 	 0.04046 	 ~...
   30 	    27 	 0.04136 	 0.04050 	 ~...
   82 	    28 	 0.05720 	 0.04084 	 ~...
   10 	    29 	 0.03836 	 0.04091 	 ~...
    8 	    30 	 0.03827 	 0.04101 	 ~...
   19 	    31 	 0.03899 	 0.04126 	 ~...
    1 	    32 	 0.03363 	 0.04129 	 ~...
   18 	    33 	 0.03897 	 0.04139 	 ~...
   13 	    34 	 0.03858 	 0.04147 	 ~...
    2 	    35 	 0.03367 	 0.04169 	 ~...
   16 	    36 	 0.03880 	 0.04209 	 ~...
   66 	    37 	 0.04758 	 0.04221 	 ~...
   56 	    38 	 0.04401 	 0.04224 	 ~...
   36 	    39 	 0.04174 	 0.04224 	 ~...
   67 	    40 	 0.04765 	 0.04224 	 ~...
   52 	    41 	 0.04341 	 0.04240 	 ~...
   49 	    42 	 0.04311 	 0.04270 	 ~...
   42 	    43 	 0.04198 	 0.04276 	 ~...
    9 	    44 	 0.03828 	 0.04279 	 ~...
   17 	    45 	 0.03886 	 0.04302 	 ~...
   51 	    46 	 0.04339 	 0.04332 	 ~...
   81 	    47 	 0.05599 	 0.04333 	 ~...
   48 	    48 	 0.04285 	 0.04369 	 ~...
   55 	    49 	 0.04386 	 0.04386 	 ~...
   71 	    50 	 0.04794 	 0.04388 	 ~...
   61 	    51 	 0.04509 	 0.04418 	 ~...
   64 	    52 	 0.04635 	 0.04444 	 ~...
   39 	    53 	 0.04187 	 0.04460 	 ~...
   63 	    54 	 0.04589 	 0.04473 	 ~...
   29 	    55 	 0.04136 	 0.04514 	 ~...
   62 	    56 	 0.04547 	 0.04549 	 ~...
   33 	    57 	 0.04163 	 0.04601 	 ~...
   23 	    58 	 0.03999 	 0.04609 	 ~...
   32 	    59 	 0.04158 	 0.04652 	 ~...
   35 	    60 	 0.04173 	 0.04658 	 ~...
   34 	    61 	 0.04164 	 0.04659 	 ~...
   54 	    62 	 0.04352 	 0.04757 	 ~...
   37 	    63 	 0.04182 	 0.04760 	 ~...
   73 	    64 	 0.04820 	 0.04777 	 ~...
   43 	    65 	 0.04202 	 0.04815 	 ~...
   25 	    66 	 0.04074 	 0.04815 	 ~...
   26 	    67 	 0.04115 	 0.04819 	 ~...
   74 	    68 	 0.04855 	 0.04843 	 ~...
   83 	    69 	 0.06223 	 0.04894 	 ~...
   79 	    70 	 0.05414 	 0.04896 	 ~...
   40 	    71 	 0.04188 	 0.04952 	 ~...
   28 	    72 	 0.04122 	 0.04953 	 ~...
   45 	    73 	 0.04229 	 0.04959 	 ~...
   75 	    74 	 0.04878 	 0.04969 	 ~...
   27 	    75 	 0.04120 	 0.04974 	 ~...
   46 	    76 	 0.04262 	 0.04978 	 ~...
   69 	    77 	 0.04783 	 0.04989 	 ~...
   57 	    78 	 0.04418 	 0.04990 	 ~...
   58 	    79 	 0.04430 	 0.05007 	 ~...
   68 	    80 	 0.04766 	 0.05012 	 ~...
   44 	    81 	 0.04214 	 0.05014 	 ~...
   65 	    82 	 0.04714 	 0.05082 	 ~...
   59 	    83 	 0.04433 	 0.05085 	 ~...
   50 	    84 	 0.04320 	 0.05086 	 ~...
   72 	    85 	 0.04799 	 0.05092 	 ~...
   38 	    86 	 0.04182 	 0.05178 	 ~...
   24 	    87 	 0.04052 	 0.05199 	 ~...
   41 	    88 	 0.04190 	 0.05259 	 ~...
   21 	    89 	 0.03943 	 0.05269 	 ~...
   31 	    90 	 0.04151 	 0.05275 	 ~...
   53 	    91 	 0.04343 	 0.05333 	 ~...
    5 	    92 	 0.03494 	 0.05340 	 ~...
   60 	    93 	 0.04438 	 0.05355 	 ~...
   47 	    94 	 0.04268 	 0.05479 	 ~...
   20 	    95 	 0.03914 	 0.05634 	 ~...
  107 	    96 	 0.23959 	 0.18803 	 m..s
   97 	    97 	 0.18422 	 0.20836 	 ~...
   98 	    98 	 0.20089 	 0.20872 	 ~...
   99 	    99 	 0.20103 	 0.21171 	 ~...
  105 	   100 	 0.23653 	 0.21637 	 ~...
  102 	   101 	 0.22214 	 0.21805 	 ~...
  101 	   102 	 0.21894 	 0.21849 	 ~...
  106 	   103 	 0.23656 	 0.23847 	 ~...
  113 	   104 	 0.27628 	 0.25215 	 ~...
   96 	   105 	 0.18260 	 0.25298 	 m..s
  100 	   106 	 0.21362 	 0.26711 	 m..s
  109 	   107 	 0.26150 	 0.27410 	 ~...
  104 	   108 	 0.23628 	 0.27441 	 m..s
  111 	   109 	 0.26729 	 0.28106 	 ~...
  120 	   110 	 0.34898 	 0.28160 	 m..s
  116 	   111 	 0.28152 	 0.28472 	 ~...
  108 	   112 	 0.24517 	 0.28533 	 m..s
  115 	   113 	 0.27807 	 0.28534 	 ~...
  103 	   114 	 0.23454 	 0.29334 	 m..s
  114 	   115 	 0.27645 	 0.29484 	 ~...
  118 	   116 	 0.31526 	 0.29550 	 ~...
  112 	   117 	 0.26911 	 0.30397 	 m..s
  110 	   118 	 0.26413 	 0.31636 	 m..s
  119 	   119 	 0.34407 	 0.37350 	 ~...
  117 	   120 	 0.31353 	 0.39525 	 m..s
==========================================
r_mrr = 0.9672275185585022
r2_mrr = 0.9326678514480591
spearmanr_mrr@5 = 0.9214109778404236
spearmanr_mrr@10 = 0.9356002807617188
spearmanr_mrr@50 = 0.9941379427909851
spearmanr_mrr@100 = 0.9945631623268127
spearmanr_mrr@All = 0.9947119951248169
==========================================
test time: 0.455
Done Testing dataset UMLS
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.260 +- 0.220
mrr vals (pred, true): 0.218, 0.316

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.03540 	 0.00101 	 m..s
    5 	     1 	 0.03777 	 0.00194 	 m..s
    7 	     2 	 0.03818 	 0.00212 	 m..s
   78 	     3 	 0.05018 	 0.00246 	 m..s
   14 	     4 	 0.03995 	 0.00250 	 m..s
   52 	     5 	 0.04507 	 0.00253 	 m..s
    1 	     6 	 0.03360 	 0.00263 	 m..s
    2 	     7 	 0.03370 	 0.00266 	 m..s
   67 	     8 	 0.04673 	 0.00282 	 m..s
   71 	     9 	 0.04705 	 0.00289 	 m..s
   40 	    10 	 0.04388 	 0.00296 	 m..s
   55 	    11 	 0.04519 	 0.00297 	 m..s
   23 	    12 	 0.04263 	 0.00297 	 m..s
   27 	    13 	 0.04307 	 0.00300 	 m..s
   11 	    14 	 0.03939 	 0.00318 	 m..s
   49 	    15 	 0.04482 	 0.00321 	 m..s
   36 	    16 	 0.04349 	 0.00331 	 m..s
   72 	    17 	 0.04734 	 0.00334 	 m..s
   56 	    18 	 0.04523 	 0.00334 	 m..s
   69 	    19 	 0.04699 	 0.00334 	 m..s
   38 	    20 	 0.04378 	 0.00336 	 m..s
    0 	    21 	 0.03283 	 0.00343 	 ~...
   42 	    22 	 0.04417 	 0.00344 	 m..s
   45 	    23 	 0.04449 	 0.00345 	 m..s
   25 	    24 	 0.04294 	 0.00347 	 m..s
   34 	    25 	 0.04344 	 0.00347 	 m..s
   39 	    26 	 0.04379 	 0.00350 	 m..s
   68 	    27 	 0.04676 	 0.00358 	 m..s
   61 	    28 	 0.04612 	 0.00361 	 m..s
   26 	    29 	 0.04300 	 0.00363 	 m..s
    3 	    30 	 0.03500 	 0.00366 	 m..s
   18 	    31 	 0.04222 	 0.00369 	 m..s
   44 	    32 	 0.04434 	 0.00371 	 m..s
   70 	    33 	 0.04699 	 0.00376 	 m..s
   64 	    34 	 0.04633 	 0.00377 	 m..s
    8 	    35 	 0.03902 	 0.00377 	 m..s
   79 	    36 	 0.05128 	 0.00380 	 m..s
   10 	    37 	 0.03936 	 0.00380 	 m..s
    6 	    38 	 0.03779 	 0.00381 	 m..s
   33 	    39 	 0.04334 	 0.00381 	 m..s
    9 	    40 	 0.03905 	 0.00384 	 m..s
   54 	    41 	 0.04519 	 0.00385 	 m..s
   31 	    42 	 0.04330 	 0.00387 	 m..s
   37 	    43 	 0.04362 	 0.00387 	 m..s
   21 	    44 	 0.04254 	 0.00387 	 m..s
   76 	    45 	 0.04876 	 0.00389 	 m..s
   24 	    46 	 0.04282 	 0.00391 	 m..s
   51 	    47 	 0.04502 	 0.00398 	 m..s
   13 	    48 	 0.03972 	 0.00399 	 m..s
   16 	    49 	 0.04145 	 0.00401 	 m..s
   59 	    50 	 0.04574 	 0.00404 	 m..s
   57 	    51 	 0.04525 	 0.00413 	 m..s
   22 	    52 	 0.04256 	 0.00414 	 m..s
   15 	    53 	 0.04126 	 0.00415 	 m..s
   74 	    54 	 0.04760 	 0.00415 	 m..s
   60 	    55 	 0.04606 	 0.00418 	 m..s
   62 	    56 	 0.04620 	 0.00428 	 m..s
   20 	    57 	 0.04251 	 0.00433 	 m..s
   50 	    58 	 0.04485 	 0.00434 	 m..s
   19 	    59 	 0.04248 	 0.00438 	 m..s
   48 	    60 	 0.04475 	 0.00439 	 m..s
   41 	    61 	 0.04417 	 0.00441 	 m..s
   47 	    62 	 0.04470 	 0.00442 	 m..s
   53 	    63 	 0.04517 	 0.00445 	 m..s
   30 	    64 	 0.04328 	 0.00448 	 m..s
   46 	    65 	 0.04469 	 0.00450 	 m..s
   17 	    66 	 0.04189 	 0.00453 	 m..s
   32 	    67 	 0.04332 	 0.00455 	 m..s
   66 	    68 	 0.04671 	 0.00458 	 m..s
   73 	    69 	 0.04756 	 0.00464 	 m..s
   43 	    70 	 0.04429 	 0.00466 	 m..s
   77 	    71 	 0.04876 	 0.00469 	 m..s
   35 	    72 	 0.04345 	 0.00472 	 m..s
   28 	    73 	 0.04312 	 0.00484 	 m..s
   29 	    74 	 0.04317 	 0.00486 	 m..s
   12 	    75 	 0.03947 	 0.00493 	 m..s
   65 	    76 	 0.04639 	 0.00515 	 m..s
   75 	    77 	 0.04851 	 0.00533 	 m..s
   63 	    78 	 0.04622 	 0.00545 	 m..s
   58 	    79 	 0.04552 	 0.00579 	 m..s
   91 	    80 	 0.06359 	 0.00601 	 m..s
   86 	    81 	 0.06163 	 0.00656 	 m..s
   90 	    82 	 0.06244 	 0.00729 	 m..s
   80 	    83 	 0.06140 	 0.00850 	 m..s
   85 	    84 	 0.06156 	 0.00858 	 m..s
   83 	    85 	 0.06146 	 0.01178 	 m..s
   81 	    86 	 0.06146 	 0.01206 	 m..s
   94 	    87 	 0.08210 	 0.01781 	 m..s
   87 	    88 	 0.06164 	 0.01930 	 m..s
   93 	    89 	 0.08111 	 0.01957 	 m..s
   84 	    90 	 0.06154 	 0.02050 	 m..s
   92 	    91 	 0.08079 	 0.02182 	 m..s
   82 	    92 	 0.06146 	 0.02455 	 m..s
   88 	    93 	 0.06166 	 0.02920 	 m..s
   89 	    94 	 0.06177 	 0.03130 	 m..s
   95 	    95 	 0.08583 	 0.04296 	 m..s
  102 	    96 	 0.15723 	 0.05291 	 MISS
   99 	    97 	 0.13769 	 0.13821 	 ~...
  103 	    98 	 0.15807 	 0.13846 	 ~...
   96 	    99 	 0.11758 	 0.14041 	 ~...
  101 	   100 	 0.15505 	 0.15098 	 ~...
  100 	   101 	 0.14218 	 0.15237 	 ~...
  107 	   102 	 0.17969 	 0.15473 	 ~...
  104 	   103 	 0.16200 	 0.17117 	 ~...
  106 	   104 	 0.17938 	 0.17383 	 ~...
  109 	   105 	 0.20491 	 0.19630 	 ~...
   97 	   106 	 0.13134 	 0.19794 	 m..s
   98 	   107 	 0.13160 	 0.19906 	 m..s
  115 	   108 	 0.27449 	 0.21732 	 m..s
  113 	   109 	 0.25832 	 0.23383 	 ~...
  114 	   110 	 0.26499 	 0.23552 	 ~...
  105 	   111 	 0.17225 	 0.23886 	 m..s
  112 	   112 	 0.23369 	 0.24879 	 ~...
  108 	   113 	 0.18473 	 0.26233 	 m..s
  120 	   114 	 0.31498 	 0.26513 	 m..s
  119 	   115 	 0.29698 	 0.27244 	 ~...
  116 	   116 	 0.28069 	 0.29612 	 ~...
  118 	   117 	 0.29398 	 0.29973 	 ~...
  111 	   118 	 0.21968 	 0.30213 	 m..s
  117 	   119 	 0.29083 	 0.31468 	 ~...
  110 	   120 	 0.21759 	 0.31567 	 m..s
==========================================
r_mrr = 0.9590129852294922
r2_mrr = 0.7709358334541321
spearmanr_mrr@5 = 0.8554524183273315
spearmanr_mrr@10 = 0.9644827246665955
spearmanr_mrr@50 = 0.9873320460319519
spearmanr_mrr@100 = 0.9912537336349487
spearmanr_mrr@All = 0.9911259412765503
==========================================
test time: 0.513
Done Testing dataset CoDExSmall
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.210 +- 0.222
mrr vals (pred, true): 0.108, 0.156

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   66 	     0 	 0.05055 	 0.00014 	 m..s
   29 	     1 	 0.05013 	 0.00016 	 m..s
   35 	     2 	 0.05019 	 0.00016 	 m..s
   47 	     3 	 0.05033 	 0.00017 	 m..s
   42 	     4 	 0.05031 	 0.00017 	 m..s
   44 	     5 	 0.05032 	 0.00017 	 m..s
   34 	     6 	 0.05015 	 0.00018 	 m..s
   63 	     7 	 0.05051 	 0.00018 	 m..s
   26 	     8 	 0.05011 	 0.00018 	 m..s
   54 	     9 	 0.05038 	 0.00018 	 m..s
   53 	    10 	 0.05038 	 0.00018 	 m..s
   33 	    11 	 0.05015 	 0.00018 	 m..s
   41 	    12 	 0.05028 	 0.00018 	 m..s
   39 	    13 	 0.05026 	 0.00019 	 m..s
   18 	    14 	 0.05002 	 0.00019 	 m..s
   61 	    15 	 0.05049 	 0.00019 	 m..s
   25 	    16 	 0.05010 	 0.00019 	 m..s
   16 	    17 	 0.04997 	 0.00020 	 m..s
   77 	    18 	 0.05093 	 0.00020 	 m..s
   13 	    19 	 0.04986 	 0.00020 	 m..s
   45 	    20 	 0.05032 	 0.00021 	 m..s
   31 	    21 	 0.05013 	 0.00021 	 m..s
   36 	    22 	 0.05021 	 0.00021 	 m..s
   19 	    23 	 0.05003 	 0.00021 	 m..s
   60 	    24 	 0.05047 	 0.00022 	 m..s
   74 	    25 	 0.05080 	 0.00022 	 m..s
   64 	    26 	 0.05052 	 0.00023 	 m..s
    2 	    27 	 0.04956 	 0.00023 	 m..s
    3 	    28 	 0.04966 	 0.00023 	 m..s
   14 	    29 	 0.04987 	 0.00024 	 m..s
    8 	    30 	 0.04984 	 0.00024 	 m..s
   20 	    31 	 0.05003 	 0.00024 	 m..s
   48 	    32 	 0.05033 	 0.00025 	 m..s
   46 	    33 	 0.05032 	 0.00025 	 m..s
   43 	    34 	 0.05031 	 0.00025 	 m..s
   21 	    35 	 0.05003 	 0.00026 	 m..s
   57 	    36 	 0.05041 	 0.00026 	 m..s
   51 	    37 	 0.05035 	 0.00026 	 m..s
   38 	    38 	 0.05025 	 0.00027 	 m..s
   78 	    39 	 0.05094 	 0.00027 	 m..s
   62 	    40 	 0.05049 	 0.00027 	 m..s
   37 	    41 	 0.05024 	 0.00027 	 m..s
   27 	    42 	 0.05011 	 0.00027 	 m..s
   59 	    43 	 0.05047 	 0.00028 	 m..s
   24 	    44 	 0.05008 	 0.00028 	 m..s
   70 	    45 	 0.05061 	 0.00028 	 m..s
   10 	    46 	 0.04985 	 0.00028 	 m..s
    7 	    47 	 0.04981 	 0.00028 	 m..s
   67 	    48 	 0.05056 	 0.00029 	 m..s
    5 	    49 	 0.04975 	 0.00029 	 m..s
   75 	    50 	 0.05084 	 0.00029 	 m..s
   69 	    51 	 0.05057 	 0.00030 	 m..s
   79 	    52 	 0.05126 	 0.00030 	 m..s
   49 	    53 	 0.05033 	 0.00030 	 m..s
   72 	    54 	 0.05074 	 0.00030 	 m..s
   15 	    55 	 0.04989 	 0.00031 	 m..s
   32 	    56 	 0.05014 	 0.00032 	 m..s
   23 	    57 	 0.05004 	 0.00032 	 m..s
   40 	    58 	 0.05026 	 0.00033 	 m..s
   11 	    59 	 0.04985 	 0.00033 	 m..s
   65 	    60 	 0.05054 	 0.00035 	 m..s
   50 	    61 	 0.05034 	 0.00038 	 m..s
   58 	    62 	 0.05045 	 0.00038 	 m..s
    0 	    63 	 0.04943 	 0.00038 	 m..s
    1 	    64 	 0.04945 	 0.00039 	 m..s
   73 	    65 	 0.05076 	 0.00041 	 m..s
   55 	    66 	 0.05039 	 0.00041 	 m..s
   17 	    67 	 0.05000 	 0.00044 	 m..s
    9 	    68 	 0.04985 	 0.00049 	 m..s
    6 	    69 	 0.04979 	 0.00053 	 m..s
   52 	    70 	 0.05036 	 0.00056 	 m..s
   68 	    71 	 0.05057 	 0.00059 	 m..s
    4 	    72 	 0.04969 	 0.00064 	 m..s
   22 	    73 	 0.05004 	 0.00066 	 m..s
   76 	    74 	 0.05085 	 0.00086 	 m..s
   30 	    75 	 0.05013 	 0.00091 	 m..s
   56 	    76 	 0.05040 	 0.00131 	 m..s
   28 	    77 	 0.05013 	 0.00160 	 m..s
   12 	    78 	 0.04986 	 0.00180 	 m..s
   71 	    79 	 0.05073 	 0.00304 	 m..s
   85 	    80 	 0.05777 	 0.05043 	 ~...
  105 	    81 	 0.09369 	 0.06844 	 ~...
   98 	    82 	 0.08617 	 0.06966 	 ~...
   91 	    83 	 0.06053 	 0.07097 	 ~...
   97 	    84 	 0.08596 	 0.07846 	 ~...
   95 	    85 	 0.08045 	 0.07934 	 ~...
   83 	    86 	 0.05773 	 0.08498 	 ~...
   84 	    87 	 0.05774 	 0.08844 	 m..s
   88 	    88 	 0.05791 	 0.08867 	 m..s
   82 	    89 	 0.05772 	 0.08873 	 m..s
   92 	    90 	 0.07449 	 0.09149 	 ~...
   96 	    91 	 0.08171 	 0.09373 	 ~...
  107 	    92 	 0.09544 	 0.09410 	 ~...
   80 	    93 	 0.05761 	 0.09666 	 m..s
  115 	    94 	 0.25361 	 0.10107 	 MISS
  100 	    95 	 0.08900 	 0.10764 	 ~...
  106 	    96 	 0.09380 	 0.11025 	 ~...
   90 	    97 	 0.05795 	 0.11100 	 m..s
   94 	    98 	 0.07857 	 0.11583 	 m..s
   93 	    99 	 0.07849 	 0.11842 	 m..s
   81 	   100 	 0.05769 	 0.11956 	 m..s
  103 	   101 	 0.09123 	 0.12240 	 m..s
  111 	   102 	 0.20184 	 0.12398 	 m..s
  104 	   103 	 0.09272 	 0.12498 	 m..s
   89 	   104 	 0.05791 	 0.12588 	 m..s
  101 	   105 	 0.08923 	 0.12754 	 m..s
   99 	   106 	 0.08731 	 0.12884 	 m..s
  102 	   107 	 0.08993 	 0.13146 	 m..s
   87 	   108 	 0.05787 	 0.13164 	 m..s
  112 	   109 	 0.21069 	 0.14319 	 m..s
  118 	   110 	 0.30895 	 0.14933 	 MISS
   86 	   111 	 0.05787 	 0.14954 	 m..s
  109 	   112 	 0.10760 	 0.15553 	 m..s
  108 	   113 	 0.10113 	 0.19855 	 m..s
  116 	   114 	 0.26542 	 0.20189 	 m..s
  110 	   115 	 0.19884 	 0.21574 	 ~...
  117 	   116 	 0.26638 	 0.25348 	 ~...
  114 	   117 	 0.23830 	 0.25855 	 ~...
  113 	   118 	 0.23264 	 0.28448 	 m..s
  120 	   119 	 0.37765 	 0.31455 	 m..s
  119 	   120 	 0.31651 	 0.34767 	 m..s
==========================================
r_mrr = 0.8259974122047424
r2_mrr = 0.5340467691421509
spearmanr_mrr@5 = 0.9739366173744202
spearmanr_mrr@10 = 0.9742792248725891
spearmanr_mrr@50 = 0.9130365252494812
spearmanr_mrr@100 = 0.9018653631210327
spearmanr_mrr@All = 0.903427004814148
==========================================
test time: 0.496
Done Testing dataset DBpedia50
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.008 +- 0.001
mrr vals (pred, true): 0.548, 0.511

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.04900 	 0.04547 	 ~...
   61 	     1 	 0.05401 	 0.04627 	 ~...
   24 	     2 	 0.04992 	 0.04721 	 ~...
    1 	     3 	 0.03982 	 0.04731 	 ~...
   79 	     4 	 0.05878 	 0.04931 	 ~...
   72 	     5 	 0.05677 	 0.04971 	 ~...
    3 	     6 	 0.04075 	 0.04973 	 ~...
   68 	     7 	 0.05534 	 0.05014 	 ~...
   39 	     8 	 0.05116 	 0.05028 	 ~...
   62 	     9 	 0.05427 	 0.05035 	 ~...
   33 	    10 	 0.05067 	 0.05046 	 ~...
   74 	    11 	 0.05709 	 0.05050 	 ~...
   57 	    12 	 0.05343 	 0.05062 	 ~...
   66 	    13 	 0.05522 	 0.05075 	 ~...
   49 	    14 	 0.05237 	 0.05096 	 ~...
   41 	    15 	 0.05151 	 0.05108 	 ~...
   19 	    16 	 0.04916 	 0.05109 	 ~...
   54 	    17 	 0.05270 	 0.05114 	 ~...
    0 	    18 	 0.03921 	 0.05116 	 ~...
   76 	    19 	 0.05733 	 0.05117 	 ~...
   15 	    20 	 0.04801 	 0.05135 	 ~...
   70 	    21 	 0.05630 	 0.05138 	 ~...
   78 	    22 	 0.05782 	 0.05138 	 ~...
    2 	    23 	 0.03996 	 0.05147 	 ~...
   46 	    24 	 0.05200 	 0.05154 	 ~...
    9 	    25 	 0.04531 	 0.05188 	 ~...
   52 	    26 	 0.05253 	 0.05194 	 ~...
   34 	    27 	 0.05084 	 0.05201 	 ~...
    6 	    28 	 0.04452 	 0.05216 	 ~...
   64 	    29 	 0.05488 	 0.05224 	 ~...
   29 	    30 	 0.05027 	 0.05225 	 ~...
   45 	    31 	 0.05195 	 0.05258 	 ~...
   14 	    32 	 0.04787 	 0.05275 	 ~...
   63 	    33 	 0.05466 	 0.05277 	 ~...
   71 	    34 	 0.05644 	 0.05279 	 ~...
   28 	    35 	 0.05021 	 0.05310 	 ~...
   40 	    36 	 0.05130 	 0.05313 	 ~...
   51 	    37 	 0.05253 	 0.05314 	 ~...
   26 	    38 	 0.05020 	 0.05316 	 ~...
   17 	    39 	 0.04841 	 0.05317 	 ~...
   73 	    40 	 0.05709 	 0.05320 	 ~...
   59 	    41 	 0.05381 	 0.05346 	 ~...
   48 	    42 	 0.05233 	 0.05350 	 ~...
   25 	    43 	 0.05006 	 0.05357 	 ~...
   31 	    44 	 0.05037 	 0.05358 	 ~...
   38 	    45 	 0.05109 	 0.05375 	 ~...
   22 	    46 	 0.04966 	 0.05412 	 ~...
    5 	    47 	 0.04391 	 0.05428 	 ~...
   13 	    48 	 0.04726 	 0.05429 	 ~...
   43 	    49 	 0.05181 	 0.05451 	 ~...
   27 	    50 	 0.05021 	 0.05455 	 ~...
   53 	    51 	 0.05259 	 0.05456 	 ~...
   20 	    52 	 0.04917 	 0.05464 	 ~...
   77 	    53 	 0.05749 	 0.05496 	 ~...
   42 	    54 	 0.05170 	 0.05498 	 ~...
   67 	    55 	 0.05532 	 0.05501 	 ~...
   60 	    56 	 0.05383 	 0.05528 	 ~...
   58 	    57 	 0.05370 	 0.05545 	 ~...
   37 	    58 	 0.05103 	 0.05566 	 ~...
   55 	    59 	 0.05280 	 0.05595 	 ~...
   35 	    60 	 0.05100 	 0.05614 	 ~...
   11 	    61 	 0.04640 	 0.05656 	 ~...
    4 	    62 	 0.04312 	 0.05664 	 ~...
   69 	    63 	 0.05618 	 0.05668 	 ~...
   50 	    64 	 0.05241 	 0.05685 	 ~...
   44 	    65 	 0.05187 	 0.05723 	 ~...
    8 	    66 	 0.04503 	 0.05733 	 ~...
    7 	    67 	 0.04501 	 0.05781 	 ~...
   56 	    68 	 0.05302 	 0.05814 	 ~...
   32 	    69 	 0.05051 	 0.05817 	 ~...
   10 	    70 	 0.04600 	 0.05817 	 ~...
   75 	    71 	 0.05713 	 0.05846 	 ~...
   47 	    72 	 0.05210 	 0.05875 	 ~...
   65 	    73 	 0.05489 	 0.05881 	 ~...
   21 	    74 	 0.04919 	 0.05888 	 ~...
   30 	    75 	 0.05036 	 0.05937 	 ~...
   16 	    76 	 0.04813 	 0.06012 	 ~...
   36 	    77 	 0.05100 	 0.06014 	 ~...
   23 	    78 	 0.04985 	 0.06053 	 ~...
   12 	    79 	 0.04678 	 0.06387 	 ~...
   93 	    80 	 0.25432 	 0.14588 	 MISS
   82 	    81 	 0.22398 	 0.18561 	 m..s
   80 	    82 	 0.21615 	 0.19332 	 ~...
   81 	    83 	 0.22048 	 0.20462 	 ~...
   87 	    84 	 0.23876 	 0.20491 	 m..s
   84 	    85 	 0.23002 	 0.21194 	 ~...
   88 	    86 	 0.23923 	 0.21207 	 ~...
   90 	    87 	 0.24121 	 0.21874 	 ~...
   89 	    88 	 0.23961 	 0.22043 	 ~...
   96 	    89 	 0.27947 	 0.24979 	 ~...
   85 	    90 	 0.23122 	 0.25657 	 ~...
   95 	    91 	 0.27273 	 0.26205 	 ~...
   86 	    92 	 0.23202 	 0.26361 	 m..s
   83 	    93 	 0.22883 	 0.27033 	 m..s
  105 	    94 	 0.37946 	 0.28083 	 m..s
  102 	    95 	 0.35900 	 0.28272 	 m..s
   94 	    96 	 0.26913 	 0.28388 	 ~...
   92 	    97 	 0.25358 	 0.28579 	 m..s
  104 	    98 	 0.37823 	 0.28999 	 m..s
   91 	    99 	 0.25325 	 0.29652 	 m..s
   99 	   100 	 0.32848 	 0.29748 	 m..s
  100 	   101 	 0.32850 	 0.30084 	 ~...
  103 	   102 	 0.36237 	 0.30840 	 m..s
   97 	   103 	 0.28974 	 0.30945 	 ~...
  101 	   104 	 0.35673 	 0.33811 	 ~...
  116 	   105 	 0.49801 	 0.33985 	 MISS
  106 	   106 	 0.38731 	 0.34292 	 m..s
   98 	   107 	 0.30535 	 0.34296 	 m..s
  107 	   108 	 0.42231 	 0.36022 	 m..s
  108 	   109 	 0.43314 	 0.37521 	 m..s
  113 	   110 	 0.49137 	 0.37760 	 MISS
  112 	   111 	 0.49135 	 0.38637 	 MISS
  110 	   112 	 0.45983 	 0.40509 	 m..s
  109 	   113 	 0.45420 	 0.41075 	 m..s
  115 	   114 	 0.49764 	 0.41730 	 m..s
  114 	   115 	 0.49754 	 0.45262 	 m..s
  111 	   116 	 0.47588 	 0.47935 	 ~...
  117 	   117 	 0.54787 	 0.51145 	 m..s
  118 	   118 	 0.61660 	 0.51473 	 MISS
  120 	   119 	 0.70756 	 0.60870 	 m..s
  119 	   120 	 0.64696 	 0.61786 	 ~...
==========================================
r_mrr = 0.9864001870155334
r2_mrr = 0.9388391971588135
spearmanr_mrr@5 = 0.9084029793739319
spearmanr_mrr@10 = 0.9376799464225769
spearmanr_mrr@50 = 0.9872539043426514
spearmanr_mrr@100 = 0.9941713213920593
spearmanr_mrr@All = 0.9947142004966736
==========================================
test time: 0.453
Done Testing dataset Kinships
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.248 +- 0.223
mrr vals (pred, true): 0.076, 0.140

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   60 	     0 	 0.04912 	 0.00043 	 m..s
   56 	     1 	 0.04905 	 0.00047 	 m..s
   46 	     2 	 0.04899 	 0.00049 	 m..s
   17 	     3 	 0.04870 	 0.00049 	 m..s
    0 	     4 	 0.04813 	 0.00049 	 m..s
   58 	     5 	 0.04906 	 0.00050 	 m..s
   35 	     6 	 0.04884 	 0.00051 	 m..s
   48 	     7 	 0.04901 	 0.00052 	 m..s
   34 	     8 	 0.04884 	 0.00052 	 m..s
    9 	     9 	 0.04850 	 0.00054 	 m..s
   29 	    10 	 0.04882 	 0.00055 	 m..s
   70 	    11 	 0.04931 	 0.00055 	 m..s
   43 	    12 	 0.04896 	 0.00055 	 m..s
    8 	    13 	 0.04845 	 0.00056 	 m..s
   22 	    14 	 0.04875 	 0.00056 	 m..s
   66 	    15 	 0.04921 	 0.00057 	 m..s
   13 	    16 	 0.04859 	 0.00058 	 m..s
   44 	    17 	 0.04897 	 0.00058 	 m..s
   16 	    18 	 0.04866 	 0.00059 	 m..s
   71 	    19 	 0.04932 	 0.00059 	 m..s
   32 	    20 	 0.04884 	 0.00060 	 m..s
   25 	    21 	 0.04876 	 0.00060 	 m..s
   26 	    22 	 0.04877 	 0.00060 	 m..s
   50 	    23 	 0.04902 	 0.00061 	 m..s
   33 	    24 	 0.04884 	 0.00062 	 m..s
   20 	    25 	 0.04871 	 0.00062 	 m..s
   77 	    26 	 0.04948 	 0.00063 	 m..s
   28 	    27 	 0.04880 	 0.00064 	 m..s
    5 	    28 	 0.04837 	 0.00069 	 m..s
    1 	    29 	 0.04814 	 0.00069 	 m..s
    6 	    30 	 0.04838 	 0.00070 	 m..s
   23 	    31 	 0.04875 	 0.00071 	 m..s
   10 	    32 	 0.04852 	 0.00072 	 m..s
   36 	    33 	 0.04886 	 0.00072 	 m..s
   31 	    34 	 0.04883 	 0.00074 	 m..s
   54 	    35 	 0.04905 	 0.00079 	 m..s
   74 	    36 	 0.04938 	 0.00081 	 m..s
   51 	    37 	 0.04903 	 0.00081 	 m..s
   38 	    38 	 0.04891 	 0.00081 	 m..s
   69 	    39 	 0.04927 	 0.00082 	 m..s
   55 	    40 	 0.04905 	 0.00084 	 m..s
   78 	    41 	 0.04953 	 0.00085 	 m..s
   47 	    42 	 0.04900 	 0.00085 	 m..s
   30 	    43 	 0.04883 	 0.00086 	 m..s
   19 	    44 	 0.04870 	 0.00087 	 m..s
   57 	    45 	 0.04905 	 0.00088 	 m..s
   12 	    46 	 0.04859 	 0.00090 	 m..s
   53 	    47 	 0.04904 	 0.00090 	 m..s
   68 	    48 	 0.04926 	 0.00093 	 m..s
   65 	    49 	 0.04921 	 0.00095 	 m..s
   21 	    50 	 0.04873 	 0.00095 	 m..s
    4 	    51 	 0.04837 	 0.00097 	 m..s
   39 	    52 	 0.04894 	 0.00098 	 m..s
   59 	    53 	 0.04907 	 0.00102 	 m..s
   15 	    54 	 0.04861 	 0.00106 	 m..s
   18 	    55 	 0.04870 	 0.00110 	 m..s
   73 	    56 	 0.04937 	 0.00112 	 m..s
   41 	    57 	 0.04895 	 0.00113 	 m..s
   62 	    58 	 0.04912 	 0.00115 	 m..s
   72 	    59 	 0.04934 	 0.00115 	 m..s
   63 	    60 	 0.04918 	 0.00116 	 m..s
   45 	    61 	 0.04899 	 0.00119 	 m..s
   67 	    62 	 0.04923 	 0.00119 	 m..s
   79 	    63 	 0.04989 	 0.00123 	 m..s
   11 	    64 	 0.04853 	 0.00131 	 m..s
   14 	    65 	 0.04860 	 0.00139 	 m..s
   40 	    66 	 0.04895 	 0.00139 	 m..s
   61 	    67 	 0.04912 	 0.00146 	 m..s
   37 	    68 	 0.04890 	 0.00148 	 m..s
   27 	    69 	 0.04880 	 0.00150 	 m..s
   64 	    70 	 0.04918 	 0.00154 	 m..s
   24 	    71 	 0.04876 	 0.00167 	 m..s
   75 	    72 	 0.04939 	 0.00167 	 m..s
   52 	    73 	 0.04903 	 0.00178 	 m..s
    3 	    74 	 0.04836 	 0.00181 	 m..s
   76 	    75 	 0.04945 	 0.00184 	 m..s
    7 	    76 	 0.04840 	 0.00193 	 m..s
   49 	    77 	 0.04902 	 0.00199 	 m..s
   42 	    78 	 0.04896 	 0.00208 	 m..s
    2 	    79 	 0.04836 	 0.00230 	 m..s
  109 	    80 	 0.09372 	 0.01712 	 m..s
   90 	    81 	 0.05757 	 0.04752 	 ~...
   92 	    82 	 0.06087 	 0.05524 	 ~...
  101 	    83 	 0.06820 	 0.06832 	 ~...
   93 	    84 	 0.06112 	 0.07041 	 ~...
   97 	    85 	 0.06376 	 0.07066 	 ~...
   91 	    86 	 0.05892 	 0.07113 	 ~...
  102 	    87 	 0.06827 	 0.07221 	 ~...
   89 	    88 	 0.05705 	 0.07228 	 ~...
   87 	    89 	 0.05562 	 0.07296 	 ~...
   80 	    90 	 0.05458 	 0.07469 	 ~...
  100 	    91 	 0.06752 	 0.07607 	 ~...
   86 	    92 	 0.05559 	 0.07732 	 ~...
   85 	    93 	 0.05546 	 0.07817 	 ~...
   88 	    94 	 0.05567 	 0.07886 	 ~...
   81 	    95 	 0.05458 	 0.08027 	 ~...
   98 	    96 	 0.06480 	 0.08351 	 ~...
   99 	    97 	 0.06663 	 0.08607 	 ~...
   95 	    98 	 0.06169 	 0.08811 	 ~...
   94 	    99 	 0.06144 	 0.08832 	 ~...
   96 	   100 	 0.06171 	 0.08873 	 ~...
  112 	   101 	 0.15966 	 0.08938 	 m..s
  105 	   102 	 0.08754 	 0.09658 	 ~...
  106 	   103 	 0.08774 	 0.09686 	 ~...
  107 	   104 	 0.08834 	 0.09711 	 ~...
  103 	   105 	 0.06914 	 0.10101 	 m..s
   82 	   106 	 0.05459 	 0.10497 	 m..s
  115 	   107 	 0.19961 	 0.12358 	 m..s
   84 	   108 	 0.05490 	 0.12857 	 m..s
  118 	   109 	 0.24399 	 0.12958 	 MISS
  111 	   110 	 0.15130 	 0.13256 	 ~...
   83 	   111 	 0.05488 	 0.13837 	 m..s
  104 	   112 	 0.07636 	 0.14022 	 m..s
  116 	   113 	 0.20721 	 0.16794 	 m..s
  110 	   114 	 0.14976 	 0.18649 	 m..s
  117 	   115 	 0.21023 	 0.19419 	 ~...
  108 	   116 	 0.09057 	 0.20396 	 MISS
  113 	   117 	 0.18237 	 0.24200 	 m..s
  114 	   118 	 0.18687 	 0.24510 	 m..s
  119 	   119 	 0.25201 	 0.31774 	 m..s
  120 	   120 	 0.30392 	 0.32174 	 ~...
==========================================
r_mrr = 0.8323495388031006
r2_mrr = 0.5104188919067383
spearmanr_mrr@5 = 0.8465469479560852
spearmanr_mrr@10 = 0.9505900740623474
spearmanr_mrr@50 = 0.9284716844558716
spearmanr_mrr@100 = 0.9155305624008179
spearmanr_mrr@All = 0.916264533996582
==========================================
test time: 0.437
Done Testing dataset OpenEA
total time taken: 1058.6169595718384
training time taken: 1003.417448759079
TWIG out ;))
================================================
------------------------------------------------
Running a TWIG experiment with tag: DistMult-all
------------------------------------------------
================================================
Using random seed: 9267945550633160
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [156, 652, 833, 74, 427, 114, 789, 690, 80, 424, 45, 239, 621, 87, 936, 964, 938, 1047, 60, 976, 237, 671, 905, 1169, 578, 554, 486, 293, 190, 638, 896, 183, 749, 967, 328, 777, 759, 767, 73, 622, 321, 734, 272, 631, 1086, 559, 22, 386, 757, 444, 94, 943, 985, 645, 17, 601, 439, 821, 706, 704, 1042, 1022, 674, 794, 612, 800, 940, 21, 855, 37, 464, 681, 1019, 1115, 220, 688, 229, 363, 900, 69, 965, 932, 468, 209, 1075, 443, 670, 834, 548, 57, 283, 316, 801, 194, 781, 946, 616, 491, 951, 885, 779, 236, 540, 423, 137, 886, 806, 614, 221, 412, 793, 260, 871, 85, 538, 154, 475, 765, 1038, 428, 648]
valid_ids (0): []
train_ids (1094): [219, 1139, 742, 599, 473, 110, 904, 994, 445, 729, 20, 1208, 82, 515, 27, 331, 498, 149, 176, 574, 879, 1101, 358, 276, 805, 889, 302, 107, 467, 571, 557, 958, 100, 682, 259, 705, 878, 665, 1121, 255, 476, 1136, 594, 118, 691, 161, 36, 24, 597, 214, 349, 343, 1097, 365, 758, 1007, 570, 434, 394, 783, 907, 1027, 1165, 268, 1214, 925, 1201, 772, 1070, 299, 482, 510, 99, 101, 673, 408, 495, 452, 54, 432, 569, 292, 142, 869, 136, 741, 150, 406, 301, 270, 497, 664, 1000, 753, 865, 731, 1106, 88, 860, 1, 414, 589, 314, 802, 1173, 1111, 167, 707, 733, 959, 509, 288, 61, 1034, 966, 62, 1009, 699, 961, 944, 581, 818, 714, 848, 754, 1163, 71, 545, 913, 315, 429, 58, 1142, 72, 1118, 972, 775, 1040, 551, 11, 591, 147, 992, 245, 294, 728, 745, 1155, 1093, 657, 722, 1184, 175, 1020, 890, 675, 677, 835, 650, 585, 103, 1077, 109, 344, 477, 208, 942, 508, 168, 297, 715, 1180, 12, 1004, 587, 135, 680, 191, 696, 1179, 819, 300, 25, 383, 556, 1073, 493, 33, 543, 243, 838, 198, 796, 212, 812, 317, 948, 1029, 1064, 178, 541, 903, 746, 874, 398, 854, 975, 151, 364, 380, 1085, 470, 647, 248, 108, 1151, 469, 384, 916, 500, 48, 1035, 105, 876, 1203, 791, 639, 1088, 1014, 866, 952, 341, 336, 843, 875, 899, 852, 713, 996, 572, 1084, 788, 484, 388, 590, 1198, 1063, 1137, 172, 43, 1162, 38, 999, 839, 877, 588, 131, 708, 817, 739, 703, 837, 256, 40, 1103, 991, 683, 141, 289, 1069, 790, 225, 602, 298, 455, 853, 660, 820, 461, 280, 340, 755, 417, 888, 90, 941, 979, 922, 1186, 446, 990, 264, 785, 735, 338, 216, 1065, 345, 418, 291, 307, 723, 144, 324, 931, 1120, 201, 768, 1041, 883, 954, 993, 1193, 1166, 596, 205, 53, 945, 522, 983, 1044, 584, 271, 898, 1132, 195, 128, 908, 1110, 170, 44, 1056, 196, 895, 1131, 823, 1122, 400, 447, 618, 89, 164, 1107, 756, 593, 351, 836, 257, 339, 1100, 751, 521, 182, 116, 238, 730, 120, 832, 687, 370, 752, 1011, 186, 106, 787, 369, 227, 960, 863, 1190, 651, 774, 1039, 1090, 595, 1116, 480, 513, 725, 911, 29, 984, 210, 1092, 719, 523, 957, 973, 778, 1099, 956, 466, 1117, 433, 873, 474, 371, 624, 113, 223, 121, 1053, 644, 356, 440, 346, 1127, 770, 304, 535, 797, 387, 656, 404, 313, 496, 327, 814, 1108, 698, 163, 558, 159, 376, 5, 641, 989, 330, 1114, 542, 532, 561, 617, 978, 526, 49, 654, 635, 962, 732, 576, 847, 598, 70, 132, 1010, 881, 15, 1170, 968, 115, 309, 251, 1033, 511, 279, 41, 92, 1006, 786, 529, 1015, 920, 372, 858, 668, 798, 472, 3, 65, 1005, 250, 1054, 1023, 64, 409, 531, 84, 586, 1094, 47, 1082, 174, 634, 970, 287, 1153, 988, 242, 8, 119, 367, 390, 503, 1018, 565, 534, 1112, 326, 28, 1026, 269, 633, 1049, 1174, 401, 816, 347, 810, 720, 357, 211, 75, 285, 487, 1016, 1187, 1071, 471, 737, 902, 413, 809, 55, 693, 930, 566, 520, 721, 718, 971, 868, 862, 710, 1043, 1194, 1025, 177, 389, 914, 342, 1001, 1089, 254, 66, 986, 1212, 685, 716, 773, 974, 919, 1191, 399, 934, 604, 295, 577, 96, 362, 188, 373, 519, 1143, 350, 935, 610, 235, 184, 296, 611, 923, 1154, 274, 662, 1119, 325, 488, 104, 152, 712, 1145, 206, 129, 766, 441, 407, 606, 9, 379, 1207, 568, 864, 844, 246, 1199, 516, 1185, 692, 921, 1072, 807, 42, 403, 667, 244, 736, 39, 536, 1130, 1076, 615, 1125, 1030, 360, 415, 953, 803, 1002, 166, 32, 335, 915, 146, 112, 160, 306, 743, 1197, 726, 547, 361, 258, 909, 1051, 6, 1164, 933, 139, 549, 1024, 1156, 1159, 421, 505, 459, 636, 14, 1061, 1095, 200, 218, 771, 880, 658, 228, 56, 567, 1096, 1175, 284, 748, 1176, 626, 275, 263, 1003, 747, 23, 391, 1192, 760, 861, 458, 411, 489, 1046, 1172, 666, 830, 655, 1152, 924, 784, 97, 607, 416, 431, 776, 906, 857, 525, 573, 892, 197, 241, 912, 1167, 240, 117, 867, 1055, 79, 897, 605, 977, 917, 173, 669, 35, 1147, 795, 499, 18, 81, 34, 1181, 663, 207, 140, 231, 609, 815, 1105, 449, 1048, 528, 204, 763, 537, 1008, 1177, 637, 348, 419, 512, 507, 286, 16, 290, 882, 148, 761, 213, 155, 91, 646, 762, 457, 171, 533, 872, 126, 518, 162, 744, 395, 969, 252, 67, 939, 193, 927, 124, 937, 145, 1168, 711, 1036, 435, 281, 2, 7, 661, 910, 133, 813, 804, 546, 125, 374, 562, 310, 623, 353, 426, 26, 0, 1157, 78, 918, 130, 366, 422, 368, 230, 870, 333, 265, 575, 1012, 579, 153, 19, 841, 628, 846, 10, 185, 603, 377, 202, 215, 143, 676, 273, 1133, 1126, 1109, 436, 524, 697, 319, 1066, 157, 506, 405, 552, 1062, 884, 59, 709, 352, 643, 527, 947, 355, 102, 1204, 485, 318, 694, 267, 68, 1189, 359, 901, 226, 560, 122, 1209, 850, 46, 437, 684, 203, 232, 1059, 247, 262, 829, 95, 450, 1150, 1195, 822, 1067, 180, 451, 1078, 701, 949, 769, 845, 1079, 997, 179, 826, 827, 583, 1161, 308, 630, 501, 266, 1213, 580, 1091, 393, 1141, 1144, 679, 600, 192, 397, 1037, 649, 123, 632, 1149, 514, 1104, 887, 530, 76, 550, 1146, 640, 653, 1202, 127, 1138, 492, 840, 385, 1124, 1028, 463, 1171, 1160, 738, 1129, 77, 553, 613, 481, 483, 217, 1017, 1200, 724, 627, 782, 402, 582, 929, 320, 30, 1211, 253, 686, 1057, 619, 1031, 625, 222, 1098, 158, 189, 1013, 86, 1182, 438, 544, 378, 824, 1178, 856, 1032, 998, 1045, 1074, 199, 332, 672, 539, 303, 410, 828, 629, 564, 1205, 555, 891, 4, 563, 504, 1058, 460, 1052, 1206, 608, 1068, 808, 859, 792, 750, 442, 1148, 689, 1134, 305, 740, 1140, 282, 851, 502, 702, 926, 52, 659, 334, 50, 98, 456, 381, 478, 1060, 1083, 430, 592, 462, 955, 642, 981, 842, 187, 224, 261, 620, 780, 392, 1158, 337, 312, 764, 490, 425, 695, 181, 465, 322, 13, 354, 1128, 1210, 311, 51, 494, 169, 517, 134, 323, 849, 982, 987, 727, 420, 893, 1050, 31, 1135, 479, 894, 831, 453, 278, 1123, 700, 277, 233, 1196, 382, 950, 1113, 63, 928, 448, 1102, 1183, 980, 111, 1188, 1021, 825, 329, 811, 1087, 799, 93, 717, 249, 83, 138, 375, 1081, 454, 1080, 678, 963, 995, 396, 165, 234]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2964617247419580
the save name prefix for this run is:  chkpt-ID_2964617247419580_tag_DistMult-all
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1'], 'CoDExSmall': ['2.1'], 'DBpedia50': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 365
rank avg (pred): 0.434 +- 0.001
mrr vals (pred, true): 0.017, 0.095
batch losses (mrrl, rdl): 0.0, 0.0003384161

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 798
rank avg (pred): 0.530 +- 0.023
mrr vals (pred, true): 0.014, 0.047
batch losses (mrrl, rdl): 0.0, 0.0002369431

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 346
rank avg (pred): 0.365 +- 0.060
mrr vals (pred, true): 0.021, 0.108
batch losses (mrrl, rdl): 0.0, 0.0001098998

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 640
rank avg (pred): 0.429 +- 0.139
mrr vals (pred, true): 0.019, 0.043
batch losses (mrrl, rdl): 0.0, 9.73528e-05

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 8
rank avg (pred): 0.099 +- 0.074
mrr vals (pred, true): 0.139, 0.534
batch losses (mrrl, rdl): 0.0, 8.90588e-05

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 12
rank avg (pred): 0.098 +- 0.087
mrr vals (pred, true): 0.216, 0.540
batch losses (mrrl, rdl): 0.0, 9.29973e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 788
rank avg (pred): 0.509 +- 0.282
mrr vals (pred, true): 0.024, 0.050
batch losses (mrrl, rdl): 0.0, 5.65432e-05

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1077
rank avg (pred): 0.065 +- 0.056
mrr vals (pred, true): 0.267, 0.541
batch losses (mrrl, rdl): 0.0, 2.01872e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 109
rank avg (pred): 0.438 +- 0.204
mrr vals (pred, true): 0.023, 0.083
batch losses (mrrl, rdl): 0.0, 0.0001807638

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 398
rank avg (pred): 0.415 +- 0.262
mrr vals (pred, true): 0.032, 0.102
batch losses (mrrl, rdl): 0.0, 0.0001600069

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 38
rank avg (pred): 0.097 +- 0.094
mrr vals (pred, true): 0.317, 0.534
batch losses (mrrl, rdl): 0.0, 8.3951e-05

Epoch over!
epoch time: 68.855

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 102
rank avg (pred): 0.426 +- 0.234
mrr vals (pred, true): 0.026, 0.085
batch losses (mrrl, rdl): 0.0, 0.000131424

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 353
rank avg (pred): 0.369 +- 0.294
mrr vals (pred, true): 0.072, 0.096
batch losses (mrrl, rdl): 0.0, 0.0001067373

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 524
rank avg (pred): 0.305 +- 0.288
mrr vals (pred, true): 0.165, 0.105
batch losses (mrrl, rdl): 0.0, 7.9935e-06

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 959
rank avg (pred): 0.677 +- 0.217
mrr vals (pred, true): 0.013, 0.050
batch losses (mrrl, rdl): 0.0, 0.0009406932

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 720
rank avg (pred): 0.444 +- 0.257
mrr vals (pred, true): 0.028, 0.045
batch losses (mrrl, rdl): 0.0, 2.0658e-06

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 796
rank avg (pred): 0.398 +- 0.286
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 0.0, 1.47176e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1206
rank avg (pred): 0.442 +- 0.301
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 0.0, 7.157e-06

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 174
rank avg (pred): 0.374 +- 0.322
mrr vals (pred, true): 0.129, 0.048
batch losses (mrrl, rdl): 0.0, 6.61363e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 514
rank avg (pred): 0.234 +- 0.260
mrr vals (pred, true): 0.258, 0.072
batch losses (mrrl, rdl): 0.0, 0.0003592953

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1168
rank avg (pred): 0.385 +- 0.278
mrr vals (pred, true): 0.081, 0.045
batch losses (mrrl, rdl): 0.0, 8.86989e-05

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 530
rank avg (pred): 0.261 +- 0.284
mrr vals (pred, true): 0.243, 0.101
batch losses (mrrl, rdl): 0.0, 8.33983e-05

Epoch over!
epoch time: 68.949

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 115
rank avg (pred): 0.368 +- 0.269
mrr vals (pred, true): 0.070, 0.094
batch losses (mrrl, rdl): 0.0, 9.00023e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1159
rank avg (pred): 0.273 +- 0.294
mrr vals (pred, true): 0.229, 0.127
batch losses (mrrl, rdl): 0.0, 3.129e-05

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 960
rank avg (pred): 0.573 +- 0.220
mrr vals (pred, true): 0.019, 0.044
batch losses (mrrl, rdl): 0.0, 0.0002543301

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 914
rank avg (pred): 0.409 +- 0.293
mrr vals (pred, true): 0.069, 0.057
batch losses (mrrl, rdl): 0.0, 8.2369e-06

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 571
rank avg (pred): 0.449 +- 0.245
mrr vals (pred, true): 0.036, 0.030
batch losses (mrrl, rdl): 0.0, 7.07137e-05

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 202
rank avg (pred): 0.360 +- 0.279
mrr vals (pred, true): 0.081, 0.050
batch losses (mrrl, rdl): 0.0, 5.44911e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 603
rank avg (pred): 0.442 +- 0.289
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0, 2.22834e-05

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 813
rank avg (pred): 0.186 +- 0.143
mrr vals (pred, true): 0.126, 0.232
batch losses (mrrl, rdl): 0.0, 4.00054e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1202
rank avg (pred): 0.374 +- 0.320
mrr vals (pred, true): 0.122, 0.043
batch losses (mrrl, rdl): 0.0, 7.33654e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 901
rank avg (pred): 0.422 +- 0.397
mrr vals (pred, true): 0.125, 0.077
batch losses (mrrl, rdl): 0.0, 8.17994e-05

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 352
rank avg (pred): 0.415 +- 0.285
mrr vals (pred, true): 0.062, 0.093
batch losses (mrrl, rdl): 0.0, 0.0001935249

Epoch over!
epoch time: 72.021

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 694
rank avg (pred): 0.426 +- 0.284
mrr vals (pred, true): 0.060, 0.052
batch losses (mrrl, rdl): 0.0, 5.0381e-06

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 418
rank avg (pred): 0.392 +- 0.261
mrr vals (pred, true): 0.058, 0.045
batch losses (mrrl, rdl): 0.0, 5.27129e-05

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 35
rank avg (pred): 0.075 +- 0.081
mrr vals (pred, true): 0.366, 0.541
batch losses (mrrl, rdl): 0.0, 3.69546e-05

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 640
rank avg (pred): 0.380 +- 0.297
mrr vals (pred, true): 0.102, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001307445

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1177
rank avg (pred): 0.450 +- 0.291
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 0.0, 2.8796e-06

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 467
rank avg (pred): 0.322 +- 0.279
mrr vals (pred, true): 0.134, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001210411

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 973
rank avg (pred): 0.054 +- 0.053
mrr vals (pred, true): 0.356, 0.535
batch losses (mrrl, rdl): 0.0, 6.4123e-06

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 570
rank avg (pred): 0.409 +- 0.282
mrr vals (pred, true): 0.075, 0.046
batch losses (mrrl, rdl): 0.0, 4.84844e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 934
rank avg (pred): 0.603 +- 0.309
mrr vals (pred, true): 0.026, 0.016
batch losses (mrrl, rdl): 0.0, 0.0005034924

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 453
rank avg (pred): 0.388 +- 0.278
mrr vals (pred, true): 0.078, 0.044
batch losses (mrrl, rdl): 0.0, 1.61511e-05

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 462
rank avg (pred): 0.354 +- 0.292
mrr vals (pred, true): 0.120, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001233821

Epoch over!
epoch time: 70.747

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 730
rank avg (pred): 0.113 +- 0.106
mrr vals (pred, true): 0.263, 0.370
batch losses (mrrl, rdl): 0.0, 3.77743e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 8
rank avg (pred): 0.026 +- 0.032
mrr vals (pred, true): 0.392, 0.534
batch losses (mrrl, rdl): 0.0, 2.2162e-06

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1050
rank avg (pred): 0.354 +- 0.290
mrr vals (pred, true): 0.130, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001539769

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 864
rank avg (pred): 0.441 +- 0.258
mrr vals (pred, true): 0.076, 0.052
batch losses (mrrl, rdl): 0.0, 1.24474e-05

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 891
rank avg (pred): 0.450 +- 0.361
mrr vals (pred, true): 0.085, 0.054
batch losses (mrrl, rdl): 0.0, 4.53956e-05

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 844
rank avg (pred): 0.544 +- 0.283
mrr vals (pred, true): 0.036, 0.045
batch losses (mrrl, rdl): 0.0, 3.77635e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 256
rank avg (pred): 0.031 +- 0.044
mrr vals (pred, true): 0.414, 0.569
batch losses (mrrl, rdl): 0.0, 2.61e-08

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 887
rank avg (pred): 0.442 +- 0.264
mrr vals (pred, true): 0.068, 0.038
batch losses (mrrl, rdl): 0.0, 1.6764e-06

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 539
rank avg (pred): 0.390 +- 0.422
mrr vals (pred, true): 0.225, 0.100
batch losses (mrrl, rdl): 0.0, 0.0001389874

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1108
rank avg (pred): 0.314 +- 0.280
mrr vals (pred, true): 0.177, 0.046
batch losses (mrrl, rdl): 0.0, 0.0002369739

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 100
rank avg (pred): 0.338 +- 0.251
mrr vals (pred, true): 0.108, 0.119
batch losses (mrrl, rdl): 0.0, 4.91353e-05

Epoch over!
epoch time: 67.746

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 320
rank avg (pred): 0.042 +- 0.049
mrr vals (pred, true): 0.320, 0.555
batch losses (mrrl, rdl): 0.5518283844, 1.468e-06

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 998
rank avg (pred): 0.067 +- 0.108
mrr vals (pred, true): 0.557, 0.555
batch losses (mrrl, rdl): 5.84289e-05, 3.41518e-05

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 593
rank avg (pred): 0.275 +- 0.136
mrr vals (pred, true): 0.076, 0.042
batch losses (mrrl, rdl): 0.0066089635, 0.0008819372

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 791
rank avg (pred): 0.515 +- 0.246
mrr vals (pred, true): 0.043, 0.050
batch losses (mrrl, rdl): 0.0005581761, 0.0001123816

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 53
rank avg (pred): 0.120 +- 0.164
mrr vals (pred, true): 0.536, 0.530
batch losses (mrrl, rdl): 0.0003480256, 0.0001838728

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 485
rank avg (pred): 0.330 +- 0.111
mrr vals (pred, true): 0.038, 0.047
batch losses (mrrl, rdl): 0.0015017485, 0.0002091184

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 121
rank avg (pred): 0.304 +- 0.121
mrr vals (pred, true): 0.052, 0.086
batch losses (mrrl, rdl): 2.85658e-05, 6.01701e-05

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 91
rank avg (pred): 0.295 +- 0.119
mrr vals (pred, true): 0.061, 0.103
batch losses (mrrl, rdl): 0.0169224516, 4.7526e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1001
rank avg (pred): 0.290 +- 0.122
mrr vals (pred, true): 0.065, 0.124
batch losses (mrrl, rdl): 0.0342376158, 4.89208e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 274
rank avg (pred): 0.109 +- 0.154
mrr vals (pred, true): 0.567, 0.547
batch losses (mrrl, rdl): 0.0039728829, 0.0001600032

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1039
rank avg (pred): 0.284 +- 0.118
mrr vals (pred, true): 0.074, 0.044
batch losses (mrrl, rdl): 0.0056358995, 0.0005242961

Epoch over!
epoch time: 65.539

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 99
rank avg (pred): 0.265 +- 0.116
mrr vals (pred, true): 0.076, 0.130
batch losses (mrrl, rdl): 0.0283481106, 4.98946e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 772
rank avg (pred): 0.462 +- 0.199
mrr vals (pred, true): 0.042, 0.057
batch losses (mrrl, rdl): 0.0006885832, 0.0001505258

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 54
rank avg (pred): 0.129 +- 0.155
mrr vals (pred, true): 0.537, 0.528
batch losses (mrrl, rdl): 0.0008342986, 0.0002109319

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 623
rank avg (pred): 0.270 +- 0.101
mrr vals (pred, true): 0.073, 0.042
batch losses (mrrl, rdl): 0.0053567351, 0.0009864949

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 197
rank avg (pred): 0.289 +- 0.099
mrr vals (pred, true): 0.065, 0.050
batch losses (mrrl, rdl): 0.0021968866, 0.000445541

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1000
rank avg (pred): 0.311 +- 0.096
mrr vals (pred, true): 0.057, 0.086
batch losses (mrrl, rdl): 0.0005473071, 0.0001007279

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 854
rank avg (pred): 0.315 +- 0.095
mrr vals (pred, true): 0.048, 0.114
batch losses (mrrl, rdl): 0.0431473516, 0.0006981391

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 379
rank avg (pred): 0.307 +- 0.125
mrr vals (pred, true): 0.080, 0.112
batch losses (mrrl, rdl): 0.0100577772, 4.20536e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 715
rank avg (pred): 0.385 +- 0.188
mrr vals (pred, true): 0.081, 0.050
batch losses (mrrl, rdl): 0.0096893199, 0.0001150889

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 203
rank avg (pred): 0.282 +- 0.108
mrr vals (pred, true): 0.077, 0.050
batch losses (mrrl, rdl): 0.0071796221, 0.0005239256

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 495
rank avg (pred): 0.148 +- 0.106
mrr vals (pred, true): 0.130, 0.166
batch losses (mrrl, rdl): 0.0134245167, 0.0002680717

Epoch over!
epoch time: 65.79

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 97
rank avg (pred): 0.325 +- 0.158
mrr vals (pred, true): 0.070, 0.142
batch losses (mrrl, rdl): 0.0528722405, 8.71102e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 581
rank avg (pred): 0.394 +- 0.224
mrr vals (pred, true): 0.069, 0.039
batch losses (mrrl, rdl): 0.0035564823, 0.0001783401

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 666
rank avg (pred): 0.485 +- 0.276
mrr vals (pred, true): 0.058, 0.047
batch losses (mrrl, rdl): 0.0005933682, 5.23965e-05

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 461
rank avg (pred): 0.411 +- 0.271
mrr vals (pred, true): 0.067, 0.050
batch losses (mrrl, rdl): 0.0028786121, 2.38466e-05

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 364
rank avg (pred): 0.384 +- 0.229
mrr vals (pred, true): 0.052, 0.107
batch losses (mrrl, rdl): 0.0302451365, 9.31308e-05

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1197
rank avg (pred): 0.396 +- 0.274
mrr vals (pred, true): 0.082, 0.045
batch losses (mrrl, rdl): 0.0105567146, 4.09566e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 347
rank avg (pred): 0.350 +- 0.211
mrr vals (pred, true): 0.062, 0.114
batch losses (mrrl, rdl): 0.0266189296, 6.64132e-05

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 815
rank avg (pred): 0.105 +- 0.117
mrr vals (pred, true): 0.272, 0.166
batch losses (mrrl, rdl): 0.1132827923, 0.0010295238

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 218
rank avg (pred): 0.467 +- 0.278
mrr vals (pred, true): 0.067, 0.051
batch losses (mrrl, rdl): 0.0028331615, 8.6658e-06

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1132
rank avg (pred): 0.370 +- 0.225
mrr vals (pred, true): 0.069, 0.044
batch losses (mrrl, rdl): 0.0037032566, 9.38247e-05

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 390
rank avg (pred): 0.341 +- 0.207
mrr vals (pred, true): 0.075, 0.134
batch losses (mrrl, rdl): 0.0341712795, 0.0001122358

Epoch over!
epoch time: 66.578

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1202
rank avg (pred): 0.495 +- 0.286
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 6.79184e-05, 4.06752e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 658
rank avg (pred): 0.511 +- 0.314
mrr vals (pred, true): 0.068, 0.046
batch losses (mrrl, rdl): 0.0031762745, 0.0001039527

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 329
rank avg (pred): 0.340 +- 0.245
mrr vals (pred, true): 0.071, 0.162
batch losses (mrrl, rdl): 0.0834212899, 0.00014435

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 206
rank avg (pred): 0.340 +- 0.247
mrr vals (pred, true): 0.074, 0.049
batch losses (mrrl, rdl): 0.0058100186, 0.0001707641

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 458
rank avg (pred): 0.326 +- 0.227
mrr vals (pred, true): 0.067, 0.041
batch losses (mrrl, rdl): 0.0030267029, 0.0002195892

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 723
rank avg (pred): 0.437 +- 0.285
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001247052, 1.1347e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 508
rank avg (pred): 0.108 +- 0.087
mrr vals (pred, true): 0.229, 0.208
batch losses (mrrl, rdl): 0.0047067381, 0.000321585

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 617
rank avg (pred): 0.372 +- 0.294
mrr vals (pred, true): 0.057, 0.043
batch losses (mrrl, rdl): 0.0005243769, 0.0002696368

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1064
rank avg (pred): 0.077 +- 0.093
mrr vals (pred, true): 0.537, 0.552
batch losses (mrrl, rdl): 0.0024659238, 4.59282e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 56
rank avg (pred): 0.074 +- 0.091
mrr vals (pred, true): 0.523, 0.523
batch losses (mrrl, rdl): 1.639e-07, 3.77399e-05

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 838
rank avg (pred): 0.371 +- 0.278
mrr vals (pred, true): 0.044, 0.061
batch losses (mrrl, rdl): 0.0003472424, 7.87891e-05

Epoch over!
epoch time: 65.811

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 805
rank avg (pred): 0.381 +- 0.325
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 6.7756e-06, 0.0001807039

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 899
rank avg (pred): 0.331 +- 0.204
mrr vals (pred, true): 0.063, 0.038
batch losses (mrrl, rdl): 0.0016959913, 0.0002906057

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 155
rank avg (pred): 0.350 +- 0.275
mrr vals (pred, true): 0.065, 0.133
batch losses (mrrl, rdl): 0.0463420376, 0.0002259165

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 877
rank avg (pred): 0.411 +- 0.359
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0001628119, 0.0001399562

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 105
rank avg (pred): 0.264 +- 0.192
mrr vals (pred, true): 0.069, 0.097
batch losses (mrrl, rdl): 0.0037541143, 0.0001080615

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 879
rank avg (pred): 0.388 +- 0.314
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 9.41729e-05, 7.8476e-05

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 199
rank avg (pred): 0.277 +- 0.236
mrr vals (pred, true): 0.079, 0.052
batch losses (mrrl, rdl): 0.0086254217, 0.0004769355

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1018
rank avg (pred): 0.324 +- 0.281
mrr vals (pred, true): 0.064, 0.073
batch losses (mrrl, rdl): 0.0019397403, 3.86717e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1067
rank avg (pred): 0.062 +- 0.077
mrr vals (pred, true): 0.549, 0.545
batch losses (mrrl, rdl): 0.0001818675, 1.77563e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 320
rank avg (pred): 0.061 +- 0.074
mrr vals (pred, true): 0.538, 0.555
batch losses (mrrl, rdl): 0.002986328, 1.86544e-05

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 618
rank avg (pred): 0.356 +- 0.324
mrr vals (pred, true): 0.056, 0.048
batch losses (mrrl, rdl): 0.0004146878, 0.0003124491

Epoch over!
epoch time: 65.317

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 876
rank avg (pred): 0.649 +- 0.399
mrr vals (pred, true): 0.037, 0.049
batch losses (mrrl, rdl): 0.0017810741, 0.0007343313

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 551
rank avg (pred): 0.375 +- 0.364
mrr vals (pred, true): 0.079, 0.105
batch losses (mrrl, rdl): 0.0063866554, 3.87266e-05

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 454
rank avg (pred): 0.257 +- 0.214
mrr vals (pred, true): 0.083, 0.047
batch losses (mrrl, rdl): 0.0108268224, 0.0005970885

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1123
rank avg (pred): 0.154 +- 0.083
mrr vals (pred, true): 0.109, 0.047
batch losses (mrrl, rdl): 0.0342822522, 0.0019034935

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 575
rank avg (pred): 0.449 +- 0.394
mrr vals (pred, true): 0.053, 0.037
batch losses (mrrl, rdl): 6.7938e-05, 0.0002403625

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 175
rank avg (pred): 0.289 +- 0.266
mrr vals (pred, true): 0.066, 0.055
batch losses (mrrl, rdl): 0.0026765021, 0.0003300545

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 42
rank avg (pred): 0.053 +- 0.061
mrr vals (pred, true): 0.486, 0.531
batch losses (mrrl, rdl): 0.0197031796, 1.8741e-06

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1141
rank avg (pred): 0.072 +- 0.061
mrr vals (pred, true): 0.348, 0.225
batch losses (mrrl, rdl): 0.1509002894, 0.0005148749

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1103
rank avg (pred): 0.368 +- 0.350
mrr vals (pred, true): 0.053, 0.088
batch losses (mrrl, rdl): 6.30736e-05, 5.17664e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1031
rank avg (pred): 0.302 +- 0.283
mrr vals (pred, true): 0.071, 0.041
batch losses (mrrl, rdl): 0.004494702, 0.0005047552

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 644
rank avg (pred): 0.415 +- 0.394
mrr vals (pred, true): 0.063, 0.035
batch losses (mrrl, rdl): 0.0017615524, 0.000265642

Epoch over!
epoch time: 65.195

Epoch 7 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 791
rank avg (pred): 0.475 +- 0.391
mrr vals (pred, true): 0.038, 0.050
batch losses (mrrl, rdl): 0.0015505757, 0.0001079631

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1109
rank avg (pred): 0.405 +- 0.379
mrr vals (pred, true): 0.061, 0.049
batch losses (mrrl, rdl): 0.0011208203, 0.0001422428

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 718
rank avg (pred): 0.407 +- 0.405
mrr vals (pred, true): 0.078, 0.047
batch losses (mrrl, rdl): 0.007733536, 0.0001755407

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 72
rank avg (pred): 0.050 +- 0.058
mrr vals (pred, true): 0.503, 0.512
batch losses (mrrl, rdl): 0.0009548272, 5.444e-07

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 778
rank avg (pred): 0.464 +- 0.429
mrr vals (pred, true): 0.049, 0.035
batch losses (mrrl, rdl): 6.988e-06, 0.0002824918

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 996
rank avg (pred): 0.044 +- 0.054
mrr vals (pred, true): 0.581, 0.557
batch losses (mrrl, rdl): 0.0056004967, 3.3554e-06

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 136
rank avg (pred): 0.296 +- 0.321
mrr vals (pred, true): 0.071, 0.080
batch losses (mrrl, rdl): 0.0044801757, 7.06129e-05

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 458
rank avg (pred): 0.277 +- 0.264
mrr vals (pred, true): 0.070, 0.041
batch losses (mrrl, rdl): 0.0040055029, 0.0004260905

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1091
rank avg (pred): 0.327 +- 0.334
mrr vals (pred, true): 0.068, 0.096
batch losses (mrrl, rdl): 0.0032720249, 4.99976e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 534
rank avg (pred): 0.283 +- 0.302
mrr vals (pred, true): 0.084, 0.050
batch losses (mrrl, rdl): 0.011492325, 0.0005706711

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 846
rank avg (pred): 0.438 +- 0.421
mrr vals (pred, true): 0.054, 0.046
batch losses (mrrl, rdl): 0.0001373229, 0.0002167878

Epoch over!
epoch time: 64.203

Epoch 8 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1090
rank avg (pred): 0.346 +- 0.358
mrr vals (pred, true): 0.071, 0.089
batch losses (mrrl, rdl): 0.004516717, 5.92366e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1071
rank avg (pred): 0.044 +- 0.056
mrr vals (pred, true): 0.587, 0.540
batch losses (mrrl, rdl): 0.0227850284, 2.1433e-06

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 952
rank avg (pred): 0.550 +- 0.381
mrr vals (pred, true): 0.039, 0.045
batch losses (mrrl, rdl): 0.0013167514, 0.0001777813

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 636
rank avg (pred): 0.389 +- 0.397
mrr vals (pred, true): 0.070, 0.041
batch losses (mrrl, rdl): 0.004117182, 0.0003125067

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 903
rank avg (pred): 0.450 +- 0.429
mrr vals (pred, true): 0.071, 0.055
batch losses (mrrl, rdl): 0.0044960985, 0.000200285

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 10
rank avg (pred): 0.048 +- 0.055
mrr vals (pred, true): 0.508, 0.545
batch losses (mrrl, rdl): 0.0137378098, 3.7229e-06

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 805
rank avg (pred): 0.469 +- 0.431
mrr vals (pred, true): 0.057, 0.042
batch losses (mrrl, rdl): 0.0005425859, 0.000204658

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1044
rank avg (pred): 0.330 +- 0.348
mrr vals (pred, true): 0.074, 0.040
batch losses (mrrl, rdl): 0.0055584866, 0.0003508804

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 862
rank avg (pred): 0.472 +- 0.407
mrr vals (pred, true): 0.053, 0.106
batch losses (mrrl, rdl): 0.0276561808, 8.0244e-05

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 270
rank avg (pred): 0.042 +- 0.050
mrr vals (pred, true): 0.549, 0.553
batch losses (mrrl, rdl): 0.0001170534, 2.886e-07

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 198
rank avg (pred): 0.253 +- 0.254
mrr vals (pred, true): 0.087, 0.051
batch losses (mrrl, rdl): 0.0134989992, 0.0006152869

Epoch over!
epoch time: 65.124

Epoch 9 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 314
rank avg (pred): 0.042 +- 0.050
mrr vals (pred, true): 0.533, 0.563
batch losses (mrrl, rdl): 0.0092699481, 2.0819e-06

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 292
rank avg (pred): 0.040 +- 0.048
mrr vals (pred, true): 0.549, 0.544
batch losses (mrrl, rdl): 0.0002688307, 4.215e-07

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 130
rank avg (pred): 0.290 +- 0.283
mrr vals (pred, true): 0.084, 0.107
batch losses (mrrl, rdl): 0.0052358694, 6.4368e-06

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1093
rank avg (pred): 0.406 +- 0.363
mrr vals (pred, true): 0.067, 0.091
batch losses (mrrl, rdl): 0.0030336417, 0.0001148135

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 513
rank avg (pred): 0.346 +- 0.388
mrr vals (pred, true): 0.066, 0.065
batch losses (mrrl, rdl): 0.0025396105, 0.0002346168

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1006
rank avg (pred): 0.365 +- 0.390
mrr vals (pred, true): 0.062, 0.154
batch losses (mrrl, rdl): 0.0843452364, 0.0002977068

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 948
rank avg (pred): 0.627 +- 0.422
mrr vals (pred, true): 0.042, 0.050
batch losses (mrrl, rdl): 0.0006547017, 0.0007168505

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1015
rank avg (pred): 0.300 +- 0.343
mrr vals (pred, true): 0.076, 0.131
batch losses (mrrl, rdl): 0.0299441051, 6.61264e-05

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 917
rank avg (pred): 0.661 +- 0.441
mrr vals (pred, true): 0.044, 0.068
batch losses (mrrl, rdl): 0.0003223124, 0.0007587731

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 160
rank avg (pred): 0.501 +- 0.420
mrr vals (pred, true): 0.064, 0.082
batch losses (mrrl, rdl): 0.0019967924, 0.0004822664

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 341
rank avg (pred): 0.529 +- 0.442
mrr vals (pred, true): 0.057, 0.178
batch losses (mrrl, rdl): 0.144729197, 0.0014650982

Epoch over!
epoch time: 65.785

Epoch 10 -- 
running batch: 0 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 750
rank avg (pred): 0.039 +- 0.044
mrr vals (pred, true): 0.466, 0.408
batch losses (mrrl, rdl): 0.0332310684, 3.52584e-05

running batch: 500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 958
rank avg (pred): 0.649 +- 0.445
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0001599747, 0.0008180137

running batch: 1000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 70
rank avg (pred): 0.038 +- 0.043
mrr vals (pred, true): 0.475, 0.535
batch losses (mrrl, rdl): 0.0352940001, 1.0443e-06

running batch: 1500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 948
rank avg (pred): 0.552 +- 0.406
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 2.2962e-06, 0.0002934973

running batch: 2000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 164
rank avg (pred): 0.307 +- 0.356
mrr vals (pred, true): 0.081, 0.047
batch losses (mrrl, rdl): 0.0098201344, 0.0004719307

running batch: 2500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 595
rank avg (pred): 0.394 +- 0.420
mrr vals (pred, true): 0.066, 0.037
batch losses (mrrl, rdl): 0.0026670685, 0.000451316

running batch: 3000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 299
rank avg (pred): 0.035 +- 0.042
mrr vals (pred, true): 0.576, 0.555
batch losses (mrrl, rdl): 0.0043543624, 1.254e-07

running batch: 3500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1119
rank avg (pred): 0.372 +- 0.406
mrr vals (pred, true): 0.076, 0.052
batch losses (mrrl, rdl): 0.006831754, 0.0002917444

running batch: 4000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 228
rank avg (pred): 0.363 +- 0.397
mrr vals (pred, true): 0.074, 0.044
batch losses (mrrl, rdl): 0.0058478219, 0.0002709742

running batch: 4500 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 815
rank avg (pred): 0.045 +- 0.033
mrr vals (pred, true): 0.228, 0.166
batch losses (mrrl, rdl): 0.037965335, 0.0017547631

running batch: 5000 / 5470 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 715
rank avg (pred): 0.657 +- 0.402
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 4.02134e-05, 0.0007316865

Epoch over!
epoch time: 65.802

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.244 +- 0.272
mrr vals (pred, true): 0.097, 0.104

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04965 	 0.01514 	 m..s
    4 	     1 	 0.04927 	 0.01667 	 m..s
   20 	     2 	 0.05140 	 0.01767 	 m..s
   23 	     3 	 0.05241 	 0.01822 	 m..s
   11 	     4 	 0.05007 	 0.02085 	 ~...
   34 	     5 	 0.06260 	 0.02127 	 m..s
   19 	     6 	 0.05131 	 0.02690 	 ~...
    8 	     7 	 0.04994 	 0.03392 	 ~...
   51 	     8 	 0.07340 	 0.03421 	 m..s
   10 	     9 	 0.05006 	 0.03570 	 ~...
   43 	    10 	 0.06938 	 0.03716 	 m..s
   93 	    11 	 0.10969 	 0.03858 	 m..s
    3 	    12 	 0.04893 	 0.03872 	 ~...
   81 	    13 	 0.08847 	 0.03921 	 m..s
   65 	    14 	 0.08156 	 0.03934 	 m..s
   62 	    15 	 0.08114 	 0.03938 	 m..s
   44 	    16 	 0.06984 	 0.04097 	 ~...
   72 	    17 	 0.08494 	 0.04100 	 m..s
    2 	    18 	 0.04800 	 0.04145 	 ~...
   41 	    19 	 0.06818 	 0.04176 	 ~...
   17 	    20 	 0.05120 	 0.04184 	 ~...
   40 	    21 	 0.06813 	 0.04194 	 ~...
   90 	    22 	 0.09571 	 0.04204 	 m..s
   63 	    23 	 0.08140 	 0.04216 	 m..s
   64 	    24 	 0.08143 	 0.04229 	 m..s
   24 	    25 	 0.05263 	 0.04294 	 ~...
   59 	    26 	 0.07879 	 0.04308 	 m..s
   69 	    27 	 0.08337 	 0.04342 	 m..s
   25 	    28 	 0.05275 	 0.04349 	 ~...
   83 	    29 	 0.08990 	 0.04374 	 m..s
    1 	    30 	 0.04779 	 0.04413 	 ~...
   67 	    31 	 0.08204 	 0.04416 	 m..s
   47 	    32 	 0.07096 	 0.04426 	 ~...
   76 	    33 	 0.08662 	 0.04464 	 m..s
   31 	    34 	 0.05560 	 0.04465 	 ~...
   30 	    35 	 0.05405 	 0.04472 	 ~...
   42 	    36 	 0.06838 	 0.04475 	 ~...
   82 	    37 	 0.08956 	 0.04480 	 m..s
   48 	    38 	 0.07109 	 0.04483 	 ~...
   37 	    39 	 0.06763 	 0.04498 	 ~...
   66 	    40 	 0.08169 	 0.04542 	 m..s
    7 	    41 	 0.04992 	 0.04554 	 ~...
   21 	    42 	 0.05165 	 0.04555 	 ~...
   61 	    43 	 0.07982 	 0.04581 	 m..s
   45 	    44 	 0.07052 	 0.04583 	 ~...
   88 	    45 	 0.09167 	 0.04587 	 m..s
   79 	    46 	 0.08779 	 0.04610 	 m..s
   38 	    47 	 0.06808 	 0.04612 	 ~...
   46 	    48 	 0.07093 	 0.04629 	 ~...
   16 	    49 	 0.05114 	 0.04665 	 ~...
   18 	    50 	 0.05123 	 0.04676 	 ~...
   58 	    51 	 0.07850 	 0.04735 	 m..s
   26 	    52 	 0.05338 	 0.04741 	 ~...
   86 	    53 	 0.09058 	 0.04763 	 m..s
   85 	    54 	 0.09055 	 0.04779 	 m..s
   22 	    55 	 0.05231 	 0.04837 	 ~...
    0 	    56 	 0.04509 	 0.04840 	 ~...
   60 	    57 	 0.07925 	 0.04862 	 m..s
    9 	    58 	 0.04996 	 0.04866 	 ~...
   36 	    59 	 0.06493 	 0.04909 	 ~...
   39 	    60 	 0.06812 	 0.04932 	 ~...
   75 	    61 	 0.08646 	 0.04934 	 m..s
   56 	    62 	 0.07836 	 0.04990 	 ~...
   12 	    63 	 0.05074 	 0.05001 	 ~...
   53 	    64 	 0.07512 	 0.05054 	 ~...
   14 	    65 	 0.05105 	 0.05095 	 ~...
   29 	    66 	 0.05402 	 0.05102 	 ~...
   27 	    67 	 0.05341 	 0.05115 	 ~...
   13 	    68 	 0.05090 	 0.05139 	 ~...
   55 	    69 	 0.07575 	 0.05234 	 ~...
   32 	    70 	 0.05566 	 0.05267 	 ~...
   15 	    71 	 0.05107 	 0.05281 	 ~...
   28 	    72 	 0.05365 	 0.05419 	 ~...
    6 	    73 	 0.04988 	 0.05461 	 ~...
   33 	    74 	 0.06157 	 0.05594 	 ~...
   77 	    75 	 0.08717 	 0.05917 	 ~...
   80 	    76 	 0.08782 	 0.05990 	 ~...
   68 	    77 	 0.08235 	 0.06789 	 ~...
   84 	    78 	 0.09029 	 0.07261 	 ~...
   35 	    79 	 0.06282 	 0.08233 	 ~...
   92 	    80 	 0.09950 	 0.08283 	 ~...
   57 	    81 	 0.07838 	 0.09027 	 ~...
   52 	    82 	 0.07360 	 0.09529 	 ~...
   49 	    83 	 0.07195 	 0.09758 	 ~...
   74 	    84 	 0.08639 	 0.10018 	 ~...
   73 	    85 	 0.08593 	 0.10112 	 ~...
   91 	    86 	 0.09686 	 0.10401 	 ~...
   54 	    87 	 0.07538 	 0.10861 	 m..s
   71 	    88 	 0.08424 	 0.11105 	 ~...
   78 	    89 	 0.08724 	 0.11551 	 ~...
   89 	    90 	 0.09284 	 0.11882 	 ~...
   50 	    91 	 0.07272 	 0.12203 	 m..s
   70 	    92 	 0.08374 	 0.13238 	 m..s
   87 	    93 	 0.09161 	 0.16382 	 m..s
   95 	    94 	 0.21974 	 0.17673 	 m..s
   97 	    95 	 0.35458 	 0.22027 	 MISS
   96 	    96 	 0.22163 	 0.24936 	 ~...
   94 	    97 	 0.19442 	 0.25994 	 m..s
  100 	    98 	 0.48983 	 0.46923 	 ~...
  101 	    99 	 0.49600 	 0.50804 	 ~...
  104 	   100 	 0.50143 	 0.51017 	 ~...
  102 	   101 	 0.49976 	 0.51097 	 ~...
   98 	   102 	 0.46022 	 0.51730 	 m..s
  106 	   103 	 0.53039 	 0.52120 	 ~...
  105 	   104 	 0.52829 	 0.52142 	 ~...
   99 	   105 	 0.48844 	 0.52229 	 m..s
  119 	   106 	 0.56231 	 0.52543 	 m..s
  107 	   107 	 0.53216 	 0.52563 	 ~...
  109 	   108 	 0.53488 	 0.52744 	 ~...
  103 	   109 	 0.50065 	 0.52851 	 ~...
  108 	   110 	 0.53286 	 0.53657 	 ~...
  113 	   111 	 0.54299 	 0.54487 	 ~...
  116 	   112 	 0.54886 	 0.54517 	 ~...
  120 	   113 	 0.56297 	 0.54663 	 ~...
  112 	   114 	 0.53837 	 0.54875 	 ~...
  111 	   115 	 0.53773 	 0.54921 	 ~...
  110 	   116 	 0.53697 	 0.55089 	 ~...
  118 	   117 	 0.55706 	 0.55300 	 ~...
  117 	   118 	 0.55666 	 0.55385 	 ~...
  114 	   119 	 0.54391 	 0.55519 	 ~...
  115 	   120 	 0.54806 	 0.56198 	 ~...
==========================================
r_mrr = 0.9892558455467224
r2_mrr = 0.9735223054885864
spearmanr_mrr@5 = 0.803783655166626
spearmanr_mrr@10 = 0.9423781633377075
spearmanr_mrr@50 = 0.9944509267807007
spearmanr_mrr@100 = 0.9954146146774292
spearmanr_mrr@All = 0.995627224445343
==========================================
test time: 0.585
Done Testing dataset UMLS
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.229 +- 0.342
mrr vals (pred, true): 0.146, 0.224

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.05018 	 0.00074 	 m..s
   13 	     1 	 0.04934 	 0.00075 	 m..s
    4 	     2 	 0.04807 	 0.00076 	 m..s
   15 	     3 	 0.04977 	 0.00076 	 m..s
   25 	     4 	 0.05080 	 0.00084 	 m..s
   28 	     5 	 0.05703 	 0.00117 	 m..s
   29 	     6 	 0.05790 	 0.00196 	 m..s
   27 	     7 	 0.05394 	 0.00219 	 m..s
   26 	     8 	 0.05089 	 0.00288 	 m..s
   14 	     9 	 0.04967 	 0.00312 	 m..s
   36 	    10 	 0.09311 	 0.00321 	 m..s
   16 	    11 	 0.04985 	 0.00324 	 m..s
   52 	    12 	 0.11246 	 0.00330 	 MISS
   39 	    13 	 0.09652 	 0.00335 	 m..s
   64 	    14 	 0.12934 	 0.00340 	 MISS
   85 	    15 	 0.14084 	 0.00344 	 MISS
    0 	    16 	 0.04621 	 0.00346 	 m..s
   62 	    17 	 0.12917 	 0.00348 	 MISS
   89 	    18 	 0.14227 	 0.00351 	 MISS
    3 	    19 	 0.04748 	 0.00367 	 m..s
   80 	    20 	 0.13773 	 0.00370 	 MISS
   93 	    21 	 0.14556 	 0.00371 	 MISS
   45 	    22 	 0.10645 	 0.00371 	 MISS
   40 	    23 	 0.09990 	 0.00372 	 m..s
   91 	    24 	 0.14248 	 0.00375 	 MISS
   59 	    25 	 0.12464 	 0.00385 	 MISS
   41 	    26 	 0.10073 	 0.00385 	 m..s
   18 	    27 	 0.05008 	 0.00390 	 m..s
   82 	    28 	 0.13878 	 0.00391 	 MISS
   69 	    29 	 0.13289 	 0.00399 	 MISS
   87 	    30 	 0.14146 	 0.00400 	 MISS
   21 	    31 	 0.05045 	 0.00403 	 m..s
    1 	    32 	 0.04657 	 0.00405 	 m..s
   11 	    33 	 0.04931 	 0.00408 	 m..s
    8 	    34 	 0.04919 	 0.00409 	 m..s
   67 	    35 	 0.13100 	 0.00412 	 MISS
   79 	    36 	 0.13759 	 0.00418 	 MISS
   70 	    37 	 0.13292 	 0.00421 	 MISS
   76 	    38 	 0.13635 	 0.00424 	 MISS
   51 	    39 	 0.11077 	 0.00426 	 MISS
   68 	    40 	 0.13259 	 0.00432 	 MISS
   10 	    41 	 0.04926 	 0.00438 	 m..s
   72 	    42 	 0.13321 	 0.00442 	 MISS
   24 	    43 	 0.05064 	 0.00444 	 m..s
   17 	    44 	 0.05002 	 0.00444 	 m..s
   43 	    45 	 0.10526 	 0.00451 	 MISS
   38 	    46 	 0.09543 	 0.00451 	 m..s
   73 	    47 	 0.13356 	 0.00469 	 MISS
   66 	    48 	 0.13029 	 0.00473 	 MISS
   37 	    49 	 0.09388 	 0.00478 	 m..s
   61 	    50 	 0.12556 	 0.00493 	 MISS
   88 	    51 	 0.14149 	 0.00498 	 MISS
    6 	    52 	 0.04815 	 0.00505 	 m..s
   74 	    53 	 0.13491 	 0.00505 	 MISS
    2 	    54 	 0.04670 	 0.00508 	 m..s
   71 	    55 	 0.13307 	 0.00511 	 MISS
    5 	    56 	 0.04814 	 0.00516 	 m..s
   84 	    57 	 0.14052 	 0.00587 	 MISS
   65 	    58 	 0.12971 	 0.00637 	 MISS
    7 	    59 	 0.04861 	 0.02208 	 ~...
   33 	    60 	 0.07205 	 0.03523 	 m..s
   34 	    61 	 0.07286 	 0.03903 	 m..s
    9 	    62 	 0.04926 	 0.04078 	 ~...
   23 	    63 	 0.05049 	 0.04576 	 ~...
   32 	    64 	 0.06364 	 0.04683 	 ~...
   30 	    65 	 0.06105 	 0.05100 	 ~...
   31 	    66 	 0.06266 	 0.05981 	 ~...
   22 	    67 	 0.05047 	 0.06300 	 ~...
   95 	    68 	 0.14735 	 0.07000 	 m..s
   19 	    69 	 0.05010 	 0.07487 	 ~...
   12 	    70 	 0.04933 	 0.07522 	 ~...
   50 	    71 	 0.10938 	 0.11825 	 ~...
   47 	    72 	 0.10749 	 0.12552 	 ~...
   57 	    73 	 0.12181 	 0.13371 	 ~...
   54 	    74 	 0.11689 	 0.13520 	 ~...
   35 	    75 	 0.09127 	 0.13633 	 m..s
   49 	    76 	 0.10934 	 0.13805 	 ~...
   42 	    77 	 0.10514 	 0.14977 	 m..s
   44 	    78 	 0.10594 	 0.15348 	 m..s
   48 	    79 	 0.10883 	 0.15696 	 m..s
   83 	    80 	 0.14044 	 0.16249 	 ~...
   46 	    81 	 0.10650 	 0.16404 	 m..s
   96 	    82 	 0.15017 	 0.16622 	 ~...
   98 	    83 	 0.18349 	 0.16739 	 ~...
   53 	    84 	 0.11541 	 0.16875 	 m..s
   77 	    85 	 0.13716 	 0.18540 	 m..s
  102 	    86 	 0.20497 	 0.18577 	 ~...
   58 	    87 	 0.12216 	 0.18798 	 m..s
  101 	    88 	 0.20370 	 0.18886 	 ~...
   63 	    89 	 0.12919 	 0.19107 	 m..s
   55 	    90 	 0.11893 	 0.19254 	 m..s
   75 	    91 	 0.13503 	 0.19771 	 m..s
   56 	    92 	 0.12055 	 0.20155 	 m..s
   78 	    93 	 0.13736 	 0.20338 	 m..s
  103 	    94 	 0.20787 	 0.21103 	 ~...
   60 	    95 	 0.12502 	 0.21117 	 m..s
  104 	    96 	 0.20889 	 0.21152 	 ~...
  110 	    97 	 0.23073 	 0.21168 	 ~...
   86 	    98 	 0.14122 	 0.21638 	 m..s
   99 	    99 	 0.19686 	 0.21735 	 ~...
  111 	   100 	 0.23198 	 0.21798 	 ~...
   81 	   101 	 0.13826 	 0.21827 	 m..s
   92 	   102 	 0.14343 	 0.21877 	 m..s
  106 	   103 	 0.22647 	 0.21991 	 ~...
   94 	   104 	 0.14636 	 0.22436 	 m..s
  100 	   105 	 0.19873 	 0.22602 	 ~...
   90 	   106 	 0.14242 	 0.23041 	 m..s
  105 	   107 	 0.22464 	 0.23088 	 ~...
   97 	   108 	 0.16444 	 0.23319 	 m..s
  119 	   109 	 0.25048 	 0.23504 	 ~...
  118 	   110 	 0.24975 	 0.23772 	 ~...
  108 	   111 	 0.22887 	 0.23828 	 ~...
  116 	   112 	 0.24622 	 0.24343 	 ~...
  117 	   113 	 0.24973 	 0.24792 	 ~...
  109 	   114 	 0.23032 	 0.25578 	 ~...
  115 	   115 	 0.24564 	 0.25635 	 ~...
  112 	   116 	 0.23929 	 0.27400 	 m..s
  107 	   117 	 0.22833 	 0.27534 	 m..s
  114 	   118 	 0.24187 	 0.27612 	 m..s
  113 	   119 	 0.23999 	 0.29249 	 m..s
  120 	   120 	 0.25759 	 0.30216 	 m..s
==========================================
r_mrr = 0.7422378063201904
r2_mrr = 0.43040895462036133
spearmanr_mrr@5 = 0.873241662979126
spearmanr_mrr@10 = 0.9446551203727722
spearmanr_mrr@50 = 0.8931384086608887
spearmanr_mrr@100 = 0.8827138543128967
spearmanr_mrr@All = 0.8957632780075073
==========================================
test time: 0.422
Done Testing dataset CoDExSmall
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.286 +- 0.362
mrr vals (pred, true): 0.111, 0.140

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   27 	     0 	 0.07123 	 5e-0500 	 m..s
   15 	     1 	 0.05237 	 5e-0500 	 m..s
    3 	     2 	 0.05019 	 7e-0500 	 m..s
   19 	     3 	 0.05407 	 0.00012 	 m..s
   66 	     4 	 0.10229 	 0.00013 	 MISS
    6 	     5 	 0.05133 	 0.00013 	 m..s
    9 	     6 	 0.05162 	 0.00013 	 m..s
   34 	     7 	 0.09205 	 0.00014 	 m..s
    8 	     8 	 0.05162 	 0.00016 	 m..s
   40 	     9 	 0.09516 	 0.00017 	 m..s
   22 	    10 	 0.05511 	 0.00018 	 m..s
   71 	    11 	 0.10420 	 0.00018 	 MISS
   67 	    12 	 0.10236 	 0.00018 	 MISS
   78 	    13 	 0.10664 	 0.00019 	 MISS
    2 	    14 	 0.04962 	 0.00019 	 m..s
   16 	    15 	 0.05238 	 0.00020 	 m..s
   61 	    16 	 0.10121 	 0.00020 	 MISS
   32 	    17 	 0.09148 	 0.00021 	 m..s
   77 	    18 	 0.10572 	 0.00021 	 MISS
   69 	    19 	 0.10332 	 0.00021 	 MISS
    0 	    20 	 0.04811 	 0.00021 	 m..s
   65 	    21 	 0.10222 	 0.00022 	 MISS
   56 	    22 	 0.09994 	 0.00022 	 m..s
   38 	    23 	 0.09469 	 0.00023 	 m..s
   59 	    24 	 0.10091 	 0.00024 	 MISS
   23 	    25 	 0.05623 	 0.00026 	 m..s
   62 	    26 	 0.10145 	 0.00027 	 MISS
   60 	    27 	 0.10102 	 0.00027 	 MISS
   26 	    28 	 0.06853 	 0.00028 	 m..s
   86 	    29 	 0.11030 	 0.00029 	 MISS
   84 	    30 	 0.10782 	 0.00030 	 MISS
    7 	    31 	 0.05155 	 0.00030 	 m..s
   14 	    32 	 0.05226 	 0.00031 	 m..s
   73 	    33 	 0.10506 	 0.00032 	 MISS
   82 	    34 	 0.10720 	 0.00034 	 MISS
   36 	    35 	 0.09331 	 0.00034 	 m..s
   35 	    36 	 0.09324 	 0.00035 	 m..s
    1 	    37 	 0.04927 	 0.00037 	 m..s
   64 	    38 	 0.10220 	 0.00038 	 MISS
   68 	    39 	 0.10254 	 0.00039 	 MISS
   54 	    40 	 0.09975 	 0.00040 	 m..s
   13 	    41 	 0.05213 	 0.00042 	 m..s
   87 	    42 	 0.11039 	 0.00043 	 MISS
   18 	    43 	 0.05269 	 0.00044 	 m..s
   57 	    44 	 0.10086 	 0.00045 	 MISS
   45 	    45 	 0.09609 	 0.00045 	 m..s
   63 	    46 	 0.10207 	 0.00046 	 MISS
   24 	    47 	 0.05792 	 0.00046 	 m..s
   81 	    48 	 0.10718 	 0.00047 	 MISS
   74 	    49 	 0.10514 	 0.00047 	 MISS
    5 	    50 	 0.05058 	 0.00063 	 m..s
   46 	    51 	 0.09672 	 0.00064 	 m..s
   33 	    52 	 0.09159 	 0.00069 	 m..s
   31 	    53 	 0.09137 	 0.00075 	 m..s
   20 	    54 	 0.05423 	 0.00091 	 m..s
   28 	    55 	 0.07591 	 0.00091 	 m..s
   79 	    56 	 0.10682 	 0.00093 	 MISS
   10 	    57 	 0.05164 	 0.00179 	 m..s
   12 	    58 	 0.05176 	 0.00267 	 m..s
   21 	    59 	 0.05495 	 0.00907 	 m..s
   89 	    60 	 0.12361 	 0.00954 	 MISS
    4 	    61 	 0.05043 	 0.00980 	 m..s
   17 	    62 	 0.05255 	 0.01934 	 m..s
   25 	    63 	 0.06682 	 0.02689 	 m..s
   11 	    64 	 0.05169 	 0.02709 	 ~...
   29 	    65 	 0.07935 	 0.02819 	 m..s
   43 	    66 	 0.09581 	 0.10262 	 ~...
   58 	    67 	 0.10087 	 0.11583 	 ~...
   37 	    68 	 0.09464 	 0.11757 	 ~...
   47 	    69 	 0.09797 	 0.12516 	 ~...
   30 	    70 	 0.09113 	 0.12701 	 m..s
   42 	    71 	 0.09520 	 0.12862 	 m..s
   70 	    72 	 0.10346 	 0.13197 	 ~...
   72 	    73 	 0.10477 	 0.13359 	 ~...
   75 	    74 	 0.10544 	 0.13428 	 ~...
   39 	    75 	 0.09505 	 0.13507 	 m..s
   53 	    76 	 0.09930 	 0.13618 	 m..s
   41 	    77 	 0.09517 	 0.13748 	 m..s
   52 	    78 	 0.09924 	 0.13909 	 m..s
   48 	    79 	 0.09825 	 0.14037 	 m..s
   88 	    80 	 0.11108 	 0.14050 	 ~...
   95 	    81 	 0.18310 	 0.14152 	 m..s
   50 	    82 	 0.09872 	 0.14309 	 m..s
   44 	    83 	 0.09599 	 0.14485 	 m..s
   80 	    84 	 0.10704 	 0.14495 	 m..s
   49 	    85 	 0.09829 	 0.14679 	 m..s
   55 	    86 	 0.09982 	 0.15579 	 m..s
   97 	    87 	 0.18845 	 0.15838 	 m..s
   94 	    88 	 0.17990 	 0.15898 	 ~...
   76 	    89 	 0.10550 	 0.16143 	 m..s
   91 	    90 	 0.14990 	 0.16385 	 ~...
   98 	    91 	 0.19086 	 0.16826 	 ~...
   83 	    92 	 0.10779 	 0.16927 	 m..s
  101 	    93 	 0.20041 	 0.17630 	 ~...
   85 	    94 	 0.10850 	 0.17704 	 m..s
   93 	    95 	 0.17621 	 0.18464 	 ~...
  100 	    96 	 0.19334 	 0.18865 	 ~...
  102 	    97 	 0.20111 	 0.19113 	 ~...
   51 	    98 	 0.09903 	 0.19279 	 m..s
   96 	    99 	 0.18315 	 0.19416 	 ~...
   92 	   100 	 0.17566 	 0.19467 	 ~...
   90 	   101 	 0.14460 	 0.21465 	 m..s
  103 	   102 	 0.20139 	 0.22149 	 ~...
  111 	   103 	 0.23687 	 0.23077 	 ~...
  104 	   104 	 0.20159 	 0.23511 	 m..s
   99 	   105 	 0.19132 	 0.23829 	 m..s
  107 	   106 	 0.21649 	 0.24458 	 ~...
  109 	   107 	 0.22853 	 0.24997 	 ~...
  112 	   108 	 0.23918 	 0.26472 	 ~...
  106 	   109 	 0.21172 	 0.27808 	 m..s
  113 	   110 	 0.24468 	 0.28674 	 m..s
  105 	   111 	 0.21124 	 0.28899 	 m..s
  110 	   112 	 0.23404 	 0.29343 	 m..s
  114 	   113 	 0.26445 	 0.29693 	 m..s
  108 	   114 	 0.22501 	 0.30201 	 m..s
  120 	   115 	 0.31200 	 0.31608 	 ~...
  118 	   116 	 0.29500 	 0.32274 	 ~...
  116 	   117 	 0.28891 	 0.33349 	 m..s
  115 	   118 	 0.28592 	 0.33444 	 m..s
  117 	   119 	 0.29498 	 0.33536 	 m..s
  119 	   120 	 0.29707 	 0.35127 	 m..s
==========================================
r_mrr = 0.872106671333313
r2_mrr = 0.619920015335083
spearmanr_mrr@5 = 0.9840550422668457
spearmanr_mrr@10 = 0.9718723893165588
spearmanr_mrr@50 = 0.9730777740478516
spearmanr_mrr@100 = 0.9059627056121826
spearmanr_mrr@All = 0.9184030294418335
==========================================
test time: 0.457
Done Testing dataset DBpedia50
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.068 +- 0.054
mrr vals (pred, true): 0.173, 0.248

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   34 	     0 	 0.14022 	 0.02078 	 MISS
    4 	     1 	 0.05752 	 0.04019 	 ~...
   11 	     2 	 0.05908 	 0.04468 	 ~...
   12 	     3 	 0.05926 	 0.04684 	 ~...
   25 	     4 	 0.06085 	 0.04771 	 ~...
   82 	     5 	 0.17164 	 0.04853 	 MISS
    7 	     6 	 0.05782 	 0.04874 	 ~...
   47 	     7 	 0.15354 	 0.04925 	 MISS
    6 	     8 	 0.05776 	 0.04997 	 ~...
   62 	     9 	 0.15965 	 0.04997 	 MISS
   15 	    10 	 0.05934 	 0.05001 	 ~...
   60 	    11 	 0.15950 	 0.05011 	 MISS
   28 	    12 	 0.11103 	 0.05029 	 m..s
   56 	    13 	 0.15711 	 0.05048 	 MISS
   23 	    14 	 0.06012 	 0.05070 	 ~...
   59 	    15 	 0.15926 	 0.05085 	 MISS
   64 	    16 	 0.15983 	 0.05088 	 MISS
   74 	    17 	 0.16680 	 0.05095 	 MISS
   10 	    18 	 0.05883 	 0.05110 	 ~...
   61 	    19 	 0.15952 	 0.05119 	 MISS
   49 	    20 	 0.15418 	 0.05138 	 MISS
   58 	    21 	 0.15804 	 0.05146 	 MISS
   55 	    22 	 0.15685 	 0.05150 	 MISS
    1 	    23 	 0.05392 	 0.05178 	 ~...
    5 	    24 	 0.05756 	 0.05183 	 ~...
   17 	    25 	 0.05941 	 0.05210 	 ~...
    9 	    26 	 0.05874 	 0.05212 	 ~...
   31 	    27 	 0.11890 	 0.05240 	 m..s
   20 	    28 	 0.05980 	 0.05252 	 ~...
   73 	    29 	 0.16541 	 0.05268 	 MISS
   22 	    30 	 0.05985 	 0.05290 	 ~...
   71 	    31 	 0.16453 	 0.05296 	 MISS
   32 	    32 	 0.12160 	 0.05314 	 m..s
   75 	    33 	 0.16705 	 0.05338 	 MISS
   16 	    34 	 0.05936 	 0.05358 	 ~...
   13 	    35 	 0.05932 	 0.05363 	 ~...
   78 	    36 	 0.16758 	 0.05371 	 MISS
   19 	    37 	 0.05947 	 0.05376 	 ~...
   70 	    38 	 0.16440 	 0.05379 	 MISS
    3 	    39 	 0.05745 	 0.05388 	 ~...
   86 	    40 	 0.19319 	 0.05412 	 MISS
   63 	    41 	 0.15977 	 0.05418 	 MISS
    2 	    42 	 0.05571 	 0.05447 	 ~...
   26 	    43 	 0.06219 	 0.05455 	 ~...
   14 	    44 	 0.05934 	 0.05470 	 ~...
   33 	    45 	 0.13829 	 0.05548 	 m..s
    8 	    46 	 0.05872 	 0.05573 	 ~...
   37 	    47 	 0.14650 	 0.05574 	 m..s
   18 	    48 	 0.05944 	 0.05603 	 ~...
   67 	    49 	 0.16139 	 0.05628 	 MISS
   52 	    50 	 0.15672 	 0.05657 	 MISS
   24 	    51 	 0.06015 	 0.05683 	 ~...
    0 	    52 	 0.04505 	 0.05708 	 ~...
   30 	    53 	 0.11286 	 0.05746 	 m..s
   65 	    54 	 0.16009 	 0.05757 	 MISS
   29 	    55 	 0.11221 	 0.05854 	 m..s
   21 	    56 	 0.05982 	 0.05858 	 ~...
   42 	    57 	 0.15016 	 0.05955 	 m..s
   54 	    58 	 0.15679 	 0.05960 	 m..s
   80 	    59 	 0.16845 	 0.05998 	 MISS
   57 	    60 	 0.15753 	 0.06047 	 m..s
   77 	    61 	 0.16756 	 0.06078 	 MISS
   68 	    62 	 0.16304 	 0.06172 	 MISS
   87 	    63 	 0.22842 	 0.09479 	 MISS
   69 	    64 	 0.16397 	 0.16674 	 ~...
   46 	    65 	 0.15186 	 0.21670 	 m..s
   41 	    66 	 0.14972 	 0.21715 	 m..s
   79 	    67 	 0.16840 	 0.21836 	 m..s
   39 	    68 	 0.14759 	 0.21967 	 m..s
   76 	    69 	 0.16736 	 0.22070 	 m..s
   66 	    70 	 0.16046 	 0.22322 	 m..s
   51 	    71 	 0.15643 	 0.22648 	 m..s
   35 	    72 	 0.14191 	 0.22734 	 m..s
   40 	    73 	 0.14843 	 0.22971 	 m..s
   85 	    74 	 0.18896 	 0.23064 	 m..s
   45 	    75 	 0.15162 	 0.23455 	 m..s
   48 	    76 	 0.15380 	 0.24002 	 m..s
   44 	    77 	 0.15079 	 0.24076 	 m..s
   50 	    78 	 0.15582 	 0.24779 	 m..s
   83 	    79 	 0.17251 	 0.24830 	 m..s
   43 	    80 	 0.15057 	 0.25189 	 MISS
   72 	    81 	 0.16499 	 0.25366 	 m..s
   81 	    82 	 0.16938 	 0.25617 	 m..s
   84 	    83 	 0.18435 	 0.25748 	 m..s
   27 	    84 	 0.11078 	 0.26072 	 MISS
   36 	    85 	 0.14336 	 0.26251 	 MISS
   53 	    86 	 0.15674 	 0.26501 	 MISS
   38 	    87 	 0.14751 	 0.27135 	 MISS
   88 	    88 	 0.27209 	 0.30868 	 m..s
   90 	    89 	 0.32844 	 0.33874 	 ~...
   92 	    90 	 0.35996 	 0.36122 	 ~...
   95 	    91 	 0.36899 	 0.37356 	 ~...
   94 	    92 	 0.36258 	 0.37516 	 ~...
   96 	    93 	 0.38175 	 0.38105 	 ~...
   93 	    94 	 0.36057 	 0.38177 	 ~...
   89 	    95 	 0.29771 	 0.38496 	 m..s
   97 	    96 	 0.38909 	 0.38764 	 ~...
   91 	    97 	 0.35689 	 0.39461 	 m..s
   98 	    98 	 0.39621 	 0.41083 	 ~...
   99 	    99 	 0.41958 	 0.41107 	 ~...
  104 	   100 	 0.42916 	 0.42728 	 ~...
  102 	   101 	 0.42553 	 0.42739 	 ~...
  116 	   102 	 0.45183 	 0.42760 	 ~...
  101 	   103 	 0.42438 	 0.42791 	 ~...
  103 	   104 	 0.42585 	 0.42847 	 ~...
  119 	   105 	 0.45845 	 0.43133 	 ~...
  100 	   106 	 0.42087 	 0.43199 	 ~...
  108 	   107 	 0.43289 	 0.43257 	 ~...
  112 	   108 	 0.43979 	 0.43351 	 ~...
  114 	   109 	 0.44636 	 0.43393 	 ~...
  105 	   110 	 0.43053 	 0.43550 	 ~...
  109 	   111 	 0.43349 	 0.43603 	 ~...
  115 	   112 	 0.44929 	 0.44039 	 ~...
  111 	   113 	 0.43599 	 0.44168 	 ~...
  110 	   114 	 0.43375 	 0.44352 	 ~...
  117 	   115 	 0.45327 	 0.44622 	 ~...
  107 	   116 	 0.43265 	 0.44730 	 ~...
  120 	   117 	 0.46039 	 0.44790 	 ~...
  106 	   118 	 0.43184 	 0.44970 	 ~...
  113 	   119 	 0.44068 	 0.45534 	 ~...
  118 	   120 	 0.45653 	 0.46006 	 ~...
==========================================
r_mrr = 0.9023691415786743
r2_mrr = 0.8034576177597046
spearmanr_mrr@5 = 0.9393834471702576
spearmanr_mrr@10 = 0.9531296491622925
spearmanr_mrr@50 = 0.9972439408302307
spearmanr_mrr@100 = 0.9226597547531128
spearmanr_mrr@All = 0.9342499375343323
==========================================
test time: 0.42
Done Testing dataset Kinships
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.286 +- 0.360
mrr vals (pred, true): 0.091, 0.063

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   13 	     0 	 0.04980 	 8e-0500 	 m..s
    0 	     1 	 0.02020 	 9e-0500 	 ~...
   28 	     2 	 0.07787 	 0.00011 	 m..s
   10 	     3 	 0.04680 	 0.00013 	 m..s
   21 	     4 	 0.05874 	 0.00023 	 m..s
   32 	     5 	 0.07968 	 0.00047 	 m..s
   85 	     6 	 0.08954 	 0.00048 	 m..s
   75 	     7 	 0.08829 	 0.00049 	 m..s
   23 	     8 	 0.05945 	 0.00050 	 m..s
   62 	     9 	 0.08699 	 0.00050 	 m..s
   74 	    10 	 0.08825 	 0.00051 	 m..s
   11 	    11 	 0.04869 	 0.00051 	 m..s
   46 	    12 	 0.08251 	 0.00051 	 m..s
   68 	    13 	 0.08739 	 0.00054 	 m..s
   87 	    14 	 0.09091 	 0.00056 	 m..s
   60 	    15 	 0.08679 	 0.00056 	 m..s
   81 	    16 	 0.08922 	 0.00056 	 m..s
   65 	    17 	 0.08728 	 0.00057 	 m..s
   77 	    18 	 0.08854 	 0.00057 	 m..s
   63 	    19 	 0.08723 	 0.00058 	 m..s
   61 	    20 	 0.08688 	 0.00059 	 m..s
    9 	    21 	 0.04654 	 0.00059 	 m..s
   35 	    22 	 0.08057 	 0.00059 	 m..s
   33 	    23 	 0.07978 	 0.00059 	 m..s
   12 	    24 	 0.04936 	 0.00060 	 m..s
   79 	    25 	 0.08905 	 0.00060 	 m..s
   47 	    26 	 0.08301 	 0.00061 	 m..s
   83 	    27 	 0.08935 	 0.00061 	 m..s
   82 	    28 	 0.08923 	 0.00061 	 m..s
   30 	    29 	 0.07931 	 0.00062 	 m..s
   78 	    30 	 0.08897 	 0.00063 	 m..s
   57 	    31 	 0.08671 	 0.00065 	 m..s
   59 	    32 	 0.08674 	 0.00066 	 m..s
   24 	    33 	 0.06111 	 0.00067 	 m..s
    6 	    34 	 0.03926 	 0.00068 	 m..s
    3 	    35 	 0.03389 	 0.00068 	 m..s
   64 	    36 	 0.08728 	 0.00068 	 m..s
   18 	    37 	 0.05645 	 0.00068 	 m..s
   71 	    38 	 0.08795 	 0.00068 	 m..s
   31 	    39 	 0.07941 	 0.00070 	 m..s
   42 	    40 	 0.08185 	 0.00071 	 m..s
   34 	    41 	 0.08040 	 0.00071 	 m..s
   56 	    42 	 0.08614 	 0.00074 	 m..s
   38 	    43 	 0.08148 	 0.00075 	 m..s
   14 	    44 	 0.05004 	 0.00077 	 m..s
   66 	    45 	 0.08731 	 0.00084 	 m..s
    5 	    46 	 0.03769 	 0.00085 	 m..s
   67 	    47 	 0.08733 	 0.00087 	 m..s
   27 	    48 	 0.07485 	 0.00088 	 m..s
   22 	    49 	 0.05910 	 0.00090 	 m..s
   20 	    50 	 0.05756 	 0.00092 	 m..s
   69 	    51 	 0.08764 	 0.00094 	 m..s
   54 	    52 	 0.08597 	 0.00098 	 m..s
    8 	    53 	 0.04563 	 0.00119 	 m..s
    2 	    54 	 0.02250 	 0.00125 	 ~...
   16 	    55 	 0.05169 	 0.00129 	 m..s
   25 	    56 	 0.07102 	 0.00129 	 m..s
    7 	    57 	 0.04537 	 0.00150 	 m..s
   17 	    58 	 0.05527 	 0.00500 	 m..s
    1 	    59 	 0.02046 	 0.00545 	 ~...
    4 	    60 	 0.03537 	 0.00637 	 ~...
   19 	    61 	 0.05733 	 0.00655 	 m..s
   15 	    62 	 0.05146 	 0.00752 	 m..s
   89 	    63 	 0.09976 	 0.00878 	 m..s
   26 	    64 	 0.07450 	 0.00925 	 m..s
   36 	    65 	 0.08074 	 0.01350 	 m..s
   37 	    66 	 0.08144 	 0.04881 	 m..s
   73 	    67 	 0.08815 	 0.04887 	 m..s
   70 	    68 	 0.08773 	 0.05020 	 m..s
   41 	    69 	 0.08183 	 0.05021 	 m..s
   58 	    70 	 0.08671 	 0.05053 	 m..s
   53 	    71 	 0.08547 	 0.05221 	 m..s
   80 	    72 	 0.08915 	 0.05226 	 m..s
   48 	    73 	 0.08402 	 0.05493 	 ~...
   44 	    74 	 0.08229 	 0.05531 	 ~...
   29 	    75 	 0.07894 	 0.05562 	 ~...
   49 	    76 	 0.08435 	 0.05693 	 ~...
   45 	    77 	 0.08246 	 0.05817 	 ~...
   40 	    78 	 0.08183 	 0.05927 	 ~...
   39 	    79 	 0.08181 	 0.06032 	 ~...
   84 	    80 	 0.08952 	 0.06111 	 ~...
   86 	    81 	 0.08991 	 0.06130 	 ~...
   72 	    82 	 0.08809 	 0.06187 	 ~...
   55 	    83 	 0.08604 	 0.06272 	 ~...
   88 	    84 	 0.09133 	 0.06298 	 ~...
   76 	    85 	 0.08842 	 0.06358 	 ~...
   52 	    86 	 0.08539 	 0.06657 	 ~...
   50 	    87 	 0.08478 	 0.06711 	 ~...
   51 	    88 	 0.08513 	 0.06743 	 ~...
   43 	    89 	 0.08220 	 0.07450 	 ~...
   91 	    90 	 0.12460 	 0.13046 	 ~...
  101 	    91 	 0.15815 	 0.13287 	 ~...
   94 	    92 	 0.14529 	 0.13299 	 ~...
   98 	    93 	 0.15487 	 0.13420 	 ~...
   92 	    94 	 0.13484 	 0.15221 	 ~...
   95 	    95 	 0.14855 	 0.15264 	 ~...
  103 	    96 	 0.16626 	 0.16133 	 ~...
   90 	    97 	 0.12195 	 0.16306 	 m..s
   93 	    98 	 0.13680 	 0.16576 	 ~...
   96 	    99 	 0.14872 	 0.17635 	 ~...
   97 	   100 	 0.14909 	 0.18566 	 m..s
   99 	   101 	 0.15664 	 0.19335 	 m..s
  102 	   102 	 0.16129 	 0.20232 	 m..s
  112 	   103 	 0.20915 	 0.22049 	 ~...
  107 	   104 	 0.18333 	 0.22435 	 m..s
  100 	   105 	 0.15768 	 0.22454 	 m..s
  109 	   106 	 0.19249 	 0.22936 	 m..s
  110 	   107 	 0.19753 	 0.22977 	 m..s
  106 	   108 	 0.17945 	 0.23545 	 m..s
  113 	   109 	 0.22234 	 0.23545 	 ~...
  104 	   110 	 0.16739 	 0.23553 	 m..s
  108 	   111 	 0.19078 	 0.24007 	 m..s
  115 	   112 	 0.23333 	 0.24736 	 ~...
  114 	   113 	 0.22890 	 0.24977 	 ~...
  111 	   114 	 0.19993 	 0.25221 	 m..s
  105 	   115 	 0.17899 	 0.25900 	 m..s
  118 	   116 	 0.27526 	 0.29939 	 ~...
  119 	   117 	 0.27685 	 0.30087 	 ~...
  117 	   118 	 0.27525 	 0.30439 	 ~...
  116 	   119 	 0.27052 	 0.30494 	 m..s
  120 	   120 	 0.28790 	 0.30750 	 ~...
==========================================
r_mrr = 0.9322936534881592
r2_mrr = 0.6223663091659546
spearmanr_mrr@5 = 0.866963267326355
spearmanr_mrr@10 = 0.9861606955528259
spearmanr_mrr@50 = 0.9760078191757202
spearmanr_mrr@100 = 0.9605394601821899
spearmanr_mrr@All = 0.9511504769325256
==========================================
test time: 0.397
Done Testing dataset OpenEA
total time taken: 1053.2839019298553
training time taken: 1005.9442534446716
TWIG out ;))
==============================================
----------------------------------------------
Running a TWIG experiment with tag: TransE-all
----------------------------------------------
==============================================
Using random seed: 3784119754917616
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [829, 579, 715, 286, 1054, 482, 777, 858, 385, 989, 942, 76, 273, 284, 384, 1097, 39, 1055, 1045, 458, 1075, 844, 1012, 669, 833, 822, 621, 1048, 58, 144, 641, 997, 902, 590, 288, 181, 923, 868, 455, 593, 856, 969, 166, 555, 1020, 410, 1108, 1104, 529, 1029, 230, 528, 608, 598, 978, 1143, 786, 335, 1205, 662, 782, 764, 738, 1162, 346, 993, 333, 454, 185, 449, 400, 402, 1124, 824, 526, 420, 890, 14, 1199, 668, 537, 836, 721, 1141, 1095, 1077, 820, 1187, 372, 1084, 303, 1213, 618, 1027, 670, 819, 895, 611, 201, 226, 949, 220, 467, 1188, 1006, 772, 893, 570, 9, 1150, 357, 582, 575, 755, 268, 292, 552, 851, 780, 680, 264]
valid_ids (0): []
train_ids (1094): [533, 31, 711, 724, 677, 246, 709, 407, 894, 236, 56, 886, 878, 741, 366, 471, 129, 33, 394, 126, 501, 803, 252, 983, 616, 702, 320, 239, 1145, 980, 922, 351, 562, 354, 1106, 961, 646, 339, 84, 846, 424, 197, 994, 131, 283, 532, 168, 931, 388, 664, 944, 30, 683, 1057, 1129, 705, 497, 636, 253, 1140, 172, 116, 312, 752, 51, 728, 343, 438, 913, 259, 206, 519, 992, 282, 213, 128, 726, 95, 1083, 757, 585, 1061, 36, 1024, 352, 1138, 1021, 1115, 812, 170, 379, 307, 446, 93, 50, 364, 941, 665, 175, 431, 332, 853, 5, 1207, 513, 1035, 88, 1096, 732, 832, 1179, 792, 1212, 367, 745, 161, 1089, 965, 766, 650, 147, 1049, 119, 717, 158, 725, 263, 453, 486, 504, 7, 591, 209, 138, 196, 1039, 774, 778, 578, 405, 612, 781, 140, 195, 1040, 580, 687, 1062, 848, 457, 324, 872, 1080, 416, 493, 64, 599, 508, 377, 1028, 1026, 176, 360, 1063, 693, 445, 1033, 1052, 676, 1202, 432, 1139, 1068, 1147, 466, 1210, 610, 473, 682, 1022, 462, 765, 550, 13, 888, 921, 988, 686, 539, 48, 469, 1, 968, 506, 231, 928, 485, 79, 238, 606, 1053, 107, 699, 647, 1081, 411, 203, 1168, 202, 370, 345, 1090, 1093, 386, 827, 1008, 889, 1158, 811, 1011, 61, 594, 173, 929, 319, 427, 932, 371, 207, 1195, 644, 480, 973, 100, 142, 396, 21, 47, 1017, 700, 451, 241, 917, 742, 991, 667, 425, 190, 1142, 1111, 254, 908, 472, 369, 796, 813, 788, 114, 1004, 1002, 478, 574, 251, 1014, 1101, 771, 399, 520, 123, 1160, 115, 484, 380, 817, 110, 1131, 1041, 971, 986, 707, 898, 1082, 23, 748, 1164, 404, 935, 842, 430, 804, 1153, 228, 857, 1091, 967, 365, 565, 547, 775, 798, 362, 41, 852, 730, 85, 1066, 885, 1125, 287, 733, 262, 223, 1174, 586, 153, 816, 563, 275, 406, 553, 304, 509, 996, 29, 628, 845, 915, 834, 415, 666, 569, 146, 17, 910, 476, 6, 218, 995, 422, 341, 544, 689, 977, 896, 134, 309, 601, 1119, 276, 964, 783, 27, 338, 1191, 1110, 356, 1114, 651, 222, 52, 749, 985, 1079, 911, 1046, 634, 413, 234, 314, 442, 1156, 671, 688, 75, 3, 799, 527, 1092, 761, 723, 102, 412, 597, 68, 210, 152, 825, 1206, 162, 428, 395, 987, 823, 112, 1151, 208, 838, 1165, 60, 308, 194, 849, 348, 188, 536, 130, 793, 649, 854, 269, 1152, 797, 433, 706, 1070, 639, 376, 522, 1192, 232, 615, 789, 660, 163, 1085, 440, 216, 59, 247, 1154, 770, 675, 567, 1036, 685, 235, 718, 62, 267, 524, 566, 16, 4, 607, 583, 1208, 183, 847, 502, 290, 787, 503, 481, 919, 769, 1170, 204, 55, 768, 1181, 401, 1144, 450, 89, 359, 1194, 926, 1204, 758, 258, 184, 1015, 882, 672, 1113, 227, 1183, 495, 573, 966, 67, 1149, 703, 592, 916, 261, 148, 108, 479, 363, 24, 193, 584, 828, 572, 1175, 378, 189, 735, 652, 270, 632, 57, 627, 746, 1116, 1056, 1193, 631, 1163, 559, 248, 877, 32, 1060, 558, 839, 899, 909, 245, 630, 576, 859, 891, 530, 321, 200, 192, 1007, 905, 802, 35, 661, 850, 744, 762, 498, 224, 127, 49, 927, 548, 602, 1067, 214, 1148, 281, 815, 1038, 417, 167, 313, 490, 368, 277, 951, 698, 595, 392, 63, 1071, 843, 217, 1087, 1198, 657, 860, 1134, 613, 141, 940, 614, 708, 938, 300, 448, 1009, 330, 101, 862, 459, 656, 881, 460, 66, 177, 46, 73, 34, 960, 943, 596, 468, 1025, 1073, 517, 545, 164, 525, 640, 409, 684, 25, 289, 1047, 255, 901, 249, 937, 936, 1155, 704, 174, 40, 867, 1065, 470, 1031, 874, 512, 340, 625, 87, 1059, 1177, 523, 78, 1013, 958, 946, 81, 191, 865, 272, 739, 681, 604, 418, 883, 982, 199, 10, 950, 291, 1135, 795, 1171, 809, 156, 237, 1058, 587, 1211, 791, 760, 393, 763, 97, 876, 496, 1000, 109, 603, 198, 1185, 325, 487, 785, 44, 1122, 531, 818, 474, 753, 716, 447, 806, 306, 505, 477, 82, 103, 461, 1074, 810, 483, 826, 212, 731, 914, 962, 518, 959, 784, 1112, 875, 970, 1157, 205, 754, 720, 12, 1086, 1166, 837, 381, 104, 871, 302, 452, 521, 69, 1076, 99, 1146, 1126, 659, 710, 855, 121, 1016, 492, 344, 38, 318, 679, 830, 390, 515, 1209, 870, 361, 805, 975, 1182, 296, 260, 976, 643, 507, 265, 435, 429, 773, 557, 776, 414, 609, 554, 337, 280, 645, 1072, 719, 642, 151, 551, 984, 514, 637, 106, 297, 18, 1196, 1161, 546, 1051, 1032, 907, 956, 70, 511, 11, 1189, 178, 256, 65, 8, 1103, 577, 77, 317, 1136, 285, 918, 349, 696, 257, 925, 866, 808, 439, 623, 383, 419, 1128, 145, 571, 543, 663, 948, 734, 305, 355, 464, 906, 750, 117, 136, 1121, 1173, 933, 180, 1088, 1044, 549, 568, 122, 840, 930, 897, 421, 0, 436, 972, 1100, 278, 751, 581, 310, 15, 1176, 912, 821, 1105, 955, 954, 374, 747, 974, 678, 358, 835, 999, 540, 629, 1137, 1159, 443, 423, 620, 998, 743, 437, 561, 72, 588, 2, 904, 740, 315, 564, 113, 1102, 229, 37, 1214, 397, 187, 1050, 242, 80, 86, 298, 873, 96, 879, 342, 1069, 92, 694, 465, 538, 311, 1019, 1118, 900, 794, 953, 1132, 981, 1180, 45, 541, 697, 1005, 353, 143, 695, 790, 800, 861, 1034, 124, 1018, 1172, 301, 179, 240, 1064, 111, 727, 759, 1042, 118, 880, 54, 456, 42, 924, 53, 329, 211, 157, 1120, 1201, 957, 1023, 499, 219, 221, 186, 98, 542, 26, 133, 963, 19, 441, 160, 690, 334, 692, 408, 1130, 90, 934, 1099, 1098, 94, 225, 137, 1200, 869, 500, 673, 1010, 326, 165, 1043, 892, 1117, 1178, 638, 807, 271, 274, 654, 382, 1184, 1123, 556, 658, 947, 516, 347, 295, 1037, 120, 600, 622, 1203, 244, 243, 1133, 589, 1169, 215, 149, 233, 266, 1167, 814, 920, 714, 887, 155, 293, 653, 713, 1003, 169, 331, 182, 535, 375, 884, 150, 43, 655, 722, 74, 391, 756, 463, 159, 328, 279, 403, 327, 475, 1001, 801, 350, 20, 1186, 712, 691, 398, 294, 434, 71, 648, 1030, 316, 1078, 990, 105, 831, 863, 28, 560, 387, 779, 841, 605, 1127, 336, 626, 1197, 494, 299, 903, 952, 132, 1190, 389, 1107, 139, 979, 635, 737, 250, 736, 426, 22, 489, 444, 619, 864, 617, 701, 322, 91, 373, 491, 729, 767, 323, 674, 135, 1094, 125, 510, 488, 534, 1109, 633, 945, 171, 939, 83, 624, 154]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7961393857648303
the save name prefix for this run is:  chkpt-ID_7961393857648303_tag_TransE-all
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1'], 'CoDExSmall': ['2.1'], 'DBpedia50': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1063
rank avg (pred): 0.433 +- 0.002
mrr vals (pred, true): 0.017, 0.605
batch losses (mrrl, rdl): 0.0, 0.0036329385

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 589
rank avg (pred): 0.386 +- 0.011
mrr vals (pred, true): 0.019, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001210826

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 127
rank avg (pred): 0.423 +- 0.008
mrr vals (pred, true): 0.017, 0.102
batch losses (mrrl, rdl): 0.0, 0.0006140803

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1106
rank avg (pred): 0.379 +- 0.014
mrr vals (pred, true): 0.019, 0.160
batch losses (mrrl, rdl): 0.0, 0.0006239586

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1056
rank avg (pred): 0.060 +- 0.001
mrr vals (pred, true): 0.110, 0.610
batch losses (mrrl, rdl): 0.0, 3.13529e-05

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 911
rank avg (pred): 0.227 +- 0.011
mrr vals (pred, true): 0.032, 0.251
batch losses (mrrl, rdl): 0.0, 0.0003701218

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 784
rank avg (pred): 0.366 +- 0.054
mrr vals (pred, true): 0.021, 0.041
batch losses (mrrl, rdl): 0.0, 0.0002230416

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 753
rank avg (pred): 0.077 +- 0.038
mrr vals (pred, true): 0.140, 0.580
batch losses (mrrl, rdl): 0.0, 5.53504e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 781
rank avg (pred): 0.319 +- 0.229
mrr vals (pred, true): 0.148, 0.090
batch losses (mrrl, rdl): 0.0, 0.0001046573

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 466
rank avg (pred): 0.306 +- 0.219
mrr vals (pred, true): 0.178, 0.038
batch losses (mrrl, rdl): 0.0, 0.000199158

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 936
rank avg (pred): 0.343 +- 0.255
mrr vals (pred, true): 0.200, 0.088
batch losses (mrrl, rdl): 0.0, 0.000108361

Epoch over!
epoch time: 63.963

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 990
rank avg (pred): 0.059 +- 0.044
mrr vals (pred, true): 0.318, 0.601
batch losses (mrrl, rdl): 0.0, 2.74015e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 416
rank avg (pred): 0.346 +- 0.284
mrr vals (pred, true): 0.229, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001441191

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 313
rank avg (pred): 0.073 +- 0.060
mrr vals (pred, true): 0.341, 0.565
batch losses (mrrl, rdl): 0.0, 5.40031e-05

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 639
rank avg (pred): 0.440 +- 0.262
mrr vals (pred, true): 0.157, 0.036
batch losses (mrrl, rdl): 0.0, 2.59765e-05

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1144
rank avg (pred): 0.186 +- 0.145
mrr vals (pred, true): 0.251, 0.152
batch losses (mrrl, rdl): 0.0, 7.16691e-05

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 551
rank avg (pred): 0.302 +- 0.177
mrr vals (pred, true): 0.165, 0.117
batch losses (mrrl, rdl): 0.0, 2.8423e-05

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 466
rank avg (pred): 0.326 +- 0.269
mrr vals (pred, true): 0.257, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001014595

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 501
rank avg (pred): 0.287 +- 0.182
mrr vals (pred, true): 0.202, 0.073
batch losses (mrrl, rdl): 0.0, 2.35662e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 905
rank avg (pred): 0.071 +- 0.061
mrr vals (pred, true): 0.385, 0.315
batch losses (mrrl, rdl): 0.0, 1.8176e-06

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 4
rank avg (pred): 0.063 +- 0.055
mrr vals (pred, true): 0.382, 0.544
batch losses (mrrl, rdl): 0.0, 2.73603e-05

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 113
rank avg (pred): 0.323 +- 0.268
mrr vals (pred, true): 0.268, 0.122
batch losses (mrrl, rdl): 0.0, 0.0002578509

Epoch over!
epoch time: 64.101

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 544
rank avg (pred): 0.246 +- 0.177
mrr vals (pred, true): 0.214, 0.076
batch losses (mrrl, rdl): 0.0, 0.0001557164

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 237
rank avg (pred): 0.333 +- 0.279
mrr vals (pred, true): 0.256, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001117773

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1098
rank avg (pred): 0.313 +- 0.283
mrr vals (pred, true): 0.321, 0.124
batch losses (mrrl, rdl): 0.0, 0.0002037125

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 920
rank avg (pred): 0.326 +- 0.280
mrr vals (pred, true): 0.288, 0.090
batch losses (mrrl, rdl): 0.0, 0.0001493389

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 225
rank avg (pred): 0.330 +- 0.274
mrr vals (pred, true): 0.276, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001326526

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 486
rank avg (pred): 0.307 +- 0.194
mrr vals (pred, true): 0.185, 0.080
batch losses (mrrl, rdl): 0.0, 1.96072e-05

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 302
rank avg (pred): 0.034 +- 0.031
mrr vals (pred, true): 0.460, 0.625
batch losses (mrrl, rdl): 0.0, 3.7203e-06

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 842
rank avg (pred): 0.329 +- 0.266
mrr vals (pred, true): 0.245, 0.085
batch losses (mrrl, rdl): 0.0, 0.0001468959

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 763
rank avg (pred): 0.335 +- 0.283
mrr vals (pred, true): 0.274, 0.067
batch losses (mrrl, rdl): 0.0, 3.5312e-05

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 901
rank avg (pred): 0.072 +- 0.065
mrr vals (pred, true): 0.387, 0.245
batch losses (mrrl, rdl): 0.0, 5.6575e-06

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 393
rank avg (pred): 0.320 +- 0.284
mrr vals (pred, true): 0.278, 0.097
batch losses (mrrl, rdl): 0.0, 8.33459e-05

Epoch over!
epoch time: 64.034

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 26
rank avg (pred): 0.013 +- 0.012
mrr vals (pred, true): 0.556, 0.619
batch losses (mrrl, rdl): 0.0, 1.2765e-06

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1112
rank avg (pred): 0.314 +- 0.272
mrr vals (pred, true): 0.301, 0.036
batch losses (mrrl, rdl): 0.0, 0.0002218808

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 107
rank avg (pred): 0.303 +- 0.253
mrr vals (pred, true): 0.290, 0.119
batch losses (mrrl, rdl): 0.0, 0.0001302503

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 879
rank avg (pred): 0.316 +- 0.272
mrr vals (pred, true): 0.317, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001635246

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1178
rank avg (pred): 0.419 +- 0.278
mrr vals (pred, true): 0.223, 0.037
batch losses (mrrl, rdl): 0.0, 3.42223e-05

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 839
rank avg (pred): 0.326 +- 0.281
mrr vals (pred, true): 0.307, 0.082
batch losses (mrrl, rdl): 0.0, 7.572e-05

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 100
rank avg (pred): 0.323 +- 0.279
mrr vals (pred, true): 0.306, 0.100
batch losses (mrrl, rdl): 0.0, 0.0001883105

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 254
rank avg (pred): 0.020 +- 0.027
mrr vals (pred, true): 0.487, 0.628
batch losses (mrrl, rdl): 0.0, 3e-09

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 657
rank avg (pred): 0.440 +- 0.232
mrr vals (pred, true): 0.162, 0.036
batch losses (mrrl, rdl): 0.0, 6.2349e-06

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 881
rank avg (pred): 0.340 +- 0.289
mrr vals (pred, true): 0.312, 0.042
batch losses (mrrl, rdl): 0.0, 9.39963e-05

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 550
rank avg (pred): 0.296 +- 0.222
mrr vals (pred, true): 0.132, 0.081
batch losses (mrrl, rdl): 0.0, 3.51769e-05

Epoch over!
epoch time: 63.883

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 597
rank avg (pred): 0.433 +- 0.252
mrr vals (pred, true): 0.178, 0.036
batch losses (mrrl, rdl): 0.0, 1.93246e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 843
rank avg (pred): 0.311 +- 0.279
mrr vals (pred, true): 0.345, 0.076
batch losses (mrrl, rdl): 0.0, 2.81314e-05

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 857
rank avg (pred): 0.331 +- 0.283
mrr vals (pred, true): 0.312, 0.085
batch losses (mrrl, rdl): 0.0, 9.48243e-05

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 774
rank avg (pred): 0.309 +- 0.271
mrr vals (pred, true): 0.348, 0.097
batch losses (mrrl, rdl): 0.0, 9.96443e-05

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 880
rank avg (pred): 0.356 +- 0.282
mrr vals (pred, true): 0.280, 0.040
batch losses (mrrl, rdl): 0.0, 4.72404e-05

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 478
rank avg (pred): 0.317 +- 0.269
mrr vals (pred, true): 0.317, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001796074

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 500
rank avg (pred): 0.314 +- 0.221
mrr vals (pred, true): 0.092, 0.104
batch losses (mrrl, rdl): 0.0, 2.09371e-05

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 788
rank avg (pred): 0.327 +- 0.280
mrr vals (pred, true): 0.316, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001516995

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 683
rank avg (pred): 0.412 +- 0.261
mrr vals (pred, true): 0.171, 0.035
batch losses (mrrl, rdl): 0.0, 1.15785e-05

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1094
rank avg (pred): 0.306 +- 0.271
mrr vals (pred, true): 0.352, 0.131
batch losses (mrrl, rdl): 0.0, 0.0001255031

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 133
rank avg (pred): 0.310 +- 0.277
mrr vals (pred, true): 0.352, 0.100
batch losses (mrrl, rdl): 0.0, 0.0001886485

Epoch over!
epoch time: 63.324

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 145
rank avg (pred): 0.310 +- 0.271
mrr vals (pred, true): 0.326, 0.078
batch losses (mrrl, rdl): 0.7614336014, 3.33264e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 751
rank avg (pred): 0.068 +- 0.118
mrr vals (pred, true): 0.463, 0.520
batch losses (mrrl, rdl): 0.0321056768, 3.3684e-05

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 476
rank avg (pred): 0.309 +- 0.180
mrr vals (pred, true): 0.083, 0.039
batch losses (mrrl, rdl): 0.0108969314, 0.0003363368

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1197
rank avg (pred): 0.446 +- 0.160
mrr vals (pred, true): 0.039, 0.039
batch losses (mrrl, rdl): 0.0012643095, 2.72197e-05

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 113
rank avg (pred): 0.334 +- 0.164
mrr vals (pred, true): 0.054, 0.122
batch losses (mrrl, rdl): 0.0462447666, 0.0002289313

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 168
rank avg (pred): 0.334 +- 0.172
mrr vals (pred, true): 0.068, 0.041
batch losses (mrrl, rdl): 0.0031824599, 0.0002288272

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 46
rank avg (pred): 0.099 +- 0.177
mrr vals (pred, true): 0.570, 0.545
batch losses (mrrl, rdl): 0.0059377849, 0.0001671744

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1200
rank avg (pred): 0.393 +- 0.176
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 7.6963e-06, 4.46916e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 797
rank avg (pred): 0.359 +- 0.159
mrr vals (pred, true): 0.062, 0.043
batch losses (mrrl, rdl): 0.0013568171, 0.0001512294

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 887
rank avg (pred): 0.333 +- 0.155
mrr vals (pred, true): 0.065, 0.038
batch losses (mrrl, rdl): 0.0023141117, 0.0002306111

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 8
rank avg (pred): 0.100 +- 0.185
mrr vals (pred, true): 0.623, 0.612
batch losses (mrrl, rdl): 0.0012153642, 0.0002041807

Epoch over!
epoch time: 65.263

Epoch 2 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1114
rank avg (pred): 0.283 +- 0.149
mrr vals (pred, true): 0.071, 0.040
batch losses (mrrl, rdl): 0.0042381142, 0.0005583774

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1120
rank avg (pred): 0.237 +- 0.145
mrr vals (pred, true): 0.092, 0.038
batch losses (mrrl, rdl): 0.0175576154, 0.0008657575

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 356
rank avg (pred): 0.297 +- 0.152
mrr vals (pred, true): 0.072, 0.147
batch losses (mrrl, rdl): 0.0560800917, 0.0001739562

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 65
rank avg (pred): 0.126 +- 0.196
mrr vals (pred, true): 0.568, 0.619
batch losses (mrrl, rdl): 0.0267876014, 0.0003350541

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1060
rank avg (pred): 0.088 +- 0.169
mrr vals (pred, true): 0.632, 0.621
batch losses (mrrl, rdl): 0.0013945932, 0.000151104

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1174
rank avg (pred): 0.384 +- 0.139
mrr vals (pred, true): 0.052, 0.036
batch losses (mrrl, rdl): 4.60158e-05, 4.0065e-05

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 567
rank avg (pred): 0.396 +- 0.126
mrr vals (pred, true): 0.045, 0.030
batch losses (mrrl, rdl): 0.0002900256, 9.07995e-05

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 100
rank avg (pred): 0.246 +- 0.132
mrr vals (pred, true): 0.081, 0.100
batch losses (mrrl, rdl): 0.0035826885, 2.24864e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 228
rank avg (pred): 0.346 +- 0.126
mrr vals (pred, true): 0.051, 0.038
batch losses (mrrl, rdl): 1.71705e-05, 0.0003153049

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 793
rank avg (pred): 0.349 +- 0.130
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 8.4692e-05, 0.0002331396

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 349
rank avg (pred): 0.291 +- 0.137
mrr vals (pred, true): 0.062, 0.127
batch losses (mrrl, rdl): 0.041444011, 0.000116684

Epoch over!
epoch time: 64.927

Epoch 3 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 311
rank avg (pred): 0.115 +- 0.182
mrr vals (pred, true): 0.571, 0.619
batch losses (mrrl, rdl): 0.0233193114, 0.000272205

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1137
rank avg (pred): 0.163 +- 0.157
mrr vals (pred, true): 0.111, 0.165
batch losses (mrrl, rdl): 0.0291979723, 3.36215e-05

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 397
rank avg (pred): 0.322 +- 0.133
mrr vals (pred, true): 0.058, 0.126
batch losses (mrrl, rdl): 0.0456693433, 0.0002360419

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 428
rank avg (pred): 0.168 +- 0.117
mrr vals (pred, true): 0.105, 0.037
batch losses (mrrl, rdl): 0.0300903507, 0.0015759186

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1004
rank avg (pred): 0.181 +- 0.130
mrr vals (pred, true): 0.107, 0.137
batch losses (mrrl, rdl): 0.0087464256, 8.95172e-05

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1070
rank avg (pred): 0.091 +- 0.159
mrr vals (pred, true): 0.616, 0.602
batch losses (mrrl, rdl): 0.0017202012, 0.0001561051

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 630
rank avg (pred): 0.346 +- 0.103
mrr vals (pred, true): 0.046, 0.032
batch losses (mrrl, rdl): 0.0001528933, 0.0002693944

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 355
rank avg (pred): 0.303 +- 0.124
mrr vals (pred, true): 0.062, 0.109
batch losses (mrrl, rdl): 0.021384282, 7.60004e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 861
rank avg (pred): 0.316 +- 0.118
mrr vals (pred, true): 0.060, 0.081
batch losses (mrrl, rdl): 0.0010252408, 3.78139e-05

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 702
rank avg (pred): 0.352 +- 0.092
mrr vals (pred, true): 0.050, 0.038
batch losses (mrrl, rdl): 5.299e-07, 0.000268813

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 734
rank avg (pred): 0.105 +- 0.153
mrr vals (pred, true): 0.412, 0.512
batch losses (mrrl, rdl): 0.1012623757, 0.0001440732

Epoch over!
epoch time: 65.606

Epoch 4 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 398
rank avg (pred): 0.244 +- 0.109
mrr vals (pred, true): 0.068, 0.139
batch losses (mrrl, rdl): 0.0503559448, 2.96605e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 695
rank avg (pred): 0.349 +- 0.090
mrr vals (pred, true): 0.047, 0.037
batch losses (mrrl, rdl): 8.29393e-05, 0.0003539361

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1038
rank avg (pred): 0.200 +- 0.113
mrr vals (pred, true): 0.094, 0.038
batch losses (mrrl, rdl): 0.019361617, 0.0012617663

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 843
rank avg (pred): 0.296 +- 0.117
mrr vals (pred, true): 0.062, 0.076
batch losses (mrrl, rdl): 0.0013494311, 3.56729e-05

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1208
rank avg (pred): 0.365 +- 0.146
mrr vals (pred, true): 0.047, 0.035
batch losses (mrrl, rdl): 8.41126e-05, 0.0002405306

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 324
rank avg (pred): 0.291 +- 0.110
mrr vals (pred, true): 0.060, 0.101
batch losses (mrrl, rdl): 0.0172667578, 3.86072e-05

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 636
rank avg (pred): 0.335 +- 0.086
mrr vals (pred, true): 0.048, 0.035
batch losses (mrrl, rdl): 5.5339e-05, 0.0002083808

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 968
rank avg (pred): 0.262 +- 0.108
mrr vals (pred, true): 0.064, 0.039
batch losses (mrrl, rdl): 0.002015606, 0.0008686401

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1130
rank avg (pred): 0.214 +- 0.096
mrr vals (pred, true): 0.079, 0.044
batch losses (mrrl, rdl): 0.00846342, 0.0010015958

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 484
rank avg (pred): 0.280 +- 0.108
mrr vals (pred, true): 0.063, 0.037
batch losses (mrrl, rdl): 0.0016611503, 0.0006148185

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1207
rank avg (pred): 0.342 +- 0.101
mrr vals (pred, true): 0.043, 0.040
batch losses (mrrl, rdl): 0.0005303414, 0.0002418262

Epoch over!
epoch time: 64.748

Epoch 5 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 926
rank avg (pred): 0.318 +- 0.081
mrr vals (pred, true): 0.051, 0.076
batch losses (mrrl, rdl): 2.07232e-05, 4.85462e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 7
rank avg (pred): 0.085 +- 0.137
mrr vals (pred, true): 0.555, 0.544
batch losses (mrrl, rdl): 0.0011689386, 9.72298e-05

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 647
rank avg (pred): 0.319 +- 0.072
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 0.0001011001, 0.0004336352

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1153
rank avg (pred): 0.136 +- 0.119
mrr vals (pred, true): 0.126, 0.161
batch losses (mrrl, rdl): 0.0116125848, 0.0002052272

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1146
rank avg (pred): 0.134 +- 0.122
mrr vals (pred, true): 0.120, 0.150
batch losses (mrrl, rdl): 0.0092025166, 0.000165356

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 145
rank avg (pred): 0.282 +- 0.099
mrr vals (pred, true): 0.058, 0.078
batch losses (mrrl, rdl): 0.0006521301, 5.15906e-05

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 543
rank avg (pred): 0.148 +- 0.107
mrr vals (pred, true): 0.090, 0.066
batch losses (mrrl, rdl): 0.01602838, 0.0009203717

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 62
rank avg (pred): 0.079 +- 0.129
mrr vals (pred, true): 0.547, 0.612
batch losses (mrrl, rdl): 0.0429285318, 9.92313e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1100
rank avg (pred): 0.192 +- 0.091
mrr vals (pred, true): 0.070, 0.117
batch losses (mrrl, rdl): 0.0218105651, 0.000139078

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 503
rank avg (pred): 0.139 +- 0.109
mrr vals (pred, true): 0.099, 0.122
batch losses (mrrl, rdl): 0.005207723, 0.0003773315

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1173
rank avg (pred): 0.302 +- 0.071
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 2.13e-08, 0.0003974562

Epoch over!
epoch time: 65.695

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 864
rank avg (pred): 0.274 +- 0.092
mrr vals (pred, true): 0.057, 0.041
batch losses (mrrl, rdl): 0.0004487006, 0.000706548

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 108
rank avg (pred): 0.240 +- 0.092
mrr vals (pred, true): 0.064, 0.079
batch losses (mrrl, rdl): 0.0019860175, 7.32549e-05

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 671
rank avg (pred): 0.275 +- 0.070
mrr vals (pred, true): 0.045, 0.040
batch losses (mrrl, rdl): 0.000231516, 0.0007746962

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 436
rank avg (pred): 0.265 +- 0.092
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 1.6325e-06, 0.0008331528

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 627
rank avg (pred): 0.294 +- 0.064
mrr vals (pred, true): 0.038, 0.036
batch losses (mrrl, rdl): 0.0013857655, 0.0004410309

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1138
rank avg (pred): 0.123 +- 0.103
mrr vals (pred, true): 0.116, 0.158
batch losses (mrrl, rdl): 0.0173159949, 0.0001968448

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 453
rank avg (pred): 0.243 +- 0.081
mrr vals (pred, true): 0.058, 0.040
batch losses (mrrl, rdl): 0.0006007822, 0.0008470898

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 999
rank avg (pred): 0.160 +- 0.091
mrr vals (pred, true): 0.096, 0.103
batch losses (mrrl, rdl): 0.0005165866, 0.000306837

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 404
rank avg (pred): 0.198 +- 0.074
mrr vals (pred, true): 0.073, 0.145
batch losses (mrrl, rdl): 0.0514385849, 3.79068e-05

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 690
rank avg (pred): 0.367 +- 0.147
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 2.60437e-05, 0.0002728845

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 710
rank avg (pred): 0.369 +- 0.136
mrr vals (pred, true): 0.043, 0.036
batch losses (mrrl, rdl): 0.0004746171, 0.0002245287

Epoch over!
epoch time: 65.517

Epoch 7 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 18
rank avg (pred): 0.077 +- 0.117
mrr vals (pred, true): 0.515, 0.474
batch losses (mrrl, rdl): 0.0171588399, 6.25004e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 940
rank avg (pred): 0.253 +- 0.077
mrr vals (pred, true): 0.056, 0.090
batch losses (mrrl, rdl): 0.0003834362, 0.0001020429

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 322
rank avg (pred): 0.068 +- 0.114
mrr vals (pred, true): 0.587, 0.559
batch losses (mrrl, rdl): 0.0078705363, 5.64054e-05

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 459
rank avg (pred): 0.286 +- 0.117
mrr vals (pred, true): 0.071, 0.045
batch losses (mrrl, rdl): 0.0043471814, 0.0004890435

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 483
rank avg (pred): 0.291 +- 0.109
mrr vals (pred, true): 0.053, 0.039
batch losses (mrrl, rdl): 0.000119072, 0.0005863789

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 200
rank avg (pred): 0.319 +- 0.136
mrr vals (pred, true): 0.057, 0.039
batch losses (mrrl, rdl): 0.0004485688, 0.0003230972

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 393
rank avg (pred): 0.269 +- 0.093
mrr vals (pred, true): 0.057, 0.097
batch losses (mrrl, rdl): 0.0005393937, 5.01389e-05

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 119
rank avg (pred): 0.227 +- 0.085
mrr vals (pred, true): 0.066, 0.114
batch losses (mrrl, rdl): 0.022883568, 3.96602e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 905
rank avg (pred): 0.078 +- 0.109
mrr vals (pred, true): 0.369, 0.315
batch losses (mrrl, rdl): 0.028808035, 1.07997e-05

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 277
rank avg (pred): 0.077 +- 0.113
mrr vals (pred, true): 0.472, 0.570
batch losses (mrrl, rdl): 0.0948370844, 7.31282e-05

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 548
rank avg (pred): 0.114 +- 0.095
mrr vals (pred, true): 0.121, 0.127
batch losses (mrrl, rdl): 0.0003793475, 0.0007161769

Epoch over!
epoch time: 64.895

Epoch 8 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1148
rank avg (pred): 0.113 +- 0.098
mrr vals (pred, true): 0.133, 0.143
batch losses (mrrl, rdl): 0.0010150808, 0.000390479

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 645
rank avg (pred): 0.265 +- 0.087
mrr vals (pred, true): 0.062, 0.038
batch losses (mrrl, rdl): 0.0014046107, 0.0005495332

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1088
rank avg (pred): 0.132 +- 0.115
mrr vals (pred, true): 0.148, 0.134
batch losses (mrrl, rdl): 0.0019024538, 0.0002281989

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 866
rank avg (pred): 0.359 +- 0.173
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 7.6915e-06, 0.0001477386

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 26
rank avg (pred): 0.064 +- 0.105
mrr vals (pred, true): 0.596, 0.619
batch losses (mrrl, rdl): 0.0050335638, 5.63553e-05

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1109
rank avg (pred): 0.162 +- 0.074
mrr vals (pred, true): 0.079, 0.038
batch losses (mrrl, rdl): 0.008155507, 0.0016155895

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 198
rank avg (pred): 0.397 +- 0.236
mrr vals (pred, true): 0.057, 0.041
batch losses (mrrl, rdl): 0.0004340701, 3.95107e-05

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 447
rank avg (pred): 0.339 +- 0.199
mrr vals (pred, true): 0.056, 0.044
batch losses (mrrl, rdl): 0.0003513273, 0.0002006521

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 265
rank avg (pred): 0.066 +- 0.103
mrr vals (pred, true): 0.584, 0.546
batch losses (mrrl, rdl): 0.0146030737, 4.86034e-05

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 719
rank avg (pred): 0.353 +- 0.182
mrr vals (pred, true): 0.041, 0.039
batch losses (mrrl, rdl): 0.0007878105, 0.0002207356

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 361
rank avg (pred): 0.313 +- 0.178
mrr vals (pred, true): 0.063, 0.114
batch losses (mrrl, rdl): 0.0266004428, 5.42057e-05

Epoch over!
epoch time: 64.39

Epoch 9 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 80
rank avg (pred): 0.049 +- 0.088
mrr vals (pred, true): 0.603, 0.615
batch losses (mrrl, rdl): 0.0013496865, 2.50474e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 636
rank avg (pred): 0.450 +- 0.284
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 2.30823e-05, 1.62589e-05

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 904
rank avg (pred): 0.084 +- 0.106
mrr vals (pred, true): 0.292, 0.306
batch losses (mrrl, rdl): 0.0019292957, 1.94845e-05

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 849
rank avg (pred): 0.326 +- 0.189
mrr vals (pred, true): 0.063, 0.091
batch losses (mrrl, rdl): 0.0018111843, 3.34067e-05

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1166
rank avg (pred): 0.304 +- 0.177
mrr vals (pred, true): 0.056, 0.037
batch losses (mrrl, rdl): 0.0003286112, 0.0002205732

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1122
rank avg (pred): 0.122 +- 0.096
mrr vals (pred, true): 0.137, 0.041
batch losses (mrrl, rdl): 0.0764463022, 0.001992418

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1193
rank avg (pred): 0.237 +- 0.055
mrr vals (pred, true): 0.050, 0.039
batch losses (mrrl, rdl): 2.3158e-06, 0.001036919

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 657
rank avg (pred): 0.388 +- 0.254
mrr vals (pred, true): 0.046, 0.036
batch losses (mrrl, rdl): 0.0001414126, 0.0001423679

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 546
rank avg (pred): 0.128 +- 0.089
mrr vals (pred, true): 0.103, 0.065
batch losses (mrrl, rdl): 0.0279193781, 0.0009476345

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 91
rank avg (pred): 0.373 +- 0.275
mrr vals (pred, true): 0.061, 0.102
batch losses (mrrl, rdl): 0.0168479867, 0.0002663081

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 429
rank avg (pred): 0.312 +- 0.176
mrr vals (pred, true): 0.067, 0.038
batch losses (mrrl, rdl): 0.0030270503, 0.0003696241

Epoch over!
epoch time: 65.42

Epoch 10 -- 
running batch: 0 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 622
rank avg (pred): 0.395 +- 0.252
mrr vals (pred, true): 0.043, 0.037
batch losses (mrrl, rdl): 0.0004643505, 6.39888e-05

running batch: 500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 210
rank avg (pred): 0.384 +- 0.274
mrr vals (pred, true): 0.071, 0.038
batch losses (mrrl, rdl): 0.0042030779, 0.0001161975

running batch: 1000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 116
rank avg (pred): 0.384 +- 0.266
mrr vals (pred, true): 0.056, 0.107
batch losses (mrrl, rdl): 0.0252720192, 0.0002418071

running batch: 1500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 646
rank avg (pred): 0.254 +- 0.099
mrr vals (pred, true): 0.040, 0.035
batch losses (mrrl, rdl): 0.0009367073, 0.0007147194

running batch: 2000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 215
rank avg (pred): 0.186 +- 0.070
mrr vals (pred, true): 0.072, 0.038
batch losses (mrrl, rdl): 0.0048867962, 0.0015220538

running batch: 2500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1080
rank avg (pred): 0.183 +- 0.075
mrr vals (pred, true): 0.074, 0.110
batch losses (mrrl, rdl): 0.0131605389, 0.0002406756

running batch: 3000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 484
rank avg (pred): 0.394 +- 0.295
mrr vals (pred, true): 0.057, 0.037
batch losses (mrrl, rdl): 0.0004428754, 8.86281e-05

running batch: 3500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 37
rank avg (pred): 0.062 +- 0.097
mrr vals (pred, true): 0.553, 0.573
batch losses (mrrl, rdl): 0.0042413007, 3.43354e-05

running batch: 4000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1166
rank avg (pred): 0.219 +- 0.057
mrr vals (pred, true): 0.045, 0.037
batch losses (mrrl, rdl): 0.0002544966, 0.0007754925

running batch: 4500 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 398
rank avg (pred): 0.220 +- 0.120
mrr vals (pred, true): 0.065, 0.139
batch losses (mrrl, rdl): 0.0544821471, 3.42151e-05

running batch: 5000 / 5470 and superbatch(1); data from TransE, UMLS, run 2.1, exp 446
rank avg (pred): 0.325 +- 0.230
mrr vals (pred, true): 0.060, 0.035
batch losses (mrrl, rdl): 0.0009679233, 0.0003731517

Epoch over!
epoch time: 65.61

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.065 +- 0.093
mrr vals (pred, true): 0.557, 0.431

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   10 	     0 	 0.03967 	 0.03230 	 ~...
    2 	     1 	 0.03729 	 0.03379 	 ~...
   58 	     2 	 0.07411 	 0.03476 	 m..s
   22 	     3 	 0.04290 	 0.03530 	 ~...
   12 	     4 	 0.04051 	 0.03550 	 ~...
   68 	     5 	 0.08163 	 0.03604 	 m..s
    3 	     6 	 0.03734 	 0.03611 	 ~...
   18 	     7 	 0.04198 	 0.03617 	 ~...
   50 	     8 	 0.06483 	 0.03691 	 ~...
    5 	     9 	 0.03775 	 0.03694 	 ~...
   17 	    10 	 0.04177 	 0.03703 	 ~...
   28 	    11 	 0.06351 	 0.03706 	 ~...
   72 	    12 	 0.08459 	 0.03785 	 m..s
    4 	    13 	 0.03737 	 0.03793 	 ~...
   28 	    14 	 0.06351 	 0.03811 	 ~...
   28 	    15 	 0.06351 	 0.03827 	 ~...
    6 	    16 	 0.03794 	 0.03828 	 ~...
   23 	    17 	 0.04364 	 0.03845 	 ~...
    9 	    18 	 0.03848 	 0.03856 	 ~...
   69 	    19 	 0.08209 	 0.03905 	 m..s
   15 	    20 	 0.04144 	 0.03915 	 ~...
   24 	    21 	 0.04440 	 0.03926 	 ~...
   49 	    22 	 0.06459 	 0.03954 	 ~...
   28 	    23 	 0.06351 	 0.03954 	 ~...
   65 	    24 	 0.07765 	 0.03963 	 m..s
   28 	    25 	 0.06351 	 0.03982 	 ~...
    8 	    26 	 0.03820 	 0.04001 	 ~...
   11 	    27 	 0.04017 	 0.04012 	 ~...
    1 	    28 	 0.03720 	 0.04021 	 ~...
   48 	    29 	 0.06384 	 0.04024 	 ~...
   63 	    30 	 0.07577 	 0.04036 	 m..s
   61 	    31 	 0.07495 	 0.04052 	 m..s
   28 	    32 	 0.06351 	 0.04060 	 ~...
   64 	    33 	 0.07590 	 0.04063 	 m..s
   21 	    34 	 0.04253 	 0.04075 	 ~...
   14 	    35 	 0.04073 	 0.04075 	 ~...
    0 	    36 	 0.03690 	 0.04081 	 ~...
   28 	    37 	 0.06351 	 0.04109 	 ~...
   27 	    38 	 0.06223 	 0.04165 	 ~...
   60 	    39 	 0.07476 	 0.04235 	 m..s
   13 	    40 	 0.04055 	 0.04239 	 ~...
   19 	    41 	 0.04213 	 0.04241 	 ~...
   16 	    42 	 0.04167 	 0.04243 	 ~...
    7 	    43 	 0.03799 	 0.04265 	 ~...
   26 	    44 	 0.05796 	 0.04276 	 ~...
   28 	    45 	 0.06351 	 0.04281 	 ~...
   81 	    46 	 0.11381 	 0.04329 	 m..s
   73 	    47 	 0.09254 	 0.04377 	 m..s
   62 	    48 	 0.07544 	 0.04576 	 ~...
   20 	    49 	 0.04228 	 0.04595 	 ~...
   28 	    50 	 0.06351 	 0.04611 	 ~...
   78 	    51 	 0.09985 	 0.06163 	 m..s
   76 	    52 	 0.09751 	 0.06424 	 m..s
   74 	    53 	 0.09555 	 0.06558 	 ~...
   75 	    54 	 0.09725 	 0.06822 	 ~...
   28 	    55 	 0.06351 	 0.06999 	 ~...
   79 	    56 	 0.10347 	 0.07279 	 m..s
   80 	    57 	 0.10479 	 0.07306 	 m..s
   28 	    58 	 0.06351 	 0.07870 	 ~...
   28 	    59 	 0.06351 	 0.08279 	 ~...
   52 	    60 	 0.06507 	 0.08291 	 ~...
   28 	    61 	 0.06351 	 0.08605 	 ~...
   28 	    62 	 0.06351 	 0.08768 	 ~...
   59 	    63 	 0.07441 	 0.08861 	 ~...
   56 	    64 	 0.07375 	 0.08934 	 ~...
   57 	    65 	 0.07376 	 0.08968 	 ~...
   51 	    66 	 0.06506 	 0.08985 	 ~...
   25 	    67 	 0.05597 	 0.09002 	 m..s
   28 	    68 	 0.06351 	 0.09072 	 ~...
   28 	    69 	 0.06351 	 0.09163 	 ~...
   28 	    70 	 0.06351 	 0.09245 	 ~...
   47 	    71 	 0.06377 	 0.09552 	 m..s
   28 	    72 	 0.06351 	 0.09676 	 m..s
   28 	    73 	 0.06351 	 0.09778 	 m..s
   53 	    74 	 0.06509 	 0.09934 	 m..s
   55 	    75 	 0.06530 	 0.11853 	 m..s
   71 	    76 	 0.08251 	 0.11978 	 m..s
   66 	    77 	 0.07804 	 0.12398 	 m..s
   54 	    78 	 0.06514 	 0.12642 	 m..s
   70 	    79 	 0.08248 	 0.13608 	 m..s
   83 	    80 	 0.11471 	 0.13895 	 ~...
   77 	    81 	 0.09860 	 0.14478 	 m..s
   67 	    82 	 0.07811 	 0.14833 	 m..s
   86 	    83 	 0.16894 	 0.14944 	 ~...
   82 	    84 	 0.11417 	 0.15032 	 m..s
   89 	    85 	 0.20596 	 0.15235 	 m..s
   84 	    86 	 0.11616 	 0.15802 	 m..s
   85 	    87 	 0.16624 	 0.15811 	 ~...
   88 	    88 	 0.19431 	 0.16790 	 ~...
   87 	    89 	 0.18444 	 0.20235 	 ~...
   90 	    90 	 0.37187 	 0.25523 	 MISS
   92 	    91 	 0.54409 	 0.42415 	 MISS
   94 	    92 	 0.55629 	 0.42680 	 MISS
   95 	    93 	 0.55688 	 0.43062 	 MISS
   93 	    94 	 0.55387 	 0.43982 	 MISS
  104 	    95 	 0.59088 	 0.50669 	 m..s
   91 	    96 	 0.53404 	 0.50860 	 ~...
  105 	    97 	 0.59125 	 0.51020 	 m..s
   96 	    98 	 0.55792 	 0.51624 	 m..s
   98 	    99 	 0.56206 	 0.53780 	 ~...
   97 	   100 	 0.56019 	 0.54273 	 ~...
  101 	   101 	 0.56988 	 0.54415 	 ~...
  100 	   102 	 0.56783 	 0.54980 	 ~...
  107 	   103 	 0.59891 	 0.55065 	 m..s
  102 	   104 	 0.57043 	 0.55234 	 ~...
  111 	   105 	 0.61203 	 0.55704 	 m..s
   99 	   106 	 0.56526 	 0.56007 	 ~...
  109 	   107 	 0.60128 	 0.56964 	 m..s
  108 	   108 	 0.60033 	 0.57114 	 ~...
  103 	   109 	 0.58426 	 0.57514 	 ~...
  115 	   110 	 0.62859 	 0.58102 	 m..s
  106 	   111 	 0.59808 	 0.58131 	 ~...
  116 	   112 	 0.63021 	 0.59711 	 m..s
  113 	   113 	 0.62765 	 0.60413 	 ~...
  119 	   114 	 0.64044 	 0.60425 	 m..s
  112 	   115 	 0.61665 	 0.60653 	 ~...
  114 	   116 	 0.62768 	 0.60827 	 ~...
  118 	   117 	 0.63968 	 0.61509 	 ~...
  120 	   118 	 0.64166 	 0.61970 	 ~...
  117 	   119 	 0.63445 	 0.63193 	 ~...
  110 	   120 	 0.60747 	 0.63332 	 ~...
==========================================
r_mrr = 0.9882776737213135
r2_mrr = 0.9668163657188416
spearmanr_mrr@5 = 0.9213004112243652
spearmanr_mrr@10 = 0.9613844752311707
spearmanr_mrr@50 = 0.988024115562439
spearmanr_mrr@100 = 0.9930733442306519
spearmanr_mrr@All = 0.9938265085220337
==========================================
test time: 0.94
Done Testing dataset UMLS
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.161 +- 0.198
mrr vals (pred, true): 0.134, 0.121

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.02977 	 0.00315 	 ~...
   46 	     1 	 0.07327 	 0.00336 	 m..s
   20 	     2 	 0.03280 	 0.00352 	 ~...
    1 	     3 	 0.02150 	 0.00356 	 ~...
   17 	     4 	 0.03201 	 0.00357 	 ~...
   36 	     5 	 0.06727 	 0.00364 	 m..s
   27 	     6 	 0.03652 	 0.00364 	 m..s
   46 	     7 	 0.07327 	 0.00368 	 m..s
   69 	     8 	 0.07592 	 0.00378 	 m..s
   13 	     9 	 0.03041 	 0.00381 	 ~...
   12 	    10 	 0.03029 	 0.00383 	 ~...
   44 	    11 	 0.07312 	 0.00384 	 m..s
   46 	    12 	 0.07327 	 0.00385 	 m..s
   46 	    13 	 0.07327 	 0.00390 	 m..s
    7 	    14 	 0.02918 	 0.00391 	 ~...
   77 	    15 	 0.07655 	 0.00393 	 m..s
   46 	    16 	 0.07327 	 0.00393 	 m..s
   83 	    17 	 0.07780 	 0.00404 	 m..s
   35 	    18 	 0.06648 	 0.00405 	 m..s
   42 	    19 	 0.07282 	 0.00413 	 m..s
   75 	    20 	 0.07646 	 0.00414 	 m..s
   46 	    21 	 0.07327 	 0.00415 	 m..s
   41 	    22 	 0.07274 	 0.00420 	 m..s
   14 	    23 	 0.03046 	 0.00421 	 ~...
   68 	    24 	 0.07590 	 0.00422 	 m..s
   18 	    25 	 0.03241 	 0.00424 	 ~...
   46 	    26 	 0.07327 	 0.00425 	 m..s
   46 	    27 	 0.07327 	 0.00425 	 m..s
   80 	    28 	 0.07665 	 0.00426 	 m..s
    0 	    29 	 0.01863 	 0.00442 	 ~...
   86 	    30 	 0.08321 	 0.00442 	 m..s
   79 	    31 	 0.07661 	 0.00454 	 m..s
   81 	    32 	 0.07677 	 0.00458 	 m..s
   74 	    33 	 0.07643 	 0.00464 	 m..s
   46 	    34 	 0.07327 	 0.00486 	 m..s
   82 	    35 	 0.07712 	 0.00499 	 m..s
   78 	    36 	 0.07657 	 0.00508 	 m..s
    4 	    37 	 0.02551 	 0.00707 	 ~...
   16 	    38 	 0.03187 	 0.00724 	 ~...
    5 	    39 	 0.02598 	 0.00810 	 ~...
    2 	    40 	 0.02238 	 0.00822 	 ~...
    3 	    41 	 0.02441 	 0.00972 	 ~...
    8 	    42 	 0.02924 	 0.01098 	 ~...
   11 	    43 	 0.03008 	 0.01105 	 ~...
   21 	    44 	 0.03299 	 0.01274 	 ~...
   26 	    45 	 0.03616 	 0.01504 	 ~...
   23 	    46 	 0.03485 	 0.01552 	 ~...
   25 	    47 	 0.03604 	 0.01618 	 ~...
   28 	    48 	 0.03716 	 0.01761 	 ~...
    6 	    49 	 0.02851 	 0.01879 	 ~...
   30 	    50 	 0.03956 	 0.01920 	 ~...
   29 	    51 	 0.03866 	 0.01991 	 ~...
   10 	    52 	 0.03006 	 0.02011 	 ~...
   19 	    53 	 0.03276 	 0.02020 	 ~...
   15 	    54 	 0.03119 	 0.02159 	 ~...
   31 	    55 	 0.04246 	 0.02199 	 ~...
   24 	    56 	 0.03572 	 0.02290 	 ~...
   32 	    57 	 0.04866 	 0.02514 	 ~...
   22 	    58 	 0.03444 	 0.02563 	 ~...
   33 	    59 	 0.04959 	 0.02676 	 ~...
   46 	    60 	 0.07327 	 0.03517 	 m..s
   37 	    61 	 0.07090 	 0.03527 	 m..s
   43 	    62 	 0.07309 	 0.03683 	 m..s
   46 	    63 	 0.07327 	 0.04331 	 ~...
   46 	    64 	 0.07327 	 0.10935 	 m..s
   95 	    65 	 0.13219 	 0.11533 	 ~...
   46 	    66 	 0.07327 	 0.11742 	 m..s
   40 	    67 	 0.07267 	 0.11872 	 m..s
   99 	    68 	 0.13356 	 0.12102 	 ~...
   98 	    69 	 0.13324 	 0.12241 	 ~...
   90 	    70 	 0.09031 	 0.12278 	 m..s
   39 	    71 	 0.07266 	 0.12419 	 m..s
   45 	    72 	 0.07315 	 0.13746 	 m..s
   92 	    73 	 0.12851 	 0.13844 	 ~...
  101 	    74 	 0.14158 	 0.14165 	 ~...
   46 	    75 	 0.07327 	 0.14277 	 m..s
   38 	    76 	 0.07266 	 0.14642 	 m..s
  100 	    77 	 0.13851 	 0.14851 	 ~...
   96 	    78 	 0.13264 	 0.15089 	 ~...
   93 	    79 	 0.12905 	 0.15188 	 ~...
  102 	    80 	 0.14207 	 0.15215 	 ~...
   34 	    81 	 0.06456 	 0.15364 	 m..s
   46 	    82 	 0.07327 	 0.15816 	 m..s
   46 	    83 	 0.07327 	 0.16385 	 m..s
   46 	    84 	 0.07327 	 0.16469 	 m..s
   46 	    85 	 0.07327 	 0.16581 	 m..s
   65 	    86 	 0.07351 	 0.17084 	 m..s
   66 	    87 	 0.07359 	 0.17104 	 m..s
   46 	    88 	 0.07327 	 0.17394 	 MISS
  105 	    89 	 0.15423 	 0.17472 	 ~...
   91 	    90 	 0.11629 	 0.17536 	 m..s
   97 	    91 	 0.13320 	 0.17881 	 m..s
  115 	    92 	 0.20602 	 0.17958 	 ~...
   94 	    93 	 0.12928 	 0.18288 	 m..s
  109 	    94 	 0.17758 	 0.18566 	 ~...
   76 	    95 	 0.07649 	 0.18821 	 MISS
  116 	    96 	 0.20663 	 0.18868 	 ~...
   72 	    97 	 0.07634 	 0.18871 	 MISS
   73 	    98 	 0.07634 	 0.18895 	 MISS
  110 	    99 	 0.17930 	 0.19419 	 ~...
   70 	   100 	 0.07616 	 0.19491 	 MISS
   85 	   101 	 0.07795 	 0.20091 	 MISS
   71 	   102 	 0.07617 	 0.20263 	 MISS
  107 	   103 	 0.16569 	 0.20334 	 m..s
   84 	   104 	 0.07795 	 0.20390 	 MISS
   67 	   105 	 0.07515 	 0.21011 	 MISS
   89 	   106 	 0.08355 	 0.21234 	 MISS
   87 	   107 	 0.08326 	 0.21271 	 MISS
   88 	   108 	 0.08334 	 0.21316 	 MISS
  104 	   109 	 0.15421 	 0.21514 	 m..s
  103 	   110 	 0.14478 	 0.22588 	 m..s
  112 	   111 	 0.18885 	 0.22604 	 m..s
  114 	   112 	 0.20514 	 0.23410 	 ~...
  113 	   113 	 0.20211 	 0.23548 	 m..s
  108 	   114 	 0.17043 	 0.24311 	 m..s
  111 	   115 	 0.17948 	 0.24444 	 m..s
  106 	   116 	 0.16371 	 0.25607 	 m..s
  120 	   117 	 0.22374 	 0.25940 	 m..s
  119 	   118 	 0.22331 	 0.26202 	 m..s
  118 	   119 	 0.22086 	 0.26317 	 m..s
  117 	   120 	 0.21577 	 0.26458 	 m..s
==========================================
r_mrr = 0.7584270238876343
r2_mrr = 0.5377940535545349
spearmanr_mrr@5 = 0.9837197661399841
spearmanr_mrr@10 = 0.9472402930259705
spearmanr_mrr@50 = 0.9724361300468445
spearmanr_mrr@100 = 0.8409507274627686
spearmanr_mrr@All = 0.8740391135215759
==========================================
test time: 0.404
Done Testing dataset CoDExSmall
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.154 +- 0.269
mrr vals (pred, true): 0.079, 0.142

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   77 	     0 	 0.06546 	 0.00017 	 m..s
   46 	     1 	 0.06528 	 0.00019 	 m..s
   75 	     2 	 0.06544 	 0.00020 	 m..s
    1 	     3 	 0.02721 	 0.00021 	 ~...
    9 	     4 	 0.03724 	 0.00021 	 m..s
   16 	     5 	 0.04015 	 0.00021 	 m..s
   79 	     6 	 0.06546 	 0.00022 	 m..s
   72 	     7 	 0.06543 	 0.00022 	 m..s
   15 	     8 	 0.04001 	 0.00022 	 m..s
   46 	     9 	 0.06528 	 0.00023 	 m..s
   31 	    10 	 0.04967 	 0.00023 	 m..s
   41 	    11 	 0.06525 	 0.00025 	 m..s
   83 	    12 	 0.06559 	 0.00026 	 m..s
    0 	    13 	 0.02509 	 0.00027 	 ~...
   69 	    14 	 0.06540 	 0.00028 	 m..s
   46 	    15 	 0.06528 	 0.00028 	 m..s
   78 	    16 	 0.06546 	 0.00029 	 m..s
   82 	    17 	 0.06551 	 0.00031 	 m..s
   25 	    18 	 0.04579 	 0.00032 	 m..s
   46 	    19 	 0.06528 	 0.00032 	 m..s
   46 	    20 	 0.06528 	 0.00033 	 m..s
   46 	    21 	 0.06528 	 0.00034 	 m..s
   80 	    22 	 0.06546 	 0.00037 	 m..s
   46 	    23 	 0.06528 	 0.00039 	 m..s
   21 	    24 	 0.04277 	 0.00041 	 m..s
   11 	    25 	 0.03879 	 0.00045 	 m..s
   42 	    26 	 0.06526 	 0.00045 	 m..s
   35 	    27 	 0.06459 	 0.00046 	 m..s
   46 	    28 	 0.06528 	 0.00046 	 m..s
   81 	    29 	 0.06548 	 0.00049 	 m..s
   18 	    30 	 0.04137 	 0.00051 	 m..s
   27 	    31 	 0.04642 	 0.00062 	 m..s
   68 	    32 	 0.06539 	 0.00063 	 m..s
   44 	    33 	 0.06527 	 0.00074 	 m..s
   36 	    34 	 0.06485 	 0.00082 	 m..s
   86 	    35 	 0.06617 	 0.00083 	 m..s
   46 	    36 	 0.06528 	 0.00136 	 m..s
   23 	    37 	 0.04478 	 0.00194 	 m..s
    4 	    38 	 0.03176 	 0.00221 	 ~...
    3 	    39 	 0.03026 	 0.00270 	 ~...
    5 	    40 	 0.03247 	 0.00351 	 ~...
   28 	    41 	 0.04669 	 0.00612 	 m..s
    2 	    42 	 0.02802 	 0.00842 	 ~...
   46 	    43 	 0.06528 	 0.01251 	 m..s
   20 	    44 	 0.04166 	 0.02183 	 ~...
   29 	    45 	 0.04826 	 0.02359 	 ~...
   12 	    46 	 0.03885 	 0.02901 	 ~...
   19 	    47 	 0.04150 	 0.03317 	 ~...
   46 	    48 	 0.06528 	 0.03325 	 m..s
    7 	    49 	 0.03438 	 0.03392 	 ~...
   40 	    50 	 0.06525 	 0.03716 	 ~...
    6 	    51 	 0.03347 	 0.03948 	 ~...
   22 	    52 	 0.04309 	 0.03972 	 ~...
   10 	    53 	 0.03869 	 0.04061 	 ~...
   30 	    54 	 0.04916 	 0.04198 	 ~...
   26 	    55 	 0.04635 	 0.04199 	 ~...
   39 	    56 	 0.06525 	 0.04262 	 ~...
   14 	    57 	 0.03921 	 0.04366 	 ~...
    8 	    58 	 0.03635 	 0.04655 	 ~...
   45 	    59 	 0.06528 	 0.05418 	 ~...
   46 	    60 	 0.06528 	 0.05421 	 ~...
   99 	    61 	 0.08002 	 0.05784 	 ~...
   95 	    62 	 0.07825 	 0.05840 	 ~...
   24 	    63 	 0.04541 	 0.06066 	 ~...
   65 	    64 	 0.06530 	 0.06100 	 ~...
   93 	    65 	 0.07806 	 0.06577 	 ~...
   38 	    66 	 0.06525 	 0.06677 	 ~...
   98 	    67 	 0.07983 	 0.06731 	 ~...
   13 	    68 	 0.03892 	 0.07127 	 m..s
   66 	    69 	 0.06530 	 0.07569 	 ~...
   91 	    70 	 0.07419 	 0.08153 	 ~...
  103 	    71 	 0.08665 	 0.08798 	 ~...
   37 	    72 	 0.06501 	 0.08805 	 ~...
  105 	    73 	 0.09361 	 0.09490 	 ~...
   33 	    74 	 0.05153 	 0.09770 	 m..s
   67 	    75 	 0.06535 	 0.09822 	 m..s
   70 	    76 	 0.06541 	 0.10178 	 m..s
   32 	    77 	 0.04999 	 0.10243 	 m..s
   90 	    78 	 0.06733 	 0.10630 	 m..s
   17 	    79 	 0.04082 	 0.10806 	 m..s
   46 	    80 	 0.06528 	 0.11379 	 m..s
  104 	    81 	 0.09215 	 0.11415 	 ~...
   46 	    82 	 0.06528 	 0.12008 	 m..s
  107 	    83 	 0.10357 	 0.13136 	 ~...
  110 	    84 	 0.12831 	 0.13370 	 ~...
   96 	    85 	 0.07873 	 0.14115 	 m..s
   97 	    86 	 0.07887 	 0.14189 	 m..s
   46 	    87 	 0.06528 	 0.15099 	 m..s
   34 	    88 	 0.06446 	 0.15126 	 m..s
  106 	    89 	 0.10045 	 0.15316 	 m..s
  108 	    90 	 0.11047 	 0.15451 	 m..s
   46 	    91 	 0.06528 	 0.15452 	 m..s
   92 	    92 	 0.07668 	 0.15703 	 m..s
   94 	    93 	 0.07825 	 0.15925 	 m..s
   46 	    94 	 0.06528 	 0.16283 	 m..s
   73 	    95 	 0.06544 	 0.16364 	 m..s
   46 	    96 	 0.06528 	 0.16581 	 MISS
   84 	    97 	 0.06561 	 0.16715 	 MISS
   76 	    98 	 0.06545 	 0.16866 	 MISS
   74 	    99 	 0.06544 	 0.17136 	 MISS
   46 	   100 	 0.06528 	 0.17251 	 MISS
   85 	   101 	 0.06561 	 0.17398 	 MISS
  112 	   102 	 0.15935 	 0.17815 	 ~...
   88 	   103 	 0.06619 	 0.18166 	 MISS
  101 	   104 	 0.08293 	 0.18536 	 MISS
   43 	   105 	 0.06526 	 0.18628 	 MISS
   71 	   106 	 0.06541 	 0.18892 	 MISS
   89 	   107 	 0.06622 	 0.19204 	 MISS
   87 	   108 	 0.06618 	 0.19371 	 MISS
  115 	   109 	 0.23561 	 0.20501 	 m..s
  100 	   110 	 0.08142 	 0.20920 	 MISS
  102 	   111 	 0.08323 	 0.21720 	 MISS
  109 	   112 	 0.12777 	 0.23564 	 MISS
  111 	   113 	 0.13142 	 0.24065 	 MISS
  116 	   114 	 0.23810 	 0.25704 	 ~...
  114 	   115 	 0.22969 	 0.26695 	 m..s
  113 	   116 	 0.21590 	 0.27160 	 m..s
  120 	   117 	 0.28507 	 0.29184 	 ~...
  119 	   118 	 0.28455 	 0.29333 	 ~...
  117 	   119 	 0.27061 	 0.30234 	 m..s
  118 	   120 	 0.28060 	 0.30943 	 ~...
==========================================
r_mrr = 0.7231718897819519
r2_mrr = 0.5039200186729431
spearmanr_mrr@5 = 0.9449581503868103
spearmanr_mrr@10 = 0.9579758644104004
spearmanr_mrr@50 = 0.9175112247467041
spearmanr_mrr@100 = 0.7886689305305481
spearmanr_mrr@All = 0.8170920610427856
==========================================
test time: 0.405
Done Testing dataset DBpedia50
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.033 +- 0.004
mrr vals (pred, true): 0.231, 0.197

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.05040 	 0.04134 	 ~...
    9 	     1 	 0.04902 	 0.04151 	 ~...
   56 	     2 	 0.05266 	 0.04188 	 ~...
    6 	     3 	 0.04896 	 0.04197 	 ~...
   62 	     4 	 0.05346 	 0.04207 	 ~...
   23 	     5 	 0.05040 	 0.04216 	 ~...
   67 	     6 	 0.05427 	 0.04251 	 ~...
   57 	     7 	 0.05306 	 0.04257 	 ~...
   59 	     8 	 0.05318 	 0.04270 	 ~...
   23 	     9 	 0.05040 	 0.04286 	 ~...
   52 	    10 	 0.05178 	 0.04289 	 ~...
   55 	    11 	 0.05244 	 0.04301 	 ~...
    2 	    12 	 0.04871 	 0.04303 	 ~...
   23 	    13 	 0.05040 	 0.04304 	 ~...
   23 	    14 	 0.05040 	 0.04306 	 ~...
   23 	    15 	 0.05040 	 0.04326 	 ~...
   48 	    16 	 0.05136 	 0.04330 	 ~...
    8 	    17 	 0.04899 	 0.04333 	 ~...
   23 	    18 	 0.05040 	 0.04335 	 ~...
   18 	    19 	 0.05019 	 0.04342 	 ~...
   13 	    20 	 0.04918 	 0.04346 	 ~...
   11 	    21 	 0.04915 	 0.04363 	 ~...
   64 	    22 	 0.05367 	 0.04366 	 ~...
   75 	    23 	 0.06121 	 0.04369 	 ~...
   50 	    24 	 0.05141 	 0.04382 	 ~...
   23 	    25 	 0.05040 	 0.04384 	 ~...
   61 	    26 	 0.05340 	 0.04390 	 ~...
   66 	    27 	 0.05422 	 0.04392 	 ~...
   63 	    28 	 0.05359 	 0.04415 	 ~...
   23 	    29 	 0.05040 	 0.04415 	 ~...
   17 	    30 	 0.04937 	 0.04428 	 ~...
    0 	    31 	 0.04859 	 0.04433 	 ~...
   23 	    32 	 0.05040 	 0.04435 	 ~...
   16 	    33 	 0.04933 	 0.04451 	 ~...
   23 	    34 	 0.05040 	 0.04463 	 ~...
   14 	    35 	 0.04919 	 0.04463 	 ~...
   43 	    36 	 0.05051 	 0.04469 	 ~...
    7 	    37 	 0.04896 	 0.04474 	 ~...
   71 	    38 	 0.05531 	 0.04476 	 ~...
    4 	    39 	 0.04880 	 0.04486 	 ~...
   60 	    40 	 0.05328 	 0.04495 	 ~...
   23 	    41 	 0.05040 	 0.04498 	 ~...
   51 	    42 	 0.05176 	 0.04501 	 ~...
   19 	    43 	 0.05026 	 0.04503 	 ~...
   23 	    44 	 0.05040 	 0.04531 	 ~...
   22 	    45 	 0.05030 	 0.04566 	 ~...
   20 	    46 	 0.05026 	 0.04609 	 ~...
    5 	    47 	 0.04882 	 0.04625 	 ~...
    3 	    48 	 0.04876 	 0.04627 	 ~...
   45 	    49 	 0.05088 	 0.04644 	 ~...
   69 	    50 	 0.05480 	 0.04651 	 ~...
   70 	    51 	 0.05485 	 0.04653 	 ~...
   23 	    52 	 0.05040 	 0.04657 	 ~...
   21 	    53 	 0.05027 	 0.04659 	 ~...
   12 	    54 	 0.04917 	 0.04680 	 ~...
   10 	    55 	 0.04912 	 0.04708 	 ~...
   23 	    56 	 0.05040 	 0.04722 	 ~...
   23 	    57 	 0.05040 	 0.04726 	 ~...
   68 	    58 	 0.05478 	 0.04733 	 ~...
   65 	    59 	 0.05381 	 0.04762 	 ~...
   58 	    60 	 0.05306 	 0.04798 	 ~...
    1 	    61 	 0.04870 	 0.04802 	 ~...
   23 	    62 	 0.05040 	 0.04831 	 ~...
   15 	    63 	 0.04927 	 0.04910 	 ~...
   23 	    64 	 0.05040 	 0.04914 	 ~...
   42 	    65 	 0.05049 	 0.05171 	 ~...
   47 	    66 	 0.05128 	 0.05179 	 ~...
   74 	    67 	 0.05798 	 0.05237 	 ~...
   53 	    68 	 0.05202 	 0.05241 	 ~...
   54 	    69 	 0.05203 	 0.05269 	 ~...
   46 	    70 	 0.05119 	 0.05271 	 ~...
   23 	    71 	 0.05040 	 0.05375 	 ~...
   49 	    72 	 0.05137 	 0.05411 	 ~...
   77 	    73 	 0.06133 	 0.05416 	 ~...
   72 	    74 	 0.05552 	 0.05424 	 ~...
   44 	    75 	 0.05061 	 0.05610 	 ~...
   73 	    76 	 0.05553 	 0.05699 	 ~...
   76 	    77 	 0.06126 	 0.05723 	 ~...
   78 	    78 	 0.06152 	 0.06201 	 ~...
   81 	    79 	 0.17686 	 0.14461 	 m..s
   80 	    80 	 0.15106 	 0.15474 	 ~...
   79 	    81 	 0.14589 	 0.17397 	 ~...
   86 	    82 	 0.21606 	 0.17772 	 m..s
   89 	    83 	 0.22800 	 0.18210 	 m..s
   91 	    84 	 0.23107 	 0.18345 	 m..s
   92 	    85 	 0.23140 	 0.19716 	 m..s
   83 	    86 	 0.20963 	 0.20159 	 ~...
   87 	    87 	 0.21805 	 0.21130 	 ~...
   84 	    88 	 0.21126 	 0.21705 	 ~...
   85 	    89 	 0.21159 	 0.23396 	 ~...
   95 	    90 	 0.24131 	 0.23600 	 ~...
   96 	    91 	 0.24216 	 0.23675 	 ~...
   82 	    92 	 0.20337 	 0.24035 	 m..s
   88 	    93 	 0.21832 	 0.24077 	 ~...
   97 	    94 	 0.24251 	 0.24660 	 ~...
   90 	    95 	 0.22966 	 0.25856 	 ~...
  106 	    96 	 0.28513 	 0.26351 	 ~...
  100 	    97 	 0.25954 	 0.26491 	 ~...
   94 	    98 	 0.23937 	 0.27075 	 m..s
  109 	    99 	 0.29112 	 0.27130 	 ~...
  111 	   100 	 0.29881 	 0.27470 	 ~...
  103 	   101 	 0.27235 	 0.27505 	 ~...
  101 	   102 	 0.26633 	 0.27588 	 ~...
   93 	   103 	 0.23842 	 0.27723 	 m..s
  105 	   104 	 0.28305 	 0.27942 	 ~...
  112 	   105 	 0.31017 	 0.28047 	 ~...
  104 	   106 	 0.27827 	 0.28138 	 ~...
   98 	   107 	 0.24378 	 0.28336 	 m..s
   99 	   108 	 0.25126 	 0.28575 	 m..s
  110 	   109 	 0.29775 	 0.29011 	 ~...
  108 	   110 	 0.28930 	 0.29129 	 ~...
  116 	   111 	 0.32494 	 0.29789 	 ~...
  102 	   112 	 0.26666 	 0.29913 	 m..s
  107 	   113 	 0.28622 	 0.30820 	 ~...
  113 	   114 	 0.31362 	 0.31930 	 ~...
  118 	   115 	 0.34038 	 0.33103 	 ~...
  119 	   116 	 0.34693 	 0.33730 	 ~...
  114 	   117 	 0.31647 	 0.34118 	 ~...
  120 	   118 	 0.35137 	 0.34246 	 ~...
  115 	   119 	 0.31788 	 0.34654 	 ~...
  117 	   120 	 0.33358 	 0.35525 	 ~...
==========================================
r_mrr = 0.9905881881713867
r2_mrr = 0.9798607230186462
spearmanr_mrr@5 = 0.9221746325492859
spearmanr_mrr@10 = 0.9359492063522339
spearmanr_mrr@50 = 0.9923887252807617
spearmanr_mrr@100 = 0.9974803328514099
spearmanr_mrr@All = 0.9977515935897827
==========================================
test time: 0.425
Done Testing dataset Kinships
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.161 +- 0.282
mrr vals (pred, true): 0.083, 0.080

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   73 	     0 	 0.05083 	 0.00597 	 m..s
   41 	     1 	 0.04962 	 0.00600 	 m..s
   24 	     2 	 0.04577 	 0.00606 	 m..s
   41 	     3 	 0.04962 	 0.00611 	 m..s
   30 	     4 	 0.04903 	 0.00613 	 m..s
   33 	     5 	 0.04932 	 0.00616 	 m..s
   41 	     6 	 0.04962 	 0.00617 	 m..s
   76 	     7 	 0.05136 	 0.00617 	 m..s
   41 	     8 	 0.04962 	 0.00622 	 m..s
   69 	     9 	 0.05046 	 0.00625 	 m..s
   41 	    10 	 0.04962 	 0.00625 	 m..s
   62 	    11 	 0.05031 	 0.00630 	 m..s
   22 	    12 	 0.04533 	 0.00630 	 m..s
    6 	    13 	 0.04319 	 0.00631 	 m..s
    8 	    14 	 0.04339 	 0.00633 	 m..s
    0 	    15 	 0.03972 	 0.00633 	 m..s
   79 	    16 	 0.05192 	 0.00634 	 m..s
   84 	    17 	 0.05501 	 0.00634 	 m..s
    1 	    18 	 0.04091 	 0.00635 	 m..s
   35 	    19 	 0.04962 	 0.00636 	 m..s
   41 	    20 	 0.04962 	 0.00639 	 m..s
   68 	    21 	 0.05043 	 0.00641 	 m..s
   41 	    22 	 0.04962 	 0.00644 	 m..s
   70 	    23 	 0.05050 	 0.00646 	 m..s
    9 	    24 	 0.04357 	 0.00647 	 m..s
   11 	    25 	 0.04436 	 0.00648 	 m..s
   20 	    26 	 0.04504 	 0.00651 	 m..s
   75 	    27 	 0.05114 	 0.00655 	 m..s
   74 	    28 	 0.05091 	 0.00656 	 m..s
   17 	    29 	 0.04473 	 0.00656 	 m..s
   66 	    30 	 0.05035 	 0.00659 	 m..s
   41 	    31 	 0.04962 	 0.00659 	 m..s
   65 	    32 	 0.05032 	 0.00660 	 m..s
   14 	    33 	 0.04451 	 0.00668 	 m..s
   39 	    34 	 0.04962 	 0.00669 	 m..s
   34 	    35 	 0.04962 	 0.00671 	 m..s
   41 	    36 	 0.04962 	 0.00673 	 m..s
   10 	    37 	 0.04415 	 0.00878 	 m..s
    4 	    38 	 0.04207 	 0.00882 	 m..s
    3 	    39 	 0.04177 	 0.00961 	 m..s
    2 	    40 	 0.04117 	 0.00971 	 m..s
    5 	    41 	 0.04221 	 0.00988 	 m..s
   15 	    42 	 0.04458 	 0.01063 	 m..s
   41 	    43 	 0.04962 	 0.01464 	 m..s
   37 	    44 	 0.04962 	 0.01884 	 m..s
   36 	    45 	 0.04962 	 0.01965 	 ~...
   41 	    46 	 0.04962 	 0.02175 	 ~...
   41 	    47 	 0.04962 	 0.02499 	 ~...
   38 	    48 	 0.04962 	 0.02595 	 ~...
   40 	    49 	 0.04962 	 0.02713 	 ~...
   19 	    50 	 0.04500 	 0.02739 	 ~...
   23 	    51 	 0.04554 	 0.02997 	 ~...
   21 	    52 	 0.04512 	 0.03202 	 ~...
   12 	    53 	 0.04449 	 0.03290 	 ~...
   60 	    54 	 0.04969 	 0.03365 	 ~...
   13 	    55 	 0.04450 	 0.03491 	 ~...
   16 	    56 	 0.04463 	 0.03717 	 ~...
   18 	    57 	 0.04493 	 0.03734 	 ~...
   61 	    58 	 0.04970 	 0.03876 	 ~...
   29 	    59 	 0.04880 	 0.04500 	 ~...
   80 	    60 	 0.05297 	 0.05662 	 ~...
   98 	    61 	 0.08339 	 0.05695 	 ~...
   94 	    62 	 0.08082 	 0.06068 	 ~...
   25 	    63 	 0.04801 	 0.06091 	 ~...
   27 	    64 	 0.04823 	 0.06311 	 ~...
   26 	    65 	 0.04805 	 0.06428 	 ~...
    7 	    66 	 0.04321 	 0.06457 	 ~...
   90 	    67 	 0.06070 	 0.06467 	 ~...
   71 	    68 	 0.05051 	 0.06935 	 ~...
   31 	    69 	 0.04910 	 0.07586 	 ~...
   32 	    70 	 0.04931 	 0.07756 	 ~...
   97 	    71 	 0.08288 	 0.08035 	 ~...
   41 	    72 	 0.04962 	 0.08500 	 m..s
   95 	    73 	 0.08195 	 0.08538 	 ~...
   96 	    74 	 0.08267 	 0.08550 	 ~...
   99 	    75 	 0.08344 	 0.08620 	 ~...
   82 	    76 	 0.05357 	 0.08633 	 m..s
   92 	    77 	 0.07953 	 0.08662 	 ~...
   41 	    78 	 0.04962 	 0.08716 	 m..s
   78 	    79 	 0.05147 	 0.08825 	 m..s
   77 	    80 	 0.05146 	 0.08995 	 m..s
   28 	    81 	 0.04880 	 0.09260 	 m..s
   91 	    82 	 0.07422 	 0.09443 	 ~...
   63 	    83 	 0.05031 	 0.09456 	 m..s
   41 	    84 	 0.04962 	 0.09500 	 m..s
   41 	    85 	 0.04962 	 0.09542 	 m..s
   41 	    86 	 0.04962 	 0.09561 	 m..s
   67 	    87 	 0.05039 	 0.09648 	 m..s
   41 	    88 	 0.04962 	 0.09660 	 m..s
  104 	    89 	 0.10113 	 0.09751 	 ~...
   41 	    90 	 0.04962 	 0.09868 	 m..s
   64 	    91 	 0.05031 	 0.10022 	 m..s
   72 	    92 	 0.05051 	 0.10348 	 m..s
   81 	    93 	 0.05349 	 0.10491 	 m..s
   85 	    94 	 0.05502 	 0.10549 	 m..s
   89 	    95 	 0.05651 	 0.10837 	 m..s
   93 	    96 	 0.08067 	 0.11092 	 m..s
   88 	    97 	 0.05520 	 0.11182 	 m..s
   83 	    98 	 0.05470 	 0.11189 	 m..s
   87 	    99 	 0.05508 	 0.12089 	 m..s
   86 	   100 	 0.05504 	 0.12635 	 m..s
  103 	   101 	 0.09319 	 0.13667 	 m..s
  102 	   102 	 0.08915 	 0.14026 	 m..s
  101 	   103 	 0.08875 	 0.14115 	 m..s
  100 	   104 	 0.08639 	 0.14341 	 m..s
  105 	   105 	 0.10270 	 0.14556 	 m..s
  106 	   106 	 0.11144 	 0.15132 	 m..s
  107 	   107 	 0.11461 	 0.16659 	 m..s
  108 	   108 	 0.12284 	 0.16702 	 m..s
  110 	   109 	 0.14171 	 0.19077 	 m..s
  112 	   110 	 0.17044 	 0.20183 	 m..s
  111 	   111 	 0.14249 	 0.20702 	 m..s
  109 	   112 	 0.13850 	 0.21399 	 m..s
  115 	   113 	 0.22746 	 0.24105 	 ~...
  116 	   114 	 0.22935 	 0.25086 	 ~...
  114 	   115 	 0.22261 	 0.26716 	 m..s
  113 	   116 	 0.21342 	 0.27280 	 m..s
  119 	   117 	 0.26047 	 0.28843 	 ~...
  120 	   118 	 0.26125 	 0.29009 	 ~...
  117 	   119 	 0.25321 	 0.29075 	 m..s
  118 	   120 	 0.25735 	 0.29280 	 m..s
==========================================
r_mrr = 0.8841599225997925
r2_mrr = 0.738566517829895
spearmanr_mrr@5 = 0.9953203201293945
spearmanr_mrr@10 = 0.975814700126648
spearmanr_mrr@50 = 0.9915239214897156
spearmanr_mrr@100 = 0.9157561659812927
spearmanr_mrr@All = 0.9128798842430115
==========================================
test time: 0.655
Done Testing dataset OpenEA
total time taken: 1017.9816126823425
training time taken: 974.3924205303192
TWIG out ;))
