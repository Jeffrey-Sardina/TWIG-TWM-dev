===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 148
rank avg (pred): 0.467 +- 0.004
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.26624e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 404
rank avg (pred): 0.493 +- 0.012
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.000122714

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1198
rank avg (pred): 0.448 +- 0.256
mrr vals (pred, true): 0.143, 0.004
batch losses (mrrl, rdl): 0.0, 1.45271e-05

Epoch over!
epoch time: 15.08

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.467 +- 0.269
mrr vals (pred, true): 0.144, 0.004
batch losses (mrrl, rdl): 0.0, 1.63779e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 58
rank avg (pred): 0.078 +- 0.049
mrr vals (pred, true): 0.203, 0.137
batch losses (mrrl, rdl): 0.0, 9.5472e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 898
rank avg (pred): 0.382 +- 0.247
mrr vals (pred, true): 0.180, 0.002
batch losses (mrrl, rdl): 0.0, 0.0003664116

Epoch over!
epoch time: 14.852

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.448 +- 0.278
mrr vals (pred, true): 0.164, 0.004
batch losses (mrrl, rdl): 0.0, 8.5858e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1206
rank avg (pred): 0.468 +- 0.272
mrr vals (pred, true): 0.141, 0.004
batch losses (mrrl, rdl): 0.0, 1.41459e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 970
rank avg (pred): 0.468 +- 0.283
mrr vals (pred, true): 0.141, 0.004
batch losses (mrrl, rdl): 0.0, 2.40917e-05

Epoch over!
epoch time: 14.868

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 417
rank avg (pred): 0.457 +- 0.279
mrr vals (pred, true): 0.140, 0.004
batch losses (mrrl, rdl): 0.0, 1.0742e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 16
rank avg (pred): 0.088 +- 0.058
mrr vals (pred, true): 0.170, 0.234
batch losses (mrrl, rdl): 0.0, 3.80907e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 170
rank avg (pred): 0.443 +- 0.280
mrr vals (pred, true): 0.129, 0.004
batch losses (mrrl, rdl): 0.0, 7.878e-06

Epoch over!
epoch time: 14.866

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 450
rank avg (pred): 0.444 +- 0.273
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0, 8.8317e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.442 +- 0.280
mrr vals (pred, true): 0.120, 0.004
batch losses (mrrl, rdl): 0.0, 6.9894e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 797
rank avg (pred): 0.443 +- 0.280
mrr vals (pred, true): 0.123, 0.005
batch losses (mrrl, rdl): 0.0, 9.876e-06

Epoch over!
epoch time: 14.748

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1157
rank avg (pred): 0.394 +- 0.261
mrr vals (pred, true): 0.133, 0.026
batch losses (mrrl, rdl): 0.0691961795, 0.0005929652

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1130
rank avg (pred): 0.459 +- 0.171
mrr vals (pred, true): 0.060, 0.004
batch losses (mrrl, rdl): 0.0009871731, 4.47328e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 404
rank avg (pred): 0.471 +- 0.183
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 0.0001193222, 5.27628e-05

Epoch over!
epoch time: 15.016

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 484
rank avg (pred): 0.487 +- 0.171
mrr vals (pred, true): 0.032, 0.004
batch losses (mrrl, rdl): 0.0031778547, 6.09433e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.444 +- 0.207
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0008012669, 2.41588e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 875
rank avg (pred): 0.450 +- 0.210
mrr vals (pred, true): 0.065, 0.006
batch losses (mrrl, rdl): 0.0022311425, 2.5255e-05

Epoch over!
epoch time: 15.088

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 205
rank avg (pred): 0.458 +- 0.201
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 1.98082e-05, 3.0496e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1073
rank avg (pred): 0.043 +- 0.028
mrr vals (pred, true): 0.188, 0.195
batch losses (mrrl, rdl): 0.0005440472, 3.15151e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 811
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.272, 0.329
batch losses (mrrl, rdl): 0.0326348729, 1.93261e-05

Epoch over!
epoch time: 15.202

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 144
rank avg (pred): 0.466 +- 0.197
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0007129466, 3.47483e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 320
rank avg (pred): 0.015 +- 0.010
mrr vals (pred, true): 0.196, 0.185
batch losses (mrrl, rdl): 0.0012729993, 8.26739e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 866
rank avg (pred): 0.452 +- 0.229
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001496456, 1.90916e-05

Epoch over!
epoch time: 15.055

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 865
rank avg (pred): 0.465 +- 0.240
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002030094, 1.52322e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 284
rank avg (pred): 0.013 +- 0.008
mrr vals (pred, true): 0.198, 0.210
batch losses (mrrl, rdl): 0.0013498146, 8.70197e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1032
rank avg (pred): 0.455 +- 0.241
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001935688, 1.28207e-05

Epoch over!
epoch time: 14.996

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 806
rank avg (pred): 0.447 +- 0.235
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001413734, 1.85284e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.472 +- 0.204
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0004128657, 2.20172e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 268
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.335, 0.308
batch losses (mrrl, rdl): 0.007146284, 1.72487e-05

Epoch over!
epoch time: 15.025

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 178
rank avg (pred): 0.459 +- 0.215
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002884749, 2.40701e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 843
rank avg (pred): 0.445 +- 0.233
mrr vals (pred, true): 0.055, 0.003
batch losses (mrrl, rdl): 0.0002051507, 2.32165e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 660
rank avg (pred): 0.480 +- 0.199
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0008460777, 2.82514e-05

Epoch over!
epoch time: 14.984

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.328, 0.296
batch losses (mrrl, rdl): 0.0099693323, 1.6153e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.462 +- 0.227
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.65163e-05, 2.03719e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.462 +- 0.219
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 9.5127e-06, 1.94021e-05

Epoch over!
epoch time: 15.243

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 485
rank avg (pred): 0.464 +- 0.215
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.00019545, 2.47993e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.021 +- 0.014
mrr vals (pred, true): 0.192, 0.177
batch losses (mrrl, rdl): 0.0023485501, 6.76339e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1187
rank avg (pred): 0.465 +- 0.210
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001296959, 1.91281e-05

Epoch over!
epoch time: 15.147

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 186
rank avg (pred): 0.469 +- 0.209
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001359541, 2.88506e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 329
rank avg (pred): 0.460 +- 0.260
mrr vals (pred, true): 0.058, 0.003
batch losses (mrrl, rdl): 0.0006885962, 1.33087e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 769
rank avg (pred): 0.435 +- 0.214
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 4.2788e-06, 5.85084e-05

Epoch over!
epoch time: 14.994

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.038 +- 0.026
mrr vals (pred, true): 0.231, 0.316

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.04733 	 0.00101 	 m..s
   40 	     1 	 0.05414 	 0.00194 	 m..s
   22 	     2 	 0.05187 	 0.00212 	 m..s
   19 	     3 	 0.05148 	 0.00246 	 m..s
   63 	     4 	 0.05866 	 0.00250 	 m..s
   51 	     5 	 0.05560 	 0.00253 	 m..s
    4 	     6 	 0.04545 	 0.00263 	 m..s
    3 	     7 	 0.04507 	 0.00266 	 m..s
   71 	     8 	 0.06139 	 0.00282 	 m..s
   46 	     9 	 0.05459 	 0.00289 	 m..s
   32 	    10 	 0.05304 	 0.00296 	 m..s
   37 	    11 	 0.05353 	 0.00297 	 m..s
    9 	    12 	 0.04965 	 0.00297 	 m..s
    5 	    13 	 0.04642 	 0.00300 	 m..s
   30 	    14 	 0.05292 	 0.00318 	 m..s
   64 	    15 	 0.05923 	 0.00321 	 m..s
   26 	    16 	 0.05233 	 0.00331 	 m..s
   61 	    17 	 0.05751 	 0.00334 	 m..s
   14 	    18 	 0.05058 	 0.00334 	 m..s
   39 	    19 	 0.05393 	 0.00334 	 m..s
   70 	    20 	 0.06012 	 0.00336 	 m..s
    1 	    21 	 0.04416 	 0.00343 	 m..s
   55 	    22 	 0.05614 	 0.00344 	 m..s
   62 	    23 	 0.05852 	 0.00345 	 m..s
   68 	    24 	 0.05987 	 0.00347 	 m..s
   18 	    25 	 0.05127 	 0.00347 	 m..s
   24 	    26 	 0.05196 	 0.00350 	 m..s
   42 	    27 	 0.05425 	 0.00358 	 m..s
   41 	    28 	 0.05420 	 0.00361 	 m..s
   11 	    29 	 0.04995 	 0.00363 	 m..s
    6 	    30 	 0.04712 	 0.00366 	 m..s
   50 	    31 	 0.05550 	 0.00369 	 m..s
   34 	    32 	 0.05320 	 0.00371 	 m..s
   35 	    33 	 0.05338 	 0.00376 	 m..s
   58 	    34 	 0.05681 	 0.00377 	 m..s
    0 	    35 	 0.04261 	 0.00377 	 m..s
   56 	    36 	 0.05628 	 0.00380 	 m..s
   31 	    37 	 0.05301 	 0.00380 	 m..s
   20 	    38 	 0.05170 	 0.00381 	 m..s
   71 	    39 	 0.06139 	 0.00381 	 m..s
    2 	    40 	 0.04418 	 0.00384 	 m..s
   15 	    41 	 0.05107 	 0.00385 	 m..s
   49 	    42 	 0.05526 	 0.00387 	 m..s
   83 	    43 	 0.06165 	 0.00387 	 m..s
   43 	    44 	 0.05429 	 0.00387 	 m..s
   59 	    45 	 0.05688 	 0.00389 	 m..s
   65 	    46 	 0.05948 	 0.00391 	 m..s
   52 	    47 	 0.05589 	 0.00398 	 m..s
   21 	    48 	 0.05183 	 0.00399 	 m..s
   86 	    49 	 0.06229 	 0.00401 	 m..s
   16 	    50 	 0.05111 	 0.00404 	 m..s
   12 	    51 	 0.05013 	 0.00413 	 m..s
   45 	    52 	 0.05447 	 0.00414 	 m..s
   84 	    53 	 0.06201 	 0.00415 	 m..s
   53 	    54 	 0.05604 	 0.00415 	 m..s
   23 	    55 	 0.05192 	 0.00418 	 m..s
   17 	    56 	 0.05122 	 0.00428 	 m..s
    8 	    57 	 0.04824 	 0.00433 	 m..s
   36 	    58 	 0.05346 	 0.00434 	 m..s
   44 	    59 	 0.05444 	 0.00438 	 m..s
   27 	    60 	 0.05246 	 0.00439 	 m..s
   54 	    61 	 0.05607 	 0.00441 	 m..s
   28 	    62 	 0.05254 	 0.00442 	 m..s
   13 	    63 	 0.05050 	 0.00445 	 m..s
   67 	    64 	 0.05973 	 0.00448 	 m..s
   10 	    65 	 0.04988 	 0.00450 	 m..s
   69 	    66 	 0.05990 	 0.00453 	 m..s
   25 	    67 	 0.05202 	 0.00455 	 m..s
   91 	    68 	 0.06753 	 0.00458 	 m..s
   66 	    69 	 0.05960 	 0.00464 	 m..s
   33 	    70 	 0.05310 	 0.00466 	 m..s
   57 	    71 	 0.05666 	 0.00469 	 m..s
   90 	    72 	 0.06538 	 0.00472 	 m..s
   92 	    73 	 0.07189 	 0.00484 	 m..s
   48 	    74 	 0.05504 	 0.00486 	 m..s
   47 	    75 	 0.05465 	 0.00493 	 m..s
   60 	    76 	 0.05738 	 0.00515 	 m..s
   82 	    77 	 0.06147 	 0.00533 	 m..s
   38 	    78 	 0.05365 	 0.00545 	 m..s
   29 	    79 	 0.05263 	 0.00579 	 m..s
   71 	    80 	 0.06139 	 0.00601 	 m..s
   71 	    81 	 0.06139 	 0.00656 	 m..s
   71 	    82 	 0.06139 	 0.00729 	 m..s
   85 	    83 	 0.06207 	 0.00850 	 m..s
   71 	    84 	 0.06139 	 0.00858 	 m..s
   71 	    85 	 0.06139 	 0.01178 	 m..s
   71 	    86 	 0.06139 	 0.01206 	 m..s
   93 	    87 	 0.08847 	 0.01781 	 m..s
   71 	    88 	 0.06139 	 0.01930 	 m..s
   94 	    89 	 0.08911 	 0.01957 	 m..s
   71 	    90 	 0.06139 	 0.02050 	 m..s
   95 	    91 	 0.08933 	 0.02182 	 m..s
   71 	    92 	 0.06139 	 0.02455 	 m..s
   89 	    93 	 0.06329 	 0.02920 	 m..s
   88 	    94 	 0.06326 	 0.03130 	 m..s
   87 	    95 	 0.06262 	 0.04296 	 ~...
  110 	    96 	 0.25445 	 0.05291 	 MISS
   96 	    97 	 0.13955 	 0.13821 	 ~...
   99 	    98 	 0.17073 	 0.13846 	 m..s
   98 	    99 	 0.14197 	 0.14041 	 ~...
  104 	   100 	 0.20441 	 0.15098 	 m..s
   97 	   101 	 0.13999 	 0.15237 	 ~...
  103 	   102 	 0.20241 	 0.15473 	 m..s
  107 	   103 	 0.20957 	 0.17117 	 m..s
  102 	   104 	 0.20203 	 0.17383 	 ~...
  106 	   105 	 0.20938 	 0.19630 	 ~...
  100 	   106 	 0.19791 	 0.19794 	 ~...
  101 	   107 	 0.19911 	 0.19906 	 ~...
  117 	   108 	 0.26407 	 0.21732 	 m..s
  115 	   109 	 0.26106 	 0.23383 	 ~...
  112 	   110 	 0.25953 	 0.23552 	 ~...
  109 	   111 	 0.24558 	 0.23886 	 ~...
  119 	   112 	 0.30567 	 0.24879 	 m..s
  105 	   113 	 0.20737 	 0.26233 	 m..s
  111 	   114 	 0.25920 	 0.26513 	 ~...
  114 	   115 	 0.26059 	 0.27244 	 ~...
  118 	   116 	 0.26579 	 0.29612 	 m..s
  120 	   117 	 0.34445 	 0.29973 	 m..s
  113 	   118 	 0.26050 	 0.30213 	 m..s
  116 	   119 	 0.26217 	 0.31468 	 m..s
  108 	   120 	 0.23051 	 0.31567 	 m..s
==========================================
r_mrr = 0.9570755958557129
r2_mrr = 0.6707533597946167
spearmanr_mrr@5 = 0.9204695820808411
spearmanr_mrr@10 = 0.6587622761726379
spearmanr_mrr@50 = 0.9874201416969299
spearmanr_mrr@100 = 0.9912111163139343
spearmanr_mrr@All = 0.9910480380058289
==========================================
test time: 0.456
Done Testing dataset CoDExSmall
total time taken: 236.5879611968994
training time taken: 225.6783103942871
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9571)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.6708)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9205)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.6588)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9874)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9912)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9910)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.8644244678307587}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 508425322870164
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 1122, 973, 460, 838, 863, 909, 21, 504, 464, 89, 865, 842, 266, 1184, 609, 899, 1110, 285, 455, 921, 748, 52, 550, 604, 328, 224, 633, 193, 763, 299, 990, 969, 1019, 1138, 272, 1171, 110, 497, 1146, 42, 696, 1046, 46, 645, 56, 300, 941, 77, 76, 954, 124, 6, 232, 32, 848, 326, 1153, 823, 364, 815, 225, 600, 576, 182, 914, 1173, 797, 1035, 854, 1113, 729, 454, 11, 427, 394, 896, 1126, 939, 140, 857, 416, 866, 214, 817, 279, 1069, 593, 543, 937, 793, 762, 1038, 432, 1060, 888, 711, 362, 818, 956, 1, 469, 142, 625, 377, 578, 506, 329, 839, 895, 929, 556, 101, 1005, 1196, 352, 260, 970, 1116, 322, 55]
valid_ids (0): []
train_ids (1094): [388, 230, 437, 675, 201, 383, 579, 862, 400, 280, 1054, 35, 10, 978, 1157, 652, 1158, 655, 268, 1088, 286, 1037, 367, 639, 318, 674, 515, 241, 1057, 591, 778, 940, 636, 1130, 713, 136, 715, 475, 619, 67, 430, 531, 8, 209, 632, 130, 786, 15, 755, 692, 319, 252, 664, 611, 424, 47, 943, 583, 827, 462, 1174, 294, 491, 1109, 363, 1197, 841, 1103, 790, 338, 1034, 78, 907, 1125, 386, 648, 912, 988, 852, 289, 29, 812, 45, 40, 856, 379, 723, 757, 777, 119, 301, 754, 568, 679, 700, 274, 1190, 993, 1089, 672, 980, 1042, 606, 544, 629, 349, 710, 314, 706, 760, 867, 221, 1136, 66, 61, 396, 1043, 667, 530, 484, 994, 1133, 596, 638, 135, 282, 313, 802, 642, 792, 307, 139, 1049, 137, 1008, 803, 335, 305, 1094, 433, 850, 1092, 602, 936, 378, 178, 492, 255, 1102, 534, 389, 87, 80, 60, 1191, 1093, 1023, 324, 779, 933, 100, 890, 112, 1058, 86, 871, 858, 958, 787, 191, 785, 949, 1017, 740, 668, 566, 85, 1121, 177, 357, 218, 238, 269, 479, 93, 687, 630, 561, 676, 744, 143, 846, 1118, 1135, 722, 292, 864, 188, 97, 892, 1072, 885, 25, 567, 102, 799, 820, 613, 637, 935, 1032, 254, 64, 964, 179, 747, 104, 195, 938, 242, 308, 1131, 944, 624, 985, 31, 81, 62, 804, 49, 735, 989, 107, 1096, 623, 580, 123, 371, 98, 798, 53, 183, 155, 312, 925, 1020, 945, 159, 283, 204, 75, 582, 1140, 766, 16, 950, 444, 1183, 196, 947, 855, 502, 1181, 805, 1214, 540, 281, 1167, 880, 331, 1067, 213, 1175, 273, 733, 824, 96, 3, 800, 791, 458, 1178, 343, 987, 374, 553, 983, 592, 247, 789, 971, 658, 1134, 441, 836, 1112, 1064, 886, 277, 918, 1154, 756, 222, 457, 346, 927, 720, 51, 170, 708, 320, 494, 468, 446, 495, 256, 486, 205, 752, 295, 1099, 878, 659, 702, 961, 761, 897, 948, 923, 1015, 117, 459, 765, 995, 354, 511, 1114, 194, 879, 290, 536, 407, 519, 1198, 192, 0, 125, 1106, 82, 37, 1039, 784, 471, 1193, 1009, 1036, 197, 1212, 529, 133, 1048, 439, 569, 651, 169, 877, 443, 122, 1194, 448, 239, 1172, 926, 38, 922, 1045, 816, 180, 1151, 1074, 1016, 649, 1024, 563, 910, 185, 425, 1101, 50, 677, 783, 128, 772, 418, 919, 1162, 693, 889, 1195, 808, 234, 873, 617, 120, 605, 369, 1176, 208, 1021, 429, 111, 271, 1168, 54, 361, 391, 891, 574, 333, 2, 828, 210, 1003, 344, 1029, 717, 549, 105, 245, 175, 533, 1161, 1170, 421, 240, 1090, 203, 265, 1111, 28, 481, 216, 1031, 138, 882, 874, 43, 1203, 1200, 17, 951, 588, 287, 339, 1041, 202, 235, 554, 1079, 164, 1145, 1210, 671, 466, 653, 745, 814, 1075, 310, 643, 1115, 1071, 911, 1185, 36, 199, 276, 641, 884, 870, 1001, 1107, 275, 351, 48, 616, 689, 33, 631, 325, 734, 622, 463, 115, 152, 306, 1091, 607, 837, 1123, 678, 782, 695, 423, 894, 682, 72, 90, 413, 1025, 825, 581, 560, 957, 1026, 844, 603, 1018, 399, 144, 974, 417, 1211, 171, 380, 258, 392, 725, 296, 703, 132, 24, 20, 627, 924, 1143, 573, 709, 381, 541, 806, 612, 408, 522, 771, 411, 490, 598, 634, 79, 893, 472, 1076, 236, 930, 1055, 1155, 160, 39, 226, 750, 847, 705, 1084, 434, 215, 341, 151, 595, 207, 1204, 116, 71, 304, 385, 233, 996, 704, 154, 906, 517, 438, 1177, 661, 356, 44, 728, 801, 770, 336, 1053, 575, 251, 382, 1105, 330, 1108, 982, 565, 913, 499, 449, 965, 521, 410, 1040, 902, 683, 1068, 545, 719, 572, 716, 810, 347, 1059, 1147, 172, 1010, 998, 262, 74, 966, 30, 1186, 309, 348, 167, 1202, 58, 376, 1061, 881, 165, 1165, 1002, 190, 920, 610, 718, 426, 398, 350, 153, 714, 302, 253, 900, 738, 1169, 422, 366, 131, 397, 781, 1033, 483, 477, 916, 688, 932, 586, 451, 764, 1201, 181, 860, 654, 480, 570, 503, 470, 849, 908, 1156, 833, 872, 528, 141, 1166, 963, 1137, 650, 539, 150, 547, 1150, 405, 1132, 999, 1187, 915, 1189, 514, 493, 1100, 984, 876, 246, 1117, 327, 70, 795, 577, 1004, 200, 7, 1087, 168, 261, 1022, 509, 1163, 1073, 690, 507, 134, 946, 12, 428, 524, 1097, 84, 206, 1207, 826, 903, 635, 473, 186, 518, 928, 173, 489, 1065, 13, 1160, 597, 628, 1027, 843, 157, 217, 1119, 419, 680, 291, 359, 584, 146, 587, 420, 952, 219, 9, 440, 527, 263, 1047, 1179, 1028, 303, 409, 1006, 450, 546, 829, 402, 721, 739, 775, 1014, 780, 1013, 231, 108, 1164, 293, 774, 482, 751, 1124, 220, 997, 284, 1213, 726, 942, 736, 384, 851, 737, 288, 360, 496, 753, 727, 807, 663, 788, 614, 227, 1192, 129, 883, 1141, 859, 387, 340, 749, 564, 662, 979, 834, 986, 353, 732, 1208, 478, 505, 187, 542, 311, 510, 68, 250, 59, 229, 730, 620, 212, 594, 315, 666, 644, 435, 237, 759, 372, 665, 548, 373, 794, 538, 65, 887, 41, 685, 647, 1030, 796, 1083, 121, 724, 488, 1188, 223, 114, 768, 370, 968, 1159, 508, 1152, 467, 975, 345, 163, 532, 853, 1062, 249, 758, 525, 162, 189, 684, 537, 321, 278, 904, 447, 332, 1086, 453, 355, 1056, 57, 557, 149, 337, 63, 585, 699, 516, 198, 211, 821, 742, 1104, 822, 590, 94, 1149, 166, 599, 819, 656, 618, 415, 741, 694, 589, 615, 1205, 646, 452, 868, 145, 1120, 512, 243, 1066, 669, 840, 19, 1199, 1052, 126, 1011, 316, 811, 869, 861, 358, 158, 555, 4, 334, 1078, 601, 395, 967, 776, 27, 686, 393, 640, 767, 461, 88, 436, 526, 501, 270, 500, 1128, 69, 513, 1098, 977, 1209, 562, 298, 1129, 1095, 401, 1070, 498, 1182, 342, 1080, 962, 73, 773, 670, 1012, 1139, 832, 485, 26, 976, 571, 259, 1082, 14, 551, 520, 113, 559, 106, 960, 184, 552, 845, 118, 442, 34, 691, 487, 905, 412, 174, 109, 1144, 813, 608, 707, 712, 22, 898, 465, 390, 673, 558, 746, 1081, 375, 323, 92, 267, 953, 23, 414, 228, 95, 5, 406, 698, 981, 476, 147, 403, 626, 1180, 835, 1148, 992, 18, 264, 1077, 445, 1127, 1007, 148, 523, 959, 769, 955, 1044, 809, 535, 176, 1206, 934, 456, 697, 127, 660, 156, 1000, 431, 297, 1050, 991, 875, 99, 731, 621, 83, 161, 244, 404, 317, 103, 681, 701, 368, 474, 931, 91, 365, 917, 830, 1063, 972, 901, 743, 1051, 1085, 257, 1142, 657, 248]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4810767613836557
the save name prefix for this run is:  chkpt-ID_4810767613836557_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.543 +- 0.002
mrr vals (pred, true): 0.001, 0.012
batch losses (mrrl, rdl): 0.0, 0.0007018688

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1106
rank avg (pred): 0.446 +- 0.290
mrr vals (pred, true): 0.100, 0.004
batch losses (mrrl, rdl): 0.0, 5.7911e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 720
rank avg (pred): 0.481 +- 0.276
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0, 2.84118e-05

Epoch over!
epoch time: 15.586

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 837
rank avg (pred): 0.458 +- 0.290
mrr vals (pred, true): 0.079, 0.005
batch losses (mrrl, rdl): 0.0, 1.071e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.466 +- 0.275
mrr vals (pred, true): 0.040, 0.005
batch losses (mrrl, rdl): 0.0, 6.0511e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.431 +- 0.284
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0, 8.3645e-06

Epoch over!
epoch time: 14.802

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.196 +- 0.188
mrr vals (pred, true): 0.097, 0.266
batch losses (mrrl, rdl): 0.0, 0.0004697367

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 305
rank avg (pred): 0.161 +- 0.154
mrr vals (pred, true): 0.087, 0.191
batch losses (mrrl, rdl): 0.0, 0.0001882336

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 631
rank avg (pred): 0.400 +- 0.291
mrr vals (pred, true): 0.081, 0.003
batch losses (mrrl, rdl): 0.0, 8.7627e-05

Epoch over!
epoch time: 14.922

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 125
rank avg (pred): 0.444 +- 0.281
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.0, 6.6633e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.456 +- 0.285
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 0.0, 6.6303e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 524
rank avg (pred): 0.161 +- 0.184
mrr vals (pred, true): 0.229, 0.009
batch losses (mrrl, rdl): 0.0, 0.0011944317

Epoch over!
epoch time: 14.804

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 891
rank avg (pred): 0.160 +- 0.191
mrr vals (pred, true): 0.286, 0.002
batch losses (mrrl, rdl): 0.0, 0.0019305273

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.460 +- 0.266
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 0.0, 1.15233e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.491 +- 0.272
mrr vals (pred, true): 0.038, 0.003
batch losses (mrrl, rdl): 0.0, 1.1566e-05

Epoch over!
epoch time: 14.758

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.165 +- 0.202
mrr vals (pred, true): 0.306, 0.184
batch losses (mrrl, rdl): 0.1487002969, 0.0002456307

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 889
rank avg (pred): 0.464 +- 0.165
mrr vals (pred, true): 0.067, 0.005
batch losses (mrrl, rdl): 0.0029567359, 4.61943e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 928
rank avg (pred): 0.484 +- 0.160
mrr vals (pred, true): 0.048, 0.003
batch losses (mrrl, rdl): 2.52936e-05, 3.93676e-05

Epoch over!
epoch time: 15.017

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.361 +- 0.202
mrr vals (pred, true): 0.142, 0.117
batch losses (mrrl, rdl): 0.0062368969, 0.0017413925

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.472 +- 0.151
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0005168925, 5.30998e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.366 +- 0.203
mrr vals (pred, true): 0.151, 0.012
batch losses (mrrl, rdl): 0.1017282084, 3.0981e-05

Epoch over!
epoch time: 14.925

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.485 +- 0.148
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0007885951, 5.17306e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 856
rank avg (pred): 0.472 +- 0.148
mrr vals (pred, true): 0.043, 0.003
batch losses (mrrl, rdl): 0.0005616292, 5.45152e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.484 +- 0.153
mrr vals (pred, true): 0.042, 0.003
batch losses (mrrl, rdl): 0.0006321548, 5.13983e-05

Epoch over!
epoch time: 15.076

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.343 +- 0.185
mrr vals (pred, true): 0.140, 0.012
batch losses (mrrl, rdl): 0.0812189206, 2.47665e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.069 +- 0.041
mrr vals (pred, true): 0.190, 0.002
batch losses (mrrl, rdl): 0.1953431219, 0.0050100712

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.474 +- 0.156
mrr vals (pred, true): 0.042, 0.005
batch losses (mrrl, rdl): 0.0005652779, 5.19777e-05

Epoch over!
epoch time: 15.028

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 647
rank avg (pred): 0.452 +- 0.166
mrr vals (pred, true): 0.069, 0.003
batch losses (mrrl, rdl): 0.003675808, 5.97395e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 97
rank avg (pred): 0.470 +- 0.161
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0005148879, 4.47087e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.460 +- 0.147
mrr vals (pred, true): 0.047, 0.003
batch losses (mrrl, rdl): 8.44896e-05, 6.29445e-05

Epoch over!
epoch time: 14.993

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.469 +- 0.166
mrr vals (pred, true): 0.069, 0.005
batch losses (mrrl, rdl): 0.0035343221, 5.66984e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 401
rank avg (pred): 0.474 +- 0.140
mrr vals (pred, true): 0.045, 0.005
batch losses (mrrl, rdl): 0.0002768138, 7.45813e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.503 +- 0.167
mrr vals (pred, true): 0.059, 0.003
batch losses (mrrl, rdl): 0.000739859, 5.86761e-05

Epoch over!
epoch time: 15.283

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.243 +- 0.130
mrr vals (pred, true): 0.159, 0.254
batch losses (mrrl, rdl): 0.0907417089, 0.0009081463

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1097
rank avg (pred): 0.484 +- 0.159
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.0005904053, 7.38032e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 475
rank avg (pred): 0.477 +- 0.124
mrr vals (pred, true): 0.038, 0.004
batch losses (mrrl, rdl): 0.0015127854, 6.69266e-05

Epoch over!
epoch time: 15.117

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.338 +- 0.180
mrr vals (pred, true): 0.157, 0.117
batch losses (mrrl, rdl): 0.0157728884, 0.001468546

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.481 +- 0.138
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004287277, 5.06843e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.474 +- 0.151
mrr vals (pred, true): 0.056, 0.003
batch losses (mrrl, rdl): 0.000328879, 5.16276e-05

Epoch over!
epoch time: 16.045

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.456 +- 0.147
mrr vals (pred, true): 0.060, 0.004
batch losses (mrrl, rdl): 0.00097887, 6.17859e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.190 +- 0.099
mrr vals (pred, true): 0.151, 0.017
batch losses (mrrl, rdl): 0.1028466225, 0.0002123316

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1177
rank avg (pred): 0.481 +- 0.148
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 3.27955e-05, 5.39174e-05

Epoch over!
epoch time: 16.066

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1001
rank avg (pred): 0.458 +- 0.126
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002207905, 6.52229e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 639
rank avg (pred): 0.477 +- 0.119
mrr vals (pred, true): 0.037, 0.004
batch losses (mrrl, rdl): 0.0016498901, 5.87675e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.482 +- 0.150
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002421364, 6.37457e-05

Epoch over!
epoch time: 15.241

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.298 +- 0.164
mrr vals (pred, true): 0.179, 0.250

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   76 	     0 	 0.17918 	 0.00088 	 MISS
   76 	     1 	 0.17918 	 0.00122 	 MISS
  104 	     2 	 0.20260 	 0.00128 	 MISS
   99 	     3 	 0.18438 	 0.00145 	 MISS
  106 	     4 	 0.20365 	 0.00155 	 MISS
   65 	     5 	 0.05904 	 0.00170 	 m..s
    8 	     6 	 0.05437 	 0.00192 	 m..s
   75 	     7 	 0.06174 	 0.00194 	 m..s
   56 	     8 	 0.05840 	 0.00204 	 m..s
   69 	     9 	 0.05995 	 0.00225 	 m..s
   64 	    10 	 0.05894 	 0.00250 	 m..s
   55 	    11 	 0.05829 	 0.00262 	 m..s
   11 	    12 	 0.05459 	 0.00267 	 m..s
   12 	    13 	 0.05470 	 0.00272 	 m..s
   61 	    14 	 0.05874 	 0.00287 	 m..s
   66 	    15 	 0.05908 	 0.00290 	 m..s
   33 	    16 	 0.05666 	 0.00300 	 m..s
   47 	    17 	 0.05795 	 0.00311 	 m..s
   22 	    18 	 0.05550 	 0.00314 	 m..s
   39 	    19 	 0.05717 	 0.00316 	 m..s
   21 	    20 	 0.05539 	 0.00321 	 m..s
   30 	    21 	 0.05641 	 0.00321 	 m..s
   43 	    22 	 0.05756 	 0.00323 	 m..s
   60 	    23 	 0.05872 	 0.00333 	 m..s
   71 	    24 	 0.06032 	 0.00335 	 m..s
   71 	    25 	 0.06032 	 0.00336 	 m..s
   45 	    26 	 0.05776 	 0.00338 	 m..s
   63 	    27 	 0.05889 	 0.00342 	 m..s
   68 	    28 	 0.05988 	 0.00348 	 m..s
   23 	    29 	 0.05578 	 0.00350 	 m..s
   28 	    30 	 0.05629 	 0.00351 	 m..s
   38 	    31 	 0.05715 	 0.00352 	 m..s
   70 	    32 	 0.06005 	 0.00361 	 m..s
   57 	    33 	 0.05842 	 0.00361 	 m..s
   14 	    34 	 0.05482 	 0.00362 	 m..s
    4 	    35 	 0.05407 	 0.00376 	 m..s
    0 	    36 	 0.05306 	 0.00381 	 m..s
   40 	    37 	 0.05725 	 0.00383 	 m..s
   46 	    38 	 0.05788 	 0.00387 	 m..s
   59 	    39 	 0.05871 	 0.00389 	 m..s
   24 	    40 	 0.05590 	 0.00396 	 m..s
   62 	    41 	 0.05887 	 0.00400 	 m..s
   53 	    42 	 0.05815 	 0.00403 	 m..s
   36 	    43 	 0.05681 	 0.00404 	 m..s
   41 	    44 	 0.05731 	 0.00407 	 m..s
    7 	    45 	 0.05431 	 0.00410 	 m..s
   24 	    46 	 0.05590 	 0.00410 	 m..s
   20 	    47 	 0.05524 	 0.00411 	 m..s
   49 	    48 	 0.05796 	 0.00412 	 m..s
    1 	    49 	 0.05315 	 0.00414 	 m..s
   29 	    50 	 0.05637 	 0.00416 	 m..s
   31 	    51 	 0.05642 	 0.00417 	 m..s
   44 	    52 	 0.05769 	 0.00419 	 m..s
   13 	    53 	 0.05477 	 0.00428 	 m..s
   52 	    54 	 0.05811 	 0.00428 	 m..s
   73 	    55 	 0.06041 	 0.00430 	 m..s
    3 	    56 	 0.05391 	 0.00431 	 m..s
   35 	    57 	 0.05680 	 0.00437 	 m..s
   27 	    58 	 0.05628 	 0.00438 	 m..s
   26 	    59 	 0.05624 	 0.00441 	 m..s
   47 	    60 	 0.05795 	 0.00442 	 m..s
    5 	    61 	 0.05411 	 0.00444 	 m..s
   42 	    62 	 0.05736 	 0.00451 	 m..s
   74 	    63 	 0.06053 	 0.00452 	 m..s
   16 	    64 	 0.05506 	 0.00453 	 m..s
   32 	    65 	 0.05655 	 0.00455 	 m..s
   19 	    66 	 0.05523 	 0.00456 	 m..s
   15 	    67 	 0.05501 	 0.00457 	 m..s
   17 	    68 	 0.05517 	 0.00458 	 m..s
   67 	    69 	 0.05984 	 0.00461 	 m..s
   17 	    70 	 0.05517 	 0.00463 	 m..s
   51 	    71 	 0.05809 	 0.00485 	 m..s
   54 	    72 	 0.05821 	 0.00489 	 m..s
    9 	    73 	 0.05452 	 0.00490 	 m..s
    6 	    74 	 0.05424 	 0.00490 	 m..s
    2 	    75 	 0.05375 	 0.00492 	 m..s
   50 	    76 	 0.05806 	 0.00498 	 m..s
   10 	    77 	 0.05453 	 0.00501 	 m..s
   37 	    78 	 0.05708 	 0.00534 	 m..s
   34 	    79 	 0.05679 	 0.00558 	 m..s
   58 	    80 	 0.05870 	 0.00577 	 m..s
   76 	    81 	 0.17918 	 0.00740 	 MISS
   76 	    82 	 0.17918 	 0.00858 	 MISS
   76 	    83 	 0.17918 	 0.01402 	 MISS
  118 	    84 	 0.25979 	 0.01818 	 MISS
  116 	    85 	 0.25485 	 0.01940 	 MISS
  101 	    86 	 0.19372 	 0.02091 	 MISS
  103 	    87 	 0.19651 	 0.02854 	 MISS
  110 	    88 	 0.22507 	 0.02942 	 MISS
   94 	    89 	 0.18267 	 0.03535 	 MISS
   99 	    90 	 0.18438 	 0.03563 	 MISS
  112 	    91 	 0.22853 	 0.04671 	 MISS
   76 	    92 	 0.17918 	 0.12561 	 m..s
   93 	    93 	 0.18156 	 0.12715 	 m..s
   95 	    94 	 0.18293 	 0.12902 	 m..s
   76 	    95 	 0.17918 	 0.13437 	 m..s
   76 	    96 	 0.17918 	 0.13525 	 m..s
   76 	    97 	 0.17918 	 0.13734 	 m..s
   76 	    98 	 0.17918 	 0.14054 	 m..s
   97 	    99 	 0.18307 	 0.14848 	 m..s
  102 	   100 	 0.19412 	 0.16128 	 m..s
   76 	   101 	 0.17918 	 0.17085 	 ~...
   76 	   102 	 0.17918 	 0.17792 	 ~...
   76 	   103 	 0.17918 	 0.17992 	 ~...
   95 	   104 	 0.18293 	 0.18103 	 ~...
   76 	   105 	 0.17918 	 0.18578 	 ~...
   76 	   106 	 0.17918 	 0.18907 	 ~...
   98 	   107 	 0.18330 	 0.20543 	 ~...
  109 	   108 	 0.22026 	 0.21151 	 ~...
  113 	   109 	 0.22918 	 0.22805 	 ~...
  114 	   110 	 0.24009 	 0.22857 	 ~...
  111 	   111 	 0.22784 	 0.24456 	 ~...
   76 	   112 	 0.17918 	 0.25004 	 m..s
  118 	   113 	 0.25979 	 0.26363 	 ~...
  107 	   114 	 0.20430 	 0.26513 	 m..s
   76 	   115 	 0.17918 	 0.26886 	 m..s
  108 	   116 	 0.21165 	 0.28931 	 m..s
  104 	   117 	 0.20260 	 0.29618 	 m..s
  120 	   118 	 0.26173 	 0.31362 	 m..s
  117 	   119 	 0.25496 	 0.34039 	 m..s
  115 	   120 	 0.25370 	 0.35668 	 MISS
==========================================
r_mrr = 0.7468575835227966
r2_mrr = 0.20220279693603516
spearmanr_mrr@5 = 0.9212474822998047
spearmanr_mrr@10 = 0.8470889925956726
spearmanr_mrr@50 = 0.7620431780815125
spearmanr_mrr@100 = 0.8340877890586853
spearmanr_mrr@All = 0.8449373245239258
==========================================
test time: 0.449
Done Testing dataset CoDExSmall
total time taken: 238.8532795906067
training time taken: 228.12469959259033
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7469)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.2022)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9212)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.8471)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.7620)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.8341)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.8449)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 4.551699120725971}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 2733616111716865
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [903, 328, 1167, 1212, 540, 1022, 856, 1148, 911, 593, 904, 377, 984, 228, 305, 281, 423, 436, 545, 162, 409, 268, 1024, 958, 349, 836, 577, 65, 772, 1197, 400, 1161, 70, 846, 831, 1187, 462, 1067, 266, 499, 914, 1119, 805, 1004, 670, 1068, 589, 101, 672, 467, 827, 79, 700, 699, 418, 659, 693, 171, 1056, 717, 394, 466, 1175, 117, 718, 665, 148, 1147, 351, 439, 1177, 966, 433, 724, 707, 196, 876, 848, 775, 762, 896, 592, 1214, 123, 729, 661, 306, 238, 89, 734, 746, 668, 711, 226, 515, 310, 100, 785, 1211, 821, 907, 562, 802, 921, 617, 404, 1111, 141, 820, 71, 233, 736, 833, 694, 360, 789, 814, 615, 382, 142, 760]
valid_ids (0): []
train_ids (1094): [894, 739, 526, 732, 764, 556, 154, 1210, 580, 634, 538, 504, 383, 235, 29, 946, 458, 867, 631, 1162, 863, 222, 1114, 888, 362, 1174, 189, 850, 774, 223, 509, 293, 1133, 2, 949, 627, 242, 740, 1059, 520, 583, 187, 103, 939, 152, 883, 697, 1087, 131, 837, 1052, 1120, 1095, 389, 77, 422, 1163, 183, 744, 195, 243, 737, 380, 301, 962, 988, 1014, 150, 960, 190, 209, 352, 551, 490, 109, 120, 172, 408, 289, 261, 182, 929, 318, 658, 572, 1008, 258, 374, 844, 241, 447, 630, 1016, 376, 1077, 229, 806, 657, 719, 927, 208, 449, 86, 517, 157, 344, 868, 399, 345, 1091, 1100, 249, 1176, 60, 675, 1043, 444, 877, 265, 855, 508, 749, 484, 1060, 88, 95, 923, 287, 59, 563, 935, 616, 1019, 225, 678, 1080, 521, 933, 144, 406, 568, 1153, 192, 854, 692, 781, 481, 1013, 165, 275, 290, 129, 721, 731, 518, 198, 1191, 1116, 1122, 548, 555, 413, 31, 512, 75, 626, 849, 606, 424, 82, 671, 611, 158, 941, 928, 695, 69, 917, 1118, 149, 940, 603, 1128, 432, 1126, 346, 943, 535, 981, 197, 461, 838, 994, 255, 800, 498, 1127, 354, 456, 916, 956, 1033, 708, 893, 1018, 1020, 1149, 401, 500, 507, 1173, 777, 277, 194, 751, 472, 353, 78, 703, 825, 14, 459, 1090, 973, 15, 899, 1089, 359, 111, 106, 664, 1182, 476, 696, 793, 451, 487, 858, 188, 704, 370, 163, 272, 743, 332, 1107, 81, 145, 473, 912, 479, 337, 440, 236, 677, 442, 944, 385, 340, 167, 19, 263, 922, 1085, 1048, 529, 674, 248, 130, 652, 17, 810, 1146, 395, 1104, 201, 125, 232, 537, 35, 990, 987, 1097, 710, 480, 327, 90, 1134, 219, 974, 733, 202, 807, 861, 181, 553, 477, 783, 947, 860, 112, 56, 1179, 654, 215, 685, 558, 1155, 586, 522, 726, 1201, 778, 748, 1037, 333, 516, 794, 124, 533, 722, 115, 690, 1154, 214, 283, 673, 906, 823, 393, 87, 411, 788, 497, 121, 930, 813, 792, 1203, 1072, 396, 51, 1031, 667, 909, 895, 811, 330, 107, 637, 884, 683, 1124, 342, 495, 104, 632, 527, 879, 567, 1098, 1140, 1054, 1075, 299, 590, 768, 308, 887, 1010, 218, 1049, 460, 367, 581, 1164, 605, 742, 761, 598, 464, 254, 375, 262, 766, 1145, 386, 1115, 513, 528, 231, 1135, 392, 865, 828, 1186, 660, 96, 274, 534, 983, 1021, 321, 547, 278, 1199, 804, 1026, 93, 797, 1006, 217, 482, 1017, 246, 1045, 1032, 784, 997, 882, 795, 334, 485, 641, 253, 610, 763, 63, 1074, 878, 76, 478, 1058, 584, 607, 859, 84, 869, 1168, 791, 118, 434, 519, 44, 381, 184, 213, 453, 622, 443, 55, 206, 651, 138, 292, 315, 959, 576, 320, 506, 132, 1151, 998, 1169, 986, 28, 525, 952, 669, 613, 957, 801, 852, 969, 752, 323, 829, 1065, 716, 220, 560, 276, 224, 474, 1166, 866, 91, 738, 595, 798, 297, 625, 180, 816, 938, 350, 999, 1159, 176, 684, 635, 174, 643, 1180, 1012, 1213, 1188, 809, 566, 133, 578, 1025, 343, 357, 1200, 597, 1108, 951, 99, 1204, 273, 483, 647, 1183, 623, 339, 361, 531, 412, 552, 68, 715, 62, 1202, 614, 758, 620, 1207, 680, 417, 10, 491, 698, 1195, 1123, 1030, 720, 913, 264, 295, 296, 1005, 179, 471, 54, 1046, 812, 369, 757, 32, 853, 1142, 542, 842, 146, 11, 286, 1079, 5, 961, 559, 1205, 965, 322, 1157, 1036, 972, 116, 1064, 985, 329, 702, 16, 221, 20, 1047, 771, 937, 830, 1138, 953, 1081, 178, 366, 1156, 435, 74, 18, 80, 39, 1063, 786, 257, 834, 430, 618, 250, 1039, 514, 288, 1112, 968, 505, 587, 872, 588, 915, 964, 431, 502, 714, 730, 799, 845, 390, 139, 204, 324, 1040, 511, 1196, 437, 642, 1035, 1160, 1184, 582, 363, 410, 532, 624, 503, 463, 325, 1076, 1129, 832, 835, 1034, 12, 446, 136, 256, 92, 151, 608, 676, 628, 317, 1002, 991, 1071, 919, 1105, 1069, 979, 585, 926, 457, 862, 523, 319, 338, 892, 594, 307, 57, 1093, 689, 1178, 279, 127, 1094, 621, 452, 114, 554, 425, 205, 160, 967, 83, 1062, 314, 619, 175, 638, 1132, 741, 280, 686, 387, 488, 169, 874, 420, 186, 230, 216, 924, 1101, 97, 438, 298, 122, 1209, 415, 723, 779, 609, 826, 153, 602, 954, 7, 364, 493, 687, 22, 161, 1165, 108, 889, 137, 1103, 536, 428, 948, 128, 931, 355, 384, 403, 237, 767, 1096, 85, 126, 379, 311, 787, 1029, 113, 886, 701, 942, 147, 441, 640, 600, 1110, 1003, 421, 549, 977, 770, 429, 46, 1023, 414, 819, 891, 378, 285, 870, 890, 1001, 776, 207, 469, 170, 840, 570, 747, 24, 98, 655, 579, 544, 902, 416, 918, 191, 901, 227, 936, 43, 1027, 454, 135, 291, 1137, 755, 1057, 1131, 662, 681, 982, 27, 591, 1051, 1009, 765, 302, 539, 405, 688, 1102, 489, 571, 569, 1136, 240, 1092, 910, 419, 709, 336, 300, 1088, 140, 851, 159, 1144, 313, 155, 649, 309, 1143, 134, 4, 934, 388, 975, 843, 30, 564, 575, 372, 646, 864, 546, 365, 871, 976, 1189, 282, 36, 727, 759, 105, 166, 45, 49, 1208, 873, 326, 510, 269, 725, 808, 34, 629, 1050, 37, 199, 1171, 407, 25, 494, 244, 21, 267, 348, 782, 356, 648, 1139, 251, 398, 465, 335, 650, 177, 978, 596, 885, 880, 501, 604, 847, 745, 612, 212, 818, 682, 304, 64, 245, 1078, 73, 200, 1172, 1194, 1028, 1185, 541, 817, 550, 316, 1117, 1130, 397, 955, 1121, 119, 1158, 271, 496, 1125, 557, 193, 6, 492, 42, 284, 639, 203, 897, 402, 920, 996, 824, 644, 445, 1, 66, 1055, 992, 713, 1099, 530, 450, 756, 347, 803, 1082, 260, 211, 391, 1073, 524, 1000, 875, 185, 995, 455, 259, 633, 656, 574, 50, 40, 1193, 247, 239, 773, 769, 67, 963, 1198, 1041, 728, 303, 565, 470, 796, 753, 1113, 23, 1141, 1152, 705, 371, 58, 1044, 48, 341, 1061, 426, 102, 358, 754, 38, 294, 599, 679, 706, 573, 252, 898, 841, 1192, 168, 1038, 9, 94, 691, 61, 1011, 905, 636, 857, 822, 1042, 486, 475, 663, 3, 210, 110, 52, 815, 1181, 945, 925, 8, 1066, 1150, 989, 173, 1109, 331, 645, 950, 653, 543, 735, 26, 270, 72, 0, 234, 1206, 750, 427, 881, 143, 993, 1015, 1053, 312, 1170, 1086, 790, 971, 900, 468, 47, 712, 164, 780, 1084, 839, 53, 1007, 448, 561, 970, 373, 666, 156, 368, 33, 1083, 601, 908, 1070, 1106, 41, 932, 980, 13, 1190]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3307245663304244
the save name prefix for this run is:  chkpt-ID_3307245663304244_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 461
rank avg (pred): 0.492 +- 0.004
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001012266

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 625
rank avg (pred): 0.463 +- 0.013
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 9.15962e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.323 +- 0.134
mrr vals (pred, true): 0.015, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004372885

Epoch over!
epoch time: 15.213

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 33
rank avg (pred): 0.290 +- 0.158
mrr vals (pred, true): 0.061, 0.109
batch losses (mrrl, rdl): 0.0, 0.0009105056

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 784
rank avg (pred): 0.290 +- 0.228
mrr vals (pred, true): 0.190, 0.003
batch losses (mrrl, rdl): 0.0, 0.0005750458

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 658
rank avg (pred): 0.368 +- 0.289
mrr vals (pred, true): 0.184, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001239911

Epoch over!
epoch time: 15.161

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 36
rank avg (pred): 0.336 +- 0.256
mrr vals (pred, true): 0.122, 0.125
batch losses (mrrl, rdl): 0.0, 0.0014729468

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.320 +- 0.274
mrr vals (pred, true): 0.229, 0.021
batch losses (mrrl, rdl): 0.0, 3.68747e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 172
rank avg (pred): 0.336 +- 0.291
mrr vals (pred, true): 0.173, 0.003
batch losses (mrrl, rdl): 0.0, 0.0003058536

Epoch over!
epoch time: 15.406

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 749
rank avg (pred): 0.351 +- 0.291
mrr vals (pred, true): 0.154, 0.182
batch losses (mrrl, rdl): 0.0, 0.001848266

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 219
rank avg (pred): 0.321 +- 0.295
mrr vals (pred, true): 0.189, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003610306

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 218
rank avg (pred): 0.315 +- 0.300
mrr vals (pred, true): 0.170, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003670467

Epoch over!
epoch time: 15.049

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 761
rank avg (pred): 0.385 +- 0.308
mrr vals (pred, true): 0.095, 0.003
batch losses (mrrl, rdl): 0.0, 0.0002136392

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 340
rank avg (pred): 0.352 +- 0.323
mrr vals (pred, true): 0.148, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002098146

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 753
rank avg (pred): 0.324 +- 0.302
mrr vals (pred, true): 0.190, 0.219
batch losses (mrrl, rdl): 0.0, 0.001626462

Epoch over!
epoch time: 14.972

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 750
rank avg (pred): 0.356 +- 0.308
mrr vals (pred, true): 0.132, 0.201
batch losses (mrrl, rdl): 0.046844773, 0.0018642611

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 683
rank avg (pred): 0.461 +- 0.228
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 2.6478e-06, 1.6992e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 252
rank avg (pred): 0.347 +- 0.264
mrr vals (pred, true): 0.107, 0.283
batch losses (mrrl, rdl): 0.3093791008, 0.0019845366

Epoch over!
epoch time: 15.235

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 325
rank avg (pred): 0.355 +- 0.266
mrr vals (pred, true): 0.102, 0.004
batch losses (mrrl, rdl): 0.0270917751, 0.0001910987

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 429
rank avg (pred): 0.379 +- 0.282
mrr vals (pred, true): 0.090, 0.006
batch losses (mrrl, rdl): 0.0156682394, 0.0001525429

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 571
rank avg (pred): 0.487 +- 0.285
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 8.76897e-05, 1.03946e-05

Epoch over!
epoch time: 15.183

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 212
rank avg (pred): 0.358 +- 0.284
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.024196703, 0.0003029309

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 732
rank avg (pred): 0.315 +- 0.292
mrr vals (pred, true): 0.118, 0.262
batch losses (mrrl, rdl): 0.2058179826, 0.0015228874

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 771
rank avg (pred): 0.349 +- 0.293
mrr vals (pred, true): 0.096, 0.003
batch losses (mrrl, rdl): 0.0207118709, 0.0008938517

Epoch over!
epoch time: 15.187

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.347 +- 0.304
mrr vals (pred, true): 0.120, 0.004
batch losses (mrrl, rdl): 0.0489092842, 0.0003204584

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 317
rank avg (pred): 0.385 +- 0.282
mrr vals (pred, true): 0.081, 0.189
batch losses (mrrl, rdl): 0.1174012721, 0.0018194735

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1089
rank avg (pred): 0.364 +- 0.286
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0344082564, 0.0002477172

Epoch over!
epoch time: 15.257

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1115
rank avg (pred): 0.335 +- 0.292
mrr vals (pred, true): 0.101, 0.004
batch losses (mrrl, rdl): 0.0263557993, 0.0005063867

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 776
rank avg (pred): 0.357 +- 0.279
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.0236650575, 0.0003713896

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 245
rank avg (pred): 0.363 +- 0.297
mrr vals (pred, true): 0.106, 0.306
batch losses (mrrl, rdl): 0.3991404474, 0.0019548293

Epoch over!
epoch time: 15.268

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1076
rank avg (pred): 0.365 +- 0.296
mrr vals (pred, true): 0.105, 0.254
batch losses (mrrl, rdl): 0.2233544588, 0.0017716342

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 569
rank avg (pred): 0.459 +- 0.337
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 6.7405e-06, 6.40579e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.365 +- 0.285
mrr vals (pred, true): 0.079, 0.003
batch losses (mrrl, rdl): 0.0086623747, 0.0003498184

Epoch over!
epoch time: 15.358

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 632
rank avg (pred): 0.453 +- 0.334
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001690629, 9.11844e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1082
rank avg (pred): 0.358 +- 0.305
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0575363562, 0.000364401

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 115
rank avg (pred): 0.339 +- 0.267
mrr vals (pred, true): 0.093, 0.003
batch losses (mrrl, rdl): 0.0181769039, 0.0004546687

Epoch over!
epoch time: 15.333

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 374
rank avg (pred): 0.357 +- 0.284
mrr vals (pred, true): 0.099, 0.005
batch losses (mrrl, rdl): 0.0239170156, 0.0003094293

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.397 +- 0.311
mrr vals (pred, true): 0.055, 0.012
batch losses (mrrl, rdl): 0.0002227586, 1.71522e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 137
rank avg (pred): 0.352 +- 0.263
mrr vals (pred, true): 0.085, 0.004
batch losses (mrrl, rdl): 0.0125956573, 0.0003353666

Epoch over!
epoch time: 15.272

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.367 +- 0.263
mrr vals (pred, true): 0.097, 0.004
batch losses (mrrl, rdl): 0.0217556246, 0.000273974

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.354 +- 0.292
mrr vals (pred, true): 0.124, 0.004
batch losses (mrrl, rdl): 0.0546181984, 0.0002894556

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 792
rank avg (pred): 0.364 +- 0.296
mrr vals (pred, true): 0.111, 0.005
batch losses (mrrl, rdl): 0.0373731405, 0.000319473

Epoch over!
epoch time: 15.709

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.365 +- 0.293
mrr vals (pred, true): 0.112, 0.174
batch losses (mrrl, rdl): 0.0389705375, 0.0014863599

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 338
rank avg (pred): 0.370 +- 0.296
mrr vals (pred, true): 0.112, 0.005
batch losses (mrrl, rdl): 0.0381562524, 0.0001902459

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 830
rank avg (pred): 0.364 +- 0.310
mrr vals (pred, true): 0.103, 0.248
batch losses (mrrl, rdl): 0.2102930695, 0.001442206

Epoch over!
epoch time: 17.22

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.608 +- 0.255
mrr vals (pred, true): 0.051, 0.001

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04931 	 0.00088 	 m..s
    2 	     1 	 0.04936 	 0.00088 	 m..s
   15 	     2 	 0.05130 	 0.00092 	 m..s
   10 	     3 	 0.05081 	 0.00102 	 m..s
    0 	     4 	 0.04685 	 0.00128 	 m..s
    5 	     5 	 0.04966 	 0.00140 	 m..s
    3 	     6 	 0.04941 	 0.00192 	 m..s
   29 	     7 	 0.05325 	 0.00213 	 m..s
  115 	     8 	 0.13632 	 0.00246 	 MISS
   34 	     9 	 0.05380 	 0.00261 	 m..s
   82 	    10 	 0.10804 	 0.00272 	 MISS
   23 	    11 	 0.05208 	 0.00279 	 m..s
   17 	    12 	 0.05144 	 0.00294 	 m..s
   21 	    13 	 0.05205 	 0.00301 	 m..s
    7 	    14 	 0.04987 	 0.00302 	 m..s
   24 	    15 	 0.05223 	 0.00304 	 m..s
   36 	    16 	 0.05391 	 0.00305 	 m..s
   27 	    17 	 0.05313 	 0.00305 	 m..s
   89 	    18 	 0.11415 	 0.00330 	 MISS
   76 	    19 	 0.10191 	 0.00331 	 m..s
  100 	    20 	 0.12896 	 0.00334 	 MISS
   17 	    21 	 0.05144 	 0.00335 	 m..s
  102 	    22 	 0.12933 	 0.00336 	 MISS
   96 	    23 	 0.12400 	 0.00343 	 MISS
    9 	    24 	 0.04994 	 0.00348 	 m..s
   28 	    25 	 0.05314 	 0.00350 	 m..s
   74 	    26 	 0.10078 	 0.00352 	 m..s
   88 	    27 	 0.11370 	 0.00353 	 MISS
   98 	    28 	 0.12763 	 0.00361 	 MISS
  112 	    29 	 0.13612 	 0.00362 	 MISS
   84 	    30 	 0.10825 	 0.00363 	 MISS
  117 	    31 	 0.13771 	 0.00366 	 MISS
   55 	    32 	 0.09144 	 0.00367 	 m..s
   26 	    33 	 0.05294 	 0.00367 	 m..s
  110 	    34 	 0.13589 	 0.00368 	 MISS
   50 	    35 	 0.08966 	 0.00368 	 m..s
   64 	    36 	 0.09818 	 0.00371 	 m..s
   71 	    37 	 0.09950 	 0.00372 	 m..s
   69 	    38 	 0.09928 	 0.00372 	 m..s
   31 	    39 	 0.05328 	 0.00373 	 m..s
  120 	    40 	 0.13891 	 0.00375 	 MISS
   97 	    41 	 0.12629 	 0.00378 	 MISS
   16 	    42 	 0.05132 	 0.00381 	 m..s
   32 	    43 	 0.05356 	 0.00381 	 m..s
   54 	    44 	 0.09127 	 0.00383 	 m..s
   21 	    45 	 0.05205 	 0.00384 	 m..s
   57 	    46 	 0.09248 	 0.00385 	 m..s
   44 	    47 	 0.05705 	 0.00386 	 m..s
   60 	    48 	 0.09611 	 0.00387 	 m..s
   67 	    49 	 0.09907 	 0.00394 	 m..s
   79 	    50 	 0.10602 	 0.00397 	 MISS
   82 	    51 	 0.10804 	 0.00404 	 MISS
   52 	    52 	 0.09045 	 0.00404 	 m..s
   10 	    53 	 0.05081 	 0.00405 	 m..s
   25 	    54 	 0.05278 	 0.00408 	 m..s
   35 	    55 	 0.05384 	 0.00410 	 m..s
   53 	    56 	 0.09113 	 0.00410 	 m..s
   81 	    57 	 0.10758 	 0.00417 	 MISS
   95 	    58 	 0.12273 	 0.00418 	 MISS
   79 	    59 	 0.10602 	 0.00421 	 MISS
   19 	    60 	 0.05187 	 0.00421 	 m..s
   66 	    61 	 0.09872 	 0.00428 	 m..s
   41 	    62 	 0.05623 	 0.00433 	 m..s
  104 	    63 	 0.13041 	 0.00434 	 MISS
   56 	    64 	 0.09201 	 0.00435 	 m..s
   43 	    65 	 0.05702 	 0.00440 	 m..s
   70 	    66 	 0.09943 	 0.00444 	 m..s
   78 	    67 	 0.10574 	 0.00444 	 MISS
   39 	    68 	 0.05522 	 0.00445 	 m..s
   14 	    69 	 0.05129 	 0.00448 	 m..s
   87 	    70 	 0.10864 	 0.00452 	 MISS
    8 	    71 	 0.04990 	 0.00452 	 m..s
   48 	    72 	 0.08947 	 0.00455 	 m..s
  107 	    73 	 0.13330 	 0.00456 	 MISS
    6 	    74 	 0.04983 	 0.00459 	 m..s
   59 	    75 	 0.09473 	 0.00460 	 m..s
   45 	    76 	 0.05847 	 0.00464 	 m..s
   13 	    77 	 0.05113 	 0.00464 	 m..s
   62 	    78 	 0.09799 	 0.00466 	 m..s
   99 	    79 	 0.12782 	 0.00469 	 MISS
   68 	    80 	 0.09925 	 0.00473 	 m..s
  116 	    81 	 0.13642 	 0.00478 	 MISS
   46 	    82 	 0.08806 	 0.00479 	 m..s
  110 	    83 	 0.13589 	 0.00498 	 MISS
   12 	    84 	 0.05102 	 0.00501 	 m..s
    4 	    85 	 0.04959 	 0.00510 	 m..s
   93 	    86 	 0.12110 	 0.00514 	 MISS
   73 	    87 	 0.10053 	 0.00531 	 m..s
  106 	    88 	 0.13281 	 0.00533 	 MISS
   19 	    89 	 0.05187 	 0.00656 	 m..s
   37 	    90 	 0.05396 	 0.01020 	 m..s
   32 	    91 	 0.05356 	 0.01098 	 m..s
   38 	    92 	 0.05447 	 0.01237 	 m..s
   29 	    93 	 0.05325 	 0.02455 	 ~...
   41 	    94 	 0.05623 	 0.02763 	 ~...
   40 	    95 	 0.05543 	 0.02838 	 ~...
   77 	    96 	 0.10461 	 0.06382 	 m..s
   58 	    97 	 0.09344 	 0.12484 	 m..s
   51 	    98 	 0.09018 	 0.12873 	 m..s
   46 	    99 	 0.08806 	 0.13628 	 m..s
   49 	   100 	 0.08955 	 0.13718 	 m..s
   75 	   101 	 0.10165 	 0.18174 	 m..s
   72 	   102 	 0.09989 	 0.18824 	 m..s
   60 	   103 	 0.09611 	 0.19121 	 m..s
   65 	   104 	 0.09826 	 0.19918 	 MISS
   63 	   105 	 0.09803 	 0.22949 	 MISS
   91 	   106 	 0.11737 	 0.23886 	 MISS
   90 	   107 	 0.11496 	 0.24352 	 MISS
  101 	   108 	 0.12905 	 0.24983 	 MISS
  105 	   109 	 0.13273 	 0.25004 	 MISS
  109 	   110 	 0.13387 	 0.25712 	 MISS
  102 	   111 	 0.12933 	 0.26436 	 MISS
  114 	   112 	 0.13621 	 0.26513 	 MISS
   92 	   113 	 0.11806 	 0.27020 	 MISS
   94 	   114 	 0.12202 	 0.28283 	 MISS
  118 	   115 	 0.13796 	 0.29488 	 MISS
   86 	   116 	 0.10858 	 0.29612 	 MISS
  113 	   117 	 0.13621 	 0.30476 	 MISS
  107 	   118 	 0.13330 	 0.30783 	 MISS
   85 	   119 	 0.10843 	 0.31910 	 MISS
  119 	   120 	 0.13842 	 0.35668 	 MISS
==========================================
r_mrr = 0.43248048424720764
r2_mrr = 0.024581074714660645
spearmanr_mrr@5 = 0.8296009302139282
spearmanr_mrr@10 = 0.9164467453956604
spearmanr_mrr@50 = 0.9231027960777283
spearmanr_mrr@100 = 0.6980196833610535
spearmanr_mrr@All = 0.6921014785766602
==========================================
test time: 0.529
Done Testing dataset CoDExSmall
total time taken: 242.41499781608582
training time taken: 231.36232614517212
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.4325)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.0246)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.8296)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9164)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9231)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.6980)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.6921)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 6.363833216903913}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 2660138701403651
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [881, 274, 983, 1157, 1150, 555, 1015, 791, 419, 719, 757, 921, 36, 364, 1188, 941, 558, 901, 198, 832, 696, 40, 61, 281, 1194, 59, 739, 378, 53, 607, 1139, 814, 675, 984, 353, 928, 1039, 255, 460, 846, 43, 362, 903, 46, 210, 375, 957, 1169, 544, 823, 310, 371, 651, 193, 27, 864, 501, 120, 751, 1115, 89, 792, 550, 882, 621, 899, 926, 620, 736, 880, 775, 594, 730, 166, 110, 496, 291, 139, 1027, 1108, 1103, 160, 799, 724, 701, 737, 654, 249, 1081, 233, 269, 156, 12, 435, 1098, 686, 441, 179, 1154, 1023, 1173, 996, 294, 908, 973, 282, 968, 1145, 333, 532, 706, 712, 655, 127, 841, 1160, 162, 184, 1, 749, 222]
valid_ids (0): []
train_ids (1094): [617, 334, 562, 1105, 1043, 844, 1159, 138, 1198, 509, 429, 24, 994, 224, 497, 627, 998, 1199, 257, 833, 802, 1085, 1165, 785, 566, 180, 860, 952, 1028, 22, 384, 403, 443, 695, 35, 810, 761, 200, 936, 77, 723, 411, 287, 347, 631, 482, 766, 100, 277, 672, 1211, 163, 355, 689, 504, 146, 1013, 48, 381, 315, 207, 949, 1017, 18, 1210, 1084, 23, 1118, 454, 769, 214, 1053, 679, 1111, 746, 389, 143, 392, 405, 913, 907, 358, 1186, 455, 918, 56, 342, 728, 506, 1022, 537, 1079, 895, 643, 1185, 244, 794, 1060, 388, 929, 866, 1051, 336, 352, 426, 1163, 106, 545, 820, 540, 524, 1208, 438, 1152, 767, 440, 58, 990, 759, 923, 155, 965, 800, 979, 126, 173, 1175, 569, 889, 1137, 112, 1004, 1126, 638, 174, 124, 644, 21, 565, 1038, 781, 424, 851, 930, 363, 39, 859, 343, 900, 571, 1046, 745, 650, 945, 665, 1029, 452, 253, 1148, 1113, 189, 959, 597, 60, 987, 1156, 290, 1026, 408, 1136, 326, 483, 891, 836, 232, 57, 539, 639, 31, 771, 1166, 503, 584, 1068, 177, 83, 659, 365, 312, 956, 523, 190, 788, 87, 19, 946, 511, 1037, 1178, 436, 932, 1020, 669, 181, 54, 783, 1000, 938, 579, 1155, 768, 196, 811, 288, 718, 185, 71, 286, 1171, 1146, 492, 714, 611, 962, 250, 808, 25, 741, 300, 1124, 670, 176, 1003, 1087, 916, 893, 47, 314, 321, 301, 734, 81, 777, 444, 434, 246, 868, 495, 848, 619, 13, 374, 450, 839, 733, 103, 20, 773, 822, 380, 711, 824, 1168, 512, 266, 637, 464, 1086, 15, 1007, 1102, 829, 385, 320, 349, 552, 653, 95, 323, 298, 105, 753, 69, 813, 704, 580, 1054, 1089, 499, 311, 1128, 152, 633, 459, 489, 169, 886, 217, 357, 1011, 747, 468, 226, 593, 279, 414, 939, 821, 570, 449, 165, 1164, 313, 517, 150, 977, 806, 26, 239, 604, 616, 234, 2, 416, 534, 585, 850, 219, 225, 588, 835, 1096, 167, 758, 583, 168, 568, 80, 855, 1106, 694, 284, 1042, 73, 919, 33, 367, 231, 656, 115, 1073, 340, 394, 1205, 433, 681, 406, 682, 360, 243, 680, 674, 911, 431, 1138, 985, 954, 151, 238, 904, 386, 1062, 710, 1200, 1101, 1070, 809, 914, 327, 272, 790, 1192, 1177, 116, 400, 215, 251, 328, 37, 415, 119, 576, 74, 410, 157, 4, 636, 687, 975, 465, 445, 1063, 1083, 1076, 1010, 149, 1024, 1197, 1190, 472, 346, 123, 974, 992, 595, 413, 471, 1080, 581, 887, 425, 778, 700, 667, 154, 872, 108, 283, 756, 892, 807, 1123, 615, 254, 1184, 264, 563, 873, 890, 303, 601, 144, 209, 285, 463, 256, 493, 995, 292, 178, 211, 396, 1117, 986, 182, 359, 270, 484, 79, 553, 705, 731, 1120, 1040, 1045, 518, 796, 235, 366, 933, 609, 755, 856, 409, 1014, 516, 369, 68, 260, 688, 332, 519, 572, 707, 319, 1181, 629, 480, 826, 828, 513, 1100, 600, 951, 420, 692, 554, 671, 625, 1092, 129, 1134, 547, 612, 344, 685, 750, 922, 10, 634, 1041, 339, 1202, 804, 118, 107, 30, 159, 702, 1201, 527, 953, 191, 469, 1167, 812, 218, 462, 838, 780, 722, 556, 86, 764, 302, 915, 248, 491, 421, 842, 652, 1064, 628, 442, 1074, 748, 430, 765, 646, 1077, 978, 970, 699, 1109, 1203, 803, 1075, 658, 894, 883, 293, 567, 502, 220, 797, 991, 7, 927, 1091, 446, 75, 795, 242, 691, 587, 976, 1093, 879, 849, 909, 11, 1110, 478, 265, 397, 370, 1135, 510, 236, 898, 542, 88, 267, 308, 716, 192, 1183, 213, 1095, 330, 819, 17, 521, 221, 131, 676, 94, 316, 278, 1125, 439, 1132, 498, 1180, 863, 546, 295, 522, 1056, 1147, 865, 66, 1078, 1055, 988, 140, 752, 657, 935, 65, 418, 677, 275, 391, 708, 531, 526, 660, 1191, 132, 473, 377, 456, 717, 51, 354, 114, 910, 982, 825, 1033, 206, 1212, 461, 479, 348, 559, 6, 1019, 268, 404, 477, 1172, 997, 96, 869, 774, 372, 194, 437, 1001, 432, 673, 98, 818, 14, 989, 111, 229, 309, 787, 241, 172, 350, 770, 128, 1129, 878, 325, 1072, 1008, 395, 1044, 683, 16, 470, 448, 548, 104, 399, 713, 1195, 912, 137, 383, 661, 62, 720, 276, 917, 195, 740, 582, 613, 3, 1193, 259, 1196, 457, 801, 1158, 1187, 1144, 549, 475, 458, 102, 34, 101, 870, 398, 109, 1067, 738, 1097, 1130, 317, 390, 122, 877, 345, 575, 201, 228, 148, 742, 133, 1149, 170, 204, 950, 1142, 862, 599, 837, 827, 525, 401, 97, 1050, 605, 205, 1127, 1006, 161, 1209, 1104, 164, 368, 42, 338, 63, 1009, 197, 341, 183, 666, 993, 817, 64, 72, 488, 533, 942, 273, 560, 754, 626, 972, 38, 32, 188, 1021, 41, 28, 980, 387, 306, 1088, 529, 793, 1030, 447, 263, 888, 958, 1107, 335, 84, 379, 136, 861, 622, 487, 937, 322, 361, 1025, 948, 896, 782, 125, 564, 1094, 481, 373, 1018, 1161, 141, 91, 29, 67, 635, 964, 1002, 649, 299, 760, 647, 1065, 476, 494, 258, 230, 684, 55, 1189, 514, 624, 763, 905, 925, 171, 1143, 591, 186, 175, 331, 981, 725, 135, 1119, 1213, 1012, 721, 417, 735, 645, 577, 535, 1153, 590, 606, 871, 407, 117, 280, 324, 93, 902, 840, 1214, 697, 1071, 955, 76, 853, 920, 474, 632, 831, 508, 969, 153, 662, 1151, 262, 45, 208, 453, 779, 1049, 1179, 515, 1058, 541, 589, 147, 451, 1057, 610, 1170, 0, 538, 726, 1052, 82, 199, 356, 145, 1112, 428, 351, 247, 121, 943, 237, 1059, 602, 1204, 252, 947, 561, 727, 158, 924, 897, 203, 1061, 1114, 52, 1162, 1016, 240, 1176, 543, 875, 640, 49, 641, 618, 729, 423, 858, 971, 467, 776, 536, 668, 815, 130, 427, 1082, 505, 642, 1034, 92, 961, 297, 1116, 608, 485, 854, 1099, 99, 9, 402, 78, 1174, 960, 5, 586, 663, 678, 134, 289, 592, 216, 847, 70, 304, 834, 296, 393, 1122, 271, 693, 551, 805, 772, 490, 212, 1047, 934, 422, 843, 1207, 857, 967, 318, 113, 466, 1005, 867, 1031, 744, 798, 614, 223, 1066, 1090, 885, 1032, 329, 376, 709, 963, 732, 245, 623, 1206, 578, 698, 966, 1131, 520, 664, 500, 816, 789, 90, 261, 142, 648, 884, 573, 762, 940, 412, 1036, 1069, 382, 598, 715, 931, 85, 703, 830, 845, 337, 307, 507, 1048, 852, 1182, 906, 530, 876, 305, 786, 1140, 8, 630, 1035, 44, 187, 743, 486, 574, 596, 690, 202, 557, 528, 1133, 603, 784, 1121, 874, 999, 50, 227, 1141, 944]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6195565893011902
the save name prefix for this run is:  chkpt-ID_6195565893011902_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 912
rank avg (pred): 0.529 +- 0.004
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 0.0004711025

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 473
rank avg (pred): 0.452 +- 0.180
mrr vals (pred, true): 0.016, 0.004
batch losses (mrrl, rdl): 0.0, 3.9462e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 81
rank avg (pred): 0.447 +- 0.274
mrr vals (pred, true): 0.110, 0.003
batch losses (mrrl, rdl): 0.0, 8.6834e-06

Epoch over!
epoch time: 15.905

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 687
rank avg (pred): 0.485 +- 0.301
mrr vals (pred, true): 0.100, 0.003
batch losses (mrrl, rdl): 0.0, 1.10296e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 432
rank avg (pred): 0.458 +- 0.274
mrr vals (pred, true): 0.091, 0.005
batch losses (mrrl, rdl): 0.0, 4.1666e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.085 +- 0.181
mrr vals (pred, true): 0.156, 0.186
batch losses (mrrl, rdl): 0.0, 4.9365e-06

Epoch over!
epoch time: 15.171

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 704
rank avg (pred): 0.474 +- 0.283
mrr vals (pred, true): 0.090, 0.005
batch losses (mrrl, rdl): 0.0, 3.4787e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 230
rank avg (pred): 0.468 +- 0.275
mrr vals (pred, true): 0.087, 0.005
batch losses (mrrl, rdl): 0.0, 3.353e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 865
rank avg (pred): 0.461 +- 0.286
mrr vals (pred, true): 0.103, 0.004
batch losses (mrrl, rdl): 0.0, 1.2919e-06

Epoch over!
epoch time: 14.96

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1119
rank avg (pred): 0.453 +- 0.264
mrr vals (pred, true): 0.093, 0.003
batch losses (mrrl, rdl): 0.0, 1.37535e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.068 +- 0.150
mrr vals (pred, true): 0.136, 0.210
batch losses (mrrl, rdl): 0.0, 4.4439e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.468 +- 0.293
mrr vals (pred, true): 0.101, 0.004
batch losses (mrrl, rdl): 0.0, 9.6589e-06

Epoch over!
epoch time: 15.884

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 471
rank avg (pred): 0.459 +- 0.274
mrr vals (pred, true): 0.095, 0.004
batch losses (mrrl, rdl): 0.0, 5.4675e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 499
rank avg (pred): 0.372 +- 0.273
mrr vals (pred, true): 0.101, 0.028
batch losses (mrrl, rdl): 0.0, 6.6921e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1174
rank avg (pred): 0.485 +- 0.277
mrr vals (pred, true): 0.083, 0.003
batch losses (mrrl, rdl): 0.0, 1.6861e-06

Epoch over!
epoch time: 14.932

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 324
rank avg (pred): 0.465 +- 0.276
mrr vals (pred, true): 0.088, 0.004
batch losses (mrrl, rdl): 0.0142318616, 1.7757e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 154
rank avg (pred): 0.505 +- 0.213
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0002329001, 5.3328e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 563
rank avg (pred): 0.468 +- 0.253
mrr vals (pred, true): 0.061, 0.006
batch losses (mrrl, rdl): 0.0011817776, 4.00841e-05

Epoch over!
epoch time: 15.157

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 355
rank avg (pred): 0.430 +- 0.237
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002928419, 6.28633e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 405
rank avg (pred): 0.487 +- 0.234
mrr vals (pred, true): 0.044, 0.004
batch losses (mrrl, rdl): 0.0003221953, 1.04152e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 192
rank avg (pred): 0.472 +- 0.278
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004752385, 1.10529e-05

Epoch over!
epoch time: 15.021

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.465 +- 0.240
mrr vals (pred, true): 0.044, 0.004
batch losses (mrrl, rdl): 0.0004094122, 1.56586e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 807
rank avg (pred): 0.689 +- 0.228
mrr vals (pred, true): 0.013, 0.004
batch losses (mrrl, rdl): 0.0134325512, 0.0008523144

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 693
rank avg (pred): 0.460 +- 0.253
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 6.59426e-05, 2.21945e-05

Epoch over!
epoch time: 14.938

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.478 +- 0.287
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 0.0001093588, 5.0624e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.040 +- 0.054
mrr vals (pred, true): 0.223, 0.238
batch losses (mrrl, rdl): 0.0022021607, 1.4023e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 977
rank avg (pred): 0.013 +- 0.035
mrr vals (pred, true): 0.265, 0.302
batch losses (mrrl, rdl): 0.014145677, 1.88003e-05

Epoch over!
epoch time: 14.927

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 42
rank avg (pred): 0.130 +- 0.098
mrr vals (pred, true): 0.190, 0.137
batch losses (mrrl, rdl): 0.0274082366, 5.7006e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.514 +- 0.277
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 8.96348e-05, 1.08076e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 114
rank avg (pred): 0.483 +- 0.295
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.07486e-05, 9.9833e-06

Epoch over!
epoch time: 14.96

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 238
rank avg (pred): 0.396 +- 0.265
mrr vals (pred, true): 0.066, 0.005
batch losses (mrrl, rdl): 0.0026514197, 0.0001603959

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 425
rank avg (pred): 0.482 +- 0.232
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 7.99625e-05, 1.03131e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 798
rank avg (pred): 0.483 +- 0.263
mrr vals (pred, true): 0.063, 0.004
batch losses (mrrl, rdl): 0.0016042184, 4.4629e-06

Epoch over!
epoch time: 15.024

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 178
rank avg (pred): 0.486 +- 0.283
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 6.15724e-05, 3.5164e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 149
rank avg (pred): 0.500 +- 0.290
mrr vals (pred, true): 0.039, 0.004
batch losses (mrrl, rdl): 0.0011148147, 5.5415e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 483
rank avg (pred): 0.479 +- 0.275
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 2e-10, 3.1571e-06

Epoch over!
epoch time: 14.969

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 579
rank avg (pred): 0.483 +- 0.287
mrr vals (pred, true): 0.058, 0.003
batch losses (mrrl, rdl): 0.0006249943, 1.27847e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.526 +- 0.287
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 8.16564e-05, 2.27198e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1036
rank avg (pred): 0.513 +- 0.297
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.000511659, 1.25885e-05

Epoch over!
epoch time: 14.961

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 237
rank avg (pred): 0.499 +- 0.276
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.11375e-05, 1.6379e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1071
rank avg (pred): 0.440 +- 0.282
mrr vals (pred, true): 0.185, 0.199
batch losses (mrrl, rdl): 0.0020416037, 0.002975706

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 947
rank avg (pred): 0.474 +- 0.364
mrr vals (pred, true): 0.042, 0.003
batch losses (mrrl, rdl): 0.0005670208, 7.51776e-05

Epoch over!
epoch time: 15.037

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 212
rank avg (pred): 0.485 +- 0.287
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 7.23633e-05, 3.729e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1050
rank avg (pred): 0.487 +- 0.277
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001420968, 4.7413e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 24
rank avg (pred): 0.161 +- 0.115
mrr vals (pred, true): 0.189, 0.242
batch losses (mrrl, rdl): 0.0277597848, 0.0003213256

Epoch over!
epoch time: 15.062

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.446 +- 0.258
mrr vals (pred, true): 0.054, 0.005

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   50 	     0 	 0.04845 	 0.00088 	 m..s
   61 	     1 	 0.05093 	 0.00092 	 m..s
   41 	     2 	 0.04759 	 0.00113 	 m..s
   63 	     3 	 0.05106 	 0.00121 	 m..s
   50 	     4 	 0.04845 	 0.00145 	 m..s
   17 	     5 	 0.04466 	 0.00192 	 m..s
   15 	     6 	 0.04396 	 0.00194 	 m..s
    1 	     7 	 0.03519 	 0.00204 	 m..s
    2 	     8 	 0.03752 	 0.00205 	 m..s
   39 	     9 	 0.04757 	 0.00253 	 m..s
   39 	    10 	 0.04757 	 0.00262 	 m..s
   62 	    11 	 0.05099 	 0.00301 	 m..s
   35 	    12 	 0.04735 	 0.00321 	 m..s
   58 	    13 	 0.05018 	 0.00323 	 m..s
   22 	    14 	 0.04614 	 0.00327 	 m..s
   67 	    15 	 0.05175 	 0.00336 	 m..s
   13 	    16 	 0.04367 	 0.00340 	 m..s
   20 	    17 	 0.04595 	 0.00344 	 m..s
   84 	    18 	 0.05541 	 0.00360 	 m..s
   45 	    19 	 0.04781 	 0.00362 	 m..s
   81 	    20 	 0.05426 	 0.00369 	 m..s
   68 	    21 	 0.05182 	 0.00371 	 m..s
   52 	    22 	 0.04898 	 0.00375 	 m..s
   49 	    23 	 0.04824 	 0.00377 	 m..s
   11 	    24 	 0.04237 	 0.00381 	 m..s
   69 	    25 	 0.05183 	 0.00387 	 m..s
   60 	    26 	 0.05086 	 0.00387 	 m..s
   48 	    27 	 0.04819 	 0.00387 	 m..s
   14 	    28 	 0.04378 	 0.00388 	 m..s
   66 	    29 	 0.05171 	 0.00392 	 m..s
   30 	    30 	 0.04696 	 0.00397 	 m..s
   16 	    31 	 0.04448 	 0.00402 	 m..s
   10 	    32 	 0.04230 	 0.00404 	 m..s
    0 	    33 	 0.03451 	 0.00407 	 m..s
   64 	    34 	 0.05110 	 0.00412 	 m..s
   47 	    35 	 0.04815 	 0.00412 	 m..s
   55 	    36 	 0.04941 	 0.00413 	 m..s
    7 	    37 	 0.04161 	 0.00414 	 m..s
   23 	    38 	 0.04619 	 0.00415 	 m..s
   42 	    39 	 0.04765 	 0.00415 	 m..s
   34 	    40 	 0.04730 	 0.00416 	 m..s
   65 	    41 	 0.05114 	 0.00421 	 m..s
   37 	    42 	 0.04744 	 0.00424 	 m..s
   56 	    43 	 0.04960 	 0.00424 	 m..s
   36 	    44 	 0.04742 	 0.00425 	 m..s
   26 	    45 	 0.04654 	 0.00429 	 m..s
    5 	    46 	 0.04088 	 0.00431 	 m..s
   18 	    47 	 0.04514 	 0.00435 	 m..s
   30 	    48 	 0.04696 	 0.00440 	 m..s
   53 	    49 	 0.04903 	 0.00443 	 m..s
   33 	    50 	 0.04720 	 0.00447 	 m..s
   25 	    51 	 0.04653 	 0.00452 	 m..s
   29 	    52 	 0.04696 	 0.00452 	 m..s
   57 	    53 	 0.05001 	 0.00453 	 m..s
   59 	    54 	 0.05074 	 0.00453 	 m..s
   12 	    55 	 0.04283 	 0.00453 	 m..s
   77 	    56 	 0.05368 	 0.00459 	 m..s
    4 	    57 	 0.04058 	 0.00467 	 m..s
   42 	    58 	 0.04765 	 0.00468 	 m..s
   32 	    59 	 0.04701 	 0.00469 	 m..s
   46 	    60 	 0.04804 	 0.00469 	 m..s
   71 	    61 	 0.05212 	 0.00471 	 m..s
   72 	    62 	 0.05213 	 0.00473 	 m..s
    3 	    63 	 0.03795 	 0.00478 	 m..s
   37 	    64 	 0.04744 	 0.00479 	 m..s
   44 	    65 	 0.04772 	 0.00486 	 m..s
   20 	    66 	 0.04595 	 0.00490 	 m..s
   24 	    67 	 0.04646 	 0.00492 	 m..s
   28 	    68 	 0.04694 	 0.00495 	 m..s
    6 	    69 	 0.04100 	 0.00507 	 m..s
    9 	    70 	 0.04220 	 0.00510 	 m..s
   19 	    71 	 0.04580 	 0.00514 	 m..s
   54 	    72 	 0.04919 	 0.00528 	 m..s
   70 	    73 	 0.05185 	 0.00530 	 m..s
   27 	    74 	 0.04678 	 0.00532 	 m..s
    8 	    75 	 0.04187 	 0.00579 	 m..s
   74 	    76 	 0.05224 	 0.00729 	 m..s
   73 	    77 	 0.05224 	 0.00764 	 m..s
   78 	    78 	 0.05390 	 0.00858 	 m..s
   75 	    79 	 0.05311 	 0.00867 	 m..s
   80 	    80 	 0.05404 	 0.01178 	 m..s
   82 	    81 	 0.05448 	 0.01944 	 m..s
   82 	    82 	 0.05448 	 0.02050 	 m..s
   85 	    83 	 0.05548 	 0.02634 	 ~...
   85 	    84 	 0.05548 	 0.02856 	 ~...
   78 	    85 	 0.05390 	 0.02863 	 ~...
   88 	    86 	 0.05848 	 0.03020 	 ~...
   87 	    87 	 0.05742 	 0.03130 	 ~...
   75 	    88 	 0.05311 	 0.04398 	 ~...
  110 	    89 	 0.20252 	 0.05291 	 MISS
  111 	    90 	 0.20620 	 0.06382 	 MISS
   90 	    91 	 0.15499 	 0.10723 	 m..s
   93 	    92 	 0.15647 	 0.12460 	 m..s
   95 	    93 	 0.16012 	 0.12799 	 m..s
   89 	    94 	 0.15448 	 0.13339 	 ~...
   98 	    95 	 0.17049 	 0.13484 	 m..s
   96 	    96 	 0.16723 	 0.13821 	 ~...
   92 	    97 	 0.15606 	 0.13961 	 ~...
   97 	    98 	 0.16737 	 0.14054 	 ~...
  115 	    99 	 0.22021 	 0.15358 	 m..s
  104 	   100 	 0.19904 	 0.17202 	 ~...
  100 	   101 	 0.19542 	 0.17714 	 ~...
  109 	   102 	 0.20182 	 0.17731 	 ~...
  102 	   103 	 0.19625 	 0.18248 	 ~...
  105 	   104 	 0.19958 	 0.18824 	 ~...
  106 	   105 	 0.19979 	 0.19063 	 ~...
  103 	   106 	 0.19830 	 0.19078 	 ~...
  108 	   107 	 0.20081 	 0.19824 	 ~...
  112 	   108 	 0.20860 	 0.19918 	 ~...
  114 	   109 	 0.21935 	 0.21151 	 ~...
   94 	   110 	 0.16000 	 0.22857 	 m..s
  116 	   111 	 0.22763 	 0.24352 	 ~...
   91 	   112 	 0.15504 	 0.24463 	 m..s
  117 	   113 	 0.24337 	 0.25585 	 ~...
  106 	   114 	 0.19979 	 0.25693 	 m..s
   99 	   115 	 0.19141 	 0.26660 	 m..s
  117 	   116 	 0.24337 	 0.26886 	 ~...
  100 	   117 	 0.19542 	 0.27685 	 m..s
  117 	   118 	 0.24337 	 0.30476 	 m..s
  120 	   119 	 0.27977 	 0.31567 	 m..s
  113 	   120 	 0.21157 	 0.34968 	 MISS
==========================================
r_mrr = 0.9414734244346619
r2_mrr = 0.728222668170929
spearmanr_mrr@5 = 0.8943166136741638
spearmanr_mrr@10 = 0.9567170739173889
spearmanr_mrr@50 = 0.9627135992050171
spearmanr_mrr@100 = 0.978679358959198
spearmanr_mrr@All = 0.9795680046081543
==========================================
test time: 0.452
Done Testing dataset CoDExSmall
total time taken: 239.13198232650757
training time taken: 227.3710069656372
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9415)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7282)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.8943)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9567)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9627)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9787)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9796)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 1.193259154723819}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 5759413499077296
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [889, 243, 476, 458, 249, 977, 351, 1060, 1164, 17, 1128, 558, 772, 321, 1105, 596, 763, 127, 732, 289, 1029, 11, 136, 99, 780, 1062, 610, 710, 1175, 384, 232, 154, 517, 456, 980, 67, 427, 626, 203, 948, 568, 997, 223, 676, 753, 87, 313, 540, 276, 174, 387, 500, 843, 1159, 163, 861, 656, 85, 139, 350, 200, 1048, 931, 643, 1106, 20, 305, 408, 273, 646, 109, 511, 1025, 1005, 740, 425, 903, 1138, 78, 96, 1211, 797, 1061, 378, 83, 590, 1134, 149, 1142, 267, 449, 1201, 651, 1031, 112, 492, 825, 1047, 1037, 312, 1178, 735, 451, 247, 585, 399, 8, 158, 370, 573, 445, 1091, 34, 468, 1024, 159, 33, 617, 201, 840, 799]
valid_ids (0): []
train_ids (1094): [148, 81, 57, 721, 877, 452, 13, 1087, 719, 496, 360, 199, 1191, 786, 100, 157, 386, 960, 569, 895, 762, 802, 282, 1127, 416, 288, 850, 238, 853, 874, 992, 406, 114, 471, 541, 742, 463, 1143, 743, 91, 553, 811, 869, 973, 804, 867, 383, 993, 356, 1054, 302, 1093, 837, 325, 286, 3, 1194, 146, 778, 432, 911, 892, 213, 1171, 1103, 974, 1075, 1006, 228, 347, 1000, 420, 579, 366, 951, 1129, 301, 74, 947, 441, 1189, 418, 934, 1044, 1161, 619, 1072, 300, 284, 152, 145, 258, 9, 698, 989, 448, 1140, 882, 1003, 217, 536, 412, 110, 222, 757, 578, 1108, 1065, 657, 730, 981, 1056, 926, 293, 134, 696, 95, 31, 439, 51, 784, 856, 202, 263, 140, 1130, 684, 443, 264, 566, 75, 1212, 25, 188, 311, 944, 1050, 830, 922, 713, 489, 1034, 28, 41, 173, 580, 277, 212, 129, 298, 822, 1079, 252, 1176, 116, 563, 625, 779, 891, 679, 642, 333, 354, 829, 470, 1173, 589, 587, 894, 235, 1022, 402, 688, 885, 1098, 820, 1213, 475, 1137, 21, 358, 582, 921, 570, 380, 1084, 411, 707, 169, 245, 197, 985, 905, 1193, 604, 1036, 618, 120, 521, 485, 22, 90, 852, 562, 331, 118, 430, 318, 1162, 237, 1059, 652, 674, 689, 828, 549, 1168, 435, 798, 600, 268, 682, 552, 988, 530, 875, 53, 224, 107, 594, 806, 858, 673, 501, 43, 183, 1184, 663, 916, 991, 942, 628, 1070, 497, 841, 122, 680, 389, 827, 1073, 274, 352, 1170, 623, 936, 789, 369, 690, 156, 132, 1158, 906, 986, 904, 966, 662, 0, 510, 297, 1206, 72, 764, 253, 338, 121, 182, 290, 720, 564, 812, 636, 700, 736, 1192, 1163, 193, 632, 315, 65, 454, 415, 442, 979, 575, 678, 395, 631, 896, 1145, 381, 505, 444, 1039, 543, 433, 927, 294, 859, 595, 1207, 768, 349, 467, 893, 461, 39, 266, 1019, 603, 1203, 512, 401, 241, 307, 1141, 211, 106, 667, 453, 538, 1012, 561, 1081, 838, 1013, 609, 635, 715, 557, 1139, 520, 1051, 447, 382, 773, 147, 781, 949, 1132, 473, 488, 373, 932, 332, 965, 704, 125, 82, 255, 162, 1126, 259, 460, 40, 180, 920, 756, 972, 377, 654, 883, 56, 160, 1076, 925, 353, 144, 117, 170, 1041, 1069, 375, 161, 546, 345, 308, 322, 250, 613, 469, 555, 30, 355, 554, 529, 1110, 910, 758, 124, 390, 964, 372, 1104, 576, 55, 751, 1026, 835, 479, 647, 598, 952, 45, 583, 18, 419, 687, 890, 1007, 1092, 697, 128, 240, 36, 24, 1052, 655, 648, 176, 192, 509, 309, 115, 634, 365, 887, 428, 348, 1205, 519, 913, 943, 769, 607, 774, 292, 1099, 990, 659, 847, 363, 760, 206, 1004, 1096, 782, 1017, 694, 560, 574, 586, 164, 14, 438, 337, 846, 186, 898, 1038, 219, 1014, 1027, 1016, 665, 103, 50, 172, 204, 477, 998, 296, 1117, 54, 744, 391, 44, 622, 481, 866, 670, 257, 1174, 915, 248, 108, 508, 591, 620, 878, 1209, 1020, 545, 1124, 946, 319, 346, 98, 930, 593, 767, 978, 84, 788, 111, 155, 503, 1177, 606, 602, 516, 12, 417, 660, 48, 1121, 15, 1066, 870, 94, 711, 261, 410, 480, 959, 1118, 1214, 32, 171, 938, 699, 340, 971, 533, 1107, 1144, 967, 1102, 1045, 937, 907, 929, 283, 761, 462, 681, 1119, 873, 539, 166, 945, 7, 275, 1077, 398, 336, 271, 181, 69, 436, 1010, 914, 734, 547, 137, 765, 1160, 216, 1035, 709, 849, 123, 615, 190, 649, 1190, 368, 816, 731, 234, 361, 499, 330, 982, 727, 198, 969, 483, 1182, 422, 209, 324, 683, 59, 1187, 1148, 800, 577, 824, 537, 150, 737, 526, 650, 785, 722, 388, 995, 611, 374, 624, 1101, 664, 712, 227, 1064, 490, 1172, 999, 983, 823, 47, 738, 379, 272, 572, 52, 633, 280, 178, 601, 1030, 175, 1149, 919, 376, 630, 437, 334, 242, 902, 42, 269, 295, 855, 138, 246, 49, 1135, 984, 826, 194, 299, 627, 933, 876, 1115, 1009, 975, 316, 868, 306, 794, 729, 1146, 1196, 431, 1071, 105, 528, 230, 565, 961, 532, 225, 950, 86, 524, 498, 394, 571, 714, 879, 637, 534, 113, 329, 702, 518, 783, 567, 434, 621, 327, 484, 871, 776, 392, 923, 502, 770, 1136, 1085, 807, 486, 1074, 1185, 66, 371, 940, 404, 35, 705, 405, 478, 515, 897, 1113, 385, 909, 1008, 359, 792, 1042, 793, 218, 1090, 1111, 527, 834, 426, 1199, 1181, 808, 513, 1, 1088, 831, 1154, 1195, 92, 917, 1001, 1100, 1002, 455, 270, 281, 341, 2, 440, 1021, 706, 599, 326, 76, 265, 658, 278, 29, 185, 865, 364, 1198, 304, 1089, 1080, 262, 725, 1125, 1028, 941, 1015, 523, 775, 80, 1040, 968, 1208, 189, 796, 605, 215, 1067, 1116, 733, 135, 862, 424, 987, 101, 953, 935, 403, 328, 1152, 863, 771, 46, 177, 343, 491, 195, 814, 133, 1197, 1167, 357, 126, 260, 413, 857, 795, 525, 588, 396, 168, 68, 1150, 550, 612, 1023, 414, 752, 1165, 1063, 19, 556, 256, 196, 141, 726, 93, 1049, 955, 494, 832, 323, 597, 535, 747, 464, 142, 482, 717, 507, 1097, 749, 970, 746, 872, 16, 908, 836, 344, 239, 754, 695, 104, 686, 208, 1078, 912, 638, 220, 291, 880, 421, 818, 1114, 860, 787, 423, 1057, 581, 817, 207, 692, 187, 231, 884, 1083, 317, 693, 342, 1094, 833, 244, 335, 644, 976, 1153, 221, 38, 559, 639, 1131, 367, 143, 407, 888, 1055, 1043, 339, 487, 668, 1179, 544, 1169, 285, 531, 254, 958, 73, 718, 89, 810, 102, 88, 167, 493, 584, 1032, 755, 362, 645, 504, 962, 27, 750, 1053, 64, 954, 70, 899, 614, 4, 10, 629, 815, 1068, 1082, 675, 691, 821, 1018, 71, 472, 446, 803, 514, 474, 506, 640, 400, 608, 708, 466, 1109, 801, 766, 723, 1202, 233, 97, 844, 842, 994, 314, 131, 918, 63, 724, 62, 939, 6, 205, 854, 809, 845, 320, 303, 79, 393, 1204, 1046, 23, 459, 457, 1011, 839, 409, 661, 522, 957, 901, 741, 790, 677, 465, 1112, 1086, 791, 1186, 881, 813, 251, 130, 928, 151, 819, 748, 701, 191, 1133, 666, 1122, 1210, 672, 61, 184, 1166, 1120, 1058, 214, 210, 1188, 777, 685, 848, 616, 1095, 963, 956, 716, 37, 551, 805, 1183, 287, 226, 1147, 77, 279, 669, 864, 641, 739, 310, 26, 900, 153, 58, 542, 119, 1151, 759, 886, 1180, 1156, 996, 924, 592, 703, 60, 236, 653, 1157, 728, 429, 397, 1155, 5, 165, 745, 450, 548, 851, 495, 1033, 179, 229, 671, 1200, 1123]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3496520609485006
the save name prefix for this run is:  chkpt-ID_3496520609485006_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 352
rank avg (pred): 0.445 +- 0.005
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001171485

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.073 +- 0.015
mrr vals (pred, true): 0.007, 0.199
batch losses (mrrl, rdl): 0.0, 8.9131e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.518 +- 0.210
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 3.45469e-05

Epoch over!
epoch time: 15.105

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 917
rank avg (pred): 0.553 +- 0.258
mrr vals (pred, true): 0.002, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002892668

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 209
rank avg (pred): 0.476 +- 0.283
mrr vals (pred, true): 0.026, 0.005
batch losses (mrrl, rdl): 0.0, 5.7687e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1160
rank avg (pred): 0.266 +- 0.227
mrr vals (pred, true): 0.057, 0.031
batch losses (mrrl, rdl): 0.0, 4.76365e-05

Epoch over!
epoch time: 14.871

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.443 +- 0.249
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 0.0, 2.58255e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 481
rank avg (pred): 0.473 +- 0.281
mrr vals (pred, true): 0.053, 0.003
batch losses (mrrl, rdl): 0.0, 5.3202e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 234
rank avg (pred): 0.500 +- 0.315
mrr vals (pred, true): 0.064, 0.004
batch losses (mrrl, rdl): 0.0, 5.6582e-06

Epoch over!
epoch time: 14.877

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.405 +- 0.270
mrr vals (pred, true): 0.073, 0.012
batch losses (mrrl, rdl): 0.0, 4.89883e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 202
rank avg (pred): 0.473 +- 0.293
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0, 2.9504e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 600
rank avg (pred): 0.512 +- 0.299
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0, 1.36813e-05

Epoch over!
epoch time: 14.898

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 634
rank avg (pred): 0.500 +- 0.297
mrr vals (pred, true): 0.083, 0.003
batch losses (mrrl, rdl): 0.0, 1.9078e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.120 +- 0.207
mrr vals (pred, true): 0.123, 0.210
batch losses (mrrl, rdl): 0.0, 9.71489e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 531
rank avg (pred): 0.427 +- 0.271
mrr vals (pred, true): 0.093, 0.007
batch losses (mrrl, rdl): 0.0, 6.8448e-06

Epoch over!
epoch time: 14.858

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 723
rank avg (pred): 0.472 +- 0.285
mrr vals (pred, true): 0.097, 0.004
batch losses (mrrl, rdl): 0.0219823699, 7.2e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 464
rank avg (pred): 0.451 +- 0.195
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 3.6297e-06, 2.91313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 976
rank avg (pred): 0.034 +- 0.019
mrr vals (pred, true): 0.177, 0.291
batch losses (mrrl, rdl): 0.1306346059, 5.7441e-06

Epoch over!
epoch time: 15.197

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 252
rank avg (pred): 0.006 +- 0.003
mrr vals (pred, true): 0.258, 0.283
batch losses (mrrl, rdl): 0.0061031161, 1.79314e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 104
rank avg (pred): 0.481 +- 0.216
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.000203051, 2.4313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.512 +- 0.237
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.86042e-05, 2.52302e-05

Epoch over!
epoch time: 15.132

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 21
rank avg (pred): 0.011 +- 0.007
mrr vals (pred, true): 0.243, 0.245
batch losses (mrrl, rdl): 2.55551e-05, 1.78767e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 957
rank avg (pred): 0.514 +- 0.236
mrr vals (pred, true): 0.038, 0.004
batch losses (mrrl, rdl): 0.0013870997, 3.35639e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.526 +- 0.256
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 1.94594e-05, 3.48163e-05

Epoch over!
epoch time: 15.187

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.070 +- 0.042
mrr vals (pred, true): 0.188, 0.186
batch losses (mrrl, rdl): 3.18461e-05, 7.8622e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.480 +- 0.288
mrr vals (pred, true): 0.054, 0.003
batch losses (mrrl, rdl): 0.0001572688, 5.4034e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 841
rank avg (pred): 0.458 +- 0.296
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 6.6049e-06, 2.502e-05

Epoch over!
epoch time: 15.505

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1
rank avg (pred): 0.015 +- 0.009
mrr vals (pred, true): 0.242, 0.229
batch losses (mrrl, rdl): 0.0017950379, 4.29224e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 921
rank avg (pred): 0.484 +- 0.299
mrr vals (pred, true): 0.055, 0.002
batch losses (mrrl, rdl): 0.0002821868, 0.0003165871

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1147
rank avg (pred): 0.422 +- 0.303
mrr vals (pred, true): 0.056, 0.028
batch losses (mrrl, rdl): 0.0003538069, 0.0005627627

Epoch over!
epoch time: 14.899

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1197
rank avg (pred): 0.522 +- 0.294
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 2.494e-07, 1.64541e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 317
rank avg (pred): 0.061 +- 0.035
mrr vals (pred, true): 0.197, 0.189
batch losses (mrrl, rdl): 0.00062934, 8.6259e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 113
rank avg (pred): 0.504 +- 0.315
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 2.43471e-05, 1.06081e-05

Epoch over!
epoch time: 14.947

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 745
rank avg (pred): 0.067 +- 0.041
mrr vals (pred, true): 0.219, 0.242
batch losses (mrrl, rdl): 0.0053713662, 1.2538e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 982
rank avg (pred): 0.040 +- 0.024
mrr vals (pred, true): 0.235, 0.155
batch losses (mrrl, rdl): 0.0638242364, 3.28405e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 939
rank avg (pred): 0.657 +- 0.255
mrr vals (pred, true): 0.048, 0.002
batch losses (mrrl, rdl): 4.72032e-05, 4.38541e-05

Epoch over!
epoch time: 15.021

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 124
rank avg (pred): 0.543 +- 0.304
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 1.1836e-06, 7.02165e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.424 +- 0.353
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002061062, 6.08834e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1131
rank avg (pred): 0.505 +- 0.313
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 1.8072e-06, 1.02431e-05

Epoch over!
epoch time: 15.017

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.282, 0.239
batch losses (mrrl, rdl): 0.0183891151, 5.86366e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 823
rank avg (pred): 0.035 +- 0.021
mrr vals (pred, true): 0.231, 0.269
batch losses (mrrl, rdl): 0.0141945006, 1.46491e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1203
rank avg (pred): 0.583 +- 0.273
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.55966e-05, 0.0001413802

Epoch over!
epoch time: 14.988

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.555 +- 0.274
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 3.2576e-06, 5.46629e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.149 +- 0.086
mrr vals (pred, true): 0.195, 0.184
batch losses (mrrl, rdl): 0.0013765795, 0.0001233835

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 263
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.326, 0.350
batch losses (mrrl, rdl): 0.0056954604, 1.68566e-05

Epoch over!
epoch time: 15.103

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.496 +- 0.292
mrr vals (pred, true): 0.052, 0.005

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.05017 	 0.00092 	 m..s
   48 	     1 	 0.05091 	 0.00131 	 m..s
   62 	     2 	 0.05169 	 0.00208 	 m..s
   62 	     3 	 0.05169 	 0.00213 	 m..s
   83 	     4 	 0.05380 	 0.00241 	 m..s
   60 	     5 	 0.05164 	 0.00242 	 m..s
   74 	     6 	 0.05247 	 0.00267 	 m..s
   70 	     7 	 0.05209 	 0.00282 	 m..s
   50 	     8 	 0.05097 	 0.00282 	 m..s
   56 	     9 	 0.05134 	 0.00288 	 m..s
   51 	    10 	 0.05099 	 0.00304 	 m..s
   12 	    11 	 0.05002 	 0.00313 	 m..s
   38 	    12 	 0.05060 	 0.00315 	 m..s
   70 	    13 	 0.05209 	 0.00317 	 m..s
   23 	    14 	 0.05025 	 0.00324 	 m..s
   43 	    15 	 0.05068 	 0.00327 	 m..s
   30 	    16 	 0.05051 	 0.00331 	 m..s
    7 	    17 	 0.04983 	 0.00334 	 m..s
   28 	    18 	 0.05033 	 0.00343 	 m..s
   14 	    19 	 0.05009 	 0.00343 	 m..s
   77 	    20 	 0.05276 	 0.00354 	 m..s
   64 	    21 	 0.05172 	 0.00355 	 m..s
   38 	    22 	 0.05060 	 0.00363 	 m..s
   82 	    23 	 0.05371 	 0.00363 	 m..s
   36 	    24 	 0.05058 	 0.00364 	 m..s
   79 	    25 	 0.05285 	 0.00366 	 m..s
    9 	    26 	 0.04988 	 0.00370 	 m..s
   40 	    27 	 0.05062 	 0.00371 	 m..s
   22 	    28 	 0.05025 	 0.00372 	 m..s
   32 	    29 	 0.05051 	 0.00373 	 m..s
   55 	    30 	 0.05129 	 0.00373 	 m..s
   53 	    31 	 0.05121 	 0.00376 	 m..s
   57 	    32 	 0.05145 	 0.00385 	 m..s
   11 	    33 	 0.04999 	 0.00386 	 m..s
   81 	    34 	 0.05346 	 0.00387 	 m..s
   35 	    35 	 0.05057 	 0.00388 	 m..s
   13 	    36 	 0.05009 	 0.00392 	 m..s
   33 	    37 	 0.05051 	 0.00394 	 m..s
   24 	    38 	 0.05031 	 0.00400 	 m..s
   40 	    39 	 0.05062 	 0.00405 	 m..s
   46 	    40 	 0.05086 	 0.00410 	 m..s
   10 	    41 	 0.04993 	 0.00410 	 m..s
   34 	    42 	 0.05057 	 0.00413 	 m..s
   52 	    43 	 0.05115 	 0.00413 	 m..s
   85 	    44 	 0.05419 	 0.00415 	 m..s
    8 	    45 	 0.04983 	 0.00416 	 m..s
   42 	    46 	 0.05065 	 0.00421 	 m..s
   37 	    47 	 0.05059 	 0.00422 	 m..s
   21 	    48 	 0.05023 	 0.00424 	 m..s
    4 	    49 	 0.04975 	 0.00428 	 m..s
   44 	    50 	 0.05071 	 0.00428 	 m..s
   75 	    51 	 0.05260 	 0.00428 	 m..s
   26 	    52 	 0.05032 	 0.00431 	 m..s
   65 	    53 	 0.05194 	 0.00434 	 m..s
   45 	    54 	 0.05075 	 0.00438 	 m..s
   18 	    55 	 0.05019 	 0.00438 	 m..s
   53 	    56 	 0.05121 	 0.00441 	 m..s
   72 	    57 	 0.05221 	 0.00447 	 m..s
   49 	    58 	 0.05097 	 0.00448 	 m..s
    3 	    59 	 0.04973 	 0.00451 	 m..s
   47 	    60 	 0.05086 	 0.00452 	 m..s
   80 	    61 	 0.05287 	 0.00453 	 m..s
   26 	    62 	 0.05032 	 0.00455 	 m..s
   69 	    63 	 0.05204 	 0.00455 	 m..s
   29 	    64 	 0.05035 	 0.00461 	 m..s
   76 	    65 	 0.05267 	 0.00461 	 m..s
   25 	    66 	 0.05031 	 0.00469 	 m..s
   67 	    67 	 0.05198 	 0.00475 	 m..s
   78 	    68 	 0.05280 	 0.00477 	 m..s
   66 	    69 	 0.05196 	 0.00489 	 m..s
   58 	    70 	 0.05154 	 0.00490 	 m..s
   19 	    71 	 0.05023 	 0.00492 	 m..s
    4 	    72 	 0.04975 	 0.00492 	 m..s
   59 	    73 	 0.05155 	 0.00496 	 m..s
   19 	    74 	 0.05023 	 0.00507 	 m..s
   16 	    75 	 0.05012 	 0.00508 	 m..s
   73 	    76 	 0.05234 	 0.00516 	 m..s
   84 	    77 	 0.05402 	 0.00529 	 m..s
    6 	    78 	 0.04976 	 0.00533 	 m..s
   68 	    79 	 0.05199 	 0.00548 	 m..s
   30 	    80 	 0.05051 	 0.00558 	 m..s
    0 	    81 	 0.04878 	 0.00729 	 m..s
    0 	    82 	 0.04878 	 0.01020 	 m..s
    2 	    83 	 0.04929 	 0.01271 	 m..s
   86 	    84 	 0.05733 	 0.02017 	 m..s
   90 	    85 	 0.07888 	 0.02355 	 m..s
   61 	    86 	 0.05167 	 0.02916 	 ~...
   89 	    87 	 0.07795 	 0.02942 	 m..s
   87 	    88 	 0.06230 	 0.03090 	 m..s
   88 	    89 	 0.07522 	 0.03153 	 m..s
   15 	    90 	 0.05012 	 0.03475 	 ~...
  104 	    91 	 0.23881 	 0.07075 	 MISS
   92 	    92 	 0.14366 	 0.10894 	 m..s
   94 	    93 	 0.14672 	 0.12593 	 ~...
   91 	    94 	 0.14287 	 0.13317 	 ~...
   93 	    95 	 0.14446 	 0.13713 	 ~...
   99 	    96 	 0.19364 	 0.17271 	 ~...
   95 	    97 	 0.19063 	 0.17434 	 ~...
   96 	    98 	 0.19146 	 0.17489 	 ~...
   98 	    99 	 0.19254 	 0.17594 	 ~...
   96 	   100 	 0.19146 	 0.18432 	 ~...
  103 	   101 	 0.22872 	 0.18489 	 m..s
  100 	   102 	 0.19504 	 0.18924 	 ~...
  101 	   103 	 0.20074 	 0.19121 	 ~...
  114 	   104 	 0.26169 	 0.19343 	 m..s
  108 	   105 	 0.24660 	 0.21046 	 m..s
  102 	   106 	 0.22673 	 0.21874 	 ~...
  108 	   107 	 0.24660 	 0.26016 	 ~...
  104 	   108 	 0.23881 	 0.26176 	 ~...
  106 	   109 	 0.24470 	 0.26363 	 ~...
  117 	   110 	 0.26985 	 0.26660 	 ~...
  106 	   111 	 0.24470 	 0.27019 	 ~...
  119 	   112 	 0.27061 	 0.27192 	 ~...
  117 	   113 	 0.26985 	 0.27506 	 ~...
  120 	   114 	 0.27507 	 0.28961 	 ~...
  111 	   115 	 0.25590 	 0.30213 	 m..s
  116 	   116 	 0.26814 	 0.31362 	 m..s
  110 	   117 	 0.24778 	 0.31382 	 m..s
  113 	   118 	 0.26120 	 0.31687 	 m..s
  115 	   119 	 0.26760 	 0.32046 	 m..s
  111 	   120 	 0.25590 	 0.33382 	 m..s
==========================================
r_mrr = 0.9754435420036316
r2_mrr = 0.7888909578323364
spearmanr_mrr@5 = 0.9719467759132385
spearmanr_mrr@10 = 0.9778348207473755
spearmanr_mrr@50 = 0.9840537309646606
spearmanr_mrr@100 = 0.9904754161834717
spearmanr_mrr@All = 0.9911209940910339
==========================================
test time: 0.457
Done Testing dataset CoDExSmall
total time taken: 237.0117266178131
training time taken: 226.09188413619995
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9754)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7889)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9719)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9778)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9841)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9905)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9911)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.7273504749919084}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 2420341342983926
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [679, 990, 369, 661, 553, 1146, 84, 284, 904, 747, 780, 460, 738, 26, 809, 230, 576, 354, 70, 366, 228, 1148, 482, 919, 439, 17, 473, 501, 942, 424, 529, 1018, 465, 160, 364, 1073, 438, 595, 155, 36, 678, 640, 630, 244, 798, 719, 98, 1103, 820, 539, 892, 340, 1039, 64, 229, 750, 390, 1205, 289, 1017, 1186, 884, 819, 388, 619, 607, 523, 475, 412, 433, 227, 664, 181, 535, 158, 1206, 561, 857, 73, 916, 178, 177, 66, 1072, 562, 866, 319, 1139, 832, 927, 1036, 614, 579, 941, 736, 908, 347, 620, 20, 715, 874, 1150, 127, 945, 1050, 195, 437, 694, 1159, 1091, 700, 1200, 257, 786, 1001, 954, 647, 291, 1154, 662, 898]
valid_ids (0): []
train_ids (1094): [2, 889, 274, 1211, 546, 888, 45, 303, 756, 410, 506, 573, 342, 753, 850, 211, 235, 329, 324, 1048, 762, 8, 140, 758, 191, 1096, 174, 748, 1006, 415, 852, 43, 102, 687, 1095, 720, 1214, 1174, 639, 253, 1077, 1011, 914, 1057, 759, 844, 1162, 1031, 1142, 681, 30, 906, 938, 723, 24, 785, 1136, 494, 571, 1016, 853, 701, 418, 318, 478, 778, 1168, 7, 769, 655, 180, 1010, 430, 245, 1172, 481, 685, 445, 559, 240, 721, 332, 400, 466, 952, 810, 830, 1093, 575, 602, 1165, 773, 962, 497, 770, 987, 746, 622, 48, 802, 527, 1191, 569, 761, 991, 827, 627, 254, 1177, 733, 507, 674, 1185, 368, 1104, 22, 122, 725, 1198, 760, 335, 545, 842, 416, 57, 1175, 406, 200, 1082, 262, 1084, 428, 273, 1098, 534, 966, 757, 735, 979, 896, 429, 551, 833, 40, 683, 548, 493, 584, 514, 741, 372, 714, 610, 643, 359, 212, 525, 1028, 978, 1060, 395, 787, 293, 1021, 246, 305, 742, 768, 170, 310, 1130, 440, 772, 444, 54, 1117, 864, 880, 811, 1065, 1047, 463, 642, 729, 184, 104, 456, 164, 280, 1079, 782, 248, 986, 1080, 1081, 617, 210, 929, 153, 91, 1045, 1128, 116, 1101, 763, 500, 988, 146, 74, 901, 88, 1030, 172, 931, 960, 333, 1090, 670, 969, 509, 25, 206, 781, 1199, 1135, 1023, 666, 32, 1094, 358, 1149, 265, 251, 441, 696, 790, 499, 949, 590, 600, 895, 570, 1169, 963, 824, 147, 636, 189, 691, 1078, 225, 276, 847, 815, 800, 615, 414, 812, 886, 572, 76, 495, 933, 68, 1127, 789, 498, 910, 754, 1153, 130, 269, 951, 1112, 1055, 258, 699, 1062, 612, 1190, 867, 281, 185, 11, 726, 169, 1075, 709, 261, 677, 50, 1015, 637, 110, 621, 817, 651, 601, 337, 652, 260, 672, 1120, 134, 1115, 932, 14, 542, 379, 151, 512, 123, 879, 85, 1170, 698, 1070, 788, 654, 835, 86, 304, 256, 975, 377, 1167, 518, 1107, 111, 634, 183, 771, 394, 710, 680, 313, 1033, 435, 79, 314, 344, 957, 426, 224, 1106, 1187, 31, 152, 108, 290, 90, 1194, 922, 327, 878, 505, 541, 1020, 234, 799, 450, 718, 480, 249, 1183, 80, 479, 974, 154, 1197, 1076, 383, 939, 851, 336, 722, 21, 131, 427, 792, 1019, 618, 166, 490, 1192, 483, 97, 774, 531, 401, 425, 357, 51, 209, 893, 259, 1181, 971, 688, 900, 202, 348, 1066, 1176, 46, 156, 215, 955, 667, 606, 1126, 1184, 221, 326, 382, 536, 204, 716, 1042, 1195, 83, 538, 398, 1013, 352, 838, 345, 924, 232, 380, 496, 1118, 1067, 845, 145, 135, 568, 766, 1124, 320, 1151, 608, 707, 386, 555, 1038, 985, 243, 1007, 277, 485, 133, 403, 476, 1009, 669, 373, 176, 1171, 409, 1051, 854, 549, 188, 1212, 1125, 744, 378, 141, 157, 190, 411, 1123, 375, 219, 961, 948, 998, 532, 72, 58, 457, 795, 580, 885, 784, 1049, 1041, 983, 624, 99, 384, 605, 75, 791, 558, 508, 165, 1032, 641, 448, 503, 374, 724, 905, 77, 980, 93, 663, 537, 404, 585, 238, 587, 162, 935, 1110, 717, 711, 182, 764, 12, 826, 692, 462, 848, 192, 873, 899, 363, 511, 139, 890, 689, 3, 422, 861, 1012, 806, 338, 739, 740, 37, 656, 446, 413, 323, 168, 351, 282, 1178, 855, 1054, 252, 1140, 526, 61, 1005, 126, 632, 604, 519, 350, 515, 1014, 603, 339, 517, 442, 1069, 81, 330, 150, 793, 1022, 920, 592, 921, 676, 103, 1152, 33, 829, 510, 504, 649, 78, 263, 207, 272, 217, 704, 436, 860, 1074, 299, 897, 596, 28, 887, 241, 682, 846, 658, 408, 1046, 447, 836, 956, 566, 593, 599, 469, 684, 730, 994, 316, 312, 965, 950, 63, 356, 5, 1003, 1133, 461, 1026, 959, 560, 1143, 449, 114, 19, 106, 367, 278, 918, 745, 397, 849, 1111, 673, 911, 236, 547, 1088, 361, 1155, 301, 112, 453, 1008, 266, 831, 113, 1083, 226, 977, 638, 218, 891, 626, 10, 591, 633, 1043, 1132, 311, 371, 894, 296, 1207, 431, 976, 1040, 13, 1037, 743, 841, 92, 69, 1109, 513, 34, 734, 247, 1059, 993, 392, 953, 283, 279, 486, 1163, 47, 816, 389, 87, 923, 35, 59, 370, 958, 1138, 402, 109, 628, 242, 376, 1134, 455, 1092, 161, 1102, 1158, 477, 1114, 65, 623, 233, 1085, 946, 871, 173, 343, 583, 671, 997, 749, 727, 15, 999, 365, 285, 1161, 9, 1052, 1208, 464, 972, 752, 470, 645, 1156, 533, 44, 930, 95, 737, 903, 391, 292, 875, 1193, 1, 1204, 52, 399, 909, 947, 657, 62, 1210, 474, 865, 297, 82, 144, 563, 487, 616, 41, 859, 1116, 697, 804, 288, 1027, 1113, 915, 71, 137, 564, 100, 237, 712, 471, 163, 231, 1119, 668, 353, 936, 708, 271, 216, 315, 765, 1145, 635, 322, 355, 360, 222, 925, 205, 818, 713, 796, 143, 825, 167, 629, 834, 94, 298, 1100, 89, 1188, 443, 201, 877, 522, 1201, 186, 872, 268, 926, 928, 1108, 912, 937, 4, 843, 705, 943, 1180, 970, 828, 732, 863, 693, 862, 597, 837, 489, 107, 49, 1000, 1196, 124, 574, 451, 528, 286, 115, 60, 581, 554, 309, 213, 128, 1122, 1173, 989, 16, 840, 1129, 516, 1087, 609, 302, 194, 492, 653, 944, 706, 675, 992, 940, 644, 776, 856, 1157, 419, 432, 1137, 660, 421, 270, 405, 813, 565, 755, 808, 917, 396, 148, 582, 648, 393, 530, 119, 779, 349, 613, 488, 196, 1179, 540, 132, 556, 594, 794, 346, 598, 381, 870, 702, 120, 807, 821, 417, 6, 686, 220, 27, 387, 214, 868, 287, 611, 179, 967, 934, 858, 331, 902, 588, 1189, 423, 193, 458, 117, 1035, 39, 767, 1029, 1097, 1024, 552, 121, 472, 0, 665, 198, 1121, 484, 996, 783, 175, 334, 1002, 23, 881, 321, 29, 1147, 275, 454, 203, 308, 1166, 521, 984, 1203, 1064, 328, 728, 325, 1141, 1089, 317, 208, 307, 982, 159, 1044, 839, 913, 199, 968, 407, 907, 101, 125, 1144, 1131, 187, 543, 520, 797, 129, 149, 544, 524, 577, 1058, 300, 171, 142, 589, 197, 650, 659, 690, 468, 306, 239, 491, 1099, 459, 995, 695, 805, 105, 1071, 646, 751, 56, 1004, 42, 586, 1025, 731, 67, 775, 38, 703, 1034, 420, 250, 964, 876, 1182, 294, 801, 822, 1068, 1209, 578, 467, 557, 18, 452, 1160, 823, 803, 118, 981, 1086, 362, 567, 96, 814, 53, 1105, 264, 136, 1056, 55, 502, 1053, 973, 295, 434, 267, 625, 1202, 255, 550, 1213, 1061, 1063, 777, 882, 385, 223, 341, 1164, 138, 631, 883, 869]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5994404721095290
the save name prefix for this run is:  chkpt-ID_5994404721095290_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 44
rank avg (pred): 0.520 +- 0.012
mrr vals (pred, true): 0.001, 0.142
batch losses (mrrl, rdl): 0.0, 0.0041585355

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.453 +- 0.285
mrr vals (pred, true): 0.064, 0.003
batch losses (mrrl, rdl): 0.0, 1.05043e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 316
rank avg (pred): 0.045 +- 0.030
mrr vals (pred, true): 0.133, 0.175
batch losses (mrrl, rdl): 0.0, 2.82624e-05

Epoch over!
epoch time: 15.019

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.428 +- 0.277
mrr vals (pred, true): 0.079, 0.004
batch losses (mrrl, rdl): 0.0, 8.4095e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 601
rank avg (pred): 0.469 +- 0.261
mrr vals (pred, true): 0.022, 0.004
batch losses (mrrl, rdl): 0.0, 7.6639e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 716
rank avg (pred): 0.454 +- 0.278
mrr vals (pred, true): 0.029, 0.003
batch losses (mrrl, rdl): 0.0, 5.573e-06

Epoch over!
epoch time: 14.939

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.064 +- 0.041
mrr vals (pred, true): 0.075, 0.177
batch losses (mrrl, rdl): 0.0, 8.778e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 862
rank avg (pred): 0.485 +- 0.273
mrr vals (pred, true): 0.016, 0.004
batch losses (mrrl, rdl): 0.0, 8.5001e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1089
rank avg (pred): 0.458 +- 0.274
mrr vals (pred, true): 0.026, 0.004
batch losses (mrrl, rdl): 0.0, 5.3386e-06

Epoch over!
epoch time: 14.894

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1111
rank avg (pred): 0.453 +- 0.261
mrr vals (pred, true): 0.028, 0.004
batch losses (mrrl, rdl): 0.0, 5.5003e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.451 +- 0.270
mrr vals (pred, true): 0.021, 0.006
batch losses (mrrl, rdl): 0.0, 4.7356e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 961
rank avg (pred): 0.522 +- 0.304
mrr vals (pred, true): 0.028, 0.003
batch losses (mrrl, rdl): 0.0, 7.50721e-05

Epoch over!
epoch time: 14.721

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 269
rank avg (pred): 0.054 +- 0.034
mrr vals (pred, true): 0.067, 0.350
batch losses (mrrl, rdl): 0.0, 1.11725e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.461 +- 0.272
mrr vals (pred, true): 0.014, 0.004
batch losses (mrrl, rdl): 0.0, 3.0006e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 706
rank avg (pred): 0.472 +- 0.269
mrr vals (pred, true): 0.009, 0.003
batch losses (mrrl, rdl): 0.0, 3.3446e-06

Epoch over!
epoch time: 14.71

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 48
rank avg (pred): 0.076 +- 0.056
mrr vals (pred, true): 0.081, 0.132
batch losses (mrrl, rdl): 0.0266513508, 9.8443e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.025 +- 0.016
mrr vals (pred, true): 0.203, 0.232
batch losses (mrrl, rdl): 0.0081466939, 1.32699e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 41
rank avg (pred): 0.254 +- 0.160
mrr vals (pred, true): 0.122, 0.143
batch losses (mrrl, rdl): 0.0045038764, 0.0006756148

Epoch over!
epoch time: 14.921

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 410
rank avg (pred): 0.486 +- 0.171
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 3.32044e-05, 5.72979e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 341
rank avg (pred): 0.488 +- 0.159
mrr vals (pred, true): 0.047, 0.003
batch losses (mrrl, rdl): 6.4179e-05, 6.012e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.487 +- 0.149
mrr vals (pred, true): 0.049, 0.023
batch losses (mrrl, rdl): 3.2453e-06, 0.0001666715

Epoch over!
epoch time: 14.922

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1080
rank avg (pred): 0.481 +- 0.152
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.95698e-05, 5.67965e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.457 +- 0.166
mrr vals (pred, true): 0.057, 0.012
batch losses (mrrl, rdl): 0.0005352223, 0.000230328

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.465 +- 0.146
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 1.63534e-05, 5.39375e-05

Epoch over!
epoch time: 14.938

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 193
rank avg (pred): 0.471 +- 0.144
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 3.861e-07, 6.00971e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.494 +- 0.158
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 8.264e-07, 5.40423e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1094
rank avg (pred): 0.465 +- 0.139
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.394e-07, 5.90349e-05

Epoch over!
epoch time: 14.966

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.135 +- 0.089
mrr vals (pred, true): 0.179, 0.174
batch losses (mrrl, rdl): 0.0002145428, 8.24932e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1172
rank avg (pred): 0.451 +- 0.156
mrr vals (pred, true): 0.056, 0.003
batch losses (mrrl, rdl): 0.0003863932, 5.80967e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 654
rank avg (pred): 0.499 +- 0.134
mrr vals (pred, true): 0.045, 0.005
batch losses (mrrl, rdl): 0.000206597, 7.72844e-05

Epoch over!
epoch time: 14.952

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 69
rank avg (pred): 0.197 +- 0.113
mrr vals (pred, true): 0.135, 0.126
batch losses (mrrl, rdl): 0.0008029164, 0.0002858913

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 769
rank avg (pred): 0.479 +- 0.153
mrr vals (pred, true): 0.053, 0.003
batch losses (mrrl, rdl): 9.54901e-05, 4.23859e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 705
rank avg (pred): 0.452 +- 0.138
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002321624, 7.08198e-05

Epoch over!
epoch time: 15.076

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 542
rank avg (pred): 0.409 +- 0.141
mrr vals (pred, true): 0.055, 0.012
batch losses (mrrl, rdl): 0.0002216515, 8.12523e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.516 +- 0.141
mrr vals (pred, true): 0.044, 0.005
batch losses (mrrl, rdl): 0.0004064675, 9.18135e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 492
rank avg (pred): 0.429 +- 0.119
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 2.37975e-05, 0.0004587141

Epoch over!
epoch time: 15.055

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 294
rank avg (pred): 0.130 +- 0.083
mrr vals (pred, true): 0.186, 0.172
batch losses (mrrl, rdl): 0.0020325903, 5.90172e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 466
rank avg (pred): 0.483 +- 0.144
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 3.68906e-05, 5.81761e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1211
rank avg (pred): 0.445 +- 0.151
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0005376621, 8.3107e-05

Epoch over!
epoch time: 15.173

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.298, 0.275
batch losses (mrrl, rdl): 0.0052101747, 1.23801e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 250
rank avg (pred): 0.072 +- 0.051
mrr vals (pred, true): 0.261, 0.289
batch losses (mrrl, rdl): 0.0081062801, 2.58351e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 485
rank avg (pred): 0.457 +- 0.144
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 2.96296e-05, 7.46133e-05

Epoch over!
epoch time: 15.25

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 295
rank avg (pred): 0.142 +- 0.094
mrr vals (pred, true): 0.208, 0.179
batch losses (mrrl, rdl): 0.008803823, 9.79556e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 142
rank avg (pred): 0.498 +- 0.142
mrr vals (pred, true): 0.039, 0.005
batch losses (mrrl, rdl): 0.0012285286, 6.18512e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 122
rank avg (pred): 0.492 +- 0.158
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 7.4937e-06, 5.91055e-05

Epoch over!
epoch time: 15.125

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.471 +- 0.157
mrr vals (pred, true): 0.052, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04112 	 0.00086 	 m..s
    5 	     1 	 0.04139 	 0.00088 	 m..s
   37 	     2 	 0.04862 	 0.00102 	 m..s
    0 	     3 	 0.03714 	 0.00107 	 m..s
    1 	     4 	 0.03729 	 0.00151 	 m..s
   13 	     5 	 0.04511 	 0.00194 	 m..s
   63 	     6 	 0.05067 	 0.00241 	 m..s
   60 	     7 	 0.05052 	 0.00241 	 m..s
    3 	     8 	 0.03989 	 0.00263 	 m..s
   15 	     9 	 0.04601 	 0.00280 	 m..s
   12 	    10 	 0.04278 	 0.00290 	 m..s
   10 	    11 	 0.04273 	 0.00292 	 m..s
   21 	    12 	 0.04678 	 0.00300 	 m..s
   10 	    13 	 0.04273 	 0.00301 	 m..s
    8 	    14 	 0.04242 	 0.00301 	 m..s
   30 	    15 	 0.04799 	 0.00310 	 m..s
   74 	    16 	 0.05207 	 0.00322 	 m..s
   78 	    17 	 0.05214 	 0.00324 	 m..s
   69 	    18 	 0.05112 	 0.00335 	 m..s
   65 	    19 	 0.05080 	 0.00338 	 m..s
   18 	    20 	 0.04669 	 0.00338 	 m..s
   51 	    21 	 0.05003 	 0.00340 	 m..s
   67 	    22 	 0.05108 	 0.00341 	 m..s
   40 	    23 	 0.04929 	 0.00343 	 m..s
    7 	    24 	 0.04237 	 0.00345 	 m..s
   62 	    25 	 0.05062 	 0.00348 	 m..s
   16 	    26 	 0.04622 	 0.00351 	 m..s
   14 	    27 	 0.04598 	 0.00351 	 m..s
   75 	    28 	 0.05211 	 0.00354 	 m..s
   52 	    29 	 0.05005 	 0.00361 	 m..s
   43 	    30 	 0.04945 	 0.00362 	 m..s
   70 	    31 	 0.05134 	 0.00362 	 m..s
    2 	    32 	 0.03965 	 0.00366 	 m..s
   41 	    33 	 0.04930 	 0.00371 	 m..s
   64 	    34 	 0.05068 	 0.00371 	 m..s
   73 	    35 	 0.05206 	 0.00372 	 m..s
   83 	    36 	 0.05246 	 0.00377 	 m..s
   91 	    37 	 0.05590 	 0.00381 	 m..s
   54 	    38 	 0.05009 	 0.00382 	 m..s
   68 	    39 	 0.05112 	 0.00387 	 m..s
   76 	    40 	 0.05213 	 0.00387 	 m..s
   29 	    41 	 0.04781 	 0.00389 	 m..s
   72 	    42 	 0.05170 	 0.00391 	 m..s
   46 	    43 	 0.04968 	 0.00395 	 m..s
   93 	    44 	 0.05703 	 0.00396 	 m..s
   76 	    45 	 0.05213 	 0.00400 	 m..s
   59 	    46 	 0.05043 	 0.00404 	 m..s
    6 	    47 	 0.04233 	 0.00408 	 m..s
   26 	    48 	 0.04746 	 0.00413 	 m..s
   85 	    49 	 0.05251 	 0.00418 	 m..s
   32 	    50 	 0.04818 	 0.00419 	 m..s
   17 	    51 	 0.04643 	 0.00423 	 m..s
   36 	    52 	 0.04850 	 0.00429 	 m..s
   81 	    53 	 0.05236 	 0.00430 	 m..s
   49 	    54 	 0.04985 	 0.00431 	 m..s
   19 	    55 	 0.04674 	 0.00433 	 m..s
   32 	    56 	 0.04818 	 0.00435 	 m..s
   39 	    57 	 0.04910 	 0.00436 	 m..s
   78 	    58 	 0.05214 	 0.00436 	 m..s
   50 	    59 	 0.04986 	 0.00444 	 m..s
   19 	    60 	 0.04674 	 0.00445 	 m..s
   55 	    61 	 0.05012 	 0.00449 	 m..s
   38 	    62 	 0.04880 	 0.00453 	 m..s
   58 	    63 	 0.05042 	 0.00455 	 m..s
   80 	    64 	 0.05231 	 0.00463 	 m..s
   47 	    65 	 0.04972 	 0.00465 	 m..s
   48 	    66 	 0.04981 	 0.00471 	 m..s
   82 	    67 	 0.05244 	 0.00472 	 m..s
   66 	    68 	 0.05099 	 0.00475 	 m..s
   41 	    69 	 0.04930 	 0.00484 	 m..s
   32 	    70 	 0.04818 	 0.00491 	 m..s
   56 	    71 	 0.05033 	 0.00493 	 m..s
   45 	    72 	 0.04961 	 0.00494 	 m..s
   31 	    73 	 0.04816 	 0.00498 	 m..s
   44 	    74 	 0.04957 	 0.00501 	 m..s
   35 	    75 	 0.04845 	 0.00504 	 m..s
    9 	    76 	 0.04269 	 0.00510 	 m..s
   53 	    77 	 0.05006 	 0.00518 	 m..s
   61 	    78 	 0.05055 	 0.00519 	 m..s
   22 	    79 	 0.04700 	 0.00524 	 m..s
   28 	    80 	 0.04780 	 0.00548 	 m..s
   57 	    81 	 0.05042 	 0.00549 	 m..s
   84 	    82 	 0.05250 	 0.00551 	 m..s
   24 	    83 	 0.04738 	 0.00571 	 m..s
   90 	    84 	 0.05500 	 0.00656 	 m..s
   24 	    85 	 0.04738 	 0.00656 	 m..s
   27 	    86 	 0.04774 	 0.00661 	 m..s
   23 	    87 	 0.04734 	 0.00680 	 m..s
   88 	    88 	 0.05461 	 0.00739 	 m..s
   89 	    89 	 0.05474 	 0.00815 	 m..s
   96 	    90 	 0.06450 	 0.02050 	 m..s
   95 	    91 	 0.06215 	 0.02455 	 m..s
   94 	    92 	 0.06154 	 0.02854 	 m..s
   92 	    93 	 0.05645 	 0.02856 	 ~...
   86 	    94 	 0.05412 	 0.03020 	 ~...
   87 	    95 	 0.05416 	 0.03090 	 ~...
   71 	    96 	 0.05137 	 0.04398 	 ~...
   97 	    97 	 0.11680 	 0.06382 	 m..s
  101 	    98 	 0.12928 	 0.12460 	 ~...
   98 	    99 	 0.12382 	 0.12873 	 ~...
   99 	   100 	 0.12736 	 0.13140 	 ~...
  102 	   101 	 0.13145 	 0.13846 	 ~...
  100 	   102 	 0.12859 	 0.14186 	 ~...
  109 	   103 	 0.19244 	 0.16128 	 m..s
  103 	   104 	 0.17601 	 0.17117 	 ~...
  105 	   105 	 0.17893 	 0.17271 	 ~...
  104 	   106 	 0.17704 	 0.17731 	 ~...
  108 	   107 	 0.18875 	 0.18497 	 ~...
  107 	   108 	 0.18744 	 0.18551 	 ~...
  112 	   109 	 0.21088 	 0.19541 	 ~...
  111 	   110 	 0.21084 	 0.19542 	 ~...
  110 	   111 	 0.19836 	 0.20060 	 ~...
  106 	   112 	 0.18306 	 0.20961 	 ~...
  117 	   113 	 0.24077 	 0.24311 	 ~...
  116 	   114 	 0.24064 	 0.24983 	 ~...
  118 	   115 	 0.25865 	 0.25585 	 ~...
  114 	   116 	 0.22692 	 0.26016 	 m..s
  113 	   117 	 0.21984 	 0.26096 	 m..s
  115 	   118 	 0.22932 	 0.27019 	 m..s
  119 	   119 	 0.27965 	 0.29833 	 ~...
  120 	   120 	 0.28350 	 0.33494 	 m..s
==========================================
r_mrr = 0.9909289479255676
r2_mrr = 0.7533255815505981
spearmanr_mrr@5 = 0.9148862361907959
spearmanr_mrr@10 = 0.9448074698448181
spearmanr_mrr@50 = 0.9935598373413086
spearmanr_mrr@100 = 0.9954349994659424
spearmanr_mrr@All = 0.9952071905136108
==========================================
test time: 0.449
Done Testing dataset CoDExSmall
total time taken: 235.86751246452332
training time taken: 225.122563123703
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9909)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7533)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9149)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9448)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9936)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9954)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9952)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.18117276266639237}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 4674981592856998
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1094, 455, 634, 32, 805, 730, 1050, 521, 278, 548, 268, 582, 594, 1174, 282, 985, 835, 649, 437, 457, 247, 188, 16, 952, 321, 855, 839, 1056, 734, 223, 930, 957, 1128, 625, 754, 1171, 945, 253, 1061, 80, 1175, 490, 655, 688, 811, 436, 320, 769, 452, 748, 774, 23, 1083, 1199, 825, 962, 991, 1054, 210, 1036, 19, 803, 653, 54, 943, 219, 259, 164, 102, 1135, 326, 145, 135, 1091, 143, 648, 37, 737, 224, 402, 958, 609, 872, 875, 533, 194, 349, 412, 818, 876, 144, 126, 1210, 715, 520, 561, 48, 979, 44, 1071, 617, 392, 1129, 106, 1102, 687, 168, 63, 492, 611, 502, 200, 903, 67, 1108, 1185, 824, 963, 986, 852, 701]
valid_ids (0): []
train_ids (1094): [242, 505, 564, 707, 435, 186, 311, 937, 865, 211, 421, 927, 178, 571, 418, 801, 753, 451, 526, 131, 196, 886, 447, 104, 1213, 971, 1113, 1006, 516, 692, 420, 816, 1131, 263, 355, 397, 14, 81, 1153, 831, 430, 140, 553, 411, 303, 925, 458, 351, 363, 93, 929, 1138, 1205, 432, 993, 907, 842, 42, 125, 1028, 1200, 1162, 751, 236, 433, 550, 21, 1154, 951, 348, 319, 1012, 1003, 441, 731, 239, 959, 75, 1074, 1045, 1209, 1164, 938, 503, 598, 1189, 782, 644, 174, 454, 255, 1151, 820, 563, 86, 627, 25, 271, 884, 95, 1157, 328, 39, 325, 1019, 1063, 1041, 79, 777, 861, 274, 2, 926, 470, 792, 132, 877, 916, 269, 826, 409, 403, 758, 723, 1110, 1156, 551, 356, 542, 401, 335, 700, 58, 92, 755, 440, 148, 66, 983, 973, 1053, 127, 1097, 515, 17, 666, 339, 620, 994, 336, 950, 804, 786, 394, 874, 49, 797, 785, 672, 531, 491, 212, 36, 696, 918, 873, 361, 374, 385, 89, 362, 749, 84, 391, 1166, 756, 152, 1121, 765, 301, 1069, 387, 159, 915, 346, 76, 1072, 989, 449, 988, 133, 156, 752, 3, 358, 583, 848, 545, 1000, 1095, 966, 379, 60, 371, 307, 844, 225, 608, 718, 243, 373, 1105, 7, 558, 287, 750, 539, 911, 31, 171, 395, 712, 262, 713, 1120, 234, 899, 427, 389, 469, 162, 535, 249, 673, 509, 1186, 961, 96, 431, 297, 182, 1158, 261, 726, 497, 97, 784, 871, 709, 15, 475, 536, 1117, 512, 357, 1150, 773, 1109, 540, 187, 960, 575, 496, 322, 481, 460, 87, 300, 304, 645, 1192, 290, 313, 256, 474, 632, 1112, 921, 172, 283, 1184, 552, 935, 1015, 947, 114, 725, 849, 612, 997, 870, 13, 312, 549, 1084, 1143, 1134, 103, 1139, 710, 636, 1132, 1089, 791, 264, 588, 1022, 488, 352, 1170, 9, 590, 891, 1197, 1020, 589, 577, 316, 101, 484, 689, 837, 417, 879, 847, 888, 864, 614, 1191, 585, 123, 284, 1034, 828, 708, 621, 459, 1079, 508, 288, 91, 1060, 822, 821, 670, 980, 604, 694, 560, 354, 344, 579, 112, 939, 154, 191, 28, 906, 534, 965, 856, 982, 1073, 578, 1206, 808, 350, 453, 257, 1093, 142, 615, 177, 169, 942, 173, 1203, 1127, 158, 829, 424, 82, 368, 817, 833, 461, 838, 720, 221, 794, 668, 139, 525, 775, 439, 895, 1011, 1141, 55, 853, 832, 678, 912, 57, 719, 1031, 203, 867, 622, 468, 727, 429, 841, 428, 1090, 562, 1165, 665, 650, 546, 176, 630, 381, 664, 109, 823, 228, 592, 423, 796, 1013, 56, 1133, 862, 232, 308, 364, 717, 799, 1049, 43, 1194, 506, 721, 631, 359, 1195, 334, 778, 149, 170, 651, 759, 776, 64, 414, 969, 462, 372, 111, 1122, 834, 685, 98, 11, 105, 1123, 541, 595, 586, 479, 1046, 342, 519, 619, 568, 810, 52, 1145, 556, 1002, 527, 341, 944, 110, 1008, 207, 466, 1027, 624, 998, 587, 1208, 760, 296, 663, 972, 260, 956, 473, 347, 254, 258, 222, 893, 779, 332, 883, 857, 1116, 377, 854, 602, 978, 216, 107, 1004, 443, 868, 416, 0, 742, 405, 662, 370, 33, 375, 714, 680, 1037, 26, 1106, 528, 738, 465, 1100, 908, 291, 547, 1058, 1142, 641, 941, 442, 252, 46, 1052, 18, 380, 1182, 658, 62, 798, 900, 121, 1086, 669, 1196, 1099, 345, 317, 1017, 529, 314, 1029, 30, 1068, 119, 554, 74, 635, 330, 887, 286, 147, 559, 65, 1025, 472, 984, 1009, 596, 1059, 281, 613, 1026, 677, 691, 338, 1092, 600, 892, 740, 360, 426, 241, 607, 530, 728, 1124, 859, 767, 977, 309, 122, 165, 1207, 1001, 61, 1169, 995, 836, 100, 647, 1148, 659, 230, 1188, 815, 1152, 214, 298, 1146, 279, 967, 1064, 1, 894, 217, 250, 643, 711, 12, 914, 642, 686, 498, 1103, 456, 922, 936, 202, 757, 1193, 408, 819, 679, 201, 406, 783, 369, 494, 40, 59, 398, 523, 4, 761, 576, 266, 566, 276, 766, 1160, 486, 538, 1168, 806, 476, 1038, 990, 763, 681, 499, 167, 569, 273, 1030, 981, 137, 231, 999, 795, 809, 199, 1167, 1107, 869, 606, 1098, 206, 716, 628, 1062, 388, 415, 733, 1190, 160, 118, 20, 337, 1178, 108, 640, 293, 878, 964, 99, 1136, 860, 190, 919, 882, 928, 1075, 1078, 1126, 544, 968, 1048, 299, 573, 386, 504, 400, 524, 846, 146, 1085, 580, 830, 248, 514, 904, 1005, 446, 1163, 850, 574, 1070, 845, 932, 764, 204, 365, 73, 41, 699, 690, 1111, 889, 781, 34, 1080, 975, 53, 866, 557, 485, 584, 272, 205, 471, 1082, 616, 376, 445, 917, 450, 136, 5, 1101, 772, 565, 195, 1076, 949, 1179, 150, 789, 605, 1137, 88, 1014, 923, 1202, 1214, 94, 741, 410, 909, 657, 814, 323, 863, 682, 209, 555, 438, 378, 27, 702, 115, 185, 157, 275, 183, 277, 623, 141, 1155, 310, 1043, 601, 218, 396, 1119, 116, 517, 913, 1147, 654, 652, 638, 50, 639, 735, 1066, 593, 1176, 629, 464, 413, 384, 113, 744, 1055, 366, 898, 992, 47, 955, 610, 1149, 1067, 189, 1183, 480, 151, 1039, 745, 353, 77, 184, 934, 463, 896, 1010, 280, 245, 885, 793, 770, 390, 138, 38, 180, 404, 974, 467, 768, 1024, 1042, 603, 237, 10, 448, 661, 851, 697, 787, 129, 289, 425, 724, 1035, 626, 591, 1130, 567, 1087, 161, 1161, 128, 570, 705, 843, 513, 1114, 1177, 996, 208, 483, 572, 660, 324, 240, 532, 407, 1051, 1104, 698, 71, 905, 45, 294, 920, 130, 790, 1172, 646, 1187, 124, 1023, 1140, 743, 1032, 444, 477, 285, 70, 671, 193, 1096, 976, 722, 746, 970, 175, 306, 72, 198, 1040, 265, 840, 802, 500, 1088, 1057, 134, 8, 902, 910, 736, 305, 315, 890, 238, 597, 478, 85, 693, 543, 181, 1159, 729, 827, 215, 226, 762, 1144, 90, 434, 333, 518, 1212, 51, 192, 706, 1081, 329, 327, 501, 674, 940, 1016, 270, 618, 511, 489, 1118, 780, 788, 897, 179, 1033, 880, 1181, 487, 704, 771, 522, 24, 399, 419, 343, 302, 68, 1211, 220, 382, 924, 684, 1047, 695, 656, 267, 166, 1018, 675, 1007, 197, 954, 739, 948, 931, 495, 510, 295, 858, 69, 1044, 812, 229, 732, 340, 1065, 1021, 227, 244, 683, 537, 987, 953, 637, 1180, 251, 881, 29, 235, 83, 813, 393, 581, 901, 383, 1077, 482, 507, 747, 946, 676, 667, 292, 633, 120, 800, 1201, 6, 331, 22, 1125, 153, 318, 422, 933, 703, 233, 213, 493, 78, 163, 1115, 367, 1204, 246, 35, 155, 807, 599, 1173, 117, 1198]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4119288082181835
the save name prefix for this run is:  chkpt-ID_4119288082181835_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.522 +- 0.007
mrr vals (pred, true): 0.001, 0.239
batch losses (mrrl, rdl): 0.0, 0.0045802491

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1141
rank avg (pred): 0.276 +- 0.142
mrr vals (pred, true): 0.128, 0.033
batch losses (mrrl, rdl): 0.0, 0.0002019572

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.440 +- 0.269
mrr vals (pred, true): 0.176, 0.004
batch losses (mrrl, rdl): 0.0, 1.15161e-05

Epoch over!
epoch time: 14.895

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 206
rank avg (pred): 0.452 +- 0.272
mrr vals (pred, true): 0.166, 0.004
batch losses (mrrl, rdl): 0.0, 9.1856e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.442 +- 0.270
mrr vals (pred, true): 0.166, 0.005
batch losses (mrrl, rdl): 0.0, 7.8999e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 272
rank avg (pred): 0.070 +- 0.043
mrr vals (pred, true): 0.181, 0.205
batch losses (mrrl, rdl): 0.0, 8.0255e-06

Epoch over!
epoch time: 14.871

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 580
rank avg (pred): 0.447 +- 0.280
mrr vals (pred, true): 0.152, 0.003
batch losses (mrrl, rdl): 0.0, 1.10899e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.443 +- 0.270
mrr vals (pred, true): 0.129, 0.005
batch losses (mrrl, rdl): 0.0, 8.6866e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 960
rank avg (pred): 0.618 +- 0.304
mrr vals (pred, true): 0.074, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004429932

Epoch over!
epoch time: 14.848

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.449 +- 0.268
mrr vals (pred, true): 0.113, 0.004
batch losses (mrrl, rdl): 0.0, 9.2762e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.444 +- 0.267
mrr vals (pred, true): 0.101, 0.004
batch losses (mrrl, rdl): 0.0, 6.4495e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.449 +- 0.289
mrr vals (pred, true): 0.131, 0.004
batch losses (mrrl, rdl): 0.0, 8.1403e-06

Epoch over!
epoch time: 14.853

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.467 +- 0.288
mrr vals (pred, true): 0.118, 0.003
batch losses (mrrl, rdl): 0.0, 1.60034e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 332
rank avg (pred): 0.456 +- 0.259
mrr vals (pred, true): 0.073, 0.005
batch losses (mrrl, rdl): 0.0, 8.1112e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.463 +- 0.264
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0, 9.7267e-06

Epoch over!
epoch time: 14.844

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.085 +- 0.055
mrr vals (pred, true): 0.108, 0.232
batch losses (mrrl, rdl): 0.1536121368, 3.3695e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.014 +- 0.011
mrr vals (pred, true): 0.238, 0.251
batch losses (mrrl, rdl): 0.001757452, 4.06352e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.463 +- 0.169
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 5.7167e-06, 4.04639e-05

Epoch over!
epoch time: 15.048

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 827
rank avg (pred): 0.029 +- 0.024
mrr vals (pred, true): 0.180, 0.319
batch losses (mrrl, rdl): 0.1941521615, 8.6789e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.473 +- 0.159
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001425237, 4.68147e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.467 +- 0.185
mrr vals (pred, true): 0.061, 0.004
batch losses (mrrl, rdl): 0.0012447637, 3.41433e-05

Epoch over!
epoch time: 15.031

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.466 +- 0.164
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 6.7049e-06, 4.70459e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.012 +- 0.010
mrr vals (pred, true): 0.261, 0.261
batch losses (mrrl, rdl): 1.632e-07, 3.34277e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.466 +- 0.151
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.25429e-05, 5.86068e-05

Epoch over!
epoch time: 14.969

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 940
rank avg (pred): 0.499 +- 0.172
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.25137e-05, 0.000185469

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 659
rank avg (pred): 0.518 +- 0.148
mrr vals (pred, true): 0.034, 0.005
batch losses (mrrl, rdl): 0.0024358865, 9.85237e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 810
rank avg (pred): 0.042 +- 0.034
mrr vals (pred, true): 0.254, 0.320
batch losses (mrrl, rdl): 0.0445607454, 2.8651e-06

Epoch over!
epoch time: 14.905

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 244
rank avg (pred): 0.111 +- 0.082
mrr vals (pred, true): 0.227, 0.298
batch losses (mrrl, rdl): 0.050827615, 0.0001250504

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.527 +- 0.284
mrr vals (pred, true): 0.067, 0.014
batch losses (mrrl, rdl): 0.0028164242, 0.0006265119

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.555 +- 0.287
mrr vals (pred, true): 0.061, 0.007
batch losses (mrrl, rdl): 0.0011557811, 0.0005468004

Epoch over!
epoch time: 14.892

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 35
rank avg (pred): 0.137 +- 0.099
mrr vals (pred, true): 0.188, 0.140
batch losses (mrrl, rdl): 0.0223896615, 5.61735e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.454 +- 0.151
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 5.58335e-05, 5.63798e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.447 +- 0.158
mrr vals (pred, true): 0.057, 0.005
batch losses (mrrl, rdl): 0.0004308574, 5.33901e-05

Epoch over!
epoch time: 14.947

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1152
rank avg (pred): 0.517 +- 0.261
mrr vals (pred, true): 0.052, 0.021
batch losses (mrrl, rdl): 3.84599e-05, 0.0012753239

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.451 +- 0.155
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.0006547162, 5.35483e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.468 +- 0.127
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0002711388, 6.2831e-05

Epoch over!
epoch time: 15.048

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 78
rank avg (pred): 0.167 +- 0.111
mrr vals (pred, true): 0.125, 0.133
batch losses (mrrl, rdl): 0.000662501, 0.0001520446

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.464 +- 0.127
mrr vals (pred, true): 0.048, 0.002
batch losses (mrrl, rdl): 4.06487e-05, 6.32741e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 334
rank avg (pred): 0.458 +- 0.134
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.79791e-05, 5.69634e-05

Epoch over!
epoch time: 15.081

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.455 +- 0.133
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 4.56856e-05, 5.99434e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1021
rank avg (pred): 0.462 +- 0.129
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 8.41e-08, 5.78523e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.632 +- 0.191
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002526758, 2.70826e-05

Epoch over!
epoch time: 15.053

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.472 +- 0.229
mrr vals (pred, true): 0.063, 0.014
batch losses (mrrl, rdl): 0.0016487266, 0.0003010936

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.454 +- 0.122
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 4.993e-07, 6.96028e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 619
rank avg (pred): 0.456 +- 0.123
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 9.0216e-06, 8.25626e-05

Epoch over!
epoch time: 15.17

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.460 +- 0.118
mrr vals (pred, true): 0.049, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04564 	 0.00092 	 m..s
    3 	     1 	 0.04514 	 0.00112 	 m..s
   15 	     2 	 0.04826 	 0.00212 	 m..s
   63 	     3 	 0.05264 	 0.00213 	 m..s
   38 	     4 	 0.04977 	 0.00226 	 m..s
   76 	     5 	 0.05442 	 0.00262 	 m..s
   63 	     6 	 0.05264 	 0.00275 	 m..s
   77 	     7 	 0.05481 	 0.00282 	 m..s
   71 	     8 	 0.05341 	 0.00289 	 m..s
   70 	     9 	 0.05324 	 0.00300 	 m..s
   68 	    10 	 0.05313 	 0.00305 	 m..s
   37 	    11 	 0.04976 	 0.00309 	 m..s
   54 	    12 	 0.05105 	 0.00317 	 m..s
   79 	    13 	 0.05484 	 0.00321 	 m..s
   24 	    14 	 0.04898 	 0.00325 	 m..s
   29 	    15 	 0.04936 	 0.00331 	 m..s
   74 	    16 	 0.05380 	 0.00335 	 m..s
   75 	    17 	 0.05417 	 0.00337 	 m..s
   73 	    18 	 0.05378 	 0.00342 	 m..s
   44 	    19 	 0.05006 	 0.00347 	 m..s
   66 	    20 	 0.05288 	 0.00347 	 m..s
   78 	    21 	 0.05483 	 0.00350 	 m..s
   40 	    22 	 0.04982 	 0.00358 	 m..s
   18 	    23 	 0.04836 	 0.00359 	 m..s
   61 	    24 	 0.05235 	 0.00363 	 m..s
   11 	    25 	 0.04811 	 0.00364 	 m..s
    1 	    26 	 0.04302 	 0.00366 	 m..s
    7 	    27 	 0.04781 	 0.00367 	 m..s
   33 	    28 	 0.04964 	 0.00367 	 m..s
   62 	    29 	 0.05241 	 0.00368 	 m..s
   22 	    30 	 0.04856 	 0.00372 	 m..s
   17 	    31 	 0.04833 	 0.00372 	 m..s
   59 	    32 	 0.05206 	 0.00375 	 m..s
   34 	    33 	 0.04966 	 0.00376 	 m..s
   27 	    34 	 0.04928 	 0.00376 	 m..s
   55 	    35 	 0.05138 	 0.00387 	 m..s
   43 	    36 	 0.04989 	 0.00390 	 m..s
   46 	    37 	 0.05017 	 0.00392 	 m..s
   53 	    38 	 0.05069 	 0.00395 	 m..s
   10 	    39 	 0.04805 	 0.00397 	 m..s
   20 	    40 	 0.04848 	 0.00398 	 m..s
   60 	    41 	 0.05235 	 0.00399 	 m..s
    8 	    42 	 0.04791 	 0.00402 	 m..s
   50 	    43 	 0.05047 	 0.00403 	 m..s
    8 	    44 	 0.04791 	 0.00405 	 m..s
   45 	    45 	 0.05013 	 0.00408 	 m..s
    2 	    46 	 0.04470 	 0.00410 	 m..s
   31 	    47 	 0.04941 	 0.00410 	 m..s
   16 	    48 	 0.04831 	 0.00413 	 m..s
   52 	    49 	 0.05068 	 0.00417 	 m..s
   48 	    50 	 0.05028 	 0.00418 	 m..s
    0 	    51 	 0.04044 	 0.00428 	 m..s
   21 	    52 	 0.04850 	 0.00438 	 m..s
   35 	    53 	 0.04969 	 0.00441 	 m..s
   67 	    54 	 0.05311 	 0.00444 	 m..s
   49 	    55 	 0.05035 	 0.00445 	 m..s
   57 	    56 	 0.05173 	 0.00448 	 m..s
   30 	    57 	 0.04941 	 0.00450 	 m..s
   11 	    58 	 0.04811 	 0.00452 	 m..s
   42 	    59 	 0.04985 	 0.00453 	 m..s
   69 	    60 	 0.05314 	 0.00456 	 m..s
   51 	    61 	 0.05053 	 0.00461 	 m..s
   23 	    62 	 0.04860 	 0.00465 	 m..s
   35 	    63 	 0.04969 	 0.00468 	 m..s
   18 	    64 	 0.04836 	 0.00469 	 m..s
   41 	    65 	 0.04982 	 0.00475 	 m..s
   26 	    66 	 0.04915 	 0.00477 	 m..s
   32 	    67 	 0.04943 	 0.00505 	 m..s
   72 	    68 	 0.05373 	 0.00515 	 m..s
   28 	    69 	 0.04928 	 0.00517 	 m..s
   47 	    70 	 0.05019 	 0.00533 	 m..s
   13 	    71 	 0.04811 	 0.00548 	 m..s
    6 	    72 	 0.04759 	 0.00571 	 m..s
   56 	    73 	 0.05168 	 0.00573 	 m..s
   39 	    74 	 0.04979 	 0.00597 	 m..s
   58 	    75 	 0.05174 	 0.00604 	 m..s
   65 	    76 	 0.05285 	 0.00610 	 m..s
   83 	    77 	 0.06138 	 0.01061 	 m..s
   82 	    78 	 0.06115 	 0.01360 	 m..s
   80 	    79 	 0.05787 	 0.01439 	 m..s
   81 	    80 	 0.05924 	 0.02430 	 m..s
   25 	    81 	 0.04907 	 0.02457 	 ~...
   14 	    82 	 0.04815 	 0.02742 	 ~...
    5 	    83 	 0.04707 	 0.03475 	 ~...
   94 	    84 	 0.20168 	 0.03563 	 MISS
   91 	    85 	 0.16611 	 0.05291 	 MISS
   84 	    86 	 0.11612 	 0.11357 	 ~...
   92 	    87 	 0.17397 	 0.13162 	 m..s
   88 	    88 	 0.13780 	 0.13249 	 ~...
   85 	    89 	 0.11940 	 0.13345 	 ~...
   86 	    90 	 0.13030 	 0.13713 	 ~...
   87 	    91 	 0.13075 	 0.13923 	 ~...
   90 	    92 	 0.15381 	 0.14215 	 ~...
   89 	    93 	 0.15018 	 0.14848 	 ~...
   96 	    94 	 0.20426 	 0.17383 	 m..s
   97 	    95 	 0.20449 	 0.17489 	 ~...
   93 	    96 	 0.18537 	 0.17714 	 ~...
  101 	    97 	 0.23033 	 0.18472 	 m..s
   95 	    98 	 0.20394 	 0.18578 	 ~...
   98 	    99 	 0.20814 	 0.19847 	 ~...
  105 	   100 	 0.25328 	 0.19940 	 m..s
  106 	   101 	 0.25720 	 0.23376 	 ~...
  103 	   102 	 0.24540 	 0.23733 	 ~...
  111 	   103 	 0.28640 	 0.24938 	 m..s
  104 	   104 	 0.24793 	 0.25693 	 ~...
   99 	   105 	 0.21595 	 0.25930 	 m..s
   99 	   106 	 0.21595 	 0.26233 	 m..s
  107 	   107 	 0.26200 	 0.26340 	 ~...
  114 	   108 	 0.30231 	 0.26465 	 m..s
  115 	   109 	 0.30416 	 0.26809 	 m..s
  102 	   110 	 0.23206 	 0.27020 	 m..s
  116 	   111 	 0.30534 	 0.28283 	 ~...
  113 	   112 	 0.29977 	 0.28961 	 ~...
  108 	   113 	 0.27997 	 0.29968 	 ~...
  117 	   114 	 0.30636 	 0.30632 	 ~...
  120 	   115 	 0.32991 	 0.30783 	 ~...
  109 	   116 	 0.28031 	 0.31687 	 m..s
  117 	   117 	 0.30636 	 0.31875 	 ~...
  119 	   118 	 0.32266 	 0.32046 	 ~...
  110 	   119 	 0.28609 	 0.32643 	 m..s
  112 	   120 	 0.29909 	 0.32947 	 m..s
==========================================
r_mrr = 0.9765118360519409
r2_mrr = 0.8296341896057129
spearmanr_mrr@5 = 0.9781984090805054
spearmanr_mrr@10 = 0.8809571266174316
spearmanr_mrr@50 = 0.9927967190742493
spearmanr_mrr@100 = 0.9964195489883423
spearmanr_mrr@All = 0.9966629147529602
==========================================
test time: 0.455
Done Testing dataset CoDExSmall
total time taken: 235.4031698703766
training time taken: 224.92171263694763
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9765)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8296)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9782)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.8810)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9928)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9964)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9967)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.653168287569315}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 782163540610260
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [961, 1011, 423, 1214, 1134, 383, 731, 1088, 1033, 394, 405, 269, 288, 898, 284, 795, 57, 586, 1017, 715, 505, 957, 519, 188, 707, 363, 848, 1132, 655, 403, 1019, 964, 670, 354, 913, 679, 14, 1199, 246, 613, 75, 178, 609, 139, 53, 52, 420, 976, 607, 908, 63, 339, 28, 1168, 989, 148, 979, 1201, 1111, 1167, 1122, 765, 1094, 949, 545, 1062, 1014, 435, 897, 772, 820, 793, 769, 82, 464, 1182, 1200, 250, 201, 697, 636, 278, 554, 238, 174, 1181, 470, 382, 1048, 991, 198, 583, 921, 945, 969, 786, 350, 1086, 774, 604, 199, 813, 1056, 140, 404, 630, 1003, 313, 1096, 451, 736, 422, 168, 682, 1149, 625, 664, 1142, 584, 181, 928]
valid_ids (0): []
train_ids (1094): [778, 955, 428, 34, 706, 13, 341, 1150, 296, 657, 230, 860, 98, 388, 1117, 766, 1080, 113, 1020, 623, 321, 80, 754, 1040, 292, 656, 216, 557, 639, 177, 1126, 789, 650, 1022, 1084, 40, 1043, 783, 755, 833, 749, 1119, 771, 830, 709, 193, 906, 693, 540, 995, 610, 454, 853, 516, 775, 56, 746, 959, 315, 15, 640, 626, 575, 307, 819, 1058, 366, 926, 294, 910, 1068, 668, 1093, 1118, 1206, 184, 498, 131, 938, 161, 412, 525, 1202, 924, 834, 179, 305, 561, 94, 1187, 227, 1103, 143, 993, 396, 816, 149, 1038, 169, 548, 432, 476, 934, 1016, 565, 990, 433, 1175, 351, 543, 718, 716, 306, 888, 665, 605, 24, 727, 735, 681, 1082, 417, 68, 799, 20, 91, 779, 598, 880, 37, 580, 197, 239, 153, 1045, 488, 266, 323, 806, 458, 868, 684, 62, 588, 751, 532, 45, 962, 343, 745, 1140, 145, 67, 603, 915, 480, 978, 1095, 495, 539, 358, 517, 499, 787, 1049, 105, 47, 332, 1107, 276, 566, 494, 1197, 36, 1083, 801, 644, 634, 1037, 275, 429, 7, 667, 688, 1015, 64, 1211, 933, 975, 815, 373, 1108, 1114, 151, 570, 1030, 289, 8, 130, 245, 175, 431, 837, 265, 96, 791, 273, 418, 1063, 796, 846, 861, 182, 32, 556, 214, 948, 981, 881, 914, 1021, 4, 911, 828, 1079, 522, 643, 2, 160, 974, 364, 812, 48, 792, 564, 460, 635, 869, 1113, 1121, 591, 42, 274, 1059, 1002, 734, 61, 982, 695, 112, 425, 389, 615, 1141, 807, 1099, 106, 618, 845, 43, 1191, 484, 549, 259, 11, 980, 157, 407, 300, 89, 1067, 713, 426, 362, 1208, 9, 16, 1024, 1209, 672, 243, 1018, 21, 659, 1129, 507, 577, 205, 691, 673, 491, 46, 864, 10, 996, 967, 386, 1055, 1213, 58, 529, 568, 109, 817, 527, 559, 666, 1092, 99, 100, 633, 882, 759, 78, 1130, 704, 855, 165, 747, 919, 1210, 832, 41, 1013, 600, 946, 1073, 760, 825, 541, 1075, 122, 395, 23, 553, 249, 999, 1098, 1177, 863, 497, 866, 329, 1158, 838, 937, 973, 1179, 271, 279, 1065, 1131, 803, 741, 459, 73, 349, 1186, 352, 381, 1036, 430, 805, 263, 689, 638, 87, 1171, 652, 1029, 518, 208, 1138, 1166, 572, 137, 827, 537, 456, 224, 101, 524, 753, 714, 449, 84, 374, 965, 1164, 761, 858, 242, 879, 69, 687, 282, 737, 573, 1178, 852, 79, 406, 324, 642, 723, 1051, 653, 763, 378, 947, 475, 55, 984, 22, 490, 253, 277, 876, 1154, 597, 357, 302, 1101, 71, 171, 38, 121, 1185, 331, 361, 896, 1105, 1143, 654, 1136, 402, 411, 611, 337, 146, 486, 399, 647, 255, 267, 283, 936, 297, 1120, 878, 442, 445, 930, 211, 994, 322, 1147, 700, 508, 138, 225, 685, 206, 166, 1115, 5, 602, 1035, 671, 83, 338, 1112, 397, 25, 1031, 463, 546, 385, 1026, 851, 943, 85, 295, 1077, 788, 972, 1100, 50, 489, 621, 971, 773, 680, 1046, 849, 1133, 103, 708, 986, 455, 675, 348, 102, 187, 1041, 528, 599, 628, 1137, 86, 1123, 590, 400, 1050, 207, 1076, 234, 334, 54, 530, 578, 1102, 195, 30, 1006, 220, 501, 1146, 236, 790, 892, 493, 416, 823, 927, 387, 506, 172, 200, 309, 514, 1189, 123, 762, 562, 438, 932, 756, 164, 465, 900, 658, 439, 1023, 750, 93, 134, 632, 854, 421, 126, 478, 369, 1170, 764, 916, 931, 585, 108, 370, 677, 631, 894, 612, 446, 574, 1087, 344, 468, 739, 88, 1061, 136, 159, 152, 19, 509, 39, 1160, 441, 885, 325, 60, 147, 1151, 533, 1124, 992, 721, 162, 1116, 717, 390, 440, 641, 857, 1004, 257, 340, 701, 163, 90, 368, 535, 232, 595, 998, 794, 606, 479, 264, 76, 678, 258, 954, 436, 1057, 719, 191, 204, 808, 326, 744, 142, 1139, 784, 150, 115, 1027, 167, 536, 218, 301, 840, 346, 119, 821, 608, 738, 569, 434, 987, 550, 1091, 733, 170, 596, 49, 720, 1, 1128, 120, 1001, 155, 593, 558, 376, 538, 291, 1097, 31, 699, 237, 829, 614, 192, 674, 124, 582, 877, 1145, 551, 342, 314, 758, 29, 345, 408, 127, 547, 141, 1039, 213, 690, 781, 710, 809, 893, 144, 229, 1012, 33, 116, 917, 703, 627, 272, 859, 1205, 299, 59, 251, 355, 335, 1053, 477, 1127, 645, 1165, 1009, 377, 929, 114, 902, 310, 826, 26, 409, 871, 901, 183, 462, 443, 353, 581, 712, 474, 552, 622, 380, 414, 316, 576, 180, 542, 132, 104, 483, 1204, 240, 1194, 466, 843, 190, 447, 1081, 1078, 1109, 392, 1176, 970, 173, 359, 696, 44, 886, 905, 317, 1106, 874, 1089, 887, 95, 1044, 722, 333, 1025, 1148, 209, 1192, 17, 935, 1000, 960, 336, 944, 918, 70, 1007, 320, 770, 850, 616, 752, 776, 1005, 940, 836, 285, 814, 555, 117, 873, 268, 683, 521, 624, 379, 473, 822, 244, 217, 247, 1196, 308, 966, 0, 711, 968, 281, 1161, 619, 223, 798, 1157, 202, 452, 1174, 444, 215, 904, 424, 110, 780, 222, 286, 523, 365, 154, 1054, 496, 977, 469, 1028, 895, 694, 676, 492, 398, 811, 637, 129, 221, 865, 298, 920, 958, 629, 92, 419, 330, 1090, 1184, 18, 125, 740, 729, 824, 1085, 233, 728, 939, 504, 1042, 1069, 328, 768, 360, 1070, 1010, 617, 601, 802, 724, 1104, 472, 1032, 1212, 810, 1066, 72, 620, 692, 81, 1159, 111, 1125, 1110, 903, 841, 1173, 950, 27, 482, 312, 196, 587, 800, 923, 1153, 280, 189, 467, 579, 1047, 203, 226, 212, 1072, 77, 371, 186, 648, 883, 804, 726, 231, 662, 1064, 3, 870, 1155, 646, 1188, 907, 74, 66, 391, 453, 797, 875, 862, 515, 97, 256, 592, 777, 1183, 303, 649, 457, 210, 651, 367, 318, 393, 767, 988, 1071, 1163, 372, 481, 867, 589, 663, 531, 1207, 1172, 831, 942, 261, 241, 65, 35, 1074, 884, 563, 705, 951, 1060, 953, 660, 725, 952, 133, 912, 156, 889, 135, 6, 842, 571, 560, 375, 327, 856, 1008, 1203, 304, 311, 260, 839, 118, 743, 963, 1169, 997, 427, 520, 899, 262, 384, 526, 248, 270, 835, 235, 544, 983, 922, 485, 128, 1052, 12, 410, 941, 1162, 185, 500, 785, 502, 293, 252, 319, 219, 461, 290, 1144, 513, 1198, 487, 471, 511, 534, 698, 702, 107, 450, 872, 512, 158, 1180, 415, 1190, 818, 661, 730, 1152, 228, 356, 401, 1156, 347, 891, 985, 890, 757, 567, 254, 956, 686, 194, 503, 413, 448, 287, 844, 742, 925, 782, 1135, 51, 748, 1195, 1034, 1193, 669, 510, 847, 594, 732, 437, 176, 909]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5560990120470893
the save name prefix for this run is:  chkpt-ID_5560990120470893_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 573
rank avg (pred): 0.529 +- 0.009
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 0.000124573

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 826
rank avg (pred): 0.110 +- 0.151
mrr vals (pred, true): 0.011, 0.312
batch losses (mrrl, rdl): 0.0, 0.0001087504

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1043
rank avg (pred): 0.449 +- 0.255
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0, 3.22859e-05

Epoch over!
epoch time: 14.891

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 424
rank avg (pred): 0.498 +- 0.289
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 3.4504e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.050 +- 0.073
mrr vals (pred, true): 0.024, 0.238
batch losses (mrrl, rdl): 0.0, 2.8914e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.478 +- 0.281
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0, 9.385e-07

Epoch over!
epoch time: 14.832

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1125
rank avg (pred): 0.475 +- 0.284
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0, 2.1425e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.466 +- 0.276
mrr vals (pred, true): 0.004, 0.004
batch losses (mrrl, rdl): 0.0, 3.226e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 965
rank avg (pred): 0.547 +- 0.271
mrr vals (pred, true): 0.003, 0.004
batch losses (mrrl, rdl): 0.0, 4.79785e-05

Epoch over!
epoch time: 14.834

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 917
rank avg (pred): 0.538 +- 0.273
mrr vals (pred, true): 0.004, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002987072

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.486 +- 0.286
mrr vals (pred, true): 0.007, 0.004
batch losses (mrrl, rdl): 0.0, 9.929e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1150
rank avg (pred): 0.265 +- 0.239
mrr vals (pred, true): 0.011, 0.030
batch losses (mrrl, rdl): 0.0, 4.67695e-05

Epoch over!
epoch time: 14.848

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.505 +- 0.293
mrr vals (pred, true): 0.005, 0.004
batch losses (mrrl, rdl): 0.0, 8.5038e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 331
rank avg (pred): 0.480 +- 0.292
mrr vals (pred, true): 0.010, 0.003
batch losses (mrrl, rdl): 0.0, 1.5261e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 321
rank avg (pred): 0.053 +- 0.102
mrr vals (pred, true): 0.070, 0.175
batch losses (mrrl, rdl): 0.0, 1.34621e-05

Epoch over!
epoch time: 14.856

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 846
rank avg (pred): 0.458 +- 0.279
mrr vals (pred, true): 0.022, 0.005
batch losses (mrrl, rdl): 0.0079492349, 4.6154e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 552
rank avg (pred): 0.339 +- 0.187
mrr vals (pred, true): 0.057, 0.010
batch losses (mrrl, rdl): 0.0005163451, 6.22456e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1002
rank avg (pred): 0.427 +- 0.273
mrr vals (pred, true): 0.065, 0.004
batch losses (mrrl, rdl): 0.0021229167, 3.28541e-05

Epoch over!
epoch time: 15.045

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1008
rank avg (pred): 0.468 +- 0.282
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0005128124, 5.069e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.580 +- 0.379
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 7.6275e-06, 0.0002269767

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1018
rank avg (pred): 0.482 +- 0.289
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0004868476, 9.73e-06

Epoch over!
epoch time: 15.033

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 869
rank avg (pred): 0.454 +- 0.280
mrr vals (pred, true): 0.056, 0.005
batch losses (mrrl, rdl): 0.0003797064, 2.4597e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 180
rank avg (pred): 0.490 +- 0.299
mrr vals (pred, true): 0.069, 0.004
batch losses (mrrl, rdl): 0.0036900835, 1.52496e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 620
rank avg (pred): 0.433 +- 0.256
mrr vals (pred, true): 0.054, 0.003
batch losses (mrrl, rdl): 0.0001610401, 2.78006e-05

Epoch over!
epoch time: 15.074

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1190
rank avg (pred): 0.448 +- 0.260
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001412622, 8.0402e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1059
rank avg (pred): 0.089 +- 0.085
mrr vals (pred, true): 0.302, 0.315
batch losses (mrrl, rdl): 0.0014960738, 6.45603e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 686
rank avg (pred): 0.452 +- 0.255
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 3.011e-06, 5.6294e-06

Epoch over!
epoch time: 15.233

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1031
rank avg (pred): 0.439 +- 0.247
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 1.45123e-05, 1.56581e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.447 +- 0.261
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001628053, 2.8109e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 776
rank avg (pred): 0.491 +- 0.279
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 7.6552e-05, 1.15116e-05

Epoch over!
epoch time: 15.117

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 101
rank avg (pred): 0.503 +- 0.292
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001229614, 4.76097e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.102 +- 0.089
mrr vals (pred, true): 0.255, 0.275
batch losses (mrrl, rdl): 0.0040982687, 0.0001128448

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 656
rank avg (pred): 0.482 +- 0.279
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 5.41284e-05, 4.488e-06

Epoch over!
epoch time: 14.916

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 570
rank avg (pred): 0.488 +- 0.282
mrr vals (pred, true): 0.055, 0.005
batch losses (mrrl, rdl): 0.0002048883, 9.2807e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 559
rank avg (pred): 0.459 +- 0.264
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 1.22784e-05, 1.8699e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 968
rank avg (pred): 0.487 +- 0.263
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 8.457e-07, 9.7406e-06

Epoch over!
epoch time: 14.886

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 557
rank avg (pred): 0.338 +- 0.193
mrr vals (pred, true): 0.055, 0.008
batch losses (mrrl, rdl): 0.0002217003, 0.0001100004

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 129
rank avg (pred): 0.489 +- 0.259
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 7.92152e-05, 2.26316e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1169
rank avg (pred): 0.463 +- 0.293
mrr vals (pred, true): 0.054, 0.002
batch losses (mrrl, rdl): 0.0001918566, 4.145e-06

Epoch over!
epoch time: 14.886

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 608
rank avg (pred): 0.634 +- 0.344
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001652843, 0.0004189112

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 900
rank avg (pred): 0.541 +- 0.278
mrr vals (pred, true): 0.041, 0.002
batch losses (mrrl, rdl): 0.0007557069, 5.08457e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.516 +- 0.277
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0003499239, 8.8061e-06

Epoch over!
epoch time: 14.87

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 103
rank avg (pred): 0.477 +- 0.304
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.00010759, 2.1167e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.523 +- 0.298
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 6.44878e-05, 4.81161e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.502 +- 0.300
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 2.02065e-05, 3.5589e-06

Epoch over!
epoch time: 14.886

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.477 +- 0.302
mrr vals (pred, true): 0.052, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.04663 	 0.00087 	 m..s
   67 	     1 	 0.05177 	 0.00088 	 m..s
   67 	     2 	 0.05177 	 0.00143 	 m..s
   67 	     3 	 0.05177 	 0.00151 	 m..s
   67 	     4 	 0.05177 	 0.00192 	 m..s
   16 	     5 	 0.04695 	 0.00208 	 m..s
   50 	     6 	 0.04988 	 0.00222 	 m..s
   44 	     7 	 0.04970 	 0.00279 	 m..s
   17 	     8 	 0.04702 	 0.00288 	 m..s
   67 	     9 	 0.05177 	 0.00297 	 m..s
   67 	    10 	 0.05177 	 0.00304 	 m..s
   27 	    11 	 0.04763 	 0.00310 	 m..s
   67 	    12 	 0.05177 	 0.00311 	 m..s
   90 	    13 	 0.05178 	 0.00317 	 m..s
   67 	    14 	 0.05177 	 0.00320 	 m..s
   37 	    15 	 0.04860 	 0.00321 	 m..s
   51 	    16 	 0.05000 	 0.00321 	 m..s
   39 	    17 	 0.04878 	 0.00324 	 m..s
    2 	    18 	 0.04504 	 0.00327 	 m..s
   66 	    19 	 0.05135 	 0.00330 	 m..s
   59 	    20 	 0.05058 	 0.00335 	 m..s
   48 	    21 	 0.04984 	 0.00335 	 m..s
   24 	    22 	 0.04746 	 0.00336 	 m..s
   46 	    23 	 0.04974 	 0.00340 	 m..s
   43 	    24 	 0.04911 	 0.00341 	 m..s
   41 	    25 	 0.04891 	 0.00342 	 m..s
   67 	    26 	 0.05177 	 0.00345 	 m..s
   26 	    27 	 0.04754 	 0.00347 	 m..s
   14 	    28 	 0.04642 	 0.00347 	 m..s
   67 	    29 	 0.05177 	 0.00348 	 m..s
   65 	    30 	 0.05099 	 0.00358 	 m..s
   23 	    31 	 0.04744 	 0.00360 	 m..s
   30 	    32 	 0.04795 	 0.00361 	 m..s
   67 	    33 	 0.05177 	 0.00362 	 m..s
   67 	    34 	 0.05177 	 0.00363 	 m..s
   62 	    35 	 0.05072 	 0.00366 	 m..s
    5 	    36 	 0.04529 	 0.00368 	 m..s
   25 	    37 	 0.04751 	 0.00370 	 m..s
   22 	    38 	 0.04735 	 0.00371 	 m..s
   56 	    39 	 0.05041 	 0.00371 	 m..s
   13 	    40 	 0.04635 	 0.00374 	 m..s
   91 	    41 	 0.05181 	 0.00375 	 m..s
   67 	    42 	 0.05177 	 0.00375 	 m..s
   67 	    43 	 0.05177 	 0.00377 	 m..s
   18 	    44 	 0.04703 	 0.00381 	 m..s
   53 	    45 	 0.05005 	 0.00381 	 m..s
   42 	    46 	 0.04898 	 0.00381 	 m..s
   67 	    47 	 0.05177 	 0.00384 	 m..s
   52 	    48 	 0.05005 	 0.00385 	 m..s
   45 	    49 	 0.04971 	 0.00387 	 m..s
   40 	    50 	 0.04884 	 0.00389 	 m..s
   60 	    51 	 0.05061 	 0.00391 	 m..s
   92 	    52 	 0.05203 	 0.00392 	 m..s
   34 	    53 	 0.04839 	 0.00394 	 m..s
   54 	    54 	 0.05026 	 0.00395 	 m..s
   64 	    55 	 0.05098 	 0.00400 	 m..s
   67 	    56 	 0.05177 	 0.00401 	 m..s
   61 	    57 	 0.05071 	 0.00402 	 m..s
   67 	    58 	 0.05177 	 0.00410 	 m..s
    3 	    59 	 0.04523 	 0.00410 	 m..s
    9 	    60 	 0.04574 	 0.00414 	 m..s
   67 	    61 	 0.05177 	 0.00415 	 m..s
   47 	    62 	 0.04981 	 0.00415 	 m..s
    1 	    63 	 0.04467 	 0.00418 	 m..s
   20 	    64 	 0.04732 	 0.00419 	 m..s
    7 	    65 	 0.04551 	 0.00421 	 m..s
   28 	    66 	 0.04771 	 0.00428 	 m..s
   36 	    67 	 0.04859 	 0.00428 	 m..s
   57 	    68 	 0.05045 	 0.00437 	 m..s
   21 	    69 	 0.04735 	 0.00438 	 m..s
    4 	    70 	 0.04527 	 0.00444 	 m..s
   19 	    71 	 0.04722 	 0.00447 	 m..s
   49 	    72 	 0.04985 	 0.00449 	 m..s
   96 	    73 	 0.05344 	 0.00452 	 m..s
   29 	    74 	 0.04793 	 0.00455 	 m..s
   11 	    75 	 0.04630 	 0.00460 	 m..s
   31 	    76 	 0.04805 	 0.00464 	 m..s
   33 	    77 	 0.04829 	 0.00464 	 m..s
   10 	    78 	 0.04610 	 0.00467 	 m..s
   67 	    79 	 0.05177 	 0.00469 	 m..s
   94 	    80 	 0.05262 	 0.00473 	 m..s
   93 	    81 	 0.05241 	 0.00474 	 m..s
    8 	    82 	 0.04557 	 0.00495 	 m..s
   67 	    83 	 0.05177 	 0.00498 	 m..s
   12 	    84 	 0.04632 	 0.00518 	 m..s
   67 	    85 	 0.05177 	 0.00524 	 m..s
   35 	    86 	 0.04848 	 0.00531 	 m..s
   32 	    87 	 0.04825 	 0.00533 	 m..s
   55 	    88 	 0.05040 	 0.00574 	 m..s
   67 	    89 	 0.05177 	 0.00610 	 m..s
   58 	    90 	 0.05048 	 0.00711 	 m..s
   63 	    91 	 0.05090 	 0.01098 	 m..s
    6 	    92 	 0.04531 	 0.01156 	 m..s
    0 	    93 	 0.04420 	 0.02355 	 ~...
   67 	    94 	 0.05177 	 0.03010 	 ~...
   95 	    95 	 0.05317 	 0.03153 	 ~...
   38 	    96 	 0.04864 	 0.03330 	 ~...
  106 	    97 	 0.18250 	 0.06382 	 MISS
  100 	    98 	 0.12720 	 0.11435 	 ~...
   98 	    99 	 0.11967 	 0.12561 	 ~...
  102 	   100 	 0.13452 	 0.13162 	 ~...
   99 	   101 	 0.12627 	 0.13345 	 ~...
  101 	   102 	 0.13168 	 0.13484 	 ~...
   97 	   103 	 0.11629 	 0.13765 	 ~...
  103 	   104 	 0.16960 	 0.17383 	 ~...
  105 	   105 	 0.17384 	 0.17409 	 ~...
  104 	   106 	 0.17131 	 0.18924 	 ~...
  107 	   107 	 0.18502 	 0.19343 	 ~...
  108 	   108 	 0.18695 	 0.19847 	 ~...
  109 	   109 	 0.18795 	 0.20961 	 ~...
  112 	   110 	 0.24459 	 0.24983 	 ~...
  110 	   111 	 0.22066 	 0.25504 	 m..s
  118 	   112 	 0.30054 	 0.26879 	 m..s
  113 	   113 	 0.24603 	 0.27244 	 ~...
  111 	   114 	 0.24370 	 0.28283 	 m..s
  117 	   115 	 0.29454 	 0.28931 	 ~...
  114 	   116 	 0.25937 	 0.29091 	 m..s
  115 	   117 	 0.27519 	 0.29538 	 ~...
  120 	   118 	 0.30368 	 0.31529 	 ~...
  116 	   119 	 0.28960 	 0.32643 	 m..s
  119 	   120 	 0.30154 	 0.34968 	 m..s
==========================================
r_mrr = 0.9845721125602722
r2_mrr = 0.7805743217468262
spearmanr_mrr@5 = 0.917407214641571
spearmanr_mrr@10 = 0.8843406438827515
spearmanr_mrr@50 = 0.9932353496551514
spearmanr_mrr@100 = 0.994907796382904
spearmanr_mrr@All = 0.9951385259628296
==========================================
test time: 0.447
Done Testing dataset CoDExSmall
total time taken: 235.53453516960144
training time taken: 224.66536045074463
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9846)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7806)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9174)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.8843)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9932)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9949)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9951)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.3027237367854809}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 4865937335602121
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1055, 1161, 0, 268, 55, 615, 1147, 285, 127, 1090, 418, 22, 313, 650, 480, 701, 484, 441, 751, 174, 910, 329, 711, 217, 662, 82, 1195, 1023, 1006, 457, 599, 558, 597, 353, 1018, 808, 271, 159, 476, 1100, 979, 557, 863, 538, 1191, 459, 641, 181, 422, 88, 300, 167, 496, 410, 115, 594, 371, 477, 1002, 565, 664, 344, 519, 391, 355, 109, 897, 70, 393, 1099, 499, 833, 603, 1039, 66, 1182, 535, 868, 908, 1091, 569, 302, 735, 988, 673, 246, 261, 307, 1128, 705, 258, 166, 502, 1185, 706, 514, 913, 992, 560, 1122, 610, 479, 63, 928, 243, 583, 1125, 952, 1050, 368, 450, 1213, 57, 256, 481, 561, 663, 257, 354, 1146, 428]
valid_ids (0): []
train_ids (1094): [80, 1035, 96, 634, 85, 782, 624, 43, 1202, 58, 991, 417, 239, 1065, 99, 28, 842, 339, 1133, 629, 3, 759, 554, 687, 406, 674, 32, 856, 654, 986, 971, 377, 1109, 154, 728, 815, 215, 1017, 400, 171, 108, 501, 273, 244, 320, 145, 446, 780, 505, 537, 464, 289, 297, 439, 581, 675, 164, 125, 282, 420, 52, 325, 1079, 158, 409, 324, 331, 838, 390, 413, 970, 186, 528, 612, 685, 704, 852, 295, 846, 608, 926, 357, 267, 894, 1108, 813, 298, 596, 632, 427, 882, 646, 720, 322, 493, 890, 1, 767, 648, 434, 198, 670, 948, 475, 415, 817, 911, 684, 1036, 254, 1180, 200, 68, 693, 540, 497, 916, 488, 382, 136, 1139, 772, 671, 487, 380, 350, 1003, 627, 474, 59, 930, 1208, 630, 750, 823, 379, 990, 375, 722, 702, 1145, 791, 1151, 444, 1193, 749, 605, 492, 53, 721, 1165, 549, 114, 276, 442, 881, 809, 618, 311, 511, 64, 983, 1038, 1061, 414, 907, 33, 194, 843, 1166, 253, 423, 788, 521, 977, 151, 31, 958, 678, 438, 577, 29, 1105, 279, 1027, 572, 367, 349, 814, 1007, 452, 697, 770, 607, 252, 707, 359, 1093, 968, 310, 1075, 250, 864, 216, 448, 5, 466, 1067, 1062, 976, 133, 898, 242, 424, 939, 544, 529, 604, 730, 1110, 163, 914, 639, 614, 854, 384, 912, 161, 1088, 628, 700, 798, 471, 736, 828, 709, 978, 202, 71, 472, 765, 555, 328, 1121, 389, 361, 517, 113, 110, 531, 48, 1092, 433, 606, 436, 146, 1207, 1011, 224, 197, 314, 695, 1179, 431, 399, 1203, 1005, 786, 762, 613, 92, 682, 394, 89, 713, 338, 90, 1137, 525, 965, 666, 345, 625, 232, 533, 172, 228, 46, 1021, 262, 1031, 130, 981, 7, 333, 909, 102, 223, 959, 1184, 771, 588, 1154, 879, 160, 369, 378, 611, 238, 951, 327, 1004, 1096, 12, 787, 547, 1160, 1149, 392, 1214, 723, 1025, 539, 1186, 315, 779, 822, 1008, 1115, 1032, 845, 857, 542, 974, 373, 653, 1033, 935, 587, 1010, 901, 1177, 1117, 938, 699, 348, 1116, 647, 893, 155, 1168, 1014, 601, 347, 1152, 411, 849, 11, 342, 169, 1150, 1054, 218, 995, 245, 803, 530, 138, 876, 177, 75, 884, 1140, 1045, 123, 463, 508, 1173, 1157, 1012, 826, 582, 103, 1049, 1174, 426, 987, 874, 1169, 638, 340, 287, 211, 567, 676, 408, 906, 1101, 1175, 84, 1148, 954, 1042, 686, 147, 1124, 343, 1172, 545, 150, 24, 17, 1070, 1041, 326, 967, 729, 21, 1136, 960, 78, 1197, 226, 1053, 972, 124, 1131, 510, 39, 1094, 998, 792, 235, 1156, 407, 1126, 563, 1026, 396, 225, 626, 805, 67, 458, 689, 1028, 144, 858, 934, 1142, 73, 131, 398, 830, 72, 1013, 896, 1051, 550, 937, 278, 506, 139, 187, 461, 299, 86, 421, 260, 1196, 366, 334, 173, 552, 395, 887, 34, 251, 30, 973, 1058, 1081, 571, 850, 284, 559, 761, 292, 275, 281, 1107, 575, 230, 1123, 790, 162, 1127, 157, 305, 668, 41, 240, 220, 236, 241, 500, 209, 925, 248, 207, 490, 467, 799, 963, 853, 383, 140, 49, 920, 1144, 1106, 744, 265, 755, 45, 1016, 919, 62, 902, 860, 997, 796, 401, 129, 742, 593, 91, 435, 941, 1155, 1095, 516, 358, 824, 376, 756, 494, 93, 316, 453, 1052, 523, 1143, 365, 25, 219, 1046, 969, 1057, 306, 449, 797, 694, 922, 107, 522, 135, 1076, 97, 768, 203, 753, 231, 121, 915, 622, 763, 748, 949, 984, 374, 69, 1135, 760, 841, 955, 179, 861, 489, 994, 943, 795, 385, 222, 623, 870, 586, 579, 512, 769, 35, 156, 341, 2, 364, 54, 717, 644, 176, 964, 255, 1212, 335, 180, 1060, 1112, 65, 775, 1114, 1043, 1086, 1190, 178, 580, 867, 405, 498, 134, 50, 1059, 773, 524, 945, 989, 195, 620, 76, 381, 658, 81, 142, 503, 1069, 286, 532, 570, 272, 309, 42, 738, 1118, 437, 456, 362, 18, 921, 486, 642, 1178, 621, 892, 757, 740, 865, 903, 714, 175, 266, 589, 568, 553, 1200, 1083, 304, 683, 840, 1015, 551, 1097, 478, 877, 672, 754, 468, 889, 708, 848, 534, 19, 527, 221, 346, 541, 895, 318, 206, 677, 758, 564, 932, 982, 386, 504, 105, 1111, 847, 283, 962, 412, 462, 372, 834, 189, 880, 578, 183, 126, 337, 652, 835, 1119, 227, 878, 56, 637, 746, 141, 1211, 270, 118, 387, 691, 120, 485, 844, 811, 592, 669, 851, 430, 839, 77, 956, 293, 1198, 831, 660, 402, 111, 429, 388, 1064, 659, 696, 731, 469, 900, 233, 716, 1044, 509, 192, 301, 872, 330, 360, 741, 518, 112, 184, 100, 36, 122, 931, 1089, 715, 781, 148, 303, 432, 288, 832, 1192, 739, 15, 566, 598, 319, 927, 836, 264, 980, 404, 536, 947, 1134, 13, 263, 869, 454, 196, 645, 655, 871, 837, 886, 212, 665, 4, 810, 1205, 933, 595, 576, 923, 515, 859, 445, 204, 710, 143, 1080, 363, 40, 294, 27, 680, 513, 416, 205, 208, 1162, 918, 117, 210, 229, 866, 1153, 74, 996, 60, 460, 829, 600, 1138, 734, 789, 774, 875, 153, 26, 1085, 336, 1048, 946, 280, 1072, 419, 1063, 816, 801, 95, 317, 1171, 584, 785, 985, 312, 1030, 793, 507, 104, 820, 899, 616, 905, 827, 885, 1037, 891, 1199, 649, 1129, 784, 1001, 259, 688, 602, 732, 247, 690, 548, 352, 1077, 465, 573, 1034, 745, 888, 291, 1098, 1141, 116, 679, 719, 1047, 1159, 802, 188, 783, 1084, 633, 296, 546, 79, 1024, 590, 447, 966, 617, 37, 483, 764, 942, 777, 993, 961, 778, 370, 149, 8, 323, 1040, 1000, 1074, 440, 718, 562, 574, 1188, 1164, 403, 940, 269, 185, 98, 168, 692, 1029, 950, 609, 1181, 543, 1102, 128, 1056, 152, 165, 234, 51, 862, 491, 87, 308, 635, 1163, 455, 743, 667, 94, 1130, 924, 20, 356, 277, 1078, 1113, 1132, 1183, 636, 1167, 591, 193, 1009, 1073, 651, 332, 213, 199, 556, 495, 137, 482, 643, 725, 526, 191, 657, 703, 61, 1066, 656, 201, 776, 470, 1120, 1189, 681, 6, 443, 47, 812, 727, 1206, 904, 807, 917, 1201, 747, 16, 726, 1104, 724, 1176, 1158, 975, 936, 1071, 1022, 818, 619, 1170, 14, 9, 929, 873, 821, 794, 883, 473, 1187, 451, 38, 855, 190, 766, 640, 1020, 804, 274, 712, 1019, 132, 397, 1209, 999, 10, 119, 520, 1210, 800, 752, 290, 170, 83, 351, 698, 631, 737, 214, 1194, 237, 953, 425, 23, 106, 944, 182, 1103, 585, 1087, 733, 321, 806, 1204, 825, 957, 44, 819, 1082, 661, 249, 1068, 101]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3234694198400357
the save name prefix for this run is:  chkpt-ID_3234694198400357_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 864
rank avg (pred): 0.562 +- 0.003
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.0002723535

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1070
rank avg (pred): 0.047 +- 0.004
mrr vals (pred, true): 0.010, 0.282
batch losses (mrrl, rdl): 0.0, 3.06e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 443
rank avg (pred): 0.495 +- 0.279
mrr vals (pred, true): 0.053, 0.003
batch losses (mrrl, rdl): 0.0, 1.05391e-05

Epoch over!
epoch time: 14.846

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 340
rank avg (pred): 0.448 +- 0.270
mrr vals (pred, true): 0.067, 0.004
batch losses (mrrl, rdl): 0.0, 2.3879e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 140
rank avg (pred): 0.461 +- 0.282
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0, 2.0914e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 789
rank avg (pred): 0.516 +- 0.279
mrr vals (pred, true): 0.035, 0.004
batch losses (mrrl, rdl): 0.0, 3.73426e-05

Epoch over!
epoch time: 14.78

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.475 +- 0.279
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001410087

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 107
rank avg (pred): 0.474 +- 0.282
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0, 3.236e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.504 +- 0.286
mrr vals (pred, true): 0.028, 0.004
batch losses (mrrl, rdl): 0.0, 9.954e-06

Epoch over!
epoch time: 14.777

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.474 +- 0.286
mrr vals (pred, true): 0.056, 0.004
batch losses (mrrl, rdl): 0.0, 4.942e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 170
rank avg (pred): 0.472 +- 0.279
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.0, 1.6972e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 644
rank avg (pred): 0.483 +- 0.282
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 0.0, 5.151e-07

Epoch over!
epoch time: 14.794

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.045 +- 0.125
mrr vals (pred, true): 0.136, 0.251
batch losses (mrrl, rdl): 0.0, 1.3533e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 468
rank avg (pred): 0.484 +- 0.285
mrr vals (pred, true): 0.062, 0.004
batch losses (mrrl, rdl): 0.0, 4.277e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 48
rank avg (pred): 0.037 +- 0.090
mrr vals (pred, true): 0.112, 0.132
batch losses (mrrl, rdl): 0.0, 4.76749e-05

Epoch over!
epoch time: 14.775

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 530
rank avg (pred): 0.406 +- 0.281
mrr vals (pred, true): 0.057, 0.008
batch losses (mrrl, rdl): 0.000520908, 8.1986e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.056 +- 0.073
mrr vals (pred, true): 0.202, 0.304
batch losses (mrrl, rdl): 0.1051550955, 6.6377e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.463 +- 0.172
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001327803, 4.06377e-05

Epoch over!
epoch time: 14.984

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 394
rank avg (pred): 0.489 +- 0.160
mrr vals (pred, true): 0.032, 0.004
batch losses (mrrl, rdl): 0.0031527542, 7.97619e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.451 +- 0.178
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 2.60121e-05, 4.28168e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 676
rank avg (pred): 0.449 +- 0.185
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 9.49447e-05, 3.69733e-05

Epoch over!
epoch time: 14.966

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 803
rank avg (pred): 0.459 +- 0.178
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.000213131, 4.69017e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 333
rank avg (pred): 0.466 +- 0.186
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001494761, 3.90179e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 926
rank avg (pred): 0.463 +- 0.176
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 5.4872e-06, 0.0007575907

Epoch over!
epoch time: 14.978

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 855
rank avg (pred): 0.469 +- 0.175
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 6.9789e-06, 4.18261e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 510
rank avg (pred): 0.352 +- 0.195
mrr vals (pred, true): 0.073, 0.047
batch losses (mrrl, rdl): 0.0053983117, 0.0001446587

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 996
rank avg (pred): 0.060 +- 0.064
mrr vals (pred, true): 0.253, 0.316
batch losses (mrrl, rdl): 0.0389149338, 8.4617e-06

Epoch over!
epoch time: 14.989

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.102 +- 0.098
mrr vals (pred, true): 0.188, 0.174
batch losses (mrrl, rdl): 0.0018055025, 2.14626e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 469
rank avg (pred): 0.473 +- 0.172
mrr vals (pred, true): 0.053, 0.005
batch losses (mrrl, rdl): 6.49911e-05, 4.06509e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 681
rank avg (pred): 0.440 +- 0.170
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.474e-07, 5.58088e-05

Epoch over!
epoch time: 14.981

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 463
rank avg (pred): 0.457 +- 0.177
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 1.64624e-05, 3.91141e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.483 +- 0.166
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.6158e-06, 5.56976e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 896
rank avg (pred): 0.355 +- 0.202
mrr vals (pred, true): 0.067, 0.001
batch losses (mrrl, rdl): 0.0028232154, 0.0008120838

Epoch over!
epoch time: 14.985

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1011
rank avg (pred): 0.446 +- 0.185
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0008074411, 4.57654e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 922
rank avg (pred): 0.482 +- 0.154
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 5.5895e-06, 0.0002574342

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 11
rank avg (pred): 0.064 +- 0.070
mrr vals (pred, true): 0.259, 0.264
batch losses (mrrl, rdl): 0.0002391992, 6.5241e-06

Epoch over!
epoch time: 14.965

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.452 +- 0.165
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001673852, 4.87166e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 874
rank avg (pred): 0.484 +- 0.160
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 6.1769e-06, 5.18367e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1150
rank avg (pred): 0.368 +- 0.193
mrr vals (pred, true): 0.061, 0.030
batch losses (mrrl, rdl): 0.0011770627, 0.0005244568

Epoch over!
epoch time: 14.936

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 316
rank avg (pred): 0.131 +- 0.118
mrr vals (pred, true): 0.188, 0.175
batch losses (mrrl, rdl): 0.0016701478, 7.9465e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 84
rank avg (pred): 0.489 +- 0.144
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 3.46619e-05, 7.40913e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 517
rank avg (pred): 0.423 +- 0.169
mrr vals (pred, true): 0.048, 0.013
batch losses (mrrl, rdl): 4.0917e-05, 0.0001424022

Epoch over!
epoch time: 14.95

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 429
rank avg (pred): 0.502 +- 0.131
mrr vals (pred, true): 0.042, 0.006
batch losses (mrrl, rdl): 0.000571358, 0.0001046568

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.415 +- 0.172
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 5.90747e-05, 8.19681e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 374
rank avg (pred): 0.491 +- 0.140
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 1.75693e-05, 6.42538e-05

Epoch over!
epoch time: 14.955

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.040 +- 0.040
mrr vals (pred, true): 0.290, 0.249

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   81 	     0 	 0.05400 	 0.00087 	 m..s
   84 	     1 	 0.05565 	 0.00088 	 m..s
   80 	     2 	 0.05392 	 0.00102 	 m..s
   93 	     3 	 0.05942 	 0.00143 	 m..s
   82 	     4 	 0.05498 	 0.00208 	 m..s
   60 	     5 	 0.05223 	 0.00226 	 m..s
   10 	     6 	 0.04748 	 0.00250 	 m..s
   12 	     7 	 0.04781 	 0.00250 	 m..s
   49 	     8 	 0.05097 	 0.00261 	 m..s
   52 	     9 	 0.05107 	 0.00262 	 m..s
   35 	    10 	 0.05027 	 0.00295 	 m..s
    8 	    11 	 0.04660 	 0.00297 	 m..s
    7 	    12 	 0.04656 	 0.00304 	 m..s
   48 	    13 	 0.05083 	 0.00308 	 m..s
   50 	    14 	 0.05099 	 0.00321 	 m..s
    1 	    15 	 0.04282 	 0.00327 	 m..s
   17 	    16 	 0.04852 	 0.00331 	 m..s
   67 	    17 	 0.05317 	 0.00331 	 m..s
   25 	    18 	 0.04932 	 0.00331 	 m..s
   55 	    19 	 0.05174 	 0.00332 	 m..s
   65 	    20 	 0.05253 	 0.00333 	 m..s
   59 	    21 	 0.05202 	 0.00336 	 m..s
   28 	    22 	 0.04955 	 0.00340 	 m..s
   31 	    23 	 0.05007 	 0.00344 	 m..s
   47 	    24 	 0.05076 	 0.00344 	 m..s
   78 	    25 	 0.05386 	 0.00361 	 m..s
    9 	    26 	 0.04679 	 0.00362 	 m..s
   26 	    27 	 0.04936 	 0.00371 	 m..s
   29 	    28 	 0.04963 	 0.00373 	 m..s
    4 	    29 	 0.04649 	 0.00373 	 m..s
   56 	    30 	 0.05184 	 0.00377 	 m..s
   51 	    31 	 0.05103 	 0.00377 	 m..s
    2 	    32 	 0.04623 	 0.00381 	 m..s
   18 	    33 	 0.04861 	 0.00385 	 m..s
    6 	    34 	 0.04652 	 0.00387 	 m..s
   92 	    35 	 0.05843 	 0.00389 	 m..s
   53 	    36 	 0.05111 	 0.00391 	 m..s
   33 	    37 	 0.05013 	 0.00393 	 m..s
   76 	    38 	 0.05380 	 0.00395 	 m..s
   13 	    39 	 0.04783 	 0.00396 	 m..s
   45 	    40 	 0.05069 	 0.00398 	 m..s
   90 	    41 	 0.05827 	 0.00398 	 m..s
   14 	    42 	 0.04805 	 0.00399 	 m..s
   58 	    43 	 0.05196 	 0.00402 	 m..s
   79 	    44 	 0.05389 	 0.00405 	 m..s
    5 	    45 	 0.04649 	 0.00406 	 m..s
   27 	    46 	 0.04944 	 0.00410 	 m..s
   89 	    47 	 0.05822 	 0.00410 	 m..s
   34 	    48 	 0.05014 	 0.00410 	 m..s
   24 	    49 	 0.04926 	 0.00412 	 m..s
   23 	    50 	 0.04921 	 0.00413 	 m..s
   83 	    51 	 0.05530 	 0.00414 	 m..s
   74 	    52 	 0.05369 	 0.00417 	 m..s
   20 	    53 	 0.04887 	 0.00417 	 m..s
   72 	    54 	 0.05363 	 0.00418 	 m..s
   91 	    55 	 0.05836 	 0.00421 	 m..s
    0 	    56 	 0.04202 	 0.00428 	 m..s
   85 	    57 	 0.05684 	 0.00429 	 m..s
   15 	    58 	 0.04837 	 0.00431 	 m..s
   68 	    59 	 0.05325 	 0.00441 	 m..s
   21 	    60 	 0.04887 	 0.00441 	 m..s
   11 	    61 	 0.04750 	 0.00445 	 m..s
   69 	    62 	 0.05326 	 0.00451 	 m..s
   54 	    63 	 0.05114 	 0.00452 	 m..s
   87 	    64 	 0.05729 	 0.00452 	 m..s
   62 	    65 	 0.05232 	 0.00464 	 m..s
   77 	    66 	 0.05385 	 0.00473 	 m..s
   71 	    67 	 0.05344 	 0.00478 	 m..s
   40 	    68 	 0.05038 	 0.00478 	 m..s
   16 	    69 	 0.04838 	 0.00505 	 m..s
   19 	    70 	 0.04882 	 0.00505 	 m..s
   66 	    71 	 0.05297 	 0.00505 	 m..s
   42 	    72 	 0.05052 	 0.00508 	 m..s
   70 	    73 	 0.05326 	 0.00518 	 m..s
    3 	    74 	 0.04646 	 0.00524 	 m..s
   37 	    75 	 0.05031 	 0.00528 	 m..s
   86 	    76 	 0.05721 	 0.00548 	 m..s
   88 	    77 	 0.05736 	 0.00550 	 m..s
   44 	    78 	 0.05067 	 0.00571 	 m..s
   63 	    79 	 0.05241 	 0.00579 	 m..s
   22 	    80 	 0.04911 	 0.00588 	 m..s
   36 	    81 	 0.05030 	 0.00601 	 m..s
   41 	    82 	 0.05040 	 0.00677 	 m..s
   43 	    83 	 0.05057 	 0.00680 	 m..s
   46 	    84 	 0.05075 	 0.00729 	 m..s
   39 	    85 	 0.05032 	 0.00780 	 m..s
   30 	    86 	 0.04979 	 0.00815 	 m..s
   38 	    87 	 0.05031 	 0.01105 	 m..s
   32 	    88 	 0.05009 	 0.01156 	 m..s
   57 	    89 	 0.05196 	 0.02742 	 ~...
   61 	    90 	 0.05230 	 0.02763 	 ~...
   73 	    91 	 0.05367 	 0.02838 	 ~...
   75 	    92 	 0.05379 	 0.02854 	 ~...
   64 	    93 	 0.05241 	 0.02863 	 ~...
   99 	    94 	 0.14084 	 0.07075 	 m..s
   94 	    95 	 0.12287 	 0.11435 	 ~...
   95 	    96 	 0.12319 	 0.12873 	 ~...
   96 	    97 	 0.12476 	 0.13140 	 ~...
   97 	    98 	 0.12637 	 0.13345 	 ~...
   98 	    99 	 0.12769 	 0.13525 	 ~...
  107 	   100 	 0.22692 	 0.15098 	 m..s
  100 	   101 	 0.17712 	 0.17992 	 ~...
  103 	   102 	 0.18578 	 0.18489 	 ~...
  104 	   103 	 0.18629 	 0.18796 	 ~...
  101 	   104 	 0.17864 	 0.18907 	 ~...
  102 	   105 	 0.18539 	 0.18924 	 ~...
  105 	   106 	 0.20041 	 0.19140 	 ~...
  106 	   107 	 0.21815 	 0.19824 	 ~...
  109 	   108 	 0.24779 	 0.21484 	 m..s
  114 	   109 	 0.29037 	 0.24879 	 m..s
  110 	   110 	 0.27346 	 0.25269 	 ~...
  108 	   111 	 0.24179 	 0.25712 	 ~...
  111 	   112 	 0.27809 	 0.26879 	 ~...
  112 	   113 	 0.27843 	 0.27192 	 ~...
  113 	   114 	 0.27910 	 0.28482 	 ~...
  115 	   115 	 0.29095 	 0.29610 	 ~...
  117 	   116 	 0.30189 	 0.30783 	 ~...
  118 	   117 	 0.31354 	 0.31181 	 ~...
  116 	   118 	 0.29197 	 0.31475 	 ~...
  120 	   119 	 0.34570 	 0.32643 	 ~...
  119 	   120 	 0.31751 	 0.33494 	 ~...
==========================================
r_mrr = 0.9888634085655212
r2_mrr = 0.8017004728317261
spearmanr_mrr@5 = 0.9505594372749329
spearmanr_mrr@10 = 0.9023712277412415
spearmanr_mrr@50 = 0.9923667311668396
spearmanr_mrr@100 = 0.9946514964103699
spearmanr_mrr@All = 0.9949408173561096
==========================================
test time: 0.454
Done Testing dataset CoDExSmall
total time taken: 234.56961941719055
training time taken: 224.12762093544006
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9889)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8017)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9506)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9024)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9924)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9947)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9949)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.2241682511248655}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7725030105057986
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [301, 116, 545, 655, 507, 808, 279, 620, 934, 531, 306, 774, 679, 510, 275, 336, 891, 85, 1150, 501, 63, 742, 1038, 413, 985, 441, 1044, 21, 383, 331, 417, 361, 1065, 257, 1119, 897, 863, 644, 15, 781, 1204, 427, 1132, 928, 231, 22, 740, 1072, 238, 1166, 31, 323, 834, 726, 298, 590, 734, 838, 699, 70, 552, 573, 93, 661, 146, 340, 927, 233, 641, 457, 779, 804, 20, 80, 468, 1196, 990, 518, 259, 855, 443, 579, 872, 302, 307, 540, 470, 90, 1016, 71, 596, 769, 1013, 1086, 992, 577, 1155, 1170, 627, 585, 938, 1, 646, 47, 462, 162, 278, 519, 756, 780, 473, 1001, 1141, 542, 304, 2, 913, 593, 1080, 961, 1022]
valid_ids (0): []
train_ids (1094): [55, 1151, 1164, 114, 725, 1110, 23, 430, 980, 280, 270, 757, 1039, 5, 475, 1193, 294, 1210, 1199, 467, 956, 801, 944, 782, 669, 532, 74, 1056, 724, 404, 1098, 563, 662, 766, 929, 344, 589, 460, 486, 712, 647, 185, 155, 1051, 17, 795, 166, 535, 628, 262, 654, 168, 131, 741, 14, 823, 1172, 702, 154, 525, 978, 659, 521, 458, 35, 260, 1178, 365, 342, 435, 497, 949, 343, 377, 536, 952, 1078, 369, 1027, 140, 332, 1055, 480, 715, 1106, 748, 642, 1198, 713, 558, 771, 72, 201, 737, 1123, 300, 657, 1111, 937, 1108, 833, 49, 348, 445, 656, 173, 447, 223, 451, 328, 407, 1003, 33, 264, 122, 719, 1160, 1149, 842, 547, 236, 248, 1100, 1168, 682, 494, 1214, 723, 574, 706, 971, 514, 106, 743, 676, 1006, 172, 1125, 539, 50, 947, 1101, 61, 925, 123, 1159, 568, 580, 455, 720, 45, 1138, 548, 556, 819, 469, 733, 716, 1209, 511, 115, 412, 605, 625, 767, 885, 1020, 1174, 1034, 516, 993, 564, 1052, 835, 75, 89, 1126, 1112, 1103, 479, 354, 253, 826, 637, 895, 129, 700, 554, 148, 82, 705, 1181, 493, 128, 1068, 1029, 915, 936, 1036, 1095, 143, 919, 698, 452, 324, 425, 630, 889, 973, 920, 738, 488, 345, 350, 164, 822, 42, 884, 764, 603, 388, 793, 492, 672, 239, 1122, 753, 1114, 214, 965, 312, 871, 401, 687, 308, 87, 193, 418, 776, 629, 138, 689, 101, 176, 600, 466, 640, 831, 946, 349, 330, 530, 960, 1010, 792, 69, 954, 1124, 703, 1066, 261, 760, 235, 569, 91, 775, 572, 132, 1088, 81, 180, 648, 1075, 597, 945, 761, 496, 645, 426, 1041, 843, 839, 228, 18, 245, 395, 456, 1045, 896, 989, 1043, 942, 582, 650, 1211, 249, 1037, 1182, 744, 181, 205, 732, 446, 624, 751, 272, 777, 914, 890, 635, 282, 633, 1208, 9, 99, 52, 88, 608, 851, 459, 288, 192, 387, 133, 1069, 537, 364, 359, 865, 51, 1186, 297, 1202, 873, 828, 182, 1053, 7, 799, 951, 158, 394, 184, 610, 1049, 905, 113, 218, 1116, 136, 888, 832, 190, 338, 968, 156, 159, 841, 255, 611, 879, 752, 199, 322, 675, 534, 24, 296, 111, 487, 878, 500, 363, 440, 506, 1063, 187, 119, 745, 276, 40, 754, 824, 68, 416, 287, 329, 232, 202, 333, 442, 549, 759, 571, 1131, 104, 43, 478, 722, 1105, 1017, 830, 1189, 439, 1008, 805, 415, 314, 1213, 988, 763, 846, 341, 327, 912, 1176, 875, 1084, 319, 709, 1067, 1060, 1071, 1096, 73, 996, 810, 584, 224, 557, 243, 286, 449, 1000, 592, 357, 171, 433, 562, 247, 1152, 409, 677, 267, 30, 38, 615, 870, 292, 790, 718, 538, 124, 178, 815, 921, 299, 567, 303, 482, 1021, 188, 693, 665, 1093, 750, 371, 317, 546, 150, 170, 561, 1135, 550, 398, 269, 1033, 707, 880, 935, 27, 126, 410, 1062, 768, 576, 903, 34, 599, 714, 421, 898, 522, 32, 673, 681, 972, 337, 382, 1073, 786, 432, 555, 708, 477, 678, 392, 553, 614, 1154, 200, 339, 1127, 785, 213, 840, 66, 623, 334, 1167, 103, 1087, 56, 802, 1134, 4, 1032, 220, 854, 244, 1142, 861, 6, 773, 1083, 995, 836, 606, 1023, 520, 64, 789, 523, 619, 1057, 1156, 121, 450, 651, 79, 994, 1185, 1194, 77, 1205, 977, 591, 1070, 618, 882, 403, 818, 1201, 527, 739, 108, 356, 899, 551, 174, 194, 736, 886, 1207, 290, 483, 429, 1148, 1143, 526, 755, 622, 1146, 346, 969, 1091, 1175, 1158, 222, 953, 1012, 820, 437, 316, 1042, 436, 320, 62, 251, 428, 959, 1104, 175, 1019, 313, 894, 691, 643, 83, 283, 1030, 604, 144, 422, 153, 680, 503, 1117, 731, 911, 1007, 25, 293, 97, 1128, 1092, 36, 1177, 533, 237, 474, 86, 570, 701, 987, 142, 632, 67, 495, 234, 844, 671, 966, 12, 216, 370, 970, 240, 660, 710, 916, 1015, 794, 639, 504, 940, 29, 607, 226, 609, 408, 102, 274, 950, 1058, 1097, 1048, 207, 285, 225, 1031, 717, 860, 509, 814, 221, 845, 380, 310, 908, 747, 1165, 362, 1139, 943, 37, 1035, 529, 1183, 137, 683, 490, 800, 857, 812, 867, 909, 163, 694, 204, 258, 697, 784, 246, 157, 385, 1157, 1147, 1018, 1107, 1144, 979, 621, 612, 256, 1094, 758, 997, 378, 864, 360, 1187, 1133, 183, 877, 721, 165, 930, 636, 829, 368, 263, 729, 1140, 1191, 100, 809, 684, 791, 229, 352, 1180, 438, 668, 1171, 955, 664, 1005, 57, 983, 400, 465, 197, 321, 666, 981, 453, 986, 1009, 1089, 134, 727, 964, 711, 230, 289, 852, 481, 1195, 749, 8, 1184, 318, 464, 1120, 783, 1059, 375, 167, 746, 1203, 941, 271, 856, 900, 817, 3, 1173, 76, 849, 141, 1077, 663, 414, 347, 59, 517, 353, 991, 1099, 16, 1047, 396, 811, 351, 859, 39, 196, 858, 893, 638, 728, 98, 94, 0, 125, 384, 420, 367, 444, 179, 847, 389, 219, 528, 393, 513, 565, 411, 907, 118, 982, 1024, 19, 373, 594, 1061, 797, 762, 1206, 559, 881, 1074, 78, 212, 962, 1102, 117, 160, 848, 177, 787, 967, 11, 53, 151, 386, 44, 311, 887, 406, 215, 381, 120, 397, 512, 827, 1130, 670, 326, 1064, 471, 1082, 203, 999, 1054, 379, 975, 883, 335, 1136, 273, 152, 112, 423, 926, 601, 26, 617, 575, 498, 765, 1081, 1161, 46, 515, 974, 667, 1090, 1169, 472, 1200, 135, 948, 505, 92, 1026, 252, 431, 217, 524, 130, 268, 1014, 1145, 209, 544, 807, 1115, 798, 54, 543, 295, 1076, 206, 391, 692, 602, 837, 1050, 109, 1011, 189, 541, 901, 254, 695, 476, 616, 566, 191, 922, 374, 910, 1153, 735, 850, 902, 658, 1197, 147, 690, 1046, 918, 1025, 1113, 1190, 1109, 778, 399, 502, 281, 816, 1163, 366, 241, 208, 957, 613, 402, 485, 499, 1085, 923, 508, 862, 95, 931, 1137, 376, 932, 419, 315, 250, 491, 372, 355, 998, 41, 578, 1188, 704, 560, 463, 1129, 906, 586, 796, 358, 461, 581, 853, 211, 96, 60, 917, 48, 107, 813, 868, 291, 127, 730, 325, 821, 933, 424, 65, 28, 110, 186, 806, 688, 169, 1004, 984, 454, 772, 788, 242, 1121, 198, 145, 595, 284, 869, 1162, 652, 305, 434, 924, 265, 1028, 583, 976, 277, 649, 390, 825, 1192, 803, 13, 195, 405, 770, 588, 161, 139, 266, 105, 587, 1002, 876, 674, 598, 1040, 1179, 1118, 149, 686, 634, 1212, 58, 84, 484, 210, 963, 874, 653, 10, 448, 892, 489, 631, 685, 866, 904, 1079, 696, 958, 227, 626, 309, 939]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8226097263966443
the save name prefix for this run is:  chkpt-ID_8226097263966443_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1127
rank avg (pred): 0.473 +- 0.004
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.60089e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 310
rank avg (pred): 0.067 +- 0.032
mrr vals (pred, true): 0.055, 0.188
batch losses (mrrl, rdl): 0.0, 1.00903e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.470 +- 0.275
mrr vals (pred, true): 0.072, 0.003
batch losses (mrrl, rdl): 0.0, 5.1069e-06

Epoch over!
epoch time: 14.811

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 980
rank avg (pred): 0.069 +- 0.060
mrr vals (pred, true): 0.131, 0.334
batch losses (mrrl, rdl): 0.0, 2.43162e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 268
rank avg (pred): 0.051 +- 0.047
mrr vals (pred, true): 0.095, 0.308
batch losses (mrrl, rdl): 0.0, 1.02599e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 384
rank avg (pred): 0.479 +- 0.286
mrr vals (pred, true): 0.023, 0.004
batch losses (mrrl, rdl): 0.0, 6.7249e-06

Epoch over!
epoch time: 14.81

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1169
rank avg (pred): 0.464 +- 0.276
mrr vals (pred, true): 0.019, 0.002
batch losses (mrrl, rdl): 0.0, 2.6529e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1195
rank avg (pred): 0.452 +- 0.274
mrr vals (pred, true): 0.019, 0.004
batch losses (mrrl, rdl): 0.0, 1.10939e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.482 +- 0.294
mrr vals (pred, true): 0.032, 0.003
batch losses (mrrl, rdl): 0.0, 1.0128e-06

Epoch over!
epoch time: 14.847

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 313
rank avg (pred): 0.078 +- 0.122
mrr vals (pred, true): 0.093, 0.189
batch losses (mrrl, rdl): 0.0, 2.2624e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1156
rank avg (pred): 0.249 +- 0.228
mrr vals (pred, true): 0.028, 0.026
batch losses (mrrl, rdl): 0.0, 2.1699e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.036 +- 0.054
mrr vals (pred, true): 0.128, 0.239
batch losses (mrrl, rdl): 0.0, 1.40097e-05

Epoch over!
epoch time: 14.846

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 993
rank avg (pred): 0.038 +- 0.052
mrr vals (pred, true): 0.116, 0.258
batch losses (mrrl, rdl): 0.0, 5.2569e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 4
rank avg (pred): 0.055 +- 0.082
mrr vals (pred, true): 0.117, 0.220
batch losses (mrrl, rdl): 0.0, 1.8567e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 16
rank avg (pred): 0.038 +- 0.057
mrr vals (pred, true): 0.105, 0.234
batch losses (mrrl, rdl): 0.0, 2.8002e-06

Epoch over!
epoch time: 14.881

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 145
rank avg (pred): 0.472 +- 0.286
mrr vals (pred, true): 0.024, 0.005
batch losses (mrrl, rdl): 0.0068808352, 2.878e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 747
rank avg (pred): 0.062 +- 0.033
mrr vals (pred, true): 0.156, 0.186
batch losses (mrrl, rdl): 0.0086081894, 8.2259e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 648
rank avg (pred): 0.449 +- 0.163
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 2.63653e-05, 5.9774e-05

Epoch over!
epoch time: 15.114

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.424 +- 0.158
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0005353014, 0.0001189575

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 772
rank avg (pred): 0.502 +- 0.189
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 2.51849e-05, 4.24363e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 494
rank avg (pred): 0.437 +- 0.148
mrr vals (pred, true): 0.043, 0.018
batch losses (mrrl, rdl): 0.0004907705, 0.0001566015

Epoch over!
epoch time: 15.041

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 68
rank avg (pred): 0.042 +- 0.025
mrr vals (pred, true): 0.203, 0.152
batch losses (mrrl, rdl): 0.0253681056, 4.22234e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 242
rank avg (pred): 0.473 +- 0.177
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0008140262, 4.44529e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1115
rank avg (pred): 0.444 +- 0.150
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0005695651, 7.73266e-05

Epoch over!
epoch time: 15.037

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1110
rank avg (pred): 0.453 +- 0.160
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 7.7798e-05, 6.43837e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1069
rank avg (pred): 0.006 +- 0.003
mrr vals (pred, true): 0.281, 0.289
batch losses (mrrl, rdl): 0.0007242861, 3.72313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 879
rank avg (pred): 0.484 +- 0.165
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0005071305, 5.23811e-05

Epoch over!
epoch time: 15.034

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 811
rank avg (pred): 0.004 +- 0.002
mrr vals (pred, true): 0.313, 0.329
batch losses (mrrl, rdl): 0.0028155502, 2.1124e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.426 +- 0.145
mrr vals (pred, true): 0.053, 0.005
batch losses (mrrl, rdl): 8.38032e-05, 8.10425e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.450 +- 0.147
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0007317768, 5.73784e-05

Epoch over!
epoch time: 15.037

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1191
rank avg (pred): 0.480 +- 0.180
mrr vals (pred, true): 0.055, 0.003
batch losses (mrrl, rdl): 0.0002516297, 3.82215e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 893
rank avg (pred): 0.571 +- 0.223
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.49655e-05, 4.14414e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 955
rank avg (pred): 0.455 +- 0.161
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 9.091e-07, 4.39239e-05

Epoch over!
epoch time: 15.024

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.134 +- 0.087
mrr vals (pred, true): 0.207, 0.173
batch losses (mrrl, rdl): 0.0118870232, 8.29304e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 352
rank avg (pred): 0.511 +- 0.214
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 2.8097e-06, 5.71035e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.104 +- 0.087
mrr vals (pred, true): 0.236, 0.232
batch losses (mrrl, rdl): 0.0001294617, 8.01771e-05

Epoch over!
epoch time: 15.054

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 829
rank avg (pred): 0.046 +- 0.036
mrr vals (pred, true): 0.239, 0.247
batch losses (mrrl, rdl): 0.0005432232, 3.01496e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.449 +- 0.184
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 4.4195e-05, 3.87445e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 839
rank avg (pred): 0.533 +- 0.257
mrr vals (pred, true): 0.047, 0.003
batch losses (mrrl, rdl): 0.000115535, 5.88608e-05

Epoch over!
epoch time: 15.042

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 701
rank avg (pred): 0.479 +- 0.198
mrr vals (pred, true): 0.044, 0.004
batch losses (mrrl, rdl): 0.0003154408, 2.54825e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 123
rank avg (pred): 0.502 +- 0.269
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001246947, 3.93224e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.431 +- 0.183
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 2.4057e-06, 0.0005062908

Epoch over!
epoch time: 15.035

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.492 +- 0.257
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 3.004e-06, 6.3991e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 850
rank avg (pred): 0.507 +- 0.284
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 7.9863e-06, 1.34443e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 587
rank avg (pred): 0.507 +- 0.266
mrr vals (pred, true): 0.041, 0.003
batch losses (mrrl, rdl): 0.0007732097, 1.9958e-06

Epoch over!
epoch time: 15.018

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.217 +- 0.134
mrr vals (pred, true): 0.169, 0.186

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   24 	     0 	 0.04692 	 0.00087 	 m..s
    0 	     1 	 0.03835 	 0.00143 	 m..s
   11 	     2 	 0.04442 	 0.00146 	 m..s
   22 	     3 	 0.04604 	 0.00196 	 m..s
   60 	     4 	 0.04915 	 0.00226 	 m..s
   84 	     5 	 0.05157 	 0.00240 	 m..s
   59 	     6 	 0.04911 	 0.00241 	 m..s
   57 	     7 	 0.04908 	 0.00242 	 m..s
   21 	     8 	 0.04601 	 0.00250 	 m..s
   74 	     9 	 0.05036 	 0.00250 	 m..s
   51 	    10 	 0.04886 	 0.00280 	 m..s
   81 	    11 	 0.05138 	 0.00296 	 m..s
   78 	    12 	 0.05097 	 0.00300 	 m..s
   26 	    13 	 0.04725 	 0.00300 	 m..s
   18 	    14 	 0.04584 	 0.00301 	 m..s
   68 	    15 	 0.04995 	 0.00304 	 m..s
   33 	    16 	 0.04788 	 0.00313 	 m..s
   17 	    17 	 0.04513 	 0.00315 	 m..s
   47 	    18 	 0.04863 	 0.00316 	 m..s
   67 	    19 	 0.04989 	 0.00317 	 m..s
   80 	    20 	 0.05125 	 0.00317 	 m..s
   86 	    21 	 0.05398 	 0.00318 	 m..s
    8 	    22 	 0.04420 	 0.00320 	 m..s
   65 	    23 	 0.04967 	 0.00324 	 m..s
   13 	    24 	 0.04496 	 0.00330 	 m..s
   55 	    25 	 0.04891 	 0.00340 	 m..s
   62 	    26 	 0.04928 	 0.00345 	 m..s
    6 	    27 	 0.04401 	 0.00348 	 m..s
   52 	    28 	 0.04887 	 0.00350 	 m..s
   20 	    29 	 0.04597 	 0.00350 	 m..s
   16 	    30 	 0.04512 	 0.00353 	 m..s
   27 	    31 	 0.04739 	 0.00362 	 m..s
    7 	    32 	 0.04410 	 0.00364 	 m..s
   35 	    33 	 0.04801 	 0.00365 	 m..s
    5 	    34 	 0.04334 	 0.00368 	 m..s
   54 	    35 	 0.04890 	 0.00370 	 m..s
   63 	    36 	 0.04928 	 0.00375 	 m..s
   61 	    37 	 0.04915 	 0.00380 	 m..s
   43 	    38 	 0.04856 	 0.00382 	 m..s
   75 	    39 	 0.05054 	 0.00391 	 m..s
    3 	    40 	 0.04265 	 0.00392 	 m..s
   14 	    41 	 0.04503 	 0.00394 	 m..s
   73 	    42 	 0.05031 	 0.00395 	 m..s
   50 	    43 	 0.04875 	 0.00396 	 m..s
   49 	    44 	 0.04864 	 0.00398 	 m..s
   23 	    45 	 0.04615 	 0.00398 	 m..s
   76 	    46 	 0.05062 	 0.00407 	 m..s
   71 	    47 	 0.05013 	 0.00408 	 m..s
   28 	    48 	 0.04743 	 0.00409 	 m..s
   53 	    49 	 0.04889 	 0.00410 	 m..s
   46 	    50 	 0.04861 	 0.00415 	 m..s
   58 	    51 	 0.04910 	 0.00421 	 m..s
   30 	    52 	 0.04771 	 0.00433 	 m..s
   83 	    53 	 0.05150 	 0.00439 	 m..s
   34 	    54 	 0.04797 	 0.00440 	 m..s
   77 	    55 	 0.05084 	 0.00442 	 m..s
   79 	    56 	 0.05113 	 0.00448 	 m..s
   19 	    57 	 0.04596 	 0.00449 	 m..s
    9 	    58 	 0.04435 	 0.00452 	 m..s
   82 	    59 	 0.05143 	 0.00460 	 m..s
   39 	    60 	 0.04831 	 0.00464 	 m..s
   10 	    61 	 0.04437 	 0.00467 	 m..s
   40 	    62 	 0.04833 	 0.00469 	 m..s
   37 	    63 	 0.04805 	 0.00473 	 m..s
   31 	    64 	 0.04772 	 0.00473 	 m..s
   42 	    65 	 0.04838 	 0.00479 	 m..s
   64 	    66 	 0.04944 	 0.00484 	 m..s
   36 	    67 	 0.04803 	 0.00489 	 m..s
   12 	    68 	 0.04446 	 0.00492 	 m..s
   70 	    69 	 0.05001 	 0.00505 	 m..s
   69 	    70 	 0.04997 	 0.00514 	 m..s
   72 	    71 	 0.05019 	 0.00516 	 m..s
   15 	    72 	 0.04510 	 0.00534 	 m..s
   38 	    73 	 0.04816 	 0.00574 	 m..s
   56 	    74 	 0.04891 	 0.00579 	 m..s
    4 	    75 	 0.04327 	 0.00682 	 m..s
   41 	    76 	 0.04838 	 0.00970 	 m..s
   32 	    77 	 0.04776 	 0.01020 	 m..s
   44 	    78 	 0.04857 	 0.01098 	 m..s
   48 	    79 	 0.04863 	 0.01152 	 m..s
   29 	    80 	 0.04767 	 0.01156 	 m..s
   45 	    81 	 0.04860 	 0.01240 	 m..s
   87 	    82 	 0.05617 	 0.02791 	 ~...
   85 	    83 	 0.05303 	 0.03020 	 ~...
   66 	    84 	 0.04988 	 0.03253 	 ~...
    1 	    85 	 0.04104 	 0.04296 	 ~...
   25 	    86 	 0.04710 	 0.04398 	 ~...
    2 	    87 	 0.04129 	 0.04656 	 ~...
   88 	    88 	 0.12311 	 0.12566 	 ~...
   90 	    89 	 0.12780 	 0.12873 	 ~...
   93 	    90 	 0.13372 	 0.12889 	 ~...
   91 	    91 	 0.13023 	 0.13162 	 ~...
   89 	    92 	 0.12434 	 0.13345 	 ~...
   92 	    93 	 0.13071 	 0.13628 	 ~...
  107 	    94 	 0.19138 	 0.15098 	 m..s
  104 	    95 	 0.18380 	 0.16128 	 ~...
  100 	    96 	 0.17794 	 0.17666 	 ~...
   98 	    97 	 0.17417 	 0.17792 	 ~...
   96 	    98 	 0.16918 	 0.17858 	 ~...
   97 	    99 	 0.17297 	 0.18174 	 ~...
  106 	   100 	 0.18727 	 0.18489 	 ~...
   95 	   101 	 0.16908 	 0.18603 	 ~...
  101 	   102 	 0.17922 	 0.18796 	 ~...
   94 	   103 	 0.16905 	 0.19023 	 ~...
  103 	   104 	 0.18353 	 0.19140 	 ~...
  108 	   105 	 0.19904 	 0.19542 	 ~...
  105 	   106 	 0.18470 	 0.19558 	 ~...
  102 	   107 	 0.18022 	 0.19847 	 ~...
  110 	   108 	 0.20091 	 0.20171 	 ~...
  117 	   109 	 0.24398 	 0.21213 	 m..s
  115 	   110 	 0.23442 	 0.22857 	 ~...
  113 	   111 	 0.22243 	 0.23383 	 ~...
  112 	   112 	 0.21266 	 0.24350 	 m..s
   99 	   113 	 0.17742 	 0.24456 	 m..s
  111 	   114 	 0.20161 	 0.25269 	 m..s
  109 	   115 	 0.19978 	 0.26233 	 m..s
  116 	   116 	 0.24205 	 0.27019 	 ~...
  114 	   117 	 0.22631 	 0.27020 	 m..s
  118 	   118 	 0.26361 	 0.29629 	 m..s
  119 	   119 	 0.28689 	 0.30632 	 ~...
  120 	   120 	 0.31201 	 0.33494 	 ~...
==========================================
r_mrr = 0.9856506586074829
r2_mrr = 0.8248074054718018
spearmanr_mrr@5 = 0.9884896278381348
spearmanr_mrr@10 = 0.9932186007499695
spearmanr_mrr@50 = 0.9936925172805786
spearmanr_mrr@100 = 0.9954708218574524
spearmanr_mrr@All = 0.9958031177520752
==========================================
test time: 0.45
Done Testing dataset CoDExSmall
total time taken: 235.56743550300598
training time taken: 225.09165287017822
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9857)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8248)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9885)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9932)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9937)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9955)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9958)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.24295902696269422}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4486418957022485
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [781, 336, 1017, 486, 670, 353, 512, 880, 26, 296, 237, 813, 615, 1012, 567, 6, 1152, 41, 613, 653, 1010, 1026, 110, 927, 471, 689, 423, 298, 162, 439, 33, 765, 735, 150, 87, 273, 1071, 342, 306, 1110, 349, 231, 258, 105, 165, 27, 483, 704, 28, 370, 576, 648, 164, 404, 772, 827, 234, 1135, 601, 973, 1167, 149, 843, 1041, 818, 35, 681, 588, 1128, 718, 773, 147, 1196, 1168, 130, 756, 794, 538, 351, 767, 463, 55, 989, 1137, 388, 760, 786, 1008, 1183, 882, 184, 350, 699, 762, 895, 1164, 253, 341, 856, 1073, 684, 1134, 53, 1015, 698, 94, 708, 933, 1214, 870, 865, 69, 955, 346, 1057, 326, 1087, 811, 1132, 330, 261]
valid_ids (0): []
train_ids (1094): [858, 700, 1031, 707, 995, 706, 731, 355, 380, 469, 1066, 948, 751, 280, 91, 36, 182, 50, 76, 254, 673, 745, 730, 826, 966, 978, 289, 920, 195, 339, 1170, 92, 444, 1011, 119, 474, 1004, 901, 816, 1206, 787, 20, 1051, 835, 657, 557, 450, 1160, 558, 8, 726, 935, 961, 1146, 153, 801, 930, 623, 1081, 80, 1093, 982, 410, 1121, 859, 1068, 1104, 470, 593, 1097, 1101, 2, 9, 1193, 127, 467, 722, 112, 603, 912, 664, 286, 74, 671, 1161, 759, 1033, 386, 154, 403, 609, 190, 903, 1151, 125, 238, 1067, 727, 16, 1117, 608, 445, 392, 947, 1118, 1094, 161, 625, 1116, 244, 1016, 540, 270, 179, 146, 646, 391, 1169, 305, 301, 831, 38, 495, 304, 285, 923, 54, 579, 577, 299, 916, 95, 658, 318, 728, 227, 878, 365, 72, 778, 398, 1034, 1083, 1032, 1047, 115, 93, 451, 476, 57, 313, 390, 884, 1019, 14, 929, 1158, 1058, 846, 531, 986, 460, 1143, 458, 205, 833, 1150, 457, 120, 368, 260, 1029, 847, 823, 627, 564, 712, 491, 784, 523, 957, 186, 377, 893, 854, 62, 268, 48, 743, 331, 1042, 675, 1106, 275, 357, 199, 369, 128, 871, 586, 639, 534, 307, 433, 178, 183, 171, 1014, 716, 951, 672, 240, 714, 894, 347, 628, 266, 739, 156, 630, 425, 886, 66, 96, 409, 375, 1197, 1124, 315, 605, 747, 251, 552, 524, 60, 659, 908, 405, 416, 785, 1075, 1200, 780, 1095, 553, 1102, 49, 389, 802, 513, 544, 562, 983, 909, 230, 505, 1122, 725, 1157, 761, 245, 382, 817, 1105, 1192, 427, 1063, 809, 497, 1202, 311, 876, 533, 116, 99, 297, 1030, 434, 1089, 796, 1074, 293, 309, 791, 898, 148, 566, 1144, 1207, 848, 507, 600, 447, 272, 89, 1209, 666, 1055, 111, 1006, 257, 776, 192, 461, 1018, 489, 738, 994, 1025, 656, 713, 680, 132, 749, 873, 508, 494, 742, 555, 1085, 674, 842, 526, 988, 1082, 103, 345, 175, 967, 1153, 78, 7, 492, 201, 650, 549, 1145, 734, 34, 705, 755, 1024, 965, 741, 757, 4, 678, 883, 70, 866, 372, 560, 71, 233, 314, 117, 750, 599, 852, 1129, 188, 431, 0, 753, 770, 550, 922, 950, 721, 44, 595, 11, 321, 928, 194, 424, 931, 596, 411, 343, 1187, 1127, 122, 1096, 335, 1072, 906, 374, 527, 913, 956, 276, 793, 782, 732, 442, 294, 1115, 1022, 572, 998, 107, 621, 774, 582, 277, 319, 963, 585, 75, 815, 529, 992, 643, 212, 701, 807, 64, 200, 252, 652, 869, 626, 1023, 758, 530, 979, 1046, 185, 402, 547, 729, 918, 970, 812, 10, 262, 581, 971, 209, 1159, 422, 810, 1125, 239, 669, 337, 56, 449, 1060, 975, 287, 717, 429, 647, 622, 768, 448, 216, 136, 518, 393, 406, 151, 720, 1009, 946, 159, 620, 528, 24, 584, 651, 248, 949, 267, 302, 515, 84, 500, 464, 934, 366, 889, 269, 378, 493, 1007, 954, 490, 17, 855, 207, 271, 316, 358, 303, 635, 911, 1103, 1002, 475, 30, 215, 376, 1162, 644, 247, 1027, 872, 710, 224, 925, 1163, 535, 1155, 541, 1133, 479, 545, 839, 536, 228, 1079, 81, 754, 636, 687, 1107, 897, 959, 452, 612, 1140, 1204, 373, 462, 829, 59, 259, 875, 282, 969, 520, 210, 624, 568, 677, 556, 1005, 891, 1194, 1184, 783, 487, 1199, 438, 1111, 703, 1176, 841, 384, 142, 940, 400, 1208, 638, 395, 25, 551, 1044, 362, 225, 29, 432, 332, 481, 418, 12, 591, 1165, 173, 58, 454, 915, 1172, 1003, 22, 746, 1203, 1213, 1136, 124, 804, 850, 694, 607, 459, 1077, 740, 942, 715, 455, 1195, 408, 498, 300, 667, 999, 905, 921, 900, 141, 68, 163, 435, 1039, 691, 800, 598, 21, 1108, 39, 98, 805, 480, 436, 143, 288, 137, 851, 256, 86, 232, 795, 764, 665, 616, 1040, 594, 709, 1201, 291, 1109, 574, 61, 797, 501, 168, 695, 32, 1182, 340, 844, 597, 1064, 140, 690, 1084, 939, 77, 104, 121, 421, 1054, 611, 539, 1001, 155, 981, 1123, 13, 655, 1174, 509, 697, 619, 100, 937, 821, 219, 649, 176, 1099, 1154, 1038, 338, 1112, 868, 102, 144, 82, 881, 312, 885, 354, 532, 634, 85, 985, 663, 478, 443, 348, 1139, 437, 1088, 170, 18, 236, 792, 902, 1037, 836, 968, 246, 158, 419, 67, 990, 504, 208, 676, 1185, 468, 1119, 367, 472, 242, 1070, 222, 97, 838, 662, 1212, 1211, 641, 352, 633, 223, 977, 5, 590, 417, 1062, 1086, 363, 1198, 693, 1180, 1156, 629, 583, 737, 592, 135, 604, 860, 323, 456, 134, 679, 385, 642, 1148, 941, 1126, 537, 1113, 1028, 887, 840, 514, 265, 571, 1191, 295, 322, 221, 867, 484, 1050, 334, 640, 580, 845, 453, 1065, 1013, 814, 888, 379, 991, 1061, 565, 465, 283, 861, 711, 987, 788, 919, 775, 241, 51, 229, 145, 1171, 356, 863, 290, 1020, 779, 129, 837, 1166, 1090, 522, 736, 19, 381, 953, 668, 413, 1053, 849, 569, 958, 325, 777, 575, 570, 281, 654, 561, 879, 926, 101, 1076, 1138, 972, 213, 1059, 1120, 139, 503, 440, 399, 692, 198, 521, 15, 1181, 359, 106, 1098, 853, 748, 789, 415, 420, 324, 752, 798, 892, 1078, 719, 1147, 177, 1175, 563, 974, 573, 1210, 542, 361, 525, 226, 733, 278, 546, 292, 167, 364, 1186, 255, 686, 519, 360, 1036, 264, 45, 138, 396, 412, 1049, 83, 114, 1205, 1021, 407, 46, 499, 799, 90, 133, 661, 830, 113, 40, 63, 511, 877, 993, 214, 1100, 485, 1069, 907, 344, 310, 1091, 702, 820, 169, 160, 824, 263, 328, 73, 47, 1092, 414, 174, 506, 517, 862, 1048, 126, 790, 944, 825, 31, 473, 1177, 217, 203, 181, 88, 193, 383, 936, 187, 397, 166, 211, 960, 688, 932, 744, 803, 52, 1190, 543, 724, 284, 482, 890, 249, 202, 945, 152, 387, 682, 606, 1179, 488, 1000, 819, 118, 874, 502, 279, 317, 578, 832, 371, 206, 610, 614, 723, 1056, 632, 996, 763, 769, 980, 131, 822, 1130, 976, 172, 917, 1178, 696, 308, 1052, 1149, 997, 554, 1142, 466, 196, 65, 864, 510, 3, 618, 899, 548, 394, 37, 204, 1189, 1114, 857, 329, 645, 430, 43, 587, 631, 1188, 617, 962, 180, 683, 426, 896, 952, 984, 766, 964, 250, 943, 191, 559, 660, 1131, 637, 428, 218, 1045, 516, 157, 23, 108, 477, 924, 320, 243, 42, 1035, 1, 1080, 327, 806, 197, 401, 274, 589, 685, 1173, 1141, 446, 441, 834, 904, 828, 109, 914, 496, 808, 189, 1043, 333, 79, 938, 235, 771, 123, 602, 220, 910]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7675666536263713
the save name prefix for this run is:  chkpt-ID_7675666536263713_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.568 +- 0.006
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002884803

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.396 +- 0.206
mrr vals (pred, true): 0.062, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001194604

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.460 +- 0.272
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.0, 1.01641e-05

Epoch over!
epoch time: 14.867

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 584
rank avg (pred): 0.450 +- 0.263
mrr vals (pred, true): 0.102, 0.003
batch losses (mrrl, rdl): 0.0, 6.225e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 740
rank avg (pred): 0.107 +- 0.070
mrr vals (pred, true): 0.140, 0.185
batch losses (mrrl, rdl): 0.0, 3.72479e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 4
rank avg (pred): 0.051 +- 0.033
mrr vals (pred, true): 0.151, 0.220
batch losses (mrrl, rdl): 0.0, 3.3903e-06

Epoch over!
epoch time: 14.822

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 376
rank avg (pred): 0.454 +- 0.271
mrr vals (pred, true): 0.074, 0.004
batch losses (mrrl, rdl): 0.0, 1.10524e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1121
rank avg (pred): 0.443 +- 0.268
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 0.0, 9.5775e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 717
rank avg (pred): 0.460 +- 0.263
mrr vals (pred, true): 0.040, 0.005
batch losses (mrrl, rdl): 0.0, 5.6403e-06

Epoch over!
epoch time: 14.837

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.460 +- 0.265
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.0, 6.3221e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.478 +- 0.274
mrr vals (pred, true): 0.034, 0.003
batch losses (mrrl, rdl): 0.0, 4.073e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1038
rank avg (pred): 0.463 +- 0.272
mrr vals (pred, true): 0.027, 0.005
batch losses (mrrl, rdl): 0.0, 2.6847e-06

Epoch over!
epoch time: 14.843

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.088 +- 0.061
mrr vals (pred, true): 0.074, 0.186
batch losses (mrrl, rdl): 0.0, 1.01153e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1103
rank avg (pred): 0.463 +- 0.268
mrr vals (pred, true): 0.017, 0.005
batch losses (mrrl, rdl): 0.0, 2.4175e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 278
rank avg (pred): 0.085 +- 0.058
mrr vals (pred, true): 0.078, 0.198
batch losses (mrrl, rdl): 0.0, 8.5438e-06

Epoch over!
epoch time: 14.838

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 122
rank avg (pred): 0.453 +- 0.257
mrr vals (pred, true): 0.016, 0.003
batch losses (mrrl, rdl): 0.0116226487, 4.2632e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 883
rank avg (pred): 0.495 +- 0.242
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0008508955, 3.57846e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.438 +- 0.221
mrr vals (pred, true): 0.049, 0.020
batch losses (mrrl, rdl): 2.8721e-06, 0.0001499951

Epoch over!
epoch time: 15.039

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 420
rank avg (pred): 0.435 +- 0.238
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001712203, 1.86215e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 173
rank avg (pred): 0.467 +- 0.240
mrr vals (pred, true): 0.057, 0.003
batch losses (mrrl, rdl): 0.0005436895, 1.34495e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 453
rank avg (pred): 0.444 +- 0.238
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002182357, 1.18958e-05

Epoch over!
epoch time: 14.985

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.436 +- 0.209
mrr vals (pred, true): 0.050, 0.023
batch losses (mrrl, rdl): 5.401e-07, 4.39189e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 142
rank avg (pred): 0.461 +- 0.241
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 3.4867e-06, 1.16763e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.020 +- 0.015
mrr vals (pred, true): 0.255, 0.248
batch losses (mrrl, rdl): 0.0004489731, 2.09707e-05

Epoch over!
epoch time: 15.01

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 596
rank avg (pred): 0.477 +- 0.245
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0002939885, 1.21402e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 403
rank avg (pred): 0.437 +- 0.226
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 1.05876e-05, 1.71736e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.019 +- 0.014
mrr vals (pred, true): 0.277, 0.304
batch losses (mrrl, rdl): 0.007429779, 1.29916e-05

Epoch over!
epoch time: 15.001

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 284
rank avg (pred): 0.169 +- 0.118
mrr vals (pred, true): 0.189, 0.210
batch losses (mrrl, rdl): 0.0040773358, 0.000216503

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 223
rank avg (pred): 0.457 +- 0.251
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 4.9041e-06, 9.7079e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 313
rank avg (pred): 0.148 +- 0.101
mrr vals (pred, true): 0.178, 0.189
batch losses (mrrl, rdl): 0.0011849523, 0.0001314468

Epoch over!
epoch time: 14.973

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.443 +- 0.245
mrr vals (pred, true): 0.061, 0.005
batch losses (mrrl, rdl): 0.0011866244, 8.9938e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.122 +- 0.084
mrr vals (pred, true): 0.184, 0.173
batch losses (mrrl, rdl): 0.0011745163, 5.44014e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.455 +- 0.247
mrr vals (pred, true): 0.063, 0.003
batch losses (mrrl, rdl): 0.0017831756, 5.7514e-06

Epoch over!
epoch time: 15.089

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 503
rank avg (pred): 0.433 +- 0.202
mrr vals (pred, true): 0.052, 0.021
batch losses (mrrl, rdl): 4.30789e-05, 7.43913e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 967
rank avg (pred): 0.467 +- 0.207
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 1.80282e-05, 2.79653e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 447
rank avg (pred): 0.438 +- 0.230
mrr vals (pred, true): 0.056, 0.005
batch losses (mrrl, rdl): 0.000410255, 1.62666e-05

Epoch over!
epoch time: 15.03

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 182
rank avg (pred): 0.454 +- 0.198
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.0001057419, 3.44271e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 67
rank avg (pred): 0.267 +- 0.176
mrr vals (pred, true): 0.131, 0.137
batch losses (mrrl, rdl): 0.0003244474, 0.00077287

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.077 +- 0.054
mrr vals (pred, true): 0.218, 0.185
batch losses (mrrl, rdl): 0.011106859, 6.5689e-06

Epoch over!
epoch time: 15.031

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.449 +- 0.225
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 6.4226e-06, 2.01737e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.463 +- 0.233
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.000184009, 1.2024e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 942
rank avg (pred): 0.503 +- 0.229
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 4.0569e-06, 0.0004235225

Epoch over!
epoch time: 15.021

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.412 +- 0.221
mrr vals (pred, true): 0.051, 0.008
batch losses (mrrl, rdl): 2.1763e-05, 8.9427e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1188
rank avg (pred): 0.457 +- 0.244
mrr vals (pred, true): 0.056, 0.004
batch losses (mrrl, rdl): 0.0003167464, 1.08893e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 971
rank avg (pred): 0.547 +- 0.241
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0006872972, 0.0001675288

Epoch over!
epoch time: 15.012

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.510 +- 0.191
mrr vals (pred, true): 0.039, 0.002

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.03729 	 0.00101 	 m..s
    6 	     1 	 0.03779 	 0.00155 	 m..s
   72 	     2 	 0.04007 	 0.00208 	 m..s
    8 	     3 	 0.03808 	 0.00222 	 m..s
   30 	     4 	 0.03900 	 0.00226 	 m..s
   70 	     5 	 0.03998 	 0.00242 	 m..s
   52 	     6 	 0.03925 	 0.00246 	 m..s
    2 	     7 	 0.03725 	 0.00272 	 m..s
    7 	     8 	 0.03801 	 0.00279 	 m..s
   63 	     9 	 0.03970 	 0.00280 	 m..s
   38 	    10 	 0.03909 	 0.00293 	 m..s
   17 	    11 	 0.03870 	 0.00304 	 m..s
   22 	    12 	 0.03889 	 0.00308 	 m..s
   72 	    13 	 0.04007 	 0.00310 	 m..s
    5 	    14 	 0.03745 	 0.00317 	 m..s
   55 	    15 	 0.03941 	 0.00325 	 m..s
    0 	    16 	 0.03662 	 0.00328 	 m..s
   29 	    17 	 0.03899 	 0.00328 	 m..s
   72 	    18 	 0.04007 	 0.00334 	 m..s
   69 	    19 	 0.03993 	 0.00334 	 m..s
   72 	    20 	 0.04007 	 0.00335 	 m..s
   11 	    21 	 0.03830 	 0.00336 	 m..s
   14 	    22 	 0.03848 	 0.00338 	 m..s
   62 	    23 	 0.03964 	 0.00343 	 m..s
   23 	    24 	 0.03891 	 0.00343 	 m..s
   18 	    25 	 0.03877 	 0.00347 	 m..s
   56 	    26 	 0.03952 	 0.00347 	 m..s
   49 	    27 	 0.03919 	 0.00350 	 m..s
   41 	    28 	 0.03912 	 0.00350 	 m..s
   72 	    29 	 0.04007 	 0.00353 	 m..s
   43 	    30 	 0.03916 	 0.00357 	 m..s
   72 	    31 	 0.04007 	 0.00358 	 m..s
   60 	    32 	 0.03963 	 0.00360 	 m..s
   10 	    33 	 0.03828 	 0.00362 	 m..s
   28 	    34 	 0.03899 	 0.00363 	 m..s
   15 	    35 	 0.03863 	 0.00367 	 m..s
   54 	    36 	 0.03929 	 0.00369 	 m..s
   92 	    37 	 0.04077 	 0.00370 	 m..s
   26 	    38 	 0.03895 	 0.00371 	 m..s
   68 	    39 	 0.03981 	 0.00371 	 m..s
   35 	    40 	 0.03905 	 0.00372 	 m..s
   72 	    41 	 0.04007 	 0.00373 	 m..s
   42 	    42 	 0.03914 	 0.00373 	 m..s
   12 	    43 	 0.03841 	 0.00375 	 m..s
   72 	    44 	 0.04007 	 0.00378 	 m..s
    1 	    45 	 0.03677 	 0.00384 	 m..s
   40 	    46 	 0.03911 	 0.00384 	 m..s
   72 	    47 	 0.04007 	 0.00387 	 m..s
   21 	    48 	 0.03888 	 0.00397 	 m..s
    3 	    49 	 0.03728 	 0.00402 	 m..s
   71 	    50 	 0.03999 	 0.00403 	 m..s
   58 	    51 	 0.03957 	 0.00409 	 m..s
   64 	    52 	 0.03971 	 0.00413 	 m..s
   25 	    53 	 0.03893 	 0.00414 	 m..s
   57 	    54 	 0.03955 	 0.00414 	 m..s
   20 	    55 	 0.03888 	 0.00414 	 m..s
   65 	    56 	 0.03971 	 0.00415 	 m..s
   61 	    57 	 0.03964 	 0.00415 	 m..s
   72 	    58 	 0.04007 	 0.00415 	 m..s
   50 	    59 	 0.03920 	 0.00418 	 m..s
   27 	    60 	 0.03895 	 0.00427 	 m..s
   72 	    61 	 0.04007 	 0.00428 	 m..s
   53 	    62 	 0.03925 	 0.00431 	 m..s
   72 	    63 	 0.04007 	 0.00432 	 m..s
   36 	    64 	 0.03906 	 0.00433 	 m..s
   59 	    65 	 0.03959 	 0.00441 	 m..s
   72 	    66 	 0.04007 	 0.00448 	 m..s
   32 	    67 	 0.03901 	 0.00448 	 m..s
   34 	    68 	 0.03905 	 0.00448 	 m..s
   72 	    69 	 0.04007 	 0.00452 	 m..s
   33 	    70 	 0.03901 	 0.00452 	 m..s
   46 	    71 	 0.03916 	 0.00453 	 m..s
   37 	    72 	 0.03907 	 0.00454 	 m..s
    9 	    73 	 0.03826 	 0.00455 	 m..s
   19 	    74 	 0.03879 	 0.00456 	 m..s
   72 	    75 	 0.04007 	 0.00461 	 m..s
   72 	    76 	 0.04007 	 0.00464 	 m..s
   44 	    77 	 0.03916 	 0.00464 	 m..s
   39 	    78 	 0.03909 	 0.00489 	 m..s
   45 	    79 	 0.03916 	 0.00492 	 m..s
   47 	    80 	 0.03917 	 0.00498 	 m..s
   66 	    81 	 0.03972 	 0.00503 	 m..s
   67 	    82 	 0.03975 	 0.00514 	 m..s
   31 	    83 	 0.03900 	 0.00519 	 m..s
   16 	    84 	 0.03864 	 0.00531 	 m..s
   48 	    85 	 0.03919 	 0.00545 	 m..s
   13 	    86 	 0.03841 	 0.00584 	 m..s
   24 	    87 	 0.03892 	 0.00780 	 m..s
   72 	    88 	 0.04007 	 0.02095 	 ~...
   93 	    89 	 0.04131 	 0.02158 	 ~...
   72 	    90 	 0.04007 	 0.02355 	 ~...
   72 	    91 	 0.04007 	 0.02430 	 ~...
  103 	    92 	 0.14698 	 0.03563 	 MISS
   72 	    93 	 0.04007 	 0.03631 	 ~...
   51 	    94 	 0.03921 	 0.03742 	 ~...
  102 	    95 	 0.13373 	 0.07075 	 m..s
   96 	    96 	 0.11191 	 0.10723 	 ~...
   94 	    97 	 0.10881 	 0.10894 	 ~...
   95 	    98 	 0.10993 	 0.12634 	 ~...
   99 	    99 	 0.11278 	 0.13162 	 ~...
  101 	   100 	 0.12364 	 0.13484 	 ~...
   98 	   101 	 0.11221 	 0.13525 	 ~...
   97 	   102 	 0.11209 	 0.14041 	 ~...
  100 	   103 	 0.11586 	 0.14295 	 ~...
  104 	   104 	 0.14884 	 0.17434 	 ~...
  107 	   105 	 0.15778 	 0.17766 	 ~...
  106 	   106 	 0.15125 	 0.18174 	 m..s
  105 	   107 	 0.14982 	 0.19023 	 m..s
  113 	   108 	 0.22741 	 0.19541 	 m..s
  112 	   109 	 0.22306 	 0.19940 	 ~...
  114 	   110 	 0.22896 	 0.21151 	 ~...
  110 	   111 	 0.20572 	 0.22805 	 ~...
  111 	   112 	 0.21420 	 0.26096 	 m..s
  116 	   113 	 0.24357 	 0.28375 	 m..s
  119 	   114 	 0.25780 	 0.28482 	 ~...
  109 	   115 	 0.20390 	 0.29538 	 m..s
  108 	   116 	 0.19782 	 0.29610 	 m..s
  118 	   117 	 0.25003 	 0.31529 	 m..s
  120 	   118 	 0.26023 	 0.31875 	 m..s
  117 	   119 	 0.24638 	 0.31910 	 m..s
  115 	   120 	 0.22907 	 0.32947 	 MISS
==========================================
r_mrr = 0.9719985127449036
r2_mrr = 0.8144637942314148
spearmanr_mrr@5 = 0.8397065997123718
spearmanr_mrr@10 = 0.9252663850784302
spearmanr_mrr@50 = 0.9869537949562073
spearmanr_mrr@100 = 0.9909932613372803
spearmanr_mrr@All = 0.9914816617965698
==========================================
test time: 0.446
Done Testing dataset CoDExSmall
total time taken: 235.5035047531128
training time taken: 224.85516095161438
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9720)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8145)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.8397)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9253)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9870)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9910)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9915)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.8330668135313317}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4303420726650775
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 421, 392, 1014, 319, 547, 417, 654, 855, 925, 1168, 371, 224, 714, 355, 213, 484, 955, 1196, 804, 409, 807, 673, 1089, 828, 62, 154, 341, 125, 27, 576, 1151, 121, 84, 1160, 435, 759, 304, 874, 259, 1157, 134, 635, 607, 272, 1099, 385, 32, 580, 598, 796, 459, 1106, 527, 321, 267, 1011, 143, 1052, 867, 591, 716, 1188, 914, 870, 227, 1204, 275, 300, 447, 68, 751, 130, 210, 410, 615, 443, 157, 348, 2, 972, 60, 736, 913, 104, 625, 480, 884, 1060, 781, 922, 787, 1091, 329, 299, 370, 303, 155, 382, 798, 425, 78, 1042, 1173, 406, 959, 659, 1067, 1203, 715, 1122, 767, 1087, 556, 400, 1057, 241, 9, 706, 856, 937]
valid_ids (0): []
train_ids (1094): [95, 353, 948, 509, 273, 1199, 644, 992, 1137, 1094, 703, 82, 456, 269, 776, 1023, 915, 649, 129, 719, 1181, 575, 243, 1053, 826, 908, 761, 971, 74, 483, 772, 5, 1019, 879, 753, 1070, 90, 747, 667, 562, 1013, 426, 102, 601, 144, 819, 839, 458, 434, 1071, 295, 287, 708, 848, 454, 1075, 850, 639, 73, 825, 558, 260, 1100, 599, 829, 296, 248, 424, 712, 1142, 919, 658, 460, 1171, 857, 211, 803, 450, 1051, 18, 1128, 1004, 895, 905, 617, 12, 939, 1193, 966, 133, 1135, 492, 1195, 1167, 301, 120, 1103, 1098, 1049, 502, 1180, 208, 713, 111, 627, 324, 583, 1007, 618, 445, 1048, 1205, 632, 318, 735, 372, 277, 265, 1147, 845, 545, 270, 284, 640, 528, 985, 1101, 264, 452, 574, 100, 860, 89, 597, 595, 784, 239, 399, 1086, 186, 451, 950, 779, 970, 503, 1012, 340, 317, 87, 175, 488, 739, 205, 431, 550, 361, 1090, 26, 1170, 397, 518, 793, 231, 590, 989, 298, 114, 201, 69, 778, 813, 721, 696, 105, 1113, 37, 81, 1159, 469, 866, 854, 165, 1043, 262, 810, 203, 57, 475, 977, 740, 873, 572, 365, 827, 1024, 1092, 332, 229, 338, 695, 146, 1214, 947, 209, 629, 853, 552, 729, 1114, 903, 1134, 1208, 790, 918, 336, 279, 46, 233, 702, 670, 80, 872, 1115, 660, 136, 1149, 64, 215, 268, 156, 592, 40, 762, 543, 768, 52, 325, 880, 920, 21, 886, 717, 1213, 608, 328, 620, 1136, 1130, 403, 1123, 780, 110, 996, 374, 823, 30, 1107, 147, 398, 221, 153, 407, 891, 25, 305, 569, 145, 1045, 91, 816, 108, 47, 333, 344, 515, 228, 22, 184, 169, 1001, 815, 968, 331, 805, 643, 648, 230, 573, 88, 560, 489, 487, 529, 737, 34, 167, 979, 782, 15, 1097, 1041, 1076, 77, 1082, 85, 462, 878, 974, 605, 500, 883, 168, 662, 347, 1059, 1210, 570, 491, 422, 1073, 466, 252, 137, 987, 170, 63, 923, 54, 844, 628, 159, 1035, 468, 537, 70, 687, 728, 775, 148, 432, 724, 1186, 764, 461, 470, 257, 151, 393, 376, 285, 638, 1132, 245, 464, 603, 1172, 471, 748, 909, 172, 668, 894, 732, 115, 786, 166, 244, 140, 783, 292, 549, 359, 619, 741, 306, 669, 742, 478, 904, 838, 534, 991, 1069, 678, 930, 297, 103, 954, 967, 119, 652, 362, 181, 189, 725, 927, 463, 282, 1085, 395, 630, 745, 1175, 794, 578, 593, 997, 865, 1009, 36, 342, 437, 536, 692, 1145, 770, 830, 1058, 897, 164, 283, 750, 1003, 664, 1184, 316, 349, 817, 932, 1006, 413, 636, 1072, 986, 360, 238, 936, 928, 423, 150, 681, 935, 7, 131, 963, 694, 1032, 246, 579, 49, 1064, 634, 477, 1118, 219, 777, 582, 98, 1080, 862, 193, 101, 863, 266, 28, 523, 522, 621, 797, 236, 709, 554, 13, 419, 943, 93, 861, 834, 1183, 482, 1, 812, 1112, 526, 727, 1095, 387, 841, 840, 56, 1044, 3, 1040, 187, 611, 92, 6, 94, 1126, 436, 960, 14, 1198, 564, 357, 586, 1150, 594, 66, 48, 614, 314, 1127, 1133, 637, 646, 1108, 308, 258, 738, 944, 377, 1038, 1111, 1010, 521, 1125, 44, 495, 501, 898, 23, 188, 726, 773, 1081, 984, 1000, 1102, 366, 749, 1143, 311, 831, 29, 1055, 286, 563, 892, 496, 588, 519, 179, 290, 253, 567, 524, 746, 525, 921, 38, 19, 1015, 888, 722, 900, 135, 814, 20, 1025, 97, 206, 415, 472, 138, 975, 799, 404, 1185, 765, 4, 194, 256, 851, 806, 774, 112, 0, 758, 565, 139, 1165, 697, 881, 345, 1212, 788, 1169, 952, 1020, 242, 679, 811, 938, 1031, 142, 988, 384, 320, 995, 1202, 1131, 680, 339, 671, 1152, 312, 71, 688, 197, 1034, 438, 976, 1008, 192, 467, 1189, 53, 416, 542, 657, 17, 514, 1206, 280, 731, 852, 655, 11, 907, 118, 96, 1046, 113, 538, 1084, 212, 801, 755, 1109, 202, 1088, 733, 641, 160, 367, 39, 754, 42, 1117, 1017, 465, 394, 504, 389, 789, 929, 1029, 10, 1047, 497, 1036, 689, 401, 67, 1096, 973, 1192, 288, 182, 122, 771, 541, 158, 785, 375, 994, 388, 58, 833, 962, 396, 1187, 978, 566, 255, 561, 875, 1139, 520, 220, 693, 141, 899, 51, 335, 822, 1021, 479, 1116, 546, 585, 701, 949, 998, 1074, 234, 1141, 846, 613, 276, 176, 43, 232, 677, 132, 1144, 1176, 808, 414, 356, 75, 183, 358, 1056, 281, 707, 1062, 683, 942, 906, 1154, 261, 204, 61, 983, 832, 704, 16, 405, 408, 1028, 45, 982, 964, 718, 1140, 474, 499, 559, 555, 1018, 606, 124, 1153, 596, 612, 802, 951, 383, 455, 730, 792, 278, 1093, 294, 418, 1191, 247, 1120, 577, 173, 289, 123, 1066, 651, 800, 548, 847, 1039, 161, 217, 1065, 882, 616, 493, 476, 207, 584, 511, 969, 1002, 581, 354, 274, 896, 626, 291, 330, 149, 1124, 763, 836, 35, 877, 390, 924, 931, 791, 1121, 1026, 126, 917, 642, 322, 869, 876, 216, 1110, 198, 1200, 177, 843, 363, 871, 720, 199, 532, 531, 1146, 1119, 1027, 901, 911, 185, 271, 1050, 956, 196, 128, 1201, 310, 859, 557, 958, 59, 430, 442, 744, 1174, 412, 498, 65, 544, 946, 218, 535, 1162, 505, 868, 481, 1077, 622, 86, 999, 449, 539, 824, 571, 698, 842, 600, 76, 350, 705, 448, 1030, 820, 178, 510, 485, 690, 490, 663, 1194, 551, 609, 1016, 666, 1022, 33, 195, 427, 457, 902, 858, 700, 769, 351, 240, 1079, 428, 486, 610, 337, 473, 1163, 734, 650, 981, 379, 107, 225, 116, 24, 1129, 334, 106, 8, 1177, 887, 645, 849, 661, 302, 1158, 453, 433, 1033, 254, 1104, 1164, 589, 684, 711, 961, 1182, 893, 1138, 55, 604, 1054, 364, 665, 533, 152, 127, 190, 309, 647, 293, 313, 352, 263, 623, 685, 682, 315, 378, 980, 200, 916, 674, 327, 933, 223, 953, 323, 391, 250, 1063, 699, 835, 889, 926, 766, 890, 381, 631, 1083, 587, 1037, 99, 540, 760, 993, 117, 516, 691, 513, 818, 1105, 343, 506, 174, 1190, 1179, 1068, 1197, 251, 1166, 72, 934, 743, 508, 180, 214, 568, 885, 1209, 386, 965, 402, 757, 672, 440, 171, 109, 517, 553, 653, 941, 368, 945, 420, 710, 1156, 1148, 723, 235, 864, 226, 249, 444, 624, 990, 346, 633, 530, 957, 676, 41, 1078, 446, 795, 373, 163, 1161, 940, 307, 1211, 222, 752, 686, 1207, 50, 162, 1061, 31, 494, 79, 512, 1005, 83, 191, 675, 821, 237, 910, 326, 369, 837, 411, 439, 1155, 912, 809, 756, 441, 380, 602, 429, 656, 507]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2464276625823700
the save name prefix for this run is:  chkpt-ID_2464276625823700_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.439 +- 0.006
mrr vals (pred, true): 0.001, 0.185
batch losses (mrrl, rdl): 0.0, 0.0028591112

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.446 +- 0.194
mrr vals (pred, true): 0.014, 0.005
batch losses (mrrl, rdl): 0.0, 3.62543e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 199
rank avg (pred): 0.468 +- 0.294
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.0, 1.7849e-06

Epoch over!
epoch time: 14.764

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.511 +- 0.292
mrr vals (pred, true): 0.044, 0.004
batch losses (mrrl, rdl): 0.0, 5.9741e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.456 +- 0.281
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 0.0, 4.4599e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 457
rank avg (pred): 0.461 +- 0.280
mrr vals (pred, true): 0.065, 0.005
batch losses (mrrl, rdl): 0.0, 3.5616e-06

Epoch over!
epoch time: 14.75

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1048
rank avg (pred): 0.467 +- 0.282
mrr vals (pred, true): 0.065, 0.004
batch losses (mrrl, rdl): 0.0, 1.8582e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.446 +- 0.279
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0, 7.1749e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 538
rank avg (pred): 0.451 +- 0.292
mrr vals (pred, true): 0.049, 0.008
batch losses (mrrl, rdl): 0.0, 1.42495e-05

Epoch over!
epoch time: 14.747

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.092 +- 0.180
mrr vals (pred, true): 0.110, 0.199
batch losses (mrrl, rdl): 0.0, 1.10296e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.528 +- 0.291
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 0.0, 4.66742e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 379
rank avg (pred): 0.463 +- 0.287
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0, 1.4357e-06

Epoch over!
epoch time: 14.744

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1066
rank avg (pred): 0.043 +- 0.115
mrr vals (pred, true): 0.129, 0.245
batch losses (mrrl, rdl): 0.0, 5.3869e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 6
rank avg (pred): 0.039 +- 0.110
mrr vals (pred, true): 0.132, 0.228
batch losses (mrrl, rdl): 0.0, 3.2262e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.464 +- 0.283
mrr vals (pred, true): 0.083, 0.004
batch losses (mrrl, rdl): 0.0, 2.373e-06

Epoch over!
epoch time: 14.772

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 564
rank avg (pred): 0.405 +- 0.276
mrr vals (pred, true): 0.072, 0.005
batch losses (mrrl, rdl): 0.0049923658, 9.276e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.462 +- 0.176
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.0001030874, 4.16038e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 766
rank avg (pred): 0.471 +- 0.179
mrr vals (pred, true): 0.044, 0.004
batch losses (mrrl, rdl): 0.0004042854, 3.15751e-05

Epoch over!
epoch time: 15.035

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 567
rank avg (pred): 0.458 +- 0.192
mrr vals (pred, true): 0.058, 0.003
batch losses (mrrl, rdl): 0.0006911873, 3.39064e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 642
rank avg (pred): 0.477 +- 0.148
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 8.136e-07, 3.91608e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1003
rank avg (pred): 0.478 +- 0.184
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0004919629, 4.34915e-05

Epoch over!
epoch time: 15.004

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 166
rank avg (pred): 0.442 +- 0.186
mrr vals (pred, true): 0.061, 0.003
batch losses (mrrl, rdl): 0.0012471381, 4.01412e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1019
rank avg (pred): 0.504 +- 0.199
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001736558, 6.07302e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.519 +- 0.173
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001417103, 0.0001009298

Epoch over!
epoch time: 15.012

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1102
rank avg (pred): 0.517 +- 0.172
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.17218e-05, 9.12691e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.471 +- 0.173
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 5.224e-07, 4.55797e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.422 +- 0.195
mrr vals (pred, true): 0.060, 0.021
batch losses (mrrl, rdl): 0.0010313098, 0.0003601765

Epoch over!
epoch time: 15.009

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 958
rank avg (pred): 0.487 +- 0.190
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002299606, 3.9607e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.617 +- 0.255
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 0.0001132645, 0.0006856817

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 76
rank avg (pred): 0.208 +- 0.129
mrr vals (pred, true): 0.137, 0.134
batch losses (mrrl, rdl): 9.40961e-05, 0.0003435782

Epoch over!
epoch time: 15.01

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1061
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.268, 0.320
batch losses (mrrl, rdl): 0.027401872, 2.34279e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1120
rank avg (pred): 0.497 +- 0.161
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 7.57631e-05, 5.19835e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.479 +- 0.180
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 4.31533e-05, 4.01522e-05

Epoch over!
epoch time: 15.017

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1194
rank avg (pred): 0.588 +- 0.214
mrr vals (pred, true): 0.038, 0.005
batch losses (mrrl, rdl): 0.0014717476, 0.0002284872

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.470 +- 0.193
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.000513369, 3.0643e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.468 +- 0.187
mrr vals (pred, true): 0.061, 0.004
batch losses (mrrl, rdl): 0.0012958094, 3.3886e-05

Epoch over!
epoch time: 15.021

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.012 +- 0.008
mrr vals (pred, true): 0.252, 0.248
batch losses (mrrl, rdl): 0.0001656065, 3.23754e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.015 +- 0.009
mrr vals (pred, true): 0.241, 0.238
batch losses (mrrl, rdl): 9.73759e-05, 1.82008e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 974
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.280, 0.216
batch losses (mrrl, rdl): 0.0407268852, 4.80584e-05

Epoch over!
epoch time: 15.029

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.006 +- 0.003
mrr vals (pred, true): 0.287, 0.296
batch losses (mrrl, rdl): 0.0008961179, 1.31166e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.358 +- 0.150
mrr vals (pred, true): 0.067, 0.020
batch losses (mrrl, rdl): 0.0027320925, 5.11991e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.488 +- 0.173
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 3.995e-07, 4.4393e-05

Epoch over!
epoch time: 15.023

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 91
rank avg (pred): 0.452 +- 0.181
mrr vals (pred, true): 0.055, 0.005
batch losses (mrrl, rdl): 0.0002181337, 3.47686e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.383 +- 0.137
mrr vals (pred, true): 0.055, 0.037
batch losses (mrrl, rdl): 0.0002758597, 0.0002866358

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.485 +- 0.178
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.22074e-05, 4.02696e-05

Epoch over!
epoch time: 14.928

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.458 +- 0.172
mrr vals (pred, true): 0.050, 0.002

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   83 	     0 	 0.05063 	 0.00087 	 m..s
   75 	     1 	 0.05015 	 0.00088 	 m..s
    0 	     2 	 0.04097 	 0.00109 	 m..s
   14 	     3 	 0.04485 	 0.00159 	 m..s
   91 	     4 	 0.05220 	 0.00204 	 m..s
   76 	     5 	 0.05016 	 0.00208 	 m..s
   11 	     6 	 0.04458 	 0.00222 	 m..s
   84 	     7 	 0.05079 	 0.00226 	 m..s
   56 	     8 	 0.04895 	 0.00259 	 m..s
   58 	     9 	 0.04905 	 0.00262 	 m..s
    9 	    10 	 0.04411 	 0.00304 	 m..s
   69 	    11 	 0.04954 	 0.00315 	 m..s
   20 	    12 	 0.04644 	 0.00316 	 m..s
   64 	    13 	 0.04945 	 0.00321 	 m..s
   61 	    14 	 0.04931 	 0.00321 	 m..s
    1 	    15 	 0.04258 	 0.00327 	 m..s
   35 	    16 	 0.04803 	 0.00329 	 m..s
   51 	    17 	 0.04880 	 0.00333 	 m..s
   38 	    18 	 0.04814 	 0.00334 	 m..s
   63 	    19 	 0.04942 	 0.00334 	 m..s
   45 	    20 	 0.04860 	 0.00335 	 m..s
   44 	    21 	 0.04859 	 0.00338 	 m..s
   47 	    22 	 0.04874 	 0.00339 	 m..s
   79 	    23 	 0.05028 	 0.00343 	 m..s
   86 	    24 	 0.05090 	 0.00343 	 m..s
   73 	    25 	 0.04991 	 0.00345 	 m..s
   65 	    26 	 0.04945 	 0.00348 	 m..s
   66 	    27 	 0.04946 	 0.00349 	 m..s
   72 	    28 	 0.04989 	 0.00358 	 m..s
   24 	    29 	 0.04696 	 0.00360 	 m..s
   81 	    30 	 0.05049 	 0.00361 	 m..s
   27 	    31 	 0.04760 	 0.00361 	 m..s
   10 	    32 	 0.04443 	 0.00362 	 m..s
   19 	    33 	 0.04610 	 0.00367 	 m..s
   30 	    34 	 0.04787 	 0.00368 	 m..s
   23 	    35 	 0.04674 	 0.00370 	 m..s
   32 	    36 	 0.04792 	 0.00372 	 m..s
   18 	    37 	 0.04582 	 0.00376 	 m..s
   31 	    38 	 0.04789 	 0.00377 	 m..s
   40 	    39 	 0.04852 	 0.00377 	 m..s
   41 	    40 	 0.04856 	 0.00379 	 m..s
   17 	    41 	 0.04548 	 0.00380 	 m..s
   28 	    42 	 0.04777 	 0.00382 	 m..s
   74 	    43 	 0.05000 	 0.00385 	 m..s
   49 	    44 	 0.04875 	 0.00387 	 m..s
   62 	    45 	 0.04940 	 0.00387 	 m..s
   46 	    46 	 0.04867 	 0.00388 	 m..s
   55 	    47 	 0.04889 	 0.00391 	 m..s
   60 	    48 	 0.04930 	 0.00392 	 m..s
   92 	    49 	 0.05300 	 0.00393 	 m..s
    5 	    50 	 0.04333 	 0.00394 	 m..s
   13 	    51 	 0.04479 	 0.00394 	 m..s
   15 	    52 	 0.04486 	 0.00395 	 m..s
    2 	    53 	 0.04274 	 0.00402 	 m..s
   33 	    54 	 0.04796 	 0.00407 	 m..s
   36 	    55 	 0.04810 	 0.00407 	 m..s
   26 	    56 	 0.04726 	 0.00408 	 m..s
   37 	    57 	 0.04813 	 0.00408 	 m..s
   82 	    58 	 0.05055 	 0.00411 	 m..s
   22 	    59 	 0.04670 	 0.00411 	 m..s
   71 	    60 	 0.04985 	 0.00413 	 m..s
   54 	    61 	 0.04886 	 0.00421 	 m..s
    4 	    62 	 0.04322 	 0.00421 	 m..s
   12 	    63 	 0.04475 	 0.00424 	 m..s
   89 	    64 	 0.05111 	 0.00428 	 m..s
   59 	    65 	 0.04906 	 0.00430 	 m..s
   39 	    66 	 0.04846 	 0.00431 	 m..s
   34 	    67 	 0.04797 	 0.00434 	 m..s
    7 	    68 	 0.04402 	 0.00441 	 m..s
   80 	    69 	 0.05046 	 0.00441 	 m..s
   70 	    70 	 0.04977 	 0.00442 	 m..s
   87 	    71 	 0.05092 	 0.00442 	 m..s
    3 	    72 	 0.04317 	 0.00443 	 m..s
   85 	    73 	 0.05086 	 0.00444 	 m..s
   50 	    74 	 0.04878 	 0.00450 	 m..s
   43 	    75 	 0.04858 	 0.00452 	 m..s
   67 	    76 	 0.04947 	 0.00453 	 m..s
   78 	    77 	 0.05027 	 0.00463 	 m..s
   68 	    78 	 0.04950 	 0.00464 	 m..s
   42 	    79 	 0.04857 	 0.00464 	 m..s
    8 	    80 	 0.04407 	 0.00467 	 m..s
   88 	    81 	 0.05094 	 0.00468 	 m..s
   21 	    82 	 0.04666 	 0.00477 	 m..s
   53 	    83 	 0.04886 	 0.00503 	 m..s
   25 	    84 	 0.04700 	 0.00504 	 m..s
   16 	    85 	 0.04544 	 0.00523 	 m..s
   90 	    86 	 0.05143 	 0.00528 	 m..s
   29 	    87 	 0.04784 	 0.00540 	 m..s
    6 	    88 	 0.04342 	 0.00548 	 m..s
   77 	    89 	 0.05026 	 0.00588 	 m..s
   48 	    90 	 0.04875 	 0.00740 	 m..s
   57 	    91 	 0.04899 	 0.00787 	 m..s
   52 	    92 	 0.04881 	 0.01222 	 m..s
   95 	    93 	 0.06288 	 0.02634 	 m..s
   94 	    94 	 0.05499 	 0.02920 	 ~...
   93 	    95 	 0.05498 	 0.03130 	 ~...
   96 	    96 	 0.09554 	 0.06382 	 m..s
   99 	    97 	 0.12079 	 0.10723 	 ~...
   97 	    98 	 0.11615 	 0.11743 	 ~...
  102 	    99 	 0.12901 	 0.13317 	 ~...
   98 	   100 	 0.11861 	 0.13596 	 ~...
  100 	   101 	 0.12212 	 0.14848 	 ~...
  101 	   102 	 0.12373 	 0.15237 	 ~...
  110 	   103 	 0.18019 	 0.17117 	 ~...
  109 	   104 	 0.17351 	 0.17489 	 ~...
  103 	   105 	 0.14400 	 0.17596 	 m..s
  105 	   106 	 0.14738 	 0.17858 	 m..s
  104 	   107 	 0.14451 	 0.17992 	 m..s
  106 	   108 	 0.15545 	 0.18103 	 ~...
  107 	   109 	 0.15595 	 0.19558 	 m..s
  111 	   110 	 0.19375 	 0.19824 	 ~...
  108 	   111 	 0.15615 	 0.20543 	 m..s
  115 	   112 	 0.23053 	 0.21213 	 ~...
  116 	   113 	 0.24415 	 0.22842 	 ~...
  114 	   114 	 0.22498 	 0.23552 	 ~...
  112 	   115 	 0.20461 	 0.23886 	 m..s
  113 	   116 	 0.20675 	 0.24025 	 m..s
  120 	   117 	 0.28391 	 0.27506 	 ~...
  117 	   118 	 0.25083 	 0.28375 	 m..s
  118 	   119 	 0.27036 	 0.30632 	 m..s
  119 	   120 	 0.27256 	 0.31362 	 m..s
==========================================
r_mrr = 0.984348714351654
r2_mrr = 0.7489566802978516
spearmanr_mrr@5 = 0.9269903302192688
spearmanr_mrr@10 = 0.9649519920349121
spearmanr_mrr@50 = 0.9868502020835876
spearmanr_mrr@100 = 0.9903587698936462
spearmanr_mrr@All = 0.9908211827278137
==========================================
test time: 0.456
Done Testing dataset CoDExSmall
total time taken: 235.21930146217346
training time taken: 224.33149480819702
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9843)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7490)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9270)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9650)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9869)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9904)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9908)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.22040548204313382}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7939117544682113
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [217, 855, 233, 256, 765, 447, 1037, 275, 276, 620, 640, 675, 1096, 401, 691, 722, 754, 1141, 1051, 880, 772, 145, 93, 1136, 290, 658, 917, 630, 1110, 146, 1039, 934, 689, 1126, 367, 926, 969, 1069, 85, 415, 462, 1142, 122, 719, 512, 711, 965, 324, 104, 112, 237, 195, 663, 509, 731, 767, 1212, 380, 713, 966, 467, 779, 537, 431, 971, 545, 204, 437, 720, 438, 956, 466, 983, 316, 1165, 29, 590, 1073, 894, 960, 959, 309, 662, 297, 653, 1124, 73, 441, 541, 449, 247, 1055, 1095, 191, 807, 674, 1034, 633, 834, 34, 420, 1076, 234, 206, 1214, 1199, 756, 829, 845, 1100, 232, 16, 686, 1001, 858, 886, 814, 626, 933, 357, 425]
valid_ids (0): []
train_ids (1094): [870, 957, 332, 1213, 666, 1115, 141, 750, 546, 391, 874, 615, 113, 348, 681, 169, 847, 744, 660, 723, 190, 1133, 248, 273, 1102, 1166, 948, 253, 1081, 1181, 444, 579, 820, 548, 595, 1083, 883, 343, 592, 538, 598, 669, 216, 1056, 943, 564, 827, 972, 696, 843, 386, 114, 840, 241, 508, 264, 850, 761, 240, 892, 821, 193, 946, 360, 314, 902, 993, 359, 143, 27, 24, 982, 1109, 514, 6, 504, 296, 735, 510, 1200, 1030, 1091, 947, 135, 147, 1107, 261, 398, 171, 1029, 23, 1047, 1175, 453, 1108, 745, 1120, 333, 460, 501, 1103, 35, 841, 542, 924, 1122, 547, 805, 131, 138, 250, 12, 102, 995, 334, 823, 494, 417, 846, 258, 1007, 1009, 1008, 422, 281, 1044, 1066, 793, 970, 656, 382, 931, 288, 629, 1, 279, 992, 238, 635, 646, 739, 1159, 305, 1211, 543, 100, 887, 601, 1035, 428, 589, 743, 519, 1012, 223, 1157, 1087, 411, 885, 303, 539, 586, 1036, 600, 1094, 351, 710, 154, 574, 647, 746, 553, 347, 918, 106, 219, 70, 738, 904, 407, 158, 403, 523, 285, 1179, 624, 955, 607, 1038, 860, 618, 1135, 1052, 912, 962, 440, 783, 818, 188, 413, 1152, 869, 1114, 1154, 94, 628, 19, 163, 676, 1129, 617, 211, 134, 162, 1049, 474, 976, 636, 725, 139, 0, 242, 377, 341, 1143, 721, 176, 389, 803, 121, 550, 436, 1150, 571, 133, 412, 639, 62, 175, 654, 697, 1188, 1067, 84, 220, 1192, 529, 762, 96, 310, 130, 1137, 554, 490, 1176, 1074, 796, 520, 1170, 435, 246, 637, 954, 363, 419, 185, 205, 737, 632, 372, 1182, 717, 1060, 930, 361, 985, 832, 263, 809, 1146, 1050, 464, 940, 362, 1209, 1167, 459, 115, 1169, 945, 173, 74, 319, 67, 648, 395, 790, 1015, 344, 1162, 891, 485, 254, 1097, 489, 1042, 667, 393, 371, 914, 14, 493, 906, 758, 222, 312, 784, 25, 1032, 741, 838, 328, 424, 968, 798, 1197, 578, 998, 495, 157, 612, 879, 320, 819, 480, 1006, 218, 552, 919, 699, 517, 476, 161, 1040, 944, 751, 602, 534, 788, 975, 1145, 349, 680, 565, 8, 203, 32, 1180, 1138, 340, 597, 842, 142, 17, 208, 409, 764, 549, 535, 69, 257, 272, 352, 268, 473, 786, 716, 5, 198, 625, 777, 921, 148, 1084, 861, 339, 346, 862, 483, 81, 949, 265, 186, 399, 575, 318, 189, 117, 868, 621, 937, 698, 321, 1045, 43, 997, 544, 336, 1104, 749, 503, 1207, 307, 655, 661, 325, 563, 867, 83, 68, 785, 373, 151, 323, 350, 540, 82, 243, 1206, 1003, 518, 48, 77, 354, 292, 181, 406, 1088, 651, 472, 236, 950, 1183, 752, 524, 172, 329, 127, 729, 753, 266, 837, 463, 470, 1160, 613, 1031, 38, 769, 967, 802, 72, 889, 442, 980, 614, 53, 446, 1054, 913, 604, 155, 649, 1119, 1057, 212, 1064, 863, 1189, 616, 136, 402, 728, 747, 706, 566, 327, 262, 877, 202, 168, 384, 1061, 366, 1093, 907, 991, 1195, 668, 1128, 488, 815, 1011, 734, 1023, 817, 408, 455, 694, 313, 915, 928, 1098, 252, 922, 562, 194, 300, 810, 789, 561, 294, 47, 1024, 787, 140, 811, 491, 1111, 559, 671, 376, 828, 245, 797, 650, 499, 461, 677, 866, 782, 199, 851, 123, 457, 1078, 610, 1046, 576, 989, 98, 707, 1193, 603, 228, 469, 652, 804, 505, 197, 718, 1149, 1071, 445, 683, 63, 1210, 153, 1041, 701, 1184, 1168, 996, 942, 1065, 911, 414, 558, 430, 80, 315, 700, 1112, 129, 831, 726, 1090, 201, 1043, 326, 283, 452, 355, 702, 584, 99, 835, 964, 583, 778, 308, 1163, 51, 688, 196, 1132, 854, 1144, 118, 670, 330, 800, 1021, 78, 56, 594, 267, 511, 378, 383, 174, 665, 591, 977, 1186, 277, 59, 1058, 105, 156, 311, 875, 1020, 42, 760, 410, 54, 251, 231, 712, 1164, 306, 1191, 1068, 335, 580, 1082, 528, 76, 732, 21, 293, 882, 773, 836, 641, 183, 214, 434, 97, 484, 95, 150, 200, 896, 923, 1022, 65, 57, 577, 905, 45, 433, 878, 496, 890, 953, 759, 31, 60, 556, 768, 79, 342, 92, 1201, 659, 605, 90, 1148, 481, 259, 1118, 269, 400, 299, 37, 727, 780, 421, 672, 572, 1053, 755, 1086, 46, 30, 1158, 492, 551, 274, 184, 774, 187, 555, 1077, 864, 15, 26, 36, 249, 479, 515, 375, 111, 695, 390, 908, 179, 581, 984, 766, 587, 1178, 465, 392, 872, 536, 1204, 533, 833, 66, 678, 244, 405, 448, 317, 3, 478, 994, 730, 939, 852, 456, 1025, 1113, 839, 423, 1147, 159, 132, 903, 282, 1048, 925, 13, 1177, 693, 963, 39, 137, 987, 52, 812, 374, 568, 429, 1187, 387, 1208, 298, 644, 951, 859, 596, 608, 876, 881, 144, 213, 426, 830, 981, 952, 1026, 11, 623, 813, 160, 1134, 9, 271, 638, 1194, 560, 385, 125, 475, 1017, 128, 1027, 192, 941, 973, 439, 1174, 215, 149, 531, 1010, 657, 1079, 126, 525, 826, 1196, 55, 497, 388, 301, 1072, 418, 1205, 679, 381, 1028, 416, 103, 61, 532, 748, 255, 848, 853, 687, 709, 642, 1203, 808, 20, 1014, 107, 432, 41, 740, 775, 229, 1019, 1127, 365, 856, 582, 781, 1105, 44, 498, 1059, 235, 631, 89, 379, 715, 221, 888, 1002, 569, 284, 487, 353, 799, 209, 794, 801, 978, 295, 606, 593, 230, 108, 337, 116, 207, 356, 120, 506, 1153, 101, 645, 958, 736, 1130, 500, 1013, 705, 88, 1116, 109, 527, 18, 1106, 124, 521, 322, 338, 304, 1016, 451, 87, 792, 450, 1198, 643, 49, 684, 516, 7, 901, 166, 673, 152, 1092, 824, 1202, 110, 929, 28, 369, 961, 742, 1151, 884, 302, 526, 1117, 227, 522, 585, 893, 471, 622, 331, 898, 1173, 427, 692, 895, 1121, 260, 397, 619, 557, 865, 1101, 170, 86, 776, 58, 177, 627, 1123, 224, 708, 4, 979, 770, 816, 1070, 900, 71, 443, 10, 871, 988, 482, 910, 849, 1155, 920, 690, 1063, 507, 573, 370, 567, 795, 822, 724, 1080, 477, 986, 599, 2, 935, 873, 1161, 1139, 1075, 33, 178, 345, 486, 1171, 167, 1085, 791, 1033, 180, 404, 1140, 91, 1125, 1190, 936, 75, 806, 825, 226, 771, 502, 733, 844, 165, 897, 899, 703, 1089, 368, 364, 999, 685, 990, 280, 182, 291, 932, 1018, 396, 1004, 164, 974, 916, 1131, 664, 704, 1172, 609, 1005, 530, 289, 278, 714, 458, 927, 468, 287, 394, 938, 22, 611, 1156, 1062, 358, 857, 513, 634, 1000, 239, 909, 588, 454, 570, 64, 270, 50, 286, 1185, 763, 757, 210, 119, 225, 1099, 682, 40]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1561269977600664
the save name prefix for this run is:  chkpt-ID_1561269977600664_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 124
rank avg (pred): 0.426 +- 0.010
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001502606

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.445 +- 0.271
mrr vals (pred, true): 0.136, 0.004
batch losses (mrrl, rdl): 0.0, 8.8014e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 824
rank avg (pred): 0.064 +- 0.040
mrr vals (pred, true): 0.153, 0.263
batch losses (mrrl, rdl): 0.0, 6.1866e-06

Epoch over!
epoch time: 14.738

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.424 +- 0.253
mrr vals (pred, true): 0.111, 0.003
batch losses (mrrl, rdl): 0.0, 3.61197e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.467 +- 0.273
mrr vals (pred, true): 0.085, 0.005
batch losses (mrrl, rdl): 0.0, 1.67752e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 890
rank avg (pred): 0.442 +- 0.271
mrr vals (pred, true): 0.080, 0.003
batch losses (mrrl, rdl): 0.0, 1.21286e-05

Epoch over!
epoch time: 14.696

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.443 +- 0.272
mrr vals (pred, true): 0.079, 0.004
batch losses (mrrl, rdl): 0.0, 5.988e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 7
rank avg (pred): 0.058 +- 0.036
mrr vals (pred, true): 0.101, 0.219
batch losses (mrrl, rdl): 0.0, 4.0465e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 68
rank avg (pred): 0.082 +- 0.058
mrr vals (pred, true): 0.091, 0.152
batch losses (mrrl, rdl): 0.0, 7.3346e-06

Epoch over!
epoch time: 14.714

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 349
rank avg (pred): 0.449 +- 0.264
mrr vals (pred, true): 0.041, 0.005
batch losses (mrrl, rdl): 0.0, 3.9369e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 246
rank avg (pred): 0.054 +- 0.038
mrr vals (pred, true): 0.107, 0.269
batch losses (mrrl, rdl): 0.0, 5.0254e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 490
rank avg (pred): 0.410 +- 0.242
mrr vals (pred, true): 0.029, 0.025
batch losses (mrrl, rdl): 0.0, 0.0001809637

Epoch over!
epoch time: 14.719

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1197
rank avg (pred): 0.474 +- 0.276
mrr vals (pred, true): 0.020, 0.004
batch losses (mrrl, rdl): 0.0, 2.886e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 907
rank avg (pred): 0.666 +- 0.304
mrr vals (pred, true): 0.026, 0.001
batch losses (mrrl, rdl): 0.0, 1.10077e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 838
rank avg (pred): 0.481 +- 0.282
mrr vals (pred, true): 0.033, 0.004
batch losses (mrrl, rdl): 0.0, 4.9986e-06

Epoch over!
epoch time: 14.71

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.071 +- 0.090
mrr vals (pred, true): 0.100, 0.036
batch losses (mrrl, rdl): 0.0245412178, 0.0008253138

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.493 +- 0.216
mrr vals (pred, true): 0.061, 0.005
batch losses (mrrl, rdl): 0.0012744666, 4.06618e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 472
rank avg (pred): 0.470 +- 0.155
mrr vals (pred, true): 0.039, 0.004
batch losses (mrrl, rdl): 0.0012307742, 4.69344e-05

Epoch over!
epoch time: 14.961

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1032
rank avg (pred): 0.441 +- 0.151
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001754616, 7.04952e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 487
rank avg (pred): 0.390 +- 0.135
mrr vals (pred, true): 0.047, 0.024
batch losses (mrrl, rdl): 9.06273e-05, 0.0001041554

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 522
rank avg (pred): 0.459 +- 0.168
mrr vals (pred, true): 0.049, 0.008
batch losses (mrrl, rdl): 3.8253e-06, 9.6072e-05

Epoch over!
epoch time: 14.91

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 854
rank avg (pred): 0.506 +- 0.198
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.14e-07, 2.82794e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 203
rank avg (pred): 0.477 +- 0.192
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.1727e-06, 2.91765e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 833
rank avg (pred): 0.017 +- 0.012
mrr vals (pred, true): 0.235, 0.257
batch losses (mrrl, rdl): 0.0047663511, 4.60388e-05

Epoch over!
epoch time: 14.927

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 239
rank avg (pred): 0.422 +- 0.180
mrr vals (pred, true): 0.063, 0.005
batch losses (mrrl, rdl): 0.0016661839, 6.84741e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 13
rank avg (pred): 0.015 +- 0.010
mrr vals (pred, true): 0.240, 0.228
batch losses (mrrl, rdl): 0.0013590928, 2.57215e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 236
rank avg (pred): 0.466 +- 0.185
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001363464, 3.55057e-05

Epoch over!
epoch time: 14.953

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 188
rank avg (pred): 0.455 +- 0.158
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 5.3718e-05, 5.32823e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 612
rank avg (pred): 0.436 +- 0.167
mrr vals (pred, true): 0.054, 0.003
batch losses (mrrl, rdl): 0.0001350406, 9.27836e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.498 +- 0.176
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002125222, 0.0002209755

Epoch over!
epoch time: 15.053

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 353
rank avg (pred): 0.478 +- 0.183
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.195e-06, 4.78938e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1173
rank avg (pred): 0.471 +- 0.172
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 3.6861e-06, 3.22618e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 785
rank avg (pred): 0.499 +- 0.162
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0006717149, 5.17552e-05

Epoch over!
epoch time: 15.034

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 577
rank avg (pred): 0.446 +- 0.147
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 5.63351e-05, 6.99192e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1205
rank avg (pred): 0.508 +- 0.232
mrr vals (pred, true): 0.057, 0.003
batch losses (mrrl, rdl): 0.0005562428, 3.73711e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 728
rank avg (pred): 0.471 +- 0.150
mrr vals (pred, true): 0.043, 0.005
batch losses (mrrl, rdl): 0.0004955658, 5.06103e-05

Epoch over!
epoch time: 15.007

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 365
rank avg (pred): 0.477 +- 0.197
mrr vals (pred, true): 0.057, 0.005
batch losses (mrrl, rdl): 0.000517275, 3.00671e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 875
rank avg (pred): 0.512 +- 0.184
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001938223, 6.83261e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 220
rank avg (pred): 0.496 +- 0.200
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.32144e-05, 3.10967e-05

Epoch over!
epoch time: 15.013

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 427
rank avg (pred): 0.474 +- 0.165
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 3.49901e-05, 4.47872e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1173
rank avg (pred): 0.441 +- 0.151
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 5.049e-07, 5.22388e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.477 +- 0.166
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.3568e-06, 4.24241e-05

Epoch over!
epoch time: 15.016

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 739
rank avg (pred): 0.095 +- 0.070
mrr vals (pred, true): 0.212, 0.191
batch losses (mrrl, rdl): 0.004450622, 1.8812e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 511
rank avg (pred): 0.374 +- 0.145
mrr vals (pred, true): 0.060, 0.029
batch losses (mrrl, rdl): 0.0009325533, 6.74193e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 536
rank avg (pred): 0.407 +- 0.132
mrr vals (pred, true): 0.051, 0.007
batch losses (mrrl, rdl): 1.81429e-05, 8.5894e-05

Epoch over!
epoch time: 15.026

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.498 +- 0.187
mrr vals (pred, true): 0.050, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   79 	     0 	 0.05224 	 0.00091 	 m..s
    8 	     1 	 0.04498 	 0.00101 	 m..s
    0 	     2 	 0.03442 	 0.00113 	 m..s
    4 	     3 	 0.04468 	 0.00146 	 m..s
   96 	     4 	 0.05477 	 0.00160 	 m..s
   33 	     5 	 0.04855 	 0.00242 	 m..s
   17 	     6 	 0.04694 	 0.00282 	 m..s
   31 	     7 	 0.04837 	 0.00296 	 m..s
   52 	     8 	 0.05054 	 0.00300 	 m..s
   65 	     9 	 0.05156 	 0.00301 	 m..s
   71 	    10 	 0.05176 	 0.00301 	 m..s
   97 	    11 	 0.05480 	 0.00305 	 m..s
    7 	    12 	 0.04496 	 0.00311 	 m..s
   18 	    13 	 0.04716 	 0.00314 	 m..s
   51 	    14 	 0.05034 	 0.00323 	 m..s
   66 	    15 	 0.05160 	 0.00325 	 m..s
   15 	    16 	 0.04666 	 0.00330 	 m..s
   95 	    17 	 0.05420 	 0.00335 	 m..s
    1 	    18 	 0.03921 	 0.00337 	 m..s
   50 	    19 	 0.05023 	 0.00341 	 m..s
   54 	    20 	 0.05061 	 0.00343 	 m..s
   64 	    21 	 0.05140 	 0.00346 	 m..s
   19 	    22 	 0.04735 	 0.00347 	 m..s
   25 	    23 	 0.04787 	 0.00352 	 m..s
   22 	    24 	 0.04772 	 0.00357 	 m..s
   85 	    25 	 0.05241 	 0.00357 	 m..s
   38 	    26 	 0.04874 	 0.00362 	 m..s
   34 	    27 	 0.04857 	 0.00363 	 m..s
   12 	    28 	 0.04642 	 0.00364 	 m..s
    5 	    29 	 0.04484 	 0.00366 	 m..s
   11 	    30 	 0.04599 	 0.00369 	 m..s
   84 	    31 	 0.05234 	 0.00371 	 m..s
   45 	    32 	 0.04930 	 0.00371 	 m..s
   32 	    33 	 0.04846 	 0.00373 	 m..s
   35 	    34 	 0.04858 	 0.00377 	 m..s
   30 	    35 	 0.04835 	 0.00377 	 m..s
   67 	    36 	 0.05168 	 0.00381 	 m..s
   58 	    37 	 0.05097 	 0.00381 	 m..s
   76 	    38 	 0.05210 	 0.00384 	 m..s
   44 	    39 	 0.04926 	 0.00387 	 m..s
   72 	    40 	 0.05193 	 0.00388 	 m..s
   23 	    41 	 0.04779 	 0.00391 	 m..s
   21 	    42 	 0.04765 	 0.00393 	 m..s
   61 	    43 	 0.05110 	 0.00395 	 m..s
   81 	    44 	 0.05230 	 0.00402 	 m..s
    9 	    45 	 0.04529 	 0.00405 	 m..s
   16 	    46 	 0.04676 	 0.00408 	 m..s
    2 	    47 	 0.04049 	 0.00409 	 m..s
   48 	    48 	 0.05010 	 0.00410 	 m..s
   28 	    49 	 0.04812 	 0.00410 	 m..s
   26 	    50 	 0.04796 	 0.00411 	 m..s
    3 	    51 	 0.04376 	 0.00412 	 m..s
   82 	    52 	 0.05230 	 0.00415 	 m..s
   49 	    53 	 0.05011 	 0.00417 	 m..s
   88 	    54 	 0.05275 	 0.00418 	 m..s
   63 	    55 	 0.05139 	 0.00420 	 m..s
   73 	    56 	 0.05205 	 0.00420 	 m..s
   59 	    57 	 0.05101 	 0.00421 	 m..s
   47 	    58 	 0.04997 	 0.00421 	 m..s
   14 	    59 	 0.04660 	 0.00422 	 m..s
   78 	    60 	 0.05222 	 0.00423 	 m..s
   36 	    61 	 0.04861 	 0.00425 	 m..s
   62 	    62 	 0.05132 	 0.00428 	 m..s
    6 	    63 	 0.04488 	 0.00428 	 m..s
   55 	    64 	 0.05076 	 0.00428 	 m..s
   13 	    65 	 0.04658 	 0.00428 	 m..s
   87 	    66 	 0.05264 	 0.00429 	 m..s
   29 	    67 	 0.04831 	 0.00430 	 m..s
   60 	    68 	 0.05107 	 0.00437 	 m..s
   68 	    69 	 0.05171 	 0.00445 	 m..s
   20 	    70 	 0.04739 	 0.00449 	 m..s
   75 	    71 	 0.05209 	 0.00453 	 m..s
   93 	    72 	 0.05408 	 0.00454 	 m..s
   69 	    73 	 0.05172 	 0.00464 	 m..s
   27 	    74 	 0.04801 	 0.00465 	 m..s
   46 	    75 	 0.04994 	 0.00466 	 m..s
   43 	    76 	 0.04925 	 0.00473 	 m..s
   39 	    77 	 0.04880 	 0.00475 	 m..s
   74 	    78 	 0.05206 	 0.00476 	 m..s
   70 	    79 	 0.05174 	 0.00477 	 m..s
   80 	    80 	 0.05228 	 0.00479 	 m..s
   53 	    81 	 0.05057 	 0.00484 	 m..s
   91 	    82 	 0.05377 	 0.00486 	 m..s
   56 	    83 	 0.05080 	 0.00502 	 m..s
   41 	    84 	 0.04912 	 0.00516 	 m..s
   83 	    85 	 0.05231 	 0.00523 	 m..s
   42 	    86 	 0.04914 	 0.00550 	 m..s
   90 	    87 	 0.05343 	 0.00558 	 m..s
   77 	    88 	 0.05220 	 0.00577 	 m..s
   57 	    89 	 0.05082 	 0.00579 	 m..s
   10 	    90 	 0.04553 	 0.00635 	 m..s
   24 	    91 	 0.04784 	 0.00694 	 m..s
   37 	    92 	 0.04865 	 0.01098 	 m..s
   40 	    93 	 0.04890 	 0.01189 	 m..s
   89 	    94 	 0.05300 	 0.02158 	 m..s
   92 	    95 	 0.05381 	 0.02268 	 m..s
   86 	    96 	 0.05252 	 0.02496 	 ~...
   94 	    97 	 0.05412 	 0.03153 	 ~...
   98 	    98 	 0.05489 	 0.03253 	 ~...
   99 	    99 	 0.14086 	 0.12593 	 ~...
  100 	   100 	 0.14979 	 0.13846 	 ~...
  101 	   101 	 0.15284 	 0.14660 	 ~...
  109 	   102 	 0.21935 	 0.15358 	 m..s
  102 	   103 	 0.16767 	 0.16626 	 ~...
  106 	   104 	 0.18548 	 0.17519 	 ~...
  103 	   105 	 0.16841 	 0.17594 	 ~...
  104 	   106 	 0.16941 	 0.17697 	 ~...
  107 	   107 	 0.20099 	 0.18464 	 ~...
  108 	   108 	 0.20575 	 0.19541 	 ~...
  105 	   109 	 0.17359 	 0.19558 	 ~...
  111 	   110 	 0.23143 	 0.23376 	 ~...
  113 	   111 	 0.24138 	 0.23733 	 ~...
  110 	   112 	 0.22843 	 0.24659 	 ~...
  116 	   113 	 0.28465 	 0.24879 	 m..s
  114 	   114 	 0.25067 	 0.25405 	 ~...
  112 	   115 	 0.23468 	 0.27244 	 m..s
  120 	   116 	 0.33172 	 0.28931 	 m..s
  115 	   117 	 0.28389 	 0.28961 	 ~...
  118 	   118 	 0.29034 	 0.29629 	 ~...
  119 	   119 	 0.29532 	 0.30476 	 ~...
  117 	   120 	 0.28716 	 0.31475 	 ~...
==========================================
r_mrr = 0.9898513555526733
r2_mrr = 0.7681354880332947
spearmanr_mrr@5 = 0.9139937162399292
spearmanr_mrr@10 = 0.9663186073303223
spearmanr_mrr@50 = 0.9955106377601624
spearmanr_mrr@100 = 0.9965620040893555
spearmanr_mrr@All = 0.9965218305587769
==========================================
test time: 0.453
Done Testing dataset CoDExSmall
total time taken: 234.85424971580505
training time taken: 223.94140148162842
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9899)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7681)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9140)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9663)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9955)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9966)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9965)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.13842530167858058}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 9648260179318608
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [256, 342, 294, 785, 881, 37, 637, 616, 430, 644, 182, 1120, 1108, 309, 1078, 336, 764, 676, 333, 349, 878, 316, 1043, 613, 949, 494, 400, 908, 150, 1056, 86, 912, 445, 966, 1168, 1018, 391, 310, 573, 847, 850, 468, 1027, 280, 1156, 653, 264, 816, 36, 326, 241, 433, 1014, 94, 950, 1214, 49, 985, 904, 725, 906, 515, 188, 1034, 499, 782, 504, 215, 772, 1064, 399, 22, 1097, 105, 810, 74, 665, 187, 190, 2, 582, 893, 814, 419, 939, 781, 550, 386, 868, 12, 962, 943, 214, 638, 829, 1066, 734, 864, 640, 561, 347, 1204, 965, 1000, 558, 983, 989, 129, 45, 208, 1158, 461, 1181, 451, 397, 262, 340, 223, 954, 120, 917]
valid_ids (0): []
train_ids (1094): [857, 650, 719, 923, 1058, 625, 783, 945, 351, 125, 1136, 25, 136, 516, 395, 691, 1118, 102, 1001, 594, 1209, 549, 1103, 147, 258, 581, 1199, 5, 1153, 35, 1009, 1207, 1096, 563, 952, 1161, 465, 463, 1134, 460, 514, 675, 371, 958, 293, 658, 385, 1157, 849, 790, 259, 799, 177, 79, 875, 517, 405, 976, 897, 696, 1200, 843, 126, 513, 181, 348, 570, 163, 388, 90, 235, 431, 830, 502, 1184, 584, 471, 910, 10, 553, 443, 1203, 862, 278, 702, 576, 731, 46, 1174, 84, 577, 376, 357, 931, 598, 145, 140, 1149, 413, 325, 1054, 915, 886, 1030, 447, 331, 678, 263, 444, 104, 52, 751, 792, 905, 663, 851, 381, 979, 526, 436, 1042, 439, 353, 291, 987, 1127, 17, 760, 980, 870, 1115, 483, 184, 1109, 416, 191, 272, 1023, 143, 818, 590, 968, 685, 379, 592, 523, 64, 845, 1143, 464, 510, 162, 547, 605, 901, 610, 42, 667, 1082, 168, 527, 362, 1186, 883, 528, 882, 710, 1048, 865, 884, 1090, 559, 1057, 377, 47, 647, 787, 78, 423, 1104, 179, 932, 578, 437, 543, 1016, 1035, 642, 1210, 257, 282, 890, 560, 902, 833, 396, 960, 273, 1017, 213, 947, 113, 18, 1179, 40, 364, 251, 662, 601, 101, 686, 648, 1020, 791, 564, 1197, 995, 707, 606, 836, 290, 1132, 859, 59, 1019, 1125, 977, 1061, 72, 828, 621, 587, 343, 1065, 300, 1085, 169, 874, 307, 769, 1113, 612, 71, 363, 654, 729, 61, 1122, 321, 212, 796, 283, 478, 82, 1005, 914, 271, 1183, 426, 111, 160, 715, 540, 754, 242, 1053, 860, 572, 450, 319, 721, 335, 304, 723, 387, 34, 519, 909, 733, 53, 265, 172, 422, 903, 132, 110, 414, 420, 951, 732, 1089, 824, 207, 1013, 529, 334, 750, 858, 997, 763, 596, 1160, 604, 938, 926, 580, 1032, 359, 531, 407, 56, 1111, 755, 503, 619, 77, 557, 848, 689, 512, 1102, 372, 724, 925, 1133, 13, 934, 1198, 907, 63, 861, 520, 548, 1169, 1110, 497, 737, 27, 509, 1081, 1114, 623, 652, 155, 631, 19, 330, 122, 314, 770, 1041, 81, 591, 834, 268, 953, 891, 1182, 133, 154, 449, 482, 1140, 922, 1116, 714, 575, 244, 198, 1010, 350, 673, 773, 608, 226, 195, 96, 706, 567, 446, 490, 511, 927, 885, 602, 1155, 275, 609, 894, 67, 73, 1193, 31, 197, 793, 963, 1202, 305, 853, 969, 481, 562, 76, 116, 837, 91, 1059, 742, 127, 417, 759, 368, 1038, 972, 554, 1173, 831, 852, 970, 356, 641, 473, 588, 425, 1093, 301, 1080, 38, 1025, 981, 1172, 541, 660, 1029, 877, 1105, 856, 1098, 217, 332, 778, 1100, 599, 495, 286, 297, 645, 1063, 7, 83, 618, 863, 937, 924, 1159, 919, 33, 827, 178, 205, 338, 216, 752, 615, 85, 539, 80, 532, 930, 477, 717, 21, 1185, 185, 462, 1092, 60, 803, 720, 672, 303, 626, 1175, 284, 109, 99, 780, 323, 1086, 398, 392, 786, 175, 346, 159, 961, 703, 586, 1037, 889, 209, 700, 839, 1189, 1021, 239, 669, 1124, 199, 248, 1040, 260, 747, 1167, 967, 1206, 739, 921, 899, 681, 176, 88, 620, 518, 245, 229, 118, 448, 164, 276, 643, 123, 51, 804, 730, 1144, 629, 1045, 138, 311, 1046, 639, 234, 811, 459, 896, 928, 825, 93, 249, 538, 753, 946, 43, 1131, 821, 1074, 95, 378, 551, 100, 141, 1015, 1117, 892, 498, 777, 535, 170, 880, 156, 112, 670, 20, 406, 920, 1135, 354, 1026, 992, 666, 222, 250, 26, 867, 728, 767, 708, 32, 148, 210, 956, 1073, 1212, 1055, 589, 131, 299, 408, 466, 795, 1107, 75, 651, 668, 194, 1084, 888, 775, 193, 579, 410, 913, 281, 69, 1163, 534, 617, 383, 274, 1091, 738, 54, 990, 1130, 1094, 634, 1024, 798, 30, 722, 48, 1075, 955, 911, 237, 55, 988, 1060, 530, 761, 552, 1004, 611, 964, 743, 657, 716, 664, 1147, 1171, 114, 491, 487, 756, 569, 1049, 871, 165, 1142, 533, 595, 522, 823, 900, 467, 687, 202, 1069, 228, 1011, 66, 603, 1076, 14, 784, 1067, 434, 693, 712, 704, 339, 556, 480, 699, 566, 1083, 1195, 246, 774, 986, 412, 374, 11, 916, 475, 600, 324, 835, 65, 684, 1201, 971, 402, 959, 500, 139, 382, 656, 1148, 384, 158, 812, 735, 415, 1154, 635, 1192, 501, 671, 855, 1176, 525, 1178, 994, 367, 121, 933, 749, 16, 805, 1051, 456, 380, 876, 255, 24, 661, 6, 745, 701, 1187, 130, 794, 269, 973, 329, 211, 746, 627, 1146, 457, 1180, 758, 674, 289, 369, 762, 574, 679, 630, 1208, 768, 128, 683, 1039, 1028, 9, 1126, 157, 295, 1141, 1121, 840, 524, 1031, 203, 935, 108, 243, 1044, 233, 292, 1079, 124, 427, 469, 236, 225, 998, 1165, 313, 1101, 887, 390, 315, 173, 8, 1, 705, 555, 328, 872, 1128, 841, 993, 942, 152, 428, 948, 394, 607, 44, 624, 3, 403, 401, 1003, 779, 842, 438, 545, 161, 221, 1205, 58, 489, 1008, 736, 655, 373, 219, 895, 404, 1006, 748, 1022, 151, 320, 247, 832, 393, 149, 117, 485, 429, 646, 200, 50, 298, 727, 39, 941, 166, 1139, 238, 869, 991, 296, 713, 789, 484, 936, 375, 288, 788, 218, 424, 680, 695, 593, 492, 317, 999, 632, 1213, 496, 366, 365, 41, 153, 822, 361, 1072, 726, 370, 231, 940, 508, 254, 62, 411, 106, 201, 192, 435, 694, 1070, 622, 698, 806, 421, 227, 279, 224, 1152, 776, 659, 565, 455, 807, 1190, 677, 232, 1099, 29, 1150, 302, 134, 312, 189, 28, 813, 771, 1106, 493, 978, 809, 546, 802, 355, 929, 252, 975, 797, 135, 360, 285, 1164, 476, 697, 144, 119, 89, 984, 1062, 472, 453, 741, 454, 568, 996, 103, 571, 918, 196, 167, 1211, 597, 479, 341, 506, 441, 765, 1052, 261, 137, 800, 146, 536, 866, 23, 186, 352, 389, 452, 633, 718, 1047, 766, 614, 521, 1138, 230, 1137, 544, 344, 220, 1123, 507, 1166, 879, 1071, 474, 740, 682, 838, 1033, 4, 345, 171, 1077, 1162, 174, 418, 583, 690, 898, 97, 488, 505, 1007, 70, 327, 92, 253, 358, 0, 711, 1170, 409, 826, 308, 266, 1129, 1068, 318, 944, 1088, 974, 57, 1012, 815, 628, 1177, 306, 982, 432, 844, 636, 87, 277, 442, 846, 458, 270, 287, 1087, 1145, 692, 470, 142, 107, 98, 649, 240, 542, 180, 801, 1036, 1151, 585, 1188, 1119, 808, 537, 957, 204, 1191, 183, 337, 1194, 1050, 1112, 322, 1002, 15, 709, 1196, 1095, 819, 440, 688, 115, 757, 486, 267, 820, 206, 873, 68, 854, 744, 817]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2110502777337082
the save name prefix for this run is:  chkpt-ID_2110502777337082_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 296
rank avg (pred): 0.466 +- 0.014
mrr vals (pred, true): 0.001, 0.178
batch losses (mrrl, rdl): 0.0, 0.0032979054

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 905
rank avg (pred): 0.387 +- 0.237
mrr vals (pred, true): 0.015, 0.001
batch losses (mrrl, rdl): 0.0, 0.0013395755

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 117
rank avg (pred): 0.442 +- 0.271
mrr vals (pred, true): 0.010, 0.004
batch losses (mrrl, rdl): 0.0, 7.1307e-06

Epoch over!
epoch time: 14.907

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.046 +- 0.031
mrr vals (pred, true): 0.108, 0.275
batch losses (mrrl, rdl): 0.0, 3.956e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 663
rank avg (pred): 0.473 +- 0.271
mrr vals (pred, true): 0.004, 0.004
batch losses (mrrl, rdl): 0.0, 8.7733e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 402
rank avg (pred): 0.434 +- 0.272
mrr vals (pred, true): 0.005, 0.004
batch losses (mrrl, rdl): 0.0, 8.4383e-06

Epoch over!
epoch time: 14.89

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 181
rank avg (pred): 0.452 +- 0.269
mrr vals (pred, true): 0.004, 0.004
batch losses (mrrl, rdl): 0.0, 6.4336e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 15
rank avg (pred): 0.056 +- 0.037
mrr vals (pred, true): 0.037, 0.234
batch losses (mrrl, rdl): 0.0, 5.4492e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 431
rank avg (pred): 0.457 +- 0.266
mrr vals (pred, true): 0.003, 0.004
batch losses (mrrl, rdl): 0.0, 3.4439e-06

Epoch over!
epoch time: 14.902

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1049
rank avg (pred): 0.460 +- 0.274
mrr vals (pred, true): 0.003, 0.003
batch losses (mrrl, rdl): 0.0, 5.0892e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1059
rank avg (pred): 0.028 +- 0.069
mrr vals (pred, true): 0.122, 0.315
batch losses (mrrl, rdl): 0.0, 2.1974e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1031
rank avg (pred): 0.466 +- 0.281
mrr vals (pred, true): 0.003, 0.005
batch losses (mrrl, rdl): 0.0, 7.87e-07

Epoch over!
epoch time: 14.911

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.482 +- 0.277
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0, 3.7946e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1178
rank avg (pred): 0.466 +- 0.285
mrr vals (pred, true): 0.003, 0.002
batch losses (mrrl, rdl): 0.0, 1.5371e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 678
rank avg (pred): 0.467 +- 0.275
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0, 1.3379e-06

Epoch over!
epoch time: 14.935

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 751
rank avg (pred): 0.095 +- 0.165
mrr vals (pred, true): 0.024, 0.198
batch losses (mrrl, rdl): 0.3025717139, 4.35577e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 594
rank avg (pred): 0.477 +- 0.362
mrr vals (pred, true): 0.022, 0.003
batch losses (mrrl, rdl): 0.0079903826, 4.29382e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 531
rank avg (pred): 0.427 +- 0.344
mrr vals (pred, true): 0.068, 0.007
batch losses (mrrl, rdl): 0.0032568001, 4.88967e-05

Epoch over!
epoch time: 15.115

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1198
rank avg (pred): 0.476 +- 0.327
mrr vals (pred, true): 0.024, 0.004
batch losses (mrrl, rdl): 0.0068425429, 9.5077e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.530 +- 0.287
mrr vals (pred, true): 0.014, 0.003
batch losses (mrrl, rdl): 0.01283741, 4.24251e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 583
rank avg (pred): 0.457 +- 0.297
mrr vals (pred, true): 0.043, 0.003
batch losses (mrrl, rdl): 0.0005172326, 1.03074e-05

Epoch over!
epoch time: 15.094

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 255
rank avg (pred): 0.018 +- 0.059
mrr vals (pred, true): 0.322, 0.277
batch losses (mrrl, rdl): 0.0203751195, 5.8409e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 777
rank avg (pred): 0.488 +- 0.312
mrr vals (pred, true): 0.026, 0.004
batch losses (mrrl, rdl): 0.005647257, 1.29392e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 770
rank avg (pred): 0.487 +- 0.304
mrr vals (pred, true): 0.043, 0.003
batch losses (mrrl, rdl): 0.0004557878, 1.24844e-05

Epoch over!
epoch time: 15.114

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 5
rank avg (pred): 0.143 +- 0.259
mrr vals (pred, true): 0.217, 0.217
batch losses (mrrl, rdl): 4.0949e-06, 0.0001638843

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.467 +- 0.289
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002932787, 4.1234e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 775
rank avg (pred): 0.465 +- 0.292
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0004337727, 5.0468e-06

Epoch over!
epoch time: 15.162

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 576
rank avg (pred): 0.502 +- 0.297
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001239862, 2.38066e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1091
rank avg (pred): 0.485 +- 0.315
mrr vals (pred, true): 0.063, 0.005
batch losses (mrrl, rdl): 0.0017044782, 2.53983e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 186
rank avg (pred): 0.618 +- 0.347
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 2.50764e-05, 0.0004589277

Epoch over!
epoch time: 15.159

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 968
rank avg (pred): 0.478 +- 0.276
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 2.6717e-06, 4.5759e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 15
rank avg (pred): 0.040 +- 0.089
mrr vals (pred, true): 0.242, 0.234
batch losses (mrrl, rdl): 0.0006866988, 1.467e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 229
rank avg (pred): 0.434 +- 0.259
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.0006567723, 2.35048e-05

Epoch over!
epoch time: 15.167

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1029
rank avg (pred): 0.560 +- 0.332
mrr vals (pred, true): 0.056, 0.005
batch losses (mrrl, rdl): 0.0004025718, 0.0001598629

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 446
rank avg (pred): 0.421 +- 0.238
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.92902e-05, 6.81475e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 752
rank avg (pred): 0.074 +- 0.125
mrr vals (pred, true): 0.212, 0.196
batch losses (mrrl, rdl): 0.0024720586, 6.9379e-06

Epoch over!
epoch time: 15.09

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.463 +- 0.268
mrr vals (pred, true): 0.059, 0.005
batch losses (mrrl, rdl): 0.0008873135, 2.6022e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 260
rank avg (pred): 0.052 +- 0.145
mrr vals (pred, true): 0.292, 0.340
batch losses (mrrl, rdl): 0.0236040633, 1.33512e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 172
rank avg (pred): 0.443 +- 0.247
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0003370634, 4.17749e-05

Epoch over!
epoch time: 15.11

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.216 +- 0.234
mrr vals (pred, true): 0.130, 0.184
batch losses (mrrl, rdl): 0.0287863649, 0.0004041169

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 96
rank avg (pred): 0.435 +- 0.238
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 2.97767e-05, 3.20919e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 110
rank avg (pred): 0.428 +- 0.220
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 5.04629e-05, 4.42915e-05

Epoch over!
epoch time: 15.19

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 882
rank avg (pred): 0.436 +- 0.245
mrr vals (pred, true): 0.056, 0.004
batch losses (mrrl, rdl): 0.0003760009, 3.79565e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 453
rank avg (pred): 0.439 +- 0.203
mrr vals (pred, true): 0.039, 0.004
batch losses (mrrl, rdl): 0.0011639134, 4.39703e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 981
rank avg (pred): 0.060 +- 0.065
mrr vals (pred, true): 0.244, 0.163
batch losses (mrrl, rdl): 0.0656276941, 6.8206e-06

Epoch over!
epoch time: 15.202

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.065 +- 0.166
mrr vals (pred, true): 0.289, 0.315

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.03950 	 0.00088 	 m..s
    4 	     1 	 0.04102 	 0.00089 	 m..s
   22 	     2 	 0.04840 	 0.00090 	 m..s
    3 	     3 	 0.03963 	 0.00091 	 m..s
   17 	     4 	 0.04789 	 0.00102 	 m..s
   12 	     5 	 0.04726 	 0.00112 	 m..s
    7 	     6 	 0.04597 	 0.00127 	 m..s
   53 	     7 	 0.05162 	 0.00170 	 m..s
   86 	     8 	 0.05626 	 0.00222 	 m..s
   45 	     9 	 0.05100 	 0.00226 	 m..s
   26 	    10 	 0.04873 	 0.00244 	 m..s
   33 	    11 	 0.04952 	 0.00288 	 m..s
   38 	    12 	 0.05002 	 0.00301 	 m..s
    1 	    13 	 0.03804 	 0.00304 	 m..s
   36 	    14 	 0.04993 	 0.00313 	 m..s
   30 	    15 	 0.04917 	 0.00313 	 m..s
   65 	    16 	 0.05318 	 0.00316 	 m..s
    8 	    17 	 0.04657 	 0.00325 	 m..s
   73 	    18 	 0.05377 	 0.00331 	 m..s
   40 	    19 	 0.05027 	 0.00333 	 m..s
   15 	    20 	 0.04759 	 0.00334 	 m..s
   85 	    21 	 0.05590 	 0.00335 	 m..s
   35 	    22 	 0.04984 	 0.00342 	 m..s
   84 	    23 	 0.05567 	 0.00345 	 m..s
   37 	    24 	 0.04995 	 0.00347 	 m..s
   71 	    25 	 0.05357 	 0.00347 	 m..s
   24 	    26 	 0.04864 	 0.00350 	 m..s
   57 	    27 	 0.05190 	 0.00351 	 m..s
   28 	    28 	 0.04902 	 0.00358 	 m..s
   48 	    29 	 0.05111 	 0.00358 	 m..s
   34 	    30 	 0.04974 	 0.00360 	 m..s
   44 	    31 	 0.05098 	 0.00363 	 m..s
   59 	    32 	 0.05214 	 0.00363 	 m..s
    0 	    33 	 0.03619 	 0.00364 	 m..s
   20 	    34 	 0.04834 	 0.00367 	 m..s
   54 	    35 	 0.05167 	 0.00368 	 m..s
   11 	    36 	 0.04708 	 0.00370 	 m..s
   81 	    37 	 0.05485 	 0.00374 	 m..s
   80 	    38 	 0.05477 	 0.00375 	 m..s
   31 	    39 	 0.04924 	 0.00375 	 m..s
   21 	    40 	 0.04836 	 0.00378 	 m..s
   47 	    41 	 0.05106 	 0.00381 	 m..s
   72 	    42 	 0.05365 	 0.00381 	 m..s
   77 	    43 	 0.05426 	 0.00385 	 m..s
   61 	    44 	 0.05226 	 0.00387 	 m..s
   82 	    45 	 0.05512 	 0.00389 	 m..s
   10 	    46 	 0.04669 	 0.00389 	 m..s
   49 	    47 	 0.05113 	 0.00391 	 m..s
   46 	    48 	 0.05102 	 0.00400 	 m..s
   74 	    49 	 0.05396 	 0.00403 	 m..s
   29 	    50 	 0.04908 	 0.00403 	 m..s
   67 	    51 	 0.05334 	 0.00406 	 m..s
   76 	    52 	 0.05408 	 0.00409 	 m..s
    9 	    53 	 0.04661 	 0.00410 	 m..s
   18 	    54 	 0.04806 	 0.00411 	 m..s
   56 	    55 	 0.05186 	 0.00415 	 m..s
   23 	    56 	 0.04856 	 0.00426 	 m..s
   60 	    57 	 0.05224 	 0.00435 	 m..s
   16 	    58 	 0.04775 	 0.00444 	 m..s
   52 	    59 	 0.05148 	 0.00444 	 m..s
    6 	    60 	 0.04568 	 0.00445 	 m..s
   83 	    61 	 0.05531 	 0.00446 	 m..s
   78 	    62 	 0.05445 	 0.00447 	 m..s
   43 	    63 	 0.05093 	 0.00448 	 m..s
   66 	    64 	 0.05333 	 0.00450 	 m..s
   13 	    65 	 0.04741 	 0.00455 	 m..s
   70 	    66 	 0.05340 	 0.00456 	 m..s
   32 	    67 	 0.04928 	 0.00458 	 m..s
   69 	    68 	 0.05338 	 0.00458 	 m..s
   58 	    69 	 0.05208 	 0.00459 	 m..s
   75 	    70 	 0.05402 	 0.00468 	 m..s
   39 	    71 	 0.05020 	 0.00469 	 m..s
   63 	    72 	 0.05314 	 0.00477 	 m..s
   68 	    73 	 0.05337 	 0.00485 	 m..s
   91 	    74 	 0.05969 	 0.00494 	 m..s
   25 	    75 	 0.04872 	 0.00505 	 m..s
   51 	    76 	 0.05129 	 0.00510 	 m..s
   19 	    77 	 0.04809 	 0.00525 	 m..s
   55 	    78 	 0.05184 	 0.00529 	 m..s
   41 	    79 	 0.05078 	 0.00533 	 m..s
   79 	    80 	 0.05464 	 0.00545 	 m..s
   42 	    81 	 0.05079 	 0.00571 	 m..s
   62 	    82 	 0.05306 	 0.00584 	 m..s
   64 	    83 	 0.05317 	 0.00610 	 m..s
   89 	    84 	 0.05847 	 0.00635 	 m..s
   50 	    85 	 0.05118 	 0.00729 	 m..s
   87 	    86 	 0.05674 	 0.00858 	 m..s
   88 	    87 	 0.05756 	 0.01237 	 m..s
   27 	    88 	 0.04893 	 0.01781 	 m..s
   90 	    89 	 0.05942 	 0.02589 	 m..s
   14 	    90 	 0.04752 	 0.02763 	 ~...
   92 	    91 	 0.06148 	 0.03066 	 m..s
   97 	    92 	 0.16591 	 0.03527 	 MISS
    5 	    93 	 0.04526 	 0.04671 	 ~...
   93 	    94 	 0.13437 	 0.12460 	 ~...
   99 	    95 	 0.17018 	 0.13055 	 m..s
   95 	    96 	 0.15635 	 0.13534 	 ~...
   94 	    97 	 0.13580 	 0.13923 	 ~...
   96 	    98 	 0.15921 	 0.14050 	 ~...
  105 	    99 	 0.20747 	 0.15358 	 m..s
  102 	   100 	 0.19140 	 0.17202 	 ~...
  104 	   101 	 0.20388 	 0.17519 	 ~...
   98 	   102 	 0.16593 	 0.17697 	 ~...
  101 	   103 	 0.17389 	 0.18632 	 ~...
  100 	   104 	 0.17119 	 0.18824 	 ~...
  113 	   105 	 0.25787 	 0.19385 	 m..s
  110 	   106 	 0.24703 	 0.21213 	 m..s
  116 	   107 	 0.27081 	 0.24453 	 ~...
  107 	   108 	 0.23234 	 0.24463 	 ~...
  112 	   109 	 0.25222 	 0.24659 	 ~...
  108 	   110 	 0.23623 	 0.25269 	 ~...
  106 	   111 	 0.22023 	 0.26233 	 m..s
  103 	   112 	 0.20283 	 0.27020 	 m..s
  114 	   113 	 0.26139 	 0.28276 	 ~...
  120 	   114 	 0.32046 	 0.28283 	 m..s
  119 	   115 	 0.30298 	 0.28454 	 ~...
  109 	   116 	 0.24236 	 0.30476 	 m..s
  118 	   117 	 0.28920 	 0.31475 	 ~...
  111 	   118 	 0.24937 	 0.31529 	 m..s
  117 	   119 	 0.28552 	 0.31835 	 m..s
  115 	   120 	 0.26545 	 0.32046 	 m..s
==========================================
r_mrr = 0.9746860265731812
r2_mrr = 0.7794003486633301
spearmanr_mrr@5 = 0.9091578125953674
spearmanr_mrr@10 = 0.9131143689155579
spearmanr_mrr@50 = 0.9937543272972107
spearmanr_mrr@100 = 0.9959288239479065
spearmanr_mrr@All = 0.9957854151725769
==========================================
test time: 0.456
Done Testing dataset CoDExSmall
total time taken: 237.22385358810425
training time taken: 226.4145119190216
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9747)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7794)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9092)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9131)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9938)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9959)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9958)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.5287708315318014}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 4586819324266094
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [284, 831, 246, 71, 19, 361, 1166, 1134, 1201, 722, 816, 3, 731, 364, 81, 1138, 112, 931, 782, 296, 691, 935, 608, 69, 336, 923, 894, 992, 297, 38, 1068, 1186, 370, 51, 735, 42, 910, 405, 305, 1167, 554, 880, 646, 221, 308, 456, 355, 248, 399, 867, 198, 555, 602, 957, 786, 34, 23, 1038, 885, 961, 377, 228, 14, 624, 876, 1070, 408, 373, 897, 779, 1094, 217, 651, 604, 467, 1159, 161, 1132, 33, 1026, 850, 466, 956, 113, 421, 1183, 1149, 703, 349, 785, 881, 643, 639, 411, 896, 201, 844, 803, 443, 520, 548, 1205, 274, 930, 967, 92, 236, 1071, 149, 693, 671, 1187, 243, 969, 622, 630, 748, 613, 777, 383, 676]
valid_ids (0): []
train_ids (1094): [1088, 1192, 629, 1098, 823, 1120, 921, 534, 186, 718, 62, 524, 340, 57, 801, 889, 258, 828, 345, 18, 1154, 1007, 26, 440, 1180, 591, 1066, 389, 479, 1050, 714, 331, 721, 496, 679, 1073, 756, 1041, 561, 668, 323, 354, 487, 346, 556, 793, 747, 653, 356, 552, 610, 615, 887, 271, 214, 877, 379, 189, 917, 893, 859, 235, 905, 1001, 860, 819, 725, 128, 631, 821, 787, 1080, 759, 483, 315, 589, 448, 523, 1165, 337, 1122, 978, 713, 412, 1004, 338, 1097, 133, 1058, 266, 974, 299, 1000, 868, 277, 1016, 963, 663, 288, 376, 550, 1101, 352, 832, 1121, 773, 400, 115, 1191, 1012, 754, 446, 962, 543, 68, 451, 817, 66, 162, 290, 1190, 692, 1059, 110, 429, 1052, 792, 928, 80, 570, 966, 845, 410, 234, 511, 1175, 392, 253, 566, 30, 260, 362, 229, 951, 997, 197, 517, 879, 884, 557, 477, 150, 560, 233, 933, 1048, 1025, 1130, 372, 494, 763, 551, 1072, 1053, 344, 1092, 934, 999, 584, 125, 837, 899, 24, 929, 525, 460, 797, 160, 285, 690, 680, 869, 1086, 843, 132, 380, 1140, 141, 302, 397, 681, 450, 1013, 516, 1069, 914, 1185, 170, 1079, 158, 537, 533, 387, 851, 509, 12, 1179, 423, 901, 968, 965, 518, 1199, 28, 343, 497, 226, 848, 758, 637, 508, 324, 209, 946, 134, 1145, 638, 925, 611, 438, 120, 307, 49, 7, 1116, 757, 665, 1177, 402, 163, 1198, 15, 872, 1125, 598, 122, 1207, 375, 108, 1168, 334, 52, 585, 1163, 804, 4, 368, 526, 401, 640, 842, 127, 318, 1024, 1114, 712, 1084, 871, 328, 977, 1076, 737, 949, 674, 64, 454, 199, 168, 195, 1011, 357, 242, 781, 565, 1184, 240, 633, 687, 612, 41, 959, 1210, 601, 67, 683, 393, 43, 970, 507, 1206, 776, 701, 188, 1081, 1054, 1003, 280, 1214, 1027, 1146, 841, 16, 21, 652, 82, 22, 491, 634, 689, 648, 856, 529, 60, 157, 97, 603, 211, 891, 79, 427, 874, 780, 164, 461, 70, 101, 586, 490, 498, 220, 1049, 655, 771, 495, 519, 251, 169, 707, 1115, 1082, 826, 558, 109, 244, 219, 590, 532, 1144, 1010, 635, 366, 938, 8, 10, 1119, 784, 1062, 121, 861, 892, 1161, 1074, 697, 649, 762, 728, 350, 453, 87, 203, 594, 696, 1113, 814, 417, 979, 107, 1006, 1089, 265, 666, 911, 796, 1176, 314, 751, 138, 862, 213, 709, 184, 878, 1021, 670, 1141, 29, 732, 47, 353, 789, 93, 1103, 937, 906, 800, 152, 866, 650, 1112, 890, 514, 208, 1015, 886, 123, 472, 455, 846, 1102, 40, 326, 499, 176, 702, 644, 799, 404, 470, 27, 658, 73, 738, 282, 1008, 645, 147, 617, 912, 990, 175, 1031, 1211, 1139, 659, 1203, 319, 824, 247, 972, 291, 1106, 656, 1197, 950, 895, 434, 593, 310, 1036, 794, 898, 1075, 1160, 11, 863, 1034, 167, 119, 976, 452, 1174, 528, 694, 322, 873, 941, 486, 984, 54, 853, 301, 1104, 1057, 717, 420, 268, 818, 135, 306, 165, 1035, 657, 954, 58, 682, 245, 185, 753, 53, 791, 191, 447, 330, 231, 730, 916, 1150, 1032, 178, 812, 1117, 143, 1039, 329, 388, 945, 286, 813, 918, 685, 391, 1093, 182, 936, 283, 1123, 85, 578, 744, 750, 1044, 287, 430, 348, 426, 275, 17, 218, 742, 926, 259, 261, 1213, 1204, 215, 146, 562, 339, 173, 545, 567, 541, 815, 705, 729, 332, 1195, 662, 437, 413, 571, 765, 1, 398, 367, 710, 588, 641, 192, 210, 1143, 661, 991, 964, 445, 1051, 194, 327, 838, 269, 196, 1037, 900, 95, 960, 320, 289, 1046, 238, 549, 431, 1107, 313, 1009, 985, 432, 493, 1083, 227, 971, 428, 190, 772, 806, 124, 711, 471, 761, 769, 920, 475, 1078, 395, 875, 913, 607, 241, 358, 580, 363, 1042, 544, 1126, 409, 442, 1200, 542, 317, 1018, 174, 333, 86, 422, 205, 749, 1047, 888, 369, 577, 778, 1181, 232, 582, 940, 1189, 1171, 724, 870, 677, 419, 530, 106, 942, 72, 131, 56, 74, 745, 802, 156, 278, 1096, 783, 407, 767, 276, 988, 335, 1055, 994, 204, 118, 609, 351, 180, 1040, 298, 256, 396, 177, 148, 114, 84, 715, 136, 295, 484, 249, 482, 1148, 675, 788, 425, 1194, 883, 1085, 102, 104, 600, 1129, 130, 1155, 347, 907, 987, 144, 932, 250, 535, 489, 212, 193, 669, 621, 740, 1100, 839, 1182, 605, 441, 513, 1014, 78, 581, 309, 625, 538, 480, 596, 223, 465, 599, 378, 628, 614, 1109, 667, 45, 371, 374, 183, 1063, 569, 111, 1162, 385, 706, 834, 65, 833, 1188, 512, 61, 1212, 312, 515, 684, 126, 840, 947, 381, 501, 727, 695, 755, 25, 403, 958, 230, 574, 903, 943, 1169, 980, 116, 986, 636, 808, 1208, 996, 1135, 536, 416, 88, 1105, 647, 311, 207, 36, 733, 1118, 811, 583, 390, 83, 764, 632, 272, 592, 418, 865, 1172, 1017, 1170, 564, 944, 807, 500, 1147, 224, 1045, 449, 1124, 618, 187, 5, 13, 1164, 506, 222, 94, 382, 1196, 365, 1110, 664, 855, 206, 281, 678, 504, 904, 1023, 252, 836, 485, 1173, 492, 1091, 273, 1202, 98, 181, 716, 1002, 809, 955, 626, 522, 540, 909, 129, 768, 760, 1028, 468, 546, 46, 37, 736, 752, 76, 316, 539, 464, 708, 798, 620, 686, 444, 90, 386, 1033, 830, 1152, 59, 1178, 254, 654, 852, 154, 159, 563, 166, 827, 1151, 939, 321, 32, 975, 433, 1137, 835, 1099, 1136, 1193, 559, 1153, 502, 137, 510, 521, 1020, 153, 304, 6, 790, 117, 435, 673, 952, 202, 478, 1064, 257, 1061, 1056, 48, 527, 847, 948, 766, 1029, 1087, 384, 103, 595, 726, 998, 225, 1127, 457, 462, 0, 20, 1043, 770, 424, 704, 849, 100, 39, 96, 924, 267, 75, 415, 91, 294, 805, 660, 989, 476, 774, 915, 89, 142, 463, 829, 35, 179, 77, 406, 575, 1133, 1157, 739, 746, 1108, 1111, 488, 723, 9, 474, 151, 719, 439, 459, 579, 572, 50, 698, 503, 775, 239, 44, 973, 858, 810, 360, 2, 325, 993, 394, 642, 1128, 469, 155, 882, 436, 1158, 795, 1060, 619, 820, 741, 1019, 140, 927, 237, 1077, 700, 31, 293, 481, 1090, 1156, 587, 292, 341, 264, 983, 99, 473, 1209, 981, 262, 105, 919, 606, 505, 139, 743, 145, 922, 864, 255, 359, 300, 1095, 200, 672, 1022, 263, 627, 573, 279, 902, 568, 688, 63, 531, 982, 576, 995, 270, 547, 171, 822, 908, 953, 1065, 616, 458, 699, 1142, 342, 597, 1067, 1131, 553, 414, 734, 1005, 825, 720, 857, 172, 623, 854, 303, 1030, 216, 55]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9851026077322748
the save name prefix for this run is:  chkpt-ID_9851026077322748_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.414 +- 0.006
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 0.0002848223

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 987
rank avg (pred): 0.076 +- 0.054
mrr vals (pred, true): 0.169, 0.307
batch losses (mrrl, rdl): 0.0, 2.81792e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1064
rank avg (pred): 0.063 +- 0.043
mrr vals (pred, true): 0.081, 0.194
batch losses (mrrl, rdl): 0.0, 9.003e-06

Epoch over!
epoch time: 14.843

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 103
rank avg (pred): 0.438 +- 0.277
mrr vals (pred, true): 0.020, 0.004
batch losses (mrrl, rdl): 0.0, 8.2905e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 53
rank avg (pred): 0.076 +- 0.052
mrr vals (pred, true): 0.082, 0.135
batch losses (mrrl, rdl): 0.0, 6.6462e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 709
rank avg (pred): 0.449 +- 0.269
mrr vals (pred, true): 0.008, 0.003
batch losses (mrrl, rdl): 0.0, 5.7605e-06

Epoch over!
epoch time: 14.913

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 533
rank avg (pred): 0.473 +- 0.279
mrr vals (pred, true): 0.006, 0.006
batch losses (mrrl, rdl): 0.0, 5.45763e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 851
rank avg (pred): 0.463 +- 0.252
mrr vals (pred, true): 0.006, 0.003
batch losses (mrrl, rdl): 0.0, 5.877e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 83
rank avg (pred): 0.451 +- 0.268
mrr vals (pred, true): 0.007, 0.005
batch losses (mrrl, rdl): 0.0, 4.907e-06

Epoch over!
epoch time: 14.988

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.458 +- 0.268
mrr vals (pred, true): 0.006, 0.004
batch losses (mrrl, rdl): 0.0, 9.7473e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 813
rank avg (pred): 0.044 +- 0.030
mrr vals (pred, true): 0.074, 0.295
batch losses (mrrl, rdl): 0.0, 5.1707e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 747
rank avg (pred): 0.072 +- 0.047
mrr vals (pred, true): 0.043, 0.186
batch losses (mrrl, rdl): 0.0, 6.214e-06

Epoch over!
epoch time: 14.988

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1100
rank avg (pred): 0.454 +- 0.257
mrr vals (pred, true): 0.004, 0.005
batch losses (mrrl, rdl): 0.0, 5.2104e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1008
rank avg (pred): 0.433 +- 0.273
mrr vals (pred, true): 0.007, 0.004
batch losses (mrrl, rdl): 0.0, 1.05699e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 169
rank avg (pred): 0.458 +- 0.273
mrr vals (pred, true): 0.004, 0.003
batch losses (mrrl, rdl): 0.0, 5.0105e-06

Epoch over!
epoch time: 14.998

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 406
rank avg (pred): 0.462 +- 0.249
mrr vals (pred, true): 0.003, 0.005
batch losses (mrrl, rdl): 0.0225090012, 5.9535e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.448 +- 0.211
mrr vals (pred, true): 0.066, 0.004
batch losses (mrrl, rdl): 0.0024940704, 2.27305e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.029 +- 0.020
mrr vals (pred, true): 0.201, 0.254
batch losses (mrrl, rdl): 0.0288014468, 7.2677e-06

Epoch over!
epoch time: 15.12

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 939
rank avg (pred): 0.452 +- 0.189
mrr vals (pred, true): 0.052, 0.002
batch losses (mrrl, rdl): 3.46043e-05, 0.0003856957

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 303
rank avg (pred): 0.042 +- 0.030
mrr vals (pred, true): 0.210, 0.176
batch losses (mrrl, rdl): 0.0112575805, 3.84604e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 333
rank avg (pred): 0.469 +- 0.207
mrr vals (pred, true): 0.070, 0.005
batch losses (mrrl, rdl): 0.00402867, 3.45968e-05

Epoch over!
epoch time: 15.229

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 877
rank avg (pred): 0.475 +- 0.191
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001739549, 3.98827e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 253
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.308, 0.319
batch losses (mrrl, rdl): 0.0010686333, 1.08272e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.454 +- 0.178
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 3.50265e-05, 3.7761e-05

Epoch over!
epoch time: 15.199

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 698
rank avg (pred): 0.488 +- 0.184
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.000123626, 5.22206e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 686
rank avg (pred): 0.453 +- 0.160
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002684168, 4.83515e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.422 +- 0.157
mrr vals (pred, true): 0.048, 0.003
batch losses (mrrl, rdl): 3.70509e-05, 0.0001158101

Epoch over!
epoch time: 15.217

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 583
rank avg (pred): 0.448 +- 0.158
mrr vals (pred, true): 0.042, 0.003
batch losses (mrrl, rdl): 0.0006369122, 6.1995e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 713
rank avg (pred): 0.449 +- 0.138
mrr vals (pred, true): 0.039, 0.004
batch losses (mrrl, rdl): 0.0012018624, 6.92489e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1084
rank avg (pred): 0.485 +- 0.189
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 2.5264e-06, 5.86189e-05

Epoch over!
epoch time: 15.21

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.461 +- 0.167
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001953901, 4.16247e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 390
rank avg (pred): 0.456 +- 0.194
mrr vals (pred, true): 0.065, 0.005
batch losses (mrrl, rdl): 0.0022493822, 3.3238e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1153
rank avg (pred): 0.386 +- 0.152
mrr vals (pred, true): 0.054, 0.021
batch losses (mrrl, rdl): 0.0001449101, 0.0003383468

Epoch over!
epoch time: 15.218

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1023
rank avg (pred): 0.480 +- 0.183
mrr vals (pred, true): 0.046, 0.005
batch losses (mrrl, rdl): 0.0001881016, 4.43392e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 849
rank avg (pred): 0.470 +- 0.178
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 0.0001134396, 2.90821e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1040
rank avg (pred): 0.527 +- 0.204
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0006379068, 7.97299e-05

Epoch over!
epoch time: 15.214

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 52
rank avg (pred): 0.255 +- 0.166
mrr vals (pred, true): 0.147, 0.126
batch losses (mrrl, rdl): 0.0045358031, 0.0006812917

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 384
rank avg (pred): 0.467 +- 0.188
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 5.79164e-05, 3.67271e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 430
rank avg (pred): 0.451 +- 0.171
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001291876, 4.32837e-05

Epoch over!
epoch time: 15.214

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 913
rank avg (pred): 0.421 +- 0.156
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.78764e-05, 0.0013642694

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 307
rank avg (pred): 0.084 +- 0.056
mrr vals (pred, true): 0.202, 0.188
batch losses (mrrl, rdl): 0.001842989, 8.0542e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1164
rank avg (pred): 0.471 +- 0.183
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0003354385, 2.78629e-05

Epoch over!
epoch time: 15.105

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 277
rank avg (pred): 0.113 +- 0.074
mrr vals (pred, true): 0.181, 0.177
batch losses (mrrl, rdl): 0.0001634475, 3.30926e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 563
rank avg (pred): 0.445 +- 0.160
mrr vals (pred, true): 0.043, 0.006
batch losses (mrrl, rdl): 0.000502628, 2.95483e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1129
rank avg (pred): 0.478 +- 0.191
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 2.25276e-05, 3.36384e-05

Epoch over!
epoch time: 15.254

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.123 +- 0.084
mrr vals (pred, true): 0.189, 0.210

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   77 	     0 	 0.05467 	 0.00102 	 m..s
    1 	     1 	 0.03960 	 0.00115 	 m..s
   74 	     2 	 0.05413 	 0.00128 	 m..s
   63 	     3 	 0.05243 	 0.00131 	 m..s
    6 	     4 	 0.04278 	 0.00140 	 m..s
    5 	     5 	 0.04274 	 0.00143 	 m..s
   66 	     6 	 0.05277 	 0.00160 	 m..s
   62 	     7 	 0.05233 	 0.00208 	 m..s
   64 	     8 	 0.05260 	 0.00212 	 m..s
   73 	     9 	 0.05404 	 0.00240 	 m..s
   82 	    10 	 0.05561 	 0.00241 	 m..s
    4 	    11 	 0.04082 	 0.00254 	 m..s
   15 	    12 	 0.04588 	 0.00279 	 m..s
   27 	    13 	 0.04847 	 0.00288 	 m..s
   58 	    14 	 0.05216 	 0.00288 	 m..s
   78 	    15 	 0.05505 	 0.00294 	 m..s
   17 	    16 	 0.04655 	 0.00296 	 m..s
   50 	    17 	 0.05087 	 0.00297 	 m..s
   42 	    18 	 0.04976 	 0.00300 	 m..s
    3 	    19 	 0.04010 	 0.00311 	 m..s
   81 	    20 	 0.05551 	 0.00316 	 m..s
    2 	    21 	 0.03992 	 0.00320 	 m..s
   37 	    22 	 0.04928 	 0.00321 	 m..s
   51 	    23 	 0.05091 	 0.00333 	 m..s
   75 	    24 	 0.05431 	 0.00338 	 m..s
   45 	    25 	 0.05005 	 0.00341 	 m..s
   87 	    26 	 0.05688 	 0.00343 	 m..s
   20 	    27 	 0.04709 	 0.00345 	 m..s
   14 	    28 	 0.04534 	 0.00347 	 m..s
   11 	    29 	 0.04491 	 0.00350 	 m..s
   86 	    30 	 0.05670 	 0.00352 	 m..s
   67 	    31 	 0.05287 	 0.00359 	 m..s
   24 	    32 	 0.04783 	 0.00366 	 m..s
   33 	    33 	 0.04896 	 0.00369 	 m..s
   60 	    34 	 0.05221 	 0.00376 	 m..s
   25 	    35 	 0.04814 	 0.00377 	 m..s
   31 	    36 	 0.04873 	 0.00377 	 m..s
    7 	    37 	 0.04292 	 0.00378 	 m..s
   83 	    38 	 0.05613 	 0.00380 	 m..s
    8 	    39 	 0.04393 	 0.00380 	 m..s
    0 	    40 	 0.03898 	 0.00386 	 m..s
   71 	    41 	 0.05337 	 0.00387 	 m..s
   13 	    42 	 0.04517 	 0.00388 	 m..s
   18 	    43 	 0.04684 	 0.00390 	 m..s
   41 	    44 	 0.04976 	 0.00392 	 m..s
   10 	    45 	 0.04439 	 0.00394 	 m..s
   69 	    46 	 0.05327 	 0.00395 	 m..s
   19 	    47 	 0.04695 	 0.00400 	 m..s
   57 	    48 	 0.05202 	 0.00402 	 m..s
   22 	    49 	 0.04773 	 0.00402 	 m..s
   85 	    50 	 0.05644 	 0.00404 	 m..s
   55 	    51 	 0.05161 	 0.00411 	 m..s
   36 	    52 	 0.04922 	 0.00413 	 m..s
   52 	    53 	 0.05099 	 0.00416 	 m..s
   26 	    54 	 0.04831 	 0.00417 	 m..s
   16 	    55 	 0.04592 	 0.00418 	 m..s
   47 	    56 	 0.05068 	 0.00421 	 m..s
   44 	    57 	 0.05003 	 0.00421 	 m..s
   35 	    58 	 0.04914 	 0.00425 	 m..s
   34 	    59 	 0.04902 	 0.00428 	 m..s
   12 	    60 	 0.04514 	 0.00429 	 m..s
   54 	    61 	 0.05149 	 0.00431 	 m..s
   23 	    62 	 0.04780 	 0.00432 	 m..s
   46 	    63 	 0.05067 	 0.00435 	 m..s
   61 	    64 	 0.05230 	 0.00438 	 m..s
   65 	    65 	 0.05264 	 0.00438 	 m..s
   39 	    66 	 0.04963 	 0.00445 	 m..s
   40 	    67 	 0.04968 	 0.00448 	 m..s
   49 	    68 	 0.05087 	 0.00456 	 m..s
   28 	    69 	 0.04858 	 0.00456 	 m..s
   21 	    70 	 0.04756 	 0.00459 	 m..s
   76 	    71 	 0.05448 	 0.00464 	 m..s
   29 	    72 	 0.04865 	 0.00465 	 m..s
   72 	    73 	 0.05349 	 0.00466 	 m..s
   80 	    74 	 0.05515 	 0.00469 	 m..s
   84 	    75 	 0.05627 	 0.00476 	 m..s
   56 	    76 	 0.05197 	 0.00492 	 m..s
   53 	    77 	 0.05111 	 0.00495 	 m..s
    9 	    78 	 0.04411 	 0.00498 	 m..s
   79 	    79 	 0.05507 	 0.00533 	 m..s
   48 	    80 	 0.05076 	 0.00533 	 m..s
   59 	    81 	 0.05217 	 0.00534 	 m..s
   68 	    82 	 0.05296 	 0.00574 	 m..s
   70 	    83 	 0.05329 	 0.00577 	 m..s
   30 	    84 	 0.04872 	 0.00711 	 m..s
   43 	    85 	 0.04990 	 0.00867 	 m..s
   32 	    86 	 0.04886 	 0.01061 	 m..s
   38 	    87 	 0.04962 	 0.01439 	 m..s
   91 	    88 	 0.07505 	 0.02355 	 m..s
   90 	    89 	 0.07282 	 0.02942 	 m..s
   89 	    90 	 0.05734 	 0.03090 	 ~...
   88 	    91 	 0.05705 	 0.03330 	 ~...
  100 	    92 	 0.15378 	 0.03527 	 MISS
   96 	    93 	 0.13566 	 0.07075 	 m..s
   92 	    94 	 0.13187 	 0.10894 	 ~...
   95 	    95 	 0.13481 	 0.12593 	 ~...
   93 	    96 	 0.13219 	 0.12634 	 ~...
   97 	    97 	 0.13887 	 0.12803 	 ~...
   98 	    98 	 0.14216 	 0.13628 	 ~...
   94 	    99 	 0.13300 	 0.13734 	 ~...
   99 	   100 	 0.14223 	 0.13806 	 ~...
  109 	   101 	 0.22697 	 0.15098 	 m..s
  101 	   102 	 0.17347 	 0.16626 	 ~...
  106 	   103 	 0.19651 	 0.17766 	 ~...
  108 	   104 	 0.21290 	 0.18578 	 ~...
  102 	   105 	 0.17926 	 0.19078 	 ~...
  103 	   106 	 0.18756 	 0.19121 	 ~...
  104 	   107 	 0.18770 	 0.19484 	 ~...
  107 	   108 	 0.21134 	 0.19940 	 ~...
  105 	   109 	 0.18911 	 0.20961 	 ~...
  110 	   110 	 0.24895 	 0.23281 	 ~...
  111 	   111 	 0.25807 	 0.24938 	 ~...
  113 	   112 	 0.26699 	 0.25004 	 ~...
  112 	   113 	 0.26504 	 0.25504 	 ~...
  115 	   114 	 0.27472 	 0.26809 	 ~...
  116 	   115 	 0.28233 	 0.26879 	 ~...
  117 	   116 	 0.28276 	 0.27192 	 ~...
  114 	   117 	 0.27249 	 0.27244 	 ~...
  118 	   118 	 0.29769 	 0.28215 	 ~...
  119 	   119 	 0.29873 	 0.29488 	 ~...
  120 	   120 	 0.30420 	 0.29973 	 ~...
==========================================
r_mrr = 0.9860864877700806
r2_mrr = 0.7623239755630493
spearmanr_mrr@5 = 0.9378167390823364
spearmanr_mrr@10 = 0.9702292084693909
spearmanr_mrr@50 = 0.9905881285667419
spearmanr_mrr@100 = 0.993972659111023
spearmanr_mrr@All = 0.9939995408058167
==========================================
test time: 0.456
Done Testing dataset CoDExSmall
total time taken: 238.03322100639343
training time taken: 227.17742896080017
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9861)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7623)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9378)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9702)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9906)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9940)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9940)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.3236660338936872}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 5557917464467117
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [536, 776, 661, 450, 1058, 515, 1159, 9, 545, 944, 152, 91, 681, 403, 281, 639, 694, 237, 885, 192, 758, 1199, 997, 980, 747, 43, 1163, 331, 894, 358, 292, 527, 990, 61, 993, 433, 383, 273, 507, 1138, 957, 1100, 1157, 1214, 122, 905, 73, 808, 249, 924, 219, 1107, 898, 766, 832, 820, 398, 550, 1172, 24, 922, 880, 189, 340, 1090, 367, 795, 893, 951, 900, 113, 979, 706, 910, 623, 1034, 402, 1054, 723, 860, 998, 368, 1030, 174, 471, 1061, 597, 326, 197, 62, 244, 33, 265, 927, 289, 742, 1140, 1213, 755, 803, 875, 656, 225, 140, 921, 1057, 72, 884, 391, 194, 112, 557, 274, 794, 253, 59, 1044, 212, 878, 58, 60]
valid_ids (0): []
train_ids (1094): [185, 401, 912, 609, 526, 941, 102, 588, 355, 1023, 1152, 704, 790, 210, 310, 566, 770, 655, 240, 36, 147, 809, 46, 1046, 899, 1015, 207, 458, 1069, 1207, 211, 916, 182, 939, 302, 975, 428, 1062, 290, 188, 642, 1070, 1174, 1202, 276, 1007, 709, 449, 77, 695, 378, 343, 381, 646, 901, 562, 1173, 834, 89, 1137, 1043, 1001, 598, 178, 807, 640, 11, 915, 349, 455, 481, 88, 464, 114, 1098, 115, 432, 670, 828, 931, 32, 144, 238, 1119, 316, 602, 518, 519, 744, 421, 865, 802, 1200, 845, 54, 215, 148, 506, 227, 153, 1197, 680, 388, 959, 844, 788, 821, 826, 982, 1005, 1047, 1193, 141, 914, 1056, 732, 396, 136, 259, 283, 389, 1110, 1080, 171, 517, 1032, 604, 40, 1167, 676, 621, 1180, 745, 569, 82, 218, 260, 606, 440, 360, 940, 962, 395, 35, 964, 778, 772, 1150, 119, 567, 268, 1022, 529, 128, 241, 970, 654, 105, 1203, 243, 356, 1106, 41, 352, 801, 1081, 612, 50, 181, 484, 1004, 1008, 262, 1040, 669, 1039, 1155, 137, 172, 15, 601, 864, 416, 693, 994, 477, 282, 871, 354, 846, 781, 607, 819, 516, 1149, 671, 659, 441, 415, 904, 638, 591, 474, 1093, 165, 126, 85, 1078, 973, 1088, 1195, 1136, 1189, 318, 1184, 969, 261, 1094, 525, 836, 103, 304, 1059, 540, 359, 193, 184, 689, 1082, 495, 869, 311, 816, 1158, 1120, 56, 721, 399, 462, 1160, 374, 1052, 867, 1182, 690, 1037, 67, 677, 216, 626, 332, 135, 955, 662, 339, 53, 929, 938, 874, 424, 1031, 280, 3, 502, 1161, 762, 1102, 1141, 538, 317, 173, 1049, 574, 838, 1143, 371, 635, 1133, 503, 786, 937, 157, 926, 622, 556, 907, 409, 7, 568, 175, 658, 101, 156, 314, 966, 97, 812, 925, 267, 217, 443, 668, 445, 30, 734, 1194, 201, 579, 902, 1072, 978, 51, 501, 679, 1105, 263, 228, 315, 410, 999, 37, 120, 111, 553, 949, 943, 1071, 633, 1186, 542, 1051, 544, 168, 223, 796, 489, 329, 511, 44, 585, 96, 775, 202, 13, 861, 420, 1073, 858, 664, 1027, 710, 840, 996, 851, 1122, 774, 342, 463, 151, 444, 1118, 269, 950, 752, 10, 341, 663, 648, 1166, 84, 879, 883, 909, 124, 737, 876, 531, 643, 68, 20, 595, 429, 627, 967, 337, 473, 974, 308, 888, 1142, 896, 815, 652, 408, 320, 992, 230, 465, 348, 75, 221, 892, 561, 232, 1135, 573, 347, 1038, 453, 14, 678, 270, 667, 239, 327, 490, 98, 437, 716, 376, 1124, 948, 499, 1154, 653, 920, 749, 1020, 229, 350, 482, 986, 1201, 512, 684, 582, 1003, 850, 264, 246, 1025, 1128, 149, 130, 1171, 1036, 176, 1178, 1168, 1147, 351, 988, 251, 647, 400, 767, 711, 377, 1177, 972, 822, 768, 254, 129, 200, 434, 1029, 613, 1095, 305, 422, 452, 746, 641, 522, 146, 323, 1017, 952, 863, 196, 719, 603, 1132, 509, 1156, 31, 100, 1145, 514, 1188, 995, 143, 1033, 636, 150, 1055, 703, 5, 468, 220, 319, 806, 780, 438, 810, 583, 480, 303, 106, 682, 700, 696, 1169, 1097, 565, 117, 1101, 38, 321, 632, 797, 532, 406, 523, 1196, 439, 71, 571, 279, 45, 191, 123, 1086, 764, 372, 908, 1125, 322, 166, 580, 508, 1117, 785, 163, 811, 451, 418, 387, 1079, 483, 1096, 170, 817, 284, 498, 42, 79, 599, 835, 674, 492, 397, 0, 1181, 563, 1065, 467, 127, 724, 25, 338, 447, 584, 634, 868, 208, 824, 513, 618, 738, 890, 843, 702, 945, 87, 534, 345, 686, 1076, 575, 707, 392, 739, 86, 205, 167, 855, 257, 1010, 731, 384, 57, 611, 177, 277, 1103, 1068, 1146, 328, 1139, 295, 954, 1091, 946, 160, 300, 18, 1075, 688, 1115, 620, 159, 161, 1035, 90, 625, 34, 287, 779, 1041, 162, 369, 1179, 630, 541, 761, 558, 4, 976, 906, 547, 554, 385, 947, 521, 829, 769, 1151, 1165, 154, 248, 935, 334, 1191, 722, 985, 226, 981, 80, 255, 754, 740, 759, 818, 577, 27, 930, 672, 109, 203, 1176, 793, 644, 827, 1011, 1063, 309, 751, 81, 714, 831, 1108, 407, 520, 206, 551, 275, 330, 637, 903, 792, 617, 825, 1074, 991, 692, 842, 76, 1016, 297, 862, 1208, 891, 699, 49, 23, 375, 121, 837, 728, 530, 475, 727, 675, 799, 581, 1013, 139, 460, 1121, 555, 1092, 570, 390, 546, 266, 1112, 1024, 17, 22, 705, 614, 748, 199, 272, 1014, 771, 763, 1009, 1048, 685, 1162, 872, 715, 1019, 19, 989, 628, 804, 800, 134, 735, 419, 236, 1116, 446, 650, 537, 870, 222, 918, 660, 651, 645, 426, 494, 491, 971, 294, 605, 805, 179, 857, 708, 1066, 487, 183, 589, 234, 125, 1083, 64, 1002, 743, 886, 26, 431, 718, 958, 65, 132, 1211, 1175, 336, 687, 936, 1, 344, 984, 209, 466, 1085, 213, 1111, 285, 1131, 756, 848, 859, 928, 1198, 497, 1134, 1153, 164, 454, 629, 960, 853, 169, 965, 1064, 942, 363, 92, 977, 765, 1164, 247, 730, 1185, 469, 791, 48, 413, 291, 624, 52, 543, 1123, 312, 63, 757, 839, 616, 733, 934, 913, 968, 1050, 78, 698, 1099, 911, 505, 1109, 430, 987, 94, 95, 535, 457, 560, 190, 887, 983, 590, 296, 1087, 823, 919, 787, 528, 673, 138, 414, 683, 373, 1067, 596, 155, 1127, 1130, 881, 877, 564, 353, 1053, 70, 252, 404, 773, 932, 852, 313, 539, 488, 195, 1190, 783, 572, 107, 66, 760, 435, 1006, 1104, 849, 666, 28, 953, 286, 245, 665, 578, 1114, 293, 559, 1206, 720, 364, 346, 83, 470, 963, 224, 600, 1028, 256, 923, 729, 961, 8, 833, 895, 99, 366, 423, 459, 69, 1187, 649, 592, 370, 301, 242, 1060, 631, 496, 1126, 712, 250, 158, 713, 576, 889, 549, 231, 610, 448, 436, 417, 21, 1113, 472, 586, 55, 411, 214, 116, 74, 271, 753, 1192, 782, 93, 278, 306, 1084, 405, 789, 1000, 933, 258, 1077, 813, 118, 362, 479, 16, 608, 1209, 1183, 594, 717, 382, 701, 361, 476, 427, 180, 814, 741, 504, 288, 1212, 548, 394, 1089, 847, 298, 204, 917, 2, 1045, 593, 524, 510, 299, 307, 412, 726, 866, 533, 854, 1026, 187, 873, 1144, 325, 365, 442, 750, 145, 697, 882, 29, 6, 47, 841, 380, 357, 335, 1148, 478, 956, 133, 198, 233, 615, 386, 830, 856, 1210, 784, 485, 552, 1018, 1204, 12, 1205, 104, 108, 1129, 777, 379, 461, 333, 657, 1042, 131, 691, 1021, 493, 110, 393, 619, 456, 425, 486, 186, 725, 1170, 500, 39, 324, 235, 897, 142, 798, 587, 736, 1012]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3165434729588150
the save name prefix for this run is:  chkpt-ID_3165434729588150_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1181
rank avg (pred): 0.601 +- 0.002
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004107453

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1047
rank avg (pred): 0.453 +- 0.262
mrr vals (pred, true): 0.009, 0.005
batch losses (mrrl, rdl): 0.0, 8.4765e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 476
rank avg (pred): 0.459 +- 0.275
mrr vals (pred, true): 0.006, 0.005
batch losses (mrrl, rdl): 0.0, 4.2227e-06

Epoch over!
epoch time: 15.108

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 522
rank avg (pred): 0.373 +- 0.250
mrr vals (pred, true): 0.008, 0.008
batch losses (mrrl, rdl): 0.0, 9.3068e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 614
rank avg (pred): 0.469 +- 0.260
mrr vals (pred, true): 0.003, 0.003
batch losses (mrrl, rdl): 0.0, 2.6375e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 57
rank avg (pred): 0.054 +- 0.136
mrr vals (pred, true): 0.123, 0.114
batch losses (mrrl, rdl): 0.0, 3.25027e-05

Epoch over!
epoch time: 15.052

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.365 +- 0.259
mrr vals (pred, true): 0.008, 0.008
batch losses (mrrl, rdl): 0.0, 3.9729e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.514 +- 0.277
mrr vals (pred, true): 0.002, 0.002
batch losses (mrrl, rdl): 0.0, 3.17454e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 702
rank avg (pred): 0.468 +- 0.273
mrr vals (pred, true): 0.003, 0.003
batch losses (mrrl, rdl): 0.0, 2.3233e-06

Epoch over!
epoch time: 15.007

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.461 +- 0.278
mrr vals (pred, true): 0.004, 0.004
batch losses (mrrl, rdl): 0.0, 2.7075e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.068 +- 0.171
mrr vals (pred, true): 0.297, 0.210
batch losses (mrrl, rdl): 0.0, 6.8097e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 96
rank avg (pred): 0.453 +- 0.283
mrr vals (pred, true): 0.006, 0.004
batch losses (mrrl, rdl): 0.0, 2.5935e-06

Epoch over!
epoch time: 14.95

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 874
rank avg (pred): 0.464 +- 0.292
mrr vals (pred, true): 0.005, 0.004
batch losses (mrrl, rdl): 0.0, 2.1882e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 970
rank avg (pred): 0.551 +- 0.315
mrr vals (pred, true): 0.003, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001749987

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 106
rank avg (pred): 0.465 +- 0.303
mrr vals (pred, true): 0.012, 0.004
batch losses (mrrl, rdl): 0.0, 4.9715e-06

Epoch over!
epoch time: 15.027

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1103
rank avg (pred): 0.470 +- 0.273
mrr vals (pred, true): 0.006, 0.005
batch losses (mrrl, rdl): 0.0196727775, 2.4199e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.424 +- 0.238
mrr vals (pred, true): 0.047, 0.017
batch losses (mrrl, rdl): 7.72429e-05, 0.0005095567

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 306
rank avg (pred): 0.119 +- 0.125
mrr vals (pred, true): 0.201, 0.182
batch losses (mrrl, rdl): 0.0038706055, 4.85849e-05

Epoch over!
epoch time: 15.116

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.442 +- 0.238
mrr vals (pred, true): 0.044, 0.005
batch losses (mrrl, rdl): 0.0003940572, 1.70238e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 11
rank avg (pred): 0.120 +- 0.119
mrr vals (pred, true): 0.210, 0.264
batch losses (mrrl, rdl): 0.0285706688, 0.0001210229

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 195
rank avg (pred): 0.448 +- 0.245
mrr vals (pred, true): 0.072, 0.004
batch losses (mrrl, rdl): 0.0046910285, 1.30266e-05

Epoch over!
epoch time: 15.01

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 617
rank avg (pred): 0.451 +- 0.235
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 2.862e-07, 1.47556e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 724
rank avg (pred): 0.466 +- 0.217
mrr vals (pred, true): 0.036, 0.004
batch losses (mrrl, rdl): 0.0020425555, 1.93315e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 528
rank avg (pred): 0.453 +- 0.229
mrr vals (pred, true): 0.043, 0.010
batch losses (mrrl, rdl): 0.0004296517, 0.0001545573

Epoch over!
epoch time: 15.006

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 51
rank avg (pred): 0.121 +- 0.089
mrr vals (pred, true): 0.158, 0.128
batch losses (mrrl, rdl): 0.0089250561, 3.33672e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 179
rank avg (pred): 0.442 +- 0.244
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.000225095, 1.63342e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1183
rank avg (pred): 0.445 +- 0.241
mrr vals (pred, true): 0.054, 0.002
batch losses (mrrl, rdl): 0.0001862967, 1.32255e-05

Epoch over!
epoch time: 14.986

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1035
rank avg (pred): 0.456 +- 0.237
mrr vals (pred, true): 0.046, 0.005
batch losses (mrrl, rdl): 0.0001669603, 1.39202e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 52
rank avg (pred): 0.237 +- 0.164
mrr vals (pred, true): 0.127, 0.126
batch losses (mrrl, rdl): 9.2353e-06, 0.0005441195

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 246
rank avg (pred): 0.064 +- 0.048
mrr vals (pred, true): 0.271, 0.269
batch losses (mrrl, rdl): 4.9406e-05, 1.40339e-05

Epoch over!
epoch time: 14.978

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 857
rank avg (pred): 0.446 +- 0.237
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.85472e-05, 2.25614e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 948
rank avg (pred): 0.521 +- 0.257
mrr vals (pred, true): 0.043, 0.003
batch losses (mrrl, rdl): 0.0005083362, 6.21083e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 46
rank avg (pred): 0.105 +- 0.078
mrr vals (pred, true): 0.170, 0.141
batch losses (mrrl, rdl): 0.0084013501, 1.54536e-05

Epoch over!
epoch time: 15.085

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 495
rank avg (pred): 0.454 +- 0.228
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 9.43431e-05, 0.0007074259

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.104 +- 0.083
mrr vals (pred, true): 0.244, 0.036
batch losses (mrrl, rdl): 0.37502262, 0.0006197771

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1117
rank avg (pred): 0.458 +- 0.228
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 5.81e-08, 1.82733e-05

Epoch over!
epoch time: 15.269

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.483 +- 0.234
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 9.03595e-05, 2.31758e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 319
rank avg (pred): 0.087 +- 0.063
mrr vals (pred, true): 0.147, 0.171
batch losses (mrrl, rdl): 0.0060037253, 9.2983e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.459 +- 0.215
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 3e-10, 2.18121e-05

Epoch over!
epoch time: 15.163

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 585
rank avg (pred): 0.512 +- 0.211
mrr vals (pred, true): 0.034, 0.004
batch losses (mrrl, rdl): 0.0024050153, 4.94912e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 722
rank avg (pred): 0.464 +- 0.221
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.000578467, 1.90926e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 343
rank avg (pred): 0.453 +- 0.211
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001457509, 2.43634e-05

Epoch over!
epoch time: 15.034

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 991
rank avg (pred): 0.157 +- 0.121
mrr vals (pred, true): 0.215, 0.174
batch losses (mrrl, rdl): 0.0166950282, 0.000179988

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.465 +- 0.209
mrr vals (pred, true): 0.047, 0.005
batch losses (mrrl, rdl): 6.71185e-05, 2.38791e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 295
rank avg (pred): 0.014 +- 0.010
mrr vals (pred, true): 0.235, 0.179
batch losses (mrrl, rdl): 0.0314672627, 9.89042e-05

Epoch over!
epoch time: 15.035

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.433 +- 0.204
mrr vals (pred, true): 0.060, 0.007

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   54 	     0 	 0.05631 	 0.00089 	 m..s
   77 	     1 	 0.05737 	 0.00102 	 m..s
    1 	     2 	 0.03669 	 0.00113 	 m..s
    5 	     3 	 0.04434 	 0.00124 	 m..s
    6 	     4 	 0.04758 	 0.00127 	 m..s
    2 	     5 	 0.04319 	 0.00151 	 m..s
    3 	     6 	 0.04400 	 0.00159 	 m..s
    7 	     7 	 0.04831 	 0.00160 	 m..s
   78 	     8 	 0.05785 	 0.00174 	 m..s
    4 	     9 	 0.04415 	 0.00192 	 m..s
   56 	    10 	 0.05737 	 0.00273 	 m..s
   53 	    11 	 0.05630 	 0.00280 	 m..s
   24 	    12 	 0.05475 	 0.00287 	 m..s
   20 	    13 	 0.05444 	 0.00289 	 m..s
   56 	    14 	 0.05737 	 0.00291 	 m..s
   56 	    15 	 0.05737 	 0.00307 	 m..s
   30 	    16 	 0.05507 	 0.00308 	 m..s
   56 	    17 	 0.05737 	 0.00317 	 m..s
   25 	    18 	 0.05484 	 0.00321 	 m..s
   40 	    19 	 0.05544 	 0.00334 	 m..s
   15 	    20 	 0.05375 	 0.00335 	 m..s
   27 	    21 	 0.05503 	 0.00346 	 m..s
   56 	    22 	 0.05737 	 0.00347 	 m..s
   29 	    23 	 0.05504 	 0.00357 	 m..s
   56 	    24 	 0.05737 	 0.00358 	 m..s
   82 	    25 	 0.05849 	 0.00369 	 m..s
   56 	    26 	 0.05737 	 0.00372 	 m..s
   56 	    27 	 0.05737 	 0.00375 	 m..s
   44 	    28 	 0.05574 	 0.00376 	 m..s
   56 	    29 	 0.05737 	 0.00376 	 m..s
   18 	    30 	 0.05432 	 0.00377 	 m..s
   48 	    31 	 0.05581 	 0.00389 	 m..s
   56 	    32 	 0.05737 	 0.00391 	 m..s
   36 	    33 	 0.05519 	 0.00396 	 m..s
   43 	    34 	 0.05565 	 0.00396 	 m..s
   47 	    35 	 0.05580 	 0.00399 	 m..s
   16 	    36 	 0.05395 	 0.00402 	 m..s
   81 	    37 	 0.05811 	 0.00403 	 m..s
   10 	    38 	 0.05072 	 0.00408 	 m..s
   56 	    39 	 0.05737 	 0.00410 	 m..s
   56 	    40 	 0.05737 	 0.00410 	 m..s
   56 	    41 	 0.05737 	 0.00411 	 m..s
   55 	    42 	 0.05684 	 0.00413 	 m..s
    0 	    43 	 0.03667 	 0.00415 	 m..s
   56 	    44 	 0.05737 	 0.00415 	 m..s
   56 	    45 	 0.05737 	 0.00416 	 m..s
   49 	    46 	 0.05586 	 0.00420 	 m..s
   26 	    47 	 0.05502 	 0.00424 	 m..s
   33 	    48 	 0.05515 	 0.00425 	 m..s
   42 	    49 	 0.05558 	 0.00428 	 m..s
    9 	    50 	 0.05033 	 0.00429 	 m..s
   31 	    51 	 0.05511 	 0.00433 	 m..s
    8 	    52 	 0.05015 	 0.00433 	 m..s
   12 	    53 	 0.05203 	 0.00433 	 m..s
   21 	    54 	 0.05447 	 0.00435 	 m..s
   35 	    55 	 0.05518 	 0.00440 	 m..s
   50 	    56 	 0.05596 	 0.00444 	 m..s
   32 	    57 	 0.05512 	 0.00444 	 m..s
   56 	    58 	 0.05737 	 0.00445 	 m..s
   52 	    59 	 0.05604 	 0.00448 	 m..s
   45 	    60 	 0.05578 	 0.00450 	 m..s
   38 	    61 	 0.05540 	 0.00455 	 m..s
   80 	    62 	 0.05805 	 0.00456 	 m..s
   39 	    63 	 0.05541 	 0.00478 	 m..s
   56 	    64 	 0.05737 	 0.00492 	 m..s
   28 	    65 	 0.05503 	 0.00496 	 m..s
   46 	    66 	 0.05580 	 0.00505 	 m..s
   83 	    67 	 0.05954 	 0.00515 	 m..s
   34 	    68 	 0.05515 	 0.00517 	 m..s
   56 	    69 	 0.05737 	 0.00545 	 m..s
   56 	    70 	 0.05737 	 0.00550 	 m..s
   41 	    71 	 0.05548 	 0.00574 	 m..s
   56 	    72 	 0.05737 	 0.00597 	 m..s
   51 	    73 	 0.05599 	 0.00604 	 m..s
   14 	    74 	 0.05366 	 0.00635 	 m..s
   84 	    75 	 0.05972 	 0.00719 	 m..s
   23 	    76 	 0.05461 	 0.00787 	 m..s
   17 	    77 	 0.05415 	 0.00815 	 m..s
   37 	    78 	 0.05535 	 0.00858 	 m..s
   19 	    79 	 0.05432 	 0.01098 	 m..s
   22 	    80 	 0.05450 	 0.01237 	 m..s
   56 	    81 	 0.05737 	 0.02634 	 m..s
   79 	    82 	 0.05802 	 0.02942 	 ~...
   85 	    83 	 0.06036 	 0.03090 	 ~...
   13 	    84 	 0.05359 	 0.03521 	 ~...
   11 	    85 	 0.05112 	 0.04296 	 ~...
   87 	    86 	 0.10665 	 0.10894 	 ~...
   86 	    87 	 0.10645 	 0.11743 	 ~...
   93 	    88 	 0.14008 	 0.12326 	 ~...
   88 	    89 	 0.10822 	 0.13339 	 ~...
   89 	    90 	 0.11184 	 0.13596 	 ~...
   91 	    91 	 0.11622 	 0.13654 	 ~...
   92 	    92 	 0.12003 	 0.13821 	 ~...
   94 	    93 	 0.14228 	 0.13846 	 ~...
   90 	    94 	 0.11187 	 0.13961 	 ~...
  100 	    95 	 0.21386 	 0.16128 	 m..s
   98 	    96 	 0.16884 	 0.16997 	 ~...
   99 	    97 	 0.17274 	 0.17271 	 ~...
   97 	    98 	 0.15438 	 0.17434 	 ~...
  101 	    99 	 0.21526 	 0.18551 	 ~...
   95 	   100 	 0.15048 	 0.19078 	 m..s
   96 	   101 	 0.15430 	 0.19918 	 m..s
  102 	   102 	 0.21804 	 0.20171 	 ~...
  105 	   103 	 0.23868 	 0.23195 	 ~...
  107 	   104 	 0.24509 	 0.23552 	 ~...
  106 	   105 	 0.23911 	 0.24180 	 ~...
  108 	   106 	 0.24763 	 0.24983 	 ~...
  111 	   107 	 0.25243 	 0.25585 	 ~...
  103 	   108 	 0.22232 	 0.25840 	 m..s
  114 	   109 	 0.26152 	 0.26465 	 ~...
  112 	   110 	 0.25331 	 0.26660 	 ~...
  116 	   111 	 0.26611 	 0.28375 	 ~...
  113 	   112 	 0.25640 	 0.28784 	 m..s
  117 	   113 	 0.26983 	 0.29833 	 ~...
  104 	   114 	 0.23775 	 0.30430 	 m..s
  109 	   115 	 0.24951 	 0.31382 	 m..s
  118 	   116 	 0.27485 	 0.31875 	 m..s
  119 	   117 	 0.28084 	 0.32046 	 m..s
  120 	   118 	 0.29727 	 0.32135 	 ~...
  115 	   119 	 0.26398 	 0.32643 	 m..s
  110 	   120 	 0.25177 	 0.33382 	 m..s
==========================================
r_mrr = 0.982153594493866
r2_mrr = 0.8142212629318237
spearmanr_mrr@5 = 0.9912766218185425
spearmanr_mrr@10 = 0.9052335023880005
spearmanr_mrr@50 = 0.9845319986343384
spearmanr_mrr@100 = 0.9898678064346313
spearmanr_mrr@All = 0.9903169870376587
==========================================
test time: 0.454
Done Testing dataset CoDExSmall
total time taken: 237.089293718338
training time taken: 226.29071307182312
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9822)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8142)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9913)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9052)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9845)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9899)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9903)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.4338369377799154}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 3827817496515613
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [226, 23, 1069, 184, 279, 627, 408, 409, 767, 595, 272, 106, 421, 938, 362, 207, 629, 598, 888, 415, 55, 12, 1080, 430, 585, 677, 248, 703, 200, 165, 856, 394, 1133, 539, 616, 914, 471, 127, 423, 407, 607, 203, 138, 543, 624, 974, 35, 779, 315, 561, 164, 747, 643, 1102, 71, 834, 395, 630, 224, 366, 281, 962, 67, 403, 546, 956, 817, 797, 680, 1023, 209, 28, 457, 427, 850, 617, 660, 141, 249, 709, 835, 126, 13, 620, 1091, 965, 515, 945, 379, 845, 812, 61, 582, 1022, 656, 462, 855, 72, 241, 433, 724, 406, 913, 580, 1114, 22, 45, 1166, 800, 1040, 339, 1213, 311, 1188, 280, 1161, 712, 973, 182, 1032, 548]
valid_ids (0): []
train_ids (1094): [128, 720, 640, 42, 1155, 260, 1158, 46, 443, 648, 437, 678, 794, 981, 1143, 826, 1063, 1202, 1013, 1151, 189, 438, 1025, 301, 450, 979, 823, 1167, 635, 190, 150, 988, 1203, 738, 731, 51, 94, 244, 198, 560, 771, 840, 626, 987, 1092, 773, 518, 488, 901, 912, 802, 1019, 1137, 98, 199, 684, 966, 692, 204, 59, 179, 833, 116, 392, 376, 1131, 1126, 1189, 468, 781, 950, 711, 188, 69, 15, 1109, 612, 1101, 628, 1116, 65, 266, 1209, 536, 553, 594, 155, 556, 1008, 633, 479, 131, 1174, 934, 611, 578, 830, 389, 192, 780, 355, 228, 1185, 107, 665, 646, 565, 424, 551, 1169, 976, 986, 191, 1181, 60, 236, 879, 1160, 1094, 1165, 210, 841, 849, 1117, 1083, 717, 208, 1049, 157, 809, 1175, 89, 744, 168, 805, 1195, 385, 922, 118, 1200, 1146, 202, 66, 520, 801, 730, 186, 796, 1029, 253, 920, 1003, 225, 445, 25, 785, 926, 872, 26, 1010, 770, 1118, 664, 758, 234, 881, 977, 32, 975, 1128, 402, 152, 31, 848, 832, 348, 869, 610, 492, 143, 896, 533, 196, 337, 153, 1211, 768, 718, 64, 156, 482, 854, 217, 992, 563, 1072, 372, 916, 282, 729, 119, 255, 755, 857, 306, 144, 527, 1140, 838, 490, 81, 398, 417, 1021, 792, 439, 762, 302, 336, 714, 657, 695, 806, 935, 275, 122, 899, 478, 382, 844, 432, 137, 257, 748, 774, 605, 606, 903, 171, 230, 690, 1179, 1110, 444, 795, 276, 428, 353, 685, 504, 526, 906, 475, 175, 27, 231, 290, 1144, 333, 882, 622, 1024, 516, 220, 951, 619, 1205, 426, 608, 1051, 791, 908, 895, 694, 4, 52, 365, 623, 486, 215, 24, 592, 753, 1170, 999, 318, 1104, 851, 383, 1044, 63, 1000, 335, 1120, 875, 1136, 1097, 452, 419, 638, 1016, 9, 980, 810, 710, 998, 615, 1050, 238, 573, 500, 82, 943, 289, 173, 741, 263, 864, 265, 575, 1046, 530, 581, 227, 245, 989, 347, 380, 877, 522, 1129, 1027, 216, 708, 576, 632, 567, 890, 268, 158, 867, 1065, 1015, 698, 1036, 101, 1115, 892, 787, 219, 944, 736, 964, 324, 146, 113, 212, 545, 273, 596, 982, 30, 889, 507, 843, 497, 505, 572, 878, 859, 474, 679, 1178, 43, 672, 370, 1020, 374, 411, 993, 343, 528, 655, 557, 295, 47, 930, 1154, 498, 40, 270, 682, 509, 1043, 304, 1100, 501, 559, 529, 1005, 50, 776, 961, 955, 1098, 931, 317, 1111, 136, 900, 727, 940, 274, 1041, 466, 880, 681, 29, 142, 719, 166, 658, 554, 434, 360, 740, 1075, 871, 1055, 994, 341, 897, 1150, 702, 465, 405, 669, 7, 705, 183, 893, 163, 332, 907, 316, 176, 853, 455, 958, 707, 991, 499, 1196, 733, 358, 1107, 160, 614, 1142, 48, 821, 641, 804, 418, 1141, 870, 330, 1124, 997, 667, 1152, 858, 798, 932, 1033, 891, 1085, 1047, 177, 484, 344, 722, 86, 11, 1009, 267, 53, 1077, 873, 162, 1156, 754, 827, 359, 493, 93, 378, 923, 625, 1157, 194, 464, 299, 750, 847, 777, 1093, 898, 503, 735, 259, 1186, 250, 495, 233, 1004, 591, 33, 613, 132, 1037, 454, 320, 531, 334, 278, 577, 808, 721, 1076, 73, 970, 953, 621, 1028, 745, 579, 1187, 960, 58, 18, 1180, 167, 201, 1149, 675, 397, 583, 76, 697, 769, 1011, 996, 502, 728, 1132, 972, 886, 1201, 524, 558, 569, 704, 737, 534, 0, 477, 414, 393, 37, 910, 793, 125, 822, 1038, 1057, 815, 1064, 453, 1067, 839, 661, 751, 1066, 663, 757, 968, 590, 650, 924, 340, 480, 723, 642, 1190, 1173, 725, 352, 1212, 799, 766, 170, 458, 990, 361, 151, 784, 172, 456, 1054, 699, 921, 825, 284, 1007, 446, 121, 863, 1134, 514, 19, 749, 952, 555, 243, 1127, 1030, 743, 1035, 283, 928, 390, 1122, 83, 496, 1073, 399, 510, 541, 1139, 49, 1006, 1042, 396, 84, 90, 254, 102, 1171, 92, 1135, 44, 971, 235, 742, 463, 1058, 140, 451, 20, 1084, 447, 364, 287, 788, 782, 659, 297, 885, 1199, 852, 133, 1145, 865, 485, 519, 1061, 1113, 95, 936, 846, 862, 367, 139, 746, 91, 647, 87, 726, 688, 21, 75, 917, 1026, 652, 1088, 1108, 760, 149, 947, 193, 772, 237, 691, 288, 1159, 902, 487, 820, 1089, 384, 1214, 449, 1204, 512, 161, 756, 828, 1130, 294, 459, 74, 425, 300, 586, 247, 790, 108, 568, 310, 670, 978, 36, 1034, 1182, 134, 588, 811, 1078, 124, 338, 494, 242, 508, 148, 765, 874, 346, 412, 1060, 214, 431, 544, 313, 861, 481, 1002, 589, 1177, 593, 473, 807, 195, 542, 789, 483, 400, 537, 5, 696, 1172, 54, 1017, 205, 375, 70, 314, 959, 1138, 1192, 229, 1095, 1070, 130, 783, 734, 239, 371, 683, 666, 538, 387, 946, 829, 887, 328, 420, 68, 422, 511, 1031, 868, 949, 566, 716, 99, 256, 57, 1062, 178, 842, 1087, 662, 114, 356, 814, 476, 246, 540, 824, 645, 429, 600, 1184, 221, 251, 836, 327, 331, 644, 404, 700, 1059, 564, 547, 584, 187, 112, 1208, 469, 277, 1148, 948, 876, 676, 1106, 1045, 915, 321, 639, 884, 1099, 883, 1123, 919, 816, 574, 96, 837, 587, 213, 441, 345, 1053, 103, 38, 1163, 1194, 115, 232, 1183, 467, 303, 312, 671, 100, 1048, 894, 14, 174, 941, 41, 686, 440, 381, 752, 206, 8, 298, 147, 985, 286, 363, 1210, 562, 532, 604, 957, 135, 775, 223, 88, 34, 939, 759, 552, 602, 351, 933, 995, 293, 1071, 307, 62, 984, 85, 687, 523, 104, 571, 342, 309, 368, 601, 109, 927, 218, 1018, 1103, 6, 386, 435, 1090, 1207, 350, 1112, 525, 271, 1039, 597, 145, 818, 325, 1164, 786, 1074, 354, 929, 261, 159, 111, 963, 105, 517, 636, 258, 262, 550, 79, 911, 909, 472, 78, 706, 1056, 388, 654, 56, 715, 269, 17, 461, 983, 442, 819, 673, 10, 954, 240, 1125, 308, 319, 506, 180, 570, 739, 129, 860, 296, 436, 1081, 413, 918, 535, 323, 1193, 1147, 448, 460, 491, 1206, 373, 377, 470, 693, 329, 1121, 1012, 80, 181, 1176, 764, 1153, 120, 322, 1068, 401, 264, 649, 1191, 357, 222, 937, 369, 252, 904, 77, 185, 110, 1197, 813, 599, 778, 1014, 1001, 609, 631, 154, 618, 549, 1096, 291, 1052, 803, 211, 1, 1198, 701, 653, 761, 117, 489, 1119, 831, 39, 416, 967, 169, 637, 2, 521, 942, 763, 603, 925, 285, 349, 905, 97, 326, 634, 732, 16, 1105, 410, 391, 668, 713, 1086, 651, 1168, 123, 689, 674, 969, 1162, 866, 1082, 197, 1079, 305, 292, 3, 513]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1767226810537736
the save name prefix for this run is:  chkpt-ID_1767226810537736_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.498 +- 0.005
mrr vals (pred, true): 0.001, 0.304
batch losses (mrrl, rdl): 0.0, 0.0044642813

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 266
rank avg (pred): 0.036 +- 0.026
mrr vals (pred, true): 0.177, 0.357
batch losses (mrrl, rdl): 0.0, 1.2978e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 505
rank avg (pred): 0.400 +- 0.269
mrr vals (pred, true): 0.070, 0.030
batch losses (mrrl, rdl): 0.0, 5.31524e-05

Epoch over!
epoch time: 14.825

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.379 +- 0.263
mrr vals (pred, true): 0.082, 0.008
batch losses (mrrl, rdl): 0.0, 7.1585e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 185
rank avg (pred): 0.428 +- 0.268
mrr vals (pred, true): 0.032, 0.006
batch losses (mrrl, rdl): 0.0, 1.61725e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1191
rank avg (pred): 0.475 +- 0.262
mrr vals (pred, true): 0.007, 0.003
batch losses (mrrl, rdl): 0.0, 7.2651e-06

Epoch over!
epoch time: 14.875

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 792
rank avg (pred): 0.481 +- 0.280
mrr vals (pred, true): 0.010, 0.005
batch losses (mrrl, rdl): 0.0, 1.21283e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 545
rank avg (pred): 0.420 +- 0.253
mrr vals (pred, true): 0.005, 0.011
batch losses (mrrl, rdl): 0.0, 9.52438e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 110
rank avg (pred): 0.466 +- 0.265
mrr vals (pred, true): 0.003, 0.005
batch losses (mrrl, rdl): 0.0, 3.7765e-06

Epoch over!
epoch time: 14.93

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.500 +- 0.287
mrr vals (pred, true): 0.002, 0.005
batch losses (mrrl, rdl): 0.0, 1.49265e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1017
rank avg (pred): 0.465 +- 0.276
mrr vals (pred, true): 0.003, 0.003
batch losses (mrrl, rdl): 0.0, 1.061e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 262
rank avg (pred): 0.052 +- 0.136
mrr vals (pred, true): 0.044, 0.318
batch losses (mrrl, rdl): 0.0, 1.39361e-05

Epoch over!
epoch time: 15.069

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 926
rank avg (pred): 0.542 +- 0.292
mrr vals (pred, true): 0.002, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002759593

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 859
rank avg (pred): 0.474 +- 0.270
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0, 4.845e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 490
rank avg (pred): 0.352 +- 0.252
mrr vals (pred, true): 0.005, 0.025
batch losses (mrrl, rdl): 0.0, 1.51334e-05

Epoch over!
epoch time: 15.083

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.491 +- 0.277
mrr vals (pred, true): 0.002, 0.004
batch losses (mrrl, rdl): 0.0227626767, 1.15958e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 132
rank avg (pred): 0.472 +- 0.261
mrr vals (pred, true): 0.040, 0.005
batch losses (mrrl, rdl): 0.0009634132, 1.09221e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 636
rank avg (pred): 0.453 +- 0.260
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004875611, 1.89086e-05

Epoch over!
epoch time: 15.316

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 134
rank avg (pred): 0.461 +- 0.253
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.24099e-05, 6.1174e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1060
rank avg (pred): 0.022 +- 0.051
mrr vals (pred, true): 0.308, 0.314
batch losses (mrrl, rdl): 0.0003651841, 6.0276e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1071
rank avg (pred): 0.032 +- 0.060
mrr vals (pred, true): 0.270, 0.199
batch losses (mrrl, rdl): 0.0491984785, 4.36691e-05

Epoch over!
epoch time: 15.262

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 869
rank avg (pred): 0.455 +- 0.243
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 3.73795e-05, 7.881e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.064 +- 0.089
mrr vals (pred, true): 0.220, 0.173
batch losses (mrrl, rdl): 0.022512896, 3.446e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 130
rank avg (pred): 0.490 +- 0.262
mrr vals (pred, true): 0.042, 0.005
batch losses (mrrl, rdl): 0.0005700893, 3.45036e-05

Epoch over!
epoch time: 15.26

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 155
rank avg (pred): 0.468 +- 0.254
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 7.38e-08, 8.185e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 25
rank avg (pred): 0.151 +- 0.228
mrr vals (pred, true): 0.189, 0.239
batch losses (mrrl, rdl): 0.0252133142, 0.0002809192

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 511
rank avg (pred): 0.416 +- 0.200
mrr vals (pred, true): 0.051, 0.029
batch losses (mrrl, rdl): 8.6102e-06, 0.0001060275

Epoch over!
epoch time: 15.26

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 157
rank avg (pred): 0.490 +- 0.261
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 7.59e-08, 1.5455e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 398
rank avg (pred): 0.468 +- 0.258
mrr vals (pred, true): 0.071, 0.005
batch losses (mrrl, rdl): 0.0043078475, 5.091e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 400
rank avg (pred): 0.467 +- 0.236
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001671771, 1.33539e-05

Epoch over!
epoch time: 15.248

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.129 +- 0.137
mrr vals (pred, true): 0.179, 0.036
batch losses (mrrl, rdl): 0.1658217609, 0.0003993569

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 21
rank avg (pred): 0.062 +- 0.120
mrr vals (pred, true): 0.239, 0.245
batch losses (mrrl, rdl): 0.0002588715, 1.56086e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 369
rank avg (pred): 0.470 +- 0.237
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 2.58329e-05, 1.94415e-05

Epoch over!
epoch time: 15.271

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 983
rank avg (pred): 0.080 +- 0.107
mrr vals (pred, true): 0.191, 0.154
batch losses (mrrl, rdl): 0.0142420689, 2.7165e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.491 +- 0.246
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 1.68123e-05, 1.28145e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 402
rank avg (pred): 0.479 +- 0.229
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.00873e-05, 1.56077e-05

Epoch over!
epoch time: 15.266

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 507
rank avg (pred): 0.432 +- 0.215
mrr vals (pred, true): 0.062, 0.043
batch losses (mrrl, rdl): 0.0013355154, 0.0003922507

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 832
rank avg (pred): 0.076 +- 0.155
mrr vals (pred, true): 0.253, 0.256
batch losses (mrrl, rdl): 7.79161e-05, 7.2105e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 941
rank avg (pred): 0.585 +- 0.261
mrr vals (pred, true): 0.037, 0.002
batch losses (mrrl, rdl): 0.0016119801, 3.681e-07

Epoch over!
epoch time: 15.257

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 623
rank avg (pred): 0.400 +- 0.200
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001636394, 0.0001769627

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 733
rank avg (pred): 0.048 +- 0.085
mrr vals (pred, true): 0.276, 0.260
batch losses (mrrl, rdl): 0.002715108, 9.1725e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1131
rank avg (pred): 0.453 +- 0.200
mrr vals (pred, true): 0.055, 0.005
batch losses (mrrl, rdl): 0.0003006821, 3.26651e-05

Epoch over!
epoch time: 15.282

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 745
rank avg (pred): 0.070 +- 0.118
mrr vals (pred, true): 0.231, 0.242
batch losses (mrrl, rdl): 0.0010722687, 1.51103e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 211
rank avg (pred): 0.486 +- 0.246
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 9.21108e-05, 9.0567e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 592
rank avg (pred): 0.516 +- 0.249
mrr vals (pred, true): 0.042, 0.003
batch losses (mrrl, rdl): 0.0006873195, 1.02237e-05

Epoch over!
epoch time: 15.183

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.494 +- 0.243
mrr vals (pred, true): 0.051, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   88 	     0 	 0.05543 	 0.00087 	 m..s
   86 	     1 	 0.05494 	 0.00088 	 m..s
   89 	     2 	 0.05708 	 0.00240 	 m..s
   57 	     3 	 0.05159 	 0.00242 	 m..s
   12 	     4 	 0.04835 	 0.00259 	 m..s
   92 	     5 	 0.05856 	 0.00261 	 m..s
   20 	     6 	 0.04926 	 0.00292 	 m..s
   83 	     7 	 0.05366 	 0.00300 	 m..s
   27 	     8 	 0.04981 	 0.00301 	 m..s
   14 	     9 	 0.04861 	 0.00313 	 m..s
   36 	    10 	 0.05065 	 0.00329 	 m..s
   85 	    11 	 0.05430 	 0.00333 	 m..s
   79 	    12 	 0.05327 	 0.00334 	 m..s
    9 	    13 	 0.04803 	 0.00336 	 m..s
    0 	    14 	 0.04242 	 0.00337 	 m..s
   29 	    15 	 0.05015 	 0.00340 	 m..s
   60 	    16 	 0.05191 	 0.00341 	 m..s
   76 	    17 	 0.05306 	 0.00343 	 m..s
   47 	    18 	 0.05106 	 0.00347 	 m..s
   62 	    19 	 0.05198 	 0.00347 	 m..s
   50 	    20 	 0.05122 	 0.00349 	 m..s
   19 	    21 	 0.04898 	 0.00353 	 m..s
   51 	    22 	 0.05141 	 0.00354 	 m..s
   81 	    23 	 0.05337 	 0.00357 	 m..s
   11 	    24 	 0.04827 	 0.00362 	 m..s
   35 	    25 	 0.05062 	 0.00364 	 m..s
    3 	    26 	 0.04617 	 0.00366 	 m..s
   42 	    27 	 0.05083 	 0.00367 	 m..s
    6 	    28 	 0.04721 	 0.00367 	 m..s
   67 	    29 	 0.05246 	 0.00368 	 m..s
   77 	    30 	 0.05313 	 0.00371 	 m..s
   33 	    31 	 0.05037 	 0.00375 	 m..s
   18 	    32 	 0.04897 	 0.00375 	 m..s
   31 	    33 	 0.05031 	 0.00376 	 m..s
   44 	    34 	 0.05094 	 0.00377 	 m..s
   90 	    35 	 0.05726 	 0.00377 	 m..s
   66 	    36 	 0.05241 	 0.00377 	 m..s
   58 	    37 	 0.05174 	 0.00382 	 m..s
   37 	    38 	 0.05072 	 0.00386 	 m..s
   68 	    39 	 0.05249 	 0.00387 	 m..s
   21 	    40 	 0.04928 	 0.00387 	 m..s
   16 	    41 	 0.04865 	 0.00388 	 m..s
   26 	    42 	 0.04972 	 0.00389 	 m..s
   52 	    43 	 0.05142 	 0.00390 	 m..s
   32 	    44 	 0.05036 	 0.00391 	 m..s
    1 	    45 	 0.04324 	 0.00392 	 m..s
   43 	    46 	 0.05083 	 0.00398 	 m..s
   73 	    47 	 0.05287 	 0.00398 	 m..s
   13 	    48 	 0.04845 	 0.00399 	 m..s
   34 	    49 	 0.05054 	 0.00400 	 m..s
   72 	    50 	 0.05284 	 0.00401 	 m..s
    7 	    51 	 0.04742 	 0.00403 	 m..s
   45 	    52 	 0.05100 	 0.00404 	 m..s
   91 	    53 	 0.05848 	 0.00407 	 m..s
   82 	    54 	 0.05359 	 0.00408 	 m..s
   10 	    55 	 0.04808 	 0.00410 	 m..s
   38 	    56 	 0.05079 	 0.00410 	 m..s
   70 	    57 	 0.05264 	 0.00412 	 m..s
   41 	    58 	 0.05082 	 0.00413 	 m..s
   63 	    59 	 0.05215 	 0.00413 	 m..s
   56 	    60 	 0.05155 	 0.00415 	 m..s
   15 	    61 	 0.04862 	 0.00421 	 m..s
   39 	    62 	 0.05080 	 0.00424 	 m..s
   30 	    63 	 0.05028 	 0.00428 	 m..s
   84 	    64 	 0.05416 	 0.00432 	 m..s
   54 	    65 	 0.05146 	 0.00442 	 m..s
   25 	    66 	 0.04969 	 0.00444 	 m..s
   24 	    67 	 0.04969 	 0.00448 	 m..s
   28 	    68 	 0.05004 	 0.00448 	 m..s
   64 	    69 	 0.05218 	 0.00450 	 m..s
   40 	    70 	 0.05082 	 0.00453 	 m..s
   59 	    71 	 0.05180 	 0.00455 	 m..s
   69 	    72 	 0.05250 	 0.00458 	 m..s
   74 	    73 	 0.05289 	 0.00461 	 m..s
   78 	    74 	 0.05320 	 0.00461 	 m..s
   65 	    75 	 0.05229 	 0.00466 	 m..s
   22 	    76 	 0.04949 	 0.00473 	 m..s
   87 	    77 	 0.05520 	 0.00473 	 m..s
   23 	    78 	 0.04964 	 0.00489 	 m..s
   48 	    79 	 0.05111 	 0.00505 	 m..s
   80 	    80 	 0.05333 	 0.00514 	 m..s
   61 	    81 	 0.05193 	 0.00520 	 m..s
   71 	    82 	 0.05270 	 0.00540 	 m..s
   17 	    83 	 0.04896 	 0.00548 	 m..s
   46 	    84 	 0.05102 	 0.00571 	 m..s
   53 	    85 	 0.05146 	 0.00577 	 m..s
   49 	    86 	 0.05119 	 0.00587 	 m..s
   55 	    87 	 0.05147 	 0.00597 	 m..s
   75 	    88 	 0.05300 	 0.00661 	 m..s
    5 	    89 	 0.04667 	 0.01061 	 m..s
    2 	    90 	 0.04565 	 0.01152 	 m..s
    8 	    91 	 0.04748 	 0.01237 	 m..s
    4 	    92 	 0.04620 	 0.01402 	 m..s
   96 	    93 	 0.14752 	 0.03535 	 MISS
  100 	    94 	 0.16402 	 0.12326 	 m..s
   95 	    95 	 0.14660 	 0.13162 	 ~...
   93 	    96 	 0.14041 	 0.13339 	 ~...
   94 	    97 	 0.14577 	 0.13525 	 ~...
  101 	    98 	 0.16444 	 0.13534 	 ~...
   99 	    99 	 0.15305 	 0.13628 	 ~...
   97 	   100 	 0.14798 	 0.13713 	 ~...
   98 	   101 	 0.14946 	 0.14041 	 ~...
  104 	   102 	 0.18672 	 0.17307 	 ~...
  102 	   103 	 0.17407 	 0.17792 	 ~...
  108 	   104 	 0.20390 	 0.18551 	 ~...
  103 	   105 	 0.18014 	 0.18632 	 ~...
  107 	   106 	 0.19334 	 0.19918 	 ~...
  105 	   107 	 0.19050 	 0.20344 	 ~...
  106 	   108 	 0.19124 	 0.20543 	 ~...
  115 	   109 	 0.26901 	 0.21151 	 m..s
  112 	   110 	 0.26182 	 0.21604 	 m..s
  110 	   111 	 0.22944 	 0.22799 	 ~...
  109 	   112 	 0.22079 	 0.24463 	 ~...
  111 	   113 	 0.24264 	 0.25269 	 ~...
  114 	   114 	 0.26475 	 0.26660 	 ~...
  113 	   115 	 0.26289 	 0.26809 	 ~...
  120 	   116 	 0.34690 	 0.28931 	 m..s
  119 	   117 	 0.32120 	 0.29629 	 ~...
  117 	   118 	 0.31466 	 0.29968 	 ~...
  118 	   119 	 0.31570 	 0.29973 	 ~...
  116 	   120 	 0.30281 	 0.32320 	 ~...
==========================================
r_mrr = 0.9855203628540039
r2_mrr = 0.7597500085830688
spearmanr_mrr@5 = 0.9902241826057434
spearmanr_mrr@10 = 0.9760814309120178
spearmanr_mrr@50 = 0.9889068603515625
spearmanr_mrr@100 = 0.9926443099975586
spearmanr_mrr@All = 0.9930030107498169
==========================================
test time: 0.455
Done Testing dataset CoDExSmall
total time taken: 238.3647015094757
training time taken: 227.855464220047
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9855)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7598)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9902)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9761)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9889)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9926)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9930)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.26382032485025775}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 4507206163146030
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [154, 1135, 347, 603, 1085, 1070, 295, 322, 944, 70, 913, 1035, 261, 956, 905, 1061, 58, 1186, 147, 827, 834, 179, 235, 458, 724, 1138, 768, 5, 38, 41, 1031, 208, 921, 90, 614, 506, 1117, 927, 938, 126, 205, 805, 312, 568, 107, 611, 939, 1154, 895, 1043, 1174, 1143, 527, 277, 909, 888, 36, 176, 986, 908, 541, 863, 646, 222, 64, 296, 139, 902, 947, 280, 532, 875, 173, 548, 362, 401, 729, 994, 1180, 448, 82, 612, 537, 122, 831, 764, 1083, 457, 288, 1013, 371, 644, 454, 728, 639, 213, 406, 412, 186, 622, 198, 211, 273, 57, 1003, 376, 855, 730, 326, 110, 967, 1077, 364, 459, 15, 502, 742, 890, 44, 1093, 543]
valid_ids (0): []
train_ids (1094): [142, 521, 1209, 499, 311, 279, 1184, 387, 814, 571, 1152, 703, 830, 467, 1165, 1214, 677, 252, 1158, 1131, 940, 105, 1028, 716, 1212, 982, 175, 518, 904, 577, 554, 14, 931, 225, 1058, 970, 9, 508, 236, 101, 451, 823, 610, 534, 748, 444, 717, 722, 32, 625, 52, 941, 873, 293, 820, 950, 1029, 1102, 455, 690, 1032, 1192, 1056, 328, 196, 664, 308, 1100, 493, 148, 576, 617, 75, 708, 230, 1161, 1006, 1210, 172, 869, 1170, 860, 245, 1207, 170, 1183, 813, 1189, 272, 128, 601, 849, 751, 232, 977, 866, 427, 1022, 268, 410, 247, 264, 450, 976, 885, 765, 251, 200, 501, 899, 123, 1160, 609, 659, 250, 95, 758, 795, 192, 1046, 290, 197, 828, 204, 332, 1109, 1016, 178, 484, 528, 442, 590, 1082, 903, 1203, 190, 431, 341, 731, 707, 565, 1208, 689, 642, 74, 524, 536, 960, 1, 18, 556, 12, 157, 253, 242, 1057, 384, 784, 98, 763, 535, 1126, 740, 397, 954, 645, 803, 483, 519, 476, 637, 1125, 1071, 934, 386, 833, 604, 846, 928, 632, 462, 901, 1063, 530, 94, 523, 88, 415, 775, 700, 973, 145, 948, 854, 265, 1167, 259, 127, 394, 143, 929, 21, 1145, 370, 393, 714, 550, 1147, 692, 355, 24, 298, 369, 923, 1047, 980, 1037, 491, 656, 1111, 1087, 61, 563, 267, 301, 806, 788, 162, 132, 774, 494, 755, 158, 999, 136, 206, 497, 1086, 1188, 801, 270, 479, 434, 782, 1053, 626, 783, 516, 238, 353, 1033, 171, 607, 338, 1042, 413, 500, 26, 68, 691, 77, 574, 544, 1140, 972, 195, 343, 822, 379, 920, 702, 630, 678, 1079, 942, 667, 358, 414, 1023, 636, 137, 80, 35, 354, 1201, 1144, 465, 850, 1090, 76, 837, 19, 1012, 449, 683, 351, 817, 649, 1052, 786, 693, 861, 1055, 865, 615, 1009, 69, 121, 481, 660, 134, 891, 1097, 769, 108, 505, 349, 894, 666, 557, 29, 1114, 221, 741, 963, 681, 496, 997, 825, 50, 53, 686, 529, 749, 1198, 174, 435, 234, 229, 7, 447, 581, 1081, 373, 320, 1091, 1162, 463, 658, 1094, 314, 1044, 1089, 971, 968, 1115, 503, 260, 654, 486, 840, 405, 1092, 1171, 974, 853, 628, 721, 1020, 780, 572, 309, 949, 859, 487, 1132, 672, 969, 843, 1200, 372, 150, 1168, 633, 1026, 348, 794, 73, 851, 857, 93, 1015, 662, 48, 597, 228, 911, 470, 687, 793, 163, 804, 83, 791, 1064, 709, 1206, 657, 648, 11, 1112, 1182, 433, 640, 1080, 701, 958, 1129, 278, 325, 870, 591, 684, 821, 383, 37, 1088, 34, 634, 316, 89, 498, 233, 871, 1039, 785, 1098, 424, 318, 220, 471, 81, 452, 824, 66, 907, 138, 995, 1050, 771, 111, 1001, 618, 647, 305, 428, 743, 1191, 365, 43, 395, 513, 438, 984, 161, 224, 345, 336, 898, 797, 990, 20, 104, 867, 943, 97, 883, 416, 564, 1038, 274, 167, 669, 916, 461, 623, 886, 436, 62, 256, 992, 605, 1155, 218, 239, 738, 51, 808, 202, 1149, 340, 1169, 85, 443, 131, 1025, 1068, 408, 652, 212, 1177, 1196, 331, 868, 919, 720, 248, 893, 72, 514, 177, 1076, 464, 802, 578, 661, 1008, 439, 790, 249, 217, 324, 596, 141, 575, 877, 1119, 246, 352, 400, 533, 297, 469, 812, 600, 113, 112, 522, 342, 650, 796, 411, 133, 219, 747, 130, 836, 776, 896, 333, 1051, 159, 166, 1130, 165, 711, 25, 1096, 598, 619, 1122, 746, 673, 965, 359, 446, 432, 892, 712, 935, 92, 1095, 350, 900, 629, 975, 194, 725, 392, 933, 989, 588, 109, 207, 185, 106, 385, 1069, 339, 398, 153, 862, 787, 382, 237, 835, 86, 540, 55, 1193, 1156, 573, 924, 203, 887, 697, 932, 732, 983, 872, 118, 284, 509, 847, 183, 13, 715, 1141, 695, 191, 389, 189, 560, 63, 811, 1116, 705, 42, 1190, 71, 149, 344, 979, 1150, 396, 30, 582, 889, 651, 511, 897, 1127, 1118, 429, 704, 445, 276, 1073, 674, 1005, 54, 262, 1011, 579, 809, 275, 475, 215, 156, 0, 1204, 880, 1041, 832, 841, 1197, 180, 1139, 1178, 488, 1120, 569, 146, 1175, 103, 752, 282, 754, 378, 45, 546, 271, 733, 1136, 1078, 594, 285, 1065, 987, 226, 879, 363, 419, 1040, 1105, 961, 243, 937, 299, 584, 231, 266, 884, 621, 981, 829, 1142, 766, 675, 99, 140, 417, 335, 144, 482, 466, 922, 957, 1000, 781, 792, 1213, 329, 1036, 1187, 561, 799, 478, 33, 539, 214, 407, 485, 551, 1004, 441, 47, 549, 10, 966, 426, 915, 882, 1179, 100, 366, 679, 1014, 437, 28, 374, 303, 1007, 515, 504, 761, 670, 507, 120, 744, 1159, 404, 468, 583, 84, 269, 114, 91, 193, 490, 79, 294, 472, 188, 606, 1067, 635, 49, 735, 914, 777, 753, 129, 17, 845, 826, 368, 698, 78, 1019, 418, 1017, 1099, 718, 608, 918, 1134, 525, 27, 223, 480, 263, 912, 489, 283, 1173, 473, 858, 337, 425, 542, 545, 517, 1137, 570, 964, 770, 737, 388, 291, 538, 696, 1172, 377, 688, 955, 685, 710, 682, 1164, 599, 1034, 254, 135, 653, 800, 310, 750, 559, 59, 878, 1048, 124, 1066, 1059, 510, 1124, 655, 164, 119, 380, 403, 399, 643, 65, 255, 391, 216, 1195, 1010, 1133, 474, 638, 988, 1166, 313, 456, 56, 745, 567, 1146, 586, 665, 819, 381, 874, 713, 760, 258, 1027, 789, 210, 402, 60, 1148, 421, 580, 360, 613, 547, 512, 936, 181, 996, 1060, 998, 1074, 945, 520, 1202, 187, 668, 592, 917, 624, 22, 553, 962, 818, 566, 453, 1045, 993, 757, 589, 807, 767, 4, 1084, 327, 201, 1104, 1002, 46, 8, 209, 727, 739, 1103, 117, 759, 810, 838, 492, 422, 773, 779, 881, 593, 816, 1018, 306, 680, 953, 951, 723, 676, 959, 925, 125, 169, 281, 1072, 23, 842, 361, 1176, 257, 699, 160, 1121, 420, 319, 526, 330, 844, 155, 357, 115, 641, 1101, 315, 555, 726, 423, 627, 16, 985, 1185, 244, 856, 1199, 1107, 663, 1049, 1153, 798, 102, 1108, 1021, 839, 1123, 1163, 671, 227, 587, 1151, 562, 734, 991, 116, 286, 168, 39, 460, 40, 151, 495, 1211, 430, 1054, 719, 558, 815, 375, 864, 910, 926, 952, 302, 184, 87, 1062, 323, 1113, 346, 1194, 1075, 304, 287, 96, 321, 300, 1205, 356, 762, 585, 1157, 620, 240, 317, 1181, 876, 706, 199, 31, 631, 334, 552, 756, 307, 367, 440, 848, 1128, 2, 182, 3, 1106, 930, 602, 1030, 736, 1024, 152, 772, 595, 616, 390, 852, 6, 978, 531, 409, 292, 241, 946, 1110, 906, 289, 477, 67, 694, 778]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2416535086208089
the save name prefix for this run is:  chkpt-ID_2416535086208089_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 397
rank avg (pred): 0.489 +- 0.003
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001039599

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.484 +- 0.006
mrr vals (pred, true): 0.001, 0.006
batch losses (mrrl, rdl): 0.0, 9.81808e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 243
rank avg (pred): 0.048 +- 0.001
mrr vals (pred, true): 0.010, 0.272
batch losses (mrrl, rdl): 0.0, 2.582e-06

Epoch over!
epoch time: 14.894

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 943
rank avg (pred): 0.493 +- 0.003
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 0.0007256399

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 396
rank avg (pred): 0.485 +- 0.003
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001044698

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1084
rank avg (pred): 0.467 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001001768

Epoch over!
epoch time: 14.876

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 883
rank avg (pred): 0.471 +- 0.001
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.2489e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.496 +- 0.001
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 8.62755e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 427
rank avg (pred): 0.492 +- 0.000
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.000103719

Epoch over!
epoch time: 14.869

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 331
rank avg (pred): 0.466 +- 0.000
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 9.53488e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.495 +- 0.000
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 0.000699805

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 135
rank avg (pred): 0.477 +- 0.000
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 8.90508e-05

Epoch over!
epoch time: 14.88

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 997
rank avg (pred): 0.056 +- 0.000
mrr vals (pred, true): 0.009, 0.314
batch losses (mrrl, rdl): 0.0, 6.0989e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 415
rank avg (pred): 0.482 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.79078e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 137
rank avg (pred): 0.475 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.02417e-05

Epoch over!
epoch time: 14.867

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 948
rank avg (pred): 0.521 +- 0.000
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0240651742, 0.0001273207

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 606
rank avg (pred): 0.418 +- 0.000
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0238371231, 0.0001823634

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 275
rank avg (pred): 0.004 +- 0.000
mrr vals (pred, true): 0.115, 0.196
batch losses (mrrl, rdl): 0.0646885037, 0.0001321474

Epoch over!
epoch time: 15.065

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.396 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0237741489, 0.000310349

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 45
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.159, 0.135
batch losses (mrrl, rdl): 0.0054289377, 0.0001871531

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.136, 0.210
batch losses (mrrl, rdl): 0.055250112, 6.37447e-05

Epoch over!
epoch time: 15.032

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 711
rank avg (pred): 0.430 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0238709319, 0.0001353351

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 536
rank avg (pred): 0.114 +- 0.000
mrr vals (pred, true): 0.004, 0.007
batch losses (mrrl, rdl): 0.0208792835, 0.0025686962

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 736
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.162, 0.064
batch losses (mrrl, rdl): 0.1265548468, 0.0015202585

Epoch over!
epoch time: 15.218

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 50
rank avg (pred): 0.002 +- 0.000
mrr vals (pred, true): 0.174, 0.131
batch losses (mrrl, rdl): 0.018193135, 0.0001648103

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 762
rank avg (pred): 0.255 +- 0.000
mrr vals (pred, true): 0.002, 0.003
batch losses (mrrl, rdl): 0.0231114626, 0.0018798576

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 712
rank avg (pred): 0.443 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0239023846, 0.000124232

Epoch over!
epoch time: 15.051

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 707
rank avg (pred): 0.448 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0239160024, 0.0001441966

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 24
rank avg (pred): 0.002 +- 0.000
mrr vals (pred, true): 0.179, 0.242
batch losses (mrrl, rdl): 0.0390921757, 3.60925e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 725
rank avg (pred): 0.420 +- 0.000
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0238435566, 0.0001507116

Epoch over!
epoch time: 15.05

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1058
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.291, 0.288
batch losses (mrrl, rdl): 8.62415e-05, 4.56219e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.423 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0238507967, 0.0001572358

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 857
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.084, 0.004
batch losses (mrrl, rdl): 0.0112871462, 0.0047358358

Epoch over!
epoch time: 15.062

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 174
rank avg (pred): 0.048 +- 0.000
mrr vals (pred, true): 0.010, 0.004
batch losses (mrrl, rdl): 0.0158446319, 0.0036224551

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 30
rank avg (pred): 0.004 +- 0.000
mrr vals (pred, true): 0.119, 0.114
batch losses (mrrl, rdl): 0.0002978014, 0.0001796695

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 748
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.154, 0.186
batch losses (mrrl, rdl): 0.0101380153, 0.0001028682

Epoch over!
epoch time: 15.074

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 816
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.159, 0.035
batch losses (mrrl, rdl): 0.1179383099, 0.0015221746

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 819
rank avg (pred): 0.002 +- 0.000
mrr vals (pred, true): 0.206, 0.243
batch losses (mrrl, rdl): 0.013423305, 0.0001304084

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 656
rank avg (pred): 0.442 +- 0.000
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0239018872, 0.0001472252

Epoch over!
epoch time: 15.066

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 423
rank avg (pred): 0.008 +- 0.000
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0007583096, 0.0044973455

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 410
rank avg (pred): 0.011 +- 0.000
mrr vals (pred, true): 0.044, 0.005
batch losses (mrrl, rdl): 0.0004064347, 0.0043398733

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 389
rank avg (pred): 0.009 +- 0.000
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 6.1053e-06, 0.0042212

Epoch over!
epoch time: 15.08

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 319
rank avg (pred): 0.002 +- 0.000
mrr vals (pred, true): 0.196, 0.171
batch losses (mrrl, rdl): 0.0061079399, 0.000108769

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 249
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.271, 0.267
batch losses (mrrl, rdl): 0.0002257905, 3.04087e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.067, 0.004
batch losses (mrrl, rdl): 0.0030027991, 0.0043394337

Epoch over!
epoch time: 15.076

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.011 +- 0.000
mrr vals (pred, true): 0.044, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   42 	     0 	 0.04272 	 0.00087 	 m..s
   56 	     1 	 0.04739 	 0.00088 	 m..s
   31 	     2 	 0.03755 	 0.00089 	 m..s
   47 	     3 	 0.04404 	 0.00122 	 m..s
    0 	     4 	 0.00087 	 0.00124 	 ~...
   29 	     5 	 0.03712 	 0.00151 	 m..s
   27 	     6 	 0.03314 	 0.00155 	 m..s
    2 	     7 	 0.00091 	 0.00170 	 ~...
   18 	     8 	 0.00108 	 0.00192 	 ~...
    5 	     9 	 0.00093 	 0.00241 	 ~...
    3 	    10 	 0.00092 	 0.00242 	 ~...
   32 	    11 	 0.03880 	 0.00244 	 m..s
   84 	    12 	 0.06122 	 0.00250 	 m..s
    7 	    13 	 0.00096 	 0.00275 	 ~...
    6 	    14 	 0.00096 	 0.00280 	 ~...
   21 	    15 	 0.00109 	 0.00282 	 ~...
   77 	    16 	 0.05708 	 0.00288 	 m..s
   13 	    17 	 0.00102 	 0.00295 	 ~...
   10 	    18 	 0.00099 	 0.00312 	 ~...
   48 	    19 	 0.04426 	 0.00315 	 m..s
   68 	    20 	 0.05154 	 0.00315 	 m..s
   18 	    21 	 0.00108 	 0.00324 	 ~...
   52 	    22 	 0.04472 	 0.00327 	 m..s
   85 	    23 	 0.06200 	 0.00330 	 m..s
   20 	    24 	 0.00108 	 0.00334 	 ~...
    8 	    25 	 0.00096 	 0.00335 	 ~...
   65 	    26 	 0.05121 	 0.00339 	 m..s
   16 	    27 	 0.00104 	 0.00345 	 ~...
   17 	    28 	 0.00105 	 0.00345 	 ~...
   61 	    29 	 0.04923 	 0.00346 	 m..s
   15 	    30 	 0.00104 	 0.00350 	 ~...
   90 	    31 	 0.06917 	 0.00368 	 m..s
   39 	    32 	 0.04154 	 0.00384 	 m..s
   59 	    33 	 0.04901 	 0.00391 	 m..s
   71 	    34 	 0.05229 	 0.00391 	 m..s
   11 	    35 	 0.00099 	 0.00394 	 ~...
   51 	    36 	 0.04462 	 0.00396 	 m..s
   82 	    37 	 0.05981 	 0.00398 	 m..s
   43 	    38 	 0.04273 	 0.00399 	 m..s
   86 	    39 	 0.06259 	 0.00403 	 m..s
   58 	    40 	 0.04889 	 0.00404 	 m..s
   83 	    41 	 0.06004 	 0.00408 	 m..s
   35 	    42 	 0.04053 	 0.00410 	 m..s
   78 	    43 	 0.05799 	 0.00414 	 m..s
   46 	    44 	 0.04399 	 0.00415 	 m..s
   57 	    45 	 0.04752 	 0.00415 	 m..s
   81 	    46 	 0.05943 	 0.00417 	 m..s
   12 	    47 	 0.00100 	 0.00421 	 ~...
   55 	    48 	 0.04623 	 0.00424 	 m..s
    9 	    49 	 0.00098 	 0.00429 	 ~...
   66 	    50 	 0.05124 	 0.00439 	 m..s
   72 	    51 	 0.05267 	 0.00441 	 m..s
   89 	    52 	 0.06382 	 0.00442 	 m..s
   76 	    53 	 0.05582 	 0.00444 	 m..s
   36 	    54 	 0.04097 	 0.00448 	 m..s
   44 	    55 	 0.04279 	 0.00451 	 m..s
   60 	    56 	 0.04910 	 0.00453 	 m..s
   53 	    57 	 0.04503 	 0.00454 	 m..s
   62 	    58 	 0.04951 	 0.00461 	 m..s
   50 	    59 	 0.04459 	 0.00463 	 m..s
   64 	    60 	 0.04978 	 0.00466 	 m..s
   79 	    61 	 0.05824 	 0.00475 	 m..s
   38 	    62 	 0.04128 	 0.00476 	 m..s
   54 	    63 	 0.04513 	 0.00477 	 m..s
   73 	    64 	 0.05393 	 0.00479 	 m..s
   49 	    65 	 0.04455 	 0.00480 	 m..s
   14 	    66 	 0.00103 	 0.00482 	 ~...
   70 	    67 	 0.05197 	 0.00492 	 m..s
   34 	    68 	 0.03923 	 0.00492 	 m..s
    1 	    69 	 0.00091 	 0.00492 	 ~...
   67 	    70 	 0.05125 	 0.00494 	 m..s
   41 	    71 	 0.04264 	 0.00495 	 m..s
   37 	    72 	 0.04123 	 0.00505 	 m..s
   80 	    73 	 0.05856 	 0.00508 	 m..s
   40 	    74 	 0.04232 	 0.00528 	 m..s
   88 	    75 	 0.06369 	 0.00540 	 m..s
   45 	    76 	 0.04283 	 0.00573 	 m..s
    4 	    77 	 0.00092 	 0.00577 	 ~...
   87 	    78 	 0.06270 	 0.00597 	 m..s
   33 	    79 	 0.03895 	 0.00694 	 m..s
   30 	    80 	 0.03725 	 0.00764 	 ~...
   23 	    81 	 0.02293 	 0.00787 	 ~...
   22 	    82 	 0.02009 	 0.01061 	 ~...
   24 	    83 	 0.02680 	 0.01189 	 ~...
   25 	    84 	 0.02731 	 0.01402 	 ~...
   75 	    85 	 0.05570 	 0.01676 	 m..s
   28 	    86 	 0.03606 	 0.01940 	 ~...
   74 	    87 	 0.05424 	 0.02050 	 m..s
   63 	    88 	 0.04963 	 0.02430 	 ~...
   26 	    89 	 0.02831 	 0.02742 	 ~...
   69 	    90 	 0.05158 	 0.02942 	 ~...
   93 	    91 	 0.13621 	 0.11435 	 ~...
   95 	    92 	 0.14112 	 0.12460 	 ~...
   91 	    93 	 0.11640 	 0.12873 	 ~...
   94 	    94 	 0.14009 	 0.13654 	 ~...
   98 	    95 	 0.15305 	 0.13806 	 ~...
   96 	    96 	 0.14395 	 0.14186 	 ~...
   92 	    97 	 0.12402 	 0.14215 	 ~...
   97 	    98 	 0.14531 	 0.14295 	 ~...
  101 	    99 	 0.18664 	 0.17085 	 ~...
  111 	   100 	 0.22653 	 0.17409 	 m..s
  110 	   101 	 0.21735 	 0.17434 	 m..s
  103 	   102 	 0.19282 	 0.17740 	 ~...
   99 	   103 	 0.17607 	 0.17766 	 ~...
  100 	   104 	 0.18623 	 0.17880 	 ~...
  102 	   105 	 0.18800 	 0.18432 	 ~...
  112 	   106 	 0.22849 	 0.18632 	 m..s
  105 	   107 	 0.20026 	 0.20171 	 ~...
  113 	   108 	 0.25785 	 0.21732 	 m..s
  106 	   109 	 0.20296 	 0.23383 	 m..s
  115 	   110 	 0.30584 	 0.25004 	 m..s
  108 	   111 	 0.20960 	 0.25176 	 m..s
  107 	   112 	 0.20331 	 0.25693 	 m..s
  104 	   113 	 0.19357 	 0.25930 	 m..s
  109 	   114 	 0.21338 	 0.26513 	 m..s
  117 	   115 	 0.32454 	 0.28215 	 m..s
  120 	   116 	 0.35414 	 0.29103 	 m..s
  118 	   117 	 0.32937 	 0.29610 	 m..s
  116 	   118 	 0.31883 	 0.29629 	 ~...
  114 	   119 	 0.28105 	 0.31910 	 m..s
  119 	   120 	 0.35412 	 0.32046 	 m..s
==========================================
r_mrr = 0.9567515850067139
r2_mrr = 0.8432610034942627
spearmanr_mrr@5 = 0.9935241341590881
spearmanr_mrr@10 = 0.9661464691162109
spearmanr_mrr@50 = 0.9819449186325073
spearmanr_mrr@100 = 0.9865978956222534
spearmanr_mrr@All = 0.9743769764900208
==========================================
test time: 0.458
Done Testing dataset CoDExSmall
total time taken: 236.4403681755066
training time taken: 225.62821221351624
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9568)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8433)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9935)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9661)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9819)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9866)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9744)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 1.2652083414723165}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 677895851252848
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [43, 1152, 474, 359, 658, 307, 587, 225, 178, 562, 1057, 774, 396, 715, 962, 502, 667, 1112, 1026, 1140, 879, 598, 959, 1018, 1144, 723, 166, 700, 883, 790, 676, 516, 85, 650, 939, 654, 175, 1039, 152, 901, 342, 1184, 12, 985, 770, 425, 57, 660, 842, 246, 923, 24, 54, 1071, 311, 276, 483, 729, 711, 366, 1133, 362, 335, 818, 347, 389, 809, 1110, 45, 837, 436, 1185, 941, 46, 343, 1101, 791, 482, 557, 549, 242, 255, 1085, 627, 219, 196, 87, 238, 61, 890, 74, 1012, 107, 885, 563, 360, 111, 397, 164, 142, 968, 892, 18, 174, 361, 162, 1132, 303, 1073, 780, 489, 220, 50, 843, 689, 236, 1115, 1202, 100, 656, 77]
valid_ids (0): []
train_ids (1094): [1056, 115, 405, 1176, 1168, 331, 772, 1000, 559, 432, 600, 556, 896, 752, 351, 1034, 792, 836, 535, 773, 767, 679, 775, 928, 7, 468, 1207, 1007, 499, 873, 903, 838, 847, 364, 233, 925, 285, 165, 584, 1019, 376, 534, 149, 292, 1035, 1206, 201, 487, 1004, 112, 727, 339, 41, 599, 607, 1155, 810, 0, 995, 446, 1067, 553, 608, 926, 145, 706, 66, 915, 1055, 1008, 63, 764, 65, 900, 691, 504, 289, 566, 506, 1211, 777, 481, 308, 105, 5, 987, 554, 8, 241, 651, 1064, 633, 296, 1143, 1043, 863, 163, 11, 78, 212, 301, 853, 393, 156, 756, 765, 944, 547, 256, 619, 907, 789, 615, 617, 804, 932, 294, 438, 4, 263, 1014, 284, 946, 442, 522, 337, 315, 686, 641, 420, 1183, 702, 128, 746, 418, 1051, 526, 492, 189, 856, 931, 922, 1096, 251, 322, 431, 623, 496, 97, 704, 755, 1038, 1082, 1029, 829, 203, 858, 788, 319, 58, 456, 701, 1186, 986, 864, 213, 579, 929, 832, 462, 473, 271, 973, 494, 695, 918, 758, 391, 747, 34, 824, 927, 28, 528, 1213, 517, 320, 787, 1161, 914, 889, 867, 611, 993, 227, 971, 527, 124, 1188, 582, 371, 1127, 458, 694, 576, 813, 674, 1125, 975, 493, 1083, 464, 561, 440, 413, 693, 514, 216, 610, 763, 120, 352, 349, 757, 109, 146, 782, 326, 317, 1098, 861, 476, 1006, 750, 421, 350, 976, 524, 110, 505, 834, 1003, 934, 471, 234, 899, 1002, 816, 161, 1158, 855, 232, 510, 139, 388, 382, 1146, 327, 583, 961, 806, 395, 625, 930, 872, 515, 414, 123, 1024, 970, 194, 1015, 906, 898, 448, 739, 1058, 148, 935, 1205, 338, 798, 1059, 274, 606, 947, 205, 1154, 681, 445, 978, 325, 759, 741, 353, 247, 191, 55, 845, 1052, 79, 1103, 605, 1089, 678, 297, 620, 616, 707, 1061, 609, 891, 383, 449, 844, 684, 210, 595, 193, 1027, 1032, 984, 558, 137, 722, 749, 118, 687, 675, 400, 1040, 507, 1028, 1163, 416, 511, 1126, 917, 1107, 158, 539, 1134, 119, 720, 379, 828, 603, 1138, 575, 893, 800, 974, 281, 187, 136, 1001, 1075, 272, 1078, 640, 306, 848, 1137, 1159, 182, 444, 384, 952, 439, 910, 530, 169, 321, 1214, 89, 718, 589, 126, 1076, 571, 200, 198, 1106, 724, 1050, 1066, 820, 332, 268, 447, 1088, 730, 104, 646, 812, 370, 1118, 868, 762, 134, 634, 390, 938, 1091, 245, 748, 799, 884, 1025, 257, 121, 1189, 463, 785, 288, 578, 531, 70, 101, 1121, 1196, 1179, 570, 839, 356, 497, 433, 406, 786, 1187, 644, 1141, 869, 409, 614, 86, 555, 967, 99, 500, 30, 1175, 44, 15, 424, 460, 354, 664, 387, 830, 717, 275, 503, 217, 1113, 1095, 969, 39, 1102, 631, 10, 1195, 696, 513, 983, 592, 685, 269, 344, 754, 870, 865, 1072, 223, 1149, 280, 1097, 62, 265, 159, 206, 170, 1120, 1047, 738, 626, 94, 22, 290, 690, 113, 851, 1191, 430, 1100, 532, 14, 710, 160, 154, 51, 1104, 441, 734, 1109, 286, 846, 260, 543, 179, 807, 1169, 733, 1124, 237, 398, 683, 283, 888, 254, 886, 1190, 977, 48, 1164, 1165, 1208, 68, 373, 668, 692, 417, 784, 211, 380, 989, 793, 150, 334, 745, 955, 1045, 1192, 33, 208, 1197, 950, 1081, 732, 466, 184, 850, 336, 453, 138, 147, 712, 966, 565, 407, 803, 355, 1212, 630, 912, 38, 725, 643, 357, 744, 1150, 92, 825, 218, 73, 1209, 188, 699, 484, 731, 1172, 860, 753, 346, 647, 310, 1063, 1181, 1011, 302, 612, 20, 551, 776, 751, 341, 1060, 680, 708, 996, 192, 958, 1044, 88, 659, 1177, 300, 1068, 963, 548, 207, 84, 540, 1135, 21, 141, 480, 450, 766, 714, 655, 64, 1079, 1178, 849, 314, 854, 1031, 429, 811, 895, 378, 728, 957, 330, 852, 239, 377, 226, 47, 1037, 1204, 999, 305, 954, 478, 1142, 1166, 185, 71, 177, 90, 1092, 1129, 519, 721, 385, 454, 833, 117, 560, 1022, 533, 264, 1156, 358, 956, 1116, 40, 601, 1194, 410, 180, 318, 726, 636, 670, 742, 943, 911, 964, 495, 1200, 657, 652, 381, 1053, 716, 965, 1123, 278, 13, 573, 541, 72, 538, 509, 1020, 1148, 399, 878, 369, 697, 781, 988, 243, 577, 1062, 221, 287, 486, 1203, 365, 114, 122, 457, 282, 1048, 933, 508, 1198, 768, 1077, 669, 876, 597, 329, 230, 737, 709, 949, 760, 31, 887, 894, 897, 719, 98, 498, 1160, 167, 1199, 427, 794, 401, 796, 544, 459, 299, 374, 713, 29, 224, 997, 545, 518, 23, 743, 593, 525, 761, 1173, 661, 95, 479, 437, 80, 435, 1167, 1005, 1023, 624, 1139, 248, 1130, 979, 677, 877, 273, 35, 443, 25, 229, 404, 937, 470, 1090, 942, 992, 1108, 1013, 3, 249, 1128, 82, 59, 982, 291, 862, 904, 1171, 157, 621, 130, 1170, 9, 635, 155, 808, 882, 452, 412, 1094, 2, 304, 1099, 270, 536, 408, 1210, 663, 313, 916, 638, 1122, 1010, 67, 96, 596, 841, 821, 568, 673, 920, 469, 204, 1182, 258, 402, 1153, 981, 666, 980, 83, 26, 222, 228, 831, 235, 994, 585, 16, 1136, 244, 665, 151, 945, 171, 426, 736, 316, 1086, 703, 771, 1174, 740, 783, 143, 1147, 586, 279, 653, 214, 1030, 682, 990, 475, 153, 490, 106, 801, 415, 467, 76, 19, 428, 613, 173, 779, 642, 594, 1084, 632, 827, 671, 688, 1021, 1131, 116, 521, 546, 924, 176, 637, 1201, 564, 199, 622, 1070, 880, 345, 902, 960, 1087, 698, 197, 202, 434, 795, 881, 940, 1151, 323, 181, 231, 909, 183, 1065, 132, 375, 1069, 125, 645, 567, 523, 835, 672, 1111, 953, 913, 875, 569, 529, 948, 580, 6, 936, 75, 419, 814, 951, 309, 572, 628, 423, 368, 705, 552, 602, 1162, 908, 991, 1, 394, 1180, 998, 649, 91, 403, 919, 485, 267, 215, 127, 253, 465, 581, 1119, 542, 312, 1033, 386, 455, 1093, 168, 53, 27, 822, 840, 819, 240, 1080, 550, 133, 293, 1016, 392, 1157, 859, 1017, 348, 735, 1049, 871, 1145, 32, 537, 491, 340, 1074, 37, 422, 472, 866, 298, 69, 817, 259, 372, 477, 857, 648, 1041, 769, 56, 1114, 144, 618, 778, 1054, 102, 277, 295, 108, 140, 823, 588, 42, 1193, 1046, 874, 261, 1105, 921, 93, 172, 190, 328, 1009, 972, 52, 250, 131, 905, 129, 802, 17, 135, 629, 662, 461, 451, 60, 324, 209, 266, 639, 103, 195, 252, 1036, 805, 590, 591, 826, 333, 363, 512, 488, 411, 574, 604, 186, 1042, 367, 49, 36, 81, 1117, 520, 501, 815, 262, 797]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9137863676932640
the save name prefix for this run is:  chkpt-ID_9137863676932640_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 990
rank avg (pred): 0.506 +- 0.008
mrr vals (pred, true): 0.001, 0.161
batch losses (mrrl, rdl): 0.0, 0.0040153908

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 237
rank avg (pred): 0.461 +- 0.291
mrr vals (pred, true): 0.106, 0.004
batch losses (mrrl, rdl): 0.0, 8.3469e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 464
rank avg (pred): 0.465 +- 0.278
mrr vals (pred, true): 0.079, 0.004
batch losses (mrrl, rdl): 0.0, 1.30959e-05

Epoch over!
epoch time: 14.942

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.059 +- 0.041
mrr vals (pred, true): 0.145, 0.261
batch losses (mrrl, rdl): 0.0, 4.4489e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.483 +- 0.270
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0, 2.28792e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 254
rank avg (pred): 0.051 +- 0.035
mrr vals (pred, true): 0.135, 0.343
batch losses (mrrl, rdl): 0.0, 8.6938e-06

Epoch over!
epoch time: 14.914

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 663
rank avg (pred): 0.468 +- 0.271
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0, 7.9341e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 526
rank avg (pred): 0.385 +- 0.250
mrr vals (pred, true): 0.073, 0.009
batch losses (mrrl, rdl): 0.0, 2.9656e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.443 +- 0.271
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0, 8.8779e-06

Epoch over!
epoch time: 14.912

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1130
rank avg (pred): 0.442 +- 0.271
mrr vals (pred, true): 0.056, 0.004
batch losses (mrrl, rdl): 0.0, 6.67e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 187
rank avg (pred): 0.496 +- 0.276
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 0.0, 2.62737e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 772
rank avg (pred): 0.509 +- 0.299
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0, 9.7247e-06

Epoch over!
epoch time: 14.923

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 264
rank avg (pred): 0.042 +- 0.026
mrr vals (pred, true): 0.097, 0.283
batch losses (mrrl, rdl): 0.0, 2.3517e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 561
rank avg (pred): 0.399 +- 0.242
mrr vals (pred, true): 0.041, 0.006
batch losses (mrrl, rdl): 0.0, 4.8699e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 101
rank avg (pred): 0.455 +- 0.263
mrr vals (pred, true): 0.033, 0.004
batch losses (mrrl, rdl): 0.0, 3.1377e-06

Epoch over!
epoch time: 14.912

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.025 +- 0.018
mrr vals (pred, true): 0.157, 0.251
batch losses (mrrl, rdl): 0.0880532563, 2.33188e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 277
rank avg (pred): 0.032 +- 0.022
mrr vals (pred, true): 0.203, 0.177
batch losses (mrrl, rdl): 0.006398086, 5.26716e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 95
rank avg (pred): 0.449 +- 0.149
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0005337141, 5.76807e-05

Epoch over!
epoch time: 15.099

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 182
rank avg (pred): 0.449 +- 0.168
mrr vals (pred, true): 0.064, 0.004
batch losses (mrrl, rdl): 0.0018666631, 5.69892e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1125
rank avg (pred): 0.491 +- 0.129
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 9.467e-06, 7.83389e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 501
rank avg (pred): 0.448 +- 0.131
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 1.08007e-05, 0.000540706

Epoch over!
epoch time: 15.096

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 595
rank avg (pred): 0.445 +- 0.112
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002562621, 9.13513e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 293
rank avg (pred): 0.138 +- 0.086
mrr vals (pred, true): 0.186, 0.180
batch losses (mrrl, rdl): 0.0004451074, 9.89774e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 371
rank avg (pred): 0.454 +- 0.137
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001550172, 5.99387e-05

Epoch over!
epoch time: 15.051

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1037
rank avg (pred): 0.479 +- 0.129
mrr vals (pred, true): 0.051, 0.005
batch losses (mrrl, rdl): 8.7507e-06, 6.77837e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 109
rank avg (pred): 0.479 +- 0.126
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.0565e-06, 6.39863e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 472
rank avg (pred): 0.472 +- 0.128
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 3.35746e-05, 6.09492e-05

Epoch over!
epoch time: 15.088

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 248
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.305, 0.300
batch losses (mrrl, rdl): 0.0003287564, 2.49087e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 129
rank avg (pred): 0.492 +- 0.137
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 9.4272e-06, 7.83863e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 814
rank avg (pred): 0.093 +- 0.068
mrr vals (pred, true): 0.254, 0.305
batch losses (mrrl, rdl): 0.0258962512, 9.28917e-05

Epoch over!
epoch time: 15.084

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.489 +- 0.139
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 3.53396e-05, 7.16619e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.098 +- 0.068
mrr vals (pred, true): 0.223, 0.266
batch losses (mrrl, rdl): 0.0182451513, 3.45419e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 167
rank avg (pred): 0.476 +- 0.138
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 6.0402e-06, 5.96268e-05

Epoch over!
epoch time: 15.103

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 248
rank avg (pred): 0.013 +- 0.010
mrr vals (pred, true): 0.303, 0.300
batch losses (mrrl, rdl): 0.0001299522, 2.02196e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 207
rank avg (pred): 0.503 +- 0.186
mrr vals (pred, true): 0.053, 0.005
batch losses (mrrl, rdl): 6.41071e-05, 4.86292e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 350
rank avg (pred): 0.483 +- 0.148
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 2.26837e-05, 7.63491e-05

Epoch over!
epoch time: 15.112

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 51
rank avg (pred): 0.106 +- 0.065
mrr vals (pred, true): 0.130, 0.128
batch losses (mrrl, rdl): 2.30024e-05, 1.50397e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 692
rank avg (pred): 0.483 +- 0.308
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 3.59231e-05, 6.9745e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.474 +- 0.190
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 5.24879e-05, 2.95557e-05

Epoch over!
epoch time: 15.109

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 630
rank avg (pred): 0.422 +- 0.253
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 3.1905e-06, 4.35521e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 75
rank avg (pred): 0.121 +- 0.090
mrr vals (pred, true): 0.155, 0.138
batch losses (mrrl, rdl): 0.0031706984, 3.03307e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 310
rank avg (pred): 0.130 +- 0.082
mrr vals (pred, true): 0.172, 0.188
batch losses (mrrl, rdl): 0.0026705849, 6.80823e-05

Epoch over!
epoch time: 15.111

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1207
rank avg (pred): 0.492 +- 0.226
mrr vals (pred, true): 0.053, 0.007
batch losses (mrrl, rdl): 8.74172e-05, 1.46565e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 789
rank avg (pred): 0.567 +- 0.288
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 5.71884e-05, 9.5814e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1043
rank avg (pred): 0.492 +- 0.252
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 9.2762e-06, 4.9613e-06

Epoch over!
epoch time: 15.087

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.094 +- 0.070
mrr vals (pred, true): 0.120, 0.140

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   37 	     0 	 0.04726 	 0.00121 	 m..s
   22 	     1 	 0.04522 	 0.00140 	 m..s
   31 	     2 	 0.04652 	 0.00170 	 m..s
   54 	     3 	 0.04867 	 0.00194 	 m..s
   83 	     4 	 0.05065 	 0.00225 	 m..s
   84 	     5 	 0.05101 	 0.00226 	 m..s
   23 	     6 	 0.04526 	 0.00241 	 m..s
   19 	     7 	 0.04478 	 0.00296 	 m..s
   64 	     8 	 0.04920 	 0.00312 	 m..s
   72 	     9 	 0.04966 	 0.00316 	 m..s
    0 	    10 	 0.03615 	 0.00317 	 m..s
   81 	    11 	 0.05058 	 0.00322 	 m..s
    2 	    12 	 0.03823 	 0.00323 	 m..s
   20 	    13 	 0.04514 	 0.00330 	 m..s
   89 	    14 	 0.05196 	 0.00331 	 m..s
   15 	    15 	 0.04365 	 0.00335 	 m..s
   93 	    16 	 0.05239 	 0.00340 	 m..s
   56 	    17 	 0.04884 	 0.00344 	 m..s
   60 	    18 	 0.04897 	 0.00347 	 m..s
   13 	    19 	 0.04319 	 0.00349 	 m..s
   74 	    20 	 0.04973 	 0.00350 	 m..s
   95 	    21 	 0.05275 	 0.00351 	 m..s
   35 	    22 	 0.04703 	 0.00359 	 m..s
    3 	    23 	 0.04037 	 0.00360 	 m..s
   30 	    24 	 0.04648 	 0.00362 	 m..s
   58 	    25 	 0.04886 	 0.00368 	 m..s
   17 	    26 	 0.04454 	 0.00372 	 m..s
   80 	    27 	 0.05054 	 0.00372 	 m..s
   16 	    28 	 0.04405 	 0.00373 	 m..s
   18 	    29 	 0.04455 	 0.00375 	 m..s
   79 	    30 	 0.05046 	 0.00376 	 m..s
   85 	    31 	 0.05123 	 0.00380 	 m..s
   53 	    32 	 0.04867 	 0.00386 	 m..s
   43 	    33 	 0.04822 	 0.00388 	 m..s
   49 	    34 	 0.04844 	 0.00388 	 m..s
   45 	    35 	 0.04823 	 0.00388 	 m..s
   52 	    36 	 0.04862 	 0.00389 	 m..s
   70 	    37 	 0.04933 	 0.00395 	 m..s
   59 	    38 	 0.04887 	 0.00396 	 m..s
   42 	    39 	 0.04819 	 0.00399 	 m..s
    5 	    40 	 0.04193 	 0.00400 	 m..s
   63 	    41 	 0.04918 	 0.00400 	 m..s
   44 	    42 	 0.04823 	 0.00403 	 m..s
   14 	    43 	 0.04343 	 0.00403 	 m..s
   46 	    44 	 0.04824 	 0.00410 	 m..s
   47 	    45 	 0.04826 	 0.00410 	 m..s
    6 	    46 	 0.04223 	 0.00410 	 m..s
   12 	    47 	 0.04294 	 0.00410 	 m..s
    1 	    48 	 0.03676 	 0.00413 	 m..s
   82 	    49 	 0.05064 	 0.00414 	 m..s
   10 	    50 	 0.04253 	 0.00416 	 m..s
   48 	    51 	 0.04837 	 0.00420 	 m..s
   94 	    52 	 0.05257 	 0.00424 	 m..s
   11 	    53 	 0.04260 	 0.00428 	 m..s
   26 	    54 	 0.04553 	 0.00428 	 m..s
   57 	    55 	 0.04885 	 0.00429 	 m..s
   41 	    56 	 0.04758 	 0.00431 	 m..s
   88 	    57 	 0.05170 	 0.00431 	 m..s
   21 	    58 	 0.04520 	 0.00432 	 m..s
   39 	    59 	 0.04746 	 0.00432 	 m..s
    8 	    60 	 0.04246 	 0.00433 	 m..s
   61 	    61 	 0.04901 	 0.00437 	 m..s
   75 	    62 	 0.04993 	 0.00441 	 m..s
   69 	    63 	 0.04932 	 0.00450 	 m..s
   73 	    64 	 0.04970 	 0.00451 	 m..s
   77 	    65 	 0.05022 	 0.00452 	 m..s
   67 	    66 	 0.04926 	 0.00453 	 m..s
    4 	    67 	 0.04141 	 0.00455 	 m..s
   62 	    68 	 0.04912 	 0.00455 	 m..s
   38 	    69 	 0.04740 	 0.00456 	 m..s
   55 	    70 	 0.04873 	 0.00459 	 m..s
   29 	    71 	 0.04641 	 0.00460 	 m..s
   66 	    72 	 0.04925 	 0.00461 	 m..s
   32 	    73 	 0.04653 	 0.00464 	 m..s
   65 	    74 	 0.04923 	 0.00465 	 m..s
   27 	    75 	 0.04570 	 0.00468 	 m..s
   91 	    76 	 0.05212 	 0.00471 	 m..s
   24 	    77 	 0.04540 	 0.00479 	 m..s
   28 	    78 	 0.04608 	 0.00493 	 m..s
   33 	    79 	 0.04665 	 0.00494 	 m..s
   40 	    80 	 0.04758 	 0.00496 	 m..s
   36 	    81 	 0.04710 	 0.00510 	 m..s
   50 	    82 	 0.04846 	 0.00514 	 m..s
   68 	    83 	 0.04931 	 0.00516 	 m..s
   51 	    84 	 0.04853 	 0.00525 	 m..s
   76 	    85 	 0.05008 	 0.00529 	 m..s
   25 	    86 	 0.04545 	 0.00584 	 m..s
   87 	    87 	 0.05160 	 0.00602 	 m..s
   34 	    88 	 0.04700 	 0.00656 	 m..s
    7 	    89 	 0.04234 	 0.00775 	 m..s
   78 	    90 	 0.05026 	 0.00815 	 m..s
    9 	    91 	 0.04252 	 0.01055 	 m..s
   92 	    92 	 0.05230 	 0.01930 	 m..s
   90 	    93 	 0.05211 	 0.02095 	 m..s
   86 	    94 	 0.05131 	 0.02742 	 ~...
   96 	    95 	 0.05736 	 0.03521 	 ~...
  106 	    96 	 0.14216 	 0.03563 	 MISS
   71 	    97 	 0.04933 	 0.03573 	 ~...
   97 	    98 	 0.11193 	 0.11357 	 ~...
   98 	    99 	 0.11268 	 0.11435 	 ~...
  104 	   100 	 0.13291 	 0.12715 	 ~...
  103 	   101 	 0.13205 	 0.13055 	 ~...
  105 	   102 	 0.13356 	 0.13094 	 ~...
   99 	   103 	 0.11789 	 0.13339 	 ~...
  101 	   104 	 0.12344 	 0.13534 	 ~...
  100 	   105 	 0.12030 	 0.13961 	 ~...
  102 	   106 	 0.12643 	 0.14054 	 ~...
  108 	   107 	 0.15812 	 0.17594 	 ~...
  107 	   108 	 0.15703 	 0.17596 	 ~...
  109 	   109 	 0.15921 	 0.18796 	 ~...
  115 	   110 	 0.22034 	 0.19541 	 ~...
  116 	   111 	 0.22071 	 0.19940 	 ~...
  110 	   112 	 0.17204 	 0.20344 	 m..s
  111 	   113 	 0.18130 	 0.23759 	 m..s
  112 	   114 	 0.20376 	 0.24180 	 m..s
  114 	   115 	 0.21852 	 0.24463 	 ~...
  117 	   116 	 0.22153 	 0.26233 	 m..s
  113 	   117 	 0.21171 	 0.26513 	 m..s
  119 	   118 	 0.25385 	 0.26879 	 ~...
  120 	   119 	 0.26081 	 0.27685 	 ~...
  118 	   120 	 0.24561 	 0.28375 	 m..s
==========================================
r_mrr = 0.9791743159294128
r2_mrr = 0.7169645428657532
spearmanr_mrr@5 = 0.9336478114128113
spearmanr_mrr@10 = 0.9120264053344727
spearmanr_mrr@50 = 0.9894272089004517
spearmanr_mrr@100 = 0.992340624332428
spearmanr_mrr@All = 0.9922672510147095
==========================================
test time: 0.464
Done Testing dataset CoDExSmall
total time taken: 236.60699772834778
training time taken: 226.01785016059875
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9792)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.7170)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9336)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9120)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9894)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9923)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9923)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.2737114330229815}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 6418307216144532
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [46, 326, 723, 811, 290, 1178, 109, 1100, 1060, 303, 50, 543, 725, 920, 1139, 137, 903, 1165, 1194, 992, 571, 1051, 48, 864, 891, 11, 269, 1007, 884, 1212, 504, 136, 837, 821, 923, 173, 408, 847, 61, 745, 558, 387, 602, 419, 889, 135, 369, 848, 793, 226, 159, 235, 366, 1062, 738, 749, 676, 353, 89, 521, 555, 939, 951, 812, 873, 1031, 1048, 542, 20, 122, 119, 431, 934, 1124, 653, 704, 927, 43, 896, 1199, 118, 563, 700, 1008, 948, 570, 150, 899, 1173, 625, 806, 639, 836, 207, 672, 286, 794, 133, 1113, 442, 1039, 196, 550, 75, 1163, 1032, 935, 1092, 892, 260, 329, 245, 1184, 1013, 351, 199, 6, 1059, 142, 265, 70]
valid_ids (0): []
train_ids (1094): [604, 1014, 804, 667, 987, 330, 529, 217, 396, 844, 409, 979, 1130, 340, 106, 1111, 575, 190, 128, 695, 721, 252, 669, 546, 318, 36, 1118, 824, 673, 562, 335, 921, 461, 1135, 78, 635, 473, 490, 394, 12, 397, 430, 141, 251, 1054, 463, 576, 455, 1112, 172, 516, 194, 1037, 1017, 210, 153, 904, 460, 828, 417, 779, 736, 377, 1190, 495, 789, 716, 1192, 805, 285, 35, 401, 241, 1144, 719, 249, 748, 827, 483, 1157, 262, 19, 801, 950, 1063, 1169, 263, 1105, 395, 858, 389, 544, 915, 445, 759, 780, 317, 724, 434, 16, 1061, 888, 204, 893, 169, 654, 877, 712, 1171, 64, 886, 1129, 810, 341, 358, 642, 706, 1127, 1027, 626, 32, 770, 1053, 280, 671, 545, 548, 485, 55, 375, 115, 90, 803, 972, 1128, 524, 675, 412, 658, 853, 502, 404, 300, 863, 945, 261, 1042, 819, 1045, 1211, 613, 197, 29, 938, 478, 616, 1133, 424, 641, 1016, 800, 458, 222, 1078, 259, 1162, 52, 418, 305, 383, 767, 778, 1121, 1015, 677, 955, 632, 782, 792, 906, 1168, 465, 1172, 435, 567, 438, 525, 559, 292, 699, 80, 357, 208, 1088, 323, 1142, 22, 650, 250, 708, 423, 228, 386, 857, 345, 468, 769, 611, 762, 1012, 557, 580, 1196, 662, 2, 963, 38, 1122, 1099, 855, 339, 157, 711, 72, 714, 739, 909, 175, 707, 615, 509, 1176, 931, 640, 1208, 471, 97, 1115, 574, 826, 1026, 645, 124, 1207, 312, 753, 722, 533, 678, 894, 1025, 825, 110, 551, 954, 385, 538, 220, 1181, 818, 701, 474, 620, 974, 58, 937, 247, 1066, 517, 216, 506, 333, 783, 332, 534, 1152, 777, 462, 60, 907, 776, 941, 831, 564, 693, 988, 997, 617, 809, 475, 713, 221, 510, 202, 1041, 796, 165, 496, 67, 230, 1154, 1095, 3, 952, 644, 99, 930, 1134, 741, 940, 597, 936, 663, 599, 96, 160, 1087, 561, 420, 198, 1109, 470, 117, 751, 4, 244, 407, 1147, 1065, 897, 63, 1188, 347, 156, 93, 39, 528, 768, 308, 1110, 609, 443, 114, 336, 761, 1179, 648, 505, 1164, 270, 912, 182, 598, 1097, 132, 715, 448, 682, 984, 28, 464, 859, 472, 594, 188, 231, 772, 1046, 88, 910, 1123, 568, 815, 275, 742, 7, 1050, 94, 279, 512, 62, 983, 54, 629, 382, 27, 282, 926, 469, 1206, 276, 755, 612, 554, 999, 246, 40, 343, 766, 69, 1033, 588, 1000, 991, 187, 25, 1005, 816, 685, 21, 327, 1209, 1098, 624, 271, 201, 497, 92, 1019, 1150, 606, 319, 998, 577, 289, 466, 807, 905, 838, 1058, 86, 1107, 870, 879, 350, 968, 1156, 586, 1070, 515, 850, 477, 833, 342, 146, 1205, 321, 985, 994, 253, 1117, 619, 1202, 348, 487, 664, 900, 492, 140, 530, 582, 17, 1187, 593, 84, 255, 949, 24, 539, 33, 774, 1177, 1167, 503, 151, 784, 652, 9, 584, 797, 508, 163, 964, 493, 414, 1114, 1201, 234, 371, 1116, 933, 149, 822, 278, 924, 861, 638, 718, 911, 908, 355, 41, 334, 1, 489, 283, 552, 608, 730, 1145, 1086, 381, 922, 928, 610, 876, 914, 10, 301, 898, 986, 447, 764, 102, 237, 970, 656, 744, 883, 291, 364, 467, 95, 829, 398, 882, 1140, 436, 630, 657, 233, 481, 281, 363, 171, 978, 823, 126, 1028, 164, 942, 752, 304, 919, 499, 307, 30, 618, 852, 129, 595, 969, 689, 1074, 378, 456, 1198, 439, 660, 953, 680, 359, 960, 154, 560, 476, 392, 81, 1020, 569, 116, 540, 874, 781, 679, 413, 1094, 865, 162, 756, 184, 798, 621, 147, 665, 243, 268, 433, 236, 393, 692, 399, 388, 296, 479, 651, 814, 684, 1214, 549, 754, 287, 183, 158, 785, 881, 1166, 846, 491, 703, 34, 367, 842, 771, 179, 82, 349, 446, 494, 537, 1149, 1071, 1126, 990, 71, 337, 91, 1079, 362, 138, 1101, 519, 743, 961, 808, 23, 871, 121, 123, 215, 709, 181, 788, 205, 1072, 536, 839, 1119, 1189, 1055, 627, 143, 1160, 573, 298, 728, 916, 178, 227, 646, 1075, 813, 1077, 170, 565, 313, 108, 180, 670, 740, 637, 1106, 134, 45, 450, 603, 391, 1186, 1175, 688, 622, 1040, 191, 498, 895, 66, 1146, 995, 384, 729, 426, 1151, 1009, 541, 229, 177, 757, 860, 176, 583, 320, 405, 1023, 1203, 103, 655, 1081, 238, 929, 242, 687, 572, 832, 589, 294, 1161, 74, 746, 0, 79, 578, 760, 1200, 862, 85, 125, 167, 1180, 284, 726, 1102, 293, 710, 288, 309, 451, 1057, 1035, 918, 1030, 522, 267, 297, 634, 1103, 449, 239, 790, 277, 354, 1043, 107, 139, 731, 315, 1155, 148, 820, 956, 403, 352, 212, 592, 659, 225, 691, 845, 325, 971, 717, 758, 390, 511, 374, 453, 834, 1143, 13, 1004, 1089, 787, 866, 1159, 735, 324, 376, 1029, 902, 1080, 100, 579, 189, 87, 444, 224, 101, 486, 869, 254, 480, 195, 1210, 406, 943, 967, 37, 733, 437, 843, 702, 1213, 981, 429, 817, 957, 131, 786, 977, 1069, 631, 311, 1001, 185, 200, 737, 240, 1131, 1064, 636, 696, 365, 1108, 65, 1148, 1052, 402, 59, 913, 454, 1174, 989, 161, 440, 1195, 856, 1022, 174, 1104, 614, 1010, 15, 885, 416, 218, 368, 791, 53, 605, 314, 683, 1068, 973, 1082, 750, 596, 1096, 57, 232, 841, 880, 373, 152, 500, 1158, 1132, 727, 425, 890, 166, 356, 47, 1021, 49, 77, 211, 993, 944, 104, 338, 1018, 306, 1036, 219, 547, 206, 1083, 531, 698, 299, 1170, 996, 372, 76, 532, 257, 1138, 773, 872, 56, 328, 581, 273, 379, 601, 26, 720, 213, 331, 527, 591, 507, 681, 428, 5, 1034, 18, 31, 623, 484, 192, 441, 1011, 1125, 1049, 674, 647, 264, 73, 459, 1093, 965, 8, 120, 1185, 1067, 258, 44, 966, 666, 1044, 501, 272, 628, 830, 849, 410, 1137, 607, 452, 370, 421, 223, 346, 302, 1193, 868, 42, 130, 975, 145, 932, 1076, 432, 1141, 566, 214, 1136, 1090, 1047, 400, 633, 947, 1120, 203, 193, 360, 887, 83, 982, 457, 690, 51, 686, 556, 316, 1153, 668, 14, 835, 361, 946, 705, 1073, 248, 105, 851, 144, 274, 1002, 734, 344, 514, 962, 427, 1091, 186, 310, 553, 765, 518, 1191, 775, 643, 959, 867, 1197, 1183, 113, 209, 322, 523, 747, 266, 98, 513, 763, 980, 917, 1024, 411, 415, 112, 68, 697, 482, 1084, 649, 1003, 1056, 422, 526, 520, 111, 295, 976, 958, 802, 840, 1038, 732, 901, 878, 925, 256, 168, 694, 1182, 854, 799, 127, 875, 587, 1204, 380, 795, 1006, 1085, 155, 600, 585, 488, 661, 535, 590]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6254364806297936
the save name prefix for this run is:  chkpt-ID_6254364806297936_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.442 +- 0.006
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001444479

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.479 +- 0.005
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.78222e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.102 +- 0.004
mrr vals (pred, true): 0.005, 0.036
batch losses (mrrl, rdl): 0.0, 0.0006935301

Epoch over!
epoch time: 15.062

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1181
rank avg (pred): 0.497 +- 0.009
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 8.50576e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 300
rank avg (pred): 0.050 +- 0.038
mrr vals (pred, true): 0.011, 0.180
batch losses (mrrl, rdl): 0.0, 2.65313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.025 +- 0.085
mrr vals (pred, true): 0.142, 0.275
batch losses (mrrl, rdl): 0.0, 1.4142e-06

Epoch over!
epoch time: 15.024

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 608
rank avg (pred): 0.482 +- 0.283
mrr vals (pred, true): 0.028, 0.004
batch losses (mrrl, rdl): 0.0, 2.379e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 188
rank avg (pred): 0.475 +- 0.281
mrr vals (pred, true): 0.029, 0.006
batch losses (mrrl, rdl): 0.0, 9.095e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.500 +- 0.277
mrr vals (pred, true): 0.025, 0.003
batch losses (mrrl, rdl): 0.0, 3.4337e-06

Epoch over!
epoch time: 14.887

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 870
rank avg (pred): 0.479 +- 0.281
mrr vals (pred, true): 0.039, 0.004
batch losses (mrrl, rdl): 0.0, 6.293e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1149
rank avg (pred): 0.279 +- 0.259
mrr vals (pred, true): 0.073, 0.033
batch losses (mrrl, rdl): 0.0, 0.0001491308

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 584
rank avg (pred): 0.480 +- 0.288
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0, 6.037e-07

Epoch over!
epoch time: 14.996

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 748
rank avg (pred): 0.092 +- 0.184
mrr vals (pred, true): 0.110, 0.186
batch losses (mrrl, rdl): 0.0, 1.61526e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 554
rank avg (pred): 0.380 +- 0.274
mrr vals (pred, true): 0.060, 0.007
batch losses (mrrl, rdl): 0.0, 3.91061e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.503 +- 0.284
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.0, 1.46079e-05

Epoch over!
epoch time: 15.035

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1175
rank avg (pred): 0.473 +- 0.282
mrr vals (pred, true): 0.047, 0.002
batch losses (mrrl, rdl): 7.06305e-05, 1.0329e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 161
rank avg (pred): 0.492 +- 0.284
mrr vals (pred, true): 0.064, 0.004
batch losses (mrrl, rdl): 0.0020214408, 1.13646e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 184
rank avg (pred): 0.484 +- 0.268
mrr vals (pred, true): 0.061, 0.003
batch losses (mrrl, rdl): 0.001122153, 3.8226e-06

Epoch over!
epoch time: 15.284

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 585
rank avg (pred): 0.480 +- 0.245
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002816773, 1.23302e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 944
rank avg (pred): 0.490 +- 0.227
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.51791e-05, 0.0007618564

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 117
rank avg (pred): 0.471 +- 0.215
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.21531e-05, 2.41811e-05

Epoch over!
epoch time: 15.269

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 807
rank avg (pred): 0.484 +- 0.239
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 9.38172e-05, 1.28895e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 5
rank avg (pred): 0.214 +- 0.270
mrr vals (pred, true): 0.173, 0.217
batch losses (mrrl, rdl): 0.0198557898, 0.0004762739

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 189
rank avg (pred): 0.496 +- 0.221
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.0001221453, 2.31581e-05

Epoch over!
epoch time: 15.252

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.090 +- 0.229
mrr vals (pred, true): 0.267, 0.239
batch losses (mrrl, rdl): 0.0078805219, 4.33839e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 200
rank avg (pred): 0.484 +- 0.193
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 8.31086e-05, 3.21614e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 880
rank avg (pred): 0.462 +- 0.177
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.000119376, 3.56908e-05

Epoch over!
epoch time: 15.139

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 867
rank avg (pred): 0.482 +- 0.184
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 7.39e-08, 3.73637e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 729
rank avg (pred): 0.071 +- 0.077
mrr vals (pred, true): 0.245, 0.265
batch losses (mrrl, rdl): 0.0039083329, 2.74848e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.479 +- 0.190
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 8.1809e-06, 2.4874e-05

Epoch over!
epoch time: 14.969

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 919
rank avg (pred): 0.502 +- 0.208
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 4.1018e-06, 1.00635e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.460 +- 0.172
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 4.06734e-05, 3.95087e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.528 +- 0.238
mrr vals (pred, true): 0.043, 0.006
batch losses (mrrl, rdl): 0.0004785359, 0.0001973813

Epoch over!
epoch time: 15.026

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.491 +- 0.218
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 3.3788e-06, 2.18689e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 668
rank avg (pred): 0.506 +- 0.257
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 1.2099e-06, 1.16825e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1006
rank avg (pred): 0.509 +- 0.276
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.17847e-05, 2.51904e-05

Epoch over!
epoch time: 15.084

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 844
rank avg (pred): 0.532 +- 0.286
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0003723426, 7.9893e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 551
rank avg (pred): 0.457 +- 0.241
mrr vals (pred, true): 0.053, 0.009
batch losses (mrrl, rdl): 6.82246e-05, 1.05379e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 978
rank avg (pred): 0.043 +- 0.032
mrr vals (pred, true): 0.250, 0.337
batch losses (mrrl, rdl): 0.075978145, 2.1393e-06

Epoch over!
epoch time: 15.085

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 568
rank avg (pred): 0.484 +- 0.273
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 2.8e-09, 1.55238e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.474 +- 0.260
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 6.11302e-05, 1.7972e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 730
rank avg (pred): 0.017 +- 0.012
mrr vals (pred, true): 0.282, 0.257
batch losses (mrrl, rdl): 0.0062411102, 1.22231e-05

Epoch over!
epoch time: 15.052

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 605
rank avg (pred): 0.457 +- 0.252
mrr vals (pred, true): 0.054, 0.003
batch losses (mrrl, rdl): 0.0001366743, 3.37628e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1044
rank avg (pred): 0.521 +- 0.315
mrr vals (pred, true): 0.049, 0.005
batch losses (mrrl, rdl): 4.2862e-06, 2.04541e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 866
rank avg (pred): 0.489 +- 0.284
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 3.61756e-05, 4.1872e-06

Epoch over!
epoch time: 15.049

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.222 +- 0.122
mrr vals (pred, true): 0.136, 0.141

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   37 	     0 	 0.04870 	 0.00092 	 m..s
    5 	     1 	 0.04040 	 0.00115 	 m..s
   23 	     2 	 0.04727 	 0.00128 	 m..s
   12 	     3 	 0.04533 	 0.00140 	 m..s
    1 	     4 	 0.03690 	 0.00145 	 m..s
    3 	     5 	 0.03903 	 0.00146 	 m..s
   32 	     6 	 0.04835 	 0.00170 	 m..s
   28 	     7 	 0.04773 	 0.00196 	 m..s
   85 	     8 	 0.05358 	 0.00204 	 m..s
   56 	     9 	 0.05034 	 0.00208 	 m..s
   89 	    10 	 0.05548 	 0.00225 	 m..s
   51 	    11 	 0.04979 	 0.00242 	 m..s
   48 	    12 	 0.04957 	 0.00280 	 m..s
   73 	    13 	 0.05201 	 0.00287 	 m..s
   87 	    14 	 0.05425 	 0.00296 	 m..s
   33 	    15 	 0.04843 	 0.00305 	 m..s
   24 	    16 	 0.04733 	 0.00309 	 m..s
   20 	    17 	 0.04699 	 0.00313 	 m..s
   68 	    18 	 0.05142 	 0.00315 	 m..s
   82 	    19 	 0.05262 	 0.00321 	 m..s
   39 	    20 	 0.04875 	 0.00325 	 m..s
   52 	    21 	 0.04982 	 0.00333 	 m..s
   81 	    22 	 0.05254 	 0.00336 	 m..s
   54 	    23 	 0.05019 	 0.00339 	 m..s
   47 	    24 	 0.04952 	 0.00340 	 m..s
    7 	    25 	 0.04212 	 0.00343 	 m..s
   70 	    26 	 0.05170 	 0.00346 	 m..s
   21 	    27 	 0.04701 	 0.00346 	 m..s
   90 	    28 	 0.05612 	 0.00347 	 m..s
   30 	    29 	 0.04802 	 0.00351 	 m..s
   66 	    30 	 0.05117 	 0.00352 	 m..s
   55 	    31 	 0.05027 	 0.00356 	 m..s
   19 	    32 	 0.04699 	 0.00363 	 m..s
   65 	    33 	 0.05107 	 0.00364 	 m..s
   17 	    34 	 0.04681 	 0.00367 	 m..s
   18 	    35 	 0.04685 	 0.00368 	 m..s
   26 	    36 	 0.04746 	 0.00368 	 m..s
   67 	    37 	 0.05130 	 0.00371 	 m..s
   74 	    38 	 0.05212 	 0.00372 	 m..s
   34 	    39 	 0.04847 	 0.00372 	 m..s
   53 	    40 	 0.05008 	 0.00378 	 m..s
   49 	    41 	 0.04966 	 0.00381 	 m..s
   62 	    42 	 0.05061 	 0.00385 	 m..s
   40 	    43 	 0.04880 	 0.00385 	 m..s
   71 	    44 	 0.05177 	 0.00400 	 m..s
   50 	    45 	 0.04973 	 0.00403 	 m..s
   76 	    46 	 0.05222 	 0.00403 	 m..s
   75 	    47 	 0.05214 	 0.00403 	 m..s
   22 	    48 	 0.04703 	 0.00405 	 m..s
   42 	    49 	 0.04895 	 0.00411 	 m..s
   60 	    50 	 0.05042 	 0.00413 	 m..s
   27 	    51 	 0.04766 	 0.00414 	 m..s
    0 	    52 	 0.03498 	 0.00415 	 m..s
   78 	    53 	 0.05225 	 0.00415 	 m..s
   15 	    54 	 0.04634 	 0.00416 	 m..s
   31 	    55 	 0.04805 	 0.00417 	 m..s
   16 	    56 	 0.04677 	 0.00420 	 m..s
   80 	    57 	 0.05250 	 0.00426 	 m..s
    8 	    58 	 0.04261 	 0.00429 	 m..s
   58 	    59 	 0.05038 	 0.00429 	 m..s
   10 	    60 	 0.04311 	 0.00433 	 m..s
   69 	    61 	 0.05165 	 0.00437 	 m..s
   41 	    62 	 0.04889 	 0.00445 	 m..s
   57 	    63 	 0.05035 	 0.00447 	 m..s
   64 	    64 	 0.05106 	 0.00452 	 m..s
   43 	    65 	 0.04900 	 0.00454 	 m..s
   63 	    66 	 0.05080 	 0.00455 	 m..s
   29 	    67 	 0.04791 	 0.00455 	 m..s
   45 	    68 	 0.04942 	 0.00458 	 m..s
    4 	    69 	 0.03935 	 0.00459 	 m..s
   59 	    70 	 0.05040 	 0.00468 	 m..s
   25 	    71 	 0.04739 	 0.00469 	 m..s
   86 	    72 	 0.05407 	 0.00475 	 m..s
    6 	    73 	 0.04212 	 0.00478 	 m..s
   84 	    74 	 0.05298 	 0.00486 	 m..s
   77 	    75 	 0.05223 	 0.00489 	 m..s
   38 	    76 	 0.04873 	 0.00492 	 m..s
   36 	    77 	 0.04861 	 0.00500 	 m..s
   46 	    78 	 0.04942 	 0.00508 	 m..s
   13 	    79 	 0.04535 	 0.00510 	 m..s
   11 	    80 	 0.04514 	 0.00525 	 m..s
   35 	    81 	 0.04854 	 0.00545 	 m..s
   44 	    82 	 0.04906 	 0.00550 	 m..s
   61 	    83 	 0.05048 	 0.00551 	 m..s
   14 	    84 	 0.04609 	 0.00602 	 m..s
    9 	    85 	 0.04295 	 0.00729 	 m..s
   72 	    86 	 0.05181 	 0.00858 	 m..s
   79 	    87 	 0.05227 	 0.00867 	 m..s
   88 	    88 	 0.05488 	 0.01240 	 m..s
   91 	    89 	 0.05617 	 0.01360 	 m..s
   83 	    90 	 0.05294 	 0.01402 	 m..s
   92 	    91 	 0.05618 	 0.02856 	 ~...
    2 	    92 	 0.03861 	 0.04671 	 ~...
   94 	    93 	 0.12927 	 0.12873 	 ~...
   99 	    94 	 0.14233 	 0.13094 	 ~...
   97 	    95 	 0.13512 	 0.13249 	 ~...
   93 	    96 	 0.12883 	 0.13339 	 ~...
   96 	    97 	 0.13448 	 0.13765 	 ~...
   95 	    98 	 0.12987 	 0.13961 	 ~...
   98 	    99 	 0.13561 	 0.14054 	 ~...
  105 	   100 	 0.21521 	 0.15098 	 m..s
  100 	   101 	 0.17717 	 0.17596 	 ~...
  104 	   102 	 0.20058 	 0.18248 	 ~...
  103 	   103 	 0.19747 	 0.18464 	 ~...
  102 	   104 	 0.19401 	 0.18497 	 ~...
  106 	   105 	 0.22402 	 0.19343 	 m..s
  101 	   106 	 0.18569 	 0.19489 	 ~...
  108 	   107 	 0.24103 	 0.22805 	 ~...
  107 	   108 	 0.23079 	 0.24178 	 ~...
  112 	   109 	 0.25829 	 0.26363 	 ~...
  109 	   110 	 0.24251 	 0.26436 	 ~...
  115 	   111 	 0.26700 	 0.27019 	 ~...
  116 	   112 	 0.27191 	 0.29612 	 ~...
  119 	   113 	 0.29383 	 0.30562 	 ~...
  111 	   114 	 0.25819 	 0.31362 	 m..s
  110 	   115 	 0.25007 	 0.31468 	 m..s
  117 	   116 	 0.27983 	 0.32135 	 m..s
  113 	   117 	 0.26164 	 0.32320 	 m..s
  114 	   118 	 0.26411 	 0.32947 	 m..s
  118 	   119 	 0.28994 	 0.34039 	 m..s
  120 	   120 	 0.29441 	 0.34968 	 m..s
==========================================
r_mrr = 0.989564061164856
r2_mrr = 0.8263136148452759
spearmanr_mrr@5 = 0.8684414625167847
spearmanr_mrr@10 = 0.8848621249198914
spearmanr_mrr@50 = 0.9922817349433899
spearmanr_mrr@100 = 0.9949594140052795
spearmanr_mrr@All = 0.9948608875274658
==========================================
test time: 0.447
Done Testing dataset CoDExSmall
total time taken: 237.29760432243347
training time taken: 226.67253613471985
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9896)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8263)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.8684)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.8849)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9923)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9950)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9949)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.3285017487676214}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 602161709425367
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [583, 551, 866, 657, 259, 465, 659, 58, 320, 1064, 78, 598, 424, 143, 812, 427, 715, 743, 716, 119, 476, 1045, 1095, 1201, 250, 818, 964, 568, 1123, 532, 742, 218, 815, 1147, 305, 601, 655, 1207, 42, 709, 264, 967, 989, 245, 711, 787, 806, 908, 53, 690, 70, 483, 50, 983, 1039, 92, 595, 828, 564, 1113, 339, 641, 438, 98, 539, 674, 757, 304, 1055, 534, 834, 960, 96, 650, 1140, 21, 1132, 826, 451, 361, 1187, 199, 47, 45, 766, 25, 379, 443, 224, 1189, 410, 139, 938, 1071, 557, 87, 1003, 767, 689, 425, 499, 852, 280, 94, 240, 306, 229, 404, 769, 615, 809, 848, 315, 553, 393, 880, 286, 473, 55, 700, 981]
valid_ids (0): []
train_ids (1094): [118, 1198, 1067, 755, 801, 11, 990, 687, 204, 108, 731, 1088, 608, 719, 1162, 833, 955, 811, 399, 1195, 431, 249, 1041, 508, 1112, 61, 524, 63, 822, 105, 185, 799, 442, 581, 756, 279, 82, 153, 702, 906, 19, 737, 708, 1120, 631, 1202, 879, 114, 323, 459, 786, 537, 299, 441, 497, 391, 490, 471, 390, 696, 46, 800, 322, 588, 673, 248, 904, 40, 289, 1103, 751, 1186, 444, 661, 559, 946, 721, 781, 179, 995, 999, 850, 837, 470, 593, 883, 717, 857, 1065, 232, 398, 209, 831, 969, 1079, 182, 876, 1094, 243, 353, 916, 724, 985, 206, 1153, 710, 88, 293, 996, 734, 336, 356, 90, 918, 591, 177, 1155, 3, 522, 1044, 1058, 176, 513, 663, 518, 874, 448, 408, 738, 367, 622, 6, 872, 360, 932, 447, 630, 437, 1209, 359, 713, 378, 1139, 1200, 155, 491, 1030, 362, 963, 382, 258, 901, 197, 34, 1072, 211, 265, 685, 17, 1046, 327, 516, 162, 853, 397, 798, 373, 902, 1026, 191, 352, 1061, 1125, 1069, 794, 541, 183, 174, 1119, 457, 1012, 4, 529, 693, 222, 219, 54, 52, 129, 103, 914, 36, 939, 157, 1197, 954, 774, 619, 558, 1036, 648, 554, 260, 384, 125, 607, 1062, 924, 1097, 933, 749, 288, 194, 582, 1037, 504, 664, 763, 1118, 329, 682, 894, 579, 484, 808, 1019, 1089, 328, 500, 1051, 1056, 184, 779, 201, 796, 950, 928, 1060, 15, 546, 851, 10, 406, 86, 152, 235, 686, 489, 1034, 1165, 727, 220, 670, 349, 364, 1005, 308, 1134, 758, 32, 523, 1059, 1110, 1031, 1160, 333, 978, 71, 186, 895, 462, 810, 135, 1068, 44, 163, 775, 1048, 110, 403, 703, 1047, 1106, 945, 1188, 324, 345, 458, 449, 885, 549, 262, 1133, 621, 411, 600, 547, 846, 231, 120, 417, 1091, 440, 246, 91, 1029, 464, 318, 1050, 535, 344, 971, 1137, 509, 861, 421, 791, 991, 538, 841, 838, 1028, 637, 1111, 503, 639, 651, 966, 266, 886, 590, 980, 530, 1013, 839, 498, 836, 854, 375, 132, 1211, 432, 667, 1178, 772, 512, 175, 270, 370, 354, 587, 22, 131, 330, 691, 705, 455, 226, 863, 43, 620, 893, 1168, 698, 1146, 1022, 975, 221, 979, 962, 366, 272, 446, 627, 778, 435, 241, 493, 931, 433, 677, 1135, 783, 1169, 909, 23, 877, 217, 422, 884, 141, 927, 343, 1194, 819, 407, 1175, 169, 212, 575, 35, 80, 935, 1156, 301, 342, 987, 488, 254, 242, 227, 198, 905, 1126, 79, 126, 122, 26, 671, 142, 210, 511, 515, 656, 74, 452, 357, 507, 603, 552, 389, 436, 665, 792, 797, 445, 1163, 602, 430, 1070, 1117, 392, 236, 1042, 643, 1124, 773, 121, 947, 616, 612, 1021, 128, 744, 1171, 707, 666, 481, 675, 1170, 1205, 255, 423, 64, 247, 816, 316, 1084, 681, 84, 735, 982, 326, 171, 596, 1213, 1087, 574, 725, 31, 1075, 127, 1074, 771, 1199, 130, 144, 37, 67, 823, 560, 1172, 635, 1129, 624, 526, 626, 911, 412, 291, 814, 913, 992, 533, 146, 625, 542, 334, 202, 337, 269, 563, 18, 567, 521, 1001, 1007, 2, 594, 62, 986, 213, 369, 623, 1157, 561, 290, 654, 660, 109, 943, 907, 573, 386, 517, 672, 576, 496, 368, 81, 860, 495, 1014, 405, 428, 817, 388, 695, 889, 592, 73, 477, 387, 613, 303, 56, 479, 60, 844, 1142, 658, 494, 365, 68, 51, 180, 85, 520, 281, 33, 788, 1138, 562, 486, 414, 543, 694, 253, 99, 413, 1077, 793, 1174, 314, 920, 346, 688, 145, 380, 205, 1158, 1073, 977, 636, 865, 1090, 502, 418, 1109, 903, 728, 482, 284, 570, 394, 454, 113, 1141, 974, 456, 1104, 1011, 569, 917, 1009, 475, 385, 97, 1183, 891, 402, 873, 528, 878, 89, 138, 1144, 506, 765, 605, 460, 115, 803, 27, 732, 642, 300, 965, 921, 958, 1114, 325, 1150, 611, 919, 813, 168, 669, 1105, 736, 57, 1159, 701, 915, 1057, 1086, 739, 1085, 1035, 401, 332, 776, 167, 1016, 1099, 1180, 505, 759, 697, 790, 1098, 1025, 28, 453, 1167, 652, 887, 133, 572, 780, 159, 1015, 429, 720, 548, 383, 825, 203, 501, 287, 712, 173, 1116, 519, 948, 647, 148, 514, 937, 868, 59, 376, 550, 770, 24, 1093, 1152, 1040, 396, 644, 633, 589, 942, 347, 117, 225, 922, 750, 1204, 678, 555, 485, 72, 899, 843, 39, 892, 76, 820, 1032, 338, 492, 350, 1149, 156, 953, 525, 718, 1179, 976, 381, 1143, 1006, 251, 733, 1131, 363, 1184, 341, 1190, 706, 134, 295, 1148, 30, 944, 746, 881, 545, 1206, 832, 102, 261, 1101, 752, 745, 1128, 1010, 923, 890, 420, 930, 1121, 150, 480, 164, 450, 586, 859, 1063, 302, 351, 1177, 298, 1161, 1027, 1182, 1164, 1130, 956, 910, 882, 319, 434, 870, 994, 1052, 277, 740, 649, 556, 107, 310, 1033, 730, 584, 181, 867, 187, 668, 1078, 604, 292, 172, 154, 215, 1049, 1108, 14, 845, 638, 577, 871, 760, 234, 925, 566, 158, 69, 609, 565, 223, 439, 1081, 1193, 311, 415, 1127, 645, 161, 170, 1122, 748, 864, 137, 101, 926, 862, 340, 1203, 267, 123, 487, 1176, 20, 952, 1004, 377, 274, 789, 296, 200, 355, 228, 140, 936, 683, 540, 847, 1192, 371, 961, 1000, 722, 723, 805, 75, 188, 151, 1020, 729, 1023, 988, 400, 632, 165, 273, 951, 1173, 472, 940, 634, 41, 1100, 777, 824, 466, 1080, 941, 1038, 1, 684, 900, 1181, 680, 48, 196, 1136, 348, 1066, 16, 1151, 1107, 256, 840, 510, 761, 571, 29, 998, 929, 136, 317, 1008, 233, 959, 312, 9, 409, 536, 762, 704, 276, 544, 372, 830, 193, 469, 106, 416, 527, 160, 331, 244, 842, 973, 897, 1102, 802, 753, 395, 124, 83, 214, 309, 934, 849, 238, 285, 461, 804, 997, 888, 100, 699, 679, 912, 653, 147, 896, 640, 1196, 112, 77, 149, 467, 252, 856, 785, 104, 617, 13, 297, 1092, 321, 95, 768, 578, 764, 307, 374, 629, 463, 784, 949, 257, 875, 858, 1096, 178, 898, 66, 335, 271, 65, 782, 662, 599, 192, 8, 754, 474, 237, 468, 827, 1017, 957, 294, 618, 313, 1191, 1212, 426, 968, 1214, 195, 597, 49, 606, 676, 610, 1210, 116, 1208, 614, 646, 263, 239, 970, 726, 747, 478, 1145, 283, 984, 628, 12, 580, 216, 190, 38, 692, 275, 714, 585, 1002, 7, 1185, 207, 972, 278, 93, 268, 111, 855, 807, 531, 1115, 835, 1018, 1166, 0, 1054, 1082, 5, 230, 189, 829, 419, 869, 166, 741, 1024, 1154, 795, 282, 1043, 208, 1076, 358, 993, 1083, 1053, 821]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  599702261446070
the save name prefix for this run is:  chkpt-ID_599702261446070_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'ComplEx': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1065
rank avg (pred): 0.514 +- 0.005
mrr vals (pred, true): 0.001, 0.244
batch losses (mrrl, rdl): 0.0, 0.0044282153

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 560
rank avg (pred): 0.368 +- 0.032
mrr vals (pred, true): 0.001, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001662597

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 587
rank avg (pred): 0.457 +- 0.278
mrr vals (pred, true): 0.127, 0.003
batch losses (mrrl, rdl): 0.0, 6.9544e-06

Epoch over!
epoch time: 14.901

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1016
rank avg (pred): 0.424 +- 0.275
mrr vals (pred, true): 0.131, 0.004
batch losses (mrrl, rdl): 0.0, 1.57682e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 153
rank avg (pred): 0.470 +- 0.290
mrr vals (pred, true): 0.088, 0.004
batch losses (mrrl, rdl): 0.0, 5.7066e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 956
rank avg (pred): 0.544 +- 0.303
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001057453

Epoch over!
epoch time: 14.877

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 530
rank avg (pred): 0.381 +- 0.263
mrr vals (pred, true): 0.067, 0.008
batch losses (mrrl, rdl): 0.0, 1.54528e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 415
rank avg (pred): 0.467 +- 0.273
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 0.0, 1.3704e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 242
rank avg (pred): 0.479 +- 0.282
mrr vals (pred, true): 0.037, 0.004
batch losses (mrrl, rdl): 0.0, 2.608e-06

Epoch over!
epoch time: 14.859

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 371
rank avg (pred): 0.465 +- 0.283
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 0.0, 2.481e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 210
rank avg (pred): 0.458 +- 0.284
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 0.0, 1.48915e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 336
rank avg (pred): 0.466 +- 0.277
mrr vals (pred, true): 0.037, 0.004
batch losses (mrrl, rdl): 0.0, 5.374e-07

Epoch over!
epoch time: 14.891

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 209
rank avg (pred): 0.465 +- 0.303
mrr vals (pred, true): 0.065, 0.005
batch losses (mrrl, rdl): 0.0, 8.4626e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.466 +- 0.279
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 0.0, 1.11883e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 611
rank avg (pred): 0.501 +- 0.286
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0, 2.4085e-06

Epoch over!
epoch time: 14.875

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.467 +- 0.277
mrr vals (pred, true): 0.060, 0.004
batch losses (mrrl, rdl): 0.0010257055, 2.4848e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 750
rank avg (pred): 0.016 +- 0.028
mrr vals (pred, true): 0.182, 0.201
batch losses (mrrl, rdl): 0.0033307842, 3.54003e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 630
rank avg (pred): 0.463 +- 0.184
mrr vals (pred, true): 0.051, 0.003
batch losses (mrrl, rdl): 1.03434e-05, 2.56146e-05

Epoch over!
epoch time: 15.085

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1170
rank avg (pred): 0.441 +- 0.191
mrr vals (pred, true): 0.057, 0.003
batch losses (mrrl, rdl): 0.0004630416, 4.72476e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 693
rank avg (pred): 0.468 +- 0.193
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.62546e-05, 3.30987e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 882
rank avg (pred): 0.465 +- 0.215
mrr vals (pred, true): 0.061, 0.004
batch losses (mrrl, rdl): 0.0012515702, 2.11899e-05

Epoch over!
epoch time: 15.056

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 914
rank avg (pred): 0.605 +- 0.280
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.3128e-05, 4.29188e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.113 +- 0.214
mrr vals (pred, true): 0.213, 0.177
batch losses (mrrl, rdl): 0.0129680792, 3.40652e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1041
rank avg (pred): 0.464 +- 0.211
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 3.7365e-06, 2.861e-05

Epoch over!
epoch time: 15.151

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.462 +- 0.226
mrr vals (pred, true): 0.059, 0.005
batch losses (mrrl, rdl): 0.0007388486, 1.6816e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 705
rank avg (pred): 0.462 +- 0.204
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 0.0001153568, 3.49621e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 895
rank avg (pred): 0.479 +- 0.191
mrr vals (pred, true): 0.042, 0.002
batch losses (mrrl, rdl): 0.0006703135, 7.57774e-05

Epoch over!
epoch time: 15.066

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 273
rank avg (pred): 0.110 +- 0.191
mrr vals (pred, true): 0.132, 0.174
batch losses (mrrl, rdl): 0.0180553738, 1.57497e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 796
rank avg (pred): 0.473 +- 0.224
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001269575, 1.94845e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 897
rank avg (pred): 0.504 +- 0.195
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.66101e-05, 0.0001324681

Epoch over!
epoch time: 15.076

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1028
rank avg (pred): 0.485 +- 0.194
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.92498e-05, 3.03822e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 718
rank avg (pred): 0.455 +- 0.209
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001415222, 3.57817e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 354
rank avg (pred): 0.471 +- 0.206
mrr vals (pred, true): 0.048, 0.005
batch losses (mrrl, rdl): 2.82963e-05, 3.57295e-05

Epoch over!
epoch time: 15.077

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 604
rank avg (pred): 0.493 +- 0.256
mrr vals (pred, true): 0.058, 0.003
batch losses (mrrl, rdl): 0.0006986989, 6.5148e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.510 +- 0.224
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 4.32905e-05, 5.10871e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 1094
rank avg (pred): 0.491 +- 0.205
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 8.05641e-05, 2.76958e-05

Epoch over!
epoch time: 15.1

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 915
rank avg (pred): 0.516 +- 0.218
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.59663e-05, 0.0006874746

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 204
rank avg (pred): 0.497 +- 0.240
mrr vals (pred, true): 0.050, 0.005
batch losses (mrrl, rdl): 3.378e-07, 1.65703e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.417 +- 0.165
mrr vals (pred, true): 0.049, 0.023
batch losses (mrrl, rdl): 1.5885e-05, 6.18969e-05

Epoch over!
epoch time: 15.014

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 121
rank avg (pred): 0.482 +- 0.213
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.99279e-05, 2.43266e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 833
rank avg (pred): 0.044 +- 0.076
mrr vals (pred, true): 0.238, 0.257
batch losses (mrrl, rdl): 0.0037947465, 7.8924e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 344
rank avg (pred): 0.511 +- 0.222
mrr vals (pred, true): 0.054, 0.005
batch losses (mrrl, rdl): 0.0001563801, 8.42229e-05

Epoch over!
epoch time: 14.995

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 480
rank avg (pred): 0.484 +- 0.234
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 0.0001065016, 1.79589e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 30
rank avg (pred): 0.303 +- 0.212
mrr vals (pred, true): 0.130, 0.114
batch losses (mrrl, rdl): 0.0028114486, 0.0009919857

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, CoDExSmall, run 2.1, exp 962
rank avg (pred): 0.544 +- 0.238
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004944645, 5.13405e-05

Epoch over!
epoch time: 14.986

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.500 +- 0.253
mrr vals (pred, true): 0.047, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.04837 	 0.00088 	 m..s
   29 	     1 	 0.04950 	 0.00242 	 m..s
    7 	     2 	 0.04733 	 0.00250 	 m..s
   71 	     3 	 0.05370 	 0.00288 	 m..s
   33 	     4 	 0.05044 	 0.00294 	 m..s
    5 	     5 	 0.04706 	 0.00297 	 m..s
    4 	     6 	 0.04698 	 0.00304 	 m..s
   39 	     7 	 0.05157 	 0.00305 	 m..s
   18 	     8 	 0.04817 	 0.00316 	 m..s
   78 	     9 	 0.05443 	 0.00317 	 m..s
   12 	    10 	 0.04785 	 0.00324 	 m..s
   27 	    11 	 0.04931 	 0.00327 	 m..s
   60 	    12 	 0.05282 	 0.00329 	 m..s
   44 	    13 	 0.05173 	 0.00335 	 m..s
   53 	    14 	 0.05240 	 0.00336 	 m..s
   23 	    15 	 0.04876 	 0.00336 	 m..s
   42 	    16 	 0.05167 	 0.00339 	 m..s
   14 	    17 	 0.04798 	 0.00343 	 m..s
   69 	    18 	 0.05330 	 0.00343 	 m..s
   65 	    19 	 0.05307 	 0.00349 	 m..s
   68 	    20 	 0.05318 	 0.00350 	 m..s
   64 	    21 	 0.05303 	 0.00354 	 m..s
   72 	    22 	 0.05371 	 0.00358 	 m..s
   63 	    23 	 0.05303 	 0.00360 	 m..s
    1 	    24 	 0.04196 	 0.00366 	 m..s
   46 	    25 	 0.05182 	 0.00367 	 m..s
   19 	    26 	 0.04822 	 0.00367 	 m..s
   30 	    27 	 0.04956 	 0.00368 	 m..s
   36 	    28 	 0.05111 	 0.00369 	 m..s
   41 	    29 	 0.05162 	 0.00373 	 m..s
   13 	    30 	 0.04788 	 0.00376 	 m..s
   57 	    31 	 0.05269 	 0.00376 	 m..s
   24 	    32 	 0.04895 	 0.00380 	 m..s
   16 	    33 	 0.04800 	 0.00382 	 m..s
   34 	    34 	 0.05058 	 0.00382 	 m..s
   21 	    35 	 0.04841 	 0.00385 	 m..s
   22 	    36 	 0.04852 	 0.00391 	 m..s
   76 	    37 	 0.05428 	 0.00401 	 m..s
    0 	    38 	 0.04168 	 0.00405 	 m..s
   75 	    39 	 0.05400 	 0.00407 	 m..s
   43 	    40 	 0.05173 	 0.00410 	 m..s
   67 	    41 	 0.05313 	 0.00411 	 m..s
   37 	    42 	 0.05142 	 0.00414 	 m..s
   54 	    43 	 0.05244 	 0.00414 	 m..s
   28 	    44 	 0.04942 	 0.00419 	 m..s
   79 	    45 	 0.05459 	 0.00420 	 m..s
   26 	    46 	 0.04920 	 0.00420 	 m..s
   70 	    47 	 0.05362 	 0.00426 	 m..s
   61 	    48 	 0.05282 	 0.00428 	 m..s
   32 	    49 	 0.05043 	 0.00429 	 m..s
    8 	    50 	 0.04740 	 0.00431 	 m..s
   56 	    51 	 0.05257 	 0.00436 	 m..s
   47 	    52 	 0.05183 	 0.00440 	 m..s
   48 	    53 	 0.05206 	 0.00441 	 m..s
   11 	    54 	 0.04779 	 0.00442 	 m..s
   25 	    55 	 0.04899 	 0.00449 	 m..s
    9 	    56 	 0.04758 	 0.00451 	 m..s
   74 	    57 	 0.05397 	 0.00464 	 m..s
    2 	    58 	 0.04648 	 0.00464 	 m..s
   66 	    59 	 0.05311 	 0.00464 	 m..s
   10 	    60 	 0.04759 	 0.00469 	 m..s
   49 	    61 	 0.05206 	 0.00472 	 m..s
   15 	    62 	 0.04800 	 0.00484 	 m..s
   55 	    63 	 0.05256 	 0.00489 	 m..s
   17 	    64 	 0.04812 	 0.00492 	 m..s
   50 	    65 	 0.05213 	 0.00493 	 m..s
   40 	    66 	 0.05161 	 0.00500 	 m..s
   58 	    67 	 0.05277 	 0.00501 	 m..s
   31 	    68 	 0.04978 	 0.00509 	 m..s
    3 	    69 	 0.04657 	 0.00509 	 m..s
    6 	    70 	 0.04709 	 0.00510 	 m..s
   62 	    71 	 0.05290 	 0.00523 	 m..s
   35 	    72 	 0.05076 	 0.00531 	 m..s
   59 	    73 	 0.05279 	 0.00533 	 m..s
   45 	    74 	 0.05178 	 0.00533 	 m..s
   80 	    75 	 0.05461 	 0.00650 	 m..s
   73 	    76 	 0.05378 	 0.00661 	 m..s
   51 	    77 	 0.05228 	 0.00680 	 m..s
   52 	    78 	 0.05238 	 0.00764 	 m..s
   83 	    79 	 0.06382 	 0.00815 	 m..s
   81 	    80 	 0.06167 	 0.00815 	 m..s
   82 	    81 	 0.06253 	 0.00850 	 m..s
   77 	    82 	 0.05438 	 0.02763 	 ~...
   84 	    83 	 0.06872 	 0.02838 	 m..s
   38 	    84 	 0.05144 	 0.03521 	 ~...
   92 	    85 	 0.15040 	 0.03563 	 MISS
   86 	    86 	 0.12903 	 0.12873 	 ~...
   91 	    87 	 0.14855 	 0.12889 	 ~...
   90 	    88 	 0.14780 	 0.13094 	 ~...
   93 	    89 	 0.15079 	 0.13317 	 ~...
   89 	    90 	 0.14558 	 0.13484 	 ~...
   88 	    91 	 0.12991 	 0.13525 	 ~...
   94 	    92 	 0.15225 	 0.13534 	 ~...
   87 	    93 	 0.12925 	 0.13654 	 ~...
   85 	    94 	 0.12861 	 0.13734 	 ~...
  101 	    95 	 0.20537 	 0.15358 	 m..s
  102 	    96 	 0.20721 	 0.16301 	 m..s
  100 	    97 	 0.19457 	 0.17307 	 ~...
   95 	    98 	 0.17887 	 0.17858 	 ~...
   96 	    99 	 0.18036 	 0.18174 	 ~...
  103 	   100 	 0.20965 	 0.18472 	 ~...
   98 	   101 	 0.18623 	 0.18632 	 ~...
   99 	   102 	 0.18848 	 0.19121 	 ~...
  106 	   103 	 0.22995 	 0.19385 	 m..s
   97 	   104 	 0.18265 	 0.19489 	 ~...
  107 	   105 	 0.23182 	 0.19940 	 m..s
  105 	   106 	 0.21500 	 0.20171 	 ~...
  104 	   107 	 0.21213 	 0.21048 	 ~...
  109 	   108 	 0.24601 	 0.23928 	 ~...
  110 	   109 	 0.26265 	 0.24025 	 ~...
  108 	   110 	 0.24344 	 0.24456 	 ~...
  111 	   111 	 0.27892 	 0.24879 	 m..s
  118 	   112 	 0.30820 	 0.28276 	 ~...
  113 	   113 	 0.29612 	 0.28931 	 ~...
  116 	   114 	 0.30532 	 0.29618 	 ~...
  119 	   115 	 0.31258 	 0.29629 	 ~...
  117 	   116 	 0.30686 	 0.30562 	 ~...
  114 	   117 	 0.29846 	 0.30632 	 ~...
  120 	   118 	 0.31548 	 0.31216 	 ~...
  112 	   119 	 0.28476 	 0.31529 	 m..s
  115 	   120 	 0.30372 	 0.32320 	 ~...
==========================================
r_mrr = 0.9888235330581665
r2_mrr = 0.8220750093460083
spearmanr_mrr@5 = 0.9704035520553589
spearmanr_mrr@10 = 0.9557638764381409
spearmanr_mrr@50 = 0.9909256100654602
spearmanr_mrr@100 = 0.9951664209365845
spearmanr_mrr@All = 0.9954899549484253
==========================================
test time: 0.448
Done Testing dataset CoDExSmall
total time taken: 236.02345538139343
training time taken: 225.4685673713684
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'CoDExSmall': tensor(0.9888)}}, 'r2_mrr': {'ComplEx': {'CoDExSmall': tensor(0.8221)}}, 'spearmanr_mrr@5': {'ComplEx': {'CoDExSmall': tensor(0.9704)}}, 'spearmanr_mrr@10': {'ComplEx': {'CoDExSmall': tensor(0.9558)}}, 'spearmanr_mrr@50': {'ComplEx': {'CoDExSmall': tensor(0.9909)}}, 'spearmanr_mrr@100': {'ComplEx': {'CoDExSmall': tensor(0.9952)}}, 'spearmanr_mrr@All': {'ComplEx': {'CoDExSmall': tensor(0.9955)}}, 'test_loss': {'ComplEx': {'CoDExSmall': 0.26436741567522404}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 8004585941334529
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [269, 920, 1178, 370, 129, 596, 853, 957, 908, 422, 861, 156, 754, 702, 891, 55, 765, 731, 209, 670, 514, 332, 1128, 1018, 1043, 941, 955, 397, 1202, 16, 516, 163, 432, 1076, 1097, 838, 198, 474, 986, 544, 638, 415, 231, 818, 686, 36, 712, 895, 915, 41, 546, 182, 1013, 61, 216, 305, 453, 962, 880, 535, 19, 537, 420, 43, 777, 902, 242, 1160, 1056, 600, 555, 884, 82, 945, 229, 1195, 877, 1136, 1015, 191, 612, 1085, 899, 931, 22, 873, 463, 746, 718, 570, 1193, 1102, 926, 1000, 1208, 339, 306, 490, 890, 134, 609, 121, 855, 476, 414, 888, 970, 426, 495, 572, 462, 882, 693, 1164, 952, 905, 684, 1007, 510, 1016, 930]
valid_ids (0): []
train_ids (1094): [1155, 131, 714, 312, 831, 192, 646, 508, 97, 1059, 918, 656, 440, 213, 876, 1023, 579, 985, 742, 423, 90, 30, 141, 640, 214, 975, 906, 469, 536, 1072, 1051, 849, 342, 1074, 1024, 889, 762, 1166, 732, 886, 629, 1048, 867, 10, 841, 392, 524, 266, 408, 78, 1009, 525, 825, 735, 438, 3, 999, 369, 815, 429, 784, 1040, 206, 433, 1036, 1100, 786, 11, 268, 1163, 1006, 154, 1148, 947, 284, 446, 245, 195, 382, 690, 388, 1168, 790, 792, 959, 736, 310, 1189, 549, 107, 625, 256, 393, 218, 193, 1027, 1196, 152, 262, 692, 604, 79, 713, 897, 738, 817, 733, 44, 40, 145, 324, 1144, 487, 606, 630, 522, 329, 892, 1068, 402, 724, 1146, 361, 1050, 334, 641, 1135, 364, 404, 1127, 751, 1037, 345, 101, 1021, 359, 576, 375, 607, 940, 354, 343, 139, 326, 367, 628, 937, 740, 111, 563, 1083, 114, 605, 1025, 688, 834, 568, 1080, 37, 898, 548, 1062, 1124, 616, 936, 878, 1120, 559, 452, 865, 956, 1, 710, 466, 1147, 648, 62, 618, 248, 827, 530, 990, 587, 1042, 309, 781, 1153, 128, 125, 564, 323, 390, 864, 919, 223, 34, 622, 1125, 814, 527, 807, 396, 140, 308, 866, 743, 602, 1031, 627, 1150, 1105, 672, 1093, 31, 1030, 820, 974, 418, 1142, 319, 796, 639, 1075, 830, 448, 1098, 939, 357, 562, 874, 1038, 389, 798, 922, 431, 540, 1212, 595, 86, 964, 542, 836, 811, 144, 589, 489, 557, 178, 511, 1179, 179, 727, 1129, 270, 954, 381, 938, 1122, 1114, 21, 1033, 806, 913, 1101, 185, 1073, 38, 430, 943, 519, 997, 721, 681, 631, 149, 437, 25, 227, 1133, 633, 824, 552, 881, 745, 483, 278, 6, 518, 108, 917, 1069, 482, 935, 445, 887, 528, 1061, 328, 60, 1077, 346, 497, 53, 135, 585, 934, 24, 479, 93, 1206, 984, 809, 883, 805, 680, 23, 5, 74, 427, 653, 228, 117, 51, 142, 478, 1175, 603, 837, 744, 9, 520, 289, 584, 54, 950, 571, 1032, 502, 973, 812, 789, 987, 443, 250, 33, 333, 835, 484, 927, 683, 772, 671, 200, 813, 492, 455, 1191, 706, 281, 729, 1003, 1165, 1065, 588, 283, 451, 1194, 1019, 1063, 803, 1113, 301, 726, 1204, 1055, 391, 59, 398, 267, 66, 533, 1103, 1197, 980, 608, 184, 907, 504, 948, 8, 1167, 1177, 791, 863, 362, 1078, 105, 903, 532, 788, 459, 292, 719, 251, 254, 293, 313, 1188, 802, 447, 635, 48, 481, 197, 655, 493, 660, 138, 341, 435, 320, 1034, 816, 57, 1108, 946, 350, 444, 285, 264, 158, 4, 159, 127, 541, 750, 967, 795, 371, 1205, 661, 1054, 110, 1190, 663, 132, 794, 632, 299, 1119, 253, 1200, 598, 868, 870, 993, 103, 473, 260, 160, 871, 842, 348, 1207, 989, 1181, 860, 416, 1180, 146, 783, 207, 829, 92, 747, 1084, 1210, 316, 166, 238, 1141, 574, 573, 409, 439, 210, 172, 924, 850, 591, 360, 475, 983, 569, 679, 1159, 928, 691, 302, 244, 896, 39, 538, 387, 566, 933, 1086, 912, 1071, 50, 304, 471, 161, 225, 311, 1096, 28, 190, 774, 116, 1092, 958, 164, 804, 95, 1070, 384, 336, 349, 383, 196, 626, 1186, 1154, 965, 461, 668, 64, 425, 77, 441, 823, 1058, 1104, 1214, 1213, 273, 534, 249, 279, 467, 45, 623, 219, 1192, 212, 720, 509, 1156, 748, 711, 406, 529, 979, 599, 852, 186, 872, 65, 1052, 428, 303, 122, 233, 1047, 1049, 230, 1008, 2, 1045, 317, 69, 457, 846, 488, 581, 1044, 1118, 298, 554, 124, 1134, 780, 717, 403, 331, 526, 921, 998, 202, 366, 1064, 769, 689, 1028, 910, 417, 685, 1137, 314, 464, 63, 297, 373, 75, 468, 1017, 173, 1089, 201, 372, 84, 1183, 87, 793, 1111, 81, 88, 678, 203, 971, 358, 755, 1138, 844, 330, 862, 1149, 171, 12, 1046, 505, 470, 47, 649, 737, 322, 465, 344, 904, 699, 376, 1117, 664, 168, 1131, 300, 176, 676, 611, 261, 411, 634, 153, 620, 76, 338, 85, 335, 966, 756, 222, 923, 286, 687, 725, 91, 352, 123, 673, 1012, 723, 550, 162, 1173, 799, 294, 593, 766, 205, 32, 1095, 1123, 840, 1203, 137, 610, 96, 650, 424, 378, 759, 401, 458, 234, 1187, 617, 102, 258, 377, 199, 115, 106, 925, 1109, 421, 1087, 1132, 1022, 208, 543, 592, 407, 480, 578, 991, 703, 645, 845, 859, 951, 94, 953, 911, 771, 356, 259, 399, 708, 477, 1116, 18, 636, 287, 1011, 1157, 739, 49, 1209, 553, 436, 130, 705, 819, 120, 1088, 558, 147, 1005, 169, 450, 151, 575, 113, 290, 1039, 778, 68, 288, 52, 263, 136, 275, 988, 1161, 770, 642, 1170, 696, 669, 856, 992, 561, 916, 583, 843, 1020, 385, 296, 380, 355, 647, 280, 1029, 351, 909, 282, 615, 722, 826, 315, 704, 507, 1099, 100, 460, 157, 978, 1106, 551, 839, 942, 1162, 80, 1041, 494, 307, 758, 1139, 71, 851, 15, 1145, 99, 1198, 601, 665, 1185, 728, 614, 810, 491, 741, 104, 694, 832, 1094, 594, 961, 565, 547, 662, 1090, 701, 26, 506, 1110, 700, 539, 972, 58, 217, 215, 67, 963, 885, 1014, 73, 1010, 340, 854, 624, 72, 667, 1152, 1171, 255, 347, 20, 1001, 666, 109, 70, 652, 764, 545, 513, 501, 413, 272, 1201, 211, 220, 232, 112, 932, 1057, 325, 643, 734, 808, 675, 879, 126, 98, 1199, 847, 405, 17, 695, 1053, 613, 83, 189, 637, 243, 386, 857, 150, 257, 621, 175, 716, 247, 496, 801, 177, 252, 1174, 1067, 1081, 761, 419, 658, 833, 27, 1107, 619, 914, 353, 828, 265, 194, 119, 749, 400, 800, 763, 240, 760, 869, 118, 1112, 976, 374, 982, 1182, 1115, 379, 531, 1176, 35, 821, 485, 944, 368, 13, 773, 89, 994, 960, 929, 295, 521, 556, 410, 500, 170, 567, 224, 752, 597, 442, 768, 682, 221, 365, 456, 776, 454, 472, 29, 582, 486, 499, 143, 204, 0, 894, 580, 654, 1151, 1184, 241, 321, 644, 785, 165, 981, 188, 657, 291, 767, 1121, 274, 226, 996, 7, 1026, 515, 995, 1169, 715, 858, 174, 394, 1140, 46, 586, 498, 56, 1035, 1158, 1082, 1079, 523, 14, 782, 787, 412, 677, 434, 797, 674, 512, 1130, 757, 42, 730, 155, 181, 183, 698, 327, 277, 1143, 276, 246, 1004, 1002, 337, 318, 822, 167, 949, 239, 659, 1126, 893, 503, 1172, 449, 1211, 1066, 709, 697, 779, 590, 363, 235, 1060, 395, 707, 577, 517, 753, 133, 901, 187, 271, 969, 900, 848, 1091, 236, 968, 775, 875, 560, 180, 237, 148, 977, 651]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9006216755560088
the save name prefix for this run is:  chkpt-ID_9006216755560088_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 85
rank avg (pred): 0.588 +- 0.002
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003833148

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.445 +- 0.259
mrr vals (pred, true): 0.077, 0.000
batch losses (mrrl, rdl): 0.0, 4.9112e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 779
rank avg (pred): 0.478 +- 0.295
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0, 1.99345e-05

Epoch over!
epoch time: 14.952

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.482 +- 0.288
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0, 2.35269e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.451 +- 0.303
mrr vals (pred, true): 0.118, 0.000
batch losses (mrrl, rdl): 0.0, 2.40877e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 533
rank avg (pred): 0.376 +- 0.256
mrr vals (pred, true): 0.122, 0.090
batch losses (mrrl, rdl): 0.0, 2.47441e-05

Epoch over!
epoch time: 14.918

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 146
rank avg (pred): 0.449 +- 0.304
mrr vals (pred, true): 0.132, 0.000
batch losses (mrrl, rdl): 0.0, 1.01206e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 447
rank avg (pred): 0.473 +- 0.280
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0, 9.6532e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 857
rank avg (pred): 0.474 +- 0.290
mrr vals (pred, true): 0.102, 0.002
batch losses (mrrl, rdl): 0.0, 3.33163e-05

Epoch over!
epoch time: 14.716

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.438 +- 0.319
mrr vals (pred, true): 0.167, 0.000
batch losses (mrrl, rdl): 0.0, 2.5406e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.453 +- 0.284
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0, 2.12702e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.460 +- 0.290
mrr vals (pred, true): 0.110, 0.001
batch losses (mrrl, rdl): 0.0, 1.74701e-05

Epoch over!
epoch time: 14.762

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 433
rank avg (pred): 0.477 +- 0.282
mrr vals (pred, true): 0.097, 0.001
batch losses (mrrl, rdl): 0.0, 1.42785e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.471 +- 0.293
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0, 3.70475e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 934
rank avg (pred): 0.452 +- 0.298
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0, 5.38146e-05

Epoch over!
epoch time: 14.771

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.463 +- 0.294
mrr vals (pred, true): 0.079, 0.001
batch losses (mrrl, rdl): 0.0085134506, 1.96079e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 553
rank avg (pred): 0.456 +- 0.237
mrr vals (pred, true): 0.032, 0.092
batch losses (mrrl, rdl): 0.0033145479, 0.0002231176

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.546 +- 0.278
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0010229984, 3.82115e-05

Epoch over!
epoch time: 14.959

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.513 +- 0.237
mrr vals (pred, true): 0.035, 0.000
batch losses (mrrl, rdl): 0.002173889, 5.40038e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 738
rank avg (pred): 0.351 +- 0.190
mrr vals (pred, true): 0.084, 0.142
batch losses (mrrl, rdl): 0.0338165052, 0.0002359972

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.495 +- 0.237
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 7.8624e-06, 2.79956e-05

Epoch over!
epoch time: 14.95

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 841
rank avg (pred): 0.490 +- 0.238
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 5.574e-07, 2.8607e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.592 +- 0.200
mrr vals (pred, true): 0.037, 0.001
batch losses (mrrl, rdl): 0.0017290669, 0.00034789

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 590
rank avg (pred): 0.104 +- 0.060
mrr vals (pred, true): 0.076, 0.003
batch losses (mrrl, rdl): 0.0067988085, 0.0021696072

Epoch over!
epoch time: 14.976

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 263
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.212, 0.356
batch losses (mrrl, rdl): 0.2082815468, 0.0003182103

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.608 +- 0.243
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0010189293, 0.0004360254

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1113
rank avg (pred): 0.448 +- 0.264
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0014002502, 1.15742e-05

Epoch over!
epoch time: 15.014

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1107
rank avg (pred): 0.472 +- 0.238
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.86068e-05, 7.04159e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.543 +- 0.211
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 8.2948e-06, 8.4205e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 87
rank avg (pred): 0.532 +- 0.270
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001125528, 9.60107e-05

Epoch over!
epoch time: 14.953

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.464 +- 0.251
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003048468, 1.66615e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.174 +- 0.101
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0023502712, 0.0024327619

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 542
rank avg (pred): 0.487 +- 0.249
mrr vals (pred, true): 0.051, 0.097
batch losses (mrrl, rdl): 2.23461e-05, 0.0005313434

Epoch over!
epoch time: 14.953

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.542 +- 0.279
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004508592, 8.78488e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.224 +- 0.167
mrr vals (pred, true): 0.139, 0.129
batch losses (mrrl, rdl): 0.0010246305, 0.000285088

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.431 +- 0.213
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.23756e-05, 4.6315e-05

Epoch over!
epoch time: 14.935

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 759
rank avg (pred): 0.430 +- 0.213
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002234374, 0.0001572443

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.466 +- 0.198
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 8.9727e-06, 7.92595e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 52
rank avg (pred): 0.427 +- 0.202
mrr vals (pred, true): 0.052, 0.122
batch losses (mrrl, rdl): 0.0484513417, 0.0004155266

Epoch over!
epoch time: 14.938

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 556
rank avg (pred): 0.404 +- 0.194
mrr vals (pred, true): 0.053, 0.079
batch losses (mrrl, rdl): 0.0001048785, 6.83094e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 223
rank avg (pred): 0.498 +- 0.189
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.19853e-05, 5.61086e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.446 +- 0.216
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 6.08886e-05, 3.09393e-05

Epoch over!
epoch time: 14.942

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 657
rank avg (pred): 0.483 +- 0.201
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 9.574e-06, 5.0209e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 811
rank avg (pred): 0.365 +- 0.240
mrr vals (pred, true): 0.175, 0.309
batch losses (mrrl, rdl): 0.179542467, 0.0010711399

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 984
rank avg (pred): 0.320 +- 0.202
mrr vals (pred, true): 0.197, 0.135
batch losses (mrrl, rdl): 0.0382629037, 8.22934e-05

Epoch over!
epoch time: 14.931

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.192 +- 0.167
mrr vals (pred, true): 0.261, 0.360

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  112 	     0 	 0.14012 	 6e-0500 	 MISS
   20 	     1 	 0.04885 	 0.00014 	 m..s
  110 	     2 	 0.09211 	 0.00015 	 m..s
   34 	     3 	 0.04911 	 0.00015 	 m..s
   23 	     4 	 0.04886 	 0.00015 	 m..s
   29 	     5 	 0.04900 	 0.00016 	 m..s
   26 	     6 	 0.04889 	 0.00017 	 m..s
   66 	     7 	 0.04928 	 0.00017 	 m..s
   48 	     8 	 0.04928 	 0.00017 	 m..s
   85 	     9 	 0.05228 	 0.00018 	 m..s
    7 	    10 	 0.04879 	 0.00018 	 m..s
   48 	    11 	 0.04928 	 0.00018 	 m..s
   81 	    12 	 0.05125 	 0.00019 	 m..s
   35 	    13 	 0.04912 	 0.00019 	 m..s
   79 	    14 	 0.05041 	 0.00019 	 m..s
   75 	    15 	 0.04969 	 0.00019 	 m..s
   80 	    16 	 0.05122 	 0.00019 	 m..s
   25 	    17 	 0.04888 	 0.00020 	 m..s
   28 	    18 	 0.04892 	 0.00020 	 m..s
    3 	    19 	 0.04848 	 0.00020 	 m..s
   48 	    20 	 0.04928 	 0.00020 	 m..s
   22 	    21 	 0.04885 	 0.00020 	 m..s
   17 	    22 	 0.04885 	 0.00020 	 m..s
   48 	    23 	 0.04928 	 0.00021 	 m..s
   36 	    24 	 0.04913 	 0.00021 	 m..s
   38 	    25 	 0.04915 	 0.00021 	 m..s
   70 	    26 	 0.04951 	 0.00021 	 m..s
   93 	    27 	 0.05581 	 0.00021 	 m..s
   31 	    28 	 0.04910 	 0.00022 	 m..s
   48 	    29 	 0.04928 	 0.00022 	 m..s
    0 	    30 	 0.04789 	 0.00022 	 m..s
   74 	    31 	 0.04967 	 0.00023 	 m..s
   15 	    32 	 0.04882 	 0.00023 	 m..s
  101 	    33 	 0.06852 	 0.00023 	 m..s
    4 	    34 	 0.04872 	 0.00024 	 m..s
   68 	    35 	 0.04942 	 0.00024 	 m..s
   94 	    36 	 0.05589 	 0.00024 	 m..s
   48 	    37 	 0.04928 	 0.00025 	 m..s
   47 	    38 	 0.04928 	 0.00025 	 m..s
   67 	    39 	 0.04933 	 0.00025 	 m..s
   48 	    40 	 0.04928 	 0.00025 	 m..s
   19 	    41 	 0.04885 	 0.00026 	 m..s
   90 	    42 	 0.05358 	 0.00026 	 m..s
   12 	    43 	 0.04880 	 0.00027 	 m..s
   41 	    44 	 0.04919 	 0.00027 	 m..s
   76 	    45 	 0.04983 	 0.00027 	 m..s
   48 	    46 	 0.04928 	 0.00027 	 m..s
   14 	    47 	 0.04881 	 0.00028 	 m..s
   46 	    48 	 0.04928 	 0.00028 	 m..s
   40 	    49 	 0.04918 	 0.00028 	 m..s
   48 	    50 	 0.04928 	 0.00028 	 m..s
   69 	    51 	 0.04950 	 0.00029 	 m..s
   91 	    52 	 0.05360 	 0.00032 	 m..s
   10 	    53 	 0.04880 	 0.00032 	 m..s
   48 	    54 	 0.04928 	 0.00032 	 m..s
   73 	    55 	 0.04957 	 0.00032 	 m..s
   48 	    56 	 0.04928 	 0.00032 	 m..s
   88 	    57 	 0.05315 	 0.00033 	 m..s
   30 	    58 	 0.04906 	 0.00033 	 m..s
    2 	    59 	 0.04820 	 0.00034 	 m..s
    9 	    60 	 0.04880 	 0.00036 	 m..s
  102 	    61 	 0.06867 	 0.00036 	 m..s
   48 	    62 	 0.04928 	 0.00036 	 m..s
   37 	    63 	 0.04914 	 0.00038 	 m..s
  108 	    64 	 0.09002 	 0.00039 	 m..s
   81 	    65 	 0.05125 	 0.00039 	 m..s
   39 	    66 	 0.04917 	 0.00040 	 m..s
   16 	    67 	 0.04883 	 0.00041 	 m..s
   71 	    68 	 0.04954 	 0.00046 	 m..s
   87 	    69 	 0.05256 	 0.00048 	 m..s
    5 	    70 	 0.04872 	 0.00049 	 m..s
   24 	    71 	 0.04887 	 0.00050 	 m..s
   11 	    72 	 0.04880 	 0.00051 	 m..s
   21 	    73 	 0.04885 	 0.00051 	 m..s
   48 	    74 	 0.04928 	 0.00051 	 m..s
   43 	    75 	 0.04921 	 0.00053 	 m..s
   42 	    76 	 0.04920 	 0.00053 	 m..s
   99 	    77 	 0.06341 	 0.00054 	 m..s
    1 	    78 	 0.04800 	 0.00056 	 m..s
   13 	    79 	 0.04881 	 0.00057 	 m..s
    6 	    80 	 0.04873 	 0.00060 	 m..s
   72 	    81 	 0.04955 	 0.00062 	 m..s
    8 	    82 	 0.04879 	 0.00064 	 m..s
   32 	    83 	 0.04910 	 0.00074 	 m..s
   33 	    84 	 0.04910 	 0.00087 	 m..s
   45 	    85 	 0.04924 	 0.00093 	 m..s
   48 	    86 	 0.04928 	 0.00100 	 m..s
   18 	    87 	 0.04885 	 0.00137 	 m..s
   98 	    88 	 0.06187 	 0.00544 	 m..s
   27 	    89 	 0.04890 	 0.00835 	 m..s
   92 	    90 	 0.05557 	 0.00845 	 m..s
   44 	    91 	 0.04923 	 0.00890 	 m..s
  107 	    92 	 0.08921 	 0.02420 	 m..s
  119 	    93 	 0.24950 	 0.04430 	 MISS
  105 	    94 	 0.08226 	 0.04577 	 m..s
   48 	    95 	 0.04928 	 0.05135 	 ~...
   77 	    96 	 0.05026 	 0.06354 	 ~...
   97 	    97 	 0.05883 	 0.07402 	 ~...
   48 	    98 	 0.04928 	 0.07916 	 ~...
   84 	    99 	 0.05195 	 0.08131 	 ~...
   83 	   100 	 0.05173 	 0.08694 	 m..s
   48 	   101 	 0.04928 	 0.08729 	 m..s
   89 	   102 	 0.05329 	 0.08788 	 m..s
  106 	   103 	 0.08834 	 0.08908 	 ~...
   95 	   104 	 0.05615 	 0.09015 	 m..s
   86 	   105 	 0.05232 	 0.09564 	 m..s
   78 	   106 	 0.05032 	 0.09680 	 m..s
   48 	   107 	 0.04928 	 0.09858 	 m..s
  103 	   108 	 0.08038 	 0.10765 	 ~...
  114 	   109 	 0.16314 	 0.10768 	 m..s
  113 	   110 	 0.15843 	 0.11891 	 m..s
   96 	   111 	 0.05805 	 0.12088 	 m..s
  100 	   112 	 0.06642 	 0.12801 	 m..s
  104 	   113 	 0.08138 	 0.13102 	 m..s
  109 	   114 	 0.09178 	 0.13670 	 m..s
  111 	   115 	 0.13284 	 0.15510 	 ~...
  118 	   116 	 0.20663 	 0.17420 	 m..s
  116 	   117 	 0.19468 	 0.22246 	 ~...
  117 	   118 	 0.19569 	 0.25278 	 m..s
  115 	   119 	 0.17980 	 0.27877 	 m..s
  120 	   120 	 0.26106 	 0.35955 	 m..s
==========================================
r_mrr = 0.7508106231689453
r2_mrr = 0.23500043153762817
spearmanr_mrr@5 = 0.9031752347946167
spearmanr_mrr@10 = 0.9503097534179688
spearmanr_mrr@50 = 0.9385789632797241
spearmanr_mrr@100 = 0.9469584822654724
spearmanr_mrr@All = 0.9484615325927734
==========================================
test time: 0.454
Done Testing dataset DBpedia50
total time taken: 229.58393812179565
training time taken: 224.1323947906494
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.7508)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.2350)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9032)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9503)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9386)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9470)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9485)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.0159022905299935}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 8817225249415981
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [161, 652, 173, 1056, 1201, 314, 832, 996, 138, 784, 79, 845, 523, 443, 991, 1029, 1195, 429, 574, 400, 1093, 497, 422, 992, 631, 880, 467, 1014, 872, 979, 968, 642, 1059, 1178, 888, 109, 344, 938, 1209, 1140, 943, 415, 302, 685, 1210, 1211, 77, 883, 1185, 794, 167, 636, 150, 827, 260, 767, 183, 772, 263, 1196, 780, 17, 902, 1146, 1062, 462, 729, 629, 1025, 549, 106, 112, 180, 802, 848, 757, 325, 997, 446, 281, 1081, 1067, 825, 507, 160, 187, 399, 635, 1179, 1212, 364, 812, 663, 411, 605, 884, 60, 124, 897, 1006, 360, 756, 1145, 492, 486, 732, 1085, 701, 604, 191, 395, 807, 569, 1105, 499, 433, 834, 117, 268, 864, 847]
valid_ids (0): []
train_ids (1094): [894, 148, 632, 954, 1053, 105, 1205, 46, 1020, 749, 207, 623, 867, 939, 1176, 722, 73, 913, 937, 509, 273, 692, 127, 1090, 179, 21, 978, 837, 555, 495, 898, 1134, 971, 45, 822, 252, 511, 1169, 1151, 831, 266, 1009, 149, 1018, 447, 1136, 93, 798, 655, 739, 724, 985, 649, 505, 582, 346, 1167, 1017, 1075, 571, 377, 34, 809, 485, 836, 74, 1064, 1011, 542, 348, 965, 489, 1164, 192, 496, 609, 349, 977, 792, 1034, 391, 607, 1104, 50, 493, 861, 1102, 689, 806, 826, 398, 987, 471, 241, 1197, 638, 1092, 1086, 1010, 853, 472, 515, 1168, 345, 1155, 423, 242, 257, 615, 855, 564, 915, 775, 830, 730, 1036, 139, 146, 1070, 580, 76, 1199, 553, 1202, 125, 208, 1039, 4, 95, 860, 375, 768, 551, 821, 70, 567, 682, 1181, 63, 1173, 116, 779, 370, 1040, 115, 586, 336, 514, 357, 30, 49, 935, 251, 300, 189, 25, 218, 870, 829, 53, 1148, 1157, 1082, 926, 673, 328, 601, 487, 243, 481, 1061, 843, 3, 587, 64, 714, 740, 1016, 1200, 442, 734, 1187, 548, 1072, 174, 1193, 1182, 1007, 285, 537, 312, 478, 665, 760, 96, 184, 130, 136, 380, 534, 698, 1089, 246, 32, 288, 828, 428, 256, 340, 190, 810, 52, 742, 1186, 1172, 330, 650, 910, 612, 1131, 1125, 129, 236, 134, 62, 568, 132, 1084, 535, 522, 808, 240, 316, 451, 85, 172, 37, 185, 875, 570, 944, 1055, 128, 824, 221, 122, 547, 407, 223, 1088, 157, 466, 710, 378, 1139, 1019, 437, 811, 877, 168, 691, 613, 849, 833, 332, 813, 386, 584, 277, 1129, 627, 475, 700, 92, 750, 169, 1076, 929, 480, 1153, 239, 624, 1184, 284, 1191, 986, 1027, 512, 800, 600, 65, 166, 674, 859, 653, 726, 1124, 751, 140, 483, 137, 804, 508, 75, 66, 1094, 389, 118, 554, 656, 404, 31, 372, 1174, 980, 660, 541, 200, 215, 470, 626, 602, 871, 502, 900, 787, 575, 367, 408, 941, 394, 320, 983, 677, 1203, 126, 886, 783, 1012, 182, 1095, 297, 453, 1120, 560, 538, 101, 181, 588, 625, 435, 863, 773, 752, 29, 697, 317, 896, 339, 86, 1043, 718, 572, 1047, 865, 606, 211, 721, 371, 1050, 1154, 479, 41, 102, 222, 620, 272, 696, 556, 1180, 313, 1096, 755, 1114, 550, 1115, 577, 999, 158, 1065, 20, 425, 392, 895, 97, 924, 513, 163, 1194, 662, 347, 283, 338, 225, 956, 282, 488, 646, 1099, 230, 846, 100, 664, 790, 321, 889, 322, 503, 258, 290, 579, 921, 374, 786, 264, 562, 1177, 852, 279, 679, 934, 213, 634, 110, 1198, 177, 269, 705, 7, 1005, 528, 795, 817, 1189, 153, 706, 342, 1058, 789, 869, 412, 1004, 899, 68, 477, 753, 839, 1204, 707, 9, 71, 456, 526, 231, 165, 510, 1206, 1038, 51, 48, 844, 401, 928, 254, 585, 13, 310, 33, 379, 227, 1071, 1112, 737, 1190, 265, 91, 741, 362, 1, 1126, 617, 1080, 57, 527, 891, 1144, 44, 459, 976, 1117, 142, 666, 936, 5, 524, 686, 1165, 879, 595, 10, 1111, 94, 969, 1214, 881, 123, 276, 205, 640, 301, 1107, 1057, 69, 908, 176, 1028, 350, 1158, 280, 1054, 120, 1013, 680, 621, 1091, 893, 611, 820, 641, 530, 521, 6, 1128, 858, 838, 1101, 1188, 684, 464, 616, 14, 1152, 558, 43, 566, 363, 1192, 973, 444, 657, 368, 121, 520, 933, 426, 953, 1078, 501, 335, 195, 396, 274, 1031, 922, 206, 341, 962, 430, 669, 704, 287, 993, 1023, 785, 1132, 295, 862, 402, 355, 418, 197, 1097, 723, 591, 1063, 1073, 99, 961, 1123, 463, 630, 659, 393, 536, 690, 498, 383, 907, 289, 532, 544, 175, 735, 914, 133, 298, 857, 376, 543, 354, 919, 1116, 1147, 226, 781, 959, 436, 967, 648, 525, 958, 531, 713, 1024, 319, 856, 381, 271, 416, 628, 709, 193, 352, 1069, 59, 717, 80, 963, 186, 388, 1100, 1208, 876, 54, 334, 927, 1142, 818, 942, 1160, 770, 874, 597, 61, 1106, 590, 1098, 947, 683, 397, 619, 1026, 776, 940, 712, 1213, 108, 318, 27, 250, 930, 596, 454, 716, 974, 949, 103, 356, 589, 687, 610, 82, 155, 309, 439, 676, 1046, 410, 851, 1161, 233, 711, 299, 162, 545, 307, 22, 291, 819, 141, 736, 835, 728, 988, 373, 461, 18, 998, 733, 214, 267, 984, 743, 643, 529, 1022, 406, 671, 23, 762, 111, 970, 678, 1051, 196, 28, 131, 1207, 905, 667, 758, 946, 668, 995, 468, 194, 306, 892, 911, 912, 906, 925, 1127, 592, 622, 720, 235, 769, 778, 1037, 1162, 761, 1138, 972, 405, 114, 490, 255, 1002, 552, 931, 15, 78, 143, 159, 637, 916, 245, 576, 26, 1159, 1156, 188, 981, 540, 771, 315, 982, 202, 854, 670, 727, 932, 1183, 945, 199, 294, 278, 702, 286, 441, 403, 901, 209, 1049, 1166, 815, 438, 661, 504, 305, 1103, 694, 1032, 87, 1030, 343, 11, 1044, 329, 293, 39, 164, 573, 296, 765, 469, 1052, 887, 1143, 764, 217, 675, 337, 1000, 868, 873, 427, 238, 744, 420, 639, 850, 1066, 658, 842, 955, 67, 424, 12, 866, 960, 715, 1118, 840, 58, 878, 517, 465, 473, 19, 951, 719, 681, 903, 748, 904, 262, 688, 1083, 647, 382, 210, 458, 559, 449, 1035, 608, 358, 593, 390, 234, 457, 38, 793, 1045, 693, 56, 308, 1068, 506, 645, 326, 156, 353, 249, 738, 259, 1133, 434, 331, 1041, 365, 203, 1021, 224, 212, 90, 89, 651, 2, 699, 654, 40, 598, 384, 614, 703, 731, 445, 603, 474, 782, 759, 918, 557, 324, 460, 801, 323, 500, 516, 599, 746, 747, 816, 1110, 275, 948, 237, 909, 1163, 539, 989, 644, 494, 232, 450, 154, 1015, 882, 1171, 533, 303, 885, 1130, 333, 1121, 431, 229, 578, 952, 359, 24, 482, 35, 563, 151, 990, 796, 1150, 432, 994, 565, 1137, 519, 920, 1001, 1077, 1033, 204, 766, 917, 448, 745, 1074, 228, 763, 244, 366, 248, 1060, 419, 369, 413, 198, 618, 561, 220, 81, 594, 440, 219, 178, 107, 361, 421, 774, 135, 546, 814, 1109, 201, 113, 387, 261, 788, 964, 119, 581, 1122, 409, 152, 144, 311, 216, 754, 16, 84, 1079, 0, 247, 1108, 170, 452, 476, 890, 1149, 484, 292, 8, 491, 47, 1170, 823, 83, 327, 777, 1042, 104, 88, 708, 1003, 1113, 803, 1135, 1141, 791, 841, 672, 633, 72, 414, 923, 417, 583, 797, 36, 799, 1087, 966, 1175, 253, 1048, 957, 42, 351, 950, 385, 975, 1119, 98, 518, 805, 145, 147, 171, 455, 55, 270, 1008, 725, 304, 695]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3863470700813678
the save name prefix for this run is:  chkpt-ID_3863470700813678_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.554 +- 0.007
mrr vals (pred, true): 0.000, 0.070
batch losses (mrrl, rdl): 0.0, 0.0004604789

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.255 +- 0.186
mrr vals (pred, true): 0.141, 0.181
batch losses (mrrl, rdl): 0.0, 8.76586e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.456 +- 0.303
mrr vals (pred, true): 0.095, 0.001
batch losses (mrrl, rdl): 0.0, 2.68459e-05

Epoch over!
epoch time: 14.986

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 736
rank avg (pred): 0.319 +- 0.235
mrr vals (pred, true): 0.130, 0.211
batch losses (mrrl, rdl): 0.0, 0.0008089507

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.332 +- 0.255
mrr vals (pred, true): 0.123, 0.101
batch losses (mrrl, rdl): 0.0, 4.52278e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.488 +- 0.298
mrr vals (pred, true): 0.107, 0.000
batch losses (mrrl, rdl): 0.0, 9.5812e-06

Epoch over!
epoch time: 14.942

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 476
rank avg (pred): 0.445 +- 0.305
mrr vals (pred, true): 0.127, 0.000
batch losses (mrrl, rdl): 0.0, 2.16005e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 113
rank avg (pred): 0.437 +- 0.310
mrr vals (pred, true): 0.130, 0.000
batch losses (mrrl, rdl): 0.0, 1.15407e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 331
rank avg (pred): 0.443 +- 0.297
mrr vals (pred, true): 0.101, 0.001
batch losses (mrrl, rdl): 0.0, 2.80107e-05

Epoch over!
epoch time: 14.853

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 94
rank avg (pred): 0.488 +- 0.290
mrr vals (pred, true): 0.078, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001146756

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 935
rank avg (pred): 0.475 +- 0.289
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0, 2.18858e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 201
rank avg (pred): 0.447 +- 0.288
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0, 5.7275e-06

Epoch over!
epoch time: 14.791

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.468 +- 0.298
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0, 3.1629e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.481 +- 0.283
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0, 5.0798e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 479
rank avg (pred): 0.484 +- 0.289
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0, 3.51409e-05

Epoch over!
epoch time: 14.974

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.503 +- 0.288
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.80802e-05, 2.19253e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 688
rank avg (pred): 0.504 +- 0.176
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0009954888, 0.0001129948

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.479 +- 0.154
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0011346736, 5.64188e-05

Epoch over!
epoch time: 15.151

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.482 +- 0.163
mrr vals (pred, true): 0.040, 0.002
batch losses (mrrl, rdl): 0.0010906493, 4.88078e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 818
rank avg (pred): 0.219 +- 0.363
mrr vals (pred, true): 0.156, 0.044
batch losses (mrrl, rdl): 0.1124058589, 3.86668e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 19
rank avg (pred): 0.248 +- 0.353
mrr vals (pred, true): 0.165, 0.000
batch losses (mrrl, rdl): 0.1323833764, 0.0007371043

Epoch over!
epoch time: 15.127

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.449 +- 0.162
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.000136763, 0.0001218518

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 98
rank avg (pred): 0.517 +- 0.256
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4.24e-08, 8.28035e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 683
rank avg (pred): 0.478 +- 0.226
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002828751, 6.84261e-05

Epoch over!
epoch time: 15.133

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 455
rank avg (pred): 0.457 +- 0.222
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.14604e-05, 2.82106e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1156
rank avg (pred): 0.378 +- 0.336
mrr vals (pred, true): 0.115, 0.110
batch losses (mrrl, rdl): 0.0002704851, 3.16704e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 947
rank avg (pred): 0.512 +- 0.281
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.09708e-05, 1.09797e-05

Epoch over!
epoch time: 15.113

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.280 +- 0.408
mrr vals (pred, true): 0.157, 0.253
batch losses (mrrl, rdl): 0.0908794254, 0.000258893

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.286 +- 0.388
mrr vals (pred, true): 0.147, 0.261
batch losses (mrrl, rdl): 0.1310404986, 7.4592e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.318 +- 0.345
mrr vals (pred, true): 0.145, 0.001
batch losses (mrrl, rdl): 0.090324536, 0.0010806122

Epoch over!
epoch time: 15.13

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.501 +- 0.246
mrr vals (pred, true): 0.037, 0.000
batch losses (mrrl, rdl): 0.001605338, 1.7557e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.423 +- 0.191
mrr vals (pred, true): 0.067, 0.090
batch losses (mrrl, rdl): 0.0028930081, 0.0002676122

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.305 +- 0.355
mrr vals (pred, true): 0.154, 0.001
batch losses (mrrl, rdl): 0.1079005823, 0.001203611

Epoch over!
epoch time: 15.094

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.355 +- 0.320
mrr vals (pred, true): 0.141, 0.000
batch losses (mrrl, rdl): 0.0823914558, 0.0003905768

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.529 +- 0.292
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0001010663, 5.9346e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.447 +- 0.213
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002659059, 3.20454e-05

Epoch over!
epoch time: 15.074

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 643
rank avg (pred): 0.476 +- 0.262
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 6.9341e-06, 4.82544e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 627
rank avg (pred): 0.468 +- 0.232
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003177692, 3.46107e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 287
rank avg (pred): 0.420 +- 0.200
mrr vals (pred, true): 0.069, 0.085
batch losses (mrrl, rdl): 0.003529178, 0.0004542982

Epoch over!
epoch time: 15.083

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.334 +- 0.357
mrr vals (pred, true): 0.148, 0.059
batch losses (mrrl, rdl): 0.0960649848, 0.0001272067

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 543
rank avg (pred): 0.426 +- 0.207
mrr vals (pred, true): 0.072, 0.088
batch losses (mrrl, rdl): 0.0049341242, 0.000147109

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.358 +- 0.358
mrr vals (pred, true): 0.152, 0.350
batch losses (mrrl, rdl): 0.3913602531, 0.0009087342

Epoch over!
epoch time: 14.961

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 249
rank avg (pred): 0.357 +- 0.317
mrr vals (pred, true): 0.135, 0.094
batch losses (mrrl, rdl): 0.071883142, 4.23002e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.334 +- 0.341
mrr vals (pred, true): 0.150, 0.186
batch losses (mrrl, rdl): 0.0132243186, 0.0001910073

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 129
rank avg (pred): 0.471 +- 0.263
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0011288178, 8.61933e-05

Epoch over!
epoch time: 15.023

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.475 +- 0.262
mrr vals (pred, true): 0.051, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  109 	     0 	 0.17350 	 6e-0500 	 MISS
   56 	     1 	 0.06470 	 0.00014 	 m..s
    2 	     2 	 0.04982 	 0.00014 	 m..s
   48 	     3 	 0.06320 	 0.00015 	 m..s
   29 	     4 	 0.05490 	 0.00015 	 m..s
   62 	     5 	 0.06681 	 0.00016 	 m..s
   26 	     6 	 0.05326 	 0.00017 	 m..s
    5 	     7 	 0.05009 	 0.00017 	 m..s
   53 	     8 	 0.06417 	 0.00017 	 m..s
   58 	     9 	 0.06562 	 0.00017 	 m..s
   57 	    10 	 0.06537 	 0.00017 	 m..s
   50 	    11 	 0.06381 	 0.00018 	 m..s
   46 	    12 	 0.06273 	 0.00019 	 m..s
   13 	    13 	 0.05084 	 0.00019 	 m..s
   64 	    14 	 0.06703 	 0.00020 	 m..s
    8 	    15 	 0.05037 	 0.00020 	 m..s
   70 	    16 	 0.06916 	 0.00020 	 m..s
   77 	    17 	 0.07027 	 0.00021 	 m..s
   38 	    18 	 0.05926 	 0.00021 	 m..s
   55 	    19 	 0.06441 	 0.00021 	 m..s
    4 	    20 	 0.05006 	 0.00021 	 m..s
   62 	    21 	 0.06681 	 0.00021 	 m..s
   21 	    22 	 0.05248 	 0.00021 	 m..s
   52 	    23 	 0.06413 	 0.00022 	 m..s
   18 	    24 	 0.05195 	 0.00022 	 m..s
   80 	    25 	 0.07117 	 0.00022 	 m..s
   31 	    26 	 0.05661 	 0.00022 	 m..s
   42 	    27 	 0.06074 	 0.00022 	 m..s
   23 	    28 	 0.05287 	 0.00023 	 m..s
   20 	    29 	 0.05245 	 0.00023 	 m..s
   25 	    30 	 0.05312 	 0.00023 	 m..s
   60 	    31 	 0.06613 	 0.00023 	 m..s
   17 	    32 	 0.05190 	 0.00023 	 m..s
   11 	    33 	 0.05056 	 0.00024 	 m..s
   67 	    34 	 0.06842 	 0.00024 	 m..s
   68 	    35 	 0.06869 	 0.00024 	 m..s
    6 	    36 	 0.05022 	 0.00024 	 m..s
    7 	    37 	 0.05022 	 0.00025 	 m..s
   45 	    38 	 0.06225 	 0.00025 	 m..s
   99 	    39 	 0.14757 	 0.00025 	 MISS
   72 	    40 	 0.06942 	 0.00026 	 m..s
   66 	    41 	 0.06790 	 0.00026 	 m..s
   35 	    42 	 0.05875 	 0.00027 	 m..s
   73 	    43 	 0.06998 	 0.00028 	 m..s
   80 	    44 	 0.07117 	 0.00028 	 m..s
   49 	    45 	 0.06334 	 0.00029 	 m..s
   44 	    46 	 0.06186 	 0.00029 	 m..s
   10 	    47 	 0.05049 	 0.00029 	 m..s
   82 	    48 	 0.07180 	 0.00030 	 m..s
   14 	    49 	 0.05109 	 0.00030 	 m..s
   54 	    50 	 0.06421 	 0.00031 	 m..s
   26 	    51 	 0.05326 	 0.00031 	 m..s
   33 	    52 	 0.05767 	 0.00031 	 m..s
   12 	    53 	 0.05073 	 0.00032 	 m..s
   22 	    54 	 0.05280 	 0.00032 	 m..s
   59 	    55 	 0.06585 	 0.00032 	 m..s
   47 	    56 	 0.06281 	 0.00033 	 m..s
   32 	    57 	 0.05761 	 0.00033 	 m..s
   29 	    58 	 0.05490 	 0.00034 	 m..s
   34 	    59 	 0.05858 	 0.00034 	 m..s
   43 	    60 	 0.06117 	 0.00035 	 m..s
   76 	    61 	 0.07024 	 0.00036 	 m..s
   39 	    62 	 0.06025 	 0.00036 	 m..s
    1 	    63 	 0.04958 	 0.00037 	 m..s
   24 	    64 	 0.05299 	 0.00038 	 m..s
   61 	    65 	 0.06618 	 0.00041 	 m..s
   51 	    66 	 0.06387 	 0.00041 	 m..s
   41 	    67 	 0.06039 	 0.00044 	 m..s
   74 	    68 	 0.07017 	 0.00046 	 m..s
    3 	    69 	 0.04983 	 0.00049 	 m..s
   78 	    70 	 0.07105 	 0.00049 	 m..s
    0 	    71 	 0.04937 	 0.00050 	 m..s
    9 	    72 	 0.05049 	 0.00050 	 m..s
   16 	    73 	 0.05165 	 0.00050 	 m..s
   15 	    74 	 0.05138 	 0.00054 	 m..s
   37 	    75 	 0.05909 	 0.00055 	 m..s
   70 	    76 	 0.06916 	 0.00055 	 m..s
   19 	    77 	 0.05223 	 0.00056 	 m..s
   39 	    78 	 0.06025 	 0.00061 	 m..s
   69 	    79 	 0.06906 	 0.00066 	 m..s
   36 	    80 	 0.05893 	 0.00075 	 m..s
   65 	    81 	 0.06771 	 0.00075 	 m..s
   75 	    82 	 0.07020 	 0.00093 	 m..s
   79 	    83 	 0.07111 	 0.00143 	 m..s
  104 	    84 	 0.15314 	 0.00544 	 MISS
   28 	    85 	 0.05427 	 0.00835 	 m..s
   90 	    86 	 0.08893 	 0.07028 	 ~...
   92 	    87 	 0.10135 	 0.07237 	 ~...
   84 	    88 	 0.07613 	 0.07272 	 ~...
   98 	    89 	 0.14338 	 0.07910 	 m..s
   93 	    90 	 0.10888 	 0.08220 	 ~...
   86 	    91 	 0.07752 	 0.08701 	 ~...
   89 	    92 	 0.08521 	 0.09120 	 ~...
   94 	    93 	 0.10929 	 0.09167 	 ~...
   97 	    94 	 0.13948 	 0.09246 	 m..s
   88 	    95 	 0.07784 	 0.09366 	 ~...
   91 	    96 	 0.09937 	 0.09573 	 ~...
   83 	    97 	 0.07610 	 0.09887 	 ~...
   87 	    98 	 0.07776 	 0.10042 	 ~...
   85 	    99 	 0.07724 	 0.10764 	 m..s
   95 	   100 	 0.11584 	 0.10821 	 ~...
  108 	   101 	 0.16591 	 0.12624 	 m..s
  101 	   102 	 0.14887 	 0.12975 	 ~...
   96 	   103 	 0.12543 	 0.13328 	 ~...
  110 	   104 	 0.17479 	 0.13971 	 m..s
  102 	   105 	 0.15111 	 0.14970 	 ~...
  107 	   106 	 0.16423 	 0.16506 	 ~...
  103 	   107 	 0.15139 	 0.17420 	 ~...
  120 	   108 	 0.23426 	 0.18675 	 m..s
  118 	   109 	 0.23216 	 0.19254 	 m..s
  118 	   110 	 0.23216 	 0.19401 	 m..s
  106 	   111 	 0.15841 	 0.23873 	 m..s
  114 	   112 	 0.19274 	 0.25491 	 m..s
  115 	   113 	 0.19317 	 0.26108 	 m..s
  111 	   114 	 0.17622 	 0.29022 	 MISS
  113 	   115 	 0.19187 	 0.29601 	 MISS
   99 	   116 	 0.14757 	 0.31471 	 MISS
  105 	   117 	 0.15421 	 0.33648 	 MISS
  116 	   118 	 0.20348 	 0.33898 	 MISS
  112 	   119 	 0.17774 	 0.35643 	 MISS
  117 	   120 	 0.20833 	 0.38067 	 MISS
==========================================
r_mrr = 0.8459336161613464
r2_mrr = 0.449853777885437
spearmanr_mrr@5 = 0.7971466779708862
spearmanr_mrr@10 = 0.9453033208847046
spearmanr_mrr@50 = 0.9638527631759644
spearmanr_mrr@100 = 0.9731753468513489
spearmanr_mrr@All = 0.9702494144439697
==========================================
test time: 0.448
Done Testing dataset DBpedia50
total time taken: 231.16220903396606
training time taken: 225.89396834373474
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8459)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4499)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.7971)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9453)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9639)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9732)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9702)}}, 'test_loss': {'ComplEx': {'DBpedia50': 2.74507422127499}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 1928397725351642
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [605, 611, 395, 444, 198, 1066, 927, 620, 182, 130, 570, 493, 954, 777, 1070, 699, 62, 664, 291, 1139, 246, 345, 505, 456, 1009, 808, 735, 740, 795, 635, 1015, 308, 2, 815, 972, 970, 663, 916, 579, 950, 303, 170, 276, 797, 1098, 945, 754, 806, 1050, 44, 534, 765, 1182, 37, 233, 968, 1210, 903, 452, 118, 172, 39, 1202, 573, 1021, 1102, 1023, 84, 714, 459, 547, 898, 1200, 421, 786, 1141, 289, 1147, 528, 1165, 1175, 935, 235, 1006, 501, 618, 438, 256, 1159, 624, 396, 461, 778, 789, 128, 1053, 5, 532, 382, 918, 403, 1164, 4, 897, 828, 856, 522, 716, 650, 1160, 14, 725, 728, 937, 915, 210, 830, 794, 173, 1113, 906]
valid_ids (0): []
train_ids (1094): [388, 774, 283, 77, 65, 689, 428, 484, 30, 1004, 615, 437, 34, 15, 324, 802, 567, 639, 378, 1156, 1157, 871, 991, 106, 647, 1135, 224, 900, 366, 936, 634, 1114, 1074, 318, 1137, 599, 361, 446, 263, 1196, 642, 422, 0, 1012, 841, 1034, 691, 178, 372, 367, 304, 538, 632, 28, 578, 562, 253, 1091, 231, 593, 307, 869, 189, 25, 383, 1024, 1128, 81, 631, 103, 751, 454, 807, 341, 1010, 498, 472, 317, 107, 633, 78, 843, 328, 64, 677, 196, 957, 833, 524, 286, 840, 545, 842, 537, 565, 944, 949, 45, 1038, 379, 810, 158, 169, 893, 1062, 924, 120, 697, 575, 1203, 1105, 503, 790, 1127, 696, 596, 1185, 1179, 448, 1011, 70, 679, 550, 1000, 419, 997, 526, 203, 1151, 912, 803, 29, 159, 832, 250, 992, 595, 115, 658, 239, 274, 877, 582, 380, 1187, 137, 717, 184, 836, 544, 1126, 127, 306, 354, 260, 757, 373, 813, 1209, 652, 878, 743, 363, 226, 564, 694, 1018, 1168, 192, 775, 365, 59, 755, 666, 53, 1108, 207, 1145, 55, 408, 1041, 301, 800, 951, 335, 353, 1104, 48, 1194, 637, 710, 1, 988, 171, 102, 441, 1110, 1022, 932, 216, 707, 58, 749, 1090, 721, 905, 453, 124, 1030, 586, 236, 901, 101, 996, 278, 331, 433, 385, 202, 649, 215, 38, 180, 1013, 393, 571, 109, 657, 1111, 1086, 72, 1020, 265, 623, 139, 1028, 870, 1082, 121, 431, 469, 722, 931, 926, 849, 352, 763, 894, 166, 476, 1097, 561, 69, 746, 907, 730, 928, 629, 1014, 850, 958, 960, 425, 54, 1036, 733, 726, 232, 976, 117, 684, 1027, 1155, 1103, 1214, 785, 105, 160, 351, 113, 26, 218, 520, 356, 164, 1007, 298, 1071, 183, 13, 1150, 872, 370, 739, 93, 793, 132, 126, 280, 964, 560, 16, 1084, 685, 781, 851, 1115, 1047, 585, 320, 604, 32, 859, 555, 475, 779, 332, 762, 252, 829, 219, 1096, 626, 470, 259, 947, 138, 262, 85, 756, 973, 603, 440, 266, 863, 540, 704, 668, 1199, 917, 497, 49, 899, 411, 277, 720, 742, 12, 1109, 548, 831, 701, 1055, 412, 142, 816, 398, 1146, 330, 375, 400, 1019, 199, 427, 804, 457, 546, 750, 188, 434, 921, 168, 97, 1046, 934, 153, 1032, 536, 284, 981, 110, 129, 1143, 1033, 297, 323, 1189, 513, 712, 402, 614, 598, 641, 930, 695, 211, 9, 237, 868, 875, 819, 350, 1134, 977, 1044, 682, 788, 442, 381, 698, 702, 1180, 409, 768, 1026, 212, 247, 860, 688, 541, 100, 884, 507, 709, 342, 1121, 622, 116, 499, 1017, 264, 11, 1130, 334, 1054, 66, 348, 52, 206, 74, 591, 47, 881, 1186, 150, 435, 602, 50, 776, 673, 193, 176, 1077, 390, 998, 1058, 509, 825, 1089, 1129, 812, 413, 465, 485, 389, 764, 1008, 423, 339, 123, 769, 619, 386, 606, 636, 455, 848, 952, 693, 601, 1193, 282, 1174, 3, 194, 191, 919, 61, 1191, 759, 99, 261, 943, 374, 993, 805, 494, 886, 576, 512, 240, 122, 466, 295, 1136, 731, 271, 18, 681, 293, 204, 149, 683, 834, 533, 1076, 521, 478, 686, 157, 787, 299, 473, 1072, 1079, 177, 706, 296, 847, 854, 1125, 975, 167, 890, 858, 165, 19, 60, 867, 1120, 185, 529, 94, 21, 445, 515, 782, 760, 201, 140, 96, 510, 135, 145, 978, 111, 747, 391, 417, 6, 86, 221, 338, 316, 865, 922, 824, 144, 588, 79, 443, 486, 880, 1178, 1201, 346, 674, 488, 439, 814, 753, 963, 1016, 1162, 518, 1207, 700, 772, 312, 883, 22, 1132, 305, 217, 965, 429, 40, 1075, 715, 979, 451, 962, 559, 961, 337, 783, 967, 392, 527, 1052, 502, 359, 1064, 162, 1059, 36, 827, 220, 490, 516, 430, 197, 941, 826, 584, 517, 761, 57, 630, 82, 1063, 251, 290, 670, 1197, 131, 895, 112, 556, 321, 450, 287, 1152, 344, 566, 1176, 1173, 491, 597, 1094, 986, 1085, 617, 577, 468, 24, 1212, 73, 410, 608, 729, 1095, 1065, 909, 672, 1166, 891, 773, 969, 487, 1068, 496, 267, 908, 1040, 1087, 879, 852, 643, 1213, 724, 424, 329, 671, 294, 362, 799, 432, 553, 325, 416, 270, 770, 938, 771, 889, 460, 1069, 1093, 874, 8, 855, 920, 156, 405, 594, 42, 758, 718, 543, 17, 1119, 35, 1045, 1100, 948, 1154, 662, 1123, 242, 228, 1122, 1167, 589, 63, 200, 745, 551, 939, 268, 360, 143, 656, 243, 492, 911, 857, 713, 358, 680, 414, 811, 736, 539, 92, 554, 690, 195, 798, 474, 214, 1083, 241, 613, 349, 319, 397, 853, 581, 477, 896, 481, 1158, 1170, 51, 1140, 1092, 272, 355, 792, 1148, 91, 519, 309, 667, 796, 181, 552, 213, 1043, 904, 186, 1124, 862, 1177, 467, 480, 506, 1190, 244, 1039, 801, 269, 114, 141, 719, 966, 33, 959, 839, 1029, 147, 987, 845, 844, 942, 946, 1035, 511, 655, 347, 404, 654, 27, 542, 646, 1060, 67, 653, 401, 310, 1107, 530, 563, 675, 384, 371, 767, 187, 1106, 300, 87, 7, 95, 1208, 621, 227, 583, 10, 676, 665, 504, 648, 245, 910, 837, 659, 368, 549, 31, 1211, 766, 154, 580, 523, 1116, 953, 41, 489, 846, 175, 956, 415, 1037, 835, 343, 1169, 1131, 406, 1056, 1138, 327, 645, 933, 133, 1206, 155, 887, 151, 902, 279, 1003, 376, 1183, 273, 995, 458, 651, 669, 838, 1101, 885, 23, 88, 678, 134, 861, 607, 703, 610, 1118, 738, 394, 1031, 1005, 90, 557, 531, 612, 660, 464, 1195, 1112, 1198, 238, 482, 864, 587, 230, 322, 146, 125, 982, 1048, 479, 734, 436, 708, 377, 98, 1081, 1001, 462, 336, 821, 744, 302, 687, 1142, 1149, 1002, 1192, 1153, 311, 223, 999, 985, 254, 179, 940, 46, 357, 640, 984, 20, 1163, 292, 471, 208, 340, 644, 866, 387, 705, 1067, 463, 1188, 369, 1133, 791, 1051, 447, 990, 108, 873, 508, 818, 809, 315, 723, 89, 711, 1184, 1057, 913, 229, 222, 1061, 234, 525, 600, 625, 737, 980, 971, 732, 314, 255, 616, 882, 119, 820, 68, 418, 592, 1049, 495, 275, 281, 257, 1088, 152, 1204, 407, 249, 258, 75, 1080, 627, 288, 364, 449, 994, 1205, 569, 568, 780, 558, 161, 190, 326, 974, 748, 483, 399, 1171, 1144, 56, 609, 955, 163, 285, 876, 174, 817, 1073, 225, 1117, 43, 923, 1042, 1099, 1161, 929, 822, 572, 426, 248, 83, 205, 661, 692, 741, 80, 104, 1025, 784, 333, 209, 313, 76, 914, 574, 752, 500, 823, 628, 1078, 1172, 925, 148, 590, 1181, 71, 888, 638, 136, 727, 514, 420, 535, 989, 983, 892]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9078661167181808
the save name prefix for this run is:  chkpt-ID_9078661167181808_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.484 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.45933e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.420 +- 0.018
mrr vals (pred, true): 0.000, 0.239
batch losses (mrrl, rdl): 0.0, 0.0010627859

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.439 +- 0.182
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002837479

Epoch over!
epoch time: 14.908

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 919
rank avg (pred): 0.412 +- 0.220
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0, 5.05624e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.393 +- 0.282
mrr vals (pred, true): 0.178, 0.000
batch losses (mrrl, rdl): 0.0, 8.41695e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1127
rank avg (pred): 0.389 +- 0.297
mrr vals (pred, true): 0.224, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001490469

Epoch over!
epoch time: 14.96

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.377 +- 0.287
mrr vals (pred, true): 0.221, 0.059
batch losses (mrrl, rdl): 0.0, 0.0005820208

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.425 +- 0.310
mrr vals (pred, true): 0.185, 0.000
batch losses (mrrl, rdl): 0.0, 8.9144e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 422
rank avg (pred): 0.397 +- 0.296
mrr vals (pred, true): 0.203, 0.000
batch losses (mrrl, rdl): 0.0, 7.90974e-05

Epoch over!
epoch time: 14.832

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 105
rank avg (pred): 0.432 +- 0.297
mrr vals (pred, true): 0.157, 0.000
batch losses (mrrl, rdl): 0.0, 9.8546e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 775
rank avg (pred): 0.377 +- 0.285
mrr vals (pred, true): 0.206, 0.000
batch losses (mrrl, rdl): 0.0, 8.15652e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.399 +- 0.292
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001175791

Epoch over!
epoch time: 14.637

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 161
rank avg (pred): 0.397 +- 0.301
mrr vals (pred, true): 0.201, 0.000
batch losses (mrrl, rdl): 0.0, 2.31095e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 268
rank avg (pred): 0.384 +- 0.284
mrr vals (pred, true): 0.182, 0.336
batch losses (mrrl, rdl): 0.0, 0.0014611498

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 721
rank avg (pred): 0.420 +- 0.303
mrr vals (pred, true): 0.179, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001224011

Epoch over!
epoch time: 14.636

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1185
rank avg (pred): 0.414 +- 0.315
mrr vals (pred, true): 0.217, 0.000
batch losses (mrrl, rdl): 0.2779844999, 4.77607e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 680
rank avg (pred): 0.508 +- 0.158
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0015561389, 0.0001313537

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 520
rank avg (pred): 0.488 +- 0.146
mrr vals (pred, true): 0.053, 0.104
batch losses (mrrl, rdl): 0.0259278249, 0.0004315665

Epoch over!
epoch time: 14.868

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.426 +- 0.166
mrr vals (pred, true): 0.077, 0.186
batch losses (mrrl, rdl): 0.1197481751, 0.0010443032

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 672
rank avg (pred): 0.456 +- 0.158
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.0052845785, 8.40733e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 319
rank avg (pred): 0.476 +- 0.186
mrr vals (pred, true): 0.081, 0.092
batch losses (mrrl, rdl): 0.0094793076, 0.0004272737

Epoch over!
epoch time: 14.946

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.697 +- 0.262
mrr vals (pred, true): 0.075, 0.001
batch losses (mrrl, rdl): 0.0062901424, 0.001208827

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 103
rank avg (pred): 0.472 +- 0.166
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022896656, 6.89862e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1086
rank avg (pred): 0.403 +- 0.359
mrr vals (pred, true): 0.102, 0.001
batch losses (mrrl, rdl): 0.0273088478, 0.000164518

Epoch over!
epoch time: 14.998

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.447 +- 0.150
mrr vals (pred, true): 0.067, 0.001
batch losses (mrrl, rdl): 0.0027645472, 0.0001653443

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 755
rank avg (pred): 0.473 +- 0.240
mrr vals (pred, true): 0.086, 0.217
batch losses (mrrl, rdl): 0.1725507677, 0.0010624981

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.447 +- 0.247
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0208252296, 0.000144764

Epoch over!
epoch time: 15.089

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1103
rank avg (pred): 0.468 +- 0.202
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0014747798, 3.63372e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 555
rank avg (pred): 0.471 +- 0.200
mrr vals (pred, true): 0.050, 0.096
batch losses (mrrl, rdl): 1.032e-07, 0.0002022753

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.424 +- 0.331
mrr vals (pred, true): 0.104, 0.187
batch losses (mrrl, rdl): 0.067831181, 0.0004274192

Epoch over!
epoch time: 14.941

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 497
rank avg (pred): 0.473 +- 0.219
mrr vals (pred, true): 0.058, 0.150
batch losses (mrrl, rdl): 0.0840727687, 0.000496272

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 132
rank avg (pred): 0.473 +- 0.229
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004773235, 2.12063e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1099
rank avg (pred): 0.478 +- 0.261
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0007581993, 1.95488e-05

Epoch over!
epoch time: 14.822

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.484 +- 0.276
mrr vals (pred, true): 0.062, 0.129
batch losses (mrrl, rdl): 0.0452400558, 0.0003314272

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.470 +- 0.280
mrr vals (pred, true): 0.071, 0.000
batch losses (mrrl, rdl): 0.0042393846, 1.72141e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.495 +- 0.257
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 4.8648e-06, 0.0001108048

Epoch over!
epoch time: 14.801

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1170
rank avg (pred): 0.475 +- 0.286
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0020549914, 4.54058e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 467
rank avg (pred): 0.479 +- 0.264
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 2.21553e-05, 0.0001082408

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 483
rank avg (pred): 0.453 +- 0.295
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.017467333, 5.27407e-05

Epoch over!
epoch time: 14.824

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.471 +- 0.290
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.002618989, 7.20387e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.460 +- 0.288
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0023246007, 8.80395e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 426
rank avg (pred): 0.448 +- 0.340
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0372115299, 0.0001005756

Epoch over!
epoch time: 14.806

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 790
rank avg (pred): 0.369 +- 0.408
mrr vals (pred, true): 0.118, 0.000
batch losses (mrrl, rdl): 0.0456947535, 0.0005179979

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1051
rank avg (pred): 0.461 +- 0.295
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0065525342, 5.1819e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 63
rank avg (pred): 0.466 +- 0.287
mrr vals (pred, true): 0.061, 0.076
batch losses (mrrl, rdl): 0.001278257, 0.0001612576

Epoch over!
epoch time: 14.806

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.470 +- 0.283
mrr vals (pred, true): 0.059, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   82 	     0 	 0.06954 	 6e-0500 	 m..s
   83 	     1 	 0.06966 	 7e-0500 	 m..s
   94 	     2 	 0.07923 	 0.00013 	 m..s
   50 	     3 	 0.06125 	 0.00015 	 m..s
  120 	     4 	 0.11763 	 0.00015 	 MISS
  109 	     5 	 0.10477 	 0.00015 	 MISS
   39 	     6 	 0.06018 	 0.00016 	 m..s
    1 	     7 	 0.05470 	 0.00017 	 m..s
   45 	     8 	 0.06093 	 0.00017 	 m..s
  102 	     9 	 0.09822 	 0.00017 	 m..s
   36 	    10 	 0.06006 	 0.00017 	 m..s
  101 	    11 	 0.09814 	 0.00018 	 m..s
   12 	    12 	 0.05802 	 0.00018 	 m..s
   62 	    13 	 0.06242 	 0.00018 	 m..s
   17 	    14 	 0.05873 	 0.00018 	 m..s
  118 	    15 	 0.11759 	 0.00019 	 MISS
   15 	    16 	 0.05830 	 0.00020 	 m..s
   72 	    17 	 0.06548 	 0.00020 	 m..s
   79 	    18 	 0.06818 	 0.00020 	 m..s
   54 	    19 	 0.06142 	 0.00021 	 m..s
  107 	    20 	 0.10232 	 0.00021 	 MISS
   53 	    21 	 0.06138 	 0.00021 	 m..s
   16 	    22 	 0.05870 	 0.00021 	 m..s
   96 	    23 	 0.09206 	 0.00021 	 m..s
   35 	    24 	 0.06005 	 0.00022 	 m..s
   20 	    25 	 0.05884 	 0.00022 	 m..s
   76 	    26 	 0.06635 	 0.00022 	 m..s
  110 	    27 	 0.10559 	 0.00023 	 MISS
   66 	    28 	 0.06277 	 0.00023 	 m..s
  100 	    29 	 0.09618 	 0.00023 	 m..s
   14 	    30 	 0.05829 	 0.00023 	 m..s
   33 	    31 	 0.06000 	 0.00023 	 m..s
  106 	    32 	 0.10091 	 0.00024 	 MISS
   22 	    33 	 0.05888 	 0.00024 	 m..s
    9 	    34 	 0.05779 	 0.00025 	 m..s
   70 	    35 	 0.06523 	 0.00025 	 m..s
   71 	    36 	 0.06548 	 0.00025 	 m..s
   33 	    37 	 0.06000 	 0.00025 	 m..s
   61 	    38 	 0.06229 	 0.00025 	 m..s
   78 	    39 	 0.06790 	 0.00026 	 m..s
   91 	    40 	 0.07879 	 0.00026 	 m..s
   58 	    41 	 0.06218 	 0.00026 	 m..s
   24 	    42 	 0.05891 	 0.00027 	 m..s
  114 	    43 	 0.11042 	 0.00027 	 MISS
   41 	    44 	 0.06052 	 0.00028 	 m..s
   26 	    45 	 0.05895 	 0.00028 	 m..s
   88 	    46 	 0.07711 	 0.00029 	 m..s
    6 	    47 	 0.05681 	 0.00029 	 m..s
   30 	    48 	 0.05947 	 0.00029 	 m..s
  116 	    49 	 0.11066 	 0.00029 	 MISS
   46 	    50 	 0.06108 	 0.00031 	 m..s
  112 	    51 	 0.10834 	 0.00031 	 MISS
   28 	    52 	 0.05931 	 0.00032 	 m..s
   95 	    53 	 0.09051 	 0.00032 	 m..s
   47 	    54 	 0.06117 	 0.00032 	 m..s
   44 	    55 	 0.06059 	 0.00032 	 m..s
   38 	    56 	 0.06011 	 0.00032 	 m..s
    5 	    57 	 0.05638 	 0.00034 	 m..s
  111 	    58 	 0.10596 	 0.00034 	 MISS
   22 	    59 	 0.05888 	 0.00036 	 m..s
   80 	    60 	 0.06891 	 0.00038 	 m..s
   74 	    61 	 0.06578 	 0.00038 	 m..s
   69 	    62 	 0.06452 	 0.00039 	 m..s
   21 	    63 	 0.05888 	 0.00039 	 m..s
  108 	    64 	 0.10268 	 0.00041 	 MISS
   30 	    65 	 0.05947 	 0.00041 	 m..s
   11 	    66 	 0.05802 	 0.00041 	 m..s
   60 	    67 	 0.06225 	 0.00049 	 m..s
   65 	    68 	 0.06274 	 0.00049 	 m..s
   55 	    69 	 0.06142 	 0.00052 	 m..s
   49 	    70 	 0.06124 	 0.00053 	 m..s
   73 	    71 	 0.06559 	 0.00055 	 m..s
    1 	    72 	 0.05470 	 0.00060 	 m..s
   47 	    73 	 0.06117 	 0.00060 	 m..s
   81 	    74 	 0.06936 	 0.00062 	 m..s
   32 	    75 	 0.05974 	 0.00064 	 m..s
   10 	    76 	 0.05797 	 0.00076 	 m..s
   42 	    77 	 0.06055 	 0.00079 	 m..s
   75 	    78 	 0.06614 	 0.00082 	 m..s
   96 	    79 	 0.09206 	 0.00089 	 m..s
   52 	    80 	 0.06134 	 0.00103 	 m..s
   29 	    81 	 0.05933 	 0.00118 	 m..s
  113 	    82 	 0.10997 	 0.00198 	 MISS
   67 	    83 	 0.06288 	 0.00220 	 m..s
   25 	    84 	 0.05894 	 0.00831 	 m..s
   87 	    85 	 0.07645 	 0.00846 	 m..s
   62 	    86 	 0.06242 	 0.00890 	 m..s
    8 	    87 	 0.05716 	 0.06408 	 ~...
    3 	    88 	 0.05612 	 0.07375 	 ~...
   17 	    89 	 0.05873 	 0.07732 	 ~...
   19 	    90 	 0.05882 	 0.07821 	 ~...
   13 	    91 	 0.05810 	 0.07977 	 ~...
   39 	    92 	 0.06018 	 0.08062 	 ~...
   98 	    93 	 0.09228 	 0.08631 	 ~...
    0 	    94 	 0.05425 	 0.08857 	 m..s
   57 	    95 	 0.06182 	 0.09248 	 m..s
    7 	    96 	 0.05688 	 0.09343 	 m..s
   43 	    97 	 0.06056 	 0.09774 	 m..s
    4 	    98 	 0.05630 	 0.09829 	 m..s
   64 	    99 	 0.06258 	 0.09921 	 m..s
   84 	   100 	 0.07146 	 0.10482 	 m..s
   27 	   101 	 0.05899 	 0.10564 	 m..s
   59 	   102 	 0.06221 	 0.10641 	 m..s
   89 	   103 	 0.07739 	 0.10853 	 m..s
   77 	   104 	 0.06770 	 0.11548 	 m..s
   37 	   105 	 0.06009 	 0.11553 	 m..s
   86 	   106 	 0.07644 	 0.12052 	 m..s
   50 	   107 	 0.06125 	 0.12437 	 m..s
   85 	   108 	 0.07581 	 0.13670 	 m..s
  105 	   109 	 0.09961 	 0.13719 	 m..s
   68 	   110 	 0.06421 	 0.14537 	 m..s
   56 	   111 	 0.06173 	 0.15149 	 m..s
   90 	   112 	 0.07800 	 0.15424 	 m..s
  102 	   113 	 0.09822 	 0.17022 	 m..s
   93 	   114 	 0.07898 	 0.17092 	 m..s
   99 	   115 	 0.09337 	 0.17311 	 m..s
   92 	   116 	 0.07883 	 0.18809 	 MISS
  118 	   117 	 0.11759 	 0.21337 	 m..s
  114 	   118 	 0.11042 	 0.22246 	 MISS
  117 	   119 	 0.11191 	 0.31651 	 MISS
  104 	   120 	 0.09852 	 0.31824 	 MISS
==========================================
r_mrr = 0.2579314410686493
r2_mrr = -0.18505942821502686
spearmanr_mrr@5 = 0.7740503549575806
spearmanr_mrr@10 = 0.8899906277656555
spearmanr_mrr@50 = 0.9347621202468872
spearmanr_mrr@100 = 0.9519985914230347
spearmanr_mrr@All = 0.949439287185669
==========================================
test time: 0.451
Done Testing dataset DBpedia50
total time taken: 228.67638516426086
training time taken: 223.33380913734436
TWIG out ;))
Ablation done!
The best results were: None
The best settings found were:

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 2519578377623106
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [946, 1115, 365, 374, 1125, 559, 777, 1046, 5, 460, 669, 927, 1156, 847, 453, 533, 565, 487, 479, 1160, 283, 502, 948, 394, 599, 953, 231, 1071, 773, 807, 687, 858, 1053, 952, 797, 1092, 302, 315, 1173, 177, 409, 14, 708, 232, 132, 430, 1129, 1209, 892, 134, 793, 1081, 623, 918, 1158, 543, 399, 75, 690, 261, 766, 1002, 654, 1, 703, 494, 506, 850, 433, 735, 792, 72, 184, 814, 505, 9, 836, 8, 1033, 758, 267, 92, 595, 928, 384, 611, 775, 45, 320, 33, 1098, 545, 345, 491, 661, 1001, 206, 883, 391, 373, 1161, 864, 974, 975, 239, 606, 523, 155, 1166, 389, 1049, 678, 934, 70, 589, 925, 939, 1057, 1176, 715, 1143]
valid_ids (0): []
train_ids (1094): [252, 720, 323, 564, 825, 423, 1184, 450, 326, 10, 577, 891, 106, 557, 629, 995, 916, 21, 1153, 637, 973, 36, 878, 930, 1084, 718, 726, 461, 738, 1132, 1083, 113, 124, 1059, 949, 586, 18, 635, 1089, 674, 1127, 1126, 982, 47, 406, 67, 520, 485, 750, 489, 879, 377, 30, 376, 762, 516, 943, 1031, 555, 986, 861, 828, 295, 388, 756, 319, 172, 412, 515, 658, 429, 572, 272, 795, 769, 696, 684, 811, 1061, 761, 273, 622, 193, 714, 960, 812, 787, 870, 278, 392, 662, 324, 655, 154, 783, 385, 857, 809, 26, 481, 932, 349, 1114, 573, 356, 846, 958, 378, 1137, 439, 138, 675, 888, 774, 298, 346, 43, 996, 819, 933, 1020, 474, 162, 544, 615, 260, 670, 1155, 170, 894, 331, 563, 192, 994, 82, 619, 826, 620, 277, 367, 1171, 390, 576, 69, 281, 1019, 317, 947, 660, 935, 709, 929, 532, 1067, 587, 386, 1186, 236, 626, 628, 1111, 173, 202, 621, 194, 1204, 122, 652, 227, 890, 411, 148, 276, 362, 1009, 457, 404, 369, 395, 601, 695, 881, 127, 845, 63, 956, 446, 496, 957, 1110, 136, 581, 452, 294, 1212, 503, 116, 51, 1203, 471, 665, 803, 765, 663, 683, 197, 62, 689, 1139, 309, 393, 1193, 451, 344, 251, 142, 189, 259, 596, 174, 329, 217, 796, 548, 989, 722, 977, 966, 1178, 438, 799, 1121, 478, 978, 1174, 490, 1076, 435, 1105, 729, 422, 732, 1078, 872, 467, 17, 185, 1116, 743, 1042, 1024, 530, 906, 649, 582, 1068, 226, 816, 269, 328, 552, 509, 354, 1179, 1128, 744, 131, 483, 1006, 1198, 125, 920, 224, 262, 425, 728, 874, 211, 27, 876, 180, 1181, 691, 806, 643, 440, 352, 782, 198, 396, 558, 991, 99, 144, 476, 482, 274, 46, 215, 962, 228, 257, 1085, 1087, 889, 468, 417, 1054, 473, 838, 1072, 550, 877, 1051, 264, 980, 831, 848, 519, 335, 1063, 1135, 221, 379, 118, 119, 1044, 1007, 1148, 157, 98, 321, 907, 1034, 723, 68, 554, 727, 48, 188, 94, 898, 842, 244, 421, 455, 786, 223, 77, 712, 763, 1177, 107, 896, 165, 528, 922, 1010, 618, 1022, 357, 80, 39, 105, 1101, 993, 470, 1090, 103, 616, 149, 753, 679, 1069, 527, 495, 608, 199, 97, 242, 746, 330, 686, 488, 900, 710, 1107, 290, 271, 1207, 1104, 535, 921, 359, 327, 759, 380, 751, 1138, 1168, 268, 146, 713, 778, 940, 549, 1014, 614, 969, 711, 1048, 147, 871, 1106, 1093, 1065, 432, 585, 694, 0, 1201, 431, 1151, 1088, 1102, 574, 229, 992, 768, 987, 682, 540, 292, 1062, 464, 1025, 334, 204, 1169, 1142, 1191, 1208, 50, 513, 602, 1013, 1058, 914, 1146, 37, 307, 954, 1008, 126, 1205, 1074, 944, 187, 212, 560, 286, 1196, 284, 141, 1035, 854, 499, 917, 588, 398, 737, 1199, 332, 716, 59, 76, 990, 901, 238, 810, 899, 931, 104, 444, 312, 873, 772, 1124, 347, 688, 979, 905, 667, 1036, 339, 869, 463, 818, 692, 536, 3, 32, 893, 225, 780, 627, 58, 579, 859, 158, 1012, 1206, 531, 524, 400, 297, 537, 760, 1170, 604, 145, 521, 650, 266, 484, 243, 325, 466, 465, 591, 827, 764, 1144, 644, 458, 1150, 970, 366, 755, 437, 419, 1162, 401, 701, 741, 1103, 843, 1094, 371, 776, 1005, 407, 168, 190, 1082, 784, 833, 52, 634, 91, 74, 250, 1004, 805, 11, 603, 348, 672, 717, 340, 265, 213, 361, 160, 849, 724, 1192, 997, 1149, 299, 248, 733, 964, 856, 959, 676, 1100, 253, 245, 1141, 343, 510, 90, 834, 112, 133, 196, 641, 525, 441, 707, 166, 924, 1163, 742, 815, 538, 280, 191, 287, 651, 445, 44, 84, 875, 129, 1038, 779, 61, 913, 1167, 121, 613, 341, 1189, 291, 195, 1064, 951, 370, 1079, 424, 1214, 115, 255, 685, 1211, 1195, 167, 1152, 1190, 454, 1086, 1210, 645, 653, 547, 919, 153, 950, 405, 29, 642, 342, 853, 6, 1077, 447, 83, 42, 813, 34, 546, 85, 902, 353, 504, 820, 486, 397, 1052, 1200, 590, 114, 702, 1165, 556, 804, 183, 443, 462, 798, 78, 1164, 408, 976, 222, 275, 1140, 955, 566, 680, 1113, 985, 88, 182, 971, 886, 968, 247, 288, 333, 1029, 794, 562, 740, 1017, 909, 988, 673, 410, 830, 817, 270, 128, 1117, 1123, 575, 64, 1073, 387, 498, 1157, 338, 363, 171, 705, 150, 730, 71, 840, 865, 426, 305, 942, 617, 293, 402, 625, 829, 1131, 719, 910, 638, 569, 984, 1041, 57, 789, 35, 375, 164, 159, 1091, 304, 12, 844, 624, 256, 96, 936, 220, 66, 1112, 1011, 648, 1095, 529, 81, 175, 137, 882, 55, 120, 434, 38, 1183, 203, 923, 631, 73, 1136, 911, 868, 1023, 210, 551, 216, 981, 143, 668, 594, 657, 785, 736, 1075, 414, 561, 235, 757, 837, 1108, 54, 28, 15, 108, 233, 40, 355, 1147, 938, 181, 771, 522, 605, 130, 704, 511, 926, 1045, 553, 1120, 752, 915, 135, 823, 459, 310, 318, 767, 500, 632, 904, 863, 246, 22, 1070, 2, 20, 381, 65, 788, 903, 580, 95, 436, 416, 748, 791, 1056, 636, 1000, 102, 633, 1040, 963, 855, 607, 364, 477, 731, 699, 802, 301, 230, 497, 442, 1021, 420, 1039, 1026, 646, 163, 140, 912, 79, 1175, 311, 1109, 967, 539, 214, 571, 427, 972, 862, 880, 475, 1188, 139, 492, 101, 822, 998, 1030, 1197, 93, 178, 205, 13, 1122, 1015, 570, 49, 337, 698, 241, 1187, 584, 1060, 860, 508, 583, 1003, 983, 999, 598, 739, 1159, 1066, 200, 1213, 382, 89, 351, 24, 87, 111, 1194, 677, 413, 821, 218, 117, 156, 1018, 313, 965, 169, 700, 1097, 824, 1037, 542, 885, 808, 518, 835, 1202, 693, 1185, 428, 1016, 1047, 541, 152, 472, 945, 176, 306, 801, 110, 469, 53, 1080, 161, 285, 360, 1096, 866, 639, 725, 1145, 754, 884, 526, 908, 578, 841, 1099, 832, 418, 659, 350, 449, 186, 41, 507, 237, 514, 597, 1130, 60, 1154, 656, 19, 852, 316, 151, 372, 358, 1050, 512, 610, 480, 109, 1043, 314, 448, 207, 895, 1119, 282, 612, 1172, 263, 897, 336, 567, 415, 16, 600, 630, 697, 201, 517, 56, 1028, 7, 368, 289, 867, 1027, 403, 279, 208, 593, 25, 300, 800, 706, 209, 456, 249, 745, 1055, 219, 887, 1134, 681, 308, 1032, 1118, 23, 1182, 671, 734, 179, 322, 839, 640, 609, 4, 501, 86, 749, 961, 383, 721, 647, 234, 790, 254, 664, 747, 1180, 123, 666, 100, 1133, 240, 770, 851, 568, 296, 534, 941, 592, 493, 303, 31, 781, 937, 258]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1252934476557140
the save name prefix for this run is:  chkpt-ID_1252934476557140_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 567
rank avg (pred): 0.588 +- 0.002
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002407546

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 997
rank avg (pred): 0.264 +- 0.186
mrr vals (pred, true): 0.176, 0.126
batch losses (mrrl, rdl): 0.0, 5.29634e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 293
rank avg (pred): 0.282 +- 0.207
mrr vals (pred, true): 0.047, 0.128
batch losses (mrrl, rdl): 0.0, 5.32645e-05

Epoch over!
epoch time: 14.786

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.198 +- 0.154
mrr vals (pred, true): 0.186, 0.332
batch losses (mrrl, rdl): 0.0, 0.0001583273

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 225
rank avg (pred): 0.455 +- 0.292
mrr vals (pred, true): 0.017, 0.000
batch losses (mrrl, rdl): 0.0, 2.7131e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.208 +- 0.159
mrr vals (pred, true): 0.046, 0.181
batch losses (mrrl, rdl): 0.0, 0.0001091731

Epoch over!
epoch time: 14.814

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 165
rank avg (pred): 0.450 +- 0.302
mrr vals (pred, true): 0.028, 0.000
batch losses (mrrl, rdl): 0.0, 2.35898e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.461 +- 0.288
mrr vals (pred, true): 0.009, 0.000
batch losses (mrrl, rdl): 0.0, 6.4154e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.475 +- 0.289
mrr vals (pred, true): 0.011, 0.000
batch losses (mrrl, rdl): 0.0, 8.74e-06

Epoch over!
epoch time: 14.906

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 510
rank avg (pred): 0.379 +- 0.279
mrr vals (pred, true): 0.023, 0.155
batch losses (mrrl, rdl): 0.0, 0.0002504928

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.218 +- 0.174
mrr vals (pred, true): 0.055, 0.144
batch losses (mrrl, rdl): 0.0, 4.18517e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1168
rank avg (pred): 0.475 +- 0.285
mrr vals (pred, true): 0.017, 0.000
batch losses (mrrl, rdl): 0.0, 2.19129e-05

Epoch over!
epoch time: 14.931

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 131
rank avg (pred): 0.493 +- 0.265
mrr vals (pred, true): 0.011, 0.000
batch losses (mrrl, rdl): 0.0, 6.2207e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.470 +- 0.290
mrr vals (pred, true): 0.023, 0.000
batch losses (mrrl, rdl): 0.0, 1.19011e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 6
rank avg (pred): 0.362 +- 0.276
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002495393

Epoch over!
epoch time: 14.969

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 782
rank avg (pred): 0.454 +- 0.280
mrr vals (pred, true): 0.019, 0.000
batch losses (mrrl, rdl): 0.0098049454, 1.25125e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1134
rank avg (pred): 0.399 +- 0.261
mrr vals (pred, true): 0.052, 0.129
batch losses (mrrl, rdl): 0.0587590933, 0.0001781575

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.534 +- 0.248
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001423643, 6.47247e-05

Epoch over!
epoch time: 15.139

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 161
rank avg (pred): 0.414 +- 0.191
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0008466062, 4.1165e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.487 +- 0.250
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.000520292, 1.41832e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 602
rank avg (pred): 0.459 +- 0.223
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.2626e-06, 1.80528e-05

Epoch over!
epoch time: 15.116

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.411 +- 0.250
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009921829, 0.0001479675

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 415
rank avg (pred): 0.447 +- 0.224
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005426226, 0.0001262215

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 400
rank avg (pred): 0.453 +- 0.232
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.3779e-06, 5.75351e-05

Epoch over!
epoch time: 14.945

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 805
rank avg (pred): 0.458 +- 0.231
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.13298e-05, 3.34623e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.463 +- 0.229
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001434422, 2.71757e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.509 +- 0.225
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004299331, 7.83506e-05

Epoch over!
epoch time: 14.961

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1022
rank avg (pred): 0.482 +- 0.218
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002807536, 3.61726e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 22
rank avg (pred): 0.416 +- 0.228
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.000145867, 6.82421e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 829
rank avg (pred): 0.068 +- 0.067
mrr vals (pred, true): 0.207, 0.173
batch losses (mrrl, rdl): 0.0113396151, 0.0007273515

Epoch over!
epoch time: 14.975

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 429
rank avg (pred): 0.463 +- 0.232
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.99264e-05, 1.96862e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 36
rank avg (pred): 0.424 +- 0.239
mrr vals (pred, true): 0.077, 0.090
batch losses (mrrl, rdl): 0.0074143102, 0.0003255378

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 445
rank avg (pred): 0.483 +- 0.215
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001756826, 5.81118e-05

Epoch over!
epoch time: 15.004

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 78
rank avg (pred): 0.427 +- 0.232
mrr vals (pred, true): 0.058, 0.109
batch losses (mrrl, rdl): 0.0253914967, 0.0004906903

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.127 +- 0.130
mrr vals (pred, true): 0.204, 0.129
batch losses (mrrl, rdl): 0.0560221821, 0.0008874802

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1144
rank avg (pred): 0.332 +- 0.260
mrr vals (pred, true): 0.075, 0.103
batch losses (mrrl, rdl): 0.0079022264, 2.14219e-05

Epoch over!
epoch time: 14.976

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 377
rank avg (pred): 0.440 +- 0.232
mrr vals (pred, true): 0.063, 0.001
batch losses (mrrl, rdl): 0.0015861529, 2.72165e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 361
rank avg (pred): 0.453 +- 0.216
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 7.4722e-05, 3.55917e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1155
rank avg (pred): 0.379 +- 0.285
mrr vals (pred, true): 0.099, 0.140
batch losses (mrrl, rdl): 0.0172089245, 3.31536e-05

Epoch over!
epoch time: 14.956

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.478 +- 0.192
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.0773e-06, 4.75983e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 457
rank avg (pred): 0.449 +- 0.213
mrr vals (pred, true): 0.055, 0.002
batch losses (mrrl, rdl): 0.0002247653, 2.15501e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.502 +- 0.171
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004514462, 6.33578e-05

Epoch over!
epoch time: 14.972

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.485 +- 0.181
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 4.37453e-05, 4.85381e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 855
rank avg (pred): 0.467 +- 0.194
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.28815e-05, 5.79158e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.453 +- 0.248
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001729607, 0.001526786

Epoch over!
epoch time: 14.984

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.532 +- 0.212
mrr vals (pred, true): 0.045, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   79 	     0 	 0.05399 	 0.00014 	 m..s
   16 	     1 	 0.04733 	 0.00015 	 m..s
    5 	     2 	 0.04496 	 0.00016 	 m..s
   45 	     3 	 0.05022 	 0.00017 	 m..s
    0 	     4 	 0.04053 	 0.00018 	 m..s
   56 	     5 	 0.05064 	 0.00018 	 m..s
   19 	     6 	 0.04756 	 0.00018 	 m..s
   30 	     7 	 0.04944 	 0.00018 	 m..s
   72 	     8 	 0.05217 	 0.00018 	 m..s
    8 	     9 	 0.04558 	 0.00018 	 m..s
   47 	    10 	 0.05022 	 0.00018 	 m..s
   69 	    11 	 0.05117 	 0.00019 	 m..s
   11 	    12 	 0.04675 	 0.00019 	 m..s
   22 	    13 	 0.04793 	 0.00020 	 m..s
   35 	    14 	 0.04983 	 0.00020 	 m..s
   17 	    15 	 0.04738 	 0.00020 	 m..s
   15 	    16 	 0.04733 	 0.00020 	 m..s
   59 	    17 	 0.05066 	 0.00020 	 m..s
   61 	    18 	 0.05067 	 0.00020 	 m..s
   20 	    19 	 0.04776 	 0.00020 	 m..s
   57 	    20 	 0.05065 	 0.00021 	 m..s
    4 	    21 	 0.04484 	 0.00021 	 m..s
   58 	    22 	 0.05065 	 0.00021 	 m..s
   29 	    23 	 0.04935 	 0.00021 	 m..s
   34 	    24 	 0.04979 	 0.00022 	 m..s
   37 	    25 	 0.04992 	 0.00022 	 m..s
   40 	    26 	 0.05003 	 0.00022 	 m..s
   85 	    27 	 0.06027 	 0.00022 	 m..s
   48 	    28 	 0.05024 	 0.00022 	 m..s
   23 	    29 	 0.04827 	 0.00023 	 m..s
   71 	    30 	 0.05164 	 0.00023 	 m..s
    6 	    31 	 0.04533 	 0.00023 	 m..s
   38 	    32 	 0.04996 	 0.00023 	 m..s
   50 	    33 	 0.05049 	 0.00024 	 m..s
   73 	    34 	 0.05230 	 0.00024 	 m..s
    2 	    35 	 0.04126 	 0.00025 	 m..s
    8 	    36 	 0.04558 	 0.00025 	 m..s
   44 	    37 	 0.05018 	 0.00025 	 m..s
   42 	    38 	 0.05009 	 0.00027 	 m..s
   31 	    39 	 0.04956 	 0.00027 	 m..s
   70 	    40 	 0.05158 	 0.00028 	 m..s
   13 	    41 	 0.04705 	 0.00028 	 m..s
   67 	    42 	 0.05084 	 0.00028 	 m..s
   55 	    43 	 0.05063 	 0.00028 	 m..s
   26 	    44 	 0.04892 	 0.00029 	 m..s
   53 	    45 	 0.05056 	 0.00029 	 m..s
   82 	    46 	 0.05816 	 0.00030 	 m..s
   25 	    47 	 0.04868 	 0.00031 	 m..s
   78 	    48 	 0.05365 	 0.00032 	 m..s
   24 	    49 	 0.04830 	 0.00033 	 m..s
   10 	    50 	 0.04667 	 0.00034 	 m..s
   39 	    51 	 0.05001 	 0.00035 	 m..s
   83 	    52 	 0.05858 	 0.00035 	 m..s
    7 	    53 	 0.04544 	 0.00036 	 m..s
   61 	    54 	 0.05067 	 0.00036 	 m..s
   65 	    55 	 0.05080 	 0.00036 	 m..s
   27 	    56 	 0.04916 	 0.00037 	 m..s
   76 	    57 	 0.05281 	 0.00037 	 m..s
    2 	    58 	 0.04126 	 0.00039 	 m..s
    1 	    59 	 0.04110 	 0.00039 	 m..s
   74 	    60 	 0.05247 	 0.00040 	 m..s
   80 	    61 	 0.05413 	 0.00040 	 m..s
   64 	    62 	 0.05071 	 0.00041 	 m..s
   75 	    63 	 0.05278 	 0.00041 	 m..s
   21 	    64 	 0.04782 	 0.00043 	 m..s
   32 	    65 	 0.04965 	 0.00043 	 m..s
   17 	    66 	 0.04738 	 0.00050 	 m..s
   52 	    67 	 0.05052 	 0.00051 	 m..s
   51 	    68 	 0.05049 	 0.00052 	 m..s
   45 	    69 	 0.05022 	 0.00054 	 m..s
   49 	    70 	 0.05044 	 0.00055 	 m..s
   68 	    71 	 0.05111 	 0.00066 	 m..s
   36 	    72 	 0.04985 	 0.00069 	 m..s
   63 	    73 	 0.05068 	 0.00071 	 m..s
   59 	    74 	 0.05066 	 0.00074 	 m..s
   12 	    75 	 0.04697 	 0.00075 	 m..s
   87 	    76 	 0.06211 	 0.00082 	 m..s
   41 	    77 	 0.05004 	 0.00087 	 m..s
   43 	    78 	 0.05009 	 0.00113 	 m..s
   14 	    79 	 0.04712 	 0.00118 	 m..s
   54 	    80 	 0.05057 	 0.00137 	 m..s
   32 	    81 	 0.04965 	 0.00161 	 m..s
   86 	    82 	 0.06055 	 0.00179 	 m..s
   65 	    83 	 0.05080 	 0.00198 	 m..s
   28 	    84 	 0.04929 	 0.00574 	 m..s
   77 	    85 	 0.05313 	 0.05395 	 ~...
  103 	    86 	 0.08434 	 0.05590 	 ~...
  111 	    87 	 0.10738 	 0.07272 	 m..s
   88 	    88 	 0.06584 	 0.07501 	 ~...
  112 	    89 	 0.13686 	 0.08100 	 m..s
   81 	    90 	 0.05779 	 0.08157 	 ~...
   94 	    91 	 0.07747 	 0.08701 	 ~...
   88 	    92 	 0.06584 	 0.08727 	 ~...
   98 	    93 	 0.08218 	 0.08821 	 ~...
   92 	    94 	 0.07387 	 0.08843 	 ~...
  105 	    95 	 0.08942 	 0.09013 	 ~...
   90 	    96 	 0.06595 	 0.09022 	 ~...
   84 	    97 	 0.05954 	 0.09528 	 m..s
  108 	    98 	 0.09624 	 0.10296 	 ~...
   93 	    99 	 0.07642 	 0.10792 	 m..s
  100 	   100 	 0.08300 	 0.10828 	 ~...
  110 	   101 	 0.10586 	 0.10983 	 ~...
   91 	   102 	 0.07380 	 0.11123 	 m..s
   95 	   103 	 0.07857 	 0.11248 	 m..s
  102 	   104 	 0.08428 	 0.11308 	 ~...
  115 	   105 	 0.15138 	 0.11516 	 m..s
   95 	   106 	 0.07857 	 0.11548 	 m..s
  116 	   107 	 0.16065 	 0.12667 	 m..s
  109 	   108 	 0.10482 	 0.12813 	 ~...
  100 	   109 	 0.08300 	 0.12939 	 m..s
  117 	   110 	 0.16152 	 0.13670 	 ~...
  107 	   111 	 0.09452 	 0.13681 	 m..s
   97 	   112 	 0.07909 	 0.14659 	 m..s
   98 	   113 	 0.08218 	 0.15149 	 m..s
  104 	   114 	 0.08831 	 0.16343 	 m..s
  105 	   115 	 0.08942 	 0.16359 	 m..s
  119 	   116 	 0.22187 	 0.21337 	 ~...
  118 	   117 	 0.21437 	 0.29862 	 m..s
  120 	   118 	 0.22385 	 0.31081 	 m..s
  114 	   119 	 0.13818 	 0.32819 	 MISS
  112 	   120 	 0.13686 	 0.33451 	 MISS
==========================================
r_mrr = 0.8560488224029541
r2_mrr = 0.46545082330703735
spearmanr_mrr@5 = 0.7877854108810425
spearmanr_mrr@10 = 0.9218237996101379
spearmanr_mrr@50 = 0.9516938924789429
spearmanr_mrr@100 = 0.9632588624954224
spearmanr_mrr@All = 0.9652354121208191
==========================================
test time: 0.453
Done Testing dataset DBpedia50
total time taken: 230.1848075389862
training time taken: 224.89540815353394
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8560)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4655)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.7878)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9218)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9517)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9633)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9652)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.4376826906609494}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 7032145443614279
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [866, 283, 496, 993, 941, 331, 1098, 131, 4, 740, 483, 908, 935, 774, 222, 221, 1114, 1155, 600, 112, 860, 629, 1067, 955, 217, 676, 85, 146, 1206, 270, 800, 594, 1182, 1167, 70, 767, 1143, 1009, 28, 339, 721, 44, 799, 356, 1141, 1078, 525, 9, 446, 148, 858, 406, 267, 651, 1148, 1005, 96, 1127, 383, 63, 364, 1178, 640, 705, 790, 957, 154, 23, 40, 390, 313, 1196, 759, 744, 350, 542, 1164, 598, 487, 426, 990, 887, 209, 127, 527, 752, 116, 1139, 902, 48, 820, 579, 1061, 129, 560, 715, 341, 913, 258, 241, 601, 1187, 208, 596, 829, 1157, 753, 1181, 295, 176, 193, 535, 617, 353, 65, 377, 561, 371, 999, 747, 891]
valid_ids (0): []
train_ids (1094): [852, 95, 595, 287, 338, 1109, 1211, 639, 569, 1100, 796, 500, 842, 1126, 683, 1119, 157, 87, 66, 697, 1016, 247, 1194, 907, 567, 1068, 144, 585, 630, 896, 155, 672, 1184, 196, 714, 756, 704, 628, 572, 304, 1004, 14, 1083, 815, 284, 865, 300, 387, 5, 205, 423, 449, 43, 194, 47, 97, 12, 849, 899, 180, 795, 807, 895, 279, 666, 855, 430, 1207, 1199, 361, 202, 987, 84, 848, 1104, 975, 973, 413, 1147, 691, 395, 792, 653, 1146, 174, 71, 164, 69, 564, 171, 328, 644, 877, 299, 39, 459, 810, 804, 605, 75, 956, 805, 86, 745, 1041, 937, 916, 537, 352, 1069, 1160, 929, 1195, 159, 893, 342, 710, 280, 102, 1032, 1081, 1163, 850, 519, 1076, 1172, 490, 551, 522, 187, 61, 699, 185, 769, 844, 200, 871, 107, 961, 124, 610, 351, 290, 378, 126, 737, 435, 1180, 302, 638, 686, 264, 190, 494, 1120, 1192, 186, 754, 637, 461, 477, 379, 559, 599, 1028, 183, 195, 1095, 854, 189, 346, 121, 1070, 897, 818, 1073, 108, 656, 322, 199, 411, 1110, 237, 486, 802, 372, 1091, 294, 1125, 192, 405, 997, 1133, 979, 1088, 969, 276, 659, 316, 1080, 1118, 652, 967, 1161, 349, 1062, 74, 924, 198, 832, 881, 265, 32, 388, 864, 689, 281, 252, 523, 964, 396, 1193, 197, 616, 803, 172, 938, 417, 528, 433, 974, 568, 1189, 354, 434, 1138, 562, 89, 251, 458, 288, 20, 1013, 268, 685, 647, 983, 761, 1102, 718, 272, 755, 340, 507, 847, 416, 584, 624, 45, 1071, 843, 403, 463, 91, 654, 243, 103, 622, 687, 158, 306, 55, 13, 64, 462, 3, 690, 359, 798, 440, 702, 720, 382, 986, 1027, 33, 249, 822, 1018, 481, 760, 1175, 976, 474, 1074, 374, 473, 1035, 757, 1144, 228, 188, 811, 1090, 750, 468, 256, 18, 266, 883, 489, 540, 0, 471, 46, 869, 840, 1152, 257, 904, 733, 150, 213, 1024, 1129, 175, 401, 472, 278, 381, 611, 1176, 402, 120, 1166, 936, 99, 557, 1072, 424, 688, 779, 271, 425, 436, 138, 137, 118, 214, 545, 1047, 1151, 1149, 1185, 565, 318, 447, 900, 602, 1077, 867, 1030, 49, 526, 26, 515, 1103, 890, 248, 1121, 548, 245, 778, 856, 708, 531, 1135, 1200, 1075, 587, 1170, 54, 771, 948, 207, 36, 173, 942, 370, 764, 879, 109, 15, 603, 658, 1053, 1064, 923, 1212, 94, 319, 184, 1036, 476, 1011, 736, 427, 100, 1168, 636, 231, 422, 835, 591, 1162, 563, 274, 674, 34, 825, 996, 119, 978, 376, 994, 170, 211, 1169, 1079, 592, 1115, 1165, 139, 791, 570, 857, 963, 934, 730, 586, 7, 442, 589, 648, 546, 24, 952, 226, 614, 597, 437, 577, 831, 541, 1210, 335, 38, 549, 385, 244, 125, 814, 837, 786, 503, 580, 882, 412, 410, 414, 1153, 552, 236, 375, 627, 136, 1038, 79, 166, 400, 1204, 457, 419, 1108, 880, 460, 10, 50, 1156, 52, 669, 218, 841, 408, 729, 1046, 1014, 233, 536, 1044, 497, 1128, 254, 1087, 431, 716, 661, 836, 315, 409, 875, 1101, 909, 991, 1105, 140, 1049, 163, 429, 1019, 332, 78, 393, 646, 132, 821, 817, 1150, 806, 1209, 445, 293, 113, 1132, 933, 928, 178, 360, 479, 111, 269, 1065, 451, 670, 945, 1171, 21, 1043, 235, 566, 1026, 41, 1066, 81, 210, 544, 513, 297, 492, 1015, 1099, 989, 1054, 839, 362, 859, 944, 1086, 777, 114, 134, 98, 732, 775, 225, 677, 524, 135, 246, 305, 530, 482, 301, 260, 147, 706, 1085, 1050, 925, 478, 1202, 62, 1021, 450, 262, 1116, 491, 863, 532, 1060, 1097, 418, 1145, 212, 1006, 1029, 696, 443, 72, 998, 204, 995, 366, 1158, 149, 152, 1039, 1188, 303, 992, 469, 224, 1117, 141, 607, 498, 980, 680, 724, 845, 735, 58, 327, 741, 663, 220, 191, 618, 499, 1023, 444, 684, 671, 939, 898, 53, 613, 1213, 931, 389, 165, 323, 307, 915, 312, 878, 1186, 593, 25, 123, 1, 60, 830, 355, 533, 398, 508, 739, 668, 1123, 1048, 678, 940, 160, 679, 550, 285, 731, 80, 358, 828, 1003, 884, 781, 665, 962, 1130, 438, 348, 960, 770, 547, 723, 971, 455, 1177, 619, 1051, 693, 620, 179, 151, 324, 277, 773, 784, 1063, 675, 454, 145, 694, 30, 888, 529, 329, 625, 512, 667, 325, 673, 853, 510, 922, 448, 35, 106, 206, 988, 682, 337, 475, 168, 711, 432, 851, 834, 167, 238, 788, 467, 465, 643, 582, 240, 1198, 912, 1179, 1010, 862, 576, 719, 16, 161, 368, 914, 615, 521, 110, 951, 336, 981, 727, 1058, 657, 292, 631, 1055, 1159, 1084, 1008, 1052, 762, 330, 230, 517, 894, 1089, 801, 509, 954, 259, 919, 573, 782, 789, 29, 984, 397, 484, 273, 751, 1096, 1190, 367, 365, 308, 612, 441, 1034, 1112, 493, 608, 333, 255, 950, 920, 6, 712, 1214, 793, 905, 311, 1092, 953, 321, 642, 581, 946, 326, 588, 664, 275, 1131, 972, 660, 17, 766, 809, 578, 813, 728, 345, 117, 816, 947, 886, 37, 1136, 1124, 966, 700, 373, 239, 558, 968, 142, 1033, 604, 115, 749, 504, 51, 932, 725, 452, 384, 1037, 310, 59, 68, 88, 1107, 153, 819, 133, 713, 1173, 420, 334, 1174, 317, 56, 758, 1042, 910, 1040, 2, 82, 538, 506, 609, 763, 543, 783, 1056, 11, 485, 965, 470, 776, 1022, 215, 949, 701, 906, 590, 872, 958, 917, 1106, 633, 539, 130, 520, 456, 812, 1017, 556, 380, 1183, 101, 911, 182, 76, 641, 219, 901, 386, 726, 128, 709, 19, 1059, 838, 394, 787, 514, 1201, 982, 282, 870, 343, 286, 428, 1093, 1007, 253, 407, 391, 1082, 742, 229, 927, 826, 505, 943, 42, 734, 681, 748, 583, 1140, 698, 518, 242, 27, 415, 892, 645, 309, 404, 1045, 466, 298, 808, 768, 861, 649, 743, 921, 31, 765, 703, 223, 846, 369, 553, 722, 344, 772, 291, 1025, 421, 480, 555, 626, 143, 1094, 738, 632, 1057, 511, 92, 650, 621, 985, 1197, 232, 216, 780, 73, 635, 623, 794, 250, 873, 692, 90, 464, 320, 717, 889, 868, 1001, 662, 105, 347, 314, 707, 575, 1002, 177, 169, 516, 823, 296, 1012, 1134, 234, 439, 57, 67, 574, 903, 162, 357, 824, 918, 930, 201, 1203, 1122, 1020, 488, 122, 1191, 926, 959, 501, 453, 104, 261, 797, 785, 363, 876, 289, 874, 606, 203, 1154, 970, 634, 1000, 263, 495, 1208, 156, 1205, 1113, 833, 227, 655, 695, 8, 1142, 885, 977, 1137, 83, 502, 571, 1111, 22, 392, 181, 827, 77, 93, 746, 399, 534, 1031, 554]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9375447042018908
the save name prefix for this run is:  chkpt-ID_9375447042018908_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 854
rank avg (pred): 0.513 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 7.8698e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 700
rank avg (pred): 0.476 +- 0.038
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.000137502

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1214
rank avg (pred): 0.500 +- 0.290
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 6.2481e-06

Epoch over!
epoch time: 14.833

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 133
rank avg (pred): 0.477 +- 0.305
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.5767e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.488 +- 0.290
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 3.05812e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 314
rank avg (pred): 0.315 +- 0.318
mrr vals (pred, true): 0.001, 0.094
batch losses (mrrl, rdl): 0.0, 4.61583e-05

Epoch over!
epoch time: 14.773

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1013
rank avg (pred): 0.513 +- 0.285
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.15169e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 86
rank avg (pred): 0.490 +- 0.293
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 5.7782e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.504 +- 0.288
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 3.45431e-05

Epoch over!
epoch time: 14.803

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 534
rank avg (pred): 0.350 +- 0.311
mrr vals (pred, true): 0.001, 0.089
batch losses (mrrl, rdl): 0.0, 3.56314e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 538
rank avg (pred): 0.347 +- 0.309
mrr vals (pred, true): 0.001, 0.088
batch losses (mrrl, rdl): 0.0, 2.70513e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1045
rank avg (pred): 0.469 +- 0.308
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.0846e-06

Epoch over!
epoch time: 14.787

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.212 +- 0.311
mrr vals (pred, true): 0.009, 0.332
batch losses (mrrl, rdl): 0.0, 0.0001206637

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.234 +- 0.320
mrr vals (pred, true): 0.005, 0.235
batch losses (mrrl, rdl): 0.0, 7.0696e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 320
rank avg (pred): 0.321 +- 0.293
mrr vals (pred, true): 0.002, 0.115
batch losses (mrrl, rdl): 0.0, 7.9398e-06

Epoch over!
epoch time: 14.77

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 382
rank avg (pred): 0.495 +- 0.291
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0248117857, 8.3442e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.538 +- 0.345
mrr vals (pred, true): 0.025, 0.000
batch losses (mrrl, rdl): 0.0062479456, 2.638e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 232
rank avg (pred): 0.495 +- 0.312
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0003015345, 7.7121e-06

Epoch over!
epoch time: 15.161

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 306
rank avg (pred): 0.352 +- 0.338
mrr vals (pred, true): 0.094, 0.121
batch losses (mrrl, rdl): 0.007245752, 5.3947e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 325
rank avg (pred): 0.504 +- 0.322
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0029114569, 6.64194e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 20
rank avg (pred): 0.494 +- 0.296
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0039021801, 1.04216e-05

Epoch over!
epoch time: 15.18

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 223
rank avg (pred): 0.465 +- 0.277
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 9.56009e-05, 1.22413e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.423 +- 0.362
mrr vals (pred, true): 0.120, 0.014
batch losses (mrrl, rdl): 0.0489241667, 0.0008928078

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 639
rank avg (pred): 0.436 +- 0.297
mrr vals (pred, true): 0.066, 0.001
batch losses (mrrl, rdl): 0.0026801308, 9.5467e-06

Epoch over!
epoch time: 15.085

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.312 +- 0.338
mrr vals (pred, true): 0.150, 0.187
batch losses (mrrl, rdl): 0.0135754189, 6.87155e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.471 +- 0.311
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0021420307, 1.75794e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.482 +- 0.309
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009871331, 5.20537e-05

Epoch over!
epoch time: 14.963

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1000
rank avg (pred): 0.479 +- 0.314
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.001240527, 1.26518e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 952
rank avg (pred): 0.479 +- 0.300
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003723626, 2.04921e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 877
rank avg (pred): 0.499 +- 0.324
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002614477, 1.41494e-05

Epoch over!
epoch time: 14.887

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 739
rank avg (pred): 0.229 +- 0.345
mrr vals (pred, true): 0.212, 0.183
batch losses (mrrl, rdl): 0.008272808, 2.10683e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 485
rank avg (pred): 0.489 +- 0.307
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.62369e-05, 1.5092e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 492
rank avg (pred): 0.344 +- 0.346
mrr vals (pred, true): 0.103, 0.092
batch losses (mrrl, rdl): 0.0284310039, 1.7869e-05

Epoch over!
epoch time: 14.909

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 619
rank avg (pred): 0.474 +- 0.303
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0007233765, 1.0707e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 731
rank avg (pred): 0.176 +- 0.323
mrr vals (pred, true): 0.248, 0.279
batch losses (mrrl, rdl): 0.0093231145, 7.60107e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 827
rank avg (pred): 0.140 +- 0.285
mrr vals (pred, true): 0.211, 0.290
batch losses (mrrl, rdl): 0.0632322878, 7.73062e-05

Epoch over!
epoch time: 14.875

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 775
rank avg (pred): 0.480 +- 0.317
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0031419657, 3.2594e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1088
rank avg (pred): 0.492 +- 0.291
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002947258, 3.6088e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.492 +- 0.283
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0005309607, 2.747e-06

Epoch over!
epoch time: 14.962

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.505 +- 0.318
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022758713, 4.19011e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.459 +- 0.294
mrr vals (pred, true): 0.070, 0.002
batch losses (mrrl, rdl): 0.0040640701, 1.05346e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1027
rank avg (pred): 0.479 +- 0.312
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0032419381, 3.4836e-06

Epoch over!
epoch time: 15.095

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.399 +- 0.333
mrr vals (pred, true): 0.078, 0.112
batch losses (mrrl, rdl): 0.0115788095, 5.9106e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1066
rank avg (pred): 0.366 +- 0.354
mrr vals (pred, true): 0.089, 0.098
batch losses (mrrl, rdl): 0.0153781986, 3.77116e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 5
rank avg (pred): 0.533 +- 0.304
mrr vals (pred, true): 0.025, 0.000
batch losses (mrrl, rdl): 0.0063018459, 3.2927e-06

Epoch over!
epoch time: 15.109

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.498 +- 0.318
mrr vals (pred, true): 0.056, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   75 	     0 	 0.05155 	 0.00013 	 m..s
    4 	     1 	 0.03530 	 0.00015 	 m..s
   93 	     2 	 0.06117 	 0.00015 	 m..s
   36 	     3 	 0.04133 	 0.00015 	 m..s
   86 	     4 	 0.05608 	 0.00015 	 m..s
   57 	     5 	 0.04437 	 0.00015 	 m..s
   46 	     6 	 0.04249 	 0.00016 	 m..s
   68 	     7 	 0.05025 	 0.00016 	 m..s
    6 	     8 	 0.03729 	 0.00016 	 m..s
    3 	     9 	 0.02770 	 0.00017 	 ~...
   20 	    10 	 0.03944 	 0.00017 	 m..s
   38 	    11 	 0.04141 	 0.00018 	 m..s
   45 	    12 	 0.04249 	 0.00018 	 m..s
   56 	    13 	 0.04323 	 0.00019 	 m..s
   16 	    14 	 0.03905 	 0.00019 	 m..s
   34 	    15 	 0.04125 	 0.00019 	 m..s
   67 	    16 	 0.04979 	 0.00020 	 m..s
   27 	    17 	 0.04025 	 0.00020 	 m..s
   21 	    18 	 0.03952 	 0.00020 	 m..s
   88 	    19 	 0.05630 	 0.00020 	 m..s
   31 	    20 	 0.04108 	 0.00021 	 m..s
   55 	    21 	 0.04322 	 0.00021 	 m..s
   14 	    22 	 0.03853 	 0.00021 	 m..s
   46 	    23 	 0.04249 	 0.00021 	 m..s
   44 	    24 	 0.04196 	 0.00022 	 m..s
   59 	    25 	 0.04463 	 0.00022 	 m..s
   39 	    26 	 0.04141 	 0.00022 	 m..s
   41 	    27 	 0.04143 	 0.00022 	 m..s
   80 	    28 	 0.05445 	 0.00022 	 m..s
   30 	    29 	 0.04042 	 0.00023 	 m..s
   58 	    30 	 0.04444 	 0.00023 	 m..s
   11 	    31 	 0.03772 	 0.00025 	 m..s
   23 	    32 	 0.04009 	 0.00025 	 m..s
   43 	    33 	 0.04154 	 0.00026 	 m..s
   80 	    34 	 0.05445 	 0.00026 	 m..s
   71 	    35 	 0.05080 	 0.00026 	 m..s
   54 	    36 	 0.04313 	 0.00026 	 m..s
   60 	    37 	 0.04466 	 0.00027 	 m..s
   24 	    38 	 0.04015 	 0.00027 	 m..s
   48 	    39 	 0.04252 	 0.00028 	 m..s
   22 	    40 	 0.03986 	 0.00028 	 m..s
   15 	    41 	 0.03867 	 0.00028 	 m..s
    2 	    42 	 0.02762 	 0.00028 	 ~...
   37 	    43 	 0.04135 	 0.00029 	 m..s
    7 	    44 	 0.03746 	 0.00031 	 m..s
   64 	    45 	 0.04883 	 0.00031 	 m..s
   25 	    46 	 0.04021 	 0.00031 	 m..s
   17 	    47 	 0.03906 	 0.00031 	 m..s
   12 	    48 	 0.03779 	 0.00031 	 m..s
   35 	    49 	 0.04133 	 0.00032 	 m..s
   33 	    50 	 0.04118 	 0.00032 	 m..s
   29 	    51 	 0.04041 	 0.00032 	 m..s
   85 	    52 	 0.05606 	 0.00032 	 m..s
    1 	    53 	 0.02620 	 0.00033 	 ~...
   89 	    54 	 0.05916 	 0.00034 	 m..s
   90 	    55 	 0.05980 	 0.00035 	 m..s
   13 	    56 	 0.03844 	 0.00035 	 m..s
   92 	    57 	 0.06031 	 0.00036 	 m..s
   53 	    58 	 0.04290 	 0.00037 	 m..s
   40 	    59 	 0.04141 	 0.00038 	 m..s
   19 	    60 	 0.03938 	 0.00039 	 m..s
   64 	    61 	 0.04883 	 0.00041 	 m..s
   10 	    62 	 0.03769 	 0.00046 	 m..s
   49 	    63 	 0.04261 	 0.00049 	 m..s
   52 	    64 	 0.04288 	 0.00049 	 m..s
   42 	    65 	 0.04143 	 0.00050 	 m..s
   62 	    66 	 0.04619 	 0.00052 	 m..s
   63 	    67 	 0.04651 	 0.00052 	 m..s
   25 	    68 	 0.04021 	 0.00054 	 m..s
   91 	    69 	 0.05998 	 0.00055 	 m..s
   74 	    70 	 0.05121 	 0.00057 	 m..s
   50 	    71 	 0.04271 	 0.00057 	 m..s
   28 	    72 	 0.04034 	 0.00059 	 m..s
   50 	    73 	 0.04271 	 0.00062 	 m..s
   18 	    74 	 0.03936 	 0.00071 	 m..s
    8 	    75 	 0.03761 	 0.00074 	 m..s
    0 	    76 	 0.02578 	 0.00087 	 ~...
   61 	    77 	 0.04565 	 0.00088 	 m..s
    9 	    78 	 0.03763 	 0.00098 	 m..s
    5 	    79 	 0.03669 	 0.00118 	 m..s
   32 	    80 	 0.04108 	 0.00294 	 m..s
   86 	    81 	 0.05608 	 0.00544 	 m..s
   94 	    82 	 0.06330 	 0.04577 	 ~...
   77 	    83 	 0.05256 	 0.05863 	 ~...
   82 	    84 	 0.05534 	 0.06074 	 ~...
   70 	    85 	 0.05056 	 0.07577 	 ~...
   73 	    86 	 0.05093 	 0.07821 	 ~...
  101 	    87 	 0.08374 	 0.08502 	 ~...
   84 	    88 	 0.05579 	 0.08721 	 m..s
   79 	    89 	 0.05316 	 0.08788 	 m..s
   76 	    90 	 0.05254 	 0.08835 	 m..s
  100 	    91 	 0.08251 	 0.09259 	 ~...
   72 	    92 	 0.05081 	 0.09379 	 m..s
   66 	    93 	 0.04950 	 0.09528 	 m..s
   82 	    94 	 0.05534 	 0.09738 	 m..s
  107 	    95 	 0.09283 	 0.10273 	 ~...
  106 	    96 	 0.09248 	 0.10296 	 ~...
   69 	    97 	 0.05026 	 0.10629 	 m..s
  103 	    98 	 0.08744 	 0.10723 	 ~...
   95 	    99 	 0.06981 	 0.10792 	 m..s
   78 	   100 	 0.05257 	 0.10832 	 m..s
  107 	   101 	 0.09283 	 0.12444 	 m..s
   99 	   102 	 0.08160 	 0.12530 	 m..s
  102 	   103 	 0.08713 	 0.12813 	 m..s
  104 	   104 	 0.08766 	 0.13084 	 m..s
   97 	   105 	 0.07925 	 0.13328 	 m..s
  105 	   106 	 0.09224 	 0.14003 	 m..s
  114 	   107 	 0.19274 	 0.14436 	 m..s
  109 	   108 	 0.11352 	 0.14537 	 m..s
   95 	   109 	 0.06981 	 0.14850 	 m..s
  120 	   110 	 0.23670 	 0.15118 	 m..s
  113 	   111 	 0.18393 	 0.17022 	 ~...
  119 	   112 	 0.23588 	 0.17303 	 m..s
   98 	   113 	 0.08139 	 0.18336 	 MISS
  110 	   114 	 0.11800 	 0.18809 	 m..s
  111 	   115 	 0.14776 	 0.19818 	 m..s
  114 	   116 	 0.19274 	 0.22047 	 ~...
  112 	   117 	 0.18265 	 0.22067 	 m..s
  116 	   118 	 0.19408 	 0.23924 	 m..s
  117 	   119 	 0.21626 	 0.29656 	 m..s
  117 	   120 	 0.21626 	 0.32819 	 MISS
==========================================
r_mrr = 0.8551116585731506
r2_mrr = 0.6052001118659973
spearmanr_mrr@5 = 0.8673897385597229
spearmanr_mrr@10 = 0.883248507976532
spearmanr_mrr@50 = 0.8959188461303711
spearmanr_mrr@100 = 0.918463408946991
spearmanr_mrr@All = 0.9239688515663147
==========================================
test time: 0.452
Done Testing dataset DBpedia50
total time taken: 230.0069944858551
training time taken: 224.6559727191925
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8551)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.6052)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.8674)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8832)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.8959)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9185)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9240)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.9310967013657319}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 5928191506465951
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1135, 1199, 168, 429, 1120, 827, 387, 1206, 760, 331, 852, 1108, 686, 344, 118, 819, 573, 198, 963, 935, 650, 1067, 718, 40, 841, 223, 643, 422, 156, 237, 67, 329, 1211, 473, 1064, 84, 1082, 224, 350, 663, 17, 1087, 561, 1142, 96, 332, 315, 811, 971, 375, 1198, 1008, 29, 820, 221, 220, 5, 148, 1066, 238, 404, 201, 778, 232, 688, 144, 1146, 165, 1012, 1157, 879, 873, 709, 536, 277, 882, 1197, 498, 524, 1192, 63, 788, 931, 653, 1193, 771, 388, 151, 670, 863, 1007, 636, 314, 776, 728, 496, 349, 185, 124, 494, 732, 154, 541, 1075, 1098, 533, 504, 1138, 587, 393, 607, 1046, 854, 608, 1116, 825, 1140, 469, 559, 463, 929]
valid_ids (0): []
train_ids (1094): [1154, 1158, 697, 1004, 91, 359, 453, 632, 545, 209, 822, 327, 1043, 896, 952, 522, 680, 932, 729, 638, 207, 1041, 720, 48, 1048, 659, 321, 1028, 250, 475, 691, 791, 217, 481, 1065, 526, 601, 540, 745, 304, 438, 295, 600, 1056, 972, 391, 1024, 953, 99, 702, 976, 574, 1015, 73, 960, 141, 779, 907, 1068, 758, 560, 282, 310, 877, 1175, 131, 136, 507, 108, 994, 384, 80, 754, 377, 285, 137, 228, 83, 525, 354, 342, 252, 229, 780, 1152, 334, 748, 895, 642, 1011, 950, 455, 208, 848, 657, 180, 172, 1190, 757, 707, 1052, 139, 58, 916, 996, 117, 894, 1039, 385, 1110, 1131, 843, 1059, 170, 286, 1061, 736, 737, 428, 549, 1009, 472, 1047, 322, 248, 263, 784, 613, 122, 1148, 1074, 528, 818, 869, 126, 551, 897, 200, 20, 598, 694, 586, 785, 1072, 700, 685, 970, 977, 660, 1101, 1117, 408, 159, 288, 502, 793, 1111, 75, 775, 566, 509, 905, 888, 458, 519, 406, 413, 734, 465, 437, 770, 163, 986, 973, 520, 471, 581, 743, 340, 205, 495, 503, 347, 805, 910, 1121, 814, 476, 182, 324, 213, 405, 801, 564, 530, 28, 606, 1194, 578, 382, 262, 731, 13, 176, 677, 287, 576, 462, 77, 113, 152, 56, 162, 1070, 202, 194, 865, 920, 762, 817, 1210, 1164, 468, 21, 480, 1174, 1038, 962, 1079, 904, 641, 1153, 542, 240, 585, 426, 487, 478, 662, 49, 204, 22, 257, 753, 150, 1163, 491, 749, 1034, 701, 403, 1005, 992, 348, 328, 1107, 1205, 1088, 974, 192, 810, 596, 695, 1027, 42, 925, 1159, 772, 269, 889, 1080, 605, 312, 673, 804, 206, 203, 357, 837, 103, 956, 1089, 866, 450, 553, 712, 816, 308, 787, 957, 272, 386, 571, 371, 740, 742, 1172, 1014, 179, 330, 409, 23, 547, 16, 730, 158, 868, 421, 27, 146, 281, 72, 693, 361, 190, 1026, 548, 479, 1203, 1020, 713, 0, 358, 1130, 510, 1053, 1186, 696, 254, 887, 432, 947, 672, 452, 618, 768, 592, 32, 993, 639, 698, 111, 107, 355, 412, 800, 690, 1050, 1109, 591, 912, 433, 1042, 774, 764, 270, 726, 903, 1195, 50, 214, 362, 682, 197, 565, 95, 1171, 915, 186, 86, 24, 485, 537, 517, 934, 1168, 937, 69, 187, 1181, 265, 338, 374, 913, 647, 215, 1045, 946, 750, 497, 477, 1165, 18, 630, 756, 216, 1208, 851, 1114, 85, 292, 160, 789, 516, 134, 1123, 1118, 3, 1188, 30, 311, 886, 420, 1051, 918, 961, 241, 500, 181, 1083, 944, 235, 460, 449, 558, 634, 142, 431, 101, 246, 759, 346, 948, 501, 105, 883, 773, 893, 747, 554, 114, 400, 424, 876, 430, 583, 1139, 684, 703, 880, 562, 674, 279, 615, 809, 802, 299, 555, 881, 710, 267, 398, 1169, 864, 383, 389, 1161, 1078, 826, 378, 445, 43, 301, 1115, 1113, 425, 951, 518, 620, 579, 751, 723, 699, 922, 7, 37, 1173, 942, 652, 411, 644, 47, 336, 130, 1177, 651, 39, 1071, 1006, 1129, 623, 563, 1150, 390, 899, 1000, 399, 352, 489, 923, 293, 927, 26, 439, 323, 326, 178, 123, 1094, 557, 648, 884, 1214, 668, 964, 175, 1097, 546, 514, 829, 1081, 132, 366, 1049, 177, 872, 844, 830, 459, 975, 924, 183, 997, 980, 78, 61, 813, 1187, 998, 376, 656, 834, 954, 654, 1044, 264, 637, 544, 102, 958, 898, 319, 523, 681, 309, 965, 725, 616, 1084, 812, 933, 236, 1145, 906, 320, 368, 276, 531, 1162, 949, 10, 715, 1143, 741, 1191, 402, 512, 1076, 506, 333, 850, 414, 143, 629, 885, 38, 360, 856, 291, 1010, 1054, 1144, 225, 917, 149, 717, 396, 33, 456, 1096, 278, 302, 714, 840, 110, 767, 233, 724, 138, 577, 945, 534, 938, 766, 679, 369, 646, 1037, 247, 870, 335, 53, 966, 230, 318, 987, 835, 461, 129, 1207, 515, 658, 727, 1141, 892, 832, 1085, 1212, 849, 188, 256, 82, 51, 307, 604, 484, 664, 511, 853, 66, 54, 1147, 397, 1002, 116, 249, 457, 539, 88, 1178, 704, 735, 1092, 1030, 1200, 259, 943, 443, 1077, 1156, 394, 763, 1196, 245, 955, 807, 1183, 164, 752, 1023, 226, 171, 687, 1179, 588, 255, 1204, 610, 365, 45, 1073, 1100, 446, 31, 1069, 1060, 621, 100, 273, 676, 55, 1003, 87, 499, 9, 434, 1032, 612, 928, 984, 353, 550, 1018, 1149, 1151, 1189, 675, 625, 4, 1036, 106, 379, 859, 372, 417, 678, 441, 985, 351, 380, 769, 593, 59, 570, 218, 999, 968, 527, 8, 1033, 1134, 661, 930, 711, 1025, 871, 283, 891, 597, 1106, 153, 1063, 846, 665, 683, 855, 862, 76, 614, 967, 483, 602, 874, 305, 275, 1013, 1155, 983, 25, 234, 645, 1095, 568, 341, 211, 532, 622, 538, 1160, 911, 133, 173, 444, 1091, 231, 300, 1119, 782, 339, 721, 556, 251, 1176, 959, 15, 505, 990, 823, 890, 161, 940, 1167, 435, 1132, 492, 68, 781, 189, 1090, 392, 89, 222, 41, 803, 655, 1099, 258, 914, 635, 35, 795, 1055, 482, 464, 169, 640, 765, 529, 589, 1209, 120, 119, 988, 594, 611, 590, 11, 1062, 135, 689, 628, 1105, 12, 280, 466, 582, 706, 839, 261, 979, 196, 1125, 1136, 343, 1133, 57, 70, 857, 908, 325, 777, 6, 109, 289, 1126, 395, 410, 284, 867, 239, 794, 166, 381, 62, 290, 79, 244, 761, 294, 666, 833, 1127, 790, 858, 808, 436, 847, 861, 242, 1180, 755, 978, 824, 36, 508, 671, 1137, 991, 609, 364, 253, 1166, 448, 1016, 274, 367, 692, 792, 93, 191, 902, 783, 838, 1201, 667, 2, 617, 467, 34, 941, 981, 580, 939, 266, 901, 603, 212, 1040, 535, 451, 271, 454, 1035, 155, 97, 969, 370, 174, 52, 313, 297, 1029, 14, 722, 926, 401, 631, 81, 786, 633, 1001, 71, 860, 875, 316, 112, 486, 1019, 1093, 1, 921, 799, 64, 806, 317, 1104, 490, 104, 1086, 708, 982, 1124, 1031, 418, 828, 243, 145, 716, 419, 298, 909, 919, 1185, 738, 74, 744, 157, 575, 94, 797, 626, 552, 669, 845, 989, 1170, 1057, 219, 831, 337, 878, 569, 427, 1128, 345, 268, 733, 415, 798, 140, 46, 193, 65, 649, 842, 184, 1184, 115, 199, 296, 373, 900, 227, 1103, 260, 1182, 167, 1017, 1022, 44, 440, 1112, 1021, 493, 407, 599, 739, 521, 543, 1122, 125, 121, 1213, 719, 619, 513, 936, 836, 1202, 705, 1102, 92, 470, 447, 60, 442, 595, 363, 356, 416, 584, 474, 567, 147, 796, 1058, 19, 572, 90, 815, 488, 627, 127, 624, 210, 306, 746, 98, 128, 195, 995, 303, 821, 423]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8921355145104759
the save name prefix for this run is:  chkpt-ID_8921355145104759_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 467
rank avg (pred): 0.527 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001243557

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1011
rank avg (pred): 0.466 +- 0.250
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0, 3.83392e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.291 +- 0.207
mrr vals (pred, true): 0.202, 0.144
batch losses (mrrl, rdl): 0.0, 0.0001270314

Epoch over!
epoch time: 14.972

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 321
rank avg (pred): 0.344 +- 0.232
mrr vals (pred, true): 0.199, 0.134
batch losses (mrrl, rdl): 0.0, 0.0002185988

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.344 +- 0.265
mrr vals (pred, true): 0.202, 0.102
batch losses (mrrl, rdl): 0.0, 3.826e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 74
rank avg (pred): 0.332 +- 0.264
mrr vals (pred, true): 0.202, 0.068
batch losses (mrrl, rdl): 0.0, 3.21098e-05

Epoch over!
epoch time: 14.944

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 42
rank avg (pred): 0.314 +- 0.265
mrr vals (pred, true): 0.215, 0.066
batch losses (mrrl, rdl): 0.0, 2.31129e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 700
rank avg (pred): 0.481 +- 0.290
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0, 1.09171e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.235 +- 0.215
mrr vals (pred, true): 0.217, 0.144
batch losses (mrrl, rdl): 0.0, 2.47881e-05

Epoch over!
epoch time: 14.954

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1190
rank avg (pred): 0.460 +- 0.300
mrr vals (pred, true): 0.146, 0.000
batch losses (mrrl, rdl): 0.0, 4.61638e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 22
rank avg (pred): 0.360 +- 0.308
mrr vals (pred, true): 0.212, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001638942

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 691
rank avg (pred): 0.458 +- 0.298
mrr vals (pred, true): 0.170, 0.000
batch losses (mrrl, rdl): 0.0, 8.5833e-06

Epoch over!
epoch time: 14.94

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 600
rank avg (pred): 0.463 +- 0.299
mrr vals (pred, true): 0.156, 0.000
batch losses (mrrl, rdl): 0.0, 1.29908e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.444 +- 0.301
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.0, 5.8465e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 619
rank avg (pred): 0.466 +- 0.299
mrr vals (pred, true): 0.140, 0.000
batch losses (mrrl, rdl): 0.0, 8.553e-06

Epoch over!
epoch time: 14.932

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1038
rank avg (pred): 0.489 +- 0.283
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0005358068, 1.1385e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 699
rank avg (pred): 0.523 +- 0.221
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.8988e-06, 8.46706e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 865
rank avg (pred): 0.531 +- 0.192
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.18646e-05, 4.42016e-05

Epoch over!
epoch time: 15.141

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 578
rank avg (pred): 0.506 +- 0.208
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.9453e-06, 5.06975e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 900
rank avg (pred): 0.513 +- 0.235
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 5e-09, 7.33125e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 649
rank avg (pred): 0.466 +- 0.163
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.28945e-05, 5.07707e-05

Epoch over!
epoch time: 15.111

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.497 +- 0.130
mrr vals (pred, true): 0.019, 0.001
batch losses (mrrl, rdl): 0.0097167892, 8.50002e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 68
rank avg (pred): 0.440 +- 0.174
mrr vals (pred, true): 0.049, 0.100
batch losses (mrrl, rdl): 4.2654e-06, 0.0004553537

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 212
rank avg (pred): 0.470 +- 0.135
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.69573e-05, 0.0001227592

Epoch over!
epoch time: 15.139

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 881
rank avg (pred): 0.505 +- 0.209
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.02307e-05, 0.0001035643

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.459 +- 0.156
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.26065e-05, 0.0001148482

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.490 +- 0.211
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 5.832e-06, 3.74814e-05

Epoch over!
epoch time: 15.102

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 402
rank avg (pred): 0.466 +- 0.220
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006705325, 2.58207e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.545 +- 0.160
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0001144234, 8.23336e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 206
rank avg (pred): 0.423 +- 0.137
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.48973e-05, 0.0002081441

Epoch over!
epoch time: 15.108

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.514 +- 0.180
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.28695e-05, 0.0001541801

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 576
rank avg (pred): 0.442 +- 0.193
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 4.7539e-06, 5.43935e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 163
rank avg (pred): 0.466 +- 0.146
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.35486e-05, 8.43346e-05

Epoch over!
epoch time: 15.099

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 705
rank avg (pred): 0.492 +- 0.132
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.91616e-05, 4.95762e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.449 +- 0.139
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.30317e-05, 6.05896e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 704
rank avg (pred): 0.491 +- 0.128
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 2.01568e-05, 8.24623e-05

Epoch over!
epoch time: 15.108

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1102
rank avg (pred): 0.472 +- 0.177
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.00164e-05, 7.12814e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 347
rank avg (pred): 0.410 +- 0.222
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0037495336, 0.00013862

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 240
rank avg (pred): 0.459 +- 0.146
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.39923e-05, 7.48975e-05

Epoch over!
epoch time: 15.005

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 14
rank avg (pred): 0.372 +- 0.186
mrr vals (pred, true): 0.073, 0.001
batch losses (mrrl, rdl): 0.0053239698, 0.0003892849

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.499 +- 0.169
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.31657e-05, 7.48619e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1092
rank avg (pred): 0.487 +- 0.174
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.3756e-06, 5.14341e-05

Epoch over!
epoch time: 14.897

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 376
rank avg (pred): 0.455 +- 0.204
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 7.5632e-06, 4.83389e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 548
rank avg (pred): 0.363 +- 0.162
mrr vals (pred, true): 0.050, 0.126
batch losses (mrrl, rdl): 0.0570008084, 8.43565e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 6
rank avg (pred): 0.348 +- 0.168
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001225587, 0.0004939315

Epoch over!
epoch time: 14.889

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.401 +- 0.215
mrr vals (pred, true): 0.133, 0.121

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   62 	     0 	 0.04919 	 0.00015 	 m..s
   61 	     1 	 0.04909 	 0.00015 	 m..s
    8 	     2 	 0.04845 	 0.00016 	 m..s
   84 	     3 	 0.05008 	 0.00016 	 m..s
   63 	     4 	 0.04920 	 0.00017 	 m..s
   15 	     5 	 0.04853 	 0.00017 	 m..s
   14 	     6 	 0.04851 	 0.00017 	 m..s
   82 	     7 	 0.04998 	 0.00018 	 m..s
   76 	     8 	 0.04960 	 0.00018 	 m..s
   60 	     9 	 0.04907 	 0.00018 	 m..s
   48 	    10 	 0.04894 	 0.00018 	 m..s
   28 	    11 	 0.04882 	 0.00018 	 m..s
   50 	    12 	 0.04896 	 0.00018 	 m..s
   11 	    13 	 0.04847 	 0.00018 	 m..s
   27 	    14 	 0.04881 	 0.00018 	 m..s
   51 	    15 	 0.04896 	 0.00019 	 m..s
   17 	    16 	 0.04856 	 0.00019 	 m..s
   46 	    17 	 0.04891 	 0.00019 	 m..s
   21 	    18 	 0.04865 	 0.00020 	 m..s
    7 	    19 	 0.04845 	 0.00020 	 m..s
   56 	    20 	 0.04900 	 0.00020 	 m..s
   49 	    21 	 0.04895 	 0.00020 	 m..s
   16 	    22 	 0.04854 	 0.00020 	 m..s
   59 	    23 	 0.04904 	 0.00021 	 m..s
   78 	    24 	 0.04962 	 0.00021 	 m..s
   30 	    25 	 0.04882 	 0.00021 	 m..s
   26 	    26 	 0.04877 	 0.00021 	 m..s
   73 	    27 	 0.04948 	 0.00021 	 m..s
   18 	    28 	 0.04856 	 0.00022 	 m..s
   55 	    29 	 0.04898 	 0.00022 	 m..s
    0 	    30 	 0.04827 	 0.00022 	 m..s
   88 	    31 	 0.05538 	 0.00022 	 m..s
   13 	    32 	 0.04849 	 0.00022 	 m..s
   37 	    33 	 0.04888 	 0.00022 	 m..s
   79 	    34 	 0.04979 	 0.00022 	 m..s
   24 	    35 	 0.04876 	 0.00023 	 m..s
   64 	    36 	 0.04929 	 0.00023 	 m..s
   12 	    37 	 0.04848 	 0.00023 	 m..s
   45 	    38 	 0.04890 	 0.00024 	 m..s
   25 	    39 	 0.04876 	 0.00024 	 m..s
   40 	    40 	 0.04888 	 0.00024 	 m..s
    8 	    41 	 0.04845 	 0.00024 	 m..s
   33 	    42 	 0.04883 	 0.00025 	 m..s
   57 	    43 	 0.04902 	 0.00025 	 m..s
   31 	    44 	 0.04882 	 0.00025 	 m..s
   87 	    45 	 0.05292 	 0.00025 	 m..s
   95 	    46 	 0.06057 	 0.00025 	 m..s
   97 	    47 	 0.06280 	 0.00025 	 m..s
   10 	    48 	 0.04846 	 0.00026 	 m..s
   39 	    49 	 0.04888 	 0.00027 	 m..s
    5 	    50 	 0.04840 	 0.00027 	 m..s
   19 	    51 	 0.04863 	 0.00028 	 m..s
   22 	    52 	 0.04870 	 0.00028 	 m..s
    2 	    53 	 0.04836 	 0.00028 	 m..s
   58 	    54 	 0.04904 	 0.00028 	 m..s
   66 	    55 	 0.04930 	 0.00029 	 m..s
   74 	    56 	 0.04949 	 0.00029 	 m..s
   23 	    57 	 0.04871 	 0.00029 	 m..s
   32 	    58 	 0.04882 	 0.00030 	 m..s
   40 	    59 	 0.04888 	 0.00032 	 m..s
   85 	    60 	 0.05027 	 0.00033 	 m..s
   36 	    61 	 0.04885 	 0.00034 	 m..s
   20 	    62 	 0.04863 	 0.00034 	 m..s
   53 	    63 	 0.04897 	 0.00034 	 m..s
   86 	    64 	 0.05251 	 0.00035 	 m..s
   90 	    65 	 0.05764 	 0.00035 	 m..s
   42 	    66 	 0.04888 	 0.00035 	 m..s
   67 	    67 	 0.04932 	 0.00037 	 m..s
    4 	    68 	 0.04837 	 0.00037 	 m..s
   80 	    69 	 0.04984 	 0.00040 	 m..s
   95 	    70 	 0.06057 	 0.00041 	 m..s
   54 	    71 	 0.04898 	 0.00042 	 m..s
   43 	    72 	 0.04889 	 0.00043 	 m..s
   72 	    73 	 0.04947 	 0.00047 	 m..s
    5 	    74 	 0.04840 	 0.00048 	 m..s
   77 	    75 	 0.04961 	 0.00048 	 m..s
   38 	    76 	 0.04888 	 0.00048 	 m..s
   44 	    77 	 0.04890 	 0.00049 	 m..s
   94 	    78 	 0.05975 	 0.00050 	 m..s
   47 	    79 	 0.04893 	 0.00070 	 m..s
   82 	    80 	 0.04998 	 0.00088 	 m..s
    2 	    81 	 0.04836 	 0.00098 	 m..s
    1 	    82 	 0.04831 	 0.00100 	 m..s
   81 	    83 	 0.04991 	 0.00104 	 m..s
   34 	    84 	 0.04883 	 0.00118 	 m..s
   52 	    85 	 0.04897 	 0.00166 	 m..s
   28 	    86 	 0.04882 	 0.00177 	 m..s
   35 	    87 	 0.04884 	 0.00285 	 m..s
   92 	    88 	 0.05895 	 0.05863 	 ~...
   68 	    89 	 0.04937 	 0.07525 	 ~...
   69 	    90 	 0.04940 	 0.07577 	 ~...
  109 	    91 	 0.12515 	 0.08100 	 m..s
   93 	    92 	 0.05953 	 0.08743 	 ~...
   89 	    93 	 0.05745 	 0.08821 	 m..s
   91 	    94 	 0.05810 	 0.09013 	 m..s
   98 	    95 	 0.06407 	 0.09016 	 ~...
   99 	    96 	 0.07384 	 0.09366 	 ~...
   71 	    97 	 0.04944 	 0.09379 	 m..s
  100 	    98 	 0.08282 	 0.09443 	 ~...
   75 	    99 	 0.04955 	 0.09694 	 m..s
  102 	   100 	 0.08452 	 0.09774 	 ~...
   70 	   101 	 0.04943 	 0.09778 	 m..s
  106 	   102 	 0.10161 	 0.10723 	 ~...
  104 	   103 	 0.09191 	 0.10821 	 ~...
  105 	   104 	 0.09961 	 0.11308 	 ~...
   65 	   105 	 0.04929 	 0.11351 	 m..s
  101 	   106 	 0.08436 	 0.11430 	 ~...
  108 	   107 	 0.10781 	 0.12018 	 ~...
  110 	   108 	 0.13253 	 0.12051 	 ~...
  103 	   109 	 0.09036 	 0.13328 	 m..s
  112 	   110 	 0.16848 	 0.13868 	 ~...
  111 	   111 	 0.14441 	 0.14603 	 ~...
  107 	   112 	 0.10285 	 0.14850 	 m..s
  115 	   113 	 0.19600 	 0.15118 	 m..s
  113 	   114 	 0.19259 	 0.18054 	 ~...
  116 	   115 	 0.20006 	 0.18814 	 ~...
  114 	   116 	 0.19587 	 0.19401 	 ~...
  118 	   117 	 0.20631 	 0.26108 	 m..s
  117 	   118 	 0.20346 	 0.29022 	 m..s
  119 	   119 	 0.27689 	 0.30915 	 m..s
  120 	   120 	 0.31847 	 0.38067 	 m..s
==========================================
r_mrr = 0.9125074148178101
r2_mrr = 0.5898690223693848
spearmanr_mrr@5 = 0.8668847680091858
spearmanr_mrr@10 = 0.899310290813446
spearmanr_mrr@50 = 0.9273886680603027
spearmanr_mrr@100 = 0.9355089068412781
spearmanr_mrr@All = 0.9372965097427368
==========================================
test time: 0.452
Done Testing dataset DBpedia50
total time taken: 231.1166477203369
training time taken: 225.80396246910095
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.9125)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.5899)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.8669)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8993)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9274)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9355)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9373)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.3926687067578314}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 3656779935486475
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [710, 466, 94, 638, 630, 732, 1135, 12, 485, 917, 27, 567, 415, 798, 830, 430, 958, 805, 660, 3, 521, 883, 327, 1130, 160, 729, 375, 1039, 557, 525, 494, 1067, 149, 661, 127, 357, 67, 680, 559, 666, 909, 442, 98, 1208, 433, 482, 65, 329, 460, 389, 89, 1085, 891, 642, 37, 1156, 617, 134, 48, 1174, 1004, 514, 93, 236, 1012, 137, 202, 694, 565, 508, 1209, 586, 759, 571, 69, 1066, 368, 1010, 331, 185, 701, 427, 859, 676, 394, 1098, 1026, 214, 738, 980, 689, 218, 831, 74, 1038, 566, 1069, 481, 601, 937, 151, 1163, 843, 1139, 355, 795, 869, 714, 32, 991, 363, 188, 632, 440, 416, 510, 952, 600, 498, 285, 83]
valid_ids (0): []
train_ids (1094): [1161, 682, 432, 106, 1164, 652, 441, 1013, 1118, 204, 112, 421, 1158, 988, 615, 1045, 283, 408, 211, 298, 978, 428, 92, 708, 944, 20, 742, 890, 913, 237, 1192, 324, 999, 108, 1047, 84, 1078, 1180, 463, 696, 450, 813, 266, 1068, 634, 70, 195, 295, 502, 374, 380, 531, 376, 971, 217, 471, 6, 29, 72, 781, 1083, 183, 1091, 912, 116, 81, 523, 1058, 301, 1185, 91, 670, 597, 168, 45, 964, 386, 848, 246, 648, 306, 656, 575, 817, 960, 1189, 393, 297, 1015, 947, 626, 14, 262, 101, 359, 837, 613, 325, 612, 923, 123, 943, 1082, 270, 103, 28, 1030, 500, 51, 370, 852, 1213, 871, 728, 649, 849, 725, 529, 975, 570, 910, 286, 665, 470, 903, 130, 469, 900, 867, 250, 113, 125, 782, 518, 820, 167, 233, 931, 1061, 724, 1207, 562, 1119, 181, 981, 955, 114, 86, 175, 267, 1194, 274, 886, 780, 135, 68, 1152, 300, 1172, 802, 213, 414, 207, 377, 655, 110, 778, 599, 1166, 367, 1142, 396, 606, 1199, 221, 703, 1170, 815, 954, 10, 446, 503, 877, 438, 1132, 337, 951, 646, 1196, 1147, 695, 138, 902, 105, 866, 834, 678, 497, 1086, 429, 119, 49, 953, 273, 618, 147, 206, 8, 1020, 1198, 1022, 894, 171, 602, 1044, 82, 225, 657, 673, 1017, 773, 833, 255, 174, 946, 145, 771, 814, 131, 46, 540, 179, 459, 102, 115, 53, 1033, 777, 737, 1094, 397, 1205, 1137, 687, 893, 936, 664, 604, 263, 1141, 581, 691, 487, 475, 1001, 406, 969, 751, 766, 230, 328, 1165, 749, 1120, 767, 579, 854, 1202, 240, 228, 349, 58, 924, 361, 139, 700, 530, 304, 1167, 38, 161, 2, 884, 824, 1053, 208, 986, 685, 410, 919, 22, 823, 532, 323, 401, 1123, 451, 847, 260, 807, 774, 492, 1128, 249, 922, 876, 744, 80, 305, 758, 23, 461, 107, 133, 775, 1150, 472, 353, 1063, 940, 42, 752, 1071, 637, 519, 235, 855, 1028, 520, 914, 573, 743, 57, 607, 403, 874, 704, 1102, 157, 1089, 1134, 1140, 662, 76, 166, 572, 790, 13, 395, 611, 568, 111, 636, 173, 1182, 289, 1151, 897, 987, 265, 199, 243, 278, 330, 495, 30, 287, 765, 473, 968, 339, 1073, 276, 580, 832, 784, 535, 934, 484, 281, 735, 383, 1116, 1023, 945, 681, 534, 811, 995, 268, 1007, 818, 574, 1181, 994, 851, 242, 616, 949, 513, 1168, 335, 210, 1065, 610, 245, 50, 804, 512, 722, 1059, 939, 545, 162, 789, 479, 1169, 41, 576, 1201, 1077, 344, 658, 1084, 1060, 384, 85, 1197, 311, 966, 378, 194, 750, 192, 1186, 358, 881, 672, 24, 476, 1131, 1173, 762, 862, 159, 983, 925, 589, 164, 794, 850, 785, 120, 290, 153, 1040, 15, 957, 1112, 241, 1021, 400, 1187, 1179, 1111, 1100, 1117, 382, 619, 60, 596, 1121, 223, 148, 1088, 836, 1153, 587, 1109, 1008, 78, 727, 561, 187, 603, 44, 803, 317, 522, 464, 756, 62, 822, 956, 845, 180, 716, 352, 643, 748, 984, 1171, 746, 1200, 783, 515, 592, 1075, 1003, 719, 1043, 840, 552, 184, 439, 1029, 40, 763, 392, 918, 261, 1037, 190, 163, 1146, 216, 1096, 404, 941, 282, 764, 313, 1154, 950, 52, 549, 399, 583, 713, 810, 829, 588, 935, 741, 129, 537, 55, 435, 1035, 828, 827, 799, 277, 996, 788, 684, 77, 248, 809, 244, 547, 1203, 598, 419, 585, 1122, 875, 1024, 226, 591, 257, 819, 594, 1042, 1124, 1108, 930, 455, 373, 64, 25, 620, 683, 1177, 1025, 916, 388, 315, 1101, 516, 635, 538, 808, 1188, 99, 787, 739, 895, 1211, 1110, 972, 674, 911, 640, 232, 797, 870, 677, 962, 431, 791, 1148, 842, 318, 959, 271, 170, 1133, 772, 224, 563, 34, 178, 320, 141, 452, 1081, 543, 668, 75, 182, 730, 448, 348, 4, 292, 1032, 1155, 443, 1127, 307, 189, 745, 1178, 191, 288, 905, 556, 321, 627, 709, 1079, 360, 66, 21, 504, 1074, 711, 1175, 857, 546, 569, 761, 932, 928, 1070, 254, 5, 688, 888, 693, 346, 861, 736, 126, 39, 1019, 593, 18, 553, 345, 391, 542, 731, 172, 467, 885, 1097, 524, 528, 308, 926, 155, 539, 247, 229, 825, 338, 302, 663, 31, 1064, 907, 47, 990, 631, 507, 898, 222, 333, 754, 623, 1195, 251, 614, 326, 203, 734, 465, 1095, 284, 63, 872, 793, 985, 712, 417, 906, 558, 239, 921, 517, 118, 371, 993, 365, 56, 1105, 733, 920, 154, 1184, 456, 444, 858, 555, 1190, 1036, 16, 671, 422, 364, 1176, 136, 122, 35, 454, 316, 697, 977, 908, 659, 1093, 294, 839, 757, 800, 54, 720, 96, 1062, 550, 901, 806, 641, 197, 915, 132, 760, 369, 879, 590, 629, 965, 1055, 645, 90, 156, 755, 970, 457, 622, 87, 434, 835, 411, 212, 379, 1113, 165, 420, 948, 124, 489, 726, 499, 1049, 989, 342, 973, 979, 385, 1092, 1183, 405, 816, 334, 942, 253, 200, 398, 1072, 275, 1002, 490, 846, 486, 477, 356, 1099, 1005, 1051, 279, 699, 1149, 625, 1126, 340, 209, 312, 605, 462, 1076, 653, 1048, 483, 844, 1046, 740, 792, 582, 838, 826, 201, 496, 1212, 480, 1103, 595, 474, 647, 882, 1125, 453, 309, 97, 1057, 1162, 1204, 564, 33, 584, 1, 707, 747, 9, 150, 272, 675, 1157, 215, 753, 887, 578, 1018, 1006, 445, 227, 544, 468, 1052, 413, 390, 1210, 1136, 896, 639, 258, 491, 144, 967, 280, 177, 776, 424, 865, 982, 880, 821, 624, 169, 650, 501, 142, 853, 95, 702, 205, 193, 362, 1000, 577, 1114, 1016, 1206, 259, 158, 873, 61, 963, 0, 878, 718, 447, 1193, 768, 1145, 992, 293, 933, 196, 1009, 509, 402, 997, 899, 79, 644, 234, 140, 73, 238, 252, 88, 1214, 43, 651, 864, 366, 146, 1143, 36, 104, 109, 1191, 1027, 927, 426, 381, 17, 770, 291, 1087, 220, 667, 387, 351, 723, 998, 621, 449, 863, 929, 121, 974, 299, 341, 198, 548, 11, 319, 526, 264, 1144, 536, 860, 554, 633, 628, 1138, 889, 1014, 527, 303, 609, 705, 493, 186, 1011, 7, 1104, 1080, 1159, 436, 706, 219, 343, 541, 892, 269, 347, 698, 296, 100, 856, 1107, 769, 1106, 1031, 721, 176, 412, 314, 407, 715, 505, 812, 1054, 152, 1115, 425, 117, 1050, 322, 654, 551, 26, 801, 437, 717, 256, 786, 59, 332, 1034, 533, 868, 904, 143, 669, 1090, 478, 372, 679, 418, 690, 511, 1056, 841, 938, 336, 458, 19, 350, 796, 71, 560, 310, 354, 961, 779, 231, 1160, 423, 409, 128, 1129, 608, 488, 506, 692, 1041, 686, 976]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3262413142098884
the save name prefix for this run is:  chkpt-ID_3262413142098884_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 976
rank avg (pred): 0.535 +- 0.006
mrr vals (pred, true): 0.000, 0.118
batch losses (mrrl, rdl): 0.0, 0.0012870305

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1195
rank avg (pred): 0.497 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.93421e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.493 +- 0.006
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001096726

Epoch over!
epoch time: 14.836

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.485 +- 0.004
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001111023

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.500 +- 0.227
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 2.53941e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.378 +- 0.319
mrr vals (pred, true): 0.003, 0.075
batch losses (mrrl, rdl): 0.0, 6.73659e-05

Epoch over!
epoch time: 14.761

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 677
rank avg (pred): 0.501 +- 0.299
mrr vals (pred, true): 0.003, 0.000
batch losses (mrrl, rdl): 0.0, 6.4497e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 232
rank avg (pred): 0.500 +- 0.299
mrr vals (pred, true): 0.006, 0.000
batch losses (mrrl, rdl): 0.0, 7.3016e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 750
rank avg (pred): 0.252 +- 0.356
mrr vals (pred, true): 0.049, 0.235
batch losses (mrrl, rdl): 0.0, 1.61981e-05

Epoch over!
epoch time: 14.763

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.516 +- 0.308
mrr vals (pred, true): 0.005, 0.000
batch losses (mrrl, rdl): 0.0, 1.4002e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.503 +- 0.304
mrr vals (pred, true): 0.021, 0.001
batch losses (mrrl, rdl): 0.0, 5.15511e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1122
rank avg (pred): 0.496 +- 0.298
mrr vals (pred, true): 0.022, 0.000
batch losses (mrrl, rdl): 0.0, 1.843e-06

Epoch over!
epoch time: 14.866

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1162
rank avg (pred): 0.498 +- 0.299
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0, 1.54528e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 840
rank avg (pred): 0.512 +- 0.312
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0, 2.4425e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 109
rank avg (pred): 0.500 +- 0.299
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0, 5.4951e-06

Epoch over!
epoch time: 14.935

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 605
rank avg (pred): 0.504 +- 0.303
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0193437468, 1.88922e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 352
rank avg (pred): 0.533 +- 0.330
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.47337e-05, 1.29491e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1165
rank avg (pred): 0.513 +- 0.329
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0002005118, 3.05193e-05

Epoch over!
epoch time: 14.925

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 462
rank avg (pred): 0.509 +- 0.315
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001030135, 1.8391e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 705
rank avg (pred): 0.563 +- 0.363
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0036121255, 7.89565e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 923
rank avg (pred): 0.469 +- 0.359
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 8.6815e-06, 5.43825e-05

Epoch over!
epoch time: 14.861

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 766
rank avg (pred): 0.448 +- 0.364
mrr vals (pred, true): 0.098, 0.000
batch losses (mrrl, rdl): 0.0229058806, 5.38399e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 16
rank avg (pred): 0.331 +- 0.400
mrr vals (pred, true): 0.085, 0.000
batch losses (mrrl, rdl): 0.0120892022, 0.0006955614

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.550 +- 0.366
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0012771309, 4.39232e-05

Epoch over!
epoch time: 14.831

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1055
rank avg (pred): 0.275 +- 0.369
mrr vals (pred, true): 0.165, 0.128
batch losses (mrrl, rdl): 0.0130217317, 3.26922e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1096
rank avg (pred): 0.480 +- 0.303
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 2.2291e-05, 1.43317e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.471 +- 0.288
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001399084, 5.1913e-06

Epoch over!
epoch time: 14.829

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 233
rank avg (pred): 0.449 +- 0.315
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 6.35364e-05, 8.52702e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 789
rank avg (pred): 0.479 +- 0.297
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001751225, 7.4153e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 719
rank avg (pred): 0.472 +- 0.308
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004660896, 1.10428e-05

Epoch over!
epoch time: 14.829

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1055
rank avg (pred): 0.259 +- 0.355
mrr vals (pred, true): 0.160, 0.128
batch losses (mrrl, rdl): 0.0096744942, 4.55206e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 248
rank avg (pred): 0.219 +- 0.348
mrr vals (pred, true): 0.294, 0.173
batch losses (mrrl, rdl): 0.1461829245, 5.3289e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.407 +- 0.282
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0034312736, 0.0002034905

Epoch over!
epoch time: 14.846

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 730
rank avg (pred): 0.331 +- 0.303
mrr vals (pred, true): 0.179, 0.329
batch losses (mrrl, rdl): 0.2257026732, 0.0006881566

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.386 +- 0.305
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0084821405, 9.23753e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.445 +- 0.317
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0021627424, 8.4467e-06

Epoch over!
epoch time: 14.83

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 153
rank avg (pred): 0.478 +- 0.304
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0007012154, 6.02938e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 123
rank avg (pred): 0.473 +- 0.273
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004830372, 7.8591e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 392
rank avg (pred): 0.478 +- 0.289
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 6.10678e-05, 5.5748e-06

Epoch over!
epoch time: 14.814

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 335
rank avg (pred): 0.487 +- 0.320
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001995029, 1.43848e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.539 +- 0.223
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.4001e-05, 6.27995e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.475 +- 0.275
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005296341, 7.4786e-06

Epoch over!
epoch time: 14.865

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1146
rank avg (pred): 0.422 +- 0.271
mrr vals (pred, true): 0.073, 0.108
batch losses (mrrl, rdl): 0.0121405479, 0.0001129001

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1202
rank avg (pred): 0.516 +- 0.233
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 3.2849e-06, 9.19933e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 125
rank avg (pred): 0.462 +- 0.262
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 6.1773e-05, 1.12514e-05

Epoch over!
epoch time: 14.844

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.517 +- 0.316
mrr vals (pred, true): 0.048, 0.001

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   21 	     0 	 0.04722 	 0.00014 	 m..s
   19 	     1 	 0.04714 	 0.00016 	 m..s
   99 	     2 	 0.06260 	 0.00016 	 m..s
   42 	     3 	 0.04776 	 0.00016 	 m..s
   28 	     4 	 0.04732 	 0.00016 	 m..s
   14 	     5 	 0.04598 	 0.00017 	 m..s
   21 	     6 	 0.04722 	 0.00017 	 m..s
   33 	     7 	 0.04758 	 0.00017 	 m..s
   34 	     8 	 0.04764 	 0.00017 	 m..s
   42 	     9 	 0.04776 	 0.00017 	 m..s
   65 	    10 	 0.04826 	 0.00017 	 m..s
   96 	    11 	 0.06115 	 0.00018 	 m..s
    7 	    12 	 0.04508 	 0.00018 	 m..s
   42 	    13 	 0.04776 	 0.00018 	 m..s
   38 	    14 	 0.04772 	 0.00019 	 m..s
   11 	    15 	 0.04582 	 0.00019 	 m..s
   91 	    16 	 0.05858 	 0.00019 	 m..s
    9 	    17 	 0.04560 	 0.00019 	 m..s
   42 	    18 	 0.04776 	 0.00019 	 m..s
   41 	    19 	 0.04775 	 0.00019 	 m..s
    1 	    20 	 0.04365 	 0.00019 	 m..s
   38 	    21 	 0.04772 	 0.00020 	 m..s
   67 	    22 	 0.04841 	 0.00020 	 m..s
   32 	    23 	 0.04753 	 0.00021 	 m..s
   18 	    24 	 0.04701 	 0.00021 	 m..s
    2 	    25 	 0.04491 	 0.00021 	 m..s
   31 	    26 	 0.04750 	 0.00022 	 m..s
  102 	    27 	 0.06537 	 0.00022 	 m..s
    4 	    28 	 0.04494 	 0.00022 	 m..s
   16 	    29 	 0.04638 	 0.00022 	 m..s
   67 	    30 	 0.04841 	 0.00024 	 m..s
   15 	    31 	 0.04634 	 0.00024 	 m..s
   42 	    32 	 0.04776 	 0.00025 	 m..s
   42 	    33 	 0.04776 	 0.00025 	 m..s
   80 	    34 	 0.05221 	 0.00025 	 m..s
   89 	    35 	 0.05694 	 0.00025 	 m..s
   93 	    36 	 0.05996 	 0.00026 	 m..s
   42 	    37 	 0.04776 	 0.00026 	 m..s
   26 	    38 	 0.04732 	 0.00026 	 m..s
   98 	    39 	 0.06130 	 0.00027 	 m..s
   42 	    40 	 0.04776 	 0.00028 	 m..s
   24 	    41 	 0.04723 	 0.00028 	 m..s
   35 	    42 	 0.04768 	 0.00029 	 m..s
   42 	    43 	 0.04776 	 0.00029 	 m..s
   27 	    44 	 0.04732 	 0.00029 	 m..s
   42 	    45 	 0.04776 	 0.00030 	 m..s
   42 	    46 	 0.04776 	 0.00031 	 m..s
   40 	    47 	 0.04775 	 0.00031 	 m..s
   84 	    48 	 0.05392 	 0.00031 	 m..s
   77 	    49 	 0.05102 	 0.00032 	 m..s
   36 	    50 	 0.04771 	 0.00032 	 m..s
   83 	    51 	 0.05371 	 0.00032 	 m..s
  100 	    52 	 0.06291 	 0.00033 	 m..s
   42 	    53 	 0.04776 	 0.00034 	 m..s
   97 	    54 	 0.06116 	 0.00034 	 m..s
   75 	    55 	 0.05020 	 0.00036 	 m..s
   42 	    56 	 0.04776 	 0.00038 	 m..s
   42 	    57 	 0.04776 	 0.00039 	 m..s
   42 	    58 	 0.04776 	 0.00040 	 m..s
   42 	    59 	 0.04776 	 0.00040 	 m..s
   12 	    60 	 0.04583 	 0.00040 	 m..s
   66 	    61 	 0.04828 	 0.00040 	 m..s
   29 	    62 	 0.04741 	 0.00041 	 m..s
   70 	    63 	 0.04858 	 0.00042 	 m..s
   10 	    64 	 0.04567 	 0.00042 	 m..s
   73 	    65 	 0.04984 	 0.00046 	 m..s
    6 	    66 	 0.04507 	 0.00051 	 m..s
   23 	    67 	 0.04722 	 0.00054 	 m..s
   82 	    68 	 0.05276 	 0.00055 	 m..s
    5 	    69 	 0.04499 	 0.00058 	 m..s
   42 	    70 	 0.04776 	 0.00060 	 m..s
   42 	    71 	 0.04776 	 0.00061 	 m..s
   25 	    72 	 0.04728 	 0.00061 	 m..s
   30 	    73 	 0.04741 	 0.00063 	 m..s
  101 	    74 	 0.06336 	 0.00064 	 m..s
   13 	    75 	 0.04594 	 0.00065 	 m..s
   64 	    76 	 0.04812 	 0.00065 	 m..s
   91 	    77 	 0.05858 	 0.00071 	 m..s
   42 	    78 	 0.04776 	 0.00074 	 m..s
   42 	    79 	 0.04776 	 0.00082 	 m..s
   63 	    80 	 0.04809 	 0.00088 	 m..s
   42 	    81 	 0.04776 	 0.00113 	 m..s
   37 	    82 	 0.04772 	 0.00115 	 m..s
   20 	    83 	 0.04717 	 0.00116 	 m..s
   81 	    84 	 0.05250 	 0.00117 	 m..s
    0 	    85 	 0.04194 	 0.00118 	 m..s
   69 	    86 	 0.04849 	 0.00210 	 m..s
   76 	    87 	 0.05039 	 0.00835 	 m..s
   90 	    88 	 0.05747 	 0.04577 	 ~...
   85 	    89 	 0.05424 	 0.05590 	 ~...
   79 	    90 	 0.05182 	 0.06461 	 ~...
   72 	    91 	 0.04928 	 0.06786 	 ~...
  105 	    92 	 0.08215 	 0.07237 	 ~...
   94 	    93 	 0.06028 	 0.07530 	 ~...
   87 	    94 	 0.05690 	 0.07732 	 ~...
   85 	    95 	 0.05424 	 0.08821 	 m..s
   74 	    96 	 0.05005 	 0.08835 	 m..s
   17 	    97 	 0.04681 	 0.09332 	 m..s
    8 	    98 	 0.04523 	 0.09680 	 m..s
   95 	    99 	 0.06099 	 0.09747 	 m..s
  107 	   100 	 0.08680 	 0.09774 	 ~...
   88 	   101 	 0.05693 	 0.09778 	 m..s
  103 	   102 	 0.07268 	 0.10137 	 ~...
   78 	   103 	 0.05121 	 0.10629 	 m..s
   71 	   104 	 0.04884 	 0.10659 	 m..s
    3 	   105 	 0.04494 	 0.10832 	 m..s
  104 	   106 	 0.07686 	 0.10983 	 m..s
  109 	   107 	 0.10195 	 0.11308 	 ~...
  106 	   108 	 0.08291 	 0.12018 	 m..s
  110 	   109 	 0.13401 	 0.12051 	 ~...
  107 	   110 	 0.08680 	 0.13328 	 m..s
  117 	   111 	 0.21472 	 0.14203 	 m..s
  113 	   112 	 0.16298 	 0.14273 	 ~...
  112 	   113 	 0.15200 	 0.14537 	 ~...
  111 	   114 	 0.14496 	 0.15510 	 ~...
  114 	   115 	 0.16321 	 0.16023 	 ~...
  116 	   116 	 0.21275 	 0.17311 	 m..s
  115 	   117 	 0.21007 	 0.17666 	 m..s
  118 	   118 	 0.22596 	 0.20478 	 ~...
  120 	   119 	 0.25707 	 0.33898 	 m..s
  119 	   120 	 0.25499 	 0.38067 	 MISS
==========================================
r_mrr = 0.8612912893295288
r2_mrr = 0.4943658113479614
spearmanr_mrr@5 = 0.9872572422027588
spearmanr_mrr@10 = 0.8293581604957581
spearmanr_mrr@50 = 0.8756412863731384
spearmanr_mrr@100 = 0.8978219032287598
spearmanr_mrr@All = 0.9021384716033936
==========================================
test time: 0.448
Done Testing dataset DBpedia50
total time taken: 228.48108887672424
training time taken: 223.09472799301147
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8613)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4944)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9873)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8294)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.8756)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.8978)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9021)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.5332512538388983}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 7189001384334985
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [286, 499, 702, 198, 786, 861, 789, 472, 1205, 800, 442, 379, 217, 338, 311, 754, 840, 365, 844, 941, 1098, 817, 386, 17, 366, 1115, 842, 832, 1033, 279, 234, 874, 78, 791, 619, 1050, 1010, 2, 109, 190, 1153, 872, 502, 742, 596, 244, 966, 169, 278, 435, 600, 873, 245, 507, 820, 346, 35, 1077, 777, 798, 1060, 439, 945, 342, 810, 1066, 494, 511, 755, 657, 233, 188, 426, 257, 1003, 979, 246, 912, 802, 28, 371, 95, 37, 1099, 1161, 505, 156, 36, 911, 117, 637, 749, 866, 849, 239, 202, 18, 667, 421, 20, 703, 121, 385, 1145, 748, 305, 1150, 273, 538, 503, 16, 335, 498, 219, 449, 42, 83, 693, 1008, 675, 143]
valid_ids (0): []
train_ids (1094): [22, 753, 1122, 545, 106, 750, 793, 1124, 122, 528, 814, 68, 668, 450, 1016, 537, 192, 1015, 404, 915, 304, 65, 1088, 145, 1168, 124, 323, 319, 1183, 553, 1146, 515, 986, 130, 1030, 1097, 578, 235, 1005, 645, 510, 374, 76, 1013, 1159, 97, 142, 819, 669, 158, 1007, 63, 953, 367, 815, 411, 210, 249, 926, 184, 100, 1172, 473, 344, 957, 173, 1086, 361, 375, 427, 956, 665, 62, 828, 659, 529, 1190, 1114, 774, 90, 1069, 821, 1207, 620, 80, 712, 1194, 289, 110, 882, 640, 564, 913, 87, 895, 66, 1211, 53, 1052, 119, 805, 86, 64, 70, 928, 977, 635, 1213, 557, 720, 1012, 247, 783, 952, 818, 642, 707, 512, 934, 322, 277, 958, 519, 463, 662, 1078, 334, 560, 1163, 614, 162, 136, 1079, 325, 443, 103, 829, 1027, 987, 296, 393, 146, 933, 845, 524, 594, 530, 718, 376, 909, 392, 13, 1164, 1141, 1109, 458, 1018, 281, 1076, 582, 1193, 220, 369, 1151, 788, 328, 1001, 1180, 556, 853, 276, 1212, 1186, 629, 149, 682, 1158, 1203, 1189, 332, 937, 506, 569, 714, 163, 388, 67, 568, 884, 464, 608, 255, 213, 1110, 520, 1177, 617, 237, 51, 406, 923, 745, 49, 701, 743, 150, 521, 484, 756, 248, 359, 253, 650, 1031, 540, 787, 459, 431, 809, 983, 1200, 587, 420, 558, 516, 187, 1123, 713, 903, 836, 161, 867, 155, 308, 761, 216, 673, 1119, 525, 422, 985, 726, 1139, 148, 626, 847, 839, 661, 355, 887, 964, 965, 630, 348, 287, 127, 32, 317, 921, 653, 275, 779, 813, 576, 40, 1026, 271, 852, 1160, 674, 708, 272, 846, 77, 310, 280, 889, 491, 1108, 180, 988, 445, 1167, 925, 114, 166, 981, 333, 434, 606, 9, 605, 960, 522, 205, 816, 1091, 211, 590, 1191, 961, 141, 92, 11, 835, 414, 1131, 838, 888, 973, 1116, 303, 942, 906, 684, 1043, 232, 687, 46, 260, 685, 274, 1063, 5, 639, 946, 984, 209, 899, 1000, 625, 559, 757, 615, 824, 944, 1198, 480, 948, 997, 695, 706, 1017, 603, 1154, 378, 900, 175, 765, 947, 574, 288, 331, 803, 597, 467, 518, 3, 1201, 879, 896, 633, 696, 1023, 416, 740, 241, 446, 1125, 768, 638, 976, 691, 466, 717, 1083, 865, 716, 972, 770, 394, 643, 655, 711, 428, 377, 1051, 381, 52, 55, 1175, 1082, 980, 739, 157, 171, 283, 834, 226, 531, 709, 1181, 1021, 1196, 1032, 284, 330, 858, 797, 101, 671, 1127, 681, 1130, 1081, 172, 399, 949, 609, 993, 179, 82, 592, 168, 723, 932, 351, 1040, 1038, 841, 475, 56, 1187, 462, 881, 290, 562, 360, 804, 527, 830, 694, 1002, 1162, 362, 892, 808, 517, 6, 267, 251, 1140, 300, 402, 876, 737, 1155, 893, 730, 151, 352, 825, 236, 566, 8, 307, 1134, 589, 1132, 1045, 490, 26, 29, 1195, 316, 93, 391, 258, 991, 123, 862, 405, 508, 759, 664, 1152, 54, 700, 120, 387, 919, 214, 1, 61, 622, 128, 731, 570, 1144, 1173, 869, 766, 543, 898, 343, 1009, 567, 357, 951, 575, 513, 955, 349, 196, 752, 623, 794, 697, 1148, 14, 546, 10, 0, 1058, 690, 591, 409, 535, 160, 1101, 48, 710, 69, 154, 137, 1209, 699, 185, 1137, 1102, 831, 1184, 91, 12, 922, 914, 182, 767, 195, 1165, 1072, 1113, 1014, 641, 950, 719, 721, 126, 165, 390, 403, 773, 34, 1179, 850, 238, 968, 380, 539, 324, 680, 261, 1118, 479, 221, 396, 910, 610, 104, 920, 1105, 400, 468, 492, 410, 1156, 859, 894, 1182, 1202, 1073, 544, 938, 250, 792, 660, 417, 183, 301, 1090, 57, 489, 908, 313, 452, 457, 384, 222, 998, 85, 782, 975, 1065, 654, 135, 1157, 45, 321, 848, 880, 7, 1025, 1138, 612, 50, 868, 59, 153, 1047, 1074, 526, 326, 465, 666, 481, 407, 886, 573, 648, 263, 176, 456, 1129, 438, 649, 1111, 131, 837, 547, 230, 1029, 495, 1034, 644, 129, 268, 159, 616, 1096, 164, 577, 771, 1093, 107, 971, 125, 140, 561, 1126, 598, 1075, 415, 593, 363, 353, 736, 939, 1054, 1024, 509, 1062, 533, 851, 181, 73, 856, 432, 84, 223, 907, 995, 580, 297, 1143, 1094, 1192, 397, 373, 532, 1176, 105, 563, 1112, 646, 240, 801, 299, 552, 74, 38, 963, 663, 19, 652, 320, 843, 218, 347, 382, 254, 71, 1039, 550, 927, 1055, 1042, 193, 1185, 747, 1019, 1061, 208, 764, 327, 1197, 424, 108, 584, 586, 312, 500, 1067, 536, 1006, 448, 823, 744, 461, 604, 493, 732, 1174, 565, 132, 588, 1178, 627, 1199, 227, 781, 822, 293, 113, 954, 1210, 336, 1107, 1092, 785, 962, 476, 918, 585, 790, 1041, 102, 796, 722, 167, 24, 437, 370, 72, 878, 1169, 116, 81, 225, 891, 425, 430, 285, 613, 47, 1049, 996, 401, 689, 534, 454, 496, 302, 408, 776, 354, 1171, 677, 1087, 812, 1080, 294, 262, 469, 778, 583, 940, 270, 902, 1103, 1044, 890, 1120, 256, 204, 970, 144, 295, 672, 571, 741, 990, 191, 88, 870, 883, 651, 133, 1133, 345, 96, 658, 337, 885, 31, 857, 318, 692, 1206, 989, 854, 98, 1037, 860, 41, 30, 523, 618, 924, 1064, 189, 775, 398, 43, 372, 292, 487, 44, 772, 197, 994, 599, 905, 1135, 967, 266, 769, 935, 215, 1106, 39, 855, 1028, 1100, 551, 784, 746, 1004, 207, 548, 554, 75, 1057, 89, 470, 264, 341, 607, 504, 314, 383, 1170, 725, 203, 486, 799, 904, 871, 897, 212, 231, 542, 572, 943, 315, 760, 864, 94, 875, 356, 99, 581, 1022, 916, 174, 1035, 632, 177, 265, 1056, 488, 4, 833, 25, 762, 482, 440, 999, 460, 350, 978, 795, 1204, 601, 200, 595, 1149, 433, 474, 229, 602, 478, 1117, 901, 471, 1095, 982, 194, 15, 826, 549, 959, 178, 364, 624, 611, 329, 930, 1048, 698, 340, 621, 1166, 735, 206, 807, 441, 634, 224, 444, 58, 483, 170, 678, 992, 1104, 79, 429, 21, 60, 112, 33, 269, 419, 827, 728, 514, 453, 368, 1020, 738, 389, 936, 1011, 477, 1214, 27, 729, 917, 1059, 111, 541, 298, 656, 199, 497, 1208, 679, 1142, 676, 152, 242, 228, 1089, 1147, 118, 631, 1071, 734, 1136, 758, 688, 751, 715, 806, 863, 1070, 1121, 186, 138, 243, 1068, 1053, 705, 727, 412, 451, 485, 115, 724, 670, 686, 733, 501, 418, 931, 139, 252, 647, 811, 1188, 395, 201, 763, 555, 306, 447, 134, 413, 780, 358, 1036, 339, 628, 969, 1046, 1084, 436, 683, 877, 282, 291, 309, 1128, 1085, 259, 636, 974, 579, 455, 929, 23, 423, 147, 704]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6331642107274654
the save name prefix for this run is:  chkpt-ID_6331642107274654_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 647
rank avg (pred): 0.519 +- 0.002
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001079331

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 940
rank avg (pred): 0.480 +- 0.289
mrr vals (pred, true): 0.121, 0.005
batch losses (mrrl, rdl): 0.0, 1.05239e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 627
rank avg (pred): 0.482 +- 0.313
mrr vals (pred, true): 0.148, 0.000
batch losses (mrrl, rdl): 0.0, 8.9938e-06

Epoch over!
epoch time: 14.676

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.485 +- 0.309
mrr vals (pred, true): 0.137, 0.000
batch losses (mrrl, rdl): 0.0, 7.3132e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1118
rank avg (pred): 0.471 +- 0.314
mrr vals (pred, true): 0.152, 0.000
batch losses (mrrl, rdl): 0.0, 1.25291e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.478 +- 0.305
mrr vals (pred, true): 0.161, 0.000
batch losses (mrrl, rdl): 0.0, 4.6288e-06

Epoch over!
epoch time: 14.671

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 606
rank avg (pred): 0.485 +- 0.314
mrr vals (pred, true): 0.167, 0.000
batch losses (mrrl, rdl): 0.0, 1.55044e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.327 +- 0.311
mrr vals (pred, true): 0.172, 0.075
batch losses (mrrl, rdl): 0.0, 7.0569e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 732
rank avg (pred): 0.190 +- 0.331
mrr vals (pred, true): 0.195, 0.381
batch losses (mrrl, rdl): 0.0, 8.66686e-05

Epoch over!
epoch time: 14.905

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 845
rank avg (pred): 0.478 +- 0.313
mrr vals (pred, true): 0.169, 0.000
batch losses (mrrl, rdl): 0.0, 1.67442e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1069
rank avg (pred): 0.304 +- 0.309
mrr vals (pred, true): 0.183, 0.160
batch losses (mrrl, rdl): 0.0, 9.3569e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 466
rank avg (pred): 0.467 +- 0.316
mrr vals (pred, true): 0.172, 0.000
batch losses (mrrl, rdl): 0.0, 1.38467e-05

Epoch over!
epoch time: 14.882

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.480 +- 0.308
mrr vals (pred, true): 0.168, 0.001
batch losses (mrrl, rdl): 0.0, 2.36909e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.232 +- 0.329
mrr vals (pred, true): 0.181, 0.350
batch losses (mrrl, rdl): 0.0, 0.0001863832

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.484 +- 0.298
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0, 3.32976e-05

Epoch over!
epoch time: 14.91

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1201
rank avg (pred): 0.493 +- 0.303
mrr vals (pred, true): 0.128, 0.001
batch losses (mrrl, rdl): 0.0606965311, 8.9896e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 148
rank avg (pred): 0.529 +- 0.296
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.17335e-05, 7.2623e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.504 +- 0.303
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006115885, 1.76812e-05

Epoch over!
epoch time: 15.068

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.382 +- 0.345
mrr vals (pred, true): 0.077, 0.014
batch losses (mrrl, rdl): 0.0073932428, 0.0013228889

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 193
rank avg (pred): 0.500 +- 0.274
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 4.119e-07, 1.40163e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 989
rank avg (pred): 0.276 +- 0.338
mrr vals (pred, true): 0.117, 0.147
batch losses (mrrl, rdl): 0.008838268, 1.83965e-05

Epoch over!
epoch time: 15.095

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 488
rank avg (pred): 0.326 +- 0.335
mrr vals (pred, true): 0.091, 0.115
batch losses (mrrl, rdl): 0.005850805, 1.05639e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 227
rank avg (pred): 0.493 +- 0.248
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 2.07723e-05, 1.25783e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 752
rank avg (pred): 0.463 +- 0.417
mrr vals (pred, true): 0.082, 0.221
batch losses (mrrl, rdl): 0.1934018433, 0.0010033523

Epoch over!
epoch time: 15.081

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 729
rank avg (pred): 0.364 +- 0.431
mrr vals (pred, true): 0.172, 0.339
batch losses (mrrl, rdl): 0.2778304517, 0.0008009961

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 34
rank avg (pred): 0.380 +- 0.257
mrr vals (pred, true): 0.061, 0.049
batch losses (mrrl, rdl): 0.0011961409, 0.0001070788

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 317
rank avg (pred): 0.307 +- 0.280
mrr vals (pred, true): 0.145, 0.134
batch losses (mrrl, rdl): 0.0013732875, 1.12558e-05

Epoch over!
epoch time: 15.075

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 405
rank avg (pred): 0.398 +- 0.251
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003444338, 0.0003237923

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.388 +- 0.265
mrr vals (pred, true): 0.070, 0.102
batch losses (mrrl, rdl): 0.0101227481, 2.08067e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 828
rank avg (pred): 0.253 +- 0.265
mrr vals (pred, true): 0.179, 0.137
batch losses (mrrl, rdl): 0.0174522195, 1.19932e-05

Epoch over!
epoch time: 15.07

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 0
rank avg (pred): 0.403 +- 0.256
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003044825, 3.53739e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.416 +- 0.247
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.78974e-05, 4.61668e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 56
rank avg (pred): 0.401 +- 0.253
mrr vals (pred, true): 0.057, 0.079
batch losses (mrrl, rdl): 0.0005025176, 0.0001702237

Epoch over!
epoch time: 15.108

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 916
rank avg (pred): 0.512 +- 0.364
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022331974, 6.14454e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 315
rank avg (pred): 0.315 +- 0.246
mrr vals (pred, true): 0.118, 0.081
batch losses (mrrl, rdl): 0.0459055416, 2.77561e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.461 +- 0.271
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 5.2891e-06, 9.3438e-06

Epoch over!
epoch time: 14.96

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 345
rank avg (pred): 0.511 +- 0.316
mrr vals (pred, true): 0.042, 0.002
batch losses (mrrl, rdl): 0.0006560848, 7.2236e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.416 +- 0.238
mrr vals (pred, true): 0.061, 0.099
batch losses (mrrl, rdl): 0.0012047008, 0.0003843323

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 688
rank avg (pred): 0.492 +- 0.252
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 8.73225e-05, 7.30579e-05

Epoch over!
epoch time: 14.983

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 593
rank avg (pred): 0.560 +- 0.313
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001491433, 4.6494e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 106
rank avg (pred): 0.524 +- 0.330
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 8.93e-08, 1.07981e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.578 +- 0.355
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.491e-07, 4.66724e-05

Epoch over!
epoch time: 15.094

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 570
rank avg (pred): 0.550 +- 0.265
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.000251783, 7.46998e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.565 +- 0.316
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 4.5847e-06, 0.0001480234

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 801
rank avg (pred): 0.481 +- 0.275
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.4535e-06, 4.60501e-05

Epoch over!
epoch time: 15.092

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.380 +- 0.239
mrr vals (pred, true): 0.076, 0.112

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   73 	     0 	 0.05976 	 0.00014 	 m..s
   85 	     1 	 0.07199 	 0.00015 	 m..s
   16 	     2 	 0.05375 	 0.00015 	 m..s
   48 	     3 	 0.05757 	 0.00015 	 m..s
   51 	     4 	 0.05794 	 0.00016 	 m..s
   72 	     5 	 0.05953 	 0.00016 	 m..s
   39 	     6 	 0.05688 	 0.00016 	 m..s
    1 	     7 	 0.05136 	 0.00016 	 m..s
   35 	     8 	 0.05590 	 0.00017 	 m..s
    0 	     9 	 0.05082 	 0.00017 	 m..s
   69 	    10 	 0.05916 	 0.00018 	 m..s
   61 	    11 	 0.05850 	 0.00019 	 m..s
   46 	    12 	 0.05746 	 0.00019 	 m..s
    9 	    13 	 0.05365 	 0.00019 	 m..s
    3 	    14 	 0.05158 	 0.00019 	 m..s
   40 	    15 	 0.05691 	 0.00019 	 m..s
   83 	    16 	 0.06705 	 0.00020 	 m..s
   22 	    17 	 0.05411 	 0.00020 	 m..s
   13 	    18 	 0.05372 	 0.00020 	 m..s
   76 	    19 	 0.06064 	 0.00020 	 m..s
    5 	    20 	 0.05218 	 0.00020 	 m..s
   52 	    21 	 0.05796 	 0.00020 	 m..s
   15 	    22 	 0.05373 	 0.00020 	 m..s
   19 	    23 	 0.05383 	 0.00020 	 m..s
   45 	    24 	 0.05726 	 0.00020 	 m..s
   56 	    25 	 0.05824 	 0.00021 	 m..s
    6 	    26 	 0.05218 	 0.00021 	 m..s
   20 	    27 	 0.05383 	 0.00021 	 m..s
   63 	    28 	 0.05867 	 0.00021 	 m..s
   33 	    29 	 0.05581 	 0.00022 	 m..s
   12 	    30 	 0.05369 	 0.00022 	 m..s
   55 	    31 	 0.05810 	 0.00022 	 m..s
   79 	    32 	 0.06237 	 0.00022 	 m..s
   26 	    33 	 0.05473 	 0.00022 	 m..s
   50 	    34 	 0.05767 	 0.00023 	 m..s
   23 	    35 	 0.05426 	 0.00023 	 m..s
   11 	    36 	 0.05369 	 0.00023 	 m..s
   38 	    37 	 0.05682 	 0.00023 	 m..s
   25 	    38 	 0.05471 	 0.00024 	 m..s
   81 	    39 	 0.06605 	 0.00025 	 m..s
   65 	    40 	 0.05877 	 0.00026 	 m..s
   17 	    41 	 0.05376 	 0.00026 	 m..s
   80 	    42 	 0.06517 	 0.00026 	 m..s
   18 	    43 	 0.05382 	 0.00026 	 m..s
   53 	    44 	 0.05803 	 0.00026 	 m..s
   36 	    45 	 0.05659 	 0.00027 	 m..s
   37 	    46 	 0.05672 	 0.00027 	 m..s
   30 	    47 	 0.05529 	 0.00028 	 m..s
    8 	    48 	 0.05358 	 0.00028 	 m..s
   84 	    49 	 0.07001 	 0.00029 	 m..s
    4 	    50 	 0.05196 	 0.00030 	 m..s
   44 	    51 	 0.05724 	 0.00030 	 m..s
   49 	    52 	 0.05761 	 0.00031 	 m..s
   47 	    53 	 0.05749 	 0.00031 	 m..s
   10 	    54 	 0.05366 	 0.00032 	 m..s
   31 	    55 	 0.05544 	 0.00032 	 m..s
   27 	    56 	 0.05495 	 0.00032 	 m..s
   71 	    57 	 0.05925 	 0.00033 	 m..s
   29 	    58 	 0.05520 	 0.00034 	 m..s
   41 	    59 	 0.05695 	 0.00036 	 m..s
   82 	    60 	 0.06617 	 0.00038 	 m..s
   60 	    61 	 0.05847 	 0.00040 	 m..s
   14 	    62 	 0.05372 	 0.00042 	 m..s
   64 	    63 	 0.05871 	 0.00044 	 m..s
   43 	    64 	 0.05721 	 0.00048 	 m..s
   59 	    65 	 0.05841 	 0.00049 	 m..s
   34 	    66 	 0.05586 	 0.00053 	 m..s
   58 	    67 	 0.05833 	 0.00054 	 m..s
   67 	    68 	 0.05902 	 0.00057 	 m..s
   54 	    69 	 0.05807 	 0.00063 	 m..s
   75 	    70 	 0.06057 	 0.00065 	 m..s
   21 	    71 	 0.05388 	 0.00086 	 m..s
   78 	    72 	 0.06152 	 0.00118 	 m..s
   24 	    73 	 0.05430 	 0.00134 	 m..s
   28 	    74 	 0.05498 	 0.00141 	 m..s
   77 	    75 	 0.06102 	 0.00143 	 m..s
   62 	    76 	 0.05855 	 0.00192 	 m..s
    7 	    77 	 0.05354 	 0.00286 	 m..s
   42 	    78 	 0.05721 	 0.00574 	 m..s
   32 	    79 	 0.05577 	 0.00842 	 m..s
    2 	    80 	 0.05144 	 0.00890 	 m..s
  107 	    81 	 0.14707 	 0.05909 	 m..s
   57 	    82 	 0.05829 	 0.06559 	 ~...
   68 	    83 	 0.05912 	 0.07732 	 ~...
   70 	    84 	 0.05924 	 0.07831 	 ~...
   91 	    85 	 0.07905 	 0.08077 	 ~...
   86 	    86 	 0.07239 	 0.08350 	 ~...
   88 	    87 	 0.07439 	 0.08574 	 ~...
  114 	    88 	 0.17557 	 0.08631 	 m..s
   89 	    89 	 0.07449 	 0.08752 	 ~...
   66 	    90 	 0.05880 	 0.09015 	 m..s
   87 	    91 	 0.07347 	 0.09167 	 ~...
   93 	    92 	 0.08167 	 0.09507 	 ~...
   94 	    93 	 0.08391 	 0.09774 	 ~...
   74 	    94 	 0.06010 	 0.10871 	 m..s
   90 	    95 	 0.07621 	 0.11218 	 m..s
   97 	    96 	 0.08893 	 0.11308 	 ~...
  115 	    97 	 0.18841 	 0.11446 	 m..s
   96 	    98 	 0.08813 	 0.12018 	 m..s
   98 	    99 	 0.08984 	 0.12190 	 m..s
   92 	   100 	 0.08119 	 0.12801 	 m..s
   99 	   101 	 0.09311 	 0.12975 	 m..s
  103 	   102 	 0.12235 	 0.13971 	 ~...
  101 	   103 	 0.10983 	 0.14457 	 m..s
   95 	   104 	 0.08676 	 0.14659 	 m..s
  100 	   105 	 0.09691 	 0.14951 	 m..s
  102 	   106 	 0.12155 	 0.14992 	 ~...
  109 	   107 	 0.15713 	 0.15118 	 ~...
  108 	   108 	 0.15099 	 0.15149 	 ~...
  119 	   109 	 0.20767 	 0.15283 	 m..s
  104 	   110 	 0.13231 	 0.16794 	 m..s
  105 	   111 	 0.13602 	 0.17482 	 m..s
  106 	   112 	 0.14570 	 0.18137 	 m..s
  111 	   113 	 0.16279 	 0.18675 	 ~...
  110 	   114 	 0.15976 	 0.20159 	 m..s
  113 	   115 	 0.16902 	 0.20291 	 m..s
  116 	   116 	 0.20006 	 0.21699 	 ~...
  118 	   117 	 0.20730 	 0.22246 	 ~...
  112 	   118 	 0.16816 	 0.23873 	 m..s
  117 	   119 	 0.20402 	 0.29684 	 m..s
  120 	   120 	 0.21913 	 0.35013 	 MISS
==========================================
r_mrr = 0.8793003559112549
r2_mrr = 0.4971069097518921
spearmanr_mrr@5 = 0.9253634214401245
spearmanr_mrr@10 = 0.8468480706214905
spearmanr_mrr@50 = 0.9265192151069641
spearmanr_mrr@100 = 0.9464265704154968
spearmanr_mrr@All = 0.9505704045295715
==========================================
test time: 0.464
Done Testing dataset DBpedia50
total time taken: 230.42452931404114
training time taken: 225.14371371269226
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8793)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4971)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9254)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8468)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9265)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9464)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9506)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.0376734239107464}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 7858386549971944
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1076, 613, 1131, 1066, 947, 526, 42, 161, 313, 914, 167, 137, 116, 668, 976, 4, 949, 1012, 1188, 735, 466, 30, 1144, 44, 225, 688, 1083, 724, 1135, 299, 1210, 749, 1178, 1057, 460, 765, 1096, 692, 933, 1119, 276, 954, 939, 1200, 3, 863, 521, 1133, 322, 705, 113, 1141, 1079, 811, 788, 575, 1168, 817, 1120, 861, 1146, 836, 58, 506, 1147, 301, 772, 881, 507, 306, 827, 1091, 345, 1016, 887, 775, 154, 391, 367, 1193, 434, 809, 532, 789, 1151, 942, 132, 839, 390, 92, 734, 852, 814, 385, 16, 717, 866, 898, 961, 439, 1173, 872, 502, 186, 401, 581, 1153, 1011, 339, 270, 807, 43, 860, 1166, 547, 1054, 41, 52, 925, 266, 1084]
valid_ids (0): []
train_ids (1094): [54, 179, 35, 321, 127, 689, 777, 833, 531, 255, 412, 729, 1212, 955, 202, 642, 570, 600, 68, 163, 617, 300, 672, 361, 386, 1183, 1094, 727, 124, 259, 125, 372, 597, 304, 612, 810, 338, 84, 184, 665, 1180, 143, 793, 366, 985, 271, 588, 1031, 134, 986, 1070, 260, 249, 712, 1127, 61, 393, 806, 957, 148, 611, 967, 1179, 980, 834, 1034, 24, 273, 441, 471, 221, 926, 737, 655, 150, 217, 624, 1080, 1020, 326, 522, 63, 1078, 65, 1152, 508, 750, 327, 91, 493, 50, 406, 956, 1014, 349, 17, 759, 1206, 1181, 1081, 101, 529, 907, 844, 258, 801, 855, 356, 1051, 1205, 910, 212, 485, 346, 920, 1007, 427, 1087, 886, 697, 649, 469, 1209, 573, 571, 880, 408, 364, 250, 122, 395, 824, 199, 274, 1041, 864, 1194, 165, 1105, 1021, 974, 781, 703, 607, 286, 940, 1088, 982, 1053, 465, 481, 213, 1177, 1037, 608, 19, 676, 494, 117, 319, 643, 281, 194, 902, 865, 563, 491, 236, 858, 470, 656, 201, 943, 103, 399, 1207, 1211, 411, 1050, 1075, 670, 287, 435, 214, 1059, 592, 572, 661, 984, 680, 82, 468, 1157, 558, 561, 244, 690, 776, 472, 604, 229, 1042, 1138, 83, 990, 368, 1114, 1160, 423, 1095, 1000, 894, 669, 882, 200, 38, 1113, 622, 12, 952, 602, 456, 365, 378, 1004, 135, 267, 1018, 93, 348, 537, 1140, 681, 293, 486, 362, 407, 1009, 1162, 1174, 331, 979, 631, 544, 657, 845, 305, 787, 13, 564, 355, 700, 492, 487, 178, 1150, 972, 1032, 422, 543, 619, 428, 1203, 342, 527, 53, 838, 144, 1202, 847, 80, 282, 536, 796, 1159, 1027, 256, 1143, 1071, 1122, 187, 679, 1099, 822, 708, 1052, 704, 999, 755, 517, 351, 591, 220, 849, 523, 535, 1136, 398, 792, 195, 662, 36, 285, 1033, 1077, 1139, 463, 231, 474, 962, 153, 303, 66, 90, 691, 936, 1171, 918, 400, 869, 946, 462, 757, 639, 81, 525, 311, 443, 577, 1074, 1030, 1008, 879, 482, 278, 911, 197, 1109, 97, 823, 515, 234, 1068, 114, 1126, 20, 627, 871, 39, 634, 761, 6, 175, 419, 878, 731, 519, 442, 170, 610, 971, 238, 929, 237, 461, 915, 1165, 753, 959, 1, 257, 752, 1115, 138, 222, 747, 994, 953, 965, 207, 1169, 504, 1060, 510, 841, 644, 152, 966, 315, 1124, 916, 94, 223, 454, 945, 298, 32, 516, 897, 1149, 867, 173, 1145, 585, 678, 742, 336, 296, 130, 785, 900, 436, 478, 713, 848, 495, 464, 176, 601, 599, 477, 387, 447, 1130, 433, 151, 1154, 183, 48, 859, 744, 1046, 695, 1061, 264, 964, 26, 873, 343, 1056, 648, 177, 459, 722, 1204, 297, 190, 254, 206, 560, 1161, 128, 181, 973, 228, 392, 913, 1103, 620, 751, 720, 371, 795, 72, 551, 760, 635, 1199, 131, 798, 1155, 14, 246, 1025, 565, 567, 158, 1110, 562, 596, 302, 198, 1006, 709, 698, 45, 628, 1187, 969, 715, 557, 1112, 1090, 808, 832, 295, 766, 556, 748, 344, 542, 226, 951, 329, 509, 889, 1055, 584, 586, 893, 388, 1176, 292, 1156, 938, 651, 437, 721, 374, 1047, 802, 22, 714, 870, 330, 379, 819, 505, 921, 800, 147, 904, 224, 1085, 931, 677, 180, 699, 598, 937, 738, 112, 272, 252, 353, 970, 603, 812, 284, 1064, 778, 280, 856, 7, 328, 1003, 786, 583, 647, 56, 782, 927, 595, 559, 307, 333, 701, 1118, 896, 1163, 968, 934, 1035, 790, 501, 323, 430, 1197, 1101, 813, 768, 1010, 998, 1015, 290, 182, 576, 324, 1023, 265, 139, 730, 696, 1190, 769, 404, 582, 318, 710, 34, 95, 1134, 410, 587, 370, 574, 1132, 928, 770, 409, 1049, 736, 1058, 1116, 449, 189, 837, 230, 804, 1129, 108, 652, 1073, 1017, 247, 1201, 1189, 241, 623, 105, 901, 233, 51, 484, 106, 205, 376, 605, 988, 917, 743, 741, 85, 636, 763, 394, 415, 767, 983, 1063, 740, 263, 851, 1102, 658, 15, 829, 268, 18, 209, 1100, 1106, 363, 73, 541, 686, 733, 1038, 118, 377, 357, 496, 438, 923, 640, 123, 453, 883, 473, 550, 253, 853, 846, 444, 615, 164, 566, 79, 102, 549, 666, 421, 1024, 702, 291, 1195, 126, 402, 745, 107, 86, 1022, 684, 219, 978, 193, 1044, 554, 876, 136, 764, 310, 70, 590, 2, 431, 350, 381, 895, 641, 129, 794, 835, 5, 892, 726, 960, 815, 868, 448, 490, 555, 172, 25, 831, 1086, 1048, 1013, 1028, 633, 28, 1184, 62, 76, 728, 716, 314, 997, 771, 87, 1137, 1158, 524, 625, 162, 308, 98, 1072, 29, 354, 141, 885, 663, 160, 779, 445, 816, 963, 110, 513, 146, 1191, 373, 739, 384, 630, 891, 498, 185, 240, 1182, 483, 414, 426, 606, 944, 653, 579, 987, 1067, 279, 1128, 580, 78, 168, 1172, 991, 1107, 958, 100, 1089, 1148, 996, 218, 204, 60, 906, 432, 1123, 251, 159, 948, 166, 930, 440, 111, 479, 1111, 216, 403, 548, 552, 1026, 553, 539, 499, 512, 667, 1192, 488, 341, 242, 694, 188, 21, 171, 719, 75, 992, 416, 683, 1029, 375, 843, 756, 1043, 840, 905, 1198, 55, 88, 289, 208, 774, 674, 1092, 783, 121, 874, 1121, 578, 10, 358, 1186, 451, 33, 57, 71, 145, 780, 1097, 142, 37, 1170, 618, 120, 500, 211, 616, 545, 903, 830, 425, 540, 11, 784, 334, 732, 1104, 59, 803, 725, 169, 196, 671, 1213, 104, 288, 380, 1082, 818, 382, 977, 935, 418, 489, 283, 932, 174, 1040, 594, 1069, 232, 269, 389, 758, 480, 693, 317, 646, 47, 654, 660, 950, 1098, 450, 664, 149, 316, 530, 850, 325, 69, 629, 40, 922, 239, 762, 995, 396, 458, 191, 320, 1108, 569, 589, 1164, 294, 993, 675, 919, 975, 452, 405, 133, 799, 685, 413, 821, 842, 888, 546, 511, 335, 707, 650, 1142, 1125, 912, 74, 797, 1036, 746, 31, 262, 687, 156, 1005, 826, 862, 534, 875, 337, 261, 533, 609, 369, 245, 27, 64, 791, 514, 397, 192, 820, 989, 115, 420, 275, 773, 429, 825, 119, 638, 340, 659, 243, 924, 909, 109, 890, 140, 215, 0, 497, 614, 1062, 857, 1039, 1185, 626, 67, 8, 89, 1214, 711, 77, 854, 1208, 347, 1175, 49, 1093, 1167, 417, 96, 475, 235, 1117, 99, 706, 424, 621, 248, 1002, 157, 1065, 210, 682, 277, 332, 568, 941, 673, 877, 312, 227, 360, 455, 593, 1001, 203, 446, 23, 309, 467, 457, 503, 723, 476, 908, 520, 637, 528, 155, 383, 981, 754, 899, 632, 1196, 718, 884, 359, 645, 352, 518, 46, 538, 1045, 805, 9, 1019, 828]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4545665955189231
the save name prefix for this run is:  chkpt-ID_4545665955189231_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 527
rank avg (pred): 0.390 +- 0.005
mrr vals (pred, true): 0.000, 0.087
batch losses (mrrl, rdl): 0.0, 0.0001635301

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 436
rank avg (pred): 0.488 +- 0.002
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001082567

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1198
rank avg (pred): 0.497 +- 0.002
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001018395

Epoch over!
epoch time: 14.731

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 642
rank avg (pred): 0.497 +- 0.002
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001308247

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.267 +- 0.002
mrr vals (pred, true): 0.000, 0.261
batch losses (mrrl, rdl): 0.0, 0.0001867165

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.497 +- 0.002
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 0.000137644

Epoch over!
epoch time: 14.709

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.317 +- 0.001
mrr vals (pred, true): 0.000, 0.113
batch losses (mrrl, rdl): 0.0, 0.0001466091

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.477 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001293888

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.498 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001376552

Epoch over!
epoch time: 14.682

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 801
rank avg (pred): 0.496 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001486366

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 638
rank avg (pred): 0.491 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 6.95869e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1082
rank avg (pred): 0.492 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.64905e-05

Epoch over!
epoch time: 14.691

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 768
rank avg (pred): 0.492 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001450474

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.372 +- 0.000
mrr vals (pred, true): 0.000, 0.070
batch losses (mrrl, rdl): 0.0, 0.0001901955

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.353 +- 0.001
mrr vals (pred, true): 0.000, 0.127
batch losses (mrrl, rdl): 0.0, 0.0001470603

Epoch over!
epoch time: 14.707

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 67
rank avg (pred): 0.357 +- 0.000
mrr vals (pred, true): 0.000, 0.098
batch losses (mrrl, rdl): 0.0248865336, 0.0001717975

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.154 +- 0.000
mrr vals (pred, true): 0.000, 0.090
batch losses (mrrl, rdl): 0.0247369539, 0.0007973309

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.130 +- 0.000
mrr vals (pred, true): 0.000, 0.120
batch losses (mrrl, rdl): 0.1442541033, 0.0009120366

Epoch over!
epoch time: 14.942

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 525
rank avg (pred): 0.361 +- 0.000
mrr vals (pred, true): 0.000, 0.108
batch losses (mrrl, rdl): 0.1170969009, 0.0001037108

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 726
rank avg (pred): 0.468 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249132477, 0.0001722733

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 487
rank avg (pred): 0.335 +- 0.000
mrr vals (pred, true): 0.000, 0.108
batch losses (mrrl, rdl): 0.1162096187, 0.0001569862

Epoch over!
epoch time: 14.913

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 846
rank avg (pred): 0.488 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249167755, 0.0001211248

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 996
rank avg (pred): 0.098 +- 0.000
mrr vals (pred, true): 0.000, 0.165
batch losses (mrrl, rdl): 0.271068871, 0.0006242559

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 309
rank avg (pred): 0.329 +- 0.000
mrr vals (pred, true): 0.000, 0.108
batch losses (mrrl, rdl): 0.1162479967, 0.0001733035

Epoch over!
epoch time: 15.012

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 199
rank avg (pred): 0.483 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249159485, 0.0001100438

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 849
rank avg (pred): 0.488 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.02491685, 0.0001274511

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 557
rank avg (pred): 0.424 +- 0.000
mrr vals (pred, true): 0.000, 0.107
batch losses (mrrl, rdl): 0.1134152785, 0.0001951858

Epoch over!
epoch time: 14.951

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.458 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249114279, 0.0001527175

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1050
rank avg (pred): 0.472 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249139927, 0.0001030333

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 699
rank avg (pred): 0.482 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249158852, 7.08878e-05

Epoch over!
epoch time: 14.941

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.485 +- 0.000
mrr vals (pred, true): 0.000, 0.146
batch losses (mrrl, rdl): 0.2121556699, 0.0005956859

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1050
rank avg (pred): 0.345 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0248824991, 0.0005462391

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.479 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249152631, 0.0001313041

Epoch over!
epoch time: 14.96

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 778
rank avg (pred): 0.479 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249152444, 9.87736e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 550
rank avg (pred): 0.379 +- 0.000
mrr vals (pred, true): 0.000, 0.095
batch losses (mrrl, rdl): 0.0248928349, 0.0001228832

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.010 +- 0.000
mrr vals (pred, true): 0.004, 0.200
batch losses (mrrl, rdl): 0.3844276667, 0.0010171545

Epoch over!
epoch time: 15.02

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 278
rank avg (pred): 0.355 +- 0.000
mrr vals (pred, true): 0.000, 0.095
batch losses (mrrl, rdl): 0.024885891, 0.0001956125

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1204
rank avg (pred): 0.489 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249170251, 0.0001340424

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1139
rank avg (pred): 0.019 +- 0.000
mrr vals (pred, true): 0.002, 0.145
batch losses (mrrl, rdl): 0.2052357793, 0.0019887516

Epoch over!
epoch time: 15.062

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 902
rank avg (pred): 0.060 +- 0.000
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0243318621, 0.0043604742

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 201
rank avg (pred): 0.474 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249143243, 7.56999e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 617
rank avg (pred): 0.489 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249170214, 0.0001301646

Epoch over!
epoch time: 14.969

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 897
rank avg (pred): 0.433 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.024906313, 0.0027138542

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 924
rank avg (pred): 0.544 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249253493, 0.0001888049

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 563
rank avg (pred): 0.354 +- 0.000
mrr vals (pred, true): 0.000, 0.092
batch losses (mrrl, rdl): 0.0248854682, 0.0001536566

Epoch over!
epoch time: 14.966

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.012, 0.108

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   62 	     0 	 9e-0500 	 7e-0500 	 ~...
    1 	     1 	 7e-0500 	 0.00014 	 ~...
   37 	     2 	 8e-0500 	 0.00014 	 ~...
   59 	     3 	 9e-0500 	 0.00015 	 ~...
    2 	     4 	 7e-0500 	 0.00016 	 ~...
   23 	     5 	 8e-0500 	 0.00016 	 ~...
   83 	     6 	 0.00011 	 0.00016 	 ~...
   49 	     7 	 9e-0500 	 0.00016 	 ~...
   41 	     8 	 8e-0500 	 0.00017 	 ~...
   13 	     9 	 8e-0500 	 0.00017 	 ~...
   14 	    10 	 8e-0500 	 0.00018 	 ~...
   69 	    11 	 0.00010 	 0.00018 	 ~...
    4 	    12 	 8e-0500 	 0.00018 	 ~...
   26 	    13 	 8e-0500 	 0.00018 	 ~...
   32 	    14 	 8e-0500 	 0.00018 	 ~...
   53 	    15 	 9e-0500 	 0.00019 	 ~...
   17 	    16 	 8e-0500 	 0.00019 	 ~...
   57 	    17 	 9e-0500 	 0.00020 	 ~...
   72 	    18 	 0.00010 	 0.00020 	 ~...
   19 	    19 	 8e-0500 	 0.00020 	 ~...
   11 	    20 	 8e-0500 	 0.00020 	 ~...
   85 	    21 	 0.00011 	 0.00020 	 ~...
   16 	    22 	 8e-0500 	 0.00021 	 ~...
   28 	    23 	 8e-0500 	 0.00021 	 ~...
   45 	    24 	 8e-0500 	 0.00022 	 ~...
    0 	    25 	 7e-0500 	 0.00022 	 ~...
   40 	    26 	 8e-0500 	 0.00022 	 ~...
   15 	    27 	 8e-0500 	 0.00023 	 ~...
   31 	    28 	 8e-0500 	 0.00023 	 ~...
   24 	    29 	 8e-0500 	 0.00023 	 ~...
   30 	    30 	 8e-0500 	 0.00023 	 ~...
   54 	    31 	 9e-0500 	 0.00024 	 ~...
   25 	    32 	 8e-0500 	 0.00024 	 ~...
   47 	    33 	 8e-0500 	 0.00024 	 ~...
   56 	    34 	 9e-0500 	 0.00025 	 ~...
    8 	    35 	 8e-0500 	 0.00025 	 ~...
   82 	    36 	 0.00011 	 0.00026 	 ~...
   52 	    37 	 9e-0500 	 0.00026 	 ~...
   73 	    38 	 0.00010 	 0.00026 	 ~...
   35 	    39 	 8e-0500 	 0.00026 	 ~...
   20 	    40 	 8e-0500 	 0.00026 	 ~...
    3 	    41 	 8e-0500 	 0.00026 	 ~...
   38 	    42 	 8e-0500 	 0.00028 	 ~...
    9 	    43 	 8e-0500 	 0.00029 	 ~...
    7 	    44 	 8e-0500 	 0.00029 	 ~...
   22 	    45 	 8e-0500 	 0.00029 	 ~...
   33 	    46 	 8e-0500 	 0.00030 	 ~...
   12 	    47 	 8e-0500 	 0.00030 	 ~...
   29 	    48 	 8e-0500 	 0.00031 	 ~...
   36 	    49 	 8e-0500 	 0.00031 	 ~...
   60 	    50 	 9e-0500 	 0.00034 	 ~...
   18 	    51 	 8e-0500 	 0.00035 	 ~...
   34 	    52 	 8e-0500 	 0.00036 	 ~...
   55 	    53 	 9e-0500 	 0.00036 	 ~...
   43 	    54 	 8e-0500 	 0.00036 	 ~...
   65 	    55 	 0.00010 	 0.00037 	 ~...
   39 	    56 	 8e-0500 	 0.00037 	 ~...
   46 	    57 	 8e-0500 	 0.00042 	 ~...
   10 	    58 	 8e-0500 	 0.00043 	 ~...
   48 	    59 	 8e-0500 	 0.00044 	 ~...
   67 	    60 	 0.00010 	 0.00044 	 ~...
   44 	    61 	 8e-0500 	 0.00046 	 ~...
   70 	    62 	 0.00010 	 0.00047 	 ~...
   81 	    63 	 0.00011 	 0.00049 	 ~...
   71 	    64 	 0.00010 	 0.00049 	 ~...
   50 	    65 	 9e-0500 	 0.00049 	 ~...
   64 	    66 	 0.00010 	 0.00053 	 ~...
   78 	    67 	 0.00010 	 0.00055 	 ~...
   86 	    68 	 0.00011 	 0.00059 	 ~...
   27 	    69 	 8e-0500 	 0.00065 	 ~...
    6 	    70 	 8e-0500 	 0.00066 	 ~...
    5 	    71 	 8e-0500 	 0.00073 	 ~...
   68 	    72 	 0.00010 	 0.00100 	 ~...
   61 	    73 	 9e-0500 	 0.00104 	 ~...
   90 	    74 	 0.00013 	 0.00176 	 ~...
   51 	    75 	 9e-0500 	 0.00198 	 ~...
   21 	    76 	 8e-0500 	 0.00294 	 ~...
   58 	    77 	 9e-0500 	 0.00639 	 ~...
  115 	    78 	 0.02332 	 0.05909 	 m..s
   77 	    79 	 0.00010 	 0.06559 	 m..s
   91 	    80 	 0.00014 	 0.06984 	 m..s
  105 	    81 	 0.00386 	 0.07205 	 m..s
   88 	    82 	 0.00012 	 0.07375 	 m..s
   66 	    83 	 0.00010 	 0.07821 	 m..s
  103 	    84 	 0.00280 	 0.08062 	 m..s
   98 	    85 	 0.00029 	 0.08350 	 m..s
  112 	    86 	 0.00868 	 0.08502 	 m..s
   99 	    87 	 0.00086 	 0.08612 	 m..s
   76 	    88 	 0.00010 	 0.08648 	 m..s
   75 	    89 	 0.00010 	 0.08729 	 m..s
   84 	    90 	 0.00011 	 0.09332 	 m..s
  108 	    91 	 0.00496 	 0.09414 	 m..s
  116 	    92 	 0.02805 	 0.09774 	 m..s
   63 	    93 	 9e-0500 	 0.09858 	 m..s
   80 	    94 	 0.00011 	 0.10160 	 MISS
   97 	    95 	 0.00029 	 0.10315 	 MISS
   87 	    96 	 0.00012 	 0.10564 	 MISS
   96 	    97 	 0.00021 	 0.10641 	 MISS
  114 	    98 	 0.01195 	 0.10768 	 m..s
   94 	    99 	 0.00018 	 0.10821 	 MISS
  120 	   100 	 0.09958 	 0.10876 	 ~...
  113 	   101 	 0.01162 	 0.11751 	 MISS
   95 	   102 	 0.00019 	 0.12051 	 MISS
  111 	   103 	 0.00672 	 0.12088 	 MISS
   74 	   104 	 0.00010 	 0.12159 	 MISS
  100 	   105 	 0.00167 	 0.12530 	 MISS
   79 	   106 	 0.00011 	 0.13256 	 MISS
   92 	   107 	 0.00015 	 0.13971 	 MISS
   93 	   108 	 0.00015 	 0.14659 	 MISS
  104 	   109 	 0.00351 	 0.16142 	 MISS
  118 	   110 	 0.07947 	 0.16343 	 m..s
   89 	   111 	 0.00013 	 0.16359 	 MISS
  102 	   112 	 0.00273 	 0.17482 	 MISS
   42 	   113 	 8e-0500 	 0.18809 	 MISS
  109 	   114 	 0.00579 	 0.21337 	 MISS
  107 	   115 	 0.00446 	 0.29022 	 MISS
  106 	   116 	 0.00443 	 0.29862 	 MISS
  119 	   117 	 0.09556 	 0.30915 	 MISS
  117 	   118 	 0.07619 	 0.31081 	 MISS
  101 	   119 	 0.00201 	 0.35563 	 MISS
  110 	   120 	 0.00619 	 0.37952 	 MISS
==========================================
r_mrr = 0.4333442449569702
r2_mrr = -0.18281078338623047
spearmanr_mrr@5 = 0.7783313989639282
spearmanr_mrr@10 = 0.8957071900367737
spearmanr_mrr@50 = 0.835525393486023
spearmanr_mrr@100 = 0.7708152532577515
spearmanr_mrr@All = 0.7674302458763123
==========================================
test time: 0.456
Done Testing dataset DBpedia50
total time taken: 229.10439085960388
training time taken: 223.72312903404236
TWIG out ;))
Ablation done!
The best results were: None
The best settings found were:

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7325289107363570
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [920, 502, 184, 1133, 500, 592, 668, 1148, 258, 1174, 767, 1046, 887, 70, 295, 487, 1175, 652, 120, 16, 516, 321, 1099, 508, 1113, 1053, 52, 970, 530, 133, 1045, 325, 243, 797, 438, 1152, 1176, 271, 1102, 608, 1170, 625, 1210, 597, 1181, 369, 851, 994, 273, 383, 89, 496, 968, 181, 1038, 969, 84, 939, 698, 929, 99, 653, 1011, 460, 177, 973, 620, 82, 1031, 977, 840, 503, 1199, 832, 279, 1060, 548, 68, 991, 439, 1016, 450, 408, 388, 87, 341, 606, 1091, 1186, 126, 563, 69, 256, 728, 1139, 251, 556, 922, 3, 14, 434, 1026, 714, 482, 864, 941, 1125, 918, 658, 716, 230, 945, 749, 574, 687, 1171, 1049, 1012, 18, 37, 580]
valid_ids (0): []
train_ids (1094): [1094, 418, 927, 400, 1054, 201, 878, 111, 550, 289, 246, 39, 40, 986, 389, 75, 506, 925, 440, 380, 891, 898, 744, 857, 119, 513, 776, 821, 672, 953, 1119, 818, 946, 691, 303, 476, 0, 909, 268, 8, 285, 1015, 199, 865, 419, 727, 1190, 223, 729, 1043, 182, 480, 924, 1201, 1197, 297, 537, 45, 1112, 573, 517, 560, 957, 1137, 551, 93, 240, 631, 1041, 338, 789, 451, 1184, 628, 1179, 769, 1040, 614, 307, 1081, 562, 1114, 1150, 889, 838, 700, 121, 585, 740, 913, 812, 187, 1159, 591, 1183, 527, 932, 993, 558, 493, 456, 237, 1117, 160, 1172, 895, 967, 42, 534, 190, 661, 130, 1059, 290, 1071, 762, 155, 208, 207, 481, 141, 1196, 826, 484, 168, 478, 1212, 218, 495, 755, 626, 124, 754, 235, 162, 224, 173, 464, 1187, 324, 463, 978, 422, 647, 1193, 1124, 809, 611, 618, 100, 28, 853, 371, 71, 35, 686, 676, 645, 702, 954, 1028, 656, 813, 346, 919, 535, 515, 526, 955, 671, 747, 486, 869, 989, 455, 533, 900, 917, 304, 145, 417, 90, 518, 1143, 1009, 961, 241, 83, 431, 1032, 1200, 884, 221, 1214, 314, 217, 1021, 1083, 349, 1204, 1065, 1019, 51, 106, 31, 879, 1131, 1156, 854, 972, 1169, 971, 326, 801, 1127, 651, 416, 567, 347, 858, 806, 191, 1108, 600, 62, 699, 421, 681, 483, 196, 603, 437, 873, 405, 77, 219, 741, 281, 781, 866, 1055, 657, 856, 97, 169, 947, 188, 981, 916, 1023, 1084, 316, 1090, 992, 150, 339, 568, 570, 1182, 212, 452, 860, 862, 791, 29, 844, 161, 1085, 475, 876, 209, 733, 469, 498, 1030, 1157, 1118, 708, 514, 612, 435, 679, 1014, 137, 158, 807, 61, 306, 1158, 131, 828, 760, 379, 151, 78, 286, 1105, 587, 1213, 931, 710, 646, 1198, 522, 731, 673, 139, 1077, 1062, 852, 694, 870, 332, 311, 110, 787, 649, 38, 565, 943, 63, 892, 144, 512, 538, 665, 842, 904, 394, 914, 291, 270, 950, 32, 178, 340, 393, 391, 1075, 489, 377, 525, 1069, 1086, 982, 1001, 1202, 56, 157, 354, 693, 1138, 1206, 829, 704, 72, 319, 677, 350, 59, 1130, 888, 598, 1164, 125, 765, 963, 983, 468, 453, 1111, 24, 644, 761, 1154, 547, 477, 293, 1189, 976, 539, 1002, 320, 616, 604, 578, 793, 890, 774, 262, 1078, 461, 726, 85, 413, 1136, 175, 1050, 770, 819, 472, 1110, 855, 752, 707, 328, 984, 1155, 544, 1101, 905, 875, 798, 272, 1057, 1018, 406, 1058, 176, 689, 105, 443, 1024, 259, 613, 846, 910, 1096, 488, 206, 269, 352, 923, 822, 25, 666, 283, 334, 609, 524, 893, 959, 412, 494, 763, 232, 88, 22, 764, 81, 958, 997, 127, 784, 936, 66, 129, 385, 12, 933, 835, 794, 238, 885, 778, 519, 331, 715, 874, 1140, 1017, 1104, 777, 629, 1191, 1100, 799, 47, 724, 1185, 542, 872, 205, 610, 594, 590, 101, 280, 662, 115, 210, 718, 951, 1020, 965, 935, 785, 630, 429, 847, 395, 287, 186, 588, 103, 617, 359, 366, 743, 102, 20, 449, 198, 593, 1177, 678, 1160, 323, 58, 147, 192, 664, 696, 1116, 109, 409, 607, 757, 897, 1064, 674, 226, 949, 639, 682, 344, 841, 446, 814, 154, 557, 583, 333, 1163, 1173, 112, 605, 786, 420, 690, 1072, 466, 479, 1033, 701, 995, 685, 54, 403, 255, 312, 149, 589, 436, 41, 734, 355, 447, 263, 723, 859, 353, 579, 153, 92, 1, 529, 128, 30, 899, 1192, 203, 523, 1147, 411, 1103, 926, 309, 908, 123, 156, 824, 660, 684, 1098, 335, 980, 596, 930, 117, 257, 601, 1207, 302, 402, 720, 17, 1109, 880, 5, 632, 1178, 10, 247, 337, 571, 1141, 64, 1037, 1006, 136, 228, 470, 737, 705, 680, 1082, 575, 713, 1208, 1079, 261, 284, 572, 670, 780, 148, 467, 135, 638, 397, 848, 342, 759, 559, 396, 751, 43, 390, 771, 552, 1162, 634, 398, 27, 800, 877, 937, 944, 962, 564, 9, 107, 1151, 881, 1180, 654, 387, 619, 810, 627, 1047, 275, 1056, 602, 26, 50, 1004, 820, 717, 911, 278, 633, 239, 849, 21, 811, 663, 1027, 1129, 473, 975, 803, 425, 1035, 659, 1097, 211, 351, 566, 985, 301, 772, 831, 492, 1168, 637, 1115, 688, 471, 202, 432, 942, 1165, 1088, 1003, 692, 640, 655, 882, 553, 1123, 775, 368, 44, 541, 528, 845, 505, 1153, 850, 1106, 624, 706, 252, 166, 372, 225, 367, 172, 966, 348, 53, 1121, 894, 424, 376, 448, 1005, 1188, 444, 902, 1194, 1205, 214, 915, 536, 370, 248, 292, 648, 134, 315, 282, 1070, 996, 180, 531, 1211, 990, 96, 392, 883, 1022, 906, 796, 98, 987, 1080, 57, 364, 94, 756, 712, 622, 839, 1034, 782, 1013, 511, 213, 1052, 374, 277, 790, 220, 222, 329, 65, 116, 545, 1042, 825, 91, 561, 896, 549, 465, 288, 1134, 48, 318, 871, 104, 650, 159, 675, 861, 520, 1149, 276, 1128, 1107, 697, 490, 76, 683, 948, 501, 298, 1063, 802, 378, 15, 363, 742, 532, 423, 1008, 95, 132, 138, 294, 499, 709, 474, 1036, 867, 365, 382, 783, 868, 952, 46, 739, 974, 113, 928, 1120, 204, 249, 736, 510, 122, 491, 921, 236, 361, 1048, 635, 384, 1051, 183, 497, 843, 555, 576, 414, 903, 362, 938, 1068, 641, 231, 1209, 360, 459, 722, 912, 80, 410, 4, 1122, 34, 695, 457, 1195, 1166, 901, 266, 1073, 586, 23, 823, 74, 67, 1000, 1087, 299, 441, 234, 956, 1132, 768, 415, 725, 152, 711, 358, 540, 750, 313, 194, 442, 357, 250, 274, 336, 1029, 356, 322, 907, 433, 13, 373, 426, 485, 507, 1135, 817, 1144, 171, 521, 254, 317, 79, 310, 330, 758, 140, 343, 1142, 1095, 253, 185, 73, 49, 33, 804, 264, 833, 595, 827, 546, 245, 195, 399, 1066, 830, 1039, 643, 19, 345, 216, 816, 454, 577, 375, 667, 584, 999, 703, 428, 233, 621, 773, 581, 163, 863, 1010, 458, 197, 308, 242, 11, 1146, 6, 427, 2, 504, 227, 215, 960, 193, 748, 179, 988, 746, 623, 836, 1161, 260, 1061, 1126, 719, 170, 1093, 615, 738, 404, 636, 554, 805, 114, 167, 244, 886, 1067, 1145, 164, 934, 60, 174, 732, 795, 582, 1074, 669, 265, 300, 642, 964, 108, 1092, 753, 1044, 1007, 1089, 305, 808, 792, 543, 407, 430, 998, 1203, 142, 599, 36, 779, 788, 55, 569, 745, 86, 267, 815, 146, 766, 979, 735, 229, 445, 327, 1167, 509, 200, 401, 143, 1025, 462, 837, 940, 834, 381, 7, 1076, 721, 730, 189, 296, 386, 165, 118]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3975236193949154
the save name prefix for this run is:  chkpt-ID_3975236193949154_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 713
rank avg (pred): 0.592 +- 0.008
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003239095

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.476 +- 0.277
mrr vals (pred, true): 0.133, 0.000
batch losses (mrrl, rdl): 0.0, 4.2812e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 475
rank avg (pred): 0.452 +- 0.276
mrr vals (pred, true): 0.180, 0.000
batch losses (mrrl, rdl): 0.0, 2.13509e-05

Epoch over!
epoch time: 14.708

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 422
rank avg (pred): 0.465 +- 0.282
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.0, 1.25542e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 39
rank avg (pred): 0.349 +- 0.214
mrr vals (pred, true): 0.196, 0.080
batch losses (mrrl, rdl): 0.0, 7.89945e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.450 +- 0.277
mrr vals (pred, true): 0.209, 0.000
batch losses (mrrl, rdl): 0.0, 2.19906e-05

Epoch over!
epoch time: 14.711

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 568
rank avg (pred): 0.453 +- 0.279
mrr vals (pred, true): 0.207, 0.000
batch losses (mrrl, rdl): 0.0, 1.18929e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1207
rank avg (pred): 0.470 +- 0.286
mrr vals (pred, true): 0.198, 0.001
batch losses (mrrl, rdl): 0.0, 5.5062e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1111
rank avg (pred): 0.472 +- 0.289
mrr vals (pred, true): 0.206, 0.001
batch losses (mrrl, rdl): 0.0, 4.31468e-05

Epoch over!
epoch time: 14.734

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.450 +- 0.275
mrr vals (pred, true): 0.205, 0.000
batch losses (mrrl, rdl): 0.0, 1.24231e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 575
rank avg (pred): 0.460 +- 0.284
mrr vals (pred, true): 0.234, 0.000
batch losses (mrrl, rdl): 0.0, 1.35829e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.453 +- 0.274
mrr vals (pred, true): 0.229, 0.001
batch losses (mrrl, rdl): 0.0, 1.25038e-05

Epoch over!
epoch time: 14.724

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1150
rank avg (pred): 0.303 +- 0.187
mrr vals (pred, true): 0.250, 0.145
batch losses (mrrl, rdl): 0.0, 4.57066e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.462 +- 0.294
mrr vals (pred, true): 0.236, 0.000
batch losses (mrrl, rdl): 0.0, 1.49863e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1081
rank avg (pred): 0.457 +- 0.276
mrr vals (pred, true): 0.218, 0.001
batch losses (mrrl, rdl): 0.0, 2.13074e-05

Epoch over!
epoch time: 14.73

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1183
rank avg (pred): 0.461 +- 0.297
mrr vals (pred, true): 0.243, 0.000
batch losses (mrrl, rdl): 0.3740948737, 2.14393e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.587 +- 0.276
mrr vals (pred, true): 0.069, 0.001
batch losses (mrrl, rdl): 0.0036881, 0.0002828804

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 679
rank avg (pred): 0.584 +- 0.331
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0007952392, 0.0003668477

Epoch over!
epoch time: 14.932

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1073
rank avg (pred): 0.064 +- 0.039
mrr vals (pred, true): 0.086, 0.082
batch losses (mrrl, rdl): 0.0128067834, 0.0013189652

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 839
rank avg (pred): 0.303 +- 0.164
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004731795, 0.0007211902

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1163
rank avg (pred): 0.686 +- 0.366
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.000740139, 0.0009781247

Epoch over!
epoch time: 14.891

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 417
rank avg (pred): 0.583 +- 0.323
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006692891, 0.0001618465

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 978
rank avg (pred): 0.087 +- 0.061
mrr vals (pred, true): 0.143, 0.213
batch losses (mrrl, rdl): 0.0487696156, 0.0004965966

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.711 +- 0.347
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004972109, 0.0011692251

Epoch over!
epoch time: 14.971

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.632 +- 0.337
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005232326, 0.0004750374

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 855
rank avg (pred): 0.595 +- 0.311
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0005078114, 0.0002841537

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 100
rank avg (pred): 0.372 +- 0.240
mrr vals (pred, true): 0.072, 0.000
batch losses (mrrl, rdl): 0.004840028, 0.0001311387

Epoch over!
epoch time: 14.953

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 182
rank avg (pred): 0.518 +- 0.305
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006460584, 3.17402e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.734 +- 0.345
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004879409, 0.0011202219

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 582
rank avg (pred): 0.633 +- 0.356
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006131295, 0.0002902786

Epoch over!
epoch time: 14.877

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.522 +- 0.314
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0011242044, 0.0001298169

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.727 +- 0.353
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004955239, 0.000933456

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 102
rank avg (pred): 0.456 +- 0.293
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0029123633, 1.68319e-05

Epoch over!
epoch time: 14.881

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.583 +- 0.288
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004311288, 0.0001408724

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 876
rank avg (pred): 0.534 +- 0.265
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002459392, 5.461e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1195
rank avg (pred): 0.575 +- 0.308
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005116819, 0.0001696564

Epoch over!
epoch time: 14.865

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1066
rank avg (pred): 0.119 +- 0.086
mrr vals (pred, true): 0.078, 0.098
batch losses (mrrl, rdl): 0.0080800289, 0.0008028205

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 144
rank avg (pred): 0.612 +- 0.305
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004489806, 0.0003533582

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1072
rank avg (pred): 0.095 +- 0.072
mrr vals (pred, true): 0.097, 0.088
batch losses (mrrl, rdl): 0.0224271696, 0.0013059728

Epoch over!
epoch time: 14.911

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 514
rank avg (pred): 0.574 +- 0.279
mrr vals (pred, true): 0.057, 0.097
batch losses (mrrl, rdl): 0.0004371587, 0.0008138061

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 75
rank avg (pred): 0.344 +- 0.265
mrr vals (pred, true): 0.075, 0.090
batch losses (mrrl, rdl): 0.0062783677, 2.6199e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 697
rank avg (pred): 0.746 +- 0.338
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.01885e-05, 0.0010499187

Epoch over!
epoch time: 14.971

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.707 +- 0.323
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.02e-08, 0.000704436

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.563 +- 0.310
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005273382, 0.0001004897

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 494
rank avg (pred): 0.108 +- 0.088
mrr vals (pred, true): 0.112, 0.113
batch losses (mrrl, rdl): 3.4739e-06, 0.0013071977

Epoch over!
epoch time: 14.967

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.898 +- 0.249
mrr vals (pred, true): 0.045, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   26 	     0 	 0.05687 	 0.00015 	 m..s
   49 	     1 	 0.05709 	 0.00016 	 m..s
   19 	     2 	 0.05671 	 0.00016 	 m..s
   93 	     3 	 0.06116 	 0.00016 	 m..s
    8 	     4 	 0.05383 	 0.00016 	 m..s
   94 	     5 	 0.06135 	 0.00016 	 m..s
   18 	     6 	 0.05670 	 0.00016 	 m..s
   34 	     7 	 0.05697 	 0.00016 	 m..s
   59 	     8 	 0.05723 	 0.00017 	 m..s
   31 	     9 	 0.05694 	 0.00017 	 m..s
   16 	    10 	 0.05668 	 0.00017 	 m..s
   46 	    11 	 0.05709 	 0.00017 	 m..s
    7 	    12 	 0.05350 	 0.00017 	 m..s
    6 	    13 	 0.05339 	 0.00017 	 m..s
    3 	    14 	 0.04557 	 0.00018 	 m..s
   20 	    15 	 0.05673 	 0.00018 	 m..s
    4 	    16 	 0.04882 	 0.00019 	 m..s
   42 	    17 	 0.05707 	 0.00020 	 m..s
   71 	    18 	 0.05745 	 0.00020 	 m..s
    0 	    19 	 0.04424 	 0.00020 	 m..s
   84 	    20 	 0.05821 	 0.00021 	 m..s
   15 	    21 	 0.05662 	 0.00021 	 m..s
   58 	    22 	 0.05723 	 0.00022 	 m..s
   51 	    23 	 0.05711 	 0.00022 	 m..s
   56 	    24 	 0.05720 	 0.00022 	 m..s
   69 	    25 	 0.05741 	 0.00022 	 m..s
   10 	    26 	 0.05572 	 0.00022 	 m..s
   45 	    27 	 0.05708 	 0.00022 	 m..s
   67 	    28 	 0.05728 	 0.00022 	 m..s
   17 	    29 	 0.05669 	 0.00023 	 m..s
   55 	    30 	 0.05719 	 0.00024 	 m..s
   41 	    31 	 0.05706 	 0.00024 	 m..s
    9 	    32 	 0.05398 	 0.00024 	 m..s
   40 	    33 	 0.05706 	 0.00024 	 m..s
   76 	    34 	 0.05758 	 0.00024 	 m..s
   63 	    35 	 0.05725 	 0.00025 	 m..s
   78 	    36 	 0.05771 	 0.00025 	 m..s
   80 	    37 	 0.05784 	 0.00025 	 m..s
   90 	    38 	 0.05927 	 0.00026 	 m..s
   70 	    39 	 0.05745 	 0.00026 	 m..s
   65 	    40 	 0.05728 	 0.00026 	 m..s
   11 	    41 	 0.05573 	 0.00027 	 m..s
   24 	    42 	 0.05686 	 0.00027 	 m..s
   29 	    43 	 0.05691 	 0.00027 	 m..s
   54 	    44 	 0.05718 	 0.00027 	 m..s
   47 	    45 	 0.05709 	 0.00028 	 m..s
   64 	    46 	 0.05727 	 0.00028 	 m..s
   30 	    47 	 0.05693 	 0.00028 	 m..s
   38 	    48 	 0.05702 	 0.00028 	 m..s
   88 	    49 	 0.05858 	 0.00029 	 m..s
   32 	    50 	 0.05695 	 0.00029 	 m..s
    1 	    51 	 0.04487 	 0.00029 	 m..s
   98 	    52 	 0.06689 	 0.00029 	 m..s
   87 	    53 	 0.05853 	 0.00029 	 m..s
   61 	    54 	 0.05724 	 0.00030 	 m..s
   52 	    55 	 0.05713 	 0.00031 	 m..s
   85 	    56 	 0.05826 	 0.00031 	 m..s
   75 	    57 	 0.05757 	 0.00032 	 m..s
   82 	    58 	 0.05810 	 0.00033 	 m..s
   77 	    59 	 0.05758 	 0.00034 	 m..s
   44 	    60 	 0.05708 	 0.00034 	 m..s
   21 	    61 	 0.05679 	 0.00034 	 m..s
    5 	    62 	 0.04892 	 0.00036 	 m..s
   66 	    63 	 0.05728 	 0.00037 	 m..s
   74 	    64 	 0.05757 	 0.00037 	 m..s
   60 	    65 	 0.05723 	 0.00037 	 m..s
   43 	    66 	 0.05707 	 0.00039 	 m..s
   81 	    67 	 0.05788 	 0.00041 	 m..s
   91 	    68 	 0.05984 	 0.00041 	 m..s
   23 	    69 	 0.05685 	 0.00041 	 m..s
   22 	    70 	 0.05685 	 0.00041 	 m..s
   36 	    71 	 0.05699 	 0.00042 	 m..s
   57 	    72 	 0.05721 	 0.00043 	 m..s
   14 	    73 	 0.05657 	 0.00045 	 m..s
   48 	    74 	 0.05709 	 0.00046 	 m..s
   86 	    75 	 0.05845 	 0.00049 	 m..s
   73 	    76 	 0.05756 	 0.00050 	 m..s
   27 	    77 	 0.05688 	 0.00060 	 m..s
   53 	    78 	 0.05718 	 0.00060 	 m..s
   25 	    79 	 0.05686 	 0.00065 	 m..s
   83 	    80 	 0.05813 	 0.00082 	 m..s
   12 	    81 	 0.05596 	 0.00104 	 m..s
   72 	    82 	 0.05756 	 0.00166 	 m..s
   13 	    83 	 0.05655 	 0.00176 	 m..s
    2 	    84 	 0.04534 	 0.00890 	 m..s
   97 	    85 	 0.06601 	 0.07237 	 ~...
   68 	    86 	 0.05740 	 0.07732 	 ~...
   39 	    87 	 0.05703 	 0.07899 	 ~...
  115 	    88 	 0.14553 	 0.08004 	 m..s
   96 	    89 	 0.06473 	 0.08077 	 ~...
   50 	    90 	 0.05711 	 0.08131 	 ~...
   95 	    91 	 0.06254 	 0.08574 	 ~...
  112 	    92 	 0.11800 	 0.09133 	 ~...
   89 	    93 	 0.05917 	 0.09160 	 m..s
  100 	    94 	 0.06738 	 0.09259 	 ~...
   37 	    95 	 0.05700 	 0.09528 	 m..s
  103 	    96 	 0.07270 	 0.09709 	 ~...
   62 	    97 	 0.05724 	 0.09747 	 m..s
   92 	    98 	 0.06110 	 0.09894 	 m..s
   35 	    99 	 0.05699 	 0.09994 	 m..s
   33 	   100 	 0.05695 	 0.10153 	 m..s
  101 	   101 	 0.06922 	 0.10168 	 m..s
  106 	   102 	 0.07902 	 0.10792 	 ~...
  116 	   103 	 0.15332 	 0.11548 	 m..s
   79 	   104 	 0.05781 	 0.12159 	 m..s
   28 	   105 	 0.05690 	 0.12593 	 m..s
   99 	   106 	 0.06701 	 0.13084 	 m..s
  104 	   107 	 0.07397 	 0.13421 	 m..s
  109 	   108 	 0.09201 	 0.14273 	 m..s
  114 	   109 	 0.14076 	 0.14537 	 ~...
  102 	   110 	 0.07148 	 0.14659 	 m..s
  108 	   111 	 0.08788 	 0.14850 	 m..s
  105 	   112 	 0.07479 	 0.14951 	 m..s
  107 	   113 	 0.08702 	 0.15039 	 m..s
  111 	   114 	 0.11314 	 0.15115 	 m..s
  110 	   115 	 0.10181 	 0.15728 	 m..s
  118 	   116 	 0.17791 	 0.17482 	 ~...
  119 	   117 	 0.21268 	 0.20291 	 ~...
  120 	   118 	 0.25311 	 0.23873 	 ~...
  113 	   119 	 0.12456 	 0.29656 	 MISS
  117 	   120 	 0.16140 	 0.31824 	 MISS
==========================================
r_mrr = 0.732869029045105
r2_mrr = 0.28416550159454346
spearmanr_mrr@5 = 0.9625279903411865
spearmanr_mrr@10 = 0.9681193232536316
spearmanr_mrr@50 = 0.8657065033912659
spearmanr_mrr@100 = 0.8610782623291016
spearmanr_mrr@All = 0.8661849498748779
==========================================
test time: 0.45
Done Testing dataset DBpedia50
total time taken: 228.61183214187622
training time taken: 223.28555154800415
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.7329)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.2842)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9625)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9681)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.8657)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.8611)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.8662)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.2840614262386225}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 3732072874912622
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [83, 176, 591, 121, 1054, 745, 993, 164, 461, 1061, 847, 194, 599, 436, 430, 362, 893, 1075, 848, 914, 1109, 119, 33, 1202, 846, 337, 1148, 629, 115, 509, 1071, 539, 311, 871, 1116, 153, 534, 936, 426, 257, 756, 37, 243, 887, 1151, 963, 958, 903, 374, 608, 308, 1070, 476, 175, 763, 899, 333, 760, 1145, 54, 603, 246, 794, 519, 796, 313, 635, 880, 56, 474, 6, 823, 829, 195, 1101, 1063, 878, 144, 790, 689, 475, 331, 815, 356, 435, 777, 491, 585, 497, 697, 465, 909, 1194, 464, 1083, 456, 989, 981, 412, 406, 930, 261, 172, 390, 378, 405, 90, 167, 384, 844, 233, 1181, 399, 531, 336, 758, 85, 163, 761, 286, 984]
valid_ids (0): []
train_ids (1094): [875, 62, 1066, 877, 741, 990, 417, 343, 1125, 929, 1014, 1123, 955, 403, 671, 1146, 801, 285, 791, 256, 792, 23, 684, 1019, 650, 35, 1126, 806, 386, 649, 938, 583, 563, 705, 87, 713, 92, 1105, 391, 607, 548, 573, 1168, 483, 707, 822, 12, 470, 965, 107, 1113, 303, 595, 1093, 126, 422, 641, 982, 493, 632, 22, 368, 67, 602, 1049, 340, 811, 578, 716, 288, 553, 111, 864, 881, 423, 214, 621, 1004, 654, 244, 189, 506, 554, 728, 751, 890, 110, 1047, 49, 407, 65, 701, 1144, 1154, 197, 98, 10, 1090, 351, 879, 326, 917, 1082, 415, 746, 648, 853, 510, 275, 320, 316, 322, 503, 365, 287, 652, 21, 865, 1053, 535, 136, 895, 1176, 278, 432, 201, 1128, 307, 885, 540, 1097, 622, 862, 1201, 112, 724, 170, 225, 185, 514, 805, 1196, 749, 886, 1078, 849, 729, 868, 342, 610, 279, 1058, 492, 84, 1163, 158, 1141, 488, 66, 908, 284, 447, 251, 772, 1080, 884, 904, 200, 557, 769, 755, 827, 81, 1212, 830, 147, 773, 1206, 670, 485, 413, 951, 562, 1189, 20, 105, 36, 738, 788, 859, 148, 262, 516, 224, 103, 732, 655, 481, 851, 935, 1012, 269, 1150, 690, 766, 370, 517, 924, 797, 236, 582, 4, 241, 734, 712, 381, 633, 80, 611, 1197, 344, 379, 874, 783, 1118, 99, 888, 957, 1167, 1110, 778, 559, 742, 793, 765, 397, 997, 387, 495, 994, 679, 389, 1041, 234, 180, 1074, 60, 845, 438, 710, 1121, 318, 151, 782, 525, 513, 623, 706, 544, 396, 134, 102, 979, 218, 42, 658, 94, 572, 926, 215, 202, 1002, 454, 346, 804, 714, 567, 424, 940, 267, 1022, 1065, 941, 187, 91, 717, 733, 678, 643, 928, 921, 1057, 1149, 32, 463, 767, 663, 31, 1133, 907, 479, 392, 905, 208, 293, 825, 1021, 77, 46, 837, 718, 58, 9, 486, 613, 520, 776, 645, 1205, 703, 1119, 532, 157, 677, 1036, 942, 1028, 1204, 211, 852, 1087, 821, 118, 814, 1214, 664, 524, 526, 143, 781, 694, 468, 1129, 931, 818, 274, 784, 489, 681, 1001, 372, 138, 108, 79, 128, 1084, 350, 1077, 660, 918, 740, 1011, 433, 832, 437, 297, 787, 428, 715, 268, 1031, 809, 150, 238, 15, 45, 161, 948, 836, 1102, 249, 76, 310, 956, 969, 1188, 100, 972, 722, 580, 527, 443, 1015, 579, 1136, 444, 1135, 89, 259, 181, 124, 7, 891, 523, 1076, 1023, 394, 575, 977, 358, 184, 473, 484, 600, 135, 114, 345, 371, 472, 744, 188, 642, 63, 634, 699, 619, 604, 26, 306, 348, 160, 858, 75, 1195, 1138, 298, 779, 537, 704, 248, 409, 449, 367, 518, 636, 292, 154, 1182, 727, 305, 589, 141, 2, 304, 70, 721, 925, 171, 1, 983, 383, 725, 496, 1186, 43, 330, 339, 651, 960, 1132, 68, 51, 477, 1112, 235, 975, 55, 731, 590, 228, 283, 120, 1172, 850, 726, 838, 876, 231, 263, 441, 624, 667, 301, 113, 500, 317, 177, 451, 946, 1127, 894, 762, 1098, 869, 460, 361, 452, 702, 295, 425, 1060, 616, 617, 291, 1020, 349, 687, 1104, 1055, 359, 676, 873, 536, 980, 166, 939, 369, 759, 352, 558, 270, 223, 471, 482, 584, 770, 834, 97, 934, 408, 14, 1180, 27, 1027, 1164, 419, 910, 1152, 854, 1140, 1016, 123, 1170, 179, 754, 954, 892, 596, 57, 1086, 82, 870, 1193, 668, 820, 1103, 469, 669, 817, 803, 637, 1203, 1029, 597, 711, 1190, 156, 1209, 686, 1174, 675, 656, 183, 101, 357, 467, 302, 199, 1183, 1072, 192, 819, 696, 962, 1124, 429, 1122, 618, 480, 866, 1159, 1198, 512, 137, 565, 324, 1211, 971, 1155, 511, 457, 528, 657, 533, 221, 780, 768, 842, 680, 896, 673, 321, 612, 59, 427, 142, 264, 872, 0, 568, 53, 901, 96, 1096, 626, 1179, 665, 190, 289, 117, 315, 764, 576, 882, 1046, 606, 1115, 1130, 1137, 753, 556, 366, 1120, 93, 266, 132, 1161, 219, 50, 808, 1067, 952, 216, 169, 442, 265, 462, 186, 1165, 355, 494, 395, 393, 335, 1039, 708, 639, 688, 205, 332, 230, 906, 341, 418, 564, 646, 478, 661, 440, 73, 1007, 691, 719, 198, 683, 282, 314, 593, 327, 798, 1013, 1114, 453, 191, 466, 529, 546, 674, 682, 382, 1079, 786, 1185, 541, 130, 571, 659, 843, 95, 976, 1160, 1037, 915, 71, 48, 598, 1009, 1038, 203, 182, 220, 226, 609, 168, 978, 44, 17, 968, 1095, 577, 61, 152, 252, 404, 1099, 334, 1200, 258, 961, 373, 106, 24, 434, 125, 1068, 775, 401, 855, 730, 1017, 1142, 146, 638, 363, 294, 19, 856, 949, 299, 222, 431, 698, 398, 995, 1005, 30, 757, 771, 992, 631, 319, 547, 1171, 1094, 5, 140, 947, 1035, 826, 416, 18, 1192, 933, 1073, 1143, 841, 1207, 986, 247, 347, 810, 227, 867, 700, 1089, 750, 239, 1106, 375, 421, 122, 1158, 1032, 490, 860, 210, 254, 34, 959, 1064, 807, 1059, 644, 902, 504, 1052, 897, 974, 1173, 1111, 196, 325, 709, 40, 816, 276, 88, 74, 64, 863, 28, 1162, 640, 991, 662, 155, 328, 988, 919, 666, 149, 920, 1033, 1048, 193, 912, 11, 605, 802, 385, 945, 414, 1157, 1026, 209, 104, 923, 109, 828, 795, 1006, 627, 999, 927, 739, 501, 1169, 1208, 857, 145, 970, 620, 883, 922, 237, 628, 439, 458, 785, 207, 515, 323, 647, 499, 950, 38, 1003, 566, 281, 581, 229, 833, 260, 543, 774, 1025, 1184, 1040, 507, 789, 300, 1091, 1024, 813, 1210, 212, 133, 521, 1050, 1177, 173, 1056, 1107, 653, 743, 13, 360, 551, 752, 296, 273, 388, 550, 290, 410, 1131, 129, 1000, 271, 39, 502, 380, 41, 800, 165, 735, 569, 420, 561, 450, 953, 1018, 1199, 987, 1117, 943, 522, 174, 1081, 1134, 25, 505, 402, 748, 3, 601, 720, 587, 549, 560, 861, 538, 250, 1139, 400, 376, 911, 630, 498, 985, 455, 240, 542, 998, 459, 162, 217, 116, 799, 206, 448, 232, 840, 329, 1043, 69, 615, 570, 1042, 139, 309, 354, 900, 1147, 487, 964, 52, 736, 1156, 545, 552, 1153, 1030, 625, 1051, 1008, 280, 508, 1213, 723, 824, 245, 16, 586, 835, 695, 672, 1085, 1088, 693, 1100, 78, 353, 692, 47, 932, 555, 204, 1034, 253, 812, 242, 1045, 1187, 446, 967, 29, 574, 839, 996, 1044, 445, 747, 685, 966, 588, 898, 159, 530, 213, 131, 277, 127, 614, 973, 594, 272, 592, 255, 8, 937, 889, 1108, 1010, 178, 916, 377, 737, 831, 913, 411, 944, 86, 338, 364, 1062, 1166, 1175, 1178, 1191, 1069, 1092, 312, 72]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4575526622048045
the save name prefix for this run is:  chkpt-ID_4575526622048045_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.545 +- 0.006
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001785318

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.483 +- 0.282
mrr vals (pred, true): 0.185, 0.000
batch losses (mrrl, rdl): 0.0, 4.8431e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.451 +- 0.281
mrr vals (pred, true): 0.209, 0.000
batch losses (mrrl, rdl): 0.0, 1.57416e-05

Epoch over!
epoch time: 14.763

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.351 +- 0.221
mrr vals (pred, true): 0.220, 0.082
batch losses (mrrl, rdl): 0.0, 6.09338e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.244 +- 0.154
mrr vals (pred, true): 0.230, 0.239
batch losses (mrrl, rdl): 0.0, 0.0001304997

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1166
rank avg (pred): 0.454 +- 0.285
mrr vals (pred, true): 0.220, 0.000
batch losses (mrrl, rdl): 0.0, 1.42808e-05

Epoch over!
epoch time: 14.713

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.194 +- 0.125
mrr vals (pred, true): 0.236, 0.235
batch losses (mrrl, rdl): 0.0, 0.000108625

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 950
rank avg (pred): 0.476 +- 0.302
mrr vals (pred, true): 0.238, 0.000
batch losses (mrrl, rdl): 0.0, 1.86433e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 831
rank avg (pred): 0.227 +- 0.147
mrr vals (pred, true): 0.245, 0.205
batch losses (mrrl, rdl): 0.0, 8.1679e-05

Epoch over!
epoch time: 14.73

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1152
rank avg (pred): 0.355 +- 0.227
mrr vals (pred, true): 0.235, 0.097
batch losses (mrrl, rdl): 0.0, 2.91211e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.282 +- 0.191
mrr vals (pred, true): 0.252, 0.187
batch losses (mrrl, rdl): 0.0, 0.0001055149

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 793
rank avg (pred): 0.457 +- 0.302
mrr vals (pred, true): 0.229, 0.001
batch losses (mrrl, rdl): 0.0, 1.64631e-05

Epoch over!
epoch time: 14.778

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1160
rank avg (pred): 0.315 +- 0.212
mrr vals (pred, true): 0.238, 0.137
batch losses (mrrl, rdl): 0.0, 4.62967e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 769
rank avg (pred): 0.451 +- 0.296
mrr vals (pred, true): 0.205, 0.000
batch losses (mrrl, rdl): 0.0, 1.18755e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 344
rank avg (pred): 0.457 +- 0.293
mrr vals (pred, true): 0.193, 0.000
batch losses (mrrl, rdl): 0.0, 8.6506e-06

Epoch over!
epoch time: 14.793

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.082 +- 0.056
mrr vals (pred, true): 0.239, 0.059
batch losses (mrrl, rdl): 0.3584433794, 0.0005607367

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 885
rank avg (pred): 0.475 +- 0.197
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005343903, 6.4445e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 67
rank avg (pred): 0.459 +- 0.238
mrr vals (pred, true): 0.060, 0.098
batch losses (mrrl, rdl): 0.0009212607, 0.0003664484

Epoch over!
epoch time: 14.99

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 611
rank avg (pred): 0.514 +- 0.157
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004824502, 8.7865e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.516 +- 0.149
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.26203e-05, 0.0001049126

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 31
rank avg (pred): 0.448 +- 0.208
mrr vals (pred, true): 0.059, 0.088
batch losses (mrrl, rdl): 0.0008740081, 0.0004037693

Epoch over!
epoch time: 14.962

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 952
rank avg (pred): 0.468 +- 0.182
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005114405, 5.16191e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 882
rank avg (pred): 0.509 +- 0.137
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001931934, 7.98002e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 258
rank avg (pred): 0.212 +- 0.175
mrr vals (pred, true): 0.198, 0.297
batch losses (mrrl, rdl): 0.0977862328, 0.0001367403

Epoch over!
epoch time: 14.978

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1042
rank avg (pred): 0.504 +- 0.134
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.14551e-05, 9.30915e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.408 +- 0.223
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0019454123, 5.44253e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.485 +- 0.136
mrr vals (pred, true): 0.055, 0.008
batch losses (mrrl, rdl): 0.0002535724, 9.19521e-05

Epoch over!
epoch time: 14.973

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 235
rank avg (pred): 0.502 +- 0.123
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.61879e-05, 6.2147e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 927
rank avg (pred): 0.497 +- 0.122
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.29418e-05, 5.4882e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.490 +- 0.122
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.2e-09, 0.0001016485

Epoch over!
epoch time: 14.976

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 193
rank avg (pred): 0.492 +- 0.122
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 5.83e-08, 7.89555e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 729
rank avg (pred): 0.035 +- 0.035
mrr vals (pred, true): 0.287, 0.339
batch losses (mrrl, rdl): 0.0273668114, 0.0003488447

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 285
rank avg (pred): 0.387 +- 0.215
mrr vals (pred, true): 0.078, 0.101
batch losses (mrrl, rdl): 0.0054442426, 0.0001500368

Epoch over!
epoch time: 14.983

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.708 +- 0.428
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0030045551, 0.0002098565

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.371 +- 0.222
mrr vals (pred, true): 0.086, 0.101
batch losses (mrrl, rdl): 0.0021930155, 0.0001336383

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 273
rank avg (pred): 0.405 +- 0.199
mrr vals (pred, true): 0.073, 0.086
batch losses (mrrl, rdl): 0.005149853, 0.0002414614

Epoch over!
epoch time: 14.973

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 684
rank avg (pred): 0.493 +- 0.120
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.47125e-05, 5.50347e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 255
rank avg (pred): 0.037 +- 0.038
mrr vals (pred, true): 0.228, 0.299
batch losses (mrrl, rdl): 0.0505454838, 0.0002933157

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 499
rank avg (pred): 0.353 +- 0.217
mrr vals (pred, true): 0.101, 0.130
batch losses (mrrl, rdl): 0.008448815, 0.000222116

Epoch over!
epoch time: 14.963

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 470
rank avg (pred): 0.486 +- 0.125
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.27496e-05, 7.47646e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 216
rank avg (pred): 0.500 +- 0.114
mrr vals (pred, true): 0.036, 0.000
batch losses (mrrl, rdl): 0.0020561153, 8.31527e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 342
rank avg (pred): 0.466 +- 0.143
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 6.56272e-05, 6.23018e-05

Epoch over!
epoch time: 14.983

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 443
rank avg (pred): 0.471 +- 0.129
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.19423e-05, 0.0001184451

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.411 +- 0.191
mrr vals (pred, true): 0.065, 0.108
batch losses (mrrl, rdl): 0.0180723462, 0.0003437221

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.489 +- 0.111
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.25138e-05, 0.0001153978

Epoch over!
epoch time: 14.955

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.490 +- 0.111
mrr vals (pred, true): 0.048, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   92 	     0 	 0.06744 	 6e-0500 	 m..s
   36 	     1 	 0.04886 	 0.00013 	 m..s
   64 	     2 	 0.04887 	 0.00014 	 m..s
   91 	     3 	 0.05812 	 0.00014 	 m..s
    2 	     4 	 0.04382 	 0.00015 	 m..s
   56 	     5 	 0.04886 	 0.00016 	 m..s
   37 	     6 	 0.04886 	 0.00016 	 m..s
   23 	     7 	 0.04878 	 0.00016 	 m..s
    0 	     8 	 0.04314 	 0.00017 	 m..s
   51 	     9 	 0.04886 	 0.00017 	 m..s
   38 	    10 	 0.04886 	 0.00017 	 m..s
   20 	    11 	 0.04866 	 0.00017 	 m..s
   34 	    12 	 0.04885 	 0.00018 	 m..s
   72 	    13 	 0.04912 	 0.00018 	 m..s
   16 	    14 	 0.04849 	 0.00019 	 m..s
   69 	    15 	 0.04893 	 0.00019 	 m..s
   62 	    16 	 0.04887 	 0.00020 	 m..s
   43 	    17 	 0.04886 	 0.00020 	 m..s
   54 	    18 	 0.04886 	 0.00020 	 m..s
   32 	    19 	 0.04885 	 0.00021 	 m..s
   79 	    20 	 0.05053 	 0.00021 	 m..s
    7 	    21 	 0.04738 	 0.00021 	 m..s
   89 	    22 	 0.05714 	 0.00022 	 m..s
   68 	    23 	 0.04888 	 0.00022 	 m..s
   74 	    24 	 0.04928 	 0.00022 	 m..s
   76 	    25 	 0.04946 	 0.00022 	 m..s
   52 	    26 	 0.04886 	 0.00023 	 m..s
   44 	    27 	 0.04886 	 0.00023 	 m..s
   17 	    28 	 0.04863 	 0.00023 	 m..s
   85 	    29 	 0.05312 	 0.00024 	 m..s
   80 	    30 	 0.05097 	 0.00024 	 m..s
   26 	    31 	 0.04882 	 0.00024 	 m..s
   11 	    32 	 0.04820 	 0.00025 	 m..s
   63 	    33 	 0.04887 	 0.00026 	 m..s
    8 	    34 	 0.04800 	 0.00026 	 m..s
   10 	    35 	 0.04818 	 0.00026 	 m..s
   25 	    36 	 0.04880 	 0.00027 	 m..s
   73 	    37 	 0.04926 	 0.00027 	 m..s
   33 	    38 	 0.04885 	 0.00028 	 m..s
   29 	    39 	 0.04884 	 0.00028 	 m..s
   14 	    40 	 0.04841 	 0.00029 	 m..s
   24 	    41 	 0.04880 	 0.00029 	 m..s
    1 	    42 	 0.04340 	 0.00029 	 m..s
   57 	    43 	 0.04886 	 0.00030 	 m..s
    6 	    44 	 0.04725 	 0.00030 	 m..s
   39 	    45 	 0.04886 	 0.00031 	 m..s
   49 	    46 	 0.04886 	 0.00032 	 m..s
   67 	    47 	 0.04888 	 0.00034 	 m..s
   84 	    48 	 0.05245 	 0.00034 	 m..s
   41 	    49 	 0.04886 	 0.00034 	 m..s
    5 	    50 	 0.04725 	 0.00034 	 m..s
    9 	    51 	 0.04817 	 0.00035 	 m..s
   75 	    52 	 0.04928 	 0.00036 	 m..s
   40 	    53 	 0.04886 	 0.00036 	 m..s
    3 	    54 	 0.04398 	 0.00036 	 m..s
   22 	    55 	 0.04875 	 0.00037 	 m..s
   46 	    56 	 0.04886 	 0.00037 	 m..s
   27 	    57 	 0.04882 	 0.00037 	 m..s
   12 	    58 	 0.04829 	 0.00038 	 m..s
   28 	    59 	 0.04883 	 0.00038 	 m..s
   19 	    60 	 0.04866 	 0.00039 	 m..s
   18 	    61 	 0.04864 	 0.00039 	 m..s
   50 	    62 	 0.04886 	 0.00040 	 m..s
   59 	    63 	 0.04886 	 0.00041 	 m..s
   15 	    64 	 0.04842 	 0.00041 	 m..s
   48 	    65 	 0.04886 	 0.00043 	 m..s
   71 	    66 	 0.04906 	 0.00044 	 m..s
    4 	    67 	 0.04711 	 0.00045 	 m..s
   30 	    68 	 0.04885 	 0.00049 	 m..s
   53 	    69 	 0.04886 	 0.00050 	 m..s
   42 	    70 	 0.04886 	 0.00050 	 m..s
   60 	    71 	 0.04887 	 0.00052 	 m..s
   31 	    72 	 0.04885 	 0.00057 	 m..s
   65 	    73 	 0.04887 	 0.00062 	 m..s
   66 	    74 	 0.04888 	 0.00064 	 m..s
   58 	    75 	 0.04886 	 0.00068 	 m..s
   21 	    76 	 0.04875 	 0.00068 	 m..s
   13 	    77 	 0.04840 	 0.00071 	 m..s
   70 	    78 	 0.04903 	 0.00071 	 m..s
   47 	    79 	 0.04886 	 0.00075 	 m..s
   61 	    80 	 0.04887 	 0.00088 	 m..s
   35 	    81 	 0.04886 	 0.00093 	 m..s
   55 	    82 	 0.04886 	 0.00161 	 m..s
   45 	    83 	 0.04886 	 0.00294 	 m..s
   83 	    84 	 0.05229 	 0.04423 	 ~...
   90 	    85 	 0.05749 	 0.07447 	 ~...
   78 	    86 	 0.05045 	 0.07732 	 ~...
   88 	    87 	 0.05596 	 0.07737 	 ~...
   77 	    88 	 0.04950 	 0.07899 	 ~...
  117 	    89 	 0.20948 	 0.08004 	 MISS
   81 	    90 	 0.05106 	 0.08157 	 m..s
   96 	    91 	 0.07464 	 0.08167 	 ~...
   82 	    92 	 0.05145 	 0.08297 	 m..s
  116 	    93 	 0.20486 	 0.08631 	 MISS
   86 	    94 	 0.05428 	 0.08857 	 m..s
  100 	    95 	 0.09628 	 0.09167 	 ~...
   99 	    96 	 0.07860 	 0.09248 	 ~...
  100 	    97 	 0.09628 	 0.09526 	 ~...
  110 	    98 	 0.13913 	 0.10876 	 m..s
   87 	    99 	 0.05455 	 0.11116 	 m..s
   94 	   100 	 0.07213 	 0.11218 	 m..s
   95 	   101 	 0.07399 	 0.11248 	 m..s
   97 	   102 	 0.07647 	 0.11430 	 m..s
   98 	   103 	 0.07790 	 0.12190 	 m..s
  103 	   104 	 0.09728 	 0.12444 	 ~...
   93 	   105 	 0.07159 	 0.12530 	 m..s
  105 	   106 	 0.09969 	 0.12939 	 ~...
  100 	   107 	 0.09628 	 0.13084 	 m..s
  107 	   108 	 0.13032 	 0.13256 	 ~...
  104 	   109 	 0.09731 	 0.13487 	 m..s
  111 	   110 	 0.14874 	 0.14691 	 ~...
  106 	   111 	 0.10758 	 0.14970 	 m..s
  109 	   112 	 0.13308 	 0.17020 	 m..s
  108 	   113 	 0.13104 	 0.17092 	 m..s
  112 	   114 	 0.16887 	 0.17303 	 ~...
  113 	   115 	 0.18186 	 0.19818 	 ~...
  114 	   116 	 0.19230 	 0.23212 	 m..s
  115 	   117 	 0.20194 	 0.26141 	 m..s
  119 	   118 	 0.25526 	 0.31651 	 m..s
  120 	   119 	 0.27130 	 0.33451 	 m..s
  118 	   120 	 0.21394 	 0.35013 	 MISS
==========================================
r_mrr = 0.8808922171592712
r2_mrr = 0.5747488737106323
spearmanr_mrr@5 = 0.8545412421226501
spearmanr_mrr@10 = 0.9198234677314758
spearmanr_mrr@50 = 0.937576413154602
spearmanr_mrr@100 = 0.9464334845542908
spearmanr_mrr@All = 0.9486486911773682
==========================================
test time: 0.454
Done Testing dataset DBpedia50
total time taken: 229.22687816619873
training time taken: 223.97587990760803
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8809)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.5747)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.8545)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9198)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9376)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9464)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9486)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.1364665984110616}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9307693417040594
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [117, 144, 1031, 1175, 455, 620, 352, 656, 737, 1129, 621, 540, 187, 790, 207, 809, 200, 675, 583, 1141, 186, 59, 211, 764, 416, 580, 1016, 14, 309, 506, 575, 626, 348, 9, 308, 770, 692, 42, 100, 678, 1065, 58, 490, 978, 461, 861, 337, 1027, 389, 820, 182, 629, 1144, 172, 424, 746, 945, 450, 953, 1102, 409, 791, 276, 62, 771, 273, 1049, 748, 1214, 1012, 862, 230, 520, 80, 1083, 990, 741, 1195, 1110, 195, 599, 381, 1077, 690, 439, 509, 388, 43, 333, 1101, 427, 848, 468, 1106, 932, 283, 923, 1021, 518, 435, 1191, 717, 394, 776, 54, 218, 78, 357, 395, 638, 1038, 217, 361, 478, 342, 341, 899, 174, 634, 683, 879]
valid_ids (0): []
train_ids (1094): [365, 827, 536, 71, 1114, 568, 609, 646, 11, 208, 481, 527, 456, 364, 666, 48, 1075, 153, 1043, 467, 814, 966, 938, 1179, 766, 561, 893, 147, 397, 137, 851, 566, 1131, 784, 1143, 617, 476, 782, 630, 661, 242, 929, 193, 1168, 65, 702, 511, 266, 1078, 607, 167, 586, 134, 808, 574, 832, 980, 697, 53, 1189, 1163, 1176, 1004, 963, 1182, 1067, 767, 448, 559, 724, 719, 838, 1007, 331, 943, 1030, 460, 500, 95, 407, 285, 290, 1089, 613, 300, 1044, 996, 973, 432, 886, 46, 839, 898, 852, 89, 1, 37, 408, 265, 974, 659, 769, 649, 1162, 327, 0, 1178, 805, 61, 225, 299, 1093, 155, 731, 1158, 521, 119, 437, 73, 594, 40, 1164, 142, 184, 1068, 856, 160, 720, 426, 721, 870, 271, 84, 760, 420, 335, 112, 1040, 732, 1050, 105, 837, 1074, 1213, 569, 336, 474, 751, 595, 457, 83, 314, 136, 952, 543, 651, 170, 1145, 874, 362, 123, 270, 818, 698, 565, 171, 1169, 502, 191, 246, 608, 1111, 637, 747, 522, 235, 558, 591, 849, 961, 1015, 56, 356, 722, 765, 18, 533, 835, 1193, 693, 802, 1133, 260, 900, 1206, 404, 1198, 446, 431, 992, 101, 801, 664, 910, 685, 1186, 998, 34, 32, 330, 366, 1072, 981, 325, 1104, 542, 178, 1196, 91, 1156, 45, 21, 673, 503, 470, 148, 994, 600, 190, 282, 858, 882, 449, 736, 645, 383, 27, 743, 627, 359, 438, 1070, 949, 551, 884, 1091, 510, 584, 247, 635, 24, 1076, 169, 578, 530, 227, 881, 799, 77, 162, 892, 368, 680, 322, 1146, 173, 164, 585, 433, 31, 1194, 794, 12, 1167, 991, 968, 87, 201, 491, 909, 412, 1166, 103, 97, 988, 582, 124, 922, 85, 632, 234, 405, 387, 777, 1032, 81, 399, 129, 1095, 1002, 597, 26, 122, 108, 1154, 1029, 1203, 339, 369, 867, 1035, 815, 278, 1197, 550, 819, 1121, 419, 50, 755, 749, 1116, 168, 1036, 555, 291, 706, 256, 1138, 1105, 414, 705, 219, 588, 343, 1118, 871, 710, 323, 13, 400, 1066, 338, 951, 311, 773, 854, 25, 66, 1071, 1019, 654, 1052, 798, 789, 263, 250, 917, 98, 465, 347, 1041, 507, 220, 708, 524, 442, 657, 891, 829, 203, 557, 403, 462, 681, 593, 386, 640, 1153, 964, 216, 684, 236, 935, 44, 340, 847, 213, 320, 307, 623, 869, 535, 28, 484, 872, 1113, 401, 729, 942, 233, 483, 677, 545, 243, 957, 810, 920, 804, 590, 324, 619, 1053, 1039, 228, 79, 596, 775, 662, 614, 1047, 946, 289, 113, 284, 612, 1125, 486, 49, 750, 1045, 668, 865, 644, 713, 926, 1211, 262, 1207, 57, 1201, 259, 948, 378, 570, 3, 598, 1134, 372, 127, 477, 1069, 116, 552, 223, 679, 908, 1185, 180, 384, 280, 1064, 444, 1155, 778, 275, 728, 60, 1092, 374, 529, 106, 703, 237, 96, 1199, 984, 822, 498, 989, 204, 35, 496, 759, 1088, 785, 650, 255, 346, 1120, 979, 377, 700, 655, 226, 985, 264, 1202, 699, 221, 671, 183, 1003, 198, 757, 315, 94, 241, 526, 367, 986, 1024, 1180, 826, 257, 140, 1136, 466, 911, 1037, 133, 1119, 1160, 660, 146, 316, 501, 279, 1152, 398, 936, 454, 135, 1073, 74, 306, 744, 1161, 855, 143, 993, 505, 1126, 312, 209, 453, 512, 956, 252, 413, 850, 761, 1135, 418, 913, 1117, 930, 92, 762, 915, 954, 360, 33, 197, 912, 834, 69, 1082, 725, 1063, 1097, 523, 674, 326, 166, 796, 430, 1107, 179, 298, 712, 756, 1025, 240, 504, 1017, 110, 396, 232, 1151, 379, 1057, 1165, 828, 669, 642, 977, 30, 303, 473, 1132, 845, 292, 695, 788, 628, 130, 317, 286, 962, 1139, 689, 139, 495, 423, 441, 254, 534, 159, 763, 1183, 803, 625, 181, 321, 853, 382, 1046, 17, 592, 924, 452, 406, 866, 999, 1042, 329, 488, 349, 1013, 1142, 258, 554, 294, 469, 1190, 745, 417, 863, 971, 873, 742, 206, 6, 676, 145, 643, 1188, 987, 480, 843, 824, 658, 781, 792, 916, 1200, 903, 967, 562, 287, 72, 519, 905, 1140, 813, 563, 840, 716, 188, 354, 1100, 1096, 29, 281, 102, 633, 571, 603, 1115, 1184, 780, 1130, 177, 268, 616, 1205, 128, 231, 548, 222, 370, 1137, 959, 1010, 768, 261, 459, 927, 772, 376, 375, 70, 515, 5, 1212, 895, 734, 1056, 463, 23, 269, 549, 831, 1210, 422, 7, 738, 154, 860, 618, 880, 138, 560, 20, 121, 955, 194, 39, 471, 931, 199, 189, 688, 429, 573, 727, 149, 421, 447, 411, 272, 983, 475, 544, 670, 876, 889, 440, 754, 914, 902, 758, 1085, 267, 812, 686, 508, 975, 1098, 328, 525, 950, 156, 1127, 779, 36, 251, 38, 783, 1173, 157, 940, 513, 126, 109, 682, 528, 553, 494, 825, 1094, 752, 1014, 539, 1108, 941, 1005, 648, 132, 64, 878, 921, 410, 19, 577, 380, 636, 93, 10, 332, 907, 1001, 479, 868, 1081, 472, 274, 1122, 833, 897, 1026, 937, 482, 497, 52, 1062, 492, 925, 538, 487, 687, 972, 353, 841, 224, 514, 906, 1000, 464, 718, 701, 704, 589, 141, 857, 176, 894, 1061, 391, 517, 939, 402, 205, 1124, 295, 358, 1192, 1059, 786, 76, 249, 229, 68, 672, 22, 816, 615, 443, 390, 118, 733, 919, 214, 1008, 665, 175, 890, 150, 373, 795, 541, 537, 753, 711, 896, 114, 653, 392, 104, 1034, 774, 2, 960, 901, 499, 944, 238, 807, 210, 1147, 1009, 904, 709, 844, 1051, 1187, 302, 192, 730, 1159, 288, 934, 1055, 877, 875, 1208, 248, 385, 107, 1099, 99, 239, 152, 564, 1209, 605, 445, 1090, 161, 696, 1174, 86, 41, 434, 610, 304, 1033, 918, 277, 1123, 888, 115, 305, 715, 516, 1181, 67, 1028, 567, 547, 997, 726, 624, 811, 691, 556, 344, 319, 371, 579, 485, 800, 318, 714, 165, 821, 393, 120, 842, 8, 297, 546, 125, 196, 75, 928, 245, 885, 576, 723, 707, 458, 16, 735, 363, 622, 965, 531, 253, 1006, 334, 1080, 995, 958, 1149, 587, 163, 976, 51, 667, 694, 641, 1204, 887, 131, 1157, 296, 451, 350, 493, 355, 55, 293, 606, 1171, 47, 1172, 740, 947, 572, 830, 90, 969, 1112, 4, 602, 82, 601, 817, 611, 1022, 806, 647, 215, 1058, 1020, 158, 836, 864, 1170, 982, 1048, 604, 1103, 652, 1054, 1148, 1150, 883, 425, 351, 15, 489, 797, 185, 202, 1018, 111, 1084, 631, 1109, 859, 532, 787, 415, 1177, 212, 1128, 846, 1023, 244, 1011, 933, 1086, 1060, 428, 1087, 345, 63, 88, 663, 1079, 310, 581, 739, 151, 436, 970, 301, 793, 823, 313, 639]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7228702395880010
the save name prefix for this run is:  chkpt-ID_7228702395880010_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 141
rank avg (pred): 0.508 +- 0.006
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001509115

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.418 +- 0.001
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002400369

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 957
rank avg (pred): 0.498 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.000128745

Epoch over!
epoch time: 14.718

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1145
rank avg (pred): 0.377 +- 0.000
mrr vals (pred, true): 0.000, 0.092
batch losses (mrrl, rdl): 0.0, 0.0001347415

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.499 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.48139e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 943
rank avg (pred): 0.493 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.60017e-05

Epoch over!
epoch time: 14.691

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 4
rank avg (pred): 0.394 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0004968666

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.488 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001395911

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 516
rank avg (pred): 0.365 +- 0.000
mrr vals (pred, true): 0.000, 0.081
batch losses (mrrl, rdl): 0.0, 0.000147599

Epoch over!
epoch time: 14.666

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 913
rank avg (pred): 0.505 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001027999

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.490 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001018561

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 460
rank avg (pred): 0.487 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 8.32548e-05

Epoch over!
epoch time: 14.779

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 166
rank avg (pred): 0.487 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001360397

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 233
rank avg (pred): 0.490 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001211407

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.497 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001374528

Epoch over!
epoch time: 14.799

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 957
rank avg (pred): 0.500 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249188878, 0.0001259478

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.029 +- 0.000
mrr vals (pred, true): 0.001, 0.220
batch losses (mrrl, rdl): 0.479926914, 0.0008189577

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.150 +- 0.000
mrr vals (pred, true): 0.000, 0.060
batch losses (mrrl, rdl): 0.0247303415, 0.0009248295

Epoch over!
epoch time: 14.981

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 180
rank avg (pred): 0.458 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249113347, 0.0001129965

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.491 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249174125, 9.73997e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 850
rank avg (pred): 0.497 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249184147, 0.0001407099

Epoch over!
epoch time: 14.964

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 493
rank avg (pred): 0.104 +- 0.000
mrr vals (pred, true): 0.000, 0.099
batch losses (mrrl, rdl): 0.0246102698, 0.0008512867

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 586
rank avg (pred): 0.480 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249155574, 0.0001013654

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1131
rank avg (pred): 0.506 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249198452, 9.78755e-05

Epoch over!
epoch time: 14.991

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 444
rank avg (pred): 0.495 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249180589, 0.0001581763

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 972
rank avg (pred): 0.075 +- 0.000
mrr vals (pred, true): 0.001, 0.105
batch losses (mrrl, rdl): 0.1087272167, 0.0012432049

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 956
rank avg (pred): 0.489 +- 0.000
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0249170363, 0.0001056391

Epoch over!
epoch time: 15.006

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.488 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249167997, 0.0001148917

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 253
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.008, 0.304
batch losses (mrrl, rdl): 0.8768776655, 0.0005306398

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.004 +- 0.000
mrr vals (pred, true): 0.009, 0.220
batch losses (mrrl, rdl): 0.4455291927, 0.000994958

Epoch over!
epoch time: 14.998

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1151
rank avg (pred): 0.047 +- 0.000
mrr vals (pred, true): 0.001, 0.133
batch losses (mrrl, rdl): 0.1734484881, 0.0016993971

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.003 +- 0.000
mrr vals (pred, true): 0.014, 0.213
batch losses (mrrl, rdl): 0.3977993727, 0.0003870135

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.429 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249055307, 0.0001896312

Epoch over!
epoch time: 14.992

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 434
rank avg (pred): 0.578 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249297749, 0.0001161322

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.041 +- 0.000
mrr vals (pred, true): 0.001, 0.075
batch losses (mrrl, rdl): 0.0240127221, 0.0013897028

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 826
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.183, 0.313
batch losses (mrrl, rdl): 0.1680747569, 0.0006067778

Epoch over!
epoch time: 14.956

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.008, 0.120
batch losses (mrrl, rdl): 0.1264024973, 0.0020563914

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.431 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249058027, 0.0003084968

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 933
rank avg (pred): 0.516 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249213092, 7.08093e-05

Epoch over!
epoch time: 14.969

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.397 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0248977002, 0.000297763

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1157
rank avg (pred): 0.050 +- 0.000
mrr vals (pred, true): 0.001, 0.107
batch losses (mrrl, rdl): 0.1132550091, 0.0018746924

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 752
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.052, 0.221
batch losses (mrrl, rdl): 0.2840771973, 0.0009763152

Epoch over!
epoch time: 14.957

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 111
rank avg (pred): 0.467 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249131694, 0.000131771

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 410
rank avg (pred): 0.491 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0249174349, 0.0001205747

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 189
rank avg (pred): 0.451 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0249100961, 8.89969e-05

Epoch over!
epoch time: 14.967

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.452 +- 0.000
mrr vals (pred, true): 0.000, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  105 	     0 	 0.03200 	 6e-0500 	 m..s
   30 	     1 	 8e-0500 	 0.00015 	 ~...
   48 	     2 	 9e-0500 	 0.00015 	 ~...
   80 	     3 	 0.00293 	 0.00015 	 ~...
   61 	     4 	 9e-0500 	 0.00016 	 ~...
   47 	     5 	 9e-0500 	 0.00016 	 ~...
   21 	     6 	 8e-0500 	 0.00017 	 ~...
   38 	     7 	 8e-0500 	 0.00017 	 ~...
   59 	     8 	 9e-0500 	 0.00017 	 ~...
   53 	     9 	 9e-0500 	 0.00017 	 ~...
   64 	    10 	 9e-0500 	 0.00017 	 ~...
    2 	    11 	 7e-0500 	 0.00018 	 ~...
   52 	    12 	 9e-0500 	 0.00018 	 ~...
   17 	    13 	 8e-0500 	 0.00018 	 ~...
    7 	    14 	 7e-0500 	 0.00018 	 ~...
   51 	    15 	 9e-0500 	 0.00018 	 ~...
   71 	    16 	 0.00011 	 0.00019 	 ~...
   34 	    17 	 8e-0500 	 0.00019 	 ~...
   67 	    18 	 9e-0500 	 0.00019 	 ~...
   94 	    19 	 0.01668 	 0.00020 	 ~...
   72 	    20 	 0.00011 	 0.00021 	 ~...
   54 	    21 	 9e-0500 	 0.00021 	 ~...
   57 	    22 	 9e-0500 	 0.00021 	 ~...
   60 	    23 	 9e-0500 	 0.00021 	 ~...
    5 	    24 	 7e-0500 	 0.00022 	 ~...
   62 	    25 	 9e-0500 	 0.00022 	 ~...
   39 	    26 	 9e-0500 	 0.00022 	 ~...
   44 	    27 	 9e-0500 	 0.00022 	 ~...
   11 	    28 	 7e-0500 	 0.00023 	 ~...
   55 	    29 	 9e-0500 	 0.00023 	 ~...
   49 	    30 	 9e-0500 	 0.00024 	 ~...
   78 	    31 	 0.00286 	 0.00024 	 ~...
   68 	    32 	 9e-0500 	 0.00024 	 ~...
   10 	    33 	 7e-0500 	 0.00024 	 ~...
   15 	    34 	 8e-0500 	 0.00024 	 ~...
   23 	    35 	 8e-0500 	 0.00024 	 ~...
   81 	    36 	 0.00330 	 0.00024 	 ~...
   45 	    37 	 9e-0500 	 0.00025 	 ~...
   12 	    38 	 7e-0500 	 0.00025 	 ~...
   14 	    39 	 8e-0500 	 0.00025 	 ~...
   50 	    40 	 9e-0500 	 0.00026 	 ~...
   73 	    41 	 0.00012 	 0.00026 	 ~...
    1 	    42 	 7e-0500 	 0.00026 	 ~...
   82 	    43 	 0.00334 	 0.00027 	 ~...
    8 	    44 	 7e-0500 	 0.00027 	 ~...
   37 	    45 	 8e-0500 	 0.00028 	 ~...
   93 	    46 	 0.01658 	 0.00028 	 ~...
   63 	    47 	 9e-0500 	 0.00029 	 ~...
    0 	    48 	 7e-0500 	 0.00030 	 ~...
   26 	    49 	 8e-0500 	 0.00030 	 ~...
   27 	    50 	 8e-0500 	 0.00030 	 ~...
   31 	    51 	 8e-0500 	 0.00031 	 ~...
   65 	    52 	 9e-0500 	 0.00031 	 ~...
   43 	    53 	 9e-0500 	 0.00032 	 ~...
   25 	    54 	 8e-0500 	 0.00033 	 ~...
   28 	    55 	 8e-0500 	 0.00034 	 ~...
  110 	    56 	 0.06804 	 0.00035 	 m..s
   19 	    57 	 8e-0500 	 0.00036 	 ~...
   75 	    58 	 0.00036 	 0.00037 	 ~...
   13 	    59 	 7e-0500 	 0.00037 	 ~...
   20 	    60 	 8e-0500 	 0.00038 	 ~...
   46 	    61 	 9e-0500 	 0.00038 	 ~...
   22 	    62 	 8e-0500 	 0.00039 	 ~...
    3 	    63 	 7e-0500 	 0.00039 	 ~...
   36 	    64 	 8e-0500 	 0.00040 	 ~...
    9 	    65 	 7e-0500 	 0.00041 	 ~...
   56 	    66 	 9e-0500 	 0.00041 	 ~...
   77 	    67 	 0.00285 	 0.00042 	 ~...
   24 	    68 	 8e-0500 	 0.00043 	 ~...
   76 	    69 	 0.00038 	 0.00043 	 ~...
    6 	    70 	 7e-0500 	 0.00044 	 ~...
   83 	    71 	 0.00340 	 0.00044 	 ~...
   29 	    72 	 8e-0500 	 0.00046 	 ~...
   32 	    73 	 8e-0500 	 0.00047 	 ~...
   40 	    74 	 9e-0500 	 0.00048 	 ~...
   58 	    75 	 9e-0500 	 0.00053 	 ~...
   95 	    76 	 0.01923 	 0.00053 	 ~...
   74 	    77 	 0.00012 	 0.00060 	 ~...
   35 	    78 	 8e-0500 	 0.00062 	 ~...
   42 	    79 	 9e-0500 	 0.00066 	 ~...
   92 	    80 	 0.01654 	 0.00068 	 ~...
  111 	    81 	 0.06960 	 0.00082 	 m..s
   33 	    82 	 8e-0500 	 0.00087 	 ~...
   41 	    83 	 9e-0500 	 0.00088 	 ~...
   70 	    84 	 0.00010 	 0.00091 	 ~...
   18 	    85 	 8e-0500 	 0.00113 	 ~...
   69 	    86 	 0.00010 	 0.00116 	 ~...
   66 	    87 	 9e-0500 	 0.00201 	 ~...
   79 	    88 	 0.00290 	 0.00220 	 ~...
   16 	    89 	 8e-0500 	 0.00842 	 ~...
    4 	    90 	 7e-0500 	 0.00890 	 ~...
  103 	    91 	 0.02478 	 0.06408 	 m..s
   98 	    92 	 0.02032 	 0.06559 	 m..s
   87 	    93 	 0.00629 	 0.07825 	 m..s
  112 	    94 	 0.08690 	 0.08062 	 ~...
  104 	    95 	 0.02595 	 0.08195 	 m..s
  100 	    96 	 0.02172 	 0.08297 	 m..s
  114 	    97 	 0.09071 	 0.08574 	 ~...
  101 	    98 	 0.02282 	 0.08648 	 m..s
   99 	    99 	 0.02167 	 0.08729 	 m..s
   91 	   100 	 0.01358 	 0.08908 	 m..s
  116 	   101 	 0.10872 	 0.09248 	 ~...
   86 	   102 	 0.00609 	 0.09441 	 m..s
   97 	   103 	 0.01954 	 0.10273 	 m..s
   84 	   104 	 0.00348 	 0.10315 	 m..s
   85 	   105 	 0.00596 	 0.10394 	 m..s
  113 	   106 	 0.08958 	 0.10794 	 ~...
   96 	   107 	 0.01928 	 0.10871 	 m..s
  102 	   108 	 0.02341 	 0.11591 	 m..s
  117 	   109 	 0.13524 	 0.12342 	 ~...
  115 	   110 	 0.09549 	 0.12813 	 m..s
  118 	   111 	 0.14566 	 0.14992 	 ~...
  120 	   112 	 0.22772 	 0.15118 	 m..s
   90 	   113 	 0.01324 	 0.16359 	 MISS
   89 	   114 	 0.01310 	 0.17020 	 MISS
  108 	   115 	 0.06074 	 0.18137 	 MISS
   88 	   116 	 0.00726 	 0.18809 	 MISS
  109 	   117 	 0.06131 	 0.20572 	 MISS
  119 	   118 	 0.15593 	 0.20969 	 m..s
  106 	   119 	 0.03293 	 0.21315 	 MISS
  107 	   120 	 0.05509 	 0.25278 	 MISS
==========================================
r_mrr = 0.6285897493362427
r2_mrr = 0.3222181797027588
spearmanr_mrr@5 = 0.9980420470237732
spearmanr_mrr@10 = 0.9708349108695984
spearmanr_mrr@50 = 0.9073750972747803
spearmanr_mrr@100 = 0.9246944785118103
spearmanr_mrr@All = 0.9271906614303589
==========================================
test time: 0.453
Done Testing dataset DBpedia50
total time taken: 229.21045088768005
training time taken: 223.8969271183014
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.6286)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.3222)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9980)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9708)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9074)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9247)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9272)}}, 'test_loss': {'ComplEx': {'DBpedia50': 4.839073709445074}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 3921256419589720
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [399, 574, 506, 1103, 115, 858, 104, 1193, 167, 1017, 1001, 829, 631, 1005, 759, 1119, 211, 65, 670, 408, 1052, 43, 668, 12, 559, 720, 933, 589, 1135, 620, 490, 347, 8, 555, 313, 921, 1025, 711, 1123, 233, 214, 661, 40, 163, 1162, 449, 1107, 249, 545, 782, 866, 511, 1164, 689, 873, 862, 430, 524, 786, 883, 173, 746, 798, 1099, 1077, 371, 121, 142, 277, 113, 429, 865, 391, 1140, 1079, 475, 1171, 788, 634, 1058, 510, 47, 64, 1045, 22, 331, 324, 1097, 685, 617, 1157, 520, 361, 898, 1134, 908, 303, 360, 536, 771, 1152, 29, 963, 1118, 55, 1030, 400, 238, 389, 1089, 825, 340, 976, 310, 783, 83, 881, 292, 1197, 37, 281]
valid_ids (0): []
train_ids (1094): [997, 961, 1083, 778, 726, 33, 1056, 548, 345, 132, 151, 1055, 965, 824, 533, 1146, 137, 845, 958, 1150, 498, 1028, 892, 308, 236, 46, 651, 74, 494, 274, 515, 492, 1125, 1029, 593, 330, 57, 1080, 801, 1061, 861, 1194, 654, 931, 509, 990, 1143, 774, 305, 658, 476, 719, 587, 627, 1120, 1211, 306, 953, 1163, 678, 923, 280, 1068, 728, 342, 750, 1202, 193, 831, 1012, 171, 546, 714, 860, 848, 842, 1138, 378, 992, 346, 54, 69, 422, 95, 1003, 124, 791, 772, 272, 1203, 797, 1004, 607, 254, 158, 610, 255, 24, 443, 608, 298, 939, 1122, 832, 265, 872, 904, 1113, 96, 472, 401, 894, 48, 962, 1169, 98, 942, 522, 1188, 101, 650, 600, 384, 715, 1145, 1149, 936, 1092, 185, 501, 710, 767, 705, 275, 1112, 833, 276, 839, 296, 199, 882, 470, 179, 950, 804, 1106, 823, 416, 840, 790, 73, 332, 740, 516, 456, 438, 508, 322, 141, 1067, 135, 512, 1046, 270, 919, 623, 643, 16, 317, 737, 530, 1110, 63, 325, 223, 71, 279, 215, 949, 1090, 849, 291, 1011, 519, 835, 590, 450, 1167, 11, 252, 150, 977, 444, 673, 915, 375, 1021, 256, 155, 1050, 1210, 15, 484, 300, 42, 392, 133, 260, 647, 625, 597, 85, 760, 358, 735, 918, 326, 1085, 421, 229, 694, 59, 602, 1127, 556, 154, 49, 834, 1027, 1104, 567, 1002, 17, 1126, 709, 803, 126, 288, 66, 646, 1074, 1013, 906, 565, 102, 686, 907, 149, 208, 967, 766, 1142, 123, 145, 1192, 1036, 459, 1174, 161, 221, 217, 1032, 235, 286, 605, 722, 930, 1209, 471, 500, 432, 1070, 856, 1181, 122, 396, 912, 811, 5, 1180, 1088, 1084, 558, 578, 415, 899, 329, 493, 1153, 1116, 920, 189, 640, 411, 419, 232, 169, 946, 390, 674, 1024, 630, 177, 87, 26, 355, 1173, 114, 77, 38, 947, 250, 316, 1151, 79, 757, 847, 564, 730, 1075, 1176, 483, 1200, 662, 261, 1183, 478, 414, 940, 815, 995, 370, 1057, 1158, 991, 407, 120, 335, 97, 1214, 127, 642, 736, 339, 367, 480, 301, 34, 386, 1124, 911, 1064, 295, 615, 1066, 850, 352, 806, 21, 159, 863, 468, 176, 944, 240, 241, 219, 165, 1148, 143, 935, 821, 945, 999, 697, 903, 328, 594, 1054, 365, 1208, 451, 307, 209, 36, 534, 902, 304, 681, 107, 495, 1000, 542, 385, 916, 60, 1044, 30, 762, 636, 70, 676, 192, 588, 659, 554, 551, 639, 251, 527, 381, 813, 707, 532, 1100, 183, 799, 552, 1206, 802, 1129, 111, 175, 537, 1059, 56, 591, 571, 688, 604, 318, 800, 1130, 78, 644, 664, 934, 569, 789, 4, 1147, 50, 178, 382, 230, 112, 729, 403, 404, 1204, 751, 1189, 693, 553, 393, 436, 1137, 764, 248, 197, 455, 1187, 723, 987, 486, 988, 110, 129, 237, 846, 1010, 649, 100, 1114, 201, 628, 338, 666, 739, 932, 362, 570, 637, 822, 614, 462, 68, 733, 893, 271, 84, 1199, 805, 473, 1022, 748, 663, 586, 2, 613, 168, 187, 960, 1033, 246, 538, 1091, 576, 388, 529, 853, 889, 680, 423, 216, 891, 464, 162, 273, 1096, 181, 491, 1060, 830, 1105, 447, 203, 952, 982, 624, 417, 146, 80, 1047, 989, 787, 349, 285, 25, 878, 160, 1136, 45, 1082, 373, 540, 58, 466, 1019, 289, 776, 1161, 854, 195, 784, 732, 702, 562, 577, 336, 67, 202, 973, 844, 377, 460, 927, 671, 1049, 1179, 134, 1071, 76, 734, 290, 268, 53, 521, 809, 633, 453, 1048, 166, 721, 138, 877, 875, 619, 870, 156, 603, 10, 794, 925, 994, 264, 814, 836, 1207, 816, 327, 528, 425, 1023, 888, 356, 13, 851, 993, 855, 14, 539, 1212, 1111, 585, 247, 81, 712, 874, 632, 452, 704, 638, 957, 409, 828, 518, 348, 363, 259, 616, 596, 312, 1168, 364, 980, 1159, 153, 1039, 859, 765, 1098, 61, 966, 242, 1132, 320, 196, 488, 157, 1191, 575, 1, 1053, 890, 535, 550, 1062, 323, 601, 344, 941, 1177, 517, 213, 905, 924, 544, 692, 580, 956, 489, 1043, 951, 928, 612, 656, 948, 970, 598, 1195, 428, 1093, 418, 368, 1108, 549, 561, 467, 257, 376, 756, 448, 1063, 106, 763, 94, 481, 374, 1175, 968, 792, 985, 445, 118, 981, 996, 3, 62, 573, 435, 350, 366, 18, 1156, 1128, 913, 869, 568, 92, 23, 188, 282, 1131, 41, 769, 103, 758, 88, 818, 745, 566, 867, 139, 210, 682, 584, 563, 1065, 198, 387, 648, 926, 871, 477, 857, 212, 547, 1155, 82, 879, 205, 117, 9, 1031, 724, 180, 224, 1086, 463, 793, 172, 838, 278, 302, 1172, 984, 174, 144, 269, 140, 505, 796, 700, 1042, 1009, 807, 1201, 752, 379, 457, 207, 194, 541, 1109, 234, 959, 657, 1115, 44, 690, 204, 513, 897, 514, 299, 669, 1073, 442, 51, 1040, 1081, 572, 1186, 969, 218, 499, 503, 753, 731, 972, 439, 297, 1117, 446, 222, 487, 0, 937, 293, 655, 557, 372, 1095, 1185, 465, 283, 755, 699, 410, 454, 785, 19, 420, 75, 130, 186, 284, 964, 90, 1178, 200, 1160, 938, 253, 667, 32, 665, 35, 433, 321, 717, 979, 837, 405, 206, 653, 485, 812, 31, 152, 427, 727, 660, 618, 641, 698, 909, 334, 191, 245, 725, 136, 914, 1144, 742, 482, 1026, 971, 599, 900, 190, 105, 747, 72, 1008, 424, 1007, 1037, 496, 955, 779, 652, 1102, 887, 182, 267, 243, 703, 777, 1205, 86, 412, 978, 744, 1133, 239, 819, 319, 27, 1139, 402, 1087, 413, 91, 896, 109, 266, 497, 582, 220, 1213, 592, 6, 351, 380, 226, 507, 917, 258, 1038, 901, 458, 773, 125, 895, 164, 394, 749, 672, 474, 1041, 52, 1018, 695, 314, 579, 469, 1072, 706, 876, 1015, 687, 116, 606, 147, 99, 262, 311, 780, 383, 583, 713, 1076, 683, 431, 170, 611, 1190, 369, 1165, 479, 93, 609, 718, 635, 395, 621, 1014, 244, 1094, 679, 131, 543, 696, 353, 1166, 852, 406, 531, 1069, 398, 28, 1034, 359, 1035, 119, 7, 626, 677, 357, 743, 983, 986, 1006, 817, 341, 761, 1182, 354, 309, 622, 184, 701, 716, 263, 910, 39, 675, 1020, 868, 708, 287, 523, 434, 343, 645, 337, 1121, 89, 595, 108, 294, 826, 810, 225, 525, 1170, 333, 886, 775, 20, 461, 954, 922, 885, 1196, 943, 741, 526, 770, 998, 1101, 768, 1141, 884, 864, 1016, 975, 560, 148, 1198, 504, 820, 1051, 502, 440, 437, 827, 227, 397, 441, 754, 880, 1078, 808, 691, 974, 841, 843, 128, 629, 1154, 228, 684, 426, 315, 738, 581, 795, 929, 781, 1184, 231]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9629633605974734
the save name prefix for this run is:  chkpt-ID_9629633605974734_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 551
rank avg (pred): 0.545 +- 0.004
mrr vals (pred, true): 0.000, 0.078
batch losses (mrrl, rdl): 0.0, 0.0006189324

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 406
rank avg (pred): 0.463 +- 0.278
mrr vals (pred, true): 0.150, 0.001
batch losses (mrrl, rdl): 0.0, 2.37393e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 997
rank avg (pred): 0.274 +- 0.177
mrr vals (pred, true): 0.223, 0.126
batch losses (mrrl, rdl): 0.0, 6.64979e-05

Epoch over!
epoch time: 14.824

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 367
rank avg (pred): 0.467 +- 0.298
mrr vals (pred, true): 0.221, 0.000
batch losses (mrrl, rdl): 0.0, 1.90943e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.333 +- 0.217
mrr vals (pred, true): 0.233, 0.060
batch losses (mrrl, rdl): 0.0, 3.81348e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 508
rank avg (pred): 0.306 +- 0.204
mrr vals (pred, true): 0.238, 0.143
batch losses (mrrl, rdl): 0.0, 8.81059e-05

Epoch over!
epoch time: 14.793

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1181
rank avg (pred): 0.468 +- 0.300
mrr vals (pred, true): 0.221, 0.000
batch losses (mrrl, rdl): 0.0, 3.71305e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 448
rank avg (pred): 0.476 +- 0.299
mrr vals (pred, true): 0.219, 0.001
batch losses (mrrl, rdl): 0.0, 2.07384e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 128
rank avg (pred): 0.457 +- 0.296
mrr vals (pred, true): 0.242, 0.001
batch losses (mrrl, rdl): 0.0, 1.24817e-05

Epoch over!
epoch time: 14.806

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 315
rank avg (pred): 0.325 +- 0.234
mrr vals (pred, true): 0.250, 0.081
batch losses (mrrl, rdl): 0.0, 3.38073e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.464 +- 0.297
mrr vals (pred, true): 0.239, 0.000
batch losses (mrrl, rdl): 0.0, 2.6687e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.271 +- 0.240
mrr vals (pred, true): 0.251, 0.235
batch losses (mrrl, rdl): 0.0, 8.79015e-05

Epoch over!
epoch time: 14.852

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1186
rank avg (pred): 0.478 +- 0.301
mrr vals (pred, true): 0.218, 0.000
batch losses (mrrl, rdl): 0.0, 6.43261e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.459 +- 0.287
mrr vals (pred, true): 0.207, 0.000
batch losses (mrrl, rdl): 0.0, 1.19823e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1175
rank avg (pred): 0.455 +- 0.293
mrr vals (pred, true): 0.215, 0.001
batch losses (mrrl, rdl): 0.0, 2.97109e-05

Epoch over!
epoch time: 14.807

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.461 +- 0.291
mrr vals (pred, true): 0.201, 0.000
batch losses (mrrl, rdl): 0.2271649241, 1.36152e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.469 +- 0.206
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0011406919, 4.55586e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 302
rank avg (pred): 0.333 +- 0.162
mrr vals (pred, true): 0.059, 0.073
batch losses (mrrl, rdl): 0.0007810112, 0.0001308583

Epoch over!
epoch time: 14.968

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 314
rank avg (pred): 0.342 +- 0.172
mrr vals (pred, true): 0.083, 0.094
batch losses (mrrl, rdl): 0.0110653639, 8.56956e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 636
rank avg (pred): 0.511 +- 0.121
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.31796e-05, 7.03685e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.494 +- 0.136
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.29255e-05, 8.80244e-05

Epoch over!
epoch time: 14.967

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 24
rank avg (pred): 0.451 +- 0.221
mrr vals (pred, true): 0.086, 0.000
batch losses (mrrl, rdl): 0.0126366969, 3.64651e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 219
rank avg (pred): 0.521 +- 0.119
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.534e-05, 0.000152724

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.508 +- 0.125
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.31619e-05, 9.40887e-05

Epoch over!
epoch time: 15.095

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1206
rank avg (pred): 0.518 +- 0.119
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.41479e-05, 7.96701e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.493 +- 0.143
mrr vals (pred, true): 0.049, 0.008
batch losses (mrrl, rdl): 1.28431e-05, 9.60693e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.303 +- 0.167
mrr vals (pred, true): 0.097, 0.146
batch losses (mrrl, rdl): 0.0237006433, 8.69152e-05

Epoch over!
epoch time: 15.115

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 486
rank avg (pred): 0.384 +- 0.195
mrr vals (pred, true): 0.093, 0.079
batch losses (mrrl, rdl): 0.0184808262, 7.7044e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.448 +- 0.146
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.13281e-05, 0.0001200986

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.465 +- 0.184
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 7.8867e-06, 7.12894e-05

Epoch over!
epoch time: 15.126

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1039
rank avg (pred): 0.516 +- 0.132
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.31349e-05, 7.28811e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.481 +- 0.116
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.64779e-05, 0.0001231574

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.467 +- 0.122
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.29921e-05, 7.31113e-05

Epoch over!
epoch time: 15.12

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.445 +- 0.184
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002477241, 6.08693e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 498
rank avg (pred): 0.334 +- 0.173
mrr vals (pred, true): 0.102, 0.120
batch losses (mrrl, rdl): 0.0032534515, 0.000172512

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 330
rank avg (pred): 0.474 +- 0.150
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.1384e-05, 8.69542e-05

Epoch over!
epoch time: 15.141

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1026
rank avg (pred): 0.515 +- 0.119
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.54566e-05, 9.68563e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 297
rank avg (pred): 0.372 +- 0.178
mrr vals (pred, true): 0.062, 0.093
batch losses (mrrl, rdl): 0.0014896868, 0.0001095732

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1006
rank avg (pred): 0.513 +- 0.125
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.31385e-05, 0.0001421198

Epoch over!
epoch time: 15.065

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.417 +- 0.191
mrr vals (pred, true): 0.056, 0.087
batch losses (mrrl, rdl): 0.0003487856, 0.0002519402

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 951
rank avg (pred): 0.515 +- 0.127
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.31319e-05, 6.56205e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.503 +- 0.132
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.28603e-05, 8.09265e-05

Epoch over!
epoch time: 14.932

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 478
rank avg (pred): 0.484 +- 0.134
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.26974e-05, 0.0001054771

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.366 +- 0.172
mrr vals (pred, true): 0.077, 0.047
batch losses (mrrl, rdl): 0.0075571123, 0.0001383856

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 91
rank avg (pred): 0.517 +- 0.117
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.43077e-05, 0.0001698703

Epoch over!
epoch time: 14.932

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.467 +- 0.146
mrr vals (pred, true): 0.049, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   91 	     0 	 0.07011 	 7e-0500 	 m..s
   27 	     1 	 0.04885 	 0.00012 	 m..s
   17 	     2 	 0.04885 	 0.00015 	 m..s
   94 	     3 	 0.07134 	 0.00015 	 m..s
   41 	     4 	 0.04886 	 0.00016 	 m..s
   60 	     5 	 0.04893 	 0.00016 	 m..s
   83 	     6 	 0.05674 	 0.00017 	 m..s
   44 	     7 	 0.04886 	 0.00017 	 m..s
   13 	     8 	 0.04885 	 0.00018 	 m..s
    7 	     9 	 0.04885 	 0.00019 	 m..s
    8 	    10 	 0.04885 	 0.00019 	 m..s
   25 	    11 	 0.04885 	 0.00019 	 m..s
   65 	    12 	 0.04899 	 0.00020 	 m..s
   54 	    13 	 0.04889 	 0.00021 	 m..s
   31 	    14 	 0.04886 	 0.00021 	 m..s
   61 	    15 	 0.04893 	 0.00022 	 m..s
   64 	    16 	 0.04896 	 0.00022 	 m..s
   11 	    17 	 0.04885 	 0.00022 	 m..s
   28 	    18 	 0.04885 	 0.00022 	 m..s
   33 	    19 	 0.04886 	 0.00023 	 m..s
   70 	    20 	 0.04923 	 0.00023 	 m..s
    2 	    21 	 0.04884 	 0.00023 	 m..s
   96 	    22 	 0.07604 	 0.00023 	 m..s
   63 	    23 	 0.04895 	 0.00023 	 m..s
   80 	    24 	 0.05529 	 0.00023 	 m..s
   56 	    25 	 0.04890 	 0.00023 	 m..s
   35 	    26 	 0.04886 	 0.00024 	 m..s
   69 	    27 	 0.04909 	 0.00024 	 m..s
   57 	    28 	 0.04891 	 0.00024 	 m..s
   37 	    29 	 0.04886 	 0.00024 	 m..s
   46 	    30 	 0.04886 	 0.00024 	 m..s
    5 	    31 	 0.04885 	 0.00024 	 m..s
   40 	    32 	 0.04886 	 0.00024 	 m..s
   15 	    33 	 0.04885 	 0.00025 	 m..s
    9 	    34 	 0.04885 	 0.00025 	 m..s
    1 	    35 	 0.04884 	 0.00025 	 m..s
    3 	    36 	 0.04884 	 0.00026 	 m..s
   38 	    37 	 0.04886 	 0.00026 	 m..s
   71 	    38 	 0.04928 	 0.00027 	 m..s
   42 	    39 	 0.04886 	 0.00027 	 m..s
   43 	    40 	 0.04886 	 0.00027 	 m..s
   82 	    41 	 0.05554 	 0.00027 	 m..s
   22 	    42 	 0.04885 	 0.00029 	 m..s
    6 	    43 	 0.04885 	 0.00029 	 m..s
   36 	    44 	 0.04886 	 0.00029 	 m..s
   19 	    45 	 0.04885 	 0.00030 	 m..s
    4 	    46 	 0.04885 	 0.00031 	 m..s
   18 	    47 	 0.04885 	 0.00031 	 m..s
   88 	    48 	 0.05990 	 0.00032 	 m..s
   59 	    49 	 0.04892 	 0.00032 	 m..s
   90 	    50 	 0.06095 	 0.00033 	 m..s
   45 	    51 	 0.04886 	 0.00034 	 m..s
   30 	    52 	 0.04886 	 0.00034 	 m..s
   10 	    53 	 0.04885 	 0.00035 	 m..s
   21 	    54 	 0.04885 	 0.00036 	 m..s
   66 	    55 	 0.04900 	 0.00036 	 m..s
   67 	    56 	 0.04901 	 0.00038 	 m..s
   34 	    57 	 0.04886 	 0.00038 	 m..s
   51 	    58 	 0.04887 	 0.00041 	 m..s
   55 	    59 	 0.04889 	 0.00041 	 m..s
   49 	    60 	 0.04887 	 0.00043 	 m..s
   26 	    61 	 0.04885 	 0.00043 	 m..s
   52 	    62 	 0.04888 	 0.00044 	 m..s
   39 	    63 	 0.04886 	 0.00045 	 m..s
    0 	    64 	 0.04884 	 0.00047 	 m..s
   23 	    65 	 0.04885 	 0.00049 	 m..s
   68 	    66 	 0.04908 	 0.00050 	 m..s
   75 	    67 	 0.05228 	 0.00054 	 m..s
   58 	    68 	 0.04892 	 0.00055 	 m..s
   24 	    69 	 0.04885 	 0.00058 	 m..s
   20 	    70 	 0.04885 	 0.00060 	 m..s
   16 	    71 	 0.04885 	 0.00062 	 m..s
   62 	    72 	 0.04894 	 0.00065 	 m..s
   14 	    73 	 0.04885 	 0.00066 	 m..s
   48 	    74 	 0.04886 	 0.00066 	 m..s
   72 	    75 	 0.04966 	 0.00071 	 m..s
   12 	    76 	 0.04885 	 0.00074 	 m..s
   50 	    77 	 0.04887 	 0.00088 	 m..s
   53 	    78 	 0.04889 	 0.00090 	 m..s
   87 	    79 	 0.05964 	 0.00100 	 m..s
   32 	    80 	 0.04886 	 0.00100 	 m..s
   29 	    81 	 0.04885 	 0.00104 	 m..s
   85 	    82 	 0.05909 	 0.00179 	 m..s
   47 	    83 	 0.04886 	 0.00201 	 m..s
   77 	    84 	 0.05331 	 0.06997 	 ~...
   78 	    85 	 0.05334 	 0.07732 	 ~...
   76 	    86 	 0.05237 	 0.07916 	 ~...
   86 	    87 	 0.05930 	 0.08378 	 ~...
   81 	    88 	 0.05533 	 0.08729 	 m..s
  101 	    89 	 0.09120 	 0.08743 	 ~...
   93 	    90 	 0.07094 	 0.08821 	 ~...
   74 	    91 	 0.05214 	 0.08835 	 m..s
  108 	    92 	 0.15153 	 0.08908 	 m..s
   98 	    93 	 0.07908 	 0.09016 	 ~...
   79 	    94 	 0.05380 	 0.09379 	 m..s
  116 	    95 	 0.19159 	 0.09413 	 m..s
   84 	    96 	 0.05774 	 0.09564 	 m..s
   95 	    97 	 0.07407 	 0.09694 	 ~...
  103 	    98 	 0.12104 	 0.09709 	 ~...
   99 	    99 	 0.09051 	 0.09798 	 ~...
  102 	   100 	 0.10339 	 0.10042 	 ~...
   89 	   101 	 0.06040 	 0.10394 	 m..s
  107 	   102 	 0.14709 	 0.10723 	 m..s
   92 	   103 	 0.07088 	 0.10828 	 m..s
   73 	   104 	 0.05128 	 0.11351 	 m..s
   97 	   105 	 0.07604 	 0.11553 	 m..s
  104 	   106 	 0.12327 	 0.11751 	 ~...
  105 	   107 	 0.13802 	 0.11859 	 ~...
  111 	   108 	 0.16660 	 0.12051 	 m..s
  100 	   109 	 0.09105 	 0.12530 	 m..s
  109 	   110 	 0.16343 	 0.12892 	 m..s
  112 	   111 	 0.17260 	 0.14992 	 ~...
  110 	   112 	 0.16617 	 0.15510 	 ~...
  114 	   113 	 0.18006 	 0.16142 	 ~...
  117 	   114 	 0.20305 	 0.16359 	 m..s
  113 	   115 	 0.17925 	 0.16794 	 ~...
  118 	   116 	 0.20448 	 0.17303 	 m..s
  106 	   117 	 0.14628 	 0.18010 	 m..s
  115 	   118 	 0.18184 	 0.19401 	 ~...
  119 	   119 	 0.22562 	 0.25278 	 ~...
  120 	   120 	 0.22951 	 0.26108 	 m..s
==========================================
r_mrr = 0.8695793151855469
r2_mrr = 0.43759626150131226
spearmanr_mrr@5 = 0.9809420704841614
spearmanr_mrr@10 = 0.9583409428596497
spearmanr_mrr@50 = 0.8913106322288513
spearmanr_mrr@100 = 0.9220151901245117
spearmanr_mrr@All = 0.925868809223175
==========================================
test time: 0.448
Done Testing dataset DBpedia50
total time taken: 230.34329557418823
training time taken: 225.00236988067627
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8696)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4376)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9809)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9583)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.8913)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9220)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9259)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.6645049141443451}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 5053379673425983
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [800, 1052, 1043, 1054, 768, 651, 691, 1084, 683, 34, 620, 211, 492, 859, 1196, 1126, 630, 924, 21, 675, 1212, 1011, 597, 1195, 355, 32, 122, 280, 743, 215, 646, 1047, 290, 810, 640, 38, 1072, 848, 1159, 448, 971, 317, 1015, 842, 169, 1139, 637, 610, 218, 1009, 1211, 607, 545, 68, 142, 312, 285, 81, 793, 165, 92, 780, 99, 658, 553, 1204, 739, 727, 742, 30, 402, 192, 35, 459, 710, 901, 827, 1180, 1022, 925, 349, 1010, 990, 988, 778, 220, 219, 426, 457, 645, 240, 1182, 514, 340, 689, 540, 585, 1137, 1121, 580, 533, 467, 1016, 360, 469, 1166, 37, 1001, 807, 509, 1118, 511, 520, 363, 346, 470, 1109, 90, 906, 109, 797]
valid_ids (0): []
train_ids (1094): [1036, 89, 128, 113, 826, 102, 1002, 724, 328, 870, 16, 348, 1103, 832, 453, 939, 143, 368, 239, 868, 140, 843, 958, 1082, 301, 394, 1039, 475, 764, 1071, 833, 518, 766, 233, 666, 1153, 622, 435, 237, 389, 1151, 1097, 548, 762, 26, 369, 927, 119, 1148, 655, 98, 1164, 82, 305, 295, 555, 977, 272, 1006, 217, 704, 1154, 178, 54, 111, 126, 412, 461, 849, 480, 874, 77, 430, 907, 1147, 300, 468, 1163, 1141, 790, 275, 883, 471, 1037, 1077, 678, 390, 858, 523, 718, 1061, 1021, 973, 584, 604, 836, 158, 314, 1089, 188, 950, 999, 451, 947, 256, 1062, 884, 129, 558, 546, 401, 1023, 760, 519, 547, 798, 15, 1091, 767, 145, 302, 352, 96, 1060, 334, 1070, 106, 638, 43, 692, 56, 878, 134, 152, 0, 266, 1132, 788, 980, 1114, 605, 544, 498, 789, 375, 654, 717, 1183, 602, 433, 136, 484, 1158, 29, 700, 866, 254, 279, 516, 1134, 831, 792, 478, 315, 60, 248, 452, 773, 753, 146, 161, 1055, 327, 955, 802, 617, 304, 75, 157, 209, 1105, 853, 247, 329, 1129, 1020, 761, 672, 822, 486, 1069, 685, 567, 1075, 1116, 964, 936, 1179, 623, 52, 750, 706, 932, 417, 441, 1003, 482, 18, 811, 578, 715, 425, 1133, 214, 679, 744, 125, 823, 730, 1181, 131, 909, 855, 13, 1143, 3, 663, 1173, 1024, 912, 444, 1034, 660, 483, 938, 515, 163, 265, 156, 711, 370, 364, 1198, 371, 564, 28, 991, 825, 1115, 9, 404, 695, 752, 454, 432, 207, 48, 919, 147, 582, 552, 942, 103, 828, 418, 398, 108, 1112, 243, 899, 261, 2, 69, 589, 307, 725, 91, 565, 1149, 694, 488, 926, 719, 194, 373, 777, 893, 698, 1078, 1144, 759, 1057, 155, 366, 481, 861, 1168, 148, 687, 84, 269, 671, 335, 282, 223, 677, 245, 466, 1028, 525, 442, 324, 1202, 337, 189, 794, 130, 643, 1136, 179, 437, 244, 784, 495, 941, 359, 587, 407, 202, 900, 493, 70, 795, 913, 350, 374, 309, 477, 673, 510, 1162, 856, 965, 403, 676, 1111, 959, 204, 963, 127, 501, 592, 499, 343, 286, 436, 319, 318, 1110, 639, 1152, 1178, 979, 267, 763, 388, 379, 1209, 74, 1012, 517, 455, 386, 414, 1160, 581, 857, 494, 353, 174, 252, 550, 543, 253, 813, 8, 65, 839, 170, 97, 19, 1029, 1100, 181, 1117, 182, 88, 1190, 903, 1170, 358, 987, 1200, 365, 886, 313, 657, 124, 1176, 1146, 805, 908, 1203, 46, 838, 62, 532, 779, 186, 1085, 61, 708, 228, 729, 809, 341, 456, 342, 1130, 326, 434, 153, 86, 869, 1169, 931, 1045, 212, 902, 575, 166, 621, 748, 918, 1184, 820, 320, 123, 120, 115, 536, 577, 594, 624, 667, 198, 627, 167, 213, 180, 193, 234, 524, 139, 847, 920, 1096, 226, 885, 559, 246, 1007, 387, 816, 176, 141, 892, 659, 682, 749, 787, 238, 303, 774, 421, 11, 865, 940, 1076, 281, 1102, 1210, 45, 331, 662, 332, 489, 852, 14, 680, 615, 786, 626, 781, 996, 76, 230, 242, 1156, 427, 380, 367, 201, 573, 873, 1066, 1208, 824, 570, 521, 867, 150, 420, 845, 1026, 216, 197, 636, 1175, 556, 527, 507, 1005, 12, 1128, 905, 232, 411, 93, 205, 1088, 1090, 603, 1155, 250, 354, 296, 87, 737, 221, 249, 653, 720, 634, 311, 1092, 1142, 1068, 362, 406, 41, 967, 1051, 968, 723, 1185, 894, 735, 688, 731, 998, 535, 864, 702, 55, 396, 782, 686, 57, 339, 24, 51, 357, 628, 100, 1177, 1150, 67, 382, 665, 500, 395, 713, 400, 283, 44, 1013, 994, 879, 1098, 571, 601, 681, 1056, 850, 443, 460, 851, 512, 1119, 502, 1035, 871, 78, 701, 834, 22, 568, 803, 946, 1046, 172, 151, 289, 72, 133, 1213, 707, 1030, 1165, 928, 785, 566, 291, 721, 473, 330, 440, 64, 933, 953, 596, 422, 149, 1058, 837, 419, 47, 6, 1048, 895, 880, 1187, 554, 208, 229, 1083, 569, 185, 1042, 1101, 929, 1140, 1108, 496, 258, 95, 195, 712, 770, 40, 751, 661, 508, 983, 551, 522, 288, 345, 531, 668, 1113, 614, 1073, 992, 5, 664, 588, 670, 560, 796, 732, 1161, 1086, 1138, 644, 1120, 916, 196, 476, 593, 117, 1205, 974, 1079, 611, 981, 1080, 583, 392, 316, 598, 1008, 812, 53, 94, 1041, 458, 464, 714, 618, 1123, 922, 1053, 557, 572, 804, 1135, 1197, 372, 83, 815, 930, 251, 917, 306, 344, 632, 1017, 429, 791, 854, 4, 995, 829, 271, 1188, 758, 308, 222, 1131, 80, 1106, 1004, 79, 923, 321, 173, 224, 472, 25, 1050, 819, 817, 875, 107, 293, 408, 538, 112, 257, 969, 338, 287, 397, 42, 876, 203, 504, 674, 292, 503, 888, 1087, 746, 877, 262, 934, 647, 462, 1207, 137, 333, 911, 1040, 487, 641, 1192, 776, 277, 599, 600, 1081, 1201, 449, 951, 684, 491, 612, 187, 887, 890, 101, 956, 446, 294, 1019, 241, 73, 574, 297, 970, 726, 184, 264, 989, 649, 1027, 323, 479, 775, 1145, 898, 881, 897, 1099, 356, 738, 1063, 497, 1025, 561, 1033, 255, 818, 997, 168, 616, 528, 361, 1214, 872, 669, 159, 138, 910, 656, 549, 1104, 225, 736, 650, 1067, 579, 431, 808, 733, 745, 409, 1171, 539, 274, 754, 860, 438, 863, 413, 972, 190, 23, 526, 191, 696, 385, 896, 474, 423, 889, 276, 709, 1065, 914, 465, 943, 841, 105, 110, 1000, 948, 741, 591, 606, 259, 1122, 757, 952, 424, 27, 284, 39, 351, 1093, 962, 298, 821, 445, 299, 450, 199, 882, 613, 116, 1059, 1074, 576, 537, 586, 957, 1194, 530, 633, 63, 1193, 747, 393, 263, 391, 984, 415, 132, 236, 1049, 625, 541, 268, 162, 830, 278, 862, 921, 177, 447, 160, 1124, 1127, 1031, 1157, 635, 505, 1189, 801, 1, 485, 336, 978, 310, 33, 183, 175, 114, 949, 1172, 405, 383, 619, 976, 975, 58, 384, 399, 506, 135, 118, 20, 210, 835, 844, 121, 347, 322, 1174, 954, 891, 609, 652, 1018, 381, 697, 716, 71, 154, 325, 1014, 705, 960, 227, 631, 915, 171, 765, 1199, 642, 513, 36, 806, 799, 814, 542, 231, 982, 985, 376, 772, 728, 756, 563, 966, 273, 10, 840, 270, 1186, 416, 200, 1044, 769, 378, 629, 937, 608, 693, 699, 206, 7, 144, 846, 377, 703, 771, 463, 590, 783, 17, 104, 986, 1032, 722, 595, 534, 59, 755, 1167, 85, 66, 428, 1107, 529, 945, 1064, 690, 648, 1191, 1094, 1206, 1125, 260, 562, 490, 235, 410, 164, 944, 740, 993, 935, 439, 1038, 734, 31, 904, 50, 1095, 49, 961]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2265162395238766
the save name prefix for this run is:  chkpt-ID_2265162395238766_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 194
rank avg (pred): 0.546 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001959761

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 54
rank avg (pred): 0.293 +- 0.204
mrr vals (pred, true): 0.011, 0.083
batch losses (mrrl, rdl): 0.0, 5.88484e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.461 +- 0.290
mrr vals (pred, true): 0.003, 0.001
batch losses (mrrl, rdl): 0.0, 2.35076e-05

Epoch over!
epoch time: 14.792

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 308
rank avg (pred): 0.292 +- 0.199
mrr vals (pred, true): 0.006, 0.092
batch losses (mrrl, rdl): 0.0, 8.38786e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.472 +- 0.292
mrr vals (pred, true): 0.003, 0.000
batch losses (mrrl, rdl): 0.0, 8.1822e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.471 +- 0.298
mrr vals (pred, true): 0.006, 0.000
batch losses (mrrl, rdl): 0.0, 2.46999e-05

Epoch over!
epoch time: 14.742

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 438
rank avg (pred): 0.453 +- 0.284
mrr vals (pred, true): 0.006, 0.000
batch losses (mrrl, rdl): 0.0, 1.20265e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1135
rank avg (pred): 0.295 +- 0.224
mrr vals (pred, true): 0.017, 0.121
batch losses (mrrl, rdl): 0.0, 3.40397e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.368 +- 0.268
mrr vals (pred, true): 0.010, 0.082
batch losses (mrrl, rdl): 0.0, 4.60302e-05

Epoch over!
epoch time: 14.738

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 279
rank avg (pred): 0.297 +- 0.220
mrr vals (pred, true): 0.015, 0.081
batch losses (mrrl, rdl): 0.0, 8.44931e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1152
rank avg (pred): 0.350 +- 0.263
mrr vals (pred, true): 0.016, 0.097
batch losses (mrrl, rdl): 0.0, 9.8368e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 175
rank avg (pred): 0.469 +- 0.269
mrr vals (pred, true): 0.004, 0.000
batch losses (mrrl, rdl): 0.0, 5.9478e-06

Epoch over!
epoch time: 14.747

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.468 +- 0.297
mrr vals (pred, true): 0.005, 0.000
batch losses (mrrl, rdl): 0.0, 2.33844e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 372
rank avg (pred): 0.501 +- 0.270
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 5.2459e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.298 +- 0.232
mrr vals (pred, true): 0.023, 0.108
batch losses (mrrl, rdl): 0.0, 4.7788e-05

Epoch over!
epoch time: 14.76

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 174
rank avg (pred): 0.506 +- 0.275
mrr vals (pred, true): 0.003, 0.000
batch losses (mrrl, rdl): 0.0224837139, 6.54658e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.459 +- 0.222
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 6.97397e-05, 9.32699e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 863
rank avg (pred): 0.500 +- 0.300
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.000168474, 8.2227e-06

Epoch over!
epoch time: 15.075

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.590 +- 0.287
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 8.04998e-05, 0.0002783629

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 75
rank avg (pred): 0.502 +- 0.315
mrr vals (pred, true): 0.083, 0.090
batch losses (mrrl, rdl): 0.0107069369, 0.0005340599

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 372
rank avg (pred): 0.447 +- 0.208
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002156714, 3.92909e-05

Epoch over!
epoch time: 15.051

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 313
rank avg (pred): 0.529 +- 0.306
mrr vals (pred, true): 0.049, 0.125
batch losses (mrrl, rdl): 0.0577257648, 0.0010645941

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1184
rank avg (pred): 0.404 +- 0.210
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001146587, 0.0001236789

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.367 +- 0.238
mrr vals (pred, true): 0.084, 0.088
batch losses (mrrl, rdl): 0.0118124727, 2.6042e-05

Epoch over!
epoch time: 15.042

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.463 +- 0.303
mrr vals (pred, true): 0.058, 0.072
batch losses (mrrl, rdl): 0.0005972923, 0.0004460402

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.021 +- 0.019
mrr vals (pred, true): 0.275, 0.332
batch losses (mrrl, rdl): 0.0321648791, 0.0002863854

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.716 +- 0.431
mrr vals (pred, true): 0.183, 0.317
batch losses (mrrl, rdl): 0.1772082746, 0.0062366151

Epoch over!
epoch time: 15.03

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.467 +- 0.224
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 9.7636e-06, 1.77386e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1141
rank avg (pred): 0.499 +- 0.351
mrr vals (pred, true): 0.170, 0.188
batch losses (mrrl, rdl): 0.0033287555, 0.0011193544

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.470 +- 0.291
mrr vals (pred, true): 0.063, 0.047
batch losses (mrrl, rdl): 0.0017809458, 0.0005498581

Epoch over!
epoch time: 15.051

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.503 +- 0.352
mrr vals (pred, true): 0.059, 0.112
batch losses (mrrl, rdl): 0.0285405852, 0.0006419776

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 65
rank avg (pred): 0.420 +- 0.208
mrr vals (pred, true): 0.063, 0.088
batch losses (mrrl, rdl): 0.0017362066, 0.0003159424

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 160
rank avg (pred): 0.482 +- 0.160
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 7.02e-06, 6.65491e-05

Epoch over!
epoch time: 15.05

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 414
rank avg (pred): 0.468 +- 0.200
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002894076, 4.08664e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 342
rank avg (pred): 0.593 +- 0.235
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002732359, 0.0003270892

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.388 +- 0.182
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002830785, 9.93252e-05

Epoch over!
epoch time: 14.975

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 720
rank avg (pred): 0.522 +- 0.211
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001132006, 3.72611e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1093
rank avg (pred): 0.464 +- 0.297
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003736941, 1.32618e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.062 +- 0.050
mrr vals (pred, true): 0.259, 0.153
batch losses (mrrl, rdl): 0.1125926971, 0.000832358

Epoch over!
epoch time: 15.023

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 896
rank avg (pred): 0.738 +- 0.423
mrr vals (pred, true): 0.073, 0.016
batch losses (mrrl, rdl): 0.0054177134, 0.0001513749

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 497
rank avg (pred): 0.269 +- 0.192
mrr vals (pred, true): 0.120, 0.150
batch losses (mrrl, rdl): 0.0089156739, 9.15339e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1192
rank avg (pred): 0.400 +- 0.207
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 2.67141e-05, 0.0001739445

Epoch over!
epoch time: 15.053

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.735 +- 0.427
mrr vals (pred, true): 0.194, 0.059
batch losses (mrrl, rdl): 0.2061212063, 0.0039766082

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 548
rank avg (pred): 0.395 +- 0.260
mrr vals (pred, true): 0.073, 0.126
batch losses (mrrl, rdl): 0.0280017685, 1.84958e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 734
rank avg (pred): 0.036 +- 0.033
mrr vals (pred, true): 0.327, 0.380
batch losses (mrrl, rdl): 0.0277984887, 0.0001738213

Epoch over!
epoch time: 15.045

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.426 +- 0.226
mrr vals (pred, true): 0.051, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   46 	     0 	 0.05311 	 0.00015 	 m..s
   23 	     1 	 0.05048 	 0.00016 	 m..s
   18 	     2 	 0.05020 	 0.00016 	 m..s
   22 	     3 	 0.05043 	 0.00016 	 m..s
   88 	     4 	 0.05907 	 0.00016 	 m..s
   24 	     5 	 0.05049 	 0.00017 	 m..s
   82 	     6 	 0.05862 	 0.00017 	 m..s
   15 	     7 	 0.05001 	 0.00017 	 m..s
   83 	     8 	 0.05879 	 0.00018 	 m..s
   86 	     9 	 0.05897 	 0.00018 	 m..s
    0 	    10 	 0.04349 	 0.00018 	 m..s
   11 	    11 	 0.04908 	 0.00019 	 m..s
   10 	    12 	 0.04890 	 0.00019 	 m..s
   90 	    13 	 0.05933 	 0.00019 	 m..s
   74 	    14 	 0.05758 	 0.00019 	 m..s
    2 	    15 	 0.04382 	 0.00020 	 m..s
   27 	    16 	 0.05055 	 0.00020 	 m..s
   36 	    17 	 0.05165 	 0.00020 	 m..s
   64 	    18 	 0.05609 	 0.00021 	 m..s
   42 	    19 	 0.05291 	 0.00021 	 m..s
   12 	    20 	 0.04989 	 0.00022 	 m..s
    9 	    21 	 0.04848 	 0.00022 	 m..s
   17 	    22 	 0.05015 	 0.00022 	 m..s
   61 	    23 	 0.05558 	 0.00023 	 m..s
   25 	    24 	 0.05050 	 0.00023 	 m..s
   20 	    25 	 0.05040 	 0.00023 	 m..s
   28 	    26 	 0.05069 	 0.00024 	 m..s
   44 	    27 	 0.05309 	 0.00024 	 m..s
    1 	    28 	 0.04362 	 0.00025 	 m..s
   58 	    29 	 0.05509 	 0.00025 	 m..s
    6 	    30 	 0.04804 	 0.00025 	 m..s
    4 	    31 	 0.04444 	 0.00025 	 m..s
   51 	    32 	 0.05342 	 0.00025 	 m..s
   21 	    33 	 0.05043 	 0.00026 	 m..s
   32 	    34 	 0.05130 	 0.00026 	 m..s
   30 	    35 	 0.05093 	 0.00026 	 m..s
   56 	    36 	 0.05496 	 0.00027 	 m..s
   33 	    37 	 0.05143 	 0.00027 	 m..s
   65 	    38 	 0.05610 	 0.00028 	 m..s
    3 	    39 	 0.04425 	 0.00028 	 m..s
   94 	    40 	 0.06647 	 0.00028 	 m..s
   19 	    41 	 0.05029 	 0.00028 	 m..s
    7 	    42 	 0.04813 	 0.00029 	 m..s
   34 	    43 	 0.05144 	 0.00029 	 m..s
   16 	    44 	 0.05011 	 0.00030 	 m..s
   13 	    45 	 0.04992 	 0.00030 	 m..s
   45 	    46 	 0.05309 	 0.00030 	 m..s
   72 	    47 	 0.05726 	 0.00030 	 m..s
   77 	    48 	 0.05830 	 0.00030 	 m..s
   93 	    49 	 0.06375 	 0.00031 	 m..s
   49 	    50 	 0.05325 	 0.00031 	 m..s
   43 	    51 	 0.05295 	 0.00032 	 m..s
   39 	    52 	 0.05228 	 0.00032 	 m..s
   80 	    53 	 0.05852 	 0.00032 	 m..s
   66 	    54 	 0.05620 	 0.00034 	 m..s
   38 	    55 	 0.05188 	 0.00035 	 m..s
   62 	    56 	 0.05587 	 0.00038 	 m..s
   73 	    57 	 0.05731 	 0.00038 	 m..s
   37 	    58 	 0.05184 	 0.00039 	 m..s
   26 	    59 	 0.05054 	 0.00039 	 m..s
   52 	    60 	 0.05352 	 0.00040 	 m..s
   48 	    61 	 0.05321 	 0.00041 	 m..s
   35 	    62 	 0.05155 	 0.00046 	 m..s
   14 	    63 	 0.04994 	 0.00047 	 m..s
   71 	    64 	 0.05715 	 0.00048 	 m..s
   68 	    65 	 0.05623 	 0.00048 	 m..s
   53 	    66 	 0.05427 	 0.00049 	 m..s
   60 	    67 	 0.05539 	 0.00050 	 m..s
   50 	    68 	 0.05339 	 0.00052 	 m..s
   29 	    69 	 0.05091 	 0.00052 	 m..s
   40 	    70 	 0.05236 	 0.00057 	 m..s
   63 	    71 	 0.05590 	 0.00061 	 m..s
   47 	    72 	 0.05313 	 0.00063 	 m..s
   57 	    73 	 0.05507 	 0.00063 	 m..s
   31 	    74 	 0.05097 	 0.00066 	 m..s
   69 	    75 	 0.05647 	 0.00066 	 m..s
    8 	    76 	 0.04834 	 0.00068 	 m..s
   67 	    77 	 0.05620 	 0.00079 	 m..s
   55 	    78 	 0.05492 	 0.00090 	 m..s
   87 	    79 	 0.05903 	 0.00113 	 m..s
   59 	    80 	 0.05520 	 0.00144 	 m..s
   76 	    81 	 0.05809 	 0.00187 	 m..s
   54 	    82 	 0.05444 	 0.00193 	 m..s
    5 	    83 	 0.04713 	 0.00286 	 m..s
   85 	    84 	 0.05891 	 0.00831 	 m..s
   70 	    85 	 0.05668 	 0.00842 	 m..s
   41 	    86 	 0.05286 	 0.00845 	 m..s
   92 	    87 	 0.05970 	 0.01803 	 m..s
   78 	    88 	 0.05833 	 0.04928 	 ~...
   84 	    89 	 0.05880 	 0.06461 	 ~...
   89 	    90 	 0.05910 	 0.07732 	 ~...
   75 	    91 	 0.05790 	 0.07831 	 ~...
   81 	    92 	 0.05861 	 0.08778 	 ~...
  105 	    93 	 0.09216 	 0.08786 	 ~...
  102 	    94 	 0.07269 	 0.09013 	 ~...
   97 	    95 	 0.06921 	 0.09244 	 ~...
  106 	    96 	 0.09637 	 0.09246 	 ~...
   97 	    97 	 0.06921 	 0.09441 	 ~...
   97 	    98 	 0.06921 	 0.09680 	 ~...
   79 	    99 	 0.05834 	 0.09994 	 m..s
  103 	   100 	 0.07331 	 0.10102 	 ~...
   96 	   101 	 0.06661 	 0.10137 	 m..s
   91 	   102 	 0.05960 	 0.10160 	 m..s
  104 	   103 	 0.08029 	 0.10273 	 ~...
   97 	   104 	 0.06921 	 0.10394 	 m..s
   97 	   105 	 0.06921 	 0.10828 	 m..s
  113 	   106 	 0.17354 	 0.10876 	 m..s
  115 	   107 	 0.17910 	 0.11644 	 m..s
   95 	   108 	 0.06648 	 0.11931 	 m..s
  114 	   109 	 0.17731 	 0.13371 	 m..s
  111 	   110 	 0.16668 	 0.14537 	 ~...
  112 	   111 	 0.17003 	 0.14927 	 ~...
  107 	   112 	 0.12071 	 0.15424 	 m..s
  110 	   113 	 0.15596 	 0.16794 	 ~...
  109 	   114 	 0.15221 	 0.17020 	 ~...
  108 	   115 	 0.13067 	 0.17200 	 m..s
  116 	   116 	 0.18600 	 0.18348 	 ~...
  117 	   117 	 0.18623 	 0.18637 	 ~...
  118 	   118 	 0.19683 	 0.20159 	 ~...
  119 	   119 	 0.22380 	 0.29022 	 m..s
  120 	   120 	 0.24680 	 0.29684 	 m..s
==========================================
r_mrr = 0.8844196796417236
r2_mrr = 0.4334491491317749
spearmanr_mrr@5 = 0.962550699710846
spearmanr_mrr@10 = 0.9802109599113464
spearmanr_mrr@50 = 0.8899049758911133
spearmanr_mrr@100 = 0.9102174043655396
spearmanr_mrr@All = 0.9154893755912781
==========================================
test time: 0.445
Done Testing dataset DBpedia50
total time taken: 230.00590252876282
training time taken: 224.62961840629578
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8844)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4334)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9626)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9802)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.8899)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9102)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9155)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.4200608211576764}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 8254701711281827
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [154, 386, 877, 1189, 328, 761, 515, 496, 6, 417, 1018, 12, 1033, 671, 206, 249, 471, 999, 909, 112, 89, 638, 75, 1184, 880, 996, 774, 293, 850, 627, 308, 835, 180, 968, 236, 416, 211, 305, 1135, 639, 406, 862, 474, 665, 1047, 334, 756, 53, 590, 1170, 487, 262, 901, 817, 194, 146, 400, 752, 534, 144, 392, 810, 551, 189, 9, 191, 1174, 605, 65, 963, 162, 115, 1002, 138, 546, 848, 787, 269, 816, 579, 1140, 500, 934, 364, 466, 1071, 223, 209, 464, 547, 882, 1012, 998, 991, 222, 647, 291, 340, 67, 271, 818, 495, 472, 491, 673, 642, 512, 425, 1150, 911, 462, 321, 373, 616, 1034, 1139, 212, 780, 1053, 168, 1046]
valid_ids (0): []
train_ids (1094): [860, 728, 635, 387, 931, 325, 208, 185, 1205, 718, 976, 623, 600, 139, 316, 809, 971, 375, 362, 480, 771, 614, 233, 260, 733, 948, 588, 48, 871, 697, 651, 26, 423, 114, 634, 856, 957, 202, 747, 490, 894, 859, 332, 1011, 1029, 201, 785, 435, 949, 1025, 70, 475, 874, 580, 456, 514, 219, 975, 352, 503, 586, 1156, 1107, 297, 674, 175, 896, 1049, 961, 42, 479, 812, 273, 612, 331, 36, 1090, 955, 1072, 152, 108, 529, 64, 734, 722, 388, 68, 649, 928, 399, 796, 513, 284, 13, 380, 956, 765, 815, 511, 606, 1207, 696, 505, 1130, 251, 50, 253, 57, 577, 715, 327, 173, 229, 1144, 281, 492, 499, 239, 889, 313, 21, 710, 192, 439, 432, 47, 596, 763, 736, 188, 164, 536, 390, 252, 247, 721, 73, 445, 426, 141, 157, 844, 618, 368, 581, 1105, 1057, 1054, 32, 675, 15, 187, 942, 1093, 829, 419, 764, 1097, 78, 19, 620, 881, 83, 203, 37, 1142, 637, 1213, 170, 397, 1121, 870, 125, 1206, 1137, 692, 724, 919, 1035, 494, 555, 1041, 729, 1208, 183, 585, 689, 1045, 24, 940, 921, 558, 270, 77, 415, 145, 200, 1125, 684, 825, 453, 318, 640, 259, 1059, 179, 307, 824, 954, 895, 570, 709, 857, 1020, 1064, 843, 572, 0, 779, 1088, 1165, 604, 1056, 395, 486, 469, 652, 461, 836, 630, 452, 1154, 1087, 484, 1101, 661, 1147, 986, 535, 323, 1060, 498, 287, 1003, 356, 962, 195, 897, 906, 1082, 995, 1199, 993, 599, 607, 1179, 806, 278, 1043, 97, 1081, 1080, 660, 625, 396, 544, 129, 133, 754, 225, 904, 147, 242, 346, 506, 374, 681, 1079, 594, 703, 1068, 369, 990, 1136, 217, 258, 410, 370, 1172, 1098, 52, 95, 1133, 485, 899, 23, 584, 978, 842, 947, 459, 1197, 153, 977, 959, 778, 427, 436, 454, 643, 578, 730, 16, 176, 447, 371, 213, 104, 44, 3, 277, 353, 751, 122, 1118, 181, 644, 772, 907, 507, 174, 548, 120, 613, 320, 1168, 650, 553, 172, 658, 750, 98, 137, 974, 1074, 762, 1076, 267, 516, 646, 159, 304, 298, 552, 890, 1119, 411, 550, 935, 830, 302, 910, 1173, 20, 711, 448, 1116, 389, 178, 363, 407, 39, 227, 929, 63, 46, 94, 401, 539, 230, 922, 420, 915, 465, 177, 985, 303, 72, 256, 800, 1051, 1160, 1091, 339, 917, 254, 805, 1134, 372, 984, 636, 451, 130, 814, 964, 442, 127, 744, 1006, 1023, 405, 621, 408, 1026, 430, 926, 615, 1063, 134, 833, 377, 967, 82, 394, 979, 1127, 608, 686, 88, 568, 808, 840, 483, 276, 566, 35, 424, 945, 5, 330, 493, 807, 398, 312, 107, 379, 1048, 705, 1092, 927, 382, 790, 617, 508, 348, 165, 549, 1181, 821, 943, 1001, 306, 1145, 749, 759, 668, 804, 932, 786, 1159, 966, 972, 1203, 1095, 158, 559, 79, 1086, 654, 1042, 69, 788, 582, 845, 329, 90, 822, 603, 463, 873, 1039, 324, 336, 151, 864, 1157, 933, 414, 91, 866, 672, 467, 7, 433, 849, 1161, 556, 521, 10, 851, 587, 893, 1106, 4, 1084, 732, 629, 51, 1099, 231, 1198, 1028, 1062, 583, 279, 884, 601, 1152, 266, 27, 656, 1055, 746, 55, 244, 66, 981, 204, 1185, 669, 450, 589, 142, 1149, 611, 335, 1169, 357, 776, 470, 983, 941, 119, 131, 1073, 1194, 939, 272, 865, 913, 803, 1036, 1192, 662, 169, 898, 628, 567, 1005, 1113, 811, 846, 99, 1148, 103, 592, 1164, 56, 720, 124, 525, 33, 1167, 443, 690, 719, 265, 648, 240, 641, 920, 126, 685, 792, 1096, 852, 1178, 404, 338, 317, 659, 524, 92, 367, 431, 885, 886, 299, 1187, 753, 109, 770, 1022, 243, 337, 393, 241, 791, 1070, 1021, 758, 878, 887, 381, 228, 121, 969, 708, 858, 973, 872, 632, 1143, 171, 161, 1102, 602, 14, 923, 478, 701, 737, 366, 87, 994, 936, 837, 1141, 1, 892, 755, 688, 383, 326, 542, 437, 828, 801, 86, 1201, 528, 111, 93, 538, 702, 235, 855, 742, 1104, 214, 532, 930, 264, 1155, 802, 717, 793, 706, 609, 449, 422, 838, 221, 952, 598, 1153, 143, 767, 199, 234, 322, 123, 1114, 1019, 554, 128, 832, 924, 704, 925, 84, 1017, 545, 193, 560, 1013, 434, 248, 1032, 509, 1196, 726, 8, 591, 667, 1209, 768, 1108, 655, 795, 38, 745, 526, 1214, 136, 1117, 781, 100, 365, 679, 45, 766, 1038, 1067, 440, 879, 149, 1158, 997, 347, 682, 989, 1075, 354, 1129, 215, 160, 216, 557, 309, 274, 488, 155, 224, 841, 226, 1191, 847, 861, 1162, 96, 593, 85, 1190, 473, 595, 446, 245, 1030, 429, 914, 315, 1146, 413, 1200, 197, 62, 28, 992, 799, 41, 533, 1037, 716, 735, 541, 156, 1186, 1110, 1050, 378, 76, 1016, 1044, 489, 903, 286, 275, 1124, 982, 1007, 1183, 739, 1008, 645, 773, 1188, 1163, 1027, 723, 163, 54, 531, 797, 1138, 820, 1031, 412, 1195, 876, 740, 481, 71, 694, 1089, 946, 1100, 707, 633, 314, 569, 29, 918, 311, 457, 831, 198, 343, 563, 875, 207, 333, 190, 700, 900, 561, 677, 34, 1182, 246, 1193, 571, 458, 182, 619, 502, 912, 958, 349, 441, 823, 296, 166, 687, 1066, 537, 220, 691, 61, 819, 40, 1009, 1112, 775, 1065, 409, 186, 813, 1166, 520, 1132, 1078, 798, 1014, 105, 350, 714, 1058, 916, 664, 1212, 853, 250, 282, 1085, 1004, 1069, 150, 295, 218, 826, 30, 527, 342, 951, 1177, 418, 43, 908, 562, 1111, 827, 476, 713, 421, 1210, 135, 1120, 725, 784, 1151, 310, 1123, 110, 980, 268, 210, 1010, 610, 345, 760, 1052, 288, 80, 518, 49, 839, 769, 22, 902, 624, 292, 285, 888, 670, 676, 965, 1024, 257, 712, 2, 540, 953, 631, 438, 794, 460, 782, 237, 699, 238, 937, 1175, 678, 359, 738, 184, 106, 510, 403, 1000, 748, 1211, 905, 1109, 575, 74, 17, 868, 25, 1131, 60, 1083, 101, 1103, 960, 653, 294, 148, 1015, 869, 666, 289, 523, 522, 564, 1204, 834, 477, 757, 657, 574, 950, 384, 205, 622, 698, 58, 113, 597, 361, 1094, 683, 1180, 1061, 1202, 504, 731, 117, 261, 468, 501, 283, 301, 196, 543, 263, 376, 519, 355, 428, 573, 102, 987, 988, 140, 626, 444, 1115, 31, 743, 530, 680, 344, 517, 1128, 280, 358, 59, 944, 116, 1122, 455, 777, 1077, 854, 482, 741, 319, 11, 132, 695, 385, 1171, 891, 360, 867, 118, 341, 167, 290, 863, 351, 402, 232, 497, 789, 663, 18, 1040, 693, 81, 255, 1126, 970, 883, 727, 783, 1176, 300, 938, 565, 576, 391]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4413811063692282
the save name prefix for this run is:  chkpt-ID_4413811063692282_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 202
rank avg (pred): 0.455 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001032805

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 857
rank avg (pred): 0.461 +- 0.105
mrr vals (pred, true): 0.005, 0.002
batch losses (mrrl, rdl): 0.0, 9.90177e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.454 +- 0.293
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0, 8.3586e-06

Epoch over!
epoch time: 14.978

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 820
rank avg (pred): 0.230 +- 0.171
mrr vals (pred, true): 0.136, 0.151
batch losses (mrrl, rdl): 0.0, 8.37932e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.457 +- 0.299
mrr vals (pred, true): 0.106, 0.001
batch losses (mrrl, rdl): 0.0, 4.50149e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.461 +- 0.293
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.0, 5.0969e-06

Epoch over!
epoch time: 14.943

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 884
rank avg (pred): 0.509 +- 0.293
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.0, 0.00010109

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.495 +- 0.292
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0, 1.225e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 239
rank avg (pred): 0.503 +- 0.293
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0, 2.9194e-06

Epoch over!
epoch time: 14.913

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.177 +- 0.191
mrr vals (pred, true): 0.099, 0.220
batch losses (mrrl, rdl): 0.0, 5.97239e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 250
rank avg (pred): 0.248 +- 0.248
mrr vals (pred, true): 0.062, 0.120
batch losses (mrrl, rdl): 0.0, 2.62717e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.496 +- 0.281
mrr vals (pred, true): 0.028, 0.000
batch losses (mrrl, rdl): 0.0, 5.2365e-06

Epoch over!
epoch time: 14.86

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 788
rank avg (pred): 0.482 +- 0.286
mrr vals (pred, true): 0.026, 0.001
batch losses (mrrl, rdl): 0.0, 3.70786e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 528
rank avg (pred): 0.331 +- 0.302
mrr vals (pred, true): 0.055, 0.093
batch losses (mrrl, rdl): 0.0, 7.89806e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.350 +- 0.297
mrr vals (pred, true): 0.052, 0.102
batch losses (mrrl, rdl): 0.0, 3.05414e-05

Epoch over!
epoch time: 14.844

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 139
rank avg (pred): 0.490 +- 0.297
mrr vals (pred, true): 0.025, 0.000
batch losses (mrrl, rdl): 0.0063514765, 1.3476e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1027
rank avg (pred): 0.468 +- 0.240
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004478411, 1.87372e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 237
rank avg (pred): 0.461 +- 0.240
mrr vals (pred, true): 0.038, 0.002
batch losses (mrrl, rdl): 0.001517023, 4.70723e-05

Epoch over!
epoch time: 15.083

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 710
rank avg (pred): 0.442 +- 0.235
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003975154, 0.0001148102

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 542
rank avg (pred): 0.393 +- 0.212
mrr vals (pred, true): 0.048, 0.097
batch losses (mrrl, rdl): 3.39237e-05, 0.0001200269

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 526
rank avg (pred): 0.386 +- 0.210
mrr vals (pred, true): 0.041, 0.070
batch losses (mrrl, rdl): 0.0007440188, 7.01971e-05

Epoch over!
epoch time: 15.045

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.417 +- 0.223
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 8.3631e-06, 0.000122307

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.459 +- 0.246
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.1701e-06, 5.11274e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 767
rank avg (pred): 0.471 +- 0.254
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004378206, 6.67e-06

Epoch over!
epoch time: 15.039

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.279 +- 0.242
mrr vals (pred, true): 0.201, 0.186
batch losses (mrrl, rdl): 0.0020989343, 0.0001708391

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1173
rank avg (pred): 0.450 +- 0.235
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001596486, 3.63914e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.443 +- 0.208
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0007324825, 3.10828e-05

Epoch over!
epoch time: 15.115

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.338 +- 0.198
mrr vals (pred, true): 0.069, 0.088
batch losses (mrrl, rdl): 0.0035481099, 6.40911e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 188
rank avg (pred): 0.520 +- 0.262
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.00011495, 0.0001313858

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.291 +- 0.236
mrr vals (pred, true): 0.227, 0.253
batch losses (mrrl, rdl): 0.0067145424, 0.0003866906

Epoch over!
epoch time: 15.159

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 216
rank avg (pred): 0.510 +- 0.244
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004945216, 2.31649e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1123
rank avg (pred): 0.467 +- 0.233
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.63905e-05, 5.30881e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.468 +- 0.226
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001425972, 3.26266e-05

Epoch over!
epoch time: 15.137

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 634
rank avg (pred): 0.457 +- 0.246
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002362958, 2.12595e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.455 +- 0.236
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004324752, 1.51592e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 473
rank avg (pred): 0.416 +- 0.230
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.73416e-05, 5.96928e-05

Epoch over!
epoch time: 15.132

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 509
rank avg (pred): 0.314 +- 0.195
mrr vals (pred, true): 0.128, 0.170
batch losses (mrrl, rdl): 0.0178921446, 9.87296e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 675
rank avg (pred): 0.466 +- 0.258
mrr vals (pred, true): 0.051, 0.008
batch losses (mrrl, rdl): 1.97647e-05, 1.95039e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 676
rank avg (pred): 0.472 +- 0.287
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.261e-07, 1.8646e-06

Epoch over!
epoch time: 15.079

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1094
rank avg (pred): 0.477 +- 0.279
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.9012e-05, 8.3489e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 870
rank avg (pred): 0.497 +- 0.324
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0028163942, 1.61583e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 531
rank avg (pred): 0.362 +- 0.217
mrr vals (pred, true): 0.068, 0.111
batch losses (mrrl, rdl): 0.0187638607, 3.81214e-05

Epoch over!
epoch time: 15.087

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 457
rank avg (pred): 0.468 +- 0.276
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 1.8631e-06, 8.476e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.465 +- 0.270
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003688008, 7.7375e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 28
rank avg (pred): 0.360 +- 0.206
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0006466856, 0.0001167988

Epoch over!
epoch time: 15.111

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.513 +- 0.319
mrr vals (pred, true): 0.043, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04304 	 0.00013 	 m..s
   45 	     1 	 0.04536 	 0.00014 	 m..s
   34 	     2 	 0.04471 	 0.00014 	 m..s
   57 	     3 	 0.04597 	 0.00014 	 m..s
   13 	     4 	 0.04337 	 0.00015 	 m..s
   49 	     5 	 0.04549 	 0.00016 	 m..s
   70 	     6 	 0.04757 	 0.00017 	 m..s
    1 	     7 	 0.04121 	 0.00017 	 m..s
   17 	     8 	 0.04345 	 0.00018 	 m..s
   12 	     9 	 0.04318 	 0.00018 	 m..s
   22 	    10 	 0.04363 	 0.00020 	 m..s
   38 	    11 	 0.04484 	 0.00020 	 m..s
   24 	    12 	 0.04377 	 0.00020 	 m..s
   54 	    13 	 0.04564 	 0.00020 	 m..s
   30 	    14 	 0.04418 	 0.00020 	 m..s
   11 	    15 	 0.04305 	 0.00021 	 m..s
   44 	    16 	 0.04523 	 0.00021 	 m..s
   36 	    17 	 0.04474 	 0.00021 	 m..s
   47 	    18 	 0.04538 	 0.00022 	 m..s
   14 	    19 	 0.04340 	 0.00022 	 m..s
   76 	    20 	 0.05255 	 0.00022 	 m..s
   52 	    21 	 0.04553 	 0.00022 	 m..s
   46 	    22 	 0.04536 	 0.00022 	 m..s
   60 	    23 	 0.04672 	 0.00023 	 m..s
   16 	    24 	 0.04343 	 0.00023 	 m..s
   83 	    25 	 0.05523 	 0.00024 	 m..s
   19 	    26 	 0.04349 	 0.00024 	 m..s
   68 	    27 	 0.04750 	 0.00025 	 m..s
   21 	    28 	 0.04362 	 0.00025 	 m..s
   59 	    29 	 0.04672 	 0.00026 	 m..s
   78 	    30 	 0.05263 	 0.00026 	 m..s
   23 	    31 	 0.04370 	 0.00026 	 m..s
   18 	    32 	 0.04345 	 0.00026 	 m..s
   51 	    33 	 0.04551 	 0.00027 	 m..s
   61 	    34 	 0.04674 	 0.00027 	 m..s
   62 	    35 	 0.04676 	 0.00028 	 m..s
   63 	    36 	 0.04682 	 0.00028 	 m..s
   40 	    37 	 0.04509 	 0.00029 	 m..s
    2 	    38 	 0.04146 	 0.00029 	 m..s
   28 	    39 	 0.04407 	 0.00030 	 m..s
   33 	    40 	 0.04462 	 0.00030 	 m..s
   32 	    41 	 0.04452 	 0.00031 	 m..s
   73 	    42 	 0.04774 	 0.00031 	 m..s
   71 	    43 	 0.04769 	 0.00032 	 m..s
   56 	    44 	 0.04584 	 0.00032 	 m..s
    3 	    45 	 0.04165 	 0.00033 	 m..s
   58 	    46 	 0.04629 	 0.00033 	 m..s
   31 	    47 	 0.04431 	 0.00033 	 m..s
   86 	    48 	 0.05847 	 0.00033 	 m..s
   35 	    49 	 0.04472 	 0.00034 	 m..s
   41 	    50 	 0.04510 	 0.00034 	 m..s
   89 	    51 	 0.05980 	 0.00035 	 m..s
   50 	    52 	 0.04550 	 0.00036 	 m..s
   20 	    53 	 0.04361 	 0.00037 	 m..s
    0 	    54 	 0.03922 	 0.00039 	 m..s
   67 	    55 	 0.04749 	 0.00040 	 m..s
   27 	    56 	 0.04399 	 0.00041 	 m..s
   42 	    57 	 0.04516 	 0.00042 	 m..s
   64 	    58 	 0.04710 	 0.00044 	 m..s
    9 	    59 	 0.04304 	 0.00044 	 m..s
   37 	    60 	 0.04480 	 0.00046 	 m..s
   72 	    61 	 0.04772 	 0.00046 	 m..s
   74 	    62 	 0.04896 	 0.00046 	 m..s
   55 	    63 	 0.04579 	 0.00046 	 m..s
   75 	    64 	 0.04961 	 0.00048 	 m..s
   39 	    65 	 0.04493 	 0.00049 	 m..s
   25 	    66 	 0.04387 	 0.00050 	 m..s
   65 	    67 	 0.04736 	 0.00052 	 m..s
   43 	    68 	 0.04516 	 0.00056 	 m..s
   48 	    69 	 0.04539 	 0.00057 	 m..s
    7 	    70 	 0.04301 	 0.00061 	 m..s
    6 	    71 	 0.04301 	 0.00066 	 m..s
   53 	    72 	 0.04554 	 0.00069 	 m..s
   15 	    73 	 0.04343 	 0.00074 	 m..s
   69 	    74 	 0.04753 	 0.00090 	 m..s
   66 	    75 	 0.04746 	 0.00093 	 m..s
   10 	    76 	 0.04305 	 0.00099 	 m..s
   26 	    77 	 0.04392 	 0.00272 	 m..s
    5 	    78 	 0.04273 	 0.00285 	 m..s
   29 	    79 	 0.04413 	 0.00574 	 m..s
    4 	    80 	 0.04244 	 0.00633 	 m..s
   77 	    81 	 0.05263 	 0.01803 	 m..s
  117 	    82 	 0.31138 	 0.04430 	 MISS
  115 	    83 	 0.29892 	 0.05391 	 MISS
  116 	    84 	 0.30226 	 0.05909 	 MISS
   95 	    85 	 0.09010 	 0.07237 	 ~...
   90 	    86 	 0.06024 	 0.07830 	 ~...
   79 	    87 	 0.05431 	 0.08694 	 m..s
   80 	    88 	 0.05431 	 0.08835 	 m..s
   87 	    89 	 0.05899 	 0.08857 	 ~...
   85 	    90 	 0.05751 	 0.09022 	 m..s
   93 	    91 	 0.07962 	 0.09248 	 ~...
   84 	    92 	 0.05599 	 0.09262 	 m..s
  113 	    93 	 0.20097 	 0.09413 	 MISS
   81 	    94 	 0.05434 	 0.09778 	 m..s
   92 	    95 	 0.07557 	 0.09894 	 ~...
   88 	    96 	 0.05925 	 0.10297 	 m..s
   82 	    97 	 0.05509 	 0.10564 	 m..s
   96 	    98 	 0.09699 	 0.10765 	 ~...
   97 	    99 	 0.10088 	 0.10792 	 ~...
  100 	   100 	 0.10882 	 0.10853 	 ~...
   94 	   101 	 0.08861 	 0.11248 	 ~...
  105 	   102 	 0.13623 	 0.11548 	 ~...
  104 	   103 	 0.13315 	 0.12051 	 ~...
  101 	   104 	 0.11097 	 0.12768 	 ~...
   91 	   105 	 0.07364 	 0.12801 	 m..s
  102 	   106 	 0.11751 	 0.12939 	 ~...
   98 	   107 	 0.10134 	 0.13421 	 m..s
  108 	   108 	 0.14675 	 0.14457 	 ~...
  109 	   109 	 0.15073 	 0.14537 	 ~...
  106 	   110 	 0.14533 	 0.14671 	 ~...
   99 	   111 	 0.10610 	 0.14850 	 m..s
  103 	   112 	 0.12356 	 0.15039 	 ~...
  107 	   113 	 0.14654 	 0.16506 	 ~...
  110 	   114 	 0.15400 	 0.16999 	 ~...
  112 	   115 	 0.20012 	 0.19401 	 ~...
  111 	   116 	 0.19146 	 0.22067 	 ~...
  114 	   117 	 0.26089 	 0.26787 	 ~...
  118 	   118 	 0.31545 	 0.29684 	 ~...
  120 	   119 	 0.33273 	 0.33208 	 ~...
  119 	   120 	 0.32263 	 0.35955 	 m..s
==========================================
r_mrr = 0.787009596824646
r2_mrr = 0.4185529947280884
spearmanr_mrr@5 = 0.9869702458381653
spearmanr_mrr@10 = 0.8556151986122131
spearmanr_mrr@50 = 0.9257835149765015
spearmanr_mrr@100 = 0.9392744302749634
spearmanr_mrr@All = 0.9423609375953674
==========================================
test time: 0.456
Done Testing dataset DBpedia50
total time taken: 231.26007437705994
training time taken: 225.99191331863403
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.7870)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4186)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9870)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8556)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9258)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9393)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9424)}}, 'test_loss': {'ComplEx': {'DBpedia50': 2.393464825105184}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 9832682499863930
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [945, 705, 91, 353, 193, 70, 587, 172, 630, 552, 500, 1093, 675, 581, 634, 351, 1116, 1031, 742, 504, 1194, 1029, 629, 1052, 97, 13, 888, 325, 19, 916, 697, 1023, 726, 1190, 1103, 408, 813, 287, 334, 731, 188, 883, 282, 703, 1135, 436, 386, 1126, 223, 230, 419, 789, 520, 340, 971, 851, 624, 1098, 61, 656, 521, 935, 260, 750, 55, 1017, 574, 128, 565, 297, 593, 542, 298, 1200, 421, 202, 842, 969, 145, 235, 761, 155, 66, 1072, 740, 1136, 606, 1111, 544, 101, 1034, 1204, 40, 492, 1077, 528, 217, 153, 657, 641, 1125, 390, 288, 1189, 371, 860, 724, 157, 51, 470, 445, 557, 1076, 163, 98, 499, 835, 10, 941, 269, 931]
valid_ids (0): []
train_ids (1094): [1048, 130, 572, 181, 1130, 125, 48, 225, 812, 820, 1011, 613, 514, 320, 144, 948, 625, 843, 1144, 413, 618, 264, 678, 921, 1008, 978, 579, 147, 1012, 914, 263, 392, 929, 361, 1037, 1114, 1210, 957, 1172, 330, 135, 982, 592, 444, 471, 468, 988, 702, 122, 881, 26, 1181, 212, 938, 307, 209, 1146, 75, 876, 178, 598, 126, 901, 1080, 597, 161, 1120, 631, 1083, 224, 694, 1207, 633, 214, 18, 563, 554, 467, 1075, 693, 380, 576, 752, 1100, 808, 666, 176, 794, 335, 1184, 547, 1068, 517, 567, 676, 900, 966, 787, 821, 186, 459, 295, 1140, 551, 1159, 268, 838, 973, 1133, 1117, 302, 893, 252, 691, 715, 160, 979, 897, 614, 642, 932, 138, 1199, 706, 133, 928, 956, 99, 684, 543, 885, 823, 985, 937, 1063, 1058, 1145, 206, 824, 774, 939, 469, 1137, 560, 725, 1059, 387, 1016, 45, 241, 1128, 910, 874, 1175, 1055, 68, 204, 911, 365, 14, 964, 24, 665, 132, 924, 233, 171, 717, 853, 785, 936, 350, 513, 140, 426, 58, 783, 1148, 285, 1187, 1009, 695, 748, 33, 254, 1060, 1087, 478, 1170, 7, 942, 456, 990, 686, 718, 1209, 1105, 627, 1074, 819, 35, 137, 1078, 420, 457, 947, 1040, 292, 1169, 965, 773, 397, 71, 418, 1092, 707, 34, 1167, 652, 727, 227, 778, 191, 446, 603, 534, 411, 1026, 951, 1155, 170, 972, 617, 1025, 1102, 404, 741, 258, 1044, 738, 647, 103, 815, 505, 757, 1001, 485, 1165, 604, 1101, 210, 541, 185, 612, 635, 739, 42, 331, 1139, 770, 879, 219, 991, 753, 296, 247, 60, 531, 1020, 195, 816, 1010, 615, 17, 305, 602, 643, 799, 136, 466, 360, 359, 927, 904, 182, 248, 115, 1113, 747, 626, 189, 479, 1211, 197, 311, 493, 1065, 246, 120, 584, 700, 890, 638, 840, 655, 756, 352, 620, 699, 646, 388, 69, 555, 134, 149, 348, 1041, 414, 434, 121, 271, 46, 889, 274, 293, 681, 95, 396, 74, 958, 280, 150, 790, 600, 332, 3, 146, 165, 1050, 1153, 711, 765, 1066, 428, 215, 1115, 915, 1123, 423, 925, 1180, 976, 222, 784, 743, 515, 501, 522, 780, 39, 211, 461, 1182, 569, 312, 997, 830, 100, 272, 429, 668, 451, 205, 1166, 848, 416, 1193, 796, 409, 90, 253, 917, 758, 975, 124, 465, 1196, 127, 2, 628, 1108, 968, 301, 775, 1150, 594, 119, 381, 131, 619, 198, 1160, 868, 512, 431, 545, 59, 909, 277, 930, 558, 229, 37, 403, 410, 1195, 907, 173, 525, 810, 218, 207, 535, 1149, 1192, 113, 950, 1019, 800, 807, 1067, 480, 63, 719, 1110, 992, 452, 76, 107, 234, 52, 795, 1039, 483, 129, 1158, 304, 412, 494, 827, 22, 237, 993, 685, 832, 886, 637, 1062, 855, 463, 899, 1178, 856, 339, 393, 88, 401, 1038, 77, 803, 142, 1035, 549, 167, 1081, 802, 1082, 766, 1032, 326, 508, 376, 908, 310, 1164, 317, 1061, 148, 329, 363, 829, 786, 980, 266, 1006, 1201, 1179, 432, 828, 902, 961, 1099, 257, 817, 1047, 995, 861, 919, 220, 546, 1173, 1107, 994, 1198, 357, 105, 611, 1147, 1154, 203, 487, 384, 448, 873, 709, 474, 481, 806, 255, 1097, 1, 869, 940, 506, 284, 123, 996, 651, 85, 720, 162, 658, 291, 1057, 519, 156, 112, 1171, 400, 44, 1091, 183, 54, 364, 782, 383, 422, 184, 788, 654, 477, 850, 511, 745, 918, 151, 427, 455, 841, 349, 43, 308, 309, 509, 847, 1015, 722, 109, 454, 953, 779, 952, 366, 949, 962, 299, 346, 208, 49, 28, 649, 143, 623, 690, 755, 674, 496, 548, 1090, 116, 912, 1127, 894, 355, 36, 159, 267, 259, 1177, 289, 83, 67, 672, 529, 871, 0, 721, 273, 1138, 696, 1152, 84, 56, 645, 523, 677, 797, 347, 1106, 243, 1096, 878, 337, 687, 863, 596, 72, 472, 437, 844, 80, 379, 50, 590, 342, 903, 236, 497, 683, 564, 875, 1186, 805, 29, 462, 998, 370, 859, 663, 1124, 793, 729, 781, 977, 834, 967, 488, 1021, 846, 884, 1049, 1208, 226, 510, 194, 15, 516, 290, 591, 1094, 1079, 11, 92, 905, 1022, 23, 1112, 327, 21, 435, 764, 728, 944, 1161, 923, 368, 440, 1205, 760, 692, 250, 792, 450, 303, 751, 262, 744, 524, 216, 716, 417, 877, 689, 960, 502, 1122, 256, 460, 891, 880, 1007, 405, 1071, 710, 640, 1141, 837, 275, 385, 1191, 660, 659, 870, 5, 6, 179, 1004, 213, 804, 653, 582, 1003, 599, 391, 1176, 1163, 857, 491, 333, 527, 374, 96, 532, 141, 586, 698, 41, 959, 57, 1131, 322, 166, 441, 389, 324, 749, 1119, 244, 164, 589, 858, 550, 458, 825, 1156, 196, 946, 588, 1064, 369, 169, 152, 490, 769, 736, 553, 270, 1168, 906, 1212, 1045, 238, 540, 679, 1013, 1183, 1024, 737, 556, 814, 375, 809, 1143, 763, 754, 896, 605, 168, 664, 154, 27, 854, 12, 87, 73, 192, 1197, 1121, 1056, 1151, 622, 319, 714, 47, 704, 670, 449, 771, 776, 378, 447, 338, 580, 9, 433, 1042, 836, 86, 245, 583, 94, 595, 661, 970, 1089, 200, 314, 983, 306, 791, 538, 1018, 475, 733, 425, 251, 1028, 16, 1086, 31, 887, 818, 1188, 577, 453, 723, 673, 1084, 680, 1053, 1033, 495, 221, 872, 377, 682, 530, 328, 438, 845, 1206, 323, 486, 424, 954, 852, 568, 610, 621, 313, 316, 1174, 526, 484, 65, 104, 518, 507, 895, 669, 81, 402, 864, 53, 667, 406, 826, 866, 578, 231, 228, 345, 25, 32, 180, 442, 701, 1070, 943, 867, 1109, 430, 239, 616, 1142, 82, 240, 913, 158, 367, 934, 734, 399, 38, 114, 232, 798, 1185, 372, 648, 498, 111, 394, 585, 89, 632, 831, 1043, 1162, 1129, 1213, 920, 898, 759, 767, 639, 315, 892, 688, 671, 362, 1046, 281, 341, 566, 536, 283, 1069, 1000, 1014, 708, 443, 8, 30, 382, 1073, 561, 199, 318, 1134, 730, 539, 822, 772, 862, 321, 356, 1095, 833, 768, 571, 1036, 746, 981, 93, 415, 999, 102, 1088, 1002, 106, 358, 644, 732, 201, 955, 265, 1005, 713, 117, 575, 559, 190, 926, 1085, 1027, 489, 503, 1132, 62, 242, 300, 118, 482, 279, 294, 398, 762, 636, 989, 849, 984, 1118, 261, 1203, 662, 1202, 4, 476, 175, 79, 249, 974, 1051, 735, 64, 601, 354, 963, 407, 608, 286, 987, 336, 276, 865, 607, 609, 177, 1104, 712, 78, 801, 573, 933, 344, 562, 395, 986, 1030, 108, 570, 110, 464, 278, 882, 187, 777, 1054, 473, 1214, 20, 922, 533, 439, 373, 811, 174, 343, 839, 139, 1157, 537, 650]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2862054390058037
the save name prefix for this run is:  chkpt-ID_2862054390058037_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 563
rank avg (pred): 0.518 +- 0.002
mrr vals (pred, true): 0.000, 0.092
batch losses (mrrl, rdl): 0.0, 0.0004259525

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 763
rank avg (pred): 0.460 +- 0.291
mrr vals (pred, true): 0.195, 0.000
batch losses (mrrl, rdl): 0.0, 1.68788e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 173
rank avg (pred): 0.464 +- 0.293
mrr vals (pred, true): 0.199, 0.000
batch losses (mrrl, rdl): 0.0, 2.6391e-05

Epoch over!
epoch time: 14.772

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.473 +- 0.291
mrr vals (pred, true): 0.193, 0.000
batch losses (mrrl, rdl): 0.0, 1.87402e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 261
rank avg (pred): 0.211 +- 0.137
mrr vals (pred, true): 0.205, 0.335
batch losses (mrrl, rdl): 0.0, 0.0002176541

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 906
rank avg (pred): 0.439 +- 0.287
mrr vals (pred, true): 0.199, 0.008
batch losses (mrrl, rdl): 0.0, 0.0006158392

Epoch over!
epoch time: 14.718

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1154
rank avg (pred): 0.348 +- 0.240
mrr vals (pred, true): 0.195, 0.081
batch losses (mrrl, rdl): 0.0, 2.44267e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.449 +- 0.288
mrr vals (pred, true): 0.183, 0.000
batch losses (mrrl, rdl): 0.0, 1.10656e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.482 +- 0.286
mrr vals (pred, true): 0.180, 0.000
batch losses (mrrl, rdl): 0.0, 3e-05

Epoch over!
epoch time: 14.728

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 2
rank avg (pred): 0.333 +- 0.254
mrr vals (pred, true): 0.193, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002050569

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1120
rank avg (pred): 0.457 +- 0.284
mrr vals (pred, true): 0.181, 0.000
batch losses (mrrl, rdl): 0.0, 1.4092e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 160
rank avg (pred): 0.480 +- 0.282
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.0, 7.0125e-06

Epoch over!
epoch time: 14.72

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 366
rank avg (pred): 0.457 +- 0.282
mrr vals (pred, true): 0.181, 0.000
batch losses (mrrl, rdl): 0.0, 5.8721e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 20
rank avg (pred): 0.348 +- 0.262
mrr vals (pred, true): 0.194, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003365055

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.462 +- 0.287
mrr vals (pred, true): 0.185, 0.000
batch losses (mrrl, rdl): 0.0, 1.04311e-05

Epoch over!
epoch time: 14.892

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 549
rank avg (pred): 0.353 +- 0.269
mrr vals (pred, true): 0.193, 0.108
batch losses (mrrl, rdl): 0.0731007308, 1.20225e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.486 +- 0.215
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.63445e-05, 3.18547e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 205
rank avg (pred): 0.504 +- 0.210
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001327668, 3.12241e-05

Epoch over!
epoch time: 15.117

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 882
rank avg (pred): 0.518 +- 0.198
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001252439, 7.72181e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.489 +- 0.216
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 6.88497e-05, 3.01937e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.073 +- 0.071
mrr vals (pred, true): 0.173, 0.161
batch losses (mrrl, rdl): 0.0012934509, 0.0011801209

Epoch over!
epoch time: 15.15

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.538 +- 0.162
mrr vals (pred, true): 0.032, 0.000
batch losses (mrrl, rdl): 0.0032318977, 0.0002006459

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 302
rank avg (pred): 0.259 +- 0.177
mrr vals (pred, true): 0.045, 0.073
batch losses (mrrl, rdl): 0.0002162909, 7.02031e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 251
rank avg (pred): 0.058 +- 0.056
mrr vals (pred, true): 0.220, 0.157
batch losses (mrrl, rdl): 0.0396201201, 0.0008689322

Epoch over!
epoch time: 15.124

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 506
rank avg (pred): 0.143 +- 0.132
mrr vals (pred, true): 0.127, 0.164
batch losses (mrrl, rdl): 0.0136533715, 0.000431558

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.523 +- 0.175
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.74646e-05, 9.79632e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 250
rank avg (pred): 0.122 +- 0.128
mrr vals (pred, true): 0.240, 0.120
batch losses (mrrl, rdl): 0.1434472054, 0.0003393037

Epoch over!
epoch time: 15.117

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 772
rank avg (pred): 0.498 +- 0.187
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004714769, 4.02977e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.514 +- 0.164
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.88783e-05, 5.2091e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.475 +- 0.170
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005637704, 5.63828e-05

Epoch over!
epoch time: 15.109

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.502 +- 0.164
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001223287, 3.77513e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.476 +- 0.170
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.43158e-05, 8.86517e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.476 +- 0.179
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001301567, 6.41346e-05

Epoch over!
epoch time: 15.118

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.229 +- 0.253
mrr vals (pred, true): 0.127, 0.132
batch losses (mrrl, rdl): 0.000259272, 0.0002290681

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 84
rank avg (pred): 0.455 +- 0.206
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.000111427, 4.11339e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.476 +- 0.207
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004527818, 5.02266e-05

Epoch over!
epoch time: 15.103

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 986
rank avg (pred): 0.401 +- 0.298
mrr vals (pred, true): 0.073, 0.119
batch losses (mrrl, rdl): 0.0211529285, 0.000151328

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.363 +- 0.268
mrr vals (pred, true): 0.156, 0.127
batch losses (mrrl, rdl): 0.0083109029, 4.3833e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.430 +- 0.231
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.12329e-05, 7.5383e-05

Epoch over!
epoch time: 15.105

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 64
rank avg (pred): 0.404 +- 0.290
mrr vals (pred, true): 0.039, 0.070
batch losses (mrrl, rdl): 0.0013058331, 0.000253663

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.477 +- 0.216
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.37283e-05, 1.96487e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.483 +- 0.198
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 7.077e-07, 2.53265e-05

Epoch over!
epoch time: 14.929

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 247
rank avg (pred): 0.215 +- 0.325
mrr vals (pred, true): 0.183, 0.098
batch losses (mrrl, rdl): 0.1756315827, 4.17156e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.482 +- 0.197
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.1233e-05, 5.50489e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1059
rank avg (pred): 0.578 +- 0.428
mrr vals (pred, true): 0.208, 0.193
batch losses (mrrl, rdl): 0.0023306718, 0.0017249769

Epoch over!
epoch time: 15.12

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.467 +- 0.218
mrr vals (pred, true): 0.042, 0.009

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   35 	     0 	 0.04998 	 0.00014 	 m..s
   99 	     1 	 0.07473 	 0.00016 	 m..s
   30 	     2 	 0.04975 	 0.00016 	 m..s
    6 	     3 	 0.04699 	 0.00017 	 m..s
   27 	     4 	 0.04963 	 0.00018 	 m..s
   49 	     5 	 0.05048 	 0.00019 	 m..s
    0 	     6 	 0.04146 	 0.00019 	 m..s
   65 	     7 	 0.05126 	 0.00019 	 m..s
   23 	     8 	 0.04945 	 0.00019 	 m..s
    9 	     9 	 0.04899 	 0.00020 	 m..s
   26 	    10 	 0.04962 	 0.00021 	 m..s
    5 	    11 	 0.04553 	 0.00021 	 m..s
   29 	    12 	 0.04973 	 0.00021 	 m..s
    7 	    13 	 0.04783 	 0.00021 	 m..s
   53 	    14 	 0.05062 	 0.00022 	 m..s
   95 	    15 	 0.07410 	 0.00022 	 m..s
   21 	    16 	 0.04943 	 0.00023 	 m..s
   45 	    17 	 0.05034 	 0.00023 	 m..s
   19 	    18 	 0.04937 	 0.00023 	 m..s
   82 	    19 	 0.05870 	 0.00023 	 m..s
   70 	    20 	 0.05285 	 0.00023 	 m..s
   24 	    21 	 0.04952 	 0.00024 	 m..s
   68 	    22 	 0.05210 	 0.00024 	 m..s
    3 	    23 	 0.04469 	 0.00024 	 m..s
   67 	    24 	 0.05184 	 0.00024 	 m..s
   43 	    25 	 0.05029 	 0.00025 	 m..s
    2 	    26 	 0.04405 	 0.00025 	 m..s
   69 	    27 	 0.05245 	 0.00025 	 m..s
   47 	    28 	 0.05036 	 0.00026 	 m..s
   58 	    29 	 0.05093 	 0.00026 	 m..s
   71 	    30 	 0.05342 	 0.00026 	 m..s
   60 	    31 	 0.05109 	 0.00027 	 m..s
   38 	    32 	 0.05016 	 0.00027 	 m..s
   56 	    33 	 0.05072 	 0.00028 	 m..s
   37 	    34 	 0.05014 	 0.00028 	 m..s
   16 	    35 	 0.04926 	 0.00028 	 m..s
   39 	    36 	 0.05018 	 0.00028 	 m..s
   50 	    37 	 0.05049 	 0.00028 	 m..s
    4 	    38 	 0.04470 	 0.00028 	 m..s
   72 	    39 	 0.05342 	 0.00029 	 m..s
   73 	    40 	 0.05349 	 0.00029 	 m..s
   52 	    41 	 0.05058 	 0.00029 	 m..s
   57 	    42 	 0.05092 	 0.00029 	 m..s
   81 	    43 	 0.05742 	 0.00029 	 m..s
   15 	    44 	 0.04925 	 0.00030 	 m..s
   28 	    45 	 0.04965 	 0.00030 	 m..s
   83 	    46 	 0.05873 	 0.00030 	 m..s
   32 	    47 	 0.04984 	 0.00030 	 m..s
   63 	    48 	 0.05118 	 0.00031 	 m..s
   74 	    49 	 0.05357 	 0.00031 	 m..s
   18 	    50 	 0.04932 	 0.00031 	 m..s
   41 	    51 	 0.05024 	 0.00032 	 m..s
   62 	    52 	 0.05111 	 0.00033 	 m..s
   55 	    53 	 0.05070 	 0.00034 	 m..s
   77 	    54 	 0.05476 	 0.00035 	 m..s
   34 	    55 	 0.04997 	 0.00035 	 m..s
  104 	    56 	 0.09169 	 0.00036 	 m..s
   76 	    57 	 0.05413 	 0.00036 	 m..s
   14 	    58 	 0.04921 	 0.00037 	 m..s
   80 	    59 	 0.05682 	 0.00037 	 m..s
   42 	    60 	 0.05027 	 0.00038 	 m..s
   20 	    61 	 0.04942 	 0.00039 	 m..s
   48 	    62 	 0.05045 	 0.00041 	 m..s
   13 	    63 	 0.04913 	 0.00041 	 m..s
   54 	    64 	 0.05070 	 0.00042 	 m..s
   59 	    65 	 0.05094 	 0.00043 	 m..s
    8 	    66 	 0.04834 	 0.00044 	 m..s
   46 	    67 	 0.05035 	 0.00045 	 m..s
   51 	    68 	 0.05051 	 0.00045 	 m..s
   11 	    69 	 0.04907 	 0.00046 	 m..s
   66 	    70 	 0.05173 	 0.00054 	 m..s
   61 	    71 	 0.05110 	 0.00055 	 m..s
   17 	    72 	 0.04932 	 0.00066 	 m..s
   10 	    73 	 0.04901 	 0.00070 	 m..s
   25 	    74 	 0.04961 	 0.00075 	 m..s
   64 	    75 	 0.05124 	 0.00076 	 m..s
   44 	    76 	 0.05034 	 0.00082 	 m..s
   31 	    77 	 0.04976 	 0.00085 	 m..s
   12 	    78 	 0.04913 	 0.00089 	 m..s
   75 	    79 	 0.05365 	 0.00090 	 m..s
   22 	    80 	 0.04944 	 0.00118 	 m..s
   40 	    81 	 0.05023 	 0.00294 	 m..s
   33 	    82 	 0.04988 	 0.00637 	 m..s
   36 	    83 	 0.05013 	 0.00842 	 m..s
    1 	    84 	 0.04249 	 0.00890 	 m..s
   97 	    85 	 0.07426 	 0.05135 	 ~...
   86 	    86 	 0.06747 	 0.05590 	 ~...
   85 	    87 	 0.06296 	 0.06354 	 ~...
   87 	    88 	 0.07392 	 0.07478 	 ~...
  101 	    89 	 0.07680 	 0.07916 	 ~...
   79 	    90 	 0.05668 	 0.07964 	 ~...
   87 	    91 	 0.07392 	 0.08475 	 ~...
  103 	    92 	 0.08215 	 0.08786 	 ~...
   98 	    93 	 0.07452 	 0.09246 	 ~...
   87 	    94 	 0.07392 	 0.09285 	 ~...
   87 	    95 	 0.07392 	 0.09332 	 ~...
   78 	    96 	 0.05635 	 0.09343 	 m..s
   96 	    97 	 0.07413 	 0.09379 	 ~...
   87 	    98 	 0.07392 	 0.09528 	 ~...
   87 	    99 	 0.07392 	 0.09738 	 ~...
   84 	   100 	 0.06257 	 0.10394 	 m..s
   87 	   101 	 0.07392 	 0.10659 	 m..s
  105 	   102 	 0.09230 	 0.10768 	 ~...
   87 	   103 	 0.07392 	 0.10918 	 m..s
  102 	   104 	 0.07731 	 0.11293 	 m..s
  100 	   105 	 0.07584 	 0.11326 	 m..s
  108 	   106 	 0.11893 	 0.12051 	 ~...
  107 	   107 	 0.10271 	 0.12975 	 ~...
  109 	   108 	 0.11939 	 0.13102 	 ~...
  106 	   109 	 0.10168 	 0.13293 	 m..s
  111 	   110 	 0.15541 	 0.13868 	 ~...
  112 	   111 	 0.18863 	 0.14992 	 m..s
  110 	   112 	 0.14260 	 0.15039 	 ~...
  113 	   113 	 0.22744 	 0.17022 	 m..s
  116 	   114 	 0.23505 	 0.20159 	 m..s
  117 	   115 	 0.23582 	 0.23463 	 ~...
  115 	   116 	 0.23426 	 0.26787 	 m..s
  118 	   117 	 0.25306 	 0.27877 	 ~...
  114 	   118 	 0.22784 	 0.31471 	 m..s
  119 	   119 	 0.26226 	 0.31478 	 m..s
  120 	   120 	 0.26340 	 0.35955 	 m..s
==========================================
r_mrr = 0.9242523908615112
r2_mrr = 0.6216121912002563
spearmanr_mrr@5 = 0.9066593050956726
spearmanr_mrr@10 = 0.858317494392395
spearmanr_mrr@50 = 0.926824152469635
spearmanr_mrr@100 = 0.9407952427864075
spearmanr_mrr@All = 0.9437097311019897
==========================================
test time: 0.461
Done Testing dataset DBpedia50
total time taken: 230.72481203079224
training time taken: 225.29571771621704
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.9243)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.6216)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9067)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8583)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9268)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9408)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9437)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.4827708585125947}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 3734228699327612
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1124, 1039, 1162, 789, 659, 331, 984, 73, 991, 424, 313, 699, 399, 693, 988, 854, 1180, 583, 810, 241, 1131, 392, 879, 429, 362, 436, 1120, 1170, 1002, 498, 1202, 556, 599, 622, 236, 132, 630, 144, 1058, 323, 1074, 637, 408, 343, 896, 1038, 1052, 1005, 49, 844, 989, 378, 41, 434, 402, 94, 2, 356, 178, 613, 96, 1111, 517, 612, 518, 750, 1025, 671, 963, 45, 825, 771, 228, 882, 1021, 481, 28, 1193, 851, 916, 186, 880, 17, 698, 177, 85, 908, 315, 1018, 227, 371, 603, 775, 925, 194, 629, 365, 314, 296, 510, 48, 224, 105, 652, 935, 1023, 1033, 254, 232, 162, 419, 496, 1158, 1119, 61, 868, 134, 508, 700, 970, 618]
valid_ids (0): []
train_ids (1094): [1159, 417, 588, 468, 1118, 864, 731, 898, 772, 62, 67, 195, 871, 153, 798, 115, 119, 631, 244, 758, 274, 109, 621, 1097, 752, 1187, 780, 1214, 322, 739, 861, 1020, 423, 180, 34, 449, 648, 95, 470, 1044, 422, 1081, 139, 40, 509, 1157, 368, 151, 867, 168, 803, 413, 1104, 1205, 900, 366, 51, 68, 142, 229, 1008, 996, 530, 372, 179, 465, 310, 866, 678, 7, 824, 978, 1129, 494, 596, 522, 924, 972, 1156, 158, 110, 1026, 1101, 968, 176, 1206, 552, 12, 1079, 403, 802, 192, 607, 560, 728, 708, 947, 376, 815, 488, 27, 1169, 982, 389, 13, 764, 1068, 735, 471, 906, 873, 350, 456, 369, 1146, 975, 383, 619, 87, 609, 933, 171, 749, 328, 623, 409, 899, 240, 977, 1004, 674, 432, 938, 462, 535, 561, 1078, 155, 1096, 672, 837, 1168, 388, 922, 1128, 527, 1147, 285, 483, 99, 710, 594, 687, 580, 395, 295, 574, 292, 416, 1212, 251, 831, 234, 445, 796, 1133, 643, 71, 357, 847, 966, 932, 415, 452, 971, 732, 628, 1108, 602, 55, 1088, 394, 892, 10, 104, 913, 777, 1053, 526, 493, 321, 474, 489, 33, 1123, 579, 799, 1196, 1048, 459, 1009, 396, 967, 635, 894, 601, 44, 1042, 636, 420, 743, 499, 39, 785, 781, 288, 919, 543, 238, 980, 454, 725, 30, 114, 691, 277, 1019, 88, 248, 734, 647, 1164, 307, 828, 849, 450, 1056, 282, 1063, 1012, 696, 231, 675, 391, 1114, 808, 839, 990, 656, 11, 216, 272, 746, 354, 537, 91, 1036, 720, 727, 976, 787, 220, 495, 877, 206, 221, 412, 129, 327, 742, 472, 941, 89, 853, 353, 212, 547, 414, 278, 170, 548, 878, 1029, 198, 1076, 249, 640, 1102, 421, 1184, 401, 337, 1177, 956, 156, 318, 146, 512, 615, 1186, 715, 358, 386, 726, 721, 102, 223, 918, 529, 289, 143, 137, 536, 1207, 776, 895, 565, 239, 1121, 901, 1110, 3, 70, 111, 161, 576, 539, 1060, 345, 103, 1087, 1103, 572, 410, 586, 1150, 848, 347, 373, 437, 654, 397, 860, 805, 65, 50, 52, 1043, 163, 501, 148, 18, 165, 598, 273, 738, 1024, 657, 890, 411, 36, 820, 202, 1185, 4, 931, 205, 426, 768, 407, 1151, 923, 477, 1051, 390, 1034, 644, 6, 773, 9, 814, 874, 641, 507, 1073, 486, 279, 24, 910, 1006, 605, 361, 237, 521, 519, 766, 538, 1011, 965, 500, 1197, 127, 697, 181, 393, 545, 306, 993, 214, 107, 428, 505, 182, 729, 639, 430, 222, 1194, 1139, 642, 166, 688, 929, 242, 571, 569, 747, 582, 1213, 1199, 845, 774, 490, 1178, 942, 979, 1155, 324, 84, 595, 435, 66, 404, 934, 418, 191, 669, 463, 464, 998, 460, 377, 485, 1149, 722, 902, 308, 1046, 75, 573, 816, 524, 287, 597, 1064, 567, 949, 233, 438, 440, 250, 606, 193, 141, 1109, 427, 384, 855, 779, 1138, 341, 575, 1145, 1045, 937, 14, 112, 21, 487, 125, 1191, 994, 200, 138, 620, 349, 246, 670, 1144, 842, 1166, 568, 885, 1135, 1099, 564, 346, 562, 259, 1122, 56, 443, 338, 632, 317, 1037, 329, 484, 64, 650, 891, 786, 267, 225, 664, 829, 447, 754, 126, 905, 159, 823, 255, 1093, 625, 253, 16, 196, 1134, 912, 907, 930, 352, 888, 340, 86, 1090, 304, 767, 515, 325, 442, 265, 719, 686, 549, 1091, 190, 266, 275, 441, 1100, 917, 903, 1015, 865, 1094, 709, 600, 887, 1075, 841, 291, 1211, 832, 293, 332, 585, 541, 1116, 380, 32, 1016, 199, 1204, 525, 461, 281, 210, 467, 319, 724, 1141, 995, 1201, 544, 843, 433, 184, 744, 260, 948, 1203, 15, 681, 634, 116, 969, 733, 869, 791, 1027, 26, 852, 1165, 666, 90, 268, 245, 627, 473, 359, 577, 1028, 286, 876, 578, 794, 817, 59, 677, 1179, 339, 197, 624, 532, 957, 793, 150, 348, 638, 1054, 174, 381, 961, 145, 203, 718, 626, 661, 952, 312, 492, 1065, 1017, 633, 122, 884, 1085, 911, 542, 1126, 958, 1160, 782, 1195, 1210, 333, 737, 1080, 1092, 658, 169, 448, 1112, 795, 1106, 1163, 665, 757, 128, 962, 858, 207, 875, 835, 673, 425, 270, 1071, 469, 262, 247, 189, 1095, 106, 1188, 534, 97, 491, 503, 8, 904, 264, 797, 320, 859, 1001, 706, 130, 992, 936, 133, 1132, 1148, 589, 283, 19, 256, 723, 550, 1062, 187, 1125, 342, 608, 431, 311, 502, 58, 1003, 497, 1030, 451, 563, 294, 444, 959, 926, 98, 551, 712, 1041, 651, 778, 1113, 570, 707, 1183, 1209, 360, 72, 1175, 881, 684, 334, 1098, 974, 857, 1067, 759, 77, 705, 883, 63, 985, 685, 164, 1172, 1161, 692, 140, 755, 363, 1031, 690, 80, 172, 748, 617, 668, 57, 1049, 783, 31, 973, 475, 101, 188, 889, 351, 1072, 846, 587, 204, 297, 983, 1105, 1032, 316, 740, 124, 20, 921, 1153, 1069, 955, 379, 950, 834, 915, 893, 559, 1040, 219, 711, 42, 154, 25, 761, 717, 986, 714, 1171, 897, 610, 528, 1189, 482, 546, 400, 53, 850, 683, 1115, 1140, 792, 804, 997, 370, 581, 1136, 964, 1089, 590, 1154, 213, 82, 680, 1057, 953, 649, 784, 215, 29, 476, 653, 655, 1000, 290, 299, 682, 614, 466, 330, 593, 252, 157, 1050, 263, 821, 160, 78, 769, 611, 702, 217, 523, 108, 1200, 1082, 1083, 382, 121, 763, 201, 756, 591, 1137, 558, 838, 694, 47, 123, 1198, 800, 480, 939, 822, 208, 807, 504, 173, 584, 1066, 704, 1077, 147, 960, 136, 663, 269, 271, 981, 943, 37, 790, 553, 1035, 439, 284, 554, 385, 336, 801, 1176, 1, 741, 243, 1181, 1014, 730, 1143, 557, 185, 344, 1022, 940, 211, 1055, 826, 1007, 944, 149, 120, 819, 387, 300, 1142, 375, 516, 230, 1084, 280, 679, 809, 646, 592, 836, 1047, 533, 736, 46, 954, 676, 43, 226, 856, 1174, 478, 667, 261, 81, 60, 92, 302, 833, 513, 22, 751, 69, 364, 367, 1086, 209, 753, 540, 1117, 406, 1173, 616, 999, 830, 788, 945, 862, 74, 765, 1182, 301, 695, 812, 303, 1127, 131, 1107, 83, 1013, 914, 1010, 1167, 1208, 920, 827, 117, 305, 453, 660, 76, 183, 1190, 946, 806, 520, 928, 840, 716, 872, 506, 818, 175, 298, 927, 35, 762, 604, 811, 886, 326, 1059, 555, 0, 93, 100, 335, 1152, 258, 566, 398, 79, 405, 38, 458, 257, 446, 167, 703, 235, 662, 1070, 374, 455, 870, 135, 951, 1192, 152, 457, 23, 745, 863, 514, 909, 701, 276, 5, 813, 1061, 987, 113, 760, 355, 218, 531, 479, 1130, 713, 770, 118, 511, 309, 645, 54, 689]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5620701021019421
the save name prefix for this run is:  chkpt-ID_5620701021019421_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 616
rank avg (pred): 0.552 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.000268893

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 360
rank avg (pred): 0.474 +- 0.302
mrr vals (pred, true): 0.018, 0.000
batch losses (mrrl, rdl): 0.0, 6.4423e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 863
rank avg (pred): 0.459 +- 0.297
mrr vals (pred, true): 0.009, 0.000
batch losses (mrrl, rdl): 0.0, 1.95975e-05

Epoch over!
epoch time: 14.773

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1088
rank avg (pred): 0.430 +- 0.312
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0, 1.7827e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 147
rank avg (pred): 0.467 +- 0.283
mrr vals (pred, true): 0.004, 0.000
batch losses (mrrl, rdl): 0.0, 5.0751e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.507 +- 0.279
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 3.66285e-05

Epoch over!
epoch time: 14.828

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 66
rank avg (pred): 0.355 +- 0.255
mrr vals (pred, true): 0.009, 0.113
batch losses (mrrl, rdl): 0.0, 7.78951e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.463 +- 0.309
mrr vals (pred, true): 0.006, 0.001
batch losses (mrrl, rdl): 0.0, 2.33131e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.476 +- 0.294
mrr vals (pred, true): 0.006, 0.000
batch losses (mrrl, rdl): 0.0, 1.2066e-05

Epoch over!
epoch time: 14.919

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 479
rank avg (pred): 0.435 +- 0.297
mrr vals (pred, true): 0.015, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001081775

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 545
rank avg (pred): 0.340 +- 0.269
mrr vals (pred, true): 0.034, 0.108
batch losses (mrrl, rdl): 0.0, 2.04275e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 127
rank avg (pred): 0.444 +- 0.293
mrr vals (pred, true): 0.007, 0.000
batch losses (mrrl, rdl): 0.0, 1.37197e-05

Epoch over!
epoch time: 14.713

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 949
rank avg (pred): 0.532 +- 0.278
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 2.44143e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 417
rank avg (pred): 0.481 +- 0.276
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 9.4593e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 44
rank avg (pred): 0.322 +- 0.304
mrr vals (pred, true): 0.024, 0.078
batch losses (mrrl, rdl): 0.0, 3.29922e-05

Epoch over!
epoch time: 14.715

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 453
rank avg (pred): 0.487 +- 0.277
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0242949501, 1.43779e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 614
rank avg (pred): 0.450 +- 0.300
mrr vals (pred, true): 0.072, 0.000
batch losses (mrrl, rdl): 0.0046874499, 1.25525e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.515 +- 0.215
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 8.21141e-05, 8.13397e-05

Epoch over!
epoch time: 14.961

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 39
rank avg (pred): 0.413 +- 0.281
mrr vals (pred, true): 0.085, 0.080
batch losses (mrrl, rdl): 0.012552212, 0.0002428062

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1070
rank avg (pred): 0.348 +- 0.301
mrr vals (pred, true): 0.126, 0.171
batch losses (mrrl, rdl): 0.019883316, 3.53334e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.485 +- 0.310
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0315516219, 1.75379e-05

Epoch over!
epoch time: 14.947

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.400 +- 0.270
mrr vals (pred, true): 0.106, 0.113
batch losses (mrrl, rdl): 0.0004309762, 0.000256782

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 848
rank avg (pred): 0.500 +- 0.210
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 3.451e-07, 3.92683e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1043
rank avg (pred): 0.479 +- 0.214
mrr vals (pred, true): 0.050, 0.008
batch losses (mrrl, rdl): 2.1355e-06, 3.76244e-05

Epoch over!
epoch time: 14.941

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.494 +- 0.199
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 8.7727e-05, 8.13752e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 718
rank avg (pred): 0.501 +- 0.190
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 4.1847e-05, 5.63939e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.319 +- 0.244
mrr vals (pred, true): 0.279, 0.213
batch losses (mrrl, rdl): 0.0430331193, 0.0007867564

Epoch over!
epoch time: 14.925

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.441 +- 0.241
mrr vals (pred, true): 0.051, 0.097
batch losses (mrrl, rdl): 1.44591e-05, 0.0005136349

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.211 +- 0.163
mrr vals (pred, true): 0.149, 0.163
batch losses (mrrl, rdl): 0.0019803548, 0.0001859692

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 983
rank avg (pred): 0.309 +- 0.202
mrr vals (pred, true): 0.093, 0.077
batch losses (mrrl, rdl): 0.0185082797, 4.98672e-05

Epoch over!
epoch time: 14.946

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1110
rank avg (pred): 0.505 +- 0.176
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001502482, 3.32107e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.474 +- 0.222
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.262e-07, 3.68804e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 29
rank avg (pred): 0.459 +- 0.231
mrr vals (pred, true): 0.051, 0.114
batch losses (mrrl, rdl): 0.0393092483, 0.0005641585

Epoch over!
epoch time: 14.935

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 911
rank avg (pred): 0.472 +- 0.217
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 3.735e-06, 3.38713e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 710
rank avg (pred): 0.471 +- 0.215
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.966e-07, 6.31827e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 301
rank avg (pred): 0.324 +- 0.230
mrr vals (pred, true): 0.105, 0.094
batch losses (mrrl, rdl): 0.0304924659, 4.28461e-05

Epoch over!
epoch time: 15.043

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.469 +- 0.213
mrr vals (pred, true): 0.050, 0.097
batch losses (mrrl, rdl): 1.7348e-06, 0.0006897351

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.505 +- 0.417
mrr vals (pred, true): 0.202, 0.239
batch losses (mrrl, rdl): 0.0137116658, 0.0014840151

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 145
rank avg (pred): 0.497 +- 0.175
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003601655, 6.40142e-05

Epoch over!
epoch time: 15.016

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 702
rank avg (pred): 0.504 +- 0.171
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 7.36693e-05, 7.18363e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.797 +- 0.384
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.0103808828, 0.0001634332

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.531 +- 0.148
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001695463, 0.0001288233

Epoch over!
epoch time: 14.917

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.504 +- 0.171
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0001099976, 8.04511e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.524 +- 0.136
mrr vals (pred, true): 0.026, 0.001
batch losses (mrrl, rdl): 0.0056234905, 0.0001116035

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 90
rank avg (pred): 0.512 +- 0.140
mrr vals (pred, true): 0.032, 0.001
batch losses (mrrl, rdl): 0.0033001932, 0.0001682517

Epoch over!
epoch time: 14.899

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.480 +- 0.186
mrr vals (pred, true): 0.050, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   95 	     0 	 0.07156 	 0.00014 	 m..s
   56 	     1 	 0.05038 	 0.00015 	 m..s
   22 	     2 	 0.04835 	 0.00016 	 m..s
   36 	     3 	 0.04924 	 0.00016 	 m..s
   88 	     4 	 0.06052 	 0.00016 	 m..s
   11 	     5 	 0.04738 	 0.00017 	 m..s
   86 	     6 	 0.05945 	 0.00017 	 m..s
   41 	     7 	 0.04945 	 0.00017 	 m..s
    5 	     8 	 0.04286 	 0.00017 	 m..s
   26 	     9 	 0.04858 	 0.00018 	 m..s
   73 	    10 	 0.05493 	 0.00019 	 m..s
   34 	    11 	 0.04903 	 0.00019 	 m..s
   50 	    12 	 0.04983 	 0.00020 	 m..s
   24 	    13 	 0.04847 	 0.00020 	 m..s
   70 	    14 	 0.05370 	 0.00020 	 m..s
   89 	    15 	 0.06056 	 0.00020 	 m..s
   37 	    16 	 0.04931 	 0.00020 	 m..s
   17 	    17 	 0.04812 	 0.00020 	 m..s
   42 	    18 	 0.04948 	 0.00021 	 m..s
   66 	    19 	 0.05213 	 0.00021 	 m..s
   15 	    20 	 0.04806 	 0.00021 	 m..s
   55 	    21 	 0.05022 	 0.00022 	 m..s
   48 	    22 	 0.04975 	 0.00022 	 m..s
   61 	    23 	 0.05110 	 0.00022 	 m..s
   20 	    24 	 0.04833 	 0.00022 	 m..s
   33 	    25 	 0.04900 	 0.00022 	 m..s
   29 	    26 	 0.04867 	 0.00022 	 m..s
   59 	    27 	 0.05071 	 0.00022 	 m..s
   62 	    28 	 0.05124 	 0.00023 	 m..s
   27 	    29 	 0.04860 	 0.00023 	 m..s
   39 	    30 	 0.04938 	 0.00024 	 m..s
   60 	    31 	 0.05077 	 0.00024 	 m..s
   77 	    32 	 0.05647 	 0.00024 	 m..s
   13 	    33 	 0.04778 	 0.00025 	 m..s
   94 	    34 	 0.07111 	 0.00025 	 m..s
   21 	    35 	 0.04834 	 0.00026 	 m..s
   63 	    36 	 0.05128 	 0.00027 	 m..s
    4 	    37 	 0.04250 	 0.00028 	 m..s
   23 	    38 	 0.04838 	 0.00028 	 m..s
    3 	    39 	 0.04246 	 0.00028 	 m..s
   43 	    40 	 0.04954 	 0.00029 	 m..s
   18 	    41 	 0.04820 	 0.00029 	 m..s
    8 	    42 	 0.04519 	 0.00029 	 m..s
   25 	    43 	 0.04851 	 0.00030 	 m..s
    9 	    44 	 0.04569 	 0.00030 	 m..s
   47 	    45 	 0.04970 	 0.00032 	 m..s
   19 	    46 	 0.04821 	 0.00033 	 m..s
    2 	    47 	 0.04189 	 0.00035 	 m..s
   75 	    48 	 0.05578 	 0.00036 	 m..s
   32 	    49 	 0.04888 	 0.00036 	 m..s
   31 	    50 	 0.04883 	 0.00036 	 m..s
    6 	    51 	 0.04308 	 0.00036 	 m..s
   71 	    52 	 0.05463 	 0.00036 	 m..s
   46 	    53 	 0.04959 	 0.00037 	 m..s
   69 	    54 	 0.05317 	 0.00037 	 m..s
   14 	    55 	 0.04803 	 0.00038 	 m..s
   45 	    56 	 0.04958 	 0.00038 	 m..s
   96 	    57 	 0.07345 	 0.00038 	 m..s
   90 	    58 	 0.06090 	 0.00040 	 m..s
   67 	    59 	 0.05289 	 0.00040 	 m..s
   44 	    60 	 0.04956 	 0.00041 	 m..s
   10 	    61 	 0.04597 	 0.00042 	 m..s
   65 	    62 	 0.05200 	 0.00043 	 m..s
   91 	    63 	 0.06113 	 0.00044 	 m..s
    1 	    64 	 0.04162 	 0.00044 	 m..s
   54 	    65 	 0.05016 	 0.00045 	 m..s
   87 	    66 	 0.05994 	 0.00045 	 m..s
   38 	    67 	 0.04937 	 0.00046 	 m..s
   84 	    68 	 0.05907 	 0.00047 	 m..s
   53 	    69 	 0.05014 	 0.00048 	 m..s
   28 	    70 	 0.04862 	 0.00048 	 m..s
   35 	    71 	 0.04921 	 0.00049 	 m..s
   82 	    72 	 0.05794 	 0.00050 	 m..s
    0 	    73 	 0.04150 	 0.00050 	 m..s
   74 	    74 	 0.05520 	 0.00050 	 m..s
   16 	    75 	 0.04806 	 0.00051 	 m..s
   93 	    76 	 0.06338 	 0.00054 	 m..s
   76 	    77 	 0.05638 	 0.00057 	 m..s
   12 	    78 	 0.04747 	 0.00060 	 m..s
   30 	    79 	 0.04883 	 0.00070 	 m..s
   80 	    80 	 0.05767 	 0.00075 	 m..s
   58 	    81 	 0.05057 	 0.00088 	 m..s
   52 	    82 	 0.05005 	 0.00089 	 m..s
   64 	    83 	 0.05136 	 0.00093 	 m..s
   51 	    84 	 0.04988 	 0.00104 	 m..s
   85 	    85 	 0.05908 	 0.00112 	 m..s
    7 	    86 	 0.04321 	 0.00115 	 m..s
   49 	    87 	 0.04980 	 0.00144 	 m..s
   92 	    88 	 0.06156 	 0.00161 	 m..s
   40 	    89 	 0.04940 	 0.00220 	 m..s
   57 	    90 	 0.05039 	 0.00574 	 m..s
   97 	    91 	 0.07413 	 0.01589 	 m..s
   83 	    92 	 0.05844 	 0.04720 	 ~...
   68 	    93 	 0.05312 	 0.05135 	 ~...
  101 	    94 	 0.08477 	 0.07237 	 ~...
   81 	    95 	 0.05776 	 0.07501 	 ~...
  105 	    96 	 0.09497 	 0.07825 	 ~...
   99 	    97 	 0.08098 	 0.07886 	 ~...
   98 	    98 	 0.07732 	 0.07899 	 ~...
  110 	    99 	 0.12529 	 0.08100 	 m..s
   79 	   100 	 0.05723 	 0.08633 	 ~...
  108 	   101 	 0.10017 	 0.09366 	 ~...
   72 	   102 	 0.05489 	 0.09858 	 m..s
   78 	   103 	 0.05650 	 0.10629 	 m..s
  100 	   104 	 0.08455 	 0.11601 	 m..s
  102 	   105 	 0.08568 	 0.12018 	 m..s
  106 	   106 	 0.09600 	 0.12530 	 ~...
  111 	   107 	 0.12723 	 0.12667 	 ~...
  114 	   108 	 0.14997 	 0.12917 	 ~...
  113 	   109 	 0.14832 	 0.13228 	 ~...
  103 	   110 	 0.08790 	 0.13487 	 m..s
  109 	   111 	 0.11711 	 0.14273 	 ~...
  117 	   112 	 0.17947 	 0.14691 	 m..s
  107 	   113 	 0.09608 	 0.14850 	 m..s
  104 	   114 	 0.09284 	 0.15510 	 m..s
  115 	   115 	 0.16912 	 0.17200 	 ~...
  112 	   116 	 0.12932 	 0.18010 	 m..s
  116 	   117 	 0.17328 	 0.23463 	 m..s
  119 	   118 	 0.22231 	 0.26108 	 m..s
  120 	   119 	 0.24459 	 0.29684 	 m..s
  118 	   120 	 0.21833 	 0.31828 	 m..s
==========================================
r_mrr = 0.9180947542190552
r2_mrr = 0.4561224579811096
spearmanr_mrr@5 = 0.9383068680763245
spearmanr_mrr@10 = 0.9657485485076904
spearmanr_mrr@50 = 0.9568532705307007
spearmanr_mrr@100 = 0.9687729477882385
spearmanr_mrr@All = 0.9696776270866394
==========================================
test time: 0.445
Done Testing dataset DBpedia50
total time taken: 229.2822790145874
training time taken: 223.93406915664673
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.9181)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4561)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9383)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9657)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9569)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9688)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9697)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.5666754157991818}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 3189340799894055
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [476, 725, 279, 1212, 672, 468, 136, 87, 1139, 798, 1012, 718, 125, 346, 963, 274, 998, 618, 642, 801, 1123, 460, 708, 695, 740, 448, 86, 855, 1172, 39, 363, 861, 748, 1192, 171, 179, 189, 555, 329, 1027, 903, 217, 784, 742, 882, 233, 670, 1167, 1190, 473, 957, 680, 819, 284, 868, 23, 428, 612, 706, 240, 653, 914, 886, 175, 1135, 1104, 387, 1060, 771, 25, 324, 447, 785, 373, 185, 561, 434, 581, 431, 402, 27, 1151, 57, 536, 354, 501, 248, 783, 715, 107, 262, 1176, 1005, 1021, 101, 797, 49, 24, 730, 55, 1069, 1092, 283, 120, 551, 439, 1018, 356, 304, 121, 1020, 609, 961, 1195, 417, 401, 864, 863, 946, 529, 243]
valid_ids (0): []
train_ids (1094): [826, 344, 176, 955, 908, 590, 1137, 498, 711, 6, 635, 809, 143, 347, 647, 1130, 1207, 518, 859, 1025, 199, 168, 699, 106, 113, 471, 1083, 358, 104, 996, 412, 749, 1096, 720, 629, 66, 948, 641, 1174, 632, 1093, 381, 605, 133, 1183, 654, 818, 896, 28, 1067, 904, 312, 254, 218, 362, 227, 763, 893, 571, 221, 508, 958, 61, 1076, 466, 1082, 506, 1197, 816, 857, 765, 319, 562, 339, 1110, 997, 1024, 63, 511, 799, 585, 366, 731, 315, 210, 391, 70, 42, 625, 902, 991, 1211, 187, 167, 999, 737, 17, 340, 1208, 1053, 602, 645, 844, 65, 219, 122, 482, 1097, 1149, 169, 601, 463, 103, 913, 787, 456, 1187, 1014, 418, 1054, 1180, 172, 823, 528, 1048, 744, 349, 916, 207, 942, 323, 769, 833, 548, 1133, 1152, 1122, 390, 941, 901, 357, 490, 791, 1203, 845, 1040, 286, 541, 228, 622, 159, 610, 132, 1072, 435, 154, 52, 747, 659, 1049, 752, 1147, 751, 735, 922, 258, 364, 375, 962, 422, 293, 1154, 15, 608, 736, 297, 194, 1168, 396, 318, 420, 502, 865, 303, 881, 974, 778, 944, 986, 514, 37, 202, 67, 796, 455, 260, 626, 1036, 91, 496, 152, 880, 1199, 290, 762, 807, 971, 1171, 539, 794, 686, 29, 252, 690, 604, 178, 987, 732, 1071, 1042, 600, 268, 734, 1163, 838, 525, 559, 643, 126, 45, 560, 296, 805, 757, 385, 156, 879, 97, 712, 3, 369, 1001, 117, 166, 513, 36, 99, 376, 619, 848, 546, 73, 1050, 636, 872, 777, 361, 1189, 814, 1131, 575, 205, 570, 867, 452, 945, 332, 827, 287, 370, 979, 11, 704, 328, 223, 151, 313, 789, 956, 874, 241, 321, 83, 884, 69, 728, 1188, 384, 994, 627, 325, 800, 289, 840, 534, 266, 353, 96, 831, 374, 410, 1185, 1162, 445, 912, 141, 267, 270, 90, 1124, 13, 939, 419, 877, 593, 147, 760, 300, 19, 389, 1008, 889, 1209, 582, 953, 427, 1143, 1114, 1205, 758, 1175, 682, 1148, 249, 1103, 140, 825, 1144, 658, 395, 229, 444, 553, 239, 157, 54, 1078, 921, 1173, 927, 907, 691, 583, 843, 1140, 115, 367, 441, 992, 59, 12, 661, 183, 576, 500, 77, 1105, 433, 1142, 246, 713, 954, 111, 1030, 892, 257, 371, 208, 808, 359, 477, 1000, 196, 407, 421, 1108, 469, 1125, 1099, 72, 173, 674, 568, 523, 929, 1145, 1007, 31, 355, 786, 662, 432, 510, 965, 1102, 1010, 898, 360, 38, 972, 937, 835, 984, 891, 550, 545, 648, 552, 723, 191, 184, 85, 453, 1158, 1153, 348, 307, 563, 968, 917, 507, 1029, 212, 928, 493, 1126, 709, 793, 84, 1136, 1196, 253, 1128, 557, 309, 821, 727, 973, 540, 135, 119, 1134, 276, 398, 621, 1041, 533, 594, 337, 926, 788, 430, 35, 377, 530, 679, 237, 842, 930, 705, 683, 660, 1178, 1051, 822, 129, 1061, 726, 782, 403, 484, 1085, 767, 1002, 675, 1202, 301, 1118, 1022, 209, 465, 888, 657, 1016, 853, 564, 897, 651, 124, 885, 467, 1094, 1166, 1015, 98, 1095, 155, 62, 776, 1198, 516, 288, 588, 216, 676, 1181, 775, 646, 255, 900, 1013, 397, 764, 1193, 829, 768, 932, 1056, 515, 556, 160, 1032, 386, 399, 162, 614, 920, 995, 697, 1116, 481, 830, 480, 538, 224, 1037, 198, 193, 285, 81, 1100, 149, 137, 1011, 1146, 624, 543, 21, 940, 895, 710, 755, 203, 74, 770, 620, 446, 580, 128, 1023, 351, 589, 305, 565, 860, 392, 273, 1191, 1017, 1073, 694, 925, 0, 1112, 792, 41, 978, 404, 474, 628, 14, 673, 164, 192, 779, 1129, 1075, 1087, 804, 414, 330, 969, 733, 153, 1028, 109, 806, 180, 756, 1206, 869, 745, 245, 519, 425, 475, 630, 436, 242, 1160, 220, 382, 849, 871, 924, 302, 1089, 894, 668, 333, 990, 1106, 1090, 1081, 472, 43, 190, 1156, 754, 450, 933, 264, 483, 158, 1052, 46, 684, 815, 985, 437, 542, 975, 89, 80, 451, 1165, 750, 1127, 442, 1111, 322, 247, 611, 1066, 34, 130, 852, 949, 1077, 211, 1141, 459, 1026, 48, 547, 139, 909, 637, 1194, 161, 7, 108, 899, 1063, 342, 890, 774, 1138, 341, 280, 841, 458, 314, 102, 1120, 526, 1214, 846, 584, 685, 873, 824, 905, 1003, 298, 537, 1004, 934, 558, 380, 177, 817, 951, 163, 26, 438, 803, 486, 1098, 1046, 429, 423, 531, 544, 138, 617, 811, 967, 1117, 406, 906, 729, 910, 597, 938, 828, 1070, 664, 485, 204, 331, 520, 719, 365, 652, 79, 512, 400, 146, 470, 766, 100, 592, 1047, 292, 616, 408, 82, 966, 724, 114, 281, 1062, 578, 505, 1091, 32, 30, 1065, 277, 649, 116, 492, 613, 1169, 231, 9, 988, 688, 509, 1031, 982, 352, 1159, 1121, 5, 236, 234, 950, 338, 700, 586, 862, 813, 943, 275, 866, 1009, 20, 671, 213, 308, 105, 44, 295, 409, 1034, 772, 75, 1084, 1045, 131, 244, 717, 856, 1044, 294, 935, 1101, 1057, 591, 1170, 47, 810, 457, 850, 174, 640, 878, 1086, 76, 278, 667, 499, 1079, 887, 875, 687, 832, 549, 644, 144, 665, 573, 802, 781, 1119, 854, 993, 915, 56, 677, 424, 638, 393, 93, 454, 596, 195, 759, 698, 977, 372, 1213, 335, 918, 1074, 1055, 837, 88, 123, 820, 413, 443, 186, 150, 22, 1, 656, 1164, 773, 142, 1064, 587, 923, 1038, 595, 368, 68, 282, 440, 663, 693, 110, 976, 148, 1201, 336, 1035, 16, 1150, 981, 188, 464, 53, 847, 1043, 603, 678, 572, 345, 689, 461, 812, 60, 983, 1039, 623, 753, 554, 478, 33, 259, 320, 989, 222, 650, 479, 681, 606, 716, 8, 1107, 487, 1080, 58, 1068, 633, 599, 870, 655, 960, 145, 836, 1200, 834, 911, 95, 462, 201, 1161, 851, 112, 721, 1006, 494, 316, 271, 405, 1177, 64, 181, 504, 134, 1179, 291, 569, 1113, 524, 261, 1088, 722, 92, 118, 883, 235, 1182, 959, 18, 858, 1033, 269, 317, 334, 701, 182, 250, 50, 1059, 78, 327, 1157, 170, 383, 306, 2, 1058, 567, 94, 952, 197, 394, 790, 1019, 527, 350, 489, 615, 256, 707, 272, 936, 532, 503, 714, 1155, 839, 4, 214, 426, 415, 1204, 535, 761, 522, 326, 1115, 311, 488, 449, 343, 165, 692, 40, 127, 225, 743, 666, 795, 411, 741, 964, 703, 491, 1184, 634, 970, 598, 702, 931, 738, 299, 1109, 232, 379, 238, 10, 200, 263, 206, 739, 980, 1132, 497, 746, 378, 388, 574, 310, 607, 566, 517, 251, 1186, 631, 265, 495, 919, 71, 579, 230, 696, 1210, 876, 639, 416, 51, 947, 577, 669, 215, 780, 226, 521]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2341358330888383
the save name prefix for this run is:  chkpt-ID_2341358330888383_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 77
rank avg (pred): 0.478 +- 0.011
mrr vals (pred, true): 0.000, 0.070
batch losses (mrrl, rdl): 0.0, 0.0007754458

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1129
rank avg (pred): 0.439 +- 0.280
mrr vals (pred, true): 0.125, 0.000
batch losses (mrrl, rdl): 0.0, 1.10236e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.464 +- 0.300
mrr vals (pred, true): 0.116, 0.000
batch losses (mrrl, rdl): 0.0, 2.03605e-05

Epoch over!
epoch time: 14.874

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.438 +- 0.306
mrr vals (pred, true): 0.133, 0.000
batch losses (mrrl, rdl): 0.0, 1.17016e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.323 +- 0.235
mrr vals (pred, true): 0.067, 0.132
batch losses (mrrl, rdl): 0.0, 4.00923e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 500
rank avg (pred): 0.349 +- 0.263
mrr vals (pred, true): 0.064, 0.150
batch losses (mrrl, rdl): 0.0, 0.0001443885

Epoch over!
epoch time: 14.903

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 938
rank avg (pred): 0.475 +- 0.296
mrr vals (pred, true): 0.026, 0.000
batch losses (mrrl, rdl): 0.0, 6.9797e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1126
rank avg (pred): 0.495 +- 0.264
mrr vals (pred, true): 0.005, 0.000
batch losses (mrrl, rdl): 0.0, 1.50785e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1153
rank avg (pred): 0.352 +- 0.258
mrr vals (pred, true): 0.022, 0.083
batch losses (mrrl, rdl): 0.0, 1.08607e-05

Epoch over!
epoch time: 14.87

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.491 +- 0.272
mrr vals (pred, true): 0.003, 0.000
batch losses (mrrl, rdl): 0.0, 1.26598e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.482 +- 0.294
mrr vals (pred, true): 0.005, 0.000
batch losses (mrrl, rdl): 0.0, 8.8779e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.425 +- 0.307
mrr vals (pred, true): 0.018, 0.000
batch losses (mrrl, rdl): 0.0, 1.69133e-05

Epoch over!
epoch time: 14.819

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 316
rank avg (pred): 0.313 +- 0.225
mrr vals (pred, true): 0.018, 0.113
batch losses (mrrl, rdl): 0.0, 6.72604e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 694
rank avg (pred): 0.481 +- 0.277
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 2.7742e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 63
rank avg (pred): 0.329 +- 0.266
mrr vals (pred, true): 0.038, 0.076
batch losses (mrrl, rdl): 0.0, 1.34856e-05

Epoch over!
epoch time: 14.726

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1001
rank avg (pred): 0.470 +- 0.276
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0227578245, 4.7175e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 919
rank avg (pred): 0.610 +- 0.271
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0009301978, 0.0004136046

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.465 +- 0.185
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0012416941, 4.30788e-05

Epoch over!
epoch time: 14.913

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 942
rank avg (pred): 0.474 +- 0.225
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002842221, 6.78698e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.141 +- 0.101
mrr vals (pred, true): 0.133, 0.163
batch losses (mrrl, rdl): 0.0093294764, 0.0005646612

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.498 +- 0.334
mrr vals (pred, true): 0.083, 0.091
batch losses (mrrl, rdl): 0.011046947, 0.0006591631

Epoch over!
epoch time: 14.911

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.584 +- 0.418
mrr vals (pred, true): 0.276, 0.261
batch losses (mrrl, rdl): 0.0021912241, 0.0023635682

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1053
rank avg (pred): 0.569 +- 0.408
mrr vals (pred, true): 0.155, 0.115
batch losses (mrrl, rdl): 0.0155577511, 0.0012169339

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 503
rank avg (pred): 0.401 +- 0.279
mrr vals (pred, true): 0.117, 0.150
batch losses (mrrl, rdl): 0.0104778074, 0.0003132212

Epoch over!
epoch time: 14.902

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 173
rank avg (pred): 0.414 +- 0.229
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.000989464, 4.03675e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1087
rank avg (pred): 0.538 +- 0.246
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.1992e-05, 0.0001193295

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 191
rank avg (pred): 0.430 +- 0.214
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002219561, 0.0001963717

Epoch over!
epoch time: 14.878

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.428 +- 0.238
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0010215979, 2.04685e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.445 +- 0.216
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003443892, 0.000117479

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 347
rank avg (pred): 0.436 +- 0.242
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006389456, 6.44162e-05

Epoch over!
epoch time: 14.87

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.337 +- 0.248
mrr vals (pred, true): 0.229, 0.253
batch losses (mrrl, rdl): 0.005854276, 0.0006976491

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1148
rank avg (pred): 0.372 +- 0.264
mrr vals (pred, true): 0.137, 0.131
batch losses (mrrl, rdl): 0.0003759895, 8.80572e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1078
rank avg (pred): 0.091 +- 0.071
mrr vals (pred, true): 0.208, 0.183
batch losses (mrrl, rdl): 0.0062261168, 0.0009593254

Epoch over!
epoch time: 14.916

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1131
rank avg (pred): 0.490 +- 0.240
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.7745e-06, 1.42584e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.457 +- 0.224
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002425402, 4.9335e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 8
rank avg (pred): 0.375 +- 0.223
mrr vals (pred, true): 0.076, 0.002
batch losses (mrrl, rdl): 0.0070008407, 0.000160754

Epoch over!
epoch time: 14.916

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.452 +- 0.200
mrr vals (pred, true): 0.056, 0.094
batch losses (mrrl, rdl): 0.0004056253, 0.0001897268

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.480 +- 0.158
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.001519621, 6.2003e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 138
rank avg (pred): 0.479 +- 0.219
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001759403, 3.74734e-05

Epoch over!
epoch time: 14.91

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 838
rank avg (pred): 0.481 +- 0.265
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003289897, 8.1822e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 814
rank avg (pred): 0.162 +- 0.127
mrr vals (pred, true): 0.274, 0.311
batch losses (mrrl, rdl): 0.0133123472, 0.0001327347

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1034
rank avg (pred): 0.420 +- 0.201
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003382852, 0.0001339724

Epoch over!
epoch time: 15.014

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.466 +- 0.194
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.13591e-05, 4.06864e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 560
rank avg (pred): 0.479 +- 0.257
mrr vals (pred, true): 0.060, 0.061
batch losses (mrrl, rdl): 0.0009369258, 0.0002171052

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.407 +- 0.219
mrr vals (pred, true): 0.060, 0.099
batch losses (mrrl, rdl): 0.001087275, 0.0003371877

Epoch over!
epoch time: 15.018

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.433 +- 0.239
mrr vals (pred, true): 0.056, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.05298 	 0.00015 	 m..s
  107 	     1 	 0.14613 	 0.00015 	 MISS
   25 	     2 	 0.05427 	 0.00016 	 m..s
   68 	     3 	 0.05695 	 0.00016 	 m..s
   18 	     4 	 0.05376 	 0.00016 	 m..s
   33 	     5 	 0.05492 	 0.00016 	 m..s
   26 	     6 	 0.05453 	 0.00017 	 m..s
   19 	     7 	 0.05381 	 0.00017 	 m..s
   52 	     8 	 0.05558 	 0.00017 	 m..s
    7 	     9 	 0.05290 	 0.00017 	 m..s
   38 	    10 	 0.05500 	 0.00017 	 m..s
    6 	    11 	 0.04929 	 0.00018 	 m..s
   15 	    12 	 0.05353 	 0.00018 	 m..s
   22 	    13 	 0.05404 	 0.00018 	 m..s
   57 	    14 	 0.05578 	 0.00018 	 m..s
   28 	    15 	 0.05458 	 0.00019 	 m..s
   79 	    16 	 0.05862 	 0.00020 	 m..s
   16 	    17 	 0.05361 	 0.00020 	 m..s
    3 	    18 	 0.04130 	 0.00020 	 m..s
   77 	    19 	 0.05851 	 0.00020 	 m..s
   54 	    20 	 0.05567 	 0.00020 	 m..s
   40 	    21 	 0.05512 	 0.00020 	 m..s
   34 	    22 	 0.05493 	 0.00020 	 m..s
   93 	    23 	 0.06247 	 0.00020 	 m..s
   82 	    24 	 0.05900 	 0.00021 	 m..s
   55 	    25 	 0.05567 	 0.00021 	 m..s
   39 	    26 	 0.05503 	 0.00022 	 m..s
   60 	    27 	 0.05626 	 0.00022 	 m..s
   62 	    28 	 0.05649 	 0.00022 	 m..s
    2 	    29 	 0.03937 	 0.00023 	 m..s
   61 	    30 	 0.05643 	 0.00024 	 m..s
   11 	    31 	 0.05318 	 0.00024 	 m..s
   87 	    32 	 0.05999 	 0.00024 	 m..s
  106 	    33 	 0.12879 	 0.00025 	 MISS
    5 	    34 	 0.04891 	 0.00025 	 m..s
   66 	    35 	 0.05662 	 0.00025 	 m..s
   17 	    36 	 0.05365 	 0.00025 	 m..s
   92 	    37 	 0.06084 	 0.00025 	 m..s
   36 	    38 	 0.05499 	 0.00026 	 m..s
   35 	    39 	 0.05495 	 0.00026 	 m..s
   71 	    40 	 0.05788 	 0.00026 	 m..s
   94 	    41 	 0.06300 	 0.00027 	 m..s
   49 	    42 	 0.05549 	 0.00027 	 m..s
   21 	    43 	 0.05396 	 0.00027 	 m..s
   56 	    44 	 0.05569 	 0.00027 	 m..s
    4 	    45 	 0.04866 	 0.00027 	 m..s
   53 	    46 	 0.05563 	 0.00028 	 m..s
   65 	    47 	 0.05657 	 0.00028 	 m..s
   43 	    48 	 0.05513 	 0.00028 	 m..s
   64 	    49 	 0.05651 	 0.00029 	 m..s
   80 	    50 	 0.05866 	 0.00029 	 m..s
   89 	    51 	 0.06045 	 0.00029 	 m..s
    0 	    52 	 0.03746 	 0.00029 	 m..s
  105 	    53 	 0.12253 	 0.00030 	 MISS
   24 	    54 	 0.05423 	 0.00030 	 m..s
   31 	    55 	 0.05482 	 0.00031 	 m..s
   14 	    56 	 0.05350 	 0.00032 	 m..s
   10 	    57 	 0.05317 	 0.00032 	 m..s
   13 	    58 	 0.05347 	 0.00032 	 m..s
   30 	    59 	 0.05474 	 0.00035 	 m..s
   83 	    60 	 0.05935 	 0.00036 	 m..s
   44 	    61 	 0.05514 	 0.00037 	 m..s
   67 	    62 	 0.05665 	 0.00039 	 m..s
   63 	    63 	 0.05650 	 0.00040 	 m..s
   27 	    64 	 0.05453 	 0.00040 	 m..s
   91 	    65 	 0.06065 	 0.00040 	 m..s
   32 	    66 	 0.05489 	 0.00041 	 m..s
   76 	    67 	 0.05851 	 0.00041 	 m..s
   50 	    68 	 0.05552 	 0.00042 	 m..s
   58 	    69 	 0.05589 	 0.00043 	 m..s
   20 	    70 	 0.05389 	 0.00043 	 m..s
   37 	    71 	 0.05500 	 0.00046 	 m..s
   73 	    72 	 0.05816 	 0.00047 	 m..s
   47 	    73 	 0.05523 	 0.00049 	 m..s
   59 	    74 	 0.05610 	 0.00050 	 m..s
   23 	    75 	 0.05423 	 0.00051 	 m..s
   72 	    76 	 0.05791 	 0.00053 	 m..s
   29 	    77 	 0.05472 	 0.00057 	 m..s
   45 	    78 	 0.05520 	 0.00060 	 m..s
   46 	    79 	 0.05521 	 0.00060 	 m..s
   48 	    80 	 0.05544 	 0.00060 	 m..s
   41 	    81 	 0.05513 	 0.00061 	 m..s
   12 	    82 	 0.05336 	 0.00061 	 m..s
   74 	    83 	 0.05828 	 0.00063 	 m..s
   42 	    84 	 0.05513 	 0.00063 	 m..s
   78 	    85 	 0.05854 	 0.00064 	 m..s
   85 	    86 	 0.05938 	 0.00064 	 m..s
    1 	    87 	 0.03797 	 0.00087 	 m..s
    9 	    88 	 0.05302 	 0.00161 	 m..s
   81 	    89 	 0.05881 	 0.00166 	 m..s
   51 	    90 	 0.05552 	 0.00220 	 m..s
   90 	    91 	 0.06058 	 0.00637 	 m..s
   95 	    92 	 0.06417 	 0.05863 	 ~...
   70 	    93 	 0.05782 	 0.07186 	 ~...
   99 	    94 	 0.08457 	 0.07730 	 ~...
   75 	    95 	 0.05850 	 0.07830 	 ~...
   86 	    96 	 0.05977 	 0.07916 	 ~...
   88 	    97 	 0.06016 	 0.07977 	 ~...
  114 	    98 	 0.25173 	 0.08004 	 MISS
  100 	    99 	 0.08779 	 0.08077 	 ~...
   97 	   100 	 0.07892 	 0.08633 	 ~...
   96 	   101 	 0.07157 	 0.08743 	 ~...
   69 	   102 	 0.05752 	 0.09564 	 m..s
   98 	   103 	 0.08308 	 0.11462 	 m..s
   84 	   104 	 0.05936 	 0.11502 	 m..s
  102 	   105 	 0.09732 	 0.11635 	 ~...
  109 	   106 	 0.16279 	 0.12051 	 m..s
  103 	   107 	 0.10575 	 0.12437 	 ~...
  101 	   108 	 0.09062 	 0.12813 	 m..s
  108 	   109 	 0.15847 	 0.13256 	 ~...
  110 	   110 	 0.17361 	 0.14537 	 ~...
  104 	   111 	 0.11797 	 0.14671 	 ~...
  111 	   112 	 0.20451 	 0.16023 	 m..s
  112 	   113 	 0.20795 	 0.17022 	 m..s
  118 	   114 	 0.33570 	 0.17315 	 MISS
  117 	   115 	 0.25386 	 0.18054 	 m..s
  113 	   116 	 0.21035 	 0.18137 	 ~...
  115 	   117 	 0.25202 	 0.20159 	 m..s
  116 	   118 	 0.25328 	 0.20291 	 m..s
  119 	   119 	 0.33968 	 0.32898 	 ~...
  120 	   120 	 0.34023 	 0.33208 	 ~...
==========================================
r_mrr = 0.8652390241622925
r2_mrr = 0.24658888578414917
spearmanr_mrr@5 = 0.7349017262458801
spearmanr_mrr@10 = 0.8446784615516663
spearmanr_mrr@50 = 0.9306135773658752
spearmanr_mrr@100 = 0.9433966875076294
spearmanr_mrr@All = 0.9455554485321045
==========================================
test time: 0.45
Done Testing dataset DBpedia50
total time taken: 229.20772314071655
training time taken: 223.90092778205872
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8652)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.2466)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.7349)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8447)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9306)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9434)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9456)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.230747782976323}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 8400672867204421
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [374, 1058, 1145, 990, 193, 568, 396, 378, 426, 1154, 264, 818, 729, 88, 840, 130, 657, 695, 105, 371, 851, 859, 514, 157, 1116, 718, 33, 960, 306, 683, 131, 1025, 251, 619, 521, 79, 361, 752, 620, 462, 1171, 154, 236, 68, 273, 643, 202, 248, 419, 920, 969, 171, 135, 569, 697, 530, 473, 909, 638, 805, 1100, 149, 563, 40, 104, 1119, 849, 678, 420, 28, 1207, 1174, 993, 363, 1035, 89, 364, 1080, 8, 1061, 921, 584, 471, 1059, 414, 14, 1094, 976, 961, 791, 1147, 1208, 565, 498, 1107, 444, 702, 50, 933, 132, 962, 449, 467, 1111, 1148, 170, 545, 490, 764, 108, 926, 298, 188, 432, 773, 877, 1045, 65, 576, 1104, 581]
valid_ids (0): []
train_ids (1094): [74, 215, 628, 393, 373, 907, 813, 435, 777, 558, 26, 241, 31, 983, 939, 1105, 185, 716, 137, 503, 84, 536, 272, 277, 213, 945, 36, 586, 801, 63, 77, 892, 222, 1146, 886, 540, 328, 1041, 85, 144, 670, 973, 1087, 3, 51, 1067, 355, 101, 201, 445, 646, 693, 747, 800, 713, 45, 383, 1160, 881, 928, 914, 474, 1167, 1179, 763, 332, 9, 267, 1026, 1126, 602, 996, 817, 138, 1037, 1185, 541, 863, 308, 1036, 946, 366, 664, 382, 1005, 1199, 862, 1144, 134, 952, 145, 727, 225, 239, 571, 81, 487, 506, 564, 303, 1124, 483, 1106, 398, 196, 1130, 370, 807, 512, 493, 684, 410, 949, 866, 237, 532, 345, 738, 865, 288, 400, 221, 1153, 428, 189, 850, 433, 1193, 612, 1162, 523, 744, 526, 1180, 392, 825, 991, 812, 124, 685, 1161, 404, 495, 746, 481, 351, 421, 788, 413, 390, 263, 475, 964, 1093, 275, 1027, 879, 457, 30, 717, 326, 607, 810, 119, 466, 216, 13, 322, 641, 680, 146, 48, 174, 864, 1189, 992, 984, 627, 142, 38, 1170, 1023, 448, 1014, 955, 407, 446, 822, 874, 291, 86, 304, 782, 292, 334, 338, 1169, 671, 311, 197, 625, 293, 1006, 1099, 2, 22, 313, 721, 107, 310, 163, 460, 637, 968, 254, 977, 1200, 944, 917, 787, 1051, 1157, 468, 786, 942, 703, 1008, 184, 577, 461, 574, 299, 422, 258, 233, 724, 387, 1102, 96, 59, 261, 954, 1016, 868, 1182, 172, 114, 870, 1081, 205, 694, 87, 916, 122, 828, 324, 83, 547, 971, 855, 882, 312, 672, 452, 191, 103, 1084, 815, 1214, 270, 7, 479, 965, 836, 901, 280, 780, 700, 1017, 329, 484, 200, 1079, 922, 736, 876, 1003, 318, 340, 488, 601, 330, 230, 268, 406, 1156, 391, 554, 896, 190, 814, 517, 218, 408, 1120, 110, 1086, 1209, 556, 1134, 665, 651, 49, 835, 1138, 12, 994, 1152, 923, 42, 301, 918, 749, 54, 653, 504, 343, 710, 603, 938, 951, 1021, 203, 327, 1108, 911, 379, 244, 931, 177, 112, 284, 679, 295, 1060, 739, 953, 243, 529, 358, 416, 476, 1206, 1151, 783, 871, 41, 346, 1071, 600, 599, 535, 943, 897, 593, 15, 891, 159, 842, 656, 1150, 224, 596, 522, 531, 799, 210, 910, 639, 1089, 803, 765, 302, 1075, 560, 46, 1078, 186, 499, 1128, 957, 320, 769, 24, 1131, 285, 742, 1018, 947, 1048, 442, 903, 102, 394, 1069, 823, 676, 615, 555, 621, 539, 180, 307, 567, 271, 337, 644, 369, 940, 425, 1066, 55, 257, 988, 265, 469, 755, 1049, 353, 411, 809, 819, 199, 1028, 294, 92, 316, 632, 590, 533, 711, 794, 978, 1139, 274, 553, 1177, 21, 315, 1112, 436, 464, 972, 347, 608, 611, 552, 737, 360, 1164, 242, 491, 767, 17, 510, 357, 1072, 551, 956, 127, 349, 1022, 440, 1034, 509, 706, 797, 1204, 659, 654, 1110, 260, 645, 906, 572, 887, 431, 214, 595, 238, 838, 456, 450, 853, 206, 528, 78, 1065, 784, 587, 1095, 37, 1039, 384, 579, 802, 682, 158, 53, 719, 1118, 1201, 662, 772, 402, 898, 262, 278, 489, 147, 970, 751, 959, 152, 336, 1109, 527, 32, 1040, 666, 123, 833, 397, 1178, 543, 1012, 57, 27, 1195, 872, 16, 1155, 919, 908, 168, 515, 635, 562, 290, 894, 289, 1011, 376, 692, 708, 614, 542, 867, 844, 1033, 1085, 93, 1142, 1015, 377, 591, 924, 756, 497, 668, 1068, 129, 350, 578, 561, 1062, 235, 1175, 438, 252, 1097, 259, 121, 1186, 472, 486, 375, 624, 709, 795, 1031, 1053, 516, 255, 401, 1183, 1202, 176, 760, 443, 941, 372, 580, 834, 1159, 1212, 843, 240, 75, 227, 598, 966, 832, 893, 1090, 1042, 689, 231, 141, 1013, 661, 161, 548, 1001, 613, 90, 981, 1057, 649, 1173, 217, 1038, 470, 570, 91, 743, 1127, 155, 854, 730, 1082, 72, 537, 181, 70, 1073, 852, 250, 636, 1181, 309, 701, 839, 754, 913, 1019, 325, 691, 477, 386, 798, 1030, 806, 405, 211, 1136, 796, 133, 811, 113, 35, 998, 1083, 781, 451, 841, 575, 362, 677, 559, 588, 546, 732, 1125, 699, 359, 1121, 557, 365, 333, 1194, 331, 245, 761, 136, 856, 793, 592, 232, 344, 888, 589, 164, 912, 634, 1055, 830, 418, 633, 1122, 974, 424, 549, 858, 502, 118, 1096, 513, 857, 228, 1198, 1172, 905, 195, 895, 1046, 669, 594, 735, 1044, 1000, 1076, 789, 999, 380, 156, 148, 204, 447, 182, 335, 463, 688, 778, 437, 321, 658, 282, 696, 726, 1054, 352, 100, 757, 500, 690, 403, 597, 929, 861, 496, 935, 779, 734, 820, 453, 720, 655, 544, 501, 167, 837, 482, 770, 889, 899, 505, 925, 323, 1140, 846, 173, 44, 1113, 675, 731, 109, 52, 585, 276, 485, 209, 279, 1137, 963, 1088, 1056, 1188, 492, 1043, 616, 162, 674, 1070, 605, 247, 458, 305, 178, 39, 741, 98, 417, 175, 283, 847, 389, 785, 494, 723, 1092, 19, 550, 0, 967, 934, 1010, 704, 524, 663, 423, 1024, 80, 915, 1190, 111, 58, 223, 715, 1184, 745, 606, 23, 975, 890, 354, 816, 1103, 314, 774, 1064, 1047, 395, 1052, 341, 712, 194, 660, 519, 1077, 1114, 705, 883, 821, 986, 234, 1063, 725, 208, 880, 25, 429, 4, 356, 1149, 10, 869, 43, 626, 1135, 1168, 116, 507, 76, 686, 936, 212, 115, 511, 652, 958, 1205, 824, 220, 73, 1196, 792, 140, 179, 286, 18, 827, 459, 1197, 1115, 604, 478, 67, 622, 650, 1117, 995, 687, 1009, 1091, 1032, 388, 885, 1007, 399, 1020, 617, 151, 1141, 296, 904, 348, 1213, 826, 534, 95, 771, 927, 246, 768, 610, 1176, 66, 368, 776, 987, 1187, 1074, 165, 117, 1166, 1002, 520, 566, 1050, 1203, 518, 790, 62, 714, 804, 47, 207, 848, 583, 948, 187, 629, 430, 1004, 845, 748, 56, 989, 1143, 441, 253, 126, 427, 342, 69, 11, 150, 982, 29, 153, 1098, 1029, 219, 873, 667, 623, 950, 82, 61, 120, 609, 125, 269, 642, 573, 166, 900, 762, 385, 454, 775, 1132, 139, 1210, 681, 226, 733, 753, 618, 229, 297, 1192, 409, 1211, 758, 631, 465, 198, 128, 630, 884, 766, 902, 722, 143, 71, 582, 508, 256, 434, 937, 6, 980, 808, 648, 1133, 979, 480, 860, 647, 455, 1191, 249, 94, 183, 1165, 1, 831, 367, 740, 317, 319, 106, 750, 878, 728, 381, 439, 64, 985, 759, 1158, 169, 829, 698, 266, 932, 60, 707, 930, 1101, 673, 640, 415, 1129, 1163, 412, 875, 1123, 339, 281, 97, 5, 300, 538, 20, 287, 160, 192, 997, 34, 99, 525]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8129535089091037
the save name prefix for this run is:  chkpt-ID_8129535089091037_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.420 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001676058

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.309 +- 0.209
mrr vals (pred, true): 0.138, 0.087
batch losses (mrrl, rdl): 0.0, 7.45726e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 888
rank avg (pred): 0.476 +- 0.316
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0, 2.7674e-06

Epoch over!
epoch time: 15.024

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.453 +- 0.302
mrr vals (pred, true): 0.027, 0.000
batch losses (mrrl, rdl): 0.0, 5.0165e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.511 +- 0.291
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 4.2563e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.510 +- 0.300
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 1.54167e-05

Epoch over!
epoch time: 14.927

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 418
rank avg (pred): 0.481 +- 0.291
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 9.204e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 930
rank avg (pred): 0.523 +- 0.306
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 1.40111e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 183
rank avg (pred): 0.475 +- 0.306
mrr vals (pred, true): 0.008, 0.000
batch losses (mrrl, rdl): 0.0, 2.0228e-06

Epoch over!
epoch time: 14.781

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 698
rank avg (pred): 0.486 +- 0.304
mrr vals (pred, true): 0.008, 0.001
batch losses (mrrl, rdl): 0.0, 1.4773e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1141
rank avg (pred): 0.294 +- 0.340
mrr vals (pred, true): 0.041, 0.188
batch losses (mrrl, rdl): 0.0, 2.08976e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1005
rank avg (pred): 0.499 +- 0.312
mrr vals (pred, true): 0.010, 0.000
batch losses (mrrl, rdl): 0.0, 2.20475e-05

Epoch over!
epoch time: 14.738

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 966
rank avg (pred): 0.521 +- 0.307
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 2.56645e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.490 +- 0.306
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 4.5074e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 830
rank avg (pred): 0.162 +- 0.266
mrr vals (pred, true): 0.063, 0.173
batch losses (mrrl, rdl): 0.0, 7.84564e-05

Epoch over!
epoch time: 14.869

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.313 +- 0.311
mrr vals (pred, true): 0.028, 0.047
batch losses (mrrl, rdl): 0.0048025874, 5.4479e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 875
rank avg (pred): 0.559 +- 0.361
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.0786e-06, 1.54321e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1167
rank avg (pred): 0.469 +- 0.295
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 3.6324e-06, 2.948e-07

Epoch over!
epoch time: 15.12

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 142
rank avg (pred): 0.579 +- 0.352
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005301781, 0.0001770402

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.482 +- 0.476
mrr vals (pred, true): 0.193, 0.153
batch losses (mrrl, rdl): 0.0165163353, 0.0009182796

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 896
rank avg (pred): 0.590 +- 0.439
mrr vals (pred, true): 0.076, 0.016
batch losses (mrrl, rdl): 0.0067838659, 7.29114e-05

Epoch over!
epoch time: 15.109

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 446
rank avg (pred): 0.572 +- 0.349
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.493e-05, 0.0001348734

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.460 +- 0.315
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 4.6997e-06, 6.3715e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.458 +- 0.364
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0016476247, 2.44041e-05

Epoch over!
epoch time: 15.115

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 544
rank avg (pred): 0.322 +- 0.252
mrr vals (pred, true): 0.063, 0.064
batch losses (mrrl, rdl): 0.0016129632, 4.59145e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 82
rank avg (pred): 0.573 +- 0.364
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.22e-08, 6.06892e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 460
rank avg (pred): 0.512 +- 0.284
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.58813e-05, 3.23693e-05

Epoch over!
epoch time: 15.096

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.415 +- 0.305
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001191962, 0.0001330676

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1084
rank avg (pred): 0.551 +- 0.330
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.1641e-06, 8.59874e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1117
rank avg (pred): 0.481 +- 0.291
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 6.8629e-06, 7.751e-07

Epoch over!
epoch time: 15.085

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.503 +- 0.315
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.4139e-06, 5.87819e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.214 +- 0.357
mrr vals (pred, true): 0.272, 0.181
batch losses (mrrl, rdl): 0.0828623623, 3.48454e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 38
rank avg (pred): 0.496 +- 0.366
mrr vals (pred, true): 0.052, 0.088
batch losses (mrrl, rdl): 2.4316e-05, 0.0002481774

Epoch over!
epoch time: 15.077

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 259
rank avg (pred): 0.394 +- 0.413
mrr vals (pred, true): 0.189, 0.298
batch losses (mrrl, rdl): 0.1185368896, 0.0006987081

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1193
rank avg (pred): 0.581 +- 0.357
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0017211346, 0.0001001862

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.543 +- 0.324
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.19378e-05, 2.57917e-05

Epoch over!
epoch time: 15.076

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1115
rank avg (pred): 0.623 +- 0.357
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 7.0268e-06, 0.0004316062

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1062
rank avg (pred): 0.540 +- 0.386
mrr vals (pred, true): 0.091, 0.096
batch losses (mrrl, rdl): 0.0170280579, 0.0007870386

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.507 +- 0.340
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.8888e-06, 1.61191e-05

Epoch over!
epoch time: 15.072

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 611
rank avg (pred): 0.513 +- 0.339
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 4.0586e-06, 1.06814e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 413
rank avg (pred): 0.507 +- 0.346
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.4619e-06, 3.44103e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 585
rank avg (pred): 0.518 +- 0.311
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.32955e-05, 6.5925e-06

Epoch over!
epoch time: 15.07

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 373
rank avg (pred): 0.513 +- 0.284
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.73746e-05, 1.68147e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.465 +- 0.271
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 9.2552e-06, 2.1229e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 128
rank avg (pred): 0.509 +- 0.350
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.01881e-05, 1.20265e-05

Epoch over!
epoch time: 15.056

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.505 +- 0.342
mrr vals (pred, true): 0.050, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04887 	 0.00012 	 m..s
   38 	     1 	 0.04915 	 0.00015 	 m..s
   78 	     2 	 0.05536 	 0.00015 	 m..s
    3 	     3 	 0.04890 	 0.00017 	 m..s
    4 	     4 	 0.04890 	 0.00017 	 m..s
   45 	     5 	 0.04924 	 0.00017 	 m..s
   48 	     6 	 0.04932 	 0.00018 	 m..s
   28 	     7 	 0.04900 	 0.00018 	 m..s
   60 	     8 	 0.04984 	 0.00018 	 m..s
   25 	     9 	 0.04898 	 0.00018 	 m..s
   74 	    10 	 0.05429 	 0.00018 	 m..s
   80 	    11 	 0.06040 	 0.00018 	 m..s
   51 	    12 	 0.04942 	 0.00018 	 m..s
   71 	    13 	 0.05335 	 0.00018 	 m..s
    2 	    14 	 0.04890 	 0.00019 	 m..s
   69 	    15 	 0.05242 	 0.00019 	 m..s
   10 	    16 	 0.04894 	 0.00019 	 m..s
   54 	    17 	 0.04957 	 0.00020 	 m..s
    6 	    18 	 0.04891 	 0.00020 	 m..s
    5 	    19 	 0.04890 	 0.00020 	 m..s
   59 	    20 	 0.04982 	 0.00020 	 m..s
   83 	    21 	 0.06601 	 0.00020 	 m..s
   82 	    22 	 0.06515 	 0.00020 	 m..s
   52 	    23 	 0.04950 	 0.00021 	 m..s
   70 	    24 	 0.05321 	 0.00021 	 m..s
   32 	    25 	 0.04907 	 0.00021 	 m..s
   26 	    26 	 0.04899 	 0.00022 	 m..s
   24 	    27 	 0.04898 	 0.00022 	 m..s
   11 	    28 	 0.04894 	 0.00022 	 m..s
   68 	    29 	 0.05205 	 0.00023 	 m..s
    9 	    30 	 0.04893 	 0.00023 	 m..s
   16 	    31 	 0.04896 	 0.00024 	 m..s
   33 	    32 	 0.04907 	 0.00024 	 m..s
   79 	    33 	 0.06030 	 0.00025 	 m..s
   65 	    34 	 0.05046 	 0.00025 	 m..s
   21 	    35 	 0.04897 	 0.00025 	 m..s
   47 	    36 	 0.04931 	 0.00025 	 m..s
   66 	    37 	 0.05071 	 0.00026 	 m..s
   81 	    38 	 0.06224 	 0.00027 	 m..s
   72 	    39 	 0.05343 	 0.00027 	 m..s
   67 	    40 	 0.05116 	 0.00027 	 m..s
   49 	    41 	 0.04936 	 0.00027 	 m..s
   90 	    42 	 0.06951 	 0.00028 	 m..s
   17 	    43 	 0.04896 	 0.00029 	 m..s
   64 	    44 	 0.05035 	 0.00029 	 m..s
    1 	    45 	 0.04888 	 0.00029 	 m..s
   55 	    46 	 0.04967 	 0.00029 	 m..s
   62 	    47 	 0.04986 	 0.00029 	 m..s
    7 	    48 	 0.04891 	 0.00030 	 m..s
   31 	    49 	 0.04906 	 0.00030 	 m..s
   76 	    50 	 0.05473 	 0.00030 	 m..s
   30 	    51 	 0.04903 	 0.00032 	 m..s
   53 	    52 	 0.04951 	 0.00033 	 m..s
   73 	    53 	 0.05421 	 0.00033 	 m..s
   19 	    54 	 0.04896 	 0.00034 	 m..s
   46 	    55 	 0.04930 	 0.00035 	 m..s
   34 	    56 	 0.04909 	 0.00039 	 m..s
   57 	    57 	 0.04979 	 0.00040 	 m..s
   27 	    58 	 0.04899 	 0.00040 	 m..s
   50 	    59 	 0.04942 	 0.00041 	 m..s
   63 	    60 	 0.04995 	 0.00041 	 m..s
    8 	    61 	 0.04893 	 0.00042 	 m..s
   58 	    62 	 0.04980 	 0.00042 	 m..s
   77 	    63 	 0.05536 	 0.00042 	 m..s
   43 	    64 	 0.04922 	 0.00043 	 m..s
   61 	    65 	 0.04985 	 0.00043 	 m..s
   44 	    66 	 0.04924 	 0.00044 	 m..s
   13 	    67 	 0.04895 	 0.00045 	 m..s
   40 	    68 	 0.04917 	 0.00050 	 m..s
   14 	    69 	 0.04895 	 0.00050 	 m..s
   41 	    70 	 0.04918 	 0.00051 	 m..s
   56 	    71 	 0.04978 	 0.00054 	 m..s
   12 	    72 	 0.04894 	 0.00056 	 m..s
   75 	    73 	 0.05444 	 0.00056 	 m..s
   35 	    74 	 0.04911 	 0.00056 	 m..s
   84 	    75 	 0.06762 	 0.00057 	 m..s
   18 	    76 	 0.04896 	 0.00057 	 m..s
   36 	    77 	 0.04913 	 0.00060 	 m..s
   22 	    78 	 0.04897 	 0.00061 	 m..s
   42 	    79 	 0.04919 	 0.00064 	 m..s
   15 	    80 	 0.04895 	 0.00065 	 m..s
   39 	    81 	 0.04916 	 0.00070 	 m..s
   96 	    82 	 0.08153 	 0.00082 	 m..s
   20 	    83 	 0.04896 	 0.00103 	 m..s
   23 	    84 	 0.04897 	 0.00141 	 m..s
   91 	    85 	 0.07322 	 0.00179 	 m..s
   29 	    86 	 0.04903 	 0.00201 	 m..s
   37 	    87 	 0.04915 	 0.00846 	 m..s
  120 	    88 	 0.26419 	 0.04430 	 MISS
  103 	    89 	 0.10161 	 0.05590 	 m..s
   97 	    90 	 0.08162 	 0.07021 	 ~...
  104 	    91 	 0.10501 	 0.08121 	 ~...
   84 	    92 	 0.06762 	 0.08157 	 ~...
   89 	    93 	 0.06830 	 0.08574 	 ~...
   84 	    94 	 0.06762 	 0.08835 	 ~...
  110 	    95 	 0.12493 	 0.08908 	 m..s
   94 	    96 	 0.07600 	 0.09120 	 ~...
  106 	    97 	 0.11438 	 0.09160 	 ~...
  105 	    98 	 0.10577 	 0.09167 	 ~...
  101 	    99 	 0.09752 	 0.09332 	 ~...
   84 	   100 	 0.06762 	 0.09379 	 ~...
   92 	   101 	 0.07481 	 0.09680 	 ~...
   84 	   102 	 0.06762 	 0.09994 	 m..s
  102 	   103 	 0.09936 	 0.10153 	 ~...
   95 	   104 	 0.08105 	 0.10273 	 ~...
  108 	   105 	 0.11588 	 0.10641 	 ~...
   99 	   106 	 0.09070 	 0.10828 	 ~...
   93 	   107 	 0.07518 	 0.10918 	 m..s
  111 	   108 	 0.15830 	 0.11751 	 m..s
  109 	   109 	 0.11968 	 0.12018 	 ~...
   98 	   110 	 0.08302 	 0.12088 	 m..s
  100 	   111 	 0.09515 	 0.12444 	 ~...
  107 	   112 	 0.11535 	 0.13084 	 ~...
  113 	   113 	 0.18059 	 0.15728 	 ~...
  115 	   114 	 0.20625 	 0.17315 	 m..s
  114 	   115 	 0.18240 	 0.18010 	 ~...
  118 	   116 	 0.26182 	 0.19254 	 m..s
  117 	   117 	 0.25935 	 0.19818 	 m..s
  112 	   118 	 0.16695 	 0.22067 	 m..s
  119 	   119 	 0.26183 	 0.31338 	 m..s
  116 	   120 	 0.24779 	 0.33898 	 m..s
==========================================
r_mrr = 0.8566057682037354
r2_mrr = 0.38115620613098145
spearmanr_mrr@5 = 0.6662463545799255
spearmanr_mrr@10 = 0.8026424050331116
spearmanr_mrr@50 = 0.9169554710388184
spearmanr_mrr@100 = 0.9418385028839111
spearmanr_mrr@All = 0.9448829889297485
==========================================
test time: 0.447
Done Testing dataset DBpedia50
total time taken: 231.0077519416809
training time taken: 225.6729919910431
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8566)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.3812)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.6662)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.8026)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9170)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9418)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9449)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.0515230483442792}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 8808177105511363
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [964, 1087, 416, 1099, 657, 90, 684, 663, 817, 1169, 190, 1070, 319, 1111, 1203, 209, 239, 421, 33, 883, 414, 776, 980, 262, 961, 797, 1113, 874, 813, 483, 325, 343, 629, 408, 250, 760, 256, 213, 444, 745, 890, 535, 2, 1018, 237, 541, 151, 1125, 995, 973, 164, 1027, 25, 1119, 88, 895, 638, 635, 38, 192, 245, 484, 290, 649, 1038, 1137, 221, 210, 1127, 215, 795, 167, 1114, 244, 958, 199, 1154, 226, 246, 19, 716, 819, 990, 548, 576, 431, 391, 1120, 1044, 1147, 451, 1045, 118, 89, 1129, 562, 860, 114, 630, 282, 63, 608, 924, 591, 855, 488, 220, 264, 1207, 620, 551, 436, 181, 862, 524, 146, 438, 777, 217, 260, 233]
valid_ids (0): []
train_ids (1094): [191, 570, 276, 85, 969, 1157, 759, 1035, 678, 285, 266, 422, 564, 446, 120, 493, 922, 746, 149, 1002, 1021, 173, 143, 669, 23, 1104, 410, 946, 24, 65, 294, 1164, 1179, 1043, 230, 232, 747, 1067, 454, 404, 393, 1059, 389, 34, 778, 531, 370, 732, 470, 258, 131, 571, 876, 911, 1053, 1153, 102, 1130, 666, 328, 133, 615, 1123, 208, 227, 871, 763, 171, 744, 891, 672, 364, 752, 656, 533, 1210, 450, 506, 566, 914, 1115, 418, 1201, 494, 688, 1, 967, 539, 611, 921, 278, 989, 626, 1167, 1069, 786, 132, 117, 1178, 82, 1071, 257, 160, 129, 715, 322, 793, 312, 751, 485, 448, 851, 694, 1074, 501, 590, 505, 315, 1185, 675, 555, 1131, 648, 559, 104, 987, 428, 296, 400, 437, 378, 1031, 1181, 22, 612, 372, 558, 800, 712, 187, 527, 174, 1191, 497, 920, 298, 942, 81, 710, 1006, 417, 887, 103, 880, 898, 465, 303, 772, 976, 882, 96, 439, 91, 55, 1092, 650, 122, 481, 931, 424, 944, 1116, 354, 701, 981, 405, 196, 134, 1083, 836, 80, 1098, 1054, 1208, 1064, 664, 766, 469, 31, 427, 473, 1174, 7, 324, 479, 538, 1200, 830, 1042, 166, 161, 9, 368, 704, 938, 1065, 326, 721, 953, 1138, 818, 308, 696, 991, 731, 279, 791, 5, 476, 283, 682, 897, 486, 42, 457, 1202, 625, 1148, 634, 782, 547, 299, 660, 1088, 121, 288, 573, 603, 348, 186, 729, 185, 347, 846, 534, 273, 521, 1198, 908, 423, 894, 651, 532, 286, 147, 542, 1205, 610, 775, 201, 277, 821, 11, 472, 951, 1030, 184, 730, 402, 520, 750, 907, 932, 528, 578, 291, 115, 335, 107, 743, 593, 838, 1072, 642, 1101, 384, 126, 842, 927, 163, 893, 1095, 222, 119, 641, 823, 503, 840, 1066, 583, 175, 219, 339, 955, 130, 1041, 1050, 912, 1060, 74, 1106, 100, 810, 30, 464, 796, 736, 837, 902, 1008, 76, 43, 1142, 783, 605, 511, 12, 714, 866, 784, 255, 905, 628, 105, 556, 235, 455, 21, 724, 1193, 875, 655, 892, 833, 67, 646, 1015, 584, 674, 617, 639, 917, 26, 1197, 1012, 755, 433, 318, 193, 904, 975, 691, 329, 690, 557, 144, 827, 240, 203, 1028, 491, 913, 383, 1168, 519, 29, 574, 1204, 361, 498, 373, 73, 1158, 176, 859, 409, 1182, 419, 61, 1062, 1188, 306, 478, 153, 490, 300, 594, 848, 442, 792, 297, 108, 327, 399, 896, 916, 965, 530, 1194, 654, 477, 48, 711, 1105, 720, 474, 394, 362, 514, 801, 901, 139, 977, 6, 374, 28, 1150, 447, 706, 1199, 62, 77, 84, 495, 680, 613, 988, 756, 367, 839, 780, 607, 197, 434, 685, 900, 683, 344, 1033, 764, 158, 986, 540, 1117, 725, 459, 658, 183, 109, 864, 36, 877, 609, 334, 1022, 1102, 1046, 456, 336, 261, 1085, 66, 737, 323, 8, 377, 979, 466, 661, 413, 577, 263, 717, 211, 722, 145, 929, 188, 142, 124, 723, 1047, 218, 739, 934, 407, 482, 906, 753, 86, 71, 886, 333, 928, 568, 57, 668, 254, 1132, 489, 627, 94, 14, 53, 748, 598, 870, 1103, 305, 204, 371, 17, 398, 452, 350, 1016, 395, 676, 1001, 18, 1189, 804, 925, 695, 853, 386, 757, 622, 252, 194, 75, 978, 930, 1165, 808, 467, 1029, 125, 919, 1007, 259, 523, 289, 1146, 487, 1049, 95, 205, 1000, 513, 342, 165, 346, 1160, 652, 923, 420, 910, 758, 1133, 236, 59, 769, 970, 1048, 600, 579, 58, 170, 1186, 356, 858, 552, 353, 845, 637, 589, 596, 1162, 1139, 4, 512, 1052, 966, 618, 238, 453, 604, 352, 1023, 47, 915, 1143, 888, 1079, 1091, 1155, 1020, 251, 549, 950, 304, 265, 35, 1134, 677, 1126, 112, 432, 281, 141, 1024, 401, 844, 247, 69, 623, 726, 993, 854, 168, 234, 621, 380, 412, 1017, 98, 162, 645, 41, 475, 592, 231, 994, 274, 667, 1211, 37, 40, 492, 948, 903, 396, 225, 956, 317, 1176, 1080, 647, 773, 500, 809, 771, 403, 575, 15, 1108, 580, 770, 508, 560, 788, 313, 554, 673, 982, 1081, 572, 1145, 873, 83, 1213, 768, 546, 563, 735, 689, 1141, 155, 0, 799, 206, 180, 207, 458, 1056, 996, 70, 101, 10, 461, 949, 1019, 1151, 767, 522, 381, 172, 831, 754, 806, 807, 1183, 719, 345, 971, 1051, 1036, 865, 449, 765, 229, 516, 275, 952, 429, 614, 27, 707, 60, 309, 1073, 128, 443, 387, 947, 1177, 116, 20, 974, 781, 606, 195, 959, 496, 1003, 960, 659, 1058, 87, 1063, 709, 526, 692, 382, 529, 847, 441, 879, 1195, 228, 662, 1009, 341, 152, 507, 1034, 899, 1109, 242, 624, 565, 631, 1082, 742, 708, 1166, 79, 178, 738, 968, 1097, 1076, 543, 567, 582, 415, 869, 159, 390, 828, 307, 941, 1184, 933, 727, 820, 992, 861, 200, 214, 1144, 284, 700, 1171, 1128, 212, 138, 111, 595, 653, 177, 826, 1214, 1039, 587, 794, 957, 272, 1180, 1055, 665, 311, 599, 97, 863, 440, 72, 445, 602, 687, 1061, 216, 136, 49, 553, 832, 1057, 45, 636, 954, 1196, 198, 733, 517, 52, 1075, 985, 320, 1011, 388, 825, 1156, 113, 1090, 137, 878, 1124, 292, 425, 270, 295, 681, 363, 338, 705, 360, 223, 545, 741, 935, 632, 515, 110, 1192, 44, 156, 616, 1089, 150, 581, 337, 426, 824, 749, 1100, 937, 1206, 812, 1212, 375, 1170, 963, 510, 1037, 586, 644, 271, 406, 1112, 834, 885, 51, 302, 358, 321, 926, 881, 798, 1122, 561, 909, 179, 918, 1078, 1149, 939, 518, 430, 1010, 544, 1121, 502, 287, 940, 157, 1086, 702, 1004, 884, 537, 154, 843, 310, 779, 1163, 135, 850, 829, 349, 202, 728, 703, 679, 268, 1068, 93, 983, 46, 355, 106, 1118, 435, 248, 785, 999, 972, 1135, 822, 867, 1175, 340, 460, 619, 1005, 718, 787, 984, 330, 480, 525, 301, 868, 169, 789, 713, 945, 293, 499, 99, 856, 463, 68, 1161, 997, 774, 585, 39, 601, 790, 734, 1094, 1084, 852, 699, 1209, 314, 811, 369, 462, 504, 1013, 550, 1152, 280, 835, 92, 56, 253, 1136, 332, 802, 698, 267, 1110, 359, 243, 316, 468, 697, 670, 123, 411, 351, 815, 379, 643, 224, 936, 597, 397, 148, 331, 841, 943, 962, 536, 762, 509, 269, 54, 1025, 1096, 249, 998, 693, 385, 1173, 872, 392, 569, 1032, 1014, 140, 1107, 1093, 64, 376, 1026, 1140, 761, 803, 1077, 32, 16, 241, 366, 671, 805, 471, 50, 357, 640, 633, 1187, 3, 127, 1190, 849, 1172, 1159, 189, 182, 78, 13, 889, 365, 857, 686, 1040, 814, 816, 740, 588]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1330777416949727
the save name prefix for this run is:  chkpt-ID_1330777416949727_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1025
rank avg (pred): 0.532 +- 0.001
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001535843

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.467 +- 0.156
mrr vals (pred, true): 0.029, 0.000
batch losses (mrrl, rdl): 0.0, 8.08927e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.333 +- 0.189
mrr vals (pred, true): 0.183, 0.000
batch losses (mrrl, rdl): 0.0, 0.0006379356

Epoch over!
epoch time: 14.858

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 501
rank avg (pred): 0.362 +- 0.206
mrr vals (pred, true): 0.192, 0.124
batch losses (mrrl, rdl): 0.0, 0.0001596145

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 686
rank avg (pred): 0.461 +- 0.262
mrr vals (pred, true): 0.195, 0.001
batch losses (mrrl, rdl): 0.0, 1.89001e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 145
rank avg (pred): 0.452 +- 0.260
mrr vals (pred, true): 0.207, 0.000
batch losses (mrrl, rdl): 0.0, 2.02226e-05

Epoch over!
epoch time: 14.888

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 708
rank avg (pred): 0.465 +- 0.276
mrr vals (pred, true): 0.203, 0.002
batch losses (mrrl, rdl): 0.0, 6.97402e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 267
rank avg (pred): 0.265 +- 0.168
mrr vals (pred, true): 0.249, 0.328
batch losses (mrrl, rdl): 0.0, 0.0004255228

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 597
rank avg (pred): 0.458 +- 0.291
mrr vals (pred, true): 0.226, 0.000
batch losses (mrrl, rdl): 0.0, 9.3279e-06

Epoch over!
epoch time: 15.006

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 721
rank avg (pred): 0.464 +- 0.292
mrr vals (pred, true): 0.219, 0.000
batch losses (mrrl, rdl): 0.0, 3.50053e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.450 +- 0.296
mrr vals (pred, true): 0.228, 0.000
batch losses (mrrl, rdl): 0.0, 1.71947e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.443 +- 0.293
mrr vals (pred, true): 0.230, 0.000
batch losses (mrrl, rdl): 0.0, 1.22373e-05

Epoch over!
epoch time: 14.997

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.450 +- 0.289
mrr vals (pred, true): 0.202, 0.000
batch losses (mrrl, rdl): 0.0, 2.24836e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 112
rank avg (pred): 0.457 +- 0.296
mrr vals (pred, true): 0.208, 0.001
batch losses (mrrl, rdl): 0.0, 9.4756e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.444 +- 0.297
mrr vals (pred, true): 0.189, 0.000
batch losses (mrrl, rdl): 0.0, 1.08674e-05

Epoch over!
epoch time: 15.016

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 810
rank avg (pred): 0.137 +- 0.099
mrr vals (pred, true): 0.227, 0.297
batch losses (mrrl, rdl): 0.0483267754, 6.32999e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 706
rank avg (pred): 0.630 +- 0.197
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.21043e-05, 0.0004881993

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 62
rank avg (pred): 0.395 +- 0.237
mrr vals (pred, true): 0.068, 0.064
batch losses (mrrl, rdl): 0.0033630296, 0.0002282659

Epoch over!
epoch time: 15.204

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.605 +- 0.241
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 8.88423e-05, 0.0003163348

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.245 +- 0.199
mrr vals (pred, true): 0.072, 0.132
batch losses (mrrl, rdl): 0.0362662934, 0.0002893704

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 659
rank avg (pred): 0.681 +- 0.238
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.41616e-05, 0.0007420374

Epoch over!
epoch time: 15.15

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.641 +- 0.258
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.351e-07, 0.0001558649

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 825
rank avg (pred): 0.039 +- 0.143
mrr vals (pred, true): 0.167, 0.261
batch losses (mrrl, rdl): 0.0891178548, 0.0005625497

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1001
rank avg (pred): 0.612 +- 0.256
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003028041, 0.0002759735

Epoch over!
epoch time: 15.164

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1175
rank avg (pred): 0.644 +- 0.260
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.18538e-05, 0.0005432979

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.594 +- 0.265
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.22508e-05, 0.0002100788

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 157
rank avg (pred): 0.608 +- 0.267
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.2977e-06, 0.0001689225

Epoch over!
epoch time: 15.084

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 558
rank avg (pred): 0.441 +- 0.253
mrr vals (pred, true): 0.068, 0.078
batch losses (mrrl, rdl): 0.0033241257, 6.62938e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 450
rank avg (pred): 0.583 +- 0.280
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 3.8e-09, 0.0001184968

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1006
rank avg (pred): 0.525 +- 0.253
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 2.89101e-05, 6.72432e-05

Epoch over!
epoch time: 15.161

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.324 +- 0.237
mrr vals (pred, true): 0.061, 0.072
batch losses (mrrl, rdl): 0.0011734159, 4.78484e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.523 +- 0.253
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.3183e-06, 3.42339e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 939
rank avg (pred): 0.575 +- 0.271
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.3395e-05, 1.7608e-05

Epoch over!
epoch time: 15.141

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.455 +- 0.209
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.21286e-05, 6.18272e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 925
rank avg (pred): 0.544 +- 0.226
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001420968, 1.67551e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 594
rank avg (pred): 0.560 +- 0.290
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.29905e-05, 5.0661e-06

Epoch over!
epoch time: 14.959

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.028 +- 0.153
mrr vals (pred, true): 0.271, 0.317
batch losses (mrrl, rdl): 0.0211044624, 9.7883e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.042 +- 0.187
mrr vals (pred, true): 0.276, 0.350
batch losses (mrrl, rdl): 0.05465547, 0.0001235311

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.541 +- 0.286
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.32736e-05, 5.52426e-05

Epoch over!
epoch time: 14.96

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.527 +- 0.266
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.011e-07, 1.33976e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1053
rank avg (pred): 0.241 +- 0.223
mrr vals (pred, true): 0.178, 0.115
batch losses (mrrl, rdl): 0.0385726243, 0.0001889596

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 482
rank avg (pred): 0.457 +- 0.335
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 4.5309e-06, 0.0001254822

Epoch over!
epoch time: 14.943

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 435
rank avg (pred): 0.542 +- 0.300
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.1177e-06, 4.96159e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 840
rank avg (pred): 0.494 +- 0.262
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.28389e-05, 2.01158e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 85
rank avg (pred): 0.543 +- 0.314
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.64209e-05, 5.24618e-05

Epoch over!
epoch time: 14.934

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.559 +- 0.303
mrr vals (pred, true): 0.049, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04886 	 0.00015 	 m..s
   44 	     1 	 0.04976 	 0.00015 	 m..s
   38 	     2 	 0.04954 	 0.00016 	 m..s
    2 	     3 	 0.04884 	 0.00017 	 m..s
   78 	     4 	 0.05025 	 0.00017 	 m..s
   20 	     5 	 0.04894 	 0.00017 	 m..s
   49 	     6 	 0.05022 	 0.00017 	 m..s
   49 	     7 	 0.05022 	 0.00017 	 m..s
   27 	     8 	 0.04903 	 0.00018 	 m..s
   49 	     9 	 0.05022 	 0.00018 	 m..s
   49 	    10 	 0.05022 	 0.00019 	 m..s
   49 	    11 	 0.05022 	 0.00019 	 m..s
    1 	    12 	 0.04879 	 0.00020 	 m..s
    0 	    13 	 0.04854 	 0.00020 	 m..s
   42 	    14 	 0.04968 	 0.00020 	 m..s
   49 	    15 	 0.05022 	 0.00020 	 m..s
   18 	    16 	 0.04891 	 0.00021 	 m..s
   49 	    17 	 0.05022 	 0.00021 	 m..s
    8 	    18 	 0.04886 	 0.00021 	 m..s
   41 	    19 	 0.04967 	 0.00022 	 m..s
   17 	    20 	 0.04891 	 0.00022 	 m..s
   22 	    21 	 0.04899 	 0.00023 	 m..s
   26 	    22 	 0.04901 	 0.00023 	 m..s
   40 	    23 	 0.04962 	 0.00023 	 m..s
   49 	    24 	 0.05022 	 0.00023 	 m..s
   43 	    25 	 0.04972 	 0.00023 	 m..s
   49 	    26 	 0.05022 	 0.00023 	 m..s
   85 	    27 	 0.05070 	 0.00023 	 m..s
   49 	    28 	 0.05022 	 0.00024 	 m..s
   12 	    29 	 0.04887 	 0.00024 	 m..s
  100 	    30 	 0.06603 	 0.00025 	 m..s
   33 	    31 	 0.04941 	 0.00025 	 m..s
    5 	    32 	 0.04886 	 0.00026 	 m..s
   14 	    33 	 0.04888 	 0.00026 	 m..s
    9 	    34 	 0.04887 	 0.00026 	 m..s
   90 	    35 	 0.05267 	 0.00027 	 m..s
   23 	    36 	 0.04899 	 0.00027 	 m..s
   15 	    37 	 0.04890 	 0.00027 	 m..s
   36 	    38 	 0.04951 	 0.00027 	 m..s
   46 	    39 	 0.05000 	 0.00028 	 m..s
   49 	    40 	 0.05022 	 0.00028 	 m..s
   49 	    41 	 0.05022 	 0.00028 	 m..s
   49 	    42 	 0.05022 	 0.00029 	 m..s
   87 	    43 	 0.05095 	 0.00029 	 m..s
   37 	    44 	 0.04952 	 0.00029 	 m..s
   88 	    45 	 0.05102 	 0.00029 	 m..s
    7 	    46 	 0.04886 	 0.00030 	 m..s
   49 	    47 	 0.05022 	 0.00030 	 m..s
   49 	    48 	 0.05022 	 0.00032 	 m..s
    3 	    49 	 0.04884 	 0.00032 	 m..s
   24 	    50 	 0.04900 	 0.00034 	 m..s
  101 	    51 	 0.06642 	 0.00036 	 m..s
   39 	    52 	 0.04958 	 0.00037 	 m..s
   16 	    53 	 0.04890 	 0.00038 	 m..s
   91 	    54 	 0.05417 	 0.00038 	 m..s
    4 	    55 	 0.04886 	 0.00038 	 m..s
   21 	    56 	 0.04898 	 0.00041 	 m..s
   35 	    57 	 0.04949 	 0.00041 	 m..s
   47 	    58 	 0.05012 	 0.00041 	 m..s
   49 	    59 	 0.05022 	 0.00041 	 m..s
   48 	    60 	 0.05021 	 0.00043 	 m..s
   30 	    61 	 0.04926 	 0.00045 	 m..s
   28 	    62 	 0.04917 	 0.00045 	 m..s
   49 	    63 	 0.05022 	 0.00046 	 m..s
   92 	    64 	 0.05576 	 0.00046 	 m..s
   49 	    65 	 0.05022 	 0.00046 	 m..s
   31 	    66 	 0.04926 	 0.00047 	 m..s
   11 	    67 	 0.04887 	 0.00055 	 m..s
   49 	    68 	 0.05022 	 0.00056 	 m..s
   49 	    69 	 0.05022 	 0.00060 	 m..s
   13 	    70 	 0.04887 	 0.00060 	 m..s
   29 	    71 	 0.04918 	 0.00064 	 m..s
   19 	    72 	 0.04893 	 0.00065 	 m..s
   45 	    73 	 0.04999 	 0.00065 	 m..s
   49 	    74 	 0.05022 	 0.00066 	 m..s
   49 	    75 	 0.05022 	 0.00068 	 m..s
   49 	    76 	 0.05022 	 0.00069 	 m..s
   25 	    77 	 0.04901 	 0.00070 	 m..s
   49 	    78 	 0.05022 	 0.00074 	 m..s
   49 	    79 	 0.05022 	 0.00075 	 m..s
   49 	    80 	 0.05022 	 0.00080 	 m..s
   49 	    81 	 0.05022 	 0.00089 	 m..s
   49 	    82 	 0.05022 	 0.00098 	 m..s
   34 	    83 	 0.04948 	 0.00103 	 m..s
   32 	    84 	 0.04935 	 0.00112 	 m..s
   10 	    85 	 0.04887 	 0.00134 	 m..s
   49 	    86 	 0.05022 	 0.00177 	 m..s
   89 	    87 	 0.05159 	 0.02420 	 ~...
  120 	    88 	 0.23526 	 0.05909 	 MISS
   79 	    89 	 0.05030 	 0.07525 	 ~...
   84 	    90 	 0.05069 	 0.07577 	 ~...
   93 	    91 	 0.05749 	 0.07655 	 ~...
   82 	    92 	 0.05048 	 0.07830 	 ~...
   97 	    93 	 0.06247 	 0.08121 	 ~...
   86 	    94 	 0.05081 	 0.08157 	 m..s
  109 	    95 	 0.13806 	 0.08631 	 m..s
   81 	    96 	 0.05040 	 0.08778 	 m..s
   94 	    97 	 0.05768 	 0.08788 	 m..s
  103 	    98 	 0.07347 	 0.09133 	 ~...
  104 	    99 	 0.08163 	 0.09184 	 ~...
   83 	   100 	 0.05051 	 0.09694 	 m..s
   95 	   101 	 0.06175 	 0.10273 	 m..s
   99 	   102 	 0.06506 	 0.10641 	 m..s
   98 	   103 	 0.06293 	 0.11293 	 m..s
   96 	   104 	 0.06205 	 0.11365 	 m..s
  111 	   105 	 0.14479 	 0.11446 	 m..s
  102 	   106 	 0.06791 	 0.11512 	 m..s
  106 	   107 	 0.10307 	 0.11644 	 ~...
  110 	   108 	 0.14457 	 0.11990 	 ~...
   80 	   109 	 0.05039 	 0.12593 	 m..s
  105 	   110 	 0.10137 	 0.14927 	 m..s
  113 	   111 	 0.15904 	 0.15283 	 ~...
  107 	   112 	 0.11158 	 0.17092 	 m..s
  108 	   113 	 0.11479 	 0.17666 	 m..s
  115 	   114 	 0.17964 	 0.18054 	 ~...
  116 	   115 	 0.18791 	 0.26141 	 m..s
  117 	   116 	 0.20910 	 0.31338 	 MISS
  114 	   117 	 0.16801 	 0.31471 	 MISS
  119 	   118 	 0.23333 	 0.31478 	 m..s
  112 	   119 	 0.15195 	 0.31824 	 MISS
  118 	   120 	 0.22345 	 0.33208 	 MISS
==========================================
r_mrr = 0.8361465334892273
r2_mrr = 0.49094557762145996
spearmanr_mrr@5 = 0.6548486948013306
spearmanr_mrr@10 = 0.9237563610076904
spearmanr_mrr@50 = 0.939529299736023
spearmanr_mrr@100 = 0.9455680251121521
spearmanr_mrr@All = 0.9472346305847168
==========================================
test time: 0.45
Done Testing dataset DBpedia50
total time taken: 231.3578164577484
training time taken: 225.92432141304016
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8361)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.4909)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.6548)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9238)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9395)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9456)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9472)}}, 'test_loss': {'ComplEx': {'DBpedia50': 1.5762394233770465}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 5089547278025486
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [787, 937, 29, 301, 1070, 635, 927, 196, 1115, 525, 300, 312, 736, 928, 889, 864, 298, 43, 52, 350, 1173, 535, 975, 554, 35, 817, 254, 463, 464, 1072, 417, 957, 800, 979, 452, 720, 209, 946, 863, 802, 1027, 398, 728, 1004, 219, 1006, 44, 12, 696, 263, 580, 100, 998, 530, 200, 467, 461, 865, 1016, 793, 965, 485, 1021, 1048, 950, 1212, 241, 949, 477, 476, 259, 275, 658, 792, 724, 1182, 282, 67, 997, 407, 506, 90, 95, 687, 992, 1064, 829, 905, 146, 977, 909, 166, 502, 544, 897, 171, 1189, 1158, 370, 754, 212, 152, 405, 118, 1118, 652, 1053, 679, 164, 1124, 468, 732, 240, 874, 859, 1194, 27, 1161, 1208, 547, 710]
valid_ids (0): []
train_ids (1094): [1008, 30, 264, 1040, 429, 1199, 83, 938, 511, 684, 879, 908, 1096, 1082, 235, 780, 339, 840, 1170, 430, 46, 790, 717, 126, 40, 640, 133, 990, 424, 1176, 986, 344, 584, 685, 92, 379, 285, 872, 267, 329, 594, 1002, 423, 1147, 167, 42, 862, 366, 17, 248, 1139, 626, 674, 1045, 903, 1063, 216, 120, 70, 1140, 1135, 910, 215, 712, 1203, 891, 58, 701, 314, 636, 286, 678, 1132, 1191, 898, 289, 776, 336, 822, 617, 706, 139, 884, 1019, 615, 586, 596, 1069, 165, 994, 272, 278, 1011, 236, 1037, 565, 311, 604, 351, 80, 831, 832, 338, 772, 1138, 522, 875, 1054, 136, 1177, 395, 188, 337, 915, 825, 1155, 1046, 322, 589, 1012, 28, 142, 764, 406, 471, 265, 1166, 99, 688, 116, 1156, 914, 475, 119, 361, 873, 644, 624, 537, 1101, 75, 797, 266, 576, 82, 552, 608, 693, 974, 297, 667, 437, 291, 761, 945, 107, 442, 33, 926, 602, 976, 184, 4, 447, 129, 571, 474, 1157, 354, 15, 1078, 1003, 357, 1030, 819, 1092, 854, 1085, 996, 32, 671, 560, 1091, 1029, 849, 161, 1175, 936, 655, 971, 381, 670, 798, 252, 697, 750, 1071, 203, 543, 597, 871, 402, 848, 568, 93, 513, 1039, 175, 1129, 587, 923, 751, 548, 741, 707, 450, 1133, 1007, 434, 162, 661, 1026, 440, 220, 823, 489, 108, 669, 1162, 987, 230, 1080, 851, 1214, 686, 494, 526, 607, 614, 238, 838, 705, 319, 247, 900, 359, 834, 155, 112, 746, 722, 193, 1009, 585, 813, 618, 555, 642, 677, 1094, 1160, 1035, 833, 714, 572, 1113, 492, 20, 190, 493, 951, 625, 1109, 699, 84, 270, 377, 481, 305, 341, 536, 31, 76, 364, 1130, 1184, 963, 540, 432, 887, 723, 153, 7, 791, 683, 13, 556, 721, 262, 308, 806, 1144, 426, 953, 1081, 61, 244, 783, 211, 1110, 404, 331, 105, 482, 490, 803, 656, 326, 205, 748, 156, 151, 726, 828, 258, 392, 878, 1057, 245, 564, 1017, 747, 738, 961, 389, 170, 1056, 1152, 601, 606, 553, 91, 892, 154, 634, 514, 1127, 869, 98, 342, 982, 866, 826, 113, 520, 901, 456, 1098, 470, 78, 1028, 56, 436, 518, 9, 1159, 824, 969, 427, 1151, 836, 808, 466, 478, 559, 956, 39, 1204, 373, 340, 49, 315, 692, 599, 8, 104, 302, 666, 74, 1117, 399, 804, 268, 630, 645, 1186, 72, 277, 89, 138, 899, 1126, 149, 16, 85, 296, 504, 144, 1076, 242, 307, 411, 130, 169, 1005, 433, 207, 659, 919, 657, 993, 197, 579, 206, 561, 1095, 739, 391, 959, 810, 71, 185, 1051, 1116, 922, 1104, 542, 1202, 11, 182, 766, 400, 1020, 249, 742, 283, 106, 1042, 501, 284, 718, 1099, 1043, 509, 88, 978, 1197, 725, 622, 1145, 348, 360, 558, 852, 676, 1185, 292, 94, 1136, 1024, 79, 893, 1150, 911, 274, 860, 327, 243, 228, 36, 480, 745, 1196, 256, 195, 1108, 439, 566, 1014, 1084, 970, 1163, 346, 163, 512, 650, 942, 664, 310, 168, 773, 68, 631, 369, 794, 421, 737, 1167, 66, 435, 647, 1119, 371, 653, 610, 223, 839, 309, 409, 985, 181, 1210, 469, 173, 455, 251, 306, 958, 21, 947, 1060, 287, 719, 498, 820, 855, 1062, 204, 416, 782, 499, 1088, 59, 397, 811, 96, 174, 713, 1077, 816, 763, 87, 330, 114, 1065, 1206, 966, 313, 731, 1190, 48, 867, 145, 62, 462, 18, 261, 1052, 1105, 562, 288, 532, 964, 807, 821, 883, 488, 1066, 902, 396, 637, 147, 1122, 765, 1154, 529, 385, 744, 131, 178, 229, 2, 378, 122, 609, 269, 38, 941, 708, 47, 743, 510, 595, 449, 621, 60, 524, 1025, 408, 213, 1022, 214, 1038, 293, 110, 1142, 1106, 1174, 629, 913, 519, 1180, 53, 809, 255, 387, 827, 487, 441, 375, 574, 704, 63, 128, 183, 186, 775, 380, 702, 843, 1153, 124, 210, 1183, 343, 534, 6, 1067, 567, 50, 727, 317, 1033, 231, 358, 237, 176, 217, 598, 944, 443, 1079, 202, 638, 208, 904, 955, 557, 25, 5, 1121, 845, 363, 1165, 374, 578, 785, 491, 691, 453, 115, 1141, 352, 1083, 222, 694, 273, 581, 1049, 14, 137, 22, 1172, 844, 316, 0, 225, 103, 189, 425, 10, 460, 523, 299, 495, 246, 479, 393, 592, 451, 973, 730, 484, 668, 778, 279, 716, 888, 508, 324, 1146, 454, 500, 709, 698, 117, 861, 1131, 199, 1086, 734, 984, 1050, 896, 980, 1198, 528, 431, 989, 916, 842, 349, 952, 332, 445, 1125, 224, 295, 880, 675, 158, 1111, 24, 894, 962, 760, 774, 386, 538, 740, 1213, 649, 260, 886, 157, 541, 1181, 895, 132, 715, 603, 383, 179, 907, 935, 77, 496, 280, 777, 700, 912, 682, 1100, 57, 367, 192, 931, 818, 34, 786, 125, 753, 943, 355, 1195, 1089, 1059, 680, 23, 323, 41, 665, 570, 353, 65, 384, 403, 801, 483, 198, 1075, 815, 805, 177, 102, 639, 218, 590, 835, 194, 917, 1207, 1, 444, 1044, 372, 1032, 140, 752, 1087, 465, 613, 1097, 1034, 1102, 660, 633, 616, 846, 939, 1192, 473, 45, 1010, 575, 521, 812, 516, 253, 159, 233, 967, 611, 779, 1178, 1058, 250, 303, 870, 876, 1187, 376, 632, 507, 853, 413, 881, 1164, 232, 1023, 81, 733, 160, 382, 134, 459, 769, 26, 191, 318, 438, 51, 837, 281, 290, 527, 1073, 918, 1090, 755, 428, 988, 294, 1000, 549, 54, 623, 172, 954, 583, 1001, 593, 410, 304, 968, 1068, 847, 588, 73, 1031, 711, 472, 930, 135, 868, 550, 109, 148, 651, 1103, 420, 619, 1201, 226, 781, 858, 789, 345, 187, 127, 995, 681, 690, 69, 768, 1209, 320, 1061, 759, 257, 394, 531, 486, 356, 920, 505, 627, 1211, 1123, 612, 1200, 271, 1137, 933, 448, 762, 150, 960, 757, 857, 663, 458, 551, 422, 758, 885, 546, 628, 227, 703, 457, 569, 515, 830, 1179, 221, 1193, 591, 418, 1047, 662, 934, 111, 850, 412, 1149, 121, 771, 648, 1055, 276, 695, 1143, 890, 201, 333, 1188, 925, 924, 999, 563, 335, 334, 365, 654, 981, 646, 643, 321, 1120, 795, 503, 767, 1205, 770, 388, 37, 1107, 991, 1036, 756, 882, 1171, 419, 673, 641, 972, 1013, 784, 414, 390, 180, 1015, 1128, 735, 796, 689, 1074, 906, 921, 573, 856, 19, 1093, 1041, 347, 814, 1168, 55, 841, 545, 940, 517, 788, 948, 729, 577, 368, 1148, 3, 86, 446, 605, 582, 97, 929, 141, 362, 415, 1018, 1134, 401, 239, 600, 328, 799, 1169, 234, 325, 497, 1114, 101, 983, 749, 1112, 932, 533, 539, 143, 620, 672, 123, 64, 877]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5143361267777201
the save name prefix for this run is:  chkpt-ID_5143361267777201_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 761
rank avg (pred): 0.516 +- 0.004
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002019507

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 454
rank avg (pred): 0.464 +- 0.265
mrr vals (pred, true): 0.136, 0.000
batch losses (mrrl, rdl): 0.0, 7.9063e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1171
rank avg (pred): 0.473 +- 0.280
mrr vals (pred, true): 0.171, 0.000
batch losses (mrrl, rdl): 0.0, 1.07629e-05

Epoch over!
epoch time: 14.943

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.350 +- 0.210
mrr vals (pred, true): 0.183, 0.087
batch losses (mrrl, rdl): 0.0, 9.45715e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 549
rank avg (pred): 0.354 +- 0.214
mrr vals (pred, true): 0.208, 0.108
batch losses (mrrl, rdl): 0.0, 4.53221e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 674
rank avg (pred): 0.470 +- 0.284
mrr vals (pred, true): 0.196, 0.001
batch losses (mrrl, rdl): 0.0, 1.16492e-05

Epoch over!
epoch time: 14.952

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 225
rank avg (pred): 0.457 +- 0.274
mrr vals (pred, true): 0.190, 0.000
batch losses (mrrl, rdl): 0.0, 2.74378e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 852
rank avg (pred): 0.433 +- 0.263
mrr vals (pred, true): 0.230, 0.000
batch losses (mrrl, rdl): 0.0, 8.60721e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 441
rank avg (pred): 0.459 +- 0.277
mrr vals (pred, true): 0.214, 0.000
batch losses (mrrl, rdl): 0.0, 2.41761e-05

Epoch over!
epoch time: 14.943

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 560
rank avg (pred): 0.314 +- 0.191
mrr vals (pred, true): 0.234, 0.061
batch losses (mrrl, rdl): 0.0, 0.0001235431

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 513
rank avg (pred): 0.351 +- 0.213
mrr vals (pred, true): 0.229, 0.096
batch losses (mrrl, rdl): 0.0, 2.88082e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.439 +- 0.266
mrr vals (pred, true): 0.234, 0.001
batch losses (mrrl, rdl): 0.0, 3.05668e-05

Epoch over!
epoch time: 14.966

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.461 +- 0.279
mrr vals (pred, true): 0.235, 0.001
batch losses (mrrl, rdl): 0.0, 1.24745e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 980
rank avg (pred): 0.309 +- 0.188
mrr vals (pred, true): 0.250, 0.177
batch losses (mrrl, rdl): 0.0, 0.0001524168

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.463 +- 0.281
mrr vals (pred, true): 0.244, 0.000
batch losses (mrrl, rdl): 0.0, 1.48773e-05

Epoch over!
epoch time: 14.967

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.340 +- 0.206
mrr vals (pred, true): 0.250, 0.091
batch losses (mrrl, rdl): 0.3989410996, 8.38516e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 353
rank avg (pred): 0.553 +- 0.251
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.43161e-05, 0.000289799

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.252 +- 0.130
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0037933665, 0.0011501921

Epoch over!
epoch time: 15.157

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.548 +- 0.261
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003854249, 0.0001345791

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 256
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.204, 0.318
batch losses (mrrl, rdl): 0.131517157, 0.0005228997

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 183
rank avg (pred): 0.569 +- 0.231
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.03216e-05, 0.0002216431

Epoch over!
epoch time: 15.126

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 551
rank avg (pred): 0.546 +- 0.264
mrr vals (pred, true): 0.051, 0.078
batch losses (mrrl, rdl): 1.00349e-05, 0.0006456629

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.006 +- 0.003
mrr vals (pred, true): 0.185, 0.200
batch losses (mrrl, rdl): 0.0023803073, 0.0010494844

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 639
rank avg (pred): 0.523 +- 0.248
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.73035e-05, 0.000132898

Epoch over!
epoch time: 15.155

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 622
rank avg (pred): 0.522 +- 0.255
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.25e-08, 0.0001340792

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 902
rank avg (pred): 0.328 +- 0.249
mrr vals (pred, true): 0.067, 0.005
batch losses (mrrl, rdl): 0.0027455939, 0.000610147

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 328
rank avg (pred): 0.502 +- 0.255
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.83e-07, 6.10004e-05

Epoch over!
epoch time: 15.142

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.510 +- 0.255
mrr vals (pred, true): 0.050, 0.094
batch losses (mrrl, rdl): 1.5138e-06, 0.00044852

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 620
rank avg (pred): 0.507 +- 0.252
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.4204e-06, 1.95853e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.589 +- 0.141
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.47779e-05, 0.0002362507

Epoch over!
epoch time: 15.045

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 674
rank avg (pred): 0.514 +- 0.235
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 9.9912e-06, 4.99892e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 390
rank avg (pred): 0.524 +- 0.236
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 1.11177e-05, 0.0001245191

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 234
rank avg (pred): 0.587 +- 0.135
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 6.95569e-05, 0.0003496528

Epoch over!
epoch time: 15.177

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.469 +- 0.353
mrr vals (pred, true): 0.082, 0.014
batch losses (mrrl, rdl): 0.0103482604, 0.0004464259

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 556
rank avg (pred): 0.465 +- 0.245
mrr vals (pred, true): 0.050, 0.079
batch losses (mrrl, rdl): 1.694e-06, 0.0002505956

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.480 +- 0.307
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002115259, 1.1082e-05

Epoch over!
epoch time: 15.143

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 587
rank avg (pred): 0.510 +- 0.206
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.16966e-05, 4.63672e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 367
rank avg (pred): 0.490 +- 0.224
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 7.1103e-06, 2.48532e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 18
rank avg (pred): 0.216 +- 0.142
mrr vals (pred, true): 0.078, 0.000
batch losses (mrrl, rdl): 0.0077381823, 0.0012348066

Epoch over!
epoch time: 15.156

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.458 +- 0.240
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 6.3441e-06, 2.2798e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 21
rank avg (pred): 0.210 +- 0.129
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002230196, 0.0011593883

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 960
rank avg (pred): 0.465 +- 0.297
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 2.89961e-05, 4.59174e-05

Epoch over!
epoch time: 15.165

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 791
rank avg (pred): 0.527 +- 0.165
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.95072e-05, 0.0001066037

running batch: 500 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 871
rank avg (pred): 0.543 +- 0.364
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.65103e-05, 2.98937e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 572
rank avg (pred): 0.476 +- 0.204
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.02756e-05, 7.13465e-05

Epoch over!
epoch time: 15.161

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.495 +- 0.186
mrr vals (pred, true): 0.049, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  102 	     0 	 0.08976 	 6e-0500 	 m..s
   21 	     1 	 0.04892 	 0.00014 	 m..s
   51 	     2 	 0.04939 	 0.00014 	 m..s
   25 	     3 	 0.04897 	 0.00015 	 m..s
   16 	     4 	 0.04889 	 0.00016 	 m..s
   17 	     5 	 0.04890 	 0.00017 	 m..s
   36 	     6 	 0.04905 	 0.00017 	 m..s
   34 	     7 	 0.04903 	 0.00017 	 m..s
   60 	     8 	 0.05111 	 0.00018 	 m..s
    4 	     9 	 0.04785 	 0.00018 	 m..s
   26 	    10 	 0.04897 	 0.00020 	 m..s
   63 	    11 	 0.05191 	 0.00020 	 m..s
   30 	    12 	 0.04898 	 0.00020 	 m..s
   46 	    13 	 0.04927 	 0.00020 	 m..s
   29 	    14 	 0.04898 	 0.00020 	 m..s
   62 	    15 	 0.05160 	 0.00020 	 m..s
   63 	    16 	 0.05191 	 0.00021 	 m..s
   86 	    17 	 0.05444 	 0.00021 	 m..s
   91 	    18 	 0.05483 	 0.00021 	 m..s
   23 	    19 	 0.04895 	 0.00022 	 m..s
   63 	    20 	 0.05191 	 0.00022 	 m..s
   32 	    21 	 0.04903 	 0.00022 	 m..s
    2 	    22 	 0.04762 	 0.00022 	 m..s
   82 	    23 	 0.05318 	 0.00022 	 m..s
   38 	    24 	 0.04910 	 0.00023 	 m..s
   63 	    25 	 0.05191 	 0.00023 	 m..s
   98 	    26 	 0.06369 	 0.00023 	 m..s
   63 	    27 	 0.05191 	 0.00023 	 m..s
   10 	    28 	 0.04835 	 0.00023 	 m..s
   87 	    29 	 0.05454 	 0.00023 	 m..s
    7 	    30 	 0.04805 	 0.00024 	 m..s
   57 	    31 	 0.05005 	 0.00024 	 m..s
   89 	    32 	 0.05474 	 0.00025 	 m..s
   63 	    33 	 0.05191 	 0.00025 	 m..s
   63 	    34 	 0.05191 	 0.00025 	 m..s
   83 	    35 	 0.05320 	 0.00026 	 m..s
    6 	    36 	 0.04800 	 0.00026 	 m..s
   76 	    37 	 0.05209 	 0.00026 	 m..s
   81 	    38 	 0.05285 	 0.00026 	 m..s
   28 	    39 	 0.04898 	 0.00027 	 m..s
   49 	    40 	 0.04929 	 0.00028 	 m..s
   27 	    41 	 0.04898 	 0.00029 	 m..s
   12 	    42 	 0.04841 	 0.00029 	 m..s
   18 	    43 	 0.04892 	 0.00029 	 m..s
   35 	    44 	 0.04905 	 0.00031 	 m..s
   15 	    45 	 0.04857 	 0.00031 	 m..s
   24 	    46 	 0.04896 	 0.00033 	 m..s
   77 	    47 	 0.05216 	 0.00033 	 m..s
   37 	    48 	 0.04909 	 0.00034 	 m..s
    9 	    49 	 0.04833 	 0.00035 	 m..s
   11 	    50 	 0.04840 	 0.00036 	 m..s
   63 	    51 	 0.05191 	 0.00036 	 m..s
    8 	    52 	 0.04806 	 0.00038 	 m..s
   63 	    53 	 0.05191 	 0.00038 	 m..s
   19 	    54 	 0.04892 	 0.00039 	 m..s
   39 	    55 	 0.04911 	 0.00040 	 m..s
    3 	    56 	 0.04783 	 0.00040 	 m..s
   79 	    57 	 0.05262 	 0.00040 	 m..s
   33 	    58 	 0.04903 	 0.00041 	 m..s
   14 	    59 	 0.04846 	 0.00041 	 m..s
    0 	    60 	 0.04645 	 0.00043 	 m..s
   52 	    61 	 0.04945 	 0.00043 	 m..s
   63 	    62 	 0.05191 	 0.00045 	 m..s
    5 	    63 	 0.04786 	 0.00046 	 m..s
   85 	    64 	 0.05341 	 0.00046 	 m..s
   22 	    65 	 0.04894 	 0.00046 	 m..s
   78 	    66 	 0.05226 	 0.00047 	 m..s
   80 	    67 	 0.05278 	 0.00048 	 m..s
   63 	    68 	 0.05191 	 0.00049 	 m..s
   20 	    69 	 0.04892 	 0.00049 	 m..s
   44 	    70 	 0.04924 	 0.00050 	 m..s
   47 	    71 	 0.04927 	 0.00050 	 m..s
   61 	    72 	 0.05152 	 0.00052 	 m..s
   90 	    73 	 0.05478 	 0.00054 	 m..s
   48 	    74 	 0.04928 	 0.00064 	 m..s
   13 	    75 	 0.04843 	 0.00068 	 m..s
    1 	    76 	 0.04741 	 0.00074 	 m..s
   63 	    77 	 0.05191 	 0.00087 	 m..s
   31 	    78 	 0.04901 	 0.00113 	 m..s
   55 	    79 	 0.04975 	 0.00117 	 m..s
   92 	    80 	 0.05603 	 0.00134 	 m..s
   63 	    81 	 0.05191 	 0.00220 	 m..s
  115 	    82 	 0.22293 	 0.05909 	 MISS
   42 	    83 	 0.04922 	 0.06354 	 ~...
   54 	    84 	 0.04970 	 0.07821 	 ~...
   56 	    85 	 0.04979 	 0.07831 	 ~...
   58 	    86 	 0.05020 	 0.07878 	 ~...
   97 	    87 	 0.06039 	 0.08220 	 ~...
   53 	    88 	 0.04957 	 0.08729 	 m..s
  100 	    89 	 0.07636 	 0.08786 	 ~...
   84 	    90 	 0.05337 	 0.08788 	 m..s
   95 	    91 	 0.05713 	 0.09414 	 m..s
  101 	    92 	 0.08414 	 0.09443 	 ~...
   88 	    93 	 0.05468 	 0.09448 	 m..s
   45 	    94 	 0.04924 	 0.09778 	 m..s
   59 	    95 	 0.05069 	 0.10153 	 m..s
   50 	    96 	 0.04933 	 0.10564 	 m..s
   41 	    97 	 0.04918 	 0.10832 	 m..s
   93 	    98 	 0.05637 	 0.10918 	 m..s
   94 	    99 	 0.05638 	 0.11293 	 m..s
   40 	   100 	 0.04917 	 0.11351 	 m..s
  113 	   101 	 0.18058 	 0.11548 	 m..s
   96 	   102 	 0.05848 	 0.11931 	 m..s
   43 	   103 	 0.04923 	 0.12159 	 m..s
  103 	   104 	 0.10098 	 0.12624 	 ~...
  106 	   105 	 0.10964 	 0.12667 	 ~...
  108 	   106 	 0.12241 	 0.13681 	 ~...
   99 	   107 	 0.06550 	 0.14574 	 m..s
  107 	   108 	 0.11852 	 0.14659 	 ~...
  105 	   109 	 0.10831 	 0.14671 	 m..s
  109 	   110 	 0.13258 	 0.15115 	 ~...
  104 	   111 	 0.10198 	 0.16359 	 m..s
  111 	   112 	 0.16228 	 0.17092 	 ~...
  114 	   113 	 0.20936 	 0.17303 	 m..s
  110 	   114 	 0.15697 	 0.18675 	 ~...
  112 	   115 	 0.16610 	 0.21140 	 m..s
  116 	   116 	 0.24501 	 0.22246 	 ~...
  117 	   117 	 0.27870 	 0.29755 	 ~...
  120 	   118 	 0.30883 	 0.31828 	 ~...
  119 	   119 	 0.29820 	 0.35643 	 m..s
  118 	   120 	 0.29737 	 0.38067 	 m..s
==========================================
r_mrr = 0.8468273878097534
r2_mrr = 0.5908529758453369
spearmanr_mrr@5 = 0.9705017805099487
spearmanr_mrr@10 = 0.9592291116714478
spearmanr_mrr@50 = 0.9171679615974426
spearmanr_mrr@100 = 0.91230309009552
spearmanr_mrr@All = 0.9144037365913391
==========================================
test time: 0.452
Done Testing dataset DBpedia50
total time taken: 232.004563331604
training time taken: 226.6604745388031
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'DBpedia50': tensor(0.8468)}}, 'r2_mrr': {'ComplEx': {'DBpedia50': tensor(0.5909)}}, 'spearmanr_mrr@5': {'ComplEx': {'DBpedia50': tensor(0.9705)}}, 'spearmanr_mrr@10': {'ComplEx': {'DBpedia50': tensor(0.9592)}}, 'spearmanr_mrr@50': {'ComplEx': {'DBpedia50': tensor(0.9172)}}, 'spearmanr_mrr@100': {'ComplEx': {'DBpedia50': tensor(0.9123)}}, 'spearmanr_mrr@All': {'ComplEx': {'DBpedia50': tensor(0.9144)}}, 'test_loss': {'ComplEx': {'DBpedia50': 0.9944475421907555}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 6439349733452592
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1115, 72, 489, 123, 780, 1133, 210, 1157, 831, 343, 192, 488, 872, 898, 274, 233, 736, 185, 29, 1177, 27, 885, 527, 33, 1087, 77, 868, 773, 995, 615, 850, 795, 818, 589, 522, 292, 925, 289, 573, 679, 973, 716, 713, 953, 971, 86, 164, 804, 314, 769, 1207, 17, 845, 1070, 1111, 191, 892, 665, 869, 179, 173, 1125, 705, 586, 680, 59, 268, 658, 1052, 864, 266, 1180, 767, 372, 799, 766, 1167, 67, 440, 649, 512, 463, 98, 295, 690, 190, 419, 281, 371, 442, 1056, 778, 65, 1124, 836, 1026, 265, 968, 507, 1140, 770, 249, 1050, 927, 46, 443, 1109, 1119, 808, 193, 553, 745, 1162, 975, 961, 917, 146, 1104, 1079, 722, 206]
valid_ids (0): []
train_ids (1094): [879, 866, 979, 653, 822, 636, 223, 1176, 1078, 834, 893, 151, 1200, 843, 92, 1066, 1130, 163, 1048, 558, 307, 1151, 39, 308, 108, 1016, 613, 83, 207, 538, 226, 221, 525, 1040, 88, 511, 1170, 135, 635, 1196, 122, 637, 389, 8, 776, 994, 639, 1146, 838, 115, 858, 403, 102, 552, 1181, 227, 729, 31, 1193, 378, 675, 867, 760, 1108, 177, 883, 1059, 454, 940, 165, 172, 283, 280, 313, 741, 820, 812, 789, 580, 242, 742, 677, 293, 980, 697, 394, 1159, 462, 490, 473, 505, 168, 340, 1047, 60, 1010, 365, 807, 588, 514, 286, 457, 1043, 315, 335, 258, 1017, 771, 988, 810, 777, 93, 417, 833, 536, 646, 1148, 398, 918, 1019, 100, 156, 548, 1014, 330, 161, 4, 1120, 861, 218, 182, 708, 853, 881, 1074, 285, 90, 1145, 1093, 414, 13, 328, 563, 184, 12, 565, 1055, 683, 784, 544, 621, 1039, 513, 517, 1057, 821, 309, 425, 167, 628, 663, 461, 215, 384, 523, 617, 64, 119, 147, 499, 668, 129, 400, 768, 32, 520, 718, 269, 578, 865, 254, 609, 974, 938, 397, 1175, 178, 405, 891, 1063, 797, 592, 754, 300, 1173, 312, 1185, 727, 350, 737, 992, 468, 390, 802, 319, 700, 627, 890, 556, 759, 691, 847, 45, 500, 744, 188, 684, 855, 431, 696, 753, 469, 775, 707, 1204, 44, 346, 51, 68, 368, 35, 271, 751, 96, 337, 862, 570, 367, 75, 608, 48, 395, 1008, 1073, 1123, 555, 1187, 385, 595, 248, 125, 1003, 447, 446, 287, 201, 998, 1006, 1144, 396, 124, 23, 967, 870, 426, 430, 428, 1156, 1032, 964, 983, 260, 1113, 393, 509, 943, 1077, 476, 839, 1172, 878, 453, 689, 720, 515, 25, 519, 765, 1142, 912, 1129, 761, 1199, 342, 949, 894, 101, 84, 450, 857, 937, 120, 416, 990, 671, 19, 590, 99, 966, 341, 415, 603, 977, 97, 171, 1081, 582, 9, 798, 692, 758, 533, 535, 436, 612, 852, 1054, 740, 1049, 1161, 1117, 638, 817, 209, 919, 1021, 541, 321, 434, 301, 537, 1209, 47, 1097, 1110, 1206, 939, 1174, 481, 131, 657, 724, 1096, 130, 1099, 1062, 1205, 1023, 1041, 726, 10, 1138, 53, 251, 1082, 1031, 574, 711, 986, 728, 731, 1083, 94, 222, 911, 1154, 375, 437, 352, 934, 1136, 28, 38, 152, 1195, 267, 903, 126, 503, 554, 928, 116, 264, 1036, 682, 965, 924, 856, 1095, 316, 475, 877, 985, 485, 549, 1131, 219, 651, 144, 634, 1210, 380, 166, 464, 1089, 725, 85, 569, 545, 596, 204, 568, 246, 1211, 876, 946, 757, 670, 1197, 1007, 357, 229, 234, 236, 501, 1164, 829, 104, 160, 1212, 214, 55, 333, 497, 551, 685, 791, 42, 957, 550, 14, 150, 63, 626, 752, 873, 1072, 654, 200, 618, 250, 591, 305, 1208, 747, 909, 1076, 87, 408, 1135, 564, 175, 622, 26, 978, 370, 95, 299, 231, 1139, 460, 1002, 56, 467, 411, 155, 240, 216, 243, 521, 133, 401, 73, 910, 1137, 921, 194, 253, 1046, 619, 926, 1033, 471, 423, 1000, 908, 805, 606, 455, 277, 382, 715, 801, 955, 960, 1068, 297, 859, 273, 294, 279, 374, 976, 623, 958, 792, 991, 734, 762, 34, 1100, 851, 772, 424, 811, 730, 76, 224, 458, 733, 688, 41, 298, 860, 334, 763, 470, 593, 914, 1203, 930, 114, 449, 399, 54, 79, 602, 571, 954, 902, 1, 291, 387, 945, 208, 647, 486, 1012, 504, 972, 1042, 1128, 1061, 52, 969, 656, 183, 493, 1116, 275, 498, 91, 388, 435, 413, 931, 539, 756, 629, 176, 1134, 153, 899, 364, 502, 577, 1213, 1192, 16, 529, 1155, 71, 296, 228, 37, 452, 981, 840, 562, 534, 532, 783, 186, 997, 290, 239, 1127, 559, 377, 721, 211, 941, 630, 1152, 659, 212, 257, 78, 989, 579, 664, 673, 1035, 687, 597, 996, 655, 474, 906, 247, 1011, 373, 1044, 1022, 1067, 1085, 681, 276, 49, 433, 703, 1029, 785, 794, 494, 441, 484, 323, 238, 1122, 1037, 339, 36, 530, 1092, 148, 451, 895, 360, 338, 897, 137, 1098, 510, 920, 439, 58, 318, 1064, 1182, 326, 616, 796, 848, 196, 356, 157, 459, 782, 409, 947, 444, 331, 379, 1020, 336, 1171, 984, 7, 3, 572, 524, 391, 50, 1163, 427, 706, 1198, 1141, 508, 607, 324, 107, 40, 650, 111, 429, 110, 203, 118, 672, 1025, 561, 678, 1015, 518, 472, 1169, 674, 963, 932, 560, 667, 587, 1086, 738, 935, 813, 788, 710, 698, 406, 149, 982, 793, 158, 660, 900, 1143, 66, 841, 863, 1105, 648, 835, 959, 492, 1045, 383, 542, 465, 610, 907, 232, 1132, 2, 213, 302, 882, 5, 594, 1202, 392, 112, 1034, 1114, 642, 901, 478, 174, 824, 1038, 1091, 252, 528, 1080, 1118, 815, 404, 482, 1009, 875, 169, 113, 109, 830, 82, 358, 922, 220, 506, 89, 288, 779, 106, 1201, 230, 1051, 1101, 1189, 632, 6, 633, 842, 162, 584, 774, 699, 1001, 1065, 583, 432, 410, 329, 11, 1106, 557, 128, 351, 546, 198, 412, 320, 601, 1153, 543, 694, 256, 823, 1053, 225, 1013, 828, 666, 1027, 614, 884, 418, 781, 235, 270, 1088, 480, 620, 790, 244, 456, 819, 755, 1178, 888, 844, 145, 359, 970, 871, 154, 1160, 719, 1103, 739, 948, 189, 746, 701, 1149, 723, 605, 714, 915, 325, 1112, 787, 929, 896, 887, 880, 272, 1158, 956, 1121, 886, 121, 662, 1194, 345, 1071, 304, 750, 278, 846, 139, 103, 62, 1166, 942, 962, 516, 263, 717, 702, 904, 611, 322, 575, 923, 826, 205, 202, 138, 1190, 748, 1102, 479, 849, 134, 1094, 21, 669, 1126, 241, 245, 354, 827, 1018, 1075, 282, 526, 366, 61, 709, 803, 951, 420, 363, 355, 1150, 933, 24, 117, 136, 70, 624, 143, 353, 448, 1165, 604, 255, 105, 676, 376, 284, 1084, 381, 1004, 438, 361, 576, 347, 644, 764, 732, 491, 43, 362, 567, 402, 181, 306, 643, 640, 141, 950, 531, 466, 1024, 695, 1030, 303, 600, 421, 261, 913, 652, 1214, 693, 814, 69, 585, 905, 22, 20, 547, 317, 311, 809, 348, 1179, 1147, 132, 993, 187, 445, 661, 496, 987, 310, 686, 631, 344, 735, 1028, 1191, 332, 1107, 566, 18, 936, 74, 180, 159, 1060, 645, 407, 477, 712, 217, 1058, 199, 1069, 80, 916, 349, 800, 57, 140, 327, 1186, 262, 816, 15, 1188, 142, 837, 0, 743, 999, 386, 749, 422, 641, 195, 599, 81, 581, 540, 944, 889, 369, 487, 786, 625, 483, 704, 197, 825, 30, 952, 259, 1005, 1184, 127, 874, 1168, 170, 1090, 854, 237, 1183, 598, 832, 806, 495]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4583656875995092
the save name prefix for this run is:  chkpt-ID_4583656875995092_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1023
rank avg (pred): 0.475 +- 0.005
mrr vals (pred, true): 0.020, 0.049
batch losses (mrrl, rdl): 0.0, 8.63584e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1064
rank avg (pred): 0.084 +- 0.060
mrr vals (pred, true): 0.249, 0.370
batch losses (mrrl, rdl): 0.0, 1.6154e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 410
rank avg (pred): 0.446 +- 0.261
mrr vals (pred, true): 0.065, 0.047
batch losses (mrrl, rdl): 0.0, 7.0718e-06

Epoch over!
epoch time: 14.818

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 564
rank avg (pred): 0.154 +- 0.122
mrr vals (pred, true): 0.181, 0.205
batch losses (mrrl, rdl): 0.0, 9.2251e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 121
rank avg (pred): 0.460 +- 0.266
mrr vals (pred, true): 0.052, 0.052
batch losses (mrrl, rdl): 0.0, 4.529e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 119
rank avg (pred): 0.463 +- 0.264
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 0.0, 8.25e-07

Epoch over!
epoch time: 14.833

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 232
rank avg (pred): 0.461 +- 0.267
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 0.0, 1.801e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 270
rank avg (pred): 0.099 +- 0.141
mrr vals (pred, true): 0.240, 0.307
batch losses (mrrl, rdl): 0.0, 1.0281e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1058
rank avg (pred): 0.071 +- 0.113
mrr vals (pred, true): 0.290, 0.480
batch losses (mrrl, rdl): 0.0, 5.662e-07

Epoch over!
epoch time: 14.834

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 794
rank avg (pred): 0.417 +- 0.276
mrr vals (pred, true): 0.077, 0.054
batch losses (mrrl, rdl): 0.0, 1.97803e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 391
rank avg (pred): 0.452 +- 0.270
mrr vals (pred, true): 0.055, 0.056
batch losses (mrrl, rdl): 0.0, 4.056e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 731
rank avg (pred): 0.083 +- 0.140
mrr vals (pred, true): 0.250, 0.618
batch losses (mrrl, rdl): 0.0, 4.86006e-05

Epoch over!
epoch time: 15.035

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 884
rank avg (pred): 0.458 +- 0.261
mrr vals (pred, true): 0.049, 0.062
batch losses (mrrl, rdl): 0.0, 5.953e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 535
rank avg (pred): 0.135 +- 0.160
mrr vals (pred, true): 0.184, 0.214
batch losses (mrrl, rdl): 0.0, 2.24488e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 932
rank avg (pred): 0.463 +- 0.266
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 0.0, 4.321e-07

Epoch over!
epoch time: 15.036

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 807
rank avg (pred): 0.449 +- 0.272
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 0.0001127398, 2.1949e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 362
rank avg (pred): 0.393 +- 0.169
mrr vals (pred, true): 0.060, 0.055
batch losses (mrrl, rdl): 0.0009285659, 8.29088e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 685
rank avg (pred): 0.474 +- 0.216
mrr vals (pred, true): 0.061, 0.056
batch losses (mrrl, rdl): 0.001275186, 2.3451e-05

Epoch over!
epoch time: 15.055

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 536
rank avg (pred): 0.221 +- 0.152
mrr vals (pred, true): 0.182, 0.205
batch losses (mrrl, rdl): 0.0054679075, 0.0001111606

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 839
rank avg (pred): 0.629 +- 0.221
mrr vals (pred, true): 0.036, 0.053
batch losses (mrrl, rdl): 0.001943994, 0.0005442381

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1082
rank avg (pred): 0.419 +- 0.201
mrr vals (pred, true): 0.083, 0.060
batch losses (mrrl, rdl): 0.0106914956, 2.55982e-05

Epoch over!
epoch time: 15.009

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 569
rank avg (pred): 0.492 +- 0.225
mrr vals (pred, true): 0.076, 0.053
batch losses (mrrl, rdl): 0.006544698, 4.21959e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 918
rank avg (pred): 0.584 +- 0.205
mrr vals (pred, true): 0.049, 0.058
batch losses (mrrl, rdl): 1.03334e-05, 0.0003993946

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1152
rank avg (pred): 0.163 +- 0.128
mrr vals (pred, true): 0.281, 0.259
batch losses (mrrl, rdl): 0.0048600771, 5.17732e-05

Epoch over!
epoch time: 15.007

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 466
rank avg (pred): 0.453 +- 0.162
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 2.28718e-05, 3.347e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 933
rank avg (pred): 0.517 +- 0.151
mrr vals (pred, true): 0.040, 0.051
batch losses (mrrl, rdl): 0.0010759085, 8.40808e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 359
rank avg (pred): 0.524 +- 0.139
mrr vals (pred, true): 0.032, 0.050
batch losses (mrrl, rdl): 0.0033994543, 9.17579e-05

Epoch over!
epoch time: 15.013

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 267
rank avg (pred): 0.076 +- 0.069
mrr vals (pred, true): 0.376, 0.304
batch losses (mrrl, rdl): 0.0509875827, 5.52026e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 299
rank avg (pred): 0.162 +- 0.125
mrr vals (pred, true): 0.294, 0.357
batch losses (mrrl, rdl): 0.039381586, 0.0001057453

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 829
rank avg (pred): 0.047 +- 0.052
mrr vals (pred, true): 0.439, 0.406
batch losses (mrrl, rdl): 0.0111039784, 2.35309e-05

Epoch over!
epoch time: 15.011

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 505
rank avg (pred): 0.290 +- 0.203
mrr vals (pred, true): 0.221, 0.216
batch losses (mrrl, rdl): 0.0002050941, 0.0005489244

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 117
rank avg (pred): 0.470 +- 0.139
mrr vals (pred, true): 0.041, 0.057
batch losses (mrrl, rdl): 0.0008092256, 5.03242e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 487
rank avg (pred): 0.256 +- 0.198
mrr vals (pred, true): 0.288, 0.230
batch losses (mrrl, rdl): 0.0347249508, 0.0004002164

Epoch over!
epoch time: 15.009

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 755
rank avg (pred): 0.061 +- 0.065
mrr vals (pred, true): 0.412, 0.338
batch losses (mrrl, rdl): 0.0538989082, 8.08228e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 396
rank avg (pred): 0.499 +- 0.135
mrr vals (pred, true): 0.036, 0.050
batch losses (mrrl, rdl): 0.0018257718, 7.58136e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 858
rank avg (pred): 0.460 +- 0.138
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002976123, 4.44047e-05

Epoch over!
epoch time: 15.004

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 874
rank avg (pred): 0.468 +- 0.131
mrr vals (pred, true): 0.042, 0.058
batch losses (mrrl, rdl): 0.000703785, 4.90998e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 34
rank avg (pred): 0.211 +- 0.161
mrr vals (pred, true): 0.297, 0.317
batch losses (mrrl, rdl): 0.0036918484, 0.0002816333

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 744
rank avg (pred): 0.101 +- 0.088
mrr vals (pred, true): 0.380, 0.322
batch losses (mrrl, rdl): 0.0331107117, 1.6015e-05

Epoch over!
epoch time: 15.181

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 254
rank avg (pred): 0.219 +- 0.176
mrr vals (pred, true): 0.321, 0.356
batch losses (mrrl, rdl): 0.012446587, 0.0004083619

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 432
rank avg (pred): 0.452 +- 0.104
mrr vals (pred, true): 0.039, 0.050
batch losses (mrrl, rdl): 0.0012629736, 6.51536e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 709
rank avg (pred): 0.476 +- 0.152
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0003747807, 4.49454e-05

Epoch over!
epoch time: 15.162

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 781
rank avg (pred): 0.435 +- 0.131
mrr vals (pred, true): 0.055, 0.052
batch losses (mrrl, rdl): 0.0002469312, 7.03079e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 856
rank avg (pred): 0.460 +- 0.128
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 1.21632e-05, 6.41279e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 99
rank avg (pred): 0.429 +- 0.132
mrr vals (pred, true): 0.055, 0.051
batch losses (mrrl, rdl): 0.0002565635, 6.25002e-05

Epoch over!
epoch time: 15.082

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.440 +- 0.118
mrr vals (pred, true): 0.047, 0.054

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04273 	 0.04544 	 ~...
   31 	     1 	 0.04808 	 0.04722 	 ~...
   66 	     2 	 0.05248 	 0.04726 	 ~...
   12 	     3 	 0.04546 	 0.04839 	 ~...
   20 	     4 	 0.04662 	 0.04840 	 ~...
   50 	     5 	 0.05017 	 0.04869 	 ~...
   54 	     6 	 0.05055 	 0.04896 	 ~...
   61 	     7 	 0.05159 	 0.04905 	 ~...
   47 	     8 	 0.04983 	 0.04915 	 ~...
   21 	     9 	 0.04667 	 0.04924 	 ~...
   14 	    10 	 0.04555 	 0.04937 	 ~...
    4 	    11 	 0.04255 	 0.04952 	 ~...
   46 	    12 	 0.04966 	 0.04973 	 ~...
   53 	    13 	 0.05052 	 0.04973 	 ~...
   29 	    14 	 0.04776 	 0.05016 	 ~...
   77 	    15 	 0.05808 	 0.05021 	 ~...
   65 	    16 	 0.05245 	 0.05035 	 ~...
   17 	    17 	 0.04589 	 0.05038 	 ~...
   52 	    18 	 0.05025 	 0.05038 	 ~...
   73 	    19 	 0.05393 	 0.05068 	 ~...
   30 	    20 	 0.04786 	 0.05083 	 ~...
   25 	    21 	 0.04720 	 0.05088 	 ~...
   59 	    22 	 0.05150 	 0.05112 	 ~...
   34 	    23 	 0.04823 	 0.05114 	 ~...
   35 	    24 	 0.04829 	 0.05126 	 ~...
   18 	    25 	 0.04654 	 0.05127 	 ~...
   45 	    26 	 0.04952 	 0.05142 	 ~...
   16 	    27 	 0.04571 	 0.05152 	 ~...
   71 	    28 	 0.05329 	 0.05164 	 ~...
   11 	    29 	 0.04520 	 0.05178 	 ~...
   24 	    30 	 0.04719 	 0.05186 	 ~...
   13 	    31 	 0.04553 	 0.05191 	 ~...
   67 	    32 	 0.05259 	 0.05224 	 ~...
   64 	    33 	 0.05239 	 0.05224 	 ~...
   76 	    34 	 0.05500 	 0.05241 	 ~...
   23 	    35 	 0.04703 	 0.05258 	 ~...
   72 	    36 	 0.05371 	 0.05272 	 ~...
   38 	    37 	 0.04898 	 0.05287 	 ~...
   59 	    38 	 0.05150 	 0.05291 	 ~...
    1 	    39 	 0.04226 	 0.05302 	 ~...
   26 	    40 	 0.04728 	 0.05303 	 ~...
   75 	    41 	 0.05473 	 0.05313 	 ~...
   22 	    42 	 0.04673 	 0.05338 	 ~...
   56 	    43 	 0.05088 	 0.05341 	 ~...
    8 	    44 	 0.04434 	 0.05346 	 ~...
    0 	    45 	 0.04205 	 0.05363 	 ~...
   15 	    46 	 0.04564 	 0.05373 	 ~...
   41 	    47 	 0.04933 	 0.05375 	 ~...
   27 	    48 	 0.04746 	 0.05428 	 ~...
   43 	    49 	 0.04948 	 0.05434 	 ~...
   55 	    50 	 0.05057 	 0.05435 	 ~...
   74 	    51 	 0.05426 	 0.05438 	 ~...
    6 	    52 	 0.04280 	 0.05447 	 ~...
   42 	    53 	 0.04937 	 0.05459 	 ~...
   62 	    54 	 0.05204 	 0.05462 	 ~...
   36 	    55 	 0.04837 	 0.05470 	 ~...
   51 	    56 	 0.05017 	 0.05493 	 ~...
   40 	    57 	 0.04909 	 0.05495 	 ~...
   32 	    58 	 0.04811 	 0.05509 	 ~...
   58 	    59 	 0.05147 	 0.05520 	 ~...
   49 	    60 	 0.05013 	 0.05531 	 ~...
   27 	    61 	 0.04746 	 0.05565 	 ~...
    9 	    62 	 0.04447 	 0.05647 	 ~...
    3 	    63 	 0.04240 	 0.05650 	 ~...
   19 	    64 	 0.04662 	 0.05680 	 ~...
   63 	    65 	 0.05210 	 0.05731 	 ~...
   44 	    66 	 0.04949 	 0.05742 	 ~...
   68 	    67 	 0.05277 	 0.05751 	 ~...
   37 	    68 	 0.04863 	 0.05755 	 ~...
   48 	    69 	 0.04993 	 0.05781 	 ~...
   57 	    70 	 0.05097 	 0.05902 	 ~...
   39 	    71 	 0.04907 	 0.05918 	 ~...
   69 	    72 	 0.05288 	 0.05988 	 ~...
   33 	    73 	 0.04822 	 0.06033 	 ~...
    7 	    74 	 0.04386 	 0.06092 	 ~...
   10 	    75 	 0.04452 	 0.06229 	 ~...
    2 	    76 	 0.04232 	 0.06240 	 ~...
   70 	    77 	 0.05298 	 0.06253 	 ~...
   94 	    78 	 0.30782 	 0.10392 	 MISS
  116 	    79 	 0.45626 	 0.15084 	 MISS
  117 	    80 	 0.47652 	 0.16239 	 MISS
   83 	    81 	 0.25648 	 0.17533 	 m..s
   78 	    82 	 0.22729 	 0.18970 	 m..s
   82 	    83 	 0.23722 	 0.20462 	 m..s
   79 	    84 	 0.22891 	 0.20561 	 ~...
   80 	    85 	 0.23244 	 0.22022 	 ~...
   85 	    86 	 0.27900 	 0.23429 	 m..s
   81 	    87 	 0.23712 	 0.23557 	 ~...
   86 	    88 	 0.27935 	 0.26361 	 ~...
   92 	    89 	 0.29883 	 0.27182 	 ~...
   88 	    90 	 0.29741 	 0.27970 	 ~...
   89 	    91 	 0.29748 	 0.28022 	 ~...
   84 	    92 	 0.27074 	 0.28091 	 ~...
   91 	    93 	 0.29855 	 0.28708 	 ~...
  108 	    94 	 0.35706 	 0.28900 	 m..s
   90 	    95 	 0.29776 	 0.29459 	 ~...
   96 	    96 	 0.30790 	 0.29748 	 ~...
   95 	    97 	 0.30784 	 0.29778 	 ~...
   93 	    98 	 0.30781 	 0.30880 	 ~...
   87 	    99 	 0.29683 	 0.30884 	 ~...
   97 	   100 	 0.30931 	 0.30980 	 ~...
  106 	   101 	 0.31919 	 0.31037 	 ~...
  105 	   102 	 0.31617 	 0.31793 	 ~...
  103 	   103 	 0.31491 	 0.31912 	 ~...
  100 	   104 	 0.31049 	 0.31936 	 ~...
  101 	   105 	 0.31160 	 0.32032 	 ~...
  102 	   106 	 0.31222 	 0.32139 	 ~...
  113 	   107 	 0.39346 	 0.32386 	 m..s
   98 	   108 	 0.30970 	 0.33811 	 ~...
  110 	   109 	 0.37988 	 0.33985 	 m..s
  104 	   110 	 0.31569 	 0.34172 	 ~...
   99 	   111 	 0.31022 	 0.34389 	 m..s
  107 	   112 	 0.32458 	 0.34970 	 ~...
  120 	   113 	 0.54351 	 0.40136 	 MISS
  115 	   114 	 0.42473 	 0.44528 	 ~...
  109 	   115 	 0.37506 	 0.44883 	 m..s
  114 	   116 	 0.41973 	 0.45844 	 m..s
  118 	   117 	 0.52467 	 0.46690 	 m..s
  110 	   118 	 0.37988 	 0.48183 	 MISS
  119 	   119 	 0.53610 	 0.48472 	 m..s
  112 	   120 	 0.38048 	 0.48829 	 MISS
==========================================
r_mrr = 0.9351956248283386
r2_mrr = 0.8519296646118164
spearmanr_mrr@5 = 0.9981823563575745
spearmanr_mrr@10 = 0.8794977068901062
spearmanr_mrr@50 = 0.9693541526794434
spearmanr_mrr@100 = 0.9879757761955261
spearmanr_mrr@All = 0.9892177581787109
==========================================
test time: 0.44
Done Testing dataset Kinships
total time taken: 233.15811252593994
training time taken: 225.54126977920532
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9352)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.8519)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9982)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.8795)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9694)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9880)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9892)}}, 'test_loss': {'ComplEx': {'Kinships': 3.20420479137465}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 6557380255459631
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1104, 175, 1011, 369, 1085, 137, 858, 802, 372, 915, 283, 653, 411, 214, 423, 1019, 231, 179, 966, 377, 625, 353, 338, 91, 1171, 192, 824, 74, 1072, 438, 367, 773, 946, 253, 1116, 1023, 701, 177, 68, 581, 104, 1109, 515, 394, 378, 380, 448, 19, 1183, 1103, 444, 401, 757, 522, 1043, 851, 328, 307, 1088, 379, 1140, 597, 229, 977, 775, 182, 1132, 1209, 166, 736, 784, 334, 938, 618, 1137, 1189, 586, 519, 698, 920, 95, 537, 774, 891, 1017, 545, 1196, 251, 965, 558, 3, 992, 210, 899, 1045, 959, 1068, 574, 462, 450, 1063, 791, 1033, 715, 278, 512, 943, 1035, 589, 937, 1139, 616, 156, 663, 551, 591, 92, 1151, 1012, 273, 439]
valid_ids (0): []
train_ids (1094): [163, 124, 1004, 576, 912, 559, 230, 391, 680, 244, 162, 381, 262, 44, 113, 553, 14, 948, 276, 1032, 612, 1027, 949, 489, 37, 1203, 960, 212, 514, 384, 473, 1200, 345, 234, 1054, 335, 905, 650, 108, 1041, 1212, 141, 189, 386, 483, 115, 390, 690, 629, 1076, 861, 485, 981, 282, 722, 1157, 704, 733, 1093, 1129, 20, 631, 643, 414, 604, 717, 542, 985, 955, 465, 794, 1053, 500, 1124, 803, 1042, 594, 787, 830, 582, 986, 25, 84, 795, 1156, 458, 1089, 297, 289, 410, 624, 630, 1111, 505, 461, 563, 118, 259, 355, 145, 647, 809, 490, 1090, 445, 1115, 164, 33, 874, 584, 106, 211, 1036, 777, 148, 1211, 729, 302, 1122, 357, 80, 467, 1049, 588, 223, 344, 838, 366, 460, 536, 286, 1168, 1021, 610, 573, 1006, 785, 205, 889, 840, 31, 711, 351, 1097, 1055, 1062, 2, 552, 879, 644, 257, 1001, 245, 1210, 796, 157, 1166, 863, 518, 430, 556, 83, 1110, 776, 914, 304, 429, 455, 929, 388, 408, 1119, 605, 991, 347, 779, 724, 590, 18, 281, 600, 1141, 934, 662, 857, 1113, 480, 602, 979, 1020, 1118, 0, 492, 562, 855, 731, 892, 620, 284, 924, 111, 1098, 557, 53, 484, 42, 199, 720, 51, 208, 767, 668, 8, 1087, 502, 261, 961, 425, 727, 1190, 907, 1079, 316, 1073, 633, 1213, 1134, 1080, 55, 1038, 815, 89, 361, 546, 491, 665, 1108, 744, 939, 782, 45, 255, 725, 543, 964, 702, 517, 716, 474, 1128, 318, 242, 5, 1074, 611, 159, 1167, 1069, 682, 693, 373, 607, 989, 511, 761, 1078, 56, 671, 585, 1114, 88, 703, 922, 726, 139, 1127, 947, 890, 99, 919, 403, 109, 61, 881, 407, 739, 471, 634, 613, 435, 326, 227, 859, 812, 468, 265, 856, 823, 399, 641, 238, 1188, 1135, 689, 925, 9, 415, 1092, 329, 478, 110, 1100, 495, 1164, 370, 486, 1174, 48, 1106, 1138, 126, 93, 930, 962, 13, 374, 990, 828, 412, 745, 1061, 799, 1152, 1182, 127, 549, 1126, 735, 7, 233, 292, 39, 246, 808, 184, 497, 475, 880, 849, 658, 1050, 382, 76, 201, 1075, 1037, 674, 237, 1084, 342, 138, 105, 526, 314, 888, 332, 1091, 453, 1178, 728, 548, 96, 902, 748, 747, 524, 967, 923, 875, 186, 942, 999, 493, 195, 375, 853, 151, 235, 645, 350, 870, 327, 1070, 165, 457, 279, 1191, 666, 1150, 30, 822, 617, 420, 877, 988, 299, 181, 107, 868, 975, 769, 886, 120, 596, 323, 94, 398, 885, 356, 308, 994, 160, 752, 1161, 734, 422, 71, 432, 1086, 538, 968, 476, 579, 1195, 1214, 406, 147, 116, 359, 1159, 798, 1013, 131, 606, 1169, 694, 664, 1179, 447, 1022, 1040, 1187, 405, 927, 393, 73, 1048, 368, 187, 354, 686, 206, 587, 781, 34, 216, 12, 1197, 341, 993, 431, 498, 27, 1123, 931, 362, 319, 599, 578, 1144, 434, 122, 443, 1056, 207, 240, 560, 213, 66, 534, 135, 1173, 452, 550, 1143, 687, 778, 275, 52, 81, 609, 1102, 309, 541, 760, 154, 149, 400, 528, 150, 1047, 692, 638, 532, 1206, 180, 958, 385, 753, 125, 63, 659, 842, 451, 882, 409, 417, 397, 887, 928, 893, 78, 732, 813, 950, 193, 232, 248, 811, 829, 315, 272, 869, 535, 1064, 392, 648, 952, 32, 1148, 903, 269, 908, 196, 984, 860, 1044, 1125, 864, 876, 667, 1099, 699, 320, 144, 190, 530, 1184, 661, 636, 695, 331, 168, 459, 140, 496, 305, 254, 170, 158, 69, 564, 848, 615, 250, 1082, 1025, 1175, 917, 1158, 568, 654, 831, 1162, 577, 1077, 264, 688, 383, 635, 996, 987, 487, 933, 936, 758, 871, 1, 161, 507, 1170, 681, 622, 194, 969, 271, 129, 567, 222, 1101, 755, 215, 70, 974, 50, 973, 134, 926, 221, 21, 569, 86, 696, 679, 814, 249, 873, 270, 516, 293, 247, 971, 554, 153, 913, 572, 178, 800, 169, 1147, 835, 43, 646, 477, 837, 626, 595, 311, 1002, 691, 296, 102, 176, 146, 520, 174, 570, 510, 709, 15, 705, 531, 1029, 963, 1000, 456, 1007, 191, 239, 119, 418, 352, 866, 1051, 426, 396, 501, 1008, 112, 509, 839, 441, 846, 527, 810, 555, 436, 404, 364, 1066, 64, 660, 185, 1131, 421, 442, 143, 883, 277, 363, 825, 343, 580, 117, 220, 130, 749, 395, 940, 513, 67, 1052, 1146, 103, 852, 675, 1058, 97, 1094, 260, 437, 1018, 252, 1142, 719, 575, 1154, 766, 770, 40, 298, 1003, 896, 449, 789, 805, 291, 1181, 983, 656, 427, 317, 878, 826, 816, 114, 503, 1185, 945, 598, 708, 1105, 1057, 676, 621, 547, 533, 583, 58, 142, 1016, 1096, 706, 730, 371, 980, 539, 1199, 780, 1028, 172, 499, 1120, 818, 28, 1059, 916, 197, 746, 628, 463, 183, 121, 79, 783, 561, 867, 592, 523, 865, 209, 918, 741, 1065, 204, 1177, 330, 951, 224, 217, 623, 754, 642, 290, 1155, 16, 333, 100, 285, 801, 566, 845, 832, 41, 77, 413, 358, 226, 797, 684, 982, 921, 714, 1149, 957, 995, 529, 339, 944, 128, 901, 1193, 488, 472, 57, 4, 521, 132, 266, 768, 713, 267, 525, 672, 935, 1095, 1071, 60, 683, 806, 26, 72, 772, 348, 895, 737, 756, 685, 155, 482, 508, 36, 469, 322, 1034, 203, 652, 750, 24, 152, 454, 325, 218, 1009, 900, 365, 854, 788, 11, 571, 1202, 1205, 1160, 1201, 87, 911, 198, 506, 65, 268, 1026, 673, 771, 274, 759, 640, 906, 1031, 910, 336, 228, 740, 295, 1107, 1067, 324, 904, 303, 1112, 35, 544, 402, 608, 862, 678, 909, 820, 428, 136, 243, 817, 346, 833, 54, 90, 970, 998, 494, 280, 932, 1136, 738, 313, 1046, 721, 416, 765, 263, 804, 424, 1005, 1204, 236, 337, 1024, 651, 38, 202, 1208, 6, 49, 956, 23, 1039, 619, 98, 301, 1194, 123, 897, 997, 1121, 884, 565, 792, 360, 85, 173, 649, 1186, 1172, 310, 637, 763, 954, 742, 200, 287, 376, 827, 639, 1081, 101, 718, 807, 601, 850, 1133, 59, 241, 171, 340, 836, 790, 657, 321, 446, 743, 62, 712, 841, 1153, 847, 479, 751, 188, 312, 75, 978, 29, 46, 22, 258, 941, 133, 953, 1130, 1014, 1083, 1010, 710, 898, 697, 723, 300, 389, 1176, 793, 540, 976, 306, 1030, 972, 762, 669, 1207, 466, 764, 593, 1015, 225, 844, 677, 627, 821, 819, 294, 288, 167, 670, 834, 1165, 1180, 1163, 1198, 614, 481, 707, 464, 433, 256, 82, 387, 1145, 1192, 47, 419, 603, 894, 504, 219, 470, 10, 349, 843, 17, 1060, 440, 655, 786, 632, 700, 1117, 872]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9211968346014076
the save name prefix for this run is:  chkpt-ID_9211968346014076_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 995
rank avg (pred): 0.460 +- 0.003
mrr vals (pred, true): 0.021, 0.458
batch losses (mrrl, rdl): 0.0, 0.0031820836

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 675
rank avg (pred): 0.480 +- 0.018
mrr vals (pred, true): 0.020, 0.054
batch losses (mrrl, rdl): 0.0, 8.46436e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 145
rank avg (pred): 0.447 +- 0.262
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 0.0, 1.9299e-06

Epoch over!
epoch time: 15.013

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 716
rank avg (pred): 0.459 +- 0.274
mrr vals (pred, true): 0.049, 0.053
batch losses (mrrl, rdl): 0.0, 1.28e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 220
rank avg (pred): 0.463 +- 0.258
mrr vals (pred, true): 0.042, 0.055
batch losses (mrrl, rdl): 0.0, 8.059e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 656
rank avg (pred): 0.461 +- 0.273
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 0.0, 3.465e-07

Epoch over!
epoch time: 15.009

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 904
rank avg (pred): 0.094 +- 0.118
mrr vals (pred, true): 0.218, 0.220
batch losses (mrrl, rdl): 0.0, 7.7711e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1059
rank avg (pred): 0.097 +- 0.124
mrr vals (pred, true): 0.219, 0.515
batch losses (mrrl, rdl): 0.0, 3.18223e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 906
rank avg (pred): 0.146 +- 0.165
mrr vals (pred, true): 0.156, 0.169
batch losses (mrrl, rdl): 0.0, 6.2675e-05

Epoch over!
epoch time: 15.011

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 244
rank avg (pred): 0.093 +- 0.116
mrr vals (pred, true): 0.209, 0.322
batch losses (mrrl, rdl): 0.0, 4.781e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1025
rank avg (pred): 0.481 +- 0.270
mrr vals (pred, true): 0.043, 0.053
batch losses (mrrl, rdl): 0.0, 4.7324e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 947
rank avg (pred): 0.472 +- 0.263
mrr vals (pred, true): 0.045, 0.063
batch losses (mrrl, rdl): 0.0, 6.8498e-06

Epoch over!
epoch time: 14.769

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 814
rank avg (pred): 0.075 +- 0.105
mrr vals (pred, true): 0.247, 0.623
batch losses (mrrl, rdl): 0.0, 3.67009e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 483
rank avg (pred): 0.478 +- 0.265
mrr vals (pred, true): 0.044, 0.047
batch losses (mrrl, rdl): 0.0, 3.6031e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 563
rank avg (pred): 0.159 +- 0.185
mrr vals (pred, true): 0.158, 0.195
batch losses (mrrl, rdl): 0.0, 2.1058e-06

Epoch over!
epoch time: 14.753

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 125
rank avg (pred): 0.452 +- 0.277
mrr vals (pred, true): 0.065, 0.053
batch losses (mrrl, rdl): 0.0021037522, 3.437e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 493
rank avg (pred): 0.137 +- 0.165
mrr vals (pred, true): 0.323, 0.239
batch losses (mrrl, rdl): 0.0711161047, 4.305e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 819
rank avg (pred): 0.123 +- 0.161
mrr vals (pred, true): 0.353, 0.442
batch losses (mrrl, rdl): 0.0804021806, 7.36696e-05

Epoch over!
epoch time: 15.002

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 745
rank avg (pred): 0.194 +- 0.168
mrr vals (pred, true): 0.274, 0.324
batch losses (mrrl, rdl): 0.0244788043, 0.0001260381

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 73
rank avg (pred): 0.273 +- 0.265
mrr vals (pred, true): 0.249, 0.290
batch losses (mrrl, rdl): 0.0171519406, 0.000473218

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1115
rank avg (pred): 0.407 +- 0.152
mrr vals (pred, true): 0.053, 0.054
batch losses (mrrl, rdl): 9.66887e-05, 8.95955e-05

Epoch over!
epoch time: 14.978

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1047
rank avg (pred): 0.444 +- 0.184
mrr vals (pred, true): 0.055, 0.053
batch losses (mrrl, rdl): 0.000270941, 2.71723e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 662
rank avg (pred): 0.439 +- 0.177
mrr vals (pred, true): 0.060, 0.057
batch losses (mrrl, rdl): 0.0009951388, 3.78586e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 711
rank avg (pred): 0.443 +- 0.177
mrr vals (pred, true): 0.057, 0.057
batch losses (mrrl, rdl): 0.0004436572, 3.15686e-05

Epoch over!
epoch time: 14.961

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 897
rank avg (pred): 0.298 +- 0.180
mrr vals (pred, true): 0.208, 0.089
batch losses (mrrl, rdl): 0.2498774379, 1.22033e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 125
rank avg (pred): 0.480 +- 0.205
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 0.0001176846, 2.50205e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 810
rank avg (pred): 0.109 +- 0.160
mrr vals (pred, true): 0.481, 0.629
batch losses (mrrl, rdl): 0.2181747705, 0.0001699019

Epoch over!
epoch time: 14.967

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 373
rank avg (pred): 0.493 +- 0.190
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 5.2792e-06, 2.35667e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 31
rank avg (pred): 0.268 +- 0.205
mrr vals (pred, true): 0.299, 0.327
batch losses (mrrl, rdl): 0.0079056909, 0.0006969917

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 705
rank avg (pred): 0.430 +- 0.126
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 7.727e-07, 6.4637e-05

Epoch over!
epoch time: 14.994

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1128
rank avg (pred): 0.448 +- 0.138
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 4.6206e-06, 5.27824e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1206
rank avg (pred): 0.528 +- 0.185
mrr vals (pred, true): 0.037, 0.052
batch losses (mrrl, rdl): 0.0017212695, 0.0001036743

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 994
rank avg (pred): 0.138 +- 0.169
mrr vals (pred, true): 0.381, 0.459
batch losses (mrrl, rdl): 0.0617162138, 0.0001142241

Epoch over!
epoch time: 14.962

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 902
rank avg (pred): 0.153 +- 0.169
mrr vals (pred, true): 0.355, 0.249
batch losses (mrrl, rdl): 0.1121929288, 3.39348e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 546
rank avg (pred): 0.286 +- 0.190
mrr vals (pred, true): 0.276, 0.218
batch losses (mrrl, rdl): 0.0334644169, 0.0005191768

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 509
rank avg (pred): 0.185 +- 0.165
mrr vals (pred, true): 0.315, 0.240
batch losses (mrrl, rdl): 0.0563940182, 9.34721e-05

Epoch over!
epoch time: 14.955

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 877
rank avg (pred): 0.414 +- 0.097
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 8.20794e-05, 0.0001144864

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1120
rank avg (pred): 0.453 +- 0.138
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 7.5034e-06, 4.70566e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 86
rank avg (pred): 0.429 +- 0.123
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001705907, 8.80852e-05

Epoch over!
epoch time: 14.995

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1091
rank avg (pred): 0.444 +- 0.110
mrr vals (pred, true): 0.046, 0.058
batch losses (mrrl, rdl): 0.0001490297, 6.38383e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 299
rank avg (pred): 0.278 +- 0.192
mrr vals (pred, true): 0.283, 0.357
batch losses (mrrl, rdl): 0.0546474978, 0.0008034681

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 40
rank avg (pred): 0.285 +- 0.183
mrr vals (pred, true): 0.265, 0.296
batch losses (mrrl, rdl): 0.0100950282, 0.0006403756

Epoch over!
epoch time: 14.99

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 276
rank avg (pred): 0.318 +- 0.212
mrr vals (pred, true): 0.263, 0.320
batch losses (mrrl, rdl): 0.0332840718, 0.0010406381

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 34
rank avg (pred): 0.271 +- 0.182
mrr vals (pred, true): 0.286, 0.317
batch losses (mrrl, rdl): 0.0095583443, 0.000664491

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 215
rank avg (pred): 0.456 +- 0.149
mrr vals (pred, true): 0.052, 0.052
batch losses (mrrl, rdl): 5.75624e-05, 4.16492e-05

Epoch over!
epoch time: 14.981

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.455 +- 0.133
mrr vals (pred, true): 0.046, 0.055

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.03834 	 0.04525 	 ~...
   42 	     1 	 0.04528 	 0.04538 	 ~...
    7 	     2 	 0.04007 	 0.04547 	 ~...
   36 	     3 	 0.04513 	 0.04627 	 ~...
   55 	     4 	 0.04560 	 0.04668 	 ~...
   18 	     5 	 0.04308 	 0.04690 	 ~...
   73 	     6 	 0.04787 	 0.04696 	 ~...
   33 	     7 	 0.04468 	 0.04714 	 ~...
   24 	     8 	 0.04395 	 0.04740 	 ~...
   30 	     9 	 0.04449 	 0.04747 	 ~...
   64 	    10 	 0.04620 	 0.04905 	 ~...
   26 	    11 	 0.04420 	 0.04922 	 ~...
   52 	    12 	 0.04553 	 0.04948 	 ~...
   78 	    13 	 0.04865 	 0.04960 	 ~...
   85 	    14 	 0.05114 	 0.05014 	 ~...
   53 	    15 	 0.04554 	 0.05016 	 ~...
   57 	    16 	 0.04567 	 0.05025 	 ~...
    9 	    17 	 0.04079 	 0.05028 	 ~...
   11 	    18 	 0.04087 	 0.05068 	 ~...
    5 	    19 	 0.03857 	 0.05071 	 ~...
   67 	    20 	 0.04651 	 0.05125 	 ~...
   27 	    21 	 0.04421 	 0.05135 	 ~...
   14 	    22 	 0.04278 	 0.05138 	 ~...
   85 	    23 	 0.05114 	 0.05147 	 ~...
   31 	    24 	 0.04451 	 0.05178 	 ~...
   50 	    25 	 0.04550 	 0.05186 	 ~...
   15 	    26 	 0.04290 	 0.05190 	 ~...
   77 	    27 	 0.04840 	 0.05215 	 ~...
   44 	    28 	 0.04531 	 0.05225 	 ~...
   34 	    29 	 0.04477 	 0.05237 	 ~...
   25 	    30 	 0.04407 	 0.05242 	 ~...
   37 	    31 	 0.04515 	 0.05274 	 ~...
   59 	    32 	 0.04569 	 0.05274 	 ~...
   79 	    33 	 0.04891 	 0.05277 	 ~...
   63 	    34 	 0.04606 	 0.05279 	 ~...
   75 	    35 	 0.04828 	 0.05293 	 ~...
   41 	    36 	 0.04523 	 0.05307 	 ~...
   85 	    37 	 0.05114 	 0.05311 	 ~...
   62 	    38 	 0.04603 	 0.05331 	 ~...
   57 	    39 	 0.04567 	 0.05331 	 ~...
   40 	    40 	 0.04523 	 0.05374 	 ~...
   23 	    41 	 0.04387 	 0.05380 	 ~...
   66 	    42 	 0.04623 	 0.05387 	 ~...
   38 	    43 	 0.04518 	 0.05397 	 ~...
   44 	    44 	 0.04531 	 0.05422 	 ~...
   83 	    45 	 0.04978 	 0.05422 	 ~...
   46 	    46 	 0.04534 	 0.05427 	 ~...
   17 	    47 	 0.04306 	 0.05438 	 ~...
   74 	    48 	 0.04792 	 0.05444 	 ~...
   39 	    49 	 0.04519 	 0.05451 	 ~...
   19 	    50 	 0.04309 	 0.05454 	 ~...
   64 	    51 	 0.04620 	 0.05470 	 ~...
   34 	    52 	 0.04477 	 0.05476 	 ~...
   10 	    53 	 0.04079 	 0.05492 	 ~...
   43 	    54 	 0.04529 	 0.05494 	 ~...
   76 	    55 	 0.04829 	 0.05496 	 ~...
    3 	    56 	 0.03762 	 0.05506 	 ~...
   60 	    57 	 0.04588 	 0.05521 	 ~...
   69 	    58 	 0.04729 	 0.05540 	 ~...
   56 	    59 	 0.04564 	 0.05543 	 ~...
   81 	    60 	 0.04928 	 0.05571 	 ~...
   68 	    61 	 0.04712 	 0.05580 	 ~...
    0 	    62 	 0.03728 	 0.05584 	 ~...
   47 	    63 	 0.04538 	 0.05596 	 ~...
    1 	    64 	 0.03753 	 0.05626 	 ~...
   82 	    65 	 0.04963 	 0.05653 	 ~...
   88 	    66 	 0.05145 	 0.05664 	 ~...
   16 	    67 	 0.04292 	 0.05687 	 ~...
   32 	    68 	 0.04452 	 0.05696 	 ~...
    6 	    69 	 0.03997 	 0.05699 	 ~...
   22 	    70 	 0.04377 	 0.05723 	 ~...
   47 	    71 	 0.04538 	 0.05740 	 ~...
   80 	    72 	 0.04903 	 0.05751 	 ~...
    1 	    73 	 0.03753 	 0.05756 	 ~...
   61 	    74 	 0.04588 	 0.05769 	 ~...
   70 	    75 	 0.04739 	 0.05785 	 ~...
   13 	    76 	 0.04168 	 0.05796 	 ~...
   21 	    77 	 0.04376 	 0.05806 	 ~...
   20 	    78 	 0.04333 	 0.05838 	 ~...
   83 	    79 	 0.04978 	 0.05840 	 ~...
   49 	    80 	 0.04543 	 0.05874 	 ~...
   72 	    81 	 0.04785 	 0.05875 	 ~...
    8 	    82 	 0.04050 	 0.05910 	 ~...
   70 	    83 	 0.04739 	 0.05937 	 ~...
   51 	    84 	 0.04551 	 0.05964 	 ~...
   28 	    85 	 0.04424 	 0.05969 	 ~...
   12 	    86 	 0.04103 	 0.06181 	 ~...
   54 	    87 	 0.04555 	 0.06229 	 ~...
   29 	    88 	 0.04428 	 0.06240 	 ~...
   90 	    89 	 0.19479 	 0.08417 	 MISS
   89 	    90 	 0.18540 	 0.16239 	 ~...
   91 	    91 	 0.24408 	 0.16390 	 m..s
   99 	    92 	 0.27316 	 0.18404 	 m..s
  105 	    93 	 0.27456 	 0.18561 	 m..s
   94 	    94 	 0.27139 	 0.18970 	 m..s
  102 	    95 	 0.27369 	 0.21207 	 m..s
   93 	    96 	 0.27113 	 0.21280 	 m..s
  103 	    97 	 0.27387 	 0.22564 	 m..s
  104 	    98 	 0.27411 	 0.23291 	 m..s
  109 	    99 	 0.31307 	 0.23557 	 m..s
   95 	   100 	 0.27157 	 0.28774 	 ~...
  106 	   101 	 0.28929 	 0.28969 	 ~...
   98 	   102 	 0.27209 	 0.28981 	 ~...
   96 	   103 	 0.27204 	 0.29445 	 ~...
  117 	   104 	 0.42912 	 0.29652 	 MISS
  116 	   105 	 0.36333 	 0.29679 	 m..s
  114 	   106 	 0.35737 	 0.29813 	 m..s
   97 	   107 	 0.27208 	 0.30022 	 ~...
  101 	   108 	 0.27354 	 0.30840 	 m..s
   92 	   109 	 0.26387 	 0.31559 	 m..s
  119 	   110 	 0.47552 	 0.31936 	 MISS
  107 	   111 	 0.29486 	 0.32141 	 ~...
  108 	   112 	 0.30009 	 0.34731 	 m..s
  100 	   113 	 0.27353 	 0.35160 	 m..s
  112 	   114 	 0.32728 	 0.35834 	 m..s
  111 	   115 	 0.32724 	 0.37130 	 m..s
  110 	   116 	 0.32366 	 0.37521 	 m..s
  113 	   117 	 0.34692 	 0.42762 	 m..s
  120 	   118 	 0.50746 	 0.43603 	 m..s
  118 	   119 	 0.43979 	 0.46828 	 ~...
  114 	   120 	 0.35737 	 0.47935 	 MISS
==========================================
r_mrr = 0.9531369209289551
r2_mrr = 0.8961843252182007
spearmanr_mrr@5 = 0.9932119846343994
spearmanr_mrr@10 = 0.9876067638397217
spearmanr_mrr@50 = 0.9713699817657471
spearmanr_mrr@100 = 0.9826306700706482
spearmanr_mrr@All = 0.9839268326759338
==========================================
test time: 0.449
Done Testing dataset Kinships
total time taken: 232.32134318351746
training time taken: 224.7998526096344
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9531)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.8962)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9932)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9876)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9714)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9826)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9839)}}, 'test_loss': {'ComplEx': {'Kinships': 1.7017734410874255}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 3888587108062485
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [753, 559, 878, 733, 355, 87, 345, 1027, 1128, 954, 919, 101, 1082, 107, 1057, 1109, 118, 569, 1040, 24, 205, 1149, 481, 626, 614, 1174, 329, 535, 365, 84, 805, 175, 834, 249, 900, 1004, 74, 890, 622, 658, 18, 867, 1014, 1134, 957, 599, 699, 125, 1133, 587, 346, 272, 662, 992, 741, 238, 56, 766, 65, 58, 140, 1018, 225, 142, 122, 486, 818, 700, 800, 1013, 304, 941, 709, 208, 273, 618, 690, 100, 1155, 885, 1093, 1168, 330, 512, 1052, 1201, 251, 10, 247, 912, 1098, 872, 385, 57, 593, 635, 918, 688, 36, 1106, 759, 513, 164, 422, 1167, 196, 1148, 201, 189, 197, 19, 491, 846, 322, 681, 421, 813, 103, 417, 143, 837]
valid_ids (0): []
train_ids (1094): [1047, 233, 182, 951, 96, 581, 851, 584, 206, 278, 1060, 174, 523, 54, 425, 881, 1058, 966, 342, 26, 1161, 651, 844, 351, 558, 446, 668, 1045, 905, 91, 1065, 848, 1062, 1159, 1108, 498, 1083, 1138, 1118, 420, 369, 432, 1136, 672, 1002, 702, 1, 1207, 312, 154, 703, 466, 437, 338, 590, 83, 531, 283, 217, 679, 831, 997, 1195, 293, 876, 689, 1172, 546, 743, 961, 817, 255, 773, 874, 270, 295, 179, 470, 479, 382, 629, 1008, 275, 379, 104, 9, 370, 548, 412, 218, 775, 552, 3, 471, 90, 282, 95, 1088, 23, 1102, 1030, 763, 782, 783, 27, 646, 522, 840, 1053, 343, 740, 788, 530, 1069, 257, 677, 404, 583, 33, 862, 325, 669, 792, 532, 52, 378, 187, 82, 698, 1039, 644, 441, 1011, 64, 996, 307, 211, 631, 311, 1132, 1094, 462, 728, 110, 1044, 1150, 215, 139, 945, 1055, 748, 588, 638, 440, 749, 585, 17, 1205, 704, 480, 572, 1092, 790, 1121, 1127, 413, 43, 429, 1012, 711, 1151, 579, 756, 279, 1203, 1214, 1208, 1105, 993, 931, 770, 673, 1183, 427, 924, 828, 750, 1163, 575, 258, 61, 297, 363, 73, 633, 285, 586, 37, 518, 316, 693, 300, 28, 871, 392, 942, 916, 712, 135, 1147, 294, 235, 1176, 248, 397, 974, 1194, 29, 76, 111, 78, 72, 445, 1070, 146, 1009, 331, 398, 1073, 129, 1107, 863, 1206, 987, 554, 746, 825, 1170, 994, 92, 320, 1007, 442, 0, 502, 598, 340, 469, 1061, 281, 415, 636, 390, 1096, 506, 1141, 1157, 31, 567, 776, 899, 594, 1140, 1111, 769, 845, 543, 906, 784, 648, 882, 461, 649, 647, 203, 1064, 214, 510, 77, 274, 39, 550, 290, 393, 683, 893, 639, 317, 11, 263, 789, 180, 1114, 624, 63, 541, 326, 276, 30, 376, 476, 949, 1129, 984, 595, 130, 403, 38, 611, 223, 986, 525, 477, 861, 86, 989, 226, 239, 229, 371, 1123, 822, 608, 1179, 50, 988, 25, 321, 373, 913, 1162, 847, 768, 798, 1085, 456, 952, 377, 287, 81, 927, 306, 944, 487, 888, 93, 640, 947, 898, 20, 496, 760, 1063, 684, 1144, 171, 973, 909, 920, 1035, 744, 910, 1017, 209, 484, 500, 141, 148, 617, 97, 1143, 864, 710, 547, 576, 515, 184, 1117, 727, 880, 88, 144, 938, 830, 368, 983, 692, 752, 765, 747, 1177, 85, 212, 34, 1178, 405, 613, 359, 123, 781, 578, 1022, 1019, 13, 682, 150, 105, 524, 1112, 811, 729, 384, 591, 1212, 678, 833, 127, 115, 691, 886, 237, 855, 106, 191, 1021, 620, 895, 45, 426, 1152, 787, 972, 1187, 625, 977, 922, 660, 250, 859, 298, 656, 832, 341, 925, 687, 244, 564, 835, 939, 870, 460, 616, 971, 894, 960, 301, 44, 120, 810, 438, 434, 779, 745, 902, 408, 483, 35, 537, 1180, 1025, 514, 328, 344, 1054, 303, 464, 334, 452, 1086, 200, 697, 252, 418, 216, 41, 519, 468, 566, 352, 738, 465, 654, 1029, 261, 520, 1185, 210, 1036, 1100, 607, 1046, 1120, 1079, 495, 1031, 169, 877, 126, 936, 731, 48, 713, 601, 1188, 675, 433, 632, 149, 1125, 315, 489, 131, 1010, 764, 75, 815, 1153, 400, 21, 387, 670, 231, 158, 161, 1192, 1066, 799, 402, 347, 1033, 956, 323, 780, 259, 475, 236, 264, 396, 665, 488, 948, 1142, 1181, 2, 641, 655, 198, 167, 178, 192, 243, 933, 1198, 501, 424, 791, 1146, 4, 1003, 609, 695, 60, 414, 1084, 70, 152, 269, 89, 319, 1184, 168, 793, 410, 542, 915, 926, 221, 853, 932, 109, 1038, 265, 778, 928, 652, 527, 254, 842, 145, 305, 367, 435, 472, 176, 1204, 1034, 119, 714, 245, 580, 562, 754, 637, 716, 8, 159, 207, 112, 540, 1091, 1197, 1126, 809, 896, 976, 589, 155, 1042, 416, 1131, 1130, 950, 940, 1196, 539, 336, 188, 386, 857, 364, 1015, 999, 722, 674, 545, 812, 721, 66, 797, 1145, 383, 645, 1154, 350, 1048, 67, 685, 1199, 302, 1202, 606, 409, 240, 114, 284, 602, 571, 138, 447, 734, 419, 664, 995, 353, 873, 998, 968, 958, 1101, 929, 1191, 934, 737, 124, 503, 1156, 478, 538, 814, 819, 544, 804, 563, 557, 356, 246, 785, 1056, 508, 241, 204, 574, 448, 1097, 310, 854, 1006, 1186, 357, 634, 268, 1165, 808, 561, 42, 715, 219, 883, 494, 102, 431, 935, 875, 962, 901, 493, 1164, 1041, 374, 1087, 267, 453, 959, 708, 505, 199, 801, 980, 761, 1075, 1166, 195, 375, 596, 868, 856, 277, 701, 40, 1016, 473, 911, 516, 865, 553, 430, 549, 965, 757, 361, 726, 193, 68, 1071, 360, 767, 917, 253, 904, 389, 395, 1175, 511, 406, 666, 348, 657, 157, 381, 884, 485, 132, 889, 492, 454, 14, 162, 923, 47, 153, 517, 723, 354, 339, 720, 930, 719, 1020, 1099, 597, 1051, 705, 556, 163, 482, 1135, 12, 394, 451, 185, 592, 843, 964, 852, 62, 286, 1119, 222, 450, 529, 953, 332, 568, 1037, 1001, 220, 170, 771, 612, 165, 509, 979, 1089, 173, 806, 291, 55, 628, 1032, 1078, 99, 1090, 457, 772, 603, 892, 1209, 16, 136, 181, 820, 725, 117, 650, 362, 1043, 1050, 113, 762, 985, 458, 299, 324, 774, 869, 850, 266, 327, 891, 1213, 718, 1049, 680, 280, 401, 582, 1139, 1113, 1171, 560, 823, 982, 391, 600, 841, 807, 735, 407, 1028, 667, 186, 838, 309, 573, 619, 160, 459, 969, 866, 1122, 615, 642, 751, 724, 630, 1110, 190, 1193, 455, 242, 333, 1210, 1104, 777, 786, 1023, 388, 829, 1158, 7, 313, 296, 46, 467, 32, 1115, 507, 227, 1103, 1124, 1005, 663, 202, 943, 289, 858, 755, 358, 49, 98, 604, 937, 366, 1137, 94, 1074, 166, 1081, 534, 839, 694, 803, 262, 423, 739, 794, 156, 288, 228, 372, 908, 661, 183, 439, 795, 80, 151, 1173, 526, 232, 1095, 730, 308, 1182, 1068, 849, 887, 975, 260, 623, 732, 497, 380, 736, 411, 69, 577, 51, 463, 1024, 551, 903, 314, 1190, 521, 318, 1076, 194, 836, 1116, 981, 133, 707, 349, 53, 536, 671, 860, 116, 1160, 271, 22, 827, 610, 490, 802, 256, 335, 970, 570, 108, 963, 555, 706, 474, 528, 676, 1059, 128, 121, 1211, 533, 172, 399, 758, 627, 1072, 1067, 914, 234, 879, 907, 821, 696, 1080, 621, 1200, 921, 71, 337, 213, 967, 444, 605, 449, 134, 1189, 991, 1026, 990, 147, 1169, 643, 717, 659, 946, 436, 1000, 230, 826, 978, 897, 796, 565, 504, 824, 224, 742, 499, 443, 1077, 955, 6, 292, 5, 428, 686, 59, 653, 177, 816, 79, 15, 137]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3516266246836159
the save name prefix for this run is:  chkpt-ID_3516266246836159_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 706
rank avg (pred): 0.567 +- 0.007
mrr vals (pred, true): 0.017, 0.048
batch losses (mrrl, rdl): 0.0, 0.0002708711

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 936
rank avg (pred): 0.330 +- 0.251
mrr vals (pred, true): 0.217, 0.052
batch losses (mrrl, rdl): 0.0, 0.0002168981

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1067
rank avg (pred): 0.308 +- 0.272
mrr vals (pred, true): 0.324, 0.405
batch losses (mrrl, rdl): 0.0, 0.0013270727

Epoch over!
epoch time: 14.797

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 819
rank avg (pred): 0.281 +- 0.271
mrr vals (pred, true): 0.390, 0.442
batch losses (mrrl, rdl): 0.0, 0.0011271053

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 545
rank avg (pred): 0.332 +- 0.279
mrr vals (pred, true): 0.304, 0.226
batch losses (mrrl, rdl): 0.0, 0.0009660363

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 951
rank avg (pred): 0.329 +- 0.279
mrr vals (pred, true): 0.325, 0.054
batch losses (mrrl, rdl): 0.0, 0.0002210404

Epoch over!
epoch time: 14.748

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 190
rank avg (pred): 0.302 +- 0.284
mrr vals (pred, true): 0.393, 0.061
batch losses (mrrl, rdl): 0.0, 0.0003340346

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 689
rank avg (pred): 0.324 +- 0.284
mrr vals (pred, true): 0.343, 0.054
batch losses (mrrl, rdl): 0.0, 0.0002229566

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 8
rank avg (pred): 0.331 +- 0.283
mrr vals (pred, true): 0.314, 0.331
batch losses (mrrl, rdl): 0.0, 0.0014706135

Epoch over!
epoch time: 14.776

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 127
rank avg (pred): 0.303 +- 0.285
mrr vals (pred, true): 0.411, 0.054
batch losses (mrrl, rdl): 0.0, 0.0003545471

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 979
rank avg (pred): 0.303 +- 0.285
mrr vals (pred, true): 0.408, 0.525
batch losses (mrrl, rdl): 0.0, 0.0015800266

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 869
rank avg (pred): 0.290 +- 0.283
mrr vals (pred, true): 0.451, 0.060
batch losses (mrrl, rdl): 0.0, 0.0003408462

Epoch over!
epoch time: 14.831

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 642
rank avg (pred): 0.374 +- 0.273
mrr vals (pred, true): 0.264, 0.057
batch losses (mrrl, rdl): 0.0, 8.4623e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 781
rank avg (pred): 0.314 +- 0.290
mrr vals (pred, true): 0.412, 0.052
batch losses (mrrl, rdl): 0.0, 0.0002783862

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 740
rank avg (pred): 0.288 +- 0.281
mrr vals (pred, true): 0.458, 0.407
batch losses (mrrl, rdl): 0.0, 0.0011307016

Epoch over!
epoch time: 14.807

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 95
rank avg (pred): 0.319 +- 0.285
mrr vals (pred, true): 0.372, 0.053
batch losses (mrrl, rdl): 1.03824687, 0.0002402802

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 293
rank avg (pred): 0.424 +- 0.204
mrr vals (pred, true): 0.138, 0.282
batch losses (mrrl, rdl): 0.2086870223, 0.0019062612

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 980
rank avg (pred): 0.378 +- 0.189
mrr vals (pred, true): 0.152, 0.528
batch losses (mrrl, rdl): 1.4179390669, 0.0022829475

Epoch over!
epoch time: 15.015

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1178
rank avg (pred): 0.458 +- 0.192
mrr vals (pred, true): 0.117, 0.046
batch losses (mrrl, rdl): 0.0455239192, 2.24081e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 631
rank avg (pred): 0.450 +- 0.182
mrr vals (pred, true): 0.115, 0.054
batch losses (mrrl, rdl): 0.0420312248, 3.0608e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 295
rank avg (pred): 0.401 +- 0.203
mrr vals (pred, true): 0.152, 0.295
batch losses (mrrl, rdl): 0.2020832896, 0.0016252415

Epoch over!
epoch time: 14.955

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 959
rank avg (pred): 0.442 +- 0.182
mrr vals (pred, true): 0.125, 0.055
batch losses (mrrl, rdl): 0.0555123948, 2.80395e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 609
rank avg (pred): 0.444 +- 0.171
mrr vals (pred, true): 0.118, 0.051
batch losses (mrrl, rdl): 0.0465102345, 4.44698e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 827
rank avg (pred): 0.399 +- 0.193
mrr vals (pred, true): 0.160, 0.362
batch losses (mrrl, rdl): 0.4097793102, 0.0018521931

Epoch over!
epoch time: 14.953

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 732
rank avg (pred): 0.343 +- 0.185
mrr vals (pred, true): 0.189, 0.626
batch losses (mrrl, rdl): 1.911364913, 0.0021596551

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1061
rank avg (pred): 0.321 +- 0.177
mrr vals (pred, true): 0.193, 0.496
batch losses (mrrl, rdl): 0.91664958, 0.0015486461

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 356
rank avg (pred): 0.395 +- 0.181
mrr vals (pred, true): 0.148, 0.058
batch losses (mrrl, rdl): 0.0951480046, 8.83425e-05

Epoch over!
epoch time: 14.996

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 529
rank avg (pred): 0.451 +- 0.151
mrr vals (pred, true): 0.096, 0.212
batch losses (mrrl, rdl): 0.1332875937, 0.0019395996

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 506
rank avg (pred): 0.434 +- 0.153
mrr vals (pred, true): 0.105, 0.233
batch losses (mrrl, rdl): 0.1634065211, 0.0017544709

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 63
rank avg (pred): 0.409 +- 0.165
mrr vals (pred, true): 0.128, 0.291
batch losses (mrrl, rdl): 0.2653428316, 0.0017051564

Epoch over!
epoch time: 15.013

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1102
rank avg (pred): 0.287 +- 0.166
mrr vals (pred, true): 0.216, 0.058
batch losses (mrrl, rdl): 0.2752773762, 0.0005202577

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 465
rank avg (pred): 0.389 +- 0.169
mrr vals (pred, true): 0.147, 0.057
batch losses (mrrl, rdl): 0.0944799781, 0.0001153434

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 374
rank avg (pred): 0.399 +- 0.158
mrr vals (pred, true): 0.123, 0.051
batch losses (mrrl, rdl): 0.0530523434, 8.67551e-05

Epoch over!
epoch time: 15.035

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 838
rank avg (pred): 0.312 +- 0.153
mrr vals (pred, true): 0.177, 0.056
batch losses (mrrl, rdl): 0.1618537009, 0.0004830698

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 624
rank avg (pred): 0.409 +- 0.137
mrr vals (pred, true): 0.102, 0.052
batch losses (mrrl, rdl): 0.0271111205, 0.0001079145

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 172
rank avg (pred): 0.391 +- 0.154
mrr vals (pred, true): 0.123, 0.055
batch losses (mrrl, rdl): 0.0534544215, 0.0001223748

Epoch over!
epoch time: 15.034

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1180
rank avg (pred): 0.405 +- 0.141
mrr vals (pred, true): 0.107, 0.053
batch losses (mrrl, rdl): 0.0327907205, 8.15513e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1193
rank avg (pred): 0.364 +- 0.156
mrr vals (pred, true): 0.151, 0.055
batch losses (mrrl, rdl): 0.1016912982, 0.0001943613

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 436
rank avg (pred): 0.378 +- 0.154
mrr vals (pred, true): 0.138, 0.057
batch losses (mrrl, rdl): 0.0774536878, 0.0001639935

Epoch over!
epoch time: 15.007

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1046
rank avg (pred): 0.337 +- 0.140
mrr vals (pred, true): 0.149, 0.049
batch losses (mrrl, rdl): 0.098726213, 0.000423153

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 388
rank avg (pred): 0.378 +- 0.152
mrr vals (pred, true): 0.131, 0.051
batch losses (mrrl, rdl): 0.0651817098, 0.0001590582

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 542
rank avg (pred): 0.386 +- 0.136
mrr vals (pred, true): 0.113, 0.248
batch losses (mrrl, rdl): 0.1823969334, 0.001421244

Epoch over!
epoch time: 15.004

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 377
rank avg (pred): 0.372 +- 0.148
mrr vals (pred, true): 0.129, 0.057
batch losses (mrrl, rdl): 0.0631539971, 0.0001980667

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1191
rank avg (pred): 0.376 +- 0.144
mrr vals (pred, true): 0.138, 0.057
batch losses (mrrl, rdl): 0.0779465139, 0.0001244083

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 610
rank avg (pred): 0.378 +- 0.129
mrr vals (pred, true): 0.109, 0.059
batch losses (mrrl, rdl): 0.0351605378, 0.0001863028

Epoch over!
epoch time: 15.014

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.373 +- 0.139
mrr vals (pred, true): 0.129, 0.328

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.11129 	 0.04544 	 m..s
   93 	     1 	 0.14085 	 0.04739 	 m..s
   58 	     2 	 0.13247 	 0.04773 	 m..s
   26 	     3 	 0.11659 	 0.04830 	 m..s
   76 	     4 	 0.13565 	 0.04863 	 m..s
   14 	     5 	 0.10721 	 0.04897 	 m..s
   56 	     6 	 0.13221 	 0.04910 	 m..s
   24 	     7 	 0.11649 	 0.04945 	 m..s
   29 	     8 	 0.11790 	 0.04954 	 m..s
   75 	     9 	 0.13560 	 0.04960 	 m..s
   72 	    10 	 0.13447 	 0.04971 	 m..s
   99 	    11 	 0.17119 	 0.04973 	 MISS
   99 	    12 	 0.17119 	 0.05018 	 MISS
   15 	    13 	 0.10729 	 0.05043 	 m..s
   99 	    14 	 0.17119 	 0.05068 	 MISS
   99 	    15 	 0.17119 	 0.05105 	 MISS
   60 	    16 	 0.13277 	 0.05114 	 m..s
   99 	    17 	 0.17119 	 0.05116 	 MISS
   22 	    18 	 0.11552 	 0.05117 	 m..s
   45 	    19 	 0.12706 	 0.05121 	 m..s
   89 	    20 	 0.14034 	 0.05154 	 m..s
   47 	    21 	 0.12910 	 0.05179 	 m..s
   99 	    22 	 0.17119 	 0.05191 	 MISS
   99 	    23 	 0.17119 	 0.05212 	 MISS
   99 	    24 	 0.17119 	 0.05216 	 MISS
    5 	    25 	 0.09771 	 0.05219 	 m..s
    7 	    26 	 0.10304 	 0.05224 	 m..s
   99 	    27 	 0.17119 	 0.05224 	 MISS
   42 	    28 	 0.12484 	 0.05250 	 m..s
   99 	    29 	 0.17119 	 0.05260 	 MISS
   62 	    30 	 0.13315 	 0.05269 	 m..s
   96 	    31 	 0.16879 	 0.05277 	 MISS
   68 	    32 	 0.13427 	 0.05279 	 m..s
   84 	    33 	 0.13769 	 0.05282 	 m..s
   27 	    34 	 0.11675 	 0.05282 	 m..s
   81 	    35 	 0.13681 	 0.05285 	 m..s
   69 	    36 	 0.13442 	 0.05294 	 m..s
   57 	    37 	 0.13241 	 0.05300 	 m..s
   38 	    38 	 0.12400 	 0.05303 	 m..s
   50 	    39 	 0.13080 	 0.05312 	 m..s
   21 	    40 	 0.11513 	 0.05315 	 m..s
   99 	    41 	 0.17119 	 0.05331 	 MISS
   11 	    42 	 0.10530 	 0.05340 	 m..s
   12 	    43 	 0.10614 	 0.05341 	 m..s
   86 	    44 	 0.13830 	 0.05341 	 m..s
   36 	    45 	 0.12245 	 0.05341 	 m..s
   83 	    46 	 0.13748 	 0.05348 	 m..s
   99 	    47 	 0.17119 	 0.05349 	 MISS
   10 	    48 	 0.10440 	 0.05391 	 m..s
   95 	    49 	 0.16833 	 0.05396 	 MISS
    2 	    50 	 0.09609 	 0.05405 	 m..s
   44 	    51 	 0.12557 	 0.05428 	 m..s
   66 	    52 	 0.13402 	 0.05434 	 m..s
   99 	    53 	 0.17119 	 0.05434 	 MISS
   35 	    54 	 0.12244 	 0.05438 	 m..s
   99 	    55 	 0.17119 	 0.05440 	 MISS
   70 	    56 	 0.13442 	 0.05457 	 m..s
   99 	    57 	 0.17119 	 0.05459 	 MISS
    4 	    58 	 0.09723 	 0.05487 	 m..s
   87 	    59 	 0.13869 	 0.05500 	 m..s
   48 	    60 	 0.12958 	 0.05526 	 m..s
    9 	    61 	 0.10410 	 0.05528 	 m..s
   98 	    62 	 0.17030 	 0.05534 	 MISS
   51 	    63 	 0.13099 	 0.05541 	 m..s
    2 	    64 	 0.09609 	 0.05543 	 m..s
   99 	    65 	 0.17119 	 0.05555 	 MISS
   90 	    66 	 0.14047 	 0.05565 	 m..s
   13 	    67 	 0.10635 	 0.05601 	 m..s
   80 	    68 	 0.13676 	 0.05609 	 m..s
   82 	    69 	 0.13746 	 0.05614 	 m..s
   78 	    70 	 0.13643 	 0.05647 	 m..s
   23 	    71 	 0.11560 	 0.05656 	 m..s
    8 	    72 	 0.10340 	 0.05680 	 m..s
   38 	    73 	 0.12400 	 0.05689 	 m..s
   63 	    74 	 0.13337 	 0.05691 	 m..s
   33 	    75 	 0.12103 	 0.05775 	 m..s
   16 	    76 	 0.10754 	 0.05791 	 m..s
   67 	    77 	 0.13410 	 0.05846 	 m..s
   85 	    78 	 0.13777 	 0.05904 	 m..s
   99 	    79 	 0.17119 	 0.05917 	 MISS
   99 	    80 	 0.17119 	 0.05990 	 MISS
   99 	    81 	 0.17119 	 0.05993 	 MISS
   77 	    82 	 0.13569 	 0.06012 	 m..s
   52 	    83 	 0.13108 	 0.06325 	 m..s
   90 	    84 	 0.14047 	 0.15084 	 ~...
    0 	    85 	 0.09540 	 0.19762 	 MISS
    1 	    86 	 0.09586 	 0.21447 	 MISS
    6 	    87 	 0.10246 	 0.21874 	 MISS
   19 	    88 	 0.11144 	 0.22567 	 MISS
   20 	    89 	 0.11489 	 0.23461 	 MISS
   16 	    90 	 0.10754 	 0.23557 	 MISS
   24 	    91 	 0.11649 	 0.25404 	 MISS
   28 	    92 	 0.11683 	 0.25657 	 MISS
   74 	    93 	 0.13557 	 0.26725 	 MISS
   49 	    94 	 0.12964 	 0.27425 	 MISS
   43 	    95 	 0.12497 	 0.27777 	 MISS
   41 	    96 	 0.12449 	 0.28377 	 MISS
   37 	    97 	 0.12295 	 0.28388 	 MISS
   30 	    98 	 0.11881 	 0.28456 	 MISS
   55 	    99 	 0.13199 	 0.28774 	 MISS
   87 	   100 	 0.13869 	 0.28900 	 MISS
   31 	   101 	 0.12010 	 0.29126 	 MISS
   40 	   102 	 0.12423 	 0.29342 	 MISS
   32 	   103 	 0.12092 	 0.30022 	 MISS
   61 	   104 	 0.13308 	 0.30348 	 MISS
   53 	   105 	 0.13151 	 0.30880 	 MISS
   33 	   106 	 0.12103 	 0.31559 	 MISS
   46 	   107 	 0.12869 	 0.32805 	 MISS
   70 	   108 	 0.13442 	 0.32831 	 MISS
   73 	   109 	 0.13491 	 0.33107 	 MISS
   59 	   110 	 0.13259 	 0.33204 	 MISS
   64 	   111 	 0.13353 	 0.33295 	 MISS
   79 	   112 	 0.13664 	 0.33351 	 MISS
   65 	   113 	 0.13373 	 0.34025 	 MISS
   92 	   114 	 0.14064 	 0.34731 	 MISS
   54 	   115 	 0.13166 	 0.35846 	 MISS
   99 	   116 	 0.17119 	 0.37521 	 MISS
   94 	   117 	 0.16482 	 0.39756 	 MISS
   99 	   118 	 0.17119 	 0.47946 	 MISS
   99 	   119 	 0.17119 	 0.61365 	 MISS
   96 	   120 	 0.16879 	 0.61829 	 MISS
==========================================
r_mrr = 0.04522072896361351
r2_mrr = -0.014479398727416992
spearmanr_mrr@5 = nan
spearmanr_mrr@10 = nan
spearmanr_mrr@50 = 0.8110320568084717
spearmanr_mrr@100 = 0.8850807547569275
spearmanr_mrr@All = 0.8389771580696106
==========================================
test time: 0.45
Done Testing dataset Kinships
total time taken: 232.17922258377075
training time taken: 224.4480218887329
TWIG out ;))
Ablation done!
The best results were: None
The best settings found were:

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 6596451614249046
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [40, 856, 354, 243, 1047, 930, 71, 444, 670, 545, 432, 717, 495, 1096, 151, 1064, 131, 17, 925, 316, 303, 464, 408, 699, 598, 787, 589, 659, 415, 861, 218, 199, 149, 29, 519, 380, 417, 1199, 429, 637, 472, 1013, 1062, 1011, 348, 312, 279, 987, 1092, 1145, 487, 225, 301, 998, 773, 179, 802, 986, 1114, 569, 1131, 591, 808, 1007, 819, 99, 448, 1132, 1044, 98, 1192, 30, 9, 664, 996, 751, 85, 441, 1112, 782, 1197, 70, 876, 244, 197, 19, 509, 570, 237, 35, 1141, 720, 726, 318, 1063, 542, 127, 496, 831, 1035, 872, 1214, 502, 11, 425, 280, 378, 812, 778, 1060, 638, 174, 484, 798, 1032, 1061, 1105, 739, 159, 248, 411]
valid_ids (0): []
train_ids (1094): [953, 363, 676, 1193, 1117, 758, 368, 1150, 173, 1140, 682, 546, 356, 1116, 592, 297, 619, 281, 193, 929, 427, 1118, 265, 379, 48, 603, 794, 599, 612, 57, 78, 374, 391, 941, 5, 737, 525, 473, 23, 962, 41, 718, 112, 167, 177, 918, 610, 1049, 658, 890, 486, 990, 416, 471, 55, 584, 1159, 491, 649, 1170, 952, 1058, 1201, 1026, 605, 656, 1079, 877, 435, 183, 1055, 246, 724, 561, 475, 403, 330, 668, 571, 433, 1110, 481, 103, 404, 1176, 974, 747, 498, 694, 826, 337, 774, 250, 544, 50, 991, 860, 1134, 67, 678, 940, 1078, 516, 950, 221, 172, 1172, 227, 674, 68, 869, 1037, 684, 314, 180, 109, 1187, 393, 1074, 326, 437, 198, 182, 617, 206, 597, 345, 700, 306, 479, 60, 508, 1208, 154, 257, 799, 1083, 290, 927, 882, 240, 847, 985, 184, 932, 145, 640, 108, 970, 390, 65, 25, 1143, 431, 106, 192, 1070, 1046, 646, 514, 234, 1059, 999, 335, 1207, 135, 976, 152, 83, 652, 552, 252, 606, 191, 84, 797, 870, 588, 818, 322, 258, 352, 625, 833, 719, 370, 650, 321, 816, 115, 777, 136, 144, 789, 942, 873, 853, 91, 107, 230, 1183, 129, 181, 697, 195, 755, 527, 1, 1169, 738, 984, 644, 517, 1137, 405, 412, 533, 308, 820, 710, 229, 434, 1088, 194, 90, 1165, 474, 1135, 439, 1202, 702, 513, 746, 1174, 743, 342, 236, 245, 754, 386, 249, 241, 1151, 982, 351, 66, 273, 259, 483, 854, 365, 1077, 862, 955, 537, 261, 510, 285, 949, 934, 1136, 223, 716, 965, 759, 1010, 232, 620, 500, 274, 213, 975, 834, 87, 141, 501, 642, 383, 1128, 707, 385, 613, 292, 1076, 943, 1147, 733, 900, 74, 752, 157, 62, 105, 395, 1095, 628, 1189, 456, 376, 667, 966, 634, 482, 842, 1041, 547, 884, 828, 864, 16, 536, 26, 800, 1023, 1001, 1084, 825, 226, 196, 735, 215, 1153, 1033, 58, 708, 443, 741, 165, 372, 212, 538, 608, 624, 27, 457, 647, 220, 898, 377, 729, 521, 373, 904, 219, 711, 305, 242, 643, 899, 12, 558, 394, 418, 761, 905, 211, 791, 1212, 333, 1210, 548, 45, 917, 32, 160, 309, 1109, 260, 1004, 1127, 187, 878, 426, 51, 288, 621, 458, 130, 75, 304, 1205, 1188, 399, 779, 703, 256, 604, 298, 132, 1206, 896, 117, 1133, 557, 323, 1085, 1175, 948, 360, 551, 630, 302, 1031, 398, 1091, 400, 979, 367, 1103, 564, 566, 410, 836, 559, 922, 217, 1022, 1106, 413, 786, 840, 291, 43, 671, 284, 1099, 122, 549, 848, 1009, 963, 375, 912, 936, 512, 645, 31, 283, 396, 865, 1016, 822, 506, 222, 307, 1098, 1005, 1052, 111, 964, 1177, 575, 419, 73, 666, 829, 504, 92, 734, 683, 147, 470, 207, 293, 857, 140, 728, 736, 845, 420, 315, 430, 709, 680, 1082, 185, 80, 654, 771, 919, 269, 926, 732, 933, 59, 202, 781, 233, 1158, 846, 1161, 4, 387, 171, 275, 1014, 715, 88, 121, 550, 524, 727, 823, 515, 300, 691, 169, 1086, 937, 636, 436, 344, 805, 289, 748, 881, 272, 995, 1071, 959, 837, 113, 447, 1051, 824, 915, 821, 332, 54, 21, 480, 69, 1045, 477, 595, 883, 1173, 287, 1018, 1093, 795, 572, 102, 453, 1190, 1015, 346, 499, 669, 95, 170, 1050, 1067, 200, 1113, 266, 247, 895, 282, 555, 578, 796, 347, 1209, 161, 1115, 449, 86, 189, 1182, 320, 1036, 908, 665, 1163, 1156, 815, 651, 693, 633, 493, 690, 407, 886, 1040, 138, 362, 263, 463, 1024, 879, 44, 166, 343, 583, 713, 104, 1102, 270, 685, 921, 830, 485, 123, 553, 692, 489, 327, 476, 579, 535, 1097, 208, 622, 1029, 851, 1198, 1017, 235, 698, 764, 63, 543, 858, 568, 397, 695, 409, 310, 349, 677, 540, 350, 772, 6, 231, 1101, 749, 0, 1185, 461, 20, 627, 1065, 971, 1171, 887, 462, 1119, 980, 522, 868, 497, 814, 655, 523, 1138, 1104, 641, 762, 13, 361, 389, 722, 175, 600, 745, 8, 1149, 954, 353, 909, 817, 278, 587, 973, 77, 1130, 148, 1000, 567, 961, 488, 601, 760, 89, 339, 712, 3, 1121, 1157, 364, 594, 39, 968, 186, 268, 892, 271, 33, 766, 139, 163, 1090, 1122, 96, 623, 616, 660, 530, 1139, 661, 1089, 945, 406, 1186, 15, 581, 730, 916, 388, 295, 1073, 1075, 534, 188, 381, 585, 835, 577, 672, 866, 903, 423, 146, 871, 255, 446, 1126, 1108, 783, 118, 1125, 1094, 355, 455, 1144, 276, 137, 478, 1006, 494, 371, 639, 635, 469, 1168, 317, 626, 528, 1012, 1019, 209, 596, 36, 859, 1148, 1008, 22, 704, 319, 1081, 422, 972, 827, 1068, 631, 262, 1087, 1053, 993, 775, 1162, 756, 459, 924, 392, 1021, 983, 253, 648, 366, 931, 565, 277, 532, 1043, 338, 1184, 744, 804, 205, 168, 341, 988, 757, 768, 384, 14, 451, 511, 238, 153, 1057, 855, 753, 47, 688, 843, 944, 541, 340, 880, 1200, 1027, 18, 1181, 28, 947, 267, 580, 442, 125, 1072, 1191, 239, 369, 1107, 440, 460, 867, 216, 554, 539, 1054, 811, 294, 844, 891, 382, 210, 679, 24, 1034, 1020, 37, 299, 629, 740, 1003, 618, 780, 958, 64, 529, 663, 1030, 526, 79, 1042, 1069, 852, 401, 214, 124, 706, 507, 1179, 531, 424, 923, 832, 42, 939, 7, 116, 997, 150, 989, 928, 334, 1100, 889, 1080, 849, 776, 264, 725, 902, 938, 742, 156, 34, 468, 901, 897, 76, 56, 602, 421, 142, 920, 143, 503, 492, 328, 910, 10, 311, 763, 93, 1160, 329, 82, 981, 1056, 49, 632, 935, 52, 977, 960, 801, 967, 445, 574, 1194, 957, 673, 1204, 1154, 53, 359, 38, 176, 101, 792, 978, 765, 615, 785, 1180, 784, 204, 1038, 696, 46, 114, 1039, 614, 788, 705, 653, 1025, 158, 358, 793, 863, 254, 1066, 992, 807, 1152, 573, 336, 1155, 1028, 164, 110, 563, 689, 190, 61, 951, 286, 582, 770, 134, 907, 590, 128, 809, 914, 296, 1146, 119, 1196, 803, 325, 875, 850, 100, 969, 838, 750, 414, 520, 609, 1124, 1164, 1178, 228, 888, 224, 810, 1111, 806, 402, 769, 723, 438, 576, 721, 675, 560, 155, 556, 994, 1048, 714, 518, 1123, 1166, 657, 133, 906, 81, 593, 1120, 586, 203, 790, 1142, 466, 251, 450, 162, 1167, 913, 894, 331, 731, 607, 452, 686, 94, 490, 662, 2, 687, 505, 841, 767, 956, 681, 893, 313, 178, 1129, 1203, 357, 946, 839, 97, 201, 611, 467, 465, 72, 911, 1211, 428, 1195, 885, 813, 562, 1002, 1213, 120, 701, 454, 324, 126, 874]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  578551382359514
the save name prefix for this run is:  chkpt-ID_578551382359514_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 177
rank avg (pred): 0.514 +- 0.002
mrr vals (pred, true): 0.019, 0.059
batch losses (mrrl, rdl): 0.0, 0.0001488678

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 214
rank avg (pred): 0.488 +- 0.075
mrr vals (pred, true): 0.020, 0.060
batch losses (mrrl, rdl): 0.0, 8.13571e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 386
rank avg (pred): 0.479 +- 0.261
mrr vals (pred, true): 0.031, 0.054
batch losses (mrrl, rdl): 0.0, 2.3795e-06

Epoch over!
epoch time: 14.861

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 447
rank avg (pred): 0.452 +- 0.258
mrr vals (pred, true): 0.035, 0.050
batch losses (mrrl, rdl): 0.0, 1.4818e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 850
rank avg (pred): 0.475 +- 0.268
mrr vals (pred, true): 0.035, 0.051
batch losses (mrrl, rdl): 0.0, 1.999e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 491
rank avg (pred): 0.117 +- 0.135
mrr vals (pred, true): 0.191, 0.257
batch losses (mrrl, rdl): 0.0, 2.537e-07

Epoch over!
epoch time: 14.804

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 926
rank avg (pred): 0.451 +- 0.268
mrr vals (pred, true): 0.042, 0.044
batch losses (mrrl, rdl): 0.0, 1.69439e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 946
rank avg (pred): 0.472 +- 0.254
mrr vals (pred, true): 0.034, 0.055
batch losses (mrrl, rdl): 0.0, 8.268e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1140
rank avg (pred): 0.144 +- 0.156
mrr vals (pred, true): 0.152, 0.319
batch losses (mrrl, rdl): 0.0, 4.27909e-05

Epoch over!
epoch time: 14.812

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 547
rank avg (pred): 0.151 +- 0.171
mrr vals (pred, true): 0.169, 0.227
batch losses (mrrl, rdl): 0.0, 7.3481e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 428
rank avg (pred): 0.461 +- 0.271
mrr vals (pred, true): 0.042, 0.048
batch losses (mrrl, rdl): 0.0, 2.1684e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 848
rank avg (pred): 0.459 +- 0.266
mrr vals (pred, true): 0.042, 0.060
batch losses (mrrl, rdl): 0.0, 1.217e-07

Epoch over!
epoch time: 14.827

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1184
rank avg (pred): 0.475 +- 0.266
mrr vals (pred, true): 0.037, 0.049
batch losses (mrrl, rdl): 0.0, 3.897e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 296
rank avg (pred): 0.100 +- 0.133
mrr vals (pred, true): 0.245, 0.297
batch losses (mrrl, rdl): 0.0, 2.19079e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1164
rank avg (pred): 0.448 +- 0.269
mrr vals (pred, true): 0.049, 0.054
batch losses (mrrl, rdl): 0.0, 1.67449e-05

Epoch over!
epoch time: 14.808

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 850
rank avg (pred): 0.464 +- 0.264
mrr vals (pred, true): 0.041, 0.051
batch losses (mrrl, rdl): 0.0007904189, 2.6713e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 761
rank avg (pred): 0.414 +- 0.256
mrr vals (pred, true): 0.078, 0.047
batch losses (mrrl, rdl): 0.007792999, 5.64987e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 307
rank avg (pred): 0.220 +- 0.292
mrr vals (pred, true): 0.329, 0.290
batch losses (mrrl, rdl): 0.015511658, 0.00017965

Epoch over!
epoch time: 15.051

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 888
rank avg (pred): 0.432 +- 0.215
mrr vals (pred, true): 0.041, 0.056
batch losses (mrrl, rdl): 0.0007457222, 4.78894e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 65
rank avg (pred): 0.277 +- 0.322
mrr vals (pred, true): 0.293, 0.309
batch losses (mrrl, rdl): 0.0024167236, 0.0005804868

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 511
rank avg (pred): 0.433 +- 0.326
mrr vals (pred, true): 0.202, 0.217
batch losses (mrrl, rdl): 0.0023896643, 0.001553214

Epoch over!
epoch time: 15.044

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 995
rank avg (pred): 0.142 +- 0.241
mrr vals (pred, true): 0.352, 0.458
batch losses (mrrl, rdl): 0.1125772148, 0.0001288984

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1207
rank avg (pred): 0.467 +- 0.252
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002646005, 2.6494e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 153
rank avg (pred): 0.427 +- 0.217
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 0.0001134614, 3.74481e-05

Epoch over!
epoch time: 15.049

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 286
rank avg (pred): 0.216 +- 0.272
mrr vals (pred, true): 0.321, 0.301
batch losses (mrrl, rdl): 0.004198397, 0.0002387936

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 844
rank avg (pred): 0.380 +- 0.194
mrr vals (pred, true): 0.062, 0.059
batch losses (mrrl, rdl): 0.0014188251, 0.0001326881

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 352
rank avg (pred): 0.469 +- 0.237
mrr vals (pred, true): 0.041, 0.051
batch losses (mrrl, rdl): 0.0008675567, 4.0089e-06

Epoch over!
epoch time: 15.015

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 245
rank avg (pred): 0.222 +- 0.265
mrr vals (pred, true): 0.322, 0.380
batch losses (mrrl, rdl): 0.0336178765, 0.0004222826

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 120
rank avg (pred): 0.444 +- 0.231
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 6.02e-08, 2.00823e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1079
rank avg (pred): 0.075 +- 0.165
mrr vals (pred, true): 0.514, 0.482
batch losses (mrrl, rdl): 0.0102815116, 4.3236e-06

Epoch over!
epoch time: 15.024

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 169
rank avg (pred): 0.452 +- 0.229
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002441729, 7.4493e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 956
rank avg (pred): 0.443 +- 0.220
mrr vals (pred, true): 0.044, 0.053
batch losses (mrrl, rdl): 0.0003484029, 1.04817e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1014
rank avg (pred): 0.397 +- 0.201
mrr vals (pred, true): 0.056, 0.056
batch losses (mrrl, rdl): 0.0003524158, 6.84433e-05

Epoch over!
epoch time: 15.037

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1126
rank avg (pred): 0.436 +- 0.221
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 2.77123e-05, 2.15592e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 577
rank avg (pred): 0.458 +- 0.240
mrr vals (pred, true): 0.044, 0.056
batch losses (mrrl, rdl): 0.0004141555, 9.1093e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 180
rank avg (pred): 0.425 +- 0.198
mrr vals (pred, true): 0.040, 0.066
batch losses (mrrl, rdl): 0.0009234654, 3.8249e-05

Epoch over!
epoch time: 15.01

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 721
rank avg (pred): 0.425 +- 0.208
mrr vals (pred, true): 0.045, 0.055
batch losses (mrrl, rdl): 0.0002269142, 5.61051e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 62
rank avg (pred): 0.188 +- 0.177
mrr vals (pred, true): 0.315, 0.333
batch losses (mrrl, rdl): 0.0033450169, 0.0002112043

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 201
rank avg (pred): 0.449 +- 0.233
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 4.36145e-05, 7.3493e-06

Epoch over!
epoch time: 14.98

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 489
rank avg (pred): 0.305 +- 0.207
mrr vals (pred, true): 0.225, 0.234
batch losses (mrrl, rdl): 0.0007947124, 0.0007033868

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1008
rank avg (pred): 0.408 +- 0.191
mrr vals (pred, true): 0.048, 0.052
batch losses (mrrl, rdl): 3.05721e-05, 4.90117e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 755
rank avg (pred): 0.435 +- 0.335
mrr vals (pred, true): 0.252, 0.338
batch losses (mrrl, rdl): 0.0755527169, 0.0019863094

Epoch over!
epoch time: 14.895

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 24
rank avg (pred): 0.299 +- 0.213
mrr vals (pred, true): 0.284, 0.284
batch losses (mrrl, rdl): 5.051e-07, 0.0006703756

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 457
rank avg (pred): 0.424 +- 0.214
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 6.33619e-05, 4.31801e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 138
rank avg (pred): 0.467 +- 0.248
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 2.75437e-05, 5.3776e-06

Epoch over!
epoch time: 14.984

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.219 +- 0.183
mrr vals (pred, true): 0.307, 0.296

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   43 	     0 	 0.05367 	 0.04538 	 ~...
   66 	     1 	 0.05635 	 0.04542 	 ~...
    9 	     2 	 0.04770 	 0.04749 	 ~...
   68 	     3 	 0.05653 	 0.04753 	 ~...
   70 	     4 	 0.05697 	 0.04776 	 ~...
   14 	     5 	 0.04864 	 0.04836 	 ~...
   75 	     6 	 0.05809 	 0.04922 	 ~...
    8 	     7 	 0.04689 	 0.04931 	 ~...
   23 	     8 	 0.04946 	 0.04968 	 ~...
   38 	     9 	 0.05272 	 0.05001 	 ~...
    7 	    10 	 0.04668 	 0.05016 	 ~...
   32 	    11 	 0.05167 	 0.05051 	 ~...
   24 	    12 	 0.04998 	 0.05061 	 ~...
   48 	    13 	 0.05428 	 0.05083 	 ~...
   10 	    14 	 0.04798 	 0.05089 	 ~...
    3 	    15 	 0.04538 	 0.05102 	 ~...
   29 	    16 	 0.05133 	 0.05102 	 ~...
   26 	    17 	 0.05037 	 0.05105 	 ~...
   67 	    18 	 0.05641 	 0.05119 	 ~...
    1 	    19 	 0.04497 	 0.05123 	 ~...
   52 	    20 	 0.05443 	 0.05128 	 ~...
   72 	    21 	 0.05744 	 0.05154 	 ~...
   22 	    22 	 0.04943 	 0.05184 	 ~...
   76 	    23 	 0.06050 	 0.05188 	 ~...
   45 	    24 	 0.05401 	 0.05194 	 ~...
   51 	    25 	 0.05443 	 0.05210 	 ~...
   64 	    26 	 0.05625 	 0.05212 	 ~...
   36 	    27 	 0.05250 	 0.05225 	 ~...
   49 	    28 	 0.05441 	 0.05274 	 ~...
   58 	    29 	 0.05517 	 0.05282 	 ~...
   28 	    30 	 0.05110 	 0.05285 	 ~...
   16 	    31 	 0.04876 	 0.05289 	 ~...
   47 	    32 	 0.05426 	 0.05300 	 ~...
   63 	    33 	 0.05614 	 0.05303 	 ~...
   70 	    34 	 0.05697 	 0.05308 	 ~...
   27 	    35 	 0.05106 	 0.05313 	 ~...
   44 	    36 	 0.05383 	 0.05313 	 ~...
   57 	    37 	 0.05468 	 0.05351 	 ~...
   42 	    38 	 0.05355 	 0.05374 	 ~...
   60 	    39 	 0.05566 	 0.05397 	 ~...
    1 	    40 	 0.04497 	 0.05405 	 ~...
   46 	    41 	 0.05416 	 0.05418 	 ~...
   18 	    42 	 0.04879 	 0.05423 	 ~...
   31 	    43 	 0.05159 	 0.05425 	 ~...
   49 	    44 	 0.05441 	 0.05434 	 ~...
   20 	    45 	 0.04899 	 0.05435 	 ~...
   14 	    46 	 0.04864 	 0.05438 	 ~...
   25 	    47 	 0.05017 	 0.05440 	 ~...
   35 	    48 	 0.05222 	 0.05447 	 ~...
    6 	    49 	 0.04582 	 0.05455 	 ~...
   59 	    50 	 0.05532 	 0.05464 	 ~...
   11 	    51 	 0.04818 	 0.05472 	 ~...
   73 	    52 	 0.05774 	 0.05493 	 ~...
    4 	    53 	 0.04559 	 0.05493 	 ~...
   37 	    54 	 0.05257 	 0.05506 	 ~...
   41 	    55 	 0.05352 	 0.05520 	 ~...
   13 	    56 	 0.04838 	 0.05523 	 ~...
   56 	    57 	 0.05467 	 0.05526 	 ~...
   19 	    58 	 0.04881 	 0.05535 	 ~...
   34 	    59 	 0.05213 	 0.05540 	 ~...
   40 	    60 	 0.05323 	 0.05556 	 ~...
   12 	    61 	 0.04837 	 0.05565 	 ~...
   55 	    62 	 0.05463 	 0.05576 	 ~...
   65 	    63 	 0.05631 	 0.05591 	 ~...
   54 	    64 	 0.05456 	 0.05595 	 ~...
   16 	    65 	 0.04876 	 0.05626 	 ~...
   21 	    66 	 0.04923 	 0.05628 	 ~...
    5 	    67 	 0.04580 	 0.05696 	 ~...
   39 	    68 	 0.05287 	 0.05751 	 ~...
   62 	    69 	 0.05587 	 0.05751 	 ~...
    0 	    70 	 0.03808 	 0.05755 	 ~...
   60 	    71 	 0.05566 	 0.05770 	 ~...
   30 	    72 	 0.05143 	 0.05782 	 ~...
   69 	    73 	 0.05679 	 0.05868 	 ~...
   74 	    74 	 0.05797 	 0.05906 	 ~...
   53 	    75 	 0.05451 	 0.05941 	 ~...
   33 	    76 	 0.05212 	 0.06181 	 ~...
   81 	    77 	 0.22250 	 0.20444 	 ~...
   78 	    78 	 0.22115 	 0.21280 	 ~...
   82 	    79 	 0.22750 	 0.21861 	 ~...
   83 	    80 	 0.22970 	 0.22564 	 ~...
   78 	    81 	 0.22115 	 0.22622 	 ~...
   80 	    82 	 0.22210 	 0.22952 	 ~...
   77 	    83 	 0.20914 	 0.24005 	 m..s
   84 	    84 	 0.23221 	 0.24847 	 ~...
   85 	    85 	 0.24470 	 0.26126 	 ~...
   92 	    86 	 0.29922 	 0.27203 	 ~...
   89 	    87 	 0.29038 	 0.27259 	 ~...
   87 	    88 	 0.27956 	 0.27912 	 ~...
   99 	    89 	 0.30609 	 0.28071 	 ~...
   96 	    90 	 0.30519 	 0.28272 	 ~...
  101 	    91 	 0.30981 	 0.28520 	 ~...
  100 	    92 	 0.30739 	 0.29634 	 ~...
   94 	    93 	 0.30240 	 0.30211 	 ~...
  102 	    94 	 0.31006 	 0.30284 	 ~...
   93 	    95 	 0.30135 	 0.31094 	 ~...
   88 	    96 	 0.28538 	 0.31559 	 m..s
   86 	    97 	 0.26924 	 0.31687 	 m..s
   97 	    98 	 0.30535 	 0.32216 	 ~...
   91 	    99 	 0.29884 	 0.32389 	 ~...
   95 	   100 	 0.30367 	 0.32599 	 ~...
   90 	   101 	 0.29403 	 0.32640 	 m..s
   98 	   102 	 0.30561 	 0.34296 	 m..s
  105 	   103 	 0.31185 	 0.34389 	 m..s
  102 	   104 	 0.31006 	 0.34970 	 m..s
  106 	   105 	 0.31585 	 0.35087 	 m..s
  110 	   106 	 0.39589 	 0.35832 	 m..s
  104 	   107 	 0.31119 	 0.36022 	 m..s
  107 	   108 	 0.37505 	 0.37015 	 ~...
  109 	   109 	 0.39057 	 0.37130 	 ~...
  119 	   110 	 0.49090 	 0.39820 	 m..s
  115 	   111 	 0.47733 	 0.41409 	 m..s
  108 	   112 	 0.38586 	 0.42508 	 m..s
  118 	   113 	 0.48499 	 0.44244 	 m..s
  120 	   114 	 0.49827 	 0.44528 	 m..s
  111 	   115 	 0.45588 	 0.45935 	 ~...
  116 	   116 	 0.47811 	 0.49383 	 ~...
  112 	   117 	 0.45822 	 0.49577 	 m..s
  114 	   118 	 0.47491 	 0.50285 	 ~...
  116 	   119 	 0.47811 	 0.51145 	 m..s
  113 	   120 	 0.46304 	 0.62777 	 MISS
==========================================
r_mrr = 0.9867793917655945
r2_mrr = 0.9731909036636353
spearmanr_mrr@5 = 0.8554558157920837
spearmanr_mrr@10 = 0.8960503339767456
spearmanr_mrr@50 = 0.9801991581916809
spearmanr_mrr@100 = 0.9933916926383972
spearmanr_mrr@All = 0.9941105842590332
==========================================
test time: 0.453
Done Testing dataset Kinships
total time taken: 232.40876150131226
training time taken: 224.6668679714203
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9868)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9732)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.8555)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.8961)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9802)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9934)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9941)}}, 'test_loss': {'ComplEx': {'Kinships': 0.734471445915915}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 361029881349393
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [707, 613, 346, 664, 275, 657, 494, 813, 854, 1002, 552, 768, 885, 880, 434, 1062, 241, 1208, 121, 326, 772, 670, 962, 1182, 610, 624, 246, 1128, 353, 143, 738, 394, 558, 294, 176, 928, 1173, 888, 1109, 695, 411, 287, 537, 534, 719, 774, 1194, 1181, 256, 1000, 324, 487, 490, 953, 614, 510, 1185, 741, 12, 315, 43, 999, 119, 977, 312, 1151, 731, 1123, 302, 421, 548, 565, 580, 97, 730, 1057, 1207, 356, 110, 297, 536, 1069, 128, 775, 933, 420, 960, 301, 1096, 67, 833, 42, 277, 770, 157, 997, 828, 0, 892, 966, 656, 209, 156, 117, 369, 41, 239, 799, 1214, 1120, 9, 336, 561, 396, 1188, 1192, 599, 713, 520, 115, 460]
valid_ids (0): []
train_ids (1094): [89, 964, 437, 823, 107, 740, 556, 637, 788, 673, 846, 1099, 581, 518, 563, 442, 243, 1138, 877, 782, 1122, 1102, 20, 79, 1176, 427, 455, 908, 433, 108, 529, 562, 1124, 945, 866, 191, 881, 967, 88, 549, 444, 391, 845, 930, 8, 389, 824, 469, 559, 943, 347, 28, 650, 340, 293, 378, 635, 1061, 501, 1133, 475, 141, 778, 30, 1141, 408, 78, 517, 667, 284, 451, 450, 797, 1139, 644, 808, 162, 919, 690, 53, 173, 764, 926, 441, 271, 37, 607, 900, 158, 1156, 446, 697, 393, 986, 418, 39, 958, 1183, 1058, 848, 48, 811, 619, 55, 682, 899, 1082, 723, 1144, 186, 10, 230, 329, 934, 991, 995, 989, 202, 1121, 497, 478, 757, 283, 579, 684, 604, 820, 70, 605, 921, 413, 99, 814, 27, 1149, 780, 1046, 2, 118, 163, 319, 998, 608, 338, 449, 929, 876, 164, 412, 365, 803, 832, 270, 791, 793, 334, 868, 840, 879, 603, 102, 931, 678, 318, 1167, 722, 519, 133, 189, 134, 498, 373, 542, 474, 183, 153, 901, 776, 233, 816, 728, 858, 1084, 950, 992, 197, 33, 1041, 452, 533, 669, 90, 939, 1178, 1190, 1035, 14, 550, 258, 633, 508, 180, 454, 404, 1213, 909, 547, 165, 990, 1093, 1142, 827, 1119, 235, 526, 756, 961, 748, 458, 317, 303, 622, 1037, 61, 703, 335, 746, 292, 266, 225, 821, 431, 345, 954, 860, 267, 101, 686, 376, 1077, 993, 320, 676, 836, 1114, 211, 1177, 212, 568, 727, 1026, 658, 1199, 742, 1168, 996, 1078, 13, 250, 291, 123, 159, 1012, 11, 523, 1022, 514, 589, 1211, 92, 804, 190, 462, 1131, 1028, 111, 1174, 865, 511, 380, 609, 62, 630, 1201, 626, 916, 1075, 841, 896, 600, 257, 1005, 124, 308, 582, 951, 830, 717, 895, 787, 1202, 515, 300, 152, 709, 479, 1148, 863, 354, 109, 538, 1191, 273, 21, 231, 1184, 203, 72, 794, 743, 112, 305, 912, 675, 1029, 735, 440, 1008, 19, 617, 1147, 578, 228, 947, 1076, 710, 1157, 483, 274, 359, 665, 259, 443, 344, 834, 1083, 390, 1193, 486, 321, 1094, 503, 1170, 22, 588, 1016, 927, 1150, 779, 155, 182, 281, 445, 374, 980, 1080, 122, 316, 187, 56, 1063, 295, 894, 627, 146, 666, 923, 789, 463, 724, 1197, 532, 903, 1034, 785, 473, 1111, 1204, 435, 453, 629, 1169, 528, 504, 482, 96, 601, 60, 1198, 1098, 652, 516, 646, 557, 649, 1052, 306, 530, 83, 861, 1068, 426, 1060, 763, 288, 711, 137, 367, 1140, 1160, 1053, 554, 645, 965, 1115, 636, 1049, 181, 265, 974, 149, 310, 975, 366, 795, 248, 1166, 969, 535, 1087, 363, 736, 459, 642, 7, 105, 883, 843, 236, 1200, 429, 681, 737, 148, 428, 130, 219, 1206, 24, 17, 1010, 289, 871, 6, 715, 247, 1006, 151, 1023, 704, 1045, 113, 783, 331, 290, 1054, 539, 80, 1048, 705, 145, 1195, 655, 918, 298, 1205, 174, 1118, 936, 777, 963, 844, 734, 1047, 718, 185, 385, 1073, 269, 956, 643, 84, 870, 1004, 521, 647, 531, 74, 971, 223, 296, 261, 95, 1125, 4, 733, 819, 714, 1051, 77, 612, 253, 34, 278, 886, 69, 216, 911, 1001, 448, 688, 314, 712, 379, 387, 477, 807, 154, 973, 496, 595, 282, 752, 382, 867, 417, 1043, 598, 304, 749, 551, 628, 352, 169, 606, 217, 25, 683, 687, 52, 208, 1009, 75, 1130, 35, 251, 244, 196, 602, 1159, 957, 761, 560, 1107, 696, 940, 691, 555, 1163, 430, 576, 1055, 527, 689, 659, 1189, 1101, 583, 720, 1196, 1038, 160, 342, 567, 1032, 812, 855, 1209, 472, 392, 1153, 424, 869, 227, 835, 898, 221, 415, 822, 1097, 955, 199, 416, 674, 893, 500, 175, 859, 671, 1135, 574, 806, 200, 611, 406, 692, 76, 1027, 663, 1064, 397, 850, 461, 466, 839, 584, 23, 252, 1154, 120, 509, 935, 800, 773, 260, 596, 623, 809, 507, 632, 790, 862, 309, 910, 1020, 47, 932, 218, 238, 395, 481, 1108, 220, 214, 874, 132, 360, 201, 699, 470, 51, 802, 31, 349, 1039, 1155, 750, 1030, 661, 760, 480, 204, 1050, 229, 762, 126, 144, 313, 100, 409, 82, 668, 784, 988, 402, 1165, 586, 591, 268, 44, 371, 592, 543, 985, 73, 525, 907, 16, 195, 1013, 616, 81, 1106, 179, 457, 884, 328, 249, 36, 1100, 725, 272, 26, 1031, 577, 357, 590, 765, 685, 29, 491, 15, 341, 398, 362, 285, 45, 91, 593, 815, 887, 938, 255, 242, 1042, 597, 585, 370, 546, 873, 286, 837, 753, 38, 401, 438, 693, 553, 142, 838, 1074, 98, 66, 135, 825, 767, 1014, 1044, 541, 226, 698, 167, 63, 1180, 18, 1024, 330, 245, 1158, 136, 372, 410, 856, 116, 193, 1015, 93, 805, 213, 58, 1210, 68, 348, 172, 403, 240, 323, 651, 1017, 745, 299, 114, 170, 1127, 864, 388, 587, 327, 1136, 489, 983, 754, 1091, 1011, 944, 755, 361, 618, 184, 522, 234, 64, 801, 215, 276, 857, 981, 810, 355, 54, 85, 1164, 224, 50, 575, 471, 672, 680, 512, 467, 513, 891, 1070, 339, 842, 702, 524, 631, 59, 1066, 653, 1088, 968, 332, 206, 572, 648, 492, 726, 638, 786, 732, 634, 979, 5, 131, 982, 906, 280, 263, 1126, 882, 1137, 1212, 1081, 493, 177, 381, 758, 660, 1105, 976, 506, 1089, 615, 818, 1161, 166, 701, 1085, 890, 1145, 484, 364, 694, 739, 468, 350, 205, 545, 677, 566, 729, 1112, 621, 1175, 759, 407, 937, 502, 540, 207, 987, 654, 942, 386, 358, 897, 625, 140, 914, 423, 1056, 351, 679, 1092, 798, 399, 569, 949, 1067, 700, 913, 46, 639, 829, 708, 383, 1007, 1186, 125, 924, 237, 311, 464, 1203, 564, 769, 1079, 1110, 831, 781, 904, 662, 432, 878, 792, 279, 422, 570, 232, 849, 439, 465, 1019, 1146, 1, 1072, 852, 1021, 161, 377, 87, 127, 922, 1090, 817, 178, 325, 984, 796, 766, 71, 436, 1187, 495, 889, 400, 1025, 1103, 1040, 447, 139, 905, 1018, 505, 49, 959, 194, 1171, 333, 826, 1132, 948, 188, 192, 307, 456, 262, 641, 171, 952, 476, 1065, 594, 1129, 499, 57, 264, 368, 222, 322, 198, 1162, 104, 721, 1036, 337, 1003, 419, 853, 872, 384, 970, 343, 751, 978, 129, 425, 375, 847, 1104, 485, 972, 210, 488, 1179, 620, 925, 716, 902, 1059, 1143, 744, 405, 414, 1116, 103, 706, 40, 1152, 86, 544, 1134, 915, 1117, 994, 573, 1071, 941, 640, 747, 1113, 1086, 946, 168, 920, 571, 65, 917, 254, 94, 851, 1172, 147, 32, 771, 106, 1095, 138, 875, 1033, 150, 3]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8561331982638825
the save name prefix for this run is:  chkpt-ID_8561331982638825_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 802
rank avg (pred): 0.561 +- 0.006
mrr vals (pred, true): 0.017, 0.049
batch losses (mrrl, rdl): 0.0, 0.0002631394

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 307
rank avg (pred): 0.114 +- 0.099
mrr vals (pred, true): 0.160, 0.290
batch losses (mrrl, rdl): 0.0, 1.50225e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 14
rank avg (pred): 0.086 +- 0.102
mrr vals (pred, true): 0.205, 0.368
batch losses (mrrl, rdl): 0.0, 3.918e-07

Epoch over!
epoch time: 14.79

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 706
rank avg (pred): 0.467 +- 0.260
mrr vals (pred, true): 0.039, 0.048
batch losses (mrrl, rdl): 0.0, 3.28e-08

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 357
rank avg (pred): 0.460 +- 0.269
mrr vals (pred, true): 0.044, 0.057
batch losses (mrrl, rdl): 0.0, 4.259e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 145
rank avg (pred): 0.457 +- 0.266
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 0.0, 4.955e-07

Epoch over!
epoch time: 14.758

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 611
rank avg (pred): 0.441 +- 0.261
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 0.0, 7.2099e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 572
rank avg (pred): 0.464 +- 0.262
mrr vals (pred, true): 0.044, 0.053
batch losses (mrrl, rdl): 0.0, 7.555e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 454
rank avg (pred): 0.462 +- 0.267
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 0.0, 2.8238e-06

Epoch over!
epoch time: 14.738

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 389
rank avg (pred): 0.438 +- 0.276
mrr vals (pred, true): 0.064, 0.051
batch losses (mrrl, rdl): 0.0, 2.03595e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 765
rank avg (pred): 0.452 +- 0.273
mrr vals (pred, true): 0.057, 0.056
batch losses (mrrl, rdl): 0.0, 3.765e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 470
rank avg (pred): 0.457 +- 0.274
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0, 1.671e-07

Epoch over!
epoch time: 14.796

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 14
rank avg (pred): 0.103 +- 0.142
mrr vals (pred, true): 0.198, 0.368
batch losses (mrrl, rdl): 0.0, 4.7209e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 496
rank avg (pred): 0.154 +- 0.168
mrr vals (pred, true): 0.141, 0.219
batch losses (mrrl, rdl): 0.0, 5.096e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 567
rank avg (pred): 0.451 +- 0.267
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0, 2.6075e-06

Epoch over!
epoch time: 14.838

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1020
rank avg (pred): 0.453 +- 0.276
mrr vals (pred, true): 0.061, 0.051
batch losses (mrrl, rdl): 0.0011960554, 9.8559e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 681
rank avg (pred): 0.452 +- 0.211
mrr vals (pred, true): 0.039, 0.053
batch losses (mrrl, rdl): 0.0011962232, 1.46873e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 989
rank avg (pred): 0.082 +- 0.137
mrr vals (pred, true): 0.470, 0.486
batch losses (mrrl, rdl): 0.0026415789, 1.45085e-05

Epoch over!
epoch time: 15.184

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 850
rank avg (pred): 0.466 +- 0.205
mrr vals (pred, true): 0.038, 0.051
batch losses (mrrl, rdl): 0.0014604187, 1.39659e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 765
rank avg (pred): 0.445 +- 0.203
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 2.35364e-05, 1.4549e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 863
rank avg (pred): 0.444 +- 0.210
mrr vals (pred, true): 0.056, 0.052
batch losses (mrrl, rdl): 0.0003138239, 1.48222e-05

Epoch over!
epoch time: 15.08

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 341
rank avg (pred): 0.464 +- 0.207
mrr vals (pred, true): 0.039, 0.053
batch losses (mrrl, rdl): 0.0011903599, 1.63277e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 512
rank avg (pred): 0.247 +- 0.211
mrr vals (pred, true): 0.242, 0.236
batch losses (mrrl, rdl): 0.000471169, 0.000340791

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1018
rank avg (pred): 0.457 +- 0.181
mrr vals (pred, true): 0.037, 0.060
batch losses (mrrl, rdl): 0.0017657883, 2.58987e-05

Epoch over!
epoch time: 14.962

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1197
rank avg (pred): 0.447 +- 0.210
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 6.3897e-06, 1.19659e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 596
rank avg (pred): 0.465 +- 0.227
mrr vals (pred, true): 0.048, 0.060
batch losses (mrrl, rdl): 5.40822e-05, 1.0029e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 445
rank avg (pred): 0.487 +- 0.235
mrr vals (pred, true): 0.042, 0.055
batch losses (mrrl, rdl): 0.0006215374, 1.4469e-05

Epoch over!
epoch time: 14.946

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 75
rank avg (pred): 0.210 +- 0.207
mrr vals (pred, true): 0.308, 0.263
batch losses (mrrl, rdl): 0.0197025463, 0.0001862079

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 687
rank avg (pred): 0.449 +- 0.239
mrr vals (pred, true): 0.065, 0.055
batch losses (mrrl, rdl): 0.002274361, 5.5927e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 554
rank avg (pred): 0.272 +- 0.204
mrr vals (pred, true): 0.217, 0.223
batch losses (mrrl, rdl): 0.0004127614, 0.0003774023

Epoch over!
epoch time: 14.924

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 281
rank avg (pred): 0.218 +- 0.211
mrr vals (pred, true): 0.314, 0.310
batch losses (mrrl, rdl): 0.0001905528, 0.0003164495

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 957
rank avg (pred): 0.458 +- 0.212
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 8.92454e-05, 1.16892e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 652
rank avg (pred): 0.449 +- 0.216
mrr vals (pred, true): 0.052, 0.056
batch losses (mrrl, rdl): 3.87421e-05, 1.05364e-05

Epoch over!
epoch time: 14.934

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 18
rank avg (pred): 0.227 +- 0.212
mrr vals (pred, true): 0.306, 0.285
batch losses (mrrl, rdl): 0.0044622612, 0.0002479481

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 864
rank avg (pred): 0.456 +- 0.186
mrr vals (pred, true): 0.039, 0.055
batch losses (mrrl, rdl): 0.0011310219, 2.42477e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 116
rank avg (pred): 0.487 +- 0.243
mrr vals (pred, true): 0.051, 0.057
batch losses (mrrl, rdl): 1.04235e-05, 1.51269e-05

Epoch over!
epoch time: 14.93

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 505
rank avg (pred): 0.269 +- 0.192
mrr vals (pred, true): 0.221, 0.216
batch losses (mrrl, rdl): 0.0002707347, 0.0004053253

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 100
rank avg (pred): 0.459 +- 0.191
mrr vals (pred, true): 0.043, 0.058
batch losses (mrrl, rdl): 0.000557461, 2.01377e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 513
rank avg (pred): 0.303 +- 0.229
mrr vals (pred, true): 0.234, 0.219
batch losses (mrrl, rdl): 0.0022867343, 0.0006175385

Epoch over!
epoch time: 14.977

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 357
rank avg (pred): 0.457 +- 0.218
mrr vals (pred, true): 0.057, 0.057
batch losses (mrrl, rdl): 0.000455297, 9.6209e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 819
rank avg (pred): 0.212 +- 0.207
mrr vals (pred, true): 0.402, 0.442
batch losses (mrrl, rdl): 0.0165465549, 0.000520252

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1160
rank avg (pred): 0.236 +- 0.197
mrr vals (pred, true): 0.274, 0.286
batch losses (mrrl, rdl): 0.0013903731, 0.0003709828

Epoch over!
epoch time: 14.942

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 827
rank avg (pred): 0.215 +- 0.206
mrr vals (pred, true): 0.399, 0.362
batch losses (mrrl, rdl): 0.013359189, 0.0003157983

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 56
rank avg (pred): 0.241 +- 0.201
mrr vals (pred, true): 0.284, 0.340
batch losses (mrrl, rdl): 0.0311105177, 0.0004864574

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 141
rank avg (pred): 0.436 +- 0.168
mrr vals (pred, true): 0.045, 0.064
batch losses (mrrl, rdl): 0.0002601183, 2.89207e-05

Epoch over!
epoch time: 14.931

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.482 +- 0.218
mrr vals (pred, true): 0.048, 0.053

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   29 	     0 	 0.04838 	 0.04542 	 ~...
   29 	     1 	 0.04838 	 0.04668 	 ~...
   29 	     2 	 0.04838 	 0.04763 	 ~...
   29 	     3 	 0.04838 	 0.04780 	 ~...
   29 	     4 	 0.04838 	 0.04783 	 ~...
   29 	     5 	 0.04838 	 0.04866 	 ~...
   15 	     6 	 0.04700 	 0.04944 	 ~...
   29 	     7 	 0.04838 	 0.04948 	 ~...
   65 	     8 	 0.04860 	 0.04973 	 ~...
   29 	     9 	 0.04838 	 0.04985 	 ~...
   29 	    10 	 0.04838 	 0.04998 	 ~...
   29 	    11 	 0.04838 	 0.05043 	 ~...
    3 	    12 	 0.04365 	 0.05068 	 ~...
   26 	    13 	 0.04819 	 0.05075 	 ~...
   29 	    14 	 0.04838 	 0.05076 	 ~...
   71 	    15 	 0.05086 	 0.05085 	 ~...
   29 	    16 	 0.04838 	 0.05096 	 ~...
   29 	    17 	 0.04838 	 0.05108 	 ~...
   11 	    18 	 0.04635 	 0.05121 	 ~...
   29 	    19 	 0.04838 	 0.05126 	 ~...
   29 	    20 	 0.04838 	 0.05138 	 ~...
   75 	    21 	 0.05957 	 0.05157 	 ~...
   29 	    22 	 0.04838 	 0.05161 	 ~...
   29 	    23 	 0.04838 	 0.05168 	 ~...
   29 	    24 	 0.04838 	 0.05169 	 ~...
   12 	    25 	 0.04656 	 0.05188 	 ~...
   73 	    26 	 0.05179 	 0.05191 	 ~...
    4 	    27 	 0.04515 	 0.05193 	 ~...
   68 	    28 	 0.04992 	 0.05203 	 ~...
   29 	    29 	 0.04838 	 0.05212 	 ~...
   29 	    30 	 0.04838 	 0.05219 	 ~...
   69 	    31 	 0.05053 	 0.05221 	 ~...
    6 	    32 	 0.04550 	 0.05227 	 ~...
   24 	    33 	 0.04788 	 0.05258 	 ~...
   14 	    34 	 0.04692 	 0.05280 	 ~...
   66 	    35 	 0.04947 	 0.05298 	 ~...
   70 	    36 	 0.05081 	 0.05307 	 ~...
   64 	    37 	 0.04853 	 0.05308 	 ~...
   29 	    38 	 0.04838 	 0.05314 	 ~...
   29 	    39 	 0.04838 	 0.05328 	 ~...
   28 	    40 	 0.04837 	 0.05331 	 ~...
   63 	    41 	 0.04839 	 0.05349 	 ~...
   29 	    42 	 0.04838 	 0.05350 	 ~...
   29 	    43 	 0.04838 	 0.05373 	 ~...
   29 	    44 	 0.04838 	 0.05376 	 ~...
   15 	    45 	 0.04700 	 0.05381 	 ~...
   15 	    46 	 0.04700 	 0.05385 	 ~...
   29 	    47 	 0.04838 	 0.05403 	 ~...
   72 	    48 	 0.05089 	 0.05412 	 ~...
   23 	    49 	 0.04741 	 0.05427 	 ~...
    2 	    50 	 0.04342 	 0.05440 	 ~...
   29 	    51 	 0.04838 	 0.05455 	 ~...
   19 	    52 	 0.04718 	 0.05469 	 ~...
   29 	    53 	 0.04838 	 0.05493 	 ~...
    8 	    54 	 0.04558 	 0.05501 	 ~...
   22 	    55 	 0.04733 	 0.05510 	 ~...
    8 	    56 	 0.04558 	 0.05540 	 ~...
    0 	    57 	 0.04313 	 0.05555 	 ~...
   26 	    58 	 0.04819 	 0.05585 	 ~...
   29 	    59 	 0.04838 	 0.05587 	 ~...
   67 	    60 	 0.04987 	 0.05596 	 ~...
   10 	    61 	 0.04593 	 0.05609 	 ~...
   13 	    62 	 0.04667 	 0.05615 	 ~...
   73 	    63 	 0.05179 	 0.05632 	 ~...
   29 	    64 	 0.04838 	 0.05638 	 ~...
    5 	    65 	 0.04547 	 0.05647 	 ~...
    7 	    66 	 0.04555 	 0.05656 	 ~...
   29 	    67 	 0.04838 	 0.05685 	 ~...
   29 	    68 	 0.04838 	 0.05723 	 ~...
   21 	    69 	 0.04723 	 0.05731 	 ~...
    0 	    70 	 0.04313 	 0.05735 	 ~...
   18 	    71 	 0.04713 	 0.05762 	 ~...
   29 	    72 	 0.04838 	 0.05872 	 ~...
   19 	    73 	 0.04718 	 0.06072 	 ~...
   29 	    74 	 0.04838 	 0.06253 	 ~...
   25 	    75 	 0.04793 	 0.06360 	 ~...
   76 	    76 	 0.19320 	 0.18404 	 ~...
   77 	    77 	 0.19674 	 0.18556 	 ~...
   80 	    78 	 0.19786 	 0.18561 	 ~...
   81 	    79 	 0.20113 	 0.19332 	 ~...
   77 	    80 	 0.19674 	 0.19518 	 ~...
   79 	    81 	 0.19693 	 0.19886 	 ~...
   83 	    82 	 0.20985 	 0.20455 	 ~...
   85 	    83 	 0.21619 	 0.20514 	 ~...
   87 	    84 	 0.21934 	 0.22952 	 ~...
   82 	    85 	 0.20129 	 0.23023 	 ~...
   84 	    86 	 0.21344 	 0.23947 	 ~...
   86 	    87 	 0.21846 	 0.24673 	 ~...
   90 	    88 	 0.27666 	 0.26647 	 ~...
   94 	    89 	 0.28600 	 0.26840 	 ~...
   88 	    90 	 0.22786 	 0.27033 	 m..s
   93 	    91 	 0.28556 	 0.27203 	 ~...
   96 	    92 	 0.29211 	 0.27694 	 ~...
  105 	    93 	 0.32819 	 0.28272 	 m..s
  104 	    94 	 0.32646 	 0.28370 	 m..s
   89 	    95 	 0.26411 	 0.29652 	 m..s
   95 	    96 	 0.28931 	 0.29778 	 ~...
  101 	    97 	 0.31135 	 0.30084 	 ~...
   96 	    98 	 0.29211 	 0.30230 	 ~...
  107 	    99 	 0.33651 	 0.30598 	 m..s
  100 	   100 	 0.30664 	 0.30765 	 ~...
   92 	   101 	 0.28485 	 0.30931 	 ~...
   98 	   102 	 0.29433 	 0.31389 	 ~...
  105 	   103 	 0.32819 	 0.31553 	 ~...
   99 	   104 	 0.29886 	 0.32599 	 ~...
  102 	   105 	 0.31600 	 0.33738 	 ~...
  108 	   106 	 0.34538 	 0.34735 	 ~...
  103 	   107 	 0.31646 	 0.34916 	 m..s
  117 	   108 	 0.44554 	 0.35832 	 m..s
  109 	   109 	 0.39780 	 0.39756 	 ~...
  109 	   110 	 0.39780 	 0.40618 	 ~...
  112 	   111 	 0.40896 	 0.41066 	 ~...
  111 	   112 	 0.40212 	 0.42462 	 ~...
   91 	   113 	 0.27868 	 0.44883 	 MISS
  119 	   114 	 0.46238 	 0.47935 	 ~...
  120 	   115 	 0.47005 	 0.47946 	 ~...
  116 	   116 	 0.44439 	 0.48073 	 m..s
  114 	   117 	 0.43577 	 0.51634 	 m..s
  115 	   118 	 0.43879 	 0.61055 	 MISS
  113 	   119 	 0.43492 	 0.61786 	 MISS
  118 	   120 	 0.44788 	 0.61829 	 MISS
==========================================
r_mrr = 0.975684404373169
r2_mrr = 0.943037748336792
spearmanr_mrr@5 = 0.7445509433746338
spearmanr_mrr@10 = 0.8816675543785095
spearmanr_mrr@50 = 0.9702533483505249
spearmanr_mrr@100 = 0.987609326839447
spearmanr_mrr@All = 0.9888837337493896
==========================================
test time: 0.455
Done Testing dataset Kinships
total time taken: 231.78310346603394
training time taken: 224.1957540512085
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9757)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9430)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.7446)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.8817)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9703)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9876)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9889)}}, 'test_loss': {'ComplEx': {'Kinships': 1.5538785639510024}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 4375677851112455
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [766, 693, 118, 763, 911, 256, 964, 471, 1144, 803, 167, 137, 1209, 470, 609, 392, 613, 337, 1192, 238, 1107, 413, 204, 14, 1012, 788, 926, 1203, 274, 243, 192, 543, 946, 93, 621, 320, 864, 229, 833, 450, 524, 90, 948, 1199, 25, 1128, 91, 82, 252, 399, 1156, 357, 684, 67, 520, 319, 504, 844, 205, 460, 43, 1173, 949, 15, 677, 1093, 87, 745, 1179, 944, 170, 221, 950, 690, 233, 922, 441, 1070, 861, 1024, 440, 588, 374, 388, 415, 899, 641, 919, 521, 531, 110, 626, 900, 152, 642, 1044, 1150, 1050, 532, 558, 29, 1076, 659, 390, 97, 294, 56, 369, 828, 222, 8, 863, 1029, 234, 1153, 1064, 420, 115, 161, 476, 187]
valid_ids (0): []
train_ids (1094): [489, 116, 757, 462, 630, 910, 160, 789, 815, 13, 829, 315, 723, 683, 493, 191, 977, 1102, 992, 33, 104, 638, 831, 625, 1043, 172, 548, 434, 920, 1084, 656, 624, 77, 184, 74, 954, 961, 1213, 96, 571, 179, 506, 726, 879, 345, 32, 376, 658, 366, 296, 819, 542, 249, 1014, 875, 494, 153, 704, 851, 469, 27, 248, 1101, 417, 1211, 368, 26, 426, 1187, 837, 63, 1138, 956, 564, 724, 279, 147, 986, 303, 173, 485, 232, 46, 528, 208, 589, 181, 34, 874, 502, 927, 406, 79, 696, 50, 397, 869, 212, 935, 411, 144, 557, 1158, 0, 583, 446, 496, 1113, 495, 422, 44, 353, 1178, 908, 1034, 577, 231, 714, 99, 546, 673, 442, 1124, 391, 481, 563, 759, 663, 322, 982, 674, 1066, 362, 482, 53, 1004, 611, 628, 1016, 281, 377, 686, 227, 605, 891, 796, 1106, 744, 890, 808, 746, 848, 1055, 1205, 1017, 769, 710, 240, 403, 459, 916, 1176, 511, 360, 801, 1167, 1127, 750, 645, 1114, 551, 523, 675, 235, 777, 62, 270, 966, 479, 258, 639, 340, 824, 1149, 1202, 794, 351, 226, 718, 717, 1069, 720, 338, 687, 1189, 994, 145, 318, 702, 1090, 293, 995, 291, 58, 335, 1036, 45, 664, 66, 509, 568, 594, 703, 164, 384, 804, 467, 188, 355, 484, 739, 151, 847, 1117, 530, 514, 540, 1032, 218, 1181, 572, 533, 267, 885, 845, 122, 522, 197, 271, 1091, 253, 40, 342, 990, 195, 70, 387, 933, 76, 826, 289, 1033, 807, 1077, 1078, 903, 92, 461, 963, 284, 883, 1135, 297, 955, 1056, 945, 786, 823, 725, 858, 358, 535, 402, 134, 456, 250, 644, 1162, 449, 872, 1061, 326, 298, 17, 778, 57, 756, 21, 1018, 758, 580, 1147, 119, 660, 359, 854, 1040, 740, 55, 816, 817, 574, 1104, 860, 443, 925, 380, 1155, 216, 1054, 830, 220, 373, 561, 1109, 131, 1201, 1180, 203, 246, 810, 813, 1120, 892, 190, 427, 913, 1129, 12, 356, 841, 873, 593, 38, 219, 1152, 775, 1208, 202, 343, 884, 898, 47, 616, 587, 214, 404, 738, 1049, 959, 431, 19, 239, 755, 1000, 194, 1099, 301, 870, 310, 51, 676, 478, 283, 378, 230, 20, 1086, 1140, 1125, 410, 261, 632, 562, 1068, 1072, 748, 727, 1087, 1142, 712, 16, 513, 897, 1134, 1067, 608, 61, 312, 454, 1133, 553, 189, 398, 780, 527, 407, 773, 277, 555, 1071, 163, 1214, 401, 695, 706, 975, 694, 464, 1042, 365, 1110, 701, 762, 272, 567, 128, 818, 886, 1059, 856, 602, 811, 41, 610, 855, 107, 790, 490, 412, 1053, 405, 1019, 425, 1011, 947, 1141, 1168, 306, 6, 73, 688, 1123, 1, 336, 1210, 843, 1145, 953, 893, 499, 1166, 505, 581, 154, 123, 765, 809, 988, 680, 483, 1185, 622, 103, 1163, 466, 767, 734, 924, 868, 985, 732, 211, 1108, 733, 722, 1175, 165, 1160, 1206, 619, 106, 223, 741, 938, 849, 965, 242, 797, 941, 292, 199, 772, 132, 1157, 10, 127, 1001, 3, 595, 64, 1008, 1174, 550, 286, 636, 997, 1028, 1082, 547, 236, 314, 751, 842, 503, 175, 308, 182, 168, 633, 501, 1080, 263, 1195, 447, 853, 171, 709, 876, 95, 678, 1085, 381, 882, 1021, 386, 1146, 350, 515, 1052, 278, 918, 24, 259, 1047, 54, 200, 812, 105, 715, 670, 692, 668, 598, 582, 288, 993, 614, 244, 371, 299, 795, 254, 917, 112, 429, 266, 325, 183, 480, 48, 661, 928, 984, 685, 970, 423, 1184, 643, 549, 921, 649, 554, 782, 363, 1046, 1062, 737, 241, 474, 1188, 419, 237, 518, 1172, 1030, 108, 80, 42, 539, 75, 156, 1015, 1005, 68, 367, 541, 498, 689, 681, 721, 951, 418, 881, 409, 859, 455, 1137, 573, 9, 749, 623, 979, 383, 1190, 565, 344, 618, 408, 37, 719, 120, 827, 331, 1165, 300, 705, 143, 81, 617, 937, 389, 545, 177, 1177, 525, 421, 822, 586, 436, 728, 468, 743, 1026, 1207, 1079, 771, 711, 334, 1009, 761, 650, 973, 430, 193, 850, 129, 433, 1139, 840, 109, 754, 1048, 287, 372, 862, 89, 747, 393, 671, 1027, 396, 457, 1023, 285, 1025, 987, 814, 328, 902, 983, 329, 30, 176, 1170, 1031, 836, 1081, 534, 1045, 1111, 735, 126, 615, 834, 1075, 996, 731, 787, 276, 140, 437, 1089, 121, 569, 206, 1197, 472, 1094, 634, 185, 85, 273, 475, 354, 764, 1041, 867, 395, 969, 888, 364, 575, 217, 280, 665, 84, 71, 1095, 11, 631, 566, 215, 497, 1010, 784, 257, 774, 930, 7, 321, 957, 444, 800, 166, 915, 894, 22, 646, 652, 559, 976, 117, 225, 904, 1060, 648, 114, 83, 486, 439, 640, 552, 1191, 835, 31, 150, 463, 180, 1171, 607, 453, 962, 1154, 667, 700, 228, 1151, 72, 793, 591, 932, 972, 906, 912, 544, 346, 1115, 538, 247, 414, 1051, 1148, 1013, 201, 999, 260, 556, 139, 779, 776, 465, 596, 1161, 302, 579, 989, 560, 60, 488, 852, 791, 599, 698, 901, 1169, 317, 923, 783, 1037, 458, 1200, 473, 432, 36, 451, 162, 305, 785, 135, 939, 974, 752, 500, 174, 931, 313, 304, 604, 59, 1002, 895, 347, 39, 4, 207, 736, 213, 662, 603, 1121, 1204, 1182, 1196, 512, 600, 936, 635, 657, 1022, 86, 1136, 1007, 295, 590, 576, 877, 1100, 5, 282, 1198, 730, 379, 536, 978, 169, 487, 210, 352, 1088, 49, 341, 492, 517, 1035, 424, 1112, 529, 111, 307, 316, 245, 196, 713, 88, 510, 805, 375, 654, 349, 339, 35, 52, 1131, 914, 729, 311, 265, 1186, 753, 78, 142, 760, 209, 871, 971, 124, 940, 1116, 255, 846, 130, 102, 672, 98, 627, 133, 157, 100, 1126, 125, 839, 998, 620, 155, 251, 880, 23, 323, 327, 943, 991, 825, 348, 1130, 1098, 798, 178, 1183, 275, 262, 647, 802, 1003, 428, 1083, 1057, 448, 697, 1063, 101, 332, 186, 578, 768, 361, 968, 149, 682, 1159, 400, 1038, 585, 929, 597, 94, 669, 1122, 1006, 1074, 435, 1058, 570, 1096, 896, 1118, 1193, 980, 1039, 18, 1073, 1020, 1105, 832, 507, 148, 1065, 141, 519, 28, 651, 438, 820, 821, 606, 909, 477, 333, 416, 445, 792, 136, 981, 65, 866, 1164, 907, 770, 1194, 592, 264, 865, 290, 691, 526, 370, 806, 146, 699, 781, 666, 269, 878, 508, 934, 708, 838, 516, 1103, 612, 491, 679, 452, 742, 158, 537, 159, 138, 113, 889, 653, 637, 707, 1092, 584, 655, 601, 960, 324, 2, 952, 716, 198, 958, 905, 1097, 394, 385, 1119, 309, 1132, 857, 967, 1212, 799, 224, 629, 1143, 69, 382, 268, 887, 330, 942]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3655748739255678
the save name prefix for this run is:  chkpt-ID_3655748739255678_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 13
rank avg (pred): 0.405 +- 0.008
mrr vals (pred, true): 0.023, 0.309
batch losses (mrrl, rdl): 0.0, 0.0017279261

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 416
rank avg (pred): 0.463 +- 0.014
mrr vals (pred, true): 0.021, 0.057
batch losses (mrrl, rdl): 0.0, 8.56603e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1151
rank avg (pred): 0.140 +- 0.160
mrr vals (pred, true): 0.177, 0.297
batch losses (mrrl, rdl): 0.0, 2.44296e-05

Epoch over!
epoch time: 14.852

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 596
rank avg (pred): 0.466 +- 0.263
mrr vals (pred, true): 0.039, 0.060
batch losses (mrrl, rdl): 0.0, 9.562e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 750
rank avg (pred): 0.109 +- 0.143
mrr vals (pred, true): 0.218, 0.411
batch losses (mrrl, rdl): 0.0, 2.70531e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 182
rank avg (pred): 0.465 +- 0.262
mrr vals (pred, true): 0.046, 0.056
batch losses (mrrl, rdl): 0.0, 5.5144e-06

Epoch over!
epoch time: 14.754

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 372
rank avg (pred): 0.481 +- 0.265
mrr vals (pred, true): 0.042, 0.062
batch losses (mrrl, rdl): 0.0, 1.63871e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 449
rank avg (pred): 0.453 +- 0.273
mrr vals (pred, true): 0.059, 0.051
batch losses (mrrl, rdl): 0.0, 9.139e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 331
rank avg (pred): 0.479 +- 0.269
mrr vals (pred, true): 0.045, 0.055
batch losses (mrrl, rdl): 0.0, 1.4104e-06

Epoch over!
epoch time: 14.917

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1136
rank avg (pred): 0.086 +- 0.118
mrr vals (pred, true): 0.249, 0.267
batch losses (mrrl, rdl): 0.0, 2.07244e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 410
rank avg (pred): 0.476 +- 0.274
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 0.0, 3.539e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 272
rank avg (pred): 0.103 +- 0.129
mrr vals (pred, true): 0.224, 0.332
batch losses (mrrl, rdl): 0.0, 6.87e-08

Epoch over!
epoch time: 14.902

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 826
rank avg (pred): 0.131 +- 0.159
mrr vals (pred, true): 0.190, 0.362
batch losses (mrrl, rdl): 0.0, 8.1406e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 561
rank avg (pred): 0.205 +- 0.205
mrr vals (pred, true): 0.133, 0.195
batch losses (mrrl, rdl): 0.0, 2.62313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 302
rank avg (pred): 0.109 +- 0.136
mrr vals (pred, true): 0.213, 0.337
batch losses (mrrl, rdl): 0.0, 6.0565e-06

Epoch over!
epoch time: 14.903

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 395
rank avg (pred): 0.464 +- 0.276
mrr vals (pred, true): 0.056, 0.054
batch losses (mrrl, rdl): 0.0003711978, 5.911e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 547
rank avg (pred): 0.129 +- 0.087
mrr vals (pred, true): 0.246, 0.227
batch losses (mrrl, rdl): 0.0037438588, 5.7495e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 708
rank avg (pred): 0.423 +- 0.126
mrr vals (pred, true): 0.047, 0.059
batch losses (mrrl, rdl): 0.0001067252, 8.59889e-05

Epoch over!
epoch time: 15.16

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1091
rank avg (pred): 0.398 +- 0.135
mrr vals (pred, true): 0.062, 0.058
batch losses (mrrl, rdl): 0.0015155146, 0.0001109307

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 406
rank avg (pred): 0.388 +- 0.129
mrr vals (pred, true): 0.066, 0.052
batch losses (mrrl, rdl): 0.0026935621, 0.0001853066

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 345
rank avg (pred): 0.476 +- 0.151
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 7.51732e-05, 3.29976e-05

Epoch over!
epoch time: 15.147

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1148
rank avg (pred): 0.098 +- 0.064
mrr vals (pred, true): 0.276, 0.284
batch losses (mrrl, rdl): 0.0006601816, 2.0756e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 643
rank avg (pred): 0.480 +- 0.163
mrr vals (pred, true): 0.054, 0.049
batch losses (mrrl, rdl): 0.0001808947, 4.12405e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 203
rank avg (pred): 0.474 +- 0.151
mrr vals (pred, true): 0.052, 0.051
batch losses (mrrl, rdl): 2.63119e-05, 4.59808e-05

Epoch over!
epoch time: 15.125

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 257
rank avg (pred): 0.035 +- 0.024
mrr vals (pred, true): 0.399, 0.369
batch losses (mrrl, rdl): 0.0088342763, 8.13697e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 795
rank avg (pred): 0.462 +- 0.141
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 3.38e-06, 4.34351e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1175
rank avg (pred): 0.474 +- 0.132
mrr vals (pred, true): 0.044, 0.055
batch losses (mrrl, rdl): 0.0003384525, 4.97346e-05

Epoch over!
epoch time: 15.123

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 380
rank avg (pred): 0.439 +- 0.118
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 0.0001049328, 6.43243e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 603
rank avg (pred): 0.462 +- 0.120
mrr vals (pred, true): 0.043, 0.059
batch losses (mrrl, rdl): 0.000491782, 5.38044e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 473
rank avg (pred): 0.481 +- 0.136
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 6.53165e-05, 4.68974e-05

Epoch over!
epoch time: 15.136

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 725
rank avg (pred): 0.494 +- 0.147
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 1.19858e-05, 6.02218e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 428
rank avg (pred): 0.500 +- 0.144
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001763559, 5.92781e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 92
rank avg (pred): 0.469 +- 0.134
mrr vals (pred, true): 0.050, 0.057
batch losses (mrrl, rdl): 5.757e-07, 4.79837e-05

Epoch over!
epoch time: 15.147

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 660
rank avg (pred): 0.468 +- 0.135
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 1.96128e-05, 5.38441e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 796
rank avg (pred): 0.539 +- 0.165
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001968478, 0.000151589

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 103
rank avg (pred): 0.469 +- 0.123
mrr vals (pred, true): 0.047, 0.054
batch losses (mrrl, rdl): 9.62551e-05, 5.05793e-05

Epoch over!
epoch time: 15.119

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 567
rank avg (pred): 0.477 +- 0.128
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 6.54798e-05, 5.18476e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 193
rank avg (pred): 0.468 +- 0.113
mrr vals (pred, true): 0.041, 0.056
batch losses (mrrl, rdl): 0.0008749549, 6.71757e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 313
rank avg (pred): 0.104 +- 0.067
mrr vals (pred, true): 0.279, 0.308
batch losses (mrrl, rdl): 0.0083823055, 2.35757e-05

Epoch over!
epoch time: 15.156

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 438
rank avg (pred): 0.452 +- 0.129
mrr vals (pred, true): 0.056, 0.058
batch losses (mrrl, rdl): 0.0003074725, 6.034e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1123
rank avg (pred): 0.425 +- 0.111
mrr vals (pred, true): 0.055, 0.061
batch losses (mrrl, rdl): 0.0002081271, 7.62192e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1078
rank avg (pred): 0.014 +- 0.009
mrr vals (pred, true): 0.509, 0.480
batch losses (mrrl, rdl): 0.0084809577, 6.82391e-05

Epoch over!
epoch time: 15.158

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 63
rank avg (pred): 0.106 +- 0.068
mrr vals (pred, true): 0.274, 0.291
batch losses (mrrl, rdl): 0.0028560506, 2.38397e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 50
rank avg (pred): 0.104 +- 0.070
mrr vals (pred, true): 0.305, 0.296
batch losses (mrrl, rdl): 0.000814201, 8.1338e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 83
rank avg (pred): 0.434 +- 0.124
mrr vals (pred, true): 0.058, 0.057
batch losses (mrrl, rdl): 0.0006115693, 6.61214e-05

Epoch over!
epoch time: 15.132

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.468 +- 0.135
mrr vals (pred, true): 0.052, 0.050

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   44 	     0 	 0.04933 	 0.04376 	 ~...
   74 	     1 	 0.05183 	 0.04658 	 ~...
   12 	     2 	 0.04781 	 0.04726 	 ~...
   52 	     3 	 0.04993 	 0.04727 	 ~...
   41 	     4 	 0.04911 	 0.04747 	 ~...
   45 	     5 	 0.04937 	 0.04763 	 ~...
   21 	     6 	 0.04851 	 0.04815 	 ~...
   34 	     7 	 0.04888 	 0.04836 	 ~...
    9 	     8 	 0.04770 	 0.04840 	 ~...
   76 	     9 	 0.05193 	 0.04897 	 ~...
   53 	    10 	 0.05000 	 0.04925 	 ~...
   54 	    11 	 0.05003 	 0.04948 	 ~...
   66 	    12 	 0.05077 	 0.04954 	 ~...
   82 	    13 	 0.05323 	 0.04971 	 ~...
   78 	    14 	 0.05195 	 0.04973 	 ~...
   79 	    15 	 0.05220 	 0.04979 	 ~...
   39 	    16 	 0.04902 	 0.05016 	 ~...
   57 	    17 	 0.05025 	 0.05018 	 ~...
   22 	    18 	 0.04852 	 0.05023 	 ~...
   80 	    19 	 0.05231 	 0.05035 	 ~...
   36 	    20 	 0.04892 	 0.05038 	 ~...
   24 	    21 	 0.04856 	 0.05064 	 ~...
   75 	    22 	 0.05192 	 0.05077 	 ~...
   84 	    23 	 0.05384 	 0.05086 	 ~...
   26 	    24 	 0.04862 	 0.05102 	 ~...
   37 	    25 	 0.04893 	 0.05103 	 ~...
   24 	    26 	 0.04856 	 0.05140 	 ~...
   49 	    27 	 0.04974 	 0.05146 	 ~...
   40 	    28 	 0.04908 	 0.05154 	 ~...
   17 	    29 	 0.04807 	 0.05168 	 ~...
   50 	    30 	 0.04975 	 0.05185 	 ~...
   32 	    31 	 0.04882 	 0.05193 	 ~...
    8 	    32 	 0.04757 	 0.05201 	 ~...
   61 	    33 	 0.05048 	 0.05207 	 ~...
   30 	    34 	 0.04879 	 0.05235 	 ~...
   69 	    35 	 0.05084 	 0.05262 	 ~...
   70 	    36 	 0.05106 	 0.05266 	 ~...
    4 	    37 	 0.04686 	 0.05275 	 ~...
   31 	    38 	 0.04881 	 0.05287 	 ~...
   58 	    39 	 0.05033 	 0.05289 	 ~...
   33 	    40 	 0.04886 	 0.05291 	 ~...
   27 	    41 	 0.04863 	 0.05303 	 ~...
   56 	    42 	 0.05020 	 0.05312 	 ~...
   62 	    43 	 0.05051 	 0.05316 	 ~...
    2 	    44 	 0.04647 	 0.05324 	 ~...
   46 	    45 	 0.04941 	 0.05328 	 ~...
   29 	    46 	 0.04874 	 0.05331 	 ~...
   14 	    47 	 0.04791 	 0.05339 	 ~...
   48 	    48 	 0.04969 	 0.05349 	 ~...
    7 	    49 	 0.04755 	 0.05361 	 ~...
   60 	    50 	 0.05042 	 0.05385 	 ~...
   18 	    51 	 0.04824 	 0.05418 	 ~...
   59 	    52 	 0.05042 	 0.05422 	 ~...
   62 	    53 	 0.05051 	 0.05422 	 ~...
    1 	    54 	 0.04643 	 0.05434 	 ~...
    5 	    55 	 0.04697 	 0.05440 	 ~...
   83 	    56 	 0.05342 	 0.05462 	 ~...
    0 	    57 	 0.04617 	 0.05472 	 ~...
   73 	    58 	 0.05182 	 0.05476 	 ~...
   77 	    59 	 0.05194 	 0.05492 	 ~...
   55 	    60 	 0.05007 	 0.05498 	 ~...
    3 	    61 	 0.04661 	 0.05501 	 ~...
   64 	    62 	 0.05054 	 0.05545 	 ~...
   43 	    63 	 0.04926 	 0.05569 	 ~...
   13 	    64 	 0.04790 	 0.05595 	 ~...
   23 	    65 	 0.04855 	 0.05614 	 ~...
   65 	    66 	 0.05073 	 0.05680 	 ~...
   10 	    67 	 0.04777 	 0.05682 	 ~...
   10 	    68 	 0.04777 	 0.05691 	 ~...
   20 	    69 	 0.04840 	 0.05699 	 ~...
   67 	    70 	 0.05080 	 0.05712 	 ~...
   16 	    71 	 0.04806 	 0.05714 	 ~...
    6 	    72 	 0.04750 	 0.05743 	 ~...
   42 	    73 	 0.04914 	 0.05749 	 ~...
   51 	    74 	 0.04983 	 0.05776 	 ~...
   81 	    75 	 0.05292 	 0.05781 	 ~...
   19 	    76 	 0.04836 	 0.05870 	 ~...
   15 	    77 	 0.04791 	 0.05873 	 ~...
   38 	    78 	 0.04901 	 0.05874 	 ~...
   72 	    79 	 0.05113 	 0.05881 	 ~...
   47 	    80 	 0.04942 	 0.05896 	 ~...
   71 	    81 	 0.05109 	 0.05944 	 ~...
   35 	    82 	 0.04888 	 0.06240 	 ~...
   68 	    83 	 0.05081 	 0.06265 	 ~...
   28 	    84 	 0.04874 	 0.06360 	 ~...
   93 	    85 	 0.25240 	 0.08417 	 MISS
   92 	    86 	 0.24614 	 0.18561 	 m..s
   89 	    87 	 0.24050 	 0.19687 	 m..s
   90 	    88 	 0.24594 	 0.19962 	 m..s
   90 	    89 	 0.24594 	 0.20720 	 m..s
   85 	    90 	 0.23418 	 0.22330 	 ~...
   86 	    91 	 0.23700 	 0.22666 	 ~...
   87 	    92 	 0.23719 	 0.23023 	 ~...
  101 	    93 	 0.29919 	 0.24697 	 m..s
   96 	    94 	 0.25906 	 0.24979 	 ~...
  107 	    95 	 0.30839 	 0.25404 	 m..s
   87 	    96 	 0.23719 	 0.25558 	 ~...
   95 	    97 	 0.25884 	 0.25895 	 ~...
  102 	    98 	 0.29977 	 0.26840 	 m..s
   94 	    99 	 0.25439 	 0.27057 	 ~...
  105 	   100 	 0.30672 	 0.28083 	 ~...
  113 	   101 	 0.32739 	 0.28230 	 m..s
  108 	   102 	 0.30893 	 0.28459 	 ~...
  105 	   103 	 0.30672 	 0.28897 	 ~...
   97 	   104 	 0.29155 	 0.29778 	 ~...
   98 	   105 	 0.29187 	 0.30931 	 ~...
  111 	   106 	 0.32282 	 0.30945 	 ~...
  115 	   107 	 0.33553 	 0.31094 	 ~...
  109 	   108 	 0.31100 	 0.31655 	 ~...
  104 	   109 	 0.30150 	 0.32032 	 ~...
  103 	   110 	 0.30101 	 0.32386 	 ~...
  114 	   111 	 0.33060 	 0.33063 	 ~...
   99 	   112 	 0.29756 	 0.34025 	 m..s
  100 	   113 	 0.29785 	 0.34389 	 m..s
  112 	   114 	 0.32738 	 0.34735 	 ~...
  110 	   115 	 0.32282 	 0.36752 	 m..s
  116 	   116 	 0.36894 	 0.37015 	 ~...
  119 	   117 	 0.44816 	 0.41066 	 m..s
  117 	   118 	 0.38621 	 0.42462 	 m..s
  118 	   119 	 0.38721 	 0.44556 	 m..s
  120 	   120 	 0.50134 	 0.48829 	 ~...
==========================================
r_mrr = 0.9809346795082092
r2_mrr = 0.9603684544563293
spearmanr_mrr@5 = 0.9387384653091431
spearmanr_mrr@10 = 0.9695040583610535
spearmanr_mrr@50 = 0.9757792949676514
spearmanr_mrr@100 = 0.9876774549484253
spearmanr_mrr@All = 0.9887465238571167
==========================================
test time: 0.467
Done Testing dataset Kinships
total time taken: 233.93329620361328
training time taken: 226.2265944480896
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9809)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9604)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9387)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9695)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9758)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9877)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9887)}}, 'test_loss': {'ComplEx': {'Kinships': 0.7747801716595859}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 7103102160345296
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [308, 400, 258, 1038, 728, 343, 1077, 815, 741, 870, 789, 49, 771, 172, 256, 92, 628, 98, 924, 743, 819, 222, 395, 833, 640, 567, 54, 1131, 662, 1031, 1035, 955, 500, 706, 476, 701, 788, 446, 173, 572, 1068, 451, 963, 947, 839, 1103, 139, 287, 317, 970, 578, 568, 398, 425, 87, 635, 782, 1189, 672, 683, 78, 834, 862, 613, 1181, 11, 390, 758, 916, 772, 365, 59, 43, 746, 176, 558, 803, 611, 295, 554, 522, 842, 826, 1139, 1098, 1022, 755, 1182, 273, 678, 379, 805, 145, 61, 666, 621, 345, 889, 1011, 203, 100, 363, 1057, 975, 501, 1119, 1095, 297, 417, 738, 253, 518, 1028, 1039, 385, 954, 602, 213, 670, 655, 749]
valid_ids (0): []
train_ids (1094): [1019, 813, 404, 1062, 966, 690, 882, 904, 936, 684, 171, 112, 52, 1006, 533, 650, 211, 366, 1199, 867, 46, 828, 458, 466, 778, 648, 1002, 221, 796, 111, 331, 940, 997, 424, 397, 534, 995, 802, 1097, 521, 653, 957, 888, 480, 1036, 871, 912, 1026, 270, 1202, 727, 474, 582, 456, 433, 1066, 877, 1116, 1105, 227, 422, 442, 1210, 384, 983, 162, 421, 1128, 119, 527, 207, 255, 248, 774, 1188, 483, 1088, 247, 6, 1111, 362, 461, 761, 1178, 1175, 1187, 619, 656, 902, 1118, 573, 1025, 581, 263, 1143, 339, 1185, 914, 1135, 1067, 376, 1212, 5, 937, 536, 751, 154, 449, 890, 1015, 801, 586, 160, 1048, 971, 905, 281, 498, 1213, 1186, 35, 195, 83, 360, 891, 488, 105, 852, 1046, 338, 1043, 1083, 876, 1075, 931, 1196, 561, 827, 816, 288, 753, 319, 570, 121, 37, 709, 926, 1134, 356, 323, 896, 283, 641, 1179, 149, 129, 490, 457, 57, 552, 750, 836, 1027, 1155, 134, 140, 123, 1209, 559, 235, 1152, 1141, 524, 1164, 158, 696, 908, 523, 642, 911, 267, 775, 622, 979, 516, 392, 695, 760, 864, 298, 620, 420, 445, 776, 617, 186, 90, 199, 76, 378, 925, 1140, 933, 18, 9, 486, 130, 494, 29, 783, 615, 375, 289, 39, 636, 732, 19, 754, 1051, 1171, 250, 1099, 718, 932, 825, 324, 597, 95, 103, 1032, 40, 1059, 785, 113, 106, 20, 10, 661, 699, 241, 848, 651, 885, 677, 440, 438, 1149, 909, 237, 810, 675, 631, 935, 806, 278, 1069, 557, 201, 795, 386, 1113, 1060, 503, 580, 21, 790, 497, 791, 41, 688, 634, 600, 934, 74, 192, 373, 807, 1120, 992, 135, 835, 799, 1165, 412, 1166, 668, 1151, 1021, 686, 450, 846, 623, 664, 282, 1170, 444, 47, 544, 820, 1056, 1124, 274, 707, 840, 152, 739, 873, 159, 469, 1096, 1130, 713, 426, 236, 1132, 232, 563, 564, 589, 1086, 372, 687, 332, 157, 437, 4, 251, 604, 850, 51, 949, 509, 944, 505, 859, 855, 126, 24, 1072, 962, 1144, 872, 549, 526, 577, 381, 710, 265, 735, 50, 1138, 34, 1109, 1195, 843, 562, 75, 942, 69, 880, 565, 973, 708, 1087, 318, 787, 1000, 837, 627, 187, 455, 124, 315, 487, 532, 797, 644, 296, 1040, 808, 689, 812, 311, 110, 674, 233, 210, 264, 368, 1091, 583, 541, 605, 733, 1052, 512, 148, 109, 822, 1012, 1044, 555, 1159, 704, 1017, 151, 430, 340, 599, 736, 71, 431, 120, 407, 1172, 155, 1089, 907, 1137, 294, 184, 1005, 79, 1016, 355, 1003, 609, 73, 535, 1049, 225, 181, 153, 238, 346, 491, 1125, 1024, 769, 1054, 624, 198, 272, 183, 127, 847, 520, 1200, 204, 218, 1147, 548, 143, 998, 857, 475, 322, 419, 245, 406, 328, 226, 798, 244, 734, 939, 17, 1142, 380, 762, 142, 652, 612, 530, 167, 132, 23, 1126, 1190, 402, 102, 547, 193, 506, 730, 344, 1127, 1009, 489, 646, 45, 128, 391, 354, 579, 694, 929, 504, 575, 767, 326, 206, 477, 1085, 770, 928, 818, 70, 436, 756, 671, 1122, 64, 1156, 405, 447, 747, 974, 780, 951, 7, 881, 175, 793, 551, 1154, 209, 163, 849, 84, 528, 657, 320, 1020, 492, 481, 587, 350, 310, 482, 886, 938, 744, 88, 133, 1110, 1146, 485, 731, 291, 721, 817, 208, 841, 189, 131, 742, 32, 725, 86, 510, 829, 1194, 341, 903, 484, 1102, 48, 58, 964, 603, 519, 453, 920, 1037, 408, 1045, 349, 465, 1047, 94, 1080, 680, 883, 923, 658, 147, 614, 632, 969, 85, 919, 369, 722, 538, 359, 856, 740, 507, 649, 1197, 921, 700, 667, 459, 993, 716, 388, 660, 276, 539, 830, 702, 773, 1081, 986, 108, 330, 1076, 313, 1, 540, 1004, 1090, 1214, 1001, 1121, 1148, 321, 953, 514, 705, 266, 1115, 1117, 495, 401, 220, 590, 115, 82, 1029, 249, 394, 66, 1100, 93, 1160, 303, 854, 268, 591, 996, 0, 36, 234, 224, 114, 1184, 389, 499, 104, 1153, 571, 616, 1041, 279, 1198, 608, 190, 682, 337, 529, 309, 712, 439, 174, 976, 335, 1050, 197, 1079, 887, 347, 462, 1074, 260, 779, 434, 768, 831, 306, 1204, 560, 821, 336, 863, 874, 286, 30, 358, 329, 606, 26, 1177, 960, 592, 205, 1106, 999, 333, 215, 794, 895, 101, 28, 804, 262, 383, 3, 415, 194, 418, 853, 765, 654, 511, 1192, 918, 697, 814, 228, 626, 387, 823, 824, 866, 1206, 427, 429, 989, 594, 637, 25, 685, 946, 737, 117, 1104, 1169, 361, 729, 645, 899, 868, 230, 243, 858, 1092, 703, 1173, 596, 435, 574, 991, 894, 89, 166, 342, 513, 432, 980, 811, 553, 146, 1163, 212, 1180, 1161, 164, 182, 792, 67, 965, 984, 1042, 96, 1071, 525, 312, 1065, 179, 219, 679, 663, 144, 277, 325, 200, 566, 851, 348, 307, 150, 576, 242, 1030, 454, 467, 630, 16, 169, 136, 414, 647, 598, 724, 1158, 994, 845, 1207, 748, 463, 659, 413, 27, 1183, 271, 80, 941, 165, 1201, 314, 15, 537, 443, 1133, 1058, 714, 959, 726, 869, 301, 676, 1053, 161, 542, 958, 441, 1203, 968, 53, 97, 493, 681, 1064, 202, 1193, 915, 191, 68, 906, 377, 838, 601, 766, 452, 217, 800, 141, 948, 900, 698, 374, 327, 809, 1174, 99, 122, 304, 786, 214, 1167, 8, 299, 1023, 62, 588, 137, 269, 14, 229, 1055, 1078, 673, 496, 22, 987, 893, 116, 107, 334, 55, 403, 952, 715, 517, 764, 865, 275, 546, 1033, 884, 1157, 917, 1205, 239, 464, 1123, 1093, 473, 879, 409, 943, 178, 1162, 364, 945, 56, 393, 44, 1094, 353, 633, 897, 502, 638, 285, 416, 292, 302, 1084, 910, 711, 988, 367, 1114, 898, 138, 1150, 399, 745, 460, 1136, 585, 717, 784, 618, 1061, 254, 1073, 930, 861, 471, 757, 231, 168, 470, 216, 396, 91, 280, 77, 13, 1018, 72, 478, 860, 752, 985, 531, 33, 972, 156, 42, 290, 81, 950, 595, 719, 196, 257, 1107, 1108, 723, 118, 1191, 982, 878, 692, 1176, 428, 1168, 927, 515, 720, 693, 351, 1007, 410, 643, 545, 901, 967, 293, 60, 259, 38, 550, 508, 1013, 832, 1101, 691, 316, 261, 305, 763, 370, 12, 125, 892, 556, 543, 669, 777, 629, 423, 875, 1014, 990, 1070, 584, 759, 31, 593, 610, 246, 1129, 180, 1112, 65, 352, 177, 977, 1008, 781, 411, 1145, 607, 1082, 1211, 1034, 665, 371, 625, 569, 170, 284, 2, 300, 1010, 185, 922, 1063, 382, 223, 252, 913, 844, 1208, 468, 961, 978, 472, 63, 188, 956, 357, 448, 240, 639, 479, 981]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7660804830617600
the save name prefix for this run is:  chkpt-ID_7660804830617600_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 985
rank avg (pred): 0.557 +- 0.003
mrr vals (pred, true): 0.017, 0.453
batch losses (mrrl, rdl): 0.0, 0.0049535008

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 383
rank avg (pred): 0.424 +- 0.190
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 0.0, 5.22025e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 223
rank avg (pred): 0.446 +- 0.244
mrr vals (pred, true): 0.129, 0.060
batch losses (mrrl, rdl): 0.0, 9.7103e-06

Epoch over!
epoch time: 14.961

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 66
rank avg (pred): 0.117 +- 0.078
mrr vals (pred, true): 0.252, 0.305
batch losses (mrrl, rdl): 0.0, 7.032e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 423
rank avg (pred): 0.437 +- 0.263
mrr vals (pred, true): 0.159, 0.058
batch losses (mrrl, rdl): 0.0, 6.3583e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1041
rank avg (pred): 0.434 +- 0.265
mrr vals (pred, true): 0.152, 0.050
batch losses (mrrl, rdl): 0.0, 1.12623e-05

Epoch over!
epoch time: 14.774

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 906
rank avg (pred): 0.168 +- 0.116
mrr vals (pred, true): 0.228, 0.169
batch losses (mrrl, rdl): 0.0, 3.3427e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 715
rank avg (pred): 0.427 +- 0.273
mrr vals (pred, true): 0.164, 0.050
batch losses (mrrl, rdl): 0.0, 9.3102e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1120
rank avg (pred): 0.445 +- 0.262
mrr vals (pred, true): 0.127, 0.055
batch losses (mrrl, rdl): 0.0, 4.5521e-06

Epoch over!
epoch time: 14.951

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 857
rank avg (pred): 0.437 +- 0.264
mrr vals (pred, true): 0.133, 0.053
batch losses (mrrl, rdl): 0.0, 4.5173e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 886
rank avg (pred): 0.434 +- 0.270
mrr vals (pred, true): 0.137, 0.057
batch losses (mrrl, rdl): 0.0, 3.4096e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1128
rank avg (pred): 0.428 +- 0.271
mrr vals (pred, true): 0.146, 0.053
batch losses (mrrl, rdl): 0.0, 9.9361e-06

Epoch over!
epoch time: 14.989

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 675
rank avg (pred): 0.440 +- 0.269
mrr vals (pred, true): 0.135, 0.054
batch losses (mrrl, rdl): 0.0, 3.9548e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1183
rank avg (pred): 0.439 +- 0.268
mrr vals (pred, true): 0.133, 0.050
batch losses (mrrl, rdl): 0.0, 4.0705e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 84
rank avg (pred): 0.438 +- 0.271
mrr vals (pred, true): 0.135, 0.053
batch losses (mrrl, rdl): 0.0, 4.3524e-06

Epoch over!
epoch time: 15.044

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 563
rank avg (pred): 0.144 +- 0.105
mrr vals (pred, true): 0.245, 0.195
batch losses (mrrl, rdl): 0.0253650136, 1.53909e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 242
rank avg (pred): 0.529 +- 0.192
mrr vals (pred, true): 0.072, 0.056
batch losses (mrrl, rdl): 0.0047736913, 0.0001376222

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 41
rank avg (pred): 0.073 +- 0.051
mrr vals (pred, true): 0.305, 0.308
batch losses (mrrl, rdl): 7.23122e-05, 3.59622e-05

Epoch over!
epoch time: 15.054

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 270
rank avg (pred): 0.070 +- 0.050
mrr vals (pred, true): 0.316, 0.307
batch losses (mrrl, rdl): 0.0009377979, 2.47711e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1178
rank avg (pred): 0.546 +- 0.172
mrr vals (pred, true): 0.058, 0.046
batch losses (mrrl, rdl): 0.0006780265, 0.0001684489

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 272
rank avg (pred): 0.067 +- 0.046
mrr vals (pred, true): 0.315, 0.332
batch losses (mrrl, rdl): 0.0029752657, 3.51877e-05

Epoch over!
epoch time: 15.014

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 25
rank avg (pred): 0.080 +- 0.055
mrr vals (pred, true): 0.295, 0.317
batch losses (mrrl, rdl): 0.0046799826, 4.21605e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 779
rank avg (pred): 0.552 +- 0.153
mrr vals (pred, true): 0.050, 0.058
batch losses (mrrl, rdl): 5.778e-07, 0.0002419402

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 202
rank avg (pred): 0.555 +- 0.146
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 4.23174e-05, 0.0002571431

Epoch over!
epoch time: 15.057

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 936
rank avg (pred): 0.545 +- 0.152
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001447202, 0.0002078931

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 293
rank avg (pred): 0.087 +- 0.056
mrr vals (pred, true): 0.277, 0.282
batch losses (mrrl, rdl): 0.0002634283, 5.22867e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 86
rank avg (pred): 0.539 +- 0.149
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 8.58711e-05, 0.0001390273

Epoch over!
epoch time: 15.027

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 447
rank avg (pred): 0.553 +- 0.140
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 6.58435e-05, 0.0001732056

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 843
rank avg (pred): 0.559 +- 0.124
mrr vals (pred, true): 0.040, 0.047
batch losses (mrrl, rdl): 0.0009402895, 0.0002094639

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 229
rank avg (pred): 0.543 +- 0.133
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 2.83105e-05, 0.0001828554

Epoch over!
epoch time: 15.035

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 998
rank avg (pred): 0.022 +- 0.015
mrr vals (pred, true): 0.440, 0.459
batch losses (mrrl, rdl): 0.0035632537, 7.13166e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 574
rank avg (pred): 0.535 +- 0.142
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001992697, 0.0001343196

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 660
rank avg (pred): 0.528 +- 0.143
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0005106219, 0.000116878

Epoch over!
epoch time: 15.031

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 677
rank avg (pred): 0.550 +- 0.106
mrr vals (pred, true): 0.035, 0.053
batch losses (mrrl, rdl): 0.0023664201, 0.0001946919

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 864
rank avg (pred): 0.523 +- 0.144
mrr vals (pred, true): 0.058, 0.055
batch losses (mrrl, rdl): 0.0005681659, 0.0001354158

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 357
rank avg (pred): 0.531 +- 0.127
mrr vals (pred, true): 0.049, 0.057
batch losses (mrrl, rdl): 2.03603e-05, 0.0001495016

Epoch over!
epoch time: 15.05

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 156
rank avg (pred): 0.519 +- 0.142
mrr vals (pred, true): 0.057, 0.047
batch losses (mrrl, rdl): 0.0004448371, 9.78102e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 194
rank avg (pred): 0.528 +- 0.116
mrr vals (pred, true): 0.047, 0.060
batch losses (mrrl, rdl): 0.0001162856, 0.0001493655

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 863
rank avg (pred): 0.538 +- 0.100
mrr vals (pred, true): 0.039, 0.052
batch losses (mrrl, rdl): 0.0012602229, 0.000173342

Epoch over!
epoch time: 15.028

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1206
rank avg (pred): 0.533 +- 0.114
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.0002430645, 0.0001810136

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 282
rank avg (pred): 0.107 +- 0.072
mrr vals (pred, true): 0.294, 0.287
batch losses (mrrl, rdl): 0.0005941944, 1.9579e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 42
rank avg (pred): 0.114 +- 0.077
mrr vals (pred, true): 0.289, 0.266
batch losses (mrrl, rdl): 0.0048791752, 1.39251e-05

Epoch over!
epoch time: 15.032

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 939
rank avg (pred): 0.499 +- 0.147
mrr vals (pred, true): 0.066, 0.049
batch losses (mrrl, rdl): 0.0026255208, 7.37195e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 515
rank avg (pred): 0.217 +- 0.144
mrr vals (pred, true): 0.248, 0.233
batch losses (mrrl, rdl): 0.0023002336, 0.0001964619

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 972
rank avg (pred): 0.011 +- 0.008
mrr vals (pred, true): 0.559, 0.394
batch losses (mrrl, rdl): 0.2719439864, 0.0001098127

Epoch over!
epoch time: 15.03

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.078 +- 0.054
mrr vals (pred, true): 0.334, 0.306

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   12 	     0 	 0.05138 	 0.04547 	 ~...
   34 	     1 	 0.05330 	 0.04716 	 ~...
   18 	     2 	 0.05197 	 0.04717 	 ~...
    4 	     3 	 0.04930 	 0.04767 	 ~...
   14 	     4 	 0.05142 	 0.04783 	 ~...
   51 	     5 	 0.05447 	 0.04823 	 ~...
   79 	     6 	 0.05840 	 0.04830 	 ~...
    6 	     7 	 0.04931 	 0.04836 	 ~...
   65 	     8 	 0.05553 	 0.04838 	 ~...
   30 	     9 	 0.05320 	 0.04840 	 ~...
   42 	    10 	 0.05372 	 0.04852 	 ~...
   20 	    11 	 0.05208 	 0.04866 	 ~...
   62 	    12 	 0.05484 	 0.04869 	 ~...
   25 	    13 	 0.05259 	 0.04872 	 ~...
    8 	    14 	 0.05033 	 0.04937 	 ~...
   43 	    15 	 0.05374 	 0.04945 	 ~...
   22 	    16 	 0.05210 	 0.04998 	 ~...
   77 	    17 	 0.05767 	 0.05035 	 ~...
   70 	    18 	 0.05604 	 0.05035 	 ~...
   68 	    19 	 0.05573 	 0.05056 	 ~...
   26 	    20 	 0.05262 	 0.05059 	 ~...
   24 	    21 	 0.05257 	 0.05064 	 ~...
   57 	    22 	 0.05476 	 0.05075 	 ~...
   67 	    23 	 0.05572 	 0.05113 	 ~...
   64 	    24 	 0.05533 	 0.05113 	 ~...
   78 	    25 	 0.05790 	 0.05154 	 ~...
   34 	    26 	 0.05330 	 0.05186 	 ~...
    4 	    27 	 0.04930 	 0.05203 	 ~...
   76 	    28 	 0.05766 	 0.05207 	 ~...
   53 	    29 	 0.05470 	 0.05212 	 ~...
   62 	    30 	 0.05484 	 0.05221 	 ~...
   74 	    31 	 0.05703 	 0.05239 	 ~...
   29 	    32 	 0.05315 	 0.05250 	 ~...
   17 	    33 	 0.05197 	 0.05274 	 ~...
    2 	    34 	 0.04874 	 0.05280 	 ~...
   80 	    35 	 0.06129 	 0.05286 	 ~...
   40 	    36 	 0.05367 	 0.05293 	 ~...
   72 	    37 	 0.05617 	 0.05313 	 ~...
   21 	    38 	 0.05209 	 0.05324 	 ~...
   58 	    39 	 0.05478 	 0.05328 	 ~...
   43 	    40 	 0.05374 	 0.05330 	 ~...
    9 	    41 	 0.05087 	 0.05336 	 ~...
   36 	    42 	 0.05332 	 0.05343 	 ~...
   71 	    43 	 0.05610 	 0.05345 	 ~...
   81 	    44 	 0.06218 	 0.05350 	 ~...
   23 	    45 	 0.05219 	 0.05351 	 ~...
   32 	    46 	 0.05325 	 0.05356 	 ~...
   37 	    47 	 0.05338 	 0.05396 	 ~...
   55 	    48 	 0.05472 	 0.05425 	 ~...
    7 	    49 	 0.04991 	 0.05434 	 ~...
   39 	    50 	 0.05365 	 0.05434 	 ~...
   60 	    51 	 0.05479 	 0.05447 	 ~...
   28 	    52 	 0.05302 	 0.05455 	 ~...
   47 	    53 	 0.05413 	 0.05457 	 ~...
   41 	    54 	 0.05368 	 0.05466 	 ~...
   47 	    55 	 0.05413 	 0.05494 	 ~...
   54 	    56 	 0.05470 	 0.05494 	 ~...
   52 	    57 	 0.05454 	 0.05534 	 ~...
   49 	    58 	 0.05421 	 0.05550 	 ~...
   69 	    59 	 0.05595 	 0.05553 	 ~...
   56 	    60 	 0.05475 	 0.05601 	 ~...
   11 	    61 	 0.05113 	 0.05626 	 ~...
   31 	    62 	 0.05321 	 0.05650 	 ~...
   46 	    63 	 0.05409 	 0.05656 	 ~...
   61 	    64 	 0.05481 	 0.05660 	 ~...
   72 	    65 	 0.05617 	 0.05664 	 ~...
    1 	    66 	 0.04833 	 0.05673 	 ~...
   13 	    67 	 0.05140 	 0.05766 	 ~...
   45 	    68 	 0.05398 	 0.05775 	 ~...
    3 	    69 	 0.04894 	 0.05787 	 ~...
   19 	    70 	 0.05201 	 0.05796 	 ~...
   27 	    71 	 0.05281 	 0.05816 	 ~...
    9 	    72 	 0.05087 	 0.05863 	 ~...
   66 	    73 	 0.05564 	 0.05874 	 ~...
    0 	    74 	 0.04792 	 0.05902 	 ~...
   15 	    75 	 0.05165 	 0.05917 	 ~...
   38 	    76 	 0.05348 	 0.05952 	 ~...
   59 	    77 	 0.05478 	 0.05982 	 ~...
   50 	    78 	 0.05427 	 0.06038 	 ~...
   16 	    79 	 0.05197 	 0.06181 	 ~...
   75 	    80 	 0.05754 	 0.06254 	 ~...
   33 	    81 	 0.05327 	 0.06325 	 ~...
   82 	    82 	 0.23289 	 0.16198 	 m..s
   83 	    83 	 0.24148 	 0.18561 	 m..s
   84 	    84 	 0.24737 	 0.18970 	 m..s
   86 	    85 	 0.26758 	 0.22302 	 m..s
   85 	    86 	 0.25990 	 0.22414 	 m..s
   89 	    87 	 0.28300 	 0.23853 	 m..s
   88 	    88 	 0.28130 	 0.25103 	 m..s
   94 	    89 	 0.28871 	 0.26350 	 ~...
   96 	    90 	 0.29618 	 0.27513 	 ~...
  102 	    91 	 0.32254 	 0.28774 	 m..s
  105 	    92 	 0.33187 	 0.29390 	 m..s
   95 	    93 	 0.29362 	 0.29459 	 ~...
   87 	    94 	 0.27299 	 0.29813 	 ~...
  107 	    95 	 0.33412 	 0.30084 	 m..s
   99 	    96 	 0.32001 	 0.30110 	 ~...
  101 	    97 	 0.32234 	 0.30230 	 ~...
   98 	    98 	 0.31577 	 0.30389 	 ~...
  106 	    99 	 0.33391 	 0.30574 	 ~...
   97 	   100 	 0.31035 	 0.30931 	 ~...
  100 	   101 	 0.32056 	 0.32078 	 ~...
  103 	   102 	 0.33127 	 0.32141 	 ~...
   90 	   103 	 0.28531 	 0.33122 	 m..s
  111 	   104 	 0.35740 	 0.33811 	 ~...
   91 	   105 	 0.28540 	 0.33847 	 m..s
  103 	   106 	 0.33127 	 0.34735 	 ~...
  110 	   107 	 0.35446 	 0.35087 	 ~...
   93 	   108 	 0.28568 	 0.35846 	 m..s
   92 	   109 	 0.28559 	 0.36169 	 m..s
  112 	   110 	 0.38237 	 0.38991 	 ~...
  112 	   111 	 0.38237 	 0.39756 	 ~...
  118 	   112 	 0.41682 	 0.40618 	 ~...
  117 	   113 	 0.41649 	 0.41200 	 ~...
  114 	   114 	 0.38470 	 0.42462 	 m..s
  119 	   115 	 0.41990 	 0.44244 	 ~...
  115 	   116 	 0.38608 	 0.46828 	 m..s
  109 	   117 	 0.35083 	 0.47946 	 MISS
  108 	   118 	 0.35040 	 0.48472 	 MISS
  116 	   119 	 0.38611 	 0.49103 	 MISS
  120 	   120 	 0.45988 	 0.62288 	 MISS
==========================================
r_mrr = 0.9760235548019409
r2_mrr = 0.9490856528282166
spearmanr_mrr@5 = 0.9068500995635986
spearmanr_mrr@10 = 0.9435082674026489
spearmanr_mrr@50 = 0.9700823426246643
spearmanr_mrr@100 = 0.9856914281845093
spearmanr_mrr@All = 0.9870410561561584
==========================================
test time: 0.457
Done Testing dataset Kinships
total time taken: 233.27229499816895
training time taken: 225.54427289962769
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9760)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9491)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9069)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9435)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9701)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9857)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9870)}}, 'test_loss': {'ComplEx': {'Kinships': 1.2574354782918817}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 1295534603164979
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [678, 1020, 592, 978, 386, 902, 585, 1161, 214, 990, 191, 370, 1166, 380, 583, 395, 1158, 96, 175, 412, 966, 205, 686, 426, 1114, 235, 816, 494, 268, 250, 152, 885, 1188, 556, 449, 824, 243, 776, 266, 1050, 455, 690, 1047, 137, 279, 771, 25, 622, 643, 1124, 1203, 1076, 843, 558, 723, 1026, 800, 194, 999, 798, 1136, 621, 100, 737, 540, 446, 66, 294, 321, 1196, 109, 86, 1160, 721, 952, 57, 1024, 20, 606, 389, 437, 105, 436, 1147, 653, 390, 892, 1112, 789, 64, 163, 871, 356, 942, 802, 150, 281, 433, 683, 672, 387, 1198, 363, 1098, 596, 792, 300, 787, 948, 668, 67, 1205, 856, 507, 283, 82, 808, 173, 1063, 719, 78]
valid_ids (0): []
train_ids (1094): [210, 724, 1058, 1057, 102, 170, 313, 471, 1127, 1039, 743, 1106, 398, 997, 377, 146, 320, 778, 341, 579, 936, 264, 311, 658, 400, 781, 200, 872, 1053, 1025, 584, 624, 142, 614, 806, 85, 330, 972, 60, 206, 807, 899, 947, 561, 325, 42, 1016, 312, 845, 211, 569, 930, 1212, 275, 399, 762, 691, 883, 1123, 533, 1162, 692, 479, 510, 361, 288, 472, 434, 756, 237, 1194, 117, 1169, 650, 637, 224, 1079, 1074, 1157, 983, 366, 37, 90, 1, 955, 661, 1038, 587, 880, 921, 416, 1113, 349, 480, 187, 877, 552, 758, 748, 249, 429, 616, 230, 984, 938, 543, 745, 524, 1060, 875, 933, 526, 810, 461, 707, 1061, 674, 575, 409, 1031, 5, 581, 1182, 302, 530, 148, 1207, 973, 53, 1164, 450, 218, 864, 701, 932, 574, 30, 945, 1011, 852, 413, 8, 1091, 931, 960, 1015, 729, 337, 1192, 115, 24, 1081, 741, 192, 744, 43, 270, 13, 430, 1036, 431, 508, 422, 958, 559, 594, 443, 1141, 1183, 139, 865, 512, 1144, 233, 154, 299, 1129, 445, 577, 404, 591, 750, 419, 623, 149, 48, 694, 156, 566, 444, 657, 582, 553, 849, 306, 628, 962, 95, 442, 38, 1101, 1134, 1199, 1149, 223, 988, 1208, 649, 597, 285, 1052, 937, 505, 768, 174, 916, 714, 1108, 71, 602, 760, 213, 1142, 318, 1121, 326, 121, 664, 659, 568, 976, 1146, 609, 166, 375, 919, 358, 1152, 1213, 588, 554, 4, 1201, 1209, 927, 333, 1180, 350, 542, 204, 483, 207, 231, 133, 92, 706, 309, 269, 995, 497, 626, 342, 642, 735, 457, 1200, 1033, 402, 420, 120, 681, 548, 992, 549, 365, 651, 459, 481, 1041, 406, 1156, 21, 844, 193, 1023, 343, 484, 335, 1004, 893, 1010, 1095, 1049, 850, 401, 454, 253, 267, 1174, 599, 696, 31, 814, 39, 730, 276, 226, 368, 232, 34, 1088, 767, 56, 963, 829, 91, 98, 272, 598, 169, 199, 357, 221, 384, 1068, 847, 72, 178, 1055, 439, 1165, 830, 669, 177, 1075, 879, 185, 1072, 151, 925, 315, 634, 1056, 532, 107, 469, 996, 751, 462, 728, 307, 636, 385, 407, 360, 1154, 926, 539, 1104, 161, 1100, 1048, 108, 424, 970, 673, 870, 125, 713, 815, 841, 1137, 1027, 527, 145, 134, 761, 564, 954, 241, 523, 456, 1155, 19, 184, 834, 410, 720, 7, 890, 906, 881, 1135, 1030, 1179, 159, 645, 9, 1019, 770, 1128, 935, 323, 212, 851, 833, 397, 991, 1195, 519, 55, 688, 47, 466, 801, 590, 652, 1046, 372, 557, 832, 703, 1006, 943, 1168, 939, 975, 1062, 1085, 853, 1028, 470, 274, 54, 334, 965, 817, 203, 418, 215, 709, 240, 520, 295, 260, 726, 739, 448, 842, 662, 998, 12, 352, 589, 114, 50, 489, 104, 1082, 563, 14, 929, 1120, 393, 1001, 23, 795, 619, 534, 89, 908, 482, 684, 18, 77, 994, 113, 980, 640, 826, 171, 1080, 347, 373, 1139, 780, 537, 1184, 52, 128, 670, 94, 256, 777, 1070, 766, 247, 515, 982, 202, 465, 560, 301, 1119, 181, 1151, 172, 676, 118, 578, 317, 51, 290, 1012, 1189, 747, 1064, 44, 509, 956, 345, 46, 251, 828, 130, 1066, 894, 961, 772, 1175, 1084, 1117, 793, 629, 705, 28, 974, 222, 873, 812, 123, 103, 284, 280, 485, 438, 40, 217, 93, 665, 804, 491, 242, 427, 435, 677, 562, 521, 234, 403, 600, 180, 49, 1008, 805, 1193, 898, 376, 884, 225, 654, 698, 124, 478, 374, 914, 1206, 1191, 87, 157, 1099, 666, 303, 825, 355, 62, 138, 944, 689, 371, 477, 625, 682, 164, 535, 378, 1122, 809, 69, 1132, 1177, 297, 704, 1211, 1014, 1021, 291, 1034, 106, 475, 501, 417, 601, 712, 112, 525, 917, 1044, 167, 239, 411, 354, 848, 388, 576, 1148, 58, 838, 603, 946, 785, 783, 1111, 153, 1037, 97, 971, 860, 143, 405, 261, 36, 660, 15, 336, 1002, 255, 278, 248, 529, 1202, 866, 887, 1045, 338, 263, 474, 511, 447, 1007, 620, 467, 219, 506, 332, 861, 132, 786, 903, 950, 904, 344, 254, 791, 820, 75, 1005, 362, 1109, 835, 551, 888, 891, 840, 220, 784, 855, 1172, 1040, 949, 201, 1204, 473, 76, 702, 453, 570, 141, 827, 1087, 981, 769, 65, 858, 45, 127, 1131, 101, 59, 1145, 126, 88, 293, 1029, 685, 84, 1126, 1077, 244, 905, 746, 32, 188, 1167, 314, 421, 147, 759, 740, 74, 464, 136, 863, 580, 440, 618, 1089, 1138, 339, 774, 382, 1115, 396, 79, 0, 878, 364, 16, 155, 895, 868, 964, 331, 639, 216, 733, 496, 1181, 238, 517, 763, 968, 168, 1018, 262, 1143, 876, 818, 708, 183, 498, 2, 874, 353, 198, 869, 451, 160, 502, 22, 189, 846, 381, 607, 1171, 492, 779, 452, 734, 1059, 1013, 907, 33, 176, 573, 277, 165, 1105, 1176, 901, 329, 1071, 236, 182, 836, 586, 1150, 595, 941, 1159, 918, 1022, 680, 367, 794, 228, 486, 928, 655, 923, 41, 17, 897, 940, 732, 531, 1009, 351, 641, 287, 711, 544, 319, 957, 752, 122, 35, 415, 1133, 782, 131, 764, 695, 328, 831, 83, 718, 715, 610, 271, 229, 819, 889, 259, 1093, 571, 555, 129, 327, 648, 742, 647, 111, 822, 1210, 208, 414, 162, 1054, 700, 1125, 631, 796, 140, 503, 909, 1116, 882, 1118, 839, 725, 550, 432, 773, 144, 273, 993, 1107, 1153, 1110, 26, 920, 493, 951, 316, 854, 186, 969, 310, 500, 1197, 209, 632, 286, 1187, 245, 1065, 633, 346, 1078, 886, 736, 246, 110, 967, 1102, 488, 545, 541, 565, 116, 468, 81, 190, 567, 857, 716, 348, 227, 383, 68, 513, 663, 627, 699, 304, 675, 755, 753, 679, 615, 986, 953, 296, 977, 989, 593, 135, 754, 915, 1163, 27, 1000, 959, 896, 1032, 460, 322, 924, 697, 803, 1103, 476, 499, 934, 656, 910, 617, 394, 922, 495, 757, 258, 11, 1073, 428, 308, 538, 536, 487, 305, 1173, 547, 504, 1185, 99, 738, 644, 1130, 289, 604, 749, 252, 80, 797, 1069, 837, 423, 369, 913, 1043, 458, 1097, 987, 257, 1035, 862, 1086, 6, 613, 195, 811, 514, 731, 788, 1140, 859, 635, 612, 867, 813, 1003, 611, 340, 1094, 693, 518, 823, 1096, 1042, 765, 197, 441, 646, 298, 687, 1178, 522, 546, 265, 179, 799, 821, 900, 630, 722, 463, 282, 1170, 667, 359, 985, 292, 73, 790, 158, 1051, 63, 727, 717, 1214, 1186, 608, 572, 912, 379, 490, 979, 911, 10, 1083, 516, 408, 671, 638, 1017, 1090, 775, 70, 119, 392, 391, 3, 29, 528, 61, 324, 1092, 710, 1190, 1067, 605, 425, 196]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4352851468419124
the save name prefix for this run is:  chkpt-ID_4352851468419124_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 981
rank avg (pred): 0.455 +- 0.006
mrr vals (pred, true): 0.021, 0.357
batch losses (mrrl, rdl): 0.0, 0.0027734579

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 336
rank avg (pred): 0.477 +- 0.008
mrr vals (pred, true): 0.020, 0.057
batch losses (mrrl, rdl): 0.0, 9.6258e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 7
rank avg (pred): 0.109 +- 0.005
mrr vals (pred, true): 0.082, 0.317
batch losses (mrrl, rdl): 0.0, 6.2133e-06

Epoch over!
epoch time: 14.913

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 811
rank avg (pred): 0.089 +- 0.005
mrr vals (pred, true): 0.098, 0.627
batch losses (mrrl, rdl): 0.0, 6.13344e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1145
rank avg (pred): 0.123 +- 0.009
mrr vals (pred, true): 0.073, 0.261
batch losses (mrrl, rdl): 0.0, 1.16749e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 192
rank avg (pred): 0.463 +- 0.267
mrr vals (pred, true): 0.067, 0.062
batch losses (mrrl, rdl): 0.0, 4.6199e-06

Epoch over!
epoch time: 14.892

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 456
rank avg (pred): 0.467 +- 0.267
mrr vals (pred, true): 0.069, 0.061
batch losses (mrrl, rdl): 0.0, 1.7536e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1080
rank avg (pred): 0.457 +- 0.280
mrr vals (pred, true): 0.094, 0.051
batch losses (mrrl, rdl): 0.0, 3.08e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 512
rank avg (pred): 0.117 +- 0.137
mrr vals (pred, true): 0.218, 0.236
batch losses (mrrl, rdl): 0.0, 6.5426e-06

Epoch over!
epoch time: 14.854

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1039
rank avg (pred): 0.470 +- 0.268
mrr vals (pred, true): 0.094, 0.053
batch losses (mrrl, rdl): 0.0, 1.6123e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 60
rank avg (pred): 0.125 +- 0.159
mrr vals (pred, true): 0.241, 0.334
batch losses (mrrl, rdl): 0.0, 1.34659e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 754
rank avg (pred): 0.129 +- 0.173
mrr vals (pred, true): 0.241, 0.310
batch losses (mrrl, rdl): 0.0, 7.292e-07

Epoch over!
epoch time: 14.844

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 255
rank avg (pred): 0.057 +- 0.084
mrr vals (pred, true): 0.343, 0.280
batch losses (mrrl, rdl): 0.0, 0.000151494

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 525
rank avg (pred): 0.158 +- 0.174
mrr vals (pred, true): 0.207, 0.211
batch losses (mrrl, rdl): 0.0, 6.634e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1212
rank avg (pred): 0.471 +- 0.277
mrr vals (pred, true): 0.103, 0.051
batch losses (mrrl, rdl): 0.0, 2.8392e-06

Epoch over!
epoch time: 15.03

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 628
rank avg (pred): 0.479 +- 0.276
mrr vals (pred, true): 0.102, 0.051
batch losses (mrrl, rdl): 0.0275330357, 1.719e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 685
rank avg (pred): 0.426 +- 0.118
mrr vals (pred, true): 0.050, 0.056
batch losses (mrrl, rdl): 1.0792e-06, 8.41657e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 543
rank avg (pred): 0.249 +- 0.149
mrr vals (pred, true): 0.190, 0.227
batch losses (mrrl, rdl): 0.0132748811, 0.0002518725

Epoch over!
epoch time: 15.286

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1168
rank avg (pred): 0.440 +- 0.100
mrr vals (pred, true): 0.043, 0.053
batch losses (mrrl, rdl): 0.0005426407, 6.48163e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 841
rank avg (pred): 0.436 +- 0.106
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 4.878e-07, 6.86112e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 909
rank avg (pred): 0.215 +- 0.149
mrr vals (pred, true): 0.278, 0.251
batch losses (mrrl, rdl): 0.0071238824, 0.0001756103

Epoch over!
epoch time: 15.258

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 253
rank avg (pred): 0.044 +- 0.031
mrr vals (pred, true): 0.388, 0.321
batch losses (mrrl, rdl): 0.0440640673, 9.31295e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 812
rank avg (pred): 0.030 +- 0.020
mrr vals (pred, true): 0.416, 0.628
batch losses (mrrl, rdl): 0.4464489818, 5.764e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 901
rank avg (pred): 0.203 +- 0.140
mrr vals (pred, true): 0.279, 0.260
batch losses (mrrl, rdl): 0.0034712013, 0.0001458752

Epoch over!
epoch time: 15.27

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 403
rank avg (pred): 0.427 +- 0.115
mrr vals (pred, true): 0.058, 0.050
batch losses (mrrl, rdl): 0.0005964283, 6.52462e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 210
rank avg (pred): 0.441 +- 0.093
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 2.20314e-05, 7.10681e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 440
rank avg (pred): 0.447 +- 0.083
mrr vals (pred, true): 0.042, 0.054
batch losses (mrrl, rdl): 0.0006063778, 7.55966e-05

Epoch over!
epoch time: 15.035

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1118
rank avg (pred): 0.440 +- 0.085
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 0.0001092036, 7.61425e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 829
rank avg (pred): 0.020 +- 0.013
mrr vals (pred, true): 0.466, 0.406
batch losses (mrrl, rdl): 0.0356861204, 7.92222e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 661
rank avg (pred): 0.423 +- 0.117
mrr vals (pred, true): 0.062, 0.051
batch losses (mrrl, rdl): 0.0015293778, 5.88996e-05

Epoch over!
epoch time: 15.027

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 486
rank avg (pred): 0.264 +- 0.174
mrr vals (pred, true): 0.240, 0.226
batch losses (mrrl, rdl): 0.0021279666, 0.0003463748

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1184
rank avg (pred): 0.439 +- 0.087
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 4.072e-07, 8.52297e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1117
rank avg (pred): 0.443 +- 0.089
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 7.2817e-06, 6.20772e-05

Epoch over!
epoch time: 15.054

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 104
rank avg (pred): 0.440 +- 0.087
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 2.39e-08, 8.48097e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 664
rank avg (pred): 0.432 +- 0.095
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001155254, 0.000129435

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1204
rank avg (pred): 0.434 +- 0.095
mrr vals (pred, true): 0.054, 0.059
batch losses (mrrl, rdl): 0.0001596873, 8.10064e-05

Epoch over!
epoch time: 15.029

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 420
rank avg (pred): 0.439 +- 0.089
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 2.17367e-05, 9.57077e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 256
rank avg (pred): 0.086 +- 0.059
mrr vals (pred, true): 0.320, 0.347
batch losses (mrrl, rdl): 0.0072495388, 1.85419e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1133
rank avg (pred): 0.447 +- 0.073
mrr vals (pred, true): 0.039, 0.055
batch losses (mrrl, rdl): 0.0012470282, 8.52034e-05

Epoch over!
epoch time: 15.037

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 682
rank avg (pred): 0.441 +- 0.078
mrr vals (pred, true): 0.046, 0.058
batch losses (mrrl, rdl): 0.0001237321, 8.08316e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1211
rank avg (pred): 0.437 +- 0.083
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 7.8799e-06, 7.81459e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 528
rank avg (pred): 0.315 +- 0.201
mrr vals (pred, true): 0.212, 0.201
batch losses (mrrl, rdl): 0.0011501266, 0.0004692043

Epoch over!
epoch time: 15.021

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 593
rank avg (pred): 0.449 +- 0.082
mrr vals (pred, true): 0.047, 0.058
batch losses (mrrl, rdl): 6.91682e-05, 7.2787e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 254
rank avg (pred): 0.070 +- 0.049
mrr vals (pred, true): 0.344, 0.356
batch losses (mrrl, rdl): 0.0013393954, 1.63243e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 530
rank avg (pred): 0.290 +- 0.194
mrr vals (pred, true): 0.241, 0.218
batch losses (mrrl, rdl): 0.0052487897, 0.0005277338

Epoch over!
epoch time: 15.017

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.436 +- 0.084
mrr vals (pred, true): 0.051, 0.060

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.05020 	 0.04696 	 ~...
   40 	     1 	 0.05144 	 0.04713 	 ~...
   71 	     2 	 0.05234 	 0.04716 	 ~...
   84 	     3 	 0.05306 	 0.04749 	 ~...
   61 	     4 	 0.05189 	 0.04767 	 ~...
   73 	     5 	 0.05236 	 0.04858 	 ~...
   12 	     6 	 0.05072 	 0.04869 	 ~...
   66 	     7 	 0.05215 	 0.04872 	 ~...
   18 	     8 	 0.05091 	 0.04879 	 ~...
   64 	     9 	 0.05202 	 0.04892 	 ~...
   51 	    10 	 0.05173 	 0.04922 	 ~...
   41 	    11 	 0.05144 	 0.04922 	 ~...
   26 	    12 	 0.05106 	 0.04960 	 ~...
   29 	    13 	 0.05110 	 0.04968 	 ~...
   21 	    14 	 0.05094 	 0.04998 	 ~...
   37 	    15 	 0.05138 	 0.05035 	 ~...
   80 	    16 	 0.05285 	 0.05050 	 ~...
   42 	    17 	 0.05147 	 0.05056 	 ~...
   77 	    18 	 0.05261 	 0.05062 	 ~...
   65 	    19 	 0.05214 	 0.05064 	 ~...
   38 	    20 	 0.05141 	 0.05067 	 ~...
   54 	    21 	 0.05177 	 0.05073 	 ~...
   35 	    22 	 0.05134 	 0.05075 	 ~...
   63 	    23 	 0.05194 	 0.05084 	 ~...
   53 	    24 	 0.05175 	 0.05086 	 ~...
   75 	    25 	 0.05241 	 0.05119 	 ~...
    5 	    26 	 0.05023 	 0.05121 	 ~...
   57 	    27 	 0.05179 	 0.05135 	 ~...
    0 	    28 	 0.04972 	 0.05138 	 ~...
    3 	    29 	 0.05009 	 0.05146 	 ~...
   14 	    30 	 0.05079 	 0.05158 	 ~...
   56 	    31 	 0.05178 	 0.05164 	 ~...
   23 	    32 	 0.05095 	 0.05168 	 ~...
   67 	    33 	 0.05215 	 0.05184 	 ~...
   83 	    34 	 0.05300 	 0.05188 	 ~...
   52 	    35 	 0.05173 	 0.05191 	 ~...
   39 	    36 	 0.05143 	 0.05225 	 ~...
   68 	    37 	 0.05222 	 0.05235 	 ~...
   16 	    38 	 0.05084 	 0.05241 	 ~...
   15 	    39 	 0.05080 	 0.05243 	 ~...
    6 	    40 	 0.05031 	 0.05261 	 ~...
    9 	    41 	 0.05059 	 0.05278 	 ~...
   85 	    42 	 0.05364 	 0.05286 	 ~...
    7 	    43 	 0.05035 	 0.05287 	 ~...
   22 	    44 	 0.05095 	 0.05313 	 ~...
   78 	    45 	 0.05265 	 0.05356 	 ~...
   60 	    46 	 0.05187 	 0.05370 	 ~...
   69 	    47 	 0.05226 	 0.05375 	 ~...
   13 	    48 	 0.05077 	 0.05392 	 ~...
    8 	    49 	 0.05047 	 0.05401 	 ~...
   27 	    50 	 0.05106 	 0.05422 	 ~...
   79 	    51 	 0.05267 	 0.05432 	 ~...
   36 	    52 	 0.05137 	 0.05435 	 ~...
   31 	    53 	 0.05113 	 0.05528 	 ~...
   24 	    54 	 0.05097 	 0.05545 	 ~...
    1 	    55 	 0.04982 	 0.05555 	 ~...
   74 	    56 	 0.05237 	 0.05571 	 ~...
   17 	    57 	 0.05087 	 0.05578 	 ~...
   81 	    58 	 0.05292 	 0.05594 	 ~...
   76 	    59 	 0.05247 	 0.05621 	 ~...
   49 	    60 	 0.05164 	 0.05654 	 ~...
   72 	    61 	 0.05235 	 0.05668 	 ~...
   59 	    62 	 0.05185 	 0.05680 	 ~...
   43 	    63 	 0.05148 	 0.05687 	 ~...
   44 	    64 	 0.05149 	 0.05688 	 ~...
   48 	    65 	 0.05156 	 0.05691 	 ~...
   58 	    66 	 0.05184 	 0.05703 	 ~...
   28 	    67 	 0.05109 	 0.05723 	 ~...
   32 	    68 	 0.05113 	 0.05728 	 ~...
   50 	    69 	 0.05173 	 0.05762 	 ~...
   33 	    70 	 0.05116 	 0.05770 	 ~...
   45 	    71 	 0.05151 	 0.05775 	 ~...
   30 	    72 	 0.05112 	 0.05776 	 ~...
   20 	    73 	 0.05092 	 0.05781 	 ~...
   55 	    74 	 0.05178 	 0.05787 	 ~...
    2 	    75 	 0.04984 	 0.05881 	 ~...
   34 	    76 	 0.05119 	 0.05900 	 ~...
   70 	    77 	 0.05229 	 0.05904 	 ~...
   47 	    78 	 0.05156 	 0.05917 	 ~...
   62 	    79 	 0.05190 	 0.05964 	 ~...
   82 	    80 	 0.05298 	 0.05973 	 ~...
   19 	    81 	 0.05092 	 0.05982 	 ~...
   11 	    82 	 0.05066 	 0.06008 	 ~...
   10 	    83 	 0.05060 	 0.06021 	 ~...
   25 	    84 	 0.05100 	 0.06038 	 ~...
   46 	    85 	 0.05156 	 0.06265 	 ~...
  115 	    86 	 0.42193 	 0.14588 	 MISS
  114 	    87 	 0.42094 	 0.15938 	 MISS
   86 	    88 	 0.26096 	 0.18561 	 m..s
   86 	    89 	 0.26096 	 0.20462 	 m..s
   86 	    90 	 0.26096 	 0.21430 	 m..s
   86 	    91 	 0.26096 	 0.22737 	 m..s
   94 	    92 	 0.30601 	 0.24886 	 m..s
   95 	    93 	 0.30630 	 0.26350 	 m..s
  107 	    94 	 0.32930 	 0.26724 	 m..s
   97 	    95 	 0.30777 	 0.26840 	 m..s
   90 	    96 	 0.27900 	 0.27033 	 ~...
   93 	    97 	 0.30258 	 0.27573 	 ~...
   96 	    98 	 0.30763 	 0.27892 	 ~...
   99 	    99 	 0.32367 	 0.28071 	 m..s
  105 	   100 	 0.32829 	 0.28244 	 m..s
   92 	   101 	 0.27985 	 0.28579 	 ~...
   91 	   102 	 0.27901 	 0.29150 	 ~...
  108 	   103 	 0.32991 	 0.29445 	 m..s
  106 	   104 	 0.32846 	 0.29778 	 m..s
  100 	   105 	 0.32570 	 0.30348 	 ~...
   98 	   106 	 0.32243 	 0.30510 	 ~...
  112 	   107 	 0.34236 	 0.30980 	 m..s
  111 	   108 	 0.34114 	 0.31094 	 m..s
  101 	   109 	 0.32639 	 0.31655 	 ~...
  104 	   110 	 0.32768 	 0.31912 	 ~...
  103 	   111 	 0.32699 	 0.32474 	 ~...
  110 	   112 	 0.33960 	 0.34172 	 ~...
  113 	   113 	 0.34861 	 0.34338 	 ~...
  109 	   114 	 0.33804 	 0.35135 	 ~...
  120 	   115 	 0.43634 	 0.37130 	 m..s
  119 	   116 	 0.43498 	 0.38537 	 m..s
  117 	   117 	 0.42534 	 0.42762 	 ~...
  118 	   118 	 0.42838 	 0.44556 	 ~...
  102 	   119 	 0.32680 	 0.44883 	 MISS
  116 	   120 	 0.42379 	 0.53394 	 MISS
==========================================
r_mrr = 0.9497575163841248
r2_mrr = 0.8784164786338806
spearmanr_mrr@5 = 0.8475512862205505
spearmanr_mrr@10 = 0.7224624156951904
spearmanr_mrr@50 = 0.9678891897201538
spearmanr_mrr@100 = 0.9831193089485168
spearmanr_mrr@All = 0.9845307469367981
==========================================
test time: 0.453
Done Testing dataset Kinships
total time taken: 233.82555651664734
training time taken: 226.03082466125488
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9498)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.8784)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.8476)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.7225)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9679)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9831)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9845)}}, 'test_loss': {'ComplEx': {'Kinships': 2.138775583349343}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 948062385052008
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [52, 563, 1115, 45, 493, 121, 392, 1211, 732, 252, 113, 938, 804, 825, 316, 961, 372, 1046, 669, 998, 1142, 515, 75, 152, 109, 803, 66, 1044, 212, 1084, 56, 1026, 817, 810, 558, 20, 214, 882, 614, 1133, 446, 506, 885, 1078, 131, 1201, 51, 719, 657, 703, 267, 147, 780, 595, 649, 280, 712, 496, 687, 343, 42, 728, 61, 202, 776, 379, 1061, 1150, 1144, 327, 958, 888, 1207, 63, 1132, 996, 277, 3, 569, 620, 1176, 319, 122, 799, 1072, 824, 1036, 1168, 919, 291, 1148, 701, 352, 706, 502, 704, 869, 295, 254, 845, 1135, 371, 546, 838, 737, 135, 808, 668, 698, 555, 1100, 1165, 1068, 647, 491, 158, 30, 631, 178, 878, 950]
valid_ids (0): []
train_ids (1094): [500, 238, 453, 495, 169, 336, 643, 1017, 1178, 389, 835, 41, 822, 17, 222, 1103, 397, 46, 304, 809, 963, 854, 747, 76, 613, 1208, 307, 617, 261, 257, 1114, 263, 265, 849, 1092, 575, 659, 209, 628, 784, 11, 276, 1185, 433, 664, 33, 755, 576, 1000, 862, 1059, 997, 432, 60, 408, 978, 124, 1099, 931, 132, 739, 119, 157, 1164, 840, 912, 133, 847, 298, 440, 443, 801, 416, 449, 108, 35, 384, 800, 218, 635, 182, 421, 1153, 211, 904, 1121, 95, 74, 1094, 905, 383, 830, 1066, 522, 1199, 898, 557, 44, 989, 606, 237, 322, 105, 480, 125, 684, 516, 691, 476, 83, 346, 84, 577, 272, 895, 789, 1049, 964, 1110, 193, 565, 782, 40, 1005, 990, 299, 674, 87, 303, 475, 884, 920, 427, 771, 438, 472, 441, 1063, 781, 922, 1210, 976, 770, 354, 334, 466, 89, 1156, 163, 987, 350, 901, 320, 488, 189, 715, 1091, 879, 615, 127, 952, 451, 1180, 458, 72, 796, 155, 1206, 1183, 1116, 305, 481, 274, 399, 422, 842, 529, 1169, 477, 380, 536, 330, 654, 850, 177, 678, 638, 233, 827, 1031, 794, 924, 154, 752, 589, 370, 868, 607, 501, 112, 525, 1012, 543, 486, 936, 183, 791, 123, 85, 955, 857, 192, 130, 805, 1147, 911, 510, 297, 129, 767, 205, 559, 150, 411, 527, 623, 1124, 381, 311, 1122, 1024, 892, 420, 1073, 685, 361, 1071, 1111, 344, 957, 73, 413, 180, 772, 984, 253, 1161, 251, 754, 512, 511, 1038, 586, 347, 459, 484, 54, 1209, 1138, 726, 841, 889, 966, 926, 430, 220, 626, 1003, 677, 206, 795, 906, 262, 562, 896, 398, 1043, 1214, 1140, 593, 1011, 144, 1143, 673, 1196, 634, 695, 326, 191, 661, 1181, 848, 723, 1002, 391, 683, 540, 448, 549, 603, 315, 245, 627, 883, 203, 890, 282, 934, 337, 1014, 1001, 582, 579, 244, 533, 969, 139, 232, 0, 520, 531, 537, 294, 983, 743, 679, 705, 1193, 221, 975, 1204, 454, 1137, 948, 9, 710, 513, 368, 915, 14, 748, 839, 479, 1163, 769, 1159, 104, 436, 1055, 713, 418, 1057, 1170, 207, 289, 1083, 1076, 640, 844, 867, 736, 417, 1197, 229, 393, 259, 1009, 145, 1025, 786, 201, 619, 175, 483, 447, 338, 4, 1125, 914, 235, 973, 834, 1104, 339, 725, 28, 530, 716, 956, 226, 50, 1152, 1155, 463, 876, 55, 1128, 410, 128, 941, 474, 688, 714, 733, 67, 341, 740, 27, 700, 1139, 317, 57, 863, 377, 199, 1106, 407, 329, 310, 1096, 734, 991, 342, 1089, 821, 930, 746, 281, 699, 621, 762, 564, 1051, 351, 376, 1171, 137, 120, 903, 223, 916, 390, 273, 78, 553, 439, 107, 190, 751, 648, 65, 609, 18, 608, 897, 618, 818, 306, 1184, 1172, 856, 325, 694, 1020, 947, 25, 548, 697, 968, 764, 1047, 1157, 1162, 761, 13, 852, 759, 717, 159, 1175, 711, 395, 1149, 12, 424, 1117, 880, 219, 1119, 521, 881, 148, 596, 909, 833, 1056, 247, 798, 1041, 47, 605, 414, 806, 239, 32, 165, 1120, 940, 1213, 58, 836, 629, 318, 204, 581, 656, 749, 756, 541, 231, 1203, 887, 450, 434, 1205, 864, 375, 571, 331, 721, 652, 369, 362, 602, 665, 170, 918, 1015, 729, 181, 278, 1053, 210, 1032, 250, 526, 625, 1064, 524, 290, 19, 161, 270, 401, 302, 788, 610, 22, 374, 1037, 913, 323, 1190, 1018, 999, 15, 572, 16, 444, 508, 300, 758, 946, 1167, 1, 279, 1022, 213, 140, 1028, 927, 312, 256, 908, 875, 283, 689, 1045, 954, 452, 561, 168, 340, 872, 1195, 1200, 101, 1075, 172, 651, 470, 599, 1006, 1189, 1202, 959, 388, 773, 967, 765, 1013, 404, 349, 675, 23, 923, 1112, 591, 1188, 308, 935, 5, 1198, 523, 859, 972, 142, 921, 69, 632, 249, 598, 36, 425, 1040, 116, 81, 409, 179, 284, 38, 1035, 542, 151, 509, 636, 62, 663, 951, 473, 1007, 1130, 1192, 637, 702, 489, 1182, 1126, 566, 1146, 91, 241, 583, 240, 584, 456, 980, 387, 1050, 1131, 230, 217, 43, 1151, 853, 503, 24, 960, 680, 568, 939, 497, 1158, 587, 768, 787, 823, 21, 29, 738, 492, 1166, 1101, 365, 103, 811, 90, 266, 328, 394, 136, 953, 482, 932, 1033, 925, 39, 720, 594, 1085, 1090, 1023, 550, 670, 622, 80, 666, 471, 630, 271, 866, 832, 464, 658, 532, 692, 797, 578, 910, 945, 560, 1065, 505, 611, 356, 707, 146, 624, 828, 763, 1102, 248, 268, 469, 861, 1098, 774, 644, 186, 332, 727, 70, 184, 360, 173, 745, 1145, 645, 1093, 1008, 551, 402, 224, 1067, 153, 400, 461, 646, 79, 1019, 415, 367, 197, 164, 275, 816, 141, 1034, 815, 200, 92, 994, 777, 301, 544, 208, 855, 460, 48, 775, 364, 499, 264, 378, 676, 545, 26, 287, 366, 457, 236, 877, 288, 187, 313, 321, 837, 539, 353, 465, 196, 437, 1134, 1129, 333, 783, 7, 98, 445, 385, 1212, 977, 6, 345, 82, 494, 215, 985, 900, 633, 462, 406, 1021, 874, 528, 348, 292, 807, 94, 485, 731, 742, 382, 655, 53, 672, 616, 1187, 363, 138, 1095, 870, 242, 386, 110, 943, 753, 760, 735, 1179, 1113, 1127, 355, 100, 357, 696, 1029, 162, 1077, 194, 426, 309, 419, 650, 1173, 59, 722, 965, 405, 792, 143, 64, 937, 682, 156, 902, 744, 335, 1186, 1105, 612, 690, 1069, 126, 871, 8, 819, 592, 49, 31, 1088, 974, 1016, 653, 174, 1048, 258, 829, 894, 730, 97, 504, 1039, 1141, 814, 1107, 358, 917, 490, 435, 986, 1154, 99, 293, 865, 428, 724, 962, 891, 114, 68, 234, 933, 71, 641, 662, 296, 1010, 944, 547, 1062, 570, 979, 285, 802, 552, 228, 2, 766, 554, 1027, 149, 468, 535, 660, 77, 779, 899, 403, 1097, 324, 102, 314, 860, 1109, 246, 255, 517, 667, 1160, 34, 970, 111, 718, 1074, 167, 518, 873, 1030, 198, 995, 580, 507, 604, 227, 590, 442, 1082, 1080, 534, 498, 639, 686, 992, 971, 812, 118, 574, 1004, 750, 269, 431, 681, 597, 1194, 981, 826, 556, 86, 478, 519, 928, 1177, 982, 1087, 1136, 886, 243, 429, 1123, 1052, 37, 1054, 793, 708, 538, 93, 117, 588, 600, 396, 567, 893, 671, 1060, 693, 1081, 487, 757, 942, 843, 846, 929, 166, 216, 195, 188, 88, 790, 160, 949, 709, 907, 1058, 467, 831, 642, 601, 1042, 820, 373, 96, 10, 115, 785, 412, 514, 1118, 813, 359, 286, 455, 741, 993, 1079, 176, 171, 851, 1174, 423, 778, 1070, 988, 858, 134, 185, 1086, 1191, 106, 1108, 260, 225, 585, 573]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4941401439333306
the save name prefix for this run is:  chkpt-ID_4941401439333306_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1093
rank avg (pred): 0.500 +- 0.002
mrr vals (pred, true): 0.019, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001207804

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 928
rank avg (pred): 0.426 +- 0.262
mrr vals (pred, true): 0.098, 0.052
batch losses (mrrl, rdl): 0.0, 7.7508e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1067
rank avg (pred): 0.083 +- 0.067
mrr vals (pred, true): 0.275, 0.405
batch losses (mrrl, rdl): 0.0, 2.0128e-06

Epoch over!
epoch time: 14.883

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 145
rank avg (pred): 0.442 +- 0.265
mrr vals (pred, true): 0.088, 0.053
batch losses (mrrl, rdl): 0.0, 2.5253e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 360
rank avg (pred): 0.437 +- 0.263
mrr vals (pred, true): 0.088, 0.052
batch losses (mrrl, rdl): 0.0, 2.6164e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 986
rank avg (pred): 0.073 +- 0.060
mrr vals (pred, true): 0.291, 0.425
batch losses (mrrl, rdl): 0.0, 4.2093e-06

Epoch over!
epoch time: 14.869

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 992
rank avg (pred): 0.085 +- 0.070
mrr vals (pred, true): 0.276, 0.375
batch losses (mrrl, rdl): 0.0, 1.6585e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 250
rank avg (pred): 0.097 +- 0.081
mrr vals (pred, true): 0.258, 0.343
batch losses (mrrl, rdl): 0.0, 2.2199e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 322
rank avg (pred): 0.153 +- 0.122
mrr vals (pred, true): 0.228, 0.278
batch losses (mrrl, rdl): 0.0, 1.54484e-05

Epoch over!
epoch time: 14.852

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 242
rank avg (pred): 0.444 +- 0.270
mrr vals (pred, true): 0.095, 0.056
batch losses (mrrl, rdl): 0.0, 2.7068e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 741
rank avg (pred): 0.101 +- 0.081
mrr vals (pred, true): 0.251, 0.398
batch losses (mrrl, rdl): 0.0, 9.8857e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 285
rank avg (pred): 0.108 +- 0.090
mrr vals (pred, true): 0.252, 0.277
batch losses (mrrl, rdl): 0.0, 2.73528e-05

Epoch over!
epoch time: 14.871

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 175
rank avg (pred): 0.452 +- 0.261
mrr vals (pred, true): 0.078, 0.050
batch losses (mrrl, rdl): 0.0, 2.0102e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1003
rank avg (pred): 0.434 +- 0.277
mrr vals (pred, true): 0.098, 0.052
batch losses (mrrl, rdl): 0.0, 1.12496e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 206
rank avg (pred): 0.454 +- 0.261
mrr vals (pred, true): 0.073, 0.050
batch losses (mrrl, rdl): 0.0, 2.0606e-06

Epoch over!
epoch time: 14.883

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 229
rank avg (pred): 0.461 +- 0.257
mrr vals (pred, true): 0.068, 0.053
batch losses (mrrl, rdl): 0.0033837256, 2.454e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 33
rank avg (pred): 0.107 +- 0.084
mrr vals (pred, true): 0.282, 0.321
batch losses (mrrl, rdl): 0.0153195485, 4.8749e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 210
rank avg (pred): 0.503 +- 0.163
mrr vals (pred, true): 0.052, 0.052
batch losses (mrrl, rdl): 3.75638e-05, 9.41259e-05

Epoch over!
epoch time: 14.928

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 596
rank avg (pred): 0.492 +- 0.157
mrr vals (pred, true): 0.052, 0.060
batch losses (mrrl, rdl): 5.27561e-05, 7.41966e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1081
rank avg (pred): 0.429 +- 0.119
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 5.24788e-05, 6.47349e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 482
rank avg (pred): 0.474 +- 0.136
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 1.10076e-05, 5.29058e-05

Epoch over!
epoch time: 15.008

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 516
rank avg (pred): 0.230 +- 0.174
mrr vals (pred, true): 0.223, 0.215
batch losses (mrrl, rdl): 0.0005850499, 0.0001871191

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 538
rank avg (pred): 0.216 +- 0.161
mrr vals (pred, true): 0.230, 0.200
batch losses (mrrl, rdl): 0.0089337202, 5.34746e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 765
rank avg (pred): 0.465 +- 0.119
mrr vals (pred, true): 0.045, 0.056
batch losses (mrrl, rdl): 0.0002276388, 5.83217e-05

Epoch over!
epoch time: 15.045

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1184
rank avg (pred): 0.474 +- 0.106
mrr vals (pred, true): 0.041, 0.049
batch losses (mrrl, rdl): 0.0007569976, 5.68221e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 87
rank avg (pred): 0.421 +- 0.111
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001307865, 0.000101316

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 889
rank avg (pred): 0.521 +- 0.138
mrr vals (pred, true): 0.044, 0.059
batch losses (mrrl, rdl): 0.0003799255, 0.0001328682

Epoch over!
epoch time: 15.043

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 175
rank avg (pred): 0.454 +- 0.113
mrr vals (pred, true): 0.038, 0.050
batch losses (mrrl, rdl): 0.001541754, 5.14013e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 857
rank avg (pred): 0.477 +- 0.110
mrr vals (pred, true): 0.042, 0.053
batch losses (mrrl, rdl): 0.0006180574, 6.98676e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 707
rank avg (pred): 0.469 +- 0.118
mrr vals (pred, true): 0.046, 0.053
batch losses (mrrl, rdl): 0.0001666167, 5.56064e-05

Epoch over!
epoch time: 15.008

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 665
rank avg (pred): 0.423 +- 0.096
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 0.0001145323, 0.0001031938

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1096
rank avg (pred): 0.427 +- 0.108
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 8.4271e-06, 8.0962e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1188
rank avg (pred): 0.434 +- 0.086
mrr vals (pred, true): 0.044, 0.056
batch losses (mrrl, rdl): 0.0003033226, 8.15123e-05

Epoch over!
epoch time: 15.037

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 656
rank avg (pred): 0.434 +- 0.096
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 2.79668e-05, 8.91601e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 247
rank avg (pred): 0.077 +- 0.058
mrr vals (pred, true): 0.346, 0.334
batch losses (mrrl, rdl): 0.001523413, 1.88796e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 688
rank avg (pred): 0.428 +- 0.128
mrr vals (pred, true): 0.055, 0.052
batch losses (mrrl, rdl): 0.0002066142, 8.13065e-05

Epoch over!
epoch time: 15.032

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 312
rank avg (pred): 0.183 +- 0.130
mrr vals (pred, true): 0.258, 0.272
batch losses (mrrl, rdl): 0.0020637256, 4.62843e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 759
rank avg (pred): 0.436 +- 0.120
mrr vals (pred, true): 0.056, 0.055
batch losses (mrrl, rdl): 0.0003826684, 8.93774e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 820
rank avg (pred): 0.029 +- 0.022
mrr vals (pred, true): 0.447, 0.430
batch losses (mrrl, rdl): 0.0029337113, 4.31802e-05

Epoch over!
epoch time: 15.025

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 418
rank avg (pred): 0.425 +- 0.106
mrr vals (pred, true): 0.057, 0.059
batch losses (mrrl, rdl): 0.0004934162, 9.44185e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 916
rank avg (pred): 0.343 +- 0.230
mrr vals (pred, true): 0.202, 0.162
batch losses (mrrl, rdl): 0.0157112125, 0.0004695485

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 81
rank avg (pred): 0.440 +- 0.102
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 4.5811e-06, 7.78561e-05

Epoch over!
epoch time: 15.041

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 347
rank avg (pred): 0.484 +- 0.183
mrr vals (pred, true): 0.039, 0.060
batch losses (mrrl, rdl): 0.0012201173, 2.82807e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 785
rank avg (pred): 0.454 +- 0.116
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 6.40579e-05, 5.02487e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 201
rank avg (pred): 0.429 +- 0.163
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 9e-10, 5.37947e-05

Epoch over!
epoch time: 15.04

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.172 +- 0.124
mrr vals (pred, true): 0.263, 0.290

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   41 	     0 	 0.04222 	 0.04603 	 ~...
   37 	     1 	 0.04193 	 0.04751 	 ~...
   55 	     2 	 0.04314 	 0.04772 	 ~...
   64 	     3 	 0.04442 	 0.04823 	 ~...
   46 	     4 	 0.04248 	 0.04882 	 ~...
    5 	     5 	 0.03818 	 0.04889 	 ~...
   22 	     6 	 0.04124 	 0.04910 	 ~...
   67 	     7 	 0.04521 	 0.04948 	 ~...
   27 	     8 	 0.04167 	 0.04971 	 ~...
   13 	     9 	 0.03946 	 0.04973 	 ~...
   25 	    10 	 0.04137 	 0.04998 	 ~...
   49 	    11 	 0.04275 	 0.05022 	 ~...
   17 	    12 	 0.04003 	 0.05038 	 ~...
    2 	    13 	 0.03626 	 0.05107 	 ~...
   50 	    14 	 0.04279 	 0.05113 	 ~...
   51 	    15 	 0.04281 	 0.05128 	 ~...
   29 	    16 	 0.04170 	 0.05129 	 ~...
   45 	    17 	 0.04244 	 0.05136 	 ~...
   42 	    18 	 0.04231 	 0.05140 	 ~...
   58 	    19 	 0.04391 	 0.05189 	 ~...
   12 	    20 	 0.03941 	 0.05191 	 ~...
   54 	    21 	 0.04296 	 0.05211 	 ~...
   26 	    22 	 0.04143 	 0.05212 	 ~...
   48 	    23 	 0.04263 	 0.05219 	 ~...
   35 	    24 	 0.04187 	 0.05235 	 ~...
   69 	    25 	 0.04647 	 0.05241 	 ~...
    1 	    26 	 0.03580 	 0.05269 	 ~...
   30 	    27 	 0.04172 	 0.05282 	 ~...
   33 	    28 	 0.04182 	 0.05286 	 ~...
   31 	    29 	 0.04173 	 0.05289 	 ~...
   36 	    30 	 0.04193 	 0.05293 	 ~...
   52 	    31 	 0.04285 	 0.05325 	 ~...
   16 	    32 	 0.03996 	 0.05331 	 ~...
   11 	    33 	 0.03932 	 0.05357 	 ~...
   24 	    34 	 0.04132 	 0.05363 	 ~...
    6 	    35 	 0.03818 	 0.05373 	 ~...
   21 	    36 	 0.04121 	 0.05376 	 ~...
   34 	    37 	 0.04186 	 0.05380 	 ~...
   65 	    38 	 0.04482 	 0.05385 	 ~...
   61 	    39 	 0.04404 	 0.05397 	 ~...
    8 	    40 	 0.03852 	 0.05401 	 ~...
   68 	    41 	 0.04558 	 0.05417 	 ~...
    7 	    42 	 0.03842 	 0.05428 	 ~...
   14 	    43 	 0.03952 	 0.05435 	 ~...
   56 	    44 	 0.04381 	 0.05438 	 ~...
   38 	    45 	 0.04194 	 0.05439 	 ~...
   32 	    46 	 0.04181 	 0.05444 	 ~...
   60 	    47 	 0.04400 	 0.05459 	 ~...
   39 	    48 	 0.04198 	 0.05466 	 ~...
   44 	    49 	 0.04243 	 0.05494 	 ~...
   62 	    50 	 0.04424 	 0.05501 	 ~...
    9 	    51 	 0.03913 	 0.05509 	 ~...
   70 	    52 	 0.04683 	 0.05510 	 ~...
   18 	    53 	 0.04038 	 0.05520 	 ~...
   53 	    54 	 0.04291 	 0.05528 	 ~...
   19 	    55 	 0.04057 	 0.05529 	 ~...
   72 	    56 	 0.04900 	 0.05593 	 ~...
   63 	    57 	 0.04431 	 0.05609 	 ~...
   10 	    58 	 0.03918 	 0.05632 	 ~...
    3 	    59 	 0.03653 	 0.05650 	 ~...
    4 	    60 	 0.03785 	 0.05689 	 ~...
   71 	    61 	 0.04761 	 0.05693 	 ~...
   57 	    62 	 0.04385 	 0.05723 	 ~...
   23 	    63 	 0.04132 	 0.05733 	 ~...
   59 	    64 	 0.04395 	 0.05769 	 ~...
   20 	    65 	 0.04091 	 0.05874 	 ~...
   28 	    66 	 0.04167 	 0.05900 	 ~...
   43 	    67 	 0.04241 	 0.05902 	 ~...
    0 	    68 	 0.03513 	 0.05918 	 ~...
   40 	    69 	 0.04201 	 0.05964 	 ~...
   66 	    70 	 0.04494 	 0.05988 	 ~...
   15 	    71 	 0.03971 	 0.06033 	 ~...
   47 	    72 	 0.04254 	 0.06229 	 ~...
  109 	    73 	 0.31402 	 0.14588 	 MISS
  110 	    74 	 0.32773 	 0.16633 	 MISS
   75 	    75 	 0.18424 	 0.18244 	 ~...
   73 	    76 	 0.17588 	 0.18561 	 ~...
   74 	    77 	 0.18023 	 0.19508 	 ~...
   77 	    78 	 0.19790 	 0.21822 	 ~...
   79 	    79 	 0.23245 	 0.21861 	 ~...
   81 	    80 	 0.23340 	 0.22622 	 ~...
   78 	    81 	 0.22857 	 0.23291 	 ~...
   76 	    82 	 0.19717 	 0.23297 	 m..s
   80 	    83 	 0.23306 	 0.23875 	 ~...
   87 	    84 	 0.25094 	 0.24979 	 ~...
   85 	    85 	 0.24895 	 0.25490 	 ~...
   82 	    86 	 0.24682 	 0.25657 	 ~...
   84 	    87 	 0.24849 	 0.26316 	 ~...
   99 	    88 	 0.27519 	 0.26647 	 ~...
   83 	    89 	 0.24817 	 0.26997 	 ~...
   97 	    90 	 0.27441 	 0.27259 	 ~...
   89 	    91 	 0.26094 	 0.27645 	 ~...
   98 	    92 	 0.27443 	 0.28083 	 ~...
   91 	    93 	 0.26872 	 0.28148 	 ~...
  108 	    94 	 0.30650 	 0.28230 	 ~...
   86 	    95 	 0.25045 	 0.28388 	 m..s
   92 	    96 	 0.27275 	 0.28459 	 ~...
  104 	    97 	 0.29511 	 0.28520 	 ~...
  103 	    98 	 0.29478 	 0.28969 	 ~...
   90 	    99 	 0.26300 	 0.29046 	 ~...
   93 	   100 	 0.27322 	 0.29123 	 ~...
  101 	   101 	 0.28501 	 0.29390 	 ~...
   96 	   102 	 0.27427 	 0.29459 	 ~...
  100 	   103 	 0.27883 	 0.30449 	 ~...
   94 	   104 	 0.27368 	 0.30510 	 m..s
   88 	   105 	 0.25631 	 0.30571 	 m..s
  105 	   106 	 0.29595 	 0.31389 	 ~...
   95 	   107 	 0.27424 	 0.32389 	 m..s
  107 	   108 	 0.30549 	 0.34025 	 m..s
  106 	   109 	 0.29950 	 0.35135 	 m..s
  111 	   110 	 0.34596 	 0.35605 	 ~...
  112 	   111 	 0.36453 	 0.35834 	 ~...
  102 	   112 	 0.28579 	 0.36586 	 m..s
  113 	   113 	 0.38348 	 0.42762 	 m..s
  114 	   114 	 0.44143 	 0.45935 	 ~...
  117 	   115 	 0.46436 	 0.46828 	 ~...
  116 	   116 	 0.46345 	 0.48000 	 ~...
  119 	   117 	 0.48740 	 0.49577 	 ~...
  115 	   118 	 0.44390 	 0.51145 	 m..s
  118 	   119 	 0.47587 	 0.62595 	 MISS
  120 	   120 	 0.55845 	 0.62874 	 m..s
==========================================
r_mrr = 0.9781723022460938
r2_mrr = 0.9490286111831665
spearmanr_mrr@5 = 0.7788113355636597
spearmanr_mrr@10 = 0.9320610165596008
spearmanr_mrr@50 = 0.9866631031036377
spearmanr_mrr@100 = 0.9939015507698059
spearmanr_mrr@All = 0.994565486907959
==========================================
test time: 0.455
Done Testing dataset Kinships
total time taken: 232.80927801132202
training time taken: 225.03243970870972
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9782)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9490)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.7788)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9321)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9867)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9939)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9946)}}, 'test_loss': {'ComplEx': {'Kinships': 1.2026993396757462}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 9858336502325712
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [669, 902, 1097, 203, 713, 906, 518, 192, 608, 315, 1199, 391, 71, 875, 561, 501, 860, 486, 724, 1063, 722, 149, 1212, 502, 579, 631, 1075, 411, 202, 1057, 92, 265, 793, 781, 924, 765, 842, 858, 1117, 488, 225, 56, 616, 605, 434, 178, 673, 69, 309, 487, 297, 215, 206, 901, 218, 1038, 310, 1109, 721, 507, 459, 29, 878, 95, 1125, 515, 566, 460, 755, 195, 967, 868, 278, 792, 437, 657, 907, 954, 370, 670, 600, 876, 861, 83, 830, 469, 201, 412, 426, 895, 466, 1163, 963, 700, 144, 745, 809, 395, 814, 267, 828, 464, 243, 893, 462, 934, 946, 942, 645, 98, 142, 485, 1011, 1072, 472, 153, 748, 1150, 372, 30, 768]
valid_ids (0): []
train_ids (1094): [870, 899, 743, 692, 877, 448, 183, 815, 603, 774, 408, 479, 266, 14, 270, 163, 1160, 343, 523, 800, 1197, 898, 834, 879, 1034, 498, 617, 483, 984, 357, 1012, 1053, 24, 1171, 602, 286, 1052, 970, 232, 1189, 23, 275, 332, 799, 969, 950, 953, 449, 402, 806, 689, 513, 1149, 49, 885, 965, 42, 574, 628, 1143, 156, 709, 606, 188, 443, 1062, 543, 569, 549, 827, 551, 918, 496, 87, 222, 558, 582, 126, 846, 396, 429, 346, 1156, 362, 941, 504, 1136, 711, 1030, 5, 1067, 398, 481, 1024, 554, 474, 478, 880, 480, 161, 750, 1152, 935, 919, 2, 634, 982, 347, 445, 575, 340, 921, 1095, 511, 734, 703, 1081, 1180, 625, 1146, 604, 9, 1015, 1201, 1008, 116, 550, 720, 679, 832, 913, 1157, 714, 674, 307, 399, 761, 312, 678, 980, 304, 939, 118, 185, 824, 651, 512, 471, 923, 74, 263, 299, 430, 510, 912, 291, 88, 848, 620, 699, 650, 268, 432, 205, 955, 1035, 306, 4, 1124, 1004, 1089, 756, 50, 84, 695, 60, 936, 305, 335, 938, 859, 422, 123, 1202, 991, 1209, 593, 509, 311, 811, 331, 122, 820, 114, 843, 764, 138, 117, 476, 247, 903, 995, 482, 668, 500, 237, 612, 302, 53, 21, 294, 663, 998, 420, 562, 255, 889, 945, 572, 624, 327, 567, 1166, 1019, 1059, 719, 588, 986, 1168, 130, 872, 18, 613, 647, 896, 1006, 406, 216, 129, 882, 1190, 1003, 660, 1086, 388, 102, 694, 390, 590, 12, 580, 182, 837, 548, 1040, 696, 121, 254, 491, 922, 1206, 51, 367, 1056, 214, 1014, 345, 1179, 160, 497, 184, 1016, 319, 470, 702, 1042, 32, 869, 637, 57, 687, 1069, 55, 850, 988, 855, 1064, 823, 981, 1126, 654, 221, 968, 622, 990, 517, 1093, 38, 900, 704, 556, 63, 1214, 355, 667, 423, 808, 17, 379, 1147, 979, 169, 829, 72, 888, 971, 147, 145, 47, 1107, 812, 492, 839, 1151, 1060, 1017, 1049, 973, 65, 409, 905, 1174, 150, 235, 977, 863, 818, 1087, 997, 494, 931, 552, 1065, 1001, 256, 405, 1158, 1043, 547, 112, 13, 317, 174, 397, 104, 1205, 328, 26, 752, 228, 619, 585, 929, 797, 382, 61, 1007, 531, 926, 966, 508, 717, 181, 68, 887, 167, 538, 961, 801, 1113, 200, 521, 1144, 15, 441, 454, 289, 1025, 753, 760, 911, 1023, 959, 1132, 1196, 648, 48, 248, 1133, 707, 716, 97, 690, 366, 854, 323, 630, 639, 693, 665, 822, 394, 447, 640, 833, 1031, 34, 686, 28, 380, 1140, 140, 641, 1137, 164, 524, 136, 360, 656, 723, 344, 1118, 300, 1200, 1068, 27, 659, 1208, 350, 37, 1029, 363, 199, 1164, 1111, 826, 555, 632, 1120, 177, 279, 762, 77, 175, 996, 601, 1002, 1047, 573, 993, 1013, 975, 1092, 1198, 168, 36, 537, 303, 64, 577, 473, 635, 733, 281, 546, 985, 318, 728, 1080, 739, 751, 179, 1203, 428, 207, 1128, 949, 416, 952, 1050, 1055, 769, 100, 134, 359, 223, 599, 847, 614, 457, 607, 726, 333, 621, 227, 994, 269, 308, 1195, 514, 587, 103, 992, 789, 1175, 831, 1058, 414, 819, 892, 76, 239, 1121, 280, 1073, 782, 962, 701, 1061, 285, 948, 727, 598, 111, 718, 576, 381, 204, 1142, 940, 784, 146, 570, 532, 909, 356, 298, 11, 463, 45, 529, 377, 339, 1108, 1114, 475, 770, 410, 386, 749, 987, 85, 1018, 320, 871, 712, 817, 1022, 1182, 240, 874, 856, 807, 93, 638, 1032, 932, 224, 143, 341, 1103, 530, 626, 533, 964, 754, 920, 1210, 1041, 535, 106, 794, 1186, 385, 477, 219, 1167, 708, 86, 883, 226, 503, 453, 383, 506, 208, 737, 115, 771, 257, 245, 705, 70, 1194, 788, 795, 776, 132, 732, 729, 786, 220, 465, 780, 1123, 862, 908, 43, 425, 545, 39, 1110, 66, 615, 1161, 455, 1054, 1066, 611, 539, 337, 1187, 736, 779, 450, 1102, 1071, 231, 44, 119, 1094, 623, 804, 99, 542, 418, 387, 241, 1122, 461, 544, 260, 1207, 435, 958, 1099, 401, 1005, 564, 677, 1009, 857, 35, 59, 917, 933, 522, 1115, 91, 519, 54, 568, 1155, 353, 1173, 400, 1076, 1172, 1105, 681, 775, 864, 1048, 790, 125, 629, 277, 253, 609, 292, 419, 1010, 904, 1193, 415, 189, 1165, 886, 928, 944, 375, 351, 671, 661, 490, 565, 698, 664, 951, 1169, 421, 1159, 796, 747, 960, 259, 1176, 284, 595, 643, 1083, 155, 213, 338, 683, 676, 915, 162, 433, 436, 1039, 943, 326, 493, 772, 198, 738, 983, 282, 322, 1036, 691, 389, 1085, 1138, 252, 79, 1192, 652, 133, 778, 374, 499, 209, 293, 413, 1037, 427, 364, 1082, 1027, 1116, 314, 1026, 851, 31, 378, 742, 384, 1112, 440, 276, 894, 107, 816, 589, 321, 844, 658, 1045, 1070, 246, 316, 528, 1145, 777, 873, 516, 1020, 444, 1000, 417, 348, 1088, 1129, 75, 526, 685, 290, 947, 194, 1046, 810, 261, 890, 6, 8, 821, 586, 706, 120, 1183, 190, 358, 937, 594, 866, 697, 1101, 16, 1106, 110, 158, 1170, 791, 525, 557, 766, 249, 210, 881, 352, 62, 578, 897, 193, 196, 141, 757, 151, 197, 1211, 217, 1021, 655, 242, 1074, 78, 313, 258, 1091, 1077, 759, 113, 1188, 1104, 135, 1028, 489, 730, 541, 1134, 90, 58, 744, 914, 458, 369, 295, 803, 20, 646, 283, 105, 19, 715, 581, 725, 767, 81, 186, 1084, 956, 930, 1191, 735, 740, 927, 439, 584, 627, 244, 288, 468, 731, 3, 392, 999, 336, 1178, 1100, 233, 972, 989, 271, 1153, 52, 325, 324, 746, 108, 139, 636, 7, 211, 368, 234, 424, 925, 82, 22, 838, 349, 301, 451, 853, 171, 741, 484, 666, 505, 373, 137, 840, 10, 1162, 456, 152, 527, 33, 841, 559, 978, 553, 802, 371, 236, 1033, 642, 1130, 644, 1096, 852, 672, 773, 849, 296, 976, 212, 835, 1135, 1177, 495, 25, 407, 262, 73, 540, 404, 675, 124, 89, 273, 1204, 1098, 571, 758, 365, 618, 798, 610, 452, 287, 159, 393, 597, 264, 1119, 910, 446, 230, 891, 128, 633, 682, 127, 1127, 591, 1, 361, 191, 0, 520, 865, 1148, 131, 67, 354, 957, 251, 680, 867, 154, 274, 534, 172, 46, 109, 41, 166, 688, 1044, 825, 229, 176, 442, 836, 165, 653, 1139, 763, 157, 330, 974, 438, 560, 467, 101, 1131, 1079, 805, 329, 148, 334, 80, 649, 1154, 813, 170, 180, 563, 596, 884, 238, 250, 376, 536, 710, 173, 1181, 1051, 1185, 96, 592, 1184, 684, 845, 662, 787, 783, 403, 785, 916, 94, 1090, 583, 1078, 272, 431, 1213, 187, 40, 342, 1141]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2738235999805460
the save name prefix for this run is:  chkpt-ID_2738235999805460_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1110
rank avg (pred): 0.534 +- 0.003
mrr vals (pred, true): 0.018, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001495515

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 778
rank avg (pred): 0.442 +- 0.276
mrr vals (pred, true): 0.114, 0.055
batch losses (mrrl, rdl): 0.0, 3.3277e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 345
rank avg (pred): 0.448 +- 0.254
mrr vals (pred, true): 0.083, 0.052
batch losses (mrrl, rdl): 0.0, 6.5448e-06

Epoch over!
epoch time: 14.843

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 461
rank avg (pred): 0.457 +- 0.261
mrr vals (pred, true): 0.081, 0.052
batch losses (mrrl, rdl): 0.0, 3.724e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1174
rank avg (pred): 0.457 +- 0.271
mrr vals (pred, true): 0.068, 0.053
batch losses (mrrl, rdl): 0.0, 2.5933e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 0
rank avg (pred): 0.328 +- 0.240
mrr vals (pred, true): 0.104, 0.316
batch losses (mrrl, rdl): 0.0, 0.001167539

Epoch over!
epoch time: 14.835

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1200
rank avg (pred): 0.463 +- 0.266
mrr vals (pred, true): 0.061, 0.060
batch losses (mrrl, rdl): 0.0, 3.1158e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 735
rank avg (pred): 0.127 +- 0.111
mrr vals (pred, true): 0.222, 0.185
batch losses (mrrl, rdl): 0.0, 2.23227e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 174
rank avg (pred): 0.456 +- 0.265
mrr vals (pred, true): 0.059, 0.059
batch losses (mrrl, rdl): 0.0, 5.374e-07

Epoch over!
epoch time: 14.847

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 840
rank avg (pred): 0.461 +- 0.266
mrr vals (pred, true): 0.053, 0.057
batch losses (mrrl, rdl): 0.0, 2.774e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 801
rank avg (pred): 0.451 +- 0.283
mrr vals (pred, true): 0.066, 0.059
batch losses (mrrl, rdl): 0.0, 2.6775e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1091
rank avg (pred): 0.450 +- 0.273
mrr vals (pred, true): 0.075, 0.058
batch losses (mrrl, rdl): 0.0, 2.253e-07

Epoch over!
epoch time: 14.848

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 911
rank avg (pred): 0.099 +- 0.102
mrr vals (pred, true): 0.301, 0.247
batch losses (mrrl, rdl): 0.0, 2.4212e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 784
rank avg (pred): 0.450 +- 0.276
mrr vals (pred, true): 0.070, 0.052
batch losses (mrrl, rdl): 0.0, 3.2512e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 816
rank avg (pred): 0.114 +- 0.129
mrr vals (pred, true): 0.321, 0.159
batch losses (mrrl, rdl): 0.0, 0.000111678

Epoch over!
epoch time: 14.852

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 569
rank avg (pred): 0.462 +- 0.259
mrr vals (pred, true): 0.058, 0.053
batch losses (mrrl, rdl): 0.0007073968, 3.04e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 393
rank avg (pred): 0.438 +- 0.137
mrr vals (pred, true): 0.052, 0.053
batch losses (mrrl, rdl): 2.96159e-05, 6.47835e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 91
rank avg (pred): 0.463 +- 0.108
mrr vals (pred, true): 0.038, 0.054
batch losses (mrrl, rdl): 0.0014264942, 5.56804e-05

Epoch over!
epoch time: 15.046

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 564
rank avg (pred): 0.288 +- 0.222
mrr vals (pred, true): 0.231, 0.205
batch losses (mrrl, rdl): 0.0068915118, 0.0003766335

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 490
rank avg (pred): 0.273 +- 0.203
mrr vals (pred, true): 0.228, 0.247
batch losses (mrrl, rdl): 0.0035914939, 0.0005405896

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1166
rank avg (pred): 0.460 +- 0.112
mrr vals (pred, true): 0.044, 0.047
batch losses (mrrl, rdl): 0.0003814218, 5.84617e-05

Epoch over!
epoch time: 15.238

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 543
rank avg (pred): 0.274 +- 0.204
mrr vals (pred, true): 0.235, 0.227
batch losses (mrrl, rdl): 0.0006847862, 0.0004165504

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 970
rank avg (pred): 0.454 +- 0.119
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 7.5e-09, 5.86778e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1142
rank avg (pred): 0.121 +- 0.101
mrr vals (pred, true): 0.321, 0.306
batch losses (mrrl, rdl): 0.0023699456, 9.8142e-06

Epoch over!
epoch time: 15.082

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 615
rank avg (pred): 0.443 +- 0.134
mrr vals (pred, true): 0.063, 0.050
batch losses (mrrl, rdl): 0.0017665755, 5.21752e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1180
rank avg (pred): 0.470 +- 0.092
mrr vals (pred, true): 0.032, 0.053
batch losses (mrrl, rdl): 0.003224974, 6.92472e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 181
rank avg (pred): 0.464 +- 0.105
mrr vals (pred, true): 0.041, 0.056
batch losses (mrrl, rdl): 0.0008518099, 6.16372e-05

Epoch over!
epoch time: 15.041

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 147
rank avg (pred): 0.453 +- 0.118
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 1.31909e-05, 6.8452e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 212
rank avg (pred): 0.442 +- 0.127
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0001451996, 5.36305e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 306
rank avg (pred): 0.169 +- 0.130
mrr vals (pred, true): 0.255, 0.293
batch losses (mrrl, rdl): 0.0144387446, 3.92839e-05

Epoch over!
epoch time: 15.007

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1168
rank avg (pred): 0.440 +- 0.129
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0003780554, 5.13864e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1179
rank avg (pred): 0.455 +- 0.108
mrr vals (pred, true): 0.044, 0.056
batch losses (mrrl, rdl): 0.0004052078, 5.87794e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 418
rank avg (pred): 0.453 +- 0.115
mrr vals (pred, true): 0.048, 0.059
batch losses (mrrl, rdl): 4.56668e-05, 5.94673e-05

Epoch over!
epoch time: 15.059

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 161
rank avg (pred): 0.449 +- 0.113
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 5.43e-05, 5.22452e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 710
rank avg (pred): 0.447 +- 0.120
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 7.888e-07, 6.39009e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 756
rank avg (pred): 0.436 +- 0.132
mrr vals (pred, true): 0.057, 0.058
batch losses (mrrl, rdl): 0.000491734, 5.39089e-05

Epoch over!
epoch time: 15.023

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1062
rank avg (pred): 0.063 +- 0.058
mrr vals (pred, true): 0.404, 0.358
batch losses (mrrl, rdl): 0.0212377496, 2.9885e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 343
rank avg (pred): 0.443 +- 0.114
mrr vals (pred, true): 0.046, 0.057
batch losses (mrrl, rdl): 0.0001652035, 6.42578e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 369
rank avg (pred): 0.448 +- 0.115
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 1.48909e-05, 5.93238e-05

Epoch over!
epoch time: 15.017

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 598
rank avg (pred): 0.441 +- 0.121
mrr vals (pred, true): 0.052, 0.058
batch losses (mrrl, rdl): 2.51311e-05, 5.40778e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 28
rank avg (pred): 0.157 +- 0.141
mrr vals (pred, true): 0.311, 0.318
batch losses (mrrl, rdl): 0.0004892283, 7.92243e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 207
rank avg (pred): 0.452 +- 0.108
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.0002224766, 6.62531e-05

Epoch over!
epoch time: 15.001

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 614
rank avg (pred): 0.434 +- 0.128
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001654097, 6.02824e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1014
rank avg (pred): 0.430 +- 0.130
mrr vals (pred, true): 0.058, 0.056
batch losses (mrrl, rdl): 0.0006128034, 5.47553e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 969
rank avg (pred): 0.451 +- 0.106
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003951322, 6.26626e-05

Epoch over!
epoch time: 15.079

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.436 +- 0.126
mrr vals (pred, true): 0.053, 0.054

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.05162 	 0.04717 	 ~...
   80 	     1 	 0.05805 	 0.04830 	 ~...
   54 	     2 	 0.05414 	 0.04852 	 ~...
   44 	     3 	 0.05399 	 0.04858 	 ~...
   31 	     4 	 0.05341 	 0.04863 	 ~...
   35 	     5 	 0.05358 	 0.04863 	 ~...
   57 	     6 	 0.05427 	 0.04891 	 ~...
   46 	     7 	 0.05403 	 0.04899 	 ~...
   49 	     8 	 0.05409 	 0.04922 	 ~...
   55 	     9 	 0.05415 	 0.04945 	 ~...
   22 	    10 	 0.05304 	 0.04994 	 ~...
   53 	    11 	 0.05413 	 0.05021 	 ~...
   34 	    12 	 0.05353 	 0.05035 	 ~...
   39 	    13 	 0.05370 	 0.05050 	 ~...
   47 	    14 	 0.05408 	 0.05051 	 ~...
   36 	    15 	 0.05362 	 0.05058 	 ~...
   38 	    16 	 0.05369 	 0.05059 	 ~...
   37 	    17 	 0.05363 	 0.05068 	 ~...
   40 	    18 	 0.05379 	 0.05085 	 ~...
   67 	    19 	 0.05486 	 0.05086 	 ~...
    4 	    20 	 0.05139 	 0.05102 	 ~...
    0 	    21 	 0.05013 	 0.05109 	 ~...
   78 	    22 	 0.05699 	 0.05112 	 ~...
   71 	    23 	 0.05509 	 0.05125 	 ~...
   23 	    24 	 0.05309 	 0.05151 	 ~...
   29 	    25 	 0.05330 	 0.05152 	 ~...
   62 	    26 	 0.05455 	 0.05168 	 ~...
   30 	    27 	 0.05333 	 0.05168 	 ~...
   21 	    28 	 0.05299 	 0.05194 	 ~...
    3 	    29 	 0.05103 	 0.05207 	 ~...
   72 	    30 	 0.05510 	 0.05209 	 ~...
   65 	    31 	 0.05476 	 0.05210 	 ~...
    9 	    32 	 0.05234 	 0.05211 	 ~...
   79 	    33 	 0.05751 	 0.05248 	 ~...
   59 	    34 	 0.05440 	 0.05311 	 ~...
   61 	    35 	 0.05452 	 0.05312 	 ~...
   56 	    36 	 0.05420 	 0.05313 	 ~...
   24 	    37 	 0.05310 	 0.05331 	 ~...
    6 	    38 	 0.05166 	 0.05338 	 ~...
   73 	    39 	 0.05511 	 0.05356 	 ~...
   12 	    40 	 0.05252 	 0.05357 	 ~...
   70 	    41 	 0.05494 	 0.05370 	 ~...
   20 	    42 	 0.05297 	 0.05376 	 ~...
   58 	    43 	 0.05433 	 0.05380 	 ~...
   74 	    44 	 0.05514 	 0.05385 	 ~...
    7 	    45 	 0.05172 	 0.05392 	 ~...
   19 	    46 	 0.05286 	 0.05433 	 ~...
   25 	    47 	 0.05313 	 0.05451 	 ~...
   14 	    48 	 0.05253 	 0.05455 	 ~...
    2 	    49 	 0.05062 	 0.05472 	 ~...
   77 	    50 	 0.05619 	 0.05476 	 ~...
   28 	    51 	 0.05322 	 0.05481 	 ~...
   41 	    52 	 0.05384 	 0.05487 	 ~...
   76 	    53 	 0.05517 	 0.05500 	 ~...
   27 	    54 	 0.05318 	 0.05507 	 ~...
   64 	    55 	 0.05463 	 0.05510 	 ~...
   17 	    56 	 0.05259 	 0.05526 	 ~...
   16 	    57 	 0.05257 	 0.05529 	 ~...
   52 	    58 	 0.05413 	 0.05540 	 ~...
   15 	    59 	 0.05253 	 0.05541 	 ~...
   50 	    60 	 0.05409 	 0.05545 	 ~...
    1 	    61 	 0.05043 	 0.05557 	 ~...
   75 	    62 	 0.05516 	 0.05605 	 ~...
   42 	    63 	 0.05389 	 0.05631 	 ~...
   45 	    64 	 0.05400 	 0.05653 	 ~...
    8 	    65 	 0.05218 	 0.05654 	 ~...
   60 	    66 	 0.05447 	 0.05664 	 ~...
   69 	    67 	 0.05486 	 0.05748 	 ~...
   66 	    68 	 0.05478 	 0.05777 	 ~...
   63 	    69 	 0.05455 	 0.05782 	 ~...
   43 	    70 	 0.05394 	 0.05825 	 ~...
   26 	    71 	 0.05314 	 0.05863 	 ~...
   33 	    72 	 0.05346 	 0.05865 	 ~...
   11 	    73 	 0.05240 	 0.05887 	 ~...
   32 	    74 	 0.05345 	 0.05906 	 ~...
   18 	    75 	 0.05281 	 0.05969 	 ~...
   10 	    76 	 0.05239 	 0.06018 	 ~...
   51 	    77 	 0.05410 	 0.06181 	 ~...
   68 	    78 	 0.05486 	 0.06229 	 ~...
   13 	    79 	 0.05252 	 0.06240 	 ~...
   48 	    80 	 0.05409 	 0.06253 	 ~...
   81 	    81 	 0.22221 	 0.16878 	 m..s
   81 	    82 	 0.22221 	 0.16879 	 m..s
   81 	    83 	 0.22221 	 0.19518 	 ~...
   81 	    84 	 0.22221 	 0.20462 	 ~...
   81 	    85 	 0.22221 	 0.21993 	 ~...
   86 	    86 	 0.23114 	 0.22414 	 ~...
   87 	    87 	 0.23579 	 0.22567 	 ~...
   88 	    88 	 0.23939 	 0.22622 	 ~...
   91 	    89 	 0.24429 	 0.22952 	 ~...
   90 	    90 	 0.24239 	 0.23291 	 ~...
   89 	    91 	 0.24235 	 0.23853 	 ~...
   93 	    92 	 0.26908 	 0.24886 	 ~...
   94 	    93 	 0.27235 	 0.26043 	 ~...
   92 	    94 	 0.26264 	 0.26361 	 ~...
   95 	    95 	 0.29523 	 0.27241 	 ~...
   97 	    96 	 0.30105 	 0.27694 	 ~...
   96 	    97 	 0.29835 	 0.27924 	 ~...
  103 	    98 	 0.31543 	 0.28459 	 m..s
   99 	    99 	 0.30484 	 0.30230 	 ~...
  105 	   100 	 0.33162 	 0.30284 	 ~...
  100 	   101 	 0.31013 	 0.30412 	 ~...
  104 	   102 	 0.32006 	 0.30449 	 ~...
  110 	   103 	 0.34579 	 0.31094 	 m..s
  106 	   104 	 0.33416 	 0.31793 	 ~...
  102 	   105 	 0.31296 	 0.32386 	 ~...
   98 	   106 	 0.30201 	 0.32389 	 ~...
  101 	   107 	 0.31014 	 0.33847 	 ~...
  107 	   108 	 0.33898 	 0.34025 	 ~...
  108 	   109 	 0.33927 	 0.34389 	 ~...
  109 	   110 	 0.34241 	 0.35160 	 ~...
  112 	   111 	 0.41407 	 0.35834 	 m..s
  114 	   112 	 0.41431 	 0.37130 	 m..s
  115 	   113 	 0.41705 	 0.40488 	 ~...
  116 	   114 	 0.42460 	 0.41066 	 ~...
  113 	   115 	 0.41416 	 0.41166 	 ~...
  118 	   116 	 0.43908 	 0.42826 	 ~...
  117 	   117 	 0.42718 	 0.44758 	 ~...
  119 	   118 	 0.44947 	 0.47946 	 ~...
  111 	   119 	 0.38664 	 0.48105 	 m..s
  120 	   120 	 0.57552 	 0.62311 	 m..s
==========================================
r_mrr = 0.9923837184906006
r2_mrr = 0.9843323826789856
spearmanr_mrr@5 = 0.986804187297821
spearmanr_mrr@10 = 0.9611734747886658
spearmanr_mrr@50 = 0.9903928637504578
spearmanr_mrr@100 = 0.9955028891563416
spearmanr_mrr@All = 0.9959290027618408
==========================================
test time: 0.458
Done Testing dataset Kinships
total time taken: 233.1505012512207
training time taken: 225.2881624698639
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9924)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9843)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9868)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9612)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9904)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9955)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9959)}}, 'test_loss': {'ComplEx': {'Kinships': 0.3511129142571008}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 6359678256679415
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [156, 391, 36, 1155, 121, 1187, 999, 1034, 1181, 324, 188, 671, 462, 944, 611, 297, 1180, 982, 496, 447, 419, 1031, 838, 595, 120, 954, 701, 325, 186, 1154, 1020, 823, 454, 368, 437, 897, 131, 817, 689, 957, 431, 989, 1096, 376, 597, 607, 445, 500, 942, 15, 974, 0, 1116, 1205, 894, 330, 1092, 1179, 364, 946, 947, 92, 1213, 916, 220, 173, 669, 930, 414, 800, 489, 977, 223, 962, 66, 226, 1055, 876, 920, 562, 711, 196, 369, 1168, 713, 275, 696, 970, 1015, 477, 1138, 641, 510, 211, 442, 1069, 1152, 1114, 745, 1172, 287, 521, 776, 443, 598, 314, 83, 773, 584, 1073, 1178, 896, 106, 923, 51, 43, 491, 268, 108, 482, 812]
valid_ids (0): []
train_ids (1094): [241, 70, 996, 1170, 787, 664, 316, 245, 82, 1087, 453, 1021, 1029, 400, 506, 875, 224, 871, 841, 1214, 222, 501, 702, 603, 933, 55, 700, 516, 546, 918, 1177, 900, 811, 628, 261, 52, 682, 380, 1001, 1129, 310, 663, 1059, 929, 69, 980, 899, 184, 303, 1058, 960, 183, 1210, 1127, 113, 828, 752, 786, 76, 483, 834, 1191, 1119, 1186, 569, 735, 623, 435, 968, 1048, 104, 1078, 657, 71, 22, 1130, 789, 809, 588, 362, 547, 613, 356, 215, 97, 652, 1123, 550, 433, 1, 146, 136, 739, 886, 1153, 851, 227, 1192, 631, 599, 311, 925, 98, 724, 1079, 1085, 344, 940, 1109, 45, 803, 457, 539, 878, 1202, 848, 943, 733, 1124, 499, 480, 290, 1035, 978, 101, 881, 615, 1016, 677, 116, 185, 528, 128, 137, 570, 783, 446, 403, 819, 887, 888, 1063, 1131, 844, 243, 276, 401, 246, 214, 648, 775, 704, 873, 493, 201, 208, 794, 1195, 165, 206, 1097, 1062, 267, 958, 190, 632, 707, 20, 481, 537, 1004, 771, 279, 38, 198, 7, 1141, 1041, 80, 1006, 144, 62, 41, 193, 1198, 505, 110, 509, 1167, 544, 565, 530, 764, 636, 488, 59, 976, 1142, 814, 6, 908, 859, 93, 174, 740, 766, 922, 522, 541, 1188, 710, 622, 604, 4, 822, 567, 804, 605, 111, 549, 606, 469, 553, 868, 421, 114, 430, 529, 601, 757, 465, 1082, 308, 690, 1045, 955, 336, 647, 1189, 885, 312, 813, 1136, 913, 177, 396, 898, 1089, 969, 591, 381, 1158, 142, 837, 640, 1139, 354, 777, 127, 576, 479, 399, 200, 154, 1011, 272, 1060, 265, 755, 239, 164, 406, 869, 65, 721, 972, 355, 644, 466, 238, 596, 586, 1106, 321, 260, 1212, 133, 1022, 744, 797, 1080, 81, 346, 117, 827, 398, 722, 616, 438, 262, 367, 747, 1203, 357, 1057, 358, 750, 1171, 450, 1132, 404, 731, 984, 1113, 649, 788, 459, 1200, 1100, 29, 484, 155, 737, 329, 497, 793, 534, 274, 973, 449, 902, 618, 458, 248, 74, 691, 831, 33, 633, 934, 545, 37, 441, 1128, 542, 444, 273, 552, 202, 75, 759, 416, 327, 1013, 912, 17, 1145, 455, 170, 257, 407, 624, 180, 763, 12, 1046, 536, 351, 298, 1182, 194, 383, 19, 798, 289, 317, 687, 883, 256, 667, 609, 96, 994, 945, 335, 820, 983, 694, 374, 328, 627, 157, 953, 285, 219, 315, 293, 408, 1122, 1149, 385, 697, 1135, 593, 845, 100, 986, 134, 10, 1094, 746, 1039, 1077, 924, 866, 387, 1126, 612, 642, 11, 532, 94, 26, 575, 1017, 77, 284, 1206, 1076, 686, 28, 580, 270, 386, 212, 566, 191, 784, 1027, 1208, 1099, 761, 865, 574, 266, 130, 125, 698, 46, 1143, 1000, 343, 168, 361, 626, 388, 533, 91, 805, 1144, 394, 807, 931, 60, 742, 675, 296, 653, 993, 397, 334, 867, 1061, 205, 513, 429, 830, 1081, 34, 617, 720, 825, 518, 850, 332, 84, 228, 145, 204, 1169, 507, 1010, 579, 1091, 614, 1088, 756, 1014, 451, 171, 1108, 242, 112, 995, 1147, 234, 880, 600, 651, 122, 233, 1201, 309, 393, 427, 439, 941, 1024, 872, 678, 30, 555, 857, 1086, 322, 48, 86, 1042, 231, 709, 568, 487, 411, 105, 808, 864, 765, 326, 835, 950, 304, 910, 192, 281, 554, 475, 412, 723, 662, 585, 410, 935, 1185, 621, 774, 858, 870, 129, 1121, 217, 1117, 187, 163, 79, 514, 464, 1084, 693, 939, 1005, 695, 1023, 502, 107, 685, 743, 178, 492, 1211, 1043, 578, 990, 420, 258, 561, 1162, 89, 147, 1008, 556, 378, 716, 668, 1065, 402, 1018, 754, 1009, 625, 1047, 413, 829, 1196, 305, 405, 861, 213, 207, 688, 816, 959, 1066, 460, 810, 31, 1003, 132, 253, 997, 975, 49, 679, 660, 948, 684, 904, 998, 225, 1054, 821, 307, 1049, 99, 353, 581, 699, 151, 926, 758, 478, 467, 727, 166, 849, 463, 269, 892, 1007, 508, 801, 149, 21, 348, 791, 313, 1174, 473, 152, 118, 160, 889, 650, 1183, 981, 379, 956, 681, 1103, 349, 712, 719, 153, 543, 61, 254, 729, 879, 474, 1125, 1072, 515, 594, 1093, 717, 538, 85, 123, 815, 498, 619, 210, 189, 306, 674, 5, 1075, 854, 961, 434, 571, 655, 425, 703, 490, 862, 1151, 54, 748, 1019, 264, 162, 503, 159, 175, 842, 68, 418, 288, 179, 1071, 906, 963, 852, 964, 782, 736, 301, 58, 1115, 1107, 24, 139, 796, 1052, 422, 135, 181, 115, 47, 1118, 428, 161, 331, 371, 337, 790, 832, 979, 472, 1150, 610, 1012, 860, 339, 456, 384, 526, 250, 563, 903, 531, 1038, 582, 88, 72, 3, 914, 486, 732, 1209, 847, 646, 1051, 148, 1173, 436, 333, 893, 656, 389, 32, 523, 323, 1161, 666, 1184, 319, 665, 1175, 373, 318, 40, 286, 560, 35, 551, 63, 971, 221, 67, 767, 252, 768, 1050, 635, 240, 1159, 991, 350, 18, 1105, 937, 370, 527, 638, 440, 714, 1194, 919, 320, 158, 251, 395, 965, 87, 525, 377, 1193, 1090, 1204, 780, 199, 583, 952, 927, 520, 27, 1033, 1163, 928, 726, 366, 172, 1040, 392, 890, 590, 9, 1104, 209, 360, 363, 230, 683, 938, 1137, 124, 840, 706, 282, 278, 195, 654, 1032, 1056, 375, 915, 461, 751, 417, 1037, 573, 572, 494, 218, 372, 102, 470, 203, 728, 432, 874, 589, 42, 818, 352, 1101, 283, 577, 967, 517, 138, 907, 56, 448, 424, 833, 8, 1074, 291, 1025, 1166, 884, 645, 824, 895, 672, 150, 409, 16, 738, 1026, 1134, 1146, 1111, 216, 909, 390, 64, 1028, 778, 519, 843, 1067, 197, 587, 237, 236, 863, 1070, 345, 634, 753, 1064, 452, 620, 249, 1190, 1165, 13, 271, 504, 1036, 932, 770, 592, 167, 708, 103, 769, 839, 73, 921, 300, 176, 799, 426, 846, 548, 949, 95, 244, 1197, 856, 294, 643, 109, 44, 535, 725, 779, 53, 715, 1140, 608, 1030, 50, 1164, 781, 365, 302, 877, 232, 673, 966, 1083, 119, 1120, 1044, 90, 229, 559, 540, 676, 512, 1157, 141, 826, 718, 182, 806, 1199, 2, 39, 1068, 987, 476, 126, 468, 78, 341, 936, 734, 25, 415, 1160, 340, 485, 760, 658, 295, 901, 670, 1133, 347, 169, 853, 1148, 951, 382, 630, 855, 629, 692, 564, 342, 905, 785, 795, 988, 1053, 637, 1207, 495, 802, 1110, 259, 792, 1095, 1176, 1156, 772, 992, 1112, 680, 235, 639, 1098, 57, 661, 836, 292, 882, 762, 247, 705, 524, 557, 659, 140, 359, 255, 749, 280, 730, 299, 14, 143, 891, 911, 423, 558, 917, 741, 985, 263, 277, 511, 1102, 1002, 23, 471, 338, 602]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5698259655636195
the save name prefix for this run is:  chkpt-ID_5698259655636195_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 520
rank avg (pred): 0.497 +- 0.013
mrr vals (pred, true): 0.019, 0.230
batch losses (mrrl, rdl): 0.0, 0.002569583

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 950
rank avg (pred): 0.460 +- 0.262
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 0.0, 8.198e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 345
rank avg (pred): 0.465 +- 0.261
mrr vals (pred, true): 0.041, 0.052
batch losses (mrrl, rdl): 0.0, 1.5217e-06

Epoch over!
epoch time: 15.015

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 272
rank avg (pred): 0.060 +- 0.060
mrr vals (pred, true): 0.253, 0.332
batch losses (mrrl, rdl): 0.0, 4.39296e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 703
rank avg (pred): 0.470 +- 0.261
mrr vals (pred, true): 0.037, 0.056
batch losses (mrrl, rdl): 0.0, 1.6063e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 201
rank avg (pred): 0.467 +- 0.256
mrr vals (pred, true): 0.037, 0.055
batch losses (mrrl, rdl): 0.0, 3.2743e-06

Epoch over!
epoch time: 14.778

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1010
rank avg (pred): 0.469 +- 0.247
mrr vals (pred, true): 0.036, 0.054
batch losses (mrrl, rdl): 0.0, 3.9245e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 259
rank avg (pred): 0.087 +- 0.115
mrr vals (pred, true): 0.259, 0.327
batch losses (mrrl, rdl): 0.0, 1.19721e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 935
rank avg (pred): 0.469 +- 0.263
mrr vals (pred, true): 0.039, 0.051
batch losses (mrrl, rdl): 0.0, 4.486e-06

Epoch over!
epoch time: 14.75

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 159
rank avg (pred): 0.467 +- 0.261
mrr vals (pred, true): 0.039, 0.051
batch losses (mrrl, rdl): 0.0, 3.32e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 565
rank avg (pred): 0.208 +- 0.206
mrr vals (pred, true): 0.140, 0.193
batch losses (mrrl, rdl): 0.0, 6.5091e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 79
rank avg (pred): 0.128 +- 0.152
mrr vals (pred, true): 0.174, 0.254
batch losses (mrrl, rdl): 0.0, 9.9907e-06

Epoch over!
epoch time: 14.959

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 924
rank avg (pred): 0.466 +- 0.264
mrr vals (pred, true): 0.042, 0.047
batch losses (mrrl, rdl): 0.0, 4.999e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1027
rank avg (pred): 0.470 +- 0.270
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 0.0, 7.5308e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 841
rank avg (pred): 0.459 +- 0.268
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 0.0, 3.889e-07

Epoch over!
epoch time: 14.963

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 289
rank avg (pred): 0.110 +- 0.136
mrr vals (pred, true): 0.207, 0.272
batch losses (mrrl, rdl): 0.0419101119, 1.65254e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1174
rank avg (pred): 0.422 +- 0.121
mrr vals (pred, true): 0.046, 0.053
batch losses (mrrl, rdl): 0.0001538706, 7.76003e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 244
rank avg (pred): 0.096 +- 0.069
mrr vals (pred, true): 0.271, 0.322
batch losses (mrrl, rdl): 0.0266548358, 3.3139e-06

Epoch over!
epoch time: 15.049

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 4
rank avg (pred): 0.060 +- 0.046
mrr vals (pred, true): 0.349, 0.325
batch losses (mrrl, rdl): 0.0056920471, 4.94822e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 84
rank avg (pred): 0.440 +- 0.105
mrr vals (pred, true): 0.039, 0.053
batch losses (mrrl, rdl): 0.0012626848, 7.60074e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 887
rank avg (pred): 0.435 +- 0.111
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002916075, 7.26429e-05

Epoch over!
epoch time: 15.199

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 808
rank avg (pred): 0.425 +- 0.123
mrr vals (pred, true): 0.055, 0.054
batch losses (mrrl, rdl): 0.0002200608, 0.0001067986

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 204
rank avg (pred): 0.442 +- 0.107
mrr vals (pred, true): 0.041, 0.057
batch losses (mrrl, rdl): 0.0008714438, 6.77412e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1113
rank avg (pred): 0.428 +- 0.128
mrr vals (pred, true): 0.058, 0.057
batch losses (mrrl, rdl): 0.0006292336, 5.26384e-05

Epoch over!
epoch time: 15.144

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 174
rank avg (pred): 0.443 +- 0.100
mrr vals (pred, true): 0.041, 0.059
batch losses (mrrl, rdl): 0.0007540218, 6.96478e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 847
rank avg (pred): 0.431 +- 0.120
mrr vals (pred, true): 0.055, 0.059
batch losses (mrrl, rdl): 0.0002557151, 7.22413e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1140
rank avg (pred): 0.113 +- 0.078
mrr vals (pred, true): 0.283, 0.319
batch losses (mrrl, rdl): 0.0130753424, 7.8907e-06

Epoch over!
epoch time: 15.045

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 301
rank avg (pred): 0.123 +- 0.083
mrr vals (pred, true): 0.263, 0.326
batch losses (mrrl, rdl): 0.0393745154, 1.10122e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 553
rank avg (pred): 0.269 +- 0.193
mrr vals (pred, true): 0.257, 0.206
batch losses (mrrl, rdl): 0.0261747129, 0.0002907749

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1197
rank avg (pred): 0.442 +- 0.105
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 1.71254e-05, 6.18932e-05

Epoch over!
epoch time: 15.171

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 835
rank avg (pred): 0.073 +- 0.054
mrr vals (pred, true): 0.346, 0.331
batch losses (mrrl, rdl): 0.0022619832, 8.34368e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 647
rank avg (pred): 0.440 +- 0.110
mrr vals (pred, true): 0.052, 0.053
batch losses (mrrl, rdl): 4.16226e-05, 6.95494e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1066
rank avg (pred): 0.048 +- 0.032
mrr vals (pred, true): 0.351, 0.453
batch losses (mrrl, rdl): 0.1038485095, 1.87131e-05

Epoch over!
epoch time: 15.219

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 405
rank avg (pred): 0.442 +- 0.111
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 0.0001085818, 6.71538e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 249
rank avg (pred): 0.095 +- 0.069
mrr vals (pred, true): 0.330, 0.289
batch losses (mrrl, rdl): 0.0168726519, 1.12029e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1021
rank avg (pred): 0.445 +- 0.092
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001269251, 7.23973e-05

Epoch over!
epoch time: 15.179

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 933
rank avg (pred): 0.442 +- 0.097
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 2.1221e-06, 7.80103e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 792
rank avg (pred): 0.436 +- 0.103
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.000237835, 6.79421e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 547
rank avg (pred): 0.328 +- 0.198
mrr vals (pred, true): 0.194, 0.227
batch losses (mrrl, rdl): 0.0106909517, 0.0008799034

Epoch over!
epoch time: 15.158

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 560
rank avg (pred): 0.332 +- 0.194
mrr vals (pred, true): 0.196, 0.203
batch losses (mrrl, rdl): 0.0004842154, 0.0006628083

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 782
rank avg (pred): 0.432 +- 0.096
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 6.99912e-05, 8.98339e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 343
rank avg (pred): 0.435 +- 0.090
mrr vals (pred, true): 0.049, 0.057
batch losses (mrrl, rdl): 4.3978e-06, 8.37574e-05

Epoch over!
epoch time: 15.183

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 609
rank avg (pred): 0.435 +- 0.098
mrr vals (pred, true): 0.054, 0.051
batch losses (mrrl, rdl): 0.0001898613, 9.85367e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 747
rank avg (pred): 0.040 +- 0.031
mrr vals (pred, true): 0.419, 0.403
batch losses (mrrl, rdl): 0.0026666804, 3.41664e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 905
rank avg (pred): 0.311 +- 0.201
mrr vals (pred, true): 0.228, 0.228
batch losses (mrrl, rdl): 2.0859e-06, 0.0005902677

Epoch over!
epoch time: 15.174

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.435 +- 0.091
mrr vals (pred, true): 0.051, 0.047

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   71 	     0 	 0.05109 	 0.04495 	 ~...
   67 	     1 	 0.05064 	 0.04624 	 ~...
   66 	     2 	 0.05056 	 0.04668 	 ~...
   68 	     3 	 0.05065 	 0.04763 	 ~...
   48 	     4 	 0.04972 	 0.04769 	 ~...
    2 	     5 	 0.04432 	 0.04815 	 ~...
   72 	     6 	 0.05131 	 0.04831 	 ~...
   83 	     7 	 0.05558 	 0.04864 	 ~...
   29 	     8 	 0.04860 	 0.04866 	 ~...
   14 	     9 	 0.04767 	 0.04869 	 ~...
   21 	    10 	 0.04808 	 0.04931 	 ~...
   38 	    11 	 0.04912 	 0.04945 	 ~...
   70 	    12 	 0.05107 	 0.04948 	 ~...
   63 	    13 	 0.05029 	 0.04948 	 ~...
    6 	    14 	 0.04655 	 0.04950 	 ~...
   55 	    15 	 0.05001 	 0.04954 	 ~...
   58 	    16 	 0.05005 	 0.04963 	 ~...
   60 	    17 	 0.05009 	 0.05056 	 ~...
   57 	    18 	 0.05003 	 0.05075 	 ~...
   24 	    19 	 0.04834 	 0.05075 	 ~...
    0 	    20 	 0.04351 	 0.05076 	 ~...
   54 	    21 	 0.05001 	 0.05080 	 ~...
   50 	    22 	 0.04980 	 0.05090 	 ~...
   49 	    23 	 0.04976 	 0.05092 	 ~...
   22 	    24 	 0.04811 	 0.05099 	 ~...
   20 	    25 	 0.04807 	 0.05127 	 ~...
   77 	    26 	 0.05307 	 0.05128 	 ~...
   27 	    27 	 0.04859 	 0.05155 	 ~...
   18 	    28 	 0.04790 	 0.05168 	 ~...
   64 	    29 	 0.05045 	 0.05171 	 ~...
   35 	    30 	 0.04885 	 0.05177 	 ~...
   56 	    31 	 0.05002 	 0.05212 	 ~...
   31 	    32 	 0.04866 	 0.05227 	 ~...
   19 	    33 	 0.04804 	 0.05258 	 ~...
   75 	    34 	 0.05216 	 0.05263 	 ~...
   47 	    35 	 0.04971 	 0.05269 	 ~...
   59 	    36 	 0.05008 	 0.05274 	 ~...
   25 	    37 	 0.04852 	 0.05286 	 ~...
    8 	    38 	 0.04691 	 0.05293 	 ~...
   34 	    39 	 0.04880 	 0.05302 	 ~...
   82 	    40 	 0.05438 	 0.05308 	 ~...
   10 	    41 	 0.04723 	 0.05315 	 ~...
   73 	    42 	 0.05143 	 0.05323 	 ~...
   41 	    43 	 0.04924 	 0.05354 	 ~...
    4 	    44 	 0.04590 	 0.05357 	 ~...
   69 	    45 	 0.05077 	 0.05365 	 ~...
   80 	    46 	 0.05412 	 0.05387 	 ~...
    3 	    47 	 0.04440 	 0.05392 	 ~...
   78 	    48 	 0.05329 	 0.05392 	 ~...
   65 	    49 	 0.05045 	 0.05404 	 ~...
   61 	    50 	 0.05015 	 0.05429 	 ~...
    1 	    51 	 0.04358 	 0.05434 	 ~...
   13 	    52 	 0.04758 	 0.05438 	 ~...
   39 	    53 	 0.04915 	 0.05438 	 ~...
   43 	    54 	 0.04927 	 0.05447 	 ~...
   36 	    55 	 0.04900 	 0.05456 	 ~...
   81 	    56 	 0.05418 	 0.05476 	 ~...
   32 	    57 	 0.04868 	 0.05484 	 ~...
   51 	    58 	 0.04982 	 0.05490 	 ~...
   11 	    59 	 0.04725 	 0.05495 	 ~...
   45 	    60 	 0.04946 	 0.05500 	 ~...
    5 	    61 	 0.04632 	 0.05531 	 ~...
   46 	    62 	 0.04962 	 0.05571 	 ~...
   79 	    63 	 0.05366 	 0.05593 	 ~...
   23 	    64 	 0.04829 	 0.05599 	 ~...
   16 	    65 	 0.04785 	 0.05614 	 ~...
   53 	    66 	 0.04999 	 0.05631 	 ~...
   28 	    67 	 0.04860 	 0.05664 	 ~...
   33 	    68 	 0.04869 	 0.05712 	 ~...
   17 	    69 	 0.04788 	 0.05748 	 ~...
   62 	    70 	 0.05026 	 0.05751 	 ~...
   12 	    71 	 0.04756 	 0.05756 	 ~...
   30 	    72 	 0.04864 	 0.05766 	 ~...
   74 	    73 	 0.05178 	 0.05770 	 ~...
   52 	    74 	 0.04998 	 0.05814 	 ~...
   15 	    75 	 0.04783 	 0.05896 	 ~...
   26 	    76 	 0.04859 	 0.05901 	 ~...
    7 	    77 	 0.04673 	 0.05904 	 ~...
    9 	    78 	 0.04691 	 0.05906 	 ~...
   37 	    79 	 0.04901 	 0.05969 	 ~...
   42 	    80 	 0.04926 	 0.06012 	 ~...
   44 	    81 	 0.04927 	 0.06014 	 ~...
   40 	    82 	 0.04918 	 0.06253 	 ~...
   76 	    83 	 0.05281 	 0.06254 	 ~...
   87 	    84 	 0.19542 	 0.08944 	 MISS
   85 	    85 	 0.18475 	 0.16198 	 ~...
  100 	    86 	 0.29392 	 0.16633 	 MISS
   86 	    87 	 0.18739 	 0.20204 	 ~...
   84 	    88 	 0.18041 	 0.20455 	 ~...
   88 	    89 	 0.22930 	 0.21861 	 ~...
   91 	    90 	 0.24402 	 0.23429 	 ~...
   90 	    91 	 0.23770 	 0.25103 	 ~...
   93 	    92 	 0.25737 	 0.25490 	 ~...
   89 	    93 	 0.23607 	 0.25558 	 ~...
   92 	    94 	 0.25255 	 0.25657 	 ~...
   98 	    95 	 0.29074 	 0.25874 	 m..s
   96 	    96 	 0.28660 	 0.26205 	 ~...
  102 	    97 	 0.29686 	 0.27425 	 ~...
  104 	    98 	 0.29696 	 0.29342 	 ~...
  108 	    99 	 0.32710 	 0.29748 	 ~...
  109 	   100 	 0.32713 	 0.30084 	 ~...
  106 	   101 	 0.29942 	 0.30230 	 ~...
   99 	   102 	 0.29317 	 0.30510 	 ~...
  101 	   103 	 0.29598 	 0.30931 	 ~...
   97 	   104 	 0.28768 	 0.30945 	 ~...
  105 	   105 	 0.29874 	 0.31009 	 ~...
  110 	   106 	 0.32975 	 0.31553 	 ~...
   94 	   107 	 0.28052 	 0.31912 	 m..s
   95 	   108 	 0.28361 	 0.32386 	 m..s
  107 	   109 	 0.32338 	 0.34916 	 ~...
  113 	   110 	 0.39174 	 0.37350 	 ~...
  116 	   111 	 0.42021 	 0.37760 	 m..s
  115 	   112 	 0.41918 	 0.38075 	 m..s
  114 	   113 	 0.39710 	 0.41730 	 ~...
  112 	   114 	 0.36205 	 0.42736 	 m..s
  117 	   115 	 0.43331 	 0.47935 	 m..s
  118 	   116 	 0.48798 	 0.48073 	 ~...
  111 	   117 	 0.33364 	 0.48087 	 MISS
  103 	   118 	 0.29690 	 0.48279 	 MISS
  119 	   119 	 0.50997 	 0.48579 	 ~...
  120 	   120 	 0.57173 	 0.62777 	 m..s
==========================================
r_mrr = 0.9755861163139343
r2_mrr = 0.950552225112915
spearmanr_mrr@5 = 0.8119819760322571
spearmanr_mrr@10 = 0.9210684299468994
spearmanr_mrr@50 = 0.9867173433303833
spearmanr_mrr@100 = 0.9927642345428467
spearmanr_mrr@All = 0.9933682084083557
==========================================
test time: 0.459
Done Testing dataset Kinships
total time taken: 234.1951608657837
training time taken: 226.4549651145935
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9756)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9506)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.8120)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9211)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9867)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9928)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9934)}}, 'test_loss': {'ComplEx': {'Kinships': 1.2062005003899685}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7267501249025243
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [409, 989, 542, 1089, 29, 837, 1000, 475, 138, 510, 477, 1202, 1137, 1169, 1177, 842, 887, 1032, 61, 483, 909, 299, 1064, 32, 63, 437, 321, 208, 1161, 880, 1027, 1149, 69, 953, 1024, 531, 224, 638, 733, 1141, 1057, 450, 937, 121, 428, 978, 524, 71, 706, 936, 374, 160, 799, 869, 849, 882, 690, 166, 715, 794, 742, 554, 752, 237, 170, 357, 1185, 573, 1113, 425, 1056, 1038, 591, 585, 895, 784, 349, 761, 866, 952, 1168, 1028, 352, 637, 495, 80, 416, 21, 1104, 611, 502, 984, 558, 151, 547, 1069, 712, 839, 165, 728, 1191, 363, 302, 228, 371, 1001, 342, 249, 1016, 649, 360, 447, 1020, 835, 135, 39, 873, 243, 227, 615, 244]
valid_ids (0): []
train_ids (1094): [236, 1010, 1187, 605, 949, 461, 106, 161, 64, 904, 432, 1171, 197, 152, 626, 184, 944, 211, 529, 993, 894, 58, 812, 72, 256, 990, 1115, 140, 355, 384, 1204, 376, 169, 484, 382, 639, 49, 998, 478, 127, 261, 408, 481, 377, 291, 193, 846, 499, 657, 130, 44, 684, 254, 1125, 137, 651, 847, 277, 636, 857, 129, 1054, 1061, 164, 1142, 65, 233, 42, 830, 927, 640, 746, 633, 513, 517, 415, 1033, 298, 923, 1071, 1152, 192, 710, 441, 762, 36, 235, 87, 340, 431, 1110, 672, 661, 570, 27, 407, 465, 1196, 1040, 1014, 919, 167, 683, 146, 482, 420, 940, 454, 1074, 536, 621, 1037, 362, 430, 831, 442, 668, 346, 419, 618, 101, 1025, 539, 1164, 183, 94, 538, 1183, 186, 608, 195, 889, 1063, 412, 1085, 1138, 457, 860, 188, 963, 1013, 304, 527, 1068, 366, 319, 89, 1005, 1214, 563, 522, 17, 823, 229, 568, 663, 738, 653, 694, 231, 888, 134, 496, 589, 95, 815, 1081, 601, 399, 720, 1167, 398, 722, 1199, 541, 18, 604, 204, 607, 404, 345, 486, 520, 1151, 859, 213, 747, 516, 187, 631, 296, 60, 190, 281, 814, 1075, 418, 403, 181, 1174, 734, 1083, 1198, 617, 6, 834, 767, 995, 353, 246, 702, 93, 773, 323, 704, 198, 1072, 11, 22, 896, 1051, 74, 309, 964, 86, 874, 813, 981, 596, 361, 796, 406, 1150, 1143, 700, 469, 545, 932, 588, 1203, 664, 459, 307, 372, 1213, 1206, 1176, 400, 848, 79, 806, 120, 779, 907, 708, 965, 717, 276, 479, 263, 180, 1043, 753, 1107, 119, 928, 508, 88, 555, 1178, 199, 31, 878, 898, 1049, 348, 492, 646, 606, 776, 7, 111, 456, 1192, 367, 645, 840, 178, 232, 511, 280, 997, 850, 23, 444, 1184, 535, 1207, 600, 385, 1039, 1165, 438, 763, 1166, 157, 335, 1046, 343, 929, 396, 43, 284, 532, 206, 553, 1131, 45, 1078, 820, 1162, 811, 1026, 123, 504, 1084, 914, 743, 82, 916, 1029, 662, 863, 117, 819, 556, 359, 821, 1126, 689, 901, 967, 1134, 526, 133, 1004, 1153, 518, 401, 951, 537, 364, 507, 854, 283, 54, 1058, 194, 270, 789, 561, 546, 77, 1117, 226, 971, 1021, 463, 678, 8, 620, 792, 336, 1136, 287, 603, 326, 827, 458, 665, 316, 320, 402, 1062, 150, 567, 790, 1070, 3, 711, 464, 103, 1127, 525, 391, 991, 676, 448, 393, 841, 185, 241, 1065, 931, 55, 1017, 758, 1140, 451, 769, 800, 209, 1159, 817, 669, 629, 303, 200, 10, 1175, 798, 274, 992, 581, 337, 973, 565, 1066, 305, 153, 756, 1189, 487, 1181, 1077, 118, 1172, 159, 1109, 324, 1112, 78, 380, 574, 132, 619, 498, 1156, 551, 724, 1211, 476, 216, 282, 311, 667, 172, 333, 977, 713, 577, 644, 658, 675, 1086, 269, 81, 330, 144, 390, 543, 632, 1145, 899, 785, 612, 462, 957, 107, 552, 698, 754, 300, 9, 616, 13, 1098, 810, 48, 5, 341, 803, 654, 257, 740, 294, 687, 918, 92, 723, 915, 489, 755, 959, 347, 614, 271, 449, 350, 35, 397, 331, 179, 258, 125, 30, 622, 911, 84, 795, 259, 41, 719, 297, 46, 83, 783, 368, 576, 1108, 770, 736, 729, 884, 845, 480, 760, 435, 314, 627, 51, 423, 1023, 485, 774, 292, 560, 578, 1048, 956, 434, 413, 650, 703, 491, 688, 679, 515, 503, 745, 727, 968, 505, 905, 175, 136, 239, 50, 1095, 386, 1148, 217, 1092, 872, 85, 116, 59, 1007, 369, 858, 994, 1179, 922, 804, 540, 950, 925, 351, 987, 697, 171, 778, 643, 630, 176, 155, 920, 506, 960, 955, 721, 912, 334, 308, 590, 1186, 1106, 521, 264, 443, 338, 634, 1011, 856, 780, 674, 113, 749, 958, 1031, 290, 1130, 864, 56, 1052, 218, 935, 440, 822, 583, 234, 312, 381, 1067, 455, 983, 1123, 329, 327, 62, 908, 288, 1055, 66, 852, 879, 225, 921, 34, 429, 471, 202, 844, 1097, 876, 1111, 344, 699, 128, 726, 253, 223, 201, 248, 387, 1135, 500, 757, 979, 76, 544, 1041, 818, 751, 610, 1096, 933, 750, 974, 1154, 154, 735, 16, 718, 189, 392, 1205, 210, 286, 808, 569, 945, 976, 917, 897, 370, 777, 356, 575, 40, 594, 247, 737, 145, 468, 519, 53, 673, 881, 240, 938, 1047, 1212, 272, 528, 0, 628, 969, 114, 1120, 828, 1059, 293, 122, 685, 268, 37, 549, 802, 1146, 405, 453, 439, 705, 205, 262, 943, 289, 493, 177, 1100, 214, 473, 1139, 771, 962, 26, 75, 1195, 1105, 851, 252, 255, 70, 732, 1163, 625, 124, 947, 642, 358, 833, 1157, 867, 670, 548, 829, 445, 1088, 207, 982, 1030, 865, 427, 394, 238, 365, 975, 692, 861, 709, 701, 325, 14, 656, 267, 514, 1093, 474, 824, 212, 666, 395, 587, 1003, 1, 714, 1018, 433, 954, 1147, 1182, 488, 765, 660, 417, 768, 1133, 104, 424, 1036, 686, 1045, 1132, 422, 942, 215, 946, 446, 597, 1173, 173, 131, 913, 1160, 12, 677, 379, 1194, 807, 843, 295, 671, 375, 1079, 986, 571, 20, 652, 1129, 1015, 306, 572, 100, 797, 1035, 885, 102, 891, 550, 680, 793, 182, 265, 67, 655, 109, 490, 47, 787, 96, 731, 832, 1193, 1118, 91, 996, 436, 388, 584, 332, 278, 279, 222, 825, 245, 494, 1002, 641, 472, 1019, 602, 383, 57, 1201, 805, 648, 322, 251, 466, 1006, 149, 15, 707, 759, 559, 930, 497, 28, 999, 313, 826, 924, 781, 242, 966, 970, 624, 1121, 509, 838, 961, 739, 168, 1180, 1094, 853, 691, 1087, 1012, 647, 191, 142, 613, 196, 373, 534, 317, 452, 906, 1114, 1188, 741, 599, 250, 90, 174, 52, 426, 862, 609, 421, 275, 1076, 143, 870, 318, 467, 566, 115, 1200, 1034, 163, 695, 220, 635, 112, 883, 460, 301, 315, 1155, 203, 411, 110, 900, 744, 378, 99, 1053, 105, 716, 221, 1102, 557, 38, 926, 2, 1060, 592, 1101, 1082, 598, 579, 786, 24, 156, 748, 470, 108, 1158, 816, 875, 354, 523, 1122, 148, 725, 772, 266, 681, 230, 530, 730, 941, 682, 1128, 219, 988, 980, 310, 939, 595, 1042, 562, 1124, 273, 1044, 389, 1080, 791, 902, 141, 855, 533, 775, 414, 1022, 19, 25, 1073, 782, 285, 972, 801, 696, 1009, 659, 580, 33, 985, 98, 1144, 68, 73, 582, 147, 903, 836, 1090, 871, 877, 501, 328, 158, 788, 890, 162, 139, 126, 766, 4, 1091, 1099, 512, 1209, 1208, 764, 623, 910, 97, 593, 934, 893, 886, 1116, 1210, 868, 339, 586, 1119, 410, 260, 1103, 1190, 1170, 809, 564, 1050, 693, 948, 1008, 1197, 892]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2511527694940704
the save name prefix for this run is:  chkpt-ID_2511527694940704_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 78
rank avg (pred): 0.491 +- 0.008
mrr vals (pred, true): 0.019, 0.263
batch losses (mrrl, rdl): 0.0, 0.002475386

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 144
rank avg (pred): 0.469 +- 0.005
mrr vals (pred, true): 0.020, 0.049
batch losses (mrrl, rdl): 0.0, 8.25045e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 619
rank avg (pred): 0.469 +- 0.008
mrr vals (pred, true): 0.020, 0.052
batch losses (mrrl, rdl): 0.0, 7.5613e-05

Epoch over!
epoch time: 14.81

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 310
rank avg (pred): 0.130 +- 0.003
mrr vals (pred, true): 0.070, 0.304
batch losses (mrrl, rdl): 0.0, 1.14631e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 175
rank avg (pred): 0.466 +- 0.030
mrr vals (pred, true): 0.020, 0.050
batch losses (mrrl, rdl): 0.0, 7.34915e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 334
rank avg (pred): 0.445 +- 0.262
mrr vals (pred, true): 0.099, 0.058
batch losses (mrrl, rdl): 0.0, 2.8354e-06

Epoch over!
epoch time: 14.78

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 92
rank avg (pred): 0.441 +- 0.267
mrr vals (pred, true): 0.101, 0.057
batch losses (mrrl, rdl): 0.0, 2.8963e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 403
rank avg (pred): 0.448 +- 0.268
mrr vals (pred, true): 0.086, 0.050
batch losses (mrrl, rdl): 0.0, 5.6436e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 805
rank avg (pred): 0.451 +- 0.262
mrr vals (pred, true): 0.066, 0.054
batch losses (mrrl, rdl): 0.0, 1.6297e-06

Epoch over!
epoch time: 14.874

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1063
rank avg (pred): 0.080 +- 0.044
mrr vals (pred, true): 0.184, 0.371
batch losses (mrrl, rdl): 0.0, 5.5422e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 147
rank avg (pred): 0.455 +- 0.262
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0, 6.2702e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 879
rank avg (pred): 0.456 +- 0.259
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 0.0, 9.649e-07

Epoch over!
epoch time: 15.011

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1153
rank avg (pred): 0.092 +- 0.055
mrr vals (pred, true): 0.193, 0.259
batch losses (mrrl, rdl): 0.0, 3.13714e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 26
rank avg (pred): 0.115 +- 0.067
mrr vals (pred, true): 0.155, 0.330
batch losses (mrrl, rdl): 0.0, 7.763e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 253
rank avg (pred): 0.111 +- 0.064
mrr vals (pred, true): 0.158, 0.321
batch losses (mrrl, rdl): 0.0, 4.7814e-06

Epoch over!
epoch time: 14.991

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1176
rank avg (pred): 0.453 +- 0.259
mrr vals (pred, true): 0.054, 0.048
batch losses (mrrl, rdl): 0.0001659356, 7.8994e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 67
rank avg (pred): 0.035 +- 0.021
mrr vals (pred, true): 0.324, 0.298
batch losses (mrrl, rdl): 0.0070990389, 0.0002135046

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 678
rank avg (pred): 0.479 +- 0.197
mrr vals (pred, true): 0.045, 0.060
batch losses (mrrl, rdl): 0.0002735023, 5.39447e-05

Epoch over!
epoch time: 14.996

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 748
rank avg (pred): 0.015 +- 0.010
mrr vals (pred, true): 0.504, 0.412
batch losses (mrrl, rdl): 0.0853011459, 0.0001058426

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 183
rank avg (pred): 0.517 +- 0.260
mrr vals (pred, true): 0.057, 0.054
batch losses (mrrl, rdl): 0.0005333059, 8.00158e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 28
rank avg (pred): 0.056 +- 0.038
mrr vals (pred, true): 0.306, 0.318
batch losses (mrrl, rdl): 0.001332802, 5.7655e-05

Epoch over!
epoch time: 14.957

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 325
rank avg (pred): 0.439 +- 0.160
mrr vals (pred, true): 0.043, 0.056
batch losses (mrrl, rdl): 0.0005263539, 3.24242e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 686
rank avg (pred): 0.492 +- 0.215
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 1.91231e-05, 5.51884e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 658
rank avg (pred): 0.474 +- 0.191
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 8.69014e-05, 1.92476e-05

Epoch over!
epoch time: 15.197

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 418
rank avg (pred): 0.472 +- 0.180
mrr vals (pred, true): 0.043, 0.059
batch losses (mrrl, rdl): 0.0004528941, 3.07411e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 421
rank avg (pred): 0.479 +- 0.183
mrr vals (pred, true): 0.043, 0.056
batch losses (mrrl, rdl): 0.0004655256, 4.01154e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 457
rank avg (pred): 0.493 +- 0.231
mrr vals (pred, true): 0.057, 0.052
batch losses (mrrl, rdl): 0.0004833433, 4.75317e-05

Epoch over!
epoch time: 14.996

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 44
rank avg (pred): 0.053 +- 0.035
mrr vals (pred, true): 0.308, 0.306
batch losses (mrrl, rdl): 5.59094e-05, 8.40095e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 548
rank avg (pred): 0.128 +- 0.084
mrr vals (pred, true): 0.217, 0.239
batch losses (mrrl, rdl): 0.0049831155, 7.5036e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 284
rank avg (pred): 0.050 +- 0.034
mrr vals (pred, true): 0.336, 0.301
batch losses (mrrl, rdl): 0.0121690426, 0.0001223873

Epoch over!
epoch time: 14.998

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 907
rank avg (pred): 0.173 +- 0.113
mrr vals (pred, true): 0.190, 0.169
batch losses (mrrl, rdl): 0.00451014, 8.38089e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 529
rank avg (pred): 0.155 +- 0.098
mrr vals (pred, true): 0.191, 0.212
batch losses (mrrl, rdl): 0.0042705783, 8.3253e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 124
rank avg (pred): 0.462 +- 0.183
mrr vals (pred, true): 0.049, 0.056
batch losses (mrrl, rdl): 1.37926e-05, 2.40373e-05

Epoch over!
epoch time: 15.011

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 901
rank avg (pred): 0.083 +- 0.058
mrr vals (pred, true): 0.298, 0.260
batch losses (mrrl, rdl): 0.0137861911, 5.32572e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 134
rank avg (pred): 0.497 +- 0.235
mrr vals (pred, true): 0.060, 0.048
batch losses (mrrl, rdl): 0.001068517, 3.77735e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 807
rank avg (pred): 0.493 +- 0.237
mrr vals (pred, true): 0.064, 0.053
batch losses (mrrl, rdl): 0.0019985675, 4.14141e-05

Epoch over!
epoch time: 15.028

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 549
rank avg (pred): 0.172 +- 0.116
mrr vals (pred, true): 0.224, 0.194
batch losses (mrrl, rdl): 0.0093034059, 8.5746e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 108
rank avg (pred): 0.422 +- 0.159
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 1.61e-08, 5.16199e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 25
rank avg (pred): 0.093 +- 0.064
mrr vals (pred, true): 0.287, 0.317
batch losses (mrrl, rdl): 0.0086099468, 2.27459e-05

Epoch over!
epoch time: 15.0

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 990
rank avg (pred): 0.052 +- 0.035
mrr vals (pred, true): 0.336, 0.385
batch losses (mrrl, rdl): 0.0242156945, 2.99834e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 13
rank avg (pred): 0.072 +- 0.052
mrr vals (pred, true): 0.322, 0.309
batch losses (mrrl, rdl): 0.0016115961, 5.41435e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 217
rank avg (pred): 0.407 +- 0.145
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 4.89368e-05, 8.58071e-05

Epoch over!
epoch time: 15.003

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 292
rank avg (pred): 0.100 +- 0.068
mrr vals (pred, true): 0.270, 0.287
batch losses (mrrl, rdl): 0.0028136002, 2.6483e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 720
rank avg (pred): 0.471 +- 0.205
mrr vals (pred, true): 0.055, 0.051
batch losses (mrrl, rdl): 0.000292235, 2.18151e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1051
rank avg (pred): 0.522 +- 0.222
mrr vals (pred, true): 0.046, 0.053
batch losses (mrrl, rdl): 0.0001444089, 0.0001072032

Epoch over!
epoch time: 14.997

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.406 +- 0.156
mrr vals (pred, true): 0.042, 0.057

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   80 	     0 	 0.06450 	 0.04669 	 ~...
   50 	     1 	 0.05394 	 0.04721 	 ~...
   50 	     2 	 0.05394 	 0.04747 	 ~...
   31 	     3 	 0.04841 	 0.04753 	 ~...
   64 	     4 	 0.05418 	 0.04803 	 ~...
   12 	     5 	 0.04421 	 0.04823 	 ~...
   50 	     6 	 0.05394 	 0.04830 	 ~...
   73 	     7 	 0.05688 	 0.04830 	 ~...
   74 	     8 	 0.05760 	 0.04851 	 ~...
   27 	     9 	 0.04815 	 0.04872 	 ~...
   25 	    10 	 0.04764 	 0.04915 	 ~...
   50 	    11 	 0.05394 	 0.04941 	 ~...
   18 	    12 	 0.04642 	 0.04941 	 ~...
   19 	    13 	 0.04691 	 0.04954 	 ~...
   40 	    14 	 0.05042 	 0.04973 	 ~...
   20 	    15 	 0.04714 	 0.04998 	 ~...
   68 	    16 	 0.05436 	 0.05000 	 ~...
   22 	    17 	 0.04747 	 0.05025 	 ~...
   50 	    18 	 0.05394 	 0.05038 	 ~...
   39 	    19 	 0.05023 	 0.05055 	 ~...
   75 	    20 	 0.05800 	 0.05056 	 ~...
   29 	    21 	 0.04837 	 0.05075 	 ~...
   50 	    22 	 0.05394 	 0.05080 	 ~...
   50 	    23 	 0.05394 	 0.05084 	 ~...
   79 	    24 	 0.06296 	 0.05086 	 ~...
   50 	    25 	 0.05394 	 0.05104 	 ~...
    1 	    26 	 0.03744 	 0.05121 	 ~...
   67 	    27 	 0.05435 	 0.05126 	 ~...
   32 	    28 	 0.04843 	 0.05136 	 ~...
   50 	    29 	 0.05394 	 0.05146 	 ~...
   50 	    30 	 0.05394 	 0.05149 	 ~...
   33 	    31 	 0.04854 	 0.05154 	 ~...
   48 	    32 	 0.05280 	 0.05168 	 ~...
   36 	    33 	 0.04928 	 0.05188 	 ~...
   21 	    34 	 0.04718 	 0.05189 	 ~...
   28 	    35 	 0.04835 	 0.05195 	 ~...
   45 	    36 	 0.05211 	 0.05212 	 ~...
   23 	    37 	 0.04749 	 0.05212 	 ~...
   44 	    38 	 0.05205 	 0.05229 	 ~...
   78 	    39 	 0.06281 	 0.05232 	 ~...
   76 	    40 	 0.05931 	 0.05237 	 ~...
   70 	    41 	 0.05493 	 0.05261 	 ~...
    3 	    42 	 0.04122 	 0.05269 	 ~...
   69 	    43 	 0.05464 	 0.05286 	 ~...
   65 	    44 	 0.05421 	 0.05351 	 ~...
    8 	    45 	 0.04305 	 0.05363 	 ~...
    9 	    46 	 0.04342 	 0.05364 	 ~...
   24 	    47 	 0.04761 	 0.05367 	 ~...
    6 	    48 	 0.04257 	 0.05376 	 ~...
   38 	    49 	 0.04984 	 0.05412 	 ~...
   66 	    50 	 0.05428 	 0.05436 	 ~...
   35 	    51 	 0.04924 	 0.05439 	 ~...
   34 	    52 	 0.04903 	 0.05440 	 ~...
   49 	    53 	 0.05348 	 0.05440 	 ~...
   11 	    54 	 0.04417 	 0.05447 	 ~...
   42 	    55 	 0.05124 	 0.05449 	 ~...
   47 	    56 	 0.05260 	 0.05464 	 ~...
   50 	    57 	 0.05394 	 0.05466 	 ~...
   26 	    58 	 0.04794 	 0.05469 	 ~...
   77 	    59 	 0.05933 	 0.05470 	 ~...
   41 	    60 	 0.05081 	 0.05484 	 ~...
   50 	    61 	 0.05394 	 0.05523 	 ~...
    2 	    62 	 0.03948 	 0.05563 	 ~...
   17 	    63 	 0.04621 	 0.05568 	 ~...
   43 	    64 	 0.05158 	 0.05576 	 ~...
   10 	    65 	 0.04391 	 0.05651 	 ~...
   30 	    66 	 0.04840 	 0.05680 	 ~...
    0 	    67 	 0.03626 	 0.05688 	 ~...
    4 	    68 	 0.04209 	 0.05694 	 ~...
   50 	    69 	 0.05394 	 0.05696 	 ~...
    5 	    70 	 0.04238 	 0.05714 	 ~...
   13 	    71 	 0.04465 	 0.05733 	 ~...
   37 	    72 	 0.04975 	 0.05740 	 ~...
   46 	    73 	 0.05232 	 0.05740 	 ~...
   16 	    74 	 0.04620 	 0.05743 	 ~...
   15 	    75 	 0.04478 	 0.05816 	 ~...
   71 	    76 	 0.05558 	 0.05863 	 ~...
    7 	    77 	 0.04292 	 0.05875 	 ~...
   14 	    78 	 0.04470 	 0.05901 	 ~...
   50 	    79 	 0.05394 	 0.05902 	 ~...
   72 	    80 	 0.05647 	 0.05988 	 ~...
   83 	    81 	 0.19234 	 0.18561 	 ~...
   85 	    82 	 0.21805 	 0.20444 	 ~...
   81 	    83 	 0.16290 	 0.20455 	 m..s
   82 	    84 	 0.19222 	 0.20720 	 ~...
   87 	    85 	 0.22226 	 0.22302 	 ~...
   88 	    86 	 0.22303 	 0.22330 	 ~...
   84 	    87 	 0.21151 	 0.22622 	 ~...
   86 	    88 	 0.21838 	 0.22687 	 ~...
   89 	    89 	 0.22627 	 0.24847 	 ~...
   93 	    90 	 0.29536 	 0.25136 	 m..s
   92 	    91 	 0.29441 	 0.27241 	 ~...
   90 	    92 	 0.27678 	 0.27892 	 ~...
   91 	    93 	 0.28220 	 0.28323 	 ~...
   95 	    94 	 0.29818 	 0.28843 	 ~...
  105 	    95 	 0.34233 	 0.28900 	 m..s
   96 	    96 	 0.29943 	 0.29123 	 ~...
   99 	    97 	 0.31034 	 0.29126 	 ~...
   94 	    98 	 0.29752 	 0.29157 	 ~...
   98 	    99 	 0.30682 	 0.29390 	 ~...
   97 	   100 	 0.30381 	 0.29679 	 ~...
  101 	   101 	 0.32129 	 0.30284 	 ~...
  109 	   102 	 0.35614 	 0.31094 	 m..s
  100 	   103 	 0.31705 	 0.31687 	 ~...
  108 	   104 	 0.35216 	 0.32216 	 ~...
  103 	   105 	 0.33298 	 0.33111 	 ~...
  102 	   106 	 0.33254 	 0.33520 	 ~...
  106 	   107 	 0.34241 	 0.33738 	 ~...
  104 	   108 	 0.33462 	 0.34389 	 ~...
  107 	   109 	 0.34456 	 0.35705 	 ~...
  112 	   110 	 0.41013 	 0.37015 	 m..s
  110 	   111 	 0.40443 	 0.41075 	 ~...
  113 	   112 	 0.41932 	 0.42112 	 ~...
  114 	   113 	 0.48272 	 0.45341 	 ~...
  116 	   114 	 0.48443 	 0.46690 	 ~...
  115 	   115 	 0.48398 	 0.47946 	 ~...
  118 	   116 	 0.55643 	 0.48073 	 m..s
  111 	   117 	 0.40753 	 0.48105 	 m..s
  117 	   118 	 0.53005 	 0.48579 	 m..s
  119 	   119 	 0.56074 	 0.53394 	 ~...
  120 	   120 	 0.62450 	 0.61365 	 ~...
==========================================
r_mrr = 0.994500994682312
r2_mrr = 0.9862475991249084
spearmanr_mrr@5 = 0.8765504360198975
spearmanr_mrr@10 = 0.9187413454055786
spearmanr_mrr@50 = 0.9945241212844849
spearmanr_mrr@100 = 0.9975221157073975
spearmanr_mrr@All = 0.9977230429649353
==========================================
test time: 0.456
Done Testing dataset Kinships
total time taken: 232.92478966712952
training time taken: 225.1151795387268
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9945)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9862)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.8766)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9187)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9945)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9975)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9977)}}, 'test_loss': {'ComplEx': {'Kinships': 0.3260312455604435}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7729071668358768
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [940, 203, 865, 35, 586, 305, 1052, 419, 548, 80, 240, 1164, 398, 383, 702, 1187, 565, 280, 867, 416, 819, 928, 562, 194, 883, 846, 892, 651, 431, 687, 601, 860, 902, 391, 344, 648, 767, 255, 872, 525, 87, 544, 335, 1048, 1139, 530, 440, 371, 117, 366, 40, 558, 54, 731, 1085, 365, 813, 510, 1068, 467, 557, 88, 165, 1046, 368, 1161, 1038, 560, 1182, 1090, 1149, 321, 653, 64, 823, 997, 1200, 497, 1167, 498, 696, 382, 1, 930, 1184, 128, 439, 723, 286, 862, 1146, 839, 121, 785, 529, 931, 76, 956, 1002, 802, 506, 658, 1169, 966, 420, 200, 152, 859, 338, 869, 552, 969, 6, 612, 413, 108, 777, 1136, 942, 1011, 204]
valid_ids (0): []
train_ids (1094): [751, 1214, 825, 184, 810, 598, 1155, 597, 1008, 900, 1041, 492, 798, 306, 864, 932, 345, 980, 805, 991, 953, 316, 1088, 977, 201, 1080, 160, 409, 272, 263, 360, 71, 703, 830, 1121, 271, 357, 1020, 155, 1083, 1213, 58, 1058, 622, 124, 1174, 303, 748, 906, 285, 198, 1034, 786, 434, 691, 449, 727, 205, 313, 1079, 415, 1181, 1141, 7, 403, 279, 626, 397, 1007, 1197, 26, 837, 994, 555, 781, 477, 776, 968, 242, 17, 712, 1054, 761, 317, 610, 499, 500, 185, 589, 57, 1190, 646, 127, 1153, 988, 649, 833, 721, 367, 699, 215, 627, 1025, 514, 296, 47, 348, 676, 1006, 1159, 520, 34, 1138, 1180, 504, 941, 1107, 485, 701, 814, 879, 1113, 1099, 1028, 827, 179, 984, 591, 590, 315, 922, 898, 618, 484, 1069, 531, 332, 175, 1032, 461, 995, 588, 1017, 1075, 395, 104, 1142, 78, 599, 1122, 667, 1091, 818, 572, 45, 1133, 519, 443, 469, 351, 292, 1104, 644, 1108, 170, 86, 125, 1015, 172, 281, 301, 522, 384, 331, 302, 473, 871, 993, 592, 518, 278, 550, 491, 325, 1024, 526, 1019, 604, 1065, 532, 845, 105, 100, 650, 577, 523, 107, 425, 311, 1204, 706, 249, 689, 692, 442, 1119, 1071, 949, 634, 451, 771, 513, 16, 923, 1029, 568, 704, 438, 686, 231, 435, 934, 261, 606, 437, 433, 1070, 613, 947, 422, 1196, 732, 584, 624, 299, 161, 166, 1188, 101, 256, 1171, 1074, 1100, 943, 364, 83, 964, 878, 333, 809, 1208, 674, 148, 844, 11, 275, 326, 329, 996, 693, 388, 246, 475, 1117, 669, 489, 816, 801, 115, 393, 159, 234, 1072, 193, 1060, 411, 549, 390, 370, 1152, 149, 962, 911, 5, 169, 319, 600, 840, 441, 739, 874, 528, 65, 540, 73, 685, 652, 665, 736, 593, 547, 831, 153, 248, 406, 886, 855, 1157, 821, 251, 112, 517, 836, 710, 679, 917, 605, 69, 378, 225, 881, 486, 524, 18, 1059, 954, 1150, 647, 223, 921, 683, 468, 615, 9, 129, 14, 151, 150, 749, 747, 570, 664, 247, 1018, 488, 904, 354, 52, 873, 471, 1192, 973, 1145, 578, 1111, 4, 211, 210, 1147, 875, 111, 958, 268, 950, 807, 386, 1185, 655, 412, 1210, 483, 447, 190, 428, 545, 445, 182, 815, 330, 1044, 671, 938, 289, 267, 353, 675, 312, 1000, 998, 714, 120, 794, 1031, 960, 457, 707, 192, 1166, 567, 220, 754, 778, 294, 737, 374, 1125, 429, 630, 539, 654, 569, 717, 718, 156, 677, 269, 72, 899, 542, 979, 177, 1151, 551, 870, 766, 30, 908, 848, 640, 581, 27, 334, 829, 218, 94, 358, 561, 768, 136, 755, 607, 1120, 288, 629, 625, 336, 511, 895, 342, 162, 1179, 349, 868, 999, 487, 219, 913, 93, 834, 284, 402, 910, 207, 959, 70, 527, 765, 1093, 660, 682, 444, 705, 180, 42, 118, 183, 452, 1096, 614, 608, 1170, 213, 926, 1110, 905, 933, 1137, 828, 937, 1055, 282, 1033, 779, 735, 852, 594, 79, 657, 1005, 851, 253, 67, 887, 1203, 232, 1115, 427, 619, 505, 373, 355, 110, 708, 681, 1082, 307, 41, 265, 1012, 212, 1131, 1086, 1168, 1004, 257, 621, 208, 135, 450, 975, 951, 713, 939, 1212, 186, 96, 609, 448, 270, 154, 756, 308, 340, 863, 639, 23, 314, 1073, 603, 227, 479, 1039, 293, 19, 733, 1123, 623, 103, 1076, 343, 241, 888, 481, 273, 961, 812, 436, 290, 59, 1050, 66, 258, 1023, 760, 740, 1081, 1126, 1135, 659, 379, 1105, 199, 971, 698, 244, 596, 914, 493, 967, 1103, 1087, 396, 324, 983, 1143, 912, 889, 298, 700, 143, 206, 1097, 470, 401, 84, 1202, 730, 924, 709, 217, 237, 38, 502, 893, 318, 347, 482, 1027, 1209, 1040, 74, 1183, 1112, 896, 22, 15, 1030, 51, 806, 356, 501, 50, 919, 595, 952, 176, 456, 222, 935, 363, 670, 230, 1094, 1148, 885, 464, 804, 564, 1078, 62, 446, 235, 495, 800, 684, 453, 841, 12, 750, 1092, 187, 1207, 259, 459, 264, 116, 1114, 13, 337, 91, 793, 797, 916, 233, 915, 769, 454, 1211, 1009, 1140, 799, 33, 533, 1102, 1062, 1003, 0, 350, 109, 31, 95, 1061, 795, 417, 1134, 847, 628, 772, 89, 46, 328, 274, 792, 925, 1042, 1084, 1177, 465, 1035, 668, 410, 987, 133, 1056, 141, 796, 729, 817, 1014, 480, 327, 811, 8, 430, 784, 146, 835, 75, 780, 189, 37, 909, 400, 250, 385, 986, 516, 25, 891, 515, 787, 191, 310, 617, 641, 521, 380, 126, 556, 387, 277, 139, 1109, 85, 494, 1098, 339, 842, 876, 77, 982, 632, 746, 119, 509, 826, 1130, 68, 715, 214, 276, 60, 929, 972, 236, 662, 1205, 462, 466, 209, 672, 948, 254, 39, 738, 123, 61, 1124, 260, 164, 167, 1176, 195, 1066, 432, 36, 1156, 376, 304, 890, 178, 965, 56, 575, 508, 145, 507, 228, 957, 690, 587, 1144, 1132, 1198, 49, 266, 537, 541, 359, 1051, 2, 571, 579, 678, 585, 789, 322, 1010, 688, 245, 985, 99, 424, 936, 1045, 90, 770, 142, 476, 1047, 389, 55, 131, 680, 734, 122, 490, 10, 974, 392, 81, 1106, 1154, 144, 407, 976, 1206, 1013, 408, 773, 460, 21, 262, 631, 790, 130, 1101, 381, 1022, 775, 1089, 822, 138, 163, 858, 849, 861, 361, 728, 224, 1158, 758, 743, 323, 1162, 907, 1057, 866, 352, 1037, 132, 229, 216, 636, 291, 602, 643, 346, 820, 741, 496, 1178, 97, 573, 472, 1063, 1128, 989, 535, 638, 978, 238, 377, 1064, 1163, 574, 877, 300, 576, 764, 853, 757, 171, 1172, 1116, 620, 1195, 405, 503, 239, 752, 719, 421, 48, 404, 918, 137, 720, 1199, 763, 546, 854, 543, 1026, 903, 897, 1173, 534, 1043, 458, 20, 43, 226, 394, 536, 656, 1165, 637, 832, 1127, 106, 197, 478, 134, 1016, 243, 1021, 633, 295, 372, 399, 616, 894, 24, 694, 970, 181, 63, 955, 158, 1194, 663, 29, 563, 673, 666, 920, 744, 341, 742, 1201, 418, 695, 320, 173, 808, 375, 850, 252, 369, 287, 945, 856, 774, 1186, 1193, 157, 1189, 283, 963, 553, 583, 753, 1036, 726, 1118, 3, 28, 857, 642, 838, 645, 788, 1049, 414, 221, 990, 782, 697, 114, 455, 725, 147, 582, 1095, 1191, 362, 722, 538, 309, 98, 803, 92, 174, 901, 102, 1175, 566, 202, 759, 635, 1160, 711, 791, 944, 426, 44, 188, 661, 843, 53, 168, 512, 992, 611, 927, 32, 1129, 196, 1001, 882, 297, 423, 580, 474, 824, 554, 880, 884, 463, 1067, 981, 113, 745, 82, 140, 716, 559, 724, 1053, 762, 783, 946, 1077]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5963786955734398
the save name prefix for this run is:  chkpt-ID_5963786955734398_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 912
rank avg (pred): 0.496 +- 0.003
mrr vals (pred, true): 0.019, 0.235
batch losses (mrrl, rdl): 0.0, 0.0023918964

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1079
rank avg (pred): 0.112 +- 0.090
mrr vals (pred, true): 0.242, 0.482
batch losses (mrrl, rdl): 0.0, 3.72681e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 384
rank avg (pred): 0.463 +- 0.264
mrr vals (pred, true): 0.074, 0.050
batch losses (mrrl, rdl): 0.0, 3.5708e-06

Epoch over!
epoch time: 15.078

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 660
rank avg (pred): 0.434 +- 0.265
mrr vals (pred, true): 0.089, 0.051
batch losses (mrrl, rdl): 0.0, 1.63992e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 101
rank avg (pred): 0.430 +- 0.273
mrr vals (pred, true): 0.085, 0.054
batch losses (mrrl, rdl): 0.0, 5.9679e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 43
rank avg (pred): 0.120 +- 0.143
mrr vals (pred, true): 0.243, 0.309
batch losses (mrrl, rdl): 0.0, 6.841e-07

Epoch over!
epoch time: 15.038

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 492
rank avg (pred): 0.147 +- 0.157
mrr vals (pred, true): 0.210, 0.232
batch losses (mrrl, rdl): 0.0, 1.08185e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1026
rank avg (pred): 0.443 +- 0.276
mrr vals (pred, true): 0.071, 0.052
batch losses (mrrl, rdl): 0.0, 1.41827e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 112
rank avg (pred): 0.457 +- 0.264
mrr vals (pred, true): 0.060, 0.049
batch losses (mrrl, rdl): 0.0, 1.2035e-06

Epoch over!
epoch time: 14.898

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 682
rank avg (pred): 0.469 +- 0.264
mrr vals (pred, true): 0.053, 0.058
batch losses (mrrl, rdl): 0.0, 6.658e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 830
rank avg (pred): 0.089 +- 0.124
mrr vals (pred, true): 0.252, 0.405
batch losses (mrrl, rdl): 0.0, 3.2364e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 470
rank avg (pred): 0.474 +- 0.257
mrr vals (pred, true): 0.042, 0.053
batch losses (mrrl, rdl): 0.0, 4.8752e-06

Epoch over!
epoch time: 14.847

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 231
rank avg (pred): 0.416 +- 0.282
mrr vals (pred, true): 0.099, 0.055
batch losses (mrrl, rdl): 0.0, 3.44026e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 126
rank avg (pred): 0.464 +- 0.267
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 0.0, 1.825e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 650
rank avg (pred): 0.471 +- 0.269
mrr vals (pred, true): 0.049, 0.056
batch losses (mrrl, rdl): 0.0, 4.6842e-06

Epoch over!
epoch time: 14.84

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 145
rank avg (pred): 0.470 +- 0.271
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 2.04239e-05, 1.6121e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1172
rank avg (pred): 0.500 +- 0.182
mrr vals (pred, true): 0.044, 0.059
batch losses (mrrl, rdl): 0.0003570624, 7.57833e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 251
rank avg (pred): 0.077 +- 0.117
mrr vals (pred, true): 0.381, 0.347
batch losses (mrrl, rdl): 0.0114295427, 2.8826e-06

Epoch over!
epoch time: 15.07

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1113
rank avg (pred): 0.418 +- 0.144
mrr vals (pred, true): 0.057, 0.057
batch losses (mrrl, rdl): 0.0004709916, 5.12205e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 167
rank avg (pred): 0.460 +- 0.138
mrr vals (pred, true): 0.038, 0.056
batch losses (mrrl, rdl): 0.001390453, 4.63133e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 410
rank avg (pred): 0.431 +- 0.132
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0001113067, 8.42805e-05

Epoch over!
epoch time: 15.067

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 518
rank avg (pred): 0.262 +- 0.203
mrr vals (pred, true): 0.246, 0.239
batch losses (mrrl, rdl): 0.0005815197, 0.0004857341

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 654
rank avg (pred): 0.452 +- 0.150
mrr vals (pred, true): 0.051, 0.056
batch losses (mrrl, rdl): 3.6451e-06, 3.57827e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 403
rank avg (pred): 0.430 +- 0.149
mrr vals (pred, true): 0.060, 0.050
batch losses (mrrl, rdl): 0.0009951313, 4.51714e-05

Epoch over!
epoch time: 15.11

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 499
rank avg (pred): 0.299 +- 0.216
mrr vals (pred, true): 0.235, 0.226
batch losses (mrrl, rdl): 0.0009135123, 0.0006173122

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 619
rank avg (pred): 0.481 +- 0.167
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 5.0257e-06, 3.93892e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 331
rank avg (pred): 0.413 +- 0.139
mrr vals (pred, true): 0.063, 0.055
batch losses (mrrl, rdl): 0.0015765831, 0.000102407

Epoch over!
epoch time: 15.076

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 287
rank avg (pred): 0.157 +- 0.169
mrr vals (pred, true): 0.307, 0.301
batch losses (mrrl, rdl): 0.000389195, 4.82177e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 48
rank avg (pred): 0.289 +- 0.230
mrr vals (pred, true): 0.251, 0.258
batch losses (mrrl, rdl): 0.000483919, 0.0005355965

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 378
rank avg (pred): 0.434 +- 0.157
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 3.04931e-05, 6.63444e-05

Epoch over!
epoch time: 15.052

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 960
rank avg (pred): 0.493 +- 0.205
mrr vals (pred, true): 0.046, 0.051
batch losses (mrrl, rdl): 0.0001434481, 2.43381e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 430
rank avg (pred): 0.459 +- 0.196
mrr vals (pred, true): 0.055, 0.052
batch losses (mrrl, rdl): 0.0002745679, 2.78123e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 561
rank avg (pred): 0.376 +- 0.291
mrr vals (pred, true): 0.238, 0.195
batch losses (mrrl, rdl): 0.0180245377, 0.0008744244

Epoch over!
epoch time: 15.09

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 86
rank avg (pred): 0.410 +- 0.117
mrr vals (pred, true): 0.048, 0.052
batch losses (mrrl, rdl): 3.84875e-05, 0.0001423985

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 242
rank avg (pred): 0.507 +- 0.230
mrr vals (pred, true): 0.047, 0.056
batch losses (mrrl, rdl): 8.26794e-05, 2.59471e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 995
rank avg (pred): 0.099 +- 0.141
mrr vals (pred, true): 0.386, 0.458
batch losses (mrrl, rdl): 0.0531011745, 2.50609e-05

Epoch over!
epoch time: 15.067

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 910
rank avg (pred): 0.187 +- 0.169
mrr vals (pred, true): 0.293, 0.245
batch losses (mrrl, rdl): 0.023038106, 9.85594e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 736
rank avg (pred): 0.148 +- 0.212
mrr vals (pred, true): 0.368, 0.162
batch losses (mrrl, rdl): 0.423422128, 6.78549e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 576
rank avg (pred): 0.449 +- 0.179
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 2.1571e-06, 2.89381e-05

Epoch over!
epoch time: 15.017

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 229
rank avg (pred): 0.433 +- 0.133
mrr vals (pred, true): 0.045, 0.053
batch losses (mrrl, rdl): 0.0002897079, 8.13209e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 898
rank avg (pred): 0.495 +- 0.326
mrr vals (pred, true): 0.194, 0.104
batch losses (mrrl, rdl): 0.0812070742, 0.0011334631

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 745
rank avg (pred): 0.162 +- 0.216
mrr vals (pred, true): 0.343, 0.324
batch losses (mrrl, rdl): 0.0036960985, 4.69817e-05

Epoch over!
epoch time: 15.031

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 780
rank avg (pred): 0.491 +- 0.203
mrr vals (pred, true): 0.044, 0.055
batch losses (mrrl, rdl): 0.0004024187, 2.17102e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 441
rank avg (pred): 0.436 +- 0.167
mrr vals (pred, true): 0.047, 0.056
batch losses (mrrl, rdl): 7.93384e-05, 4.80143e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 269
rank avg (pred): 0.181 +- 0.224
mrr vals (pred, true): 0.310, 0.353
batch losses (mrrl, rdl): 0.0186620802, 0.0001350203

Epoch over!
epoch time: 15.055

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.504 +- 0.260
mrr vals (pred, true): 0.046, 0.043

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   62 	     0 	 0.04589 	 0.04268 	 ~...
   11 	     1 	 0.04306 	 0.04495 	 ~...
   80 	     2 	 0.04735 	 0.04544 	 ~...
   56 	     3 	 0.04552 	 0.04696 	 ~...
   72 	     4 	 0.04628 	 0.04731 	 ~...
   75 	     5 	 0.04659 	 0.04763 	 ~...
   23 	     6 	 0.04382 	 0.04772 	 ~...
   21 	     7 	 0.04376 	 0.04783 	 ~...
    8 	     8 	 0.04286 	 0.04882 	 ~...
   14 	     9 	 0.04325 	 0.04893 	 ~...
   58 	    10 	 0.04569 	 0.04905 	 ~...
   52 	    11 	 0.04524 	 0.04922 	 ~...
   63 	    12 	 0.04595 	 0.04931 	 ~...
   10 	    13 	 0.04291 	 0.04943 	 ~...
   66 	    14 	 0.04603 	 0.04947 	 ~...
   22 	    15 	 0.04380 	 0.04948 	 ~...
   73 	    16 	 0.04653 	 0.04950 	 ~...
   17 	    17 	 0.04355 	 0.04989 	 ~...
   77 	    18 	 0.04697 	 0.05014 	 ~...
   19 	    19 	 0.04369 	 0.05059 	 ~...
   35 	    20 	 0.04438 	 0.05090 	 ~...
   31 	    21 	 0.04434 	 0.05110 	 ~...
   71 	    22 	 0.04626 	 0.05114 	 ~...
   53 	    23 	 0.04525 	 0.05121 	 ~...
   54 	    24 	 0.04537 	 0.05138 	 ~...
   78 	    25 	 0.04701 	 0.05147 	 ~...
   65 	    26 	 0.04602 	 0.05154 	 ~...
   28 	    27 	 0.04410 	 0.05158 	 ~...
   13 	    28 	 0.04321 	 0.05161 	 ~...
   76 	    29 	 0.04674 	 0.05186 	 ~...
   39 	    30 	 0.04451 	 0.05212 	 ~...
   59 	    31 	 0.04571 	 0.05216 	 ~...
    1 	    32 	 0.04184 	 0.05224 	 ~...
   40 	    33 	 0.04452 	 0.05235 	 ~...
    7 	    34 	 0.04274 	 0.05242 	 ~...
   30 	    35 	 0.04433 	 0.05265 	 ~...
    2 	    36 	 0.04222 	 0.05274 	 ~...
   51 	    37 	 0.04524 	 0.05275 	 ~...
   38 	    38 	 0.04449 	 0.05277 	 ~...
   12 	    39 	 0.04315 	 0.05280 	 ~...
   41 	    40 	 0.04463 	 0.05286 	 ~...
   67 	    41 	 0.04607 	 0.05290 	 ~...
   36 	    42 	 0.04442 	 0.05321 	 ~...
   47 	    43 	 0.04504 	 0.05351 	 ~...
   18 	    44 	 0.04366 	 0.05362 	 ~...
   34 	    45 	 0.04438 	 0.05363 	 ~...
   55 	    46 	 0.04551 	 0.05364 	 ~...
   79 	    47 	 0.04714 	 0.05374 	 ~...
   70 	    48 	 0.04614 	 0.05376 	 ~...
   44 	    49 	 0.04494 	 0.05381 	 ~...
   33 	    50 	 0.04437 	 0.05392 	 ~...
   48 	    51 	 0.04508 	 0.05406 	 ~...
    5 	    52 	 0.04255 	 0.05434 	 ~...
   60 	    53 	 0.04585 	 0.05456 	 ~...
   16 	    54 	 0.04354 	 0.05463 	 ~...
   61 	    55 	 0.04589 	 0.05470 	 ~...
   24 	    56 	 0.04383 	 0.05490 	 ~...
   69 	    57 	 0.04614 	 0.05495 	 ~...
   64 	    58 	 0.04596 	 0.05501 	 ~...
   50 	    59 	 0.04519 	 0.05501 	 ~...
   57 	    60 	 0.04558 	 0.05524 	 ~...
    0 	    61 	 0.04118 	 0.05565 	 ~...
    4 	    62 	 0.04252 	 0.05566 	 ~...
   27 	    63 	 0.04396 	 0.05585 	 ~...
   32 	    64 	 0.04436 	 0.05631 	 ~...
   68 	    65 	 0.04611 	 0.05651 	 ~...
   15 	    66 	 0.04328 	 0.05656 	 ~...
   45 	    67 	 0.04500 	 0.05660 	 ~...
   25 	    68 	 0.04384 	 0.05682 	 ~...
   43 	    69 	 0.04485 	 0.05685 	 ~...
   74 	    70 	 0.04654 	 0.05688 	 ~...
    3 	    71 	 0.04238 	 0.05723 	 ~...
   42 	    72 	 0.04481 	 0.05725 	 ~...
   49 	    73 	 0.04516 	 0.05825 	 ~...
   20 	    74 	 0.04370 	 0.05863 	 ~...
   46 	    75 	 0.04502 	 0.05902 	 ~...
   26 	    76 	 0.04391 	 0.05988 	 ~...
    6 	    77 	 0.04264 	 0.06021 	 ~...
    9 	    78 	 0.04290 	 0.06049 	 ~...
   37 	    79 	 0.04446 	 0.06181 	 ~...
   29 	    80 	 0.04429 	 0.06325 	 ~...
   83 	    81 	 0.19549 	 0.18556 	 ~...
   82 	    82 	 0.19088 	 0.18561 	 ~...
   89 	    83 	 0.21655 	 0.19332 	 ~...
   88 	    84 	 0.21016 	 0.20204 	 ~...
   87 	    85 	 0.20965 	 0.20266 	 ~...
   81 	    86 	 0.19015 	 0.20455 	 ~...
   93 	    87 	 0.23758 	 0.20831 	 ~...
   84 	    88 	 0.19590 	 0.21072 	 ~...
   86 	    89 	 0.20731 	 0.21194 	 ~...
   91 	    90 	 0.22077 	 0.21833 	 ~...
   85 	    91 	 0.20372 	 0.22043 	 ~...
   90 	    92 	 0.22034 	 0.23082 	 ~...
   94 	    93 	 0.23802 	 0.23297 	 ~...
   99 	    94 	 0.25603 	 0.23386 	 ~...
   92 	    95 	 0.22405 	 0.23947 	 ~...
  107 	    96 	 0.27429 	 0.24886 	 ~...
   97 	    97 	 0.25076 	 0.26724 	 ~...
  100 	    98 	 0.25683 	 0.26738 	 ~...
   96 	    99 	 0.24663 	 0.27892 	 m..s
   95 	   100 	 0.24386 	 0.27975 	 m..s
  111 	   101 	 0.30289 	 0.27991 	 ~...
  101 	   102 	 0.25818 	 0.28244 	 ~...
  103 	   103 	 0.26180 	 0.28520 	 ~...
  108 	   104 	 0.28949 	 0.29126 	 ~...
  104 	   105 	 0.26286 	 0.29157 	 ~...
  102 	   106 	 0.26094 	 0.29634 	 m..s
  105 	   107 	 0.26303 	 0.29813 	 m..s
  106 	   108 	 0.27114 	 0.30095 	 ~...
  113 	   109 	 0.31316 	 0.30803 	 ~...
  112 	   110 	 0.31305 	 0.31900 	 ~...
   98 	   111 	 0.25449 	 0.32078 	 m..s
  109 	   112 	 0.29777 	 0.34296 	 m..s
  110 	   113 	 0.30029 	 0.34880 	 m..s
  115 	   114 	 0.39831 	 0.42736 	 ~...
  114 	   115 	 0.37301 	 0.44244 	 m..s
  116 	   116 	 0.40382 	 0.44883 	 m..s
  117 	   117 	 0.44299 	 0.46828 	 ~...
  118 	   118 	 0.45421 	 0.51634 	 m..s
  120 	   119 	 0.64287 	 0.61786 	 ~...
  119 	   120 	 0.57431 	 0.61829 	 m..s
==========================================
r_mrr = 0.9934824109077454
r2_mrr = 0.9794799089431763
spearmanr_mrr@5 = 0.9560196399688721
spearmanr_mrr@10 = 0.9775970578193665
spearmanr_mrr@50 = 0.9932233691215515
spearmanr_mrr@100 = 0.9966847896575928
spearmanr_mrr@All = 0.9969480037689209
==========================================
test time: 0.453
Done Testing dataset Kinships
total time taken: 233.50045371055603
training time taken: 225.80034852027893
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9935)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9795)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9560)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9776)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9932)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9967)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9969)}}, 'test_loss': {'ComplEx': {'Kinships': 0.3986652349776705}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 2087341477706789
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1131, 719, 180, 534, 362, 582, 406, 49, 30, 260, 654, 533, 454, 668, 435, 136, 1038, 1115, 428, 573, 1059, 526, 1135, 1161, 1136, 446, 282, 1064, 786, 1175, 297, 448, 767, 660, 880, 672, 1182, 442, 445, 319, 1090, 188, 1104, 352, 1148, 381, 314, 749, 935, 639, 1058, 1033, 66, 870, 1114, 345, 1036, 1091, 1128, 731, 214, 389, 433, 1177, 588, 970, 845, 696, 353, 713, 962, 1032, 122, 1031, 889, 368, 738, 158, 1194, 185, 784, 558, 431, 360, 865, 43, 648, 16, 825, 739, 929, 727, 1169, 456, 1139, 882, 896, 1041, 186, 290, 421, 357, 472, 1189, 398, 513, 22, 1009, 135, 1086, 516, 1053, 225, 793, 597, 229, 1068, 56, 817, 333, 804]
valid_ids (0): []
train_ids (1094): [1127, 625, 438, 1003, 443, 688, 584, 647, 855, 768, 462, 670, 1039, 336, 169, 753, 805, 565, 58, 9, 591, 497, 24, 594, 969, 1142, 973, 227, 892, 808, 71, 423, 531, 1179, 750, 385, 177, 194, 971, 1082, 850, 467, 1055, 557, 34, 1207, 875, 261, 243, 702, 15, 479, 482, 1022, 154, 1019, 240, 107, 546, 709, 583, 1170, 26, 1203, 405, 394, 890, 1134, 128, 887, 1147, 239, 644, 88, 296, 1000, 867, 280, 488, 155, 139, 72, 40, 1085, 152, 20, 555, 697, 525, 972, 537, 1210, 790, 129, 824, 795, 1071, 359, 329, 550, 404, 545, 1073, 759, 674, 328, 919, 383, 208, 800, 690, 121, 1010, 1101, 838, 599, 407, 632, 311, 1046, 821, 595, 755, 1057, 102, 653, 903, 473, 118, 138, 737, 1008, 344, 918, 293, 905, 695, 572, 980, 444, 846, 1199, 891, 411, 106, 848, 560, 1049, 274, 922, 210, 209, 952, 150, 1099, 728, 54, 313, 600, 1097, 221, 1163, 763, 337, 272, 942, 267, 940, 590, 1196, 104, 1119, 37, 832, 553, 592, 593, 12, 163, 351, 307, 125, 736, 377, 187, 224, 1125, 914, 975, 89, 233, 1001, 748, 778, 758, 1020, 413, 801, 965, 735, 770, 162, 782, 91, 315, 103, 449, 602, 764, 202, 999, 205, 618, 977, 577, 363, 430, 387, 396, 953, 603, 938, 292, 252, 90, 245, 1109, 643, 773, 447, 478, 429, 923, 666, 141, 756, 571, 1074, 1080, 628, 716, 327, 989, 204, 528, 1, 646, 190, 957, 686, 101, 112, 524, 857, 461, 212, 655, 966, 1112, 258, 909, 217, 536, 1171, 98, 171, 564, 1056, 301, 403, 1120, 1117, 140, 495, 802, 1193, 1111, 123, 1197, 420, 500, 341, 343, 829, 853, 1105, 1072, 601, 28, 757, 788, 833, 1106, 60, 415, 623, 985, 798, 1116, 393, 651, 835, 97, 324, 694, 774, 146, 700, 634, 743, 1202, 1140, 105, 1152, 160, 562, 269, 111, 55, 687, 254, 250, 2, 1211, 32, 94, 742, 611, 199, 303, 943, 609, 399, 132, 51, 1156, 538, 80, 904, 263, 883, 656, 561, 1100, 249, 235, 412, 391, 510, 663, 598, 1123, 1002, 807, 35, 285, 1145, 417, 785, 416, 349, 616, 920, 876, 326, 69, 715, 636, 59, 554, 149, 885, 916, 578, 453, 509, 316, 499, 476, 242, 858, 1084, 166, 771, 745, 681, 703, 1153, 1167, 287, 680, 1006, 752, 300, 976, 939, 859, 1176, 77, 685, 7, 523, 791, 304, 535, 1209, 926, 1118, 1201, 419, 1204, 1208, 161, 854, 330, 514, 792, 1122, 126, 1113, 606, 1035, 946, 987, 959, 678, 4, 979, 127, 621, 286, 675, 871, 724, 789, 997, 271, 1077, 207, 388, 459, 995, 677, 33, 541, 455, 318, 803, 219, 153, 366, 458, 563, 1093, 8, 721, 321, 1029, 507, 1184, 119, 772, 908, 676, 947, 839, 506, 740, 305, 504, 799, 197, 57, 827, 130, 944, 134, 256, 951, 480, 559, 775, 813, 949, 860, 266, 568, 1137, 92, 819, 116, 452, 1070, 990, 376, 331, 502, 222, 1047, 378, 450, 1164, 355, 1124, 61, 3, 1166, 732, 1165, 607, 174, 75, 512, 604, 532, 822, 1052, 991, 338, 569, 640, 203, 556, 41, 373, 769, 983, 361, 165, 487, 1107, 131, 1158, 734, 856, 667, 426, 1027, 895, 884, 956, 936, 408, 705, 610, 1014, 191, 868, 626, 317, 661, 1198, 810, 996, 1045, 629, 701, 751, 86, 682, 501, 347, 761, 1069, 234, 852, 156, 649, 1007, 469, 549, 1185, 552, 907, 963, 196, 226, 124, 893, 994, 760, 707, 1013, 142, 729, 1205, 465, 284, 615, 605, 527, 818, 613, 236, 776, 79, 400, 877, 659, 1021, 230, 794, 878, 27, 539, 1102, 114, 746, 783, 1030, 25, 924, 109, 21, 517, 457, 974, 439, 570, 998, 812, 237, 1040, 145, 993, 567, 1061, 159, 503, 862, 650, 664, 1044, 830, 170, 1004, 281, 251, 372, 619, 950, 722, 11, 879, 726, 14, 1126, 382, 99, 652, 367, 836, 1149, 87, 780, 566, 576, 18, 706, 712, 1213, 179, 1011, 714, 520, 295, 365, 1138, 1129, 108, 247, 547, 933, 1181, 172, 1005, 580, 1043, 238, 23, 492, 183, 1130, 548, 978, 1172, 268, 522, 490, 1195, 496, 1180, 110, 491, 255, 216, 955, 948, 1078, 828, 181, 228, 1162, 837, 1186, 692, 189, 288, 1075, 100, 143, 115, 1095, 967, 861, 1168, 45, 277, 485, 954, 117, 386, 777, 1024, 323, 44, 1087, 515, 241, 635, 873, 657, 375, 0, 1025, 814, 466, 67, 894, 1191, 133, 63, 299, 289, 354, 587, 586, 913, 340, 74, 866, 992, 816, 711, 48, 1103, 310, 410, 708, 575, 1092, 941, 1212, 471, 463, 574, 96, 881, 42, 342, 542, 278, 1200, 718, 521, 662, 1094, 910, 70, 671, 1121, 725, 73, 358, 834, 38, 620, 425, 427, 397, 13, 52, 809, 275, 864, 374, 232, 62, 1155, 151, 231, 691, 630, 811, 698, 765, 246, 717, 787, 470, 669, 831, 1067, 346, 356, 432, 309, 164, 218, 322, 1060, 308, 1098, 257, 863, 380, 401, 53, 474, 1048, 930, 481, 744, 988, 508, 422, 928, 291, 283, 849, 120, 544, 518, 1026, 826, 184, 900, 684, 1141, 148, 897, 766, 76, 19, 982, 223, 505, 781, 451, 84, 902, 665, 434, 596, 402, 642, 981, 638, 1062, 1066, 1018, 530, 608, 741, 440, 392, 540, 325, 206, 872, 201, 658, 464, 641, 306, 175, 1174, 1081, 1012, 797, 475, 1160, 1042, 424, 1159, 1178, 178, 898, 81, 312, 1110, 5, 921, 710, 36, 371, 47, 248, 617, 441, 265, 543, 390, 1150, 198, 631, 915, 1015, 689, 511, 6, 418, 95, 262, 335, 484, 1076, 1079, 1146, 409, 730, 673, 82, 622, 612, 1051, 264, 843, 931, 984, 182, 961, 200, 320, 1023, 579, 78, 215, 193, 841, 779, 294, 699, 899, 173, 645, 364, 350, 85, 1037, 1206, 276, 958, 192, 494, 796, 901, 83, 437, 869, 820, 348, 436, 298, 934, 851, 1132, 806, 720, 723, 679, 815, 1054, 468, 302, 874, 906, 65, 1183, 1190, 1154, 31, 195, 384, 414, 633, 519, 379, 986, 489, 147, 113, 273, 220, 259, 1133, 581, 1143, 529, 964, 144, 176, 886, 683, 844, 339, 927, 1017, 1088, 733, 925, 213, 334, 370, 395, 483, 960, 211, 253, 1192, 627, 823, 244, 1151, 840, 460, 637, 68, 551, 754, 911, 168, 137, 64, 270, 493, 1034, 46, 704, 937, 167, 932, 29, 912, 968, 1187, 486, 157, 1083, 1096, 1065, 1016, 1144, 693, 847, 1050, 279, 50, 1063, 477, 498, 332, 1108, 93, 945, 842, 1173, 589, 624, 17, 1214, 917, 1028, 585, 10, 888, 614, 1157, 369, 39, 747, 762, 1188, 1089]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3736844726192935
the save name prefix for this run is:  chkpt-ID_3736844726192935_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 921
rank avg (pred): 0.578 +- 0.002
mrr vals (pred, true): 0.017, 0.056
batch losses (mrrl, rdl): 0.0, 0.000316154

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1191
rank avg (pred): 0.449 +- 0.206
mrr vals (pred, true): 0.084, 0.057
batch losses (mrrl, rdl): 0.0, 1.92148e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1075
rank avg (pred): 0.045 +- 0.031
mrr vals (pred, true): 0.361, 0.448
batch losses (mrrl, rdl): 0.0, 2.59594e-05

Epoch over!
epoch time: 15.036

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 123
rank avg (pred): 0.430 +- 0.259
mrr vals (pred, true): 0.169, 0.048
batch losses (mrrl, rdl): 0.0, 1.56743e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 195
rank avg (pred): 0.444 +- 0.260
mrr vals (pred, true): 0.157, 0.060
batch losses (mrrl, rdl): 0.0, 8.5408e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 924
rank avg (pred): 0.457 +- 0.255
mrr vals (pred, true): 0.137, 0.047
batch losses (mrrl, rdl): 0.0, 5.6302e-06

Epoch over!
epoch time: 15.037

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1171
rank avg (pred): 0.440 +- 0.263
mrr vals (pred, true): 0.153, 0.045
batch losses (mrrl, rdl): 0.0, 6.0933e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 257
rank avg (pred): 0.087 +- 0.061
mrr vals (pred, true): 0.294, 0.369
batch losses (mrrl, rdl): 0.0, 4.5458e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1138
rank avg (pred): 0.130 +- 0.093
mrr vals (pred, true): 0.277, 0.310
batch losses (mrrl, rdl): 0.0, 2.27614e-05

Epoch over!
epoch time: 14.878

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1062
rank avg (pred): 0.107 +- 0.074
mrr vals (pred, true): 0.274, 0.358
batch losses (mrrl, rdl): 0.0, 5.6264e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 925
rank avg (pred): 0.463 +- 0.253
mrr vals (pred, true): 0.104, 0.058
batch losses (mrrl, rdl): 0.0, 7.1225e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 952
rank avg (pred): 0.461 +- 0.256
mrr vals (pred, true): 0.116, 0.053
batch losses (mrrl, rdl): 0.0, 1.51386e-05

Epoch over!
epoch time: 14.753

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 346
rank avg (pred): 0.427 +- 0.271
mrr vals (pred, true): 0.156, 0.051
batch losses (mrrl, rdl): 0.0, 1.79807e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1165
rank avg (pred): 0.456 +- 0.261
mrr vals (pred, true): 0.119, 0.051
batch losses (mrrl, rdl): 0.0, 1.98722e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 926
rank avg (pred): 0.460 +- 0.245
mrr vals (pred, true): 0.100, 0.044
batch losses (mrrl, rdl): 0.0, 5.2157e-06

Epoch over!
epoch time: 14.829

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1198
rank avg (pred): 0.457 +- 0.258
mrr vals (pred, true): 0.105, 0.053
batch losses (mrrl, rdl): 0.0297712237, 1.02416e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 6
rank avg (pred): 0.057 +- 0.039
mrr vals (pred, true): 0.331, 0.319
batch losses (mrrl, rdl): 0.0013596208, 7.24959e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 291
rank avg (pred): 0.102 +- 0.071
mrr vals (pred, true): 0.287, 0.276
batch losses (mrrl, rdl): 0.0010523368, 3.94387e-05

Epoch over!
epoch time: 15.023

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 760
rank avg (pred): 0.379 +- 0.147
mrr vals (pred, true): 0.069, 0.055
batch losses (mrrl, rdl): 0.0035615393, 0.0001362771

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 802
rank avg (pred): 0.415 +- 0.138
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.4e-09, 8.71718e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1106
rank avg (pred): 0.440 +- 0.171
mrr vals (pred, true): 0.065, 0.053
batch losses (mrrl, rdl): 0.002155974, 3.83265e-05

Epoch over!
epoch time: 15.038

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 31
rank avg (pred): 0.098 +- 0.068
mrr vals (pred, true): 0.289, 0.327
batch losses (mrrl, rdl): 0.014501527, 3.9781e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 743
rank avg (pred): 0.050 +- 0.035
mrr vals (pred, true): 0.359, 0.390
batch losses (mrrl, rdl): 0.0094470344, 4.10274e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1188
rank avg (pred): 0.413 +- 0.118
mrr vals (pred, true): 0.038, 0.056
batch losses (mrrl, rdl): 0.0013272667, 0.0001015765

Epoch over!
epoch time: 15.01

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 641
rank avg (pred): 0.437 +- 0.170
mrr vals (pred, true): 0.067, 0.059
batch losses (mrrl, rdl): 0.0027247723, 3.634e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 794
rank avg (pred): 0.412 +- 0.128
mrr vals (pred, true): 0.044, 0.054
batch losses (mrrl, rdl): 0.0003485502, 8.87503e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1187
rank avg (pred): 0.440 +- 0.138
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.000200631, 5.30442e-05

Epoch over!
epoch time: 15.209

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 349
rank avg (pred): 0.412 +- 0.132
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002619115, 0.0001430517

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 900
rank avg (pred): 0.156 +- 0.113
mrr vals (pred, true): 0.277, 0.254
batch losses (mrrl, rdl): 0.0051748478, 2.59386e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1133
rank avg (pred): 0.350 +- 0.144
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 2.0928e-06, 0.0003214076

Epoch over!
epoch time: 14.96

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 13
rank avg (pred): 0.054 +- 0.039
mrr vals (pred, true): 0.349, 0.309
batch losses (mrrl, rdl): 0.0154409371, 9.90304e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 183
rank avg (pred): 0.339 +- 0.168
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 6.8501e-06, 0.0003394137

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 317
rank avg (pred): 0.157 +- 0.117
mrr vals (pred, true): 0.285, 0.304
batch losses (mrrl, rdl): 0.0037394371, 2.87277e-05

Epoch over!
epoch time: 14.971

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 806
rank avg (pred): 0.357 +- 0.173
mrr vals (pred, true): 0.044, 0.051
batch losses (mrrl, rdl): 0.000319956, 0.000317544

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 776
rank avg (pred): 0.313 +- 0.193
mrr vals (pred, true): 0.057, 0.053
batch losses (mrrl, rdl): 0.0004753355, 0.0004693223

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1049
rank avg (pred): 0.412 +- 0.168
mrr vals (pred, true): 0.057, 0.052
batch losses (mrrl, rdl): 0.0004736398, 7.37401e-05

Epoch over!
epoch time: 14.977

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 329
rank avg (pred): 0.432 +- 0.157
mrr vals (pred, true): 0.042, 0.053
batch losses (mrrl, rdl): 0.0005885331, 6.13051e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1172
rank avg (pred): 0.403 +- 0.176
mrr vals (pred, true): 0.041, 0.059
batch losses (mrrl, rdl): 0.0008058712, 8.91633e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 960
rank avg (pred): 0.344 +- 0.224
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 6.5667e-06, 0.0002574176

Epoch over!
epoch time: 15.044

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 890
rank avg (pred): 0.412 +- 0.177
mrr vals (pred, true): 0.041, 0.047
batch losses (mrrl, rdl): 0.0007648812, 0.0001338919

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 862
rank avg (pred): 0.393 +- 0.194
mrr vals (pred, true): 0.044, 0.053
batch losses (mrrl, rdl): 0.0003345678, 0.0001097053

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1132
rank avg (pred): 0.409 +- 0.173
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0001471837, 6.96346e-05

Epoch over!
epoch time: 15.021

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 335
rank avg (pred): 0.429 +- 0.165
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002901528, 7.00213e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 263
rank avg (pred): 0.055 +- 0.043
mrr vals (pred, true): 0.392, 0.345
batch losses (mrrl, rdl): 0.022545049, 5.506e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 453
rank avg (pred): 0.428 +- 0.172
mrr vals (pred, true): 0.046, 0.055
batch losses (mrrl, rdl): 0.0001520084, 4.22416e-05

Epoch over!
epoch time: 15.015

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.431 +- 0.170
mrr vals (pred, true): 0.049, 0.052

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   14 	     0 	 0.04416 	 0.04495 	 ~...
   40 	     1 	 0.04594 	 0.04716 	 ~...
   82 	     2 	 0.05316 	 0.04744 	 ~...
   79 	     3 	 0.05219 	 0.04751 	 ~...
   16 	     4 	 0.04434 	 0.04783 	 ~...
   12 	     5 	 0.04409 	 0.04803 	 ~...
   61 	     6 	 0.04890 	 0.04905 	 ~...
   66 	     7 	 0.05001 	 0.04910 	 ~...
   24 	     8 	 0.04481 	 0.04915 	 ~...
   42 	     9 	 0.04605 	 0.04925 	 ~...
   19 	    10 	 0.04450 	 0.04947 	 ~...
   22 	    11 	 0.04475 	 0.04950 	 ~...
   56 	    12 	 0.04798 	 0.04983 	 ~...
   84 	    13 	 0.05498 	 0.04989 	 ~...
   74 	    14 	 0.05118 	 0.04998 	 ~...
   76 	    15 	 0.05160 	 0.05067 	 ~...
   51 	    16 	 0.04742 	 0.05076 	 ~...
   83 	    17 	 0.05411 	 0.05090 	 ~...
    0 	    18 	 0.04179 	 0.05121 	 ~...
   49 	    19 	 0.04716 	 0.05129 	 ~...
   21 	    20 	 0.04459 	 0.05129 	 ~...
   72 	    21 	 0.05094 	 0.05136 	 ~...
   33 	    22 	 0.04499 	 0.05155 	 ~...
    7 	    23 	 0.04373 	 0.05157 	 ~...
   68 	    24 	 0.05012 	 0.05179 	 ~...
   28 	    25 	 0.04489 	 0.05190 	 ~...
   70 	    26 	 0.05025 	 0.05194 	 ~...
   65 	    27 	 0.04982 	 0.05195 	 ~...
   59 	    28 	 0.04868 	 0.05212 	 ~...
    8 	    29 	 0.04373 	 0.05237 	 ~...
   17 	    30 	 0.04440 	 0.05250 	 ~...
    9 	    31 	 0.04381 	 0.05265 	 ~...
   46 	    32 	 0.04687 	 0.05274 	 ~...
   64 	    33 	 0.04961 	 0.05302 	 ~...
   15 	    34 	 0.04420 	 0.05309 	 ~...
   58 	    35 	 0.04865 	 0.05331 	 ~...
   18 	    36 	 0.04447 	 0.05338 	 ~...
   78 	    37 	 0.05201 	 0.05349 	 ~...
   62 	    38 	 0.04915 	 0.05358 	 ~...
   20 	    39 	 0.04453 	 0.05365 	 ~...
   77 	    40 	 0.05179 	 0.05374 	 ~...
    2 	    41 	 0.04261 	 0.05376 	 ~...
   29 	    42 	 0.04490 	 0.05401 	 ~...
   43 	    43 	 0.04641 	 0.05404 	 ~...
   27 	    44 	 0.04488 	 0.05412 	 ~...
   80 	    45 	 0.05220 	 0.05427 	 ~...
   38 	    46 	 0.04548 	 0.05428 	 ~...
   55 	    47 	 0.04797 	 0.05434 	 ~...
   60 	    48 	 0.04869 	 0.05439 	 ~...
   11 	    49 	 0.04403 	 0.05447 	 ~...
   67 	    50 	 0.05010 	 0.05456 	 ~...
    6 	    51 	 0.04370 	 0.05459 	 ~...
   25 	    52 	 0.04481 	 0.05464 	 ~...
   63 	    53 	 0.04936 	 0.05470 	 ~...
   75 	    54 	 0.05142 	 0.05503 	 ~...
   48 	    55 	 0.04710 	 0.05509 	 ~...
   54 	    56 	 0.04789 	 0.05526 	 ~...
   44 	    57 	 0.04647 	 0.05528 	 ~...
   30 	    58 	 0.04490 	 0.05531 	 ~...
   69 	    59 	 0.05013 	 0.05578 	 ~...
   23 	    60 	 0.04479 	 0.05618 	 ~...
   13 	    61 	 0.04412 	 0.05630 	 ~...
   10 	    62 	 0.04403 	 0.05647 	 ~...
   52 	    63 	 0.04753 	 0.05653 	 ~...
   50 	    64 	 0.04723 	 0.05660 	 ~...
   37 	    65 	 0.04534 	 0.05673 	 ~...
    4 	    66 	 0.04347 	 0.05687 	 ~...
   57 	    67 	 0.04837 	 0.05723 	 ~...
   53 	    68 	 0.04773 	 0.05733 	 ~...
    3 	    69 	 0.04339 	 0.05735 	 ~...
   81 	    70 	 0.05282 	 0.05743 	 ~...
   39 	    71 	 0.04586 	 0.05766 	 ~...
   34 	    72 	 0.04506 	 0.05770 	 ~...
   73 	    73 	 0.05110 	 0.05773 	 ~...
    1 	    74 	 0.04184 	 0.05796 	 ~...
   35 	    75 	 0.04512 	 0.05800 	 ~...
   85 	    76 	 0.05736 	 0.05814 	 ~...
   71 	    77 	 0.05071 	 0.05863 	 ~...
   26 	    78 	 0.04486 	 0.05902 	 ~...
   31 	    79 	 0.04494 	 0.05918 	 ~...
   41 	    80 	 0.04601 	 0.05964 	 ~...
   45 	    81 	 0.04649 	 0.06053 	 ~...
    5 	    82 	 0.04352 	 0.06111 	 ~...
   32 	    83 	 0.04499 	 0.06163 	 ~...
   47 	    84 	 0.04693 	 0.06253 	 ~...
   36 	    85 	 0.04512 	 0.06568 	 ~...
   92 	    86 	 0.23379 	 0.16633 	 m..s
   86 	    87 	 0.21988 	 0.18561 	 m..s
   86 	    88 	 0.21988 	 0.19886 	 ~...
   86 	    89 	 0.21988 	 0.20447 	 ~...
   86 	    90 	 0.21988 	 0.21521 	 ~...
   86 	    91 	 0.21988 	 0.21874 	 ~...
   86 	    92 	 0.21988 	 0.22061 	 ~...
   95 	    93 	 0.27441 	 0.26724 	 ~...
   94 	    94 	 0.27382 	 0.27513 	 ~...
   96 	    95 	 0.27472 	 0.28083 	 ~...
   93 	    96 	 0.26675 	 0.28148 	 ~...
  103 	    97 	 0.30240 	 0.28388 	 ~...
   98 	    98 	 0.28793 	 0.28658 	 ~...
   99 	    99 	 0.28888 	 0.28764 	 ~...
  104 	   100 	 0.31114 	 0.29748 	 ~...
  106 	   101 	 0.32008 	 0.29813 	 ~...
  101 	   102 	 0.29147 	 0.30230 	 ~...
   97 	   103 	 0.28610 	 0.30510 	 ~...
  110 	   104 	 0.35460 	 0.30588 	 m..s
  102 	   105 	 0.29398 	 0.30931 	 ~...
  107 	   106 	 0.32466 	 0.31529 	 ~...
  100 	   107 	 0.29060 	 0.32389 	 m..s
  105 	   108 	 0.31980 	 0.34025 	 ~...
  108 	   109 	 0.33339 	 0.36586 	 m..s
  113 	   110 	 0.38406 	 0.36975 	 ~...
  111 	   111 	 0.37872 	 0.37015 	 ~...
  117 	   112 	 0.44009 	 0.40618 	 m..s
  112 	   113 	 0.38402 	 0.40967 	 ~...
  114 	   114 	 0.43742 	 0.41200 	 ~...
  116 	   115 	 0.43969 	 0.41409 	 ~...
  115 	   116 	 0.43944 	 0.46828 	 ~...
  118 	   117 	 0.48649 	 0.48006 	 ~...
  109 	   118 	 0.34395 	 0.48279 	 MISS
  119 	   119 	 0.56702 	 0.51473 	 m..s
  120 	   120 	 0.57342 	 0.61786 	 m..s
==========================================
r_mrr = 0.9899417757987976
r2_mrr = 0.9782915711402893
spearmanr_mrr@5 = 0.8010166883468628
spearmanr_mrr@10 = 0.8967653512954712
spearmanr_mrr@50 = 0.9925844073295593
spearmanr_mrr@100 = 0.9959061741828918
spearmanr_mrr@All = 0.9962314963340759
==========================================
test time: 0.448
Done Testing dataset Kinships
total time taken: 233.01310920715332
training time taken: 225.26090812683105
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9899)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9783)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.8010)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.8968)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9926)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9959)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9962)}}, 'test_loss': {'ComplEx': {'Kinships': 0.45076575642451644}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 6157148552337508
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [461, 1151, 745, 655, 689, 447, 754, 1140, 653, 691, 631, 510, 527, 123, 704, 196, 1051, 377, 137, 1027, 672, 981, 198, 928, 1076, 81, 1085, 499, 1071, 925, 215, 860, 199, 651, 238, 944, 1160, 1026, 122, 954, 426, 1074, 213, 643, 821, 617, 1007, 281, 52, 747, 540, 544, 398, 249, 729, 416, 248, 216, 712, 474, 1134, 403, 911, 1182, 378, 739, 563, 306, 9, 484, 25, 124, 721, 857, 325, 1045, 702, 1165, 262, 1000, 898, 1192, 331, 280, 1036, 197, 1002, 918, 775, 1022, 705, 824, 355, 440, 6, 1070, 568, 60, 1052, 17, 768, 688, 1014, 1193, 489, 927, 781, 21, 869, 1078, 1171, 1167, 84, 567, 1159, 10, 33, 517, 538, 562, 906]
valid_ids (0): []
train_ids (1094): [1212, 289, 465, 236, 539, 592, 476, 243, 1062, 376, 276, 985, 962, 131, 1046, 1099, 140, 120, 365, 518, 706, 256, 723, 916, 995, 233, 532, 485, 318, 173, 0, 1179, 316, 397, 301, 1149, 175, 935, 576, 336, 91, 1166, 1094, 208, 480, 228, 763, 362, 624, 176, 473, 941, 969, 1198, 727, 838, 434, 1132, 1173, 542, 939, 1141, 1073, 1180, 564, 346, 1133, 1177, 913, 503, 107, 324, 839, 335, 1121, 493, 658, 580, 410, 246, 622, 895, 896, 265, 585, 467, 218, 508, 468, 843, 291, 1169, 350, 810, 61, 194, 639, 1191, 610, 322, 558, 652, 1195, 1164, 1131, 1033, 253, 628, 1185, 613, 247, 951, 159, 149, 433, 386, 823, 687, 412, 383, 504, 345, 1048, 222, 675, 487, 28, 908, 597, 1112, 746, 51, 909, 574, 674, 988, 699, 885, 1013, 118, 830, 369, 603, 219, 251, 1058, 531, 141, 1100, 1049, 758, 615, 195, 514, 250, 96, 894, 1186, 638, 300, 186, 1196, 840, 43, 817, 997, 36, 1130, 591, 812, 1043, 390, 919, 1200, 636, 446, 227, 498, 549, 171, 1162, 1135, 1189, 978, 1020, 99, 1136, 179, 464, 224, 308, 214, 442, 646, 1095, 1075, 828, 922, 799, 311, 633, 816, 825, 14, 22, 333, 849, 1090, 695, 833, 303, 697, 996, 707, 694, 522, 725, 12, 555, 525, 798, 806, 845, 73, 682, 654, 413, 1029, 1003, 621, 1152, 1109, 74, 770, 483, 35, 192, 618, 943, 1201, 548, 1187, 590, 854, 328, 145, 418, 802, 663, 1143, 1154, 183, 871, 642, 1142, 815, 1064, 327, 762, 1213, 423, 989, 551, 842, 665, 559, 41, 818, 611, 268, 891, 856, 181, 92, 637, 1103, 931, 286, 600, 341, 874, 1088, 62, 992, 809, 1093, 711, 866, 670, 1108, 797, 430, 1042, 69, 190, 737, 2, 143, 819, 1044, 582, 156, 160, 556, 698, 88, 728, 1032, 589, 1053, 226, 482, 38, 3, 696, 44, 1050, 547, 1174, 760, 307, 204, 112, 947, 506, 488, 1125, 632, 1190, 1101, 769, 20, 220, 945, 732, 822, 800, 391, 202, 952, 528, 279, 462, 870, 910, 1056, 258, 625, 701, 379, 740, 835, 964, 1117, 623, 382, 210, 949, 163, 406, 172, 1008, 553, 374, 557, 859, 298, 387, 608, 97, 594, 892, 667, 310, 1001, 715, 1170, 805, 960, 101, 209, 212, 776, 385, 130, 836, 515, 782, 1104, 450, 546, 27, 720, 753, 332, 139, 774, 417, 620, 785, 719, 779, 647, 358, 76, 933, 57, 1161, 887, 95, 765, 363, 158, 321, 834, 142, 1176, 545, 89, 968, 505, 135, 569, 889, 678, 463, 972, 873, 1006, 449, 1089, 102, 755, 428, 109, 133, 448, 1158, 366, 221, 552, 154, 673, 861, 491, 437, 263, 957, 241, 645, 1205, 852, 1054, 169, 207, 152, 104, 855, 1081, 686, 534, 7, 37, 578, 629, 472, 924, 607, 255, 877, 930, 876, 1031, 1037, 370, 1097, 1175, 605, 305, 445, 151, 312, 1040, 1098, 296, 994, 1012, 264, 693, 513, 477, 736, 486, 541, 40, 188, 973, 150, 231, 656, 260, 319, 1123, 79, 273, 676, 90, 648, 1069, 535, 804, 1015, 864, 167, 343, 177, 184, 759, 337, 659, 767, 259, 1066, 93, 731, 1178, 826, 794, 381, 404, 875, 113, 294, 752, 103, 814, 1111, 347, 537, 901, 1146, 1148, 1019, 614, 115, 352, 903, 1059, 166, 1197, 980, 235, 1030, 459, 217, 201, 1113, 297, 1153, 436, 117, 583, 66, 974, 902, 1087, 1110, 764, 269, 827, 971, 900, 1206, 134, 271, 274, 356, 703, 1, 1055, 612, 1168, 126, 929, 965, 1194, 1139, 444, 677, 384, 844, 791, 441, 396, 496, 1183, 389, 47, 23, 714, 1156, 257, 86, 1004, 455, 106, 1009, 1082, 1207, 394, 185, 923, 533, 886, 1079, 1010, 1086, 148, 1202, 494, 664, 329, 986, 627, 427, 439, 334, 948, 967, 888, 560, 75, 979, 914, 921, 717, 868, 174, 1118, 1096, 1188, 530, 741, 203, 30, 657, 58, 744, 1063, 966, 1210, 554, 272, 1077, 650, 982, 1028, 666, 983, 1204, 526, 1091, 111, 419, 1083, 932, 671, 561, 283, 573, 1181, 1129, 65, 180, 915, 851, 1126, 127, 116, 883, 309, 48, 1214, 619, 904, 681, 644, 1038, 285, 708, 593, 63, 626, 566, 351, 1145, 880, 315, 435, 338, 795, 722, 1211, 119, 270, 1025, 178, 684, 453, 354, 261, 314, 277, 225, 950, 425, 460, 862, 187, 1155, 1061, 409, 451, 232, 878, 771, 414, 1068, 1065, 371, 516, 32, 344, 4, 572, 847, 718, 683, 466, 304, 793, 478, 523, 479, 738, 1120, 407, 372, 586, 811, 481, 780, 831, 596, 15, 543, 938, 223, 846, 588, 302, 602, 278, 39, 205, 565, 164, 71, 500, 108, 920, 599, 807, 934, 54, 1209, 5, 1172, 367, 1163, 606, 733, 716, 373, 411, 709, 942, 520, 146, 778, 239, 400, 813, 1128, 136, 266, 211, 431, 955, 293, 13, 8, 317, 45, 457, 1060, 832, 288, 59, 340, 193, 157, 640, 368, 863, 495, 469, 679, 867, 42, 83, 240, 275, 342, 899, 155, 422, 31, 16, 926, 787, 395, 19, 990, 550, 501, 245, 1017, 710, 897, 443, 206, 796, 1016, 524, 380, 1057, 353, 730, 85, 529, 168, 144, 668, 128, 595, 244, 571, 584, 700, 907, 1021, 1092, 78, 641, 890, 1184, 1124, 841, 749, 375, 242, 536, 64, 53, 320, 1041, 46, 323, 977, 1023, 287, 72, 77, 121, 837, 349, 601, 748, 165, 1018, 853, 751, 1137, 507, 685, 937, 454, 200, 230, 415, 497, 575, 429, 829, 991, 326, 1127, 786, 777, 742, 401, 49, 267, 56, 884, 129, 34, 879, 132, 1157, 912, 850, 1034, 295, 872, 734, 392, 191, 458, 432, 959, 348, 773, 808, 313, 290, 987, 70, 865, 940, 1084, 452, 976, 635, 1105, 772, 361, 284, 254, 1147, 55, 399, 471, 357, 512, 456, 519, 669, 11, 609, 364, 750, 1047, 1005, 604, 713, 690, 125, 50, 661, 587, 660, 237, 956, 110, 388, 788, 946, 502, 958, 420, 511, 87, 438, 475, 735, 1138, 803, 80, 162, 1122, 1199, 790, 993, 402, 470, 67, 299, 234, 1102, 726, 252, 147, 998, 1144, 1119, 1072, 509, 858, 792, 936, 521, 492, 359, 917, 801, 24, 882, 68, 649, 598, 421, 393, 905, 292, 161, 581, 784, 229, 1115, 724, 1039, 360, 692, 761, 579, 26, 1116, 182, 616, 94, 138, 405, 100, 662, 408, 105, 1080, 783, 961, 1106, 282, 170, 1107, 634, 680, 570, 29, 330, 577, 881, 424, 953, 339, 1203, 970, 153, 1067, 848, 1035, 114, 757, 984, 1150, 82, 1114, 999, 18, 98, 766, 1024, 820, 789, 743, 1208, 756, 1011, 893, 630, 189, 963, 975, 490]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9636499047713794
the save name prefix for this run is:  chkpt-ID_9636499047713794_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 738
rank avg (pred): 0.501 +- 0.001
mrr vals (pred, true): 0.019, 0.406
batch losses (mrrl, rdl): 0.0, 0.0037412844

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 414
rank avg (pred): 0.462 +- 0.266
mrr vals (pred, true): 0.046, 0.051
batch losses (mrrl, rdl): 0.0, 3.35e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 202
rank avg (pred): 0.457 +- 0.273
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 0.0, 1.008e-07

Epoch over!
epoch time: 14.862

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 779
rank avg (pred): 0.458 +- 0.252
mrr vals (pred, true): 0.046, 0.058
batch losses (mrrl, rdl): 0.0, 1.1387e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 73
rank avg (pred): 0.129 +- 0.152
mrr vals (pred, true): 0.198, 0.290
batch losses (mrrl, rdl): 0.0, 1.044e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 974
rank avg (pred): 0.069 +- 0.101
mrr vals (pred, true): 0.265, 0.381
batch losses (mrrl, rdl): 0.0, 8.3448e-06

Epoch over!
epoch time: 14.81

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 842
rank avg (pred): 0.460 +- 0.264
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 0.0, 9.56e-08

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 670
rank avg (pred): 0.444 +- 0.263
mrr vals (pred, true): 0.058, 0.055
batch losses (mrrl, rdl): 0.0, 3.82e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 808
rank avg (pred): 0.460 +- 0.269
mrr vals (pred, true): 0.057, 0.054
batch losses (mrrl, rdl): 0.0, 4.3516e-06

Epoch over!
epoch time: 14.798

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 924
rank avg (pred): 0.453 +- 0.271
mrr vals (pred, true): 0.061, 0.047
batch losses (mrrl, rdl): 0.0, 5.843e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 433
rank avg (pred): 0.476 +- 0.274
mrr vals (pred, true): 0.055, 0.056
batch losses (mrrl, rdl): 0.0, 1.34714e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1059
rank avg (pred): 0.100 +- 0.139
mrr vals (pred, true): 0.225, 0.515
batch losses (mrrl, rdl): 0.0, 3.46879e-05

Epoch over!
epoch time: 14.835

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 641
rank avg (pred): 0.462 +- 0.265
mrr vals (pred, true): 0.058, 0.059
batch losses (mrrl, rdl): 0.0, 1.961e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 87
rank avg (pred): 0.485 +- 0.270
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 0.0, 3.3783e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 782
rank avg (pred): 0.456 +- 0.277
mrr vals (pred, true): 0.074, 0.048
batch losses (mrrl, rdl): 0.0, 4.486e-06

Epoch over!
epoch time: 14.844

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 791
rank avg (pred): 0.460 +- 0.273
mrr vals (pred, true): 0.069, 0.059
batch losses (mrrl, rdl): 0.0034374371, 4.125e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 64
rank avg (pred): 0.040 +- 0.023
mrr vals (pred, true): 0.325, 0.282
batch losses (mrrl, rdl): 0.0184561703, 0.0002080201

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 219
rank avg (pred): 0.469 +- 0.177
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0004014149, 2.71315e-05

Epoch over!
epoch time: 15.032

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 776
rank avg (pred): 0.541 +- 0.210
mrr vals (pred, true): 0.039, 0.053
batch losses (mrrl, rdl): 0.0012131615, 0.0001421854

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1112
rank avg (pred): 0.559 +- 0.195
mrr vals (pred, true): 0.039, 0.052
batch losses (mrrl, rdl): 0.0012687686, 0.000199003

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 577
rank avg (pred): 0.438 +- 0.181
mrr vals (pred, true): 0.059, 0.056
batch losses (mrrl, rdl): 0.0007561091, 3.96967e-05

Epoch over!
epoch time: 15.01

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 733
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.685, 0.614
batch losses (mrrl, rdl): 0.0502866469, 2.00582e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 490
rank avg (pred): 0.112 +- 0.064
mrr vals (pred, true): 0.212, 0.247
batch losses (mrrl, rdl): 0.0123795662, 1.32486e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 48
rank avg (pred): 0.063 +- 0.037
mrr vals (pred, true): 0.276, 0.258
batch losses (mrrl, rdl): 0.0029910682, 0.0001350684

Epoch over!
epoch time: 15.046

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1117
rank avg (pred): 0.438 +- 0.136
mrr vals (pred, true): 0.046, 0.055
batch losses (mrrl, rdl): 0.0001240266, 4.32702e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1077
rank avg (pred): 0.023 +- 0.014
mrr vals (pred, true): 0.411, 0.491
batch losses (mrrl, rdl): 0.0645825565, 5.85242e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1004
rank avg (pred): 0.371 +- 0.118
mrr vals (pred, true): 0.056, 0.051
batch losses (mrrl, rdl): 0.0003840274, 0.0002025197

Epoch over!
epoch time: 15.041

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 822
rank avg (pred): 0.033 +- 0.019
mrr vals (pred, true): 0.359, 0.448
batch losses (mrrl, rdl): 0.0794001073, 4.14631e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 598
rank avg (pred): 0.420 +- 0.129
mrr vals (pred, true): 0.051, 0.058
batch losses (mrrl, rdl): 1.69016e-05, 7.18579e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 212
rank avg (pred): 0.461 +- 0.134
mrr vals (pred, true): 0.047, 0.054
batch losses (mrrl, rdl): 0.000119347, 4.35308e-05

Epoch over!
epoch time: 15.042

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 854
rank avg (pred): 0.463 +- 0.123
mrr vals (pred, true): 0.043, 0.053
batch losses (mrrl, rdl): 0.0005391262, 5.57512e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 494
rank avg (pred): 0.118 +- 0.074
mrr vals (pred, true): 0.256, 0.270
batch losses (mrrl, rdl): 0.0020956064, 4.7168e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 960
rank avg (pred): 0.457 +- 0.119
mrr vals (pred, true): 0.046, 0.051
batch losses (mrrl, rdl): 0.0001281768, 5.29421e-05

Epoch over!
epoch time: 15.048

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 309
rank avg (pred): 0.086 +- 0.053
mrr vals (pred, true): 0.274, 0.279
batch losses (mrrl, rdl): 0.0002369192, 3.14047e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 169
rank avg (pred): 0.447 +- 0.131
mrr vals (pred, true): 0.053, 0.054
batch losses (mrrl, rdl): 8.99818e-05, 5.22067e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 573
rank avg (pred): 0.437 +- 0.102
mrr vals (pred, true): 0.044, 0.054
batch losses (mrrl, rdl): 0.0003728333, 8.85752e-05

Epoch over!
epoch time: 15.039

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 603
rank avg (pred): 0.443 +- 0.108
mrr vals (pred, true): 0.046, 0.059
batch losses (mrrl, rdl): 0.0001632072, 6.59528e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 427
rank avg (pred): 0.497 +- 0.118
mrr vals (pred, true): 0.041, 0.054
batch losses (mrrl, rdl): 0.0007275051, 8.0173e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 3
rank avg (pred): 0.053 +- 0.034
mrr vals (pred, true): 0.337, 0.290
batch losses (mrrl, rdl): 0.0219684094, 8.65863e-05

Epoch over!
epoch time: 15.044

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 915
rank avg (pred): 0.284 +- 0.172
mrr vals (pred, true): 0.199, 0.164
batch losses (mrrl, rdl): 0.0121492976, 0.0001620352

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 418
rank avg (pred): 0.461 +- 0.114
mrr vals (pred, true): 0.047, 0.059
batch losses (mrrl, rdl): 6.38779e-05, 5.76757e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 471
rank avg (pred): 0.472 +- 0.118
mrr vals (pred, true): 0.042, 0.054
batch losses (mrrl, rdl): 0.0006554121, 5.49788e-05

Epoch over!
epoch time: 15.01

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 76
rank avg (pred): 0.086 +- 0.057
mrr vals (pred, true): 0.313, 0.280
batch losses (mrrl, rdl): 0.0113094375, 5.40835e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 263
rank avg (pred): 0.073 +- 0.047
mrr vals (pred, true): 0.312, 0.345
batch losses (mrrl, rdl): 0.0106180515, 2.51497e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1068
rank avg (pred): 0.019 +- 0.012
mrr vals (pred, true): 0.478, 0.468
batch losses (mrrl, rdl): 0.0008582487, 6.27559e-05

Epoch over!
epoch time: 15.009

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.427 +- 0.117
mrr vals (pred, true): 0.053, 0.052

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    3 	     0 	 0.04815 	 0.04525 	 ~...
   26 	     1 	 0.05321 	 0.04538 	 ~...
   13 	     2 	 0.05253 	 0.04696 	 ~...
   27 	     3 	 0.05332 	 0.04716 	 ~...
   37 	     4 	 0.05386 	 0.04741 	 ~...
   60 	     5 	 0.05514 	 0.04751 	 ~...
    4 	     6 	 0.04869 	 0.04783 	 ~...
    9 	     7 	 0.05156 	 0.04815 	 ~...
   66 	     8 	 0.05563 	 0.04839 	 ~...
   52 	     9 	 0.05439 	 0.04892 	 ~...
   45 	    10 	 0.05418 	 0.04910 	 ~...
   61 	    11 	 0.05514 	 0.04924 	 ~...
   36 	    12 	 0.05386 	 0.04945 	 ~...
   22 	    13 	 0.05309 	 0.04948 	 ~...
   10 	    14 	 0.05181 	 0.04954 	 ~...
   35 	    15 	 0.05385 	 0.04954 	 ~...
   56 	    16 	 0.05498 	 0.05035 	 ~...
   40 	    17 	 0.05397 	 0.05046 	 ~...
   34 	    18 	 0.05381 	 0.05056 	 ~...
   73 	    19 	 0.05832 	 0.05083 	 ~...
   25 	    20 	 0.05320 	 0.05085 	 ~...
    2 	    21 	 0.04789 	 0.05107 	 ~...
    6 	    22 	 0.04961 	 0.05114 	 ~...
   41 	    23 	 0.05402 	 0.05114 	 ~...
   12 	    24 	 0.05247 	 0.05151 	 ~...
   63 	    25 	 0.05534 	 0.05161 	 ~...
   58 	    26 	 0.05505 	 0.05168 	 ~...
   67 	    27 	 0.05658 	 0.05188 	 ~...
   33 	    28 	 0.05377 	 0.05189 	 ~...
   15 	    29 	 0.05261 	 0.05198 	 ~...
    5 	    30 	 0.04934 	 0.05207 	 ~...
   62 	    31 	 0.05522 	 0.05212 	 ~...
   44 	    32 	 0.05408 	 0.05224 	 ~...
   70 	    33 	 0.05703 	 0.05224 	 ~...
   14 	    34 	 0.05253 	 0.05226 	 ~...
   48 	    35 	 0.05428 	 0.05239 	 ~...
   55 	    36 	 0.05492 	 0.05241 	 ~...
   71 	    37 	 0.05760 	 0.05242 	 ~...
   72 	    38 	 0.05805 	 0.05274 	 ~...
   51 	    39 	 0.05436 	 0.05277 	 ~...
   54 	    40 	 0.05479 	 0.05285 	 ~...
   38 	    41 	 0.05393 	 0.05290 	 ~...
   30 	    42 	 0.05348 	 0.05300 	 ~...
   19 	    43 	 0.05304 	 0.05300 	 ~...
   64 	    44 	 0.05560 	 0.05340 	 ~...
   28 	    45 	 0.05335 	 0.05346 	 ~...
   46 	    46 	 0.05420 	 0.05348 	 ~...
   57 	    47 	 0.05501 	 0.05385 	 ~...
   20 	    48 	 0.05308 	 0.05413 	 ~...
   47 	    49 	 0.05424 	 0.05422 	 ~...
   49 	    50 	 0.05429 	 0.05434 	 ~...
   24 	    51 	 0.05315 	 0.05438 	 ~...
   42 	    52 	 0.05405 	 0.05438 	 ~...
    0 	    53 	 0.04694 	 0.05440 	 ~...
    1 	    54 	 0.04788 	 0.05485 	 ~...
   43 	    55 	 0.05405 	 0.05542 	 ~...
   17 	    56 	 0.05282 	 0.05545 	 ~...
   21 	    57 	 0.05308 	 0.05550 	 ~...
   68 	    58 	 0.05659 	 0.05555 	 ~...
   65 	    59 	 0.05563 	 0.05559 	 ~...
   59 	    60 	 0.05514 	 0.05584 	 ~...
   69 	    61 	 0.05669 	 0.05585 	 ~...
   50 	    62 	 0.05431 	 0.05596 	 ~...
   18 	    63 	 0.05292 	 0.05599 	 ~...
   16 	    64 	 0.05281 	 0.05628 	 ~...
   39 	    65 	 0.05393 	 0.05643 	 ~...
   23 	    66 	 0.05310 	 0.05651 	 ~...
   31 	    67 	 0.05352 	 0.05653 	 ~...
    8 	    68 	 0.05083 	 0.05654 	 ~...
   11 	    69 	 0.05205 	 0.05660 	 ~...
    7 	    70 	 0.04965 	 0.05755 	 ~...
   29 	    71 	 0.05342 	 0.05825 	 ~...
   74 	    72 	 0.05846 	 0.05846 	 ~...
   53 	    73 	 0.05467 	 0.05988 	 ~...
   32 	    74 	 0.05368 	 0.06012 	 ~...
   79 	    75 	 0.21698 	 0.10392 	 MISS
   75 	    76 	 0.21399 	 0.16878 	 m..s
   81 	    77 	 0.22197 	 0.19508 	 ~...
   75 	    78 	 0.21399 	 0.20018 	 ~...
   75 	    79 	 0.21399 	 0.20204 	 ~...
   75 	    80 	 0.21399 	 0.20455 	 ~...
   80 	    81 	 0.22041 	 0.21430 	 ~...
   86 	    82 	 0.23975 	 0.22022 	 ~...
   82 	    83 	 0.22900 	 0.22043 	 ~...
   84 	    84 	 0.23480 	 0.22580 	 ~...
   83 	    85 	 0.22912 	 0.22784 	 ~...
   85 	    86 	 0.23598 	 0.23429 	 ~...
   87 	    87 	 0.26541 	 0.24697 	 ~...
   88 	    88 	 0.27464 	 0.26725 	 ~...
   98 	    89 	 0.29933 	 0.28272 	 ~...
   89 	    90 	 0.27823 	 0.28323 	 ~...
  104 	    91 	 0.30508 	 0.28520 	 ~...
   99 	    92 	 0.30227 	 0.28579 	 ~...
  103 	    93 	 0.30444 	 0.28900 	 ~...
   90 	    94 	 0.28118 	 0.29046 	 ~...
   95 	    95 	 0.29468 	 0.29337 	 ~...
  102 	    96 	 0.30299 	 0.29398 	 ~...
  100 	    97 	 0.30234 	 0.29652 	 ~...
  107 	    98 	 0.32853 	 0.30980 	 ~...
   91 	    99 	 0.28464 	 0.30997 	 ~...
   93 	   100 	 0.28678 	 0.31655 	 ~...
  101 	   101 	 0.30291 	 0.31900 	 ~...
  105 	   102 	 0.30985 	 0.31936 	 ~...
   97 	   103 	 0.29679 	 0.32139 	 ~...
   92 	   104 	 0.28470 	 0.32386 	 m..s
  106 	   105 	 0.31002 	 0.33107 	 ~...
   94 	   106 	 0.28894 	 0.33148 	 m..s
   96 	   107 	 0.29673 	 0.33409 	 m..s
  108 	   108 	 0.33312 	 0.34970 	 ~...
  113 	   109 	 0.39859 	 0.35741 	 m..s
  109 	   110 	 0.34202 	 0.36022 	 ~...
  111 	   111 	 0.38588 	 0.36283 	 ~...
  117 	   112 	 0.43902 	 0.40258 	 m..s
  116 	   113 	 0.43091 	 0.41409 	 ~...
  110 	   114 	 0.37141 	 0.42762 	 m..s
  112 	   115 	 0.39135 	 0.43012 	 m..s
  115 	   116 	 0.41788 	 0.43291 	 ~...
  114 	   117 	 0.40219 	 0.44556 	 m..s
  119 	   118 	 0.46131 	 0.48000 	 ~...
  118 	   119 	 0.45975 	 0.48829 	 ~...
  120 	   120 	 0.62795 	 0.60870 	 ~...
==========================================
r_mrr = 0.9913315176963806
r2_mrr = 0.9825870394706726
spearmanr_mrr@5 = 0.984277606010437
spearmanr_mrr@10 = 0.9707822203636169
spearmanr_mrr@50 = 0.9836455583572388
spearmanr_mrr@100 = 0.994616687297821
spearmanr_mrr@All = 0.9952342510223389
==========================================
test time: 0.45
Done Testing dataset Kinships
total time taken: 232.62320613861084
training time taken: 224.92951798439026
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9913)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9826)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9843)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9708)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9836)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9946)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9952)}}, 'test_loss': {'ComplEx': {'Kinships': 0.41695685484228306}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 4621304194246565
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [785, 1090, 60, 1127, 181, 612, 681, 70, 95, 1035, 720, 365, 643, 832, 935, 445, 391, 192, 983, 211, 1044, 604, 1171, 455, 875, 963, 637, 930, 1214, 901, 26, 1110, 504, 49, 1052, 484, 187, 372, 442, 179, 1049, 704, 1109, 629, 713, 255, 305, 1096, 805, 94, 837, 924, 244, 352, 56, 91, 534, 490, 960, 562, 953, 256, 625, 863, 1179, 480, 336, 765, 198, 556, 111, 771, 979, 974, 1108, 666, 1141, 467, 874, 748, 921, 1006, 133, 197, 632, 1012, 499, 932, 1071, 344, 191, 900, 552, 216, 972, 812, 1203, 119, 368, 335, 259, 544, 155, 396, 1060, 1069, 170, 193, 33, 1080, 9, 491, 428, 16, 184, 497, 645, 110, 672, 280, 1038]
valid_ids (0): []
train_ids (1094): [1002, 520, 728, 4, 112, 439, 833, 814, 618, 860, 636, 233, 297, 570, 324, 61, 517, 1200, 862, 320, 726, 53, 267, 686, 651, 703, 321, 715, 118, 742, 1021, 939, 716, 987, 648, 955, 652, 316, 1166, 376, 28, 298, 970, 1170, 12, 671, 760, 357, 262, 127, 587, 877, 63, 677, 403, 242, 898, 460, 196, 628, 1160, 471, 945, 413, 423, 1125, 740, 1158, 754, 880, 51, 1093, 503, 125, 1104, 857, 350, 64, 616, 1033, 1210, 478, 931, 1128, 241, 1190, 1173, 1199, 456, 190, 132, 876, 23, 816, 100, 710, 1068, 1204, 212, 400, 1087, 143, 554, 980, 157, 793, 234, 688, 917, 349, 370, 735, 1119, 992, 177, 355, 188, 565, 425, 801, 772, 724, 905, 270, 661, 576, 910, 1167, 543, 333, 1026, 58, 692, 340, 164, 301, 1151, 718, 328, 392, 257, 693, 856, 500, 285, 142, 165, 849, 1115, 828, 228, 701, 402, 887, 678, 360, 2, 1070, 904, 803, 751, 1001, 1092, 730, 1032, 437, 896, 123, 514, 535, 14, 1189, 548, 1123, 598, 1091, 238, 521, 865, 827, 1154, 382, 607, 727, 691, 844, 667, 773, 443, 1024, 419, 229, 343, 1000, 861, 501, 408, 135, 452, 1046, 753, 1034, 1113, 555, 258, 1121, 946, 202, 1043, 810, 302, 787, 1134, 537, 659, 83, 690, 226, 729, 733, 5, 362, 743, 454, 778, 545, 845, 1082, 398, 653, 568, 1106, 278, 951, 811, 22, 466, 487, 139, 796, 513, 140, 790, 920, 325, 182, 627, 81, 928, 32, 749, 231, 93, 783, 1019, 218, 1054, 39, 220, 882, 249, 1135, 683, 642, 42, 240, 43, 304, 567, 1163, 1099, 265, 154, 6, 1156, 999, 223, 731, 489, 289, 758, 410, 1076, 638, 55, 1029, 1027, 750, 989, 1097, 675, 558, 19, 994, 649, 1098, 136, 381, 1103, 1025, 871, 709, 872, 266, 462, 591, 1061, 44, 622, 434, 902, 213, 770, 746, 639, 640, 676, 650, 98, 75, 279, 1020, 1072, 86, 364, 848, 563, 912, 717, 571, 741, 103, 1102, 779, 829, 664, 797, 1150, 647, 294, 1051, 971, 283, 705, 464, 1036, 804, 1201, 474, 791, 475, 41, 243, 538, 1195, 878, 966, 152, 149, 371, 1062, 245, 909, 263, 996, 532, 852, 404, 137, 1015, 831, 699, 327, 359, 663, 855, 1172, 206, 407, 73, 1122, 172, 79, 1111, 1152, 588, 107, 163, 679, 1039, 1146, 405, 124, 582, 1011, 891, 919, 850, 236, 1066, 883, 511, 609, 1126, 732, 397, 502, 1013, 488, 892, 739, 67, 146, 834, 985, 208, 153, 766, 841, 899, 117, 313, 239, 941, 846, 401, 272, 318, 777, 62, 531, 134, 47, 174, 1153, 433, 395, 260, 10, 97, 505, 820, 200, 1009, 603, 988, 826, 696, 1048, 468, 818, 326, 385, 194, 299, 156, 424, 338, 224, 601, 312, 546, 203, 654, 431, 936, 367, 248, 291, 635, 600, 768, 626, 1074, 1053, 522, 387, 50, 1184, 269, 533, 261, 700, 903, 859, 209, 342, 390, 482, 1157, 662, 914, 998, 685, 300, 440, 755, 1145, 807, 674, 981, 416, 1144, 1007, 1088, 1116, 906, 160, 784, 351, 575, 354, 101, 281, 965, 519, 169, 725, 1081, 830, 331, 621, 293, 427, 762, 329, 623, 409, 374, 1185, 246, 120, 389, 615, 114, 252, 493, 1022, 54, 108, 1129, 1073, 895, 1124, 721, 1197, 232, 589, 290, 159, 961, 485, 737, 109, 967, 429, 633, 843, 1047, 1175, 84, 457, 126, 885, 795, 1107, 168, 578, 30, 130, 88, 559, 774, 864, 166, 1030, 1079, 553, 1089, 1014, 1064, 547, 470, 557, 449, 494, 524, 465, 509, 90, 995, 606, 1120, 913, 486, 1177, 610, 894, 453, 161, 45, 1114, 247, 761, 1174, 207, 1193, 1050, 377, 78, 923, 583, 836, 800, 426, 435, 492, 657, 958, 144, 947, 1085, 341, 162, 235, 46, 66, 148, 719, 682, 1202, 253, 1205, 210, 337, 145, 230, 916, 847, 776, 0, 1078, 96, 1100, 808, 375, 48, 1212, 890, 332, 711, 366, 420, 1065, 764, 254, 1147, 756, 436, 822, 68, 630, 527, 508, 788, 35, 937, 373, 65, 948, 602, 418, 798, 506, 838, 363, 1137, 1196, 323, 660, 1132, 498, 507, 215, 217, 964, 438, 978, 1213, 599, 167, 214, 706, 1056, 412, 512, 353, 879, 1182, 1136, 31, 881, 21, 824, 417, 221, 689, 89, 673, 594, 933, 668, 572, 461, 189, 1003, 379, 529, 282, 736, 817, 34, 430, 997, 934, 287, 813, 669, 646, 150, 141, 1040, 57, 451, 8, 80, 586, 292, 356, 1194, 712, 1028, 1055, 1140, 183, 1143, 178, 561, 631, 697, 1010, 617, 525, 542, 1207, 411, 929, 794, 339, 869, 528, 204, 694, 195, 85, 432, 311, 510, 707, 873, 515, 870, 1186, 317, 422, 954, 388, 968, 866, 446, 1117, 886, 665, 275, 1183, 227, 835, 199, 620, 472, 76, 539, 264, 71, 151, 481, 1178, 769, 180, 1005, 1211, 7, 477, 550, 1017, 819, 644, 399, 296, 1037, 1180, 113, 853, 977, 346, 1045, 1191, 131, 925, 969, 670, 634, 516, 450, 775, 483, 745, 786, 767, 823, 584, 944, 579, 908, 394, 911, 723, 1016, 1075, 976, 574, 752, 384, 893, 13, 59, 1181, 744, 369, 782, 448, 20, 698, 1004, 1008, 802, 868, 840, 611, 38, 815, 1176, 121, 361, 334, 176, 858, 943, 173, 271, 1165, 959, 286, 479, 92, 889, 1083, 496, 82, 102, 1162, 915, 1101, 115, 956, 447, 825, 577, 821, 406, 72, 122, 29, 1155, 77, 1059, 310, 476, 307, 458, 358, 596, 747, 597, 347, 421, 414, 780, 580, 128, 940, 473, 147, 684, 25, 36, 658, 990, 288, 982, 1094, 695, 763, 1133, 605, 984, 738, 15, 222, 295, 18, 1118, 308, 1149, 781, 87, 69, 927, 655, 581, 702, 806, 595, 759, 839, 27, 566, 185, 315, 348, 104, 734, 942, 592, 986, 809, 564, 792, 415, 129, 641, 938, 3, 1023, 303, 158, 1198, 17, 268, 888, 918, 624, 523, 495, 569, 1206, 273, 1105, 314, 251, 526, 319, 441, 1095, 799, 1084, 619, 378, 1168, 867, 277, 560, 991, 74, 897, 950, 1161, 907, 393, 383, 1057, 138, 593, 1208, 530, 722, 1164, 1042, 1131, 459, 40, 250, 205, 957, 613, 52, 276, 789, 1130, 1187, 1086, 1209, 309, 1138, 551, 1112, 1067, 949, 469, 1169, 585, 1018, 540, 757, 237, 175, 714, 973, 37, 284, 106, 518, 656, 219, 201, 975, 884, 330, 1188, 608, 274, 590, 962, 24, 463, 536, 922, 225, 1058, 851, 11, 116, 952, 842, 614, 1041, 1139, 708, 1142, 993, 687, 345, 1031, 444, 549, 541, 171, 380, 680, 306, 186, 1192, 1148, 573, 105, 1, 322, 386, 854, 99, 926, 1159, 1063, 1077]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9275023596512702
the save name prefix for this run is:  chkpt-ID_9275023596512702_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 105
rank avg (pred): 0.552 +- 0.004
mrr vals (pred, true): 0.017, 0.060
batch losses (mrrl, rdl): 0.0, 0.0002079837

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 313
rank avg (pred): 0.103 +- 0.065
mrr vals (pred, true): 0.186, 0.308
batch losses (mrrl, rdl): 0.0, 2.43429e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 436
rank avg (pred): 0.464 +- 0.262
mrr vals (pred, true): 0.061, 0.057
batch losses (mrrl, rdl): 0.0, 2.5553e-06

Epoch over!
epoch time: 14.911

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 175
rank avg (pred): 0.461 +- 0.263
mrr vals (pred, true): 0.060, 0.050
batch losses (mrrl, rdl): 0.0, 2.6365e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 565
rank avg (pred): 0.180 +- 0.133
mrr vals (pred, true): 0.137, 0.193
batch losses (mrrl, rdl): 0.0, 1.03471e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 340
rank avg (pred): 0.447 +- 0.264
mrr vals (pred, true): 0.053, 0.061
batch losses (mrrl, rdl): 0.0, 3.169e-07

Epoch over!
epoch time: 14.876

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 392
rank avg (pred): 0.462 +- 0.264
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 0.0, 1.3699e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 911
rank avg (pred): 0.186 +- 0.158
mrr vals (pred, true): 0.133, 0.247
batch losses (mrrl, rdl): 0.0, 7.74858e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 968
rank avg (pred): 0.469 +- 0.257
mrr vals (pred, true): 0.041, 0.053
batch losses (mrrl, rdl): 0.0, 4.16e-07

Epoch over!
epoch time: 14.852

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 236
rank avg (pred): 0.448 +- 0.263
mrr vals (pred, true): 0.048, 0.057
batch losses (mrrl, rdl): 0.0, 6.7234e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 237
rank avg (pred): 0.467 +- 0.269
mrr vals (pred, true): 0.044, 0.055
batch losses (mrrl, rdl): 0.0, 4.87e-08

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 512
rank avg (pred): 0.170 +- 0.172
mrr vals (pred, true): 0.173, 0.236
batch losses (mrrl, rdl): 0.0, 3.92521e-05

Epoch over!
epoch time: 14.874

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1193
rank avg (pred): 0.478 +- 0.263
mrr vals (pred, true): 0.043, 0.055
batch losses (mrrl, rdl): 0.0, 6.0084e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 933
rank avg (pred): 0.471 +- 0.264
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0, 7.33e-08

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 45
rank avg (pred): 0.139 +- 0.160
mrr vals (pred, true): 0.203, 0.270
batch losses (mrrl, rdl): 0.0, 3.1807e-06

Epoch over!
epoch time: 14.853

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1188
rank avg (pred): 0.469 +- 0.260
mrr vals (pred, true): 0.047, 0.056
batch losses (mrrl, rdl): 0.0001135583, 7.788e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1195
rank avg (pred): 0.577 +- 0.318
mrr vals (pred, true): 0.045, 0.049
batch losses (mrrl, rdl): 0.0002281972, 0.0001864861

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1022
rank avg (pred): 0.547 +- 0.316
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0003375177, 0.0001268961

Epoch over!
epoch time: 15.106

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 573
rank avg (pred): 0.514 +- 0.284
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0001818526, 2.63624e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 750
rank avg (pred): 0.203 +- 0.265
mrr vals (pred, true): 0.287, 0.411
batch losses (mrrl, rdl): 0.1534496844, 0.0003925208

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 108
rank avg (pred): 0.490 +- 0.244
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 0.0001194309, 2.88705e-05

Epoch over!
epoch time: 15.047

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 509
rank avg (pred): 0.296 +- 0.321
mrr vals (pred, true): 0.240, 0.240
batch losses (mrrl, rdl): 1.4185e-06, 0.0006438909

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 998
rank avg (pred): 0.162 +- 0.248
mrr vals (pred, true): 0.409, 0.459
batch losses (mrrl, rdl): 0.0250750203, 0.0001944852

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 926
rank avg (pred): 0.452 +- 0.173
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.0007931192, 4.05513e-05

Epoch over!
epoch time: 15.045

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 385
rank avg (pred): 0.487 +- 0.235
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 4.2496e-06, 8.5175e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1165
rank avg (pred): 0.493 +- 0.197
mrr vals (pred, true): 0.038, 0.051
batch losses (mrrl, rdl): 0.0014191127, 5.28154e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 653
rank avg (pred): 0.448 +- 0.211
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001376334, 1.60202e-05

Epoch over!
epoch time: 15.064

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1094
rank avg (pred): 0.477 +- 0.231
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 2.6892e-06, 1.03258e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 449
rank avg (pred): 0.486 +- 0.242
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 8.91152e-05, 1.71627e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 239
rank avg (pred): 0.491 +- 0.269
mrr vals (pred, true): 0.055, 0.056
batch losses (mrrl, rdl): 0.0002458605, 1.57664e-05

Epoch over!
epoch time: 15.092

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 257
rank avg (pred): 0.247 +- 0.325
mrr vals (pred, true): 0.346, 0.369
batch losses (mrrl, rdl): 0.0054035941, 0.0005640183

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1195
rank avg (pred): 0.465 +- 0.211
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 0.0001076863, 8.6273e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 419
rank avg (pred): 0.467 +- 0.227
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 3.92646e-05, 6.636e-06

Epoch over!
epoch time: 15.053

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 895
rank avg (pred): 0.115 +- 0.227
mrr vals (pred, true): 0.470, 0.481
batch losses (mrrl, rdl): 0.001222524, 0.0001263487

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1195
rank avg (pred): 0.453 +- 0.190
mrr vals (pred, true): 0.041, 0.049
batch losses (mrrl, rdl): 0.0007586554, 2.03751e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 262
rank avg (pred): 0.279 +- 0.342
mrr vals (pred, true): 0.306, 0.331
batch losses (mrrl, rdl): 0.0064336788, 0.0006288432

Epoch over!
epoch time: 15.054

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1081
rank avg (pred): 0.431 +- 0.194
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 3.67191e-05, 2.71139e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 269
rank avg (pred): 0.254 +- 0.338
mrr vals (pred, true): 0.336, 0.353
batch losses (mrrl, rdl): 0.0029713395, 0.0005110357

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 459
rank avg (pred): 0.472 +- 0.241
mrr vals (pred, true): 0.052, 0.054
batch losses (mrrl, rdl): 5.08934e-05, 5.9615e-06

Epoch over!
epoch time: 15.079

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1014
rank avg (pred): 0.456 +- 0.246
mrr vals (pred, true): 0.057, 0.056
batch losses (mrrl, rdl): 0.0005053552, 2.3092e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1102
rank avg (pred): 0.505 +- 0.266
mrr vals (pred, true): 0.046, 0.058
batch losses (mrrl, rdl): 0.0001462311, 4.10321e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1011
rank avg (pred): 0.455 +- 0.232
mrr vals (pred, true): 0.052, 0.062
batch losses (mrrl, rdl): 3.1955e-05, 5.2832e-06

Epoch over!
epoch time: 15.072

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 687
rank avg (pred): 0.526 +- 0.275
mrr vals (pred, true): 0.047, 0.055
batch losses (mrrl, rdl): 0.0001003607, 7.03603e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 563
rank avg (pred): 0.402 +- 0.361
mrr vals (pred, true): 0.194, 0.195
batch losses (mrrl, rdl): 1.0585e-05, 0.0011542594

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 71
rank avg (pred): 0.291 +- 0.353
mrr vals (pred, true): 0.333, 0.303
batch losses (mrrl, rdl): 0.0089196078, 0.0007715691

Epoch over!
epoch time: 15.073

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.355 +- 0.156
mrr vals (pred, true): 0.058, 0.053

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04633 	 0.04525 	 ~...
   49 	     1 	 0.05348 	 0.04627 	 ~...
    8 	     2 	 0.04649 	 0.04716 	 ~...
    1 	     3 	 0.04267 	 0.04717 	 ~...
    7 	     4 	 0.04647 	 0.04767 	 ~...
   14 	     5 	 0.04849 	 0.04803 	 ~...
   30 	     6 	 0.05097 	 0.04852 	 ~...
   72 	     7 	 0.05710 	 0.04879 	 ~...
   17 	     8 	 0.04922 	 0.04891 	 ~...
   69 	     9 	 0.05595 	 0.04892 	 ~...
   40 	    10 	 0.05222 	 0.04901 	 ~...
   68 	    11 	 0.05582 	 0.04930 	 ~...
   23 	    12 	 0.05032 	 0.04931 	 ~...
   57 	    13 	 0.05476 	 0.04944 	 ~...
   27 	    14 	 0.05075 	 0.04960 	 ~...
   78 	    15 	 0.05803 	 0.04961 	 ~...
   70 	    16 	 0.05605 	 0.04963 	 ~...
   66 	    17 	 0.05568 	 0.04985 	 ~...
   29 	    18 	 0.05094 	 0.04989 	 ~...
   34 	    19 	 0.05181 	 0.04997 	 ~...
   76 	    20 	 0.05777 	 0.05004 	 ~...
   25 	    21 	 0.05055 	 0.05014 	 ~...
   33 	    22 	 0.05144 	 0.05028 	 ~...
   46 	    23 	 0.05301 	 0.05068 	 ~...
   59 	    24 	 0.05495 	 0.05079 	 ~...
   58 	    25 	 0.05486 	 0.05086 	 ~...
   38 	    26 	 0.05208 	 0.05090 	 ~...
   60 	    27 	 0.05525 	 0.05102 	 ~...
    3 	    28 	 0.04374 	 0.05108 	 ~...
   55 	    29 	 0.05392 	 0.05124 	 ~...
    0 	    30 	 0.04217 	 0.05126 	 ~...
    2 	    31 	 0.04347 	 0.05129 	 ~...
   18 	    32 	 0.04935 	 0.05136 	 ~...
   80 	    33 	 0.05970 	 0.05154 	 ~...
   15 	    34 	 0.04881 	 0.05158 	 ~...
   61 	    35 	 0.05529 	 0.05164 	 ~...
    9 	    36 	 0.04698 	 0.05186 	 ~...
    5 	    37 	 0.04633 	 0.05201 	 ~...
   83 	    38 	 0.06090 	 0.05224 	 ~...
   31 	    39 	 0.05120 	 0.05226 	 ~...
   75 	    40 	 0.05766 	 0.05248 	 ~...
   22 	    41 	 0.05031 	 0.05285 	 ~...
   10 	    42 	 0.04732 	 0.05287 	 ~...
   21 	    43 	 0.05008 	 0.05287 	 ~...
   35 	    44 	 0.05186 	 0.05289 	 ~...
   45 	    45 	 0.05300 	 0.05302 	 ~...
   82 	    46 	 0.05984 	 0.05308 	 ~...
   56 	    47 	 0.05464 	 0.05311 	 ~...
   79 	    48 	 0.05803 	 0.05321 	 ~...
   54 	    49 	 0.05387 	 0.05340 	 ~...
   47 	    50 	 0.05325 	 0.05360 	 ~...
   62 	    51 	 0.05538 	 0.05374 	 ~...
   20 	    52 	 0.04981 	 0.05379 	 ~...
   24 	    53 	 0.05038 	 0.05385 	 ~...
   28 	    54 	 0.05090 	 0.05396 	 ~...
   71 	    55 	 0.05625 	 0.05413 	 ~...
   42 	    56 	 0.05280 	 0.05422 	 ~...
   37 	    57 	 0.05196 	 0.05438 	 ~...
   67 	    58 	 0.05580 	 0.05440 	 ~...
   81 	    59 	 0.05975 	 0.05451 	 ~...
   44 	    60 	 0.05300 	 0.05456 	 ~...
   77 	    61 	 0.05799 	 0.05492 	 ~...
   50 	    62 	 0.05355 	 0.05493 	 ~...
   41 	    63 	 0.05274 	 0.05547 	 ~...
   13 	    64 	 0.04828 	 0.05566 	 ~...
   12 	    65 	 0.04781 	 0.05575 	 ~...
   53 	    66 	 0.05387 	 0.05577 	 ~...
   43 	    67 	 0.05291 	 0.05605 	 ~...
    4 	    68 	 0.04623 	 0.05614 	 ~...
   36 	    69 	 0.05191 	 0.05626 	 ~...
   64 	    70 	 0.05544 	 0.05628 	 ~...
   48 	    71 	 0.05328 	 0.05631 	 ~...
   26 	    72 	 0.05074 	 0.05647 	 ~...
   16 	    73 	 0.04899 	 0.05656 	 ~...
   65 	    74 	 0.05548 	 0.05714 	 ~...
   52 	    75 	 0.05378 	 0.05751 	 ~...
   51 	    76 	 0.05356 	 0.05776 	 ~...
   19 	    77 	 0.04941 	 0.05817 	 ~...
   73 	    78 	 0.05713 	 0.05863 	 ~...
   11 	    79 	 0.04769 	 0.06158 	 ~...
   63 	    80 	 0.05542 	 0.06229 	 ~...
   32 	    81 	 0.05122 	 0.06240 	 ~...
   74 	    82 	 0.05738 	 0.06253 	 ~...
   39 	    83 	 0.05208 	 0.06325 	 ~...
   86 	    84 	 0.20692 	 0.18556 	 ~...
   87 	    85 	 0.20823 	 0.19687 	 ~...
   84 	    86 	 0.18879 	 0.19886 	 ~...
   85 	    87 	 0.19703 	 0.20204 	 ~...
   89 	    88 	 0.21970 	 0.22043 	 ~...
   90 	    89 	 0.23810 	 0.22580 	 ~...
   88 	    90 	 0.21891 	 0.22737 	 ~...
   92 	    91 	 0.25640 	 0.23386 	 ~...
   91 	    92 	 0.24219 	 0.24673 	 ~...
   94 	    93 	 0.26408 	 0.25404 	 ~...
   93 	    94 	 0.26178 	 0.25657 	 ~...
   95 	    95 	 0.26522 	 0.26043 	 ~...
   96 	    96 	 0.29185 	 0.27513 	 ~...
  103 	    97 	 0.33470 	 0.27991 	 m..s
  102 	    98 	 0.33033 	 0.28272 	 m..s
   99 	    99 	 0.31200 	 0.28520 	 ~...
  100 	   100 	 0.31445 	 0.30211 	 ~...
  107 	   101 	 0.34667 	 0.30588 	 m..s
  101 	   102 	 0.32831 	 0.31687 	 ~...
   98 	   103 	 0.30808 	 0.32139 	 ~...
  108 	   104 	 0.34785 	 0.32216 	 ~...
  110 	   105 	 0.34970 	 0.32749 	 ~...
  109 	   106 	 0.34877 	 0.33010 	 ~...
   97 	   107 	 0.30777 	 0.33409 	 ~...
  104 	   108 	 0.33487 	 0.34025 	 ~...
  106 	   109 	 0.34541 	 0.34735 	 ~...
  105 	   110 	 0.34240 	 0.34880 	 ~...
  111 	   111 	 0.39925 	 0.36283 	 m..s
  113 	   112 	 0.42981 	 0.37821 	 m..s
  117 	   113 	 0.49366 	 0.38075 	 MISS
  116 	   114 	 0.48856 	 0.39397 	 m..s
  114 	   115 	 0.42999 	 0.41166 	 ~...
  112 	   116 	 0.42971 	 0.42648 	 ~...
  115 	   117 	 0.48819 	 0.48073 	 ~...
  119 	   118 	 0.51788 	 0.50285 	 ~...
  118 	   119 	 0.51413 	 0.52550 	 ~...
  120 	   120 	 0.61604 	 0.62777 	 ~...
==========================================
r_mrr = 0.9935435652732849
r2_mrr = 0.9814643263816833
spearmanr_mrr@5 = 0.9520742893218994
spearmanr_mrr@10 = 0.9604281187057495
spearmanr_mrr@50 = 0.9944034814834595
spearmanr_mrr@100 = 0.9971815943717957
spearmanr_mrr@All = 0.9974175691604614
==========================================
test time: 0.45
Done Testing dataset Kinships
total time taken: 233.28790760040283
training time taken: 225.513906955719
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9935)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9815)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9521)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9604)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9944)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9972)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9974)}}, 'test_loss': {'ComplEx': {'Kinships': 0.43819944379356457}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 6014342729436296
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [933, 451, 139, 565, 32, 681, 301, 768, 997, 275, 1043, 682, 1076, 122, 647, 318, 711, 162, 518, 806, 26, 365, 467, 1188, 950, 1034, 1047, 276, 1041, 816, 138, 56, 1071, 696, 893, 305, 661, 667, 354, 461, 500, 851, 508, 892, 241, 248, 874, 459, 680, 803, 598, 499, 1028, 780, 141, 1159, 358, 773, 417, 387, 59, 1068, 329, 889, 308, 93, 729, 287, 497, 293, 217, 822, 1183, 491, 595, 644, 231, 144, 456, 463, 443, 990, 257, 199, 811, 774, 92, 850, 900, 957, 703, 1194, 1101, 1114, 866, 920, 337, 984, 1210, 342, 447, 943, 264, 1207, 1038, 747, 812, 315, 1124, 614, 385, 743, 130, 1150, 759, 418, 529, 845, 549, 190, 12]
valid_ids (0): []
train_ids (1094): [243, 359, 637, 233, 131, 254, 906, 394, 140, 678, 937, 1088, 1001, 112, 170, 973, 1091, 1030, 291, 453, 925, 70, 121, 273, 945, 979, 1070, 183, 157, 1087, 401, 297, 16, 6, 319, 801, 80, 875, 498, 403, 0, 1048, 640, 1190, 786, 73, 707, 1072, 1147, 5, 128, 953, 18, 826, 955, 125, 502, 568, 570, 754, 655, 299, 507, 1064, 48, 536, 864, 970, 67, 999, 651, 259, 582, 163, 590, 578, 750, 1119, 348, 147, 912, 1138, 534, 23, 1002, 956, 402, 148, 914, 627, 201, 1152, 82, 1196, 361, 630, 441, 47, 340, 390, 731, 708, 3, 668, 562, 211, 616, 378, 589, 44, 135, 849, 1121, 242, 221, 799, 187, 234, 435, 2, 686, 936, 642, 971, 338, 533, 1016, 83, 844, 9, 830, 755, 993, 886, 30, 374, 1051, 477, 346, 884, 693, 1129, 302, 1090, 1184, 1049, 292, 913, 208, 751, 567, 336, 1069, 831, 876, 371, 865, 787, 895, 462, 127, 664, 1092, 908, 171, 68, 1052, 653, 734, 416, 530, 1199, 195, 193, 382, 891, 495, 232, 709, 757, 1073, 213, 594, 1165, 882, 662, 1213, 1094, 975, 656, 372, 724, 422, 603, 623, 256, 861, 215, 926, 289, 314, 120, 996, 457, 785, 89, 349, 99, 114, 690, 804, 972, 853, 353, 620, 17, 102, 540, 260, 1055, 397, 689, 1059, 209, 873, 174, 373, 172, 272, 1115, 294, 1050, 1127, 1025, 493, 312, 409, 295, 646, 878, 448, 322, 1180, 395, 426, 756, 558, 615, 216, 1093, 355, 388, 622, 519, 1200, 645, 576, 181, 1128, 1105, 184, 607, 1044, 452, 1098, 357, 261, 407, 597, 868, 444, 612, 596, 1172, 400, 1134, 657, 897, 194, 569, 995, 1163, 1102, 255, 420, 100, 1085, 325, 154, 634, 24, 1166, 165, 64, 415, 1078, 290, 688, 784, 320, 438, 362, 1181, 1133, 1136, 7, 899, 1029, 62, 846, 1193, 285, 1, 196, 699, 79, 252, 356, 909, 555, 847, 538, 676, 240, 1089, 1067, 161, 683, 1009, 776, 670, 915, 1131, 1169, 506, 949, 929, 631, 366, 944, 572, 1013, 760, 725, 283, 694, 212, 722, 470, 476, 697, 687, 798, 334, 133, 492, 251, 728, 81, 559, 692, 189, 503, 20, 1117, 1126, 496, 109, 857, 341, 303, 599, 939, 1061, 863, 191, 721, 951, 742, 718, 767, 982, 1053, 263, 1185, 421, 986, 542, 1208, 19, 898, 108, 547, 65, 455, 643, 1026, 1198, 442, 249, 11, 1007, 84, 408, 1111, 494, 737, 168, 250, 541, 137, 855, 186, 204, 560, 1173, 219, 654, 429, 910, 160, 980, 460, 1045, 546, 704, 126, 1122, 1060, 1178, 1176, 1182, 870, 509, 770, 228, 35, 1024, 4, 740, 489, 879, 669, 968, 685, 98, 1008, 524, 129, 245, 1022, 333, 40, 962, 766, 1201, 958, 152, 517, 931, 626, 428, 1135, 505, 1123, 544, 328, 71, 87, 611, 1132, 436, 466, 1209, 522, 1142, 1082, 270, 577, 632, 396, 1074, 1171, 1214, 155, 304, 749, 205, 76, 53, 856, 548, 324, 574, 928, 1189, 1112, 91, 807, 1162, 368, 46, 85, 1191, 1141, 575, 1042, 280, 862, 410, 226, 1021, 473, 1006, 298, 633, 1179, 829, 1158, 658, 179, 316, 1167, 758, 907, 814, 188, 1149, 300, 563, 977, 379, 1204, 472, 34, 535, 150, 921, 363, 941, 672, 665, 554, 641, 1003, 1096, 389, 119, 795, 278, 267, 413, 286, 331, 1153, 978, 386, 841, 539, 922, 296, 445, 824, 712, 384, 592, 1058, 309, 419, 75, 738, 713, 458, 335, 652, 182, 613, 714, 809, 802, 375, 449, 779, 1000, 684, 629, 989, 918, 424, 1108, 1205, 177, 1081, 720, 1192, 1027, 815, 1175, 919, 347, 1066, 867, 116, 63, 551, 229, 425, 156, 777, 1174, 51, 134, 207, 1168, 545, 214, 430, 671, 480, 404, 45, 1120, 1143, 1032, 800, 772, 946, 105, 789, 601, 588, 97, 464, 218, 871, 446, 377, 1097, 1106, 967, 606, 513, 1186, 872, 727, 468, 271, 571, 963, 57, 210, 192, 36, 166, 1139, 14, 532, 153, 790, 103, 167, 123, 159, 381, 310, 1160, 791, 778, 1077, 1063, 835, 832, 1140, 1014, 746, 501, 1037, 948, 96, 220, 974, 650, 332, 1020, 27, 432, 37, 515, 1005, 543, 317, 29, 719, 164, 723, 604, 307, 1054, 197, 732, 836, 427, 1056, 392, 769, 1109, 383, 823, 78, 149, 411, 618, 1211, 61, 1011, 819, 848, 247, 268, 843, 200, 556, 1107, 820, 206, 1206, 883, 700, 610, 808, 587, 602, 981, 930, 94, 414, 237, 203, 115, 833, 675, 142, 483, 330, 1099, 821, 639, 660, 579, 834, 940, 666, 752, 924, 469, 952, 564, 440, 1151, 399, 965, 903, 474, 258, 486, 960, 364, 1104, 434, 691, 817, 854, 225, 282, 516, 398, 465, 431, 736, 227, 514, 1125, 880, 605, 1018, 136, 994, 288, 175, 887, 894, 369, 1039, 584, 33, 230, 905, 178, 284, 72, 992, 762, 566, 698, 169, 730, 1031, 934, 744, 10, 1100, 1195, 1083, 1004, 679, 158, 748, 405, 145, 901, 837, 1144, 327, 521, 763, 69, 1035, 1116, 1046, 306, 964, 88, 1017, 281, 1177, 323, 797, 311, 705, 512, 151, 583, 608, 966, 617, 15, 13, 673, 1155, 380, 481, 479, 842, 916, 49, 343, 942, 827, 146, 31, 969, 710, 765, 412, 106, 593, 1040, 976, 561, 813, 695, 352, 360, 701, 902, 1075, 223, 244, 277, 1156, 52, 1023, 1161, 573, 1010, 246, 591, 321, 393, 706, 21, 781, 663, 520, 107, 628, 185, 1157, 1170, 235, 828, 43, 553, 433, 55, 796, 376, 74, 648, 132, 1062, 860, 54, 531, 537, 42, 659, 50, 624, 1154, 735, 726, 1203, 339, 1118, 581, 782, 58, 881, 1057, 266, 1145, 90, 124, 39, 585, 77, 274, 877, 677, 326, 988, 173, 557, 586, 528, 202, 482, 1015, 840, 715, 775, 888, 110, 1033, 771, 625, 238, 423, 262, 983, 527, 454, 985, 600, 702, 478, 609, 764, 488, 180, 961, 852, 253, 487, 101, 954, 923, 825, 635, 1113, 236, 370, 485, 41, 1080, 437, 510, 733, 959, 1148, 1187, 117, 525, 739, 987, 38, 367, 1084, 1146, 279, 741, 838, 344, 935, 1164, 858, 313, 717, 917, 938, 111, 471, 224, 619, 636, 947, 22, 1079, 1197, 550, 351, 761, 113, 406, 1065, 504, 998, 896, 674, 523, 345, 927, 788, 552, 176, 1110, 526, 66, 25, 753, 1103, 859, 1202, 269, 716, 511, 638, 450, 1095, 439, 885, 1130, 222, 911, 104, 28, 1137, 198, 904, 810, 621, 890, 794, 1019, 580, 869, 792, 839, 239, 475, 1036, 818, 95, 783, 391, 60, 991, 649, 793, 1012, 350, 805, 265, 1086, 490, 745, 118, 1212, 86, 8, 932, 484, 143]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5787634573844018
the save name prefix for this run is:  chkpt-ID_5787634573844018_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 46
rank avg (pred): 0.443 +- 0.004
mrr vals (pred, true): 0.021, 0.280
batch losses (mrrl, rdl): 0.0, 0.0019635288

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1051
rank avg (pred): 0.484 +- 0.226
mrr vals (pred, true): 0.023, 0.053
batch losses (mrrl, rdl): 0.0, 9.7789e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 225
rank avg (pred): 0.454 +- 0.259
mrr vals (pred, true): 0.032, 0.055
batch losses (mrrl, rdl): 0.0, 1.8045e-06

Epoch over!
epoch time: 14.883

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 733
rank avg (pred): 0.090 +- 0.131
mrr vals (pred, true): 0.216, 0.614
batch losses (mrrl, rdl): 0.0, 7.41759e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 3
rank avg (pred): 0.108 +- 0.145
mrr vals (pred, true): 0.190, 0.290
batch losses (mrrl, rdl): 0.0, 4.17e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 754
rank avg (pred): 0.141 +- 0.165
mrr vals (pred, true): 0.149, 0.310
batch losses (mrrl, rdl): 0.0, 5.6185e-06

Epoch over!
epoch time: 14.955

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 361
rank avg (pred): 0.473 +- 0.272
mrr vals (pred, true): 0.038, 0.058
batch losses (mrrl, rdl): 0.0, 3.7797e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 555
rank avg (pred): 0.153 +- 0.164
mrr vals (pred, true): 0.135, 0.182
batch losses (mrrl, rdl): 0.0, 1.61325e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 377
rank avg (pred): 0.473 +- 0.269
mrr vals (pred, true): 0.041, 0.057
batch losses (mrrl, rdl): 0.0, 3.229e-07

Epoch over!
epoch time: 14.943

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1067
rank avg (pred): 0.074 +- 0.099
mrr vals (pred, true): 0.237, 0.405
batch losses (mrrl, rdl): 0.0, 7.853e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 996
rank avg (pred): 0.108 +- 0.137
mrr vals (pred, true): 0.188, 0.511
batch losses (mrrl, rdl): 0.0, 4.52831e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1156
rank avg (pred): 0.095 +- 0.114
mrr vals (pred, true): 0.232, 0.271
batch losses (mrrl, rdl): 0.0, 1.4228e-05

Epoch over!
epoch time: 14.944

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1192
rank avg (pred): 0.466 +- 0.264
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0, 1.0227e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 123
rank avg (pred): 0.488 +- 0.279
mrr vals (pred, true): 0.045, 0.048
batch losses (mrrl, rdl): 0.0, 1.957e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 786
rank avg (pred): 0.466 +- 0.271
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 0.0, 2.4463e-06

Epoch over!
epoch time: 14.92

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1110
rank avg (pred): 0.470 +- 0.260
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 5.9095e-05, 6.01e-08

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 646
rank avg (pred): 0.613 +- 0.356
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001502771, 0.0002877139

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 922
rank avg (pred): 0.584 +- 0.341
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.000111706, 0.0002136794

Epoch over!
epoch time: 15.102

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 492
rank avg (pred): 0.340 +- 0.387
mrr vals (pred, true): 0.215, 0.232
batch losses (mrrl, rdl): 0.0028441465, 0.0009220084

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 687
rank avg (pred): 0.513 +- 0.265
mrr vals (pred, true): 0.044, 0.055
batch losses (mrrl, rdl): 0.0003731082, 5.53491e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 405
rank avg (pred): 0.518 +- 0.258
mrr vals (pred, true): 0.044, 0.051
batch losses (mrrl, rdl): 0.0003685095, 5.46498e-05

Epoch over!
epoch time: 15.015

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 109
rank avg (pred): 0.431 +- 0.204
mrr vals (pred, true): 0.054, 0.059
batch losses (mrrl, rdl): 0.0001543403, 2.54132e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 488
rank avg (pred): 0.178 +- 0.168
mrr vals (pred, true): 0.265, 0.264
batch losses (mrrl, rdl): 1.41038e-05, 0.0001246164

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 581
rank avg (pred): 0.483 +- 0.234
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 4.48601e-05, 1.75836e-05

Epoch over!
epoch time: 14.967

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1072
rank avg (pred): 0.153 +- 0.169
mrr vals (pred, true): 0.348, 0.358
batch losses (mrrl, rdl): 0.0009898435, 0.0001534538

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 625
rank avg (pred): 0.484 +- 0.214
mrr vals (pred, true): 0.044, 0.046
batch losses (mrrl, rdl): 0.0004193771, 2.76556e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 922
rank avg (pred): 0.521 +- 0.271
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 6.94477e-05, 6.81815e-05

Epoch over!
epoch time: 15.027

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1075
rank avg (pred): 0.134 +- 0.163
mrr vals (pred, true): 0.430, 0.448
batch losses (mrrl, rdl): 0.0029520243, 0.0001084351

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 724
rank avg (pred): 0.441 +- 0.207
mrr vals (pred, true): 0.058, 0.049
batch losses (mrrl, rdl): 0.0006895011, 1.81745e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 555
rank avg (pred): 0.209 +- 0.155
mrr vals (pred, true): 0.185, 0.182
batch losses (mrrl, rdl): 5.66191e-05, 2.91226e-05

Epoch over!
epoch time: 15.025

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 94
rank avg (pred): 0.469 +- 0.208
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 1.80497e-05, 1.92389e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 196
rank avg (pred): 0.437 +- 0.185
mrr vals (pred, true): 0.050, 0.060
batch losses (mrrl, rdl): 1.8691e-06, 2.8991e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1214
rank avg (pred): 0.457 +- 0.195
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 3.77248e-05, 2.26339e-05

Epoch over!
epoch time: 15.128

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 707
rank avg (pred): 0.519 +- 0.252
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 5.98789e-05, 7.89285e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 862
rank avg (pred): 0.470 +- 0.209
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 4.092e-07, 1.75888e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 140
rank avg (pred): 0.433 +- 0.180
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 1.67743e-05, 3.39699e-05

Epoch over!
epoch time: 15.043

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1176
rank avg (pred): 0.485 +- 0.210
mrr vals (pred, true): 0.045, 0.048
batch losses (mrrl, rdl): 0.000224813, 1.8769e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 369
rank avg (pred): 0.497 +- 0.218
mrr vals (pred, true): 0.044, 0.049
batch losses (mrrl, rdl): 0.0003293873, 3.60131e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1112
rank avg (pred): 0.539 +- 0.205
mrr vals (pred, true): 0.038, 0.052
batch losses (mrrl, rdl): 0.0014339433, 0.0001275748

Epoch over!
epoch time: 15.071

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 85
rank avg (pred): 0.407 +- 0.152
mrr vals (pred, true): 0.044, 0.056
batch losses (mrrl, rdl): 0.0003611799, 9.59518e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 739
rank avg (pred): 0.129 +- 0.167
mrr vals (pred, true): 0.480, 0.414
batch losses (mrrl, rdl): 0.0437192097, 6.88056e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 527
rank avg (pred): 0.211 +- 0.164
mrr vals (pred, true): 0.241, 0.220
batch losses (mrrl, rdl): 0.0043690703, 8.72525e-05

Epoch over!
epoch time: 15.079

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 942
rank avg (pred): 0.480 +- 0.229
mrr vals (pred, true): 0.053, 0.054
batch losses (mrrl, rdl): 0.0001060588, 3.27135e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 835
rank avg (pred): 0.182 +- 0.178
mrr vals (pred, true): 0.315, 0.331
batch losses (mrrl, rdl): 0.0026419084, 8.95771e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 854
rank avg (pred): 0.482 +- 0.210
mrr vals (pred, true): 0.046, 0.053
batch losses (mrrl, rdl): 0.0001533522, 1.61394e-05

Epoch over!
epoch time: 15.065

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.443 +- 0.214
mrr vals (pred, true): 0.066, 0.051

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   30 	     0 	 0.06581 	 0.04690 	 ~...
   30 	     1 	 0.06581 	 0.04764 	 ~...
   30 	     2 	 0.06581 	 0.04863 	 ~...
   18 	     3 	 0.06245 	 0.04890 	 ~...
    5 	     4 	 0.05591 	 0.04910 	 ~...
    2 	     5 	 0.05068 	 0.04947 	 ~...
   27 	     6 	 0.06460 	 0.04948 	 ~...
   30 	     7 	 0.06581 	 0.04950 	 ~...
   15 	     8 	 0.06095 	 0.04952 	 ~...
   30 	     9 	 0.06581 	 0.04954 	 ~...
   66 	    10 	 0.07141 	 0.04983 	 ~...
    3 	    11 	 0.05081 	 0.05028 	 ~...
   60 	    12 	 0.06660 	 0.05038 	 ~...
   30 	    13 	 0.06581 	 0.05056 	 ~...
   30 	    14 	 0.06581 	 0.05067 	 ~...
   68 	    15 	 0.07306 	 0.05085 	 ~...
   69 	    16 	 0.07378 	 0.05088 	 ~...
   30 	    17 	 0.06581 	 0.05096 	 ~...
   30 	    18 	 0.06581 	 0.05104 	 ~...
   65 	    19 	 0.07065 	 0.05122 	 ~...
    6 	    20 	 0.05649 	 0.05127 	 ~...
   67 	    21 	 0.07205 	 0.05135 	 ~...
   30 	    22 	 0.06581 	 0.05138 	 ~...
   10 	    23 	 0.05883 	 0.05198 	 ~...
   30 	    24 	 0.06581 	 0.05219 	 ~...
   30 	    25 	 0.06581 	 0.05224 	 ~...
   30 	    26 	 0.06581 	 0.05258 	 ~...
   28 	    27 	 0.06549 	 0.05282 	 ~...
   29 	    28 	 0.06552 	 0.05291 	 ~...
   30 	    29 	 0.06581 	 0.05300 	 ~...
   71 	    30 	 0.07545 	 0.05307 	 ~...
    7 	    31 	 0.05783 	 0.05313 	 ~...
   30 	    32 	 0.06581 	 0.05315 	 ~...
   30 	    33 	 0.06581 	 0.05325 	 ~...
   30 	    34 	 0.06581 	 0.05340 	 ~...
   23 	    35 	 0.06330 	 0.05361 	 ~...
    1 	    36 	 0.04783 	 0.05373 	 ~...
   62 	    37 	 0.06881 	 0.05375 	 ~...
   64 	    38 	 0.07032 	 0.05387 	 ~...
   72 	    39 	 0.07806 	 0.05392 	 ~...
    4 	    40 	 0.05553 	 0.05417 	 ~...
   17 	    41 	 0.06218 	 0.05425 	 ~...
   16 	    42 	 0.06208 	 0.05433 	 ~...
   21 	    43 	 0.06258 	 0.05434 	 ~...
   74 	    44 	 0.07943 	 0.05436 	 ~...
   30 	    45 	 0.06581 	 0.05438 	 ~...
   20 	    46 	 0.06256 	 0.05440 	 ~...
   30 	    47 	 0.06581 	 0.05451 	 ~...
   19 	    48 	 0.06256 	 0.05457 	 ~...
   13 	    49 	 0.06064 	 0.05498 	 ~...
   14 	    50 	 0.06066 	 0.05506 	 ~...
   22 	    51 	 0.06321 	 0.05520 	 ~...
   30 	    52 	 0.06581 	 0.05520 	 ~...
   75 	    53 	 0.07998 	 0.05534 	 ~...
    0 	    54 	 0.04748 	 0.05555 	 ~...
   11 	    55 	 0.05885 	 0.05566 	 ~...
   26 	    56 	 0.06460 	 0.05609 	 ~...
   24 	    57 	 0.06345 	 0.05664 	 ~...
   30 	    58 	 0.06581 	 0.05703 	 ~...
   30 	    59 	 0.06581 	 0.05712 	 ~...
   59 	    60 	 0.06591 	 0.05735 	 ~...
   30 	    61 	 0.06581 	 0.05751 	 ~...
   76 	    62 	 0.08275 	 0.05770 	 ~...
   30 	    63 	 0.06581 	 0.05807 	 ~...
   61 	    64 	 0.06759 	 0.05816 	 ~...
   73 	    65 	 0.07840 	 0.05817 	 ~...
    8 	    66 	 0.05790 	 0.05863 	 ~...
   70 	    67 	 0.07504 	 0.05874 	 ~...
   63 	    68 	 0.06952 	 0.05874 	 ~...
   25 	    69 	 0.06380 	 0.05882 	 ~...
   30 	    70 	 0.06581 	 0.05902 	 ~...
   30 	    71 	 0.06581 	 0.05918 	 ~...
   30 	    72 	 0.06581 	 0.05952 	 ~...
   30 	    73 	 0.06581 	 0.06053 	 ~...
   12 	    74 	 0.06051 	 0.06092 	 ~...
    9 	    75 	 0.05801 	 0.06325 	 ~...
   30 	    76 	 0.06581 	 0.06353 	 ~...
  105 	    77 	 0.37778 	 0.15938 	 MISS
   77 	    78 	 0.20699 	 0.19332 	 ~...
   79 	    79 	 0.23016 	 0.19396 	 m..s
   80 	    80 	 0.23478 	 0.21194 	 ~...
   78 	    81 	 0.22708 	 0.21418 	 ~...
   81 	    82 	 0.25626 	 0.22580 	 m..s
   84 	    83 	 0.27202 	 0.23386 	 m..s
   82 	    84 	 0.25674 	 0.23853 	 ~...
   83 	    85 	 0.27134 	 0.25103 	 ~...
   91 	    86 	 0.31111 	 0.25404 	 m..s
   85 	    87 	 0.27786 	 0.25657 	 ~...
   87 	    88 	 0.28651 	 0.27694 	 ~...
   86 	    89 	 0.28580 	 0.27912 	 ~...
   92 	    90 	 0.31141 	 0.28199 	 ~...
   96 	    91 	 0.34615 	 0.28370 	 m..s
   89 	    92 	 0.29112 	 0.28459 	 ~...
   90 	    93 	 0.30938 	 0.29097 	 ~...
   88 	    94 	 0.29048 	 0.29398 	 ~...
   97 	    95 	 0.34761 	 0.30084 	 m..s
   98 	    96 	 0.35072 	 0.30574 	 m..s
   93 	    97 	 0.32711 	 0.32045 	 ~...
   95 	    98 	 0.33864 	 0.32599 	 ~...
   94 	    99 	 0.33255 	 0.33010 	 ~...
  101 	   100 	 0.35658 	 0.33520 	 ~...
  100 	   101 	 0.35631 	 0.33811 	 ~...
  102 	   102 	 0.35717 	 0.34025 	 ~...
   99 	   103 	 0.35550 	 0.34880 	 ~...
  103 	   104 	 0.35836 	 0.34916 	 ~...
  108 	   105 	 0.38663 	 0.36022 	 ~...
  110 	   106 	 0.40520 	 0.36283 	 m..s
  106 	   107 	 0.37797 	 0.36880 	 ~...
  109 	   108 	 0.38762 	 0.38537 	 ~...
  112 	   109 	 0.45602 	 0.38991 	 m..s
  117 	   110 	 0.50721 	 0.40258 	 MISS
  104 	   111 	 0.36804 	 0.42826 	 m..s
  113 	   112 	 0.46342 	 0.44556 	 ~...
  114 	   113 	 0.48353 	 0.44843 	 m..s
  107 	   114 	 0.38164 	 0.44883 	 m..s
  111 	   115 	 0.41600 	 0.45341 	 m..s
  116 	   116 	 0.49949 	 0.46828 	 m..s
  115 	   117 	 0.48549 	 0.51634 	 m..s
  118 	   118 	 0.55943 	 0.60870 	 m..s
  120 	   119 	 0.56959 	 0.62714 	 m..s
  119 	   120 	 0.56120 	 0.62777 	 m..s
==========================================
r_mrr = 0.9823678135871887
r2_mrr = 0.957409679889679
spearmanr_mrr@5 = 0.9841711521148682
spearmanr_mrr@10 = 0.9399765133857727
spearmanr_mrr@50 = 0.9896672964096069
spearmanr_mrr@100 = 0.9947763681411743
spearmanr_mrr@All = 0.9950820207595825
==========================================
test time: 0.442
Done Testing dataset Kinships
total time taken: 233.39015197753906
training time taken: 225.62006425857544
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9824)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9574)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9842)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9400)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9897)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9948)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9951)}}, 'test_loss': {'ComplEx': {'Kinships': 1.3073170384741388}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 8738157128374912
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [182, 208, 162, 276, 796, 1173, 926, 1183, 460, 514, 302, 93, 950, 581, 678, 630, 1016, 116, 757, 456, 880, 511, 213, 167, 835, 526, 212, 370, 468, 1209, 934, 873, 477, 1124, 1094, 656, 1053, 445, 444, 153, 466, 1205, 257, 764, 216, 991, 551, 1195, 336, 913, 859, 588, 156, 102, 576, 1002, 846, 516, 541, 774, 734, 317, 327, 498, 347, 1158, 717, 919, 505, 1000, 160, 847, 899, 256, 698, 751, 715, 562, 1161, 1089, 136, 537, 1153, 932, 333, 839, 821, 726, 411, 398, 719, 197, 958, 801, 422, 963, 268, 1047, 906, 418, 1159, 542, 828, 353, 1065, 777, 176, 38, 969, 490, 134, 352, 471, 528, 454, 1034, 791, 401, 391, 945, 785]
valid_ids (0): []
train_ids (1094): [818, 691, 739, 856, 14, 243, 1208, 95, 977, 356, 232, 824, 1181, 465, 1003, 907, 935, 980, 177, 651, 1101, 831, 1033, 1037, 683, 78, 961, 191, 1149, 558, 819, 278, 659, 224, 1118, 749, 1202, 339, 384, 1045, 1168, 185, 201, 499, 536, 1060, 974, 22, 138, 36, 1018, 643, 1160, 381, 475, 396, 290, 54, 627, 822, 1207, 931, 483, 82, 172, 282, 917, 793, 393, 882, 28, 410, 1138, 669, 649, 535, 273, 790, 1120, 1152, 794, 461, 611, 787, 673, 369, 463, 646, 120, 916, 539, 253, 187, 1070, 805, 608, 255, 569, 52, 557, 879, 12, 313, 744, 32, 436, 925, 591, 41, 840, 429, 311, 237, 448, 375, 103, 1106, 682, 488, 1199, 128, 293, 567, 1014, 766, 1054, 944, 56, 101, 83, 1164, 686, 681, 628, 540, 378, 1027, 264, 2, 148, 431, 632, 741, 110, 813, 631, 439, 481, 606, 929, 836, 583, 357, 784, 523, 877, 140, 768, 263, 13, 395, 1043, 890, 100, 587, 598, 416, 137, 8, 620, 275, 892, 662, 1114, 721, 655, 484, 420, 966, 850, 27, 44, 756, 1042, 1147, 638, 462, 61, 474, 188, 952, 1082, 214, 954, 616, 210, 555, 520, 159, 872, 15, 968, 79, 489, 1052, 529, 1174, 178, 692, 809, 563, 512, 392, 1194, 1049, 1004, 453, 179, 604, 509, 151, 970, 267, 192, 437, 500, 1007, 402, 1046, 600, 96, 697, 441, 106, 761, 299, 1040, 1105, 226, 808, 891, 578, 527, 885, 838, 782, 48, 610, 1171, 765, 139, 605, 1073, 1196, 113, 222, 355, 298, 860, 513, 1197, 458, 566, 957, 487, 451, 26, 123, 889, 60, 613, 920, 740, 586, 705, 1031, 675, 534, 430, 990, 670, 29, 1112, 1175, 181, 190, 132, 617, 320, 995, 17, 53, 90, 72, 478, 37, 579, 911, 1097, 1136, 144, 799, 387, 109, 309, 1129, 360, 364, 1186, 80, 773, 1140, 700, 225, 830, 564, 665, 1122, 130, 747, 811, 1125, 58, 108, 343, 1064, 1155, 946, 377, 825, 399, 236, 390, 359, 271, 1066, 903, 769, 806, 1063, 1096, 637, 1130, 728, 762, 574, 967, 888, 1109, 924, 1163, 729, 5, 57, 975, 440, 491, 152, 900, 400, 857, 205, 64, 1036, 1166, 997, 435, 476, 671, 374, 230, 914, 404, 1, 99, 571, 450, 875, 1087, 565, 994, 397, 20, 142, 170, 742, 778, 1024, 827, 350, 68, 304, 1115, 202, 1144, 125, 1151, 358, 10, 168, 973, 19, 81, 862, 989, 1139, 789, 259, 164, 547, 365, 624, 269, 549, 962, 603, 295, 1099, 300, 895, 442, 447, 217, 443, 707, 948, 292, 992, 65, 1090, 696, 972, 1178, 635, 652, 413, 690, 909, 55, 956, 797, 45, 1142, 1146, 876, 301, 609, 193, 1137, 1148, 708, 843, 59, 1029, 1069, 131, 905, 896, 570, 486, 965, 626, 296, 1201, 1211, 783, 897, 220, 702, 976, 1111, 874, 280, 941, 942, 1204, 864, 519, 1102, 1035, 883, 438, 325, 284, 1038, 363, 73, 869, 775, 89, 998, 676, 1019, 124, 265, 260, 940, 1015, 196, 452, 209, 1188, 1008, 324, 251, 544, 332, 129, 711, 1116, 126, 1131, 495, 650, 663, 999, 982, 479, 693, 723, 607, 661, 1020, 894, 158, 522, 1121, 211, 814, 1192, 955, 1126, 227, 127, 229, 653, 105, 657, 680, 303, 1072, 1132, 1079, 781, 845, 1128, 335, 703, 326, 908, 233, 200, 507, 85, 625, 281, 709, 1032, 904, 927, 618, 508, 199, 533, 1086, 798, 959, 532, 928, 91, 341, 660, 104, 936, 1156, 861, 473, 351, 832, 385, 780, 33, 244, 349, 94, 922, 97, 1172, 713, 346, 530, 1048, 433, 985, 867, 538, 379, 1076, 6, 901, 679, 594, 1010, 800, 504, 515, 169, 143, 1084, 858, 674, 270, 687, 561, 362, 286, 98, 572, 577, 1212, 344, 145, 1104, 7, 49, 1117, 701, 585, 853, 330, 758, 1154, 792, 596, 615, 389, 77, 1162, 415, 664, 634, 122, 219, 1210, 1057, 943, 1088, 645, 366, 834, 1206, 146, 1006, 316, 470, 964, 239, 1170, 868, 1044, 174, 854, 340, 111, 1193, 754, 246, 310, 795, 114, 1062, 844, 1214, 1067, 510, 672, 75, 367, 0, 175, 750, 878, 641, 1103, 469, 612, 107, 779, 371, 803, 403, 40, 710, 1189, 86, 1059, 724, 485, 614, 328, 658, 531, 88, 550, 988, 545, 1176, 893, 506, 767, 1184, 307, 863, 851, 221, 407, 306, 1141, 601, 321, 815, 714, 9, 112, 548, 971, 619, 1058, 589, 1203, 786, 329, 580, 373, 414, 186, 412, 590, 978, 1182, 1150, 1092, 69, 50, 247, 748, 331, 745, 1167, 71, 746, 382, 319, 983, 380, 46, 1026, 11, 1135, 426, 1023, 39, 1185, 250, 736, 277, 770, 812, 939, 147, 42, 599, 1200, 30, 92, 1012, 849, 31, 1078, 887, 755, 848, 287, 376, 16, 923, 737, 575, 194, 870, 622, 647, 63, 712, 171, 322, 1028, 241, 449, 1190, 223, 1110, 837, 621, 930, 492, 279, 1127, 1091, 644, 51, 3, 688, 231, 816, 315, 884, 249, 386, 871, 35, 235, 446, 423, 135, 738, 117, 464, 735, 1077, 866, 1061, 1083, 74, 1085, 141, 833, 1123, 157, 1022, 1039, 518, 368, 718, 274, 684, 1017, 953, 568, 546, 602, 685, 457, 1051, 266, 240, 501, 66, 689, 165, 1119, 759, 722, 155, 560, 184, 1165, 195, 732, 425, 636, 1005, 203, 354, 204, 502, 348, 639, 521, 118, 163, 459, 405, 1100, 543, 949, 4, 87, 248, 283, 996, 573, 342, 951, 121, 763, 305, 772, 1179, 323, 1071, 388, 297, 294, 261, 826, 553, 455, 150, 1133, 406, 1198, 133, 752, 372, 18, 215, 1108, 334, 1177, 595, 979, 1050, 47, 623, 947, 807, 198, 912, 1169, 654, 417, 480, 1098, 915, 23, 76, 960, 1113, 242, 582, 1011, 1055, 424, 394, 115, 427, 262, 694, 308, 720, 67, 337, 497, 1074, 21, 776, 1001, 993, 695, 706, 285, 312, 841, 743, 1095, 288, 593, 493, 938, 1134, 525, 318, 881, 207, 1107, 1068, 238, 408, 987, 161, 421, 716, 788, 731, 119, 43, 886, 206, 1056, 432, 668, 1030, 667, 218, 820, 467, 725, 252, 228, 1093, 291, 1041, 183, 189, 554, 842, 733, 428, 642, 503, 804, 254, 234, 817, 345, 154, 34, 802, 482, 1143, 1081, 597, 984, 727, 981, 918, 810, 434, 245, 855, 494, 1021, 552, 730, 1145, 419, 937, 314, 1075, 559, 62, 1009, 898, 556, 1187, 1080, 704, 771, 986, 1180, 338, 173, 760, 1157, 910, 823, 1013, 852, 648, 409, 84, 149, 166, 699, 666, 24, 383, 921, 180, 584, 592, 258, 361, 640, 496, 472, 524, 517, 629, 25, 933, 865, 289, 829, 70, 1191, 1025, 902, 677, 753, 633, 272, 1213]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1711305876456518
the save name prefix for this run is:  chkpt-ID_1711305876456518_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1107
rank avg (pred): 0.553 +- 0.007
mrr vals (pred, true): 0.017, 0.059
batch losses (mrrl, rdl): 0.0, 0.0002135597

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 569
rank avg (pred): 0.426 +- 0.263
mrr vals (pred, true): 0.116, 0.053
batch losses (mrrl, rdl): 0.0, 9.7757e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1134
rank avg (pred): 0.114 +- 0.080
mrr vals (pred, true): 0.221, 0.267
batch losses (mrrl, rdl): 0.0, 3.3099e-06

Epoch over!
epoch time: 14.997

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 848
rank avg (pred): 0.445 +- 0.262
mrr vals (pred, true): 0.087, 0.060
batch losses (mrrl, rdl): 0.0, 1.9588e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 683
rank avg (pred): 0.454 +- 0.269
mrr vals (pred, true): 0.078, 0.060
batch losses (mrrl, rdl): 0.0, 1.19261e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 507
rank avg (pred): 0.157 +- 0.111
mrr vals (pred, true): 0.169, 0.205
batch losses (mrrl, rdl): 0.0, 1.25838e-05

Epoch over!
epoch time: 14.985

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 260
rank avg (pred): 0.089 +- 0.069
mrr vals (pred, true): 0.256, 0.370
batch losses (mrrl, rdl): 0.0, 3.3204e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1090
rank avg (pred): 0.454 +- 0.268
mrr vals (pred, true): 0.066, 0.050
batch losses (mrrl, rdl): 0.0, 2.3569e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 713
rank avg (pred): 0.454 +- 0.263
mrr vals (pred, true): 0.064, 0.063
batch losses (mrrl, rdl): 0.0, 1.3344e-06

Epoch over!
epoch time: 14.971

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 803
rank avg (pred): 0.439 +- 0.262
mrr vals (pred, true): 0.067, 0.059
batch losses (mrrl, rdl): 0.0, 8.764e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 766
rank avg (pred): 0.453 +- 0.260
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0, 2.8426e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 96
rank avg (pred): 0.449 +- 0.262
mrr vals (pred, true): 0.068, 0.051
batch losses (mrrl, rdl): 0.0, 1.1755e-06

Epoch over!
epoch time: 14.887

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 55
rank avg (pred): 0.109 +- 0.093
mrr vals (pred, true): 0.270, 0.327
batch losses (mrrl, rdl): 0.0, 3.9651e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1067
rank avg (pred): 0.078 +- 0.070
mrr vals (pred, true): 0.316, 0.405
batch losses (mrrl, rdl): 0.0, 1.66e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1131
rank avg (pred): 0.482 +- 0.276
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 0.0, 6.0576e-06

Epoch over!
epoch time: 14.856

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 794
rank avg (pred): 0.440 +- 0.267
mrr vals (pred, true): 0.066, 0.054
batch losses (mrrl, rdl): 0.0026484046, 2.8672e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1060
rank avg (pred): 0.027 +- 0.021
mrr vals (pred, true): 0.466, 0.503
batch losses (mrrl, rdl): 0.0132177407, 2.74383e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 69
rank avg (pred): 0.181 +- 0.126
mrr vals (pred, true): 0.240, 0.272
batch losses (mrrl, rdl): 0.0103908442, 9.508e-05

Epoch over!
epoch time: 15.078

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 754
rank avg (pred): 0.152 +- 0.117
mrr vals (pred, true): 0.305, 0.310
batch losses (mrrl, rdl): 0.0002530186, 2.19636e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 224
rank avg (pred): 0.448 +- 0.171
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 4.7229e-06, 2.78634e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 342
rank avg (pred): 0.456 +- 0.153
mrr vals (pred, true): 0.039, 0.051
batch losses (mrrl, rdl): 0.0011157951, 3.84942e-05

Epoch over!
epoch time: 15.064

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1130
rank avg (pred): 0.448 +- 0.161
mrr vals (pred, true): 0.044, 0.053
batch losses (mrrl, rdl): 0.0003663156, 4.69419e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 905
rank avg (pred): 0.307 +- 0.221
mrr vals (pred, true): 0.255, 0.228
batch losses (mrrl, rdl): 0.0071504433, 0.000580456

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 565
rank avg (pred): 0.341 +- 0.233
mrr vals (pred, true): 0.218, 0.193
batch losses (mrrl, rdl): 0.0059873355, 0.0005665174

Epoch over!
epoch time: 15.141

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 297
rank avg (pred): 0.141 +- 0.098
mrr vals (pred, true): 0.280, 0.302
batch losses (mrrl, rdl): 0.0047723977, 2.12463e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1179
rank avg (pred): 0.471 +- 0.140
mrr vals (pred, true): 0.041, 0.056
batch losses (mrrl, rdl): 0.0007883575, 4.59783e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 769
rank avg (pred): 0.449 +- 0.153
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001299286, 3.833e-05

Epoch over!
epoch time: 15.205

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 555
rank avg (pred): 0.338 +- 0.228
mrr vals (pred, true): 0.214, 0.182
batch losses (mrrl, rdl): 0.0097410819, 0.0006212262

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 252
rank avg (pred): 0.102 +- 0.070
mrr vals (pred, true): 0.301, 0.282
batch losses (mrrl, rdl): 0.0034883353, 3.98961e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1200
rank avg (pred): 0.463 +- 0.133
mrr vals (pred, true): 0.043, 0.060
batch losses (mrrl, rdl): 0.0005613808, 5.12116e-05

Epoch over!
epoch time: 15.265

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 319
rank avg (pred): 0.159 +- 0.116
mrr vals (pred, true): 0.300, 0.281
batch losses (mrrl, rdl): 0.0036769062, 1.22938e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 481
rank avg (pred): 0.458 +- 0.137
mrr vals (pred, true): 0.049, 0.053
batch losses (mrrl, rdl): 4.3876e-06, 4.83759e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 232
rank avg (pred): 0.458 +- 0.133
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 8.72129e-05, 4.72238e-05

Epoch over!
epoch time: 15.113

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1024
rank avg (pred): 0.439 +- 0.152
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0005477531, 4.50827e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 596
rank avg (pred): 0.456 +- 0.128
mrr vals (pred, true): 0.048, 0.060
batch losses (mrrl, rdl): 4.06161e-05, 5.0509e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 798
rank avg (pred): 0.471 +- 0.107
mrr vals (pred, true): 0.038, 0.050
batch losses (mrrl, rdl): 0.0013622318, 6.51335e-05

Epoch over!
epoch time: 15.117

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 964
rank avg (pred): 0.446 +- 0.138
mrr vals (pred, true): 0.056, 0.057
batch losses (mrrl, rdl): 0.0004025463, 4.68182e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 646
rank avg (pred): 0.461 +- 0.120
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 0.0001079782, 5.41185e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 168
rank avg (pred): 0.449 +- 0.124
mrr vals (pred, true): 0.051, 0.056
batch losses (mrrl, rdl): 1.76429e-05, 5.141e-05

Epoch over!
epoch time: 15.194

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 450
rank avg (pred): 0.455 +- 0.118
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 1.66518e-05, 5.15757e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 238
rank avg (pred): 0.446 +- 0.125
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 0.0001044177, 7.15112e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 641
rank avg (pred): 0.442 +- 0.126
mrr vals (pred, true): 0.055, 0.059
batch losses (mrrl, rdl): 0.000247667, 5.73506e-05

Epoch over!
epoch time: 15.108

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 479
rank avg (pred): 0.444 +- 0.120
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 5.15837e-05, 6.28767e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 559
rank avg (pred): 0.404 +- 0.261
mrr vals (pred, true): 0.209, 0.198
batch losses (mrrl, rdl): 0.0012344354, 0.0013199431

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 892
rank avg (pred): 0.037 +- 0.026
mrr vals (pred, true): 0.406, 0.449
batch losses (mrrl, rdl): 0.0180351678, 8.1516e-06

Epoch over!
epoch time: 15.215

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.446 +- 0.118
mrr vals (pred, true): 0.053, 0.056

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04911 	 0.04376 	 ~...
   83 	     1 	 0.05781 	 0.04420 	 ~...
   20 	     2 	 0.05204 	 0.04603 	 ~...
   78 	     3 	 0.05611 	 0.04668 	 ~...
   45 	     4 	 0.05328 	 0.04731 	 ~...
    8 	     5 	 0.05015 	 0.04763 	 ~...
    5 	     6 	 0.04959 	 0.04772 	 ~...
   19 	     7 	 0.05186 	 0.04778 	 ~...
   60 	     8 	 0.05404 	 0.04818 	 ~...
   66 	     9 	 0.05453 	 0.04830 	 ~...
   27 	    10 	 0.05273 	 0.04851 	 ~...
   56 	    11 	 0.05375 	 0.04852 	 ~...
    4 	    12 	 0.04954 	 0.04876 	 ~...
   69 	    13 	 0.05473 	 0.04903 	 ~...
   14 	    14 	 0.05111 	 0.04908 	 ~...
   62 	    15 	 0.05418 	 0.04925 	 ~...
   67 	    16 	 0.05466 	 0.04941 	 ~...
   85 	    17 	 0.05957 	 0.04971 	 ~...
   23 	    18 	 0.05215 	 0.05014 	 ~...
   46 	    19 	 0.05331 	 0.05025 	 ~...
    7 	    20 	 0.04994 	 0.05028 	 ~...
   70 	    21 	 0.05478 	 0.05038 	 ~...
   64 	    22 	 0.05423 	 0.05080 	 ~...
    3 	    23 	 0.04928 	 0.05121 	 ~...
   43 	    24 	 0.05324 	 0.05123 	 ~...
   63 	    25 	 0.05421 	 0.05136 	 ~...
   32 	    26 	 0.05284 	 0.05168 	 ~...
   41 	    27 	 0.05319 	 0.05174 	 ~...
   25 	    28 	 0.05251 	 0.05188 	 ~...
   82 	    29 	 0.05662 	 0.05216 	 ~...
   12 	    30 	 0.05086 	 0.05221 	 ~...
   40 	    31 	 0.05312 	 0.05226 	 ~...
   76 	    32 	 0.05542 	 0.05232 	 ~...
   52 	    33 	 0.05366 	 0.05268 	 ~...
   35 	    34 	 0.05299 	 0.05274 	 ~...
   18 	    35 	 0.05178 	 0.05285 	 ~...
   86 	    36 	 0.06031 	 0.05286 	 ~...
   73 	    37 	 0.05513 	 0.05307 	 ~...
   80 	    38 	 0.05652 	 0.05312 	 ~...
   24 	    39 	 0.05237 	 0.05313 	 ~...
   81 	    40 	 0.05655 	 0.05321 	 ~...
    9 	    41 	 0.05034 	 0.05338 	 ~...
   13 	    42 	 0.05095 	 0.05350 	 ~...
   59 	    43 	 0.05395 	 0.05358 	 ~...
   37 	    44 	 0.05301 	 0.05361 	 ~...
   75 	    45 	 0.05540 	 0.05370 	 ~...
   48 	    46 	 0.05340 	 0.05375 	 ~...
   30 	    47 	 0.05274 	 0.05379 	 ~...
   51 	    48 	 0.05361 	 0.05380 	 ~...
   55 	    49 	 0.05373 	 0.05392 	 ~...
   22 	    50 	 0.05215 	 0.05398 	 ~...
   50 	    51 	 0.05359 	 0.05404 	 ~...
    6 	    52 	 0.04969 	 0.05412 	 ~...
   31 	    53 	 0.05282 	 0.05417 	 ~...
   61 	    54 	 0.05417 	 0.05427 	 ~...
   39 	    55 	 0.05310 	 0.05444 	 ~...
   21 	    56 	 0.05213 	 0.05451 	 ~...
   26 	    57 	 0.05253 	 0.05456 	 ~...
   77 	    58 	 0.05559 	 0.05469 	 ~...
   29 	    59 	 0.05273 	 0.05498 	 ~...
   84 	    60 	 0.05943 	 0.05521 	 ~...
   65 	    61 	 0.05425 	 0.05524 	 ~...
   28 	    62 	 0.05273 	 0.05540 	 ~...
   49 	    63 	 0.05354 	 0.05550 	 ~...
   16 	    64 	 0.05136 	 0.05569 	 ~...
   71 	    65 	 0.05492 	 0.05571 	 ~...
   11 	    66 	 0.05079 	 0.05571 	 ~...
   42 	    67 	 0.05321 	 0.05580 	 ~...
   47 	    68 	 0.05336 	 0.05585 	 ~...
   10 	    69 	 0.05075 	 0.05614 	 ~...
   58 	    70 	 0.05389 	 0.05631 	 ~...
   36 	    71 	 0.05300 	 0.05651 	 ~...
   33 	    72 	 0.05292 	 0.05656 	 ~...
   74 	    73 	 0.05528 	 0.05660 	 ~...
   68 	    74 	 0.05466 	 0.05693 	 ~...
    2 	    75 	 0.04919 	 0.05699 	 ~...
   17 	    76 	 0.05177 	 0.05723 	 ~...
   79 	    77 	 0.05636 	 0.05860 	 ~...
   44 	    78 	 0.05328 	 0.05868 	 ~...
   15 	    79 	 0.05117 	 0.05882 	 ~...
   53 	    80 	 0.05366 	 0.05894 	 ~...
   72 	    81 	 0.05493 	 0.05901 	 ~...
    0 	    82 	 0.04860 	 0.05910 	 ~...
   54 	    83 	 0.05369 	 0.05961 	 ~...
   57 	    84 	 0.05384 	 0.05982 	 ~...
   34 	    85 	 0.05298 	 0.06053 	 ~...
   38 	    86 	 0.05309 	 0.06111 	 ~...
   87 	    87 	 0.20564 	 0.08417 	 MISS
   87 	    88 	 0.20564 	 0.16878 	 m..s
   87 	    89 	 0.20564 	 0.18404 	 ~...
   91 	    90 	 0.20724 	 0.20118 	 ~...
   87 	    91 	 0.20564 	 0.20204 	 ~...
  101 	    92 	 0.24435 	 0.20831 	 m..s
   99 	    93 	 0.23508 	 0.21207 	 ~...
   92 	    94 	 0.20880 	 0.21521 	 ~...
   96 	    95 	 0.22302 	 0.21623 	 ~...
   97 	    96 	 0.22953 	 0.21714 	 ~...
   93 	    97 	 0.21364 	 0.22061 	 ~...
   98 	    98 	 0.23319 	 0.22722 	 ~...
   95 	    99 	 0.21828 	 0.23036 	 ~...
   94 	   100 	 0.21786 	 0.23595 	 ~...
  102 	   101 	 0.26000 	 0.24673 	 ~...
  100 	   102 	 0.24094 	 0.24847 	 ~...
  103 	   103 	 0.28036 	 0.25895 	 ~...
  107 	   104 	 0.30809 	 0.29150 	 ~...
  106 	   105 	 0.30679 	 0.29398 	 ~...
  109 	   106 	 0.31373 	 0.30389 	 ~...
  104 	   107 	 0.30319 	 0.31912 	 ~...
  105 	   108 	 0.30507 	 0.32045 	 ~...
  111 	   109 	 0.32656 	 0.32286 	 ~...
  108 	   110 	 0.31263 	 0.33111 	 ~...
  112 	   111 	 0.33345 	 0.33738 	 ~...
  110 	   112 	 0.31805 	 0.34735 	 ~...
  113 	   113 	 0.34058 	 0.36880 	 ~...
  116 	   114 	 0.40184 	 0.38637 	 ~...
  114 	   115 	 0.35731 	 0.39820 	 m..s
  117 	   116 	 0.42696 	 0.40967 	 ~...
  118 	   117 	 0.42786 	 0.41066 	 ~...
  119 	   118 	 0.43207 	 0.43012 	 ~...
  115 	   119 	 0.40119 	 0.45038 	 m..s
  120 	   120 	 0.52684 	 0.62140 	 m..s
==========================================
r_mrr = 0.9891363382339478
r2_mrr = 0.9779604077339172
spearmanr_mrr@5 = 0.9824769496917725
spearmanr_mrr@10 = 0.9449982643127441
spearmanr_mrr@50 = 0.985107958316803
spearmanr_mrr@100 = 0.9909871816635132
spearmanr_mrr@All = 0.9916674494743347
==========================================
test time: 0.446
Done Testing dataset Kinships
total time taken: 234.42605781555176
training time taken: 226.6540322303772
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9891)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9780)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9825)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9450)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9851)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9910)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9917)}}, 'test_loss': {'ComplEx': {'Kinships': 0.49604612695839023}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 9746375646397996
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1167, 329, 79, 739, 593, 723, 231, 545, 309, 845, 698, 418, 65, 606, 84, 46, 1128, 1048, 915, 1134, 943, 840, 1017, 669, 821, 381, 810, 987, 842, 477, 12, 668, 147, 205, 125, 294, 286, 1001, 434, 687, 129, 707, 4, 240, 60, 899, 1179, 53, 480, 8, 2, 499, 392, 85, 1089, 813, 190, 106, 1209, 939, 248, 311, 1140, 478, 218, 645, 543, 524, 647, 308, 704, 1163, 226, 442, 1051, 1143, 1211, 729, 276, 41, 1176, 1055, 555, 201, 1117, 1170, 140, 626, 1212, 378, 176, 568, 1027, 1103, 193, 516, 155, 721, 981, 1174, 886, 651, 1070, 763, 627, 650, 1138, 942, 1186, 825, 426, 73, 1189, 1120, 802, 55, 920, 28, 918, 123, 498]
valid_ids (0): []
train_ids (1094): [1214, 379, 624, 951, 561, 330, 642, 881, 115, 66, 401, 991, 214, 1037, 181, 1173, 590, 307, 796, 888, 1210, 277, 528, 353, 831, 163, 768, 388, 697, 210, 851, 678, 900, 209, 760, 709, 296, 1191, 1041, 877, 969, 549, 998, 23, 130, 14, 132, 879, 490, 634, 441, 1095, 520, 930, 482, 793, 649, 278, 718, 858, 811, 445, 394, 166, 578, 790, 346, 944, 1203, 1145, 622, 798, 803, 1151, 596, 563, 1091, 13, 458, 518, 570, 773, 658, 15, 1015, 198, 1164, 875, 786, 456, 252, 1102, 457, 454, 670, 86, 1193, 199, 931, 435, 154, 38, 247, 334, 841, 836, 238, 419, 871, 685, 323, 996, 592, 859, 466, 955, 267, 856, 400, 769, 1129, 1177, 903, 986, 883, 491, 523, 715, 157, 743, 961, 664, 756, 680, 179, 666, 64, 259, 728, 688, 1094, 101, 701, 373, 783, 994, 58, 504, 675, 356, 317, 800, 475, 682, 677, 325, 312, 1078, 94, 548, 372, 791, 459, 290, 933, 767, 200, 74, 78, 119, 156, 118, 162, 358, 742, 1202, 967, 422, 429, 185, 342, 1026, 249, 175, 661, 139, 173, 817, 1034, 148, 1108, 17, 1036, 88, 1, 6, 1004, 246, 905, 387, 510, 243, 1118, 174, 1074, 1086, 572, 988, 919, 801, 486, 260, 646, 1201, 1008, 272, 902, 618, 740, 684, 500, 546, 1049, 494, 241, 603, 3, 979, 1058, 288, 135, 391, 415, 211, 508, 759, 517, 982, 1052, 83, 1156, 1032, 1110, 313, 735, 892, 167, 228, 1038, 62, 713, 1077, 519, 109, 237, 522, 749, 965, 348, 389, 253, 124, 737, 122, 215, 245, 295, 702, 576, 908, 950, 142, 357, 133, 623, 619, 1047, 47, 1035, 533, 1013, 285, 302, 131, 421, 439, 127, 946, 361, 301, 1127, 750, 316, 1107, 872, 45, 54, 861, 385, 492, 582, 695, 693, 452, 44, 992, 474, 1132, 909, 481, 1093, 273, 239, 1040, 7, 540, 662, 1012, 380, 34, 714, 159, 1050, 894, 414, 635, 959, 51, 1072, 298, 692, 581, 970, 416, 826, 954, 878, 244, 852, 183, 844, 922, 332, 483, 1061, 613, 656, 374, 77, 300, 324, 962, 1161, 39, 1162, 972, 192, 912, 364, 819, 876, 397, 1084, 489, 1081, 1105, 462, 29, 496, 799, 997, 1206, 449, 539, 754, 553, 406, 639, 604, 1087, 1147, 460, 1188, 1064, 365, 591, 676, 863, 839, 691, 417, 487, 515, 37, 91, 168, 1204, 367, 161, 751, 87, 507, 432, 509, 1022, 617, 377, 68, 476, 1130, 112, 99, 1131, 1045, 121, 32, 734, 537, 279, 679, 512, 1075, 340, 868, 726, 804, 110, 443, 484, 690, 337, 1199, 319, 448, 806, 587, 1207, 59, 1180, 1005, 536, 874, 48, 1028, 755, 468, 1009, 706, 938, 990, 404, 424, 72, 832, 194, 1139, 322, 1079, 779, 1115, 621, 223, 1113, 144, 265, 502, 262, 164, 1141, 384, 667, 673, 315, 873, 1080, 327, 846, 923, 689, 889, 141, 440, 36, 1144, 648, 61, 203, 830, 978, 1200, 178, 146, 150, 565, 1205, 1178, 220, 1148, 453, 1184, 562, 501, 652, 571, 1057, 25, 1106, 824, 532, 18, 172, 386, 822, 1063, 197, 829, 229, 398, 616, 1160, 665, 632, 927, 344, 1194, 1016, 789, 747, 674, 864, 525, 848, 631, 281, 828, 347, 1066, 816, 446, 1175, 196, 409, 188, 977, 1185, 663, 93, 1071, 89, 940, 33, 369, 1100, 1169, 1062, 69, 823, 1114, 1116, 67, 960, 455, 352, 236, 827, 834, 1073, 1195, 165, 1125, 866, 580, 999, 584, 1025, 436, 808, 1136, 376, 463, 363, 552, 867, 686, 270, 535, 80, 1135, 608, 995, 897, 630, 683, 534, 22, 629, 941, 812, 907, 657, 271, 287, 493, 473, 1159, 741, 1183, 395, 503, 611, 855, 405, 904, 331, 306, 521, 513, 469, 428, 948, 1197, 202, 612, 92, 182, 338, 382, 1190, 564, 318, 814, 1146, 339, 153, 1150, 283, 1068, 105, 1152, 917, 76, 1192, 297, 497, 488, 470, 1056, 292, 5, 350, 1213, 1098, 722, 774, 1082, 560, 926, 1097, 833, 609, 314, 396, 929, 731, 880, 427, 780, 765, 542, 579, 42, 1096, 1003, 35, 56, 643, 974, 1119, 355, 711, 558, 547, 720, 901, 890, 180, 752, 100, 304, 408, 354, 43, 1208, 26, 934, 820, 70, 569, 11, 1023, 1067, 818, 640, 256, 310, 186, 605, 1033, 895, 264, 653, 506, 250, 1133, 976, 1149, 993, 230, 321, 225, 853, 138, 465, 328, 30, 705, 407, 869, 1030, 169, 1007, 431, 104, 968, 438, 75, 1109, 757, 937, 849, 1024, 600, 40, 654, 712, 402, 914, 594, 187, 343, 730, 255, 764, 207, 805, 589, 191, 1099, 1153, 116, 857, 781, 50, 766, 732, 1142, 550, 275, 413, 1172, 97, 703, 336, 1006, 785, 370, 637, 544, 947, 10, 699, 1122, 189, 217, 303, 251, 1171, 511, 126, 1069, 1124, 744, 838, 807, 433, 1053, 794, 966, 1154, 1076, 222, 538, 242, 410, 206, 924, 708, 160, 911, 753, 375, 27, 636, 117, 610, 633, 925, 280, 450, 120, 49, 1168, 1029, 566, 1059, 1181, 1000, 1002, 913, 102, 638, 655, 216, 556, 1137, 795, 696, 599, 447, 885, 158, 472, 351, 577, 149, 615, 531, 583, 284, 758, 57, 671, 644, 541, 326, 9, 854, 588, 1155, 882, 145, 906, 299, 1187, 896, 20, 865, 261, 921, 1090, 614, 461, 952, 1011, 305, 985, 527, 788, 430, 1092, 108, 1157, 776, 557, 607, 575, 898, 554, 221, 850, 574, 291, 1010, 710, 598, 916, 467, 98, 213, 371, 815, 771, 16, 893, 177, 748, 862, 514, 366, 204, 257, 551, 293, 641, 341, 320, 96, 843, 1198, 111, 597, 82, 601, 1101, 567, 208, 945, 887, 1043, 1018, 891, 234, 778, 24, 1021, 274, 953, 736, 681, 983, 333, 787, 949, 1060, 1166, 263, 1019, 1085, 1112, 725, 95, 266, 797, 761, 254, 775, 936, 727, 19, 719, 485, 971, 1031, 345, 349, 63, 784, 745, 103, 659, 471, 235, 1165, 772, 1044, 128, 700, 1039, 809, 151, 530, 1123, 1046, 137, 717, 479, 620, 383, 625, 733, 31, 770, 1083, 152, 269, 777, 90, 425, 1196, 444, 884, 660, 233, 762, 411, 1020, 989, 628, 975, 390, 170, 1126, 1121, 716, 694, 136, 224, 738, 107, 113, 282, 935, 505, 792, 1054, 268, 184, 52, 870, 362, 956, 360, 602, 420, 227, 335, 928, 359, 212, 289, 232, 910, 403, 81, 219, 782, 437, 0, 984, 393, 258, 526, 21, 964, 860, 1104, 195, 957, 134, 963, 585, 495, 835, 1158, 464, 114, 595, 399, 573, 171, 1065, 973, 1042, 451, 559, 746, 423, 412, 932, 837, 71, 1014, 672, 368, 586, 143, 529, 1111, 1088, 724, 847, 980, 1182, 958]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3625581143562470
the save name prefix for this run is:  chkpt-ID_3625581143562470_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 806
rank avg (pred): 0.542 +- 0.003
mrr vals (pred, true): 0.018, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001648391

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 604
rank avg (pred): 0.457 +- 0.271
mrr vals (pred, true): 0.046, 0.055
batch losses (mrrl, rdl): 0.0, 2.5729e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 109
rank avg (pred): 0.464 +- 0.261
mrr vals (pred, true): 0.039, 0.059
batch losses (mrrl, rdl): 0.0, 2.387e-07

Epoch over!
epoch time: 15.007

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1062
rank avg (pred): 0.060 +- 0.078
mrr vals (pred, true): 0.315, 0.358
batch losses (mrrl, rdl): 0.0, 3.02831e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 405
rank avg (pred): 0.465 +- 0.266
mrr vals (pred, true): 0.041, 0.051
batch losses (mrrl, rdl): 0.0, 3.499e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 206
rank avg (pred): 0.448 +- 0.276
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 0.0, 3.092e-06

Epoch over!
epoch time: 14.893

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 137
rank avg (pred): 0.474 +- 0.263
mrr vals (pred, true): 0.038, 0.054
batch losses (mrrl, rdl): 0.0, 1.4439e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 264
rank avg (pred): 0.120 +- 0.135
mrr vals (pred, true): 0.186, 0.291
batch losses (mrrl, rdl): 0.0, 2.7577e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 515
rank avg (pred): 0.122 +- 0.146
mrr vals (pred, true): 0.202, 0.233
batch losses (mrrl, rdl): 0.0, 1.729e-07

Epoch over!
epoch time: 14.969

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 453
rank avg (pred): 0.463 +- 0.259
mrr vals (pred, true): 0.043, 0.055
batch losses (mrrl, rdl): 0.0, 1.4775e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 683
rank avg (pred): 0.472 +- 0.270
mrr vals (pred, true): 0.043, 0.060
batch losses (mrrl, rdl): 0.0, 1.52759e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1042
rank avg (pred): 0.472 +- 0.263
mrr vals (pred, true): 0.044, 0.064
batch losses (mrrl, rdl): 0.0, 6.9054e-06

Epoch over!
epoch time: 14.986

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 748
rank avg (pred): 0.100 +- 0.138
mrr vals (pred, true): 0.230, 0.412
batch losses (mrrl, rdl): 0.0, 8.7994e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 672
rank avg (pred): 0.463 +- 0.271
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 0.0, 8.2346e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1126
rank avg (pred): 0.466 +- 0.268
mrr vals (pred, true): 0.047, 0.055
batch losses (mrrl, rdl): 0.0, 4.696e-07

Epoch over!
epoch time: 14.878

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 409
rank avg (pred): 0.458 +- 0.291
mrr vals (pred, true): 0.066, 0.057
batch losses (mrrl, rdl): 0.0027044439, 2.4649e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 36
rank avg (pred): 0.228 +- 0.209
mrr vals (pred, true): 0.256, 0.274
batch losses (mrrl, rdl): 0.003458553, 0.0002382301

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 847
rank avg (pred): 0.447 +- 0.159
mrr vals (pred, true): 0.050, 0.059
batch losses (mrrl, rdl): 6.66e-08, 3.75902e-05

Epoch over!
epoch time: 15.085

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 635
rank avg (pred): 0.427 +- 0.155
mrr vals (pred, true): 0.053, 0.056
batch losses (mrrl, rdl): 9.14841e-05, 5.04901e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 113
rank avg (pred): 0.445 +- 0.133
mrr vals (pred, true): 0.044, 0.050
batch losses (mrrl, rdl): 0.0003883913, 5.81987e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 214
rank avg (pred): 0.445 +- 0.130
mrr vals (pred, true): 0.044, 0.060
batch losses (mrrl, rdl): 0.0003698454, 5.30069e-05

Epoch over!
epoch time: 15.08

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1157
rank avg (pred): 0.209 +- 0.200
mrr vals (pred, true): 0.265, 0.281
batch losses (mrrl, rdl): 0.0025134746, 0.0002177052

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 822
rank avg (pred): 0.161 +- 0.169
mrr vals (pred, true): 0.301, 0.448
batch losses (mrrl, rdl): 0.2165130079, 0.0002033975

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 285
rank avg (pred): 0.215 +- 0.191
mrr vals (pred, true): 0.268, 0.277
batch losses (mrrl, rdl): 0.0008840838, 0.0001625347

Epoch over!
epoch time: 15.2

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 890
rank avg (pred): 0.449 +- 0.116
mrr vals (pred, true): 0.043, 0.047
batch losses (mrrl, rdl): 0.000521694, 8.01678e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1047
rank avg (pred): 0.443 +- 0.133
mrr vals (pred, true): 0.052, 0.053
batch losses (mrrl, rdl): 4.92287e-05, 5.2911e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 618
rank avg (pred): 0.436 +- 0.131
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001336762, 6.6265e-05

Epoch over!
epoch time: 15.245

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 871
rank avg (pred): 0.455 +- 0.105
mrr vals (pred, true): 0.038, 0.056
batch losses (mrrl, rdl): 0.0014385382, 7.42644e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 794
rank avg (pred): 0.461 +- 0.122
mrr vals (pred, true): 0.043, 0.054
batch losses (mrrl, rdl): 0.0004424585, 5.22047e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 22
rank avg (pred): 0.223 +- 0.191
mrr vals (pred, true): 0.257, 0.315
batch losses (mrrl, rdl): 0.0342167728, 0.0002882826

Epoch over!
epoch time: 15.238

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 23
rank avg (pred): 0.199 +- 0.187
mrr vals (pred, true): 0.293, 0.361
batch losses (mrrl, rdl): 0.0471180454, 0.0002969578

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 567
rank avg (pred): 0.436 +- 0.129
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 3.75745e-05, 6.15932e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 202
rank avg (pred): 0.437 +- 0.117
mrr vals (pred, true): 0.047, 0.055
batch losses (mrrl, rdl): 6.70386e-05, 6.47927e-05

Epoch over!
epoch time: 15.24

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 656
rank avg (pred): 0.438 +- 0.113
mrr vals (pred, true): 0.044, 0.054
batch losses (mrrl, rdl): 0.0003025787, 7.45408e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 500
rank avg (pred): 0.300 +- 0.198
mrr vals (pred, true): 0.220, 0.251
batch losses (mrrl, rdl): 0.00932974, 0.0007212093

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 631
rank avg (pred): 0.434 +- 0.114
mrr vals (pred, true): 0.046, 0.054
batch losses (mrrl, rdl): 0.0001243282, 8.35359e-05

Epoch over!
epoch time: 15.244

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 262
rank avg (pred): 0.216 +- 0.196
mrr vals (pred, true): 0.287, 0.331
batch losses (mrrl, rdl): 0.0196880214, 0.0002835751

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 160
rank avg (pred): 0.432 +- 0.122
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 5.48111e-05, 9.41108e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 872
rank avg (pred): 0.452 +- 0.091
mrr vals (pred, true): 0.038, 0.056
batch losses (mrrl, rdl): 0.0013839181, 8.58208e-05

Epoch over!
epoch time: 15.244

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 592
rank avg (pred): 0.436 +- 0.112
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 2.45502e-05, 7.66615e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 48
rank avg (pred): 0.259 +- 0.202
mrr vals (pred, true): 0.270, 0.258
batch losses (mrrl, rdl): 0.0012943357, 0.0003854927

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 159
rank avg (pred): 0.434 +- 0.112
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 9.3836e-06, 8.76449e-05

Epoch over!
epoch time: 15.264

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 823
rank avg (pred): 0.118 +- 0.148
mrr vals (pred, true): 0.392, 0.427
batch losses (mrrl, rdl): 0.012252898, 5.19954e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 465
rank avg (pred): 0.438 +- 0.101
mrr vals (pred, true): 0.048, 0.057
batch losses (mrrl, rdl): 5.94539e-05, 7.62579e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1124
rank avg (pred): 0.442 +- 0.094
mrr vals (pred, true): 0.046, 0.054
batch losses (mrrl, rdl): 0.0001600367, 7.22607e-05

Epoch over!
epoch time: 15.073

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.441 +- 0.095
mrr vals (pred, true): 0.047, 0.051

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   26 	     0 	 0.04864 	 0.04538 	 ~...
   35 	     1 	 0.04894 	 0.04547 	 ~...
    7 	     2 	 0.04547 	 0.04690 	 ~...
   56 	     3 	 0.05065 	 0.04772 	 ~...
   58 	     4 	 0.05089 	 0.04830 	 ~...
   48 	     5 	 0.04987 	 0.04839 	 ~...
   20 	     6 	 0.04826 	 0.04851 	 ~...
   55 	     7 	 0.05063 	 0.04856 	 ~...
    4 	     8 	 0.04525 	 0.04889 	 ~...
   67 	     9 	 0.05155 	 0.04897 	 ~...
   28 	    10 	 0.04870 	 0.04922 	 ~...
   30 	    11 	 0.04876 	 0.04927 	 ~...
   24 	    12 	 0.04849 	 0.04941 	 ~...
   73 	    13 	 0.05355 	 0.04961 	 ~...
    0 	    14 	 0.04368 	 0.05016 	 ~...
   50 	    15 	 0.05012 	 0.05051 	 ~...
   45 	    16 	 0.04973 	 0.05058 	 ~...
   32 	    17 	 0.04884 	 0.05071 	 ~...
   59 	    18 	 0.05101 	 0.05073 	 ~...
   63 	    19 	 0.05117 	 0.05080 	 ~...
   76 	    20 	 0.05439 	 0.05086 	 ~...
   11 	    21 	 0.04661 	 0.05114 	 ~...
   51 	    22 	 0.05012 	 0.05129 	 ~...
   68 	    23 	 0.05173 	 0.05140 	 ~...
   60 	    24 	 0.05112 	 0.05164 	 ~...
   47 	    25 	 0.04979 	 0.05177 	 ~...
   14 	    26 	 0.04760 	 0.05212 	 ~...
   15 	    27 	 0.04787 	 0.05221 	 ~...
   39 	    28 	 0.04948 	 0.05239 	 ~...
   44 	    29 	 0.04972 	 0.05271 	 ~...
   69 	    30 	 0.05206 	 0.05279 	 ~...
   52 	    31 	 0.05029 	 0.05282 	 ~...
   33 	    32 	 0.04884 	 0.05290 	 ~...
   61 	    33 	 0.05113 	 0.05294 	 ~...
   38 	    34 	 0.04925 	 0.05302 	 ~...
   10 	    35 	 0.04618 	 0.05303 	 ~...
   53 	    36 	 0.05051 	 0.05314 	 ~...
   70 	    37 	 0.05274 	 0.05317 	 ~...
   77 	    38 	 0.05519 	 0.05325 	 ~...
   72 	    39 	 0.05344 	 0.05340 	 ~...
   46 	    40 	 0.04974 	 0.05348 	 ~...
   19 	    41 	 0.04820 	 0.05349 	 ~...
   22 	    42 	 0.04836 	 0.05357 	 ~...
    5 	    43 	 0.04539 	 0.05375 	 ~...
   62 	    44 	 0.05114 	 0.05387 	 ~...
    9 	    45 	 0.04551 	 0.05392 	 ~...
   34 	    46 	 0.04889 	 0.05401 	 ~...
   66 	    47 	 0.05153 	 0.05406 	 ~...
   16 	    48 	 0.04806 	 0.05429 	 ~...
   54 	    49 	 0.05059 	 0.05438 	 ~...
   75 	    50 	 0.05400 	 0.05444 	 ~...
   36 	    51 	 0.04899 	 0.05451 	 ~...
   27 	    52 	 0.04870 	 0.05463 	 ~...
   17 	    53 	 0.04812 	 0.05469 	 ~...
   13 	    54 	 0.04732 	 0.05481 	 ~...
   49 	    55 	 0.05001 	 0.05501 	 ~...
   40 	    56 	 0.04956 	 0.05510 	 ~...
   37 	    57 	 0.04916 	 0.05541 	 ~...
   74 	    58 	 0.05365 	 0.05545 	 ~...
   43 	    59 	 0.04971 	 0.05556 	 ~...
    3 	    60 	 0.04479 	 0.05557 	 ~...
   21 	    61 	 0.04829 	 0.05562 	 ~...
    6 	    62 	 0.04543 	 0.05614 	 ~...
   29 	    63 	 0.04873 	 0.05647 	 ~...
   12 	    64 	 0.04677 	 0.05654 	 ~...
   57 	    65 	 0.05085 	 0.05668 	 ~...
   71 	    66 	 0.05335 	 0.05688 	 ~...
   42 	    67 	 0.04959 	 0.05691 	 ~...
    8 	    68 	 0.04547 	 0.05699 	 ~...
   18 	    69 	 0.04815 	 0.05710 	 ~...
   25 	    70 	 0.04861 	 0.05773 	 ~...
   41 	    71 	 0.04958 	 0.05791 	 ~...
   65 	    72 	 0.05137 	 0.05793 	 ~...
    1 	    73 	 0.04393 	 0.05796 	 ~...
   64 	    74 	 0.05122 	 0.05846 	 ~...
   23 	    75 	 0.04845 	 0.05882 	 ~...
    2 	    76 	 0.04420 	 0.05918 	 ~...
   31 	    77 	 0.04876 	 0.06092 	 ~...
   81 	    78 	 0.21502 	 0.08417 	 MISS
   78 	    79 	 0.20914 	 0.16390 	 m..s
   82 	    80 	 0.21786 	 0.18244 	 m..s
   83 	    81 	 0.22832 	 0.20831 	 ~...
   80 	    82 	 0.21424 	 0.21521 	 ~...
   85 	    83 	 0.23815 	 0.22330 	 ~...
   86 	    84 	 0.24267 	 0.22564 	 ~...
   84 	    85 	 0.23411 	 0.22580 	 ~...
   79 	    86 	 0.21410 	 0.22666 	 ~...
   87 	    87 	 0.25828 	 0.24233 	 ~...
   93 	    88 	 0.29065 	 0.25409 	 m..s
   88 	    89 	 0.26704 	 0.26725 	 ~...
   89 	    90 	 0.28304 	 0.26840 	 ~...
   95 	    91 	 0.29918 	 0.27924 	 ~...
   92 	    92 	 0.28989 	 0.28022 	 ~...
   98 	    93 	 0.30726 	 0.28370 	 ~...
   91 	    94 	 0.28981 	 0.28999 	 ~...
  100 	    95 	 0.30938 	 0.30095 	 ~...
  107 	    96 	 0.32974 	 0.30574 	 ~...
  106 	    97 	 0.32950 	 0.30765 	 ~...
  105 	    98 	 0.32912 	 0.30880 	 ~...
   90 	    99 	 0.28358 	 0.31009 	 ~...
   99 	   100 	 0.30894 	 0.31542 	 ~...
  102 	   101 	 0.31187 	 0.31804 	 ~...
  104 	   102 	 0.32206 	 0.31936 	 ~...
   94 	   103 	 0.29694 	 0.32045 	 ~...
  108 	   104 	 0.33002 	 0.32289 	 ~...
  103 	   105 	 0.32170 	 0.32467 	 ~...
  101 	   106 	 0.31178 	 0.32737 	 ~...
  111 	   107 	 0.34653 	 0.33063 	 ~...
   96 	   108 	 0.30017 	 0.33409 	 m..s
  109 	   109 	 0.34527 	 0.33691 	 ~...
  112 	   110 	 0.39444 	 0.35741 	 m..s
  110 	   111 	 0.34628 	 0.36022 	 ~...
   97 	   112 	 0.30072 	 0.36586 	 m..s
  114 	   113 	 0.41190 	 0.41409 	 ~...
  113 	   114 	 0.40623 	 0.41730 	 ~...
  115 	   115 	 0.43818 	 0.43012 	 ~...
  117 	   116 	 0.48812 	 0.48829 	 ~...
  116 	   117 	 0.48564 	 0.49383 	 ~...
  119 	   118 	 0.58402 	 0.60870 	 ~...
  118 	   119 	 0.58347 	 0.61829 	 m..s
  120 	   120 	 0.59659 	 0.62874 	 m..s
==========================================
r_mrr = 0.9920831322669983
r2_mrr = 0.9841453433036804
spearmanr_mrr@5 = 0.9989042282104492
spearmanr_mrr@10 = 0.9912837743759155
spearmanr_mrr@50 = 0.9879958033561707
spearmanr_mrr@100 = 0.9942479729652405
spearmanr_mrr@All = 0.9948071837425232
==========================================
test time: 0.456
Done Testing dataset Kinships
total time taken: 234.8897635936737
training time taken: 227.11288046836853
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9921)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9841)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9989)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9913)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9880)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9942)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9948)}}, 'test_loss': {'ComplEx': {'Kinships': 0.5069463907420868}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 7268825403311360
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [114, 464, 951, 167, 838, 264, 605, 883, 385, 362, 480, 257, 671, 399, 949, 635, 495, 267, 327, 434, 469, 232, 406, 796, 127, 903, 290, 386, 839, 205, 510, 1130, 522, 889, 943, 524, 247, 1104, 665, 870, 484, 725, 1165, 1131, 855, 530, 1142, 479, 997, 259, 142, 731, 1013, 965, 1173, 905, 1149, 502, 754, 1029, 825, 514, 305, 314, 994, 494, 1084, 897, 1139, 189, 75, 345, 584, 597, 970, 578, 273, 186, 1175, 105, 64, 1113, 860, 794, 518, 251, 645, 1001, 1186, 37, 433, 297, 979, 526, 1023, 1026, 1191, 66, 938, 262, 203, 892, 225, 1159, 335, 431, 1109, 587, 696, 644, 676, 161, 873, 1058, 109, 963, 588, 972, 1018, 298, 150]
valid_ids (0): []
train_ids (1094): [737, 1086, 712, 791, 470, 988, 374, 246, 1033, 413, 230, 284, 1208, 924, 913, 1079, 261, 1027, 556, 1020, 710, 1, 72, 48, 172, 577, 1138, 319, 581, 663, 133, 819, 307, 593, 455, 65, 682, 772, 210, 702, 801, 175, 25, 738, 888, 183, 170, 740, 332, 199, 858, 223, 638, 121, 705, 637, 35, 793, 811, 953, 786, 300, 289, 238, 893, 500, 1148, 920, 1016, 196, 700, 1071, 1164, 561, 436, 869, 643, 85, 1178, 709, 1017, 201, 437, 438, 106, 371, 1060, 912, 499, 70, 1065, 271, 124, 782, 1147, 533, 751, 56, 392, 887, 1050, 824, 166, 23, 727, 329, 389, 277, 6, 143, 572, 458, 564, 901, 1196, 1195, 285, 160, 188, 103, 67, 847, 917, 973, 342, 693, 1106, 129, 102, 1068, 1036, 1161, 357, 922, 0, 1085, 1019, 942, 844, 616, 685, 1102, 1205, 1028, 258, 803, 907, 789, 218, 746, 641, 989, 932, 1077, 153, 14, 854, 51, 785, 68, 842, 1140, 711, 465, 366, 435, 704, 720, 107, 417, 862, 809, 1108, 229, 573, 235, 1123, 1194, 1204, 352, 1074, 178, 214, 759, 835, 843, 180, 475, 274, 872, 453, 304, 978, 675, 1133, 110, 960, 269, 139, 539, 615, 781, 574, 805, 954, 804, 1037, 5, 36, 303, 381, 10, 909, 850, 351, 969, 29, 977, 1180, 940, 910, 1115, 89, 242, 836, 1078, 764, 719, 516, 292, 163, 1124, 1193, 123, 625, 830, 47, 513, 456, 1152, 380, 477, 856, 63, 322, 713, 1129, 1031, 220, 1064, 84, 370, 78, 55, 492, 760, 1080, 817, 598, 1177, 871, 1083, 1141, 12, 604, 411, 197, 840, 239, 278, 559, 545, 906, 1155, 523, 363, 1057, 250, 895, 648, 620, 831, 689, 1111, 1046, 46, 981, 387, 686, 761, 294, 155, 253, 118, 491, 1095, 1054, 899, 1039, 591, 945, 213, 86, 621, 485, 1048, 337, 962, 111, 557, 1214, 624, 313, 349, 445, 373, 651, 444, 4, 652, 777, 1171, 224, 308, 779, 99, 173, 410, 324, 569, 885, 424, 664, 276, 1120, 310, 1009, 355, 339, 657, 776, 54, 33, 995, 1000, 179, 254, 404, 890, 642, 1200, 863, 1066, 1192, 87, 402, 22, 1047, 1117, 369, 1024, 159, 553, 1088, 565, 286, 265, 359, 320, 612, 1097, 1090, 1010, 136, 575, 1127, 993, 724, 618, 818, 900, 291, 815, 134, 1059, 157, 698, 875, 971, 1153, 611, 3, 209, 991, 266, 535, 1005, 946, 808, 623, 1135, 681, 821, 916, 763, 1211, 1014, 877, 252, 45, 190, 1157, 34, 976, 586, 959, 1126, 145, 420, 62, 957, 1012, 443, 282, 472, 18, 1212, 745, 138, 741, 233, 521, 1032, 119, 736, 39, 659, 662, 739, 442, 1021, 93, 268, 1082, 212, 354, 542, 200, 439, 204, 228, 1052, 547, 688, 774, 857, 1184, 914, 462, 589, 16, 76, 1063, 832, 829, 1183, 636, 617, 83, 1101, 493, 390, 944, 592, 409, 629, 116, 128, 400, 783, 104, 987, 837, 21, 936, 653, 275, 600, 240, 628, 1201, 998, 607, 904, 185, 8, 926, 98, 807, 800, 488, 806, 376, 50, 596, 350, 1107, 911, 501, 1150, 930, 112, 551, 896, 100, 487, 429, 918, 227, 706, 130, 790, 19, 137, 146, 958, 115, 394, 94, 1174, 343, 852, 861, 208, 947, 1210, 234, 1007, 517, 1045, 53, 921, 428, 184, 672, 463, 418, 1125, 716, 9, 1143, 1176, 554, 568, 154, 723, 602, 1003, 1197, 865, 966, 646, 309, 288, 538, 627, 483, 610, 679, 216, 44, 1006, 982, 79, 407, 853, 1053, 243, 822, 415, 849, 563, 703, 721, 38, 732, 931, 1116, 140, 459, 941, 511, 1185, 1051, 631, 955, 583, 828, 735, 177, 622, 1092, 90, 207, 687, 1022, 722, 669, 714, 202, 255, 498, 1002, 915, 684, 579, 1011, 718, 908, 1154, 697, 670, 331, 325, 729, 169, 403, 353, 975, 1114, 473, 347, 509, 919, 476, 287, 874, 1035, 1076, 1030, 466, 1073, 762, 328, 384, 747, 851, 1158, 396, 1069, 1167, 999, 540, 1110, 823, 742, 626, 950, 367, 211, 414, 395, 1203, 421, 750, 765, 408, 531, 1156, 678, 787, 194, 30, 1137, 28, 97, 80, 534, 474, 377, 323, 548, 401, 74, 1041, 985, 486, 176, 650, 1100, 58, 92, 302, 933, 77, 1202, 40, 990, 489, 576, 496, 528, 24, 881, 749, 1199, 13, 582, 222, 773, 20, 15, 59, 263, 1144, 471, 879, 26, 144, 792, 1172, 1038, 1213, 364, 769, 683, 81, 968, 894, 859, 171, 427, 599, 507, 694, 231, 505, 1081, 346, 449, 802, 733, 440, 1207, 541, 752, 690, 344, 1187, 147, 101, 864, 668, 1061, 379, 468, 447, 326, 43, 1163, 91, 1189, 1128, 580, 237, 236, 775, 1098, 187, 1004, 378, 546, 1166, 317, 726, 937, 281, 544, 249, 245, 543, 397, 1087, 788, 673, 333, 241, 165, 193, 226, 368, 658, 120, 519, 1103, 318, 336, 192, 69, 221, 886, 656, 1008, 148, 164, 82, 748, 570, 640, 1099, 1043, 1062, 358, 1160, 206, 529, 95, 983, 734, 717, 608, 467, 934, 964, 191, 550, 778, 301, 609, 71, 884, 996, 168, 730, 757, 1067, 1198, 244, 256, 590, 61, 633, 1188, 867, 753, 1181, 283, 1040, 131, 135, 1056, 634, 866, 149, 992, 1162, 215, 299, 898, 7, 356, 1070, 695, 383, 96, 1132, 1146, 360, 306, 986, 929, 549, 701, 948, 571, 1136, 174, 503, 667, 552, 743, 1145, 1091, 31, 260, 446, 482, 388, 461, 1034, 41, 432, 27, 768, 60, 11, 606, 594, 295, 639, 460, 771, 152, 567, 699, 820, 1134, 655, 984, 848, 393, 311, 537, 452, 1096, 692, 923, 334, 691, 654, 448, 430, 677, 755, 756, 158, 845, 361, 382, 122, 661, 279, 614, 766, 770, 419, 902, 1209, 666, 195, 555, 296, 649, 497, 315, 1015, 57, 562, 88, 1072, 1093, 797, 1025, 481, 1118, 647, 833, 660, 372, 812, 113, 451, 536, 391, 151, 312, 141, 270, 1170, 891, 426, 454, 532, 952, 450, 1055, 365, 595, 2, 1119, 619, 156, 961, 841, 1094, 1049, 826, 585, 398, 272, 416, 882, 1112, 880, 967, 744, 527, 182, 707, 330, 217, 1105, 348, 425, 928, 125, 767, 506, 1151, 1075, 939, 248, 504, 799, 1121, 520, 32, 632, 52, 827, 1179, 1089, 876, 117, 49, 758, 1182, 674, 316, 108, 708, 956, 925, 375, 798, 478, 17, 1044, 566, 601, 980, 423, 321, 1168, 508, 613, 1042, 219, 715, 834, 868, 422, 603, 878, 515, 457, 814, 1206, 680, 795, 405, 132, 340, 1190, 441, 560, 280, 1169, 558, 73, 490, 412, 338, 816, 846, 341, 1122, 126, 512, 198, 813, 42, 630, 525, 728, 181, 162, 784, 780, 935, 293, 974, 810, 927]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1869797868787988
the save name prefix for this run is:  chkpt-ID_1869797868787988_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 312
rank avg (pred): 0.511 +- 0.006
mrr vals (pred, true): 0.019, 0.272
batch losses (mrrl, rdl): 0.0, 0.0028399811

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 465
rank avg (pred): 0.460 +- 0.255
mrr vals (pred, true): 0.106, 0.057
batch losses (mrrl, rdl): 0.0, 1.02773e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 44
rank avg (pred): 0.118 +- 0.090
mrr vals (pred, true): 0.264, 0.306
batch losses (mrrl, rdl): 0.0, 4.1039e-06

Epoch over!
epoch time: 14.866

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 768
rank avg (pred): 0.439 +- 0.268
mrr vals (pred, true): 0.125, 0.051
batch losses (mrrl, rdl): 0.0, 4.8086e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 178
rank avg (pred): 0.449 +- 0.262
mrr vals (pred, true): 0.092, 0.052
batch losses (mrrl, rdl): 0.0, 3.3359e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 10
rank avg (pred): 0.097 +- 0.079
mrr vals (pred, true): 0.272, 0.331
batch losses (mrrl, rdl): 0.0, 4.3128e-06

Epoch over!
epoch time: 14.836

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 748
rank avg (pred): 0.101 +- 0.082
mrr vals (pred, true): 0.261, 0.412
batch losses (mrrl, rdl): 0.0, 1.00912e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 48
rank avg (pred): 0.123 +- 0.100
mrr vals (pred, true): 0.234, 0.258
batch losses (mrrl, rdl): 0.0, 7.7007e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 564
rank avg (pred): 0.175 +- 0.142
mrr vals (pred, true): 0.172, 0.205
batch losses (mrrl, rdl): 0.0, 6.5997e-06

Epoch over!
epoch time: 14.829

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1012
rank avg (pred): 0.452 +- 0.269
mrr vals (pred, true): 0.063, 0.055
batch losses (mrrl, rdl): 0.0, 1.1804e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1212
rank avg (pred): 0.449 +- 0.261
mrr vals (pred, true): 0.054, 0.051
batch losses (mrrl, rdl): 0.0, 1.50318e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 859
rank avg (pred): 0.447 +- 0.265
mrr vals (pred, true): 0.069, 0.055
batch losses (mrrl, rdl): 0.0, 2.5836e-06

Epoch over!
epoch time: 14.822

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 208
rank avg (pred): 0.463 +- 0.255
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 0.0, 2.873e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 737
rank avg (pred): 0.131 +- 0.131
mrr vals (pred, true): 0.272, 0.146
batch losses (mrrl, rdl): 0.0, 8.25472e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 937
rank avg (pred): 0.461 +- 0.267
mrr vals (pred, true): 0.058, 0.057
batch losses (mrrl, rdl): 0.0, 1.8651e-06

Epoch over!
epoch time: 14.814

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 62
rank avg (pred): 0.114 +- 0.122
mrr vals (pred, true): 0.326, 0.333
batch losses (mrrl, rdl): 0.0004264773, 9.8787e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 482
rank avg (pred): 0.409 +- 0.136
mrr vals (pred, true): 0.039, 0.053
batch losses (mrrl, rdl): 0.0012879397, 0.0001338221

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 611
rank avg (pred): 0.389 +- 0.153
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 1.01087e-05, 0.0001344159

Epoch over!
epoch time: 14.966

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 299
rank avg (pred): 0.127 +- 0.107
mrr vals (pred, true): 0.288, 0.357
batch losses (mrrl, rdl): 0.0473835021, 2.53464e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 496
rank avg (pred): 0.282 +- 0.216
mrr vals (pred, true): 0.215, 0.219
batch losses (mrrl, rdl): 0.0001194731, 0.0004496146

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 499
rank avg (pred): 0.278 +- 0.208
mrr vals (pred, true): 0.207, 0.226
batch losses (mrrl, rdl): 0.0037015951, 0.0004910493

Epoch over!
epoch time: 14.941

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 672
rank avg (pred): 0.402 +- 0.153
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 3.1518e-06, 0.0001633781

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 909
rank avg (pred): 0.098 +- 0.075
mrr vals (pred, true): 0.292, 0.251
batch losses (mrrl, rdl): 0.0167232677, 3.0523e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 925
rank avg (pred): 0.407 +- 0.148
mrr vals (pred, true): 0.049, 0.058
batch losses (mrrl, rdl): 1.90471e-05, 0.0001045796

Epoch over!
epoch time: 14.935

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1067
rank avg (pred): 0.047 +- 0.041
mrr vals (pred, true): 0.417, 0.405
batch losses (mrrl, rdl): 0.0013584743, 2.73399e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 498
rank avg (pred): 0.272 +- 0.204
mrr vals (pred, true): 0.227, 0.208
batch losses (mrrl, rdl): 0.0035609701, 0.0003333224

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 907
rank avg (pred): 0.327 +- 0.222
mrr vals (pred, true): 0.168, 0.169
batch losses (mrrl, rdl): 3.2933e-06, 0.0002564214

Epoch over!
epoch time: 14.924

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 477
rank avg (pred): 0.410 +- 0.151
mrr vals (pred, true): 0.052, 0.051
batch losses (mrrl, rdl): 3.02053e-05, 7.87693e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 746
rank avg (pred): 0.117 +- 0.094
mrr vals (pred, true): 0.315, 0.331
batch losses (mrrl, rdl): 0.0025917608, 8.07e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 528
rank avg (pred): 0.292 +- 0.215
mrr vals (pred, true): 0.227, 0.201
batch losses (mrrl, rdl): 0.0064892983, 0.0003463317

Epoch over!
epoch time: 15.023

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1072
rank avg (pred): 0.020 +- 0.016
mrr vals (pred, true): 0.493, 0.358
batch losses (mrrl, rdl): 0.1824805886, 8.7128e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 187
rank avg (pred): 0.438 +- 0.135
mrr vals (pred, true): 0.042, 0.053
batch losses (mrrl, rdl): 0.0006308658, 4.87141e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1114
rank avg (pred): 0.416 +- 0.154
mrr vals (pred, true): 0.057, 0.058
batch losses (mrrl, rdl): 0.0005352441, 7.86255e-05

Epoch over!
epoch time: 15.033

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 818
rank avg (pred): 0.229 +- 0.183
mrr vals (pred, true): 0.275, 0.151
batch losses (mrrl, rdl): 0.1543680131, 5.00061e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1093
rank avg (pred): 0.438 +- 0.136
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002353754, 4.7828e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1008
rank avg (pred): 0.427 +- 0.142
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 1.5159e-06, 5.06885e-05

Epoch over!
epoch time: 15.029

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 501
rank avg (pred): 0.328 +- 0.235
mrr vals (pred, true): 0.232, 0.224
batch losses (mrrl, rdl): 0.0006593005, 0.0008204287

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 341
rank avg (pred): 0.426 +- 0.142
mrr vals (pred, true): 0.055, 0.053
batch losses (mrrl, rdl): 0.0002249639, 6.90691e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 821
rank avg (pred): 0.023 +- 0.018
mrr vals (pred, true): 0.484, 0.430
batch losses (mrrl, rdl): 0.0289110448, 6.49037e-05

Epoch over!
epoch time: 15.022

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1181
rank avg (pred): 0.446 +- 0.115
mrr vals (pred, true): 0.039, 0.049
batch losses (mrrl, rdl): 0.0011583866, 5.99861e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 511
rank avg (pred): 0.366 +- 0.231
mrr vals (pred, true): 0.194, 0.217
batch losses (mrrl, rdl): 0.0055124522, 0.0010261583

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 789
rank avg (pred): 0.441 +- 0.121
mrr vals (pred, true): 0.045, 0.058
batch losses (mrrl, rdl): 0.0002217792, 6.51695e-05

Epoch over!
epoch time: 15.015

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 879
rank avg (pred): 0.432 +- 0.141
mrr vals (pred, true): 0.054, 0.051
batch losses (mrrl, rdl): 0.0001307385, 6.27647e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 703
rank avg (pred): 0.437 +- 0.122
mrr vals (pred, true): 0.045, 0.056
batch losses (mrrl, rdl): 0.0002743267, 7.1575e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 471
rank avg (pred): 0.432 +- 0.125
mrr vals (pred, true): 0.046, 0.054
batch losses (mrrl, rdl): 0.0001300143, 5.89266e-05

Epoch over!
epoch time: 15.011

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.426 +- 0.131
mrr vals (pred, true): 0.050, 0.056

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   33 	     0 	 0.04759 	 0.04495 	 ~...
    4 	     1 	 0.04326 	 0.04690 	 ~...
   70 	     2 	 0.05070 	 0.04714 	 ~...
   25 	     3 	 0.04697 	 0.04727 	 ~...
   47 	     4 	 0.04853 	 0.04731 	 ~...
   20 	     5 	 0.04675 	 0.04763 	 ~...
   35 	     6 	 0.04769 	 0.04773 	 ~...
   26 	     7 	 0.04720 	 0.04778 	 ~...
   58 	     8 	 0.04939 	 0.04852 	 ~...
   42 	     9 	 0.04796 	 0.04856 	 ~...
   61 	    10 	 0.04964 	 0.04863 	 ~...
   77 	    11 	 0.05503 	 0.04905 	 ~...
    9 	    12 	 0.04524 	 0.04925 	 ~...
   56 	    13 	 0.04917 	 0.04941 	 ~...
   17 	    14 	 0.04653 	 0.04950 	 ~...
   21 	    15 	 0.04676 	 0.04994 	 ~...
   18 	    16 	 0.04657 	 0.05014 	 ~...
   38 	    17 	 0.04788 	 0.05059 	 ~...
   67 	    18 	 0.05017 	 0.05062 	 ~...
   36 	    19 	 0.04774 	 0.05067 	 ~...
   23 	    20 	 0.04688 	 0.05068 	 ~...
   54 	    21 	 0.04892 	 0.05074 	 ~...
   43 	    22 	 0.04813 	 0.05086 	 ~...
   71 	    23 	 0.05113 	 0.05105 	 ~...
    6 	    24 	 0.04413 	 0.05107 	 ~...
   45 	    25 	 0.04818 	 0.05113 	 ~...
   44 	    26 	 0.04815 	 0.05142 	 ~...
   15 	    27 	 0.04626 	 0.05155 	 ~...
   12 	    28 	 0.04620 	 0.05157 	 ~...
   14 	    29 	 0.04624 	 0.05164 	 ~...
   29 	    30 	 0.04735 	 0.05185 	 ~...
   19 	    31 	 0.04667 	 0.05207 	 ~...
   74 	    32 	 0.05180 	 0.05212 	 ~...
   24 	    33 	 0.04691 	 0.05241 	 ~...
   10 	    34 	 0.04591 	 0.05250 	 ~...
   73 	    35 	 0.05167 	 0.05262 	 ~...
   69 	    36 	 0.05036 	 0.05274 	 ~...
   72 	    37 	 0.05139 	 0.05286 	 ~...
   46 	    38 	 0.04826 	 0.05311 	 ~...
   51 	    39 	 0.04860 	 0.05335 	 ~...
   57 	    40 	 0.04927 	 0.05354 	 ~...
    8 	    41 	 0.04520 	 0.05391 	 ~...
   55 	    42 	 0.04914 	 0.05425 	 ~...
   41 	    43 	 0.04795 	 0.05432 	 ~...
    1 	    44 	 0.04238 	 0.05434 	 ~...
    0 	    45 	 0.04166 	 0.05435 	 ~...
   59 	    46 	 0.04946 	 0.05449 	 ~...
   50 	    47 	 0.04860 	 0.05457 	 ~...
   52 	    48 	 0.04861 	 0.05459 	 ~...
   76 	    49 	 0.05316 	 0.05470 	 ~...
   64 	    50 	 0.04976 	 0.05500 	 ~...
   27 	    51 	 0.04731 	 0.05503 	 ~...
   11 	    52 	 0.04598 	 0.05510 	 ~...
   22 	    53 	 0.04678 	 0.05510 	 ~...
   40 	    54 	 0.04794 	 0.05526 	 ~...
    7 	    55 	 0.04438 	 0.05531 	 ~...
   34 	    56 	 0.04761 	 0.05531 	 ~...
   63 	    57 	 0.04970 	 0.05545 	 ~...
   28 	    58 	 0.04731 	 0.05569 	 ~...
   16 	    59 	 0.04652 	 0.05578 	 ~...
   68 	    60 	 0.05022 	 0.05593 	 ~...
   66 	    61 	 0.04999 	 0.05601 	 ~...
   65 	    62 	 0.04995 	 0.05627 	 ~...
   31 	    63 	 0.04744 	 0.05628 	 ~...
    5 	    64 	 0.04359 	 0.05673 	 ~...
    2 	    65 	 0.04303 	 0.05688 	 ~...
   49 	    66 	 0.04857 	 0.05691 	 ~...
   39 	    67 	 0.04789 	 0.05693 	 ~...
   37 	    68 	 0.04777 	 0.05725 	 ~...
   60 	    69 	 0.04949 	 0.05740 	 ~...
   75 	    70 	 0.05204 	 0.05769 	 ~...
   13 	    71 	 0.04622 	 0.05782 	 ~...
   53 	    72 	 0.04886 	 0.05825 	 ~...
   48 	    73 	 0.04855 	 0.05900 	 ~...
   32 	    74 	 0.04745 	 0.05901 	 ~...
    3 	    75 	 0.04320 	 0.05902 	 ~...
   30 	    76 	 0.04739 	 0.05973 	 ~...
   62 	    77 	 0.04966 	 0.05990 	 ~...
   79 	    78 	 0.19159 	 0.08944 	 MISS
   80 	    79 	 0.21517 	 0.18970 	 ~...
   82 	    80 	 0.22283 	 0.20444 	 ~...
   78 	    81 	 0.18526 	 0.20455 	 ~...
   84 	    82 	 0.22463 	 0.21833 	 ~...
   81 	    83 	 0.21846 	 0.22061 	 ~...
   88 	    84 	 0.24577 	 0.22226 	 ~...
   86 	    85 	 0.22694 	 0.22330 	 ~...
   83 	    86 	 0.22451 	 0.22622 	 ~...
   89 	    87 	 0.24953 	 0.22825 	 ~...
   85 	    88 	 0.22680 	 0.23036 	 ~...
   87 	    89 	 0.23438 	 0.23853 	 ~...
   91 	    90 	 0.27464 	 0.26316 	 ~...
   90 	    91 	 0.25398 	 0.27033 	 ~...
  103 	    92 	 0.31583 	 0.28244 	 m..s
  108 	    93 	 0.32632 	 0.28764 	 m..s
   92 	    94 	 0.28931 	 0.28774 	 ~...
  104 	    95 	 0.31612 	 0.28812 	 ~...
   99 	    96 	 0.29885 	 0.29097 	 ~...
  102 	    97 	 0.30932 	 0.29126 	 ~...
  101 	    98 	 0.30760 	 0.29398 	 ~...
  109 	    99 	 0.32957 	 0.29748 	 m..s
   95 	   100 	 0.29457 	 0.29813 	 ~...
   93 	   101 	 0.28955 	 0.30230 	 ~...
   97 	   102 	 0.29505 	 0.30449 	 ~...
   98 	   103 	 0.29631 	 0.30510 	 ~...
  110 	   104 	 0.33343 	 0.30571 	 ~...
   94 	   105 	 0.29364 	 0.30997 	 ~...
  100 	   106 	 0.29987 	 0.31691 	 ~...
  107 	   107 	 0.32095 	 0.32749 	 ~...
  105 	   108 	 0.31762 	 0.33148 	 ~...
  111 	   109 	 0.33967 	 0.33351 	 ~...
  113 	   110 	 0.37143 	 0.34731 	 ~...
  106 	   111 	 0.32060 	 0.34880 	 ~...
   96 	   112 	 0.29488 	 0.36586 	 m..s
  112 	   113 	 0.34764 	 0.36880 	 ~...
  117 	   114 	 0.47132 	 0.39397 	 m..s
  114 	   115 	 0.42701 	 0.44883 	 ~...
  115 	   116 	 0.45860 	 0.45940 	 ~...
  116 	   117 	 0.46657 	 0.48006 	 ~...
  118 	   118 	 0.54820 	 0.51634 	 m..s
  119 	   119 	 0.58355 	 0.52550 	 m..s
  120 	   120 	 0.71138 	 0.61786 	 m..s
==========================================
r_mrr = 0.9911900162696838
r2_mrr = 0.9776886105537415
spearmanr_mrr@5 = 0.98953777551651
spearmanr_mrr@10 = 0.9847816824913025
spearmanr_mrr@50 = 0.9886142015457153
spearmanr_mrr@100 = 0.9954404234886169
spearmanr_mrr@All = 0.9958769679069519
==========================================
test time: 0.454
Done Testing dataset Kinships
total time taken: 232.23569893836975
training time taken: 224.529887676239
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9912)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9777)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9895)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9848)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9886)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9954)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9959)}}, 'test_loss': {'ComplEx': {'Kinships': 0.5856374744616915}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 9679830691813768
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1177, 1124, 1156, 596, 908, 968, 151, 288, 222, 320, 879, 71, 1097, 420, 1049, 904, 376, 670, 976, 59, 971, 146, 422, 822, 686, 805, 1179, 1046, 203, 902, 560, 574, 1213, 872, 115, 571, 375, 354, 147, 223, 857, 1188, 128, 67, 1070, 1130, 983, 60, 511, 544, 132, 250, 416, 276, 395, 27, 285, 568, 552, 591, 733, 549, 674, 703, 397, 638, 1161, 442, 78, 839, 349, 600, 569, 482, 374, 538, 326, 1146, 1037, 602, 874, 491, 1076, 835, 304, 832, 383, 1170, 1044, 300, 119, 371, 437, 96, 825, 720, 291, 993, 790, 318, 948, 414, 601, 542, 262, 550, 943, 745, 346, 32, 58, 637, 33, 193, 972, 764, 1073, 387, 396, 1022, 1157]
valid_ids (0): []
train_ids (1094): [951, 780, 900, 1103, 470, 333, 775, 294, 364, 880, 583, 767, 995, 366, 485, 1133, 803, 697, 402, 946, 595, 867, 1206, 689, 682, 1144, 1209, 1064, 233, 1152, 124, 1104, 380, 1061, 915, 415, 748, 743, 363, 1095, 357, 315, 413, 779, 118, 740, 1, 208, 650, 236, 24, 807, 737, 516, 1159, 1114, 473, 1172, 194, 990, 759, 950, 1155, 1138, 772, 960, 228, 1154, 590, 350, 1047, 1185, 688, 136, 17, 8, 980, 94, 786, 35, 478, 214, 79, 69, 890, 246, 488, 1027, 113, 384, 480, 706, 14, 244, 529, 710, 1084, 625, 114, 356, 88, 612, 1212, 741, 580, 582, 599, 752, 570, 417, 575, 0, 158, 160, 64, 1096, 75, 152, 234, 837, 373, 669, 907, 263, 271, 289, 514, 2, 418, 82, 506, 979, 455, 210, 99, 1067, 668, 1062, 445, 273, 827, 1151, 309, 257, 308, 46, 252, 566, 1071, 339, 921, 444, 731, 1136, 1189, 1182, 217, 524, 917, 186, 957, 1078, 57, 409, 66, 185, 1207, 451, 859, 896, 428, 45, 817, 765, 1134, 997, 665, 799, 588, 713, 351, 86, 781, 241, 213, 42, 1112, 852, 1186, 751, 1023, 385, 695, 794, 277, 715, 711, 44, 535, 942, 801, 1183, 704, 476, 1034, 259, 965, 808, 533, 792, 29, 988, 548, 267, 148, 1056, 172, 358, 28, 1092, 1148, 256, 1029, 1201, 831, 1098, 607, 547, 1202, 657, 238, 197, 165, 541, 581, 526, 848, 1033, 307, 1140, 265, 1042, 130, 104, 818, 83, 1164, 796, 986, 812, 592, 314, 646, 398, 632, 347, 1025, 609, 279, 709, 199, 435, 875, 945, 520, 92, 937, 360, 1091, 742, 378, 1171, 1055, 487, 40, 935, 73, 1142, 139, 1077, 1063, 405, 1032, 174, 303, 523, 787, 249, 474, 343, 429, 850, 809, 159, 873, 275, 778, 1020, 654, 206, 725, 851, 753, 446, 153, 441, 919, 348, 838, 135, 1173, 994, 826, 131, 1019, 763, 19, 954, 127, 532, 620, 386, 776, 1162, 565, 1191, 192, 112, 498, 530, 1153, 934, 1135, 618, 175, 209, 628, 286, 701, 20, 426, 1060, 608, 394, 678, 811, 564, 149, 297, 785, 157, 95, 166, 593, 388, 1126, 129, 885, 317, 1002, 758, 705, 109, 1143, 189, 1122, 352, 963, 52, 525, 50, 1106, 649, 31, 1163, 369, 522, 1200, 43, 920, 509, 856, 145, 598, 847, 683, 164, 707, 195, 992, 1053, 1093, 782, 1102, 510, 540, 829, 627, 125, 939, 179, 558, 681, 231, 264, 998, 450, 211, 278, 853, 693, 141, 658, 1030, 1058, 16, 866, 224, 1190, 777, 865, 431, 201, 749, 475, 138, 673, 340, 505, 621, 955, 298, 666, 361, 200, 739, 1045, 63, 15, 421, 68, 245, 163, 845, 773, 1088, 101, 1080, 708, 161, 48, 331, 1016, 1205, 98, 292, 918, 723, 393, 133, 824, 268, 1111, 1101, 302, 841, 727, 225, 329, 1011, 977, 334, 864, 221, 495, 282, 586, 814, 311, 270, 1079, 472, 1015, 528, 962, 483, 248, 6, 589, 551, 1039, 432, 54, 181, 1072, 806, 1013, 162, 176, 984, 1194, 77, 577, 1083, 928, 1038, 672, 458, 901, 755, 25, 36, 1204, 519, 468, 287, 447, 37, 750, 168, 377, 1131, 89, 1149, 513, 648, 359, 239, 1057, 337, 906, 1147, 486, 728, 736, 881, 336, 187, 182, 154, 1113, 970, 820, 605, 362, 675, 255, 55, 215, 382, 220, 655, 1166, 137, 714, 501, 1203, 408, 721, 912, 1005, 1006, 389, 462, 1109, 1160, 894, 93, 103, 878, 843, 345, 85, 372, 958, 461, 407, 1074, 11, 1196, 768, 830, 576, 1099, 585, 467, 332, 536, 891, 321, 512, 640, 893, 999, 554, 338, 633, 1043, 521, 121, 770, 367, 481, 527, 626, 110, 242, 207, 889, 793, 1007, 237, 761, 611, 296, 281, 330, 771, 218, 229, 471, 499, 272, 1187, 518, 579, 205, 464, 440, 927, 406, 30, 760, 1169, 293, 854, 1054, 253, 117, 892, 327, 717, 846, 313, 925, 634, 973, 235, 619, 660, 687, 274, 784, 517, 1116, 563, 41, 1125, 84, 684, 1107, 1021, 876, 493, 1208, 546, 269, 539, 614, 1059, 860, 622, 774, 623, 617, 243, 284, 1087, 804, 1090, 1100, 494, 1094, 310, 38, 430, 479, 819, 457, 21, 567, 810, 123, 1210, 1035, 4, 1069, 1178, 987, 72, 232, 1127, 813, 1004, 328, 23, 1017, 863, 1121, 802, 1012, 316, 9, 910, 1028, 1041, 12, 578, 718, 929, 691, 465, 365, 1105, 1132, 1108, 562, 322, 700, 769, 1214, 531, 676, 1003, 463, 490, 65, 1066, 449, 766, 884, 722, 1158, 306, 956, 883, 1031, 216, 643, 1120, 62, 433, 368, 100, 436, 381, 738, 923, 651, 855, 401, 754, 871, 434, 261, 212, 886, 1052, 1082, 677, 299, 438, 120, 897, 1137, 106, 858, 844, 1081, 325, 469, 941, 940, 319, 604, 1010, 91, 423, 989, 1180, 425, 692, 254, 1184, 1141, 730, 47, 1181, 90, 1050, 895, 107, 557, 641, 1008, 219, 823, 543, 51, 815, 150, 724, 190, 969, 134, 559, 170, 140, 1009, 1123, 797, 204, 370, 696, 1195, 290, 1117, 198, 644, 22, 694, 909, 453, 122, 905, 266, 639, 959, 922, 828, 645, 74, 1119, 18, 926, 656, 10, 964, 419, 967, 662, 553, 653, 353, 953, 126, 783, 1168, 484, 76, 791, 1075, 459, 762, 545, 400, 573, 903, 911, 610, 180, 877, 702, 312, 789, 1115, 301, 561, 944, 931, 240, 3, 636, 1192, 226, 492, 869, 230, 53, 335, 156, 833, 39, 975, 642, 251, 111, 454, 556, 1048, 982, 635, 404, 887, 26, 667, 1198, 795, 712, 1089, 424, 659, 816, 898, 1068, 344, 34, 504, 1040, 489, 7, 756, 439, 1175, 978, 836, 191, 477, 981, 116, 616, 924, 403, 143, 729, 1128, 1211, 991, 196, 914, 169, 1085, 584, 664, 679, 932, 448, 70, 515, 800, 930, 324, 183, 280, 821, 295, 81, 671, 690, 699, 1197, 1150, 173, 502, 882, 142, 500, 56, 508, 341, 1145, 757, 1051, 497, 355, 283, 849, 108, 735, 537, 615, 399, 507, 390, 933, 456, 177, 936, 961, 171, 572, 888, 716, 49, 13, 594, 105, 985, 952, 870, 555, 661, 260, 966, 1000, 938, 606, 410, 167, 247, 411, 613, 747, 460, 342, 1167, 746, 323, 842, 974, 1129, 102, 913, 534, 1165, 685, 680, 947, 496, 652, 834, 996, 188, 1001, 1193, 412, 178, 427, 1086, 597, 663, 1014, 734, 624, 452, 392, 788, 798, 1026, 726, 391, 5, 1176, 443, 1110, 629, 97, 899, 647, 80, 227, 202, 840, 305, 862, 630, 1018, 87, 1024, 1065, 184, 144, 258, 155, 868, 732, 1036, 1174, 587, 744, 861, 1199, 1139, 949, 503, 61, 379, 631, 698, 466, 603, 719, 1118, 916]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4791485652718769
the save name prefix for this run is:  chkpt-ID_4791485652718769_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'ComplEx': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 890
rank avg (pred): 0.533 +- 0.005
mrr vals (pred, true): 0.018, 0.047
batch losses (mrrl, rdl): 0.0, 0.000119285

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 229
rank avg (pred): 0.454 +- 0.240
mrr vals (pred, true): 0.135, 0.053
batch losses (mrrl, rdl): 0.0, 8.3849e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 164
rank avg (pred): 0.442 +- 0.269
mrr vals (pred, true): 0.152, 0.053
batch losses (mrrl, rdl): 0.0, 4.4716e-06

Epoch over!
epoch time: 14.784

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 263
rank avg (pred): 0.118 +- 0.085
mrr vals (pred, true): 0.260, 0.345
batch losses (mrrl, rdl): 0.0, 8.8168e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 20
rank avg (pred): 0.101 +- 0.078
mrr vals (pred, true): 0.270, 0.351
batch losses (mrrl, rdl): 0.0, 3.5468e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 924
rank avg (pred): 0.446 +- 0.270
mrr vals (pred, true): 0.091, 0.047
batch losses (mrrl, rdl): 0.0, 4.8925e-06

Epoch over!
epoch time: 14.754

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 950
rank avg (pred): 0.456 +- 0.272
mrr vals (pred, true): 0.085, 0.050
batch losses (mrrl, rdl): 0.0, 2.4589e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 837
rank avg (pred): 0.453 +- 0.277
mrr vals (pred, true): 0.073, 0.054
batch losses (mrrl, rdl): 0.0, 2.2816e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 840
rank avg (pred): 0.453 +- 0.273
mrr vals (pred, true): 0.062, 0.057
batch losses (mrrl, rdl): 0.0, 1.4459e-06

Epoch over!
epoch time: 14.727

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 436
rank avg (pred): 0.453 +- 0.260
mrr vals (pred, true): 0.056, 0.057
batch losses (mrrl, rdl): 0.0, 8.697e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 186
rank avg (pred): 0.460 +- 0.271
mrr vals (pred, true): 0.052, 0.052
batch losses (mrrl, rdl): 0.0, 3.5909e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 160
rank avg (pred): 0.463 +- 0.259
mrr vals (pred, true): 0.044, 0.049
batch losses (mrrl, rdl): 0.0, 2.4324e-06

Epoch over!
epoch time: 14.902

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 401
rank avg (pred): 0.455 +- 0.260
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 0.0, 3.0555e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 231
rank avg (pred): 0.474 +- 0.259
mrr vals (pred, true): 0.041, 0.055
batch losses (mrrl, rdl): 0.0, 3.0537e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 118
rank avg (pred): 0.460 +- 0.258
mrr vals (pred, true): 0.042, 0.053
batch losses (mrrl, rdl): 0.0, 3.509e-07

Epoch over!
epoch time: 14.91

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 251
rank avg (pred): 0.092 +- 0.092
mrr vals (pred, true): 0.274, 0.347
batch losses (mrrl, rdl): 0.0533375479, 1.2137e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 888
rank avg (pred): 0.466 +- 0.160
mrr vals (pred, true): 0.054, 0.056
batch losses (mrrl, rdl): 0.00012385, 3.84443e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 648
rank avg (pred): 0.415 +- 0.180
mrr vals (pred, true): 0.092, 0.049
batch losses (mrrl, rdl): 0.01797566, 6.76245e-05

Epoch over!
epoch time: 15.135

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 373
rank avg (pred): 0.447 +- 0.152
mrr vals (pred, true): 0.055, 0.050
batch losses (mrrl, rdl): 0.000245821, 4.72928e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 212
rank avg (pred): 0.466 +- 0.125
mrr vals (pred, true): 0.038, 0.054
batch losses (mrrl, rdl): 0.0013368012, 4.82542e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 840
rank avg (pred): 0.423 +- 0.137
mrr vals (pred, true): 0.052, 0.057
batch losses (mrrl, rdl): 4.14062e-05, 8.2236e-05

Epoch over!
epoch time: 15.23

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 980
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.617, 0.528
batch losses (mrrl, rdl): 0.0787745267, 5.88565e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 784
rank avg (pred): 0.437 +- 0.150
mrr vals (pred, true): 0.058, 0.052
batch losses (mrrl, rdl): 0.000587266, 5.61275e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 483
rank avg (pred): 0.452 +- 0.145
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 2.0699e-06, 4.20402e-05

Epoch over!
epoch time: 15.232

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 36
rank avg (pred): 0.143 +- 0.100
mrr vals (pred, true): 0.258, 0.274
batch losses (mrrl, rdl): 0.0026844051, 7.1306e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1080
rank avg (pred): 0.479 +- 0.122
mrr vals (pred, true): 0.035, 0.051
batch losses (mrrl, rdl): 0.0022228647, 6.71615e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1178
rank avg (pred): 0.451 +- 0.153
mrr vals (pred, true): 0.058, 0.046
batch losses (mrrl, rdl): 0.0005700702, 4.29034e-05

Epoch over!
epoch time: 15.026

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1113
rank avg (pred): 0.455 +- 0.146
mrr vals (pred, true): 0.053, 0.057
batch losses (mrrl, rdl): 6.69198e-05, 4.63516e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 541
rank avg (pred): 0.301 +- 0.202
mrr vals (pred, true): 0.217, 0.236
batch losses (mrrl, rdl): 0.0035293275, 0.00066473

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 394
rank avg (pred): 0.467 +- 0.135
mrr vals (pred, true): 0.045, 0.053
batch losses (mrrl, rdl): 0.00028215, 5.30201e-05

Epoch over!
epoch time: 15.102

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 257
rank avg (pred): 0.059 +- 0.043
mrr vals (pred, true): 0.370, 0.369
batch losses (mrrl, rdl): 5.6887e-06, 3.0646e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 35
rank avg (pred): 0.122 +- 0.088
mrr vals (pred, true): 0.306, 0.343
batch losses (mrrl, rdl): 0.0134907924, 2.07624e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 391
rank avg (pred): 0.478 +- 0.121
mrr vals (pred, true): 0.041, 0.056
batch losses (mrrl, rdl): 0.0008386965, 6.11395e-05

Epoch over!
epoch time: 15.285

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 235
rank avg (pred): 0.466 +- 0.141
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 1.11779e-05, 4.70469e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 527
rank avg (pred): 0.371 +- 0.257
mrr vals (pred, true): 0.235, 0.220
batch losses (mrrl, rdl): 0.0021872378, 0.0011353067

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 467
rank avg (pred): 0.464 +- 0.132
mrr vals (pred, true): 0.049, 0.056
batch losses (mrrl, rdl): 2.5523e-06, 5.53896e-05

Epoch over!
epoch time: 15.28

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 461
rank avg (pred): 0.472 +- 0.126
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001546363, 5.70918e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 74
rank avg (pred): 0.077 +- 0.053
mrr vals (pred, true): 0.326, 0.300
batch losses (mrrl, rdl): 0.0067629102, 5.70343e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 419
rank avg (pred): 0.455 +- 0.140
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 0.0001184291, 4.60183e-05

Epoch over!
epoch time: 15.283

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 467
rank avg (pred): 0.469 +- 0.121
mrr vals (pred, true): 0.042, 0.056
batch losses (mrrl, rdl): 0.0006157272, 6.23862e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 133
rank avg (pred): 0.460 +- 0.135
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 3.78e-08, 5.25924e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 1212
rank avg (pred): 0.457 +- 0.133
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 2.6072e-06, 5.85794e-05

Epoch over!
epoch time: 15.258

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 705
rank avg (pred): 0.473 +- 0.116
mrr vals (pred, true): 0.043, 0.053
batch losses (mrrl, rdl): 0.0005622782, 6.75832e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 821
rank avg (pred): 0.032 +- 0.024
mrr vals (pred, true): 0.444, 0.430
batch losses (mrrl, rdl): 0.0020396924, 4.61414e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, Kinships, run 2.1, exp 102
rank avg (pred): 0.455 +- 0.134
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 2.51771e-05, 4.53559e-05

Epoch over!
epoch time: 15.146

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.453 +- 0.131
mrr vals (pred, true): 0.050, 0.049

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   33 	     0 	 0.04527 	 0.04690 	 ~...
   36 	     1 	 0.04552 	 0.04753 	 ~...
   67 	     2 	 0.04913 	 0.04769 	 ~...
   65 	     3 	 0.04882 	 0.04831 	 ~...
   38 	     4 	 0.04567 	 0.04838 	 ~...
   31 	     5 	 0.04508 	 0.04840 	 ~...
    3 	     6 	 0.04259 	 0.04876 	 ~...
   61 	     7 	 0.04792 	 0.04882 	 ~...
   52 	     8 	 0.04701 	 0.04893 	 ~...
   23 	     9 	 0.04479 	 0.04896 	 ~...
   70 	    10 	 0.05038 	 0.04915 	 ~...
   25 	    11 	 0.04480 	 0.04944 	 ~...
   60 	    12 	 0.04788 	 0.04985 	 ~...
   69 	    13 	 0.04928 	 0.04998 	 ~...
   26 	    14 	 0.04482 	 0.05050 	 ~...
   14 	    15 	 0.04404 	 0.05059 	 ~...
    1 	    16 	 0.04220 	 0.05080 	 ~...
    4 	    17 	 0.04307 	 0.05099 	 ~...
   48 	    18 	 0.04660 	 0.05102 	 ~...
   39 	    19 	 0.04575 	 0.05110 	 ~...
   13 	    20 	 0.04400 	 0.05121 	 ~...
   40 	    21 	 0.04580 	 0.05121 	 ~...
   27 	    22 	 0.04482 	 0.05129 	 ~...
    8 	    23 	 0.04373 	 0.05146 	 ~...
   59 	    24 	 0.04773 	 0.05146 	 ~...
   47 	    25 	 0.04659 	 0.05149 	 ~...
   44 	    26 	 0.04597 	 0.05154 	 ~...
   63 	    27 	 0.04812 	 0.05157 	 ~...
   42 	    28 	 0.04595 	 0.05168 	 ~...
   58 	    29 	 0.04770 	 0.05228 	 ~...
   12 	    30 	 0.04389 	 0.05239 	 ~...
   62 	    31 	 0.04792 	 0.05243 	 ~...
   73 	    32 	 0.05354 	 0.05248 	 ~...
   55 	    33 	 0.04739 	 0.05248 	 ~...
   50 	    34 	 0.04673 	 0.05263 	 ~...
   64 	    35 	 0.04851 	 0.05274 	 ~...
   68 	    36 	 0.04923 	 0.05277 	 ~...
    6 	    37 	 0.04329 	 0.05282 	 ~...
   72 	    38 	 0.05188 	 0.05286 	 ~...
   49 	    39 	 0.04661 	 0.05289 	 ~...
    7 	    40 	 0.04361 	 0.05302 	 ~...
   17 	    41 	 0.04406 	 0.05303 	 ~...
   51 	    42 	 0.04678 	 0.05311 	 ~...
    0 	    43 	 0.04129 	 0.05320 	 ~...
   43 	    44 	 0.04595 	 0.05356 	 ~...
   71 	    45 	 0.05186 	 0.05375 	 ~...
   10 	    46 	 0.04387 	 0.05375 	 ~...
   57 	    47 	 0.04768 	 0.05381 	 ~...
   37 	    48 	 0.04561 	 0.05396 	 ~...
   32 	    49 	 0.04509 	 0.05455 	 ~...
   19 	    50 	 0.04453 	 0.05496 	 ~...
    9 	    51 	 0.04385 	 0.05501 	 ~...
   29 	    52 	 0.04492 	 0.05520 	 ~...
    5 	    53 	 0.04310 	 0.05555 	 ~...
    2 	    54 	 0.04226 	 0.05565 	 ~...
   35 	    55 	 0.04544 	 0.05576 	 ~...
   16 	    56 	 0.04405 	 0.05609 	 ~...
   41 	    57 	 0.04587 	 0.05614 	 ~...
   11 	    58 	 0.04388 	 0.05614 	 ~...
   22 	    59 	 0.04466 	 0.05647 	 ~...
   30 	    60 	 0.04501 	 0.05651 	 ~...
   53 	    61 	 0.04714 	 0.05696 	 ~...
   15 	    62 	 0.04405 	 0.05697 	 ~...
   21 	    63 	 0.04456 	 0.05703 	 ~...
   28 	    64 	 0.04485 	 0.05742 	 ~...
   45 	    65 	 0.04615 	 0.05777 	 ~...
   66 	    66 	 0.04902 	 0.05817 	 ~...
   54 	    67 	 0.04733 	 0.05817 	 ~...
   18 	    68 	 0.04418 	 0.05881 	 ~...
   56 	    69 	 0.04740 	 0.05902 	 ~...
   20 	    70 	 0.04456 	 0.05930 	 ~...
   24 	    71 	 0.04479 	 0.06008 	 ~...
   34 	    72 	 0.04530 	 0.06014 	 ~...
   46 	    73 	 0.04632 	 0.06360 	 ~...
   74 	    74 	 0.20880 	 0.15981 	 m..s
   77 	    75 	 0.21013 	 0.18556 	 ~...
   76 	    76 	 0.20984 	 0.19396 	 ~...
   78 	    77 	 0.21169 	 0.20018 	 ~...
   80 	    78 	 0.21772 	 0.20266 	 ~...
   79 	    79 	 0.21649 	 0.20491 	 ~...
   74 	    80 	 0.20880 	 0.21714 	 ~...
   98 	    81 	 0.29397 	 0.21968 	 m..s
   81 	    82 	 0.23308 	 0.22043 	 ~...
   82 	    83 	 0.24963 	 0.24847 	 ~...
  107 	    84 	 0.33353 	 0.24886 	 m..s
   96 	    85 	 0.29269 	 0.25657 	 m..s
   92 	    86 	 0.27010 	 0.26350 	 ~...
   85 	    87 	 0.25153 	 0.26738 	 ~...
   84 	    88 	 0.25116 	 0.27057 	 ~...
   86 	    89 	 0.25925 	 0.27425 	 ~...
   88 	    90 	 0.26086 	 0.27645 	 ~...
   94 	    91 	 0.28216 	 0.27737 	 ~...
   89 	    92 	 0.26104 	 0.27912 	 ~...
   83 	    93 	 0.25098 	 0.28091 	 ~...
   95 	    94 	 0.28914 	 0.28897 	 ~...
   97 	    95 	 0.29396 	 0.29778 	 ~...
  104 	    96 	 0.31857 	 0.30284 	 ~...
   99 	    97 	 0.30781 	 0.31037 	 ~...
  103 	    98 	 0.30892 	 0.32045 	 ~...
  102 	    99 	 0.30857 	 0.32139 	 ~...
   91 	   100 	 0.26527 	 0.32386 	 m..s
  100 	   101 	 0.30798 	 0.32474 	 ~...
  106 	   102 	 0.32049 	 0.32831 	 ~...
   90 	   103 	 0.26146 	 0.33111 	 m..s
   93 	   104 	 0.27105 	 0.33148 	 m..s
  105 	   105 	 0.31961 	 0.33295 	 ~...
  101 	   106 	 0.30819 	 0.33409 	 ~...
  109 	   107 	 0.33979 	 0.33520 	 ~...
  108 	   108 	 0.33943 	 0.33811 	 ~...
  110 	   109 	 0.37326 	 0.34338 	 ~...
   87 	   110 	 0.26023 	 0.36586 	 MISS
  112 	   111 	 0.40335 	 0.37350 	 ~...
  113 	   112 	 0.40527 	 0.37821 	 ~...
  118 	   113 	 0.49386 	 0.39397 	 m..s
  115 	   114 	 0.45611 	 0.42648 	 ~...
  114 	   115 	 0.40592 	 0.44556 	 m..s
  116 	   116 	 0.46012 	 0.44843 	 ~...
  111 	   117 	 0.40114 	 0.45539 	 m..s
  119 	   118 	 0.50969 	 0.48103 	 ~...
  117 	   119 	 0.46440 	 0.48829 	 ~...
  120 	   120 	 0.65202 	 0.61365 	 m..s
==========================================
r_mrr = 0.9861027598381042
r2_mrr = 0.969498872756958
spearmanr_mrr@5 = 0.9992814064025879
spearmanr_mrr@10 = 0.9814372062683105
spearmanr_mrr@50 = 0.9889554977416992
spearmanr_mrr@100 = 0.9969769716262817
spearmanr_mrr@All = 0.9973194003105164
==========================================
test time: 0.455
Done Testing dataset Kinships
total time taken: 234.2461235523224
training time taken: 226.51844692230225
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'Kinships': tensor(0.9861)}}, 'r2_mrr': {'ComplEx': {'Kinships': tensor(0.9695)}}, 'spearmanr_mrr@5': {'ComplEx': {'Kinships': tensor(0.9993)}}, 'spearmanr_mrr@10': {'ComplEx': {'Kinships': tensor(0.9814)}}, 'spearmanr_mrr@50': {'ComplEx': {'Kinships': tensor(0.9890)}}, 'spearmanr_mrr@100': {'ComplEx': {'Kinships': tensor(0.9970)}}, 'spearmanr_mrr@All': {'ComplEx': {'Kinships': tensor(0.9973)}}, 'test_loss': {'ComplEx': {'Kinships': 0.7017791451744415}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 5627025037156091
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [680, 963, 757, 956, 260, 369, 615, 707, 155, 1042, 1158, 444, 35, 10, 483, 1195, 456, 278, 380, 676, 404, 361, 558, 595, 1096, 753, 824, 184, 302, 445, 821, 806, 1006, 491, 78, 919, 472, 339, 501, 393, 308, 1125, 955, 63, 170, 1035, 1082, 606, 416, 773, 1151, 1005, 494, 780, 1025, 81, 712, 44, 1003, 381, 697, 534, 981, 476, 493, 91, 1185, 1087, 812, 432, 610, 458, 427, 1144, 437, 1001, 276, 1032, 590, 1170, 167, 489, 301, 45, 435, 1047, 253, 677, 309, 896, 394, 180, 759, 957, 284, 6, 231, 843, 862, 341, 622, 270, 886, 724, 482, 609, 1190, 582, 249, 411, 1037, 300, 1046, 499, 777, 656, 541, 587, 150, 169, 888]
valid_ids (0): []
train_ids (1094): [876, 714, 1054, 836, 626, 735, 469, 705, 214, 200, 585, 219, 426, 275, 1147, 791, 1112, 321, 915, 316, 382, 880, 576, 783, 910, 1064, 351, 140, 916, 505, 235, 992, 313, 421, 303, 422, 671, 27, 689, 1074, 924, 850, 322, 979, 1045, 723, 368, 224, 594, 365, 277, 258, 1161, 165, 754, 34, 203, 75, 778, 845, 701, 1002, 447, 584, 646, 933, 33, 455, 428, 431, 917, 131, 147, 352, 363, 570, 878, 722, 550, 178, 904, 1167, 935, 841, 630, 747, 204, 12, 987, 1036, 751, 989, 185, 117, 190, 336, 130, 868, 79, 462, 503, 264, 729, 892, 991, 653, 825, 621, 182, 272, 359, 1092, 279, 1056, 1068, 1198, 920, 826, 375, 202, 1146, 866, 13, 1072, 1152, 134, 581, 823, 323, 406, 1097, 767, 732, 567, 781, 51, 1209, 758, 312, 1148, 733, 739, 121, 738, 177, 464, 583, 267, 1093, 632, 521, 254, 858, 737, 861, 822, 116, 288, 527, 1040, 771, 996, 223, 1019, 1207, 209, 251, 111, 776, 634, 601, 479, 954, 752, 402, 995, 710, 761, 335, 1041, 523, 1039, 439, 396, 210, 1077, 304, 135, 334, 233, 967, 171, 373, 236, 305, 354, 156, 95, 348, 693, 648, 160, 330, 695, 152, 1110, 1140, 545, 1066, 389, 488, 914, 813, 698, 1100, 946, 941, 1179, 1114, 28, 16, 810, 485, 403, 814, 317, 417, 337, 832, 803, 788, 438, 26, 531, 230, 1169, 237, 819, 326, 779, 283, 512, 148, 0, 942, 1023, 468, 103, 800, 597, 263, 1070, 1141, 529, 694, 86, 669, 683, 811, 287, 1192, 1214, 226, 285, 655, 645, 704, 1142, 159, 372, 378, 311, 64, 446, 978, 629, 608, 854, 681, 36, 708, 687, 1033, 1136, 959, 887, 212, 936, 672, 137, 30, 1199, 1095, 572, 419, 715, 938, 804, 1048, 997, 1120, 189, 833, 504, 232, 478, 205, 1171, 213, 520, 740, 480, 867, 763, 742, 589, 3, 430, 1126, 179, 186, 1085, 29, 643, 709, 153, 986, 901, 85, 486, 22, 969, 1067, 562, 123, 424, 731, 703, 201, 1, 115, 119, 515, 261, 197, 314, 384, 1017, 217, 631, 328, 21, 1015, 994, 926, 766, 647, 1084, 908, 506, 741, 295, 132, 1128, 87, 4, 498, 1108, 1210, 183, 1000, 1124, 559, 667, 619, 39, 1213, 97, 965, 195, 420, 674, 652, 43, 664, 72, 1094, 891, 168, 875, 1132, 101, 1078, 852, 1180, 98, 816, 1186, 641, 1183, 885, 1156, 442, 307, 657, 551, 857, 1104, 227, 640, 17, 221, 817, 452, 573, 93, 474, 487, 1020, 1043, 829, 906, 644, 569, 556, 557, 661, 149, 1168, 358, 222, 9, 53, 555, 749, 921, 1079, 42, 877, 496, 691, 495, 440, 787, 770, 789, 984, 273, 1212, 1118, 871, 141, 292, 471, 526, 400, 666, 423, 349, 1201, 614, 70, 1090, 47, 616, 259, 319, 864, 225, 1091, 847, 248, 174, 897, 870, 1065, 164, 364, 720, 173, 357, 542, 346, 196, 1160, 890, 490, 240, 124, 966, 846, 790, 792, 1007, 1098, 106, 41, 18, 947, 1172, 976, 638, 678, 772, 931, 88, 840, 660, 1049, 162, 815, 74, 988, 863, 566, 122, 983, 246, 1135, 1089, 528, 11, 768, 105, 869, 940, 786, 463, 552, 580, 1127, 762, 802, 510, 84, 252, 338, 320, 280, 112, 1069, 807, 443, 1044, 553, 533, 68, 860, 1071, 838, 848, 579, 554, 964, 166, 623, 310, 1202, 387, 407, 399, 1101, 827, 90, 470, 1165, 306, 668, 620, 588, 719, 107, 736, 208, 434, 408, 1163, 1184, 565, 903, 48, 1193, 289, 350, 66, 612, 1197, 899, 1189, 398, 1107, 952, 795, 1155, 1159, 71, 1164, 1031, 1004, 905, 1157, 1010, 1143, 568, 637, 120, 410, 799, 229, 663, 578, 519, 198, 2, 395, 716, 466, 522, 571, 675, 717, 59, 745, 118, 49, 73, 89, 1162, 379, 1175, 1024, 500, 414, 23, 713, 8, 61, 670, 513, 1061, 1038, 982, 839, 543, 256, 561, 937, 376, 879, 1153, 271, 1204, 5, 344, 797, 525, 893, 702, 243, 785, 535, 1028, 726, 925, 1026, 62, 922, 808, 126, 161, 467, 1057, 711, 889, 818, 748, 930, 586, 627, 325, 900, 356, 158, 939, 998, 146, 297, 592, 244, 782, 1055, 774, 730, 696, 682, 530, 650, 750, 1203, 1188, 798, 392, 281, 775, 546, 50, 928, 949, 1182, 52, 635, 67, 1021, 980, 1200, 176, 388, 38, 290, 865, 1133, 756, 274, 658, 1012, 977, 46, 1191, 327, 835, 1076, 602, 884, 207, 429, 517, 109, 607, 397, 331, 1060, 902, 157, 142, 507, 706, 460, 332, 138, 367, 538, 129, 206, 796, 25, 401, 599, 1109, 1119, 370, 728, 299, 968, 1149, 564, 990, 639, 1138, 918, 617, 37, 54, 518, 65, 481, 333, 1206, 642, 909, 451, 929, 970, 242, 673, 927, 92, 347, 769, 895, 366, 69, 958, 1130, 220, 537, 296, 1154, 593, 1115, 794, 113, 60, 1177, 600, 355, 743, 125, 257, 974, 139, 1117, 744, 1030, 524, 881, 801, 96, 127, 649, 151, 145, 191, 943, 324, 1173, 475, 40, 102, 1022, 15, 1121, 1122, 872, 549, 108, 1062, 172, 1137, 502, 286, 809, 842, 725, 293, 577, 1134, 82, 574, 234, 511, 1083, 1063, 686, 374, 187, 746, 611, 837, 547, 448, 973, 383, 291, 1008, 898, 405, 1102, 1086, 1211, 188, 1073, 1178, 651, 449, 32, 718, 932, 975, 266, 948, 856, 907, 1014, 993, 193, 509, 563, 7, 1080, 1081, 874, 175, 412, 684, 575, 596, 163, 318, 951, 853, 239, 265, 1181, 665, 19, 128, 282, 241, 883, 540, 459, 727, 1013, 1058, 913, 211, 247, 154, 1113, 960, 497, 536, 516, 418, 269, 1018, 961, 1099, 215, 685, 618, 972, 24, 385, 1088, 94, 945, 457, 484, 985, 734, 199, 1105, 688, 1050, 100, 342, 953, 544, 433, 76, 477, 934, 605, 1027, 923, 262, 450, 1145, 390, 514, 1174, 1029, 1011, 539, 58, 1111, 1051, 20, 454, 83, 834, 345, 636, 912, 465, 628, 755, 1053, 1116, 1150, 820, 844, 245, 218, 255, 461, 1131, 415, 492, 473, 268, 114, 360, 654, 603, 238, 353, 851, 699, 315, 894, 194, 453, 143, 441, 436, 1075, 250, 679, 1009, 136, 849, 692, 613, 1208, 1106, 944, 181, 662, 1123, 690, 329, 1205, 700, 31, 192, 598, 391, 216, 532, 764, 55, 377, 425, 873, 298, 1176, 591, 1139, 828, 1194, 1187, 228, 659, 144, 760, 294, 1034, 99, 971, 371, 859, 409, 1016, 77, 110, 560, 999, 1196, 56, 1052, 548, 721, 413, 805, 343, 604, 340, 1059, 133, 1166, 793, 911, 386, 625, 784, 104, 14, 508, 1129, 882, 765, 624, 950, 57, 80, 831, 855, 633, 830, 362, 1103, 962]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6318669839843700
the save name prefix for this run is:  chkpt-ID_6318669839843700_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1093
rank avg (pred): 0.531 +- 0.004
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001365957

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 617
rank avg (pred): 0.482 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001053753

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 906
rank avg (pred): 0.424 +- 0.007
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0010738

Epoch over!
epoch time: 15.192

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 351
rank avg (pred): 0.490 +- 0.026
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001159971

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 893
rank avg (pred): 0.395 +- 0.051
mrr vals (pred, true): 0.000, 0.011
batch losses (mrrl, rdl): 0.0, 0.0009218683

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 814
rank avg (pred): 0.163 +- 0.115
mrr vals (pred, true): 0.001, 0.349
batch losses (mrrl, rdl): 0.0, 0.0002409712

Epoch over!
epoch time: 15.138

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 892
rank avg (pred): 0.397 +- 0.157
mrr vals (pred, true): 0.000, 0.045
batch losses (mrrl, rdl): 0.0, 0.0003744903

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1065
rank avg (pred): 0.241 +- 0.224
mrr vals (pred, true): 0.000, 0.102
batch losses (mrrl, rdl): 0.0, 0.000179148

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 983
rank avg (pred): 0.381 +- 0.316
mrr vals (pred, true): 0.000, 0.081
batch losses (mrrl, rdl): 0.0, 1.32861e-05

Epoch over!
epoch time: 15.119

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 179
rank avg (pred): 0.517 +- 0.282
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.5673e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 504
rank avg (pred): 0.388 +- 0.315
mrr vals (pred, true): 0.000, 0.203
batch losses (mrrl, rdl): 0.0, 0.0003563082

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.518 +- 0.282
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 4.7262e-06

Epoch over!
epoch time: 15.124

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 474
rank avg (pred): 0.519 +- 0.290
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 4.9104e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 87
rank avg (pred): 0.521 +- 0.292
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 5.5117e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 105
rank avg (pred): 0.515 +- 0.280
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 4.0203e-06

Epoch over!
epoch time: 15.13

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 688
rank avg (pred): 0.508 +- 0.286
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0248273462, 5.0219e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 511
rank avg (pred): 0.241 +- 0.413
mrr vals (pred, true): 0.102, 0.223
batch losses (mrrl, rdl): 0.1477098763, 9.33933e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 692
rank avg (pred): 0.524 +- 0.319
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 8.99054e-05, 6.1048e-06

Epoch over!
epoch time: 15.351

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 0
rank avg (pred): 0.443 +- 0.333
mrr vals (pred, true): 0.066, 0.072
batch losses (mrrl, rdl): 0.0024065869, 0.0002997498

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 755
rank avg (pred): 0.318 +- 0.414
mrr vals (pred, true): 0.110, 0.190
batch losses (mrrl, rdl): 0.0653241724, 0.0002271732

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 252
rank avg (pred): 0.339 +- 0.436
mrr vals (pred, true): 0.119, 0.259
batch losses (mrrl, rdl): 0.1963442862, 0.0008979274

Epoch over!
epoch time: 15.254

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 158
rank avg (pred): 0.508 +- 0.289
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.000301384, 1.6453e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 425
rank avg (pred): 0.488 +- 0.300
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0007643405, 6.8272e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 850
rank avg (pred): 0.468 +- 0.274
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004291913, 2.22892e-05

Epoch over!
epoch time: 15.168

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 894
rank avg (pred): 0.268 +- 0.338
mrr vals (pred, true): 0.126, 0.016
batch losses (mrrl, rdl): 0.0579301193, 0.0008289936

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 508
rank avg (pred): 0.282 +- 0.354
mrr vals (pred, true): 0.165, 0.218
batch losses (mrrl, rdl): 0.028027568, 7.14821e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 621
rank avg (pred): 0.492 +- 0.274
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 8.8915e-06, 5.9339e-06

Epoch over!
epoch time: 15.154

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 364
rank avg (pred): 0.503 +- 0.270
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001021045, 5.1707e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 525
rank avg (pred): 0.412 +- 0.301
mrr vals (pred, true): 0.068, 0.064
batch losses (mrrl, rdl): 0.0033754993, 3.22521e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 256
rank avg (pred): 0.246 +- 0.332
mrr vals (pred, true): 0.189, 0.272
batch losses (mrrl, rdl): 0.0675808042, 0.0002525676

Epoch over!
epoch time: 15.175

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 354
rank avg (pred): 0.495 +- 0.282
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 4.79713e-05, 3.0922e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 649
rank avg (pred): 0.503 +- 0.292
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002217417, 7.473e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 746
rank avg (pred): 0.284 +- 0.292
mrr vals (pred, true): 0.123, 0.212
batch losses (mrrl, rdl): 0.0805420876, 0.0001258917

Epoch over!
epoch time: 15.356

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 446
rank avg (pred): 0.478 +- 0.259
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0011286864, 1.35032e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 140
rank avg (pred): 0.472 +- 0.262
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.40769e-05, 1.071e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 346
rank avg (pred): 0.503 +- 0.308
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.000182389, 2.8511e-06

Epoch over!
epoch time: 15.338

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 786
rank avg (pred): 0.543 +- 0.291
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 7.174e-07, 2.76469e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 199
rank avg (pred): 0.485 +- 0.270
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 8.57002e-05, 6.0094e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 255
rank avg (pred): 0.231 +- 0.253
mrr vals (pred, true): 0.182, 0.256
batch losses (mrrl, rdl): 0.0551306643, 0.0003338259

Epoch over!
epoch time: 15.283

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 297
rank avg (pred): 0.378 +- 0.268
mrr vals (pred, true): 0.060, 0.068
batch losses (mrrl, rdl): 0.0010785564, 1.69282e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 343
rank avg (pred): 0.493 +- 0.294
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.46316e-05, 2.367e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1162
rank avg (pred): 0.577 +- 0.333
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001598957, 0.0001040091

Epoch over!
epoch time: 15.135

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 208
rank avg (pred): 0.472 +- 0.293
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 6.95e-07, 1.52369e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 566
rank avg (pred): 0.432 +- 0.268
mrr vals (pred, true): 0.050, 0.056
batch losses (mrrl, rdl): 1.21e-08, 7.49779e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 428
rank avg (pred): 0.459 +- 0.286
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 7.54e-07, 4.05786e-05

Epoch over!
epoch time: 15.116

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.503 +- 0.299
mrr vals (pred, true): 0.045, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04695 	 0.00042 	 m..s
   41 	     1 	 0.05077 	 0.00044 	 m..s
    3 	     2 	 0.04552 	 0.00048 	 m..s
   82 	     3 	 0.05370 	 0.00050 	 m..s
   61 	     4 	 0.05207 	 0.00050 	 m..s
   24 	     5 	 0.04919 	 0.00051 	 m..s
   80 	     6 	 0.05351 	 0.00051 	 m..s
   15 	     7 	 0.04880 	 0.00052 	 m..s
   39 	     8 	 0.05073 	 0.00053 	 m..s
   47 	     9 	 0.05112 	 0.00054 	 m..s
   88 	    10 	 0.05436 	 0.00054 	 m..s
   30 	    11 	 0.04942 	 0.00055 	 m..s
    6 	    12 	 0.04671 	 0.00056 	 m..s
   22 	    13 	 0.04913 	 0.00057 	 m..s
   33 	    14 	 0.04948 	 0.00058 	 m..s
   29 	    15 	 0.04940 	 0.00060 	 m..s
   71 	    16 	 0.05299 	 0.00060 	 m..s
    1 	    17 	 0.04548 	 0.00061 	 m..s
   72 	    18 	 0.05304 	 0.00061 	 m..s
   10 	    19 	 0.04817 	 0.00061 	 m..s
   38 	    20 	 0.05067 	 0.00063 	 m..s
   64 	    21 	 0.05228 	 0.00064 	 m..s
   84 	    22 	 0.05407 	 0.00064 	 m..s
    0 	    23 	 0.04539 	 0.00064 	 m..s
   79 	    24 	 0.05332 	 0.00064 	 m..s
   14 	    25 	 0.04877 	 0.00070 	 m..s
   36 	    26 	 0.05055 	 0.00072 	 m..s
   51 	    27 	 0.05155 	 0.00073 	 m..s
   59 	    28 	 0.05194 	 0.00074 	 m..s
   46 	    29 	 0.05111 	 0.00074 	 m..s
   68 	    30 	 0.05269 	 0.00074 	 m..s
   16 	    31 	 0.04884 	 0.00074 	 m..s
   90 	    32 	 0.05541 	 0.00075 	 m..s
   44 	    33 	 0.05079 	 0.00076 	 m..s
   53 	    34 	 0.05167 	 0.00076 	 m..s
   73 	    35 	 0.05310 	 0.00077 	 m..s
   13 	    36 	 0.04874 	 0.00078 	 m..s
   83 	    37 	 0.05384 	 0.00080 	 m..s
   85 	    38 	 0.05411 	 0.00081 	 m..s
   81 	    39 	 0.05356 	 0.00081 	 m..s
   70 	    40 	 0.05297 	 0.00082 	 m..s
   86 	    41 	 0.05424 	 0.00084 	 m..s
   57 	    42 	 0.05181 	 0.00084 	 m..s
   18 	    43 	 0.04890 	 0.00088 	 m..s
   48 	    44 	 0.05118 	 0.00088 	 m..s
   69 	    45 	 0.05274 	 0.00088 	 m..s
    4 	    46 	 0.04588 	 0.00089 	 m..s
    5 	    47 	 0.04604 	 0.00090 	 m..s
   12 	    48 	 0.04829 	 0.00091 	 m..s
   43 	    49 	 0.05079 	 0.00092 	 m..s
   63 	    50 	 0.05221 	 0.00094 	 m..s
    2 	    51 	 0.04549 	 0.00101 	 m..s
   49 	    52 	 0.05132 	 0.00101 	 m..s
   26 	    53 	 0.04924 	 0.00103 	 m..s
   77 	    54 	 0.05329 	 0.00104 	 m..s
   35 	    55 	 0.05003 	 0.00104 	 m..s
   52 	    56 	 0.05165 	 0.00105 	 m..s
   55 	    57 	 0.05172 	 0.00110 	 m..s
   78 	    58 	 0.05330 	 0.00114 	 m..s
   65 	    59 	 0.05234 	 0.00115 	 m..s
   56 	    60 	 0.05175 	 0.00115 	 m..s
   27 	    61 	 0.04925 	 0.00119 	 m..s
   23 	    62 	 0.04916 	 0.00121 	 m..s
   28 	    63 	 0.04930 	 0.00133 	 m..s
   66 	    64 	 0.05244 	 0.00139 	 m..s
   42 	    65 	 0.05078 	 0.00146 	 m..s
   45 	    66 	 0.05100 	 0.00148 	 m..s
   34 	    67 	 0.04973 	 0.00153 	 m..s
   74 	    68 	 0.05312 	 0.00154 	 m..s
    7 	    69 	 0.04678 	 0.00159 	 m..s
   37 	    70 	 0.05062 	 0.00167 	 m..s
   89 	    71 	 0.05534 	 0.00171 	 m..s
   32 	    72 	 0.04946 	 0.00171 	 m..s
   17 	    73 	 0.04888 	 0.00174 	 m..s
    9 	    74 	 0.04814 	 0.00176 	 m..s
   76 	    75 	 0.05328 	 0.00181 	 m..s
   75 	    76 	 0.05327 	 0.00181 	 m..s
   31 	    77 	 0.04945 	 0.00186 	 m..s
   21 	    78 	 0.04909 	 0.00200 	 m..s
   25 	    79 	 0.04920 	 0.00217 	 m..s
   40 	    80 	 0.05073 	 0.00227 	 m..s
   11 	    81 	 0.04826 	 0.00231 	 m..s
   20 	    82 	 0.04902 	 0.00245 	 m..s
   19 	    83 	 0.04899 	 0.00350 	 m..s
  107 	    84 	 0.12939 	 0.01254 	 MISS
   92 	    85 	 0.06445 	 0.04752 	 ~...
   93 	    86 	 0.06560 	 0.04959 	 ~...
  106 	    87 	 0.12258 	 0.05624 	 m..s
   94 	    88 	 0.07050 	 0.06278 	 ~...
   54 	    89 	 0.05169 	 0.06384 	 ~...
   91 	    90 	 0.05742 	 0.06445 	 ~...
   96 	    91 	 0.07291 	 0.06450 	 ~...
   50 	    92 	 0.05145 	 0.06558 	 ~...
   95 	    93 	 0.07153 	 0.06890 	 ~...
   62 	    94 	 0.05215 	 0.07113 	 ~...
  108 	    95 	 0.13060 	 0.07170 	 m..s
   87 	    96 	 0.05429 	 0.07213 	 ~...
  115 	    97 	 0.20141 	 0.07225 	 MISS
   97 	    98 	 0.07446 	 0.07250 	 ~...
  102 	    99 	 0.11613 	 0.07493 	 m..s
  103 	   100 	 0.11627 	 0.07867 	 m..s
  116 	   101 	 0.20172 	 0.07963 	 MISS
   60 	   102 	 0.05200 	 0.08018 	 ~...
   98 	   103 	 0.07776 	 0.08027 	 ~...
  104 	   104 	 0.11924 	 0.08041 	 m..s
   58 	   105 	 0.05182 	 0.08449 	 m..s
   99 	   106 	 0.07848 	 0.08521 	 ~...
   67 	   107 	 0.05249 	 0.08794 	 m..s
  112 	   108 	 0.14171 	 0.09686 	 m..s
  113 	   109 	 0.14393 	 0.09711 	 m..s
  101 	   110 	 0.09545 	 0.12857 	 m..s
  105 	   111 	 0.11967 	 0.12925 	 ~...
  100 	   112 	 0.09074 	 0.14451 	 m..s
  110 	   113 	 0.13581 	 0.16480 	 ~...
  111 	   114 	 0.14038 	 0.17972 	 m..s
  109 	   115 	 0.13082 	 0.18250 	 m..s
  114 	   116 	 0.19433 	 0.18564 	 ~...
  118 	   117 	 0.20793 	 0.26597 	 m..s
  119 	   118 	 0.20831 	 0.27307 	 m..s
  120 	   119 	 0.22294 	 0.29363 	 m..s
  117 	   120 	 0.20704 	 0.32392 	 MISS
==========================================
r_mrr = 0.8535242080688477
r2_mrr = 0.3930022716522217
spearmanr_mrr@5 = 0.8329699039459229
spearmanr_mrr@10 = 0.8173376321792603
spearmanr_mrr@50 = 0.9300811290740967
spearmanr_mrr@100 = 0.949110746383667
spearmanr_mrr@All = 0.9520179629325867
==========================================
test time: 0.455
Done Testing dataset OpenEA
total time taken: 254.96083450317383
training time taken: 228.49850726127625
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8535)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.3930)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.8330)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.8173)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9301)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9491)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9520)}}, 'test_loss': {'ComplEx': {'OpenEA': 1.3532436750947454}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 9578538038609070
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [620, 1020, 329, 1171, 858, 249, 1115, 176, 966, 923, 215, 389, 623, 562, 302, 324, 229, 263, 419, 1203, 847, 880, 739, 998, 440, 888, 248, 710, 1106, 288, 396, 1127, 887, 808, 1206, 782, 631, 748, 1119, 769, 31, 678, 242, 1099, 184, 757, 260, 875, 694, 790, 610, 200, 1068, 776, 46, 1180, 702, 365, 1151, 788, 132, 959, 500, 659, 204, 523, 687, 817, 1175, 896, 539, 864, 940, 158, 379, 391, 559, 425, 439, 924, 581, 1103, 587, 648, 162, 371, 1212, 239, 763, 690, 749, 984, 1192, 466, 818, 795, 363, 475, 778, 452, 462, 540, 746, 23, 489, 871, 962, 698, 272, 988, 412, 291, 120, 1147, 1077, 553, 815, 618, 98, 609, 221]
valid_ids (0): []
train_ids (1094): [787, 181, 297, 147, 29, 281, 898, 450, 773, 846, 1056, 1148, 433, 568, 897, 807, 293, 919, 717, 1105, 464, 392, 93, 18, 490, 201, 920, 1154, 517, 1052, 624, 600, 792, 1143, 1118, 573, 820, 825, 238, 415, 634, 435, 222, 264, 980, 65, 469, 1022, 593, 252, 731, 234, 542, 22, 987, 431, 308, 340, 637, 766, 115, 1110, 527, 479, 52, 917, 35, 616, 1098, 432, 546, 455, 76, 67, 960, 567, 674, 833, 992, 45, 767, 764, 352, 1079, 999, 1201, 621, 355, 1094, 693, 1125, 974, 438, 643, 633, 884, 1161, 967, 822, 358, 1018, 564, 413, 592, 226, 1182, 666, 699, 575, 989, 971, 668, 774, 194, 997, 28, 232, 1091, 783, 1062, 118, 346, 47, 803, 740, 1166, 834, 382, 712, 544, 642, 685, 713, 359, 853, 571, 467, 253, 1021, 483, 1071, 1023, 797, 626, 629, 1093, 119, 1037, 1028, 647, 1202, 628, 481, 276, 1156, 401, 709, 1191, 294, 217, 1104, 1000, 859, 1015, 726, 470, 134, 986, 321, 497, 4, 225, 1160, 649, 196, 163, 169, 1186, 357, 160, 370, 289, 195, 1102, 918, 100, 381, 102, 138, 1038, 107, 1096, 451, 1090, 56, 211, 180, 977, 613, 762, 874, 170, 771, 420, 186, 1041, 233, 636, 561, 531, 916, 759, 137, 1111, 473, 1047, 255, 727, 758, 1117, 1025, 597, 730, 1165, 760, 150, 965, 903, 548, 877, 1139, 349, 237, 19, 70, 832, 1185, 198, 85, 1149, 216, 798, 388, 913, 1051, 1162, 973, 948, 251, 696, 104, 503, 1024, 259, 535, 656, 1060, 101, 1057, 686, 77, 208, 277, 84, 1109, 886, 1152, 632, 582, 1135, 1164, 437, 453, 484, 520, 1211, 824, 522, 5, 376, 703, 518, 968, 755, 579, 243, 446, 931, 635, 1014, 405, 284, 1138, 1133, 675, 625, 7, 1064, 143, 1004, 951, 835, 32, 779, 975, 614, 86, 323, 283, 384, 122, 105, 909, 1072, 958, 839, 867, 828, 729, 353, 1005, 1080, 250, 534, 41, 1045, 350, 1101, 129, 369, 1140, 905, 1214, 156, 397, 171, 873, 700, 203, 135, 943, 378, 953, 202, 494, 849, 280, 61, 870, 1061, 1126, 399, 715, 88, 59, 73, 861, 53, 262, 644, 44, 1034, 444, 268, 258, 1026, 768, 558, 509, 220, 338, 735, 1196, 1074, 994, 417, 716, 906, 947, 205, 848, 1008, 317, 780, 166, 1007, 505, 409, 14, 179, 476, 1054, 673, 21, 930, 765, 83, 336, 922, 69, 1176, 80, 360, 454, 136, 1011, 572, 1173, 142, 153, 1058, 366, 511, 615, 598, 602, 1153, 182, 372, 1210, 254, 826, 552, 275, 299, 63, 599, 1112, 172, 1084, 578, 944, 830, 996, 1078, 159, 55, 993, 810, 802, 537, 328, 528, 653, 1194, 236, 214, 510, 25, 227, 301, 1050, 364, 1204, 1035, 530, 1174, 210, 801, 175, 661, 719, 374, 213, 584, 942, 486, 811, 0, 459, 337, 292, 448, 168, 662, 287, 1006, 991, 402, 50, 133, 112, 646, 1159, 796, 403, 62, 167, 532, 1097, 271, 688, 718, 1137, 241, 821, 261, 844, 315, 1184, 1036, 79, 458, 152, 131, 639, 836, 1048, 747, 929, 508, 689, 926, 191, 894, 752, 295, 816, 447, 8, 212, 316, 725, 1046, 941, 630, 161, 309, 94, 68, 491, 851, 183, 1172, 722, 342, 895, 892, 445, 720, 770, 640, 1017, 524, 970, 326, 197, 1200, 13, 1145, 304, 144, 1086, 273, 576, 738, 536, 979, 334, 589, 1065, 485, 914, 1197, 854, 869, 850, 925, 612, 177, 488, 27, 937, 1095, 428, 300, 743, 165, 800, 728, 723, 857, 332, 981, 928, 495, 36, 130, 480, 734, 1042, 964, 1027, 704, 619, 1059, 956, 286, 591, 43, 375, 310, 889, 1029, 921, 1199, 1190, 827, 680, 560, 1142, 344, 513, 406, 9, 1040, 333, 706, 1167, 541, 487, 840, 306, 1085, 207, 141, 282, 805, 603, 125, 1, 985, 708, 188, 651, 679, 550, 1114, 1066, 411, 842, 976, 496, 878, 1136, 1113, 580, 588, 1129, 416, 1044, 1134, 1157, 51, 377, 116, 1187, 1002, 111, 912, 343, 2, 1116, 794, 590, 502, 347, 1012, 789, 348, 876, 872, 585, 775, 240, 1055, 1010, 1131, 934, 1189, 89, 705, 113, 1177, 1069, 506, 421, 732, 1087, 891, 461, 902, 911, 607, 809, 477, 314, 1130, 12, 1063, 331, 20, 148, 54, 193, 617, 1144, 66, 1195, 804, 395, 935, 269, 654, 6, 482, 952, 414, 145, 963, 1123, 777, 128, 430, 692, 110, 865, 187, 26, 38, 799, 504, 681, 278, 650, 424, 1132, 361, 711, 1049, 784, 1108, 669, 695, 1073, 881, 1188, 91, 772, 622, 37, 174, 545, 1208, 990, 1158, 521, 322, 551, 74, 831, 10, 390, 17, 744, 751, 761, 1053, 154, 982, 745, 1181, 157, 901, 185, 164, 933, 1183, 501, 684, 404, 515, 983, 676, 368, 367, 422, 586, 950, 427, 936, 596, 879, 472, 189, 474, 893, 721, 1088, 99, 638, 387, 742, 1169, 672, 190, 664, 554, 78, 463, 30, 298, 793, 1193, 307, 383, 219, 841, 611, 426, 883, 408, 516, 829, 493, 677, 34, 1009, 819, 311, 1120, 781, 97, 199, 750, 449, 663, 682, 945, 274, 478, 303, 58, 103, 1178, 39, 574, 305, 290, 319, 556, 512, 33, 1003, 436, 645, 256, 813, 423, 863, 538, 279, 791, 209, 407, 57, 658, 140, 24, 16, 223, 837, 1121, 498, 410, 946, 320, 670, 285, 978, 87, 492, 665, 296, 786, 957, 149, 547, 60, 855, 441, 1075, 660, 151, 72, 1107, 499, 1092, 40, 714, 1043, 124, 90, 915, 868, 139, 265, 1076, 106, 230, 885, 1198, 362, 1082, 457, 1122, 595, 386, 701, 555, 608, 244, 270, 1163, 96, 82, 533, 393, 121, 932, 456, 549, 318, 908, 11, 641, 525, 900, 529, 570, 860, 351, 224, 228, 961, 890, 114, 1030, 856, 1039, 206, 1019, 606, 838, 1141, 1124, 519, 117, 939, 1033, 460, 1170, 235, 927, 1205, 1150, 1100, 48, 81, 852, 1128, 812, 418, 335, 380, 92, 443, 652, 594, 691, 1067, 434, 247, 604, 109, 938, 907, 862, 465, 972, 514, 1179, 995, 339, 507, 569, 313, 42, 1213, 356, 823, 246, 655, 1146, 753, 904, 429, 1070, 442, 949, 1032, 123, 707, 1089, 1016, 266, 1081, 385, 15, 400, 736, 178, 95, 468, 267, 173, 325, 330, 1209, 601, 1001, 354, 192, 126, 394, 49, 557, 565, 583, 1031, 683, 231, 1168, 741, 667, 910, 845, 955, 1207, 1013, 373, 471, 754, 341, 806, 785, 345, 127, 526, 724, 64, 756, 954, 577, 327, 899, 1083, 657, 737, 882, 566, 969, 71, 3, 543, 697, 108, 155, 398, 257, 245, 605, 563, 866, 218, 75, 1155, 146, 814, 627, 733, 671, 312, 843]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  677701165176765
the save name prefix for this run is:  chkpt-ID_677701165176765_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 820
rank avg (pred): 0.491 +- 0.003
mrr vals (pred, true): 0.000, 0.134
batch losses (mrrl, rdl): 0.0, 0.0008198053

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 157
rank avg (pred): 0.512 +- 0.307
mrr vals (pred, true): 0.072, 0.001
batch losses (mrrl, rdl): 0.0, 1.87298e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 597
rank avg (pred): 0.483 +- 0.292
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0, 8.191e-07

Epoch over!
epoch time: 14.893

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 834
rank avg (pred): 0.383 +- 0.307
mrr vals (pred, true): 0.070, 0.184
batch losses (mrrl, rdl): 0.0, 0.0003326466

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 948
rank avg (pred): 0.477 +- 0.301
mrr vals (pred, true): 0.092, 0.001
batch losses (mrrl, rdl): 0.0, 2.1758e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 662
rank avg (pred): 0.478 +- 0.305
mrr vals (pred, true): 0.067, 0.001
batch losses (mrrl, rdl): 0.0, 1.22594e-05

Epoch over!
epoch time: 14.91

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 44
rank avg (pred): 0.351 +- 0.312
mrr vals (pred, true): 0.082, 0.088
batch losses (mrrl, rdl): 0.0, 3.3344e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 254
rank avg (pred): 0.213 +- 0.287
mrr vals (pred, true): 0.120, 0.286
batch losses (mrrl, rdl): 0.0, 5.82509e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 130
rank avg (pred): 0.494 +- 0.297
mrr vals (pred, true): 0.068, 0.002
batch losses (mrrl, rdl): 0.0, 3.633e-07

Epoch over!
epoch time: 14.905

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 39
rank avg (pred): 0.353 +- 0.324
mrr vals (pred, true): 0.096, 0.089
batch losses (mrrl, rdl): 0.0, 3.356e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 992
rank avg (pred): 0.368 +- 0.326
mrr vals (pred, true): 0.108, 0.071
batch losses (mrrl, rdl): 0.0, 5.3518e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1029
rank avg (pred): 0.481 +- 0.294
mrr vals (pred, true): 0.078, 0.001
batch losses (mrrl, rdl): 0.0, 7.2335e-06

Epoch over!
epoch time: 14.897

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 62
rank avg (pred): 0.363 +- 0.310
mrr vals (pred, true): 0.103, 0.071
batch losses (mrrl, rdl): 0.0, 2.6104e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 42
rank avg (pred): 0.345 +- 0.313
mrr vals (pred, true): 0.109, 0.088
batch losses (mrrl, rdl): 0.0, 2.2786e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1107
rank avg (pred): 0.528 +- 0.298
mrr vals (pred, true): 0.074, 0.001
batch losses (mrrl, rdl): 0.0, 1.47745e-05

Epoch over!
epoch time: 14.766

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.289 +- 0.285
mrr vals (pred, true): 0.111, 0.103
batch losses (mrrl, rdl): 0.0006423663, 2.87354e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 270
rank avg (pred): 0.466 +- 0.266
mrr vals (pred, true): 0.076, 0.069
batch losses (mrrl, rdl): 0.0069624204, 0.0002098642

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1161
rank avg (pred): 0.508 +- 0.253
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.000195929, 1.12054e-05

Epoch over!
epoch time: 15.049

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1083
rank avg (pred): 0.512 +- 0.262
mrr vals (pred, true): 0.047, 0.002
batch losses (mrrl, rdl): 0.0001117708, 7.9962e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1196
rank avg (pred): 0.492 +- 0.286
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0015138375, 9.6915e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 474
rank avg (pred): 0.477 +- 0.243
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002031713, 2.73332e-05

Epoch over!
epoch time: 15.016

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 555
rank avg (pred): 0.469 +- 0.304
mrr vals (pred, true): 0.072, 0.070
batch losses (mrrl, rdl): 0.0050278772, 0.0001667954

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 995
rank avg (pred): 0.432 +- 0.288
mrr vals (pred, true): 0.061, 0.100
batch losses (mrrl, rdl): 0.001130091, 0.0001459951

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 2
rank avg (pred): 0.364 +- 0.318
mrr vals (pred, true): 0.084, 0.123
batch losses (mrrl, rdl): 0.0151534118, 5.97132e-05

Epoch over!
epoch time: 15.024

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 580
rank avg (pred): 0.478 +- 0.283
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0008274204, 1.38981e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 791
rank avg (pred): 0.546 +- 0.288
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002738352, 2.08395e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 17
rank avg (pred): 0.245 +- 0.317
mrr vals (pred, true): 0.161, 0.272
batch losses (mrrl, rdl): 0.1230955422, 9.95544e-05

Epoch over!
epoch time: 15.023

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 441
rank avg (pred): 0.475 +- 0.277
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.29275e-05, 5.37482e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 787
rank avg (pred): 0.523 +- 0.275
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003894489, 6.226e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 352
rank avg (pred): 0.430 +- 0.284
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.46116e-05, 0.0001253781

Epoch over!
epoch time: 15.033

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 357
rank avg (pred): 0.441 +- 0.276
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0005206623, 9.83824e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1034
rank avg (pred): 0.520 +- 0.283
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.77596e-05, 6.4611e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 131
rank avg (pred): 0.514 +- 0.275
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.98656e-05, 6.4855e-06

Epoch over!
epoch time: 15.196

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 920
rank avg (pred): 0.487 +- 0.273
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.93992e-05, 2.56067e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 472
rank avg (pred): 0.494 +- 0.270
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002444528, 1.37029e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1036
rank avg (pred): 0.502 +- 0.293
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 7.4422e-06, 7.4506e-06

Epoch over!
epoch time: 15.252

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 768
rank avg (pred): 0.574 +- 0.314
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.72116e-05, 7.02272e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 14
rank avg (pred): 0.238 +- 0.333
mrr vals (pred, true): 0.253, 0.271
batch losses (mrrl, rdl): 0.0031850056, 5.64935e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 626
rank avg (pred): 0.468 +- 0.298
mrr vals (pred, true): 0.045, 0.002
batch losses (mrrl, rdl): 0.0002628294, 5.03937e-05

Epoch over!
epoch time: 15.239

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 504
rank avg (pred): 0.250 +- 0.327
mrr vals (pred, true): 0.265, 0.203
batch losses (mrrl, rdl): 0.03814153, 9.7642e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 536
rank avg (pred): 0.399 +- 0.324
mrr vals (pred, true): 0.062, 0.062
batch losses (mrrl, rdl): 0.001493983, 1.08391e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 507
rank avg (pred): 0.208 +- 0.259
mrr vals (pred, true): 0.253, 0.204
batch losses (mrrl, rdl): 0.0242933538, 1.69324e-05

Epoch over!
epoch time: 15.26

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 555
rank avg (pred): 0.415 +- 0.309
mrr vals (pred, true): 0.055, 0.070
batch losses (mrrl, rdl): 0.0002912302, 2.97313e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 392
rank avg (pred): 0.481 +- 0.307
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.10409e-05, 2.8043e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1008
rank avg (pred): 0.512 +- 0.311
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.39127e-05, 6.8347e-06

Epoch over!
epoch time: 15.243

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.512 +- 0.283
mrr vals (pred, true): 0.048, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04487 	 0.00038 	 m..s
   23 	     1 	 0.04659 	 0.00048 	 m..s
   11 	     2 	 0.04537 	 0.00048 	 m..s
   82 	     3 	 0.05418 	 0.00050 	 m..s
   91 	     4 	 0.05494 	 0.00051 	 m..s
   22 	     5 	 0.04657 	 0.00051 	 m..s
   82 	     6 	 0.05418 	 0.00053 	 m..s
   16 	     7 	 0.04591 	 0.00054 	 m..s
   54 	     8 	 0.04902 	 0.00054 	 m..s
    2 	     9 	 0.04465 	 0.00055 	 m..s
   36 	    10 	 0.04766 	 0.00056 	 m..s
   53 	    11 	 0.04901 	 0.00056 	 m..s
   57 	    12 	 0.04916 	 0.00057 	 m..s
   69 	    13 	 0.05134 	 0.00057 	 m..s
    4 	    14 	 0.04467 	 0.00057 	 m..s
    2 	    15 	 0.04465 	 0.00059 	 m..s
   35 	    16 	 0.04763 	 0.00060 	 m..s
   86 	    17 	 0.05457 	 0.00060 	 m..s
    5 	    18 	 0.04469 	 0.00060 	 m..s
   32 	    19 	 0.04741 	 0.00060 	 m..s
   66 	    20 	 0.05046 	 0.00060 	 m..s
   11 	    21 	 0.04537 	 0.00061 	 m..s
    1 	    22 	 0.04425 	 0.00062 	 m..s
   59 	    23 	 0.04924 	 0.00062 	 m..s
   76 	    24 	 0.05216 	 0.00063 	 m..s
   29 	    25 	 0.04731 	 0.00063 	 m..s
   18 	    26 	 0.04605 	 0.00066 	 m..s
   88 	    27 	 0.05466 	 0.00066 	 m..s
   24 	    28 	 0.04667 	 0.00066 	 m..s
   61 	    29 	 0.04952 	 0.00067 	 m..s
   88 	    30 	 0.05466 	 0.00067 	 m..s
   64 	    31 	 0.04977 	 0.00067 	 m..s
   81 	    32 	 0.05327 	 0.00068 	 m..s
   85 	    33 	 0.05422 	 0.00068 	 m..s
   40 	    34 	 0.04785 	 0.00068 	 m..s
   31 	    35 	 0.04734 	 0.00069 	 m..s
   28 	    36 	 0.04725 	 0.00069 	 m..s
    6 	    37 	 0.04481 	 0.00070 	 m..s
   44 	    38 	 0.04841 	 0.00070 	 m..s
   65 	    39 	 0.04994 	 0.00071 	 m..s
   62 	    40 	 0.04956 	 0.00074 	 m..s
    9 	    41 	 0.04488 	 0.00075 	 m..s
   54 	    42 	 0.04902 	 0.00075 	 m..s
   10 	    43 	 0.04505 	 0.00077 	 m..s
   80 	    44 	 0.05316 	 0.00078 	 m..s
   47 	    45 	 0.04855 	 0.00078 	 m..s
   21 	    46 	 0.04654 	 0.00081 	 m..s
    7 	    47 	 0.04482 	 0.00081 	 m..s
   77 	    48 	 0.05243 	 0.00082 	 m..s
   67 	    49 	 0.05068 	 0.00085 	 m..s
   49 	    50 	 0.04870 	 0.00085 	 m..s
    0 	    51 	 0.04422 	 0.00086 	 m..s
   70 	    52 	 0.05142 	 0.00087 	 m..s
   45 	    53 	 0.04844 	 0.00089 	 m..s
   51 	    54 	 0.04882 	 0.00094 	 m..s
   52 	    55 	 0.04899 	 0.00097 	 m..s
   42 	    56 	 0.04795 	 0.00098 	 m..s
   84 	    57 	 0.05420 	 0.00098 	 m..s
   39 	    58 	 0.04785 	 0.00100 	 m..s
   30 	    59 	 0.04733 	 0.00100 	 m..s
   90 	    60 	 0.05486 	 0.00100 	 m..s
   40 	    61 	 0.04785 	 0.00100 	 m..s
   58 	    62 	 0.04922 	 0.00101 	 m..s
   34 	    63 	 0.04757 	 0.00103 	 m..s
   74 	    64 	 0.05212 	 0.00103 	 m..s
   46 	    65 	 0.04852 	 0.00112 	 m..s
   87 	    66 	 0.05461 	 0.00114 	 m..s
   78 	    67 	 0.05268 	 0.00114 	 m..s
   14 	    68 	 0.04590 	 0.00115 	 m..s
   20 	    69 	 0.04635 	 0.00116 	 m..s
   50 	    70 	 0.04882 	 0.00129 	 m..s
   92 	    71 	 0.05499 	 0.00130 	 m..s
   60 	    72 	 0.04938 	 0.00135 	 m..s
   13 	    73 	 0.04553 	 0.00146 	 m..s
   72 	    74 	 0.05161 	 0.00165 	 m..s
   14 	    75 	 0.04590 	 0.00165 	 m..s
   73 	    76 	 0.05170 	 0.00169 	 m..s
   38 	    77 	 0.04781 	 0.00172 	 m..s
   19 	    78 	 0.04624 	 0.00174 	 m..s
   25 	    79 	 0.04671 	 0.00184 	 m..s
   33 	    80 	 0.04742 	 0.00189 	 m..s
   37 	    81 	 0.04777 	 0.00191 	 m..s
   68 	    82 	 0.05074 	 0.00193 	 m..s
   25 	    83 	 0.04671 	 0.00206 	 m..s
   17 	    84 	 0.04594 	 0.00207 	 m..s
   27 	    85 	 0.04686 	 0.00229 	 m..s
   79 	    86 	 0.05311 	 0.00364 	 m..s
  105 	    87 	 0.07328 	 0.00581 	 m..s
  106 	    88 	 0.07338 	 0.00638 	 m..s
  117 	    89 	 0.25701 	 0.01254 	 MISS
   97 	    90 	 0.05734 	 0.05308 	 ~...
   95 	    91 	 0.05691 	 0.05495 	 ~...
  108 	    92 	 0.09191 	 0.05624 	 m..s
   93 	    93 	 0.05672 	 0.05886 	 ~...
   56 	    94 	 0.04912 	 0.06558 	 ~...
   71 	    95 	 0.05152 	 0.06676 	 ~...
   48 	    96 	 0.04856 	 0.06712 	 ~...
   96 	    97 	 0.05717 	 0.07240 	 ~...
   94 	    98 	 0.05674 	 0.07421 	 ~...
   43 	    99 	 0.04837 	 0.07514 	 ~...
   63 	   100 	 0.04964 	 0.07611 	 ~...
   75 	   101 	 0.05215 	 0.07648 	 ~...
  107 	   102 	 0.08859 	 0.07963 	 ~...
   98 	   103 	 0.05761 	 0.08410 	 ~...
  101 	   104 	 0.06457 	 0.10378 	 m..s
  100 	   105 	 0.06398 	 0.10698 	 m..s
  112 	   106 	 0.12512 	 0.11871 	 ~...
  110 	   107 	 0.12508 	 0.12494 	 ~...
  110 	   108 	 0.12508 	 0.12857 	 ~...
  109 	   109 	 0.10172 	 0.12958 	 ~...
  113 	   110 	 0.12522 	 0.13087 	 ~...
  103 	   111 	 0.06671 	 0.13372 	 m..s
  114 	   112 	 0.12523 	 0.13560 	 ~...
   99 	   113 	 0.06211 	 0.14133 	 m..s
  104 	   114 	 0.06777 	 0.14416 	 m..s
  116 	   115 	 0.25328 	 0.18508 	 m..s
  102 	   116 	 0.06510 	 0.21233 	 MISS
  115 	   117 	 0.24661 	 0.29363 	 m..s
  120 	   118 	 0.27028 	 0.29526 	 ~...
  119 	   119 	 0.27015 	 0.29859 	 ~...
  117 	   120 	 0.25701 	 0.34919 	 m..s
==========================================
r_mrr = 0.7965563535690308
r2_mrr = 0.41564685106277466
spearmanr_mrr@5 = 0.7533038854598999
spearmanr_mrr@10 = 0.8772631287574768
spearmanr_mrr@50 = 0.9192847609519958
spearmanr_mrr@100 = 0.9263429641723633
spearmanr_mrr@All = 0.929065465927124
==========================================
test time: 0.457
Done Testing dataset OpenEA
total time taken: 253.2616982460022
training time taken: 226.17351293563843
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.7966)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.4156)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.7533)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.8773)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9193)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9263)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9291)}}, 'test_loss': {'ComplEx': {'OpenEA': 1.0842553629495342}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 8943215232102266
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1048, 1068, 1012, 496, 142, 80, 1104, 306, 226, 4, 799, 873, 84, 613, 1108, 55, 237, 83, 937, 738, 382, 1013, 296, 1161, 1061, 827, 1020, 479, 1030, 38, 90, 953, 797, 1203, 1034, 86, 133, 275, 268, 18, 1112, 831, 978, 821, 701, 312, 396, 12, 244, 230, 228, 642, 639, 885, 210, 984, 598, 49, 924, 121, 472, 119, 166, 505, 888, 1186, 176, 510, 1005, 656, 747, 155, 657, 162, 285, 617, 968, 484, 658, 489, 1166, 167, 1199, 838, 1143, 906, 216, 565, 494, 264, 1006, 1167, 135, 940, 7, 693, 486, 516, 807, 82, 542, 1040, 1037, 1052, 463, 1025, 746, 87, 735, 644, 1043, 626, 820, 730, 962, 1152, 409, 485, 1036, 917, 520]
valid_ids (0): []
train_ids (1094): [1007, 459, 1131, 305, 803, 804, 1063, 708, 1154, 1059, 842, 321, 1057, 234, 384, 905, 932, 106, 192, 1195, 32, 352, 614, 28, 127, 1046, 1078, 1111, 1180, 722, 488, 893, 497, 548, 713, 531, 1162, 871, 220, 1130, 91, 326, 99, 37, 1003, 443, 622, 624, 1023, 1065, 1120, 538, 118, 575, 1049, 883, 270, 715, 332, 233, 360, 442, 290, 76, 1002, 94, 753, 774, 79, 736, 213, 303, 519, 635, 0, 467, 178, 1188, 594, 462, 665, 1200, 775, 853, 951, 277, 828, 407, 1055, 802, 822, 1145, 426, 338, 660, 209, 530, 766, 1075, 1213, 574, 661, 963, 196, 337, 999, 131, 517, 336, 1193, 198, 362, 819, 1032, 100, 421, 719, 381, 535, 707, 501, 864, 895, 779, 177, 1054, 1207, 731, 525, 374, 544, 11, 325, 588, 65, 1092, 441, 430, 255, 1122, 30, 51, 1132, 604, 982, 621, 111, 95, 359, 239, 279, 1102, 561, 206, 385, 278, 1087, 638, 868, 700, 221, 683, 493, 651, 712, 414, 1156, 633, 972, 444, 402, 1026, 322, 636, 343, 720, 699, 431, 449, 840, 907, 58, 959, 768, 1045, 1204, 834, 474, 481, 53, 438, 1093, 1202, 796, 559, 1079, 364, 299, 434, 181, 23, 33, 781, 291, 283, 1191, 369, 584, 50, 218, 896, 1080, 227, 714, 350, 579, 400, 539, 97, 455, 85, 767, 954, 592, 541, 132, 750, 1, 199, 1085, 928, 1010, 377, 760, 180, 846, 787, 1201, 345, 1101, 1123, 667, 1174, 881, 17, 418, 757, 550, 504, 1091, 41, 892, 625, 25, 814, 1171, 419, 568, 911, 174, 1008, 785, 253, 1135, 694, 734, 356, 808, 1113, 789, 380, 1069, 589, 26, 406, 717, 778, 543, 829, 946, 355, 487, 113, 772, 643, 901, 903, 507, 263, 784, 690, 294, 971, 1173, 465, 231, 791, 521, 666, 1185, 36, 70, 692, 859, 123, 764, 894, 1172, 733, 185, 511, 897, 1077, 931, 391, 183, 751, 632, 117, 967, 170, 351, 1121, 383, 242, 1160, 677, 1175, 96, 1000, 674, 71, 1165, 347, 495, 593, 47, 1138, 62, 865, 482, 891, 756, 282, 152, 556, 211, 801, 102, 393, 115, 1021, 236, 532, 925, 59, 267, 841, 995, 104, 606, 970, 786, 798, 920, 620, 318, 52, 655, 728, 109, 609, 31, 1070, 882, 762, 450, 718, 676, 42, 288, 116, 250, 172, 836, 794, 949, 673, 314, 957, 223, 1129, 600, 912, 1164, 653, 1190, 522, 468, 1117, 114, 1194, 397, 145, 195, 862, 705, 867, 1009, 1187, 998, 46, 861, 508, 456, 358, 562, 184, 1004, 502, 1126, 551, 696, 1110, 554, 1184, 723, 628, 850, 825, 150, 961, 182, 1056, 466, 378, 256, 761, 1144, 165, 514, 874, 14, 63, 448, 533, 57, 153, 191, 260, 317, 737, 988, 523, 222, 390, 366, 682, 1064, 35, 73, 103, 681, 680, 577, 637, 134, 981, 387, 404, 721, 1103, 179, 1197, 685, 729, 289, 440, 432, 711, 411, 599, 149, 405, 884, 899, 190, 156, 433, 24, 607, 413, 204, 6, 212, 476, 765, 460, 706, 710, 194, 478, 395, 500, 560, 975, 889, 420, 1146, 458, 688, 107, 1050, 410, 408, 368, 1125, 547, 93, 439, 950, 399, 566, 980, 668, 740, 641, 146, 371, 812, 994, 852, 754, 866, 445, 926, 569, 748, 663, 634, 265, 157, 608, 858, 938, 1119, 1017, 1105, 331, 1139, 545, 1137, 66, 704, 847, 996, 349, 886, 365, 246, 716, 851, 1019, 136, 993, 943, 973, 702, 1183, 72, 528, 1076, 346, 1149, 983, 175, 88, 375, 1029, 755, 571, 247, 379, 776, 1095, 403, 258, 54, 936, 672, 709, 126, 143, 354, 570, 168, 208, 526, 363, 125, 1086, 915, 1016, 670, 171, 913, 469, 743, 164, 424, 647, 910, 373, 394, 724, 537, 1018, 138, 534, 45, 1177, 428, 811, 1060, 215, 447, 876, 1134, 266, 1053, 1163, 1044, 1028, 140, 939, 415, 572, 631, 902, 583, 969, 339, 1205, 1155, 646, 1074, 16, 344, 159, 74, 947, 112, 29, 1192, 879, 64, 612, 650, 348, 471, 13, 1157, 147, 129, 898, 238, 887, 1153, 202, 353, 506, 890, 578, 1118, 48, 1039, 576, 923, 948, 823, 1210, 284, 752, 652, 933, 1031, 934, 997, 503, 837, 451, 669, 271, 68, 161, 989, 619, 726, 813, 662, 582, 334, 308, 311, 1170, 108, 742, 872, 992, 990, 141, 67, 1116, 262, 40, 857, 122, 130, 557, 1182, 780, 187, 137, 1098, 1176, 1088, 596, 540, 173, 1022, 75, 914, 214, 78, 437, 1107, 464, 985, 77, 788, 844, 679, 329, 909, 916, 935, 1011, 1066, 965, 1089, 225, 512, 461, 5, 800, 56, 269, 741, 217, 310, 1099, 816, 515, 361, 292, 691, 684, 602, 388, 848, 229, 330, 1128, 877, 158, 645, 333, 3, 128, 429, 203, 648, 616, 454, 281, 945, 1096, 453, 900, 1179, 880, 553, 795, 34, 1109, 1001, 782, 1090, 860, 315, 929, 309, 1124, 524, 695, 1051, 219, 328, 659, 110, 991, 154, 790, 372, 398, 297, 603, 1140, 490, 1209, 151, 19, 295, 739, 1208, 197, 81, 640, 629, 649, 623, 9, 323, 878, 327, 697, 964, 758, 1206, 930, 1168, 416, 826, 769, 1115, 956, 1159, 1178, 849, 835, 986, 499, 324, 952, 839, 927, 8, 549, 342, 810, 870, 1094, 615, 245, 987, 200, 189, 427, 921, 105, 703, 139, 955, 186, 316, 376, 581, 918, 148, 1133, 818, 773, 357, 809, 480, 43, 483, 1035, 976, 725, 341, 386, 120, 1147, 1214, 22, 763, 272, 792, 830, 759, 770, 207, 1072, 611, 1047, 163, 425, 509, 518, 477, 919, 856, 473, 601, 273, 417, 1127, 298, 563, 1027, 261, 201, 678, 412, 44, 1151, 783, 144, 276, 513, 590, 340, 960, 302, 287, 254, 1073, 10, 1084, 942, 875, 301, 567, 1097, 98, 205, 843, 470, 1083, 664, 39, 1081, 529, 855, 689, 232, 854, 675, 304, 1038, 1014, 370, 1181, 1189, 313, 727, 597, 806, 1196, 1158, 1033, 280, 744, 536, 686, 160, 452, 241, 1141, 498, 558, 591, 817, 815, 491, 89, 698, 1100, 1150, 587, 527, 60, 286, 1106, 69, 805, 966, 446, 20, 457, 1148, 492, 193, 319, 15, 618, 586, 274, 555, 1136, 435, 908, 293, 941, 124, 21, 552, 61, 423, 845, 654, 101, 2, 1114, 1041, 904, 422, 595, 564, 1211, 367, 958, 610, 188, 732, 252, 824, 580, 248, 257, 1142, 863, 401, 251, 240, 1015, 1062, 793, 475, 389, 687, 1169, 1042, 259, 605, 944, 224, 436, 546, 27, 335, 243, 300, 92, 979, 671, 392, 627, 585, 777, 832, 573, 1198, 1212, 320, 1067, 833, 1071, 235, 1058, 1024, 1082, 749, 974, 745, 249, 977, 922, 169, 869, 630, 771, 307]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6536493115483800
the save name prefix for this run is:  chkpt-ID_6536493115483800_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 564
rank avg (pred): 0.481 +- 0.008
mrr vals (pred, true): 0.000, 0.047
batch losses (mrrl, rdl): 0.0, 0.0003631922

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 407
rank avg (pred): 0.393 +- 0.256
mrr vals (pred, true): 0.169, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001141287

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 864
rank avg (pred): 0.423 +- 0.305
mrr vals (pred, true): 0.226, 0.001
batch losses (mrrl, rdl): 0.0, 3.93868e-05

Epoch over!
epoch time: 15.04

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 977
rank avg (pred): 0.376 +- 0.285
mrr vals (pred, true): 0.246, 0.133
batch losses (mrrl, rdl): 0.0, 0.0002177974

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 596
rank avg (pred): 0.401 +- 0.301
mrr vals (pred, true): 0.252, 0.001
batch losses (mrrl, rdl): 0.0, 6.77635e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 30
rank avg (pred): 0.390 +- 0.298
mrr vals (pred, true): 0.268, 0.079
batch losses (mrrl, rdl): 0.0, 9.36009e-05

Epoch over!
epoch time: 14.993

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 772
rank avg (pred): 0.398 +- 0.301
mrr vals (pred, true): 0.266, 0.001
batch losses (mrrl, rdl): 0.0, 9.04113e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 461
rank avg (pred): 0.420 +- 0.312
mrr vals (pred, true): 0.249, 0.001
batch losses (mrrl, rdl): 0.0, 3.41512e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 840
rank avg (pred): 0.372 +- 0.288
mrr vals (pred, true): 0.278, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001583795

Epoch over!
epoch time: 15.068

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 10
rank avg (pred): 0.388 +- 0.291
mrr vals (pred, true): 0.255, 0.266
batch losses (mrrl, rdl): 0.0, 0.0014037113

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 0
rank avg (pred): 0.395 +- 0.296
mrr vals (pred, true): 0.244, 0.072
batch losses (mrrl, rdl): 0.0, 0.0003346329

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.409 +- 0.307
mrr vals (pred, true): 0.264, 0.001
batch losses (mrrl, rdl): 0.0, 5.99044e-05

Epoch over!
epoch time: 14.957

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 864
rank avg (pred): 0.402 +- 0.293
mrr vals (pred, true): 0.230, 0.001
batch losses (mrrl, rdl): 0.0, 8.02755e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1125
rank avg (pred): 0.390 +- 0.304
mrr vals (pred, true): 0.286, 0.001
batch losses (mrrl, rdl): 0.0, 9.60313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 839
rank avg (pred): 0.371 +- 0.289
mrr vals (pred, true): 0.280, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001556645

Epoch over!
epoch time: 14.946

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 913
rank avg (pred): 0.474 +- 0.325
mrr vals (pred, true): 0.241, 0.002
batch losses (mrrl, rdl): 0.3650980592, 0.0001023571

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 570
rank avg (pred): 0.342 +- 0.231
mrr vals (pred, true): 0.092, 0.001
batch losses (mrrl, rdl): 0.0175189674, 0.0003761533

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 160
rank avg (pred): 0.439 +- 0.234
mrr vals (pred, true): 0.064, 0.002
batch losses (mrrl, rdl): 0.0020035338, 4.84598e-05

Epoch over!
epoch time: 15.118

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 620
rank avg (pred): 0.421 +- 0.270
mrr vals (pred, true): 0.064, 0.002
batch losses (mrrl, rdl): 0.0020032176, 6.0451e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 788
rank avg (pred): 0.383 +- 0.233
mrr vals (pred, true): 0.087, 0.001
batch losses (mrrl, rdl): 0.014036091, 0.0002053147

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 281
rank avg (pred): 0.431 +- 0.311
mrr vals (pred, true): 0.051, 0.088
batch losses (mrrl, rdl): 1.49226e-05, 0.0001496886

Epoch over!
epoch time: 15.107

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.383 +- 0.266
mrr vals (pred, true): 0.063, 0.001
batch losses (mrrl, rdl): 0.0017136516, 0.0001901922

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 568
rank avg (pred): 0.424 +- 0.282
mrr vals (pred, true): 0.064, 0.001
batch losses (mrrl, rdl): 0.001843939, 6.29259e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 221
rank avg (pred): 0.408 +- 0.299
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.1549e-06, 0.0001106687

Epoch over!
epoch time: 15.116

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 683
rank avg (pred): 0.475 +- 0.291
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004458937, 9.4225e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 14
rank avg (pred): 0.339 +- 0.234
mrr vals (pred, true): 0.117, 0.271
batch losses (mrrl, rdl): 0.2367251515, 0.0006365819

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 207
rank avg (pred): 0.432 +- 0.277
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0008064486, 4.82264e-05

Epoch over!
epoch time: 15.117

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 566
rank avg (pred): 0.396 +- 0.283
mrr vals (pred, true): 0.054, 0.056
batch losses (mrrl, rdl): 0.0001841471, 4.08446e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 725
rank avg (pred): 0.461 +- 0.302
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001111464, 1.75091e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 332
rank avg (pred): 0.400 +- 0.231
mrr vals (pred, true): 0.082, 0.001
batch losses (mrrl, rdl): 0.0100536365, 0.0001425289

Epoch over!
epoch time: 15.189

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 871
rank avg (pred): 0.406 +- 0.249
mrr vals (pred, true): 0.084, 0.001
batch losses (mrrl, rdl): 0.0118291797, 9.62836e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1091
rank avg (pred): 0.460 +- 0.307
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.20424e-05, 2.19864e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 616
rank avg (pred): 0.474 +- 0.303
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004882751, 1.04144e-05

Epoch over!
epoch time: 15.255

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 664
rank avg (pred): 0.430 +- 0.278
mrr vals (pred, true): 0.068, 0.001
batch losses (mrrl, rdl): 0.0032838606, 5.87426e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 707
rank avg (pred): 0.403 +- 0.314
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.58804e-05, 0.0001019525

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 825
rank avg (pred): 0.425 +- 0.258
mrr vals (pred, true): 0.099, 0.216
batch losses (mrrl, rdl): 0.1368743479, 0.0010206797

Epoch over!
epoch time: 15.281

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 587
rank avg (pred): 0.316 +- 0.270
mrr vals (pred, true): 0.100, 0.001
batch losses (mrrl, rdl): 0.0245654956, 0.000417586

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.354 +- 0.251
mrr vals (pred, true): 0.113, 0.171
batch losses (mrrl, rdl): 0.0329825282, 0.0001834301

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1126
rank avg (pred): 0.429 +- 0.334
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0021676626, 6.50582e-05

Epoch over!
epoch time: 15.159

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 201
rank avg (pred): 0.435 +- 0.319
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001472349, 5.75423e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 293
rank avg (pred): 0.402 +- 0.311
mrr vals (pred, true): 0.052, 0.079
batch losses (mrrl, rdl): 3.2089e-05, 9.71931e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 483
rank avg (pred): 0.446 +- 0.293
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003371157, 3.21279e-05

Epoch over!
epoch time: 15.133

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1176
rank avg (pred): 0.420 +- 0.293
mrr vals (pred, true): 0.070, 0.001
batch losses (mrrl, rdl): 0.0040049842, 5.14612e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 604
rank avg (pred): 0.461 +- 0.292
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002541329, 1.51359e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 397
rank avg (pred): 0.443 +- 0.297
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003230061, 2.98562e-05

Epoch over!
epoch time: 15.14

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.448 +- 0.326
mrr vals (pred, true): 0.059, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   60 	     0 	 0.06329 	 0.00038 	 m..s
   53 	     1 	 0.05869 	 0.00038 	 m..s
   48 	     2 	 0.05647 	 0.00047 	 m..s
   73 	     3 	 0.07823 	 0.00047 	 m..s
   42 	     4 	 0.05510 	 0.00048 	 m..s
   21 	     5 	 0.05204 	 0.00049 	 m..s
  105 	     6 	 0.09195 	 0.00050 	 m..s
   81 	     7 	 0.08317 	 0.00053 	 m..s
   33 	     8 	 0.05411 	 0.00053 	 m..s
   66 	     9 	 0.07158 	 0.00053 	 m..s
   38 	    10 	 0.05471 	 0.00054 	 m..s
   13 	    11 	 0.04995 	 0.00054 	 m..s
   90 	    12 	 0.08740 	 0.00054 	 m..s
   47 	    13 	 0.05638 	 0.00055 	 m..s
   15 	    14 	 0.05065 	 0.00055 	 m..s
    9 	    15 	 0.04903 	 0.00056 	 m..s
   77 	    16 	 0.08262 	 0.00056 	 m..s
   16 	    17 	 0.05099 	 0.00056 	 m..s
   49 	    18 	 0.05673 	 0.00056 	 m..s
   56 	    19 	 0.06076 	 0.00057 	 m..s
   87 	    20 	 0.08526 	 0.00057 	 m..s
  113 	    21 	 0.11674 	 0.00060 	 MISS
   31 	    22 	 0.05392 	 0.00061 	 m..s
   85 	    23 	 0.08402 	 0.00063 	 m..s
    6 	    24 	 0.04840 	 0.00066 	 m..s
  110 	    25 	 0.10186 	 0.00066 	 MISS
   34 	    26 	 0.05436 	 0.00066 	 m..s
   67 	    27 	 0.07393 	 0.00068 	 m..s
  107 	    28 	 0.09266 	 0.00068 	 m..s
   30 	    29 	 0.05345 	 0.00068 	 m..s
   69 	    30 	 0.07452 	 0.00070 	 m..s
   28 	    31 	 0.05309 	 0.00073 	 m..s
   63 	    32 	 0.06843 	 0.00073 	 m..s
  103 	    33 	 0.09145 	 0.00076 	 m..s
   23 	    34 	 0.05246 	 0.00077 	 m..s
   57 	    35 	 0.06138 	 0.00077 	 m..s
   24 	    36 	 0.05259 	 0.00077 	 m..s
   72 	    37 	 0.07820 	 0.00078 	 m..s
   12 	    38 	 0.04949 	 0.00078 	 m..s
   52 	    39 	 0.05849 	 0.00081 	 m..s
  100 	    40 	 0.08950 	 0.00082 	 m..s
    6 	    41 	 0.04840 	 0.00083 	 m..s
   98 	    42 	 0.08792 	 0.00084 	 m..s
  117 	    43 	 0.11771 	 0.00085 	 MISS
   94 	    44 	 0.08760 	 0.00085 	 m..s
   92 	    45 	 0.08754 	 0.00088 	 m..s
   82 	    46 	 0.08327 	 0.00089 	 m..s
   27 	    47 	 0.05308 	 0.00090 	 m..s
   54 	    48 	 0.05887 	 0.00094 	 m..s
   29 	    49 	 0.05319 	 0.00095 	 m..s
   59 	    50 	 0.06310 	 0.00095 	 m..s
   17 	    51 	 0.05132 	 0.00095 	 m..s
  102 	    52 	 0.09041 	 0.00103 	 m..s
   40 	    53 	 0.05503 	 0.00108 	 m..s
    3 	    54 	 0.04466 	 0.00115 	 m..s
   35 	    55 	 0.05441 	 0.00115 	 m..s
   65 	    56 	 0.06870 	 0.00122 	 m..s
   45 	    57 	 0.05582 	 0.00126 	 m..s
   10 	    58 	 0.04911 	 0.00130 	 m..s
   13 	    59 	 0.04995 	 0.00132 	 m..s
   89 	    60 	 0.08707 	 0.00133 	 m..s
   44 	    61 	 0.05553 	 0.00134 	 m..s
   71 	    62 	 0.07790 	 0.00135 	 m..s
   43 	    63 	 0.05543 	 0.00136 	 m..s
   98 	    64 	 0.08792 	 0.00144 	 m..s
  108 	    65 	 0.10102 	 0.00146 	 m..s
   57 	    66 	 0.06138 	 0.00146 	 m..s
   97 	    67 	 0.08787 	 0.00153 	 m..s
   50 	    68 	 0.05816 	 0.00165 	 m..s
   93 	    69 	 0.08756 	 0.00166 	 m..s
   77 	    70 	 0.08262 	 0.00167 	 m..s
    0 	    71 	 0.04350 	 0.00170 	 m..s
   36 	    72 	 0.05447 	 0.00176 	 m..s
   91 	    73 	 0.08744 	 0.00179 	 m..s
   25 	    74 	 0.05270 	 0.00186 	 m..s
   41 	    75 	 0.05506 	 0.00229 	 m..s
   74 	    76 	 0.07863 	 0.00239 	 m..s
  119 	    77 	 0.11794 	 0.00245 	 MISS
   55 	    78 	 0.05919 	 0.00268 	 m..s
  118 	    79 	 0.11788 	 0.00350 	 MISS
  101 	    80 	 0.08972 	 0.01790 	 m..s
   62 	    81 	 0.06402 	 0.05180 	 ~...
   22 	    82 	 0.05226 	 0.05524 	 ~...
   61 	    83 	 0.06359 	 0.05624 	 ~...
    8 	    84 	 0.04882 	 0.06998 	 ~...
    5 	    85 	 0.04581 	 0.07085 	 ~...
    4 	    86 	 0.04515 	 0.07487 	 ~...
    1 	    87 	 0.04356 	 0.07609 	 m..s
   18 	    88 	 0.05144 	 0.07709 	 ~...
   20 	    89 	 0.05184 	 0.07885 	 ~...
   26 	    90 	 0.05282 	 0.07893 	 ~...
   46 	    91 	 0.05588 	 0.07945 	 ~...
   19 	    92 	 0.05165 	 0.07981 	 ~...
   37 	    93 	 0.05469 	 0.08157 	 ~...
    2 	    94 	 0.04441 	 0.08220 	 m..s
   39 	    95 	 0.05494 	 0.08269 	 ~...
   32 	    96 	 0.05393 	 0.08565 	 m..s
   10 	    97 	 0.04911 	 0.08956 	 m..s
   96 	    98 	 0.08770 	 0.09414 	 ~...
   94 	    99 	 0.08760 	 0.09679 	 ~...
   63 	   100 	 0.06843 	 0.09711 	 ~...
   51 	   101 	 0.05847 	 0.10378 	 m..s
   88 	   102 	 0.08704 	 0.10882 	 ~...
   76 	   103 	 0.08125 	 0.12925 	 m..s
   83 	   104 	 0.08343 	 0.13377 	 m..s
   75 	   105 	 0.07920 	 0.13560 	 m..s
   79 	   106 	 0.08286 	 0.13818 	 m..s
   80 	   107 	 0.08311 	 0.13862 	 m..s
  113 	   108 	 0.11674 	 0.16228 	 m..s
  119 	   109 	 0.11794 	 0.16486 	 m..s
  103 	   110 	 0.09145 	 0.16553 	 m..s
   67 	   111 	 0.07393 	 0.17901 	 MISS
   70 	   112 	 0.07627 	 0.20773 	 MISS
   86 	   113 	 0.08518 	 0.21233 	 MISS
   84 	   114 	 0.08382 	 0.21589 	 MISS
  106 	   115 	 0.09257 	 0.21998 	 MISS
  109 	   116 	 0.10108 	 0.24378 	 MISS
  113 	   117 	 0.11674 	 0.25409 	 MISS
  112 	   118 	 0.11654 	 0.25792 	 MISS
  113 	   119 	 0.11674 	 0.28260 	 MISS
  111 	   120 	 0.11303 	 0.31921 	 MISS
==========================================
r_mrr = 0.3934849202632904
r2_mrr = 0.017274141311645508
spearmanr_mrr@5 = 0.6139386296272278
spearmanr_mrr@10 = 0.7736902832984924
spearmanr_mrr@50 = 0.9533176422119141
spearmanr_mrr@100 = 0.9045510292053223
spearmanr_mrr@All = 0.889940619468689
==========================================
test time: 0.452
Done Testing dataset OpenEA
total time taken: 253.87190175056458
training time taken: 227.08212208747864
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.3935)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.0173)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.6139)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.7737)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9533)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9046)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.8899)}}, 'test_loss': {'ComplEx': {'OpenEA': 3.004511543584158}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 8649224008780068
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [944, 1089, 595, 390, 42, 649, 1207, 196, 515, 280, 768, 566, 484, 377, 696, 587, 1112, 1182, 1162, 1206, 347, 834, 364, 109, 983, 1023, 873, 557, 205, 240, 769, 1148, 640, 313, 780, 69, 1046, 284, 739, 679, 968, 841, 363, 234, 705, 775, 389, 576, 1075, 92, 233, 692, 228, 934, 898, 1141, 79, 182, 247, 1170, 597, 1122, 187, 1103, 999, 637, 848, 1050, 671, 41, 810, 1131, 645, 1150, 1078, 74, 37, 307, 3, 132, 817, 517, 842, 475, 1077, 913, 266, 838, 615, 106, 844, 967, 55, 312, 948, 904, 612, 231, 851, 641, 631, 764, 700, 716, 1049, 93, 894, 489, 925, 202, 1011, 658, 404, 620, 90, 607, 1191, 1124, 388, 476, 682]
valid_ids (0): []
train_ids (1094): [1203, 467, 334, 384, 580, 1095, 431, 353, 103, 283, 878, 57, 808, 602, 984, 865, 112, 1054, 168, 559, 793, 354, 500, 1094, 448, 456, 748, 308, 26, 507, 603, 840, 1096, 606, 117, 110, 237, 752, 830, 754, 585, 1193, 221, 563, 327, 734, 323, 293, 1066, 571, 1187, 742, 573, 674, 446, 230, 211, 1137, 1022, 1045, 782, 1163, 397, 115, 497, 877, 1083, 723, 922, 625, 963, 341, 1130, 525, 63, 686, 1149, 708, 709, 857, 905, 181, 490, 596, 920, 67, 383, 1060, 744, 798, 733, 1038, 111, 164, 1195, 14, 568, 550, 369, 335, 47, 521, 959, 660, 1012, 1169, 1108, 1192, 36, 470, 78, 1210, 81, 766, 207, 940, 486, 1194, 462, 243, 303, 1025, 324, 522, 320, 281, 439, 755, 222, 249, 860, 460, 425, 670, 888, 778, 264, 31, 511, 1073, 896, 1009, 415, 1070, 893, 1074, 491, 368, 1000, 217, 609, 836, 342, 652, 590, 238, 608, 478, 1185, 1111, 1105, 1015, 185, 884, 1091, 929, 84, 482, 450, 188, 720, 169, 150, 676, 982, 560, 1040, 627, 474, 819, 909, 882, 891, 794, 972, 965, 455, 405, 229, 866, 886, 427, 96, 361, 738, 225, 371, 424, 911, 776, 139, 166, 728, 367, 1199, 711, 788, 813, 214, 235, 91, 971, 492, 178, 773, 97, 791, 897, 287, 680, 1209, 402, 195, 310, 916, 72, 862, 823, 206, 879, 105, 444, 758, 1005, 158, 663, 726, 138, 691, 378, 694, 732, 128, 296, 21, 39, 787, 1160, 902, 989, 693, 80, 129, 531, 756, 582, 814, 820, 907, 262, 95, 970, 518, 969, 535, 272, 688, 433, 359, 1063, 703, 487, 642, 116, 58, 459, 120, 43, 113, 339, 1202, 761, 466, 1175, 244, 942, 1064, 19, 956, 662, 173, 1153, 108, 547, 251, 1028, 1177, 1126, 365, 509, 689, 978, 70, 148, 976, 118, 863, 122, 928, 1084, 99, 328, 953, 825, 549, 245, 598, 858, 849, 604, 1183, 1110, 165, 410, 100, 301, 561, 23, 512, 88, 1115, 765, 643, 73, 85, 736, 1180, 1161, 1188, 1184, 721, 695, 375, 917, 151, 623, 393, 1020, 534, 687, 685, 48, 322, 161, 937, 1013, 938, 520, 458, 387, 83, 56, 827, 713, 815, 29, 380, 204, 346, 718, 599, 437, 505, 629, 803, 302, 852, 1156, 741, 993, 605, 762, 174, 297, 579, 870, 943, 781, 628, 1007, 149, 701, 480, 140, 386, 853, 1057, 1154, 826, 831, 892, 715, 1164, 316, 868, 610, 331, 519, 213, 17, 880, 408, 1113, 1138, 481, 918, 20, 786, 259, 574, 276, 406, 15, 528, 1098, 179, 900, 1143, 832, 632, 44, 381, 1068, 966, 921, 1076, 50, 1213, 554, 919, 89, 270, 294, 667, 600, 472, 1071, 382, 309, 385, 1142, 1065, 1168, 923, 634, 1003, 184, 876, 1147, 747, 1128, 792, 170, 783, 398, 273, 952, 453, 946, 6, 1, 871, 532, 7, 552, 22, 134, 413, 958, 4, 1114, 1006, 1106, 1031, 49, 40, 278, 1211, 936, 931, 10, 712, 160, 1198, 617, 477, 318, 68, 469, 275, 30, 583, 290, 445, 355, 556, 332, 947, 435, 86, 1109, 45, 414, 60, 565, 1146, 291, 451, 465, 949, 881, 153, 906, 197, 760, 508, 1008, 1041, 344, 699, 1086, 855, 1033, 219, 770, 910, 636, 990, 471, 570, 669, 422, 429, 960, 601, 421, 653, 510, 730, 821, 927, 802, 659, 325, 975, 1201, 1061, 423, 1181, 933, 553, 194, 1107, 1117, 254, 357, 434, 32, 856, 449, 621, 416, 321, 227, 763, 183, 1173, 704, 403, 533, 717, 572, 145, 767, 1082, 1048, 499, 805, 498, 279, 784, 1099, 666, 1059, 0, 1152, 156, 209, 1055, 816, 930, 1030, 447, 282, 76, 980, 883, 162, 504, 119, 804, 33, 678, 167, 1212, 1190, 986, 376, 874, 11, 530, 1035, 647, 981, 955, 1039, 286, 1140, 267, 53, 991, 198, 845, 27, 724, 176, 1090, 1085, 330, 13, 630, 65, 1016, 352, 241, 391, 277, 903, 172, 319, 208, 51, 1021, 562, 463, 690, 295, 239, 646, 257, 861, 996, 1166, 464, 396, 624, 524, 224, 189, 885, 616, 345, 326, 289, 785, 260, 746, 300, 232, 1136, 985, 1157, 141, 1024, 593, 314, 664, 443, 180, 25, 697, 974, 255, 87, 142, 672, 650, 226, 835, 157, 250, 454, 1018, 348, 154, 581, 992, 797, 638, 75, 253, 268, 311, 867, 722, 392, 1139, 542, 926, 588, 263, 789, 483, 1134, 1127, 1056, 473, 774, 411, 35, 751, 107, 432, 753, 159, 513, 932, 875, 673, 529, 358, 9, 1125, 537, 1176, 743, 125, 1159, 1120, 997, 199, 1155, 1004, 133, 790, 190, 292, 1051, 24, 1135, 98, 5, 1123, 592, 683, 812, 379, 1072, 635, 135, 1104, 274, 757, 502, 644, 901, 555, 1034, 102, 681, 366, 759, 372, 962, 1208, 495, 941, 523, 727, 548, 258, 1119, 305, 1179, 2, 1037, 977, 661, 1133, 828, 771, 1174, 1052, 436, 419, 203, 514, 216, 218, 104, 137, 220, 236, 1043, 210, 1097, 38, 614, 987, 468, 362, 964, 847, 1116, 725, 114, 833, 503, 1027, 577, 698, 77, 401, 155, 737, 795, 1014, 527, 146, 915, 315, 144, 409, 1019, 546, 538, 558, 1214, 126, 806, 412, 945, 998, 749, 1204, 261, 796, 285, 957, 123, 200, 1165, 370, 655, 1069, 850, 54, 269, 924, 64, 488, 130, 740, 1145, 356, 1151, 418, 1017, 1081, 163, 540, 1087, 818, 1053, 622, 799, 706, 246, 594, 317, 651, 395, 1062, 611, 506, 995, 61, 545, 417, 714, 306, 543, 252, 399, 1158, 843, 526, 961, 908, 567, 299, 899, 101, 340, 811, 801, 750, 1029, 1196, 461, 338, 201, 1172, 62, 824, 152, 648, 349, 360, 452, 46, 973, 939, 177, 1100, 1079, 800, 854, 485, 677, 440, 1205, 829, 591, 864, 1186, 131, 1132, 1044, 654, 639, 979, 336, 1121, 1088, 428, 374, 869, 442, 1092, 337, 1001, 890, 1042, 59, 872, 496, 895, 304, 288, 516, 951, 719, 501, 1002, 626, 578, 575, 66, 1101, 430, 1189, 136, 988, 1058, 541, 400, 950, 256, 564, 121, 16, 589, 407, 1178, 657, 772, 1171, 889, 551, 777, 186, 1129, 668, 1026, 192, 729, 809, 242, 351, 1010, 675, 215, 420, 438, 143, 1144, 1093, 684, 1167, 613, 1197, 394, 994, 171, 618, 779, 707, 193, 954, 493, 457, 223, 586, 248, 147, 702, 731, 536, 18, 8, 656, 1032, 191, 569, 265, 887, 1067, 494, 373, 912, 822, 329, 271, 1200, 544, 12, 479, 665, 837, 1080, 1036, 584, 633, 82, 333, 94, 212, 846, 619, 350, 71, 34, 745, 441, 914, 807, 735, 127, 124, 1047, 52, 28, 426, 175, 298, 1102, 710, 839, 935, 343, 1118, 859, 539]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3889179129344006
the save name prefix for this run is:  chkpt-ID_3889179129344006_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 783
rank avg (pred): 0.547 +- 0.004
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001592552

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 903
rank avg (pred): 0.358 +- 0.224
mrr vals (pred, true): 0.096, 0.001
batch losses (mrrl, rdl): 0.0, 0.0009977841

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 33
rank avg (pred): 0.319 +- 0.228
mrr vals (pred, true): 0.130, 0.071
batch losses (mrrl, rdl): 0.0, 4.65083e-05

Epoch over!
epoch time: 14.952

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1156
rank avg (pred): 0.364 +- 0.256
mrr vals (pred, true): 0.108, 0.105
batch losses (mrrl, rdl): 0.0, 0.0001019622

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 918
rank avg (pred): 0.503 +- 0.309
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0, 2.11378e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1081
rank avg (pred): 0.469 +- 0.277
mrr vals (pred, true): 0.018, 0.002
batch losses (mrrl, rdl): 0.0, 5.7879e-06

Epoch over!
epoch time: 14.94

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 449
rank avg (pred): 0.466 +- 0.303
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 0.0, 8.5093e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 632
rank avg (pred): 0.481 +- 0.279
mrr vals (pred, true): 0.017, 0.001
batch losses (mrrl, rdl): 0.0, 4.7855e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 453
rank avg (pred): 0.478 +- 0.297
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.0, 3.4819e-06

Epoch over!
epoch time: 14.954

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 487
rank avg (pred): 0.289 +- 0.276
mrr vals (pred, true): 0.046, 0.077
batch losses (mrrl, rdl): 0.0, 5.9078e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 800
rank avg (pred): 0.500 +- 0.295
mrr vals (pred, true): 0.018, 0.001
batch losses (mrrl, rdl): 0.0, 4.9755e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 96
rank avg (pred): 0.475 +- 0.288
mrr vals (pred, true): 0.013, 0.001
batch losses (mrrl, rdl): 0.0, 1.9317e-06

Epoch over!
epoch time: 14.947

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 653
rank avg (pred): 0.489 +- 0.291
mrr vals (pred, true): 0.013, 0.000
batch losses (mrrl, rdl): 0.0, 1.5564e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 224
rank avg (pred): 0.476 +- 0.295
mrr vals (pred, true): 0.015, 0.001
batch losses (mrrl, rdl): 0.0, 4.1443e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.492 +- 0.286
mrr vals (pred, true): 0.008, 0.001
batch losses (mrrl, rdl): 0.0, 2.1048e-06

Epoch over!
epoch time: 14.947

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.279 +- 0.306
mrr vals (pred, true): 0.043, 0.171
batch losses (mrrl, rdl): 0.164049089, 9.6028e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 311
rank avg (pred): 0.339 +- 0.241
mrr vals (pred, true): 0.074, 0.079
batch losses (mrrl, rdl): 0.0056204968, 3.11449e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 966
rank avg (pred): 0.443 +- 0.320
mrr vals (pred, true): 0.037, 0.001
batch losses (mrrl, rdl): 0.0017587857, 2.38374e-05

Epoch over!
epoch time: 15.241

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 573
rank avg (pred): 0.455 +- 0.336
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003448585, 2.22382e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 874
rank avg (pred): 0.529 +- 0.388
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 2.02018e-05, 0.0001049546

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 837
rank avg (pred): 0.560 +- 0.359
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 5.8455e-06, 0.0001147764

Epoch over!
epoch time: 15.125

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 219
rank avg (pred): 0.467 +- 0.339
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0007449085, 2.48281e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1099
rank avg (pred): 0.491 +- 0.258
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001596111, 1.25833e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 807
rank avg (pred): 0.733 +- 0.331
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.0034766826, 0.0009048335

Epoch over!
epoch time: 15.123

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.449 +- 0.313
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003223772, 1.79663e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 355
rank avg (pred): 0.488 +- 0.313
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008025225, 1.22845e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 880
rank avg (pred): 0.600 +- 0.345
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0004107348, 0.0001918552

Epoch over!
epoch time: 15.123

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 50
rank avg (pred): 0.354 +- 0.249
mrr vals (pred, true): 0.109, 0.084
batch losses (mrrl, rdl): 0.0347349197, 4.03081e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 104
rank avg (pred): 0.474 +- 0.279
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003099384, 3.6302e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 263
rank avg (pred): 0.317 +- 0.260
mrr vals (pred, true): 0.103, 0.299
batch losses (mrrl, rdl): 0.3817391992, 0.0008906875

Epoch over!
epoch time: 15.106

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.404 +- 0.291
mrr vals (pred, true): 0.130, 0.138
batch losses (mrrl, rdl): 0.0007115871, 0.0002464009

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 352
rank avg (pred): 0.426 +- 0.323
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0012709323, 4.3538e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 304
rank avg (pred): 0.253 +- 0.212
mrr vals (pred, true): 0.107, 0.067
batch losses (mrrl, rdl): 0.0324548706, 0.0002171392

Epoch over!
epoch time: 15.102

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 299
rank avg (pred): 0.239 +- 0.202
mrr vals (pred, true): 0.119, 0.065
batch losses (mrrl, rdl): 0.0480240062, 0.0002797475

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 845
rank avg (pred): 0.450 +- 0.361
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 7.6482e-06, 6.44468e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 128
rank avg (pred): 0.375 +- 0.292
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0021299114, 0.0001813126

Epoch over!
epoch time: 15.107

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1213
rank avg (pred): 0.461 +- 0.364
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002186079, 4.56624e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 871
rank avg (pred): 0.639 +- 0.376
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0015054613, 0.0003871416

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 805
rank avg (pred): 0.533 +- 0.395
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 4.0931e-06, 9.46624e-05

Epoch over!
epoch time: 15.115

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 100
rank avg (pred): 0.386 +- 0.299
mrr vals (pred, true): 0.064, 0.001
batch losses (mrrl, rdl): 0.0020559677, 0.0001140167

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 111
rank avg (pred): 0.493 +- 0.302
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001884269, 3.437e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 491
rank avg (pred): 0.326 +- 0.254
mrr vals (pred, true): 0.070, 0.097
batch losses (mrrl, rdl): 0.0038939337, 3.38289e-05

Epoch over!
epoch time: 15.109

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 899
rank avg (pred): 0.515 +- 0.397
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0027235746, 0.0016689228

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1071
rank avg (pred): 0.318 +- 0.215
mrr vals (pred, true): 0.085, 0.071
batch losses (mrrl, rdl): 0.01203172, 7.19756e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 519
rank avg (pred): 0.390 +- 0.254
mrr vals (pred, true): 0.068, 0.076
batch losses (mrrl, rdl): 0.0032827535, 5.91479e-05

Epoch over!
epoch time: 15.109

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.573 +- 0.467
mrr vals (pred, true): 0.033, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   91 	     0 	 0.06471 	 0.00020 	 m..s
   15 	     1 	 0.04485 	 0.00048 	 m..s
   73 	     2 	 0.04873 	 0.00048 	 m..s
   51 	     3 	 0.04513 	 0.00048 	 m..s
   81 	     4 	 0.05049 	 0.00049 	 m..s
   74 	     5 	 0.04906 	 0.00049 	 m..s
   15 	     6 	 0.04485 	 0.00049 	 m..s
    6 	     7 	 0.03912 	 0.00051 	 m..s
   64 	     8 	 0.04562 	 0.00051 	 m..s
   15 	     9 	 0.04485 	 0.00052 	 m..s
   15 	    10 	 0.04485 	 0.00053 	 m..s
   84 	    11 	 0.05151 	 0.00053 	 m..s
   71 	    12 	 0.04826 	 0.00053 	 m..s
   15 	    13 	 0.04485 	 0.00054 	 m..s
   14 	    14 	 0.04441 	 0.00055 	 m..s
   15 	    15 	 0.04485 	 0.00055 	 m..s
   15 	    16 	 0.04485 	 0.00056 	 m..s
    8 	    17 	 0.03927 	 0.00056 	 m..s
   82 	    18 	 0.05082 	 0.00058 	 m..s
   71 	    19 	 0.04826 	 0.00058 	 m..s
    3 	    20 	 0.03337 	 0.00058 	 m..s
    6 	    21 	 0.03912 	 0.00059 	 m..s
   77 	    22 	 0.04955 	 0.00060 	 m..s
   15 	    23 	 0.04485 	 0.00061 	 m..s
   15 	    24 	 0.04485 	 0.00062 	 m..s
   75 	    25 	 0.04932 	 0.00063 	 m..s
   58 	    26 	 0.04517 	 0.00064 	 m..s
   65 	    27 	 0.04602 	 0.00064 	 m..s
   11 	    28 	 0.04147 	 0.00065 	 m..s
   15 	    29 	 0.04485 	 0.00066 	 m..s
   15 	    30 	 0.04485 	 0.00067 	 m..s
   15 	    31 	 0.04485 	 0.00068 	 m..s
   66 	    32 	 0.04724 	 0.00068 	 m..s
    0 	    33 	 0.03316 	 0.00070 	 m..s
   79 	    34 	 0.05043 	 0.00071 	 m..s
    5 	    35 	 0.03618 	 0.00072 	 m..s
   15 	    36 	 0.04485 	 0.00072 	 m..s
    1 	    37 	 0.03318 	 0.00077 	 m..s
   15 	    38 	 0.04485 	 0.00081 	 m..s
   49 	    39 	 0.04512 	 0.00082 	 m..s
   15 	    40 	 0.04485 	 0.00084 	 m..s
   70 	    41 	 0.04801 	 0.00085 	 m..s
   79 	    42 	 0.05043 	 0.00088 	 m..s
   59 	    43 	 0.04517 	 0.00089 	 m..s
    2 	    44 	 0.03325 	 0.00090 	 m..s
   15 	    45 	 0.04485 	 0.00090 	 m..s
   67 	    46 	 0.04740 	 0.00090 	 m..s
    4 	    47 	 0.03444 	 0.00091 	 m..s
   52 	    48 	 0.04514 	 0.00091 	 m..s
   15 	    49 	 0.04485 	 0.00093 	 m..s
   15 	    50 	 0.04485 	 0.00094 	 m..s
   54 	    51 	 0.04516 	 0.00098 	 m..s
   85 	    52 	 0.05166 	 0.00099 	 m..s
   92 	    53 	 0.07308 	 0.00100 	 m..s
   53 	    54 	 0.04515 	 0.00103 	 m..s
   76 	    55 	 0.04936 	 0.00106 	 m..s
   12 	    56 	 0.04344 	 0.00108 	 m..s
   68 	    57 	 0.04779 	 0.00110 	 m..s
   68 	    58 	 0.04779 	 0.00114 	 m..s
   15 	    59 	 0.04485 	 0.00119 	 m..s
    9 	    60 	 0.03977 	 0.00121 	 m..s
   15 	    61 	 0.04485 	 0.00121 	 m..s
   15 	    62 	 0.04485 	 0.00122 	 m..s
   15 	    63 	 0.04485 	 0.00123 	 m..s
   56 	    64 	 0.04517 	 0.00123 	 m..s
   61 	    65 	 0.04521 	 0.00125 	 m..s
   15 	    66 	 0.04485 	 0.00131 	 m..s
   15 	    67 	 0.04485 	 0.00133 	 m..s
   15 	    68 	 0.04485 	 0.00135 	 m..s
   15 	    69 	 0.04485 	 0.00136 	 m..s
   13 	    70 	 0.04360 	 0.00138 	 m..s
   15 	    71 	 0.04485 	 0.00142 	 m..s
   63 	    72 	 0.04529 	 0.00144 	 m..s
   15 	    73 	 0.04485 	 0.00146 	 m..s
   62 	    74 	 0.04525 	 0.00148 	 m..s
   92 	    75 	 0.07308 	 0.00154 	 m..s
   78 	    76 	 0.05000 	 0.00156 	 m..s
   56 	    77 	 0.04517 	 0.00159 	 m..s
   15 	    78 	 0.04485 	 0.00162 	 m..s
   15 	    79 	 0.04485 	 0.00165 	 m..s
   15 	    80 	 0.04485 	 0.00172 	 m..s
   50 	    81 	 0.04513 	 0.00176 	 m..s
   55 	    82 	 0.04517 	 0.00181 	 m..s
   15 	    83 	 0.04485 	 0.00184 	 m..s
   10 	    84 	 0.04124 	 0.00191 	 m..s
   60 	    85 	 0.04518 	 0.00193 	 m..s
   15 	    86 	 0.04485 	 0.00199 	 m..s
   15 	    87 	 0.04485 	 0.00206 	 m..s
   83 	    88 	 0.05109 	 0.00248 	 m..s
  117 	    89 	 0.15167 	 0.00638 	 MISS
   95 	    90 	 0.07533 	 0.01575 	 m..s
  109 	    91 	 0.10785 	 0.05575 	 m..s
   87 	    92 	 0.05638 	 0.05624 	 ~...
  100 	    93 	 0.08258 	 0.06998 	 ~...
  106 	    94 	 0.10481 	 0.07020 	 m..s
   94 	    95 	 0.07340 	 0.07111 	 ~...
   88 	    96 	 0.05685 	 0.07464 	 ~...
   90 	    97 	 0.06167 	 0.07528 	 ~...
  103 	    98 	 0.08899 	 0.07636 	 ~...
   89 	    99 	 0.06108 	 0.07936 	 ~...
  102 	   100 	 0.08483 	 0.07981 	 ~...
  108 	   101 	 0.10636 	 0.08018 	 ~...
   97 	   102 	 0.08060 	 0.08043 	 ~...
   86 	   103 	 0.05557 	 0.08083 	 ~...
  111 	   104 	 0.11694 	 0.08188 	 m..s
  112 	   105 	 0.11919 	 0.08353 	 m..s
  106 	   106 	 0.10481 	 0.08484 	 ~...
  104 	   107 	 0.10196 	 0.08673 	 ~...
  105 	   108 	 0.10369 	 0.08743 	 ~...
   97 	   109 	 0.08060 	 0.08765 	 ~...
   99 	   110 	 0.08068 	 0.09730 	 ~...
   96 	   111 	 0.07739 	 0.10497 	 ~...
  101 	   112 	 0.08261 	 0.10850 	 ~...
  114 	   113 	 0.14173 	 0.12860 	 ~...
  113 	   114 	 0.14152 	 0.13087 	 ~...
  115 	   115 	 0.14265 	 0.13779 	 ~...
  120 	   116 	 0.19644 	 0.14416 	 m..s
  115 	   117 	 0.14265 	 0.17937 	 m..s
  118 	   118 	 0.15177 	 0.18426 	 m..s
  110 	   119 	 0.11668 	 0.29805 	 MISS
  119 	   120 	 0.19568 	 0.32345 	 MISS
==========================================
r_mrr = 0.838590145111084
r2_mrr = 0.29087793827056885
spearmanr_mrr@5 = 0.9930435419082642
spearmanr_mrr@10 = 0.9751747846603394
spearmanr_mrr@50 = 0.9643082022666931
spearmanr_mrr@100 = 0.9741501212120056
spearmanr_mrr@All = 0.9716621041297913
==========================================
test time: 0.451
Done Testing dataset OpenEA
total time taken: 252.32367300987244
training time taken: 226.46135306358337
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8386)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.2909)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9930)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9752)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9643)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9742)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9717)}}, 'test_loss': {'ComplEx': {'OpenEA': 1.0780228418698243}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 4406815375814997
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [761, 505, 562, 176, 275, 946, 1057, 65, 677, 435, 1003, 316, 629, 6, 493, 469, 1126, 973, 972, 662, 170, 1052, 547, 245, 63, 947, 260, 305, 796, 412, 873, 471, 130, 307, 1025, 88, 293, 1213, 67, 780, 958, 341, 579, 1087, 1013, 957, 552, 922, 621, 474, 857, 681, 371, 1118, 168, 504, 1082, 393, 320, 81, 620, 941, 935, 416, 915, 660, 657, 489, 464, 5, 1069, 968, 659, 672, 954, 224, 930, 1122, 34, 1092, 799, 899, 771, 337, 793, 348, 907, 249, 548, 124, 826, 1059, 154, 346, 928, 936, 1074, 196, 248, 906, 634, 481, 719, 51, 667, 312, 1047, 1184, 352, 292, 1155, 580, 1066, 46, 19, 955, 155, 298, 965, 537, 628]
valid_ids (0): []
train_ids (1094): [363, 429, 439, 769, 649, 66, 1120, 1166, 756, 752, 492, 15, 299, 171, 160, 706, 856, 134, 49, 209, 576, 1015, 453, 267, 318, 701, 656, 90, 210, 240, 705, 655, 982, 236, 456, 1199, 151, 21, 286, 809, 120, 324, 904, 676, 811, 712, 84, 38, 447, 863, 483, 0, 678, 404, 24, 496, 508, 1127, 482, 690, 526, 165, 1007, 396, 525, 45, 633, 715, 909, 1108, 1186, 114, 1170, 1147, 564, 903, 913, 1188, 700, 779, 397, 218, 709, 432, 823, 524, 1048, 441, 78, 478, 458, 843, 617, 116, 687, 1102, 1002, 1114, 201, 142, 688, 626, 446, 204, 50, 350, 250, 514, 355, 83, 306, 668, 268, 69, 1131, 152, 1138, 383, 836, 574, 1113, 1130, 1169, 644, 121, 82, 1104, 539, 786, 242, 625, 992, 635, 586, 553, 43, 994, 42, 716, 175, 686, 596, 1094, 459, 259, 444, 1124, 327, 911, 332, 815, 178, 421, 588, 842, 300, 787, 1111, 281, 797, 729, 875, 1078, 1075, 1037, 1010, 926, 816, 520, 311, 472, 1183, 859, 892, 463, 1070, 180, 379, 543, 764, 261, 732, 531, 1165, 94, 989, 422, 1099, 658, 365, 1021, 1091, 282, 96, 169, 1089, 238, 1054, 97, 174, 288, 987, 122, 889, 64, 228, 833, 960, 146, 618, 361, 362, 1152, 244, 399, 415, 713, 411, 727, 212, 1088, 473, 25, 452, 289, 74, 339, 794, 931, 841, 380, 791, 647, 751, 883, 358, 1171, 943, 87, 343, 623, 517, 369, 652, 241, 106, 101, 52, 1020, 527, 637, 516, 325, 825, 1022, 1158, 1036, 849, 3, 1204, 717, 1190, 986, 375, 1051, 840, 592, 254, 1045, 347, 129, 434, 227, 1, 1129, 486, 126, 322, 491, 17, 118, 860, 1016, 753, 185, 133, 247, 231, 874, 297, 726, 56, 378, 1205, 609, 253, 1042, 1153, 988, 1049, 419, 974, 949, 942, 30, 1145, 1001, 387, 739, 742, 758, 226, 923, 1109, 342, 448, 1163, 905, 285, 886, 290, 23, 590, 575, 683, 1062, 150, 394, 119, 179, 95, 561, 790, 140, 895, 1202, 141, 1136, 1083, 1008, 927, 788, 502, 803, 912, 865, 978, 1177, 748, 266, 445, 449, 582, 653, 530, 100, 230, 145, 1157, 1160, 512, 166, 398, 1115, 1116, 924, 798, 484, 1211, 319, 541, 725, 1096, 424, 898, 685, 692, 650, 206, 506, 772, 736, 466, 53, 565, 921, 390, 420, 59, 1105, 345, 832, 1189, 137, 1024, 814, 1072, 808, 645, 433, 349, 619, 465, 213, 567, 1060, 135, 85, 1123, 340, 409, 896, 359, 515, 536, 740, 861, 571, 568, 223, 44, 545, 741, 805, 270, 951, 746, 77, 707, 1173, 953, 829, 890, 884, 99, 559, 669, 1076, 12, 607, 894, 880, 916, 1065, 919, 1162, 551, 1207, 1140, 499, 812, 781, 197, 881, 403, 981, 1117, 377, 188, 354, 721, 125, 891, 211, 164, 893, 643, 1030, 161, 795, 1141, 792, 730, 876, 144, 1084, 388, 428, 785, 186, 670, 356, 554, 997, 112, 32, 1035, 985, 13, 71, 877, 544, 93, 522, 1133, 73, 127, 256, 374, 60, 589, 1101, 1208, 1164, 443, 996, 485, 975, 835, 674, 1156, 747, 11, 392, 866, 616, 538, 479, 631, 405, 494, 334, 869, 959, 804, 367, 1097, 593, 385, 990, 1100, 1142, 262, 837, 172, 711, 39, 333, 167, 57, 183, 901, 470, 878, 55, 979, 132, 980, 680, 689, 819, 1029, 279, 945, 1073, 540, 838, 423, 766, 182, 1139, 1014, 693, 759, 14, 651, 703, 817, 1005, 578, 386, 533, 1023, 9, 1172, 323, 274, 1154, 920, 495, 622, 854, 54, 222, 908, 939, 722, 148, 313, 535, 302, 630, 86, 743, 770, 971, 778, 871, 1004, 303, 1034, 219, 549, 31, 646, 202, 918, 1119, 400, 550, 455, 283, 738, 783, 364, 113, 1018, 1041, 220, 762, 846, 983, 1009, 1144, 440, 1110, 511, 1196, 128, 608, 1081, 239, 189, 357, 407, 143, 902, 984, 602, 47, 1159, 442, 1182, 1178, 696, 29, 910, 308, 20, 581, 4, 208, 207, 317, 115, 666, 867, 532, 572, 105, 1038, 1148, 139, 1193, 431, 569, 914, 7, 451, 627, 510, 757, 710, 1135, 271, 776, 675, 89, 566, 462, 699, 950, 818, 153, 373, 967, 138, 314, 595, 782, 521, 879, 111, 933, 425, 131, 603, 1214, 731, 1187, 682, 252, 760, 70, 612, 853, 698, 10, 92, 872, 828, 16, 488, 413, 868, 615, 583, 1080, 587, 1112, 557, 1149, 110, 370, 376, 654, 468, 372, 177, 735, 401, 845, 745, 149, 1128, 1071, 767, 601, 273, 221, 68, 336, 714, 107, 858, 763, 136, 265, 1200, 956, 1086, 27, 263, 534, 624, 414, 497, 855, 750, 187, 190, 1064, 234, 822, 704, 410, 1046, 1017, 232, 287, 599, 217, 503, 437, 246, 1012, 1137, 278, 614, 36, 807, 1077, 1168, 591, 1006, 847, 528, 870, 938, 391, 802, 203, 184, 368, 821, 1150, 594, 940, 1058, 665, 1151, 673, 691, 103, 108, 810, 1044, 467, 1212, 1180, 830, 834, 734, 944, 1174, 1028, 613, 560, 237, 330, 1167, 1103, 888, 755, 679, 610, 1146, 328, 934, 200, 461, 1203, 329, 296, 806, 156, 848, 585, 76, 784, 684, 215, 487, 917, 104, 948, 584, 258, 952, 475, 315, 885, 1192, 280, 18, 335, 887, 1206, 708, 642, 1053, 22, 661, 862, 418, 62, 162, 507, 850, 26, 1011, 326, 961, 999, 765, 40, 309, 563, 963, 600, 1085, 611, 301, 454, 205, 966, 577, 1161, 864, 546, 1055, 509, 598, 897, 460, 604, 395, 519, 80, 518, 264, 1068, 295, 33, 824, 523, 255, 638, 882, 1132, 243, 158, 664, 1195, 851, 194, 831, 269, 852, 1067, 79, 969, 75, 925, 501, 338, 1033, 389, 1056, 1098, 932, 417, 558, 820, 500, 1179, 789, 937, 498, 480, 1181, 632, 697, 195, 58, 353, 102, 35, 801, 157, 1185, 720, 477, 723, 304, 663, 733, 1191, 702, 277, 294, 37, 402, 1121, 384, 193, 991, 91, 737, 344, 61, 1176, 695, 276, 360, 1043, 1143, 774, 198, 970, 1000, 597, 1079, 408, 284, 671, 436, 28, 331, 718, 199, 694, 117, 381, 1090, 728, 1175, 844, 964, 192, 1027, 773, 1197, 406, 800, 1040, 768, 427, 272, 1063, 542, 827, 724, 555, 48, 775, 1125, 490, 438, 976, 382, 929, 977, 8, 123, 1095, 1210, 1050, 163, 1061, 1031, 556, 998, 321, 98, 1201, 310, 639, 457, 839, 173, 1093, 1026, 529, 214, 1032, 366, 233, 636, 426, 159, 181, 1039, 962, 900, 1194, 570, 640, 291, 41, 191, 2, 351, 1209, 573, 476, 749, 606, 235, 813, 216, 648, 450, 1198, 995, 993, 513, 430, 147, 1134, 72, 777, 1019, 1106, 229, 754, 641, 1107, 744, 109, 225, 257, 605, 251]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7580743448180963
the save name prefix for this run is:  chkpt-ID_7580743448180963_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 497
rank avg (pred): 0.414 +- 0.003
mrr vals (pred, true): 0.000, 0.182
batch losses (mrrl, rdl): 0.0, 0.0005994276

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 545
rank avg (pred): 0.357 +- 0.004
mrr vals (pred, true): 0.000, 0.085
batch losses (mrrl, rdl): 0.0, 0.000159775

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 792
rank avg (pred): 0.540 +- 0.270
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.04023e-05

Epoch over!
epoch time: 14.985

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 932
rank avg (pred): 0.541 +- 0.256
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.6299e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 999
rank avg (pred): 0.511 +- 0.274
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 6.2635e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 175
rank avg (pred): 0.521 +- 0.271
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 4.9199e-06

Epoch over!
epoch time: 14.977

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 26
rank avg (pred): 0.174 +- 0.286
mrr vals (pred, true): 0.002, 0.293
batch losses (mrrl, rdl): 0.0, 2.68906e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1002
rank avg (pred): 0.511 +- 0.283
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 1.399e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 357
rank avg (pred): 0.520 +- 0.281
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 3.4926e-06

Epoch over!
epoch time: 15.011

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 283
rank avg (pred): 0.388 +- 0.324
mrr vals (pred, true): 0.000, 0.087
batch losses (mrrl, rdl): 0.0, 2.22559e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 908
rank avg (pred): 0.543 +- 0.274
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004136801

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 575
rank avg (pred): 0.512 +- 0.280
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 1.2296e-06

Epoch over!
epoch time: 14.945

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 964
rank avg (pred): 0.512 +- 0.262
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 2.7033e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1194
rank avg (pred): 0.513 +- 0.280
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.0411e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 639
rank avg (pred): 0.491 +- 0.295
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 8.196e-07

Epoch over!
epoch time: 14.947

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 570
rank avg (pred): 0.527 +- 0.284
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.024821518, 6.1514e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 543
rank avg (pred): 0.342 +- 0.343
mrr vals (pred, true): 0.051, 0.083
batch losses (mrrl, rdl): 9.9497e-06, 9.9383e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1129
rank avg (pred): 0.442 +- 0.337
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0020305419, 2.84324e-05

Epoch over!
epoch time: 15.123

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 985
rank avg (pred): 0.389 +- 0.301
mrr vals (pred, true): 0.063, 0.101
batch losses (mrrl, rdl): 0.0147232655, 0.0001591879

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 467
rank avg (pred): 0.442 +- 0.323
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001991599, 3.1619e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 383
rank avg (pred): 0.486 +- 0.355
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004948397, 3.30654e-05

Epoch over!
epoch time: 15.112

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 287
rank avg (pred): 0.398 +- 0.303
mrr vals (pred, true): 0.066, 0.088
batch losses (mrrl, rdl): 0.0024639647, 8.22395e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 256
rank avg (pred): 0.285 +- 0.246
mrr vals (pred, true): 0.210, 0.272
batch losses (mrrl, rdl): 0.0381959379, 0.0005421382

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1162
rank avg (pred): 0.381 +- 0.300
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0023218207, 0.0001733313

Epoch over!
epoch time: 15.108

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 565
rank avg (pred): 0.436 +- 0.319
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 2.24188e-05, 0.0001240696

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 58
rank avg (pred): 0.407 +- 0.312
mrr vals (pred, true): 0.054, 0.065
batch losses (mrrl, rdl): 0.0001240533, 7.76877e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 220
rank avg (pred): 0.520 +- 0.351
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0009385657, 7.06765e-05

Epoch over!
epoch time: 15.104

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 334
rank avg (pred): 0.424 +- 0.317
mrr vals (pred, true): 0.054, 0.002
batch losses (mrrl, rdl): 0.0001460252, 7.01159e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 734
rank avg (pred): 0.211 +- 0.230
mrr vals (pred, true): 0.270, 0.343
batch losses (mrrl, rdl): 0.0532791428, 0.0003770649

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 998
rank avg (pred): 0.308 +- 0.265
mrr vals (pred, true): 0.083, 0.125
batch losses (mrrl, rdl): 0.0176162608, 1.90655e-05

Epoch over!
epoch time: 15.128

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 597
rank avg (pred): 0.509 +- 0.341
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001740267, 3.06639e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 625
rank avg (pred): 0.498 +- 0.336
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 1.0363e-06, 1.3083e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 253
rank avg (pred): 0.243 +- 0.293
mrr vals (pred, true): 0.226, 0.273
batch losses (mrrl, rdl): 0.0217166953, 0.0001937155

Epoch over!
epoch time: 15.108

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 574
rank avg (pred): 0.492 +- 0.327
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.15013e-05, 4.2496e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 569
rank avg (pred): 0.427 +- 0.300
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.92471e-05, 6.37119e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1007
rank avg (pred): 0.482 +- 0.319
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 4.372e-07, 2.6512e-06

Epoch over!
epoch time: 15.099

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 94
rank avg (pred): 0.477 +- 0.315
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 1.695e-07, 5.2261e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 203
rank avg (pred): 0.478 +- 0.315
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 7.7298e-06, 2.4465e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.409 +- 0.327
mrr vals (pred, true): 0.075, 0.103
batch losses (mrrl, rdl): 0.0076789064, 0.0001033294

Epoch over!
epoch time: 15.109

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 139
rank avg (pred): 0.501 +- 0.328
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.26768e-05, 9.5966e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 254
rank avg (pred): 0.194 +- 0.281
mrr vals (pred, true): 0.226, 0.286
batch losses (mrrl, rdl): 0.0355905443, 1.2172e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1001
rank avg (pred): 0.493 +- 0.324
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.64366e-05, 2.941e-06

Epoch over!
epoch time: 15.129

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 567
rank avg (pred): 0.504 +- 0.319
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.38276e-05, 1.12499e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 444
rank avg (pred): 0.402 +- 0.294
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003034157, 0.0001316761

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 541
rank avg (pred): 0.385 +- 0.337
mrr vals (pred, true): 0.051, 0.072
batch losses (mrrl, rdl): 3.7137e-06, 7.2921e-06

Epoch over!
epoch time: 15.109

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.483 +- 0.336
mrr vals (pred, true): 0.051, 0.000

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  100 	     0 	 0.07471 	 0.00022 	 m..s
   99 	     1 	 0.06183 	 0.00038 	 m..s
   98 	     2 	 0.06177 	 0.00040 	 m..s
   26 	     3 	 0.04966 	 0.00044 	 m..s
   64 	     4 	 0.05085 	 0.00047 	 m..s
   48 	     5 	 0.05061 	 0.00050 	 m..s
   48 	     6 	 0.05061 	 0.00050 	 m..s
   23 	     7 	 0.04966 	 0.00051 	 m..s
   76 	     8 	 0.05157 	 0.00051 	 m..s
   32 	     9 	 0.04985 	 0.00051 	 m..s
   68 	    10 	 0.05090 	 0.00051 	 m..s
   39 	    11 	 0.05043 	 0.00052 	 m..s
   18 	    12 	 0.04953 	 0.00052 	 m..s
   53 	    13 	 0.05067 	 0.00053 	 m..s
   62 	    14 	 0.05083 	 0.00053 	 m..s
   52 	    15 	 0.05065 	 0.00055 	 m..s
   34 	    16 	 0.04999 	 0.00055 	 m..s
   14 	    17 	 0.04949 	 0.00056 	 m..s
   43 	    18 	 0.05059 	 0.00056 	 m..s
   36 	    19 	 0.05009 	 0.00056 	 m..s
   73 	    20 	 0.05121 	 0.00057 	 m..s
   97 	    21 	 0.05943 	 0.00058 	 m..s
   75 	    22 	 0.05152 	 0.00058 	 m..s
   33 	    23 	 0.04985 	 0.00058 	 m..s
   25 	    24 	 0.04966 	 0.00060 	 m..s
    7 	    25 	 0.04920 	 0.00061 	 m..s
   27 	    26 	 0.04979 	 0.00061 	 m..s
   35 	    27 	 0.05007 	 0.00062 	 m..s
   27 	    28 	 0.04979 	 0.00063 	 m..s
   46 	    29 	 0.05060 	 0.00063 	 m..s
   57 	    30 	 0.05074 	 0.00064 	 m..s
   84 	    31 	 0.05280 	 0.00066 	 m..s
   54 	    32 	 0.05067 	 0.00069 	 m..s
   55 	    33 	 0.05072 	 0.00071 	 m..s
   40 	    34 	 0.05044 	 0.00071 	 m..s
   29 	    35 	 0.04980 	 0.00072 	 m..s
   30 	    36 	 0.04984 	 0.00073 	 m..s
   58 	    37 	 0.05075 	 0.00075 	 m..s
   72 	    38 	 0.05121 	 0.00076 	 m..s
   20 	    39 	 0.04954 	 0.00076 	 m..s
   70 	    40 	 0.05108 	 0.00077 	 m..s
    0 	    41 	 0.04896 	 0.00078 	 m..s
    9 	    42 	 0.04935 	 0.00078 	 m..s
   48 	    43 	 0.05061 	 0.00080 	 m..s
   95 	    44 	 0.05536 	 0.00080 	 m..s
   17 	    45 	 0.04952 	 0.00080 	 m..s
   79 	    46 	 0.05257 	 0.00081 	 m..s
   16 	    47 	 0.04951 	 0.00083 	 m..s
   19 	    48 	 0.04953 	 0.00083 	 m..s
   30 	    49 	 0.04984 	 0.00088 	 m..s
   23 	    50 	 0.04966 	 0.00090 	 m..s
   67 	    51 	 0.05089 	 0.00091 	 m..s
   21 	    52 	 0.04960 	 0.00091 	 m..s
   71 	    53 	 0.05108 	 0.00093 	 m..s
   65 	    54 	 0.05086 	 0.00101 	 m..s
   22 	    55 	 0.04961 	 0.00101 	 m..s
    1 	    56 	 0.04898 	 0.00102 	 m..s
   96 	    57 	 0.05691 	 0.00104 	 m..s
   61 	    58 	 0.05082 	 0.00104 	 m..s
   10 	    59 	 0.04943 	 0.00106 	 m..s
   59 	    60 	 0.05081 	 0.00107 	 m..s
    4 	    61 	 0.04901 	 0.00112 	 m..s
   63 	    62 	 0.05084 	 0.00115 	 m..s
   15 	    63 	 0.04950 	 0.00116 	 m..s
    3 	    64 	 0.04900 	 0.00118 	 m..s
   12 	    65 	 0.04945 	 0.00129 	 m..s
   44 	    66 	 0.05059 	 0.00146 	 m..s
    6 	    67 	 0.04906 	 0.00148 	 m..s
   13 	    68 	 0.04946 	 0.00151 	 m..s
   47 	    69 	 0.05060 	 0.00154 	 m..s
   60 	    70 	 0.05082 	 0.00155 	 m..s
   69 	    71 	 0.05102 	 0.00160 	 m..s
   65 	    72 	 0.05086 	 0.00166 	 m..s
   45 	    73 	 0.05060 	 0.00167 	 m..s
    8 	    74 	 0.04935 	 0.00176 	 m..s
   11 	    75 	 0.04944 	 0.00181 	 m..s
    5 	    76 	 0.04903 	 0.00191 	 m..s
   56 	    77 	 0.05073 	 0.00207 	 m..s
   77 	    78 	 0.05159 	 0.00227 	 m..s
   74 	    79 	 0.05125 	 0.00231 	 m..s
    2 	    80 	 0.04898 	 0.00263 	 m..s
   37 	    81 	 0.05034 	 0.05253 	 ~...
   42 	    82 	 0.05047 	 0.05495 	 ~...
  107 	    83 	 0.17407 	 0.05624 	 MISS
   81 	    84 	 0.05270 	 0.06882 	 ~...
   81 	    85 	 0.05270 	 0.07020 	 ~...
   81 	    86 	 0.05270 	 0.07066 	 ~...
   87 	    87 	 0.05316 	 0.07080 	 ~...
   89 	    88 	 0.05322 	 0.07085 	 ~...
  112 	    89 	 0.19401 	 0.07170 	 MISS
   41 	    90 	 0.05045 	 0.07194 	 ~...
  114 	    91 	 0.22753 	 0.07225 	 MISS
   86 	    92 	 0.05311 	 0.07472 	 ~...
   88 	    93 	 0.05320 	 0.07489 	 ~...
   51 	    94 	 0.05065 	 0.07621 	 ~...
   89 	    95 	 0.05322 	 0.07874 	 ~...
   85 	    96 	 0.05283 	 0.07894 	 ~...
  116 	    97 	 0.22800 	 0.07963 	 MISS
   94 	    98 	 0.05439 	 0.07981 	 ~...
   93 	    99 	 0.05384 	 0.08118 	 ~...
   38 	   100 	 0.05041 	 0.08325 	 m..s
   91 	   101 	 0.05352 	 0.08410 	 m..s
   78 	   102 	 0.05236 	 0.08451 	 m..s
   80 	   103 	 0.05266 	 0.08511 	 m..s
   92 	   104 	 0.05365 	 0.08521 	 m..s
  104 	   105 	 0.14619 	 0.08960 	 m..s
  111 	   106 	 0.19080 	 0.09364 	 m..s
  109 	   107 	 0.18399 	 0.09562 	 m..s
  103 	   108 	 0.14294 	 0.10268 	 m..s
  101 	   109 	 0.13360 	 0.10416 	 ~...
  117 	   110 	 0.22925 	 0.12358 	 MISS
  101 	   111 	 0.13360 	 0.12682 	 ~...
  119 	   112 	 0.22966 	 0.12958 	 MISS
  118 	   113 	 0.22953 	 0.13637 	 m..s
  105 	   114 	 0.16634 	 0.14116 	 ~...
  108 	   115 	 0.17485 	 0.16794 	 ~...
  106 	   116 	 0.16716 	 0.20306 	 m..s
  113 	   117 	 0.21391 	 0.21646 	 ~...
  110 	   118 	 0.19061 	 0.21998 	 ~...
  115 	   119 	 0.22780 	 0.27826 	 m..s
  120 	   120 	 0.23008 	 0.29363 	 m..s
==========================================
r_mrr = 0.7803831696510315
r2_mrr = 0.19240742921829224
spearmanr_mrr@5 = 0.7802292704582214
spearmanr_mrr@10 = 0.6788428425788879
spearmanr_mrr@50 = 0.8232529163360596
spearmanr_mrr@100 = 0.8664373755455017
spearmanr_mrr@All = 0.8726957440376282
==========================================
test time: 0.45
Done Testing dataset OpenEA
total time taken: 253.43856596946716
training time taken: 226.45516395568848
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.7804)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.1924)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.7802)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.6788)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.8233)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.8664)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.8727)}}, 'test_loss': {'ComplEx': {'OpenEA': 1.9068838651764963}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 6861649783297940
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1050, 1113, 462, 279, 217, 75, 1074, 588, 986, 0, 624, 587, 204, 955, 861, 1110, 304, 923, 1056, 337, 158, 113, 522, 880, 692, 144, 80, 648, 724, 761, 371, 1132, 837, 66, 251, 447, 833, 355, 245, 260, 809, 179, 103, 511, 1037, 1196, 151, 816, 73, 97, 980, 82, 896, 405, 1012, 44, 237, 1082, 987, 764, 618, 660, 943, 438, 492, 708, 524, 1060, 222, 711, 731, 451, 652, 1199, 963, 948, 1160, 606, 209, 1011, 733, 244, 975, 77, 621, 49, 785, 556, 54, 302, 206, 116, 464, 993, 680, 569, 771, 376, 581, 367, 401, 1122, 124, 308, 67, 889, 122, 1191, 104, 1032, 659, 466, 544, 663, 180, 625, 1182, 669, 1112, 735, 315]
valid_ids (0): []
train_ids (1094): [358, 257, 594, 665, 386, 823, 1135, 770, 41, 309, 472, 318, 571, 129, 1129, 170, 820, 592, 699, 620, 391, 817, 615, 480, 292, 187, 893, 956, 351, 608, 1169, 233, 327, 970, 787, 593, 945, 1093, 1006, 561, 661, 64, 517, 1154, 742, 61, 1044, 372, 38, 235, 795, 579, 612, 1023, 636, 352, 545, 477, 395, 1007, 181, 573, 37, 788, 605, 1024, 381, 917, 457, 236, 433, 696, 397, 599, 991, 46, 275, 285, 446, 902, 469, 202, 713, 159, 421, 542, 633, 916, 726, 1005, 1202, 737, 555, 243, 877, 562, 239, 796, 547, 350, 149, 1141, 491, 1173, 228, 580, 1043, 1094, 954, 19, 1167, 941, 85, 740, 965, 707, 929, 627, 1038, 404, 939, 71, 838, 130, 961, 282, 90, 178, 39, 425, 1047, 390, 900, 1186, 388, 488, 199, 473, 51, 1021, 436, 422, 366, 1069, 1068, 1013, 600, 754, 514, 1017, 671, 250, 853, 74, 242, 1104, 1214, 842, 512, 115, 157, 389, 804, 1101, 437, 3, 818, 972, 62, 377, 769, 790, 306, 96, 319, 380, 345, 173, 453, 111, 1130, 373, 1073, 667, 474, 485, 368, 730, 1193, 746, 1097, 949, 1076, 523, 208, 591, 307, 238, 1162, 1146, 647, 224, 276, 280, 163, 506, 471, 392, 1161, 835, 28, 1095, 461, 489, 810, 914, 240, 765, 936, 828, 751, 1091, 253, 1036, 481, 942, 784, 1001, 136, 962, 622, 60, 69, 273, 768, 487, 529, 321, 637, 65, 91, 675, 172, 1055, 317, 48, 1121, 241, 1140, 1151, 767, 133, 921, 26, 774, 1170, 32, 753, 867, 234, 1210, 226, 1209, 881, 1086, 537, 1025, 1212, 1138, 155, 1148, 777, 799, 775, 803, 577, 297, 84, 118, 1134, 278, 286, 213, 52, 1123, 411, 50, 969, 63, 320, 901, 574, 834, 402, 1019, 468, 844, 840, 1072, 174, 14, 550, 518, 681, 20, 106, 776, 190, 448, 128, 1053, 9, 197, 1051, 1010, 576, 791, 503, 688, 254, 1163, 727, 349, 99, 734, 656, 16, 854, 335, 482, 836, 841, 990, 538, 1157, 1126, 974, 22, 897, 839, 56, 750, 15, 449, 132, 1045, 895, 1015, 246, 870, 634, 288, 638, 831, 979, 1195, 604, 499, 322, 676, 829, 903, 192, 100, 756, 684, 326, 1178, 695, 223, 105, 879, 1099, 348, 513, 258, 697, 1144, 568, 643, 365, 379, 406, 188, 6, 875, 31, 1035, 1083, 310, 305, 617, 1070, 398, 479, 685, 121, 454, 966, 189, 932, 443, 271, 1111, 1030, 229, 109, 868, 1064, 323, 314, 30, 778, 1149, 797, 748, 1071, 1198, 1018, 93, 23, 919, 525, 891, 533, 1197, 5, 1084, 832, 1100, 387, 1090, 1125, 885, 346, 1061, 344, 316, 720, 263, 215, 456, 140, 483, 535, 444, 1004, 1042, 642, 757, 40, 595, 1139, 802, 1033, 845, 938, 994, 114, 169, 1119, 982, 1203, 664, 603, 560, 721, 1128, 168, 430, 565, 300, 435, 898, 221, 1200, 17, 420, 510, 865, 1, 575, 646, 918, 888, 951, 700, 851, 1080, 295, 644, 364, 890, 112, 1040, 957, 682, 528, 551, 231, 850, 270, 291, 882, 156, 567, 922, 35, 557, 13, 908, 801, 983, 1192, 450, 752, 476, 267, 1171, 214, 42, 1114, 564, 811, 1046, 718, 290, 164, 1127, 1208, 826, 710, 704, 709, 1048, 1079, 911, 1176, 412, 907, 1002, 539, 905, 123, 334, 1153, 651, 1054, 184, 410, 1059, 928, 780, 670, 944, 356, 950, 301, 1087, 819, 862, 264, 1118, 583, 195, 86, 498, 857, 182, 497, 806, 312, 1031, 613, 920, 10, 460, 978, 18, 674, 741, 691, 493, 732, 937, 1088, 927, 629, 328, 33, 1065, 959, 72, 931, 393, 698, 200, 296, 101, 486, 1137, 988, 1034, 666, 378, 1143, 176, 1000, 747, 331, 201, 792, 382, 459, 1177, 4, 1014, 532, 715, 299, 230, 415, 1009, 601, 892, 1081, 507, 255, 654, 423, 1039, 755, 261, 343, 690, 141, 1052, 484, 269, 543, 559, 536, 871, 496, 909, 672, 521, 693, 728, 616, 495, 196, 655, 194, 678, 668, 1057, 878, 1179, 641, 409, 166, 925, 611, 793, 375, 662, 992, 960, 95, 626, 120, 717, 519, 177, 1159, 821, 1181, 773, 640, 714, 126, 183, 1098, 333, 632, 1205, 884, 455, 293, 1120, 763, 869, 143, 658, 984, 582, 849, 34, 1166, 650, 70, 1041, 1190, 400, 515, 154, 403, 798, 1109, 702, 428, 940, 653, 419, 274, 1106, 1003, 298, 998, 935, 995, 578, 502, 374, 976, 848, 930, 598, 813, 1066, 470, 794, 500, 369, 546, 29, 1085, 822, 883, 716, 501, 805, 967, 540, 520, 824, 899, 162, 738, 370, 701, 504, 89, 683, 1201, 610, 873, 441, 1105, 452, 687, 1078, 27, 527, 330, 153, 946, 973, 146, 1152, 160, 342, 915, 619, 1206, 703, 341, 766, 508, 530, 1062, 1150, 249, 859, 887, 87, 1027, 139, 552, 572, 294, 1168, 394, 458, 736, 142, 78, 516, 1096, 256, 906, 1063, 847, 262, 947, 723, 43, 1092, 989, 1175, 55, 566, 924, 399, 762, 781, 1108, 814, 505, 1089, 1174, 359, 589, 259, 825, 1029, 1115, 1147, 705, 1189, 1131, 706, 102, 36, 266, 968, 760, 864, 509, 597, 933, 219, 431, 161, 843, 440, 465, 800, 442, 759, 11, 874, 347, 332, 686, 852, 383, 24, 812, 541, 913, 904, 1020, 289, 1049, 628, 427, 807, 147, 88, 478, 602, 1008, 772, 872, 414, 212, 417, 165, 1124, 584, 463, 786, 729, 531, 152, 558, 248, 712, 98, 125, 635, 630, 150, 1156, 657, 1026, 117, 186, 277, 1067, 1016, 1107, 216, 338, 110, 860, 631, 272, 268, 1204, 999, 2, 745, 45, 743, 198, 434, 1077, 127, 283, 1022, 1158, 815, 1165, 1184, 808, 1117, 21, 47, 1207, 83, 353, 108, 886, 855, 287, 490, 1194, 689, 25, 357, 119, 220, 934, 876, 252, 76, 167, 549, 534, 467, 997, 1103, 426, 131, 175, 137, 385, 225, 719, 284, 863, 609, 185, 694, 590, 894, 445, 92, 1142, 1136, 1188, 526, 679, 408, 926, 1075, 649, 138, 134, 677, 971, 362, 191, 1145, 846, 247, 354, 645, 1180, 324, 858, 952, 1028, 1172, 311, 313, 1187, 339, 856, 673, 193, 1058, 59, 912, 303, 416, 1102, 8, 439, 432, 171, 1211, 548, 107, 782, 1133, 553, 12, 1164, 586, 148, 265, 1185, 953, 964, 361, 407, 739, 135, 360, 722, 1116, 207, 585, 639, 985, 384, 554, 218, 1155, 996, 145, 614, 827, 977, 475, 758, 623, 1183, 830, 607, 424, 79, 779, 227, 396, 232, 1213, 363, 725, 329, 749, 570, 783, 281, 210, 981, 789, 58, 413, 57, 68, 325, 958, 429, 336, 596, 7, 866, 81, 418, 211, 340, 203, 205, 563, 94, 744, 910, 494, 53]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7239210844666301
the save name prefix for this run is:  chkpt-ID_7239210844666301_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 514
rank avg (pred): 0.486 +- 0.004
mrr vals (pred, true): 0.000, 0.078
batch losses (mrrl, rdl): 0.0, 0.0005055669

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1016
rank avg (pred): 0.482 +- 0.296
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 4.4254e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 255
rank avg (pred): 0.220 +- 0.252
mrr vals (pred, true): 0.004, 0.256
batch losses (mrrl, rdl): 0.0, 0.0002500087

Epoch over!
epoch time: 14.968

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.403 +- 0.313
mrr vals (pred, true): 0.001, 0.105
batch losses (mrrl, rdl): 0.0, 0.0001333622

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 693
rank avg (pred): 0.502 +- 0.290
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 3.415e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 811
rank avg (pred): 0.169 +- 0.257
mrr vals (pred, true): 0.007, 0.326
batch losses (mrrl, rdl): 0.0, 2.00421e-05

Epoch over!
epoch time: 14.945

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 211
rank avg (pred): 0.491 +- 0.296
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.0354e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 762
rank avg (pred): 0.466 +- 0.290
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.23645e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.485 +- 0.279
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.3173e-06

Epoch over!
epoch time: 14.953

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 884
rank avg (pred): 0.507 +- 0.281
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 4.659e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 427
rank avg (pred): 0.479 +- 0.299
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 5.6684e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.497 +- 0.285
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 3.33e-07

Epoch over!
epoch time: 14.934

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1161
rank avg (pred): 0.500 +- 0.287
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.013e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 997
rank avg (pred): 0.333 +- 0.312
mrr vals (pred, true): 0.005, 0.139
batch losses (mrrl, rdl): 0.0, 2.57343e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 322
rank avg (pred): 0.345 +- 0.320
mrr vals (pred, true): 0.007, 0.071
batch losses (mrrl, rdl): 0.0, 2.1654e-06

Epoch over!
epoch time: 14.958

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 368
rank avg (pred): 0.484 +- 0.284
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0246886928, 8.628e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 513
rank avg (pred): 0.479 +- 0.331
mrr vals (pred, true): 0.013, 0.078
batch losses (mrrl, rdl): 0.0134895705, 0.000283621

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 515
rank avg (pred): 0.374 +- 0.315
mrr vals (pred, true): 0.053, 0.079
batch losses (mrrl, rdl): 0.000110704, 5.9572e-06

Epoch over!
epoch time: 15.154

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 332
rank avg (pred): 0.496 +- 0.332
mrr vals (pred, true): 0.030, 0.001
batch losses (mrrl, rdl): 0.0040557804, 1.45684e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 783
rank avg (pred): 0.465 +- 0.351
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0089908103, 2.5548e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1001
rank avg (pred): 0.484 +- 0.331
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006276908, 8.2292e-06

Epoch over!
epoch time: 15.137

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 559
rank avg (pred): 0.412 +- 0.321
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0003170795, 4.30817e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 390
rank avg (pred): 0.453 +- 0.330
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002385474, 3.00104e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 448
rank avg (pred): 0.431 +- 0.317
mrr vals (pred, true): 0.077, 0.001
batch losses (mrrl, rdl): 0.0072837286, 5.2762e-05

Epoch over!
epoch time: 15.161

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1173
rank avg (pred): 0.458 +- 0.320
mrr vals (pred, true): 0.060, 0.002
batch losses (mrrl, rdl): 0.0010030402, 1.64767e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 589
rank avg (pred): 0.496 +- 0.339
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.39688e-05, 1.83048e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 184
rank avg (pred): 0.523 +- 0.357
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0021290155, 4.15762e-05

Epoch over!
epoch time: 15.22

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 857
rank avg (pred): 0.498 +- 0.317
mrr vals (pred, true): 0.030, 0.001
batch losses (mrrl, rdl): 0.0038722283, 1.8011e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 34
rank avg (pred): 0.406 +- 0.340
mrr vals (pred, true): 0.058, 0.075
batch losses (mrrl, rdl): 0.0006489409, 2.73272e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 526
rank avg (pred): 0.481 +- 0.327
mrr vals (pred, true): 0.037, 0.079
batch losses (mrrl, rdl): 0.0015668577, 0.0002467069

Epoch over!
epoch time: 15.299

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 17
rank avg (pred): 0.255 +- 0.341
mrr vals (pred, true): 0.134, 0.272
batch losses (mrrl, rdl): 0.1916615963, 0.0001079743

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 230
rank avg (pred): 0.480 +- 0.335
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001868741, 9.2447e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 391
rank avg (pred): 0.485 +- 0.330
mrr vals (pred, true): 0.040, 0.002
batch losses (mrrl, rdl): 0.0009823074, 5.8087e-06

Epoch over!
epoch time: 15.294

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 718
rank avg (pred): 0.450 +- 0.324
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004288419, 3.95067e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 870
rank avg (pred): 0.509 +- 0.317
mrr vals (pred, true): 0.030, 0.001
batch losses (mrrl, rdl): 0.0038377307, 1.20593e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 806
rank avg (pred): 0.422 +- 0.285
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0007934775, 9.23718e-05

Epoch over!
epoch time: 15.279

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 86
rank avg (pred): 0.461 +- 0.306
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 9.6907e-06, 1.04452e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 487
rank avg (pred): 0.326 +- 0.317
mrr vals (pred, true): 0.102, 0.077
batch losses (mrrl, rdl): 0.0275601279, 2.9276e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 622
rank avg (pred): 0.463 +- 0.318
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0013171872, 1.42495e-05

Epoch over!
epoch time: 15.14

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 13
rank avg (pred): 0.296 +- 0.342
mrr vals (pred, true): 0.226, 0.261
batch losses (mrrl, rdl): 0.0120803788, 0.0003420527

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 385
rank avg (pred): 0.465 +- 0.316
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.51106e-05, 2.07126e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 108
rank avg (pred): 0.458 +- 0.318
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.2284e-05, 2.73764e-05

Epoch over!
epoch time: 15.286

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 628
rank avg (pred): 0.459 +- 0.328
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005768689, 2.70339e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1150
rank avg (pred): 0.273 +- 0.310
mrr vals (pred, true): 0.170, 0.138
batch losses (mrrl, rdl): 0.0106595196, 2.08594e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 143
rank avg (pred): 0.466 +- 0.317
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 7.4e-09, 1.57066e-05

Epoch over!
epoch time: 15.293

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.498 +- 0.313
mrr vals (pred, true): 0.045, 0.000

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   82 	     0 	 0.04933 	 0.00047 	 m..s
   79 	     1 	 0.04917 	 0.00047 	 m..s
   55 	     2 	 0.04514 	 0.00049 	 m..s
   63 	     3 	 0.04753 	 0.00050 	 m..s
   84 	     4 	 0.04936 	 0.00051 	 m..s
   15 	     5 	 0.04259 	 0.00054 	 m..s
   13 	     6 	 0.04246 	 0.00055 	 m..s
   61 	     7 	 0.04738 	 0.00055 	 m..s
   28 	     8 	 0.04344 	 0.00055 	 m..s
    1 	     9 	 0.03754 	 0.00056 	 m..s
   26 	    10 	 0.04326 	 0.00057 	 m..s
   76 	    11 	 0.04875 	 0.00057 	 m..s
   11 	    12 	 0.04236 	 0.00057 	 m..s
   17 	    13 	 0.04278 	 0.00057 	 m..s
   24 	    14 	 0.04312 	 0.00058 	 m..s
   45 	    15 	 0.04430 	 0.00061 	 m..s
    2 	    16 	 0.03827 	 0.00061 	 m..s
   42 	    17 	 0.04417 	 0.00061 	 m..s
   31 	    18 	 0.04362 	 0.00062 	 m..s
   81 	    19 	 0.04920 	 0.00067 	 m..s
   20 	    20 	 0.04304 	 0.00067 	 m..s
   25 	    21 	 0.04321 	 0.00069 	 m..s
   41 	    22 	 0.04411 	 0.00069 	 m..s
   33 	    23 	 0.04366 	 0.00069 	 m..s
   42 	    24 	 0.04417 	 0.00069 	 m..s
    0 	    25 	 0.03743 	 0.00070 	 m..s
   57 	    26 	 0.04542 	 0.00071 	 m..s
   96 	    27 	 0.05155 	 0.00071 	 m..s
   70 	    28 	 0.04838 	 0.00072 	 m..s
   72 	    29 	 0.04852 	 0.00073 	 m..s
   77 	    30 	 0.04891 	 0.00075 	 m..s
   36 	    31 	 0.04383 	 0.00076 	 m..s
   14 	    32 	 0.04252 	 0.00077 	 m..s
   44 	    33 	 0.04417 	 0.00077 	 m..s
   74 	    34 	 0.04861 	 0.00078 	 m..s
   23 	    35 	 0.04311 	 0.00081 	 m..s
   27 	    36 	 0.04331 	 0.00081 	 m..s
   62 	    37 	 0.04750 	 0.00082 	 m..s
   56 	    38 	 0.04523 	 0.00083 	 m..s
   93 	    39 	 0.05036 	 0.00083 	 m..s
   94 	    40 	 0.05077 	 0.00085 	 m..s
   19 	    41 	 0.04297 	 0.00086 	 m..s
   65 	    42 	 0.04765 	 0.00087 	 m..s
   64 	    43 	 0.04756 	 0.00088 	 m..s
   22 	    44 	 0.04310 	 0.00088 	 m..s
   52 	    45 	 0.04462 	 0.00089 	 m..s
   46 	    46 	 0.04437 	 0.00089 	 m..s
   54 	    47 	 0.04476 	 0.00090 	 m..s
   75 	    48 	 0.04870 	 0.00091 	 m..s
   37 	    49 	 0.04394 	 0.00091 	 m..s
  100 	    50 	 0.05645 	 0.00092 	 m..s
   68 	    51 	 0.04829 	 0.00095 	 m..s
   29 	    52 	 0.04351 	 0.00101 	 m..s
   53 	    53 	 0.04467 	 0.00102 	 m..s
   40 	    54 	 0.04409 	 0.00104 	 m..s
   34 	    55 	 0.04369 	 0.00106 	 m..s
   17 	    56 	 0.04278 	 0.00106 	 m..s
   80 	    57 	 0.04920 	 0.00107 	 m..s
  101 	    58 	 0.05673 	 0.00109 	 m..s
   60 	    59 	 0.04736 	 0.00112 	 m..s
   78 	    60 	 0.04916 	 0.00114 	 m..s
   98 	    61 	 0.05196 	 0.00114 	 m..s
   87 	    62 	 0.04963 	 0.00115 	 m..s
   38 	    63 	 0.04401 	 0.00116 	 m..s
   16 	    64 	 0.04270 	 0.00121 	 m..s
   12 	    65 	 0.04245 	 0.00122 	 m..s
   50 	    66 	 0.04449 	 0.00125 	 m..s
   71 	    67 	 0.04842 	 0.00128 	 m..s
   73 	    68 	 0.04860 	 0.00129 	 m..s
   85 	    69 	 0.04945 	 0.00130 	 m..s
   47 	    70 	 0.04440 	 0.00133 	 m..s
   49 	    71 	 0.04447 	 0.00136 	 m..s
   86 	    72 	 0.04948 	 0.00137 	 m..s
   97 	    73 	 0.05180 	 0.00144 	 m..s
  101 	    74 	 0.05673 	 0.00160 	 m..s
   69 	    75 	 0.04833 	 0.00166 	 m..s
   88 	    76 	 0.04998 	 0.00166 	 m..s
   10 	    77 	 0.04235 	 0.00171 	 m..s
   35 	    78 	 0.04370 	 0.00176 	 m..s
   47 	    79 	 0.04440 	 0.00178 	 m..s
   51 	    80 	 0.04455 	 0.00268 	 m..s
  105 	    81 	 0.08321 	 0.01037 	 m..s
   95 	    82 	 0.05143 	 0.01254 	 m..s
  104 	    83 	 0.08218 	 0.01790 	 m..s
  103 	    84 	 0.07245 	 0.04963 	 ~...
   32 	    85 	 0.04363 	 0.06558 	 ~...
   58 	    86 	 0.04551 	 0.06727 	 ~...
    9 	    87 	 0.04174 	 0.07013 	 ~...
   39 	    88 	 0.04406 	 0.07087 	 ~...
    4 	    89 	 0.03948 	 0.07189 	 m..s
  111 	    90 	 0.15581 	 0.07201 	 m..s
   66 	    91 	 0.04796 	 0.07307 	 ~...
   99 	    92 	 0.05212 	 0.07360 	 ~...
    6 	    93 	 0.03952 	 0.07360 	 m..s
    4 	    94 	 0.03948 	 0.07667 	 m..s
    7 	    95 	 0.04098 	 0.07719 	 m..s
   30 	    96 	 0.04358 	 0.07732 	 m..s
   83 	    97 	 0.04936 	 0.07945 	 m..s
   59 	    98 	 0.04727 	 0.08351 	 m..s
    3 	    99 	 0.03931 	 0.08449 	 m..s
    7 	   100 	 0.04098 	 0.08511 	 m..s
   66 	   101 	 0.04796 	 0.08523 	 m..s
   89 	   102 	 0.05008 	 0.08565 	 m..s
   21 	   103 	 0.04305 	 0.08794 	 m..s
   90 	   104 	 0.05021 	 0.10268 	 m..s
   90 	   105 	 0.05021 	 0.10389 	 m..s
   90 	   106 	 0.05021 	 0.10621 	 m..s
  113 	   107 	 0.18072 	 0.10882 	 m..s
  108 	   108 	 0.12300 	 0.12940 	 ~...
  112 	   109 	 0.17903 	 0.13118 	 m..s
  106 	   110 	 0.10027 	 0.13290 	 m..s
  113 	   111 	 0.18072 	 0.13637 	 m..s
  109 	   112 	 0.13472 	 0.13837 	 ~...
  107 	   113 	 0.12116 	 0.13941 	 ~...
  110 	   114 	 0.15081 	 0.16711 	 ~...
  116 	   115 	 0.21425 	 0.17068 	 m..s
  115 	   116 	 0.20835 	 0.17323 	 m..s
  117 	   117 	 0.23366 	 0.22307 	 ~...
  118 	   118 	 0.24385 	 0.29363 	 m..s
  119 	   119 	 0.31466 	 0.31774 	 ~...
  120 	   120 	 0.32053 	 0.34668 	 ~...
==========================================
r_mrr = 0.8619552254676819
r2_mrr = 0.5743410587310791
spearmanr_mrr@5 = 0.8986718654632568
spearmanr_mrr@10 = 0.9539837837219238
spearmanr_mrr@50 = 0.9190258979797363
spearmanr_mrr@100 = 0.9175117611885071
spearmanr_mrr@All = 0.9204932451248169
==========================================
test time: 0.462
Done Testing dataset OpenEA
total time taken: 253.15287375450134
training time taken: 227.49440693855286
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8620)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5743)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.8987)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9540)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9190)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9175)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9205)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.4396041334343863}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 6482657741887021
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 398, 756, 12, 874, 22, 364, 889, 1062, 726, 1200, 1190, 984, 1058, 329, 1061, 356, 1059, 109, 1201, 451, 169, 852, 151, 457, 103, 791, 1117, 462, 1086, 570, 622, 588, 904, 880, 70, 550, 671, 982, 466, 944, 277, 345, 1166, 524, 1076, 1060, 763, 698, 104, 97, 555, 408, 1033, 371, 782, 219, 840, 663, 812, 192, 1021, 851, 882, 148, 83, 404, 1210, 801, 688, 1131, 813, 1195, 773, 884, 420, 266, 611, 1146, 303, 646, 717, 665, 470, 708, 308, 856, 295, 247, 713, 75, 336, 686, 312, 896, 1179, 651, 358, 253, 768, 695, 87, 641, 1051, 1112, 675, 1066, 9, 43, 534, 347, 977, 218, 414, 48, 1073, 960, 117, 57, 324, 258]
valid_ids (0): []
train_ids (1094): [631, 888, 229, 521, 829, 1134, 283, 206, 758, 484, 116, 1014, 200, 4, 659, 839, 980, 734, 598, 415, 24, 58, 992, 183, 918, 681, 1083, 209, 93, 584, 576, 316, 13, 461, 184, 73, 737, 684, 822, 687, 966, 362, 112, 927, 624, 1142, 664, 105, 82, 1100, 507, 1071, 1042, 479, 139, 615, 548, 797, 897, 761, 513, 1137, 958, 542, 91, 581, 232, 394, 1095, 114, 627, 596, 667, 311, 418, 964, 163, 496, 1120, 429, 994, 54, 1046, 1212, 764, 1043, 1136, 1054, 1090, 231, 402, 1056, 842, 578, 477, 733, 640, 78, 893, 214, 634, 1070, 363, 1018, 288, 1079, 1211, 1129, 1160, 1025, 629, 1214, 201, 527, 31, 1057, 268, 707, 69, 493, 950, 728, 1085, 118, 1194, 385, 1186, 557, 208, 536, 1022, 185, 1209, 847, 305, 620, 350, 252, 915, 544, 282, 338, 241, 905, 817, 153, 1102, 121, 240, 62, 403, 1093, 929, 337, 518, 644, 571, 635, 1088, 907, 514, 1159, 648, 1135, 873, 849, 946, 926, 357, 98, 908, 271, 260, 895, 178, 628, 776, 986, 1154, 608, 603, 452, 1109, 310, 975, 77, 724, 807, 878, 181, 894, 1163, 868, 930, 826, 906, 770, 872, 1004, 538, 475, 870, 126, 509, 767, 999, 1035, 1064, 891, 696, 101, 1105, 552, 658, 775, 670, 561, 1080, 1157, 317, 1007, 769, 269, 690, 900, 787, 354, 1165, 832, 574, 600, 610, 784, 1067, 328, 39, 1144, 445, 860, 111, 861, 1203, 621, 431, 92, 215, 540, 592, 579, 931, 395, 469, 72, 1132, 94, 1001, 468, 341, 467, 259, 692, 494, 924, 210, 1050, 175, 106, 71, 1149, 941, 981, 1097, 254, 939, 963, 1124, 649, 18, 1138, 819, 545, 1111, 14, 704, 248, 437, 612, 327, 102, 745, 1016, 1072, 645, 459, 188, 837, 1030, 1032, 1099, 196, 361, 916, 711, 546, 373, 195, 23, 1172, 68, 700, 140, 221, 809, 323, 625, 935, 932, 705, 47, 865, 84, 938, 501, 1092, 454, 164, 146, 313, 572, 344, 771, 256, 6, 460, 383, 875, 198, 1063, 1037, 44, 549, 742, 1084, 1041, 177, 922, 693, 359, 1202, 883, 294, 482, 405, 1104, 388, 480, 987, 816, 859, 376, 1015, 430, 1204, 458, 249, 558, 1068, 1167, 564, 187, 224, 747, 993, 824, 848, 374, 709, 774, 1143, 1171, 142, 152, 413, 732, 792, 29, 340, 730, 890, 330, 261, 647, 1052, 1000, 162, 393, 1045, 235, 844, 1127, 63, 632, 971, 1110, 729, 715, 138, 560, 1173, 5, 506, 1213, 779, 159, 64, 320, 522, 504, 86, 191, 391, 786, 1011, 16, 650, 951, 34, 519, 27, 720, 1119, 973, 1147, 727, 161, 435, 302, 367, 407, 805, 562, 1118, 869, 1196, 854, 275, 1181, 307, 28, 585, 677, 2, 1180, 1019, 954, 296, 250, 702, 703, 274, 100, 793, 427, 197, 499, 474, 515, 10, 7, 265, 272, 952, 202, 446, 559, 51, 335, 236, 1148, 920, 390, 909, 943, 483, 1023, 488, 264, 988, 974, 193, 589, 478, 913, 953, 602, 934, 133, 1115, 1207, 113, 382, 147, 537, 877, 820, 565, 790, 910, 88, 61, 143, 19, 529, 136, 531, 36, 160, 1178, 1098, 948, 750, 473, 66, 788, 392, 230, 997, 237, 850, 59, 378, 638, 991, 76, 297, 587, 743, 901, 123, 95, 321, 1158, 290, 251, 270, 497, 516, 680, 438, 682, 656, 660, 1096, 52, 502, 301, 11, 1003, 1048, 962, 976, 676, 421, 735, 306, 1128, 879, 706, 278, 520, 485, 1155, 1103, 442, 755, 567, 845, 979, 389, 867, 315, 661, 45, 1024, 547, 360, 613, 1123, 158, 1133, 885, 168, 881, 553, 637, 242, 65, 1182, 956, 67, 503, 937, 1031, 970, 1141, 137, 871, 368, 490, 1038, 1193, 1187, 772, 481, 370, 190, 1161, 352, 666, 583, 996, 556, 808, 1125, 35, 551, 42, 287, 911, 381, 289, 498, 157, 463, 56, 510, 921, 1013, 377, 212, 149, 972, 380, 1005, 318, 796, 15, 967, 1208, 1049, 226, 834, 273, 1199, 194, 762, 49, 1027, 448, 293, 211, 606, 85, 436, 823, 836, 714, 495, 652, 601, 339, 419, 783, 286, 721, 145, 108, 748, 736, 366, 1055, 125, 351, 127, 505, 719, 657, 1106, 914, 955, 563, 186, 811, 853, 1188, 1184, 741, 617, 789, 607, 925, 653, 759, 20, 577, 55, 440, 525, 575, 623, 90, 299, 864, 375, 353, 279, 1008, 1034, 633, 46, 444, 990, 1140, 511, 122, 1108, 233, 60, 566, 616, 678, 1176, 766, 1183, 155, 841, 898, 120, 471, 1162, 862, 802, 1139, 795, 995, 372, 346, 441, 298, 285, 425, 618, 1151, 1192, 781, 985, 267, 857, 33, 412, 220, 863, 343, 928, 131, 411, 80, 1009, 892, 983, 489, 334, 207, 642, 167, 757, 1012, 417, 17, 590, 110, 227, 738, 32, 284, 81, 532, 1026, 238, 154, 1010, 225, 777, 754, 722, 234, 426, 1205, 639, 753, 683, 716, 300, 539, 173, 798, 619, 827, 1177, 500, 902, 456, 517, 838, 406, 569, 858, 593, 523, 333, 216, 434, 486, 189, 936, 785, 1126, 8, 855, 3, 673, 450, 203, 1197, 945, 476, 989, 1156, 443, 947, 1185, 1121, 1206, 40, 1130, 806, 1069, 379, 396, 1006, 141, 843, 26, 1002, 746, 53, 541, 156, 424, 150, 449, 751, 917, 1017, 401, 508, 968, 512, 694, 262, 739, 662, 778, 107, 205, 1020, 1116, 99, 731, 170, 314, 487, 416, 899, 554, 400, 1174, 528, 326, 174, 535, 228, 204, 1044, 50, 998, 369, 332, 573, 655, 876, 325, 213, 580, 712, 794, 543, 626, 526, 568, 594, 171, 799, 432, 166, 723, 978, 0, 800, 835, 1036, 397, 886, 699, 1153, 129, 725, 959, 830, 447, 679, 309, 1145, 933, 291, 1089, 472, 179, 1164, 1082, 1053, 292, 263, 1150, 255, 940, 132, 1101, 1191, 804, 409, 1175, 828, 691, 1077, 172, 217, 453, 582, 280, 833, 319, 199, 180, 919, 810, 144, 923, 239, 387, 365, 597, 942, 903, 386, 643, 1075, 1087, 79, 689, 384, 25, 244, 1122, 455, 718, 669, 355, 1198, 74, 410, 654, 1169, 614, 96, 399, 1078, 1107, 464, 222, 530, 331, 423, 38, 128, 428, 124, 740, 348, 182, 30, 349, 887, 697, 1170, 674, 165, 322, 949, 37, 599, 961, 591, 710, 765, 744, 492, 245, 1094, 1189, 130, 1065, 803, 89, 815, 821, 21, 912, 630, 1168, 965, 969, 609, 257, 223, 1047, 1091, 276, 701, 135, 604, 1039, 533, 41, 749, 246, 866, 586, 1028, 134, 439, 1074, 752, 176, 281, 846, 119, 685, 957, 818, 1113, 115, 814, 1040, 1114, 422, 491, 595, 1152, 1, 636, 243, 780, 465, 605, 825, 342, 668, 1081, 304, 672, 433, 760, 1029]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  48310380606809
the save name prefix for this run is:  chkpt-ID_48310380606809_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 536
rank avg (pred): 0.475 +- 0.006
mrr vals (pred, true): 0.000, 0.062
batch losses (mrrl, rdl): 0.0, 0.0003843683

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 377
rank avg (pred): 0.496 +- 0.012
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001068835

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1096
rank avg (pred): 0.439 +- 0.281
mrr vals (pred, true): 0.098, 0.002
batch losses (mrrl, rdl): 0.0, 1.73248e-05

Epoch over!
epoch time: 14.957

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 119
rank avg (pred): 0.451 +- 0.309
mrr vals (pred, true): 0.119, 0.001
batch losses (mrrl, rdl): 0.0, 1.49804e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 607
rank avg (pred): 0.483 +- 0.306
mrr vals (pred, true): 0.078, 0.001
batch losses (mrrl, rdl): 0.0, 2.1192e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 556
rank avg (pred): 0.398 +- 0.305
mrr vals (pred, true): 0.083, 0.070
batch losses (mrrl, rdl): 0.0, 2.63815e-05

Epoch over!
epoch time: 14.899

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 427
rank avg (pred): 0.476 +- 0.299
mrr vals (pred, true): 0.076, 0.001
batch losses (mrrl, rdl): 0.0, 4.412e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 728
rank avg (pred): 0.493 +- 0.304
mrr vals (pred, true): 0.077, 0.001
batch losses (mrrl, rdl): 0.0, 9.115e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 36
rank avg (pred): 0.381 +- 0.311
mrr vals (pred, true): 0.103, 0.083
batch losses (mrrl, rdl): 0.0, 2.1566e-05

Epoch over!
epoch time: 14.825

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 59
rank avg (pred): 0.358 +- 0.310
mrr vals (pred, true): 0.098, 0.070
batch losses (mrrl, rdl): 0.0, 7.1834e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 539
rank avg (pred): 0.359 +- 0.315
mrr vals (pred, true): 0.099, 0.059
batch losses (mrrl, rdl): 0.0, 1.30599e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 795
rank avg (pred): 0.480 +- 0.304
mrr vals (pred, true): 0.090, 0.001
batch losses (mrrl, rdl): 0.0, 3.5721e-06

Epoch over!
epoch time: 14.824

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 282
rank avg (pred): 0.357 +- 0.305
mrr vals (pred, true): 0.109, 0.077
batch losses (mrrl, rdl): 0.0, 6.1323e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 443
rank avg (pred): 0.493 +- 0.307
mrr vals (pred, true): 0.081, 0.001
batch losses (mrrl, rdl): 0.0, 1.0438e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 275
rank avg (pred): 0.352 +- 0.310
mrr vals (pred, true): 0.102, 0.071
batch losses (mrrl, rdl): 0.0, 1.28426e-05

Epoch over!
epoch time: 14.833

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 67
rank avg (pred): 0.346 +- 0.316
mrr vals (pred, true): 0.114, 0.085
batch losses (mrrl, rdl): 0.0413052812, 5.0994e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1173
rank avg (pred): 0.589 +- 0.300
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 1.84193e-05, 0.0001512001

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 591
rank avg (pred): 0.486 +- 0.300
mrr vals (pred, true): 0.059, 0.002
batch losses (mrrl, rdl): 0.0007407795, 5.4725e-06

Epoch over!
epoch time: 15.127

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 498
rank avg (pred): 0.278 +- 0.373
mrr vals (pred, true): 0.117, 0.169
batch losses (mrrl, rdl): 0.0269551743, 3.58202e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 8
rank avg (pred): 0.211 +- 0.375
mrr vals (pred, true): 0.181, 0.122
batch losses (mrrl, rdl): 0.0340438671, 0.0002109878

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 150
rank avg (pred): 0.492 +- 0.306
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001620417, 3.4096e-06

Epoch over!
epoch time: 15.055

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 199
rank avg (pred): 0.508 +- 0.295
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0005531326, 2.0956e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 454
rank avg (pred): 0.512 +- 0.272
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.50088e-05, 7.0088e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1043
rank avg (pred): 0.509 +- 0.275
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001138759, 4.4132e-06

Epoch over!
epoch time: 15.101

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 638
rank avg (pred): 0.457 +- 0.324
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.000999785, 5.30141e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 241
rank avg (pred): 0.517 +- 0.268
mrr vals (pred, true): 0.042, 0.000
batch losses (mrrl, rdl): 0.0006442529, 7.205e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 576
rank avg (pred): 0.508 +- 0.289
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.27165e-05, 2.2481e-06

Epoch over!
epoch time: 15.135

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 875
rank avg (pred): 0.479 +- 0.302
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.46138e-05, 2.47329e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 293
rank avg (pred): 0.515 +- 0.339
mrr vals (pred, true): 0.051, 0.079
batch losses (mrrl, rdl): 8.6221e-06, 0.0004734606

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 417
rank avg (pred): 0.491 +- 0.290
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 8.3566e-06, 3.7665e-06

Epoch over!
epoch time: 15.141

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 89
rank avg (pred): 0.486 +- 0.325
mrr vals (pred, true): 0.068, 0.001
batch losses (mrrl, rdl): 0.0033429146, 7.5306e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1204
rank avg (pred): 0.507 +- 0.287
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 7.8488e-06, 4.1607e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1197
rank avg (pred): 0.489 +- 0.310
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4.53e-08, 1.16934e-05

Epoch over!
epoch time: 15.157

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 846
rank avg (pred): 0.483 +- 0.296
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.000130071, 1.6174e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 412
rank avg (pred): 0.487 +- 0.304
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0014007009, 6.8364e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 934
rank avg (pred): 0.510 +- 0.249
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006297929, 2.73936e-05

Epoch over!
epoch time: 15.139

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 499
rank avg (pred): 0.413 +- 0.372
mrr vals (pred, true): 0.136, 0.180
batch losses (mrrl, rdl): 0.0190409739, 0.0004316574

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 488
rank avg (pred): 0.427 +- 0.371
mrr vals (pred, true): 0.082, 0.097
batch losses (mrrl, rdl): 0.01021895, 0.0001948036

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 479
rank avg (pred): 0.515 +- 0.276
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.20078e-05, 6.2776e-06

Epoch over!
epoch time: 15.139

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 922
rank avg (pred): 0.524 +- 0.251
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0020030709, 1.23796e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 385
rank avg (pred): 0.488 +- 0.295
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 9.452e-07, 1.09275e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 232
rank avg (pred): 0.483 +- 0.298
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.18116e-05, 1.07688e-05

Epoch over!
epoch time: 15.149

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 859
rank avg (pred): 0.497 +- 0.293
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.65146e-05, 1.06265e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 416
rank avg (pred): 0.513 +- 0.296
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 2.19175e-05, 3.7397e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 766
rank avg (pred): 0.479 +- 0.310
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0008869085, 2.43396e-05

Epoch over!
epoch time: 15.12

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.362 +- 0.406
mrr vals (pred, true): 0.131, 0.166

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   60 	     0 	 0.05491 	 0.00043 	 m..s
   27 	     1 	 0.05033 	 0.00045 	 m..s
   91 	     2 	 0.05919 	 0.00047 	 m..s
   73 	     3 	 0.05594 	 0.00051 	 m..s
   31 	     4 	 0.05113 	 0.00051 	 m..s
   55 	     5 	 0.05448 	 0.00052 	 m..s
   94 	     6 	 0.06348 	 0.00052 	 m..s
   65 	     7 	 0.05540 	 0.00052 	 m..s
   81 	     8 	 0.05631 	 0.00053 	 m..s
   22 	     9 	 0.05000 	 0.00053 	 m..s
    6 	    10 	 0.04649 	 0.00053 	 m..s
   92 	    11 	 0.06046 	 0.00055 	 m..s
   38 	    12 	 0.05361 	 0.00055 	 m..s
    5 	    13 	 0.04636 	 0.00057 	 m..s
    9 	    14 	 0.04789 	 0.00057 	 m..s
   61 	    15 	 0.05509 	 0.00057 	 m..s
   80 	    16 	 0.05624 	 0.00058 	 m..s
    3 	    17 	 0.04558 	 0.00058 	 m..s
   68 	    18 	 0.05562 	 0.00059 	 m..s
   13 	    19 	 0.04936 	 0.00061 	 m..s
   33 	    20 	 0.05164 	 0.00061 	 m..s
   16 	    21 	 0.04943 	 0.00062 	 m..s
    7 	    22 	 0.04688 	 0.00063 	 m..s
   53 	    23 	 0.05440 	 0.00064 	 m..s
    8 	    24 	 0.04694 	 0.00066 	 m..s
   56 	    25 	 0.05450 	 0.00068 	 m..s
    4 	    26 	 0.04631 	 0.00069 	 m..s
   63 	    27 	 0.05527 	 0.00072 	 m..s
   45 	    28 	 0.05402 	 0.00072 	 m..s
   13 	    29 	 0.04936 	 0.00073 	 m..s
   88 	    30 	 0.05873 	 0.00074 	 m..s
   76 	    31 	 0.05614 	 0.00075 	 m..s
   71 	    32 	 0.05584 	 0.00076 	 m..s
   96 	    33 	 0.06357 	 0.00077 	 m..s
   58 	    34 	 0.05483 	 0.00077 	 m..s
   67 	    35 	 0.05557 	 0.00078 	 m..s
   21 	    36 	 0.04993 	 0.00079 	 m..s
   54 	    37 	 0.05440 	 0.00081 	 m..s
   88 	    38 	 0.05873 	 0.00081 	 m..s
    1 	    39 	 0.04069 	 0.00081 	 m..s
   87 	    40 	 0.05865 	 0.00081 	 m..s
   74 	    41 	 0.05595 	 0.00082 	 m..s
   66 	    42 	 0.05549 	 0.00084 	 m..s
   79 	    43 	 0.05623 	 0.00084 	 m..s
   49 	    44 	 0.05426 	 0.00086 	 m..s
    0 	    45 	 0.04047 	 0.00087 	 m..s
   38 	    46 	 0.05361 	 0.00087 	 m..s
   32 	    47 	 0.05162 	 0.00088 	 m..s
   46 	    48 	 0.05410 	 0.00088 	 m..s
   72 	    49 	 0.05591 	 0.00088 	 m..s
   27 	    50 	 0.05033 	 0.00089 	 m..s
   34 	    51 	 0.05169 	 0.00089 	 m..s
   82 	    52 	 0.05712 	 0.00090 	 m..s
   36 	    53 	 0.05323 	 0.00091 	 m..s
   69 	    54 	 0.05577 	 0.00091 	 m..s
    2 	    55 	 0.04535 	 0.00097 	 m..s
   40 	    56 	 0.05379 	 0.00100 	 m..s
   62 	    57 	 0.05524 	 0.00101 	 m..s
   20 	    58 	 0.04988 	 0.00109 	 m..s
   29 	    59 	 0.05055 	 0.00110 	 m..s
   69 	    60 	 0.05577 	 0.00113 	 m..s
   78 	    61 	 0.05622 	 0.00113 	 m..s
   48 	    62 	 0.05421 	 0.00119 	 m..s
   94 	    63 	 0.06348 	 0.00119 	 m..s
   35 	    64 	 0.05217 	 0.00121 	 m..s
   64 	    65 	 0.05533 	 0.00122 	 m..s
   23 	    66 	 0.05010 	 0.00129 	 m..s
   59 	    67 	 0.05488 	 0.00137 	 m..s
   26 	    68 	 0.05022 	 0.00138 	 m..s
   52 	    69 	 0.05432 	 0.00142 	 m..s
   30 	    70 	 0.05059 	 0.00144 	 m..s
   42 	    71 	 0.05392 	 0.00148 	 m..s
   84 	    72 	 0.05791 	 0.00153 	 m..s
   18 	    73 	 0.04960 	 0.00156 	 m..s
   24 	    74 	 0.05018 	 0.00159 	 m..s
   11 	    75 	 0.04921 	 0.00160 	 m..s
   77 	    76 	 0.05619 	 0.00167 	 m..s
   83 	    77 	 0.05733 	 0.00169 	 m..s
   50 	    78 	 0.05430 	 0.00170 	 m..s
   24 	    79 	 0.05018 	 0.00180 	 m..s
   75 	    80 	 0.05596 	 0.00200 	 m..s
   10 	    81 	 0.04885 	 0.00210 	 m..s
  100 	    82 	 0.06421 	 0.00230 	 m..s
   12 	    83 	 0.04936 	 0.00267 	 m..s
   93 	    84 	 0.06131 	 0.01254 	 m..s
  104 	    85 	 0.07733 	 0.04959 	 ~...
   37 	    86 	 0.05324 	 0.06684 	 ~...
   86 	    87 	 0.05815 	 0.06915 	 ~...
   15 	    88 	 0.04940 	 0.06957 	 ~...
   51 	    89 	 0.05431 	 0.07079 	 ~...
   19 	    90 	 0.04972 	 0.07189 	 ~...
   90 	    91 	 0.05891 	 0.07221 	 ~...
   44 	    92 	 0.05399 	 0.07284 	 ~...
   17 	    93 	 0.04950 	 0.07296 	 ~...
  102 	    94 	 0.06497 	 0.07307 	 ~...
   85 	    95 	 0.05799 	 0.07334 	 ~...
   99 	    96 	 0.06389 	 0.07675 	 ~...
  103 	    97 	 0.06558 	 0.07948 	 ~...
   47 	    98 	 0.05411 	 0.07981 	 ~...
   57 	    99 	 0.05454 	 0.08449 	 ~...
   43 	   100 	 0.05393 	 0.08500 	 m..s
   41 	   101 	 0.05386 	 0.09031 	 m..s
   97 	   102 	 0.06363 	 0.09449 	 m..s
  105 	   103 	 0.07809 	 0.10244 	 ~...
  101 	   104 	 0.06489 	 0.10378 	 m..s
   98 	   105 	 0.06385 	 0.10416 	 m..s
  109 	   106 	 0.13997 	 0.10850 	 m..s
  108 	   107 	 0.13408 	 0.13256 	 ~...
  106 	   108 	 0.12788 	 0.13906 	 ~...
  114 	   109 	 0.20631 	 0.16228 	 m..s
  107 	   110 	 0.13133 	 0.16553 	 m..s
  114 	   111 	 0.20631 	 0.16794 	 m..s
  114 	   112 	 0.20631 	 0.17068 	 m..s
  111 	   113 	 0.19940 	 0.24378 	 m..s
  111 	   114 	 0.19940 	 0.24510 	 m..s
  110 	   115 	 0.18950 	 0.26070 	 m..s
  113 	   116 	 0.20137 	 0.27307 	 m..s
  119 	   117 	 0.26187 	 0.27717 	 ~...
  120 	   118 	 0.26374 	 0.29805 	 m..s
  118 	   119 	 0.21893 	 0.32392 	 MISS
  117 	   120 	 0.21631 	 0.35511 	 MISS
==========================================
r_mrr = 0.9078283905982971
r2_mrr = 0.5953218936920166
spearmanr_mrr@5 = 0.9363641738891602
spearmanr_mrr@10 = 0.8235681056976318
spearmanr_mrr@50 = 0.9352120757102966
spearmanr_mrr@100 = 0.9394912719726562
spearmanr_mrr@All = 0.9425568580627441
==========================================
test time: 0.446
Done Testing dataset OpenEA
total time taken: 251.9939889907837
training time taken: 226.05863904953003
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.9078)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5953)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9364)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.8236)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9352)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9395)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9426)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.6274241306164186}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 6947424689162423
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [680, 187, 1195, 1209, 706, 634, 65, 1104, 1144, 354, 186, 20, 889, 956, 34, 437, 393, 13, 239, 163, 1121, 129, 1153, 100, 873, 1125, 640, 135, 519, 692, 1143, 281, 905, 702, 531, 795, 472, 767, 215, 805, 227, 765, 97, 310, 548, 755, 684, 182, 976, 323, 149, 555, 991, 1028, 433, 502, 119, 544, 931, 708, 448, 1064, 619, 188, 256, 56, 52, 642, 987, 730, 605, 726, 514, 184, 543, 985, 1006, 710, 462, 107, 960, 255, 403, 496, 970, 830, 290, 971, 452, 379, 880, 817, 820, 655, 404, 269, 1050, 371, 274, 953, 370, 1166, 131, 1089, 1155, 580, 174, 1113, 358, 537, 807, 938, 349, 1086, 77, 941, 159, 1182, 681, 739, 45]
valid_ids (0): []
train_ids (1094): [202, 920, 685, 483, 123, 503, 417, 1011, 471, 1184, 3, 615, 67, 900, 212, 746, 198, 291, 355, 745, 1021, 1061, 979, 485, 524, 36, 891, 977, 328, 944, 1134, 588, 788, 427, 989, 1053, 487, 796, 439, 850, 939, 731, 787, 528, 568, 579, 718, 943, 1042, 424, 23, 1157, 398, 1024, 272, 651, 654, 1025, 1140, 848, 99, 425, 832, 968, 1126, 165, 553, 141, 887, 1213, 972, 384, 352, 311, 564, 451, 459, 1117, 583, 1022, 614, 336, 88, 108, 645, 912, 895, 11, 828, 219, 922, 41, 539, 280, 855, 80, 675, 205, 673, 1000, 993, 647, 676, 683, 735, 742, 810, 857, 842, 203, 698, 901, 1099, 264, 266, 1069, 94, 789, 999, 570, 1062, 286, 1167, 587, 443, 183, 1014, 378, 1047, 1201, 672, 824, 136, 1199, 919, 854, 663, 407, 516, 2, 162, 1194, 969, 558, 390, 933, 566, 415, 1039, 102, 150, 966, 696, 846, 46, 319, 813, 497, 192, 156, 259, 1204, 493, 1015, 1004, 662, 701, 76, 986, 14, 885, 177, 721, 190, 74, 73, 5, 957, 500, 575, 597, 997, 888, 126, 1055, 581, 152, 1188, 1038, 760, 603, 703, 9, 118, 1116, 963, 779, 978, 722, 24, 83, 777, 438, 302, 171, 27, 340, 1138, 728, 1177, 1081, 1163, 1109, 418, 7, 373, 1192, 374, 321, 1072, 1070, 248, 940, 284, 312, 87, 643, 1026, 1149, 4, 535, 406, 526, 506, 909, 426, 271, 623, 567, 911, 392, 786, 1020, 545, 275, 1178, 447, 515, 372, 865, 902, 361, 952, 723, 649, 608, 444, 862, 1058, 430, 1123, 229, 1137, 1044, 875, 630, 877, 69, 611, 1068, 195, 737, 1093, 1196, 1171, 273, 324, 556, 921, 679, 343, 251, 552, 429, 756, 839, 12, 1135, 903, 385, 932, 666, 220, 317, 216, 750, 762, 523, 1175, 1100, 768, 278, 359, 1131, 1148, 169, 298, 831, 613, 856, 122, 660, 584, 946, 53, 859, 296, 646, 1008, 160, 185, 1054, 709, 241, 173, 480, 214, 1193, 419, 199, 441, 1190, 181, 127, 874, 422, 207, 360, 369, 1161, 780, 363, 775, 344, 1096, 51, 98, 814, 1165, 1059, 837, 314, 917, 664, 436, 794, 628, 232, 844, 306, 261, 833, 113, 61, 297, 235, 1033, 860, 711, 1208, 180, 863, 157, 39, 1010, 697, 717, 914, 293, 465, 973, 221, 1115, 670, 234, 784, 1052, 213, 937, 481, 428, 890, 823, 1187, 450, 130, 687, 620, 479, 599, 81, 208, 578, 827, 951, 1048, 573, 617, 460, 638, 759, 405, 276, 690, 521, 469, 641, 1073, 397, 766, 197, 283, 179, 456, 58, 476, 498, 959, 95, 154, 994, 294, 1211, 211, 16, 1120, 125, 1156, 246, 288, 849, 6, 408, 980, 674, 299, 534, 389, 82, 851, 870, 300, 204, 1160, 876, 111, 287, 494, 59, 618, 801, 734, 104, 1063, 560, 432, 1046, 380, 1210, 116, 1152, 106, 930, 467, 260, 342, 138, 604, 412, 729, 838, 505, 818, 791, 1019, 1037, 455, 414, 509, 861, 667, 1159, 749, 1075, 103, 333, 490, 461, 132, 1197, 339, 258, 413, 1207, 489, 650, 1023, 399, 947, 91, 542, 883, 1085, 879, 50, 804, 364, 1118, 326, 350, 338, 929, 279, 318, 893, 1092, 84, 96, 172, 1066, 665, 48, 852, 1007, 38, 764, 700, 598, 43, 247, 453, 693, 508, 231, 1133, 836, 945, 1082, 1173, 572, 1198, 44, 341, 1127, 602, 1017, 704, 401, 466, 1110, 1108, 377, 1150, 409, 715, 238, 1147, 420, 621, 562, 771, 716, 178, 847, 688, 411, 304, 1056, 529, 906, 520, 170, 327, 240, 538, 1076, 499, 536, 1087, 591, 858, 49, 1111, 25, 253, 1016, 151, 918, 153, 896, 206, 974, 86, 29, 934, 1067, 793, 0, 1060, 935, 797, 1083, 586, 478, 301, 1141, 164, 714, 292, 309, 1128, 686, 1130, 353, 201, 1049, 217, 228, 778, 774, 965, 637, 809, 593, 362, 601, 511, 1088, 594, 744, 431, 334, 1091, 267, 325, 936, 892, 223, 513, 30, 996, 741, 926, 470, 950, 31, 329, 886, 434, 772, 47, 899, 995, 904, 268, 908, 482, 845, 368, 386, 866, 316, 143, 798, 622, 610, 1124, 1158, 840, 659, 32, 134, 33, 961, 639, 725, 872, 561, 191, 829, 17, 803, 168, 585, 491, 501, 1095, 1103, 923, 492, 609, 592, 282, 1106, 1029, 1174, 694, 320, 28, 554, 115, 388, 55, 758, 525, 1119, 569, 1094, 120, 773, 982, 137, 488, 245, 616, 236, 913, 458, 1105, 332, 607, 563, 648, 92, 210, 1186, 967, 1214, 357, 313, 189, 224, 475, 815, 477, 954, 927, 295, 15, 747, 486, 1018, 990, 1036, 1102, 175, 60, 468, 233, 1202, 89, 1162, 330, 606, 770, 565, 1079, 93, 507, 1002, 816, 423, 416, 910, 834, 1146, 669, 148, 1098, 806, 75, 63, 1142, 376, 66, 656, 10, 1203, 166, 85, 551, 736, 547, 22, 769, 668, 121, 707, 652, 395, 1189, 345, 72, 557, 1090, 1077, 1041, 577, 8, 624, 699, 1057, 464, 1078, 161, 532, 691, 445, 924, 695, 209, 671, 315, 1145, 819, 644, 263, 1185, 517, 252, 79, 265, 719, 864, 595, 442, 661, 600, 1009, 1206, 367, 732, 351, 800, 992, 146, 1043, 915, 1114, 826, 303, 1170, 869, 811, 571, 783, 1132, 139, 394, 381, 867, 225, 196, 112, 155, 942, 712, 790, 193, 754, 1101, 218, 881, 907, 1176, 504, 981, 550, 365, 705, 449, 26, 230, 308, 955, 446, 21, 495, 1034, 249, 1191, 18, 743, 549, 1045, 257, 1084, 474, 1097, 1112, 402, 894, 871, 1169, 792, 176, 305, 761, 677, 738, 142, 101, 878, 117, 1001, 1, 105, 337, 625, 853, 90, 753, 988, 147, 78, 626, 62, 1151, 1212, 574, 612, 1003, 821, 812, 627, 1168, 133, 799, 140, 897, 391, 1179, 653, 1027, 421, 1030, 158, 410, 382, 1065, 958, 733, 802, 1032, 484, 882, 720, 916, 396, 678, 540, 1074, 243, 37, 454, 194, 242, 763, 400, 387, 109, 782, 1013, 1180, 64, 383, 1181, 322, 1035, 636, 658, 254, 40, 335, 463, 781, 262, 998, 713, 962, 657, 776, 682, 751, 727, 740, 114, 785, 757, 596, 331, 822, 35, 128, 57, 541, 632, 530, 843, 250, 1154, 589, 167, 510, 522, 226, 1164, 110, 346, 440, 576, 356, 222, 1012, 1200, 144, 590, 925, 949, 473, 457, 635, 983, 68, 54, 1139, 975, 1183, 200, 582, 347, 289, 868, 546, 42, 71, 270, 375, 285, 1205, 1071, 808, 984, 1107, 1040, 1136, 724, 884, 928, 559, 841, 825, 145, 689, 518, 948, 1051, 512, 1005, 19, 964, 1031, 527, 631, 629, 307, 237, 633, 533, 748, 277, 1129, 898, 752, 244, 70, 1122, 1080, 1172, 435, 348, 366, 835, 124]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8924187220196391
the save name prefix for this run is:  chkpt-ID_8924187220196391_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 579
rank avg (pred): 0.462 +- 0.005
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.000133469

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 901
rank avg (pred): 0.431 +- 0.002
mrr vals (pred, true): 0.000, 0.003
batch losses (mrrl, rdl): 0.0, 0.0007578877

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 446
rank avg (pred): 0.494 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001138389

Epoch over!
epoch time: 14.967

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 737
rank avg (pred): 0.234 +- 0.001
mrr vals (pred, true): 0.000, 0.017
batch losses (mrrl, rdl): 0.0, 0.0001844516

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 855
rank avg (pred): 0.497 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001098747

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 285
rank avg (pred): 0.354 +- 0.002
mrr vals (pred, true): 0.000, 0.079
batch losses (mrrl, rdl): 0.0, 0.0001644505

Epoch over!
epoch time: 14.91

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.498 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001123595

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 896
rank avg (pred): 0.544 +- 0.002
mrr vals (pred, true): 0.000, 0.013
batch losses (mrrl, rdl): 0.0, 0.000236127

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1185
rank avg (pred): 0.498 +- 0.000
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001121178

Epoch over!
epoch time: 14.904

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 315
rank avg (pred): 0.273 +- 0.001
mrr vals (pred, true): 0.000, 0.074
batch losses (mrrl, rdl): 0.0, 0.0002957007

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 23
rank avg (pred): 0.152 +- 0.002
mrr vals (pred, true): 0.000, 0.295
batch losses (mrrl, rdl): 0.0, 8.22229e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 625
rank avg (pred): 0.502 +- 0.000
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001141034

Epoch over!
epoch time: 14.92

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 542
rank avg (pred): 0.374 +- 0.001
mrr vals (pred, true): 0.000, 0.076
batch losses (mrrl, rdl): 0.0, 0.0001588104

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 234
rank avg (pred): 0.499 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001104854

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 695
rank avg (pred): 0.500 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001087271

Epoch over!
epoch time: 14.917

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.499 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0248666648, 0.0001101837

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 251
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.013, 0.131
batch losses (mrrl, rdl): 0.1393286735, 0.0016892889

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 264
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.135, 0.258
batch losses (mrrl, rdl): 0.1519394368, 0.0001945208

Epoch over!
epoch time: 15.146

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1119
rank avg (pred): 0.486 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0248631351, 0.0001179007

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 624
rank avg (pred): 0.573 +- 0.297
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 9.527e-06, 0.0001698196

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 625
rank avg (pred): 0.504 +- 0.261
mrr vals (pred, true): 0.059, 0.002
batch losses (mrrl, rdl): 0.0008960699, 3.20283e-05

Epoch over!
epoch time: 15.142

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 704
rank avg (pred): 0.506 +- 0.261
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.000478149, 3.76605e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.445 +- 0.233
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.12091e-05, 2.99893e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 810
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.243, 0.323
batch losses (mrrl, rdl): 0.0647686124, 0.0004002425

Epoch over!
epoch time: 15.148

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1167
rank avg (pred): 0.468 +- 0.240
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008405412, 1.69414e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 200
rank avg (pred): 0.445 +- 0.262
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.000215831, 1.48065e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 993
rank avg (pred): 0.034 +- 0.020
mrr vals (pred, true): 0.083, 0.104
batch losses (mrrl, rdl): 0.0045720316, 0.0018017452

Epoch over!
epoch time: 15.265

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 76
rank avg (pred): 0.018 +- 0.011
mrr vals (pred, true): 0.105, 0.084
batch losses (mrrl, rdl): 0.0302154031, 0.0022332915

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 304
rank avg (pred): 0.065 +- 0.036
mrr vals (pred, true): 0.059, 0.067
batch losses (mrrl, rdl): 0.0007780615, 0.0019656697

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 836
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.151, 0.194
batch losses (mrrl, rdl): 0.0185931362, 0.0009852273

Epoch over!
epoch time: 15.32

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 973
rank avg (pred): 0.007 +- 0.004
mrr vals (pred, true): 0.115, 0.096
batch losses (mrrl, rdl): 0.0423270911, 0.0021356188

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 85
rank avg (pred): 0.456 +- 0.272
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.0837e-05, 1.05853e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 381
rank avg (pred): 0.407 +- 0.256
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0009910603, 7.8781e-05

Epoch over!
epoch time: 15.169

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.029 +- 0.018
mrr vals (pred, true): 0.085, 0.105
batch losses (mrrl, rdl): 0.0038448907, 0.0018907195

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 728
rank avg (pred): 0.469 +- 0.289
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0005237738, 6.3504e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 362
rank avg (pred): 0.445 +- 0.295
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 2.1302e-06, 1.82273e-05

Epoch over!
epoch time: 15.229

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 520
rank avg (pred): 0.220 +- 0.155
mrr vals (pred, true): 0.063, 0.082
batch losses (mrrl, rdl): 0.0015946093, 0.0003759839

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 625
rank avg (pred): 0.491 +- 0.294
mrr vals (pred, true): 0.039, 0.002
batch losses (mrrl, rdl): 0.0011064617, 1.16358e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 715
rank avg (pred): 0.492 +- 0.312
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.36676e-05, 2.3714e-05

Epoch over!
epoch time: 15.211

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 559
rank avg (pred): 0.043 +- 0.028
mrr vals (pred, true): 0.085, 0.053
batch losses (mrrl, rdl): 0.0125912465, 0.0022924934

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 995
rank avg (pred): 0.062 +- 0.042
mrr vals (pred, true): 0.073, 0.100
batch losses (mrrl, rdl): 0.00535096, 0.0016438774

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 235
rank avg (pred): 0.545 +- 0.322
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.65185e-05, 9.81403e-05

Epoch over!
epoch time: 15.227

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 574
rank avg (pred): 0.418 +- 0.277
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0010495001, 6.79135e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 432
rank avg (pred): 0.503 +- 0.318
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002823725, 2.95505e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 516
rank avg (pred): 0.394 +- 0.248
mrr vals (pred, true): 0.046, 0.075
batch losses (mrrl, rdl): 0.0001330585, 6.61204e-05

Epoch over!
epoch time: 15.231

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.507 +- 0.305
mrr vals (pred, true): 0.047, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.03914 	 0.00044 	 m..s
   42 	     1 	 0.04641 	 0.00044 	 m..s
   77 	     2 	 0.05044 	 0.00049 	 m..s
   29 	     3 	 0.04459 	 0.00049 	 m..s
   24 	     4 	 0.04408 	 0.00053 	 m..s
   63 	     5 	 0.04819 	 0.00055 	 m..s
   66 	     6 	 0.04851 	 0.00055 	 m..s
   12 	     7 	 0.04321 	 0.00056 	 m..s
    0 	     8 	 0.03224 	 0.00057 	 m..s
    9 	     9 	 0.04024 	 0.00057 	 m..s
   62 	    10 	 0.04813 	 0.00057 	 m..s
    1 	    11 	 0.03423 	 0.00058 	 m..s
   75 	    12 	 0.04947 	 0.00058 	 m..s
   72 	    13 	 0.04908 	 0.00059 	 m..s
    4 	    14 	 0.03794 	 0.00060 	 m..s
   46 	    15 	 0.04666 	 0.00060 	 m..s
   43 	    16 	 0.04647 	 0.00060 	 m..s
   15 	    17 	 0.04323 	 0.00061 	 m..s
   61 	    18 	 0.04803 	 0.00063 	 m..s
   41 	    19 	 0.04623 	 0.00063 	 m..s
   50 	    20 	 0.04736 	 0.00065 	 m..s
    2 	    21 	 0.03525 	 0.00065 	 m..s
   76 	    22 	 0.05029 	 0.00065 	 m..s
   44 	    23 	 0.04655 	 0.00066 	 m..s
   53 	    24 	 0.04771 	 0.00067 	 m..s
   10 	    25 	 0.04155 	 0.00068 	 m..s
    8 	    26 	 0.04023 	 0.00069 	 m..s
   54 	    27 	 0.04775 	 0.00070 	 m..s
   22 	    28 	 0.04390 	 0.00070 	 m..s
   73 	    29 	 0.04937 	 0.00072 	 m..s
    6 	    30 	 0.03881 	 0.00074 	 m..s
   49 	    31 	 0.04728 	 0.00077 	 m..s
   39 	    32 	 0.04586 	 0.00078 	 m..s
   60 	    33 	 0.04803 	 0.00081 	 m..s
   13 	    34 	 0.04322 	 0.00081 	 m..s
   36 	    35 	 0.04536 	 0.00082 	 m..s
   27 	    36 	 0.04431 	 0.00085 	 m..s
   17 	    37 	 0.04329 	 0.00087 	 m..s
   68 	    38 	 0.04858 	 0.00087 	 m..s
   23 	    39 	 0.04401 	 0.00088 	 m..s
   51 	    40 	 0.04738 	 0.00088 	 m..s
   64 	    41 	 0.04828 	 0.00089 	 m..s
   56 	    42 	 0.04782 	 0.00089 	 m..s
   14 	    43 	 0.04322 	 0.00092 	 m..s
    5 	    44 	 0.03814 	 0.00093 	 m..s
   40 	    45 	 0.04606 	 0.00093 	 m..s
   11 	    46 	 0.04259 	 0.00095 	 m..s
    3 	    47 	 0.03556 	 0.00097 	 m..s
   48 	    48 	 0.04717 	 0.00101 	 m..s
   52 	    49 	 0.04761 	 0.00101 	 m..s
   74 	    50 	 0.04940 	 0.00101 	 m..s
   55 	    51 	 0.04782 	 0.00102 	 m..s
   34 	    52 	 0.04509 	 0.00108 	 m..s
   45 	    53 	 0.04660 	 0.00114 	 m..s
   21 	    54 	 0.04378 	 0.00122 	 m..s
   65 	    55 	 0.04832 	 0.00126 	 m..s
   30 	    56 	 0.04465 	 0.00129 	 m..s
   69 	    57 	 0.04874 	 0.00130 	 m..s
   67 	    58 	 0.04858 	 0.00132 	 m..s
   33 	    59 	 0.04506 	 0.00132 	 m..s
   38 	    60 	 0.04574 	 0.00137 	 m..s
   71 	    61 	 0.04896 	 0.00141 	 m..s
   18 	    62 	 0.04352 	 0.00142 	 m..s
   37 	    63 	 0.04549 	 0.00144 	 m..s
   26 	    64 	 0.04420 	 0.00146 	 m..s
   58 	    65 	 0.04800 	 0.00148 	 m..s
   28 	    66 	 0.04438 	 0.00150 	 m..s
   32 	    67 	 0.04493 	 0.00153 	 m..s
   20 	    68 	 0.04369 	 0.00159 	 m..s
   70 	    69 	 0.04877 	 0.00170 	 m..s
   31 	    70 	 0.04472 	 0.00170 	 m..s
   47 	    71 	 0.04690 	 0.00176 	 m..s
   35 	    72 	 0.04524 	 0.00186 	 m..s
   59 	    73 	 0.04802 	 0.00207 	 m..s
   25 	    74 	 0.04419 	 0.00208 	 m..s
   78 	    75 	 0.05091 	 0.00239 	 m..s
   79 	    76 	 0.05448 	 0.00249 	 m..s
   57 	    77 	 0.04800 	 0.00263 	 m..s
   19 	    78 	 0.04366 	 0.00267 	 m..s
   16 	    79 	 0.04325 	 0.00350 	 m..s
  106 	    80 	 0.09635 	 0.00638 	 m..s
   94 	    81 	 0.07528 	 0.05253 	 ~...
  100 	    82 	 0.08127 	 0.05670 	 ~...
   92 	    83 	 0.07151 	 0.06405 	 ~...
   96 	    84 	 0.07584 	 0.06832 	 ~...
   90 	    85 	 0.06303 	 0.06878 	 ~...
   87 	    86 	 0.06143 	 0.06887 	 ~...
   85 	    87 	 0.05676 	 0.06957 	 ~...
   86 	    88 	 0.06121 	 0.07472 	 ~...
   84 	    89 	 0.05671 	 0.07621 	 ~...
   80 	    90 	 0.05655 	 0.07633 	 ~...
   80 	    91 	 0.05655 	 0.07732 	 ~...
  105 	    92 	 0.08957 	 0.07752 	 ~...
   80 	    93 	 0.05655 	 0.07802 	 ~...
  103 	    94 	 0.08786 	 0.07867 	 ~...
   97 	    95 	 0.07674 	 0.07946 	 ~...
   98 	    96 	 0.07698 	 0.08027 	 ~...
   88 	    97 	 0.06185 	 0.08094 	 ~...
   95 	    98 	 0.07561 	 0.08176 	 ~...
   80 	    99 	 0.05655 	 0.08260 	 ~...
   99 	   100 	 0.07847 	 0.08269 	 ~...
   93 	   101 	 0.07502 	 0.08301 	 ~...
   91 	   102 	 0.06396 	 0.08451 	 ~...
  104 	   103 	 0.08800 	 0.08523 	 ~...
   89 	   104 	 0.06239 	 0.08827 	 ~...
  102 	   105 	 0.08365 	 0.08960 	 ~...
  101 	   106 	 0.08128 	 0.10101 	 ~...
  108 	   107 	 0.11545 	 0.12472 	 ~...
  110 	   108 	 0.11889 	 0.12940 	 ~...
  109 	   109 	 0.11630 	 0.13040 	 ~...
  112 	   110 	 0.13017 	 0.13377 	 ~...
  113 	   111 	 0.13651 	 0.14416 	 ~...
  111 	   112 	 0.12976 	 0.17901 	 m..s
  107 	   113 	 0.11198 	 0.18061 	 m..s
  116 	   114 	 0.17309 	 0.19048 	 ~...
  114 	   115 	 0.15739 	 0.25594 	 m..s
  117 	   116 	 0.17382 	 0.26112 	 m..s
  115 	   117 	 0.16600 	 0.27154 	 MISS
  120 	   118 	 0.33836 	 0.29051 	 m..s
  118 	   119 	 0.24995 	 0.29555 	 m..s
  119 	   120 	 0.28218 	 0.31921 	 m..s
==========================================
r_mrr = 0.9191290736198425
r2_mrr = 0.6548910140991211
spearmanr_mrr@5 = 0.9814993739128113
spearmanr_mrr@10 = 0.8494020104408264
spearmanr_mrr@50 = 0.9319980144500732
spearmanr_mrr@100 = 0.9390770792961121
spearmanr_mrr@All = 0.9425818920135498
==========================================
test time: 0.448
Done Testing dataset OpenEA
total time taken: 254.3441252708435
training time taken: 227.1636905670166
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.9191)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.6549)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9815)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.8494)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9320)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9391)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9426)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.6514181823795298}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 248854857392949
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1211, 1039, 1024, 1045, 346, 868, 1178, 486, 972, 559, 1005, 586, 345, 910, 240, 697, 1016, 473, 276, 544, 785, 923, 712, 422, 1091, 851, 191, 1193, 284, 156, 135, 1046, 788, 74, 857, 1190, 214, 650, 930, 85, 1022, 321, 366, 328, 37, 509, 558, 954, 1185, 208, 1047, 980, 1172, 225, 455, 695, 897, 320, 822, 1173, 853, 836, 304, 1195, 978, 783, 905, 453, 1034, 1114, 1101, 431, 901, 16, 1062, 1199, 1187, 298, 67, 680, 1205, 247, 339, 909, 57, 1023, 464, 59, 663, 975, 160, 662, 230, 55, 125, 743, 89, 1127, 1036, 831, 1021, 1043, 928, 686, 382, 1017, 1037, 361, 893, 171, 234, 206, 931, 823, 102, 362, 844, 1176, 1029, 1133, 848]
valid_ids (0): []
train_ids (1094): [229, 974, 802, 1140, 572, 766, 643, 891, 161, 46, 621, 35, 651, 342, 1074, 392, 1106, 1111, 458, 405, 219, 237, 1166, 879, 541, 854, 424, 23, 878, 472, 68, 791, 270, 924, 1148, 301, 294, 657, 18, 114, 1104, 146, 614, 71, 988, 251, 306, 113, 981, 62, 120, 631, 952, 263, 871, 655, 1159, 497, 772, 505, 460, 122, 1000, 86, 757, 996, 810, 1164, 186, 211, 202, 52, 1038, 1009, 606, 282, 167, 212, 319, 790, 737, 491, 373, 428, 725, 636, 1081, 136, 646, 872, 805, 729, 259, 960, 986, 1060, 1150, 384, 956, 970, 1126, 1213, 1189, 1082, 898, 756, 637, 1174, 285, 203, 222, 1144, 951, 730, 439, 530, 959, 581, 529, 315, 65, 1085, 358, 163, 781, 717, 661, 1069, 798, 40, 91, 469, 900, 421, 476, 704, 1160, 927, 890, 1063, 579, 850, 554, 795, 387, 971, 832, 0, 538, 42, 170, 95, 312, 349, 552, 946, 88, 393, 17, 296, 451, 741, 1117, 13, 847, 773, 876, 999, 896, 243, 417, 326, 1033, 944, 1067, 112, 934, 261, 1068, 1151, 852, 1170, 198, 1026, 985, 779, 565, 82, 601, 624, 875, 1027, 882, 334, 410, 752, 919, 61, 484, 111, 258, 808, 322, 341, 1025, 533, 204, 815, 244, 97, 1186, 997, 443, 188, 1102, 1110, 696, 286, 1049, 197, 589, 327, 635, 706, 902, 1059, 765, 1012, 277, 300, 1157, 682, 653, 1184, 746, 892, 1162, 31, 49, 307, 673, 96, 169, 827, 11, 659, 448, 691, 152, 364, 351, 649, 209, 271, 668, 739, 1212, 465, 1080, 1115, 179, 196, 137, 175, 652, 1031, 224, 47, 1096, 543, 1073, 189, 913, 354, 218, 969, 545, 596, 837, 709, 758, 498, 964, 488, 1154, 724, 966, 426, 54, 516, 1077, 15, 512, 19, 908, 1145, 178, 1122, 1105, 866, 671, 679, 1057, 575, 518, 616, 314, 423, 53, 444, 246, 76, 926, 963, 627, 1015, 289, 618, 1138, 1194, 904, 714, 887, 602, 45, 1191, 733, 845, 69, 195, 703, 1055, 140, 941, 654, 1072, 744, 490, 12, 800, 429, 28, 479, 617, 493, 953, 674, 480, 993, 22, 861, 990, 563, 194, 690, 399, 94, 1208, 1196, 587, 440, 406, 705, 580, 401, 1130, 1135, 478, 1088, 1116, 1070, 979, 281, 310, 80, 408, 1093, 846, 1206, 711, 376, 355, 98, 1120, 600, 238, 75, 605, 820, 461, 58, 819, 998, 26, 797, 93, 275, 403, 590, 814, 502, 976, 867, 522, 534, 164, 1078, 736, 60, 269, 1134, 253, 553, 430, 692, 681, 159, 548, 666, 1201, 391, 266, 531, 1066, 367, 172, 803, 526, 597, 1006, 335, 279, 557, 475, 368, 254, 940, 442, 914, 1020, 4, 574, 571, 105, 449, 148, 760, 241, 1041, 825, 1058, 280, 436, 166, 267, 1198, 829, 777, 684, 492, 297, 347, 402, 268, 1108, 793, 119, 48, 495, 496, 916, 374, 799, 1139, 27, 100, 955, 948, 239, 157, 344, 221, 231, 824, 363, 457, 155, 912, 886, 834, 1032, 1147, 371, 216, 332, 1118, 701, 550, 720, 1042, 745, 644, 419, 883, 806, 626, 794, 818, 884, 1103, 1065, 713, 353, 380, 710, 459, 856, 379, 608, 1119, 860, 501, 201, 456, 412, 523, 400, 610, 787, 607, 283, 370, 950, 865, 642, 639, 915, 356, 217, 468, 911, 200, 1136, 1094, 228, 1050, 1207, 124, 386, 775, 427, 292, 467, 1052, 994, 6, 1188, 8, 336, 716, 107, 390, 132, 835, 1149, 513, 236, 984, 641, 205, 761, 540, 176, 1123, 907, 723, 769, 658, 1087, 450, 973, 369, 29, 1161, 255, 1203, 735, 570, 32, 123, 629, 595, 982, 311, 957, 942, 562, 1053, 149, 1141, 708, 72, 1125, 154, 598, 308, 1056, 564, 372, 508, 784, 903, 360, 535, 287, 707, 1167, 494, 1156, 129, 647, 774, 759, 992, 260, 862, 226, 256, 1181, 223, 732, 1097, 395, 265, 1177, 1202, 833, 471, 274, 25, 1044, 740, 133, 210, 435, 329, 546, 452, 323, 539, 619, 889, 511, 1210, 117, 551, 66, 165, 375, 573, 536, 937, 388, 858, 676, 1165, 1197, 193, 965, 1183, 764, 272, 77, 173, 1086, 989, 295, 183, 1048, 299, 935, 1040, 63, 507, 863, 325, 670, 1168, 1018, 357, 625, 525, 843, 1004, 1002, 762, 138, 313, 103, 87, 1095, 153, 925, 591, 252, 192, 667, 394, 750, 151, 660, 5, 343, 599, 1214, 447, 1146, 318, 288, 104, 524, 411, 1008, 877, 177, 592, 36, 232, 885, 477, 1152, 213, 583, 144, 184, 734, 398, 264, 542, 101, 273, 338, 78, 895, 630, 73, 638, 1099, 977, 83, 51, 420, 1142, 888, 180, 500, 330, 1011, 147, 532, 1175, 121, 807, 90, 727, 849, 1155, 109, 547, 499, 503, 1171, 162, 881, 770, 158, 812, 1131, 967, 612, 816, 1019, 10, 396, 632, 126, 434, 813, 728, 1137, 1132, 39, 142, 1054, 463, 672, 894, 958, 1010, 873, 350, 482, 418, 483, 932, 506, 567, 220, 1128, 613, 722, 38, 603, 921, 995, 859, 731, 207, 1013, 1180, 754, 316, 584, 576, 1100, 780, 317, 1169, 1076, 604, 947, 675, 792, 474, 487, 1179, 1153, 755, 445, 811, 520, 748, 1192, 1084, 839, 446, 1089, 470, 1003, 917, 593, 648, 749, 181, 702, 880, 9, 961, 233, 519, 56, 462, 389, 348, 21, 383, 687, 128, 929, 409, 830, 416, 782, 767, 385, 127, 768, 987, 840, 1075, 585, 141, 397, 415, 333, 801, 577, 182, 809, 106, 1007, 771, 485, 249, 309, 116, 582, 262, 293, 664, 555, 185, 517, 804, 1035, 1083, 786, 70, 174, 1121, 1071, 404, 425, 763, 215, 568, 337, 1143, 150, 1051, 821, 623, 1124, 1079, 870, 515, 3, 700, 527, 227, 1028, 139, 1113, 906, 826, 1200, 698, 685, 939, 751, 689, 110, 556, 145, 566, 1129, 694, 44, 441, 1092, 79, 968, 413, 528, 433, 874, 1163, 796, 699, 1014, 949, 290, 628, 715, 721, 719, 108, 1001, 432, 678, 1109, 352, 594, 41, 250, 521, 438, 377, 381, 1064, 115, 1158, 669, 359, 130, 936, 869, 168, 726, 34, 609, 828, 945, 842, 1112, 278, 753, 560, 131, 365, 134, 1090, 578, 645, 7, 688, 81, 64, 437, 143, 569, 841, 615, 738, 1107, 303, 991, 43, 331, 864, 30, 481, 778, 305, 84, 665, 199, 33, 257, 414, 24, 245, 99, 187, 611, 817, 933, 838, 489, 789, 633, 1061, 855, 983, 190, 378, 514, 504, 454, 537, 291, 918, 588, 747, 302, 943, 340, 634, 920, 693, 50, 922, 324, 1204, 640, 656, 1182, 510, 407, 677, 118, 92, 20, 248, 14, 938, 242, 620, 1098, 1, 1030, 235, 899, 2, 742, 466, 962, 1209, 622, 549, 561, 683, 718, 776]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6506170420749743
the save name prefix for this run is:  chkpt-ID_6506170420749743_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 350
rank avg (pred): 0.554 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001916503

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 684
rank avg (pred): 0.451 +- 0.269
mrr vals (pred, true): 0.145, 0.001
batch losses (mrrl, rdl): 0.0, 1.51778e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 732
rank avg (pred): 0.185 +- 0.125
mrr vals (pred, true): 0.219, 0.342
batch losses (mrrl, rdl): 0.0, 0.0002755846

Epoch over!
epoch time: 15.06

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1074
rank avg (pred): 0.282 +- 0.195
mrr vals (pred, true): 0.227, 0.103
batch losses (mrrl, rdl): 0.0, 7.96211e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 267
rank avg (pred): 0.144 +- 0.101
mrr vals (pred, true): 0.225, 0.264
batch losses (mrrl, rdl): 0.0, 0.0001051733

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1078
rank avg (pred): 0.305 +- 0.231
mrr vals (pred, true): 0.229, 0.129
batch losses (mrrl, rdl): 0.0, 4.44467e-05

Epoch over!
epoch time: 14.985

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 248
rank avg (pred): 0.237 +- 0.170
mrr vals (pred, true): 0.204, 0.130
batch losses (mrrl, rdl): 0.0, 0.0001065975

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 517
rank avg (pred): 0.349 +- 0.248
mrr vals (pred, true): 0.186, 0.075
batch losses (mrrl, rdl): 0.0, 3.43223e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 688
rank avg (pred): 0.471 +- 0.303
mrr vals (pred, true): 0.143, 0.001
batch losses (mrrl, rdl): 0.0, 1.23662e-05

Epoch over!
epoch time: 14.999

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 375
rank avg (pred): 0.467 +- 0.300
mrr vals (pred, true): 0.121, 0.001
batch losses (mrrl, rdl): 0.0, 1.05562e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 780
rank avg (pred): 0.479 +- 0.293
mrr vals (pred, true): 0.093, 0.001
batch losses (mrrl, rdl): 0.0, 8.0856e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 409
rank avg (pred): 0.418 +- 0.317
mrr vals (pred, true): 0.134, 0.001
batch losses (mrrl, rdl): 0.0, 5.91313e-05

Epoch over!
epoch time: 15.074

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 920
rank avg (pred): 0.498 +- 0.287
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0, 1.37555e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 185
rank avg (pred): 0.468 +- 0.304
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0, 6.7959e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 311
rank avg (pred): 0.365 +- 0.284
mrr vals (pred, true): 0.054, 0.079
batch losses (mrrl, rdl): 0.0, 3.65153e-05

Epoch over!
epoch time: 14.916

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 374
rank avg (pred): 0.487 +- 0.292
mrr vals (pred, true): 0.026, 0.001
batch losses (mrrl, rdl): 0.0056887073, 6.8352e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 671
rank avg (pred): 0.186 +- 0.223
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0152398814, 0.0016561673

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 33
rank avg (pred): 0.399 +- 0.245
mrr vals (pred, true): 0.067, 0.071
batch losses (mrrl, rdl): 0.0028465232, 7.67407e-05

Epoch over!
epoch time: 15.107

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.362 +- 0.286
mrr vals (pred, true): 0.070, 0.002
batch losses (mrrl, rdl): 0.0039905403, 0.0002539632

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 803
rank avg (pred): 0.601 +- 0.232
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0018348729, 0.0002390201

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 398
rank avg (pred): 0.546 +- 0.251
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 6.6319e-06, 0.000106096

Epoch over!
epoch time: 15.089

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 814
rank avg (pred): 0.003 +- 0.006
mrr vals (pred, true): 0.370, 0.349
batch losses (mrrl, rdl): 0.0043897945, 9.02883e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 565
rank avg (pred): 0.425 +- 0.257
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 4.638e-06, 0.0001016061

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 756
rank avg (pred): 0.538 +- 0.255
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 6.1014e-06, 6.52182e-05

Epoch over!
epoch time: 15.085

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1192
rank avg (pred): 0.590 +- 0.240
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001751537, 0.0002172552

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1140
rank avg (pred): 0.279 +- 0.287
mrr vals (pred, true): 0.104, 0.183
batch losses (mrrl, rdl): 0.0620042682, 6.8362e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 142
rank avg (pred): 0.541 +- 0.239
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0009675825, 7.83781e-05

Epoch over!
epoch time: 15.089

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 64
rank avg (pred): 0.332 +- 0.278
mrr vals (pred, true): 0.047, 0.086
batch losses (mrrl, rdl): 7.61158e-05, 5.2917e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 702
rank avg (pred): 0.569 +- 0.222
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0019477915, 0.0001653883

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 704
rank avg (pred): 0.498 +- 0.234
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0005801217, 2.03706e-05

Epoch over!
epoch time: 15.098

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 850
rank avg (pred): 0.483 +- 0.254
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001522656, 8.7564e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 150
rank avg (pred): 0.429 +- 0.252
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 6.92223e-05, 5.81617e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 310
rank avg (pred): 0.247 +- 0.277
mrr vals (pred, true): 0.064, 0.081
batch losses (mrrl, rdl): 0.0019054012, 0.0001389943

Epoch over!
epoch time: 15.084

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 285
rank avg (pred): 0.337 +- 0.279
mrr vals (pred, true): 0.047, 0.079
batch losses (mrrl, rdl): 7.31177e-05, 4.9541e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 221
rank avg (pred): 0.499 +- 0.248
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.000106012, 1.33825e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 281
rank avg (pred): 0.320 +- 0.295
mrr vals (pred, true): 0.057, 0.088
batch losses (mrrl, rdl): 0.0005230677, 4.7309e-06

Epoch over!
epoch time: 15.099

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 865
rank avg (pred): 0.474 +- 0.248
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.69254e-05, 1.25217e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 915
rank avg (pred): 0.450 +- 0.291
mrr vals (pred, true): 0.084, 0.000
batch losses (mrrl, rdl): 0.0116824871, 0.0005579345

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 499
rank avg (pred): 0.389 +- 0.289
mrr vals (pred, true): 0.087, 0.180
batch losses (mrrl, rdl): 0.0859904438, 0.0005143874

Epoch over!
epoch time: 15.051

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1059
rank avg (pred): 0.211 +- 0.239
mrr vals (pred, true): 0.206, 0.168
batch losses (mrrl, rdl): 0.0143911205, 5.76278e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 410
rank avg (pred): 0.469 +- 0.266
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0004209524, 9.6532e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 219
rank avg (pred): 0.465 +- 0.243
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.000392134, 1.80438e-05

Epoch over!
epoch time: 15.094

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.474 +- 0.257
mrr vals (pred, true): 0.068, 0.080
batch losses (mrrl, rdl): 0.0032730142, 0.0004142342

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 86
rank avg (pred): 0.447 +- 0.265
mrr vals (pred, true): 0.066, 0.001
batch losses (mrrl, rdl): 0.0024251528, 1.41112e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.472 +- 0.238
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 7.1435e-06, 1.67906e-05

Epoch over!
epoch time: 14.988

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.502 +- 0.220
mrr vals (pred, true): 0.044, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  102 	     0 	 0.04952 	 0.00013 	 m..s
   63 	     1 	 0.04409 	 0.00045 	 m..s
   99 	     2 	 0.04914 	 0.00047 	 m..s
   79 	     3 	 0.04599 	 0.00049 	 m..s
   72 	     4 	 0.04450 	 0.00050 	 m..s
    3 	     5 	 0.03840 	 0.00051 	 m..s
   35 	     6 	 0.04195 	 0.00051 	 m..s
  103 	     7 	 0.04959 	 0.00051 	 m..s
   23 	     8 	 0.04087 	 0.00052 	 m..s
   16 	     9 	 0.04020 	 0.00053 	 m..s
   42 	    10 	 0.04258 	 0.00053 	 m..s
  100 	    11 	 0.04926 	 0.00053 	 m..s
   76 	    12 	 0.04532 	 0.00054 	 m..s
   93 	    13 	 0.04843 	 0.00054 	 m..s
   17 	    14 	 0.04026 	 0.00056 	 m..s
   82 	    15 	 0.04637 	 0.00056 	 m..s
    0 	    16 	 0.03577 	 0.00056 	 m..s
   10 	    17 	 0.03986 	 0.00056 	 m..s
    6 	    18 	 0.03889 	 0.00057 	 m..s
    9 	    19 	 0.03953 	 0.00057 	 m..s
   74 	    20 	 0.04485 	 0.00058 	 m..s
   64 	    21 	 0.04413 	 0.00058 	 m..s
   56 	    22 	 0.04328 	 0.00058 	 m..s
   37 	    23 	 0.04224 	 0.00058 	 m..s
   71 	    24 	 0.04441 	 0.00059 	 m..s
   58 	    25 	 0.04367 	 0.00061 	 m..s
   12 	    26 	 0.04005 	 0.00061 	 m..s
   68 	    27 	 0.04437 	 0.00063 	 m..s
   70 	    28 	 0.04439 	 0.00064 	 m..s
   86 	    29 	 0.04721 	 0.00064 	 m..s
   15 	    30 	 0.04019 	 0.00068 	 m..s
   24 	    31 	 0.04105 	 0.00069 	 m..s
   13 	    32 	 0.04015 	 0.00069 	 m..s
   40 	    33 	 0.04249 	 0.00070 	 m..s
   29 	    34 	 0.04129 	 0.00070 	 m..s
   80 	    35 	 0.04600 	 0.00071 	 m..s
    4 	    36 	 0.03849 	 0.00072 	 m..s
   30 	    37 	 0.04134 	 0.00072 	 m..s
    5 	    38 	 0.03851 	 0.00073 	 m..s
   57 	    39 	 0.04342 	 0.00073 	 m..s
   65 	    40 	 0.04414 	 0.00073 	 m..s
   34 	    41 	 0.04194 	 0.00074 	 m..s
   41 	    42 	 0.04251 	 0.00076 	 m..s
   85 	    43 	 0.04716 	 0.00081 	 m..s
   21 	    44 	 0.04046 	 0.00083 	 m..s
   83 	    45 	 0.04687 	 0.00084 	 m..s
   18 	    46 	 0.04028 	 0.00085 	 m..s
   66 	    47 	 0.04417 	 0.00085 	 m..s
   69 	    48 	 0.04438 	 0.00089 	 m..s
   22 	    49 	 0.04083 	 0.00091 	 m..s
    1 	    50 	 0.03827 	 0.00093 	 m..s
   54 	    51 	 0.04310 	 0.00094 	 m..s
   43 	    52 	 0.04263 	 0.00097 	 m..s
    8 	    53 	 0.03932 	 0.00099 	 m..s
   38 	    54 	 0.04227 	 0.00099 	 m..s
   32 	    55 	 0.04188 	 0.00099 	 m..s
   88 	    56 	 0.04751 	 0.00100 	 m..s
    7 	    57 	 0.03910 	 0.00101 	 m..s
   60 	    58 	 0.04379 	 0.00101 	 m..s
   19 	    59 	 0.04029 	 0.00106 	 m..s
   39 	    60 	 0.04242 	 0.00106 	 m..s
   81 	    61 	 0.04628 	 0.00107 	 m..s
   62 	    62 	 0.04407 	 0.00110 	 m..s
   47 	    63 	 0.04280 	 0.00123 	 m..s
   14 	    64 	 0.04016 	 0.00125 	 m..s
   77 	    65 	 0.04541 	 0.00126 	 m..s
   11 	    66 	 0.04001 	 0.00132 	 m..s
   48 	    67 	 0.04283 	 0.00142 	 m..s
   27 	    68 	 0.04118 	 0.00143 	 m..s
   90 	    69 	 0.04794 	 0.00146 	 m..s
   84 	    70 	 0.04714 	 0.00148 	 m..s
   20 	    71 	 0.04033 	 0.00149 	 m..s
   78 	    72 	 0.04546 	 0.00151 	 m..s
   28 	    73 	 0.04123 	 0.00153 	 m..s
  104 	    74 	 0.05005 	 0.00156 	 m..s
   25 	    75 	 0.04106 	 0.00159 	 m..s
   33 	    76 	 0.04190 	 0.00171 	 m..s
   36 	    77 	 0.04196 	 0.00188 	 m..s
  101 	    78 	 0.04943 	 0.00200 	 m..s
   50 	    79 	 0.04287 	 0.00210 	 m..s
   75 	    80 	 0.04496 	 0.00224 	 m..s
  106 	    81 	 0.05176 	 0.00228 	 m..s
   44 	    82 	 0.04269 	 0.00234 	 m..s
   31 	    83 	 0.04179 	 0.00245 	 m..s
   55 	    84 	 0.04312 	 0.00248 	 m..s
   94 	    85 	 0.04881 	 0.00249 	 m..s
   26 	    86 	 0.04116 	 0.00283 	 m..s
   52 	    87 	 0.04302 	 0.00319 	 m..s
   89 	    88 	 0.04781 	 0.00332 	 m..s
   87 	    89 	 0.04749 	 0.00487 	 m..s
   91 	    90 	 0.04800 	 0.00655 	 m..s
  107 	    91 	 0.06633 	 0.01147 	 m..s
   96 	    92 	 0.04886 	 0.04752 	 ~...
  108 	    93 	 0.07513 	 0.05180 	 ~...
   97 	    94 	 0.04888 	 0.05308 	 ~...
   49 	    95 	 0.04286 	 0.06450 	 ~...
   46 	    96 	 0.04278 	 0.06727 	 ~...
   61 	    97 	 0.04381 	 0.06882 	 ~...
  105 	    98 	 0.05130 	 0.06915 	 ~...
   53 	    99 	 0.04306 	 0.06998 	 ~...
   51 	   100 	 0.04290 	 0.07041 	 ~...
   45 	   101 	 0.04276 	 0.07079 	 ~...
    2 	   102 	 0.03831 	 0.07732 	 m..s
   92 	   103 	 0.04829 	 0.07869 	 m..s
   98 	   104 	 0.04911 	 0.07894 	 ~...
   73 	   105 	 0.04477 	 0.08018 	 m..s
   95 	   106 	 0.04882 	 0.08188 	 m..s
   59 	   107 	 0.04379 	 0.08511 	 m..s
   67 	   108 	 0.04419 	 0.08673 	 m..s
  109 	   109 	 0.08666 	 0.09364 	 ~...
  111 	   110 	 0.10658 	 0.10850 	 ~...
  110 	   111 	 0.09324 	 0.13290 	 m..s
  115 	   112 	 0.14055 	 0.16486 	 ~...
  117 	   113 	 0.15417 	 0.16553 	 ~...
  112 	   114 	 0.12336 	 0.17323 	 m..s
  116 	   115 	 0.15163 	 0.17796 	 ~...
  118 	   116 	 0.15902 	 0.17976 	 ~...
  113 	   117 	 0.12493 	 0.18588 	 m..s
  120 	   118 	 0.18614 	 0.19419 	 ~...
  119 	   119 	 0.18235 	 0.22630 	 m..s
  114 	   120 	 0.13794 	 0.25904 	 MISS
==========================================
r_mrr = 0.8762961626052856
r2_mrr = 0.44229698181152344
spearmanr_mrr@5 = 0.9626815319061279
spearmanr_mrr@10 = 0.9545090794563293
spearmanr_mrr@50 = 0.9235312342643738
spearmanr_mrr@100 = 0.9316990971565247
spearmanr_mrr@All = 0.9348886609077454
==========================================
test time: 0.453
Done Testing dataset OpenEA
total time taken: 253.36022400856018
training time taken: 226.28268575668335
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8763)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.4423)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9627)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9545)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9235)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9317)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9349)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.35263687671977095}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 9809430085648068
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [249, 567, 568, 165, 236, 294, 940, 534, 43, 700, 200, 517, 539, 119, 379, 774, 266, 887, 521, 665, 53, 506, 873, 735, 1199, 1136, 880, 608, 911, 1200, 614, 76, 704, 894, 697, 746, 317, 729, 215, 275, 351, 59, 221, 947, 487, 394, 852, 392, 913, 158, 995, 636, 813, 960, 855, 1127, 677, 457, 789, 1018, 1207, 364, 599, 450, 761, 384, 844, 734, 1056, 966, 156, 357, 603, 826, 1082, 622, 57, 554, 580, 587, 346, 1196, 792, 747, 50, 99, 1094, 128, 621, 646, 917, 155, 1083, 935, 522, 615, 129, 1179, 850, 338, 739, 589, 388, 270, 444, 1081, 27, 254, 975, 963, 1057, 293, 879, 716, 1051, 724, 1000, 967, 14, 781, 39]
valid_ids (0): []
train_ids (1094): [1151, 430, 407, 989, 121, 252, 153, 73, 297, 369, 548, 387, 1111, 992, 328, 222, 157, 721, 843, 173, 490, 47, 1016, 949, 332, 380, 479, 1053, 1166, 519, 762, 1201, 1180, 478, 647, 1108, 118, 1078, 285, 202, 600, 1144, 659, 37, 930, 552, 780, 799, 1045, 239, 234, 1080, 657, 987, 886, 642, 45, 2, 835, 964, 494, 1097, 296, 415, 562, 709, 1021, 925, 997, 1107, 115, 612, 637, 923, 102, 448, 232, 1152, 149, 514, 341, 485, 311, 756, 620, 310, 5, 891, 134, 556, 907, 447, 48, 1154, 518, 875, 1150, 453, 764, 49, 899, 1091, 982, 1187, 1100, 1104, 861, 77, 948, 837, 1041, 1213, 177, 131, 515, 922, 19, 183, 101, 1121, 722, 1113, 810, 24, 736, 182, 359, 313, 206, 1023, 83, 1062, 595, 7, 312, 512, 993, 974, 968, 730, 1095, 634, 8, 315, 1146, 1070, 15, 306, 97, 573, 1020, 924, 1172, 190, 753, 605, 830, 921, 360, 566, 122, 141, 1074, 145, 1130, 583, 890, 290, 148, 495, 941, 508, 240, 154, 1208, 927, 784, 247, 928, 288, 1131, 976, 585, 1064, 832, 225, 1015, 21, 817, 794, 733, 216, 609, 897, 1055, 65, 439, 814, 62, 878, 866, 405, 1077, 549, 1, 708, 523, 606, 871, 1126, 1158, 1147, 227, 208, 300, 1120, 51, 815, 498, 641, 624, 117, 644, 318, 1156, 824, 591, 1195, 936, 1177, 691, 81, 827, 427, 1181, 1068, 888, 437, 586, 96, 1167, 682, 1106, 1205, 12, 147, 648, 785, 330, 749, 1197, 807, 31, 867, 320, 616, 668, 418, 468, 203, 868, 623, 434, 858, 180, 262, 795, 325, 1099, 314, 191, 55, 36, 1019, 272, 486, 802, 1139, 1191, 651, 488, 130, 212, 725, 1138, 425, 1079, 416, 771, 667, 841, 933, 454, 267, 500, 424, 419, 1149, 782, 493, 1001, 981, 653, 635, 957, 461, 1093, 1036, 932, 503, 678, 451, 712, 983, 466, 675, 470, 399, 582, 805, 342, 806, 86, 366, 1028, 449, 618, 446, 740, 885, 251, 669, 690, 139, 825, 819, 662, 1109, 125, 223, 271, 1098, 323, 302, 1054, 198, 391, 246, 598, 322, 1185, 398, 590, 304, 741, 1049, 1164, 538, 1034, 100, 643, 769, 18, 475, 95, 260, 187, 69, 853, 919, 463, 631, 576, 596, 776, 686, 671, 1115, 250, 905, 570, 146, 319, 816, 1169, 460, 396, 483, 1141, 151, 854, 607, 788, 869, 934, 41, 462, 1159, 1198, 877, 684, 710, 1037, 492, 417, 383, 38, 26, 803, 13, 990, 632, 979, 337, 564, 1044, 344, 847, 195, 783, 1137, 1162, 143, 1148, 501, 986, 32, 652, 527, 228, 1046, 939, 857, 279, 253, 985, 496, 105, 124, 674, 565, 1008, 348, 88, 625, 166, 135, 352, 908, 71, 779, 541, 104, 718, 680, 574, 1178, 1043, 1184, 358, 353, 220, 1029, 1006, 1135, 1076, 70, 482, 1087, 382, 1088, 244, 343, 201, 1048, 859, 1063, 876, 308, 441, 849, 355, 257, 906, 276, 1212, 980, 531, 560, 577, 409, 658, 80, 588, 563, 903, 679, 796, 142, 703, 443, 263, 9, 823, 1165, 289, 828, 970, 772, 414, 973, 640, 248, 1209, 836, 112, 1174, 773, 1168, 433, 58, 426, 1030, 110, 901, 687, 1124, 75, 10, 422, 602, 445, 918, 797, 209, 1203, 367, 601, 333, 186, 1142, 1125, 683, 61, 532, 743, 85, 305, 649, 472, 84, 1122, 196, 282, 1005, 261, 133, 831, 1007, 731, 386, 786, 277, 1129, 862, 713, 507, 1105, 412, 403, 226, 865, 1173, 726, 915, 714, 1112, 952, 243, 459, 159, 1022, 872, 1089, 1133, 988, 1110, 889, 1160, 268, 555, 25, 695, 1072, 860, 238, 1067, 745, 1175, 511, 959, 287, 693, 17, 804, 309, 611, 1014, 1052, 845, 1059, 720, 916, 404, 1123, 170, 469, 29, 1075, 224, 1002, 120, 1204, 787, 584, 702, 1040, 301, 1101, 1118, 630, 480, 281, 955, 1186, 937, 210, 676, 1193, 233, 594, 345, 23, 619, 1092, 856, 535, 497, 701, 1206, 558, 72, 436, 1011, 553, 758, 663, 870, 1116, 401, 321, 484, 881, 798, 231, 999, 64, 286, 950, 1214, 349, 205, 138, 1170, 91, 20, 628, 520, 280, 529, 1004, 207, 163, 274, 654, 370, 476, 1060, 1143, 800, 1153, 748, 473, 1024, 1009, 193, 229, 400, 898, 93, 471, 356, 406, 1066, 1031, 579, 108, 113, 895, 435, 965, 1017, 411, 971, 42, 372, 874, 811, 181, 1065, 499, 1190, 126, 1038, 11, 46, 1096, 481, 884, 793, 362, 197, 516, 160, 35, 543, 1183, 984, 581, 6, 455, 4, 199, 1119, 269, 218, 298, 944, 502, 705, 892, 1071, 820, 336, 350, 528, 597, 1032, 685, 375, 1188, 775, 996, 883, 1102, 510, 78, 1010, 1134, 335, 339, 303, 385, 1171, 540, 474, 738, 509, 211, 592, 242, 1176, 699, 137, 395, 1042, 63, 910, 1035, 397, 402, 381, 452, 732, 389, 467, 327, 464, 1058, 377, 777, 109, 938, 1085, 168, 882, 757, 28, 575, 245, 465, 1189, 978, 82, 833, 299, 421, 132, 1117, 89, 52, 778, 744, 442, 767, 656, 639, 283, 544, 106, 692, 750, 954, 264, 1114, 1061, 572, 54, 140, 256, 161, 696, 929, 22, 66, 571, 67, 525, 295, 284, 90, 423, 533, 962, 1025, 629, 664, 74, 16, 670, 926, 34, 961, 956, 127, 645, 920, 638, 904, 909, 144, 331, 545, 717, 505, 136, 742, 537, 791, 324, 185, 896, 536, 184, 162, 361, 1145, 1050, 863, 1155, 661, 846, 1047, 840, 707, 334, 900, 103, 998, 851, 189, 107, 491, 754, 408, 613, 44, 340, 194, 390, 1026, 839, 672, 204, 688, 719, 994, 550, 291, 1013, 546, 1012, 728, 801, 326, 255, 706, 977, 68, 347, 1073, 114, 376, 542, 969, 278, 374, 116, 650, 834, 912, 169, 593, 1140, 373, 92, 413, 838, 1033, 40, 259, 829, 991, 711, 513, 213, 428, 559, 943, 171, 953, 660, 557, 1090, 1161, 60, 87, 727, 1027, 241, 561, 617, 766, 902, 432, 456, 864, 307, 56, 378, 760, 604, 217, 763, 1132, 1128, 931, 94, 1210, 755, 178, 551, 808, 235, 365, 176, 214, 759, 167, 30, 192, 273, 1211, 458, 812, 33, 316, 914, 821, 723, 569, 329, 578, 477, 633, 0, 123, 530, 715, 438, 79, 809, 1194, 410, 946, 524, 152, 737, 393, 681, 150, 1182, 770, 1003, 666, 179, 431, 98, 694, 751, 1103, 429, 842, 626, 752, 627, 673, 174, 175, 848, 972, 354, 822, 768, 292, 1039, 1086, 489, 893, 1084, 689, 3, 790, 230, 172, 698, 188, 111, 237, 1163, 363, 1202, 1192, 164, 818, 440, 258, 504, 547, 371, 765, 526, 958, 420, 265, 655, 945, 1069, 951, 368, 610, 942, 1157, 219]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  738739121042679
the save name prefix for this run is:  chkpt-ID_738739121042679_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 262
rank avg (pred): 0.434 +- 0.018
mrr vals (pred, true): 0.000, 0.287
batch losses (mrrl, rdl): 0.0, 0.0022738236

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 992
rank avg (pred): 0.285 +- 0.200
mrr vals (pred, true): 0.165, 0.071
batch losses (mrrl, rdl): 0.0, 9.21254e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 482
rank avg (pred): 0.470 +- 0.306
mrr vals (pred, true): 0.128, 0.000
batch losses (mrrl, rdl): 0.0, 1.07241e-05

Epoch over!
epoch time: 14.876

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 159
rank avg (pred): 0.475 +- 0.303
mrr vals (pred, true): 0.121, 0.001
batch losses (mrrl, rdl): 0.0, 1.22301e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 821
rank avg (pred): 0.306 +- 0.230
mrr vals (pred, true): 0.151, 0.129
batch losses (mrrl, rdl): 0.0, 4.78086e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 479
rank avg (pred): 0.480 +- 0.309
mrr vals (pred, true): 0.117, 0.001
batch losses (mrrl, rdl): 0.0, 1.14429e-05

Epoch over!
epoch time: 14.926

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 367
rank avg (pred): 0.466 +- 0.293
mrr vals (pred, true): 0.104, 0.001
batch losses (mrrl, rdl): 0.0, 8.0439e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 941
rank avg (pred): 0.505 +- 0.324
mrr vals (pred, true): 0.096, 0.001
batch losses (mrrl, rdl): 0.0, 2.64796e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 345
rank avg (pred): 0.471 +- 0.295
mrr vals (pred, true): 0.076, 0.002
batch losses (mrrl, rdl): 0.0, 5.359e-06

Epoch over!
epoch time: 14.94

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1091
rank avg (pred): 0.490 +- 0.277
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0, 8.9062e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1177
rank avg (pred): 0.486 +- 0.297
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0, 4.1765e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 361
rank avg (pred): 0.475 +- 0.305
mrr vals (pred, true): 0.046, 0.002
batch losses (mrrl, rdl): 0.0, 5.5753e-06

Epoch over!
epoch time: 14.937

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 786
rank avg (pred): 0.485 +- 0.301
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0, 1.623e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 195
rank avg (pred): 0.493 +- 0.292
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0, 2.387e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 872
rank avg (pred): 0.539 +- 0.288
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.0, 1.87114e-05

Epoch over!
epoch time: 14.911

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 33
rank avg (pred): 0.339 +- 0.302
mrr vals (pred, true): 0.054, 0.071
batch losses (mrrl, rdl): 0.0001584668, 3.4677e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1202
rank avg (pred): 0.475 +- 0.182
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.000618519, 5.00234e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1159
rank avg (pred): 0.252 +- 0.249
mrr vals (pred, true): 0.155, 0.140
batch losses (mrrl, rdl): 0.0022297595, 3.14636e-05

Epoch over!
epoch time: 15.143

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.338 +- 0.222
mrr vals (pred, true): 0.097, 0.138
batch losses (mrrl, rdl): 0.0167997945, 9.53667e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.400 +- 0.178
mrr vals (pred, true): 0.068, 0.105
batch losses (mrrl, rdl): 0.0136877457, 0.0002172955

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 356
rank avg (pred): 0.492 +- 0.178
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002654443, 4.52916e-05

Epoch over!
epoch time: 15.105

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 88
rank avg (pred): 0.525 +- 0.236
mrr vals (pred, true): 0.056, 0.002
batch losses (mrrl, rdl): 0.0003542849, 4.10731e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 908
rank avg (pred): 0.502 +- 0.293
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0109632071, 0.0005326306

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 956
rank avg (pred): 0.557 +- 0.263
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.33446e-05, 6.4108e-05

Epoch over!
epoch time: 15.116

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 262
rank avg (pred): 0.122 +- 0.173
mrr vals (pred, true): 0.298, 0.287
batch losses (mrrl, rdl): 0.0011064617, 2.64755e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 797
rank avg (pred): 0.457 +- 0.228
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004596343, 5.65156e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1085
rank avg (pred): 0.522 +- 0.283
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.51631e-05, 3.7087e-06

Epoch over!
epoch time: 15.077

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 515
rank avg (pred): 0.398 +- 0.133
mrr vals (pred, true): 0.054, 0.079
batch losses (mrrl, rdl): 0.0001829233, 0.000140135

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 189
rank avg (pred): 0.494 +- 0.246
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.02242e-05, 1.44462e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 707
rank avg (pred): 0.486 +- 0.240
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.000355696, 2.28614e-05

Epoch over!
epoch time: 15.121

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 393
rank avg (pred): 0.492 +- 0.261
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.31783e-05, 8.6253e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 680
rank avg (pred): 0.487 +- 0.249
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006554658, 1.83679e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 945
rank avg (pred): 0.557 +- 0.300
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0009456303, 3.0072e-05

Epoch over!
epoch time: 15.138

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1085
rank avg (pred): 0.533 +- 0.309
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 5.22e-08, 6.9079e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1106
rank avg (pred): 0.506 +- 0.306
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 0.0001172018, 3.9657e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1117
rank avg (pred): 0.529 +- 0.328
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001889735, 1.07162e-05

Epoch over!
epoch time: 15.121

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.479 +- 0.285
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002131117, 1.59681e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.505 +- 0.311
mrr vals (pred, true): 0.052, 0.002
batch losses (mrrl, rdl): 6.05068e-05, 7.6272e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 290
rank avg (pred): 0.336 +- 0.186
mrr vals (pred, true): 0.084, 0.078
batch losses (mrrl, rdl): 0.0112506449, 7.2297e-05

Epoch over!
epoch time: 15.115

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 90
rank avg (pred): 0.517 +- 0.334
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002205826, 1.32583e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.392 +- 0.210
mrr vals (pred, true): 0.059, 0.080
batch losses (mrrl, rdl): 0.0008206657, 7.85795e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 112
rank avg (pred): 0.524 +- 0.309
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.6051e-06, 5.2651e-06

Epoch over!
epoch time: 15.135

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 777
rank avg (pred): 0.510 +- 0.303
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 4.94347e-05, 4.8856e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 326
rank avg (pred): 0.511 +- 0.313
mrr vals (pred, true): 0.052, 0.002
batch losses (mrrl, rdl): 4.81567e-05, 6.2957e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 649
rank avg (pred): 0.505 +- 0.314
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 6.0796e-05, 1.00672e-05

Epoch over!
epoch time: 15.149

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.274 +- 0.188
mrr vals (pred, true): 0.150, 0.080

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.05138 	 0.00044 	 m..s
   31 	     1 	 0.05305 	 0.00047 	 m..s
   40 	     2 	 0.05394 	 0.00050 	 m..s
   78 	     3 	 0.05700 	 0.00050 	 m..s
   45 	     4 	 0.05458 	 0.00051 	 m..s
   74 	     5 	 0.05685 	 0.00051 	 m..s
   72 	     6 	 0.05678 	 0.00052 	 m..s
   16 	     7 	 0.05218 	 0.00053 	 m..s
   44 	     8 	 0.05411 	 0.00053 	 m..s
    3 	     9 	 0.05126 	 0.00054 	 m..s
   54 	    10 	 0.05561 	 0.00054 	 m..s
    0 	    11 	 0.04863 	 0.00055 	 m..s
   88 	    12 	 0.05813 	 0.00056 	 m..s
   80 	    13 	 0.05723 	 0.00057 	 m..s
    6 	    14 	 0.05162 	 0.00059 	 m..s
   17 	    15 	 0.05218 	 0.00060 	 m..s
   76 	    16 	 0.05688 	 0.00060 	 m..s
   41 	    17 	 0.05400 	 0.00060 	 m..s
   81 	    18 	 0.05727 	 0.00061 	 m..s
    8 	    19 	 0.05167 	 0.00061 	 m..s
   21 	    20 	 0.05238 	 0.00062 	 m..s
   67 	    21 	 0.05663 	 0.00063 	 m..s
   24 	    22 	 0.05271 	 0.00067 	 m..s
   25 	    23 	 0.05275 	 0.00067 	 m..s
   43 	    24 	 0.05404 	 0.00069 	 m..s
   69 	    25 	 0.05668 	 0.00069 	 m..s
   63 	    26 	 0.05620 	 0.00070 	 m..s
    2 	    27 	 0.05056 	 0.00071 	 m..s
   64 	    28 	 0.05624 	 0.00072 	 m..s
   18 	    29 	 0.05218 	 0.00072 	 m..s
   15 	    30 	 0.05213 	 0.00073 	 m..s
   13 	    31 	 0.05205 	 0.00077 	 m..s
   19 	    32 	 0.05226 	 0.00077 	 m..s
   51 	    33 	 0.05520 	 0.00078 	 m..s
   62 	    34 	 0.05617 	 0.00078 	 m..s
   20 	    35 	 0.05227 	 0.00079 	 m..s
   29 	    36 	 0.05291 	 0.00084 	 m..s
   23 	    37 	 0.05252 	 0.00086 	 m..s
   12 	    38 	 0.05200 	 0.00086 	 m..s
   79 	    39 	 0.05721 	 0.00088 	 m..s
   50 	    40 	 0.05520 	 0.00089 	 m..s
   39 	    41 	 0.05383 	 0.00090 	 m..s
   65 	    42 	 0.05630 	 0.00091 	 m..s
   71 	    43 	 0.05677 	 0.00092 	 m..s
   49 	    44 	 0.05508 	 0.00093 	 m..s
   82 	    45 	 0.05734 	 0.00095 	 m..s
   11 	    46 	 0.05179 	 0.00097 	 m..s
    1 	    47 	 0.05051 	 0.00099 	 m..s
   83 	    48 	 0.05743 	 0.00099 	 m..s
   53 	    49 	 0.05554 	 0.00100 	 m..s
   42 	    50 	 0.05403 	 0.00101 	 m..s
   10 	    51 	 0.05177 	 0.00102 	 m..s
   73 	    52 	 0.05685 	 0.00103 	 m..s
   38 	    53 	 0.05375 	 0.00104 	 m..s
   33 	    54 	 0.05318 	 0.00105 	 m..s
   92 	    55 	 0.06336 	 0.00108 	 m..s
   28 	    56 	 0.05287 	 0.00109 	 m..s
   46 	    57 	 0.05479 	 0.00109 	 m..s
   34 	    58 	 0.05332 	 0.00111 	 m..s
   89 	    59 	 0.05813 	 0.00112 	 m..s
   37 	    60 	 0.05367 	 0.00114 	 m..s
   14 	    61 	 0.05205 	 0.00119 	 m..s
   60 	    62 	 0.05612 	 0.00121 	 m..s
   77 	    63 	 0.05697 	 0.00121 	 m..s
   87 	    64 	 0.05812 	 0.00122 	 m..s
    9 	    65 	 0.05174 	 0.00123 	 m..s
   22 	    66 	 0.05244 	 0.00130 	 m..s
    5 	    67 	 0.05155 	 0.00137 	 m..s
   66 	    68 	 0.05636 	 0.00138 	 m..s
   85 	    69 	 0.05793 	 0.00142 	 m..s
   84 	    70 	 0.05746 	 0.00153 	 m..s
   94 	    71 	 0.06641 	 0.00154 	 m..s
   27 	    72 	 0.05281 	 0.00162 	 m..s
   52 	    73 	 0.05553 	 0.00165 	 m..s
   36 	    74 	 0.05344 	 0.00170 	 m..s
   86 	    75 	 0.05809 	 0.00176 	 m..s
   61 	    76 	 0.05616 	 0.00182 	 m..s
    7 	    77 	 0.05167 	 0.00207 	 m..s
   30 	    78 	 0.05297 	 0.00217 	 m..s
   70 	    79 	 0.05671 	 0.00230 	 m..s
   32 	    80 	 0.05315 	 0.00244 	 m..s
   90 	    81 	 0.05882 	 0.00637 	 m..s
   95 	    82 	 0.06766 	 0.01575 	 m..s
  105 	    83 	 0.11889 	 0.01790 	 MISS
   93 	    84 	 0.06424 	 0.04959 	 ~...
   91 	    85 	 0.06308 	 0.05886 	 ~...
   58 	    86 	 0.05598 	 0.06890 	 ~...
   57 	    87 	 0.05590 	 0.06893 	 ~...
   56 	    88 	 0.05582 	 0.07041 	 ~...
   55 	    89 	 0.05574 	 0.07079 	 ~...
   59 	    90 	 0.05598 	 0.07085 	 ~...
  100 	    91 	 0.07510 	 0.07225 	 ~...
   35 	    92 	 0.05339 	 0.07464 	 ~...
   96 	    93 	 0.06889 	 0.07542 	 ~...
   47 	    94 	 0.05498 	 0.07667 	 ~...
  104 	    95 	 0.09822 	 0.07674 	 ~...
  101 	    96 	 0.07513 	 0.07874 	 ~...
   26 	    97 	 0.05276 	 0.07881 	 ~...
  109 	    98 	 0.15028 	 0.07963 	 m..s
   48 	    99 	 0.05501 	 0.08298 	 ~...
   98 	   100 	 0.07016 	 0.08403 	 ~...
   99 	   101 	 0.07307 	 0.08403 	 ~...
   68 	   102 	 0.05665 	 0.08500 	 ~...
   97 	   103 	 0.06949 	 0.08653 	 ~...
   75 	   104 	 0.05686 	 0.08943 	 m..s
  103 	   105 	 0.09692 	 0.09806 	 ~...
  102 	   106 	 0.08012 	 0.09997 	 ~...
  106 	   107 	 0.12228 	 0.13290 	 ~...
  110 	   108 	 0.15863 	 0.13818 	 ~...
  108 	   109 	 0.12499 	 0.13941 	 ~...
  107 	   110 	 0.12460 	 0.14116 	 ~...
  111 	   111 	 0.15870 	 0.14416 	 ~...
  112 	   112 	 0.23030 	 0.21233 	 ~...
  115 	   113 	 0.24536 	 0.21646 	 ~...
  113 	   114 	 0.23738 	 0.22656 	 ~...
  114 	   115 	 0.23760 	 0.27066 	 m..s
  116 	   116 	 0.25845 	 0.28613 	 ~...
  117 	   117 	 0.30141 	 0.29805 	 ~...
  119 	   118 	 0.31809 	 0.32174 	 ~...
  118 	   119 	 0.31258 	 0.34339 	 m..s
  120 	   120 	 0.32077 	 0.35511 	 m..s
==========================================
r_mrr = 0.93739253282547
r2_mrr = 0.6458625793457031
spearmanr_mrr@5 = 0.850212574005127
spearmanr_mrr@10 = 0.9675770401954651
spearmanr_mrr@50 = 0.9634782075881958
spearmanr_mrr@100 = 0.9580487012863159
spearmanr_mrr@All = 0.959173321723938
==========================================
test time: 0.462
Done Testing dataset OpenEA
total time taken: 253.4085078239441
training time taken: 226.28279542922974
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.9374)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.6459)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.8502)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9676)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9635)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9580)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9592)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.3364132811766467}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9873698302411392
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [762, 66, 432, 1202, 294, 39, 567, 1147, 777, 602, 769, 449, 649, 1152, 1185, 834, 594, 27, 148, 907, 308, 831, 171, 63, 127, 787, 647, 324, 993, 4, 268, 606, 1105, 1138, 561, 192, 274, 348, 252, 626, 720, 733, 965, 368, 452, 1171, 303, 45, 689, 1089, 112, 859, 1076, 134, 732, 318, 215, 694, 759, 283, 974, 908, 233, 1026, 577, 972, 273, 1044, 755, 1072, 657, 56, 634, 570, 548, 288, 607, 1181, 955, 679, 636, 195, 936, 446, 132, 305, 67, 453, 601, 1167, 827, 784, 349, 255, 523, 469, 86, 953, 416, 441, 901, 60, 403, 1002, 1104, 117, 249, 395, 476, 855, 155, 772, 1004, 630, 1, 1123, 1033, 1112, 59, 181, 1018]
valid_ids (0): []
train_ids (1094): [625, 269, 442, 880, 43, 921, 1193, 848, 204, 42, 819, 235, 793, 226, 1042, 579, 278, 969, 1135, 718, 1094, 1117, 325, 773, 1150, 311, 151, 598, 892, 468, 1028, 767, 517, 736, 533, 1078, 208, 717, 525, 1166, 605, 652, 711, 1145, 376, 668, 241, 345, 1091, 481, 394, 77, 987, 33, 618, 358, 933, 35, 482, 573, 26, 196, 1178, 31, 483, 1197, 826, 412, 669, 456, 444, 666, 1027, 1151, 611, 945, 957, 123, 1070, 995, 970, 1127, 914, 433, 179, 32, 266, 1148, 1130, 912, 222, 828, 341, 174, 1132, 583, 739, 357, 644, 223, 317, 340, 497, 1172, 778, 667, 65, 14, 615, 923, 99, 422, 562, 795, 991, 1075, 1155, 845, 404, 136, 124, 844, 806, 49, 699, 355, 296, 108, 761, 331, 629, 725, 664, 735, 2, 1122, 113, 724, 1020, 856, 1097, 290, 310, 319, 366, 837, 240, 242, 792, 841, 334, 838, 417, 628, 927, 488, 994, 106, 674, 541, 1116, 584, 935, 810, 182, 1096, 952, 703, 938, 378, 558, 57, 847, 138, 980, 374, 530, 111, 941, 705, 860, 750, 684, 54, 686, 776, 540, 162, 840, 1156, 764, 413, 803, 375, 971, 351, 74, 1034, 1164, 708, 802, 238, 698, 816, 490, 922, 1095, 64, 291, 1184, 332, 339, 542, 899, 861, 986, 508, 1006, 1124, 323, 489, 169, 219, 1161, 61, 516, 999, 157, 865, 830, 898, 1199, 135, 559, 532, 267, 808, 16, 352, 823, 115, 161, 259, 1209, 1015, 966, 425, 960, 930, 146, 299, 87, 613, 217, 1175, 239, 362, 889, 595, 1046, 79, 638, 563, 298, 985, 729, 279, 757, 919, 48, 383, 102, 367, 377, 621, 475, 165, 1037, 514, 254, 902, 380, 356, 1092, 313, 459, 905, 890, 1189, 121, 515, 300, 1131, 1086, 427, 659, 926, 758, 1031, 774, 1049, 1213, 493, 506, 1174, 676, 518, 670, 1068, 663, 440, 692, 963, 5, 280, 580, 578, 1036, 1170, 186, 96, 0, 1214, 557, 555, 1201, 682, 1050, 639, 47, 504, 545, 193, 436, 1139, 906, 867, 854, 843, 164, 248, 1059, 141, 526, 1208, 916, 565, 159, 829, 1067, 384, 321, 982, 363, 133, 1200, 75, 978, 507, 833, 943, 603, 910, 1108, 501, 200, 813, 693, 329, 1168, 1111, 715, 1203, 680, 1207, 650, 846, 896, 731, 372, 807, 988, 1023, 875, 706, 852, 874, 502, 1190, 599, 891, 172, 307, 402, 665, 770, 637, 72, 284, 789, 373, 261, 658, 864, 796, 1063, 500, 198, 839, 301, 1003, 920, 983, 328, 979, 385, 275, 1143, 260, 958, 431, 918, 90, 868, 588, 513, 354, 166, 998, 547, 528, 1016, 939, 480, 178, 1005, 462, 954, 69, 44, 83, 289, 185, 466, 435, 597, 1182, 1137, 783, 152, 1014, 272, 1035, 293, 105, 225, 23, 145, 643, 815, 94, 744, 1051, 142, 306, 917, 879, 10, 928, 1073, 258, 499, 473, 287, 552, 95, 1045, 721, 968, 173, 478, 343, 853, 81, 753, 1160, 660, 471, 590, 387, 641, 1109, 58, 140, 976, 393, 286, 477, 544, 961, 369, 420, 398, 582, 713, 104, 900, 696, 536, 257, 336, 798, 766, 1113, 396, 1030, 391, 871, 418, 655, 126, 405, 175, 486, 116, 1153, 1146, 690, 1093, 946, 1157, 1038, 445, 569, 1187, 1158, 1173, 534, 738, 688, 414, 131, 964, 194, 1142, 388, 704, 232, 791, 183, 749, 821, 103, 661, 568, 295, 948, 487, 246, 866, 474, 264, 1128, 1085, 82, 782, 672, 959, 197, 617, 415, 677, 1065, 1024, 631, 1007, 519, 726, 277, 40, 429, 389, 593, 924, 231, 479, 817, 915, 1040, 206, 276, 234, 538, 765, 1165, 212, 465, 785, 330, 484, 1194, 529, 539, 1163, 737, 114, 454, 213, 80, 1191, 400, 646, 651, 270, 297, 1062, 691, 156, 88, 591, 797, 977, 709, 485, 997, 962, 1126, 1019, 632, 247, 1079, 36, 304, 990, 931, 316, 543, 781, 392, 230, 1022, 122, 671, 93, 158, 1060, 101, 849, 408, 956, 681, 15, 913, 419, 804, 904, 522, 271, 1056, 22, 884, 1114, 46, 428, 604, 575, 627, 382, 742, 244, 862, 84, 70, 434, 13, 1082, 521, 592, 572, 447, 458, 775, 472, 1069, 424, 160, 62, 909, 614, 397, 722, 771, 747, 1180, 335, 1084, 421, 633, 1008, 818, 455, 29, 1144, 1159, 1088, 292, 741, 873, 869, 12, 184, 423, 89, 535, 37, 1102, 799, 24, 1000, 7, 1043, 149, 346, 399, 546, 1101, 850, 877, 746, 675, 3, 654, 409, 685, 600, 911, 364, 209, 20, 189, 809, 788, 1196, 386, 17, 925, 167, 401, 430, 190, 531, 551, 832, 886, 100, 730, 712, 78, 576, 1048, 125, 1115, 251, 110, 327, 130, 338, 587, 586, 322, 511, 616, 623, 800, 333, 1134, 752, 610, 1012, 188, 1154, 147, 950, 550, 1136, 932, 505, 492, 1183, 723, 951, 224, 1186, 894, 949, 858, 256, 463, 967, 1118, 1074, 822, 1047, 370, 885, 1011, 585, 1087, 1061, 872, 228, 851, 1133, 883, 38, 697, 406, 411, 437, 6, 221, 1032, 461, 53, 1107, 262, 220, 320, 187, 748, 439, 740, 495, 150, 745, 76, 878, 214, 168, 1103, 285, 897, 144, 347, 253, 1212, 407, 210, 1129, 1140, 1125, 379, 734, 199, 768, 836, 727, 236, 520, 973, 780, 707, 128, 1195, 25, 1029, 763, 619, 1058, 716, 553, 814, 450, 1204, 989, 1081, 229, 350, 390, 751, 887, 940, 937, 805, 1211, 443, 683, 1055, 801, 812, 91, 218, 1179, 381, 41, 835, 202, 154, 1188, 656, 498, 464, 635, 620, 825, 888, 1098, 1071, 560, 1077, 460, 581, 648, 622, 1110, 714, 981, 109, 984, 281, 794, 728, 85, 119, 245, 73, 139, 68, 207, 779, 50, 934, 28, 1013, 496, 566, 608, 1010, 700, 1177, 1099, 881, 9, 237, 8, 1169, 1162, 754, 1066, 687, 929, 360, 554, 903, 1192, 263, 662, 1053, 1149, 1119, 512, 882, 1100, 312, 143, 177, 1206, 589, 1198, 120, 1039, 1121, 564, 55, 491, 1041, 30, 710, 97, 760, 992, 1009, 18, 302, 205, 701, 211, 503, 640, 361, 107, 702, 1001, 365, 21, 153, 1064, 52, 337, 34, 1141, 426, 243, 1080, 265, 876, 216, 893, 201, 975, 1210, 92, 1205, 203, 359, 137, 510, 842, 645, 863, 537, 371, 11, 51, 457, 944, 191, 653, 410, 719, 857, 180, 571, 820, 19, 524, 1120, 695, 326, 1083, 451, 824, 470, 170, 942, 811, 227, 309, 448, 947, 353, 596, 1021, 494, 163, 624, 642, 1017, 612, 609, 118, 556, 574, 527, 1054, 438, 743, 314, 71, 250, 870, 895, 1090, 467, 786, 98, 315, 1025, 1106, 1176, 678, 1052, 509, 756, 344, 673, 1057, 176, 790, 282, 129, 342, 549, 996]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2703556478373032
the save name prefix for this run is:  chkpt-ID_2703556478373032_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.485 +- 0.006
mrr vals (pred, true): 0.000, 0.080
batch losses (mrrl, rdl): 0.0, 0.0005017184

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 813
rank avg (pred): 0.243 +- 0.167
mrr vals (pred, true): 0.145, 0.355
batch losses (mrrl, rdl): 0.0, 0.0007575248

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 969
rank avg (pred): 0.477 +- 0.311
mrr vals (pred, true): 0.118, 0.001
batch losses (mrrl, rdl): 0.0, 1.22982e-05

Epoch over!
epoch time: 14.877

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 966
rank avg (pred): 0.454 +- 0.306
mrr vals (pred, true): 0.132, 0.001
batch losses (mrrl, rdl): 0.0, 1.22378e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 880
rank avg (pred): 0.476 +- 0.300
mrr vals (pred, true): 0.083, 0.001
batch losses (mrrl, rdl): 0.0, 8.2197e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1093
rank avg (pred): 0.492 +- 0.283
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0, 1.23141e-05

Epoch over!
epoch time: 14.861

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 361
rank avg (pred): 0.493 +- 0.293
mrr vals (pred, true): 0.057, 0.002
batch losses (mrrl, rdl): 0.0, 1.19217e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1196
rank avg (pred): 0.476 +- 0.284
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0, 4.3484e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 168
rank avg (pred): 0.491 +- 0.293
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0, 5.367e-07

Epoch over!
epoch time: 14.876

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 337
rank avg (pred): 0.471 +- 0.298
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0, 3.3975e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 440
rank avg (pred): 0.507 +- 0.297
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0, 9.485e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 976
rank avg (pred): 0.267 +- 0.268
mrr vals (pred, true): 0.088, 0.125
batch losses (mrrl, rdl): 0.0, 1.089e-05

Epoch over!
epoch time: 14.912

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 928
rank avg (pred): 0.461 +- 0.305
mrr vals (pred, true): 0.066, 0.001
batch losses (mrrl, rdl): 0.0, 2.4613e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1016
rank avg (pred): 0.497 +- 0.311
mrr vals (pred, true): 0.067, 0.002
batch losses (mrrl, rdl): 0.0, 2.4753e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 359
rank avg (pred): 0.510 +- 0.317
mrr vals (pred, true): 0.066, 0.001
batch losses (mrrl, rdl): 0.0, 4.1656e-06

Epoch over!
epoch time: 14.925

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.492 +- 0.306
mrr vals (pred, true): 0.071, 0.001
batch losses (mrrl, rdl): 0.0042371387, 1.4082e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 715
rank avg (pred): 0.468 +- 0.137
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 8.06127e-05, 7.6445e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1145
rank avg (pred): 0.343 +- 0.144
mrr vals (pred, true): 0.069, 0.079
batch losses (mrrl, rdl): 0.0037212898, 9.87557e-05

Epoch over!
epoch time: 15.146

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1024
rank avg (pred): 0.459 +- 0.135
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 4.5085e-06, 8.94909e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 160
rank avg (pred): 0.506 +- 0.158
mrr vals (pred, true): 0.046, 0.002
batch losses (mrrl, rdl): 0.000195658, 6.74203e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.337 +- 0.133
mrr vals (pred, true): 0.067, 0.071
batch losses (mrrl, rdl): 0.002854113, 0.0001164742

Epoch over!
epoch time: 15.125

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 129
rank avg (pred): 0.497 +- 0.205
mrr vals (pred, true): 0.055, 0.002
batch losses (mrrl, rdl): 0.0002291817, 3.21547e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 68
rank avg (pred): 0.358 +- 0.125
mrr vals (pred, true): 0.063, 0.089
batch losses (mrrl, rdl): 0.0016146365, 0.0001242598

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1086
rank avg (pred): 0.572 +- 0.215
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 3.16283e-05, 0.0001768582

Epoch over!
epoch time: 15.139

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 286
rank avg (pred): 0.369 +- 0.122
mrr vals (pred, true): 0.057, 0.077
batch losses (mrrl, rdl): 0.00043128, 0.0001430626

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 672
rank avg (pred): 0.500 +- 0.208
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.12036e-05, 2.86385e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 662
rank avg (pred): 0.450 +- 0.215
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003329356, 6.90334e-05

Epoch over!
epoch time: 15.125

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 444
rank avg (pred): 0.500 +- 0.235
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002545755, 1.20759e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.381 +- 0.158
mrr vals (pred, true): 0.054, 0.071
batch losses (mrrl, rdl): 0.0001873382, 0.0001073996

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1010
rank avg (pred): 0.499 +- 0.258
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4.513e-07, 4.4482e-06

Epoch over!
epoch time: 15.092

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 745
rank avg (pred): 0.129 +- 0.154
mrr vals (pred, true): 0.216, 0.218
batch losses (mrrl, rdl): 8.00282e-05, 0.0001446831

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.474 +- 0.271
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 5.7487e-06, 3.19439e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 997
rank avg (pred): 0.282 +- 0.209
mrr vals (pred, true): 0.151, 0.139
batch losses (mrrl, rdl): 0.0013938753, 4.73792e-05

Epoch over!
epoch time: 15.111

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1119
rank avg (pred): 0.523 +- 0.312
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.82361e-05, 5.4061e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1096
rank avg (pred): 0.539 +- 0.312
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 3.2173e-06, 1.869e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1114
rank avg (pred): 0.554 +- 0.336
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.90313e-05, 3.64249e-05

Epoch over!
epoch time: 15.212

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1209
rank avg (pred): 0.490 +- 0.301
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.88156e-05, 1.98059e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 191
rank avg (pred): 0.490 +- 0.294
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.0753e-06, 2.92386e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 223
rank avg (pred): 0.463 +- 0.282
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 4.549e-06, 5.71358e-05

Epoch over!
epoch time: 15.094

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 203
rank avg (pred): 0.518 +- 0.315
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 5.456e-07, 8.3566e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 183
rank avg (pred): 0.548 +- 0.329
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 2.7235e-06, 2.46371e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 797
rank avg (pred): 0.471 +- 0.298
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.000132384, 4.86915e-05

Epoch over!
epoch time: 15.11

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 371
rank avg (pred): 0.534 +- 0.319
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0005648147, 1.50854e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 250
rank avg (pred): 0.189 +- 0.219
mrr vals (pred, true): 0.210, 0.109
batch losses (mrrl, rdl): 0.1030842885, 0.0001735427

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.450 +- 0.273
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 6.50878e-05, 6.92904e-05

Epoch over!
epoch time: 15.126

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.569 +- 0.337
mrr vals (pred, true): 0.049, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   40 	     0 	 0.05026 	 0.00021 	 m..s
   55 	     1 	 0.05105 	 0.00042 	 m..s
   61 	     2 	 0.05119 	 0.00044 	 m..s
    4 	     3 	 0.04651 	 0.00048 	 m..s
   67 	     4 	 0.05133 	 0.00049 	 m..s
   83 	     5 	 0.05227 	 0.00050 	 m..s
   63 	     6 	 0.05124 	 0.00051 	 m..s
   84 	     7 	 0.05230 	 0.00051 	 m..s
   27 	     8 	 0.04888 	 0.00052 	 m..s
   29 	     9 	 0.04903 	 0.00052 	 m..s
   20 	    10 	 0.04835 	 0.00052 	 m..s
   91 	    11 	 0.05395 	 0.00053 	 m..s
   95 	    12 	 0.05470 	 0.00053 	 m..s
   22 	    13 	 0.04844 	 0.00053 	 m..s
   38 	    14 	 0.04992 	 0.00055 	 m..s
    5 	    15 	 0.04665 	 0.00055 	 m..s
    0 	    16 	 0.04449 	 0.00056 	 m..s
   78 	    17 	 0.05197 	 0.00056 	 m..s
   17 	    18 	 0.04818 	 0.00057 	 m..s
   45 	    19 	 0.05049 	 0.00058 	 m..s
    6 	    20 	 0.04668 	 0.00058 	 m..s
   65 	    21 	 0.05127 	 0.00058 	 m..s
   12 	    22 	 0.04719 	 0.00058 	 m..s
   70 	    23 	 0.05150 	 0.00059 	 m..s
   21 	    24 	 0.04837 	 0.00060 	 m..s
   23 	    25 	 0.04845 	 0.00060 	 m..s
   72 	    26 	 0.05154 	 0.00061 	 m..s
   43 	    27 	 0.05036 	 0.00062 	 m..s
   53 	    28 	 0.05103 	 0.00063 	 m..s
   30 	    29 	 0.04909 	 0.00065 	 m..s
   79 	    30 	 0.05204 	 0.00068 	 m..s
   66 	    31 	 0.05129 	 0.00068 	 m..s
   32 	    32 	 0.04915 	 0.00071 	 m..s
   92 	    33 	 0.05404 	 0.00073 	 m..s
   46 	    34 	 0.05060 	 0.00076 	 m..s
   89 	    35 	 0.05383 	 0.00077 	 m..s
   88 	    36 	 0.05356 	 0.00081 	 m..s
   47 	    37 	 0.05083 	 0.00082 	 m..s
   26 	    38 	 0.04876 	 0.00083 	 m..s
   19 	    39 	 0.04833 	 0.00083 	 m..s
   24 	    40 	 0.04863 	 0.00084 	 m..s
   13 	    41 	 0.04731 	 0.00085 	 m..s
   35 	    42 	 0.04940 	 0.00088 	 m..s
   56 	    43 	 0.05108 	 0.00089 	 m..s
   42 	    44 	 0.05030 	 0.00091 	 m..s
   41 	    45 	 0.05027 	 0.00092 	 m..s
    7 	    46 	 0.04672 	 0.00093 	 m..s
   73 	    47 	 0.05156 	 0.00100 	 m..s
   15 	    48 	 0.04776 	 0.00100 	 m..s
   90 	    49 	 0.05389 	 0.00101 	 m..s
    8 	    50 	 0.04676 	 0.00106 	 m..s
   93 	    51 	 0.05446 	 0.00109 	 m..s
   57 	    52 	 0.05112 	 0.00110 	 m..s
   25 	    53 	 0.04864 	 0.00113 	 m..s
   31 	    54 	 0.04909 	 0.00113 	 m..s
   94 	    55 	 0.05466 	 0.00116 	 m..s
   34 	    56 	 0.04934 	 0.00121 	 m..s
   18 	    57 	 0.04833 	 0.00123 	 m..s
   14 	    58 	 0.04752 	 0.00125 	 m..s
    9 	    59 	 0.04681 	 0.00126 	 m..s
   36 	    60 	 0.04945 	 0.00127 	 m..s
   87 	    61 	 0.05306 	 0.00144 	 m..s
   54 	    62 	 0.05104 	 0.00144 	 m..s
   16 	    63 	 0.04805 	 0.00144 	 m..s
   33 	    64 	 0.04929 	 0.00148 	 m..s
   60 	    65 	 0.05114 	 0.00150 	 m..s
   37 	    66 	 0.04958 	 0.00153 	 m..s
   10 	    67 	 0.04690 	 0.00159 	 m..s
    1 	    68 	 0.04482 	 0.00159 	 m..s
    2 	    69 	 0.04539 	 0.00165 	 m..s
   52 	    70 	 0.05101 	 0.00165 	 m..s
   28 	    71 	 0.04890 	 0.00170 	 m..s
   81 	    72 	 0.05214 	 0.00171 	 m..s
   11 	    73 	 0.04719 	 0.00176 	 m..s
   74 	    74 	 0.05158 	 0.00176 	 m..s
   51 	    75 	 0.05098 	 0.00206 	 m..s
    3 	    76 	 0.04545 	 0.00208 	 m..s
   71 	    77 	 0.05151 	 0.00239 	 m..s
   68 	    78 	 0.05134 	 0.00319 	 m..s
   80 	    79 	 0.05205 	 0.00332 	 m..s
   98 	    80 	 0.05656 	 0.05068 	 ~...
   69 	    81 	 0.05135 	 0.06381 	 ~...
   97 	    82 	 0.05532 	 0.06602 	 ~...
   59 	    83 	 0.05114 	 0.06689 	 ~...
   49 	    84 	 0.05094 	 0.06878 	 ~...
   58 	    85 	 0.05113 	 0.06887 	 ~...
   75 	    86 	 0.05170 	 0.06893 	 ~...
   48 	    87 	 0.05085 	 0.07041 	 ~...
   44 	    88 	 0.05048 	 0.07080 	 ~...
  104 	    89 	 0.06110 	 0.07240 	 ~...
  103 	    90 	 0.05987 	 0.07254 	 ~...
   50 	    91 	 0.05097 	 0.07284 	 ~...
  101 	    92 	 0.05817 	 0.07542 	 ~...
   39 	    93 	 0.05007 	 0.07621 	 ~...
   62 	    94 	 0.05123 	 0.07648 	 ~...
   82 	    95 	 0.05222 	 0.07719 	 ~...
  105 	    96 	 0.06155 	 0.07867 	 ~...
  109 	    97 	 0.12217 	 0.07963 	 m..s
   96 	    98 	 0.05472 	 0.08157 	 ~...
   64 	    99 	 0.05126 	 0.08449 	 m..s
   77 	   100 	 0.05188 	 0.08511 	 m..s
   86 	   101 	 0.05241 	 0.08521 	 m..s
   76 	   102 	 0.05171 	 0.08738 	 m..s
   85 	   103 	 0.05231 	 0.08943 	 m..s
  106 	   104 	 0.07672 	 0.09024 	 ~...
  107 	   105 	 0.08097 	 0.09364 	 ~...
  100 	   106 	 0.05795 	 0.09449 	 m..s
  110 	   107 	 0.13372 	 0.09679 	 m..s
  111 	   108 	 0.13697 	 0.10039 	 m..s
  102 	   109 	 0.05918 	 0.10389 	 m..s
   99 	   110 	 0.05758 	 0.10698 	 m..s
  108 	   111 	 0.08276 	 0.14792 	 m..s
  112 	   112 	 0.16005 	 0.16553 	 ~...
  117 	   113 	 0.20838 	 0.18426 	 ~...
  113 	   114 	 0.18201 	 0.19048 	 ~...
  114 	   115 	 0.19145 	 0.21589 	 ~...
  115 	   116 	 0.19606 	 0.25594 	 m..s
  116 	   117 	 0.20262 	 0.25863 	 m..s
  120 	   118 	 0.29277 	 0.28260 	 ~...
  119 	   119 	 0.28873 	 0.34191 	 m..s
  118 	   120 	 0.28818 	 0.34668 	 m..s
==========================================
r_mrr = 0.8919997215270996
r2_mrr = 0.621320366859436
spearmanr_mrr@5 = 0.8351022601127625
spearmanr_mrr@10 = 0.9438804388046265
spearmanr_mrr@50 = 0.937204122543335
spearmanr_mrr@100 = 0.912842333316803
spearmanr_mrr@All = 0.9143435955047607
==========================================
test time: 0.449
Done Testing dataset OpenEA
total time taken: 252.56006383895874
training time taken: 226.1905472278595
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8920)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.6213)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.8351)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9439)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9372)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9128)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9143)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.39826840541081765}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 5172973932778372
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [280, 600, 907, 1062, 374, 523, 805, 1003, 215, 631, 1068, 475, 284, 558, 1085, 1191, 291, 560, 294, 203, 403, 62, 76, 603, 462, 486, 1148, 645, 234, 37, 1060, 923, 617, 163, 1099, 289, 38, 458, 251, 910, 788, 457, 288, 908, 1199, 1035, 859, 1192, 122, 650, 538, 705, 383, 134, 1049, 712, 120, 355, 90, 390, 927, 463, 361, 834, 742, 1040, 47, 53, 273, 671, 468, 1103, 111, 184, 948, 644, 585, 756, 953, 1050, 114, 776, 310, 1088, 1177, 881, 502, 321, 186, 206, 816, 672, 1135, 438, 888, 778, 734, 1193, 180, 1190, 919, 144, 620, 815, 869, 980, 865, 745, 945, 192, 65, 181, 137, 611, 784, 484, 398, 382, 845, 1044, 257]
valid_ids (0): []
train_ids (1094): [2, 417, 360, 349, 126, 938, 877, 450, 892, 681, 991, 1189, 1027, 951, 491, 591, 536, 1009, 26, 1057, 441, 36, 154, 115, 365, 1207, 193, 1111, 467, 613, 281, 952, 790, 1146, 689, 833, 148, 185, 1071, 297, 685, 641, 5, 88, 362, 937, 875, 1136, 950, 550, 470, 909, 31, 678, 225, 194, 1063, 990, 775, 731, 563, 728, 303, 500, 75, 473, 1, 421, 526, 573, 1138, 219, 87, 1007, 406, 849, 619, 1084, 1201, 802, 905, 898, 764, 1184, 338, 1140, 1129, 757, 525, 427, 168, 737, 1203, 1104, 456, 850, 460, 183, 616, 551, 187, 800, 597, 535, 814, 165, 241, 1178, 556, 70, 1149, 527, 188, 436, 1034, 694, 922, 335, 483, 1016, 174, 581, 981, 547, 231, 341, 565, 658, 205, 106, 683, 69, 768, 202, 508, 182, 548, 592, 896, 1131, 394, 1139, 837, 965, 711, 846, 723, 443, 367, 248, 430, 609, 626, 214, 41, 707, 1180, 395, 1123, 74, 855, 916, 399, 579, 259, 854, 763, 102, 578, 818, 260, 242, 724, 197, 544, 1118, 304, 844, 140, 46, 770, 1043, 608, 381, 930, 522, 83, 1015, 531, 18, 246, 546, 141, 782, 411, 868, 656, 1058, 640, 939, 435, 787, 60, 512, 623, 123, 847, 666, 434, 894, 1018, 873, 354, 1041, 906, 409, 166, 667, 632, 332, 1098, 1141, 655, 539, 179, 1032, 12, 794, 20, 1055, 925, 1090, 1107, 35, 679, 433, 1045, 84, 369, 429, 715, 700, 577, 221, 1144, 595, 903, 553, 1028, 1075, 1095, 736, 149, 228, 884, 1026, 1174, 29, 109, 52, 1185, 765, 1114, 561, 265, 358, 960, 690, 67, 413, 566, 505, 261, 676, 904, 199, 914, 841, 63, 575, 1073, 493, 157, 1061, 697, 1213, 777, 612, 159, 405, 956, 746, 375, 407, 286, 465, 112, 971, 774, 453, 449, 201, 751, 240, 913, 668, 268, 1117, 39, 59, 469, 510, 1022, 388, 1024, 810, 572, 296, 58, 836, 256, 1170, 900, 132, 207, 300, 1211, 1013, 196, 840, 696, 891, 1179, 835, 1054, 624, 598, 860, 738, 116, 32, 567, 1160, 743, 889, 718, 902, 801, 110, 385, 744, 516, 978, 879, 255, 218, 210, 147, 629, 773, 485, 10, 1030, 55, 410, 637, 1079, 236, 1167, 518, 1186, 599, 931, 97, 571, 628, 758, 125, 684, 1051, 941, 1183, 819, 830, 601, 85, 22, 540, 232, 425, 714, 1065, 857, 285, 1070, 498, 191, 542, 649, 162, 301, 95, 279, 799, 682, 1064, 771, 530, 478, 1134, 721, 1006, 515, 1048, 986, 1208, 1153, 893, 827, 298, 1151, 404, 424, 309, 252, 1000, 378, 1108, 1169, 719, 570, 1212, 447, 665, 804, 969, 178, 1082, 659, 959, 471, 245, 444, 25, 973, 829, 876, 1156, 1127, 208, 933, 489, 277, 583, 1092, 1168, 135, 755, 867, 161, 344, 293, 1200, 464, 45, 6, 532, 964, 331, 861, 1089, 313, 432, 1158, 1161, 11, 839, 588, 662, 769, 1157, 552, 543, 669, 920, 652, 209, 809, 48, 164, 9, 618, 722, 271, 440, 370, 574, 108, 828, 1078, 50, 51, 785, 308, 926, 885, 1069, 1175, 0, 1125, 371, 138, 704, 1142, 17, 795, 1019, 488, 13, 477, 807, 630, 852, 145, 340, 693, 1188, 160, 99, 1100, 673, 227, 129, 677, 509, 451, 576, 8, 1052, 121, 985, 146, 1025, 363, 1021, 94, 1074, 554, 647, 1101, 741, 942, 366, 979, 1038, 480, 235, 562, 1042, 482, 377, 408, 976, 119, 912, 921, 584, 393, 476, 1166, 798, 643, 1204, 1072, 1012, 1094, 494, 306, 564, 1087, 994, 1110, 992, 492, 44, 886, 322, 198, 1206, 459, 1143, 687, 1116, 1173, 93, 557, 549, 337, 727, 226, 173, 663, 496, 754, 786, 287, 177, 1010, 783, 901, 710, 230, 915, 761, 513, 247, 519, 79, 89, 262, 1029, 534, 80, 16, 72, 352, 838, 68, 706, 1122, 968, 998, 254, 302, 307, 28, 1155, 243, 1059, 77, 3, 153, 314, 709, 717, 797, 503, 319, 91, 1076, 1133, 1172, 212, 428, 1004, 1036, 386, 461, 590, 947, 733, 1046, 105, 858, 688, 823, 989, 529, 944, 448, 131, 1093, 781, 64, 336, 158, 330, 1198, 521, 455, 878, 270, 275, 848, 379, 325, 883, 292, 472, 528, 987, 1124, 670, 189, 347, 1120, 524, 1039, 753, 537, 767, 824, 957, 222, 1112, 169, 98, 278, 653, 664, 803, 946, 81, 373, 1163, 954, 283, 339, 351, 156, 200, 621, 789, 686, 1097, 416, 233, 596, 634, 353, 880, 862, 555, 133, 752, 870, 1162, 499, 1105, 1154, 474, 238, 1209, 295, 582, 654, 1113, 993, 1202, 982, 760, 949, 372, 446, 176, 396, 580, 559, 78, 1119, 541, 895, 675, 387, 863, 842, 350, 167, 40, 211, 749, 935, 272, 820, 1023, 7, 899, 1210, 934, 1031, 983, 258, 504, 56, 1182, 871, 817, 691, 725, 639, 812, 917, 1159, 1106, 821, 329, 402, 720, 481, 420, 155, 517, 412, 882, 1067, 791, 282, 928, 61, 1152, 1066, 1047, 1147, 324, 96, 674, 1008, 419, 1091, 1145, 346, 1002, 276, 713, 497, 651, 740, 729, 702, 320, 635, 19, 418, 328, 606, 150, 701, 312, 266, 204, 376, 107, 415, 290, 772, 364, 988, 34, 82, 750, 171, 15, 1017, 334, 636, 124, 1033, 831, 962, 811, 943, 454, 1121, 23, 1187, 972, 311, 414, 1083, 918, 1037, 431, 359, 1171, 43, 479, 152, 445, 229, 130, 391, 586, 924, 118, 1150, 269, 1056, 698, 426, 217, 250, 139, 14, 661, 514, 759, 607, 642, 30, 1011, 342, 822, 911, 762, 997, 615, 104, 42, 299, 963, 223, 897, 368, 316, 101, 955, 1165, 1176, 466, 54, 4, 73, 851, 128, 237, 1130, 507, 511, 806, 1005, 175, 216, 890, 439, 345, 961, 808, 589, 323, 646, 735, 136, 190, 977, 506, 348, 24, 1194, 66, 452, 545, 100, 648, 249, 703, 380, 151, 327, 1086, 1096, 936, 1102, 813, 796, 1137, 195, 716, 929, 357, 940, 932, 856, 501, 57, 27, 708, 975, 966, 220, 423, 779, 1195, 730, 103, 692, 326, 263, 224, 1128, 587, 333, 793, 1132, 1001, 33, 638, 490, 853, 984, 117, 887, 400, 397, 699, 315, 726, 1109, 1181, 1164, 1080, 610, 843, 318, 604, 732, 633, 660, 627, 71, 487, 780, 384, 832, 422, 974, 1053, 739, 970, 622, 995, 239, 593, 864, 21, 695, 569, 1197, 792, 680, 356, 49, 172, 533, 264, 274, 495, 343, 392, 866, 1077, 967, 568, 520, 996, 1214, 1081, 213, 958, 127, 170, 602, 142, 1014, 267, 401, 143, 748, 747, 253, 874, 625, 1115, 614, 92, 389, 766, 113, 86, 594, 825, 437, 305, 1205, 244, 872, 605, 826, 1196, 657, 1126, 999, 317, 442, 1020]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9657136412197398
the save name prefix for this run is:  chkpt-ID_9657136412197398_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1024
rank avg (pred): 0.526 +- 0.002
mrr vals (pred, true): 0.000, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001276488

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 871
rank avg (pred): 0.512 +- 0.340
mrr vals (pred, true): 0.066, 0.001
batch losses (mrrl, rdl): 0.0, 2.02676e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.480 +- 0.308
mrr vals (pred, true): 0.058, 0.002
batch losses (mrrl, rdl): 0.0, 4.7274e-06

Epoch over!
epoch time: 14.986

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 655
rank avg (pred): 0.502 +- 0.308
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 0.0, 8.178e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 566
rank avg (pred): 0.392 +- 0.319
mrr vals (pred, true): 0.080, 0.056
batch losses (mrrl, rdl): 0.0, 7.1863e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 867
rank avg (pred): 0.498 +- 0.319
mrr vals (pred, true): 0.081, 0.001
batch losses (mrrl, rdl): 0.0, 1.8823e-06

Epoch over!
epoch time: 15.009

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 992
rank avg (pred): 0.370 +- 0.305
mrr vals (pred, true): 0.068, 0.071
batch losses (mrrl, rdl): 0.0, 7.0453e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 663
rank avg (pred): 0.483 +- 0.309
mrr vals (pred, true): 0.094, 0.001
batch losses (mrrl, rdl): 0.0, 6.1813e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 848
rank avg (pred): 0.524 +- 0.295
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0, 1.19679e-05

Epoch over!
epoch time: 15.033

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1137
rank avg (pred): 0.222 +- 0.278
mrr vals (pred, true): 0.107, 0.143
batch losses (mrrl, rdl): 0.0, 0.0001280757

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.509 +- 0.306
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0, 1.4717e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 892
rank avg (pred): 0.559 +- 0.329
mrr vals (pred, true): 0.096, 0.045
batch losses (mrrl, rdl): 0.0, 0.0001342565

Epoch over!
epoch time: 15.052

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 688
rank avg (pred): 0.493 +- 0.310
mrr vals (pred, true): 0.100, 0.001
batch losses (mrrl, rdl): 0.0, 1.6545e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 211
rank avg (pred): 0.485 +- 0.300
mrr vals (pred, true): 0.085, 0.001
batch losses (mrrl, rdl): 0.0, 2.7368e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 703
rank avg (pred): 0.488 +- 0.313
mrr vals (pred, true): 0.107, 0.001
batch losses (mrrl, rdl): 0.0, 1.3712e-06

Epoch over!
epoch time: 15.001

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 394
rank avg (pred): 0.496 +- 0.303
mrr vals (pred, true): 0.083, 0.002
batch losses (mrrl, rdl): 0.0106516192, 1.6791e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 29
rank avg (pred): 0.452 +- 0.319
mrr vals (pred, true): 0.063, 0.067
batch losses (mrrl, rdl): 0.0016447951, 9.91436e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 145
rank avg (pred): 0.520 +- 0.282
mrr vals (pred, true): 0.034, 0.002
batch losses (mrrl, rdl): 0.0026147345, 6.9931e-06

Epoch over!
epoch time: 15.062

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 424
rank avg (pred): 0.505 +- 0.268
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003279384, 9.7913e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1059
rank avg (pred): 0.411 +- 0.334
mrr vals (pred, true): 0.107, 0.168
batch losses (mrrl, rdl): 0.0376724824, 0.0002405326

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 762
rank avg (pred): 0.565 +- 0.255
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0007341618, 7.8697e-05

Epoch over!
epoch time: 15.065

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 44
rank avg (pred): 0.403 +- 0.330
mrr vals (pred, true): 0.049, 0.088
batch losses (mrrl, rdl): 2.8806e-06, 4.72804e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 576
rank avg (pred): 0.494 +- 0.231
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008694287, 1.45257e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 580
rank avg (pred): 0.490 +- 0.269
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.06485e-05, 2.296e-06

Epoch over!
epoch time: 15.047

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 969
rank avg (pred): 0.508 +- 0.279
mrr vals (pred, true): 0.033, 0.001
batch losses (mrrl, rdl): 0.0028719997, 2.0221e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 300
rank avg (pred): 0.419 +- 0.298
mrr vals (pred, true): 0.052, 0.063
batch losses (mrrl, rdl): 4.89876e-05, 5.25293e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 450
rank avg (pred): 0.490 +- 0.285
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 6.10255e-05, 6.5483e-06

Epoch over!
epoch time: 15.044

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 974
rank avg (pred): 0.266 +- 0.275
mrr vals (pred, true): 0.133, 0.090
batch losses (mrrl, rdl): 0.0685217381, 6.82327e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 592
rank avg (pred): 0.507 +- 0.272
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.13489e-05, 4.7758e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 164
rank avg (pred): 0.478 +- 0.233
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001239607, 2.47344e-05

Epoch over!
epoch time: 15.017

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1179
rank avg (pred): 0.483 +- 0.251
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0003731118, 5.8927e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 689
rank avg (pred): 0.470 +- 0.311
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.72299e-05, 1.9521e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1076
rank avg (pred): 0.354 +- 0.318
mrr vals (pred, true): 0.055, 0.094
batch losses (mrrl, rdl): 0.0002074685, 5.2358e-06

Epoch over!
epoch time: 15.03

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 443
rank avg (pred): 0.482 +- 0.316
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.44164e-05, 1.22711e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 4
rank avg (pred): 0.329 +- 0.283
mrr vals (pred, true): 0.132, 0.097
batch losses (mrrl, rdl): 0.0676049367, 3.42766e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1058
rank avg (pred): 0.301 +- 0.254
mrr vals (pred, true): 0.135, 0.139
batch losses (mrrl, rdl): 0.0001954639, 2.08986e-05

Epoch over!
epoch time: 15.037

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 388
rank avg (pred): 0.496 +- 0.300
mrr vals (pred, true): 0.041, 0.002
batch losses (mrrl, rdl): 0.0008413871, 5.916e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 319
rank avg (pred): 0.372 +- 0.314
mrr vals (pred, true): 0.083, 0.076
batch losses (mrrl, rdl): 0.0108431894, 1.09151e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 45
rank avg (pred): 0.371 +- 0.316
mrr vals (pred, true): 0.079, 0.079
batch losses (mrrl, rdl): 0.0083836149, 9.7148e-06

Epoch over!
epoch time: 15.032

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 559
rank avg (pred): 0.437 +- 0.277
mrr vals (pred, true): 0.102, 0.053
batch losses (mrrl, rdl): 0.0266176183, 0.0001230516

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 868
rank avg (pred): 0.514 +- 0.278
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 2.9505e-06, 5.3925e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1128
rank avg (pred): 0.433 +- 0.312
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.94914e-05, 6.79781e-05

Epoch over!
epoch time: 15.04

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 779
rank avg (pred): 0.479 +- 0.288
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.86827e-05, 1.2827e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 965
rank avg (pred): 0.463 +- 0.321
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.0786e-06, 2.61053e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 59
rank avg (pred): 0.437 +- 0.318
mrr vals (pred, true): 0.052, 0.070
batch losses (mrrl, rdl): 2.33561e-05, 9.72909e-05

Epoch over!
epoch time: 15.044

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.394 +- 0.320
mrr vals (pred, true): 0.049, 0.085

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   94 	     0 	 0.05559 	 0.00021 	 m..s
   87 	     1 	 0.05172 	 0.00048 	 m..s
   48 	     2 	 0.04639 	 0.00049 	 m..s
   56 	     3 	 0.04665 	 0.00050 	 m..s
   30 	     4 	 0.04596 	 0.00051 	 m..s
    5 	     5 	 0.04473 	 0.00051 	 m..s
   70 	     6 	 0.04828 	 0.00052 	 m..s
   28 	     7 	 0.04586 	 0.00054 	 m..s
   54 	     8 	 0.04650 	 0.00054 	 m..s
   89 	     9 	 0.05474 	 0.00054 	 m..s
   23 	    10 	 0.04562 	 0.00056 	 m..s
   16 	    11 	 0.04530 	 0.00056 	 m..s
   33 	    12 	 0.04610 	 0.00056 	 m..s
   58 	    13 	 0.04717 	 0.00056 	 m..s
   85 	    14 	 0.05111 	 0.00057 	 m..s
   47 	    15 	 0.04636 	 0.00057 	 m..s
   80 	    16 	 0.04862 	 0.00057 	 m..s
    0 	    17 	 0.04178 	 0.00058 	 m..s
   93 	    18 	 0.05543 	 0.00058 	 m..s
   60 	    19 	 0.04762 	 0.00059 	 m..s
   12 	    20 	 0.04512 	 0.00060 	 m..s
    9 	    21 	 0.04503 	 0.00061 	 m..s
   10 	    22 	 0.04504 	 0.00061 	 m..s
   53 	    23 	 0.04649 	 0.00063 	 m..s
   63 	    24 	 0.04794 	 0.00065 	 m..s
   32 	    25 	 0.04608 	 0.00066 	 m..s
   77 	    26 	 0.04852 	 0.00066 	 m..s
   72 	    27 	 0.04834 	 0.00067 	 m..s
   92 	    28 	 0.05501 	 0.00068 	 m..s
   38 	    29 	 0.04623 	 0.00069 	 m..s
   49 	    30 	 0.04640 	 0.00070 	 m..s
   26 	    31 	 0.04575 	 0.00071 	 m..s
   19 	    32 	 0.04552 	 0.00072 	 m..s
   29 	    33 	 0.04590 	 0.00072 	 m..s
    1 	    34 	 0.04413 	 0.00075 	 m..s
   45 	    35 	 0.04634 	 0.00075 	 m..s
   42 	    36 	 0.04628 	 0.00076 	 m..s
   14 	    37 	 0.04524 	 0.00076 	 m..s
   43 	    38 	 0.04634 	 0.00076 	 m..s
   86 	    39 	 0.05155 	 0.00077 	 m..s
   44 	    40 	 0.04634 	 0.00078 	 m..s
   66 	    41 	 0.04809 	 0.00079 	 m..s
   64 	    42 	 0.04799 	 0.00080 	 m..s
   37 	    43 	 0.04619 	 0.00081 	 m..s
    2 	    44 	 0.04441 	 0.00081 	 m..s
   52 	    45 	 0.04648 	 0.00082 	 m..s
   73 	    46 	 0.04837 	 0.00083 	 m..s
   57 	    47 	 0.04708 	 0.00084 	 m..s
   17 	    48 	 0.04536 	 0.00085 	 m..s
   39 	    49 	 0.04626 	 0.00086 	 m..s
   50 	    50 	 0.04644 	 0.00089 	 m..s
   71 	    51 	 0.04833 	 0.00090 	 m..s
   13 	    52 	 0.04515 	 0.00091 	 m..s
    8 	    53 	 0.04492 	 0.00092 	 m..s
   24 	    54 	 0.04568 	 0.00094 	 m..s
   67 	    55 	 0.04815 	 0.00095 	 m..s
    4 	    56 	 0.04451 	 0.00095 	 m..s
    6 	    57 	 0.04478 	 0.00103 	 m..s
   40 	    58 	 0.04627 	 0.00106 	 m..s
   15 	    59 	 0.04528 	 0.00106 	 m..s
   91 	    60 	 0.05496 	 0.00110 	 m..s
   27 	    61 	 0.04578 	 0.00111 	 m..s
   68 	    62 	 0.04818 	 0.00121 	 m..s
   34 	    63 	 0.04613 	 0.00122 	 m..s
   21 	    64 	 0.04559 	 0.00126 	 m..s
   69 	    65 	 0.04820 	 0.00130 	 m..s
   22 	    66 	 0.04559 	 0.00134 	 m..s
    3 	    67 	 0.04442 	 0.00135 	 m..s
   31 	    68 	 0.04600 	 0.00135 	 m..s
   59 	    69 	 0.04751 	 0.00146 	 m..s
   35 	    70 	 0.04613 	 0.00153 	 m..s
   61 	    71 	 0.04782 	 0.00156 	 m..s
   25 	    72 	 0.04569 	 0.00159 	 m..s
   41 	    73 	 0.04628 	 0.00167 	 m..s
   65 	    74 	 0.04809 	 0.00168 	 m..s
   46 	    75 	 0.04635 	 0.00172 	 m..s
   76 	    76 	 0.04850 	 0.00180 	 m..s
   20 	    77 	 0.04556 	 0.00180 	 m..s
   36 	    78 	 0.04616 	 0.00184 	 m..s
   18 	    79 	 0.04543 	 0.00191 	 m..s
   88 	    80 	 0.05174 	 0.00193 	 m..s
   11 	    81 	 0.04507 	 0.00208 	 m..s
   51 	    82 	 0.04645 	 0.00227 	 m..s
    7 	    83 	 0.04491 	 0.00279 	 m..s
   55 	    84 	 0.04652 	 0.00487 	 m..s
  108 	    85 	 0.07440 	 0.01037 	 m..s
   97 	    86 	 0.06380 	 0.04752 	 ~...
  109 	    87 	 0.07670 	 0.05180 	 ~...
   98 	    88 	 0.06397 	 0.05603 	 ~...
   96 	    89 	 0.06230 	 0.06172 	 ~...
   75 	    90 	 0.04846 	 0.06381 	 ~...
   90 	    91 	 0.05477 	 0.06915 	 ~...
   62 	    92 	 0.04784 	 0.07113 	 ~...
  103 	    93 	 0.06998 	 0.07240 	 ~...
  104 	    94 	 0.06999 	 0.07421 	 ~...
  105 	    95 	 0.07002 	 0.07542 	 ~...
   83 	    96 	 0.04905 	 0.07648 	 ~...
  106 	    97 	 0.07114 	 0.07650 	 ~...
  100 	    98 	 0.06914 	 0.07869 	 ~...
   84 	    99 	 0.04911 	 0.08018 	 m..s
   81 	   100 	 0.04879 	 0.08094 	 m..s
   99 	   101 	 0.06751 	 0.08403 	 ~...
   78 	   102 	 0.04854 	 0.08451 	 m..s
   82 	   103 	 0.04897 	 0.08484 	 m..s
  101 	   104 	 0.06966 	 0.08502 	 ~...
  102 	   105 	 0.06969 	 0.08653 	 ~...
   74 	   106 	 0.04845 	 0.08673 	 m..s
   79 	   107 	 0.04861 	 0.08956 	 m..s
  111 	   108 	 0.10052 	 0.09838 	 ~...
   95 	   109 	 0.05699 	 0.10497 	 m..s
  114 	   110 	 0.13829 	 0.13118 	 ~...
  107 	   111 	 0.07172 	 0.13560 	 m..s
  112 	   112 	 0.11017 	 0.17068 	 m..s
  113 	   113 	 0.11676 	 0.17323 	 m..s
  110 	   114 	 0.08820 	 0.18061 	 m..s
  115 	   115 	 0.14305 	 0.18394 	 m..s
  118 	   116 	 0.19561 	 0.18426 	 ~...
  116 	   117 	 0.17632 	 0.21845 	 m..s
  117 	   118 	 0.19033 	 0.28885 	 m..s
  119 	   119 	 0.29495 	 0.34339 	 m..s
  120 	   120 	 0.31526 	 0.34919 	 m..s
==========================================
r_mrr = 0.8965134620666504
r2_mrr = 0.580228328704834
spearmanr_mrr@5 = 0.9015786051750183
spearmanr_mrr@10 = 0.9552696347236633
spearmanr_mrr@50 = 0.9375739097595215
spearmanr_mrr@100 = 0.9319225549697876
spearmanr_mrr@All = 0.9332171678543091
==========================================
test time: 0.455
Done Testing dataset OpenEA
total time taken: 252.11328649520874
training time taken: 225.96547317504883
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8965)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5802)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9016)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9553)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9376)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9319)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9332)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.4815168194218131}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4163391549642081
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [99, 118, 141, 185, 545, 371, 343, 178, 584, 105, 576, 360, 1089, 672, 90, 875, 588, 816, 66, 294, 1020, 1137, 884, 1060, 851, 305, 366, 1086, 525, 1107, 170, 89, 70, 1110, 931, 983, 941, 156, 135, 65, 24, 372, 972, 204, 148, 1162, 184, 1075, 817, 74, 181, 597, 233, 1213, 270, 659, 990, 44, 892, 564, 1059, 19, 1080, 359, 571, 589, 802, 918, 812, 868, 325, 562, 695, 137, 977, 784, 1104, 384, 140, 120, 515, 201, 92, 1018, 624, 818, 910, 732, 231, 398, 1091, 67, 1131, 420, 462, 96, 1147, 361, 403, 388, 712, 905, 1156, 149, 858, 35, 529, 841, 1039, 1175, 590, 1056, 650, 1077, 516, 538, 1134, 530, 1132, 772, 308]
valid_ids (0): []
train_ids (1094): [408, 856, 575, 635, 313, 397, 847, 736, 1109, 651, 162, 1173, 58, 441, 880, 1144, 968, 1121, 332, 652, 499, 282, 660, 922, 368, 490, 1016, 47, 20, 988, 203, 131, 994, 456, 460, 862, 84, 240, 832, 748, 730, 451, 1182, 517, 326, 1030, 1076, 284, 257, 520, 215, 155, 123, 463, 111, 714, 81, 121, 951, 791, 1125, 956, 682, 800, 993, 852, 632, 933, 197, 39, 750, 303, 1054, 30, 965, 395, 518, 688, 767, 787, 212, 763, 605, 836, 1153, 324, 1029, 725, 1011, 2, 205, 318, 866, 647, 400, 1120, 206, 1158, 843, 729, 107, 487, 929, 986, 351, 1036, 1145, 604, 970, 154, 536, 1050, 586, 833, 225, 414, 164, 295, 1202, 1035, 432, 393, 1025, 136, 707, 1185, 1200, 312, 641, 160, 298, 0, 10, 283, 467, 996, 1068, 864, 26, 1118, 826, 168, 1196, 1032, 46, 244, 908, 405, 991, 183, 623, 989, 1017, 759, 182, 778, 631, 407, 268, 1177, 1043, 963, 654, 412, 1005, 734, 874, 331, 819, 116, 602, 514, 261, 582, 777, 234, 57, 553, 34, 1122, 637, 345, 378, 265, 599, 1087, 191, 198, 329, 504, 1053, 134, 825, 834, 279, 940, 448, 1006, 942, 895, 363, 1210, 709, 1031, 840, 161, 813, 949, 1151, 238, 354, 967, 731, 43, 72, 1205, 806, 511, 239, 367, 483, 958, 815, 1176, 783, 276, 1096, 78, 999, 565, 510, 879, 327, 698, 848, 452, 127, 1198, 292, 1212, 346, 782, 622, 726, 867, 307, 322, 680, 761, 200, 1214, 1037, 1069, 396, 32, 380, 608, 417, 1099, 104, 566, 861, 323, 98, 1195, 716, 296, 42, 413, 1190, 668, 556, 1171, 513, 176, 560, 373, 913, 364, 755, 321, 959, 1150, 925, 390, 213, 115, 628, 1027, 574, 891, 442, 1057, 63, 992, 100, 1046, 945, 1026, 163, 612, 649, 653, 1186, 438, 465, 581, 435, 541, 1055, 960, 281, 344, 756, 846, 110, 569, 315, 102, 287, 260, 677, 440, 842, 109, 1167, 1061, 543, 665, 1111, 619, 291, 1002, 975, 386, 873, 558, 754, 780, 55, 1040, 850, 655, 310, 697, 190, 717, 916, 216, 152, 86, 489, 610, 6, 1041, 814, 1038, 686, 245, 479, 662, 921, 1090, 890, 209, 1058, 1142, 1088, 609, 894, 876, 596, 337, 503, 319, 938, 129, 500, 97, 254, 739, 1157, 727, 831, 979, 51, 1094, 450, 824, 616, 807, 603, 795, 352, 917, 17, 1166, 468, 93, 693, 1100, 138, 948, 657, 689, 320, 355, 1204, 12, 835, 555, 580, 585, 563, 139, 781, 13, 611, 765, 551, 811, 444, 579, 676, 1045, 304, 334, 227, 255, 27, 1052, 477, 751, 69, 18, 410, 747, 1074, 406, 1085, 108, 669, 839, 786, 60, 906, 330, 379, 445, 1178, 436, 422, 1130, 926, 350, 1082, 274, 1012, 557, 969, 776, 194, 849, 1165, 286, 704, 76, 1115, 694, 1083, 1103, 187, 353, 1188, 188, 593, 117, 333, 1102, 976, 526, 886, 130, 700, 471, 627, 340, 1194, 167, 923, 230, 486, 443, 457, 752, 845, 301, 882, 1070, 722, 885, 498, 382, 1129, 392, 1064, 249, 822, 903, 1169, 935, 914, 171, 173, 22, 928, 626, 68, 218, 1, 280, 1023, 174, 837, 570, 803, 1199, 973, 741, 1143, 1007, 61, 144, 658, 1095, 540, 703, 1119, 470, 1049, 645, 595, 877, 674, 1138, 1097, 370, 246, 431, 646, 667, 259, 705, 642, 869, 1164, 606, 671, 794, 219, 860, 453, 1207, 953, 939, 1141, 1136, 122, 1009, 888, 357, 1133, 253, 25, 224, 328, 528, 636, 980, 1066, 912, 1154, 1191, 8, 512, 491, 1073, 724, 533, 64, 36, 237, 449, 40, 1028, 769, 447, 165, 946, 568, 29, 1187, 893, 132, 53, 547, 666, 808, 683, 347, 537, 293, 620, 1172, 911, 962, 1170, 1098, 23, 54, 484, 768, 770, 87, 419, 1019, 971, 242, 1146, 250, 681, 349, 1042, 241, 670, 944, 1139, 143, 421, 83, 28, 461, 853, 169, 252, 1093, 829, 592, 426, 1003, 454, 428, 101, 472, 544, 263, 844, 644, 774, 1101, 1078, 85, 930, 126, 455, 193, 901, 766, 598, 522, 1116, 362, 519, 936, 369, 402, 961, 809, 495, 915, 125, 1081, 91, 493, 713, 399, 1128, 56, 749, 38, 1001, 1179, 964, 799, 830, 737, 1192, 235, 199, 934, 673, 801, 195, 95, 740, 887, 207, 1208, 550, 920, 177, 974, 424, 572, 473, 31, 735, 299, 760, 661, 621, 7, 899, 663, 220, 706, 356, 497, 718, 1161, 810, 573, 387, 823, 600, 338, 1211, 300, 685, 710, 409, 643, 37, 854, 521, 721, 966, 805, 501, 587, 1108, 711, 317, 863, 466, 878, 383, 1168, 1201, 266, 900, 272, 523, 932, 764, 427, 696, 924, 629, 679, 285, 745, 430, 554, 699, 865, 11, 151, 508, 103, 481, 1063, 335, 625, 1160, 264, 309, 385, 546, 978, 401, 561, 229, 952, 210, 339, 1051, 532, 567, 821, 1044, 273, 243, 857, 228, 62, 88, 640, 1124, 534, 998, 475, 416, 771, 133, 75, 757, 859, 316, 889, 909, 506, 1072, 271, 474, 434, 648, 897, 719, 947, 708, 828, 159, 221, 377, 142, 548, 375, 982, 798, 1155, 728, 179, 404, 1034, 507, 79, 664, 502, 5, 336, 1065, 509, 492, 549, 290, 394, 256, 943, 145, 50, 494, 937, 773, 189, 114, 425, 52, 691, 793, 478, 1127, 476, 753, 871, 723, 1180, 262, 1206, 77, 1010, 630, 838, 775, 1000, 870, 1106, 656, 788, 1092, 790, 855, 1024, 746, 1135, 71, 1071, 236, 957, 358, 59, 701, 480, 577, 94, 439, 1184, 226, 927, 302, 1203, 211, 1048, 690, 147, 898, 1197, 469, 1014, 1193, 995, 289, 128, 157, 146, 1008, 247, 1126, 954, 985, 415, 45, 1163, 559, 119, 1149, 80, 376, 275, 269, 437, 881, 223, 459, 984, 1209, 1112, 180, 684, 175, 618, 202, 41, 1123, 423, 997, 381, 594, 1033, 365, 524, 3, 827, 488, 1183, 1140, 374, 702, 1062, 675, 633, 389, 542, 527, 1004, 1079, 715, 617, 539, 904, 196, 429, 797, 464, 1117, 446, 634, 591, 48, 907, 678, 614, 505, 742, 158, 692, 613, 106, 981, 872, 150, 1148, 222, 15, 433, 758, 1084, 1174, 902, 601, 391, 1114, 531, 820, 277, 1067, 792, 278, 987, 306, 16, 919, 789, 348, 779, 955, 314, 1105, 1181, 733, 267, 153, 112, 1159, 166, 796, 418, 639, 762, 192, 124, 341, 342, 1021, 552, 583, 21, 49, 33, 744, 248, 4, 578, 1152, 687, 411, 311, 1013, 535, 258, 785, 1113, 14, 297, 896, 186, 615, 251, 208, 485, 1015, 804, 607, 1189, 720, 9, 113, 1047, 496, 458, 217, 288, 214, 883, 950, 1022, 82, 172, 73, 738, 232, 482, 743, 638]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2659160883128310
the save name prefix for this run is:  chkpt-ID_2659160883128310_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 187
rank avg (pred): 0.534 +- 0.005
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001470469

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 502
rank avg (pred): 0.326 +- 0.042
mrr vals (pred, true): 0.000, 0.181
batch losses (mrrl, rdl): 0.0, 0.00028336

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 585
rank avg (pred): 0.470 +- 0.299
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 7.6051e-06

Epoch over!
epoch time: 14.925

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 598
rank avg (pred): 0.502 +- 0.300
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.594e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 376
rank avg (pred): 0.495 +- 0.294
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 4.056e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 482
rank avg (pred): 0.485 +- 0.275
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.3216e-06

Epoch over!
epoch time: 14.901

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 856
rank avg (pred): 0.522 +- 0.299
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 7.385e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 513
rank avg (pred): 0.380 +- 0.317
mrr vals (pred, true): 0.001, 0.078
batch losses (mrrl, rdl): 0.0, 6.6611e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 152
rank avg (pred): 0.510 +- 0.286
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 7.6659e-06

Epoch over!
epoch time: 14.936

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 724
rank avg (pred): 0.508 +- 0.312
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 3.1509e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1130
rank avg (pred): 0.485 +- 0.285
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.2062e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 448
rank avg (pred): 0.494 +- 0.289
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 2.295e-07

Epoch over!
epoch time: 14.972

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1114
rank avg (pred): 0.506 +- 0.300
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 2.5814e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 394
rank avg (pred): 0.494 +- 0.296
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 1.1509e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 702
rank avg (pred): 0.491 +- 0.289
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 5.086e-07

Epoch over!
epoch time: 14.991

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1047
rank avg (pred): 0.501 +- 0.295
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0246593338, 1.075e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 708
rank avg (pred): 0.484 +- 0.370
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 7.024e-07, 3.27917e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 744
rank avg (pred): 0.322 +- 0.383
mrr vals (pred, true): 0.083, 0.206
batch losses (mrrl, rdl): 0.1508560181, 0.0001842894

Epoch over!
epoch time: 15.189

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 374
rank avg (pred): 0.468 +- 0.360
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.0035311852, 3.09574e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1021
rank avg (pred): 0.464 +- 0.355
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0003875848, 2.54502e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 783
rank avg (pred): 0.425 +- 0.366
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0058239773, 0.0001266077

Epoch over!
epoch time: 15.129

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 843
rank avg (pred): 0.410 +- 0.362
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0008152306, 0.0001725001

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1094
rank avg (pred): 0.447 +- 0.327
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 4.057e-06, 2.11212e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1185
rank avg (pred): 0.469 +- 0.350
mrr vals (pred, true): 0.048, 0.002
batch losses (mrrl, rdl): 4.94965e-05, 2.30879e-05

Epoch over!
epoch time: 15.066

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 291
rank avg (pred): 0.372 +- 0.343
mrr vals (pred, true): 0.081, 0.074
batch losses (mrrl, rdl): 0.0098654255, 1.16361e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 614
rank avg (pred): 0.464 +- 0.351
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.000967323, 3.05307e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 893
rank avg (pred): 0.267 +- 0.363
mrr vals (pred, true): 0.102, 0.011
batch losses (mrrl, rdl): 0.0267743804, 0.0017199323

Epoch over!
epoch time: 15.054

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 469
rank avg (pred): 0.459 +- 0.317
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0018595259, 1.55926e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1051
rank avg (pred): 0.410 +- 0.321
mrr vals (pred, true): 0.063, 0.001
batch losses (mrrl, rdl): 0.001726646, 0.0001116971

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 913
rank avg (pred): 0.476 +- 0.349
mrr vals (pred, true): 0.059, 0.002
batch losses (mrrl, rdl): 0.0008528918, 0.0001623824

Epoch over!
epoch time: 15.096

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1146
rank avg (pred): 0.348 +- 0.339
mrr vals (pred, true): 0.090, 0.102
batch losses (mrrl, rdl): 0.0014703731, 1.9072e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1177
rank avg (pred): 0.466 +- 0.326
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008405029, 9.6942e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 750
rank avg (pred): 0.306 +- 0.327
mrr vals (pred, true): 0.130, 0.168
batch losses (mrrl, rdl): 0.0146568203, 7.16435e-05

Epoch over!
epoch time: 15.104

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 114
rank avg (pred): 0.441 +- 0.279
mrr vals (pred, true): 0.043, 0.002
batch losses (mrrl, rdl): 0.000545598, 1.65348e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 625
rank avg (pred): 0.446 +- 0.306
mrr vals (pred, true): 0.055, 0.002
batch losses (mrrl, rdl): 0.000297492, 1.89868e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1033
rank avg (pred): 0.471 +- 0.315
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.77163e-05, 9.6386e-06

Epoch over!
epoch time: 15.138

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 396
rank avg (pred): 0.448 +- 0.302
mrr vals (pred, true): 0.044, 0.002
batch losses (mrrl, rdl): 0.0003821607, 2.03778e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1105
rank avg (pred): 0.436 +- 0.303
mrr vals (pred, true): 0.062, 0.003
batch losses (mrrl, rdl): 0.0014159643, 2.39546e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 91
rank avg (pred): 0.459 +- 0.293
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 8e-10, 8.5784e-06

Epoch over!
epoch time: 15.132

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1109
rank avg (pred): 0.494 +- 0.318
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4.485e-07, 2.80425e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 31
rank avg (pred): 0.438 +- 0.272
mrr vals (pred, true): 0.034, 0.067
batch losses (mrrl, rdl): 0.0025020782, 0.0001822601

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 186
rank avg (pred): 0.454 +- 0.298
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.63696e-05, 1.0895e-05

Epoch over!
epoch time: 15.128

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 395
rank avg (pred): 0.451 +- 0.263
mrr vals (pred, true): 0.033, 0.001
batch losses (mrrl, rdl): 0.0029361518, 1.96849e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 551
rank avg (pred): 0.437 +- 0.276
mrr vals (pred, true): 0.052, 0.079
batch losses (mrrl, rdl): 3.15684e-05, 0.0002370027

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 296
rank avg (pred): 0.391 +- 0.242
mrr vals (pred, true): 0.056, 0.079
batch losses (mrrl, rdl): 0.0003368597, 0.0001237862

Epoch over!
epoch time: 15.13

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.450 +- 0.287
mrr vals (pred, true): 0.054, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   78 	     0 	 0.05268 	 0.00045 	 m..s
   41 	     1 	 0.04814 	 0.00048 	 m..s
   76 	     2 	 0.05260 	 0.00051 	 m..s
   27 	     3 	 0.04614 	 0.00054 	 m..s
   68 	     4 	 0.05197 	 0.00055 	 m..s
   34 	     5 	 0.04709 	 0.00055 	 m..s
    8 	     6 	 0.04152 	 0.00056 	 m..s
   50 	     7 	 0.04990 	 0.00057 	 m..s
    3 	     8 	 0.03981 	 0.00057 	 m..s
   67 	     9 	 0.05189 	 0.00058 	 m..s
   24 	    10 	 0.04491 	 0.00059 	 m..s
   21 	    11 	 0.04405 	 0.00059 	 m..s
   78 	    12 	 0.05268 	 0.00060 	 m..s
   35 	    13 	 0.04751 	 0.00060 	 m..s
   83 	    14 	 0.05363 	 0.00060 	 m..s
   84 	    15 	 0.05363 	 0.00060 	 m..s
    5 	    16 	 0.04041 	 0.00061 	 m..s
   59 	    17 	 0.05111 	 0.00067 	 m..s
   39 	    18 	 0.04803 	 0.00069 	 m..s
   31 	    19 	 0.04700 	 0.00072 	 m..s
   36 	    20 	 0.04781 	 0.00073 	 m..s
    0 	    21 	 0.03835 	 0.00073 	 m..s
   60 	    22 	 0.05112 	 0.00074 	 m..s
   73 	    23 	 0.05237 	 0.00074 	 m..s
   58 	    24 	 0.05097 	 0.00075 	 m..s
   87 	    25 	 0.05418 	 0.00076 	 m..s
   55 	    26 	 0.05043 	 0.00076 	 m..s
   28 	    27 	 0.04676 	 0.00080 	 m..s
   57 	    28 	 0.05081 	 0.00083 	 m..s
   30 	    29 	 0.04699 	 0.00084 	 m..s
   37 	    30 	 0.04785 	 0.00085 	 m..s
    4 	    31 	 0.04032 	 0.00086 	 m..s
   61 	    32 	 0.05123 	 0.00088 	 m..s
   72 	    33 	 0.05233 	 0.00089 	 m..s
   25 	    34 	 0.04511 	 0.00090 	 m..s
   11 	    35 	 0.04182 	 0.00093 	 m..s
   78 	    36 	 0.05268 	 0.00093 	 m..s
    6 	    37 	 0.04041 	 0.00094 	 m..s
   52 	    38 	 0.04999 	 0.00099 	 m..s
   85 	    39 	 0.05369 	 0.00100 	 m..s
    1 	    40 	 0.03889 	 0.00102 	 m..s
   10 	    41 	 0.04169 	 0.00103 	 m..s
   89 	    42 	 0.05528 	 0.00105 	 m..s
   77 	    43 	 0.05262 	 0.00111 	 m..s
   51 	    44 	 0.04994 	 0.00112 	 m..s
   14 	    45 	 0.04230 	 0.00113 	 m..s
   18 	    46 	 0.04289 	 0.00120 	 m..s
   53 	    47 	 0.05031 	 0.00120 	 m..s
   74 	    48 	 0.05237 	 0.00129 	 m..s
    7 	    49 	 0.04075 	 0.00132 	 m..s
   22 	    50 	 0.04426 	 0.00133 	 m..s
   42 	    51 	 0.04870 	 0.00133 	 m..s
   44 	    52 	 0.04880 	 0.00136 	 m..s
   54 	    53 	 0.05034 	 0.00138 	 m..s
   56 	    54 	 0.05047 	 0.00138 	 m..s
   90 	    55 	 0.05641 	 0.00139 	 m..s
   13 	    56 	 0.04217 	 0.00140 	 m..s
   20 	    57 	 0.04363 	 0.00141 	 m..s
   32 	    58 	 0.04700 	 0.00142 	 m..s
   63 	    59 	 0.05144 	 0.00144 	 m..s
   23 	    60 	 0.04490 	 0.00144 	 m..s
   69 	    61 	 0.05201 	 0.00144 	 m..s
   19 	    62 	 0.04328 	 0.00146 	 m..s
   47 	    63 	 0.04912 	 0.00146 	 m..s
    2 	    64 	 0.03979 	 0.00149 	 m..s
   16 	    65 	 0.04263 	 0.00153 	 m..s
   15 	    66 	 0.04260 	 0.00162 	 m..s
   12 	    67 	 0.04197 	 0.00164 	 m..s
   29 	    68 	 0.04677 	 0.00165 	 m..s
   26 	    69 	 0.04529 	 0.00165 	 m..s
   17 	    70 	 0.04272 	 0.00180 	 m..s
   71 	    71 	 0.05217 	 0.00180 	 m..s
   92 	    72 	 0.05800 	 0.00181 	 m..s
   62 	    73 	 0.05131 	 0.00184 	 m..s
    9 	    74 	 0.04168 	 0.00184 	 m..s
   45 	    75 	 0.04890 	 0.00208 	 m..s
   64 	    76 	 0.05146 	 0.00239 	 m..s
   94 	    77 	 0.06113 	 0.00249 	 m..s
   43 	    78 	 0.04877 	 0.00267 	 m..s
   96 	    79 	 0.06337 	 0.00487 	 m..s
  117 	    80 	 0.26711 	 0.00581 	 MISS
  118 	    81 	 0.27610 	 0.00638 	 MISS
  119 	    82 	 0.27898 	 0.01037 	 MISS
  105 	    83 	 0.07886 	 0.04540 	 m..s
   97 	    84 	 0.06399 	 0.04703 	 ~...
   98 	    85 	 0.06403 	 0.05495 	 ~...
   99 	    86 	 0.06426 	 0.05603 	 ~...
   40 	    87 	 0.04808 	 0.06428 	 ~...
   82 	    88 	 0.05337 	 0.06890 	 ~...
   86 	    89 	 0.05390 	 0.07080 	 ~...
   70 	    90 	 0.05211 	 0.07113 	 ~...
   38 	    91 	 0.04797 	 0.07228 	 ~...
   33 	    92 	 0.04707 	 0.07487 	 ~...
   48 	    93 	 0.04941 	 0.07540 	 ~...
   93 	    94 	 0.05876 	 0.07542 	 ~...
   66 	    95 	 0.05184 	 0.07719 	 ~...
   49 	    96 	 0.04950 	 0.07936 	 ~...
  101 	    97 	 0.06970 	 0.08027 	 ~...
  100 	    98 	 0.06793 	 0.08083 	 ~...
   95 	    99 	 0.06173 	 0.08188 	 ~...
   91 	   100 	 0.05641 	 0.08449 	 ~...
   88 	   101 	 0.05438 	 0.08451 	 m..s
   46 	   102 	 0.04909 	 0.08508 	 m..s
   75 	   103 	 0.05256 	 0.08511 	 m..s
   81 	   104 	 0.05312 	 0.08794 	 m..s
   65 	   105 	 0.05148 	 0.09031 	 m..s
  106 	   106 	 0.08155 	 0.09364 	 ~...
  103 	   107 	 0.07097 	 0.09730 	 ~...
  109 	   108 	 0.11156 	 0.10118 	 ~...
  102 	   109 	 0.06993 	 0.10479 	 m..s
  104 	   110 	 0.07097 	 0.10698 	 m..s
  111 	   111 	 0.12807 	 0.13087 	 ~...
  107 	   112 	 0.09011 	 0.13256 	 m..s
  108 	   113 	 0.09278 	 0.13941 	 m..s
  110 	   114 	 0.11828 	 0.14306 	 ~...
  113 	   115 	 0.20061 	 0.16794 	 m..s
  112 	   116 	 0.19496 	 0.17068 	 ~...
  114 	   117 	 0.23765 	 0.25971 	 ~...
  115 	   118 	 0.25961 	 0.27826 	 ~...
  120 	   119 	 0.27979 	 0.32392 	 m..s
  116 	   120 	 0.25998 	 0.34191 	 m..s
==========================================
r_mrr = 0.6646853089332581
r2_mrr = 0.18001717329025269
spearmanr_mrr@5 = 0.9588333964347839
spearmanr_mrr@10 = 0.7319212555885315
spearmanr_mrr@50 = 0.887523353099823
spearmanr_mrr@100 = 0.8955157995223999
spearmanr_mrr@All = 0.9010586738586426
==========================================
test time: 0.459
Done Testing dataset OpenEA
total time taken: 253.4846329689026
training time taken: 226.35844588279724
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.6647)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.1800)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9588)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.7319)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.8875)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.8955)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9011)}}, 'test_loss': {'ComplEx': {'OpenEA': 1.7622528013853298}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 4815533610657581
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [246, 952, 259, 564, 92, 1210, 994, 742, 110, 1171, 1143, 215, 580, 262, 623, 425, 21, 490, 229, 3, 178, 610, 1068, 1057, 25, 249, 281, 650, 790, 993, 1063, 431, 1033, 1008, 266, 112, 157, 1162, 1211, 627, 755, 765, 603, 424, 908, 822, 1021, 678, 1193, 740, 1098, 975, 657, 582, 950, 270, 343, 197, 34, 486, 322, 593, 273, 234, 1016, 460, 293, 642, 918, 662, 708, 988, 959, 156, 668, 739, 344, 379, 94, 934, 52, 992, 557, 601, 254, 520, 962, 541, 922, 658, 677, 581, 48, 910, 146, 64, 130, 1155, 764, 5, 168, 572, 165, 1093, 535, 1133, 15, 1074, 998, 127, 1131, 1061, 365, 944, 473, 1109, 892, 710, 268, 751, 1005]
valid_ids (0): []
train_ids (1094): [219, 760, 715, 1026, 149, 1213, 57, 702, 134, 615, 345, 480, 212, 1060, 835, 301, 394, 114, 855, 663, 653, 537, 1092, 1097, 88, 479, 831, 1132, 47, 628, 1121, 218, 575, 781, 488, 420, 984, 83, 9, 1066, 107, 498, 524, 1158, 1078, 275, 95, 300, 1001, 550, 738, 961, 442, 812, 70, 1043, 566, 629, 217, 186, 871, 693, 870, 290, 417, 1204, 1056, 438, 45, 435, 184, 966, 1198, 144, 841, 309, 400, 848, 508, 965, 1180, 607, 754, 291, 26, 354, 141, 940, 792, 1000, 546, 443, 357, 298, 1095, 898, 261, 151, 421, 671, 687, 86, 74, 507, 0, 265, 482, 771, 885, 469, 609, 888, 744, 447, 849, 103, 277, 904, 113, 779, 489, 314, 690, 163, 466, 982, 1176, 643, 495, 311, 1054, 1117, 717, 203, 890, 748, 308, 866, 857, 865, 821, 619, 87, 987, 830, 378, 875, 280, 207, 883, 1019, 478, 98, 891, 1208, 785, 1029, 85, 1126, 963, 237, 328, 78, 100, 1114, 423, 932, 384, 1062, 720, 803, 1045, 971, 660, 173, 1144, 12, 929, 373, 366, 2, 1166, 823, 576, 1183, 410, 382, 774, 879, 757, 1138, 787, 313, 39, 169, 376, 351, 882, 459, 145, 172, 91, 729, 712, 24, 29, 17, 1172, 1091, 463, 768, 968, 1173, 53, 894, 1145, 705, 521, 1018, 493, 1163, 125, 868, 540, 419, 121, 859, 986, 182, 450, 725, 170, 716, 1152, 224, 287, 63, 1202, 321, 383, 1165, 105, 1125, 1207, 35, 93, 18, 4, 867, 636, 1042, 513, 719, 1123, 617, 612, 1004, 342, 304, 198, 1148, 625, 844, 230, 776, 798, 1002, 483, 590, 302, 369, 363, 1147, 96, 555, 737, 356, 1041, 913, 468, 786, 797, 858, 56, 955, 377, 104, 251, 1159, 210, 359, 766, 728, 752, 534, 303, 679, 811, 1113, 554, 1201, 205, 721, 750, 462, 645, 532, 245, 808, 334, 109, 414, 1070, 1122, 856, 1178, 333, 452, 630, 142, 574, 1164, 842, 272, 772, 81, 887, 282, 364, 166, 746, 549, 7, 264, 1059, 602, 990, 1096, 135, 850, 767, 1187, 128, 506, 1194, 689, 701, 1003, 214, 139, 604, 23, 533, 241, 189, 16, 296, 516, 375, 519, 976, 122, 390, 225, 1076, 396, 22, 565, 531, 732, 547, 618, 267, 248, 1081, 640, 299, 1177, 133, 880, 499, 718, 353, 408, 500, 1013, 621, 77, 350, 89, 73, 67, 526, 683, 514, 682, 818, 1052, 881, 349, 46, 773, 1119, 152, 733, 55, 399, 324, 51, 132, 957, 54, 14, 11, 919, 1214, 1046, 1129, 634, 434, 905, 1101, 84, 544, 530, 568, 1192, 412, 1058, 199, 795, 131, 999, 209, 320, 921, 522, 115, 387, 405, 295, 876, 845, 348, 1038, 430, 1174, 644, 228, 372, 639, 253, 699, 27, 1009, 759, 911, 256, 886, 1146, 1151, 852, 902, 806, 1007, 1141, 76, 188, 896, 1032, 588, 1084, 30, 553, 154, 1099, 1047, 255, 780, 579, 325, 415, 641, 174, 980, 1195, 1127, 1137, 872, 727, 614, 318, 525, 567, 825, 223, 595, 1161, 936, 561, 160, 611, 698, 815, 793, 920, 801, 392, 1006, 1079, 1136, 613, 153, 406, 1206, 161, 43, 310, 99, 445, 654, 1088, 691, 1011, 1154, 448, 10, 429, 745, 380, 1010, 928, 1055, 492, 843, 1209, 711, 809, 232, 1139, 362, 32, 502, 1196, 196, 707, 1075, 545, 1106, 997, 72, 222, 402, 370, 457, 813, 200, 931, 1182, 374, 407, 206, 860, 620, 973, 204, 509, 1031, 454, 758, 220, 187, 583, 158, 191, 368, 464, 167, 829, 80, 472, 909, 433, 1130, 247, 1035, 926, 924, 50, 647, 775, 700, 242, 937, 42, 227, 724, 969, 58, 735, 893, 1087, 297, 527, 573, 1024, 1150, 346, 594, 985, 585, 504, 596, 1199, 233, 861, 907, 1184, 659, 202, 529, 586, 782, 260, 60, 386, 709, 1175, 1053, 147, 942, 467, 958, 578, 41, 978, 484, 1179, 238, 970, 951, 176, 824, 75, 385, 960, 1181, 1025, 355, 938, 827, 477, 305, 1080, 800, 213, 171, 116, 817, 1103, 180, 1083, 626, 862, 1190, 129, 61, 159, 1157, 675, 676, 646, 292, 672, 179, 1128, 286, 393, 927, 743, 539, 111, 226, 1102, 221, 68, 804, 1167, 140, 244, 79, 807, 941, 991, 637, 558, 1034, 749, 1115, 622, 945, 632, 1090, 306, 914, 401, 599, 736, 404, 470, 1086, 126, 597, 446, 570, 1212, 784, 331, 1064, 38, 69, 552, 956, 317, 1124, 193, 1108, 118, 326, 563, 416, 802, 847, 1140, 571, 917, 106, 703, 589, 694, 1170, 954, 923, 1, 605, 136, 820, 669, 826, 162, 741, 258, 428, 953, 901, 458, 474, 608, 819, 651, 1089, 491, 933, 455, 1065, 706, 930, 36, 1200, 235, 395, 269, 681, 8, 243, 461, 1189, 195, 475, 816, 481, 427, 1094, 1153, 398, 981, 97, 884, 667, 528, 471, 633, 897, 236, 624, 1205, 391, 1023, 889, 316, 551, 1082, 1085, 799, 1168, 686, 1120, 1160, 389, 330, 794, 211, 692, 684, 323, 912, 515, 285, 1051, 44, 1048, 426, 878, 638, 964, 763, 1028, 403, 65, 441, 307, 388, 895, 777, 935, 352, 216, 497, 695, 190, 577, 358, 503, 837, 789, 381, 979, 949, 332, 102, 494, 680, 274, 1203, 769, 730, 1135, 543, 90, 347, 1110, 397, 338, 164, 796, 279, 836, 451, 906, 119, 1037, 239, 834, 1191, 148, 231, 194, 512, 974, 1039, 143, 900, 432, 723, 439, 335, 1100, 569, 833, 201, 704, 722, 559, 150, 449, 713, 1112, 117, 1077, 538, 697, 252, 283, 371, 476, 840, 674, 40, 562, 1036, 66, 13, 289, 869, 120, 123, 548, 1022, 485, 838, 327, 1030, 1067, 465, 496, 319, 972, 696, 284, 591, 989, 183, 832, 1044, 600, 903, 814, 155, 437, 517, 805, 294, 943, 33, 436, 59, 846, 340, 864, 731, 422, 853, 560, 409, 863, 606, 839, 616, 747, 1116, 584, 288, 453, 1071, 670, 598, 649, 1069, 665, 874, 761, 208, 983, 339, 271, 948, 877, 1149, 501, 337, 1012, 315, 181, 518, 542, 413, 361, 810, 329, 1169, 177, 240, 341, 1107, 418, 1111, 635, 977, 1027, 185, 278, 263, 854, 31, 756, 946, 1104, 71, 440, 360, 1017, 124, 411, 20, 192, 666, 661, 685, 631, 1073, 1015, 536, 648, 947, 37, 788, 276, 510, 108, 1142, 367, 652, 783, 556, 456, 137, 1156, 1049, 312, 511, 28, 967, 664, 673, 1020, 336, 995, 1197, 851, 778, 791, 82, 1185, 762, 49, 175, 250, 916, 925, 1186, 828, 1072, 138, 656, 1188, 996, 726, 1050, 753, 62, 1040, 915, 505, 1118, 6, 873, 1105, 523, 444, 939, 257, 655, 714, 592, 688, 487, 734, 19, 1014, 770, 101, 587, 1134, 899]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9632047060569872
the save name prefix for this run is:  chkpt-ID_9632047060569872_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 733
rank avg (pred): 0.562 +- 0.005
mrr vals (pred, true): 0.000, 0.347
batch losses (mrrl, rdl): 0.0, 0.0049530002

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 722
rank avg (pred): 0.448 +- 0.289
mrr vals (pred, true): 0.078, 0.001
batch losses (mrrl, rdl): 0.0, 1.53641e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 721
rank avg (pred): 0.472 +- 0.298
mrr vals (pred, true): 0.081, 0.001
batch losses (mrrl, rdl): 0.0, 7.6599e-06

Epoch over!
epoch time: 14.955

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 42
rank avg (pred): 0.330 +- 0.227
mrr vals (pred, true): 0.089, 0.088
batch losses (mrrl, rdl): 0.0, 4.04539e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 20
rank avg (pred): 0.225 +- 0.172
mrr vals (pred, true): 0.095, 0.291
batch losses (mrrl, rdl): 0.0, 0.0002539804

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1064
rank avg (pred): 0.314 +- 0.258
mrr vals (pred, true): 0.054, 0.064
batch losses (mrrl, rdl): 0.0, 3.38671e-05

Epoch over!
epoch time: 14.938

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 768
rank avg (pred): 0.439 +- 0.301
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 0.0, 2.73675e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 930
rank avg (pred): 0.540 +- 0.330
mrr vals (pred, true): 0.028, 0.001
batch losses (mrrl, rdl): 0.0, 4.80431e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1198
rank avg (pred): 0.480 +- 0.307
mrr vals (pred, true): 0.028, 0.001
batch losses (mrrl, rdl): 0.0, 4.3524e-06

Epoch over!
epoch time: 14.928

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 288
rank avg (pred): 0.325 +- 0.309
mrr vals (pred, true): 0.047, 0.072
batch losses (mrrl, rdl): 0.0, 7.0599e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 209
rank avg (pred): 0.510 +- 0.281
mrr vals (pred, true): 0.030, 0.001
batch losses (mrrl, rdl): 0.0, 5.5179e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1166
rank avg (pred): 0.501 +- 0.317
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0, 2.1483e-06

Epoch over!
epoch time: 14.933

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 77
rank avg (pred): 0.325 +- 0.326
mrr vals (pred, true): 0.064, 0.085
batch losses (mrrl, rdl): 0.0, 1.13323e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1082
rank avg (pred): 0.505 +- 0.290
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0, 4.5672e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 357
rank avg (pred): 0.490 +- 0.294
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0, 4.853e-07

Epoch over!
epoch time: 14.929

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 946
rank avg (pred): 0.542 +- 0.294
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001207075, 2.68072e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.283 +- 0.391
mrr vals (pred, true): 0.129, 0.171
batch losses (mrrl, rdl): 0.0177164953, 4.67425e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 54
rank avg (pred): 0.484 +- 0.318
mrr vals (pred, true): 0.067, 0.071
batch losses (mrrl, rdl): 0.0029582912, 0.0002614774

Epoch over!
epoch time: 15.148

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 638
rank avg (pred): 0.525 +- 0.248
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0014457166, 1.43838e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 338
rank avg (pred): 0.487 +- 0.290
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 4.40873e-05, 6.3442e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 971
rank avg (pred): 0.625 +- 0.262
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0007779294, 0.0002786165

Epoch over!
epoch time: 15.142

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 65
rank avg (pred): 0.482 +- 0.301
mrr vals (pred, true): 0.054, 0.085
batch losses (mrrl, rdl): 0.0001745163, 0.0003362333

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 913
rank avg (pred): 0.484 +- 0.331
mrr vals (pred, true): 0.069, 0.002
batch losses (mrrl, rdl): 0.0034736213, 0.0002087124

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 982
rank avg (pred): 0.410 +- 0.362
mrr vals (pred, true): 0.067, 0.072
batch losses (mrrl, rdl): 0.0027402388, 4.49496e-05

Epoch over!
epoch time: 15.145

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 328
rank avg (pred): 0.498 +- 0.297
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.85215e-05, 4.0176e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 598
rank avg (pred): 0.514 +- 0.265
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003370612, 8.3777e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 179
rank avg (pred): 0.534 +- 0.279
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.000104218, 1.12489e-05

Epoch over!
epoch time: 15.131

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 808
rank avg (pred): 0.519 +- 0.285
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001335746, 5.6413e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 353
rank avg (pred): 0.538 +- 0.251
mrr vals (pred, true): 0.039, 0.002
batch losses (mrrl, rdl): 0.0011731536, 1.88423e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 920
rank avg (pred): 0.546 +- 0.266
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0009370368, 1.51344e-05

Epoch over!
epoch time: 15.124

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 587
rank avg (pred): 0.478 +- 0.329
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005697985, 1.74137e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 960
rank avg (pred): 0.625 +- 0.282
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003272778, 0.0002136856

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.507 +- 0.266
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.06284e-05, 8.142e-06

Epoch over!
epoch time: 15.133

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 240
rank avg (pred): 0.504 +- 0.268
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.23008e-05, 8.9549e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 555
rank avg (pred): 0.428 +- 0.293
mrr vals (pred, true): 0.047, 0.070
batch losses (mrrl, rdl): 7.768e-05, 4.42848e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 280
rank avg (pred): 0.448 +- 0.280
mrr vals (pred, true): 0.043, 0.085
batch losses (mrrl, rdl): 0.0005158576, 0.0001202817

Epoch over!
epoch time: 15.126

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 602
rank avg (pred): 0.470 +- 0.271
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004872048, 7.34861e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 744
rank avg (pred): 0.291 +- 0.429
mrr vals (pred, true): 0.218, 0.206
batch losses (mrrl, rdl): 0.0015094012, 0.0002061057

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 24
rank avg (pred): 0.285 +- 0.429
mrr vals (pred, true): 0.241, 0.260
batch losses (mrrl, rdl): 0.0035105688, 0.0006496027

Epoch over!
epoch time: 15.143

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1168
rank avg (pred): 0.503 +- 0.264
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.72098e-05, 1.52513e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 192
rank avg (pred): 0.514 +- 0.253
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003107188, 1.46128e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 929
rank avg (pred): 0.496 +- 0.274
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 9.83e-08, 2.26381e-05

Epoch over!
epoch time: 15.149

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 306
rank avg (pred): 0.404 +- 0.297
mrr vals (pred, true): 0.049, 0.077
batch losses (mrrl, rdl): 1.81517e-05, 3.34583e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1075
rank avg (pred): 0.458 +- 0.387
mrr vals (pred, true): 0.072, 0.097
batch losses (mrrl, rdl): 0.0048415884, 0.0002198043

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 68
rank avg (pred): 0.429 +- 0.291
mrr vals (pred, true): 0.052, 0.089
batch losses (mrrl, rdl): 3.24605e-05, 8.09722e-05

Epoch over!
epoch time: 15.116

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.247 +- 0.410
mrr vals (pred, true): 0.120, 0.080

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   82 	     0 	 0.05505 	 0.00021 	 m..s
   24 	     1 	 0.04604 	 0.00047 	 m..s
   13 	     2 	 0.04510 	 0.00048 	 m..s
    8 	     3 	 0.04462 	 0.00051 	 m..s
   28 	     4 	 0.04634 	 0.00051 	 m..s
   18 	     5 	 0.04556 	 0.00052 	 m..s
   50 	     6 	 0.04857 	 0.00053 	 m..s
   15 	     7 	 0.04518 	 0.00053 	 m..s
   11 	     8 	 0.04481 	 0.00055 	 m..s
    1 	     9 	 0.04184 	 0.00055 	 m..s
   56 	    10 	 0.04944 	 0.00056 	 m..s
    0 	    11 	 0.04178 	 0.00056 	 m..s
   65 	    12 	 0.05088 	 0.00058 	 m..s
   17 	    13 	 0.04530 	 0.00058 	 m..s
   12 	    14 	 0.04504 	 0.00059 	 m..s
    4 	    15 	 0.04398 	 0.00059 	 m..s
   69 	    16 	 0.05116 	 0.00060 	 m..s
    7 	    17 	 0.04452 	 0.00060 	 m..s
   22 	    18 	 0.04598 	 0.00061 	 m..s
   36 	    19 	 0.04726 	 0.00062 	 m..s
   79 	    20 	 0.05393 	 0.00063 	 m..s
   42 	    21 	 0.04779 	 0.00063 	 m..s
   60 	    22 	 0.05038 	 0.00064 	 m..s
   71 	    23 	 0.05123 	 0.00064 	 m..s
   54 	    24 	 0.04917 	 0.00068 	 m..s
   16 	    25 	 0.04519 	 0.00069 	 m..s
    2 	    26 	 0.04246 	 0.00070 	 m..s
   39 	    27 	 0.04755 	 0.00070 	 m..s
    3 	    28 	 0.04283 	 0.00072 	 m..s
   74 	    29 	 0.05231 	 0.00072 	 m..s
   63 	    30 	 0.05045 	 0.00077 	 m..s
    5 	    31 	 0.04422 	 0.00077 	 m..s
   67 	    32 	 0.05099 	 0.00079 	 m..s
   21 	    33 	 0.04595 	 0.00079 	 m..s
   20 	    34 	 0.04581 	 0.00081 	 m..s
   75 	    35 	 0.05233 	 0.00081 	 m..s
   76 	    36 	 0.05253 	 0.00082 	 m..s
    6 	    37 	 0.04443 	 0.00086 	 m..s
   52 	    38 	 0.04892 	 0.00088 	 m..s
   53 	    39 	 0.04916 	 0.00088 	 m..s
   59 	    40 	 0.05029 	 0.00089 	 m..s
   58 	    41 	 0.04973 	 0.00093 	 m..s
   19 	    42 	 0.04571 	 0.00097 	 m..s
   57 	    43 	 0.04967 	 0.00097 	 m..s
   43 	    44 	 0.04783 	 0.00099 	 m..s
   61 	    45 	 0.05041 	 0.00100 	 m..s
   72 	    46 	 0.05126 	 0.00103 	 m..s
   33 	    47 	 0.04701 	 0.00103 	 m..s
   27 	    48 	 0.04632 	 0.00103 	 m..s
   37 	    49 	 0.04734 	 0.00106 	 m..s
   25 	    50 	 0.04607 	 0.00116 	 m..s
   10 	    51 	 0.04475 	 0.00116 	 m..s
   45 	    52 	 0.04803 	 0.00117 	 m..s
   34 	    53 	 0.04721 	 0.00123 	 m..s
   41 	    54 	 0.04773 	 0.00125 	 m..s
   68 	    55 	 0.05103 	 0.00130 	 m..s
   73 	    56 	 0.05146 	 0.00139 	 m..s
   62 	    57 	 0.05042 	 0.00139 	 m..s
   29 	    58 	 0.04658 	 0.00144 	 m..s
    9 	    59 	 0.04466 	 0.00145 	 m..s
   46 	    60 	 0.04807 	 0.00151 	 m..s
   64 	    61 	 0.05058 	 0.00169 	 m..s
   31 	    62 	 0.04670 	 0.00174 	 m..s
   77 	    63 	 0.05277 	 0.00176 	 m..s
   70 	    64 	 0.05119 	 0.00184 	 m..s
   26 	    65 	 0.04630 	 0.00186 	 m..s
   38 	    66 	 0.04737 	 0.00189 	 m..s
   66 	    67 	 0.05089 	 0.00198 	 m..s
   30 	    68 	 0.04669 	 0.00200 	 m..s
   14 	    69 	 0.04512 	 0.00207 	 m..s
   55 	    70 	 0.04922 	 0.00234 	 m..s
   35 	    71 	 0.04722 	 0.00245 	 m..s
   78 	    72 	 0.05344 	 0.00487 	 m..s
   94 	    73 	 0.06608 	 0.04540 	 ~...
   81 	    74 	 0.05499 	 0.04703 	 ~...
   97 	    75 	 0.08064 	 0.05180 	 ~...
   80 	    76 	 0.05429 	 0.06068 	 ~...
   49 	    77 	 0.04856 	 0.06381 	 ~...
   48 	    78 	 0.04845 	 0.06890 	 ~...
   89 	    79 	 0.06211 	 0.07066 	 ~...
   86 	    80 	 0.06110 	 0.07073 	 ~...
   87 	    81 	 0.06171 	 0.07104 	 ~...
  100 	    82 	 0.12048 	 0.07111 	 m..s
   23 	    83 	 0.04601 	 0.07213 	 ~...
   51 	    84 	 0.04882 	 0.07472 	 ~...
   98 	    85 	 0.08309 	 0.07475 	 ~...
   40 	    86 	 0.04770 	 0.07528 	 ~...
   84 	    87 	 0.06000 	 0.07874 	 ~...
   92 	    88 	 0.06377 	 0.07948 	 ~...
  105 	    89 	 0.13164 	 0.07963 	 m..s
   99 	    90 	 0.12043 	 0.08036 	 m..s
   32 	    91 	 0.04673 	 0.08220 	 m..s
   83 	    92 	 0.05850 	 0.08269 	 ~...
   93 	    93 	 0.06435 	 0.08301 	 ~...
   47 	    94 	 0.04831 	 0.08575 	 m..s
   44 	    95 	 0.04792 	 0.08827 	 m..s
   85 	    96 	 0.06054 	 0.08960 	 ~...
   90 	    97 	 0.06217 	 0.10268 	 m..s
   91 	    98 	 0.06228 	 0.10389 	 m..s
   88 	    99 	 0.06198 	 0.10555 	 m..s
  103 	   100 	 0.12434 	 0.11871 	 ~...
  102 	   101 	 0.12376 	 0.12358 	 ~...
  101 	   102 	 0.12131 	 0.12494 	 ~...
   95 	   103 	 0.06706 	 0.13290 	 m..s
  104 	   104 	 0.12482 	 0.13560 	 ~...
   96 	   105 	 0.07543 	 0.14116 	 m..s
  106 	   106 	 0.13674 	 0.14416 	 ~...
  109 	   107 	 0.14617 	 0.14575 	 ~...
  107 	   108 	 0.14528 	 0.16228 	 ~...
  110 	   109 	 0.16520 	 0.16706 	 ~...
  108 	   110 	 0.14615 	 0.17976 	 m..s
  111 	   111 	 0.16791 	 0.18394 	 ~...
  113 	   112 	 0.18582 	 0.19048 	 ~...
  114 	   113 	 0.20068 	 0.24200 	 m..s
  116 	   114 	 0.23318 	 0.25288 	 ~...
  115 	   115 	 0.20211 	 0.27955 	 m..s
  117 	   116 	 0.24752 	 0.28122 	 m..s
  119 	   117 	 0.26208 	 0.28260 	 ~...
  112 	   118 	 0.18293 	 0.28613 	 MISS
  118 	   119 	 0.25856 	 0.28749 	 ~...
  120 	   120 	 0.28488 	 0.29805 	 ~...
==========================================
r_mrr = 0.919541597366333
r2_mrr = 0.7116194367408752
spearmanr_mrr@5 = 0.9633094668388367
spearmanr_mrr@10 = 0.8496059775352478
spearmanr_mrr@50 = 0.9729501008987427
spearmanr_mrr@100 = 0.9507079720497131
spearmanr_mrr@All = 0.9530842900276184
==========================================
test time: 0.463
Done Testing dataset OpenEA
total time taken: 253.8115677833557
training time taken: 226.51154685020447
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.9195)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.7116)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9633)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.8496)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9730)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9507)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9531)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.5960381619215696}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 3920567194370276
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [388, 775, 888, 579, 663, 357, 506, 214, 89, 372, 867, 307, 208, 1175, 172, 642, 993, 1005, 563, 558, 7, 1183, 43, 70, 1153, 546, 770, 246, 187, 381, 447, 489, 585, 378, 390, 715, 661, 193, 281, 85, 1152, 1060, 363, 112, 157, 1192, 32, 236, 29, 100, 1154, 396, 476, 120, 152, 404, 190, 932, 394, 653, 826, 416, 335, 505, 1049, 1204, 501, 305, 693, 473, 599, 493, 437, 813, 502, 571, 842, 491, 130, 697, 898, 687, 1047, 735, 223, 71, 329, 889, 1167, 438, 1000, 837, 584, 1053, 976, 670, 801, 1027, 44, 515, 864, 119, 131, 971, 276, 827, 751, 460, 425, 561, 519, 373, 610, 279, 86, 139, 948, 996, 729, 603, 251]
valid_ids (0): []
train_ids (1094): [213, 485, 533, 707, 1117, 349, 1004, 762, 31, 391, 723, 588, 1078, 630, 941, 957, 21, 862, 103, 30, 1190, 1038, 312, 446, 270, 740, 125, 601, 886, 819, 1011, 15, 475, 1057, 738, 52, 512, 299, 925, 646, 154, 828, 166, 1012, 896, 692, 250, 574, 1023, 69, 41, 589, 146, 121, 672, 592, 1186, 1066, 737, 641, 780, 622, 698, 821, 829, 868, 778, 934, 294, 61, 626, 793, 654, 354, 528, 1029, 1178, 235, 424, 161, 123, 91, 1169, 822, 68, 1164, 488, 612, 718, 40, 262, 189, 232, 796, 160, 392, 1101, 665, 1142, 1062, 598, 755, 303, 726, 355, 426, 352, 342, 700, 1203, 183, 716, 1080, 849, 1189, 529, 451, 648, 174, 368, 1010, 768, 1042, 550, 1126, 38, 51, 406, 118, 521, 836, 313, 947, 960, 455, 138, 365, 772, 159, 99, 840, 359, 710, 1025, 746, 129, 1075, 317, 221, 107, 903, 1081, 732, 800, 596, 149, 790, 967, 508, 841, 559, 1065, 635, 669, 291, 371, 703, 478, 75, 1063, 1006, 217, 551, 383, 962, 802, 1150, 468, 815, 771, 535, 321, 666, 482, 853, 855, 562, 965, 542, 135, 414, 311, 517, 701, 852, 901, 206, 835, 348, 469, 547, 415, 351, 197, 760, 1120, 496, 1100, 831, 549, 440, 839, 134, 1021, 399, 163, 894, 42, 922, 578, 938, 266, 1149, 370, 345, 195, 298, 736, 548, 807, 804, 434, 611, 20, 36, 48, 974, 725, 1119, 464, 795, 178, 369, 117, 682, 463, 1013, 538, 531, 744, 1077, 227, 683, 1094, 743, 791, 316, 634, 225, 946, 1043, 685, 27, 985, 680, 1185, 587, 1140, 169, 1137, 106, 1113, 741, 207, 456, 572, 385, 769, 1195, 878, 875, 429, 93, 238, 586, 895, 293, 577, 173, 268, 444, 500, 252, 880, 1079, 1184, 1114, 498, 63, 26, 997, 607, 609, 904, 817, 116, 629, 647, 1035, 869, 748, 1197, 799, 84, 1107, 192, 420, 1028, 185, 649, 633, 97, 1097, 758, 727, 1084, 691, 65, 600, 73, 619, 516, 428, 617, 871, 659, 1116, 64, 297, 1022, 866, 955, 750, 1198, 17, 4, 376, 490, 461, 708, 1045, 824, 1214, 540, 580, 47, 568, 315, 713, 637, 209, 702, 714, 690, 644, 541, 194, 353, 1020, 931, 304, 274, 927, 620, 308, 1118, 92, 1115, 556, 430, 24, 1099, 199, 514, 203, 215, 151, 337, 233, 1095, 1069, 495, 1034, 344, 411, 782, 882, 1098, 263, 1112, 761, 511, 1155, 499, 966, 928, 614, 109, 570, 747, 720, 115, 763, 306, 413, 991, 523, 144, 893, 838, 259, 834, 393, 1146, 675, 1207, 261, 230, 786, 58, 497, 581, 22, 981, 1187, 54, 854, 709, 458, 324, 806, 892, 1172, 441, 454, 1026, 459, 5, 846, 285, 220, 504, 436, 1211, 623, 216, 162, 127, 228, 11, 845, 101, 814, 899, 389, 62, 167, 242, 569, 356, 1181, 1200, 487, 977, 1170, 492, 678, 231, 972, 936, 328, 1179, 265, 857, 764, 1129, 239, 50, 1030, 124, 883, 12, 273, 384, 472, 906, 13, 905, 212, 699, 658, 890, 552, 452, 379, 1109, 939, 1014, 145, 168, 797, 865, 1159, 900, 1138, 986, 210, 543, 560, 933, 636, 474, 361, 945, 767, 908, 509, 310, 963, 861, 327, 914, 792, 926, 1058, 677, 382, 643, 1024, 628, 255, 1051, 616, 314, 887, 39, 76, 566, 776, 417, 387, 553, 794, 188, 1122, 0, 1128, 1143, 1196, 35, 87, 929, 565, 1160, 1212, 833, 133, 367, 959, 19, 798, 1134, 527, 386, 526, 789, 1072, 320, 300, 339, 2, 733, 1108, 1032, 465, 1007, 198, 921, 431, 289, 330, 990, 830, 583, 442, 1166, 1103, 343, 419, 241, 975, 410, 450, 979, 1105, 877, 754, 95, 8, 397, 1206, 923, 322, 992, 260, 229, 486, 935, 6, 181, 1135, 1208, 818, 1144, 590, 1133, 536, 940, 1180, 122, 57, 968, 1083, 812, 811, 844, 10, 667, 248, 752, 1067, 374, 432, 94, 1050, 72, 785, 573, 1074, 1056, 924, 1059, 1157, 1131, 287, 919, 427, 650, 809, 773, 90, 518, 1052, 377, 591, 1205, 1070, 114, 673, 987, 942, 78, 958, 175, 247, 111, 902, 597, 1018, 480, 332, 564, 350, 34, 618, 301, 756, 218, 621, 724, 978, 136, 158, 59, 734, 1139, 477, 16, 326, 916, 989, 284, 870, 1089, 645, 457, 1076, 483, 105, 681, 45, 863, 243, 88, 325, 671, 1033, 721, 765, 18, 627, 874, 631, 334, 625, 143, 471, 341, 1082, 9, 881, 104, 576, 1158, 850, 346, 267, 980, 1136, 848, 937, 582, 400, 1046, 719, 1209, 333, 1093, 366, 695, 155, 81, 532, 292, 689, 944, 1161, 234, 271, 401, 1041, 1037, 165, 398, 915, 479, 257, 728, 176, 323, 53, 843, 439, 1111, 1182, 575, 1092, 67, 608, 171, 885, 872, 674, 956, 766, 749, 200, 338, 1188, 555, 803, 331, 453, 950, 706, 1201, 730, 153, 113, 137, 781, 522, 503, 205, 49, 1176, 742, 264, 98, 1193, 557, 1087, 1003, 240, 1156, 994, 722, 449, 1102, 1199, 897, 982, 448, 539, 1125, 66, 1141, 731, 1, 196, 407, 1106, 999, 534, 911, 951, 375, 879, 825, 668, 1019, 1194, 759, 912, 606, 55, 340, 918, 191, 180, 286, 28, 1213, 148, 278, 1165, 909, 593, 953, 1174, 433, 783, 943, 1073, 275, 177, 686, 1001, 147, 421, 510, 96, 1104, 1096, 662, 638, 108, 1110, 816, 254, 704, 403, 1086, 83, 202, 405, 80, 358, 110, 545, 920, 624, 402, 364, 1040, 484, 408, 605, 126, 1090, 362, 3, 1151, 290, 983, 445, 74, 1055, 688, 705, 820, 132, 684, 211, 1191, 530, 1168, 1163, 282, 23, 1162, 470, 1036, 998, 604, 1171, 296, 513, 319, 652, 664, 224, 466, 226, 660, 283, 891, 219, 494, 711, 1124, 988, 970, 969, 954, 657, 524, 712, 336, 467, 1048, 787, 245, 615, 808, 1015, 961, 784, 1002, 102, 860, 184, 779, 788, 253, 1044, 973, 632, 640, 422, 1009, 60, 1123, 1039, 182, 984, 141, 858, 1088, 676, 302, 156, 525, 856, 696, 876, 917, 1031, 1064, 204, 280, 851, 694, 1147, 186, 520, 613, 380, 423, 481, 1173, 964, 309, 82, 873, 1145, 554, 79, 655, 1121, 753, 1130, 409, 272, 537, 639, 595, 810, 823, 256, 462, 1210, 295, 1017, 777, 1132, 907, 774, 237, 952, 25, 288, 884, 164, 1054, 1071, 201, 37, 757, 140, 258, 745, 1016, 1177, 847, 142, 77, 1202, 656, 859, 418, 1148, 739, 443, 717, 222, 594, 602, 544, 1061, 507, 46, 995, 1085, 1068, 949, 179, 567, 805, 910, 170, 269, 412, 1127, 435, 14, 347, 930, 277, 249, 1091, 679, 244, 318, 1008, 128, 56, 395, 913, 360, 651, 150, 832, 33]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  989689467638119
the save name prefix for this run is:  chkpt-ID_989689467638119_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 291
rank avg (pred): 0.447 +- 0.006
mrr vals (pred, true): 0.000, 0.074
batch losses (mrrl, rdl): 0.0, 0.0003534663

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 350
rank avg (pred): 0.491 +- 0.008
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001106607

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 516
rank avg (pred): 0.377 +- 0.315
mrr vals (pred, true): 0.001, 0.075
batch losses (mrrl, rdl): 0.0, 4.8863e-06

Epoch over!
epoch time: 14.848

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 547
rank avg (pred): 0.403 +- 0.311
mrr vals (pred, true): 0.002, 0.083
batch losses (mrrl, rdl): 0.0, 1.97502e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1065
rank avg (pred): 0.299 +- 0.285
mrr vals (pred, true): 0.005, 0.102
batch losses (mrrl, rdl): 0.0, 3.91158e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 97
rank avg (pred): 0.492 +- 0.305
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 1.0227e-06

Epoch over!
epoch time: 14.834

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1172
rank avg (pred): 0.479 +- 0.307
mrr vals (pred, true): 0.002, 0.002
batch losses (mrrl, rdl): 0.0, 2.3718e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 94
rank avg (pred): 0.519 +- 0.318
mrr vals (pred, true): 0.001, 0.002
batch losses (mrrl, rdl): 0.0, 1.50186e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1020
rank avg (pred): 0.513 +- 0.300
mrr vals (pred, true): 0.001, 0.002
batch losses (mrrl, rdl): 0.0, 3.5621e-06

Epoch over!
epoch time: 14.843

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 665
rank avg (pred): 0.485 +- 0.313
mrr vals (pred, true): 0.005, 0.001
batch losses (mrrl, rdl): 0.0, 4.7455e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 659
rank avg (pred): 0.478 +- 0.307
mrr vals (pred, true): 0.009, 0.001
batch losses (mrrl, rdl): 0.0, 2.5379e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 879
rank avg (pred): 0.493 +- 0.303
mrr vals (pred, true): 0.003, 0.000
batch losses (mrrl, rdl): 0.0, 1.2e-06

Epoch over!
epoch time: 14.916

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 638
rank avg (pred): 0.494 +- 0.312
mrr vals (pred, true): 0.007, 0.001
batch losses (mrrl, rdl): 0.0, 1.693e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 233
rank avg (pred): 0.496 +- 0.296
mrr vals (pred, true): 0.002, 0.001
batch losses (mrrl, rdl): 0.0, 6.797e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 789
rank avg (pred): 0.475 +- 0.300
mrr vals (pred, true): 0.007, 0.001
batch losses (mrrl, rdl): 0.0, 3.3244e-06

Epoch over!
epoch time: 14.823

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1195
rank avg (pred): 0.504 +- 0.301
mrr vals (pred, true): 0.002, 0.001
batch losses (mrrl, rdl): 0.0233853161, 7.47e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 91
rank avg (pred): 0.429 +- 0.347
mrr vals (pred, true): 0.079, 0.002
batch losses (mrrl, rdl): 0.0084732641, 0.0001094955

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 224
rank avg (pred): 0.480 +- 0.309
mrr vals (pred, true): 0.029, 0.001
batch losses (mrrl, rdl): 0.0042668036, 1.22805e-05

Epoch over!
epoch time: 15.074

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 677
rank avg (pred): 0.462 +- 0.311
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004481767, 2.73446e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 431
rank avg (pred): 0.479 +- 0.309
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001181042, 7.5427e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 33
rank avg (pred): 0.398 +- 0.313
mrr vals (pred, true): 0.054, 0.071
batch losses (mrrl, rdl): 0.0001658503, 1.82619e-05

Epoch over!
epoch time: 15.045

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 873
rank avg (pred): 0.466 +- 0.303
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001424343, 1.95367e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 129
rank avg (pred): 0.500 +- 0.287
mrr vals (pred, true): 0.047, 0.002
batch losses (mrrl, rdl): 7.11403e-05, 1.3138e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 420
rank avg (pred): 0.505 +- 0.300
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 9.278e-07, 9.745e-07

Epoch over!
epoch time: 15.014

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1055
rank avg (pred): 0.311 +- 0.351
mrr vals (pred, true): 0.092, 0.089
batch losses (mrrl, rdl): 0.017450653, 3.80404e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.550 +- 0.272
mrr vals (pred, true): 0.035, 0.001
batch losses (mrrl, rdl): 0.0022129228, 3.39674e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 879
rank avg (pred): 0.524 +- 0.262
mrr vals (pred, true): 0.042, 0.000
batch losses (mrrl, rdl): 0.0006607956, 8.1738e-06

Epoch over!
epoch time: 15.083

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 654
rank avg (pred): 0.495 +- 0.267
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.47994e-05, 1.09514e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 578
rank avg (pred): 0.489 +- 0.294
mrr vals (pred, true): 0.060, 0.002
batch losses (mrrl, rdl): 0.0010787391, 6.398e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 952
rank avg (pred): 0.507 +- 0.227
mrr vals (pred, true): 0.025, 0.001
batch losses (mrrl, rdl): 0.0061165188, 2.17917e-05

Epoch over!
epoch time: 15.084

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1080
rank avg (pred): 0.513 +- 0.265
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004707349, 7.0999e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 894
rank avg (pred): 0.619 +- 0.345
mrr vals (pred, true): 0.071, 0.016
batch losses (mrrl, rdl): 0.0042780447, 0.0002751626

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 974
rank avg (pred): 0.331 +- 0.345
mrr vals (pred, true): 0.092, 0.090
batch losses (mrrl, rdl): 0.017406702, 1.15519e-05

Epoch over!
epoch time: 15.178

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 257
rank avg (pred): 0.211 +- 0.349
mrr vals (pred, true): 0.175, 0.289
batch losses (mrrl, rdl): 0.129194513, 6.0232e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1092
rank avg (pred): 0.469 +- 0.272
mrr vals (pred, true): 0.056, 0.002
batch losses (mrrl, rdl): 0.0003962697, 1.77773e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 682
rank avg (pred): 0.452 +- 0.268
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.000384943, 7.03732e-05

Epoch over!
epoch time: 15.108

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 455
rank avg (pred): 0.525 +- 0.279
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.38307e-05, 6.2567e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 267
rank avg (pred): 0.126 +- 0.289
mrr vals (pred, true): 0.248, 0.264
batch losses (mrrl, rdl): 0.0023419324, 4.82146e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.513 +- 0.287
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001445235, 2.6609e-06

Epoch over!
epoch time: 15.108

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 331
rank avg (pred): 0.516 +- 0.289
mrr vals (pred, true): 0.055, 0.002
batch losses (mrrl, rdl): 0.0002580764, 4.4189e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 452
rank avg (pred): 0.513 +- 0.278
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.51394e-05, 2.4823e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 129
rank avg (pred): 0.517 +- 0.287
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.23227e-05, 4.3203e-06

Epoch over!
epoch time: 15.111

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1179
rank avg (pred): 0.527 +- 0.290
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 3.1719e-06, 1.14612e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 8
rank avg (pred): 0.230 +- 0.325
mrr vals (pred, true): 0.192, 0.122
batch losses (mrrl, rdl): 0.0491033904, 6.75346e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 598
rank avg (pred): 0.507 +- 0.290
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0011141305, 2.1773e-06

Epoch over!
epoch time: 15.109

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.471 +- 0.306
mrr vals (pred, true): 0.049, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   96 	     0 	 0.06315 	 0.00020 	 m..s
   65 	     1 	 0.05001 	 0.00048 	 m..s
   49 	     2 	 0.04814 	 0.00048 	 m..s
   71 	     3 	 0.05210 	 0.00049 	 m..s
   92 	     4 	 0.05879 	 0.00051 	 m..s
   64 	     5 	 0.04987 	 0.00051 	 m..s
   70 	     6 	 0.05171 	 0.00052 	 m..s
   46 	     7 	 0.04782 	 0.00053 	 m..s
   14 	     8 	 0.04207 	 0.00053 	 m..s
    9 	     9 	 0.04132 	 0.00053 	 m..s
    3 	    10 	 0.04078 	 0.00054 	 m..s
   87 	    11 	 0.05667 	 0.00055 	 m..s
   17 	    12 	 0.04220 	 0.00055 	 m..s
   39 	    13 	 0.04724 	 0.00056 	 m..s
   12 	    14 	 0.04155 	 0.00057 	 m..s
   66 	    15 	 0.05055 	 0.00057 	 m..s
    2 	    16 	 0.04011 	 0.00058 	 m..s
   48 	    17 	 0.04803 	 0.00058 	 m..s
   22 	    18 	 0.04366 	 0.00059 	 m..s
   13 	    19 	 0.04167 	 0.00059 	 m..s
    8 	    20 	 0.04124 	 0.00061 	 m..s
   16 	    21 	 0.04208 	 0.00067 	 m..s
    4 	    22 	 0.04090 	 0.00067 	 m..s
   37 	    23 	 0.04712 	 0.00068 	 m..s
    0 	    24 	 0.03623 	 0.00070 	 m..s
   47 	    25 	 0.04793 	 0.00070 	 m..s
   72 	    26 	 0.05220 	 0.00071 	 m..s
    5 	    27 	 0.04098 	 0.00072 	 m..s
   93 	    28 	 0.05934 	 0.00074 	 m..s
   52 	    29 	 0.04848 	 0.00074 	 m..s
   36 	    30 	 0.04702 	 0.00075 	 m..s
   11 	    31 	 0.04134 	 0.00078 	 m..s
   33 	    32 	 0.04610 	 0.00081 	 m..s
   54 	    33 	 0.04867 	 0.00081 	 m..s
   90 	    34 	 0.05742 	 0.00081 	 m..s
   18 	    35 	 0.04247 	 0.00082 	 m..s
   44 	    36 	 0.04765 	 0.00082 	 m..s
   20 	    37 	 0.04251 	 0.00083 	 m..s
   19 	    38 	 0.04249 	 0.00083 	 m..s
   82 	    39 	 0.05445 	 0.00084 	 m..s
   89 	    40 	 0.05725 	 0.00085 	 m..s
   81 	    41 	 0.05425 	 0.00085 	 m..s
   34 	    42 	 0.04659 	 0.00086 	 m..s
    1 	    43 	 0.03983 	 0.00087 	 m..s
   31 	    44 	 0.04601 	 0.00089 	 m..s
   30 	    45 	 0.04588 	 0.00089 	 m..s
    6 	    46 	 0.04113 	 0.00090 	 m..s
   56 	    47 	 0.04891 	 0.00091 	 m..s
   10 	    48 	 0.04133 	 0.00103 	 m..s
   91 	    49 	 0.05829 	 0.00106 	 m..s
   27 	    50 	 0.04442 	 0.00112 	 m..s
   60 	    51 	 0.04963 	 0.00115 	 m..s
   15 	    52 	 0.04208 	 0.00115 	 m..s
   50 	    53 	 0.04846 	 0.00117 	 m..s
   61 	    54 	 0.04970 	 0.00120 	 m..s
   40 	    55 	 0.04730 	 0.00122 	 m..s
   68 	    56 	 0.05113 	 0.00126 	 m..s
   51 	    57 	 0.04847 	 0.00126 	 m..s
   62 	    58 	 0.04978 	 0.00130 	 m..s
    7 	    59 	 0.04115 	 0.00130 	 m..s
   35 	    60 	 0.04676 	 0.00135 	 m..s
   23 	    61 	 0.04378 	 0.00137 	 m..s
   79 	    62 	 0.05387 	 0.00138 	 m..s
   59 	    63 	 0.04930 	 0.00142 	 m..s
   83 	    64 	 0.05538 	 0.00144 	 m..s
   63 	    65 	 0.04986 	 0.00146 	 m..s
   53 	    66 	 0.04855 	 0.00151 	 m..s
   45 	    67 	 0.04774 	 0.00159 	 m..s
   58 	    68 	 0.04923 	 0.00162 	 m..s
   84 	    69 	 0.05595 	 0.00169 	 m..s
   32 	    70 	 0.04607 	 0.00174 	 m..s
   42 	    71 	 0.04753 	 0.00174 	 m..s
   28 	    72 	 0.04519 	 0.00184 	 m..s
   43 	    73 	 0.04764 	 0.00186 	 m..s
   29 	    74 	 0.04565 	 0.00194 	 m..s
   41 	    75 	 0.04750 	 0.00206 	 m..s
   38 	    76 	 0.04722 	 0.00217 	 m..s
   25 	    77 	 0.04429 	 0.00229 	 m..s
   21 	    78 	 0.04281 	 0.00245 	 m..s
   85 	    79 	 0.05617 	 0.00256 	 m..s
  101 	    80 	 0.08790 	 0.01790 	 m..s
   98 	    81 	 0.06461 	 0.04752 	 ~...
   97 	    82 	 0.06400 	 0.05068 	 ~...
  103 	    83 	 0.10440 	 0.05624 	 m..s
   99 	    84 	 0.06961 	 0.06018 	 ~...
   57 	    85 	 0.04909 	 0.06450 	 ~...
   78 	    86 	 0.05379 	 0.06666 	 ~...
   76 	    87 	 0.05336 	 0.07020 	 ~...
   74 	    88 	 0.05247 	 0.07080 	 ~...
  102 	    89 	 0.10339 	 0.07170 	 m..s
   75 	    90 	 0.05335 	 0.07188 	 ~...
   73 	    91 	 0.05238 	 0.07360 	 ~...
   86 	    92 	 0.05661 	 0.07469 	 ~...
   26 	    93 	 0.04435 	 0.07633 	 m..s
   55 	    94 	 0.04878 	 0.07936 	 m..s
   94 	    95 	 0.06000 	 0.07946 	 ~...
   24 	    96 	 0.04410 	 0.07971 	 m..s
  115 	    97 	 0.20123 	 0.08036 	 MISS
   77 	    98 	 0.05371 	 0.08144 	 ~...
   95 	    99 	 0.06126 	 0.08157 	 ~...
   69 	   100 	 0.05137 	 0.08500 	 m..s
   80 	   101 	 0.05415 	 0.08794 	 m..s
   88 	   102 	 0.05712 	 0.08827 	 m..s
   67 	   103 	 0.05099 	 0.09031 	 m..s
  104 	   104 	 0.11254 	 0.09360 	 ~...
  113 	   105 	 0.16309 	 0.09414 	 m..s
  108 	   106 	 0.13744 	 0.09686 	 m..s
  100 	   107 	 0.07616 	 0.10389 	 ~...
  105 	   108 	 0.12350 	 0.12472 	 ~...
  117 	   109 	 0.24283 	 0.13118 	 MISS
  106 	   110 	 0.12602 	 0.14022 	 ~...
  107 	   111 	 0.13720 	 0.16480 	 ~...
  114 	   112 	 0.18303 	 0.16706 	 ~...
  109 	   113 	 0.14961 	 0.17068 	 ~...
  110 	   114 	 0.15118 	 0.18061 	 ~...
  112 	   115 	 0.15586 	 0.21589 	 m..s
  111 	   116 	 0.15169 	 0.21646 	 m..s
  116 	   117 	 0.23931 	 0.21998 	 ~...
  118 	   118 	 0.25082 	 0.22656 	 ~...
  120 	   119 	 0.33388 	 0.32174 	 ~...
  119 	   120 	 0.27205 	 0.35511 	 m..s
==========================================
r_mrr = 0.8709293603897095
r2_mrr = 0.5596927404403687
spearmanr_mrr@5 = 0.9139145612716675
spearmanr_mrr@10 = 0.9341247081756592
spearmanr_mrr@50 = 0.9444891810417175
spearmanr_mrr@100 = 0.9455246329307556
spearmanr_mrr@All = 0.9488579034805298
==========================================
test time: 0.45
Done Testing dataset OpenEA
total time taken: 251.91663074493408
training time taken: 225.63714742660522
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8709)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5597)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9139)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9341)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9445)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9455)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9489)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.9076116882408769}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 1019067770769621
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [160, 350, 301, 1082, 833, 1075, 773, 1163, 679, 71, 1099, 1001, 886, 355, 934, 827, 478, 457, 85, 980, 405, 1199, 1045, 1016, 745, 343, 634, 183, 1151, 241, 490, 139, 432, 881, 1116, 628, 492, 427, 800, 593, 1019, 522, 671, 565, 33, 320, 844, 1043, 166, 832, 345, 871, 186, 597, 6, 37, 15, 931, 283, 31, 36, 1214, 809, 613, 209, 333, 863, 5, 1169, 92, 596, 911, 625, 292, 686, 505, 1106, 1192, 1112, 471, 440, 504, 996, 841, 976, 280, 539, 586, 563, 247, 542, 375, 133, 1000, 1046, 2, 421, 117, 402, 211, 867, 100, 38, 795, 746, 188, 822, 319, 772, 232, 922, 304, 1061, 1160, 954, 511, 792, 296, 313, 973, 972]
valid_ids (0): []
train_ids (1094): [153, 129, 668, 59, 837, 239, 875, 155, 1170, 1053, 1107, 210, 857, 681, 299, 459, 644, 159, 851, 770, 606, 664, 226, 620, 967, 474, 62, 847, 1135, 873, 790, 1113, 1104, 363, 461, 879, 534, 760, 1042, 912, 13, 852, 122, 936, 1130, 932, 588, 1052, 380, 271, 739, 47, 791, 257, 683, 592, 1209, 14, 164, 723, 710, 57, 1057, 450, 484, 455, 660, 674, 755, 263, 937, 463, 659, 214, 16, 479, 887, 506, 696, 639, 849, 1054, 466, 584, 317, 332, 680, 602, 717, 741, 112, 882, 147, 344, 958, 477, 916, 158, 802, 530, 1109, 60, 284, 44, 884, 915, 751, 142, 486, 1050, 90, 1136, 354, 180, 148, 818, 276, 720, 1080, 1012, 558, 295, 688, 1133, 538, 338, 961, 356, 322, 665, 369, 1028, 771, 902, 1097, 305, 1068, 785, 1171, 1069, 248, 1131, 149, 892, 234, 677, 1184, 870, 562, 581, 782, 693, 26, 673, 655, 497, 1188, 987, 56, 279, 262, 874, 437, 22, 105, 803, 494, 716, 124, 174, 964, 499, 1147, 616, 136, 1100, 401, 1206, 204, 704, 945, 327, 220, 1128, 788, 985, 1180, 362, 1140, 315, 1166, 860, 167, 889, 191, 430, 896, 443, 955, 1093, 982, 981, 288, 780, 104, 1005, 168, 289, 811, 373, 1181, 152, 1033, 1048, 151, 53, 446, 491, 109, 742, 311, 781, 1123, 1146, 144, 1179, 907, 715, 743, 68, 260, 1105, 607, 970, 971, 8, 685, 1134, 901, 465, 551, 767, 118, 424, 274, 1201, 485, 829, 618, 605, 670, 258, 520, 1111, 583, 73, 146, 27, 225, 456, 98, 571, 794, 173, 946, 603, 267, 859, 687, 1035, 483, 331, 748, 691, 163, 959, 406, 30, 4, 357, 1013, 556, 784, 652, 589, 502, 787, 302, 177, 552, 1009, 1145, 645, 595, 546, 582, 796, 1132, 1120, 400, 736, 370, 196, 1154, 66, 776, 10, 452, 950, 1143, 49, 389, 858, 72, 481, 222, 1090, 540, 713, 1190, 1194, 268, 730, 75, 281, 391, 1176, 866, 1153, 992, 666, 434, 507, 270, 734, 995, 1167, 470, 761, 451, 162, 84, 642, 557, 87, 115, 65, 678, 205, 238, 880, 106, 726, 850, 498, 1186, 952, 1138, 891, 960, 190, 839, 918, 744, 690, 469, 138, 573, 116, 1081, 903, 19, 610, 909, 899, 58, 994, 7, 1011, 309, 777, 633, 468, 990, 1165, 462, 291, 1007, 612, 853, 207, 926, 622, 64, 108, 855, 448, 1018, 399, 1193, 414, 699, 436, 169, 667, 12, 1095, 1115, 500, 754, 388, 1110, 826, 41, 366, 88, 1213, 614, 553, 1020, 272, 949, 1041, 1197, 778, 1114, 298, 18, 365, 1049, 817, 352, 273, 890, 398, 371, 286, 29, 544, 1051, 250, 442, 569, 783, 404, 55, 384, 185, 988, 905, 348, 176, 962, 1155, 445, 1141, 351, 646, 547, 480, 413, 856, 753, 578, 335, 213, 626, 577, 925, 1148, 545, 339, 1059, 719, 711, 775, 113, 476, 1039, 227, 923, 762, 42, 135, 733, 1210, 1078, 805, 103, 181, 361, 554, 750, 658, 527, 1094, 82, 983, 623, 801, 300, 906, 812, 130, 526, 1108, 132, 641, 816, 842, 813, 1067, 287, 587, 728, 245, 640, 221, 194, 254, 897, 187, 1092, 215, 23, 835, 819, 525, 383, 1070, 1152, 150, 1064, 297, 314, 757, 920, 650, 458, 1119, 991, 797, 409, 269, 1162, 143, 316, 473, 1044, 516, 449, 460, 727, 1198, 255, 426, 156, 1077, 1062, 501, 865, 694, 868, 1, 428, 224, 714, 1177, 233, 904, 1004, 1126, 611, 524, 1172, 1030, 422, 823, 814, 97, 307, 1122, 845, 475, 360, 382, 464, 948, 1200, 165, 24, 749, 1182, 81, 561, 28, 854, 637, 528, 1003, 374, 769, 917, 549, 120, 1014, 878, 1204, 940, 698, 1002, 89, 649, 944, 86, 705, 953, 718, 764, 885, 123, 285, 708, 838, 95, 206, 1010, 512, 564, 1183, 377, 341, 80, 77, 386, 729, 393, 1084, 864, 1058, 536, 121, 198, 576, 199, 256, 1158, 1079, 417, 924, 806, 358, 786, 643, 579, 251, 326, 966, 703, 34, 560, 1060, 862, 707, 676, 69, 1159, 941, 763, 631, 1164, 487, 555, 178, 692, 193, 259, 913, 653, 189, 1187, 325, 243, 514, 48, 1023, 395, 342, 131, 101, 372, 725, 532, 495, 240, 977, 482, 519, 630, 935, 947, 700, 444, 963, 489, 110, 282, 175, 454, 94, 965, 294, 624, 1125, 535, 1137, 140, 550, 201, 598, 548, 779, 1096, 735, 789, 933, 1073, 1161, 1025, 46, 410, 793, 594, 1017, 721, 376, 340, 1142, 392, 747, 1174, 337, 419, 346, 919, 689, 647, 617, 651, 52, 575, 831, 1040, 321, 78, 230, 848, 537, 1006, 208, 1091, 111, 888, 367, 893, 407, 1211, 252, 910, 657, 425, 394, 894, 431, 91, 408, 615, 218, 1196, 834, 799, 323, 161, 43, 1089, 1034, 810, 11, 423, 1212, 228, 1118, 975, 35, 695, 861, 998, 574, 197, 1029, 1008, 1031, 590, 654, 277, 1063, 508, 334, 938, 202, 1207, 264, 1027, 70, 956, 608, 128, 1083, 808, 599, 137, 1117, 979, 876, 621, 3, 828, 1121, 521, 722, 93, 1032, 518, 39, 927, 541, 349, 566, 182, 1074, 840, 877, 737, 821, 636, 672, 1098, 438, 732, 435, 830, 968, 1127, 1157, 114, 1102, 1072, 766, 872, 107, 51, 1066, 330, 1024, 984, 195, 266, 324, 364, 9, 824, 869, 1071, 702, 381, 74, 219, 125, 669, 1056, 416, 242, 154, 1205, 756, 290, 663, 411, 591, 278, 758, 986, 768, 1055, 900, 433, 1202, 914, 820, 568, 804, 40, 930, 418, 1185, 368, 303, 533, 701, 509, 712, 898, 632, 157, 635, 928, 403, 1021, 1087, 619, 825, 385, 223, 629, 738, 570, 1036, 503, 308, 1065, 774, 353, 724, 1175, 310, 908, 706, 1037, 1088, 61, 172, 96, 1173, 974, 329, 453, 604, 192, 1101, 1208, 212, 648, 387, 656, 265, 1149, 543, 531, 993, 969, 63, 429, 336, 883, 249, 510, 179, 347, 1085, 127, 559, 1015, 1178, 517, 929, 759, 21, 32, 1195, 99, 25, 378, 217, 921, 79, 76, 203, 390, 50, 1129, 493, 661, 798, 45, 415, 1124, 740, 638, 957, 1026, 601, 752, 1189, 978, 1139, 1076, 229, 496, 807, 293, 585, 1144, 0, 126, 609, 765, 200, 600, 328, 441, 627, 662, 379, 1047, 216, 306, 261, 1203, 709, 989, 467, 846, 119, 244, 1086, 231, 529, 682, 939, 253, 237, 236, 312, 1168, 397, 141, 447, 580, 235, 675, 951, 1191, 246, 697, 943, 942, 184, 572, 83, 170, 420, 997, 134, 1156, 815, 513, 567, 515, 67, 275, 684, 1022, 20, 102, 396, 54, 412, 836, 1150, 359, 171, 999, 472, 318, 1038, 731, 17, 843, 439, 488, 895, 523, 1103, 145]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6496665747151938
the save name prefix for this run is:  chkpt-ID_6496665747151938_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 664
rank avg (pred): 0.412 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002978998

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 696
rank avg (pred): 0.510 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001127355

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.485 +- 0.007
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001112012

Epoch over!
epoch time: 14.917

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 779
rank avg (pred): 0.521 +- 0.067
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001120992

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 110
rank avg (pred): 0.519 +- 0.250
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 2.19807e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 930
rank avg (pred): 0.536 +- 0.266
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.07216e-05

Epoch over!
epoch time: 14.897

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 997
rank avg (pred): 0.360 +- 0.349
mrr vals (pred, true): 0.000, 0.139
batch losses (mrrl, rdl): 0.0, 3.53813e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1137
rank avg (pred): 0.325 +- 0.337
mrr vals (pred, true): 0.000, 0.143
batch losses (mrrl, rdl): 0.0, 1.27457e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 685
rank avg (pred): 0.519 +- 0.272
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.18097e-05

Epoch over!
epoch time: 14.854

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 721
rank avg (pred): 0.529 +- 0.277
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 8.5063e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 728
rank avg (pred): 0.543 +- 0.279
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.01914e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 627
rank avg (pred): 0.529 +- 0.280
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.0376e-06

Epoch over!
epoch time: 14.889

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1097
rank avg (pred): 0.536 +- 0.281
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 1.38645e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 262
rank avg (pred): 0.173 +- 0.313
mrr vals (pred, true): 0.002, 0.287
batch losses (mrrl, rdl): 0.0, 8.33908e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 726
rank avg (pred): 0.535 +- 0.289
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.2509e-06

Epoch over!
epoch time: 14.829

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1207
rank avg (pred): 0.522 +- 0.282
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0248381719, 6.6752e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 567
rank avg (pred): 0.512 +- 0.272
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.1174e-06, 3.8311e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 667
rank avg (pred): 0.501 +- 0.315
mrr vals (pred, true): 0.034, 0.001
batch losses (mrrl, rdl): 0.0025757989, 1.29496e-05

Epoch over!
epoch time: 15.033

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 442
rank avg (pred): 0.634 +- 0.275
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0005548173, 0.0003434455

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 526
rank avg (pred): 0.450 +- 0.328
mrr vals (pred, true): 0.063, 0.079
batch losses (mrrl, rdl): 0.0017021503, 9.31832e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 846
rank avg (pred): 0.456 +- 0.299
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 4.68805e-05, 8.51512e-05

Epoch over!
epoch time: 15.014

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 469
rank avg (pred): 0.582 +- 0.318
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0007351037, 7.12859e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1010
rank avg (pred): 0.437 +- 0.313
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005627801, 0.0001165758

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 265
rank avg (pred): 0.180 +- 0.366
mrr vals (pred, true): 0.256, 0.285
batch losses (mrrl, rdl): 0.0086252745, 0.0001569979

Epoch over!
epoch time: 15.01

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 884
rank avg (pred): 0.467 +- 0.306
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.13466e-05, 7.4556e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 282
rank avg (pred): 0.484 +- 0.316
mrr vals (pred, true): 0.055, 0.077
batch losses (mrrl, rdl): 0.0002770612, 0.0002045875

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1181
rank avg (pred): 0.515 +- 0.321
mrr vals (pred, true): 0.056, 0.002
batch losses (mrrl, rdl): 0.000368211, 6.4428e-06

Epoch over!
epoch time: 15.075

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1036
rank avg (pred): 0.483 +- 0.356
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.005309728, 4.61626e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 502
rank avg (pred): 0.289 +- 0.410
mrr vals (pred, true): 0.093, 0.181
batch losses (mrrl, rdl): 0.0770198107, 8.34408e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 167
rank avg (pred): 0.414 +- 0.336
mrr vals (pred, true): 0.066, 0.001
batch losses (mrrl, rdl): 0.0024062702, 0.0002319368

Epoch over!
epoch time: 15.115

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 212
rank avg (pred): 0.489 +- 0.312
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006591002, 2.6124e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 716
rank avg (pred): 0.543 +- 0.321
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 7.36e-08, 2.58495e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.487 +- 0.314
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 7.59699e-05, 2.28353e-05

Epoch over!
epoch time: 15.119

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 938
rank avg (pred): 0.513 +- 0.320
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.98707e-05, 1.61236e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 154
rank avg (pred): 0.473 +- 0.311
mrr vals (pred, true): 0.046, 0.002
batch losses (mrrl, rdl): 0.0001994739, 5.17396e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 382
rank avg (pred): 0.514 +- 0.317
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002419894, 7.6706e-06

Epoch over!
epoch time: 15.13

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1153
rank avg (pred): 0.472 +- 0.389
mrr vals (pred, true): 0.061, 0.079
batch losses (mrrl, rdl): 0.0012618502, 0.0001892892

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 109
rank avg (pred): 0.516 +- 0.315
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001353285, 7.381e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 975
rank avg (pred): 0.319 +- 0.418
mrr vals (pred, true): 0.101, 0.133
batch losses (mrrl, rdl): 0.0099565666, 9.25134e-05

Epoch over!
epoch time: 15.12

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 125
rank avg (pred): 0.507 +- 0.313
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001442662, 1.02028e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 839
rank avg (pred): 0.467 +- 0.293
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.90806e-05, 6.10198e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 715
rank avg (pred): 0.514 +- 0.315
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 5.4507e-06, 9.1327e-06

Epoch over!
epoch time: 15.149

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1133
rank avg (pred): 0.519 +- 0.311
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001137334, 6.5106e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 919
rank avg (pred): 0.521 +- 0.315
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003279218, 9.5323e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 604
rank avg (pred): 0.499 +- 0.309
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001654028, 2.0578e-05

Epoch over!
epoch time: 15.119

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.467 +- 0.300
mrr vals (pred, true): 0.046, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.04563 	 0.00043 	 m..s
   28 	     1 	 0.05030 	 0.00048 	 m..s
   69 	     2 	 0.05347 	 0.00049 	 m..s
   19 	     3 	 0.04927 	 0.00049 	 m..s
   18 	     4 	 0.04921 	 0.00050 	 m..s
   50 	     5 	 0.05210 	 0.00050 	 m..s
   15 	     6 	 0.04761 	 0.00050 	 m..s
   76 	     7 	 0.05456 	 0.00050 	 m..s
   49 	     8 	 0.05200 	 0.00050 	 m..s
    1 	     9 	 0.04236 	 0.00051 	 m..s
   31 	    10 	 0.05068 	 0.00051 	 m..s
   36 	    11 	 0.05137 	 0.00052 	 m..s
   44 	    12 	 0.05192 	 0.00052 	 m..s
   34 	    13 	 0.05134 	 0.00053 	 m..s
   64 	    14 	 0.05307 	 0.00053 	 m..s
   92 	    15 	 0.06082 	 0.00053 	 m..s
   83 	    16 	 0.05537 	 0.00054 	 m..s
   24 	    17 	 0.04992 	 0.00055 	 m..s
   12 	    18 	 0.04687 	 0.00055 	 m..s
   27 	    19 	 0.05018 	 0.00055 	 m..s
    0 	    20 	 0.04176 	 0.00056 	 m..s
   23 	    21 	 0.04965 	 0.00060 	 m..s
   10 	    22 	 0.04618 	 0.00060 	 m..s
    5 	    23 	 0.04503 	 0.00061 	 m..s
   25 	    24 	 0.04995 	 0.00063 	 m..s
   72 	    25 	 0.05391 	 0.00063 	 m..s
   60 	    26 	 0.05291 	 0.00064 	 m..s
   70 	    27 	 0.05370 	 0.00064 	 m..s
   93 	    28 	 0.06088 	 0.00064 	 m..s
    6 	    29 	 0.04543 	 0.00065 	 m..s
   90 	    30 	 0.05891 	 0.00066 	 m..s
   91 	    31 	 0.05951 	 0.00068 	 m..s
   22 	    32 	 0.04965 	 0.00069 	 m..s
   78 	    33 	 0.05477 	 0.00072 	 m..s
   16 	    34 	 0.04762 	 0.00073 	 m..s
   42 	    35 	 0.05177 	 0.00075 	 m..s
   26 	    36 	 0.05002 	 0.00078 	 m..s
   37 	    37 	 0.05142 	 0.00078 	 m..s
   30 	    38 	 0.05050 	 0.00081 	 m..s
   17 	    39 	 0.04896 	 0.00081 	 m..s
   61 	    40 	 0.05304 	 0.00084 	 m..s
   79 	    41 	 0.05498 	 0.00085 	 m..s
   66 	    42 	 0.05318 	 0.00089 	 m..s
   77 	    43 	 0.05457 	 0.00090 	 m..s
   45 	    44 	 0.05195 	 0.00090 	 m..s
   11 	    45 	 0.04618 	 0.00093 	 m..s
   41 	    46 	 0.05173 	 0.00095 	 m..s
    2 	    47 	 0.04459 	 0.00098 	 m..s
   13 	    48 	 0.04706 	 0.00099 	 m..s
   74 	    49 	 0.05421 	 0.00104 	 m..s
   94 	    50 	 0.06150 	 0.00112 	 m..s
   56 	    51 	 0.05255 	 0.00113 	 m..s
   71 	    52 	 0.05377 	 0.00114 	 m..s
    4 	    53 	 0.04474 	 0.00115 	 m..s
   65 	    54 	 0.05312 	 0.00121 	 m..s
   80 	    55 	 0.05508 	 0.00126 	 m..s
   59 	    56 	 0.05287 	 0.00126 	 m..s
   63 	    57 	 0.05305 	 0.00130 	 m..s
   40 	    58 	 0.05172 	 0.00136 	 m..s
    9 	    59 	 0.04588 	 0.00136 	 m..s
   53 	    60 	 0.05240 	 0.00139 	 m..s
   67 	    61 	 0.05323 	 0.00139 	 m..s
    3 	    62 	 0.04470 	 0.00139 	 m..s
   20 	    63 	 0.04945 	 0.00142 	 m..s
   29 	    64 	 0.05038 	 0.00148 	 m..s
   75 	    65 	 0.05456 	 0.00148 	 m..s
   14 	    66 	 0.04752 	 0.00151 	 m..s
   43 	    67 	 0.05189 	 0.00155 	 m..s
   46 	    68 	 0.05197 	 0.00178 	 m..s
   81 	    69 	 0.05518 	 0.00184 	 m..s
    8 	    70 	 0.04577 	 0.00188 	 m..s
   73 	    71 	 0.05411 	 0.00188 	 m..s
   39 	    72 	 0.05169 	 0.00210 	 m..s
   68 	    73 	 0.05324 	 0.00234 	 m..s
   52 	    74 	 0.05238 	 0.00364 	 m..s
   95 	    75 	 0.06625 	 0.00637 	 m..s
   97 	    76 	 0.08237 	 0.04963 	 m..s
   84 	    77 	 0.05545 	 0.05524 	 ~...
   82 	    78 	 0.05532 	 0.05886 	 ~...
   85 	    79 	 0.05560 	 0.06018 	 ~...
   51 	    80 	 0.05214 	 0.06445 	 ~...
   48 	    81 	 0.05200 	 0.06712 	 ~...
   35 	    82 	 0.05136 	 0.06727 	 ~...
   47 	    83 	 0.05197 	 0.07098 	 ~...
  102 	    84 	 0.14496 	 0.07225 	 m..s
   98 	    85 	 0.09075 	 0.07475 	 ~...
   89 	    86 	 0.05625 	 0.07489 	 ~...
   88 	    87 	 0.05602 	 0.07607 	 ~...
   33 	    88 	 0.05131 	 0.07609 	 ~...
   38 	    89 	 0.05151 	 0.07636 	 ~...
   62 	    90 	 0.05304 	 0.07667 	 ~...
   86 	    91 	 0.05575 	 0.07893 	 ~...
   87 	    92 	 0.05600 	 0.07894 	 ~...
   21 	    93 	 0.04954 	 0.08144 	 m..s
   58 	    94 	 0.05287 	 0.08315 	 m..s
   57 	    95 	 0.05270 	 0.08484 	 m..s
   54 	    96 	 0.05240 	 0.08673 	 m..s
   55 	    97 	 0.05246 	 0.08738 	 m..s
   32 	    98 	 0.05108 	 0.08956 	 m..s
  100 	    99 	 0.09308 	 0.09364 	 ~...
   99 	   100 	 0.09277 	 0.09562 	 ~...
   96 	   101 	 0.07016 	 0.09730 	 ~...
  106 	   102 	 0.15849 	 0.10850 	 m..s
  109 	   103 	 0.18574 	 0.12298 	 m..s
  110 	   104 	 0.18636 	 0.12358 	 m..s
  101 	   105 	 0.11478 	 0.12472 	 ~...
  104 	   106 	 0.14889 	 0.12857 	 ~...
  103 	   107 	 0.14825 	 0.13837 	 ~...
  105 	   108 	 0.15497 	 0.14022 	 ~...
  116 	   109 	 0.19490 	 0.16228 	 m..s
  111 	   110 	 0.19042 	 0.16711 	 ~...
  112 	   111 	 0.19063 	 0.17210 	 ~...
  117 	   112 	 0.19661 	 0.17323 	 ~...
  113 	   113 	 0.19217 	 0.17976 	 ~...
  118 	   114 	 0.20343 	 0.20306 	 ~...
  114 	   115 	 0.19396 	 0.21233 	 ~...
  107 	   116 	 0.18009 	 0.21589 	 m..s
  115 	   117 	 0.19468 	 0.21845 	 ~...
  119 	   118 	 0.21100 	 0.21998 	 ~...
  120 	   119 	 0.21280 	 0.22307 	 ~...
  108 	   120 	 0.18097 	 0.24200 	 m..s
==========================================
r_mrr = 0.8844630122184753
r2_mrr = 0.5477654933929443
spearmanr_mrr@5 = 0.7911776304244995
spearmanr_mrr@10 = 0.8215705752372742
spearmanr_mrr@50 = 0.9202867150306702
spearmanr_mrr@100 = 0.9172337055206299
spearmanr_mrr@All = 0.9229835271835327
==========================================
test time: 0.448
Done Testing dataset OpenEA
total time taken: 252.82944440841675
training time taken: 225.7269835472107
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8845)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5478)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.7912)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.8216)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9203)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9172)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9230)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.3881088457046644}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 9756890460457694
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [118, 727, 639, 169, 259, 247, 650, 657, 72, 562, 1195, 527, 606, 755, 310, 897, 277, 773, 518, 55, 1174, 204, 389, 203, 322, 824, 1160, 156, 136, 278, 315, 430, 820, 1101, 514, 29, 1097, 90, 930, 1142, 705, 818, 48, 1041, 423, 669, 565, 242, 616, 808, 1007, 126, 1130, 387, 1086, 580, 291, 954, 386, 519, 442, 356, 956, 28, 747, 999, 678, 624, 468, 813, 1020, 655, 859, 778, 597, 1052, 757, 328, 195, 1107, 849, 871, 450, 838, 276, 429, 879, 321, 659, 708, 338, 487, 101, 1154, 147, 1067, 103, 544, 1050, 654, 378, 970, 721, 109, 236, 789, 68, 913, 979, 684, 553, 438, 69, 269, 548, 782, 666, 963, 500, 695, 807]
valid_ids (0): []
train_ids (1094): [1005, 941, 350, 369, 191, 225, 449, 841, 905, 903, 302, 100, 508, 1186, 167, 318, 1116, 288, 306, 298, 874, 792, 981, 368, 688, 626, 816, 482, 420, 216, 185, 31, 556, 969, 1188, 454, 448, 1037, 32, 73, 345, 647, 393, 637, 696, 98, 102, 407, 633, 81, 57, 947, 1162, 1057, 1197, 590, 681, 592, 750, 854, 480, 946, 17, 887, 717, 1091, 806, 1151, 991, 1025, 989, 348, 215, 467, 1208, 643, 890, 484, 940, 220, 533, 283, 1093, 382, 823, 319, 282, 1148, 111, 46, 35, 343, 326, 543, 926, 844, 300, 992, 962, 598, 998, 155, 119, 117, 235, 1083, 65, 993, 1158, 501, 836, 271, 437, 1213, 43, 1143, 1015, 296, 359, 309, 144, 62, 402, 3, 50, 812, 1136, 1017, 1121, 1126, 329, 741, 233, 686, 744, 183, 713, 948, 754, 724, 6, 244, 826, 769, 796, 740, 625, 748, 88, 324, 596, 292, 984, 738, 405, 392, 1056, 953, 172, 1040, 404, 106, 1094, 358, 868, 59, 486, 371, 312, 390, 40, 465, 495, 1103, 1210, 492, 896, 1157, 672, 1112, 629, 509, 21, 950, 157, 847, 139, 559, 478, 133, 327, 16, 75, 619, 209, 1042, 765, 253, 563, 1194, 339, 1178, 80, 477, 1019, 113, 837, 1198, 1135, 1193, 523, 84, 1085, 251, 410, 772, 479, 1184, 538, 1129, 289, 604, 572, 770, 603, 673, 682, 435, 884, 461, 421, 140, 973, 964, 829, 581, 781, 898, 928, 266, 1055, 174, 894, 614, 1137, 560, 463, 313, 93, 357, 452, 267, 293, 944, 9, 680, 228, 1082, 71, 424, 376, 489, 561, 189, 797, 192, 1024, 907, 960, 336, 609, 1045, 631, 380, 122, 4, 190, 995, 394, 317, 337, 888, 99, 63, 349, 95, 408, 436, 971, 586, 308, 763, 985, 911, 728, 1051, 1014, 952, 1204, 273, 86, 11, 187, 1199, 447, 720, 471, 163, 1000, 123, 112, 51, 297, 904, 397, 749, 360, 77, 497, 431, 74, 845, 257, 652, 311, 1190, 703, 1095, 158, 146, 1008, 882, 900, 1175, 131, 1073, 23, 8, 166, 937, 746, 188, 777, 355, 370, 272, 587, 700, 1089, 367, 1203, 822, 779, 648, 843, 965, 211, 199, 1072, 10, 531, 712, 908, 400, 613, 1100, 164, 493, 466, 212, 511, 1119, 197, 615, 1038, 210, 444, 857, 1039, 1013, 1044, 383, 858, 706, 364, 224, 873, 1070, 529, 285, 915, 1187, 105, 475, 301, 130, 249, 114, 958, 446, 260, 656, 184, 331, 153, 7, 230, 891, 1110, 583, 351, 665, 1043, 34, 1201, 256, 229, 840, 726, 830, 760, 469, 1063, 756, 481, 1167, 545, 664, 1062, 795, 284, 280, 1205, 558, 588, 935, 886, 906, 551, 675, 499, 638, 827, 1122, 921, 1109, 316, 472, 1068, 555, 617, 132, 361, 374, 120, 799, 968, 129, 207, 1035, 365, 237, 1115, 142, 1152, 577, 1023, 776, 243, 264, 60, 549, 507, 175, 222, 918, 414, 1108, 1202, 354, 662, 938, 1032, 608, 434, 165, 645, 115, 290, 82, 951, 697, 372, 1081, 398, 742, 1077, 121, 42, 453, 221, 505, 22, 379, 766, 49, 1114, 725, 340, 774, 138, 671, 182, 1117, 1146, 1034, 108, 70, 893, 30, 988, 1002, 94, 522, 238, 160, 731, 737, 1047, 1102, 694, 37, 729, 263, 936, 618, 649, 710, 1159, 245, 885, 286, 530, 455, 517, 539, 801, 1026, 409, 1132, 929, 1182, 1149, 205, 912, 1124, 149, 683, 957, 512, 584, 254, 252, 804, 1133, 783, 150, 1211, 794, 470, 574, 793, 265, 537, 262, 942, 1145, 335, 832, 601, 241, 743, 788, 460, 1001, 219, 1036, 540, 1214, 864, 417, 589, 1131, 176, 767, 931, 735, 1076, 498, 64, 388, 761, 107, 1079, 186, 1031, 1206, 1169, 734, 303, 18, 1156, 506, 855, 934, 1171, 395, 1078, 320, 524, 1030, 628, 270, 861, 208, 787, 711, 1049, 1054, 1098, 52, 526, 701, 181, 1058, 687, 814, 67, 860, 217, 171, 240, 1092, 866, 294, 780, 223, 892, 239, 983, 699, 899, 798, 862, 47, 927, 564, 817, 585, 1029, 571, 620, 853, 702, 715, 917, 723, 594, 552, 1165, 125, 104, 803, 1183, 883, 753, 570, 491, 299, 218, 159, 152, 232, 976, 1087, 784, 600, 825, 58, 810, 819, 575, 314, 1180, 137, 110, 206, 363, 611, 1113, 45, 373, 719, 987, 674, 535, 762, 325, 36, 426, 1141, 975, 154, 226, 850, 704, 972, 1191, 670, 933, 602, 464, 96, 15, 143, 261, 366, 248, 54, 751, 881, 768, 775, 441, 341, 925, 1176, 916, 168, 640, 676, 352, 20, 1128, 307, 634, 97, 406, 610, 1099, 815, 432, 986, 800, 66, 595, 736, 396, 231, 644, 1147, 457, 811, 1144, 733, 630, 196, 661, 542, 1064, 250, 573, 990, 1118, 722, 173, 679, 1209, 433, 677, 790, 623, 213, 1090, 605, 914, 274, 385, 764, 867, 334, 1016, 834, 692, 901, 578, 831, 287, 716, 473, 1066, 939, 1074, 848, 25, 412, 591, 1125, 377, 1006, 690, 547, 346, 179, 966, 56, 1033, 342, 89, 872, 1046, 919, 332, 835, 520, 621, 1106, 967, 2, 490, 323, 974, 949, 961, 170, 116, 923, 1170, 1140, 91, 279, 1120, 162, 642, 128, 151, 1164, 1163, 305, 627, 791, 576, 24, 200, 27, 875, 691, 1069, 83, 636, 730, 1060, 641, 33, 646, 1153, 909, 1123, 503, 502, 268, 347, 483, 660, 439, 413, 698, 87, 458, 13, 76, 714, 488, 0, 344, 1096, 994, 504, 536, 234, 1155, 1177, 61, 943, 1179, 550, 1111, 476, 1075, 515, 579, 635, 295, 977, 541, 569, 910, 902, 759, 1012, 689, 945, 14, 607, 663, 425, 304, 1071, 281, 428, 440, 582, 1018, 38, 1021, 932, 255, 534, 1134, 653, 1084, 178, 1212, 375, 1189, 865, 399, 718, 496, 1053, 214, 878, 978, 474, 668, 148, 1, 246, 846, 401, 821, 456, 920, 193, 79, 828, 1161, 513, 445, 135, 557, 180, 833, 889, 877, 593, 459, 384, 1196, 528, 997, 124, 1059, 612, 959, 1028, 809, 568, 19, 41, 732, 924, 554, 275, 658, 693, 1105, 516, 1088, 599, 707, 1027, 418, 53, 1150, 1065, 1010, 198, 12, 771, 44, 416, 39, 922, 805, 996, 419, 739, 863, 566, 876, 1207, 1181, 1004, 852, 895, 856, 1104, 1185, 1080, 745, 870, 1011, 1172, 709, 1022, 415, 26, 1173, 1003, 202, 78, 510, 1048, 485, 141, 521, 494, 786, 955, 1127, 443, 632, 422, 451, 1192, 785, 145, 651, 177, 411, 227, 353, 758, 258, 980, 330, 1168, 1139, 391, 667, 880, 685, 567, 194, 546, 92, 532, 381, 851, 1061, 842, 85, 427, 1166, 127, 403, 1009, 5, 161, 982, 462, 752, 333, 839, 802, 1200, 525, 622, 869, 134, 362, 201, 1138]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1728454002594199
the save name prefix for this run is:  chkpt-ID_1728454002594199_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 394
rank avg (pred): 0.467 +- 0.001
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001218371

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1168
rank avg (pred): 0.501 +- 0.313
mrr vals (pred, true): 0.003, 0.001
batch losses (mrrl, rdl): 0.0, 2.66661e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 595
rank avg (pred): 0.469 +- 0.307
mrr vals (pred, true): 0.003, 0.001
batch losses (mrrl, rdl): 0.0, 6.4743e-06

Epoch over!
epoch time: 14.893

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 411
rank avg (pred): 0.463 +- 0.294
mrr vals (pred, true): 0.005, 0.001
batch losses (mrrl, rdl): 0.0, 5.653e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 510
rank avg (pred): 0.228 +- 0.209
mrr vals (pred, true): 0.024, 0.208
batch losses (mrrl, rdl): 0.0, 5.55309e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 933
rank avg (pred): 0.455 +- 0.294
mrr vals (pred, true): 0.005, 0.001
batch losses (mrrl, rdl): 0.0, 3.06239e-05

Epoch over!
epoch time: 14.893

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 317
rank avg (pred): 0.300 +- 0.279
mrr vals (pred, true): 0.013, 0.072
batch losses (mrrl, rdl): 0.0, 1.66098e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 295
rank avg (pred): 0.313 +- 0.309
mrr vals (pred, true): 0.006, 0.077
batch losses (mrrl, rdl): 0.0, 7.4272e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 89
rank avg (pred): 0.486 +- 0.307
mrr vals (pred, true): 0.006, 0.001
batch losses (mrrl, rdl): 0.0, 7.4511e-06

Epoch over!
epoch time: 14.914

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 513
rank avg (pred): 0.376 +- 0.315
mrr vals (pred, true): 0.012, 0.078
batch losses (mrrl, rdl): 0.0, 1.052e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1019
rank avg (pred): 0.494 +- 0.297
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 7.232e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1084
rank avg (pred): 0.491 +- 0.304
mrr vals (pred, true): 0.001, 0.002
batch losses (mrrl, rdl): 0.0, 6.706e-07

Epoch over!
epoch time: 14.93

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 373
rank avg (pred): 0.473 +- 0.317
mrr vals (pred, true): 0.173, 0.002
batch losses (mrrl, rdl): 0.0, 3.8018e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 731
rank avg (pred): 0.164 +- 0.261
mrr vals (pred, true): 0.191, 0.318
batch losses (mrrl, rdl): 0.0, 1.53733e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 846
rank avg (pred): 0.502 +- 0.302
mrr vals (pred, true): 0.003, 0.001
batch losses (mrrl, rdl): 0.0, 1.2507e-06

Epoch over!
epoch time: 14.905

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 529
rank avg (pred): 0.386 +- 0.312
mrr vals (pred, true): 0.018, 0.072
batch losses (mrrl, rdl): 0.0103400275, 1.66876e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 475
rank avg (pred): 0.493 +- 0.284
mrr vals (pred, true): 0.028, 0.001
batch losses (mrrl, rdl): 0.0050319131, 3.1114e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 993
rank avg (pred): 0.377 +- 0.302
mrr vals (pred, true): 0.091, 0.104
batch losses (mrrl, rdl): 0.0015732499, 8.21702e-05

Epoch over!
epoch time: 15.074

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 51
rank avg (pred): 0.370 +- 0.301
mrr vals (pred, true): 0.094, 0.081
batch losses (mrrl, rdl): 0.0195939075, 3.13491e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 686
rank avg (pred): 0.434 +- 0.259
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0009081026, 6.36406e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 134
rank avg (pred): 0.503 +- 0.268
mrr vals (pred, true): 0.048, 0.002
batch losses (mrrl, rdl): 3.07214e-05, 6.5908e-06

Epoch over!
epoch time: 15.054

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.504 +- 0.266
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002033473, 7.4113e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 113
rank avg (pred): 0.446 +- 0.262
mrr vals (pred, true): 0.058, 0.002
batch losses (mrrl, rdl): 0.0006598596, 4.82353e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1109
rank avg (pred): 0.523 +- 0.250
mrr vals (pred, true): 0.034, 0.001
batch losses (mrrl, rdl): 0.0025426687, 1.52359e-05

Epoch over!
epoch time: 15.065

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 908
rank avg (pred): 0.778 +- 0.301
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.0014032847, 0.0001875008

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 302
rank avg (pred): 0.400 +- 0.246
mrr vals (pred, true): 0.057, 0.066
batch losses (mrrl, rdl): 0.0005055045, 4.70261e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 629
rank avg (pred): 0.474 +- 0.261
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0004070111, 1.68718e-05

Epoch over!
epoch time: 15.056

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 770
rank avg (pred): 0.492 +- 0.311
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0013013271, 5.8696e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 441
rank avg (pred): 0.533 +- 0.297
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003538881, 3.5872e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 350
rank avg (pred): 0.515 +- 0.297
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001046964, 3.2698e-06

Epoch over!
epoch time: 15.053

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 921
rank avg (pred): 0.507 +- 0.271
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0013935217, 3.7933e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 230
rank avg (pred): 0.497 +- 0.296
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.49433e-05, 1.8592e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 738
rank avg (pred): 0.318 +- 0.296
mrr vals (pred, true): 0.127, 0.139
batch losses (mrrl, rdl): 0.0012530643, 2.83097e-05

Epoch over!
epoch time: 15.065

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 685
rank avg (pred): 0.502 +- 0.309
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 6.35411e-05, 5.5922e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 130
rank avg (pred): 0.480 +- 0.274
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 7.79639e-05, 1.45535e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 873
rank avg (pred): 0.501 +- 0.283
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.97119e-05, 8.163e-07

Epoch over!
epoch time: 15.044

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 875
rank avg (pred): 0.483 +- 0.277
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4e-10, 1.57687e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 100
rank avg (pred): 0.516 +- 0.285
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006444012, 6.1527e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 506
rank avg (pred): 0.180 +- 0.256
mrr vals (pred, true): 0.265, 0.227
batch losses (mrrl, rdl): 0.0146905202, 4.06514e-05

Epoch over!
epoch time: 15.073

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 18
rank avg (pred): 0.221 +- 0.307
mrr vals (pred, true): 0.230, 0.254
batch losses (mrrl, rdl): 0.0060128276, 0.0003830716

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 108
rank avg (pred): 0.508 +- 0.287
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 5.3679e-06, 8.094e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 208
rank avg (pred): 0.519 +- 0.308
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 7.5638e-06, 3.9451e-06

Epoch over!
epoch time: 15.325

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 603
rank avg (pred): 0.504 +- 0.299
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.36231e-05, 3.1151e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.428 +- 0.245
mrr vals (pred, true): 0.055, 0.071
batch losses (mrrl, rdl): 0.0002976404, 0.0001229001

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1069
rank avg (pred): 0.293 +- 0.266
mrr vals (pred, true): 0.135, 0.127
batch losses (mrrl, rdl): 0.0007359721, 1.20235e-05

Epoch over!
epoch time: 15.121

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.516 +- 0.303
mrr vals (pred, true): 0.050, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.04767 	 0.00013 	 m..s
   88 	     1 	 0.05453 	 0.00045 	 m..s
   40 	     2 	 0.04978 	 0.00049 	 m..s
   21 	     3 	 0.04779 	 0.00050 	 m..s
   87 	     4 	 0.05426 	 0.00050 	 m..s
   94 	     5 	 0.06155 	 0.00051 	 m..s
   71 	     6 	 0.05141 	 0.00051 	 m..s
   67 	     7 	 0.05105 	 0.00053 	 m..s
   81 	     8 	 0.05317 	 0.00055 	 m..s
    0 	     9 	 0.03613 	 0.00055 	 m..s
   56 	    10 	 0.05045 	 0.00056 	 m..s
   85 	    11 	 0.05373 	 0.00056 	 m..s
   60 	    12 	 0.05062 	 0.00058 	 m..s
    5 	    13 	 0.04378 	 0.00059 	 m..s
   84 	    14 	 0.05344 	 0.00060 	 m..s
   39 	    15 	 0.04973 	 0.00060 	 m..s
   47 	    16 	 0.05008 	 0.00060 	 m..s
   33 	    17 	 0.04949 	 0.00060 	 m..s
   69 	    18 	 0.05117 	 0.00061 	 m..s
    3 	    19 	 0.04370 	 0.00061 	 m..s
   49 	    20 	 0.05017 	 0.00062 	 m..s
   51 	    21 	 0.05022 	 0.00063 	 m..s
   65 	    22 	 0.05097 	 0.00063 	 m..s
   25 	    23 	 0.04838 	 0.00063 	 m..s
   15 	    24 	 0.04715 	 0.00065 	 m..s
   55 	    25 	 0.05042 	 0.00066 	 m..s
   59 	    26 	 0.05057 	 0.00067 	 m..s
   64 	    27 	 0.05088 	 0.00067 	 m..s
   77 	    28 	 0.05208 	 0.00067 	 m..s
   22 	    29 	 0.04782 	 0.00068 	 m..s
   18 	    30 	 0.04755 	 0.00068 	 m..s
    1 	    31 	 0.03616 	 0.00071 	 m..s
    9 	    32 	 0.04508 	 0.00071 	 m..s
    7 	    33 	 0.04456 	 0.00072 	 m..s
   68 	    34 	 0.05114 	 0.00073 	 m..s
   30 	    35 	 0.04929 	 0.00073 	 m..s
   66 	    36 	 0.05098 	 0.00074 	 m..s
   61 	    37 	 0.05072 	 0.00074 	 m..s
   93 	    38 	 0.05720 	 0.00075 	 m..s
   78 	    39 	 0.05252 	 0.00075 	 m..s
   83 	    40 	 0.05337 	 0.00076 	 m..s
   29 	    41 	 0.04924 	 0.00077 	 m..s
   91 	    42 	 0.05600 	 0.00078 	 m..s
   45 	    43 	 0.05000 	 0.00081 	 m..s
   46 	    44 	 0.05002 	 0.00081 	 m..s
   52 	    45 	 0.05022 	 0.00081 	 m..s
   36 	    46 	 0.04957 	 0.00083 	 m..s
   86 	    47 	 0.05389 	 0.00086 	 m..s
   38 	    48 	 0.04970 	 0.00087 	 m..s
   79 	    49 	 0.05260 	 0.00088 	 m..s
   73 	    50 	 0.05155 	 0.00088 	 m..s
   96 	    51 	 0.06248 	 0.00089 	 m..s
   41 	    52 	 0.04989 	 0.00090 	 m..s
   89 	    53 	 0.05458 	 0.00093 	 m..s
   34 	    54 	 0.04956 	 0.00095 	 m..s
   95 	    55 	 0.06221 	 0.00095 	 m..s
   26 	    56 	 0.04904 	 0.00097 	 m..s
   32 	    57 	 0.04947 	 0.00099 	 m..s
   57 	    58 	 0.05051 	 0.00106 	 m..s
   90 	    59 	 0.05597 	 0.00107 	 m..s
   74 	    60 	 0.05171 	 0.00107 	 m..s
   63 	    61 	 0.05086 	 0.00119 	 m..s
   54 	    62 	 0.05041 	 0.00120 	 m..s
   42 	    63 	 0.04990 	 0.00133 	 m..s
   80 	    64 	 0.05293 	 0.00135 	 m..s
   48 	    65 	 0.05008 	 0.00136 	 m..s
   44 	    66 	 0.04998 	 0.00138 	 m..s
   50 	    67 	 0.05018 	 0.00141 	 m..s
   72 	    68 	 0.05148 	 0.00142 	 m..s
   58 	    69 	 0.05051 	 0.00142 	 m..s
   92 	    70 	 0.05645 	 0.00144 	 m..s
   82 	    71 	 0.05320 	 0.00146 	 m..s
    8 	    72 	 0.04493 	 0.00148 	 m..s
   13 	    73 	 0.04645 	 0.00154 	 m..s
   27 	    74 	 0.04912 	 0.00160 	 m..s
   75 	    75 	 0.05184 	 0.00165 	 m..s
   70 	    76 	 0.05131 	 0.00181 	 m..s
    4 	    77 	 0.04378 	 0.00200 	 m..s
   37 	    78 	 0.04970 	 0.00221 	 m..s
   62 	    79 	 0.05079 	 0.00228 	 m..s
    2 	    80 	 0.04238 	 0.00267 	 m..s
  107 	    81 	 0.11424 	 0.00581 	 MISS
   99 	    82 	 0.06356 	 0.05495 	 ~...
   98 	    83 	 0.06300 	 0.05524 	 ~...
   43 	    84 	 0.04993 	 0.06384 	 ~...
   16 	    85 	 0.04735 	 0.06450 	 ~...
   35 	    86 	 0.04957 	 0.06666 	 ~...
   17 	    87 	 0.04738 	 0.06676 	 ~...
   24 	    88 	 0.04819 	 0.06684 	 ~...
   19 	    89 	 0.04759 	 0.06998 	 ~...
  106 	    90 	 0.07275 	 0.07104 	 ~...
   28 	    91 	 0.04922 	 0.07131 	 ~...
  102 	    92 	 0.06924 	 0.07360 	 ~...
  105 	    93 	 0.07015 	 0.07421 	 ~...
   97 	    94 	 0.06286 	 0.07469 	 ~...
   12 	    95 	 0.04638 	 0.07621 	 ~...
    6 	    96 	 0.04432 	 0.07633 	 m..s
  108 	    97 	 0.11912 	 0.07674 	 m..s
   10 	    98 	 0.04511 	 0.07732 	 m..s
   11 	    99 	 0.04539 	 0.07802 	 m..s
  104 	   100 	 0.06983 	 0.07869 	 ~...
   23 	   101 	 0.04784 	 0.07888 	 m..s
  103 	   102 	 0.06961 	 0.07948 	 ~...
   14 	   103 	 0.04684 	 0.08032 	 m..s
   31 	   104 	 0.04938 	 0.08043 	 m..s
  101 	   105 	 0.06875 	 0.08076 	 ~...
   53 	   106 	 0.05032 	 0.08094 	 m..s
  100 	   107 	 0.06789 	 0.08607 	 ~...
   76 	   108 	 0.05201 	 0.08873 	 m..s
  115 	   109 	 0.18907 	 0.10850 	 m..s
  110 	   110 	 0.14273 	 0.13377 	 ~...
  111 	   111 	 0.14356 	 0.13818 	 ~...
  109 	   112 	 0.14079 	 0.13837 	 ~...
  112 	   113 	 0.15814 	 0.17032 	 ~...
  116 	   114 	 0.19857 	 0.18250 	 ~...
  113 	   115 	 0.16884 	 0.18384 	 ~...
  114 	   116 	 0.17474 	 0.18508 	 ~...
  117 	   117 	 0.22887 	 0.19048 	 m..s
  118 	   118 	 0.24106 	 0.27955 	 m..s
  120 	   119 	 0.34628 	 0.29555 	 m..s
  119 	   120 	 0.31732 	 0.35511 	 m..s
==========================================
r_mrr = 0.87764972448349
r2_mrr = 0.5304350852966309
spearmanr_mrr@5 = 0.9137200117111206
spearmanr_mrr@10 = 0.9574902653694153
spearmanr_mrr@50 = 0.9380799531936646
spearmanr_mrr@100 = 0.9263201951980591
spearmanr_mrr@All = 0.9289346933364868
==========================================
test time: 0.464
Done Testing dataset OpenEA
total time taken: 253.34943985939026
training time taken: 225.9378423690796
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8776)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5304)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9137)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9575)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9381)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9263)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9289)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.28373788241765396}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 9518718177570708
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1210, 194, 641, 313, 70, 619, 870, 399, 114, 747, 814, 1074, 299, 990, 408, 533, 1184, 788, 652, 445, 510, 298, 134, 936, 761, 373, 631, 950, 242, 757, 552, 559, 806, 215, 1140, 630, 364, 1066, 851, 113, 361, 317, 336, 1123, 664, 58, 1051, 265, 1141, 961, 784, 211, 468, 830, 549, 876, 824, 369, 296, 970, 1002, 1046, 725, 706, 1087, 946, 78, 654, 987, 891, 1107, 774, 1015, 1007, 1177, 727, 1095, 1164, 429, 161, 1014, 1139, 338, 66, 43, 907, 1100, 940, 135, 496, 268, 1120, 30, 1134, 126, 288, 769, 127, 287, 882, 512, 417, 65, 1006, 63, 332, 1076, 1093, 246, 48, 266, 231, 170, 99, 1191, 286, 820, 310, 1031, 1034, 1200]
valid_ids (0): []
train_ids (1094): [1001, 1106, 795, 182, 39, 580, 267, 89, 1017, 1154, 230, 574, 251, 188, 274, 312, 1128, 34, 219, 729, 1010, 668, 270, 646, 359, 992, 185, 1181, 318, 923, 410, 781, 1113, 307, 837, 577, 61, 1117, 557, 1108, 543, 50, 1194, 181, 759, 1047, 472, 1144, 400, 488, 110, 446, 335, 18, 1075, 667, 958, 213, 731, 603, 611, 62, 910, 174, 467, 195, 901, 1187, 59, 570, 939, 1195, 469, 1138, 191, 197, 10, 671, 464, 379, 662, 168, 573, 1092, 300, 1028, 849, 615, 499, 120, 698, 8, 1061, 3, 539, 985, 815, 407, 843, 693, 133, 236, 736, 276, 495, 812, 819, 550, 1077, 282, 956, 362, 913, 81, 854, 690, 492, 1085, 721, 579, 884, 278, 422, 542, 835, 793, 609, 535, 404, 79, 1190, 460, 1212, 218, 13, 315, 331, 732, 1068, 778, 1207, 953, 431, 800, 1173, 229, 622, 897, 389, 890, 345, 685, 618, 886, 975, 1202, 889, 105, 162, 214, 606, 75, 728, 344, 1097, 386, 121, 1158, 909, 302, 694, 660, 643, 128, 46, 520, 1201, 171, 762, 1122, 115, 439, 1180, 1142, 1011, 714, 771, 97, 275, 165, 665, 93, 466, 692, 928, 764, 945, 462, 629, 143, 677, 1118, 49, 358, 443, 932, 56, 963, 55, 334, 986, 6, 144, 568, 395, 484, 816, 476, 929, 661, 22, 280, 83, 21, 106, 9, 799, 968, 776, 486, 900, 1040, 962, 597, 919, 1130, 482, 333, 937, 471, 453, 31, 921, 637, 387, 420, 506, 1088, 766, 823, 1125, 645, 147, 254, 786, 263, 966, 595, 647, 767, 636, 983, 521, 454, 621, 628, 750, 765, 922, 982, 365, 1090, 138, 186, 1161, 610, 723, 357, 1114, 192, 250, 223, 354, 1168, 887, 73, 12, 497, 247, 942, 297, 1151, 598, 734, 749, 881, 872, 118, 426, 1099, 697, 423, 1105, 927, 259, 941, 902, 1170, 591, 57, 976, 457, 1179, 136, 911, 51, 1155, 674, 680, 841, 123, 850, 411, 1132, 663, 713, 374, 614, 451, 415, 959, 479, 166, 157, 979, 391, 80, 1145, 478, 208, 522, 366, 739, 632, 256, 253, 1044, 1196, 1033, 756, 742, 707, 514, 828, 33, 689, 281, 380, 515, 717, 633, 206, 883, 316, 183, 225, 69, 679, 1124, 895, 430, 1185, 1056, 403, 791, 150, 805, 1193, 1069, 783, 1104, 1023, 967, 1082, 624, 915, 808, 592, 593, 237, 401, 527, 832, 1127, 817, 903, 447, 224, 355, 437, 813, 561, 234, 878, 260, 108, 743, 390, 501, 1050, 669, 130, 103, 481, 563, 1183, 483, 1166, 311, 1018, 1121, 695, 172, 980, 1027, 1041, 1131, 602, 160, 279, 772, 938, 860, 227, 1133, 862, 523, 1004, 1080, 673, 72, 5, 914, 682, 261, 1143, 565, 394, 1174, 1029, 1149, 998, 32, 715, 988, 341, 537, 617, 807, 353, 232, 947, 1214, 779, 811, 865, 129, 705, 294, 1081, 290, 413, 888, 683, 16, 834, 656, 1101, 1111, 777, 1037, 612, 973, 175, 432, 474, 754, 578, 948, 567, 90, 271, 119, 1103, 733, 735, 691, 45, 60, 301, 789, 1160, 531, 240, 124, 1156, 53, 37, 24, 1186, 583, 326, 760, 676, 726, 327, 498, 875, 880, 504, 566, 584, 613, 763, 562, 560, 320, 758, 517, 485, 972, 587, 965, 1175, 26, 92, 822, 588, 351, 17, 861, 519, 226, 551, 319, 507, 600, 122, 1162, 155, 221, 569, 1009, 125, 590, 217, 235, 809, 651, 427, 1137, 249, 38, 370, 530, 285, 1119, 601, 203, 222, 383, 393, 245, 684, 330, 686, 867, 864, 424, 272, 1035, 797, 1102, 23, 1091, 594, 933, 871, 94, 85, 193, 154, 1203, 1163, 1135, 382, 201, 951, 513, 82, 198, 398, 428, 1208, 949, 825, 720, 199, 712, 1063, 1059, 737, 36, 169, 918, 196, 845, 414, 996, 894, 1152, 833, 352, 14, 586, 792, 220, 164, 112, 141, 1204, 156, 371, 752, 1182, 1003, 555, 456, 931, 19, 1096, 1112, 339, 649, 392, 323, 88, 27, 96, 1021, 1072, 991, 255, 152, 347, 869, 76, 1053, 91, 704, 406, 678, 836, 703, 233, 289, 178, 892, 377, 29, 435, 925, 745, 978, 239, 605, 11, 238, 116, 384, 623, 20, 1205, 490, 1169, 785, 216, 548, 879, 204, 1054, 190, 984, 441, 367, 269, 346, 145, 421, 450, 1079, 151, 572, 974, 644, 1153, 844, 205, 1062, 350, 360, 173, 412, 1115, 780, 1084, 1005, 314, 1116, 396, 989, 409, 896, 545, 1070, 740, 688, 639, 1165, 455, 264, 840, 1055, 252, 1110, 701, 1094, 452, 526, 885, 305, 955, 957, 340, 810, 184, 708, 971, 718, 343, 648, 1126, 262, 804, 1012, 102, 1020, 787, 1043, 342, 167, 1052, 589, 1206, 1086, 473, 475, 981, 101, 363, 505, 1032, 434, 782, 935, 855, 283, 368, 842, 658, 487, 348, 977, 829, 375, 388, 177, 874, 244, 709, 738, 1048, 711, 596, 744, 207, 827, 857, 626, 25, 681, 440, 444, 848, 803, 906, 416, 107, 2, 321, 1065, 328, 202, 877, 98, 200, 753, 372, 163, 148, 1057, 449, 821, 719, 67, 995, 1049, 1022, 746, 655, 529, 180, 659, 1030, 153, 916, 137, 558, 1199, 634, 322, 494, 770, 638, 1167, 42, 1171, 571, 385, 607, 491, 775, 710, 650, 1147, 1197, 68, 863, 378, 818, 502, 1038, 748, 158, 964, 1064, 768, 773, 436, 1036, 500, 64, 179, 926, 7, 858, 1178, 1071, 459, 576, 994, 856, 74, 554, 672, 303, 801, 15, 1078, 142, 4, 556, 640, 581, 904, 405, 349, 553, 920, 325, 866, 899, 912, 1060, 189, 509, 470, 461, 1039, 309, 924, 293, 159, 826, 397, 541, 52, 442, 1209, 503, 304, 243, 859, 716, 722, 846, 620, 1129, 402, 508, 585, 796, 873, 839, 176, 139, 997, 1188, 852, 1098, 356, 893, 131, 1024, 564, 699, 657, 209, 696, 730, 687, 306, 532, 241, 1042, 627, 146, 1016, 575, 117, 798, 625, 675, 1013, 480, 376, 969, 1189, 547, 1159, 1008, 905, 40, 257, 954, 111, 538, 943, 751, 1148, 794, 582, 228, 329, 425, 518, 86, 544, 790, 666, 1089, 831, 516, 292, 635, 1211, 277, 493, 642, 1058, 433, 1213, 44, 741, 132, 960, 1150, 700, 511, 84, 1073, 1198, 477, 87, 868, 525, 210, 616, 1067, 534, 1083, 109, 0, 755, 381, 419, 291, 1192, 934, 930, 465, 448, 847, 54, 999, 1025, 1176, 528, 524, 28, 438, 35, 77, 100, 308, 284, 149, 838, 944, 724, 546, 853, 47, 212, 608, 489, 908, 1045, 258, 295, 248, 1146, 95, 1, 898, 599, 1157, 917, 273, 1136, 187, 540, 463, 702, 1000, 604, 140, 993, 653, 324, 458, 952, 418, 104, 71, 1109, 1026, 1172, 802, 670, 536, 337, 1019, 41]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7897406682794256
the save name prefix for this run is:  chkpt-ID_7897406682794256_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 153
rank avg (pred): 0.482 +- 0.007
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001158608

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 224
rank avg (pred): 0.518 +- 0.321
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0, 3.86568e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 802
rank avg (pred): 0.477 +- 0.305
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0, 6.5287e-06

Epoch over!
epoch time: 14.894

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1009
rank avg (pred): 0.483 +- 0.290
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 0.0, 2.5645e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1132
rank avg (pred): 0.496 +- 0.316
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0, 2.6805e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 739
rank avg (pred): 0.264 +- 0.309
mrr vals (pred, true): 0.087, 0.144
batch losses (mrrl, rdl): 0.0, 1.42222e-05

Epoch over!
epoch time: 14.839

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 102
rank avg (pred): 0.488 +- 0.298
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0, 4.895e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 598
rank avg (pred): 0.504 +- 0.309
mrr vals (pred, true): 0.072, 0.001
batch losses (mrrl, rdl): 0.0, 2.1992e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 628
rank avg (pred): 0.502 +- 0.305
mrr vals (pred, true): 0.074, 0.001
batch losses (mrrl, rdl): 0.0, 2.2359e-06

Epoch over!
epoch time: 14.853

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 626
rank avg (pred): 0.500 +- 0.317
mrr vals (pred, true): 0.083, 0.002
batch losses (mrrl, rdl): 0.0, 1.8187e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 792
rank avg (pred): 0.503 +- 0.320
mrr vals (pred, true): 0.093, 0.001
batch losses (mrrl, rdl): 0.0, 1.919e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1055
rank avg (pred): 0.359 +- 0.331
mrr vals (pred, true): 0.103, 0.089
batch losses (mrrl, rdl): 0.0, 9.2988e-06

Epoch over!
epoch time: 14.85

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 920
rank avg (pred): 0.509 +- 0.305
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0, 1.3883e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 46
rank avg (pred): 0.335 +- 0.314
mrr vals (pred, true): 0.101, 0.084
batch losses (mrrl, rdl): 0.0, 1.2256e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 229
rank avg (pred): 0.505 +- 0.309
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0, 9.244e-07

Epoch over!
epoch time: 14.913

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 986
rank avg (pred): 0.323 +- 0.302
mrr vals (pred, true): 0.107, 0.106
batch losses (mrrl, rdl): 4.9884e-06, 9.5847e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1111
rank avg (pred): 0.554 +- 0.276
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 2.23972e-05, 2.89689e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 776
rank avg (pred): 0.589 +- 0.266
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.22403e-05, 8.94919e-05

Epoch over!
epoch time: 15.291

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 511
rank avg (pred): 0.320 +- 0.442
mrr vals (pred, true): 0.154, 0.223
batch losses (mrrl, rdl): 0.0481168479, 0.0001999902

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.492 +- 0.248
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 7.0092e-06, 2.15287e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 901
rank avg (pred): 0.525 +- 0.344
mrr vals (pred, true): 0.066, 0.003
batch losses (mrrl, rdl): 0.0024171718, 0.0001272785

Epoch over!
epoch time: 15.277

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 702
rank avg (pred): 0.482 +- 0.270
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.74601e-05, 3.1251e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.519 +- 0.283
mrr vals (pred, true): 0.057, 0.002
batch losses (mrrl, rdl): 0.0005435412, 7.4339e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 567
rank avg (pred): 0.520 +- 0.268
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001334708, 8.4157e-06

Epoch over!
epoch time: 15.255

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 339
rank avg (pred): 0.533 +- 0.279
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.000736818, 7.2435e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 651
rank avg (pred): 0.525 +- 0.259
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003712732, 9.7779e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 948
rank avg (pred): 0.554 +- 0.257
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.13209e-05, 4.50052e-05

Epoch over!
epoch time: 15.253

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 818
rank avg (pred): 0.210 +- 0.359
mrr vals (pred, true): 0.249, 0.006
batch losses (mrrl, rdl): 0.3978182077, 0.0002221123

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 720
rank avg (pred): 0.499 +- 0.270
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.15764e-05, 1.24755e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 224
rank avg (pred): 0.445 +- 0.265
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.001846221, 9.80655e-05

Epoch over!
epoch time: 15.147

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 539
rank avg (pred): 0.389 +- 0.291
mrr vals (pred, true): 0.066, 0.059
batch losses (mrrl, rdl): 0.0026700092, 8.5245e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 571
rank avg (pred): 0.530 +- 0.259
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 6.02662e-05, 1.54873e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 867
rank avg (pred): 0.520 +- 0.241
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001079679, 1.83607e-05

Epoch over!
epoch time: 15.148

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 572
rank avg (pred): 0.505 +- 0.234
mrr vals (pred, true): 0.054, 0.002
batch losses (mrrl, rdl): 0.0001611178, 1.90638e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 279
rank avg (pred): 0.388 +- 0.263
mrr vals (pred, true): 0.056, 0.074
batch losses (mrrl, rdl): 0.0003131651, 3.14571e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 355
rank avg (pred): 0.498 +- 0.258
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.000296802, 1.2858e-05

Epoch over!
epoch time: 15.056

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 670
rank avg (pred): 0.505 +- 0.275
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.000376355, 5.2455e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 285
rank avg (pred): 0.409 +- 0.319
mrr vals (pred, true): 0.050, 0.079
batch losses (mrrl, rdl): 1.8195e-06, 2.3522e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 562
rank avg (pred): 0.409 +- 0.321
mrr vals (pred, true): 0.065, 0.055
batch losses (mrrl, rdl): 0.0023901286, 1.55871e-05

Epoch over!
epoch time: 15.049

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 205
rank avg (pred): 0.526 +- 0.253
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0003002982, 1.23131e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 166
rank avg (pred): 0.515 +- 0.271
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.17101e-05, 5.9766e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 277
rank avg (pred): 0.348 +- 0.270
mrr vals (pred, true): 0.050, 0.067
batch losses (mrrl, rdl): 3.418e-07, 2.78811e-05

Epoch over!
epoch time: 15.062

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1204
rank avg (pred): 0.521 +- 0.262
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 9.617e-07, 9.3718e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 125
rank avg (pred): 0.517 +- 0.254
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.80774e-05, 1.29787e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1106
rank avg (pred): 0.502 +- 0.286
mrr vals (pred, true): 0.060, 0.004
batch losses (mrrl, rdl): 0.0010881289, 3.3612e-06

Epoch over!
epoch time: 15.053

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.529 +- 0.276
mrr vals (pred, true): 0.053, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   84 	     0 	 0.05486 	 0.00043 	 m..s
   52 	     1 	 0.05253 	 0.00047 	 m..s
   63 	     2 	 0.05300 	 0.00052 	 m..s
   59 	     3 	 0.05283 	 0.00053 	 m..s
   50 	     4 	 0.05253 	 0.00055 	 m..s
   23 	     5 	 0.05156 	 0.00056 	 m..s
   83 	     6 	 0.05441 	 0.00057 	 m..s
   85 	     7 	 0.05488 	 0.00057 	 m..s
  102 	     8 	 0.07857 	 0.00058 	 m..s
   88 	     9 	 0.05499 	 0.00058 	 m..s
    8 	    10 	 0.04688 	 0.00058 	 m..s
   66 	    11 	 0.05317 	 0.00058 	 m..s
   65 	    12 	 0.05311 	 0.00059 	 m..s
   44 	    13 	 0.05226 	 0.00059 	 m..s
   19 	    14 	 0.05137 	 0.00060 	 m..s
   22 	    15 	 0.05146 	 0.00060 	 m..s
   14 	    16 	 0.05112 	 0.00060 	 m..s
   69 	    17 	 0.05360 	 0.00060 	 m..s
   17 	    18 	 0.05120 	 0.00062 	 m..s
   76 	    19 	 0.05403 	 0.00062 	 m..s
    2 	    20 	 0.04562 	 0.00063 	 m..s
   61 	    21 	 0.05291 	 0.00064 	 m..s
   77 	    22 	 0.05405 	 0.00064 	 m..s
   16 	    23 	 0.05120 	 0.00065 	 m..s
   80 	    24 	 0.05424 	 0.00068 	 m..s
   45 	    25 	 0.05232 	 0.00068 	 m..s
   13 	    26 	 0.05066 	 0.00070 	 m..s
   15 	    27 	 0.05118 	 0.00073 	 m..s
   67 	    28 	 0.05327 	 0.00074 	 m..s
   70 	    29 	 0.05360 	 0.00074 	 m..s
   62 	    30 	 0.05299 	 0.00077 	 m..s
   71 	    31 	 0.05382 	 0.00080 	 m..s
   74 	    32 	 0.05394 	 0.00080 	 m..s
    9 	    33 	 0.04698 	 0.00081 	 m..s
   78 	    34 	 0.05411 	 0.00081 	 m..s
   90 	    35 	 0.05555 	 0.00081 	 m..s
   56 	    36 	 0.05275 	 0.00082 	 m..s
   10 	    37 	 0.04703 	 0.00084 	 m..s
   26 	    38 	 0.05162 	 0.00085 	 m..s
   72 	    39 	 0.05385 	 0.00086 	 m..s
   12 	    40 	 0.04998 	 0.00087 	 m..s
   55 	    41 	 0.05275 	 0.00088 	 m..s
    0 	    42 	 0.04478 	 0.00088 	 m..s
   82 	    43 	 0.05439 	 0.00089 	 m..s
    4 	    44 	 0.04588 	 0.00094 	 m..s
   47 	    45 	 0.05238 	 0.00094 	 m..s
   89 	    46 	 0.05500 	 0.00095 	 m..s
   86 	    47 	 0.05489 	 0.00095 	 m..s
   18 	    48 	 0.05132 	 0.00097 	 m..s
   20 	    49 	 0.05139 	 0.00098 	 m..s
   87 	    50 	 0.05498 	 0.00100 	 m..s
   73 	    51 	 0.05389 	 0.00110 	 m..s
   79 	    52 	 0.05419 	 0.00111 	 m..s
   29 	    53 	 0.05165 	 0.00121 	 m..s
   75 	    54 	 0.05398 	 0.00123 	 m..s
   28 	    55 	 0.05164 	 0.00125 	 m..s
    5 	    56 	 0.04604 	 0.00132 	 m..s
   81 	    57 	 0.05427 	 0.00148 	 m..s
    7 	    58 	 0.04618 	 0.00153 	 m..s
   11 	    59 	 0.04718 	 0.00153 	 m..s
   64 	    60 	 0.05304 	 0.00155 	 m..s
   68 	    61 	 0.05333 	 0.00156 	 m..s
   31 	    62 	 0.05168 	 0.00159 	 m..s
    1 	    63 	 0.04553 	 0.00168 	 m..s
    3 	    64 	 0.04579 	 0.00171 	 m..s
    6 	    65 	 0.04608 	 0.00172 	 m..s
   21 	    66 	 0.05145 	 0.00176 	 m..s
   49 	    67 	 0.05253 	 0.00190 	 m..s
   36 	    68 	 0.05181 	 0.00194 	 m..s
   60 	    69 	 0.05291 	 0.00195 	 m..s
   27 	    70 	 0.05164 	 0.00200 	 m..s
   32 	    71 	 0.05177 	 0.00200 	 m..s
   35 	    72 	 0.05180 	 0.00231 	 m..s
   53 	    73 	 0.05261 	 0.00231 	 m..s
   25 	    74 	 0.05158 	 0.00233 	 m..s
   37 	    75 	 0.05185 	 0.00273 	 m..s
   38 	    76 	 0.05193 	 0.00350 	 m..s
  103 	    77 	 0.08416 	 0.02580 	 m..s
   96 	    78 	 0.06450 	 0.05308 	 ~...
   99 	    79 	 0.06737 	 0.06283 	 ~...
   34 	    80 	 0.05180 	 0.06496 	 ~...
   39 	    81 	 0.05201 	 0.06533 	 ~...
   33 	    82 	 0.05179 	 0.06882 	 ~...
   24 	    83 	 0.05157 	 0.07194 	 ~...
   95 	    84 	 0.06110 	 0.07225 	 ~...
   92 	    85 	 0.05918 	 0.07240 	 ~...
   91 	    86 	 0.05878 	 0.07493 	 ~...
   30 	    87 	 0.05168 	 0.07595 	 ~...
   41 	    88 	 0.05212 	 0.07636 	 ~...
   42 	    89 	 0.05222 	 0.07688 	 ~...
   54 	    90 	 0.05268 	 0.07719 	 ~...
   94 	    91 	 0.06018 	 0.07893 	 ~...
   40 	    92 	 0.05211 	 0.07943 	 ~...
   93 	    93 	 0.05978 	 0.07948 	 ~...
   97 	    94 	 0.06578 	 0.08027 	 ~...
  110 	    95 	 0.12367 	 0.08036 	 m..s
   46 	    96 	 0.05238 	 0.08094 	 ~...
   58 	    97 	 0.05278 	 0.08451 	 m..s
   51 	    98 	 0.05253 	 0.08500 	 m..s
   57 	    99 	 0.05277 	 0.08521 	 m..s
   43 	   100 	 0.05223 	 0.08811 	 m..s
   48 	   101 	 0.05242 	 0.09031 	 m..s
   98 	   102 	 0.06616 	 0.09449 	 ~...
  105 	   103 	 0.10387 	 0.10118 	 ~...
  100 	   104 	 0.06794 	 0.10268 	 m..s
  101 	   105 	 0.06797 	 0.10416 	 m..s
  107 	   106 	 0.11182 	 0.12940 	 ~...
  104 	   107 	 0.10380 	 0.13040 	 ~...
  108 	   108 	 0.11507 	 0.13377 	 ~...
  106 	   109 	 0.11037 	 0.13758 	 ~...
  111 	   110 	 0.12569 	 0.13818 	 ~...
  109 	   111 	 0.11635 	 0.17901 	 m..s
  113 	   112 	 0.16492 	 0.17937 	 ~...
  112 	   113 	 0.13824 	 0.18250 	 m..s
  114 	   114 	 0.16552 	 0.18272 	 ~...
  115 	   115 	 0.19329 	 0.20773 	 ~...
  119 	   116 	 0.27359 	 0.22233 	 m..s
  117 	   117 	 0.26288 	 0.28260 	 ~...
  118 	   118 	 0.26352 	 0.28534 	 ~...
  120 	   119 	 0.30095 	 0.29805 	 ~...
  116 	   120 	 0.25579 	 0.34917 	 m..s
==========================================
r_mrr = 0.8849636316299438
r2_mrr = 0.6112507581710815
spearmanr_mrr@5 = 0.910797119140625
spearmanr_mrr@10 = 0.9266769886016846
spearmanr_mrr@50 = 0.9473671317100525
spearmanr_mrr@100 = 0.9158649444580078
spearmanr_mrr@All = 0.9180000424385071
==========================================
test time: 0.452
Done Testing dataset OpenEA
total time taken: 253.0137598514557
training time taken: 226.40261888504028
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8850)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.6113)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9108)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9267)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9474)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9159)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9180)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.33940035032901505}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 3462737707435580
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [348, 587, 690, 422, 1087, 1113, 1102, 1070, 718, 1007, 818, 283, 543, 788, 354, 466, 450, 974, 11, 581, 1058, 1153, 1176, 731, 429, 1027, 252, 617, 1049, 10, 656, 2, 1096, 414, 1040, 472, 349, 203, 573, 574, 179, 223, 1045, 195, 343, 126, 138, 971, 1152, 534, 924, 779, 630, 28, 449, 793, 680, 186, 318, 471, 438, 668, 1095, 1012, 127, 250, 1154, 567, 726, 739, 143, 684, 1061, 1100, 536, 397, 334, 856, 1130, 29, 813, 180, 377, 838, 1026, 965, 358, 26, 106, 91, 57, 1125, 744, 707, 613, 82, 745, 5, 524, 647, 237, 828, 798, 385, 817, 1201, 485, 141, 895, 125, 1075, 976, 132, 1134, 792, 981, 352, 593, 622, 37, 1167]
valid_ids (0): []
train_ids (1094): [662, 113, 72, 735, 599, 746, 420, 41, 124, 1029, 807, 13, 987, 251, 521, 430, 275, 749, 269, 474, 929, 810, 761, 315, 796, 845, 277, 1000, 140, 952, 879, 620, 665, 253, 435, 486, 356, 776, 1107, 46, 948, 339, 116, 1207, 270, 1002, 852, 464, 220, 691, 1074, 1023, 1052, 395, 708, 706, 728, 689, 338, 766, 1137, 679, 403, 234, 335, 468, 479, 58, 565, 1047, 850, 1011, 607, 591, 263, 712, 325, 985, 546, 932, 207, 511, 115, 1043, 871, 644, 1164, 145, 173, 1031, 612, 866, 1132, 463, 922, 1206, 271, 60, 304, 507, 713, 477, 226, 980, 1156, 1051, 864, 205, 1163, 715, 440, 942, 289, 1073, 246, 475, 88, 762, 357, 15, 646, 111, 104, 437, 164, 258, 421, 905, 369, 160, 146, 936, 148, 944, 73, 518, 215, 1035, 632, 1092, 447, 50, 875, 200, 383, 688, 957, 753, 1099, 98, 1112, 213, 280, 1090, 506, 272, 137, 884, 1006, 488, 1148, 1146, 703, 777, 19, 681, 1055, 337, 306, 1143, 156, 915, 109, 541, 1003, 917, 392, 1105, 405, 309, 25, 659, 448, 526, 557, 729, 963, 227, 1205, 151, 281, 1120, 727, 451, 56, 1180, 881, 692, 638, 578, 1089, 470, 925, 624, 804, 1067, 797, 682, 87, 457, 547, 501, 598, 854, 674, 1122, 769, 240, 569, 803, 938, 1, 121, 428, 224, 1159, 673, 1157, 190, 800, 108, 1078, 34, 1028, 621, 552, 120, 157, 697, 278, 669, 637, 577, 633, 1128, 542, 161, 778, 1084, 1165, 836, 3, 874, 594, 103, 433, 855, 815, 738, 782, 1129, 678, 0, 896, 685, 123, 310, 1133, 68, 666, 892, 867, 409, 331, 602, 634, 1119, 719, 1021, 711, 802, 248, 1138, 4, 527, 386, 606, 1179, 1140, 554, 105, 490, 494, 610, 1004, 49, 130, 259, 355, 698, 912, 76, 496, 314, 1068, 805, 1053, 1033, 336, 24, 163, 1145, 222, 1185, 1196, 1010, 840, 27, 467, 441, 580, 84, 1189, 321, 588, 973, 517, 1014, 85, 366, 69, 919, 902, 904, 520, 1039, 273, 282, 305, 63, 891, 1093, 102, 516, 954, 562, 893, 575, 332, 460, 1079, 975, 945, 1136, 8, 242, 795, 823, 966, 481, 1178, 1135, 509, 185, 529, 316, 1103, 604, 1057, 424, 625, 843, 302, 298, 461, 772, 211, 771, 265, 899, 671, 616, 1104, 476, 780, 510, 301, 675, 1115, 709, 950, 653, 131, 523, 255, 327, 375, 1085, 900, 1169, 172, 279, 364, 216, 1204, 903, 379, 993, 655, 531, 672, 81, 1117, 576, 89, 806, 737, 1081, 1032, 841, 353, 20, 889, 225, 670, 1025, 221, 906, 754, 747, 142, 821, 152, 725, 897, 308, 695, 911, 947, 1036, 787, 834, 781, 1160, 489, 398, 401, 287, 652, 373, 1094, 916, 419, 44, 333, 544, 826, 601, 733, 360, 590, 631, 714, 851, 1155, 410, 1088, 734, 824, 515, 989, 540, 372, 293, 36, 898, 1202, 362, 848, 908, 859, 1059, 799, 537, 928, 495, 14, 59, 636, 999, 960, 1181, 230, 1060, 940, 110, 654, 721, 442, 595, 561, 1101, 784, 1038, 808, 344, 538, 35, 786, 1149, 701, 342, 951, 872, 31, 645, 52, 93, 499, 346, 558, 505, 564, 21, 70, 720, 370, 231, 23, 1212, 857, 432, 687, 865, 415, 789, 1191, 1083, 615, 64, 700, 380, 43, 175, 1192, 243, 1172, 459, 1211, 870, 1168, 994, 1147, 443, 758, 514, 849, 1210, 677, 371, 641, 862, 1080, 1194, 791, 750, 394, 74, 1126, 907, 444, 560, 427, 991, 133, 820, 704, 159, 266, 453, 436, 400, 365, 785, 241, 502, 969, 997, 291, 949, 1144, 664, 54, 100, 986, 260, 1158, 320, 307, 760, 1097, 295, 38, 229, 1187, 946, 608, 18, 303, 261, 535, 7, 53, 877, 292, 86, 964, 445, 847, 732, 1008, 118, 579, 387, 267, 559, 30, 12, 1173, 1009, 1062, 9, 40, 51, 742, 473, 519, 497, 801, 17, 770, 378, 978, 1141, 219, 883, 1013, 1124, 1108, 584, 193, 1109, 183, 1034, 1024, 551, 264, 326, 650, 553, 247, 830, 767, 995, 284, 723, 702, 484, 286, 117, 811, 33, 533, 341, 1042, 239, 45, 1213, 351, 210, 773, 941, 97, 933, 456, 218, 605, 206, 254, 194, 563, 1063, 837, 139, 136, 176, 667, 724, 1200, 550, 894, 431, 831, 888, 626, 736, 743, 972, 683, 640, 609, 1199, 1175, 775, 408, 890, 262, 794, 530, 763, 1183, 934, 513, 412, 833, 937, 465, 149, 959, 827, 571, 589, 319, 860, 1195, 618, 66, 730, 1098, 740, 382, 256, 1030, 492, 62, 885, 328, 1170, 1069, 1188, 909, 6, 212, 67, 487, 882, 78, 858, 299, 135, 174, 1182, 196, 1005, 1131, 312, 1116, 452, 710, 619, 480, 955, 918, 592, 1022, 1150, 99, 313, 979, 661, 1076, 1151, 92, 868, 233, 77, 404, 192, 1214, 235, 71, 764, 658, 829, 390, 114, 570, 413, 198, 819, 290, 388, 887, 330, 144, 417, 178, 853, 628, 55, 759, 1127, 503, 112, 783, 300, 95, 921, 170, 648, 583, 340, 80, 90, 238, 825, 705, 322, 1066, 717, 1114, 676, 1086, 1121, 627, 878, 166, 699, 285, 512, 439, 696, 426, 482, 158, 268, 914, 32, 276, 177, 167, 107, 1041, 317, 693, 182, 1190, 236, 1177, 1015, 968, 469, 1001, 943, 751, 491, 359, 1017, 1050, 1209, 1016, 367, 1174, 545, 329, 774, 150, 154, 741, 660, 1110, 1171, 752, 585, 901, 643, 931, 381, 582, 168, 835, 376, 1203, 572, 1019, 982, 839, 930, 249, 47, 165, 504, 294, 549, 923, 956, 1077, 716, 454, 809, 996, 181, 926, 399, 402, 478, 147, 539, 311, 48, 939, 844, 217, 962, 756, 548, 556, 927, 83, 406, 1071, 984, 425, 155, 209, 347, 1056, 913, 61, 184, 187, 297, 1106, 350, 389, 1048, 768, 323, 920, 863, 635, 816, 384, 345, 1193, 197, 169, 416, 1046, 748, 649, 992, 22, 876, 1082, 274, 79, 757, 411, 861, 961, 953, 983, 1020, 603, 458, 1118, 204, 597, 765, 1166, 16, 361, 967, 566, 935, 288, 1037, 418, 1111, 232, 988, 391, 122, 191, 958, 188, 880, 1184, 1091, 998, 910, 555, 722, 245, 94, 201, 39, 686, 639, 694, 600, 832, 842, 483, 508, 990, 101, 1054, 257, 651, 434, 202, 629, 886, 657, 423, 1123, 462, 873, 119, 790, 1072, 96, 244, 65, 1018, 393, 663, 128, 296, 977, 1161, 396, 596, 134, 363, 522, 525, 611, 1197, 1064, 446, 42, 208, 498, 532, 75, 846, 623, 324, 162, 214, 970, 528, 1186, 1139, 1044, 1142, 228, 129, 199, 869, 614, 755, 812, 171, 1208, 642, 493, 368, 455, 568, 500, 1065, 407, 586, 1198, 814, 189, 374, 153, 822, 1162]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7560007509420446
the save name prefix for this run is:  chkpt-ID_7560007509420446_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1118
rank avg (pred): 0.555 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.000165988

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 893
rank avg (pred): 0.376 +- 0.205
mrr vals (pred, true): 0.123, 0.011
batch losses (mrrl, rdl): 0.0, 0.0007462251

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 655
rank avg (pred): 0.443 +- 0.291
mrr vals (pred, true): 0.205, 0.001
batch losses (mrrl, rdl): 0.0, 2.23093e-05

Epoch over!
epoch time: 14.886

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1073
rank avg (pred): 0.282 +- 0.194
mrr vals (pred, true): 0.225, 0.073
batch losses (mrrl, rdl): 0.0, 0.0001351352

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 67
rank avg (pred): 0.365 +- 0.261
mrr vals (pred, true): 0.208, 0.085
batch losses (mrrl, rdl): 0.0, 8.61101e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 145
rank avg (pred): 0.467 +- 0.300
mrr vals (pred, true): 0.165, 0.002
batch losses (mrrl, rdl): 0.0, 1.14769e-05

Epoch over!
epoch time: 14.861

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 812
rank avg (pred): 0.175 +- 0.131
mrr vals (pred, true): 0.193, 0.324
batch losses (mrrl, rdl): 0.0, 9.1781e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 458
rank avg (pred): 0.471 +- 0.305
mrr vals (pred, true): 0.135, 0.001
batch losses (mrrl, rdl): 0.0, 1.14642e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 510
rank avg (pred): 0.337 +- 0.257
mrr vals (pred, true): 0.148, 0.208
batch losses (mrrl, rdl): 0.0, 0.0003985045

Epoch over!
epoch time: 14.889

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 767
rank avg (pred): 0.496 +- 0.294
mrr vals (pred, true): 0.103, 0.001
batch losses (mrrl, rdl): 0.0, 2.09898e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 823
rank avg (pred): 0.201 +- 0.182
mrr vals (pred, true): 0.172, 0.178
batch losses (mrrl, rdl): 0.0, 7.56398e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 907
rank avg (pred): 0.523 +- 0.310
mrr vals (pred, true): 0.071, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003161726

Epoch over!
epoch time: 14.904

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 60
rank avg (pred): 0.318 +- 0.281
mrr vals (pred, true): 0.137, 0.067
batch losses (mrrl, rdl): 0.0, 9.7948e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 920
rank avg (pred): 0.501 +- 0.284
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0, 7.8278e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 365
rank avg (pred): 0.508 +- 0.286
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.0, 1.09613e-05

Epoch over!
epoch time: 14.904

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1029
rank avg (pred): 0.484 +- 0.295
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002789995, 2.53e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 611
rank avg (pred): 0.582 +- 0.183
mrr vals (pred, true): 0.050, 0.002
batch losses (mrrl, rdl): 1.914e-07, 0.0002530096

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 418
rank avg (pred): 0.481 +- 0.268
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002179559, 1.18867e-05

Epoch over!
epoch time: 15.153

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 55
rank avg (pred): 0.544 +- 0.204
mrr vals (pred, true): 0.064, 0.070
batch losses (mrrl, rdl): 0.0019617514, 0.0008240435

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 21
rank avg (pred): 0.056 +- 0.113
mrr vals (pred, true): 0.275, 0.253
batch losses (mrrl, rdl): 0.004694358, 4.62522e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 961
rank avg (pred): 0.532 +- 0.186
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.82552e-05, 8.32347e-05

Epoch over!
epoch time: 15.228

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 156
rank avg (pred): 0.528 +- 0.181
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006370936, 8.23336e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 999
rank avg (pred): 0.482 +- 0.289
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0010663801, 5.1456e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 894
rank avg (pred): 0.601 +- 0.329
mrr vals (pred, true): 0.076, 0.016
batch losses (mrrl, rdl): 0.0065566236, 0.0002280558

Epoch over!
epoch time: 15.232

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 207
rank avg (pred): 0.474 +- 0.194
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4.77e-08, 4.16333e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 364
rank avg (pred): 0.475 +- 0.183
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 6.1877e-06, 5.27412e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 927
rank avg (pred): 0.504 +- 0.146
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0007372824, 6.33568e-05

Epoch over!
epoch time: 15.071

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 528
rank avg (pred): 0.476 +- 0.171
mrr vals (pred, true): 0.054, 0.077
batch losses (mrrl, rdl): 0.0001370629, 0.0003893892

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 899
rank avg (pred): 0.463 +- 0.341
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0282368027, 0.002179387

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 996
rank avg (pred): 0.346 +- 0.213
mrr vals (pred, true): 0.143, 0.140
batch losses (mrrl, rdl): 7.76385e-05, 0.0001301063

Epoch over!
epoch time: 15.098

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1111
rank avg (pred): 0.471 +- 0.266
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.000242213, 7.6873e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 84
rank avg (pred): 0.420 +- 0.226
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 0.0001068221, 0.0001151741

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1018
rank avg (pred): 0.456 +- 0.181
mrr vals (pred, true): 0.047, 0.002
batch losses (mrrl, rdl): 8.78493e-05, 5.80301e-05

Epoch over!
epoch time: 15.106

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 980
rank avg (pred): 0.601 +- 0.322
mrr vals (pred, true): 0.065, 0.173
batch losses (mrrl, rdl): 0.1173764393, 0.0020548401

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1035
rank avg (pred): 0.449 +- 0.204
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002319493, 5.13772e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 394
rank avg (pred): 0.464 +- 0.164
mrr vals (pred, true): 0.046, 0.002
batch losses (mrrl, rdl): 0.000173076, 5.90295e-05

Epoch over!
epoch time: 15.105

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 747
rank avg (pred): 0.242 +- 0.174
mrr vals (pred, true): 0.158, 0.138
batch losses (mrrl, rdl): 0.004031735, 9.90373e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 899
rank avg (pred): 0.627 +- 0.391
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.0135109434, 0.0006987948

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 420
rank avg (pred): 0.439 +- 0.212
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 4.84167e-05, 5.30692e-05

Epoch over!
epoch time: 15.088

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 152
rank avg (pred): 0.445 +- 0.180
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 3.4893e-06, 6.62697e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 217
rank avg (pred): 0.451 +- 0.189
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.50678e-05, 5.11254e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 274
rank avg (pred): 0.429 +- 0.241
mrr vals (pred, true): 0.052, 0.069
batch losses (mrrl, rdl): 2.66276e-05, 0.0001213022

Epoch over!
epoch time: 15.149

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 957
rank avg (pred): 0.486 +- 0.229
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.26521e-05, 1.95138e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1117
rank avg (pred): 0.474 +- 0.264
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002102692, 2.49455e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 939
rank avg (pred): 0.500 +- 0.253
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 2.17509e-05, 1.078e-05

Epoch over!
epoch time: 15.196

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.499 +- 0.294
mrr vals (pred, true): 0.053, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04718 	 0.00038 	 m..s
   46 	     1 	 0.04968 	 0.00047 	 m..s
   85 	     2 	 0.05383 	 0.00047 	 m..s
   27 	     3 	 0.04926 	 0.00048 	 m..s
    4 	     4 	 0.04822 	 0.00048 	 m..s
   16 	     5 	 0.04893 	 0.00049 	 m..s
   25 	     6 	 0.04918 	 0.00049 	 m..s
   35 	     7 	 0.04950 	 0.00050 	 m..s
    9 	     8 	 0.04831 	 0.00051 	 m..s
   31 	     9 	 0.04940 	 0.00052 	 m..s
   29 	    10 	 0.04928 	 0.00053 	 m..s
   22 	    11 	 0.04910 	 0.00053 	 m..s
   17 	    12 	 0.04897 	 0.00054 	 m..s
   33 	    13 	 0.04944 	 0.00055 	 m..s
   57 	    14 	 0.05018 	 0.00055 	 m..s
   39 	    15 	 0.04960 	 0.00055 	 m..s
   78 	    16 	 0.05249 	 0.00056 	 m..s
   12 	    17 	 0.04860 	 0.00058 	 m..s
   37 	    18 	 0.04957 	 0.00058 	 m..s
   70 	    19 	 0.05121 	 0.00060 	 m..s
   23 	    20 	 0.04912 	 0.00060 	 m..s
   79 	    21 	 0.05250 	 0.00062 	 m..s
    5 	    22 	 0.04824 	 0.00064 	 m..s
   43 	    23 	 0.04962 	 0.00067 	 m..s
    8 	    24 	 0.04828 	 0.00067 	 m..s
   18 	    25 	 0.04898 	 0.00073 	 m..s
   73 	    26 	 0.05223 	 0.00073 	 m..s
   58 	    27 	 0.05022 	 0.00075 	 m..s
   28 	    28 	 0.04926 	 0.00077 	 m..s
   60 	    29 	 0.05030 	 0.00077 	 m..s
   55 	    30 	 0.05004 	 0.00078 	 m..s
   88 	    31 	 0.05672 	 0.00079 	 m..s
   40 	    32 	 0.04961 	 0.00080 	 m..s
   90 	    33 	 0.05722 	 0.00081 	 m..s
   41 	    34 	 0.04961 	 0.00082 	 m..s
   14 	    35 	 0.04878 	 0.00082 	 m..s
   13 	    36 	 0.04877 	 0.00082 	 m..s
   44 	    37 	 0.04964 	 0.00083 	 m..s
   11 	    38 	 0.04834 	 0.00085 	 m..s
   50 	    39 	 0.04982 	 0.00086 	 m..s
   56 	    40 	 0.05018 	 0.00087 	 m..s
   48 	    41 	 0.04970 	 0.00087 	 m..s
    1 	    42 	 0.04726 	 0.00087 	 m..s
   36 	    43 	 0.04950 	 0.00089 	 m..s
   81 	    44 	 0.05274 	 0.00091 	 m..s
   86 	    45 	 0.05404 	 0.00092 	 m..s
   72 	    46 	 0.05184 	 0.00092 	 m..s
   64 	    47 	 0.05045 	 0.00094 	 m..s
   83 	    48 	 0.05309 	 0.00095 	 m..s
   34 	    49 	 0.04944 	 0.00097 	 m..s
   32 	    50 	 0.04942 	 0.00099 	 m..s
    7 	    51 	 0.04826 	 0.00101 	 m..s
   42 	    52 	 0.04962 	 0.00102 	 m..s
   49 	    53 	 0.04971 	 0.00109 	 m..s
   54 	    54 	 0.05002 	 0.00110 	 m..s
   51 	    55 	 0.04988 	 0.00112 	 m..s
   63 	    56 	 0.05040 	 0.00112 	 m..s
   89 	    57 	 0.05692 	 0.00114 	 m..s
   38 	    58 	 0.04958 	 0.00115 	 m..s
    6 	    59 	 0.04825 	 0.00119 	 m..s
   26 	    60 	 0.04920 	 0.00119 	 m..s
   62 	    61 	 0.05039 	 0.00123 	 m..s
   59 	    62 	 0.05026 	 0.00125 	 m..s
   45 	    63 	 0.04965 	 0.00125 	 m..s
   61 	    64 	 0.05038 	 0.00126 	 m..s
   66 	    65 	 0.05047 	 0.00126 	 m..s
   53 	    66 	 0.04992 	 0.00127 	 m..s
   84 	    67 	 0.05344 	 0.00130 	 m..s
   87 	    68 	 0.05445 	 0.00131 	 m..s
   82 	    69 	 0.05288 	 0.00139 	 m..s
   30 	    70 	 0.04938 	 0.00139 	 m..s
   10 	    71 	 0.04831 	 0.00153 	 m..s
   21 	    72 	 0.04907 	 0.00156 	 m..s
   47 	    73 	 0.04969 	 0.00165 	 m..s
   68 	    74 	 0.05092 	 0.00166 	 m..s
   52 	    75 	 0.04988 	 0.00170 	 m..s
   69 	    76 	 0.05096 	 0.00171 	 m..s
   20 	    77 	 0.04906 	 0.00186 	 m..s
   19 	    78 	 0.04899 	 0.00190 	 m..s
   65 	    79 	 0.05046 	 0.00200 	 m..s
    3 	    80 	 0.04773 	 0.00200 	 m..s
   15 	    81 	 0.04886 	 0.00211 	 m..s
    2 	    82 	 0.04739 	 0.00231 	 m..s
   24 	    83 	 0.04913 	 0.00268 	 m..s
  106 	    84 	 0.13508 	 0.00581 	 MISS
  105 	    85 	 0.12147 	 0.00638 	 MISS
   97 	    86 	 0.06478 	 0.01398 	 m..s
   95 	    87 	 0.06062 	 0.04959 	 ~...
   99 	    88 	 0.06719 	 0.06197 	 ~...
   74 	    89 	 0.05224 	 0.06666 	 ~...
   77 	    90 	 0.05245 	 0.07079 	 ~...
   71 	    91 	 0.05121 	 0.07189 	 ~...
   91 	    92 	 0.05818 	 0.07254 	 ~...
   92 	    93 	 0.05828 	 0.07469 	 ~...
   75 	    94 	 0.05232 	 0.07888 	 ~...
   93 	    95 	 0.05835 	 0.07946 	 ~...
   96 	    96 	 0.06247 	 0.08041 	 ~...
   94 	    97 	 0.05837 	 0.08157 	 ~...
   67 	    98 	 0.05048 	 0.08260 	 m..s
   80 	    99 	 0.05260 	 0.08673 	 m..s
   76 	   100 	 0.05245 	 0.08738 	 m..s
  109 	   101 	 0.14534 	 0.09024 	 m..s
   98 	   102 	 0.06710 	 0.09730 	 m..s
  107 	   103 	 0.13573 	 0.10118 	 m..s
  110 	   104 	 0.15309 	 0.10876 	 m..s
  101 	   105 	 0.10858 	 0.12239 	 ~...
  112 	   106 	 0.17263 	 0.12298 	 m..s
  113 	   107 	 0.18572 	 0.12358 	 m..s
  102 	   108 	 0.11560 	 0.12472 	 ~...
  103 	   109 	 0.11917 	 0.13496 	 ~...
  104 	   110 	 0.12046 	 0.13906 	 ~...
  108 	   111 	 0.13665 	 0.14416 	 ~...
  100 	   112 	 0.07874 	 0.16228 	 m..s
  114 	   113 	 0.21851 	 0.20609 	 ~...
  115 	   114 	 0.22445 	 0.21845 	 ~...
  111 	   115 	 0.16968 	 0.25863 	 m..s
  116 	   116 	 0.23703 	 0.26597 	 ~...
  117 	   117 	 0.26861 	 0.27048 	 ~...
  119 	   118 	 0.31484 	 0.29278 	 ~...
  120 	   119 	 0.39213 	 0.31774 	 m..s
  118 	   120 	 0.30688 	 0.35511 	 m..s
==========================================
r_mrr = 0.9006509184837341
r2_mrr = 0.5908939838409424
spearmanr_mrr@5 = 0.9706206321716309
spearmanr_mrr@10 = 0.9474472999572754
spearmanr_mrr@50 = 0.9571342468261719
spearmanr_mrr@100 = 0.9586916565895081
spearmanr_mrr@All = 0.9597795009613037
==========================================
test time: 0.456
Done Testing dataset OpenEA
total time taken: 253.58000588417053
training time taken: 226.33491706848145
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.9007)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5909)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9706)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9474)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9571)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9587)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9598)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.5832692088220028}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 5801956104574769
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [653, 860, 612, 314, 240, 198, 819, 1207, 303, 538, 39, 886, 118, 844, 1138, 812, 65, 1020, 654, 823, 813, 7, 1146, 940, 918, 722, 1071, 1101, 1127, 943, 545, 24, 779, 488, 321, 710, 412, 241, 824, 977, 143, 1171, 432, 145, 738, 1130, 1191, 349, 727, 376, 973, 329, 517, 778, 1210, 360, 846, 553, 929, 1068, 705, 1095, 1148, 262, 319, 324, 556, 19, 286, 298, 641, 433, 856, 110, 1211, 224, 231, 78, 10, 476, 362, 68, 96, 58, 658, 739, 116, 648, 993, 661, 481, 15, 1116, 593, 1155, 273, 884, 948, 1136, 1160, 22, 1159, 1150, 1175, 135, 250, 112, 177, 358, 740, 1198, 875, 1126, 104, 798, 913, 81, 1031, 88, 1084, 868]
valid_ids (0): []
train_ids (1094): [902, 1063, 499, 1214, 1162, 14, 881, 622, 313, 745, 200, 185, 749, 1205, 683, 951, 939, 228, 797, 946, 893, 459, 652, 861, 270, 513, 1038, 1061, 930, 239, 344, 138, 410, 1104, 1118, 53, 698, 46, 578, 1196, 417, 777, 74, 463, 729, 1152, 614, 113, 20, 995, 1081, 1112, 328, 1115, 223, 703, 184, 796, 560, 63, 3, 910, 1135, 139, 934, 368, 528, 34, 597, 841, 1074, 310, 1145, 464, 1089, 157, 1111, 781, 204, 478, 715, 170, 1097, 446, 985, 1092, 523, 912, 221, 408, 697, 857, 1174, 941, 374, 889, 950, 832, 132, 592, 4, 984, 471, 1088, 381, 491, 390, 126, 516, 642, 305, 1064, 1085, 706, 936, 510, 456, 987, 965, 916, 404, 1093, 1076, 140, 816, 772, 770, 594, 531, 547, 67, 655, 477, 980, 549, 551, 1161, 76, 274, 758, 820, 422, 976, 308, 264, 101, 169, 1086, 828, 233, 611, 1042, 188, 807, 561, 982, 855, 1046, 931, 259, 426, 159, 769, 77, 685, 1208, 581, 66, 1029, 427, 1098, 890, 1033, 1035, 1187, 786, 1166, 75, 290, 175, 263, 60, 445, 507, 789, 768, 215, 567, 398, 450, 334, 955, 49, 315, 1034, 602, 373, 487, 41, 1040, 171, 862, 492, 983, 699, 386, 580, 880, 5, 585, 95, 800, 133, 539, 1186, 1066, 247, 219, 746, 1180, 48, 668, 1021, 162, 227, 1010, 1028, 13, 178, 93, 402, 356, 307, 391, 497, 586, 1128, 651, 428, 461, 482, 708, 0, 434, 106, 974, 158, 1080, 448, 914, 346, 971, 266, 355, 625, 701, 563, 990, 129, 922, 692, 92, 1, 335, 1057, 667, 662, 211, 474, 333, 256, 1007, 97, 557, 366, 532, 136, 1050, 351, 562, 850, 766, 265, 297, 283, 309, 94, 848, 811, 559, 453, 70, 921, 1099, 1056, 899, 1137, 436, 963, 127, 599, 1024, 757, 760, 629, 30, 666, 1058, 1179, 638, 217, 439, 89, 1140, 944, 1003, 485, 1121, 573, 679, 575, 782, 73, 623, 750, 595, 851, 111, 806, 898, 440, 268, 44, 42, 416, 484, 199, 361, 301, 479, 784, 245, 767, 1082, 588, 189, 71, 988, 103, 731, 790, 1001, 1009, 690, 1000, 659, 1107, 359, 455, 826, 393, 1078, 1192, 515, 252, 1144, 343, 700, 718, 1172, 647, 151, 258, 1185, 124, 1072, 325, 1168, 222, 1206, 957, 128, 107, 892, 192, 609, 424, 645, 236, 814, 1036, 480, 534, 163, 1132, 707, 1164, 568, 392, 338, 509, 882, 603, 379, 1125, 146, 799, 591, 649, 958, 587, 1193, 1197, 1123, 792, 1065, 260, 206, 1070, 780, 318, 415, 395, 1043, 815, 454, 646, 1190, 878, 529, 883, 458, 37, 730, 733, 656, 870, 421, 267, 933, 1077, 570, 414, 747, 1067, 947, 923, 762, 524, 743, 967, 134, 377, 1044, 583, 238, 845, 1079, 340, 430, 1204, 1049, 952, 1200, 681, 279, 311, 992, 871, 207, 1142, 300, 119, 613, 33, 339, 1014, 837, 213, 617, 407, 1060, 558, 776, 680, 122, 387, 153, 1165, 907, 804, 342, 1102, 840, 399, 716, 801, 1018, 775, 388, 214, 1139, 888, 571, 350, 576, 537, 341, 803, 550, 858, 664, 981, 326, 879, 1114, 773, 90, 425, 149, 520, 131, 1188, 1026, 903, 624, 29, 853, 353, 809, 1117, 953, 1131, 689, 755, 911, 438, 540, 925, 28, 367, 1189, 272, 1016, 1203, 917, 242, 320, 8, 754, 117, 842, 1090, 835, 927, 596, 928, 100, 304, 915, 831, 548, 744, 244, 942, 442, 62, 966, 1032, 369, 32, 839, 1048, 1041, 12, 142, 997, 634, 536, 821, 905, 35, 1108, 210, 1017, 682, 9, 1153, 843, 187, 678, 579, 87, 1199, 150, 633, 695, 429, 1170, 232, 23, 726, 765, 357, 1201, 409, 261, 774, 628, 237, 643, 1213, 80, 449, 873, 723, 891, 444, 759, 732, 1052, 383, 31, 1184, 64, 55, 630, 1122, 56, 156, 1030, 371, 486, 709, 589, 869, 504, 1141, 872, 51, 605, 1091, 637, 1012, 27, 1096, 544, 737, 500, 1022, 1051, 670, 618, 166, 527, 793, 574, 1015, 191, 1013, 394, 543, 401, 1173, 50, 179, 616, 287, 45, 1109, 834, 639, 864, 694, 431, 312, 938, 1045, 909, 203, 181, 1129, 631, 569, 11, 564, 229, 54, 1133, 601, 288, 1154, 867, 234, 1124, 714, 959, 209, 174, 230, 1027, 382, 818, 771, 610, 194, 475, 669, 202, 226, 535, 437, 721, 640, 734, 155, 1195, 251, 173, 460, 742, 6, 753, 822, 673, 384, 172, 193, 671, 626, 322, 720, 180, 121, 735, 787, 176, 908, 59, 494, 1176, 761, 291, 937, 1181, 473, 829, 403, 691, 863, 25, 866, 375, 420, 498, 675, 141, 489, 483, 712, 584, 152, 741, 830, 490, 43, 82, 1113, 400, 525, 1177, 496, 600, 1059, 462, 900, 541, 752, 493, 1073, 852, 364, 827, 472, 397, 802, 975, 518, 1134, 1039, 1156, 91, 216, 69, 165, 302, 277, 465, 469, 190, 197, 495, 748, 632, 865, 249, 330, 713, 269, 503, 336, 552, 650, 299, 725, 61, 347, 808, 1006, 317, 764, 389, 1143, 137, 756, 1110, 332, 998, 621, 457, 447, 316, 423, 1103, 904, 751, 130, 989, 606, 345, 275, 144, 1158, 590, 294, 289, 443, 1047, 411, 530, 969, 526, 235, 212, 859, 1025, 105, 26, 196, 788, 608, 1209, 704, 86, 546, 501, 183, 719, 84, 1183, 847, 791, 1194, 566, 354, 108, 254, 182, 348, 665, 413, 962, 895, 785, 1169, 278, 164, 452, 1157, 1106, 817, 1037, 406, 1202, 1100, 38, 419, 378, 810, 99, 577, 924, 280, 996, 40, 293, 468, 160, 964, 627, 565, 218, 887, 805, 1094, 257, 21, 147, 763, 370, 702, 385, 148, 1151, 876, 120, 285, 968, 16, 125, 663, 674, 514, 994, 208, 572, 380, 874, 636, 243, 961, 932, 1120, 795, 838, 396, 906, 1055, 337, 79, 687, 711, 1005, 919, 644, 72, 1147, 186, 693, 1182, 620, 123, 615, 978, 896, 522, 1119, 1053, 306, 195, 672, 435, 972, 1019, 619, 295, 225, 418, 83, 502, 466, 1087, 470, 877, 783, 1075, 1163, 255, 154, 677, 970, 323, 935, 467, 47, 949, 598, 686, 102, 885, 926, 441, 542, 954, 246, 728, 635, 511, 201, 161, 114, 115, 284, 1054, 1212, 901, 331, 849, 248, 999, 18, 897, 292, 555, 991, 1178, 220, 854, 945, 894, 98, 363, 521, 372, 688, 281, 508, 205, 282, 1083, 109, 451, 533, 1004, 405, 296, 1149, 607, 512, 684, 825, 2, 1011, 582, 271, 505, 657, 519, 736, 960, 979, 836, 676, 554, 1105, 85, 167, 352, 327, 794, 1062, 1069, 1002, 1167, 506, 696, 604, 920, 365, 986, 17, 168, 1008, 57, 833, 52, 717, 1023, 724, 956, 276, 660, 36, 253]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2172405729290142
the save name prefix for this run is:  chkpt-ID_2172405729290142_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 791
rank avg (pred): 0.587 +- 0.007
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002811621

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 184
rank avg (pred): 0.437 +- 0.285
mrr vals (pred, true): 0.104, 0.001
batch losses (mrrl, rdl): 0.0, 2.71365e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 608
rank avg (pred): 0.443 +- 0.296
mrr vals (pred, true): 0.091, 0.001
batch losses (mrrl, rdl): 0.0, 1.83108e-05

Epoch over!
epoch time: 14.902

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1028
rank avg (pred): 0.428 +- 0.293
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0, 3.33143e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 815
rank avg (pred): 0.198 +- 0.173
mrr vals (pred, true): 0.082, 0.349
batch losses (mrrl, rdl): 0.0, 0.0004440625

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 192
rank avg (pred): 0.488 +- 0.290
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0, 2.3405e-06

Epoch over!
epoch time: 14.852

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 332
rank avg (pred): 0.490 +- 0.308
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0, 2.75e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 557
rank avg (pred): 0.357 +- 0.313
mrr vals (pred, true): 0.058, 0.075
batch losses (mrrl, rdl): 0.0, 9.609e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 3
rank avg (pred): 0.174 +- 0.251
mrr vals (pred, true): 0.126, 0.071
batch losses (mrrl, rdl): 0.0, 0.0002193282

Epoch over!
epoch time: 14.845

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 573
rank avg (pred): 0.484 +- 0.286
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0, 1.5974e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 117
rank avg (pred): 0.501 +- 0.300
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0, 8.727e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1212
rank avg (pred): 0.485 +- 0.289
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0, 3.3759e-06

Epoch over!
epoch time: 14.801

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 383
rank avg (pred): 0.477 +- 0.293
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0, 1.9563e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 49
rank avg (pred): 0.319 +- 0.323
mrr vals (pred, true): 0.092, 0.079
batch losses (mrrl, rdl): 0.0, 2.10601e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1099
rank avg (pred): 0.497 +- 0.288
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0, 6.0707e-06

Epoch over!
epoch time: 14.861

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 807
rank avg (pred): 0.513 +- 0.288
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.40345e-05, 9.7054e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1214
rank avg (pred): 0.505 +- 0.292
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.001341464, 1.3607e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 430
rank avg (pred): 0.486 +- 0.311
mrr vals (pred, true): 0.072, 0.001
batch losses (mrrl, rdl): 0.0046460885, 5.4769e-06

Epoch over!
epoch time: 16.386

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 508
rank avg (pred): 0.241 +- 0.382
mrr vals (pred, true): 0.163, 0.218
batch losses (mrrl, rdl): 0.0302156955, 4.19602e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 350
rank avg (pred): 0.491 +- 0.315
mrr vals (pred, true): 0.067, 0.001
batch losses (mrrl, rdl): 0.0027820943, 1.9019e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 186
rank avg (pred): 0.489 +- 0.306
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0007013838, 5.8251e-06

Epoch over!
epoch time: 16.393

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 455
rank avg (pred): 0.491 +- 0.249
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001538837, 1.97565e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 770
rank avg (pred): 0.490 +- 0.283
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001282996, 8.5282e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 557
rank avg (pred): 0.483 +- 0.295
mrr vals (pred, true): 0.054, 0.075
batch losses (mrrl, rdl): 0.0001948122, 0.0002384039

Epoch over!
epoch time: 18.468

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 267
rank avg (pred): 0.189 +- 0.384
mrr vals (pred, true): 0.267, 0.264
batch losses (mrrl, rdl): 0.0001113813, 0.0002629398

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 220
rank avg (pred): 0.500 +- 0.263
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0002023573, 7.6935e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 852
rank avg (pred): 0.487 +- 0.276
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 2.0028e-05, 2.09379e-05

Epoch over!
epoch time: 17.397

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1076
rank avg (pred): 0.449 +- 0.302
mrr vals (pred, true): 0.057, 0.094
batch losses (mrrl, rdl): 0.0005374087, 0.0001851805

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 721
rank avg (pred): 0.515 +- 0.259
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003307459, 8.9606e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 207
rank avg (pred): 0.490 +- 0.247
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003204831, 1.83168e-05

Epoch over!
epoch time: 17.773

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 151
rank avg (pred): 0.505 +- 0.269
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.24241e-05, 5.7444e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 591
rank avg (pred): 0.483 +- 0.259
mrr vals (pred, true): 0.047, 0.002
batch losses (mrrl, rdl): 0.0001043609, 1.75136e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1152
rank avg (pred): 0.361 +- 0.240
mrr vals (pred, true): 0.080, 0.082
batch losses (mrrl, rdl): 0.0090673855, 3.32033e-05

Epoch over!
epoch time: 16.281

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 524
rank avg (pred): 0.441 +- 0.257
mrr vals (pred, true): 0.049, 0.072
batch losses (mrrl, rdl): 2.9109e-06, 0.0001783172

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 589
rank avg (pred): 0.487 +- 0.269
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001193101, 1.40833e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1158
rank avg (pred): 0.440 +- 0.376
mrr vals (pred, true): 0.136, 0.145
batch losses (mrrl, rdl): 0.0006830289, 0.0003239709

Epoch over!
epoch time: 15.092

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 86
rank avg (pred): 0.530 +- 0.324
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002828299, 2.37646e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 692
rank avg (pred): 0.468 +- 0.258
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 5.521e-07, 3.73175e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 493
rank avg (pred): 0.379 +- 0.355
mrr vals (pred, true): 0.127, 0.072
batch losses (mrrl, rdl): 0.0585229099, 4.60985e-05

Epoch over!
epoch time: 15.109

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 456
rank avg (pred): 0.517 +- 0.284
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001718998, 3.7208e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 222
rank avg (pred): 0.488 +- 0.294
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002033164, 1.17958e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 983
rank avg (pred): 0.408 +- 0.309
mrr vals (pred, true): 0.074, 0.081
batch losses (mrrl, rdl): 0.0056953165, 4.25386e-05

Epoch over!
epoch time: 15.183

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 438
rank avg (pred): 0.522 +- 0.299
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.571e-06, 1.4278e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 664
rank avg (pred): 0.483 +- 0.287
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.65143e-05, 2.25027e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 232
rank avg (pred): 0.505 +- 0.290
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 9.808e-07, 2.0027e-06

Epoch over!
epoch time: 15.123

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.438 +- 0.290
mrr vals (pred, true): 0.052, 0.000

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   13 	     0 	 0.04635 	 0.00043 	 m..s
   49 	     1 	 0.04858 	 0.00047 	 m..s
   15 	     2 	 0.04705 	 0.00048 	 m..s
   74 	     3 	 0.05236 	 0.00049 	 m..s
    6 	     4 	 0.04587 	 0.00049 	 m..s
   38 	     5 	 0.04807 	 0.00050 	 m..s
    5 	     6 	 0.04539 	 0.00053 	 m..s
   30 	     7 	 0.04760 	 0.00053 	 m..s
   60 	     8 	 0.05014 	 0.00055 	 m..s
    4 	     9 	 0.04461 	 0.00055 	 m..s
   45 	    10 	 0.04838 	 0.00055 	 m..s
   64 	    11 	 0.05053 	 0.00057 	 m..s
   53 	    12 	 0.04876 	 0.00057 	 m..s
   65 	    13 	 0.05076 	 0.00060 	 m..s
   39 	    14 	 0.04807 	 0.00060 	 m..s
    0 	    15 	 0.04251 	 0.00061 	 m..s
   32 	    16 	 0.04774 	 0.00064 	 m..s
   23 	    17 	 0.04735 	 0.00064 	 m..s
   58 	    18 	 0.04956 	 0.00064 	 m..s
    3 	    19 	 0.04416 	 0.00065 	 m..s
    7 	    20 	 0.04603 	 0.00065 	 m..s
   46 	    21 	 0.04839 	 0.00066 	 m..s
   44 	    22 	 0.04836 	 0.00067 	 m..s
   55 	    23 	 0.04880 	 0.00068 	 m..s
    1 	    24 	 0.04326 	 0.00070 	 m..s
   54 	    25 	 0.04877 	 0.00070 	 m..s
    2 	    26 	 0.04335 	 0.00072 	 m..s
   10 	    27 	 0.04626 	 0.00072 	 m..s
   41 	    28 	 0.04834 	 0.00073 	 m..s
   21 	    29 	 0.04731 	 0.00073 	 m..s
   43 	    30 	 0.04835 	 0.00074 	 m..s
   56 	    31 	 0.04890 	 0.00075 	 m..s
   57 	    32 	 0.04955 	 0.00077 	 m..s
   29 	    33 	 0.04759 	 0.00078 	 m..s
   27 	    34 	 0.04754 	 0.00078 	 m..s
   14 	    35 	 0.04644 	 0.00080 	 m..s
   40 	    36 	 0.04820 	 0.00081 	 m..s
    9 	    37 	 0.04621 	 0.00082 	 m..s
   19 	    38 	 0.04721 	 0.00084 	 m..s
   59 	    39 	 0.04964 	 0.00085 	 m..s
   51 	    40 	 0.04872 	 0.00086 	 m..s
   48 	    41 	 0.04856 	 0.00090 	 m..s
   61 	    42 	 0.05016 	 0.00091 	 m..s
   63 	    43 	 0.05025 	 0.00092 	 m..s
   17 	    44 	 0.04713 	 0.00094 	 m..s
   12 	    45 	 0.04631 	 0.00095 	 m..s
   26 	    46 	 0.04753 	 0.00099 	 m..s
   62 	    47 	 0.05020 	 0.00100 	 m..s
   42 	    48 	 0.04834 	 0.00100 	 m..s
   67 	    49 	 0.05108 	 0.00101 	 m..s
   20 	    50 	 0.04729 	 0.00102 	 m..s
   33 	    51 	 0.04775 	 0.00103 	 m..s
   82 	    52 	 0.05334 	 0.00109 	 m..s
   11 	    53 	 0.04628 	 0.00110 	 m..s
   36 	    54 	 0.04796 	 0.00110 	 m..s
    8 	    55 	 0.04620 	 0.00115 	 m..s
   50 	    56 	 0.04863 	 0.00116 	 m..s
   25 	    57 	 0.04748 	 0.00120 	 m..s
   31 	    58 	 0.04761 	 0.00122 	 m..s
   28 	    59 	 0.04755 	 0.00132 	 m..s
   66 	    60 	 0.05090 	 0.00142 	 m..s
   34 	    61 	 0.04781 	 0.00145 	 m..s
   73 	    62 	 0.05232 	 0.00154 	 m..s
   92 	    63 	 0.06281 	 0.00154 	 m..s
   22 	    64 	 0.04732 	 0.00159 	 m..s
   69 	    65 	 0.05166 	 0.00160 	 m..s
   16 	    66 	 0.04709 	 0.00164 	 m..s
   52 	    67 	 0.04876 	 0.00165 	 m..s
   68 	    68 	 0.05142 	 0.00169 	 m..s
   18 	    69 	 0.04714 	 0.00170 	 m..s
   24 	    70 	 0.04746 	 0.00177 	 m..s
   37 	    71 	 0.04798 	 0.00190 	 m..s
   35 	    72 	 0.04786 	 0.00196 	 m..s
   47 	    73 	 0.04852 	 0.00228 	 m..s
   71 	    74 	 0.05203 	 0.05603 	 ~...
   89 	    75 	 0.05497 	 0.06381 	 ~...
   88 	    76 	 0.05494 	 0.06533 	 ~...
   77 	    77 	 0.05275 	 0.06676 	 ~...
   85 	    78 	 0.05423 	 0.06882 	 ~...
   72 	    79 	 0.05228 	 0.07013 	 ~...
   90 	    80 	 0.05795 	 0.07066 	 ~...
   84 	    81 	 0.05394 	 0.07284 	 ~...
   86 	    82 	 0.05431 	 0.07464 	 ~...
   80 	    83 	 0.05297 	 0.07493 	 ~...
   83 	    84 	 0.05348 	 0.07607 	 ~...
   75 	    85 	 0.05262 	 0.07688 	 ~...
   81 	    86 	 0.05298 	 0.07869 	 ~...
   79 	    87 	 0.05283 	 0.08451 	 m..s
   78 	    88 	 0.05279 	 0.08508 	 m..s
   70 	    89 	 0.05189 	 0.08832 	 m..s
   76 	    90 	 0.05268 	 0.08873 	 m..s
   87 	    91 	 0.05434 	 0.08943 	 m..s
   98 	    92 	 0.08008 	 0.08960 	 ~...
  111 	    93 	 0.20246 	 0.09414 	 MISS
   93 	    94 	 0.06755 	 0.09562 	 ~...
  108 	    95 	 0.16073 	 0.09658 	 m..s
   94 	    96 	 0.06892 	 0.09806 	 ~...
   97 	    97 	 0.07986 	 0.10244 	 ~...
   91 	    98 	 0.06228 	 0.10389 	 m..s
   96 	    99 	 0.07856 	 0.10497 	 ~...
  112 	   100 	 0.20966 	 0.10876 	 MISS
  107 	   101 	 0.15739 	 0.12821 	 ~...
   95 	   102 	 0.07541 	 0.13256 	 m..s
  100 	   103 	 0.10750 	 0.13560 	 ~...
  103 	   104 	 0.14192 	 0.13779 	 ~...
  101 	   105 	 0.13624 	 0.13837 	 ~...
  104 	   106 	 0.15210 	 0.13862 	 ~...
  102 	   107 	 0.14173 	 0.14035 	 ~...
  105 	   108 	 0.15219 	 0.14416 	 ~...
  106 	   109 	 0.15247 	 0.14575 	 ~...
   99 	   110 	 0.08212 	 0.14792 	 m..s
  110 	   111 	 0.18421 	 0.17796 	 ~...
  109 	   112 	 0.17991 	 0.18250 	 ~...
  113 	   113 	 0.22443 	 0.24200 	 ~...
  118 	   114 	 0.28884 	 0.25971 	 ~...
  114 	   115 	 0.24698 	 0.26597 	 ~...
  117 	   116 	 0.28414 	 0.27717 	 ~...
  116 	   117 	 0.28163 	 0.27826 	 ~...
  115 	   118 	 0.27812 	 0.28749 	 ~...
  120 	   119 	 0.34998 	 0.32392 	 ~...
  119 	   120 	 0.33997 	 0.35511 	 ~...
==========================================
r_mrr = 0.9119961857795715
r2_mrr = 0.7199350595474243
spearmanr_mrr@5 = 0.974155843257904
spearmanr_mrr@10 = 0.9644283056259155
spearmanr_mrr@50 = 0.9646260142326355
spearmanr_mrr@100 = 0.9372473955154419
spearmanr_mrr@All = 0.9388760924339294
==========================================
test time: 0.458
Done Testing dataset OpenEA
total time taken: 263.8204171657562
training time taken: 237.9575171470642
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.9120)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.7199)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.9742)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.9644)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9646)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9372)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9389)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.6402991152699542}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 2867934705572319
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [273, 1069, 167, 1062, 1148, 829, 845, 259, 648, 962, 198, 1091, 768, 465, 229, 71, 928, 125, 1133, 500, 1060, 258, 540, 481, 240, 402, 1097, 954, 1038, 596, 750, 268, 618, 433, 541, 1008, 159, 634, 670, 808, 621, 424, 788, 1178, 515, 435, 440, 960, 185, 989, 306, 580, 869, 1096, 1162, 278, 628, 676, 1093, 523, 727, 778, 573, 1183, 1186, 54, 480, 1005, 362, 723, 30, 1037, 0, 272, 737, 1023, 158, 665, 529, 572, 299, 138, 162, 333, 751, 155, 642, 130, 1111, 80, 524, 1003, 1120, 86, 1179, 686, 936, 193, 886, 752, 1079, 669, 581, 985, 445, 422, 320, 630, 196, 485, 836, 406, 50, 766, 1212, 121, 399, 318, 224, 862, 970]
valid_ids (0): []
train_ids (1094): [549, 986, 96, 969, 965, 8, 1174, 99, 827, 1116, 1207, 95, 823, 351, 565, 415, 213, 458, 476, 378, 918, 108, 764, 660, 493, 291, 254, 854, 508, 7, 620, 917, 41, 872, 847, 797, 644, 559, 852, 382, 721, 404, 56, 753, 395, 1129, 900, 667, 1044, 1134, 1034, 76, 1042, 1196, 1015, 757, 932, 899, 856, 168, 428, 60, 156, 576, 106, 256, 895, 470, 153, 958, 334, 939, 1172, 715, 189, 225, 1100, 1142, 521, 1105, 477, 762, 871, 979, 688, 612, 833, 1201, 771, 232, 1039, 649, 934, 709, 898, 52, 641, 503, 188, 817, 342, 1081, 956, 288, 756, 165, 1077, 1110, 301, 1199, 449, 777, 1108, 1, 821, 251, 443, 5, 365, 774, 711, 1094, 453, 53, 851, 81, 510, 1040, 608, 116, 358, 674, 1175, 681, 38, 384, 587, 197, 993, 905, 244, 740, 853, 1157, 180, 902, 890, 484, 151, 706, 386, 287, 1188, 200, 906, 730, 1022, 83, 974, 369, 437, 326, 680, 84, 646, 945, 135, 1019, 471, 911, 638, 846, 784, 325, 13, 266, 296, 606, 139, 1128, 271, 653, 277, 230, 340, 889, 1000, 617, 1187, 640, 514, 867, 380, 1112, 792, 594, 673, 302, 1018, 354, 913, 113, 1011, 1151, 915, 131, 261, 550, 805, 62, 564, 448, 136, 909, 262, 554, 769, 316, 314, 35, 461, 544, 699, 988, 603, 1125, 749, 298, 849, 822, 147, 1047, 242, 370, 964, 300, 607, 1055, 943, 473, 166, 1084, 772, 619, 357, 692, 190, 210, 187, 701, 528, 922, 234, 442, 178, 1029, 379, 941, 828, 700, 839, 388, 1090, 1159, 401, 546, 951, 1198, 629, 492, 475, 1088, 819, 359, 102, 947, 1043, 601, 1113, 1024, 579, 114, 560, 1017, 809, 746, 679, 313, 450, 352, 894, 537, 513, 177, 434, 1049, 1036, 781, 840, 48, 51, 431, 720, 85, 545, 661, 775, 1074, 91, 982, 657, 134, 604, 233, 491, 4, 1184, 1181, 1004, 593, 743, 356, 1013, 861, 1169, 149, 331, 536, 425, 441, 639, 360, 439, 810, 1130, 1126, 322, 1053, 1086, 1149, 1204, 971, 726, 487, 668, 78, 3, 403, 1076, 966, 1177, 870, 297, 804, 563, 901, 882, 803, 1046, 526, 1213, 409, 307, 908, 144, 164, 1085, 677, 488, 247, 710, 937, 1058, 1176, 760, 1194, 411, 570, 509, 345, 1132, 94, 289, 1164, 1109, 28, 1059, 636, 977, 651, 685, 24, 1098, 145, 92, 548, 115, 120, 184, 684, 530, 235, 430, 119, 160, 1123, 381, 744, 623, 332, 1061, 348, 782, 260, 65, 209, 578, 383, 157, 400, 252, 930, 542, 285, 31, 516, 324, 191, 474, 655, 1200, 935, 49, 864, 891, 408, 100, 44, 281, 192, 598, 1027, 372, 495, 691, 798, 447, 812, 972, 506, 174, 645, 19, 517, 218, 724, 245, 837, 204, 172, 1156, 843, 57, 678, 610, 779, 831, 1057, 1095, 309, 920, 1206, 417, 1078, 1001, 944, 703, 525, 926, 888, 72, 66, 1168, 1021, 532, 534, 647, 759, 697, 583, 21, 783, 616, 223, 995, 222, 444, 1064, 104, 967, 738, 531, 1124, 269, 1160, 1182, 276, 733, 787, 754, 339, 558, 844, 820, 780, 707, 363, 103, 719, 9, 1103, 838, 903, 1115, 652, 1192, 925, 734, 816, 1009, 146, 1016, 90, 722, 690, 877, 295, 466, 410, 884, 976, 1092, 343, 940, 555, 23, 875, 248, 148, 518, 605, 270, 58, 6, 1209, 519, 814, 539, 328, 214, 929, 127, 512, 10, 1006, 1185, 868, 459, 698, 330, 758, 42, 212, 98, 567, 385, 566, 61, 815, 141, 373, 413, 118, 991, 143, 597, 226, 535, 1150, 132, 946, 1155, 88, 834, 338, 129, 973, 483, 632, 1070, 446, 527, 712, 1173, 835, 396, 635, 75, 405, 883, 239, 1170, 1138, 274, 1203, 571, 137, 574, 859, 228, 1161, 765, 350, 1014, 631, 586, 317, 414, 975, 490, 702, 1214, 552, 662, 1211, 950, 507, 1137, 1141, 217, 142, 745, 264, 714, 310, 865, 874, 1054, 938, 39, 455, 556, 390, 561, 1121, 171, 377, 824, 551, 418, 1165, 241, 472, 795, 801, 216, 992, 1056, 613, 436, 1205, 429, 695, 452, 921, 663, 650, 767, 366, 68, 93, 286, 112, 105, 1143, 305, 857, 280, 785, 876, 624, 367, 451, 391, 879, 195, 694, 1102, 207, 1208, 394, 1050, 708, 625, 376, 1045, 1107, 17, 959, 265, 186, 654, 283, 599, 194, 791, 1135, 1114, 64, 832, 591, 1025, 220, 32, 375, 907, 253, 687, 637, 562, 237, 543, 881, 133, 150, 1041, 454, 152, 1158, 392, 1166, 553, 109, 59, 825, 892, 124, 412, 77, 1106, 615, 948, 501, 371, 1026, 1020, 931, 312, 996, 557, 353, 584, 689, 855, 89, 63, 199, 349, 742, 1190, 1118, 547, 463, 910, 1193, 347, 761, 999, 206, 981, 456, 1153, 215, 208, 643, 1117, 811, 73, 1197, 957, 850, 998, 980, 329, 614, 683, 468, 1051, 656, 1145, 464, 74, 387, 729, 997, 739, 2, 374, 671, 896, 842, 609, 904, 666, 675, 111, 1089, 290, 292, 978, 1007, 522, 589, 1202, 323, 494, 588, 716, 728, 70, 294, 497, 1033, 696, 161, 1104, 1012, 40, 961, 955, 421, 592, 478, 863, 154, 987, 55, 600, 236, 914, 1163, 924, 101, 97, 736, 860, 858, 1136, 953, 1147, 255, 368, 438, 622, 1146, 337, 43, 46, 467, 117, 173, 311, 990, 346, 489, 123, 246, 140, 182, 1140, 813, 848, 963, 718, 15, 419, 802, 18, 1191, 704, 968, 87, 994, 1075, 29, 201, 179, 426, 407, 1073, 279, 933, 12, 1083, 1065, 1063, 122, 505, 175, 1082, 82, 912, 585, 397, 682, 773, 47, 885, 763, 1032, 243, 664, 611, 169, 45, 748, 511, 126, 893, 282, 569, 79, 460, 1010, 69, 1071, 202, 984, 327, 176, 308, 504, 211, 227, 732, 1144, 658, 389, 1031, 927, 1066, 1171, 238, 1127, 293, 1099, 275, 267, 486, 878, 582, 713, 1072, 20, 577, 361, 16, 336, 1028, 806, 183, 747, 741, 533, 1048, 335, 923, 538, 344, 416, 1189, 36, 659, 11, 303, 107, 284, 799, 250, 818, 1139, 693, 315, 873, 1180, 887, 786, 34, 830, 1101, 163, 27, 457, 776, 26, 800, 203, 1122, 1087, 1002, 181, 897, 219, 432, 393, 983, 602, 790, 1035, 469, 257, 22, 916, 626, 355, 423, 502, 420, 633, 1052, 1154, 482, 919, 462, 731, 590, 826, 725, 1152, 841, 321, 952, 568, 949, 364, 942, 627, 880, 595, 37, 575, 398, 1167, 249, 1119, 304, 1067, 427, 796, 479, 231, 1030, 496, 25, 170, 807, 789, 1068, 1195, 794, 33, 14, 1131, 205, 498, 755, 499, 705, 341, 319, 128, 793, 672, 263, 735, 67, 866, 1210, 520, 110, 770, 1080, 221, 717]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1812625387827192
the save name prefix for this run is:  chkpt-ID_1812625387827192_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'ComplEx': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 564
rank avg (pred): 0.518 +- 0.010
mrr vals (pred, true): 0.000, 0.047
batch losses (mrrl, rdl): 0.0, 0.000551232

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.232 +- 0.145
mrr vals (pred, true): 0.136, 0.103
batch losses (mrrl, rdl): 0.0, 0.0002418349

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1089
rank avg (pred): 0.446 +- 0.298
mrr vals (pred, true): 0.171, 0.001
batch losses (mrrl, rdl): 0.0, 1.25041e-05

Epoch over!
epoch time: 14.945

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 96
rank avg (pred): 0.471 +- 0.312
mrr vals (pred, true): 0.160, 0.001
batch losses (mrrl, rdl): 0.0, 1.2061e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 513
rank avg (pred): 0.365 +- 0.266
mrr vals (pred, true): 0.187, 0.078
batch losses (mrrl, rdl): 0.0, 3.9319e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 792
rank avg (pred): 0.462 +- 0.306
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 0.0, 1.07256e-05

Epoch over!
epoch time: 14.903

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1146
rank avg (pred): 0.291 +- 0.235
mrr vals (pred, true): 0.135, 0.102
batch losses (mrrl, rdl): 0.0, 2.56442e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 129
rank avg (pred): 0.484 +- 0.295
mrr vals (pred, true): 0.001, 0.002
batch losses (mrrl, rdl): 0.0, 9.768e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 168
rank avg (pred): 0.483 +- 0.287
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 3.085e-06

Epoch over!
epoch time: 15.138

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 557
rank avg (pred): 0.339 +- 0.329
mrr vals (pred, true): 0.009, 0.075
batch losses (mrrl, rdl): 0.0, 1.20463e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1166
rank avg (pred): 0.493 +- 0.302
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 7.023e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 17
rank avg (pred): 0.262 +- 0.302
mrr vals (pred, true): 0.005, 0.272
batch losses (mrrl, rdl): 0.0, 0.0001406933

Epoch over!
epoch time: 14.986

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 754
rank avg (pred): 0.196 +- 0.263
mrr vals (pred, true): 0.006, 0.192
batch losses (mrrl, rdl): 0.0, 2.55232e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 898
rank avg (pred): 0.468 +- 0.311
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 0.0023875542

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 290
rank avg (pred): 0.337 +- 0.319
mrr vals (pred, true): 0.007, 0.078
batch losses (mrrl, rdl): 0.0, 5.9361e-06

Epoch over!
epoch time: 15.065

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 951
rank avg (pred): 0.515 +- 0.296
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.02437439, 5.6334e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1182
rank avg (pred): 0.452 +- 0.320
mrr vals (pred, true): 0.030, 0.002
batch losses (mrrl, rdl): 0.0038276347, 3.29807e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.485 +- 0.336
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0007372005, 1.3525e-05

Epoch over!
epoch time: 15.326

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 661
rank avg (pred): 0.485 +- 0.345
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.78192e-05, 2.13397e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 852
rank avg (pred): 0.513 +- 0.364
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0013319799, 3.18167e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1087
rank avg (pred): 0.484 +- 0.322
mrr vals (pred, true): 0.036, 0.002
batch losses (mrrl, rdl): 0.0020972041, 5.54e-06

Epoch over!
epoch time: 15.099

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 103
rank avg (pred): 0.372 +- 0.321
mrr vals (pred, true): 0.037, 0.002
batch losses (mrrl, rdl): 0.0016249192, 0.000247548

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 156
rank avg (pred): 0.540 +- 0.361
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0006129469, 5.64717e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 27
rank avg (pred): 0.447 +- 0.361
mrr vals (pred, true): 0.053, 0.069
batch losses (mrrl, rdl): 7.54832e-05, 0.0001877266

Epoch over!
epoch time: 15.296

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 34
rank avg (pred): 0.447 +- 0.356
mrr vals (pred, true): 0.055, 0.075
batch losses (mrrl, rdl): 0.0002286705, 0.0001602717

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 139
rank avg (pred): 0.426 +- 0.330
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002074465, 5.58794e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1191
rank avg (pred): 0.445 +- 0.334
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.36687e-05, 4.18822e-05

Epoch over!
epoch time: 15.314

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 68
rank avg (pred): 0.433 +- 0.335
mrr vals (pred, true): 0.059, 0.089
batch losses (mrrl, rdl): 0.0007920539, 0.0001830499

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1155
rank avg (pred): 0.416 +- 0.338
mrr vals (pred, true): 0.061, 0.090
batch losses (mrrl, rdl): 0.0012425238, 0.0002306298

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 935
rank avg (pred): 0.494 +- 0.304
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0029589795, 5.4839e-06

Epoch over!
epoch time: 15.076

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 429
rank avg (pred): 0.487 +- 0.358
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.81347e-05, 3.7876e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 123
rank avg (pred): 0.433 +- 0.311
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 6.10523e-05, 4.45095e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 396
rank avg (pred): 0.531 +- 0.346
mrr vals (pred, true): 0.038, 0.002
batch losses (mrrl, rdl): 0.0014309676, 2.58597e-05

Epoch over!
epoch time: 15.131

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 180
rank avg (pred): 0.475 +- 0.356
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.11088e-05, 2.51479e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 104
rank avg (pred): 0.500 +- 0.349
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002183408, 4.3125e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 732
rank avg (pred): 0.178 +- 0.322
mrr vals (pred, true): 0.332, 0.342
batch losses (mrrl, rdl): 0.0009528247, 0.0002339089

Epoch over!
epoch time: 15.156

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 340
rank avg (pred): 0.477 +- 0.322
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.81967e-05, 7.4876e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 331
rank avg (pred): 0.481 +- 0.322
mrr vals (pred, true): 0.060, 0.002
batch losses (mrrl, rdl): 0.0010012733, 8.4807e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1113
rank avg (pred): 0.522 +- 0.329
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 3.5709e-06, 3.25218e-05

Epoch over!
epoch time: 15.114

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 744
rank avg (pred): 0.225 +- 0.344
mrr vals (pred, true): 0.238, 0.206
batch losses (mrrl, rdl): 0.0099787628, 2.76516e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 377
rank avg (pred): 0.449 +- 0.263
mrr vals (pred, true): 0.037, 0.001
batch losses (mrrl, rdl): 0.001772167, 2.91857e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 987
rank avg (pred): 0.283 +- 0.302
mrr vals (pred, true): 0.144, 0.129
batch losses (mrrl, rdl): 0.0020865821, 5.2862e-06

Epoch over!
epoch time: 15.103

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 731
rank avg (pred): 0.193 +- 0.290
mrr vals (pred, true): 0.328, 0.318
batch losses (mrrl, rdl): 0.0010480122, 8.09041e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 184
rank avg (pred): 0.483 +- 0.340
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0007654006, 1.77547e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.497 +- 0.324
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.4799e-06, 1.27176e-05

Epoch over!
epoch time: 15.133

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.466 +- 0.293
mrr vals (pred, true): 0.042, 0.064

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.02956 	 0.00048 	 ~...
   88 	     1 	 0.04366 	 0.00050 	 m..s
   13 	     2 	 0.03392 	 0.00051 	 m..s
   40 	     3 	 0.04110 	 0.00051 	 m..s
   26 	     4 	 0.03947 	 0.00053 	 m..s
   38 	     5 	 0.04105 	 0.00053 	 m..s
   75 	     6 	 0.04217 	 0.00054 	 m..s
   19 	     7 	 0.03521 	 0.00055 	 m..s
   60 	     8 	 0.04180 	 0.00056 	 m..s
   93 	     9 	 0.04537 	 0.00056 	 m..s
   85 	    10 	 0.04343 	 0.00056 	 m..s
   65 	    11 	 0.04193 	 0.00057 	 m..s
   15 	    12 	 0.03401 	 0.00058 	 m..s
   49 	    13 	 0.04133 	 0.00058 	 m..s
   21 	    14 	 0.03867 	 0.00058 	 m..s
   11 	    15 	 0.03370 	 0.00060 	 m..s
   62 	    16 	 0.04187 	 0.00061 	 m..s
   12 	    17 	 0.03388 	 0.00063 	 m..s
   34 	    18 	 0.04088 	 0.00064 	 m..s
   81 	    19 	 0.04311 	 0.00064 	 m..s
   28 	    20 	 0.04022 	 0.00064 	 m..s
    1 	    21 	 0.02898 	 0.00065 	 ~...
   82 	    22 	 0.04324 	 0.00068 	 m..s
   43 	    23 	 0.04119 	 0.00069 	 m..s
   80 	    24 	 0.04309 	 0.00069 	 m..s
   63 	    25 	 0.04188 	 0.00071 	 m..s
   22 	    26 	 0.03892 	 0.00072 	 m..s
    7 	    27 	 0.03302 	 0.00072 	 m..s
   20 	    28 	 0.03841 	 0.00073 	 m..s
   54 	    29 	 0.04164 	 0.00075 	 m..s
   24 	    30 	 0.03933 	 0.00075 	 m..s
   68 	    31 	 0.04196 	 0.00076 	 m..s
   18 	    32 	 0.03477 	 0.00077 	 m..s
   36 	    33 	 0.04099 	 0.00078 	 m..s
   89 	    34 	 0.04373 	 0.00078 	 m..s
   52 	    35 	 0.04157 	 0.00078 	 m..s
   94 	    36 	 0.04561 	 0.00079 	 m..s
   53 	    37 	 0.04163 	 0.00080 	 m..s
    6 	    38 	 0.03296 	 0.00080 	 m..s
   68 	    39 	 0.04196 	 0.00081 	 m..s
   68 	    40 	 0.04196 	 0.00082 	 m..s
   67 	    41 	 0.04195 	 0.00082 	 m..s
   95 	    42 	 0.04633 	 0.00084 	 m..s
   23 	    43 	 0.03900 	 0.00085 	 m..s
   87 	    44 	 0.04348 	 0.00090 	 m..s
   30 	    45 	 0.04041 	 0.00093 	 m..s
   25 	    46 	 0.03939 	 0.00094 	 m..s
   47 	    47 	 0.04127 	 0.00095 	 m..s
    0 	    48 	 0.02869 	 0.00097 	 ~...
   31 	    49 	 0.04047 	 0.00097 	 m..s
    4 	    50 	 0.03260 	 0.00098 	 m..s
    9 	    51 	 0.03336 	 0.00098 	 m..s
   44 	    52 	 0.04119 	 0.00102 	 m..s
   66 	    53 	 0.04195 	 0.00102 	 m..s
   99 	    54 	 0.04720 	 0.00103 	 m..s
   77 	    55 	 0.04238 	 0.00103 	 m..s
   78 	    56 	 0.04258 	 0.00103 	 m..s
    3 	    57 	 0.03245 	 0.00105 	 m..s
  105 	    58 	 0.05085 	 0.00105 	 m..s
   14 	    59 	 0.03393 	 0.00112 	 m..s
   17 	    60 	 0.03474 	 0.00115 	 m..s
   61 	    61 	 0.04185 	 0.00123 	 m..s
   57 	    62 	 0.04175 	 0.00125 	 m..s
   29 	    63 	 0.04032 	 0.00127 	 m..s
   83 	    64 	 0.04333 	 0.00130 	 m..s
   86 	    65 	 0.04344 	 0.00135 	 m..s
   96 	    66 	 0.04654 	 0.00144 	 m..s
   68 	    67 	 0.04196 	 0.00146 	 m..s
   46 	    68 	 0.04123 	 0.00148 	 m..s
    8 	    69 	 0.03316 	 0.00150 	 m..s
    5 	    70 	 0.03293 	 0.00151 	 m..s
   50 	    71 	 0.04136 	 0.00153 	 m..s
   64 	    72 	 0.04190 	 0.00159 	 m..s
   10 	    73 	 0.03367 	 0.00176 	 m..s
   33 	    74 	 0.04071 	 0.00186 	 m..s
   16 	    75 	 0.03411 	 0.00186 	 m..s
   51 	    76 	 0.04149 	 0.00188 	 m..s
   91 	    77 	 0.04401 	 0.00198 	 m..s
   32 	    78 	 0.04066 	 0.00221 	 m..s
   27 	    79 	 0.03995 	 0.00227 	 m..s
   79 	    80 	 0.04271 	 0.00230 	 m..s
   90 	    81 	 0.04373 	 0.00245 	 m..s
   37 	    82 	 0.04102 	 0.00248 	 m..s
   92 	    83 	 0.04420 	 0.00256 	 m..s
   84 	    84 	 0.04337 	 0.00283 	 m..s
  106 	    85 	 0.06089 	 0.01712 	 m..s
   68 	    86 	 0.04196 	 0.06381 	 ~...
   55 	    87 	 0.04166 	 0.06384 	 ~...
   56 	    88 	 0.04174 	 0.06496 	 ~...
   98 	    89 	 0.04718 	 0.06915 	 ~...
   74 	    90 	 0.04214 	 0.07087 	 ~...
   39 	    91 	 0.04106 	 0.07189 	 m..s
  114 	    92 	 0.12427 	 0.07201 	 m..s
   41 	    93 	 0.04110 	 0.07213 	 m..s
   42 	    94 	 0.04118 	 0.07228 	 m..s
   97 	    95 	 0.04662 	 0.07254 	 ~...
   45 	    96 	 0.04121 	 0.07514 	 m..s
   58 	    97 	 0.04175 	 0.07611 	 m..s
   48 	    98 	 0.04128 	 0.07648 	 m..s
   68 	    99 	 0.04196 	 0.07709 	 m..s
  101 	   100 	 0.04799 	 0.07894 	 m..s
   35 	   101 	 0.04090 	 0.07936 	 m..s
   76 	   102 	 0.04222 	 0.07943 	 m..s
   59 	   103 	 0.04175 	 0.08144 	 m..s
  104 	   104 	 0.05080 	 0.08403 	 m..s
  100 	   105 	 0.04749 	 0.08565 	 m..s
  103 	   106 	 0.05030 	 0.10101 	 m..s
  102 	   107 	 0.04900 	 0.10497 	 m..s
  107 	   108 	 0.07851 	 0.12476 	 m..s
  109 	   109 	 0.08500 	 0.12682 	 m..s
  113 	   110 	 0.12417 	 0.13938 	 ~...
  108 	   111 	 0.08212 	 0.14208 	 m..s
  118 	   112 	 0.14859 	 0.16706 	 ~...
  119 	   113 	 0.15615 	 0.16814 	 ~...
  117 	   114 	 0.13783 	 0.17068 	 m..s
  110 	   115 	 0.10830 	 0.18508 	 m..s
  115 	   116 	 0.12717 	 0.18649 	 m..s
  116 	   117 	 0.13189 	 0.19419 	 m..s
  111 	   118 	 0.11506 	 0.26070 	 MISS
  112 	   119 	 0.12400 	 0.27955 	 MISS
  120 	   120 	 0.24349 	 0.28260 	 m..s
==========================================
r_mrr = 0.8342933654785156
r2_mrr = 0.5168883800506592
spearmanr_mrr@5 = 0.6640165448188782
spearmanr_mrr@10 = 0.7860488891601562
spearmanr_mrr@50 = 0.902582585811615
spearmanr_mrr@100 = 0.9011791944503784
spearmanr_mrr@All = 0.9072730541229248
==========================================
test time: 0.449
Done Testing dataset OpenEA
total time taken: 253.70173692703247
training time taken: 227.2846212387085
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'OpenEA': tensor(0.8343)}}, 'r2_mrr': {'ComplEx': {'OpenEA': tensor(0.5169)}}, 'spearmanr_mrr@5': {'ComplEx': {'OpenEA': tensor(0.6640)}}, 'spearmanr_mrr@10': {'ComplEx': {'OpenEA': tensor(0.7860)}}, 'spearmanr_mrr@50': {'ComplEx': {'OpenEA': tensor(0.9026)}}, 'spearmanr_mrr@100': {'ComplEx': {'OpenEA': tensor(0.9012)}}, 'spearmanr_mrr@All': {'ComplEx': {'OpenEA': tensor(0.9073)}}, 'test_loss': {'ComplEx': {'OpenEA': 0.9302318305199151}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 6356540888285267
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [769, 87, 592, 230, 665, 1117, 224, 1005, 924, 791, 1012, 25, 774, 741, 640, 1177, 676, 1175, 765, 469, 528, 39, 597, 1022, 115, 326, 440, 1089, 529, 1183, 4, 312, 1196, 576, 796, 546, 1108, 667, 1209, 617, 797, 746, 1140, 684, 995, 79, 188, 879, 27, 954, 1098, 1194, 623, 400, 424, 549, 764, 443, 779, 457, 490, 52, 955, 1066, 75, 985, 1006, 466, 1199, 289, 514, 706, 1195, 536, 92, 286, 887, 273, 480, 107, 788, 1128, 1214, 1122, 383, 1141, 385, 866, 541, 63, 532, 988, 239, 705, 1114, 896, 288, 1028, 1021, 975, 904, 609, 979, 453, 1146, 23, 1189, 1192, 262, 1008, 1020, 365, 714, 743, 504, 1084, 956, 118, 1061, 409, 448]
valid_ids (0): []
train_ids (1094): [276, 434, 1001, 538, 176, 957, 1166, 876, 53, 812, 168, 712, 161, 477, 451, 50, 733, 1212, 817, 936, 732, 611, 895, 1124, 1045, 635, 557, 1015, 859, 49, 583, 654, 467, 475, 923, 547, 297, 938, 252, 153, 622, 513, 946, 629, 739, 468, 15, 870, 185, 311, 16, 67, 343, 405, 548, 917, 878, 348, 74, 329, 174, 442, 666, 159, 108, 352, 331, 318, 726, 539, 1031, 959, 54, 47, 994, 1011, 540, 175, 1184, 1135, 89, 869, 757, 1046, 431, 200, 403, 197, 621, 523, 134, 64, 411, 822, 221, 700, 679, 905, 38, 939, 99, 35, 1118, 2, 776, 287, 1126, 958, 626, 551, 1, 1208, 398, 624, 800, 1093, 1151, 120, 1064, 196, 152, 685, 57, 178, 659, 429, 246, 897, 270, 966, 642, 997, 987, 1203, 267, 31, 494, 132, 638, 608, 1013, 484, 1123, 688, 497, 760, 110, 802, 669, 692, 1065, 643, 1047, 610, 521, 853, 444, 641, 96, 963, 1129, 360, 682, 1127, 750, 708, 1072, 186, 275, 821, 207, 281, 543, 1018, 804, 890, 1168, 898, 686, 689, 696, 672, 116, 1039, 1063, 1137, 1092, 157, 124, 103, 1100, 845, 1102, 1073, 423, 601, 272, 7, 978, 908, 603, 208, 104, 562, 412, 170, 1054, 1088, 839, 71, 1144, 868, 880, 893, 1167, 217, 264, 1062, 903, 965, 314, 1139, 40, 41, 293, 485, 282, 785, 742, 888, 234, 202, 1048, 449, 699, 1033, 1101, 581, 921, 430, 478, 225, 509, 974, 1105, 187, 674, 852, 336, 154, 740, 783, 930, 832, 1017, 810, 70, 1174, 268, 618, 1076, 518, 274, 1049, 369, 86, 941, 968, 932, 427, 826, 1121, 407, 775, 515, 1052, 1103, 929, 32, 292, 399, 1023, 579, 697, 66, 198, 359, 1142, 687, 269, 58, 373, 259, 1016, 702, 149, 323, 1145, 1120, 990, 195, 761, 462, 588, 19, 980, 1010, 219, 823, 1171, 942, 169, 203, 872, 1160, 432, 371, 856, 633, 349, 607, 565, 1155, 193, 1024, 1032, 848, 998, 829, 127, 752, 94, 1068, 945, 871, 1115, 310, 280, 709, 144, 332, 820, 652, 28, 495, 46, 439, 340, 486, 194, 177, 338, 33, 793, 719, 837, 366, 704, 222, 111, 693, 544, 350, 61, 698, 450, 465, 260, 136, 1173, 566, 671, 662, 1202, 782, 503, 807, 648, 1119, 631, 1161, 483, 656, 1110, 805, 59, 1143, 73, 780, 1069, 236, 1111, 1050, 999, 106, 1083, 561, 126, 925, 510, 842, 660, 794, 150, 381, 456, 716, 237, 316, 1136, 747, 458, 62, 482, 211, 394, 408, 657, 506, 137, 934, 192, 1152, 858, 818, 755, 645, 902, 651, 795, 964, 996, 410, 605, 1037, 786, 390, 1132, 1188, 567, 113, 637, 906, 296, 119, 675, 128, 320, 319, 644, 498, 860, 512, 231, 291, 758, 309, 838, 1026, 1204, 83, 44, 962, 68, 537, 8, 210, 604, 1014, 885, 1138, 1116, 1200, 1112, 78, 1190, 1198, 613, 285, 492, 1162, 971, 900, 522, 1034, 806, 1091, 1193, 294, 668, 139, 572, 1164, 241, 322, 460, 731, 146, 48, 1019, 1109, 815, 591, 505, 80, 745, 1056, 1071, 171, 535, 76, 101, 554, 673, 1150, 22, 375, 18, 650, 803, 166, 341, 681, 690, 91, 813, 232, 756, 727, 213, 553, 725, 940, 29, 147, 777, 1169, 722, 874, 694, 163, 1213, 26, 970, 1125, 701, 416, 358, 121, 1082, 125, 388, 370, 393, 1029, 1148, 661, 531, 599, 972, 335, 1009, 9, 798, 827, 949, 162, 730, 738, 206, 754, 784, 265, 14, 455, 873, 778, 1070, 952, 11, 209, 695, 406, 81, 301, 13, 257, 151, 441, 768, 459, 1149, 620, 658, 1179, 397, 1085, 347, 43, 402, 736, 333, 525, 315, 428, 167, 391, 636, 911, 916, 1003, 418, 619, 243, 436, 1186, 317, 6, 1051, 865, 248, 1187, 474, 36, 364, 1159, 17, 568, 10, 596, 723, 830, 993, 573, 950, 1131, 834, 395, 960, 363, 122, 1079, 586, 500, 533, 1035, 435, 901, 711, 182, 133, 487, 117, 1055, 244, 1201, 463, 1163, 508, 984, 1004, 922, 598, 284, 550, 563, 88, 307, 184, 1205, 417, 678, 261, 1007, 446, 367, 142, 479, 472, 24, 615, 892, 824, 251, 828, 308, 843, 361, 910, 1206, 772, 238, 799, 851, 437, 520, 664, 471, 863, 720, 977, 909, 1030, 634, 1067, 787, 218, 156, 299, 1095, 992, 967, 1025, 578, 1059, 158, 809, 321, 425, 100, 191, 933, 919, 683, 969, 511, 496, 889, 1096, 254, 339, 594, 875, 519, 862, 630, 374, 721, 1157, 45, 707, 42, 216, 1113, 491, 789, 499, 1147, 21, 526, 1087, 190, 951, 65, 1133, 600, 1000, 105, 90, 362, 585, 891, 713, 849, 833, 212, 372, 214, 98, 12, 593, 556, 324, 376, 734, 907, 438, 899, 691, 915, 173, 0, 220, 447, 476, 344, 247, 131, 180, 844, 249, 229, 918, 846, 790, 1053, 72, 302, 84, 155, 392, 728, 781, 886, 1042, 534, 189, 138, 290, 1038, 612, 1058, 857, 751, 748, 953, 337, 1107, 753, 976, 710, 85, 882, 847, 183, 1043, 1078, 555, 1158, 773, 34, 1207, 545, 914, 627, 564, 368, 422, 3, 488, 181, 628, 715, 836, 1002, 819, 384, 574, 461, 452, 677, 172, 334, 112, 811, 1074, 1156, 517, 655, 855, 766, 1027, 1106, 1060, 670, 935, 396, 379, 145, 353, 841, 552, 271, 912, 129, 524, 135, 1077, 413, 481, 215, 380, 1170, 542, 864, 123, 983, 30, 737, 1176, 831, 445, 95, 881, 1134, 1075, 279, 840, 1090, 767, 724, 258, 861, 278, 386, 305, 808, 1094, 926, 357, 303, 947, 404, 298, 1036, 614, 1081, 735, 931, 93, 328, 342, 382, 606, 351, 235, 419, 266, 60, 1181, 82, 346, 1130, 420, 470, 516, 894, 304, 632, 749, 825, 454, 927, 102, 587, 421, 164, 1172, 356, 5, 464, 595, 165, 1154, 616, 944, 507, 256, 109, 639, 414, 1191, 255, 729, 1080, 228, 703, 245, 816, 1211, 354, 501, 570, 1099, 663, 493, 199, 489, 130, 647, 577, 401, 883, 204, 961, 771, 1104, 530, 575, 560, 1057, 283, 625, 226, 233, 989, 884, 377, 1182, 680, 143, 559, 1153, 973, 242, 649, 502, 814, 646, 928, 1097, 51, 355, 913, 1178, 602, 295, 69, 55, 584, 97, 300, 205, 948, 867, 763, 569, 1165, 378, 433, 1210, 415, 991, 1040, 1197, 982, 943, 327, 240, 387, 1180, 56, 571, 877, 77, 770, 762, 141, 179, 37, 792, 201, 1185, 1041, 114, 835, 20, 653, 313, 580, 850, 250, 854, 426, 330, 717, 590, 473, 160, 263, 718, 759, 148, 253, 1044, 801, 277, 345, 527, 558, 389, 589, 920, 582, 1086, 981, 937, 306, 227, 140, 223, 744, 325, 986]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9065006184699382
the save name prefix for this run is:  chkpt-ID_9065006184699382_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 944
rank avg (pred): 0.552 +- 0.007
mrr vals (pred, true): 0.013, 0.030
batch losses (mrrl, rdl): 0.0, 0.0001145187

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1056
rank avg (pred): 0.129 +- 0.086
mrr vals (pred, true): 0.207, 0.290
batch losses (mrrl, rdl): 0.0, 2.85705e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 630
rank avg (pred): 0.447 +- 0.247
mrr vals (pred, true): 0.089, 0.043
batch losses (mrrl, rdl): 0.0, 4.5915e-06

Epoch over!
epoch time: 15.058

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 137
rank avg (pred): 0.426 +- 0.252
mrr vals (pred, true): 0.093, 0.048
batch losses (mrrl, rdl): 0.0, 1.14026e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 849
rank avg (pred): 0.435 +- 0.259
mrr vals (pred, true): 0.089, 0.044
batch losses (mrrl, rdl): 0.0, 8.3325e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 264
rank avg (pred): 0.194 +- 0.136
mrr vals (pred, true): 0.149, 0.171
batch losses (mrrl, rdl): 0.0, 3.06264e-05

Epoch over!
epoch time: 14.95

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 151
rank avg (pred): 0.437 +- 0.260
mrr vals (pred, true): 0.075, 0.049
batch losses (mrrl, rdl): 0.0, 1.09692e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 277
rank avg (pred): 0.162 +- 0.190
mrr vals (pred, true): 0.163, 0.260
batch losses (mrrl, rdl): 0.0, 8.6583e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 374
rank avg (pred): 0.429 +- 0.259
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 0.0, 1.1772e-06

Epoch over!
epoch time: 14.987

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 442
rank avg (pred): 0.442 +- 0.256
mrr vals (pred, true): 0.041, 0.047
batch losses (mrrl, rdl): 0.0, 4.0857e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1083
rank avg (pred): 0.413 +- 0.272
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 0.0, 1.72637e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 675
rank avg (pred): 0.450 +- 0.260
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0, 1.6605e-06

Epoch over!
epoch time: 15.04

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1127
rank avg (pred): 0.441 +- 0.272
mrr vals (pred, true): 0.057, 0.052
batch losses (mrrl, rdl): 0.0, 3.8656e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 864
rank avg (pred): 0.443 +- 0.265
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 0.0, 1.2092e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 615
rank avg (pred): 0.416 +- 0.266
mrr vals (pred, true): 0.080, 0.041
batch losses (mrrl, rdl): 0.0, 2.26202e-05

Epoch over!
epoch time: 15.09

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 296
rank avg (pred): 0.200 +- 0.217
mrr vals (pred, true): 0.142, 0.215
batch losses (mrrl, rdl): 0.0523348227, 7.236e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 914
rank avg (pred): 0.454 +- 0.168
mrr vals (pred, true): 0.053, 0.021
batch losses (mrrl, rdl): 0.0001010985, 0.0002903655

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1109
rank avg (pred): 0.447 +- 0.201
mrr vals (pred, true): 0.060, 0.044
batch losses (mrrl, rdl): 0.001055893, 2.066e-05

Epoch over!
epoch time: 15.297

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 189
rank avg (pred): 0.482 +- 0.191
mrr vals (pred, true): 0.039, 0.043
batch losses (mrrl, rdl): 0.001149045, 4.63791e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1034
rank avg (pred): 0.478 +- 0.190
mrr vals (pred, true): 0.027, 0.042
batch losses (mrrl, rdl): 0.0055179643, 3.04562e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1171
rank avg (pred): 0.487 +- 0.172
mrr vals (pred, true): 0.035, 0.035
batch losses (mrrl, rdl): 0.0022837878, 2.8755e-05

Epoch over!
epoch time: 15.265

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 712
rank avg (pred): 0.466 +- 0.186
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 7.26091e-05, 4.90556e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 305
rank avg (pred): 0.078 +- 0.097
mrr vals (pred, true): 0.246, 0.249
batch losses (mrrl, rdl): 9.59472e-05, 9.96191e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 192
rank avg (pred): 0.507 +- 0.219
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.000126654, 0.0001226778

Epoch over!
epoch time: 15.264

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 773
rank avg (pred): 0.502 +- 0.224
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 5e-10, 9.68052e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 280
rank avg (pred): 0.087 +- 0.094
mrr vals (pred, true): 0.241, 0.223
batch losses (mrrl, rdl): 0.0034243308, 0.0001891956

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 474
rank avg (pred): 0.491 +- 0.204
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 4.07923e-05, 0.000115218

Epoch over!
epoch time: 15.266

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1
rank avg (pred): 0.099 +- 0.122
mrr vals (pred, true): 0.232, 0.241
batch losses (mrrl, rdl): 0.0009078754, 7.95993e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 384
rank avg (pred): 0.514 +- 0.192
mrr vals (pred, true): 0.031, 0.044
batch losses (mrrl, rdl): 0.003607065, 0.0001117284

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1206
rank avg (pred): 0.483 +- 0.182
mrr vals (pred, true): 0.037, 0.046
batch losses (mrrl, rdl): 0.0016175263, 5.2541e-05

Epoch over!
epoch time: 15.273

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 728
rank avg (pred): 0.477 +- 0.200
mrr vals (pred, true): 0.045, 0.046
batch losses (mrrl, rdl): 0.0002995355, 3.65833e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1207
rank avg (pred): 0.470 +- 0.200
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.95849e-05, 2.59797e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 307
rank avg (pred): 0.177 +- 0.213
mrr vals (pred, true): 0.212, 0.219
batch losses (mrrl, rdl): 0.0004364236, 3.9697e-06

Epoch over!
epoch time: 15.21

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 826
rank avg (pred): 0.129 +- 0.200
mrr vals (pred, true): 0.255, 0.245
batch losses (mrrl, rdl): 0.0010666163, 5.28003e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 339
rank avg (pred): 0.514 +- 0.218
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 8.9313e-06, 0.0001914685

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 237
rank avg (pred): 0.466 +- 0.198
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 9.0103e-06, 3.07918e-05

Epoch over!
epoch time: 15.145

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 250
rank avg (pred): 0.150 +- 0.214
mrr vals (pred, true): 0.222, 0.243
batch losses (mrrl, rdl): 0.0044911597, 4.8094e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 133
rank avg (pred): 0.457 +- 0.194
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 7.49158e-05, 3.08144e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.472 +- 0.186
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0002586301, 4.06399e-05

Epoch over!
epoch time: 15.094

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 235
rank avg (pred): 0.464 +- 0.207
mrr vals (pred, true): 0.054, 0.046
batch losses (mrrl, rdl): 0.0001330721, 3.52807e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 844
rank avg (pred): 0.446 +- 0.200
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 0.0001133377, 1.31197e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 394
rank avg (pred): 0.460 +- 0.198
mrr vals (pred, true): 0.055, 0.056
batch losses (mrrl, rdl): 0.0002069849, 2.64406e-05

Epoch over!
epoch time: 15.045

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 844
rank avg (pred): 0.480 +- 0.185
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003390756, 3.52234e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 342
rank avg (pred): 0.454 +- 0.201
mrr vals (pred, true): 0.057, 0.043
batch losses (mrrl, rdl): 0.0005339329, 3.27893e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 909
rank avg (pred): 0.445 +- 0.199
mrr vals (pred, true): 0.058, 0.023
batch losses (mrrl, rdl): 0.0006205974, 0.0002612705

Epoch over!
epoch time: 15.087

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.469 +- 0.198
mrr vals (pred, true): 0.053, 0.047

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   14 	     0 	 0.04639 	 0.02111 	 ~...
   40 	     1 	 0.04914 	 0.02204 	 ~...
   17 	     2 	 0.04677 	 0.02263 	 ~...
   13 	     3 	 0.04600 	 0.02302 	 ~...
   21 	     4 	 0.04724 	 0.02319 	 ~...
   54 	     5 	 0.05129 	 0.02342 	 ~...
    2 	     6 	 0.04230 	 0.02428 	 ~...
    3 	     7 	 0.04230 	 0.02442 	 ~...
    4 	     8 	 0.04296 	 0.02459 	 ~...
    1 	     9 	 0.04209 	 0.02481 	 ~...
    7 	    10 	 0.04398 	 0.02497 	 ~...
    0 	    11 	 0.04181 	 0.02497 	 ~...
   10 	    12 	 0.04527 	 0.02530 	 ~...
    5 	    13 	 0.04329 	 0.02586 	 ~...
   33 	    14 	 0.04804 	 0.02626 	 ~...
   18 	    15 	 0.04701 	 0.03016 	 ~...
   52 	    16 	 0.05111 	 0.03125 	 ~...
   41 	    17 	 0.04920 	 0.03405 	 ~...
   32 	    18 	 0.04780 	 0.03500 	 ~...
   16 	    19 	 0.04671 	 0.03500 	 ~...
   34 	    20 	 0.04823 	 0.03523 	 ~...
   57 	    21 	 0.05145 	 0.03682 	 ~...
   38 	    22 	 0.04913 	 0.03929 	 ~...
   53 	    23 	 0.05114 	 0.03930 	 ~...
   48 	    24 	 0.05024 	 0.04020 	 ~...
   64 	    25 	 0.05254 	 0.04036 	 ~...
   87 	    26 	 0.05475 	 0.04053 	 ~...
   90 	    27 	 0.05565 	 0.04106 	 ~...
   23 	    28 	 0.04727 	 0.04147 	 ~...
   47 	    29 	 0.05023 	 0.04176 	 ~...
   69 	    30 	 0.05458 	 0.04221 	 ~...
   94 	    31 	 0.05863 	 0.04238 	 ~...
   15 	    32 	 0.04640 	 0.04255 	 ~...
   42 	    33 	 0.04947 	 0.04258 	 ~...
   93 	    34 	 0.05779 	 0.04260 	 ~...
   50 	    35 	 0.05053 	 0.04260 	 ~...
   69 	    36 	 0.05458 	 0.04261 	 ~...
   46 	    37 	 0.05017 	 0.04279 	 ~...
   88 	    38 	 0.05565 	 0.04301 	 ~...
   58 	    39 	 0.05154 	 0.04302 	 ~...
   92 	    40 	 0.05584 	 0.04308 	 ~...
   25 	    41 	 0.04749 	 0.04319 	 ~...
   22 	    42 	 0.04725 	 0.04337 	 ~...
   69 	    43 	 0.05458 	 0.04338 	 ~...
   69 	    44 	 0.05458 	 0.04369 	 ~...
   88 	    45 	 0.05565 	 0.04393 	 ~...
   43 	    46 	 0.05011 	 0.04397 	 ~...
   66 	    47 	 0.05254 	 0.04410 	 ~...
   69 	    48 	 0.05458 	 0.04460 	 ~...
   11 	    49 	 0.04575 	 0.04512 	 ~...
   69 	    50 	 0.05458 	 0.04531 	 ~...
   59 	    51 	 0.05160 	 0.04533 	 ~...
   27 	    52 	 0.04771 	 0.04586 	 ~...
   68 	    53 	 0.05351 	 0.04592 	 ~...
   29 	    54 	 0.04772 	 0.04613 	 ~...
   30 	    55 	 0.04772 	 0.04614 	 ~...
   69 	    56 	 0.05458 	 0.04642 	 ~...
   20 	    57 	 0.04712 	 0.04669 	 ~...
   69 	    58 	 0.05458 	 0.04690 	 ~...
   31 	    59 	 0.04776 	 0.04712 	 ~...
   24 	    60 	 0.04749 	 0.04718 	 ~...
   64 	    61 	 0.05254 	 0.04727 	 ~...
   44 	    62 	 0.05013 	 0.04747 	 ~...
   86 	    63 	 0.05462 	 0.04763 	 ~...
   27 	    64 	 0.04771 	 0.04816 	 ~...
   45 	    65 	 0.05014 	 0.04851 	 ~...
   91 	    66 	 0.05575 	 0.04852 	 ~...
   26 	    67 	 0.04753 	 0.04865 	 ~...
    9 	    68 	 0.04489 	 0.04866 	 ~...
   61 	    69 	 0.05183 	 0.04871 	 ~...
   38 	    70 	 0.04913 	 0.04876 	 ~...
   60 	    71 	 0.05169 	 0.04890 	 ~...
   69 	    72 	 0.05458 	 0.04951 	 ~...
   51 	    73 	 0.05104 	 0.04953 	 ~...
   55 	    74 	 0.05138 	 0.04961 	 ~...
   69 	    75 	 0.05458 	 0.04974 	 ~...
   36 	    76 	 0.04840 	 0.04984 	 ~...
   69 	    77 	 0.05458 	 0.05025 	 ~...
   69 	    78 	 0.05458 	 0.05051 	 ~...
   62 	    79 	 0.05225 	 0.05069 	 ~...
   35 	    80 	 0.04829 	 0.05091 	 ~...
   49 	    81 	 0.05045 	 0.05144 	 ~...
   56 	    82 	 0.05145 	 0.05162 	 ~...
    8 	    83 	 0.04436 	 0.05223 	 ~...
   69 	    84 	 0.05458 	 0.05233 	 ~...
   69 	    85 	 0.05458 	 0.05235 	 ~...
   37 	    86 	 0.04909 	 0.05246 	 ~...
   62 	    87 	 0.05225 	 0.05272 	 ~...
   69 	    88 	 0.05458 	 0.05275 	 ~...
   67 	    89 	 0.05350 	 0.05296 	 ~...
   69 	    90 	 0.05458 	 0.05324 	 ~...
   12 	    91 	 0.04590 	 0.05374 	 ~...
   69 	    92 	 0.05458 	 0.05448 	 ~...
    6 	    93 	 0.04381 	 0.05450 	 ~...
   95 	    94 	 0.05993 	 0.05647 	 ~...
   19 	    95 	 0.04710 	 0.05814 	 ~...
   99 	    96 	 0.20702 	 0.18959 	 ~...
   98 	    97 	 0.20585 	 0.19872 	 ~...
  104 	    98 	 0.22395 	 0.20165 	 ~...
   96 	    99 	 0.19662 	 0.20277 	 ~...
   97 	   100 	 0.19672 	 0.20350 	 ~...
  106 	   101 	 0.23010 	 0.20501 	 ~...
  101 	   102 	 0.21571 	 0.20603 	 ~...
  102 	   103 	 0.21731 	 0.20614 	 ~...
  103 	   104 	 0.22033 	 0.21089 	 ~...
  107 	   105 	 0.23162 	 0.21298 	 ~...
  112 	   106 	 0.24517 	 0.21610 	 ~...
  105 	   107 	 0.22717 	 0.21817 	 ~...
  100 	   108 	 0.21515 	 0.22157 	 ~...
  110 	   109 	 0.23914 	 0.23984 	 ~...
  108 	   110 	 0.23360 	 0.24218 	 ~...
  109 	   111 	 0.23604 	 0.24445 	 ~...
  119 	   112 	 0.28754 	 0.25916 	 ~...
  115 	   113 	 0.26955 	 0.26624 	 ~...
  118 	   114 	 0.28750 	 0.27773 	 ~...
  114 	   115 	 0.26726 	 0.28034 	 ~...
  117 	   116 	 0.28212 	 0.28472 	 ~...
  113 	   117 	 0.26688 	 0.28862 	 ~...
  120 	   118 	 0.33921 	 0.29633 	 m..s
  111 	   119 	 0.24426 	 0.33485 	 m..s
  116 	   120 	 0.27949 	 0.33543 	 m..s
==========================================
r_mrr = 0.9851359724998474
r2_mrr = 0.9636072516441345
spearmanr_mrr@5 = 0.6842949986457825
spearmanr_mrr@10 = 0.8756165504455566
spearmanr_mrr@50 = 0.9957250356674194
spearmanr_mrr@100 = 0.9971563816070557
spearmanr_mrr@All = 0.9957758188247681
==========================================
test time: 0.461
Done Testing dataset UMLS
total time taken: 233.85920929908752
training time taken: 227.56286716461182
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9851)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9636)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.6843)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.8756)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9957)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9972)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9958)}}, 'test_loss': {'ComplEx': {'UMLS': 0.2030514016169036}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 5651539382329707
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [135, 1122, 1123, 336, 751, 733, 1099, 1015, 780, 15, 862, 909, 652, 296, 533, 396, 910, 86, 235, 1167, 831, 418, 468, 995, 547, 606, 69, 259, 35, 822, 1109, 654, 55, 101, 471, 271, 40, 596, 644, 840, 496, 9, 450, 1064, 994, 999, 949, 142, 324, 1039, 545, 122, 922, 874, 292, 210, 12, 317, 150, 81, 1210, 1211, 19, 456, 578, 811, 158, 1208, 532, 669, 841, 955, 729, 309, 356, 284, 881, 1028, 118, 600, 62, 427, 80, 60, 1044, 1108, 1200, 358, 1106, 382, 113, 244, 180, 375, 707, 104, 894, 583, 480, 693, 1130, 633, 1153, 944, 63, 683, 576, 624, 1095, 254, 160, 232, 936, 992, 925, 1023, 134, 307, 1119, 53, 98]
valid_ids (0): []
train_ids (1094): [732, 1129, 31, 898, 569, 1073, 331, 1075, 723, 68, 318, 447, 574, 7, 902, 1185, 764, 1021, 812, 207, 247, 77, 1030, 166, 655, 917, 686, 384, 139, 828, 455, 887, 861, 623, 304, 884, 1067, 1076, 835, 608, 719, 478, 827, 1152, 437, 1184, 137, 131, 406, 993, 90, 1175, 517, 904, 1118, 119, 85, 908, 1048, 1027, 1040, 738, 224, 800, 842, 241, 627, 511, 220, 954, 818, 38, 152, 1083, 873, 613, 523, 105, 335, 103, 757, 73, 796, 258, 1071, 813, 629, 591, 411, 233, 133, 273, 609, 168, 938, 385, 963, 626, 50, 327, 605, 374, 226, 1178, 529, 1002, 1029, 399, 33, 88, 978, 240, 1057, 865, 801, 515, 27, 1156, 986, 173, 449, 28, 880, 866, 1045, 689, 293, 93, 1018, 1173, 572, 1006, 863, 299, 489, 253, 1059, 465, 269, 748, 984, 741, 282, 48, 663, 414, 1159, 1125, 959, 246, 1144, 303, 473, 342, 37, 877, 516, 1035, 320, 1084, 567, 408, 407, 653, 1098, 99, 313, 188, 724, 937, 194, 16, 830, 262, 1046, 580, 337, 46, 321, 581, 488, 777, 616, 183, 193, 466, 551, 1212, 363, 648, 1198, 590, 250, 83, 675, 270, 537, 784, 893, 255, 369, 96, 394, 678, 283, 54, 1205, 1213, 798, 1072, 1056, 349, 839, 546, 965, 381, 360, 820, 1069, 117, 853, 837, 493, 467, 251, 620, 1082, 597, 25, 225, 1014, 632, 556, 367, 111, 575, 799, 1036, 343, 522, 485, 915, 430, 772, 577, 951, 859, 833, 649, 805, 17, 285, 639, 91, 461, 615, 933, 964, 768, 1116, 976, 1020, 962, 900, 746, 940, 990, 205, 1145, 229, 682, 409, 735, 291, 147, 997, 975, 75, 191, 804, 202, 870, 395, 787, 988, 792, 921, 212, 524, 1017, 743, 110, 501, 416, 958, 1147, 51, 67, 1160, 1024, 445, 508, 1157, 1139, 770, 345, 797, 680, 1050, 479, 647, 1037, 236, 1186, 643, 895, 330, 704, 595, 339, 1062, 4, 305, 346, 1007, 939, 361, 961, 112, 916, 94, 433, 593, 377, 472, 390, 1187, 29, 278, 889, 788, 114, 239, 1126, 1060, 1140, 1022, 1052, 1194, 991, 286, 222, 340, 720, 891, 469, 503, 539, 5, 3, 507, 477, 231, 1100, 121, 243, 209, 622, 351, 177, 747, 989, 957, 1000, 1199, 412, 857, 20, 1032, 423, 475, 1077, 594, 298, 116, 178, 540, 514, 758, 357, 1170, 228, 1009, 165, 359, 404, 932, 58, 876, 370, 727, 948, 341, 1074, 696, 132, 711, 66, 985, 364, 76, 1120, 846, 635, 1128, 1121, 1103, 734, 1143, 699, 599, 1171, 474, 332, 167, 334, 392, 753, 1, 120, 371, 448, 268, 181, 982, 1102, 1068, 301, 308, 72, 338, 603, 124, 943, 400, 878, 927, 500, 476, 1192, 26, 934, 1104, 294, 829, 314, 659, 497, 520, 728, 176, 1065, 171, 352, 1162, 143, 1207, 457, 190, 843, 1070, 435, 692, 519, 261, 879, 1081, 218, 1079, 710, 601, 149, 509, 434, 771, 151, 697, 996, 95, 534, 1190, 602, 754, 157, 1058, 163, 319, 774, 598, 950, 263, 806, 536, 24, 582, 946, 192, 326, 276, 204, 1114, 875, 868, 836, 6, 482, 755, 1203, 641, 779, 1165, 953, 1150, 36, 242, 460, 211, 184, 107, 1078, 92, 494, 323, 290, 1195, 353, 730, 1010, 667, 745, 1133, 295, 535, 1117, 1137, 199, 267, 589, 281, 570, 664, 454, 544, 931, 1209, 762, 79, 826, 808, 1026, 911, 981, 824, 956, 718, 1005, 1097, 417, 215, 297, 1166, 513, 817, 726, 967, 791, 694, 1158, 59, 484, 175, 402, 850, 636, 431, 1138, 775, 531, 558, 527, 1016, 39, 23, 89, 776, 851, 347, 860, 154, 49, 855, 1063, 803, 854, 737, 362, 703, 621, 453, 1182, 918, 715, 1176, 781, 760, 712, 156, 554, 844, 376, 1183, 773, 230, 125, 617, 492, 277, 1164, 530, 216, 538, 1149, 186, 750, 144, 568, 138, 611, 56, 1174, 969, 832, 896, 966, 420, 782, 637, 410, 196, 1001, 1169, 383, 483, 279, 213, 930, 252, 128, 1088, 783, 306, 161, 312, 84, 897, 1188, 552, 288, 713, 127, 555, 785, 354, 821, 487, 610, 1142, 333, 172, 634, 890, 740, 847, 1093, 426, 238, 973, 22, 765, 725, 1094, 260, 1003, 106, 1080, 695, 189, 825, 200, 1124, 1196, 565, 1177, 906, 656, 491, 650, 61, 266, 1148, 795, 329, 109, 245, 607, 506, 257, 1180, 662, 1089, 325, 563, 668, 618, 419, 155, 322, 365, 289, 71, 924, 838, 810, 794, 115, 405, 1112, 566, 219, 858, 1206, 1042, 767, 604, 510, 1115, 1155, 642, 78, 871, 214, 300, 0, 1091, 1054, 174, 438, 651, 1197, 1189, 793, 739, 612, 386, 550, 987, 1204, 677, 690, 901, 665, 972, 100, 1090, 945, 1151, 681, 145, 1013, 920, 548, 864, 311, 439, 398, 368, 1038, 481, 1055, 979, 108, 170, 1132, 350, 778, 882, 197, 1136, 1172, 907, 403, 32, 328, 815, 684, 21, 462, 18, 913, 802, 935, 1011, 1111, 470, 549, 823, 672, 561, 1201, 814, 912, 687, 153, 661, 1101, 717, 164, 942, 502, 899, 749, 499, 716, 44, 415, 658, 223, 756, 892, 670, 87, 185, 721, 968, 74, 960, 237, 441, 393, 130, 1066, 701, 452, 631, 187, 425, 42, 287, 126, 941, 1134, 1214, 769, 809, 742, 1043, 1191, 645, 424, 458, 97, 47, 444, 521, 8, 169, 34, 348, 10, 586, 217, 974, 736, 464, 705, 630, 1161, 43, 206, 234, 182, 459, 265, 807, 440, 1146, 355, 1163, 201, 162, 1047, 1096, 486, 790, 208, 413, 869, 141, 274, 1051, 302, 428, 14, 674, 422, 1110, 221, 679, 1033, 379, 1061, 366, 671, 971, 542, 528, 638, 573, 316, 280, 998, 387, 512, 1127, 136, 1019, 559, 272, 436, 660, 518, 429, 848, 947, 614, 1193, 905, 421, 700, 977, 983, 849, 1008, 446, 819, 1053, 625, 903, 786, 140, 845, 709, 706, 553, 65, 691, 564, 926, 432, 744, 179, 525, 557, 543, 1049, 761, 41, 45, 560, 463, 195, 1092, 1087, 685, 585, 579, 443, 588, 198, 584, 310, 722, 852, 13, 264, 1034, 248, 752, 708, 70, 391, 397, 1179, 203, 82, 698, 587, 541, 1141, 102, 619, 1113, 57, 759, 1168, 952, 628, 1012, 495, 249, 592, 872, 834, 1154, 646, 789, 378, 1086, 919, 816, 702, 315, 498, 1085, 30, 571, 1025, 344, 442, 1031, 867, 766, 372, 389, 123, 451, 562, 970, 64, 676, 888, 666, 52, 856, 980, 2, 688, 886, 256, 714, 148, 159, 380, 129, 401, 11, 640, 1131, 1105, 1135, 227, 388, 1181, 914, 929, 505, 885, 504, 1202, 146, 883, 731, 1107, 490, 373, 923, 673, 1004, 526, 1041, 928, 275, 657, 763]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2586741028841227
the save name prefix for this run is:  chkpt-ID_2586741028841227_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 816
rank avg (pred): 0.454 +- 0.003
mrr vals (pred, true): 0.016, 0.218
batch losses (mrrl, rdl): 0.0, 0.0017350203

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1030
rank avg (pred): 0.414 +- 0.043
mrr vals (pred, true): 0.018, 0.051
batch losses (mrrl, rdl): 0.0, 9.42106e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 415
rank avg (pred): 0.418 +- 0.268
mrr vals (pred, true): 0.165, 0.044
batch losses (mrrl, rdl): 0.0, 5.8807e-06

Epoch over!
epoch time: 15.057

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 448
rank avg (pred): 0.395 +- 0.265
mrr vals (pred, true): 0.187, 0.050
batch losses (mrrl, rdl): 0.0, 1.78839e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1103
rank avg (pred): 0.404 +- 0.274
mrr vals (pred, true): 0.203, 0.042
batch losses (mrrl, rdl): 0.0, 1.13403e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 531
rank avg (pred): 0.296 +- 0.188
mrr vals (pred, true): 0.180, 0.026
batch losses (mrrl, rdl): 0.0, 0.0008204562

Epoch over!
epoch time: 14.829

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 746
rank avg (pred): 0.278 +- 0.186
mrr vals (pred, true): 0.208, 0.242
batch losses (mrrl, rdl): 0.0, 0.0002572874

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 935
rank avg (pred): 0.409 +- 0.273
mrr vals (pred, true): 0.208, 0.027
batch losses (mrrl, rdl): 0.0, 0.000106548

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 199
rank avg (pred): 0.413 +- 0.278
mrr vals (pred, true): 0.219, 0.045
batch losses (mrrl, rdl): 0.0, 8.2836e-06

Epoch over!
epoch time: 14.86

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 253
rank avg (pred): 0.267 +- 0.190
mrr vals (pred, true): 0.244, 0.216
batch losses (mrrl, rdl): 0.0, 0.0002231223

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 318
rank avg (pred): 0.283 +- 0.201
mrr vals (pred, true): 0.242, 0.201
batch losses (mrrl, rdl): 0.0, 0.0001825074

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1082
rank avg (pred): 0.408 +- 0.272
mrr vals (pred, true): 0.216, 0.040
batch losses (mrrl, rdl): 0.0, 7.6244e-06

Epoch over!
epoch time: 14.988

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 828
rank avg (pred): 0.276 +- 0.205
mrr vals (pred, true): 0.274, 0.328
batch losses (mrrl, rdl): 0.0, 0.000584609

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 684
rank avg (pred): 0.415 +- 0.271
mrr vals (pred, true): 0.212, 0.052
batch losses (mrrl, rdl): 0.0, 5.843e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 11
rank avg (pred): 0.285 +- 0.215
mrr vals (pred, true): 0.273, 0.220
batch losses (mrrl, rdl): 0.0, 0.0003378271

Epoch over!
epoch time: 15.038

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 537
rank avg (pred): 0.268 +- 0.211
mrr vals (pred, true): 0.294, 0.024
batch losses (mrrl, rdl): 0.5964409709, 0.0009440994

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1136
rank avg (pred): 0.230 +- 0.122
mrr vals (pred, true): 0.160, 0.026
batch losses (mrrl, rdl): 0.1200160757, 0.0012134747

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1141
rank avg (pred): 0.201 +- 0.113
mrr vals (pred, true): 0.194, 0.023
batch losses (mrrl, rdl): 0.2070591599, 0.0016776299

Epoch over!
epoch time: 15.294

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 627
rank avg (pred): 0.479 +- 0.136
mrr vals (pred, true): 0.053, 0.038
batch losses (mrrl, rdl): 6.43212e-05, 4.41894e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 117
rank avg (pred): 0.488 +- 0.108
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 9.5129e-05, 9.13704e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 30
rank avg (pred): 0.307 +- 0.176
mrr vals (pred, true): 0.188, 0.248
batch losses (mrrl, rdl): 0.0362024158, 0.0004651369

Epoch over!
epoch time: 15.08

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 588
rank avg (pred): 0.443 +- 0.091
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 1.06478e-05, 5.67804e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 990
rank avg (pred): 0.176 +- 0.097
mrr vals (pred, true): 0.207, 0.249
batch losses (mrrl, rdl): 0.0176280942, 2.23195e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 803
rank avg (pred): 0.447 +- 0.090
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 2.6085e-05, 5.90136e-05

Epoch over!
epoch time: 15.232

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 267
rank avg (pred): 0.365 +- 0.188
mrr vals (pred, true): 0.161, 0.175
batch losses (mrrl, rdl): 0.0019977598, 0.0005807949

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 623
rank avg (pred): 0.430 +- 0.082
mrr vals (pred, true): 0.044, 0.047
batch losses (mrrl, rdl): 0.000394547, 7.0215e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 488
rank avg (pred): 0.337 +- 0.182
mrr vals (pred, true): 0.191, 0.026
batch losses (mrrl, rdl): 0.1991795003, 0.0005445836

Epoch over!
epoch time: 15.227

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 957
rank avg (pred): 0.412 +- 0.086
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 0.0001201867, 0.0001319714

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1056
rank avg (pred): 0.187 +- 0.091
mrr vals (pred, true): 0.170, 0.290
batch losses (mrrl, rdl): 0.1437349468, 4.00757e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1049
rank avg (pred): 0.435 +- 0.085
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 9.92e-08, 7.9032e-05

Epoch over!
epoch time: 15.234

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 294
rank avg (pred): 0.350 +- 0.175
mrr vals (pred, true): 0.167, 0.192
batch losses (mrrl, rdl): 0.0063717514, 0.0006321556

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 250
rank avg (pred): 0.343 +- 0.180
mrr vals (pred, true): 0.172, 0.243
batch losses (mrrl, rdl): 0.0510345995, 0.0008150977

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1144
rank avg (pred): 0.112 +- 0.060
mrr vals (pred, true): 0.226, 0.026
batch losses (mrrl, rdl): 0.3114521503, 0.0031172654

Epoch over!
epoch time: 15.254

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 49
rank avg (pred): 0.370 +- 0.175
mrr vals (pred, true): 0.156, 0.188
batch losses (mrrl, rdl): 0.0103645576, 0.0007080511

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 886
rank avg (pred): 0.448 +- 0.086
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 0.0001075392, 7.06097e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 819
rank avg (pred): 0.078 +- 0.042
mrr vals (pred, true): 0.254, 0.324
batch losses (mrrl, rdl): 0.0488009229, 5.39256e-05

Epoch over!
epoch time: 15.234

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 25
rank avg (pred): 0.367 +- 0.174
mrr vals (pred, true): 0.161, 0.199
batch losses (mrrl, rdl): 0.014057925, 0.0006524429

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1189
rank avg (pred): 0.436 +- 0.085
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 9.23e-07, 6.7209e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 481
rank avg (pred): 0.436 +- 0.099
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001231465, 5.11927e-05

Epoch over!
epoch time: 15.012

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1204
rank avg (pred): 0.443 +- 0.088
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 2.11164e-05, 7.60175e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 331
rank avg (pred): 0.435 +- 0.106
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001507873, 5.61682e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 239
rank avg (pred): 0.437 +- 0.102
mrr vals (pred, true): 0.054, 0.051
batch losses (mrrl, rdl): 0.0001225944, 6.53043e-05

Epoch over!
epoch time: 14.992

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 618
rank avg (pred): 0.434 +- 0.092
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 9.92964e-05, 7.09735e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 217
rank avg (pred): 0.453 +- 0.085
mrr vals (pred, true): 0.044, 0.051
batch losses (mrrl, rdl): 0.0003240567, 8.19003e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 108
rank avg (pred): 0.440 +- 0.090
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 3.95332e-05, 6.15376e-05

Epoch over!
epoch time: 14.989

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.438 +- 0.090
mrr vals (pred, true): 0.052, 0.045

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  115 	     0 	 0.22680 	 0.02045 	 MISS
  118 	     1 	 0.25480 	 0.02144 	 MISS
   79 	     2 	 0.17168 	 0.02305 	 MISS
  116 	     3 	 0.22917 	 0.02336 	 MISS
  102 	     4 	 0.17448 	 0.02467 	 MISS
   79 	     5 	 0.17168 	 0.02497 	 MISS
   79 	     6 	 0.17168 	 0.02518 	 MISS
   79 	     7 	 0.17168 	 0.02626 	 MISS
  111 	     8 	 0.19848 	 0.02849 	 MISS
   49 	     9 	 0.05247 	 0.02966 	 ~...
   70 	    10 	 0.05275 	 0.03018 	 ~...
   38 	    11 	 0.05236 	 0.03213 	 ~...
   10 	    12 	 0.05204 	 0.03549 	 ~...
   40 	    13 	 0.05237 	 0.03699 	 ~...
   44 	    14 	 0.05240 	 0.03708 	 ~...
   37 	    15 	 0.05236 	 0.03747 	 ~...
   66 	    16 	 0.05271 	 0.03830 	 ~...
   60 	    17 	 0.05261 	 0.03964 	 ~...
   41 	    18 	 0.05238 	 0.04032 	 ~...
   29 	    19 	 0.05226 	 0.04042 	 ~...
   39 	    20 	 0.05236 	 0.04045 	 ~...
   67 	    21 	 0.05271 	 0.04046 	 ~...
   19 	    22 	 0.05217 	 0.04050 	 ~...
   30 	    23 	 0.05227 	 0.04101 	 ~...
   52 	    24 	 0.05250 	 0.04115 	 ~...
    8 	    25 	 0.05202 	 0.04139 	 ~...
   13 	    26 	 0.05206 	 0.04255 	 ~...
    5 	    27 	 0.05181 	 0.04258 	 ~...
   27 	    28 	 0.05225 	 0.04278 	 ~...
   68 	    29 	 0.05272 	 0.04303 	 ~...
   12 	    30 	 0.05206 	 0.04332 	 ~...
   33 	    31 	 0.05231 	 0.04337 	 ~...
   76 	    32 	 0.05362 	 0.04369 	 ~...
   47 	    33 	 0.05241 	 0.04369 	 ~...
   24 	    34 	 0.05223 	 0.04408 	 ~...
   31 	    35 	 0.05228 	 0.04418 	 ~...
   13 	    36 	 0.05206 	 0.04436 	 ~...
   61 	    37 	 0.05266 	 0.04448 	 ~...
   54 	    38 	 0.05254 	 0.04474 	 ~...
   21 	    39 	 0.05222 	 0.04498 	 ~...
   17 	    40 	 0.05214 	 0.04505 	 ~...
   18 	    41 	 0.05214 	 0.04513 	 ~...
   58 	    42 	 0.05260 	 0.04517 	 ~...
    1 	    43 	 0.05164 	 0.04537 	 ~...
   76 	    44 	 0.05362 	 0.04550 	 ~...
   65 	    45 	 0.05268 	 0.04560 	 ~...
   35 	    46 	 0.05235 	 0.04615 	 ~...
   32 	    47 	 0.05231 	 0.04639 	 ~...
   46 	    48 	 0.05241 	 0.04642 	 ~...
   69 	    49 	 0.05273 	 0.04646 	 ~...
   53 	    50 	 0.05250 	 0.04649 	 ~...
   72 	    51 	 0.05290 	 0.04668 	 ~...
   16 	    52 	 0.05212 	 0.04688 	 ~...
   22 	    53 	 0.05223 	 0.04699 	 ~...
   48 	    54 	 0.05247 	 0.04706 	 ~...
   44 	    55 	 0.05240 	 0.04747 	 ~...
   11 	    56 	 0.05205 	 0.04747 	 ~...
    0 	    57 	 0.05161 	 0.04761 	 ~...
   43 	    58 	 0.05239 	 0.04776 	 ~...
    3 	    59 	 0.05173 	 0.04815 	 ~...
   62 	    60 	 0.05266 	 0.04852 	 ~...
   27 	    61 	 0.05225 	 0.04897 	 ~...
   15 	    62 	 0.05208 	 0.04907 	 ~...
   50 	    63 	 0.05248 	 0.04928 	 ~...
   26 	    64 	 0.05224 	 0.04949 	 ~...
   51 	    65 	 0.05249 	 0.04952 	 ~...
    6 	    66 	 0.05185 	 0.04978 	 ~...
   56 	    67 	 0.05255 	 0.04989 	 ~...
   71 	    68 	 0.05282 	 0.05007 	 ~...
   33 	    69 	 0.05231 	 0.05012 	 ~...
    3 	    70 	 0.05173 	 0.05016 	 ~...
    9 	    71 	 0.05203 	 0.05026 	 ~...
   73 	    72 	 0.05300 	 0.05041 	 ~...
    2 	    73 	 0.05170 	 0.05056 	 ~...
   36 	    74 	 0.05235 	 0.05069 	 ~...
   64 	    75 	 0.05267 	 0.05095 	 ~...
   58 	    76 	 0.05260 	 0.05159 	 ~...
   20 	    77 	 0.05221 	 0.05197 	 ~...
   78 	    78 	 0.05390 	 0.05244 	 ~...
   63 	    79 	 0.05266 	 0.05275 	 ~...
   75 	    80 	 0.05347 	 0.05300 	 ~...
   25 	    81 	 0.05223 	 0.05309 	 ~...
   42 	    82 	 0.05239 	 0.05320 	 ~...
   74 	    83 	 0.05327 	 0.05355 	 ~...
    7 	    84 	 0.05198 	 0.05439 	 ~...
   57 	    85 	 0.05259 	 0.05466 	 ~...
   54 	    86 	 0.05254 	 0.05851 	 ~...
   22 	    87 	 0.05223 	 0.05989 	 ~...
   79 	    88 	 0.17168 	 0.18625 	 ~...
  105 	    89 	 0.17743 	 0.19234 	 ~...
   79 	    90 	 0.17168 	 0.20119 	 ~...
   79 	    91 	 0.17168 	 0.20191 	 m..s
   79 	    92 	 0.17168 	 0.20414 	 m..s
  100 	    93 	 0.17215 	 0.20501 	 m..s
  103 	    94 	 0.17532 	 0.20836 	 m..s
   79 	    95 	 0.17168 	 0.20936 	 m..s
   99 	    96 	 0.17178 	 0.21244 	 m..s
  101 	    97 	 0.17293 	 0.21393 	 m..s
   79 	    98 	 0.17168 	 0.21474 	 m..s
   79 	    99 	 0.17168 	 0.21723 	 m..s
   79 	   100 	 0.17168 	 0.21767 	 m..s
  106 	   101 	 0.17820 	 0.21805 	 m..s
   79 	   102 	 0.17168 	 0.21875 	 m..s
   79 	   103 	 0.17168 	 0.21902 	 m..s
   79 	   104 	 0.17168 	 0.22077 	 m..s
   79 	   105 	 0.17168 	 0.22505 	 m..s
   79 	   106 	 0.17168 	 0.22546 	 m..s
   79 	   107 	 0.17168 	 0.24656 	 m..s
   79 	   108 	 0.17168 	 0.25289 	 m..s
   79 	   109 	 0.17168 	 0.25298 	 m..s
  104 	   110 	 0.17597 	 0.27224 	 m..s
  110 	   111 	 0.19487 	 0.27497 	 m..s
  108 	   112 	 0.19455 	 0.28533 	 m..s
  112 	   113 	 0.22070 	 0.28654 	 m..s
  107 	   114 	 0.19126 	 0.28862 	 m..s
  109 	   115 	 0.19470 	 0.28917 	 m..s
  114 	   116 	 0.22311 	 0.30732 	 m..s
  113 	   117 	 0.22285 	 0.31426 	 m..s
  120 	   118 	 0.26425 	 0.37350 	 MISS
  119 	   119 	 0.26070 	 0.40129 	 MISS
  117 	   120 	 0.25156 	 0.52223 	 MISS
==========================================
r_mrr = 0.7769196629524231
r2_mrr = 0.5920042395591736
spearmanr_mrr@5 = 0.7522581219673157
spearmanr_mrr@10 = 0.8116324543952942
spearmanr_mrr@50 = 0.7777579426765442
spearmanr_mrr@100 = 0.8779671788215637
spearmanr_mrr@All = 0.8902981281280518
==========================================
test time: 0.451
Done Testing dataset UMLS
total time taken: 233.07149267196655
training time taken: 226.7818603515625
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.7769)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.5920)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.7523)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.8116)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.7778)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.8780)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.8903)}}, 'test_loss': {'ComplEx': {'UMLS': 4.233230211364571}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 4164796326378518
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [560, 1031, 995, 253, 759, 167, 782, 810, 1053, 331, 419, 1145, 887, 465, 20, 1062, 515, 1109, 406, 870, 394, 433, 837, 163, 755, 1127, 747, 613, 868, 961, 719, 423, 1138, 1097, 429, 495, 1155, 100, 997, 1195, 344, 835, 464, 590, 342, 641, 78, 674, 350, 592, 784, 930, 1077, 974, 373, 752, 449, 1197, 411, 488, 150, 191, 443, 1039, 343, 296, 1014, 379, 775, 620, 552, 608, 1105, 1079, 70, 991, 366, 935, 452, 2, 10, 307, 538, 249, 1169, 534, 603, 81, 1040, 990, 516, 300, 1111, 655, 566, 883, 110, 1206, 918, 606, 1007, 934, 518, 268, 1135, 374, 178, 37, 664, 677, 562, 754, 98, 1005, 557, 867, 710, 509, 13, 1084, 506]
valid_ids (0): []
train_ids (1094): [22, 60, 229, 232, 849, 134, 352, 902, 258, 1018, 476, 499, 335, 1027, 236, 625, 574, 186, 976, 55, 200, 1148, 1203, 800, 1081, 539, 1093, 401, 803, 547, 987, 309, 933, 1199, 1037, 391, 585, 50, 851, 571, 283, 950, 497, 123, 393, 458, 916, 369, 1041, 211, 758, 648, 251, 891, 1198, 430, 231, 273, 266, 83, 153, 1193, 226, 705, 1209, 267, 928, 1150, 372, 390, 767, 512, 896, 580, 706, 422, 1189, 1194, 593, 578, 334, 400, 543, 56, 1087, 901, 884, 546, 44, 118, 508, 228, 256, 227, 834, 619, 280, 937, 84, 607, 278, 3, 505, 1136, 47, 721, 527, 777, 424, 771, 487, 993, 722, 324, 358, 862, 744, 371, 193, 1012, 806, 561, 72, 627, 567, 327, 616, 704, 612, 760, 126, 564, 426, 1151, 772, 982, 1142, 120, 1057, 795, 913, 1046, 829, 878, 763, 609, 355, 716, 39, 953, 656, 1200, 329, 1187, 214, 520, 676, 692, 428, 846, 111, 182, 213, 1114, 1033, 942, 1035, 733, 869, 1143, 1066, 271, 463, 962, 146, 439, 412, 977, 454, 569, 336, 48, 1118, 776, 940, 1202, 90, 669, 302, 748, 573, 712, 955, 462, 645, 1179, 340, 646, 1165, 315, 801, 483, 220, 575, 364, 166, 368, 230, 49, 172, 42, 840, 377, 351, 717, 614, 91, 694, 1064, 672, 605, 1073, 637, 764, 1167, 970, 144, 769, 684, 973, 688, 1069, 409, 699, 639, 128, 685, 839, 570, 187, 818, 600, 504, 709, 734, 510, 147, 906, 670, 250, 531, 151, 965, 968, 1147, 565, 132, 1049, 633, 1044, 1130, 384, 602, 77, 943, 295, 1100, 856, 808, 700, 94, 853, 496, 316, 1140, 479, 260, 859, 347, 532, 349, 514, 957, 1117, 666, 421, 594, 778, 1170, 86, 756, 293, 208, 1, 338, 819, 702, 148, 204, 723, 773, 678, 568, 630, 217, 682, 598, 751, 116, 434, 1085, 353, 450, 407, 345, 323, 720, 1186, 790, 362, 203, 536, 498, 781, 107, 58, 34, 67, 701, 171, 32, 736, 1211, 233, 1107, 413, 774, 164, 958, 542, 860, 507, 291, 241, 1090, 981, 1160, 888, 1098, 629, 742, 1113, 885, 820, 375, 880, 442, 92, 1052, 794, 1086, 1099, 255, 821, 33, 844, 1180, 152, 524, 1110, 38, 1096, 816, 1009, 112, 817, 1208, 749, 681, 319, 671, 197, 73, 12, 944, 183, 828, 554, 396, 822, 445, 718, 673, 1002, 513, 188, 555, 1204, 88, 272, 221, 523, 456, 1092, 915, 640, 611, 397, 1201, 461, 892, 61, 201, 1154, 1068, 738, 1029, 952, 740, 960, 339, 866, 783, 130, 304, 1146, 89, 798, 165, 1063, 313, 850, 427, 235, 852, 979, 951, 95, 395, 1070, 383, 321, 796, 224, 1125, 572, 757, 954, 858, 1123, 921, 437, 971, 1175, 1188, 785, 949, 337, 1191, 779, 525, 836, 1008, 544, 15, 1101, 1132, 1015, 948, 1156, 727, 926, 730, 26, 41, 761, 938, 446, 924, 745, 199, 1185, 467, 889, 871, 244, 176, 596, 399, 802, 825, 865, 459, 621, 804, 1152, 984, 502, 588, 945, 234, 4, 281, 247, 1072, 841, 586, 357, 583, 890, 689, 1054, 257, 533, 886, 170, 481, 469, 989, 207, 1034, 998, 62, 482, 190, 63, 1112, 141, 1128, 117, 1119, 854, 581, 154, 1042, 1023, 1047, 653, 695, 1016, 1176, 985, 925, 654, 136, 491, 1164, 180, 963, 1174, 435, 624, 667, 305, 486, 563, 75, 1153, 108, 365, 158, 725, 815, 1076, 24, 558, 1059, 872, 301, 826, 405, 897, 662, 46, 1074, 320, 1051, 770, 996, 947, 1134, 277, 259, 743, 348, 876, 40, 380, 282, 941, 189, 276, 1032, 832, 441, 1022, 587, 101, 169, 184, 929, 1122, 739, 1080, 381, 292, 219, 1192, 807, 96, 80, 121, 1129, 1020, 553, 959, 238, 679, 792, 274, 1006, 848, 519, 517, 480, 932, 332, 905, 660, 863, 983, 715, 711, 1106, 104, 927, 1182, 54, 601, 138, 162, 298, 31, 873, 425, 216, 25, 908, 591, 1048, 980, 71, 668, 696, 500, 29, 746, 1050, 1214, 0, 972, 793, 766, 537, 159, 857, 978, 827, 661, 1157, 209, 417, 106, 814, 577, 1095, 521, 436, 875, 306, 787, 831, 1133, 1061, 899, 444, 1038, 1104, 245, 328, 912, 838, 579, 881, 448, 707, 907, 1011, 105, 447, 697, 113, 1166, 354, 535, 728, 370, 762, 303, 285, 248, 914, 1159, 489, 404, 644, 133, 286, 994, 657, 194, 526, 830, 389, 341, 1163, 222, 634, 917, 1141, 330, 93, 698, 988, 548, 855, 269, 103, 545, 904, 97, 797, 177, 652, 911, 1004, 842, 893, 102, 768, 1089, 789, 1071, 1030, 1003, 1115, 530, 432, 638, 1000, 408, 210, 894, 137, 946, 1013, 17, 1082, 845, 809, 824, 922, 173, 460, 683, 149, 551, 378, 478, 402, 909, 470, 474, 59, 582, 690, 416, 576, 431, 724, 731, 529, 23, 843, 82, 36, 753, 264, 99, 556, 385, 967, 311, 440, 765, 490, 610, 79, 185, 484, 312, 1144, 874, 920, 1168, 468, 1213, 45, 453, 541, 246, 658, 1124, 1196, 986, 472, 1149, 969, 279, 492, 387, 51, 703, 931, 114, 52, 750, 522, 847, 270, 729, 599, 359, 5, 813, 9, 265, 196, 1083, 903, 618, 218, 687, 1043, 1210, 1103, 617, 503, 1065, 414, 168, 1137, 691, 791, 1028, 181, 494, 1184, 595, 1067, 741, 43, 1177, 882, 923, 966, 382, 628, 636, 635, 290, 737, 7, 589, 1091, 1056, 360, 786, 726, 549, 642, 318, 939, 74, 174, 326, 115, 262, 615, 473, 287, 900, 1205, 289, 261, 477, 418, 1055, 294, 1158, 501, 192, 322, 1139, 919, 788, 131, 663, 28, 225, 299, 511, 1162, 212, 1078, 237, 1088, 363, 317, 650, 392, 263, 6, 205, 1120, 895, 124, 202, 780, 451, 713, 861, 1131, 455, 175, 438, 1025, 386, 156, 157, 87, 1126, 68, 66, 139, 1001, 85, 223, 999, 179, 65, 1207, 805, 356, 275, 127, 799, 649, 64, 475, 16, 1094, 936, 665, 140, 735, 680, 57, 877, 415, 1121, 69, 626, 242, 1173, 240, 125, 1190, 1021, 18, 686, 485, 457, 956, 584, 145, 1172, 143, 346, 622, 631, 812, 651, 647, 252, 333, 528, 898, 1045, 8, 1183, 604, 471, 1060, 308, 19, 35, 864, 811, 14, 1024, 53, 1075, 11, 879, 992, 1108, 493, 398, 325, 76, 1181, 975, 109, 1102, 1161, 643, 1058, 708, 714, 195, 1036, 254, 675, 239, 659, 540, 376, 559, 693, 129, 160, 1019, 964, 310, 388, 1026, 420, 823, 1116, 1171, 1212, 142, 632, 1178, 623, 297, 314, 466, 367, 243, 1017, 833, 135, 155, 597, 732, 910, 30, 1010, 403, 198, 215, 119, 161, 410, 206, 361, 27, 284, 122, 550, 21, 288]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8378122620358232
the save name prefix for this run is:  chkpt-ID_8378122620358232_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 237
rank avg (pred): 0.581 +- 0.005
mrr vals (pred, true): 0.013, 0.048
batch losses (mrrl, rdl): 0.0, 0.0004552911

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 816
rank avg (pred): 0.371 +- 0.227
mrr vals (pred, true): 0.134, 0.218
batch losses (mrrl, rdl): 0.0, 0.0009973434

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1059
rank avg (pred): 0.335 +- 0.235
mrr vals (pred, true): 0.186, 0.282
batch losses (mrrl, rdl): 0.0, 0.0006348589

Epoch over!
epoch time: 14.846

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 311
rank avg (pred): 0.344 +- 0.237
mrr vals (pred, true): 0.175, 0.228
batch losses (mrrl, rdl): 0.0, 0.000895124

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1187
rank avg (pred): 0.418 +- 0.277
mrr vals (pred, true): 0.191, 0.032
batch losses (mrrl, rdl): 0.0, 1.59929e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 862
rank avg (pred): 0.333 +- 0.256
mrr vals (pred, true): 0.238, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001430509

Epoch over!
epoch time: 14.851

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 382
rank avg (pred): 0.315 +- 0.253
mrr vals (pred, true): 0.261, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001515232

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 757
rank avg (pred): 0.325 +- 0.259
mrr vals (pred, true): 0.255, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001937014

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 447
rank avg (pred): 0.311 +- 0.259
mrr vals (pred, true): 0.275, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001608779

Epoch over!
epoch time: 14.988

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 857
rank avg (pred): 0.317 +- 0.264
mrr vals (pred, true): 0.279, 0.059
batch losses (mrrl, rdl): 0.0, 0.0001512545

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 489
rank avg (pred): 0.416 +- 0.257
mrr vals (pred, true): 0.209, 0.026
batch losses (mrrl, rdl): 0.0, 8.44859e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 980
rank avg (pred): 0.344 +- 0.265
mrr vals (pred, true): 0.257, 0.284
batch losses (mrrl, rdl): 0.0, 0.0007043695

Epoch over!
epoch time: 15.048

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 417
rank avg (pred): 0.337 +- 0.267
mrr vals (pred, true): 0.265, 0.064
batch losses (mrrl, rdl): 0.0, 0.0001038551

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 468
rank avg (pred): 0.306 +- 0.268
mrr vals (pred, true): 0.326, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001803967

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1185
rank avg (pred): 0.455 +- 0.247
mrr vals (pred, true): 0.159, 0.035
batch losses (mrrl, rdl): 0.0, 4.23864e-05

Epoch over!
epoch time: 14.927

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 679
rank avg (pred): 0.461 +- 0.252
mrr vals (pred, true): 0.161, 0.046
batch losses (mrrl, rdl): 0.1233391836, 2.80785e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 166
rank avg (pred): 0.371 +- 0.191
mrr vals (pred, true): 0.114, 0.052
batch losses (mrrl, rdl): 0.0407215618, 6.69206e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 834
rank avg (pred): 0.399 +- 0.173
mrr vals (pred, true): 0.089, 0.249
batch losses (mrrl, rdl): 0.2542753816, 0.0011526045

Epoch over!
epoch time: 15.05

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 225
rank avg (pred): 0.381 +- 0.186
mrr vals (pred, true): 0.109, 0.045
batch losses (mrrl, rdl): 0.0352654271, 5.31991e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.364 +- 0.188
mrr vals (pred, true): 0.125, 0.052
batch losses (mrrl, rdl): 0.056367971, 9.28881e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 634
rank avg (pred): 0.463 +- 0.121
mrr vals (pred, true): 0.054, 0.046
batch losses (mrrl, rdl): 0.0001869298, 5.08957e-05

Epoch over!
epoch time: 15.016

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1052
rank avg (pred): 0.325 +- 0.183
mrr vals (pred, true): 0.135, 0.047
batch losses (mrrl, rdl): 0.0722818896, 0.0002690671

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 262
rank avg (pred): 0.384 +- 0.173
mrr vals (pred, true): 0.114, 0.222
batch losses (mrrl, rdl): 0.1149395555, 0.0008257129

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 483
rank avg (pred): 0.394 +- 0.162
mrr vals (pred, true): 0.104, 0.051
batch losses (mrrl, rdl): 0.0287758093, 7.56202e-05

Epoch over!
epoch time: 14.991

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 104
rank avg (pred): 0.400 +- 0.157
mrr vals (pred, true): 0.102, 0.055
batch losses (mrrl, rdl): 0.0272352025, 7.39016e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1130
rank avg (pred): 0.350 +- 0.173
mrr vals (pred, true): 0.117, 0.043
batch losses (mrrl, rdl): 0.0451826453, 0.0001843525

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 42
rank avg (pred): 0.393 +- 0.157
mrr vals (pred, true): 0.101, 0.200
batch losses (mrrl, rdl): 0.0994431302, 0.0008801491

Epoch over!
epoch time: 14.992

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1174
rank avg (pred): 0.470 +- 0.110
mrr vals (pred, true): 0.049, 0.034
batch losses (mrrl, rdl): 2.11435e-05, 5.52528e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 5
rank avg (pred): 0.374 +- 0.151
mrr vals (pred, true): 0.111, 0.238
batch losses (mrrl, rdl): 0.1632987708, 0.0011337624

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 354
rank avg (pred): 0.362 +- 0.152
mrr vals (pred, true): 0.116, 0.045
batch losses (mrrl, rdl): 0.0435357504, 0.0001970703

Epoch over!
epoch time: 15.012

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 187
rank avg (pred): 0.365 +- 0.139
mrr vals (pred, true): 0.106, 0.049
batch losses (mrrl, rdl): 0.0308600832, 0.0001047418

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 658
rank avg (pred): 0.429 +- 0.106
mrr vals (pred, true): 0.057, 0.048
batch losses (mrrl, rdl): 0.0005358569, 5.64431e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 399
rank avg (pred): 0.399 +- 0.142
mrr vals (pred, true): 0.100, 0.054
batch losses (mrrl, rdl): 0.0251597129, 6.60926e-05

Epoch over!
epoch time: 15.231

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 353
rank avg (pred): 0.351 +- 0.168
mrr vals (pred, true): 0.143, 0.051
batch losses (mrrl, rdl): 0.0869606435, 0.0001719361

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 120
rank avg (pred): 0.394 +- 0.142
mrr vals (pred, true): 0.107, 0.044
batch losses (mrrl, rdl): 0.0330620036, 9.47798e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 355
rank avg (pred): 0.355 +- 0.150
mrr vals (pred, true): 0.123, 0.046
batch losses (mrrl, rdl): 0.0536829308, 0.0002308915

Epoch over!
epoch time: 15.255

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1020
rank avg (pred): 0.362 +- 0.167
mrr vals (pred, true): 0.134, 0.041
batch losses (mrrl, rdl): 0.0712372959, 0.0001850397

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1075
rank avg (pred): 0.364 +- 0.167
mrr vals (pred, true): 0.130, 0.273
batch losses (mrrl, rdl): 0.2046811581, 0.0009915824

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 430
rank avg (pred): 0.391 +- 0.136
mrr vals (pred, true): 0.105, 0.055
batch losses (mrrl, rdl): 0.0298971757, 8.27735e-05

Epoch over!
epoch time: 15.237

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 628
rank avg (pred): 0.443 +- 0.088
mrr vals (pred, true): 0.047, 0.038
batch losses (mrrl, rdl): 8.78184e-05, 6.70508e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 138
rank avg (pred): 0.387 +- 0.149
mrr vals (pred, true): 0.117, 0.044
batch losses (mrrl, rdl): 0.0454884954, 7.12785e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 30
rank avg (pred): 0.394 +- 0.142
mrr vals (pred, true): 0.110, 0.248
batch losses (mrrl, rdl): 0.1893897057, 0.0011222853

Epoch over!
epoch time: 15.233

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 654
rank avg (pred): 0.426 +- 0.092
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 7.5865e-05, 6.11129e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 729
rank avg (pred): 0.364 +- 0.160
mrr vals (pred, true): 0.133, 0.374
batch losses (mrrl, rdl): 0.577945888, 0.0015886648

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 858
rank avg (pred): 0.364 +- 0.161
mrr vals (pred, true): 0.135, 0.051
batch losses (mrrl, rdl): 0.0720658973, 0.0001239307

Epoch over!
epoch time: 15.203

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.420 +- 0.088
mrr vals (pred, true): 0.053, 0.025

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   25 	     0 	 0.05324 	 0.02165 	 m..s
    4 	     1 	 0.05224 	 0.02283 	 ~...
    0 	     2 	 0.05165 	 0.02291 	 ~...
   23 	     3 	 0.05324 	 0.02301 	 m..s
   21 	     4 	 0.05306 	 0.02383 	 ~...
    1 	     5 	 0.05182 	 0.02414 	 ~...
    9 	     6 	 0.05270 	 0.02453 	 ~...
   18 	     7 	 0.05295 	 0.02460 	 ~...
   33 	     8 	 0.05413 	 0.02475 	 ~...
   10 	     9 	 0.05274 	 0.02479 	 ~...
    3 	    10 	 0.05216 	 0.02494 	 ~...
   31 	    11 	 0.05409 	 0.02520 	 ~...
   11 	    12 	 0.05278 	 0.02523 	 ~...
   20 	    13 	 0.05296 	 0.02539 	 ~...
   30 	    14 	 0.05397 	 0.02552 	 ~...
   29 	    15 	 0.05388 	 0.02554 	 ~...
   16 	    16 	 0.05289 	 0.02638 	 ~...
   34 	    17 	 0.05440 	 0.02642 	 ~...
   35 	    18 	 0.05447 	 0.02679 	 ~...
   36 	    19 	 0.05470 	 0.02979 	 ~...
   39 	    20 	 0.05581 	 0.03139 	 ~...
   27 	    21 	 0.05353 	 0.03512 	 ~...
   28 	    22 	 0.05381 	 0.03671 	 ~...
  105 	    23 	 0.14731 	 0.03721 	 MISS
   17 	    24 	 0.05290 	 0.03747 	 ~...
    7 	    25 	 0.05232 	 0.03811 	 ~...
   54 	    26 	 0.11216 	 0.03815 	 m..s
  109 	    27 	 0.15015 	 0.03920 	 MISS
    5 	    28 	 0.05224 	 0.03927 	 ~...
   31 	    29 	 0.05409 	 0.03942 	 ~...
   52 	    30 	 0.11127 	 0.03976 	 m..s
  101 	    31 	 0.14602 	 0.03999 	 MISS
   63 	    32 	 0.11618 	 0.04004 	 m..s
    2 	    33 	 0.05193 	 0.04020 	 ~...
   49 	    34 	 0.10973 	 0.04046 	 m..s
   64 	    35 	 0.11661 	 0.04050 	 m..s
   26 	    36 	 0.05329 	 0.04070 	 ~...
   45 	    37 	 0.10837 	 0.04101 	 m..s
   68 	    38 	 0.11800 	 0.04143 	 m..s
   40 	    39 	 0.05613 	 0.04147 	 ~...
   82 	    40 	 0.12286 	 0.04155 	 m..s
   38 	    41 	 0.05483 	 0.04169 	 ~...
   83 	    42 	 0.12304 	 0.04174 	 m..s
   89 	    43 	 0.14357 	 0.04175 	 MISS
  109 	    44 	 0.15015 	 0.04176 	 MISS
   18 	    45 	 0.05295 	 0.04190 	 ~...
  114 	    46 	 0.16434 	 0.04235 	 MISS
   85 	    47 	 0.12430 	 0.04251 	 m..s
  102 	    48 	 0.14646 	 0.04255 	 MISS
   61 	    49 	 0.11598 	 0.04272 	 m..s
   45 	    50 	 0.10837 	 0.04286 	 m..s
   22 	    51 	 0.05308 	 0.04355 	 ~...
   44 	    52 	 0.10772 	 0.04388 	 m..s
   11 	    53 	 0.05278 	 0.04403 	 ~...
   91 	    54 	 0.14489 	 0.04408 	 MISS
  114 	    55 	 0.16434 	 0.04418 	 MISS
  114 	    56 	 0.16434 	 0.04419 	 MISS
  112 	    57 	 0.15098 	 0.04436 	 MISS
   58 	    58 	 0.11565 	 0.04480 	 m..s
   53 	    59 	 0.11178 	 0.04506 	 m..s
   36 	    60 	 0.05470 	 0.04519 	 ~...
   15 	    61 	 0.05289 	 0.04524 	 ~...
    6 	    62 	 0.05225 	 0.04543 	 ~...
  114 	    63 	 0.16434 	 0.04551 	 MISS
   14 	    64 	 0.05288 	 0.04559 	 ~...
  113 	    65 	 0.16422 	 0.04613 	 MISS
  114 	    66 	 0.16434 	 0.04692 	 MISS
   97 	    67 	 0.14551 	 0.04696 	 m..s
   78 	    68 	 0.12160 	 0.04702 	 m..s
   70 	    69 	 0.11903 	 0.04758 	 m..s
   77 	    70 	 0.12124 	 0.04763 	 m..s
   48 	    71 	 0.10903 	 0.04764 	 m..s
   72 	    72 	 0.12005 	 0.04788 	 m..s
   71 	    73 	 0.11969 	 0.04815 	 m..s
  103 	    74 	 0.14664 	 0.04851 	 m..s
  114 	    75 	 0.16434 	 0.04894 	 MISS
   81 	    76 	 0.12281 	 0.04952 	 m..s
   55 	    77 	 0.11268 	 0.04952 	 m..s
   78 	    78 	 0.12160 	 0.04973 	 m..s
   42 	    79 	 0.10709 	 0.04974 	 m..s
   73 	    80 	 0.12011 	 0.04986 	 m..s
   96 	    81 	 0.14536 	 0.04991 	 m..s
   13 	    82 	 0.05283 	 0.05025 	 ~...
   60 	    83 	 0.11596 	 0.05151 	 m..s
  106 	    84 	 0.14749 	 0.05218 	 m..s
  107 	    85 	 0.14860 	 0.05263 	 m..s
    8 	    86 	 0.05264 	 0.05358 	 ~...
   66 	    87 	 0.11794 	 0.05367 	 m..s
   74 	    88 	 0.12021 	 0.05385 	 m..s
   47 	    89 	 0.10859 	 0.05518 	 m..s
   69 	    90 	 0.11838 	 0.05608 	 m..s
   23 	    91 	 0.05324 	 0.05977 	 ~...
   88 	    92 	 0.13760 	 0.06233 	 m..s
   50 	    93 	 0.11038 	 0.20178 	 m..s
   41 	    94 	 0.10375 	 0.20499 	 MISS
   67 	    95 	 0.11798 	 0.20953 	 m..s
   85 	    96 	 0.12430 	 0.21142 	 m..s
   51 	    97 	 0.11066 	 0.21474 	 MISS
   62 	    98 	 0.11603 	 0.21616 	 MISS
   43 	    99 	 0.10729 	 0.21695 	 MISS
   75 	   100 	 0.12054 	 0.21875 	 m..s
   65 	   101 	 0.11747 	 0.21944 	 MISS
   57 	   102 	 0.11543 	 0.22238 	 MISS
   56 	   103 	 0.11397 	 0.22517 	 MISS
   58 	   104 	 0.11565 	 0.24394 	 MISS
   80 	   105 	 0.12184 	 0.24680 	 MISS
   92 	   106 	 0.14494 	 0.24930 	 MISS
   84 	   107 	 0.12334 	 0.24954 	 MISS
   93 	   108 	 0.14510 	 0.25215 	 MISS
   76 	   109 	 0.12078 	 0.25434 	 MISS
   99 	   110 	 0.14599 	 0.25820 	 MISS
  108 	   111 	 0.14887 	 0.26769 	 MISS
   95 	   112 	 0.14535 	 0.27282 	 MISS
  104 	   113 	 0.14686 	 0.27339 	 MISS
   98 	   114 	 0.14560 	 0.27441 	 MISS
  111 	   115 	 0.15025 	 0.27737 	 MISS
   90 	   116 	 0.14370 	 0.28460 	 MISS
   87 	   117 	 0.13580 	 0.28488 	 MISS
   94 	   118 	 0.14533 	 0.28862 	 MISS
  100 	   119 	 0.14599 	 0.31748 	 MISS
  114 	   120 	 0.16434 	 0.38817 	 MISS
==========================================
r_mrr = 0.4338703453540802
r2_mrr = 0.161748468875885
spearmanr_mrr@5 = nan
spearmanr_mrr@10 = 0.3321138620376587
spearmanr_mrr@50 = 0.8906489014625549
spearmanr_mrr@100 = 0.6964507699012756
spearmanr_mrr@All = 0.7205482721328735
==========================================
test time: 0.46
Done Testing dataset UMLS
total time taken: 232.5375530719757
training time taken: 226.3685324192047
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.4339)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.1617)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(nan)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.3321)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.8906)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.6965)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.7205)}}, 'test_loss': {'ComplEx': {'UMLS': 7.958224467482069}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 4158082564001064
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [851, 970, 941, 1181, 290, 1055, 223, 456, 1111, 1041, 168, 1016, 643, 513, 1191, 806, 286, 844, 723, 654, 455, 543, 812, 35, 790, 8, 1206, 1013, 763, 1066, 783, 32, 746, 101, 647, 0, 762, 589, 811, 885, 438, 557, 72, 1054, 1019, 903, 190, 569, 1214, 693, 779, 606, 300, 856, 733, 1141, 755, 909, 553, 306, 91, 817, 335, 356, 98, 244, 936, 1211, 720, 821, 1040, 886, 107, 722, 445, 770, 1075, 656, 246, 708, 1083, 302, 1014, 894, 69, 124, 996, 12, 1188, 685, 673, 1138, 1133, 460, 995, 1020, 753, 860, 854, 577, 1032, 110, 55, 1001, 1144, 534, 331, 614, 1050, 556, 324, 441, 814, 646, 748, 1137, 925, 914, 237, 17, 843]
valid_ids (0): []
train_ids (1094): [1183, 409, 993, 254, 129, 348, 354, 11, 1025, 778, 572, 102, 954, 910, 873, 1168, 578, 1061, 121, 248, 205, 1116, 615, 1189, 346, 325, 135, 786, 645, 29, 443, 946, 1154, 252, 588, 655, 1199, 282, 703, 43, 231, 394, 780, 220, 1148, 242, 974, 709, 1186, 846, 351, 801, 973, 85, 336, 175, 239, 681, 119, 966, 566, 530, 463, 565, 943, 210, 206, 263, 568, 1142, 1165, 50, 279, 905, 798, 19, 564, 256, 932, 309, 1200, 899, 418, 117, 944, 519, 277, 808, 729, 384, 667, 694, 24, 116, 953, 196, 1121, 809, 37, 1120, 1092, 355, 591, 251, 824, 400, 159, 581, 737, 209, 764, 634, 776, 850, 1126, 486, 311, 847, 1036, 1108, 296, 934, 266, 784, 982, 317, 392, 493, 405, 579, 1089, 829, 459, 1064, 616, 187, 662, 1063, 1067, 668, 980, 1115, 623, 92, 96, 1174, 484, 573, 482, 114, 388, 483, 613, 1021, 1150, 9, 489, 619, 350, 61, 153, 532, 485, 1155, 601, 716, 13, 174, 930, 358, 293, 415, 1039, 488, 1078, 963, 548, 14, 916, 1079, 434, 431, 374, 838, 1034, 857, 1159, 120, 490, 807, 751, 901, 452, 151, 1091, 1197, 215, 22, 956, 592, 81, 700, 1022, 1024, 314, 44, 841, 945, 620, 961, 1038, 988, 171, 872, 136, 449, 1146, 79, 867, 393, 782, 524, 679, 510, 1049, 386, 428, 525, 474, 211, 118, 272, 113, 672, 659, 717, 881, 462, 962, 426, 836, 514, 791, 981, 695, 712, 390, 688, 347, 1017, 420, 837, 39, 469, 1134, 774, 179, 464, 247, 416, 36, 476, 342, 567, 407, 499, 152, 896, 537, 128, 738, 660, 697, 203, 343, 454, 163, 305, 298, 1031, 106, 874, 931, 732, 502, 627, 882, 34, 1045, 1163, 1070, 200, 366, 1009, 923, 965, 926, 235, 76, 180, 719, 299, 487, 1167, 1190, 38, 978, 466, 1060, 1210, 198, 516, 768, 823, 41, 1011, 835, 517, 714, 461, 749, 657, 705, 212, 570, 991, 397, 253, 58, 536, 138, 871, 370, 147, 1000, 505, 391, 262, 772, 792, 741, 352, 225, 559, 707, 329, 423, 1099, 491, 78, 598, 410, 64, 1005, 1008, 216, 261, 169, 5, 590, 757, 547, 633, 275, 142, 60, 731, 1145, 866, 947, 1104, 382, 1112, 1028, 436, 228, 642, 940, 994, 86, 93, 1114, 88, 696, 1147, 383, 535, 1131, 740, 249, 270, 134, 1195, 189, 1012, 1002, 442, 398, 637, 429, 666, 84, 53, 349, 357, 967, 494, 735, 1042, 640, 972, 820, 202, 937, 361, 337, 194, 378, 952, 754, 711, 998, 1110, 268, 876, 255, 795, 521, 1100, 1158, 170, 1130, 1082, 1095, 868, 204, 140, 1101, 105, 968, 864, 853, 907, 976, 321, 173, 1088, 904, 130, 935, 31, 1156, 1173, 511, 1058, 260, 1015, 148, 1207, 523, 793, 481, 267, 1179, 222, 736, 859, 1003, 23, 161, 928, 341, 312, 345, 542, 766, 1162, 594, 810, 691, 167, 777, 307, 758, 471, 1212, 1084, 411, 715, 531, 677, 922, 964, 546, 137, 218, 702, 865, 665, 1194, 376, 593, 319, 89, 622, 1, 880, 586, 879, 726, 146, 1023, 412, 710, 457, 315, 805, 425, 51, 447, 333, 773, 1029, 408, 734, 177, 550, 214, 295, 401, 1073, 18, 115, 574, 1132, 477, 861, 765, 1157, 977, 969, 512, 427, 451, 54, 834, 1201, 75, 605, 1007, 771, 869, 1187, 472, 2, 796, 372, 815, 90, 626, 1103, 375, 689, 156, 385, 52, 653, 1026, 503, 554, 674, 127, 500, 756, 890, 323, 1097, 1059, 164, 924, 612, 403, 1068, 775, 387, 430, 960, 1004, 243, 301, 920, 957, 1202, 1193, 833, 424, 379, 234, 987, 219, 625, 241, 1065, 334, 744, 467, 1123, 951, 419, 1081, 603, 877, 1175, 638, 74, 549, 364, 721, 139, 558, 699, 100, 229, 652, 435, 406, 958, 617, 504, 1170, 663, 813, 208, 389, 56, 979, 166, 7, 1184, 328, 761, 373, 71, 433, 316, 340, 221, 274, 1052, 607, 111, 1209, 1171, 258, 585, 313, 680, 639, 395, 1178, 870, 671, 73, 1151, 618, 676, 551, 10, 1125, 1136, 1169, 624, 132, 911, 1135, 898, 197, 144, 948, 59, 125, 1037, 292, 875, 818, 600, 42, 294, 289, 718, 57, 949, 848, 172, 849, 584, 902, 4, 479, 1072, 402, 583, 1048, 1077, 30, 1117, 320, 828, 70, 917, 377, 830, 1047, 632, 83, 938, 360, 562, 658, 602, 192, 288, 413, 497, 25, 832, 1203, 528, 297, 199, 915, 1185, 109, 785, 478, 651, 629, 399, 155, 108, 21, 178, 892, 381, 77, 1149, 819, 992, 97, 912, 154, 803, 285, 112, 322, 839, 713, 575, 188, 670, 919, 444, 1198, 826, 541, 103, 636, 522, 1129, 975, 825, 40, 888, 701, 628, 468, 538, 362, 1152, 788, 104, 509, 822, 893, 184, 769, 587, 599, 730, 123, 887, 236, 747, 1090, 45, 1122, 939, 929, 6, 669, 889, 1053, 95, 986, 94, 781, 595, 1128, 230, 326, 631, 692, 1033, 900, 984, 563, 80, 304, 257, 1124, 496, 47, 518, 145, 66, 143, 728, 67, 165, 831, 515, 816, 1196, 1119, 470, 1205, 126, 999, 706, 264, 475, 310, 396, 227, 745, 276, 863, 99, 149, 157, 895, 291, 799, 1035, 927, 664, 158, 725, 122, 540, 1076, 989, 609, 498, 271, 308, 1109, 959, 608, 527, 245, 1213, 28, 724, 611, 884, 1118, 27, 1176, 480, 742, 1098, 1094, 68, 201, 597, 743, 1027, 1057, 529, 683, 1071, 641, 48, 1018, 465, 1127, 365, 1030, 648, 840, 678, 422, 339, 1204, 359, 787, 273, 186, 265, 767, 698, 238, 507, 727, 560, 49, 1074, 883, 439, 582, 942, 1085, 862, 533, 950, 1139, 3, 368, 908, 259, 858, 181, 287, 162, 684, 526, 690, 539, 552, 440, 417, 635, 1113, 269, 913, 508, 802, 353, 1044, 501, 176, 630, 15, 224, 369, 1096, 555, 571, 1161, 1172, 363, 404, 1140, 344, 897, 1164, 16, 794, 1056, 46, 327, 332, 1069, 131, 580, 675, 1192, 150, 450, 1051, 182, 852, 217, 281, 226, 997, 918, 278, 797, 1086, 878, 891, 1208, 827, 621, 193, 62, 446, 185, 371, 1043, 800, 432, 506, 983, 644, 492, 63, 421, 933, 380, 804, 160, 1006, 1080, 971, 1102, 473, 789, 195, 1180, 1182, 752, 661, 686, 561, 759, 545, 1087, 207, 284, 855, 250, 191, 280, 604, 330, 414, 303, 842, 183, 1010, 990, 750, 596, 649, 87, 495, 687, 650, 682, 82, 1093, 213, 704, 845, 739, 26, 1160, 448, 520, 232, 985, 576, 65, 1062, 1143, 1166, 1105, 1153, 233, 1046, 437, 610, 20, 1106, 283, 921, 133, 318, 367, 338, 955, 240, 544, 760, 906, 453, 458, 141, 33, 1107, 1177]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5244356936068407
the save name prefix for this run is:  chkpt-ID_5244356936068407_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 47
rank avg (pred): 0.530 +- 0.005
mrr vals (pred, true): 0.014, 0.215
batch losses (mrrl, rdl): 0.0, 0.0026150746

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 145
rank avg (pred): 0.407 +- 0.232
mrr vals (pred, true): 0.178, 0.044
batch losses (mrrl, rdl): 0.0, 1.60401e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 487
rank avg (pred): 0.464 +- 0.270
mrr vals (pred, true): 0.197, 0.024
batch losses (mrrl, rdl): 0.0, 2.80984e-05

Epoch over!
epoch time: 14.928

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 764
rank avg (pred): 0.423 +- 0.245
mrr vals (pred, true): 0.195, 0.043
batch losses (mrrl, rdl): 0.0, 8.4788e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 366
rank avg (pred): 0.423 +- 0.243
mrr vals (pred, true): 0.190, 0.041
batch losses (mrrl, rdl): 0.0, 8.2905e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 275
rank avg (pred): 0.133 +- 0.088
mrr vals (pred, true): 0.265, 0.264
batch losses (mrrl, rdl): 0.0, 2.9582e-05

Epoch over!
epoch time: 15.039

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 536
rank avg (pred): 0.478 +- 0.276
mrr vals (pred, true): 0.168, 0.021
batch losses (mrrl, rdl): 0.0, 3.13555e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 548
rank avg (pred): 0.479 +- 0.270
mrr vals (pred, true): 0.144, 0.024
batch losses (mrrl, rdl): 0.0, 1.82204e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 451
rank avg (pred): 0.413 +- 0.244
mrr vals (pred, true): 0.146, 0.060
batch losses (mrrl, rdl): 0.0, 4.9613e-06

Epoch over!
epoch time: 15.032

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 132
rank avg (pred): 0.428 +- 0.249
mrr vals (pred, true): 0.141, 0.048
batch losses (mrrl, rdl): 0.0, 9.3025e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1029
rank avg (pred): 0.400 +- 0.255
mrr vals (pred, true): 0.161, 0.050
batch losses (mrrl, rdl): 0.0, 1.72461e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 177
rank avg (pred): 0.422 +- 0.253
mrr vals (pred, true): 0.129, 0.053
batch losses (mrrl, rdl): 0.0, 3.6183e-06

Epoch over!
epoch time: 15.014

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 419
rank avg (pred): 0.433 +- 0.256
mrr vals (pred, true): 0.122, 0.040
batch losses (mrrl, rdl): 0.0, 3.8241e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 478
rank avg (pred): 0.435 +- 0.263
mrr vals (pred, true): 0.114, 0.052
batch losses (mrrl, rdl): 0.0, 7.2032e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 764
rank avg (pred): 0.432 +- 0.256
mrr vals (pred, true): 0.084, 0.043
batch losses (mrrl, rdl): 0.0, 3.8626e-06

Epoch over!
epoch time: 14.978

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 633
rank avg (pred): 0.451 +- 0.238
mrr vals (pred, true): 0.059, 0.040
batch losses (mrrl, rdl): 0.0007273431, 4.4629e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 417
rank avg (pred): 0.472 +- 0.183
mrr vals (pred, true): 0.047, 0.064
batch losses (mrrl, rdl): 7.33497e-05, 6.32703e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 699
rank avg (pred): 0.506 +- 0.160
mrr vals (pred, true): 0.040, 0.045
batch losses (mrrl, rdl): 0.0010618924, 0.0001372583

Epoch over!
epoch time: 15.186

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 134
rank avg (pred): 0.462 +- 0.193
mrr vals (pred, true): 0.055, 0.052
batch losses (mrrl, rdl): 0.0002326901, 4.34617e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 504
rank avg (pred): 0.483 +- 0.178
mrr vals (pred, true): 0.047, 0.025
batch losses (mrrl, rdl): 8.36404e-05, 1.21079e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 690
rank avg (pred): 0.500 +- 0.166
mrr vals (pred, true): 0.041, 0.043
batch losses (mrrl, rdl): 0.0008060311, 0.0001172103

Epoch over!
epoch time: 15.12

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 454
rank avg (pred): 0.476 +- 0.189
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001733824, 6.19752e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 535
rank avg (pred): 0.472 +- 0.191
mrr vals (pred, true): 0.055, 0.024
batch losses (mrrl, rdl): 0.0002607834, 1.25247e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 154
rank avg (pred): 0.480 +- 0.189
mrr vals (pred, true): 0.055, 0.047
batch losses (mrrl, rdl): 0.0002476737, 7.91781e-05

Epoch over!
epoch time: 15.097

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 493
rank avg (pred): 0.489 +- 0.179
mrr vals (pred, true): 0.046, 0.024
batch losses (mrrl, rdl): 0.0001347934, 8.5063e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 233
rank avg (pred): 0.496 +- 0.173
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002489505, 0.000118726

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 530
rank avg (pred): 0.487 +- 0.178
mrr vals (pred, true): 0.049, 0.027
batch losses (mrrl, rdl): 2.5221e-06, 8.33e-06

Epoch over!
epoch time: 15.091

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1092
rank avg (pred): 0.481 +- 0.185
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 2.72045e-05, 7.27681e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 522
rank avg (pred): 0.483 +- 0.178
mrr vals (pred, true): 0.054, 0.025
batch losses (mrrl, rdl): 0.0001372643, 7.7486e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 374
rank avg (pred): 0.496 +- 0.163
mrr vals (pred, true): 0.046, 0.045
batch losses (mrrl, rdl): 0.0001798053, 0.0001094421

Epoch over!
epoch time: 15.087

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 132
rank avg (pred): 0.464 +- 0.189
mrr vals (pred, true): 0.070, 0.048
batch losses (mrrl, rdl): 0.0039669611, 5.6159e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 578
rank avg (pred): 0.485 +- 0.172
mrr vals (pred, true): 0.051, 0.035
batch losses (mrrl, rdl): 5.3489e-06, 3.00415e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 800
rank avg (pred): 0.499 +- 0.153
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002365918, 0.0001104536

Epoch over!
epoch time: 15.088

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 149
rank avg (pred): 0.489 +- 0.160
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 4.1785e-06, 9.07602e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 701
rank avg (pred): 0.480 +- 0.162
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001970799, 7.9753e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 507
rank avg (pred): 0.502 +- 0.133
mrr vals (pred, true): 0.040, 0.026
batch losses (mrrl, rdl): 0.0009371054, 1.52282e-05

Epoch over!
epoch time: 15.09

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 228
rank avg (pred): 0.479 +- 0.157
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001002946, 8.64841e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 940
rank avg (pred): 0.476 +- 0.145
mrr vals (pred, true): 0.048, 0.040
batch losses (mrrl, rdl): 4.04052e-05, 2.326e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 301
rank avg (pred): 0.174 +- 0.108
mrr vals (pred, true): 0.238, 0.256
batch losses (mrrl, rdl): 0.0033882048, 2.07225e-05

Epoch over!
epoch time: 15.083

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 343
rank avg (pred): 0.493 +- 0.118
mrr vals (pred, true): 0.039, 0.055
batch losses (mrrl, rdl): 0.0011853465, 0.0001195019

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 287
rank avg (pred): 0.233 +- 0.145
mrr vals (pred, true): 0.231, 0.212
batch losses (mrrl, rdl): 0.0035327103, 7.55991e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 558
rank avg (pred): 0.474 +- 0.136
mrr vals (pred, true): 0.049, 0.025
batch losses (mrrl, rdl): 5.0756e-06, 2.39439e-05

Epoch over!
epoch time: 15.091

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 236
rank avg (pred): 0.468 +- 0.144
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002410272, 6.88685e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 955
rank avg (pred): 0.476 +- 0.127
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0002293458, 8.27243e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1053
rank avg (pred): 0.107 +- 0.071
mrr vals (pred, true): 0.282, 0.277
batch losses (mrrl, rdl): 0.0001836282, 3.99622e-05

Epoch over!
epoch time: 15.098

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.445 +- 0.153
mrr vals (pred, true): 0.068, 0.042

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   34 	     0 	 0.06457 	 0.02014 	 m..s
   55 	     1 	 0.06609 	 0.02094 	 m..s
   34 	     2 	 0.06457 	 0.02144 	 m..s
    4 	     3 	 0.06013 	 0.02149 	 m..s
   11 	     4 	 0.06142 	 0.02263 	 m..s
   12 	     5 	 0.06188 	 0.02301 	 m..s
    2 	     6 	 0.05980 	 0.02304 	 m..s
   53 	     7 	 0.06596 	 0.02336 	 m..s
    0 	     8 	 0.05908 	 0.02414 	 m..s
   62 	     9 	 0.06658 	 0.02571 	 m..s
   49 	    10 	 0.06562 	 0.02642 	 m..s
   48 	    11 	 0.06557 	 0.02749 	 m..s
    1 	    12 	 0.05979 	 0.02767 	 m..s
   13 	    13 	 0.06213 	 0.02843 	 m..s
    3 	    14 	 0.06008 	 0.02898 	 m..s
   22 	    15 	 0.06336 	 0.02966 	 m..s
   52 	    16 	 0.06572 	 0.03747 	 ~...
    9 	    17 	 0.06094 	 0.03798 	 ~...
   63 	    18 	 0.06676 	 0.03811 	 ~...
   75 	    19 	 0.06814 	 0.03837 	 ~...
   46 	    20 	 0.06529 	 0.03920 	 ~...
   86 	    21 	 0.07134 	 0.03948 	 m..s
   88 	    22 	 0.07315 	 0.03957 	 m..s
   14 	    23 	 0.06227 	 0.03964 	 ~...
   84 	    24 	 0.06961 	 0.04019 	 ~...
   29 	    25 	 0.06406 	 0.04022 	 ~...
   69 	    26 	 0.06723 	 0.04046 	 ~...
   54 	    27 	 0.06599 	 0.04050 	 ~...
   60 	    28 	 0.06634 	 0.04053 	 ~...
   76 	    29 	 0.06832 	 0.04080 	 ~...
   26 	    30 	 0.06390 	 0.04081 	 ~...
   16 	    31 	 0.06278 	 0.04084 	 ~...
   84 	    32 	 0.06961 	 0.04108 	 ~...
   80 	    33 	 0.06844 	 0.04157 	 ~...
   31 	    34 	 0.06424 	 0.04175 	 ~...
   67 	    35 	 0.06712 	 0.04206 	 ~...
   82 	    36 	 0.06892 	 0.04221 	 ~...
   76 	    37 	 0.06832 	 0.04221 	 ~...
   78 	    38 	 0.06840 	 0.04282 	 ~...
   58 	    39 	 0.06623 	 0.04395 	 ~...
   28 	    40 	 0.06396 	 0.04403 	 ~...
   30 	    41 	 0.06413 	 0.04407 	 ~...
    6 	    42 	 0.06075 	 0.04437 	 ~...
   17 	    43 	 0.06302 	 0.04473 	 ~...
   56 	    44 	 0.06609 	 0.04474 	 ~...
   21 	    45 	 0.06327 	 0.04475 	 ~...
   23 	    46 	 0.06343 	 0.04514 	 ~...
   70 	    47 	 0.06742 	 0.04517 	 ~...
   24 	    48 	 0.06383 	 0.04537 	 ~...
   73 	    49 	 0.06800 	 0.04549 	 ~...
    6 	    50 	 0.06075 	 0.04559 	 ~...
   65 	    51 	 0.06689 	 0.04564 	 ~...
   51 	    52 	 0.06569 	 0.04594 	 ~...
   72 	    53 	 0.06749 	 0.04617 	 ~...
   32 	    54 	 0.06441 	 0.04620 	 ~...
   44 	    55 	 0.06526 	 0.04626 	 ~...
   36 	    56 	 0.06457 	 0.04652 	 ~...
   37 	    57 	 0.06476 	 0.04676 	 ~...
   24 	    58 	 0.06383 	 0.04750 	 ~...
   70 	    59 	 0.06742 	 0.04760 	 ~...
   33 	    60 	 0.06441 	 0.04760 	 ~...
   42 	    61 	 0.06496 	 0.04782 	 ~...
   59 	    62 	 0.06634 	 0.04820 	 ~...
   27 	    63 	 0.06393 	 0.04854 	 ~...
   41 	    64 	 0.06488 	 0.04866 	 ~...
   10 	    65 	 0.06114 	 0.04871 	 ~...
   78 	    66 	 0.06840 	 0.04876 	 ~...
   64 	    67 	 0.06682 	 0.04895 	 ~...
    5 	    68 	 0.06017 	 0.04897 	 ~...
   17 	    69 	 0.06302 	 0.04911 	 ~...
   39 	    70 	 0.06486 	 0.04955 	 ~...
   17 	    71 	 0.06302 	 0.04970 	 ~...
   40 	    72 	 0.06487 	 0.04973 	 ~...
   73 	    73 	 0.06800 	 0.04976 	 ~...
   83 	    74 	 0.06910 	 0.04989 	 ~...
   57 	    75 	 0.06618 	 0.04991 	 ~...
   68 	    76 	 0.06721 	 0.05007 	 ~...
   43 	    77 	 0.06513 	 0.05016 	 ~...
   20 	    78 	 0.06303 	 0.05037 	 ~...
   15 	    79 	 0.06235 	 0.05089 	 ~...
   81 	    80 	 0.06868 	 0.05125 	 ~...
   87 	    81 	 0.07150 	 0.05128 	 ~...
   50 	    82 	 0.06564 	 0.05151 	 ~...
    8 	    83 	 0.06094 	 0.05160 	 ~...
   47 	    84 	 0.06535 	 0.05208 	 ~...
   38 	    85 	 0.06482 	 0.05238 	 ~...
   61 	    86 	 0.06648 	 0.05299 	 ~...
   66 	    87 	 0.06707 	 0.05299 	 ~...
   45 	    88 	 0.06528 	 0.05518 	 ~...
   90 	    89 	 0.24353 	 0.18867 	 m..s
   91 	    90 	 0.24645 	 0.19157 	 m..s
   94 	    91 	 0.25261 	 0.19234 	 m..s
   89 	    92 	 0.24346 	 0.20295 	 m..s
   99 	    93 	 0.26104 	 0.20936 	 m..s
  108 	    94 	 0.29739 	 0.21142 	 m..s
   98 	    95 	 0.25969 	 0.21298 	 m..s
  102 	    96 	 0.27114 	 0.21318 	 m..s
  106 	    97 	 0.28384 	 0.21997 	 m..s
  105 	    98 	 0.27986 	 0.22969 	 m..s
  101 	    99 	 0.26820 	 0.23182 	 m..s
  108 	   100 	 0.29739 	 0.24218 	 m..s
   96 	   101 	 0.25403 	 0.24354 	 ~...
  103 	   102 	 0.27914 	 0.24515 	 m..s
   97 	   103 	 0.25739 	 0.24656 	 ~...
   92 	   104 	 0.25240 	 0.24680 	 ~...
  103 	   105 	 0.27914 	 0.25298 	 ~...
   92 	   106 	 0.25240 	 0.25327 	 ~...
  110 	   107 	 0.30610 	 0.25916 	 m..s
  100 	   108 	 0.26289 	 0.26949 	 ~...
   95 	   109 	 0.25269 	 0.27224 	 ~...
  110 	   110 	 0.30610 	 0.27290 	 m..s
  112 	   111 	 0.30968 	 0.27479 	 m..s
  113 	   112 	 0.31750 	 0.28534 	 m..s
  114 	   113 	 0.32473 	 0.28862 	 m..s
  107 	   114 	 0.29691 	 0.29550 	 ~...
  118 	   115 	 0.35025 	 0.31214 	 m..s
  119 	   116 	 0.36168 	 0.32376 	 m..s
  119 	   117 	 0.36168 	 0.37683 	 ~...
  116 	   118 	 0.33838 	 0.40129 	 m..s
  117 	   119 	 0.34004 	 0.52223 	 MISS
  115 	   120 	 0.32900 	 0.55007 	 MISS
==========================================
r_mrr = 0.954139232635498
r2_mrr = 0.8714951872825623
spearmanr_mrr@5 = 0.9668601751327515
spearmanr_mrr@10 = 0.920123815536499
spearmanr_mrr@50 = 0.9430325627326965
spearmanr_mrr@100 = 0.9665108919143677
spearmanr_mrr@All = 0.9690337777137756
==========================================
test time: 0.458
Done Testing dataset UMLS
total time taken: 232.8078875541687
training time taken: 226.51285099983215
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9541)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.8715)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9669)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9201)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9430)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9665)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9690)}}, 'test_loss': {'ComplEx': {'UMLS': 1.5856923123938031}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 2946777720149246
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1191, 1198, 844, 764, 967, 1108, 163, 997, 475, 406, 1032, 438, 10, 301, 598, 114, 766, 94, 1189, 1073, 239, 221, 1106, 459, 299, 1094, 825, 1059, 188, 603, 718, 285, 729, 1153, 461, 228, 982, 211, 328, 638, 747, 251, 914, 183, 119, 890, 78, 23, 678, 270, 1070, 112, 387, 149, 16, 518, 369, 871, 317, 477, 271, 423, 582, 1083, 1197, 207, 1145, 541, 605, 971, 195, 796, 452, 556, 1107, 485, 1125, 595, 1130, 504, 64, 93, 927, 643, 1181, 466, 142, 912, 201, 933, 490, 560, 763, 618, 1014, 783, 876, 327, 1148, 234, 401, 362, 963, 469, 591, 516, 744, 889, 36, 86, 934, 510, 500, 283, 891, 378, 960, 1211, 430, 937, 751]
valid_ids (0): []
train_ids (1094): [250, 448, 872, 657, 139, 802, 1116, 118, 403, 232, 690, 315, 263, 680, 677, 205, 588, 414, 198, 335, 1007, 1017, 1087, 361, 351, 1000, 51, 101, 1149, 993, 219, 364, 190, 642, 529, 308, 374, 561, 865, 800, 75, 812, 12, 174, 257, 822, 458, 27, 587, 1018, 537, 377, 359, 859, 658, 321, 1174, 1127, 870, 907, 853, 622, 137, 676, 866, 241, 615, 1185, 838, 383, 606, 509, 50, 204, 837, 750, 863, 433, 136, 84, 609, 372, 238, 206, 334, 1011, 147, 661, 972, 542, 793, 1203, 757, 132, 584, 696, 453, 717, 1028, 412, 88, 1023, 399, 245, 808, 512, 92, 897, 165, 65, 1192, 1112, 1204, 619, 557, 942, 956, 172, 887, 881, 987, 592, 602, 1043, 69, 919, 616, 784, 1140, 330, 1027, 77, 995, 670, 1114, 140, 666, 1133, 878, 181, 858, 1054, 624, 631, 760, 1210, 544, 291, 262, 267, 386, 1057, 148, 1119, 70, 1035, 604, 1172, 809, 737, 26, 146, 325, 868, 303, 265, 687, 1209, 785, 277, 1177, 476, 21, 54, 811, 445, 502, 305, 958, 788, 1193, 947, 425, 1040, 135, 553, 1051, 978, 884, 765, 166, 924, 363, 572, 778, 813, 786, 220, 1080, 869, 820, 679, 667, 520, 311, 79, 635, 662, 1199, 429, 877, 422, 906, 53, 713, 704, 324, 600, 979, 523, 768, 545, 155, 1179, 431, 154, 959, 489, 388, 437, 495, 1086, 629, 353, 1121, 90, 157, 72, 814, 875, 216, 105, 19, 748, 1006, 885, 227, 1205, 496, 945, 860, 835, 96, 306, 319, 625, 1030, 352, 948, 381, 1202, 1024, 462, 25, 1074, 307, 883, 152, 1136, 240, 761, 827, 652, 1100, 217, 513, 1056, 411, 464, 955, 707, 1003, 1176, 338, 597, 224, 432, 1009, 428, 138, 1019, 98, 733, 444, 856, 867, 612, 436, 567, 1013, 797, 607, 536, 4, 3, 1103, 734, 375, 177, 688, 357, 492, 1122, 486, 894, 392, 834, 1010, 903, 918, 845, 1058, 76, 274, 134, 235, 282, 817, 628, 904, 0, 532, 850, 49, 1081, 1002, 1161, 692, 725, 930, 196, 585, 81, 530, 921, 465, 647, 596, 39, 929, 479, 37, 184, 31, 511, 1085, 935, 278, 649, 575, 446, 824, 771, 656, 439, 720, 538, 117, 840, 1111, 297, 161, 113, 164, 525, 1012, 176, 440, 331, 427, 836, 191, 329, 1093, 1004, 830, 1170, 455, 1077, 210, 749, 1186, 1098, 549, 173, 233, 1123, 818, 1079, 34, 823, 1214, 347, 555, 491, 370, 129, 281, 1190, 807, 468, 715, 539, 832, 1066, 290, 1092, 418, 703, 527, 1160, 655, 449, 1132, 781, 879, 989, 266, 124, 222, 613, 312, 791, 805, 6, 974, 627, 1049, 546, 554, 954, 861, 373, 57, 395, 68, 899, 531, 304, 1041, 158, 1072, 130, 1104, 1061, 60, 360, 279, 497, 258, 478, 719, 48, 675, 908, 82, 964, 1162, 1105, 208, 41, 1053, 1147, 970, 202, 169, 379, 1099, 52, 1151, 1163, 400, 1206, 524, 116, 1015, 941, 775, 548, 1029, 13, 517, 1052, 237, 225, 571, 922, 1, 43, 849, 528, 326, 623, 759, 731, 63, 66, 313, 474, 882, 900, 193, 650, 754, 1137, 203, 159, 626, 724, 833, 981, 1101, 382, 333, 242, 673, 801, 709, 648, 253, 926, 566, 1064, 794, 1022, 18, 986, 99, 482, 695, 1152, 855, 199, 85, 776, 244, 633, 804, 236, 269, 292, 1129, 246, 699, 435, 151, 209, 186, 799, 975, 131, 1158, 746, 192, 711, 640, 366, 38, 102, 574, 484, 310, 1126, 772, 89, 33, 1165, 819, 1141, 1048, 1071, 637, 790, 255, 434, 153, 722, 35, 671, 773, 1078, 1055, 969, 1039, 864, 442, 632, 2, 29, 547, 231, 1175, 562, 590, 276, 910, 880, 782, 473, 215, 1180, 965, 940, 58, 1062, 726, 332, 994, 74, 946, 44, 294, 417, 59, 471, 145, 32, 573, 973, 415, 913, 636, 936, 91, 55, 569, 714, 180, 223, 443, 932, 498, 1021, 212, 314, 777, 447, 295, 1168, 515, 1008, 456, 911, 17, 1171, 712, 630, 621, 533, 200, 309, 80, 745, 886, 341, 350, 951, 405, 394, 391, 691, 1045, 742, 915, 1155, 20, 700, 1088, 1031, 732, 293, 487, 938, 249, 694, 551, 182, 284, 980, 342, 888, 874, 318, 344, 62, 179, 110, 730, 356, 769, 654, 185, 552, 701, 339, 843, 302, 952, 558, 559, 384, 968, 1201, 122, 286, 1090, 8, 170, 862, 1076, 1115, 9, 289, 961, 108, 998, 376, 962, 774, 390, 905, 581, 1159, 852, 1084, 1033, 1025, 1124, 1036, 1113, 839, 568, 420, 1005, 343, 398, 95, 463, 144, 736, 727, 803, 1046, 610, 893, 348, 999, 419, 787, 992, 1135, 322, 218, 426, 254, 107, 349, 816, 1139, 421, 653, 71, 296, 460, 735, 355, 24, 577, 336, 917, 841, 1089, 540, 522, 815, 753, 563, 273, 645, 741, 873, 128, 990, 1120, 1038, 1144, 681, 697, 1020, 583, 614, 189, 1167, 895, 898, 944, 1016, 288, 1195, 925, 248, 11, 789, 1134, 143, 1047, 684, 826, 550, 413, 586, 708, 916, 22, 404, 187, 261, 716, 127, 28, 1068, 663, 123, 739, 1143, 847, 1001, 646, 1082, 721, 949, 396, 214, 389, 901, 56, 1146, 519, 535, 617, 47, 494, 109, 167, 402, 1102, 1208, 508, 977, 685, 393, 320, 1050, 171, 795, 689, 408, 1091, 424, 892, 1128, 450, 454, 792, 639, 300, 1067, 665, 762, 457, 247, 30, 260, 280, 682, 705, 641, 472, 42, 1138, 7, 743, 779, 104, 470, 14, 40, 410, 1178, 565, 505, 1118, 1164, 1187, 570, 710, 175, 115, 256, 1063, 1075, 156, 416, 1183, 358, 848, 259, 996, 966, 593, 1034, 752, 1156, 168, 160, 578, 976, 1109, 943, 1110, 842, 230, 87, 272, 337, 564, 659, 162, 488, 738, 106, 243, 821, 931, 923, 493, 61, 1182, 126, 756, 483, 985, 1212, 521, 920, 580, 1207, 831, 5, 83, 668, 45, 514, 345, 594, 503, 767, 674, 758, 506, 298, 599, 501, 1060, 1131, 601, 854, 67, 371, 957, 397, 409, 73, 770, 611, 723, 953, 620, 316, 229, 991, 579, 103, 507, 1196, 634, 150, 1065, 576, 851, 1173, 197, 1117, 706, 1154, 683, 828, 672, 1194, 909, 1096, 950, 1097, 1142, 346, 983, 121, 664, 340, 120, 939, 467, 213, 141, 323, 100, 287, 275, 226, 1200, 589, 846, 698, 1213, 441, 385, 252, 928, 1157, 984, 693, 857, 1095, 499, 702, 480, 133, 97, 368, 669, 1169, 1042, 902, 810, 1150, 686, 651, 125, 1044, 526, 1188, 798, 268, 988, 481, 896, 780, 354, 660, 194, 1184, 46, 178, 806, 740, 534, 608, 380, 1166, 111, 644, 755, 543, 829, 1037, 264, 451, 407, 15, 1069, 365, 1026, 728, 367]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1663036519262332
the save name prefix for this run is:  chkpt-ID_1663036519262332_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 773
rank avg (pred): 0.569 +- 0.003
mrr vals (pred, true): 0.013, 0.049
batch losses (mrrl, rdl): 0.0, 0.0004159252

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 616
rank avg (pred): 0.454 +- 0.249
mrr vals (pred, true): 0.115, 0.042
batch losses (mrrl, rdl): 0.0, 3.5971e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 962
rank avg (pred): 0.447 +- 0.257
mrr vals (pred, true): 0.075, 0.040
batch losses (mrrl, rdl): 0.0, 5.2045e-06

Epoch over!
epoch time: 15.131

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1021
rank avg (pred): 0.439 +- 0.258
mrr vals (pred, true): 0.067, 0.044
batch losses (mrrl, rdl): 0.0, 9.4419e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 616
rank avg (pred): 0.461 +- 0.244
mrr vals (pred, true): 0.040, 0.042
batch losses (mrrl, rdl): 0.0, 8.479e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 273
rank avg (pred): 0.167 +- 0.207
mrr vals (pred, true): 0.157, 0.240
batch losses (mrrl, rdl): 0.0, 5.9993e-06

Epoch over!
epoch time: 15.1

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 499
rank avg (pred): 0.508 +- 0.239
mrr vals (pred, true): 0.034, 0.026
batch losses (mrrl, rdl): 0.0, 1.7829e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 292
rank avg (pred): 0.191 +- 0.217
mrr vals (pred, true): 0.144, 0.202
batch losses (mrrl, rdl): 0.0, 4.0303e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 505
rank avg (pred): 0.522 +- 0.234
mrr vals (pred, true): 0.030, 0.030
batch losses (mrrl, rdl): 0.0, 6.4318e-06

Epoch over!
epoch time: 14.962

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1143
rank avg (pred): 0.488 +- 0.243
mrr vals (pred, true): 0.039, 0.022
batch losses (mrrl, rdl): 0.0, 5.3105e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 809
rank avg (pred): 0.445 +- 0.260
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 0.0, 7.9247e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 830
rank avg (pred): 0.130 +- 0.196
mrr vals (pred, true): 0.196, 0.315
batch losses (mrrl, rdl): 0.0, 2.5733e-06

Epoch over!
epoch time: 14.999

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 742
rank avg (pred): 0.137 +- 0.185
mrr vals (pred, true): 0.167, 0.298
batch losses (mrrl, rdl): 0.0, 3.4903e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1005
rank avg (pred): 0.439 +- 0.257
mrr vals (pred, true): 0.043, 0.049
batch losses (mrrl, rdl): 0.0, 8.201e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1140
rank avg (pred): 0.483 +- 0.239
mrr vals (pred, true): 0.032, 0.026
batch losses (mrrl, rdl): 0.0, 3.3259e-06

Epoch over!
epoch time: 15.084

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 941
rank avg (pred): 0.467 +- 0.252
mrr vals (pred, true): 0.037, 0.028
batch losses (mrrl, rdl): 0.0017822376, 8.5945e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1100
rank avg (pred): 0.344 +- 0.163
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 3.06581e-05, 0.0001673669

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 73
rank avg (pred): 0.073 +- 0.053
mrr vals (pred, true): 0.227, 0.216
batch losses (mrrl, rdl): 0.001039174, 0.0002746113

Epoch over!
epoch time: 15.335

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1117
rank avg (pred): 0.360 +- 0.168
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 7.1854e-06, 0.000111796

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1045
rank avg (pred): 0.372 +- 0.180
mrr vals (pred, true): 0.056, 0.044
batch losses (mrrl, rdl): 0.0003202204, 8.41424e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 289
rank avg (pred): 0.107 +- 0.084
mrr vals (pred, true): 0.218, 0.190
batch losses (mrrl, rdl): 0.0078151505, 0.0002770141

Epoch over!
epoch time: 15.33

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 780
rank avg (pred): 0.354 +- 0.197
mrr vals (pred, true): 0.088, 0.038
batch losses (mrrl, rdl): 0.0146712139, 0.0001471758

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 899
rank avg (pred): 0.391 +- 0.184
mrr vals (pred, true): 0.052, 0.055
batch losses (mrrl, rdl): 3.31188e-05, 4.48778e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 976
rank avg (pred): 0.068 +- 0.090
mrr vals (pred, true): 0.290, 0.301
batch losses (mrrl, rdl): 0.001173822, 0.0001776981

Epoch over!
epoch time: 15.322

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 575
rank avg (pred): 0.426 +- 0.150
mrr vals (pred, true): 0.034, 0.041
batch losses (mrrl, rdl): 0.0026420378, 4.56301e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 514
rank avg (pred): 0.412 +- 0.175
mrr vals (pred, true): 0.049, 0.024
batch losses (mrrl, rdl): 3.9848e-06, 0.00019087

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 948
rank avg (pred): 0.420 +- 0.154
mrr vals (pred, true): 0.033, 0.053
batch losses (mrrl, rdl): 0.002973329, 4.87834e-05

Epoch over!
epoch time: 15.285

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 277
rank avg (pred): 0.156 +- 0.135
mrr vals (pred, true): 0.217, 0.260
batch losses (mrrl, rdl): 0.0184386075, 1.06451e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 537
rank avg (pred): 0.427 +- 0.172
mrr vals (pred, true): 0.046, 0.024
batch losses (mrrl, rdl): 0.0001599071, 0.0001117438

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1120
rank avg (pred): 0.436 +- 0.181
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 2.6325e-05, 2.51532e-05

Epoch over!
epoch time: 15.243

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 257
rank avg (pred): 0.188 +- 0.146
mrr vals (pred, true): 0.232, 0.243
batch losses (mrrl, rdl): 0.0012104672, 2.10327e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 817
rank avg (pred): 0.063 +- 0.127
mrr vals (pred, true): 0.374, 0.230
batch losses (mrrl, rdl): 0.2087529004, 0.0001712978

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 952
rank avg (pred): 0.458 +- 0.147
mrr vals (pred, true): 0.030, 0.049
batch losses (mrrl, rdl): 0.0038929074, 4.15529e-05

Epoch over!
epoch time: 15.118

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1049
rank avg (pred): 0.429 +- 0.190
mrr vals (pred, true): 0.060, 0.044
batch losses (mrrl, rdl): 0.0009180205, 2.78018e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 932
rank avg (pred): 0.437 +- 0.158
mrr vals (pred, true): 0.039, 0.036
batch losses (mrrl, rdl): 0.0011215319, 1.96944e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 57
rank avg (pred): 0.262 +- 0.175
mrr vals (pred, true): 0.187, 0.243
batch losses (mrrl, rdl): 0.0311046354, 0.00027786

Epoch over!
epoch time: 14.995

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 872
rank avg (pred): 0.443 +- 0.196
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001939824, 1.83143e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 609
rank avg (pred): 0.474 +- 0.176
mrr vals (pred, true): 0.039, 0.039
batch losses (mrrl, rdl): 0.0011480125, 4.07765e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 909
rank avg (pred): 0.444 +- 0.160
mrr vals (pred, true): 0.043, 0.023
batch losses (mrrl, rdl): 0.0005283303, 0.0002643282

Epoch over!
epoch time: 14.998

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 807
rank avg (pred): 0.429 +- 0.180
mrr vals (pred, true): 0.055, 0.052
batch losses (mrrl, rdl): 0.0002453214, 2.71862e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 859
rank avg (pred): 0.425 +- 0.180
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001936456, 2.13337e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 950
rank avg (pred): 0.447 +- 0.171
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 0.0001203756, 2.79886e-05

Epoch over!
epoch time: 15.011

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 227
rank avg (pred): 0.432 +- 0.182
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0002003167, 2.43413e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1033
rank avg (pred): 0.477 +- 0.218
mrr vals (pred, true): 0.058, 0.047
batch losses (mrrl, rdl): 0.0005957944, 7.2701e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 431
rank avg (pred): 0.423 +- 0.172
mrr vals (pred, true): 0.057, 0.056
batch losses (mrrl, rdl): 0.0005328305, 3.13026e-05

Epoch over!
epoch time: 15.07

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.470 +- 0.200
mrr vals (pred, true): 0.053, 0.052

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   28 	     0 	 0.05148 	 0.02004 	 m..s
   13 	     1 	 0.05043 	 0.02094 	 ~...
    6 	     2 	 0.04832 	 0.02149 	 ~...
   96 	     3 	 0.06692 	 0.02154 	 m..s
    8 	     4 	 0.04956 	 0.02428 	 ~...
   26 	     5 	 0.05145 	 0.02453 	 ~...
    5 	     6 	 0.04824 	 0.02460 	 ~...
   11 	     7 	 0.04992 	 0.02479 	 ~...
    2 	     8 	 0.04586 	 0.02497 	 ~...
    3 	     9 	 0.04668 	 0.02530 	 ~...
    0 	    10 	 0.04482 	 0.02536 	 ~...
    4 	    11 	 0.04793 	 0.02554 	 ~...
   26 	    12 	 0.05145 	 0.02579 	 ~...
   22 	    13 	 0.05104 	 0.02721 	 ~...
    1 	    14 	 0.04571 	 0.02794 	 ~...
   30 	    15 	 0.05166 	 0.02849 	 ~...
   20 	    16 	 0.05096 	 0.02979 	 ~...
   31 	    17 	 0.05184 	 0.03629 	 ~...
   14 	    18 	 0.05052 	 0.03637 	 ~...
   18 	    19 	 0.05094 	 0.03663 	 ~...
   37 	    20 	 0.05317 	 0.03798 	 ~...
   44 	    21 	 0.05342 	 0.03815 	 ~...
   14 	    22 	 0.05052 	 0.03856 	 ~...
   57 	    23 	 0.05378 	 0.03888 	 ~...
   50 	    24 	 0.05349 	 0.03923 	 ~...
   52 	    25 	 0.05356 	 0.03950 	 ~...
   94 	    26 	 0.06177 	 0.04022 	 ~...
   42 	    27 	 0.05335 	 0.04053 	 ~...
   54 	    28 	 0.05359 	 0.04088 	 ~...
   29 	    29 	 0.05166 	 0.04101 	 ~...
   53 	    30 	 0.05356 	 0.04108 	 ~...
   82 	    31 	 0.05419 	 0.04147 	 ~...
   70 	    32 	 0.05416 	 0.04175 	 ~...
   76 	    33 	 0.05418 	 0.04214 	 ~...
   58 	    34 	 0.05383 	 0.04224 	 ~...
   36 	    35 	 0.05304 	 0.04252 	 ~...
   12 	    36 	 0.05037 	 0.04270 	 ~...
   73 	    37 	 0.05417 	 0.04278 	 ~...
   24 	    38 	 0.05106 	 0.04279 	 ~...
   84 	    39 	 0.05420 	 0.04292 	 ~...
   86 	    40 	 0.05420 	 0.04301 	 ~...
   95 	    41 	 0.06247 	 0.04319 	 ~...
   47 	    42 	 0.05345 	 0.04355 	 ~...
   85 	    43 	 0.05420 	 0.04364 	 ~...
   90 	    44 	 0.05423 	 0.04381 	 ~...
   55 	    45 	 0.05369 	 0.04395 	 ~...
   71 	    46 	 0.05417 	 0.04403 	 ~...
   87 	    47 	 0.05421 	 0.04463 	 ~...
   83 	    48 	 0.05420 	 0.04466 	 ~...
   45 	    49 	 0.05343 	 0.04471 	 ~...
   22 	    50 	 0.05104 	 0.04494 	 ~...
   40 	    51 	 0.05328 	 0.04497 	 ~...
   41 	    52 	 0.05332 	 0.04524 	 ~...
   59 	    53 	 0.05384 	 0.04533 	 ~...
   68 	    54 	 0.05416 	 0.04544 	 ~...
   35 	    55 	 0.05275 	 0.04564 	 ~...
   75 	    56 	 0.05418 	 0.04584 	 ~...
   33 	    57 	 0.05272 	 0.04599 	 ~...
   80 	    58 	 0.05419 	 0.04608 	 ~...
   72 	    59 	 0.05417 	 0.04668 	 ~...
   19 	    60 	 0.05096 	 0.04670 	 ~...
   74 	    61 	 0.05417 	 0.04705 	 ~...
   63 	    62 	 0.05413 	 0.04724 	 ~...
   51 	    63 	 0.05352 	 0.04747 	 ~...
   32 	    64 	 0.05216 	 0.04788 	 ~...
   21 	    65 	 0.05104 	 0.04819 	 ~...
   66 	    66 	 0.05416 	 0.04843 	 ~...
   91 	    67 	 0.05591 	 0.04848 	 ~...
   38 	    68 	 0.05318 	 0.04897 	 ~...
   89 	    69 	 0.05423 	 0.04928 	 ~...
   60 	    70 	 0.05404 	 0.04939 	 ~...
   43 	    71 	 0.05339 	 0.04958 	 ~...
   76 	    72 	 0.05418 	 0.04970 	 ~...
   61 	    73 	 0.05410 	 0.04974 	 ~...
    7 	    74 	 0.04918 	 0.05025 	 ~...
   78 	    75 	 0.05418 	 0.05051 	 ~...
   39 	    76 	 0.05328 	 0.05108 	 ~...
   81 	    77 	 0.05419 	 0.05115 	 ~...
   56 	    78 	 0.05372 	 0.05129 	 ~...
   49 	    79 	 0.05346 	 0.05160 	 ~...
   64 	    80 	 0.05416 	 0.05160 	 ~...
   88 	    81 	 0.05423 	 0.05184 	 ~...
   61 	    82 	 0.05410 	 0.05214 	 ~...
   48 	    83 	 0.05346 	 0.05232 	 ~...
   92 	    84 	 0.05666 	 0.05238 	 ~...
   34 	    85 	 0.05273 	 0.05275 	 ~...
   65 	    86 	 0.05416 	 0.05324 	 ~...
   25 	    87 	 0.05134 	 0.05339 	 ~...
   68 	    88 	 0.05416 	 0.05354 	 ~...
   16 	    89 	 0.05067 	 0.05367 	 ~...
   17 	    90 	 0.05073 	 0.05371 	 ~...
   10 	    91 	 0.04989 	 0.05439 	 ~...
   46 	    92 	 0.05343 	 0.05450 	 ~...
   93 	    93 	 0.06067 	 0.05518 	 ~...
    9 	    94 	 0.04964 	 0.05546 	 ~...
   78 	    95 	 0.05418 	 0.05801 	 ~...
   66 	    96 	 0.05416 	 0.06248 	 ~...
  105 	    97 	 0.21623 	 0.18352 	 m..s
  106 	    98 	 0.22028 	 0.18625 	 m..s
   97 	    99 	 0.18400 	 0.18627 	 ~...
  104 	   100 	 0.21144 	 0.19773 	 ~...
  108 	   101 	 0.22246 	 0.19986 	 ~...
   99 	   102 	 0.20244 	 0.20499 	 ~...
  109 	   103 	 0.22356 	 0.20953 	 ~...
  110 	   104 	 0.22533 	 0.21089 	 ~...
   98 	   105 	 0.19169 	 0.22048 	 ~...
  118 	   106 	 0.29141 	 0.23090 	 m..s
  101 	   107 	 0.20386 	 0.23731 	 m..s
  113 	   108 	 0.27443 	 0.24222 	 m..s
  102 	   109 	 0.20503 	 0.24618 	 m..s
  103 	   110 	 0.20590 	 0.25289 	 m..s
  119 	   111 	 0.29201 	 0.25316 	 m..s
  100 	   112 	 0.20366 	 0.25592 	 m..s
  106 	   113 	 0.22028 	 0.26800 	 m..s
  112 	   114 	 0.27423 	 0.27157 	 ~...
  111 	   115 	 0.26890 	 0.28160 	 ~...
  114 	   116 	 0.28389 	 0.28460 	 ~...
  116 	   117 	 0.28969 	 0.28654 	 ~...
  115 	   118 	 0.28471 	 0.29484 	 ~...
  117 	   119 	 0.29056 	 0.31748 	 ~...
  120 	   120 	 0.40633 	 0.37350 	 m..s
==========================================
r_mrr = 0.9810700416564941
r2_mrr = 0.952383279800415
spearmanr_mrr@5 = 0.9411628246307373
spearmanr_mrr@10 = 0.9243903160095215
spearmanr_mrr@50 = 0.9949535131454468
spearmanr_mrr@100 = 0.9958422780036926
spearmanr_mrr@All = 0.9937055110931396
==========================================
test time: 0.454
Done Testing dataset UMLS
total time taken: 233.74037313461304
training time taken: 227.44826436042786
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9811)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9524)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9412)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9244)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9950)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9958)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9937)}}, 'test_loss': {'ComplEx': {'UMLS': 0.2546707422243344}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 188013678276971
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [448, 1140, 706, 938, 920, 713, 512, 955, 343, 247, 251, 650, 624, 1167, 824, 73, 1072, 1052, 315, 31, 1143, 362, 291, 398, 881, 516, 941, 1064, 265, 503, 604, 1028, 993, 359, 583, 1191, 616, 862, 451, 437, 1026, 373, 552, 1208, 934, 301, 915, 555, 827, 102, 111, 490, 456, 261, 755, 727, 271, 211, 921, 811, 59, 1007, 136, 319, 159, 1061, 171, 1042, 815, 368, 700, 514, 417, 699, 903, 104, 355, 491, 878, 345, 679, 924, 865, 684, 848, 1090, 1154, 1111, 868, 329, 1119, 246, 668, 1151, 462, 565, 1011, 392, 1130, 507, 654, 595, 113, 310, 496, 334, 1144, 284, 859, 909, 985, 26, 596, 901, 972, 116, 834, 347, 639, 1006, 64]
valid_ids (0): []
train_ids (1094): [734, 923, 820, 61, 479, 888, 991, 1192, 1084, 775, 959, 344, 433, 60, 976, 761, 337, 214, 85, 235, 103, 78, 790, 300, 1093, 649, 593, 370, 831, 286, 193, 431, 253, 1060, 48, 954, 527, 133, 248, 1166, 323, 629, 348, 690, 902, 95, 515, 1160, 488, 80, 270, 866, 67, 203, 772, 774, 965, 1134, 240, 139, 1145, 622, 471, 958, 267, 852, 750, 625, 1066, 850, 269, 742, 469, 1127, 30, 293, 13, 145, 1124, 427, 1185, 784, 91, 208, 1059, 541, 27, 961, 661, 819, 243, 832, 298, 452, 146, 292, 502, 580, 435, 1152, 495, 169, 33, 705, 289, 328, 696, 330, 672, 776, 358, 646, 52, 242, 1147, 173, 928, 641, 945, 275, 758, 425, 288, 135, 125, 741, 238, 188, 245, 636, 314, 632, 1087, 140, 1039, 974, 953, 789, 485, 4, 1057, 230, 695, 989, 258, 688, 818, 539, 1016, 423, 562, 964, 383, 588, 1073, 1126, 760, 408, 781, 434, 1103, 224, 465, 950, 1034, 1139, 841, 42, 600, 735, 528, 1138, 204, 978, 25, 709, 382, 152, 18, 1010, 1069, 296, 1000, 87, 380, 88, 1070, 256, 883, 162, 384, 1027, 262, 454, 131, 914, 297, 336, 614, 376, 333, 342, 560, 1190, 1189, 49, 36, 659, 160, 788, 179, 510, 890, 1032, 470, 353, 405, 917, 35, 1, 257, 759, 43, 1180, 57, 17, 123, 803, 341, 1047, 872, 1079, 83, 157, 24, 6, 797, 704, 611, 712, 178, 1009, 1213, 281, 693, 236, 806, 810, 143, 0, 129, 1101, 692, 112, 108, 906, 586, 263, 79, 1102, 655, 436, 896, 1062, 278, 364, 187, 498, 1086, 874, 1125, 899, 780, 725, 466, 678, 927, 299, 1179, 174, 844, 324, 77, 1038, 482, 508, 190, 519, 846, 607, 545, 69, 394, 82, 717, 168, 1211, 99, 305, 295, 898, 971, 241, 1053, 492, 949, 664, 254, 418, 463, 520, 1163, 833, 402, 1077, 894, 880, 737, 363, 182, 1020, 1155, 445, 326, 547, 1022, 432, 575, 1033, 360, 925, 1193, 396, 533, 406, 266, 645, 592, 707, 480, 656, 130, 1178, 1108, 674, 826, 1168, 536, 598, 980, 773, 889, 745, 449, 115, 651, 316, 748, 724, 209, 1097, 1099, 986, 711, 200, 121, 181, 340, 829, 54, 155, 177, 1162, 642, 792, 893, 46, 733, 926, 671, 122, 335, 1063, 919, 802, 609, 682, 1199, 1159, 1201, 777, 1049, 1068, 578, 809, 1023, 442, 93, 1210, 500, 723, 569, 1091, 369, 677, 1196, 887, 814, 239, 106, 877, 752, 601, 97, 387, 791, 1209, 1116, 1114, 793, 997, 1165, 1177, 1109, 252, 105, 984, 150, 1129, 154, 134, 1078, 1107, 447, 746, 215, 443, 56, 937, 72, 1149, 1133, 557, 356, 892, 415, 44, 1045, 666, 863, 107, 414, 631, 786, 51, 977, 1182, 563, 210, 956, 232, 703, 313, 932, 566, 764, 633, 1141, 823, 697, 783, 626, 897, 474, 172, 517, 627, 1195, 378, 464, 982, 47, 948, 2, 184, 762, 216, 1002, 572, 352, 1054, 778, 548, 207, 571, 873, 610, 1089, 1104, 603, 581, 264, 75, 272, 867, 1025, 732, 426, 176, 553, 990, 65, 992, 701, 1004, 579, 768, 605, 1136, 1171, 5, 259, 165, 274, 838, 148, 416, 501, 412, 973, 331, 311, 694, 837, 618, 744, 141, 255, 233, 619, 1008, 879, 870, 404, 225, 635, 975, 8, 658, 568, 615, 506, 119, 804, 419, 166, 1001, 808, 205, 836, 497, 101, 570, 918, 1187, 484, 96, 62, 967, 1065, 726, 722, 461, 933, 41, 429, 1153, 637, 845, 856, 98, 529, 321, 673, 411, 339, 1029, 683, 486, 1169, 90, 186, 161, 1015, 936, 453, 1115, 317, 1112, 390, 1118, 280, 708, 1174, 822, 294, 439, 621, 1113, 1173, 460, 499, 74, 351, 638, 11, 1048, 306, 620, 1212, 189, 231, 170, 1003, 1202, 63, 195, 250, 202, 285, 374, 10, 222, 21, 652, 851, 58, 312, 244, 676, 930, 180, 81, 308, 84, 935, 276, 350, 1088, 37, 675, 559, 191, 957, 891, 441, 194, 736, 1204, 346, 494, 1146, 669, 908, 1206, 1161, 1095, 458, 1021, 381, 869, 361, 142, 389, 302, 132, 522, 393, 1150, 147, 525, 151, 770, 1110, 540, 640, 28, 662, 318, 840, 537, 385, 686, 564, 630, 531, 1158, 785, 756, 857, 943, 354, 218, 357, 794, 535, 660, 719, 32, 1198, 981, 1157, 422, 825, 446, 50, 283, 807, 511, 468, 237, 249, 749, 798, 963, 183, 1050, 307, 648, 1031, 766, 7, 667, 574, 573, 864, 1067, 279, 89, 1131, 407, 118, 994, 738, 1044, 1075, 542, 1186, 110, 634, 303, 1071, 613, 702, 413, 591, 226, 504, 623, 397, 403, 916, 39, 710, 117, 1024, 830, 680, 922, 799, 821, 1098, 728, 584, 729, 521, 1156, 260, 332, 475, 1184, 558, 175, 1203, 327, 1043, 290, 585, 375, 716, 409, 1017, 787, 951, 1056, 721, 444, 489, 472, 45, 577, 450, 907, 66, 731, 1018, 1005, 886, 1122, 718, 1040, 969, 882, 1121, 29, 1058, 944, 860, 372, 582, 968, 1092, 526, 843, 282, 338, 400, 1170, 939, 952, 1120, 40, 1105, 556, 1012, 228, 68, 534, 156, 192, 1080, 754, 608, 1200, 153, 587, 518, 430, 128, 467, 219, 395, 979, 23, 849, 998, 1106, 277, 962, 1037, 3, 1181, 1013, 532, 871, 483, 670, 858, 1148, 628, 320, 144, 388, 590, 199, 1081, 367, 795, 94, 842, 647, 1100, 1137, 1096, 322, 1172, 1051, 455, 365, 796, 1117, 853, 546, 38, 15, 86, 714, 904, 206, 905, 910, 687, 549, 19, 550, 1183, 987, 782, 771, 665, 1197, 597, 911, 401, 1135, 213, 1175, 988, 653, 900, 1142, 109, 481, 767, 715, 421, 551, 440, 828, 942, 966, 371, 124, 420, 201, 14, 554, 854, 685, 1164, 876, 234, 839, 273, 995, 1035, 304, 544, 1094, 92, 120, 386, 509, 126, 895, 543, 763, 524, 602, 999, 720, 220, 1207, 55, 366, 138, 801, 813, 1019, 114, 1214, 1014, 391, 309, 164, 473, 730, 1128, 855, 1194, 127, 612, 599, 197, 561, 1055, 698, 1082, 399, 487, 1188, 1041, 478, 817, 983, 76, 884, 223, 753, 1205, 816, 137, 765, 530, 589, 428, 457, 523, 960, 71, 513, 812, 185, 1083, 739, 377, 349, 221, 158, 912, 198, 663, 1176, 740, 149, 594, 1036, 1076, 644, 459, 847, 379, 16, 70, 913, 1123, 212, 217, 227, 196, 410, 163, 885, 996, 606, 493, 1132, 779, 576, 691, 9, 970, 20, 538, 757, 34, 931, 946, 229, 438, 1085, 167, 929, 805, 940, 617, 800, 53, 769, 643, 22, 751, 477, 875, 743, 1074, 747, 681, 424, 689, 268, 947, 1046, 505, 861, 657, 100, 12, 1030, 476, 287, 567, 325, 835]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7414060921254504
the save name prefix for this run is:  chkpt-ID_7414060921254504_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 401
rank avg (pred): 0.558 +- 0.004
mrr vals (pred, true): 0.013, 0.051
batch losses (mrrl, rdl): 0.0, 0.0003614689

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 200
rank avg (pred): 0.444 +- 0.007
mrr vals (pred, true): 0.017, 0.044
batch losses (mrrl, rdl): 0.0, 7.8769e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1038
rank avg (pred): 0.463 +- 0.258
mrr vals (pred, true): 0.032, 0.043
batch losses (mrrl, rdl): 0.0, 9.81e-07

Epoch over!
epoch time: 15.1

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1053
rank avg (pred): 0.173 +- 0.209
mrr vals (pred, true): 0.114, 0.277
batch losses (mrrl, rdl): 0.0, 2.78623e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 463
rank avg (pred): 0.445 +- 0.257
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.0, 5.61e-08

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 137
rank avg (pred): 0.438 +- 0.260
mrr vals (pred, true): 0.042, 0.048
batch losses (mrrl, rdl): 0.0, 4.3492e-06

Epoch over!
epoch time: 15.003

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 181
rank avg (pred): 0.437 +- 0.260
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 0.0, 1.1953e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 617
rank avg (pred): 0.466 +- 0.261
mrr vals (pred, true): 0.033, 0.035
batch losses (mrrl, rdl): 0.0, 1.604e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 435
rank avg (pred): 0.449 +- 0.275
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0, 1.895e-07

Epoch over!
epoch time: 15.038

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 671
rank avg (pred): 0.441 +- 0.261
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.0, 1.0448e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 476
rank avg (pred): 0.431 +- 0.262
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0, 2.2496e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1197
rank avg (pred): 0.477 +- 0.256
mrr vals (pred, true): 0.030, 0.045
batch losses (mrrl, rdl): 0.0, 3.6091e-06

Epoch over!
epoch time: 15.035

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 761
rank avg (pred): 0.438 +- 0.247
mrr vals (pred, true): 0.039, 0.045
batch losses (mrrl, rdl): 0.0, 5.429e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 923
rank avg (pred): 0.423 +- 0.273
mrr vals (pred, true): 0.046, 0.037
batch losses (mrrl, rdl): 0.0, 4.93482e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 340
rank avg (pred): 0.438 +- 0.265
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 0.0, 4.585e-07

Epoch over!
epoch time: 15.008

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 210
rank avg (pred): 0.453 +- 0.255
mrr vals (pred, true): 0.037, 0.044
batch losses (mrrl, rdl): 0.0017304928, 1.49e-08

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1043
rank avg (pred): 0.476 +- 0.207
mrr vals (pred, true): 0.047, 0.056
batch losses (mrrl, rdl): 8.58117e-05, 4.37682e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1156
rank avg (pred): 0.436 +- 0.223
mrr vals (pred, true): 0.061, 0.024
batch losses (mrrl, rdl): 0.0011275873, 4.91753e-05

Epoch over!
epoch time: 15.334

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 553
rank avg (pred): 0.492 +- 0.238
mrr vals (pred, true): 0.039, 0.029
batch losses (mrrl, rdl): 0.0013059455, 6.1189e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 542
rank avg (pred): 0.428 +- 0.201
mrr vals (pred, true): 0.053, 0.027
batch losses (mrrl, rdl): 7.14123e-05, 0.0001244404

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 991
rank avg (pred): 0.242 +- 0.281
mrr vals (pred, true): 0.284, 0.252
batch losses (mrrl, rdl): 0.0099927569, 0.0002811031

Epoch over!
epoch time: 15.21

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 82
rank avg (pred): 0.453 +- 0.198
mrr vals (pred, true): 0.042, 0.040
batch losses (mrrl, rdl): 0.0005821584, 1.84678e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 870
rank avg (pred): 0.463 +- 0.214
mrr vals (pred, true): 0.051, 0.062
batch losses (mrrl, rdl): 1.12909e-05, 7.10669e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 524
rank avg (pred): 0.479 +- 0.222
mrr vals (pred, true): 0.049, 0.023
batch losses (mrrl, rdl): 4.7411e-06, 1.34615e-05

Epoch over!
epoch time: 15.133

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1062
rank avg (pred): 0.176 +- 0.228
mrr vals (pred, true): 0.341, 0.273
batch losses (mrrl, rdl): 0.0461881198, 5.43741e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 773
rank avg (pred): 0.463 +- 0.210
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 9.3776e-06, 3.01753e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 289
rank avg (pred): 0.328 +- 0.229
mrr vals (pred, true): 0.193, 0.190
batch losses (mrrl, rdl): 0.0001187901, 0.0003234253

Epoch over!
epoch time: 15.179

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 847
rank avg (pred): 0.438 +- 0.188
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.61713e-05, 1.93069e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 550
rank avg (pred): 0.488 +- 0.216
mrr vals (pred, true): 0.044, 0.026
batch losses (mrrl, rdl): 0.0003335528, 7.1424e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 617
rank avg (pred): 0.450 +- 0.268
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 5.70993e-05, 1.8602e-06

Epoch over!
epoch time: 15.061

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 854
rank avg (pred): 0.421 +- 0.183
mrr vals (pred, true): 0.048, 0.046
batch losses (mrrl, rdl): 4.56273e-05, 3.02506e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 809
rank avg (pred): 0.430 +- 0.187
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 1.1867e-06, 2.17343e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 91
rank avg (pred): 0.409 +- 0.188
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 7.02675e-05, 2.41968e-05

Epoch over!
epoch time: 15.077

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 505
rank avg (pred): 0.459 +- 0.211
mrr vals (pred, true): 0.051, 0.030
batch losses (mrrl, rdl): 5.9451e-06, 3.10471e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 504
rank avg (pred): 0.456 +- 0.219
mrr vals (pred, true): 0.058, 0.025
batch losses (mrrl, rdl): 0.0006587646, 3.71573e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 69
rank avg (pred): 0.313 +- 0.205
mrr vals (pred, true): 0.217, 0.209
batch losses (mrrl, rdl): 0.0005782676, 0.000378564

Epoch over!
epoch time: 15.069

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 996
rank avg (pred): 0.239 +- 0.209
mrr vals (pred, true): 0.255, 0.295
batch losses (mrrl, rdl): 0.0163043868, 0.0001448495

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 694
rank avg (pred): 0.476 +- 0.256
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0005052333, 2.69098e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1067
rank avg (pred): 0.255 +- 0.246
mrr vals (pred, true): 0.268, 0.316
batch losses (mrrl, rdl): 0.0229904521, 0.0003310815

Epoch over!
epoch time: 15.081

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 187
rank avg (pred): 0.417 +- 0.184
mrr vals (pred, true): 0.054, 0.049
batch losses (mrrl, rdl): 0.0001367316, 2.04022e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1050
rank avg (pred): 0.464 +- 0.201
mrr vals (pred, true): 0.046, 0.045
batch losses (mrrl, rdl): 0.0001462112, 4.85127e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 676
rank avg (pred): 0.436 +- 0.242
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 1.6839e-06, 1.0893e-06

Epoch over!
epoch time: 15.107

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1195
rank avg (pred): 0.400 +- 0.225
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 5.1079e-06, 3.25223e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 206
rank avg (pred): 0.425 +- 0.185
mrr vals (pred, true): 0.052, 0.037
batch losses (mrrl, rdl): 4.48625e-05, 3.14862e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 306
rank avg (pred): 0.319 +- 0.205
mrr vals (pred, true): 0.230, 0.192
batch losses (mrrl, rdl): 0.0145755373, 0.0004237016

Epoch over!
epoch time: 15.084

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.443 +- 0.190
mrr vals (pred, true): 0.046, 0.050

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.04626 	 0.02014 	 ~...
   25 	     1 	 0.04643 	 0.02036 	 ~...
   36 	     2 	 0.04652 	 0.02050 	 ~...
   23 	     3 	 0.04640 	 0.02180 	 ~...
   22 	     4 	 0.04639 	 0.02336 	 ~...
   61 	     5 	 0.04829 	 0.02393 	 ~...
   32 	     6 	 0.04650 	 0.02414 	 ~...
   57 	     7 	 0.04789 	 0.02424 	 ~...
   51 	     8 	 0.04745 	 0.02442 	 ~...
   55 	     9 	 0.04764 	 0.02442 	 ~...
   59 	    10 	 0.04814 	 0.02467 	 ~...
   52 	    11 	 0.04750 	 0.02479 	 ~...
   61 	    12 	 0.04829 	 0.02497 	 ~...
   54 	    13 	 0.04764 	 0.02544 	 ~...
   63 	    14 	 0.04832 	 0.02561 	 ~...
   64 	    15 	 0.04849 	 0.02569 	 ~...
   26 	    16 	 0.04643 	 0.02571 	 ~...
   56 	    17 	 0.04765 	 0.02626 	 ~...
   60 	    18 	 0.04828 	 0.02633 	 ~...
   53 	    19 	 0.04754 	 0.02638 	 ~...
   81 	    20 	 0.05064 	 0.02843 	 ~...
   68 	    21 	 0.05001 	 0.02964 	 ~...
   91 	    22 	 0.05099 	 0.02979 	 ~...
   69 	    23 	 0.05018 	 0.03016 	 ~...
   71 	    24 	 0.05022 	 0.03057 	 ~...
   77 	    25 	 0.05035 	 0.03213 	 ~...
   78 	    26 	 0.05059 	 0.03611 	 ~...
    3 	    27 	 0.04529 	 0.03721 	 ~...
   85 	    28 	 0.05068 	 0.03727 	 ~...
    2 	    29 	 0.04510 	 0.03920 	 ~...
   18 	    30 	 0.04626 	 0.03971 	 ~...
   47 	    31 	 0.04677 	 0.03976 	 ~...
   80 	    32 	 0.05063 	 0.04042 	 ~...
   82 	    33 	 0.05065 	 0.04088 	 ~...
   82 	    34 	 0.05065 	 0.04115 	 ~...
   66 	    35 	 0.04996 	 0.04139 	 ~...
   43 	    36 	 0.04670 	 0.04147 	 ~...
   75 	    37 	 0.05029 	 0.04168 	 ~...
    0 	    38 	 0.04451 	 0.04255 	 ~...
   50 	    39 	 0.04682 	 0.04276 	 ~...
    6 	    40 	 0.04556 	 0.04278 	 ~...
   15 	    41 	 0.04609 	 0.04303 	 ~...
   76 	    42 	 0.05032 	 0.04311 	 ~...
   39 	    43 	 0.04659 	 0.04330 	 ~...
   89 	    44 	 0.05075 	 0.04337 	 ~...
   12 	    45 	 0.04582 	 0.04369 	 ~...
   74 	    46 	 0.05029 	 0.04396 	 ~...
   88 	    47 	 0.05070 	 0.04397 	 ~...
   45 	    48 	 0.04673 	 0.04460 	 ~...
   21 	    49 	 0.04631 	 0.04485 	 ~...
   72 	    50 	 0.05029 	 0.04507 	 ~...
   27 	    51 	 0.04645 	 0.04527 	 ~...
    8 	    52 	 0.04558 	 0.04527 	 ~...
   65 	    53 	 0.04988 	 0.04537 	 ~...
   14 	    54 	 0.04599 	 0.04549 	 ~...
   48 	    55 	 0.04678 	 0.04551 	 ~...
   38 	    56 	 0.04657 	 0.04610 	 ~...
   86 	    57 	 0.05070 	 0.04641 	 ~...
   30 	    58 	 0.04648 	 0.04657 	 ~...
   11 	    59 	 0.04579 	 0.04674 	 ~...
   34 	    60 	 0.04651 	 0.04687 	 ~...
    7 	    61 	 0.04557 	 0.04705 	 ~...
   24 	    62 	 0.04642 	 0.04734 	 ~...
   13 	    63 	 0.04584 	 0.04736 	 ~...
   79 	    64 	 0.05060 	 0.04748 	 ~...
    1 	    65 	 0.04465 	 0.04755 	 ~...
   33 	    66 	 0.04651 	 0.04789 	 ~...
   49 	    67 	 0.04680 	 0.04793 	 ~...
   40 	    68 	 0.04660 	 0.04804 	 ~...
   35 	    69 	 0.04652 	 0.04824 	 ~...
   67 	    70 	 0.04996 	 0.04836 	 ~...
   27 	    71 	 0.04645 	 0.04836 	 ~...
   58 	    72 	 0.04796 	 0.04898 	 ~...
    5 	    73 	 0.04554 	 0.04923 	 ~...
   90 	    74 	 0.05086 	 0.04949 	 ~...
   31 	    75 	 0.04648 	 0.04951 	 ~...
   16 	    76 	 0.04617 	 0.04958 	 ~...
   41 	    77 	 0.04661 	 0.04958 	 ~...
   42 	    78 	 0.04661 	 0.05007 	 ~...
   72 	    79 	 0.05029 	 0.05030 	 ~...
    4 	    80 	 0.04539 	 0.05091 	 ~...
   10 	    81 	 0.04578 	 0.05092 	 ~...
   87 	    82 	 0.05070 	 0.05160 	 ~...
   84 	    83 	 0.05067 	 0.05162 	 ~...
   37 	    84 	 0.04654 	 0.05305 	 ~...
   18 	    85 	 0.04626 	 0.05466 	 ~...
   29 	    86 	 0.04647 	 0.05518 	 ~...
   45 	    87 	 0.04673 	 0.05851 	 ~...
   44 	    88 	 0.04670 	 0.05954 	 ~...
    9 	    89 	 0.04567 	 0.05989 	 ~...
   70 	    90 	 0.05019 	 0.06019 	 ~...
   17 	    91 	 0.04619 	 0.06389 	 ~...
   97 	    92 	 0.19446 	 0.18224 	 ~...
   93 	    93 	 0.19080 	 0.18803 	 ~...
   92 	    94 	 0.19054 	 0.19602 	 ~...
   96 	    95 	 0.19418 	 0.19958 	 ~...
  103 	    96 	 0.20818 	 0.19986 	 ~...
   94 	    97 	 0.19081 	 0.20922 	 ~...
  112 	    98 	 0.25164 	 0.21142 	 m..s
   95 	    99 	 0.19201 	 0.21637 	 ~...
  100 	   100 	 0.20193 	 0.21862 	 ~...
   98 	   101 	 0.20175 	 0.22050 	 ~...
   99 	   102 	 0.20183 	 0.22546 	 ~...
  105 	   103 	 0.21188 	 0.24226 	 m..s
  107 	   104 	 0.22384 	 0.24233 	 ~...
  106 	   105 	 0.22360 	 0.24618 	 ~...
  110 	   106 	 0.24975 	 0.24876 	 ~...
  101 	   107 	 0.20479 	 0.25289 	 m..s
  107 	   108 	 0.22384 	 0.25327 	 ~...
  102 	   109 	 0.20499 	 0.25592 	 m..s
  109 	   110 	 0.24805 	 0.26624 	 ~...
  104 	   111 	 0.21179 	 0.26711 	 m..s
  111 	   112 	 0.25135 	 0.28353 	 m..s
  116 	   113 	 0.26580 	 0.28472 	 ~...
  117 	   114 	 0.28379 	 0.28722 	 ~...
  115 	   115 	 0.26525 	 0.28800 	 ~...
  114 	   116 	 0.26219 	 0.28917 	 ~...
  118 	   117 	 0.29523 	 0.30203 	 ~...
  113 	   118 	 0.26085 	 0.30688 	 m..s
  120 	   119 	 0.46520 	 0.40129 	 m..s
  119 	   120 	 0.43090 	 0.53924 	 MISS
==========================================
r_mrr = 0.9832027554512024
r2_mrr = 0.9572120904922485
spearmanr_mrr@5 = 0.9390877485275269
spearmanr_mrr@10 = 0.9547597765922546
spearmanr_mrr@50 = 0.9934538006782532
spearmanr_mrr@100 = 0.9951724410057068
spearmanr_mrr@All = 0.9932268857955933
==========================================
test time: 0.451
Done Testing dataset UMLS
total time taken: 233.28916239738464
training time taken: 226.97819828987122
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9832)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9572)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9391)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9548)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9935)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9952)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9932)}}, 'test_loss': {'ComplEx': {'UMLS': 0.3787250399509503}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 8444590309522739
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [76, 165, 1189, 1037, 297, 477, 961, 298, 419, 1020, 662, 283, 252, 1198, 217, 107, 625, 53, 718, 309, 1026, 719, 1126, 695, 276, 336, 63, 775, 966, 513, 69, 9, 698, 450, 933, 1138, 476, 796, 1068, 1158, 833, 468, 246, 1109, 764, 1072, 1167, 720, 1173, 866, 249, 488, 219, 347, 663, 949, 1036, 12, 870, 948, 378, 1103, 478, 624, 729, 105, 1120, 587, 1062, 907, 205, 597, 694, 969, 575, 245, 728, 794, 186, 1063, 1153, 1134, 1028, 810, 855, 380, 51, 777, 655, 84, 376, 548, 388, 512, 1106, 418, 1172, 860, 287, 807, 1097, 492, 1016, 788, 1193, 199, 881, 830, 52, 772, 838, 504, 940, 1168, 67, 1092, 639, 410, 349, 306, 740]
valid_ids (0): []
train_ids (1094): [316, 883, 960, 590, 745, 806, 113, 333, 6, 634, 282, 224, 391, 100, 657, 366, 1075, 439, 344, 24, 676, 85, 467, 771, 364, 1034, 699, 536, 129, 227, 1123, 198, 483, 289, 799, 1061, 218, 882, 660, 352, 626, 659, 299, 592, 399, 516, 572, 1080, 386, 646, 820, 501, 171, 371, 260, 102, 1005, 327, 528, 704, 827, 871, 1132, 658, 184, 1119, 836, 974, 1145, 610, 200, 640, 884, 317, 363, 954, 464, 1130, 578, 278, 749, 593, 632, 161, 737, 191, 222, 1013, 466, 1127, 338, 277, 1212, 1083, 1104, 19, 786, 446, 232, 251, 300, 511, 579, 25, 918, 742, 1024, 325, 425, 550, 911, 226, 598, 411, 677, 782, 343, 586, 842, 875, 1115, 485, 1003, 326, 221, 817, 731, 0, 955, 549, 557, 608, 864, 945, 1078, 95, 18, 1162, 194, 906, 475, 137, 1043, 725, 576, 38, 1164, 987, 151, 43, 116, 1211, 1131, 708, 407, 714, 1204, 319, 872, 233, 329, 210, 917, 484, 717, 862, 398, 45, 1121, 547, 944, 808, 878, 994, 766, 1214, 438, 902, 41, 1160, 888, 111, 818, 259, 1091, 652, 1105, 874, 225, 1066, 527, 192, 971, 859, 992, 507, 611, 1152, 334, 594, 1067, 583, 112, 675, 1056, 1151, 175, 840, 891, 591, 561, 998, 465, 361, 519, 1004, 551, 254, 824, 821, 1009, 279, 440, 953, 1179, 815, 869, 1018, 978, 294, 152, 1015, 189, 795, 1012, 1213, 372, 641, 1038, 711, 423, 1156, 750, 401, 500, 243, 709, 479, 865, 999, 964, 1190, 1186, 1055, 975, 1117, 1114, 724, 1176, 895, 90, 257, 631, 781, 916, 451, 16, 293, 901, 1049, 56, 377, 1031, 1157, 877, 931, 497, 89, 558, 828, 995, 207, 1079, 912, 448, 495, 846, 546, 285, 145, 650, 403, 427, 216, 605, 211, 397, 885, 253, 1202, 898, 685, 1163, 702, 793, 235, 360, 845, 314, 305, 1140, 915, 973, 564, 230, 302, 1101, 890, 664, 447, 988, 80, 453, 751, 413, 730, 124, 1144, 1074, 496, 613, 762, 980, 1141, 288, 643, 726, 1082, 369, 942, 989, 754, 819, 62, 803, 900, 585, 792, 78, 1011, 311, 61, 160, 455, 1139, 541, 435, 963, 441, 48, 4, 710, 854, 537, 125, 602, 164, 588, 979, 991, 458, 573, 193, 798, 722, 134, 935, 7, 957, 1154, 1006, 17, 237, 426, 560, 983, 236, 837, 930, 581, 33, 228, 248, 802, 787, 733, 149, 168, 545, 339, 679, 1107, 402, 665, 540, 672, 746, 559, 263, 647, 926, 35, 101, 616, 150, 568, 91, 542, 385, 32, 959, 269, 522, 517, 370, 11, 582, 622, 144, 462, 1094, 1064, 563, 1051, 108, 82, 571, 206, 58, 908, 744, 445, 104, 812, 554, 437, 518, 400, 460, 456, 143, 1057, 753, 894, 1108, 505, 654, 690, 972, 123, 1021, 805, 684, 159, 47, 538, 977, 280, 472, 923, 158, 167, 574, 223, 118, 135, 39, 619, 10, 743, 521, 14, 270, 858, 117, 1116, 213, 1030, 1178, 984, 1122, 436, 509, 748, 444, 406, 1027, 22, 967, 1175, 706, 1085, 229, 741, 284, 1185, 290, 375, 1194, 274, 180, 739, 539, 351, 498, 968, 928, 331, 785, 404, 889, 629, 201, 530, 1102, 31, 23, 244, 183, 952, 146, 520, 122, 59, 1195, 927, 75, 1135, 357, 36, 1088, 181, 394, 153, 1183, 320, 767, 196, 667, 947, 166, 454, 13, 569, 555, 422, 275, 1201, 524, 127, 487, 301, 345, 212, 98, 723, 81, 374, 382, 162, 1184, 332, 147, 801, 489, 847, 886, 321, 261, 128, 656, 929, 997, 922, 609, 1040, 126, 876, 156, 532, 544, 44, 919, 896, 1071, 379, 1090, 409, 661, 1008, 1035, 1019, 1147, 308, 508, 474, 635, 323, 670, 490, 5, 88, 1205, 307, 687, 1001, 770, 328, 946, 420, 1033, 809, 531, 666, 66, 633, 1060, 615, 982, 1089, 442, 303, 179, 1118, 1196, 50, 494, 849, 29, 1069, 1133, 214, 202, 703, 603, 457, 178, 1045, 242, 873, 1039, 271, 797, 73, 716, 140, 950, 1150, 1041, 177, 848, 1046, 1148, 843, 1146, 256, 491, 757, 553, 1029, 1136, 627, 342, 250, 682, 239, 829, 231, 1014, 638, 700, 296, 503, 1042, 408, 674, 139, 486, 774, 20, 705, 238, 1054, 607, 1199, 783, 822, 72, 1182, 258, 1174, 1099, 241, 1207, 1169, 359, 904, 1050, 697, 292, 965, 350, 3, 30, 70, 390, 46, 892, 94, 1170, 312, 692, 99, 768, 234, 932, 459, 120, 264, 119, 1076, 42, 387, 49, 392, 396, 1208, 804, 693, 905, 247, 1161, 920, 480, 92, 188, 1191, 71, 620, 1053, 368, 880, 909, 141, 93, 636, 132, 1000, 1192, 861, 358, 623, 473, 1166, 169, 1113, 267, 913, 310, 956, 776, 595, 738, 867, 434, 1044, 707, 565, 1111, 903, 506, 841, 962, 514, 163, 823, 157, 384, 1022, 1149, 715, 381, 1181, 77, 780, 649, 1187, 1081, 197, 863, 567, 57, 1047, 470, 562, 1200, 1032, 1155, 130, 64, 552, 760, 173, 1010, 834, 1007, 1077, 8, 255, 1143, 15, 266, 596, 1098, 800, 172, 732, 1058, 668, 1084, 354, 535, 533, 170, 897, 893, 938, 761, 240, 621, 826, 735, 713, 1002, 773, 529, 60, 421, 27, 87, 993, 996, 618, 601, 315, 934, 758, 121, 452, 272, 681, 68, 1197, 790, 482, 2, 1129, 645, 789, 103, 1112, 493, 37, 778, 182, 752, 148, 510, 471, 925, 1203, 1159, 499, 1142, 853, 612, 268, 577, 281, 1070, 1171, 174, 154, 96, 405, 689, 1165, 97, 651, 1025, 40, 262, 831, 449, 110, 734, 109, 1125, 1209, 628, 79, 642, 1128, 1, 791, 1023, 970, 1073, 921, 373, 736, 653, 644, 857, 114, 958, 851, 155, 669, 337, 543, 424, 136, 1110, 106, 696, 365, 187, 678, 839, 131, 1124, 1188, 526, 763, 415, 769, 941, 176, 712, 525, 566, 813, 324, 1086, 1095, 1052, 630, 431, 584, 556, 220, 295, 747, 1100, 1096, 825, 142, 1210, 356, 291, 195, 852, 887, 463, 515, 721, 850, 606, 688, 937, 215, 910, 637, 614, 976, 835, 814, 856, 346, 1087, 1180, 534, 26, 648, 580, 1017, 924, 765, 727, 185, 432, 570, 686, 86, 265, 589, 74, 939, 318, 416, 65, 383, 832, 443, 55, 523, 417, 502, 1206, 412, 868, 208, 429, 691, 353, 341, 599, 83, 680, 811, 286, 304, 393, 335, 469, 330, 844, 115, 1093, 203, 617, 951, 362, 395, 701, 367, 755, 981, 1065, 914, 355, 816, 1059, 28, 34, 190, 600, 943, 759, 673, 671, 21, 985, 348, 879, 683, 1048, 756, 209, 204, 340, 461, 138, 1137, 433, 430, 936, 1177, 779, 784, 481, 54, 899, 322, 414, 604, 273, 313, 389, 986, 428, 133, 990]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4876231479988150
the save name prefix for this run is:  chkpt-ID_4876231479988150_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 975
rank avg (pred): 0.511 +- 0.008
mrr vals (pred, true): 0.014, 0.296
batch losses (mrrl, rdl): 0.0, 0.0027232803

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 571
rank avg (pred): 0.452 +- 0.249
mrr vals (pred, true): 0.169, 0.041
batch losses (mrrl, rdl): 0.0, 1.39445e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 32
rank avg (pred): 0.171 +- 0.103
mrr vals (pred, true): 0.220, 0.232
batch losses (mrrl, rdl): 0.0, 1.92071e-05

Epoch over!
epoch time: 14.887

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 976
rank avg (pred): 0.124 +- 0.075
mrr vals (pred, true): 0.236, 0.301
batch losses (mrrl, rdl): 0.0, 4.39572e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 308
rank avg (pred): 0.165 +- 0.103
mrr vals (pred, true): 0.198, 0.228
batch losses (mrrl, rdl): 0.0, 1.91619e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1175
rank avg (pred): 0.432 +- 0.227
mrr vals (pred, true): 0.117, 0.035
batch losses (mrrl, rdl): 0.0, 8.7008e-06

Epoch over!
epoch time: 14.875

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 257
rank avg (pred): 0.126 +- 0.084
mrr vals (pred, true): 0.243, 0.243
batch losses (mrrl, rdl): 0.0, 5.35409e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 293
rank avg (pred): 0.193 +- 0.125
mrr vals (pred, true): 0.191, 0.214
batch losses (mrrl, rdl): 0.0, 1.46836e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 896
rank avg (pred): 0.531 +- 0.263
mrr vals (pred, true): 0.044, 0.023
batch losses (mrrl, rdl): 0.0, 0.000130789

Epoch over!
epoch time: 14.933

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 912
rank avg (pred): 0.566 +- 0.247
mrr vals (pred, true): 0.029, 0.020
batch losses (mrrl, rdl): 0.0, 1.91532e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 136
rank avg (pred): 0.428 +- 0.265
mrr vals (pred, true): 0.070, 0.043
batch losses (mrrl, rdl): 0.0, 3.4022e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 398
rank avg (pred): 0.443 +- 0.256
mrr vals (pred, true): 0.029, 0.048
batch losses (mrrl, rdl): 0.0, 8.419e-07

Epoch over!
epoch time: 15.102

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 975
rank avg (pred): 0.124 +- 0.142
mrr vals (pred, true): 0.217, 0.296
batch losses (mrrl, rdl): 0.0, 1.57334e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 353
rank avg (pred): 0.451 +- 0.258
mrr vals (pred, true): 0.041, 0.051
batch losses (mrrl, rdl): 0.0, 2.8782e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 970
rank avg (pred): 0.475 +- 0.251
mrr vals (pred, true): 0.030, 0.051
batch losses (mrrl, rdl): 0.0, 2.06103e-05

Epoch over!
epoch time: 15.116

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 825
rank avg (pred): 0.155 +- 0.193
mrr vals (pred, true): 0.164, 0.253
batch losses (mrrl, rdl): 0.0797656998, 2.9127e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 985
rank avg (pred): 0.014 +- 0.000
mrr vals (pred, true): 0.349, 0.285
batch losses (mrrl, rdl): 0.0408385284, 0.000565439

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 661
rank avg (pred): 0.176 +- 0.002
mrr vals (pred, true): 0.041, 0.041
batch losses (mrrl, rdl): 0.0008479696, 0.0017933323

Epoch over!
epoch time: 15.182

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 185
rank avg (pred): 0.256 +- 0.005
mrr vals (pred, true): 0.028, 0.048
batch losses (mrrl, rdl): 0.0047113532, 0.0008258835

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 786
rank avg (pred): 0.128 +- 0.003
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002556707, 0.0022282237

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 931
rank avg (pred): 0.157 +- 0.002
mrr vals (pred, true): 0.045, 0.029
batch losses (mrrl, rdl): 0.0002107379, 0.0022945371

Epoch over!
epoch time: 15.075

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1144
rank avg (pred): 0.138 +- 0.001
mrr vals (pred, true): 0.051, 0.026
batch losses (mrrl, rdl): 2.03115e-05, 0.0028164925

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1171
rank avg (pred): 0.148 +- 0.001
mrr vals (pred, true): 0.048, 0.035
batch losses (mrrl, rdl): 4.00374e-05, 0.0022128539

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1002
rank avg (pred): 0.124 +- 0.001
mrr vals (pred, true): 0.057, 0.042
batch losses (mrrl, rdl): 0.0004317812, 0.0020552496

Epoch over!
epoch time: 15.01

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 140
rank avg (pred): 0.163 +- 0.001
mrr vals (pred, true): 0.044, 0.040
batch losses (mrrl, rdl): 0.0003933877, 0.0017319518

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 666
rank avg (pred): 0.138 +- 0.000
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 1.74759e-05, 0.0022143479

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.166 +- 0.000
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0004850433, 0.0019999496

Epoch over!
epoch time: 15.052

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 319
rank avg (pred): 0.037 +- 0.000
mrr vals (pred, true): 0.166, 0.188
batch losses (mrrl, rdl): 0.0046977871, 0.0006857028

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 146
rank avg (pred): 0.161 +- 0.000
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003229167, 0.0016785761

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1152
rank avg (pred): 0.136 +- 0.000
mrr vals (pred, true): 0.052, 0.024
batch losses (mrrl, rdl): 4.31005e-05, 0.003064339

Epoch over!
epoch time: 15.11

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 531
rank avg (pred): 0.152 +- 0.000
mrr vals (pred, true): 0.047, 0.026
batch losses (mrrl, rdl): 0.0001053849, 0.0028073082

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 285
rank avg (pred): 0.032 +- 0.000
mrr vals (pred, true): 0.191, 0.186
batch losses (mrrl, rdl): 0.0001922204, 0.0006455824

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 541
rank avg (pred): 0.157 +- 0.000
mrr vals (pred, true): 0.045, 0.024
batch losses (mrrl, rdl): 0.0002136865, 0.0029552884

Epoch over!
epoch time: 15.216

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 456
rank avg (pred): 0.153 +- 0.000
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.0001240121, 0.0020675166

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 955
rank avg (pred): 0.133 +- 0.000
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 0.0001094699, 0.0020370667

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 251
rank avg (pred): 0.024 +- 0.000
mrr vals (pred, true): 0.241, 0.246
batch losses (mrrl, rdl): 0.0002697463, 0.0003367432

Epoch over!
epoch time: 15.238

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 930
rank avg (pred): 0.150 +- 0.000
mrr vals (pred, true): 0.047, 0.035
batch losses (mrrl, rdl): 7.54774e-05, 0.0022298314

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 415
rank avg (pred): 0.143 +- 0.000
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 2.7373e-06, 0.0018953501

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 150
rank avg (pred): 0.143 +- 0.000
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 7.499e-07, 0.0020684195

Epoch over!
epoch time: 15.296

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 773
rank avg (pred): 0.136 +- 0.000
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 4.78261e-05, 0.0019818151

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 471
rank avg (pred): 0.149 +- 0.000
mrr vals (pred, true): 0.048, 0.047
batch losses (mrrl, rdl): 5.59362e-05, 0.0021503149

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 47
rank avg (pred): 0.031 +- 0.000
mrr vals (pred, true): 0.195, 0.215
batch losses (mrrl, rdl): 0.0038347822, 0.0004877081

Epoch over!
epoch time: 15.216

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 555
rank avg (pred): 0.156 +- 0.000
mrr vals (pred, true): 0.046, 0.024
batch losses (mrrl, rdl): 0.0001823508, 0.0027258678

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 688
rank avg (pred): 0.150 +- 0.000
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 6.10566e-05, 0.0017066746

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 206
rank avg (pred): 0.150 +- 0.000
mrr vals (pred, true): 0.047, 0.037
batch losses (mrrl, rdl): 6.31477e-05, 0.0020749227

Epoch over!
epoch time: 15.085

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.034 +- 0.000
mrr vals (pred, true): 0.180, 0.219

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04175 	 0.02037 	 ~...
    1 	     1 	 0.04302 	 0.02172 	 ~...
   24 	     2 	 0.04602 	 0.02311 	 ~...
   23 	     3 	 0.04596 	 0.02414 	 ~...
   80 	     4 	 0.05121 	 0.02441 	 ~...
   19 	     5 	 0.04554 	 0.02530 	 ~...
   75 	     6 	 0.05011 	 0.02552 	 ~...
    2 	     7 	 0.04372 	 0.02569 	 ~...
   20 	     8 	 0.04560 	 0.02642 	 ~...
   28 	     9 	 0.04647 	 0.02721 	 ~...
   25 	    10 	 0.04634 	 0.02767 	 ~...
    3 	    11 	 0.04389 	 0.02849 	 ~...
   21 	    12 	 0.04563 	 0.03213 	 ~...
   15 	    13 	 0.04507 	 0.03458 	 ~...
   32 	    14 	 0.04801 	 0.03544 	 ~...
   21 	    15 	 0.04563 	 0.03572 	 ~...
   18 	    16 	 0.04519 	 0.03682 	 ~...
   12 	    17 	 0.04506 	 0.03715 	 ~...
   13 	    18 	 0.04507 	 0.03884 	 ~...
    8 	    19 	 0.04479 	 0.03923 	 ~...
   58 	    20 	 0.04862 	 0.03960 	 ~...
   16 	    21 	 0.04511 	 0.03973 	 ~...
   62 	    22 	 0.04889 	 0.03999 	 ~...
   72 	    23 	 0.04964 	 0.04004 	 ~...
   17 	    24 	 0.04517 	 0.04042 	 ~...
   32 	    25 	 0.04801 	 0.04053 	 ~...
   77 	    26 	 0.05056 	 0.04080 	 ~...
   46 	    27 	 0.04811 	 0.04111 	 ~...
   55 	    28 	 0.04860 	 0.04129 	 ~...
   57 	    29 	 0.04861 	 0.04221 	 ~...
   32 	    30 	 0.04801 	 0.04240 	 ~...
   32 	    31 	 0.04801 	 0.04252 	 ~...
   82 	    32 	 0.05228 	 0.04255 	 ~...
   47 	    33 	 0.04814 	 0.04255 	 ~...
   32 	    34 	 0.04801 	 0.04283 	 ~...
   10 	    35 	 0.04497 	 0.04311 	 ~...
   87 	    36 	 0.06000 	 0.04319 	 ~...
   76 	    37 	 0.05017 	 0.04369 	 ~...
   59 	    38 	 0.04863 	 0.04375 	 ~...
   54 	    39 	 0.04854 	 0.04436 	 ~...
   32 	    40 	 0.04801 	 0.04439 	 ~...
   49 	    41 	 0.04837 	 0.04493 	 ~...
   14 	    42 	 0.04507 	 0.04497 	 ~...
   89 	    43 	 0.08072 	 0.04512 	 m..s
   85 	    44 	 0.05379 	 0.04513 	 ~...
   27 	    45 	 0.04647 	 0.04519 	 ~...
   32 	    46 	 0.04801 	 0.04526 	 ~...
    9 	    47 	 0.04484 	 0.04530 	 ~...
   70 	    48 	 0.04956 	 0.04533 	 ~...
   64 	    49 	 0.04903 	 0.04574 	 ~...
   53 	    50 	 0.04847 	 0.04594 	 ~...
    5 	    51 	 0.04456 	 0.04620 	 ~...
   45 	    52 	 0.04809 	 0.04625 	 ~...
   84 	    53 	 0.05326 	 0.04650 	 ~...
   78 	    54 	 0.05112 	 0.04651 	 ~...
   68 	    55 	 0.04940 	 0.04658 	 ~...
   61 	    56 	 0.04888 	 0.04668 	 ~...
   26 	    57 	 0.04643 	 0.04679 	 ~...
   79 	    58 	 0.05118 	 0.04713 	 ~...
   47 	    59 	 0.04814 	 0.04755 	 ~...
   71 	    60 	 0.04956 	 0.04757 	 ~...
   11 	    61 	 0.04497 	 0.04760 	 ~...
   31 	    62 	 0.04707 	 0.04823 	 ~...
   63 	    63 	 0.04890 	 0.04836 	 ~...
   32 	    64 	 0.04801 	 0.04843 	 ~...
   90 	    65 	 0.09280 	 0.04866 	 m..s
   51 	    66 	 0.04837 	 0.04907 	 ~...
   67 	    67 	 0.04939 	 0.04978 	 ~...
   74 	    68 	 0.04999 	 0.05019 	 ~...
   81 	    69 	 0.05219 	 0.05033 	 ~...
   32 	    70 	 0.04801 	 0.05051 	 ~...
   69 	    71 	 0.04946 	 0.05056 	 ~...
   91 	    72 	 0.09471 	 0.05080 	 m..s
   65 	    73 	 0.04912 	 0.05081 	 ~...
    6 	    74 	 0.04456 	 0.05108 	 ~...
   52 	    75 	 0.04843 	 0.05108 	 ~...
   32 	    76 	 0.04801 	 0.05159 	 ~...
   73 	    77 	 0.04984 	 0.05206 	 ~...
   56 	    78 	 0.04860 	 0.05208 	 ~...
   32 	    79 	 0.04801 	 0.05208 	 ~...
    6 	    80 	 0.04456 	 0.05306 	 ~...
   85 	    81 	 0.05379 	 0.05340 	 ~...
   66 	    82 	 0.04914 	 0.05354 	 ~...
   60 	    83 	 0.04879 	 0.05358 	 ~...
   30 	    84 	 0.04689 	 0.05450 	 ~...
   29 	    85 	 0.04662 	 0.05453 	 ~...
   49 	    86 	 0.04837 	 0.05466 	 ~...
   32 	    87 	 0.04801 	 0.05482 	 ~...
   83 	    88 	 0.05248 	 0.05498 	 ~...
   32 	    89 	 0.04801 	 0.05937 	 ~...
    4 	    90 	 0.04452 	 0.05977 	 ~...
   88 	    91 	 0.06099 	 0.06233 	 ~...
   92 	    92 	 0.17711 	 0.18285 	 ~...
   99 	    93 	 0.20364 	 0.19157 	 ~...
  110 	    94 	 0.23681 	 0.19234 	 m..s
  112 	    95 	 0.24552 	 0.19336 	 m..s
   94 	    96 	 0.18033 	 0.20350 	 ~...
   96 	    97 	 0.19765 	 0.20501 	 ~...
   96 	    98 	 0.19765 	 0.20936 	 ~...
  107 	    99 	 0.21879 	 0.21171 	 ~...
   98 	   100 	 0.20238 	 0.21228 	 ~...
   99 	   101 	 0.20364 	 0.21244 	 ~...
   95 	   102 	 0.18693 	 0.21767 	 m..s
  110 	   103 	 0.23681 	 0.21805 	 ~...
   93 	   104 	 0.18018 	 0.21883 	 m..s
  101 	   105 	 0.20881 	 0.23731 	 ~...
  102 	   106 	 0.21386 	 0.24074 	 ~...
  114 	   107 	 0.26732 	 0.24394 	 ~...
  108 	   108 	 0.21889 	 0.24955 	 m..s
  114 	   109 	 0.26732 	 0.25327 	 ~...
  103 	   110 	 0.21418 	 0.25680 	 m..s
  105 	   111 	 0.21765 	 0.27339 	 m..s
  113 	   112 	 0.26554 	 0.27425 	 ~...
  105 	   113 	 0.21765 	 0.27476 	 m..s
  109 	   114 	 0.21911 	 0.27862 	 m..s
  104 	   115 	 0.21745 	 0.30688 	 m..s
  117 	   116 	 0.32738 	 0.30704 	 ~...
  118 	   117 	 0.33630 	 0.31480 	 ~...
  116 	   118 	 0.31111 	 0.32148 	 ~...
  119 	   119 	 0.51508 	 0.37350 	 MISS
  120 	   120 	 0.52716 	 0.38817 	 MISS
==========================================
r_mrr = 0.96095871925354
r2_mrr = 0.9172039031982422
spearmanr_mrr@5 = 0.9933086633682251
spearmanr_mrr@10 = 0.9736194014549255
spearmanr_mrr@50 = 0.9612138867378235
spearmanr_mrr@100 = 0.9746018648147583
spearmanr_mrr@All = 0.9744246602058411
==========================================
test time: 0.45
Done Testing dataset UMLS
total time taken: 233.04033041000366
training time taken: 226.87477254867554
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9610)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9172)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9933)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9736)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9612)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9746)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9744)}}, 'test_loss': {'ComplEx': {'UMLS': 0.9825525972992182}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 5978971835762898
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [369, 120, 795, 94, 37, 1130, 45, 753, 684, 674, 696, 166, 659, 281, 115, 1064, 167, 584, 435, 230, 151, 103, 1155, 440, 305, 551, 735, 609, 228, 393, 700, 145, 149, 979, 1079, 504, 234, 314, 960, 56, 1180, 245, 448, 755, 642, 152, 127, 450, 1044, 349, 293, 79, 3, 1066, 630, 1060, 987, 612, 477, 489, 126, 456, 134, 377, 338, 1020, 1012, 886, 1095, 318, 853, 449, 434, 452, 692, 1055, 643, 699, 2, 889, 421, 874, 763, 367, 767, 806, 591, 399, 28, 430, 774, 657, 292, 352, 77, 733, 303, 278, 284, 271, 731, 32, 563, 727, 388, 695, 796, 1062, 29, 108, 70, 770, 593, 326, 507, 673, 137, 202, 896, 36, 60]
valid_ids (0): []
train_ids (1094): [836, 219, 906, 1108, 1028, 835, 778, 285, 1188, 701, 539, 88, 231, 82, 344, 1103, 244, 559, 408, 819, 373, 962, 1045, 1074, 636, 924, 365, 378, 572, 30, 663, 1073, 233, 375, 1153, 676, 57, 794, 777, 485, 342, 1000, 904, 486, 1, 150, 279, 73, 1134, 358, 282, 834, 158, 518, 46, 928, 1024, 764, 900, 501, 533, 42, 788, 930, 585, 850, 1147, 514, 320, 1082, 62, 8, 997, 33, 481, 811, 797, 535, 865, 851, 468, 394, 1131, 1009, 210, 100, 154, 1029, 945, 333, 597, 856, 43, 959, 1139, 536, 1206, 474, 807, 1112, 398, 1152, 467, 1038, 144, 54, 756, 649, 466, 654, 236, 901, 1101, 383, 1199, 269, 634, 1173, 520, 578, 723, 1201, 39, 1089, 718, 111, 340, 446, 689, 624, 1058, 482, 223, 698, 222, 498, 1156, 725, 800, 266, 976, 620, 571, 991, 368, 295, 346, 427, 569, 929, 783, 263, 799, 53, 948, 982, 445, 552, 191, 74, 1176, 1174, 980, 509, 419, 250, 64, 1068, 59, 632, 220, 923, 112, 709, 1136, 854, 537, 458, 1181, 490, 878, 55, 354, 156, 1026, 370, 1075, 740, 357, 561, 376, 1013, 613, 267, 10, 617, 493, 195, 883, 229, 1123, 603, 970, 1034, 265, 893, 1115, 206, 50, 1110, 704, 720, 784, 26, 499, 1076, 844, 791, 61, 863, 337, 892, 86, 1172, 290, 1063, 608, 650, 429, 1177, 1190, 93, 644, 1143, 255, 885, 629, 1141, 760, 451, 779, 817, 1184, 528, 175, 538, 633, 803, 25, 882, 18, 666, 589, 312, 992, 905, 833, 23, 1135, 240, 251, 204, 707, 356, 869, 1167, 647, 761, 1121, 1118, 862, 994, 1178, 1146, 384, 1168, 128, 990, 706, 768, 981, 1039, 575, 616, 185, 690, 464, 570, 940, 469, 502, 76, 1050, 762, 957, 1124, 343, 436, 1200, 136, 1195, 804, 1054, 545, 899, 306, 416, 147, 789, 826, 14, 621, 180, 574, 247, 871, 51, 280, 652, 34, 1086, 159, 553, 773, 606, 867, 1003, 1014, 69, 1059, 177, 380, 140, 668, 955, 168, 268, 703, 525, 1070, 772, 734, 876, 179, 732, 550, 442, 1017, 476, 232, 1088, 736, 861, 505, 825, 98, 497, 1032, 832, 323, 996, 1021, 249, 1071, 995, 967, 40, 386, 11, 463, 963, 728, 974, 1025, 347, 403, 895, 1209, 171, 390, 917, 1056, 371, 218, 461, 745, 364, 860, 645, 207, 192, 96, 1030, 106, 586, 546, 161, 888, 837, 679, 238, 1036, 565, 381, 661, 1037, 66, 38, 129, 299, 1212, 1149, 361, 838, 330, 1207, 1057, 1004, 527, 488, 260, 410, 363, 1041, 12, 694, 1120, 24, 397, 710, 1114, 109, 557, 224, 939, 261, 592, 677, 664, 444, 843, 27, 739, 573, 366, 857, 242, 1191, 387, 576, 500, 170, 712, 362, 877, 226, 359, 396, 5, 614, 301, 107, 812, 153, 172, 431, 447, 688, 872, 717, 1154, 201, 726, 311, 691, 71, 830, 534, 667, 0, 1011, 355, 300, 908, 125, 771, 286, 521, 680, 287, 164, 360, 414, 954, 903, 217, 933, 610, 855, 273, 927, 142, 309, 316, 22, 187, 818, 407, 462, 1091, 852, 291, 7, 600, 298, 480, 1019, 105, 952, 588, 155, 1043, 1104, 328, 189, 315, 1016, 622, 988, 926, 160, 915, 880, 969, 313, 749, 814, 200, 526, 524, 846, 656, 548, 1122, 1077, 675, 1093, 582, 289, 389, 67, 637, 457, 41, 351, 759, 1162, 382, 971, 277, 935, 625, 1150, 769, 133, 859, 215, 124, 141, 822, 670, 1144, 197, 47, 567, 516, 748, 583, 983, 1008, 121, 44, 1015, 1163, 925, 214, 1052, 919, 907, 15, 58, 441, 581, 638, 198, 1080, 1182, 671, 1048, 4, 619, 602, 181, 90, 385, 203, 813, 114, 1145, 765, 1100, 78, 868, 472, 437, 824, 6, 395, 35, 747, 785, 491, 635, 782, 989, 21, 640, 1208, 790, 345, 465, 938, 681, 248, 1194, 555, 601, 823, 708, 662, 512, 68, 746, 870, 1179, 506, 1159, 1196, 296, 798, 1203, 805, 669, 473, 958, 544, 423, 1072, 1046, 65, 225, 847, 246, 890, 1160, 227, 986, 1157, 235, 63, 757, 319, 627, 16, 169, 1061, 831, 401, 237, 912, 484, 122, 1113, 1127, 542, 1125, 460, 212, 751, 193, 19, 110, 148, 475, 272, 288, 307, 1211, 325, 993, 417, 426, 116, 776, 1069, 1165, 594, 787, 875, 1205, 420, 839, 335, 239, 372, 693, 322, 91, 99, 428, 302, 257, 579, 174, 540, 658, 1001, 199, 1098, 182, 631, 781, 913, 178, 615, 1002, 711, 20, 503, 1053, 117, 123, 660, 866, 80, 1099, 827, 921, 937, 750, 580, 842, 392, 492, 946, 968, 184, 1106, 391, 157, 879, 519, 941, 332, 510, 132, 495, 998, 1105, 1027, 453, 966, 1119, 262, 840, 716, 702, 118, 902, 1051, 415, 1092, 741, 1006, 729, 549, 808, 329, 183, 598, 81, 918, 1133, 1085, 1161, 264, 494, 83, 909, 422, 1187, 683, 1204, 241, 276, 1164, 1166, 715, 884, 459, 119, 374, 400, 596, 139, 530, 1067, 845, 705, 49, 1040, 1111, 984, 1007, 243, 1158, 1137, 438, 327, 1102, 858, 1065, 775, 1169, 639, 496, 651, 522, 252, 841, 413, 881, 911, 547, 934, 944, 953, 554, 810, 604, 104, 873, 529, 1151, 1186, 424, 999, 914, 1083, 478, 1116, 646, 1214, 221, 211, 849, 1078, 1128, 1109, 411, 1189, 531, 758, 665, 730, 283, 898, 714, 936, 682, 517, 947, 256, 87, 1084, 1018, 931, 949, 821, 253, 321, 942, 13, 828, 829, 17, 891, 802, 102, 961, 1097, 331, 766, 72, 532, 687, 1140, 194, 623, 176, 1132, 820, 412, 560, 590, 1138, 566, 1033, 595, 1183, 568, 894, 562, 965, 1193, 916, 173, 95, 208, 1087, 483, 956, 1192, 425, 190, 738, 628, 641, 697, 165, 722, 724, 186, 310, 487, 686, 848, 213, 815, 897, 1094, 932, 618, 1031, 743, 471, 339, 439, 1210, 443, 607, 1185, 1005, 887, 943, 801, 977, 75, 648, 1049, 341, 577, 1081, 455, 146, 143, 985, 1170, 508, 52, 754, 270, 1142, 744, 742, 1042, 1202, 1010, 543, 479, 135, 209, 1175, 304, 324, 1117, 163, 317, 922, 672, 786, 1035, 433, 752, 97, 541, 864, 1096, 1090, 274, 611, 964, 85, 780, 406, 587, 972, 48, 721, 336, 920, 511, 1023, 1213, 259, 1129, 162, 405, 816, 432, 353, 404, 713, 950, 719, 685, 308, 84, 605, 951, 258, 1171, 350, 564, 793, 1047, 975, 409, 294, 101, 9, 275, 1198, 626, 205, 454, 978, 653, 348, 402, 910, 196, 113, 1197, 1022, 334, 138, 1107, 513, 678, 1126, 130, 973, 556, 558, 418, 216, 379, 792, 131, 737, 655, 297, 188, 599, 89, 809, 31, 254, 1148, 470, 523, 92, 515]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1443753691825442
the save name prefix for this run is:  chkpt-ID_1443753691825442_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 829
rank avg (pred): 0.505 +- 0.007
mrr vals (pred, true): 0.015, 0.336
batch losses (mrrl, rdl): 0.0, 0.0031288436

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 315
rank avg (pred): 0.184 +- 0.190
mrr vals (pred, true): 0.150, 0.196
batch losses (mrrl, rdl): 0.0, 1.444e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 553
rank avg (pred): 0.462 +- 0.270
mrr vals (pred, true): 0.084, 0.029
batch losses (mrrl, rdl): 0.0, 3.92713e-05

Epoch over!
epoch time: 14.918

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 966
rank avg (pred): 0.456 +- 0.263
mrr vals (pred, true): 0.081, 0.041
batch losses (mrrl, rdl): 0.0, 1.01636e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1018
rank avg (pred): 0.451 +- 0.261
mrr vals (pred, true): 0.071, 0.047
batch losses (mrrl, rdl): 0.0, 7.046e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 487
rank avg (pred): 0.516 +- 0.264
mrr vals (pred, true): 0.062, 0.024
batch losses (mrrl, rdl): 0.0, 5.3972e-06

Epoch over!
epoch time: 14.902

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 304
rank avg (pred): 0.163 +- 0.194
mrr vals (pred, true): 0.154, 0.243
batch losses (mrrl, rdl): 0.0, 7.945e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1191
rank avg (pred): 0.453 +- 0.262
mrr vals (pred, true): 0.080, 0.052
batch losses (mrrl, rdl): 0.0, 8.812e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 648
rank avg (pred): 0.453 +- 0.260
mrr vals (pred, true): 0.076, 0.047
batch losses (mrrl, rdl): 0.0, 1.9456e-06

Epoch over!
epoch time: 14.928

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1034
rank avg (pred): 0.478 +- 0.256
mrr vals (pred, true): 0.055, 0.042
batch losses (mrrl, rdl): 0.0, 9.9671e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1069
rank avg (pred): 0.158 +- 0.198
mrr vals (pred, true): 0.170, 0.275
batch losses (mrrl, rdl): 0.0, 2.23232e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1084
rank avg (pred): 0.435 +- 0.265
mrr vals (pred, true): 0.079, 0.042
batch losses (mrrl, rdl): 0.0, 4.8026e-06

Epoch over!
epoch time: 14.941

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 730
rank avg (pred): 0.139 +- 0.201
mrr vals (pred, true): 0.196, 0.386
batch losses (mrrl, rdl): 0.0, 2.45549e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1014
rank avg (pred): 0.440 +- 0.263
mrr vals (pred, true): 0.075, 0.042
batch losses (mrrl, rdl): 0.0, 7.5196e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1137
rank avg (pred): 0.496 +- 0.249
mrr vals (pred, true): 0.059, 0.027
batch losses (mrrl, rdl): 0.0, 4.8824e-06

Epoch over!
epoch time: 14.905

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 20
rank avg (pred): 0.205 +- 0.214
mrr vals (pred, true): 0.136, 0.202
batch losses (mrrl, rdl): 0.0439346395, 1.08585e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 368
rank avg (pred): 0.249 +- 0.019
mrr vals (pred, true): 0.029, 0.053
batch losses (mrrl, rdl): 0.0042894292, 0.000876846

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 811
rank avg (pred): 0.024 +- 0.006
mrr vals (pred, true): 0.255, 0.401
batch losses (mrrl, rdl): 0.2152197212, 0.0001389798

Epoch over!
epoch time: 15.149

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 192
rank avg (pred): 0.277 +- 0.072
mrr vals (pred, true): 0.042, 0.047
batch losses (mrrl, rdl): 0.0006220016, 0.0005928961

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 324
rank avg (pred): 0.300 +- 0.089
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.000343129, 0.0004915141

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.352 +- 0.099
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0005512541, 0.0002276177

Epoch over!
epoch time: 15.086

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 199
rank avg (pred): 0.307 +- 0.091
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0004182353, 0.0005188254

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1174
rank avg (pred): 0.370 +- 0.111
mrr vals (pred, true): 0.050, 0.034
batch losses (mrrl, rdl): 2.2748e-06, 0.0002010452

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 622
rank avg (pred): 0.365 +- 0.105
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.0001924991, 0.0002491762

Epoch over!
epoch time: 15.083

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 219
rank avg (pred): 0.351 +- 0.113
mrr vals (pred, true): 0.058, 0.046
batch losses (mrrl, rdl): 0.0006772567, 0.0002308831

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 898
rank avg (pred): 0.325 +- 0.103
mrr vals (pred, true): 0.053, 0.061
batch losses (mrrl, rdl): 0.0001135983, 0.0001912435

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 193
rank avg (pred): 0.376 +- 0.118
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001794008, 0.0001492332

Epoch over!
epoch time: 15.079

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 557
rank avg (pred): 0.334 +- 0.100
mrr vals (pred, true): 0.044, 0.023
batch losses (mrrl, rdl): 0.0004189719, 0.0007153337

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 797
rank avg (pred): 0.380 +- 0.126
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 8.57194e-05, 9.39829e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 518
rank avg (pred): 0.355 +- 0.117
mrr vals (pred, true): 0.054, 0.026
batch losses (mrrl, rdl): 0.0002008797, 0.0005769879

Epoch over!
epoch time: 15.076

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 832
rank avg (pred): 0.031 +- 0.011
mrr vals (pred, true): 0.265, 0.302
batch losses (mrrl, rdl): 0.0135050695, 0.0002818658

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 784
rank avg (pred): 0.348 +- 0.117
mrr vals (pred, true): 0.057, 0.042
batch losses (mrrl, rdl): 0.0005517914, 0.0002262583

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 493
rank avg (pred): 0.474 +- 0.158
mrr vals (pred, true): 0.050, 0.024
batch losses (mrrl, rdl): 1.5368e-06, 2.75925e-05

Epoch over!
epoch time: 15.102

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 877
rank avg (pred): 0.377 +- 0.126
mrr vals (pred, true): 0.057, 0.046
batch losses (mrrl, rdl): 0.0004565602, 0.0001033597

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1201
rank avg (pred): 0.381 +- 0.118
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 5.5473e-06, 0.0001080369

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 532
rank avg (pred): 0.496 +- 0.176
mrr vals (pred, true): 0.051, 0.025
batch losses (mrrl, rdl): 4.7669e-06, 5.3314e-06

Epoch over!
epoch time: 15.107

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 769
rank avg (pred): 0.393 +- 0.133
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 1.00993e-05, 8.86517e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 952
rank avg (pred): 0.377 +- 0.118
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.0479e-06, 0.00012103

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 926
rank avg (pred): 0.389 +- 0.098
mrr vals (pred, true): 0.035, 0.029
batch losses (mrrl, rdl): 0.0021675157, 0.0002093924

Epoch over!
epoch time: 15.063

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 927
rank avg (pred): 0.393 +- 0.110
mrr vals (pred, true): 0.041, 0.045
batch losses (mrrl, rdl): 0.0008489338, 0.0001344013

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1067
rank avg (pred): 0.031 +- 0.013
mrr vals (pred, true): 0.295, 0.316
batch losses (mrrl, rdl): 0.0047576297, 0.0003156409

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 550
rank avg (pred): 0.552 +- 0.203
mrr vals (pred, true): 0.055, 0.026
batch losses (mrrl, rdl): 0.0002288554, 5.69222e-05

Epoch over!
epoch time: 15.061

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 178
rank avg (pred): 0.409 +- 0.132
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 8.23214e-05, 5.40579e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 336
rank avg (pred): 0.375 +- 0.120
mrr vals (pred, true): 0.054, 0.050
batch losses (mrrl, rdl): 0.0001732735, 0.0001291985

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 698
rank avg (pred): 0.488 +- 0.157
mrr vals (pred, true): 0.045, 0.053
batch losses (mrrl, rdl): 0.0002479107, 0.0001267098

Epoch over!
epoch time: 15.078

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.447 +- 0.120
mrr vals (pred, true): 0.037, 0.052

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   69 	     0 	 0.03957 	 0.02144 	 ~...
   83 	     1 	 0.05446 	 0.02302 	 m..s
   72 	     2 	 0.03968 	 0.02383 	 ~...
   71 	     3 	 0.03967 	 0.02476 	 ~...
   72 	     4 	 0.03968 	 0.02530 	 ~...
   72 	     5 	 0.03968 	 0.02588 	 ~...
   72 	     6 	 0.03968 	 0.02633 	 ~...
   33 	     7 	 0.03679 	 0.03629 	 ~...
   57 	     8 	 0.03698 	 0.03631 	 ~...
   51 	     9 	 0.03689 	 0.03704 	 ~...
   62 	    10 	 0.03878 	 0.03815 	 ~...
   30 	    11 	 0.03678 	 0.03856 	 ~...
   47 	    12 	 0.03686 	 0.03930 	 ~...
   67 	    13 	 0.03937 	 0.04022 	 ~...
   59 	    14 	 0.03711 	 0.04036 	 ~...
   44 	    15 	 0.03686 	 0.04053 	 ~...
   37 	    16 	 0.03680 	 0.04061 	 ~...
   76 	    17 	 0.03981 	 0.04106 	 ~...
   55 	    18 	 0.03695 	 0.04108 	 ~...
   65 	    19 	 0.03915 	 0.04132 	 ~...
   56 	    20 	 0.03696 	 0.04165 	 ~...
    9 	    21 	 0.03659 	 0.04179 	 ~...
   34 	    22 	 0.03679 	 0.04206 	 ~...
   13 	    23 	 0.03666 	 0.04238 	 ~...
   36 	    24 	 0.03680 	 0.04238 	 ~...
   61 	    25 	 0.03804 	 0.04278 	 ~...
   11 	    26 	 0.03660 	 0.04279 	 ~...
   20 	    27 	 0.03671 	 0.04279 	 ~...
   78 	    28 	 0.04014 	 0.04282 	 ~...
   81 	    29 	 0.04246 	 0.04306 	 ~...
   22 	    30 	 0.03673 	 0.04340 	 ~...
   49 	    31 	 0.03687 	 0.04353 	 ~...
   43 	    32 	 0.03685 	 0.04364 	 ~...
   54 	    33 	 0.03693 	 0.04373 	 ~...
   53 	    34 	 0.03692 	 0.04396 	 ~...
    8 	    35 	 0.03657 	 0.04431 	 ~...
   16 	    36 	 0.03668 	 0.04444 	 ~...
   14 	    37 	 0.03667 	 0.04463 	 ~...
   45 	    38 	 0.03686 	 0.04507 	 ~...
   39 	    39 	 0.03680 	 0.04518 	 ~...
   17 	    40 	 0.03669 	 0.04531 	 ~...
   66 	    41 	 0.03922 	 0.04533 	 ~...
   48 	    42 	 0.03687 	 0.04543 	 ~...
    6 	    43 	 0.03650 	 0.04594 	 ~...
    7 	    44 	 0.03652 	 0.04608 	 ~...
   32 	    45 	 0.03679 	 0.04680 	 ~...
    4 	    46 	 0.03648 	 0.04690 	 ~...
   35 	    47 	 0.03680 	 0.04722 	 ~...
   64 	    48 	 0.03901 	 0.04724 	 ~...
   12 	    49 	 0.03664 	 0.04744 	 ~...
   46 	    50 	 0.03686 	 0.04749 	 ~...
   38 	    51 	 0.03680 	 0.04757 	 ~...
    5 	    52 	 0.03650 	 0.04758 	 ~...
   70 	    53 	 0.03966 	 0.04759 	 ~...
    2 	    54 	 0.03646 	 0.04775 	 ~...
    3 	    55 	 0.03646 	 0.04778 	 ~...
   79 	    56 	 0.04022 	 0.04782 	 ~...
   27 	    57 	 0.03678 	 0.04843 	 ~...
   40 	    58 	 0.03681 	 0.04852 	 ~...
    0 	    59 	 0.03629 	 0.04868 	 ~...
   21 	    60 	 0.03673 	 0.04879 	 ~...
   68 	    61 	 0.03940 	 0.04895 	 ~...
   24 	    62 	 0.03675 	 0.04951 	 ~...
   50 	    63 	 0.03688 	 0.04964 	 ~...
   60 	    64 	 0.03774 	 0.05007 	 ~...
   82 	    65 	 0.04261 	 0.05012 	 ~...
   52 	    66 	 0.03691 	 0.05030 	 ~...
   42 	    67 	 0.03683 	 0.05108 	 ~...
   80 	    68 	 0.04030 	 0.05108 	 ~...
   15 	    69 	 0.03668 	 0.05158 	 ~...
   26 	    70 	 0.03678 	 0.05159 	 ~...
   31 	    71 	 0.03678 	 0.05160 	 ~...
   18 	    72 	 0.03671 	 0.05162 	 ~...
   58 	    73 	 0.03711 	 0.05197 	 ~...
   29 	    74 	 0.03678 	 0.05199 	 ~...
   63 	    75 	 0.03898 	 0.05242 	 ~...
   41 	    76 	 0.03682 	 0.05244 	 ~...
   25 	    77 	 0.03677 	 0.05333 	 ~...
   19 	    78 	 0.03671 	 0.05385 	 ~...
   28 	    79 	 0.03678 	 0.05396 	 ~...
   23 	    80 	 0.03674 	 0.05424 	 ~...
   10 	    81 	 0.03659 	 0.05479 	 ~...
   77 	    82 	 0.04010 	 0.05546 	 ~...
    1 	    83 	 0.03645 	 0.05647 	 ~...
   89 	    84 	 0.20006 	 0.19773 	 ~...
   87 	    85 	 0.19402 	 0.20146 	 ~...
   88 	    86 	 0.19662 	 0.20191 	 ~...
   85 	    87 	 0.19176 	 0.20277 	 ~...
   86 	    88 	 0.19386 	 0.20694 	 ~...
  104 	    89 	 0.22842 	 0.20872 	 ~...
  113 	    90 	 0.26908 	 0.21142 	 m..s
   90 	    91 	 0.20168 	 0.21358 	 ~...
   93 	    92 	 0.20962 	 0.21902 	 ~...
   84 	    93 	 0.18217 	 0.21916 	 m..s
  111 	    94 	 0.25719 	 0.21997 	 m..s
   91 	    95 	 0.20558 	 0.22238 	 ~...
   92 	    96 	 0.20938 	 0.22517 	 ~...
  103 	    97 	 0.22600 	 0.22546 	 ~...
  102 	    98 	 0.22580 	 0.22610 	 ~...
  107 	    99 	 0.23376 	 0.22739 	 ~...
   94 	   100 	 0.21071 	 0.23132 	 ~...
  100 	   101 	 0.22398 	 0.23182 	 ~...
   97 	   102 	 0.22268 	 0.23554 	 ~...
   96 	   103 	 0.22173 	 0.24517 	 ~...
  106 	   104 	 0.23356 	 0.24868 	 ~...
   99 	   105 	 0.22358 	 0.24879 	 ~...
   95 	   106 	 0.21932 	 0.25289 	 m..s
  105 	   107 	 0.23276 	 0.25434 	 ~...
  109 	   108 	 0.24583 	 0.25820 	 ~...
  114 	   109 	 0.26999 	 0.25916 	 ~...
   98 	   110 	 0.22353 	 0.26449 	 m..s
  115 	   111 	 0.27438 	 0.27339 	 ~...
  108 	   112 	 0.24527 	 0.27425 	 ~...
  118 	   113 	 0.32098 	 0.28534 	 m..s
  117 	   114 	 0.28742 	 0.28917 	 ~...
  116 	   115 	 0.28500 	 0.29468 	 ~...
  112 	   116 	 0.26205 	 0.29922 	 m..s
  101 	   117 	 0.22515 	 0.32159 	 m..s
  110 	   118 	 0.24832 	 0.33543 	 m..s
  120 	   119 	 0.35768 	 0.39525 	 m..s
  119 	   120 	 0.33341 	 0.52223 	 MISS
==========================================
r_mrr = 0.9744453430175781
r2_mrr = 0.9368217587471008
spearmanr_mrr@5 = 0.913955807685852
spearmanr_mrr@10 = 0.9310948848724365
spearmanr_mrr@50 = 0.9779398441314697
spearmanr_mrr@100 = 0.9895554184913635
spearmanr_mrr@All = 0.9896801114082336
==========================================
test time: 0.457
Done Testing dataset UMLS
total time taken: 232.31544995307922
training time taken: 225.9462924003601
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9744)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9368)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9140)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9311)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9779)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9896)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9897)}}, 'test_loss': {'ComplEx': {'UMLS': 0.8553724451630842}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 8999291551734174
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [675, 1190, 754, 547, 1124, 1044, 865, 872, 435, 22, 930, 1154, 538, 829, 187, 904, 31, 264, 137, 798, 57, 1199, 308, 14, 657, 239, 635, 1148, 464, 407, 3, 34, 608, 1193, 874, 286, 81, 452, 1084, 108, 1092, 684, 1030, 498, 380, 195, 790, 1058, 431, 58, 739, 492, 460, 230, 429, 32, 152, 356, 679, 724, 1150, 189, 994, 976, 662, 83, 554, 301, 839, 766, 1013, 225, 394, 869, 200, 854, 203, 1028, 1158, 1036, 475, 402, 622, 915, 215, 636, 700, 740, 1063, 691, 950, 964, 54, 275, 841, 836, 53, 176, 655, 282, 1061, 514, 1115, 197, 614, 974, 512, 738, 806, 773, 353, 1011, 405, 449, 1054, 870, 999, 126, 925, 1128, 887]
valid_ids (0): []
train_ids (1094): [287, 646, 261, 150, 1007, 96, 260, 975, 418, 1138, 690, 21, 558, 420, 314, 807, 556, 256, 1187, 24, 1067, 342, 666, 626, 39, 155, 576, 1031, 348, 1096, 470, 1163, 408, 191, 501, 320, 664, 891, 143, 1095, 699, 654, 693, 885, 535, 843, 159, 338, 30, 871, 568, 75, 811, 1198, 183, 1145, 471, 289, 127, 827, 447, 933, 998, 712, 210, 713, 673, 91, 945, 584, 45, 844, 572, 349, 719, 727, 764, 440, 528, 645, 1039, 139, 786, 436, 333, 559, 344, 583, 232, 414, 1023, 606, 172, 178, 952, 332, 1125, 551, 1099, 109, 746, 495, 1186, 1055, 1008, 480, 1175, 965, 1118, 1104, 561, 658, 1110, 876, 650, 787, 609, 883, 617, 224, 94, 20, 161, 1083, 817, 51, 723, 41, 847, 595, 43, 1172, 849, 92, 520, 866, 973, 598, 451, 273, 294, 784, 346, 979, 400, 422, 89, 620, 7, 732, 1079, 428, 624, 1053, 672, 879, 237, 124, 831, 388, 1000, 526, 749, 771, 1068, 1091, 252, 1041, 509, 1171, 506, 316, 565, 6, 816, 863, 992, 1214, 988, 95, 823, 206, 969, 483, 669, 819, 758, 902, 317, 369, 184, 539, 567, 981, 467, 511, 634, 1042, 959, 395, 1144, 809, 789, 780, 36, 623, 505, 726, 777, 86, 1184, 540, 1161, 355, 234, 1093, 413, 752, 1022, 110, 1126, 404, 966, 111, 423, 165, 741, 985, 1064, 419, 1006, 121, 28, 493, 962, 141, 382, 1166, 337, 949, 469, 778, 1, 456, 227, 97, 284, 1089, 1177, 698, 579, 199, 890, 995, 345, 951, 2, 160, 446, 209, 37, 74, 882, 11, 603, 785, 115, 715, 755, 298, 1017, 151, 534, 769, 707, 38, 250, 877, 211, 1035, 360, 430, 262, 607, 229, 441, 895, 1155, 990, 1162, 641, 631, 987, 834, 1076, 508, 359, 205, 958, 272, 399, 795, 295, 1066, 833, 848, 69, 398, 1108, 1117, 582, 709, 309, 1057, 873, 1038, 1159, 352, 15, 926, 1212, 1050, 590, 403, 26, 1120, 642, 67, 424, 437, 174, 135, 737, 112, 198, 372, 922, 481, 310, 941, 1073, 996, 462, 543, 1195, 611, 425, 899, 1121, 47, 1014, 213, 307, 146, 1209, 868, 957, 894, 814, 136, 326, 1116, 934, 82, 689, 168, 410, 65, 961, 524, 1183, 1101, 571, 772, 628, 937, 763, 46, 19, 1181, 397, 148, 648, 49, 181, 156, 238, 490, 193, 68, 1080, 782, 253, 731, 909, 363, 536, 72, 936, 293, 1143, 842, 970, 574, 861, 730, 542, 940, 171, 835, 248, 103, 1088, 244, 531, 850, 742, 1133, 494, 670, 1129, 663, 61, 1179, 668, 466, 602, 682, 312, 240, 828, 101, 963, 276, 12, 661, 625, 1109, 1137, 1078, 529, 247, 792, 305, 245, 725, 781, 1025, 718, 274, 1009, 1146, 1056, 120, 443, 545, 881, 503, 1202, 302, 1003, 1060, 893, 900, 604, 99, 1152, 593, 564, 13, 533, 1130, 640, 1071, 299, 802, 599, 521, 325, 824, 60, 947, 916, 800, 736, 406, 55, 913, 853, 695, 118, 569, 911, 1192, 889, 163, 759, 570, 219, 341, 370, 711, 799, 804, 627, 105, 175, 458, 285, 830, 903, 504, 1142, 686, 600, 455, 217, 257, 993, 812, 1047, 167, 519, 960, 288, 463, 884, 208, 1107, 1139, 376, 207, 591, 594, 216, 703, 179, 476, 619, 415, 927, 427, 246, 465, 1059, 350, 910, 222, 361, 201, 618, 653, 1100, 489, 1168, 632, 194, 867, 813, 1164, 1156, 914, 665, 1103, 1037, 845, 704, 837, 750, 767, 1111, 1019, 235, 1134, 401, 4, 17, 647, 532, 552, 177, 459, 548, 362, 808, 140, 880, 76, 263, 324, 319, 1106, 1094, 696, 1204, 633, 474, 496, 444, 296, 375, 649, 279, 897, 1102, 577, 5, 856, 613, 530, 450, 851, 1208, 129, 596, 541, 616, 892, 838, 581, 846, 630, 610, 1051, 687, 1026, 760, 1140, 585, 1176, 939, 328, 776, 468, 281, 986, 339, 18, 643, 484, 365, 327, 29, 720, 1211, 182, 1112, 1002, 85, 1153, 79, 1151, 421, 70, 580, 122, 852, 233, 434, 1081, 921, 439, 390, 1165, 605, 144, 637, 416, 783, 445, 557, 347, 271, 292, 1147, 87, 477, 1205, 917, 343, 212, 329, 685, 300, 131, 1034, 659, 832, 1070, 1075, 932, 566, 223, 396, 1122, 549, 1210, 793, 803, 676, 1123, 857, 1131, 1033, 442, 523, 1191, 923, 639, 157, 983, 16, 1136, 678, 461, 384, 331, 1090, 259, 721, 323, 123, 747, 714, 1197, 912, 929, 745, 956, 1105, 88, 93, 133, 204, 1024, 1200, 128, 389, 214, 66, 753, 202, 864, 818, 955, 1020, 688, 1004, 190, 946, 774, 334, 928, 537, 770, 472, 729, 280, 9, 107, 656, 48, 1213, 290, 1032, 638, 255, 1049, 322, 943, 791, 671, 562, 392, 515, 63, 516, 1169, 134, 935, 756, 44, 218, 378, 249, 706, 354, 762, 651, 1097, 417, 1207, 391, 454, 313, 100, 251, 908, 196, 297, 265, 1074, 267, 587, 1119, 1157, 801, 1132, 1043, 710, 1021, 954, 652, 73, 254, 1065, 166, 944, 231, 681, 1189, 8, 491, 705, 330, 905, 386, 387, 486, 448, 25, 589, 433, 924, 701, 728, 886, 318, 862, 517, 1082, 907, 488, 980, 106, 513, 743, 525, 735, 59, 164, 80, 42, 982, 487, 967, 660, 482, 518, 919, 186, 1077, 578, 192, 810, 270, 775, 716, 479, 1160, 500, 351, 826, 102, 1040, 40, 1052, 972, 236, 268, 381, 147, 1206, 154, 119, 371, 117, 383, 291, 888, 145, 779, 278, 366, 563, 1027, 615, 825, 522, 1029, 377, 340, 357, 629, 304, 717, 412, 898, 52, 989, 241, 373, 1141, 258, 303, 510, 185, 978, 677, 597, 125, 502, 393, 71, 822, 1086, 1098, 1185, 796, 113, 269, 1196, 226, 560, 918, 667, 306, 188, 336, 1001, 586, 840, 901, 149, 942, 553, 1046, 1174, 173, 84, 612, 457, 78, 860, 744, 1173, 1085, 507, 0, 478, 1180, 920, 138, 385, 761, 374, 815, 243, 748, 132, 158, 768, 485, 1182, 1016, 77, 575, 1005, 90, 697, 1113, 1018, 931, 315, 311, 473, 1170, 453, 797, 266, 938, 1203, 1135, 708, 991, 734, 426, 644, 98, 1048, 592, 1010, 875, 694, 27, 180, 1045, 1072, 497, 116, 321, 1167, 1194, 722, 805, 953, 153, 283, 114, 1087, 1188, 335, 896, 997, 499, 573, 358, 765, 242, 1114, 794, 1012, 555, 821, 601, 368, 438, 64, 977, 33, 62, 169, 674, 757, 788, 702, 1201, 550, 142, 948, 680, 878, 968, 859, 971, 432, 544, 10, 104, 56, 1127, 221, 546, 1178, 820, 277, 1062, 364, 855, 1069, 621, 1149, 733, 228, 588, 858, 35, 220, 50, 692, 984, 1015, 379, 367, 409, 751, 162, 683, 906, 130, 170, 23, 411, 527]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8248558604546647
the save name prefix for this run is:  chkpt-ID_8248558604546647_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 780
rank avg (pred): 0.462 +- 0.004
mrr vals (pred, true): 0.016, 0.038
batch losses (mrrl, rdl): 0.0, 7.14009e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 428
rank avg (pred): 0.434 +- 0.004
mrr vals (pred, true): 0.017, 0.052
batch losses (mrrl, rdl): 0.0, 8.75113e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 112
rank avg (pred): 0.444 +- 0.003
mrr vals (pred, true): 0.017, 0.043
batch losses (mrrl, rdl): 0.0, 7.92946e-05

Epoch over!
epoch time: 15.132

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1033
rank avg (pred): 0.446 +- 0.002
mrr vals (pred, true): 0.016, 0.047
batch losses (mrrl, rdl): 0.0, 7.59875e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 105
rank avg (pred): 0.440 +- 0.002
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0, 7.6178e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 756
rank avg (pred): 0.480 +- 0.000
mrr vals (pred, true): 0.015, 0.050
batch losses (mrrl, rdl): 0.0, 0.000106217

Epoch over!
epoch time: 14.993

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 907
rank avg (pred): 0.547 +- 0.000
mrr vals (pred, true): 0.013, 0.020
batch losses (mrrl, rdl): 0.0, 3.30955e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 954
rank avg (pred): 0.479 +- 0.000
mrr vals (pred, true): 0.015, 0.058
batch losses (mrrl, rdl): 0.0, 0.0001112332

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 86
rank avg (pred): 0.443 +- 0.000
mrr vals (pred, true): 0.017, 0.054
batch losses (mrrl, rdl): 0.0, 8.28135e-05

Epoch over!
epoch time: 14.898

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 134
rank avg (pred): 0.445 +- 0.000
mrr vals (pred, true): 0.016, 0.052
batch losses (mrrl, rdl): 0.0, 8.37843e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 226
rank avg (pred): 0.432 +- 0.000
mrr vals (pred, true): 0.017, 0.051
batch losses (mrrl, rdl): 0.0, 8.89366e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 220
rank avg (pred): 0.439 +- 0.000
mrr vals (pred, true): 0.017, 0.048
batch losses (mrrl, rdl): 0.0, 8.00508e-05

Epoch over!
epoch time: 14.954

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 886
rank avg (pred): 0.446 +- 0.000
mrr vals (pred, true): 0.016, 0.048
batch losses (mrrl, rdl): 0.0, 8.56208e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 361
rank avg (pred): 0.448 +- 0.000
mrr vals (pred, true): 0.016, 0.050
batch losses (mrrl, rdl): 0.0, 8.50772e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 400
rank avg (pred): 0.457 +- 0.000
mrr vals (pred, true): 0.016, 0.052
batch losses (mrrl, rdl): 0.0, 8.34828e-05

Epoch over!
epoch time: 15.047

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 979
rank avg (pred): 0.183 +- 0.000
mrr vals (pred, true): 0.039, 0.335
batch losses (mrrl, rdl): 0.8774371147, 4.92729e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1214
rank avg (pred): 0.218 +- 0.073
mrr vals (pred, true): 0.045, 0.049
batch losses (mrrl, rdl): 0.000258964, 0.0010532541

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 325
rank avg (pred): 0.224 +- 0.078
mrr vals (pred, true): 0.048, 0.052
batch losses (mrrl, rdl): 4.69502e-05, 0.0010264912

Epoch over!
epoch time: 15.164

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 932
rank avg (pred): 0.220 +- 0.075
mrr vals (pred, true): 0.048, 0.036
batch losses (mrrl, rdl): 5.89247e-05, 0.0012747189

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 422
rank avg (pred): 0.265 +- 0.072
mrr vals (pred, true): 0.034, 0.052
batch losses (mrrl, rdl): 0.0027041009, 0.0007340354

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 722
rank avg (pred): 0.272 +- 0.101
mrr vals (pred, true): 0.055, 0.051
batch losses (mrrl, rdl): 0.0002803909, 0.0005512908

Epoch over!
epoch time: 15.077

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 602
rank avg (pred): 0.275 +- 0.084
mrr vals (pred, true): 0.039, 0.039
batch losses (mrrl, rdl): 0.0011792874, 0.0007041529

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 784
rank avg (pred): 0.196 +- 0.072
mrr vals (pred, true): 0.071, 0.042
batch losses (mrrl, rdl): 0.004417439, 0.0013210764

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 80
rank avg (pred): 0.043 +- 0.016
mrr vals (pred, true): 0.202, 0.201
batch losses (mrrl, rdl): 8.4002e-06, 0.0004731104

Epoch over!
epoch time: 15.075

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 639
rank avg (pred): 0.328 +- 0.121
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001412502, 0.0003847455

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 345
rank avg (pred): 0.379 +- 0.151
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.000185676, 0.0001217658

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 515
rank avg (pred): 0.506 +- 0.194
mrr vals (pred, true): 0.041, 0.025
batch losses (mrrl, rdl): 0.0007270051, 4.7202e-06

Epoch over!
epoch time: 15.088

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 445
rank avg (pred): 0.431 +- 0.169
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 5.002e-06, 2.84359e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 257
rank avg (pred): 0.042 +- 0.016
mrr vals (pred, true): 0.207, 0.243
batch losses (mrrl, rdl): 0.0133408085, 0.0003670474

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 794
rank avg (pred): 0.446 +- 0.184
mrr vals (pred, true): 0.058, 0.050
batch losses (mrrl, rdl): 0.0006340305, 3.58885e-05

Epoch over!
epoch time: 15.006

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 540
rank avg (pred): 0.494 +- 0.199
mrr vals (pred, true): 0.055, 0.026
batch losses (mrrl, rdl): 0.0002111249, 5.4499e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1165
rank avg (pred): 0.517 +- 0.196
mrr vals (pred, true): 0.048, 0.036
batch losses (mrrl, rdl): 4.24427e-05, 0.0001725172

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 847
rank avg (pred): 0.441 +- 0.189
mrr vals (pred, true): 0.070, 0.046
batch losses (mrrl, rdl): 0.0040856069, 2.00945e-05

Epoch over!
epoch time: 14.992

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 399
rank avg (pred): 0.401 +- 0.154
mrr vals (pred, true): 0.049, 0.054
batch losses (mrrl, rdl): 7.1356e-06, 5.48614e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 443
rank avg (pred): 0.437 +- 0.160
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001395277, 3.78755e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 461
rank avg (pred): 0.423 +- 0.161
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 8.02311e-05, 4.94267e-05

Epoch over!
epoch time: 14.945

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 139
rank avg (pred): 0.404 +- 0.164
mrr vals (pred, true): 0.058, 0.044
batch losses (mrrl, rdl): 0.0006384603, 5.39993e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 835
rank avg (pred): 0.057 +- 0.019
mrr vals (pred, true): 0.150, 0.285
batch losses (mrrl, rdl): 0.1824500561, 0.0002402134

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 630
rank avg (pred): 0.452 +- 0.170
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 2.1045e-06, 2.91343e-05

Epoch over!
epoch time: 14.956

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 103
rank avg (pred): 0.424 +- 0.152
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 1.582e-05, 3.88545e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 919
rank avg (pred): 0.497 +- 0.186
mrr vals (pred, true): 0.053, 0.039
batch losses (mrrl, rdl): 0.0001035856, 4.35929e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 487
rank avg (pred): 0.515 +- 0.200
mrr vals (pred, true): 0.058, 0.024
batch losses (mrrl, rdl): 0.0007186658, 5.6336e-06

Epoch over!
epoch time: 14.95

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 617
rank avg (pred): 0.445 +- 0.177
mrr vals (pred, true): 0.060, 0.035
batch losses (mrrl, rdl): 0.0010404702, 2.33253e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 370
rank avg (pred): 0.405 +- 0.156
mrr vals (pred, true): 0.053, 0.056
batch losses (mrrl, rdl): 9.46083e-05, 4.18216e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 623
rank avg (pred): 0.452 +- 0.173
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 0.0001200016, 3.62116e-05

Epoch over!
epoch time: 14.964

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.447 +- 0.173
mrr vals (pred, true): 0.054, 0.049

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.05006 	 0.02050 	 ~...
   19 	     1 	 0.05271 	 0.02172 	 m..s
    1 	     2 	 0.04873 	 0.02204 	 ~...
   18 	     3 	 0.05204 	 0.02234 	 ~...
   17 	     4 	 0.05204 	 0.02291 	 ~...
    3 	     5 	 0.04926 	 0.02305 	 ~...
    0 	     6 	 0.04865 	 0.02350 	 ~...
    4 	     7 	 0.04960 	 0.02414 	 ~...
    2 	     8 	 0.04885 	 0.02441 	 ~...
    7 	     9 	 0.05048 	 0.02442 	 ~...
   15 	    10 	 0.05197 	 0.02569 	 ~...
    5 	    11 	 0.04992 	 0.02579 	 ~...
   13 	    12 	 0.05155 	 0.02820 	 ~...
   14 	    13 	 0.05171 	 0.02966 	 ~...
   10 	    14 	 0.05111 	 0.03512 	 ~...
   87 	    15 	 0.05640 	 0.03544 	 ~...
   41 	    16 	 0.05486 	 0.03671 	 ~...
   52 	    17 	 0.05517 	 0.03815 	 ~...
   28 	    18 	 0.05439 	 0.03889 	 ~...
   42 	    19 	 0.05488 	 0.03957 	 ~...
   20 	    20 	 0.05291 	 0.04061 	 ~...
   52 	    21 	 0.05517 	 0.04080 	 ~...
   52 	    22 	 0.05517 	 0.04084 	 ~...
   16 	    23 	 0.05200 	 0.04136 	 ~...
   52 	    24 	 0.05517 	 0.04174 	 ~...
   83 	    25 	 0.05529 	 0.04176 	 ~...
   52 	    26 	 0.05517 	 0.04224 	 ~...
   82 	    27 	 0.05527 	 0.04255 	 ~...
   51 	    28 	 0.05516 	 0.04279 	 ~...
   52 	    29 	 0.05517 	 0.04283 	 ~...
   52 	    30 	 0.05517 	 0.04314 	 ~...
   50 	    31 	 0.05516 	 0.04327 	 ~...
   52 	    32 	 0.05517 	 0.04407 	 ~...
   52 	    33 	 0.05517 	 0.04418 	 ~...
   44 	    34 	 0.05499 	 0.04424 	 ~...
   36 	    35 	 0.05482 	 0.04450 	 ~...
   52 	    36 	 0.05517 	 0.04474 	 ~...
    9 	    37 	 0.05082 	 0.04483 	 ~...
   49 	    38 	 0.05512 	 0.04484 	 ~...
   52 	    39 	 0.05517 	 0.04488 	 ~...
   90 	    40 	 0.06157 	 0.04493 	 ~...
   52 	    41 	 0.05517 	 0.04505 	 ~...
   38 	    42 	 0.05485 	 0.04543 	 ~...
   12 	    43 	 0.05141 	 0.04586 	 ~...
   21 	    44 	 0.05317 	 0.04609 	 ~...
   85 	    45 	 0.05541 	 0.04613 	 ~...
   43 	    46 	 0.05494 	 0.04617 	 ~...
   52 	    47 	 0.05517 	 0.04626 	 ~...
   25 	    48 	 0.05412 	 0.04641 	 ~...
   52 	    49 	 0.05517 	 0.04658 	 ~...
   91 	    50 	 0.06187 	 0.04668 	 ~...
   52 	    51 	 0.05517 	 0.04670 	 ~...
   37 	    52 	 0.05484 	 0.04690 	 ~...
   52 	    53 	 0.05517 	 0.04705 	 ~...
   11 	    54 	 0.05123 	 0.04713 	 ~...
   22 	    55 	 0.05362 	 0.04736 	 ~...
   52 	    56 	 0.05517 	 0.04744 	 ~...
   88 	    57 	 0.06090 	 0.04747 	 ~...
   47 	    58 	 0.05510 	 0.04758 	 ~...
   45 	    59 	 0.05501 	 0.04775 	 ~...
   52 	    60 	 0.05517 	 0.04815 	 ~...
   26 	    61 	 0.05413 	 0.04823 	 ~...
   23 	    62 	 0.05402 	 0.04872 	 ~...
   40 	    63 	 0.05485 	 0.04873 	 ~...
   86 	    64 	 0.05564 	 0.04890 	 ~...
   39 	    65 	 0.05485 	 0.04895 	 ~...
   48 	    66 	 0.05510 	 0.04898 	 ~...
   89 	    67 	 0.06151 	 0.04898 	 ~...
   35 	    68 	 0.05474 	 0.04917 	 ~...
   52 	    69 	 0.05517 	 0.04932 	 ~...
   46 	    70 	 0.05501 	 0.04964 	 ~...
   52 	    71 	 0.05517 	 0.04974 	 ~...
   27 	    72 	 0.05432 	 0.04987 	 ~...
   32 	    73 	 0.05446 	 0.04996 	 ~...
   52 	    74 	 0.05517 	 0.05004 	 ~...
   84 	    75 	 0.05537 	 0.05012 	 ~...
   29 	    76 	 0.05441 	 0.05030 	 ~...
   34 	    77 	 0.05472 	 0.05051 	 ~...
   52 	    78 	 0.05517 	 0.05053 	 ~...
   52 	    79 	 0.05517 	 0.05146 	 ~...
   52 	    80 	 0.05517 	 0.05147 	 ~...
   31 	    81 	 0.05445 	 0.05162 	 ~...
   33 	    82 	 0.05459 	 0.05184 	 ~...
   52 	    83 	 0.05517 	 0.05197 	 ~...
   52 	    84 	 0.05517 	 0.05214 	 ~...
   52 	    85 	 0.05517 	 0.05280 	 ~...
   24 	    86 	 0.05408 	 0.05358 	 ~...
    8 	    87 	 0.05068 	 0.05453 	 ~...
   30 	    88 	 0.05442 	 0.05479 	 ~...
   52 	    89 	 0.05517 	 0.05584 	 ~...
   52 	    90 	 0.05517 	 0.05608 	 ~...
   52 	    91 	 0.05517 	 0.06233 	 ~...
   94 	    92 	 0.19996 	 0.17062 	 ~...
   92 	    93 	 0.19681 	 0.18889 	 ~...
  101 	    94 	 0.22103 	 0.19455 	 ~...
  102 	    95 	 0.22356 	 0.21298 	 ~...
   98 	    96 	 0.21575 	 0.21767 	 ~...
  100 	    97 	 0.21988 	 0.22647 	 ~...
  106 	    98 	 0.25269 	 0.22787 	 ~...
   97 	    99 	 0.21327 	 0.23015 	 ~...
  105 	   100 	 0.23853 	 0.23182 	 ~...
   96 	   101 	 0.21260 	 0.23739 	 ~...
  103 	   102 	 0.22593 	 0.24051 	 ~...
   99 	   103 	 0.21980 	 0.24226 	 ~...
   95 	   104 	 0.21104 	 0.24305 	 m..s
   93 	   105 	 0.19928 	 0.24517 	 m..s
  109 	   106 	 0.25933 	 0.24954 	 ~...
  104 	   107 	 0.23300 	 0.25592 	 ~...
  107 	   108 	 0.25283 	 0.26409 	 ~...
  111 	   109 	 0.26136 	 0.26624 	 ~...
  108 	   110 	 0.25493 	 0.26769 	 ~...
  120 	   111 	 0.29991 	 0.27476 	 ~...
  118 	   112 	 0.28620 	 0.27479 	 ~...
  117 	   113 	 0.27912 	 0.27497 	 ~...
  114 	   114 	 0.26638 	 0.28106 	 ~...
  116 	   115 	 0.27469 	 0.29106 	 ~...
  113 	   116 	 0.26628 	 0.29763 	 m..s
  110 	   117 	 0.26055 	 0.30117 	 m..s
  112 	   118 	 0.26490 	 0.30704 	 m..s
  115 	   119 	 0.26965 	 0.30991 	 m..s
  119 	   120 	 0.29155 	 0.33593 	 m..s
==========================================
r_mrr = 0.9899733662605286
r2_mrr = 0.9664919972419739
spearmanr_mrr@5 = 0.9280503392219543
spearmanr_mrr@10 = 0.9701524972915649
spearmanr_mrr@50 = 0.9972153902053833
spearmanr_mrr@100 = 0.9981458783149719
spearmanr_mrr@All = 0.9965265989303589
==========================================
test time: 0.457
Done Testing dataset UMLS
total time taken: 232.0013403892517
training time taken: 225.70986938476562
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9900)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9665)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9281)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9702)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9972)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9981)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9965)}}, 'test_loss': {'ComplEx': {'UMLS': 0.2115193117488161}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7060350119780399
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [5, 805, 286, 1143, 435, 934, 1022, 133, 95, 58, 880, 1051, 504, 154, 553, 563, 717, 115, 110, 634, 65, 73, 198, 1196, 673, 712, 1004, 277, 584, 87, 278, 774, 854, 1016, 543, 411, 1060, 691, 929, 643, 1171, 246, 454, 758, 671, 452, 1081, 931, 9, 517, 849, 713, 838, 353, 1009, 937, 121, 153, 464, 692, 965, 417, 61, 847, 1191, 718, 275, 397, 1203, 247, 702, 366, 619, 788, 190, 684, 1121, 651, 344, 412, 979, 1189, 626, 163, 430, 70, 336, 525, 611, 865, 193, 169, 885, 1063, 1109, 54, 1002, 75, 436, 943, 891, 1206, 839, 3, 1182, 125, 372, 935, 475, 941, 697, 94, 162, 157, 349, 215, 164, 745, 382, 920, 882]
valid_ids (0): []
train_ids (1094): [704, 977, 1027, 195, 593, 1011, 723, 374, 1153, 1170, 476, 1043, 203, 676, 829, 1197, 602, 24, 342, 566, 730, 981, 22, 565, 354, 1093, 279, 334, 1086, 1126, 680, 338, 884, 40, 251, 777, 12, 1024, 42, 472, 996, 401, 227, 1095, 570, 390, 487, 310, 952, 524, 714, 963, 883, 1032, 118, 288, 742, 479, 808, 313, 600, 1018, 922, 1067, 518, 1089, 1144, 638, 314, 721, 596, 1124, 26, 1213, 796, 761, 62, 188, 519, 261, 1173, 262, 1155, 1106, 33, 670, 273, 533, 243, 940, 999, 72, 498, 375, 878, 710, 819, 1005, 863, 373, 564, 751, 968, 509, 844, 358, 705, 347, 630, 370, 269, 659, 298, 421, 511, 155, 1042, 1167, 597, 821, 1154, 441, 1122, 181, 287, 816, 31, 822, 914, 581, 424, 232, 1158, 495, 1147, 8, 689, 780, 257, 1015, 781, 749, 936, 677, 368, 44, 962, 924, 141, 926, 221, 96, 648, 1090, 457, 541, 100, 520, 143, 384, 842, 471, 0, 523, 453, 1113, 1099, 1142, 136, 1070, 901, 84, 127, 728, 848, 202, 802, 423, 501, 398, 477, 1161, 1199, 268, 657, 151, 776, 506, 944, 337, 798, 522, 173, 515, 365, 402, 210, 348, 381, 199, 99, 325, 285, 507, 655, 1119, 894, 703, 1098, 1052, 947, 988, 1148, 893, 661, 679, 1149, 399, 625, 39, 473, 272, 1177, 1118, 306, 624, 224, 492, 152, 116, 632, 732, 574, 289, 172, 910, 1207, 209, 254, 429, 332, 711, 793, 1053, 991, 956, 1120, 606, 330, 682, 818, 779, 434, 1047, 258, 367, 1136, 888, 311, 204, 1078, 836, 55, 736, 200, 415, 183, 890, 1160, 244, 201, 1056, 989, 1187, 1, 174, 845, 177, 823, 803, 315, 985, 864, 902, 1037, 752, 1025, 862, 1039, 953, 343, 361, 1192, 571, 57, 416, 1146, 391, 160, 456, 165, 63, 535, 403, 521, 800, 396, 117, 276, 431, 407, 496, 628, 1048, 1163, 499, 585, 556, 539, 250, 1017, 66, 978, 139, 967, 30, 304, 29, 771, 966, 120, 971, 785, 753, 147, 1133, 983, 1097, 1083, 420, 91, 846, 355, 872, 1080, 861, 239, 1190, 265, 877, 644, 948, 292, 783, 598, 159, 178, 345, 629, 1059, 1102, 681, 744, 414, 589, 573, 876, 1034, 986, 896, 184, 1050, 1058, 807, 175, 167, 132, 329, 1145, 674, 488, 824, 294, 898, 83, 10, 911, 995, 50, 378, 1201, 1175, 171, 545, 256, 303, 46, 129, 510, 913, 961, 757, 1210, 897, 90, 992, 908, 56, 623, 119, 892, 792, 128, 1033, 765, 59, 568, 791, 1020, 25, 49, 124, 1044, 969, 383, 960, 413, 1214, 1074, 470, 86, 512, 724, 380, 97, 137, 98, 357, 806, 104, 312, 1031, 1007, 690, 52, 35, 1092, 907, 32, 820, 85, 516, 706, 255, 814, 871, 906, 1066, 938, 1021, 609, 588, 219, 812, 1195, 1029, 1019, 895, 997, 536, 642, 1165, 494, 222, 558, 1030, 784, 1172, 1091, 1069, 662, 592, 738, 833, 333, 69, 653, 260, 899, 238, 449, 616, 231, 1138, 851, 850, 404, 701, 1073, 1100, 4, 928, 595, 346, 664, 466, 603, 1188, 933, 1115, 437, 1134, 675, 528, 392, 426, 686, 142, 249, 958, 750, 950, 841, 830, 582, 694, 319, 668, 107, 480, 252, 537, 317, 647, 503, 340, 130, 811, 15, 957, 557, 1023, 81, 1176, 916, 295, 734, 23, 170, 264, 284, 1151, 185, 725, 612, 410, 508, 881, 707, 970, 562, 405, 708, 267, 280, 546, 551, 731, 442, 263, 804, 1041, 205, 78, 932, 485, 283, 461, 554, 18, 608, 1127, 1162, 666, 775, 1064, 234, 719, 356, 422, 220, 229, 406, 605, 591, 866, 1204, 586, 236, 610, 1085, 909, 534, 235, 741, 318, 613, 60, 856, 870, 645, 633, 481, 601, 538, 951, 74, 326, 631, 1159, 835, 327, 700, 460, 1075, 514, 206, 650, 77, 458, 868, 179, 857, 927, 555, 1026, 832, 307, 561, 212, 1084, 2, 76, 770, 134, 1082, 196, 377, 1096, 1036, 646, 641, 813, 620, 1065, 530, 1186, 14, 359, 639, 1168, 576, 754, 635, 148, 1200, 855, 575, 604, 1179, 843, 1012, 735, 579, 443, 1152, 1107, 580, 695, 858, 917, 739, 131, 1157, 722, 385, 1088, 305, 772, 1208, 875, 1185, 51, 213, 683, 696, 955, 905, 799, 187, 240, 113, 439, 1003, 446, 297, 879, 89, 308, 678, 293, 976, 1150, 331, 994, 6, 138, 740, 726, 1045, 590, 889, 316, 320, 1116, 1049, 400, 621, 699, 497, 1117, 1001, 21, 194, 489, 778, 904, 688, 1132, 409, 502, 20, 161, 433, 617, 105, 270, 149, 1014, 1141, 242, 321, 787, 903, 782, 371, 408, 544, 1178, 853, 873, 1181, 919, 448, 290, 7, 972, 607, 687, 451, 109, 376, 114, 225, 11, 1040, 930, 53, 786, 918, 186, 826, 746, 82, 150, 166, 716, 693, 615, 500, 1166, 795, 360, 432, 1130, 767, 428, 867, 445, 1035, 363, 925, 1104, 37, 387, 1131, 1076, 28, 68, 351, 145, 36, 815, 984, 685, 1174, 302, 447, 228, 663, 658, 324, 300, 17, 218, 825, 1125, 1055, 483, 1209, 747, 323, 762, 474, 140, 1008, 38, 831, 395, 923, 103, 946, 810, 80, 743, 531, 362, 168, 259, 230, 146, 637, 484, 886, 817, 764, 773, 869, 945, 13, 982, 1183, 720, 733, 266, 660, 939, 462, 189, 282, 364, 1028, 766, 1110, 418, 809, 559, 350, 135, 1068, 1071, 223, 440, 438, 1103, 990, 527, 64, 301, 614, 993, 101, 1180, 245, 964, 915, 1205, 1062, 794, 123, 191, 1013, 912, 1094, 1038, 790, 532, 542, 1000, 122, 217, 698, 1202, 1105, 1072, 328, 837, 1193, 1198, 102, 636, 1077, 709, 1061, 973, 106, 197, 47, 335, 214, 797, 1079, 840, 526, 71, 468, 126, 216, 748, 455, 552, 1114, 763, 388, 1156, 560, 207, 954, 550, 789, 393, 467, 656, 1135, 860, 425, 1054, 112, 975, 769, 649, 1046, 192, 801, 41, 1164, 1101, 834, 34, 1128, 88, 980, 921, 296, 942, 998, 465, 291, 322, 341, 180, 1010, 427, 727, 529, 493, 852, 271, 1123, 665, 622, 450, 486, 19, 233, 578, 309, 547, 572, 1006, 1129, 389, 92, 1140, 1194, 79, 1108, 729, 874, 419, 182, 491, 241, 93, 226, 672, 379, 828, 43, 654, 1137, 339, 859, 211, 482, 1184, 594, 490, 469, 652, 111, 759, 768, 760, 987, 577, 827, 176, 459, 549, 478, 394, 16, 756, 386, 281, 583, 1169, 1211, 248, 45, 156, 444, 108, 1057, 67, 887, 715, 587, 755, 48, 567, 144, 640, 737, 1112, 208, 1087, 959, 1139, 352, 569, 618, 253, 299, 974, 548, 599, 1212, 669, 369, 900, 667, 274, 540, 27, 463, 1111, 627, 949, 237, 513, 158, 505]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2593247042893800
the save name prefix for this run is:  chkpt-ID_2593247042893800_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1091
rank avg (pred): 0.468 +- 0.003
mrr vals (pred, true): 0.016, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001036237

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 570
rank avg (pred): 0.459 +- 0.003
mrr vals (pred, true): 0.016, 0.042
batch losses (mrrl, rdl): 0.0, 6.71009e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 537
rank avg (pred): 0.500 +- 0.003
mrr vals (pred, true): 0.015, 0.024
batch losses (mrrl, rdl): 0.0, 3.65354e-05

Epoch over!
epoch time: 14.946

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 228
rank avg (pred): 0.451 +- 0.002
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 7.45296e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 910
rank avg (pred): 0.538 +- 0.002
mrr vals (pred, true): 0.014, 0.020
batch losses (mrrl, rdl): 0.0, 4.91055e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 212
rank avg (pred): 0.449 +- 0.004
mrr vals (pred, true): 0.016, 0.047
batch losses (mrrl, rdl): 0.0, 8.36477e-05

Epoch over!
epoch time: 14.9

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 446
rank avg (pred): 0.447 +- 0.004
mrr vals (pred, true): 0.016, 0.046
batch losses (mrrl, rdl): 0.0, 8.1e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 823
rank avg (pred): 0.113 +- 0.002
mrr vals (pred, true): 0.062, 0.321
batch losses (mrrl, rdl): 0.0, 3.76482e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 925
rank avg (pred): 0.471 +- 0.050
mrr vals (pred, true): 0.016, 0.030
batch losses (mrrl, rdl): 0.0, 5.2047e-05

Epoch over!
epoch time: 14.891

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1102
rank avg (pred): 0.447 +- 0.227
mrr vals (pred, true): 0.101, 0.043
batch losses (mrrl, rdl): 0.0, 2.11934e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 200
rank avg (pred): 0.417 +- 0.253
mrr vals (pred, true): 0.139, 0.044
batch losses (mrrl, rdl): 0.0, 5.3048e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 629
rank avg (pred): 0.440 +- 0.243
mrr vals (pred, true): 0.082, 0.038
batch losses (mrrl, rdl): 0.0, 4.8059e-06

Epoch over!
epoch time: 14.897

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 470
rank avg (pred): 0.425 +- 0.267
mrr vals (pred, true): 0.133, 0.050
batch losses (mrrl, rdl): 0.0, 1.48542e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 223
rank avg (pred): 0.424 +- 0.256
mrr vals (pred, true): 0.109, 0.045
batch losses (mrrl, rdl): 0.0, 4.1937e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 740
rank avg (pred): 0.169 +- 0.108
mrr vals (pred, true): 0.165, 0.307
batch losses (mrrl, rdl): 0.0, 6.2309e-05

Epoch over!
epoch time: 14.9

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 548
rank avg (pred): 0.502 +- 0.212
mrr vals (pred, true): 0.055, 0.024
batch losses (mrrl, rdl): 0.0002826957, 2.2416e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1148
rank avg (pred): 0.450 +- 0.173
mrr vals (pred, true): 0.054, 0.026
batch losses (mrrl, rdl): 0.0001941574, 2.66291e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 642
rank avg (pred): 0.477 +- 0.174
mrr vals (pred, true): 0.044, 0.042
batch losses (mrrl, rdl): 0.0003071949, 4.6434e-05

Epoch over!
epoch time: 15.192

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 174
rank avg (pred): 0.455 +- 0.152
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 2.91362e-05, 3.22079e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1069
rank avg (pred): 0.152 +- 0.099
mrr vals (pred, true): 0.253, 0.275
batch losses (mrrl, rdl): 0.0047810343, 4.85798e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 84
rank avg (pred): 0.460 +- 0.134
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 2.5953e-06, 5.11547e-05

Epoch over!
epoch time: 15.202

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 300
rank avg (pred): 0.226 +- 0.141
mrr vals (pred, true): 0.230, 0.247
batch losses (mrrl, rdl): 0.0027849893, 0.000145427

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 913
rank avg (pred): 0.510 +- 0.156
mrr vals (pred, true): 0.050, 0.021
batch losses (mrrl, rdl): 8.03e-08, 9.29661e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 763
rank avg (pred): 0.484 +- 0.116
mrr vals (pred, true): 0.045, 0.040
batch losses (mrrl, rdl): 0.0002935463, 7.21335e-05

Epoch over!
epoch time: 15.135

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 359
rank avg (pred): 0.462 +- 0.143
mrr vals (pred, true): 0.057, 0.048
batch losses (mrrl, rdl): 0.0005434747, 4.46051e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 556
rank avg (pred): 0.496 +- 0.122
mrr vals (pred, true): 0.052, 0.021
batch losses (mrrl, rdl): 3.95018e-05, 1.57923e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1123
rank avg (pred): 0.473 +- 0.112
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 6.324e-07, 8.61056e-05

Epoch over!
epoch time: 15.076

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 533
rank avg (pred): 0.476 +- 0.114
mrr vals (pred, true): 0.047, 0.026
batch losses (mrrl, rdl): 6.27199e-05, 4.74776e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 431
rank avg (pred): 0.486 +- 0.100
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 4.85299e-05, 0.000106207

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 928
rank avg (pred): 0.493 +- 0.105
mrr vals (pred, true): 0.044, 0.039
batch losses (mrrl, rdl): 0.0003333007, 7.0635e-05

Epoch over!
epoch time: 15.089

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 310
rank avg (pred): 0.335 +- 0.219
mrr vals (pred, true): 0.244, 0.220
batch losses (mrrl, rdl): 0.0056887693, 0.000577007

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 476
rank avg (pred): 0.438 +- 0.124
mrr vals (pred, true): 0.057, 0.044
batch losses (mrrl, rdl): 0.0004436919, 5.58682e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 51
rank avg (pred): 0.258 +- 0.164
mrr vals (pred, true): 0.196, 0.183
batch losses (mrrl, rdl): 0.0017563459, 6.4411e-05

Epoch over!
epoch time: 15.116

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 116
rank avg (pred): 0.468 +- 0.097
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 1.70275e-05, 7.14101e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1042
rank avg (pred): 0.469 +- 0.104
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 4.613e-07, 6.89996e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 924
rank avg (pred): 0.468 +- 0.091
mrr vals (pred, true): 0.048, 0.030
batch losses (mrrl, rdl): 4.41398e-05, 4.77602e-05

Epoch over!
epoch time: 15.114

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 82
rank avg (pred): 0.442 +- 0.097
mrr vals (pred, true): 0.050, 0.040
batch losses (mrrl, rdl): 1.8e-09, 6.43256e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 138
rank avg (pred): 0.423 +- 0.103
mrr vals (pred, true): 0.053, 0.044
batch losses (mrrl, rdl): 8.15753e-05, 5.63088e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 777
rank avg (pred): 0.457 +- 0.092
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 2.33554e-05, 5.5068e-05

Epoch over!
epoch time: 15.082

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 260
rank avg (pred): 0.227 +- 0.161
mrr vals (pred, true): 0.240, 0.233
batch losses (mrrl, rdl): 0.0004529678, 9.94364e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 370
rank avg (pred): 0.438 +- 0.087
mrr vals (pred, true): 0.049, 0.056
batch losses (mrrl, rdl): 1.57829e-05, 7.21432e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 722
rank avg (pred): 0.451 +- 0.106
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 3.6514e-06, 6.31973e-05

Epoch over!
epoch time: 15.264

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 816
rank avg (pred): 0.193 +- 0.137
mrr vals (pred, true): 0.256, 0.218
batch losses (mrrl, rdl): 0.0144424029, 3.07302e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 432
rank avg (pred): 0.449 +- 0.096
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 7.25e-08, 6.82985e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 752
rank avg (pred): 0.230 +- 0.180
mrr vals (pred, true): 0.310, 0.274
batch losses (mrrl, rdl): 0.0124730123, 0.0002040585

Epoch over!
epoch time: 15.256

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.231 +- 0.160
mrr vals (pred, true): 0.250, 0.238

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   35 	     0 	 0.04988 	 0.02144 	 ~...
   95 	     1 	 0.05260 	 0.02154 	 m..s
   68 	     2 	 0.05114 	 0.02180 	 ~...
    1 	     3 	 0.04860 	 0.02304 	 ~...
    0 	     4 	 0.04855 	 0.02423 	 ~...
   58 	     5 	 0.05068 	 0.02530 	 ~...
    5 	     6 	 0.04881 	 0.02640 	 ~...
   20 	     7 	 0.04946 	 0.02679 	 ~...
   10 	     8 	 0.04904 	 0.02843 	 ~...
    3 	     9 	 0.04875 	 0.02898 	 ~...
   12 	    10 	 0.04912 	 0.02912 	 ~...
   16 	    11 	 0.04933 	 0.02964 	 ~...
   40 	    12 	 0.05002 	 0.02979 	 ~...
   41 	    13 	 0.05003 	 0.03056 	 ~...
   13 	    14 	 0.04913 	 0.03388 	 ~...
    8 	    15 	 0.04889 	 0.03520 	 ~...
    7 	    16 	 0.04884 	 0.03587 	 ~...
   92 	    17 	 0.05216 	 0.03629 	 ~...
    9 	    18 	 0.04891 	 0.03663 	 ~...
   48 	    19 	 0.05029 	 0.03727 	 ~...
   26 	    20 	 0.04955 	 0.03815 	 ~...
   23 	    21 	 0.04951 	 0.03923 	 ~...
   83 	    22 	 0.05172 	 0.03929 	 ~...
   44 	    23 	 0.05015 	 0.04081 	 ~...
   56 	    24 	 0.05056 	 0.04106 	 ~...
   28 	    25 	 0.04965 	 0.04108 	 ~...
   31 	    26 	 0.04971 	 0.04126 	 ~...
   93 	    27 	 0.05225 	 0.04134 	 ~...
   84 	    28 	 0.05172 	 0.04143 	 ~...
   19 	    29 	 0.04942 	 0.04163 	 ~...
   91 	    30 	 0.05207 	 0.04174 	 ~...
   52 	    31 	 0.05034 	 0.04174 	 ~...
   75 	    32 	 0.05132 	 0.04177 	 ~...
   14 	    33 	 0.04915 	 0.04206 	 ~...
   29 	    34 	 0.04970 	 0.04224 	 ~...
   78 	    35 	 0.05143 	 0.04315 	 ~...
   24 	    36 	 0.04952 	 0.04353 	 ~...
   76 	    37 	 0.05133 	 0.04362 	 ~...
   42 	    38 	 0.05009 	 0.04410 	 ~...
   46 	    39 	 0.05022 	 0.04436 	 ~...
   77 	    40 	 0.05137 	 0.04475 	 ~...
   74 	    41 	 0.05126 	 0.04480 	 ~...
   71 	    42 	 0.05122 	 0.04507 	 ~...
   98 	    43 	 0.05271 	 0.04512 	 ~...
   94 	    44 	 0.05226 	 0.04531 	 ~...
   65 	    45 	 0.05108 	 0.04550 	 ~...
    4 	    46 	 0.04880 	 0.04559 	 ~...
   69 	    47 	 0.05117 	 0.04570 	 ~...
   70 	    48 	 0.05121 	 0.04574 	 ~...
   54 	    49 	 0.05044 	 0.04575 	 ~...
   62 	    50 	 0.05088 	 0.04614 	 ~...
   67 	    51 	 0.05114 	 0.04615 	 ~...
   39 	    52 	 0.05002 	 0.04618 	 ~...
   33 	    53 	 0.04978 	 0.04620 	 ~...
   85 	    54 	 0.05174 	 0.04626 	 ~...
   53 	    55 	 0.05042 	 0.04640 	 ~...
   97 	    56 	 0.05269 	 0.04668 	 ~...
   57 	    57 	 0.05066 	 0.04677 	 ~...
   15 	    58 	 0.04922 	 0.04680 	 ~...
   45 	    59 	 0.05021 	 0.04715 	 ~...
   50 	    60 	 0.05033 	 0.04719 	 ~...
   43 	    61 	 0.05012 	 0.04727 	 ~...
   22 	    62 	 0.04950 	 0.04736 	 ~...
   32 	    63 	 0.04976 	 0.04736 	 ~...
   25 	    64 	 0.04953 	 0.04740 	 ~...
   61 	    65 	 0.05083 	 0.04744 	 ~...
   17 	    66 	 0.04938 	 0.04764 	 ~...
   73 	    67 	 0.05125 	 0.04788 	 ~...
   66 	    68 	 0.05110 	 0.04793 	 ~...
   18 	    69 	 0.04941 	 0.04814 	 ~...
   38 	    70 	 0.05001 	 0.04857 	 ~...
   37 	    71 	 0.04993 	 0.04862 	 ~...
   88 	    72 	 0.05188 	 0.04869 	 ~...
  100 	    73 	 0.05313 	 0.04896 	 ~...
   72 	    74 	 0.05125 	 0.04898 	 ~...
   89 	    75 	 0.05189 	 0.04900 	 ~...
   79 	    76 	 0.05144 	 0.04917 	 ~...
   47 	    77 	 0.05024 	 0.04921 	 ~...
   96 	    78 	 0.05261 	 0.04953 	 ~...
   86 	    79 	 0.05178 	 0.04968 	 ~...
   60 	    80 	 0.05076 	 0.04969 	 ~...
   80 	    81 	 0.05150 	 0.04973 	 ~...
   81 	    82 	 0.05151 	 0.04978 	 ~...
   36 	    83 	 0.04990 	 0.04987 	 ~...
   55 	    84 	 0.05048 	 0.05024 	 ~...
   87 	    85 	 0.05183 	 0.05080 	 ~...
    2 	    86 	 0.04871 	 0.05084 	 ~...
   49 	    87 	 0.05032 	 0.05100 	 ~...
   30 	    88 	 0.04970 	 0.05108 	 ~...
   90 	    89 	 0.05197 	 0.05132 	 ~...
   59 	    90 	 0.05071 	 0.05147 	 ~...
   21 	    91 	 0.04948 	 0.05158 	 ~...
   11 	    92 	 0.04904 	 0.05160 	 ~...
   51 	    93 	 0.05033 	 0.05162 	 ~...
   82 	    94 	 0.05157 	 0.05178 	 ~...
   99 	    95 	 0.05281 	 0.05208 	 ~...
   63 	    96 	 0.05097 	 0.05214 	 ~...
    6 	    97 	 0.04882 	 0.05450 	 ~...
   34 	    98 	 0.04987 	 0.05524 	 ~...
   27 	    99 	 0.04955 	 0.05546 	 ~...
   64 	   100 	 0.05097 	 0.06389 	 ~...
  101 	   101 	 0.19867 	 0.20603 	 ~...
  104 	   102 	 0.22478 	 0.21298 	 ~...
  102 	   103 	 0.20026 	 0.21637 	 ~...
  110 	   104 	 0.23569 	 0.21735 	 ~...
  103 	   105 	 0.22145 	 0.21805 	 ~...
  105 	   106 	 0.22504 	 0.22517 	 ~...
  113 	   107 	 0.23869 	 0.22647 	 ~...
  114 	   108 	 0.24447 	 0.22739 	 ~...
  112 	   109 	 0.23762 	 0.23015 	 ~...
  117 	   110 	 0.25005 	 0.23847 	 ~...
  111 	   111 	 0.23733 	 0.24233 	 ~...
  106 	   112 	 0.23026 	 0.24517 	 ~...
  116 	   113 	 0.24967 	 0.24927 	 ~...
  109 	   114 	 0.23532 	 0.25163 	 ~...
  107 	   115 	 0.23159 	 0.25327 	 ~...
  108 	   116 	 0.23477 	 0.25965 	 ~...
  115 	   117 	 0.24808 	 0.26409 	 ~...
  118 	   118 	 0.29571 	 0.27476 	 ~...
  119 	   119 	 0.30115 	 0.29922 	 ~...
  120 	   120 	 0.30257 	 0.33543 	 m..s
==========================================
r_mrr = 0.9917352199554443
r2_mrr = 0.9765031337738037
spearmanr_mrr@5 = 0.7776184678077698
spearmanr_mrr@10 = 0.8905342817306519
spearmanr_mrr@50 = 0.9971939921379089
spearmanr_mrr@100 = 0.997467577457428
spearmanr_mrr@All = 0.9943401217460632
==========================================
test time: 0.46
Done Testing dataset UMLS
total time taken: 232.85749053955078
training time taken: 226.52959036827087
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9917)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9765)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.7776)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.8905)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9972)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9975)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9943)}}, 'test_loss': {'ComplEx': {'UMLS': 0.058592544783095946}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4052553802049859
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [494, 819, 48, 119, 926, 21, 941, 853, 149, 918, 559, 235, 745, 1159, 160, 998, 481, 101, 981, 438, 87, 691, 311, 199, 154, 736, 404, 840, 731, 1074, 526, 362, 631, 1057, 1144, 1069, 181, 930, 642, 1111, 1059, 1191, 500, 905, 892, 988, 281, 598, 657, 250, 194, 761, 929, 846, 239, 4, 592, 407, 1193, 118, 528, 628, 345, 12, 995, 847, 370, 303, 405, 381, 331, 83, 660, 114, 447, 771, 174, 275, 430, 532, 137, 184, 151, 1179, 820, 955, 506, 1183, 227, 896, 999, 480, 389, 1134, 578, 670, 335, 204, 386, 312, 1004, 1046, 666, 1136, 925, 1151, 256, 553, 314, 875, 426, 1025, 457, 1042, 423, 196, 128, 726, 183, 837, 675]
valid_ids (0): []
train_ids (1094): [167, 801, 635, 1009, 938, 43, 513, 658, 724, 483, 984, 1044, 569, 88, 246, 367, 1122, 329, 462, 954, 649, 377, 313, 681, 877, 78, 1107, 208, 816, 602, 283, 551, 225, 398, 232, 202, 629, 1024, 186, 104, 1080, 942, 983, 835, 58, 556, 959, 169, 826, 659, 343, 916, 730, 529, 1195, 778, 318, 809, 932, 568, 1153, 350, 1188, 1211, 226, 1181, 346, 936, 347, 760, 26, 203, 969, 961, 717, 200, 779, 467, 460, 251, 238, 357, 1203, 1073, 1089, 365, 915, 1139, 600, 616, 135, 1021, 28, 706, 1124, 813, 650, 355, 328, 1053, 361, 651, 69, 782, 790, 1119, 945, 597, 1058, 1065, 108, 962, 1170, 366, 47, 150, 290, 525, 822, 815, 319, 887, 599, 1112, 116, 161, 738, 258, 842, 7, 299, 744, 767, 453, 109, 534, 451, 1196, 935, 53, 713, 757, 197, 1060, 968, 637, 212, 1110, 2, 41, 76, 475, 445, 1189, 673, 978, 469, 39, 780, 672, 223, 27, 839, 173, 818, 538, 1036, 478, 913, 185, 843, 84, 1047, 701, 255, 8, 163, 297, 80, 201, 752, 363, 982, 394, 55, 278, 1066, 110, 425, 966, 254, 291, 1062, 482, 580, 1209, 705, 606, 1143, 1091, 624, 523, 100, 269, 604, 567, 172, 718, 63, 920, 464, 348, 266, 1051, 805, 1104, 975, 287, 52, 68, 751, 1027, 1050, 1167, 123, 552, 1147, 931, 140, 284, 872, 686, 791, 924, 393, 618, 155, 699, 1101, 878, 817, 472, 1052, 899, 1205, 384, 158, 97, 832, 1037, 428, 808, 92, 737, 353, 585, 170, 764, 264, 126, 418, 667, 766, 54, 987, 861, 911, 1114, 294, 530, 298, 369, 385, 320, 862, 1076, 956, 632, 112, 122, 698, 844, 1141, 450, 215, 106, 17, 358, 406, 121, 684, 749, 507, 669, 973, 317, 136, 1063, 683, 933, 742, 1100, 1199, 399, 890, 166, 1173, 537, 171, 213, 415, 566, 865, 216, 646, 1210, 806, 620, 950, 410, 1149, 708, 895, 1000, 1155, 96, 309, 1045, 1001, 117, 879, 882, 514, 644, 536, 833, 509, 289, 391, 1093, 746, 241, 836, 416, 417, 187, 485, 1096, 231, 517, 612, 619, 591, 1108, 609, 753, 1115, 282, 1010, 980, 923, 831, 424, 162, 1088, 505, 851, 1182, 723, 678, 458, 263, 339, 1082, 1163, 1032, 664, 1017, 1034, 1165, 653, 133, 587, 687, 1169, 636, 383, 1022, 177, 876, 857, 220, 120, 1213, 1142, 73, 419, 456, 46, 946, 1038, 1056, 610, 855, 560, 125, 1092, 323, 704, 316, 883, 849, 70, 596, 633, 1113, 572, 222, 952, 38, 614, 265, 268, 807, 712, 34, 325, 31, 739, 957, 182, 648, 221, 189, 692, 61, 91, 468, 471, 1040, 322, 621, 75, 668, 561, 531, 965, 180, 991, 334, 812, 130, 240, 337, 850, 1098, 1099, 1105, 493, 914, 214, 1200, 288, 465, 548, 589, 466, 810, 582, 153, 868, 488, 588, 793, 401, 1028, 729, 565, 944, 1008, 178, 545, 829, 277, 1041, 728, 206, 190, 1118, 781, 754, 889, 1106, 1176, 308, 1148, 721, 584, 814, 971, 16, 727, 1020, 86, 558, 1048, 1164, 429, 62, 1067, 891, 1013, 295, 1130, 997, 512, 1078, 515, 127, 486, 501, 533, 919, 378, 433, 315, 72, 830, 510, 696, 823, 750, 30, 770, 586, 327, 245, 1121, 888, 341, 702, 662, 1083, 703, 402, 711, 260, 490, 205, 679, 976, 640, 421, 301, 741, 827, 338, 427, 784, 1177, 943, 852, 411, 564, 1156, 762, 863, 797, 800, 302, 958, 422, 716, 293, 138, 1054, 547, 60, 1150, 1186, 884, 102, 917, 522, 387, 789, 714, 113, 951, 804, 1180, 627, 593, 81, 19, 1003, 590, 652, 1202, 37, 732, 330, 157, 774, 95, 233, 502, 211, 700, 463, 625, 270, 605, 9, 147, 1135, 615, 6, 986, 848, 1187, 324, 379, 336, 249, 440, 1031, 626, 375, 351, 663, 449, 276, 98, 11, 307, 376, 40, 1095, 272, 803, 539, 168, 601, 496, 775, 993, 247, 927, 722, 477, 689, 934, 229, 1126, 111, 1162, 747, 107, 420, 979, 859, 874, 574, 1019, 45, 434, 693, 970, 413, 368, 1109, 1174, 1064, 132, 431, 783, 25, 1131, 1129, 885, 99, 179, 543, 563, 65, 305, 880, 1157, 546, 654, 1016, 74, 661, 967, 397, 455, 300, 725, 886, 437, 395, 51, 603, 611, 454, 29, 1125, 380, 907, 382, 734, 1166, 93, 459, 195, 148, 1084, 685, 193, 146, 124, 444, 1094, 484, 64, 1127, 720, 479, 364, 985, 321, 23, 165, 1071, 1152, 499, 436, 49, 912, 1175, 50, 535, 1197, 1123, 688, 949, 1006, 446, 655, 1208, 676, 207, 396, 939, 710, 948, 412, 131, 544, 1072, 1061, 217, 356, 198, 191, 243, 841, 115, 1012, 474, 10, 613, 252, 359, 432, 1079, 66, 921, 448, 267, 228, 1033, 352, 541, 1, 1023, 792, 285, 1161, 641, 373, 134, 715, 645, 763, 787, 825, 583, 210, 1140, 142, 443, 1132, 860, 24, 400, 188, 230, 571, 497, 897, 639, 409, 1086, 103, 594, 85, 1102, 1178, 82, 292, 595, 575, 273, 498, 94, 867, 296, 740, 516, 845, 79, 898, 1068, 244, 963, 607, 1207, 105, 773, 175, 1030, 665, 176, 218, 286, 695, 802, 1116, 452, 489, 1201, 145, 557, 1145, 893, 234, 156, 504, 77, 579, 1097, 542, 977, 152, 866, 236, 439, 519, 326, 71, 237, 304, 838, 1206, 1026, 647, 992, 570, 694, 758, 1185, 937, 13, 22, 748, 390, 796, 1039, 1137, 333, 392, 261, 690, 1018, 1146, 871, 881, 634, 511, 1049, 521, 1171, 129, 3, 491, 549, 765, 697, 864, 371, 994, 550, 209, 59, 503, 372, 909, 508, 257, 403, 870, 461, 253, 1055, 57, 1011, 682, 143, 1070, 974, 262, 928, 798, 36, 360, 719, 608, 1168, 902, 90, 576, 527, 540, 996, 990, 577, 858, 1002, 67, 414, 349, 44, 1090, 733, 1133, 1190, 811, 442, 671, 18, 755, 788, 89, 972, 0, 518, 344, 735, 159, 960, 1007, 139, 495, 1103, 340, 901, 1212, 408, 772, 35, 940, 487, 1154, 164, 854, 1085, 476, 1029, 922, 869, 821, 279, 1117, 989, 617, 470, 374, 900, 834, 904, 242, 15, 756, 5, 1160, 873, 1194, 776, 828, 20, 573, 56, 656, 824, 680, 581, 524, 1128, 562, 777, 1081, 769, 1077, 492, 630, 42, 759, 310, 622, 388, 274, 1005, 259, 1184, 1015, 271, 33, 1214, 1035, 280, 192, 964, 441, 555, 623, 342, 794, 677, 906, 638, 473, 709, 707, 908, 856, 14, 894, 1158, 799, 743, 520, 219, 306, 910, 1192, 1087, 795, 32, 332, 141, 1120, 786, 554, 785, 1198, 643, 1172, 1138, 1014, 248, 903, 947, 354, 224, 1043, 674, 1204, 144, 1075, 435, 768, 953]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7495428340862845
the save name prefix for this run is:  chkpt-ID_7495428340862845_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 18
rank avg (pred): 0.473 +- 0.005
mrr vals (pred, true): 0.016, 0.184
batch losses (mrrl, rdl): 0.0, 0.0016499168

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1200
rank avg (pred): 0.462 +- 0.002
mrr vals (pred, true): 0.016, 0.047
batch losses (mrrl, rdl): 0.0, 7.83655e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 298
rank avg (pred): 0.201 +- 0.002
mrr vals (pred, true): 0.036, 0.250
batch losses (mrrl, rdl): 0.0, 7.96458e-05

Epoch over!
epoch time: 15.044

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 212
rank avg (pred): 0.444 +- 0.002
mrr vals (pred, true): 0.017, 0.047
batch losses (mrrl, rdl): 0.0, 8.18382e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 911
rank avg (pred): 0.519 +- 0.002
mrr vals (pred, true): 0.014, 0.020
batch losses (mrrl, rdl): 0.0, 7.07077e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 346
rank avg (pred): 0.436 +- 0.003
mrr vals (pred, true): 0.017, 0.051
batch losses (mrrl, rdl): 0.0, 8.72992e-05

Epoch over!
epoch time: 15.036

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1131
rank avg (pred): 0.445 +- 0.002
mrr vals (pred, true): 0.016, 0.043
batch losses (mrrl, rdl): 0.0, 6.96828e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1062
rank avg (pred): 0.172 +- 0.001
mrr vals (pred, true): 0.042, 0.273
batch losses (mrrl, rdl): 0.0, 3.11345e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 822
rank avg (pred): 0.141 +- 0.001
mrr vals (pred, true): 0.050, 0.307
batch losses (mrrl, rdl): 0.0, 2.48024e-05

Epoch over!
epoch time: 15.034

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 491
rank avg (pred): 0.504 +- 0.001
mrr vals (pred, true): 0.015, 0.024
batch losses (mrrl, rdl): 0.0, 3.45943e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1084
rank avg (pred): 0.452 +- 0.000
mrr vals (pred, true): 0.016, 0.042
batch losses (mrrl, rdl): 0.0, 8.88154e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 997
rank avg (pred): 0.203 +- 0.001
mrr vals (pred, true): 0.036, 0.285
batch losses (mrrl, rdl): 0.0, 4.63734e-05

Epoch over!
epoch time: 15.005

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 172
rank avg (pred): 0.453 +- 0.000
mrr vals (pred, true): 0.016, 0.047
batch losses (mrrl, rdl): 0.0, 7.83277e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 817
rank avg (pred): 0.164 +- 0.001
mrr vals (pred, true): 0.044, 0.230
batch losses (mrrl, rdl): 0.0, 2.11246e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 279
rank avg (pred): 0.152 +- 0.001
mrr vals (pred, true): 0.047, 0.217
batch losses (mrrl, rdl): 0.0, 6.91809e-05

Epoch over!
epoch time: 14.918

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 435
rank avg (pred): 0.446 +- 0.000
mrr vals (pred, true): 0.016, 0.047
batch losses (mrrl, rdl): 0.011239659, 8.76424e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 262
rank avg (pred): 0.053 +- 0.040
mrr vals (pred, true): 0.252, 0.222
batch losses (mrrl, rdl): 0.0094949938, 0.0004468722

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 646
rank avg (pred): 0.572 +- 0.209
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 7.83526e-05, 0.0003383209

Epoch over!
epoch time: 15.109

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 863
rank avg (pred): 0.483 +- 0.147
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003498111, 8.81081e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 859
rank avg (pred): 0.443 +- 0.137
mrr vals (pred, true): 0.048, 0.047
batch losses (mrrl, rdl): 6.19425e-05, 4.27313e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1141
rank avg (pred): 0.538 +- 0.187
mrr vals (pred, true): 0.049, 0.023
batch losses (mrrl, rdl): 1.31243e-05, 7.09675e-05

Epoch over!
epoch time: 15.104

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 935
rank avg (pred): 0.499 +- 0.181
mrr vals (pred, true): 0.054, 0.027
batch losses (mrrl, rdl): 0.0001359868, 6.3126e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 238
rank avg (pred): 0.468 +- 0.147
mrr vals (pred, true): 0.049, 0.054
batch losses (mrrl, rdl): 4.4114e-06, 8.57105e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 476
rank avg (pred): 0.466 +- 0.131
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001267349, 6.66988e-05

Epoch over!
epoch time: 15.07

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 593
rank avg (pred): 0.457 +- 0.138
mrr vals (pred, true): 0.048, 0.037
batch losses (mrrl, rdl): 5.70103e-05, 3.88155e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 580
rank avg (pred): 0.458 +- 0.142
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 1.361e-07, 3.31774e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.434 +- 0.098
mrr vals (pred, true): 0.042, 0.052
batch losses (mrrl, rdl): 0.0006228516, 5.97107e-05

Epoch over!
epoch time: 15.084

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1073
rank avg (pred): 0.063 +- 0.044
mrr vals (pred, true): 0.310, 0.272
batch losses (mrrl, rdl): 0.0147031909, 0.000148509

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 248
rank avg (pred): 0.235 +- 0.147
mrr vals (pred, true): 0.223, 0.274
batch losses (mrrl, rdl): 0.0256065931, 0.0002509506

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1013
rank avg (pred): 0.497 +- 0.125
mrr vals (pred, true): 0.048, 0.046
batch losses (mrrl, rdl): 3.0676e-05, 0.0001255728

Epoch over!
epoch time: 15.073

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1072
rank avg (pred): 0.138 +- 0.090
mrr vals (pred, true): 0.267, 0.307
batch losses (mrrl, rdl): 0.0159787349, 1.17954e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1125
rank avg (pred): 0.504 +- 0.153
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.000148199, 0.0001277546

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 33
rank avg (pred): 0.263 +- 0.158
mrr vals (pred, true): 0.231, 0.212
batch losses (mrrl, rdl): 0.0036421863, 0.0002496187

Epoch over!
epoch time: 15.086

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 616
rank avg (pred): 0.429 +- 0.118
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 5.18172e-05, 7.21882e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 308
rank avg (pred): 0.259 +- 0.159
mrr vals (pred, true): 0.236, 0.228
batch losses (mrrl, rdl): 0.0006846382, 0.0001672519

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 479
rank avg (pred): 0.432 +- 0.119
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 1.03954e-05, 5.05167e-05

Epoch over!
epoch time: 15.183

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 579
rank avg (pred): 0.474 +- 0.118
mrr vals (pred, true): 0.048, 0.041
batch losses (mrrl, rdl): 5.28006e-05, 4.72747e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 795
rank avg (pred): 0.422 +- 0.114
mrr vals (pred, true): 0.051, 0.041
batch losses (mrrl, rdl): 1.81235e-05, 7.73261e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1106
rank avg (pred): 0.444 +- 0.128
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 7.24762e-05, 4.77321e-05

Epoch over!
epoch time: 15.22

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 935
rank avg (pred): 0.467 +- 0.131
mrr vals (pred, true): 0.052, 0.027
batch losses (mrrl, rdl): 5.24478e-05, 4.93954e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1100
rank avg (pred): 0.441 +- 0.120
mrr vals (pred, true): 0.046, 0.051
batch losses (mrrl, rdl): 0.0001310312, 4.79902e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 147
rank avg (pred): 0.424 +- 0.134
mrr vals (pred, true): 0.049, 0.037
batch losses (mrrl, rdl): 9.8492e-06, 8.00965e-05

Epoch over!
epoch time: 15.227

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 473
rank avg (pred): 0.423 +- 0.137
mrr vals (pred, true): 0.047, 0.054
batch losses (mrrl, rdl): 9.38971e-05, 4.74518e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 502
rank avg (pred): 0.485 +- 0.131
mrr vals (pred, true): 0.050, 0.022
batch losses (mrrl, rdl): 2.0428e-06, 3.82264e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 868
rank avg (pred): 0.395 +- 0.137
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002611994, 6.91613e-05

Epoch over!
epoch time: 15.199

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.468 +- 0.145
mrr vals (pred, true): 0.052, 0.026

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04846 	 0.01773 	 m..s
   58 	     1 	 0.05079 	 0.02010 	 m..s
   54 	     2 	 0.05060 	 0.02302 	 ~...
   47 	     3 	 0.05036 	 0.02311 	 ~...
   23 	     4 	 0.04954 	 0.02447 	 ~...
   26 	     5 	 0.04955 	 0.02459 	 ~...
   40 	     6 	 0.05016 	 0.02482 	 ~...
    1 	     7 	 0.04631 	 0.02497 	 ~...
   90 	     8 	 0.05260 	 0.02536 	 ~...
    8 	     9 	 0.04914 	 0.02539 	 ~...
   45 	    10 	 0.05030 	 0.02544 	 ~...
    9 	    11 	 0.04925 	 0.02571 	 ~...
    0 	    12 	 0.04631 	 0.02579 	 ~...
   75 	    13 	 0.05174 	 0.02609 	 ~...
   52 	    14 	 0.05053 	 0.02646 	 ~...
   25 	    15 	 0.04955 	 0.02843 	 ~...
   22 	    16 	 0.04953 	 0.02898 	 ~...
   94 	    17 	 0.05365 	 0.02943 	 ~...
   95 	    18 	 0.05412 	 0.02966 	 ~...
   27 	    19 	 0.04957 	 0.03500 	 ~...
   30 	    20 	 0.04980 	 0.03512 	 ~...
   77 	    21 	 0.05181 	 0.03549 	 ~...
    4 	    22 	 0.04896 	 0.03587 	 ~...
    5 	    23 	 0.04897 	 0.03730 	 ~...
   43 	    24 	 0.05024 	 0.03825 	 ~...
   57 	    25 	 0.05078 	 0.03838 	 ~...
   49 	    26 	 0.05043 	 0.03920 	 ~...
   48 	    27 	 0.05043 	 0.03950 	 ~...
   44 	    28 	 0.05024 	 0.04020 	 ~...
   62 	    29 	 0.05089 	 0.04061 	 ~...
   87 	    30 	 0.05250 	 0.04080 	 ~...
   36 	    31 	 0.04996 	 0.04101 	 ~...
    6 	    32 	 0.04906 	 0.04165 	 ~...
   34 	    33 	 0.04989 	 0.04169 	 ~...
   33 	    34 	 0.04985 	 0.04270 	 ~...
    3 	    35 	 0.04879 	 0.04337 	 ~...
   66 	    36 	 0.05117 	 0.04380 	 ~...
   70 	    37 	 0.05134 	 0.04381 	 ~...
   72 	    38 	 0.05160 	 0.04403 	 ~...
   88 	    39 	 0.05253 	 0.04418 	 ~...
   50 	    40 	 0.05048 	 0.04418 	 ~...
   93 	    41 	 0.05357 	 0.04418 	 ~...
   15 	    42 	 0.04945 	 0.04420 	 ~...
   80 	    43 	 0.05212 	 0.04446 	 ~...
   16 	    44 	 0.04946 	 0.04460 	 ~...
   86 	    45 	 0.05240 	 0.04473 	 ~...
   13 	    46 	 0.04940 	 0.04476 	 ~...
   55 	    47 	 0.05070 	 0.04493 	 ~...
   35 	    48 	 0.04991 	 0.04493 	 ~...
   82 	    49 	 0.05219 	 0.04494 	 ~...
   83 	    50 	 0.05228 	 0.04505 	 ~...
   59 	    51 	 0.05080 	 0.04510 	 ~...
   64 	    52 	 0.05104 	 0.04515 	 ~...
   46 	    53 	 0.05034 	 0.04527 	 ~...
   85 	    54 	 0.05239 	 0.04549 	 ~...
   11 	    55 	 0.04931 	 0.04564 	 ~...
   81 	    56 	 0.05217 	 0.04565 	 ~...
   71 	    57 	 0.05134 	 0.04584 	 ~...
   68 	    58 	 0.05128 	 0.04608 	 ~...
   76 	    59 	 0.05179 	 0.04618 	 ~...
   31 	    60 	 0.04980 	 0.04635 	 ~...
   67 	    61 	 0.05123 	 0.04642 	 ~...
    7 	    62 	 0.04906 	 0.04646 	 ~...
   60 	    63 	 0.05080 	 0.04695 	 ~...
   51 	    64 	 0.05053 	 0.04712 	 ~...
   24 	    65 	 0.04954 	 0.04727 	 ~...
   56 	    66 	 0.05073 	 0.04736 	 ~...
   14 	    67 	 0.04945 	 0.04759 	 ~...
   74 	    68 	 0.05170 	 0.04775 	 ~...
   78 	    69 	 0.05204 	 0.04777 	 ~...
   17 	    70 	 0.04947 	 0.04819 	 ~...
   19 	    71 	 0.04948 	 0.04827 	 ~...
   39 	    72 	 0.05014 	 0.04872 	 ~...
   21 	    73 	 0.04953 	 0.04879 	 ~...
   65 	    74 	 0.05105 	 0.04879 	 ~...
   69 	    75 	 0.05132 	 0.04886 	 ~...
   73 	    76 	 0.05165 	 0.04953 	 ~...
   79 	    77 	 0.05212 	 0.04958 	 ~...
   41 	    78 	 0.05022 	 0.04989 	 ~...
   89 	    79 	 0.05255 	 0.05004 	 ~...
   10 	    80 	 0.04926 	 0.05051 	 ~...
   37 	    81 	 0.05002 	 0.05068 	 ~...
   84 	    82 	 0.05232 	 0.05151 	 ~...
   42 	    83 	 0.05022 	 0.05157 	 ~...
   29 	    84 	 0.04978 	 0.05160 	 ~...
   53 	    85 	 0.05055 	 0.05178 	 ~...
   63 	    86 	 0.05102 	 0.05215 	 ~...
   12 	    87 	 0.04933 	 0.05275 	 ~...
   91 	    88 	 0.05270 	 0.05299 	 ~...
   18 	    89 	 0.04947 	 0.05300 	 ~...
   92 	    90 	 0.05352 	 0.05320 	 ~...
   32 	    91 	 0.04982 	 0.05367 	 ~...
   61 	    92 	 0.05081 	 0.05409 	 ~...
   20 	    93 	 0.04953 	 0.05453 	 ~...
   28 	    94 	 0.04960 	 0.05546 	 ~...
   38 	    95 	 0.05004 	 0.05611 	 ~...
   99 	    96 	 0.23176 	 0.19234 	 m..s
  102 	    97 	 0.23941 	 0.19545 	 m..s
   98 	    98 	 0.23024 	 0.20165 	 ~...
   97 	    99 	 0.19645 	 0.20688 	 ~...
   96 	   100 	 0.19126 	 0.20756 	 ~...
  104 	   101 	 0.24109 	 0.20872 	 m..s
  105 	   102 	 0.24437 	 0.21610 	 ~...
  101 	   103 	 0.23911 	 0.22610 	 ~...
  103 	   104 	 0.23958 	 0.22773 	 ~...
  100 	   105 	 0.23726 	 0.23554 	 ~...
  106 	   106 	 0.24795 	 0.24338 	 ~...
  108 	   107 	 0.25709 	 0.24927 	 ~...
  110 	   108 	 0.29158 	 0.26234 	 ~...
  107 	   109 	 0.24839 	 0.26409 	 ~...
  117 	   110 	 0.30463 	 0.26525 	 m..s
  110 	   111 	 0.29158 	 0.26880 	 ~...
  110 	   112 	 0.29158 	 0.27463 	 ~...
  116 	   113 	 0.30371 	 0.28160 	 ~...
  110 	   114 	 0.29158 	 0.28743 	 ~...
  110 	   115 	 0.29158 	 0.28862 	 ~...
  109 	   116 	 0.26939 	 0.29247 	 ~...
  119 	   117 	 0.31095 	 0.32358 	 ~...
  118 	   118 	 0.30710 	 0.33429 	 ~...
  110 	   119 	 0.29158 	 0.33485 	 m..s
  120 	   120 	 0.37891 	 0.39525 	 ~...
==========================================
r_mrr = 0.9893356561660767
r2_mrr = 0.9703632593154907
spearmanr_mrr@5 = 0.9132758975028992
spearmanr_mrr@10 = 0.9144891500473022
spearmanr_mrr@50 = 0.9941226840019226
spearmanr_mrr@100 = 0.9957777857780457
spearmanr_mrr@All = 0.993498682975769
==========================================
test time: 0.451
Done Testing dataset UMLS
total time taken: 233.02339816093445
training time taken: 226.8535692691803
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9893)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9704)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9133)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9145)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9941)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9958)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9935)}}, 'test_loss': {'ComplEx': {'UMLS': 0.15629299875945435}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4414916609848972
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [829, 74, 155, 1121, 1086, 191, 344, 1171, 896, 614, 153, 449, 857, 619, 1059, 434, 719, 594, 76, 261, 1036, 980, 731, 419, 134, 212, 190, 1123, 353, 211, 327, 560, 443, 730, 704, 75, 737, 404, 1106, 301, 563, 1136, 1102, 1093, 1074, 14, 591, 986, 467, 1170, 766, 627, 348, 971, 218, 420, 341, 297, 595, 753, 411, 1185, 93, 1176, 21, 120, 406, 983, 665, 396, 450, 934, 73, 51, 1031, 1048, 796, 909, 194, 706, 335, 1205, 1165, 587, 552, 247, 1113, 238, 649, 834, 132, 256, 260, 855, 1, 905, 1070, 1005, 528, 41, 334, 982, 492, 1000, 455, 604, 39, 990, 1053, 184, 641, 844, 89, 973, 500, 452, 209, 965, 498, 997, 920]
valid_ids (0): []
train_ids (1094): [1033, 551, 150, 9, 200, 948, 44, 15, 1180, 571, 930, 741, 299, 270, 784, 136, 374, 114, 732, 532, 929, 817, 622, 117, 554, 354, 795, 277, 129, 103, 1193, 557, 638, 1064, 1041, 520, 978, 987, 1016, 502, 293, 488, 540, 61, 1181, 280, 265, 616, 881, 830, 803, 565, 17, 1058, 818, 887, 513, 85, 154, 534, 388, 842, 287, 442, 879, 257, 911, 698, 972, 776, 1083, 1076, 1152, 224, 139, 462, 700, 10, 904, 1139, 1066, 788, 690, 50, 1198, 446, 979, 692, 1046, 273, 339, 1084, 309, 146, 439, 26, 564, 473, 138, 593, 580, 415, 456, 494, 168, 635, 727, 1056, 674, 860, 1047, 944, 1111, 1129, 64, 285, 54, 135, 651, 286, 820, 611, 445, 384, 232, 728, 505, 1196, 996, 765, 600, 414, 937, 1199, 116, 409, 1004, 679, 395, 538, 548, 831, 172, 1208, 1120, 440, 175, 791, 780, 561, 459, 963, 480, 48, 63, 1044, 824, 271, 1142, 382, 49, 599, 774, 142, 694, 639, 52, 244, 152, 330, 1168, 615, 304, 957, 512, 800, 642, 268, 609, 799, 787, 1039, 1037, 1001, 1130, 461, 838, 399, 1188, 72, 691, 969, 206, 36, 710, 606, 387, 797, 882, 137, 1020, 1071, 763, 547, 629, 94, 230, 722, 424, 597, 106, 441, 880, 681, 507, 32, 883, 417, 1122, 959, 715, 141, 598, 158, 1124, 740, 759, 1161, 504, 27, 99, 1211, 1002, 65, 68, 754, 274, 847, 901, 392, 756, 523, 8, 912, 977, 5, 368, 1184, 18, 1051, 655, 542, 203, 644, 251, 664, 954, 733, 482, 917, 861, 358, 1197, 1203, 1015, 1081, 111, 497, 670, 97, 290, 123, 777, 695, 886, 1105, 808, 966, 1169, 479, 485, 778, 592, 267, 789, 603, 573, 453, 975, 1023, 248, 839, 1024, 159, 284, 148, 1101, 677, 903, 663, 1183, 632, 363, 854, 914, 43, 687, 678, 661, 984, 913, 900, 867, 1144, 558, 28, 510, 174, 643, 899, 360, 1077, 131, 416, 12, 1092, 993, 1206, 515, 846, 188, 553, 768, 947, 840, 658, 669, 264, 617, 1209, 107, 1114, 1116, 1189, 127, 1146, 310, 870, 810, 724, 813, 47, 918, 331, 620, 31, 88, 156, 457, 356, 648, 1025, 288, 118, 180, 83, 657, 113, 725, 760, 877, 1008, 586, 3, 1212, 197, 974, 1140, 0, 832, 352, 570, 1164, 805, 888, 13, 916, 323, 885, 34, 1104, 367, 1028, 991, 222, 377, 544, 1191, 1095, 216, 567, 42, 329, 584, 794, 460, 472, 1009, 165, 219, 263, 1043, 1125, 1022, 852, 1158, 637, 748, 764, 961, 130, 898, 828, 183, 1112, 254, 864, 1182, 432, 253, 365, 549, 204, 1207, 522, 489, 448, 1190, 124, 454, 202, 444, 407, 1091, 233, 906, 541, 856, 226, 702, 652, 343, 699, 381, 466, 40, 246, 1119, 667, 291, 1145, 585, 259, 755, 1110, 734, 543, 953, 696, 516, 676, 24, 943, 964, 941, 385, 682, 827, 1089, 1160, 231, 56, 735, 179, 207, 486, 1153, 100, 581, 921, 187, 751, 804, 823, 793, 1061, 508, 1055, 305, 853, 144, 30, 785, 321, 346, 84, 869, 1078, 618, 902, 295, 509, 371, 79, 720, 347, 173, 758, 316, 925, 82, 1054, 147, 78, 370, 357, 315, 59, 60, 1162, 1010, 430, 866, 333, 895, 897, 272, 110, 87, 1137, 506, 413, 579, 401, 1202, 96, 556, 1163, 716, 198, 1067, 1131, 1103, 1073, 868, 390, 647, 19, 1032, 1049, 950, 939, 1052, 189, 6, 688, 1150, 940, 640, 653, 524, 81, 1143, 45, 711, 359, 283, 355, 1157, 275, 626, 956, 307, 386, 1147, 182, 90, 214, 311, 1118, 662, 607, 393, 306, 806, 529, 518, 527, 66, 421, 140, 1087, 671, 1149, 514, 892, 942, 757, 968, 628, 1148, 398, 583, 62, 340, 525, 252, 533, 790, 569, 709, 164, 20, 1214, 469, 625, 517, 1156, 496, 336, 229, 503, 1108, 729, 468, 77, 484, 1179, 383, 463, 405, 845, 816, 318, 1141, 962, 910, 672, 697, 239, 511, 574, 889, 490, 1079, 621, 163, 1035, 919, 437, 874, 366, 976, 361, 33, 255, 798, 848, 1201, 825, 499, 320, 1019, 822, 701, 372, 458, 807, 133, 1213, 612, 435, 631, 1097, 145, 91, 478, 1109, 927, 217, 960, 199, 495, 213, 705, 588, 526, 177, 1175, 71, 119, 412, 952, 37, 559, 782, 809, 746, 302, 750, 22, 1011, 470, 429, 1017, 313, 907, 531, 235, 521, 1115, 668, 769, 225, 128, 908, 545, 410, 4, 105, 596, 493, 933, 708, 1187, 1133, 1030, 935, 477, 575, 380, 364, 240, 319, 471, 258, 186, 1127, 221, 167, 772, 865, 634, 210, 1100, 1014, 427, 143, 195, 814, 636, 241, 428, 296, 1085, 476, 276, 1042, 1090, 322, 659, 815, 125, 481, 1134, 1080, 325, 605, 938, 601, 802, 836, 878, 786, 151, 1060, 101, 1021, 46, 703, 324, 1068, 876, 104, 1007, 1195, 161, 893, 1003, 922, 723, 841, 298, 958, 1050, 590, 872, 999, 176, 562, 1151, 602, 871, 126, 185, 781, 819, 86, 249, 337, 38, 689, 80, 530, 242, 623, 1154, 369, 474, 890, 29, 749, 693, 994, 1126, 536, 955, 752, 122, 650, 464, 1167, 767, 171, 537, 718, 539, 736, 215, 770, 1128, 717, 109, 884, 379, 779, 58, 436, 418, 350, 985, 576, 1012, 645, 1138, 835, 108, 673, 608, 1082, 11, 236, 431, 192, 572, 928, 308, 932, 1096, 568, 170, 833, 314, 851, 519, 1132, 926, 1038, 837, 761, 685, 747, 1062, 773, 294, 223, 112, 378, 53, 712, 397, 891, 610, 1204, 228, 578, 1045, 646, 282, 894, 25, 1178, 1099, 550, 1192, 70, 491, 1172, 328, 69, 394, 745, 1027, 949, 589, 1135, 812, 95, 738, 115, 266, 660, 98, 713, 169, 1029, 92, 1040, 1210, 220, 821, 451, 402, 7, 403, 555, 624, 317, 546, 1194, 1065, 995, 423, 196, 945, 426, 686, 1075, 389, 245, 281, 946, 289, 55, 160, 577, 465, 633, 475, 923, 57, 35, 1006, 862, 936, 1057, 102, 873, 762, 408, 501, 967, 859, 863, 707, 630, 375, 988, 205, 1034, 1013, 234, 227, 1098, 680, 262, 771, 1200, 743, 237, 23, 422, 181, 989, 67, 656, 1063, 721, 345, 269, 1072, 970, 300, 801, 850, 312, 487, 157, 775, 362, 915, 193, 1174, 998, 875, 201, 858, 684, 162, 121, 433, 391, 373, 981, 1159, 683, 1173, 666, 483, 726, 811, 376, 566, 2, 1155, 739, 1088, 1094, 438, 744, 425, 292, 1117, 178, 843, 278, 1107, 826, 992, 166, 742, 349, 16, 1026, 208, 849, 338, 447, 1186, 783, 332, 1069, 1018, 582, 250, 400, 326, 535, 303, 1166, 924, 951, 351, 654, 149, 279, 792, 613, 243, 342, 1177, 931, 675, 714]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9259870135941546
the save name prefix for this run is:  chkpt-ID_9259870135941546_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 959
rank avg (pred): 0.455 +- 0.006
mrr vals (pred, true): 0.016, 0.042
batch losses (mrrl, rdl): 0.0, 7.52804e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 691
rank avg (pred): 0.470 +- 0.010
mrr vals (pred, true): 0.016, 0.047
batch losses (mrrl, rdl): 0.0, 8.66118e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 310
rank avg (pred): 0.172 +- 0.007
mrr vals (pred, true): 0.042, 0.220
batch losses (mrrl, rdl): 0.0, 4.8014e-05

Epoch over!
epoch time: 15.07

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 793
rank avg (pred): 0.444 +- 0.042
mrr vals (pred, true): 0.017, 0.041
batch losses (mrrl, rdl): 0.0, 7.36895e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 160
rank avg (pred): 0.453 +- 0.263
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 0.0, 3.3262e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 791
rank avg (pred): 0.446 +- 0.266
mrr vals (pred, true): 0.058, 0.049
batch losses (mrrl, rdl): 0.0, 8.1574e-06

Epoch over!
epoch time: 15.003

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 535
rank avg (pred): 0.509 +- 0.247
mrr vals (pred, true): 0.040, 0.024
batch losses (mrrl, rdl): 0.0, 3.1342e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 392
rank avg (pred): 0.432 +- 0.260
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.0, 4.5603e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 771
rank avg (pred): 0.444 +- 0.257
mrr vals (pred, true): 0.055, 0.045
batch losses (mrrl, rdl): 0.0, 1.443e-07

Epoch over!
epoch time: 14.831

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 724
rank avg (pred): 0.433 +- 0.278
mrr vals (pred, true): 0.083, 0.044
batch losses (mrrl, rdl): 0.0, 2.6998e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 179
rank avg (pred): 0.437 +- 0.263
mrr vals (pred, true): 0.067, 0.054
batch losses (mrrl, rdl): 0.0, 3.1658e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 221
rank avg (pred): 0.464 +- 0.259
mrr vals (pred, true): 0.049, 0.058
batch losses (mrrl, rdl): 0.0, 7.6671e-06

Epoch over!
epoch time: 14.834

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1146
rank avg (pred): 0.482 +- 0.247
mrr vals (pred, true): 0.053, 0.023
batch losses (mrrl, rdl): 0.0, 6.6807e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 279
rank avg (pred): 0.180 +- 0.201
mrr vals (pred, true): 0.140, 0.217
batch losses (mrrl, rdl): 0.0, 1.1064e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 651
rank avg (pred): 0.439 +- 0.260
mrr vals (pred, true): 0.066, 0.049
batch losses (mrrl, rdl): 0.0, 1.1861e-06

Epoch over!
epoch time: 14.855

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 127
rank avg (pred): 0.447 +- 0.264
mrr vals (pred, true): 0.062, 0.045
batch losses (mrrl, rdl): 0.0015196642, 4.7058e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 549
rank avg (pred): 0.523 +- 0.225
mrr vals (pred, true): 0.029, 0.023
batch losses (mrrl, rdl): 0.0043413318, 2.2615e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 418
rank avg (pred): 0.512 +- 0.234
mrr vals (pred, true): 0.040, 0.051
batch losses (mrrl, rdl): 0.0010470628, 6.99975e-05

Epoch over!
epoch time: 14.987

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 484
rank avg (pred): 0.500 +- 0.246
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 9.49932e-05, 4.50462e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 519
rank avg (pred): 0.482 +- 0.209
mrr vals (pred, true): 0.051, 0.023
batch losses (mrrl, rdl): 4.2343e-06, 4.47239e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 7
rank avg (pred): 0.217 +- 0.305
mrr vals (pred, true): 0.258, 0.243
batch losses (mrrl, rdl): 0.0024562646, 0.0001071402

Epoch over!
epoch time: 14.972

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1137
rank avg (pred): 0.476 +- 0.208
mrr vals (pred, true): 0.057, 0.027
batch losses (mrrl, rdl): 0.000442123, 9.8147e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 864
rank avg (pred): 0.474 +- 0.188
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 8.2939e-06, 2.91651e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1196
rank avg (pred): 0.468 +- 0.170
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 5.42412e-05, 3.03124e-05

Epoch over!
epoch time: 14.974

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1064
rank avg (pred): 0.177 +- 0.259
mrr vals (pred, true): 0.285, 0.289
batch losses (mrrl, rdl): 0.0002151368, 5.63645e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1194
rank avg (pred): 0.490 +- 0.200
mrr vals (pred, true): 0.057, 0.047
batch losses (mrrl, rdl): 0.0004507346, 6.6672e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1142
rank avg (pred): 0.456 +- 0.130
mrr vals (pred, true): 0.048, 0.024
batch losses (mrrl, rdl): 4.06635e-05, 3.67897e-05

Epoch over!
epoch time: 14.975

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 621
rank avg (pred): 0.509 +- 0.196
mrr vals (pred, true): 0.048, 0.042
batch losses (mrrl, rdl): 5.35084e-05, 6.56513e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 922
rank avg (pred): 0.520 +- 0.143
mrr vals (pred, true): 0.045, 0.037
batch losses (mrrl, rdl): 0.0002192804, 5.33735e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 100
rank avg (pred): 0.450 +- 0.121
mrr vals (pred, true): 0.047, 0.040
batch losses (mrrl, rdl): 0.0001199882, 4.70224e-05

Epoch over!
epoch time: 15.022

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 930
rank avg (pred): 0.479 +- 0.129
mrr vals (pred, true): 0.047, 0.035
batch losses (mrrl, rdl): 8.18833e-05, 2.60974e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1174
rank avg (pred): 0.445 +- 0.105
mrr vals (pred, true): 0.053, 0.034
batch losses (mrrl, rdl): 0.0001173163, 5.54683e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1003
rank avg (pred): 0.422 +- 0.087
mrr vals (pred, true): 0.048, 0.040
batch losses (mrrl, rdl): 2.84339e-05, 7.787e-05

Epoch over!
epoch time: 15.02

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 54
rank avg (pred): 0.308 +- 0.190
mrr vals (pred, true): 0.230, 0.230
batch losses (mrrl, rdl): 1.622e-07, 0.000532299

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 487
rank avg (pred): 0.498 +- 0.145
mrr vals (pred, true): 0.051, 0.024
batch losses (mrrl, rdl): 1.40062e-05, 2.0917e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 293
rank avg (pred): 0.322 +- 0.183
mrr vals (pred, true): 0.202, 0.214
batch losses (mrrl, rdl): 0.0013636374, 0.0004193706

Epoch over!
epoch time: 15.014

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 790
rank avg (pred): 0.419 +- 0.087
mrr vals (pred, true): 0.052, 0.041
batch losses (mrrl, rdl): 2.90861e-05, 8.1595e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 895
rank avg (pred): 0.551 +- 0.199
mrr vals (pred, true): 0.054, 0.021
batch losses (mrrl, rdl): 0.0001749791, 0.0001011384

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 540
rank avg (pred): 0.474 +- 0.120
mrr vals (pred, true): 0.052, 0.026
batch losses (mrrl, rdl): 2.80515e-05, 5.35334e-05

Epoch over!
epoch time: 14.965

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 144
rank avg (pred): 0.416 +- 0.085
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 3.86959e-05, 7.79507e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 811
rank avg (pred): 0.132 +- 0.160
mrr vals (pred, true): 0.349, 0.401
batch losses (mrrl, rdl): 0.0278211832, 3.9558e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 410
rank avg (pred): 0.414 +- 0.088
mrr vals (pred, true): 0.055, 0.055
batch losses (mrrl, rdl): 0.0002563288, 7.34155e-05

Epoch over!
epoch time: 14.976

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 198
rank avg (pred): 0.420 +- 0.081
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002184435, 9.35855e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 357
rank avg (pred): 0.425 +- 0.080
mrr vals (pred, true): 0.043, 0.050
batch losses (mrrl, rdl): 0.0004483782, 6.31101e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 215
rank avg (pred): 0.421 +- 0.082
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.000154931, 7.85314e-05

Epoch over!
epoch time: 14.953

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.175 +- 0.147
mrr vals (pred, true): 0.305, 0.336

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   74 	     0 	 0.05052 	 0.01773 	 m..s
    7 	     1 	 0.04565 	 0.02144 	 ~...
   87 	     2 	 0.05237 	 0.02302 	 ~...
   68 	     3 	 0.05008 	 0.02336 	 ~...
   20 	     4 	 0.04693 	 0.02350 	 ~...
   33 	     5 	 0.04789 	 0.02441 	 ~...
   34 	     6 	 0.04791 	 0.02459 	 ~...
    6 	     7 	 0.04564 	 0.02460 	 ~...
   27 	     8 	 0.04764 	 0.02536 	 ~...
   32 	     9 	 0.04789 	 0.02638 	 ~...
   52 	    10 	 0.04893 	 0.02646 	 ~...
   70 	    11 	 0.05023 	 0.02964 	 ~...
   63 	    12 	 0.04969 	 0.02979 	 ~...
   46 	    13 	 0.04837 	 0.03417 	 ~...
   45 	    14 	 0.04837 	 0.03473 	 ~...
    1 	    15 	 0.04367 	 0.03520 	 ~...
    4 	    16 	 0.04487 	 0.03582 	 ~...
    0 	    17 	 0.04340 	 0.03695 	 ~...
   78 	    18 	 0.05086 	 0.03749 	 ~...
   81 	    19 	 0.05090 	 0.03811 	 ~...
   28 	    20 	 0.04765 	 0.03815 	 ~...
    5 	    21 	 0.04513 	 0.03829 	 ~...
   11 	    22 	 0.04616 	 0.03856 	 ~...
   19 	    23 	 0.04681 	 0.03884 	 ~...
   80 	    24 	 0.05088 	 0.03947 	 ~...
   38 	    25 	 0.04823 	 0.03957 	 ~...
   42 	    26 	 0.04831 	 0.04004 	 ~...
   79 	    27 	 0.05087 	 0.04088 	 ~...
   57 	    28 	 0.04905 	 0.04147 	 ~...
   31 	    29 	 0.04784 	 0.04163 	 ~...
   40 	    30 	 0.04825 	 0.04190 	 ~...
   21 	    31 	 0.04695 	 0.04224 	 ~...
    2 	    32 	 0.04432 	 0.04283 	 ~...
   18 	    33 	 0.04669 	 0.04293 	 ~...
   37 	    34 	 0.04813 	 0.04336 	 ~...
   65 	    35 	 0.04977 	 0.04381 	 ~...
   84 	    36 	 0.05103 	 0.04397 	 ~...
   44 	    37 	 0.04836 	 0.04440 	 ~...
   75 	    38 	 0.05059 	 0.04444 	 ~...
   50 	    39 	 0.04879 	 0.04471 	 ~...
   13 	    40 	 0.04640 	 0.04473 	 ~...
   55 	    41 	 0.04899 	 0.04475 	 ~...
   14 	    42 	 0.04657 	 0.04476 	 ~...
   48 	    43 	 0.04866 	 0.04480 	 ~...
   61 	    44 	 0.04923 	 0.04485 	 ~...
   76 	    45 	 0.05060 	 0.04490 	 ~...
   41 	    46 	 0.04830 	 0.04515 	 ~...
   16 	    47 	 0.04666 	 0.04533 	 ~...
    9 	    48 	 0.04566 	 0.04575 	 ~...
   17 	    49 	 0.04668 	 0.04577 	 ~...
   12 	    50 	 0.04637 	 0.04650 	 ~...
   82 	    51 	 0.05091 	 0.04668 	 ~...
   54 	    52 	 0.04895 	 0.04692 	 ~...
   43 	    53 	 0.04835 	 0.04702 	 ~...
   67 	    54 	 0.04992 	 0.04715 	 ~...
   73 	    55 	 0.05052 	 0.04748 	 ~...
   35 	    56 	 0.04803 	 0.04758 	 ~...
   39 	    57 	 0.04824 	 0.04763 	 ~...
    8 	    58 	 0.04565 	 0.04764 	 ~...
   72 	    59 	 0.05049 	 0.04776 	 ~...
   22 	    60 	 0.04716 	 0.04815 	 ~...
   77 	    61 	 0.05074 	 0.04815 	 ~...
   53 	    62 	 0.04895 	 0.04825 	 ~...
   86 	    63 	 0.05208 	 0.04851 	 ~...
   25 	    64 	 0.04738 	 0.04868 	 ~...
   60 	    65 	 0.04919 	 0.04879 	 ~...
   69 	    66 	 0.05015 	 0.04984 	 ~...
   47 	    67 	 0.04845 	 0.05041 	 ~...
   26 	    68 	 0.04760 	 0.05067 	 ~...
   29 	    69 	 0.04776 	 0.05084 	 ~...
   24 	    70 	 0.04738 	 0.05125 	 ~...
   36 	    71 	 0.04807 	 0.05147 	 ~...
   23 	    72 	 0.04723 	 0.05157 	 ~...
   30 	    73 	 0.04782 	 0.05159 	 ~...
   85 	    74 	 0.05179 	 0.05238 	 ~...
   51 	    75 	 0.04880 	 0.05244 	 ~...
   15 	    76 	 0.04662 	 0.05263 	 ~...
   66 	    77 	 0.04987 	 0.05275 	 ~...
   56 	    78 	 0.04901 	 0.05299 	 ~...
   62 	    79 	 0.04954 	 0.05339 	 ~...
   58 	    80 	 0.04908 	 0.05352 	 ~...
   49 	    81 	 0.04869 	 0.05367 	 ~...
   64 	    82 	 0.04975 	 0.05371 	 ~...
   71 	    83 	 0.05029 	 0.05398 	 ~...
    3 	    84 	 0.04444 	 0.05491 	 ~...
   59 	    85 	 0.04914 	 0.05655 	 ~...
   10 	    86 	 0.04601 	 0.05887 	 ~...
   83 	    87 	 0.05100 	 0.05977 	 ~...
   89 	    88 	 0.19926 	 0.18285 	 ~...
   95 	    89 	 0.22903 	 0.19545 	 m..s
   88 	    90 	 0.19538 	 0.19958 	 ~...
   91 	    91 	 0.20263 	 0.20603 	 ~...
   90 	    92 	 0.19945 	 0.20688 	 ~...
   93 	    93 	 0.20611 	 0.21637 	 ~...
   97 	    94 	 0.23373 	 0.21817 	 ~...
   92 	    95 	 0.20503 	 0.21883 	 ~...
  106 	    96 	 0.27023 	 0.21997 	 m..s
   94 	    97 	 0.21087 	 0.22717 	 ~...
   96 	    98 	 0.23111 	 0.23290 	 ~...
   98 	    99 	 0.23923 	 0.24051 	 ~...
   99 	   100 	 0.23999 	 0.24074 	 ~...
  103 	   101 	 0.24580 	 0.24107 	 ~...
  112 	   102 	 0.29093 	 0.24222 	 m..s
  100 	   103 	 0.24048 	 0.24233 	 ~...
  102 	   104 	 0.24333 	 0.24360 	 ~...
  104 	   105 	 0.26049 	 0.24876 	 ~...
  110 	   106 	 0.28785 	 0.24930 	 m..s
  101 	   107 	 0.24050 	 0.25592 	 ~...
  107 	   108 	 0.28478 	 0.26234 	 ~...
  113 	   109 	 0.29283 	 0.27737 	 ~...
  116 	   110 	 0.30294 	 0.28160 	 ~...
  118 	   111 	 0.30656 	 0.28449 	 ~...
  114 	   112 	 0.29699 	 0.28460 	 ~...
  115 	   113 	 0.29734 	 0.28691 	 ~...
  105 	   114 	 0.26258 	 0.29334 	 m..s
  108 	   115 	 0.28634 	 0.29447 	 ~...
  109 	   116 	 0.28754 	 0.29484 	 ~...
  111 	   117 	 0.28786 	 0.29647 	 ~...
  117 	   118 	 0.30483 	 0.33593 	 m..s
  120 	   119 	 0.38411 	 0.38637 	 ~...
  119 	   120 	 0.37935 	 0.39525 	 ~...
==========================================
r_mrr = 0.9912924766540527
r2_mrr = 0.9800727963447571
spearmanr_mrr@5 = 0.9471464157104492
spearmanr_mrr@10 = 0.9675793051719666
spearmanr_mrr@50 = 0.9963214993476868
spearmanr_mrr@100 = 0.9977669715881348
spearmanr_mrr@All = 0.9963082075119019
==========================================
test time: 0.453
Done Testing dataset UMLS
total time taken: 231.20857095718384
training time taken: 224.9150412082672
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9913)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9801)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9471)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9676)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9963)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9978)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9963)}}, 'test_loss': {'ComplEx': {'UMLS': 0.15381852548489405}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9167631414731028
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [183, 368, 378, 29, 355, 608, 1044, 159, 937, 632, 452, 787, 1139, 1083, 1028, 232, 225, 857, 127, 1128, 642, 269, 718, 546, 208, 258, 1058, 645, 474, 224, 994, 156, 149, 1181, 412, 247, 451, 860, 963, 864, 1169, 26, 1189, 1023, 607, 590, 1084, 1203, 273, 335, 792, 1007, 326, 321, 46, 701, 919, 867, 688, 5, 628, 50, 985, 868, 391, 1052, 785, 1062, 1094, 119, 1002, 881, 264, 779, 1099, 463, 626, 419, 835, 908, 397, 732, 1177, 896, 746, 749, 255, 724, 1073, 577, 244, 565, 1048, 76, 400, 930, 409, 467, 630, 965, 674, 744, 114, 756, 14, 89, 1059, 4, 856, 767, 1145, 662, 466, 346, 19, 635, 912, 976, 726, 621, 131]
valid_ids (0): []
train_ids (1094): [791, 560, 20, 592, 43, 885, 151, 1102, 816, 352, 1187, 152, 1089, 629, 1130, 148, 680, 877, 393, 1026, 521, 643, 349, 300, 212, 354, 528, 44, 1012, 941, 10, 572, 563, 612, 299, 568, 973, 248, 814, 24, 890, 761, 784, 178, 458, 457, 671, 865, 365, 218, 486, 259, 694, 307, 364, 175, 793, 717, 48, 55, 1109, 327, 1172, 506, 1167, 1024, 317, 1027, 1198, 519, 822, 58, 145, 333, 1070, 1166, 338, 1066, 265, 465, 1199, 1036, 808, 1034, 293, 880, 613, 1141, 197, 888, 611, 1212, 53, 324, 682, 494, 382, 473, 54, 838, 777, 1118, 186, 525, 476, 206, 489, 492, 339, 1085, 990, 619, 286, 66, 435, 947, 1133, 698, 1180, 319, 13, 192, 130, 485, 562, 1110, 190, 1041, 128, 641, 686, 508, 848, 1086, 52, 418, 1079, 32, 692, 405, 254, 1096, 712, 989, 115, 341, 647, 834, 998, 56, 549, 770, 510, 797, 1069, 617, 844, 1098, 236, 579, 551, 772, 266, 667, 1132, 1082, 984, 1021, 573, 217, 1, 179, 1148, 786, 1164, 943, 1195, 314, 668, 491, 700, 207, 841, 81, 533, 322, 432, 453, 72, 1088, 1121, 445, 427, 1123, 847, 481, 921, 803, 819, 51, 297, 184, 650, 11, 271, 1191, 539, 1106, 1095, 395, 422, 95, 390, 850, 1149, 980, 1107, 821, 714, 1018, 876, 227, 1116, 366, 854, 403, 443, 972, 411, 15, 514, 578, 535, 64, 977, 1157, 558, 1140, 182, 813, 203, 362, 361, 374, 1093, 234, 1020, 869, 603, 878, 261, 901, 1035, 952, 518, 1202, 421, 721, 407, 669, 678, 728, 389, 738, 129, 79, 110, 472, 195, 820, 922, 1193, 360, 436, 530, 1075, 1192, 308, 773, 291, 589, 933, 120, 863, 1153, 342, 511, 1103, 725, 1174, 1184, 739, 135, 537, 371, 656, 804, 1186, 543, 1003, 30, 69, 139, 213, 294, 719, 133, 383, 1124, 239, 210, 637, 460, 1068, 547, 597, 1163, 150, 141, 999, 801, 798, 1105, 570, 523, 556, 991, 929, 468, 737, 1127, 758, 983, 97, 1178, 379, 1113, 915, 618, 196, 123, 639, 882, 113, 303, 709, 279, 1160, 1016, 544, 41, 843, 82, 211, 425, 100, 855, 978, 1134, 502, 62, 237, 99, 962, 1131, 161, 766, 522, 931, 274, 344, 1090, 504, 968, 742, 1108, 388, 776, 318, 788, 108, 644, 1074, 493, 909, 1072, 516, 1143, 907, 552, 1051, 245, 455, 126, 103, 1115, 858, 554, 795, 974, 903, 38, 685, 174, 424, 722, 104, 969, 125, 666, 730, 61, 600, 497, 1117, 323, 580, 1104, 187, 157, 1154, 337, 328, 906, 1014, 587, 954, 997, 950, 194, 311, 757, 495, 640, 40, 1129, 831, 594, 910, 567, 741, 70, 47, 924, 440, 825, 134, 859, 697, 242, 413, 1214, 193, 1097, 117, 137, 80, 331, 569, 509, 1037, 914, 689, 702, 658, 625, 1147, 45, 653, 301, 233, 221, 396, 616, 655, 837, 584, 849, 755, 272, 800, 462, 461, 817, 471, 270, 484, 1001, 57, 598, 789, 898, 296, 111, 745, 0, 1033, 1207, 928, 960, 691, 155, 751, 1179, 480, 280, 765, 892, 988, 143, 760, 27, 257, 623, 415, 571, 759, 512, 1137, 260, 85, 845, 332, 1013, 290, 967, 936, 434, 92, 73, 163, 981, 209, 874, 200, 169, 633, 1060, 381, 1005, 870, 1125, 112, 1170, 575, 353, 267, 1019, 966, 581, 1071, 601, 1208, 394, 91, 284, 235, 564, 566, 109, 615, 517, 59, 935, 945, 67, 553, 241, 144, 86, 399, 891, 951, 794, 631, 446, 665, 747, 1136, 1008, 214, 96, 879, 531, 1176, 65, 861, 343, 1063, 705, 651, 470, 199, 807, 1204, 648, 703, 735, 604, 1142, 832, 1182, 557, 1009, 281, 240, 423, 1040, 75, 586, 146, 806, 1171, 1077, 942, 1206, 138, 226, 526, 659, 369, 513, 74, 191, 829, 1087, 275, 310, 288, 33, 450, 923, 140, 764, 172, 68, 713, 905, 1100, 204, 1165, 345, 740, 1168, 1043, 283, 315, 654, 768, 1022, 610, 636, 98, 408, 356, 704, 1111, 263, 176, 731, 305, 1138, 1185, 118, 250, 180, 282, 993, 231, 414, 602, 1112, 363, 527, 1025, 417, 1173, 136, 707, 536, 7, 574, 77, 828, 763, 727, 664, 173, 524, 548, 1030, 588, 627, 359, 500, 420, 708, 812, 1076, 529, 672, 609, 351, 826, 313, 1101, 501, 3, 94, 1080, 622, 624, 370, 555, 833, 1054, 559, 1120, 743, 538, 325, 979, 503, 673, 734, 679, 720, 416, 818, 1197, 1049, 306, 278, 900, 1031, 105, 1196, 12, 723, 1045, 1067, 147, 1135, 456, 1144, 576, 154, 925, 1194, 449, 1029, 699, 444, 439, 836, 285, 88, 475, 895, 358, 312, 1047, 532, 448, 165, 695, 437, 28, 1081, 167, 1175, 926, 302, 614, 373, 309, 944, 663, 406, 1057, 769, 1159, 1119, 158, 1061, 596, 357, 606, 84, 101, 889, 189, 902, 330, 887, 550, 783, 469, 238, 866, 478, 591, 1053, 781, 715, 1188, 488, 824, 545, 367, 505, 124, 1011, 805, 431, 953, 957, 36, 334, 934, 401, 31, 918, 304, 22, 60, 911, 428, 706, 220, 752, 799, 652, 410, 675, 780, 477, 634, 684, 1210, 499, 277, 677, 386, 223, 116, 599, 1056, 71, 398, 595, 292, 681, 459, 729, 987, 1211, 438, 298, 16, 593, 649, 690, 774, 375, 295, 251, 683, 1183, 336, 790, 661, 541, 823, 932, 9, 168, 871, 660, 830, 657, 992, 166, 995, 894, 585, 1152, 202, 961, 736, 1078, 583, 1038, 496, 253, 1213, 778, 1114, 842, 958, 540, 185, 920, 733, 853, 716, 430, 377, 927, 498, 1158, 229, 638, 1039, 262, 205, 872, 25, 646, 605, 904, 122, 1017, 1015, 1126, 479, 87, 268, 996, 1190, 940, 753, 1065, 17, 181, 1150, 162, 827, 8, 956, 487, 955, 846, 975, 1055, 507, 775, 852, 873, 875, 380, 1091, 483, 748, 676, 316, 170, 107, 490, 320, 347, 49, 970, 106, 1201, 939, 693, 198, 201, 102, 153, 986, 1122, 35, 938, 946, 442, 132, 520, 447, 482, 160, 6, 1161, 1050, 1205, 256, 2, 620, 1146, 1032, 710, 392, 219, 899, 188, 1162, 1155, 372, 329, 289, 782, 1156, 230, 851, 948, 350, 810, 249, 982, 276, 78, 287, 1092, 711, 177, 1151, 802, 90, 949, 897, 534, 39, 815, 754, 222, 142, 964, 913, 542, 1010, 809, 404, 1046, 34, 464, 582, 959, 441, 696, 37, 384, 670, 840, 215, 1042, 83, 454, 1004, 884, 402, 93, 1064, 243, 1209, 164, 1006, 893, 771, 839, 1200, 687, 376, 18, 796, 228, 917, 561, 426, 216, 886, 21, 121, 246, 1000, 385, 387, 750, 429, 916, 762, 252, 883, 23, 63, 340, 515, 171, 433, 971, 862, 42, 348, 811]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5633655587527450
the save name prefix for this run is:  chkpt-ID_5633655587527450_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 854
rank avg (pred): 0.513 +- 0.005
mrr vals (pred, true): 0.014, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001312286

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1095
rank avg (pred): 0.423 +- 0.169
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0, 2.6192e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 958
rank avg (pred): 0.449 +- 0.241
mrr vals (pred, true): 0.134, 0.046
batch losses (mrrl, rdl): 0.0, 1.11376e-05

Epoch over!
epoch time: 14.886

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 431
rank avg (pred): 0.420 +- 0.249
mrr vals (pred, true): 0.144, 0.056
batch losses (mrrl, rdl): 0.0, 5.4561e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 789
rank avg (pred): 0.426 +- 0.265
mrr vals (pred, true): 0.139, 0.049
batch losses (mrrl, rdl): 0.0, 4.7155e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 813
rank avg (pred): 0.131 +- 0.097
mrr vals (pred, true): 0.228, 0.550
batch losses (mrrl, rdl): 0.0, 0.0001442853

Epoch over!
epoch time: 15.034

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 521
rank avg (pred): 0.494 +- 0.248
mrr vals (pred, true): 0.086, 0.025
batch losses (mrrl, rdl): 0.0, 7.47e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 207
rank avg (pred): 0.420 +- 0.254
mrr vals (pred, true): 0.104, 0.045
batch losses (mrrl, rdl): 0.0, 2.7575e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 826
rank avg (pred): 0.161 +- 0.126
mrr vals (pred, true): 0.207, 0.245
batch losses (mrrl, rdl): 0.0, 1.21556e-05

Epoch over!
epoch time: 15.038

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1072
rank avg (pred): 0.143 +- 0.113
mrr vals (pred, true): 0.219, 0.307
batch losses (mrrl, rdl): 0.0, 1.28193e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1066
rank avg (pred): 0.155 +- 0.125
mrr vals (pred, true): 0.206, 0.259
batch losses (mrrl, rdl): 0.0, 2.76794e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1045
rank avg (pred): 0.450 +- 0.258
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 0.0, 1.139e-05

Epoch over!
epoch time: 15.016

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 599
rank avg (pred): 0.434 +- 0.242
mrr vals (pred, true): 0.061, 0.039
batch losses (mrrl, rdl): 0.0, 1.14722e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 725
rank avg (pred): 0.399 +- 0.255
mrr vals (pred, true): 0.077, 0.045
batch losses (mrrl, rdl): 0.0, 1.82219e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 803
rank avg (pred): 0.457 +- 0.259
mrr vals (pred, true): 0.043, 0.043
batch losses (mrrl, rdl): 0.0, 5.8142e-06

Epoch over!
epoch time: 15.034

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 567
rank avg (pred): 0.409 +- 0.245
mrr vals (pred, true): 0.068, 0.037
batch losses (mrrl, rdl): 0.0032447234, 1.92791e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 152
rank avg (pred): 0.437 +- 0.176
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 4.71125e-05, 2.73707e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 535
rank avg (pred): 0.503 +- 0.210
mrr vals (pred, true): 0.048, 0.024
batch losses (mrrl, rdl): 3.76219e-05, 4.6e-06

Epoch over!
epoch time: 15.2

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 587
rank avg (pred): 0.420 +- 0.173
mrr vals (pred, true): 0.058, 0.039
batch losses (mrrl, rdl): 0.0006045742, 4.0097e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 623
rank avg (pred): 0.477 +- 0.139
mrr vals (pred, true): 0.034, 0.047
batch losses (mrrl, rdl): 0.0026075637, 7.32807e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 636
rank avg (pred): 0.445 +- 0.163
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 3.66238e-05, 3.34774e-05

Epoch over!
epoch time: 15.087

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 523
rank avg (pred): 0.503 +- 0.200
mrr vals (pred, true): 0.054, 0.022
batch losses (mrrl, rdl): 0.0001674639, 3.9739e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 745
rank avg (pred): 0.138 +- 0.117
mrr vals (pred, true): 0.297, 0.249
batch losses (mrrl, rdl): 0.02253424, 1.95938e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 417
rank avg (pred): 0.452 +- 0.144
mrr vals (pred, true): 0.042, 0.064
batch losses (mrrl, rdl): 0.0005881479, 5.48745e-05

Epoch over!
epoch time: 15.232

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 263
rank avg (pred): 0.242 +- 0.173
mrr vals (pred, true): 0.211, 0.217
batch losses (mrrl, rdl): 0.0003418296, 0.0001556089

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 503
rank avg (pred): 0.492 +- 0.166
mrr vals (pred, true): 0.044, 0.026
batch losses (mrrl, rdl): 0.0003557423, 9.271e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 0
rank avg (pred): 0.360 +- 0.212
mrr vals (pred, true): 0.158, 0.244
batch losses (mrrl, rdl): 0.0738223493, 0.0008254445

Epoch over!
epoch time: 15.205

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 137
rank avg (pred): 0.446 +- 0.154
mrr vals (pred, true): 0.052, 0.048
batch losses (mrrl, rdl): 4.41145e-05, 5.42919e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 576
rank avg (pred): 0.444 +- 0.151
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001504204, 3.7993e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 641
rank avg (pred): 0.437 +- 0.152
mrr vals (pred, true): 0.057, 0.042
batch losses (mrrl, rdl): 0.0005515711, 2.6462e-05

Epoch over!
epoch time: 15.007

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1009
rank avg (pred): 0.448 +- 0.145
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 8.115e-06, 4.49677e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 190
rank avg (pred): 0.453 +- 0.144
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 6.823e-06, 3.82582e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1104
rank avg (pred): 0.442 +- 0.148
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002578634, 4.59058e-05

Epoch over!
epoch time: 15.002

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 312
rank avg (pred): 0.261 +- 0.177
mrr vals (pred, true): 0.215, 0.202
batch losses (mrrl, rdl): 0.0017666457, 0.0001512227

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 809
rank avg (pred): 0.441 +- 0.129
mrr vals (pred, true): 0.046, 0.051
batch losses (mrrl, rdl): 0.0001453785, 5.11798e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 483
rank avg (pred): 0.450 +- 0.130
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002784056, 5.31366e-05

Epoch over!
epoch time: 15.014

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 31
rank avg (pred): 0.289 +- 0.204
mrr vals (pred, true): 0.229, 0.242
batch losses (mrrl, rdl): 0.0017120729, 0.0004848469

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1057
rank avg (pred): 0.231 +- 0.173
mrr vals (pred, true): 0.273, 0.265
batch losses (mrrl, rdl): 0.0006392542, 9.49857e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 134
rank avg (pred): 0.451 +- 0.131
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 0.0001168447, 5.63256e-05

Epoch over!
epoch time: 15.025

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 561
rank avg (pred): 0.535 +- 0.168
mrr vals (pred, true): 0.043, 0.024
batch losses (mrrl, rdl): 0.0004577128, 5.37897e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 236
rank avg (pred): 0.445 +- 0.130
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 1.6209e-05, 5.66565e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1202
rank avg (pred): 0.442 +- 0.141
mrr vals (pred, true): 0.055, 0.048
batch losses (mrrl, rdl): 0.0002201853, 5.54728e-05

Epoch over!
epoch time: 15.04

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1030
rank avg (pred): 0.442 +- 0.139
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 7.20466e-05, 4.58938e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 168
rank avg (pred): 0.440 +- 0.135
mrr vals (pred, true): 0.055, 0.050
batch losses (mrrl, rdl): 0.0002214355, 4.41132e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 923
rank avg (pred): 0.463 +- 0.133
mrr vals (pred, true): 0.046, 0.037
batch losses (mrrl, rdl): 0.0001741756, 2.3065e-05

Epoch over!
epoch time: 15.215

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.447 +- 0.121
mrr vals (pred, true): 0.045, 0.046

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   46 	     0 	 0.04669 	 0.02004 	 ~...
    3 	     1 	 0.04340 	 0.02072 	 ~...
   10 	     2 	 0.04440 	 0.02292 	 ~...
   87 	     3 	 0.04963 	 0.02302 	 ~...
   40 	     4 	 0.04650 	 0.02424 	 ~...
   48 	     5 	 0.04680 	 0.02453 	 ~...
   30 	     6 	 0.04607 	 0.02586 	 ~...
   70 	     7 	 0.04794 	 0.03125 	 ~...
   65 	     8 	 0.04774 	 0.03139 	 ~...
   21 	     9 	 0.04518 	 0.03512 	 ~...
   39 	    10 	 0.04650 	 0.03663 	 ~...
   58 	    11 	 0.04714 	 0.03671 	 ~...
   50 	    12 	 0.04686 	 0.03701 	 ~...
    2 	    13 	 0.04297 	 0.03721 	 ~...
   27 	    14 	 0.04589 	 0.03798 	 ~...
   72 	    15 	 0.04806 	 0.03811 	 ~...
   83 	    16 	 0.04929 	 0.03815 	 ~...
   63 	    17 	 0.04758 	 0.03838 	 ~...
   64 	    18 	 0.04764 	 0.03923 	 ~...
   90 	    19 	 0.05016 	 0.03937 	 ~...
   52 	    20 	 0.04696 	 0.04004 	 ~...
   76 	    21 	 0.04843 	 0.04046 	 ~...
   61 	    22 	 0.04753 	 0.04080 	 ~...
   79 	    23 	 0.04868 	 0.04165 	 ~...
    4 	    24 	 0.04355 	 0.04176 	 ~...
    1 	    25 	 0.04287 	 0.04177 	 ~...
   35 	    26 	 0.04638 	 0.04209 	 ~...
   67 	    27 	 0.04781 	 0.04238 	 ~...
    0 	    28 	 0.04188 	 0.04255 	 ~...
   33 	    29 	 0.04638 	 0.04306 	 ~...
    8 	    30 	 0.04421 	 0.04308 	 ~...
   45 	    31 	 0.04669 	 0.04338 	 ~...
   38 	    32 	 0.04649 	 0.04340 	 ~...
   51 	    33 	 0.04689 	 0.04367 	 ~...
   15 	    34 	 0.04471 	 0.04369 	 ~...
   25 	    35 	 0.04574 	 0.04376 	 ~...
   12 	    36 	 0.04454 	 0.04381 	 ~...
    6 	    37 	 0.04363 	 0.04395 	 ~...
   81 	    38 	 0.04903 	 0.04397 	 ~...
   82 	    39 	 0.04919 	 0.04403 	 ~...
   26 	    40 	 0.04586 	 0.04413 	 ~...
   78 	    41 	 0.04854 	 0.04450 	 ~...
   28 	    42 	 0.04589 	 0.04471 	 ~...
   11 	    43 	 0.04447 	 0.04473 	 ~...
   14 	    44 	 0.04469 	 0.04484 	 ~...
   59 	    45 	 0.04718 	 0.04507 	 ~...
   86 	    46 	 0.04935 	 0.04515 	 ~...
   68 	    47 	 0.04786 	 0.04518 	 ~...
   55 	    48 	 0.04712 	 0.04543 	 ~...
   88 	    49 	 0.05000 	 0.04543 	 ~...
   41 	    50 	 0.04655 	 0.04551 	 ~...
   20 	    51 	 0.04515 	 0.04564 	 ~...
   53 	    52 	 0.04701 	 0.04574 	 ~...
   18 	    53 	 0.04508 	 0.04584 	 ~...
    7 	    54 	 0.04410 	 0.04608 	 ~...
   17 	    55 	 0.04496 	 0.04608 	 ~...
   36 	    56 	 0.04642 	 0.04610 	 ~...
    9 	    57 	 0.04435 	 0.04639 	 ~...
   43 	    58 	 0.04665 	 0.04642 	 ~...
   29 	    59 	 0.04603 	 0.04649 	 ~...
   23 	    60 	 0.04564 	 0.04705 	 ~...
   60 	    61 	 0.04724 	 0.04736 	 ~...
   69 	    62 	 0.04791 	 0.04793 	 ~...
   73 	    63 	 0.04807 	 0.04804 	 ~...
   56 	    64 	 0.04712 	 0.04807 	 ~...
   44 	    65 	 0.04666 	 0.04815 	 ~...
   32 	    66 	 0.04629 	 0.04819 	 ~...
   77 	    67 	 0.04852 	 0.04823 	 ~...
   84 	    68 	 0.04929 	 0.04827 	 ~...
   24 	    69 	 0.04572 	 0.04876 	 ~...
   22 	    70 	 0.04539 	 0.04890 	 ~...
   34 	    71 	 0.04638 	 0.04894 	 ~...
   66 	    72 	 0.04776 	 0.04894 	 ~...
   37 	    73 	 0.04648 	 0.04960 	 ~...
   31 	    74 	 0.04613 	 0.04974 	 ~...
   19 	    75 	 0.04510 	 0.05022 	 ~...
   47 	    76 	 0.04677 	 0.05084 	 ~...
   80 	    77 	 0.04898 	 0.05095 	 ~...
   71 	    78 	 0.04804 	 0.05135 	 ~...
   13 	    79 	 0.04461 	 0.05197 	 ~...
   16 	    80 	 0.04480 	 0.05213 	 ~...
   91 	    81 	 0.05019 	 0.05233 	 ~...
   62 	    82 	 0.04753 	 0.05299 	 ~...
   57 	    83 	 0.04714 	 0.05305 	 ~...
   42 	    84 	 0.04664 	 0.05354 	 ~...
   54 	    85 	 0.04704 	 0.05446 	 ~...
    5 	    86 	 0.04360 	 0.05450 	 ~...
   89 	    87 	 0.05014 	 0.05524 	 ~...
   49 	    88 	 0.04684 	 0.05634 	 ~...
   74 	    89 	 0.04811 	 0.05654 	 ~...
   75 	    90 	 0.04825 	 0.05887 	 ~...
   85 	    91 	 0.04933 	 0.05954 	 ~...
   92 	    92 	 0.19297 	 0.17062 	 ~...
   92 	    93 	 0.19297 	 0.17571 	 ~...
  103 	    94 	 0.22806 	 0.18292 	 m..s
   99 	    95 	 0.19466 	 0.20017 	 ~...
   92 	    96 	 0.19297 	 0.20414 	 ~...
  102 	    97 	 0.22398 	 0.20511 	 ~...
   92 	    98 	 0.19297 	 0.20559 	 ~...
  108 	    99 	 0.23421 	 0.21610 	 ~...
   92 	   100 	 0.19297 	 0.21862 	 ~...
   98 	   101 	 0.19406 	 0.21883 	 ~...
   97 	   102 	 0.19352 	 0.21956 	 ~...
  105 	   103 	 0.23043 	 0.23090 	 ~...
  111 	   104 	 0.24101 	 0.23847 	 ~...
  100 	   105 	 0.20773 	 0.23984 	 m..s
  104 	   106 	 0.22990 	 0.24051 	 ~...
  107 	   107 	 0.23302 	 0.24218 	 ~...
  110 	   108 	 0.23789 	 0.24233 	 ~...
  101 	   109 	 0.21414 	 0.24879 	 m..s
  114 	   110 	 0.24795 	 0.27157 	 ~...
  109 	   111 	 0.23643 	 0.27224 	 m..s
  113 	   112 	 0.24644 	 0.27339 	 ~...
  115 	   113 	 0.25343 	 0.27497 	 ~...
  112 	   114 	 0.24165 	 0.28160 	 m..s
  116 	   115 	 0.25352 	 0.28472 	 m..s
  106 	   116 	 0.23262 	 0.28488 	 m..s
  117 	   117 	 0.26442 	 0.29106 	 ~...
  118 	   118 	 0.26553 	 0.30117 	 m..s
  119 	   119 	 0.30814 	 0.30309 	 ~...
  120 	   120 	 0.34366 	 0.52429 	 MISS
==========================================
r_mrr = 0.9798649549484253
r2_mrr = 0.9455906748771667
spearmanr_mrr@5 = 0.8625746369361877
spearmanr_mrr@10 = 0.8734683394432068
spearmanr_mrr@50 = 0.9749091863632202
spearmanr_mrr@100 = 0.984657883644104
spearmanr_mrr@All = 0.9849352240562439
==========================================
test time: 0.458
Done Testing dataset UMLS
total time taken: 232.80057644844055
training time taken: 226.5017809867859
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9799)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9456)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.8626)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.8735)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9749)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9847)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9849)}}, 'test_loss': {'ComplEx': {'UMLS': 0.5377917003497714}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 7647586144446678
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [319, 684, 458, 1028, 95, 1030, 1005, 22, 1040, 1147, 16, 308, 1107, 423, 126, 622, 554, 252, 232, 1045, 69, 711, 1070, 912, 142, 107, 973, 8, 607, 598, 330, 1162, 256, 1037, 457, 546, 400, 902, 1150, 1098, 564, 93, 415, 112, 395, 533, 119, 923, 933, 736, 505, 679, 692, 1009, 310, 41, 705, 236, 231, 167, 1092, 634, 191, 511, 47, 893, 435, 671, 766, 914, 71, 1058, 83, 1158, 78, 490, 515, 99, 619, 212, 396, 344, 296, 991, 335, 154, 321, 331, 111, 324, 816, 827, 203, 524, 224, 984, 73, 101, 237, 1197, 626, 757, 136, 1206, 25, 1117, 482, 275, 980, 307, 1011, 161, 820, 683, 102, 386, 559, 206, 1075, 710, 541]
valid_ids (0): []
train_ids (1094): [558, 333, 76, 149, 156, 105, 834, 693, 18, 433, 160, 447, 108, 585, 517, 936, 573, 1026, 556, 943, 650, 158, 449, 181, 843, 1074, 407, 441, 771, 70, 399, 787, 743, 15, 942, 814, 786, 1202, 502, 891, 632, 989, 1211, 828, 775, 680, 378, 38, 34, 141, 312, 687, 143, 548, 470, 450, 273, 1101, 1160, 1193, 175, 864, 261, 796, 493, 589, 593, 96, 553, 561, 724, 235, 1120, 139, 819, 313, 244, 993, 213, 185, 479, 188, 210, 1081, 487, 475, 536, 1064, 354, 516, 1049, 216, 389, 337, 653, 91, 674, 401, 217, 588, 369, 1130, 600, 468, 255, 1133, 483, 165, 956, 1114, 1212, 825, 714, 940, 60, 663, 254, 1099, 418, 532, 609, 972, 403, 350, 295, 186, 159, 978, 822, 1207, 1125, 57, 197, 253, 114, 329, 730, 380, 950, 865, 596, 1127, 886, 790, 769, 464, 897, 514, 473, 1199, 494, 570, 1189, 788, 123, 264, 540, 268, 484, 701, 1059, 961, 1149, 1001, 1029, 753, 385, 336, 677, 397, 987, 339, 1181, 463, 910, 768, 669, 260, 737, 641, 767, 1104, 826, 416, 909, 813, 522, 859, 187, 1177, 945, 409, 408, 1185, 249, 1140, 151, 537, 219, 200, 1085, 388, 660, 127, 162, 193, 86, 633, 98, 1078, 997, 215, 202, 1032, 963, 722, 1024, 690, 341, 81, 300, 667, 462, 839, 207, 610, 440, 670, 853, 452, 53, 841, 976, 420, 673, 563, 751, 1126, 240, 872, 284, 625, 120, 469, 225, 259, 1128, 686, 172, 949, 64, 643, 856, 205, 754, 325, 14, 1010, 930, 873, 177, 1000, 527, 958, 583, 580, 779, 1144, 921, 1068, 1113, 1034, 12, 960, 1191, 829, 17, 509, 990, 497, 1111, 1093, 662, 520, 474, 620, 798, 461, 1018, 1088, 824, 1151, 343, 994, 602, 1129, 655, 118, 306, 547, 274, 176, 1155, 729, 1036, 979, 866, 691, 831, 1091, 752, 265, 241, 145, 568, 749, 735, 276, 289, 40, 1015, 529, 1062, 196, 1124, 1006, 982, 731, 353, 180, 364, 964, 326, 62, 672, 640, 292, 1135, 271, 879, 748, 209, 135, 682, 318, 806, 1143, 944, 624, 234, 122, 981, 565, 460, 444, 1119, 830, 821, 1132, 305, 900, 1056, 179, 361, 1080, 850, 1198, 572, 780, 718, 642, 807, 290, 1072, 345, 1071, 1063, 1201, 616, 1131, 746, 1066, 72, 323, 1173, 639, 1067, 715, 957, 405, 346, 815, 838, 507, 519, 1213, 594, 974, 478, 192, 293, 489, 613, 500, 46, 608, 906, 774, 28, 849, 1031, 442, 518, 379, 723, 499, 611, 184, 538, 1115, 720, 257, 1214, 1174, 525, 229, 294, 419, 1209, 629, 1204, 471, 654, 157, 999, 392, 592, 628, 742, 542, 847, 621, 694, 531, 0, 685, 155, 125, 869, 835, 799, 501, 1152, 7, 727, 576, 656, 51, 472, 975, 131, 837, 85, 201, 148, 510, 63, 851, 584, 248, 368, 1178, 917, 696, 373, 436, 246, 54, 706, 812, 486, 1087, 476, 760, 1208, 699, 808, 223, 377, 934, 1044, 163, 817, 803, 492, 281, 645, 1042, 48, 84, 567, 56, 1021, 169, 466, 668, 21, 648, 44, 1002, 427, 777, 347, 429, 1008, 394, 94, 174, 852, 1137, 283, 506, 431, 681, 1182, 55, 311, 1105, 907, 384, 1176, 2, 858, 1016, 50, 1136, 1186, 659, 716, 285, 922, 1096, 560, 366, 762, 646, 320, 863, 27, 908, 352, 291, 1188, 6, 954, 376, 132, 586, 267, 1153, 398, 459, 413, 1073, 709, 758, 1027, 183, 481, 355, 279, 10, 59, 810, 818, 414, 846, 359, 243, 637, 387, 890, 801, 348, 1138, 417, 1053, 445, 892, 1203, 959, 92, 889, 230, 712, 666, 759, 1134, 534, 1014, 1159, 363, 434, 1055, 342, 913, 896, 977, 166, 147, 1090, 103, 390, 623, 566, 877, 1079, 1004, 925, 178, 371, 194, 82, 948, 675, 597, 985, 535, 1164, 581, 951, 1175, 222, 1025, 1118, 928, 1020, 996, 1154, 446, 1, 65, 823, 190, 133, 947, 938, 443, 233, 832, 875, 734, 728, 198, 406, 881, 916, 932, 1168, 258, 905, 657, 772, 1065, 303, 49, 739, 130, 732, 848, 713, 1035, 338, 704, 761, 884, 37, 995, 842, 860, 871, 513, 455, 612, 770, 1196, 432, 1041, 695, 883, 1165, 356, 404, 496, 1192, 1112, 1157, 1169, 569, 1180, 1033, 631, 488, 1046, 375, 526, 778, 150, 39, 508, 880, 530, 1103, 327, 349, 117, 854, 789, 170, 903, 270, 870, 523, 763, 658, 430, 491, 317, 575, 204, 664, 740, 1003, 700, 134, 1048, 747, 811, 1060, 374, 726, 451, 595, 89, 1051, 1210, 689, 391, 577, 765, 968, 795, 1183, 1187, 967, 110, 334, 247, 652, 782, 887, 926, 599, 1146, 571, 986, 485, 314, 635, 227, 426, 358, 717, 1116, 868, 935, 512, 545, 124, 802, 164, 195, 745, 833, 1122, 1095, 189, 1200, 80, 638, 800, 1023, 1022, 61, 617, 661, 467, 792, 214, 1145, 878, 58, 360, 1148, 939, 857, 402, 1156, 263, 1057, 885, 220, 773, 9, 970, 721, 272, 836, 702, 946, 480, 1076, 182, 603, 410, 888, 703, 750, 199, 1061, 895, 797, 1190, 100, 168, 297, 738, 42, 121, 931, 755, 1007, 1094, 579, 1121, 362, 67, 1054, 678, 1179, 302, 971, 4, 1194, 282, 146, 676, 30, 152, 19, 109, 615, 1038, 1017, 315, 1109, 744, 924, 1097, 601, 74, 874, 919, 250, 340, 647, 1106, 299, 733, 955, 791, 1139, 113, 920, 1163, 862, 381, 707, 756, 1195, 77, 636, 1013, 221, 899, 741, 228, 277, 90, 578, 266, 173, 614, 36, 644, 465, 75, 962, 861, 918, 1052, 1161, 421, 698, 88, 901, 504, 697, 137, 911, 555, 298, 651, 262, 1166, 218, 544, 528, 665, 370, 411, 590, 367, 280, 840, 992, 1110, 618, 129, 211, 915, 688, 781, 1123, 288, 33, 138, 498, 153, 562, 587, 453, 725, 550, 238, 785, 966, 793, 591, 425, 630, 383, 13, 68, 844, 927, 5, 23, 549, 97, 26, 539, 316, 965, 328, 32, 867, 1089, 357, 301, 438, 29, 454, 552, 304, 941, 1043, 87, 3, 24, 439, 437, 627, 422, 606, 1083, 983, 31, 845, 1184, 226, 1141, 1019, 882, 937, 809, 1172, 495, 128, 898, 708, 322, 1084, 557, 1039, 428, 245, 1167, 988, 1012, 242, 365, 805, 605, 1171, 719, 20, 351, 424, 43, 604, 521, 309, 794, 239, 649, 1050, 106, 764, 894, 783, 45, 116, 52, 477, 551, 412, 35, 11, 208, 144, 503, 1077, 574, 287, 1086, 1069, 140, 448, 998, 904, 855, 1047, 1108, 79, 776, 876, 1205, 969, 382, 1100, 66, 332, 1142, 582, 1082, 1170, 953, 804, 115, 543, 104, 952, 286, 171, 251, 929, 1102, 278, 456, 269, 372, 393, 784]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3661426538552693
the save name prefix for this run is:  chkpt-ID_3661426538552693_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 18
rank avg (pred): 0.549 +- 0.004
mrr vals (pred, true): 0.013, 0.184
batch losses (mrrl, rdl): 0.0, 0.002659427

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 604
rank avg (pred): 0.445 +- 0.214
mrr vals (pred, true): 0.117, 0.047
batch losses (mrrl, rdl): 0.0, 1.16671e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 737
rank avg (pred): 0.126 +- 0.076
mrr vals (pred, true): 0.237, 0.293
batch losses (mrrl, rdl): 0.0, 3.5121e-05

Epoch over!
epoch time: 14.865

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1210
rank avg (pred): 0.435 +- 0.241
mrr vals (pred, true): 0.155, 0.053
batch losses (mrrl, rdl): 0.0, 1.3678e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 226
rank avg (pred): 0.418 +- 0.255
mrr vals (pred, true): 0.175, 0.051
batch losses (mrrl, rdl): 0.0, 7.1997e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 847
rank avg (pred): 0.444 +- 0.254
mrr vals (pred, true): 0.127, 0.046
batch losses (mrrl, rdl): 0.0, 7.0895e-06

Epoch over!
epoch time: 14.837

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 627
rank avg (pred): 0.425 +- 0.236
mrr vals (pred, true): 0.130, 0.038
batch losses (mrrl, rdl): 0.0, 1.42328e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 289
rank avg (pred): 0.188 +- 0.118
mrr vals (pred, true): 0.148, 0.190
batch losses (mrrl, rdl): 0.0, 4.00427e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 815
rank avg (pred): 0.117 +- 0.077
mrr vals (pred, true): 0.183, 0.539
batch losses (mrrl, rdl): 0.0, 9.19806e-05

Epoch over!
epoch time: 14.813

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 785
rank avg (pred): 0.406 +- 0.276
mrr vals (pred, true): 0.165, 0.052
batch losses (mrrl, rdl): 0.0, 5.2576e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 488
rank avg (pred): 0.484 +- 0.233
mrr vals (pred, true): 0.064, 0.026
batch losses (mrrl, rdl): 0.0, 8.8317e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 596
rank avg (pred): 0.430 +- 0.239
mrr vals (pred, true): 0.068, 0.041
batch losses (mrrl, rdl): 0.0, 3.6322e-06

Epoch over!
epoch time: 14.818

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 889
rank avg (pred): 0.417 +- 0.270
mrr vals (pred, true): 0.101, 0.047
batch losses (mrrl, rdl): 0.0, 4.6335e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1080
rank avg (pred): 0.422 +- 0.264
mrr vals (pred, true): 0.079, 0.042
batch losses (mrrl, rdl): 0.0, 2.5513e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1213
rank avg (pred): 0.440 +- 0.247
mrr vals (pred, true): 0.058, 0.051
batch losses (mrrl, rdl): 0.0, 8.1862e-06

Epoch over!
epoch time: 14.815

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 531
rank avg (pred): 0.483 +- 0.235
mrr vals (pred, true): 0.050, 0.026
batch losses (mrrl, rdl): 5.3e-08, 9.843e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 403
rank avg (pred): 0.496 +- 0.230
mrr vals (pred, true): 0.057, 0.050
batch losses (mrrl, rdl): 0.0005393095, 8.33052e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 81
rank avg (pred): 0.512 +- 0.218
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 3.75956e-05, 0.0001875033

Epoch over!
epoch time: 15.074

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 756
rank avg (pred): 0.402 +- 0.198
mrr vals (pred, true): 0.068, 0.050
batch losses (mrrl, rdl): 0.0032523805, 2.41578e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 263
rank avg (pred): 0.107 +- 0.068
mrr vals (pred, true): 0.225, 0.217
batch losses (mrrl, rdl): 0.0006550653, 9.7964e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1209
rank avg (pred): 0.536 +- 0.199
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.0011753894, 0.0002480157

Epoch over!
epoch time: 15.047

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 811
rank avg (pred): 0.031 +- 0.020
mrr vals (pred, true): 0.353, 0.401
batch losses (mrrl, rdl): 0.023358617, 0.0001182226

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1026
rank avg (pred): 0.485 +- 0.221
mrr vals (pred, true): 0.056, 0.048
batch losses (mrrl, rdl): 0.000308368, 7.51664e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 91
rank avg (pred): 0.498 +- 0.225
mrr vals (pred, true): 0.059, 0.049
batch losses (mrrl, rdl): 0.000737498, 0.0001323529

Epoch over!
epoch time: 15.032

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 286
rank avg (pred): 0.132 +- 0.087
mrr vals (pred, true): 0.239, 0.213
batch losses (mrrl, rdl): 0.006800862, 6.33279e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 169
rank avg (pred): 0.481 +- 0.229
mrr vals (pred, true): 0.066, 0.049
batch losses (mrrl, rdl): 0.0026212586, 0.0001083096

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 578
rank avg (pred): 0.497 +- 0.214
mrr vals (pred, true): 0.050, 0.035
batch losses (mrrl, rdl): 1.66e-08, 3.53384e-05

Epoch over!
epoch time: 15.018

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1208
rank avg (pred): 0.494 +- 0.218
mrr vals (pred, true): 0.054, 0.049
batch losses (mrrl, rdl): 0.0001611821, 9.56347e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 82
rank avg (pred): 0.507 +- 0.201
mrr vals (pred, true): 0.042, 0.040
batch losses (mrrl, rdl): 0.0006234468, 0.0001057267

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 390
rank avg (pred): 0.490 +- 0.213
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 6.4016e-06, 7.02838e-05

Epoch over!
epoch time: 15.034

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1194
rank avg (pred): 0.503 +- 0.201
mrr vals (pred, true): 0.042, 0.047
batch losses (mrrl, rdl): 0.0006870274, 0.0001368118

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 403
rank avg (pred): 0.489 +- 0.214
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 2.32298e-05, 7.10857e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 176
rank avg (pred): 0.483 +- 0.211
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 1.27312e-05, 4.94105e-05

Epoch over!
epoch time: 15.049

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 72
rank avg (pred): 0.304 +- 0.206
mrr vals (pred, true): 0.226, 0.189
batch losses (mrrl, rdl): 0.014114948, 0.0003093988

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 669
rank avg (pred): 0.480 +- 0.211
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001456804, 7.66252e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 248
rank avg (pred): 0.129 +- 0.086
mrr vals (pred, true): 0.256, 0.274
batch losses (mrrl, rdl): 0.0032418265, 1.09034e-05

Epoch over!
epoch time: 15.028

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 367
rank avg (pred): 0.512 +- 0.177
mrr vals (pred, true): 0.031, 0.054
batch losses (mrrl, rdl): 0.0034241863, 0.0001251536

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 377
rank avg (pred): 0.478 +- 0.205
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 2.24576e-05, 6.45774e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 587
rank avg (pred): 0.486 +- 0.178
mrr vals (pred, true): 0.035, 0.039
batch losses (mrrl, rdl): 0.0022919932, 4.11297e-05

Epoch over!
epoch time: 14.989

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 627
rank avg (pred): 0.475 +- 0.204
mrr vals (pred, true): 0.050, 0.038
batch losses (mrrl, rdl): 5.043e-07, 2.0545e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 664
rank avg (pred): 0.467 +- 0.200
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 0.000107793, 5.50024e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 927
rank avg (pred): 0.468 +- 0.192
mrr vals (pred, true): 0.042, 0.045
batch losses (mrrl, rdl): 0.0005776213, 1.9669e-05

Epoch over!
epoch time: 14.986

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1006
rank avg (pred): 0.474 +- 0.207
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 3.9651e-06, 4.79206e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 745
rank avg (pred): 0.193 +- 0.130
mrr vals (pred, true): 0.253, 0.249
batch losses (mrrl, rdl): 0.0001265689, 3.83725e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 146
rank avg (pred): 0.447 +- 0.206
mrr vals (pred, true): 0.061, 0.052
batch losses (mrrl, rdl): 0.0012584855, 2.70955e-05

Epoch over!
epoch time: 14.985

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.411 +- 0.243
mrr vals (pred, true): 0.191, 0.188

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   89 	     0 	 0.05824 	 0.02004 	 m..s
   36 	     1 	 0.05464 	 0.02094 	 m..s
    9 	     2 	 0.05324 	 0.02172 	 m..s
   83 	     3 	 0.05522 	 0.02206 	 m..s
    8 	     4 	 0.05324 	 0.02234 	 m..s
    5 	     5 	 0.05291 	 0.02235 	 m..s
   12 	     6 	 0.05365 	 0.02328 	 m..s
    0 	     7 	 0.05127 	 0.02370 	 ~...
   10 	     8 	 0.05344 	 0.02428 	 ~...
   37 	     9 	 0.05464 	 0.02497 	 ~...
   13 	    10 	 0.05392 	 0.02520 	 ~...
    2 	    11 	 0.05158 	 0.02579 	 ~...
   35 	    12 	 0.05460 	 0.02582 	 ~...
    7 	    13 	 0.05312 	 0.02586 	 ~...
    3 	    14 	 0.05213 	 0.02626 	 ~...
    1 	    15 	 0.05132 	 0.02721 	 ~...
   86 	    16 	 0.05655 	 0.02782 	 ~...
   11 	    17 	 0.05362 	 0.02820 	 ~...
    6 	    18 	 0.05299 	 0.02951 	 ~...
   87 	    19 	 0.05750 	 0.03544 	 ~...
   42 	    20 	 0.05483 	 0.03608 	 ~...
   16 	    21 	 0.05412 	 0.03673 	 ~...
   42 	    22 	 0.05483 	 0.03732 	 ~...
   27 	    23 	 0.05442 	 0.03889 	 ~...
   26 	    24 	 0.05441 	 0.03950 	 ~...
   42 	    25 	 0.05483 	 0.03971 	 ~...
   42 	    26 	 0.05483 	 0.04032 	 ~...
   42 	    27 	 0.05483 	 0.04070 	 ~...
   42 	    28 	 0.05483 	 0.04080 	 ~...
   42 	    29 	 0.05483 	 0.04101 	 ~...
    4 	    30 	 0.05264 	 0.04163 	 ~...
   42 	    31 	 0.05483 	 0.04174 	 ~...
   42 	    32 	 0.05483 	 0.04221 	 ~...
   88 	    33 	 0.05789 	 0.04224 	 ~...
   42 	    34 	 0.05483 	 0.04241 	 ~...
   33 	    35 	 0.05451 	 0.04255 	 ~...
   42 	    36 	 0.05483 	 0.04276 	 ~...
   42 	    37 	 0.05483 	 0.04292 	 ~...
   20 	    38 	 0.05418 	 0.04302 	 ~...
   42 	    39 	 0.05483 	 0.04308 	 ~...
   38 	    40 	 0.05476 	 0.04362 	 ~...
   19 	    41 	 0.05414 	 0.04367 	 ~...
   42 	    42 	 0.05483 	 0.04393 	 ~...
   40 	    43 	 0.05480 	 0.04404 	 ~...
   23 	    44 	 0.05432 	 0.04460 	 ~...
   42 	    45 	 0.05483 	 0.04460 	 ~...
   42 	    46 	 0.05483 	 0.04524 	 ~...
   42 	    47 	 0.05483 	 0.04543 	 ~...
   42 	    48 	 0.05483 	 0.04559 	 ~...
   42 	    49 	 0.05483 	 0.04565 	 ~...
   42 	    50 	 0.05483 	 0.04574 	 ~...
   42 	    51 	 0.05483 	 0.04584 	 ~...
   90 	    52 	 0.05901 	 0.04592 	 ~...
   42 	    53 	 0.05483 	 0.04639 	 ~...
   18 	    54 	 0.05413 	 0.04640 	 ~...
   29 	    55 	 0.05442 	 0.04641 	 ~...
   31 	    56 	 0.05446 	 0.04680 	 ~...
   42 	    57 	 0.05483 	 0.04692 	 ~...
   42 	    58 	 0.05483 	 0.04701 	 ~...
   42 	    59 	 0.05483 	 0.04702 	 ~...
   42 	    60 	 0.05483 	 0.04705 	 ~...
   84 	    61 	 0.05540 	 0.04715 	 ~...
   30 	    62 	 0.05445 	 0.04727 	 ~...
   42 	    63 	 0.05483 	 0.04744 	 ~...
   42 	    64 	 0.05483 	 0.04764 	 ~...
   21 	    65 	 0.05422 	 0.04820 	 ~...
   27 	    66 	 0.05442 	 0.04851 	 ~...
   42 	    67 	 0.05483 	 0.04873 	 ~...
   42 	    68 	 0.05483 	 0.04883 	 ~...
   14 	    69 	 0.05393 	 0.04901 	 ~...
   42 	    70 	 0.05483 	 0.04922 	 ~...
   42 	    71 	 0.05483 	 0.04928 	 ~...
   22 	    72 	 0.05424 	 0.04964 	 ~...
   42 	    73 	 0.05483 	 0.04989 	 ~...
   42 	    74 	 0.05483 	 0.04991 	 ~...
   42 	    75 	 0.05483 	 0.05016 	 ~...
   17 	    76 	 0.05413 	 0.05041 	 ~...
   32 	    77 	 0.05450 	 0.05053 	 ~...
   41 	    78 	 0.05482 	 0.05087 	 ~...
   25 	    79 	 0.05439 	 0.05129 	 ~...
   85 	    80 	 0.05587 	 0.05144 	 ~...
   42 	    81 	 0.05483 	 0.05151 	 ~...
   34 	    82 	 0.05456 	 0.05158 	 ~...
   39 	    83 	 0.05478 	 0.05160 	 ~...
   15 	    84 	 0.05394 	 0.05162 	 ~...
   24 	    85 	 0.05434 	 0.05233 	 ~...
   42 	    86 	 0.05483 	 0.05235 	 ~...
   42 	    87 	 0.05483 	 0.05275 	 ~...
   42 	    88 	 0.05483 	 0.05299 	 ~...
   42 	    89 	 0.05483 	 0.05385 	 ~...
   42 	    90 	 0.05483 	 0.05482 	 ~...
   91 	    91 	 0.18063 	 0.17571 	 ~...
  104 	    92 	 0.23919 	 0.18352 	 m..s
   93 	    93 	 0.19148 	 0.18803 	 ~...
   99 	    94 	 0.22272 	 0.18889 	 m..s
  108 	    95 	 0.24447 	 0.19336 	 m..s
  109 	    96 	 0.24493 	 0.19545 	 m..s
   97 	    97 	 0.21038 	 0.19872 	 ~...
   92 	    98 	 0.18499 	 0.20499 	 ~...
  100 	    99 	 0.22943 	 0.20936 	 ~...
   95 	   100 	 0.20493 	 0.21474 	 ~...
   96 	   101 	 0.21012 	 0.21482 	 ~...
   94 	   102 	 0.19575 	 0.21637 	 ~...
  113 	   103 	 0.29171 	 0.21777 	 m..s
  102 	   104 	 0.23497 	 0.21875 	 ~...
  101 	   105 	 0.23355 	 0.22050 	 ~...
  103 	   106 	 0.23844 	 0.22399 	 ~...
  106 	   107 	 0.24047 	 0.22787 	 ~...
  111 	   108 	 0.26983 	 0.24222 	 ~...
  105 	   109 	 0.23975 	 0.24360 	 ~...
  110 	   110 	 0.24821 	 0.24515 	 ~...
  117 	   111 	 0.30856 	 0.25215 	 m..s
  107 	   112 	 0.24059 	 0.26409 	 ~...
  112 	   113 	 0.29165 	 0.27290 	 ~...
   98 	   114 	 0.21983 	 0.28353 	 m..s
  114 	   115 	 0.30224 	 0.28449 	 ~...
  120 	   116 	 0.33471 	 0.28691 	 m..s
  118 	   117 	 0.30975 	 0.29106 	 ~...
  116 	   118 	 0.30785 	 0.29247 	 ~...
  115 	   119 	 0.30636 	 0.29583 	 ~...
  119 	   120 	 0.31742 	 0.33429 	 ~...
==========================================
r_mrr = 0.9824934601783752
r2_mrr = 0.9418627023696899
spearmanr_mrr@5 = 0.9762521386146545
spearmanr_mrr@10 = 0.9638379812240601
spearmanr_mrr@50 = 0.9985224604606628
spearmanr_mrr@100 = 0.9985067844390869
spearmanr_mrr@All = 0.9952590465545654
==========================================
test time: 0.452
Done Testing dataset UMLS
total time taken: 231.2056622505188
training time taken: 224.8944787979126
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9825)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9419)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9763)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9638)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9985)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9985)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9953)}}, 'test_loss': {'ComplEx': {'UMLS': 0.3298736532124167}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 7561796508551420
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1107, 97, 110, 296, 593, 1061, 884, 584, 723, 814, 275, 1158, 585, 236, 873, 794, 316, 120, 257, 524, 454, 644, 994, 251, 209, 439, 143, 552, 34, 786, 102, 195, 437, 1059, 659, 648, 535, 916, 743, 860, 1041, 456, 39, 344, 410, 136, 833, 772, 630, 656, 368, 1081, 742, 1170, 734, 286, 1183, 527, 592, 731, 699, 858, 2, 272, 243, 608, 427, 1130, 1091, 478, 46, 800, 510, 1123, 1079, 1117, 558, 639, 303, 930, 1040, 329, 1032, 354, 747, 531, 963, 632, 1180, 202, 388, 25, 101, 754, 453, 1151, 849, 670, 782, 911, 855, 685, 827, 781, 149, 152, 1199, 1136, 341, 784, 904, 1034, 161, 79, 571, 857, 253, 588, 1038, 809, 520]
valid_ids (0): []
train_ids (1094): [231, 1200, 1194, 627, 53, 1067, 996, 999, 466, 561, 1046, 540, 701, 666, 763, 1145, 788, 828, 966, 18, 324, 162, 591, 738, 505, 856, 297, 562, 1208, 1143, 450, 726, 1088, 951, 898, 1075, 408, 929, 706, 509, 1150, 720, 1212, 59, 93, 949, 1149, 314, 151, 711, 843, 635, 522, 62, 595, 1074, 971, 247, 1184, 918, 596, 917, 96, 266, 113, 302, 704, 586, 471, 768, 629, 312, 1166, 616, 125, 186, 981, 1021, 625, 852, 1213, 157, 845, 882, 155, 997, 219, 838, 894, 214, 75, 360, 896, 52, 1089, 1129, 146, 1182, 137, 823, 705, 812, 933, 108, 24, 836, 326, 1152, 555, 413, 1169, 693, 578, 69, 990, 100, 1196, 181, 669, 60, 696, 462, 441, 495, 196, 1192, 1163, 1049, 163, 173, 807, 61, 425, 765, 1175, 1050, 539, 551, 513, 1177, 311, 459, 573, 802, 475, 816, 435, 1179, 570, 1039, 279, 291, 1173, 641, 1132, 132, 660, 987, 872, 259, 67, 716, 910, 364, 198, 1037, 847, 725, 708, 806, 640, 939, 899, 366, 68, 1124, 1156, 1148, 977, 897, 690, 657, 901, 915, 867, 376, 416, 1206, 1154, 107, 988, 863, 776, 564, 150, 839, 301, 942, 862, 662, 804, 688, 727, 99, 1095, 835, 650, 773, 17, 1030, 321, 790, 523, 735, 905, 874, 759, 677, 1099, 164, 109, 883, 1178, 1109, 634, 741, 504, 131, 1197, 605, 117, 227, 822, 45, 534, 1168, 610, 574, 941, 499, 791, 945, 438, 1140, 292, 651, 166, 94, 1137, 671, 760, 82, 33, 517, 755, 955, 665, 547, 653, 41, 545, 140, 1120, 853, 43, 153, 1119, 1209, 861, 1204, 496, 598, 1090, 433, 159, 418, 697, 736, 1103, 992, 587, 372, 572, 374, 141, 1157, 749, 216, 753, 282, 1104, 834, 909, 514, 11, 642, 167, 124, 448, 906, 568, 430, 1098, 380, 255, 234, 1052, 832, 947, 950, 174, 232, 133, 698, 398, 1096, 1, 1214, 28, 948, 479, 85, 356, 484, 193, 178, 925, 1073, 976, 1051, 449, 550, 1024, 361, 819, 1131, 938, 780, 746, 879, 77, 944, 490, 404, 799, 128, 254, 1188, 820, 1016, 530, 500, 721, 207, 502, 581, 943, 261, 549, 240, 226, 924, 1181, 728, 464, 1133, 875, 419, 371, 895, 565, 707, 1004, 348, 222, 919, 74, 378, 968, 402, 560, 1057, 31, 414, 1118, 1029, 813, 777, 104, 215, 365, 307, 870, 970, 633, 91, 421, 1086, 702, 262, 145, 623, 1036, 638, 937, 691, 353, 446, 637, 779, 362, 258, 556, 491, 205, 335, 188, 423, 912, 27, 1055, 118, 399, 373, 333, 935, 846, 512, 160, 1189, 190, 271, 907, 972, 305, 946, 165, 339, 1071, 864, 48, 1082, 264, 563, 506, 1110, 516, 1033, 44, 934, 732, 63, 485, 3, 470, 293, 194, 1072, 220, 81, 959, 793, 184, 310, 488, 603, 826, 199, 375, 1043, 306, 64, 844, 168, 675, 260, 1122, 775, 521, 962, 319, 1153, 111, 4, 767, 544, 428, 1164, 796, 1015, 1159, 76, 526, 476, 903, 626, 49, 758, 112, 1017, 985, 619, 224, 83, 290, 1193, 406, 511, 142, 468, 575, 1187, 877, 805, 245, 367, 686, 400, 931, 238, 14, 761, 1108, 472, 869, 147, 922, 559, 242, 1093, 762, 515, 1048, 330, 729, 778, 1080, 553, 55, 350, 1142, 612, 1106, 72, 957, 722, 998, 129, 211, 463, 602, 267, 12, 315, 1026, 225, 116, 473, 983, 1077, 984, 927, 868, 617, 606, 288, 345, 713, 582, 287, 609, 771, 590, 751, 105, 825, 172, 1141, 733, 975, 280, 489, 993, 1031, 213, 655, 1065, 649, 1000, 498, 718, 923, 15, 98, 1085, 447, 1101, 37, 217, 1205, 210, 1054, 338, 1028, 458, 477, 170, 1147, 405, 1068, 1014, 230, 19, 90, 383, 1211, 385, 953, 246, 276, 1138, 229, 566, 886, 431, 770, 542, 304, 795, 32, 218, 821, 134, 1027, 600, 180, 1172, 355, 42, 1198, 115, 589, 235, 268, 393, 135, 1139, 370, 5, 407, 652, 681, 474, 601, 661, 541, 349, 359, 871, 465, 831, 611, 667, 532, 1105, 1210, 452, 241, 119, 792, 840, 557, 1176, 533, 538, 127, 810, 284, 1007, 442, 1003, 389, 979, 123, 1047, 548, 1005, 390, 1146, 392, 1025, 961, 57, 859, 1083, 13, 274, 646, 1011, 183, 1066, 336, 739, 817, 177, 579, 1135, 850, 1060, 269, 1171, 1207, 709, 342, 801, 295, 798, 467, 546, 1202, 973, 613, 967, 952, 543, 1069, 672, 1092, 185, 594, 501, 386, 913, 429, 694, 926, 851, 175, 347, 518, 684, 604, 830, 583, 841, 249, 1195, 692, 273, 325, 346, 285, 340, 171, 618, 1162, 233, 281, 1010, 432, 426, 461, 678, 1012, 964, 144, 622, 921, 434, 503, 189, 1191, 1165, 958, 674, 803, 854, 481, 680, 51, 480, 409, 451, 878, 318, 1203, 577, 599, 621, 757, 748, 536, 58, 201, 80, 0, 1062, 769, 1013, 332, 84, 250, 569, 337, 8, 223, 837, 645, 244, 936, 842, 537, 270, 334, 92, 1134, 1186, 299, 774, 787, 554, 424, 352, 865, 744, 208, 928, 308, 986, 1190, 156, 797, 766, 1115, 412, 715, 668, 712, 212, 1161, 908, 703, 197, 507, 436, 679, 440, 457, 1018, 65, 1076, 1053, 20, 35, 737, 176, 1008, 891, 395, 1009, 126, 417, 1125, 890, 687, 1112, 700, 519, 382, 182, 88, 1185, 1114, 256, 415, 169, 1121, 228, 313, 1144, 673, 978, 394, 265, 508, 263, 443, 866, 239, 10, 607, 636, 676, 1084, 730, 1002, 138, 719, 1078, 756, 1035, 940, 486, 1174, 358, 283, 114, 885, 497, 237, 1111, 932, 1126, 38, 663, 397, 29, 887, 529, 750, 492, 1045, 615, 1023, 824, 331, 139, 1113, 95, 469, 717, 73, 848, 36, 298, 483, 1155, 78, 888, 1160, 40, 70, 158, 1116, 960, 252, 829, 710, 148, 1063, 289, 322, 154, 203, 16, 695, 323, 21, 122, 444, 278, 482, 320, 221, 351, 317, 54, 764, 789, 989, 752, 422, 363, 22, 328, 624, 1100, 880, 893, 26, 1042, 980, 689, 294, 920, 66, 580, 494, 56, 576, 493, 965, 991, 309, 1019, 1056, 420, 785, 300, 377, 455, 191, 384, 956, 683, 1167, 525, 187, 1102, 597, 47, 1006, 87, 647, 277, 881, 1001, 89, 815, 343, 387, 808, 396, 1128, 1087, 1094, 892, 179, 740, 1097, 248, 628, 200, 889, 682, 103, 643, 206, 614, 379, 654, 1201, 954, 369, 130, 357, 811, 658, 664, 528, 30, 620, 995, 106, 391, 487, 23, 631, 1058, 969, 876, 445, 7, 121, 403, 71, 900, 9, 714, 1127, 567, 745, 783, 460, 982, 914, 902, 1020, 401, 86, 1044, 204, 50, 327, 192, 724, 1022, 1070, 818, 381, 411, 974, 1064, 6]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9297594232673226
the save name prefix for this run is:  chkpt-ID_9297594232673226_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 919
rank avg (pred): 0.442 +- 0.003
mrr vals (pred, true): 0.017, 0.039
batch losses (mrrl, rdl): 0.0, 7.84916e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 825
rank avg (pred): 0.166 +- 0.129
mrr vals (pred, true): 0.125, 0.253
batch losses (mrrl, rdl): 0.0, 9.5109e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 254
rank avg (pred): 0.161 +- 0.180
mrr vals (pred, true): 0.133, 0.221
batch losses (mrrl, rdl): 0.0, 1.33216e-05

Epoch over!
epoch time: 14.845

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 245
rank avg (pred): 0.167 +- 0.195
mrr vals (pred, true): 0.138, 0.274
batch losses (mrrl, rdl): 0.0, 1.69837e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1018
rank avg (pred): 0.447 +- 0.256
mrr vals (pred, true): 0.031, 0.047
batch losses (mrrl, rdl): 0.0, 1.5842e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 422
rank avg (pred): 0.439 +- 0.264
mrr vals (pred, true): 0.043, 0.052
batch losses (mrrl, rdl): 0.0, 2.33e-07

Epoch over!
epoch time: 14.938

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 579
rank avg (pred): 0.437 +- 0.247
mrr vals (pred, true): 0.036, 0.041
batch losses (mrrl, rdl): 0.0, 1.74567e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1209
rank avg (pred): 0.434 +- 0.259
mrr vals (pred, true): 0.044, 0.047
batch losses (mrrl, rdl): 0.0, 1.018e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 313
rank avg (pred): 0.185 +- 0.207
mrr vals (pred, true): 0.124, 0.206
batch losses (mrrl, rdl): 0.0, 1.6433e-06

Epoch over!
epoch time: 15.064

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 465
rank avg (pred): 0.440 +- 0.263
mrr vals (pred, true): 0.039, 0.050
batch losses (mrrl, rdl): 0.0, 1.1923e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 886
rank avg (pred): 0.440 +- 0.281
mrr vals (pred, true): 0.060, 0.048
batch losses (mrrl, rdl): 0.0, 4.259e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 160
rank avg (pred): 0.436 +- 0.263
mrr vals (pred, true): 0.044, 0.053
batch losses (mrrl, rdl): 0.0, 3.499e-07

Epoch over!
epoch time: 15.065

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 103
rank avg (pred): 0.446 +- 0.257
mrr vals (pred, true): 0.036, 0.047
batch losses (mrrl, rdl): 0.0, 4.3437e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 910
rank avg (pred): 0.547 +- 0.243
mrr vals (pred, true): 0.025, 0.020
batch losses (mrrl, rdl): 0.0, 2.74527e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1097
rank avg (pred): 0.425 +- 0.262
mrr vals (pred, true): 0.047, 0.040
batch losses (mrrl, rdl): 0.0, 1.97808e-05

Epoch over!
epoch time: 14.821

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 837
rank avg (pred): 0.435 +- 0.265
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.000231211, 4.8608e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 132
rank avg (pred): 0.447 +- 0.273
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 1.538e-07, 3.6823e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 21
rank avg (pred): 0.146 +- 0.233
mrr vals (pred, true): 0.198, 0.207
batch losses (mrrl, rdl): 0.0008163753, 8.44941e-05

Epoch over!
epoch time: 15.051

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 989
rank avg (pred): 0.110 +- 0.227
mrr vals (pred, true): 0.247, 0.282
batch losses (mrrl, rdl): 0.0120506268, 0.0001458198

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 599
rank avg (pred): 0.450 +- 0.262
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 5.30963e-05, 6.406e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 169
rank avg (pred): 0.481 +- 0.246
mrr vals (pred, true): 0.030, 0.049
batch losses (mrrl, rdl): 0.0041284561, 5.55498e-05

Epoch over!
epoch time: 15.178

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 24
rank avg (pred): 0.142 +- 0.228
mrr vals (pred, true): 0.199, 0.167
batch losses (mrrl, rdl): 0.0102641936, 0.0001852863

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 10
rank avg (pred): 0.137 +- 0.238
mrr vals (pred, true): 0.216, 0.210
batch losses (mrrl, rdl): 0.000462442, 7.14744e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 377
rank avg (pred): 0.447 +- 0.267
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.0001725603, 1.7434e-06

Epoch over!
epoch time: 15.025

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 228
rank avg (pred): 0.470 +- 0.257
mrr vals (pred, true): 0.039, 0.045
batch losses (mrrl, rdl): 0.001293423, 2.25986e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 131
rank avg (pred): 0.436 +- 0.246
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0001409454, 4.6032e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 233
rank avg (pred): 0.449 +- 0.266
mrr vals (pred, true): 0.059, 0.051
batch losses (mrrl, rdl): 0.0007511475, 4.1085e-06

Epoch over!
epoch time: 15.039

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 205
rank avg (pred): 0.472 +- 0.265
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 5.83923e-05, 1.37669e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1114
rank avg (pred): 0.446 +- 0.242
mrr vals (pred, true): 0.043, 0.054
batch losses (mrrl, rdl): 0.0005221585, 3.4558e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 877
rank avg (pred): 0.447 +- 0.252
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 2.45686e-05, 4.9792e-06

Epoch over!
epoch time: 15.06

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 19
rank avg (pred): 0.150 +- 0.234
mrr vals (pred, true): 0.216, 0.204
batch losses (mrrl, rdl): 0.0013152068, 6.23693e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 566
rank avg (pred): 0.460 +- 0.255
mrr vals (pred, true): 0.057, 0.023
batch losses (mrrl, rdl): 0.0004356841, 5.59325e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1021
rank avg (pred): 0.426 +- 0.240
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 1.656e-07, 5.1258e-06

Epoch over!
epoch time: 15.022

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1002
rank avg (pred): 0.440 +- 0.236
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0002718628, 4.7542e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 78
rank avg (pred): 0.203 +- 0.233
mrr vals (pred, true): 0.183, 0.205
batch losses (mrrl, rdl): 0.0047088913, 5.7752e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 70
rank avg (pred): 0.168 +- 0.233
mrr vals (pred, true): 0.212, 0.225
batch losses (mrrl, rdl): 0.0016978382, 1.91847e-05

Epoch over!
epoch time: 15.008

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 808
rank avg (pred): 0.454 +- 0.238
mrr vals (pred, true): 0.044, 0.046
batch losses (mrrl, rdl): 0.0003321687, 7.9989e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 440
rank avg (pred): 0.455 +- 0.251
mrr vals (pred, true): 0.051, 0.056
batch losses (mrrl, rdl): 1.69731e-05, 7.0651e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1169
rank avg (pred): 0.417 +- 0.237
mrr vals (pred, true): 0.050, 0.031
batch losses (mrrl, rdl): 1.003e-06, 5.23329e-05

Epoch over!
epoch time: 15.087

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 26
rank avg (pred): 0.195 +- 0.243
mrr vals (pred, true): 0.218, 0.219
batch losses (mrrl, rdl): 1.07253e-05, 6.8081e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 91
rank avg (pred): 0.446 +- 0.244
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 6.86163e-05, 5.578e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1201
rank avg (pred): 0.400 +- 0.218
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 1.17615e-05, 4.96137e-05

Epoch over!
epoch time: 15.218

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 887
rank avg (pred): 0.468 +- 0.255
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 0.0001042805, 4.9204e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 927
rank avg (pred): 0.431 +- 0.253
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 8.7063e-06, 3.15361e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 603
rank avg (pred): 0.403 +- 0.248
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.0019459298, 8.93938e-05

Epoch over!
epoch time: 15.065

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.439 +- 0.239
mrr vals (pred, true): 0.046, 0.051

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.04907 	 0.02022 	 ~...
   31 	     1 	 0.04969 	 0.02172 	 ~...
   78 	     2 	 0.05025 	 0.02192 	 ~...
   18 	     3 	 0.04904 	 0.02204 	 ~...
   81 	     4 	 0.05034 	 0.02328 	 ~...
   85 	     5 	 0.05037 	 0.02418 	 ~...
   86 	     6 	 0.05039 	 0.02531 	 ~...
   82 	     7 	 0.05036 	 0.02543 	 ~...
   48 	     8 	 0.05013 	 0.02544 	 ~...
   83 	     9 	 0.05036 	 0.02622 	 ~...
   77 	    10 	 0.05021 	 0.02638 	 ~...
   16 	    11 	 0.04879 	 0.02646 	 ~...
   80 	    12 	 0.05029 	 0.02664 	 ~...
   76 	    13 	 0.05019 	 0.02794 	 ~...
    3 	    14 	 0.04608 	 0.03500 	 ~...
   19 	    15 	 0.04905 	 0.03512 	 ~...
   39 	    16 	 0.04982 	 0.03671 	 ~...
    0 	    17 	 0.04539 	 0.03695 	 ~...
   29 	    18 	 0.04963 	 0.03701 	 ~...
   49 	    19 	 0.05014 	 0.03704 	 ~...
   47 	    20 	 0.05010 	 0.03931 	 ~...
   49 	    21 	 0.05014 	 0.03971 	 ~...
   88 	    22 	 0.05056 	 0.04020 	 ~...
   90 	    23 	 0.05066 	 0.04077 	 ~...
   49 	    24 	 0.05014 	 0.04080 	 ~...
   91 	    25 	 0.05066 	 0.04109 	 ~...
   15 	    26 	 0.04859 	 0.04127 	 ~...
   27 	    27 	 0.04937 	 0.04164 	 ~...
   49 	    28 	 0.05014 	 0.04179 	 ~...
    6 	    29 	 0.04733 	 0.04235 	 ~...
    1 	    30 	 0.04553 	 0.04238 	 ~...
   49 	    31 	 0.05014 	 0.04241 	 ~...
   49 	    32 	 0.05014 	 0.04251 	 ~...
   89 	    33 	 0.05057 	 0.04270 	 ~...
   32 	    34 	 0.04970 	 0.04276 	 ~...
   11 	    35 	 0.04793 	 0.04278 	 ~...
   87 	    36 	 0.05041 	 0.04311 	 ~...
   13 	    37 	 0.04838 	 0.04311 	 ~...
   49 	    38 	 0.05014 	 0.04336 	 ~...
   22 	    39 	 0.04915 	 0.04340 	 ~...
   38 	    40 	 0.04977 	 0.04353 	 ~...
    9 	    41 	 0.04780 	 0.04365 	 ~...
   49 	    42 	 0.05014 	 0.04369 	 ~...
   36 	    43 	 0.04973 	 0.04410 	 ~...
   45 	    44 	 0.05007 	 0.04444 	 ~...
   42 	    45 	 0.04984 	 0.04504 	 ~...
   49 	    46 	 0.05014 	 0.04505 	 ~...
   79 	    47 	 0.05025 	 0.04507 	 ~...
   43 	    48 	 0.04987 	 0.04542 	 ~...
   23 	    49 	 0.04925 	 0.04564 	 ~...
   10 	    50 	 0.04781 	 0.04581 	 ~...
    2 	    51 	 0.04590 	 0.04586 	 ~...
   49 	    52 	 0.05014 	 0.04594 	 ~...
   26 	    53 	 0.04931 	 0.04594 	 ~...
   49 	    54 	 0.05014 	 0.04608 	 ~...
   25 	    55 	 0.04926 	 0.04650 	 ~...
   12 	    56 	 0.04831 	 0.04659 	 ~...
   34 	    57 	 0.04971 	 0.04734 	 ~...
   49 	    58 	 0.05014 	 0.04736 	 ~...
   21 	    59 	 0.04909 	 0.04757 	 ~...
   49 	    60 	 0.05014 	 0.04764 	 ~...
   35 	    61 	 0.04971 	 0.04776 	 ~...
   46 	    62 	 0.05008 	 0.04789 	 ~...
   49 	    63 	 0.05014 	 0.04819 	 ~...
   14 	    64 	 0.04858 	 0.04819 	 ~...
   49 	    65 	 0.05014 	 0.04822 	 ~...
   49 	    66 	 0.05014 	 0.04879 	 ~...
   49 	    67 	 0.05014 	 0.04883 	 ~...
   28 	    68 	 0.04951 	 0.04911 	 ~...
   17 	    69 	 0.04886 	 0.04970 	 ~...
   40 	    70 	 0.04983 	 0.04973 	 ~...
   49 	    71 	 0.05014 	 0.04989 	 ~...
    8 	    72 	 0.04767 	 0.04991 	 ~...
   49 	    73 	 0.05014 	 0.05007 	 ~...
   49 	    74 	 0.05014 	 0.05019 	 ~...
    5 	    75 	 0.04633 	 0.05024 	 ~...
   30 	    76 	 0.04965 	 0.05033 	 ~...
   49 	    77 	 0.05014 	 0.05082 	 ~...
   37 	    78 	 0.04974 	 0.05112 	 ~...
    4 	    79 	 0.04614 	 0.05129 	 ~...
    7 	    80 	 0.04755 	 0.05144 	 ~...
   44 	    81 	 0.05006 	 0.05184 	 ~...
   49 	    82 	 0.05014 	 0.05208 	 ~...
   49 	    83 	 0.05014 	 0.05235 	 ~...
   84 	    84 	 0.05036 	 0.05299 	 ~...
   49 	    85 	 0.05014 	 0.05305 	 ~...
   49 	    86 	 0.05014 	 0.05439 	 ~...
   49 	    87 	 0.05014 	 0.05479 	 ~...
   33 	    88 	 0.04970 	 0.05498 	 ~...
   24 	    89 	 0.04926 	 0.05611 	 ~...
   41 	    90 	 0.04984 	 0.05887 	 ~...
   49 	    91 	 0.05014 	 0.06077 	 ~...
   93 	    92 	 0.20253 	 0.19520 	 ~...
   92 	    93 	 0.19883 	 0.19872 	 ~...
   95 	    94 	 0.20607 	 0.20017 	 ~...
   94 	    95 	 0.20325 	 0.20277 	 ~...
   97 	    96 	 0.21442 	 0.21298 	 ~...
   99 	    97 	 0.21581 	 0.21474 	 ~...
  101 	    98 	 0.22326 	 0.21616 	 ~...
   98 	    99 	 0.21468 	 0.21817 	 ~...
  102 	   100 	 0.22506 	 0.22870 	 ~...
   96 	   101 	 0.21286 	 0.23554 	 ~...
  100 	   102 	 0.22287 	 0.23739 	 ~...
  104 	   103 	 0.23423 	 0.24307 	 ~...
  106 	   104 	 0.23625 	 0.24618 	 ~...
  109 	   105 	 0.24781 	 0.24954 	 ~...
  105 	   106 	 0.23525 	 0.25212 	 ~...
  108 	   107 	 0.24474 	 0.25434 	 ~...
  110 	   108 	 0.26382 	 0.25820 	 ~...
  103 	   109 	 0.23401 	 0.26409 	 m..s
  112 	   110 	 0.27128 	 0.26624 	 ~...
  111 	   111 	 0.26922 	 0.27497 	 ~...
  114 	   112 	 0.27705 	 0.28034 	 ~...
  115 	   113 	 0.27760 	 0.28160 	 ~...
  107 	   114 	 0.24049 	 0.28353 	 m..s
  116 	   115 	 0.28184 	 0.29788 	 ~...
  117 	   116 	 0.29440 	 0.31748 	 ~...
  113 	   117 	 0.27310 	 0.32148 	 m..s
  120 	   118 	 0.36182 	 0.39525 	 m..s
  119 	   119 	 0.34036 	 0.52755 	 MISS
  118 	   120 	 0.33956 	 0.55007 	 MISS
==========================================
r_mrr = 0.9746030569076538
r2_mrr = 0.927678644657135
spearmanr_mrr@5 = 0.8997374176979065
spearmanr_mrr@10 = 0.949602484703064
spearmanr_mrr@50 = 0.9672057628631592
spearmanr_mrr@100 = 0.9792303442955017
spearmanr_mrr@All = 0.9798740148544312
==========================================
test time: 0.452
Done Testing dataset UMLS
total time taken: 232.11232018470764
training time taken: 225.94932103157043
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9746)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9277)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.8997)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9496)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9672)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9792)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9799)}}, 'test_loss': {'ComplEx': {'UMLS': 0.8835836710588865}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 4595585880310682
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [388, 540, 865, 1067, 219, 452, 128, 520, 82, 1194, 812, 1145, 796, 451, 595, 1184, 501, 269, 820, 75, 725, 346, 317, 1133, 999, 937, 907, 677, 49, 1135, 1077, 1192, 376, 1018, 1183, 234, 697, 408, 781, 961, 596, 475, 723, 1102, 551, 766, 934, 915, 787, 261, 1016, 87, 561, 514, 951, 849, 375, 629, 558, 424, 41, 614, 762, 1083, 944, 120, 410, 1031, 1076, 1042, 136, 378, 608, 1011, 179, 827, 557, 0, 125, 1171, 928, 609, 1052, 946, 436, 981, 1108, 839, 444, 74, 1149, 1071, 230, 461, 326, 488, 980, 38, 1087, 768, 736, 496, 936, 1091, 1061, 441, 312, 301, 503, 751, 902, 840, 309, 960, 917, 689, 547, 354, 888, 583, 1169]
valid_ids (0): []
train_ids (1094): [214, 860, 743, 535, 474, 1072, 783, 339, 536, 1144, 458, 704, 1147, 1006, 977, 447, 712, 460, 1004, 328, 351, 299, 925, 331, 735, 133, 20, 1179, 683, 330, 678, 1103, 590, 493, 858, 306, 1025, 693, 806, 575, 355, 1134, 1180, 985, 127, 39, 563, 94, 793, 538, 1195, 1120, 797, 969, 726, 11, 40, 43, 421, 1211, 641, 566, 308, 177, 727, 236, 1054, 976, 525, 202, 788, 28, 967, 276, 1117, 546, 1146, 1062, 739, 701, 1079, 950, 430, 871, 549, 155, 186, 630, 957, 192, 758, 366, 764, 51, 635, 978, 18, 286, 502, 809, 1056, 1206, 1074, 504, 524, 35, 84, 416, 1188, 497, 1037, 882, 19, 457, 37, 938, 1170, 989, 495, 382, 91, 541, 1167, 322, 478, 500, 114, 1209, 34, 918, 259, 325, 464, 792, 876, 449, 879, 1047, 920, 134, 1043, 229, 73, 1093, 794, 836, 819, 627, 1201, 151, 422, 974, 318, 67, 613, 526, 107, 209, 838, 13, 141, 42, 32, 365, 336, 529, 89, 949, 878, 696, 657, 1051, 801, 210, 1007, 1106, 775, 687, 645, 1115, 958, 906, 383, 581, 706, 675, 837, 1138, 168, 356, 599, 682, 972, 1199, 1023, 1033, 767, 1212, 598, 1125, 624, 92, 1173, 756, 579, 1040, 1082, 110, 498, 224, 333, 130, 188, 1113, 637, 446, 785, 287, 971, 901, 982, 208, 909, 869, 477, 810, 1126, 303, 667, 390, 250, 1143, 691, 803, 1039, 1168, 752, 1129, 165, 257, 203, 513, 244, 572, 198, 361, 543, 634, 1196, 98, 965, 533, 206, 160, 968, 720, 845, 872, 491, 834, 8, 1160, 272, 892, 252, 518, 745, 509, 459, 176, 404, 988, 400, 190, 335, 567, 830, 44, 143, 811, 1048, 53, 1109, 63, 144, 1068, 352, 1104, 211, 419, 164, 698, 610, 121, 782, 732, 774, 294, 398, 861, 1015, 101, 653, 896, 226, 601, 759, 1154, 104, 199, 1022, 207, 146, 991, 997, 508, 126, 48, 649, 555, 170, 631, 277, 138, 377, 505, 432, 275, 485, 552, 194, 1034, 183, 738, 402, 147, 266, 467, 1110, 1012, 883, 1114, 577, 1009, 232, 406, 1041, 843, 476, 510, 342, 197, 620, 469, 973, 1142, 1096, 887, 770, 671, 562, 1176, 1127, 1159, 640, 201, 674, 799, 1075, 426, 233, 707, 757, 407, 1090, 626, 282, 486, 17, 670, 135, 59, 519, 494, 145, 189, 633, 445, 1029, 531, 271, 1122, 1044, 340, 619, 862, 72, 580, 578, 76, 1081, 874, 948, 718, 857, 688, 360, 399, 821, 995, 1128, 1151, 24, 945, 156, 223, 154, 332, 1116, 215, 795, 933, 60, 929, 908, 260, 492, 647, 471, 714, 484, 185, 337, 55, 293, 288, 983, 600, 329, 656, 1163, 1038, 530, 1130, 109, 1028, 427, 913, 487, 1137, 900, 798, 643, 172, 69, 975, 602, 713, 852, 853, 1165, 623, 1045, 456, 220, 36, 550, 831, 964, 894, 943, 870, 440, 1198, 1046, 175, 415, 264, 1050, 216, 813, 822, 26, 606, 30, 196, 658, 923, 746, 428, 1002, 393, 664, 826, 695, 709, 1024, 638, 187, 576, 1136, 521, 771, 1182, 313, 780, 80, 523, 490, 1013, 279, 846, 374, 280, 411, 132, 153, 243, 676, 717, 1161, 240, 62, 668, 397, 438, 1099, 47, 511, 1186, 574, 784, 573, 439, 711, 1197, 195, 654, 21, 1119, 644, 1140, 603, 632, 1191, 180, 1060, 911, 966, 113, 2, 570, 680, 81, 405, 754, 534, 516, 434, 265, 1124, 499, 642, 947, 1153, 86, 615, 777, 1185, 1, 730, 661, 23, 320, 765, 939, 1204, 802, 64, 646, 418, 897, 465, 1010, 300, 169, 481, 1080, 88, 564, 979, 297, 1019, 545, 112, 1088, 1214, 554, 1095, 686, 228, 50, 517, 1105, 263, 618, 1084, 790, 855, 984, 970, 448, 1139, 1059, 385, 921, 314, 565, 662, 345, 150, 305, 992, 122, 591, 854, 221, 560, 85, 817, 225, 1020, 1073, 379, 367, 1014, 14, 942, 1187, 129, 1049, 281, 710, 825, 222, 914, 859, 205, 990, 721, 159, 96, 54, 744, 111, 741, 249, 124, 715, 585, 68, 708, 1027, 6, 395, 241, 589, 1157, 506, 1026, 479, 255, 181, 903, 289, 731, 472, 4, 716, 1057, 1175, 804, 173, 284, 79, 359, 231, 895, 387, 941, 256, 384, 319, 1150, 414, 663, 423, 470, 703, 954, 1030, 334, 184, 639, 648, 152, 381, 12, 1202, 528, 728, 679, 344, 905, 962, 953, 848, 267, 694, 1152, 149, 315, 749, 1035, 994, 636, 927, 401, 235, 115, 1055, 864, 1086, 1207, 25, 559, 108, 242, 396, 83, 556, 295, 171, 116, 327, 453, 1164, 450, 1111, 323, 700, 245, 926, 246, 1101, 886, 212, 893, 239, 412, 455, 1181, 57, 166, 597, 290, 829, 254, 856, 357, 515, 1089, 372, 586, 137, 916, 321, 553, 99, 705, 773, 607, 621, 274, 311, 77, 786, 1200, 778, 22, 217, 996, 835, 587, 659, 1069, 262, 368, 489, 924, 1210, 425, 588, 1193, 193, 462, 1000, 507, 763, 1053, 748, 1036, 729, 413, 594, 625, 370, 660, 998, 10, 304, 877, 97, 512, 386, 273, 347, 1003, 779, 52, 1141, 316, 733, 433, 1158, 1190, 956, 58, 932, 815, 409, 358, 268, 364, 218, 343, 291, 1166, 772, 747, 940, 702, 1065, 685, 571, 31, 544, 750, 139, 1132, 1100, 881, 690, 665, 963, 655, 103, 1174, 7, 1005, 33, 106, 666, 442, 776, 466, 842, 278, 734, 986, 1097, 161, 1118, 1078, 1094, 1008, 302, 394, 29, 722, 338, 100, 650, 1178, 740, 71, 417, 542, 371, 922, 955, 463, 119, 348, 828, 296, 885, 866, 78, 292, 527, 93, 1208, 429, 537, 363, 611, 15, 253, 118, 868, 285, 1189, 959, 307, 480, 1032, 889, 369, 616, 163, 420, 760, 45, 832, 341, 1107, 1205, 875, 873, 952, 403, 27, 65, 258, 380, 3, 807, 66, 298, 805, 684, 851, 568, 1021, 1063, 157, 1156, 651, 823, 816, 719, 935, 353, 890, 761, 247, 373, 158, 737, 123, 612, 56, 1098, 389, 841, 131, 863, 1121, 473, 673, 884, 1058, 1155, 174, 227, 681, 930, 1085, 102, 1064, 652, 362, 1162, 251, 850, 824, 891, 548, 617, 569, 753, 808, 5, 1131, 919, 604, 248, 454, 1203, 117, 904, 213, 993, 283, 742, 238, 789, 468, 910, 1092, 880, 270, 1148, 1172, 90, 105, 1177, 1112, 431, 61, 582, 9, 435, 1017, 912, 437, 669, 482, 349, 724, 692, 593, 178, 847, 898, 755, 899, 443, 791, 522, 70, 672, 532, 191, 140, 844, 867, 699, 622, 628, 392, 1066, 584, 204, 605, 1070, 833, 95, 310, 167, 350, 800, 1123, 1213, 142, 769, 987, 483, 182, 391, 16, 46, 1001, 148, 200, 818, 931, 814, 162, 237, 539, 592, 324]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5611550623534398
the save name prefix for this run is:  chkpt-ID_5611550623534398_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 879
rank avg (pred): 0.477 +- 0.006
mrr vals (pred, true): 0.015, 0.053
batch losses (mrrl, rdl): 0.0, 0.0001122698

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 336
rank avg (pred): 0.443 +- 0.013
mrr vals (pred, true): 0.017, 0.050
batch losses (mrrl, rdl): 0.0, 7.17223e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 633
rank avg (pred): 0.453 +- 0.235
mrr vals (pred, true): 0.020, 0.040
batch losses (mrrl, rdl): 0.0, 2.75287e-05

Epoch over!
epoch time: 14.927

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 221
rank avg (pred): 0.470 +- 0.258
mrr vals (pred, true): 0.020, 0.058
batch losses (mrrl, rdl): 0.0, 5.1223e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 578
rank avg (pred): 0.488 +- 0.272
mrr vals (pred, true): 0.020, 0.035
batch losses (mrrl, rdl): 0.0, 6.6881e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 643
rank avg (pred): 0.436 +- 0.236
mrr vals (pred, true): 0.021, 0.041
batch losses (mrrl, rdl): 0.0, 4.12079e-05

Epoch over!
epoch time: 14.969

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 642
rank avg (pred): 0.459 +- 0.235
mrr vals (pred, true): 0.020, 0.042
batch losses (mrrl, rdl): 0.0, 2.796e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1201
rank avg (pred): 0.451 +- 0.257
mrr vals (pred, true): 0.021, 0.052
batch losses (mrrl, rdl): 0.0, 2.1713e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1033
rank avg (pred): 0.464 +- 0.256
mrr vals (pred, true): 0.021, 0.047
batch losses (mrrl, rdl): 0.0, 6.9718e-06

Epoch over!
epoch time: 14.946

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1112
rank avg (pred): 0.452 +- 0.259
mrr vals (pred, true): 0.022, 0.049
batch losses (mrrl, rdl): 0.0, 4.1001e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 947
rank avg (pred): 0.472 +- 0.236
mrr vals (pred, true): 0.019, 0.047
batch losses (mrrl, rdl): 0.0, 1.14682e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 481
rank avg (pred): 0.470 +- 0.270
mrr vals (pred, true): 0.021, 0.044
batch losses (mrrl, rdl): 0.0, 7.1985e-06

Epoch over!
epoch time: 14.83

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 393
rank avg (pred): 0.447 +- 0.256
mrr vals (pred, true): 0.022, 0.047
batch losses (mrrl, rdl): 0.0, 1.8527e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1203
rank avg (pred): 0.429 +- 0.258
mrr vals (pred, true): 0.024, 0.048
batch losses (mrrl, rdl): 0.0, 1.26551e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 174
rank avg (pred): 0.454 +- 0.255
mrr vals (pred, true): 0.022, 0.044
batch losses (mrrl, rdl): 0.0, 2.9385e-06

Epoch over!
epoch time: 14.8

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 563
rank avg (pred): 0.526 +- 0.218
mrr vals (pred, true): 0.016, 0.021
batch losses (mrrl, rdl): 0.0112847835, 1.14292e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 27
rank avg (pred): 0.098 +- 0.120
mrr vals (pred, true): 0.238, 0.244
batch losses (mrrl, rdl): 0.000466549, 7.69677e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 48
rank avg (pred): 0.102 +- 0.114
mrr vals (pred, true): 0.215, 0.208
batch losses (mrrl, rdl): 0.0004992262, 0.0002101615

Epoch over!
epoch time: 15.074

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 334
rank avg (pred): 0.446 +- 0.402
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 4.71176e-05, 0.0001311277

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 972
rank avg (pred): 0.089 +- 0.100
mrr vals (pred, true): 0.320, 0.287
batch losses (mrrl, rdl): 0.0104972795, 6.87209e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 162
rank avg (pred): 0.391 +- 0.351
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 2.0985e-05, 8.55265e-05

Epoch over!
epoch time: 15.069

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 113
rank avg (pred): 0.441 +- 0.395
mrr vals (pred, true): 0.049, 0.059
batch losses (mrrl, rdl): 7.9031e-06, 0.0001080203

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 59
rank avg (pred): 0.096 +- 0.096
mrr vals (pred, true): 0.241, 0.267
batch losses (mrrl, rdl): 0.0065962952, 4.61664e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 466
rank avg (pred): 0.411 +- 0.356
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.8951e-06, 7.2479e-05

Epoch over!
epoch time: 15.071

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 564
rank avg (pred): 0.401 +- 0.334
mrr vals (pred, true): 0.047, 0.024
batch losses (mrrl, rdl): 0.0001122737, 0.0002546161

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1044
rank avg (pred): 0.397 +- 0.344
mrr vals (pred, true): 0.052, 0.052
batch losses (mrrl, rdl): 3.36925e-05, 6.35532e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 734
rank avg (pred): 0.088 +- 0.097
mrr vals (pred, true): 0.381, 0.528
batch losses (mrrl, rdl): 0.2159807086, 3.14085e-05

Epoch over!
epoch time: 15.055

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 446
rank avg (pred): 0.402 +- 0.345
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 3.3877e-06, 4.90218e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 664
rank avg (pred): 0.426 +- 0.364
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 4.4934e-06, 6.77913e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 757
rank avg (pred): 0.466 +- 0.413
mrr vals (pred, true): 0.057, 0.047
batch losses (mrrl, rdl): 0.000428717, 0.0001710599

Epoch over!
epoch time: 15.004

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 31
rank avg (pred): 0.098 +- 0.094
mrr vals (pred, true): 0.231, 0.242
batch losses (mrrl, rdl): 0.0013061345, 6.77973e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 442
rank avg (pred): 0.384 +- 0.312
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 7.4144e-06, 3.25245e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1050
rank avg (pred): 0.410 +- 0.349
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001088592, 5.45161e-05

Epoch over!
epoch time: 15.02

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 825
rank avg (pred): 0.100 +- 0.093
mrr vals (pred, true): 0.223, 0.253
batch losses (mrrl, rdl): 0.0088748224, 0.0001140314

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 97
rank avg (pred): 0.418 +- 0.354
mrr vals (pred, true): 0.054, 0.039
batch losses (mrrl, rdl): 0.0001391289, 7.30601e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 950
rank avg (pred): 0.491 +- 0.434
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001571967, 0.0002409165

Epoch over!
epoch time: 15.077

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 54
rank avg (pred): 0.100 +- 0.092
mrr vals (pred, true): 0.221, 0.230
batch losses (mrrl, rdl): 0.0007880394, 9.3213e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 65
rank avg (pred): 0.099 +- 0.092
mrr vals (pred, true): 0.227, 0.217
batch losses (mrrl, rdl): 0.0010065832, 0.000128565

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1030
rank avg (pred): 0.420 +- 0.341
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 6.61873e-05, 3.95395e-05

Epoch over!
epoch time: 15.089

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.404 +- 0.332
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 4.75001e-05, 4.10261e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 993
rank avg (pred): 0.096 +- 0.091
mrr vals (pred, true): 0.282, 0.288
batch losses (mrrl, rdl): 0.0003564625, 5.57569e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 612
rank avg (pred): 0.409 +- 0.331
mrr vals (pred, true): 0.050, 0.036
batch losses (mrrl, rdl): 4.637e-07, 6.29398e-05

Epoch over!
epoch time: 15.093

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 750
rank avg (pred): 0.093 +- 0.091
mrr vals (pred, true): 0.296, 0.250
batch losses (mrrl, rdl): 0.0212820955, 8.0696e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 755
rank avg (pred): 0.091 +- 0.088
mrr vals (pred, true): 0.296, 0.211
batch losses (mrrl, rdl): 0.071566388, 0.0002090308

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 964
rank avg (pred): 0.486 +- 0.380
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0004893951, 0.0001436373

Epoch over!
epoch time: 15.076

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.408 +- 0.329
mrr vals (pred, true): 0.053, 0.046

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05258 	 0.01926 	 m..s
    0 	     1 	 0.05258 	 0.02037 	 m..s
    0 	     2 	 0.05258 	 0.02050 	 m..s
    0 	     3 	 0.05258 	 0.02301 	 ~...
    0 	     4 	 0.05258 	 0.02305 	 ~...
   84 	     5 	 0.05514 	 0.02370 	 m..s
    0 	     6 	 0.05258 	 0.02442 	 ~...
    0 	     7 	 0.05258 	 0.02448 	 ~...
    0 	     8 	 0.05258 	 0.02453 	 ~...
   81 	     9 	 0.05502 	 0.02467 	 m..s
   83 	    10 	 0.05507 	 0.02475 	 m..s
    0 	    11 	 0.05258 	 0.02476 	 ~...
    0 	    12 	 0.05258 	 0.02543 	 ~...
   85 	    13 	 0.05516 	 0.02552 	 ~...
   80 	    14 	 0.05499 	 0.02561 	 ~...
    0 	    15 	 0.05258 	 0.02619 	 ~...
    0 	    16 	 0.05258 	 0.02634 	 ~...
    0 	    17 	 0.05258 	 0.02664 	 ~...
   87 	    18 	 0.05543 	 0.02782 	 ~...
    0 	    19 	 0.05258 	 0.02979 	 ~...
    0 	    20 	 0.05258 	 0.03018 	 ~...
    0 	    21 	 0.05258 	 0.03029 	 ~...
    0 	    22 	 0.05258 	 0.03139 	 ~...
    0 	    23 	 0.05258 	 0.03500 	 ~...
    0 	    24 	 0.05258 	 0.03520 	 ~...
   91 	    25 	 0.05556 	 0.03663 	 ~...
    0 	    26 	 0.05258 	 0.03671 	 ~...
    0 	    27 	 0.05258 	 0.03771 	 ~...
    0 	    28 	 0.05258 	 0.03815 	 ~...
   92 	    29 	 0.05557 	 0.03858 	 ~...
    0 	    30 	 0.05258 	 0.03909 	 ~...
    0 	    31 	 0.05258 	 0.03930 	 ~...
    0 	    32 	 0.05258 	 0.03942 	 ~...
    0 	    33 	 0.05258 	 0.03957 	 ~...
   90 	    34 	 0.05554 	 0.03964 	 ~...
   76 	    35 	 0.05444 	 0.04005 	 ~...
   77 	    36 	 0.05444 	 0.04081 	 ~...
    0 	    37 	 0.05258 	 0.04088 	 ~...
    0 	    38 	 0.05258 	 0.04115 	 ~...
   86 	    39 	 0.05529 	 0.04139 	 ~...
   78 	    40 	 0.05456 	 0.04224 	 ~...
    0 	    41 	 0.05258 	 0.04238 	 ~...
   96 	    42 	 0.05695 	 0.04259 	 ~...
    0 	    43 	 0.05258 	 0.04261 	 ~...
    0 	    44 	 0.05258 	 0.04276 	 ~...
    0 	    45 	 0.05258 	 0.04279 	 ~...
    0 	    46 	 0.05258 	 0.04293 	 ~...
    0 	    47 	 0.05258 	 0.04364 	 ~...
    0 	    48 	 0.05258 	 0.04395 	 ~...
    0 	    49 	 0.05258 	 0.04410 	 ~...
    0 	    50 	 0.05258 	 0.04418 	 ~...
    0 	    51 	 0.05258 	 0.04439 	 ~...
    0 	    52 	 0.05258 	 0.04444 	 ~...
    0 	    53 	 0.05258 	 0.04519 	 ~...
    0 	    54 	 0.05258 	 0.04525 	 ~...
   74 	    55 	 0.05430 	 0.04533 	 ~...
    0 	    56 	 0.05258 	 0.04542 	 ~...
    0 	    57 	 0.05258 	 0.04549 	 ~...
    0 	    58 	 0.05258 	 0.04574 	 ~...
    0 	    59 	 0.05258 	 0.04581 	 ~...
    0 	    60 	 0.05258 	 0.04594 	 ~...
   93 	    61 	 0.05567 	 0.04608 	 ~...
   73 	    62 	 0.05428 	 0.04635 	 ~...
    0 	    63 	 0.05258 	 0.04635 	 ~...
   95 	    64 	 0.05585 	 0.04668 	 ~...
    0 	    65 	 0.05258 	 0.04669 	 ~...
    0 	    66 	 0.05258 	 0.04677 	 ~...
    0 	    67 	 0.05258 	 0.04690 	 ~...
    0 	    68 	 0.05258 	 0.04705 	 ~...
    0 	    69 	 0.05258 	 0.04720 	 ~...
    0 	    70 	 0.05258 	 0.04736 	 ~...
    0 	    71 	 0.05258 	 0.04747 	 ~...
    0 	    72 	 0.05258 	 0.04760 	 ~...
    0 	    73 	 0.05258 	 0.04814 	 ~...
   88 	    74 	 0.05546 	 0.04822 	 ~...
    0 	    75 	 0.05258 	 0.04822 	 ~...
    0 	    76 	 0.05258 	 0.04854 	 ~...
   94 	    77 	 0.05580 	 0.04898 	 ~...
   79 	    78 	 0.05457 	 0.04953 	 ~...
   75 	    79 	 0.05439 	 0.04994 	 ~...
    0 	    80 	 0.05258 	 0.05132 	 ~...
    0 	    81 	 0.05258 	 0.05135 	 ~...
    0 	    82 	 0.05258 	 0.05208 	 ~...
    0 	    83 	 0.05258 	 0.05214 	 ~...
   82 	    84 	 0.05503 	 0.05223 	 ~...
    0 	    85 	 0.05258 	 0.05232 	 ~...
    0 	    86 	 0.05258 	 0.05263 	 ~...
    0 	    87 	 0.05258 	 0.05299 	 ~...
   89 	    88 	 0.05554 	 0.05320 	 ~...
    0 	    89 	 0.05258 	 0.05336 	 ~...
    0 	    90 	 0.05258 	 0.05354 	 ~...
    0 	    91 	 0.05258 	 0.05355 	 ~...
    0 	    92 	 0.05258 	 0.05434 	 ~...
    0 	    93 	 0.05258 	 0.05498 	 ~...
    0 	    94 	 0.05258 	 0.05501 	 ~...
    0 	    95 	 0.05258 	 0.05549 	 ~...
    0 	    96 	 0.05258 	 0.05954 	 ~...
  105 	    97 	 0.22966 	 0.18625 	 m..s
   99 	    98 	 0.21373 	 0.18840 	 ~...
   97 	    99 	 0.20037 	 0.19958 	 ~...
  101 	   100 	 0.22272 	 0.20165 	 ~...
  100 	   101 	 0.21617 	 0.20559 	 ~...
   98 	   102 	 0.20709 	 0.20603 	 ~...
  102 	   103 	 0.22603 	 0.21244 	 ~...
  108 	   104 	 0.24712 	 0.21940 	 ~...
  104 	   105 	 0.22865 	 0.22717 	 ~...
  103 	   106 	 0.22771 	 0.24354 	 ~...
  107 	   107 	 0.24598 	 0.24360 	 ~...
  106 	   108 	 0.23669 	 0.25592 	 ~...
  110 	   109 	 0.24916 	 0.26624 	 ~...
  116 	   110 	 0.28484 	 0.26880 	 ~...
  111 	   111 	 0.26318 	 0.27282 	 ~...
  117 	   112 	 0.28598 	 0.28248 	 ~...
  113 	   113 	 0.27088 	 0.28353 	 ~...
  109 	   114 	 0.24813 	 0.28449 	 m..s
  118 	   115 	 0.29238 	 0.28654 	 ~...
  112 	   116 	 0.26586 	 0.29247 	 ~...
  115 	   117 	 0.27758 	 0.29649 	 ~...
  114 	   118 	 0.27748 	 0.31636 	 m..s
  119 	   119 	 0.29711 	 0.33429 	 m..s
  120 	   120 	 0.46964 	 0.37683 	 m..s
==========================================
r_mrr = 0.9837843179702759
r2_mrr = 0.9542043805122375
spearmanr_mrr@5 = 0.9013112783432007
spearmanr_mrr@10 = 0.9006788730621338
spearmanr_mrr@50 = 0.9876090288162231
spearmanr_mrr@100 = 0.9904241561889648
spearmanr_mrr@All = 0.987496554851532
==========================================
test time: 0.462
Done Testing dataset UMLS
total time taken: 231.84127712249756
training time taken: 225.5941286087036
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9838)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9542)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9013)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9007)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9876)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9904)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9875)}}, 'test_loss': {'ComplEx': {'UMLS': 0.21721770909789484}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 1612160193359283
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [938, 972, 594, 1062, 707, 268, 149, 492, 498, 164, 205, 169, 1059, 64, 410, 467, 857, 506, 168, 516, 141, 587, 982, 778, 995, 450, 153, 815, 52, 727, 462, 876, 1071, 1160, 7, 1065, 341, 533, 642, 614, 850, 156, 101, 195, 932, 828, 175, 38, 214, 1066, 69, 899, 956, 49, 239, 259, 50, 114, 1034, 210, 1052, 703, 1150, 352, 790, 829, 15, 436, 860, 574, 655, 877, 942, 912, 1077, 332, 641, 461, 1172, 821, 891, 402, 40, 1070, 741, 914, 864, 1035, 846, 830, 495, 204, 931, 93, 54, 1000, 883, 1105, 398, 274, 1085, 137, 194, 424, 490, 142, 80, 167, 980, 468, 759, 186, 1199, 249, 1146, 333, 228, 465, 109, 319, 913]
valid_ids (0): []
train_ids (1094): [643, 1054, 390, 459, 170, 1109, 1159, 423, 1076, 216, 1198, 796, 885, 666, 154, 904, 965, 890, 709, 702, 281, 1082, 543, 359, 95, 362, 305, 292, 1157, 378, 715, 211, 1009, 338, 173, 380, 57, 1184, 718, 1001, 408, 925, 1064, 859, 672, 652, 696, 845, 517, 162, 1058, 266, 792, 900, 496, 1080, 1135, 621, 855, 116, 512, 847, 620, 1132, 523, 1056, 923, 514, 470, 1144, 172, 474, 697, 43, 560, 147, 945, 1063, 767, 566, 1121, 48, 190, 1137, 227, 111, 488, 483, 356, 487, 978, 657, 96, 1049, 371, 671, 662, 577, 105, 250, 593, 629, 803, 501, 313, 497, 504, 957, 277, 330, 562, 276, 191, 695, 464, 705, 376, 152, 763, 988, 893, 412, 303, 690, 183, 133, 670, 664, 1033, 813, 881, 793, 768, 234, 393, 837, 589, 640, 532, 819, 404, 728, 1100, 983, 254, 608, 750, 2, 610, 1007, 576, 304, 388, 955, 355, 260, 689, 112, 311, 279, 820, 218, 324, 529, 1048, 946, 521, 622, 905, 789, 280, 325, 432, 746, 1020, 335, 1204, 618, 1142, 273, 761, 1191, 299, 23, 916, 421, 475, 350, 647, 201, 1189, 706, 947, 638, 903, 1188, 486, 1015, 688, 1086, 711, 700, 22, 235, 633, 535, 312, 18, 106, 811, 994, 122, 694, 775, 285, 1158, 26, 799, 525, 209, 977, 315, 624, 1061, 527, 76, 185, 45, 975, 723, 554, 1036, 328, 708, 872, 880, 744, 882, 480, 921, 970, 749, 1180, 635, 809, 1155, 399, 55, 91, 1029, 561, 1002, 1094, 351, 951, 25, 935, 752, 1067, 934, 329, 1167, 320, 964, 403, 895, 1053, 839, 555, 748, 918, 973, 21, 384, 413, 1092, 62, 129, 924, 851, 135, 814, 863, 676, 1143, 1127, 32, 1182, 817, 547, 862, 354, 1173, 306, 314, 827, 519, 997, 406, 578, 673, 791, 1210, 217, 753, 908, 505, 1113, 1031, 1003, 684, 1125, 582, 233, 826, 1038, 781, 558, 747, 202, 823, 739, 1045, 1117, 286, 919, 686, 220, 886, 255, 263, 454, 774, 646, 783, 472, 785, 1164, 1186, 258, 853, 625, 818, 262, 1108, 223, 960, 1040, 367, 1026, 71, 968, 836, 79, 317, 630, 110, 230, 119, 226, 471, 644, 445, 457, 1055, 418, 1177, 801, 447, 734, 756, 1043, 685, 1019, 118, 1141, 868, 572, 1018, 28, 639, 508, 1179, 383, 448, 681, 834, 270, 1128, 927, 735, 1193, 120, 929, 326, 309, 933, 143, 225, 677, 392, 717, 564, 1106, 909, 342, 1098, 27, 166, 47, 966, 298, 580, 1116, 874, 442, 1200, 1044, 430, 1081, 939, 1010, 1, 1073, 381, 856, 779, 650, 787, 100, 1089, 617, 39, 1114, 1075, 745, 337, 867, 293, 449, 146, 780, 1197, 999, 573, 452, 1097, 247, 769, 557, 431, 275, 444, 331, 81, 1041, 634, 757, 503, 556, 336, 1008, 8, 363, 1087, 1027, 282, 1178, 401, 522, 387, 518, 660, 1162, 981, 489, 1112, 13, 873, 160, 824, 509, 540, 692, 656, 427, 733, 887, 41, 295, 611, 658, 1124, 51, 161, 591, 720, 372, 682, 30, 1147, 651, 417, 788, 632, 419, 16, 502, 236, 294, 165, 848, 451, 871, 986, 797, 565, 145, 484, 396, 1131, 838, 550, 714, 499, 31, 1017, 11, 971, 36, 974, 65, 1057, 949, 928, 626, 1187, 963, 157, 539, 536, 1039, 316, 231, 825, 922, 439, 180, 123, 699, 721, 70, 1025, 14, 126, 301, 113, 24, 575, 579, 212, 1051, 667, 1004, 1016, 1023, 206, 967, 34, 272, 1154, 961, 806, 1183, 493, 1068, 425, 89, 606, 1096, 368, 108, 150, 1005, 1069, 17, 1014, 866, 807, 1030, 264, 366, 244, 736, 798, 713, 159, 422, 102, 99, 1169, 414, 1088, 346, 87, 510, 800, 669, 278, 976, 725, 340, 370, 598, 77, 842, 251, 858, 121, 0, 1176, 772, 852, 892, 615, 590, 1047, 1151, 198, 930, 546, 1111, 1161, 950, 958, 835, 906, 740, 151, 46, 265, 1032, 894, 491, 944, 178, 107, 537, 911, 548, 426, 737, 901, 473, 1060, 6, 365, 1122, 879, 203, 869, 1202, 758, 888, 1134, 810, 1208, 948, 229, 391, 453, 307, 1211, 870, 1195, 1022, 224, 680, 37, 520, 1168, 917, 1213, 177, 940, 989, 581, 171, 405, 455, 382, 72, 415, 1129, 463, 83, 310, 1207, 1093, 348, 1136, 513, 732, 954, 524, 637, 659, 252, 1138, 534, 66, 571, 369, 103, 645, 68, 795, 920, 10, 605, 138, 61, 738, 544, 477, 377, 90, 179, 124, 878, 551, 466, 131, 679, 321, 300, 1103, 693, 411, 1209, 649, 1149, 802, 751, 1050, 42, 567, 104, 1013, 654, 1175, 74, 400, 754, 29, 437, 1139, 188, 271, 619, 59, 538, 196, 33, 60, 1078, 623, 221, 588, 822, 1126, 985, 482, 428, 500, 5, 861, 322, 148, 898, 764, 1046, 1133, 443, 726, 139, 1115, 993, 1072, 596, 990, 559, 339, 1120, 35, 794, 242, 12, 1212, 782, 1192, 1024, 568, 193, 585, 353, 952, 1201, 78, 962, 144, 1166, 469, 553, 816, 584, 722, 832, 115, 192, 902, 831, 507, 88, 347, 597, 1101, 1079, 1084, 698, 616, 94, 586, 284, 786, 343, 843, 334, 607, 456, 1110, 476, 125, 875, 385, 215, 601, 261, 784, 528, 327, 82, 1148, 140, 840, 176, 84, 222, 805, 600, 675, 290, 361, 1104, 75, 1091, 53, 602, 238, 627, 663, 1006, 926, 479, 1196, 665, 1206, 397, 155, 189, 1021, 127, 460, 1095, 612, 1099, 770, 163, 1140, 1205, 854, 1012, 563, 760, 134, 743, 1203, 648, 3, 174, 996, 1118, 766, 1119, 130, 1130, 979, 628, 849, 440, 941, 357, 661, 73, 288, 1028, 604, 549, 776, 246, 943, 199, 1102, 485, 257, 4, 897, 613, 833, 729, 701, 386, 1214, 394, 674, 287, 429, 953, 937, 991, 511, 691, 592, 889, 1011, 478, 267, 58, 240, 291, 253, 712, 1037, 132, 213, 128, 599, 197, 207, 187, 297, 433, 884, 364, 1163, 716, 896, 344, 283, 1190, 910, 907, 136, 308, 1123, 1107, 86, 603, 1185, 812, 777, 1194, 583, 609, 765, 243, 19, 407, 959, 545, 44, 771, 998, 435, 724, 9, 992, 67, 1170, 97, 379, 289, 1074, 755, 318, 184, 1090, 804, 241, 182, 208, 389, 20, 984, 936, 730, 323, 416, 653, 569, 446, 541, 530, 481, 117, 595, 844, 678, 631, 92, 219, 1083, 1153, 1042, 841, 375, 762, 773, 256, 438, 269, 302, 969, 683, 742, 515, 1165, 808, 409, 374, 531, 1171, 494, 1174, 1145, 704, 865, 245, 248, 731, 1156, 1152, 200, 63, 542, 158, 719, 434, 441, 296, 1181, 360, 915, 85, 358, 987, 668, 56, 232, 373, 552, 710, 237, 570, 181, 420, 349, 458, 636, 345, 395, 98, 687, 526]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  751859937684772
the save name prefix for this run is:  chkpt-ID_751859937684772_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 933
rank avg (pred): 0.425 +- 0.007
mrr vals (pred, true): 0.017, 0.027
batch losses (mrrl, rdl): 0.0, 0.0001521741

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 29
rank avg (pred): 0.175 +- 0.191
mrr vals (pred, true): 0.071, 0.249
batch losses (mrrl, rdl): 0.0, 1.54552e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 861
rank avg (pred): 0.465 +- 0.261
mrr vals (pred, true): 0.025, 0.047
batch losses (mrrl, rdl): 0.0, 1.1077e-06

Epoch over!
epoch time: 14.947

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 673
rank avg (pred): 0.477 +- 0.254
mrr vals (pred, true): 0.023, 0.042
batch losses (mrrl, rdl): 0.0, 2.1348e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1193
rank avg (pred): 0.440 +- 0.257
mrr vals (pred, true): 0.030, 0.055
batch losses (mrrl, rdl): 0.0, 8.213e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 841
rank avg (pred): 0.437 +- 0.256
mrr vals (pred, true): 0.031, 0.047
batch losses (mrrl, rdl): 0.0, 3.263e-07

Epoch over!
epoch time: 15.016

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 227
rank avg (pred): 0.439 +- 0.253
mrr vals (pred, true): 0.031, 0.054
batch losses (mrrl, rdl): 0.0, 8.201e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 732
rank avg (pred): 0.142 +- 0.191
mrr vals (pred, true): 0.107, 0.524
batch losses (mrrl, rdl): 0.0, 0.0001435279

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 691
rank avg (pred): 0.423 +- 0.255
mrr vals (pred, true): 0.036, 0.047
batch losses (mrrl, rdl): 0.0, 9.5258e-06

Epoch over!
epoch time: 15.083

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1159
rank avg (pred): 0.474 +- 0.243
mrr vals (pred, true): 0.026, 0.025
batch losses (mrrl, rdl): 0.0, 9.8633e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 242
rank avg (pred): 0.429 +- 0.265
mrr vals (pred, true): 0.038, 0.046
batch losses (mrrl, rdl): 0.0, 1.89523e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 631
rank avg (pred): 0.445 +- 0.259
mrr vals (pred, true): 0.032, 0.038
batch losses (mrrl, rdl): 0.0, 8.0389e-06

Epoch over!
epoch time: 15.082

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 711
rank avg (pred): 0.434 +- 0.254
mrr vals (pred, true): 0.034, 0.049
batch losses (mrrl, rdl): 0.0, 1.5795e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 363
rank avg (pred): 0.437 +- 0.267
mrr vals (pred, true): 0.038, 0.048
batch losses (mrrl, rdl): 0.0, 2.638e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 637
rank avg (pred): 0.446 +- 0.267
mrr vals (pred, true): 0.033, 0.041
batch losses (mrrl, rdl): 0.0, 1.23094e-05

Epoch over!
epoch time: 15.086

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 408
rank avg (pred): 0.422 +- 0.264
mrr vals (pred, true): 0.040, 0.050
batch losses (mrrl, rdl): 0.0010606773, 8.0653e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 947
rank avg (pred): 0.424 +- 0.292
mrr vals (pred, true): 0.043, 0.047
batch losses (mrrl, rdl): 0.0005094481, 1.33964e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 833
rank avg (pred): 0.185 +- 0.280
mrr vals (pred, true): 0.291, 0.321
batch losses (mrrl, rdl): 0.0093980916, 8.72334e-05

Epoch over!
epoch time: 15.31

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 582
rank avg (pred): 0.425 +- 0.294
mrr vals (pred, true): 0.056, 0.036
batch losses (mrrl, rdl): 0.0004039955, 3.13684e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 233
rank avg (pred): 0.401 +- 0.280
mrr vals (pred, true): 0.062, 0.051
batch losses (mrrl, rdl): 0.0015542081, 2.02954e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 334
rank avg (pred): 0.420 +- 0.280
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.0001864103, 7.8045e-06

Epoch over!
epoch time: 15.25

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1047
rank avg (pred): 0.440 +- 0.283
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 4.47412e-05, 5.353e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 541
rank avg (pred): 0.435 +- 0.273
mrr vals (pred, true): 0.046, 0.024
batch losses (mrrl, rdl): 0.0001267678, 0.0001691908

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1208
rank avg (pred): 0.417 +- 0.279
mrr vals (pred, true): 0.061, 0.049
batch losses (mrrl, rdl): 0.0012763556, 8.4221e-06

Epoch over!
epoch time: 15.162

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 275
rank avg (pred): 0.172 +- 0.203
mrr vals (pred, true): 0.255, 0.264
batch losses (mrrl, rdl): 0.0007706751, 1.49834e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1102
rank avg (pred): 0.456 +- 0.268
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0002308955, 7.7014e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 724
rank avg (pred): 0.409 +- 0.250
mrr vals (pred, true): 0.065, 0.044
batch losses (mrrl, rdl): 0.0021169959, 1.45676e-05

Epoch over!
epoch time: 15.065

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 551
rank avg (pred): 0.458 +- 0.273
mrr vals (pred, true): 0.047, 0.025
batch losses (mrrl, rdl): 9.12294e-05, 6.44458e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 569
rank avg (pred): 0.450 +- 0.261
mrr vals (pred, true): 0.048, 0.038
batch losses (mrrl, rdl): 5.74956e-05, 5.332e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 584
rank avg (pred): 0.449 +- 0.265
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 2.8729e-05, 8.63e-07

Epoch over!
epoch time: 15.216

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 969
rank avg (pred): 0.448 +- 0.265
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 6.05722e-05, 1.7039e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1193
rank avg (pred): 0.442 +- 0.250
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 4.83145e-05, 3.0582e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 961
rank avg (pred): 0.427 +- 0.253
mrr vals (pred, true): 0.059, 0.045
batch losses (mrrl, rdl): 0.0007543363, 4.7207e-06

Epoch over!
epoch time: 15.283

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 317
rank avg (pred): 0.178 +- 0.168
mrr vals (pred, true): 0.236, 0.186
batch losses (mrrl, rdl): 0.0245110895, 5.2571e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 600
rank avg (pred): 0.474 +- 0.255
mrr vals (pred, true): 0.039, 0.037
batch losses (mrrl, rdl): 0.0013162657, 4.4587e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 985
rank avg (pred): 0.155 +- 0.169
mrr vals (pred, true): 0.292, 0.285
batch losses (mrrl, rdl): 0.0005005494, 2.7043e-06

Epoch over!
epoch time: 15.295

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 229
rank avg (pred): 0.459 +- 0.271
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 8.71625e-05, 1.16356e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 433
rank avg (pred): 0.444 +- 0.253
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 4.1423e-06, 6.157e-07

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 589
rank avg (pred): 0.473 +- 0.270
mrr vals (pred, true): 0.043, 0.040
batch losses (mrrl, rdl): 0.0005495108, 3.9029e-06

Epoch over!
epoch time: 15.287

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 139
rank avg (pred): 0.468 +- 0.260
mrr vals (pred, true): 0.040, 0.044
batch losses (mrrl, rdl): 0.001003026, 5.5152e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 244
rank avg (pred): 0.214 +- 0.191
mrr vals (pred, true): 0.201, 0.272
batch losses (mrrl, rdl): 0.0513039827, 0.000161229

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 911
rank avg (pred): 0.522 +- 0.284
mrr vals (pred, true): 0.045, 0.020
batch losses (mrrl, rdl): 0.0002582915, 6.40698e-05

Epoch over!
epoch time: 15.128

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 488
rank avg (pred): 0.400 +- 0.229
mrr vals (pred, true): 0.064, 0.026
batch losses (mrrl, rdl): 0.0018943595, 0.0002047685

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 55
rank avg (pred): 0.210 +- 0.170
mrr vals (pred, true): 0.221, 0.247
batch losses (mrrl, rdl): 0.0067237662, 7.40481e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 549
rank avg (pred): 0.473 +- 0.267
mrr vals (pred, true): 0.053, 0.023
batch losses (mrrl, rdl): 8.28125e-05, 3.25178e-05

Epoch over!
epoch time: 15.254

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.486 +- 0.280
mrr vals (pred, true): 0.042, 0.036

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03818 	 0.02004 	 ~...
    1 	     1 	 0.03906 	 0.02072 	 ~...
    8 	     2 	 0.04166 	 0.02094 	 ~...
    2 	     3 	 0.03936 	 0.02154 	 ~...
   30 	     4 	 0.04356 	 0.02165 	 ~...
   14 	     5 	 0.04239 	 0.02234 	 ~...
   12 	     6 	 0.04197 	 0.02342 	 ~...
   39 	     7 	 0.04408 	 0.02350 	 ~...
   45 	     8 	 0.04458 	 0.02441 	 ~...
    5 	     9 	 0.04085 	 0.02479 	 ~...
   35 	    10 	 0.04385 	 0.02497 	 ~...
   52 	    11 	 0.04519 	 0.02539 	 ~...
   52 	    12 	 0.04519 	 0.02626 	 ~...
   27 	    13 	 0.04333 	 0.02758 	 ~...
    7 	    14 	 0.04149 	 0.02912 	 ~...
   52 	    15 	 0.04519 	 0.03137 	 ~...
   20 	    16 	 0.04290 	 0.03561 	 ~...
   10 	    17 	 0.04175 	 0.03611 	 ~...
   26 	    18 	 0.04329 	 0.03647 	 ~...
    3 	    19 	 0.03957 	 0.03715 	 ~...
   19 	    20 	 0.04288 	 0.03749 	 ~...
   18 	    21 	 0.04288 	 0.03829 	 ~...
   52 	    22 	 0.04519 	 0.03884 	 ~...
   52 	    23 	 0.04519 	 0.03957 	 ~...
   11 	    24 	 0.04189 	 0.04009 	 ~...
   86 	    25 	 0.04543 	 0.04080 	 ~...
   13 	    26 	 0.04223 	 0.04084 	 ~...
   42 	    27 	 0.04431 	 0.04098 	 ~...
   46 	    28 	 0.04481 	 0.04101 	 ~...
   52 	    29 	 0.04519 	 0.04164 	 ~...
   52 	    30 	 0.04519 	 0.04165 	 ~...
   52 	    31 	 0.04519 	 0.04190 	 ~...
   24 	    32 	 0.04324 	 0.04238 	 ~...
   52 	    33 	 0.04519 	 0.04261 	 ~...
   52 	    34 	 0.04519 	 0.04336 	 ~...
   51 	    35 	 0.04501 	 0.04357 	 ~...
   21 	    36 	 0.04308 	 0.04378 	 ~...
   43 	    37 	 0.04433 	 0.04381 	 ~...
   52 	    38 	 0.04519 	 0.04396 	 ~...
   22 	    39 	 0.04311 	 0.04419 	 ~...
   52 	    40 	 0.04519 	 0.04448 	 ~...
   34 	    41 	 0.04377 	 0.04463 	 ~...
   44 	    42 	 0.04453 	 0.04515 	 ~...
   52 	    43 	 0.04519 	 0.04559 	 ~...
   33 	    44 	 0.04370 	 0.04567 	 ~...
   29 	    45 	 0.04351 	 0.04570 	 ~...
    4 	    46 	 0.03966 	 0.04586 	 ~...
   31 	    47 	 0.04367 	 0.04594 	 ~...
   52 	    48 	 0.04519 	 0.04608 	 ~...
   40 	    49 	 0.04430 	 0.04642 	 ~...
   52 	    50 	 0.04519 	 0.04677 	 ~...
   47 	    51 	 0.04482 	 0.04687 	 ~...
    6 	    52 	 0.04121 	 0.04692 	 ~...
   52 	    53 	 0.04519 	 0.04696 	 ~...
   52 	    54 	 0.04519 	 0.04712 	 ~...
   52 	    55 	 0.04519 	 0.04715 	 ~...
   52 	    56 	 0.04519 	 0.04736 	 ~...
   38 	    57 	 0.04401 	 0.04775 	 ~...
   16 	    58 	 0.04270 	 0.04777 	 ~...
   49 	    59 	 0.04495 	 0.04778 	 ~...
   52 	    60 	 0.04519 	 0.04793 	 ~...
   52 	    61 	 0.04519 	 0.04815 	 ~...
    9 	    62 	 0.04170 	 0.04816 	 ~...
   17 	    63 	 0.04278 	 0.04869 	 ~...
   52 	    64 	 0.04519 	 0.04898 	 ~...
   52 	    65 	 0.04519 	 0.04907 	 ~...
   52 	    66 	 0.04519 	 0.04916 	 ~...
   50 	    67 	 0.04495 	 0.04928 	 ~...
   52 	    68 	 0.04519 	 0.04986 	 ~...
   52 	    69 	 0.04519 	 0.04989 	 ~...
   15 	    70 	 0.04267 	 0.05037 	 ~...
   52 	    71 	 0.04519 	 0.05051 	 ~...
   23 	    72 	 0.04320 	 0.05115 	 ~...
   52 	    73 	 0.04519 	 0.05159 	 ~...
   41 	    74 	 0.04431 	 0.05184 	 ~...
   52 	    75 	 0.04519 	 0.05231 	 ~...
   52 	    76 	 0.04519 	 0.05232 	 ~...
   32 	    77 	 0.04369 	 0.05275 	 ~...
   25 	    78 	 0.04328 	 0.05358 	 ~...
   87 	    79 	 0.04677 	 0.05379 	 ~...
   28 	    80 	 0.04343 	 0.05385 	 ~...
   52 	    81 	 0.04519 	 0.05466 	 ~...
   52 	    82 	 0.04519 	 0.05498 	 ~...
   37 	    83 	 0.04395 	 0.05527 	 ~...
   36 	    84 	 0.04389 	 0.05530 	 ~...
   52 	    85 	 0.04519 	 0.05654 	 ~...
   48 	    86 	 0.04491 	 0.05887 	 ~...
   52 	    87 	 0.04519 	 0.05937 	 ~...
   90 	    88 	 0.18978 	 0.18803 	 ~...
   91 	    89 	 0.19445 	 0.18840 	 ~...
  101 	    90 	 0.22705 	 0.19986 	 ~...
   92 	    91 	 0.20476 	 0.20119 	 ~...
   89 	    92 	 0.18530 	 0.20350 	 ~...
   93 	    93 	 0.20895 	 0.20836 	 ~...
   96 	    94 	 0.21373 	 0.20936 	 ~...
   95 	    95 	 0.21275 	 0.21393 	 ~...
   88 	    96 	 0.17381 	 0.21695 	 m..s
   98 	    97 	 0.22547 	 0.21723 	 ~...
  103 	    98 	 0.23696 	 0.21940 	 ~...
   94 	    99 	 0.21146 	 0.21956 	 ~...
  100 	   100 	 0.22691 	 0.22505 	 ~...
   97 	   101 	 0.22335 	 0.23015 	 ~...
  105 	   102 	 0.26709 	 0.24222 	 ~...
  102 	   103 	 0.23355 	 0.24271 	 ~...
   99 	   104 	 0.22569 	 0.24394 	 ~...
  113 	   105 	 0.28425 	 0.25916 	 ~...
  107 	   106 	 0.27182 	 0.27282 	 ~...
  111 	   107 	 0.28202 	 0.27339 	 ~...
  112 	   108 	 0.28295 	 0.27672 	 ~...
  114 	   109 	 0.28694 	 0.27773 	 ~...
  106 	   110 	 0.26844 	 0.28160 	 ~...
  110 	   111 	 0.28195 	 0.28248 	 ~...
  104 	   112 	 0.26664 	 0.28449 	 ~...
  115 	   113 	 0.28749 	 0.28722 	 ~...
  109 	   114 	 0.28099 	 0.28862 	 ~...
  108 	   115 	 0.27628 	 0.29484 	 ~...
  117 	   116 	 0.31243 	 0.31480 	 ~...
  118 	   117 	 0.31303 	 0.32376 	 ~...
  116 	   118 	 0.31195 	 0.32827 	 ~...
  119 	   119 	 0.31325 	 0.33593 	 ~...
  120 	   120 	 0.37151 	 0.53924 	 MISS
==========================================
r_mrr = 0.9844697713851929
r2_mrr = 0.9664549231529236
spearmanr_mrr@5 = 0.9981346726417542
spearmanr_mrr@10 = 0.9585433602333069
spearmanr_mrr@50 = 0.9768823385238647
spearmanr_mrr@100 = 0.9874249696731567
spearmanr_mrr@All = 0.9871182441711426
==========================================
test time: 0.462
Done Testing dataset UMLS
total time taken: 234.20905089378357
training time taken: 227.93697953224182
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9845)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9665)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9981)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9585)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9769)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9874)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9871)}}, 'test_loss': {'ComplEx': {'UMLS': 0.3943483642924548}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 1044488866231566
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1045, 996, 752, 477, 1061, 127, 145, 324, 868, 1004, 980, 282, 737, 1054, 670, 596, 1204, 555, 1034, 807, 566, 1105, 161, 574, 901, 428, 172, 8, 887, 260, 736, 358, 496, 768, 421, 657, 1002, 668, 157, 1026, 683, 564, 252, 875, 588, 397, 415, 95, 21, 794, 1152, 375, 822, 296, 18, 32, 389, 528, 4, 1166, 1062, 678, 224, 616, 539, 398, 171, 193, 823, 101, 480, 602, 898, 207, 427, 1114, 653, 303, 766, 1146, 1209, 640, 939, 56, 895, 677, 94, 99, 442, 1214, 1, 867, 618, 824, 1174, 356, 937, 318, 917, 9, 724, 628, 655, 1092, 904, 634, 612, 785, 133, 658, 239, 1135, 847, 940, 832, 580, 543, 636, 842, 723, 601]
valid_ids (0): []
train_ids (1094): [347, 363, 294, 877, 33, 355, 1097, 827, 242, 423, 1190, 290, 212, 110, 385, 926, 744, 31, 135, 142, 43, 1048, 283, 26, 447, 335, 492, 75, 179, 124, 441, 302, 816, 746, 1158, 188, 341, 617, 1159, 745, 406, 367, 988, 629, 1212, 525, 701, 764, 35, 295, 1084, 976, 568, 696, 1155, 586, 931, 989, 160, 413, 440, 198, 219, 1180, 865, 792, 644, 565, 394, 638, 719, 675, 818, 153, 422, 776, 372, 1094, 647, 469, 512, 377, 154, 278, 29, 460, 246, 905, 569, 292, 424, 689, 864, 797, 250, 273, 203, 1178, 165, 945, 47, 221, 495, 1169, 206, 825, 1070, 790, 16, 1038, 1142, 84, 177, 620, 15, 238, 520, 393, 609, 591, 134, 891, 614, 882, 479, 92, 382, 1203, 481, 370, 1027, 1138, 1102, 626, 829, 914, 30, 1015, 1132, 264, 1069, 557, 80, 1201, 848, 1085, 409, 130, 400, 274, 879, 83, 763, 269, 184, 881, 187, 468, 606, 354, 772, 1023, 12, 942, 115, 3, 656, 838, 1012, 299, 116, 201, 1091, 787, 774, 1161, 987, 364, 28, 232, 540, 930, 906, 654, 690, 458, 464, 851, 146, 831, 1068, 953, 225, 777, 288, 353, 78, 652, 418, 811, 575, 208, 907, 50, 77, 1148, 245, 861, 622, 54, 156, 51, 25, 671, 333, 1103, 513, 563, 809, 1000, 1183, 1195, 204, 1107, 36, 584, 1167, 799, 974, 401, 682, 371, 781, 497, 680, 106, 476, 1003, 1189, 840, 552, 708, 39, 532, 1181, 137, 1087, 465, 44, 1207, 470, 681, 843, 272, 1039, 268, 973, 812, 326, 784, 503, 1050, 1080, 852, 969, 359, 1041, 151, 241, 317, 615, 711, 649, 494, 585, 804, 265, 873, 510, 722, 605, 700, 6, 404, 1144, 1006, 795, 839, 337, 491, 801, 1131, 1020, 573, 925, 14, 780, 1013, 841, 1210, 360, 1205, 600, 408, 506, 863, 651, 966, 1150, 1188, 769, 985, 498, 410, 381, 1018, 211, 734, 862, 633, 1024, 941, 709, 79, 163, 920, 718, 185, 61, 886, 261, 1196, 248, 523, 1047, 558, 10, 1119, 1074, 548, 453, 791, 511, 924, 170, 1170, 577, 1078, 820, 998, 439, 277, 527, 489, 22, 287, 1162, 1049, 1184, 280, 817, 448, 553, 169, 88, 1096, 338, 733, 1056, 771, 305, 1077, 1194, 844, 182, 915, 125, 1008, 281, 541, 738, 392, 673, 872, 289, 55, 155, 762, 68, 627, 461, 478, 661, 484, 900, 902, 805, 1187, 229, 107, 666, 1206, 630, 183, 175, 200, 674, 226, 316, 1057, 405, 1083, 1089, 70, 1106, 1186, 603, 703, 1125, 1136, 632, 102, 452, 454, 304, 432, 459, 425, 921, 343, 908, 991, 625, 725, 685, 810, 395, 71, 1176, 971, 223, 387, 166, 149, 132, 645, 164, 910, 430, 128, 978, 414, 173, 1192, 1033, 800, 378, 159, 451, 582, 1028, 730, 209, 63, 195, 1075, 443, 1177, 147, 830, 803, 234, 814, 118, 947, 715, 748, 1011, 150, 1073, 399, 899, 779, 849, 581, 19, 275, 467, 213, 714, 913, 312, 589, 379, 554, 485, 117, 1182, 687, 1055, 426, 1072, 802, 643, 202, 300, 707, 216, 950, 948, 648, 97, 136, 667, 366, 598, 191, 883, 89, 753, 619, 858, 694, 73, 328, 49, 1154, 435, 693, 1172, 501, 309, 1160, 48, 108, 631, 610, 728, 1139, 58, 487, 60, 253, 1213, 1115, 438, 1140, 949, 1052, 521, 716, 1137, 897, 1151, 433, 702, 332, 981, 144, 650, 7, 933, 1199, 514, 505, 1021, 977, 1036, 217, 0, 1198, 1208, 967, 912, 621, 306, 123, 104, 1014, 559, 999, 1164, 1168, 502, 594, 982, 607, 1126, 662, 551, 320, 826, 599, 853, 37, 416, 313, 257, 168, 798, 271, 876, 919, 932, 972, 293, 383, 1043, 1143, 240, 178, 420, 1095, 517, 53, 534, 1082, 1079, 436, 1202, 1127, 1040, 330, 1029, 608, 138, 340, 105, 986, 1122, 466, 391, 1112, 970, 993, 922, 373, 754, 199, 263, 390, 402, 819, 344, 374, 361, 41, 778, 595, 1076, 870, 297, 1156, 994, 957, 279, 325, 562, 833, 960, 255, 936, 793, 522, 815, 903, 1053, 749, 888, 775, 943, 231, 916, 500, 1117, 1134, 958, 93, 944, 96, 315, 531, 515, 1120, 592, 992, 65, 834, 646, 866, 46, 604, 751, 59, 66, 1153, 2, 42, 230, 82, 270, 90, 721, 894, 664, 856, 859, 549, 729, 334, 176, 896, 91, 788, 742, 783, 327, 1065, 663, 561, 686, 13, 869, 141, 597, 975, 342, 1108, 113, 542, 1086, 533, 109, 419, 688, 57, 961, 11, 1060, 1130, 412, 1171, 720, 1116, 345, 483, 194, 556, 1200, 472, 590, 880, 307, 417, 997, 923, 1110, 139, 1059, 119, 362, 570, 482, 946, 86, 535, 301, 488, 965, 298, 938, 699, 210, 1175, 786, 45, 1191, 267, 773, 676, 52, 1064, 34, 835, 504, 72, 756, 310, 1007, 254, 747, 291, 111, 1104, 544, 1147, 437, 695, 1017, 5, 205, 403, 1037, 350, 81, 131, 152, 1001, 143, 368, 782, 929, 739, 457, 836, 259, 1128, 486, 1051, 761, 235, 1044, 576, 311, 1113, 1099, 321, 509, 243, 962, 1179, 471, 20, 1118, 572, 963, 64, 190, 351, 74, 526, 329, 1046, 679, 38, 1123, 704, 984, 705, 1025, 1185, 613, 623, 755, 684, 323, 434, 806, 1032, 537, 918, 956, 357, 909, 507, 1173, 1081, 174, 1009, 692, 979, 186, 384, 571, 431, 770, 928, 284, 1163, 697, 256, 1031, 854, 456, 710, 624, 251, 1035, 1121, 247, 98, 473, 122, 346, 1005, 837, 499, 121, 611, 796, 855, 935, 158, 757, 740, 1124, 1109, 369, 536, 884, 927, 964, 735, 579, 322, 518, 743, 713, 197, 845, 545, 24, 336, 103, 237, 1090, 878, 717, 114, 23, 990, 821, 1098, 893, 760, 758, 712, 167, 560, 181, 641, 1157, 860, 892, 983, 808, 429, 1016, 706, 1030, 493, 550, 215, 529, 1100, 1042, 727, 214, 228, 446, 635, 463, 857, 349, 189, 750, 593, 959, 233, 954, 1145, 1022, 1071, 17, 1129, 126, 192, 995, 871, 1101, 911, 27, 885, 444, 474, 388, 1093, 352, 726, 266, 731, 87, 530, 951, 741, 286, 1193, 218, 450, 1063, 516, 850, 789, 220, 376, 129, 660, 462, 319, 828, 546, 583, 698, 547, 538, 180, 767, 1067, 40, 952, 1088, 85, 227, 258, 249, 339, 196, 759, 455, 691, 813, 519, 162, 1058, 244, 732, 968, 148, 1133, 407, 411, 934, 380, 765, 672, 639, 314, 490, 348, 236, 120, 112, 140, 1149, 396, 1019, 331, 308, 874, 222, 1211, 642, 846, 100, 445, 1111, 955, 67, 69, 524, 76, 669, 665, 889, 365, 1197, 578, 637, 1141, 890, 62, 386, 1165, 587, 449, 475, 262, 1066, 285, 276, 1010, 508, 567, 659]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1104067080804520
the save name prefix for this run is:  chkpt-ID_1104067080804520_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 443
rank avg (pred): 0.631 +- 0.004
mrr vals (pred, true): 0.012, 0.048
batch losses (mrrl, rdl): 0.0, 0.0009186367

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 689
rank avg (pred): 0.440 +- 0.236
mrr vals (pred, true): 0.121, 0.055
batch losses (mrrl, rdl): 0.0, 1.74671e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 370
rank avg (pred): 0.434 +- 0.264
mrr vals (pred, true): 0.142, 0.056
batch losses (mrrl, rdl): 0.0, 2.42184e-05

Epoch over!
epoch time: 15.138

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 108
rank avg (pred): 0.409 +- 0.258
mrr vals (pred, true): 0.150, 0.043
batch losses (mrrl, rdl): 0.0, 1.42038e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 504
rank avg (pred): 0.479 +- 0.264
mrr vals (pred, true): 0.113, 0.025
batch losses (mrrl, rdl): 0.0, 1.61819e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 763
rank avg (pred): 0.425 +- 0.264
mrr vals (pred, true): 0.106, 0.040
batch losses (mrrl, rdl): 0.0, 3.4789e-06

Epoch over!
epoch time: 15.572

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 199
rank avg (pred): 0.427 +- 0.264
mrr vals (pred, true): 0.104, 0.045
batch losses (mrrl, rdl): 0.0, 3.5105e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 873
rank avg (pred): 0.450 +- 0.247
mrr vals (pred, true): 0.056, 0.056
batch losses (mrrl, rdl): 0.0, 7.9016e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 294
rank avg (pred): 0.185 +- 0.206
mrr vals (pred, true): 0.180, 0.192
batch losses (mrrl, rdl): 0.0, 8.757e-07

Epoch over!
epoch time: 14.956

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1181
rank avg (pred): 0.452 +- 0.268
mrr vals (pred, true): 0.067, 0.038
batch losses (mrrl, rdl): 0.0, 1.37443e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 327
rank avg (pred): 0.410 +- 0.268
mrr vals (pred, true): 0.073, 0.053
batch losses (mrrl, rdl): 0.0, 5.7161e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 967
rank avg (pred): 0.440 +- 0.251
mrr vals (pred, true): 0.063, 0.047
batch losses (mrrl, rdl): 0.0, 1.1171e-06

Epoch over!
epoch time: 14.92

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1058
rank avg (pred): 0.151 +- 0.194
mrr vals (pred, true): 0.190, 0.291
batch losses (mrrl, rdl): 0.0, 9.8325e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 108
rank avg (pred): 0.433 +- 0.265
mrr vals (pred, true): 0.059, 0.043
batch losses (mrrl, rdl): 0.0, 5.4836e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 374
rank avg (pred): 0.459 +- 0.261
mrr vals (pred, true): 0.041, 0.045
batch losses (mrrl, rdl): 0.0, 5.8529e-06

Epoch over!
epoch time: 15.129

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 120
rank avg (pred): 0.441 +- 0.262
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002574323, 7.388e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 339
rank avg (pred): 0.484 +- 0.274
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 2.52197e-05, 6.42161e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 380
rank avg (pred): 0.492 +- 0.303
mrr vals (pred, true): 0.058, 0.047
batch losses (mrrl, rdl): 0.0006339705, 2.84428e-05

Epoch over!
epoch time: 15.274

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 789
rank avg (pred): 0.492 +- 0.276
mrr vals (pred, true): 0.044, 0.049
batch losses (mrrl, rdl): 0.0003495446, 2.07439e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 256
rank avg (pred): 0.185 +- 0.280
mrr vals (pred, true): 0.233, 0.195
batch losses (mrrl, rdl): 0.0143015152, 2.13725e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 208
rank avg (pred): 0.466 +- 0.245
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 4.24154e-05, 2.47349e-05

Epoch over!
epoch time: 15.114

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 429
rank avg (pred): 0.462 +- 0.241
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 3.18794e-05, 2.45683e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 514
rank avg (pred): 0.480 +- 0.230
mrr vals (pred, true): 0.044, 0.024
batch losses (mrrl, rdl): 0.0004080875, 2.49147e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1044
rank avg (pred): 0.463 +- 0.255
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 0.000118377, 1.82353e-05

Epoch over!
epoch time: 15.129

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 19
rank avg (pred): 0.255 +- 0.308
mrr vals (pred, true): 0.187, 0.204
batch losses (mrrl, rdl): 0.0028145954, 0.0001198868

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 510
rank avg (pred): 0.499 +- 0.275
mrr vals (pred, true): 0.048, 0.028
batch losses (mrrl, rdl): 3.97976e-05, 1.3677e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 220
rank avg (pred): 0.430 +- 0.213
mrr vals (pred, true): 0.054, 0.048
batch losses (mrrl, rdl): 0.0001791065, 1.11369e-05

Epoch over!
epoch time: 15.159

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 130
rank avg (pred): 0.425 +- 0.202
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.39181e-05, 1.40938e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1124
rank avg (pred): 0.401 +- 0.216
mrr vals (pred, true): 0.073, 0.053
batch losses (mrrl, rdl): 0.0053063566, 2.10419e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 466
rank avg (pred): 0.437 +- 0.207
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.0036e-06, 1.02746e-05

Epoch over!
epoch time: 15.118

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 262
rank avg (pred): 0.264 +- 0.283
mrr vals (pred, true): 0.181, 0.222
batch losses (mrrl, rdl): 0.0168133061, 0.000149442

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1127
rank avg (pred): 0.435 +- 0.196
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 7.05476e-05, 1.77508e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 899
rank avg (pred): 0.461 +- 0.220
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 1.0656e-05, 0.0001409561

Epoch over!
epoch time: 15.134

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 611
rank avg (pred): 0.441 +- 0.216
mrr vals (pred, true): 0.055, 0.041
batch losses (mrrl, rdl): 0.0002085957, 1.02366e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 872
rank avg (pred): 0.423 +- 0.185
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 2.7057e-06, 3.4791e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 48
rank avg (pred): 0.243 +- 0.257
mrr vals (pred, true): 0.184, 0.208
batch losses (mrrl, rdl): 0.0057783555, 6.984e-05

Epoch over!
epoch time: 15.285

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 770
rank avg (pred): 0.429 +- 0.188
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 5.88102e-05, 2.67833e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1097
rank avg (pred): 0.419 +- 0.194
mrr vals (pred, true): 0.057, 0.040
batch losses (mrrl, rdl): 0.0004836236, 3.35455e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 659
rank avg (pred): 0.410 +- 0.187
mrr vals (pred, true): 0.061, 0.048
batch losses (mrrl, rdl): 0.001147912, 3.82923e-05

Epoch over!
epoch time: 15.085

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1182
rank avg (pred): 0.429 +- 0.176
mrr vals (pred, true): 0.048, 0.034
batch losses (mrrl, rdl): 2.6796e-05, 1.98059e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 820
rank avg (pred): 0.189 +- 0.227
mrr vals (pred, true): 0.314, 0.334
batch losses (mrrl, rdl): 0.0040632784, 0.0001786962

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 667
rank avg (pred): 0.431 +- 0.181
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 2.12473e-05, 2.66589e-05

Epoch over!
epoch time: 15.221

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1050
rank avg (pred): 0.414 +- 0.180
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001111536, 2.21801e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 672
rank avg (pred): 0.429 +- 0.173
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.0001275193, 2.85916e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 426
rank avg (pred): 0.427 +- 0.170
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 6.13487e-05, 3.27525e-05

Epoch over!
epoch time: 15.077

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.429 +- 0.162
mrr vals (pred, true): 0.044, 0.044

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03914 	 0.01926 	 ~...
   79 	     1 	 0.05028 	 0.02036 	 ~...
   94 	     2 	 0.06625 	 0.02102 	 m..s
   73 	     3 	 0.04868 	 0.02204 	 ~...
   32 	     4 	 0.04477 	 0.02283 	 ~...
   58 	     5 	 0.04643 	 0.02304 	 ~...
   28 	     6 	 0.04448 	 0.02342 	 ~...
   17 	     7 	 0.04355 	 0.02370 	 ~...
   45 	     8 	 0.04550 	 0.02406 	 ~...
   71 	     9 	 0.04860 	 0.02442 	 ~...
   72 	    10 	 0.04866 	 0.02459 	 ~...
   81 	    11 	 0.05472 	 0.02467 	 m..s
   92 	    12 	 0.05826 	 0.02475 	 m..s
   33 	    13 	 0.04484 	 0.02554 	 ~...
   50 	    14 	 0.04600 	 0.02933 	 ~...
    5 	    15 	 0.04050 	 0.03159 	 ~...
   51 	    16 	 0.04620 	 0.03405 	 ~...
   14 	    17 	 0.04275 	 0.03440 	 ~...
   66 	    18 	 0.04719 	 0.03544 	 ~...
    3 	    19 	 0.04035 	 0.03629 	 ~...
   40 	    20 	 0.04531 	 0.03631 	 ~...
   42 	    21 	 0.04540 	 0.03647 	 ~...
    7 	    22 	 0.04076 	 0.03663 	 ~...
   31 	    23 	 0.04472 	 0.03749 	 ~...
   54 	    24 	 0.04628 	 0.03838 	 ~...
   53 	    25 	 0.04627 	 0.03875 	 ~...
   65 	    26 	 0.04714 	 0.03942 	 ~...
   77 	    27 	 0.04922 	 0.03946 	 ~...
    6 	    28 	 0.04075 	 0.03960 	 ~...
   76 	    29 	 0.04901 	 0.04032 	 ~...
   48 	    30 	 0.04587 	 0.04053 	 ~...
   15 	    31 	 0.04319 	 0.04061 	 ~...
   37 	    32 	 0.04507 	 0.04081 	 ~...
   41 	    33 	 0.04534 	 0.04109 	 ~...
   67 	    34 	 0.04729 	 0.04115 	 ~...
   34 	    35 	 0.04491 	 0.04164 	 ~...
   56 	    36 	 0.04635 	 0.04168 	 ~...
   10 	    37 	 0.04165 	 0.04174 	 ~...
   22 	    38 	 0.04409 	 0.04177 	 ~...
   89 	    39 	 0.05764 	 0.04224 	 ~...
   49 	    40 	 0.04597 	 0.04241 	 ~...
   57 	    41 	 0.04641 	 0.04270 	 ~...
   38 	    42 	 0.04525 	 0.04308 	 ~...
   24 	    43 	 0.04417 	 0.04315 	 ~...
    9 	    44 	 0.04164 	 0.04393 	 ~...
   23 	    45 	 0.04409 	 0.04404 	 ~...
   29 	    46 	 0.04461 	 0.04431 	 ~...
   70 	    47 	 0.04813 	 0.04446 	 ~...
   52 	    48 	 0.04624 	 0.04450 	 ~...
   27 	    49 	 0.04445 	 0.04466 	 ~...
   21 	    50 	 0.04397 	 0.04471 	 ~...
   69 	    51 	 0.04797 	 0.04474 	 ~...
   16 	    52 	 0.04339 	 0.04505 	 ~...
   35 	    53 	 0.04503 	 0.04518 	 ~...
   82 	    54 	 0.05606 	 0.04551 	 ~...
   43 	    55 	 0.04542 	 0.04609 	 ~...
   91 	    56 	 0.05804 	 0.04613 	 ~...
   93 	    57 	 0.05871 	 0.04618 	 ~...
   90 	    58 	 0.05772 	 0.04635 	 ~...
   46 	    59 	 0.04554 	 0.04640 	 ~...
   88 	    60 	 0.05730 	 0.04695 	 ~...
   60 	    61 	 0.04691 	 0.04696 	 ~...
   87 	    62 	 0.05670 	 0.04696 	 ~...
    8 	    63 	 0.04114 	 0.04701 	 ~...
   63 	    64 	 0.04701 	 0.04706 	 ~...
    2 	    65 	 0.04002 	 0.04711 	 ~...
   12 	    66 	 0.04246 	 0.04712 	 ~...
   13 	    67 	 0.04250 	 0.04749 	 ~...
   20 	    68 	 0.04382 	 0.04755 	 ~...
   26 	    69 	 0.04426 	 0.04757 	 ~...
   74 	    70 	 0.04871 	 0.04793 	 ~...
   61 	    71 	 0.04698 	 0.04843 	 ~...
   30 	    72 	 0.04465 	 0.04871 	 ~...
   59 	    73 	 0.04659 	 0.04891 	 ~...
   85 	    74 	 0.05625 	 0.04894 	 ~...
   86 	    75 	 0.05645 	 0.04904 	 ~...
   36 	    76 	 0.04505 	 0.04921 	 ~...
    1 	    77 	 0.03943 	 0.04958 	 ~...
   19 	    78 	 0.04374 	 0.04971 	 ~...
   11 	    79 	 0.04218 	 0.04989 	 ~...
    4 	    80 	 0.04038 	 0.05016 	 ~...
   84 	    81 	 0.05616 	 0.05033 	 ~...
   47 	    82 	 0.04579 	 0.05051 	 ~...
   18 	    83 	 0.04359 	 0.05178 	 ~...
   25 	    84 	 0.04419 	 0.05189 	 ~...
   78 	    85 	 0.04983 	 0.05206 	 ~...
   83 	    86 	 0.05608 	 0.05213 	 ~...
   62 	    87 	 0.04699 	 0.05275 	 ~...
   44 	    88 	 0.04543 	 0.05299 	 ~...
   64 	    89 	 0.04714 	 0.05355 	 ~...
   39 	    90 	 0.04526 	 0.05358 	 ~...
   55 	    91 	 0.04633 	 0.05374 	 ~...
   68 	    92 	 0.04765 	 0.05524 	 ~...
   75 	    93 	 0.04883 	 0.06019 	 ~...
   80 	    94 	 0.05452 	 0.06132 	 ~...
   98 	    95 	 0.18451 	 0.18366 	 ~...
  103 	    96 	 0.23033 	 0.19336 	 m..s
  100 	    97 	 0.22028 	 0.19455 	 ~...
   96 	    98 	 0.18170 	 0.20146 	 ~...
   97 	    99 	 0.18228 	 0.20688 	 ~...
   95 	   100 	 0.17839 	 0.21474 	 m..s
  102 	   101 	 0.22810 	 0.21610 	 ~...
   99 	   102 	 0.21782 	 0.21805 	 ~...
  108 	   103 	 0.24696 	 0.23182 	 ~...
  106 	   104 	 0.23931 	 0.23290 	 ~...
  101 	   105 	 0.22043 	 0.23554 	 ~...
  104 	   106 	 0.23056 	 0.24107 	 ~...
  105 	   107 	 0.23232 	 0.24515 	 ~...
  109 	   108 	 0.24928 	 0.26449 	 ~...
  112 	   109 	 0.28427 	 0.26624 	 ~...
  107 	   110 	 0.24689 	 0.27339 	 ~...
  115 	   111 	 0.31479 	 0.27441 	 m..s
  110 	   112 	 0.27016 	 0.27479 	 ~...
  113 	   113 	 0.30693 	 0.28449 	 ~...
  120 	   114 	 0.34811 	 0.29247 	 m..s
  119 	   115 	 0.34050 	 0.29334 	 m..s
  111 	   116 	 0.27991 	 0.29550 	 ~...
  114 	   117 	 0.31086 	 0.30203 	 ~...
  116 	   118 	 0.31651 	 0.30223 	 ~...
  118 	   119 	 0.32012 	 0.30732 	 ~...
  117 	   120 	 0.31777 	 0.32107 	 ~...
==========================================
r_mrr = 0.9869747757911682
r2_mrr = 0.9702934622764587
spearmanr_mrr@5 = 0.9102256298065186
spearmanr_mrr@10 = 0.9692471623420715
spearmanr_mrr@50 = 0.9958819150924683
spearmanr_mrr@100 = 0.9971950650215149
spearmanr_mrr@All = 0.9956700205802917
==========================================
test time: 0.455
Done Testing dataset UMLS
total time taken: 234.1148281097412
training time taken: 227.77503371238708
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9870)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9703)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9102)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9692)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9959)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9972)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9957)}}, 'test_loss': {'ComplEx': {'UMLS': 0.1878272514732089}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 5515221797571992
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 1195, 820, 194, 683, 655, 831, 348, 356, 508, 1133, 582, 1169, 622, 120, 1101, 896, 1082, 430, 772, 454, 293, 521, 956, 231, 766, 647, 28, 44, 424, 845, 522, 730, 800, 516, 916, 892, 681, 964, 108, 218, 1172, 871, 737, 426, 4, 302, 375, 1043, 250, 135, 621, 403, 668, 109, 489, 84, 178, 885, 1120, 285, 1129, 275, 714, 188, 307, 803, 79, 753, 506, 274, 207, 855, 542, 1033, 336, 951, 485, 1025, 312, 994, 172, 1023, 1180, 43, 477, 280, 71, 331, 480, 571, 286, 321, 527, 840, 707, 317, 507, 875, 821, 182, 1022, 5, 1107, 134, 578, 924, 487, 1051, 532, 911, 860, 967, 1058, 1056, 351, 806, 7, 333, 96, 284]
valid_ids (0): []
train_ids (1094): [1171, 446, 973, 689, 166, 648, 999, 889, 882, 1071, 510, 1158, 1007, 619, 54, 858, 193, 808, 775, 539, 591, 301, 1048, 963, 722, 920, 809, 852, 756, 596, 229, 1065, 258, 85, 1103, 1034, 607, 369, 742, 1000, 904, 138, 149, 1087, 927, 1038, 1057, 401, 651, 568, 798, 835, 501, 1191, 928, 22, 63, 287, 1003, 221, 1140, 853, 1092, 626, 179, 884, 1168, 711, 376, 1132, 303, 136, 822, 765, 863, 550, 678, 814, 311, 437, 304, 575, 1064, 799, 23, 1165, 1040, 502, 1046, 244, 982, 364, 197, 189, 415, 656, 530, 747, 124, 614, 104, 1181, 584, 491, 83, 78, 617, 366, 91, 757, 33, 776, 484, 1096, 763, 606, 988, 206, 434, 248, 278, 208, 387, 212, 338, 987, 1111, 670, 1167, 225, 93, 75, 975, 1066, 211, 719, 349, 443, 129, 1045, 167, 14, 627, 624, 433, 40, 946, 630, 205, 910, 932, 583, 77, 29, 613, 923, 886, 322, 25, 837, 386, 515, 700, 786, 830, 754, 482, 903, 725, 297, 748, 1123, 488, 517, 448, 1104, 620, 17, 92, 930, 483, 833, 352, 906, 439, 611, 1061, 127, 601, 745, 665, 922, 368, 761, 807, 750, 374, 51, 1115, 631, 1077, 520, 984, 940, 815, 240, 98, 458, 277, 131, 457, 848, 378, 1203, 524, 384, 460, 383, 65, 868, 123, 751, 654, 1173, 267, 1069, 818, 636, 535, 47, 423, 1073, 396, 1024, 417, 393, 173, 61, 196, 1075, 995, 657, 644, 1002, 664, 661, 350, 381, 1189, 525, 156, 459, 685, 435, 970, 771, 245, 645, 1012, 849, 890, 1183, 1067, 699, 891, 1068, 1137, 597, 1124, 866, 59, 537, 721, 263, 48, 152, 346, 1141, 812, 492, 153, 341, 314, 228, 603, 1032, 879, 315, 473, 1078, 168, 122, 695, 767, 1105, 888, 313, 1106, 810, 1028, 704, 11, 316, 1197, 816, 279, 395, 1146, 824, 1150, 6, 696, 623, 504, 1006, 309, 1080, 150, 604, 1042, 998, 743, 706, 1153, 1193, 705, 87, 406, 388, 839, 672, 580, 158, 431, 690, 133, 909, 478, 468, 877, 1142, 405, 300, 217, 804, 119, 382, 1109, 1052, 1049, 101, 496, 900, 455, 13, 358, 541, 1015, 659, 559, 58, 677, 861, 81, 702, 394, 829, 200, 662, 99, 214, 371, 1102, 213, 834, 26, 555, 529, 760, 540, 817, 407, 740, 534, 788, 1113, 1093, 2, 334, 996, 1059, 404, 1116, 1044, 1214, 842, 880, 602, 37, 838, 409, 979, 513, 294, 981, 345, 836, 19, 320, 1089, 52, 986, 254, 936, 941, 989, 1019, 634, 715, 653, 872, 219, 512, 997, 347, 789, 180, 183, 117, 640, 794, 397, 1074, 1047, 398, 115, 1206, 377, 1147, 961, 447, 605, 1054, 857, 86, 62, 174, 847, 107, 3, 42, 1005, 974, 232, 795, 697, 471, 633, 587, 268, 944, 1016, 558, 768, 971, 1010, 1182, 41, 676, 226, 980, 292, 171, 116, 778, 1151, 867, 373, 256, 486, 780, 1017, 733, 805, 526, 444, 306, 185, 577, 324, 908, 342, 1186, 10, 1163, 1081, 1205, 1127, 1185, 408, 164, 1070, 841, 339, 203, 576, 402, 1152, 694, 1076, 445, 1085, 56, 901, 440, 1201, 146, 637, 247, 972, 533, 1014, 581, 728, 893, 1091, 1121, 545, 118, 1021, 94, 514, 15, 660, 658, 693, 1190, 450, 9, 939, 145, 242, 1198, 111, 220, 210, 566, 1118, 227, 246, 1041, 421, 686, 625, 692, 1031, 1122, 355, 441, 27, 764, 1013, 243, 1139, 241, 49, 475, 472, 21, 142, 370, 429, 671, 1160, 1149, 716, 959, 1039, 30, 414, 960, 72, 698, 432, 16, 1148, 412, 914, 574, 1098, 418, 106, 67, 774, 912, 53, 8, 565, 599, 155, 462, 1063, 717, 1095, 36, 1145, 723, 466, 1188, 953, 420, 259, 389, 328, 380, 519, 64, 865, 162, 1174, 76, 428, 175, 663, 105, 562, 1020, 873, 144, 523, 792, 554, 870, 992, 463, 112, 103, 801, 948, 1192, 323, 593, 609, 669, 755, 641, 271, 400, 399, 549, 257, 813, 934, 856, 216, 736, 703, 628, 954, 1204, 456, 390, 379, 308, 639, 915, 202, 673, 195, 1060, 12, 360, 283, 586, 365, 585, 361, 925, 147, 758, 55, 262, 413, 80, 24, 467, 1001, 139, 913, 497, 735, 330, 598, 594, 187, 1187, 340, 427, 438, 125, 790, 887, 864, 531, 160, 1035, 362, 326, 260, 712, 600, 335, 1143, 165, 553, 281, 746, 894, 121, 1164, 966, 720, 1210, 359, 773, 159, 500, 926, 186, 230, 222, 547, 643, 588, 1166, 569, 744, 255, 132, 354, 827, 895, 236, 325, 151, 770, 363, 319, 688, 176, 949, 68, 1196, 223, 666, 1170, 419, 32, 169, 235, 933, 968, 201, 291, 1179, 1084, 947, 1126, 1062, 781, 797, 851, 785, 897, 557, 727, 969, 181, 552, 1117, 950, 990, 782, 465, 1159, 1008, 1125, 787, 272, 385, 1131, 114, 215, 110, 1086, 876, 907, 113, 1004, 650, 983, 929, 252, 793, 918, 976, 1055, 615, 69, 157, 1176, 18, 629, 137, 1112, 1128, 209, 874, 57, 1100, 1097, 82, 461, 1154, 945, 823, 470, 449, 1029, 1110, 343, 238, 687, 1094, 411, 97, 573, 701, 1099, 1030, 570, 495, 731, 416, 955, 1155, 921, 616, 1083, 60, 859, 679, 161, 1079, 684, 919, 73, 234, 344, 551, 50, 762, 266, 590, 608, 1018, 498, 962, 952, 902, 958, 713, 957, 779, 204, 253, 1156, 811, 724, 708, 154, 937, 675, 938, 935, 31, 734, 632, 46, 1200, 265, 680, 474, 869, 464, 567, 141, 1088, 1175, 563, 518, 493, 784, 442, 452, 1199, 729, 469, 276, 1114, 579, 546, 1211, 1209, 1119, 392, 20, 572, 367, 353, 646, 783, 592, 548, 1161, 436, 140, 251, 1177, 494, 589, 1194, 34, 726, 1208, 479, 391, 1108, 100, 828, 357, 95, 638, 652, 505, 564, 1130, 148, 741, 825, 1212, 1213, 425, 282, 1135, 1184, 718, 337, 70, 38, 422, 769, 1009, 826, 878, 273, 819, 691, 844, 881, 1138, 1162, 270, 556, 1053, 862, 832, 511, 642, 796, 66, 1157, 310, 674, 991, 290, 682, 905, 264, 1134, 528, 289, 749, 846, 1090, 192, 237, 536, 45, 372, 130, 177, 1036, 239, 224, 883, 198, 977, 791, 184, 732, 1202, 595, 965, 978, 163, 752, 1144, 143, 709, 90, 74, 1207, 649, 476, 503, 190, 543, 499, 298, 261, 233, 1037, 759, 170, 917, 1, 993, 1027, 738, 329, 451, 318, 0, 269, 199, 410, 538, 943, 191, 88, 1072, 544, 509, 490, 89, 1050, 739, 850, 35, 288, 102, 560, 332, 1026, 612, 128, 854, 327, 985, 931, 295, 777, 942, 635, 710, 610, 126, 39, 249, 305, 618, 899, 1011, 802, 898, 843, 561, 667, 1136, 296, 453, 481, 299]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9483891844270800
the save name prefix for this run is:  chkpt-ID_9483891844270800_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1187
rank avg (pred): 0.484 +- 0.005
mrr vals (pred, true): 0.015, 0.032
batch losses (mrrl, rdl): 0.0, 7.71177e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 776
rank avg (pred): 0.396 +- 0.238
mrr vals (pred, true): 0.193, 0.046
batch losses (mrrl, rdl): 0.0, 2.34266e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 379
rank avg (pred): 0.427 +- 0.247
mrr vals (pred, true): 0.172, 0.042
batch losses (mrrl, rdl): 0.0, 5.8373e-06

Epoch over!
epoch time: 14.881

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1136
rank avg (pred): 0.463 +- 0.268
mrr vals (pred, true): 0.164, 0.026
batch losses (mrrl, rdl): 0.0, 2.23413e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1110
rank avg (pred): 0.430 +- 0.253
mrr vals (pred, true): 0.151, 0.043
batch losses (mrrl, rdl): 0.0, 1.07942e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 763
rank avg (pred): 0.431 +- 0.245
mrr vals (pred, true): 0.126, 0.040
batch losses (mrrl, rdl): 0.0, 4.5848e-06

Epoch over!
epoch time: 15.064

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 195
rank avg (pred): 0.436 +- 0.238
mrr vals (pred, true): 0.111, 0.052
batch losses (mrrl, rdl): 0.0, 9.0035e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 929
rank avg (pred): 0.452 +- 0.252
mrr vals (pred, true): 0.116, 0.036
batch losses (mrrl, rdl): 0.0, 3.775e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 442
rank avg (pred): 0.431 +- 0.259
mrr vals (pred, true): 0.094, 0.047
batch losses (mrrl, rdl): 0.0, 5.4881e-06

Epoch over!
epoch time: 14.915

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1148
rank avg (pred): 0.421 +- 0.295
mrr vals (pred, true): 0.185, 0.026
batch losses (mrrl, rdl): 0.0, 7.39669e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 389
rank avg (pred): 0.440 +- 0.257
mrr vals (pred, true): 0.076, 0.044
batch losses (mrrl, rdl): 0.0, 7.4094e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 87
rank avg (pred): 0.409 +- 0.281
mrr vals (pred, true): 0.119, 0.050
batch losses (mrrl, rdl): 0.0, 2.4873e-06

Epoch over!
epoch time: 14.901

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1068
rank avg (pred): 0.160 +- 0.189
mrr vals (pred, true): 0.181, 0.279
batch losses (mrrl, rdl): 0.0, 1.06206e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 580
rank avg (pred): 0.452 +- 0.253
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0, 1.0925e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 340
rank avg (pred): 0.441 +- 0.263
mrr vals (pred, true): 0.071, 0.048
batch losses (mrrl, rdl): 0.0, 2.4269e-06

Epoch over!
epoch time: 14.899

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 334
rank avg (pred): 0.439 +- 0.260
mrr vals (pred, true): 0.068, 0.045
batch losses (mrrl, rdl): 0.0033253294, 6.745e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 329
rank avg (pred): 0.565 +- 0.280
mrr vals (pred, true): 0.059, 0.047
batch losses (mrrl, rdl): 0.0008752811, 0.0002381141

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 928
rank avg (pred): 0.529 +- 0.236
mrr vals (pred, true): 0.052, 0.039
batch losses (mrrl, rdl): 4.33366e-05, 7.81313e-05

Epoch over!
epoch time: 15.093

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 266
rank avg (pred): 0.310 +- 0.323
mrr vals (pred, true): 0.227, 0.218
batch losses (mrrl, rdl): 0.0008372465, 0.000348663

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 166
rank avg (pred): 0.474 +- 0.175
mrr vals (pred, true): 0.054, 0.052
batch losses (mrrl, rdl): 0.000163262, 6.63519e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1063
rank avg (pred): 0.255 +- 0.281
mrr vals (pred, true): 0.247, 0.275
batch losses (mrrl, rdl): 0.007719567, 0.0003871893

Epoch over!
epoch time: 15.072

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 673
rank avg (pred): 0.471 +- 0.158
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 4.0773e-06, 3.59972e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 524
rank avg (pred): 0.457 +- 0.131
mrr vals (pred, true): 0.047, 0.023
batch losses (mrrl, rdl): 6.6084e-05, 7.23379e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 326
rank avg (pred): 0.444 +- 0.122
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 5.33445e-05, 4.65599e-05

Epoch over!
epoch time: 15.145

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1138
rank avg (pred): 0.431 +- 0.122
mrr vals (pred, true): 0.054, 0.026
batch losses (mrrl, rdl): 0.000142982, 6.05052e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 186
rank avg (pred): 0.448 +- 0.111
mrr vals (pred, true): 0.049, 0.059
batch losses (mrrl, rdl): 1.16945e-05, 6.68046e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.449 +- 0.115
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 1.06254e-05, 5.33621e-05

Epoch over!
epoch time: 15.214

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 374
rank avg (pred): 0.438 +- 0.107
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 2.4844e-06, 5.97874e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 761
rank avg (pred): 0.429 +- 0.107
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 2.13889e-05, 5.33457e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 111
rank avg (pred): 0.433 +- 0.102
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 2.01145e-05, 5.98734e-05

Epoch over!
epoch time: 15.176

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1057
rank avg (pred): 0.205 +- 0.202
mrr vals (pred, true): 0.314, 0.265
batch losses (mrrl, rdl): 0.0239058416, 3.9502e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1085
rank avg (pred): 0.438 +- 0.095
mrr vals (pred, true): 0.049, 0.054
batch losses (mrrl, rdl): 2.15559e-05, 6.74481e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1188
rank avg (pred): 0.454 +- 0.104
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 4.774e-07, 5.68579e-05

Epoch over!
epoch time: 15.306

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 354
rank avg (pred): 0.435 +- 0.099
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 2.82297e-05, 7.6846e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 886
rank avg (pred): 0.432 +- 0.087
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 2.44672e-05, 7.45416e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 815
rank avg (pred): 0.155 +- 0.198
mrr vals (pred, true): 0.512, 0.539
batch losses (mrrl, rdl): 0.0074716574, 0.0003000143

Epoch over!
epoch time: 15.315

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 653
rank avg (pred): 0.471 +- 0.124
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002602506, 0.0001066436

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 397
rank avg (pred): 0.425 +- 0.083
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 1.24145e-05, 7.05096e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 738
rank avg (pred): 0.253 +- 0.196
mrr vals (pred, true): 0.285, 0.310
batch losses (mrrl, rdl): 0.0060526701, 0.0004164826

Epoch over!
epoch time: 15.097

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 991
rank avg (pred): 0.239 +- 0.193
mrr vals (pred, true): 0.300, 0.252
batch losses (mrrl, rdl): 0.0225233193, 0.0002440391

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 693
rank avg (pred): 0.488 +- 0.135
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 2.90793e-05, 6.28129e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 233
rank avg (pred): 0.413 +- 0.089
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 0.0001067527, 7.4291e-05

Epoch over!
epoch time: 15.117

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1061
rank avg (pred): 0.218 +- 0.199
mrr vals (pred, true): 0.300, 0.266
batch losses (mrrl, rdl): 0.0115646478, 5.47106e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1202
rank avg (pred): 0.491 +- 0.142
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 2.07858e-05, 7.2586e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 726
rank avg (pred): 0.460 +- 0.119
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 7.322e-07, 6.83978e-05

Epoch over!
epoch time: 15.079

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.470 +- 0.142
mrr vals (pred, true): 0.057, 0.034

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   72 	     0 	 0.05306 	 0.02010 	 m..s
    6 	     1 	 0.04973 	 0.02022 	 ~...
   45 	     2 	 0.05147 	 0.02192 	 ~...
   74 	     3 	 0.05320 	 0.02302 	 m..s
   31 	     4 	 0.05089 	 0.02356 	 ~...
   53 	     5 	 0.05162 	 0.02479 	 ~...
   47 	     6 	 0.05149 	 0.02496 	 ~...
   33 	     7 	 0.05102 	 0.02497 	 ~...
   54 	     8 	 0.05167 	 0.02531 	 ~...
    5 	     9 	 0.04969 	 0.02539 	 ~...
   55 	    10 	 0.05171 	 0.02548 	 ~...
    4 	    11 	 0.04946 	 0.02568 	 ~...
   25 	    12 	 0.05068 	 0.02588 	 ~...
    3 	    13 	 0.04944 	 0.02633 	 ~...
   46 	    14 	 0.05147 	 0.02676 	 ~...
   75 	    15 	 0.05320 	 0.03016 	 ~...
   89 	    16 	 0.05631 	 0.03139 	 ~...
   90 	    17 	 0.05668 	 0.03448 	 ~...
   60 	    18 	 0.05229 	 0.03549 	 ~...
   48 	    19 	 0.05151 	 0.03637 	 ~...
   40 	    20 	 0.05132 	 0.03715 	 ~...
   67 	    21 	 0.05285 	 0.03889 	 ~...
   42 	    22 	 0.05136 	 0.03929 	 ~...
   79 	    23 	 0.05340 	 0.03948 	 ~...
   44 	    24 	 0.05137 	 0.03980 	 ~...
   73 	    25 	 0.05311 	 0.04032 	 ~...
   14 	    26 	 0.05015 	 0.04080 	 ~...
   51 	    27 	 0.05154 	 0.04099 	 ~...
   68 	    28 	 0.05287 	 0.04101 	 ~...
   61 	    29 	 0.05231 	 0.04127 	 ~...
   29 	    30 	 0.05077 	 0.04130 	 ~...
   87 	    31 	 0.05592 	 0.04147 	 ~...
   71 	    32 	 0.05289 	 0.04209 	 ~...
   35 	    33 	 0.05105 	 0.04224 	 ~...
   39 	    34 	 0.05116 	 0.04238 	 ~...
   52 	    35 	 0.05154 	 0.04252 	 ~...
    9 	    36 	 0.04989 	 0.04261 	 ~...
   13 	    37 	 0.05005 	 0.04264 	 ~...
   12 	    38 	 0.05002 	 0.04272 	 ~...
   83 	    39 	 0.05395 	 0.04279 	 ~...
   59 	    40 	 0.05226 	 0.04344 	 ~...
   80 	    41 	 0.05363 	 0.04378 	 ~...
   69 	    42 	 0.05287 	 0.04418 	 ~...
   66 	    43 	 0.05259 	 0.04440 	 ~...
   70 	    44 	 0.05288 	 0.04444 	 ~...
   77 	    45 	 0.05325 	 0.04456 	 ~...
   43 	    46 	 0.05136 	 0.04466 	 ~...
   57 	    47 	 0.05212 	 0.04474 	 ~...
   15 	    48 	 0.05018 	 0.04483 	 ~...
   11 	    49 	 0.05002 	 0.04487 	 ~...
   84 	    50 	 0.05398 	 0.04498 	 ~...
   58 	    51 	 0.05214 	 0.04515 	 ~...
    7 	    52 	 0.04979 	 0.04599 	 ~...
   27 	    53 	 0.05075 	 0.04620 	 ~...
   36 	    54 	 0.05105 	 0.04650 	 ~...
   22 	    55 	 0.05037 	 0.04670 	 ~...
   10 	    56 	 0.04989 	 0.04695 	 ~...
   32 	    57 	 0.05101 	 0.04711 	 ~...
   50 	    58 	 0.05152 	 0.04713 	 ~...
   88 	    59 	 0.05597 	 0.04722 	 ~...
   24 	    60 	 0.05061 	 0.04736 	 ~...
   19 	    61 	 0.05024 	 0.04816 	 ~...
   76 	    62 	 0.05322 	 0.04822 	 ~...
   34 	    63 	 0.05103 	 0.04843 	 ~...
   49 	    64 	 0.05151 	 0.04848 	 ~...
   56 	    65 	 0.05189 	 0.04854 	 ~...
   17 	    66 	 0.05019 	 0.04895 	 ~...
   82 	    67 	 0.05390 	 0.04900 	 ~...
   41 	    68 	 0.05132 	 0.04922 	 ~...
   81 	    69 	 0.05386 	 0.04961 	 ~...
   30 	    70 	 0.05081 	 0.04978 	 ~...
   16 	    71 	 0.05018 	 0.04990 	 ~...
   65 	    72 	 0.05259 	 0.05019 	 ~...
    1 	    73 	 0.04917 	 0.05025 	 ~...
   86 	    74 	 0.05548 	 0.05095 	 ~...
   64 	    75 	 0.05250 	 0.05129 	 ~...
   23 	    76 	 0.05040 	 0.05151 	 ~...
   37 	    77 	 0.05111 	 0.05231 	 ~...
   18 	    78 	 0.05019 	 0.05244 	 ~...
   28 	    79 	 0.05076 	 0.05275 	 ~...
   85 	    80 	 0.05477 	 0.05320 	 ~...
   21 	    81 	 0.05034 	 0.05355 	 ~...
   20 	    82 	 0.05028 	 0.05355 	 ~...
   62 	    83 	 0.05232 	 0.05358 	 ~...
    8 	    84 	 0.04979 	 0.05367 	 ~...
   38 	    85 	 0.05112 	 0.05439 	 ~...
    2 	    86 	 0.04941 	 0.05491 	 ~...
    0 	    87 	 0.04912 	 0.05546 	 ~...
   63 	    88 	 0.05239 	 0.05567 	 ~...
   78 	    89 	 0.05327 	 0.05868 	 ~...
   26 	    90 	 0.05070 	 0.06019 	 ~...
   91 	    91 	 0.20248 	 0.17571 	 ~...
   94 	    92 	 0.20830 	 0.18625 	 ~...
   95 	    93 	 0.22579 	 0.18627 	 m..s
   98 	    94 	 0.22778 	 0.19882 	 ~...
   96 	    95 	 0.22599 	 0.20165 	 ~...
   92 	    96 	 0.20410 	 0.20277 	 ~...
   97 	    97 	 0.22762 	 0.21298 	 ~...
   93 	    98 	 0.20782 	 0.21358 	 ~...
  104 	    99 	 0.23183 	 0.21610 	 ~...
  101 	   100 	 0.22987 	 0.21875 	 ~...
  114 	   101 	 0.26648 	 0.21997 	 m..s
  100 	   102 	 0.22972 	 0.22269 	 ~...
  105 	   103 	 0.23185 	 0.22399 	 ~...
  107 	   104 	 0.23634 	 0.22505 	 ~...
  106 	   105 	 0.23323 	 0.22546 	 ~...
  108 	   106 	 0.23710 	 0.23132 	 ~...
  109 	   107 	 0.24022 	 0.23847 	 ~...
   99 	   108 	 0.22893 	 0.24271 	 ~...
  102 	   109 	 0.23011 	 0.24338 	 ~...
  103 	   110 	 0.23169 	 0.25751 	 ~...
  110 	   111 	 0.24105 	 0.26409 	 ~...
  111 	   112 	 0.24126 	 0.26949 	 ~...
  113 	   113 	 0.26372 	 0.27497 	 ~...
  118 	   114 	 0.27767 	 0.29009 	 ~...
  117 	   115 	 0.27482 	 0.29106 	 ~...
  112 	   116 	 0.24920 	 0.29334 	 m..s
  115 	   117 	 0.27357 	 0.31426 	 m..s
  116 	   118 	 0.27473 	 0.32376 	 m..s
  119 	   119 	 0.27924 	 0.33429 	 m..s
  120 	   120 	 0.37391 	 0.38637 	 ~...
==========================================
r_mrr = 0.9865686297416687
r2_mrr = 0.9643291234970093
spearmanr_mrr@5 = 0.9167608618736267
spearmanr_mrr@10 = 0.9171367287635803
spearmanr_mrr@50 = 0.988136887550354
spearmanr_mrr@100 = 0.9930191040039062
spearmanr_mrr@All = 0.9913727641105652
==========================================
test time: 0.458
Done Testing dataset UMLS
total time taken: 232.954735994339
training time taken: 226.7444314956665
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9866)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9643)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9168)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9171)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9881)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9930)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9914)}}, 'test_loss': {'ComplEx': {'UMLS': 0.2198676116167917}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 8244399546254000
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1060, 801, 1208, 828, 877, 1212, 542, 701, 605, 169, 258, 1214, 364, 958, 342, 1132, 1151, 447, 388, 712, 1067, 987, 848, 718, 946, 514, 474, 1045, 574, 1186, 573, 68, 1119, 1001, 643, 179, 1005, 418, 476, 998, 765, 1191, 802, 819, 743, 73, 1093, 55, 311, 370, 850, 85, 500, 945, 270, 22, 1113, 1101, 281, 437, 881, 793, 110, 1107, 140, 385, 781, 563, 313, 259, 64, 888, 465, 1162, 623, 528, 580, 147, 163, 1163, 1129, 575, 80, 72, 473, 45, 874, 1199, 1180, 914, 74, 428, 666, 586, 102, 1206, 422, 18, 703, 224, 876, 1207, 1193, 1125, 862, 676, 243, 367, 842, 1116, 31, 717, 1131, 111, 649, 589, 84, 98, 546, 549, 578]
valid_ids (0): []
train_ids (1094): [343, 745, 937, 290, 933, 971, 512, 762, 190, 996, 257, 395, 795, 999, 867, 269, 1139, 6, 1104, 941, 543, 530, 411, 240, 13, 628, 777, 836, 77, 1144, 408, 53, 787, 796, 627, 1011, 602, 1188, 317, 323, 731, 617, 369, 857, 1148, 960, 495, 678, 423, 1053, 978, 750, 596, 986, 1146, 1213, 957, 276, 684, 620, 641, 1173, 780, 1020, 3, 44, 824, 568, 1178, 1039, 394, 969, 1044, 119, 1031, 37, 136, 1106, 1077, 1043, 964, 417, 362, 1081, 1130, 901, 135, 360, 839, 1003, 63, 640, 485, 608, 966, 203, 336, 65, 351, 95, 544, 738, 331, 501, 1012, 358, 157, 558, 403, 146, 1156, 296, 122, 306, 696, 779, 928, 220, 1085, 735, 400, 917, 809, 48, 484, 1052, 175, 421, 847, 920, 774, 739, 814, 798, 107, 896, 1059, 446, 886, 916, 1205, 366, 532, 831, 1021, 639, 727, 379, 803, 973, 794, 16, 834, 659, 27, 315, 1124, 116, 148, 382, 271, 1098, 349, 491, 143, 97, 521, 621, 87, 137, 195, 912, 807, 690, 478, 940, 266, 759, 1210, 10, 565, 1142, 883, 99, 1176, 167, 609, 209, 984, 249, 595, 1120, 723, 371, 1200, 754, 210, 94, 247, 644, 715, 786, 572, 763, 518, 237, 131, 672, 679, 769, 742, 1016, 1022, 1165, 354, 553, 406, 845, 926, 611, 193, 552, 631, 215, 126, 356, 1051, 165, 441, 1166, 846, 455, 898, 681, 142, 600, 128, 895, 646, 184, 348, 1126, 699, 1203, 91, 668, 365, 466, 730, 78, 930, 768, 982, 338, 316, 705, 218, 448, 171, 350, 504, 661, 934, 670, 947, 330, 944, 652, 935, 1075, 1155, 439, 52, 62, 583, 186, 704, 541, 413, 961, 288, 129, 101, 693, 721, 33, 398, 967, 866, 785, 687, 166, 412, 938, 855, 231, 92, 1080, 757, 792, 1088, 272, 638, 189, 261, 219, 393, 223, 334, 861, 954, 811, 321, 274, 650, 576, 70, 1, 75, 1114, 674, 487, 327, 716, 1108, 1152, 904, 138, 725, 496, 921, 997, 459, 654, 103, 235, 81, 604, 980, 925, 1159, 1128, 525, 335, 882, 783, 481, 929, 17, 616, 314, 775, 192, 337, 613, 188, 303, 469, 806, 594, 979, 851, 199, 1024, 1076, 410, 226, 244, 767, 1046, 637, 238, 284, 1175, 430, 658, 776, 943, 818, 634, 225, 59, 897, 172, 8, 50, 720, 279, 517, 840, 7, 368, 93, 632, 962, 753, 139, 664, 429, 629, 560, 708, 100, 1103, 234, 907, 507, 347, 1000, 1068, 1167, 1150, 216, 655, 452, 434, 79, 1030, 325, 663, 860, 823, 965, 1069, 1154, 38, 387, 622, 1035, 438, 653, 426, 1038, 510, 592, 158, 1185, 480, 714, 46, 183, 145, 1019, 339, 955, 733, 1177, 173, 729, 265, 11, 443, 582, 346, 1023, 185, 76, 206, 380, 196, 956, 180, 432, 458, 150, 267, 407, 1058, 1004, 378, 737, 373, 988, 669, 397, 642, 21, 490, 472, 1079, 112, 177, 756, 0, 1057, 509, 1095, 660, 564, 133, 740, 656, 635, 372, 409, 263, 1141, 260, 713, 1140, 1184, 830, 497, 837, 821, 939, 1195, 436, 1096, 985, 462, 1174, 1121, 1158, 1008, 1072, 555, 377, 598, 547, 537, 1050, 47, 275, 607, 707, 1157, 1149, 149, 645, 242, 1190, 88, 844, 86, 612, 719, 667, 648, 141, 894, 289, 698, 268, 320, 651, 771, 675, 599, 829, 503, 359, 755, 550, 1028, 153, 233, 307, 151, 591, 711, 1187, 121, 770, 948, 1168, 1183, 993, 435, 697, 300, 352, 14, 799, 1074, 477, 20, 557, 869, 43, 1169, 1123, 1048, 505, 1209, 854, 995, 590, 1172, 1034, 250, 1164, 174, 1100, 245, 301, 181, 1118, 647, 39, 927, 162, 1115, 1078, 890, 15, 752, 841, 470, 618, 766, 567, 545, 236, 130, 710, 902, 515, 187, 424, 892, 326, 483, 533, 1027, 1007, 310, 923, 453, 256, 593, 1026, 885, 386, 601, 264, 606, 294, 1002, 588, 908, 662, 416, 363, 118, 843, 832, 114, 58, 451, 764, 1117, 304, 1192, 502, 734, 1086, 357, 950, 125, 772, 345, 633, 527, 863, 891, 332, 1135, 390, 859, 871, 695, 893, 1083, 60, 1091, 56, 873, 461, 983, 19, 536, 51, 619, 878, 1202, 431, 880, 872, 922, 1041, 198, 952, 383, 1036, 569, 168, 1211, 35, 963, 161, 889, 826, 252, 784, 1062, 584, 324, 117, 1089, 746, 254, 722, 36, 182, 905, 1014, 511, 1197, 24, 106, 164, 865, 680, 402, 127, 556, 686, 626, 688, 749, 630, 1092, 990, 391, 1160, 40, 253, 1055, 683, 442, 513, 906, 468, 810, 977, 789, 489, 1013, 949, 516, 475, 603, 457, 534, 308, 875, 23, 1204, 706, 492, 747, 1127, 389, 227, 54, 773, 790, 344, 788, 176, 942, 726, 778, 109, 1009, 486, 178, 1110, 816, 34, 741, 152, 838, 559, 856, 144, 1161, 585, 229, 571, 539, 587, 579, 636, 376, 1070, 858, 392, 761, 1102, 1097, 493, 1063, 333, 1112, 425, 82, 526, 624, 531, 381, 911, 454, 420, 991, 241, 989, 915, 538, 1029, 736, 1094, 482, 760, 1137, 1182, 1181, 813, 124, 728, 1133, 282, 523, 519, 204, 554, 919, 909, 278, 1105, 4, 804, 551, 105, 852, 1006, 1090, 12, 820, 903, 671, 1138, 213, 69, 924, 160, 471, 968, 797, 899, 932, 207, 1109, 32, 522, 159, 211, 1015, 868, 700, 812, 953, 170, 1040, 1179, 577, 535, 202, 1073, 433, 67, 1065, 566, 318, 49, 581, 155, 450, 42, 808, 191, 57, 665, 156, 1017, 1025, 910, 197, 286, 239, 744, 355, 1189, 1087, 835, 709, 913, 154, 419, 570, 1054, 508, 682, 918, 123, 488, 329, 214, 405, 614, 1056, 83, 104, 115, 399, 959, 758, 900, 230, 822, 232, 689, 479, 1084, 71, 677, 322, 89, 994, 506, 1170, 751, 440, 981, 833, 685, 291, 248, 732, 1201, 384, 297, 610, 972, 277, 361, 800, 951, 970, 5, 305, 396, 374, 1033, 1099, 1018, 815, 1136, 61, 1134, 66, 625, 974, 134, 975, 201, 246, 1010, 931, 499, 1082, 319, 691, 936, 205, 887, 1145, 1049, 673, 1061, 228, 445, 222, 540, 415, 1198, 30, 217, 328, 1147, 524, 1071, 791, 29, 298, 25, 827, 2, 1042, 375, 26, 1032, 1066, 1037, 108, 194, 28, 340, 9, 597, 120, 817, 657, 251, 849, 561, 1047, 992, 692, 1143, 309, 444, 221, 302, 280, 805, 1122, 498, 293, 262, 884, 113, 312, 782, 529, 90, 1171, 460, 520, 132, 562, 467, 1194, 283, 1153, 96, 702, 414, 825, 464, 427, 879, 548, 456, 449, 615, 353, 1111, 976, 200, 864, 299, 404, 1064, 463, 694, 285, 273, 724, 287, 748, 292, 401, 295, 853, 212, 870, 494, 208, 1196, 255, 341, 41]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8667048587025227
the save name prefix for this run is:  chkpt-ID_8667048587025227_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 997
rank avg (pred): 0.424 +- 0.003
mrr vals (pred, true): 0.017, 0.285
batch losses (mrrl, rdl): 0.0, 0.0011673603

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 345
rank avg (pred): 0.447 +- 0.010
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 8.13286e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 191
rank avg (pred): 0.478 +- 0.188
mrr vals (pred, true): 0.017, 0.047
batch losses (mrrl, rdl): 0.0, 3.01639e-05

Epoch over!
epoch time: 14.885

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 256
rank avg (pred): 0.167 +- 0.228
mrr vals (pred, true): 0.079, 0.195
batch losses (mrrl, rdl): 0.0, 3.18001e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 35
rank avg (pred): 0.172 +- 0.199
mrr vals (pred, true): 0.072, 0.253
batch losses (mrrl, rdl): 0.0, 1.3513e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 456
rank avg (pred): 0.464 +- 0.266
mrr vals (pred, true): 0.020, 0.050
batch losses (mrrl, rdl): 0.0, 1.34637e-05

Epoch over!
epoch time: 14.794

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 779
rank avg (pred): 0.452 +- 0.240
mrr vals (pred, true): 0.020, 0.049
batch losses (mrrl, rdl): 0.0, 5.8447e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 709
rank avg (pred): 0.474 +- 0.260
mrr vals (pred, true): 0.020, 0.049
batch losses (mrrl, rdl): 0.0, 4.446e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 461
rank avg (pred): 0.487 +- 0.261
mrr vals (pred, true): 0.019, 0.052
batch losses (mrrl, rdl): 0.0, 7.1084e-06

Epoch over!
epoch time: 14.795

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1192
rank avg (pred): 0.469 +- 0.254
mrr vals (pred, true): 0.020, 0.052
batch losses (mrrl, rdl): 0.0, 1.55456e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 840
rank avg (pred): 0.447 +- 0.253
mrr vals (pred, true): 0.021, 0.053
batch losses (mrrl, rdl): 0.0, 5.9796e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 680
rank avg (pred): 0.444 +- 0.245
mrr vals (pred, true): 0.021, 0.056
batch losses (mrrl, rdl): 0.0, 3.8959e-06

Epoch over!
epoch time: 14.81

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 355
rank avg (pred): 0.467 +- 0.259
mrr vals (pred, true): 0.020, 0.046
batch losses (mrrl, rdl): 0.0, 3.8837e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 780
rank avg (pred): 0.427 +- 0.247
mrr vals (pred, true): 0.023, 0.038
batch losses (mrrl, rdl): 0.0, 2.12962e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 406
rank avg (pred): 0.449 +- 0.265
mrr vals (pred, true): 0.022, 0.054
batch losses (mrrl, rdl): 0.0, 2.6872e-06

Epoch over!
epoch time: 14.832

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 558
rank avg (pred): 0.544 +- 0.210
mrr vals (pred, true): 0.015, 0.025
batch losses (mrrl, rdl): 0.0119626336, 1.5637e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 251
rank avg (pred): 0.254 +- 0.335
mrr vals (pred, true): 0.365, 0.246
batch losses (mrrl, rdl): 0.1412958205, 0.0003443429

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 159
rank avg (pred): 0.357 +- 0.336
mrr vals (pred, true): 0.052, 0.048
batch losses (mrrl, rdl): 6.19848e-05, 0.00016018

Epoch over!
epoch time: 15.192

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 330
rank avg (pred): 0.393 +- 0.365
mrr vals (pred, true): 0.048, 0.052
batch losses (mrrl, rdl): 3.78716e-05, 0.0001146078

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 431
rank avg (pred): 0.355 +- 0.341
mrr vals (pred, true): 0.056, 0.056
batch losses (mrrl, rdl): 0.0003582441, 0.0001538386

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 247
rank avg (pred): 0.207 +- 0.274
mrr vals (pred, true): 0.273, 0.242
batch losses (mrrl, rdl): 0.0091421334, 9.54048e-05

Epoch over!
epoch time: 15.266

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 496
rank avg (pred): 0.392 +- 0.362
mrr vals (pred, true): 0.048, 0.025
batch losses (mrrl, rdl): 3.09589e-05, 0.0003719894

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 420
rank avg (pred): 0.386 +- 0.361
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 2.2037e-06, 9.32378e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 999
rank avg (pred): 0.390 +- 0.353
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 5.3874e-06, 0.0001218115

Epoch over!
epoch time: 15.185

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 788
rank avg (pred): 0.362 +- 0.328
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 6.2111e-06, 0.0001842226

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1190
rank avg (pred): 0.399 +- 0.351
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0001047244, 8.57594e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 296
rank avg (pred): 0.193 +- 0.219
mrr vals (pred, true): 0.209, 0.215
batch losses (mrrl, rdl): 0.0003047699, 1.30368e-05

Epoch over!
epoch time: 15.118

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 356
rank avg (pred): 0.418 +- 0.372
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 6.30382e-05, 8.44327e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1090
rank avg (pred): 0.415 +- 0.370
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 6.539e-07, 0.0001028354

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.396 +- 0.350
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 1.28973e-05, 6.98092e-05

Epoch over!
epoch time: 15.118

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 277
rank avg (pred): 0.150 +- 0.160
mrr vals (pred, true): 0.254, 0.260
batch losses (mrrl, rdl): 0.0003499094, 7.9172e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 936
rank avg (pred): 0.431 +- 0.373
mrr vals (pred, true): 0.047, 0.040
batch losses (mrrl, rdl): 8.19754e-05, 0.0001313202

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1081
rank avg (pred): 0.402 +- 0.357
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 0.0001033737, 6.47801e-05

Epoch over!
epoch time: 15.058

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 156
rank avg (pred): 0.371 +- 0.323
mrr vals (pred, true): 0.052, 0.057
batch losses (mrrl, rdl): 3.14987e-05, 7.1119e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 349
rank avg (pred): 0.373 +- 0.330
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 5.828e-06, 8.58799e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 544
rank avg (pred): 0.402 +- 0.349
mrr vals (pred, true): 0.049, 0.026
batch losses (mrrl, rdl): 2.20753e-05, 0.000277775

Epoch over!
epoch time: 15.097

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 503
rank avg (pred): 0.392 +- 0.345
mrr vals (pred, true): 0.051, 0.026
batch losses (mrrl, rdl): 2.08142e-05, 0.0003017061

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 344
rank avg (pred): 0.398 +- 0.340
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 6.0019e-05, 7.87598e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1043
rank avg (pred): 0.453 +- 0.371
mrr vals (pred, true): 0.040, 0.056
batch losses (mrrl, rdl): 0.0010841618, 7.23946e-05

Epoch over!
epoch time: 15.082

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1159
rank avg (pred): 0.451 +- 0.398
mrr vals (pred, true): 0.052, 0.025
batch losses (mrrl, rdl): 2.33569e-05, 0.0002471574

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 556
rank avg (pred): 0.445 +- 0.384
mrr vals (pred, true): 0.048, 0.021
batch losses (mrrl, rdl): 5.72905e-05, 0.0002209591

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 276
rank avg (pred): 0.121 +- 0.121
mrr vals (pred, true): 0.224, 0.257
batch losses (mrrl, rdl): 0.0109523777, 2.56094e-05

Epoch over!
epoch time: 15.075

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 78
rank avg (pred): 0.127 +- 0.126
mrr vals (pred, true): 0.177, 0.205
batch losses (mrrl, rdl): 0.0077457549, 0.0001193432

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 626
rank avg (pred): 0.399 +- 0.354
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.678e-05, 7.88877e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 42
rank avg (pred): 0.131 +- 0.125
mrr vals (pred, true): 0.163, 0.200
batch losses (mrrl, rdl): 0.0136854025, 8.40376e-05

Epoch over!
epoch time: 15.2

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.109 +- 0.104
mrr vals (pred, true): 0.299, 0.299

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   67 	     0 	 0.05354 	 0.02094 	 m..s
   75 	     1 	 0.05416 	 0.02144 	 m..s
   74 	     2 	 0.05409 	 0.02319 	 m..s
   65 	     3 	 0.05318 	 0.02442 	 ~...
   43 	     4 	 0.05086 	 0.02459 	 ~...
   47 	     5 	 0.05108 	 0.02536 	 ~...
   57 	     6 	 0.05221 	 0.02544 	 ~...
   40 	     7 	 0.05050 	 0.02586 	 ~...
   62 	     8 	 0.05266 	 0.02676 	 ~...
   10 	     9 	 0.04801 	 0.03197 	 ~...
   59 	    10 	 0.05234 	 0.03549 	 ~...
   29 	    11 	 0.04942 	 0.03608 	 ~...
   69 	    12 	 0.05387 	 0.03647 	 ~...
   63 	    13 	 0.05273 	 0.03744 	 ~...
   61 	    14 	 0.05258 	 0.03749 	 ~...
   52 	    15 	 0.05139 	 0.03888 	 ~...
   24 	    16 	 0.04912 	 0.03923 	 ~...
   26 	    17 	 0.04921 	 0.03971 	 ~...
   54 	    18 	 0.05154 	 0.04019 	 ~...
   76 	    19 	 0.05420 	 0.04034 	 ~...
   35 	    20 	 0.05017 	 0.04038 	 ~...
   37 	    21 	 0.05026 	 0.04050 	 ~...
    2 	    22 	 0.04566 	 0.04099 	 ~...
   68 	    23 	 0.05372 	 0.04108 	 ~...
   70 	    24 	 0.05387 	 0.04111 	 ~...
   78 	    25 	 0.05429 	 0.04144 	 ~...
    8 	    26 	 0.04760 	 0.04214 	 ~...
   73 	    27 	 0.05405 	 0.04224 	 ~...
   53 	    28 	 0.05150 	 0.04238 	 ~...
   15 	    29 	 0.04819 	 0.04238 	 ~...
   25 	    30 	 0.04913 	 0.04259 	 ~...
   51 	    31 	 0.05138 	 0.04279 	 ~...
   14 	    32 	 0.04819 	 0.04286 	 ~...
   11 	    33 	 0.04813 	 0.04286 	 ~...
   90 	    34 	 0.05629 	 0.04301 	 ~...
   87 	    35 	 0.05545 	 0.04303 	 ~...
   50 	    36 	 0.05125 	 0.04306 	 ~...
   45 	    37 	 0.05098 	 0.04308 	 ~...
    1 	    38 	 0.04437 	 0.04313 	 ~...
   84 	    39 	 0.05517 	 0.04319 	 ~...
   71 	    40 	 0.05387 	 0.04343 	 ~...
    6 	    41 	 0.04628 	 0.04344 	 ~...
   66 	    42 	 0.05339 	 0.04369 	 ~...
   58 	    43 	 0.05229 	 0.04375 	 ~...
   42 	    44 	 0.05067 	 0.04397 	 ~...
    7 	    45 	 0.04709 	 0.04404 	 ~...
   18 	    46 	 0.04840 	 0.04441 	 ~...
   55 	    47 	 0.05205 	 0.04460 	 ~...
   64 	    48 	 0.05289 	 0.04490 	 ~...
   13 	    49 	 0.04818 	 0.04559 	 ~...
   77 	    50 	 0.05425 	 0.04567 	 ~...
    5 	    51 	 0.04605 	 0.04577 	 ~...
   17 	    52 	 0.04823 	 0.04586 	 ~...
   72 	    53 	 0.05397 	 0.04594 	 ~...
   86 	    54 	 0.05531 	 0.04594 	 ~...
   89 	    55 	 0.05581 	 0.04612 	 ~...
   33 	    56 	 0.05000 	 0.04713 	 ~...
   46 	    57 	 0.05102 	 0.04718 	 ~...
   28 	    58 	 0.04941 	 0.04740 	 ~...
   22 	    59 	 0.04890 	 0.04781 	 ~...
   31 	    60 	 0.04975 	 0.04788 	 ~...
   82 	    61 	 0.05456 	 0.04789 	 ~...
   49 	    62 	 0.05123 	 0.04819 	 ~...
   91 	    63 	 0.05633 	 0.04822 	 ~...
    9 	    64 	 0.04771 	 0.04838 	 ~...
   23 	    65 	 0.04903 	 0.04851 	 ~...
   41 	    66 	 0.05052 	 0.04869 	 ~...
    4 	    67 	 0.04604 	 0.04871 	 ~...
   36 	    68 	 0.05022 	 0.04886 	 ~...
   92 	    69 	 0.05667 	 0.04904 	 ~...
   32 	    70 	 0.04990 	 0.04938 	 ~...
   16 	    71 	 0.04822 	 0.04949 	 ~...
   39 	    72 	 0.05045 	 0.04960 	 ~...
   83 	    73 	 0.05496 	 0.04973 	 ~...
   94 	    74 	 0.06066 	 0.04976 	 ~...
   34 	    75 	 0.05003 	 0.04986 	 ~...
   80 	    76 	 0.05434 	 0.05000 	 ~...
   79 	    77 	 0.05432 	 0.05012 	 ~...
   21 	    78 	 0.04888 	 0.05056 	 ~...
   60 	    79 	 0.05247 	 0.05092 	 ~...
   56 	    80 	 0.05209 	 0.05100 	 ~...
   81 	    81 	 0.05451 	 0.05115 	 ~...
   95 	    82 	 0.06183 	 0.05129 	 ~...
   19 	    83 	 0.04870 	 0.05160 	 ~...
   27 	    84 	 0.04938 	 0.05176 	 ~...
   12 	    85 	 0.04817 	 0.05189 	 ~...
    0 	    86 	 0.04436 	 0.05338 	 ~...
   85 	    87 	 0.05529 	 0.05381 	 ~...
   20 	    88 	 0.04871 	 0.05398 	 ~...
   48 	    89 	 0.05112 	 0.05424 	 ~...
   38 	    90 	 0.05027 	 0.05434 	 ~...
   44 	    91 	 0.05092 	 0.05448 	 ~...
   30 	    92 	 0.04967 	 0.05453 	 ~...
   88 	    93 	 0.05571 	 0.05549 	 ~...
   93 	    94 	 0.05701 	 0.05611 	 ~...
    3 	    95 	 0.04595 	 0.05989 	 ~...
  103 	    96 	 0.21751 	 0.18292 	 m..s
   98 	    97 	 0.19021 	 0.18366 	 ~...
   96 	    98 	 0.18790 	 0.18867 	 ~...
   99 	    99 	 0.19023 	 0.18889 	 ~...
  106 	   100 	 0.24314 	 0.19986 	 m..s
  101 	   101 	 0.19494 	 0.20119 	 ~...
  105 	   102 	 0.23663 	 0.20550 	 m..s
  104 	   103 	 0.22386 	 0.21393 	 ~...
  100 	   104 	 0.19285 	 0.21637 	 ~...
  109 	   105 	 0.25182 	 0.21849 	 m..s
   97 	   106 	 0.18865 	 0.21916 	 m..s
  108 	   107 	 0.25101 	 0.22048 	 m..s
  113 	   108 	 0.25684 	 0.22610 	 m..s
  102 	   109 	 0.20294 	 0.22717 	 ~...
  112 	   110 	 0.25386 	 0.22773 	 ~...
  107 	   111 	 0.25043 	 0.22870 	 ~...
  110 	   112 	 0.25225 	 0.24226 	 ~...
  111 	   113 	 0.25358 	 0.24656 	 ~...
  118 	   114 	 0.32506 	 0.28034 	 m..s
  114 	   115 	 0.28006 	 0.28743 	 ~...
  116 	   116 	 0.29940 	 0.29922 	 ~...
  117 	   117 	 0.30198 	 0.31636 	 ~...
  115 	   118 	 0.28950 	 0.32159 	 m..s
  120 	   119 	 0.34485 	 0.32358 	 ~...
  119 	   120 	 0.34409 	 0.32827 	 ~...
==========================================
r_mrr = 0.9877874851226807
r2_mrr = 0.9664894938468933
spearmanr_mrr@5 = 0.8391799926757812
spearmanr_mrr@10 = 0.9493848085403442
spearmanr_mrr@50 = 0.9965294599533081
spearmanr_mrr@100 = 0.9975341558456421
spearmanr_mrr@All = 0.9962508082389832
==========================================
test time: 0.458
Done Testing dataset UMLS
total time taken: 232.28423261642456
training time taken: 225.9773886203766
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9878)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9665)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.8392)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.9494)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9965)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9975)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9963)}}, 'test_loss': {'ComplEx': {'UMLS': 0.1785565858372138}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean freq rel', 's mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 7035051392482141
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1020, 771, 661, 1038, 432, 544, 94, 402, 344, 1213, 703, 964, 1141, 1161, 179, 756, 1034, 842, 271, 380, 189, 439, 896, 176, 1184, 63, 644, 469, 423, 500, 301, 908, 636, 815, 801, 556, 51, 751, 631, 679, 673, 280, 972, 881, 437, 913, 989, 361, 884, 466, 140, 763, 678, 543, 474, 1187, 906, 443, 755, 351, 349, 522, 1202, 617, 42, 398, 783, 700, 975, 795, 91, 255, 625, 102, 64, 701, 804, 648, 101, 774, 711, 582, 650, 944, 163, 1013, 853, 546, 482, 653, 519, 607, 251, 1178, 710, 405, 1052, 1, 341, 905, 234, 600, 1182, 515, 86, 39, 30, 666, 8, 169, 1169, 514, 747, 969, 410, 890, 198, 332, 986, 342, 148]
valid_ids (0): []
train_ids (1094): [37, 529, 698, 817, 588, 605, 214, 173, 545, 172, 1201, 203, 420, 974, 143, 118, 586, 295, 524, 288, 922, 359, 1078, 707, 990, 481, 619, 206, 246, 321, 278, 732, 465, 312, 1066, 209, 1006, 537, 260, 871, 257, 592, 379, 300, 865, 868, 134, 553, 159, 493, 982, 940, 264, 632, 733, 71, 621, 608, 195, 1157, 525, 317, 126, 557, 1071, 657, 594, 44, 850, 40, 1059, 576, 542, 365, 429, 794, 115, 128, 117, 244, 646, 1051, 866, 776, 23, 389, 89, 767, 717, 1193, 430, 1212, 252, 424, 746, 139, 171, 394, 523, 916, 156, 170, 1144, 699, 1117, 1042, 814, 1080, 491, 348, 1063, 1167, 1145, 616, 475, 655, 535, 454, 910, 955, 885, 360, 721, 427, 259, 505, 254, 1073, 434, 689, 520, 1067, 683, 452, 263, 324, 26, 144, 66, 723, 180, 483, 207, 199, 162, 387, 480, 346, 399, 215, 762, 65, 343, 937, 970, 953, 1105, 108, 1174, 1021, 496, 994, 1019, 851, 859, 193, 502, 338, 618, 538, 995, 530, 1092, 55, 1011, 408, 183, 382, 153, 981, 114, 383, 936, 928, 1082, 213, 1069, 790, 392, 897, 1154, 297, 510, 1166, 310, 941, 925, 821, 414, 381, 1135, 722, 460, 663, 779, 1110, 370, 791, 580, 561, 99, 421, 1137, 555, 58, 1035, 1168, 1127, 495, 436, 1057, 838, 568, 28, 220, 934, 415, 745, 235, 891, 456, 285, 240, 942, 1043, 385, 654, 355, 323, 141, 1143, 258, 799, 660, 548, 373, 959, 83, 759, 521, 879, 957, 468, 107, 658, 643, 630, 1008, 0, 231, 2, 1016, 286, 591, 966, 787, 656, 847, 681, 290, 73, 917, 499, 82, 1090, 1097, 367, 1060, 1208, 854, 219, 31, 1017, 81, 478, 824, 888, 760, 21, 1004, 1048, 1185, 448, 637, 882, 41, 503, 1196, 517, 662, 714, 1120, 813, 1087, 1054, 708, 739, 413, 145, 1115, 536, 315, 575, 682, 59, 190, 729, 1147, 291, 407, 674, 695, 444, 340, 404, 1203, 740, 60, 489, 563, 100, 914, 416, 949, 135, 980, 36, 731, 769, 803, 585, 22, 287, 438, 362, 999, 61, 222, 840, 1152, 276, 1014, 668, 1083, 810, 877, 789, 541, 1068, 898, 1150, 13, 299, 425, 154, 777, 702, 48, 742, 820, 54, 844, 292, 599, 236, 119, 131, 138, 827, 441, 649, 933, 1195, 802, 1176, 1119, 1146, 160, 479, 1050, 684, 996, 793, 841, 418, 1031, 178, 67, 694, 24, 924, 142, 87, 487, 1018, 640, 572, 719, 157, 567, 511, 307, 1181, 528, 1151, 211, 302, 902, 224, 217, 741, 16, 1088, 303, 70, 624, 62, 68, 1200, 1102, 758, 845, 304, 744, 1037, 1121, 333, 554, 1026, 319, 455, 593, 1138, 88, 186, 1130, 692, 946, 216, 98, 184, 363, 129, 889, 377, 880, 947, 272, 768, 440, 136, 50, 196, 457, 1077, 628, 550, 1192, 17, 904, 106, 1056, 52, 1036, 704, 870, 1126, 15, 326, 1053, 1197, 828, 1189, 861, 35, 839, 531, 943, 57, 686, 831, 150, 856, 132, 1015, 920, 1070, 893, 921, 356, 256, 248, 664, 1091, 225, 597, 38, 788, 1045, 1075, 635, 212, 562, 1160, 998, 785, 1081, 19, 1003, 34, 459, 133, 886, 1095, 716, 1153, 895, 32, 1109, 453, 33, 1159, 1047, 289, 780, 1118, 1139, 816, 876, 188, 146, 266, 976, 147, 926, 110, 1122, 95, 464, 1030, 1162, 396, 1142, 602, 867, 761, 167, 347, 612, 911, 93, 449, 743, 12, 269, 105, 488, 473, 670, 596, 948, 993, 516, 1194, 932, 1116, 490, 725, 353, 6, 352, 450, 645, 372, 875, 1061, 798, 375, 935, 293, 862, 610, 1103, 241, 371, 279, 579, 14, 598, 296, 570, 477, 952, 1210, 1177, 1106, 152, 497, 208, 900, 90, 1046, 9, 29, 846, 691, 281, 426, 805, 237, 298, 915, 1096, 313, 1134, 238, 1112, 200, 676, 166, 175, 325, 1100, 565, 931, 792, 967, 174, 938, 311, 122, 80, 1032, 485, 706, 1124, 182, 1072, 647, 498, 903, 819, 963, 873, 614, 1132, 335, 829, 374, 1128, 1188, 1179, 1089, 539, 693, 274, 390, 1111, 641, 331, 634, 626, 328, 1064, 1114, 202, 1131, 391, 601, 595, 267, 675, 187, 233, 354, 27, 161, 53, 494, 1007, 560, 10, 1094, 262, 97, 532, 318, 111, 345, 56, 164, 368, 507, 445, 401, 518, 907, 47, 754, 718, 513, 228, 1033, 467, 566, 210, 18, 104, 927, 807, 1002, 834, 569, 181, 417, 736, 1086, 403, 388, 1163, 1204, 734, 221, 872, 284, 249, 526, 357, 687, 484, 458, 1190, 75, 765, 860, 1183, 74, 918, 406, 1074, 901, 177, 393, 43, 735, 337, 615, 712, 973, 168, 149, 322, 1108, 1155, 1027, 84, 316, 137, 463, 571, 165, 314, 151, 232, 728, 894, 77, 606, 709, 930, 306, 121, 1098, 672, 589, 965, 46, 85, 1180, 835, 247, 431, 832, 961, 864, 800, 113, 985, 1136, 49, 185, 909, 978, 447, 945, 69, 727, 613, 826, 581, 533, 369, 358, 808, 609, 887, 275, 977, 336, 627, 504, 1164, 45, 577, 962, 629, 283, 578, 96, 811, 836, 1198, 486, 1171, 76, 1041, 1158, 1076, 749, 848, 1207, 855, 992, 782, 651, 451, 786, 837, 757, 752, 770, 277, 573, 1023, 833, 812, 1170, 818, 378, 282, 112, 669, 883, 1165, 892, 3, 671, 192, 1009, 1214, 1065, 1000, 1025, 386, 1085, 508, 971, 857, 843, 103, 419, 991, 11, 400, 652, 1205, 509, 724, 574, 622, 784, 191, 205, 988, 334, 1058, 327, 534, 830, 899, 158, 1123, 584, 633, 7, 397, 878, 250, 750, 984, 223, 125, 968, 737, 696, 923, 201, 1084, 1125, 680, 753, 435, 825, 229, 726, 1191, 1173, 130, 713, 796, 1024, 665, 869, 690, 273, 620, 1028, 772, 547, 919, 912, 677, 950, 1099, 852, 72, 748, 320, 1039, 204, 261, 1029, 242, 4, 705, 1209, 411, 587, 116, 461, 720, 78, 590, 364, 1001, 611, 623, 715, 552, 1093, 512, 778, 1022, 849, 639, 446, 1156, 956, 997, 527, 194, 1040, 939, 858, 1148, 1129, 476, 329, 309, 5, 688, 270, 124, 979, 1206, 1079, 603, 1140, 604, 79, 781, 1186, 1104, 551, 412, 558, 730, 350, 1055, 471, 472, 384, 127, 25, 123, 109, 642, 697, 1133, 376, 239, 766, 958, 366, 409, 806, 1113, 197, 305, 422, 227, 667, 1062, 1044, 1012, 983, 1175, 428, 1101, 308, 1010, 1172, 339, 1211, 549, 265, 775, 559, 797, 243, 395, 442, 462, 155, 863, 268, 1049, 659, 1107, 987, 218, 230, 929, 294, 822, 330, 564, 492, 506, 1005, 685, 764, 823, 809, 253, 638, 20, 1199, 874, 954, 433, 226, 738, 501, 583, 951, 773, 245, 1149, 120, 540, 470, 92, 960]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1845299487710782
the save name prefix for this run is:  chkpt-ID_1845299487710782_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 827
rank avg (pred): 0.543 +- 0.005
mrr vals (pred, true): 0.014, 0.284
batch losses (mrrl, rdl): 0.0, 0.0031385652

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 10
rank avg (pred): 0.190 +- 0.097
mrr vals (pred, true): 0.173, 0.210
batch losses (mrrl, rdl): 0.0, 2.48479e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1126
rank avg (pred): 0.423 +- 0.249
mrr vals (pred, true): 0.217, 0.051
batch losses (mrrl, rdl): 0.0, 7.0368e-06

Epoch over!
epoch time: 15.075

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 573
rank avg (pred): 0.437 +- 0.249
mrr vals (pred, true): 0.207, 0.043
batch losses (mrrl, rdl): 0.0, 7.0122e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 142
rank avg (pred): 0.412 +- 0.254
mrr vals (pred, true): 0.227, 0.049
batch losses (mrrl, rdl): 0.0, 7.6694e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 782
rank avg (pred): 0.421 +- 0.246
mrr vals (pred, true): 0.195, 0.043
batch losses (mrrl, rdl): 0.0, 7.3664e-06

Epoch over!
epoch time: 15.065

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 642
rank avg (pred): 0.431 +- 0.245
mrr vals (pred, true): 0.195, 0.042
batch losses (mrrl, rdl): 0.0, 6.4696e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1147
rank avg (pred): 0.474 +- 0.262
mrr vals (pred, true): 0.177, 0.026
batch losses (mrrl, rdl): 0.0, 2.82533e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1154
rank avg (pred): 0.456 +- 0.279
mrr vals (pred, true): 0.183, 0.024
batch losses (mrrl, rdl): 0.0, 2.69537e-05

Epoch over!
epoch time: 14.971

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 562
rank avg (pred): 0.490 +- 0.265
mrr vals (pred, true): 0.121, 0.025
batch losses (mrrl, rdl): 0.0, 1.87737e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 96
rank avg (pred): 0.442 +- 0.253
mrr vals (pred, true): 0.071, 0.054
batch losses (mrrl, rdl): 0.0, 2.452e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 173
rank avg (pred): 0.436 +- 0.266
mrr vals (pred, true): 0.110, 0.045
batch losses (mrrl, rdl): 0.0, 1.6958e-06

Epoch over!
epoch time: 14.887

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 981
rank avg (pred): 0.119 +- 0.173
mrr vals (pred, true): 0.249, 0.269
batch losses (mrrl, rdl): 0.0, 1.34806e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 347
rank avg (pred): 0.425 +- 0.264
mrr vals (pred, true): 0.093, 0.048
batch losses (mrrl, rdl): 0.0, 1.6923e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 128
rank avg (pred): 0.426 +- 0.268
mrr vals (pred, true): 0.094, 0.046
batch losses (mrrl, rdl): 0.0, 8.5356e-06

Epoch over!
epoch time: 14.955

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 727
rank avg (pred): 0.454 +- 0.254
mrr vals (pred, true): 0.048, 0.044
batch losses (mrrl, rdl): 3.44298e-05, 1.2858e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 295
rank avg (pred): 0.144 +- 0.216
mrr vals (pred, true): 0.235, 0.215
batch losses (mrrl, rdl): 0.0039728829, 5.10598e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1167
rank avg (pred): 0.483 +- 0.183
mrr vals (pred, true): 0.052, 0.032
batch losses (mrrl, rdl): 3.39457e-05, 5.04966e-05

Epoch over!
epoch time: 15.289

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 854
rank avg (pred): 0.466 +- 0.148
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 7.1321e-06, 3.82788e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 823
rank avg (pred): 0.117 +- 0.205
mrr vals (pred, true): 0.296, 0.321
batch losses (mrrl, rdl): 0.006071425, 6.4575e-06

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1080
rank avg (pred): 0.458 +- 0.138
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 1.66743e-05, 5.36116e-05

Epoch over!
epoch time: 15.288

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 739
rank avg (pred): 0.097 +- 0.168
mrr vals (pred, true): 0.306, 0.298
batch losses (mrrl, rdl): 0.0006680408, 8.0661e-06

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 452
rank avg (pred): 0.459 +- 0.131
mrr vals (pred, true): 0.048, 0.038
batch losses (mrrl, rdl): 2.97805e-05, 4.50974e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 869
rank avg (pred): 0.449 +- 0.149
mrr vals (pred, true): 0.060, 0.045
batch losses (mrrl, rdl): 0.0009083951, 4.45284e-05

Epoch over!
epoch time: 15.305

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 429
rank avg (pred): 0.454 +- 0.137
mrr vals (pred, true): 0.055, 0.050
batch losses (mrrl, rdl): 0.0002247915, 6.00226e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1117
rank avg (pred): 0.460 +- 0.128
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 4.94991e-05, 6.11859e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 630
rank avg (pred): 0.457 +- 0.131
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.31207e-05, 4.58961e-05

Epoch over!
epoch time: 15.322

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 447
rank avg (pred): 0.470 +- 0.110
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0006808558, 9.43398e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 672
rank avg (pred): 0.450 +- 0.124
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 7.52128e-05, 6.31285e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 12
rank avg (pred): 0.203 +- 0.150
mrr vals (pred, true): 0.244, 0.192
batch losses (mrrl, rdl): 0.0271445848, 1.64472e-05

Epoch over!
epoch time: 15.314

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 656
rank avg (pred): 0.439 +- 0.128
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 4.46938e-05, 4.82296e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 766
rank avg (pred): 0.442 +- 0.135
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.0001833707, 4.10547e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 594
rank avg (pred): 0.443 +- 0.107
mrr vals (pred, true): 0.042, 0.037
batch losses (mrrl, rdl): 0.0006800998, 6.42796e-05

Epoch over!
epoch time: 15.275

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 662
rank avg (pred): 0.449 +- 0.109
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003530287, 5.9886e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 105
rank avg (pred): 0.442 +- 0.119
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 8.32146e-05, 4.83101e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 980
rank avg (pred): 0.162 +- 0.145
mrr vals (pred, true): 0.282, 0.284
batch losses (mrrl, rdl): 6.59282e-05, 1.09774e-05

Epoch over!
epoch time: 15.201

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 749
rank avg (pred): 0.109 +- 0.148
mrr vals (pred, true): 0.310, 0.303
batch losses (mrrl, rdl): 0.0004724074, 7.533e-07

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 550
rank avg (pred): 0.485 +- 0.138
mrr vals (pred, true): 0.045, 0.026
batch losses (mrrl, rdl): 0.0002473779, 2.72229e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1017
rank avg (pred): 0.416 +- 0.139
mrr vals (pred, true): 0.065, 0.044
batch losses (mrrl, rdl): 0.0022789971, 6.53123e-05

Epoch over!
epoch time: 15.095

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 937
rank avg (pred): 0.432 +- 0.111
mrr vals (pred, true): 0.048, 0.037
batch losses (mrrl, rdl): 2.42072e-05, 5.94755e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 103
rank avg (pred): 0.434 +- 0.110
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 1.27603e-05, 5.78396e-05

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 588
rank avg (pred): 0.433 +- 0.111
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 6.2442e-06, 5.7624e-05

Epoch over!
epoch time: 15.083

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 222
rank avg (pred): 0.430 +- 0.113
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 4.9137e-06, 5.65924e-05

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 15
rank avg (pred): 0.317 +- 0.192
mrr vals (pred, true): 0.222, 0.208
batch losses (mrrl, rdl): 0.0017626433, 0.000431675

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 18
rank avg (pred): 0.326 +- 0.201
mrr vals (pred, true): 0.234, 0.184
batch losses (mrrl, rdl): 0.0252630655, 0.0004604419

Epoch over!
epoch time: 15.023

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.424 +- 0.107
mrr vals (pred, true): 0.050, 0.041

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   12 	     0 	 0.04641 	 0.01773 	 ~...
   11 	     1 	 0.04602 	 0.02072 	 ~...
   27 	     2 	 0.04839 	 0.02072 	 ~...
    5 	     3 	 0.04501 	 0.02149 	 ~...
    1 	     4 	 0.04460 	 0.02263 	 ~...
    3 	     5 	 0.04476 	 0.02292 	 ~...
    2 	     6 	 0.04467 	 0.02302 	 ~...
    6 	     7 	 0.04529 	 0.02304 	 ~...
    8 	     8 	 0.04551 	 0.02442 	 ~...
   31 	     9 	 0.04866 	 0.02476 	 ~...
    9 	    10 	 0.04562 	 0.02496 	 ~...
   10 	    11 	 0.04563 	 0.02520 	 ~...
    0 	    12 	 0.04383 	 0.02536 	 ~...
    4 	    13 	 0.04478 	 0.02586 	 ~...
    7 	    14 	 0.04535 	 0.02623 	 ~...
   83 	    15 	 0.05034 	 0.03018 	 ~...
   72 	    16 	 0.05002 	 0.03029 	 ~...
   67 	    17 	 0.04985 	 0.03139 	 ~...
   93 	    18 	 0.05119 	 0.03237 	 ~...
   70 	    19 	 0.04994 	 0.03388 	 ~...
   92 	    20 	 0.05117 	 0.03448 	 ~...
   95 	    21 	 0.05191 	 0.03500 	 ~...
   80 	    22 	 0.05016 	 0.03629 	 ~...
   39 	    23 	 0.04904 	 0.03637 	 ~...
   75 	    24 	 0.05007 	 0.03699 	 ~...
   84 	    25 	 0.05036 	 0.03825 	 ~...
   77 	    26 	 0.05014 	 0.03971 	 ~...
   79 	    27 	 0.05015 	 0.03973 	 ~...
   44 	    28 	 0.04926 	 0.04022 	 ~...
   46 	    29 	 0.04932 	 0.04034 	 ~...
   66 	    30 	 0.04979 	 0.04053 	 ~...
   69 	    31 	 0.04987 	 0.04070 	 ~...
   34 	    32 	 0.04875 	 0.04091 	 ~...
   35 	    33 	 0.04888 	 0.04101 	 ~...
   45 	    34 	 0.04927 	 0.04106 	 ~...
   28 	    35 	 0.04844 	 0.04118 	 ~...
   20 	    36 	 0.04794 	 0.04132 	 ~...
   47 	    37 	 0.04934 	 0.04164 	 ~...
   63 	    38 	 0.04967 	 0.04206 	 ~...
   55 	    39 	 0.04952 	 0.04238 	 ~...
   56 	    40 	 0.04952 	 0.04286 	 ~...
   41 	    41 	 0.04909 	 0.04311 	 ~...
   30 	    42 	 0.04858 	 0.04319 	 ~...
   22 	    43 	 0.04818 	 0.04327 	 ~...
   78 	    44 	 0.05015 	 0.04336 	 ~...
   48 	    45 	 0.04935 	 0.04364 	 ~...
   85 	    46 	 0.05036 	 0.04367 	 ~...
   96 	    47 	 0.05193 	 0.04369 	 ~...
   13 	    48 	 0.04668 	 0.04369 	 ~...
   94 	    49 	 0.05130 	 0.04397 	 ~...
   51 	    50 	 0.04940 	 0.04471 	 ~...
   49 	    51 	 0.04937 	 0.04473 	 ~...
   25 	    52 	 0.04836 	 0.04483 	 ~...
   16 	    53 	 0.04726 	 0.04493 	 ~...
   17 	    54 	 0.04756 	 0.04504 	 ~...
   59 	    55 	 0.04959 	 0.04505 	 ~...
   86 	    56 	 0.05038 	 0.04559 	 ~...
   82 	    57 	 0.05030 	 0.04609 	 ~...
   74 	    58 	 0.05006 	 0.04617 	 ~...
   18 	    59 	 0.04782 	 0.04629 	 ~...
   57 	    60 	 0.04953 	 0.04641 	 ~...
   32 	    61 	 0.04870 	 0.04658 	 ~...
   40 	    62 	 0.04908 	 0.04659 	 ~...
   58 	    63 	 0.04955 	 0.04670 	 ~...
   62 	    64 	 0.04964 	 0.04679 	 ~...
   52 	    65 	 0.04945 	 0.04736 	 ~...
   15 	    66 	 0.04696 	 0.04759 	 ~...
   23 	    67 	 0.04825 	 0.04763 	 ~...
   64 	    68 	 0.04975 	 0.04764 	 ~...
   50 	    69 	 0.04940 	 0.04771 	 ~...
   71 	    70 	 0.04996 	 0.04788 	 ~...
   24 	    71 	 0.04832 	 0.04789 	 ~...
   68 	    72 	 0.04986 	 0.04793 	 ~...
   36 	    73 	 0.04897 	 0.04819 	 ~...
   53 	    74 	 0.04945 	 0.04819 	 ~...
   43 	    75 	 0.04918 	 0.04836 	 ~...
   81 	    76 	 0.05020 	 0.04866 	 ~...
   73 	    77 	 0.05004 	 0.04869 	 ~...
   38 	    78 	 0.04903 	 0.04891 	 ~...
   90 	    79 	 0.05072 	 0.04898 	 ~...
   61 	    80 	 0.04962 	 0.04901 	 ~...
   97 	    81 	 0.05196 	 0.04904 	 ~...
   14 	    82 	 0.04668 	 0.04939 	 ~...
   42 	    83 	 0.04909 	 0.04960 	 ~...
   26 	    84 	 0.04838 	 0.04968 	 ~...
   37 	    85 	 0.04898 	 0.04974 	 ~...
   76 	    86 	 0.05010 	 0.04989 	 ~...
   33 	    87 	 0.04874 	 0.05019 	 ~...
   99 	    88 	 0.05411 	 0.05022 	 ~...
   91 	    89 	 0.05101 	 0.05030 	 ~...
   54 	    90 	 0.04949 	 0.05087 	 ~...
   89 	    91 	 0.05068 	 0.05108 	 ~...
   87 	    92 	 0.05047 	 0.05116 	 ~...
   19 	    93 	 0.04789 	 0.05206 	 ~...
   21 	    94 	 0.04810 	 0.05324 	 ~...
   65 	    95 	 0.04977 	 0.05434 	 ~...
   88 	    96 	 0.05062 	 0.05439 	 ~...
   60 	    97 	 0.04960 	 0.05498 	 ~...
   98 	    98 	 0.05309 	 0.05518 	 ~...
   29 	    99 	 0.04847 	 0.05868 	 ~...
  100 	   100 	 0.19765 	 0.18285 	 ~...
  105 	   101 	 0.21592 	 0.19986 	 ~...
  100 	   102 	 0.19765 	 0.20036 	 ~...
  103 	   103 	 0.20970 	 0.20501 	 ~...
  111 	   104 	 0.23698 	 0.20511 	 m..s
  109 	   105 	 0.23065 	 0.21142 	 ~...
  102 	   106 	 0.20408 	 0.21817 	 ~...
  107 	   107 	 0.22834 	 0.22269 	 ~...
  114 	   108 	 0.25379 	 0.24107 	 ~...
  106 	   109 	 0.22429 	 0.24515 	 ~...
  112 	   110 	 0.23957 	 0.24618 	 ~...
  104 	   111 	 0.21136 	 0.24793 	 m..s
  110 	   112 	 0.23625 	 0.25289 	 ~...
  108 	   113 	 0.22987 	 0.25592 	 ~...
  113 	   114 	 0.24272 	 0.28209 	 m..s
  116 	   115 	 0.29339 	 0.28654 	 ~...
  119 	   116 	 0.31386 	 0.28722 	 ~...
  117 	   117 	 0.30551 	 0.29633 	 ~...
  115 	   118 	 0.26735 	 0.29647 	 ~...
  118 	   119 	 0.31132 	 0.31748 	 ~...
  120 	   120 	 0.35881 	 0.53924 	 MISS
==========================================
r_mrr = 0.9771844148635864
r2_mrr = 0.9432370662689209
spearmanr_mrr@5 = 0.9695448875427246
spearmanr_mrr@10 = 0.8182171583175659
spearmanr_mrr@50 = 0.9761423468589783
spearmanr_mrr@100 = 0.9821151494979858
spearmanr_mrr@All = 0.9813913702964783
==========================================
test time: 0.453
Done Testing dataset UMLS
total time taken: 233.8458812236786
training time taken: 227.6123993396759
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'ComplEx': {'UMLS': tensor(0.9772)}}, 'r2_mrr': {'ComplEx': {'UMLS': tensor(0.9432)}}, 'spearmanr_mrr@5': {'ComplEx': {'UMLS': tensor(0.9695)}}, 'spearmanr_mrr@10': {'ComplEx': {'UMLS': tensor(0.8182)}}, 'spearmanr_mrr@50': {'ComplEx': {'UMLS': tensor(0.9761)}}, 'spearmanr_mrr@100': {'ComplEx': {'UMLS': tensor(0.9821)}}, 'spearmanr_mrr@All': {'ComplEx': {'UMLS': tensor(0.9814)}}, 'test_loss': {'ComplEx': {'UMLS': 0.42529979025857756}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num rels', 's num rels'}

