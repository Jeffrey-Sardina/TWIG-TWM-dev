using splits:
test_ids (121): [737, 952, 400, 33, 314, 1131, 490, 406, 172, 65, 372, 1041, 532, 1100, 459, 581, 370, 1149, 174, 136, 570, 193, 673, 428, 811, 971, 772, 655, 101, 341, 443, 374, 366, 463, 320, 482, 505, 859, 627, 470, 9, 298, 523, 509, 468, 414, 295, 527, 164, 734, 77, 562, 203, 1012, 501, 1068, 175, 566, 758, 431, 183, 379, 460, 598, 998, 877, 559, 1201, 214, 429, 130, 821, 205, 39, 994, 434, 541, 867, 671, 323, 353, 385, 890, 309, 1103, 462, 290, 795, 162, 256, 229, 437, 815, 775, 792, 1080, 720, 421, 213, 210, 139, 71, 456, 487, 733, 1146, 1056, 275, 2, 537, 729, 479, 178, 1116, 619, 592, 149, 74, 597, 1019, 59]
valid_ids (0): []
train_ids (1082): [404, 823, 417, 1199, 1200, 626, 357, 255, 422, 1143, 109, 1043, 303, 497, 576, 868, 350, 1144, 1142, 645, 299, 930, 122, 885, 1114, 499, 956, 107, 128, 938, 286, 98, 725, 878, 716, 236, 359, 1040, 863, 761, 1139, 24, 788, 508, 1082, 354, 525, 1195, 493, 270, 809, 176, 965, 785, 23, 724, 796, 906, 652, 1209, 243, 97, 1210, 557, 612, 16, 52, 940, 768, 215, 869, 129, 818, 682, 476, 943, 348, 1151, 263, 252, 1207, 170, 549, 951, 683, 703, 961, 640, 231, 89, 1138, 954, 571, 882, 837, 540, 628, 727, 444, 783, 926, 920, 553, 1162, 778, 1160, 1072, 600, 103, 63, 528, 445, 326, 621, 542, 73, 909, 102, 274, 1090, 908, 637, 990, 44, 306, 717, 11, 548, 1190, 631, 590, 641, 64, 376, 27, 160, 296, 819, 494, 836, 147, 651, 339, 516, 209, 524, 744, 833, 485, 60, 268, 511, 1093, 762, 446, 770, 810, 642, 146, 206, 1051, 1132, 90, 1014, 916, 848, 936, 92, 272, 782, 1036, 931, 610, 94, 1003, 672, 32, 794, 1032, 664, 790, 644, 383, 199, 285, 384, 211, 827, 1211, 1205, 273, 1000, 604, 378, 1021, 329, 118, 216, 283, 873, 680, 62, 647, 977, 1081, 767, 224, 830, 700, 401, 1130, 835, 622, 1015, 405, 331, 981, 786, 669, 271, 58, 1064, 334, 185, 876, 874, 1063, 791, 36, 741, 556, 1092, 555, 1192, 646, 304, 301, 968, 132, 934, 630, 1147, 43, 1198, 1156, 580, 144, 755, 797, 1004, 318, 564, 471, 677, 397, 204, 486, 227, 244, 86, 693, 691, 1097, 457, 807, 864, 61, 975, 857, 19, 674, 230, 698, 31, 161, 711, 1170, 927, 158, 141, 466, 391, 903, 735, 240, 234, 1024, 569, 87, 840, 1171, 1074, 958, 382, 1048, 565, 601, 461, 614, 1010, 1214, 847, 852, 316, 862, 1159, 208, 1065, 543, 1167, 12, 478, 574, 1059, 402, 850, 1108, 1164, 247, 297, 855, 338, 1011, 302, 529, 248, 856, 573, 135, 335, 535, 595, 567, 708, 638, 394, 731, 17, 765, 939, 993, 344, 992, 177, 865, 1124, 430, 596, 407, 408, 892, 839, 728, 656, 1202, 312, 793, 69, 425, 784, 116, 1098, 96, 393, 803, 79, 932, 438, 748, 300, 661, 1137, 265, 454, 56, 880, 1153, 313, 901, 999, 440, 660, 829, 732, 279, 30, 1152, 399, 1185, 191, 1067, 1053, 841, 676, 960, 345, 150, 771, 838, 872, 453, 1125, 398, 389, 757, 123, 442, 1168, 870, 427, 419, 1155, 311, 435, 586, 957, 937, 217, 989, 915, 328, 280, 212, 1060, 832, 452, 946, 603, 618, 131, 914, 266, 568, 202, 242, 380, 650, 776, 589, 824, 469, 745, 1115, 200, 684, 1181, 1091, 1148, 1150, 663, 245, 632, 488, 1077, 387, 561, 154, 518, 226, 145, 392, 657, 893, 982, 420, 111, 1066, 481, 602, 616, 753, 881, 261, 180, 1183, 492, 337, 985, 143, 235, 85, 477, 560, 751, 196, 774, 1086, 681, 159, 808, 246, 846, 108, 510, 1121, 1039, 587, 395, 289, 363, 1141, 1075, 258, 779, 685, 37, 293, 45, 843, 142, 1028, 1029, 410, 1076, 491, 812, 228, 292, 104, 95, 1023, 1052, 294, 110, 1135, 582, 904, 168, 696, 390, 884, 432, 976, 688, 907, 218, 1037, 594, 902, 773, 912, 1078, 232, 545, 7, 653, 666, 822, 500, 277, 895, 3, 900, 70, 588, 250, 973, 78, 764, 514, 106, 742, 368, 522, 364, 308, 633, 845, 701, 659, 577, 377, 451, 978, 1163, 13, 719, 1174, 752, 648, 1070, 155, 156, 613, 195, 257, 842, 1180, 828, 151, 51, 403, 875, 480, 861, 281, 533, 1129, 1191, 327, 259, 249, 57, 439, 1047, 219, 506, 899, 583, 1208, 558, 709, 458, 584, 187, 207, 100, 346, 473, 723, 512, 760, 980, 918, 620, 321, 38, 704, 769, 269, 6, 342, 1062, 264, 221, 72, 635, 817, 1187, 608, 1118, 536, 789, 157, 1013, 519, 1173, 502, 93, 1126, 552, 225, 138, 29, 718, 386, 924, 464, 315, 1054, 84, 365, 88, 343, 959, 1179, 966, 554, 1085, 125, 1026, 148, 591, 349, 1017, 1087, 911, 787, 941, 813, 997, 333, 367, 369, 1007, 330, 360, 169, 1044, 629, 467, 675, 953, 251, 1140, 182, 935, 222, 189, 1034, 1020, 239, 233, 974, 969, 714, 665, 513, 267, 860, 801, 1197, 475, 1104, 886, 1009, 679, 1055, 572, 707, 1042, 970, 133, 1033, 802, 575, 192, 35, 923, 496, 68, 1084, 26, 942, 416, 1095, 362, 706, 1177, 388, 396, 40, 1083, 825, 988, 749, 188, 1109, 799, 831, 426, 739, 721, 798, 310, 50, 689, 49, 1212, 436, 1101, 165, 910, 746, 887, 634, 352, 5, 919, 351, 578, 1184, 891, 921, 1203, 917, 319, 962, 126, 153, 547, 1196, 668, 124, 692, 1165, 1111, 465, 1134, 1006, 984, 375, 849, 121, 282, 544, 538, 922, 925, 678, 715, 1106, 517, 448, 18, 1158, 447, 223, 1079, 928, 1120, 1071, 28, 1206, 585, 423, 91, 1186, 740, 699, 253, 713, 105, 995, 1110, 455, 0, 409, 539, 1049, 743, 173, 450, 654, 534, 1069, 670, 806, 184, 241, 197, 507, 441, 1094, 894, 826, 412, 167, 171, 1096, 53, 47, 747, 1172, 780, 690, 347, 356, 1123, 1035, 972, 42, 14, 41, 1025, 609, 550, 22, 697, 1102, 963, 498, 759, 137, 1128, 866, 967, 284, 858, 624, 194, 1117, 1089, 114, 1136, 777, 411, 1038, 504, 1193, 766, 1127, 1008, 433, 750, 1188, 605, 611, 413, 814, 1016, 186, 483, 726, 639, 46, 489, 948, 1, 649, 1057, 238, 1018, 987, 955, 888, 805, 472, 1213, 325, 67, 4, 834, 736, 1169, 933, 317, 336, 983, 1122, 1005, 607, 503, 515, 694, 1145, 1133, 979, 945, 667, 76, 332, 521, 152, 643, 10, 1194, 897, 201, 1178, 913, 687, 1099, 658, 756, 1031, 127, 83, 871, 686, 1073, 322, 1001, 1154, 1088, 474, 854, 324, 99, 291, 695, 623, 1112, 905, 625, 179, 198, 986, 119, 1002, 702, 134, 287, 190, 754, 163, 964, 1050, 763, 563, 80, 21, 278, 373, 1027, 636, 54, 307, 220, 996, 340, 1161, 1046, 112, 25, 82, 896, 1045, 853, 947, 816, 15, 949, 166, 20, 730, 781, 66, 844, 1189, 1119, 738, 117, 944, 381, 599, 424, 520, 889, 615, 81, 898, 484, 991, 115, 593, 1176, 546, 1105, 1022, 820, 276, 950, 181, 305, 883, 75, 260, 113, 851, 1175, 879, 722, 530, 526, 1157, 415, 418, 55, 1166, 48, 551, 237, 120, 929, 1058, 662, 1061, 579, 361, 531, 1030, 804, 705, 1113, 371, 1204, 710, 8, 254, 712, 800]
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0
rank avg (pred, true): 0.510, 0.090
rank std (pred, true): 0.003, 0.147
mrr vals (pred, true): 0.014, 0.379
losses (mrrl, rdl): 0.0, 0.0001518863

running batch: 500
rank avg (pred, true): 0.385, 0.578
rank std (pred, true): 0.010, 0.177
mrr vals (pred, true): 0.019, 0.020
losses (mrrl, rdl): 0.0, 6.52e-08

running batch: 1000
rank avg (pred, true): 0.375, 0.442
rank std (pred, true): 0.008, 0.261
mrr vals (pred, true): 0.020, 0.047
losses (mrrl, rdl): 0.0, 9.3e-09

running batch: 1500
rank avg (pred, true): 0.369, 0.094
rank std (pred, true): 0.007, 0.150
mrr vals (pred, true): 0.020, 0.388
losses (mrrl, rdl): 0.0, 1.27e-07

running batch: 2000
rank avg (pred, true): 0.390, 0.453
rank std (pred, true): 0.003, 0.257
mrr vals (pred, true): 0.019, 0.043
losses (mrrl, rdl): 0.0, 8.3e-09

Epoch over!
epoch time: 45.144
loss values:
	mrrl: 0.0
	rdl: 1.49352e-05

Epoch 2 -- 
running batch: 0
rank avg (pred, true): 0.385, 0.432
rank std (pred, true): 0.003, 0.264
mrr vals (pred, true): 0.019, 0.050
losses (mrrl, rdl): 0.0, 2.6266e-06

running batch: 500
rank avg (pred, true): 0.367, 0.442
rank std (pred, true): 0.002, 0.270
mrr vals (pred, true): 0.020, 0.052
losses (mrrl, rdl): 0.0, 1.14e-08

running batch: 1000
rank avg (pred, true): 0.397, 0.454
rank std (pred, true): 0.001, 0.263
mrr vals (pred, true): 0.018, 0.042
losses (mrrl, rdl): 0.0, 7.1e-09

running batch: 1500
rank avg (pred, true): 0.407, 0.439
rank std (pred, true): 0.000, 0.260
mrr vals (pred, true): 0.018, 0.042
losses (mrrl, rdl): 0.0, 3e-09

running batch: 2000
rank avg (pred, true): 0.384, 0.207
rank std (pred, true): 0.000, 0.217
mrr vals (pred, true): 0.019, 0.180
losses (mrrl, rdl): 0.0, 5.58e-08

Epoch over!
epoch time: 42.527
loss values:
	mrrl: 0.0
	rdl: 1.47258e-05

Done training phase:  0
Epoch 1 -- 
running batch: 0
rank avg (pred, true): 0.390, 0.454
rank std (pred, true): 0.000, 0.265
mrr vals (pred, true): 0.019, 0.042
losses (mrrl, rdl): 0.0052289832, 4.2354e-06

running batch: 500
rank avg (pred, true): 0.104, 0.463
rank std (pred, true): 0.005, 0.254
mrr vals (pred, true): 0.067, 0.039
losses (mrrl, rdl): 1.56826e-05, 2.246e-07

running batch: 1000
rank avg (pred, true): 0.092, 0.443
rank std (pred, true): 0.003, 0.264
mrr vals (pred, true): 0.075, 0.051
losses (mrrl, rdl): 1.17251e-05, 2.195e-07

running batch: 1500
rank avg (pred, true): 0.086, 0.460
rank std (pred, true): 0.002, 0.253
mrr vals (pred, true): 0.080, 0.041
losses (mrrl, rdl): 2.95533e-05, 2.464e-07

running batch: 2000
rank avg (pred, true): 0.067, 0.427
rank std (pred, true): 0.002, 0.254
mrr vals (pred, true): 0.100, 0.048
losses (mrrl, rdl): 5.52153e-05, 2.312e-07

Epoch over!
epoch time: 44.418
loss values:
	mrrl: 0.0846977369
	rdl: 9.50186e-05

Epoch 2 -- 
running batch: 0
rank avg (pred, true): 0.059, 0.583
rank std (pred, true): 0.001, 0.180
mrr vals (pred, true): 0.112, 0.020
losses (mrrl, rdl): 0.084057413, 0.0002379907

running batch: 500
rank avg (pred, true): 0.060, 0.461
rank std (pred, true): 0.001, 0.263
mrr vals (pred, true): 0.110, 0.047
losses (mrrl, rdl): 7.78199e-05, 2.845e-07

running batch: 1000
rank avg (pred, true): 0.094, 0.591
rank std (pred, true): 0.000, 0.181
mrr vals (pred, true): 0.074, 0.022
losses (mrrl, rdl): 5.36682e-05, 4.322e-07

running batch: 1500
rank avg (pred, true): 0.088, 0.514
rank std (pred, true): 0.000, 0.211
mrr vals (pred, true): 0.078, 0.025
losses (mrrl, rdl): 5.74233e-05, 3.175e-07

running batch: 2000
rank avg (pred, true): 0.081, 0.180
rank std (pred, true): 0.000, 0.203
mrr vals (pred, true): 0.084, 0.205
losses (mrrl, rdl): 0.0002913605, 1.78e-08

Epoch over!
epoch time: 43.317
loss values:
	mrrl: 0.082297354
	rdl: 9.87552e-05

Epoch 3 -- 
running batch: 0
rank avg (pred, true): 0.075, 0.496
rank std (pred, true): 0.000, 0.235
mrr vals (pred, true): 0.091, 0.030
losses (mrrl, rdl): 0.0364964008, 0.0001542963

running batch: 500
rank avg (pred, true): 0.080, 0.452
rank std (pred, true): 0.000, 0.266
mrr vals (pred, true): 0.085, 0.047
losses (mrrl, rdl): 2.98503e-05, 2.432e-07

running batch: 1000
rank avg (pred, true): 0.081, 0.510
rank std (pred, true): 0.000, 0.220
mrr vals (pred, true): 0.085, 0.027
losses (mrrl, rdl): 6.66923e-05, 3.236e-07

running batch: 1500
rank avg (pred, true): 0.078, 0.135
rank std (pred, true): 0.000, 0.183
mrr vals (pred, true): 0.087, 0.309
losses (mrrl, rdl): 0.0009824546, 6e-09

running batch: 2000
rank avg (pred, true): 0.080, 0.499
rank std (pred, true): 0.000, 0.211
mrr vals (pred, true): 0.085, 0.026
losses (mrrl, rdl): 7.0908e-05, 3.1e-07

Epoch over!
epoch time: 43.059
loss values:
	mrrl: 0.0819195394
	rdl: 9.94639e-05

Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred, true): 0.092, 0.152
rank std (pred, true): 0.000, 0.180
mrr vals (pred, true): 0.075, 0.293

Testing data for dataloader(s) UMLS
==========================================

Predicted MRRs
------------------------------------------
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07789133489131927
0.0756606012582779
0.07517789304256439
0.08180087059736252
0.08332029730081558
0.08088991791009903
0.0808391347527504
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07788090407848358
0.08334725350141525
0.07935848087072372
0.07517789304256439
0.07517789304256439
0.07954919338226318
0.08332809060811996
0.08182697743177414
0.0819205716252327
0.07517789304256439
0.07517789304256439
0.08200482279062271
0.07517789304256439
0.07517789304256439
0.07776078581809998
0.07517789304256439
0.07566027343273163
0.08083236962556839
0.07517789304256439
0.07572866976261139
0.08191350847482681
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07960715889930725
0.07778458297252655
0.08083237707614899
0.08091528713703156
0.08332067728042603
0.08077289164066315
0.07517789304256439
0.08090909570455551
0.08090850710868835
0.07517789304256439
0.07935883104801178
0.08327480405569077
0.07579204440116882
0.07517789304256439
0.07517789304256439
0.07947434484958649
0.07954514771699905
0.07563023269176483
0.07517789304256439
0.07952404767274857
0.07517789304256439
0.07795056700706482
0.07517789304256439
0.07517789304256439
0.08332090824842453
0.08332090824842453
0.08180168271064758
0.07517789304256439
0.07960715144872665
0.07517789304256439
0.0794297382235527
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07794452458620071
0.07571221143007278
0.07954954355955124
0.07954493165016174
0.08326820284128189
0.08320692181587219
0.08097390830516815
0.07517789304256439
0.07517789304256439
0.08326820284128189
0.07786187529563904
0.07517789304256439
0.07954268157482147
0.0757245346903801
0.08193942159414291
0.07517789304256439
0.08098147809505463
0.08335386961698532
0.07951696217060089
0.07952460646629333
0.0818617045879364
0.07578583806753159
0.08201272785663605
0.08342184871435165
0.08334182947874069
0.07517789304256439
0.07570552080869675
0.07517789304256439
0.07517789304256439
0.0819205716252327
0.07566676288843155
0.07517789304256439
0.08320632576942444
0.07961325347423553
0.08079376071691513
0.08090360462665558
0.08186149597167969
0.08327458053827286
0.07517789304256439
0.08342102915048599
0.07517789304256439
0.07571171224117279
0.08334244042634964
0.07517789304256439
0.07517789304256439
0.07947434484958649
0.07517789304256439
0.0818275511264801
0.07788725942373276
0.08186855912208557
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07789133489131927
0.0756606012582779
0.07517789304256439
0.08180087059736252
0.08332029730081558
0.08088991791009903
0.0808391347527504
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07788090407848358
0.08334725350141525
0.07935848087072372
0.07517789304256439
0.07517789304256439
0.07954919338226318
0.08332809060811996
0.08182697743177414
0.0819205716252327
0.07517789304256439
0.07517789304256439
0.08200482279062271
0.07517789304256439
0.07517789304256439
0.07776078581809998
0.07517789304256439
0.07566027343273163
0.08083236962556839
0.07517789304256439
0.07572866976261139
0.08191350847482681
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07960715889930725
0.07778458297252655
0.08083237707614899
0.08091528713703156
0.08332067728042603
0.08077289164066315
0.07517789304256439
0.08090909570455551
0.08090850710868835
0.07517789304256439
0.07935883104801178
0.08327480405569077
0.07579204440116882
0.07517789304256439
0.07517789304256439
0.07947434484958649
0.07954514771699905
0.07563023269176483
0.07517789304256439
0.07952404767274857
0.07517789304256439
0.07795056700706482
0.07517789304256439
0.07517789304256439
0.08332090824842453
0.08332090824842453
0.08180168271064758
0.07517789304256439
0.07960715144872665
0.07517789304256439
0.0794297382235527
0.07517789304256439
0.07517789304256439
0.07517789304256439
0.07794452458620071
0.07571221143007278
0.07954954355955124
0.07954493165016174
0.08326820284128189
0.08320692181587219
0.08097390830516815
0.07517789304256439
0.07517789304256439
0.08326820284128189
0.07786187529563904
0.07517789304256439
0.07954268157482147
0.0757245346903801
0.08193942159414291
0.07517789304256439
0.08098147809505463
0.08335386961698532
0.07951696217060089
0.07952460646629333
0.0818617045879364
0.07578583806753159
0.08201272785663605
0.08342184871435165
0.08334182947874069
0.07517789304256439
0.07570552080869675
0.07517789304256439
0.07517789304256439
0.0819205716252327
0.07566676288843155
0.07517789304256439
0.08320632576942444
0.07961325347423553
0.08079376071691513
0.08090360462665558
0.08186149597167969
0.08327458053827286
0.07517789304256439
0.08342102915048599
0.07517789304256439
0.07571171224117279
0.08334244042634964
0.07517789304256439
0.07517789304256439
0.07947434484958649
0.07517789304256439
0.0818275511264801
0.07788725942373276
0.08186855912208557

True MRRs
------------------------------------------
0.2933429181575775
0.04894914850592613
0.052330438047647476
0.21188470721244812
0.2087239921092987
0.043131161481142044
0.024965764954686165
0.053674906492233276
0.047111403197050095
0.21735121309757233
0.0485723577439785
0.04910529777407646
0.024967318400740623
0.051105983555316925
0.06247895956039429
0.04329434782266617
0.0561051107943058
0.02634074166417122
0.043796293437480927
0.04276314750313759
0.041882168501615524
0.04315338283777237
0.04206246882677078
0.05189264938235283
0.4012860059738159
0.053712714463472366
0.05018715560436249
0.053577572107315063
0.04988525062799454
0.043359920382499695
0.04763151332736015
0.04506322368979454
0.04142684489488602
0.0437612421810627
0.19711504876613617
0.05086950212717056
0.029514744877815247
0.04673737660050392
0.03810818865895271
0.0497579388320446
0.21805109083652496
0.24954774975776672
0.021644406020641327
0.02522987127304077
0.04906542971730232
0.04542294517159462
0.21511073410511017
0.02530823089182377
0.04569697007536888
0.527546763420105
0.20694199204444885
0.02494025230407715
0.04873070493340492
0.04035703465342522
0.023699799552559853
0.27862465381622314
0.05530436709523201
0.022825516760349274
0.04896436259150505
0.05583959445357323
0.04563980922102928
0.04154709726572037
0.0440678633749485
0.0395006388425827
0.28743401169776917
0.0459364615380764
0.02578861080110073
0.052132781594991684
0.049160003662109375
0.04973667114973068
0.04665983468294144
0.3237551748752594
0.054659076035022736
0.21817156672477722
0.2749718427658081
0.048676349222660065
0.024276068434119225
0.04894132912158966
0.046802714467048645
0.22591783106327057
0.051474858075380325
0.05448304861783981
0.04939459636807442
0.2124388962984085
0.04239540919661522
0.046872787177562714
0.20295372605323792
0.04132223501801491
0.04917151853442192
0.19544872641563416
0.04760483652353287
0.047888096421957016
0.539236307144165
0.042549338191747665
0.04412742704153061
0.042412299662828445
0.0475965179502964
0.047488581389188766
0.0466027557849884
0.04448326304554939
0.04440216347575188
0.2239893227815628
0.05007190257310867
0.02356334961950779
0.5222266316413879
0.023421667516231537
0.29009348154067993
0.2640892267227173
0.25434398651123047
0.02397974207997322
0.37350213527679443
0.04898177832365036
0.04271852225065231
0.04838435351848602
0.04163471236824989
0.04019593447446823
0.046081915497779846
0.22716620564460754
0.036817263811826706
0.04549330100417137
0.26710519194602966
0.29170307517051697
0.03873639926314354
0.04887552559375763
0.21826569736003876
0.22490499913692474
0.053078703582286835
0.02561151422560215
0.045300744473934174
0.04507102817296982
0.22899894416332245
0.039896219968795776
0.04221995919942856
0.024652302265167236
0.04322371259331703
0.05477889999747276
0.04856272041797638
0.043716732412576675
0.02358461730182171
0.0530950203537941
0.05122962221503258
0.0440521277487278
0.04952201247215271
0.0402698740363121
0.04968045651912689
0.439321368932724
0.047046590596437454
0.047289926558732986
0.04898383840918541
0.0446520559489727
0.04849739372730255
0.05133156478404999
0.05418115109205246
0.04412993788719177
0.048782091587781906
0.2040712684392929
0.04094018414616585
0.023108310997486115
0.046395957469940186
0.03740327060222626
0.04482435807585716
0.21149252355098724
0.25558286905288696
0.025617599487304688
0.024972569197416306
0.05250078812241554
0.04704217612743378
0.19881661236286163
0.02659224160015583
0.04734626039862633
0.5166931748390198
0.19615748524665833
0.024938728660345078
0.05064123868942261
0.04912451654672623
0.025235667824745178
0.2699081003665924
0.05482087284326553
0.024883754551410675
0.04297412559390068
0.05545489117503166
0.0439322330057621
0.04371722415089607
0.06075972318649292
0.04512091353535652
0.29482901096343994
0.04077114164829254
0.02093230001628399
0.05163269862532616
0.05349745228886604
0.04705798625946045
0.04833891615271568
0.31115442514419556
0.04748158901929855
0.20854902267456055
0.31670305132865906
0.04329497739672661
0.02312866784632206
0.045520342886447906
0.04505467042326927
0.21029967069625854
0.04742934927344322
0.047009244561195374
0.04518929496407509
0.1807640939950943
0.044104624539613724
0.05632684752345085
0.22156009078025818
0.04081650823354721
0.04047299921512604
0.22706596553325653
0.044477637857198715
0.05137067288160324
0.5725268721580505
0.05319836363196373
0.04989299178123474
0.04066719487309456
0.05066550895571709
0.04714187607169151
0.05004604905843735
0.046208590269088745
0.04870794340968132
0.23564931750297546
0.04650633782148361
0.022325072437524796
0.5190045833587646
0.0270525049418211
0.26543691754341125
0.2575664222240448
0.2615586817264557
0.024184217676520348
0.41442370414733887
0.04730156809091568
0.05578451231122017
0.04910745099186897
0.037044428288936615
0.04150528460741043
0.04658273980021477
0.22660984098911285
0.04653491824865341
0.043970681726932526
0.25326257944107056

r_mrr = tensor([[1.0000, 0.0091],
        [0.0091, 1.0000]])
r2_mrr = -0.03866446018218994
test_loss: 32.407951075350866
	test time: 1.987
Done Testing dataset UMLS
total time taken: 225.8453450202942
training time taken: 220.45924139022827
