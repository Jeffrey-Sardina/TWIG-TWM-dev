Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_test-kgems
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [10, 0]
rank_dist_loss_coeffs: [1, 0]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 10 and 2: 0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.423 +- 0.005
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0106594721, 0.0001304942

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.188 +- 0.032
mrr vals (pred, true): 0.039, 0.052
batch losses (mrrl, rdl): 0.0011446398, 0.0014491479

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1198
rank avg (pred): 0.229 +- 0.031
mrr vals (pred, true): 0.032, 0.045
batch losses (mrrl, rdl): 0.0031601707, 0.0010673022

Epoch over!
epoch time: 15.13

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 794
rank avg (pred): 0.124 +- 0.023
mrr vals (pred, true): 0.059, 0.050
batch losses (mrrl, rdl): 0.0008528575, 0.0020748312

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 58
rank avg (pred): 0.030 +- 0.006
mrr vals (pred, true): 0.207, 0.226
batch losses (mrrl, rdl): 0.003733089, 0.0003473006

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 898
rank avg (pred): 0.090 +- 0.025
mrr vals (pred, true): 0.088, 0.061
batch losses (mrrl, rdl): 0.0142582217, 0.0019816214

Epoch over!
epoch time: 14.964

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1022
rank avg (pred): 0.137 +- 0.027
mrr vals (pred, true): 0.055, 0.039
batch losses (mrrl, rdl): 0.0002754286, 0.0019689191

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1206
rank avg (pred): 0.194 +- 0.045
mrr vals (pred, true): 0.042, 0.046
batch losses (mrrl, rdl): 0.0006361235, 0.0014366577

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 970
rank avg (pred): 0.201 +- 0.044
mrr vals (pred, true): 0.041, 0.051
batch losses (mrrl, rdl): 0.0008621903, 0.00131896

Epoch over!
epoch time: 15.157

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 417
rank avg (pred): 0.159 +- 0.042
mrr vals (pred, true): 0.055, 0.064
batch losses (mrrl, rdl): 0.000234461, 0.0017132236

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 16
rank avg (pred): 0.035 +- 0.009
mrr vals (pred, true): 0.196, 0.184
batch losses (mrrl, rdl): 0.0014980173, 0.0005447356

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 170
rank avg (pred): 0.177 +- 0.049
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 5.02073e-05, 0.0016249623

Epoch over!
epoch time: 14.41

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.203 +- 0.053
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.0002618166, 0.0012407778

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 100
rank avg (pred): 0.196 +- 0.055
mrr vals (pred, true): 0.050, 0.040
batch losses (mrrl, rdl): 7.647e-07, 0.0014728257

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 797
rank avg (pred): 0.192 +- 0.041
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0004158632, 0.0012759254

Epoch over!
epoch time: 13.354

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1157
rank avg (pred): 0.171 +- 0.051
mrr vals (pred, true): 0.059, 0.027
batch losses (mrrl, rdl): 0.0008406123, 0.0021159933

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1130
rank avg (pred): 0.202 +- 0.061
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001509928, 0.0013530508

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 404
rank avg (pred): 0.239 +- 0.067
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.000258754, 0.0009450443

Epoch over!
epoch time: 15.188

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 484
rank avg (pred): 0.234 +- 0.071
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 8.4438e-06, 0.0009679107

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 428
rank avg (pred): 0.249 +- 0.072
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001621365, 0.0007388354

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 875
rank avg (pred): 0.243 +- 0.076
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.6856e-05, 0.0009691845

Epoch over!
epoch time: 13.882

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 205
rank avg (pred): 0.249 +- 0.080
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 7.95971e-05, 0.0008718439

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1073
rank avg (pred): 0.026 +- 0.009
mrr vals (pred, true): 0.271, 0.272
batch losses (mrrl, rdl): 8.846e-06, 0.0003027263

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 811
rank avg (pred): 0.020 +- 0.007
mrr vals (pred, true): 0.307, 0.401
batch losses (mrrl, rdl): 0.0889624581, 0.0001511006

Epoch over!
epoch time: 13.717

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 144
rank avg (pred): 0.299 +- 0.097
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 1.047e-07, 0.0004688012

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 320
rank avg (pred): 0.037 +- 0.014
mrr vals (pred, true): 0.220, 0.197
batch losses (mrrl, rdl): 0.005397853, 0.0006323569

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 866
rank avg (pred): 0.331 +- 0.083
mrr vals (pred, true): 0.040, 0.049
batch losses (mrrl, rdl): 0.0010747714, 0.0003611058

Epoch over!
epoch time: 14.346

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 865
rank avg (pred): 0.318 +- 0.097
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 2.19005e-05, 0.000475243

running batch: 500 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 284
rank avg (pred): 0.032 +- 0.012
mrr vals (pred, true): 0.242, 0.225
batch losses (mrrl, rdl): 0.0026503438, 0.0005255841

running batch: 1000 / 1094 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1032
rank avg (pred): 0.306 +- 0.103
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0003777999, 0.0004319482

Epoch over!
epoch time: 14.91

Saving checkpoint at [1] epoch 10
Done training phase:  0
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.035 +- 0.012
mrr vals (pred, true): 0.223, 0.295

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   93 	     0 	 0.05218 	 0.02393 	 ~...
   67 	     1 	 0.04603 	 0.02414 	 ~...
   88 	     2 	 0.05174 	 0.02424 	 ~...
   88 	     3 	 0.05174 	 0.02476 	 ~...
   81 	     4 	 0.04958 	 0.02481 	 ~...
   80 	     5 	 0.04955 	 0.02543 	 ~...
   75 	     6 	 0.04825 	 0.02544 	 ~...
   94 	     7 	 0.05230 	 0.02552 	 ~...
   87 	     8 	 0.05158 	 0.02570 	 ~...
   69 	     9 	 0.04648 	 0.02571 	 ~...
   68 	    10 	 0.04636 	 0.02579 	 ~...
   91 	    11 	 0.05181 	 0.02609 	 ~...
   82 	    12 	 0.05020 	 0.02623 	 ~...
   74 	    13 	 0.04746 	 0.02633 	 ~...
   11 	    14 	 0.03738 	 0.02721 	 ~...
   76 	    15 	 0.04827 	 0.02758 	 ~...
   70 	    16 	 0.04652 	 0.02767 	 ~...
    0 	    17 	 0.03580 	 0.02843 	 ~...
    9 	    18 	 0.03710 	 0.03512 	 ~...
   16 	    19 	 0.03806 	 0.03647 	 ~...
    2 	    20 	 0.03629 	 0.03695 	 ~...
    1 	    21 	 0.03629 	 0.03730 	 ~...
   18 	    22 	 0.03807 	 0.03889 	 ~...
   13 	    23 	 0.03774 	 0.03923 	 ~...
   22 	    24 	 0.03975 	 0.03937 	 ~...
   14 	    25 	 0.03796 	 0.03973 	 ~...
   34 	    26 	 0.04196 	 0.04046 	 ~...
   86 	    27 	 0.05137 	 0.04050 	 ~...
   79 	    28 	 0.04904 	 0.04084 	 ~...
   19 	    29 	 0.03837 	 0.04091 	 ~...
   26 	    30 	 0.04116 	 0.04101 	 ~...
   25 	    31 	 0.04104 	 0.04126 	 ~...
    7 	    32 	 0.03702 	 0.04129 	 ~...
   15 	    33 	 0.03801 	 0.04139 	 ~...
   12 	    34 	 0.03759 	 0.04147 	 ~...
   24 	    35 	 0.04019 	 0.04169 	 ~...
    6 	    36 	 0.03675 	 0.04209 	 ~...
   35 	    37 	 0.04206 	 0.04221 	 ~...
   56 	    38 	 0.04456 	 0.04224 	 ~...
   53 	    39 	 0.04396 	 0.04224 	 ~...
   31 	    40 	 0.04179 	 0.04224 	 ~...
   32 	    41 	 0.04180 	 0.04240 	 ~...
   20 	    42 	 0.03841 	 0.04270 	 ~...
   65 	    43 	 0.04571 	 0.04276 	 ~...
   10 	    44 	 0.03732 	 0.04279 	 ~...
    4 	    45 	 0.03659 	 0.04302 	 ~...
    8 	    46 	 0.03705 	 0.04332 	 ~...
   95 	    47 	 0.05388 	 0.04333 	 ~...
   92 	    48 	 0.05183 	 0.04369 	 ~...
   72 	    49 	 0.04703 	 0.04386 	 ~...
   85 	    50 	 0.05117 	 0.04388 	 ~...
   55 	    51 	 0.04449 	 0.04418 	 ~...
   66 	    52 	 0.04584 	 0.04444 	 ~...
   44 	    53 	 0.04293 	 0.04460 	 ~...
   60 	    54 	 0.04482 	 0.04473 	 ~...
   58 	    55 	 0.04468 	 0.04514 	 ~...
   57 	    56 	 0.04467 	 0.04549 	 ~...
   52 	    57 	 0.04384 	 0.04601 	 ~...
    3 	    58 	 0.03652 	 0.04609 	 ~...
   51 	    59 	 0.04383 	 0.04652 	 ~...
   83 	    60 	 0.05087 	 0.04658 	 ~...
   59 	    61 	 0.04471 	 0.04659 	 ~...
   61 	    62 	 0.04494 	 0.04757 	 ~...
   30 	    63 	 0.04178 	 0.04760 	 ~...
   36 	    64 	 0.04211 	 0.04777 	 ~...
   78 	    65 	 0.04885 	 0.04815 	 ~...
   62 	    66 	 0.04494 	 0.04815 	 ~...
   46 	    67 	 0.04295 	 0.04819 	 ~...
   45 	    68 	 0.04294 	 0.04843 	 ~...
   71 	    69 	 0.04678 	 0.04894 	 ~...
   84 	    70 	 0.05105 	 0.04896 	 ~...
   42 	    71 	 0.04270 	 0.04952 	 ~...
   48 	    72 	 0.04299 	 0.04953 	 ~...
   21 	    73 	 0.03920 	 0.04959 	 ~...
   40 	    74 	 0.04266 	 0.04969 	 ~...
   47 	    75 	 0.04296 	 0.04974 	 ~...
   28 	    76 	 0.04161 	 0.04978 	 ~...
   88 	    77 	 0.05174 	 0.04989 	 ~...
   63 	    78 	 0.04509 	 0.04990 	 ~...
   37 	    79 	 0.04251 	 0.05007 	 ~...
   29 	    80 	 0.04164 	 0.05012 	 ~...
   50 	    81 	 0.04342 	 0.05014 	 ~...
   49 	    82 	 0.04336 	 0.05082 	 ~...
   38 	    83 	 0.04252 	 0.05085 	 ~...
   73 	    84 	 0.04723 	 0.05086 	 ~...
   33 	    85 	 0.04189 	 0.05092 	 ~...
   43 	    86 	 0.04285 	 0.05178 	 ~...
   64 	    87 	 0.04564 	 0.05199 	 ~...
   41 	    88 	 0.04268 	 0.05259 	 ~...
    5 	    89 	 0.03675 	 0.05269 	 ~...
   54 	    90 	 0.04405 	 0.05275 	 ~...
   77 	    91 	 0.04880 	 0.05333 	 ~...
   23 	    92 	 0.03996 	 0.05340 	 ~...
   39 	    93 	 0.04262 	 0.05355 	 ~...
   27 	    94 	 0.04117 	 0.05479 	 ~...
   17 	    95 	 0.03807 	 0.05634 	 ~...
   99 	    96 	 0.18783 	 0.18803 	 ~...
  105 	    97 	 0.20329 	 0.20836 	 ~...
  103 	    98 	 0.20296 	 0.20872 	 ~...
  104 	    99 	 0.20306 	 0.21171 	 ~...
   96 	   100 	 0.17531 	 0.21637 	 m..s
  102 	   101 	 0.20094 	 0.21805 	 ~...
   97 	   102 	 0.18534 	 0.21849 	 m..s
  101 	   103 	 0.19645 	 0.23847 	 m..s
  109 	   104 	 0.23741 	 0.25215 	 ~...
  100 	   105 	 0.19331 	 0.25298 	 m..s
   98 	   106 	 0.18693 	 0.26711 	 m..s
  106 	   107 	 0.20809 	 0.27410 	 m..s
  116 	   108 	 0.27561 	 0.27441 	 ~...
  117 	   109 	 0.27967 	 0.28106 	 ~...
  112 	   110 	 0.24820 	 0.28160 	 m..s
  108 	   111 	 0.23556 	 0.28472 	 m..s
  111 	   112 	 0.24522 	 0.28533 	 m..s
  115 	   113 	 0.27208 	 0.28534 	 ~...
  118 	   114 	 0.30956 	 0.29334 	 ~...
  110 	   115 	 0.23753 	 0.29484 	 m..s
  107 	   116 	 0.22337 	 0.29550 	 m..s
  114 	   117 	 0.25837 	 0.30397 	 m..s
  113 	   118 	 0.25737 	 0.31636 	 m..s
  120 	   119 	 0.41367 	 0.37350 	 m..s
  119 	   120 	 0.39094 	 0.39525 	 ~...
==========================================
r_mrr = 0.9804859161376953
r2_mrr = 0.9510025978088379
spearmanr_mrr@5 = 0.9966015219688416
spearmanr_mrr@10 = 0.9944702386856079
spearmanr_mrr@50 = 0.9895769953727722
spearmanr_mrr@100 = 0.9928909540176392
spearmanr_mrr@All = 0.992412269115448
==========================================
test time: 0.534
Done Testing dataset UMLS
total time taken: 152.8325710296631
training time taken: 145.69823241233826
TWIG out ;))
