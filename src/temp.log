Running on a grid of size 3
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
- loading run 2.2...
Creating the train-test split
using splits:
test_ids (121): [905, 1053, 769, 496, 369, 66, 731, 534, 827, 350, 409, 1078, 129, 642, 215, 868, 637, 245, 578, 38, 257, 418, 710, 900, 797, 619, 481, 358, 522, 842, 289, 178, 1135, 273, 860, 719, 708, 614, 435, 181, 222, 1129, 695, 1173, 40, 462, 591, 1151, 1082, 656, 683, 1107, 97, 523, 313, 1043, 142, 746, 1, 939, 733, 1166, 372, 1154, 260, 246, 443, 1084, 1060, 50, 752, 340, 916, 1208, 882, 294, 77, 883, 136, 511, 988, 845, 537, 206, 717, 184, 1115, 35, 25, 742, 623, 118, 151, 93, 428, 517, 577, 96, 407, 696, 1017, 22, 722, 238, 546, 586, 223, 1029, 240, 815, 1200, 743, 279, 133, 789, 747, 1021, 1117, 1013, 1165, 1164]
valid_ids (0): []
train_ids (1094): [954, 559, 685, 385, 210, 477, 924, 909, 991, 1074, 1103, 814, 729, 580, 292, 843, 284, 319, 579, 182, 874, 312, 515, 1111, 71, 687, 94, 829, 1171, 321, 904, 364, 711, 379, 13, 946, 354, 14, 1143, 296, 121, 1010, 1160, 54, 285, 1187, 412, 562, 20, 524, 532, 23, 847, 148, 542, 180, 832, 1066, 64, 204, 958, 1081, 545, 468, 851, 194, 183, 959, 327, 264, 892, 1210, 525, 502, 718, 974, 1051, 788, 123, 404, 984, 948, 60, 903, 1134, 383, 895, 341, 654, 1014, 951, 1006, 1002, 70, 863, 931, 871, 675, 344, 1168, 173, 684, 199, 828, 658, 236, 639, 398, 821, 335, 485, 138, 499, 536, 373, 49, 1126, 985, 99, 555, 76, 439, 63, 367, 723, 1209, 229, 422, 971, 840, 715, 1163, 563, 1035, 995, 693, 583, 810, 458, 305, 896, 1125, 442, 377, 299, 486, 613, 849, 197, 581, 92, 425, 530, 137, 975, 59, 431, 787, 445, 1184, 866, 1190, 271, 1076, 1026, 942, 459, 766, 1131, 1018, 220, 913, 0, 1097, 353, 763, 915, 798, 456, 1138, 26, 47, 84, 655, 589, 802, 961, 1068, 28, 702, 79, 436, 652, 277, 1055, 351, 258, 282, 699, 816, 807, 706, 636, 449, 389, 608, 823, 287, 348, 774, 612, 1114, 689, 1123, 671, 1213, 45, 161, 880, 228, 936, 893, 114, 1088, 6, 187, 474, 594, 548, 30, 561, 750, 879, 259, 139, 550, 605, 1012, 714, 566, 999, 759, 929, 1207, 872, 772, 254, 853, 126, 1046, 1048, 1016, 930, 741, 186, 979, 643, 567, 326, 601, 314, 62, 145, 247, 29, 734, 491, 597, 396, 933, 345, 855, 1003, 1153, 552, 4, 1072, 198, 1177, 405, 441, 368, 32, 1185, 135, 399, 164, 336, 785, 230, 175, 1001, 1049, 149, 660, 592, 869, 771, 297, 211, 37, 328, 1019, 471, 700, 421, 1124, 645, 1023, 838, 610, 611, 410, 115, 311, 111, 444, 628, 740, 976, 1033, 69, 162, 243, 952, 570, 1186, 864, 189, 1159, 1009, 1158, 1121, 914, 521, 1015, 185, 43, 51, 1193, 964, 730, 2, 1139, 1182, 68, 169, 382, 779, 963, 1162, 488, 1214, 854, 478, 278, 873, 1085, 575, 1147, 465, 632, 306, 967, 86, 657, 212, 361, 320, 1047, 332, 875, 1080, 492, 234, 938, 1024, 250, 163, 676, 475, 899, 644, 765, 467, 83, 735, 427, 170, 844, 1062, 293, 159, 811, 584, 982, 171, 65, 1044, 27, 1188, 476, 907, 834, 420, 678, 541, 500, 9, 1178, 179, 518, 214, 232, 574, 362, 1022, 716, 725, 603, 33, 682, 833, 739, 1145, 165, 704, 463, 727, 1119, 520, 568, 461, 1108, 870, 758, 713, 104, 1065, 670, 510, 253, 107, 668, 91, 480, 1000, 953, 1034, 1042, 1116, 698, 889, 681, 935, 703, 144, 207, 196, 1194, 1130, 408, 861, 721, 791, 587, 910, 1089, 156, 519, 464, 140, 609, 1045, 113, 732, 707, 760, 887, 824, 943, 599, 1058, 176, 846, 606, 1087, 554, 1050, 15, 533, 573, 820, 46, 793, 438, 370, 267, 52, 1102, 5, 978, 124, 1030, 1031, 1176, 1150, 805, 450, 831, 672, 1096, 209, 783, 635, 1061, 322, 482, 325, 237, 339, 227, 777, 616, 987, 817, 756, 795, 590, 757, 423, 419, 881, 501, 57, 390, 737, 87, 429, 302, 1052, 965, 667, 1136, 891, 898, 950, 1094, 908, 622, 304, 479, 764, 749, 95, 498, 1077, 470, 331, 751, 794, 877, 1152, 649, 3, 1059, 90, 1083, 17, 565, 338, 620, 310, 307, 659, 316, 1206, 487, 143, 98, 108, 607, 18, 274, 694, 48, 922, 926, 1039, 664, 147, 472, 692, 112, 401, 41, 81, 571, 625, 117, 391, 890, 531, 356, 116, 615, 8, 224, 1120, 1025, 337, 457, 490, 1086, 380, 44, 1132, 1057, 217, 784, 359, 630, 301, 1197, 1037, 720, 941, 626, 944, 994, 549, 1172, 780, 663, 1011, 576, 269, 529, 955, 856, 318, 544, 266, 809, 627, 897, 808, 1205, 451, 298, 980, 852, 150, 262, 160, 203, 132, 483, 270, 1181, 508, 188, 1109, 493, 666, 937, 1093, 484, 11, 906, 177, 219, 395, 1095, 244, 709, 990, 497, 901, 411, 973, 233, 403, 1203, 862, 928, 134, 89, 661, 452, 755, 1113, 598, 128, 800, 1148, 738, 363, 1054, 674, 466, 73, 388, 648, 782, 235, 200, 724, 754, 371, 213, 193, 867, 424, 770, 166, 790, 1098, 1192, 705, 1149, 1112, 433, 413, 503, 665, 691, 360, 1028, 596, 582, 1175, 12, 323, 208, 662, 414, 1174, 80, 276, 384, 878, 744, 1202, 378, 557, 968, 221, 242, 263, 812, 792, 640, 669, 773, 837, 1041, 960, 947, 1110, 526, 588, 1122, 884, 392, 315, 272, 761, 1101, 152, 925, 1071, 781, 1004, 917, 74, 303, 72, 1100, 986, 387, 527, 88, 440, 78, 504, 343, 125, 1199, 1007, 381, 195, 1118, 256, 507, 248, 295, 969, 818, 1104, 1005, 981, 776, 553, 992, 560, 1161, 1008, 796, 154, 1064, 923, 172, 241, 631, 67, 416, 288, 902, 894, 1198, 505, 324, 1040, 600, 168, 146, 572, 679, 157, 921, 778, 618, 535, 799, 397, 918, 540, 448, 446, 85, 300, 803, 155, 585, 806, 1201, 218, 1155, 374, 494, 205, 701, 317, 1189, 697, 75, 255, 261, 1195, 453, 1179, 841, 506, 945, 249, 857, 1020, 130, 342, 120, 329, 333, 291, 885, 1211, 1070, 34, 813, 850, 1167, 775, 82, 347, 768, 538, 7, 934, 415, 426, 998, 1183, 595, 556, 962, 355, 365, 275, 268, 551, 997, 1142, 122, 1157, 876, 251, 819, 61, 454, 932, 53, 202, 417, 617, 966, 231, 56, 31, 201, 1156, 106, 1067, 1073, 690, 957, 16, 376, 539, 602, 24, 927, 972, 280, 1032, 912, 949, 920, 366, 393, 21, 216, 191, 919, 728, 888, 1212, 432, 352, 753, 641, 473, 346, 673, 109, 865, 1063, 58, 745, 386, 651, 989, 469, 447, 940, 158, 911, 1056, 835, 455, 1092, 804, 996, 100, 604, 334, 543, 1090, 460, 495, 174, 1196, 983, 826, 513, 638, 190, 858, 330, 101, 647, 646, 624, 1099, 102, 434, 226, 308, 514, 400, 375, 801, 1127, 825, 283, 1075, 42, 993, 153, 528, 634, 406, 110, 1180, 1141, 956, 39, 1146, 1169, 19, 686, 558, 192, 680, 349, 653, 712, 629, 512, 265, 1133, 1191, 1137, 394, 564, 131, 547, 119, 736, 1079, 1204, 437, 1140, 239, 489, 830, 10, 767, 977, 103, 726, 105, 1170, 762, 886, 36, 688, 1036, 836, 290, 970, 1038, 167, 839, 633, 141, 252, 1105, 677, 430, 1144, 1091, 309, 402, 286, 127, 1128, 281, 822, 650, 1027, 516, 859, 786, 509, 55, 225, 569, 1106, 357, 593, 748, 621, 848, 1069]
Converting data to tensors
Normalising data
Finalising data preprocessing
running TWIG with settings:
test_ratio: 0.1
valid_ratio: 0.0
normalisation: minmax
n_bins: 15
optimiser: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 1 and 2: 1
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 676
rank avg (pred): 0.413 +- 0.002
mrr vals (pred, true): 0.018, 0.043
batch losses (mrrl, rdl): 0.0, 0.0369552337

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 544
rank avg (pred): 0.471 +- 0.001
mrr vals (pred, true): 0.016, 0.026
batch losses (mrrl, rdl): 0.0, 0.0230023749

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 595
rank avg (pred): 0.473 +- 0.000
mrr vals (pred, true): 0.016, 0.038
batch losses (mrrl, rdl): 0.0, 0.0276470855

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 575
rank avg (pred): 0.484 +- 0.000
mrr vals (pred, true): 0.015, 0.041
batch losses (mrrl, rdl): 0.0, 0.0294573214

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 928
rank avg (pred): 0.455 +- 0.000
mrr vals (pred, true): 0.016, 0.035
batch losses (mrrl, rdl): 0.0, 0.0243864376

Epoch over!
epoch time: 20.59

Saving checkpoint at [1] epoch 1
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 805
rank avg (pred): 0.437 +- 0.000
mrr vals (pred, true): 0.017, 0.047
batch losses (mrrl, rdl): 1.3395347595, 0.0367480256

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 512
rank avg (pred): 0.163 +- 0.000
mrr vals (pred, true): 0.044, 0.023
batch losses (mrrl, rdl): 0.0421646424, 0.7976071835

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 268
rank avg (pred): 0.033 +- 0.000
mrr vals (pred, true): 0.183, 0.217
batch losses (mrrl, rdl): 3.134554863, 0.2277860045

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 432
rank avg (pred): 0.152 +- 0.000
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 0.0129389847, 0.6039220095

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 616
rank avg (pred): 0.159 +- 0.000
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0333244279, 0.5790855289

Epoch over!
epoch time: 21.88

Saving checkpoint at [1] epoch 1
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.141 +- 0.000
mrr vals (pred, true): 0.050, 0.018

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  130 	     0 	 0.05017 	 0.01773 	 m..s
   96 	     1 	 0.04822 	 0.02003 	 ~...
   96 	     2 	 0.04822 	 0.02009 	 ~...
  130 	     3 	 0.05017 	 0.02012 	 m..s
   54 	     4 	 0.04447 	 0.02164 	 ~...
    6 	     5 	 0.04065 	 0.02192 	 ~...
    6 	     6 	 0.04065 	 0.02202 	 ~...
   56 	     7 	 0.04455 	 0.02235 	 ~...
   52 	     8 	 0.04420 	 0.02345 	 ~...
   40 	     9 	 0.04315 	 0.02390 	 ~...
   36 	    10 	 0.04286 	 0.02398 	 ~...
   44 	    11 	 0.04377 	 0.02414 	 ~...
   74 	    12 	 0.04716 	 0.02414 	 ~...
   36 	    13 	 0.04286 	 0.02418 	 ~...
   50 	    14 	 0.04418 	 0.02423 	 ~...
  114 	    15 	 0.04951 	 0.02430 	 ~...
   60 	    16 	 0.04595 	 0.02467 	 ~...
  114 	    17 	 0.04951 	 0.02475 	 ~...
   56 	    18 	 0.04455 	 0.02487 	 ~...
   52 	    19 	 0.04420 	 0.02496 	 ~...
   74 	    20 	 0.04716 	 0.02499 	 ~...
   38 	    21 	 0.04299 	 0.02501 	 ~...
   40 	    22 	 0.04315 	 0.02544 	 ~...
   54 	    23 	 0.04447 	 0.02562 	 ~...
   38 	    24 	 0.04299 	 0.02586 	 ~...
   50 	    25 	 0.04418 	 0.02591 	 ~...
   60 	    26 	 0.04595 	 0.02637 	 ~...
   12 	    27 	 0.04110 	 0.02933 	 ~...
   44 	    28 	 0.04377 	 0.02953 	 ~...
   16 	    29 	 0.04160 	 0.03074 	 ~...
   32 	    30 	 0.04266 	 0.03159 	 ~...
   18 	    31 	 0.04177 	 0.03182 	 ~...
   18 	    32 	 0.04177 	 0.03197 	 ~...
   32 	    33 	 0.04266 	 0.03420 	 ~...
    8 	    34 	 0.04099 	 0.03458 	 ~...
  136 	    35 	 0.05071 	 0.03461 	 ~...
   12 	    36 	 0.04110 	 0.03513 	 ~...
   98 	    37 	 0.04851 	 0.03542 	 ~...
  136 	    38 	 0.05071 	 0.03549 	 ~...
    8 	    39 	 0.04099 	 0.03559 	 ~...
   16 	    40 	 0.04160 	 0.03582 	 ~...
   78 	    41 	 0.04727 	 0.03664 	 ~...
   68 	    42 	 0.04653 	 0.03704 	 ~...
  104 	    43 	 0.04866 	 0.03732 	 ~...
    2 	    44 	 0.04028 	 0.03732 	 ~...
  124 	    45 	 0.04984 	 0.03811 	 ~...
   30 	    46 	 0.04262 	 0.03832 	 ~...
   70 	    47 	 0.04669 	 0.03851 	 ~...
   90 	    48 	 0.04757 	 0.03856 	 ~...
  128 	    49 	 0.05002 	 0.03931 	 ~...
   42 	    50 	 0.04319 	 0.03943 	 ~...
   98 	    51 	 0.04851 	 0.03957 	 ~...
  174 	    52 	 0.07168 	 0.03980 	 m..s
  126 	    53 	 0.04994 	 0.03990 	 ~...
   86 	    54 	 0.04735 	 0.04032 	 ~...
  162 	    55 	 0.05972 	 0.04050 	 ~...
   84 	    56 	 0.04734 	 0.04070 	 ~...
  160 	    57 	 0.05766 	 0.04080 	 ~...
   72 	    58 	 0.04695 	 0.04099 	 ~...
  110 	    59 	 0.04910 	 0.04107 	 ~...
  124 	    60 	 0.04984 	 0.04114 	 ~...
  164 	    61 	 0.05984 	 0.04122 	 ~...
   70 	    62 	 0.04669 	 0.04129 	 ~...
  162 	    63 	 0.05972 	 0.04130 	 ~...
    4 	    64 	 0.04052 	 0.04144 	 ~...
   68 	    65 	 0.04653 	 0.04163 	 ~...
   78 	    66 	 0.04727 	 0.04165 	 ~...
  172 	    67 	 0.06455 	 0.04176 	 ~...
   80 	    68 	 0.04731 	 0.04206 	 ~...
  164 	    69 	 0.05984 	 0.04224 	 ~...
  128 	    70 	 0.05002 	 0.04227 	 ~...
  120 	    71 	 0.04971 	 0.04272 	 ~...
   26 	    72 	 0.04225 	 0.04276 	 ~...
  140 	    73 	 0.05085 	 0.04280 	 ~...
  118 	    74 	 0.04964 	 0.04306 	 ~...
  176 	    75 	 0.07788 	 0.04315 	 m..s
  154 	    76 	 0.05611 	 0.04338 	 ~...
  156 	    77 	 0.05641 	 0.04361 	 ~...
   82 	    78 	 0.04732 	 0.04362 	 ~...
  100 	    79 	 0.04853 	 0.04368 	 ~...
  116 	    80 	 0.04955 	 0.04372 	 ~...
   82 	    81 	 0.04732 	 0.04373 	 ~...
  152 	    82 	 0.05547 	 0.04388 	 ~...
  110 	    83 	 0.04910 	 0.04410 	 ~...
  166 	    84 	 0.06002 	 0.04419 	 ~...
  106 	    85 	 0.04890 	 0.04420 	 ~...
   48 	    86 	 0.04400 	 0.04423 	 ~...
   28 	    87 	 0.04233 	 0.04468 	 ~...
  134 	    88 	 0.05039 	 0.04476 	 ~...
  144 	    89 	 0.05126 	 0.04485 	 ~...
   90 	    90 	 0.04757 	 0.04488 	 ~...
   20 	    91 	 0.04188 	 0.04514 	 ~...
  160 	    92 	 0.05766 	 0.04539 	 ~...
  176 	    93 	 0.07788 	 0.04551 	 m..s
  108 	    94 	 0.04894 	 0.04564 	 ~...
  142 	    95 	 0.05096 	 0.04585 	 ~...
  154 	    96 	 0.05611 	 0.04595 	 ~...
   62 	    97 	 0.04619 	 0.04617 	 ~...
   28 	    98 	 0.04233 	 0.04642 	 ~...
   64 	    99 	 0.04644 	 0.04659 	 ~...
  172 	   100 	 0.06455 	 0.04661 	 ~...
   84 	   101 	 0.04734 	 0.04669 	 ~...
    2 	   102 	 0.04028 	 0.04670 	 ~...
   48 	   103 	 0.04400 	 0.04671 	 ~...
   92 	   104 	 0.04783 	 0.04687 	 ~...
   24 	   105 	 0.04214 	 0.04691 	 ~...
   10 	   106 	 0.04101 	 0.04699 	 ~...
   88 	   107 	 0.04748 	 0.04706 	 ~...
  134 	   108 	 0.05039 	 0.04713 	 ~...
  104 	   109 	 0.04866 	 0.04718 	 ~...
   62 	   110 	 0.04619 	 0.04722 	 ~...
  116 	   111 	 0.04955 	 0.04725 	 ~...
  140 	   112 	 0.05085 	 0.04727 	 ~...
   64 	   113 	 0.04644 	 0.04740 	 ~...
    0 	   114 	 0.03973 	 0.04741 	 ~...
   94 	   115 	 0.04790 	 0.04744 	 ~...
   66 	   116 	 0.04647 	 0.04750 	 ~...
  158 	   117 	 0.05739 	 0.04757 	 ~...
   34 	   118 	 0.04269 	 0.04758 	 ~...
   94 	   119 	 0.04790 	 0.04761 	 ~...
   58 	   120 	 0.04564 	 0.04763 	 ~...
  170 	   121 	 0.06408 	 0.04771 	 ~...
  152 	   122 	 0.05547 	 0.04785 	 ~...
  158 	   123 	 0.05739 	 0.04824 	 ~...
   72 	   124 	 0.04695 	 0.04849 	 ~...
  126 	   125 	 0.04994 	 0.04857 	 ~...
  132 	   126 	 0.05024 	 0.04879 	 ~...
   22 	   127 	 0.04205 	 0.04879 	 ~...
  118 	   128 	 0.04964 	 0.04887 	 ~...
   46 	   129 	 0.04384 	 0.04890 	 ~...
   34 	   130 	 0.04269 	 0.04894 	 ~...
  178 	   131 	 0.07972 	 0.04898 	 m..s
  178 	   132 	 0.07972 	 0.04904 	 m..s
  146 	   133 	 0.05329 	 0.04916 	 ~...
   42 	   134 	 0.04319 	 0.04921 	 ~...
   24 	   135 	 0.04214 	 0.04928 	 ~...
  138 	   136 	 0.05080 	 0.04929 	 ~...
  108 	   137 	 0.04894 	 0.04936 	 ~...
    4 	   138 	 0.04052 	 0.04949 	 ~...
  102 	   139 	 0.04855 	 0.04950 	 ~...
   20 	   140 	 0.04188 	 0.04950 	 ~...
  168 	   141 	 0.06114 	 0.04956 	 ~...
  150 	   142 	 0.05480 	 0.04968 	 ~...
   14 	   143 	 0.04111 	 0.04987 	 ~...
  146 	   144 	 0.05329 	 0.05004 	 ~...
   14 	   145 	 0.04111 	 0.05007 	 ~...
   76 	   146 	 0.04717 	 0.05012 	 ~...
   46 	   147 	 0.04384 	 0.05023 	 ~...
  106 	   148 	 0.04890 	 0.05029 	 ~...
  156 	   149 	 0.05641 	 0.05056 	 ~...
   10 	   150 	 0.04101 	 0.05092 	 ~...
  102 	   151 	 0.04855 	 0.05108 	 ~...
   26 	   152 	 0.04225 	 0.05123 	 ~...
  100 	   153 	 0.04853 	 0.05128 	 ~...
  170 	   154 	 0.06408 	 0.05129 	 ~...
  138 	   155 	 0.05080 	 0.05133 	 ~...
   58 	   156 	 0.04564 	 0.05133 	 ~...
   76 	   157 	 0.04717 	 0.05144 	 ~...
  148 	   158 	 0.05364 	 0.05146 	 ~...
   66 	   159 	 0.04647 	 0.05150 	 ~...
  122 	   160 	 0.04979 	 0.05160 	 ~...
   88 	   161 	 0.04748 	 0.05172 	 ~...
  150 	   162 	 0.05480 	 0.05189 	 ~...
  174 	   163 	 0.07168 	 0.05242 	 ~...
  112 	   164 	 0.04945 	 0.05246 	 ~...
  148 	   165 	 0.05364 	 0.05264 	 ~...
  144 	   166 	 0.05126 	 0.05275 	 ~...
   22 	   167 	 0.04205 	 0.05278 	 ~...
   86 	   168 	 0.04735 	 0.05305 	 ~...
  122 	   169 	 0.04979 	 0.05345 	 ~...
   30 	   170 	 0.04262 	 0.05352 	 ~...
  142 	   171 	 0.05096 	 0.05355 	 ~...
  166 	   172 	 0.06002 	 0.05429 	 ~...
  168 	   173 	 0.06114 	 0.05446 	 ~...
    0 	   174 	 0.03973 	 0.05567 	 ~...
  120 	   175 	 0.04971 	 0.05578 	 ~...
   92 	   176 	 0.04783 	 0.05633 	 ~...
  132 	   177 	 0.05024 	 0.05653 	 ~...
   80 	   178 	 0.04731 	 0.05977 	 ~...
  112 	   179 	 0.04945 	 0.06429 	 ~...
  208 	   180 	 0.23203 	 0.18108 	 m..s
  206 	   181 	 0.22893 	 0.18889 	 m..s
  180 	   182 	 0.15433 	 0.18959 	 m..s
  186 	   183 	 0.17318 	 0.18965 	 ~...
  186 	   184 	 0.17318 	 0.19213 	 ~...
  206 	   185 	 0.22893 	 0.19277 	 m..s
  190 	   186 	 0.18598 	 0.19544 	 ~...
  182 	   187 	 0.15833 	 0.19616 	 m..s
  180 	   188 	 0.15433 	 0.19773 	 m..s
  208 	   189 	 0.23203 	 0.19872 	 m..s
  192 	   190 	 0.19321 	 0.20331 	 ~...
  220 	   191 	 0.28126 	 0.20538 	 m..s
  202 	   192 	 0.20420 	 0.20550 	 ~...
  182 	   193 	 0.15833 	 0.20694 	 m..s
  194 	   194 	 0.19503 	 0.20727 	 ~...
  194 	   195 	 0.19503 	 0.20960 	 ~...
  188 	   196 	 0.18301 	 0.20961 	 ~...
  202 	   197 	 0.20420 	 0.21102 	 ~...
  184 	   198 	 0.15891 	 0.21290 	 m..s
  190 	   199 	 0.18598 	 0.21693 	 m..s
  192 	   200 	 0.19321 	 0.21723 	 ~...
  188 	   201 	 0.18301 	 0.21940 	 m..s
  184 	   202 	 0.15891 	 0.21956 	 m..s
  212 	   203 	 0.26042 	 0.22762 	 m..s
  218 	   204 	 0.27819 	 0.22778 	 m..s
  210 	   205 	 0.24956 	 0.22812 	 ~...
  220 	   206 	 0.28126 	 0.23290 	 m..s
  196 	   207 	 0.19773 	 0.23887 	 m..s
  196 	   208 	 0.19773 	 0.23984 	 m..s
  216 	   209 	 0.27432 	 0.24013 	 m..s
  216 	   210 	 0.27432 	 0.24107 	 m..s
  210 	   211 	 0.24956 	 0.24218 	 ~...
  212 	   212 	 0.26042 	 0.24307 	 ~...
  214 	   213 	 0.26166 	 0.25268 	 ~...
  204 	   214 	 0.21156 	 0.25298 	 m..s
  218 	   215 	 0.27819 	 0.25327 	 ~...
  198 	   216 	 0.20102 	 0.26320 	 m..s
  228 	   217 	 0.33253 	 0.26390 	 m..s
  204 	   218 	 0.21156 	 0.27092 	 m..s
  226 	   219 	 0.33087 	 0.27144 	 m..s
  224 	   220 	 0.28974 	 0.27255 	 ~...
  198 	   221 	 0.20102 	 0.27378 	 m..s
  214 	   222 	 0.26166 	 0.27425 	 ~...
  228 	   223 	 0.33253 	 0.27441 	 m..s
  234 	   224 	 0.34287 	 0.27737 	 m..s
  222 	   225 	 0.28804 	 0.27899 	 ~...
  200 	   226 	 0.20302 	 0.28029 	 m..s
  230 	   227 	 0.33356 	 0.28034 	 m..s
  222 	   228 	 0.28804 	 0.28353 	 ~...
  230 	   229 	 0.33356 	 0.29154 	 m..s
  226 	   230 	 0.33087 	 0.29788 	 m..s
  224 	   231 	 0.28974 	 0.29922 	 ~...
  234 	   232 	 0.34287 	 0.30019 	 m..s
  232 	   233 	 0.33681 	 0.31748 	 ~...
  232 	   234 	 0.33681 	 0.31911 	 ~...
  200 	   235 	 0.20302 	 0.33485 	 MISS
  238 	   236 	 0.46776 	 0.37064 	 m..s
  238 	   237 	 0.46776 	 0.39525 	 m..s
  236 	   238 	 0.44632 	 0.51900 	 m..s
  236 	   239 	 0.44632 	 0.52223 	 m..s
  240 	   240 	 0.47876 	 0.53924 	 m..s
  240 	   241 	 0.47876 	 0.57253 	 m..s
==========================================
r_mrr = 0.9688244462013245
r2_mrr = 0.9370133280754089
spearmanr_mrr@5 = 0.9720355272293091
spearmanr_mrr@10 = 0.8981444239616394
spearmanr_mrr@50 = 0.9371558427810669
spearmanr_mrr@100 = 0.9809443950653076
spearmanr_mrr@All = 0.9885849952697754
==========================================
test time: 0.706
Done Testing dataset UMLS
total time taken: 53.85898947715759
training time taken: 43.24220609664917
TWIG out ;))
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
- loading run 2.2...
Creating the train-test split
using splits:
test_ids (121): [1145, 1027, 49, 923, 576, 318, 496, 287, 792, 162, 490, 799, 31, 119, 684, 935, 413, 678, 371, 3, 1002, 494, 467, 434, 446, 113, 333, 339, 970, 269, 814, 1008, 92, 55, 971, 405, 72, 1165, 1013, 1064, 6, 932, 380, 196, 215, 595, 1107, 77, 1057, 476, 951, 566, 123, 245, 410, 295, 1036, 526, 60, 373, 139, 1089, 647, 724, 733, 974, 754, 897, 506, 731, 443, 575, 625, 697, 409, 793, 662, 100, 348, 919, 1154, 942, 1174, 426, 1212, 720, 1210, 471, 1037, 1175, 87, 199, 400, 944, 78, 501, 612, 511, 346, 217, 1012, 9, 219, 797, 624, 5, 239, 398, 672, 385, 1060, 472, 630, 23, 283, 229, 1180, 756, 1125, 1097, 829]
valid_ids (0): []
train_ids (1094): [145, 925, 187, 141, 804, 1014, 258, 18, 842, 210, 859, 1142, 234, 961, 1160, 507, 65, 1067, 412, 628, 721, 497, 462, 1115, 343, 1065, 516, 354, 851, 729, 1045, 153, 1031, 597, 455, 604, 4, 568, 693, 726, 342, 635, 200, 313, 885, 916, 773, 1094, 1040, 617, 143, 545, 1208, 225, 937, 512, 698, 668, 833, 1030, 810, 1093, 642, 658, 688, 910, 1003, 310, 248, 86, 653, 892, 852, 952, 601, 44, 362, 191, 763, 47, 956, 660, 1162, 904, 717, 15, 247, 1113, 594, 835, 1011, 1192, 936, 242, 1205, 1098, 336, 967, 659, 709, 1116, 1016, 996, 304, 931, 38, 340, 188, 164, 456, 42, 316, 278, 1047, 760, 407, 1042, 990, 515, 254, 235, 1061, 165, 854, 569, 148, 1010, 751, 381, 108, 213, 540, 105, 388, 872, 748, 1049, 186, 1022, 605, 537, 807, 482, 646, 334, 982, 734, 156, 691, 574, 463, 530, 1187, 801, 1134, 79, 908, 857, 226, 649, 1198, 1140, 755, 900, 465, 230, 757, 332, 839, 432, 375, 549, 109, 80, 844, 101, 838, 1129, 524, 588, 991, 221, 74, 167, 1068, 582, 1081, 831, 890, 1000, 1120, 927, 973, 1017, 445, 747, 20, 46, 690, 607, 345, 906, 940, 609, 1199, 415, 648, 948, 517, 62, 907, 1054, 638, 429, 669, 32, 469, 1127, 152, 767, 1038, 713, 1181, 752, 1164, 884, 1073, 292, 586, 306, 1005, 987, 1018, 558, 220, 1206, 241, 289, 856, 1195, 325, 848, 324, 488, 36, 818, 89, 212, 2, 192, 1156, 563, 977, 632, 218, 125, 707, 271, 896, 356, 197, 1168, 899, 387, 619, 552, 1183, 205, 775, 1158, 634, 144, 778, 544, 1121, 303, 1188, 994, 980, 359, 1063, 201, 1182, 1197, 1087, 459, 525, 76, 183, 843, 126, 988, 93, 441, 1048, 730, 259, 307, 305, 63, 823, 534, 57, 84, 319, 614, 1034, 868, 1148, 70, 1032, 1105, 403, 411, 960, 771, 163, 158, 17, 1136, 1143, 743, 150, 589, 296, 570, 54, 1026, 255, 1124, 875, 645, 121, 135, 404, 1084, 629, 502, 160, 674, 997, 244, 450, 700, 592, 631, 97, 117, 928, 358, 1128, 802, 344, 111, 753, 452, 508, 676, 812, 67, 335, 518, 663, 593, 120, 300, 1104, 408, 417, 665, 836, 173, 355, 266, 209, 274, 369, 110, 495, 233, 28, 222, 1103, 228, 664, 962, 620, 422, 98, 954, 236, 59, 679, 232, 286, 862, 276, 671, 172, 207, 873, 1083, 61, 817, 478, 66, 1019, 1001, 449, 211, 256, 550, 115, 637, 1151, 91, 644, 855, 777, 102, 986, 1075, 0, 94, 351, 401, 252, 554, 1062, 708, 955, 924, 367, 170, 834, 264, 48, 68, 489, 929, 579, 440, 1059, 587, 282, 972, 1201, 666, 308, 603, 958, 964, 389, 939, 22, 82, 618, 1092, 539, 813, 1200, 1167, 85, 513, 965, 523, 277, 368, 133, 571, 250, 1072, 37, 25, 480, 204, 419, 159, 565, 877, 395, 1117, 699, 583, 299, 270, 893, 10, 759, 921, 291, 312, 447, 803, 732, 657, 481, 1052, 457, 865, 816, 1209, 1007, 861, 365, 251, 742, 922, 1114, 1150, 681, 1133, 567, 137, 1078, 1169, 784, 214, 257, 837, 718, 898, 673, 692, 39, 1147, 484, 541, 50, 202, 309, 770, 572, 769, 1194, 140, 238, 746, 1130, 1135, 1112, 560, 330, 181, 180, 176, 41, 828, 364, 1152, 983, 532, 431, 1110, 34, 1004, 613, 487, 379, 509, 486, 661, 142, 128, 69, 555, 1076, 500, 33, 946, 979, 95, 442, 562, 878, 394, 261, 376, 809, 1058, 189, 776, 285, 1163, 1033, 1077, 1029, 623, 1177, 548, 112, 45, 704, 262, 288, 329, 815, 783, 529, 1184, 290, 853, 832, 337, 468, 273, 13, 1095, 479, 1131, 491, 1046, 1021, 1190, 1178, 166, 1123, 428, 26, 522, 279, 433, 425, 396, 670, 905, 338, 830, 12, 328, 124, 146, 847, 514, 895, 805, 933, 680, 169, 1028, 1108, 464, 761, 366, 819, 800, 378, 687, 998, 949, 636, 416, 882, 874, 40, 1159, 27, 1172, 14, 138, 1015, 796, 985, 772, 90, 785, 941, 374, 30, 106, 1079, 397, 519, 154, 869, 298, 795, 917, 116, 887, 547, 499, 1153, 781, 341, 947, 969, 542, 584, 722, 650, 1071, 744, 656, 912, 1191, 1157, 168, 198, 723, 652, 174, 131, 195, 788, 577, 223, 860, 52, 926, 701, 323, 1119, 227, 711, 75, 29, 1171, 557, 786, 349, 1161, 886, 966, 596, 43, 849, 136, 710, 451, 475, 1126, 600, 766, 578, 989, 155, 943, 915, 53, 608, 782, 787, 639, 454, 864, 521, 444, 745, 975, 363, 762, 377, 686, 702, 866, 1039, 1204, 640, 498, 311, 284, 96, 474, 1086, 984, 889, 263, 1024, 1102, 421, 56, 458, 1118, 1101, 122, 272, 1207, 626, 826, 694, 293, 1096, 858, 73, 765, 739, 553, 633, 1050, 768, 360, 1193, 427, 134, 149, 585, 683, 372, 880, 206, 580, 423, 294, 1149, 240, 976, 237, 821, 1111, 551, 253, 559, 685, 806, 510, 883, 402, 528, 930, 1173, 361, 58, 1203, 867, 503, 420, 194, 1185, 527, 616, 1214, 118, 820, 370, 841, 1051, 418, 281, 321, 641, 1085, 1035, 598, 331, 1189, 621, 992, 703, 1, 938, 203, 727, 1213, 573, 1138, 808, 902, 477, 1088, 439, 216, 267, 740, 602, 774, 758, 470, 894, 822, 695, 1025, 505, 8, 651, 193, 326, 406, 845, 888, 871, 438, 712, 127, 606, 725, 103, 1043, 627, 1137, 750, 1080, 911, 622, 741, 504, 114, 424, 824, 950, 414, 1122, 1146, 706, 825, 399, 561, 959, 1141, 473, 1091, 682, 460, 246, 315, 840, 615, 129, 1186, 945, 1170, 11, 556, 667, 794, 863, 789, 81, 599, 177, 208, 327, 881, 171, 790, 1155, 382, 591, 88, 846, 1041, 301, 716, 437, 320, 249, 35, 268, 914, 934, 492, 1109, 1139, 353, 779, 876, 543, 99, 130, 147, 1066, 531, 981, 179, 485, 957, 71, 1009, 850, 390, 978, 999, 968, 715, 870, 705, 1166, 798, 453, 677, 655, 654, 435, 64, 16, 448, 19, 383, 995, 728, 231, 675, 302, 738, 182, 386, 260, 581, 993, 735, 714, 1055, 161, 357, 903, 780, 737, 1070, 611, 1074, 811, 1099, 564, 1044, 791, 920, 1196, 391, 696, 1106, 1082, 24, 175, 891, 1202, 913, 190, 185, 918, 184, 314, 347, 719, 384, 1053, 535, 1132, 461, 352, 1020, 265, 243, 610, 21, 1023, 104, 83, 392, 1090, 317, 350, 533, 493, 436, 520, 1056, 590, 7, 224, 1069, 536, 953, 546, 1100, 107, 483, 827, 178, 1179, 1144, 879, 275, 1006, 689, 132, 157, 297, 280, 749, 736, 151, 430, 393, 1211, 764, 51, 538, 909, 466, 963, 643, 901, 322, 1176]
Converting data to tensors
Normalising data
Finalising data preprocessing
running TWIG with settings:
test_ratio: 0.1
valid_ratio: 0.0
normalisation: minmax
n_bins: 30
optimiser: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 1 and 2: 1
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 468
rank avg (pred): 0.493 +- 0.003
mrr vals (pred, true): 0.015, 0.053
batch losses (mrrl, rdl): 0.0, 0.0210845657

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 300
rank avg (pred): 0.167 +- 0.006
mrr vals (pred, true): 0.043, 0.247
batch losses (mrrl, rdl): 0.0, 0.0049811048

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 741
rank avg (pred): 0.182 +- 0.005
mrr vals (pred, true): 0.039, 0.278
batch losses (mrrl, rdl): 0.0, 0.0080994507

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 698
rank avg (pred): 0.488 +- 0.000
mrr vals (pred, true): 0.015, 0.053
batch losses (mrrl, rdl): 0.0, 0.0216491297

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 962
rank avg (pred): 0.474 +- 0.000
mrr vals (pred, true): 0.015, 0.040
batch losses (mrrl, rdl): 0.0, 0.0128572518

Epoch over!
epoch time: 36.281

Saving checkpoint at [1] epoch 1
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 124
rank avg (pred): 0.445 +- 0.010
mrr vals (pred, true): 0.017, 0.047
batch losses (mrrl, rdl): 1.3579864502, 0.0129182804

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 392
rank avg (pred): 0.190 +- 0.000
mrr vals (pred, true): 0.038, 0.053
batch losses (mrrl, rdl): 0.1864964068, 0.2112368792

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 473
rank avg (pred): 0.133 +- 0.000
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 0.0107133035, 0.3119214177

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 25
rank avg (pred): 0.030 +- 0.000
mrr vals (pred, true): 0.199, 0.199
batch losses (mrrl, rdl): 0.0008651762, 0.1255174875

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 627
rank avg (pred): 0.154 +- 0.000
mrr vals (pred, true): 0.046, 0.037
batch losses (mrrl, rdl): 0.0163475908, 0.3093243837

Epoch over!
epoch time: 36.717

Saving checkpoint at [1] epoch 1
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.170 +- 0.000
mrr vals (pred, true): 0.042, 0.025

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  124 	     0 	 0.05034 	 0.02183 	 ~...
    6 	     1 	 0.04240 	 0.02193 	 ~...
  110 	     2 	 0.04895 	 0.02235 	 ~...
    4 	     3 	 0.04230 	 0.02283 	 ~...
   72 	     4 	 0.04764 	 0.02370 	 ~...
    0 	     5 	 0.04195 	 0.02414 	 ~...
    6 	     6 	 0.04240 	 0.02447 	 ~...
    2 	     7 	 0.04198 	 0.02453 	 ~...
  118 	     8 	 0.04931 	 0.02467 	 ~...
  110 	     9 	 0.04895 	 0.02487 	 ~...
    4 	    10 	 0.04230 	 0.02488 	 ~...
    2 	    11 	 0.04198 	 0.02495 	 ~...
  112 	    12 	 0.04900 	 0.02497 	 ~...
    0 	    13 	 0.04195 	 0.02499 	 ~...
   72 	    14 	 0.04764 	 0.02524 	 ~...
  124 	    15 	 0.05034 	 0.02539 	 ~...
  112 	    16 	 0.04900 	 0.02561 	 ~...
  104 	    17 	 0.04871 	 0.02609 	 ~...
  118 	    18 	 0.04931 	 0.02637 	 ~...
   20 	    19 	 0.04453 	 0.02679 	 ~...
   12 	    20 	 0.04378 	 0.02695 	 ~...
  104 	    21 	 0.04871 	 0.02780 	 ~...
   18 	    22 	 0.04450 	 0.03018 	 ~...
   54 	    23 	 0.04646 	 0.03048 	 ~...
  114 	    24 	 0.04907 	 0.03074 	 ~...
  160 	    25 	 0.05386 	 0.03107 	 ~...
   12 	    26 	 0.04378 	 0.03137 	 ~...
   30 	    27 	 0.04525 	 0.03224 	 ~...
   42 	    28 	 0.04581 	 0.03289 	 ~...
   20 	    29 	 0.04453 	 0.03390 	 ~...
   30 	    30 	 0.04525 	 0.03440 	 ~...
  126 	    31 	 0.05039 	 0.03518 	 ~...
   18 	    32 	 0.04450 	 0.03521 	 ~...
    8 	    33 	 0.04288 	 0.03523 	 ~...
   42 	    34 	 0.04581 	 0.03523 	 ~...
    8 	    35 	 0.04288 	 0.03561 	 ~...
  114 	    36 	 0.04907 	 0.03582 	 ~...
  126 	    37 	 0.05039 	 0.03631 	 ~...
   26 	    38 	 0.04487 	 0.03666 	 ~...
   54 	    39 	 0.04646 	 0.03673 	 ~...
   38 	    40 	 0.04554 	 0.03687 	 ~...
  148 	    41 	 0.05254 	 0.03688 	 ~...
   24 	    42 	 0.04469 	 0.03698 	 ~...
  158 	    43 	 0.05382 	 0.03728 	 ~...
  150 	    44 	 0.05260 	 0.03753 	 ~...
   74 	    45 	 0.04795 	 0.03801 	 ~...
  130 	    46 	 0.05049 	 0.03813 	 ~...
   86 	    47 	 0.04827 	 0.03839 	 ~...
   38 	    48 	 0.04554 	 0.03937 	 ~...
  136 	    49 	 0.05068 	 0.03948 	 ~...
   84 	    50 	 0.04811 	 0.03973 	 ~...
  138 	    51 	 0.05089 	 0.03974 	 ~...
   94 	    52 	 0.04853 	 0.03976 	 ~...
  136 	    53 	 0.05068 	 0.03985 	 ~...
  160 	    54 	 0.05386 	 0.03998 	 ~...
  140 	    55 	 0.05182 	 0.03999 	 ~...
   88 	    56 	 0.04840 	 0.04022 	 ~...
   68 	    57 	 0.04754 	 0.04036 	 ~...
   86 	    58 	 0.04827 	 0.04042 	 ~...
   84 	    59 	 0.04811 	 0.04045 	 ~...
  120 	    60 	 0.04993 	 0.04046 	 ~...
  134 	    61 	 0.05060 	 0.04047 	 ~...
  146 	    62 	 0.05240 	 0.04055 	 ~...
  140 	    63 	 0.05182 	 0.04062 	 ~...
   74 	    64 	 0.04795 	 0.04088 	 ~...
  148 	    65 	 0.05254 	 0.04111 	 ~...
  176 	    66 	 0.05675 	 0.04119 	 ~...
  182 	    67 	 0.06385 	 0.04144 	 ~...
   90 	    68 	 0.04844 	 0.04144 	 ~...
  158 	    69 	 0.05382 	 0.04177 	 ~...
  120 	    70 	 0.04993 	 0.04189 	 ~...
   34 	    71 	 0.04535 	 0.04191 	 ~...
  122 	    72 	 0.05028 	 0.04214 	 ~...
   26 	    73 	 0.04487 	 0.04238 	 ~...
  102 	    74 	 0.04867 	 0.04257 	 ~...
  164 	    75 	 0.05489 	 0.04257 	 ~...
  150 	    76 	 0.05260 	 0.04258 	 ~...
   58 	    77 	 0.04671 	 0.04283 	 ~...
   64 	    78 	 0.04688 	 0.04284 	 ~...
   32 	    79 	 0.04531 	 0.04302 	 ~...
   50 	    80 	 0.04604 	 0.04307 	 ~...
  162 	    81 	 0.05453 	 0.04329 	 ~...
  108 	    82 	 0.04884 	 0.04329 	 ~...
  170 	    83 	 0.05567 	 0.04335 	 ~...
  168 	    84 	 0.05563 	 0.04338 	 ~...
   90 	    85 	 0.04844 	 0.04340 	 ~...
  182 	    86 	 0.06385 	 0.04346 	 ~...
   96 	    87 	 0.04854 	 0.04353 	 ~...
  144 	    88 	 0.05222 	 0.04369 	 ~...
   56 	    89 	 0.04667 	 0.04375 	 ~...
   66 	    90 	 0.04751 	 0.04393 	 ~...
  184 	    91 	 0.06424 	 0.04413 	 ~...
   48 	    92 	 0.04604 	 0.04440 	 ~...
   64 	    93 	 0.04688 	 0.04441 	 ~...
   28 	    94 	 0.04521 	 0.04447 	 ~...
   40 	    95 	 0.04559 	 0.04448 	 ~...
  130 	    96 	 0.05049 	 0.04450 	 ~...
  178 	    97 	 0.05685 	 0.04451 	 ~...
   88 	    98 	 0.04840 	 0.04471 	 ~...
   16 	    99 	 0.04426 	 0.04487 	 ~...
   46 	   100 	 0.04597 	 0.04493 	 ~...
  154 	   101 	 0.05298 	 0.04504 	 ~...
  170 	   102 	 0.05567 	 0.04505 	 ~...
   24 	   103 	 0.04469 	 0.04510 	 ~...
   76 	   104 	 0.04796 	 0.04526 	 ~...
  122 	   105 	 0.05028 	 0.04542 	 ~...
  116 	   106 	 0.04913 	 0.04573 	 ~...
   52 	   107 	 0.04613 	 0.04574 	 ~...
   36 	   108 	 0.04546 	 0.04584 	 ~...
  168 	   109 	 0.05563 	 0.04595 	 ~...
   82 	   110 	 0.04808 	 0.04596 	 ~...
  102 	   111 	 0.04867 	 0.04601 	 ~...
   60 	   112 	 0.04677 	 0.04616 	 ~...
   62 	   113 	 0.04687 	 0.04617 	 ~...
  186 	   114 	 0.07394 	 0.04621 	 ~...
   14 	   115 	 0.04404 	 0.04622 	 ~...
   34 	   116 	 0.04535 	 0.04625 	 ~...
   56 	   117 	 0.04667 	 0.04631 	 ~...
  106 	   118 	 0.04883 	 0.04658 	 ~...
  176 	   119 	 0.05675 	 0.04669 	 ~...
  116 	   120 	 0.04913 	 0.04688 	 ~...
   78 	   121 	 0.04799 	 0.04694 	 ~...
   80 	   122 	 0.04802 	 0.04701 	 ~...
   22 	   123 	 0.04458 	 0.04705 	 ~...
   62 	   124 	 0.04687 	 0.04722 	 ~...
   66 	   125 	 0.04751 	 0.04731 	 ~...
   58 	   126 	 0.04671 	 0.04741 	 ~...
  128 	   127 	 0.05045 	 0.04760 	 ~...
   40 	   128 	 0.04559 	 0.04760 	 ~...
  132 	   129 	 0.05053 	 0.04762 	 ~...
  100 	   130 	 0.04864 	 0.04763 	 ~...
  174 	   131 	 0.05667 	 0.04771 	 ~...
  144 	   132 	 0.05222 	 0.04781 	 ~...
   76 	   133 	 0.04796 	 0.04793 	 ~...
  132 	   134 	 0.05053 	 0.04814 	 ~...
   60 	   135 	 0.04677 	 0.04815 	 ~...
   70 	   136 	 0.04758 	 0.04822 	 ~...
  146 	   137 	 0.05240 	 0.04823 	 ~...
   46 	   138 	 0.04597 	 0.04844 	 ~...
  166 	   139 	 0.05551 	 0.04848 	 ~...
  108 	   140 	 0.04884 	 0.04868 	 ~...
   48 	   141 	 0.04604 	 0.04871 	 ~...
   92 	   142 	 0.04850 	 0.04888 	 ~...
   70 	   143 	 0.04758 	 0.04897 	 ~...
  152 	   144 	 0.05280 	 0.04907 	 ~...
  172 	   145 	 0.05572 	 0.04911 	 ~...
   36 	   146 	 0.04546 	 0.04911 	 ~...
   68 	   147 	 0.04754 	 0.04912 	 ~...
  134 	   148 	 0.05060 	 0.04917 	 ~...
   82 	   149 	 0.04808 	 0.04918 	 ~...
   44 	   150 	 0.04584 	 0.04949 	 ~...
   96 	   151 	 0.04854 	 0.04953 	 ~...
   10 	   152 	 0.04329 	 0.04987 	 ~...
  184 	   153 	 0.06424 	 0.04989 	 ~...
   10 	   154 	 0.04329 	 0.05007 	 ~...
  186 	   155 	 0.07394 	 0.05022 	 ~...
  172 	   156 	 0.05572 	 0.05049 	 ~...
   16 	   157 	 0.04426 	 0.05051 	 ~...
  128 	   158 	 0.05045 	 0.05067 	 ~...
   94 	   159 	 0.04853 	 0.05068 	 ~...
  106 	   160 	 0.04883 	 0.05073 	 ~...
   50 	   161 	 0.04604 	 0.05081 	 ~...
  164 	   162 	 0.05489 	 0.05086 	 ~...
   14 	   163 	 0.04404 	 0.05089 	 ~...
  152 	   164 	 0.05280 	 0.05100 	 ~...
   52 	   165 	 0.04613 	 0.05109 	 ~...
  174 	   166 	 0.05667 	 0.05129 	 ~...
  100 	   167 	 0.04864 	 0.05133 	 ~...
  162 	   168 	 0.05453 	 0.05135 	 ~...
   98 	   169 	 0.04858 	 0.05162 	 ~...
  166 	   170 	 0.05551 	 0.05231 	 ~...
   92 	   171 	 0.04850 	 0.05233 	 ~...
  180 	   172 	 0.06000 	 0.05246 	 ~...
  138 	   173 	 0.05089 	 0.05296 	 ~...
   32 	   174 	 0.04531 	 0.05309 	 ~...
  156 	   175 	 0.05369 	 0.05367 	 ~...
   22 	   176 	 0.04458 	 0.05371 	 ~...
   80 	   177 	 0.04802 	 0.05448 	 ~...
  154 	   178 	 0.05298 	 0.05479 	 ~...
   44 	   179 	 0.04584 	 0.05482 	 ~...
  142 	   180 	 0.05209 	 0.05491 	 ~...
  178 	   181 	 0.05685 	 0.05498 	 ~...
   78 	   182 	 0.04799 	 0.05501 	 ~...
  142 	   183 	 0.05209 	 0.05513 	 ~...
  156 	   184 	 0.05369 	 0.05522 	 ~...
   98 	   185 	 0.04858 	 0.05554 	 ~...
   28 	   186 	 0.04521 	 0.05851 	 ~...
  180 	   187 	 0.06000 	 0.06429 	 ~...
  192 	   188 	 0.16980 	 0.17977 	 ~...
  190 	   189 	 0.16131 	 0.18420 	 ~...
  196 	   190 	 0.18601 	 0.18840 	 ~...
  194 	   191 	 0.18524 	 0.18867 	 ~...
  196 	   192 	 0.18601 	 0.19146 	 ~...
  198 	   193 	 0.19946 	 0.19616 	 ~...
  194 	   194 	 0.18524 	 0.19826 	 ~...
  188 	   195 	 0.15904 	 0.19882 	 m..s
  192 	   196 	 0.16980 	 0.20146 	 m..s
  190 	   197 	 0.16131 	 0.20499 	 m..s
  204 	   198 	 0.21117 	 0.20559 	 ~...
  198 	   199 	 0.19946 	 0.20694 	 ~...
  204 	   200 	 0.21117 	 0.20876 	 ~...
  212 	   201 	 0.24480 	 0.21089 	 m..s
  218 	   202 	 0.27331 	 0.21149 	 m..s
  202 	   203 	 0.20777 	 0.21171 	 ~...
  188 	   204 	 0.15904 	 0.21511 	 m..s
  218 	   205 	 0.27331 	 0.21805 	 m..s
  200 	   206 	 0.20380 	 0.21902 	 ~...
  224 	   207 	 0.29059 	 0.22459 	 m..s
  214 	   208 	 0.24985 	 0.22597 	 ~...
  220 	   209 	 0.27447 	 0.22707 	 m..s
  212 	   210 	 0.24480 	 0.22731 	 ~...
  200 	   211 	 0.20380 	 0.23039 	 ~...
  208 	   212 	 0.23347 	 0.23156 	 ~...
  202 	   213 	 0.20777 	 0.23311 	 ~...
  206 	   214 	 0.21394 	 0.23401 	 ~...
  206 	   215 	 0.21394 	 0.23731 	 ~...
  228 	   216 	 0.30222 	 0.23847 	 m..s
  208 	   217 	 0.23347 	 0.24226 	 ~...
  214 	   218 	 0.24985 	 0.24265 	 ~...
  220 	   219 	 0.27447 	 0.24517 	 ~...
  210 	   220 	 0.24065 	 0.24656 	 ~...
  224 	   221 	 0.29059 	 0.24954 	 m..s
  226 	   222 	 0.29750 	 0.25268 	 m..s
  228 	   223 	 0.30222 	 0.25526 	 m..s
  210 	   224 	 0.24065 	 0.26048 	 ~...
  230 	   225 	 0.33561 	 0.26464 	 m..s
  232 	   226 	 0.33881 	 0.26525 	 m..s
  230 	   227 	 0.33561 	 0.26769 	 m..s
  222 	   228 	 0.28680 	 0.27255 	 ~...
  226 	   229 	 0.29750 	 0.27425 	 ~...
  216 	   230 	 0.26680 	 0.28917 	 ~...
  232 	   231 	 0.33881 	 0.29179 	 m..s
  216 	   232 	 0.26680 	 0.29611 	 ~...
  222 	   233 	 0.28680 	 0.29922 	 ~...
  234 	   234 	 0.34410 	 0.31981 	 ~...
  234 	   235 	 0.34410 	 0.33593 	 ~...
  236 	   236 	 0.39922 	 0.37064 	 ~...
  236 	   237 	 0.39922 	 0.39525 	 ~...
  240 	   238 	 0.41853 	 0.51900 	 MISS
  240 	   239 	 0.41853 	 0.52223 	 MISS
  238 	   240 	 0.40268 	 0.55007 	 MISS
  238 	   241 	 0.40268 	 0.56022 	 MISS
==========================================
r_mrr = 0.9714576601982117
r2_mrr = 0.9402307271957397
spearmanr_mrr@5 = 0.7400929927825928
spearmanr_mrr@10 = 0.895126461982727
spearmanr_mrr@50 = 0.8897742033004761
spearmanr_mrr@100 = 0.9700220823287964
spearmanr_mrr@All = 0.9807268381118774
==========================================
test time: 1.351
Done Testing dataset UMLS
total time taken: 88.63834714889526
training time taken: 74.36412978172302
TWIG out ;))
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
- loading run 2.2...
Creating the train-test split
using splits:
test_ids (121): [240, 536, 742, 312, 298, 4, 1137, 71, 768, 538, 461, 776, 684, 131, 324, 261, 481, 1128, 478, 343, 412, 923, 829, 769, 762, 573, 189, 1125, 227, 761, 42, 880, 152, 921, 715, 604, 1059, 454, 335, 264, 1016, 116, 982, 902, 729, 1009, 306, 453, 450, 608, 775, 328, 1192, 920, 758, 396, 465, 447, 757, 697, 129, 85, 246, 563, 1147, 1120, 6, 832, 935, 609, 790, 200, 1129, 672, 746, 644, 321, 613, 183, 858, 688, 628, 1102, 1045, 785, 598, 1167, 811, 502, 1118, 1046, 9, 267, 988, 207, 518, 1143, 361, 34, 165, 160, 1054, 789, 731, 1015, 61, 588, 522, 865, 162, 1032, 546, 397, 440, 718, 1029, 1213, 176, 1145, 602, 314]
valid_ids (0): []
train_ids (1094): [145, 881, 151, 73, 974, 994, 1065, 372, 16, 704, 273, 248, 941, 520, 511, 932, 1105, 1185, 27, 777, 854, 39, 492, 747, 665, 504, 853, 766, 106, 610, 1108, 505, 1207, 1199, 759, 945, 101, 499, 863, 153, 682, 213, 1123, 648, 1011, 823, 611, 701, 163, 1130, 867, 595, 79, 651, 185, 1021, 750, 817, 373, 467, 540, 794, 370, 685, 1041, 220, 1152, 1044, 1076, 50, 751, 65, 716, 885, 754, 846, 104, 510, 649, 1077, 808, 140, 764, 980, 1153, 491, 976, 87, 51, 78, 484, 202, 401, 1084, 938, 409, 445, 471, 296, 430, 178, 411, 43, 348, 795, 353, 260, 376, 708, 875, 1089, 1033, 992, 893, 464, 472, 981, 214, 824, 601, 1113, 894, 984, 516, 469, 605, 833, 154, 585, 969, 80, 550, 554, 1068, 1175, 436, 852, 455, 181, 422, 720, 995, 951, 1081, 76, 884, 134, 243, 1160, 897, 130, 721, 392, 1, 188, 801, 791, 1020, 54, 1200, 98, 752, 173, 136, 449, 537, 138, 559, 48, 964, 172, 459, 1075, 617, 973, 237, 913, 241, 44, 1173, 1090, 68, 671, 1043, 315, 907, 655, 1072, 1014, 497, 848, 132, 395, 418, 597, 657, 184, 828, 1017, 1187, 206, 979, 212, 394, 155, 669, 378, 426, 380, 782, 222, 303, 1103, 583, 215, 13, 375, 1126, 385, 367, 45, 486, 1197, 55, 1165, 1048, 8, 399, 526, 441, 23, 643, 139, 251, 57, 286, 916, 806, 93, 736, 114, 413, 64, 1040, 1080, 1100, 779, 133, 325, 218, 167, 1115, 1055, 621, 744, 1010, 804, 767, 1053, 1183, 331, 549, 535, 473, 438, 269, 257, 446, 30, 339, 431, 596, 1004, 738, 1155, 280, 904, 962, 1214, 987, 475, 740, 1144, 191, 614, 587, 855, 415, 1038, 886, 830, 1135, 1151, 1005, 532, 294, 507, 84, 72, 1169, 270, 631, 111, 141, 112, 1111, 926, 1019, 1154, 322, 209, 1078, 1082, 5, 641, 109, 281, 1037, 901, 1191, 650, 1085, 1086, 149, 47, 959, 914, 41, 53, 551, 574, 711, 577, 195, 388, 157, 1205, 384, 743, 878, 123, 1121, 891, 763, 501, 59, 898, 219, 755, 679, 99, 584, 942, 998, 33, 1172, 922, 586, 713, 798, 390, 1202, 31, 1162, 233, 519, 879, 1018, 476, 263, 1161, 1166, 1178, 1094, 691, 147, 658, 316, 850, 346, 802, 1012, 86, 12, 105, 146, 734, 1177, 954, 524, 386, 557, 307, 927, 463, 836, 369, 517, 947, 487, 341, 873, 1101, 856, 1098, 803, 282, 211, 654, 760, 327, 120, 349, 594, 645, 1156, 805, 1013, 402, 265, 966, 205, 1027, 1069, 872, 170, 320, 695, 533, 580, 81, 946, 462, 225, 377, 1024, 527, 748, 500, 675, 128, 786, 166, 678, 983, 1104, 337, 989, 703, 765, 778, 972, 433, 860, 626, 637, 939, 381, 521, 1099, 965, 75, 512, 592, 416, 933, 302, 635, 1061, 338, 326, 456, 1095, 272, 1140, 1079, 771, 807, 275, 444, 179, 287, 531, 870, 681, 692, 175, 2, 915, 624, 783, 534, 171, 1031, 1034, 117, 741, 694, 308, 849, 606, 556, 845, 88, 1042, 1007, 36, 428, 482, 796, 383, 690, 958, 1074, 676, 634, 1056, 677, 291, 94, 485, 816, 733, 943, 276, 40, 1127, 1157, 319, 639, 868, 17, 827, 198, 174, 180, 66, 515, 110, 529, 496, 223, 360, 687, 513, 834, 543, 899, 1066, 279, 1142, 840, 292, 837, 996, 882, 745, 1188, 799, 889, 548, 318, 1067, 625, 201, 11, 869, 877, 929, 108, 797, 58, 20, 1060, 1070, 457, 553, 821, 813, 773, 819, 895, 525, 1206, 581, 427, 843, 1062, 636, 452, 143, 1141, 1097, 956, 352, 406, 37, 25, 539, 247, 231, 28, 359, 579, 815, 590, 268, 393, 960, 523, 1131, 1000, 1051, 990, 770, 632, 1164, 148, 374, 362, 918, 414, 944, 739, 350, 693, 410, 137, 566, 405, 135, 561, 400, 217, 1168, 256, 1195, 627, 582, 24, 874, 421, 252, 1212, 638, 97, 667, 1109, 530, 310, 793, 948, 544, 1203, 971, 871, 255, 429, 589, 423, 812, 1181, 448, 127, 633, 203, 313, 1087, 1150, 892, 250, 186, 161, 355, 864, 1122, 552, 389, 753, 1091, 159, 936, 1022, 1083, 940, 1119, 831, 883, 60, 888, 931, 3, 498, 1063, 977, 1158, 368, 993, 652, 404, 7, 293, 702, 1028, 192, 1057, 673, 1189, 663, 800, 0, 451, 1171, 787, 728, 818, 612, 289, 32, 1039, 336, 164, 735, 949, 1026, 187, 558, 142, 49, 90, 1030, 905, 288, 1058, 21, 662, 169, 1093, 749, 506, 56, 309, 311, 495, 77, 122, 717, 857, 278, 1201, 1186, 91, 698, 483, 647, 236, 443, 387, 1184, 890, 1116, 115, 616, 118, 1180, 474, 952, 358, 488, 1210, 719, 92, 700, 826, 398, 1194, 1176, 304, 668, 193, 1096, 909, 1106, 89, 46, 844, 470, 235, 434, 38, 1035, 333, 74, 674, 466, 961, 70, 810, 593, 1052, 238, 285, 22, 266, 18, 1148, 330, 144, 156, 642, 640, 357, 62, 825, 226, 780, 653, 196, 571, 666, 242, 67, 1139, 103, 670, 567, 656, 622, 555, 1003, 340, 953, 774, 254, 937, 177, 458, 344, 686, 245, 706, 407, 194, 737, 489, 274, 934, 970, 1170, 244, 230, 494, 1204, 216, 575, 572, 514, 317, 1196, 10, 210, 724, 859, 912, 784, 1159, 371, 991, 967, 323, 851, 1163, 930, 365, 332, 950, 629, 364, 391, 578, 1190, 908, 607, 253, 382, 917, 1114, 1006, 190, 297, 618, 125, 820, 182, 1133, 707, 861, 1008, 356, 712, 52, 876, 503, 1001, 442, 1064, 541, 479, 957, 432, 121, 1134, 705, 299, 528, 999, 435, 158, 258, 408, 683, 509, 19, 102, 208, 565, 107, 283, 15, 477, 232, 630, 591, 600, 1132, 955, 329, 29, 814, 14, 570, 363, 822, 100, 345, 1073, 660, 1174, 493, 603, 301, 710, 1182, 113, 1071, 560, 437, 659, 168, 896, 542, 680, 1110, 781, 726, 866, 277, 646, 197, 1211, 756, 925, 615, 468, 661, 480, 842, 887, 35, 284, 725, 689, 419, 1138, 599, 124, 564, 997, 1146, 792, 1107, 1208, 204, 305, 841, 228, 847, 1050, 569, 239, 862, 1124, 562, 1025, 271, 838, 1136, 732, 568, 334, 1036, 975, 1112, 900, 417, 347, 619, 714, 420, 576, 839, 924, 928, 1092, 730, 1149, 985, 911, 221, 1088, 545, 259, 1198, 26, 1179, 126, 229, 968, 295, 727, 342, 664, 547, 262, 903, 723, 69, 424, 906, 1209, 809, 699, 150, 508, 96, 351, 460, 963, 249, 1193, 978, 63, 1049, 95, 224, 82, 722, 620, 788, 83, 986, 439, 290, 199, 1047, 696, 623, 119, 910, 234, 425, 366, 300, 709, 1117, 1002, 379, 354, 403, 772, 1023, 490, 919, 835]
Converting data to tensors
Normalising data
Finalising data preprocessing
running TWIG with settings:
test_ratio: 0.1
valid_ratio: 0.0
normalisation: minmax
n_bins: 60
optimiser: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 1 and 2: 1
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1184
rank avg (pred): 0.540 +- 0.001
mrr vals (pred, true): 0.014, 0.035
batch losses (mrrl, rdl): 0.0, 0.0167942494

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1214
rank avg (pred): 0.468 +- 0.006
mrr vals (pred, true): 0.016, 0.046
batch losses (mrrl, rdl): 0.0, 0.0045819576

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1190
rank avg (pred): 0.482 +- 0.004
mrr vals (pred, true): 0.015, 0.046
batch losses (mrrl, rdl): 0.0, 0.0049685426

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 100
rank avg (pred): 0.452 +- 0.003
mrr vals (pred, true): 0.016, 0.040
batch losses (mrrl, rdl): 0.0, 0.0041371919

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 112
rank avg (pred): 0.457 +- 0.003
mrr vals (pred, true): 0.016, 0.043
batch losses (mrrl, rdl): 0.0, 0.0047661196

Epoch over!
epoch time: 69.559

Saving checkpoint at [1] epoch 1
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 883
rank avg (pred): 0.457 +- 0.003
mrr vals (pred, true): 0.016, 0.044
batch losses (mrrl, rdl): 1.3799644709, 0.004585627

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 705
rank avg (pred): 0.281 +- 0.002
mrr vals (pred, true): 0.026, 0.050
batch losses (mrrl, rdl): 0.7158364654, 0.0413734876

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 50
rank avg (pred): 0.033 +- 0.001
mrr vals (pred, true): 0.185, 0.213
batch losses (mrrl, rdl): 2.002314806, 0.039943736

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 652
rank avg (pred): 0.080 +- 0.001
mrr vals (pred, true): 0.085, 0.048
batch losses (mrrl, rdl): 1.5001631975, 0.1938159764

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 269
rank avg (pred): 0.024 +- 0.000
mrr vals (pred, true): 0.235, 0.209
batch losses (mrrl, rdl): 1.792620182, 0.0441889651

Epoch over!
epoch time: 64.41

Saving checkpoint at [1] epoch 1
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.178 +- 0.001
mrr vals (pred, true): 0.040, 0.044

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   60 	     0 	 0.04132 	 0.02055 	 ~...
  126 	     1 	 0.04720 	 0.02111 	 ~...
  128 	     2 	 0.04722 	 0.02144 	 ~...
  126 	     3 	 0.04720 	 0.02177 	 ~...
   60 	     4 	 0.04132 	 0.02180 	 ~...
   72 	     5 	 0.04217 	 0.02240 	 ~...
   18 	     6 	 0.03898 	 0.02255 	 ~...
  124 	     7 	 0.04633 	 0.02291 	 ~...
  124 	     8 	 0.04633 	 0.02332 	 ~...
   56 	     9 	 0.04130 	 0.02345 	 ~...
   72 	    10 	 0.04217 	 0.02424 	 ~...
   90 	    11 	 0.04294 	 0.02453 	 ~...
   28 	    12 	 0.04008 	 0.02454 	 ~...
   90 	    13 	 0.04294 	 0.02495 	 ~...
   56 	    14 	 0.04130 	 0.02496 	 ~...
   80 	    15 	 0.04249 	 0.02501 	 ~...
   68 	    16 	 0.04215 	 0.02549 	 ~...
  110 	    17 	 0.04429 	 0.02554 	 ~...
   68 	    18 	 0.04215 	 0.02582 	 ~...
   80 	    19 	 0.04249 	 0.02586 	 ~...
  128 	    20 	 0.04722 	 0.02646 	 ~...
  140 	    21 	 0.05025 	 0.02679 	 ~...
  110 	    22 	 0.04429 	 0.02702 	 ~...
   28 	    23 	 0.04008 	 0.02749 	 ~...
   18 	    24 	 0.03898 	 0.02782 	 ~...
   78 	    25 	 0.04244 	 0.02964 	 ~...
   92 	    26 	 0.04313 	 0.03048 	 ~...
   74 	    27 	 0.04219 	 0.03057 	 ~...
  168 	    28 	 0.05570 	 0.03213 	 ~...
  168 	    29 	 0.05570 	 0.03236 	 ~...
  140 	    30 	 0.05025 	 0.03390 	 ~...
  160 	    31 	 0.05424 	 0.03671 	 ~...
   92 	    32 	 0.04313 	 0.03673 	 ~...
  170 	    33 	 0.05772 	 0.03811 	 ~...
  150 	    34 	 0.05346 	 0.03838 	 ~...
  170 	    35 	 0.05772 	 0.03844 	 ~...
  174 	    36 	 0.05865 	 0.03849 	 ~...
  176 	    37 	 0.05901 	 0.03906 	 ~...
  158 	    38 	 0.05389 	 0.03909 	 ~...
  164 	    39 	 0.05451 	 0.03923 	 ~...
   22 	    40 	 0.03937 	 0.03927 	 ~...
  158 	    41 	 0.05389 	 0.03930 	 ~...
   78 	    42 	 0.04244 	 0.03937 	 ~...
  162 	    43 	 0.05426 	 0.03946 	 ~...
  144 	    44 	 0.05252 	 0.03950 	 ~...
   74 	    45 	 0.04219 	 0.03953 	 ~...
  182 	    46 	 0.06094 	 0.03986 	 ~...
   32 	    47 	 0.04014 	 0.03995 	 ~...
   32 	    48 	 0.04014 	 0.04046 	 ~...
   10 	    49 	 0.03861 	 0.04047 	 ~...
  148 	    50 	 0.05310 	 0.04049 	 ~...
  136 	    51 	 0.04884 	 0.04081 	 ~...
  134 	    52 	 0.04876 	 0.04084 	 ~...
  138 	    53 	 0.04933 	 0.04099 	 ~...
   50 	    54 	 0.04097 	 0.04099 	 ~...
  154 	    55 	 0.05356 	 0.04109 	 ~...
  136 	    56 	 0.04884 	 0.04119 	 ~...
  154 	    57 	 0.05356 	 0.04121 	 ~...
  150 	    58 	 0.05346 	 0.04149 	 ~...
   70 	    59 	 0.04217 	 0.04191 	 ~...
   30 	    60 	 0.04011 	 0.04214 	 ~...
   48 	    61 	 0.04097 	 0.04252 	 ~...
  116 	    62 	 0.04500 	 0.04255 	 ~...
  142 	    63 	 0.05169 	 0.04260 	 ~...
  130 	    64 	 0.04799 	 0.04269 	 ~...
   82 	    65 	 0.04256 	 0.04270 	 ~...
   26 	    66 	 0.03969 	 0.04271 	 ~...
  160 	    67 	 0.05424 	 0.04277 	 ~...
  114 	    68 	 0.04490 	 0.04280 	 ~...
   66 	    69 	 0.04166 	 0.04287 	 ~...
   52 	    70 	 0.04110 	 0.04293 	 ~...
  184 	    71 	 0.06136 	 0.04297 	 ~...
    0 	    72 	 0.03620 	 0.04327 	 ~...
  138 	    73 	 0.04933 	 0.04343 	 ~...
    4 	    74 	 0.03741 	 0.04362 	 ~...
  176 	    75 	 0.05901 	 0.04369 	 ~...
   14 	    76 	 0.03893 	 0.04393 	 ~...
    2 	    77 	 0.03730 	 0.04404 	 ~...
   16 	    78 	 0.03894 	 0.04407 	 ~...
  106 	    79 	 0.04376 	 0.04420 	 ~...
   34 	    80 	 0.04032 	 0.04423 	 ~...
  102 	    81 	 0.04348 	 0.04423 	 ~...
   24 	    82 	 0.03968 	 0.04424 	 ~...
  104 	    83 	 0.04376 	 0.04434 	 ~...
   40 	    84 	 0.04080 	 0.04436 	 ~...
   44 	    85 	 0.04084 	 0.04457 	 ~...
   12 	    86 	 0.03882 	 0.04466 	 ~...
  174 	    87 	 0.05865 	 0.04473 	 ~...
  186 	    88 	 0.06447 	 0.04480 	 ~...
   96 	    89 	 0.04323 	 0.04507 	 ~...
  144 	    90 	 0.05252 	 0.04512 	 ~...
   36 	    91 	 0.04058 	 0.04530 	 ~...
   30 	    92 	 0.04011 	 0.04542 	 ~...
   14 	    93 	 0.03893 	 0.04564 	 ~...
  152 	    94 	 0.05355 	 0.04566 	 ~...
  162 	    95 	 0.05426 	 0.04577 	 ~...
  122 	    96 	 0.04612 	 0.04579 	 ~...
   38 	    97 	 0.04064 	 0.04581 	 ~...
   64 	    98 	 0.04156 	 0.04584 	 ~...
  112 	    99 	 0.04451 	 0.04586 	 ~...
   98 	   100 	 0.04334 	 0.04599 	 ~...
   82 	   101 	 0.04256 	 0.04621 	 ~...
  156 	   102 	 0.05359 	 0.04631 	 ~...
  112 	   103 	 0.04451 	 0.04635 	 ~...
   16 	   104 	 0.03894 	 0.04651 	 ~...
   44 	   105 	 0.04084 	 0.04657 	 ~...
   54 	   106 	 0.04128 	 0.04666 	 ~...
    4 	   107 	 0.03741 	 0.04667 	 ~...
  100 	   108 	 0.04348 	 0.04670 	 ~...
   34 	   109 	 0.04032 	 0.04671 	 ~...
  118 	   110 	 0.04543 	 0.04689 	 ~...
   48 	   111 	 0.04097 	 0.04704 	 ~...
  182 	   112 	 0.06094 	 0.04715 	 ~...
  114 	   113 	 0.04490 	 0.04727 	 ~...
   46 	   114 	 0.04096 	 0.04728 	 ~...
  104 	   115 	 0.04376 	 0.04736 	 ~...
  122 	   116 	 0.04612 	 0.04741 	 ~...
  164 	   117 	 0.05451 	 0.04745 	 ~...
  148 	   118 	 0.05310 	 0.04748 	 ~...
  172 	   119 	 0.05821 	 0.04762 	 ~...
  120 	   120 	 0.04589 	 0.04781 	 ~...
  134 	   121 	 0.04876 	 0.04783 	 ~...
   84 	   122 	 0.04257 	 0.04796 	 ~...
   76 	   123 	 0.04231 	 0.04803 	 ~...
   62 	   124 	 0.04135 	 0.04806 	 ~...
   58 	   125 	 0.04130 	 0.04813 	 ~...
  172 	   126 	 0.05821 	 0.04814 	 ~...
   66 	   127 	 0.04166 	 0.04815 	 ~...
   50 	   128 	 0.04097 	 0.04849 	 ~...
   54 	   129 	 0.04128 	 0.04886 	 ~...
   36 	   130 	 0.04058 	 0.04890 	 ~...
   20 	   131 	 0.03931 	 0.04890 	 ~...
   88 	   132 	 0.04265 	 0.04894 	 ~...
  184 	   133 	 0.06136 	 0.04896 	 ~...
  186 	   134 	 0.06447 	 0.04898 	 ~...
  100 	   135 	 0.04348 	 0.04908 	 ~...
  166 	   136 	 0.05464 	 0.04911 	 ~...
   10 	   137 	 0.03861 	 0.04917 	 ~...
   88 	   138 	 0.04265 	 0.04920 	 ~...
  132 	   139 	 0.04833 	 0.04929 	 ~...
   42 	   140 	 0.04082 	 0.04938 	 ~...
    6 	   141 	 0.03764 	 0.04956 	 ~...
  120 	   142 	 0.04589 	 0.04969 	 ~...
   94 	   143 	 0.04319 	 0.04970 	 ~...
   22 	   144 	 0.03937 	 0.04970 	 ~...
   12 	   145 	 0.03882 	 0.04986 	 ~...
   40 	   146 	 0.04080 	 0.04986 	 ~...
   38 	   147 	 0.04064 	 0.05016 	 ~...
   46 	   148 	 0.04096 	 0.05019 	 ~...
   20 	   149 	 0.03931 	 0.05023 	 ~...
  106 	   150 	 0.04376 	 0.05029 	 ~...
   76 	   151 	 0.04231 	 0.05041 	 ~...
   24 	   152 	 0.03968 	 0.05043 	 ~...
  108 	   153 	 0.04428 	 0.05045 	 ~...
  166 	   154 	 0.05464 	 0.05049 	 ~...
    8 	   155 	 0.03836 	 0.05068 	 ~...
  178 	   156 	 0.06013 	 0.05095 	 ~...
   96 	   157 	 0.04323 	 0.05103 	 ~...
  130 	   158 	 0.04799 	 0.05112 	 ~...
  180 	   159 	 0.06047 	 0.05116 	 ~...
  132 	   160 	 0.04833 	 0.05133 	 ~...
   86 	   161 	 0.04262 	 0.05157 	 ~...
   70 	   162 	 0.04217 	 0.05159 	 ~...
  146 	   163 	 0.05284 	 0.05162 	 ~...
  156 	   164 	 0.05359 	 0.05199 	 ~...
   58 	   165 	 0.04130 	 0.05208 	 ~...
   98 	   166 	 0.04334 	 0.05208 	 ~...
  178 	   167 	 0.06013 	 0.05213 	 ~...
  142 	   168 	 0.05169 	 0.05223 	 ~...
   86 	   169 	 0.04262 	 0.05232 	 ~...
   84 	   170 	 0.04257 	 0.05235 	 ~...
  118 	   171 	 0.04543 	 0.05299 	 ~...
   64 	   172 	 0.04156 	 0.05300 	 ~...
  116 	   173 	 0.04500 	 0.05320 	 ~...
   52 	   174 	 0.04110 	 0.05322 	 ~...
  180 	   175 	 0.06047 	 0.05340 	 ~...
   42 	   176 	 0.04082 	 0.05365 	 ~...
    2 	   177 	 0.03730 	 0.05380 	 ~...
    0 	   178 	 0.03620 	 0.05403 	 ~...
   26 	   179 	 0.03969 	 0.05409 	 ~...
    6 	   180 	 0.03764 	 0.05446 	 ~...
    8 	   181 	 0.03836 	 0.05451 	 ~...
   62 	   182 	 0.04135 	 0.05479 	 ~...
   94 	   183 	 0.04319 	 0.05518 	 ~...
  102 	   184 	 0.04348 	 0.05524 	 ~...
  146 	   185 	 0.05284 	 0.05554 	 ~...
  152 	   186 	 0.05355 	 0.05634 	 ~...
  108 	   187 	 0.04428 	 0.05647 	 ~...
  208 	   188 	 0.19278 	 0.17062 	 ~...
  208 	   189 	 0.19278 	 0.17262 	 ~...
  210 	   190 	 0.19401 	 0.17547 	 ~...
  188 	   191 	 0.14133 	 0.17571 	 m..s
  206 	   192 	 0.19237 	 0.18020 	 ~...
  210 	   193 	 0.19401 	 0.18760 	 ~...
  194 	   194 	 0.17454 	 0.19157 	 ~...
  206 	   195 	 0.19237 	 0.19958 	 ~...
  192 	   196 	 0.16408 	 0.20036 	 m..s
  196 	   197 	 0.17606 	 0.20165 	 ~...
  188 	   198 	 0.14133 	 0.20542 	 m..s
  192 	   199 	 0.16408 	 0.20643 	 m..s
  212 	   200 	 0.20307 	 0.20872 	 ~...
  216 	   201 	 0.22461 	 0.21149 	 ~...
  222 	   202 	 0.24545 	 0.21610 	 ~...
  216 	   203 	 0.22461 	 0.21805 	 ~...
  196 	   204 	 0.17606 	 0.22162 	 m..s
  202 	   205 	 0.18902 	 0.22399 	 m..s
  212 	   206 	 0.20307 	 0.22490 	 ~...
  194 	   207 	 0.17454 	 0.22581 	 m..s
  220 	   208 	 0.23550 	 0.22597 	 ~...
  224 	   209 	 0.24702 	 0.22778 	 ~...
  226 	   210 	 0.24999 	 0.22812 	 ~...
  200 	   211 	 0.17986 	 0.23123 	 m..s
  202 	   212 	 0.18902 	 0.23565 	 m..s
  222 	   213 	 0.24545 	 0.23696 	 ~...
  200 	   214 	 0.17986 	 0.23739 	 m..s
  226 	   215 	 0.24999 	 0.24218 	 ~...
  220 	   216 	 0.23550 	 0.24265 	 ~...
  198 	   217 	 0.17930 	 0.24392 	 m..s
  204 	   218 	 0.19114 	 0.24955 	 m..s
  198 	   219 	 0.17930 	 0.25163 	 m..s
  224 	   220 	 0.24702 	 0.25327 	 ~...
  204 	   221 	 0.19114 	 0.25558 	 m..s
  230 	   222 	 0.30910 	 0.27144 	 m..s
  228 	   223 	 0.29637 	 0.27479 	 ~...
  190 	   224 	 0.16082 	 0.28029 	 MISS
  218 	   225 	 0.23385 	 0.28160 	 m..s
  214 	   226 	 0.20804 	 0.28782 	 m..s
  218 	   227 	 0.23385 	 0.29019 	 m..s
  214 	   228 	 0.20804 	 0.29484 	 m..s
  228 	   229 	 0.29637 	 0.29780 	 ~...
  230 	   230 	 0.30910 	 0.29788 	 ~...
  232 	   231 	 0.32205 	 0.30223 	 ~...
  232 	   232 	 0.32205 	 0.30851 	 ~...
  234 	   233 	 0.33517 	 0.31981 	 ~...
  190 	   234 	 0.16082 	 0.33485 	 MISS
  234 	   235 	 0.33517 	 0.33593 	 ~...
  238 	   236 	 0.41802 	 0.37064 	 m..s
  236 	   237 	 0.41739 	 0.37350 	 m..s
  238 	   238 	 0.41802 	 0.39525 	 ~...
  240 	   239 	 0.43330 	 0.40129 	 m..s
  236 	   240 	 0.41739 	 0.41442 	 ~...
  240 	   241 	 0.43330 	 0.43932 	 ~...
==========================================
r_mrr = 0.9674295783042908
r2_mrr = 0.9351667761802673
spearmanr_mrr@5 = 0.8420700430870056
spearmanr_mrr@10 = 0.9302670955657959
spearmanr_mrr@50 = 0.9845486283302307
spearmanr_mrr@100 = 0.9871463775634766
spearmanr_mrr@All = 0.9908809065818787
==========================================
test time: 2.017
Done Testing dataset UMLS
total time taken: 159.9702959060669
training time taken: 135.99626541137695
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'UMLS': tensor(0.9715)}, 'r2_mrr': {'UMLS': tensor(0.9402)}, 'spearmanr_mrr@5': {'UMLS': tensor(0.7401)}, 'spearmanr_mrr@10': {'UMLS': tensor(0.8951)}, 'spearmanr_mrr@50': {'UMLS': tensor(0.8898)}, 'spearmanr_mrr@100': {'UMLS': tensor(0.9700)}, 'spearmanr_mrr@All': {'UMLS': tensor(0.9807)}, 'test_loss': {'UMLS': 1842.4979324936867}}
The best settings found were:
normalisation: minmax
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [1, 1]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]

Now training your final model!
Starting TWIG!
Loading datasets
Loading UMLS...
- loading run 2.1...
- loading run 2.2...
Creating the train-test split
using splits:
test_ids (121): [526, 350, 157, 194, 130, 49, 1050, 92, 226, 68, 1017, 1036, 228, 330, 847, 270, 998, 1145, 100, 290, 885, 94, 182, 410, 1120, 567, 660, 926, 1066, 709, 365, 64, 196, 1170, 1082, 832, 518, 193, 451, 977, 1199, 622, 1047, 230, 517, 914, 1155, 1187, 1080, 1147, 736, 1067, 1031, 321, 40, 397, 669, 468, 264, 282, 388, 127, 500, 133, 763, 633, 55, 26, 947, 58, 217, 342, 925, 435, 1169, 156, 1182, 1088, 231, 1211, 309, 504, 390, 478, 795, 298, 154, 75, 848, 102, 1086, 735, 484, 578, 232, 115, 988, 101, 768, 29, 114, 15, 584, 855, 317, 442, 923, 1107, 149, 33, 687, 446, 52, 1209, 825, 793, 623, 1174, 809, 572, 898]
valid_ids (0): []
train_ids (1094): [617, 458, 960, 23, 370, 934, 946, 966, 418, 1180, 1136, 1189, 699, 804, 719, 491, 421, 1097, 909, 1190, 651, 563, 1091, 1101, 957, 519, 872, 754, 1100, 534, 1061, 608, 917, 335, 1063, 752, 300, 830, 817, 906, 714, 394, 1085, 70, 234, 25, 642, 543, 396, 405, 1102, 958, 986, 6, 14, 1052, 681, 747, 1194, 1099, 785, 878, 132, 907, 818, 177, 181, 443, 559, 541, 351, 187, 838, 996, 674, 219, 545, 124, 1167, 781, 408, 46, 1151, 842, 1161, 767, 1053, 634, 1162, 520, 296, 229, 672, 355, 654, 1065, 78, 653, 1051, 1198, 1015, 88, 764, 766, 510, 1060, 198, 1022, 1173, 1183, 950, 546, 753, 18, 1117, 1159, 464, 874, 1029, 1046, 368, 433, 385, 216, 469, 973, 897, 483, 918, 595, 641, 964, 962, 743, 952, 1089, 531, 945, 581, 844, 948, 406, 493, 116, 929, 374, 900, 724, 680, 74, 3, 237, 225, 1084, 824, 647, 1203, 295, 965, 1115, 867, 894, 652, 615, 757, 215, 782, 1075, 881, 1139, 265, 1131, 203, 721, 788, 554, 257, 942, 1055, 470, 129, 453, 827, 1129, 411, 1160, 125, 536, 1095, 1126, 631, 331, 93, 1008, 1012, 883, 904, 297, 141, 211, 1193, 1030, 539, 17, 940, 286, 619, 51, 482, 557, 621, 1021, 1206, 126, 834, 730, 773, 969, 902, 547, 1028, 738, 496, 812, 716, 577, 341, 1093, 381, 361, 54, 276, 516, 140, 289, 1132, 407, 415, 1092, 486, 937, 81, 932, 589, 713, 462, 551, 787, 334, 791, 1202, 240, 803, 792, 673, 599, 935, 912, 137, 884, 967, 542, 566, 603, 1130, 61, 903, 197, 303, 495, 661, 751, 200, 444, 744, 325, 227, 238, 41, 627, 854, 43, 27, 1096, 467, 953, 895, 941, 573, 1034, 5, 1116, 60, 839, 463, 9, 50, 258, 273, 1019, 185, 53, 436, 455, 90, 207, 106, 1158, 871, 199, 214, 382, 1213, 591, 288, 352, 525, 337, 625, 2, 338, 353, 87, 1041, 120, 1076, 594, 618, 356, 1087, 1205, 865, 693, 158, 210, 1007, 1000, 1140, 602, 798, 254, 987, 320, 794, 989, 514, 733, 645, 10, 670, 650, 366, 80, 244, 490, 432, 99, 555, 1109, 684, 708, 235, 710, 160, 1072, 1009, 820, 833, 737, 95, 876, 107, 471, 191, 404, 202, 870, 1210, 742, 799, 722, 153, 123, 671, 278, 279, 972, 85, 802, 173, 362, 816, 1178, 246, 574, 658, 481, 218, 1083, 332, 981, 1149, 638, 822, 1163, 762, 924, 968, 845, 683, 450, 393, 69, 729, 930, 527, 560, 796, 1171, 367, 1070, 668, 639, 643, 261, 242, 56, 267, 1148, 836, 971, 135, 167, 299, 667, 758, 503, 163, 509, 993, 318, 1146, 963, 190, 164, 701, 956, 223, 497, 324, 460, 807, 920, 728, 741, 380, 201, 512, 810, 395, 571, 893, 313, 949, 561, 255, 727, 1106, 933, 580, 524, 283, 1006, 251, 826, 852, 913, 249, 686, 612, 439, 635, 755, 239, 862, 636, 457, 108, 777, 649, 428, 1069, 1081, 1135, 565, 20, 241, 315, 1111, 841, 931, 786, 850, 136, 695, 302, 1037, 498, 447, 688, 430, 1122, 739, 323, 253, 208, 152, 860, 37, 919, 151, 441, 212, 828, 707, 974, 287, 725, 448, 489, 882, 951, 175, 590, 712, 84, 905, 979, 119, 245, 703, 1164, 488, 663, 875, 776, 538, 280, 1057, 357, 936, 413, 480, 665, 811, 434, 65, 82, 511, 508, 35, 259, 256, 438, 604, 533, 740, 570, 19, 364, 955, 423, 1185, 452, 384, 646, 383, 540, 62, 569, 371, 759, 1181, 71, 746, 336, 502, 103, 377, 779, 605, 1, 916, 601, 864, 465, 606, 479, 859, 562, 38, 138, 319, 308, 976, 596, 1044, 588, 59, 1208, 706, 305, 984, 857, 513, 1062, 552, 314, 611, 600, 401, 523, 1074, 696, 1143, 1123, 726, 349, 345, 403, 614, 620, 772, 765, 409, 831, 1153, 656, 532, 322, 28, 440, 4, 1138, 664, 36, 1079, 873, 112, 506, 274, 221, 626, 1142, 72, 1027, 806, 943, 461, 13, 698, 1188, 294, 8, 414, 583, 690, 761, 169, 889, 474, 144, 675, 91, 843, 1010, 139, 83, 922, 165, 170, 1134, 454, 263, 568, 475, 985, 416, 629, 863, 775, 975, 307, 76, 1207, 921, 676, 888, 174, 310, 1018, 262, 1020, 911, 145, 1133, 326, 171, 575, 715, 1197, 424, 392, 586, 990, 373, 598, 783, 311, 172, 1201, 769, 343, 346, 146, 339, 213, 369, 928, 750, 821, 272, 204, 429, 1071, 340, 354, 1212, 938, 521, 980, 316, 944, 1184, 1179, 32, 528, 1003, 179, 1043, 48, 399, 702, 808, 220, 89, 720, 814, 1195, 910, 908, 1176, 1001, 628, 1054, 564, 192, 176, 732, 1196, 21, 1059, 398, 1073, 593, 333, 711, 113, 477, 1108, 801, 417, 1157, 887, 79, 359, 425, 1098, 535, 915, 183, 1016, 1144, 472, 1124, 869, 1026, 745, 537, 1078, 499, 1154, 77, 780, 879, 723, 73, 1005, 544, 756, 1105, 222, 632, 402, 1118, 45, 819, 982, 437, 691, 548, 1068, 734, 0, 329, 39, 391, 104, 1128, 188, 689, 685, 426, 189, 31, 494, 679, 57, 1156, 999, 63, 749, 118, 16, 431, 236, 770, 1033, 30, 269, 877, 558, 1090, 378, 1040, 375, 1049, 328, 992, 961, 47, 449, 271, 122, 312, 1127, 1172, 1038, 186, 1048, 748, 853, 978, 150, 168, 1200, 142, 487, 587, 607, 549, 507, 797, 970, 344, 360, 248, 771, 637, 42, 456, 7, 347, 250, 301, 121, 731, 1056, 419, 285, 522, 616, 556, 613, 1011, 284, 1103, 1112, 34, 376, 1013, 682, 66, 1025, 1165, 386, 44, 1064, 1077, 597, 206, 1002, 195, 224, 358, 412, 459, 856, 692, 98, 1014, 205, 1121, 1023, 899, 260, 243, 281, 476, 1214, 304, 501, 694, 835, 868, 427, 67, 760, 861, 155, 1186, 11, 891, 372, 466, 582, 111, 896, 293, 655, 275, 530, 306, 697, 609, 143, 1039, 823, 184, 678, 22, 515, 983, 718, 994, 610, 1110, 846, 1168, 550, 829, 892, 247, 1104, 161, 148, 117, 1191, 1166, 12, 1042, 1204, 1177, 997, 420, 1137, 886, 209, 277, 837, 180, 1032, 1004, 162, 851, 790, 485, 579, 939, 492, 1152, 592, 644, 96, 1175, 387, 422, 657, 529, 553, 576, 166, 717, 1035, 849, 1150, 86, 473, 880, 640, 705, 630, 927, 991, 959, 1141, 348, 105, 677, 389, 110, 901, 784, 1114, 266, 24, 363, 445, 97, 774, 666, 1113, 778, 704, 954, 789, 109, 134, 866, 292, 995, 1024, 327, 505, 1058, 700, 268, 800, 1094, 1045, 662, 400, 659, 805, 379, 890, 178, 813, 815, 233, 147, 585, 291, 159, 1125, 648, 858, 131, 252, 1192, 624, 1119, 840, 128]
Converting data to tensors
Normalising data
Finalising data preprocessing
running TWIG with settings:
test_ratio: 0.1
valid_ratio: 0.0
normalisation: minmax
n_bins: 30
optimiser: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 560
rank avg (pred): 0.521 +- 0.002
mrr vals (pred, true): 0.014, 0.026
batch losses (mrrl, rdl): 0.0, 0.0048868535

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 1041
rank avg (pred): 0.457 +- 0.009
mrr vals (pred, true): 0.016, 0.049
batch losses (mrrl, rdl): 0.0, 0.0136207407

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 591
rank avg (pred): 0.469 +- 0.000
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 0.0116106318

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 269
rank avg (pred): 0.184 +- 0.019
mrr vals (pred, true): 0.039, 0.209
batch losses (mrrl, rdl): 0.0, 0.0062570078

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 531
rank avg (pred): 0.475 +- 0.001
mrr vals (pred, true): 0.015, 0.024
batch losses (mrrl, rdl): 0.0, 0.0082717771

Epoch over!
epoch time: 40.344

Epoch 2 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 720
rank avg (pred): 0.467 +- 0.000
mrr vals (pred, true): 0.016, 0.051
batch losses (mrrl, rdl): 0.0, 0.0141757773

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 797
rank avg (pred): 0.469 +- 0.025
mrr vals (pred, true): 0.016, 0.052
batch losses (mrrl, rdl): 0.0, 0.0164474286

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 333
rank avg (pred): 0.454 +- 0.044
mrr vals (pred, true): 0.016, 0.052
batch losses (mrrl, rdl): 0.0, 0.0126211764

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 1140
rank avg (pred): 0.518 +- 0.082
mrr vals (pred, true): 0.015, 0.026
batch losses (mrrl, rdl): 0.0, 0.0051457868

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 430
rank avg (pred): 0.447 +- 0.259
mrr vals (pred, true): 0.058, 0.055
batch losses (mrrl, rdl): 0.0, 0.0010645533

Epoch over!
epoch time: 39.162

Saving checkpoint at [1] epoch 2
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 524
rank avg (pred): 0.526 +- 0.246
mrr vals (pred, true): 0.033, 0.023
batch losses (mrrl, rdl): 0.3083527982, 0.0007256572

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1025
rank avg (pred): 0.451 +- 0.161
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0085709607, 0.0069336509

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 215
rank avg (pred): 0.356 +- 0.205
mrr vals (pred, true): 0.127, 0.050
batch losses (mrrl, rdl): 7.2985320091, 0.0170316026

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 259
rank avg (pred): 0.202 +- 0.188
mrr vals (pred, true): 0.267, 0.204
batch losses (mrrl, rdl): 9.9484729767, 0.0044464436

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1040
rank avg (pred): 0.436 +- 0.125
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.1390388906, 0.0070146741

Epoch over!
epoch time: 40.093

Epoch 2 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 627
rank avg (pred): 0.221 +- 0.154
mrr vals (pred, true): 0.047, 0.037
batch losses (mrrl, rdl): 0.0135282204, 0.1675770134

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1184
rank avg (pred): 0.225 +- 0.162
mrr vals (pred, true): 0.046, 0.035
batch losses (mrrl, rdl): 0.0161690451, 0.1361697763

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 975
rank avg (pred): 0.059 +- 0.049
mrr vals (pred, true): 0.356, 0.296
batch losses (mrrl, rdl): 14.6667127609, 0.041339457

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 710
rank avg (pred): 0.232 +- 0.180
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 0.0001707056, 0.1320089251

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 368
rank avg (pred): 0.442 +- 0.130
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 0.0010958442, 0.0067469249

Epoch over!
epoch time: 39.24

Epoch 3 -- 
running batch: 0 / 2188 and superbatch(1); data from UMLS, run 2.1, exp 45
rank avg (pred): 0.256 +- 0.180
mrr vals (pred, true): 0.207, 0.219
batch losses (mrrl, rdl): 0.4045618474, 0.0236644112

running batch: 500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 124
rank avg (pred): 0.400 +- 0.106
mrr vals (pred, true): 0.039, 0.049
batch losses (mrrl, rdl): 0.1403627992, 0.0158299096

running batch: 1000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 1049
rank avg (pred): 0.411 +- 0.111
mrr vals (pred, true): 0.048, 0.044
batch losses (mrrl, rdl): 0.0035680861, 0.010172178

running batch: 1500 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 583
rank avg (pred): 0.232 +- 0.182
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 0.0001970118, 0.1533810347

running batch: 2000 / 2188 and superbatch(1); data from UMLS, run 2.2, exp 450
rank avg (pred): 0.455 +- 0.100
mrr vals (pred, true): 0.044, 0.042
batch losses (mrrl, rdl): 0.0444324426, 0.0090887602

Epoch over!
epoch time: 38.828

Saving checkpoint at [1] epoch 3
Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.348 +- 0.118
mrr vals (pred, true): 0.051, 0.024

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  134 	     0 	 0.05834 	 0.01894 	 m..s
  134 	     1 	 0.05834 	 0.02094 	 m..s
   58 	     2 	 0.05122 	 0.02193 	 ~...
   32 	     3 	 0.04845 	 0.02314 	 ~...
  110 	     4 	 0.05561 	 0.02383 	 m..s
  110 	     5 	 0.05561 	 0.02404 	 m..s
   72 	     6 	 0.05161 	 0.02423 	 ~...
   58 	     7 	 0.05122 	 0.02447 	 ~...
  108 	     8 	 0.05555 	 0.02453 	 m..s
  108 	     9 	 0.05555 	 0.02495 	 m..s
   16 	    10 	 0.04490 	 0.02530 	 ~...
   32 	    11 	 0.04845 	 0.02536 	 ~...
  106 	    12 	 0.05547 	 0.02549 	 ~...
   78 	    13 	 0.05237 	 0.02554 	 ~...
   16 	    14 	 0.04490 	 0.02569 	 ~...
  106 	    15 	 0.05547 	 0.02582 	 ~...
   72 	    16 	 0.05161 	 0.02591 	 ~...
   78 	    17 	 0.05237 	 0.02702 	 ~...
   24 	    18 	 0.04581 	 0.02943 	 ~...
   22 	    19 	 0.04568 	 0.02966 	 ~...
   24 	    20 	 0.04581 	 0.03025 	 ~...
   18 	    21 	 0.04540 	 0.03048 	 ~...
   14 	    22 	 0.04490 	 0.03139 	 ~...
   22 	    23 	 0.04568 	 0.03193 	 ~...
    0 	    24 	 0.04300 	 0.03210 	 ~...
    2 	    25 	 0.04302 	 0.03224 	 ~...
   12 	    26 	 0.04324 	 0.03237 	 ~...
   12 	    27 	 0.04324 	 0.03252 	 ~...
   14 	    28 	 0.04490 	 0.03275 	 ~...
    0 	    29 	 0.04300 	 0.03388 	 ~...
    2 	    30 	 0.04302 	 0.03440 	 ~...
   56 	    31 	 0.05115 	 0.03461 	 ~...
   56 	    32 	 0.05115 	 0.03549 	 ~...
    4 	    33 	 0.04304 	 0.03576 	 ~...
   98 	    34 	 0.05414 	 0.03629 	 ~...
   42 	    35 	 0.05041 	 0.03659 	 ~...
   18 	    36 	 0.04540 	 0.03673 	 ~...
    4 	    37 	 0.04304 	 0.03695 	 ~...
   96 	    38 	 0.05407 	 0.03698 	 ~...
   30 	    39 	 0.04825 	 0.03732 	 ~...
   26 	    40 	 0.04792 	 0.03808 	 ~...
   26 	    41 	 0.04792 	 0.03889 	 ~...
   38 	    42 	 0.04973 	 0.03943 	 ~...
  152 	    43 	 0.06466 	 0.03947 	 ~...
   40 	    44 	 0.05007 	 0.03971 	 ~...
  170 	    45 	 0.07923 	 0.03980 	 m..s
   64 	    46 	 0.05145 	 0.04007 	 ~...
  154 	    47 	 0.06502 	 0.04022 	 ~...
  136 	    48 	 0.05860 	 0.04041 	 ~...
   28 	    49 	 0.04808 	 0.04045 	 ~...
   46 	    50 	 0.05087 	 0.04046 	 ~...
   48 	    51 	 0.05094 	 0.04064 	 ~...
  174 	    52 	 0.08465 	 0.04067 	 m..s
   52 	    53 	 0.05101 	 0.04081 	 ~...
  164 	    54 	 0.07535 	 0.04082 	 m..s
  146 	    55 	 0.06059 	 0.04120 	 ~...
  164 	    56 	 0.07535 	 0.04132 	 m..s
  168 	    57 	 0.07757 	 0.04144 	 m..s
   46 	    58 	 0.05087 	 0.04189 	 ~...
  174 	    59 	 0.08465 	 0.04241 	 m..s
  162 	    60 	 0.07417 	 0.04252 	 m..s
   66 	    61 	 0.05145 	 0.04264 	 ~...
  158 	    62 	 0.06946 	 0.04283 	 ~...
   76 	    63 	 0.05183 	 0.04286 	 ~...
   94 	    64 	 0.05384 	 0.04290 	 ~...
   54 	    65 	 0.05108 	 0.04292 	 ~...
  138 	    66 	 0.05884 	 0.04292 	 ~...
   54 	    67 	 0.05108 	 0.04305 	 ~...
  120 	    68 	 0.05636 	 0.04307 	 ~...
  112 	    69 	 0.05574 	 0.04315 	 ~...
  146 	    70 	 0.06059 	 0.04316 	 ~...
  152 	    71 	 0.06466 	 0.04318 	 ~...
   42 	    72 	 0.05041 	 0.04330 	 ~...
   88 	    73 	 0.05300 	 0.04332 	 ~...
  168 	    74 	 0.07757 	 0.04346 	 m..s
   70 	    75 	 0.05156 	 0.04353 	 ~...
   70 	    76 	 0.05156 	 0.04356 	 ~...
  116 	    77 	 0.05603 	 0.04369 	 ~...
  160 	    78 	 0.07173 	 0.04372 	 ~...
   94 	    79 	 0.05384 	 0.04381 	 ~...
   62 	    80 	 0.05139 	 0.04388 	 ~...
   90 	    81 	 0.05315 	 0.04423 	 ~...
  114 	    82 	 0.05592 	 0.04431 	 ~...
  148 	    83 	 0.06156 	 0.04451 	 ~...
    6 	    84 	 0.04316 	 0.04463 	 ~...
  100 	    85 	 0.05424 	 0.04463 	 ~...
   74 	    86 	 0.05164 	 0.04465 	 ~...
  142 	    87 	 0.05908 	 0.04465 	 ~...
  142 	    88 	 0.05908 	 0.04473 	 ~...
   28 	    89 	 0.04808 	 0.04481 	 ~...
   68 	    90 	 0.05146 	 0.04487 	 ~...
   80 	    91 	 0.05277 	 0.04502 	 ~...
   96 	    92 	 0.05407 	 0.04510 	 ~...
  124 	    93 	 0.05698 	 0.04515 	 ~...
   64 	    94 	 0.05145 	 0.04518 	 ~...
  150 	    95 	 0.06443 	 0.04526 	 ~...
  102 	    96 	 0.05436 	 0.04531 	 ~...
   10 	    97 	 0.04322 	 0.04586 	 ~...
  166 	    98 	 0.07664 	 0.04586 	 m..s
  124 	    99 	 0.05698 	 0.04593 	 ~...
  136 	   100 	 0.05860 	 0.04594 	 ~...
  140 	   101 	 0.05891 	 0.04596 	 ~...
   86 	   102 	 0.05288 	 0.04599 	 ~...
  176 	   103 	 0.08663 	 0.04603 	 m..s
  118 	   104 	 0.05625 	 0.04608 	 ~...
  180 	   105 	 0.09225 	 0.04618 	 m..s
  100 	   106 	 0.05424 	 0.04618 	 ~...
  178 	   107 	 0.08791 	 0.04620 	 m..s
  166 	   108 	 0.07664 	 0.04635 	 m..s
   92 	   109 	 0.05323 	 0.04639 	 ~...
  182 	   110 	 0.09412 	 0.04650 	 m..s
  118 	   111 	 0.05625 	 0.04658 	 ~...
   50 	   112 	 0.05101 	 0.04666 	 ~...
  114 	   113 	 0.05592 	 0.04690 	 ~...
  132 	   114 	 0.05812 	 0.04696 	 ~...
  156 	   115 	 0.06847 	 0.04699 	 ~...
   44 	   116 	 0.05059 	 0.04704 	 ~...
  162 	   117 	 0.07417 	 0.04704 	 ~...
    6 	   118 	 0.04316 	 0.04712 	 ~...
   30 	   119 	 0.04825 	 0.04718 	 ~...
  180 	   120 	 0.09225 	 0.04720 	 m..s
  182 	   121 	 0.09412 	 0.04721 	 m..s
  160 	   122 	 0.07173 	 0.04725 	 ~...
   66 	   123 	 0.05145 	 0.04727 	 ~...
   20 	   124 	 0.04557 	 0.04735 	 ~...
    8 	   125 	 0.04320 	 0.04739 	 ~...
  158 	   126 	 0.06946 	 0.04741 	 ~...
  130 	   127 	 0.05793 	 0.04744 	 ~...
   10 	   128 	 0.04322 	 0.04751 	 ~...
   76 	   129 	 0.05183 	 0.04752 	 ~...
  130 	   130 	 0.05793 	 0.04761 	 ~...
  172 	   131 	 0.08335 	 0.04771 	 m..s
  116 	   132 	 0.05603 	 0.04781 	 ~...
   62 	   133 	 0.05139 	 0.04785 	 ~...
  104 	   134 	 0.05534 	 0.04786 	 ~...
   98 	   135 	 0.05414 	 0.04807 	 ~...
   40 	   136 	 0.05007 	 0.04822 	 ~...
   20 	   137 	 0.04557 	 0.04831 	 ~...
   50 	   138 	 0.05101 	 0.04834 	 ~...
   60 	   139 	 0.05125 	 0.04844 	 ~...
   48 	   140 	 0.05094 	 0.04846 	 ~...
  122 	   141 	 0.05680 	 0.04848 	 ~...
  144 	   142 	 0.05940 	 0.04852 	 ~...
  126 	   143 	 0.05711 	 0.04861 	 ~...
  144 	   144 	 0.05940 	 0.04870 	 ~...
   68 	   145 	 0.05146 	 0.04890 	 ~...
    8 	   146 	 0.04320 	 0.04897 	 ~...
   36 	   147 	 0.04860 	 0.04900 	 ~...
  128 	   148 	 0.05740 	 0.04907 	 ~...
  140 	   149 	 0.05891 	 0.04918 	 ~...
   38 	   150 	 0.04973 	 0.04921 	 ~...
   80 	   151 	 0.05277 	 0.04922 	 ~...
  102 	   152 	 0.05436 	 0.04922 	 ~...
  112 	   153 	 0.05574 	 0.04952 	 ~...
  154 	   154 	 0.06502 	 0.04953 	 ~...
   74 	   155 	 0.05164 	 0.04989 	 ~...
  178 	   156 	 0.08791 	 0.05021 	 m..s
   88 	   157 	 0.05300 	 0.05053 	 ~...
  126 	   158 	 0.05711 	 0.05062 	 ~...
  120 	   159 	 0.05636 	 0.05081 	 ~...
  138 	   160 	 0.05884 	 0.05082 	 ~...
  176 	   161 	 0.08663 	 0.05092 	 m..s
   36 	   162 	 0.04860 	 0.05101 	 ~...
  156 	   163 	 0.06847 	 0.05117 	 ~...
  172 	   164 	 0.08335 	 0.05129 	 m..s
  104 	   165 	 0.05534 	 0.05131 	 ~...
   92 	   166 	 0.05323 	 0.05149 	 ~...
   86 	   167 	 0.05288 	 0.05208 	 ~...
   60 	   168 	 0.05125 	 0.05215 	 ~...
  122 	   169 	 0.05680 	 0.05235 	 ~...
  170 	   170 	 0.07923 	 0.05242 	 ~...
  128 	   171 	 0.05740 	 0.05250 	 ~...
  150 	   172 	 0.06443 	 0.05263 	 ~...
   34 	   173 	 0.04847 	 0.05269 	 ~...
   84 	   174 	 0.05287 	 0.05442 	 ~...
  132 	   175 	 0.05812 	 0.05463 	 ~...
  148 	   176 	 0.06156 	 0.05498 	 ~...
   90 	   177 	 0.05315 	 0.05524 	 ~...
   82 	   178 	 0.05284 	 0.05593 	 ~...
   44 	   179 	 0.05059 	 0.05654 	 ~...
   34 	   180 	 0.04847 	 0.05729 	 ~...
   52 	   181 	 0.05101 	 0.05837 	 ~...
   84 	   182 	 0.05287 	 0.05954 	 ~...
   82 	   183 	 0.05284 	 0.06132 	 ~...
  192 	   184 	 0.19516 	 0.17062 	 ~...
  192 	   185 	 0.19516 	 0.17262 	 ~...
  184 	   186 	 0.18063 	 0.17571 	 ~...
  204 	   187 	 0.21406 	 0.18076 	 m..s
  196 	   188 	 0.20561 	 0.18625 	 ~...
  190 	   189 	 0.19035 	 0.18840 	 ~...
  188 	   190 	 0.18473 	 0.18914 	 ~...
  190 	   191 	 0.19035 	 0.19146 	 ~...
  206 	   192 	 0.21415 	 0.19455 	 ~...
  186 	   193 	 0.18064 	 0.19777 	 ~...
  212 	   194 	 0.21983 	 0.19986 	 ~...
  198 	   195 	 0.20570 	 0.20295 	 ~...
  210 	   196 	 0.21838 	 0.20331 	 ~...
  196 	   197 	 0.20561 	 0.20348 	 ~...
  186 	   198 	 0.18064 	 0.20350 	 ~...
  184 	   199 	 0.18063 	 0.20542 	 ~...
  188 	   200 	 0.18473 	 0.20603 	 ~...
  208 	   201 	 0.21733 	 0.20836 	 ~...
  206 	   202 	 0.21415 	 0.20911 	 ~...
  208 	   203 	 0.21733 	 0.20990 	 ~...
  202 	   204 	 0.21340 	 0.21188 	 ~...
  204 	   205 	 0.21406 	 0.21244 	 ~...
  212 	   206 	 0.21983 	 0.21328 	 ~...
  210 	   207 	 0.21838 	 0.21723 	 ~...
  202 	   208 	 0.21340 	 0.21827 	 ~...
  224 	   209 	 0.23269 	 0.21849 	 ~...
  194 	   210 	 0.19951 	 0.21862 	 ~...
  214 	   211 	 0.22193 	 0.22048 	 ~...
  194 	   212 	 0.19951 	 0.22095 	 ~...
  198 	   213 	 0.20570 	 0.22156 	 ~...
  218 	   214 	 0.22479 	 0.22647 	 ~...
  224 	   215 	 0.23269 	 0.23267 	 ~...
  218 	   216 	 0.22479 	 0.24273 	 ~...
  214 	   217 	 0.22193 	 0.24291 	 ~...
  220 	   218 	 0.22641 	 0.24656 	 ~...
  226 	   219 	 0.24178 	 0.24879 	 ~...
  222 	   220 	 0.22903 	 0.24955 	 ~...
  234 	   221 	 0.28302 	 0.25316 	 ~...
  222 	   222 	 0.22903 	 0.25558 	 ~...
  230 	   223 	 0.26506 	 0.25916 	 ~...
  220 	   224 	 0.22641 	 0.26048 	 m..s
  226 	   225 	 0.24178 	 0.26317 	 ~...
  230 	   226 	 0.26506 	 0.26351 	 ~...
  234 	   227 	 0.28302 	 0.26665 	 ~...
  216 	   228 	 0.22416 	 0.28029 	 m..s
  228 	   229 	 0.25881 	 0.28030 	 ~...
  238 	   230 	 0.34345 	 0.28031 	 m..s
  200 	   231 	 0.21336 	 0.28743 	 m..s
  240 	   232 	 0.34710 	 0.28750 	 m..s
  232 	   233 	 0.26951 	 0.29174 	 ~...
  238 	   234 	 0.34345 	 0.29247 	 m..s
  240 	   235 	 0.34710 	 0.29468 	 m..s
  200 	   236 	 0.21336 	 0.29483 	 m..s
  236 	   237 	 0.31856 	 0.30223 	 ~...
  232 	   238 	 0.26951 	 0.30397 	 m..s
  236 	   239 	 0.31856 	 0.30851 	 ~...
  228 	   240 	 0.25881 	 0.31636 	 m..s
  216 	   241 	 0.22416 	 0.33485 	 MISS
==========================================
r_mrr = 0.9725514650344849
r2_mrr = 0.9309878349304199
spearmanr_mrr@5 = 0.5655752420425415
spearmanr_mrr@10 = 0.8042119741439819
spearmanr_mrr@50 = 0.9185845255851746
spearmanr_mrr@100 = 0.9874159693717957
spearmanr_mrr@All = 0.9922237396240234
==========================================
test time: 1.088
Done Testing dataset UMLS
total time taken: 213.38033199310303
training time taken: 198.76688146591187
TWIG out ;))
