=====================================================
-----------------------------------------------------
Running a TWIG experiment with tag: ComplEx-omit-UMLS
-----------------------------------------------------
=====================================================
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_ComplEx-omit-UMLS
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 148
rank avg (pred): 0.420 +- 0.004
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001388674

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1134
rank avg (pred): 0.332 +- 0.163
mrr vals (pred, true): 0.000, 0.109
batch losses (mrrl, rdl): 0.0, 6.2159e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 184
rank avg (pred): 0.488 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001030922

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 14
rank avg (pred): 0.255 +- 0.177
mrr vals (pred, true): 0.000, 0.292
batch losses (mrrl, rdl): 0.0, 0.0001168383

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.490 +- 0.018
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001435392

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 575
rank avg (pred): 0.496 +- 0.012
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001890009

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 91
rank avg (pred): 0.500 +- 0.020
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001010617

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 767
rank avg (pred): 0.506 +- 0.025
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001140617

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1198
rank avg (pred): 0.464 +- 0.123
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001086854

Epoch over!
epoch time: 50.644

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 450
rank avg (pred): 0.451 +- 0.231
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 5.34957e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 103
rank avg (pred): 0.503 +- 0.228
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 5.26259e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 555
rank avg (pred): 0.398 +- 0.291
mrr vals (pred, true): 0.022, 0.090
batch losses (mrrl, rdl): 0.0, 5.71614e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 875
rank avg (pred): 0.469 +- 0.286
mrr vals (pred, true): 0.004, 0.000
batch losses (mrrl, rdl): 0.0, 5.00783e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 100
rank avg (pred): 0.464 +- 0.307
mrr vals (pred, true): 0.005, 0.000
batch losses (mrrl, rdl): 0.0, 1.01209e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 671
rank avg (pred): 0.443 +- 0.313
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 1.86957e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 255
rank avg (pred): 0.252 +- 0.298
mrr vals (pred, true): 0.039, 0.298
batch losses (mrrl, rdl): 0.0, 0.0002064802

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 962
rank avg (pred): 0.486 +- 0.297
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.85537e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.489 +- 0.275
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 4.25813e-05

Epoch over!
epoch time: 50.994

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 144
rank avg (pred): 0.481 +- 0.282
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.37406e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 566
rank avg (pred): 0.372 +- 0.288
mrr vals (pred, true): 0.002, 0.067
batch losses (mrrl, rdl): 0.0, 5.5025e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1064
rank avg (pred): 0.231 +- 0.299
mrr vals (pred, true): 0.059, 0.118
batch losses (mrrl, rdl): 0.0, 0.0002359142

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 899
rank avg (pred): 0.506 +- 0.300
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.000991981

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 320
rank avg (pred): 0.211 +- 0.263
mrr vals (pred, true): 0.030, 0.080
batch losses (mrrl, rdl): 0.0, 0.0002548059

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1156
rank avg (pred): 0.312 +- 0.305
mrr vals (pred, true): 0.002, 0.139
batch losses (mrrl, rdl): 0.0, 1.41631e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 928
rank avg (pred): 0.445 +- 0.313
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 1.11478e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 884
rank avg (pred): 0.493 +- 0.283
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 4.8421e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 866
rank avg (pred): 0.475 +- 0.286
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.86808e-05

Epoch over!
epoch time: 49.827

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.235 +- 0.294
mrr vals (pred, true): 0.027, 0.320
batch losses (mrrl, rdl): 0.0, 0.0004439809

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 29
rank avg (pred): 0.282 +- 0.314
mrr vals (pred, true): 0.010, 0.064
batch losses (mrrl, rdl): 0.0, 2.65775e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 765
rank avg (pred): 0.496 +- 0.267
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 4.0993e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 328
rank avg (pred): 0.452 +- 0.288
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.3149e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 168
rank avg (pred): 0.479 +- 0.283
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0, 1.03173e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 802
rank avg (pred): 0.491 +- 0.265
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 7.04748e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 931
rank avg (pred): 0.509 +- 0.255
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.34083e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 610
rank avg (pred): 0.481 +- 0.268
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.6569e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 120
rank avg (pred): 0.456 +- 0.289
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.46868e-05

Epoch over!
epoch time: 50.73

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.407 +- 0.281
mrr vals (pred, true): 0.000, 0.082
batch losses (mrrl, rdl): 0.0, 8.1537e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.467 +- 0.290
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 6.6752e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.491 +- 0.266
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 5.14617e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1110
rank avg (pred): 0.446 +- 0.299
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 1.07206e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 689
rank avg (pred): 0.469 +- 0.288
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.3517e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 702
rank avg (pred): 0.487 +- 0.269
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.7314e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 387
rank avg (pred): 0.448 +- 0.294
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 3.78917e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 521
rank avg (pred): 0.314 +- 0.280
mrr vals (pred, true): 0.001, 0.104
batch losses (mrrl, rdl): 0.0, 4.58054e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 511
rank avg (pred): 0.260 +- 0.232
mrr vals (pred, true): 0.001, 0.153
batch losses (mrrl, rdl): 0.0, 2.90503e-05

Epoch over!
epoch time: 49.583

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 388
rank avg (pred): 0.473 +- 0.283
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0248290952, 6.1102e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 908
rank avg (pred): 0.283 +- 0.260
mrr vals (pred, true): 0.010, 0.001
batch losses (mrrl, rdl): 0.016004324, 0.0021434135

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.301 +- 0.289
mrr vals (pred, true): 0.046, 0.129
batch losses (mrrl, rdl): 0.0689288005, 8.7947e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 291
rank avg (pred): 0.287 +- 0.285
mrr vals (pred, true): 0.068, 0.133
batch losses (mrrl, rdl): 0.0420216322, 3.37328e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1109
rank avg (pred): 0.376 +- 0.294
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0031364292, 0.0001103059

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.413 +- 0.300
mrr vals (pred, true): 0.037, 0.000
batch losses (mrrl, rdl): 0.0015923991, 0.0001463264

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.389 +- 0.284
mrr vals (pred, true): 0.071, 0.001
batch losses (mrrl, rdl): 0.0042389794, 2.17967e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1037
rank avg (pred): 0.414 +- 0.278
mrr vals (pred, true): 0.051, 0.002
batch losses (mrrl, rdl): 2.0057e-05, 2.11778e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 263
rank avg (pred): 0.257 +- 0.268
mrr vals (pred, true): 0.195, 0.369
batch losses (mrrl, rdl): 0.3012811542, 0.0003617821

Epoch over!
epoch time: 51.268

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 109
rank avg (pred): 0.418 +- 0.277
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.0014571863, 4.58394e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 652
rank avg (pred): 0.422 +- 0.272
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0010772467, 2.79007e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1101
rank avg (pred): 0.400 +- 0.271
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0005856091, 0.0001065634

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.414 +- 0.265
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.4298e-05, 5.94335e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1199
rank avg (pred): 0.419 +- 0.260
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.00229e-05, 0.0001838175

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 28
rank avg (pred): 0.312 +- 0.259
mrr vals (pred, true): 0.102, 0.078
batch losses (mrrl, rdl): 0.0266452618, 2.00657e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 353
rank avg (pred): 0.407 +- 0.253
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0008600318, 6.1477e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 149
rank avg (pred): 0.407 +- 0.251
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002375027, 5.36621e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 664
rank avg (pred): 0.405 +- 0.250
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003349114, 0.0002349133

Epoch over!
epoch time: 51.753

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.407 +- 0.247
mrr vals (pred, true): 0.030, 0.001
batch losses (mrrl, rdl): 0.003867513, 7.50313e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 286
rank avg (pred): 0.318 +- 0.279
mrr vals (pred, true): 0.092, 0.124
batch losses (mrrl, rdl): 0.009942038, 1.41581e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 765
rank avg (pred): 0.465 +- 0.300
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001173559, 7.0885e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 468
rank avg (pred): 0.444 +- 0.312
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0012370334, 2.34062e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1043
rank avg (pred): 0.513 +- 0.324
mrr vals (pred, true): 0.015, 0.000
batch losses (mrrl, rdl): 0.0123874806, 8.08664e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.487 +- 0.315
mrr vals (pred, true): 0.034, 0.000
batch losses (mrrl, rdl): 0.0026970974, 1.21661e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 78
rank avg (pred): 0.318 +- 0.325
mrr vals (pred, true): 0.101, 0.071
batch losses (mrrl, rdl): 0.0255345087, 4.6628e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 140
rank avg (pred): 0.488 +- 0.334
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 6.75552e-05, 5.36762e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 120
rank avg (pred): 0.514 +- 0.342
mrr vals (pred, true): 0.027, 0.000
batch losses (mrrl, rdl): 0.0051024905, 2.76024e-05

Epoch over!
epoch time: 50.514

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.513 +- 0.341
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.89374e-05, 7.00826e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 40
rank avg (pred): 0.314 +- 0.305
mrr vals (pred, true): 0.077, 0.084
batch losses (mrrl, rdl): 0.0071499031, 2.15709e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 784
rank avg (pred): 0.453 +- 0.307
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.000985787, 0.000151826

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1138
rank avg (pred): 0.427 +- 0.374
mrr vals (pred, true): 0.088, 0.138
batch losses (mrrl, rdl): 0.0249489658, 7.9873e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 363
rank avg (pred): 0.497 +- 0.339
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.477e-07, 6.62804e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 50
rank avg (pred): 0.270 +- 0.294
mrr vals (pred, true): 0.142, 0.091
batch losses (mrrl, rdl): 0.0847267061, 2.08358e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.463 +- 0.326
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003061477, 2.44026e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 862
rank avg (pred): 0.491 +- 0.333
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0008515108, 3.06169e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.253 +- 0.233
mrr vals (pred, true): 0.149, 0.092
batch losses (mrrl, rdl): 0.0974460244, 6.98584e-05

Epoch over!
epoch time: 50.698

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 909
rank avg (pred): 0.357 +- 0.325
mrr vals (pred, true): 0.104, 0.008
batch losses (mrrl, rdl): 0.0289131496, 0.0003676494

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 377
rank avg (pred): 0.508 +- 0.352
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0017002942, 3.25253e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1056
rank avg (pred): 0.232 +- 0.214
mrr vals (pred, true): 0.219, 0.167
batch losses (mrrl, rdl): 0.0269737449, 9.48489e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 408
rank avg (pred): 0.517 +- 0.357
mrr vals (pred, true): 0.021, 0.000
batch losses (mrrl, rdl): 0.0081699518, 3.3435e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 714
rank avg (pred): 0.451 +- 0.337
mrr vals (pred, true): 0.109, 0.001
batch losses (mrrl, rdl): 0.0350757465, 9.02675e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 132
rank avg (pred): 0.477 +- 0.349
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.13522e-05, 1.47494e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 212
rank avg (pred): 0.485 +- 0.354
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.76956e-05, 7.82676e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1159
rank avg (pred): 0.304 +- 0.305
mrr vals (pred, true): 0.127, 0.114
batch losses (mrrl, rdl): 0.0015437243, 2.3401e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 328
rank avg (pred): 0.450 +- 0.354
mrr vals (pred, true): 0.084, 0.000
batch losses (mrrl, rdl): 0.0114679076, 1.95878e-05

Epoch over!
epoch time: 50.378

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.508 +- 0.370
mrr vals (pred, true): 0.031, 0.000
batch losses (mrrl, rdl): 0.0035123851, 3.78472e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 327
rank avg (pred): 0.509 +- 0.367
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0008468489, 1.94907e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 500
rank avg (pred): 0.305 +- 0.314
mrr vals (pred, true): 0.094, 0.149
batch losses (mrrl, rdl): 0.0292137824, 1.16249e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 592
rank avg (pred): 0.510 +- 0.367
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009062695, 2.88149e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 159
rank avg (pred): 0.478 +- 0.361
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.02534e-05, 4.80227e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 80
rank avg (pred): 0.284 +- 0.296
mrr vals (pred, true): 0.104, 0.083
batch losses (mrrl, rdl): 0.0295997523, 1.70725e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 237
rank avg (pred): 0.433 +- 0.339
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004595787, 4.35653e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 234
rank avg (pred): 0.484 +- 0.377
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0020433171, 0.0001544257

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 658
rank avg (pred): 0.476 +- 0.381
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003074514, 6.12547e-05

Epoch over!
epoch time: 49.581

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.234 +- 0.268
mrr vals (pred, true): 0.206, 0.320
batch losses (mrrl, rdl): 0.1298471391, 0.0003554134

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 57
rank avg (pred): 0.259 +- 0.290
mrr vals (pred, true): 0.102, 0.074
batch losses (mrrl, rdl): 0.0272560231, 0.0001630925

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 141
rank avg (pred): 0.479 +- 0.384
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004351276, 5.93752e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 108
rank avg (pred): 0.515 +- 0.398
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.000156743, 0.0001075648

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 727
rank avg (pred): 0.485 +- 0.379
mrr vals (pred, true): 0.072, 0.000
batch losses (mrrl, rdl): 0.004685685, 8.103e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1158
rank avg (pred): 0.333 +- 0.387
mrr vals (pred, true): 0.109, 0.131
batch losses (mrrl, rdl): 0.0051427521, 5.61732e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.365 +- 0.408
mrr vals (pred, true): 0.067, 0.068
batch losses (mrrl, rdl): 0.0030094958, 0.0001179792

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 18
rank avg (pred): 0.274 +- 0.344
mrr vals (pred, true): 0.177, 0.318
batch losses (mrrl, rdl): 0.1963879317, 0.0003264413

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 691
rank avg (pred): 0.439 +- 0.382
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0086413976, 0.0001372874

Epoch over!
epoch time: 50.523

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 739
rank avg (pred): 0.271 +- 0.338
mrr vals (pred, true): 0.181, 0.152
batch losses (mrrl, rdl): 0.0085560679, 5.2882e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1078
rank avg (pred): 0.252 +- 0.315
mrr vals (pred, true): 0.161, 0.137
batch losses (mrrl, rdl): 0.0058957613, 5.58578e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 953
rank avg (pred): 0.523 +- 0.403
mrr vals (pred, true): 0.064, 0.001
batch losses (mrrl, rdl): 0.0019999098, 9.59235e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 837
rank avg (pred): 0.449 +- 0.376
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 9.70289e-05, 7.39142e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 675
rank avg (pred): 0.464 +- 0.389
mrr vals (pred, true): 0.024, 0.000
batch losses (mrrl, rdl): 0.0065627457, 0.0002080917

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 788
rank avg (pred): 0.524 +- 0.394
mrr vals (pred, true): 0.023, 0.000
batch losses (mrrl, rdl): 0.0071332273, 8.21119e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.528 +- 0.410
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0005165605, 7.5979e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 4
rank avg (pred): 0.272 +- 0.311
mrr vals (pred, true): 0.177, 0.108
batch losses (mrrl, rdl): 0.0472851768, 1.9267e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 577
rank avg (pred): 0.438 +- 0.390
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0030260822, 0.0001142981

Epoch over!
epoch time: 51.285

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 348
rank avg (pred): 0.495 +- 0.409
mrr vals (pred, true): 0.042, 0.000
batch losses (mrrl, rdl): 0.000640541, 0.0001173489

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 475
rank avg (pred): 0.485 +- 0.407
mrr vals (pred, true): 0.071, 0.000
batch losses (mrrl, rdl): 0.0044658203, 0.000102262

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.277 +- 0.352
mrr vals (pred, true): 0.101, 0.093
batch losses (mrrl, rdl): 0.0262734145, 9.80494e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.460 +- 0.397
mrr vals (pred, true): 0.067, 0.001
batch losses (mrrl, rdl): 0.0028988733, 6.36545e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 94
rank avg (pred): 0.460 +- 0.400
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0008211423, 7.48088e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 447
rank avg (pred): 0.472 +- 0.407
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002859203, 7.40096e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.237 +- 0.314
mrr vals (pred, true): 0.162, 0.113
batch losses (mrrl, rdl): 0.0245398115, 0.0001463262

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.483 +- 0.410
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.16542e-05, 7.55809e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 228
rank avg (pred): 0.473 +- 0.402
mrr vals (pred, true): 0.042, 0.000
batch losses (mrrl, rdl): 0.0006617523, 0.0001457128

Epoch over!
epoch time: 51.504

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 268
rank avg (pred): 0.204 +- 0.287
mrr vals (pred, true): 0.212, 0.350
batch losses (mrrl, rdl): 0.1908542365, 6.94399e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.500 +- 0.404
mrr vals (pred, true): 0.064, 0.001
batch losses (mrrl, rdl): 0.0018417764, 7.02009e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 400
rank avg (pred): 0.478 +- 0.414
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0035264967, 0.0001409635

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 522
rank avg (pred): 0.340 +- 0.412
mrr vals (pred, true): 0.041, 0.093
batch losses (mrrl, rdl): 0.000748942, 0.0001775978

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 191
rank avg (pred): 0.463 +- 0.400
mrr vals (pred, true): 0.030, 0.000
batch losses (mrrl, rdl): 0.0040184092, 0.0001313056

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 567
rank avg (pred): 0.554 +- 0.436
mrr vals (pred, true): 0.036, 0.000
batch losses (mrrl, rdl): 0.0018478027, 0.0001878058

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1207
rank avg (pred): 0.547 +- 0.441
mrr vals (pred, true): 0.036, 0.000
batch losses (mrrl, rdl): 0.0019228093, 0.0001830215

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.221 +- 0.301
mrr vals (pred, true): 0.209, 0.188
batch losses (mrrl, rdl): 0.0043200646, 0.0001288337

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 322
rank avg (pred): 0.217 +- 0.292
mrr vals (pred, true): 0.097, 0.110
batch losses (mrrl, rdl): 0.001855179, 0.0002387288

Epoch over!
epoch time: 50.636

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.220 +- 0.298
mrr vals (pred, true): 0.145, 0.156

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   56 	     0 	 0.06036 	 0.00014 	 m..s
   35 	     1 	 0.05860 	 0.00016 	 m..s
    0 	     2 	 0.05054 	 0.00016 	 m..s
   36 	     3 	 0.05868 	 0.00017 	 m..s
   64 	     4 	 0.06061 	 0.00017 	 m..s
   12 	     5 	 0.05477 	 0.00017 	 m..s
   37 	     6 	 0.05889 	 0.00018 	 m..s
   18 	     7 	 0.05679 	 0.00018 	 m..s
    9 	     8 	 0.05436 	 0.00018 	 m..s
   13 	     9 	 0.05550 	 0.00018 	 m..s
   32 	    10 	 0.05852 	 0.00018 	 m..s
   15 	    11 	 0.05651 	 0.00018 	 m..s
   16 	    12 	 0.05660 	 0.00018 	 m..s
   48 	    13 	 0.05953 	 0.00019 	 m..s
   44 	    14 	 0.05912 	 0.00019 	 m..s
   58 	    15 	 0.06045 	 0.00019 	 m..s
   20 	    16 	 0.05737 	 0.00019 	 m..s
   10 	    17 	 0.05437 	 0.00020 	 m..s
   63 	    18 	 0.06060 	 0.00020 	 m..s
    4 	    19 	 0.05167 	 0.00020 	 m..s
    8 	    20 	 0.05425 	 0.00021 	 m..s
   39 	    21 	 0.05899 	 0.00021 	 m..s
   71 	    22 	 0.06148 	 0.00021 	 m..s
   34 	    23 	 0.05857 	 0.00021 	 m..s
   68 	    24 	 0.06102 	 0.00022 	 m..s
   65 	    25 	 0.06078 	 0.00022 	 m..s
   42 	    26 	 0.05903 	 0.00023 	 m..s
   47 	    27 	 0.05924 	 0.00023 	 m..s
   79 	    28 	 0.06671 	 0.00023 	 m..s
   75 	    29 	 0.06164 	 0.00024 	 m..s
   53 	    30 	 0.06029 	 0.00024 	 m..s
   43 	    31 	 0.05911 	 0.00024 	 m..s
   21 	    32 	 0.05742 	 0.00025 	 m..s
   72 	    33 	 0.06149 	 0.00025 	 m..s
   51 	    34 	 0.05971 	 0.00025 	 m..s
    2 	    35 	 0.05079 	 0.00026 	 m..s
   27 	    36 	 0.05843 	 0.00026 	 m..s
   40 	    37 	 0.05899 	 0.00026 	 m..s
   29 	    38 	 0.05844 	 0.00027 	 m..s
   24 	    39 	 0.05790 	 0.00027 	 m..s
   41 	    40 	 0.05901 	 0.00027 	 m..s
   73 	    41 	 0.06149 	 0.00027 	 m..s
   45 	    42 	 0.05913 	 0.00027 	 m..s
   69 	    43 	 0.06106 	 0.00028 	 m..s
   14 	    44 	 0.05630 	 0.00028 	 m..s
   66 	    45 	 0.06091 	 0.00028 	 m..s
   54 	    46 	 0.06030 	 0.00028 	 m..s
   76 	    47 	 0.06164 	 0.00028 	 m..s
   67 	    48 	 0.06095 	 0.00029 	 m..s
   77 	    49 	 0.06164 	 0.00029 	 m..s
   60 	    50 	 0.06048 	 0.00029 	 m..s
   11 	    51 	 0.05447 	 0.00030 	 m..s
   61 	    52 	 0.06052 	 0.00030 	 m..s
   19 	    53 	 0.05728 	 0.00030 	 m..s
   62 	    54 	 0.06057 	 0.00030 	 m..s
    5 	    55 	 0.05321 	 0.00031 	 m..s
    7 	    56 	 0.05413 	 0.00032 	 m..s
   38 	    57 	 0.05896 	 0.00032 	 m..s
   30 	    58 	 0.05844 	 0.00033 	 m..s
   26 	    59 	 0.05842 	 0.00033 	 m..s
   59 	    60 	 0.06046 	 0.00035 	 m..s
   57 	    61 	 0.06040 	 0.00038 	 m..s
   33 	    62 	 0.05853 	 0.00038 	 m..s
   25 	    63 	 0.05813 	 0.00038 	 m..s
   74 	    64 	 0.06152 	 0.00039 	 m..s
   70 	    65 	 0.06117 	 0.00041 	 m..s
   17 	    66 	 0.05677 	 0.00041 	 m..s
    1 	    67 	 0.05070 	 0.00044 	 m..s
   23 	    68 	 0.05775 	 0.00049 	 m..s
    6 	    69 	 0.05400 	 0.00053 	 m..s
   31 	    70 	 0.05852 	 0.00056 	 m..s
   55 	    71 	 0.06036 	 0.00059 	 m..s
   52 	    72 	 0.06029 	 0.00064 	 m..s
   50 	    73 	 0.05958 	 0.00066 	 m..s
   28 	    74 	 0.05844 	 0.00086 	 m..s
   46 	    75 	 0.05919 	 0.00091 	 m..s
   22 	    76 	 0.05760 	 0.00131 	 m..s
   49 	    77 	 0.05955 	 0.00160 	 m..s
   78 	    78 	 0.06164 	 0.00180 	 m..s
    3 	    79 	 0.05161 	 0.00304 	 m..s
   84 	    80 	 0.08523 	 0.05043 	 m..s
  101 	    81 	 0.12235 	 0.06844 	 m..s
   96 	    82 	 0.11027 	 0.06966 	 m..s
   86 	    83 	 0.08866 	 0.07097 	 ~...
  107 	    84 	 0.12840 	 0.07846 	 m..s
   89 	    85 	 0.10111 	 0.07934 	 ~...
   80 	    86 	 0.07718 	 0.08498 	 ~...
   82 	    87 	 0.08064 	 0.08844 	 ~...
   92 	    88 	 0.10472 	 0.08867 	 ~...
   81 	    89 	 0.07989 	 0.08873 	 ~...
   95 	    90 	 0.10773 	 0.09149 	 ~...
   93 	    91 	 0.10505 	 0.09373 	 ~...
  106 	    92 	 0.12726 	 0.09410 	 m..s
   83 	    93 	 0.08324 	 0.09666 	 ~...
  112 	    94 	 0.17879 	 0.10107 	 m..s
   98 	    95 	 0.11426 	 0.10764 	 ~...
  102 	    96 	 0.12315 	 0.11025 	 ~...
   88 	    97 	 0.09852 	 0.11100 	 ~...
  104 	    98 	 0.12596 	 0.11583 	 ~...
  103 	    99 	 0.12517 	 0.11842 	 ~...
   85 	   100 	 0.08536 	 0.11956 	 m..s
  109 	   101 	 0.14735 	 0.12240 	 ~...
  115 	   102 	 0.20517 	 0.12398 	 m..s
   87 	   103 	 0.09718 	 0.12498 	 ~...
   90 	   104 	 0.10407 	 0.12588 	 ~...
   99 	   105 	 0.11614 	 0.12754 	 ~...
  105 	   106 	 0.12645 	 0.12884 	 ~...
  100 	   107 	 0.12180 	 0.13146 	 ~...
   91 	   108 	 0.10430 	 0.13164 	 ~...
  116 	   109 	 0.21296 	 0.14319 	 m..s
  114 	   110 	 0.19101 	 0.14933 	 m..s
   94 	   111 	 0.10635 	 0.14954 	 m..s
  108 	   112 	 0.14487 	 0.15553 	 ~...
   97 	   113 	 0.11368 	 0.19855 	 m..s
  118 	   114 	 0.22389 	 0.20189 	 ~...
  113 	   115 	 0.18723 	 0.21574 	 ~...
  117 	   116 	 0.21459 	 0.25348 	 m..s
  110 	   117 	 0.16262 	 0.25855 	 m..s
  111 	   118 	 0.17010 	 0.28448 	 MISS
  119 	   119 	 0.23338 	 0.31455 	 m..s
  120 	   120 	 0.23808 	 0.34767 	 MISS
==========================================
r_mrr = 0.9125961661338806
r2_mrr = 0.48663604259490967
spearmanr_mrr@5 = 0.9862232208251953
spearmanr_mrr@10 = 0.9843027591705322
spearmanr_mrr@50 = 0.9719425439834595
spearmanr_mrr@100 = 0.982950747013092
spearmanr_mrr@All = 0.9839180707931519
==========================================
test time: 0.615
Done Testing dataset DBpedia50
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.139 +- 0.222
mrr vals (pred, true): 0.162, 0.316

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   72 	     0 	 0.05325 	 0.00101 	 m..s
   77 	     1 	 0.05328 	 0.00194 	 m..s
   76 	     2 	 0.05327 	 0.00212 	 m..s
    4 	     3 	 0.04845 	 0.00246 	 m..s
   23 	     4 	 0.05158 	 0.00250 	 m..s
   55 	     5 	 0.05280 	 0.00253 	 m..s
    0 	     6 	 0.04769 	 0.00263 	 m..s
    1 	     7 	 0.04773 	 0.00266 	 m..s
   67 	     8 	 0.05306 	 0.00282 	 m..s
   59 	     9 	 0.05287 	 0.00289 	 m..s
   19 	    10 	 0.05137 	 0.00296 	 m..s
   17 	    11 	 0.05116 	 0.00297 	 m..s
   68 	    12 	 0.05307 	 0.00297 	 m..s
   64 	    13 	 0.05302 	 0.00300 	 m..s
   53 	    14 	 0.05277 	 0.00318 	 m..s
   58 	    15 	 0.05286 	 0.00321 	 m..s
   39 	    16 	 0.05227 	 0.00331 	 m..s
    7 	    17 	 0.04979 	 0.00334 	 m..s
   33 	    18 	 0.05216 	 0.00334 	 m..s
    2 	    19 	 0.04797 	 0.00334 	 m..s
    5 	    20 	 0.04927 	 0.00336 	 m..s
    3 	    21 	 0.04818 	 0.00343 	 m..s
   16 	    22 	 0.05114 	 0.00344 	 m..s
   57 	    23 	 0.05285 	 0.00345 	 m..s
    6 	    24 	 0.04964 	 0.00347 	 m..s
   65 	    25 	 0.05303 	 0.00347 	 m..s
   37 	    26 	 0.05225 	 0.00350 	 m..s
   18 	    27 	 0.05132 	 0.00358 	 m..s
   51 	    28 	 0.05273 	 0.00361 	 m..s
   38 	    29 	 0.05227 	 0.00363 	 m..s
   73 	    30 	 0.05325 	 0.00366 	 m..s
   27 	    31 	 0.05212 	 0.00369 	 m..s
   31 	    32 	 0.05215 	 0.00371 	 m..s
   25 	    33 	 0.05191 	 0.00376 	 m..s
    8 	    34 	 0.04984 	 0.00377 	 m..s
   74 	    35 	 0.05325 	 0.00377 	 m..s
   78 	    36 	 0.05338 	 0.00380 	 m..s
   52 	    37 	 0.05277 	 0.00380 	 m..s
   75 	    38 	 0.05327 	 0.00381 	 m..s
   63 	    39 	 0.05295 	 0.00381 	 m..s
   71 	    40 	 0.05325 	 0.00384 	 m..s
   40 	    41 	 0.05228 	 0.00385 	 m..s
   48 	    42 	 0.05245 	 0.00387 	 m..s
   36 	    43 	 0.05218 	 0.00387 	 m..s
   14 	    44 	 0.05099 	 0.00387 	 m..s
   60 	    45 	 0.05290 	 0.00389 	 m..s
   35 	    46 	 0.05217 	 0.00391 	 m..s
   56 	    47 	 0.05280 	 0.00398 	 m..s
   50 	    48 	 0.05268 	 0.00399 	 m..s
   12 	    49 	 0.05027 	 0.00401 	 m..s
   34 	    50 	 0.05217 	 0.00404 	 m..s
   22 	    51 	 0.05153 	 0.00413 	 m..s
   42 	    52 	 0.05231 	 0.00414 	 m..s
   79 	    53 	 0.05651 	 0.00415 	 m..s
   11 	    54 	 0.05005 	 0.00415 	 m..s
   46 	    55 	 0.05244 	 0.00418 	 m..s
   69 	    56 	 0.05313 	 0.00428 	 m..s
   66 	    57 	 0.05305 	 0.00433 	 m..s
   20 	    58 	 0.05139 	 0.00434 	 m..s
   43 	    59 	 0.05231 	 0.00438 	 m..s
   21 	    60 	 0.05152 	 0.00439 	 m..s
   13 	    61 	 0.05053 	 0.00441 	 m..s
   47 	    62 	 0.05244 	 0.00442 	 m..s
   32 	    63 	 0.05216 	 0.00445 	 m..s
   10 	    64 	 0.05001 	 0.00448 	 m..s
   49 	    65 	 0.05250 	 0.00450 	 m..s
   28 	    66 	 0.05215 	 0.00453 	 m..s
   45 	    67 	 0.05233 	 0.00455 	 m..s
   54 	    68 	 0.05279 	 0.00458 	 m..s
   62 	    69 	 0.05294 	 0.00464 	 m..s
   30 	    70 	 0.05215 	 0.00466 	 m..s
    9 	    71 	 0.04990 	 0.00469 	 m..s
   15 	    72 	 0.05100 	 0.00472 	 m..s
   26 	    73 	 0.05211 	 0.00484 	 m..s
   44 	    74 	 0.05231 	 0.00486 	 m..s
   70 	    75 	 0.05320 	 0.00493 	 m..s
   61 	    76 	 0.05292 	 0.00515 	 m..s
   24 	    77 	 0.05163 	 0.00533 	 m..s
   41 	    78 	 0.05229 	 0.00545 	 m..s
   29 	    79 	 0.05215 	 0.00579 	 m..s
   86 	    80 	 0.07681 	 0.00601 	 m..s
   84 	    81 	 0.07604 	 0.00656 	 m..s
   81 	    82 	 0.07208 	 0.00729 	 m..s
   85 	    83 	 0.07643 	 0.00850 	 m..s
   82 	    84 	 0.07255 	 0.00858 	 m..s
   83 	    85 	 0.07310 	 0.01178 	 m..s
   80 	    86 	 0.07040 	 0.01206 	 m..s
   95 	    87 	 0.09583 	 0.01781 	 m..s
   88 	    88 	 0.08448 	 0.01930 	 m..s
   94 	    89 	 0.09265 	 0.01957 	 m..s
   91 	    90 	 0.08802 	 0.02050 	 m..s
   93 	    91 	 0.09161 	 0.02182 	 m..s
   92 	    92 	 0.08868 	 0.02455 	 m..s
   90 	    93 	 0.08739 	 0.02920 	 m..s
   89 	    94 	 0.08730 	 0.03130 	 m..s
   87 	    95 	 0.08207 	 0.04296 	 m..s
  111 	    96 	 0.23888 	 0.05291 	 MISS
   97 	    97 	 0.10866 	 0.13821 	 ~...
   96 	    98 	 0.10290 	 0.13846 	 m..s
   99 	    99 	 0.11644 	 0.14041 	 ~...
  105 	   100 	 0.18368 	 0.15098 	 m..s
   98 	   101 	 0.10919 	 0.15237 	 m..s
  104 	   102 	 0.17368 	 0.15473 	 ~...
  100 	   103 	 0.15576 	 0.17117 	 ~...
  102 	   104 	 0.17215 	 0.17383 	 ~...
  106 	   105 	 0.18574 	 0.19630 	 ~...
  107 	   106 	 0.18929 	 0.19794 	 ~...
  108 	   107 	 0.19084 	 0.19906 	 ~...
  112 	   108 	 0.24500 	 0.21732 	 ~...
  110 	   109 	 0.23100 	 0.23383 	 ~...
  109 	   110 	 0.21546 	 0.23552 	 ~...
  113 	   111 	 0.25825 	 0.23886 	 ~...
  117 	   112 	 0.32475 	 0.24879 	 m..s
  103 	   113 	 0.17220 	 0.26233 	 m..s
  118 	   114 	 0.32787 	 0.26513 	 m..s
  119 	   115 	 0.33860 	 0.27244 	 m..s
  114 	   116 	 0.27413 	 0.29612 	 ~...
  115 	   117 	 0.29787 	 0.29973 	 ~...
  116 	   118 	 0.30831 	 0.30213 	 ~...
  120 	   119 	 0.33915 	 0.31468 	 ~...
  101 	   120 	 0.16169 	 0.31567 	 MISS
==========================================
r_mrr = 0.9316728711128235
r2_mrr = 0.6503698229789734
spearmanr_mrr@5 = 0.9201987385749817
spearmanr_mrr@10 = 0.9820659160614014
spearmanr_mrr@50 = 0.971605122089386
spearmanr_mrr@100 = 0.9809955954551697
spearmanr_mrr@All = 0.9820252656936646
==========================================
test time: 0.409
Done Testing dataset CoDExSmall
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.117 +- 0.111
mrr vals (pred, true): 0.339, 0.511

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   24 	     0 	 0.04916 	 0.04547 	 ~...
   72 	     1 	 0.05306 	 0.04627 	 ~...
   37 	     2 	 0.04977 	 0.04721 	 ~...
   41 	     3 	 0.04992 	 0.04731 	 ~...
   60 	     4 	 0.05126 	 0.04931 	 ~...
    0 	     5 	 0.04197 	 0.04971 	 ~...
   32 	     6 	 0.04940 	 0.04973 	 ~...
   76 	     7 	 0.05596 	 0.05014 	 ~...
   42 	     8 	 0.04995 	 0.05028 	 ~...
   68 	     9 	 0.05171 	 0.05035 	 ~...
   43 	    10 	 0.04998 	 0.05046 	 ~...
   65 	    11 	 0.05128 	 0.05050 	 ~...
   47 	    12 	 0.05020 	 0.05062 	 ~...
   79 	    13 	 0.06008 	 0.05075 	 ~...
   57 	    14 	 0.05117 	 0.05096 	 ~...
   56 	    15 	 0.05117 	 0.05108 	 ~...
   54 	    16 	 0.05113 	 0.05109 	 ~...
   16 	    17 	 0.04800 	 0.05114 	 ~...
   14 	    18 	 0.04763 	 0.05116 	 ~...
   62 	    19 	 0.05127 	 0.05117 	 ~...
    8 	    20 	 0.04586 	 0.05135 	 ~...
   63 	    21 	 0.05127 	 0.05138 	 ~...
   59 	    22 	 0.05126 	 0.05138 	 ~...
   40 	    23 	 0.04991 	 0.05147 	 ~...
   13 	    24 	 0.04689 	 0.05154 	 ~...
    9 	    25 	 0.04615 	 0.05188 	 ~...
   29 	    26 	 0.04933 	 0.05194 	 ~...
   23 	    27 	 0.04899 	 0.05201 	 ~...
   11 	    28 	 0.04632 	 0.05216 	 ~...
   75 	    29 	 0.05534 	 0.05224 	 ~...
   33 	    30 	 0.04941 	 0.05225 	 ~...
   53 	    31 	 0.05108 	 0.05258 	 ~...
   21 	    32 	 0.04853 	 0.05275 	 ~...
   34 	    33 	 0.04944 	 0.05277 	 ~...
   74 	    34 	 0.05441 	 0.05279 	 ~...
   51 	    35 	 0.05081 	 0.05310 	 ~...
   18 	    36 	 0.04817 	 0.05313 	 ~...
   77 	    37 	 0.05632 	 0.05314 	 ~...
   22 	    38 	 0.04864 	 0.05316 	 ~...
   78 	    39 	 0.05702 	 0.05317 	 ~...
   20 	    40 	 0.04841 	 0.05320 	 ~...
   67 	    41 	 0.05171 	 0.05346 	 ~...
    3 	    42 	 0.04278 	 0.05350 	 ~...
   55 	    43 	 0.05116 	 0.05357 	 ~...
   44 	    44 	 0.05007 	 0.05358 	 ~...
   69 	    45 	 0.05177 	 0.05375 	 ~...
   38 	    46 	 0.04979 	 0.05412 	 ~...
   52 	    47 	 0.05105 	 0.05428 	 ~...
   50 	    48 	 0.05080 	 0.05429 	 ~...
   26 	    49 	 0.04920 	 0.05451 	 ~...
   58 	    50 	 0.05117 	 0.05455 	 ~...
   30 	    51 	 0.04934 	 0.05456 	 ~...
    4 	    52 	 0.04354 	 0.05464 	 ~...
   66 	    53 	 0.05135 	 0.05496 	 ~...
   19 	    54 	 0.04836 	 0.05498 	 ~...
   73 	    55 	 0.05361 	 0.05501 	 ~...
   71 	    56 	 0.05285 	 0.05528 	 ~...
   61 	    57 	 0.05127 	 0.05545 	 ~...
   35 	    58 	 0.04946 	 0.05566 	 ~...
   25 	    59 	 0.04919 	 0.05595 	 ~...
   70 	    60 	 0.05177 	 0.05614 	 ~...
    5 	    61 	 0.04361 	 0.05656 	 ~...
   28 	    62 	 0.04930 	 0.05664 	 ~...
   64 	    63 	 0.05128 	 0.05668 	 ~...
   46 	    64 	 0.05013 	 0.05685 	 ~...
   27 	    65 	 0.04921 	 0.05723 	 ~...
   12 	    66 	 0.04658 	 0.05733 	 ~...
   48 	    67 	 0.05064 	 0.05781 	 ~...
   31 	    68 	 0.04935 	 0.05814 	 ~...
   39 	    69 	 0.04980 	 0.05817 	 ~...
   10 	    70 	 0.04622 	 0.05817 	 ~...
    1 	    71 	 0.04199 	 0.05846 	 ~...
   17 	    72 	 0.04810 	 0.05875 	 ~...
    2 	    73 	 0.04260 	 0.05881 	 ~...
    6 	    74 	 0.04476 	 0.05888 	 ~...
   15 	    75 	 0.04769 	 0.05937 	 ~...
    7 	    76 	 0.04527 	 0.06012 	 ~...
   45 	    77 	 0.05013 	 0.06014 	 ~...
   36 	    78 	 0.04977 	 0.06053 	 ~...
   49 	    79 	 0.05067 	 0.06387 	 ~...
   87 	    80 	 0.21273 	 0.14588 	 m..s
   81 	    81 	 0.18280 	 0.18561 	 ~...
   86 	    82 	 0.20324 	 0.19332 	 ~...
   88 	    83 	 0.23053 	 0.20462 	 ~...
   82 	    84 	 0.18757 	 0.20491 	 ~...
   84 	    85 	 0.20182 	 0.21194 	 ~...
   85 	    86 	 0.20276 	 0.21207 	 ~...
   80 	    87 	 0.17809 	 0.21874 	 m..s
   83 	    88 	 0.19022 	 0.22043 	 m..s
   89 	    89 	 0.23916 	 0.24979 	 ~...
   99 	    90 	 0.27470 	 0.25657 	 ~...
   93 	    91 	 0.25287 	 0.26205 	 ~...
   98 	    92 	 0.27037 	 0.26361 	 ~...
  100 	    93 	 0.28771 	 0.27033 	 ~...
  105 	    94 	 0.30024 	 0.28083 	 ~...
  110 	    95 	 0.37440 	 0.28272 	 m..s
   95 	    96 	 0.25648 	 0.28388 	 ~...
   91 	    97 	 0.25144 	 0.28579 	 m..s
   97 	    98 	 0.26508 	 0.28999 	 ~...
   92 	    99 	 0.25195 	 0.29652 	 m..s
  103 	   100 	 0.29749 	 0.29748 	 ~...
  104 	   101 	 0.29920 	 0.30084 	 ~...
   94 	   102 	 0.25358 	 0.30840 	 m..s
  111 	   103 	 0.38855 	 0.30945 	 m..s
   90 	   104 	 0.24474 	 0.33811 	 m..s
  117 	   105 	 0.45902 	 0.33985 	 MISS
  112 	   106 	 0.40430 	 0.34292 	 m..s
   96 	   107 	 0.25950 	 0.34296 	 m..s
  114 	   108 	 0.42501 	 0.36022 	 m..s
  107 	   109 	 0.30443 	 0.37521 	 m..s
  102 	   110 	 0.29314 	 0.37760 	 m..s
  101 	   111 	 0.29142 	 0.38637 	 m..s
  109 	   112 	 0.34408 	 0.40509 	 m..s
  113 	   113 	 0.41881 	 0.41075 	 ~...
  116 	   114 	 0.45695 	 0.41730 	 m..s
  106 	   115 	 0.30198 	 0.45262 	 MISS
  115 	   116 	 0.44647 	 0.47935 	 m..s
  108 	   117 	 0.33899 	 0.51145 	 MISS
  118 	   118 	 0.46993 	 0.51473 	 m..s
  119 	   119 	 0.49180 	 0.60870 	 MISS
  120 	   120 	 0.49897 	 0.61786 	 MISS
==========================================
r_mrr = 0.965562641620636
r2_mrr = 0.9262399077415466
spearmanr_mrr@5 = 0.9780714511871338
spearmanr_mrr@10 = 0.9611504077911377
spearmanr_mrr@50 = 0.9877855181694031
spearmanr_mrr@100 = 0.9943815469741821
spearmanr_mrr@All = 0.994891345500946
==========================================
test time: 0.41
Done Testing dataset Kinships
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.242 +- 0.319
mrr vals (pred, true): 0.134, 0.140

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   51 	     0 	 0.04955 	 0.00043 	 m..s
    8 	     1 	 0.04515 	 0.00047 	 m..s
   48 	     2 	 0.04941 	 0.00049 	 m..s
    5 	     3 	 0.04432 	 0.00049 	 m..s
   25 	     4 	 0.04827 	 0.00049 	 m..s
   36 	     5 	 0.04871 	 0.00050 	 m..s
   72 	     6 	 0.05098 	 0.00051 	 m..s
   27 	     7 	 0.04852 	 0.00052 	 m..s
   43 	     8 	 0.04907 	 0.00052 	 m..s
    1 	     9 	 0.04230 	 0.00054 	 m..s
   20 	    10 	 0.04767 	 0.00055 	 m..s
   30 	    11 	 0.04854 	 0.00055 	 m..s
   31 	    12 	 0.04859 	 0.00055 	 m..s
   76 	    13 	 0.05110 	 0.00056 	 m..s
   75 	    14 	 0.05109 	 0.00056 	 m..s
   68 	    15 	 0.05060 	 0.00057 	 m..s
    6 	    16 	 0.04495 	 0.00058 	 m..s
   32 	    17 	 0.04860 	 0.00058 	 m..s
   23 	    18 	 0.04797 	 0.00059 	 m..s
    3 	    19 	 0.04304 	 0.00059 	 m..s
   15 	    20 	 0.04697 	 0.00060 	 m..s
   73 	    21 	 0.05098 	 0.00060 	 m..s
   71 	    22 	 0.05098 	 0.00060 	 m..s
    0 	    23 	 0.04218 	 0.00061 	 m..s
   29 	    24 	 0.04853 	 0.00062 	 m..s
   35 	    25 	 0.04866 	 0.00062 	 m..s
   70 	    26 	 0.05072 	 0.00063 	 m..s
   64 	    27 	 0.05028 	 0.00064 	 m..s
   47 	    28 	 0.04916 	 0.00069 	 m..s
   74 	    29 	 0.05100 	 0.00069 	 m..s
    4 	    30 	 0.04307 	 0.00070 	 m..s
   78 	    31 	 0.05110 	 0.00071 	 m..s
    2 	    32 	 0.04237 	 0.00072 	 m..s
   49 	    33 	 0.04942 	 0.00072 	 m..s
   44 	    34 	 0.04907 	 0.00074 	 m..s
   21 	    35 	 0.04770 	 0.00079 	 m..s
   60 	    36 	 0.05017 	 0.00081 	 m..s
   12 	    37 	 0.04557 	 0.00081 	 m..s
    9 	    38 	 0.04524 	 0.00081 	 m..s
   62 	    39 	 0.05024 	 0.00082 	 m..s
   33 	    40 	 0.04861 	 0.00084 	 m..s
   63 	    41 	 0.05027 	 0.00085 	 m..s
   17 	    42 	 0.04718 	 0.00085 	 m..s
   28 	    43 	 0.04853 	 0.00086 	 m..s
   39 	    44 	 0.04897 	 0.00087 	 m..s
   13 	    45 	 0.04616 	 0.00088 	 m..s
   34 	    46 	 0.04863 	 0.00090 	 m..s
   56 	    47 	 0.05007 	 0.00090 	 m..s
   69 	    48 	 0.05063 	 0.00093 	 m..s
   41 	    49 	 0.04899 	 0.00095 	 m..s
   37 	    50 	 0.04889 	 0.00095 	 m..s
   77 	    51 	 0.05110 	 0.00097 	 m..s
   16 	    52 	 0.04704 	 0.00098 	 m..s
   55 	    53 	 0.05007 	 0.00102 	 m..s
   38 	    54 	 0.04894 	 0.00106 	 m..s
   10 	    55 	 0.04526 	 0.00110 	 m..s
   66 	    56 	 0.05051 	 0.00112 	 m..s
   40 	    57 	 0.04897 	 0.00113 	 m..s
   42 	    58 	 0.04900 	 0.00115 	 m..s
   67 	    59 	 0.05055 	 0.00115 	 m..s
   11 	    60 	 0.04533 	 0.00116 	 m..s
   58 	    61 	 0.05014 	 0.00119 	 m..s
   57 	    62 	 0.05010 	 0.00119 	 m..s
   61 	    63 	 0.05020 	 0.00123 	 m..s
   52 	    64 	 0.05001 	 0.00131 	 m..s
   79 	    65 	 0.05523 	 0.00139 	 m..s
   45 	    66 	 0.04908 	 0.00139 	 m..s
   22 	    67 	 0.04785 	 0.00146 	 m..s
    7 	    68 	 0.04506 	 0.00148 	 m..s
   14 	    69 	 0.04681 	 0.00150 	 m..s
   18 	    70 	 0.04719 	 0.00154 	 m..s
   50 	    71 	 0.04944 	 0.00167 	 m..s
   65 	    72 	 0.05042 	 0.00167 	 m..s
   59 	    73 	 0.05015 	 0.00178 	 m..s
   54 	    74 	 0.05002 	 0.00181 	 m..s
   24 	    75 	 0.04809 	 0.00184 	 m..s
   26 	    76 	 0.04851 	 0.00193 	 m..s
   19 	    77 	 0.04759 	 0.00199 	 m..s
   46 	    78 	 0.04913 	 0.00208 	 m..s
   53 	    79 	 0.05002 	 0.00230 	 m..s
   92 	    80 	 0.09524 	 0.01712 	 m..s
   83 	    81 	 0.07234 	 0.04752 	 ~...
   86 	    82 	 0.07734 	 0.05524 	 ~...
  101 	    83 	 0.11226 	 0.06832 	 m..s
   89 	    84 	 0.09227 	 0.07041 	 ~...
  107 	    85 	 0.11808 	 0.07066 	 m..s
   96 	    86 	 0.09844 	 0.07113 	 ~...
  102 	    87 	 0.11302 	 0.07221 	 m..s
   84 	    88 	 0.07494 	 0.07228 	 ~...
   81 	    89 	 0.07058 	 0.07296 	 ~...
   93 	    90 	 0.09548 	 0.07469 	 ~...
  105 	    91 	 0.11626 	 0.07607 	 m..s
   82 	    92 	 0.07087 	 0.07732 	 ~...
   80 	    93 	 0.06740 	 0.07817 	 ~...
   85 	    94 	 0.07588 	 0.07886 	 ~...
   88 	    95 	 0.08976 	 0.08027 	 ~...
   97 	    96 	 0.10085 	 0.08351 	 ~...
  109 	    97 	 0.13687 	 0.08607 	 m..s
  104 	    98 	 0.11576 	 0.08811 	 ~...
  103 	    99 	 0.11500 	 0.08832 	 ~...
   94 	   100 	 0.09596 	 0.08873 	 ~...
  116 	   101 	 0.20702 	 0.08938 	 MISS
   98 	   102 	 0.10432 	 0.09658 	 ~...
   99 	   103 	 0.10610 	 0.09686 	 ~...
  100 	   104 	 0.11151 	 0.09711 	 ~...
  106 	   105 	 0.11698 	 0.10101 	 ~...
   95 	   106 	 0.09700 	 0.10497 	 ~...
  112 	   107 	 0.16969 	 0.12358 	 m..s
   91 	   108 	 0.09512 	 0.12857 	 m..s
  114 	   109 	 0.18297 	 0.12958 	 m..s
  115 	   110 	 0.19849 	 0.13256 	 m..s
   90 	   111 	 0.09491 	 0.13837 	 m..s
  108 	   112 	 0.13437 	 0.14022 	 ~...
  118 	   113 	 0.21886 	 0.16794 	 m..s
  113 	   114 	 0.17882 	 0.18649 	 ~...
  117 	   115 	 0.20880 	 0.19419 	 ~...
   87 	   116 	 0.08834 	 0.20396 	 MISS
  111 	   117 	 0.16039 	 0.24200 	 m..s
  110 	   118 	 0.15252 	 0.24510 	 m..s
  120 	   119 	 0.23372 	 0.31774 	 m..s
  119 	   120 	 0.22885 	 0.32174 	 m..s
==========================================
r_mrr = 0.8953655362129211
r2_mrr = 0.4958149790763855
spearmanr_mrr@5 = 0.9503238201141357
spearmanr_mrr@10 = 0.9542934894561768
spearmanr_mrr@50 = 0.972324550151825
spearmanr_mrr@100 = 0.9840117692947388
spearmanr_mrr@All = 0.9848542809486389
==========================================
test time: 0.447
Done Testing dataset OpenEA
total time taken: 807.579386472702
training time taken: 762.0169622898102
TWIG out ;))
===========================================================
-----------------------------------------------------------
Running a TWIG experiment with tag: ComplEx-omit-CoDExSmall
-----------------------------------------------------------
===========================================================
Using random seed: 9619042407053324
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading DBpedia50...
- loading run 2.1...
Loading UMLS...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [696, 892, 591, 428, 826, 733, 434, 392, 877, 661, 350, 1159, 257, 681, 1073, 996, 867, 49, 391, 981, 176, 803, 674, 1075, 228, 273, 542, 409, 1189, 781, 102, 94, 922, 1156, 709, 1045, 80, 936, 506, 1174, 930, 946, 564, 660, 1167, 870, 839, 953, 283, 890, 247, 1198, 124, 1166, 276, 1033, 270, 866, 1084, 848, 271, 44, 742, 766, 635, 623, 689, 443, 155, 753, 445, 630, 1192, 345, 577, 400, 432, 394, 369, 667, 557, 452, 225, 15, 120, 1063, 241, 1083, 693, 484, 928, 942, 644, 626, 420, 562, 869, 677, 91, 548, 267, 560, 1071, 904, 216, 387, 256, 575, 983, 1002, 321, 1138, 727, 728, 246, 78, 796, 201, 1210, 795, 897]
valid_ids (0): []
train_ids (1094): [1062, 203, 791, 81, 421, 765, 545, 1004, 1081, 659, 372, 792, 554, 601, 135, 889, 211, 22, 111, 175, 954, 968, 563, 1090, 383, 579, 431, 1203, 14, 482, 16, 553, 535, 873, 436, 126, 370, 1122, 977, 539, 504, 1098, 691, 589, 1003, 860, 780, 1105, 173, 527, 157, 1095, 289, 352, 347, 724, 1154, 1110, 417, 1103, 634, 583, 316, 1188, 13, 373, 122, 773, 438, 117, 608, 1029, 119, 1202, 475, 607, 712, 174, 830, 24, 611, 35, 296, 398, 934, 251, 878, 960, 595, 490, 805, 419, 704, 769, 278, 299, 559, 97, 145, 255, 1108, 536, 200, 153, 1201, 458, 541, 84, 685, 47, 414, 284, 499, 1112, 813, 492, 967, 453, 859, 113, 580, 324, 195, 42, 261, 593, 1032, 442, 790, 808, 23, 901, 236, 621, 931, 800, 625, 642, 90, 719, 263, 138, 598, 192, 1102, 181, 558, 1008, 786, 1020, 164, 56, 815, 1176, 1043, 713, 974, 945, 943, 956, 746, 964, 367, 1000, 898, 313, 757, 1023, 1074, 669, 252, 1030, 469, 923, 690, 565, 701, 64, 972, 493, 306, 375, 483, 346, 37, 590, 308, 687, 896, 657, 767, 547, 853, 695, 573, 1191, 1026, 778, 913, 819, 990, 1053, 1148, 747, 202, 1179, 797, 739, 319, 1009, 17, 811, 836, 1153, 1123, 551, 222, 331, 468, 907, 36, 1120, 641, 314, 79, 163, 481, 250, 950, 652, 43, 651, 718, 646, 1017, 430, 823, 62, 703, 1051, 682, 388, 478, 785, 522, 882, 863, 872, 905, 1014, 218, 955, 903, 328, 132, 253, 349, 508, 997, 121, 663, 1168, 96, 944, 393, 471, 957, 456, 4, 457, 92, 1212, 1182, 286, 502, 749, 1111, 1119, 226, 686, 351, 825, 1106, 292, 1025, 675, 329, 470, 408, 864, 154, 538, 341, 807, 993, 1064, 624, 109, 533, 1005, 27, 862, 917, 678, 606, 186, 1169, 513, 828, 531, 1172, 407, 520, 1124, 592, 242, 384, 543, 929, 1177, 386, 676, 812, 317, 1184, 1171, 390, 895, 985, 1126, 489, 68, 1208, 266, 193, 268, 672, 763, 217, 1040, 984, 1101, 774, 137, 544, 1049, 1149, 926, 83, 497, 886, 664, 1132, 566, 1134, 738, 963, 1125, 170, 992, 378, 1178, 546, 189, 297, 512, 31, 1034, 282, 1136, 179, 67, 425, 653, 498, 1100, 424, 935, 485, 45, 259, 82, 572, 918, 1079, 924, 816, 206, 389, 277, 610, 74, 1190, 940, 343, 404, 1173, 884, 423, 279, 374, 516, 735, 998, 707, 694, 700, 534, 658, 298, 234, 88, 561, 65, 770, 711, 11, 1035, 1139, 920, 361, 978, 587, 103, 507, 969, 288, 160, 172, 804, 744, 332, 150, 168, 1187, 224, 902, 939, 1076, 838, 574, 857, 671, 354, 569, 480, 412, 1072, 662, 1144, 842, 212, 576, 1099, 999, 61, 221, 750, 60, 809, 427, 951, 868, 1157, 1038, 127, 320, 876, 28, 139, 148, 315, 303, 721, 745, 159, 1022, 613, 357, 1021, 1060, 798, 1088, 818, 114, 326, 523, 501, 528, 233, 526, 156, 881, 645, 1082, 597, 656, 865, 1057, 301, 167, 1140, 66, 518, 281, 970, 488, 287, 552, 698, 665, 568, 355, 824, 169, 1047, 142, 1048, 995, 339, 1041, 401, 1150, 888, 1068, 937, 1069, 777, 782, 147, 131, 89, 166, 751, 989, 952, 982, 1097, 237, 1028, 1039, 1137, 1121, 639, 455, 845, 771, 726, 397, 602, 725, 376, 1158, 57, 360, 318, 323, 275, 437, 1016, 146, 582, 1145, 300, 1052, 649, 451, 215, 764, 976, 444, 476, 841, 1, 209, 705, 1096, 312, 1024, 731, 697, 194, 723, 814, 640, 258, 158, 736, 670, 136, 628, 609, 772, 783, 377, 54, 448, 487, 1131, 198, 1181, 33, 1042, 799, 776, 177, 820, 706, 337, 5, 1113, 880, 941, 618, 1011, 1067, 702, 413, 1155, 447, 847, 643, 463, 891, 1185, 1094, 30, 584, 262, 72, 229, 264, 688, 787, 411, 1036, 115, 784, 269, 979, 21, 165, 619, 93, 1013, 594, 612, 85, 19, 462, 550, 336, 108, 162, 631, 921, 1077, 1209, 912, 852, 600, 405, 1091, 307, 254, 197, 655, 887, 144, 73, 916, 638, 861, 396, 364, 433, 1085, 359, 59, 243, 449, 632, 291, 858, 1054, 305, 627, 491, 908, 650, 52, 555, 829, 837, 473, 743, 208, 141, 1092, 309, 549, 380, 180, 1133, 1194, 1193, 758, 966, 1204, 714, 537, 927, 40, 817, 617, 32, 213, 1058, 893, 810, 260, 708, 8, 885, 620, 1142, 1089, 605, 616, 1019, 906, 910, 467, 402, 511, 1161, 604, 426, 789, 76, 915, 588, 134, 210, 1010, 1205, 477, 1037, 450, 129, 55, 1114, 1086, 231, 464, 603, 1118, 112, 333, 760, 340, 1160, 395, 762, 403, 622, 932, 748, 26, 1128, 1213, 761, 435, 875, 835, 794, 415, 673, 196, 806, 496, 586, 801, 1127, 710, 230, 1031, 188, 775, 1165, 1056, 1152, 191, 184, 855, 454, 1135, 1080, 879, 406, 1163, 86, 9, 190, 500, 143, 379, 1143, 6, 571, 737, 116, 684, 71, 938, 517, 140, 104, 272, 10, 925, 106, 29, 39, 654, 732, 948, 371, 570, 465, 717, 1151, 362, 58, 521, 1078, 12, 1015, 363, 716, 1199, 1211, 933, 556, 720, 519, 1195, 1197, 883, 38, 1206, 822, 290, 2, 1109, 18, 479, 986, 899, 614, 204, 100, 1141, 461, 410, 474, 1044, 1147, 107, 1093, 844, 515, 965, 105, 48, 220, 894, 125, 988, 310, 178, 1066, 1117, 1183, 1065, 734, 636, 50, 991, 827, 730, 171, 532, 994, 540, 840, 99, 666, 514, 509, 581, 238, 294, 34, 648, 1001, 679, 1104, 199, 285, 185, 440, 265, 295, 441, 1046, 1006, 525, 75, 754, 459, 715, 368, 7, 245, 335, 235, 949, 123, 802, 118, 637, 1214, 973, 768, 3, 161, 530, 1162, 152, 629, 110, 95, 831, 69, 647, 851, 439, 1186, 330, 756, 130, 460, 959, 214, 51, 683, 524, 752, 1146, 128, 365, 779, 741, 446, 416, 293, 182, 909, 101, 615, 1061, 381, 1116, 740, 599, 240, 1207, 874, 399, 729, 46, 353, 1018, 911, 596, 1007, 1170, 871, 227, 961, 133, 850, 466, 1200, 856, 239, 87, 205, 567, 971, 1175, 849, 962, 503, 0, 53, 914, 834, 249, 987, 759, 382, 1196, 699, 232, 919, 25, 975, 755, 338, 578, 833, 585, 846, 1107, 356, 327, 70, 1012, 248, 311, 958, 486, 680, 149, 304, 223, 1050, 692, 1129, 322, 422, 207, 280, 358, 722, 41, 1087, 187, 98, 77, 832, 788, 20, 668, 793, 1130, 274, 418, 1180, 366, 325, 342, 529, 1164, 385, 633, 900, 302, 510, 1059, 821, 494, 344, 1115, 854, 1070, 429, 980, 1055, 334, 947, 244, 348, 183, 505, 151, 495, 1027, 843, 472, 63, 219]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4468635412320671
the save name prefix for this run is:  chkpt-ID_4468635412320671_tag_ComplEx-omit-CoDExSmall
running TWIG with settings:
data_to_load: {'ComplEx': {'DBpedia50': ['2.1'], 'UMLS': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 95
rank avg (pred): 0.501 +- 0.006
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001241098

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 234
rank avg (pred): 0.535 +- 0.223
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 7.43806e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.518 +- 0.267
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.85047e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1173
rank avg (pred): 0.518 +- 0.287
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 9.0823e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 503
rank avg (pred): 0.354 +- 0.317
mrr vals (pred, true): 0.000, 0.139
batch losses (mrrl, rdl): 0.0, 1.54701e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 759
rank avg (pred): 0.511 +- 0.303
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 3.32023e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1051
rank avg (pred): 0.520 +- 0.305
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 7.7421e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.527 +- 0.292
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.21102e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 621
rank avg (pred): 0.511 +- 0.287
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 8.6077e-06

Epoch over!
epoch time: 51.235

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 477
rank avg (pred): 0.521 +- 0.296
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 6.74441e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 341
rank avg (pred): 0.511 +- 0.305
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.62218e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 785
rank avg (pred): 0.508 +- 0.287
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 1.68875e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 909
rank avg (pred): 0.534 +- 0.283
mrr vals (pred, true): 0.000, 0.008
batch losses (mrrl, rdl): 0.0, 1.76815e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1077
rank avg (pred): 0.369 +- 0.318
mrr vals (pred, true): 0.002, 0.154
batch losses (mrrl, rdl): 0.0, 0.000135459

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 398
rank avg (pred): 0.513 +- 0.306
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.16e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1121
rank avg (pred): 0.534 +- 0.298
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 2.2695e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 258
rank avg (pred): 0.179 +- 0.247
mrr vals (pred, true): 0.002, 0.299
batch losses (mrrl, rdl): 0.0, 1.58059e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.514 +- 0.287
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.45631e-05

Epoch over!
epoch time: 50.683

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 555
rank avg (pred): 0.382 +- 0.335
mrr vals (pred, true): 0.001, 0.090
batch losses (mrrl, rdl): 0.0, 1.99261e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 965
rank avg (pred): 0.525 +- 0.291
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 2.80321e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 509
rank avg (pred): 0.312 +- 0.342
mrr vals (pred, true): 0.002, 0.160
batch losses (mrrl, rdl): 0.0, 1.20997e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 355
rank avg (pred): 0.531 +- 0.293
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 8.3953e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 945
rank avg (pred): 0.518 +- 0.296
mrr vals (pred, true): 0.019, 0.000
batch losses (mrrl, rdl): 0.0, 2.8379e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 895
rank avg (pred): 0.556 +- 0.302
mrr vals (pred, true): 0.031, 0.028
batch losses (mrrl, rdl): 0.0, 0.0002840454

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 384
rank avg (pred): 0.508 +- 0.294
mrr vals (pred, true): 0.019, 0.000
batch losses (mrrl, rdl): 0.0, 2.19176e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1160
rank avg (pred): 0.380 +- 0.307
mrr vals (pred, true): 0.051, 0.126
batch losses (mrrl, rdl): 0.0, 6.31561e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 248
rank avg (pred): 0.250 +- 0.319
mrr vals (pred, true): 0.047, 0.149
batch losses (mrrl, rdl): 0.0, 9.38075e-05

Epoch over!
epoch time: 51.052

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 642
rank avg (pred): 0.520 +- 0.291
mrr vals (pred, true): 0.006, 0.001
batch losses (mrrl, rdl): 0.0, 3.5959e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.316 +- 0.322
mrr vals (pred, true): 0.023, 0.132
batch losses (mrrl, rdl): 0.0, 3.46785e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 159
rank avg (pred): 0.512 +- 0.301
mrr vals (pred, true): 0.021, 0.000
batch losses (mrrl, rdl): 0.0, 7.9582e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 360
rank avg (pred): 0.513 +- 0.288
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.0, 3.0637e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 762
rank avg (pred): 0.473 +- 0.271
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0, 7.91811e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.512 +- 0.292
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0, 2.4371e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 538
rank avg (pred): 0.420 +- 0.305
mrr vals (pred, true): 0.043, 0.078
batch losses (mrrl, rdl): 0.0, 1.35543e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.524 +- 0.297
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0, 3.18547e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 547
rank avg (pred): 0.334 +- 0.303
mrr vals (pred, true): 0.065, 0.102
batch losses (mrrl, rdl): 0.0, 4.94773e-05

Epoch over!
epoch time: 50.947

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1067
rank avg (pred): 0.409 +- 0.329
mrr vals (pred, true): 0.043, 0.122
batch losses (mrrl, rdl): 0.0, 1.63786e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 914
rank avg (pred): 0.556 +- 0.282
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0, 6.45354e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.512 +- 0.283
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0, 3.13707e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 695
rank avg (pred): 0.509 +- 0.282
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0, 2.7254e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 763
rank avg (pred): 0.521 +- 0.302
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0, 1.95762e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 254
rank avg (pred): 0.204 +- 0.294
mrr vals (pred, true): 0.073, 0.314
batch losses (mrrl, rdl): 0.0, 1.04922e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1012
rank avg (pred): 0.517 +- 0.291
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0, 1.4539e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1162
rank avg (pred): 0.500 +- 0.284
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.0, 2.37572e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1124
rank avg (pred): 0.502 +- 0.287
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0, 2.71368e-05

Epoch over!
epoch time: 51.049

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 593
rank avg (pred): 0.522 +- 0.286
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002461986, 6.6961e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 503
rank avg (pred): 0.337 +- 0.462
mrr vals (pred, true): 0.066, 0.139
batch losses (mrrl, rdl): 0.0528069921, 0.0001360126

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 337
rank avg (pred): 0.415 +- 0.419
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0034159105, 0.000456323

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.218 +- 0.389
mrr vals (pred, true): 0.173, 0.217
batch losses (mrrl, rdl): 0.0188283231, 6.12105e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 244
rank avg (pred): 0.323 +- 0.452
mrr vals (pred, true): 0.119, 0.119
batch losses (mrrl, rdl): 4.74e-08, 0.0002354359

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.421 +- 0.410
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002942709, 0.0003980289

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1205
rank avg (pred): 0.431 +- 0.408
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0001051736, 0.0001033035

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 213
rank avg (pred): 0.420 +- 0.408
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.0869e-06, 0.0004898297

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.289 +- 0.435
mrr vals (pred, true): 0.146, 0.189
batch losses (mrrl, rdl): 0.0184983537, 0.0001256955

Epoch over!
epoch time: 53.594

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.417 +- 0.414
mrr vals (pred, true): 0.054, 0.002
batch losses (mrrl, rdl): 0.0001834891, 0.0002654158

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 886
rank avg (pred): 0.417 +- 0.417
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 7.8231e-06, 0.0004966202

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1061
rank avg (pred): 0.269 +- 0.428
mrr vals (pred, true): 0.157, 0.208
batch losses (mrrl, rdl): 0.0267212074, 9.39258e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1118
rank avg (pred): 0.405 +- 0.421
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001450432, 0.0002730988

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.398 +- 0.430
mrr vals (pred, true): 0.068, 0.020
batch losses (mrrl, rdl): 0.0033071567, 0.0011740869

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 600
rank avg (pred): 0.415 +- 0.413
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0030580861, 0.0002910088

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 68
rank avg (pred): 0.312 +- 0.452
mrr vals (pred, true): 0.172, 0.094
batch losses (mrrl, rdl): 0.1500368714, 0.0001813007

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 474
rank avg (pred): 0.434 +- 0.422
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 3.8956e-06, 0.0001703704

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 992
rank avg (pred): 0.324 +- 0.461
mrr vals (pred, true): 0.168, 0.078
batch losses (mrrl, rdl): 0.1389922202, 0.0002319555

Epoch over!
epoch time: 54.712

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.440 +- 0.422
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.241e-06, 0.0002379487

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.486 +- 0.421
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.13e-08, 0.0001557735

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 103
rank avg (pred): 0.430 +- 0.422
mrr vals (pred, true): 0.031, 0.000
batch losses (mrrl, rdl): 0.0035791514, 0.0002190841

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 818
rank avg (pred): 0.323 +- 0.462
mrr vals (pred, true): 0.140, 0.054
batch losses (mrrl, rdl): 0.0814227685, 0.0003033892

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 573
rank avg (pred): 0.451 +- 0.426
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.19197e-05, 0.0002913043

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 440
rank avg (pred): 0.449 +- 0.422
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.000207361, 0.0003121081

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1005
rank avg (pred): 0.429 +- 0.427
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 5.4453e-06, 0.0002266531

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 682
rank avg (pred): 0.439 +- 0.422
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 6.874e-07, 0.0002953368

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1143
rank avg (pred): 0.430 +- 0.422
mrr vals (pred, true): 0.069, 0.098
batch losses (mrrl, rdl): 0.0035789907, 0.0001174954

Epoch over!
epoch time: 54.769

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 966
rank avg (pred): 0.439 +- 0.427
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 7.18957e-05, 0.0002382651

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1207
rank avg (pred): 0.456 +- 0.421
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001336644, 0.0002948779

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 944
rank avg (pred): 0.408 +- 0.428
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0040751984, 0.0005315123

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 398
rank avg (pred): 0.798 +- 0.363
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.57959e-05, 0.0017046125

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 252
rank avg (pred): 0.586 +- 0.479
mrr vals (pred, true): 0.117, 0.307
batch losses (mrrl, rdl): 0.3589375317, 0.0026094005

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 835
rank avg (pred): 0.343 +- 0.469
mrr vals (pred, true): 0.130, 0.271
batch losses (mrrl, rdl): 0.1987990439, 0.0004125508

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1157
rank avg (pred): 0.654 +- 0.437
mrr vals (pred, true): 0.065, 0.106
batch losses (mrrl, rdl): 0.016353026, 0.0017229399

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 373
rank avg (pred): 0.650 +- 0.438
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 5.582e-06, 0.0004793282

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 971
rank avg (pred): 0.632 +- 0.427
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.33544e-05, 0.0004417812

Epoch over!
epoch time: 55.1

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 376
rank avg (pred): 0.627 +- 0.428
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0005977092, 0.000465786

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.519 +- 0.431
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003243706, 0.0001573687

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1064
rank avg (pred): 0.241 +- 0.392
mrr vals (pred, true): 0.114, 0.118
batch losses (mrrl, rdl): 0.0001800489, 0.0003981372

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 496
rank avg (pred): 0.256 +- 0.373
mrr vals (pred, true): 0.130, 0.147
batch losses (mrrl, rdl): 0.0026825061, 0.0001401037

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1193
rank avg (pred): 0.588 +- 0.405
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005106293, 0.0002476931

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 138
rank avg (pred): 0.550 +- 0.435
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0017832454, 0.0001706903

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 926
rank avg (pred): 0.575 +- 0.420
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.000379717, 0.0001967145

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1110
rank avg (pred): 0.501 +- 0.400
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004905789, 8.17031e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 116
rank avg (pred): 0.544 +- 0.386
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.3216e-06, 9.37333e-05

Epoch over!
epoch time: 55.027

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 990
rank avg (pred): 0.223 +- 0.368
mrr vals (pred, true): 0.102, 0.082
batch losses (mrrl, rdl): 0.0265947282, 0.0003354408

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 456
rank avg (pred): 0.596 +- 0.414
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.61814e-05, 0.0002707792

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1077
rank avg (pred): 0.209 +- 0.376
mrr vals (pred, true): 0.151, 0.154
batch losses (mrrl, rdl): 0.0001250088, 0.0001489893

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 349
rank avg (pred): 0.545 +- 0.413
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0016829137, 0.0002236947

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1099
rank avg (pred): 0.565 +- 0.415
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.2293e-05, 0.0002353616

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 251
rank avg (pred): 0.202 +- 0.358
mrr vals (pred, true): 0.121, 0.168
batch losses (mrrl, rdl): 0.0220742561, 0.0002541858

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 209
rank avg (pred): 0.573 +- 0.419
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0005932403, 0.0001251191

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.571 +- 0.423
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 3.234e-07, 0.0001585772

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 373
rank avg (pred): 0.506 +- 0.410
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.09615e-05, 0.0001015358

Epoch over!
epoch time: 52.899

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 597
rank avg (pred): 0.577 +- 0.427
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.00050988, 0.0003621122

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.479 +- 0.397
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.36438e-05, 0.000123736

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.224 +- 0.359
mrr vals (pred, true): 0.098, 0.079
batch losses (mrrl, rdl): 0.0228795391, 0.0003531732

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 702
rank avg (pred): 0.556 +- 0.420
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 4.32626e-05, 0.0001343017

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 412
rank avg (pred): 0.534 +- 0.424
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003239549, 0.0001404944

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 569
rank avg (pred): 0.557 +- 0.421
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.38571e-05, 0.0001720652

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 496
rank avg (pred): 0.328 +- 0.397
mrr vals (pred, true): 0.096, 0.147
batch losses (mrrl, rdl): 0.0257976241, 4.52235e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 209
rank avg (pred): 0.531 +- 0.434
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.50028e-05, 0.000169158

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 910
rank avg (pred): 0.516 +- 0.439
mrr vals (pred, true): 0.071, 0.020
batch losses (mrrl, rdl): 0.0043060598, 0.0001197621

Epoch over!
epoch time: 54.016

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 741
rank avg (pred): 0.205 +- 0.367
mrr vals (pred, true): 0.092, 0.208
batch losses (mrrl, rdl): 0.1334552765, 5.50052e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 948
rank avg (pred): 0.537 +- 0.410
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0012795512, 0.0001665001

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1142
rank avg (pred): 0.149 +- 0.219
mrr vals (pred, true): 0.143, 0.189
batch losses (mrrl, rdl): 0.021190038, 0.0003700522

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.482 +- 0.389
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 9.74601e-05, 4.63918e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.206 +- 0.344
mrr vals (pred, true): 0.082, 0.096
batch losses (mrrl, rdl): 0.0101223625, 0.0003657058

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1204
rank avg (pred): 0.439 +- 0.376
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002738063, 0.0001465815

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 104
rank avg (pred): 0.484 +- 0.387
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0014610913, 4.86669e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 163
rank avg (pred): 0.483 +- 0.390
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 7.57e-08, 9.3166e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 982
rank avg (pred): 0.191 +- 0.339
mrr vals (pred, true): 0.113, 0.110
batch losses (mrrl, rdl): 8.388e-05, 0.0003910785

Epoch over!
epoch time: 54.392

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 93
rank avg (pred): 0.469 +- 0.394
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.1985e-06, 8.22313e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 570
rank avg (pred): 0.506 +- 0.399
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.5489e-06, 7.89245e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1129
rank avg (pred): 0.481 +- 0.396
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 4.6724e-06, 6.99031e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 654
rank avg (pred): 0.529 +- 0.394
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.00537e-05, 7.03427e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 159
rank avg (pred): 0.444 +- 0.381
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 4.1424e-06, 0.0001408654

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 995
rank avg (pred): 0.115 +- 0.233
mrr vals (pred, true): 0.153, 0.094
batch losses (mrrl, rdl): 0.1053358763, 0.0007194303

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 218
rank avg (pred): 0.511 +- 0.399
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.3593e-06, 8.32081e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.214 +- 0.356
mrr vals (pred, true): 0.105, 0.129
batch losses (mrrl, rdl): 0.0057371976, 0.000229957

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 1023
rank avg (pred): 0.484 +- 0.391
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 4.859e-06, 7.48729e-05

Epoch over!
epoch time: 53.282

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 856
rank avg (pred): 0.501 +- 0.394
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 8.898e-06, 8.35354e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 999
rank avg (pred): 0.499 +- 0.397
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.4e-09, 8.31313e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 338
rank avg (pred): 0.477 +- 0.391
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 9.1023e-06, 6.94911e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 386
rank avg (pred): 0.493 +- 0.408
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 4.89627e-05, 0.0001142395

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.477 +- 0.389
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.2146e-06, 9.52264e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 791
rank avg (pred): 0.524 +- 0.397
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.23934e-05, 8.73151e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 879
rank avg (pred): 0.529 +- 0.395
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 8.2553e-06, 9.09145e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 655
rank avg (pred): 0.539 +- 0.400
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 6.84759e-05, 0.0001201831

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, DBpedia50, run 2.1, exp 912
rank avg (pred): 0.508 +- 0.387
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0015245295, 0.0001699552

Epoch over!
epoch time: 52.492

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.505 +- 0.397
mrr vals (pred, true): 0.047, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   88 	     0 	 0.07330 	 7e-0500 	 m..s
   42 	     1 	 0.04726 	 0.00012 	 m..s
   30 	     2 	 0.04678 	 0.00014 	 m..s
   47 	     3 	 0.04752 	 0.00014 	 m..s
   61 	     4 	 0.04850 	 0.00015 	 m..s
    9 	     5 	 0.04610 	 0.00015 	 m..s
   16 	     6 	 0.04640 	 0.00016 	 m..s
   36 	     7 	 0.04696 	 0.00016 	 m..s
   46 	     8 	 0.04751 	 0.00017 	 m..s
   51 	     9 	 0.04776 	 0.00018 	 m..s
   13 	    10 	 0.04620 	 0.00018 	 m..s
   34 	    11 	 0.04694 	 0.00018 	 m..s
    2 	    12 	 0.04565 	 0.00019 	 m..s
    0 	    13 	 0.04559 	 0.00019 	 m..s
   25 	    14 	 0.04658 	 0.00021 	 m..s
   35 	    15 	 0.04695 	 0.00021 	 m..s
   20 	    16 	 0.04646 	 0.00021 	 m..s
   15 	    17 	 0.04633 	 0.00021 	 m..s
   38 	    18 	 0.04719 	 0.00021 	 m..s
   41 	    19 	 0.04725 	 0.00021 	 m..s
    5 	    20 	 0.04576 	 0.00022 	 m..s
   45 	    21 	 0.04750 	 0.00022 	 m..s
   23 	    22 	 0.04653 	 0.00022 	 m..s
   33 	    23 	 0.04693 	 0.00022 	 m..s
   53 	    24 	 0.04789 	 0.00023 	 m..s
   21 	    25 	 0.04651 	 0.00023 	 m..s
   76 	    26 	 0.05031 	 0.00023 	 m..s
   68 	    27 	 0.04900 	 0.00024 	 m..s
   74 	    28 	 0.04988 	 0.00024 	 m..s
   54 	    29 	 0.04798 	 0.00024 	 m..s
   11 	    30 	 0.04614 	 0.00025 	 m..s
   62 	    31 	 0.04855 	 0.00025 	 m..s
    4 	    32 	 0.04573 	 0.00025 	 m..s
   40 	    33 	 0.04723 	 0.00025 	 m..s
   55 	    34 	 0.04820 	 0.00025 	 m..s
   14 	    35 	 0.04621 	 0.00025 	 m..s
   63 	    36 	 0.04855 	 0.00026 	 m..s
   80 	    37 	 0.05105 	 0.00026 	 m..s
   18 	    38 	 0.04645 	 0.00026 	 m..s
    3 	    39 	 0.04572 	 0.00026 	 m..s
   10 	    40 	 0.04611 	 0.00026 	 m..s
    1 	    41 	 0.04562 	 0.00026 	 m..s
   49 	    42 	 0.04760 	 0.00026 	 m..s
   39 	    43 	 0.04721 	 0.00027 	 m..s
   79 	    44 	 0.05063 	 0.00027 	 m..s
   75 	    45 	 0.04992 	 0.00027 	 m..s
   22 	    46 	 0.04652 	 0.00028 	 m..s
   43 	    47 	 0.04726 	 0.00028 	 m..s
   58 	    48 	 0.04836 	 0.00028 	 m..s
    8 	    49 	 0.04595 	 0.00029 	 m..s
   12 	    50 	 0.04618 	 0.00029 	 m..s
   73 	    51 	 0.04969 	 0.00029 	 m..s
   67 	    52 	 0.04897 	 0.00029 	 m..s
   50 	    53 	 0.04765 	 0.00030 	 m..s
   71 	    54 	 0.04927 	 0.00030 	 m..s
   31 	    55 	 0.04682 	 0.00030 	 m..s
   64 	    56 	 0.04864 	 0.00031 	 m..s
   17 	    57 	 0.04641 	 0.00031 	 m..s
   24 	    58 	 0.04654 	 0.00031 	 m..s
   56 	    59 	 0.04823 	 0.00033 	 m..s
   65 	    60 	 0.04887 	 0.00034 	 m..s
   44 	    61 	 0.04747 	 0.00034 	 m..s
   52 	    62 	 0.04780 	 0.00035 	 m..s
   26 	    63 	 0.04664 	 0.00035 	 m..s
   27 	    64 	 0.04666 	 0.00036 	 m..s
   57 	    65 	 0.04825 	 0.00038 	 m..s
   72 	    66 	 0.04934 	 0.00038 	 m..s
   60 	    67 	 0.04848 	 0.00041 	 m..s
   29 	    68 	 0.04675 	 0.00043 	 m..s
    6 	    69 	 0.04589 	 0.00043 	 m..s
   59 	    70 	 0.04843 	 0.00049 	 m..s
    7 	    71 	 0.04593 	 0.00049 	 m..s
   78 	    72 	 0.05055 	 0.00052 	 m..s
   37 	    73 	 0.04698 	 0.00053 	 m..s
   48 	    74 	 0.04759 	 0.00053 	 m..s
   28 	    75 	 0.04674 	 0.00059 	 m..s
   81 	    76 	 0.07100 	 0.00066 	 m..s
   19 	    77 	 0.04646 	 0.00072 	 m..s
   70 	    78 	 0.04923 	 0.00076 	 m..s
   69 	    79 	 0.04912 	 0.00086 	 m..s
   32 	    80 	 0.04692 	 0.00095 	 m..s
   66 	    81 	 0.04889 	 0.00149 	 m..s
   77 	    82 	 0.05034 	 0.00209 	 m..s
   96 	    83 	 0.08840 	 0.04783 	 m..s
  105 	    84 	 0.13364 	 0.06449 	 m..s
   81 	    85 	 0.07100 	 0.06847 	 ~...
   92 	    86 	 0.08395 	 0.07100 	 ~...
   87 	    87 	 0.07309 	 0.07344 	 ~...
   95 	    88 	 0.08821 	 0.07523 	 ~...
   93 	    89 	 0.08481 	 0.07649 	 ~...
  104 	    90 	 0.13322 	 0.07722 	 m..s
   94 	    91 	 0.08775 	 0.07888 	 ~...
   81 	    92 	 0.07100 	 0.08070 	 ~...
  101 	    93 	 0.09958 	 0.08339 	 ~...
   81 	    94 	 0.07100 	 0.08484 	 ~...
   81 	    95 	 0.07100 	 0.08836 	 ~...
   97 	    96 	 0.08873 	 0.09126 	 ~...
  103 	    97 	 0.12924 	 0.09403 	 m..s
  115 	    98 	 0.30507 	 0.09764 	 MISS
   81 	    99 	 0.07100 	 0.09815 	 ~...
   98 	   100 	 0.08904 	 0.09956 	 ~...
  108 	   101 	 0.13880 	 0.10667 	 m..s
   99 	   102 	 0.09323 	 0.11035 	 ~...
  106 	   103 	 0.13441 	 0.11289 	 ~...
   91 	   104 	 0.07624 	 0.11432 	 m..s
  100 	   105 	 0.09461 	 0.11470 	 ~...
  114 	   106 	 0.30317 	 0.11563 	 MISS
  107 	   107 	 0.13504 	 0.12157 	 ~...
  102 	   108 	 0.10706 	 0.12543 	 ~...
   90 	   109 	 0.07533 	 0.13804 	 m..s
   89 	   110 	 0.07473 	 0.13930 	 m..s
  109 	   111 	 0.15009 	 0.15553 	 ~...
  117 	   112 	 0.32209 	 0.16562 	 MISS
  112 	   113 	 0.26104 	 0.22657 	 m..s
  110 	   114 	 0.16300 	 0.22938 	 m..s
  113 	   115 	 0.28197 	 0.28448 	 ~...
  111 	   116 	 0.16593 	 0.29207 	 MISS
  116 	   117 	 0.31674 	 0.29426 	 ~...
  118 	   118 	 0.32481 	 0.31527 	 ~...
  119 	   119 	 0.35326 	 0.32350 	 ~...
  120 	   120 	 0.35544 	 0.38203 	 ~...
==========================================
r_mrr = 0.8772740960121155
r2_mrr = 0.5649229288101196
spearmanr_mrr@5 = 0.8298715353012085
spearmanr_mrr@10 = 0.8812366724014282
spearmanr_mrr@50 = 0.960850179195404
spearmanr_mrr@100 = 0.9669960141181946
spearmanr_mrr@All = 0.9683772325515747
==========================================
test time: 0.455
Done Testing dataset DBpedia50
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.401 +- 0.383
mrr vals (pred, true): 0.052, 0.044

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04937 	 0.02010 	 ~...
    0 	     1 	 0.04490 	 0.02204 	 ~...
    0 	     2 	 0.04490 	 0.02301 	 ~...
    0 	     3 	 0.04490 	 0.02370 	 ~...
    0 	     4 	 0.04490 	 0.02414 	 ~...
   83 	     5 	 0.05231 	 0.02432 	 ~...
    7 	     6 	 0.04505 	 0.02460 	 ~...
   93 	     7 	 0.06238 	 0.02482 	 m..s
    0 	     8 	 0.04490 	 0.02494 	 ~...
   87 	     9 	 0.05330 	 0.02539 	 ~...
   92 	    10 	 0.05549 	 0.02642 	 ~...
    0 	    11 	 0.04490 	 0.02676 	 ~...
   68 	    12 	 0.05187 	 0.02933 	 ~...
   23 	    13 	 0.05140 	 0.03137 	 ~...
   85 	    14 	 0.05258 	 0.03213 	 ~...
   34 	    15 	 0.05150 	 0.03440 	 ~...
   14 	    16 	 0.05115 	 0.03512 	 ~...
   70 	    17 	 0.05191 	 0.03629 	 ~...
   50 	    18 	 0.05161 	 0.03708 	 ~...
   53 	    19 	 0.05167 	 0.03811 	 ~...
   73 	    20 	 0.05195 	 0.03815 	 ~...
   74 	    21 	 0.05201 	 0.03856 	 ~...
   10 	    22 	 0.05105 	 0.03858 	 ~...
   18 	    23 	 0.05130 	 0.03942 	 ~...
   12 	    24 	 0.05107 	 0.03964 	 ~...
   81 	    25 	 0.05221 	 0.03971 	 ~...
    6 	    26 	 0.04492 	 0.03998 	 ~...
   38 	    27 	 0.05153 	 0.04091 	 ~...
   61 	    28 	 0.05175 	 0.04111 	 ~...
   11 	    29 	 0.05106 	 0.04132 	 ~...
   79 	    30 	 0.05218 	 0.04176 	 ~...
   77 	    31 	 0.05213 	 0.04177 	 ~...
   33 	    32 	 0.05148 	 0.04224 	 ~...
   45 	    33 	 0.05158 	 0.04259 	 ~...
   57 	    34 	 0.05170 	 0.04260 	 ~...
   40 	    35 	 0.05156 	 0.04261 	 ~...
   13 	    36 	 0.05107 	 0.04264 	 ~...
   28 	    37 	 0.05142 	 0.04301 	 ~...
   66 	    38 	 0.05185 	 0.04305 	 ~...
   59 	    39 	 0.05173 	 0.04330 	 ~...
   67 	    40 	 0.05185 	 0.04338 	 ~...
   35 	    41 	 0.05151 	 0.04340 	 ~...
   64 	    42 	 0.05182 	 0.04369 	 ~...
   51 	    43 	 0.05165 	 0.04373 	 ~...
   72 	    44 	 0.05194 	 0.04381 	 ~...
   52 	    45 	 0.05167 	 0.04386 	 ~...
   82 	    46 	 0.05226 	 0.04388 	 ~...
   80 	    47 	 0.05221 	 0.04395 	 ~...
   46 	    48 	 0.05158 	 0.04396 	 ~...
   22 	    49 	 0.05140 	 0.04404 	 ~...
   44 	    50 	 0.05158 	 0.04444 	 ~...
   16 	    51 	 0.05124 	 0.04456 	 ~...
   25 	    52 	 0.05142 	 0.04463 	 ~...
   29 	    53 	 0.05144 	 0.04484 	 ~...
   21 	    54 	 0.05138 	 0.04485 	 ~...
   88 	    55 	 0.05369 	 0.04493 	 ~...
   17 	    56 	 0.05128 	 0.04497 	 ~...
   54 	    57 	 0.05167 	 0.04517 	 ~...
   84 	    58 	 0.05242 	 0.04527 	 ~...
    9 	    59 	 0.05105 	 0.04533 	 ~...
   32 	    60 	 0.05148 	 0.04543 	 ~...
   48 	    61 	 0.05161 	 0.04543 	 ~...
   62 	    62 	 0.05177 	 0.04544 	 ~...
   27 	    63 	 0.05142 	 0.04574 	 ~...
   26 	    64 	 0.05142 	 0.04594 	 ~...
   42 	    65 	 0.05157 	 0.04620 	 ~...
   39 	    66 	 0.05153 	 0.04629 	 ~...
   43 	    67 	 0.05157 	 0.04652 	 ~...
   91 	    68 	 0.05470 	 0.04668 	 ~...
   41 	    69 	 0.05157 	 0.04670 	 ~...
   36 	    70 	 0.05151 	 0.04676 	 ~...
   30 	    71 	 0.05144 	 0.04718 	 ~...
   86 	    72 	 0.05258 	 0.04722 	 ~...
   47 	    73 	 0.05159 	 0.04763 	 ~...
   31 	    74 	 0.05146 	 0.04822 	 ~...
   89 	    75 	 0.05402 	 0.04866 	 ~...
   69 	    76 	 0.05187 	 0.04866 	 ~...
   37 	    77 	 0.05152 	 0.04868 	 ~...
   90 	    78 	 0.05435 	 0.04894 	 ~...
   15 	    79 	 0.05124 	 0.04900 	 ~...
   24 	    80 	 0.05141 	 0.04939 	 ~...
   71 	    81 	 0.05192 	 0.05067 	 ~...
   58 	    82 	 0.05172 	 0.05092 	 ~...
   78 	    83 	 0.05216 	 0.05160 	 ~...
   75 	    84 	 0.05209 	 0.05189 	 ~...
   49 	    85 	 0.05161 	 0.05215 	 ~...
   63 	    86 	 0.05179 	 0.05223 	 ~...
   76 	    87 	 0.05213 	 0.05233 	 ~...
   20 	    88 	 0.05137 	 0.05309 	 ~...
   60 	    89 	 0.05174 	 0.05446 	 ~...
   56 	    90 	 0.05168 	 0.05450 	 ~...
   19 	    91 	 0.05134 	 0.05501 	 ~...
   55 	    92 	 0.05167 	 0.05608 	 ~...
   65 	    93 	 0.05182 	 0.06233 	 ~...
   94 	    94 	 0.20968 	 0.17547 	 m..s
   96 	    95 	 0.21834 	 0.17571 	 m..s
   99 	    96 	 0.22596 	 0.18840 	 m..s
  102 	    97 	 0.22971 	 0.19545 	 m..s
  107 	    98 	 0.24071 	 0.20119 	 m..s
   95 	    99 	 0.21540 	 0.20499 	 ~...
   97 	   100 	 0.22060 	 0.20836 	 ~...
  116 	   101 	 0.28244 	 0.21997 	 m..s
  101 	   102 	 0.22766 	 0.22048 	 ~...
  103 	   103 	 0.23434 	 0.23731 	 ~...
  100 	   104 	 0.22703 	 0.23984 	 ~...
  104 	   105 	 0.23445 	 0.24233 	 ~...
  108 	   106 	 0.24338 	 0.24307 	 ~...
  117 	   107 	 0.28464 	 0.24480 	 m..s
  105 	   108 	 0.23603 	 0.25289 	 ~...
  106 	   109 	 0.23844 	 0.25327 	 ~...
   98 	   110 	 0.22513 	 0.25680 	 m..s
  109 	   111 	 0.24597 	 0.25751 	 ~...
  115 	   112 	 0.28143 	 0.26880 	 ~...
  110 	   113 	 0.26800 	 0.27157 	 ~...
  113 	   114 	 0.27492 	 0.27290 	 ~...
  111 	   115 	 0.27104 	 0.27476 	 ~...
  112 	   116 	 0.27191 	 0.28248 	 ~...
  114 	   117 	 0.27744 	 0.29447 	 ~...
  118 	   118 	 0.30623 	 0.29550 	 ~...
  119 	   119 	 0.32744 	 0.29788 	 ~...
  120 	   120 	 0.43297 	 0.52223 	 m..s
==========================================
r_mrr = 0.985962450504303
r2_mrr = 0.9629151821136475
spearmanr_mrr@5 = 0.9663465619087219
spearmanr_mrr@10 = 0.9705862998962402
spearmanr_mrr@50 = 0.9878625273704529
spearmanr_mrr@100 = 0.9920524954795837
spearmanr_mrr@All = 0.991363525390625
==========================================
test time: 0.504
Done Testing dataset UMLS
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.182 +- 0.000
mrr vals (pred, true): 0.051, 0.050

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   32 	     0 	 0.04848 	 0.04658 	 ~...
   44 	     1 	 0.04937 	 0.04713 	 ~...
    9 	     2 	 0.04525 	 0.04722 	 ~...
   35 	     3 	 0.04860 	 0.04739 	 ~...
    8 	     4 	 0.04511 	 0.04778 	 ~...
    2 	     5 	 0.03829 	 0.04803 	 ~...
   38 	     6 	 0.04873 	 0.04897 	 ~...
    5 	     7 	 0.04093 	 0.04903 	 ~...
   15 	     8 	 0.04626 	 0.04931 	 ~...
   13 	     9 	 0.04572 	 0.04947 	 ~...
   80 	    10 	 0.05415 	 0.04948 	 ~...
   59 	    11 	 0.05057 	 0.04950 	 ~...
   21 	    12 	 0.04735 	 0.04956 	 ~...
   77 	    13 	 0.05262 	 0.04961 	 ~...
   17 	    14 	 0.04678 	 0.04973 	 ~...
   41 	    15 	 0.04889 	 0.04990 	 ~...
   48 	    16 	 0.04967 	 0.05001 	 ~...
   62 	    17 	 0.05073 	 0.05023 	 ~...
   37 	    18 	 0.04870 	 0.05055 	 ~...
    0 	    19 	 0.03770 	 0.05056 	 ~...
   70 	    20 	 0.05175 	 0.05067 	 ~...
   65 	    21 	 0.05105 	 0.05114 	 ~...
   49 	    22 	 0.04991 	 0.05126 	 ~...
   54 	    23 	 0.05008 	 0.05127 	 ~...
   36 	    24 	 0.04864 	 0.05129 	 ~...
   23 	    25 	 0.04780 	 0.05138 	 ~...
   67 	    26 	 0.05118 	 0.05140 	 ~...
   11 	    27 	 0.04551 	 0.05161 	 ~...
   52 	    28 	 0.05000 	 0.05171 	 ~...
   64 	    29 	 0.05093 	 0.05190 	 ~...
   42 	    30 	 0.04906 	 0.05207 	 ~...
   24 	    31 	 0.04783 	 0.05221 	 ~...
   31 	    32 	 0.04838 	 0.05226 	 ~...
   12 	    33 	 0.04563 	 0.05229 	 ~...
    6 	    34 	 0.04114 	 0.05250 	 ~...
   60 	    35 	 0.05068 	 0.05258 	 ~...
   22 	    36 	 0.04769 	 0.05266 	 ~...
   57 	    37 	 0.05046 	 0.05277 	 ~...
   10 	    38 	 0.04541 	 0.05278 	 ~...
   56 	    39 	 0.05035 	 0.05286 	 ~...
   18 	    40 	 0.04692 	 0.05303 	 ~...
    3 	    41 	 0.04033 	 0.05317 	 ~...
   63 	    42 	 0.05074 	 0.05331 	 ~...
   20 	    43 	 0.04717 	 0.05340 	 ~...
   19 	    44 	 0.04711 	 0.05341 	 ~...
   74 	    45 	 0.05209 	 0.05353 	 ~...
   34 	    46 	 0.04858 	 0.05392 	 ~...
   55 	    47 	 0.05014 	 0.05422 	 ~...
   66 	    48 	 0.05108 	 0.05431 	 ~...
   25 	    49 	 0.04791 	 0.05436 	 ~...
   26 	    50 	 0.04801 	 0.05438 	 ~...
   33 	    51 	 0.04858 	 0.05440 	 ~...
   51 	    52 	 0.04998 	 0.05456 	 ~...
   78 	    53 	 0.05266 	 0.05461 	 ~...
   50 	    54 	 0.04995 	 0.05466 	 ~...
   45 	    55 	 0.04947 	 0.05469 	 ~...
   16 	    56 	 0.04627 	 0.05476 	 ~...
   79 	    57 	 0.05390 	 0.05494 	 ~...
   75 	    58 	 0.05226 	 0.05501 	 ~...
   47 	    59 	 0.04958 	 0.05510 	 ~...
   71 	    60 	 0.05194 	 0.05510 	 ~...
   40 	    61 	 0.04886 	 0.05526 	 ~...
   39 	    62 	 0.04873 	 0.05541 	 ~...
   61 	    63 	 0.05068 	 0.05547 	 ~...
   46 	    64 	 0.04948 	 0.05559 	 ~...
   14 	    65 	 0.04582 	 0.05584 	 ~...
   58 	    66 	 0.05047 	 0.05585 	 ~...
   30 	    67 	 0.04837 	 0.05596 	 ~...
   43 	    68 	 0.04922 	 0.05601 	 ~...
   73 	    69 	 0.05201 	 0.05628 	 ~...
   68 	    70 	 0.05124 	 0.05631 	 ~...
   76 	    71 	 0.05229 	 0.05673 	 ~...
   72 	    72 	 0.05199 	 0.05694 	 ~...
    4 	    73 	 0.04077 	 0.05696 	 ~...
   69 	    74 	 0.05144 	 0.05703 	 ~...
   28 	    75 	 0.04821 	 0.05796 	 ~...
    7 	    76 	 0.04406 	 0.05874 	 ~...
    1 	    77 	 0.03796 	 0.05930 	 ~...
   29 	    78 	 0.04825 	 0.05988 	 ~...
   27 	    79 	 0.04806 	 0.06012 	 ~...
   53 	    80 	 0.05001 	 0.06163 	 ~...
   88 	    81 	 0.23797 	 0.08944 	 MISS
   81 	    82 	 0.22912 	 0.20204 	 ~...
   87 	    83 	 0.23289 	 0.20266 	 m..s
   81 	    84 	 0.22912 	 0.20497 	 ~...
   81 	    85 	 0.22912 	 0.21968 	 ~...
   81 	    86 	 0.22912 	 0.23082 	 ~...
   89 	    87 	 0.25237 	 0.23297 	 ~...
   81 	    88 	 0.22912 	 0.23947 	 ~...
   81 	    89 	 0.22912 	 0.24847 	 ~...
   92 	    90 	 0.32085 	 0.26350 	 m..s
   90 	    91 	 0.28708 	 0.27057 	 ~...
   95 	    92 	 0.33171 	 0.27513 	 m..s
   93 	    93 	 0.32382 	 0.27892 	 m..s
   96 	    94 	 0.33283 	 0.28774 	 m..s
  100 	    95 	 0.34780 	 0.29157 	 m..s
  103 	    96 	 0.36076 	 0.29398 	 m..s
   98 	    97 	 0.34066 	 0.29445 	 m..s
   99 	    98 	 0.34253 	 0.30426 	 m..s
  106 	    99 	 0.38551 	 0.30449 	 m..s
  101 	   100 	 0.35387 	 0.30592 	 m..s
  107 	   101 	 0.38922 	 0.30598 	 m..s
   97 	   102 	 0.33349 	 0.30681 	 ~...
  102 	   103 	 0.35681 	 0.30945 	 m..s
   91 	   104 	 0.31401 	 0.31009 	 ~...
   94 	   105 	 0.33084 	 0.32045 	 ~...
  108 	   106 	 0.39489 	 0.32805 	 m..s
  104 	   107 	 0.37902 	 0.33351 	 m..s
  105 	   108 	 0.37968 	 0.34735 	 m..s
  116 	   109 	 0.45694 	 0.35741 	 m..s
  109 	   110 	 0.39746 	 0.36169 	 m..s
  113 	   111 	 0.41997 	 0.36283 	 m..s
  110 	   112 	 0.40261 	 0.36880 	 m..s
  112 	   113 	 0.41845 	 0.37130 	 m..s
  111 	   114 	 0.41264 	 0.37350 	 m..s
  115 	   115 	 0.44945 	 0.37821 	 m..s
  118 	   116 	 0.47068 	 0.42112 	 m..s
  114 	   117 	 0.43345 	 0.44758 	 ~...
  117 	   118 	 0.47062 	 0.44883 	 ~...
  119 	   119 	 0.55066 	 0.51145 	 m..s
  120 	   120 	 0.65153 	 0.61365 	 m..s
==========================================
r_mrr = 0.9909241795539856
r2_mrr = 0.9504286050796509
spearmanr_mrr@5 = 0.996298611164093
spearmanr_mrr@10 = 0.9836163520812988
spearmanr_mrr@50 = 0.9860896468162537
spearmanr_mrr@100 = 0.9938789010047913
spearmanr_mrr@All = 0.9944508671760559
==========================================
test time: 0.487
Done Testing dataset Kinships
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.493 +- 0.380
mrr vals (pred, true): 0.049, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   88 	     0 	 0.05896 	 0.00013 	 m..s
   40 	     1 	 0.04880 	 0.00043 	 m..s
   15 	     2 	 0.04865 	 0.00045 	 m..s
   43 	     3 	 0.04882 	 0.00049 	 m..s
   17 	     4 	 0.04866 	 0.00049 	 m..s
   10 	     5 	 0.04860 	 0.00050 	 m..s
   22 	     6 	 0.04868 	 0.00051 	 m..s
    1 	     7 	 0.04846 	 0.00051 	 m..s
    5 	     8 	 0.04849 	 0.00053 	 m..s
   14 	     9 	 0.04863 	 0.00054 	 m..s
    0 	    10 	 0.04844 	 0.00055 	 m..s
   36 	    11 	 0.04875 	 0.00055 	 m..s
   62 	    12 	 0.04899 	 0.00055 	 m..s
   16 	    13 	 0.04865 	 0.00055 	 m..s
   78 	    14 	 0.04954 	 0.00055 	 m..s
   39 	    15 	 0.04879 	 0.00056 	 m..s
   19 	    16 	 0.04866 	 0.00056 	 m..s
   58 	    17 	 0.04897 	 0.00056 	 m..s
   31 	    18 	 0.04874 	 0.00057 	 m..s
   75 	    19 	 0.04942 	 0.00057 	 m..s
   49 	    20 	 0.04887 	 0.00057 	 m..s
   27 	    21 	 0.04872 	 0.00058 	 m..s
   34 	    22 	 0.04875 	 0.00058 	 m..s
    4 	    23 	 0.04849 	 0.00058 	 m..s
    9 	    24 	 0.04858 	 0.00061 	 m..s
   52 	    25 	 0.04889 	 0.00061 	 m..s
   56 	    26 	 0.04895 	 0.00062 	 m..s
   79 	    27 	 0.05000 	 0.00062 	 m..s
   61 	    28 	 0.04898 	 0.00062 	 m..s
   70 	    29 	 0.04917 	 0.00064 	 m..s
   11 	    30 	 0.04860 	 0.00065 	 m..s
   53 	    31 	 0.04891 	 0.00066 	 m..s
   23 	    32 	 0.04869 	 0.00067 	 m..s
   20 	    33 	 0.04867 	 0.00068 	 m..s
   41 	    34 	 0.04880 	 0.00068 	 m..s
   28 	    35 	 0.04872 	 0.00070 	 m..s
    3 	    36 	 0.04848 	 0.00072 	 m..s
   12 	    37 	 0.04861 	 0.00075 	 m..s
   24 	    38 	 0.04869 	 0.00076 	 m..s
    2 	    39 	 0.04848 	 0.00077 	 m..s
   72 	    40 	 0.04925 	 0.00077 	 m..s
   29 	    41 	 0.04873 	 0.00078 	 m..s
   37 	    42 	 0.04876 	 0.00078 	 m..s
   33 	    43 	 0.04875 	 0.00079 	 m..s
   44 	    44 	 0.04883 	 0.00079 	 m..s
   32 	    45 	 0.04875 	 0.00080 	 m..s
   69 	    46 	 0.04904 	 0.00082 	 m..s
   30 	    47 	 0.04873 	 0.00084 	 m..s
   55 	    48 	 0.04894 	 0.00085 	 m..s
   74 	    49 	 0.04939 	 0.00085 	 m..s
   80 	    50 	 0.05035 	 0.00086 	 m..s
   67 	    51 	 0.04903 	 0.00087 	 m..s
    6 	    52 	 0.04853 	 0.00091 	 m..s
   81 	    53 	 0.05793 	 0.00100 	 m..s
   48 	    54 	 0.04886 	 0.00107 	 m..s
   13 	    55 	 0.04862 	 0.00114 	 m..s
   21 	    56 	 0.04867 	 0.00116 	 m..s
   59 	    57 	 0.04897 	 0.00116 	 m..s
   26 	    58 	 0.04871 	 0.00116 	 m..s
   38 	    59 	 0.04878 	 0.00122 	 m..s
   73 	    60 	 0.04934 	 0.00123 	 m..s
   66 	    61 	 0.04903 	 0.00133 	 m..s
   50 	    62 	 0.04887 	 0.00134 	 m..s
   65 	    63 	 0.04902 	 0.00134 	 m..s
   76 	    64 	 0.04943 	 0.00139 	 m..s
   46 	    65 	 0.04885 	 0.00141 	 m..s
    8 	    66 	 0.04858 	 0.00144 	 m..s
   25 	    67 	 0.04871 	 0.00153 	 m..s
   47 	    68 	 0.04886 	 0.00154 	 m..s
   63 	    69 	 0.04900 	 0.00169 	 m..s
   18 	    70 	 0.04866 	 0.00170 	 m..s
   64 	    71 	 0.04902 	 0.00171 	 m..s
   54 	    72 	 0.04893 	 0.00176 	 m..s
   51 	    73 	 0.04888 	 0.00176 	 m..s
   35 	    74 	 0.04875 	 0.00184 	 m..s
   45 	    75 	 0.04884 	 0.00189 	 m..s
   71 	    76 	 0.04923 	 0.00194 	 m..s
   57 	    77 	 0.04896 	 0.00196 	 m..s
   68 	    78 	 0.04904 	 0.00200 	 m..s
   77 	    79 	 0.04947 	 0.00210 	 m..s
   42 	    80 	 0.04880 	 0.00217 	 m..s
   60 	    81 	 0.04898 	 0.00244 	 m..s
    7 	    82 	 0.04854 	 0.00263 	 m..s
  100 	    83 	 0.08031 	 0.04540 	 m..s
   81 	    84 	 0.05793 	 0.04703 	 ~...
   81 	    85 	 0.05793 	 0.05495 	 ~...
   87 	    86 	 0.05870 	 0.06172 	 ~...
   96 	    87 	 0.07302 	 0.06381 	 ~...
   94 	    88 	 0.07223 	 0.06450 	 ~...
   97 	    89 	 0.07329 	 0.06890 	 ~...
   99 	    90 	 0.07764 	 0.06921 	 ~...
  106 	    91 	 0.10092 	 0.07066 	 m..s
  104 	    92 	 0.10020 	 0.07073 	 ~...
  103 	    93 	 0.09782 	 0.07334 	 ~...
   92 	    94 	 0.06886 	 0.07493 	 ~...
   81 	    95 	 0.05793 	 0.07528 	 ~...
   81 	    96 	 0.05793 	 0.07609 	 ~...
   81 	    97 	 0.05793 	 0.07621 	 ~...
   93 	    98 	 0.06976 	 0.07869 	 ~...
   95 	    99 	 0.07257 	 0.07945 	 ~...
  115 	   100 	 0.21289 	 0.08036 	 MISS
  108 	   101 	 0.10393 	 0.08041 	 ~...
  105 	   102 	 0.10077 	 0.08083 	 ~...
  101 	   103 	 0.08080 	 0.08565 	 ~...
   98 	   104 	 0.07665 	 0.08738 	 ~...
  102 	   105 	 0.08503 	 0.08794 	 ~...
  107 	   106 	 0.10143 	 0.09730 	 ~...
   89 	   107 	 0.05942 	 0.10479 	 m..s
  114 	   108 	 0.21118 	 0.10850 	 MISS
  109 	   109 	 0.11219 	 0.14022 	 ~...
   91 	   110 	 0.06032 	 0.14035 	 m..s
   90 	   111 	 0.05983 	 0.14792 	 m..s
  112 	   112 	 0.17432 	 0.18394 	 ~...
  110 	   113 	 0.12093 	 0.18564 	 m..s
  111 	   114 	 0.12324 	 0.21646 	 m..s
  118 	   115 	 0.25342 	 0.22656 	 ~...
  113 	   116 	 0.18536 	 0.24200 	 m..s
  120 	   117 	 0.29103 	 0.26355 	 ~...
  116 	   118 	 0.22944 	 0.27154 	 m..s
  117 	   119 	 0.25314 	 0.28885 	 m..s
  119 	   120 	 0.28792 	 0.34668 	 m..s
==========================================
r_mrr = 0.8858798146247864
r2_mrr = 0.5570724010467529
spearmanr_mrr@5 = 0.8728271126747131
spearmanr_mrr@10 = 0.9634197950363159
spearmanr_mrr@50 = 0.9682521224021912
spearmanr_mrr@100 = 0.9655399322509766
spearmanr_mrr@All = 0.965907633304596
==========================================
test time: 0.437
Done Testing dataset OpenEA
total time taken: 837.5035467147827
training time taken: 797.2896275520325
TWIG out ;))
==========================================================
----------------------------------------------------------
Running a TWIG experiment with tag: ComplEx-omit-DBpedia50
----------------------------------------------------------
==========================================================
Using random seed: 592982219284977
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [449, 259, 296, 245, 708, 1028, 739, 573, 806, 861, 244, 1131, 150, 155, 145, 1017, 815, 969, 1035, 388, 883, 69, 816, 452, 320, 888, 720, 948, 73, 1063, 1206, 1084, 907, 1008, 88, 805, 411, 41, 348, 1162, 945, 589, 107, 16, 741, 707, 1201, 1153, 448, 380, 1159, 726, 298, 994, 665, 71, 690, 464, 223, 1161, 420, 76, 778, 1139, 568, 875, 925, 242, 442, 17, 337, 849, 823, 300, 1168, 534, 836, 563, 75, 725, 853, 974, 841, 202, 435, 538, 430, 1074, 426, 1138, 271, 308, 1082, 993, 1021, 95, 1134, 370, 904, 206, 235, 1064, 79, 779, 1118, 357, 346, 1187, 74, 290, 524, 533, 237, 1077, 736, 419, 500, 691, 483, 1194, 1200]
valid_ids (0): []
train_ids (1094): [210, 345, 610, 774, 748, 87, 1184, 837, 949, 913, 418, 1112, 614, 1164, 504, 195, 645, 214, 923, 158, 144, 251, 1103, 634, 486, 233, 868, 246, 567, 55, 139, 724, 90, 569, 584, 938, 777, 82, 505, 137, 1173, 723, 1071, 801, 460, 378, 212, 336, 1170, 19, 1144, 1080, 522, 462, 344, 230, 1012, 56, 1127, 832, 655, 1049, 236, 867, 896, 485, 1128, 248, 1010, 1081, 338, 857, 496, 757, 480, 314, 706, 136, 1136, 26, 780, 148, 309, 789, 1043, 642, 36, 481, 791, 782, 1067, 618, 1175, 692, 408, 578, 1204, 40, 897, 231, 262, 472, 1111, 680, 1149, 455, 170, 91, 1091, 582, 943, 1055, 1110, 910, 848, 1181, 958, 165, 201, 456, 521, 1090, 797, 447, 620, 562, 985, 682, 898, 773, 874, 1137, 184, 322, 523, 677, 174, 367, 240, 755, 1087, 260, 842, 950, 8, 529, 77, 882, 288, 1026, 176, 674, 362, 1022, 992, 156, 261, 1116, 1120, 147, 732, 1020, 593, 312, 947, 939, 965, 459, 1002, 1007, 334, 892, 624, 268, 852, 1023, 761, 595, 932, 1056, 530, 85, 196, 482, 440, 792, 872, 256, 205, 445, 57, 1013, 98, 263, 238, 9, 840, 166, 1003, 118, 120, 303, 1122, 310, 885, 433, 478, 495, 515, 862, 763, 601, 940, 649, 710, 62, 964, 771, 508, 221, 625, 1132, 1135, 12, 656, 1151, 810, 224, 575, 44, 621, 669, 397, 1016, 851, 547, 1133, 918, 394, 793, 450, 977, 787, 1169, 23, 368, 1160, 944, 458, 1130, 354, 978, 152, 1140, 884, 425, 181, 477, 752, 1098, 914, 764, 149, 1086, 216, 809, 516, 903, 255, 901, 727, 252, 465, 347, 824, 712, 291, 1050, 839, 804, 232, 220, 468, 587, 239, 451, 125, 13, 858, 67, 657, 1079, 681, 72, 768, 1182, 105, 66, 68, 487, 403, 924, 722, 115, 18, 467, 526, 1058, 122, 846, 128, 306, 1015, 560, 927, 800, 1030, 954, 1060, 112, 735, 129, 424, 579, 329, 1048, 385, 142, 319, 689, 648, 200, 45, 197, 25, 1106, 686, 525, 1034, 894, 355, 109, 814, 2, 952, 416, 991, 183, 695, 728, 92, 951, 59, 1148, 1006, 887, 1163, 351, 553, 694, 47, 597, 227, 796, 287, 572, 1198, 100, 542, 650, 811, 531, 1000, 1033, 474, 880, 877, 491, 61, 250, 664, 492, 592, 340, 339, 550, 1157, 1032, 756, 988, 1107, 305, 820, 50, 1152, 110, 24, 225, 639, 1212, 512, 177, 361, 295, 70, 283, 133, 651, 999, 313, 405, 891, 282, 311, 444, 869, 911, 160, 762, 488, 795, 60, 599, 463, 955, 342, 860, 1199, 493, 192, 652, 519, 432, 863, 38, 937, 672, 280, 173, 182, 153, 1054, 776, 933, 1018, 507, 381, 350, 1147, 35, 359, 514, 1039, 180, 830, 879, 0, 22, 704, 404, 1040, 766, 83, 185, 34, 254, 54, 89, 1095, 513, 241, 364, 876, 1036, 881, 65, 343, 1202, 843, 596, 215, 784, 586, 981, 585, 178, 1059, 33, 986, 29, 159, 97, 27, 571, 971, 890, 1053, 52, 730, 870, 635, 161, 818, 323, 765, 930, 671, 168, 383, 864, 375, 1171, 395, 198, 847, 749, 715, 873, 273, 32, 1119, 1195, 803, 84, 926, 900, 293, 920, 612, 335, 476, 807, 441, 711, 549, 127, 1047, 1041, 701, 743, 683, 497, 631, 946, 406, 498, 43, 325, 274, 646, 103, 410, 86, 503, 194, 113, 915, 1069, 407, 1014, 169, 967, 226, 679, 865, 844, 580, 1037, 653, 535, 1044, 750, 790, 141, 1141, 675, 1031, 272, 546, 1076, 638, 566, 172, 632, 819, 229, 510, 1097, 438, 49, 1155, 733, 1208, 772, 1078, 636, 328, 124, 541, 58, 1102, 157, 798, 187, 365, 673, 422, 1051, 304, 543, 959, 794, 565, 889, 1192, 866, 1052, 996, 384, 162, 123, 358, 349, 1065, 605, 1124, 1025, 324, 983, 140, 437, 1068, 471, 1126, 402, 301, 436, 431, 1113, 581, 833, 289, 4, 1213, 1121, 1176, 591, 400, 1108, 968, 812, 1165, 501, 20, 698, 709, 321, 511, 828, 1072, 398, 716, 453, 916, 676, 330, 998, 644, 270, 53, 941, 266, 130, 555, 132, 317, 6, 265, 615, 427, 908, 1009, 559, 191, 962, 473, 253, 696, 15, 629, 285, 167, 574, 307, 203, 1004, 972, 1177, 769, 475, 643, 393, 623, 31, 637, 606, 1114, 352, 439, 545, 1156, 1210, 528, 470, 1166, 1197, 494, 721, 116, 738, 1061, 258, 1070, 602, 218, 751, 502, 982, 1092, 647, 10, 413, 279, 1100, 590, 1154, 678, 1172, 1109, 845, 1089, 893, 1038, 39, 179, 1142, 922, 1066, 737, 1073, 1178, 660, 484, 906, 126, 878, 760, 564, 617, 48, 81, 518, 1129, 257, 603, 247, 163, 186, 101, 740, 1101, 417, 1029, 813, 286, 921, 276, 1024, 713, 1214, 360, 1203, 415, 556, 1046, 222, 207, 134, 78, 961, 838, 99, 428, 228, 1115, 770, 640, 1105, 517, 102, 1191, 626, 537, 817, 995, 781, 666, 294, 987, 544, 275, 277, 1019, 1075, 714, 960, 754, 1193, 963, 423, 243, 131, 7, 548, 1088, 96, 705, 979, 461, 46, 469, 1099, 188, 912, 570, 386, 700, 552, 1158, 341, 934, 850, 509, 561, 997, 1207, 302, 1179, 719, 390, 199, 935, 1083, 28, 376, 213, 146, 744, 825, 905, 1093, 1104, 326, 667, 557, 662, 332, 209, 396, 775, 371, 208, 936, 457, 434, 729, 808, 871, 745, 540, 909, 1146, 154, 609, 953, 942, 1150, 802, 5, 668, 611, 37, 654, 583, 699, 731, 702, 297, 104, 931, 284, 822, 249, 598, 490, 94, 1190, 607, 1211, 399, 264, 670, 369, 143, 121, 975, 114, 717, 379, 454, 886, 1167, 391, 1005, 51, 1123, 164, 759, 269, 119, 1027, 281, 856, 30, 3, 753, 956, 551, 633, 854, 613, 409, 366, 93, 1188, 1057, 278, 827, 1042, 1196, 211, 1001, 42, 684, 600, 973, 1045, 989, 734, 363, 554, 382, 928, 919, 1145, 190, 11, 594, 377, 479, 788, 138, 693, 106, 917, 401, 520, 389, 630, 1209, 135, 327, 21, 783, 1174, 333, 899, 392, 966, 980, 661, 1117, 767, 1085, 1186, 234, 1189, 499, 742, 189, 628, 193, 1062, 685, 111, 1185, 835, 1, 831, 619, 539, 747, 80, 799, 443, 64, 108, 414, 785, 532, 1125, 204, 356, 506, 316, 315, 387, 786, 895, 171, 658, 688, 976, 373, 855, 1180, 758, 746, 536, 527, 826, 489, 622, 659, 372, 318, 834, 576, 117, 331, 267, 466, 902, 219, 1183, 990, 577, 446, 1011, 14, 641, 558, 718, 929, 1143, 421, 588, 429, 1205, 627, 703, 63, 616, 151, 859, 412, 299, 970, 292, 957, 353, 829, 217, 604, 608, 1094, 697, 687, 663, 175, 1096, 984, 374, 821]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6457513365134998
the save name prefix for this run is:  chkpt-ID_6457513365134998_tag_ComplEx-omit-DBpedia50
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 22
rank avg (pred): 0.505 +- 0.004
mrr vals (pred, true): 0.015, 0.189
batch losses (mrrl, rdl): 0.0, 0.0023075303

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 13
rank avg (pred): 0.191 +- 0.052
mrr vals (pred, true): 0.040, 0.219
batch losses (mrrl, rdl): 0.0, 3.16021e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1202
rank avg (pred): 0.457 +- 0.021
mrr vals (pred, true): 0.016, 0.048
batch losses (mrrl, rdl): 0.0, 9.20008e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 999
rank avg (pred): 0.457 +- 0.081
mrr vals (pred, true): 0.017, 0.044
batch losses (mrrl, rdl): 0.0, 6.46785e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 424
rank avg (pred): 0.446 +- 0.176
mrr vals (pred, true): 0.033, 0.043
batch losses (mrrl, rdl): 0.0, 2.45646e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 566
rank avg (pred): 0.379 +- 0.220
mrr vals (pred, true): 0.104, 0.023
batch losses (mrrl, rdl): 0.0, 0.0002840435

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 272
rank avg (pred): 0.210 +- 0.173
mrr vals (pred, true): 0.161, 0.252
batch losses (mrrl, rdl): 0.0, 9.4137e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 656
rank avg (pred): 0.451 +- 0.249
mrr vals (pred, true): 0.084, 0.046
batch losses (mrrl, rdl): 0.0, 1.90221e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 365
rank avg (pred): 0.459 +- 0.253
mrr vals (pred, true): 0.073, 0.049
batch losses (mrrl, rdl): 0.0, 5.1782e-06

Epoch over!
epoch time: 53.587

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 757
rank avg (pred): 0.459 +- 0.262
mrr vals (pred, true): 0.080, 0.047
batch losses (mrrl, rdl): 0.0, 3.842e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 161
rank avg (pred): 0.425 +- 0.254
mrr vals (pred, true): 0.089, 0.042
batch losses (mrrl, rdl): 0.0, 5.5147e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 208
rank avg (pred): 0.448 +- 0.246
mrr vals (pred, true): 0.078, 0.048
batch losses (mrrl, rdl): 0.0, 1.19957e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 730
rank avg (pred): 0.094 +- 0.151
mrr vals (pred, true): 0.346, 0.386
batch losses (mrrl, rdl): 0.0, 4.192e-07

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 787
rank avg (pred): 0.451 +- 0.285
mrr vals (pred, true): 0.119, 0.046
batch losses (mrrl, rdl): 0.0, 7.2395e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1165
rank avg (pred): 0.436 +- 0.262
mrr vals (pred, true): 0.098, 0.036
batch losses (mrrl, rdl): 0.0, 2.2786e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 868
rank avg (pred): 0.408 +- 0.251
mrr vals (pred, true): 0.122, 0.046
batch losses (mrrl, rdl): 0.0, 3.615e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 694
rank avg (pred): 0.432 +- 0.252
mrr vals (pred, true): 0.096, 0.045
batch losses (mrrl, rdl): 0.0, 1.8006e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 590
rank avg (pred): 0.428 +- 0.262
mrr vals (pred, true): 0.096, 0.044
batch losses (mrrl, rdl): 0.0, 4.152e-06

Epoch over!
epoch time: 55.156

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1179
rank avg (pred): 0.450 +- 0.196
mrr vals (pred, true): 0.053, 0.037
batch losses (mrrl, rdl): 0.0, 1.22219e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 27
rank avg (pred): 0.159 +- 0.182
mrr vals (pred, true): 0.259, 0.244
batch losses (mrrl, rdl): 0.0, 3.5822e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 468
rank avg (pred): 0.430 +- 0.254
mrr vals (pred, true): 0.059, 0.049
batch losses (mrrl, rdl): 0.0, 3.0905e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 585
rank avg (pred): 0.448 +- 0.268
mrr vals (pred, true): 0.081, 0.041
batch losses (mrrl, rdl): 0.0, 1.1096e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 821
rank avg (pred): 0.112 +- 0.156
mrr vals (pred, true): 0.310, 0.324
batch losses (mrrl, rdl): 0.0, 2.2688e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 943
rank avg (pred): 0.461 +- 0.279
mrr vals (pred, true): 0.085, 0.031
batch losses (mrrl, rdl): 0.0, 1.40645e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 455
rank avg (pred): 0.433 +- 0.270
mrr vals (pred, true): 0.077, 0.051
batch losses (mrrl, rdl): 0.0, 7.033e-07

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 800
rank avg (pred): 0.450 +- 0.266
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 0.0, 1.2487e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 882
rank avg (pred): 0.435 +- 0.270
mrr vals (pred, true): 0.070, 0.042
batch losses (mrrl, rdl): 0.0, 4.6951e-06

Epoch over!
epoch time: 50.52

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 283
rank avg (pred): 0.164 +- 0.188
mrr vals (pred, true): 0.307, 0.237
batch losses (mrrl, rdl): 0.0, 2.2588e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 459
rank avg (pred): 0.454 +- 0.264
mrr vals (pred, true): 0.035, 0.062
batch losses (mrrl, rdl): 0.0, 1.78715e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 126
rank avg (pred): 0.444 +- 0.274
mrr vals (pred, true): 0.094, 0.050
batch losses (mrrl, rdl): 0.0, 2.0601e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 458
rank avg (pred): 0.453 +- 0.272
mrr vals (pred, true): 0.057, 0.052
batch losses (mrrl, rdl): 0.0, 1.68102e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 484
rank avg (pred): 0.429 +- 0.277
mrr vals (pred, true): 0.065, 0.043
batch losses (mrrl, rdl): 0.0, 2.1862e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1037
rank avg (pred): 0.449 +- 0.259
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 0.0, 5.1732e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 971
rank avg (pred): 0.455 +- 0.264
mrr vals (pred, true): 0.037, 0.054
batch losses (mrrl, rdl): 0.0, 7.313e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1083
rank avg (pred): 0.449 +- 0.258
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.0, 1.3712e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 339
rank avg (pred): 0.432 +- 0.265
mrr vals (pred, true): 0.076, 0.055
batch losses (mrrl, rdl): 0.0, 4.4649e-06

Epoch over!
epoch time: 51.274

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 458
rank avg (pred): 0.453 +- 0.254
mrr vals (pred, true): 0.034, 0.052
batch losses (mrrl, rdl): 0.0, 1.36219e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 91
rank avg (pred): 0.454 +- 0.268
mrr vals (pred, true): 0.059, 0.049
batch losses (mrrl, rdl): 0.0, 1.05095e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 964
rank avg (pred): 0.462 +- 0.263
mrr vals (pred, true): 0.035, 0.045
batch losses (mrrl, rdl): 0.0, 4.7865e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 826
rank avg (pred): 0.144 +- 0.172
mrr vals (pred, true): 0.297, 0.245
batch losses (mrrl, rdl): 0.0, 9.4036e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 649
rank avg (pred): 0.450 +- 0.241
mrr vals (pred, true): 0.032, 0.045
batch losses (mrrl, rdl): 0.0, 3.1096e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 553
rank avg (pred): 0.486 +- 0.251
mrr vals (pred, true): 0.044, 0.029
batch losses (mrrl, rdl): 0.0, 7.9992e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 454
rank avg (pred): 0.430 +- 0.276
mrr vals (pred, true): 0.120, 0.047
batch losses (mrrl, rdl): 0.0, 9.862e-07

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 65
rank avg (pred): 0.181 +- 0.185
mrr vals (pred, true): 0.259, 0.217
batch losses (mrrl, rdl): 0.0, 8.3069e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 748
rank avg (pred): 0.131 +- 0.168
mrr vals (pred, true): 0.311, 0.312
batch losses (mrrl, rdl): 0.0, 4.9669e-06

Epoch over!
epoch time: 50.576

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 293
rank avg (pred): 0.158 +- 0.187
mrr vals (pred, true): 0.367, 0.214
batch losses (mrrl, rdl): 0.2347047776, 8.4716e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 616
rank avg (pred): 0.443 +- 0.274
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 2.8582e-06, 1.54177e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 28
rank avg (pred): 0.196 +- 0.260
mrr vals (pred, true): 0.262, 0.231
batch losses (mrrl, rdl): 0.0093161454, 2.64563e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 22
rank avg (pred): 0.216 +- 0.240
mrr vals (pred, true): 0.193, 0.189
batch losses (mrrl, rdl): 0.0002113783, 3.71556e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 979
rank avg (pred): 0.147 +- 0.248
mrr vals (pred, true): 0.284, 0.335
batch losses (mrrl, rdl): 0.0261032544, 9.8554e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 362
rank avg (pred): 0.404 +- 0.257
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 0.0001110059, 3.1998e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 871
rank avg (pred): 0.426 +- 0.269
mrr vals (pred, true): 0.052, 0.048
batch losses (mrrl, rdl): 2.37345e-05, 7.3654e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 984
rank avg (pred): 0.169 +- 0.284
mrr vals (pred, true): 0.308, 0.296
batch losses (mrrl, rdl): 0.0014806434, 2.92828e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 918
rank avg (pred): 0.428 +- 0.281
mrr vals (pred, true): 0.058, 0.042
batch losses (mrrl, rdl): 0.0006050221, 3.28812e-05

Epoch over!
epoch time: 54.795

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 122
rank avg (pred): 0.448 +- 0.264
mrr vals (pred, true): 0.035, 0.046
batch losses (mrrl, rdl): 0.0021150711, 2.8739e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1054
rank avg (pred): 0.137 +- 0.276
mrr vals (pred, true): 0.352, 0.275
batch losses (mrrl, rdl): 0.058920417, 3.35036e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1012
rank avg (pred): 0.455 +- 0.316
mrr vals (pred, true): 0.054, 0.040
batch losses (mrrl, rdl): 0.0001377086, 1.15785e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 581
rank avg (pred): 0.439 +- 0.297
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001941291, 1.71276e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1075
rank avg (pred): 0.180 +- 0.313
mrr vals (pred, true): 0.291, 0.273
batch losses (mrrl, rdl): 0.0031232284, 5.60839e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 378
rank avg (pred): 0.418 +- 0.285
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 4.882e-07, 1.05977e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 873
rank avg (pred): 0.467 +- 0.332
mrr vals (pred, true): 0.053, 0.056
batch losses (mrrl, rdl): 0.0001042978, 1.54421e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 802
rank avg (pred): 0.471 +- 0.334
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0003291782, 4.34044e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 705
rank avg (pred): 0.452 +- 0.321
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 0.0001138749, 1.47303e-05

Epoch over!
epoch time: 56.91

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 687
rank avg (pred): 0.426 +- 0.319
mrr vals (pred, true): 0.068, 0.053
batch losses (mrrl, rdl): 0.0034203129, 1.79224e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1087
rank avg (pred): 0.456 +- 0.323
mrr vals (pred, true): 0.052, 0.039
batch losses (mrrl, rdl): 5.77893e-05, 1.74448e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 393
rank avg (pred): 0.437 +- 0.321
mrr vals (pred, true): 0.068, 0.047
batch losses (mrrl, rdl): 0.0032781411, 1.57058e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 218
rank avg (pred): 0.399 +- 0.273
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 1.24134e-05, 4.69071e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 654
rank avg (pred): 0.462 +- 0.314
mrr vals (pred, true): 0.038, 0.045
batch losses (mrrl, rdl): 0.0013522489, 1.3902e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 416
rank avg (pred): 0.461 +- 0.319
mrr vals (pred, true): 0.044, 0.045
batch losses (mrrl, rdl): 0.0003584649, 1.13254e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 703
rank avg (pred): 0.466 +- 0.320
mrr vals (pred, true): 0.042, 0.042
batch losses (mrrl, rdl): 0.0006651112, 1.67203e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 8
rank avg (pred): 0.201 +- 0.279
mrr vals (pred, true): 0.255, 0.245
batch losses (mrrl, rdl): 0.0009054179, 4.21343e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 559
rank avg (pred): 0.523 +- 0.289
mrr vals (pred, true): 0.035, 0.026
batch losses (mrrl, rdl): 0.0022719863, 3.78626e-05

Epoch over!
epoch time: 55.31

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 157
rank avg (pred): 0.385 +- 0.248
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 9.80457e-05, 0.0001131743

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 854
rank avg (pred): 0.420 +- 0.298
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002199029, 3.83159e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 990
rank avg (pred): 0.151 +- 0.232
mrr vals (pred, true): 0.263, 0.249
batch losses (mrrl, rdl): 0.0018240266, 1.56527e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 532
rank avg (pred): 0.451 +- 0.280
mrr vals (pred, true): 0.047, 0.025
batch losses (mrrl, rdl): 8.91704e-05, 6.53384e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 32
rank avg (pred): 0.251 +- 0.270
mrr vals (pred, true): 0.184, 0.232
batch losses (mrrl, rdl): 0.022472471, 0.0001238621

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 15
rank avg (pred): 0.212 +- 0.241
mrr vals (pred, true): 0.241, 0.208
batch losses (mrrl, rdl): 0.0107309036, 8.7075e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 558
rank avg (pred): 0.415 +- 0.266
mrr vals (pred, true): 0.059, 0.025
batch losses (mrrl, rdl): 0.0007758135, 0.0001289954

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 922
rank avg (pred): 0.457 +- 0.322
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0014379119, 4.34548e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 407
rank avg (pred): 0.410 +- 0.265
mrr vals (pred, true): 0.043, 0.050
batch losses (mrrl, rdl): 0.0005558067, 2.92718e-05

Epoch over!
epoch time: 55.403

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 893
rank avg (pred): 0.513 +- 0.317
mrr vals (pred, true): 0.080, 0.022
batch losses (mrrl, rdl): 0.0091314334, 0.0001774194

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 22
rank avg (pred): 0.240 +- 0.257
mrr vals (pred, true): 0.213, 0.189
batch losses (mrrl, rdl): 0.0060131196, 7.56572e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.456 +- 0.320
mrr vals (pred, true): 0.061, 0.052
batch losses (mrrl, rdl): 0.0011515629, 1.55653e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 722
rank avg (pred): 0.419 +- 0.291
mrr vals (pred, true): 0.052, 0.051
batch losses (mrrl, rdl): 2.27828e-05, 8.4251e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 608
rank avg (pred): 0.394 +- 0.266
mrr vals (pred, true): 0.054, 0.037
batch losses (mrrl, rdl): 0.0001710708, 0.0001271335

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 470
rank avg (pred): 0.439 +- 0.299
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 6.926e-07, 7.8152e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 605
rank avg (pred): 0.434 +- 0.294
mrr vals (pred, true): 0.047, 0.039
batch losses (mrrl, rdl): 9.78492e-05, 3.27304e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1054
rank avg (pred): 0.212 +- 0.256
mrr vals (pred, true): 0.223, 0.275
batch losses (mrrl, rdl): 0.0271612313, 9.14831e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 649
rank avg (pred): 0.449 +- 0.284
mrr vals (pred, true): 0.044, 0.045
batch losses (mrrl, rdl): 0.0003144506, 1.9073e-06

Epoch over!
epoch time: 55.47

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 50
rank avg (pred): 0.229 +- 0.246
mrr vals (pred, true): 0.192, 0.220
batch losses (mrrl, rdl): 0.0073613189, 2.76767e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 689
rank avg (pred): 0.386 +- 0.286
mrr vals (pred, true): 0.064, 0.055
batch losses (mrrl, rdl): 0.0018826606, 5.96173e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 905
rank avg (pred): 0.438 +- 0.349
mrr vals (pred, true): 0.050, 0.018
batch losses (mrrl, rdl): 5.29e-07, 0.0006940946

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 25
rank avg (pred): 0.203 +- 0.242
mrr vals (pred, true): 0.223, 0.199
batch losses (mrrl, rdl): 0.0056752507, 5.0329e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1136
rank avg (pred): 0.427 +- 0.274
mrr vals (pred, true): 0.039, 0.026
batch losses (mrrl, rdl): 0.0011558668, 0.0001017968

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 909
rank avg (pred): 0.466 +- 0.370
mrr vals (pred, true): 0.063, 0.023
batch losses (mrrl, rdl): 0.0018182944, 0.0003747299

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 7
rank avg (pred): 0.206 +- 0.219
mrr vals (pred, true): 0.205, 0.243
batch losses (mrrl, rdl): 0.013910437, 4.37595e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 454
rank avg (pred): 0.413 +- 0.284
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001736207, 1.68855e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 433
rank avg (pred): 0.428 +- 0.287
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 5.21122e-05, 1.19234e-05

Epoch over!
epoch time: 54.538

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 797
rank avg (pred): 0.423 +- 0.276
mrr vals (pred, true): 0.047, 0.052
batch losses (mrrl, rdl): 7.04234e-05, 9.795e-07

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 784
rank avg (pred): 0.390 +- 0.256
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 7.58558e-05, 5.15454e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 467
rank avg (pred): 0.388 +- 0.253
mrr vals (pred, true): 0.038, 0.048
batch losses (mrrl, rdl): 0.0013457017, 7.25736e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 675
rank avg (pred): 0.429 +- 0.280
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 7.55641e-05, 7.7377e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 112
rank avg (pred): 0.403 +- 0.275
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 0.0001083701, 1.5761e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 916
rank avg (pred): 0.426 +- 0.308
mrr vals (pred, true): 0.040, 0.022
batch losses (mrrl, rdl): 0.0009889668, 0.0006382992

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 147
rank avg (pred): 0.481 +- 0.298
mrr vals (pred, true): 0.046, 0.037
batch losses (mrrl, rdl): 0.0001584437, 1.1126e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 587
rank avg (pred): 0.479 +- 0.296
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.0001240804, 1.36025e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 28
rank avg (pred): 0.177 +- 0.241
mrr vals (pred, true): 0.271, 0.231
batch losses (mrrl, rdl): 0.0155284591, 6.7177e-06

Epoch over!
epoch time: 56.047

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 384
rank avg (pred): 0.466 +- 0.303
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 1.44923e-05, 1.4064e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1137
rank avg (pred): 0.450 +- 0.346
mrr vals (pred, true): 0.063, 0.027
batch losses (mrrl, rdl): 0.0017693831, 0.0001430897

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 557
rank avg (pred): 0.380 +- 0.306
mrr vals (pred, true): 0.051, 0.023
batch losses (mrrl, rdl): 4.0266e-06, 0.0004723447

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 995
rank avg (pred): 0.123 +- 0.233
mrr vals (pred, true): 0.329, 0.289
batch losses (mrrl, rdl): 0.0161779635, 5.31249e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 930
rank avg (pred): 0.423 +- 0.269
mrr vals (pred, true): 0.047, 0.035
batch losses (mrrl, rdl): 9.8011e-05, 5.31539e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 935
rank avg (pred): 0.465 +- 0.295
mrr vals (pred, true): 0.047, 0.027
batch losses (mrrl, rdl): 7.06274e-05, 5.06232e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 617
rank avg (pred): 0.460 +- 0.287
mrr vals (pred, true): 0.045, 0.035
batch losses (mrrl, rdl): 0.0002254702, 3.5979e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1052
rank avg (pred): 0.418 +- 0.281
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0001392231, 1.75866e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1124
rank avg (pred): 0.472 +- 0.299
mrr vals (pred, true): 0.049, 0.053
batch losses (mrrl, rdl): 2.1775e-05, 3.24607e-05

Epoch over!
epoch time: 54.444

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 446
rank avg (pred): 0.440 +- 0.281
mrr vals (pred, true): 0.043, 0.046
batch losses (mrrl, rdl): 0.0004229173, 3.8353e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 139
rank avg (pred): 0.426 +- 0.280
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 7.2184e-06, 7.4094e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1040
rank avg (pred): 0.487 +- 0.304
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 5.14779e-05, 6.62853e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 832
rank avg (pred): 0.138 +- 0.223
mrr vals (pred, true): 0.270, 0.302
batch losses (mrrl, rdl): 0.0102216052, 8.532e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 918
rank avg (pred): 0.461 +- 0.281
mrr vals (pred, true): 0.053, 0.042
batch losses (mrrl, rdl): 7.8649e-05, 4.1688e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 957
rank avg (pred): 0.370 +- 0.240
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001508336, 0.0001764067

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 852
rank avg (pred): 0.446 +- 0.282
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.0002041561, 2.2682e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 272
rank avg (pred): 0.124 +- 0.195
mrr vals (pred, true): 0.251, 0.252
batch losses (mrrl, rdl): 7.1284e-06, 2.85288e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 680
rank avg (pred): 0.471 +- 0.304
mrr vals (pred, true): 0.048, 0.056
batch losses (mrrl, rdl): 2.33141e-05, 3.43658e-05

Epoch over!
epoch time: 56.182

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 744
rank avg (pred): 0.152 +- 0.209
mrr vals (pred, true): 0.234, 0.231
batch losses (mrrl, rdl): 8.439e-05, 2.29309e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 902
rank avg (pred): 0.520 +- 0.352
mrr vals (pred, true): 0.049, 0.028
batch losses (mrrl, rdl): 4.6416e-06, 0.0002055489

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 288
rank avg (pred): 0.165 +- 0.225
mrr vals (pred, true): 0.217, 0.206
batch losses (mrrl, rdl): 0.0012291663, 4.92108e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 15
rank avg (pred): 0.148 +- 0.216
mrr vals (pred, true): 0.235, 0.208
batch losses (mrrl, rdl): 0.007294931, 5.25591e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 110
rank avg (pred): 0.413 +- 0.294
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 3.07e-08, 1.18966e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 876
rank avg (pred): 0.426 +- 0.284
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002766405, 4.7666e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 432
rank avg (pred): 0.370 +- 0.280
mrr vals (pred, true): 0.045, 0.046
batch losses (mrrl, rdl): 0.0002649954, 0.0001637439

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 559
rank avg (pred): 0.479 +- 0.387
mrr vals (pred, true): 0.052, 0.026
batch losses (mrrl, rdl): 4.75746e-05, 0.0001995583

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 793
rank avg (pred): 0.479 +- 0.333
mrr vals (pred, true): 0.045, 0.041
batch losses (mrrl, rdl): 0.000286387, 3.19208e-05

Epoch over!
epoch time: 55.509

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.493 +- 0.320
mrr vals (pred, true): 0.048, 0.048

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   71 	     0 	 0.05229 	 0.02037 	 m..s
   71 	     1 	 0.05229 	 0.02144 	 m..s
   78 	     2 	 0.05231 	 0.02204 	 m..s
   71 	     3 	 0.05229 	 0.02291 	 ~...
   82 	     4 	 0.05318 	 0.02292 	 m..s
   80 	     5 	 0.05279 	 0.02311 	 ~...
   71 	     6 	 0.05229 	 0.02328 	 ~...
   71 	     7 	 0.05229 	 0.02414 	 ~...
   86 	     8 	 0.05948 	 0.02482 	 m..s
   71 	     9 	 0.05229 	 0.02536 	 ~...
   71 	    10 	 0.05229 	 0.02626 	 ~...
   84 	    11 	 0.05356 	 0.02642 	 ~...
   85 	    12 	 0.05460 	 0.02849 	 ~...
    4 	    13 	 0.04345 	 0.02966 	 ~...
   49 	    14 	 0.04950 	 0.03237 	 ~...
   68 	    15 	 0.05191 	 0.03358 	 ~...
   11 	    16 	 0.04458 	 0.03572 	 ~...
    9 	    17 	 0.04431 	 0.03608 	 ~...
   59 	    18 	 0.05012 	 0.03732 	 ~...
   12 	    19 	 0.04467 	 0.03815 	 ~...
    1 	    20 	 0.04316 	 0.03980 	 ~...
    8 	    21 	 0.04412 	 0.04004 	 ~...
   23 	    22 	 0.04584 	 0.04009 	 ~...
   38 	    23 	 0.04813 	 0.04019 	 ~...
   64 	    24 	 0.05100 	 0.04101 	 ~...
   47 	    25 	 0.04888 	 0.04109 	 ~...
    7 	    26 	 0.04411 	 0.04118 	 ~...
   33 	    27 	 0.04749 	 0.04174 	 ~...
   29 	    28 	 0.04725 	 0.04174 	 ~...
    0 	    29 	 0.04024 	 0.04176 	 ~...
   45 	    30 	 0.04885 	 0.04179 	 ~...
   63 	    31 	 0.05096 	 0.04221 	 ~...
    2 	    32 	 0.04328 	 0.04255 	 ~...
   81 	    33 	 0.05295 	 0.04257 	 ~...
   48 	    34 	 0.04894 	 0.04287 	 ~...
   13 	    35 	 0.04483 	 0.04313 	 ~...
   70 	    36 	 0.05226 	 0.04343 	 ~...
   43 	    37 	 0.04867 	 0.04357 	 ~...
   61 	    38 	 0.05047 	 0.04372 	 ~...
   34 	    39 	 0.04771 	 0.04381 	 ~...
   60 	    40 	 0.05030 	 0.04393 	 ~...
   65 	    41 	 0.05106 	 0.04410 	 ~...
    6 	    42 	 0.04407 	 0.04410 	 ~...
   16 	    43 	 0.04495 	 0.04419 	 ~...
   54 	    44 	 0.04993 	 0.04431 	 ~...
   24 	    45 	 0.04619 	 0.04477 	 ~...
   22 	    46 	 0.04574 	 0.04480 	 ~...
   62 	    47 	 0.05049 	 0.04514 	 ~...
   53 	    48 	 0.04991 	 0.04525 	 ~...
   56 	    49 	 0.05004 	 0.04559 	 ~...
   36 	    50 	 0.04791 	 0.04594 	 ~...
   30 	    51 	 0.04737 	 0.04639 	 ~...
   35 	    52 	 0.04771 	 0.04646 	 ~...
   37 	    53 	 0.04808 	 0.04658 	 ~...
    3 	    54 	 0.04336 	 0.04669 	 ~...
   21 	    55 	 0.04567 	 0.04679 	 ~...
   20 	    56 	 0.04558 	 0.04695 	 ~...
   25 	    57 	 0.04639 	 0.04696 	 ~...
   51 	    58 	 0.04979 	 0.04699 	 ~...
   10 	    59 	 0.04434 	 0.04719 	 ~...
   14 	    60 	 0.04490 	 0.04719 	 ~...
   79 	    61 	 0.05243 	 0.04736 	 ~...
   19 	    62 	 0.04550 	 0.04744 	 ~...
   57 	    63 	 0.05011 	 0.04747 	 ~...
   83 	    64 	 0.05318 	 0.04750 	 ~...
   40 	    65 	 0.04848 	 0.04758 	 ~...
   18 	    66 	 0.04513 	 0.04759 	 ~...
   66 	    67 	 0.05119 	 0.04760 	 ~...
   50 	    68 	 0.04974 	 0.04781 	 ~...
   41 	    69 	 0.04849 	 0.04820 	 ~...
   69 	    70 	 0.05204 	 0.04827 	 ~...
   26 	    71 	 0.04643 	 0.04876 	 ~...
   15 	    72 	 0.04491 	 0.04895 	 ~...
   31 	    73 	 0.04747 	 0.04951 	 ~...
   67 	    74 	 0.05126 	 0.04952 	 ~...
   39 	    75 	 0.04836 	 0.04977 	 ~...
   32 	    76 	 0.04748 	 0.04984 	 ~...
   17 	    77 	 0.04507 	 0.05067 	 ~...
   28 	    78 	 0.04713 	 0.05085 	 ~...
   52 	    79 	 0.04988 	 0.05135 	 ~...
   55 	    80 	 0.05002 	 0.05213 	 ~...
   44 	    81 	 0.04880 	 0.05340 	 ~...
   46 	    82 	 0.04887 	 0.05367 	 ~...
   58 	    83 	 0.05011 	 0.05491 	 ~...
   42 	    84 	 0.04852 	 0.05546 	 ~...
    5 	    85 	 0.04350 	 0.05549 	 ~...
   27 	    86 	 0.04694 	 0.05611 	 ~...
  102 	    87 	 0.27145 	 0.18352 	 m..s
   93 	    88 	 0.23767 	 0.19712 	 m..s
   90 	    89 	 0.22676 	 0.20277 	 ~...
   92 	    90 	 0.23756 	 0.20295 	 m..s
   87 	    91 	 0.22191 	 0.20603 	 ~...
   95 	    92 	 0.24838 	 0.20936 	 m..s
  104 	    93 	 0.28208 	 0.21318 	 m..s
  103 	    94 	 0.27352 	 0.21393 	 m..s
   94 	    95 	 0.23836 	 0.21474 	 ~...
   88 	    96 	 0.22603 	 0.21637 	 ~...
  109 	    97 	 0.30666 	 0.21777 	 m..s
   89 	    98 	 0.22622 	 0.21883 	 ~...
  100 	    99 	 0.26563 	 0.22399 	 m..s
   91 	   100 	 0.23565 	 0.22717 	 ~...
  101 	   101 	 0.26727 	 0.22787 	 m..s
   99 	   102 	 0.26541 	 0.24360 	 ~...
   96 	   103 	 0.25619 	 0.24680 	 ~...
   97 	   104 	 0.26184 	 0.24955 	 ~...
   98 	   105 	 0.26197 	 0.25289 	 ~...
  116 	   106 	 0.32252 	 0.26234 	 m..s
  119 	   107 	 0.36062 	 0.26769 	 m..s
  105 	   108 	 0.28416 	 0.27224 	 ~...
  118 	   109 	 0.35745 	 0.27282 	 m..s
  107 	   110 	 0.29034 	 0.27425 	 ~...
  114 	   111 	 0.32082 	 0.27476 	 m..s
  112 	   112 	 0.31894 	 0.27497 	 m..s
  110 	   113 	 0.31312 	 0.27773 	 m..s
  106 	   114 	 0.28425 	 0.28106 	 ~...
  113 	   115 	 0.31983 	 0.28800 	 m..s
  115 	   116 	 0.32100 	 0.28917 	 m..s
  108 	   117 	 0.30436 	 0.29247 	 ~...
  117 	   118 	 0.33542 	 0.29763 	 m..s
  111 	   119 	 0.31596 	 0.32107 	 ~...
  120 	   120 	 0.53921 	 0.53924 	 ~...
==========================================
r_mrr = 0.9862105250358582
r2_mrr = 0.937877357006073
spearmanr_mrr@5 = 0.99358731508255
spearmanr_mrr@10 = 0.9932202696800232
spearmanr_mrr@50 = 0.9947635531425476
spearmanr_mrr@100 = 0.9972506761550903
spearmanr_mrr@All = 0.9963847994804382
==========================================
test time: 0.473
Done Testing dataset UMLS
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.490 +- 0.329
mrr vals (pred, true): 0.045, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   79 	     0 	 0.04767 	 0.00088 	 m..s
   74 	     1 	 0.04694 	 0.00102 	 m..s
    8 	     2 	 0.04308 	 0.00109 	 m..s
   23 	     3 	 0.04400 	 0.00222 	 m..s
    5 	     4 	 0.04296 	 0.00261 	 m..s
   20 	     5 	 0.04383 	 0.00264 	 m..s
    6 	     6 	 0.04304 	 0.00274 	 m..s
   50 	     7 	 0.04580 	 0.00288 	 m..s
   55 	     8 	 0.04597 	 0.00294 	 m..s
   21 	     9 	 0.04387 	 0.00300 	 m..s
   39 	    10 	 0.04506 	 0.00301 	 m..s
   56 	    11 	 0.04598 	 0.00310 	 m..s
   24 	    12 	 0.04404 	 0.00311 	 m..s
   15 	    13 	 0.04361 	 0.00312 	 m..s
   75 	    14 	 0.04699 	 0.00313 	 m..s
   77 	    15 	 0.04701 	 0.00324 	 m..s
   58 	    16 	 0.04608 	 0.00331 	 m..s
   62 	    17 	 0.04624 	 0.00343 	 m..s
   28 	    18 	 0.04459 	 0.00343 	 m..s
   37 	    19 	 0.04497 	 0.00348 	 m..s
   17 	    20 	 0.04371 	 0.00354 	 m..s
   41 	    21 	 0.04520 	 0.00357 	 m..s
   72 	    22 	 0.04691 	 0.00366 	 m..s
   68 	    23 	 0.04660 	 0.00367 	 m..s
   71 	    24 	 0.04690 	 0.00367 	 m..s
   59 	    25 	 0.04612 	 0.00371 	 m..s
   13 	    26 	 0.04358 	 0.00377 	 m..s
   53 	    27 	 0.04589 	 0.00378 	 m..s
   67 	    28 	 0.04657 	 0.00379 	 m..s
   10 	    29 	 0.04323 	 0.00381 	 m..s
   65 	    30 	 0.04650 	 0.00381 	 m..s
   47 	    31 	 0.04571 	 0.00381 	 m..s
   31 	    32 	 0.04474 	 0.00381 	 m..s
   60 	    33 	 0.04617 	 0.00381 	 m..s
   34 	    34 	 0.04482 	 0.00386 	 m..s
   54 	    35 	 0.04596 	 0.00387 	 m..s
   40 	    36 	 0.04506 	 0.00391 	 m..s
   61 	    37 	 0.04624 	 0.00395 	 m..s
   51 	    38 	 0.04580 	 0.00396 	 m..s
   78 	    39 	 0.04705 	 0.00399 	 m..s
   22 	    40 	 0.04395 	 0.00402 	 m..s
   57 	    41 	 0.04605 	 0.00402 	 m..s
   46 	    42 	 0.04547 	 0.00405 	 m..s
   44 	    43 	 0.04534 	 0.00406 	 m..s
   29 	    44 	 0.04464 	 0.00414 	 m..s
   32 	    45 	 0.04478 	 0.00415 	 m..s
    0 	    46 	 0.04101 	 0.00415 	 m..s
    4 	    47 	 0.04293 	 0.00417 	 m..s
   26 	    48 	 0.04428 	 0.00417 	 m..s
    2 	    49 	 0.04255 	 0.00418 	 m..s
   12 	    50 	 0.04352 	 0.00420 	 m..s
    1 	    51 	 0.04254 	 0.00421 	 m..s
   70 	    52 	 0.04671 	 0.00421 	 m..s
   25 	    53 	 0.04415 	 0.00422 	 m..s
   30 	    54 	 0.04472 	 0.00423 	 m..s
    9 	    55 	 0.04319 	 0.00426 	 m..s
   45 	    56 	 0.04543 	 0.00428 	 m..s
    7 	    57 	 0.04306 	 0.00442 	 m..s
   69 	    58 	 0.04664 	 0.00443 	 m..s
   33 	    59 	 0.04479 	 0.00445 	 m..s
   49 	    60 	 0.04576 	 0.00453 	 m..s
   35 	    61 	 0.04491 	 0.00454 	 m..s
   76 	    62 	 0.04699 	 0.00454 	 m..s
   42 	    63 	 0.04520 	 0.00463 	 m..s
   14 	    64 	 0.04360 	 0.00467 	 m..s
   66 	    65 	 0.04656 	 0.00467 	 m..s
   16 	    66 	 0.04370 	 0.00468 	 m..s
    3 	    67 	 0.04289 	 0.00470 	 m..s
   11 	    68 	 0.04343 	 0.00478 	 m..s
   27 	    69 	 0.04430 	 0.00492 	 m..s
   38 	    70 	 0.04497 	 0.00502 	 m..s
   43 	    71 	 0.04531 	 0.00508 	 m..s
   36 	    72 	 0.04496 	 0.00519 	 m..s
   48 	    73 	 0.04575 	 0.00525 	 m..s
   63 	    74 	 0.04627 	 0.00545 	 m..s
   19 	    75 	 0.04381 	 0.00597 	 m..s
   79 	    76 	 0.04767 	 0.00602 	 m..s
   79 	    77 	 0.04767 	 0.00604 	 m..s
   79 	    78 	 0.04767 	 0.00680 	 m..s
   79 	    79 	 0.04767 	 0.00780 	 m..s
   79 	    80 	 0.04767 	 0.00895 	 m..s
   79 	    81 	 0.04767 	 0.02017 	 ~...
   18 	    82 	 0.04377 	 0.02091 	 ~...
   52 	    83 	 0.04586 	 0.02355 	 ~...
   64 	    84 	 0.04645 	 0.02856 	 ~...
   73 	    85 	 0.04691 	 0.02942 	 ~...
   86 	    86 	 0.04984 	 0.03090 	 ~...
  119 	    87 	 0.26447 	 0.03527 	 MISS
  118 	    88 	 0.26404 	 0.06382 	 MISS
   88 	    89 	 0.14485 	 0.12484 	 ~...
   87 	    90 	 0.14111 	 0.12634 	 ~...
  101 	    91 	 0.15740 	 0.13055 	 ~...
   96 	    92 	 0.15147 	 0.13437 	 ~...
   90 	    93 	 0.14769 	 0.13628 	 ~...
   94 	    94 	 0.15029 	 0.13765 	 ~...
   98 	    95 	 0.15493 	 0.13846 	 ~...
   95 	    96 	 0.15096 	 0.14295 	 ~...
   93 	    97 	 0.14972 	 0.17766 	 ~...
   89 	    98 	 0.14595 	 0.17992 	 m..s
  104 	    99 	 0.16171 	 0.18464 	 ~...
   99 	   100 	 0.15650 	 0.18472 	 ~...
   92 	   101 	 0.14937 	 0.18489 	 m..s
   91 	   102 	 0.14896 	 0.19023 	 m..s
  107 	   103 	 0.22359 	 0.19063 	 m..s
  100 	   104 	 0.15727 	 0.19385 	 m..s
   97 	   105 	 0.15278 	 0.19484 	 m..s
  102 	   106 	 0.16127 	 0.19944 	 m..s
  109 	   107 	 0.23094 	 0.20998 	 ~...
  117 	   108 	 0.25300 	 0.21604 	 m..s
  113 	   109 	 0.24938 	 0.23376 	 ~...
  106 	   110 	 0.16474 	 0.23671 	 m..s
  103 	   111 	 0.16149 	 0.25176 	 m..s
  105 	   112 	 0.16301 	 0.25840 	 m..s
  111 	   113 	 0.24869 	 0.26016 	 ~...
  108 	   114 	 0.22966 	 0.26886 	 m..s
  110 	   115 	 0.24478 	 0.29103 	 m..s
  112 	   116 	 0.24927 	 0.29612 	 m..s
  120 	   117 	 0.32588 	 0.29618 	 ~...
  116 	   118 	 0.25230 	 0.29833 	 m..s
  115 	   119 	 0.25192 	 0.30562 	 m..s
  114 	   120 	 0.25003 	 0.30632 	 m..s
==========================================
r_mrr = 0.9169087409973145
r2_mrr = 0.735309362411499
spearmanr_mrr@5 = 0.7402285933494568
spearmanr_mrr@10 = 0.5454564690589905
spearmanr_mrr@50 = 0.9660531282424927
spearmanr_mrr@100 = 0.981285810470581
spearmanr_mrr@All = 0.9827315211296082
==========================================
test time: 0.436
Done Testing dataset CoDExSmall
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.447 +- 0.323
mrr vals (pred, true): 0.042, 0.051

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   65 	     0 	 0.04332 	 0.04420 	 ~...
   26 	     1 	 0.04217 	 0.04721 	 ~...
   47 	     2 	 0.04278 	 0.04763 	 ~...
    9 	     3 	 0.04185 	 0.04772 	 ~...
   12 	     4 	 0.04193 	 0.04772 	 ~...
   45 	     5 	 0.04257 	 0.04799 	 ~...
   19 	     6 	 0.04201 	 0.04891 	 ~...
   32 	     7 	 0.04227 	 0.04961 	 ~...
   38 	     8 	 0.04238 	 0.05016 	 ~...
   55 	     9 	 0.04296 	 0.05035 	 ~...
   16 	    10 	 0.04197 	 0.05038 	 ~...
   59 	    11 	 0.04321 	 0.05053 	 ~...
   64 	    12 	 0.04330 	 0.05062 	 ~...
   57 	    13 	 0.04303 	 0.05071 	 ~...
   63 	    14 	 0.04328 	 0.05102 	 ~...
    8 	    15 	 0.04183 	 0.05102 	 ~...
   36 	    16 	 0.04233 	 0.05103 	 ~...
   14 	    17 	 0.04194 	 0.05121 	 ~...
   62 	    18 	 0.04328 	 0.05121 	 ~...
   24 	    19 	 0.04210 	 0.05122 	 ~...
   67 	    20 	 0.04361 	 0.05123 	 ~...
   39 	    21 	 0.04245 	 0.05135 	 ~...
   31 	    22 	 0.04224 	 0.05142 	 ~...
   35 	    23 	 0.04228 	 0.05158 	 ~...
   68 	    24 	 0.04377 	 0.05168 	 ~...
   23 	    25 	 0.04208 	 0.05179 	 ~...
   69 	    26 	 0.04383 	 0.05179 	 ~...
   56 	    27 	 0.04297 	 0.05203 	 ~...
   51 	    28 	 0.04285 	 0.05207 	 ~...
    1 	    29 	 0.04160 	 0.05212 	 ~...
   37 	    30 	 0.04236 	 0.05225 	 ~...
   54 	    31 	 0.04289 	 0.05226 	 ~...
   66 	    32 	 0.04335 	 0.05236 	 ~...
   70 	    33 	 0.04386 	 0.05239 	 ~...
   46 	    34 	 0.04263 	 0.05239 	 ~...
    2 	    35 	 0.04161 	 0.05269 	 ~...
   21 	    36 	 0.04206 	 0.05291 	 ~...
   22 	    37 	 0.04207 	 0.05302 	 ~...
   33 	    38 	 0.04227 	 0.05311 	 ~...
   60 	    39 	 0.04321 	 0.05314 	 ~...
   52 	    40 	 0.04286 	 0.05343 	 ~...
   25 	    41 	 0.04213 	 0.05370 	 ~...
   30 	    42 	 0.04224 	 0.05374 	 ~...
   20 	    43 	 0.04204 	 0.05396 	 ~...
   61 	    44 	 0.04326 	 0.05423 	 ~...
   71 	    45 	 0.04390 	 0.05447 	 ~...
   44 	    46 	 0.04257 	 0.05456 	 ~...
    6 	    47 	 0.04176 	 0.05461 	 ~...
   29 	    48 	 0.04220 	 0.05493 	 ~...
    7 	    49 	 0.04180 	 0.05495 	 ~...
   15 	    50 	 0.04195 	 0.05501 	 ~...
    4 	    51 	 0.04168 	 0.05510 	 ~...
   40 	    52 	 0.04246 	 0.05523 	 ~...
   43 	    53 	 0.04255 	 0.05529 	 ~...
   18 	    54 	 0.04199 	 0.05540 	 ~...
   28 	    55 	 0.04220 	 0.05550 	 ~...
   17 	    56 	 0.04197 	 0.05568 	 ~...
   42 	    57 	 0.04252 	 0.05626 	 ~...
    5 	    58 	 0.04169 	 0.05632 	 ~...
   48 	    59 	 0.04278 	 0.05654 	 ~...
   72 	    60 	 0.04400 	 0.05680 	 ~...
   53 	    61 	 0.04289 	 0.05689 	 ~...
   13 	    62 	 0.04194 	 0.05725 	 ~...
    0 	    63 	 0.04146 	 0.05735 	 ~...
   41 	    64 	 0.04248 	 0.05743 	 ~...
    3 	    65 	 0.04166 	 0.05755 	 ~...
   34 	    66 	 0.04227 	 0.05770 	 ~...
   27 	    67 	 0.04218 	 0.05782 	 ~...
   11 	    68 	 0.04189 	 0.05816 	 ~...
   73 	    69 	 0.04410 	 0.05881 	 ~...
   49 	    70 	 0.04281 	 0.05881 	 ~...
   10 	    71 	 0.04185 	 0.05993 	 ~...
   58 	    72 	 0.04306 	 0.06014 	 ~...
   50 	    73 	 0.04282 	 0.06049 	 ~...
  109 	    74 	 0.34050 	 0.15938 	 MISS
  108 	    75 	 0.33736 	 0.16239 	 MISS
   74 	    76 	 0.19343 	 0.16879 	 ~...
   74 	    77 	 0.19343 	 0.19508 	 ~...
   74 	    78 	 0.19343 	 0.19886 	 ~...
   74 	    79 	 0.19343 	 0.20018 	 ~...
   74 	    80 	 0.19343 	 0.20447 	 ~...
   81 	    81 	 0.20772 	 0.21968 	 ~...
   74 	    82 	 0.19343 	 0.22330 	 ~...
   74 	    83 	 0.19343 	 0.25103 	 m..s
   88 	    84 	 0.23058 	 0.25409 	 ~...
   94 	    85 	 0.26118 	 0.25895 	 ~...
   82 	    86 	 0.22028 	 0.26316 	 m..s
   83 	    87 	 0.22655 	 0.26725 	 m..s
   95 	    88 	 0.27608 	 0.27241 	 ~...
   87 	    89 	 0.22927 	 0.27975 	 m..s
   90 	    90 	 0.25240 	 0.28764 	 m..s
   91 	    91 	 0.25270 	 0.28897 	 m..s
   86 	    92 	 0.22883 	 0.28999 	 m..s
   93 	    93 	 0.26074 	 0.29398 	 m..s
   92 	    94 	 0.25422 	 0.29725 	 m..s
   84 	    95 	 0.22663 	 0.29813 	 m..s
   89 	    96 	 0.24859 	 0.30022 	 m..s
  102 	    97 	 0.30691 	 0.30284 	 ~...
   98 	    98 	 0.29977 	 0.30426 	 ~...
  103 	    99 	 0.30878 	 0.30574 	 ~...
   99 	   100 	 0.30260 	 0.30588 	 ~...
  100 	   101 	 0.30568 	 0.30765 	 ~...
   85 	   102 	 0.22695 	 0.31009 	 m..s
   97 	   103 	 0.29961 	 0.31691 	 ~...
  106 	   104 	 0.32658 	 0.32216 	 ~...
   96 	   105 	 0.28979 	 0.32474 	 m..s
  101 	   106 	 0.30633 	 0.32749 	 ~...
  105 	   107 	 0.32657 	 0.33985 	 ~...
  104 	   108 	 0.32297 	 0.34970 	 ~...
  114 	   109 	 0.38785 	 0.37015 	 ~...
  112 	   110 	 0.38668 	 0.37130 	 ~...
  107 	   111 	 0.33586 	 0.37989 	 m..s
  118 	   112 	 0.43518 	 0.38075 	 m..s
  110 	   113 	 0.37454 	 0.39756 	 ~...
  117 	   114 	 0.40321 	 0.41409 	 ~...
  111 	   115 	 0.37822 	 0.42736 	 m..s
  116 	   116 	 0.39131 	 0.43291 	 m..s
  115 	   117 	 0.38835 	 0.45539 	 m..s
  113 	   118 	 0.38718 	 0.45940 	 m..s
  119 	   119 	 0.46489 	 0.49103 	 ~...
  120 	   120 	 0.63242 	 0.62288 	 ~...
==========================================
r_mrr = 0.9748835563659668
r2_mrr = 0.9426828026771545
spearmanr_mrr@5 = 0.9951466917991638
spearmanr_mrr@10 = 0.9639225602149963
spearmanr_mrr@50 = 0.9842245578765869
spearmanr_mrr@100 = 0.995590329170227
spearmanr_mrr@All = 0.9960471391677856
==========================================
test time: 0.407
Done Testing dataset Kinships
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.472 +- 0.347
mrr vals (pred, true): 0.040, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   67 	     0 	 0.04110 	 0.00048 	 m..s
   65 	     1 	 0.04089 	 0.00048 	 m..s
   12 	     2 	 0.03760 	 0.00050 	 m..s
   44 	     3 	 0.03965 	 0.00052 	 m..s
   46 	     4 	 0.03979 	 0.00052 	 m..s
   27 	     5 	 0.03880 	 0.00052 	 m..s
   59 	     6 	 0.04058 	 0.00053 	 m..s
   40 	     7 	 0.03928 	 0.00054 	 m..s
   14 	     8 	 0.03771 	 0.00055 	 m..s
   41 	     9 	 0.03931 	 0.00055 	 m..s
   45 	    10 	 0.03971 	 0.00055 	 m..s
   13 	    11 	 0.03764 	 0.00057 	 m..s
   20 	    12 	 0.03809 	 0.00057 	 m..s
   79 	    13 	 0.07556 	 0.00058 	 m..s
    9 	    14 	 0.03731 	 0.00059 	 m..s
   62 	    15 	 0.04071 	 0.00059 	 m..s
   28 	    16 	 0.03893 	 0.00059 	 m..s
   33 	    17 	 0.03898 	 0.00060 	 m..s
   57 	    18 	 0.04028 	 0.00060 	 m..s
   19 	    19 	 0.03796 	 0.00062 	 m..s
    5 	    20 	 0.03688 	 0.00063 	 m..s
   55 	    21 	 0.04021 	 0.00064 	 m..s
    7 	    22 	 0.03716 	 0.00064 	 m..s
   35 	    23 	 0.03906 	 0.00066 	 m..s
   10 	    24 	 0.03739 	 0.00067 	 m..s
   25 	    25 	 0.03842 	 0.00067 	 m..s
   18 	    26 	 0.03796 	 0.00067 	 m..s
   56 	    27 	 0.04024 	 0.00069 	 m..s
   63 	    28 	 0.04076 	 0.00070 	 m..s
   70 	    29 	 0.04125 	 0.00072 	 m..s
   34 	    30 	 0.03899 	 0.00072 	 m..s
   21 	    31 	 0.03814 	 0.00074 	 m..s
   47 	    32 	 0.03983 	 0.00074 	 m..s
    4 	    33 	 0.03686 	 0.00076 	 m..s
   11 	    34 	 0.03756 	 0.00076 	 m..s
   29 	    35 	 0.03893 	 0.00078 	 m..s
   66 	    36 	 0.04097 	 0.00081 	 m..s
   60 	    37 	 0.04063 	 0.00081 	 m..s
   17 	    38 	 0.03794 	 0.00085 	 m..s
   54 	    39 	 0.04020 	 0.00087 	 m..s
   32 	    40 	 0.03895 	 0.00088 	 m..s
   71 	    41 	 0.04125 	 0.00089 	 m..s
   39 	    42 	 0.03922 	 0.00090 	 m..s
   64 	    43 	 0.04081 	 0.00091 	 m..s
   69 	    44 	 0.04120 	 0.00091 	 m..s
   15 	    45 	 0.03787 	 0.00093 	 m..s
    3 	    46 	 0.03686 	 0.00095 	 m..s
   50 	    47 	 0.03987 	 0.00098 	 m..s
   22 	    48 	 0.03823 	 0.00099 	 m..s
   78 	    49 	 0.07552 	 0.00100 	 m..s
   72 	    50 	 0.04130 	 0.00101 	 m..s
   51 	    51 	 0.03995 	 0.00103 	 m..s
    8 	    52 	 0.03726 	 0.00103 	 m..s
    6 	    53 	 0.03694 	 0.00103 	 m..s
    2 	    54 	 0.03651 	 0.00104 	 m..s
   31 	    55 	 0.03895 	 0.00106 	 m..s
   16 	    56 	 0.03788 	 0.00110 	 m..s
   38 	    57 	 0.03919 	 0.00111 	 m..s
   58 	    58 	 0.04030 	 0.00115 	 m..s
   42 	    59 	 0.03944 	 0.00119 	 m..s
   49 	    60 	 0.03987 	 0.00119 	 m..s
   24 	    61 	 0.03840 	 0.00127 	 m..s
   68 	    62 	 0.04114 	 0.00127 	 m..s
   73 	    63 	 0.04174 	 0.00132 	 m..s
   30 	    64 	 0.03894 	 0.00133 	 m..s
    1 	    65 	 0.03647 	 0.00142 	 m..s
   53 	    66 	 0.04014 	 0.00143 	 m..s
   52 	    67 	 0.04004 	 0.00156 	 m..s
   43 	    68 	 0.03954 	 0.00160 	 m..s
   36 	    69 	 0.03909 	 0.00162 	 m..s
   23 	    70 	 0.03835 	 0.00166 	 m..s
   26 	    71 	 0.03876 	 0.00170 	 m..s
   37 	    72 	 0.03914 	 0.00176 	 m..s
   48 	    73 	 0.03983 	 0.00177 	 m..s
    0 	    74 	 0.03477 	 0.00196 	 m..s
   61 	    75 	 0.04068 	 0.00200 	 m..s
  119 	    76 	 0.22408 	 0.01037 	 MISS
  118 	    77 	 0.22381 	 0.01618 	 MISS
   79 	    78 	 0.07556 	 0.04959 	 ~...
   79 	    79 	 0.07556 	 0.05603 	 ~...
   79 	    80 	 0.07556 	 0.06018 	 ~...
   91 	    81 	 0.07645 	 0.06278 	 ~...
   79 	    82 	 0.07556 	 0.06283 	 ~...
   87 	    83 	 0.07608 	 0.06405 	 ~...
   93 	    84 	 0.07663 	 0.06882 	 ~...
   95 	    85 	 0.07668 	 0.06921 	 ~...
   90 	    86 	 0.07640 	 0.07073 	 ~...
   79 	    87 	 0.07556 	 0.07189 	 ~...
  101 	    88 	 0.07763 	 0.07307 	 ~...
  106 	    89 	 0.07852 	 0.07752 	 ~...
  100 	    90 	 0.07716 	 0.07893 	 ~...
  103 	    91 	 0.07784 	 0.07894 	 ~...
   74 	    92 	 0.07541 	 0.07946 	 ~...
   88 	    93 	 0.07621 	 0.08043 	 ~...
   89 	    94 	 0.07638 	 0.08144 	 ~...
  104 	    95 	 0.07801 	 0.08188 	 ~...
  105 	    96 	 0.07809 	 0.08351 	 ~...
   99 	    97 	 0.07694 	 0.08353 	 ~...
  102 	    98 	 0.07767 	 0.08403 	 ~...
   98 	    99 	 0.07688 	 0.08449 	 ~...
   96 	   100 	 0.07676 	 0.08743 	 ~...
  111 	   101 	 0.19706 	 0.09024 	 MISS
   75 	   102 	 0.07547 	 0.10118 	 ~...
   97 	   103 	 0.07679 	 0.10268 	 ~...
   94 	   104 	 0.07665 	 0.10389 	 ~...
   92 	   105 	 0.07650 	 0.10555 	 ~...
  116 	   106 	 0.20499 	 0.10882 	 m..s
  107 	   107 	 0.09818 	 0.13087 	 m..s
  117 	   108 	 0.20637 	 0.13637 	 m..s
   76 	   109 	 0.07549 	 0.13758 	 m..s
   86 	   110 	 0.07577 	 0.14035 	 m..s
  108 	   111 	 0.18000 	 0.14416 	 m..s
   77 	   112 	 0.07552 	 0.14792 	 m..s
  110 	   113 	 0.19028 	 0.17571 	 ~...
  109 	   114 	 0.18917 	 0.17796 	 ~...
   79 	   115 	 0.07556 	 0.18508 	 MISS
  115 	   116 	 0.20227 	 0.19419 	 ~...
  112 	   117 	 0.20099 	 0.25904 	 m..s
  113 	   118 	 0.20145 	 0.27213 	 m..s
  114 	   119 	 0.20197 	 0.27955 	 m..s
  120 	   120 	 0.23399 	 0.34919 	 MISS
==========================================
r_mrr = 0.7849441170692444
r2_mrr = 0.4790371060371399
spearmanr_mrr@5 = 0.8826497793197632
spearmanr_mrr@10 = 0.9418189525604248
spearmanr_mrr@50 = 0.8768840432167053
spearmanr_mrr@100 = 0.928911030292511
spearmanr_mrr@All = 0.9343393445014954
==========================================
test time: 0.401
Done Testing dataset OpenEA
total time taken: 862.7681329250336
training time taken: 817.5954926013947
TWIG out ;))
=========================================================
---------------------------------------------------------
Running a TWIG experiment with tag: ComplEx-omit-Kinships
---------------------------------------------------------
=========================================================
Using random seed: 3518388971351461
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [84, 1021, 389, 644, 558, 456, 582, 890, 746, 1184, 1082, 493, 70, 671, 178, 353, 1144, 1131, 1140, 611, 711, 1132, 941, 761, 1038, 728, 907, 620, 672, 1177, 853, 994, 973, 453, 303, 446, 278, 972, 726, 789, 773, 55, 172, 479, 868, 754, 509, 918, 72, 402, 813, 786, 151, 109, 771, 949, 731, 744, 88, 11, 1086, 1116, 928, 437, 495, 667, 898, 385, 92, 1053, 653, 533, 421, 230, 921, 373, 689, 201, 633, 141, 619, 7, 902, 787, 1094, 54, 934, 967, 1155, 1015, 1090, 564, 246, 1048, 273, 677, 1039, 963, 1211, 545, 435, 48, 163, 841, 1101, 589, 904, 548, 675, 6, 712, 824, 814, 937, 580, 1146, 842, 59, 150, 183, 549]
valid_ids (0): []
train_ids (1094): [349, 638, 668, 783, 1192, 703, 24, 1022, 1013, 64, 559, 978, 631, 1193, 1141, 65, 41, 926, 535, 30, 1137, 1126, 614, 377, 1169, 301, 637, 737, 615, 990, 474, 179, 613, 1173, 962, 632, 78, 882, 75, 579, 1197, 315, 485, 328, 396, 1010, 321, 279, 414, 487, 294, 753, 584, 1160, 248, 382, 656, 574, 1096, 33, 29, 900, 1135, 1062, 803, 1113, 541, 134, 807, 1212, 395, 1024, 1058, 513, 126, 959, 348, 119, 191, 364, 602, 381, 577, 547, 208, 145, 1142, 307, 911, 698, 1196, 210, 752, 314, 1157, 529, 5, 587, 718, 357, 90, 182, 891, 567, 757, 636, 570, 1166, 655, 610, 186, 678, 1079, 715, 153, 203, 50, 477, 447, 361, 332, 1181, 1000, 407, 1139, 704, 939, 1171, 808, 463, 27, 924, 1202, 401, 334, 1175, 657, 999, 884, 893, 1121, 552, 745, 946, 257, 512, 472, 1078, 1061, 177, 538, 500, 457, 379, 1049, 531, 874, 1210, 490, 82, 253, 727, 1208, 267, 708, 276, 968, 897, 740, 909, 31, 806, 649, 940, 861, 852, 1068, 164, 199, 951, 1007, 162, 228, 809, 977, 411, 195, 748, 209, 945, 80, 100, 96, 1063, 659, 1030, 38, 735, 1060, 242, 591, 161, 578, 923, 355, 784, 1206, 527, 422, 496, 681, 739, 238, 1071, 1153, 523, 66, 165, 998, 216, 800, 1073, 310, 1164, 839, 181, 1204, 18, 342, 1089, 536, 171, 499, 1059, 391, 1168, 539, 1004, 225, 701, 258, 111, 222, 1083, 1189, 647, 944, 1138, 931, 685, 1092, 1170, 658, 604, 707, 724, 44, 498, 465, 475, 49, 1002, 622, 947, 516, 239, 1199, 330, 469, 790, 202, 89, 879, 392, 650, 864, 562, 569, 1018, 1097, 296, 14, 818, 1040, 1069, 1110, 665, 264, 511, 764, 629, 427, 212, 338, 551, 585, 550, 455, 281, 1150, 796, 957, 618, 938, 227, 247, 759, 877, 1129, 964, 426, 1012, 1107, 736, 299, 471, 544, 127, 1130, 32, 961, 237, 627, 304, 252, 12, 480, 169, 1035, 337, 306, 663, 45, 452, 110, 372, 514, 123, 1103, 129, 157, 691, 3, 1067, 601, 721, 149, 260, 976, 1066, 115, 674, 233, 270, 607, 62, 344, 673, 743, 854, 223, 229, 1149, 546, 448, 8, 476, 505, 794, 1127, 857, 843, 404, 139, 1112, 906, 155, 158, 1120, 661, 4, 1098, 867, 23, 872, 1205, 98, 285, 383, 760, 350, 922, 67, 501, 717, 63, 873, 320, 845, 777, 1016, 639, 537, 654, 224, 542, 434, 581, 741, 635, 125, 241, 896, 575, 819, 837, 648, 34, 318, 838, 952, 568, 1057, 170, 251, 363, 380, 53, 836, 767, 112, 782, 680, 974, 823, 28, 1043, 811, 887, 733, 322, 138, 825, 1214, 690, 105, 324, 1031, 1027, 339, 1085, 1182, 981, 1172, 984, 827, 1093, 532, 903, 40, 847, 679, 370, 226, 899, 240, 106, 74, 865, 664, 73, 1055, 588, 1187, 311, 993, 231, 975, 616, 1207, 1044, 207, 1042, 176, 76, 520, 185, 1052, 1201, 725, 489, 458, 595, 768, 58, 325, 467, 995, 117, 461, 259, 651, 192, 652, 61, 19, 1034, 605, 908, 1134, 415, 772, 1045, 13, 298, 933, 47, 686, 987, 22, 486, 849, 687, 121, 1188, 1133, 625, 184, 817, 830, 1111, 729, 878, 133, 1179, 52, 713, 910, 846, 1147, 828, 326, 747, 858, 300, 102, 1088, 433, 244, 491, 1143, 408, 1151, 822, 596, 912, 1025, 388, 1198, 266, 484, 815, 188, 0, 1072, 137, 397, 540, 935, 506, 438, 526, 147, 869, 749, 812, 492, 901, 1076, 220, 566, 293, 142, 821, 722, 104, 113, 42, 1064, 916, 1102, 51, 268, 175, 572, 205, 758, 174, 327, 706, 462, 93, 9, 386, 101, 1162, 986, 518, 1213, 346, 282, 400, 543, 130, 702, 960, 1003, 152, 1186, 517, 1050, 1119, 525, 1095, 797, 345, 368, 309, 291, 522, 503, 429, 354, 1114, 219, 254, 876, 454, 398, 443, 850, 218, 362, 571, 1183, 1077, 943, 159, 1118, 989, 81, 779, 272, 132, 856, 785, 710, 555, 925, 1174, 624, 676, 716, 36, 1056, 255, 720, 1163, 410, 699, 1009, 623, 979, 116, 641, 25, 1194, 997, 440, 792, 449, 1014, 608, 781, 1145, 43, 460, 431, 697, 982, 221, 507, 880, 198, 37, 988, 692, 1099, 1185, 780, 816, 1041, 840, 885, 1081, 274, 554, 1036, 289, 617, 57, 154, 1161, 15, 10, 39, 576, 319, 1070, 1108, 958, 146, 886, 1176, 1148, 445, 375, 200, 954, 365, 196, 1158, 1165, 286, 765, 331, 660, 983, 2, 166, 459, 425, 256, 235, 936, 776, 628, 269, 197, 340, 1020, 835, 510, 914, 236, 1019, 371, 466, 187, 323, 684, 870, 1017, 160, 430, 136, 1074, 966, 497, 855, 358, 406, 1203, 1037, 107, 942, 643, 888, 108, 419, 750, 483, 20, 646, 524, 99, 1195, 424, 683, 1005, 769, 917, 1006, 376, 439, 393, 234, 167, 468, 1080, 719, 56, 1122, 1106, 16, 829, 356, 432, 232, 390, 594, 194, 860, 77, 1033, 359, 1023, 436, 470, 420, 971, 68, 985, 630, 97, 832, 831, 805, 1100, 1065, 441, 770, 612, 519, 801, 297, 69, 553, 1046, 249, 804, 730, 214, 416, 913, 387, 919, 573, 333, 859, 955, 1008, 606, 565, 305, 851, 778, 384, 295, 1156, 464, 329, 834, 313, 662, 444, 206, 875, 1191, 213, 1051, 418, 217, 243, 124, 970, 557, 1084, 991, 280, 915, 742, 405, 645, 290, 775, 592, 1109, 798, 950, 642, 996, 723, 369, 143, 1032, 599, 833, 173, 1075, 1159, 71, 302, 122, 1001, 508, 992, 1200, 1087, 261, 481, 352, 1180, 343, 277, 670, 312, 1117, 1136, 284, 766, 751, 693, 969, 403, 494, 131, 215, 563, 669, 621, 732, 763, 180, 35, 140, 275, 598, 423, 148, 1123, 600, 1105, 450, 412, 79, 1154, 378, 795, 956, 871, 1026, 46, 502, 590, 417, 85, 755, 1128, 929, 193, 1115, 317, 530, 734, 700, 292, 953, 478, 86, 95, 688, 738, 263, 366, 118, 583, 1, 442, 168, 351, 190, 413, 283, 504, 609, 705, 114, 756, 682, 892, 144, 980, 640, 120, 863, 409, 626, 94, 1167, 802, 597, 367, 1152, 930, 316, 135, 515, 250, 17, 336, 60, 1178, 920, 1054, 271, 189, 360, 534, 694, 774, 128, 1011, 820, 927, 21, 905, 862, 211, 245, 1190, 87, 788, 265, 791, 482, 844, 374, 948, 451, 810, 521, 895, 287, 83, 1028, 762, 473, 932, 428, 714, 1209, 695, 634, 262, 883, 561, 1047, 666, 308, 881, 156, 866, 793, 347, 394, 556, 848, 341, 1124, 709, 91, 965, 1125, 204, 26, 586, 528, 1104, 593, 560, 399, 335, 889, 1091, 826, 894, 1029, 799, 696, 288, 488, 103, 603]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6165116354356427
the save name prefix for this run is:  chkpt-ID_6165116354356427_tag_ComplEx-omit-Kinships
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1'], 'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 137
rank avg (pred): 0.549 +- 0.004
mrr vals (pred, true): 0.013, 0.048
batch losses (mrrl, rdl): 0.0, 0.0003801736

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 923
rank avg (pred): 0.492 +- 0.007
mrr vals (pred, true): 0.015, 0.037
batch losses (mrrl, rdl): 0.0, 5.33397e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 394
rank avg (pred): 0.488 +- 0.009
mrr vals (pred, true): 0.015, 0.056
batch losses (mrrl, rdl): 0.0, 0.0001119643

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 695
rank avg (pred): 0.510 +- 0.005
mrr vals (pred, true): 0.014, 0.051
batch losses (mrrl, rdl): 0.0, 0.000196111

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1079
rank avg (pred): 0.177 +- 0.064
mrr vals (pred, true): 0.058, 0.258
batch losses (mrrl, rdl): 0.0, 5.09632e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 147
rank avg (pred): 0.499 +- 0.292
mrr vals (pred, true): 0.047, 0.037
batch losses (mrrl, rdl): 0.0, 2.55602e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 616
rank avg (pred): 0.468 +- 0.274
mrr vals (pred, true): 0.031, 0.042
batch losses (mrrl, rdl): 0.0, 7.8919e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1164
rank avg (pred): 0.464 +- 0.262
mrr vals (pred, true): 0.022, 0.032
batch losses (mrrl, rdl): 0.0, 3.9873e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1124
rank avg (pred): 0.483 +- 0.275
mrr vals (pred, true): 0.022, 0.053
batch losses (mrrl, rdl): 0.0, 1.65746e-05

Epoch over!
epoch time: 54.06

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 400
rank avg (pred): 0.462 +- 0.273
mrr vals (pred, true): 0.022, 0.052
batch losses (mrrl, rdl): 0.0, 6.4207e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 285
rank avg (pred): 0.171 +- 0.199
mrr vals (pred, true): 0.079, 0.186
batch losses (mrrl, rdl): 0.0, 2.44233e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 181
rank avg (pred): 0.464 +- 0.258
mrr vals (pred, true): 0.020, 0.049
batch losses (mrrl, rdl): 0.0, 6.5468e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 307
rank avg (pred): 0.242 +- 0.213
mrr vals (pred, true): 0.042, 0.219
batch losses (mrrl, rdl): 0.0, 5.45661e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1165
rank avg (pred): 0.479 +- 0.243
mrr vals (pred, true): 0.019, 0.036
batch losses (mrrl, rdl): 0.0, 1.08788e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 269
rank avg (pred): 0.159 +- 0.199
mrr vals (pred, true): 0.076, 0.206
batch losses (mrrl, rdl): 0.0, 1.33341e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 266
rank avg (pred): 0.154 +- 0.180
mrr vals (pred, true): 0.080, 0.218
batch losses (mrrl, rdl): 0.0, 2.39742e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 259
rank avg (pred): 0.148 +- 0.174
mrr vals (pred, true): 0.112, 0.214
batch losses (mrrl, rdl): 0.0, 2.59941e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 418
rank avg (pred): 0.447 +- 0.267
mrr vals (pred, true): 0.024, 0.051
batch losses (mrrl, rdl): 0.0, 9.4925e-06

Epoch over!
epoch time: 51.506

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 826
rank avg (pred): 0.175 +- 0.235
mrr vals (pred, true): 0.118, 0.245
batch losses (mrrl, rdl): 0.0, 5.5484e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 254
rank avg (pred): 0.148 +- 0.170
mrr vals (pred, true): 0.082, 0.221
batch losses (mrrl, rdl): 0.0, 3.58018e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 658
rank avg (pred): 0.468 +- 0.257
mrr vals (pred, true): 0.020, 0.048
batch losses (mrrl, rdl): 0.0, 6.6102e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 324
rank avg (pred): 0.457 +- 0.261
mrr vals (pred, true): 0.021, 0.050
batch losses (mrrl, rdl): 0.0, 4.9872e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1133
rank avg (pred): 0.476 +- 0.260
mrr vals (pred, true): 0.021, 0.049
batch losses (mrrl, rdl): 0.0, 5.9674e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1208
rank avg (pred): 0.466 +- 0.264
mrr vals (pred, true): 0.022, 0.049
batch losses (mrrl, rdl): 0.0, 4.4726e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1066
rank avg (pred): 0.159 +- 0.203
mrr vals (pred, true): 0.099, 0.259
batch losses (mrrl, rdl): 0.0, 1.54431e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 328
rank avg (pred): 0.471 +- 0.277
mrr vals (pred, true): 0.024, 0.043
batch losses (mrrl, rdl): 0.0, 4.6184e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 684
rank avg (pred): 0.454 +- 0.253
mrr vals (pred, true): 0.024, 0.052
batch losses (mrrl, rdl): 0.0, 3.4417e-06

Epoch over!
epoch time: 54.111

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 569
rank avg (pred): 0.453 +- 0.257
mrr vals (pred, true): 0.028, 0.038
batch losses (mrrl, rdl): 0.0, 8.4157e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 561
rank avg (pred): 0.512 +- 0.246
mrr vals (pred, true): 0.025, 0.024
batch losses (mrrl, rdl): 0.0, 4.4127e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 22
rank avg (pred): 0.180 +- 0.199
mrr vals (pred, true): 0.099, 0.189
batch losses (mrrl, rdl): 0.0, 2.2771e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 293
rank avg (pred): 0.215 +- 0.210
mrr vals (pred, true): 0.078, 0.214
batch losses (mrrl, rdl): 0.0, 8.4827e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1100
rank avg (pred): 0.477 +- 0.279
mrr vals (pred, true): 0.030, 0.051
batch losses (mrrl, rdl): 0.0, 1.56609e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 388
rank avg (pred): 0.454 +- 0.285
mrr vals (pred, true): 0.040, 0.046
batch losses (mrrl, rdl): 0.0, 4.8143e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 940
rank avg (pred): 0.556 +- 0.272
mrr vals (pred, true): 0.017, 0.040
batch losses (mrrl, rdl): 0.0, 6.66648e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 743
rank avg (pred): 0.099 +- 0.208
mrr vals (pred, true): 0.254, 0.280
batch losses (mrrl, rdl): 0.0, 5.54336e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 539
rank avg (pred): 0.559 +- 0.224
mrr vals (pred, true): 0.028, 0.026
batch losses (mrrl, rdl): 0.0, 3.87105e-05

Epoch over!
epoch time: 53.283

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 380
rank avg (pred): 0.462 +- 0.275
mrr vals (pred, true): 0.038, 0.047
batch losses (mrrl, rdl): 0.0, 1.3692e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 881
rank avg (pred): 0.492 +- 0.272
mrr vals (pred, true): 0.037, 0.044
batch losses (mrrl, rdl): 0.0, 4.10304e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 457
rank avg (pred): 0.448 +- 0.273
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 0.0, 5.1195e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 615
rank avg (pred): 0.466 +- 0.248
mrr vals (pred, true): 0.033, 0.041
batch losses (mrrl, rdl): 0.0, 2.567e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 897
rank avg (pred): 0.563 +- 0.269
mrr vals (pred, true): 0.048, 0.040
batch losses (mrrl, rdl): 0.0, 0.0003367956

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 920
rank avg (pred): 0.486 +- 0.248
mrr vals (pred, true): 0.033, 0.030
batch losses (mrrl, rdl): 0.0, 8.049e-07

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 307
rank avg (pred): 0.190 +- 0.220
mrr vals (pred, true): 0.141, 0.219
batch losses (mrrl, rdl): 0.0, 3.2887e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 588
rank avg (pred): 0.460 +- 0.259
mrr vals (pred, true): 0.043, 0.041
batch losses (mrrl, rdl): 0.0, 2.0782e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 503
rank avg (pred): 0.503 +- 0.245
mrr vals (pred, true): 0.056, 0.026
batch losses (mrrl, rdl): 0.0, 2.9437e-06

Epoch over!
epoch time: 55.314

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 695
rank avg (pred): 0.453 +- 0.265
mrr vals (pred, true): 0.059, 0.051
batch losses (mrrl, rdl): 0.0008470066, 4.9382e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 703
rank avg (pred): 0.619 +- 0.297
mrr vals (pred, true): 0.027, 0.042
batch losses (mrrl, rdl): 0.0052663246, 0.0004327618

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 94
rank avg (pred): 0.551 +- 0.339
mrr vals (pred, true): 0.051, 0.036
batch losses (mrrl, rdl): 1.23409e-05, 0.0001276257

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 534
rank avg (pred): 0.721 +- 0.293
mrr vals (pred, true): 0.045, 0.024
batch losses (mrrl, rdl): 0.0002302076, 0.0006895033

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 165
rank avg (pred): 0.559 +- 0.334
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 5.45975e-05, 0.0001614466

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 217
rank avg (pred): 0.562 +- 0.316
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0004629123, 0.0002592343

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 738
rank avg (pred): 0.245 +- 0.364
mrr vals (pred, true): 0.300, 0.310
batch losses (mrrl, rdl): 0.0009686278, 0.0003270896

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 374
rank avg (pred): 0.632 +- 0.307
mrr vals (pred, true): 0.038, 0.045
batch losses (mrrl, rdl): 0.0014855738, 0.0004823638

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1175
rank avg (pred): 0.641 +- 0.291
mrr vals (pred, true): 0.039, 0.035
batch losses (mrrl, rdl): 0.0011831614, 0.0005469352

Epoch over!
epoch time: 53.88

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 354
rank avg (pred): 0.569 +- 0.306
mrr vals (pred, true): 0.058, 0.045
batch losses (mrrl, rdl): 0.00069953, 0.0001522111

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 482
rank avg (pred): 0.566 +- 0.293
mrr vals (pred, true): 0.056, 0.051
batch losses (mrrl, rdl): 0.0003310755, 0.0001552229

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 922
rank avg (pred): 0.667 +- 0.254
mrr vals (pred, true): 0.031, 0.037
batch losses (mrrl, rdl): 0.0037048375, 0.0005071437

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 919
rank avg (pred): 0.583 +- 0.271
mrr vals (pred, true): 0.050, 0.039
batch losses (mrrl, rdl): 3.1e-09, 0.000164142

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 460
rank avg (pred): 0.558 +- 0.282
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 1.57078e-05, 0.0001517773

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1103
rank avg (pred): 0.530 +- 0.267
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.0001751901, 6.8899e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 727
rank avg (pred): 0.683 +- 0.260
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 6.91981e-05, 0.0010000309

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 975
rank avg (pred): 0.190 +- 0.265
mrr vals (pred, true): 0.329, 0.296
batch losses (mrrl, rdl): 0.0109137492, 4.59436e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 960
rank avg (pred): 0.718 +- 0.232
mrr vals (pred, true): 0.033, 0.043
batch losses (mrrl, rdl): 0.0029089926, 0.0011844313

Epoch over!
epoch time: 52.564

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 131
rank avg (pred): 0.521 +- 0.249
mrr vals (pred, true): 0.057, 0.049
batch losses (mrrl, rdl): 0.0005491424, 7.1794e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 788
rank avg (pred): 0.481 +- 0.212
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 9.21008e-05, 1.68843e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1008
rank avg (pred): 0.518 +- 0.250
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 1.87316e-05, 7.2436e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 588
rank avg (pred): 0.522 +- 0.219
mrr vals (pred, true): 0.048, 0.041
batch losses (mrrl, rdl): 2.44641e-05, 5.33278e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 142
rank avg (pred): 0.453 +- 0.182
mrr vals (pred, true): 0.057, 0.049
batch losses (mrrl, rdl): 0.0005534535, 3.26673e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1013
rank avg (pred): 0.443 +- 0.157
mrr vals (pred, true): 0.048, 0.046
batch losses (mrrl, rdl): 3.49439e-05, 3.94347e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 613
rank avg (pred): 0.449 +- 0.168
mrr vals (pred, true): 0.053, 0.038
batch losses (mrrl, rdl): 6.41533e-05, 2.6384e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 480
rank avg (pred): 0.428 +- 0.130
mrr vals (pred, true): 0.049, 0.053
batch losses (mrrl, rdl): 2.625e-06, 4.98635e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 696
rank avg (pred): 0.449 +- 0.156
mrr vals (pred, true): 0.048, 0.044
batch losses (mrrl, rdl): 4.25327e-05, 3.84629e-05

Epoch over!
epoch time: 53.666

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 142
rank avg (pred): 0.424 +- 0.132
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002891363, 5.2761e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 232
rank avg (pred): 0.437 +- 0.138
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 2.2583e-06, 5.21891e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 492
rank avg (pred): 0.760 +- 0.316
mrr vals (pred, true): 0.061, 0.024
batch losses (mrrl, rdl): 0.0012069981, 0.0008867344

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 543
rank avg (pred): 0.667 +- 0.314
mrr vals (pred, true): 0.062, 0.023
batch losses (mrrl, rdl): 0.001352041, 0.0003334478

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 834
rank avg (pred): 0.189 +- 0.184
mrr vals (pred, true): 0.355, 0.249
batch losses (mrrl, rdl): 0.1129006967, 2.27126e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 656
rank avg (pred): 0.439 +- 0.108
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 9.8049e-05, 5.54924e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 974
rank avg (pred): 0.197 +- 0.192
mrr vals (pred, true): 0.271, 0.268
batch losses (mrrl, rdl): 0.0001096909, 7.31448e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 567
rank avg (pred): 0.452 +- 0.125
mrr vals (pred, true): 0.043, 0.037
batch losses (mrrl, rdl): 0.0004781304, 4.2815e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 971
rank avg (pred): 0.454 +- 0.122
mrr vals (pred, true): 0.040, 0.054
batch losses (mrrl, rdl): 0.0009316104, 5.31914e-05

Epoch over!
epoch time: 55.809

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 877
rank avg (pred): 0.435 +- 0.121
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 3.58247e-05, 5.14584e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 572
rank avg (pred): 0.404 +- 0.124
mrr vals (pred, true): 0.073, 0.041
batch losses (mrrl, rdl): 0.0053341622, 6.97032e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 145
rank avg (pred): 0.426 +- 0.098
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001854356, 6.78199e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 798
rank avg (pred): 0.419 +- 0.086
mrr vals (pred, true): 0.046, 0.045
batch losses (mrrl, rdl): 0.00015506, 8.83977e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 159
rank avg (pred): 0.426 +- 0.098
mrr vals (pred, true): 0.055, 0.048
batch losses (mrrl, rdl): 0.0002213717, 6.8074e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 795
rank avg (pred): 0.420 +- 0.092
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 6.8881e-05, 9.03773e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 546
rank avg (pred): 0.632 +- 0.287
mrr vals (pred, true): 0.053, 0.026
batch losses (mrrl, rdl): 8.89567e-05, 0.000220485

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 255
rank avg (pred): 0.294 +- 0.189
mrr vals (pred, true): 0.244, 0.205
batch losses (mrrl, rdl): 0.0150779532, 0.0002478041

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 779
rank avg (pred): 0.414 +- 0.093
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 4.85442e-05, 7.83737e-05

Epoch over!
epoch time: 52.723

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1091
rank avg (pred): 0.411 +- 0.089
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.33272e-05, 6.99587e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 314
rank avg (pred): 0.293 +- 0.185
mrr vals (pred, true): 0.239, 0.209
batch losses (mrrl, rdl): 0.009180028, 0.0002633989

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 681
rank avg (pred): 0.397 +- 0.096
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001115621, 0.0001124069

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 944
rank avg (pred): 0.421 +- 0.086
mrr vals (pred, true): 0.036, 0.030
batch losses (mrrl, rdl): 0.0020116202, 0.0001431279

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 69
rank avg (pred): 0.309 +- 0.189
mrr vals (pred, true): 0.209, 0.209
batch losses (mrrl, rdl): 3.17e-08, 0.0003598462

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 440
rank avg (pred): 0.401 +- 0.088
mrr vals (pred, true): 0.052, 0.056
batch losses (mrrl, rdl): 4.21314e-05, 0.0001095983

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 450
rank avg (pred): 0.411 +- 0.100
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 8.46497e-05, 7.75211e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 251
rank avg (pred): 0.281 +- 0.183
mrr vals (pred, true): 0.254, 0.246
batch losses (mrrl, rdl): 0.0005372301, 0.0004627347

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 2
rank avg (pred): 0.285 +- 0.178
mrr vals (pred, true): 0.248, 0.254
batch losses (mrrl, rdl): 0.0004168016, 0.0004291844

Epoch over!
epoch time: 57.448

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1077
rank avg (pred): 0.264 +- 0.161
mrr vals (pred, true): 0.227, 0.273
batch losses (mrrl, rdl): 0.0213888884, 0.0001653093

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 866
rank avg (pred): 0.376 +- 0.068
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0006480847, 0.0001940551

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 354
rank avg (pred): 0.392 +- 0.091
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 1.46419e-05, 0.0001649362

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 410
rank avg (pred): 0.382 +- 0.071
mrr vals (pred, true): 0.045, 0.055
batch losses (mrrl, rdl): 0.0002922955, 0.0001291965

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 738
rank avg (pred): 0.252 +- 0.170
mrr vals (pred, true): 0.248, 0.310
batch losses (mrrl, rdl): 0.03895244, 0.0003913687

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1078
rank avg (pred): 0.217 +- 0.171
mrr vals (pred, true): 0.282, 0.274
batch losses (mrrl, rdl): 0.0007112377, 5.23535e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 348
rank avg (pred): 0.387 +- 0.076
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 3.6596e-06, 0.0001153265

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 177
rank avg (pred): 0.397 +- 0.096
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 1.21606e-05, 0.0001161854

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 626
rank avg (pred): 0.389 +- 0.107
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002048498, 0.0001209557

Epoch over!
epoch time: 54.842

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 961
rank avg (pred): 0.397 +- 0.117
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 1.481e-07, 0.0001148204

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 41
rank avg (pred): 0.298 +- 0.199
mrr vals (pred, true): 0.228, 0.244
batch losses (mrrl, rdl): 0.0022794437, 0.0003514399

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1027
rank avg (pred): 0.348 +- 0.083
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0004672969, 0.0002007276

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 60
rank avg (pred): 0.299 +- 0.178
mrr vals (pred, true): 0.204, 0.219
batch losses (mrrl, rdl): 0.002353051, 0.0004661606

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 277
rank avg (pred): 0.244 +- 0.156
mrr vals (pred, true): 0.216, 0.260
batch losses (mrrl, rdl): 0.0186903756, 0.0002428214

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 431
rank avg (pred): 0.359 +- 0.080
mrr vals (pred, true): 0.055, 0.056
batch losses (mrrl, rdl): 0.0002964888, 0.0002182866

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 647
rank avg (pred): 0.367 +- 0.079
mrr vals (pred, true): 0.052, 0.039
batch losses (mrrl, rdl): 3.05609e-05, 0.0002202476

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1076
rank avg (pred): 0.245 +- 0.163
mrr vals (pred, true): 0.242, 0.296
batch losses (mrrl, rdl): 0.030043209, 0.0001502626

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 632
rank avg (pred): 0.379 +- 0.121
mrr vals (pred, true): 0.051, 0.037
batch losses (mrrl, rdl): 1.63179e-05, 0.0002514193

Epoch over!
epoch time: 55.248

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 123
rank avg (pred): 0.350 +- 0.073
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 7.9267e-05, 0.0002340006

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 171
rank avg (pred): 0.357 +- 0.086
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 3.2185e-06, 0.0002087961

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 858
rank avg (pred): 0.343 +- 0.072
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.000458452, 0.0002776814

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1123
rank avg (pred): 0.347 +- 0.076
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 9.1084e-06, 0.0002379527

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 426
rank avg (pred): 0.353 +- 0.095
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 1.12097e-05, 0.0002383217

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 594
rank avg (pred): 0.358 +- 0.089
mrr vals (pred, true): 0.049, 0.037
batch losses (mrrl, rdl): 9.8363e-06, 0.0003326006

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 716
rank avg (pred): 0.351 +- 0.085
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 1.1538e-05, 0.0002492962

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 285
rank avg (pred): 0.327 +- 0.192
mrr vals (pred, true): 0.184, 0.186
batch losses (mrrl, rdl): 5.72734e-05, 0.0003583411

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 968
rank avg (pred): 0.353 +- 0.091
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 3.36094e-05, 0.0002347482

Epoch over!
epoch time: 54.527

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 372
rank avg (pred): 0.359 +- 0.107
mrr vals (pred, true): 0.043, 0.049
batch losses (mrrl, rdl): 0.0004361868, 0.0001963253

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 846
rank avg (pred): 0.362 +- 0.107
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003448393, 0.0002087285

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 971
rank avg (pred): 0.345 +- 0.089
mrr vals (pred, true): 0.043, 0.054
batch losses (mrrl, rdl): 0.0004328991, 0.0002442991

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 93
rank avg (pred): 0.360 +- 0.119
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 4.10119e-05, 0.0001885171

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 702
rank avg (pred): 0.356 +- 0.088
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 1.6406e-06, 0.0001884498

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 935
rank avg (pred): 0.351 +- 0.095
mrr vals (pred, true): 0.050, 0.027
batch losses (mrrl, rdl): 2.745e-07, 0.0005671812

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 766
rank avg (pred): 0.334 +- 0.093
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0002365128, 0.0003409766

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 52
rank avg (pred): 0.272 +- 0.159
mrr vals (pred, true): 0.223, 0.203
batch losses (mrrl, rdl): 0.0036803132, 0.0001516201

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 276
rank avg (pred): 0.268 +- 0.155
mrr vals (pred, true): 0.233, 0.257
batch losses (mrrl, rdl): 0.0058003985, 0.0003086339

Epoch over!
epoch time: 56.006

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.349 +- 0.090
mrr vals (pred, true): 0.050, 0.047

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04198 	 0.02037 	 ~...
   69 	     1 	 0.05094 	 0.02165 	 ~...
    5 	     2 	 0.04633 	 0.02204 	 ~...
   88 	     3 	 0.05235 	 0.02319 	 ~...
   94 	     4 	 0.06249 	 0.02342 	 m..s
   77 	     5 	 0.05153 	 0.02360 	 ~...
    0 	     6 	 0.04149 	 0.02370 	 ~...
   95 	     7 	 0.06251 	 0.02383 	 m..s
   93 	     8 	 0.05838 	 0.02414 	 m..s
   92 	     9 	 0.05640 	 0.02518 	 m..s
   40 	    10 	 0.04937 	 0.02523 	 ~...
    2 	    11 	 0.04247 	 0.02543 	 ~...
   96 	    12 	 0.06640 	 0.02571 	 m..s
    3 	    13 	 0.04385 	 0.02626 	 ~...
   56 	    14 	 0.05013 	 0.02626 	 ~...
   74 	    15 	 0.05110 	 0.02782 	 ~...
   75 	    16 	 0.05133 	 0.02843 	 ~...
   21 	    17 	 0.04842 	 0.02979 	 ~...
   78 	    18 	 0.05155 	 0.03029 	 ~...
   17 	    19 	 0.04807 	 0.03057 	 ~...
   58 	    20 	 0.05016 	 0.03125 	 ~...
   42 	    21 	 0.04949 	 0.03637 	 ~...
   85 	    22 	 0.05208 	 0.03663 	 ~...
   43 	    23 	 0.04951 	 0.03749 	 ~...
   84 	    24 	 0.05204 	 0.03858 	 ~...
   32 	    25 	 0.04922 	 0.03927 	 ~...
   86 	    26 	 0.05222 	 0.03942 	 ~...
   91 	    27 	 0.05454 	 0.03947 	 ~...
   66 	    28 	 0.05047 	 0.03976 	 ~...
   37 	    29 	 0.04929 	 0.03980 	 ~...
   19 	    30 	 0.04832 	 0.04019 	 ~...
   87 	    31 	 0.05222 	 0.04045 	 ~...
   67 	    32 	 0.05061 	 0.04046 	 ~...
   30 	    33 	 0.04904 	 0.04098 	 ~...
   47 	    34 	 0.04971 	 0.04109 	 ~...
   83 	    35 	 0.05193 	 0.04126 	 ~...
   33 	    36 	 0.04924 	 0.04163 	 ~...
   27 	    37 	 0.04892 	 0.04169 	 ~...
   14 	    38 	 0.04777 	 0.04260 	 ~...
   45 	    39 	 0.04959 	 0.04272 	 ~...
   31 	    40 	 0.04919 	 0.04301 	 ~...
   15 	    41 	 0.04783 	 0.04311 	 ~...
   64 	    42 	 0.05038 	 0.04313 	 ~...
   16 	    43 	 0.04806 	 0.04344 	 ~...
   60 	    44 	 0.05021 	 0.04365 	 ~...
   36 	    45 	 0.04926 	 0.04369 	 ~...
   63 	    46 	 0.05035 	 0.04369 	 ~...
   41 	    47 	 0.04947 	 0.04378 	 ~...
   13 	    48 	 0.04765 	 0.04408 	 ~...
   20 	    49 	 0.04834 	 0.04410 	 ~...
   35 	    50 	 0.04926 	 0.04446 	 ~...
   62 	    51 	 0.05030 	 0.04473 	 ~...
   12 	    52 	 0.04764 	 0.04473 	 ~...
   80 	    53 	 0.05156 	 0.04493 	 ~...
    8 	    54 	 0.04735 	 0.04513 	 ~...
   10 	    55 	 0.04749 	 0.04527 	 ~...
   49 	    56 	 0.04996 	 0.04551 	 ~...
   90 	    57 	 0.05258 	 0.04564 	 ~...
   23 	    58 	 0.04859 	 0.04596 	 ~...
   55 	    59 	 0.05012 	 0.04608 	 ~...
   25 	    60 	 0.04874 	 0.04620 	 ~...
   70 	    61 	 0.05095 	 0.04670 	 ~...
   11 	    62 	 0.04762 	 0.04680 	 ~...
   28 	    63 	 0.04895 	 0.04690 	 ~...
    7 	    64 	 0.04735 	 0.04705 	 ~...
   50 	    65 	 0.04998 	 0.04711 	 ~...
   52 	    66 	 0.05004 	 0.04713 	 ~...
   22 	    67 	 0.04854 	 0.04744 	 ~...
   65 	    68 	 0.05039 	 0.04747 	 ~...
   38 	    69 	 0.04930 	 0.04749 	 ~...
   71 	    70 	 0.05097 	 0.04759 	 ~...
   46 	    71 	 0.04965 	 0.04788 	 ~...
   18 	    72 	 0.04826 	 0.04789 	 ~...
   79 	    73 	 0.05156 	 0.04819 	 ~...
   26 	    74 	 0.04878 	 0.04827 	 ~...
    6 	    75 	 0.04688 	 0.04838 	 ~...
   89 	    76 	 0.05240 	 0.04872 	 ~...
   34 	    77 	 0.04924 	 0.04879 	 ~...
   44 	    78 	 0.04951 	 0.04891 	 ~...
   73 	    79 	 0.05105 	 0.04897 	 ~...
   59 	    80 	 0.05020 	 0.04898 	 ~...
   51 	    81 	 0.05001 	 0.04898 	 ~...
   82 	    82 	 0.05168 	 0.04901 	 ~...
   57 	    83 	 0.05015 	 0.04904 	 ~...
   68 	    84 	 0.05090 	 0.04917 	 ~...
   72 	    85 	 0.05098 	 0.04929 	 ~...
   54 	    86 	 0.05012 	 0.04939 	 ~...
   39 	    87 	 0.04932 	 0.04952 	 ~...
   48 	    88 	 0.04978 	 0.05007 	 ~...
    9 	    89 	 0.04740 	 0.05049 	 ~...
   81 	    90 	 0.05163 	 0.05100 	 ~...
   29 	    91 	 0.04899 	 0.05147 	 ~...
   53 	    92 	 0.05009 	 0.05235 	 ~...
   61 	    93 	 0.05022 	 0.05338 	 ~...
   24 	    94 	 0.04860 	 0.05448 	 ~...
   76 	    95 	 0.05147 	 0.05501 	 ~...
    4 	    96 	 0.04514 	 0.06132 	 ~...
  100 	    97 	 0.23915 	 0.18867 	 m..s
   97 	    98 	 0.23906 	 0.20756 	 m..s
  105 	    99 	 0.28390 	 0.21983 	 m..s
  102 	   100 	 0.25090 	 0.22517 	 ~...
  106 	   101 	 0.28629 	 0.22739 	 m..s
   97 	   102 	 0.23906 	 0.23015 	 ~...
  111 	   103 	 0.32536 	 0.23090 	 m..s
  104 	   104 	 0.28201 	 0.23554 	 m..s
  103 	   105 	 0.26467 	 0.23984 	 ~...
  117 	   106 	 0.37042 	 0.24218 	 MISS
  108 	   107 	 0.29030 	 0.24265 	 m..s
  107 	   108 	 0.28857 	 0.24271 	 m..s
   97 	   109 	 0.23906 	 0.24656 	 ~...
  114 	   110 	 0.34085 	 0.24954 	 m..s
  109 	   111 	 0.29290 	 0.25327 	 m..s
  101 	   112 	 0.24065 	 0.26711 	 ~...
  110 	   113 	 0.29945 	 0.27497 	 ~...
  116 	   114 	 0.35717 	 0.27737 	 m..s
  113 	   115 	 0.33832 	 0.28691 	 m..s
  112 	   116 	 0.33691 	 0.28722 	 m..s
  115 	   117 	 0.35285 	 0.30203 	 m..s
  119 	   118 	 0.46561 	 0.39525 	 m..s
  120 	   119 	 0.47078 	 0.55007 	 m..s
  118 	   120 	 0.45773 	 0.55011 	 m..s
==========================================
r_mrr = 0.9718351364135742
r2_mrr = 0.9162212610244751
spearmanr_mrr@5 = 0.9045727252960205
spearmanr_mrr@10 = 0.942573606967926
spearmanr_mrr@50 = 0.9734699130058289
spearmanr_mrr@100 = 0.9809840321540833
spearmanr_mrr@All = 0.9811084270477295
==========================================
test time: 0.537
Done Testing dataset UMLS
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.492 +- 0.268
mrr vals (pred, true): 0.058, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   95 	     0 	 0.07583 	 7e-0500 	 m..s
   10 	     1 	 0.05445 	 0.00013 	 m..s
   35 	     2 	 0.05678 	 0.00014 	 m..s
   75 	     3 	 0.06128 	 0.00015 	 m..s
    4 	     4 	 0.05350 	 0.00016 	 m..s
   79 	     5 	 0.06207 	 0.00016 	 m..s
   14 	     6 	 0.05469 	 0.00016 	 m..s
   71 	     7 	 0.06088 	 0.00017 	 m..s
   12 	     8 	 0.05466 	 0.00018 	 m..s
    5 	     9 	 0.05357 	 0.00019 	 m..s
   46 	    10 	 0.05735 	 0.00019 	 m..s
   74 	    11 	 0.06123 	 0.00019 	 m..s
   68 	    12 	 0.05998 	 0.00019 	 m..s
   84 	    13 	 0.06328 	 0.00020 	 m..s
   54 	    14 	 0.05791 	 0.00020 	 m..s
   44 	    15 	 0.05729 	 0.00020 	 m..s
   36 	    16 	 0.05689 	 0.00021 	 m..s
   69 	    17 	 0.06003 	 0.00021 	 m..s
   27 	    18 	 0.05631 	 0.00021 	 m..s
   29 	    19 	 0.05650 	 0.00021 	 m..s
   55 	    20 	 0.05791 	 0.00021 	 m..s
   41 	    21 	 0.05716 	 0.00022 	 m..s
   33 	    22 	 0.05670 	 0.00022 	 m..s
   78 	    23 	 0.06203 	 0.00022 	 m..s
   42 	    24 	 0.05720 	 0.00022 	 m..s
   17 	    25 	 0.05505 	 0.00022 	 m..s
   32 	    26 	 0.05667 	 0.00023 	 m..s
   24 	    27 	 0.05569 	 0.00023 	 m..s
   45 	    28 	 0.05735 	 0.00023 	 m..s
   39 	    29 	 0.05697 	 0.00023 	 m..s
    0 	    30 	 0.05129 	 0.00024 	 m..s
   47 	    31 	 0.05736 	 0.00024 	 m..s
    6 	    32 	 0.05392 	 0.00024 	 m..s
    1 	    33 	 0.05176 	 0.00024 	 m..s
   20 	    34 	 0.05559 	 0.00024 	 m..s
   18 	    35 	 0.05524 	 0.00025 	 m..s
   86 	    36 	 0.06363 	 0.00025 	 m..s
   72 	    37 	 0.06092 	 0.00025 	 m..s
   22 	    38 	 0.05563 	 0.00026 	 m..s
    3 	    39 	 0.05295 	 0.00026 	 m..s
   82 	    40 	 0.06263 	 0.00026 	 m..s
   77 	    41 	 0.06198 	 0.00026 	 m..s
   53 	    42 	 0.05787 	 0.00026 	 m..s
   16 	    43 	 0.05493 	 0.00026 	 m..s
   57 	    44 	 0.05817 	 0.00027 	 m..s
   56 	    45 	 0.05812 	 0.00027 	 m..s
   59 	    46 	 0.05843 	 0.00027 	 m..s
   66 	    47 	 0.05923 	 0.00027 	 m..s
   43 	    48 	 0.05722 	 0.00027 	 m..s
    2 	    49 	 0.05229 	 0.00027 	 m..s
   26 	    50 	 0.05623 	 0.00029 	 m..s
   52 	    51 	 0.05783 	 0.00029 	 m..s
    9 	    52 	 0.05444 	 0.00030 	 m..s
   51 	    53 	 0.05775 	 0.00031 	 m..s
   49 	    54 	 0.05756 	 0.00031 	 m..s
   58 	    55 	 0.05842 	 0.00033 	 m..s
   11 	    56 	 0.05459 	 0.00034 	 m..s
   25 	    57 	 0.05618 	 0.00034 	 m..s
   50 	    58 	 0.05772 	 0.00035 	 m..s
    8 	    59 	 0.05426 	 0.00035 	 m..s
   19 	    60 	 0.05534 	 0.00036 	 m..s
   70 	    61 	 0.06027 	 0.00037 	 m..s
   28 	    62 	 0.05641 	 0.00038 	 m..s
   38 	    63 	 0.05697 	 0.00038 	 m..s
   40 	    64 	 0.05711 	 0.00039 	 m..s
   60 	    65 	 0.05852 	 0.00041 	 m..s
   83 	    66 	 0.06293 	 0.00041 	 m..s
   21 	    67 	 0.05560 	 0.00046 	 m..s
   62 	    68 	 0.05882 	 0.00052 	 m..s
   61 	    69 	 0.05869 	 0.00053 	 m..s
   37 	    70 	 0.05696 	 0.00054 	 m..s
   63 	    71 	 0.05896 	 0.00054 	 m..s
   30 	    72 	 0.05657 	 0.00055 	 m..s
   34 	    73 	 0.05671 	 0.00056 	 m..s
   76 	    74 	 0.06139 	 0.00058 	 m..s
   67 	    75 	 0.05984 	 0.00066 	 m..s
   13 	    76 	 0.05468 	 0.00079 	 m..s
    7 	    77 	 0.05418 	 0.00093 	 m..s
   81 	    78 	 0.06254 	 0.00096 	 m..s
   23 	    79 	 0.05568 	 0.00103 	 m..s
   64 	    80 	 0.05904 	 0.00113 	 m..s
   15 	    81 	 0.05481 	 0.00149 	 m..s
   31 	    82 	 0.05660 	 0.00484 	 m..s
   80 	    83 	 0.06242 	 0.00705 	 m..s
   65 	    84 	 0.05916 	 0.05043 	 ~...
   89 	    85 	 0.07497 	 0.06009 	 ~...
   48 	    86 	 0.05748 	 0.06847 	 ~...
   73 	    87 	 0.06099 	 0.07873 	 ~...
   92 	    88 	 0.07504 	 0.07934 	 ~...
   87 	    89 	 0.06694 	 0.08504 	 ~...
   89 	    90 	 0.07497 	 0.08609 	 ~...
   93 	    91 	 0.07506 	 0.08810 	 ~...
   85 	    92 	 0.06345 	 0.08867 	 ~...
  106 	    93 	 0.14826 	 0.08882 	 m..s
   97 	    94 	 0.07756 	 0.08958 	 ~...
   99 	    95 	 0.08458 	 0.08984 	 ~...
   96 	    96 	 0.07607 	 0.09126 	 ~...
  110 	    97 	 0.17191 	 0.09181 	 m..s
   89 	    98 	 0.07497 	 0.09434 	 ~...
  112 	    99 	 0.17680 	 0.09764 	 m..s
  107 	   100 	 0.15237 	 0.09781 	 m..s
   98 	   101 	 0.07861 	 0.09804 	 ~...
   88 	   102 	 0.06815 	 0.09815 	 m..s
  103 	   103 	 0.09901 	 0.10535 	 ~...
  111 	   104 	 0.17416 	 0.10811 	 m..s
  100 	   105 	 0.08620 	 0.11100 	 ~...
   94 	   106 	 0.07546 	 0.11377 	 m..s
  104 	   107 	 0.10441 	 0.11537 	 ~...
  108 	   108 	 0.16896 	 0.12319 	 m..s
  101 	   109 	 0.09786 	 0.12469 	 ~...
  102 	   110 	 0.09876 	 0.13614 	 m..s
  105 	   111 	 0.14729 	 0.15961 	 ~...
  109 	   112 	 0.16947 	 0.18436 	 ~...
  114 	   113 	 0.23518 	 0.22917 	 ~...
  116 	   114 	 0.28927 	 0.24985 	 m..s
  117 	   115 	 0.30258 	 0.26914 	 m..s
  115 	   116 	 0.27610 	 0.27098 	 ~...
  113 	   117 	 0.19799 	 0.28218 	 m..s
  118 	   118 	 0.31513 	 0.30273 	 ~...
  120 	   119 	 0.32034 	 0.33249 	 ~...
  119 	   120 	 0.32006 	 0.34767 	 ~...
==========================================
r_mrr = 0.9221703410148621
r2_mrr = 0.571780800819397
spearmanr_mrr@5 = 0.9098809361457825
spearmanr_mrr@10 = 0.9491065144538879
spearmanr_mrr@50 = 0.9450778365135193
spearmanr_mrr@100 = 0.9489995241165161
spearmanr_mrr@All = 0.9512476921081543
==========================================
test time: 0.471
Done Testing dataset DBpedia50
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.529 +- 0.261
mrr vals (pred, true): 0.049, 0.005

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.02197 	 0.00088 	 ~...
    8 	     1 	 0.03879 	 0.00102 	 m..s
   21 	     2 	 0.04465 	 0.00146 	 m..s
    0 	     3 	 0.01540 	 0.00151 	 ~...
   29 	     4 	 0.04576 	 0.00151 	 m..s
   17 	     5 	 0.04416 	 0.00192 	 m..s
   72 	     6 	 0.05113 	 0.00194 	 m..s
   77 	     7 	 0.05179 	 0.00225 	 m..s
   39 	     8 	 0.04671 	 0.00259 	 m..s
   88 	     9 	 0.05296 	 0.00261 	 m..s
   85 	    10 	 0.05257 	 0.00262 	 m..s
   79 	    11 	 0.05190 	 0.00264 	 m..s
   28 	    12 	 0.04561 	 0.00266 	 m..s
   82 	    13 	 0.05241 	 0.00282 	 m..s
   73 	    14 	 0.05136 	 0.00283 	 m..s
   34 	    15 	 0.04627 	 0.00292 	 m..s
   80 	    16 	 0.05191 	 0.00293 	 m..s
   13 	    17 	 0.04331 	 0.00296 	 m..s
   33 	    18 	 0.04620 	 0.00301 	 m..s
   16 	    19 	 0.04405 	 0.00301 	 m..s
   12 	    20 	 0.04326 	 0.00304 	 m..s
   52 	    21 	 0.04831 	 0.00305 	 m..s
   86 	    22 	 0.05288 	 0.00323 	 m..s
   42 	    23 	 0.04683 	 0.00325 	 m..s
   55 	    24 	 0.04874 	 0.00325 	 m..s
   64 	    25 	 0.05020 	 0.00330 	 m..s
   84 	    26 	 0.05250 	 0.00340 	 m..s
   35 	    27 	 0.04629 	 0.00350 	 m..s
   50 	    28 	 0.04806 	 0.00355 	 m..s
   38 	    29 	 0.04671 	 0.00367 	 m..s
   58 	    30 	 0.04922 	 0.00370 	 m..s
   76 	    31 	 0.05162 	 0.00372 	 m..s
   69 	    32 	 0.05083 	 0.00373 	 m..s
   75 	    33 	 0.05154 	 0.00373 	 m..s
   45 	    34 	 0.04724 	 0.00374 	 m..s
   60 	    35 	 0.04948 	 0.00376 	 m..s
   43 	    36 	 0.04702 	 0.00377 	 m..s
   31 	    37 	 0.04594 	 0.00378 	 m..s
   27 	    38 	 0.04550 	 0.00381 	 m..s
   68 	    39 	 0.05073 	 0.00382 	 m..s
   11 	    40 	 0.04312 	 0.00386 	 m..s
   65 	    41 	 0.05022 	 0.00387 	 m..s
   57 	    42 	 0.04901 	 0.00393 	 m..s
   49 	    43 	 0.04783 	 0.00400 	 m..s
   83 	    44 	 0.05247 	 0.00404 	 m..s
   41 	    45 	 0.04682 	 0.00404 	 m..s
   48 	    46 	 0.04774 	 0.00405 	 m..s
   67 	    47 	 0.05048 	 0.00406 	 m..s
   18 	    48 	 0.04418 	 0.00410 	 m..s
   81 	    49 	 0.05191 	 0.00410 	 m..s
   78 	    50 	 0.05182 	 0.00412 	 m..s
   40 	    51 	 0.04671 	 0.00414 	 m..s
   71 	    52 	 0.05110 	 0.00416 	 m..s
   36 	    53 	 0.04660 	 0.00418 	 m..s
   44 	    54 	 0.04712 	 0.00421 	 m..s
   32 	    55 	 0.04595 	 0.00424 	 m..s
   89 	    56 	 0.05323 	 0.00425 	 m..s
   20 	    57 	 0.04448 	 0.00429 	 m..s
   23 	    58 	 0.04498 	 0.00431 	 m..s
   15 	    59 	 0.04393 	 0.00438 	 m..s
   53 	    60 	 0.04845 	 0.00438 	 m..s
   74 	    61 	 0.05150 	 0.00443 	 m..s
   19 	    62 	 0.04447 	 0.00447 	 m..s
   14 	    63 	 0.04348 	 0.00458 	 m..s
   10 	    64 	 0.04289 	 0.00459 	 m..s
   56 	    65 	 0.04896 	 0.00463 	 m..s
   59 	    66 	 0.04933 	 0.00464 	 m..s
   25 	    67 	 0.04537 	 0.00465 	 m..s
   46 	    68 	 0.04728 	 0.00465 	 m..s
   30 	    69 	 0.04583 	 0.00467 	 m..s
   26 	    70 	 0.04539 	 0.00467 	 m..s
   91 	    71 	 0.05635 	 0.00473 	 m..s
   63 	    72 	 0.05009 	 0.00476 	 m..s
   24 	    73 	 0.04529 	 0.00482 	 m..s
   37 	    74 	 0.04663 	 0.00491 	 m..s
   62 	    75 	 0.04970 	 0.00492 	 m..s
   66 	    76 	 0.05031 	 0.00492 	 m..s
   70 	    77 	 0.05086 	 0.00498 	 m..s
   51 	    78 	 0.04816 	 0.00508 	 m..s
   87 	    79 	 0.05295 	 0.00520 	 m..s
    4 	    80 	 0.02749 	 0.00523 	 ~...
   90 	    81 	 0.05346 	 0.00525 	 m..s
   61 	    82 	 0.04954 	 0.00533 	 m..s
   22 	    83 	 0.04473 	 0.00534 	 m..s
   47 	    84 	 0.04739 	 0.00545 	 m..s
    3 	    85 	 0.02667 	 0.00604 	 ~...
    2 	    86 	 0.02623 	 0.00729 	 ~...
   54 	    87 	 0.04863 	 0.00775 	 m..s
   93 	    88 	 0.06035 	 0.01061 	 m..s
   92 	    89 	 0.05714 	 0.01098 	 m..s
   96 	    90 	 0.06846 	 0.01930 	 m..s
    5 	    91 	 0.03202 	 0.02268 	 ~...
    7 	    92 	 0.03813 	 0.02442 	 ~...
   95 	    93 	 0.06174 	 0.02791 	 m..s
   94 	    94 	 0.06161 	 0.02854 	 m..s
    9 	    95 	 0.04049 	 0.03521 	 ~...
    6 	    96 	 0.03406 	 0.04969 	 ~...
   97 	    97 	 0.14991 	 0.11357 	 m..s
  100 	    98 	 0.15001 	 0.12326 	 ~...
  102 	    99 	 0.15914 	 0.12873 	 m..s
   97 	   100 	 0.14991 	 0.13249 	 ~...
   97 	   101 	 0.14991 	 0.13525 	 ~...
  101 	   102 	 0.15102 	 0.13821 	 ~...
  103 	   103 	 0.17167 	 0.17434 	 ~...
  104 	   104 	 0.19466 	 0.17596 	 ~...
  105 	   105 	 0.20117 	 0.19847 	 ~...
  116 	   106 	 0.28159 	 0.21151 	 m..s
  108 	   107 	 0.23938 	 0.21924 	 ~...
  107 	   108 	 0.23636 	 0.22805 	 ~...
  117 	   109 	 0.28248 	 0.22842 	 m..s
  113 	   110 	 0.26564 	 0.22949 	 m..s
  110 	   111 	 0.25653 	 0.23733 	 ~...
  115 	   112 	 0.27974 	 0.24645 	 m..s
  106 	   113 	 0.23150 	 0.25176 	 ~...
  109 	   114 	 0.24994 	 0.25433 	 ~...
  114 	   115 	 0.27500 	 0.26340 	 ~...
  111 	   116 	 0.26284 	 0.26363 	 ~...
  112 	   117 	 0.26362 	 0.26879 	 ~...
  119 	   118 	 0.36820 	 0.27244 	 m..s
  118 	   119 	 0.35035 	 0.29538 	 m..s
  120 	   120 	 0.38402 	 0.30476 	 m..s
==========================================
r_mrr = 0.9813368916511536
r2_mrr = 0.7676388025283813
spearmanr_mrr@5 = 0.8854740262031555
spearmanr_mrr@10 = 0.9241008162498474
spearmanr_mrr@50 = 0.9872066974639893
spearmanr_mrr@100 = 0.9907547831535339
spearmanr_mrr@All = 0.9900417327880859
==========================================
test time: 0.629
Done Testing dataset CoDExSmall
Testing model with KGEM ComplEx and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.506 +- 0.261
mrr vals (pred, true): 0.056, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   98 	     0 	 0.08161 	 0.00020 	 m..s
   49 	     1 	 0.05538 	 0.00047 	 m..s
   53 	     2 	 0.05552 	 0.00048 	 m..s
   40 	     3 	 0.05481 	 0.00049 	 m..s
   86 	     4 	 0.05899 	 0.00050 	 m..s
    1 	     5 	 0.05070 	 0.00051 	 m..s
   12 	     6 	 0.05236 	 0.00051 	 m..s
   83 	     7 	 0.05753 	 0.00052 	 m..s
   10 	     8 	 0.05218 	 0.00053 	 m..s
   42 	     9 	 0.05491 	 0.00053 	 m..s
    8 	    10 	 0.05194 	 0.00054 	 m..s
   55 	    11 	 0.05564 	 0.00055 	 m..s
   26 	    12 	 0.05387 	 0.00056 	 m..s
   30 	    13 	 0.05423 	 0.00056 	 m..s
   52 	    14 	 0.05549 	 0.00056 	 m..s
   11 	    15 	 0.05220 	 0.00057 	 m..s
   70 	    16 	 0.05656 	 0.00057 	 m..s
   76 	    17 	 0.05676 	 0.00057 	 m..s
   71 	    18 	 0.05660 	 0.00058 	 m..s
   29 	    19 	 0.05405 	 0.00058 	 m..s
   54 	    20 	 0.05562 	 0.00058 	 m..s
   60 	    21 	 0.05585 	 0.00060 	 m..s
   45 	    22 	 0.05519 	 0.00060 	 m..s
   82 	    23 	 0.05738 	 0.00061 	 m..s
    3 	    24 	 0.05116 	 0.00062 	 m..s
   72 	    25 	 0.05663 	 0.00063 	 m..s
   47 	    26 	 0.05529 	 0.00063 	 m..s
   38 	    27 	 0.05478 	 0.00063 	 m..s
   66 	    28 	 0.05636 	 0.00064 	 m..s
   69 	    29 	 0.05645 	 0.00065 	 m..s
   46 	    30 	 0.05521 	 0.00065 	 m..s
    2 	    31 	 0.05086 	 0.00066 	 m..s
   80 	    32 	 0.05714 	 0.00066 	 m..s
    0 	    33 	 0.05047 	 0.00066 	 m..s
   81 	    34 	 0.05726 	 0.00069 	 m..s
   68 	    35 	 0.05637 	 0.00071 	 m..s
   56 	    36 	 0.05576 	 0.00071 	 m..s
   74 	    37 	 0.05670 	 0.00072 	 m..s
    6 	    38 	 0.05192 	 0.00072 	 m..s
   51 	    39 	 0.05549 	 0.00073 	 m..s
   41 	    40 	 0.05482 	 0.00073 	 m..s
   44 	    41 	 0.05514 	 0.00074 	 m..s
    9 	    42 	 0.05209 	 0.00075 	 m..s
   20 	    43 	 0.05323 	 0.00076 	 m..s
   75 	    44 	 0.05675 	 0.00077 	 m..s
   50 	    45 	 0.05538 	 0.00077 	 m..s
   14 	    46 	 0.05258 	 0.00077 	 m..s
   64 	    47 	 0.05624 	 0.00077 	 m..s
   15 	    48 	 0.05276 	 0.00080 	 m..s
   62 	    49 	 0.05613 	 0.00083 	 m..s
   21 	    50 	 0.05332 	 0.00083 	 m..s
   18 	    51 	 0.05300 	 0.00088 	 m..s
   67 	    52 	 0.05636 	 0.00088 	 m..s
   16 	    53 	 0.05283 	 0.00089 	 m..s
   31 	    54 	 0.05431 	 0.00091 	 m..s
   37 	    55 	 0.05475 	 0.00093 	 m..s
    7 	    56 	 0.05193 	 0.00094 	 m..s
   58 	    57 	 0.05578 	 0.00098 	 m..s
   43 	    58 	 0.05506 	 0.00100 	 m..s
   28 	    59 	 0.05401 	 0.00102 	 m..s
   33 	    60 	 0.05435 	 0.00104 	 m..s
   24 	    61 	 0.05337 	 0.00111 	 m..s
   61 	    62 	 0.05589 	 0.00111 	 m..s
   22 	    63 	 0.05333 	 0.00112 	 m..s
   34 	    64 	 0.05442 	 0.00115 	 m..s
    5 	    65 	 0.05176 	 0.00121 	 m..s
   19 	    66 	 0.05301 	 0.00134 	 m..s
   32 	    67 	 0.05434 	 0.00135 	 m..s
   39 	    68 	 0.05479 	 0.00139 	 m..s
   35 	    69 	 0.05466 	 0.00142 	 m..s
   77 	    70 	 0.05681 	 0.00155 	 m..s
   27 	    71 	 0.05393 	 0.00159 	 m..s
   48 	    72 	 0.05534 	 0.00160 	 m..s
   79 	    73 	 0.05696 	 0.00167 	 m..s
   57 	    74 	 0.05577 	 0.00179 	 m..s
    4 	    75 	 0.05150 	 0.00182 	 m..s
   65 	    76 	 0.05631 	 0.00184 	 m..s
   23 	    77 	 0.05337 	 0.00191 	 m..s
   63 	    78 	 0.05614 	 0.00194 	 m..s
   17 	    79 	 0.05294 	 0.00200 	 m..s
   13 	    80 	 0.05257 	 0.00228 	 m..s
   73 	    81 	 0.05665 	 0.00231 	 m..s
   85 	    82 	 0.05861 	 0.00267 	 m..s
   78 	    83 	 0.05695 	 0.00533 	 m..s
   25 	    84 	 0.05383 	 0.04703 	 ~...
   36 	    85 	 0.05468 	 0.04752 	 ~...
   59 	    86 	 0.05582 	 0.06283 	 ~...
   94 	    87 	 0.06170 	 0.06381 	 ~...
   97 	    88 	 0.06477 	 0.06384 	 ~...
   87 	    89 	 0.05994 	 0.06998 	 ~...
   90 	    90 	 0.06006 	 0.07041 	 ~...
   87 	    91 	 0.05994 	 0.07087 	 ~...
   99 	    92 	 0.08341 	 0.07170 	 ~...
  105 	    93 	 0.13031 	 0.07225 	 m..s
   96 	    94 	 0.06345 	 0.07284 	 ~...
   84 	    95 	 0.05786 	 0.07595 	 ~...
   95 	    96 	 0.06259 	 0.07621 	 ~...
   87 	    97 	 0.05994 	 0.07948 	 ~...
  101 	    98 	 0.08531 	 0.08027 	 ~...
  110 	    99 	 0.15167 	 0.08036 	 m..s
   91 	   100 	 0.06007 	 0.08076 	 ~...
   93 	   101 	 0.06125 	 0.08508 	 ~...
  102 	   102 	 0.09975 	 0.08960 	 ~...
   92 	   103 	 0.06079 	 0.09031 	 ~...
  108 	   104 	 0.13625 	 0.09360 	 m..s
  109 	   105 	 0.13630 	 0.09364 	 m..s
  106 	   106 	 0.13376 	 0.09414 	 m..s
  107 	   107 	 0.13484 	 0.09562 	 m..s
  103 	   108 	 0.10072 	 0.10244 	 ~...
  100 	   109 	 0.08500 	 0.10555 	 ~...
  104 	   110 	 0.10649 	 0.16744 	 m..s
  112 	   111 	 0.18442 	 0.18250 	 ~...
  114 	   112 	 0.22683 	 0.18272 	 m..s
  116 	   113 	 0.27182 	 0.19208 	 m..s
  115 	   114 	 0.25556 	 0.20609 	 m..s
  117 	   115 	 0.29199 	 0.21233 	 m..s
  113 	   116 	 0.21056 	 0.22630 	 ~...
  111 	   117 	 0.16941 	 0.27048 	 MISS
  119 	   118 	 0.30657 	 0.31774 	 ~...
  120 	   119 	 0.30691 	 0.34917 	 m..s
  118 	   120 	 0.30432 	 0.35511 	 m..s
==========================================
r_mrr = 0.9139379262924194
r2_mrr = 0.5491926670074463
spearmanr_mrr@5 = 0.9622818827629089
spearmanr_mrr@10 = 0.8888931274414062
spearmanr_mrr@50 = 0.9506308436393738
spearmanr_mrr@100 = 0.951898992061615
spearmanr_mrr@All = 0.9536368250846863
==========================================
test time: 0.48
Done Testing dataset OpenEA
total time taken: 864.249502658844
training time taken: 817.2657022476196
TWIG out ;))
=======================================================
-------------------------------------------------------
Running a TWIG experiment with tag: ComplEx-omit-OpenEA
-------------------------------------------------------
=======================================================
Using random seed: 4312924015159758
Starting TWIG!
Loading datasets
Loading ComplEx...
Loading UMLS...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [426, 1033, 551, 590, 223, 243, 10, 782, 249, 30, 610, 300, 1072, 456, 1051, 654, 704, 112, 303, 476, 678, 840, 62, 354, 599, 83, 882, 68, 543, 1116, 1134, 435, 138, 325, 8, 128, 447, 1048, 1145, 734, 1185, 1078, 1, 790, 566, 904, 500, 392, 334, 302, 1000, 1142, 548, 397, 470, 67, 616, 250, 308, 762, 48, 364, 936, 373, 890, 44, 614, 468, 699, 1137, 150, 881, 1187, 499, 1045, 537, 617, 1049, 1054, 828, 1076, 791, 1111, 266, 1186, 183, 649, 1166, 349, 672, 1090, 264, 180, 227, 492, 1188, 420, 103, 673, 514, 319, 77, 411, 1117, 384, 1107, 935, 196, 742, 506, 306, 895, 771, 336, 853, 410, 644, 640, 985, 580, 1088]
valid_ids (0): []
train_ids (1094): [416, 1148, 712, 1168, 588, 832, 5, 870, 886, 217, 1015, 167, 145, 557, 40, 929, 505, 766, 1082, 959, 558, 292, 552, 1085, 45, 294, 1024, 301, 350, 669, 1043, 922, 465, 166, 1149, 119, 694, 222, 394, 754, 452, 596, 983, 648, 413, 776, 1135, 993, 865, 440, 709, 81, 471, 247, 205, 0, 253, 173, 657, 607, 246, 532, 748, 730, 650, 1055, 735, 1061, 964, 370, 50, 275, 938, 280, 793, 1081, 230, 775, 157, 521, 237, 108, 967, 441, 327, 38, 116, 1064, 37, 818, 101, 3, 942, 212, 1065, 22, 1138, 842, 133, 659, 105, 419, 1018, 51, 322, 352, 710, 887, 976, 850, 1083, 620, 768, 554, 871, 1179, 1099, 307, 494, 706, 700, 899, 980, 466, 587, 567, 386, 772, 697, 930, 429, 634, 486, 281, 390, 32, 1204, 815, 95, 581, 723, 433, 1058, 696, 705, 478, 1164, 996, 106, 849, 53, 918, 718, 42, 7, 823, 562, 165, 797, 242, 1001, 777, 1005, 231, 209, 663, 36, 1080, 861, 1210, 1115, 813, 879, 124, 309, 1075, 202, 794, 682, 372, 1034, 1112, 208, 193, 954, 598, 337, 690, 26, 56, 536, 664, 377, 944, 780, 104, 656, 18, 473, 1170, 326, 1122, 1208, 661, 1002, 403, 190, 273, 767, 318, 655, 685, 677, 213, 698, 845, 343, 963, 272, 523, 928, 100, 34, 982, 595, 126, 91, 1126, 39, 512, 932, 873, 405, 58, 130, 1189, 310, 296, 269, 195, 726, 159, 459, 1071, 82, 331, 46, 1041, 950, 129, 438, 1119, 1087, 965, 827, 800, 872, 398, 695, 1205, 490, 458, 931, 550, 1070, 1120, 387, 1100, 1214, 369, 962, 324, 185, 668, 1050, 838, 1052, 589, 939, 911, 52, 457, 539, 1038, 211, 197, 118, 78, 807, 184, 756, 670, 233, 844, 639, 582, 346, 257, 316, 110, 839, 256, 732, 1203, 626, 544, 979, 837, 722, 973, 1209, 520, 123, 717, 376, 805, 192, 439, 203, 774, 445, 141, 393, 560, 507, 684, 906, 970, 1098, 825, 578, 356, 417, 365, 146, 357, 574, 676, 708, 1178, 453, 605, 746, 641, 1062, 204, 1035, 353, 618, 576, 235, 1009, 1202, 781, 1104, 1150, 645, 1182, 702, 747, 991, 263, 1130, 615, 17, 773, 6, 1195, 244, 115, 510, 633, 1197, 826, 513, 1003, 884, 632, 431, 374, 400, 218, 475, 921, 531, 188, 867, 132, 1017, 367, 1109, 1016, 528, 951, 602, 175, 1171, 1161, 516, 47, 295, 530, 79, 380, 686, 395, 923, 915, 1180, 1175, 436, 70, 994, 527, 662, 795, 1108, 315, 907, 1183, 731, 577, 1159, 986, 728, 1067, 329, 627, 418, 80, 97, 846, 391, 920, 1096, 1006, 955, 1152, 341, 716, 631, 234, 31, 396, 509, 87, 181, 1154, 869, 1042, 759, 1106, 64, 517, 796, 526, 1023, 1047, 629, 41, 290, 852, 859, 725, 966, 975, 1128, 85, 291, 125, 806, 251, 745, 1089, 1012, 1101, 503, 646, 570, 958, 803, 999, 625, 1136, 320, 25, 455, 485, 883, 591, 606, 88, 755, 338, 1151, 271, 1110, 750, 609, 761, 1139, 33, 675, 274, 134, 424, 630, 93, 432, 1133, 239, 1192, 293, 948, 228, 182, 98, 140, 114, 194, 434, 573, 1057, 894, 297, 20, 24, 968, 277, 428, 538, 482, 168, 786, 1031, 221, 136, 1044, 628, 241, 409, 919, 621, 174, 21, 375, 821, 1114, 851, 363, 407, 483, 833, 758, 736, 1121, 421, 220, 779, 1191, 624, 666, 533, 348, 917, 802, 122, 1113, 972, 769, 720, 990, 347, 693, 856, 812, 854, 120, 1026, 715, 289, 245, 688, 299, 161, 254, 860, 1213, 945, 479, 389, 366, 1008, 279, 314, 90, 240, 804, 402, 1091, 368, 809, 898, 467, 637, 63, 752, 689, 355, 658, 1144, 55, 926, 312, 1141, 721, 501, 957, 863, 992, 819, 760, 563, 604, 1066, 29, 102, 897, 9, 719, 54, 788, 586, 1056, 437, 260, 707, 449, 1201, 252, 808, 568, 1190, 592, 214, 947, 446, 739, 1020, 692, 740, 489, 741, 177, 1102, 978, 910, 381, 425, 597, 671, 622, 1028, 822, 412, 901, 73, 555, 135, 733, 969, 738, 170, 496, 545, 925, 1181, 1073, 1129, 238, 75, 792, 1105, 313, 1184, 332, 111, 1007, 787, 484, 127, 156, 158, 23, 824, 876, 96, 956, 724, 862, 344, 541, 1010, 535, 924, 564, 877, 1124, 191, 147, 878, 1156, 579, 493, 464, 462, 16, 572, 519, 953, 547, 176, 1094, 2, 283, 1163, 206, 210, 345, 583, 683, 172, 481, 1157, 753, 84, 1118, 997, 282, 857, 358, 916, 981, 113, 321, 933, 518, 160, 495, 913, 268, 1040, 998, 892, 635, 187, 1004, 600, 1155, 1125, 186, 1174, 1212, 778, 556, 875, 1039, 71, 810, 216, 1021, 288, 109, 613, 117, 1206, 66, 502, 454, 1194, 546, 789, 902, 422, 278, 569, 1032, 559, 408, 908, 593, 553, 941, 836, 12, 594, 99, 1123, 89, 154, 92, 1103, 258, 858, 451, 703, 855, 540, 152, 388, 831, 974, 1014, 107, 585, 423, 498, 143, 843, 255, 86, 575, 1095, 362, 1037, 480, 764, 785, 149, 960, 305, 744, 60, 76, 912, 333, 137, 207, 469, 674, 1086, 515, 65, 763, 798, 460, 757, 989, 665, 934, 946, 1167, 1059, 399, 977, 711, 749, 603, 608, 226, 1036, 549, 949, 1079, 984, 830, 360, 342, 896, 19, 448, 508, 799, 542, 584, 378, 571, 265, 219, 148, 401, 339, 653, 784, 636, 267, 893, 200, 1019, 1146, 903, 524, 874, 1077, 612, 35, 601, 383, 1160, 463, 1199, 1140, 891, 1027, 371, 1097, 1198, 1165, 382, 864, 139, 504, 1132, 43, 430, 651, 442, 328, 565, 1143, 816, 215, 1158, 619, 142, 727, 679, 74, 1011, 914, 1025, 286, 379, 28, 1063, 414, 359, 647, 13, 1092, 1207, 820, 1074, 937, 660, 987, 487, 404, 131, 1172, 638, 783, 534, 943, 1053, 525, 1177, 561, 155, 814, 199, 1060, 259, 888, 169, 351, 1196, 522, 443, 1022, 765, 835, 1093, 1030, 262, 284, 59, 511, 179, 121, 1153, 248, 4, 988, 162, 450, 751, 311, 1013, 229, 406, 201, 14, 847, 713, 385, 189, 743, 829, 961, 330, 323, 61, 900, 236, 868, 1176, 1162, 1211, 687, 335, 927, 652, 340, 885, 880, 444, 995, 474, 94, 225, 287, 817, 770, 491, 889, 1029, 529, 497, 461, 611, 224, 15, 477, 905, 1127, 270, 642, 261, 866, 171, 232, 57, 623, 276, 1200, 1147, 848, 69, 317, 834, 1069, 909, 681, 667, 1169, 198, 701, 49, 27, 164, 1173, 144, 304, 153, 427, 472, 811, 298, 72, 952, 1084, 415, 11, 680, 841, 940, 1193, 691, 1131, 285, 488, 163, 801, 643, 714, 361, 178, 737, 1068, 151, 729, 1046, 971]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2438518885392531
the save name prefix for this run is:  chkpt-ID_2438518885392531_tag_ComplEx-omit-OpenEA
running TWIG with settings:
data_to_load: {'ComplEx': {'UMLS': ['2.1'], 'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 864
rank avg (pred): 0.432 +- 0.004
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 9.88928e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 37
rank avg (pred): 0.170 +- 0.114
mrr vals (pred, true): 0.049, 0.222
batch losses (mrrl, rdl): 0.0, 1.94497e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1189
rank avg (pred): 0.541 +- 0.245
mrr vals (pred, true): 0.016, 0.054
batch losses (mrrl, rdl): 0.0, 0.000147595

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 255
rank avg (pred): 0.123 +- 0.172
mrr vals (pred, true): 0.084, 0.205
batch losses (mrrl, rdl): 0.0, 0.0001445035

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 23
rank avg (pred): 0.151 +- 0.197
mrr vals (pred, true): 0.073, 0.211
batch losses (mrrl, rdl): 0.0, 3.21982e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 142
rank avg (pred): 0.480 +- 0.258
mrr vals (pred, true): 0.019, 0.049
batch losses (mrrl, rdl): 0.0, 1.83617e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.485 +- 0.257
mrr vals (pred, true): 0.019, 0.045
batch losses (mrrl, rdl): 0.0, 6.966e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 13
rank avg (pred): 0.096 +- 0.129
mrr vals (pred, true): 0.094, 0.219
batch losses (mrrl, rdl): 0.0, 0.000142772

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 665
rank avg (pred): 0.478 +- 0.248
mrr vals (pred, true): 0.019, 0.050
batch losses (mrrl, rdl): 0.0, 3.10319e-05

Epoch over!
epoch time: 53.216

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 549
rank avg (pred): 0.475 +- 0.248
mrr vals (pred, true): 0.019, 0.023
batch losses (mrrl, rdl): 0.0, 7.59405e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1012
rank avg (pred): 0.457 +- 0.265
mrr vals (pred, true): 0.021, 0.040
batch losses (mrrl, rdl): 0.0, 4.2374e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 252
rank avg (pred): 0.173 +- 0.196
mrr vals (pred, true): 0.068, 0.193
batch losses (mrrl, rdl): 0.0, 2.46632e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 216
rank avg (pred): 0.463 +- 0.260
mrr vals (pred, true): 0.020, 0.045
batch losses (mrrl, rdl): 0.0, 3.3734e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 710
rank avg (pred): 0.459 +- 0.250
mrr vals (pred, true): 0.020, 0.041
batch losses (mrrl, rdl): 0.0, 3.0031e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 33
rank avg (pred): 0.188 +- 0.194
mrr vals (pred, true): 0.062, 0.212
batch losses (mrrl, rdl): 0.0, 9.7344e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 981
rank avg (pred): 0.149 +- 0.180
mrr vals (pred, true): 0.082, 0.269
batch losses (mrrl, rdl): 0.0, 6.142e-07

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1096
rank avg (pred): 0.473 +- 0.261
mrr vals (pred, true): 0.020, 0.043
batch losses (mrrl, rdl): 0.0, 4.7815e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 730
rank avg (pred): 0.119 +- 0.147
mrr vals (pred, true): 0.097, 0.386
batch losses (mrrl, rdl): 0.0, 6.1383e-06

Epoch over!
epoch time: 53.896

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 623
rank avg (pred): 0.451 +- 0.252
mrr vals (pred, true): 0.021, 0.047
batch losses (mrrl, rdl): 0.0, 2.7793e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 121
rank avg (pred): 0.455 +- 0.258
mrr vals (pred, true): 0.021, 0.041
batch losses (mrrl, rdl): 0.0, 1.23513e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 157
rank avg (pred): 0.440 +- 0.255
mrr vals (pred, true): 0.022, 0.041
batch losses (mrrl, rdl): 0.0, 1.80687e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1098
rank avg (pred): 0.451 +- 0.260
mrr vals (pred, true): 0.022, 0.046
batch losses (mrrl, rdl): 0.0, 2.114e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1030
rank avg (pred): 0.445 +- 0.236
mrr vals (pred, true): 0.021, 0.051
batch losses (mrrl, rdl): 0.0, 4.0889e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 911
rank avg (pred): 0.590 +- 0.193
mrr vals (pred, true): 0.014, 0.020
batch losses (mrrl, rdl): 0.0, 2.8809e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 11
rank avg (pred): 0.140 +- 0.171
mrr vals (pred, true): 0.095, 0.220
batch losses (mrrl, rdl): 0.0, 2.51733e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 986
rank avg (pred): 0.225 +- 0.252
mrr vals (pred, true): 0.072, 0.296
batch losses (mrrl, rdl): 0.0, 0.0001252015

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 685
rank avg (pred): 0.481 +- 0.264
mrr vals (pred, true): 0.020, 0.046
batch losses (mrrl, rdl): 0.0, 6.0802e-06

Epoch over!
epoch time: 53.561

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 228
rank avg (pred): 0.448 +- 0.256
mrr vals (pred, true): 0.022, 0.045
batch losses (mrrl, rdl): 0.0, 1.3543e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 207
rank avg (pred): 0.525 +- 0.271
mrr vals (pred, true): 0.019, 0.045
batch losses (mrrl, rdl): 0.0, 9.00807e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1139
rank avg (pred): 0.464 +- 0.269
mrr vals (pred, true): 0.023, 0.023
batch losses (mrrl, rdl): 0.0, 5.36966e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 884
rank avg (pred): 0.468 +- 0.259
mrr vals (pred, true): 0.021, 0.045
batch losses (mrrl, rdl): 0.0, 5.5102e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1087
rank avg (pred): 0.429 +- 0.251
mrr vals (pred, true): 0.023, 0.039
batch losses (mrrl, rdl): 0.0, 1.27551e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1008
rank avg (pred): 0.447 +- 0.256
mrr vals (pred, true): 0.022, 0.044
batch losses (mrrl, rdl): 0.0, 1.1697e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 146
rank avg (pred): 0.463 +- 0.257
mrr vals (pred, true): 0.021, 0.052
batch losses (mrrl, rdl): 0.0, 5.9518e-06

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 20
rank avg (pred): 0.158 +- 0.179
mrr vals (pred, true): 0.082, 0.202
batch losses (mrrl, rdl): 0.0, 9.2208e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 818
rank avg (pred): 0.152 +- 0.167
mrr vals (pred, true): 0.090, 0.239
batch losses (mrrl, rdl): 0.0, 1.915e-07

Epoch over!
epoch time: 54.244

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 357
rank avg (pred): 0.456 +- 0.262
mrr vals (pred, true): 0.022, 0.050
batch losses (mrrl, rdl): 0.0, 1.25833e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 911
rank avg (pred): 0.425 +- 0.247
mrr vals (pred, true): 0.022, 0.020
batch losses (mrrl, rdl): 0.0, 0.0004883925

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 251
rank avg (pred): 0.152 +- 0.185
mrr vals (pred, true): 0.098, 0.246
batch losses (mrrl, rdl): 0.0, 2.0673e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 982
rank avg (pred): 0.164 +- 0.206
mrr vals (pred, true): 0.099, 0.295
batch losses (mrrl, rdl): 0.0, 2.53315e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 937
rank avg (pred): 0.459 +- 0.239
mrr vals (pred, true): 0.020, 0.037
batch losses (mrrl, rdl): 0.0, 5.4476e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 3
rank avg (pred): 0.156 +- 0.185
mrr vals (pred, true): 0.096, 0.245
batch losses (mrrl, rdl): 0.0, 1.2147e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 733
rank avg (pred): 0.115 +- 0.153
mrr vals (pred, true): 0.158, 0.522
batch losses (mrrl, rdl): 0.0, 7.91054e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 332
rank avg (pred): 0.439 +- 0.252
mrr vals (pred, true): 0.023, 0.046
batch losses (mrrl, rdl): 0.0, 6.0625e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 5
rank avg (pred): 0.122 +- 0.165
mrr vals (pred, true): 0.162, 0.238
batch losses (mrrl, rdl): 0.0, 6.7338e-06

Epoch over!
epoch time: 52.953

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 453
rank avg (pred): 0.434 +- 0.258
mrr vals (pred, true): 0.023, 0.052
batch losses (mrrl, rdl): 0.0070772804, 7.0065e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1001
rank avg (pred): 0.289 +- 0.095
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 5.16364e-05, 0.0005161254

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 424
rank avg (pred): 0.453 +- 0.183
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 5.0679e-06, 1.94887e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 425
rank avg (pred): 0.423 +- 0.217
mrr vals (pred, true): 0.052, 0.051
batch losses (mrrl, rdl): 3.71031e-05, 8.625e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1041
rank avg (pred): 0.390 +- 0.208
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0001259581, 6.19367e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 371
rank avg (pred): 0.444 +- 0.268
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001505395, 5.9349e-06

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 148
rank avg (pred): 0.377 +- 0.227
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.0001657083, 0.0001637439

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 530
rank avg (pred): 0.469 +- 0.276
mrr vals (pred, true): 0.069, 0.027
batch losses (mrrl, rdl): 0.0035566872, 5.11296e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 718
rank avg (pred): 0.432 +- 0.249
mrr vals (pred, true): 0.041, 0.039
batch losses (mrrl, rdl): 0.0007365065, 2.76014e-05

Epoch over!
epoch time: 54.5

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 807
rank avg (pred): 0.476 +- 0.272
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001301129, 2.38739e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 428
rank avg (pred): 0.339 +- 0.218
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 8.28419e-05, 0.0001605091

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 194
rank avg (pred): 0.440 +- 0.270
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.0001401069, 3.0376e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 448
rank avg (pred): 0.433 +- 0.264
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 2.63843e-05, 7.3353e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 332
rank avg (pred): 0.461 +- 0.277
mrr vals (pred, true): 0.057, 0.046
batch losses (mrrl, rdl): 0.0004598091, 2.8985e-06

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 794
rank avg (pred): 0.466 +- 0.279
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 7.3308e-06, 1.66734e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 237
rank avg (pred): 0.371 +- 0.235
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001294422, 0.000112203

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 583
rank avg (pred): 0.455 +- 0.276
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001327494, 1.08376e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 74
rank avg (pred): 0.064 +- 0.046
mrr vals (pred, true): 0.200, 0.227
batch losses (mrrl, rdl): 0.0071169715, 0.0003408529

Epoch over!
epoch time: 54.808

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 51
rank avg (pred): 0.098 +- 0.058
mrr vals (pred, true): 0.160, 0.183
batch losses (mrrl, rdl): 0.005211032, 0.0003375041

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 739
rank avg (pred): 0.030 +- 0.022
mrr vals (pred, true): 0.300, 0.298
batch losses (mrrl, rdl): 3.86012e-05, 0.0001934255

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 399
rank avg (pred): 0.441 +- 0.297
mrr vals (pred, true): 0.043, 0.054
batch losses (mrrl, rdl): 0.0004279544, 5.3016e-06

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 278
rank avg (pred): 0.055 +- 0.046
mrr vals (pred, true): 0.267, 0.227
batch losses (mrrl, rdl): 0.0156714283, 0.0002273165

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 648
rank avg (pred): 0.481 +- 0.318
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 7.826e-07, 2.44806e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 58
rank avg (pred): 0.068 +- 0.048
mrr vals (pred, true): 0.182, 0.226
batch losses (mrrl, rdl): 0.0202182699, 0.0001764603

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 328
rank avg (pred): 0.495 +- 0.342
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 6.21872e-05, 4.18487e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 865
rank avg (pred): 0.185 +- 0.085
mrr vals (pred, true): 0.062, 0.049
batch losses (mrrl, rdl): 0.0014022568, 0.0016375028

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 37
rank avg (pred): 0.089 +- 0.107
mrr vals (pred, true): 0.166, 0.222
batch losses (mrrl, rdl): 0.0319311917, 0.000194563

Epoch over!
epoch time: 54.979

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 631
rank avg (pred): 0.428 +- 0.326
mrr vals (pred, true): 0.058, 0.038
batch losses (mrrl, rdl): 0.0005745342, 9.30357e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 190
rank avg (pred): 0.433 +- 0.337
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 2.3619e-06, 7.04934e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 638
rank avg (pred): 0.413 +- 0.366
mrr vals (pred, true): 0.058, 0.043
batch losses (mrrl, rdl): 0.0005876652, 0.0001846214

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 524
rank avg (pred): 0.643 +- 0.252
mrr vals (pred, true): 0.054, 0.023
batch losses (mrrl, rdl): 0.0001561265, 0.0002930117

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 694
rank avg (pred): 0.379 +- 0.249
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0004289312, 8.85477e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1156
rank avg (pred): 0.605 +- 0.269
mrr vals (pred, true): 0.073, 0.024
batch losses (mrrl, rdl): 0.0051975483, 0.0002434373

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 495
rank avg (pred): 0.710 +- 0.251
mrr vals (pred, true): 0.056, 0.022
batch losses (mrrl, rdl): 0.0003593853, 0.0007537976

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 178
rank avg (pred): 0.463 +- 0.293
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 4.1025e-05, 1.40826e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 109
rank avg (pred): 0.424 +- 0.287
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 1.71724e-05, 8.8453e-06

Epoch over!
epoch time: 56.775

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 59
rank avg (pred): 0.059 +- 0.056
mrr vals (pred, true): 0.212, 0.267
batch losses (mrrl, rdl): 0.0305783916, 0.0001629232

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 535
rank avg (pred): 0.700 +- 0.258
mrr vals (pred, true): 0.050, 0.024
batch losses (mrrl, rdl): 3.045e-07, 0.0006581942

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 774
rank avg (pred): 0.404 +- 0.266
mrr vals (pred, true): 0.052, 0.041
batch losses (mrrl, rdl): 2.64572e-05, 4.16555e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1197
rank avg (pred): 0.526 +- 0.239
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0004336765, 9.44883e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 589
rank avg (pred): 0.506 +- 0.261
mrr vals (pred, true): 0.057, 0.040
batch losses (mrrl, rdl): 0.0005494782, 6.79558e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 93
rank avg (pred): 0.533 +- 0.227
mrr vals (pred, true): 0.045, 0.053
batch losses (mrrl, rdl): 0.000269542, 0.0002378486

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 676
rank avg (pred): 0.341 +- 0.128
mrr vals (pred, true): 0.044, 0.043
batch losses (mrrl, rdl): 0.000354677, 0.0002570321

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 144
rank avg (pred): 0.408 +- 0.184
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 8.14e-08, 3.1962e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 163
rank avg (pred): 0.458 +- 0.213
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001625481, 3.30478e-05

Epoch over!
epoch time: 55.919

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1149
rank avg (pred): 0.277 +- 0.092
mrr vals (pred, true): 0.056, 0.026
batch losses (mrrl, rdl): 0.0003876929, 0.000849021

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 378
rank avg (pred): 0.343 +- 0.174
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 1.34528e-05, 0.0001529252

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 23
rank avg (pred): 0.054 +- 0.033
mrr vals (pred, true): 0.229, 0.211
batch losses (mrrl, rdl): 0.0031779518, 0.000376826

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 547
rank avg (pred): 0.599 +- 0.233
mrr vals (pred, true): 0.076, 0.023
batch losses (mrrl, rdl): 0.0065032202, 0.0001750142

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 838
rank avg (pred): 0.280 +- 0.126
mrr vals (pred, true): 0.055, 0.051
batch losses (mrrl, rdl): 0.000245369, 0.0005611912

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 124
rank avg (pred): 0.446 +- 0.223
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 3.6e-09, 1.16912e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 552
rank avg (pred): 0.635 +- 0.192
mrr vals (pred, true): 0.049, 0.026
batch losses (mrrl, rdl): 3.0861e-06, 0.0003041717

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 914
rank avg (pred): 0.346 +- 0.186
mrr vals (pred, true): 0.072, 0.021
batch losses (mrrl, rdl): 0.0049420684, 0.0010660269

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 679
rank avg (pred): 0.428 +- 0.202
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0001615083, 1.62e-05

Epoch over!
epoch time: 55.232

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 916
rank avg (pred): 0.642 +- 0.218
mrr vals (pred, true): 0.058, 0.022
batch losses (mrrl, rdl): 0.0005668149, 6.58038e-05

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 252
rank avg (pred): 0.041 +- 0.027
mrr vals (pred, true): 0.256, 0.193
batch losses (mrrl, rdl): 0.0392174758, 0.000573649

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 74
rank avg (pred): 0.058 +- 0.033
mrr vals (pred, true): 0.226, 0.227
batch losses (mrrl, rdl): 2.26633e-05, 0.0003777258

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 225
rank avg (pred): 0.482 +- 0.246
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 8.7351e-06, 6.83218e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 575
rank avg (pred): 0.296 +- 0.154
mrr vals (pred, true): 0.059, 0.041
batch losses (mrrl, rdl): 0.0007781818, 0.0005317385

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 574
rank avg (pred): 0.405 +- 0.222
mrr vals (pred, true): 0.048, 0.036
batch losses (mrrl, rdl): 3.10024e-05, 6.11797e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 710
rank avg (pred): 0.409 +- 0.277
mrr vals (pred, true): 0.051, 0.041
batch losses (mrrl, rdl): 7.9788e-06, 1.07825e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 557
rank avg (pred): 0.703 +- 0.233
mrr vals (pred, true): 0.046, 0.023
batch losses (mrrl, rdl): 0.0001344665, 0.0005808711

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1195
rank avg (pred): 0.518 +- 0.259
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 7.25537e-05, 0.0001283258

Epoch over!
epoch time: 54.055

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 831
rank avg (pred): 0.039 +- 0.040
mrr vals (pred, true): 0.280, 0.314
batch losses (mrrl, rdl): 0.0119386185, 0.0002278328

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 666
rank avg (pred): 0.507 +- 0.241
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 1.58175e-05, 7.47621e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 415
rank avg (pred): 0.493 +- 0.297
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 9.9732e-06, 7.91008e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1178
rank avg (pred): 0.472 +- 0.274
mrr vals (pred, true): 0.051, 0.034
batch losses (mrrl, rdl): 9.1208e-06, 4.09621e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 367
rank avg (pred): 0.396 +- 0.331
mrr vals (pred, true): 0.059, 0.054
batch losses (mrrl, rdl): 0.0008689035, 6.51553e-05

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 653
rank avg (pred): 0.419 +- 0.322
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 1.43244e-05, 2.24702e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 713
rank avg (pred): 0.468 +- 0.277
mrr vals (pred, true): 0.049, 0.037
batch losses (mrrl, rdl): 1.12172e-05, 2.19641e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 1024
rank avg (pred): 0.420 +- 0.252
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 8.5449e-06, 5.7377e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 769
rank avg (pred): 0.468 +- 0.290
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 7.72909e-05, 2.51951e-05

Epoch over!
epoch time: 56.003

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 652
rank avg (pred): 0.447 +- 0.299
mrr vals (pred, true): 0.052, 0.048
batch losses (mrrl, rdl): 2.71614e-05, 9.0738e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 169
rank avg (pred): 0.433 +- 0.264
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.6307e-06, 9.4483e-06

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 505
rank avg (pred): 0.671 +- 0.277
mrr vals (pred, true): 0.051, 0.030
batch losses (mrrl, rdl): 1.42151e-05, 0.0004413545

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 396
rank avg (pred): 0.365 +- 0.259
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0003751215, 9.85816e-05

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 57
rank avg (pred): 0.053 +- 0.034
mrr vals (pred, true): 0.232, 0.243
batch losses (mrrl, rdl): 0.0011459498, 0.0002557948

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 526
rank avg (pred): 0.598 +- 0.265
mrr vals (pred, true): 0.056, 0.024
batch losses (mrrl, rdl): 0.0003958055, 0.0001145124

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 359
rank avg (pred): 0.447 +- 0.258
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 1.9337e-06, 9.991e-07

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 387
rank avg (pred): 0.464 +- 0.281
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 5.77967e-05, 9.7975e-06

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 578
rank avg (pred): 0.427 +- 0.248
mrr vals (pred, true): 0.050, 0.035
batch losses (mrrl, rdl): 1.5912e-06, 2.00912e-05

Epoch over!
epoch time: 55.615

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 479
rank avg (pred): 0.420 +- 0.233
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.779e-07, 5.0458e-06

running batch: 500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 611
rank avg (pred): 0.424 +- 0.232
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 9.67015e-05, 1.43915e-05

running batch: 1000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 596
rank avg (pred): 0.392 +- 0.264
mrr vals (pred, true): 0.056, 0.041
batch losses (mrrl, rdl): 0.0003099472, 4.33808e-05

running batch: 1500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 650
rank avg (pred): 0.425 +- 0.224
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 6.0654e-06, 7.7643e-06

running batch: 2000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 421
rank avg (pred): 0.503 +- 0.257
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 2.566e-07, 0.0001042942

running batch: 2500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 574
rank avg (pred): 0.432 +- 0.245
mrr vals (pred, true): 0.052, 0.036
batch losses (mrrl, rdl): 2.45577e-05, 1.62922e-05

running batch: 3000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 698
rank avg (pred): 0.474 +- 0.274
mrr vals (pred, true): 0.057, 0.053
batch losses (mrrl, rdl): 0.0004440801, 6.41286e-05

running batch: 3500 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 693
rank avg (pred): 0.454 +- 0.213
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 1.5673e-06, 1.01102e-05

running batch: 4000 / 4376 and superbatch(1); data from ComplEx, UMLS, run 2.1, exp 930
rank avg (pred): 0.440 +- 0.202
mrr vals (pred, true): 0.050, 0.035
batch losses (mrrl, rdl): 1.5912e-06, 1.42122e-05

Epoch over!
epoch time: 55.501

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM ComplEx and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.474 +- 0.210
mrr vals (pred, true): 0.049, 0.054

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04471 	 0.02102 	 ~...
    0 	     1 	 0.04471 	 0.02204 	 ~...
    0 	     2 	 0.04471 	 0.02283 	 ~...
    0 	     3 	 0.04471 	 0.02304 	 ~...
    0 	     4 	 0.04471 	 0.02311 	 ~...
   91 	     5 	 0.06176 	 0.02374 	 m..s
    0 	     6 	 0.04471 	 0.02398 	 ~...
    0 	     7 	 0.04471 	 0.02414 	 ~...
    0 	     8 	 0.04471 	 0.02441 	 ~...
    0 	     9 	 0.04471 	 0.02442 	 ~...
    0 	    10 	 0.04471 	 0.02453 	 ~...
    0 	    11 	 0.04471 	 0.02476 	 ~...
    0 	    12 	 0.04471 	 0.02536 	 ~...
    0 	    13 	 0.04471 	 0.02539 	 ~...
    0 	    14 	 0.04471 	 0.02639 	 ~...
   75 	    15 	 0.04950 	 0.02679 	 ~...
    0 	    16 	 0.04471 	 0.02749 	 ~...
   77 	    17 	 0.04960 	 0.02933 	 ~...
   72 	    18 	 0.04945 	 0.03197 	 ~...
   81 	    19 	 0.04971 	 0.03237 	 ~...
   82 	    20 	 0.04979 	 0.03405 	 ~...
   65 	    21 	 0.04936 	 0.03473 	 ~...
   87 	    22 	 0.04998 	 0.03500 	 ~...
   90 	    23 	 0.05007 	 0.03672 	 ~...
   80 	    24 	 0.04968 	 0.03749 	 ~...
   37 	    25 	 0.04886 	 0.03829 	 ~...
   86 	    26 	 0.04992 	 0.03885 	 ~...
   21 	    27 	 0.04811 	 0.03920 	 ~...
   85 	    28 	 0.04989 	 0.03957 	 ~...
   53 	    29 	 0.04909 	 0.03964 	 ~...
   70 	    30 	 0.04940 	 0.03976 	 ~...
   59 	    31 	 0.04930 	 0.04080 	 ~...
   57 	    32 	 0.04925 	 0.04081 	 ~...
   27 	    33 	 0.04846 	 0.04084 	 ~...
   46 	    34 	 0.04898 	 0.04120 	 ~...
   84 	    35 	 0.04987 	 0.04168 	 ~...
   62 	    36 	 0.04933 	 0.04206 	 ~...
   66 	    37 	 0.04939 	 0.04224 	 ~...
   15 	    38 	 0.04794 	 0.04224 	 ~...
   52 	    39 	 0.04907 	 0.04251 	 ~...
   67 	    40 	 0.04939 	 0.04292 	 ~...
   74 	    41 	 0.04947 	 0.04330 	 ~...
   88 	    42 	 0.04998 	 0.04369 	 ~...
   26 	    43 	 0.04838 	 0.04369 	 ~...
   50 	    44 	 0.04905 	 0.04375 	 ~...
   83 	    45 	 0.04986 	 0.04403 	 ~...
   18 	    46 	 0.04807 	 0.04404 	 ~...
   64 	    47 	 0.04935 	 0.04420 	 ~...
   76 	    48 	 0.04958 	 0.04433 	 ~...
   28 	    49 	 0.04854 	 0.04437 	 ~...
   24 	    50 	 0.04836 	 0.04440 	 ~...
   51 	    51 	 0.04907 	 0.04471 	 ~...
   20 	    52 	 0.04810 	 0.04473 	 ~...
   42 	    53 	 0.04893 	 0.04480 	 ~...
   71 	    54 	 0.04941 	 0.04485 	 ~...
   41 	    55 	 0.04893 	 0.04490 	 ~...
   32 	    56 	 0.04873 	 0.04493 	 ~...
   69 	    57 	 0.04940 	 0.04507 	 ~...
   43 	    58 	 0.04894 	 0.04510 	 ~...
   44 	    59 	 0.04894 	 0.04514 	 ~...
   47 	    60 	 0.04898 	 0.04527 	 ~...
   56 	    61 	 0.04920 	 0.04537 	 ~...
   61 	    62 	 0.04933 	 0.04542 	 ~...
   38 	    63 	 0.04889 	 0.04564 	 ~...
   68 	    64 	 0.04940 	 0.04635 	 ~...
   79 	    65 	 0.04963 	 0.04722 	 ~...
   19 	    66 	 0.04809 	 0.04722 	 ~...
   30 	    67 	 0.04865 	 0.04744 	 ~...
   45 	    68 	 0.04896 	 0.04759 	 ~...
   54 	    69 	 0.04911 	 0.04815 	 ~...
   17 	    70 	 0.04807 	 0.04838 	 ~...
   31 	    71 	 0.04872 	 0.04865 	 ~...
   39 	    72 	 0.04890 	 0.04886 	 ~...
   23 	    73 	 0.04819 	 0.04900 	 ~...
   29 	    74 	 0.04858 	 0.04907 	 ~...
   25 	    75 	 0.04838 	 0.04939 	 ~...
   78 	    76 	 0.04961 	 0.04952 	 ~...
   33 	    77 	 0.04873 	 0.04976 	 ~...
   73 	    78 	 0.04945 	 0.04978 	 ~...
   40 	    79 	 0.04893 	 0.05007 	 ~...
   35 	    80 	 0.04881 	 0.05026 	 ~...
   58 	    81 	 0.04929 	 0.05049 	 ~...
   48 	    82 	 0.04899 	 0.05067 	 ~...
   89 	    83 	 0.05000 	 0.05108 	 ~...
   16 	    84 	 0.04802 	 0.05129 	 ~...
   22 	    85 	 0.04815 	 0.05144 	 ~...
   63 	    86 	 0.04933 	 0.05194 	 ~...
   55 	    87 	 0.04917 	 0.05320 	 ~...
   49 	    88 	 0.04901 	 0.05367 	 ~...
   34 	    89 	 0.04874 	 0.05409 	 ~...
   36 	    90 	 0.04883 	 0.05498 	 ~...
   60 	    91 	 0.04932 	 0.05524 	 ~...
   97 	    92 	 0.22835 	 0.17062 	 m..s
   95 	    93 	 0.22153 	 0.18803 	 m..s
  101 	    94 	 0.23786 	 0.19157 	 m..s
   93 	    95 	 0.21523 	 0.20694 	 ~...
   92 	    96 	 0.21317 	 0.20756 	 ~...
  107 	    97 	 0.25857 	 0.20953 	 m..s
   96 	    98 	 0.22422 	 0.21228 	 ~...
  104 	    99 	 0.24280 	 0.21787 	 ~...
  100 	   100 	 0.23740 	 0.21849 	 ~...
   99 	   101 	 0.23613 	 0.22505 	 ~...
  105 	   102 	 0.24904 	 0.22787 	 ~...
  115 	   103 	 0.27858 	 0.22870 	 m..s
  102 	   104 	 0.23966 	 0.23554 	 ~...
  111 	   105 	 0.27024 	 0.24107 	 ~...
  113 	   106 	 0.27769 	 0.24338 	 m..s
  110 	   107 	 0.26901 	 0.24394 	 ~...
  114 	   108 	 0.27804 	 0.24515 	 m..s
  103 	   109 	 0.24272 	 0.24680 	 ~...
   94 	   110 	 0.21807 	 0.24793 	 ~...
   98 	   111 	 0.23324 	 0.25751 	 ~...
  106 	   112 	 0.25539 	 0.26949 	 ~...
  112 	   113 	 0.27673 	 0.27378 	 ~...
  117 	   114 	 0.31398 	 0.27479 	 m..s
  108 	   115 	 0.26428 	 0.28472 	 ~...
  109 	   116 	 0.26640 	 0.29649 	 m..s
  118 	   117 	 0.33251 	 0.29788 	 m..s
  116 	   118 	 0.28223 	 0.30688 	 ~...
  119 	   119 	 0.38013 	 0.32827 	 m..s
  120 	   120 	 0.42060 	 0.52755 	 MISS
==========================================
r_mrr = 0.9814969301223755
r2_mrr = 0.9553652405738831
spearmanr_mrr@5 = 0.8351035714149475
spearmanr_mrr@10 = 0.8760567307472229
spearmanr_mrr@50 = 0.9830196499824524
spearmanr_mrr@100 = 0.9894095063209534
spearmanr_mrr@All = 0.9888092875480652
==========================================
test time: 0.531
Done Testing dataset UMLS
Testing model with KGEM ComplEx and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.568 +- 0.367
mrr vals (pred, true): 0.049, 0.004

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   40 	     0 	 0.04900 	 0.00015 	 m..s
   15 	     1 	 0.04890 	 0.00015 	 m..s
    4 	     2 	 0.04881 	 0.00016 	 m..s
   61 	     3 	 0.04904 	 0.00016 	 m..s
   55 	     4 	 0.04903 	 0.00017 	 m..s
   63 	     5 	 0.04906 	 0.00017 	 m..s
    9 	     6 	 0.04883 	 0.00017 	 m..s
   71 	     7 	 0.04921 	 0.00018 	 m..s
   74 	     8 	 0.04922 	 0.00019 	 m..s
   48 	     9 	 0.04902 	 0.00019 	 m..s
   25 	    10 	 0.04894 	 0.00019 	 m..s
   23 	    11 	 0.04893 	 0.00020 	 m..s
   54 	    12 	 0.04903 	 0.00020 	 m..s
   27 	    13 	 0.04894 	 0.00021 	 m..s
    3 	    14 	 0.04880 	 0.00021 	 m..s
   60 	    15 	 0.04904 	 0.00021 	 m..s
   49 	    16 	 0.04902 	 0.00021 	 m..s
   75 	    17 	 0.04931 	 0.00021 	 m..s
    6 	    18 	 0.04881 	 0.00022 	 m..s
   21 	    19 	 0.04893 	 0.00022 	 m..s
   36 	    20 	 0.04899 	 0.00022 	 m..s
   67 	    21 	 0.04912 	 0.00022 	 m..s
   18 	    22 	 0.04891 	 0.00022 	 m..s
   39 	    23 	 0.04900 	 0.00023 	 m..s
   20 	    24 	 0.04891 	 0.00023 	 m..s
   22 	    25 	 0.04893 	 0.00023 	 m..s
   69 	    26 	 0.04919 	 0.00023 	 m..s
   37 	    27 	 0.04899 	 0.00024 	 m..s
   73 	    28 	 0.04921 	 0.00025 	 m..s
   47 	    29 	 0.04902 	 0.00026 	 m..s
   59 	    30 	 0.04904 	 0.00026 	 m..s
   70 	    31 	 0.04920 	 0.00026 	 m..s
   45 	    32 	 0.04901 	 0.00026 	 m..s
   42 	    33 	 0.04900 	 0.00026 	 m..s
   17 	    34 	 0.04891 	 0.00027 	 m..s
   16 	    35 	 0.04891 	 0.00029 	 m..s
    0 	    36 	 0.04877 	 0.00029 	 m..s
   38 	    37 	 0.04899 	 0.00029 	 m..s
   13 	    38 	 0.04890 	 0.00030 	 m..s
    2 	    39 	 0.04880 	 0.00030 	 m..s
    8 	    40 	 0.04882 	 0.00031 	 m..s
   11 	    41 	 0.04886 	 0.00031 	 m..s
   46 	    42 	 0.04901 	 0.00032 	 m..s
   62 	    43 	 0.04905 	 0.00032 	 m..s
   64 	    44 	 0.04908 	 0.00034 	 m..s
   10 	    45 	 0.04884 	 0.00034 	 m..s
   41 	    46 	 0.04900 	 0.00035 	 m..s
   44 	    47 	 0.04901 	 0.00036 	 m..s
   57 	    48 	 0.04904 	 0.00036 	 m..s
    5 	    49 	 0.04881 	 0.00037 	 m..s
   12 	    50 	 0.04888 	 0.00037 	 m..s
   65 	    51 	 0.04908 	 0.00038 	 m..s
   53 	    52 	 0.04903 	 0.00038 	 m..s
   43 	    53 	 0.04901 	 0.00039 	 m..s
   66 	    54 	 0.04909 	 0.00041 	 m..s
   52 	    55 	 0.04902 	 0.00043 	 m..s
   32 	    56 	 0.04895 	 0.00043 	 m..s
   26 	    57 	 0.04894 	 0.00046 	 m..s
   56 	    58 	 0.04903 	 0.00046 	 m..s
    7 	    59 	 0.04882 	 0.00047 	 m..s
   29 	    60 	 0.04894 	 0.00049 	 m..s
    1 	    61 	 0.04879 	 0.00049 	 m..s
   58 	    62 	 0.04904 	 0.00053 	 m..s
   31 	    63 	 0.04895 	 0.00055 	 m..s
   34 	    64 	 0.04897 	 0.00055 	 m..s
   51 	    65 	 0.04902 	 0.00055 	 m..s
   35 	    66 	 0.04898 	 0.00057 	 m..s
   30 	    67 	 0.04895 	 0.00063 	 m..s
   76 	    68 	 0.05984 	 0.00066 	 m..s
   27 	    69 	 0.04894 	 0.00074 	 m..s
   14 	    70 	 0.04890 	 0.00076 	 m..s
   19 	    71 	 0.04891 	 0.00093 	 m..s
   68 	    72 	 0.04913 	 0.00117 	 m..s
   50 	    73 	 0.04902 	 0.00128 	 m..s
   24 	    74 	 0.04893 	 0.00209 	 m..s
   33 	    75 	 0.04895 	 0.00358 	 m..s
   72 	    76 	 0.04921 	 0.00839 	 m..s
   76 	    77 	 0.05984 	 0.02801 	 m..s
  106 	    78 	 0.07944 	 0.04857 	 m..s
   99 	    79 	 0.07810 	 0.05892 	 ~...
   76 	    80 	 0.05984 	 0.06267 	 ~...
   76 	    81 	 0.05984 	 0.06447 	 ~...
   76 	    82 	 0.05984 	 0.06665 	 ~...
   94 	    83 	 0.07588 	 0.08566 	 ~...
   97 	    84 	 0.07795 	 0.08958 	 ~...
   92 	    85 	 0.07485 	 0.09350 	 ~...
   76 	    86 	 0.05984 	 0.09371 	 m..s
   96 	    87 	 0.07674 	 0.09373 	 ~...
   98 	    88 	 0.07806 	 0.09410 	 ~...
  102 	    89 	 0.07836 	 0.09434 	 ~...
  111 	    90 	 0.22793 	 0.09535 	 MISS
  116 	    91 	 0.22806 	 0.09767 	 MISS
   76 	    92 	 0.05984 	 0.09770 	 m..s
   76 	    93 	 0.05984 	 0.09815 	 m..s
  103 	    94 	 0.07864 	 0.10111 	 ~...
  104 	    95 	 0.07886 	 0.10116 	 ~...
   76 	    96 	 0.05984 	 0.10722 	 m..s
   93 	    97 	 0.07567 	 0.10855 	 m..s
   76 	    98 	 0.05984 	 0.10876 	 m..s
  114 	    99 	 0.22806 	 0.11464 	 MISS
  115 	   100 	 0.22806 	 0.11589 	 MISS
  112 	   101 	 0.22794 	 0.11637 	 MISS
   76 	   102 	 0.05984 	 0.11956 	 m..s
  101 	   103 	 0.07826 	 0.12260 	 m..s
   95 	   104 	 0.07642 	 0.12543 	 m..s
  107 	   105 	 0.08355 	 0.12884 	 m..s
  100 	   106 	 0.07814 	 0.13472 	 m..s
   76 	   107 	 0.05984 	 0.13589 	 m..s
  109 	   108 	 0.11619 	 0.13680 	 ~...
  105 	   109 	 0.07901 	 0.13903 	 m..s
  108 	   110 	 0.08504 	 0.13996 	 m..s
   76 	   111 	 0.05984 	 0.14851 	 m..s
  117 	   112 	 0.22835 	 0.15378 	 m..s
   76 	   113 	 0.05984 	 0.16562 	 MISS
   76 	   114 	 0.05984 	 0.17784 	 MISS
   91 	   115 	 0.06784 	 0.18915 	 MISS
  110 	   116 	 0.22575 	 0.22657 	 ~...
  113 	   117 	 0.22801 	 0.24594 	 ~...
  119 	   118 	 0.22903 	 0.29478 	 m..s
  120 	   119 	 0.22936 	 0.34035 	 MISS
  118 	   120 	 0.22856 	 0.37989 	 MISS
==========================================
r_mrr = 0.7350039482116699
r2_mrr = 0.4383177161216736
spearmanr_mrr@5 = 0.9914104342460632
spearmanr_mrr@10 = 0.9712249636650085
spearmanr_mrr@50 = 0.7992247939109802
spearmanr_mrr@100 = 0.8385657668113708
spearmanr_mrr@All = 0.844825804233551
==========================================
test time: 0.52
Done Testing dataset DBpedia50
Testing model with KGEM ComplEx and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.462 +- 0.305
mrr vals (pred, true): 0.048, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03820 	 0.00102 	 m..s
   76 	     1 	 0.04920 	 0.00115 	 m..s
    0 	     2 	 0.03820 	 0.00155 	 m..s
   36 	     3 	 0.04845 	 0.00226 	 m..s
   78 	     4 	 0.04933 	 0.00240 	 m..s
   42 	     5 	 0.04857 	 0.00241 	 m..s
   80 	     6 	 0.04942 	 0.00259 	 m..s
   38 	     7 	 0.04851 	 0.00261 	 m..s
   54 	     8 	 0.04871 	 0.00264 	 m..s
   57 	     9 	 0.04881 	 0.00272 	 m..s
   90 	    10 	 0.05153 	 0.00282 	 m..s
   59 	    11 	 0.04887 	 0.00288 	 m..s
   60 	    12 	 0.04892 	 0.00294 	 m..s
   82 	    13 	 0.04954 	 0.00301 	 m..s
   53 	    14 	 0.04870 	 0.00312 	 m..s
   84 	    15 	 0.04964 	 0.00313 	 m..s
   40 	    16 	 0.04855 	 0.00323 	 m..s
   64 	    17 	 0.04899 	 0.00327 	 m..s
   29 	    18 	 0.04826 	 0.00330 	 m..s
   46 	    19 	 0.04860 	 0.00331 	 m..s
   86 	    20 	 0.04966 	 0.00332 	 m..s
   22 	    21 	 0.04800 	 0.00341 	 m..s
   85 	    22 	 0.04965 	 0.00345 	 m..s
   15 	    23 	 0.04627 	 0.00348 	 m..s
   88 	    24 	 0.04976 	 0.00350 	 m..s
   37 	    25 	 0.04851 	 0.00359 	 m..s
   83 	    26 	 0.04962 	 0.00364 	 m..s
   45 	    27 	 0.04860 	 0.00368 	 m..s
   25 	    28 	 0.04818 	 0.00370 	 m..s
   23 	    29 	 0.04803 	 0.00371 	 m..s
   19 	    30 	 0.04782 	 0.00371 	 m..s
   89 	    31 	 0.04986 	 0.00373 	 m..s
   44 	    32 	 0.04859 	 0.00376 	 m..s
   51 	    33 	 0.04867 	 0.00377 	 m..s
   65 	    34 	 0.04900 	 0.00380 	 m..s
   49 	    35 	 0.04863 	 0.00383 	 m..s
   35 	    36 	 0.04840 	 0.00384 	 m..s
   43 	    37 	 0.04858 	 0.00402 	 m..s
   67 	    38 	 0.04905 	 0.00404 	 m..s
   26 	    39 	 0.04820 	 0.00407 	 m..s
   74 	    40 	 0.04916 	 0.00410 	 m..s
   77 	    41 	 0.04927 	 0.00413 	 m..s
   27 	    42 	 0.04821 	 0.00413 	 m..s
   31 	    43 	 0.04830 	 0.00415 	 m..s
   81 	    44 	 0.04949 	 0.00415 	 m..s
   70 	    45 	 0.04909 	 0.00416 	 m..s
   21 	    46 	 0.04800 	 0.00418 	 m..s
   68 	    47 	 0.04906 	 0.00428 	 m..s
   52 	    48 	 0.04868 	 0.00436 	 m..s
   34 	    49 	 0.04835 	 0.00438 	 m..s
   18 	    50 	 0.04767 	 0.00447 	 m..s
   72 	    51 	 0.04914 	 0.00448 	 m..s
   87 	    52 	 0.04976 	 0.00448 	 m..s
   50 	    53 	 0.04866 	 0.00451 	 m..s
   69 	    54 	 0.04908 	 0.00452 	 m..s
   58 	    55 	 0.04883 	 0.00452 	 m..s
   71 	    56 	 0.04910 	 0.00456 	 m..s
   33 	    57 	 0.04834 	 0.00458 	 m..s
   75 	    58 	 0.04919 	 0.00458 	 m..s
   30 	    59 	 0.04826 	 0.00459 	 m..s
   61 	    60 	 0.04894 	 0.00459 	 m..s
   48 	    61 	 0.04862 	 0.00464 	 m..s
   28 	    62 	 0.04825 	 0.00467 	 m..s
   17 	    63 	 0.04725 	 0.00474 	 m..s
   55 	    64 	 0.04872 	 0.00476 	 m..s
   73 	    65 	 0.04915 	 0.00477 	 m..s
   41 	    66 	 0.04855 	 0.00484 	 m..s
   16 	    67 	 0.04664 	 0.00486 	 m..s
   56 	    68 	 0.04874 	 0.00489 	 m..s
   32 	    69 	 0.04834 	 0.00504 	 m..s
   24 	    70 	 0.04816 	 0.00509 	 m..s
   63 	    71 	 0.04898 	 0.00518 	 m..s
   39 	    72 	 0.04854 	 0.00523 	 m..s
   20 	    73 	 0.04793 	 0.00525 	 m..s
   47 	    74 	 0.04860 	 0.00529 	 m..s
   62 	    75 	 0.04897 	 0.00529 	 m..s
   79 	    76 	 0.04933 	 0.00545 	 m..s
   66 	    77 	 0.04902 	 0.00587 	 m..s
    0 	    78 	 0.03820 	 0.00694 	 m..s
    0 	    79 	 0.03820 	 0.00832 	 ~...
    0 	    80 	 0.03820 	 0.00850 	 ~...
    0 	    81 	 0.03820 	 0.01061 	 ~...
    0 	    82 	 0.03820 	 0.01105 	 ~...
    0 	    83 	 0.03820 	 0.01402 	 ~...
    0 	    84 	 0.03820 	 0.01940 	 ~...
    0 	    85 	 0.03820 	 0.01944 	 ~...
    0 	    86 	 0.03820 	 0.02017 	 ~...
    0 	    87 	 0.03820 	 0.02355 	 ~...
    0 	    88 	 0.03820 	 0.02763 	 ~...
   91 	    89 	 0.06684 	 0.03153 	 m..s
    0 	    90 	 0.03820 	 0.03475 	 ~...
    0 	    91 	 0.03820 	 0.03631 	 ~...
   92 	    92 	 0.11510 	 0.11373 	 ~...
   97 	    93 	 0.13809 	 0.12715 	 ~...
   94 	    94 	 0.12958 	 0.13249 	 ~...
   96 	    95 	 0.13727 	 0.13596 	 ~...
   93 	    96 	 0.12149 	 0.13713 	 ~...
   95 	    97 	 0.13617 	 0.14215 	 ~...
   98 	    98 	 0.14343 	 0.15237 	 ~...
  102 	    99 	 0.17644 	 0.17117 	 ~...
   99 	   100 	 0.16223 	 0.17596 	 ~...
  101 	   101 	 0.16760 	 0.17992 	 ~...
  100 	   102 	 0.16736 	 0.18174 	 ~...
  106 	   103 	 0.18609 	 0.19140 	 ~...
  105 	   104 	 0.18586 	 0.19484 	 ~...
  107 	   105 	 0.19221 	 0.19542 	 ~...
  118 	   106 	 0.23469 	 0.20171 	 m..s
  116 	   107 	 0.22928 	 0.21046 	 ~...
  112 	   108 	 0.22862 	 0.22857 	 ~...
  119 	   109 	 0.23979 	 0.24025 	 ~...
  110 	   110 	 0.22754 	 0.24802 	 ~...
  104 	   111 	 0.18177 	 0.25405 	 m..s
  103 	   112 	 0.17972 	 0.26233 	 m..s
  108 	   113 	 0.21991 	 0.26465 	 m..s
  111 	   114 	 0.22821 	 0.26660 	 m..s
  120 	   115 	 0.24421 	 0.27020 	 ~...
  115 	   116 	 0.22898 	 0.27192 	 m..s
  113 	   117 	 0.22878 	 0.28276 	 m..s
  109 	   118 	 0.22440 	 0.28454 	 m..s
  114 	   119 	 0.22892 	 0.28931 	 m..s
  117 	   120 	 0.23194 	 0.35668 	 MISS
==========================================
r_mrr = 0.9798469543457031
r2_mrr = 0.8001406788825989
spearmanr_mrr@5 = 0.8600870370864868
spearmanr_mrr@10 = 0.9107347726821899
spearmanr_mrr@50 = 0.992555558681488
spearmanr_mrr@100 = 0.9950570464134216
spearmanr_mrr@All = 0.9946768879890442
==========================================
test time: 0.473
Done Testing dataset CoDExSmall
Testing model with KGEM ComplEx and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.358 +- 0.247
mrr vals (pred, true): 0.049, 0.057

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.04733 	 0.04681 	 ~...
   61 	     1 	 0.05147 	 0.04713 	 ~...
   39 	     2 	 0.04972 	 0.04716 	 ~...
   10 	     3 	 0.04681 	 0.04739 	 ~...
   75 	     4 	 0.06465 	 0.04740 	 ~...
   63 	     5 	 0.05194 	 0.04763 	 ~...
   19 	     6 	 0.04794 	 0.04767 	 ~...
   33 	     7 	 0.04879 	 0.04772 	 ~...
   59 	     8 	 0.05139 	 0.04780 	 ~...
   37 	     9 	 0.04955 	 0.04836 	 ~...
   56 	    10 	 0.05056 	 0.04856 	 ~...
   50 	    11 	 0.05024 	 0.04866 	 ~...
   20 	    12 	 0.04799 	 0.04954 	 ~...
   34 	    13 	 0.04887 	 0.04989 	 ~...
   46 	    14 	 0.05011 	 0.04998 	 ~...
   14 	    15 	 0.04708 	 0.05000 	 ~...
   58 	    16 	 0.05114 	 0.05006 	 ~...
   49 	    17 	 0.05020 	 0.05015 	 ~...
   66 	    18 	 0.05225 	 0.05043 	 ~...
   62 	    19 	 0.05148 	 0.05062 	 ~...
   71 	    20 	 0.05293 	 0.05067 	 ~...
   67 	    21 	 0.05229 	 0.05123 	 ~...
   69 	    22 	 0.05258 	 0.05125 	 ~...
   57 	    23 	 0.05088 	 0.05129 	 ~...
   53 	    24 	 0.05046 	 0.05140 	 ~...
   72 	    25 	 0.05333 	 0.05149 	 ~...
    7 	    26 	 0.04641 	 0.05174 	 ~...
    9 	    27 	 0.04670 	 0.05179 	 ~...
   24 	    28 	 0.04823 	 0.05188 	 ~...
   74 	    29 	 0.05674 	 0.05190 	 ~...
   68 	    30 	 0.05248 	 0.05219 	 ~...
   40 	    31 	 0.04977 	 0.05228 	 ~...
   32 	    32 	 0.04857 	 0.05229 	 ~...
   13 	    33 	 0.04690 	 0.05248 	 ~...
    2 	    34 	 0.04480 	 0.05291 	 ~...
   11 	    35 	 0.04684 	 0.05299 	 ~...
   18 	    36 	 0.04768 	 0.05320 	 ~...
   28 	    37 	 0.04847 	 0.05324 	 ~...
   12 	    38 	 0.04687 	 0.05339 	 ~...
   65 	    39 	 0.05220 	 0.05340 	 ~...
   16 	    40 	 0.04762 	 0.05363 	 ~...
   47 	    41 	 0.05012 	 0.05381 	 ~...
   54 	    42 	 0.05050 	 0.05394 	 ~...
   52 	    43 	 0.05032 	 0.05405 	 ~...
   21 	    44 	 0.04808 	 0.05423 	 ~...
   60 	    45 	 0.05142 	 0.05438 	 ~...
   29 	    46 	 0.04851 	 0.05438 	 ~...
   45 	    47 	 0.05008 	 0.05440 	 ~...
   64 	    48 	 0.05214 	 0.05447 	 ~...
    4 	    49 	 0.04568 	 0.05463 	 ~...
    5 	    50 	 0.04578 	 0.05481 	 ~...
   30 	    51 	 0.04852 	 0.05501 	 ~...
   42 	    52 	 0.04985 	 0.05520 	 ~...
   22 	    53 	 0.04813 	 0.05540 	 ~...
    8 	    54 	 0.04651 	 0.05555 	 ~...
    6 	    55 	 0.04591 	 0.05584 	 ~...
   41 	    56 	 0.04984 	 0.05599 	 ~...
   36 	    57 	 0.04908 	 0.05630 	 ~...
   70 	    58 	 0.05293 	 0.05643 	 ~...
   31 	    59 	 0.04853 	 0.05654 	 ~...
   55 	    60 	 0.05056 	 0.05656 	 ~...
   43 	    61 	 0.04987 	 0.05668 	 ~...
    0 	    62 	 0.04432 	 0.05733 	 ~...
   38 	    63 	 0.04963 	 0.05748 	 ~...
    3 	    64 	 0.04539 	 0.05756 	 ~...
   48 	    65 	 0.05013 	 0.05814 	 ~...
   51 	    66 	 0.05025 	 0.05840 	 ~...
    1 	    67 	 0.04468 	 0.05870 	 ~...
   73 	    68 	 0.05336 	 0.05872 	 ~...
   44 	    69 	 0.04988 	 0.05887 	 ~...
   35 	    70 	 0.04897 	 0.05910 	 ~...
   27 	    71 	 0.04844 	 0.05982 	 ~...
   25 	    72 	 0.04825 	 0.06012 	 ~...
   26 	    73 	 0.04826 	 0.06014 	 ~...
   23 	    74 	 0.04821 	 0.06053 	 ~...
   17 	    75 	 0.04765 	 0.06568 	 ~...
   76 	    76 	 0.22111 	 0.18404 	 m..s
   76 	    77 	 0.22111 	 0.21207 	 ~...
   76 	    78 	 0.22111 	 0.21968 	 ~...
   76 	    79 	 0.22111 	 0.21993 	 ~...
   76 	    80 	 0.22111 	 0.22580 	 ~...
   76 	    81 	 0.22111 	 0.22666 	 ~...
   76 	    82 	 0.22111 	 0.23036 	 ~...
   76 	    83 	 0.22111 	 0.23207 	 ~...
   76 	    84 	 0.22111 	 0.23297 	 ~...
   76 	    85 	 0.22111 	 0.23947 	 ~...
   76 	    86 	 0.22111 	 0.25103 	 ~...
   94 	    87 	 0.28733 	 0.25822 	 ~...
   76 	    88 	 0.22111 	 0.26126 	 m..s
   76 	    89 	 0.22111 	 0.26725 	 m..s
   93 	    90 	 0.28258 	 0.28083 	 ~...
  107 	    91 	 0.32710 	 0.28900 	 m..s
   91 	    92 	 0.26745 	 0.29097 	 ~...
   96 	    93 	 0.29675 	 0.29337 	 ~...
   76 	    94 	 0.22111 	 0.29679 	 m..s
   98 	    95 	 0.30428 	 0.29778 	 ~...
   92 	    96 	 0.27958 	 0.30571 	 ~...
  106 	    97 	 0.32224 	 0.30574 	 ~...
   99 	    98 	 0.30552 	 0.30592 	 ~...
  110 	    99 	 0.33744 	 0.30803 	 ~...
  103 	   100 	 0.31066 	 0.30840 	 ~...
  100 	   101 	 0.30623 	 0.30884 	 ~...
  109 	   102 	 0.33719 	 0.31094 	 ~...
  101 	   103 	 0.30708 	 0.32389 	 ~...
  102 	   104 	 0.31009 	 0.32474 	 ~...
   97 	   105 	 0.30304 	 0.32640 	 ~...
  112 	   106 	 0.34677 	 0.33063 	 ~...
  104 	   107 	 0.31860 	 0.33107 	 ~...
  105 	   108 	 0.32162 	 0.33281 	 ~...
  111 	   109 	 0.33864 	 0.33738 	 ~...
   95 	   110 	 0.29640 	 0.34172 	 m..s
  108 	   111 	 0.33674 	 0.34338 	 ~...
  115 	   112 	 0.39440 	 0.35834 	 m..s
  118 	   113 	 0.44399 	 0.38526 	 m..s
  119 	   114 	 0.46624 	 0.41066 	 m..s
  116 	   115 	 0.41571 	 0.42112 	 ~...
  114 	   116 	 0.36720 	 0.44556 	 m..s
  113 	   117 	 0.36338 	 0.45262 	 m..s
  117 	   118 	 0.42746 	 0.48000 	 m..s
   76 	   119 	 0.22111 	 0.48105 	 MISS
  120 	   120 	 0.49065 	 0.62140 	 MISS
==========================================
r_mrr = 0.972075879573822
r2_mrr = 0.9406576156616211
spearmanr_mrr@5 = 0.8851399421691895
spearmanr_mrr@10 = 0.9360436797142029
spearmanr_mrr@50 = 0.9805545210838318
spearmanr_mrr@100 = 0.9939827919006348
spearmanr_mrr@All = 0.994685709476471
==========================================
test time: 0.657
Done Testing dataset Kinships
total time taken: 850.8623056411743
training time taken: 823.6003384590149
TWIG out ;))
======================================================
------------------------------------------------------
Running a TWIG experiment with tag: DistMult-omit-UMLS
------------------------------------------------------
======================================================
Using random seed: 9815775071984576
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [875, 134, 742, 411, 1167, 958, 1028, 1211, 150, 718, 637, 275, 1091, 643, 629, 852, 1154, 255, 212, 396, 75, 902, 744, 992, 816, 821, 1080, 21, 184, 1191, 917, 540, 1201, 811, 717, 1105, 1047, 966, 818, 4, 1112, 666, 826, 612, 1204, 1027, 468, 921, 1076, 1187, 970, 61, 157, 133, 663, 1122, 533, 955, 39, 620, 310, 557, 900, 798, 652, 943, 459, 413, 609, 280, 885, 336, 258, 360, 145, 1142, 261, 1159, 370, 788, 1119, 960, 762, 922, 667, 750, 882, 692, 466, 1005, 435, 584, 971, 189, 650, 465, 477, 210, 968, 1004, 156, 592, 289, 698, 1063, 801, 438, 196, 909, 454, 284, 20, 1002, 10, 845, 302, 63, 642, 111, 641, 710]
valid_ids (0): []
train_ids (1094): [192, 283, 143, 790, 1013, 442, 653, 956, 286, 361, 593, 1046, 320, 379, 294, 1058, 824, 299, 674, 685, 41, 967, 325, 98, 868, 1061, 638, 817, 1023, 437, 205, 68, 18, 141, 252, 1000, 7, 672, 455, 940, 797, 706, 434, 560, 312, 933, 301, 873, 1208, 1090, 124, 371, 761, 614, 806, 248, 544, 500, 728, 1041, 132, 236, 485, 194, 1188, 43, 528, 1155, 491, 656, 741, 342, 1001, 201, 624, 998, 582, 660, 695, 58, 285, 520, 825, 626, 226, 47, 659, 827, 951, 1021, 858, 1097, 794, 1205, 1086, 552, 768, 756, 439, 920, 514, 28, 241, 224, 1045, 990, 346, 1006, 15, 1181, 463, 963, 899, 253, 5, 225, 259, 804, 1110, 57, 146, 953, 305, 767, 222, 566, 276, 293, 611, 1212, 230, 947, 673, 590, 214, 170, 847, 849, 462, 269, 106, 736, 187, 503, 237, 1166, 260, 74, 701, 884, 478, 1124, 330, 1085, 989, 127, 219, 893, 1031, 565, 152, 8, 238, 897, 670, 743, 1174, 627, 490, 375, 423, 1099, 1060, 623, 517, 972, 242, 568, 757, 829, 169, 78, 957, 17, 746, 887, 733, 978, 995, 245, 792, 1196, 675, 328, 266, 104, 1108, 101, 264, 1134, 168, 452, 333, 343, 965, 1071, 918, 1066, 1109, 107, 382, 591, 340, 209, 724, 595, 550, 327, 114, 1168, 281, 795, 314, 1007, 505, 1193, 730, 843, 740, 373, 1185, 576, 518, 429, 1177, 416, 26, 1114, 834, 108, 1147, 66, 993, 359, 914, 932, 334, 304, 153, 3, 1144, 1149, 380, 1161, 1189, 257, 874, 1111, 1081, 76, 70, 121, 165, 939, 488, 1104, 1126, 410, 807, 389, 33, 878, 693, 908, 408, 791, 1098, 697, 498, 1140, 263, 508, 928, 1143, 147, 839, 598, 708, 1051, 883, 854, 679, 115, 648, 982, 524, 833, 1214, 1107, 158, 1068, 929, 282, 64, 1048, 431, 721, 347, 1103, 97, 217, 91, 527, 604, 159, 916, 577, 919, 548, 1035, 1156, 535, 727, 329, 694, 944, 94, 102, 402, 567, 46, 749, 537, 677, 56, 14, 22, 203, 1164, 54, 1062, 296, 860, 1039, 594, 1018, 686, 1093, 492, 32, 243, 395, 1034, 844, 95, 912, 387, 661, 109, 731, 619, 935, 521, 1101, 1127, 172, 927, 1059, 1121, 483, 578, 772, 617, 946, 1011, 915, 732, 628, 421, 810, 1087, 1199, 950, 1115, 941, 240, 51, 473, 649, 1064, 901, 499, 543, 358, 129, 1169, 1137, 188, 668, 440, 1136, 853, 321, 144, 599, 362, 616, 118, 539, 415, 1025, 554, 763, 425, 181, 140, 880, 213, 898, 996, 148, 100, 381, 930, 867, 755, 525, 249, 290, 1017, 980, 40, 602, 924, 420, 139, 596, 1200, 782, 27, 85, 813, 44, 376, 464, 105, 923, 931, 388, 716, 220, 1129, 1067, 1065, 227, 444, 1198, 546, 316, 783, 199, 662, 601, 600, 417, 722, 303, 345, 879, 1037, 942, 1206, 1050, 861, 979, 703, 622, 93, 311, 23, 553, 393, 441, 889, 707, 631, 625, 62, 211, 547, 446, 218, 770, 288, 719, 96, 689, 729, 457, 467, 403, 246, 223, 778, 401, 430, 1056, 353, 952, 447, 1096, 786, 711, 130, 323, 1130, 1162, 635, 1003, 888, 364, 831, 49, 1209, 809, 449, 207, 1116, 789, 81, 551, 149, 206, 608, 974, 117, 309, 6, 541, 496, 871, 1139, 383, 29, 198, 322, 1088, 865, 277, 549, 926, 112, 270, 925, 658, 268, 961, 164, 45, 1148, 532, 976, 519, 815, 531, 529, 581, 745, 448, 1044, 1089, 183, 191, 1029, 1020, 1030, 369, 587, 128, 799, 244, 1069, 190, 907, 1055, 841, 87, 832, 870, 555, 12, 418, 297, 536, 31, 235, 308, 511, 991, 131, 1179, 997, 605, 182, 231, 775, 610, 671, 973, 647, 186, 80, 1160, 136, 318, 352, 300, 699, 481, 869, 586, 509, 174, 59, 633, 19, 295, 715, 110, 1100, 574, 1170, 808, 954, 313, 751, 851, 964, 16, 428, 588, 645, 73, 1057, 771, 705, 512, 356, 556, 766, 344, 254, 354, 1012, 200, 1180, 848, 372, 338, 351, 412, 690, 480, 779, 267, 1092, 391, 432, 748, 1038, 1053, 167, 726, 945, 739, 399, 1113, 331, 669, 138, 1194, 474, 1043, 1078, 1141, 936, 427, 155, 559, 1132, 513, 368, 903, 71, 977, 580, 969, 1019, 83, 784, 1015, 857, 337, 122, 406, 278, 274, 348, 42, 99, 988, 1033, 683, 1213, 959, 948, 696, 759, 573, 530, 137, 603, 820, 119, 828, 489, 676, 166, 856, 202, 800, 60, 160, 571, 1128, 35, 180, 193, 1135, 229, 702, 84, 793, 764, 753, 630, 422, 495, 911, 796, 640, 655, 234, 319, 700, 866, 583, 460, 651, 89, 458, 949, 639, 981, 291, 589, 1151, 256, 494, 615, 665, 720, 386, 904, 836, 175, 1176, 247, 1153, 1175, 475, 664, 126, 456, 962, 1182, 905, 161, 823, 1049, 414, 1016, 712, 385, 443, 684, 424, 204, 819, 688, 1026, 569, 910, 1173, 678, 1171, 324, 390, 1183, 239, 497, 208, 113, 486, 838, 1207, 232, 265, 994, 1024, 69, 850, 419, 397, 545, 472, 765, 1165, 250, 937, 179, 506, 842, 272, 846, 377, 1202, 934, 233, 1131, 562, 67, 682, 392, 400, 271, 65, 1186, 691, 307, 374, 1094, 738, 872, 1040, 984, 805, 1117, 453, 363, 516, 341, 723, 450, 216, 103, 773, 983, 1106, 975, 1195, 317, 891, 394, 1, 1123, 34, 1118, 896, 1197, 737, 426, 570, 378, 471, 522, 613, 171, 90, 326, 822, 315, 564, 502, 365, 445, 36, 1072, 349, 142, 758, 890, 769, 504, 335, 876, 398, 1163, 1075, 123, 787, 735, 262, 814, 618, 725, 37, 1145, 173, 1178, 621, 120, 55, 177, 52, 713, 1070, 92, 1150, 709, 151, 734, 228, 1184, 1138, 487, 88, 154, 38, 855, 82, 780, 79, 987, 523, 607, 221, 367, 644, 135, 1192, 469, 863, 125, 1008, 1084, 355, 895, 534, 938, 606, 484, 493, 339, 986, 501, 2, 176, 451, 48, 687, 279, 292, 1032, 72, 585, 515, 185, 24, 802, 1095, 646, 840, 298, 482, 53, 11, 86, 77, 704, 1010, 561, 163, 785, 1079, 195, 892, 1054, 654, 404, 1014, 634, 1102, 13, 251, 1190, 835, 1172, 837, 306, 405, 384, 50, 436, 1133, 1082, 558, 657, 877, 886, 25, 178, 197, 479, 476, 830, 632, 162, 366, 774, 760, 999, 714, 563, 1152, 1052, 1077, 859, 777, 507, 864, 538, 680, 1158, 776, 812, 30, 407, 0, 1042, 433, 579, 572, 1203, 781, 470, 461, 215, 1036, 542, 913, 985, 881, 1022, 754, 575, 1146, 9, 273, 526, 1125, 350, 681, 752, 747, 1073, 1120, 510, 116, 862, 1074, 409, 906, 894, 1009, 1210, 636, 332, 597, 1157, 357, 1083, 803, 287]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3007467243737712
the save name prefix for this run is:  chkpt-ID_3007467243737712_tag_DistMult-omit-UMLS
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 678
rank avg (pred): 0.568 +- 0.004
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002153568

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 932
rank avg (pred): 0.556 +- 0.010
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0019061007

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 274
rank avg (pred): 0.105 +- 0.039
mrr vals (pred, true): 0.001, 0.206
batch losses (mrrl, rdl): 0.0, 0.0002021514

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 34
rank avg (pred): 0.104 +- 0.064
mrr vals (pred, true): 0.004, 0.181
batch losses (mrrl, rdl): 0.0, 0.0002762464

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 842
rank avg (pred): 0.508 +- 0.215
mrr vals (pred, true): 0.004, 0.000
batch losses (mrrl, rdl): 0.0, 4.74165e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1143
rank avg (pred): 0.241 +- 0.217
mrr vals (pred, true): 0.062, 0.224
batch losses (mrrl, rdl): 0.0, 5.4615e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 493
rank avg (pred): 0.200 +- 0.210
mrr vals (pred, true): 0.123, 0.262
batch losses (mrrl, rdl): 0.0, 7.33756e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 837
rank avg (pred): 0.444 +- 0.296
mrr vals (pred, true): 0.094, 0.009
batch losses (mrrl, rdl): 0.0, 6.7621e-06

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 671
rank avg (pred): 0.387 +- 0.258
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003133041

Epoch over!
epoch time: 55.407

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1176
rank avg (pred): 0.406 +- 0.258
mrr vals (pred, true): 0.044, 0.140
batch losses (mrrl, rdl): 0.0, 0.0001206433

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 264
rank avg (pred): 0.165 +- 0.217
mrr vals (pred, true): 0.133, 0.369
batch losses (mrrl, rdl): 0.0, 8.8721e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.443 +- 0.307
mrr vals (pred, true): 0.129, 0.000
batch losses (mrrl, rdl): 0.0, 1.67623e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.605 +- 0.392
mrr vals (pred, true): 0.124, 0.000
batch losses (mrrl, rdl): 0.0, 0.0012931174

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 755
rank avg (pred): 0.235 +- 0.241
mrr vals (pred, true): 0.196, 0.156
batch losses (mrrl, rdl): 0.0, 5.14142e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 920
rank avg (pred): 0.635 +- 0.361
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003163696

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 350
rank avg (pred): 0.367 +- 0.265
mrr vals (pred, true): 0.165, 0.159
batch losses (mrrl, rdl): 0.0, 0.0001294891

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 381
rank avg (pred): 0.377 +- 0.258
mrr vals (pred, true): 0.177, 0.149
batch losses (mrrl, rdl): 0.0, 0.0001793226

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 357
rank avg (pred): 0.363 +- 0.261
mrr vals (pred, true): 0.176, 0.135
batch losses (mrrl, rdl): 0.0, 9.10153e-05

Epoch over!
epoch time: 56.435

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.258 +- 0.256
mrr vals (pred, true): 0.194, 0.263
batch losses (mrrl, rdl): 0.0, 3.18604e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 56
rank avg (pred): 0.127 +- 0.183
mrr vals (pred, true): 0.207, 0.198
batch losses (mrrl, rdl): 0.0, 3.35092e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 86
rank avg (pred): 0.386 +- 0.269
mrr vals (pred, true): 0.153, 0.137
batch losses (mrrl, rdl): 0.0, 0.0002230185

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.223 +- 0.255
mrr vals (pred, true): 0.229, 0.228
batch losses (mrrl, rdl): 0.0, 9.54201e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 286
rank avg (pred): 0.111 +- 0.181
mrr vals (pred, true): 0.340, 0.258
batch losses (mrrl, rdl): 0.0, 5.09948e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 850
rank avg (pred): 0.439 +- 0.306
mrr vals (pred, true): 0.166, 0.000
batch losses (mrrl, rdl): 0.0, 4.86578e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 524
rank avg (pred): 0.230 +- 0.248
mrr vals (pred, true): 0.257, 0.202
batch losses (mrrl, rdl): 0.0, 3.51551e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 777
rank avg (pred): 0.425 +- 0.303
mrr vals (pred, true): 0.167, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004575953

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1023
rank avg (pred): 0.384 +- 0.288
mrr vals (pred, true): 0.222, 0.160
batch losses (mrrl, rdl): 0.0, 0.0001870764

Epoch over!
epoch time: 50.5

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 657
rank avg (pred): 0.395 +- 0.283
mrr vals (pred, true): 0.163, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001170017

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 334
rank avg (pred): 0.405 +- 0.289
mrr vals (pred, true): 0.088, 0.158
batch losses (mrrl, rdl): 0.0, 0.0001945764

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 299
rank avg (pred): 0.142 +- 0.226
mrr vals (pred, true): 0.297, 0.210
batch losses (mrrl, rdl): 0.0, 5.0972e-06

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.407 +- 0.290
mrr vals (pred, true): 0.146, 0.000
batch losses (mrrl, rdl): 0.0, 3.32169e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.248 +- 0.252
mrr vals (pred, true): 0.138, 0.228
batch losses (mrrl, rdl): 0.0, 5.32021e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.381 +- 0.294
mrr vals (pred, true): 0.184, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002273383

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 287
rank avg (pred): 0.156 +- 0.220
mrr vals (pred, true): 0.244, 0.260
batch losses (mrrl, rdl): 0.0, 3.18768e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.238 +- 0.268
mrr vals (pred, true): 0.223, 0.296
batch losses (mrrl, rdl): 0.0, 1.80572e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 127
rank avg (pred): 0.359 +- 0.295
mrr vals (pred, true): 0.249, 0.147
batch losses (mrrl, rdl): 0.0, 7.44234e-05

Epoch over!
epoch time: 52.052

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 26
rank avg (pred): 0.115 +- 0.181
mrr vals (pred, true): 0.264, 0.371
batch losses (mrrl, rdl): 0.0, 3.91016e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.382 +- 0.306
mrr vals (pred, true): 0.227, 0.000
batch losses (mrrl, rdl): 0.0, 1.58193e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 443
rank avg (pred): 0.370 +- 0.287
mrr vals (pred, true): 0.188, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001400669

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 312
rank avg (pred): 0.170 +- 0.239
mrr vals (pred, true): 0.276, 0.158
batch losses (mrrl, rdl): 0.0, 2.7026e-06

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 107
rank avg (pred): 0.371 +- 0.293
mrr vals (pred, true): 0.192, 0.165
batch losses (mrrl, rdl): 0.0, 0.0002316684

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 226
rank avg (pred): 0.396 +- 0.285
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001438705

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.381 +- 0.302
mrr vals (pred, true): 0.161, 0.008
batch losses (mrrl, rdl): 0.0, 9.51313e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1117
rank avg (pred): 0.360 +- 0.294
mrr vals (pred, true): 0.252, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001911836

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1128
rank avg (pred): 0.362 +- 0.290
mrr vals (pred, true): 0.201, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001228231

Epoch over!
epoch time: 50.922

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1120
rank avg (pred): 0.369 +- 0.297
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.1606605202, 0.0002110402

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 927
rank avg (pred): 0.498 +- 0.301
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004465024, 0.0001017757

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 126
rank avg (pred): 0.306 +- 0.283
mrr vals (pred, true): 0.091, 0.186
batch losses (mrrl, rdl): 0.0902559608, 2.50546e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 8
rank avg (pred): 0.133 +- 0.212
mrr vals (pred, true): 0.238, 0.342
batch losses (mrrl, rdl): 0.1093643978, 1.7861e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 453
rank avg (pred): 0.258 +- 0.284
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0202675518, 0.0007661904

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.253 +- 0.279
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0177306272, 0.0009361908

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.242 +- 0.279
mrr vals (pred, true): 0.090, 0.160
batch losses (mrrl, rdl): 0.049907621, 4.1688e-06

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 838
rank avg (pred): 0.340 +- 0.250
mrr vals (pred, true): 0.022, 0.006
batch losses (mrrl, rdl): 0.0080909375, 0.0001974955

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 29
rank avg (pred): 0.151 +- 0.238
mrr vals (pred, true): 0.286, 0.208
batch losses (mrrl, rdl): 0.0605347827, 3.4201e-06

Epoch over!
epoch time: 51.786

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 657
rank avg (pred): 0.243 +- 0.274
mrr vals (pred, true): 0.102, 0.001
batch losses (mrrl, rdl): 0.0265521891, 0.0009817029

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 647
rank avg (pred): 0.243 +- 0.273
mrr vals (pred, true): 0.111, 0.147
batch losses (mrrl, rdl): 0.0129801398, 0.0001618681

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 660
rank avg (pred): 0.239 +- 0.265
mrr vals (pred, true): 0.096, 0.001
batch losses (mrrl, rdl): 0.0216161069, 0.0008106453

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 908
rank avg (pred): 0.302 +- 0.303
mrr vals (pred, true): 0.040, 0.009
batch losses (mrrl, rdl): 0.0009423135, 0.0025373125

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 585
rank avg (pred): 0.232 +- 0.259
mrr vals (pred, true): 0.101, 0.152
batch losses (mrrl, rdl): 0.0262940004, 0.0003008612

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 158
rank avg (pred): 0.223 +- 0.259
mrr vals (pred, true): 0.143, 0.165
batch losses (mrrl, rdl): 0.0045360504, 4.09211e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 507
rank avg (pred): 0.167 +- 0.245
mrr vals (pred, true): 0.170, 0.290
batch losses (mrrl, rdl): 0.1447173059, 0.0001346593

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 373
rank avg (pred): 0.233 +- 0.245
mrr vals (pred, true): 0.084, 0.169
batch losses (mrrl, rdl): 0.0730976611, 0.0001495682

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1162
rank avg (pred): 0.231 +- 0.251
mrr vals (pred, true): 0.111, 0.133
batch losses (mrrl, rdl): 0.004665948, 0.0003917103

Epoch over!
epoch time: 52.554

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 529
rank avg (pred): 0.185 +- 0.255
mrr vals (pred, true): 0.136, 0.173
batch losses (mrrl, rdl): 0.0133123472, 6.26e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 782
rank avg (pred): 0.386 +- 0.274
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002698898, 0.0011697738

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 236
rank avg (pred): 0.231 +- 0.239
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0275235921, 0.0011847593

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 793
rank avg (pred): 0.332 +- 0.246
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002168793, 0.0003261328

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 488
rank avg (pred): 0.192 +- 0.284
mrr vals (pred, true): 0.217, 0.262
batch losses (mrrl, rdl): 0.0198582597, 5.20021e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 124
rank avg (pred): 0.270 +- 0.257
mrr vals (pred, true): 0.092, 0.176
batch losses (mrrl, rdl): 0.070178397, 6.12455e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1170
rank avg (pred): 0.235 +- 0.222
mrr vals (pred, true): 0.098, 0.127
batch losses (mrrl, rdl): 0.0088224737, 0.0001779867

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 334
rank avg (pred): 0.297 +- 0.271
mrr vals (pred, true): 0.100, 0.158
batch losses (mrrl, rdl): 0.0335600562, 1.58089e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.242 +- 0.220
mrr vals (pred, true): 0.091, 0.165
batch losses (mrrl, rdl): 0.055153586, 3.71076e-05

Epoch over!
epoch time: 59.126

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.240 +- 0.344
mrr vals (pred, true): 0.256, 0.263
batch losses (mrrl, rdl): 0.0005692604, 1.54148e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 704
rank avg (pred): 0.280 +- 0.239
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.013660362, 0.0007862342

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 946
rank avg (pred): 0.320 +- 0.233
mrr vals (pred, true): 0.077, 0.001
batch losses (mrrl, rdl): 0.0070624896, 0.0002531399

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 202
rank avg (pred): 0.310 +- 0.275
mrr vals (pred, true): 0.107, 0.000
batch losses (mrrl, rdl): 0.0326025374, 0.0004223821

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 575
rank avg (pred): 0.302 +- 0.278
mrr vals (pred, true): 0.091, 0.121
batch losses (mrrl, rdl): 0.0089886002, 6.0678e-06

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 887
rank avg (pred): 0.343 +- 0.227
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0007847761, 0.0003157204

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 573
rank avg (pred): 0.328 +- 0.295
mrr vals (pred, true): 0.104, 0.138
batch losses (mrrl, rdl): 0.0115392227, 5.8897e-06

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 234
rank avg (pred): 0.325 +- 0.281
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.020757664, 0.0002686935

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 474
rank avg (pred): 0.340 +- 0.290
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0108776046, 0.0002902818

Epoch over!
epoch time: 55.67

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1213
rank avg (pred): 0.223 +- 0.193
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0211578459, 0.0010661928

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 935
rank avg (pred): 0.425 +- 0.283
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005594849, 0.0035067501

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1014
rank avg (pred): 0.226 +- 0.190
mrr vals (pred, true): 0.101, 0.168
batch losses (mrrl, rdl): 0.0436662436, 0.0001071744

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1101
rank avg (pred): 0.216 +- 0.191
mrr vals (pred, true): 0.103, 0.135
batch losses (mrrl, rdl): 0.010187733, 9.80797e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 590
rank avg (pred): 0.379 +- 0.321
mrr vals (pred, true): 0.100, 0.159
batch losses (mrrl, rdl): 0.0349331573, 8.52001e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1023
rank avg (pred): 0.211 +- 0.181
mrr vals (pred, true): 0.086, 0.160
batch losses (mrrl, rdl): 0.0549967252, 0.0002168268

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 748
rank avg (pred): 0.145 +- 0.191
mrr vals (pred, true): 0.153, 0.103
batch losses (mrrl, rdl): 0.0253265575, 6.64853e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 965
rank avg (pred): 0.319 +- 0.221
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0039632847, 0.0007722343

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1034
rank avg (pred): 0.217 +- 0.171
mrr vals (pred, true): 0.086, 0.000
batch losses (mrrl, rdl): 0.0129733309, 0.0012210493

Epoch over!
epoch time: 56.491

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 94
rank avg (pred): 0.317 +- 0.262
mrr vals (pred, true): 0.091, 0.193
batch losses (mrrl, rdl): 0.1041891277, 1.86694e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 369
rank avg (pred): 0.279 +- 0.242
mrr vals (pred, true): 0.093, 0.160
batch losses (mrrl, rdl): 0.0447830968, 1.40078e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 908
rank avg (pred): 0.517 +- 0.341
mrr vals (pred, true): 0.053, 0.009
batch losses (mrrl, rdl): 8.97116e-05, 0.0004979059

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 910
rank avg (pred): 0.157 +- 0.178
mrr vals (pred, true): 0.127, 0.027
batch losses (mrrl, rdl): 0.0587018803, 0.0017609337

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.325 +- 0.262
mrr vals (pred, true): 0.086, 0.000
batch losses (mrrl, rdl): 0.0128172422, 0.0002725897

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.255 +- 0.215
mrr vals (pred, true): 0.089, 0.146
batch losses (mrrl, rdl): 0.032643605, 7.1918e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 782
rank avg (pred): 0.241 +- 0.168
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0066907723, 0.0032265808

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 949
rank avg (pred): 0.310 +- 0.220
mrr vals (pred, true): 0.077, 0.001
batch losses (mrrl, rdl): 0.007354971, 0.0005681925

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 526
rank avg (pred): 0.174 +- 0.207
mrr vals (pred, true): 0.137, 0.195
batch losses (mrrl, rdl): 0.0329561569, 0.00026976

Epoch over!
epoch time: 60.085

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 621
rank avg (pred): 0.328 +- 0.266
mrr vals (pred, true): 0.090, 0.129
batch losses (mrrl, rdl): 0.0146034211, 4.44099e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 623
rank avg (pred): 0.273 +- 0.221
mrr vals (pred, true): 0.093, 0.122
batch losses (mrrl, rdl): 0.0084689204, 0.0001431868

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.346 +- 0.283
mrr vals (pred, true): 0.088, 0.175
batch losses (mrrl, rdl): 0.0752091482, 4.25984e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 939
rank avg (pred): 0.293 +- 0.199
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.0102739986, 0.0061901086

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 933
rank avg (pred): 0.380 +- 0.253
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.0052713845, 0.0041852496

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 431
rank avg (pred): 0.389 +- 0.345
mrr vals (pred, true): 0.121, 0.000
batch losses (mrrl, rdl): 0.0509854555, 0.0001558607

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 487
rank avg (pred): 0.299 +- 0.396
mrr vals (pred, true): 0.282, 0.278
batch losses (mrrl, rdl): 0.0002173487, 0.0001344251

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1120
rank avg (pred): 0.167 +- 0.143
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0318067819, 0.0021432482

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.429 +- 0.275
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.3242e-06, 3.61577e-05

Epoch over!
epoch time: 61.606

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 727
rank avg (pred): 0.351 +- 0.298
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0260175355, 0.0003778858

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 631
rank avg (pred): 0.345 +- 0.303
mrr vals (pred, true): 0.086, 0.137
batch losses (mrrl, rdl): 0.026434496, 2.38689e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 546
rank avg (pred): 0.169 +- 0.215
mrr vals (pred, true): 0.140, 0.182
batch losses (mrrl, rdl): 0.0172917955, 0.0001970609

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.085 +- 0.131
mrr vals (pred, true): 0.262, 0.247
batch losses (mrrl, rdl): 0.0024542147, 0.0003093553

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 84
rank avg (pred): 0.398 +- 0.353
mrr vals (pred, true): 0.097, 0.146
batch losses (mrrl, rdl): 0.0238343067, 0.0001365376

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1033
rank avg (pred): 0.358 +- 0.316
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0160191227, 0.000356324

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1160
rank avg (pred): 0.101 +- 0.145
mrr vals (pred, true): 0.223, 0.225
batch losses (mrrl, rdl): 3.32689e-05, 0.0003914158

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.087 +- 0.131
mrr vals (pred, true): 0.251, 0.221
batch losses (mrrl, rdl): 0.0089505995, 0.0001939138

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 433
rank avg (pred): 0.392 +- 0.359
mrr vals (pred, true): 0.099, 0.001
batch losses (mrrl, rdl): 0.0236000419, 0.0001739035

Epoch over!
epoch time: 61.503

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 175
rank avg (pred): 0.416 +- 0.371
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0271148235, 0.0001670279

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 414
rank avg (pred): 0.462 +- 0.392
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0221878514, 4.74339e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 518
rank avg (pred): 0.344 +- 0.406
mrr vals (pred, true): 0.151, 0.190
batch losses (mrrl, rdl): 0.0152721452, 0.0001235522

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 522
rank avg (pred): 0.257 +- 0.338
mrr vals (pred, true): 0.142, 0.191
batch losses (mrrl, rdl): 0.0235227905, 1.21512e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.425 +- 0.387
mrr vals (pred, true): 0.089, 0.144
batch losses (mrrl, rdl): 0.030437246, 0.0003294642

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1174
rank avg (pred): 0.365 +- 0.341
mrr vals (pred, true): 0.083, 0.123
batch losses (mrrl, rdl): 0.0161976703, 9.4103e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 539
rank avg (pred): 0.310 +- 0.414
mrr vals (pred, true): 0.210, 0.210
batch losses (mrrl, rdl): 7.906e-07, 0.0001659883

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 299
rank avg (pred): 0.105 +- 0.159
mrr vals (pred, true): 0.223, 0.210
batch losses (mrrl, rdl): 0.001857128, 5.12054e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.469 +- 0.405
mrr vals (pred, true): 0.091, 0.000
batch losses (mrrl, rdl): 0.0172126871, 0.0001152456

Epoch over!
epoch time: 59.955

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 881
rank avg (pred): 0.439 +- 0.328
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.96786e-05, 1.54515e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1147
rank avg (pred): 0.233 +- 0.321
mrr vals (pred, true): 0.148, 0.206
batch losses (mrrl, rdl): 0.0333105363, 9.7595e-06

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 218
rank avg (pred): 0.430 +- 0.390
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0156787895, 0.0001741144

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.421 +- 0.394
mrr vals (pred, true): 0.121, 0.000
batch losses (mrrl, rdl): 0.0509510711, 6.30178e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 219
rank avg (pred): 0.465 +- 0.401
mrr vals (pred, true): 0.078, 0.000
batch losses (mrrl, rdl): 0.0078483187, 9.42648e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 24
rank avg (pred): 0.359 +- 0.465
mrr vals (pred, true): 0.366, 0.329
batch losses (mrrl, rdl): 0.0132598579, 0.0007899635

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.114 +- 0.191
mrr vals (pred, true): 0.269, 0.221
batch losses (mrrl, rdl): 0.0230211429, 6.98157e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 33
rank avg (pred): 0.068 +- 0.098
mrr vals (pred, true): 0.195, 0.193
batch losses (mrrl, rdl): 2.09706e-05, 0.0002331001

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.058 +- 0.090
mrr vals (pred, true): 0.289, 0.247
batch losses (mrrl, rdl): 0.0182418916, 0.0004887286

Epoch over!
epoch time: 57.485

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.505 +- 0.376
mrr vals (pred, true): 0.065, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04579 	 7e-0500 	 m..s
   10 	     1 	 0.05167 	 0.00011 	 m..s
    9 	     2 	 0.05134 	 0.00012 	 m..s
   46 	     3 	 0.09606 	 0.00013 	 m..s
   90 	     4 	 0.11996 	 0.00015 	 MISS
   33 	     5 	 0.09463 	 0.00015 	 m..s
   80 	     6 	 0.10663 	 0.00017 	 MISS
   16 	     7 	 0.05947 	 0.00018 	 m..s
   38 	     8 	 0.09550 	 0.00018 	 m..s
   28 	     9 	 0.09393 	 0.00018 	 m..s
   76 	    10 	 0.10601 	 0.00019 	 MISS
   31 	    11 	 0.09423 	 0.00019 	 m..s
   63 	    12 	 0.10183 	 0.00019 	 MISS
   34 	    13 	 0.09469 	 0.00019 	 m..s
   13 	    14 	 0.05816 	 0.00020 	 m..s
   15 	    15 	 0.05909 	 0.00021 	 m..s
   69 	    16 	 0.10334 	 0.00021 	 MISS
   67 	    17 	 0.10320 	 0.00021 	 MISS
   12 	    18 	 0.05681 	 0.00021 	 m..s
   66 	    19 	 0.10307 	 0.00021 	 MISS
    0 	    20 	 0.03300 	 0.00022 	 m..s
    1 	    21 	 0.03497 	 0.00023 	 m..s
   62 	    22 	 0.10170 	 0.00023 	 MISS
    4 	    23 	 0.04564 	 0.00025 	 m..s
   35 	    24 	 0.09480 	 0.00025 	 m..s
   58 	    25 	 0.10079 	 0.00026 	 MISS
   85 	    26 	 0.10719 	 0.00026 	 MISS
   43 	    27 	 0.09594 	 0.00026 	 m..s
   44 	    28 	 0.09596 	 0.00027 	 m..s
   65 	    29 	 0.10300 	 0.00030 	 MISS
   50 	    30 	 0.09716 	 0.00031 	 m..s
   79 	    31 	 0.10650 	 0.00031 	 MISS
   17 	    32 	 0.06162 	 0.00032 	 m..s
   56 	    33 	 0.09993 	 0.00033 	 m..s
   22 	    34 	 0.08056 	 0.00034 	 m..s
   47 	    35 	 0.09650 	 0.00035 	 m..s
    3 	    36 	 0.04466 	 0.00035 	 m..s
   18 	    37 	 0.06468 	 0.00036 	 m..s
   87 	    38 	 0.10744 	 0.00037 	 MISS
   84 	    39 	 0.10718 	 0.00038 	 MISS
   26 	    40 	 0.09251 	 0.00041 	 m..s
   86 	    41 	 0.10724 	 0.00041 	 MISS
   78 	    42 	 0.10632 	 0.00044 	 MISS
   11 	    43 	 0.05415 	 0.00046 	 m..s
   74 	    44 	 0.10492 	 0.00071 	 MISS
    6 	    45 	 0.04588 	 0.00071 	 m..s
   61 	    46 	 0.10169 	 0.00075 	 MISS
   75 	    47 	 0.10533 	 0.00076 	 MISS
   41 	    48 	 0.09574 	 0.00111 	 m..s
    2 	    49 	 0.04182 	 0.00118 	 m..s
   14 	    50 	 0.05825 	 0.00149 	 m..s
   73 	    51 	 0.10448 	 0.00171 	 MISS
    7 	    52 	 0.04600 	 0.00457 	 m..s
    8 	    53 	 0.04967 	 0.00826 	 m..s
   19 	    54 	 0.06774 	 0.01645 	 m..s
   24 	    55 	 0.08346 	 0.02535 	 m..s
   20 	    56 	 0.07244 	 0.02556 	 m..s
   49 	    57 	 0.09713 	 0.02598 	 m..s
   29 	    58 	 0.09395 	 0.02684 	 m..s
   30 	    59 	 0.09412 	 0.02819 	 m..s
   89 	    60 	 0.11409 	 0.05864 	 m..s
   39 	    61 	 0.09551 	 0.10737 	 ~...
   42 	    62 	 0.09584 	 0.11647 	 ~...
   57 	    63 	 0.10054 	 0.11677 	 ~...
   92 	    64 	 0.12569 	 0.11903 	 ~...
   40 	    65 	 0.09558 	 0.12273 	 ~...
   68 	    66 	 0.10330 	 0.12485 	 ~...
   45 	    67 	 0.09597 	 0.12649 	 m..s
   59 	    68 	 0.10084 	 0.12985 	 ~...
   53 	    69 	 0.09831 	 0.13393 	 m..s
   36 	    70 	 0.09486 	 0.13636 	 m..s
   91 	    71 	 0.12452 	 0.13671 	 ~...
   48 	    72 	 0.09684 	 0.13683 	 m..s
   64 	    73 	 0.10185 	 0.13690 	 m..s
   21 	    74 	 0.07746 	 0.13800 	 m..s
   23 	    75 	 0.08199 	 0.13961 	 m..s
   51 	    76 	 0.09727 	 0.14050 	 m..s
   93 	    77 	 0.16527 	 0.14152 	 ~...
   55 	    78 	 0.09883 	 0.14441 	 m..s
   37 	    79 	 0.09541 	 0.14679 	 m..s
   72 	    80 	 0.10400 	 0.14843 	 m..s
   83 	    81 	 0.10717 	 0.15324 	 m..s
   70 	    82 	 0.10334 	 0.15457 	 m..s
   32 	    83 	 0.09428 	 0.15489 	 m..s
   81 	    84 	 0.10681 	 0.15711 	 m..s
   71 	    85 	 0.10359 	 0.15762 	 m..s
   60 	    86 	 0.10135 	 0.15768 	 m..s
   77 	    87 	 0.10613 	 0.15939 	 m..s
   52 	    88 	 0.09768 	 0.16170 	 m..s
   88 	    89 	 0.10996 	 0.16303 	 m..s
   96 	    90 	 0.17616 	 0.16385 	 ~...
   27 	    91 	 0.09370 	 0.16490 	 m..s
   82 	    92 	 0.10683 	 0.16809 	 m..s
   54 	    93 	 0.09864 	 0.16945 	 m..s
   25 	    94 	 0.09055 	 0.17246 	 m..s
   98 	    95 	 0.21666 	 0.19391 	 ~...
   95 	    96 	 0.17006 	 0.19747 	 ~...
   94 	    97 	 0.16972 	 0.19765 	 ~...
  101 	    98 	 0.22606 	 0.19952 	 ~...
  103 	    99 	 0.23462 	 0.20354 	 m..s
  109 	   100 	 0.24582 	 0.20836 	 m..s
  102 	   101 	 0.23359 	 0.21099 	 ~...
  105 	   102 	 0.23970 	 0.21432 	 ~...
   99 	   103 	 0.21807 	 0.21750 	 ~...
  100 	   104 	 0.22090 	 0.22441 	 ~...
  104 	   105 	 0.23903 	 0.22741 	 ~...
  112 	   106 	 0.26507 	 0.23125 	 m..s
   97 	   107 	 0.21588 	 0.23254 	 ~...
  108 	   108 	 0.24546 	 0.26095 	 ~...
  113 	   109 	 0.30036 	 0.26422 	 m..s
  106 	   110 	 0.24212 	 0.27032 	 ~...
  107 	   111 	 0.24318 	 0.27050 	 ~...
  110 	   112 	 0.25262 	 0.29470 	 m..s
  111 	   113 	 0.26335 	 0.30319 	 m..s
  119 	   114 	 0.38714 	 0.32274 	 m..s
  116 	   115 	 0.37763 	 0.33015 	 m..s
  120 	   116 	 0.39314 	 0.33456 	 m..s
  117 	   117 	 0.37971 	 0.35340 	 ~...
  115 	   118 	 0.37263 	 0.35651 	 ~...
  114 	   119 	 0.37134 	 0.36131 	 ~...
  118 	   120 	 0.38245 	 0.36606 	 ~...
==========================================
r_mrr = 0.8478534817695618
r2_mrr = 0.631294846534729
spearmanr_mrr@5 = 0.8510327339172363
spearmanr_mrr@10 = 0.9269749522209167
spearmanr_mrr@50 = 0.9789891839027405
spearmanr_mrr@100 = 0.8645064234733582
spearmanr_mrr@All = 0.887319028377533
==========================================
test time: 0.412
Done Testing dataset DBpedia50
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.458 +- 0.343
mrr vals (pred, true): 0.045, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   10 	     0 	 0.03735 	 0.00073 	 m..s
    9 	     1 	 0.03718 	 0.00074 	 m..s
    5 	     2 	 0.03449 	 0.00076 	 m..s
   18 	     3 	 0.04313 	 0.00104 	 m..s
   25 	     4 	 0.05937 	 0.00196 	 m..s
   26 	     5 	 0.06047 	 0.00210 	 m..s
   24 	     6 	 0.05931 	 0.00214 	 m..s
   11 	     7 	 0.03860 	 0.00288 	 m..s
   54 	     8 	 0.09900 	 0.00321 	 m..s
   74 	     9 	 0.11782 	 0.00341 	 MISS
   16 	    10 	 0.04151 	 0.00348 	 m..s
   94 	    11 	 0.12312 	 0.00350 	 MISS
   89 	    12 	 0.11943 	 0.00355 	 MISS
   55 	    13 	 0.09920 	 0.00363 	 m..s
   63 	    14 	 0.11420 	 0.00373 	 MISS
    7 	    15 	 0.03459 	 0.00374 	 m..s
   90 	    16 	 0.11945 	 0.00375 	 MISS
   34 	    17 	 0.07372 	 0.00385 	 m..s
   76 	    18 	 0.11862 	 0.00388 	 MISS
   84 	    19 	 0.11914 	 0.00392 	 MISS
   91 	    20 	 0.11951 	 0.00392 	 MISS
   77 	    21 	 0.11888 	 0.00393 	 MISS
   17 	    22 	 0.04277 	 0.00393 	 m..s
   48 	    23 	 0.09282 	 0.00394 	 m..s
   14 	    24 	 0.04082 	 0.00395 	 m..s
   44 	    25 	 0.08717 	 0.00396 	 m..s
   68 	    26 	 0.11609 	 0.00398 	 MISS
   43 	    27 	 0.08670 	 0.00400 	 m..s
    2 	    28 	 0.03253 	 0.00404 	 ~...
   30 	    29 	 0.07182 	 0.00412 	 m..s
   83 	    30 	 0.11909 	 0.00412 	 MISS
   60 	    31 	 0.10591 	 0.00417 	 MISS
   82 	    32 	 0.11905 	 0.00420 	 MISS
    3 	    33 	 0.03394 	 0.00422 	 ~...
    4 	    34 	 0.03442 	 0.00422 	 m..s
   19 	    35 	 0.04481 	 0.00423 	 m..s
   75 	    36 	 0.11820 	 0.00427 	 MISS
   28 	    37 	 0.07122 	 0.00428 	 m..s
   59 	    38 	 0.10377 	 0.00429 	 m..s
   12 	    39 	 0.04001 	 0.00438 	 m..s
   56 	    40 	 0.10099 	 0.00438 	 m..s
   46 	    41 	 0.08945 	 0.00439 	 m..s
   87 	    42 	 0.11926 	 0.00441 	 MISS
   13 	    43 	 0.04076 	 0.00444 	 m..s
   88 	    44 	 0.11934 	 0.00457 	 MISS
   36 	    45 	 0.07740 	 0.00485 	 m..s
   65 	    46 	 0.11491 	 0.00486 	 MISS
    6 	    47 	 0.03454 	 0.00489 	 ~...
   39 	    48 	 0.08248 	 0.00498 	 m..s
   15 	    49 	 0.04129 	 0.00502 	 m..s
   62 	    50 	 0.11246 	 0.00511 	 MISS
   21 	    51 	 0.05566 	 0.00532 	 m..s
   32 	    52 	 0.07238 	 0.00559 	 m..s
   92 	    53 	 0.11956 	 0.00586 	 MISS
   85 	    54 	 0.11923 	 0.00587 	 MISS
    1 	    55 	 0.02870 	 0.01819 	 ~...
    0 	    56 	 0.02742 	 0.02364 	 ~...
   23 	    57 	 0.05918 	 0.03246 	 ~...
   20 	    58 	 0.04838 	 0.03250 	 ~...
   61 	    59 	 0.11102 	 0.03850 	 m..s
    8 	    60 	 0.03636 	 0.03909 	 ~...
   95 	    61 	 0.13108 	 0.04382 	 m..s
   52 	    62 	 0.09705 	 0.05100 	 m..s
   96 	    63 	 0.14528 	 0.05918 	 m..s
   98 	    64 	 0.14905 	 0.06592 	 m..s
   57 	    65 	 0.10126 	 0.10430 	 ~...
   22 	    66 	 0.05630 	 0.11465 	 m..s
   40 	    67 	 0.08333 	 0.12830 	 m..s
   27 	    68 	 0.06083 	 0.12831 	 m..s
   31 	    69 	 0.07206 	 0.12924 	 m..s
   41 	    70 	 0.08457 	 0.13520 	 m..s
  100 	    71 	 0.16200 	 0.14258 	 ~...
   35 	    72 	 0.07450 	 0.14888 	 m..s
   53 	    73 	 0.09721 	 0.15118 	 m..s
   50 	    74 	 0.09608 	 0.15356 	 m..s
   29 	    75 	 0.07124 	 0.15548 	 m..s
   42 	    76 	 0.08637 	 0.15738 	 m..s
   47 	    77 	 0.08988 	 0.16203 	 m..s
   51 	    78 	 0.09701 	 0.16268 	 m..s
   45 	    79 	 0.08741 	 0.16363 	 m..s
   33 	    80 	 0.07240 	 0.17057 	 m..s
   58 	    81 	 0.10287 	 0.17539 	 m..s
  103 	    82 	 0.20084 	 0.18169 	 ~...
   72 	    83 	 0.11687 	 0.18579 	 m..s
   49 	    84 	 0.09564 	 0.19394 	 m..s
   73 	    85 	 0.11768 	 0.19691 	 m..s
   38 	    86 	 0.08159 	 0.19891 	 MISS
   93 	    87 	 0.12100 	 0.20045 	 m..s
  109 	    88 	 0.23063 	 0.20587 	 ~...
  101 	    89 	 0.19025 	 0.20718 	 ~...
  102 	    90 	 0.19351 	 0.20824 	 ~...
   67 	    91 	 0.11597 	 0.21152 	 m..s
   80 	    92 	 0.11893 	 0.21545 	 m..s
   81 	    93 	 0.11904 	 0.21594 	 m..s
  111 	    94 	 0.23996 	 0.21636 	 ~...
   99 	    95 	 0.15712 	 0.21735 	 m..s
   64 	    96 	 0.11469 	 0.21748 	 MISS
   37 	    97 	 0.07976 	 0.21766 	 MISS
   70 	    98 	 0.11619 	 0.21860 	 MISS
  119 	    99 	 0.27331 	 0.21930 	 m..s
   71 	   100 	 0.11680 	 0.22174 	 MISS
  104 	   101 	 0.22051 	 0.22183 	 ~...
  117 	   102 	 0.26369 	 0.22254 	 m..s
  107 	   103 	 0.22868 	 0.22339 	 ~...
   69 	   104 	 0.11610 	 0.22436 	 MISS
   66 	   105 	 0.11491 	 0.22649 	 MISS
  120 	   106 	 0.28245 	 0.22713 	 m..s
   78 	   107 	 0.11889 	 0.22931 	 MISS
   79 	   108 	 0.11892 	 0.23410 	 MISS
  108 	   109 	 0.22915 	 0.23702 	 ~...
  116 	   110 	 0.25828 	 0.23808 	 ~...
  118 	   111 	 0.26718 	 0.23940 	 ~...
  114 	   112 	 0.25591 	 0.24284 	 ~...
   86 	   113 	 0.11925 	 0.24346 	 MISS
  115 	   114 	 0.25750 	 0.24792 	 ~...
  106 	   115 	 0.22563 	 0.25534 	 ~...
  113 	   116 	 0.25262 	 0.25739 	 ~...
   97 	   117 	 0.14888 	 0.26008 	 MISS
  105 	   118 	 0.22361 	 0.26518 	 m..s
  110 	   119 	 0.23446 	 0.27668 	 m..s
  112 	   120 	 0.24995 	 0.29784 	 m..s
==========================================
r_mrr = 0.6707720160484314
r2_mrr = 0.42619842290878296
spearmanr_mrr@5 = 0.9770834445953369
spearmanr_mrr@10 = 0.9616661071777344
spearmanr_mrr@50 = 0.8451964855194092
spearmanr_mrr@100 = 0.8258781433105469
spearmanr_mrr@All = 0.8604001402854919
==========================================
test time: 0.567
Done Testing dataset CoDExSmall
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.223 +- 0.227
mrr vals (pred, true): 0.078, 0.055

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.09562 	 0.02078 	 m..s
   85 	     1 	 0.22678 	 0.02078 	 MISS
    5 	     2 	 0.03317 	 0.04019 	 ~...
   12 	     3 	 0.04811 	 0.04497 	 ~...
   10 	     4 	 0.03905 	 0.04507 	 ~...
   79 	     5 	 0.12311 	 0.04654 	 m..s
   11 	     6 	 0.04280 	 0.04771 	 ~...
    7 	     7 	 0.03333 	 0.04833 	 ~...
   17 	     8 	 0.05570 	 0.04854 	 ~...
   55 	     9 	 0.11966 	 0.04883 	 m..s
   33 	    10 	 0.11743 	 0.04888 	 m..s
   31 	    11 	 0.11704 	 0.04899 	 m..s
   38 	    12 	 0.11755 	 0.04929 	 m..s
   46 	    13 	 0.11887 	 0.04934 	 m..s
   66 	    14 	 0.12092 	 0.04935 	 m..s
   22 	    15 	 0.11394 	 0.04948 	 m..s
   29 	    16 	 0.11666 	 0.04963 	 m..s
   57 	    17 	 0.11993 	 0.04997 	 m..s
   16 	    18 	 0.05522 	 0.05000 	 ~...
   26 	    19 	 0.11517 	 0.05014 	 m..s
    9 	    20 	 0.03862 	 0.05022 	 ~...
   64 	    21 	 0.12074 	 0.05037 	 m..s
    1 	    22 	 0.02801 	 0.05040 	 ~...
   72 	    23 	 0.12141 	 0.05095 	 m..s
   83 	    24 	 0.13529 	 0.05099 	 m..s
   71 	    25 	 0.12131 	 0.05130 	 m..s
   75 	    26 	 0.12205 	 0.05145 	 m..s
   53 	    27 	 0.11900 	 0.05189 	 m..s
    0 	    28 	 0.02749 	 0.05197 	 ~...
   65 	    29 	 0.12083 	 0.05214 	 m..s
   18 	    30 	 0.06258 	 0.05233 	 ~...
   15 	    31 	 0.05408 	 0.05241 	 ~...
    6 	    32 	 0.03324 	 0.05270 	 ~...
   25 	    33 	 0.11438 	 0.05286 	 m..s
    4 	    34 	 0.03306 	 0.05340 	 ~...
   78 	    35 	 0.12296 	 0.05353 	 m..s
    2 	    36 	 0.03070 	 0.05363 	 ~...
    8 	    37 	 0.03656 	 0.05370 	 ~...
   68 	    38 	 0.12099 	 0.05376 	 m..s
   39 	    39 	 0.11755 	 0.05381 	 m..s
   60 	    40 	 0.12012 	 0.05394 	 m..s
   40 	    41 	 0.11766 	 0.05418 	 m..s
   34 	    42 	 0.11752 	 0.05423 	 m..s
   19 	    43 	 0.07760 	 0.05469 	 ~...
    3 	    44 	 0.03237 	 0.05526 	 ~...
   51 	    45 	 0.11893 	 0.05560 	 m..s
   13 	    46 	 0.05147 	 0.05603 	 ~...
   37 	    47 	 0.11753 	 0.05616 	 m..s
   74 	    48 	 0.12189 	 0.05706 	 m..s
   48 	    49 	 0.11890 	 0.05728 	 m..s
   77 	    50 	 0.12265 	 0.05797 	 m..s
   76 	    51 	 0.12210 	 0.05843 	 m..s
   27 	    52 	 0.11580 	 0.05845 	 m..s
   58 	    53 	 0.12004 	 0.05854 	 m..s
   84 	    54 	 0.16375 	 0.05917 	 MISS
   59 	    55 	 0.12004 	 0.06029 	 m..s
   14 	    56 	 0.05172 	 0.06476 	 ~...
   67 	    57 	 0.12097 	 0.16042 	 m..s
   42 	    58 	 0.11771 	 0.18850 	 m..s
  100 	    59 	 0.42115 	 0.20295 	 MISS
   47 	    60 	 0.11887 	 0.21218 	 m..s
   28 	    61 	 0.11648 	 0.21942 	 MISS
   80 	    62 	 0.12340 	 0.22306 	 m..s
   69 	    63 	 0.12099 	 0.22588 	 MISS
   43 	    64 	 0.11785 	 0.22971 	 MISS
   36 	    65 	 0.11753 	 0.23128 	 MISS
   24 	    66 	 0.11437 	 0.23183 	 MISS
   61 	    67 	 0.12013 	 0.23337 	 MISS
   82 	    68 	 0.12551 	 0.23442 	 MISS
   63 	    69 	 0.12048 	 0.23527 	 MISS
   49 	    70 	 0.11891 	 0.23928 	 MISS
   62 	    71 	 0.12023 	 0.24070 	 MISS
   52 	    72 	 0.11893 	 0.24548 	 MISS
   56 	    73 	 0.11968 	 0.24588 	 MISS
   81 	    74 	 0.12394 	 0.24683 	 MISS
   41 	    75 	 0.11766 	 0.24708 	 MISS
   32 	    76 	 0.11717 	 0.24830 	 MISS
   23 	    77 	 0.11413 	 0.25007 	 MISS
   35 	    78 	 0.11753 	 0.25074 	 MISS
   45 	    79 	 0.11843 	 0.25083 	 MISS
   21 	    80 	 0.11326 	 0.25379 	 MISS
   70 	    81 	 0.12113 	 0.25572 	 MISS
   50 	    82 	 0.11891 	 0.25732 	 MISS
   73 	    83 	 0.12143 	 0.25853 	 MISS
   86 	    84 	 0.25367 	 0.26032 	 ~...
   30 	    85 	 0.11675 	 0.26410 	 MISS
   54 	    86 	 0.11958 	 0.26719 	 MISS
   44 	    87 	 0.11798 	 0.26720 	 MISS
   90 	    88 	 0.36519 	 0.28251 	 m..s
   87 	    89 	 0.28119 	 0.35817 	 m..s
   92 	    90 	 0.37954 	 0.36122 	 ~...
   89 	    91 	 0.29692 	 0.36367 	 m..s
   98 	    92 	 0.40881 	 0.37923 	 ~...
   88 	    93 	 0.28214 	 0.38496 	 MISS
   91 	    94 	 0.37339 	 0.38655 	 ~...
   93 	    95 	 0.39334 	 0.38735 	 ~...
   94 	    96 	 0.39407 	 0.39255 	 ~...
   96 	    97 	 0.39948 	 0.39308 	 ~...
   95 	    98 	 0.39613 	 0.40034 	 ~...
   97 	    99 	 0.39990 	 0.41107 	 ~...
   99 	   100 	 0.42110 	 0.41283 	 ~...
  107 	   101 	 0.43804 	 0.41396 	 ~...
  116 	   102 	 0.44521 	 0.42791 	 ~...
  103 	   103 	 0.43326 	 0.42947 	 ~...
  111 	   104 	 0.44083 	 0.43176 	 ~...
  105 	   105 	 0.43556 	 0.43538 	 ~...
  108 	   106 	 0.43839 	 0.43581 	 ~...
  118 	   107 	 0.44699 	 0.43597 	 ~...
  115 	   108 	 0.44506 	 0.43849 	 ~...
  106 	   109 	 0.43713 	 0.43934 	 ~...
  119 	   110 	 0.44976 	 0.43991 	 ~...
  113 	   111 	 0.44212 	 0.44077 	 ~...
  102 	   112 	 0.43062 	 0.44327 	 ~...
  110 	   113 	 0.44022 	 0.44335 	 ~...
  109 	   114 	 0.43982 	 0.44352 	 ~...
  114 	   115 	 0.44497 	 0.44770 	 ~...
  112 	   116 	 0.44115 	 0.44800 	 ~...
  101 	   117 	 0.42899 	 0.44804 	 ~...
  117 	   118 	 0.44696 	 0.45054 	 ~...
  120 	   119 	 0.45112 	 0.45085 	 ~...
  104 	   120 	 0.43491 	 0.45465 	 ~...
==========================================
r_mrr = 0.8695406913757324
r2_mrr = 0.7501780986785889
spearmanr_mrr@5 = 0.8892636299133301
spearmanr_mrr@10 = 0.9387085437774658
spearmanr_mrr@50 = 0.9726234674453735
spearmanr_mrr@100 = 0.8857810497283936
spearmanr_mrr@All = 0.9088715314865112
==========================================
test time: 0.517
Done Testing dataset Kinships
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.448 +- 0.338
mrr vals (pred, true): 0.039, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.02504 	 8e-0500 	 ~...
    8 	     1 	 0.02775 	 0.00037 	 ~...
    9 	     2 	 0.02893 	 0.00038 	 ~...
    7 	     3 	 0.02519 	 0.00049 	 ~...
   65 	     4 	 0.05629 	 0.00049 	 m..s
   82 	     5 	 0.05742 	 0.00051 	 m..s
    3 	     6 	 0.02427 	 0.00052 	 ~...
   41 	     7 	 0.05452 	 0.00052 	 m..s
   10 	     8 	 0.02917 	 0.00053 	 ~...
   32 	     9 	 0.05405 	 0.00055 	 m..s
    0 	    10 	 0.01666 	 0.00056 	 ~...
   74 	    11 	 0.05703 	 0.00059 	 m..s
   17 	    12 	 0.03639 	 0.00059 	 m..s
   86 	    13 	 0.06479 	 0.00060 	 m..s
   69 	    14 	 0.05643 	 0.00061 	 m..s
   76 	    15 	 0.05717 	 0.00062 	 m..s
   53 	    16 	 0.05541 	 0.00063 	 m..s
   26 	    17 	 0.05346 	 0.00063 	 m..s
   50 	    18 	 0.05529 	 0.00063 	 m..s
   12 	    19 	 0.03289 	 0.00063 	 m..s
   61 	    20 	 0.05617 	 0.00063 	 m..s
    4 	    21 	 0.02494 	 0.00064 	 ~...
   58 	    22 	 0.05597 	 0.00064 	 m..s
   70 	    23 	 0.05644 	 0.00064 	 m..s
   78 	    24 	 0.05719 	 0.00065 	 m..s
    1 	    25 	 0.01788 	 0.00065 	 ~...
   80 	    26 	 0.05740 	 0.00066 	 m..s
   63 	    27 	 0.05624 	 0.00066 	 m..s
   11 	    28 	 0.03096 	 0.00067 	 m..s
   42 	    29 	 0.05455 	 0.00067 	 m..s
   48 	    30 	 0.05500 	 0.00067 	 m..s
   72 	    31 	 0.05675 	 0.00068 	 m..s
   33 	    32 	 0.05411 	 0.00070 	 m..s
   56 	    33 	 0.05581 	 0.00071 	 m..s
   27 	    34 	 0.05360 	 0.00073 	 m..s
   45 	    35 	 0.05455 	 0.00073 	 m..s
   30 	    36 	 0.05382 	 0.00074 	 m..s
   16 	    37 	 0.03483 	 0.00076 	 m..s
   13 	    38 	 0.03388 	 0.00077 	 m..s
   18 	    39 	 0.03876 	 0.00078 	 m..s
   43 	    40 	 0.05455 	 0.00078 	 m..s
    2 	    41 	 0.02234 	 0.00080 	 ~...
   67 	    42 	 0.05633 	 0.00081 	 m..s
   15 	    43 	 0.03456 	 0.00083 	 m..s
   36 	    44 	 0.05419 	 0.00084 	 m..s
   77 	    45 	 0.05717 	 0.00087 	 m..s
   51 	    46 	 0.05532 	 0.00089 	 m..s
   14 	    47 	 0.03394 	 0.00089 	 m..s
    6 	    48 	 0.02511 	 0.00097 	 ~...
   62 	    49 	 0.05621 	 0.00099 	 m..s
   22 	    50 	 0.05118 	 0.00099 	 m..s
   29 	    51 	 0.05366 	 0.00103 	 m..s
   37 	    52 	 0.05426 	 0.00107 	 m..s
   24 	    53 	 0.05335 	 0.00107 	 m..s
   85 	    54 	 0.05806 	 0.00438 	 m..s
   20 	    55 	 0.04422 	 0.00464 	 m..s
   19 	    56 	 0.04004 	 0.00541 	 m..s
   88 	    57 	 0.06751 	 0.01350 	 m..s
   89 	    58 	 0.07078 	 0.01500 	 m..s
   87 	    59 	 0.06732 	 0.01958 	 m..s
   81 	    60 	 0.05741 	 0.03790 	 ~...
   64 	    61 	 0.05624 	 0.04300 	 ~...
   47 	    62 	 0.05488 	 0.05072 	 ~...
   60 	    63 	 0.05617 	 0.05080 	 ~...
   90 	    64 	 0.09315 	 0.05487 	 m..s
   73 	    65 	 0.05699 	 0.05544 	 ~...
   39 	    66 	 0.05441 	 0.05563 	 ~...
   66 	    67 	 0.05631 	 0.05576 	 ~...
   49 	    68 	 0.05503 	 0.05693 	 ~...
   40 	    69 	 0.05445 	 0.05884 	 ~...
   68 	    70 	 0.05634 	 0.05963 	 ~...
   84 	    71 	 0.05746 	 0.06035 	 ~...
   83 	    72 	 0.05742 	 0.06083 	 ~...
   52 	    73 	 0.05534 	 0.06143 	 ~...
   31 	    74 	 0.05384 	 0.06258 	 ~...
   35 	    75 	 0.05416 	 0.06298 	 ~...
   54 	    76 	 0.05547 	 0.06397 	 ~...
   57 	    77 	 0.05586 	 0.06446 	 ~...
   59 	    78 	 0.05612 	 0.06471 	 ~...
   38 	    79 	 0.05438 	 0.06650 	 ~...
   44 	    80 	 0.05455 	 0.07014 	 ~...
   25 	    81 	 0.05345 	 0.07170 	 ~...
   75 	    82 	 0.05704 	 0.07304 	 ~...
   21 	    83 	 0.04965 	 0.07431 	 ~...
   46 	    84 	 0.05465 	 0.07431 	 ~...
   34 	    85 	 0.05415 	 0.07507 	 ~...
   79 	    86 	 0.05719 	 0.07637 	 ~...
   55 	    87 	 0.05555 	 0.07694 	 ~...
   23 	    88 	 0.05319 	 0.07720 	 ~...
   28 	    89 	 0.05365 	 0.07964 	 ~...
   94 	    90 	 0.13978 	 0.13046 	 ~...
  101 	    91 	 0.18656 	 0.13802 	 m..s
   71 	    92 	 0.05660 	 0.13825 	 m..s
   91 	    93 	 0.11878 	 0.14374 	 ~...
   95 	    94 	 0.14460 	 0.15264 	 ~...
   92 	    95 	 0.12039 	 0.15453 	 m..s
  104 	    96 	 0.19572 	 0.15826 	 m..s
  105 	    97 	 0.19647 	 0.16345 	 m..s
   96 	    98 	 0.16891 	 0.17141 	 ~...
   97 	    99 	 0.17711 	 0.17490 	 ~...
   99 	   100 	 0.18239 	 0.18313 	 ~...
   98 	   101 	 0.17977 	 0.18394 	 ~...
   93 	   102 	 0.13612 	 0.18435 	 m..s
  109 	   103 	 0.20442 	 0.19177 	 ~...
  112 	   104 	 0.22201 	 0.20101 	 ~...
  107 	   105 	 0.20074 	 0.20293 	 ~...
  106 	   106 	 0.19914 	 0.21545 	 ~...
  103 	   107 	 0.19074 	 0.22141 	 m..s
  111 	   108 	 0.22113 	 0.22369 	 ~...
  100 	   109 	 0.18368 	 0.22507 	 m..s
  102 	   110 	 0.18956 	 0.23436 	 m..s
  108 	   111 	 0.20280 	 0.23907 	 m..s
  110 	   112 	 0.21057 	 0.24123 	 m..s
  113 	   113 	 0.25557 	 0.26176 	 ~...
  118 	   114 	 0.30998 	 0.27544 	 m..s
  114 	   115 	 0.29430 	 0.29842 	 ~...
  115 	   116 	 0.29752 	 0.29885 	 ~...
  119 	   117 	 0.31406 	 0.29939 	 ~...
  116 	   118 	 0.30221 	 0.30026 	 ~...
  117 	   119 	 0.30883 	 0.30201 	 ~...
  120 	   120 	 0.32271 	 0.30759 	 ~...
==========================================
r_mrr = 0.9446106553077698
r2_mrr = 0.835873007774353
spearmanr_mrr@5 = 0.9496352076530457
spearmanr_mrr@10 = 0.978808581829071
spearmanr_mrr@50 = 0.9923720955848694
spearmanr_mrr@100 = 0.9591426253318787
spearmanr_mrr@All = 0.9636365175247192
==========================================
test time: 0.524
Done Testing dataset OpenEA
total time taken: 881.2735533714294
training time taken: 843.7820017337799
TWIG out ;))
============================================================
------------------------------------------------------------
Running a TWIG experiment with tag: DistMult-omit-CoDExSmall
------------------------------------------------------------
============================================================
Using random seed: 4000941416119963
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Loading UMLS...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1094, 147, 320, 142, 506, 1023, 810, 713, 62, 1204, 427, 849, 212, 791, 5, 946, 207, 1054, 1128, 297, 125, 445, 786, 96, 883, 391, 205, 1134, 741, 197, 854, 9, 378, 797, 896, 558, 411, 394, 728, 359, 103, 938, 1205, 1146, 811, 665, 331, 356, 685, 634, 588, 602, 739, 476, 573, 50, 334, 869, 508, 984, 547, 0, 989, 526, 178, 798, 309, 779, 592, 184, 538, 599, 1143, 137, 208, 863, 122, 1151, 1012, 708, 670, 789, 1164, 613, 318, 637, 595, 1132, 473, 305, 1191, 598, 432, 518, 589, 390, 764, 175, 232, 1214, 1013, 584, 876, 336, 512, 995, 987, 783, 364, 53, 307, 698, 1212, 395, 493, 1050, 702, 145, 65, 644, 504]
valid_ids (0): []
train_ids (1094): [958, 695, 1001, 626, 957, 927, 884, 715, 673, 451, 1207, 842, 605, 1149, 278, 824, 213, 420, 867, 1127, 1138, 294, 788, 667, 859, 78, 174, 1007, 209, 1015, 316, 777, 256, 983, 74, 1091, 509, 1187, 240, 214, 1048, 853, 262, 397, 326, 1019, 381, 725, 734, 233, 736, 461, 447, 1161, 724, 1086, 159, 793, 1133, 991, 976, 152, 11, 1157, 204, 1044, 116, 446, 366, 1063, 244, 1178, 1040, 941, 819, 274, 1024, 136, 1163, 1030, 462, 693, 164, 139, 550, 37, 844, 229, 773, 194, 659, 195, 45, 1136, 468, 873, 671, 263, 118, 259, 236, 1072, 338, 856, 928, 857, 1066, 690, 689, 998, 40, 969, 60, 295, 254, 1110, 80, 615, 14, 638, 276, 1062, 631, 1039, 409, 286, 535, 800, 183, 745, 533, 444, 459, 94, 603, 1009, 1190, 1162, 633, 910, 73, 433, 249, 69, 729, 238, 1100, 648, 1192, 947, 483, 34, 466, 932, 960, 448, 683, 807, 865, 21, 1145, 943, 120, 805, 523, 1017, 1210, 162, 920, 455, 950, 972, 70, 234, 953, 303, 653, 796, 760, 157, 862, 597, 242, 351, 886, 29, 19, 109, 128, 396, 66, 918, 292, 1109, 703, 945, 1115, 1158, 58, 426, 130, 271, 753, 510, 84, 488, 930, 948, 314, 112, 712, 581, 536, 746, 1087, 716, 401, 1103, 88, 864, 994, 921, 1045, 41, 1076, 1002, 383, 325, 940, 556, 99, 161, 1025, 955, 986, 61, 344, 1020, 596, 990, 678, 687, 1106, 731, 1092, 108, 481, 1167, 169, 1194, 1154, 382, 590, 710, 226, 377, 1203, 289, 911, 1022, 67, 701, 430, 1034, 12, 1099, 850, 482, 68, 220, 419, 146, 570, 42, 388, 434, 330, 442, 651, 1061, 301, 892, 903, 821, 26, 1147, 652, 771, 1046, 345, 642, 569, 1021, 1200, 600, 770, 549, 369, 680, 1206, 1123, 269, 260, 349, 718, 422, 532, 1170, 341, 440, 181, 537, 1126, 202, 1104, 804, 935, 13, 1000, 429, 469, 664, 1108, 17, 416, 1064, 565, 121, 568, 282, 840, 464, 614, 553, 834, 845, 563, 491, 982, 580, 198, 606, 980, 949, 475, 641, 170, 747, 500, 454, 222, 1179, 879, 216, 732, 934, 769, 266, 835, 726, 79, 328, 917, 417, 387, 129, 1047, 425, 578, 1031, 474, 350, 296, 135, 30, 404, 196, 85, 77, 315, 348, 424, 23, 979, 313, 806, 907, 252, 705, 1137, 355, 616, 924, 1119, 110, 10, 933, 898, 571, 636, 792, 1174, 258, 38, 1112, 221, 607, 929, 185, 815, 457, 585, 403, 682, 93, 838, 399, 179, 49, 87, 1043, 794, 211, 988, 1165, 153, 35, 304, 201, 505, 265, 374, 711, 277, 1, 963, 308, 102, 625, 1006, 993, 143, 342, 300, 357, 189, 962, 354, 719, 163, 577, 1144, 386, 384, 98, 514, 893, 942, 6, 219, 182, 72, 738, 978, 477, 435, 1069, 894, 16, 101, 961, 371, 1172, 215, 609, 158, 327, 25, 1183, 751, 594, 756, 273, 629, 1196, 618, 1098, 191, 661, 439, 372, 1135, 1052, 361, 310, 467, 496, 1156, 623, 279, 915, 543, 104, 696, 8, 81, 1038, 24, 1213, 802, 758, 511, 936, 1083, 826, 492, 288, 522, 539, 83, 490, 720, 704, 754, 1140, 1182, 900, 923, 954, 752, 1097, 239, 1105, 56, 306, 803, 833, 1085, 329, 970, 250, 255, 591, 302, 180, 380, 567, 321, 133, 575, 192, 187, 766, 985, 97, 428, 76, 645, 132, 400, 166, 612, 319, 1208, 1075, 1152, 203, 33, 847, 311, 379, 582, 902, 525, 1197, 675, 55, 676, 604, 405, 437, 27, 373, 458, 1090, 317, 843, 981, 640, 529, 71, 587, 32, 546, 1011, 370, 551, 1005, 1125, 507, 775, 620, 322, 3, 270, 1029, 332, 155, 828, 407, 346, 1193, 748, 126, 593, 186, 486, 471, 438, 421, 1160, 937, 662, 740, 333, 951, 1093, 247, 552, 1095, 882, 31, 997, 360, 1088, 1079, 1209, 755, 767, 261, 1176, 790, 1130, 168, 199, 684, 1089, 959, 822, 156, 1033, 138, 709, 131, 173, 347, 275, 63, 908, 889, 1077, 367, 763, 1131, 414, 105, 837, 545, 28, 513, 2, 964, 223, 875, 733, 820, 852, 489, 285, 916, 15, 555, 813, 628, 343, 848, 887, 646, 1071, 393, 774, 619, 851, 721, 564, 688, 290, 1150, 785, 1065, 881, 906, 742, 812, 47, 1188, 799, 999, 737, 601, 141, 494, 829, 630, 691, 967, 54, 973, 1148, 776, 465, 218, 974, 231, 926, 106, 272, 365, 1041, 450, 880, 431, 20, 611, 808, 697, 502, 1068, 358, 666, 7, 324, 237, 717, 217, 524, 576, 554, 154, 148, 408, 860, 866, 1027, 759, 1117, 1018, 795, 858, 825, 1202, 1057, 541, 874, 579, 727, 694, 193, 971, 235, 780, 36, 1004, 562, 692, 398, 127, 501, 1055, 134, 368, 817, 931, 1056, 89, 520, 1169, 323, 287, 251, 647, 521, 608, 1032, 965, 188, 210, 1102, 762, 52, 743, 114, 1042, 782, 352, 1201, 1078, 516, 165, 1124, 861, 257, 140, 460, 339, 498, 413, 519, 706, 669, 1181, 714, 830, 1111, 658, 171, 478, 392, 919, 1053, 484, 160, 51, 952, 905, 586, 264, 298, 1189, 1036, 353, 1199, 528, 768, 385, 561, 735, 679, 43, 200, 456, 855, 530, 750, 632, 1096, 293, 1014, 784, 95, 1035, 113, 1081, 284, 635, 827, 487, 787, 224, 622, 245, 668, 968, 228, 527, 836, 899, 925, 778, 479, 1186, 1101, 177, 176, 639, 1155, 1060, 1067, 283, 801, 4, 660, 1139, 299, 649, 470, 150, 59, 337, 656, 1028, 674, 681, 1168, 643, 686, 48, 761, 291, 412, 453, 583, 912, 410, 1184, 1173, 64, 744, 86, 624, 149, 557, 657, 1074, 655, 1084, 1129, 574, 1051, 441, 700, 654, 966, 627, 90, 831, 677, 809, 119, 499, 1026, 389, 977, 463, 100, 1198, 1118, 1171, 312, 814, 749, 888, 1010, 1166, 517, 996, 895, 243, 1185, 335, 39, 816, 230, 248, 542, 267, 663, 885, 757, 449, 939, 617, 621, 375, 111, 1114, 117, 877, 1073, 540, 443, 846, 1142, 225, 515, 57, 531, 897, 495, 1180, 1037, 891, 913, 841, 227, 1195, 544, 1058, 878, 123, 992, 480, 418, 75, 1159, 650, 772, 1175, 452, 115, 363, 1211, 1121, 868, 956, 246, 415, 268, 253, 280, 871, 423, 548, 1082, 151, 91, 914, 1116, 572, 1070, 82, 832, 472, 1008, 1059, 376, 1016, 206, 340, 241, 281, 497, 904, 560, 890, 1122, 44, 818, 406, 172, 870, 1080, 1120, 144, 22, 559, 1107, 909, 107, 975, 1153, 503, 1141, 672, 872, 922, 436, 485, 765, 18, 723, 190, 1003, 1177, 167, 534, 901, 124, 823, 707, 699, 46, 944, 722, 730, 362, 610, 781, 1113, 839, 402, 566, 92, 1049]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4437506816128977
the save name prefix for this run is:  chkpt-ID_4437506816128977_tag_DistMult-omit-CoDExSmall
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1'], 'UMLS': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 100
rank avg (pred): 0.491 +- 0.008
mrr vals (pred, true): 0.000, 0.200
batch losses (mrrl, rdl): 0.0, 0.0006362324

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 999
rank avg (pred): 0.457 +- 0.015
mrr vals (pred, true): 0.000, 0.164
batch losses (mrrl, rdl): 0.0, 0.0005145249

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 772
rank avg (pred): 0.515 +- 0.061
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0006468192

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 32
rank avg (pred): 0.193 +- 0.302
mrr vals (pred, true): 0.001, 0.182
batch losses (mrrl, rdl): 0.0, 3.28292e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 274
rank avg (pred): 0.159 +- 0.307
mrr vals (pred, true): 0.002, 0.206
batch losses (mrrl, rdl): 0.0, 1.81797e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 40
rank avg (pred): 0.178 +- 0.332
mrr vals (pred, true): 0.003, 0.228
batch losses (mrrl, rdl): 0.0, 2.4654e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 974
rank avg (pred): 0.121 +- 0.252
mrr vals (pred, true): 0.002, 0.301
batch losses (mrrl, rdl): 0.0, 1.91068e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 847
rank avg (pred): 0.524 +- 0.279
mrr vals (pred, true): 0.000, 0.017
batch losses (mrrl, rdl): 0.0, 0.0004041699

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 922
rank avg (pred): 0.558 +- 0.263
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004782523

Epoch over!
epoch time: 57.925

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 21
rank avg (pred): 0.235 +- 0.365
mrr vals (pred, true): 0.001, 0.323
batch losses (mrrl, rdl): 0.0, 0.0001818352

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1042
rank avg (pred): 0.469 +- 0.324
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 7.28317e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1197
rank avg (pred): 0.488 +- 0.337
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 3.18545e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 785
rank avg (pred): 0.576 +- 0.279
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 8.72312e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 880
rank avg (pred): 0.494 +- 0.304
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.0676e-06

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 871
rank avg (pred): 0.544 +- 0.287
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 2.03719e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 131
rank avg (pred): 0.419 +- 0.312
mrr vals (pred, true): 0.000, 0.156
batch losses (mrrl, rdl): 0.0, 0.0002132873

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 816
rank avg (pred): 0.255 +- 0.324
mrr vals (pred, true): 0.001, 0.026
batch losses (mrrl, rdl): 0.0, 0.0004090851

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 734
rank avg (pred): 0.262 +- 0.317
mrr vals (pred, true): 0.000, 0.010
batch losses (mrrl, rdl): 0.0, 2.41064e-05

Epoch over!
epoch time: 60.214

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 650
rank avg (pred): 0.458 +- 0.362
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.45148e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1067
rank avg (pred): 0.116 +- 0.262
mrr vals (pred, true): 0.008, 0.312
batch losses (mrrl, rdl): 0.0, 3.99252e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 340
rank avg (pred): 0.415 +- 0.345
mrr vals (pred, true): 0.000, 0.176
batch losses (mrrl, rdl): 0.0, 7.20029e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 370
rank avg (pred): 0.438 +- 0.346
mrr vals (pred, true): 0.000, 0.153
batch losses (mrrl, rdl): 0.0, 0.00025932

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1002
rank avg (pred): 0.460 +- 0.306
mrr vals (pred, true): 0.000, 0.165
batch losses (mrrl, rdl): 0.0, 0.0004809271

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 348
rank avg (pred): 0.415 +- 0.321
mrr vals (pred, true): 0.000, 0.168
batch losses (mrrl, rdl): 0.0, 0.0002023888

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.115 +- 0.222
mrr vals (pred, true): 0.011, 0.214
batch losses (mrrl, rdl): 0.0, 0.0001139994

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.416 +- 0.324
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.000132148

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 401
rank avg (pred): 0.377 +- 0.309
mrr vals (pred, true): 0.001, 0.147
batch losses (mrrl, rdl): 0.0, 7.20196e-05

Epoch over!
epoch time: 59.863

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 539
rank avg (pred): 0.298 +- 0.362
mrr vals (pred, true): 0.004, 0.210
batch losses (mrrl, rdl): 0.0, 7.31071e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.437 +- 0.332
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 2.82067e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 774
rank avg (pred): 0.514 +- 0.314
mrr vals (pred, true): 0.000, 0.014
batch losses (mrrl, rdl): 0.0, 0.0001373544

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 905
rank avg (pred): 0.408 +- 0.379
mrr vals (pred, true): 0.000, 0.027
batch losses (mrrl, rdl): 0.0, 0.0005948219

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 660
rank avg (pred): 0.459 +- 0.341
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 9.907e-06

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 751
rank avg (pred): 0.216 +- 0.305
mrr vals (pred, true): 0.006, 0.148
batch losses (mrrl, rdl): 0.0, 2.2478e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 718
rank avg (pred): 0.471 +- 0.353
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 1.64246e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 217
rank avg (pred): 0.399 +- 0.316
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001904577

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 988
rank avg (pred): 0.144 +- 0.268
mrr vals (pred, true): 0.057, 0.297
batch losses (mrrl, rdl): 0.0, 1.54259e-05

Epoch over!
epoch time: 58.337

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 780
rank avg (pred): 0.562 +- 0.332
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002340431

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 656
rank avg (pred): 0.429 +- 0.350
mrr vals (pred, true): 0.004, 0.000
batch losses (mrrl, rdl): 0.0, 4.84424e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 279
rank avg (pred): 0.128 +- 0.253
mrr vals (pred, true): 0.077, 0.214
batch losses (mrrl, rdl): 0.0, 1.05033e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 415
rank avg (pred): 0.425 +- 0.325
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 3.29974e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 604
rank avg (pred): 0.430 +- 0.345
mrr vals (pred, true): 0.002, 0.124
batch losses (mrrl, rdl): 0.0, 0.0001195664

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 612
rank avg (pred): 0.471 +- 0.352
mrr vals (pred, true): 0.002, 0.147
batch losses (mrrl, rdl): 0.0, 0.0001949864

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 653
rank avg (pred): 0.465 +- 0.348
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 3.18624e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.152 +- 0.296
mrr vals (pred, true): 0.173, 0.255
batch losses (mrrl, rdl): 0.0, 1.1376e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 754
rank avg (pred): 0.328 +- 0.289
mrr vals (pred, true): 0.001, 0.116
batch losses (mrrl, rdl): 0.0, 0.0002018817

Epoch over!
epoch time: 55.796

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.587 +- 0.338
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0248397533, 2.038e-06

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1080
rank avg (pred): 0.352 +- 0.251
mrr vals (pred, true): 0.049, 0.144
batch losses (mrrl, rdl): 0.0910031199, 8.3594e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 104
rank avg (pred): 0.382 +- 0.269
mrr vals (pred, true): 0.097, 0.172
batch losses (mrrl, rdl): 0.0569076687, 0.0002370233

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 520
rank avg (pred): 0.572 +- 0.433
mrr vals (pred, true): 0.220, 0.153
batch losses (mrrl, rdl): 0.0450697504, 0.0015603948

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 349
rank avg (pred): 0.388 +- 0.277
mrr vals (pred, true): 0.112, 0.172
batch losses (mrrl, rdl): 0.0363363102, 0.0001495299

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 227
rank avg (pred): 0.354 +- 0.256
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0034380089, 0.0001627838

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 484
rank avg (pred): 0.371 +- 0.275
mrr vals (pred, true): 0.093, 0.001
batch losses (mrrl, rdl): 0.0187079124, 7.57746e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 469
rank avg (pred): 0.357 +- 0.253
mrr vals (pred, true): 0.147, 0.000
batch losses (mrrl, rdl): 0.0945153832, 0.0001885695

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 977
rank avg (pred): 0.259 +- 0.214
mrr vals (pred, true): 0.284, 0.327
batch losses (mrrl, rdl): 0.0184425749, 0.0003445375

Epoch over!
epoch time: 57.445

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.280 +- 0.224
mrr vals (pred, true): 0.252, 0.267
batch losses (mrrl, rdl): 0.0023799581, 0.0004507383

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.361 +- 0.270
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0014528114, 0.0003653564

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 204
rank avg (pred): 0.335 +- 0.253
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0067096646, 0.0003664459

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 134
rank avg (pred): 0.341 +- 0.257
mrr vals (pred, true): 0.091, 0.158
batch losses (mrrl, rdl): 0.0442447178, 8.6154e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 361
rank avg (pred): 0.384 +- 0.291
mrr vals (pred, true): 0.078, 0.133
batch losses (mrrl, rdl): 0.0303401612, 0.0001280437

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.414 +- 0.324
mrr vals (pred, true): 0.072, 0.138
batch losses (mrrl, rdl): 0.0437540896, 7.52255e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1117
rank avg (pred): 0.316 +- 0.246
mrr vals (pred, true): 0.112, 0.000
batch losses (mrrl, rdl): 0.0386452377, 0.0004826414

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 934
rank avg (pred): 0.571 +- 0.390
mrr vals (pred, true): 0.025, 0.000
batch losses (mrrl, rdl): 0.00625548, 0.0017728367

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 34
rank avg (pred): 0.250 +- 0.214
mrr vals (pred, true): 0.237, 0.181
batch losses (mrrl, rdl): 0.0312287528, 0.0001435804

Epoch over!
epoch time: 59.838

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 879
rank avg (pred): 0.486 +- 0.336
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008702875, 7.87114e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 295
rank avg (pred): 0.261 +- 0.251
mrr vals (pred, true): 0.273, 0.315
batch losses (mrrl, rdl): 0.0174611341, 0.0003472135

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.340 +- 0.266
mrr vals (pred, true): 0.100, 0.124
batch losses (mrrl, rdl): 0.005655258, 2.59673e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 501
rank avg (pred): 0.222 +- 0.203
mrr vals (pred, true): 0.250, 0.288
batch losses (mrrl, rdl): 0.0141679915, 9.64972e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1129
rank avg (pred): 0.368 +- 0.290
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0017707609, 0.0002517472

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1201
rank avg (pred): 0.349 +- 0.264
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0031155869, 0.0003529529

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 102
rank avg (pred): 0.316 +- 0.267
mrr vals (pred, true): 0.090, 0.159
batch losses (mrrl, rdl): 0.0475916676, 2.33817e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1085
rank avg (pred): 0.309 +- 0.258
mrr vals (pred, true): 0.075, 0.159
batch losses (mrrl, rdl): 0.0718533918, 3.10945e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 329
rank avg (pred): 0.371 +- 0.309
mrr vals (pred, true): 0.095, 0.157
batch losses (mrrl, rdl): 0.038510818, 4.10651e-05

Epoch over!
epoch time: 58.287

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 823
rank avg (pred): 0.278 +- 0.309
mrr vals (pred, true): 0.200, 0.193
batch losses (mrrl, rdl): 0.0004982055, 0.0002580275

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 664
rank avg (pred): 0.422 +- 0.358
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0287250169, 0.0002962175

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.280 +- 0.288
mrr vals (pred, true): 0.212, 0.164
batch losses (mrrl, rdl): 0.0231610592, 1.11448e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 204
rank avg (pred): 0.425 +- 0.343
mrr vals (pred, true): 0.110, 0.000
batch losses (mrrl, rdl): 0.0358111337, 7.83195e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1117
rank avg (pred): 0.332 +- 0.312
mrr vals (pred, true): 0.134, 0.000
batch losses (mrrl, rdl): 0.0702422112, 0.0004731444

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 921
rank avg (pred): 0.573 +- 0.397
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.002814224, 0.0002715521

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 326
rank avg (pred): 0.394 +- 0.345
mrr vals (pred, true): 0.101, 0.152
batch losses (mrrl, rdl): 0.0263541155, 8.65276e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.462 +- 0.361
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0200396255, 6.4382e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.183 +- 0.237
mrr vals (pred, true): 0.299, 0.273
batch losses (mrrl, rdl): 0.0069806352, 4.85964e-05

Epoch over!
epoch time: 56.448

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 652
rank avg (pred): 0.467 +- 0.376
mrr vals (pred, true): 0.084, 0.001
batch losses (mrrl, rdl): 0.011510578, 7.2559e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 203
rank avg (pred): 0.350 +- 0.324
mrr vals (pred, true): 0.072, 0.000
batch losses (mrrl, rdl): 0.0046758298, 0.0005500369

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 680
rank avg (pred): 0.439 +- 0.360
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0424264595, 2.60202e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.294 +- 0.306
mrr vals (pred, true): 0.164, 0.140
batch losses (mrrl, rdl): 0.0060675484, 0.0001292251

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1154
rank avg (pred): 0.209 +- 0.264
mrr vals (pred, true): 0.204, 0.197
batch losses (mrrl, rdl): 0.000493132, 0.0002500889

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 631
rank avg (pred): 0.445 +- 0.356
mrr vals (pred, true): 0.112, 0.137
batch losses (mrrl, rdl): 0.0065621869, 0.0002560251

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 498
rank avg (pred): 0.205 +- 0.285
mrr vals (pred, true): 0.273, 0.277
batch losses (mrrl, rdl): 0.0001470854, 0.0001138133

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1092
rank avg (pred): 0.436 +- 0.371
mrr vals (pred, true): 0.087, 0.167
batch losses (mrrl, rdl): 0.0645875856, 0.0003965644

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 143
rank avg (pred): 0.356 +- 0.335
mrr vals (pred, true): 0.083, 0.148
batch losses (mrrl, rdl): 0.0433347411, 1.8424e-05

Epoch over!
epoch time: 59.539

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 858
rank avg (pred): 0.609 +- 0.427
mrr vals (pred, true): 0.046, 0.005
batch losses (mrrl, rdl): 0.0001792166, 0.0004162682

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 37
rank avg (pred): 0.245 +- 0.290
mrr vals (pred, true): 0.232, 0.238
batch losses (mrrl, rdl): 0.0003821473, 2.13248e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 826
rank avg (pred): 0.362 +- 0.349
mrr vals (pred, true): 0.142, 0.198
batch losses (mrrl, rdl): 0.0307668839, 0.0005358587

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 343
rank avg (pred): 0.371 +- 0.345
mrr vals (pred, true): 0.111, 0.198
batch losses (mrrl, rdl): 0.0755144581, 3.99607e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 550
rank avg (pred): 0.346 +- 0.329
mrr vals (pred, true): 0.169, 0.185
batch losses (mrrl, rdl): 0.0026734397, 2.50253e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.559 +- 0.416
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0025610751, 0.0018847428

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.586 +- 0.423
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0021461023, 0.0003815984

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.575 +- 0.419
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.88063e-05, 0.0002883454

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 32
rank avg (pred): 0.188 +- 0.243
mrr vals (pred, true): 0.235, 0.182
batch losses (mrrl, rdl): 0.0282060131, 1.72218e-05

Epoch over!
epoch time: 58.614

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.358 +- 0.334
mrr vals (pred, true): 0.064, 0.164
batch losses (mrrl, rdl): 0.1017949879, 2.54261e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 632
rank avg (pred): 0.369 +- 0.351
mrr vals (pred, true): 0.090, 0.127
batch losses (mrrl, rdl): 0.0137227364, 3.25121e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 720
rank avg (pred): 0.408 +- 0.365
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0024236236, 9.6523e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 693
rank avg (pred): 0.470 +- 0.380
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022001411, 4.77428e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 412
rank avg (pred): 0.377 +- 0.346
mrr vals (pred, true): 0.121, 0.000
batch losses (mrrl, rdl): 0.0500891544, 0.0001443088

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 14
rank avg (pred): 0.154 +- 0.293
mrr vals (pred, true): 0.387, 0.347
batch losses (mrrl, rdl): 0.0159021299, 5.6172e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 215
rank avg (pred): 0.418 +- 0.370
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.0101493746, 0.0001136874

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 725
rank avg (pred): 0.471 +- 0.380
mrr vals (pred, true): 0.073, 0.001
batch losses (mrrl, rdl): 0.005499905, 0.0001320307

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 915
rank avg (pred): 0.491 +- 0.391
mrr vals (pred, true): 0.132, 0.010
batch losses (mrrl, rdl): 0.0666861907, 0.0006047644

Epoch over!
epoch time: 55.791

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 327
rank avg (pred): 0.416 +- 0.367
mrr vals (pred, true): 0.082, 0.112
batch losses (mrrl, rdl): 0.0089333821, 0.0002003407

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1178
rank avg (pred): 0.382 +- 0.355
mrr vals (pred, true): 0.114, 0.137
batch losses (mrrl, rdl): 0.0049569383, 4.5178e-06

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 276
rank avg (pred): 0.240 +- 0.260
mrr vals (pred, true): 0.241, 0.211
batch losses (mrrl, rdl): 0.0090250345, 8.79705e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.543 +- 0.402
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.00060785, 5.71549e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.381 +- 0.354
mrr vals (pred, true): 0.093, 0.008
batch losses (mrrl, rdl): 0.0185943916, 0.0002431832

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 130
rank avg (pred): 0.364 +- 0.340
mrr vals (pred, true): 0.069, 0.163
batch losses (mrrl, rdl): 0.0886642858, 8.713e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 706
rank avg (pred): 0.444 +- 0.378
mrr vals (pred, true): 0.078, 0.000
batch losses (mrrl, rdl): 0.0080507696, 0.0001379475

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1091
rank avg (pred): 0.521 +- 0.410
mrr vals (pred, true): 0.073, 0.163
batch losses (mrrl, rdl): 0.0815622658, 0.0007259901

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1065
rank avg (pred): 0.182 +- 0.212
mrr vals (pred, true): 0.303, 0.312
batch losses (mrrl, rdl): 0.0009238527, 4.29687e-05

Epoch over!
epoch time: 54.723

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 402
rank avg (pred): 0.389 +- 0.350
mrr vals (pred, true): 0.076, 0.178
batch losses (mrrl, rdl): 0.1045577824, 4.43178e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 16
rank avg (pred): 0.208 +- 0.254
mrr vals (pred, true): 0.333, 0.348
batch losses (mrrl, rdl): 0.0020634003, 7.66985e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 132
rank avg (pred): 0.395 +- 0.371
mrr vals (pred, true): 0.101, 0.153
batch losses (mrrl, rdl): 0.0264189616, 6.06718e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.409 +- 0.378
mrr vals (pred, true): 0.104, 0.177
batch losses (mrrl, rdl): 0.0534387603, 8.78741e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 778
rank avg (pred): 0.546 +- 0.408
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002230519, 0.0001301179

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1004
rank avg (pred): 0.432 +- 0.388
mrr vals (pred, true): 0.085, 0.137
batch losses (mrrl, rdl): 0.0265393183, 0.000212389

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.325 +- 0.331
mrr vals (pred, true): 0.098, 0.032
batch losses (mrrl, rdl): 0.023431886, 7.24604e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 906
rank avg (pred): 0.501 +- 0.407
mrr vals (pred, true): 0.105, 0.017
batch losses (mrrl, rdl): 0.0308008902, 0.000788837

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1120
rank avg (pred): 0.405 +- 0.372
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0153400237, 0.0002510637

Epoch over!
epoch time: 58.042

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.269 +- 0.335
mrr vals (pred, true): 0.260, 0.263
batch losses (mrrl, rdl): 9.59804e-05, 5.6714e-06

running batch: 500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 338
rank avg (pred): 0.500 +- 0.422
mrr vals (pred, true): 0.069, 0.139
batch losses (mrrl, rdl): 0.0483160987, 0.0006100521

running batch: 1000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 658
rank avg (pred): 0.434 +- 0.401
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0419144332, 0.0003197226

running batch: 1500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 211
rank avg (pred): 0.455 +- 0.400
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0203964766, 0.0001272149

running batch: 2000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.412 +- 0.391
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0141300987, 0.0002833255

running batch: 2500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.563 +- 0.412
mrr vals (pred, true): 0.082, 0.001
batch losses (mrrl, rdl): 0.0104945889, 0.0002056162

running batch: 3000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 496
rank avg (pred): 0.270 +- 0.367
mrr vals (pred, true): 0.270, 0.254
batch losses (mrrl, rdl): 0.0024556203, 7.2725e-06

running batch: 3500 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 189
rank avg (pred): 0.393 +- 0.373
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0424436629, 0.0002636393

running batch: 4000 / 4376 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1121
rank avg (pred): 0.448 +- 0.395
mrr vals (pred, true): 0.123, 0.000
batch losses (mrrl, rdl): 0.0529966578, 8.33688e-05

Epoch over!
epoch time: 56.937

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.400 +- 0.379
mrr vals (pred, true): 0.119, 0.178

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.07701 	 5e-0500 	 m..s
   67 	     1 	 0.12285 	 0.00015 	 MISS
    1 	     2 	 0.08021 	 0.00016 	 m..s
    1 	     3 	 0.08021 	 0.00016 	 m..s
    1 	     4 	 0.08021 	 0.00018 	 m..s
   66 	     5 	 0.12282 	 0.00018 	 MISS
   85 	     6 	 0.12735 	 0.00018 	 MISS
   22 	     7 	 0.11751 	 0.00019 	 MISS
    1 	     8 	 0.08021 	 0.00020 	 m..s
   61 	     9 	 0.12185 	 0.00020 	 MISS
   68 	    10 	 0.12291 	 0.00021 	 MISS
    1 	    11 	 0.08021 	 0.00021 	 m..s
    1 	    12 	 0.08021 	 0.00021 	 m..s
   86 	    13 	 0.12757 	 0.00021 	 MISS
   62 	    14 	 0.12192 	 0.00022 	 MISS
   72 	    15 	 0.12337 	 0.00022 	 MISS
   74 	    16 	 0.12343 	 0.00023 	 MISS
   58 	    17 	 0.12153 	 0.00025 	 MISS
   20 	    18 	 0.11610 	 0.00025 	 MISS
   71 	    19 	 0.12337 	 0.00026 	 MISS
    1 	    20 	 0.08021 	 0.00026 	 m..s
   63 	    21 	 0.12248 	 0.00026 	 MISS
   70 	    22 	 0.12311 	 0.00027 	 MISS
    1 	    23 	 0.08021 	 0.00029 	 m..s
   32 	    24 	 0.11903 	 0.00031 	 MISS
   50 	    25 	 0.12042 	 0.00031 	 MISS
   82 	    26 	 0.12474 	 0.00033 	 MISS
   84 	    27 	 0.12731 	 0.00033 	 MISS
   52 	    28 	 0.12091 	 0.00034 	 MISS
    1 	    29 	 0.08021 	 0.00034 	 m..s
   16 	    30 	 0.08810 	 0.00034 	 m..s
   21 	    31 	 0.11714 	 0.00040 	 MISS
   23 	    32 	 0.11762 	 0.00040 	 MISS
   36 	    33 	 0.11938 	 0.00044 	 MISS
   41 	    34 	 0.11983 	 0.00045 	 MISS
   24 	    35 	 0.11762 	 0.00046 	 MISS
   27 	    36 	 0.11802 	 0.00051 	 MISS
    1 	    37 	 0.08021 	 0.00056 	 m..s
   76 	    38 	 0.12377 	 0.00069 	 MISS
   30 	    39 	 0.11893 	 0.00078 	 MISS
   64 	    40 	 0.12252 	 0.00080 	 MISS
   80 	    41 	 0.12447 	 0.00091 	 MISS
   15 	    42 	 0.08737 	 0.00091 	 m..s
    1 	    43 	 0.08021 	 0.00118 	 m..s
   69 	    44 	 0.12292 	 0.00188 	 MISS
   14 	    45 	 0.08140 	 0.00267 	 m..s
    1 	    46 	 0.08021 	 0.00828 	 m..s
    1 	    47 	 0.08021 	 0.00828 	 m..s
   38 	    48 	 0.11957 	 0.03790 	 m..s
   51 	    49 	 0.12052 	 0.05864 	 m..s
   43 	    50 	 0.11993 	 0.10212 	 ~...
   18 	    51 	 0.11507 	 0.11583 	 ~...
   33 	    52 	 0.11931 	 0.12252 	 ~...
   88 	    53 	 0.17502 	 0.12263 	 m..s
   46 	    54 	 0.12001 	 0.12368 	 ~...
   45 	    55 	 0.11998 	 0.12649 	 ~...
   17 	    56 	 0.11269 	 0.12922 	 ~...
   19 	    57 	 0.11575 	 0.13015 	 ~...
   73 	    58 	 0.12342 	 0.13109 	 ~...
   31 	    59 	 0.11898 	 0.13144 	 ~...
   34 	    60 	 0.11933 	 0.13238 	 ~...
   53 	    61 	 0.12098 	 0.13254 	 ~...
   77 	    62 	 0.12383 	 0.13654 	 ~...
   59 	    63 	 0.12159 	 0.13690 	 ~...
   47 	    64 	 0.12011 	 0.13794 	 ~...
   79 	    65 	 0.12396 	 0.14208 	 ~...
   87 	    66 	 0.17186 	 0.14294 	 ~...
   65 	    67 	 0.12281 	 0.14604 	 ~...
   42 	    68 	 0.11991 	 0.14613 	 ~...
   44 	    69 	 0.11996 	 0.14843 	 ~...
   28 	    70 	 0.11819 	 0.14927 	 m..s
   78 	    71 	 0.12390 	 0.15489 	 m..s
   25 	    72 	 0.11775 	 0.15565 	 m..s
   48 	    73 	 0.12013 	 0.15762 	 m..s
   26 	    74 	 0.11779 	 0.15773 	 m..s
   49 	    75 	 0.12023 	 0.15786 	 m..s
   57 	    76 	 0.12149 	 0.15928 	 m..s
   89 	    77 	 0.18448 	 0.15951 	 ~...
   81 	    78 	 0.12468 	 0.16004 	 m..s
   35 	    79 	 0.11934 	 0.16041 	 m..s
  105 	    80 	 0.25948 	 0.16176 	 m..s
   29 	    81 	 0.11849 	 0.16376 	 m..s
   99 	    82 	 0.22766 	 0.16408 	 m..s
   54 	    83 	 0.12099 	 0.16513 	 m..s
   60 	    84 	 0.12169 	 0.16629 	 m..s
   75 	    85 	 0.12353 	 0.16652 	 m..s
   39 	    86 	 0.11966 	 0.16703 	 m..s
   40 	    87 	 0.11974 	 0.17511 	 m..s
   93 	    88 	 0.21813 	 0.17592 	 m..s
   55 	    89 	 0.12106 	 0.17681 	 m..s
   37 	    90 	 0.11940 	 0.17787 	 m..s
   83 	    91 	 0.12481 	 0.17827 	 m..s
   94 	    92 	 0.22138 	 0.18096 	 m..s
   56 	    93 	 0.12124 	 0.18120 	 m..s
   90 	    94 	 0.19096 	 0.19017 	 ~...
   97 	    95 	 0.22384 	 0.19113 	 m..s
   95 	    96 	 0.22143 	 0.19298 	 ~...
   91 	    97 	 0.19561 	 0.19489 	 ~...
   96 	    98 	 0.22209 	 0.20377 	 ~...
  108 	    99 	 0.27355 	 0.21862 	 m..s
   92 	   100 	 0.21548 	 0.22405 	 ~...
  102 	   101 	 0.24408 	 0.22873 	 ~...
  101 	   102 	 0.22983 	 0.24329 	 ~...
  100 	   103 	 0.22982 	 0.25673 	 ~...
  106 	   104 	 0.26824 	 0.25835 	 ~...
  109 	   105 	 0.27408 	 0.26209 	 ~...
   98 	   106 	 0.22648 	 0.26378 	 m..s
  104 	   107 	 0.25269 	 0.26570 	 ~...
  103 	   108 	 0.25178 	 0.26880 	 ~...
  113 	   109 	 0.29111 	 0.26952 	 ~...
  116 	   110 	 0.29429 	 0.27314 	 ~...
  115 	   111 	 0.29378 	 0.27673 	 ~...
  111 	   112 	 0.28801 	 0.28166 	 ~...
  114 	   113 	 0.29225 	 0.28396 	 ~...
  110 	   114 	 0.28601 	 0.28879 	 ~...
  107 	   115 	 0.26976 	 0.30216 	 m..s
  112 	   116 	 0.28868 	 0.31604 	 ~...
  117 	   117 	 0.32205 	 0.32433 	 ~...
  119 	   118 	 0.33277 	 0.34516 	 ~...
  118 	   119 	 0.32894 	 0.35217 	 ~...
  120 	   120 	 0.34286 	 0.36330 	 ~...
==========================================
r_mrr = 0.827136754989624
r2_mrr = 0.5256333351135254
spearmanr_mrr@5 = 0.9143491983413696
spearmanr_mrr@10 = 0.9558807015419006
spearmanr_mrr@50 = 0.9455682635307312
spearmanr_mrr@100 = 0.8340086340904236
spearmanr_mrr@All = 0.8648974299430847
==========================================
test time: 0.526
Done Testing dataset DBpedia50
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.559 +- 0.376
mrr vals (pred, true): 0.062, 0.110

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   13 	     0 	 0.04862 	 0.02085 	 ~...
   16 	     1 	 0.05834 	 0.02127 	 m..s
    0 	     2 	 0.04819 	 0.02690 	 ~...
    0 	     3 	 0.04819 	 0.02851 	 ~...
    0 	     4 	 0.04819 	 0.03457 	 ~...
   50 	     5 	 0.06295 	 0.03521 	 ~...
   20 	     6 	 0.06184 	 0.03707 	 ~...
   31 	     7 	 0.06206 	 0.03713 	 ~...
   74 	     8 	 0.06811 	 0.03861 	 ~...
   27 	     9 	 0.06201 	 0.03919 	 ~...
   14 	    10 	 0.05076 	 0.04145 	 ~...
   83 	    11 	 0.07281 	 0.04214 	 m..s
   58 	    12 	 0.06463 	 0.04242 	 ~...
   47 	    13 	 0.06282 	 0.04304 	 ~...
   26 	    14 	 0.06201 	 0.04318 	 ~...
   54 	    15 	 0.06307 	 0.04339 	 ~...
   76 	    16 	 0.06839 	 0.04365 	 ~...
    0 	    17 	 0.04819 	 0.04413 	 ~...
   39 	    18 	 0.06247 	 0.04423 	 ~...
   62 	    19 	 0.06572 	 0.04441 	 ~...
   53 	    20 	 0.06305 	 0.04465 	 ~...
   77 	    21 	 0.06849 	 0.04533 	 ~...
   19 	    22 	 0.06176 	 0.04536 	 ~...
   25 	    23 	 0.06198 	 0.04559 	 ~...
   49 	    24 	 0.06294 	 0.04578 	 ~...
    0 	    25 	 0.04819 	 0.04580 	 ~...
   48 	    26 	 0.06287 	 0.04581 	 ~...
   51 	    27 	 0.06296 	 0.04585 	 ~...
   30 	    28 	 0.06203 	 0.04615 	 ~...
   42 	    29 	 0.06255 	 0.04619 	 ~...
   43 	    30 	 0.06279 	 0.04626 	 ~...
   64 	    31 	 0.06657 	 0.04630 	 ~...
   35 	    32 	 0.06230 	 0.04728 	 ~...
    0 	    33 	 0.04819 	 0.04738 	 ~...
   68 	    34 	 0.06747 	 0.04755 	 ~...
   29 	    35 	 0.06203 	 0.04797 	 ~...
   18 	    36 	 0.06173 	 0.04819 	 ~...
    0 	    37 	 0.04819 	 0.04827 	 ~...
   38 	    38 	 0.06245 	 0.04828 	 ~...
   52 	    39 	 0.06299 	 0.04856 	 ~...
    0 	    40 	 0.04819 	 0.04881 	 ~...
   46 	    41 	 0.06282 	 0.04922 	 ~...
   82 	    42 	 0.07217 	 0.04941 	 ~...
    0 	    43 	 0.04819 	 0.04949 	 ~...
   33 	    44 	 0.06215 	 0.04983 	 ~...
   45 	    45 	 0.06280 	 0.05000 	 ~...
    0 	    46 	 0.04819 	 0.05009 	 ~...
   90 	    47 	 0.09441 	 0.05028 	 m..s
   63 	    48 	 0.06618 	 0.05036 	 ~...
   55 	    49 	 0.06309 	 0.05049 	 ~...
   84 	    50 	 0.07322 	 0.05082 	 ~...
   34 	    51 	 0.06228 	 0.05108 	 ~...
   67 	    52 	 0.06742 	 0.05133 	 ~...
   56 	    53 	 0.06332 	 0.05193 	 ~...
   15 	    54 	 0.05103 	 0.05219 	 ~...
    0 	    55 	 0.04819 	 0.05266 	 ~...
   17 	    56 	 0.06172 	 0.05331 	 ~...
   22 	    57 	 0.06186 	 0.05348 	 ~...
   21 	    58 	 0.06184 	 0.05439 	 ~...
   41 	    59 	 0.06254 	 0.05456 	 ~...
   44 	    60 	 0.06279 	 0.05501 	 ~...
   91 	    61 	 0.09494 	 0.05990 	 m..s
   86 	    62 	 0.08900 	 0.06424 	 ~...
   85 	    63 	 0.08703 	 0.07057 	 ~...
   36 	    64 	 0.06230 	 0.07471 	 ~...
   66 	    65 	 0.06706 	 0.07486 	 ~...
   80 	    66 	 0.06972 	 0.08288 	 ~...
   81 	    67 	 0.07074 	 0.08345 	 ~...
   69 	    68 	 0.06773 	 0.08475 	 ~...
   60 	    69 	 0.06547 	 0.09027 	 ~...
   73 	    70 	 0.06782 	 0.09130 	 ~...
    0 	    71 	 0.04819 	 0.10013 	 m..s
   70 	    72 	 0.06774 	 0.10651 	 m..s
   59 	    73 	 0.06509 	 0.10771 	 m..s
   23 	    74 	 0.06187 	 0.11028 	 m..s
   65 	    75 	 0.06691 	 0.11350 	 m..s
    0 	    76 	 0.04819 	 0.11373 	 m..s
   78 	    77 	 0.06850 	 0.11661 	 m..s
   40 	    78 	 0.06249 	 0.12052 	 m..s
   72 	    79 	 0.06781 	 0.12080 	 m..s
   79 	    80 	 0.06869 	 0.12426 	 m..s
   61 	    81 	 0.06549 	 0.12660 	 m..s
   87 	    82 	 0.09072 	 0.12739 	 m..s
   24 	    83 	 0.06196 	 0.12861 	 m..s
   71 	    84 	 0.06779 	 0.13138 	 m..s
   75 	    85 	 0.06822 	 0.13372 	 m..s
   88 	    86 	 0.09129 	 0.13948 	 m..s
   37 	    87 	 0.06232 	 0.14243 	 m..s
   57 	    88 	 0.06333 	 0.14586 	 m..s
   89 	    89 	 0.09205 	 0.14609 	 m..s
   92 	    90 	 0.09928 	 0.14821 	 m..s
   28 	    91 	 0.06202 	 0.14995 	 m..s
   32 	    92 	 0.06209 	 0.15406 	 m..s
   95 	    93 	 0.15522 	 0.20213 	 m..s
   96 	    94 	 0.15838 	 0.20778 	 m..s
   93 	    95 	 0.12995 	 0.21843 	 m..s
   98 	    96 	 0.16789 	 0.24174 	 m..s
   94 	    97 	 0.14061 	 0.24207 	 MISS
   97 	    98 	 0.16388 	 0.26153 	 m..s
  100 	    99 	 0.22464 	 0.26525 	 m..s
   99 	   100 	 0.22364 	 0.36005 	 MISS
  101 	   101 	 0.43816 	 0.37516 	 m..s
  102 	   102 	 0.47761 	 0.51881 	 m..s
  117 	   103 	 0.54202 	 0.52247 	 ~...
  103 	   104 	 0.50461 	 0.52928 	 ~...
  113 	   105 	 0.53956 	 0.53020 	 ~...
  114 	   106 	 0.53977 	 0.53079 	 ~...
  108 	   107 	 0.52679 	 0.53090 	 ~...
  111 	   108 	 0.53123 	 0.53116 	 ~...
  106 	   109 	 0.52244 	 0.53662 	 ~...
  112 	   110 	 0.53386 	 0.53719 	 ~...
  104 	   111 	 0.51444 	 0.54255 	 ~...
  107 	   112 	 0.52249 	 0.54390 	 ~...
  105 	   113 	 0.51881 	 0.54725 	 ~...
  118 	   114 	 0.54220 	 0.54894 	 ~...
  110 	   115 	 0.53102 	 0.55203 	 ~...
  119 	   116 	 0.54263 	 0.55206 	 ~...
  116 	   117 	 0.54192 	 0.55334 	 ~...
  115 	   118 	 0.54113 	 0.55481 	 ~...
  109 	   119 	 0.53036 	 0.55581 	 ~...
  120 	   120 	 0.54360 	 0.55872 	 ~...
==========================================
r_mrr = 0.9783504605293274
r2_mrr = 0.953922688961029
spearmanr_mrr@5 = 0.9662852883338928
spearmanr_mrr@10 = 0.925997257232666
spearmanr_mrr@50 = 0.9933796525001526
spearmanr_mrr@100 = 0.9842643737792969
spearmanr_mrr@All = 0.983991801738739
==========================================
test time: 0.507
Done Testing dataset UMLS
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.189 +- 0.168
mrr vals (pred, true): 0.127, 0.260

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   84 	     0 	 0.16469 	 0.02078 	 MISS
   74 	     1 	 0.13586 	 0.04634 	 m..s
   67 	     2 	 0.13367 	 0.04754 	 m..s
    0 	     3 	 0.04393 	 0.04855 	 ~...
   13 	     4 	 0.04536 	 0.04874 	 ~...
   64 	     5 	 0.13356 	 0.04883 	 m..s
   45 	     6 	 0.12962 	 0.04888 	 m..s
   15 	     7 	 0.05470 	 0.04910 	 ~...
   60 	     8 	 0.13272 	 0.04929 	 m..s
   59 	     9 	 0.13266 	 0.04967 	 m..s
    0 	    10 	 0.04393 	 0.04997 	 ~...
    0 	    11 	 0.04393 	 0.05001 	 ~...
   19 	    12 	 0.12375 	 0.05005 	 m..s
   70 	    13 	 0.13424 	 0.05029 	 m..s
   82 	    14 	 0.14592 	 0.05033 	 m..s
    0 	    15 	 0.04393 	 0.05112 	 ~...
   44 	    16 	 0.12880 	 0.05113 	 m..s
   56 	    17 	 0.13125 	 0.05116 	 m..s
   29 	    18 	 0.12709 	 0.05130 	 m..s
    0 	    19 	 0.04393 	 0.05131 	 ~...
   63 	    20 	 0.13314 	 0.05146 	 m..s
   14 	    21 	 0.05353 	 0.05178 	 ~...
   55 	    22 	 0.13114 	 0.05206 	 m..s
   83 	    23 	 0.14617 	 0.05214 	 m..s
   34 	    24 	 0.12783 	 0.05227 	 m..s
   23 	    25 	 0.12637 	 0.05230 	 m..s
   20 	    26 	 0.12483 	 0.05276 	 m..s
    0 	    27 	 0.04393 	 0.05284 	 ~...
   75 	    28 	 0.13591 	 0.05286 	 m..s
   77 	    29 	 0.13643 	 0.05324 	 m..s
   78 	    30 	 0.13782 	 0.05325 	 m..s
   32 	    31 	 0.12752 	 0.05337 	 m..s
    0 	    32 	 0.04393 	 0.05343 	 ~...
    0 	    33 	 0.04393 	 0.05363 	 ~...
    0 	    34 	 0.04393 	 0.05374 	 ~...
   81 	    35 	 0.14591 	 0.05395 	 m..s
    0 	    36 	 0.04393 	 0.05446 	 ~...
    0 	    37 	 0.04393 	 0.05447 	 ~...
    0 	    38 	 0.04393 	 0.05460 	 ~...
   73 	    39 	 0.13583 	 0.05482 	 m..s
   62 	    40 	 0.13284 	 0.05486 	 m..s
   21 	    41 	 0.12598 	 0.05533 	 m..s
   25 	    42 	 0.12675 	 0.05536 	 m..s
   22 	    43 	 0.12630 	 0.05589 	 m..s
   24 	    44 	 0.12653 	 0.05840 	 m..s
   61 	    45 	 0.13282 	 0.05854 	 m..s
   51 	    46 	 0.13066 	 0.05971 	 m..s
    0 	    47 	 0.04393 	 0.06101 	 ~...
   26 	    48 	 0.12697 	 0.18802 	 m..s
   33 	    49 	 0.12768 	 0.20919 	 m..s
   71 	    50 	 0.13434 	 0.22051 	 m..s
   80 	    51 	 0.14006 	 0.22191 	 m..s
   18 	    52 	 0.12320 	 0.22497 	 MISS
   30 	    53 	 0.12714 	 0.22898 	 MISS
   16 	    54 	 0.11742 	 0.22899 	 MISS
   27 	    55 	 0.12701 	 0.23134 	 MISS
   72 	    56 	 0.13446 	 0.23183 	 m..s
   37 	    57 	 0.12799 	 0.23322 	 MISS
   42 	    58 	 0.12849 	 0.23328 	 MISS
   52 	    59 	 0.13075 	 0.23337 	 MISS
   48 	    60 	 0.12981 	 0.23496 	 MISS
   38 	    61 	 0.12803 	 0.23527 	 MISS
   58 	    62 	 0.13266 	 0.23724 	 MISS
   40 	    63 	 0.12812 	 0.23730 	 MISS
   69 	    64 	 0.13421 	 0.23872 	 MISS
   47 	    65 	 0.12974 	 0.24073 	 MISS
   53 	    66 	 0.13089 	 0.24121 	 MISS
   39 	    67 	 0.12808 	 0.24548 	 MISS
   76 	    68 	 0.13602 	 0.24569 	 MISS
   43 	    69 	 0.12856 	 0.24863 	 MISS
   68 	    70 	 0.13384 	 0.24880 	 MISS
   36 	    71 	 0.12796 	 0.25129 	 MISS
   79 	    72 	 0.13854 	 0.25319 	 MISS
   28 	    73 	 0.12707 	 0.25355 	 MISS
   65 	    74 	 0.13362 	 0.25432 	 MISS
   35 	    75 	 0.12790 	 0.25445 	 MISS
   54 	    76 	 0.13092 	 0.25499 	 MISS
   57 	    77 	 0.13167 	 0.25572 	 MISS
   41 	    78 	 0.12829 	 0.25655 	 MISS
   31 	    79 	 0.12715 	 0.26030 	 MISS
   17 	    80 	 0.12166 	 0.26501 	 MISS
   50 	    81 	 0.12995 	 0.26716 	 MISS
   49 	    82 	 0.12986 	 0.26914 	 MISS
   46 	    83 	 0.12972 	 0.27397 	 MISS
   66 	    84 	 0.13365 	 0.27616 	 MISS
   86 	    85 	 0.28768 	 0.28251 	 ~...
   85 	    86 	 0.28674 	 0.30493 	 ~...
  101 	    87 	 0.40492 	 0.33846 	 m..s
   90 	    88 	 0.37161 	 0.35933 	 ~...
   99 	    89 	 0.40092 	 0.36576 	 m..s
   92 	    90 	 0.37974 	 0.36692 	 ~...
   93 	    91 	 0.38098 	 0.37356 	 ~...
   89 	    92 	 0.37085 	 0.37824 	 ~...
   88 	    93 	 0.36692 	 0.37832 	 ~...
   87 	    94 	 0.36009 	 0.38009 	 ~...
   95 	    95 	 0.39445 	 0.38337 	 ~...
   91 	    96 	 0.37389 	 0.38613 	 ~...
   96 	    97 	 0.39615 	 0.39438 	 ~...
  100 	    98 	 0.40148 	 0.39602 	 ~...
   94 	    99 	 0.38658 	 0.39788 	 ~...
   98 	   100 	 0.40054 	 0.39919 	 ~...
   97 	   101 	 0.40024 	 0.40353 	 ~...
  103 	   102 	 0.41870 	 0.41130 	 ~...
  109 	   103 	 0.43355 	 0.41748 	 ~...
  104 	   104 	 0.42172 	 0.42330 	 ~...
  108 	   105 	 0.42662 	 0.43359 	 ~...
  110 	   106 	 0.43373 	 0.43650 	 ~...
  111 	   107 	 0.43485 	 0.43964 	 ~...
  105 	   108 	 0.42220 	 0.44009 	 ~...
  114 	   109 	 0.43761 	 0.44102 	 ~...
  112 	   110 	 0.43711 	 0.44173 	 ~...
  107 	   111 	 0.42429 	 0.44342 	 ~...
  116 	   112 	 0.43784 	 0.44344 	 ~...
  117 	   113 	 0.43896 	 0.44446 	 ~...
  115 	   114 	 0.43769 	 0.44618 	 ~...
  102 	   115 	 0.41145 	 0.44625 	 m..s
  113 	   116 	 0.43739 	 0.44649 	 ~...
  106 	   117 	 0.42232 	 0.44750 	 ~...
  119 	   118 	 0.43987 	 0.45836 	 ~...
  120 	   119 	 0.44142 	 0.46137 	 ~...
  118 	   120 	 0.43963 	 0.46786 	 ~...
==========================================
r_mrr = 0.8652445673942566
r2_mrr = 0.7366588115692139
spearmanr_mrr@5 = 0.9418648481369019
spearmanr_mrr@10 = 0.8846307992935181
spearmanr_mrr@50 = 0.9674625396728516
spearmanr_mrr@100 = 0.8590237498283386
spearmanr_mrr@All = 0.8869943022727966
==========================================
test time: 0.422
Done Testing dataset Kinships
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.370 +- 0.342
mrr vals (pred, true): 0.073, 0.063

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04450 	 0.00011 	 m..s
    1 	     1 	 0.04624 	 0.00029 	 m..s
    1 	     2 	 0.04624 	 0.00041 	 m..s
   76 	     3 	 0.07634 	 0.00047 	 m..s
   80 	     4 	 0.07688 	 0.00048 	 m..s
   71 	     5 	 0.07604 	 0.00049 	 m..s
   63 	     6 	 0.07536 	 0.00050 	 m..s
   70 	     7 	 0.07584 	 0.00050 	 m..s
   22 	     8 	 0.07165 	 0.00053 	 m..s
   66 	     9 	 0.07562 	 0.00055 	 m..s
   32 	    10 	 0.07278 	 0.00055 	 m..s
   20 	    11 	 0.07062 	 0.00056 	 m..s
    1 	    12 	 0.04624 	 0.00056 	 m..s
   41 	    13 	 0.07337 	 0.00058 	 m..s
   68 	    14 	 0.07569 	 0.00058 	 m..s
   72 	    15 	 0.07604 	 0.00059 	 m..s
   86 	    16 	 0.07927 	 0.00061 	 m..s
    1 	    17 	 0.04624 	 0.00061 	 m..s
   24 	    18 	 0.07173 	 0.00061 	 m..s
   82 	    19 	 0.07709 	 0.00063 	 m..s
   23 	    20 	 0.07173 	 0.00063 	 m..s
   74 	    21 	 0.07609 	 0.00063 	 m..s
   21 	    22 	 0.07138 	 0.00064 	 m..s
    1 	    23 	 0.04624 	 0.00067 	 m..s
    1 	    24 	 0.04624 	 0.00068 	 m..s
    1 	    25 	 0.04624 	 0.00068 	 m..s
    1 	    26 	 0.04624 	 0.00068 	 m..s
    1 	    27 	 0.04624 	 0.00068 	 m..s
   85 	    28 	 0.07910 	 0.00073 	 m..s
   62 	    29 	 0.07494 	 0.00073 	 m..s
   58 	    30 	 0.07465 	 0.00074 	 m..s
   64 	    31 	 0.07539 	 0.00075 	 m..s
    1 	    32 	 0.04624 	 0.00075 	 m..s
    1 	    33 	 0.04624 	 0.00077 	 m..s
   69 	    34 	 0.07570 	 0.00077 	 m..s
   67 	    35 	 0.07564 	 0.00078 	 m..s
   84 	    36 	 0.07906 	 0.00079 	 m..s
    1 	    37 	 0.04624 	 0.00080 	 m..s
   61 	    38 	 0.07489 	 0.00082 	 m..s
   50 	    39 	 0.07381 	 0.00086 	 m..s
   15 	    40 	 0.05078 	 0.00088 	 m..s
   36 	    41 	 0.07304 	 0.00099 	 m..s
   52 	    42 	 0.07418 	 0.00099 	 m..s
   16 	    43 	 0.05125 	 0.00108 	 m..s
   30 	    44 	 0.07270 	 0.00109 	 m..s
   27 	    45 	 0.07203 	 0.00116 	 m..s
    1 	    46 	 0.04624 	 0.00406 	 m..s
   14 	    47 	 0.04699 	 0.00545 	 m..s
   40 	    48 	 0.07337 	 0.03512 	 m..s
   29 	    49 	 0.07237 	 0.04147 	 m..s
   19 	    50 	 0.07036 	 0.04317 	 ~...
   79 	    51 	 0.07648 	 0.04487 	 m..s
   34 	    52 	 0.07300 	 0.04587 	 ~...
   17 	    53 	 0.06815 	 0.04732 	 ~...
   31 	    54 	 0.07274 	 0.04898 	 ~...
   18 	    55 	 0.06988 	 0.05053 	 ~...
   26 	    56 	 0.07186 	 0.05187 	 ~...
   46 	    57 	 0.07350 	 0.05223 	 ~...
   47 	    58 	 0.07358 	 0.05231 	 ~...
   28 	    59 	 0.07215 	 0.05262 	 ~...
   51 	    60 	 0.07407 	 0.05487 	 ~...
   42 	    61 	 0.07343 	 0.05653 	 ~...
   43 	    62 	 0.07345 	 0.05836 	 ~...
   39 	    63 	 0.07331 	 0.05861 	 ~...
   33 	    64 	 0.07298 	 0.05905 	 ~...
   45 	    65 	 0.07348 	 0.05963 	 ~...
   35 	    66 	 0.07301 	 0.06011 	 ~...
   48 	    67 	 0.07360 	 0.06035 	 ~...
   65 	    68 	 0.07561 	 0.06050 	 ~...
   57 	    69 	 0.07462 	 0.06054 	 ~...
   25 	    70 	 0.07183 	 0.06146 	 ~...
   37 	    71 	 0.07305 	 0.06319 	 ~...
   54 	    72 	 0.07424 	 0.06378 	 ~...
   49 	    73 	 0.07367 	 0.06517 	 ~...
   56 	    74 	 0.07443 	 0.06559 	 ~...
   55 	    75 	 0.07429 	 0.06609 	 ~...
   83 	    76 	 0.07714 	 0.06625 	 ~...
   73 	    77 	 0.07608 	 0.06783 	 ~...
   38 	    78 	 0.07324 	 0.06947 	 ~...
   78 	    79 	 0.07645 	 0.07170 	 ~...
   53 	    80 	 0.07423 	 0.07228 	 ~...
   77 	    81 	 0.07639 	 0.07310 	 ~...
   81 	    82 	 0.07704 	 0.07507 	 ~...
   60 	    83 	 0.07477 	 0.07577 	 ~...
   59 	    84 	 0.07469 	 0.07637 	 ~...
   44 	    85 	 0.07346 	 0.07694 	 ~...
   75 	    86 	 0.07617 	 0.08886 	 ~...
   89 	    87 	 0.13446 	 0.12554 	 ~...
   88 	    88 	 0.12576 	 0.12784 	 ~...
   92 	    89 	 0.17256 	 0.13488 	 m..s
   90 	    90 	 0.14275 	 0.14677 	 ~...
   87 	    91 	 0.11955 	 0.15018 	 m..s
   93 	    92 	 0.17686 	 0.15060 	 ~...
   96 	    93 	 0.18394 	 0.15883 	 ~...
   95 	    94 	 0.18164 	 0.16162 	 ~...
   91 	    95 	 0.14788 	 0.17319 	 ~...
  108 	    96 	 0.24670 	 0.18082 	 m..s
   94 	    97 	 0.17788 	 0.18671 	 ~...
   98 	    98 	 0.18556 	 0.19335 	 ~...
  106 	    99 	 0.24105 	 0.19936 	 m..s
   99 	   100 	 0.18586 	 0.20633 	 ~...
  107 	   101 	 0.24293 	 0.20655 	 m..s
   97 	   102 	 0.18493 	 0.21217 	 ~...
  104 	   103 	 0.22126 	 0.21741 	 ~...
  101 	   104 	 0.18890 	 0.21861 	 ~...
  100 	   105 	 0.18869 	 0.22069 	 m..s
  103 	   106 	 0.22065 	 0.23016 	 ~...
  102 	   107 	 0.22004 	 0.23311 	 ~...
  111 	   108 	 0.26627 	 0.24895 	 ~...
  105 	   109 	 0.23025 	 0.25281 	 ~...
  110 	   110 	 0.26192 	 0.26014 	 ~...
  109 	   111 	 0.25947 	 0.26103 	 ~...
  120 	   112 	 0.29125 	 0.26924 	 ~...
  112 	   113 	 0.28049 	 0.27105 	 ~...
  118 	   114 	 0.28593 	 0.27224 	 ~...
  116 	   115 	 0.28462 	 0.27386 	 ~...
  119 	   116 	 0.28638 	 0.27439 	 ~...
  114 	   117 	 0.28359 	 0.27749 	 ~...
  113 	   118 	 0.28184 	 0.27913 	 ~...
  117 	   119 	 0.28583 	 0.28170 	 ~...
  115 	   120 	 0.28458 	 0.29437 	 ~...
==========================================
r_mrr = 0.9526149034500122
r2_mrr = 0.7742301225662231
spearmanr_mrr@5 = 0.9919087290763855
spearmanr_mrr@10 = 0.8649789094924927
spearmanr_mrr@50 = 0.994659423828125
spearmanr_mrr@100 = 0.9633699059486389
spearmanr_mrr@All = 0.9671865701675415
==========================================
test time: 0.441
Done Testing dataset OpenEA
total time taken: 905.5250041484833
training time taken: 869.8827650547028
TWIG out ;))
===========================================================
-----------------------------------------------------------
Running a TWIG experiment with tag: DistMult-omit-DBpedia50
-----------------------------------------------------------
===========================================================
Using random seed: 6040223866049769
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [203, 289, 437, 176, 472, 708, 86, 436, 692, 318, 209, 659, 1068, 18, 274, 1183, 588, 1073, 770, 1008, 419, 233, 28, 264, 1021, 949, 165, 989, 465, 103, 878, 253, 1165, 1193, 491, 1031, 957, 1084, 1076, 205, 128, 996, 594, 744, 725, 188, 1169, 718, 122, 607, 705, 685, 557, 14, 167, 738, 824, 1042, 1137, 1145, 508, 739, 290, 1077, 772, 262, 633, 999, 171, 732, 213, 856, 113, 879, 243, 148, 1032, 886, 81, 583, 295, 505, 752, 1103, 155, 101, 181, 776, 97, 245, 673, 898, 516, 8, 556, 358, 1157, 803, 12, 65, 907, 945, 613, 852, 1113, 186, 768, 1082, 332, 25, 586, 415, 838, 714, 959, 1211, 1061, 1060, 2, 489, 77]
valid_ids (0): []
train_ids (1094): [603, 106, 299, 733, 390, 1159, 823, 541, 280, 41, 7, 1033, 1056, 540, 531, 1013, 421, 915, 83, 580, 1184, 479, 913, 324, 864, 40, 689, 868, 932, 523, 137, 728, 854, 422, 1161, 570, 366, 63, 825, 538, 224, 1138, 1178, 385, 784, 1198, 533, 751, 30, 60, 216, 273, 1209, 4, 589, 656, 636, 231, 1123, 1212, 998, 1146, 266, 748, 543, 174, 746, 16, 6, 627, 795, 38, 681, 888, 997, 394, 707, 1019, 417, 575, 1147, 653, 1116, 225, 21, 853, 226, 339, 427, 796, 778, 648, 343, 1173, 334, 982, 162, 307, 620, 109, 974, 484, 874, 1072, 377, 272, 317, 1035, 577, 1004, 292, 1086, 1024, 204, 1196, 654, 1149, 504, 929, 966, 867, 34, 526, 1186, 1096, 316, 48, 1127, 373, 558, 236, 1199, 300, 676, 788, 190, 1030, 827, 1119, 700, 839, 221, 1, 984, 920, 1174, 727, 134, 1201, 569, 1192, 147, 629, 96, 11, 624, 889, 988, 545, 303, 814, 511, 798, 364, 813, 953, 13, 980, 515, 329, 53, 1210, 702, 163, 1059, 560, 293, 440, 1034, 1204, 409, 688, 956, 24, 922, 133, 198, 399, 39, 1182, 294, 970, 775, 647, 964, 45, 256, 288, 275, 614, 19, 454, 834, 973, 576, 383, 31, 469, 946, 127, 257, 192, 695, 759, 69, 412, 281, 616, 1003, 563, 743, 1143, 816, 675, 895, 830, 535, 457, 977, 799, 658, 214, 1111, 36, 282, 5, 678, 963, 42, 456, 1064, 509, 423, 126, 1043, 840, 1203, 474, 1114, 828, 201, 49, 741, 632, 680, 80, 522, 765, 187, 1214, 606, 458, 857, 968, 933, 365, 100, 116, 1022, 1104, 1018, 1036, 439, 433, 1150, 805, 1188, 893, 1142, 235, 175, 286, 393, 507, 706, 309, 1176, 52, 388, 480, 786, 1026, 1170, 498, 464, 1179, 1110, 62, 954, 183, 23, 836, 851, 22, 1083, 271, 379, 975, 450, 140, 1202, 495, 342, 64, 657, 27, 179, 781, 761, 861, 844, 686, 574, 721, 1108, 84, 625, 641, 260, 817, 551, 747, 1129, 1156, 1081, 150, 182, 1006, 89, 117, 735, 666, 131, 550, 934, 153, 951, 780, 667, 1213, 587, 146, 37, 220, 468, 514, 510, 585, 623, 443, 519, 121, 46, 442, 1095, 448, 925, 791, 1014, 428, 527, 777, 1005, 111, 961, 833, 698, 135, 261, 75, 125, 789, 646, 242, 239, 375, 208, 1055, 107, 118, 869, 251, 756, 1080, 723, 1044, 742, 1102, 1175, 750, 196, 152, 315, 724, 826, 890, 562, 232, 859, 552, 352, 1074, 426, 952, 567, 502, 482, 430, 159, 301, 760, 74, 1120, 476, 139, 1181, 29, 880, 615, 769, 860, 918, 626, 493, 995, 701, 930, 928, 240, 501, 333, 802, 771, 33, 967, 524, 617, 1164, 694, 887, 793, 532, 981, 525, 169, 716, 129, 779, 1141, 228, 941, 715, 1057, 431, 161, 110, 0, 438, 218, 1075, 320, 917, 79, 212, 200, 250, 486, 20, 322, 54, 528, 679, 754, 1132, 809, 278, 85, 268, 1172, 237, 983, 1001, 763, 635, 1131, 832, 908, 156, 722, 850, 279, 1094, 265, 1207, 699, 561, 1158, 471, 872, 1151, 820, 114, 1012, 141, 259, 969, 35, 72, 674, 1140, 960, 1078, 1070, 336, 444, 604, 572, 1051, 356, 631, 1126, 909, 229, 302, 1200, 650, 990, 420, 1002, 1053, 806, 1045, 819, 10, 598, 902, 344, 357, 276, 942, 1194, 962, 326, 401, 382, 539, 931, 191, 1009, 592, 459, 1097, 50, 244, 1062, 1117, 571, 323, 497, 1023, 445, 87, 605, 682, 368, 173, 1105, 376, 61, 1130, 392, 642, 944, 207, 737, 258, 950, 669, 325, 429, 926, 883, 948, 712, 269, 441, 696, 404, 972, 881, 870, 758, 1047, 609, 520, 151, 67, 136, 17, 882, 1085, 729, 168, 1136, 690, 965, 370, 346, 1208, 720, 546, 1118, 992, 414, 461, 406, 411, 1038, 104, 608, 391, 1027, 578, 211, 1098, 397, 108, 132, 924, 683, 145, 993, 1115, 1100, 537, 905, 591, 764, 227, 408, 1069, 157, 645, 901, 875, 345, 536, 785, 194, 904, 308, 855, 306, 88, 434, 1128, 858, 1093, 1195, 717, 416, 185, 1163, 119, 1148, 579, 418, 354, 847, 660, 73, 611, 835, 130, 327, 197, 794, 863, 555, 939, 801, 319, 753, 387, 1144, 371, 407, 534, 494, 160, 164, 389, 395, 831, 238, 568, 643, 313, 206, 410, 44, 986, 912, 1109, 1048, 821, 199, 1063, 865, 287, 1079, 978, 884, 662, 1191, 487, 1139, 582, 70, 398, 704, 1016, 671, 1066, 462, 1122, 1039, 138, 82, 766, 178, 384, 542, 369, 466, 195, 1029, 590, 602, 677, 687, 223, 349, 919, 713, 1107, 811, 263, 815, 1135, 1065, 112, 1171, 899, 453, 914, 1167, 600, 599, 341, 1052, 651, 719, 304, 372, 755, 749, 1011, 782, 622, 338, 1180, 518, 485, 503, 1160, 215, 362, 249, 170, 57, 773, 55, 425, 774, 193, 936, 473, 296, 1046, 234, 1153, 1190, 142, 413, 804, 1206, 1166, 66, 730, 896, 328, 1112, 1187, 565, 247, 488, 1000, 123, 241, 649, 1185, 47, 230, 396, 807, 610, 347, 71, 166, 529, 1125, 818, 787, 378, 449, 340, 691, 1121, 353, 1090, 102, 381, 697, 891, 731, 499, 639, 312, 1106, 971, 566, 991, 923, 330, 937, 180, 246, 668, 350, 757, 892, 581, 736, 921, 1010, 553, 547, 601, 843, 403, 483, 32, 380, 548, 210, 559, 363, 976, 1091, 1101, 285, 885, 837, 1099, 492, 900, 927, 217, 284, 1089, 1124, 1050, 726, 1088, 270, 693, 652, 305, 734, 655, 573, 481, 470, 115, 862, 143, 120, 432, 1134, 513, 26, 59, 910, 916, 876, 202, 92, 94, 402, 866, 619, 1037, 894, 544, 703, 321, 435, 554, 1025, 640, 331, 663, 475, 593, 822, 709, 564, 361, 1020, 189, 355, 1177, 1067, 628, 584, 1058, 452, 644, 955, 3, 767, 506, 797, 248, 849, 58, 172, 711, 808, 478, 672, 15, 1040, 99, 943, 184, 1162, 985, 1087, 810, 549, 446, 68, 374, 994, 848, 958, 903, 938, 314, 740, 252, 78, 897, 447, 298, 1049, 463, 530, 595, 684, 348, 277, 222, 940, 762, 612, 1054, 56, 105, 621, 337, 311, 935, 386, 670, 877, 783, 637, 634, 95, 979, 310, 812, 947, 792, 846, 1152, 1197, 367, 297, 158, 1007, 512, 1071, 351, 517, 477, 630, 400, 800, 1155, 845, 841, 219, 90, 597, 500, 1092, 911, 521, 1168, 661, 149, 124, 1017, 144, 451, 91, 177, 98, 359, 1154, 496, 490, 664, 829, 790, 76, 1189, 596, 906, 51, 987, 1205, 93, 1028, 360, 291, 254, 455, 665, 1015, 710, 283, 638, 424, 43, 405, 267, 460, 871, 745, 255, 467, 842, 335, 154, 618, 1133, 9, 1041, 873]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2956373704058465
the save name prefix for this run is:  chkpt-ID_2956373704058465_tag_DistMult-omit-DBpedia50
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1106
rank avg (pred): 0.469 +- 0.001
mrr vals (pred, true): 0.016, 0.131
batch losses (mrrl, rdl): 0.0, 0.0008400672

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 830
rank avg (pred): 0.129 +- 0.019
mrr vals (pred, true): 0.056, 0.513
batch losses (mrrl, rdl): 0.0, 0.0001601223

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1155
rank avg (pred): 0.261 +- 0.063
mrr vals (pred, true): 0.029, 0.128
batch losses (mrrl, rdl): 0.0, 8.54897e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 884
rank avg (pred): 0.502 +- 0.015
mrr vals (pred, true): 0.015, 0.045
batch losses (mrrl, rdl): 0.0, 0.000108919

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 9
rank avg (pred): 0.052 +- 0.046
mrr vals (pred, true): 0.233, 0.549
batch losses (mrrl, rdl): 0.0, 5.3306e-06

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 582
rank avg (pred): 0.476 +- 0.201
mrr vals (pred, true): 0.019, 0.034
batch losses (mrrl, rdl): 0.0, 2.26935e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 178
rank avg (pred): 0.416 +- 0.275
mrr vals (pred, true): 0.039, 0.046
batch losses (mrrl, rdl): 0.0, 9.1806e-06

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 632
rank avg (pred): 0.448 +- 0.315
mrr vals (pred, true): 0.043, 0.045
batch losses (mrrl, rdl): 0.0, 1.69146e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1119
rank avg (pred): 0.395 +- 0.275
mrr vals (pred, true): 0.041, 0.052
batch losses (mrrl, rdl): 0.0, 4.47112e-05

Epoch over!
epoch time: 57.328

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 810
rank avg (pred): 0.090 +- 0.085
mrr vals (pred, true): 0.200, 0.360
batch losses (mrrl, rdl): 0.0, 4.6627e-06

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 843
rank avg (pred): 0.527 +- 0.295
mrr vals (pred, true): 0.042, 0.043
batch losses (mrrl, rdl): 0.0, 2.7945e-06

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 651
rank avg (pred): 0.535 +- 0.299
mrr vals (pred, true): 0.042, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001546587

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 706
rank avg (pred): 0.432 +- 0.325
mrr vals (pred, true): 0.068, 0.049
batch losses (mrrl, rdl): 0.0, 1.90077e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1189
rank avg (pred): 0.499 +- 0.313
mrr vals (pred, true): 0.059, 0.038
batch losses (mrrl, rdl): 0.0, 3.50177e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 850
rank avg (pred): 0.473 +- 0.306
mrr vals (pred, true): 0.098, 0.037
batch losses (mrrl, rdl): 0.0, 1.21276e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 219
rank avg (pred): 0.368 +- 0.313
mrr vals (pred, true): 0.137, 0.048
batch losses (mrrl, rdl): 0.0, 9.05377e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 424
rank avg (pred): 0.337 +- 0.259
mrr vals (pred, true): 0.134, 0.043
batch losses (mrrl, rdl): 0.0, 0.0002038007

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 90
rank avg (pred): 0.445 +- 0.322
mrr vals (pred, true): 0.111, 0.127
batch losses (mrrl, rdl): 0.0, 0.0005553315

Epoch over!
epoch time: 58.2

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 552
rank avg (pred): 0.254 +- 0.277
mrr vals (pred, true): 0.157, 0.053
batch losses (mrrl, rdl): 0.0, 0.0005970257

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1010
rank avg (pred): 0.381 +- 0.311
mrr vals (pred, true): 0.139, 0.088
batch losses (mrrl, rdl): 0.0, 4.84154e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 125
rank avg (pred): 0.321 +- 0.251
mrr vals (pred, true): 0.125, 0.131
batch losses (mrrl, rdl): 0.0, 4.75662e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 427
rank avg (pred): 0.390 +- 0.297
mrr vals (pred, true): 0.141, 0.046
batch losses (mrrl, rdl): 0.0, 3.18438e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 591
rank avg (pred): 0.456 +- 0.316
mrr vals (pred, true): 0.099, 0.042
batch losses (mrrl, rdl): 0.0, 1.95391e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 970
rank avg (pred): 0.577 +- 0.311
mrr vals (pred, true): 0.072, 0.046
batch losses (mrrl, rdl): 0.0, 0.0002965272

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 693
rank avg (pred): 0.470 +- 0.296
mrr vals (pred, true): 0.098, 0.042
batch losses (mrrl, rdl): 0.0, 2.98746e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1062
rank avg (pred): 0.068 +- 0.067
mrr vals (pred, true): 0.263, 0.536
batch losses (mrrl, rdl): 0.0, 2.44503e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 998
rank avg (pred): 0.082 +- 0.078
mrr vals (pred, true): 0.249, 0.555
batch losses (mrrl, rdl): 0.0, 5.5053e-05

Epoch over!
epoch time: 59.372

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 474
rank avg (pred): 0.372 +- 0.301
mrr vals (pred, true): 0.165, 0.042
batch losses (mrrl, rdl): 0.0, 7.8811e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 447
rank avg (pred): 0.316 +- 0.281
mrr vals (pred, true): 0.199, 0.056
batch losses (mrrl, rdl): 0.0, 0.0001769058

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 889
rank avg (pred): 0.487 +- 0.322
mrr vals (pred, true): 0.134, 0.042
batch losses (mrrl, rdl): 0.0, 6.21627e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 51
rank avg (pred): 0.099 +- 0.103
mrr vals (pred, true): 0.297, 0.520
batch losses (mrrl, rdl): 0.0, 6.91668e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 740
rank avg (pred): 0.072 +- 0.079
mrr vals (pred, true): 0.393, 0.536
batch losses (mrrl, rdl): 0.0, 3.10445e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1201
rank avg (pred): 0.375 +- 0.323
mrr vals (pred, true): 0.198, 0.049
batch losses (mrrl, rdl): 0.0, 4.69724e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 722
rank avg (pred): 0.408 +- 0.314
mrr vals (pred, true): 0.168, 0.045
batch losses (mrrl, rdl): 0.0, 2.58091e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 85
rank avg (pred): 0.409 +- 0.291
mrr vals (pred, true): 0.125, 0.098
batch losses (mrrl, rdl): 0.0, 0.0001269331

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 597
rank avg (pred): 0.386 +- 0.322
mrr vals (pred, true): 0.207, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001144886

Epoch over!
epoch time: 61.148

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 76
rank avg (pred): 0.125 +- 0.124
mrr vals (pred, true): 0.210, 0.528
batch losses (mrrl, rdl): 0.0, 0.0001638031

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 222
rank avg (pred): 0.340 +- 0.278
mrr vals (pred, true): 0.222, 0.060
batch losses (mrrl, rdl): 0.0, 8.56812e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1171
rank avg (pred): 0.391 +- 0.317
mrr vals (pred, true): 0.221, 0.050
batch losses (mrrl, rdl): 0.0, 6.09194e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1039
rank avg (pred): 0.383 +- 0.296
mrr vals (pred, true): 0.203, 0.044
batch losses (mrrl, rdl): 0.0, 3.70202e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 340
rank avg (pred): 0.409 +- 0.276
mrr vals (pred, true): 0.170, 0.134
batch losses (mrrl, rdl): 0.0, 0.0004442247

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 681
rank avg (pred): 0.421 +- 0.291
mrr vals (pred, true): 0.164, 0.046
batch losses (mrrl, rdl): 0.0, 5.054e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 567
rank avg (pred): 0.448 +- 0.288
mrr vals (pred, true): 0.163, 0.050
batch losses (mrrl, rdl): 0.0, 2.227e-06

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 506
rank avg (pred): 0.154 +- 0.245
mrr vals (pred, true): 0.281, 0.242
batch losses (mrrl, rdl): 0.0, 4.25547e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 969
rank avg (pred): 0.649 +- 0.286
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 0.0, 0.0007586002

Epoch over!
epoch time: 56.031

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 470
rank avg (pred): 0.386 +- 0.292
mrr vals (pred, true): 0.227, 0.053
batch losses (mrrl, rdl): 0.3143302798, 4.15735e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 787
rank avg (pred): 0.684 +- 0.314
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002207622, 0.0011074541

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1197
rank avg (pred): 0.629 +- 0.321
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.0001651565, 0.0005568127

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 682
rank avg (pred): 0.402 +- 0.148
mrr vals (pred, true): 0.063, 0.055
batch losses (mrrl, rdl): 0.0016941772, 5.21431e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 125
rank avg (pred): 0.370 +- 0.127
mrr vals (pred, true): 0.060, 0.131
batch losses (mrrl, rdl): 0.0512753129, 0.0002104536

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 326
rank avg (pred): 0.365 +- 0.119
mrr vals (pred, true): 0.058, 0.113
batch losses (mrrl, rdl): 0.0302630197, 7.59088e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 111
rank avg (pred): 0.358 +- 0.124
mrr vals (pred, true): 0.062, 0.097
batch losses (mrrl, rdl): 0.0014139571, 7.84139e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 268
rank avg (pred): 0.168 +- 0.202
mrr vals (pred, true): 0.547, 0.554
batch losses (mrrl, rdl): 0.0005126657, 0.000539109

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 272
rank avg (pred): 0.171 +- 0.198
mrr vals (pred, true): 0.540, 0.545
batch losses (mrrl, rdl): 0.0002421174, 0.000538356

Epoch over!
epoch time: 60.119

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 74
rank avg (pred): 0.163 +- 0.191
mrr vals (pred, true): 0.531, 0.527
batch losses (mrrl, rdl): 0.0001179606, 0.0004295708

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 66
rank avg (pred): 0.167 +- 0.191
mrr vals (pred, true): 0.532, 0.516
batch losses (mrrl, rdl): 0.0027675696, 0.000400992

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 978
rank avg (pred): 0.153 +- 0.188
mrr vals (pred, true): 0.585, 0.547
batch losses (mrrl, rdl): 0.0147131123, 0.0004105491

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 73
rank avg (pred): 0.168 +- 0.189
mrr vals (pred, true): 0.535, 0.521
batch losses (mrrl, rdl): 0.0018232659, 0.0004501212

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 861
rank avg (pred): 0.473 +- 0.220
mrr vals (pred, true): 0.054, 0.104
batch losses (mrrl, rdl): 0.0255995318, 5.85849e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1205
rank avg (pred): 0.371 +- 0.124
mrr vals (pred, true): 0.062, 0.043
batch losses (mrrl, rdl): 0.0014104871, 0.0001306658

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 766
rank avg (pred): 0.431 +- 0.205
mrr vals (pred, true): 0.060, 0.050
batch losses (mrrl, rdl): 0.0010442545, 1.6241e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 759
rank avg (pred): 0.572 +- 0.265
mrr vals (pred, true): 0.030, 0.034
batch losses (mrrl, rdl): 0.003947916, 4.60329e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 966
rank avg (pred): 0.543 +- 0.276
mrr vals (pred, true): 0.041, 0.047
batch losses (mrrl, rdl): 0.0007453008, 0.0001823276

Epoch over!
epoch time: 61.087

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 53
rank avg (pred): 0.153 +- 0.173
mrr vals (pred, true): 0.522, 0.530
batch losses (mrrl, rdl): 0.0006314388, 0.0003482728

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 22
rank avg (pred): 0.151 +- 0.171
mrr vals (pred, true): 0.554, 0.551
batch losses (mrrl, rdl): 0.0001208606, 0.0003826226

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 875
rank avg (pred): 0.423 +- 0.181
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 3.692e-07, 3.46459e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 619
rank avg (pred): 0.390 +- 0.155
mrr vals (pred, true): 0.059, 0.044
batch losses (mrrl, rdl): 0.0008113664, 0.0002027477

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 542
rank avg (pred): 0.248 +- 0.144
mrr vals (pred, true): 0.145, 0.124
batch losses (mrrl, rdl): 0.0046535479, 0.0001933165

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 362
rank avg (pred): 0.244 +- 0.104
mrr vals (pred, true): 0.075, 0.108
batch losses (mrrl, rdl): 0.0108712632, 0.0001230496

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 926
rank avg (pred): 0.631 +- 0.336
mrr vals (pred, true): 0.039, 0.015
batch losses (mrrl, rdl): 0.0011297124, 0.0005906766

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 902
rank avg (pred): 0.258 +- 0.101
mrr vals (pred, true): 0.083, 0.070
batch losses (mrrl, rdl): 0.0111395968, 0.0004012607

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 924
rank avg (pred): 0.624 +- 0.331
mrr vals (pred, true): 0.035, 0.016
batch losses (mrrl, rdl): 0.0023597027, 0.0005117893

Epoch over!
epoch time: 62.673

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1030
rank avg (pred): 0.284 +- 0.078
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.0001602433, 0.0005688675

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1004
rank avg (pred): 0.248 +- 0.089
mrr vals (pred, true): 0.062, 0.101
batch losses (mrrl, rdl): 0.0158050042, 0.0001632179

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 41
rank avg (pred): 0.148 +- 0.160
mrr vals (pred, true): 0.511, 0.530
batch losses (mrrl, rdl): 0.0039177127, 0.0003211733

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 212
rank avg (pred): 0.274 +- 0.088
mrr vals (pred, true): 0.074, 0.049
batch losses (mrrl, rdl): 0.0055962992, 0.0006562638

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 5
rank avg (pred): 0.146 +- 0.161
mrr vals (pred, true): 0.540, 0.522
batch losses (mrrl, rdl): 0.0030746057, 0.0003200301

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1189
rank avg (pred): 0.430 +- 0.203
mrr vals (pred, true): 0.060, 0.038
batch losses (mrrl, rdl): 0.0010689074, 1.90996e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 374
rank avg (pred): 0.208 +- 0.102
mrr vals (pred, true): 0.076, 0.108
batch losses (mrrl, rdl): 0.009818553, 0.0003126164

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 746
rank avg (pred): 0.156 +- 0.149
mrr vals (pred, true): 0.195, 0.170
batch losses (mrrl, rdl): 0.0063100043, 0.0003280553

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 302
rank avg (pred): 0.134 +- 0.153
mrr vals (pred, true): 0.545, 0.555
batch losses (mrrl, rdl): 0.0008843417, 0.0002799614

Epoch over!
epoch time: 63.672

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 68
rank avg (pred): 0.139 +- 0.156
mrr vals (pred, true): 0.530, 0.541
batch losses (mrrl, rdl): 0.0010461276, 0.0002844388

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 885
rank avg (pred): 0.534 +- 0.277
mrr vals (pred, true): 0.042, 0.051
batch losses (mrrl, rdl): 0.0005873099, 0.0001374491

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1001
rank avg (pred): 0.216 +- 0.092
mrr vals (pred, true): 0.084, 0.124
batch losses (mrrl, rdl): 0.0160325058, 0.0001491239

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1144
rank avg (pred): 0.283 +- 0.102
mrr vals (pred, true): 0.082, 0.140
batch losses (mrrl, rdl): 0.033013247, 7.80004e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 757
rank avg (pred): 0.620 +- 0.354
mrr vals (pred, true): 0.040, 0.055
batch losses (mrrl, rdl): 0.0009274024, 0.0003339765

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 95
rank avg (pred): 0.183 +- 0.112
mrr vals (pred, true): 0.087, 0.102
batch losses (mrrl, rdl): 0.0024554827, 0.0003056392

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 443
rank avg (pred): 0.262 +- 0.075
mrr vals (pred, true): 0.064, 0.045
batch losses (mrrl, rdl): 0.0020515281, 0.00073482

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 626
rank avg (pred): 0.327 +- 0.111
mrr vals (pred, true): 0.060, 0.040
batch losses (mrrl, rdl): 0.0009303149, 0.0005156815

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1192
rank avg (pred): 0.412 +- 0.220
mrr vals (pred, true): 0.069, 0.051
batch losses (mrrl, rdl): 0.0035194596, 2.87759e-05

Epoch over!
epoch time: 63.074

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1090
rank avg (pred): 0.309 +- 0.105
mrr vals (pred, true): 0.064, 0.089
batch losses (mrrl, rdl): 0.0019413054, 6.86141e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 873
rank avg (pred): 0.547 +- 0.320
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 8.23661e-05, 0.000186537

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1027
rank avg (pred): 0.193 +- 0.096
mrr vals (pred, true): 0.078, 0.045
batch losses (mrrl, rdl): 0.0079984395, 0.0013660179

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 479
rank avg (pred): 0.256 +- 0.081
mrr vals (pred, true): 0.059, 0.042
batch losses (mrrl, rdl): 0.0007717102, 0.0007585561

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 482
rank avg (pred): 0.248 +- 0.123
mrr vals (pred, true): 0.080, 0.050
batch losses (mrrl, rdl): 0.0087212957, 0.0007606795

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 962
rank avg (pred): 0.627 +- 0.363
mrr vals (pred, true): 0.034, 0.047
batch losses (mrrl, rdl): 0.0026482579, 0.0004451163

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 326
rank avg (pred): 0.210 +- 0.090
mrr vals (pred, true): 0.073, 0.113
batch losses (mrrl, rdl): 0.0160892867, 0.0003814588

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 593
rank avg (pred): 0.451 +- 0.271
mrr vals (pred, true): 0.083, 0.042
batch losses (mrrl, rdl): 0.0106041674, 2.2998e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 47
rank avg (pred): 0.130 +- 0.140
mrr vals (pred, true): 0.533, 0.532
batch losses (mrrl, rdl): 4.741e-07, 0.000219905

Epoch over!
epoch time: 62.073

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 334
rank avg (pred): 0.215 +- 0.134
mrr vals (pred, true): 0.076, 0.154
batch losses (mrrl, rdl): 0.0605786182, 9.77203e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 544
rank avg (pred): 0.231 +- 0.175
mrr vals (pred, true): 0.094, 0.080
batch losses (mrrl, rdl): 0.0190463867, 0.0006685054

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 163
rank avg (pred): 0.188 +- 0.099
mrr vals (pred, true): 0.081, 0.054
batch losses (mrrl, rdl): 0.0099021485, 0.0014444056

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 521
rank avg (pred): 0.195 +- 0.217
mrr vals (pred, true): 0.154, 0.135
batch losses (mrrl, rdl): 0.0036437393, 0.0003805138

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 525
rank avg (pred): 0.404 +- 0.246
mrr vals (pred, true): 0.101, 0.057
batch losses (mrrl, rdl): 0.0262636691, 5.06548e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 229
rank avg (pred): 0.237 +- 0.136
mrr vals (pred, true): 0.077, 0.041
batch losses (mrrl, rdl): 0.0074057104, 0.0009797544

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 34
rank avg (pred): 0.115 +- 0.126
mrr vals (pred, true): 0.535, 0.523
batch losses (mrrl, rdl): 0.0015299602, 0.0001478198

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1176
rank avg (pred): 0.360 +- 0.191
mrr vals (pred, true): 0.060, 0.036
batch losses (mrrl, rdl): 0.0009156007, 0.0003242423

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 794
rank avg (pred): 0.592 +- 0.383
mrr vals (pred, true): 0.049, 0.051
batch losses (mrrl, rdl): 2.09984e-05, 0.0003305361

Epoch over!
epoch time: 57.678

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 19
rank avg (pred): 0.115 +- 0.125
mrr vals (pred, true): 0.545, 0.549
batch losses (mrrl, rdl): 0.0001330604, 0.0001592144

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 775
rank avg (pred): 0.603 +- 0.385
mrr vals (pred, true): 0.047, 0.055
batch losses (mrrl, rdl): 6.79907e-05, 0.0003700393

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 463
rank avg (pred): 0.210 +- 0.135
mrr vals (pred, true): 0.089, 0.042
batch losses (mrrl, rdl): 0.0151503757, 0.001105956

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 64
rank avg (pred): 0.114 +- 0.123
mrr vals (pred, true): 0.519, 0.529
batch losses (mrrl, rdl): 0.0008408664, 0.0001304968

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 67
rank avg (pred): 0.109 +- 0.119
mrr vals (pred, true): 0.542, 0.535
batch losses (mrrl, rdl): 0.0004789685, 0.0001228372

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1197
rank avg (pred): 0.367 +- 0.200
mrr vals (pred, true): 0.055, 0.045
batch losses (mrrl, rdl): 0.000252126, 0.0001261016

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 563
rank avg (pred): 0.322 +- 0.198
mrr vals (pred, true): 0.087, 0.112
batch losses (mrrl, rdl): 0.0060872352, 4.99311e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 143
rank avg (pred): 0.214 +- 0.165
mrr vals (pred, true): 0.089, 0.118
batch losses (mrrl, rdl): 0.0084699653, 0.0002463731

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 523
rank avg (pred): 0.359 +- 0.224
mrr vals (pred, true): 0.094, 0.072
batch losses (mrrl, rdl): 0.0196887348, 0.0001245258

Epoch over!
epoch time: 61.534

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 13
rank avg (pred): 0.106 +- 0.113
mrr vals (pred, true): 0.537, 0.551
batch losses (mrrl, rdl): 0.0021638437, 0.0001325457

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 485
rank avg (pred): 0.222 +- 0.173
mrr vals (pred, true): 0.075, 0.047
batch losses (mrrl, rdl): 0.0062271971, 0.0008999701

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 699
rank avg (pred): 0.359 +- 0.211
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0003008476, 0.0002210185

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 833
rank avg (pred): 0.105 +- 0.110
mrr vals (pred, true): 0.421, 0.469
batch losses (mrrl, rdl): 0.023408914, 6.61217e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1158
rank avg (pred): 0.227 +- 0.202
mrr vals (pred, true): 0.095, 0.127
batch losses (mrrl, rdl): 0.0103138555, 6.79515e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1151
rank avg (pred): 0.183 +- 0.211
mrr vals (pred, true): 0.121, 0.148
batch losses (mrrl, rdl): 0.0072096852, 0.0001357526

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 568
rank avg (pred): 0.447 +- 0.280
mrr vals (pred, true): 0.054, 0.035
batch losses (mrrl, rdl): 0.0001630733, 3.36068e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 645
rank avg (pred): 0.342 +- 0.212
mrr vals (pred, true): 0.067, 0.034
batch losses (mrrl, rdl): 0.0030112721, 0.0004348693

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 378
rank avg (pred): 0.196 +- 0.098
mrr vals (pred, true): 0.065, 0.083
batch losses (mrrl, rdl): 0.0022980317, 0.0005153798

Epoch over!
epoch time: 60.802

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1007
rank avg (pred): 0.224 +- 0.184
mrr vals (pred, true): 0.084, 0.141
batch losses (mrrl, rdl): 0.0327350385, 7.75268e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 412
rank avg (pred): 0.167 +- 0.101
mrr vals (pred, true): 0.082, 0.051
batch losses (mrrl, rdl): 0.0102562774, 0.0016813071

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1112
rank avg (pred): 0.183 +- 0.166
mrr vals (pred, true): 0.077, 0.049
batch losses (mrrl, rdl): 0.0072589647, 0.0012521506

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1123
rank avg (pred): 0.137 +- 0.083
mrr vals (pred, true): 0.099, 0.047
batch losses (mrrl, rdl): 0.0238006394, 0.0021207696

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1022
rank avg (pred): 0.191 +- 0.140
mrr vals (pred, true): 0.074, 0.116
batch losses (mrrl, rdl): 0.0173685476, 0.000215703

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1118
rank avg (pred): 0.184 +- 0.161
mrr vals (pred, true): 0.073, 0.048
batch losses (mrrl, rdl): 0.0054858234, 0.0014553063

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 468
rank avg (pred): 0.170 +- 0.128
mrr vals (pred, true): 0.076, 0.045
batch losses (mrrl, rdl): 0.0065829093, 0.0016871379

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 997
rank avg (pred): 0.090 +- 0.096
mrr vals (pred, true): 0.539, 0.547
batch losses (mrrl, rdl): 0.000616336, 7.93982e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1162
rank avg (pred): 0.526 +- 0.373
mrr vals (pred, true): 0.050, 0.038
batch losses (mrrl, rdl): 1.2877e-06, 8.06503e-05

Epoch over!
epoch time: 59.693

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.214 +- 0.172
mrr vals (pred, true): 0.062, 0.050

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.04236 	 0.01876 	 ~...
   12 	     1 	 0.04133 	 0.03204 	 ~...
    8 	     2 	 0.04063 	 0.03255 	 ~...
   23 	     3 	 0.05065 	 0.03399 	 ~...
   24 	     4 	 0.05286 	 0.03679 	 ~...
   24 	     5 	 0.05286 	 0.03917 	 ~...
   20 	     6 	 0.04969 	 0.03921 	 ~...
   38 	     7 	 0.05978 	 0.04091 	 ~...
   17 	     8 	 0.04786 	 0.04242 	 ~...
   66 	     9 	 0.06384 	 0.04294 	 ~...
    9 	    10 	 0.04083 	 0.04304 	 ~...
   46 	    11 	 0.06201 	 0.04365 	 ~...
   24 	    12 	 0.05286 	 0.04430 	 ~...
    5 	    13 	 0.04020 	 0.04449 	 ~...
    0 	    14 	 0.03640 	 0.04453 	 ~...
   72 	    15 	 0.06419 	 0.04460 	 ~...
   24 	    16 	 0.05286 	 0.04508 	 ~...
   24 	    17 	 0.05286 	 0.04536 	 ~...
   24 	    18 	 0.05286 	 0.04559 	 ~...
   11 	    19 	 0.04093 	 0.04563 	 ~...
   24 	    20 	 0.05286 	 0.04595 	 ~...
   18 	    21 	 0.04831 	 0.04598 	 ~...
   24 	    22 	 0.05286 	 0.04626 	 ~...
    6 	    23 	 0.04025 	 0.04633 	 ~...
   24 	    24 	 0.05286 	 0.04665 	 ~...
    1 	    25 	 0.03670 	 0.04678 	 ~...
   57 	    26 	 0.06255 	 0.04688 	 ~...
   42 	    27 	 0.06191 	 0.04698 	 ~...
   21 	    28 	 0.04973 	 0.04716 	 ~...
   76 	    29 	 0.06777 	 0.04730 	 ~...
   53 	    30 	 0.06233 	 0.04744 	 ~...
   68 	    31 	 0.06402 	 0.04759 	 ~...
   69 	    32 	 0.06412 	 0.04771 	 ~...
   62 	    33 	 0.06334 	 0.04779 	 ~...
   58 	    34 	 0.06270 	 0.04832 	 ~...
   75 	    35 	 0.06752 	 0.04832 	 ~...
   24 	    36 	 0.05286 	 0.04844 	 ~...
   22 	    37 	 0.04999 	 0.04856 	 ~...
   54 	    38 	 0.06245 	 0.04870 	 ~...
   44 	    39 	 0.06201 	 0.04934 	 ~...
   47 	    40 	 0.06214 	 0.04955 	 ~...
   64 	    41 	 0.06354 	 0.04989 	 ~...
   56 	    42 	 0.06255 	 0.04990 	 ~...
    3 	    43 	 0.03953 	 0.05022 	 ~...
   67 	    44 	 0.06400 	 0.05054 	 ~...
   24 	    45 	 0.05286 	 0.05069 	 ~...
   73 	    46 	 0.06490 	 0.05074 	 ~...
   24 	    47 	 0.05286 	 0.05122 	 ~...
   14 	    48 	 0.04174 	 0.05144 	 ~...
   65 	    49 	 0.06369 	 0.05147 	 ~...
   52 	    50 	 0.06226 	 0.05240 	 ~...
   37 	    51 	 0.05297 	 0.05274 	 ~...
   16 	    52 	 0.04745 	 0.05344 	 ~...
   19 	    53 	 0.04882 	 0.05362 	 ~...
   13 	    54 	 0.04140 	 0.05419 	 ~...
   10 	    55 	 0.04086 	 0.05484 	 ~...
    4 	    56 	 0.03986 	 0.05749 	 ~...
   24 	    57 	 0.05286 	 0.05890 	 ~...
   78 	    58 	 0.08370 	 0.06022 	 ~...
    2 	    59 	 0.03765 	 0.06067 	 ~...
   79 	    60 	 0.08374 	 0.06245 	 ~...
   77 	    61 	 0.06795 	 0.07881 	 ~...
   71 	    62 	 0.06415 	 0.08456 	 ~...
   70 	    63 	 0.06413 	 0.08797 	 ~...
   63 	    64 	 0.06340 	 0.09087 	 ~...
   59 	    65 	 0.06286 	 0.09187 	 ~...
   45 	    66 	 0.06201 	 0.10013 	 m..s
   50 	    67 	 0.06217 	 0.10125 	 m..s
    7 	    68 	 0.04032 	 0.10419 	 m..s
   60 	    69 	 0.06297 	 0.10464 	 m..s
   82 	    70 	 0.10203 	 0.10979 	 ~...
   51 	    71 	 0.06223 	 0.11112 	 m..s
   48 	    72 	 0.06215 	 0.11350 	 m..s
   55 	    73 	 0.06248 	 0.12025 	 m..s
   41 	    74 	 0.06128 	 0.13160 	 m..s
   43 	    75 	 0.06201 	 0.13310 	 m..s
   81 	    76 	 0.09168 	 0.13415 	 m..s
   39 	    77 	 0.06008 	 0.13780 	 m..s
   61 	    78 	 0.06299 	 0.14090 	 m..s
   80 	    79 	 0.08898 	 0.14114 	 m..s
   49 	    80 	 0.06215 	 0.14243 	 m..s
   74 	    81 	 0.06564 	 0.14586 	 m..s
   40 	    82 	 0.06056 	 0.15690 	 m..s
   83 	    83 	 0.12350 	 0.18459 	 m..s
   84 	    84 	 0.13455 	 0.20347 	 m..s
   85 	    85 	 0.14522 	 0.20778 	 m..s
   86 	    86 	 0.15939 	 0.22949 	 m..s
   88 	    87 	 0.27529 	 0.24936 	 ~...
   89 	    88 	 0.27780 	 0.24983 	 ~...
   87 	    89 	 0.18871 	 0.28799 	 m..s
   90 	    90 	 0.48197 	 0.42624 	 m..s
   91 	    91 	 0.49101 	 0.45875 	 m..s
   92 	    92 	 0.50640 	 0.51881 	 ~...
   93 	    93 	 0.50839 	 0.51956 	 ~...
   95 	    94 	 0.53040 	 0.53116 	 ~...
   94 	    95 	 0.52856 	 0.53137 	 ~...
  116 	    96 	 0.54930 	 0.53145 	 ~...
  103 	    97 	 0.54089 	 0.53150 	 ~...
  112 	    98 	 0.54630 	 0.53200 	 ~...
  102 	    99 	 0.54062 	 0.53399 	 ~...
  107 	   100 	 0.54255 	 0.53484 	 ~...
  104 	   101 	 0.54181 	 0.53652 	 ~...
   98 	   102 	 0.53835 	 0.53719 	 ~...
   97 	   103 	 0.53742 	 0.53799 	 ~...
  109 	   104 	 0.54306 	 0.53966 	 ~...
  111 	   105 	 0.54621 	 0.54053 	 ~...
  106 	   106 	 0.54206 	 0.54425 	 ~...
  101 	   107 	 0.53898 	 0.54633 	 ~...
   96 	   108 	 0.53057 	 0.54664 	 ~...
  120 	   109 	 0.55371 	 0.54935 	 ~...
   99 	   110 	 0.53845 	 0.55007 	 ~...
  105 	   111 	 0.54198 	 0.55053 	 ~...
  113 	   112 	 0.54646 	 0.55731 	 ~...
  119 	   113 	 0.55363 	 0.55836 	 ~...
  114 	   114 	 0.54700 	 0.55872 	 ~...
  117 	   115 	 0.54970 	 0.55948 	 ~...
  108 	   116 	 0.54276 	 0.56058 	 ~...
  110 	   117 	 0.54375 	 0.56083 	 ~...
  118 	   118 	 0.54971 	 0.56198 	 ~...
  115 	   119 	 0.54907 	 0.56332 	 ~...
  100 	   120 	 0.53863 	 0.56693 	 ~...
==========================================
r_mrr = 0.9897850155830383
r2_mrr = 0.9776663780212402
spearmanr_mrr@5 = 0.8588445782661438
spearmanr_mrr@10 = 0.843477189540863
spearmanr_mrr@50 = 0.9960645437240601
spearmanr_mrr@100 = 0.9925017952919006
spearmanr_mrr@All = 0.9929516315460205
==========================================
test time: 0.716
Done Testing dataset UMLS
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.321 +- 0.325
mrr vals (pred, true): 0.099, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   22 	     0 	 0.07990 	 0.00091 	 m..s
   14 	     1 	 0.06292 	 0.00138 	 m..s
   50 	     2 	 0.09307 	 0.00327 	 m..s
   44 	     3 	 0.08640 	 0.00333 	 m..s
   63 	     4 	 0.09784 	 0.00342 	 m..s
    6 	     5 	 0.05687 	 0.00343 	 m..s
   78 	     6 	 0.10235 	 0.00343 	 m..s
   59 	     7 	 0.09690 	 0.00346 	 m..s
   65 	     8 	 0.09810 	 0.00357 	 m..s
   86 	     9 	 0.11025 	 0.00362 	 MISS
   20 	    10 	 0.07805 	 0.00373 	 m..s
   64 	    11 	 0.09800 	 0.00375 	 m..s
   73 	    12 	 0.10040 	 0.00382 	 m..s
   31 	    13 	 0.08609 	 0.00385 	 m..s
   31 	    14 	 0.08609 	 0.00390 	 m..s
   80 	    15 	 0.10343 	 0.00392 	 m..s
    5 	    16 	 0.05668 	 0.00393 	 m..s
   31 	    17 	 0.08609 	 0.00395 	 m..s
   31 	    18 	 0.08609 	 0.00395 	 m..s
   31 	    19 	 0.08609 	 0.00396 	 m..s
   72 	    20 	 0.09943 	 0.00400 	 m..s
   53 	    21 	 0.09658 	 0.00402 	 m..s
   31 	    22 	 0.08609 	 0.00411 	 m..s
   67 	    23 	 0.09882 	 0.00413 	 m..s
   74 	    24 	 0.10128 	 0.00416 	 m..s
   56 	    25 	 0.09679 	 0.00418 	 m..s
   76 	    26 	 0.10157 	 0.00419 	 m..s
    0 	    27 	 0.03898 	 0.00431 	 m..s
   47 	    28 	 0.09009 	 0.00432 	 m..s
   51 	    29 	 0.09510 	 0.00438 	 m..s
   17 	    30 	 0.07560 	 0.00441 	 m..s
   31 	    31 	 0.08609 	 0.00442 	 m..s
    1 	    32 	 0.04053 	 0.00443 	 m..s
   13 	    33 	 0.06169 	 0.00444 	 m..s
    3 	    34 	 0.05382 	 0.00445 	 m..s
   10 	    35 	 0.05937 	 0.00445 	 m..s
   49 	    36 	 0.09145 	 0.00451 	 m..s
   54 	    37 	 0.09665 	 0.00474 	 m..s
   87 	    38 	 0.11054 	 0.00486 	 MISS
    2 	    39 	 0.04539 	 0.00495 	 m..s
   61 	    40 	 0.09714 	 0.00521 	 m..s
   31 	    41 	 0.08609 	 0.00559 	 m..s
    9 	    42 	 0.05925 	 0.00607 	 m..s
   83 	    43 	 0.10577 	 0.00639 	 m..s
    4 	    44 	 0.05534 	 0.02089 	 m..s
    8 	    45 	 0.05842 	 0.02595 	 m..s
   29 	    46 	 0.08563 	 0.03850 	 m..s
    7 	    47 	 0.05718 	 0.03909 	 ~...
   26 	    48 	 0.08311 	 0.04495 	 m..s
   12 	    49 	 0.06141 	 0.04586 	 ~...
   24 	    50 	 0.08208 	 0.04889 	 m..s
   15 	    51 	 0.06307 	 0.05678 	 ~...
   23 	    52 	 0.08207 	 0.05691 	 ~...
   27 	    53 	 0.08333 	 0.05762 	 ~...
   85 	    54 	 0.10890 	 0.06720 	 m..s
   11 	    55 	 0.05968 	 0.07508 	 ~...
   21 	    56 	 0.07893 	 0.11018 	 m..s
   16 	    57 	 0.07062 	 0.11825 	 m..s
   31 	    58 	 0.08609 	 0.12053 	 m..s
   92 	    59 	 0.20723 	 0.12831 	 m..s
   25 	    60 	 0.08246 	 0.12878 	 m..s
   31 	    61 	 0.08609 	 0.12907 	 m..s
   28 	    62 	 0.08547 	 0.14456 	 m..s
   19 	    63 	 0.07782 	 0.14468 	 m..s
   18 	    64 	 0.07646 	 0.14724 	 m..s
   30 	    65 	 0.08601 	 0.15307 	 m..s
   31 	    66 	 0.08609 	 0.15393 	 m..s
   31 	    67 	 0.08609 	 0.15735 	 m..s
   89 	    68 	 0.20403 	 0.15748 	 m..s
   45 	    69 	 0.08674 	 0.15763 	 m..s
   31 	    70 	 0.08609 	 0.16391 	 m..s
   48 	    71 	 0.09096 	 0.16622 	 m..s
   91 	    72 	 0.20714 	 0.16695 	 m..s
   90 	    73 	 0.20550 	 0.17482 	 m..s
   66 	    74 	 0.09812 	 0.17541 	 m..s
   46 	    75 	 0.08743 	 0.17543 	 m..s
   81 	    76 	 0.10347 	 0.17776 	 m..s
   77 	    77 	 0.10168 	 0.18028 	 m..s
   88 	    78 	 0.11062 	 0.18239 	 m..s
   84 	    79 	 0.10875 	 0.19231 	 m..s
   70 	    80 	 0.09915 	 0.19245 	 m..s
   57 	    81 	 0.09686 	 0.19510 	 m..s
   98 	    82 	 0.24293 	 0.19708 	 m..s
   55 	    83 	 0.09675 	 0.19726 	 MISS
   75 	    84 	 0.10152 	 0.19766 	 m..s
   68 	    85 	 0.09889 	 0.19778 	 m..s
   69 	    86 	 0.09890 	 0.19892 	 MISS
   60 	    87 	 0.09712 	 0.19924 	 MISS
   58 	    88 	 0.09687 	 0.20042 	 MISS
  101 	    89 	 0.24524 	 0.20056 	 m..s
   82 	    90 	 0.10558 	 0.20081 	 m..s
   62 	    91 	 0.09732 	 0.20143 	 MISS
   71 	    92 	 0.09927 	 0.20695 	 MISS
   79 	    93 	 0.10267 	 0.20811 	 MISS
  114 	    94 	 0.26333 	 0.20951 	 m..s
   52 	    95 	 0.09596 	 0.21990 	 MISS
  103 	    96 	 0.24918 	 0.21997 	 ~...
  105 	    97 	 0.25664 	 0.22199 	 m..s
  108 	    98 	 0.25940 	 0.22270 	 m..s
  102 	    99 	 0.24786 	 0.22495 	 ~...
  100 	   100 	 0.24466 	 0.22640 	 ~...
   97 	   101 	 0.23718 	 0.22675 	 ~...
  112 	   102 	 0.26040 	 0.23018 	 m..s
   99 	   103 	 0.24430 	 0.23706 	 ~...
  104 	   104 	 0.25056 	 0.23893 	 ~...
   95 	   105 	 0.23002 	 0.23944 	 ~...
   94 	   106 	 0.22747 	 0.24123 	 ~...
   93 	   107 	 0.21162 	 0.24317 	 m..s
  106 	   108 	 0.25809 	 0.24480 	 ~...
   96 	   109 	 0.23174 	 0.24744 	 ~...
  107 	   110 	 0.25906 	 0.25739 	 ~...
  118 	   111 	 0.27028 	 0.27607 	 ~...
  117 	   112 	 0.27010 	 0.27841 	 ~...
  111 	   113 	 0.26033 	 0.27880 	 ~...
  119 	   114 	 0.27166 	 0.29138 	 ~...
  110 	   115 	 0.26019 	 0.29784 	 m..s
  109 	   116 	 0.26000 	 0.29787 	 m..s
  113 	   117 	 0.26116 	 0.29862 	 m..s
  120 	   118 	 0.27290 	 0.30182 	 ~...
  115 	   119 	 0.26527 	 0.31364 	 m..s
  116 	   120 	 0.26770 	 0.32036 	 m..s
==========================================
r_mrr = 0.7597191333770752
r2_mrr = 0.5614413022994995
spearmanr_mrr@5 = 0.8960381746292114
spearmanr_mrr@10 = 0.9509882926940918
spearmanr_mrr@50 = 0.8281006217002869
spearmanr_mrr@100 = 0.8115621209144592
spearmanr_mrr@All = 0.8478193283081055
==========================================
test time: 0.467
Done Testing dataset CoDExSmall
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.131 +- 0.074
mrr vals (pred, true): 0.120, 0.053

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.06924 	 0.02078 	 m..s
   37 	     1 	 0.12106 	 0.04634 	 m..s
   65 	     2 	 0.12935 	 0.04654 	 m..s
    8 	     3 	 0.05690 	 0.04803 	 ~...
   10 	     4 	 0.05858 	 0.04957 	 ~...
   67 	     5 	 0.13024 	 0.04999 	 m..s
    4 	     6 	 0.05193 	 0.05087 	 ~...
    0 	     7 	 0.03009 	 0.05114 	 ~...
   12 	     8 	 0.06237 	 0.05131 	 ~...
   33 	     9 	 0.11932 	 0.05140 	 m..s
   41 	    10 	 0.12130 	 0.05168 	 m..s
   29 	    11 	 0.11782 	 0.05181 	 m..s
   69 	    12 	 0.13310 	 0.05186 	 m..s
   50 	    13 	 0.12892 	 0.05189 	 m..s
   50 	    14 	 0.12892 	 0.05193 	 m..s
   25 	    15 	 0.11470 	 0.05196 	 m..s
   50 	    16 	 0.12892 	 0.05227 	 m..s
    9 	    17 	 0.05838 	 0.05244 	 ~...
    5 	    18 	 0.05406 	 0.05249 	 ~...
    2 	    19 	 0.03570 	 0.05254 	 ~...
    3 	    20 	 0.04956 	 0.05262 	 ~...
   63 	    21 	 0.12894 	 0.05263 	 m..s
   22 	    22 	 0.11182 	 0.05279 	 m..s
   32 	    23 	 0.11889 	 0.05286 	 m..s
   36 	    24 	 0.12008 	 0.05297 	 m..s
   42 	    25 	 0.12206 	 0.05298 	 m..s
   45 	    26 	 0.12492 	 0.05299 	 m..s
   50 	    27 	 0.12892 	 0.05352 	 m..s
    7 	    28 	 0.05485 	 0.05370 	 ~...
   39 	    29 	 0.12112 	 0.05379 	 m..s
   50 	    30 	 0.12892 	 0.05381 	 m..s
   31 	    31 	 0.11835 	 0.05398 	 m..s
    6 	    32 	 0.05436 	 0.05415 	 ~...
   66 	    33 	 0.12989 	 0.05439 	 m..s
   77 	    34 	 0.14574 	 0.05453 	 m..s
   76 	    35 	 0.14350 	 0.05465 	 m..s
   50 	    36 	 0.12892 	 0.05474 	 m..s
   28 	    37 	 0.11671 	 0.05488 	 m..s
   50 	    38 	 0.12892 	 0.05560 	 m..s
   14 	    39 	 0.06569 	 0.05582 	 ~...
    1 	    40 	 0.03118 	 0.05620 	 ~...
   13 	    41 	 0.06292 	 0.05683 	 ~...
   72 	    42 	 0.13598 	 0.05707 	 m..s
   71 	    43 	 0.13562 	 0.05743 	 m..s
   50 	    44 	 0.12892 	 0.05840 	 m..s
   11 	    45 	 0.05914 	 0.06022 	 ~...
   24 	    46 	 0.11459 	 0.06074 	 m..s
   49 	    47 	 0.12700 	 0.06078 	 m..s
   16 	    48 	 0.10283 	 0.06102 	 m..s
   44 	    49 	 0.12370 	 0.09433 	 ~...
   34 	    50 	 0.11992 	 0.18207 	 m..s
   78 	    51 	 0.20831 	 0.20613 	 ~...
   50 	    52 	 0.12892 	 0.21624 	 m..s
   19 	    53 	 0.11169 	 0.21829 	 MISS
   40 	    54 	 0.12114 	 0.22764 	 MISS
   46 	    55 	 0.12545 	 0.22885 	 MISS
   38 	    56 	 0.12109 	 0.22897 	 MISS
   27 	    57 	 0.11535 	 0.23242 	 MISS
   17 	    58 	 0.10784 	 0.23295 	 MISS
   43 	    59 	 0.12338 	 0.23382 	 MISS
   20 	    60 	 0.11178 	 0.23687 	 MISS
   50 	    61 	 0.12892 	 0.23724 	 MISS
   23 	    62 	 0.11455 	 0.23729 	 MISS
   74 	    63 	 0.13962 	 0.24026 	 MISS
   30 	    64 	 0.11820 	 0.24262 	 MISS
   47 	    65 	 0.12580 	 0.24319 	 MISS
   50 	    66 	 0.12892 	 0.24547 	 MISS
   70 	    67 	 0.13544 	 0.24569 	 MISS
   75 	    68 	 0.14152 	 0.24581 	 MISS
   50 	    69 	 0.12892 	 0.24669 	 MISS
   73 	    70 	 0.13685 	 0.24880 	 MISS
   18 	    71 	 0.11135 	 0.24952 	 MISS
   68 	    72 	 0.13267 	 0.25748 	 MISS
   79 	    73 	 0.30487 	 0.26032 	 m..s
   26 	    74 	 0.11514 	 0.26135 	 MISS
   50 	    75 	 0.12892 	 0.26338 	 MISS
   35 	    76 	 0.12005 	 0.26716 	 MISS
   64 	    77 	 0.12897 	 0.26746 	 MISS
   21 	    78 	 0.11180 	 0.27501 	 MISS
   48 	    79 	 0.12592 	 0.27928 	 MISS
   85 	    80 	 0.40304 	 0.35525 	 m..s
   82 	    81 	 0.39419 	 0.37401 	 ~...
   81 	    82 	 0.38647 	 0.37669 	 ~...
   93 	    83 	 0.42578 	 0.38105 	 m..s
   87 	    84 	 0.41059 	 0.39224 	 ~...
   84 	    85 	 0.40039 	 0.39308 	 ~...
   83 	    86 	 0.39527 	 0.39310 	 ~...
   80 	    87 	 0.38297 	 0.39474 	 ~...
   90 	    88 	 0.41798 	 0.39919 	 ~...
   88 	    89 	 0.41556 	 0.40465 	 ~...
   89 	    90 	 0.41632 	 0.40495 	 ~...
   91 	    91 	 0.42197 	 0.41130 	 ~...
   92 	    92 	 0.42317 	 0.41259 	 ~...
  112 	    93 	 0.44820 	 0.41419 	 m..s
  105 	    94 	 0.44727 	 0.41563 	 m..s
   95 	    95 	 0.42922 	 0.41934 	 ~...
   86 	    96 	 0.40859 	 0.42314 	 ~...
  107 	    97 	 0.44740 	 0.42959 	 ~...
  117 	    98 	 0.45416 	 0.43284 	 ~...
   96 	    99 	 0.42929 	 0.43359 	 ~...
   98 	   100 	 0.44130 	 0.43393 	 ~...
  116 	   101 	 0.45403 	 0.43568 	 ~...
  104 	   102 	 0.44690 	 0.43783 	 ~...
  103 	   103 	 0.44414 	 0.43931 	 ~...
  106 	   104 	 0.44738 	 0.43967 	 ~...
   97 	   105 	 0.44037 	 0.44075 	 ~...
   94 	   106 	 0.42684 	 0.44232 	 ~...
  100 	   107 	 0.44160 	 0.44344 	 ~...
  108 	   108 	 0.44772 	 0.44527 	 ~...
  101 	   109 	 0.44206 	 0.44555 	 ~...
  111 	   110 	 0.44820 	 0.44604 	 ~...
  109 	   111 	 0.44790 	 0.44691 	 ~...
  115 	   112 	 0.45351 	 0.44706 	 ~...
  102 	   113 	 0.44343 	 0.44770 	 ~...
   99 	   114 	 0.44147 	 0.45054 	 ~...
  113 	   115 	 0.44834 	 0.45098 	 ~...
  118 	   116 	 0.45424 	 0.45100 	 ~...
  110 	   117 	 0.44809 	 0.45483 	 ~...
  119 	   118 	 0.45705 	 0.46245 	 ~...
  114 	   119 	 0.44893 	 0.46786 	 ~...
  120 	   120 	 0.45736 	 0.47648 	 ~...
==========================================
r_mrr = 0.9029305577278137
r2_mrr = 0.8117469549179077
spearmanr_mrr@5 = 0.898774266242981
spearmanr_mrr@10 = 0.8486147522926331
spearmanr_mrr@50 = 0.964065670967102
spearmanr_mrr@100 = 0.9047215580940247
spearmanr_mrr@All = 0.923957347869873
==========================================
test time: 0.487
Done Testing dataset Kinships
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.328 +- 0.378
mrr vals (pred, true): 0.089, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.05098 	 0.00010 	 m..s
    4 	     1 	 0.04094 	 0.00027 	 m..s
    7 	     2 	 0.04246 	 0.00037 	 m..s
    8 	     3 	 0.04357 	 0.00043 	 m..s
   24 	     4 	 0.08438 	 0.00051 	 m..s
   41 	     5 	 0.08997 	 0.00051 	 m..s
    1 	     6 	 0.03234 	 0.00051 	 m..s
   37 	     7 	 0.08910 	 0.00051 	 m..s
   76 	     8 	 0.10032 	 0.00053 	 m..s
    9 	     9 	 0.04440 	 0.00055 	 m..s
   59 	    10 	 0.09720 	 0.00055 	 m..s
   59 	    11 	 0.09720 	 0.00055 	 m..s
   28 	    12 	 0.08615 	 0.00056 	 m..s
   51 	    13 	 0.09408 	 0.00056 	 m..s
   59 	    14 	 0.09720 	 0.00058 	 m..s
    5 	    15 	 0.04203 	 0.00058 	 m..s
   55 	    16 	 0.09560 	 0.00059 	 m..s
    0 	    17 	 0.03160 	 0.00059 	 m..s
   31 	    18 	 0.08758 	 0.00061 	 m..s
   45 	    19 	 0.09122 	 0.00061 	 m..s
   59 	    20 	 0.09720 	 0.00062 	 m..s
   44 	    21 	 0.09077 	 0.00062 	 m..s
   10 	    22 	 0.04452 	 0.00062 	 m..s
   59 	    23 	 0.09720 	 0.00063 	 m..s
   57 	    24 	 0.09584 	 0.00063 	 m..s
   59 	    25 	 0.09720 	 0.00064 	 m..s
   59 	    26 	 0.09720 	 0.00067 	 m..s
   22 	    27 	 0.08300 	 0.00068 	 m..s
   25 	    28 	 0.08448 	 0.00071 	 m..s
    6 	    29 	 0.04219 	 0.00071 	 m..s
   34 	    30 	 0.08844 	 0.00071 	 m..s
   29 	    31 	 0.08711 	 0.00072 	 m..s
   12 	    32 	 0.04694 	 0.00073 	 m..s
   43 	    33 	 0.09028 	 0.00073 	 m..s
   39 	    34 	 0.08992 	 0.00075 	 m..s
   53 	    35 	 0.09487 	 0.00077 	 m..s
   74 	    36 	 0.09854 	 0.00077 	 m..s
   47 	    37 	 0.09275 	 0.00078 	 m..s
   59 	    38 	 0.09720 	 0.00081 	 m..s
   16 	    39 	 0.07695 	 0.00081 	 m..s
   33 	    40 	 0.08806 	 0.00084 	 m..s
   58 	    41 	 0.09624 	 0.00087 	 m..s
   13 	    42 	 0.04734 	 0.00090 	 m..s
    3 	    43 	 0.03984 	 0.00093 	 m..s
   73 	    44 	 0.09818 	 0.00115 	 m..s
   52 	    45 	 0.09450 	 0.00118 	 m..s
   32 	    46 	 0.08805 	 0.00359 	 m..s
    2 	    47 	 0.03457 	 0.00510 	 ~...
   11 	    48 	 0.04485 	 0.00628 	 m..s
   14 	    49 	 0.04954 	 0.00635 	 m..s
   72 	    50 	 0.09776 	 0.00718 	 m..s
   50 	    51 	 0.09343 	 0.04381 	 m..s
   23 	    52 	 0.08435 	 0.04831 	 m..s
   49 	    53 	 0.09325 	 0.04960 	 m..s
   59 	    54 	 0.09720 	 0.05052 	 m..s
   26 	    55 	 0.08500 	 0.05079 	 m..s
   59 	    56 	 0.09720 	 0.05184 	 m..s
   46 	    57 	 0.09167 	 0.05445 	 m..s
   18 	    58 	 0.08225 	 0.05537 	 ~...
   78 	    59 	 0.10400 	 0.05643 	 m..s
   17 	    60 	 0.07976 	 0.05685 	 ~...
   30 	    61 	 0.08744 	 0.05721 	 m..s
   59 	    62 	 0.09720 	 0.05760 	 m..s
   20 	    63 	 0.08231 	 0.05763 	 ~...
   40 	    64 	 0.08994 	 0.05775 	 m..s
   36 	    65 	 0.08907 	 0.06011 	 ~...
   59 	    66 	 0.09720 	 0.06050 	 m..s
   35 	    67 	 0.08897 	 0.06203 	 ~...
   42 	    68 	 0.08999 	 0.06246 	 ~...
   59 	    69 	 0.09720 	 0.06293 	 m..s
   56 	    70 	 0.09561 	 0.06625 	 ~...
   48 	    71 	 0.09297 	 0.06752 	 ~...
   54 	    72 	 0.09536 	 0.06828 	 ~...
   27 	    73 	 0.08577 	 0.06958 	 ~...
   21 	    74 	 0.08272 	 0.07354 	 ~...
   38 	    75 	 0.08956 	 0.07450 	 ~...
   19 	    76 	 0.08228 	 0.07837 	 ~...
   75 	    77 	 0.09881 	 0.08738 	 ~...
   77 	    78 	 0.10052 	 0.08886 	 ~...
   88 	    79 	 0.17427 	 0.12182 	 m..s
   87 	    80 	 0.17293 	 0.12784 	 m..s
   79 	    81 	 0.14786 	 0.13113 	 ~...
   80 	    82 	 0.15329 	 0.13825 	 ~...
   94 	    83 	 0.21726 	 0.13902 	 m..s
   85 	    84 	 0.16622 	 0.14394 	 ~...
   95 	    85 	 0.22170 	 0.15071 	 m..s
   81 	    86 	 0.15460 	 0.15855 	 ~...
   82 	    87 	 0.15943 	 0.17490 	 ~...
   83 	    88 	 0.16293 	 0.17523 	 ~...
   86 	    89 	 0.17252 	 0.19277 	 ~...
  104 	    90 	 0.25296 	 0.19476 	 m..s
  108 	    91 	 0.26415 	 0.20101 	 m..s
   84 	    92 	 0.16416 	 0.21418 	 m..s
  107 	    93 	 0.26359 	 0.21801 	 m..s
   96 	    94 	 0.22307 	 0.22069 	 ~...
  110 	    95 	 0.26703 	 0.22332 	 m..s
  106 	    96 	 0.26192 	 0.23206 	 ~...
  105 	    97 	 0.26023 	 0.24123 	 ~...
   91 	    98 	 0.19351 	 0.24736 	 m..s
  109 	    99 	 0.26456 	 0.25281 	 ~...
  116 	   100 	 0.29610 	 0.25365 	 m..s
   89 	   101 	 0.18449 	 0.25400 	 m..s
  119 	   102 	 0.30121 	 0.25821 	 m..s
  115 	   103 	 0.29549 	 0.26056 	 m..s
  120 	   104 	 0.30573 	 0.26103 	 m..s
   90 	   105 	 0.18901 	 0.26952 	 m..s
   93 	   106 	 0.19554 	 0.27386 	 m..s
   98 	   107 	 0.22970 	 0.27889 	 m..s
  102 	   108 	 0.24758 	 0.27911 	 m..s
   92 	   109 	 0.19360 	 0.27939 	 m..s
  100 	   110 	 0.23677 	 0.28088 	 m..s
   97 	   111 	 0.22926 	 0.28348 	 m..s
  101 	   112 	 0.24644 	 0.29794 	 m..s
   99 	   113 	 0.23449 	 0.29860 	 m..s
  113 	   114 	 0.27864 	 0.30040 	 ~...
  112 	   115 	 0.27811 	 0.30219 	 ~...
  114 	   116 	 0.28319 	 0.30458 	 ~...
  103 	   117 	 0.24944 	 0.30501 	 m..s
  111 	   118 	 0.26952 	 0.30631 	 m..s
  117 	   119 	 0.29690 	 0.32201 	 ~...
  118 	   120 	 0.30011 	 0.32380 	 ~...
==========================================
r_mrr = 0.926953911781311
r2_mrr = 0.700580358505249
spearmanr_mrr@5 = 0.8705505728721619
spearmanr_mrr@10 = 0.8742659091949463
spearmanr_mrr@50 = 0.9934985637664795
spearmanr_mrr@100 = 0.9813769459724426
spearmanr_mrr@All = 0.976830780506134
==========================================
test time: 0.567
Done Testing dataset OpenEA
total time taken: 949.0423550605774
training time taken: 906.9166901111603
TWIG out ;))
==========================================================
----------------------------------------------------------
Running a TWIG experiment with tag: DistMult-omit-Kinships
----------------------------------------------------------
==========================================================
Using random seed: 8094523267597361
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [723, 759, 371, 1146, 1027, 174, 178, 873, 971, 680, 153, 993, 938, 808, 1053, 732, 1043, 289, 767, 841, 1105, 1158, 145, 203, 809, 390, 483, 70, 198, 358, 1030, 1115, 856, 683, 209, 528, 1037, 236, 133, 650, 1016, 101, 200, 212, 963, 65, 760, 1020, 974, 1148, 900, 297, 84, 1143, 929, 1004, 452, 414, 1040, 997, 166, 580, 921, 152, 630, 954, 394, 223, 871, 531, 12, 1199, 507, 1155, 1127, 85, 517, 973, 221, 811, 1066, 1207, 664, 559, 228, 836, 787, 1070, 342, 952, 862, 1076, 506, 154, 1058, 689, 275, 513, 188, 755, 515, 608, 736, 190, 361, 882, 413, 626, 260, 812, 350, 417, 872, 891, 161, 55, 896, 1193, 1051, 386, 7]
valid_ids (0): []
train_ids (1094): [320, 52, 524, 881, 493, 146, 412, 609, 202, 1090, 1087, 1175, 909, 1156, 435, 860, 578, 119, 642, 752, 1080, 670, 262, 1111, 124, 6, 1008, 665, 347, 238, 628, 247, 76, 907, 312, 544, 1009, 4, 518, 20, 1138, 254, 868, 1202, 60, 568, 1097, 1149, 553, 772, 1184, 432, 770, 835, 3, 286, 268, 569, 499, 826, 1197, 446, 197, 660, 803, 322, 936, 391, 604, 1082, 156, 1065, 976, 485, 996, 825, 282, 1063, 389, 637, 1032, 646, 148, 338, 982, 96, 845, 484, 1166, 1060, 820, 806, 418, 656, 988, 440, 1026, 88, 157, 185, 264, 46, 999, 766, 1055, 966, 40, 298, 562, 627, 495, 1101, 287, 1086, 913, 1049, 596, 269, 1061, 186, 775, 514, 1109, 10, 106, 1050, 567, 296, 740, 960, 116, 278, 311, 374, 59, 304, 492, 934, 631, 1022, 768, 1213, 832, 1205, 722, 986, 919, 73, 266, 582, 170, 14, 666, 425, 29, 632, 191, 1035, 103, 924, 370, 1214, 455, 305, 448, 941, 243, 211, 349, 671, 169, 823, 144, 756, 669, 47, 694, 32, 283, 318, 1084, 373, 588, 1151, 376, 1162, 1021, 942, 502, 828, 490, 237, 1054, 1025, 258, 1024, 619, 922, 551, 1161, 587, 792, 834, 348, 1188, 509, 494, 339, 1186, 468, 437, 654, 1000, 613, 345, 424, 421, 323, 1164, 1012, 950, 1159, 220, 434, 81, 487, 54, 530, 741, 830, 300, 281, 1002, 512, 677, 648, 126, 1196, 893, 989, 204, 1137, 277, 48, 805, 1160, 1, 699, 1133, 78, 901, 138, 263, 205, 57, 280, 855, 556, 510, 42, 193, 884, 464, 351, 99, 638, 180, 931, 122, 590, 41, 857, 923, 393, 9, 179, 173, 218, 1209, 876, 888, 975, 521, 139, 255, 112, 1134, 90, 737, 847, 327, 114, 387, 704, 944, 1135, 718, 621, 398, 915, 465, 693, 710, 130, 538, 307, 887, 381, 994, 1007, 215, 163, 397, 385, 360, 991, 451, 815, 879, 878, 726, 583, 372, 906, 1094, 352, 1041, 1079, 50, 728, 1059, 869, 35, 984, 457, 678, 1093, 23, 388, 267, 121, 953, 1210, 168, 150, 754, 1140, 908, 108, 585, 261, 560, 319, 573, 458, 475, 172, 542, 546, 217, 431, 1033, 539, 503, 75, 15, 21, 100, 894, 864, 466, 532, 733, 777, 1018, 207, 363, 234, 708, 1081, 575, 235, 480, 80, 926, 1047, 1013, 795, 570, 813, 526, 141, 213, 477, 27, 763, 858, 1171, 1203, 128, 313, 408, 83, 406, 519, 636, 624, 782, 87, 1118, 252, 633, 707, 1028, 1154, 245, 978, 1192, 1211, 644, 1167, 82, 123, 998, 1046, 482, 1113, 328, 276, 356, 409, 34, 427, 735, 183, 635, 714, 1104, 657, 353, 194, 396, 511, 629, 291, 522, 776, 1157, 462, 930, 355, 395, 949, 645, 555, 639, 581, 600, 655, 216, 1085, 131, 980, 584, 706, 1017, 1069, 720, 709, 159, 1075, 308, 93, 175, 764, 271, 566, 486, 1144, 481, 702, 38, 663, 410, 416, 1180, 1092, 115, 969, 933, 1042, 147, 804, 695, 1129, 64, 1083, 160, 403, 927, 1098, 240, 17, 232, 799, 284, 1177, 143, 898, 717, 705, 1145, 8, 840, 69, 603, 801, 690, 295, 951, 750, 184, 279, 18, 306, 249, 802, 224, 317, 309, 1200, 199, 1126, 827, 1003, 151, 1100, 549, 1121, 443, 1136, 561, 865, 326, 875, 725, 788, 1071, 1208, 762, 687, 920, 753, 1039, 474, 885, 716, 5, 842, 1088, 536, 1117, 229, 1139, 700, 1119, 478, 109, 955, 469, 816, 375, 1023, 226, 1110, 798, 614, 577, 344, 605, 599, 634, 739, 939, 33, 738, 299, 182, 1106, 92, 1116, 1191, 167, 1064, 367, 149, 165, 918, 430, 1112, 447, 889, 758, 593, 316, 649, 684, 107, 36, 290, 357, 615, 369, 201, 331, 1078, 43, 819, 1114, 595, 617, 1182, 162, 479, 905, 426, 793, 897, 242, 449, 734, 53, 384, 686, 886, 1172, 844, 1147, 928, 523, 335, 571, 972, 658, 365, 916, 712, 1108, 661, 899, 68, 659, 453, 1183, 257, 294, 757, 0, 1194, 640, 917, 558, 979, 303, 866, 574, 541, 807, 794, 935, 643, 534, 851, 697, 880, 1077, 456, 105, 473, 701, 2, 292, 439, 1212, 673, 1176, 501, 433, 711, 817, 450, 797, 843, 829, 293, 132, 496, 111, 1048, 682, 746, 341, 1125, 343, 742, 977, 488, 607, 848, 1123, 239, 703, 1067, 1174, 579, 231, 22, 550, 890, 778, 1005, 789, 273, 402, 591, 49, 721, 227, 1014, 786, 463, 987, 158, 102, 576, 1036, 1185, 554, 743, 472, 1178, 419, 1179, 594, 622, 773, 1173, 961, 196, 97, 214, 785, 1099, 1130, 334, 498, 824, 903, 946, 117, 118, 970, 1206, 134, 852, 668, 459, 589, 285, 572, 586, 89, 423, 983, 359, 25, 727, 253, 895, 1107, 11, 912, 821, 769, 810, 713, 1204, 610, 192, 1045, 957, 206, 460, 564, 37, 1120, 525, 730, 612, 985, 72, 761, 543, 259, 164, 256, 1169, 652, 1124, 378, 1168, 540, 383, 724, 947, 814, 874, 625, 653, 176, 274, 904, 910, 696, 563, 155, 244, 681, 302, 210, 833, 620, 641, 377, 13, 422, 354, 937, 1031, 846, 850, 187, 956, 685, 748, 747, 1073, 1201, 336, 442, 959, 535, 324, 505, 870, 729, 914, 611, 19, 537, 618, 790, 1015, 476, 854, 995, 698, 16, 606, 113, 831, 692, 233, 86, 1102, 401, 520, 222, 31, 839, 1095, 818, 415, 958, 329, 30, 679, 120, 1122, 429, 407, 547, 992, 651, 181, 557, 1019, 39, 404, 527, 444, 529, 45, 346, 967, 863, 288, 859, 1089, 428, 489, 314, 454, 964, 95, 127, 461, 441, 195, 962, 62, 968, 565, 745, 675, 362, 791, 765, 94, 1165, 270, 676, 497, 110, 943, 1044, 63, 779, 1029, 552, 491, 1034, 71, 265, 177, 189, 783, 861, 382, 301, 467, 932, 948, 219, 332, 781, 1103, 945, 1190, 61, 715, 330, 548, 399, 225, 74, 597, 321, 368, 623, 1006, 333, 379, 1189, 1128, 58, 784, 28, 940, 248, 135, 137, 672, 902, 688, 392, 662, 749, 471, 1057, 310, 744, 98, 1131, 837, 1011, 500, 250, 1187, 1001, 1163, 800, 51, 470, 796, 780, 853, 598, 911, 251, 892, 731, 719, 925, 1195, 230, 1038, 774, 504, 1072, 26, 838, 508, 1141, 140, 438, 142, 241, 1181, 366, 877, 691, 104, 965, 1152, 66, 883, 67, 1096, 1074, 420, 364, 516, 77, 849, 1132, 436, 1142, 340, 411, 592, 602, 990, 24, 79, 601, 1198, 822, 272, 533, 981, 91, 751, 1010, 1091, 1153, 667, 337, 674, 400, 325, 1170, 1062, 1056, 171, 315, 771, 445, 616, 44, 1068, 129, 136, 208, 1150, 545, 380, 647, 1052, 125, 867, 405, 56, 246]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9851972570344662
the save name prefix for this run is:  chkpt-ID_9851972570344662_tag_DistMult-omit-Kinships
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1'], 'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 22
rank avg (pred): 0.510 +- 0.002
mrr vals (pred, true): 0.014, 0.551
batch losses (mrrl, rdl): 0.0, 0.0048554139

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 982
rank avg (pred): 0.078 +- 0.030
mrr vals (pred, true): 0.103, 0.545
batch losses (mrrl, rdl): 0.0, 3.75671e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1177
rank avg (pred): 0.361 +- 0.244
mrr vals (pred, true): 0.100, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001113718

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 525
rank avg (pred): 0.277 +- 0.229
mrr vals (pred, true): 0.242, 0.057
batch losses (mrrl, rdl): 0.0, 0.0004469852

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 651
rank avg (pred): 0.386 +- 0.281
mrr vals (pred, true): 0.203, 0.051
batch losses (mrrl, rdl): 0.0, 1.62443e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 729
rank avg (pred): 0.084 +- 0.081
mrr vals (pred, true): 0.417, 0.368
batch losses (mrrl, rdl): 0.0, 2.8903e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 673
rank avg (pred): 0.493 +- 0.276
mrr vals (pred, true): 0.107, 0.047
batch losses (mrrl, rdl): 0.0, 8.63249e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 192
rank avg (pred): 0.384 +- 0.240
mrr vals (pred, true): 0.165, 0.051
batch losses (mrrl, rdl): 0.0, 2.0317e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 385
rank avg (pred): 0.334 +- 0.260
mrr vals (pred, true): 0.283, 0.095
batch losses (mrrl, rdl): 0.0, 1.58009e-05

Epoch over!
epoch time: 60.995

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 681
rank avg (pred): 0.416 +- 0.268
mrr vals (pred, true): 0.196, 0.046
batch losses (mrrl, rdl): 0.0, 6.0027e-06

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 568
rank avg (pred): 0.377 +- 0.287
mrr vals (pred, true): 0.271, 0.035
batch losses (mrrl, rdl): 0.0, 0.0001174462

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 490
rank avg (pred): 0.214 +- 0.203
mrr vals (pred, true): 0.418, 0.215
batch losses (mrrl, rdl): 0.0, 2.33027e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 454
rank avg (pred): 0.382 +- 0.260
mrr vals (pred, true): 0.213, 0.047
batch losses (mrrl, rdl): 0.0, 2.36442e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 708
rank avg (pred): 0.397 +- 0.285
mrr vals (pred, true): 0.266, 0.045
batch losses (mrrl, rdl): 0.0, 1.27579e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 850
rank avg (pred): 0.429 +- 0.287
mrr vals (pred, true): 0.219, 0.037
batch losses (mrrl, rdl): 0.0, 2.9845e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 503
rank avg (pred): 0.225 +- 0.187
mrr vals (pred, true): 0.352, 0.235
batch losses (mrrl, rdl): 0.0, 5.1641e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 103
rank avg (pred): 0.351 +- 0.271
mrr vals (pred, true): 0.297, 0.146
batch losses (mrrl, rdl): 0.0, 0.0003187915

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 86
rank avg (pred): 0.330 +- 0.274
mrr vals (pred, true): 0.337, 0.085
batch losses (mrrl, rdl): 0.0, 6.2292e-06

Epoch over!
epoch time: 58.673

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 931
rank avg (pred): 0.618 +- 0.280
mrr vals (pred, true): 0.096, 0.016
batch losses (mrrl, rdl): 0.0, 0.0002151343

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 456
rank avg (pred): 0.383 +- 0.263
mrr vals (pred, true): 0.236, 0.054
batch losses (mrrl, rdl): 0.0, 1.09255e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 202
rank avg (pred): 0.342 +- 0.277
mrr vals (pred, true): 0.329, 0.050
batch losses (mrrl, rdl): 0.0, 6.34126e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1097
rank avg (pred): 0.369 +- 0.283
mrr vals (pred, true): 0.301, 0.101
batch losses (mrrl, rdl): 0.0, 0.0001876039

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 32
rank avg (pred): 0.045 +- 0.082
mrr vals (pred, true): 0.485, 0.523
batch losses (mrrl, rdl): 0.0, 2.4237e-06

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 831
rank avg (pred): 0.062 +- 0.083
mrr vals (pred, true): 0.429, 0.468
batch losses (mrrl, rdl): 0.0, 1.6532e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 450
rank avg (pred): 0.368 +- 0.261
mrr vals (pred, true): 0.226, 0.053
batch losses (mrrl, rdl): 0.0, 3.27433e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 761
rank avg (pred): 0.457 +- 0.303
mrr vals (pred, true): 0.198, 0.036
batch losses (mrrl, rdl): 0.0, 4.07318e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 837
rank avg (pred): 0.478 +- 0.258
mrr vals (pred, true): 0.128, 0.052
batch losses (mrrl, rdl): 0.0, 3.63141e-05

Epoch over!
epoch time: 59.531

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 396
rank avg (pred): 0.343 +- 0.271
mrr vals (pred, true): 0.286, 0.147
batch losses (mrrl, rdl): 0.0, 0.0002773392

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1153
rank avg (pred): 0.311 +- 0.265
mrr vals (pred, true): 0.296, 0.145
batch losses (mrrl, rdl): 0.0, 0.0001299183

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 292
rank avg (pred): 0.069 +- 0.093
mrr vals (pred, true): 0.439, 0.544
batch losses (mrrl, rdl): 0.0, 2.90585e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 445
rank avg (pred): 0.333 +- 0.279
mrr vals (pred, true): 0.330, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001325093

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1031
rank avg (pred): 0.339 +- 0.274
mrr vals (pred, true): 0.296, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001950249

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 182
rank avg (pred): 0.298 +- 0.266
mrr vals (pred, true): 0.338, 0.051
batch losses (mrrl, rdl): 0.0, 0.0002206301

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 206
rank avg (pred): 0.379 +- 0.266
mrr vals (pred, true): 0.216, 0.049
batch losses (mrrl, rdl): 0.0, 1.97043e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 682
rank avg (pred): 0.369 +- 0.294
mrr vals (pred, true): 0.293, 0.055
batch losses (mrrl, rdl): 0.0, 2.95522e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 551
rank avg (pred): 0.321 +- 0.273
mrr vals (pred, true): 0.334, 0.105
batch losses (mrrl, rdl): 0.0, 8.9164e-06

Epoch over!
epoch time: 56.907

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 457
rank avg (pred): 0.358 +- 0.281
mrr vals (pred, true): 0.278, 0.047
batch losses (mrrl, rdl): 0.0, 7.69762e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 30
rank avg (pred): 0.051 +- 0.086
mrr vals (pred, true): 0.436, 0.535
batch losses (mrrl, rdl): 0.0, 3.4803e-06

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1145
rank avg (pred): 0.239 +- 0.230
mrr vals (pred, true): 0.395, 0.134
batch losses (mrrl, rdl): 0.0, 1.06906e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 48
rank avg (pred): 0.028 +- 0.060
mrr vals (pred, true): 0.554, 0.509
batch losses (mrrl, rdl): 0.0, 5.685e-06

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 731
rank avg (pred): 0.185 +- 0.215
mrr vals (pred, true): 0.333, 0.431
batch losses (mrrl, rdl): 0.0, 0.000381463

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 40
rank avg (pred): 0.036 +- 0.079
mrr vals (pred, true): 0.570, 0.532
batch losses (mrrl, rdl): 0.0, 1.53e-07

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 47
rank avg (pred): 0.032 +- 0.083
mrr vals (pred, true): 0.591, 0.532
batch losses (mrrl, rdl): 0.0, 5.286e-07

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 91
rank avg (pred): 0.304 +- 0.281
mrr vals (pred, true): 0.358, 0.103
batch losses (mrrl, rdl): 0.0, 2.25127e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 743
rank avg (pred): 0.041 +- 0.094
mrr vals (pred, true): 0.608, 0.412
batch losses (mrrl, rdl): 0.0, 8.232e-06

Epoch over!
epoch time: 58.151

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 115
rank avg (pred): 0.399 +- 0.257
mrr vals (pred, true): 0.124, 0.094
batch losses (mrrl, rdl): 0.0552327484, 0.0002047412

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 984
rank avg (pred): 0.058 +- 0.131
mrr vals (pred, true): 0.566, 0.556
batch losses (mrrl, rdl): 0.0010946563, 2.50897e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 388
rank avg (pred): 0.512 +- 0.210
mrr vals (pred, true): 0.086, 0.110
batch losses (mrrl, rdl): 0.0055602901, 0.0009691904

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 290
rank avg (pred): 0.055 +- 0.109
mrr vals (pred, true): 0.595, 0.567
batch losses (mrrl, rdl): 0.0077251326, 1.99606e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1194
rank avg (pred): 0.556 +- 0.151
mrr vals (pred, true): 0.055, 0.050
batch losses (mrrl, rdl): 0.0002281851, 0.0003167724

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 265
rank avg (pred): 0.083 +- 0.127
mrr vals (pred, true): 0.545, 0.561
batch losses (mrrl, rdl): 0.0027145972, 7.82254e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1131
rank avg (pred): 0.512 +- 0.174
mrr vals (pred, true): 0.075, 0.049
batch losses (mrrl, rdl): 0.0064767906, 0.0001301554

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1173
rank avg (pred): 0.536 +- 0.161
mrr vals (pred, true): 0.067, 0.040
batch losses (mrrl, rdl): 0.0029627807, 9.06331e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 585
rank avg (pred): 0.537 +- 0.157
mrr vals (pred, true): 0.067, 0.043
batch losses (mrrl, rdl): 0.002889524, 0.0001191961

Epoch over!
epoch time: 55.78

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 681
rank avg (pred): 0.586 +- 0.155
mrr vals (pred, true): 0.059, 0.046
batch losses (mrrl, rdl): 0.0007811771, 0.0005029102

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1092
rank avg (pred): 0.494 +- 0.163
mrr vals (pred, true): 0.075, 0.111
batch losses (mrrl, rdl): 0.0134024899, 0.0008860273

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 128
rank avg (pred): 0.492 +- 0.158
mrr vals (pred, true): 0.072, 0.100
batch losses (mrrl, rdl): 0.0080353934, 0.0007008307

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 611
rank avg (pred): 0.579 +- 0.183
mrr vals (pred, true): 0.068, 0.040
batch losses (mrrl, rdl): 0.0031425441, 0.0001976031

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 884
rank avg (pred): 0.510 +- 0.124
mrr vals (pred, true): 0.061, 0.045
batch losses (mrrl, rdl): 0.0011150043, 0.0001158327

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 325
rank avg (pred): 0.482 +- 0.144
mrr vals (pred, true): 0.064, 0.087
batch losses (mrrl, rdl): 0.0020590799, 0.0004321081

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 159
rank avg (pred): 0.453 +- 0.157
mrr vals (pred, true): 0.073, 0.083
batch losses (mrrl, rdl): 0.0054477393, 0.0003246173

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 368
rank avg (pred): 0.467 +- 0.147
mrr vals (pred, true): 0.066, 0.120
batch losses (mrrl, rdl): 0.0289510339, 0.0007199546

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 177
rank avg (pred): 0.452 +- 0.151
mrr vals (pred, true): 0.074, 0.043
batch losses (mrrl, rdl): 0.0058051748, 3.45642e-05

Epoch over!
epoch time: 57.815

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 724
rank avg (pred): 0.628 +- 0.238
mrr vals (pred, true): 0.066, 0.052
batch losses (mrrl, rdl): 0.0027213446, 0.0005240634

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1213
rank avg (pred): 0.648 +- 0.244
mrr vals (pred, true): 0.059, 0.056
batch losses (mrrl, rdl): 0.0008960663, 0.0007049556

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 238
rank avg (pred): 0.420 +- 0.157
mrr vals (pred, true): 0.082, 0.046
batch losses (mrrl, rdl): 0.0099961609, 3.63191e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 880
rank avg (pred): 0.465 +- 0.126
mrr vals (pred, true): 0.073, 0.052
batch losses (mrrl, rdl): 0.0054992726, 6.32972e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 589
rank avg (pred): 0.613 +- 0.246
mrr vals (pred, true): 0.063, 0.043
batch losses (mrrl, rdl): 0.0016701352, 0.0002530791

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 399
rank avg (pred): 0.376 +- 0.162
mrr vals (pred, true): 0.079, 0.129
batch losses (mrrl, rdl): 0.0257008355, 0.0003508839

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 391
rank avg (pred): 0.413 +- 0.147
mrr vals (pred, true): 0.081, 0.091
batch losses (mrrl, rdl): 0.0097193951, 0.0001857455

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 176
rank avg (pred): 0.370 +- 0.157
mrr vals (pred, true): 0.086, 0.051
batch losses (mrrl, rdl): 0.0132122245, 9.61234e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 741
rank avg (pred): 0.189 +- 0.214
mrr vals (pred, true): 0.294, 0.375
batch losses (mrrl, rdl): 0.0655588359, 0.0003675387

Epoch over!
epoch time: 56.407

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 482
rank avg (pred): 0.399 +- 0.137
mrr vals (pred, true): 0.074, 0.050
batch losses (mrrl, rdl): 0.0057573612, 5.69222e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 450
rank avg (pred): 0.406 +- 0.136
mrr vals (pred, true): 0.078, 0.053
batch losses (mrrl, rdl): 0.0078974031, 6.43844e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 499
rank avg (pred): 0.212 +- 0.200
mrr vals (pred, true): 0.202, 0.212
batch losses (mrrl, rdl): 0.0011440716, 1.39378e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 800
rank avg (pred): 0.429 +- 0.086
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 2.29727e-05, 6.0236e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1174
rank avg (pred): 0.611 +- 0.274
mrr vals (pred, true): 0.061, 0.036
batch losses (mrrl, rdl): 0.0012500304, 0.0002076298

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 355
rank avg (pred): 0.319 +- 0.167
mrr vals (pred, true): 0.135, 0.086
batch losses (mrrl, rdl): 0.0721069127, 2.30891e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 103
rank avg (pred): 0.393 +- 0.123
mrr vals (pred, true): 0.073, 0.146
batch losses (mrrl, rdl): 0.0531347878, 0.0004642941

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 396
rank avg (pred): 0.392 +- 0.121
mrr vals (pred, true): 0.074, 0.147
batch losses (mrrl, rdl): 0.0536175631, 0.0004541241

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 96
rank avg (pred): 0.359 +- 0.138
mrr vals (pred, true): 0.083, 0.121
batch losses (mrrl, rdl): 0.0142854536, 0.0001997486

Epoch over!
epoch time: 56.022

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 17
rank avg (pred): 0.155 +- 0.205
mrr vals (pred, true): 0.520, 0.526
batch losses (mrrl, rdl): 0.0003577911, 0.0004165286

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 830
rank avg (pred): 0.133 +- 0.190
mrr vals (pred, true): 0.580, 0.513
batch losses (mrrl, rdl): 0.046048671, 0.0002715219

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 849
rank avg (pred): 0.408 +- 0.085
mrr vals (pred, true): 0.051, 0.029
batch losses (mrrl, rdl): 1.97318e-05, 0.0003375151

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 206
rank avg (pred): 0.386 +- 0.113
mrr vals (pred, true): 0.071, 0.049
batch losses (mrrl, rdl): 0.0043691248, 0.0001036366

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1122
rank avg (pred): 0.349 +- 0.136
mrr vals (pred, true): 0.083, 0.042
batch losses (mrrl, rdl): 0.0110883899, 0.0002261969

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 135
rank avg (pred): 0.382 +- 0.109
mrr vals (pred, true): 0.075, 0.087
batch losses (mrrl, rdl): 0.0060674106, 9.19963e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 546
rank avg (pred): 0.542 +- 0.277
mrr vals (pred, true): 0.074, 0.064
batch losses (mrrl, rdl): 0.0060002995, 0.0002396104

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 103
rank avg (pred): 0.335 +- 0.140
mrr vals (pred, true): 0.086, 0.146
batch losses (mrrl, rdl): 0.0354522765, 0.0001911792

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 46
rank avg (pred): 0.140 +- 0.188
mrr vals (pred, true): 0.541, 0.527
batch losses (mrrl, rdl): 0.0018107321, 0.0002826675

Epoch over!
epoch time: 59.635

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 60
rank avg (pred): 0.151 +- 0.190
mrr vals (pred, true): 0.497, 0.511
batch losses (mrrl, rdl): 0.0020332278, 0.00032625

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 118
rank avg (pred): 0.348 +- 0.122
mrr vals (pred, true): 0.084, 0.145
batch losses (mrrl, rdl): 0.0374074765, 0.000297642

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 617
rank avg (pred): 0.570 +- 0.283
mrr vals (pred, true): 0.060, 0.043
batch losses (mrrl, rdl): 0.0010109663, 8.64762e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 471
rank avg (pred): 0.344 +- 0.127
mrr vals (pred, true): 0.082, 0.052
batch losses (mrrl, rdl): 0.0099438075, 0.0002166137

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 15
rank avg (pred): 0.134 +- 0.182
mrr vals (pred, true): 0.556, 0.555
batch losses (mrrl, rdl): 1.68251e-05, 0.0002805962

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 224
rank avg (pred): 0.366 +- 0.100
mrr vals (pred, true): 0.070, 0.045
batch losses (mrrl, rdl): 0.0041557774, 0.0001742127

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 62
rank avg (pred): 0.142 +- 0.183
mrr vals (pred, true): 0.536, 0.531
batch losses (mrrl, rdl): 0.0003045183, 0.0003310175

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 988
rank avg (pred): 0.143 +- 0.185
mrr vals (pred, true): 0.546, 0.561
batch losses (mrrl, rdl): 0.0021920446, 0.0003403012

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1173
rank avg (pred): 0.557 +- 0.285
mrr vals (pred, true): 0.063, 0.040
batch losses (mrrl, rdl): 0.0015696457, 3.58996e-05

Epoch over!
epoch time: 59.442

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 601
rank avg (pred): 0.630 +- 0.294
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001461223, 0.0001999674

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 586
rank avg (pred): 0.629 +- 0.305
mrr vals (pred, true): 0.059, 0.034
batch losses (mrrl, rdl): 0.0007459037, 0.000223436

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 400
rank avg (pred): 0.354 +- 0.106
mrr vals (pred, true): 0.078, 0.122
batch losses (mrrl, rdl): 0.0194248445, 0.000210912

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 568
rank avg (pred): 0.646 +- 0.315
mrr vals (pred, true): 0.063, 0.035
batch losses (mrrl, rdl): 0.0016100311, 0.0003304787

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1062
rank avg (pred): 0.152 +- 0.182
mrr vals (pred, true): 0.481, 0.536
batch losses (mrrl, rdl): 0.0298972204, 0.0003782325

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 426
rank avg (pred): 0.347 +- 0.104
mrr vals (pred, true): 0.084, 0.050
batch losses (mrrl, rdl): 0.0116384318, 0.0003543499

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1198
rank avg (pred): 0.540 +- 0.277
mrr vals (pred, true): 0.056, 0.047
batch losses (mrrl, rdl): 0.0003276616, 9.08487e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 888
rank avg (pred): 0.364 +- 0.076
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002779011, 0.0001988967

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1137
rank avg (pred): 0.190 +- 0.177
mrr vals (pred, true): 0.263, 0.229
batch losses (mrrl, rdl): 0.0110113677, 1.25974e-05

Epoch over!
epoch time: 57.776

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 585
rank avg (pred): 0.626 +- 0.316
mrr vals (pred, true): 0.060, 0.043
batch losses (mrrl, rdl): 0.0009666556, 0.0002369307

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1171
rank avg (pred): 0.572 +- 0.307
mrr vals (pred, true): 0.059, 0.050
batch losses (mrrl, rdl): 0.000786431, 0.0001231618

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 30
rank avg (pred): 0.224 +- 0.179
mrr vals (pred, true): 0.285, 0.535
batch losses (mrrl, rdl): 0.624555409, 0.0008175648

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 4
rank avg (pred): 0.129 +- 0.168
mrr vals (pred, true): 0.534, 0.545
batch losses (mrrl, rdl): 0.0013748002, 0.0002572354

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 611
rank avg (pred): 0.579 +- 0.310
mrr vals (pred, true): 0.060, 0.040
batch losses (mrrl, rdl): 0.0010651356, 6.72482e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 307
rank avg (pred): 0.124 +- 0.166
mrr vals (pred, true): 0.560, 0.547
batch losses (mrrl, rdl): 0.0017216087, 0.0002317035

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1124
rank avg (pred): 0.332 +- 0.104
mrr vals (pred, true): 0.073, 0.042
batch losses (mrrl, rdl): 0.0052549345, 0.0003076896

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1055
rank avg (pred): 0.135 +- 0.174
mrr vals (pred, true): 0.536, 0.565
batch losses (mrrl, rdl): 0.0086852107, 0.0002974998

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1106
rank avg (pred): 0.353 +- 0.089
mrr vals (pred, true): 0.069, 0.131
batch losses (mrrl, rdl): 0.0389215685, 0.0001854989

Epoch over!
epoch time: 56.466

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 982
rank avg (pred): 0.139 +- 0.173
mrr vals (pred, true): 0.533, 0.545
batch losses (mrrl, rdl): 0.0014825878, 0.0003084575

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 775
rank avg (pred): 0.361 +- 0.069
mrr vals (pred, true): 0.051, 0.055
batch losses (mrrl, rdl): 1.05282e-05, 0.0001955199

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1161
rank avg (pred): 0.585 +- 0.327
mrr vals (pred, true): 0.071, 0.039
batch losses (mrrl, rdl): 0.0043829805, 8.84978e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1180
rank avg (pred): 0.567 +- 0.306
mrr vals (pred, true): 0.054, 0.045
batch losses (mrrl, rdl): 0.0001276497, 0.0001170586

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 25
rank avg (pred): 0.133 +- 0.172
mrr vals (pred, true): 0.548, 0.531
batch losses (mrrl, rdl): 0.0027619004, 0.0002661379

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1152
rank avg (pred): 0.407 +- 0.288
mrr vals (pred, true): 0.151, 0.135
batch losses (mrrl, rdl): 0.0026053996, 0.00037491

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 987
rank avg (pred): 0.133 +- 0.176
mrr vals (pred, true): 0.566, 0.552
batch losses (mrrl, rdl): 0.0020077683, 0.0002925318

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 706
rank avg (pred): 0.537 +- 0.300
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 9.94016e-05, 8.29371e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 41
rank avg (pred): 0.132 +- 0.169
mrr vals (pred, true): 0.540, 0.530
batch losses (mrrl, rdl): 0.0009583716, 0.000247493

Epoch over!
epoch time: 59.573

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 177
rank avg (pred): 0.334 +- 0.098
mrr vals (pred, true): 0.083, 0.043
batch losses (mrrl, rdl): 0.0110292723, 0.0003064816

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 173
rank avg (pred): 0.350 +- 0.085
mrr vals (pred, true): 0.067, 0.052
batch losses (mrrl, rdl): 0.0030335533, 0.000236621

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1084
rank avg (pred): 0.347 +- 0.081
mrr vals (pred, true): 0.063, 0.157
batch losses (mrrl, rdl): 0.0873289332, 0.0003344447

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 253
rank avg (pred): 0.133 +- 0.168
mrr vals (pred, true): 0.547, 0.561
batch losses (mrrl, rdl): 0.0018338243, 0.0002911456

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 570
rank avg (pred): 0.604 +- 0.329
mrr vals (pred, true): 0.058, 0.046
batch losses (mrrl, rdl): 0.0006685476, 0.000186727

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 219
rank avg (pred): 0.339 +- 0.100
mrr vals (pred, true): 0.082, 0.048
batch losses (mrrl, rdl): 0.0105396798, 0.000242805

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 385
rank avg (pred): 0.336 +- 0.095
mrr vals (pred, true): 0.080, 0.095
batch losses (mrrl, rdl): 0.0090911277, 7.12074e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 56
rank avg (pred): 0.138 +- 0.171
mrr vals (pred, true): 0.529, 0.523
batch losses (mrrl, rdl): 0.0004256603, 0.0002970581

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 943
rank avg (pred): 0.771 +- 0.323
mrr vals (pred, true): 0.042, 0.017
batch losses (mrrl, rdl): 0.0006052256, 6.04128e-05

Epoch over!
epoch time: 56.275

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.526 +- 0.303
mrr vals (pred, true): 0.061, 0.044

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04733 	 0.01662 	 m..s
    1 	     1 	 0.04400 	 0.01697 	 ~...
   95 	     2 	 0.17026 	 0.01876 	 MISS
    2 	     3 	 0.04400 	 0.02085 	 ~...
   30 	     4 	 0.06684 	 0.02127 	 m..s
   15 	     5 	 0.05760 	 0.03193 	 ~...
   14 	     6 	 0.05749 	 0.03392 	 ~...
   27 	     7 	 0.06379 	 0.03661 	 ~...
   12 	     8 	 0.05521 	 0.03702 	 ~...
   32 	     9 	 0.07052 	 0.03858 	 m..s
   26 	    10 	 0.06359 	 0.03972 	 ~...
   18 	    11 	 0.06092 	 0.04000 	 ~...
    0 	    12 	 0.04329 	 0.04100 	 ~...
   11 	    13 	 0.05429 	 0.04184 	 ~...
    4 	    14 	 0.04425 	 0.04302 	 ~...
   42 	    15 	 0.07657 	 0.04357 	 m..s
   16 	    16 	 0.06059 	 0.04361 	 ~...
   58 	    17 	 0.08092 	 0.04377 	 m..s
   29 	    18 	 0.06651 	 0.04379 	 ~...
   25 	    19 	 0.06350 	 0.04401 	 ~...
   52 	    20 	 0.07932 	 0.04416 	 m..s
   34 	    21 	 0.07242 	 0.04441 	 ~...
   81 	    22 	 0.09322 	 0.04450 	 m..s
    5 	    23 	 0.04502 	 0.04452 	 ~...
   61 	    24 	 0.08206 	 0.04464 	 m..s
   28 	    25 	 0.06554 	 0.04470 	 ~...
   80 	    26 	 0.09246 	 0.04474 	 m..s
   38 	    27 	 0.07495 	 0.04495 	 m..s
   37 	    28 	 0.07491 	 0.04531 	 ~...
    8 	    29 	 0.05296 	 0.04554 	 ~...
   10 	    30 	 0.05310 	 0.04562 	 ~...
   13 	    31 	 0.05732 	 0.04575 	 ~...
   65 	    32 	 0.08306 	 0.04619 	 m..s
   60 	    33 	 0.08189 	 0.04631 	 m..s
    7 	    34 	 0.05291 	 0.04671 	 ~...
   43 	    35 	 0.07680 	 0.04700 	 ~...
   24 	    36 	 0.06339 	 0.04719 	 ~...
   48 	    37 	 0.07843 	 0.04730 	 m..s
   51 	    38 	 0.07916 	 0.04735 	 m..s
   23 	    39 	 0.06250 	 0.04750 	 ~...
   19 	    40 	 0.06133 	 0.04768 	 ~...
   20 	    41 	 0.06136 	 0.04775 	 ~...
   40 	    42 	 0.07653 	 0.04790 	 ~...
   68 	    43 	 0.08429 	 0.04812 	 m..s
   62 	    44 	 0.08238 	 0.04934 	 m..s
   66 	    45 	 0.08342 	 0.04941 	 m..s
   33 	    46 	 0.07057 	 0.04951 	 ~...
   54 	    47 	 0.08003 	 0.04955 	 m..s
   49 	    48 	 0.07848 	 0.05022 	 ~...
    9 	    49 	 0.05307 	 0.05069 	 ~...
   50 	    50 	 0.07911 	 0.05073 	 ~...
    3 	    51 	 0.04425 	 0.05096 	 ~...
   55 	    52 	 0.08024 	 0.05110 	 ~...
   82 	    53 	 0.09461 	 0.05117 	 m..s
   22 	    54 	 0.06182 	 0.05144 	 ~...
   35 	    55 	 0.07267 	 0.05165 	 ~...
   64 	    56 	 0.08298 	 0.05197 	 m..s
   86 	    57 	 0.10547 	 0.05244 	 m..s
   56 	    58 	 0.08060 	 0.05344 	 ~...
   31 	    59 	 0.07028 	 0.05365 	 ~...
   39 	    60 	 0.07650 	 0.05398 	 ~...
   79 	    61 	 0.09051 	 0.05468 	 m..s
   85 	    62 	 0.10456 	 0.05540 	 m..s
   76 	    63 	 0.08817 	 0.05549 	 m..s
   88 	    64 	 0.11186 	 0.05917 	 m..s
   84 	    65 	 0.10288 	 0.06455 	 m..s
   87 	    66 	 0.10949 	 0.07545 	 m..s
   47 	    67 	 0.07802 	 0.07881 	 ~...
   21 	    68 	 0.06175 	 0.08233 	 ~...
   57 	    69 	 0.08080 	 0.08475 	 ~...
   72 	    70 	 0.08493 	 0.09187 	 ~...
   63 	    71 	 0.08298 	 0.09758 	 ~...
   53 	    72 	 0.07975 	 0.10112 	 ~...
   41 	    73 	 0.07655 	 0.10126 	 ~...
   83 	    74 	 0.09617 	 0.10197 	 ~...
   67 	    75 	 0.08408 	 0.10361 	 ~...
   70 	    76 	 0.08473 	 0.10391 	 ~...
   36 	    77 	 0.07336 	 0.10545 	 m..s
   17 	    78 	 0.06085 	 0.10574 	 m..s
   46 	    79 	 0.07787 	 0.10756 	 ~...
   75 	    80 	 0.08799 	 0.10907 	 ~...
   69 	    81 	 0.08430 	 0.11882 	 m..s
   73 	    82 	 0.08519 	 0.12426 	 m..s
   78 	    83 	 0.08937 	 0.12523 	 m..s
   94 	    84 	 0.16791 	 0.12681 	 m..s
   77 	    85 	 0.08920 	 0.12795 	 m..s
   91 	    86 	 0.16354 	 0.12843 	 m..s
   44 	    87 	 0.07725 	 0.12928 	 m..s
   89 	    88 	 0.12515 	 0.13010 	 ~...
   74 	    89 	 0.08555 	 0.13182 	 m..s
   71 	    90 	 0.08480 	 0.13372 	 m..s
   90 	    91 	 0.16301 	 0.13948 	 ~...
   93 	    92 	 0.16550 	 0.14093 	 ~...
   45 	    93 	 0.07733 	 0.14266 	 m..s
   92 	    94 	 0.16513 	 0.14609 	 ~...
   59 	    95 	 0.08102 	 0.15339 	 m..s
   96 	    96 	 0.24058 	 0.18150 	 m..s
   98 	    97 	 0.27962 	 0.20395 	 m..s
   99 	    98 	 0.29966 	 0.24174 	 m..s
   97 	    99 	 0.25193 	 0.24502 	 ~...
  102 	   100 	 0.41361 	 0.26525 	 MISS
  100 	   101 	 0.39895 	 0.28799 	 MISS
  101 	   102 	 0.40885 	 0.32168 	 m..s
  104 	   103 	 0.52844 	 0.51740 	 ~...
  106 	   104 	 0.53363 	 0.52928 	 ~...
  105 	   105 	 0.53344 	 0.53116 	 ~...
  103 	   106 	 0.52770 	 0.53485 	 ~...
  116 	   107 	 0.55555 	 0.53487 	 ~...
  115 	   108 	 0.55424 	 0.53728 	 ~...
  109 	   109 	 0.54532 	 0.53966 	 ~...
  114 	   110 	 0.55116 	 0.53971 	 ~...
  107 	   111 	 0.53570 	 0.54126 	 ~...
  119 	   112 	 0.55927 	 0.54280 	 ~...
  111 	   113 	 0.54908 	 0.54425 	 ~...
  108 	   114 	 0.54170 	 0.54652 	 ~...
  117 	   115 	 0.55611 	 0.54712 	 ~...
  118 	   116 	 0.55813 	 0.54813 	 ~...
  120 	   117 	 0.55949 	 0.54936 	 ~...
  112 	   118 	 0.54935 	 0.55007 	 ~...
  110 	   119 	 0.54733 	 0.55712 	 ~...
  113 	   120 	 0.55064 	 0.56198 	 ~...
==========================================
r_mrr = 0.9809667468070984
r2_mrr = 0.953596830368042
spearmanr_mrr@5 = 0.8720766305923462
spearmanr_mrr@10 = 0.8975297808647156
spearmanr_mrr@50 = 0.98569655418396
spearmanr_mrr@100 = 0.9897037744522095
spearmanr_mrr@All = 0.9905523061752319
==========================================
test time: 0.445
Done Testing dataset UMLS
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.415 +- 0.255
mrr vals (pred, true): 0.084, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.04731 	 5e-0500 	 m..s
    3 	     1 	 0.01326 	 0.00011 	 ~...
   25 	     2 	 0.06944 	 0.00013 	 m..s
   39 	     3 	 0.07853 	 0.00015 	 m..s
   35 	     4 	 0.07714 	 0.00016 	 m..s
   52 	     5 	 0.08419 	 0.00016 	 m..s
   23 	     6 	 0.06221 	 0.00017 	 m..s
   40 	     7 	 0.07854 	 0.00018 	 m..s
   73 	     8 	 0.09571 	 0.00018 	 m..s
   26 	     9 	 0.07295 	 0.00018 	 m..s
   38 	    10 	 0.07847 	 0.00019 	 m..s
   59 	    11 	 0.09023 	 0.00019 	 m..s
   29 	    12 	 0.07402 	 0.00019 	 m..s
   14 	    13 	 0.03771 	 0.00019 	 m..s
   78 	    14 	 0.09713 	 0.00019 	 m..s
   43 	    15 	 0.08093 	 0.00020 	 m..s
   34 	    16 	 0.07571 	 0.00020 	 m..s
   19 	    17 	 0.04852 	 0.00021 	 m..s
   75 	    18 	 0.09589 	 0.00021 	 m..s
   60 	    19 	 0.09046 	 0.00021 	 m..s
   56 	    20 	 0.08834 	 0.00022 	 m..s
   82 	    21 	 0.10095 	 0.00022 	 MISS
   51 	    22 	 0.08417 	 0.00022 	 m..s
   13 	    23 	 0.03755 	 0.00022 	 m..s
   32 	    24 	 0.07460 	 0.00023 	 m..s
   79 	    25 	 0.09804 	 0.00023 	 m..s
   83 	    26 	 0.10119 	 0.00023 	 MISS
   42 	    27 	 0.07948 	 0.00024 	 m..s
   30 	    28 	 0.07413 	 0.00024 	 m..s
    7 	    29 	 0.03015 	 0.00024 	 ~...
   74 	    30 	 0.09578 	 0.00025 	 m..s
   61 	    31 	 0.09066 	 0.00026 	 m..s
    2 	    32 	 0.01300 	 0.00027 	 ~...
   67 	    33 	 0.09379 	 0.00027 	 m..s
    9 	    34 	 0.03258 	 0.00028 	 m..s
   63 	    35 	 0.09121 	 0.00029 	 m..s
   58 	    36 	 0.08974 	 0.00030 	 m..s
    6 	    37 	 0.02898 	 0.00031 	 ~...
   66 	    38 	 0.09353 	 0.00032 	 m..s
    0 	    39 	 0.00060 	 0.00033 	 ~...
   54 	    40 	 0.08754 	 0.00034 	 m..s
    1 	    41 	 0.00452 	 0.00035 	 ~...
    4 	    42 	 0.01758 	 0.00037 	 ~...
   48 	    43 	 0.08351 	 0.00038 	 m..s
   28 	    44 	 0.07393 	 0.00039 	 m..s
   21 	    45 	 0.04935 	 0.00042 	 m..s
   37 	    46 	 0.07818 	 0.00043 	 m..s
   71 	    47 	 0.09553 	 0.00044 	 m..s
   18 	    48 	 0.04848 	 0.00046 	 m..s
   65 	    49 	 0.09276 	 0.00047 	 m..s
   62 	    50 	 0.09082 	 0.00048 	 m..s
    5 	    51 	 0.02842 	 0.00050 	 ~...
    8 	    52 	 0.03076 	 0.00063 	 m..s
   12 	    53 	 0.03302 	 0.00090 	 m..s
   11 	    54 	 0.03284 	 0.00267 	 m..s
   85 	    55 	 0.10708 	 0.00504 	 MISS
   10 	    56 	 0.03262 	 0.00554 	 ~...
   16 	    57 	 0.04661 	 0.00823 	 m..s
   20 	    58 	 0.04866 	 0.01606 	 m..s
   15 	    59 	 0.04165 	 0.02709 	 ~...
   22 	    60 	 0.05703 	 0.02819 	 ~...
   86 	    61 	 0.10943 	 0.05033 	 m..s
   87 	    62 	 0.11075 	 0.05864 	 m..s
   31 	    63 	 0.07419 	 0.13287 	 m..s
   27 	    64 	 0.07386 	 0.13359 	 m..s
   33 	    65 	 0.07547 	 0.13683 	 m..s
   24 	    66 	 0.06935 	 0.13896 	 m..s
   77 	    67 	 0.09626 	 0.14134 	 m..s
   53 	    68 	 0.08687 	 0.14137 	 m..s
   64 	    69 	 0.09230 	 0.14309 	 m..s
   89 	    70 	 0.15056 	 0.14379 	 ~...
   69 	    71 	 0.09516 	 0.14632 	 m..s
   47 	    72 	 0.08277 	 0.15086 	 m..s
   57 	    73 	 0.08935 	 0.15226 	 m..s
   92 	    74 	 0.16348 	 0.15238 	 ~...
   44 	    75 	 0.08104 	 0.15362 	 m..s
   91 	    76 	 0.15871 	 0.15473 	 ~...
   80 	    77 	 0.09926 	 0.15486 	 m..s
   72 	    78 	 0.09565 	 0.15532 	 m..s
   76 	    79 	 0.09603 	 0.15541 	 m..s
   88 	    80 	 0.12072 	 0.15591 	 m..s
   36 	    81 	 0.07787 	 0.15762 	 m..s
   50 	    82 	 0.08364 	 0.15883 	 m..s
   81 	    83 	 0.09960 	 0.16170 	 m..s
   41 	    84 	 0.07929 	 0.16376 	 m..s
   96 	    85 	 0.19533 	 0.16644 	 ~...
   49 	    86 	 0.08356 	 0.16703 	 m..s
   45 	    87 	 0.08146 	 0.16809 	 m..s
   93 	    88 	 0.17468 	 0.17499 	 ~...
   68 	    89 	 0.09506 	 0.17565 	 m..s
   97 	    90 	 0.20238 	 0.17592 	 ~...
   55 	    91 	 0.08795 	 0.17611 	 m..s
   95 	    92 	 0.17982 	 0.17630 	 ~...
   70 	    93 	 0.09539 	 0.17704 	 m..s
   46 	    94 	 0.08240 	 0.17807 	 m..s
   84 	    95 	 0.10168 	 0.19208 	 m..s
   94 	    96 	 0.17501 	 0.19917 	 ~...
  102 	    97 	 0.21554 	 0.20377 	 ~...
   90 	    98 	 0.15270 	 0.21254 	 m..s
   98 	    99 	 0.20681 	 0.21272 	 ~...
  104 	   100 	 0.21923 	 0.21432 	 ~...
  105 	   101 	 0.22706 	 0.21534 	 ~...
  100 	   102 	 0.21391 	 0.21714 	 ~...
   99 	   103 	 0.21252 	 0.22405 	 ~...
  103 	   104 	 0.21591 	 0.22461 	 ~...
  101 	   105 	 0.21438 	 0.25673 	 m..s
  106 	   106 	 0.24357 	 0.27311 	 ~...
  111 	   107 	 0.28522 	 0.27314 	 ~...
  113 	   108 	 0.30201 	 0.27466 	 ~...
  109 	   109 	 0.26808 	 0.29003 	 ~...
  107 	   110 	 0.26140 	 0.29470 	 m..s
  117 	   111 	 0.32486 	 0.30141 	 ~...
  116 	   112 	 0.31894 	 0.30207 	 ~...
  110 	   113 	 0.27876 	 0.30319 	 ~...
  118 	   114 	 0.32969 	 0.31099 	 ~...
  108 	   115 	 0.26487 	 0.31202 	 m..s
  115 	   116 	 0.31190 	 0.32533 	 ~...
  112 	   117 	 0.30176 	 0.32804 	 ~...
  120 	   118 	 0.35621 	 0.35127 	 ~...
  114 	   119 	 0.30784 	 0.36809 	 m..s
  119 	   120 	 0.34666 	 0.37250 	 ~...
==========================================
r_mrr = 0.8585090637207031
r2_mrr = 0.702233076095581
spearmanr_mrr@5 = 0.9512109756469727
spearmanr_mrr@10 = 0.9600496888160706
spearmanr_mrr@50 = 0.9745136499404907
spearmanr_mrr@100 = 0.8928530812263489
spearmanr_mrr@All = 0.9093752503395081
==========================================
test time: 0.499
Done Testing dataset DBpedia50
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.414 +- 0.235
mrr vals (pred, true): 0.067, 0.005

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.03980 	 0.00073 	 m..s
    3 	     1 	 0.03418 	 0.00075 	 m..s
    2 	     2 	 0.03415 	 0.00076 	 m..s
   41 	     3 	 0.08547 	 0.00104 	 m..s
   18 	     4 	 0.05996 	 0.00149 	 m..s
   19 	     5 	 0.06203 	 0.00196 	 m..s
   14 	     6 	 0.05566 	 0.00219 	 m..s
   67 	     7 	 0.11029 	 0.00310 	 MISS
   62 	     8 	 0.10959 	 0.00320 	 MISS
   31 	     9 	 0.07123 	 0.00321 	 m..s
   27 	    10 	 0.06994 	 0.00326 	 m..s
   69 	    11 	 0.11098 	 0.00330 	 MISS
   10 	    12 	 0.04414 	 0.00331 	 m..s
   48 	    13 	 0.10419 	 0.00340 	 MISS
   58 	    14 	 0.10819 	 0.00343 	 MISS
   49 	    15 	 0.10462 	 0.00350 	 MISS
    5 	    16 	 0.03494 	 0.00351 	 m..s
   76 	    17 	 0.11387 	 0.00351 	 MISS
    4 	    18 	 0.03490 	 0.00363 	 m..s
   92 	    19 	 0.12065 	 0.00365 	 MISS
   78 	    20 	 0.11503 	 0.00370 	 MISS
   87 	    21 	 0.11887 	 0.00371 	 MISS
   28 	    22 	 0.07071 	 0.00373 	 m..s
   20 	    23 	 0.06635 	 0.00373 	 m..s
   29 	    24 	 0.07114 	 0.00376 	 m..s
   13 	    25 	 0.05526 	 0.00385 	 m..s
   68 	    26 	 0.11084 	 0.00387 	 MISS
   93 	    27 	 0.12591 	 0.00393 	 MISS
    6 	    28 	 0.03938 	 0.00395 	 m..s
   70 	    29 	 0.11099 	 0.00403 	 MISS
   12 	    30 	 0.05254 	 0.00408 	 m..s
   24 	    31 	 0.06798 	 0.00411 	 m..s
   53 	    32 	 0.10590 	 0.00413 	 MISS
   79 	    33 	 0.11537 	 0.00418 	 MISS
   80 	    34 	 0.11591 	 0.00419 	 MISS
    0 	    35 	 0.03056 	 0.00422 	 ~...
   54 	    36 	 0.10621 	 0.00424 	 MISS
   81 	    37 	 0.11596 	 0.00427 	 MISS
   35 	    38 	 0.07240 	 0.00427 	 m..s
   94 	    39 	 0.12736 	 0.00433 	 MISS
   21 	    40 	 0.06640 	 0.00438 	 m..s
   34 	    41 	 0.07159 	 0.00438 	 m..s
   52 	    42 	 0.10531 	 0.00446 	 MISS
   30 	    43 	 0.07123 	 0.00451 	 m..s
   71 	    44 	 0.11137 	 0.00459 	 MISS
   63 	    45 	 0.10966 	 0.00462 	 MISS
   88 	    46 	 0.11893 	 0.00463 	 MISS
   51 	    47 	 0.10515 	 0.00469 	 MISS
   56 	    48 	 0.10682 	 0.00471 	 MISS
   75 	    49 	 0.11328 	 0.00471 	 MISS
   22 	    50 	 0.06738 	 0.00481 	 m..s
   59 	    51 	 0.10838 	 0.00498 	 MISS
    9 	    52 	 0.04316 	 0.00498 	 m..s
    1 	    53 	 0.03335 	 0.00504 	 ~...
    8 	    54 	 0.04205 	 0.00508 	 m..s
   15 	    55 	 0.05587 	 0.00516 	 m..s
   16 	    56 	 0.05591 	 0.02123 	 m..s
   36 	    57 	 0.08327 	 0.03459 	 m..s
   38 	    58 	 0.08414 	 0.03469 	 m..s
   39 	    59 	 0.08421 	 0.03523 	 m..s
   17 	    60 	 0.05663 	 0.03743 	 ~...
   37 	    61 	 0.08387 	 0.04999 	 m..s
   47 	    62 	 0.09356 	 0.05123 	 m..s
   45 	    63 	 0.09239 	 0.05290 	 m..s
   40 	    64 	 0.08490 	 0.05414 	 m..s
   43 	    65 	 0.09191 	 0.05677 	 m..s
   23 	    66 	 0.06751 	 0.05678 	 ~...
   42 	    67 	 0.08712 	 0.05856 	 ~...
   46 	    68 	 0.09246 	 0.06077 	 m..s
   44 	    69 	 0.09210 	 0.06145 	 m..s
   99 	    70 	 0.17500 	 0.06720 	 MISS
   11 	    71 	 0.05121 	 0.07522 	 ~...
   26 	    72 	 0.06955 	 0.10894 	 m..s
  100 	    73 	 0.17611 	 0.12580 	 m..s
   25 	    74 	 0.06883 	 0.13062 	 m..s
   96 	    75 	 0.14016 	 0.14076 	 ~...
  107 	    76 	 0.21528 	 0.14257 	 m..s
  101 	    77 	 0.17857 	 0.14888 	 ~...
   32 	    78 	 0.07149 	 0.14941 	 m..s
   33 	    79 	 0.07150 	 0.15508 	 m..s
   98 	    80 	 0.16304 	 0.15737 	 ~...
   84 	    81 	 0.11757 	 0.17776 	 m..s
  103 	    82 	 0.20243 	 0.17869 	 ~...
   77 	    83 	 0.11467 	 0.18342 	 m..s
   50 	    84 	 0.10479 	 0.18540 	 m..s
   72 	    85 	 0.11155 	 0.19254 	 m..s
   66 	    86 	 0.11022 	 0.19394 	 m..s
   61 	    87 	 0.10868 	 0.19534 	 m..s
   73 	    88 	 0.11205 	 0.19823 	 m..s
   86 	    89 	 0.11868 	 0.19856 	 m..s
  105 	    90 	 0.20728 	 0.20056 	 ~...
   60 	    91 	 0.10844 	 0.20143 	 m..s
   74 	    92 	 0.11302 	 0.20310 	 m..s
   91 	    93 	 0.11940 	 0.20502 	 m..s
  102 	    94 	 0.20210 	 0.20874 	 ~...
  104 	    95 	 0.20713 	 0.20881 	 ~...
   55 	    96 	 0.10656 	 0.21152 	 MISS
   85 	    97 	 0.11848 	 0.21545 	 m..s
   90 	    98 	 0.11922 	 0.21860 	 m..s
   95 	    99 	 0.12821 	 0.21875 	 m..s
   82 	   100 	 0.11664 	 0.21877 	 MISS
   57 	   101 	 0.10767 	 0.22411 	 MISS
   64 	   102 	 0.11002 	 0.22855 	 MISS
   89 	   103 	 0.11918 	 0.22917 	 MISS
   65 	   104 	 0.11012 	 0.23132 	 MISS
   83 	   105 	 0.11676 	 0.23165 	 MISS
  109 	   106 	 0.22341 	 0.23182 	 ~...
  117 	   107 	 0.23230 	 0.23504 	 ~...
   97 	   108 	 0.14694 	 0.23662 	 m..s
  106 	   109 	 0.20964 	 0.23702 	 ~...
  112 	   110 	 0.22504 	 0.23706 	 ~...
  113 	   111 	 0.22624 	 0.24592 	 ~...
  108 	   112 	 0.22212 	 0.25434 	 m..s
  119 	   113 	 0.23575 	 0.25466 	 ~...
  110 	   114 	 0.22426 	 0.25739 	 m..s
  114 	   115 	 0.23138 	 0.25954 	 ~...
  116 	   116 	 0.23212 	 0.26639 	 m..s
  118 	   117 	 0.23384 	 0.26771 	 m..s
  120 	   118 	 0.23670 	 0.29434 	 m..s
  111 	   119 	 0.22494 	 0.29784 	 m..s
  115 	   120 	 0.23170 	 0.29964 	 m..s
==========================================
r_mrr = 0.7230523228645325
r2_mrr = 0.454887330532074
spearmanr_mrr@5 = 0.9158204197883606
spearmanr_mrr@10 = 0.8706703186035156
spearmanr_mrr@50 = 0.8491584658622742
spearmanr_mrr@100 = 0.8612806797027588
spearmanr_mrr@All = 0.8792789578437805
==========================================
test time: 0.461
Done Testing dataset CoDExSmall
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.399 +- 0.200
mrr vals (pred, true): 0.049, 0.002

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   57 	     0 	 0.05316 	 0.00011 	 m..s
   30 	     1 	 0.05100 	 0.00048 	 m..s
    5 	     2 	 0.04362 	 0.00048 	 m..s
   36 	     3 	 0.05211 	 0.00049 	 m..s
   58 	     4 	 0.05321 	 0.00049 	 m..s
    7 	     5 	 0.04379 	 0.00050 	 m..s
   59 	     6 	 0.05327 	 0.00051 	 m..s
   50 	     7 	 0.05277 	 0.00051 	 m..s
    0 	     8 	 0.03811 	 0.00052 	 m..s
   44 	     9 	 0.05246 	 0.00052 	 m..s
   38 	    10 	 0.05219 	 0.00052 	 m..s
   25 	    11 	 0.04994 	 0.00053 	 m..s
   11 	    12 	 0.04495 	 0.00053 	 m..s
   47 	    13 	 0.05264 	 0.00054 	 m..s
   56 	    14 	 0.05316 	 0.00056 	 m..s
   45 	    15 	 0.05260 	 0.00058 	 m..s
   51 	    16 	 0.05281 	 0.00058 	 m..s
   74 	    17 	 0.05416 	 0.00059 	 m..s
   81 	    18 	 0.05527 	 0.00060 	 m..s
   31 	    19 	 0.05117 	 0.00060 	 m..s
    8 	    20 	 0.04388 	 0.00060 	 m..s
   64 	    21 	 0.05345 	 0.00061 	 m..s
   12 	    22 	 0.04523 	 0.00061 	 m..s
   28 	    23 	 0.05051 	 0.00061 	 m..s
   17 	    24 	 0.04753 	 0.00063 	 m..s
    3 	    25 	 0.04061 	 0.00064 	 m..s
   73 	    26 	 0.05408 	 0.00065 	 m..s
   46 	    27 	 0.05261 	 0.00066 	 m..s
   80 	    28 	 0.05472 	 0.00066 	 m..s
   37 	    29 	 0.05214 	 0.00068 	 m..s
   60 	    30 	 0.05327 	 0.00068 	 m..s
   83 	    31 	 0.05555 	 0.00068 	 m..s
   16 	    32 	 0.04737 	 0.00068 	 m..s
   53 	    33 	 0.05293 	 0.00069 	 m..s
   33 	    34 	 0.05166 	 0.00070 	 m..s
    4 	    35 	 0.04066 	 0.00073 	 m..s
   61 	    36 	 0.05336 	 0.00073 	 m..s
   24 	    37 	 0.04990 	 0.00073 	 m..s
   34 	    38 	 0.05168 	 0.00073 	 m..s
   41 	    39 	 0.05243 	 0.00077 	 m..s
   48 	    40 	 0.05266 	 0.00078 	 m..s
    9 	    41 	 0.04390 	 0.00079 	 m..s
   79 	    42 	 0.05467 	 0.00080 	 m..s
   78 	    43 	 0.05459 	 0.00084 	 m..s
   29 	    44 	 0.05058 	 0.00085 	 m..s
   18 	    45 	 0.04754 	 0.00098 	 m..s
   69 	    46 	 0.05361 	 0.00101 	 m..s
   15 	    47 	 0.04635 	 0.00108 	 m..s
    6 	    48 	 0.04375 	 0.00112 	 m..s
   84 	    49 	 0.08221 	 0.00115 	 m..s
   23 	    50 	 0.04955 	 0.00117 	 m..s
   13 	    51 	 0.04624 	 0.00117 	 m..s
   14 	    52 	 0.04630 	 0.00129 	 m..s
   20 	    53 	 0.04887 	 0.00181 	 m..s
   71 	    54 	 0.05382 	 0.00344 	 m..s
    2 	    55 	 0.04014 	 0.00540 	 m..s
    1 	    56 	 0.04009 	 0.00545 	 m..s
   19 	    57 	 0.04772 	 0.00635 	 m..s
   10 	    58 	 0.04470 	 0.00637 	 m..s
   85 	    59 	 0.10467 	 0.00718 	 m..s
   22 	    60 	 0.04923 	 0.01350 	 m..s
   65 	    61 	 0.05352 	 0.04147 	 ~...
   66 	    62 	 0.05355 	 0.04381 	 ~...
   72 	    63 	 0.05390 	 0.04882 	 ~...
   49 	    64 	 0.05271 	 0.04887 	 ~...
   26 	    65 	 0.04999 	 0.05014 	 ~...
   21 	    66 	 0.04896 	 0.05230 	 ~...
   63 	    67 	 0.05344 	 0.05457 	 ~...
   87 	    68 	 0.11645 	 0.05487 	 m..s
   27 	    69 	 0.05005 	 0.05553 	 ~...
   86 	    70 	 0.11504 	 0.05583 	 m..s
   39 	    71 	 0.05227 	 0.05707 	 ~...
   55 	    72 	 0.05310 	 0.05828 	 ~...
   70 	    73 	 0.05367 	 0.05961 	 ~...
   76 	    74 	 0.05431 	 0.06023 	 ~...
   52 	    75 	 0.05290 	 0.06035 	 ~...
   68 	    76 	 0.05359 	 0.06130 	 ~...
   32 	    77 	 0.05135 	 0.06302 	 ~...
   35 	    78 	 0.05203 	 0.06397 	 ~...
   54 	    79 	 0.05293 	 0.06524 	 ~...
   77 	    80 	 0.05455 	 0.06650 	 ~...
   62 	    81 	 0.05338 	 0.06711 	 ~...
   67 	    82 	 0.05359 	 0.06947 	 ~...
   42 	    83 	 0.05243 	 0.06970 	 ~...
   82 	    84 	 0.05548 	 0.07587 	 ~...
   75 	    85 	 0.05419 	 0.07943 	 ~...
   43 	    86 	 0.05244 	 0.07964 	 ~...
   40 	    87 	 0.05241 	 0.08738 	 m..s
   88 	    88 	 0.13737 	 0.13100 	 ~...
   97 	    89 	 0.18977 	 0.13488 	 m..s
   94 	    90 	 0.16796 	 0.13955 	 ~...
   91 	    91 	 0.14800 	 0.14021 	 ~...
   90 	    92 	 0.14677 	 0.14670 	 ~...
   96 	    93 	 0.18486 	 0.15026 	 m..s
  102 	    94 	 0.20072 	 0.16345 	 m..s
   89 	    95 	 0.14032 	 0.17008 	 ~...
   93 	    96 	 0.16574 	 0.17047 	 ~...
   92 	    97 	 0.16495 	 0.17630 	 ~...
   95 	    98 	 0.17175 	 0.18566 	 ~...
  100 	    99 	 0.19903 	 0.18671 	 ~...
  109 	   100 	 0.23813 	 0.20101 	 m..s
  101 	   101 	 0.20034 	 0.20773 	 ~...
   98 	   102 	 0.19320 	 0.21003 	 ~...
  104 	   103 	 0.20189 	 0.21193 	 ~...
  103 	   104 	 0.20159 	 0.21217 	 ~...
   99 	   105 	 0.19786 	 0.22069 	 ~...
  105 	   106 	 0.21146 	 0.22448 	 ~...
  106 	   107 	 0.21715 	 0.22645 	 ~...
  108 	   108 	 0.21960 	 0.23996 	 ~...
  107 	   109 	 0.21894 	 0.24123 	 ~...
  110 	   110 	 0.23860 	 0.25774 	 ~...
  111 	   111 	 0.24600 	 0.26887 	 ~...
  114 	   112 	 0.27557 	 0.27326 	 ~...
  117 	   113 	 0.28215 	 0.27439 	 ~...
  118 	   114 	 0.28264 	 0.27439 	 ~...
  115 	   115 	 0.28001 	 0.27640 	 ~...
  112 	   116 	 0.26301 	 0.27724 	 ~...
  113 	   117 	 0.27482 	 0.28142 	 ~...
  116 	   118 	 0.28182 	 0.29794 	 ~...
  120 	   119 	 0.30573 	 0.30087 	 ~...
  119 	   120 	 0.29267 	 0.31170 	 ~...
==========================================
r_mrr = 0.959223747253418
r2_mrr = 0.8362132906913757
spearmanr_mrr@5 = 0.8470671772956848
spearmanr_mrr@10 = 0.8299024105072021
spearmanr_mrr@50 = 0.9892576932907104
spearmanr_mrr@100 = 0.975566029548645
spearmanr_mrr@All = 0.977409839630127
==========================================
test time: 0.422
Done Testing dataset OpenEA
total time taken: 910.3639147281647
training time taken: 871.461416721344
TWIG out ;))
========================================================
--------------------------------------------------------
Running a TWIG experiment with tag: DistMult-omit-OpenEA
--------------------------------------------------------
========================================================
Using random seed: 8600991174609466
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [870, 1119, 39, 661, 67, 26, 457, 288, 757, 136, 560, 1104, 1001, 461, 121, 1093, 61, 921, 1172, 438, 441, 432, 117, 379, 1118, 894, 873, 340, 342, 977, 565, 847, 210, 328, 1140, 248, 1064, 0, 639, 237, 866, 688, 764, 703, 595, 900, 840, 193, 70, 611, 631, 329, 1029, 720, 28, 521, 676, 335, 228, 978, 369, 640, 296, 646, 964, 323, 800, 986, 159, 948, 204, 303, 568, 993, 799, 588, 896, 924, 700, 147, 1120, 1124, 146, 113, 447, 158, 851, 258, 1105, 337, 487, 941, 752, 332, 794, 280, 172, 594, 65, 48, 952, 186, 880, 872, 211, 929, 809, 1096, 428, 825, 83, 1030, 754, 969, 208, 411, 928, 608, 1152, 1112, 359]
valid_ids (0): []
train_ids (1094): [628, 957, 253, 808, 425, 242, 839, 859, 42, 1125, 821, 730, 1106, 281, 1178, 843, 996, 925, 291, 59, 161, 123, 788, 421, 1039, 430, 483, 16, 858, 95, 633, 1142, 771, 979, 864, 76, 903, 1099, 271, 1182, 804, 1123, 200, 550, 719, 251, 638, 672, 362, 517, 659, 823, 325, 14, 968, 120, 85, 1122, 56, 98, 5, 316, 272, 31, 232, 278, 1068, 542, 108, 1050, 137, 315, 711, 891, 499, 1044, 726, 698, 124, 484, 861, 338, 401, 1179, 66, 753, 1073, 370, 2, 404, 769, 1176, 785, 12, 805, 1004, 1108, 145, 244, 702, 1188, 333, 1200, 574, 227, 692, 919, 1024, 54, 1098, 183, 390, 747, 44, 547, 177, 1116, 576, 321, 174, 920, 285, 944, 561, 235, 463, 184, 304, 8, 410, 617, 259, 855, 229, 549, 491, 994, 155, 1002, 1046, 543, 512, 134, 603, 1159, 883, 857, 18, 489, 784, 778, 577, 775, 1076, 1000, 490, 630, 202, 74, 182, 110, 424, 1160, 621, 1067, 665, 1048, 122, 908, 6, 389, 1051, 706, 482, 341, 1101, 915, 279, 910, 786, 976, 845, 1204, 149, 987, 1018, 967, 904, 1180, 678, 79, 78, 1206, 444, 619, 274, 256, 105, 395, 27, 84, 882, 380, 1077, 647, 1031, 1028, 875, 416, 451, 494, 417, 1121, 934, 933, 99, 37, 356, 265, 596, 735, 357, 385, 807, 1209, 687, 842, 887, 1155, 673, 1181, 420, 545, 637, 297, 974, 709, 1015, 835, 538, 460, 493, 209, 471, 35, 909, 890, 622, 168, 257, 1214, 1157, 269, 571, 722, 32, 320, 60, 442, 431, 522, 867, 299, 473, 1014, 1036, 756, 693, 492, 1033, 171, 94, 898, 535, 1085, 1109, 1006, 1151, 102, 236, 497, 779, 150, 4, 100, 292, 980, 555, 834, 852, 454, 951, 437, 1088, 742, 755, 990, 192, 58, 398, 985, 445, 570, 936, 361, 1191, 846, 1032, 162, 674, 984, 1017, 501, 836, 219, 1183, 115, 214, 334, 306, 705, 878, 350, 701, 686, 946, 1042, 170, 383, 429, 412, 906, 9, 216, 1139, 1166, 767, 408, 650, 488, 1194, 573, 128, 507, 1080, 668, 459, 627, 1086, 19, 131, 599, 1161, 578, 911, 1090, 384, 284, 736, 406, 829, 217, 759, 239, 879, 718, 889, 583, 166, 917, 399, 225, 1071, 721, 863, 774, 160, 1005, 141, 133, 1115, 36, 932, 677, 354, 313, 1089, 541, 135, 998, 125, 1094, 1153, 716, 455, 190, 949, 1111, 427, 527, 400, 1069, 132, 97, 888, 69, 1135, 1126, 886, 699, 226, 114, 360, 819, 51, 1169, 393, 865, 107, 309, 1156, 358, 252, 215, 811, 194, 326, 1170, 86, 790, 1184, 881, 196, 453, 653, 481, 273, 344, 414, 405, 563, 520, 305, 462, 1010, 553, 287, 1162, 681, 746, 249, 308, 88, 1145, 1130, 704, 50, 587, 970, 1060, 679, 266, 671, 1208, 264, 798, 181, 1137, 620, 760, 41, 1134, 238, 118, 1149, 505, 143, 77, 614, 939, 623, 339, 415, 1058, 991, 433, 409, 680, 1007, 345, 777, 876, 584, 532, 947, 832, 73, 310, 247, 548, 1092, 17, 1034, 1063, 352, 1199, 945, 862, 598, 551, 624, 466, 1174, 1186, 263, 973, 1127, 250, 1110, 745, 975, 516, 860, 372, 667, 129, 116, 3, 645, 897, 806, 761, 597, 1047, 685, 81, 604, 776, 744, 1091, 439, 426, 1207, 452, 768, 1100, 1196, 830, 854, 1146, 856, 500, 176, 901, 803, 913, 43, 1185, 1040, 1038, 899, 475, 376, 802, 156, 153, 524, 387, 938, 1158, 386, 289, 198, 286, 783, 732, 275, 1022, 378, 435, 999, 963, 498, 234, 694, 916, 708, 995, 734, 261, 644, 616, 314, 1078, 49, 53, 394, 90, 1171, 960, 1, 515, 797, 817, 423, 510, 221, 163, 741, 1173, 1203, 343, 1131, 740, 642, 572, 918, 795, 109, 1026, 780, 262, 922, 554, 710, 814, 822, 1056, 643, 697, 648, 87, 440, 724, 348, 179, 233, 62, 770, 185, 812, 758, 294, 956, 373, 531, 139, 13, 625, 206, 255, 629, 363, 593, 418, 374, 530, 506, 656, 766, 649, 245, 575, 364, 955, 1210, 1045, 1138, 564, 801, 504, 581, 293, 104, 290, 682, 449, 1205, 818, 651, 397, 1008, 670, 586, 853, 850, 1054, 926, 662, 606, 1141, 144, 22, 1102, 46, 820, 371, 1057, 502, 610, 511, 57, 407, 327, 34, 589, 29, 612, 324, 848, 953, 1013, 346, 241, 355, 223, 268, 71, 884, 1009, 1143, 302, 402, 64, 632, 82, 1081, 1202, 1107, 1132, 330, 231, 148, 1167, 240, 824, 591, 1053, 212, 1197, 1012, 1192, 1055, 322, 205, 658, 727, 833, 464, 496, 592, 38, 207, 470, 601, 106, 927, 254, 874, 844, 92, 849, 567, 534, 712, 1187, 743, 72, 413, 178, 988, 523, 392, 101, 1083, 877, 618, 474, 1095, 1016, 1213, 675, 539, 191, 1003, 1177, 902, 509, 1129, 1020, 222, 391, 1087, 695, 937, 126, 1147, 458, 728, 654, 331, 366, 220, 971, 765, 25, 602, 983, 748, 714, 479, 559, 1097, 950, 381, 1175, 476, 45, 486, 787, 349, 1052, 213, 930, 635, 962, 180, 569, 127, 965, 605, 230, 518, 893, 958, 664, 246, 195, 10, 317, 696, 810, 626, 1075, 689, 93, 382, 300, 636, 307, 982, 940, 353, 566, 615, 738, 1079, 465, 1114, 669, 772, 813, 188, 1065, 301, 1190, 119, 336, 276, 1082, 826, 961, 152, 1043, 725, 816, 80, 243, 519, 282, 838, 283, 607, 469, 267, 749, 436, 7, 684, 96, 544, 1193, 1168, 737, 763, 1023, 175, 585, 1011, 733, 260, 1027, 526, 641, 707, 562, 270, 21, 750, 634, 1070, 189, 319, 1049, 298, 1074, 450, 1103, 91, 164, 47, 199, 30, 789, 312, 871, 723, 600, 868, 15, 167, 885, 52, 1021, 63, 1195, 1189, 130, 103, 1061, 295, 989, 1025, 20, 1211, 151, 942, 365, 157, 895, 582, 683, 666, 311, 75, 33, 1072, 55, 552, 1066, 546, 609, 796, 367, 1163, 660, 347, 931, 1084, 1019, 89, 434, 224, 773, 468, 203, 138, 478, 1165, 792, 140, 731, 1062, 739, 1059, 923, 655, 1113, 935, 351, 528, 1136, 691, 556, 11, 456, 154, 375, 218, 1154, 914, 715, 446, 579, 762, 480, 485, 525, 943, 828, 513, 388, 503, 981, 791, 869, 827, 1198, 529, 24, 537, 652, 717, 1133, 657, 557, 1037, 558, 495, 663, 165, 422, 112, 514, 1150, 781, 508, 1148, 368, 540, 23, 905, 1128, 837, 142, 1201, 377, 1117, 997, 472, 201, 972, 815, 959, 729, 841, 197, 419, 1144, 713, 536, 966, 751, 173, 448, 403, 992, 467, 1212, 892, 590, 477, 318, 613, 1164, 690, 907, 831, 187, 954, 580, 40, 912, 443, 533, 396, 793, 169, 68, 277, 1035, 1041, 782, 111]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2455185670128053
the save name prefix for this run is:  chkpt-ID_2455185670128053_tag_DistMult-omit-OpenEA
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1'], 'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 312
rank avg (pred): 0.568 +- 0.008
mrr vals (pred, true): 0.013, 0.542
batch losses (mrrl, rdl): 0.0, 0.0059405551

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 135
rank avg (pred): 0.403 +- 0.072
mrr vals (pred, true): 0.019, 0.087
batch losses (mrrl, rdl): 0.0, 0.000135919

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 31
rank avg (pred): 0.081 +- 0.048
mrr vals (pred, true): 0.134, 0.520
batch losses (mrrl, rdl): 0.0, 3.43027e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 200
rank avg (pred): 0.328 +- 0.170
mrr vals (pred, true): 0.054, 0.051
batch losses (mrrl, rdl): 0.0, 0.000286368

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1176
rank avg (pred): 0.402 +- 0.237
mrr vals (pred, true): 0.105, 0.036
batch losses (mrrl, rdl): 0.0, 7.18981e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 597
rank avg (pred): 0.421 +- 0.242
mrr vals (pred, true): 0.110, 0.038
batch losses (mrrl, rdl): 0.0, 3.15167e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 546
rank avg (pred): 0.242 +- 0.183
mrr vals (pred, true): 0.200, 0.064
batch losses (mrrl, rdl): 0.0, 0.0005300922

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 352
rank avg (pred): 0.317 +- 0.248
mrr vals (pred, true): 0.213, 0.093
batch losses (mrrl, rdl): 0.0, 5.5855e-06

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 992
rank avg (pred): 0.025 +- 0.021
mrr vals (pred, true): 0.432, 0.533
batch losses (mrrl, rdl): 0.0, 2.5142e-06

Epoch over!
epoch time: 53.779

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 313
rank avg (pred): 0.072 +- 0.054
mrr vals (pred, true): 0.289, 0.555
batch losses (mrrl, rdl): 0.0, 2.74569e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 589
rank avg (pred): 0.436 +- 0.254
mrr vals (pred, true): 0.184, 0.043
batch losses (mrrl, rdl): 0.0, 1.96648e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 152
rank avg (pred): 0.322 +- 0.258
mrr vals (pred, true): 0.244, 0.104
batch losses (mrrl, rdl): 0.0, 6.17889e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1133
rank avg (pred): 0.333 +- 0.240
mrr vals (pred, true): 0.243, 0.048
batch losses (mrrl, rdl): 0.0, 0.0001494356

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 622
rank avg (pred): 0.431 +- 0.265
mrr vals (pred, true): 0.208, 0.042
batch losses (mrrl, rdl): 0.0, 9.0204e-06

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 216
rank avg (pred): 0.391 +- 0.263
mrr vals (pred, true): 0.241, 0.043
batch losses (mrrl, rdl): 0.0, 2.15881e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 682
rank avg (pred): 0.453 +- 0.273
mrr vals (pred, true): 0.192, 0.055
batch losses (mrrl, rdl): 0.0, 4.76072e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 539
rank avg (pred): 0.245 +- 0.219
mrr vals (pred, true): 0.330, 0.100
batch losses (mrrl, rdl): 0.0, 0.0001987266

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1057
rank avg (pred): 0.057 +- 0.052
mrr vals (pred, true): 0.414, 0.564
batch losses (mrrl, rdl): 0.0, 1.22547e-05

Epoch over!
epoch time: 53.85

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 501
rank avg (pred): 0.242 +- 0.207
mrr vals (pred, true): 0.301, 0.191
batch losses (mrrl, rdl): 0.0, 3.03174e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 884
rank avg (pred): 0.428 +- 0.279
mrr vals (pred, true): 0.217, 0.045
batch losses (mrrl, rdl): 0.0, 5.811e-06

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 52
rank avg (pred): 0.114 +- 0.112
mrr vals (pred, true): 0.384, 0.511
batch losses (mrrl, rdl): 0.0, 0.0001289142

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1010
rank avg (pred): 0.330 +- 0.280
mrr vals (pred, true): 0.315, 0.088
batch losses (mrrl, rdl): 0.0, 1.19048e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 66
rank avg (pred): 0.074 +- 0.075
mrr vals (pred, true): 0.392, 0.516
batch losses (mrrl, rdl): 0.0, 1.62791e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 142
rank avg (pred): 0.344 +- 0.285
mrr vals (pred, true): 0.303, 0.083
batch losses (mrrl, rdl): 0.0, 4.20553e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 598
rank avg (pred): 0.422 +- 0.248
mrr vals (pred, true): 0.195, 0.054
batch losses (mrrl, rdl): 0.0, 1.27256e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 726
rank avg (pred): 0.343 +- 0.235
mrr vals (pred, true): 0.266, 0.045
batch losses (mrrl, rdl): 0.0, 0.0001270071

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 860
rank avg (pred): 0.441 +- 0.256
mrr vals (pred, true): 0.174, 0.045
batch losses (mrrl, rdl): 0.0, 2.64888e-05

Epoch over!
epoch time: 53.074

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 199
rank avg (pred): 0.352 +- 0.290
mrr vals (pred, true): 0.307, 0.052
batch losses (mrrl, rdl): 0.0, 5.93439e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 935
rank avg (pred): 0.536 +- 0.260
mrr vals (pred, true): 0.082, 0.015
batch losses (mrrl, rdl): 0.0, 0.0008855996

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 853
rank avg (pred): 0.495 +- 0.276
mrr vals (pred, true): 0.137, 0.081
batch losses (mrrl, rdl): 0.0, 4.42291e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 776
rank avg (pred): 0.446 +- 0.276
mrr vals (pred, true): 0.207, 0.046
batch losses (mrrl, rdl): 0.0, 8.185e-06

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 755
rank avg (pred): 0.260 +- 0.199
mrr vals (pred, true): 0.267, 0.245
batch losses (mrrl, rdl): 0.0, 0.0001401144

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1174
rank avg (pred): 0.419 +- 0.286
mrr vals (pred, true): 0.250, 0.036
batch losses (mrrl, rdl): 0.0, 3.34434e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 393
rank avg (pred): 0.326 +- 0.297
mrr vals (pred, true): 0.344, 0.113
batch losses (mrrl, rdl): 0.0, 7.01316e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 786
rank avg (pred): 0.508 +- 0.279
mrr vals (pred, true): 0.115, 0.053
batch losses (mrrl, rdl): 0.0, 0.0001384581

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1069
rank avg (pred): 0.062 +- 0.056
mrr vals (pred, true): 0.413, 0.548
batch losses (mrrl, rdl): 0.0, 1.30925e-05

Epoch over!
epoch time: 55.509

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 643
rank avg (pred): 0.387 +- 0.274
mrr vals (pred, true): 0.281, 0.047
batch losses (mrrl, rdl): 0.0, 4.97351e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 674
rank avg (pred): 0.384 +- 0.258
mrr vals (pred, true): 0.235, 0.049
batch losses (mrrl, rdl): 0.0, 5.76027e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 314
rank avg (pred): 0.053 +- 0.049
mrr vals (pred, true): 0.434, 0.563
batch losses (mrrl, rdl): 0.0, 8.9445e-06

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 934
rank avg (pred): 0.500 +- 0.266
mrr vals (pred, true): 0.107, 0.016
batch losses (mrrl, rdl): 0.0, 0.001018186

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 782
rank avg (pred): 0.525 +- 0.253
mrr vals (pred, true): 0.087, 0.074
batch losses (mrrl, rdl): 0.0, 5.44414e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 177
rank avg (pred): 0.379 +- 0.284
mrr vals (pred, true): 0.293, 0.043
batch losses (mrrl, rdl): 0.0, 3.2338e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1091
rank avg (pred): 0.341 +- 0.278
mrr vals (pred, true): 0.339, 0.096
batch losses (mrrl, rdl): 0.0, 2.6233e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1177
rank avg (pred): 0.408 +- 0.285
mrr vals (pred, true): 0.276, 0.042
batch losses (mrrl, rdl): 0.0, 1.26891e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 375
rank avg (pred): 0.340 +- 0.266
mrr vals (pred, true): 0.309, 0.124
batch losses (mrrl, rdl): 0.0, 0.000167721

Epoch over!
epoch time: 53.956

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 492
rank avg (pred): 0.246 +- 0.209
mrr vals (pred, true): 0.367, 0.185
batch losses (mrrl, rdl): 0.3313374519, 2.93794e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 345
rank avg (pred): 0.495 +- 0.201
mrr vals (pred, true): 0.067, 0.117
batch losses (mrrl, rdl): 0.0257464908, 0.0009259545

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 776
rank avg (pred): 0.308 +- 0.092
mrr vals (pred, true): 0.052, 0.046
batch losses (mrrl, rdl): 2.32976e-05, 0.000550708

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 656
rank avg (pred): 0.487 +- 0.228
mrr vals (pred, true): 0.072, 0.047
batch losses (mrrl, rdl): 0.0047678365, 9.24761e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 386
rank avg (pred): 0.409 +- 0.178
mrr vals (pred, true): 0.067, 0.101
batch losses (mrrl, rdl): 0.0118777817, 0.0002186125

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 42
rank avg (pred): 0.016 +- 0.017
mrr vals (pred, true): 0.502, 0.531
batch losses (mrrl, rdl): 0.0083515299, 1.98923e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 59
rank avg (pred): 0.014 +- 0.015
mrr vals (pred, true): 0.488, 0.525
batch losses (mrrl, rdl): 0.0139954761, 1.27112e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1197
rank avg (pred): 0.417 +- 0.162
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 7.28791e-05, 3.4261e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 348
rank avg (pred): 0.359 +- 0.140
mrr vals (pred, true): 0.061, 0.110
batch losses (mrrl, rdl): 0.0236785971, 0.0001140823

Epoch over!
epoch time: 60.432

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 551
rank avg (pred): 0.226 +- 0.131
mrr vals (pred, true): 0.110, 0.105
batch losses (mrrl, rdl): 0.0002575737, 0.0003495907

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 466
rank avg (pred): 0.314 +- 0.146
mrr vals (pred, true): 0.083, 0.054
batch losses (mrrl, rdl): 0.011201716, 0.000279974

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 629
rank avg (pred): 0.367 +- 0.165
mrr vals (pred, true): 0.056, 0.039
batch losses (mrrl, rdl): 0.000347776, 0.0002665977

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 19
rank avg (pred): 0.011 +- 0.016
mrr vals (pred, true): 0.596, 0.549
batch losses (mrrl, rdl): 0.0222638529, 1.48789e-05

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 118
rank avg (pred): 0.283 +- 0.125
mrr vals (pred, true): 0.077, 0.145
batch losses (mrrl, rdl): 0.0462197512, 8.19966e-05

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 423
rank avg (pred): 0.289 +- 0.119
mrr vals (pred, true): 0.073, 0.039
batch losses (mrrl, rdl): 0.0053589223, 0.0005526871

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 932
rank avg (pred): 0.396 +- 0.164
mrr vals (pred, true): 0.038, 0.015
batch losses (mrrl, rdl): 0.001369228, 0.002652226

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 940
rank avg (pred): 0.398 +- 0.167
mrr vals (pred, true): 0.040, 0.018
batch losses (mrrl, rdl): 0.000960101, 0.0023493594

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 675
rank avg (pred): 0.381 +- 0.155
mrr vals (pred, true): 0.042, 0.046
batch losses (mrrl, rdl): 0.0005924847, 9.09788e-05

Epoch over!
epoch time: 59.521

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 270
rank avg (pred): 0.028 +- 0.047
mrr vals (pred, true): 0.501, 0.553
batch losses (mrrl, rdl): 0.0271269027, 2.7863e-06

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 629
rank avg (pred): 0.395 +- 0.167
mrr vals (pred, true): 0.045, 0.039
batch losses (mrrl, rdl): 0.0002144103, 0.0001630301

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 717
rank avg (pred): 0.372 +- 0.177
mrr vals (pred, true): 0.055, 0.055
batch losses (mrrl, rdl): 0.0002763483, 0.0001140326

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 380
rank avg (pred): 0.279 +- 0.137
mrr vals (pred, true): 0.084, 0.082
batch losses (mrrl, rdl): 0.0116522843, 0.0001480021

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 716
rank avg (pred): 0.381 +- 0.161
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.0012897693, 0.0001266308

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 783
rank avg (pred): 0.262 +- 0.085
mrr vals (pred, true): 0.037, 0.052
batch losses (mrrl, rdl): 0.0016418343, 0.0007324172

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 915
rank avg (pred): 0.305 +- 0.163
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.001883139, 0.0004929101

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 454
rank avg (pred): 0.274 +- 0.133
mrr vals (pred, true): 0.076, 0.047
batch losses (mrrl, rdl): 0.0066979006, 0.0005804184

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 567
rank avg (pred): 0.368 +- 0.153
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 5.9632e-06, 0.0002359234

Epoch over!
epoch time: 59.193

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1046
rank avg (pred): 0.292 +- 0.120
mrr vals (pred, true): 0.063, 0.047
batch losses (mrrl, rdl): 0.0016286215, 0.0005599069

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 157
rank avg (pred): 0.281 +- 0.110
mrr vals (pred, true): 0.064, 0.102
batch losses (mrrl, rdl): 0.0150653999, 4.55052e-05

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 144
rank avg (pred): 0.277 +- 0.111
mrr vals (pred, true): 0.062, 0.130
batch losses (mrrl, rdl): 0.0465447418, 5.58394e-05

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 124
rank avg (pred): 0.267 +- 0.115
mrr vals (pred, true): 0.073, 0.088
batch losses (mrrl, rdl): 0.0050736163, 0.0001417451

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 649
rank avg (pred): 0.349 +- 0.150
mrr vals (pred, true): 0.056, 0.047
batch losses (mrrl, rdl): 0.0004170118, 0.0001800326

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1197
rank avg (pred): 0.354 +- 0.148
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 3.13e-08, 0.0001590808

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 16
rank avg (pred): 0.033 +- 0.064
mrr vals (pred, true): 0.525, 0.549
batch losses (mrrl, rdl): 0.0059328186, 2.91e-08

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 404
rank avg (pred): 0.260 +- 0.113
mrr vals (pred, true): 0.066, 0.108
batch losses (mrrl, rdl): 0.0173164103, 3.65116e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 225
rank avg (pred): 0.240 +- 0.110
mrr vals (pred, true): 0.075, 0.050
batch losses (mrrl, rdl): 0.0061490764, 0.0008471753

Epoch over!
epoch time: 56.536

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 965
rank avg (pred): 0.235 +- 0.086
mrr vals (pred, true): 0.056, 0.049
batch losses (mrrl, rdl): 0.0004200982, 0.0009877491

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1041
rank avg (pred): 0.209 +- 0.101
mrr vals (pred, true): 0.077, 0.046
batch losses (mrrl, rdl): 0.0074851643, 0.0011109727

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 189
rank avg (pred): 0.248 +- 0.104
mrr vals (pred, true): 0.062, 0.046
batch losses (mrrl, rdl): 0.0014942362, 0.0009182081

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 797
rank avg (pred): 0.177 +- 0.082
mrr vals (pred, true): 0.054, 0.048
batch losses (mrrl, rdl): 0.0001427866, 0.001445358

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 628
rank avg (pred): 0.304 +- 0.147
mrr vals (pred, true): 0.062, 0.047
batch losses (mrrl, rdl): 0.0013433985, 0.0004807368

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1068
rank avg (pred): 0.021 +- 0.051
mrr vals (pred, true): 0.604, 0.532
batch losses (mrrl, rdl): 0.051395189, 4.8126e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1159
rank avg (pred): 0.170 +- 0.143
mrr vals (pred, true): 0.159, 0.127
batch losses (mrrl, rdl): 0.0106297573, 0.0002023272

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 877
rank avg (pred): 0.255 +- 0.098
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0001496692, 0.0008206094

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 88
rank avg (pred): 0.230 +- 0.115
mrr vals (pred, true): 0.073, 0.084
batch losses (mrrl, rdl): 0.005145424, 0.0002899428

Epoch over!
epoch time: 56.715

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 984
rank avg (pred): 0.060 +- 0.093
mrr vals (pred, true): 0.486, 0.556
batch losses (mrrl, rdl): 0.0489434265, 1.92657e-05

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 912
rank avg (pred): 0.203 +- 0.142
mrr vals (pred, true): 0.083, 0.057
batch losses (mrrl, rdl): 0.010939897, 0.0009995066

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1121
rank avg (pred): 0.259 +- 0.133
mrr vals (pred, true): 0.071, 0.052
batch losses (mrrl, rdl): 0.0044635693, 0.0007398531

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 689
rank avg (pred): 0.305 +- 0.130
mrr vals (pred, true): 0.056, 0.044
batch losses (mrrl, rdl): 0.0003897949, 0.0004263521

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 431
rank avg (pred): 0.244 +- 0.148
mrr vals (pred, true): 0.078, 0.042
batch losses (mrrl, rdl): 0.0077491337, 0.0008776258

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 827
rank avg (pred): 0.135 +- 0.180
mrr vals (pred, true): 0.473, 0.243
batch losses (mrrl, rdl): 0.5300155878, 2.2185e-06

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 350
rank avg (pred): 0.203 +- 0.168
mrr vals (pred, true): 0.100, 0.105
batch losses (mrrl, rdl): 0.0003497649, 0.0002377821

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1079
rank avg (pred): 0.056 +- 0.086
mrr vals (pred, true): 0.562, 0.554
batch losses (mrrl, rdl): 0.0006069384, 1.31881e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 996
rank avg (pred): 0.103 +- 0.128
mrr vals (pred, true): 0.507, 0.557
batch losses (mrrl, rdl): 0.0249727797, 0.0001355525

Epoch over!
epoch time: 58.348

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 635
rank avg (pred): 0.308 +- 0.105
mrr vals (pred, true): 0.045, 0.038
batch losses (mrrl, rdl): 0.0002611247, 0.0006747487

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 467
rank avg (pred): 0.258 +- 0.140
mrr vals (pred, true): 0.061, 0.050
batch losses (mrrl, rdl): 0.0012462088, 0.0005468249

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 312
rank avg (pred): 0.109 +- 0.139
mrr vals (pred, true): 0.513, 0.542
batch losses (mrrl, rdl): 0.00805555, 0.0001296792

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 243
rank avg (pred): 0.095 +- 0.115
mrr vals (pred, true): 0.521, 0.561
batch losses (mrrl, rdl): 0.0158773717, 0.0001099989

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1045
rank avg (pred): 0.229 +- 0.140
mrr vals (pred, true): 0.072, 0.049
batch losses (mrrl, rdl): 0.004884168, 0.0009059497

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 972
rank avg (pred): 0.100 +- 0.125
mrr vals (pred, true): 0.570, 0.514
batch losses (mrrl, rdl): 0.0308332406, 7.78501e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 21
rank avg (pred): 0.114 +- 0.144
mrr vals (pred, true): 0.557, 0.549
batch losses (mrrl, rdl): 0.0005463798, 0.0001595135

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 372
rank avg (pred): 0.222 +- 0.149
mrr vals (pred, true): 0.084, 0.105
batch losses (mrrl, rdl): 0.0043101255, 7.63916e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1000
rank avg (pred): 0.233 +- 0.150
mrr vals (pred, true): 0.073, 0.086
batch losses (mrrl, rdl): 0.0051287194, 0.0002690762

Epoch over!
epoch time: 57.839

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 545
rank avg (pred): 0.252 +- 0.134
mrr vals (pred, true): 0.101, 0.123
batch losses (mrrl, rdl): 0.0049525071, 0.0001645727

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1171
rank avg (pred): 0.301 +- 0.094
mrr vals (pred, true): 0.044, 0.050
batch losses (mrrl, rdl): 0.0003596001, 0.0006109861

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1203
rank avg (pred): 0.292 +- 0.104
mrr vals (pred, true): 0.047, 0.051
batch losses (mrrl, rdl): 0.0001146047, 0.0004940009

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 292
rank avg (pred): 0.127 +- 0.159
mrr vals (pred, true): 0.538, 0.544
batch losses (mrrl, rdl): 0.0003929859, 0.0002293952

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 951
rank avg (pred): 0.307 +- 0.104
mrr vals (pred, true): 0.029, 0.048
batch losses (mrrl, rdl): 0.0046102749, 0.0005774958

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 912
rank avg (pred): 0.232 +- 0.147
mrr vals (pred, true): 0.066, 0.057
batch losses (mrrl, rdl): 0.00271425, 0.0007655788

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1173
rank avg (pred): 0.289 +- 0.100
mrr vals (pred, true): 0.050, 0.040
batch losses (mrrl, rdl): 6e-10, 0.0009083647

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 366
rank avg (pred): 0.231 +- 0.140
mrr vals (pred, true): 0.075, 0.130
batch losses (mrrl, rdl): 0.0300926361, 9.12296e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 934
rank avg (pred): 0.271 +- 0.118
mrr vals (pred, true): 0.040, 0.016
batch losses (mrrl, rdl): 0.0009651881, 0.004671033

Epoch over!
epoch time: 58.46

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 845
rank avg (pred): 0.333 +- 0.091
mrr vals (pred, true): 0.026, 0.072
batch losses (mrrl, rdl): 0.0055777403, 0.0007105351

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 721
rank avg (pred): 0.294 +- 0.087
mrr vals (pred, true): 0.039, 0.051
batch losses (mrrl, rdl): 0.0012955769, 0.0004918027

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1214
rank avg (pred): 0.307 +- 0.121
mrr vals (pred, true): 0.058, 0.049
batch losses (mrrl, rdl): 0.0006046304, 0.00040488

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 811
rank avg (pred): 0.173 +- 0.171
mrr vals (pred, true): 0.298, 0.265
batch losses (mrrl, rdl): 0.0106067713, 0.0001950306

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 94
rank avg (pred): 0.212 +- 0.131
mrr vals (pred, true): 0.083, 0.122
batch losses (mrrl, rdl): 0.0156000331, 0.0001240969

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 621
rank avg (pred): 0.306 +- 0.096
mrr vals (pred, true): 0.038, 0.037
batch losses (mrrl, rdl): 0.0014258723, 0.0008158408

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 55
rank avg (pred): 0.106 +- 0.142
mrr vals (pred, true): 0.507, 0.517
batch losses (mrrl, rdl): 0.0009897368, 0.0001247777

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 166
rank avg (pred): 0.209 +- 0.128
mrr vals (pred, true): 0.085, 0.046
batch losses (mrrl, rdl): 0.0121698044, 0.0010308857

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 148
rank avg (pred): 0.234 +- 0.123
mrr vals (pred, true): 0.070, 0.111
batch losses (mrrl, rdl): 0.0167148635, 0.0001329037

Epoch over!
epoch time: 57.176

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 829
rank avg (pred): 0.118 +- 0.143
mrr vals (pred, true): 0.450, 0.532
batch losses (mrrl, rdl): 0.0680743232, 0.0001713836

running batch: 500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1201
rank avg (pred): 0.344 +- 0.169
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 2.5426e-05, 0.0001695705

running batch: 1000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 216
rank avg (pred): 0.212 +- 0.122
mrr vals (pred, true): 0.080, 0.043
batch losses (mrrl, rdl): 0.0092927301, 0.001128173

running batch: 1500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 119
rank avg (pred): 0.216 +- 0.116
mrr vals (pred, true): 0.073, 0.116
batch losses (mrrl, rdl): 0.0185874365, 0.0001181593

running batch: 2000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 868
rank avg (pred): 0.252 +- 0.106
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003519883, 0.0008134639

running batch: 2500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 814
rank avg (pred): 0.203 +- 0.138
mrr vals (pred, true): 0.115, 0.145
batch losses (mrrl, rdl): 0.0090975603, 9.93706e-05

running batch: 3000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 502
rank avg (pred): 0.267 +- 0.206
mrr vals (pred, true): 0.246, 0.185
batch losses (mrrl, rdl): 0.0378365368, 3.15266e-05

running batch: 3500 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 833
rank avg (pred): 0.102 +- 0.137
mrr vals (pred, true): 0.521, 0.469
batch losses (mrrl, rdl): 0.0266542826, 6.83971e-05

running batch: 4000 / 4376 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 104
rank avg (pred): 0.209 +- 0.115
mrr vals (pred, true): 0.084, 0.118
batch losses (mrrl, rdl): 0.0115641765, 0.0001315124

Epoch over!
epoch time: 59.077

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.310 +- 0.054
mrr vals (pred, true): 0.025, 0.049

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.02978 	 0.01574 	 ~...
    0 	     1 	 0.02243 	 0.01602 	 ~...
    9 	     2 	 0.03007 	 0.01662 	 ~...
    7 	     3 	 0.02899 	 0.01697 	 ~...
   11 	     4 	 0.03275 	 0.02063 	 ~...
    6 	     5 	 0.02845 	 0.02125 	 ~...
   12 	     6 	 0.03289 	 0.02127 	 ~...
   35 	     7 	 0.04466 	 0.02942 	 ~...
   14 	     8 	 0.03408 	 0.03457 	 ~...
   21 	     9 	 0.04128 	 0.03469 	 ~...
   31 	    10 	 0.04377 	 0.03543 	 ~...
   38 	    11 	 0.04692 	 0.03661 	 ~...
   15 	    12 	 0.03903 	 0.03679 	 ~...
   20 	    13 	 0.04103 	 0.03707 	 ~...
   84 	    14 	 0.07632 	 0.03861 	 m..s
   25 	    15 	 0.04214 	 0.03872 	 ~...
   29 	    16 	 0.04235 	 0.04008 	 ~...
   39 	    17 	 0.04704 	 0.04022 	 ~...
   47 	    18 	 0.07176 	 0.04080 	 m..s
   23 	    19 	 0.04169 	 0.04155 	 ~...
   24 	    20 	 0.04177 	 0.04155 	 ~...
   28 	    21 	 0.04234 	 0.04176 	 ~...
   47 	    22 	 0.07176 	 0.04204 	 ~...
   47 	    23 	 0.07176 	 0.04205 	 ~...
   47 	    24 	 0.07176 	 0.04214 	 ~...
   32 	    25 	 0.04411 	 0.04279 	 ~...
   17 	    26 	 0.03995 	 0.04302 	 ~...
   22 	    27 	 0.04144 	 0.04329 	 ~...
   47 	    28 	 0.07176 	 0.04377 	 ~...
    1 	    29 	 0.02244 	 0.04452 	 ~...
    4 	    30 	 0.02635 	 0.04460 	 ~...
   27 	    31 	 0.04232 	 0.04493 	 ~...
   89 	    32 	 0.07832 	 0.04531 	 m..s
   47 	    33 	 0.07176 	 0.04549 	 ~...
    5 	    34 	 0.02833 	 0.04555 	 ~...
    2 	    35 	 0.02476 	 0.04562 	 ~...
   34 	    36 	 0.04464 	 0.04581 	 ~...
   47 	    37 	 0.07176 	 0.04582 	 ~...
   83 	    38 	 0.07480 	 0.04604 	 ~...
   30 	    39 	 0.04250 	 0.04612 	 ~...
   19 	    40 	 0.04099 	 0.04665 	 ~...
   93 	    41 	 0.08066 	 0.04682 	 m..s
   47 	    42 	 0.07176 	 0.04704 	 ~...
   47 	    43 	 0.07176 	 0.04754 	 ~...
   16 	    44 	 0.03956 	 0.04775 	 ~...
   47 	    45 	 0.07176 	 0.04840 	 ~...
   33 	    46 	 0.04424 	 0.04851 	 ~...
   37 	    47 	 0.04519 	 0.04856 	 ~...
    3 	    48 	 0.02523 	 0.04860 	 ~...
   92 	    49 	 0.07947 	 0.04879 	 m..s
   36 	    50 	 0.04515 	 0.04954 	 ~...
   10 	    51 	 0.03022 	 0.04966 	 ~...
   85 	    52 	 0.07654 	 0.04972 	 ~...
   18 	    53 	 0.04097 	 0.05000 	 ~...
   26 	    54 	 0.04214 	 0.05069 	 ~...
   41 	    55 	 0.05008 	 0.05095 	 ~...
   47 	    56 	 0.07176 	 0.05147 	 ~...
   47 	    57 	 0.07176 	 0.05161 	 ~...
   13 	    58 	 0.03373 	 0.05178 	 ~...
   88 	    59 	 0.07816 	 0.05234 	 ~...
   47 	    60 	 0.07176 	 0.05305 	 ~...
   47 	    61 	 0.07176 	 0.05348 	 ~...
   90 	    62 	 0.07867 	 0.05443 	 ~...
   42 	    63 	 0.05597 	 0.05461 	 ~...
   91 	    64 	 0.07883 	 0.05562 	 ~...
   40 	    65 	 0.04937 	 0.05732 	 ~...
   47 	    66 	 0.07176 	 0.05734 	 ~...
   94 	    67 	 0.08268 	 0.07486 	 ~...
   47 	    68 	 0.07176 	 0.07982 	 ~...
   47 	    69 	 0.07176 	 0.08021 	 ~...
   44 	    70 	 0.06732 	 0.08233 	 ~...
   47 	    71 	 0.07176 	 0.08319 	 ~...
   47 	    72 	 0.07176 	 0.08597 	 ~...
   47 	    73 	 0.07176 	 0.09081 	 ~...
   47 	    74 	 0.07176 	 0.09087 	 ~...
   47 	    75 	 0.07176 	 0.09419 	 ~...
   47 	    76 	 0.07176 	 0.09529 	 ~...
   47 	    77 	 0.07176 	 0.09795 	 ~...
   47 	    78 	 0.07176 	 0.10026 	 ~...
   45 	    79 	 0.06895 	 0.10034 	 m..s
   47 	    80 	 0.07176 	 0.10125 	 ~...
   47 	    81 	 0.07176 	 0.10756 	 m..s
   87 	    82 	 0.07815 	 0.11191 	 m..s
   47 	    83 	 0.07176 	 0.11219 	 m..s
   47 	    84 	 0.07176 	 0.11517 	 m..s
   47 	    85 	 0.07176 	 0.11661 	 m..s
   86 	    86 	 0.07733 	 0.12400 	 m..s
   82 	    87 	 0.07327 	 0.13246 	 m..s
   47 	    88 	 0.07176 	 0.13355 	 m..s
   43 	    89 	 0.05750 	 0.13486 	 m..s
   46 	    90 	 0.07150 	 0.13495 	 m..s
   47 	    91 	 0.07176 	 0.14099 	 m..s
   81 	    92 	 0.07318 	 0.14266 	 m..s
   47 	    93 	 0.07176 	 0.14361 	 m..s
   47 	    94 	 0.07176 	 0.16228 	 m..s
   97 	    95 	 0.15795 	 0.17970 	 ~...
   96 	    96 	 0.15479 	 0.21282 	 m..s
   98 	    97 	 0.17283 	 0.24196 	 m..s
   95 	    98 	 0.11659 	 0.30827 	 MISS
   99 	    99 	 0.45256 	 0.42624 	 ~...
  100 	   100 	 0.50023 	 0.50782 	 ~...
  102 	   101 	 0.50099 	 0.50916 	 ~...
  113 	   102 	 0.52480 	 0.51598 	 ~...
  101 	   103 	 0.50074 	 0.52807 	 ~...
  107 	   104 	 0.51552 	 0.53116 	 ~...
  104 	   105 	 0.50659 	 0.53137 	 ~...
  105 	   106 	 0.50740 	 0.53464 	 ~...
  103 	   107 	 0.50448 	 0.53485 	 m..s
  110 	   108 	 0.51948 	 0.53601 	 ~...
  106 	   109 	 0.51458 	 0.53995 	 ~...
  116 	   110 	 0.52585 	 0.54412 	 ~...
  118 	   111 	 0.52675 	 0.54677 	 ~...
  120 	   112 	 0.53204 	 0.54738 	 ~...
  109 	   113 	 0.51822 	 0.54829 	 m..s
  111 	   114 	 0.52107 	 0.55070 	 ~...
  117 	   115 	 0.52590 	 0.55245 	 ~...
  112 	   116 	 0.52372 	 0.55334 	 ~...
  108 	   117 	 0.51755 	 0.55712 	 m..s
  119 	   118 	 0.53016 	 0.55884 	 ~...
  114 	   119 	 0.52532 	 0.56143 	 m..s
  115 	   120 	 0.52571 	 0.56755 	 m..s
==========================================
r_mrr = 0.9855075478553772
r2_mrr = 0.9667706489562988
spearmanr_mrr@5 = 0.9354026913642883
spearmanr_mrr@10 = 0.9353633522987366
spearmanr_mrr@50 = 0.9942426681518555
spearmanr_mrr@100 = 0.9894096851348877
spearmanr_mrr@All = 0.9901740550994873
==========================================
test time: 0.48
Done Testing dataset UMLS
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.288 +- 0.089
mrr vals (pred, true): 0.049, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   24 	     0 	 0.05049 	 5e-0500 	 m..s
    9 	     1 	 0.04648 	 5e-0500 	 m..s
    1 	     2 	 0.02563 	 0.00010 	 ~...
    5 	     3 	 0.04306 	 0.00011 	 m..s
   45 	     4 	 0.09810 	 0.00015 	 m..s
    8 	     5 	 0.04572 	 0.00016 	 m..s
   85 	     6 	 0.09822 	 0.00016 	 m..s
   35 	     7 	 0.09545 	 0.00017 	 m..s
   45 	     8 	 0.09810 	 0.00018 	 m..s
   45 	     9 	 0.09810 	 0.00018 	 m..s
   40 	    10 	 0.09765 	 0.00018 	 m..s
   15 	    11 	 0.04922 	 0.00019 	 m..s
    3 	    12 	 0.04289 	 0.00019 	 m..s
   91 	    13 	 0.09912 	 0.00019 	 m..s
   45 	    14 	 0.09810 	 0.00020 	 m..s
   30 	    15 	 0.09265 	 0.00021 	 m..s
   90 	    16 	 0.09905 	 0.00022 	 m..s
   43 	    17 	 0.09785 	 0.00022 	 m..s
   92 	    18 	 0.09928 	 0.00022 	 m..s
   14 	    19 	 0.04921 	 0.00023 	 m..s
   45 	    20 	 0.09810 	 0.00024 	 m..s
   23 	    21 	 0.05047 	 0.00024 	 m..s
   20 	    22 	 0.04933 	 0.00025 	 m..s
   81 	    23 	 0.09818 	 0.00026 	 m..s
   45 	    24 	 0.09810 	 0.00026 	 m..s
   10 	    25 	 0.04888 	 0.00027 	 m..s
   45 	    26 	 0.09810 	 0.00027 	 m..s
   12 	    27 	 0.04914 	 0.00027 	 m..s
   45 	    28 	 0.09810 	 0.00029 	 m..s
   11 	    29 	 0.04890 	 0.00029 	 m..s
   93 	    30 	 0.09961 	 0.00030 	 m..s
   45 	    31 	 0.09810 	 0.00031 	 m..s
   45 	    32 	 0.09810 	 0.00031 	 m..s
    0 	    33 	 0.02557 	 0.00033 	 ~...
   45 	    34 	 0.09810 	 0.00033 	 m..s
   45 	    35 	 0.09810 	 0.00035 	 m..s
   79 	    36 	 0.09815 	 0.00036 	 m..s
   33 	    37 	 0.09515 	 0.00036 	 m..s
   86 	    38 	 0.09825 	 0.00038 	 m..s
   80 	    39 	 0.09818 	 0.00040 	 m..s
   45 	    40 	 0.09810 	 0.00042 	 m..s
    2 	    41 	 0.04276 	 0.00044 	 m..s
   45 	    42 	 0.09810 	 0.00044 	 m..s
   22 	    43 	 0.04947 	 0.00046 	 m..s
   17 	    44 	 0.04927 	 0.00046 	 m..s
   16 	    45 	 0.04922 	 0.00050 	 m..s
    6 	    46 	 0.04414 	 0.00054 	 m..s
   13 	    47 	 0.04918 	 0.00072 	 m..s
   45 	    48 	 0.09810 	 0.00117 	 m..s
   18 	    49 	 0.04930 	 0.00179 	 m..s
   29 	    50 	 0.09264 	 0.00257 	 m..s
    4 	    51 	 0.04304 	 0.00375 	 m..s
    7 	    52 	 0.04454 	 0.00554 	 m..s
   21 	    53 	 0.04937 	 0.00980 	 m..s
   19 	    54 	 0.04930 	 0.01742 	 m..s
   25 	    55 	 0.05744 	 0.02819 	 ~...
   27 	    56 	 0.09098 	 0.11626 	 ~...
   32 	    57 	 0.09367 	 0.11876 	 ~...
   26 	    58 	 0.08926 	 0.11961 	 m..s
   31 	    59 	 0.09288 	 0.12252 	 ~...
   89 	    60 	 0.09903 	 0.12785 	 ~...
   87 	    61 	 0.09828 	 0.13015 	 m..s
   45 	    62 	 0.09810 	 0.13345 	 m..s
   28 	    63 	 0.09231 	 0.13414 	 m..s
   45 	    64 	 0.09810 	 0.13618 	 m..s
   34 	    65 	 0.09537 	 0.13748 	 m..s
   42 	    66 	 0.09778 	 0.13749 	 m..s
   45 	    67 	 0.09810 	 0.13898 	 m..s
   45 	    68 	 0.09810 	 0.14093 	 m..s
   45 	    69 	 0.09810 	 0.14414 	 m..s
   83 	    70 	 0.09821 	 0.14432 	 m..s
   37 	    71 	 0.09734 	 0.14549 	 m..s
   45 	    72 	 0.09810 	 0.14995 	 m..s
   44 	    73 	 0.09787 	 0.15197 	 m..s
   88 	    74 	 0.09860 	 0.15343 	 m..s
   41 	    75 	 0.09766 	 0.15523 	 m..s
   36 	    76 	 0.09719 	 0.15541 	 m..s
   45 	    77 	 0.09810 	 0.15705 	 m..s
   45 	    78 	 0.09810 	 0.15909 	 m..s
   45 	    79 	 0.09810 	 0.15928 	 m..s
   45 	    80 	 0.09810 	 0.16009 	 m..s
   95 	    81 	 0.11800 	 0.16436 	 m..s
   45 	    82 	 0.09810 	 0.16456 	 m..s
   45 	    83 	 0.09810 	 0.16562 	 m..s
   39 	    84 	 0.09764 	 0.16652 	 m..s
   45 	    85 	 0.09810 	 0.16799 	 m..s
   45 	    86 	 0.09810 	 0.16809 	 m..s
  103 	    87 	 0.17362 	 0.16947 	 ~...
   45 	    88 	 0.09810 	 0.16980 	 m..s
   45 	    89 	 0.09810 	 0.17409 	 m..s
   84 	    90 	 0.09821 	 0.17434 	 m..s
   45 	    91 	 0.09810 	 0.17552 	 m..s
   82 	    92 	 0.09820 	 0.17565 	 m..s
   45 	    93 	 0.09810 	 0.17646 	 m..s
   45 	    94 	 0.09810 	 0.17849 	 m..s
   94 	    95 	 0.11654 	 0.17963 	 m..s
   97 	    96 	 0.14524 	 0.19494 	 m..s
   98 	    97 	 0.15975 	 0.19952 	 m..s
   38 	    98 	 0.09752 	 0.20620 	 MISS
  106 	    99 	 0.19958 	 0.21099 	 ~...
  101 	   100 	 0.16907 	 0.21250 	 m..s
  100 	   101 	 0.16467 	 0.21272 	 m..s
   99 	   102 	 0.16125 	 0.22441 	 m..s
   96 	   103 	 0.14178 	 0.23497 	 m..s
  111 	   104 	 0.23828 	 0.25474 	 ~...
  105 	   105 	 0.19459 	 0.25673 	 m..s
  102 	   106 	 0.17063 	 0.25950 	 m..s
  114 	   107 	 0.27788 	 0.26345 	 ~...
  110 	   108 	 0.23374 	 0.27311 	 m..s
  109 	   109 	 0.22756 	 0.27477 	 m..s
  108 	   110 	 0.21391 	 0.27768 	 m..s
  107 	   111 	 0.21330 	 0.28984 	 m..s
  113 	   112 	 0.24904 	 0.30005 	 m..s
  104 	   113 	 0.18441 	 0.31337 	 MISS
  112 	   114 	 0.23969 	 0.32199 	 m..s
  115 	   115 	 0.28563 	 0.32433 	 m..s
  119 	   116 	 0.31191 	 0.32678 	 ~...
  120 	   117 	 0.31921 	 0.34832 	 ~...
  116 	   118 	 0.29726 	 0.35651 	 m..s
  118 	   119 	 0.30923 	 0.35940 	 m..s
  117 	   120 	 0.30782 	 0.37136 	 m..s
==========================================
r_mrr = 0.8292691111564636
r2_mrr = 0.6282882690429688
spearmanr_mrr@5 = 0.9893549084663391
spearmanr_mrr@10 = 0.9478150010108948
spearmanr_mrr@50 = 0.9923614263534546
spearmanr_mrr@100 = 0.8163031935691833
spearmanr_mrr@All = 0.8537136316299438
==========================================
test time: 0.463
Done Testing dataset DBpedia50
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.287 +- 0.087
mrr vals (pred, true): 0.032, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   13 	     0 	 0.04316 	 0.00073 	 m..s
    7 	     1 	 0.03663 	 0.00073 	 m..s
   10 	     2 	 0.04127 	 0.00075 	 m..s
    1 	     3 	 0.02171 	 0.00075 	 ~...
    9 	     4 	 0.03986 	 0.00076 	 m..s
   23 	     5 	 0.05707 	 0.00116 	 m..s
   25 	     6 	 0.07818 	 0.00196 	 m..s
   24 	     7 	 0.05713 	 0.00219 	 m..s
   32 	     8 	 0.08622 	 0.00277 	 m..s
   45 	     9 	 0.10821 	 0.00327 	 MISS
   45 	    10 	 0.10821 	 0.00344 	 MISS
   86 	    11 	 0.10920 	 0.00355 	 MISS
   45 	    12 	 0.10821 	 0.00359 	 MISS
   79 	    13 	 0.10859 	 0.00359 	 MISS
   28 	    14 	 0.08390 	 0.00363 	 m..s
   15 	    15 	 0.04320 	 0.00367 	 m..s
   45 	    16 	 0.10821 	 0.00371 	 MISS
   34 	    17 	 0.08659 	 0.00371 	 m..s
   81 	    18 	 0.10878 	 0.00372 	 MISS
   12 	    19 	 0.04295 	 0.00373 	 m..s
   45 	    20 	 0.10821 	 0.00379 	 MISS
   91 	    21 	 0.11172 	 0.00382 	 MISS
    4 	    22 	 0.03180 	 0.00388 	 ~...
   92 	    23 	 0.11215 	 0.00392 	 MISS
    3 	    24 	 0.03122 	 0.00395 	 ~...
   45 	    25 	 0.10821 	 0.00398 	 MISS
    5 	    26 	 0.03357 	 0.00399 	 ~...
   40 	    27 	 0.10081 	 0.00400 	 m..s
    8 	    28 	 0.03955 	 0.00403 	 m..s
    6 	    29 	 0.03646 	 0.00403 	 m..s
   45 	    30 	 0.10821 	 0.00416 	 MISS
   45 	    31 	 0.10821 	 0.00419 	 MISS
   45 	    32 	 0.10821 	 0.00424 	 MISS
   20 	    33 	 0.04724 	 0.00427 	 m..s
   45 	    34 	 0.10821 	 0.00429 	 MISS
   43 	    35 	 0.10536 	 0.00434 	 MISS
   14 	    36 	 0.04316 	 0.00438 	 m..s
   45 	    37 	 0.10821 	 0.00441 	 MISS
   45 	    38 	 0.10821 	 0.00443 	 MISS
   85 	    39 	 0.10902 	 0.00452 	 MISS
   90 	    40 	 0.11149 	 0.00462 	 MISS
   21 	    41 	 0.04804 	 0.00467 	 m..s
   45 	    42 	 0.10821 	 0.00471 	 MISS
   80 	    43 	 0.10876 	 0.00483 	 MISS
   89 	    44 	 0.11136 	 0.00493 	 MISS
   45 	    45 	 0.10821 	 0.00497 	 MISS
   16 	    46 	 0.04325 	 0.00498 	 m..s
    0 	    47 	 0.02166 	 0.00504 	 ~...
   19 	    48 	 0.04491 	 0.00505 	 m..s
   29 	    49 	 0.08390 	 0.00530 	 m..s
   45 	    50 	 0.10821 	 0.00586 	 MISS
   11 	    51 	 0.04177 	 0.00616 	 m..s
    2 	    52 	 0.02400 	 0.02133 	 ~...
   17 	    53 	 0.04333 	 0.03453 	 ~...
   93 	    54 	 0.11532 	 0.03628 	 m..s
   95 	    55 	 0.11921 	 0.03927 	 m..s
   22 	    56 	 0.04872 	 0.04165 	 ~...
   96 	    57 	 0.11965 	 0.05472 	 m..s
   94 	    58 	 0.11626 	 0.05615 	 m..s
   18 	    59 	 0.04373 	 0.05984 	 ~...
   27 	    60 	 0.08376 	 0.09914 	 ~...
   26 	    61 	 0.08219 	 0.12053 	 m..s
   41 	    62 	 0.10108 	 0.13488 	 m..s
   31 	    63 	 0.08464 	 0.14256 	 m..s
   30 	    64 	 0.08406 	 0.14601 	 m..s
   39 	    65 	 0.10017 	 0.14724 	 m..s
   98 	    66 	 0.14932 	 0.14733 	 ~...
   42 	    67 	 0.10409 	 0.14938 	 m..s
   36 	    68 	 0.09246 	 0.15508 	 m..s
   35 	    69 	 0.08760 	 0.15698 	 m..s
   99 	    70 	 0.16255 	 0.15748 	 ~...
   44 	    71 	 0.10557 	 0.15958 	 m..s
   97 	    72 	 0.14046 	 0.16040 	 ~...
   33 	    73 	 0.08646 	 0.16404 	 m..s
   37 	    74 	 0.09387 	 0.16654 	 m..s
   45 	    75 	 0.10821 	 0.17926 	 m..s
  100 	    76 	 0.19530 	 0.18169 	 ~...
   87 	    77 	 0.10939 	 0.18252 	 m..s
   83 	    78 	 0.10887 	 0.18579 	 m..s
   45 	    79 	 0.10821 	 0.18639 	 m..s
   45 	    80 	 0.10821 	 0.18699 	 m..s
   45 	    81 	 0.10821 	 0.18798 	 m..s
   88 	    82 	 0.11040 	 0.18944 	 m..s
   45 	    83 	 0.10821 	 0.19231 	 m..s
   45 	    84 	 0.10821 	 0.19245 	 m..s
  104 	    85 	 0.20440 	 0.19708 	 ~...
  107 	    86 	 0.22134 	 0.20056 	 ~...
   45 	    87 	 0.10821 	 0.20079 	 m..s
   45 	    88 	 0.10821 	 0.20433 	 m..s
  105 	    89 	 0.20583 	 0.20499 	 ~...
  101 	    90 	 0.19570 	 0.20824 	 ~...
  102 	    91 	 0.20051 	 0.20874 	 ~...
   45 	    92 	 0.10821 	 0.21238 	 MISS
   45 	    93 	 0.10821 	 0.21293 	 MISS
   45 	    94 	 0.10821 	 0.21372 	 MISS
   45 	    95 	 0.10821 	 0.21545 	 MISS
  106 	    96 	 0.21265 	 0.21580 	 ~...
   45 	    97 	 0.10821 	 0.21671 	 MISS
   45 	    98 	 0.10821 	 0.21770 	 MISS
   84 	    99 	 0.10895 	 0.21878 	 MISS
   45 	   100 	 0.10821 	 0.21993 	 MISS
  119 	   101 	 0.25473 	 0.22001 	 m..s
  114 	   102 	 0.24431 	 0.22067 	 ~...
   45 	   103 	 0.10821 	 0.22550 	 MISS
  118 	   104 	 0.25377 	 0.22713 	 ~...
  103 	   105 	 0.20083 	 0.22879 	 ~...
   82 	   106 	 0.10883 	 0.22917 	 MISS
   45 	   107 	 0.10821 	 0.23779 	 MISS
  115 	   108 	 0.24783 	 0.24029 	 ~...
   45 	   109 	 0.10821 	 0.24087 	 MISS
   45 	   110 	 0.10821 	 0.24180 	 MISS
   38 	   111 	 0.09694 	 0.24858 	 MISS
  110 	   112 	 0.23275 	 0.25083 	 ~...
  116 	   113 	 0.24989 	 0.25263 	 ~...
  111 	   114 	 0.23365 	 0.25434 	 ~...
  108 	   115 	 0.22823 	 0.26518 	 m..s
  112 	   116 	 0.24182 	 0.28516 	 m..s
  113 	   117 	 0.24225 	 0.29500 	 m..s
  109 	   118 	 0.23030 	 0.29861 	 m..s
  117 	   119 	 0.25112 	 0.30466 	 m..s
  120 	   120 	 0.25572 	 0.31924 	 m..s
==========================================
r_mrr = 0.6942089796066284
r2_mrr = 0.4660564064979553
spearmanr_mrr@5 = 0.9239424467086792
spearmanr_mrr@10 = 0.9158241748809814
spearmanr_mrr@50 = 0.8795173168182373
spearmanr_mrr@100 = 0.7458508014678955
spearmanr_mrr@All = 0.8067644238471985
==========================================
test time: 0.493
Done Testing dataset CoDExSmall
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.322 +- 0.048
mrr vals (pred, true): 0.030, 0.060

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   24 	     0 	 0.08568 	 0.02078 	 m..s
    8 	     1 	 0.03252 	 0.04507 	 ~...
    2 	     2 	 0.02956 	 0.04618 	 ~...
   42 	     3 	 0.10015 	 0.04664 	 m..s
   42 	     4 	 0.10015 	 0.04757 	 m..s
   42 	     5 	 0.10015 	 0.04768 	 m..s
   14 	     6 	 0.03740 	 0.04779 	 ~...
   42 	     7 	 0.10015 	 0.04853 	 m..s
   21 	     8 	 0.04375 	 0.04858 	 ~...
   42 	     9 	 0.10015 	 0.04899 	 m..s
   18 	    10 	 0.03991 	 0.04927 	 ~...
    7 	    11 	 0.03231 	 0.04945 	 ~...
   10 	    12 	 0.03435 	 0.04958 	 ~...
    4 	    13 	 0.02962 	 0.04997 	 ~...
   11 	    14 	 0.03480 	 0.04997 	 ~...
   42 	    15 	 0.10015 	 0.04999 	 m..s
   42 	    16 	 0.10015 	 0.05071 	 m..s
   15 	    17 	 0.03744 	 0.05088 	 ~...
   28 	    18 	 0.09484 	 0.05090 	 m..s
   37 	    19 	 0.09620 	 0.05093 	 m..s
   85 	    20 	 0.10754 	 0.05099 	 m..s
   34 	    21 	 0.09539 	 0.05101 	 m..s
    9 	    22 	 0.03327 	 0.05126 	 ~...
   42 	    23 	 0.10015 	 0.05130 	 m..s
   80 	    24 	 0.10578 	 0.05138 	 m..s
   13 	    25 	 0.03687 	 0.05252 	 ~...
   27 	    26 	 0.09483 	 0.05309 	 m..s
    3 	    27 	 0.02956 	 0.05331 	 ~...
   81 	    28 	 0.10606 	 0.05333 	 m..s
   42 	    29 	 0.10015 	 0.05336 	 m..s
   12 	    30 	 0.03490 	 0.05341 	 ~...
   88 	    31 	 0.11205 	 0.05353 	 m..s
   42 	    32 	 0.10015 	 0.05395 	 m..s
   22 	    33 	 0.04568 	 0.05428 	 ~...
    5 	    34 	 0.02995 	 0.05429 	 ~...
   17 	    35 	 0.03969 	 0.05432 	 ~...
   20 	    36 	 0.04176 	 0.05470 	 ~...
   87 	    37 	 0.10985 	 0.05513 	 m..s
   83 	    38 	 0.10665 	 0.05533 	 m..s
    6 	    39 	 0.03116 	 0.05552 	 ~...
   79 	    40 	 0.10455 	 0.05570 	 m..s
   19 	    41 	 0.04162 	 0.05573 	 ~...
   32 	    42 	 0.09511 	 0.05574 	 m..s
   84 	    43 	 0.10695 	 0.05605 	 m..s
   82 	    44 	 0.10657 	 0.05612 	 m..s
   42 	    45 	 0.10015 	 0.05746 	 m..s
   42 	    46 	 0.10015 	 0.05760 	 m..s
   39 	    47 	 0.09636 	 0.05789 	 m..s
   42 	    48 	 0.10015 	 0.05843 	 m..s
    1 	    49 	 0.02954 	 0.05879 	 ~...
   16 	    50 	 0.03859 	 0.05920 	 ~...
   42 	    51 	 0.10015 	 0.05955 	 m..s
    0 	    52 	 0.02954 	 0.06025 	 m..s
   42 	    53 	 0.10015 	 0.06190 	 m..s
   90 	    54 	 0.15602 	 0.10886 	 m..s
   42 	    55 	 0.10015 	 0.18207 	 m..s
   29 	    56 	 0.09485 	 0.18802 	 m..s
   42 	    57 	 0.10015 	 0.19116 	 m..s
   78 	    58 	 0.10383 	 0.20089 	 m..s
   33 	    59 	 0.09538 	 0.20275 	 MISS
   42 	    60 	 0.10015 	 0.20332 	 MISS
   23 	    61 	 0.08492 	 0.20925 	 MISS
   41 	    62 	 0.09676 	 0.21450 	 MISS
   35 	    63 	 0.09591 	 0.21466 	 MISS
   25 	    64 	 0.09408 	 0.21624 	 MISS
   42 	    65 	 0.10015 	 0.21670 	 MISS
   42 	    66 	 0.10015 	 0.21942 	 MISS
   30 	    67 	 0.09506 	 0.21967 	 MISS
   42 	    68 	 0.10015 	 0.22190 	 MISS
   89 	    69 	 0.11440 	 0.22497 	 MISS
   42 	    70 	 0.10015 	 0.22593 	 MISS
   42 	    71 	 0.10015 	 0.22982 	 MISS
   42 	    72 	 0.10015 	 0.23242 	 MISS
   31 	    73 	 0.09508 	 0.23373 	 MISS
   38 	    74 	 0.09634 	 0.23711 	 MISS
   86 	    75 	 0.10902 	 0.23790 	 MISS
   42 	    76 	 0.10015 	 0.23872 	 MISS
   77 	    77 	 0.10118 	 0.24013 	 MISS
   40 	    78 	 0.09664 	 0.24084 	 MISS
   42 	    79 	 0.10015 	 0.24098 	 MISS
   42 	    80 	 0.10015 	 0.24298 	 MISS
   42 	    81 	 0.10015 	 0.24474 	 MISS
   42 	    82 	 0.10015 	 0.24548 	 MISS
   42 	    83 	 0.10015 	 0.24764 	 MISS
   26 	    84 	 0.09469 	 0.24880 	 MISS
   42 	    85 	 0.10015 	 0.25163 	 MISS
   42 	    86 	 0.10015 	 0.25213 	 MISS
   42 	    87 	 0.10015 	 0.25361 	 MISS
   42 	    88 	 0.10015 	 0.25896 	 MISS
   36 	    89 	 0.09620 	 0.26151 	 MISS
   76 	    90 	 0.10112 	 0.26309 	 MISS
   99 	    91 	 0.37196 	 0.35525 	 ~...
   97 	    92 	 0.34604 	 0.36106 	 ~...
   91 	    93 	 0.16038 	 0.36815 	 MISS
   93 	    94 	 0.32574 	 0.37045 	 m..s
   94 	    95 	 0.32743 	 0.37192 	 m..s
   95 	    96 	 0.33044 	 0.38333 	 m..s
   92 	    97 	 0.24308 	 0.38496 	 MISS
   98 	    98 	 0.34722 	 0.39610 	 m..s
   96 	    99 	 0.33140 	 0.40923 	 m..s
  113 	   100 	 0.42502 	 0.42358 	 ~...
  120 	   101 	 0.44136 	 0.42698 	 ~...
  100 	   102 	 0.40734 	 0.42863 	 ~...
  107 	   103 	 0.41865 	 0.42881 	 ~...
  109 	   104 	 0.41989 	 0.42947 	 ~...
  118 	   105 	 0.43245 	 0.43250 	 ~...
  108 	   106 	 0.41894 	 0.43359 	 ~...
  105 	   107 	 0.41576 	 0.43446 	 ~...
  110 	   108 	 0.42184 	 0.43650 	 ~...
  111 	   109 	 0.42204 	 0.44010 	 ~...
  115 	   110 	 0.42967 	 0.44133 	 ~...
  106 	   111 	 0.41609 	 0.44232 	 ~...
  102 	   112 	 0.41375 	 0.44327 	 ~...
  104 	   113 	 0.41419 	 0.44432 	 m..s
  101 	   114 	 0.41040 	 0.44688 	 m..s
  119 	   115 	 0.43746 	 0.45085 	 ~...
  116 	   116 	 0.43112 	 0.45208 	 ~...
  117 	   117 	 0.43118 	 0.45312 	 ~...
  114 	   118 	 0.42953 	 0.45465 	 ~...
  103 	   119 	 0.41408 	 0.46434 	 m..s
  112 	   120 	 0.42252 	 0.47042 	 m..s
==========================================
r_mrr = 0.8786956667900085
r2_mrr = 0.7089787125587463
spearmanr_mrr@5 = 0.9983305335044861
spearmanr_mrr@10 = 0.9746884703636169
spearmanr_mrr@50 = 0.9888745546340942
spearmanr_mrr@100 = 0.8752292394638062
spearmanr_mrr@All = 0.8980651497840881
==========================================
test time: 0.512
Done Testing dataset Kinships
total time taken: 886.1349542140961
training time taken: 855.6093533039093
TWIG out ;))
====================================================
----------------------------------------------------
Running a TWIG experiment with tag: TransE-omit-UMLS
----------------------------------------------------
====================================================
Using random seed: 4897399437227077
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [815, 138, 1111, 780, 513, 231, 251, 817, 939, 557, 449, 415, 92, 744, 723, 53, 406, 919, 536, 740, 674, 694, 612, 291, 152, 1201, 1157, 1055, 216, 1153, 1142, 351, 252, 23, 368, 1096, 1147, 980, 539, 823, 288, 1095, 28, 917, 362, 530, 308, 71, 668, 369, 62, 399, 1088, 534, 184, 720, 140, 1010, 689, 121, 719, 1145, 515, 572, 1129, 593, 244, 1070, 631, 432, 466, 790, 866, 56, 590, 181, 1212, 836, 565, 911, 200, 100, 403, 858, 350, 517, 1089, 380, 50, 761, 367, 789, 700, 656, 1086, 345, 1196, 1046, 1097, 169, 820, 754, 519, 856, 770, 289, 702, 950, 730, 204, 523, 1071, 564, 438, 752, 979, 710, 802, 339, 440, 607]
valid_ids (0): []
train_ids (1094): [872, 738, 177, 533, 1124, 222, 235, 622, 1154, 446, 58, 667, 1197, 1084, 265, 786, 801, 182, 774, 899, 1184, 113, 839, 1037, 61, 894, 825, 463, 282, 993, 857, 426, 577, 949, 296, 103, 79, 124, 1170, 417, 303, 313, 148, 371, 264, 750, 278, 885, 805, 880, 924, 122, 327, 961, 318, 1204, 404, 55, 887, 293, 372, 749, 139, 87, 948, 520, 721, 496, 614, 274, 645, 460, 366, 773, 1083, 144, 365, 134, 437, 561, 431, 247, 1081, 717, 500, 392, 159, 741, 743, 865, 558, 493, 800, 1143, 1198, 467, 320, 39, 205, 476, 732, 693, 1185, 867, 66, 108, 510, 273, 860, 841, 986, 442, 1195, 1116, 884, 1011, 709, 713, 1080, 1017, 1009, 224, 981, 868, 711, 306, 976, 1128, 988, 1012, 672, 771, 609, 191, 84, 1177, 1182, 798, 1193, 158, 683, 1136, 143, 81, 640, 232, 602, 314, 358, 846, 553, 955, 414, 12, 501, 870, 647, 356, 471, 794, 125, 1045, 692, 705, 173, 1063, 229, 163, 424, 1034, 964, 1085, 1078, 843, 677, 1021, 346, 653, 119, 370, 1099, 150, 862, 544, 330, 1062, 408, 978, 678, 652, 942, 766, 1053, 237, 599, 1206, 834, 43, 718, 971, 241, 1160, 502, 1051, 575, 542, 135, 828, 585, 485, 101, 482, 347, 230, 176, 412, 413, 902, 934, 861, 1065, 782, 484, 30, 1115, 691, 546, 223, 960, 171, 573, 716, 1156, 280, 1138, 277, 527, 188, 1203, 676, 1041, 20, 506, 1191, 1158, 915, 864, 455, 1146, 206, 1007, 387, 199, 824, 1028, 428, 418, 419, 742, 175, 385, 254, 529, 379, 384, 566, 562, 172, 509, 1, 624, 395, 888, 161, 965, 991, 1114, 123, 77, 646, 359, 145, 458, 487, 129, 583, 75, 1014, 765, 203, 42, 128, 1118, 863, 281, 433, 522, 210, 263, 312, 1178, 315, 14, 326, 556, 443, 1187, 528, 649, 937, 616, 755, 1027, 600, 287, 410, 568, 1110, 57, 871, 165, 1044, 611, 323, 1200, 85, 968, 109, 355, 661, 98, 735, 731, 821, 1001, 634, 9, 390, 389, 420, 316, 745, 258, 246, 555, 987, 456, 552, 250, 525, 918, 1031, 436, 120, 974, 733, 559, 488, 444, 82, 938, 588, 877, 543, 829, 724, 5, 900, 453, 1135, 895, 257, 969, 999, 239, 570, 441, 927, 650, 928, 405, 268, 1106, 891, 1039, 957, 837, 592, 168, 722, 571, 804, 294, 348, 335, 186, 72, 554, 272, 325, 267, 107, 68, 869, 833, 286, 299, 1015, 41, 1006, 1033, 317, 1137, 512, 142, 1029, 984, 388, 799, 629, 1036, 492, 457, 941, 59, 427, 1202, 727, 298, 40, 86, 784, 913, 34, 975, 758, 91, 97, 1209, 65, 518, 256, 192, 808, 1068, 183, 22, 893, 1213, 1087, 1091, 1159, 297, 617, 569, 715, 377, 96, 541, 255, 3, 400, 819, 430, 1038, 578, 416, 655, 342, 1150, 1077, 1205, 37, 341, 464, 454, 1105, 343, 725, 684, 892, 213, 881, 842, 332, 1140, 548, 137, 378, 1144, 233, 907, 1120, 967, 89, 322, 947, 707, 373, 0, 340, 270, 106, 921, 753, 1174, 1122, 673, 882, 1181, 945, 615, 337, 115, 209, 932, 779, 1098, 1040, 697, 1016, 219, 1112, 618, 598, 503, 772, 1113, 60, 217, 331, 16, 302, 167, 149, 67, 242, 1119, 703, 202, 1060, 698, 1002, 234, 243, 1192, 757, 681, 52, 73, 117, 1004, 1047, 1023, 398, 1165, 1183, 651, 276, 586, 935, 929, 822, 956, 596, 1094, 178, 1100, 551, 483, 793, 748, 897, 666, 468, 74, 1104, 1172, 791, 818, 922, 1092, 363, 962, 472, 728, 879, 11, 80, 1125, 381, 1126, 549, 411, 916, 734, 806, 636, 17, 1121, 290, 1048, 847, 376, 174, 215, 589, 659, 505, 708, 249, 187, 480, 197, 963, 490, 931, 936, 1133, 660, 470, 155, 1052, 514, 507, 1134, 662, 136, 429, 635, 669, 1155, 982, 751, 591, 382, 1210, 360, 912, 619, 344, 516, 35, 658, 901, 671, 985, 51, 1024, 809, 1079, 1151, 1152, 574, 952, 304, 1005, 319, 153, 29, 357, 445, 157, 531, 1188, 248, 425, 781, 386, 1103, 776, 364, 284, 307, 1035, 8, 788, 494, 45, 620, 211, 105, 253, 953, 954, 110, 285, 910, 193, 787, 1176, 654, 491, 194, 1076, 657, 461, 21, 759, 597, 682, 292, 1003, 94, 695, 769, 853, 998, 116, 816, 421, 1018, 1072, 1139, 958, 63, 112, 777, 792, 944, 859, 391, 305, 688, 127, 604, 459, 271, 729, 875, 1175, 826, 300, 196, 361, 95, 160, 279, 739, 462, 164, 146, 475, 64, 545, 626, 114, 851, 69, 989, 737, 1043, 1075, 1025, 803, 587, 845, 189, 796, 909, 1057, 1186, 26, 852, 603, 259, 336, 321, 946, 567, 795, 352, 524, 27, 1207, 434, 481, 687, 876, 873, 452, 896, 266, 890, 1127, 1199, 1020, 848, 511, 154, 813, 580, 1123, 301, 422, 1141, 19, 435, 1168, 992, 1050, 811, 396, 402, 1022, 156, 221, 394, 497, 240, 118, 18, 1073, 983, 354, 625, 10, 1042, 973, 2, 245, 1148, 638, 409, 201, 104, 762, 349, 1171, 1074, 70, 147, 473, 838, 353, 633, 810, 849, 479, 1008, 195, 54, 990, 639, 874, 47, 329, 49, 775, 220, 90, 995, 898, 1161, 579, 1032, 889, 295, 663, 827, 972, 465, 576, 831, 644, 36, 141, 1056, 1054, 477, 1162, 111, 943, 46, 448, 225, 190, 131, 581, 1000, 605, 664, 1117, 33, 375, 474, 1059, 88, 76, 1013, 923, 906, 7, 685, 627, 451, 959, 478, 1058, 44, 797, 538, 1173, 6, 812, 966, 807, 997, 706, 499, 238, 328, 162, 764, 594, 712, 489, 1082, 132, 401, 1180, 908, 690, 1214, 714, 208, 486, 642, 696, 783, 1061, 1093, 648, 778, 886, 855, 260, 310, 632, 78, 878, 768, 608, 1066, 1064, 756, 1049, 15, 498, 1167, 32, 99, 504, 423, 680, 180, 996, 218, 1019, 130, 397, 563, 508, 93, 726, 1130, 925, 940, 1107, 275, 675, 535, 595, 1166, 309, 606, 198, 641, 835, 970, 747, 637, 1026, 333, 704, 31, 904, 1131, 1030, 686, 1194, 665, 623, 830, 166, 311, 227, 1090, 628, 102, 450, 407, 179, 699, 170, 584, 526, 582, 262, 1069, 185, 540, 930, 226, 840, 439, 532, 736, 1101, 83, 1102, 763, 1179, 613, 48, 447, 994, 1132, 926, 951, 13, 25, 1109, 1189, 760, 844, 151, 469, 495, 610, 324, 643, 1108, 236, 338, 914, 126, 1149, 521, 214, 24, 1067, 547, 679, 746, 374, 560, 905, 261, 630, 903, 977, 832, 212, 207, 1190, 283, 1163, 38, 393, 701, 383, 4, 883, 920, 1164, 133, 814, 850, 854, 1169, 228, 670, 550, 933, 537, 621, 785, 767, 269, 1211, 601, 334, 1208]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8055265427463218
the save name prefix for this run is:  chkpt-ID_8055265427463218_tag_TransE-omit-UMLS
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 182
rank avg (pred): 0.572 +- 0.002
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.000273553

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 353
rank avg (pred): 0.370 +- 0.148
mrr vals (pred, true): 0.000, 0.109
batch losses (mrrl, rdl): 0.0, 0.0006255595

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.099 +- 0.059
mrr vals (pred, true): 0.020, 0.134
batch losses (mrrl, rdl): 0.0, 2.94704e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.357 +- 0.268
mrr vals (pred, true): 0.114, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001519434

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 8
rank avg (pred): 0.069 +- 0.062
mrr vals (pred, true): 0.125, 0.120
batch losses (mrrl, rdl): 0.0, 2.08662e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1081
rank avg (pred): 0.335 +- 0.303
mrr vals (pred, true): 0.095, 0.170
batch losses (mrrl, rdl): 0.0, 0.0006814678

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.051 +- 0.049
mrr vals (pred, true): 0.161, 0.123
batch losses (mrrl, rdl): 0.0, 5.33755e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 132
rank avg (pred): 0.356 +- 0.295
mrr vals (pred, true): 0.116, 0.012
batch losses (mrrl, rdl): 0.0, 0.0002931322

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 166
rank avg (pred): 0.367 +- 0.308
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0, 0.000210166

Epoch over!
epoch time: 54.068

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 663
rank avg (pred): 0.360 +- 0.312
mrr vals (pred, true): 0.144, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003042402

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 511
rank avg (pred): 0.197 +- 0.212
mrr vals (pred, true): 0.204, 0.056
batch losses (mrrl, rdl): 0.0, 0.0001434883

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 27
rank avg (pred): 0.092 +- 0.091
mrr vals (pred, true): 0.157, 0.028
batch losses (mrrl, rdl): 0.0, 3.28573e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 264
rank avg (pred): 0.075 +- 0.074
mrr vals (pred, true): 0.147, 0.095
batch losses (mrrl, rdl): 0.0, 6.2152e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 423
rank avg (pred): 0.303 +- 0.286
mrr vals (pred, true): 0.169, 0.000
batch losses (mrrl, rdl): 0.0, 0.0006161029

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 392
rank avg (pred): 0.307 +- 0.295
mrr vals (pred, true): 0.201, 0.129
batch losses (mrrl, rdl): 0.0, 0.0004165955

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 447
rank avg (pred): 0.314 +- 0.296
mrr vals (pred, true): 0.224, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002988343

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 788
rank avg (pred): 0.306 +- 0.308
mrr vals (pred, true): 0.197, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002836153

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1207
rank avg (pred): 0.354 +- 0.358
mrr vals (pred, true): 0.205, 0.000
batch losses (mrrl, rdl): 0.0, 0.000125813

Epoch over!
epoch time: 54.022

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 9
rank avg (pred): 0.121 +- 0.122
mrr vals (pred, true): 0.182, 0.082
batch losses (mrrl, rdl): 0.0, 3.15778e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 392
rank avg (pred): 0.310 +- 0.312
mrr vals (pred, true): 0.184, 0.129
batch losses (mrrl, rdl): 0.0, 0.0004478509

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 431
rank avg (pred): 0.318 +- 0.309
mrr vals (pred, true): 0.167, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004060726

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 922
rank avg (pred): 0.320 +- 0.305
mrr vals (pred, true): 0.136, 0.150
batch losses (mrrl, rdl): 0.0, 0.0003885158

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 929
rank avg (pred): 0.314 +- 0.306
mrr vals (pred, true): 0.175, 0.158
batch losses (mrrl, rdl): 0.0, 0.0003668688

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 837
rank avg (pred): 0.313 +- 0.307
mrr vals (pred, true): 0.124, 0.176
batch losses (mrrl, rdl): 0.0, 0.0002788305

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1087
rank avg (pred): 0.316 +- 0.295
mrr vals (pred, true): 0.155, 0.199
batch losses (mrrl, rdl): 0.0, 0.0005788875

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 532
rank avg (pred): 0.235 +- 0.256
mrr vals (pred, true): 0.168, 0.053
batch losses (mrrl, rdl): 0.0, 2.68966e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1012
rank avg (pred): 0.328 +- 0.293
mrr vals (pred, true): 0.159, 0.167
batch losses (mrrl, rdl): 0.0, 0.000556547

Epoch over!
epoch time: 55.038

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 725
rank avg (pred): 0.393 +- 0.338
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0, 6.11457e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 326
rank avg (pred): 0.314 +- 0.314
mrr vals (pred, true): 0.134, 0.110
batch losses (mrrl, rdl): 0.0, 0.0002806763

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 654
rank avg (pred): 0.367 +- 0.342
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001645888

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 488
rank avg (pred): 0.214 +- 0.232
mrr vals (pred, true): 0.165, 0.094
batch losses (mrrl, rdl): 0.0, 7.67067e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 352
rank avg (pred): 0.338 +- 0.304
mrr vals (pred, true): 0.114, 0.066
batch losses (mrrl, rdl): 0.0, 0.000272736

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 914
rank avg (pred): 0.106 +- 0.103
mrr vals (pred, true): 0.151, 0.145
batch losses (mrrl, rdl): 0.0, 3.36287e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 781
rank avg (pred): 0.303 +- 0.301
mrr vals (pred, true): 0.118, 0.164
batch losses (mrrl, rdl): 0.0, 0.0004452018

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 737
rank avg (pred): 0.089 +- 0.087
mrr vals (pred, true): 0.138, 0.085
batch losses (mrrl, rdl): 0.0, 4.68259e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 474
rank avg (pred): 0.311 +- 0.308
mrr vals (pred, true): 0.173, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002075705

Epoch over!
epoch time: 54.655

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 354
rank avg (pred): 0.325 +- 0.309
mrr vals (pred, true): 0.126, 0.034
batch losses (mrrl, rdl): 0.0, 0.0001920164

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 359
rank avg (pred): 0.314 +- 0.302
mrr vals (pred, true): 0.167, 0.111
batch losses (mrrl, rdl): 0.0, 0.0002804106

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.358 +- 0.348
mrr vals (pred, true): 0.144, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002406274

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.342 +- 0.284
mrr vals (pred, true): 0.112, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002299033

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 48
rank avg (pred): 0.067 +- 0.071
mrr vals (pred, true): 0.172, 0.120
batch losses (mrrl, rdl): 0.0, 3.54664e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 237
rank avg (pred): 0.356 +- 0.301
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002324564

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 728
rank avg (pred): 0.352 +- 0.346
mrr vals (pred, true): 0.164, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002260464

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.324 +- 0.292
mrr vals (pred, true): 0.140, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001958559

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 958
rank avg (pred): 0.328 +- 0.296
mrr vals (pred, true): 0.126, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003486241

Epoch over!
epoch time: 52.633

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.094 +- 0.108
mrr vals (pred, true): 0.180, 0.077
batch losses (mrrl, rdl): 0.1696463227, 6.124e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.363 +- 0.296
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 9.5759e-05, 0.0001042195

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1206
rank avg (pred): 0.341 +- 0.292
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0069961888, 0.0003271446

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 485
rank avg (pred): 0.274 +- 0.255
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0151663003, 0.0004471593

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 888
rank avg (pred): 0.278 +- 0.259
mrr vals (pred, true): 0.084, 0.000
batch losses (mrrl, rdl): 0.0112684853, 0.000631321

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 809
rank avg (pred): 0.269 +- 0.254
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 9.99825e-05, 0.000717408

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.059 +- 0.064
mrr vals (pred, true): 0.083, 0.072
batch losses (mrrl, rdl): 0.0108898943, 9.75842e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 317
rank avg (pred): 0.038 +- 0.048
mrr vals (pred, true): 0.146, 0.180
batch losses (mrrl, rdl): 0.0114442166, 4.34862e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 667
rank avg (pred): 0.274 +- 0.269
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0067673693, 0.0007331347

Epoch over!
epoch time: 58.384

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 757
rank avg (pred): 0.266 +- 0.269
mrr vals (pred, true): 0.077, 0.163
batch losses (mrrl, rdl): 0.0739542022, 0.0001063868

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 481
rank avg (pred): 0.290 +- 0.264
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.84763e-05, 0.0005070299

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 148
rank avg (pred): 0.285 +- 0.264
mrr vals (pred, true): 0.058, 0.025
batch losses (mrrl, rdl): 0.0005737709, 5.21662e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1119
rank avg (pred): 0.223 +- 0.266
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0190599039, 0.0011959615

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 893
rank avg (pred): 0.185 +- 0.257
mrr vals (pred, true): 0.101, 0.088
batch losses (mrrl, rdl): 0.0264110249, 0.0001434001

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 965
rank avg (pred): 0.257 +- 0.261
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.000385134, 0.0008972658

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 15
rank avg (pred): 0.131 +- 0.199
mrr vals (pred, true): 0.119, 0.093
batch losses (mrrl, rdl): 0.0470694341, 1.6812e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.220 +- 0.258
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0020859204, 0.0010088276

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 971
rank avg (pred): 0.201 +- 0.252
mrr vals (pred, true): 0.069, 0.001
batch losses (mrrl, rdl): 0.0034706169, 0.0010353767

Epoch over!
epoch time: 58.529

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 732
rank avg (pred): 0.166 +- 0.238
mrr vals (pred, true): 0.077, 0.091
batch losses (mrrl, rdl): 0.0075356709, 0.0001501863

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 861
rank avg (pred): 0.203 +- 0.243
mrr vals (pred, true): 0.069, 0.170
batch losses (mrrl, rdl): 0.1006236225, 1.38536e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 657
rank avg (pred): 0.256 +- 0.236
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004418729, 0.0006727552

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1169
rank avg (pred): 0.258 +- 0.234
mrr vals (pred, true): 0.053, 0.070
batch losses (mrrl, rdl): 8.24437e-05, 2.61935e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 661
rank avg (pred): 0.254 +- 0.233
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0017642906, 0.000432735

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 66
rank avg (pred): 0.106 +- 0.172
mrr vals (pred, true): 0.102, 0.052
batch losses (mrrl, rdl): 0.0269831903, 1.33688e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1028
rank avg (pred): 0.186 +- 0.234
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0111093642, 0.001122069

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 940
rank avg (pred): 0.192 +- 0.232
mrr vals (pred, true): 0.077, 0.172
batch losses (mrrl, rdl): 0.0897533372, 2.06786e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 116
rank avg (pred): 0.206 +- 0.228
mrr vals (pred, true): 0.062, 0.056
batch losses (mrrl, rdl): 0.0013320958, 2.18113e-05

Epoch over!
epoch time: 59.628

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 473
rank avg (pred): 0.212 +- 0.224
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.75771e-05, 0.0011354717

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1100
rank avg (pred): 0.179 +- 0.225
mrr vals (pred, true): 0.074, 0.154
batch losses (mrrl, rdl): 0.0644278079, 3.68558e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 319
rank avg (pred): 0.068 +- 0.111
mrr vals (pred, true): 0.129, 0.114
batch losses (mrrl, rdl): 0.0020231858, 5.31582e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 876
rank avg (pred): 0.173 +- 0.220
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0040436117, 0.0014566159

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 424
rank avg (pred): 0.200 +- 0.220
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0013541844, 0.0009963722

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 441
rank avg (pred): 0.237 +- 0.222
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.87115e-05, 0.0006680206

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 662
rank avg (pred): 0.205 +- 0.214
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022617669, 0.0011651099

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 567
rank avg (pred): 0.200 +- 0.207
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.002199708, 0.0003807821

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 733
rank avg (pred): 0.131 +- 0.197
mrr vals (pred, true): 0.088, 0.089
batch losses (mrrl, rdl): 0.0146570057, 4.65858e-05

Epoch over!
epoch time: 58.292

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1119
rank avg (pred): 0.165 +- 0.211
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0088162003, 0.0018811222

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1011
rank avg (pred): 0.176 +- 0.209
mrr vals (pred, true): 0.067, 0.187
batch losses (mrrl, rdl): 0.1456767768, 1.69074e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 426
rank avg (pred): 0.168 +- 0.206
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0064334385, 0.0018238816

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 444
rank avg (pred): 0.168 +- 0.206
mrr vals (pred, true): 0.071, 0.000
batch losses (mrrl, rdl): 0.0042555472, 0.0017821378

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.181 +- 0.198
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002996416, 0.0017016139

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.156 +- 0.206
mrr vals (pred, true): 0.076, 0.191
batch losses (mrrl, rdl): 0.130467087, 8.7967e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 749
rank avg (pred): 0.088 +- 0.140
mrr vals (pred, true): 0.114, 0.149
batch losses (mrrl, rdl): 0.011841394, 1.04738e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1148
rank avg (pred): 0.090 +- 0.141
mrr vals (pred, true): 0.101, 0.088
batch losses (mrrl, rdl): 0.0257425867, 0.0003103085

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.056 +- 0.089
mrr vals (pred, true): 0.123, 0.098
batch losses (mrrl, rdl): 0.0535620302, 0.0005018339

Epoch over!
epoch time: 56.399

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 555
rank avg (pred): 0.131 +- 0.194
mrr vals (pred, true): 0.093, 0.029
batch losses (mrrl, rdl): 0.0184942428, 0.0003333666

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 488
rank avg (pred): 0.125 +- 0.189
mrr vals (pred, true): 0.088, 0.094
batch losses (mrrl, rdl): 0.014755059, 0.0004172569

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 616
rank avg (pred): 0.164 +- 0.190
mrr vals (pred, true): 0.070, 0.005
batch losses (mrrl, rdl): 0.0038813322, 0.0004917249

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 357
rank avg (pred): 0.159 +- 0.192
mrr vals (pred, true): 0.065, 0.037
batch losses (mrrl, rdl): 0.0021757085, 0.0001907254

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 202
rank avg (pred): 0.176 +- 0.190
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.3671e-06, 0.001799808

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 340
rank avg (pred): 0.155 +- 0.192
mrr vals (pred, true): 0.064, 0.052
batch losses (mrrl, rdl): 0.0019966511, 1.94773e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 279
rank avg (pred): 0.109 +- 0.169
mrr vals (pred, true): 0.088, 0.085
batch losses (mrrl, rdl): 0.0147907911, 1.76924e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 672
rank avg (pred): 0.182 +- 0.180
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.36277e-05, 0.0016363885

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1036
rank avg (pred): 0.138 +- 0.193
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.01011327, 0.0023021086

Epoch over!
epoch time: 55.376

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.159 +- 0.190
mrr vals (pred, true): 0.069, 0.072
batch losses (mrrl, rdl): 0.0036596241, 0.0003509974

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 547
rank avg (pred): 0.145 +- 0.191
mrr vals (pred, true): 0.095, 0.059
batch losses (mrrl, rdl): 0.0201730058, 0.0002366608

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.054 +- 0.081
mrr vals (pred, true): 0.168, 0.204
batch losses (mrrl, rdl): 0.0126280989, 1.87315e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 413
rank avg (pred): 0.164 +- 0.197
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006117226, 0.0015865257

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1194
rank avg (pred): 0.175 +- 0.190
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0024279938, 0.00209108

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1197
rank avg (pred): 0.173 +- 0.190
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0032762391, 0.0012483947

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.108 +- 0.152
mrr vals (pred, true): 0.103, 0.041
batch losses (mrrl, rdl): 0.0279126093, 0.0003965512

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.083 +- 0.124
mrr vals (pred, true): 0.129, 0.128
batch losses (mrrl, rdl): 8.7766e-06, 1.35248e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1065
rank avg (pred): 0.046 +- 0.094
mrr vals (pred, true): 0.220, 0.287
batch losses (mrrl, rdl): 0.0455062464, 1.06145e-05

Epoch over!
epoch time: 55.864

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 561
rank avg (pred): 0.139 +- 0.182
mrr vals (pred, true): 0.078, 0.027
batch losses (mrrl, rdl): 0.0080863079, 0.0003205399

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 58
rank avg (pred): 0.104 +- 0.144
mrr vals (pred, true): 0.102, 0.067
batch losses (mrrl, rdl): 0.0271377489, 3.29817e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1134
rank avg (pred): 0.118 +- 0.160
mrr vals (pred, true): 0.097, 0.097
batch losses (mrrl, rdl): 0.0224157795, 0.000411654

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 46
rank avg (pred): 0.091 +- 0.130
mrr vals (pred, true): 0.104, 0.124
batch losses (mrrl, rdl): 0.0043237638, 1.10164e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1122
rank avg (pred): 0.152 +- 0.187
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.0055172238, 0.0021087322

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 899
rank avg (pred): 0.139 +- 0.178
mrr vals (pred, true): 0.070, 0.107
batch losses (mrrl, rdl): 0.0136281047, 4.33818e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 509
rank avg (pred): 0.104 +- 0.146
mrr vals (pred, true): 0.097, 0.098
batch losses (mrrl, rdl): 0.022384122, 0.0005961786

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.159 +- 0.182
mrr vals (pred, true): 0.073, 0.001
batch losses (mrrl, rdl): 0.0054508112, 0.0022280614

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1203
rank avg (pred): 0.170 +- 0.172
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0022705714, 0.0017203905

Epoch over!
epoch time: 56.918

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 899
rank avg (pred): 0.134 +- 0.173
mrr vals (pred, true): 0.086, 0.107
batch losses (mrrl, rdl): 0.0041448716, 3.62056e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 107
rank avg (pred): 0.154 +- 0.176
mrr vals (pred, true): 0.064, 0.068
batch losses (mrrl, rdl): 0.0019704453, 7.37769e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 357
rank avg (pred): 0.157 +- 0.172
mrr vals (pred, true): 0.069, 0.037
batch losses (mrrl, rdl): 0.0035877405, 0.000219626

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 773
rank avg (pred): 0.150 +- 0.177
mrr vals (pred, true): 0.073, 0.172
batch losses (mrrl, rdl): 0.0975633264, 6.52969e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 667
rank avg (pred): 0.163 +- 0.174
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0030897334, 0.0019280261

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 178
rank avg (pred): 0.155 +- 0.171
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.99653e-05, 0.00207403

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 338
rank avg (pred): 0.147 +- 0.177
mrr vals (pred, true): 0.073, 0.127
batch losses (mrrl, rdl): 0.0291878618, 4.10443e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 751
rank avg (pred): 0.086 +- 0.129
mrr vals (pred, true): 0.105, 0.205
batch losses (mrrl, rdl): 0.1014591008, 1.71777e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 142
rank avg (pred): 0.162 +- 0.170
mrr vals (pred, true): 0.073, 0.016
batch losses (mrrl, rdl): 0.0054230872, 0.000166933

Epoch over!
epoch time: 57.084

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 628
rank avg (pred): 0.167 +- 0.172
mrr vals (pred, true): 0.072, 0.003
batch losses (mrrl, rdl): 0.0049850065, 0.0002697602

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 909
rank avg (pred): 0.126 +- 0.157
mrr vals (pred, true): 0.081, 0.095
batch losses (mrrl, rdl): 0.0094860345, 3.15527e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1059
rank avg (pred): 0.060 +- 0.124
mrr vals (pred, true): 0.271, 0.310
batch losses (mrrl, rdl): 0.0151948743, 1.46678e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.152 +- 0.172
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0017287144, 0.0017792402

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 614
rank avg (pred): 0.167 +- 0.161
mrr vals (pred, true): 0.069, 0.039
batch losses (mrrl, rdl): 0.0035428044, 0.0002745231

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 118
rank avg (pred): 0.174 +- 0.164
mrr vals (pred, true): 0.063, 0.019
batch losses (mrrl, rdl): 0.0017754694, 0.0001175724

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 55
rank avg (pred): 0.147 +- 0.167
mrr vals (pred, true): 0.120, 0.056
batch losses (mrrl, rdl): 0.0496296212, 6.17914e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 141
rank avg (pred): 0.184 +- 0.156
mrr vals (pred, true): 0.056, 0.011
batch losses (mrrl, rdl): 0.0003844606, 9.4716e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 421
rank avg (pred): 0.180 +- 0.160
mrr vals (pred, true): 0.068, 0.001
batch losses (mrrl, rdl): 0.0033718648, 0.0019893297

Epoch over!
epoch time: 56.472

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.161 +- 0.170
mrr vals (pred, true): 0.087, 0.085

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.06836 	 0.00019 	 m..s
   67 	     1 	 0.07381 	 0.00019 	 m..s
    0 	     2 	 0.06836 	 0.00021 	 m..s
    0 	     3 	 0.06836 	 0.00022 	 m..s
   48 	     4 	 0.07151 	 0.00022 	 m..s
   16 	     5 	 0.06845 	 0.00023 	 m..s
    0 	     6 	 0.06836 	 0.00023 	 m..s
    0 	     7 	 0.06836 	 0.00028 	 m..s
   57 	     8 	 0.07225 	 0.00028 	 m..s
   52 	     9 	 0.07192 	 0.00029 	 m..s
   25 	    10 	 0.06935 	 0.00029 	 m..s
    0 	    11 	 0.06836 	 0.00029 	 m..s
   65 	    12 	 0.07346 	 0.00030 	 m..s
   27 	    13 	 0.06968 	 0.00030 	 m..s
    0 	    14 	 0.06836 	 0.00032 	 m..s
   38 	    15 	 0.07049 	 0.00032 	 m..s
   29 	    16 	 0.06980 	 0.00033 	 m..s
   39 	    17 	 0.07049 	 0.00034 	 m..s
    0 	    18 	 0.06836 	 0.00035 	 m..s
   17 	    19 	 0.06867 	 0.00037 	 m..s
   41 	    20 	 0.07067 	 0.00039 	 m..s
   34 	    21 	 0.07037 	 0.00044 	 m..s
   54 	    22 	 0.07211 	 0.00045 	 m..s
    0 	    23 	 0.06836 	 0.00045 	 m..s
   43 	    24 	 0.07105 	 0.00046 	 m..s
   32 	    25 	 0.06999 	 0.00048 	 m..s
   21 	    26 	 0.06892 	 0.00050 	 m..s
    0 	    27 	 0.06836 	 0.00050 	 m..s
   44 	    28 	 0.07106 	 0.00053 	 m..s
   19 	    29 	 0.06876 	 0.00054 	 m..s
   62 	    30 	 0.07337 	 0.00054 	 m..s
   40 	    31 	 0.07050 	 0.00055 	 m..s
   22 	    32 	 0.06924 	 0.00071 	 m..s
   24 	    33 	 0.06929 	 0.00089 	 m..s
   20 	    34 	 0.06892 	 0.00111 	 m..s
   33 	    35 	 0.07017 	 0.00171 	 m..s
    0 	    36 	 0.06836 	 0.00217 	 m..s
    0 	    37 	 0.06836 	 0.00908 	 m..s
   18 	    38 	 0.06871 	 0.01117 	 m..s
    0 	    39 	 0.06836 	 0.01300 	 m..s
   28 	    40 	 0.06980 	 0.01678 	 m..s
   81 	    41 	 0.09262 	 0.02036 	 m..s
   42 	    42 	 0.07075 	 0.02510 	 m..s
   74 	    43 	 0.08735 	 0.02767 	 m..s
   30 	    44 	 0.06987 	 0.02825 	 m..s
   76 	    45 	 0.08766 	 0.03064 	 m..s
    0 	    46 	 0.06836 	 0.03157 	 m..s
   45 	    47 	 0.07107 	 0.03184 	 m..s
   82 	    48 	 0.09297 	 0.03482 	 m..s
   15 	    49 	 0.06842 	 0.03948 	 ~...
   78 	    50 	 0.08972 	 0.04060 	 m..s
   35 	    51 	 0.07038 	 0.04449 	 ~...
    0 	    52 	 0.06836 	 0.04655 	 ~...
   85 	    53 	 0.09603 	 0.04743 	 m..s
   23 	    54 	 0.06926 	 0.04749 	 ~...
   47 	    55 	 0.07136 	 0.05298 	 ~...
   37 	    56 	 0.07039 	 0.05453 	 ~...
   51 	    57 	 0.07189 	 0.05784 	 ~...
   79 	    58 	 0.09055 	 0.05932 	 m..s
   49 	    59 	 0.07158 	 0.06196 	 ~...
   36 	    60 	 0.07039 	 0.06900 	 ~...
   94 	    61 	 0.10509 	 0.07584 	 ~...
   93 	    62 	 0.10360 	 0.07774 	 ~...
   73 	    63 	 0.08628 	 0.08014 	 ~...
   84 	    64 	 0.09515 	 0.08120 	 ~...
   86 	    65 	 0.09679 	 0.08200 	 ~...
   87 	    66 	 0.09679 	 0.08249 	 ~...
   96 	    67 	 0.11186 	 0.08336 	 ~...
   75 	    68 	 0.08743 	 0.08498 	 ~...
   98 	    69 	 0.11368 	 0.09117 	 ~...
   97 	    70 	 0.11239 	 0.09392 	 ~...
   46 	    71 	 0.07130 	 0.09668 	 ~...
   92 	    72 	 0.10306 	 0.09750 	 ~...
   83 	    73 	 0.09470 	 0.09849 	 ~...
  108 	    74 	 0.15236 	 0.09949 	 m..s
  109 	    75 	 0.15515 	 0.10123 	 m..s
   99 	    76 	 0.11439 	 0.10160 	 ~...
   77 	    77 	 0.08880 	 0.10204 	 ~...
   95 	    78 	 0.11170 	 0.10468 	 ~...
  100 	    79 	 0.11533 	 0.10539 	 ~...
   50 	    80 	 0.07189 	 0.10673 	 m..s
  112 	    81 	 0.16275 	 0.10961 	 m..s
  106 	    82 	 0.12878 	 0.11003 	 ~...
  111 	    83 	 0.16166 	 0.11026 	 m..s
  110 	    84 	 0.15821 	 0.11095 	 m..s
  104 	    85 	 0.12079 	 0.11415 	 ~...
   55 	    86 	 0.07213 	 0.11817 	 m..s
   64 	    87 	 0.07344 	 0.12731 	 m..s
   89 	    88 	 0.10004 	 0.12878 	 ~...
  105 	    89 	 0.12124 	 0.13225 	 ~...
   66 	    90 	 0.07347 	 0.13466 	 m..s
  107 	    91 	 0.14129 	 0.13650 	 ~...
   53 	    92 	 0.07207 	 0.13857 	 m..s
   88 	    93 	 0.09848 	 0.14115 	 m..s
   63 	    94 	 0.07337 	 0.14605 	 m..s
   26 	    95 	 0.06944 	 0.14998 	 m..s
   56 	    96 	 0.07217 	 0.15482 	 m..s
   31 	    97 	 0.06999 	 0.15654 	 m..s
   80 	    98 	 0.09232 	 0.16282 	 m..s
   59 	    99 	 0.07278 	 0.16581 	 m..s
   61 	   100 	 0.07293 	 0.17136 	 m..s
   60 	   101 	 0.07289 	 0.17251 	 m..s
   58 	   102 	 0.07239 	 0.17325 	 MISS
  113 	   103 	 0.20541 	 0.17597 	 ~...
  114 	   104 	 0.20838 	 0.18278 	 ~...
  116 	   105 	 0.23129 	 0.18863 	 m..s
   72 	   106 	 0.07559 	 0.19187 	 MISS
   70 	   107 	 0.07508 	 0.19204 	 MISS
   90 	   108 	 0.10072 	 0.19286 	 m..s
   68 	   109 	 0.07490 	 0.19371 	 MISS
   71 	   110 	 0.07540 	 0.20030 	 MISS
   91 	   111 	 0.10259 	 0.20305 	 MISS
   69 	   112 	 0.07494 	 0.21796 	 MISS
  115 	   113 	 0.21639 	 0.21903 	 ~...
  103 	   114 	 0.11637 	 0.22896 	 MISS
  101 	   115 	 0.11574 	 0.22960 	 MISS
  102 	   116 	 0.11612 	 0.24065 	 MISS
  117 	   117 	 0.24786 	 0.25704 	 ~...
  119 	   118 	 0.31965 	 0.30066 	 ~...
  120 	   119 	 0.32319 	 0.31534 	 ~...
  118 	   120 	 0.30776 	 0.32098 	 ~...
==========================================
r_mrr = 0.6639695167541504
r2_mrr = 0.40650928020477295
spearmanr_mrr@5 = 0.9968221187591553
spearmanr_mrr@10 = 0.9794027209281921
spearmanr_mrr@50 = 0.9514462351799011
spearmanr_mrr@100 = 0.8727411031723022
spearmanr_mrr@All = 0.8662223815917969
==========================================
test time: 0.417
Done Testing dataset DBpedia50
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.226 +- 0.159
mrr vals (pred, true): 0.158, 0.153

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.08752 	 0.00348 	 m..s
   51 	     1 	 0.11769 	 0.00352 	 MISS
   18 	     2 	 0.08752 	 0.00358 	 m..s
   18 	     3 	 0.08752 	 0.00361 	 m..s
   82 	     4 	 0.12773 	 0.00363 	 MISS
   18 	     5 	 0.08752 	 0.00363 	 m..s
   18 	     6 	 0.08752 	 0.00364 	 m..s
   18 	     7 	 0.08752 	 0.00365 	 m..s
   18 	     8 	 0.08752 	 0.00371 	 m..s
   18 	     9 	 0.08752 	 0.00376 	 m..s
   58 	    10 	 0.11913 	 0.00376 	 MISS
   13 	    11 	 0.08377 	 0.00383 	 m..s
   61 	    12 	 0.12029 	 0.00390 	 MISS
   41 	    13 	 0.10397 	 0.00397 	 MISS
   35 	    14 	 0.08851 	 0.00397 	 m..s
   18 	    15 	 0.08752 	 0.00397 	 m..s
   54 	    16 	 0.11828 	 0.00398 	 MISS
   60 	    17 	 0.12025 	 0.00405 	 MISS
   43 	    18 	 0.11307 	 0.00406 	 MISS
   12 	    19 	 0.08288 	 0.00411 	 m..s
   74 	    20 	 0.12350 	 0.00413 	 MISS
   39 	    21 	 0.09965 	 0.00414 	 m..s
   79 	    22 	 0.12718 	 0.00415 	 MISS
   52 	    23 	 0.11825 	 0.00419 	 MISS
   18 	    24 	 0.08752 	 0.00421 	 m..s
   72 	    25 	 0.12293 	 0.00421 	 MISS
   65 	    26 	 0.12166 	 0.00422 	 MISS
   15 	    27 	 0.08734 	 0.00427 	 m..s
   36 	    28 	 0.09634 	 0.00435 	 m..s
   42 	    29 	 0.10409 	 0.00438 	 m..s
   57 	    30 	 0.11859 	 0.00451 	 MISS
   49 	    31 	 0.11717 	 0.00453 	 MISS
   84 	    32 	 0.12886 	 0.00454 	 MISS
   45 	    33 	 0.11402 	 0.00456 	 MISS
   50 	    34 	 0.11756 	 0.00485 	 MISS
   68 	    35 	 0.12253 	 0.00516 	 MISS
   18 	    36 	 0.08752 	 0.00650 	 m..s
   18 	    37 	 0.08752 	 0.00898 	 m..s
   18 	    38 	 0.08752 	 0.01055 	 m..s
    2 	    39 	 0.07352 	 0.01584 	 m..s
    1 	    40 	 0.07341 	 0.01762 	 m..s
   14 	    41 	 0.08731 	 0.01879 	 m..s
    0 	    42 	 0.07302 	 0.01910 	 m..s
   18 	    43 	 0.08752 	 0.01953 	 m..s
    3 	    44 	 0.07369 	 0.01976 	 m..s
    5 	    45 	 0.07485 	 0.02009 	 m..s
   18 	    46 	 0.08752 	 0.02011 	 m..s
   38 	    47 	 0.09763 	 0.02127 	 m..s
    4 	    48 	 0.07474 	 0.02351 	 m..s
    6 	    49 	 0.07568 	 0.02471 	 m..s
   33 	    50 	 0.08814 	 0.02559 	 m..s
   16 	    51 	 0.08743 	 0.02608 	 m..s
   34 	    52 	 0.08823 	 0.02655 	 m..s
   17 	    53 	 0.08751 	 0.03033 	 m..s
   10 	    54 	 0.08099 	 0.03482 	 m..s
    8 	    55 	 0.07884 	 0.03596 	 m..s
    7 	    56 	 0.07873 	 0.03662 	 m..s
    9 	    57 	 0.07884 	 0.03752 	 m..s
   11 	    58 	 0.08102 	 0.03904 	 m..s
   37 	    59 	 0.09741 	 0.10076 	 ~...
   94 	    60 	 0.16947 	 0.11238 	 m..s
   44 	    61 	 0.11348 	 0.12016 	 ~...
   95 	    62 	 0.17385 	 0.12241 	 m..s
   48 	    63 	 0.11681 	 0.12266 	 ~...
   91 	    64 	 0.14843 	 0.12840 	 ~...
   96 	    65 	 0.17583 	 0.13253 	 m..s
   92 	    66 	 0.15591 	 0.13286 	 ~...
   59 	    67 	 0.11938 	 0.13392 	 ~...
   62 	    68 	 0.12032 	 0.13693 	 ~...
   47 	    69 	 0.11661 	 0.13809 	 ~...
   56 	    70 	 0.11831 	 0.14285 	 ~...
   53 	    71 	 0.11828 	 0.14866 	 m..s
   98 	    72 	 0.17893 	 0.14879 	 m..s
   90 	    73 	 0.13743 	 0.15121 	 ~...
   97 	    74 	 0.17667 	 0.15133 	 ~...
   93 	    75 	 0.15809 	 0.15323 	 ~...
   46 	    76 	 0.11649 	 0.15745 	 m..s
   76 	    77 	 0.12411 	 0.15816 	 m..s
   55 	    78 	 0.11830 	 0.16139 	 m..s
   77 	    79 	 0.12442 	 0.16581 	 m..s
   40 	    80 	 0.10088 	 0.16626 	 m..s
   70 	    81 	 0.12283 	 0.16773 	 m..s
   66 	    82 	 0.12188 	 0.16812 	 m..s
   71 	    83 	 0.12293 	 0.16959 	 m..s
  104 	    84 	 0.19389 	 0.17211 	 ~...
  100 	    85 	 0.19004 	 0.17242 	 ~...
   69 	    86 	 0.12281 	 0.17260 	 m..s
   63 	    87 	 0.12102 	 0.17544 	 m..s
   64 	    88 	 0.12119 	 0.17727 	 m..s
   80 	    89 	 0.12743 	 0.17788 	 m..s
   75 	    90 	 0.12357 	 0.17973 	 m..s
   67 	    91 	 0.12226 	 0.18067 	 m..s
   81 	    92 	 0.12748 	 0.18074 	 m..s
  103 	    93 	 0.19361 	 0.18784 	 ~...
  117 	    94 	 0.27163 	 0.18868 	 m..s
   99 	    95 	 0.18303 	 0.18874 	 ~...
   78 	    96 	 0.12456 	 0.18895 	 m..s
  101 	    97 	 0.19300 	 0.18895 	 ~...
   83 	    98 	 0.12775 	 0.18910 	 m..s
   73 	    99 	 0.12340 	 0.19017 	 m..s
  102 	   100 	 0.19337 	 0.19419 	 ~...
  111 	   101 	 0.22994 	 0.19569 	 m..s
  116 	   102 	 0.26474 	 0.19574 	 m..s
  108 	   103 	 0.22327 	 0.20455 	 ~...
  109 	   104 	 0.22521 	 0.20816 	 ~...
   87 	   105 	 0.13282 	 0.21234 	 m..s
   85 	   106 	 0.13224 	 0.21271 	 m..s
   89 	   107 	 0.13437 	 0.21451 	 m..s
   88 	   108 	 0.13377 	 0.21492 	 m..s
  105 	   109 	 0.19947 	 0.21514 	 ~...
   86 	   110 	 0.13238 	 0.21807 	 m..s
  106 	   111 	 0.19991 	 0.22172 	 ~...
  112 	   112 	 0.23012 	 0.22900 	 ~...
  115 	   113 	 0.25862 	 0.23323 	 ~...
  113 	   114 	 0.25345 	 0.23346 	 ~...
  114 	   115 	 0.25494 	 0.23706 	 ~...
  110 	   116 	 0.22764 	 0.24547 	 ~...
  107 	   117 	 0.21647 	 0.25210 	 m..s
  119 	   118 	 0.29585 	 0.25993 	 m..s
  118 	   119 	 0.29284 	 0.26384 	 ~...
  120 	   120 	 0.29700 	 0.26816 	 ~...
==========================================
r_mrr = 0.7561902403831482
r2_mrr = 0.4053509831428528
spearmanr_mrr@5 = 0.9679276347160339
spearmanr_mrr@10 = 0.9730890989303589
spearmanr_mrr@50 = 0.9820458292961121
spearmanr_mrr@100 = 0.8329071402549744
spearmanr_mrr@All = 0.8664096593856812
==========================================
test time: 0.416
Done Testing dataset CoDExSmall
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.231 +- 0.153
mrr vals (pred, true): 0.176, 0.224

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   52 	     0 	 0.04680 	 0.04001 	 ~...
   48 	     1 	 0.04412 	 0.04053 	 ~...
   62 	     2 	 0.05840 	 0.04062 	 ~...
   27 	     3 	 0.03531 	 0.04071 	 ~...
    0 	     4 	 0.03137 	 0.04150 	 ~...
   44 	     5 	 0.04152 	 0.04202 	 ~...
   57 	     6 	 0.04916 	 0.04215 	 ~...
    0 	     7 	 0.03137 	 0.04224 	 ~...
   41 	     8 	 0.03950 	 0.04227 	 ~...
   21 	     9 	 0.03287 	 0.04247 	 ~...
    0 	    10 	 0.03137 	 0.04265 	 ~...
   20 	    11 	 0.03286 	 0.04278 	 ~...
   65 	    12 	 0.05924 	 0.04279 	 ~...
   34 	    13 	 0.03809 	 0.04284 	 ~...
   19 	    14 	 0.03240 	 0.04298 	 ~...
   22 	    15 	 0.03382 	 0.04313 	 ~...
    0 	    16 	 0.03137 	 0.04319 	 ~...
   59 	    17 	 0.05332 	 0.04326 	 ~...
    0 	    18 	 0.03137 	 0.04330 	 ~...
   60 	    19 	 0.05418 	 0.04335 	 ~...
    0 	    20 	 0.03137 	 0.04341 	 ~...
    0 	    21 	 0.03137 	 0.04363 	 ~...
   16 	    22 	 0.03159 	 0.04363 	 ~...
   29 	    23 	 0.03578 	 0.04387 	 ~...
    0 	    24 	 0.03137 	 0.04390 	 ~...
   24 	    25 	 0.03399 	 0.04404 	 ~...
   32 	    26 	 0.03651 	 0.04405 	 ~...
   54 	    27 	 0.04809 	 0.04415 	 ~...
    0 	    28 	 0.03137 	 0.04432 	 ~...
   33 	    29 	 0.03725 	 0.04434 	 ~...
    0 	    30 	 0.03137 	 0.04435 	 ~...
   56 	    31 	 0.04850 	 0.04436 	 ~...
   40 	    32 	 0.03867 	 0.04452 	 ~...
   17 	    33 	 0.03217 	 0.04453 	 ~...
   31 	    34 	 0.03651 	 0.04454 	 ~...
   67 	    35 	 0.06255 	 0.04460 	 ~...
   25 	    36 	 0.03419 	 0.04466 	 ~...
    0 	    37 	 0.03137 	 0.04569 	 ~...
   26 	    38 	 0.03449 	 0.04576 	 ~...
    0 	    39 	 0.03137 	 0.04628 	 ~...
   58 	    40 	 0.05017 	 0.04635 	 ~...
   43 	    41 	 0.04144 	 0.04644 	 ~...
   39 	    42 	 0.03863 	 0.04672 	 ~...
   18 	    43 	 0.03227 	 0.04725 	 ~...
    0 	    44 	 0.03137 	 0.04741 	 ~...
    0 	    45 	 0.03137 	 0.04748 	 ~...
   61 	    46 	 0.05459 	 0.04798 	 ~...
   66 	    47 	 0.05935 	 0.04814 	 ~...
   37 	    48 	 0.03819 	 0.04874 	 ~...
   42 	    49 	 0.03991 	 0.04874 	 ~...
   51 	    50 	 0.04658 	 0.04890 	 ~...
   38 	    51 	 0.03863 	 0.04917 	 ~...
   23 	    52 	 0.03388 	 0.04941 	 ~...
   63 	    53 	 0.05848 	 0.04955 	 ~...
   46 	    54 	 0.04287 	 0.05014 	 ~...
   15 	    55 	 0.03150 	 0.05179 	 ~...
   47 	    56 	 0.04320 	 0.05188 	 ~...
   35 	    57 	 0.03814 	 0.05199 	 ~...
   30 	    58 	 0.03602 	 0.05247 	 ~...
   50 	    59 	 0.04655 	 0.05291 	 ~...
   45 	    60 	 0.04156 	 0.05327 	 ~...
   28 	    61 	 0.03575 	 0.05392 	 ~...
   71 	    62 	 0.07893 	 0.05404 	 ~...
    0 	    63 	 0.03137 	 0.05411 	 ~...
   64 	    64 	 0.05910 	 0.05427 	 ~...
   69 	    65 	 0.07410 	 0.05531 	 ~...
   53 	    66 	 0.04785 	 0.05586 	 ~...
   36 	    67 	 0.03817 	 0.05607 	 ~...
   49 	    68 	 0.04456 	 0.05665 	 ~...
   68 	    69 	 0.07364 	 0.05723 	 ~...
   72 	    70 	 0.08097 	 0.05740 	 ~...
   55 	    71 	 0.04824 	 0.05806 	 ~...
   70 	    72 	 0.07553 	 0.06201 	 ~...
   83 	    73 	 0.20861 	 0.10280 	 MISS
   77 	    74 	 0.18356 	 0.16203 	 ~...
   73 	    75 	 0.17006 	 0.17681 	 ~...
   89 	    76 	 0.22574 	 0.17765 	 m..s
   88 	    77 	 0.22116 	 0.18345 	 m..s
   85 	    78 	 0.21536 	 0.19965 	 ~...
   78 	    79 	 0.18801 	 0.20744 	 ~...
   74 	    80 	 0.17604 	 0.21070 	 m..s
   87 	    81 	 0.21735 	 0.21694 	 ~...
   79 	    82 	 0.19186 	 0.21832 	 ~...
   82 	    83 	 0.20372 	 0.22012 	 ~...
   84 	    84 	 0.21114 	 0.22106 	 ~...
   76 	    85 	 0.17771 	 0.22159 	 m..s
   86 	    86 	 0.21735 	 0.22373 	 ~...
   75 	    87 	 0.17647 	 0.22449 	 m..s
   93 	    88 	 0.23761 	 0.22748 	 ~...
   81 	    89 	 0.20219 	 0.22913 	 ~...
   80 	    90 	 0.19943 	 0.23013 	 m..s
   92 	    91 	 0.23621 	 0.23100 	 ~...
   96 	    92 	 0.25480 	 0.24127 	 ~...
   91 	    93 	 0.23261 	 0.24432 	 ~...
   97 	    94 	 0.25609 	 0.24573 	 ~...
  100 	    95 	 0.25933 	 0.25524 	 ~...
   90 	    96 	 0.22763 	 0.25569 	 ~...
  101 	    97 	 0.25963 	 0.25812 	 ~...
  109 	    98 	 0.30118 	 0.25848 	 m..s
   99 	    99 	 0.25865 	 0.26028 	 ~...
   95 	   100 	 0.25263 	 0.26046 	 ~...
   98 	   101 	 0.25838 	 0.26302 	 ~...
   94 	   102 	 0.23869 	 0.26639 	 ~...
  103 	   103 	 0.26035 	 0.27200 	 ~...
  115 	   104 	 0.33352 	 0.27749 	 m..s
  113 	   105 	 0.32894 	 0.28167 	 m..s
  114 	   106 	 0.33021 	 0.28516 	 m..s
  112 	   107 	 0.30646 	 0.28547 	 ~...
  102 	   108 	 0.25995 	 0.28575 	 ~...
  108 	   109 	 0.29908 	 0.28601 	 ~...
  107 	   110 	 0.28980 	 0.28647 	 ~...
  110 	   111 	 0.30337 	 0.28696 	 ~...
  111 	   112 	 0.30574 	 0.29005 	 ~...
  105 	   113 	 0.26761 	 0.29265 	 ~...
  106 	   114 	 0.27950 	 0.29354 	 ~...
  117 	   115 	 0.34521 	 0.29789 	 m..s
  116 	   116 	 0.33927 	 0.30148 	 m..s
  104 	   117 	 0.26697 	 0.30820 	 m..s
  119 	   118 	 0.36856 	 0.33146 	 m..s
  118 	   119 	 0.36476 	 0.34190 	 ~...
  120 	   120 	 0.36970 	 0.34740 	 ~...
==========================================
r_mrr = 0.9812489748001099
r2_mrr = 0.9565199613571167
spearmanr_mrr@5 = 0.9872352480888367
spearmanr_mrr@10 = 0.9353747367858887
spearmanr_mrr@50 = 0.9593330025672913
spearmanr_mrr@100 = 0.9925305843353271
spearmanr_mrr@All = 0.9934277534484863
==========================================
test time: 0.485
Done Testing dataset Kinships
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.171 +- 0.170
mrr vals (pred, true): 0.069, 0.069

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   48 	     0 	 0.04685 	 0.00581 	 m..s
   41 	     1 	 0.04633 	 0.00593 	 m..s
    0 	     2 	 0.04536 	 0.00599 	 m..s
   38 	     3 	 0.04623 	 0.00610 	 m..s
   43 	     4 	 0.04655 	 0.00616 	 m..s
   16 	     5 	 0.04539 	 0.00624 	 m..s
    0 	     6 	 0.04536 	 0.00629 	 m..s
    0 	     7 	 0.04536 	 0.00631 	 m..s
   21 	     8 	 0.04555 	 0.00632 	 m..s
   29 	     9 	 0.04590 	 0.00636 	 m..s
   22 	    10 	 0.04567 	 0.00639 	 m..s
   17 	    11 	 0.04547 	 0.00639 	 m..s
    0 	    12 	 0.04536 	 0.00641 	 m..s
   67 	    13 	 0.04889 	 0.00642 	 m..s
    0 	    14 	 0.04536 	 0.00642 	 m..s
   39 	    15 	 0.04623 	 0.00642 	 m..s
   27 	    16 	 0.04585 	 0.00644 	 m..s
    0 	    17 	 0.04536 	 0.00644 	 m..s
   40 	    18 	 0.04624 	 0.00645 	 m..s
   20 	    19 	 0.04555 	 0.00646 	 m..s
   25 	    20 	 0.04571 	 0.00646 	 m..s
   24 	    21 	 0.04569 	 0.00647 	 m..s
   65 	    22 	 0.04852 	 0.00648 	 m..s
    0 	    23 	 0.04536 	 0.00651 	 m..s
    0 	    24 	 0.04536 	 0.00652 	 m..s
   33 	    25 	 0.04607 	 0.00653 	 m..s
    0 	    26 	 0.04536 	 0.00653 	 m..s
   34 	    27 	 0.04617 	 0.00655 	 m..s
   32 	    28 	 0.04599 	 0.00658 	 m..s
   44 	    29 	 0.04656 	 0.00659 	 m..s
   62 	    30 	 0.04843 	 0.00660 	 m..s
   52 	    31 	 0.04714 	 0.00662 	 m..s
   19 	    32 	 0.04549 	 0.00663 	 m..s
   57 	    33 	 0.04740 	 0.00663 	 m..s
    0 	    34 	 0.04536 	 0.00669 	 m..s
   54 	    35 	 0.04728 	 0.00671 	 m..s
    0 	    36 	 0.04536 	 0.01052 	 m..s
   18 	    37 	 0.04548 	 0.01113 	 m..s
    0 	    38 	 0.04536 	 0.01355 	 m..s
    0 	    39 	 0.04536 	 0.01398 	 m..s
   28 	    40 	 0.04590 	 0.01894 	 ~...
   23 	    41 	 0.04568 	 0.01960 	 ~...
   30 	    42 	 0.04593 	 0.02177 	 ~...
   45 	    43 	 0.04656 	 0.02179 	 ~...
   42 	    44 	 0.04638 	 0.02327 	 ~...
    0 	    45 	 0.04536 	 0.02377 	 ~...
   37 	    46 	 0.04618 	 0.02557 	 ~...
   35 	    47 	 0.04618 	 0.02652 	 ~...
   15 	    48 	 0.04538 	 0.03491 	 ~...
   47 	    49 	 0.04674 	 0.03605 	 ~...
   36 	    50 	 0.04618 	 0.03625 	 ~...
   46 	    51 	 0.04671 	 0.03700 	 ~...
    0 	    52 	 0.04536 	 0.03734 	 ~...
   51 	    53 	 0.04712 	 0.04039 	 ~...
   49 	    54 	 0.04689 	 0.04205 	 ~...
   82 	    55 	 0.07813 	 0.04777 	 m..s
   76 	    56 	 0.06935 	 0.04907 	 ~...
   74 	    57 	 0.06888 	 0.05013 	 ~...
   73 	    58 	 0.06723 	 0.05170 	 ~...
   81 	    59 	 0.07754 	 0.05645 	 ~...
   50 	    60 	 0.04711 	 0.05706 	 ~...
   66 	    61 	 0.04853 	 0.05741 	 ~...
   53 	    62 	 0.04726 	 0.06318 	 ~...
   77 	    63 	 0.07110 	 0.06335 	 ~...
   55 	    64 	 0.04730 	 0.06488 	 ~...
   78 	    65 	 0.07252 	 0.06688 	 ~...
   75 	    66 	 0.06900 	 0.06932 	 ~...
   64 	    67 	 0.04850 	 0.07065 	 ~...
   63 	    68 	 0.04843 	 0.07477 	 ~...
   79 	    69 	 0.07379 	 0.07764 	 ~...
   85 	    70 	 0.08306 	 0.08095 	 ~...
   95 	    71 	 0.10552 	 0.08425 	 ~...
   88 	    72 	 0.08581 	 0.08550 	 ~...
   89 	    73 	 0.08815 	 0.08761 	 ~...
   92 	    74 	 0.09409 	 0.08818 	 ~...
   93 	    75 	 0.09496 	 0.08823 	 ~...
   83 	    76 	 0.08011 	 0.09087 	 ~...
   31 	    77 	 0.04599 	 0.09181 	 m..s
  105 	    78 	 0.11894 	 0.09268 	 ~...
   84 	    79 	 0.08119 	 0.09486 	 ~...
   60 	    80 	 0.04796 	 0.09542 	 m..s
   26 	    81 	 0.04575 	 0.09630 	 m..s
   56 	    82 	 0.04733 	 0.09650 	 m..s
   58 	    83 	 0.04751 	 0.09673 	 m..s
  104 	    84 	 0.11830 	 0.09751 	 ~...
   59 	    85 	 0.04786 	 0.09868 	 m..s
   61 	    86 	 0.04800 	 0.10022 	 m..s
   86 	    87 	 0.08398 	 0.10147 	 ~...
   97 	    88 	 0.10833 	 0.10274 	 ~...
   87 	    89 	 0.08398 	 0.10484 	 ~...
  101 	    90 	 0.11137 	 0.10569 	 ~...
   96 	    91 	 0.10726 	 0.10611 	 ~...
   98 	    92 	 0.11028 	 0.10775 	 ~...
   94 	    93 	 0.09564 	 0.10948 	 ~...
  106 	    94 	 0.13236 	 0.11167 	 ~...
   70 	    95 	 0.05042 	 0.11182 	 m..s
   99 	    96 	 0.11052 	 0.11213 	 ~...
   72 	    97 	 0.05109 	 0.11449 	 m..s
   80 	    98 	 0.07649 	 0.11815 	 m..s
   69 	    99 	 0.05025 	 0.12139 	 m..s
   71 	   100 	 0.05084 	 0.12157 	 m..s
   68 	   101 	 0.05019 	 0.12635 	 m..s
  109 	   102 	 0.16482 	 0.12649 	 m..s
  108 	   103 	 0.16118 	 0.12687 	 m..s
   90 	   104 	 0.08916 	 0.14201 	 m..s
   91 	   105 	 0.09195 	 0.14538 	 m..s
  110 	   106 	 0.16877 	 0.14759 	 ~...
  111 	   107 	 0.17320 	 0.15947 	 ~...
  107 	   108 	 0.14653 	 0.16863 	 ~...
  112 	   109 	 0.17459 	 0.19480 	 ~...
  102 	   110 	 0.11166 	 0.20702 	 m..s
  116 	   111 	 0.25546 	 0.21616 	 m..s
  100 	   112 	 0.11111 	 0.21638 	 MISS
  103 	   113 	 0.11202 	 0.21743 	 MISS
  115 	   114 	 0.23889 	 0.24481 	 ~...
  117 	   115 	 0.27328 	 0.25086 	 ~...
  114 	   116 	 0.22977 	 0.27042 	 m..s
  113 	   117 	 0.22635 	 0.27546 	 m..s
  119 	   118 	 0.34403 	 0.28924 	 m..s
  118 	   119 	 0.33298 	 0.29087 	 m..s
  120 	   120 	 0.34727 	 0.29220 	 m..s
==========================================
r_mrr = 0.8646948337554932
r2_mrr = 0.7372726202011108
spearmanr_mrr@5 = 0.9989223480224609
spearmanr_mrr@10 = 0.9405938982963562
spearmanr_mrr@50 = 0.976969838142395
spearmanr_mrr@100 = 0.9391010999679565
spearmanr_mrr@All = 0.9318493008613586
==========================================
test time: 0.475
Done Testing dataset OpenEA
total time taken: 884.26606798172
training time taken: 845.3197119235992
TWIG out ;))
==========================================================
----------------------------------------------------------
Running a TWIG experiment with tag: TransE-omit-CoDExSmall
----------------------------------------------------------
==========================================================
Using random seed: 8887480699248815
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Loading UMLS...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [413, 543, 212, 267, 359, 137, 362, 868, 612, 1058, 530, 457, 702, 379, 43, 1090, 958, 772, 3, 851, 1037, 876, 1150, 983, 225, 1132, 325, 375, 911, 894, 520, 607, 853, 767, 532, 1140, 725, 610, 248, 1196, 152, 503, 956, 103, 237, 343, 210, 852, 653, 739, 191, 380, 661, 1137, 29, 357, 828, 969, 643, 60, 206, 465, 1028, 539, 39, 1087, 417, 968, 677, 307, 892, 166, 756, 277, 1131, 985, 59, 474, 505, 850, 304, 182, 1177, 411, 602, 220, 19, 167, 804, 239, 812, 406, 668, 700, 63, 917, 264, 632, 654, 1111, 403, 226, 903, 405, 350, 71, 1073, 663, 214, 628, 222, 1202, 566, 1053, 1164, 1104, 499, 107, 445, 444, 1148]
valid_ids (0): []
train_ids (1094): [727, 1134, 586, 897, 338, 336, 412, 744, 841, 382, 295, 578, 886, 811, 533, 531, 955, 165, 316, 737, 1071, 342, 492, 1041, 944, 269, 416, 579, 988, 821, 686, 443, 580, 550, 121, 595, 553, 299, 625, 132, 1050, 779, 1044, 455, 1078, 10, 24, 679, 1056, 283, 757, 561, 617, 994, 109, 1203, 377, 877, 249, 1020, 945, 32, 782, 568, 1014, 939, 68, 692, 603, 506, 879, 1075, 949, 794, 507, 896, 315, 719, 1209, 1158, 976, 1040, 551, 145, 40, 387, 1192, 992, 1026, 1061, 895, 292, 1030, 693, 1200, 1120, 101, 647, 793, 844, 477, 1152, 1161, 36, 933, 1003, 1012, 1046, 893, 120, 221, 546, 200, 926, 592, 575, 207, 381, 54, 211, 824, 332, 203, 360, 367, 701, 921, 418, 1024, 1173, 1068, 510, 255, 862, 112, 186, 288, 704, 1057, 698, 1000, 243, 487, 778, 1106, 293, 318, 108, 131, 1181, 639, 898, 266, 833, 341, 1174, 904, 873, 1103, 485, 859, 22, 330, 244, 799, 415, 195, 683, 606, 753, 721, 1118, 1207, 390, 900, 62, 861, 1138, 184, 1036, 932, 749, 802, 720, 909, 14, 1025, 710, 280, 1213, 627, 978, 1135, 432, 1126, 615, 558, 114, 394, 170, 37, 100, 993, 997, 937, 941, 773, 986, 301, 869, 616, 96, 223, 1108, 1059, 1144, 70, 278, 733, 158, 25, 541, 65, 20, 990, 275, 426, 438, 787, 93, 1004, 1195, 312, 965, 484, 134, 159, 38, 822, 453, 1054, 571, 594, 0, 238, 153, 289, 224, 1116, 439, 662, 363, 218, 201, 473, 1051, 609, 929, 420, 392, 759, 1160, 376, 913, 308, 122, 1208, 1045, 1001, 729, 957, 786, 730, 966, 435, 110, 723, 829, 324, 45, 831, 930, 1008, 1035, 1172, 147, 97, 640, 126, 322, 26, 117, 409, 755, 513, 630, 151, 854, 46, 669, 274, 356, 519, 1107, 529, 177, 599, 41, 796, 400, 401, 1034, 460, 488, 694, 168, 544, 1142, 738, 194, 940, 185, 281, 279, 437, 666, 570, 329, 1121, 476, 16, 636, 891, 53, 646, 247, 931, 809, 872, 242, 883, 227, 526, 88, 540, 1117, 1076, 659, 268, 486, 58, 263, 1067, 623, 1062, 943, 552, 528, 11, 106, 712, 331, 303, 849, 1109, 160, 838, 51, 368, 645, 1186, 94, 934, 86, 583, 466, 1099, 79, 670, 442, 928, 497, 761, 547, 1147, 874, 276, 310, 884, 111, 72, 74, 989, 887, 472, 321, 589, 30, 783, 1096, 252, 1091, 1089, 736, 819, 523, 13, 138, 372, 2, 726, 1176, 664, 257, 180, 448, 1201, 745, 395, 905, 84, 1153, 867, 764, 601, 501, 1212, 1105, 801, 980, 50, 771, 614, 711, 311, 422, 559, 129, 815, 85, 290, 234, 1143, 569, 282, 708, 385, 977, 535, 1100, 1031, 89, 328, 69, 345, 430, 104, 1019, 971, 189, 996, 498, 964, 576, 792, 790, 527, 671, 605, 656, 866, 475, 834, 21, 919, 752, 500, 56, 427, 747, 378, 468, 843, 511, 173, 673, 75, 962, 241, 902, 461, 27, 1097, 870, 848, 765, 573, 459, 128, 355, 770, 521, 1101, 637, 250, 880, 1211, 1123, 560, 1063, 197, 1094, 800, 344, 450, 90, 518, 1080, 565, 424, 718, 846, 1162, 407, 549, 80, 236, 735, 496, 619, 648, 1047, 1006, 44, 384, 429, 464, 213, 386, 483, 489, 351, 1048, 681, 34, 291, 414, 374, 469, 865, 404, 734, 715, 674, 740, 798, 845, 722, 1146, 1184, 706, 1127, 924, 696, 366, 1175, 320, 73, 155, 629, 724, 172, 1119, 1169, 626, 675, 963, 467, 228, 732, 174, 339, 524, 1165, 102, 660, 219, 140, 534, 1168, 972, 774, 1159, 1052, 157, 655, 81, 613, 1072, 556, 912, 229, 192, 791, 542, 309, 419, 1179, 1145, 270, 130, 1204, 1188, 479, 847, 183, 1197, 1199, 915, 35, 425, 899, 918, 421, 514, 408, 1154, 731, 948, 364, 515, 572, 1190, 716, 83, 99, 1113, 1011, 760, 77, 1185, 973, 297, 751, 522, 148, 502, 216, 491, 797, 393, 1102, 287, 743, 995, 555, 1042, 398, 396, 1194, 881, 327, 494, 1066, 676, 781, 458, 391, 1079, 454, 82, 641, 591, 150, 920, 12, 979, 952, 517, 823, 832, 371, 317, 649, 57, 1022, 272, 658, 584, 871, 118, 842, 901, 1032, 1049, 574, 690, 875, 1139, 92, 64, 302, 816, 298, 98, 672, 169, 908, 916, 682, 33, 349, 597, 925, 684, 914, 190, 358, 480, 490, 446, 433, 1178, 115, 1002, 265, 326, 554, 947, 423, 334, 1130, 789, 9, 634, 1017, 175, 52, 826, 1093, 685, 161, 305, 906, 588, 471, 259, 253, 1083, 139, 352, 1023, 638, 703, 707, 313, 462, 306, 179, 780, 611, 431, 17, 689, 123, 667, 333, 23, 741, 748, 87, 149, 1086, 1055, 233, 230, 1015, 680, 951, 600, 807, 240, 493, 258, 1115, 495, 1198, 813, 728, 1005, 635, 762, 839, 199, 410, 235, 769, 1114, 545, 1074, 825, 397, 456, 163, 585, 810, 621, 245, 205, 198, 856, 709, 946, 910, 436, 564, 1133, 959, 1151, 76, 717, 836, 231, 4, 563, 217, 907, 1033, 286, 262, 784, 806, 95, 48, 840, 504, 188, 1095, 620, 652, 6, 1124, 651, 1081, 346, 795, 960, 678, 982, 608, 47, 548, 1077, 1029, 998, 855, 999, 1129, 482, 254, 337, 136, 557, 775, 581, 1170, 754, 15, 923, 1122, 817, 1110, 119, 365, 642, 935, 260, 209, 714, 1128, 1183, 590, 516, 215, 1157, 950, 1085, 1125, 1010, 388, 42, 399, 785, 171, 1018, 936, 827, 622, 7, 451, 105, 300, 294, 713, 1182, 593, 508, 1156, 370, 447, 31, 814, 967, 1069, 697, 135, 885, 981, 1, 181, 481, 402, 1092, 434, 449, 1088, 1210, 536, 805, 176, 1214, 18, 452, 113, 204, 142, 857, 353, 162, 657, 927, 1027, 271, 954, 1136, 208, 49, 803, 820, 835, 133, 587, 567, 624, 537, 273, 750, 348, 78, 688, 1189, 512, 776, 758, 577, 860, 441, 509, 28, 938, 124, 991, 1007, 858, 361, 942, 974, 319, 478, 125, 695, 1039, 633, 984, 888, 691, 1084, 1043, 373, 878, 1167, 1070, 1187, 296, 340, 354, 890, 127, 284, 830, 768, 143, 314, 67, 1166, 256, 970, 650, 961, 837, 146, 631, 440, 335, 1163, 889, 347, 202, 91, 1155, 285, 1193, 665, 763, 953, 1141, 463, 1180, 187, 788, 562, 596, 61, 1149, 705, 246, 987, 538, 1021, 141, 604, 154, 975, 1082, 193, 864, 1065, 1171, 1038, 470, 428, 1191, 644, 5, 922, 1112, 178, 389, 1098, 687, 261, 742, 8, 1064, 525, 369, 116, 1205, 882, 66, 818, 1009, 777, 1013, 156, 232, 1060, 746, 164, 598, 618, 1206, 323, 144, 699, 1016, 196, 863, 766, 251, 55, 808, 383, 582]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7682341606175606
the save name prefix for this run is:  chkpt-ID_7682341606175606_tag_TransE-omit-CoDExSmall
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1'], 'UMLS': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1030
rank avg (pred): 0.512 +- 0.016
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001297881

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 17
rank avg (pred): 0.091 +- 0.064
mrr vals (pred, true): 0.038, 0.221
batch losses (mrrl, rdl): 0.0, 2.35956e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.294 +- 0.231
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004717253

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.316 +- 0.250
mrr vals (pred, true): 0.158, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002762541

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 115
rank avg (pred): 0.339 +- 0.264
mrr vals (pred, true): 0.186, 0.017
batch losses (mrrl, rdl): 0.0, 0.0003122824

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 678
rank avg (pred): 0.337 +- 0.271
mrr vals (pred, true): 0.211, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002129704

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.063 +- 0.053
mrr vals (pred, true): 0.357, 0.085
batch losses (mrrl, rdl): 0.0, 7.71267e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 667
rank avg (pred): 0.352 +- 0.283
mrr vals (pred, true): 0.241, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002304527

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 897
rank avg (pred): 0.091 +- 0.078
mrr vals (pred, true): 0.391, 0.107
batch losses (mrrl, rdl): 0.0, 4.87817e-05

Epoch over!
epoch time: 51.244

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 416
rank avg (pred): 0.335 +- 0.282
mrr vals (pred, true): 0.362, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001752444

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1099
rank avg (pred): 0.292 +- 0.292
mrr vals (pred, true): 0.378, 0.159
batch losses (mrrl, rdl): 0.0, 0.000298052

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 317
rank avg (pred): 0.035 +- 0.036
mrr vals (pred, true): 0.401, 0.180
batch losses (mrrl, rdl): 0.0, 4.92872e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 414
rank avg (pred): 0.305 +- 0.301
mrr vals (pred, true): 0.388, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004471111

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.338 +- 0.319
mrr vals (pred, true): 0.304, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002136879

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1181
rank avg (pred): 0.356 +- 0.295
mrr vals (pred, true): 0.297, 0.036
batch losses (mrrl, rdl): 0.0, 0.0001719055

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 859
rank avg (pred): 0.330 +- 0.307
mrr vals (pred, true): 0.346, 0.157
batch losses (mrrl, rdl): 0.0, 0.0004071407

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 15
rank avg (pred): 0.100 +- 0.138
mrr vals (pred, true): 0.419, 0.093
batch losses (mrrl, rdl): 0.0, 1.54239e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 750
rank avg (pred): 0.065 +- 0.112
mrr vals (pred, true): 0.490, 0.212
batch losses (mrrl, rdl): 0.0, 7.1867e-06

Epoch over!
epoch time: 52.758

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.364 +- 0.300
mrr vals (pred, true): 0.319, 0.000
batch losses (mrrl, rdl): 0.0, 3.92911e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.091 +- 0.143
mrr vals (pred, true): 0.379, 0.195
batch losses (mrrl, rdl): 0.0, 2.77124e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 581
rank avg (pred): 0.357 +- 0.299
mrr vals (pred, true): 0.365, 0.035
batch losses (mrrl, rdl): 0.0, 0.000136098

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 834
rank avg (pred): 0.076 +- 0.145
mrr vals (pred, true): 0.480, 0.245
batch losses (mrrl, rdl): 0.0, 1.59447e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.357 +- 0.296
mrr vals (pred, true): 0.352, 0.003
batch losses (mrrl, rdl): 0.0, 8.53914e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 676
rank avg (pred): 0.358 +- 0.298
mrr vals (pred, true): 0.375, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004203819

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.344 +- 0.305
mrr vals (pred, true): 0.301, 0.000
batch losses (mrrl, rdl): 0.0, 0.00019622

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1036
rank avg (pred): 0.277 +- 0.290
mrr vals (pred, true): 0.438, 0.000
batch losses (mrrl, rdl): 0.0, 0.000692417

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 574
rank avg (pred): 0.357 +- 0.300
mrr vals (pred, true): 0.366, 0.005
batch losses (mrrl, rdl): 0.0, 7.65105e-05

Epoch over!
epoch time: 51.568

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 387
rank avg (pred): 0.305 +- 0.310
mrr vals (pred, true): 0.353, 0.043
batch losses (mrrl, rdl): 0.0, 0.0002951416

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 393
rank avg (pred): 0.296 +- 0.302
mrr vals (pred, true): 0.394, 0.058
batch losses (mrrl, rdl): 0.0, 9.01143e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 510
rank avg (pred): 0.250 +- 0.264
mrr vals (pred, true): 0.393, 0.060
batch losses (mrrl, rdl): 0.0, 3.11062e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 698
rank avg (pred): 0.353 +- 0.303
mrr vals (pred, true): 0.351, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001424484

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.341 +- 0.304
mrr vals (pred, true): 0.371, 0.135
batch losses (mrrl, rdl): 0.0, 0.0005787601

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.324 +- 0.319
mrr vals (pred, true): 0.302, 0.114
batch losses (mrrl, rdl): 0.0, 0.0001936776

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 203
rank avg (pred): 0.282 +- 0.297
mrr vals (pred, true): 0.438, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004601387

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 486
rank avg (pred): 0.254 +- 0.273
mrr vals (pred, true): 0.400, 0.028
batch losses (mrrl, rdl): 0.0, 1.5752e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.307 +- 0.317
mrr vals (pred, true): 0.383, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001910242

Epoch over!
epoch time: 51.964

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 859
rank avg (pred): 0.306 +- 0.318
mrr vals (pred, true): 0.425, 0.157
batch losses (mrrl, rdl): 0.0, 0.0002965875

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 750
rank avg (pred): 0.061 +- 0.128
mrr vals (pred, true): 0.505, 0.212
batch losses (mrrl, rdl): 0.0, 4.3453e-06

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1012
rank avg (pred): 0.315 +- 0.310
mrr vals (pred, true): 0.369, 0.167
batch losses (mrrl, rdl): 0.0, 0.0005198601

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 412
rank avg (pred): 0.313 +- 0.311
mrr vals (pred, true): 0.348, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003557685

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.390 +- 0.314
mrr vals (pred, true): 0.297, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002445427

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 165
rank avg (pred): 0.332 +- 0.304
mrr vals (pred, true): 0.275, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002108758

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 770
rank avg (pred): 0.307 +- 0.316
mrr vals (pred, true): 0.399, 0.173
batch losses (mrrl, rdl): 0.0, 0.0005001727

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 525
rank avg (pred): 0.278 +- 0.265
mrr vals (pred, true): 0.404, 0.034
batch losses (mrrl, rdl): 0.0, 3.64564e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 577
rank avg (pred): 0.367 +- 0.301
mrr vals (pred, true): 0.357, 0.006
batch losses (mrrl, rdl): 0.0, 9.89559e-05

Epoch over!
epoch time: 52.427

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1205
rank avg (pred): 0.377 +- 0.308
mrr vals (pred, true): 0.356, 0.000
batch losses (mrrl, rdl): 0.937415719, 0.0001156499

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 482
rank avg (pred): 0.406 +- 0.216
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009698689, 8.5556e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 213
rank avg (pred): 0.401 +- 0.215
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.84901e-05, 0.0001599583

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 879
rank avg (pred): 0.374 +- 0.233
mrr vals (pred, true): 0.110, 0.000
batch losses (mrrl, rdl): 0.0358854793, 0.0001959786

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 99
rank avg (pred): 0.385 +- 0.222
mrr vals (pred, true): 0.065, 0.012
batch losses (mrrl, rdl): 0.0021809144, 0.0006718007

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 53
rank avg (pred): 0.044 +- 0.108
mrr vals (pred, true): 0.247, 0.183
batch losses (mrrl, rdl): 0.0409665368, 1.89461e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 655
rank avg (pred): 0.366 +- 0.206
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0057956069, 0.0003017654

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.365 +- 0.201
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0013568699, 9.32733e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 340
rank avg (pred): 0.387 +- 0.248
mrr vals (pred, true): 0.091, 0.052
batch losses (mrrl, rdl): 0.0168569218, 0.0008938788

Epoch over!
epoch time: 54.091

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 266
rank avg (pred): 0.062 +- 0.109
mrr vals (pred, true): 0.220, 0.245
batch losses (mrrl, rdl): 0.006180401, 1.81946e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.383 +- 0.237
mrr vals (pred, true): 0.060, 0.081
batch losses (mrrl, rdl): 0.0009144661, 0.0008829599

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 112
rank avg (pred): 0.397 +- 0.251
mrr vals (pred, true): 0.054, 0.028
batch losses (mrrl, rdl): 0.0001956405, 0.0007064865

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 749
rank avg (pred): 0.163 +- 0.195
mrr vals (pred, true): 0.087, 0.149
batch losses (mrrl, rdl): 0.0384800322, 0.000153887

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 597
rank avg (pred): 0.343 +- 0.176
mrr vals (pred, true): 0.043, 0.005
batch losses (mrrl, rdl): 0.0005126758, 5.88588e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 491
rank avg (pred): 0.254 +- 0.178
mrr vals (pred, true): 0.079, 0.071
batch losses (mrrl, rdl): 0.008297855, 0.0001462917

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1069
rank avg (pred): 0.101 +- 0.155
mrr vals (pred, true): 0.314, 0.305
batch losses (mrrl, rdl): 0.0007080694, 9.19392e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 774
rank avg (pred): 0.359 +- 0.230
mrr vals (pred, true): 0.066, 0.157
batch losses (mrrl, rdl): 0.0829581916, 0.000534791

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 679
rank avg (pred): 0.325 +- 0.176
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0013203856, 0.0007918634

Epoch over!
epoch time: 52.845

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 605
rank avg (pred): 0.321 +- 0.189
mrr vals (pred, true): 0.082, 0.032
batch losses (mrrl, rdl): 0.0103925997, 9.34487e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 442
rank avg (pred): 0.333 +- 0.243
mrr vals (pred, true): 0.071, 0.000
batch losses (mrrl, rdl): 0.0043956614, 0.0002635467

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 308
rank avg (pred): 0.084 +- 0.124
mrr vals (pred, true): 0.187, 0.111
batch losses (mrrl, rdl): 0.0579332262, 3.73706e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 309
rank avg (pred): 0.141 +- 0.180
mrr vals (pred, true): 0.118, 0.070
batch losses (mrrl, rdl): 0.0468325317, 2.87324e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.347 +- 0.250
mrr vals (pred, true): 0.081, 0.125
batch losses (mrrl, rdl): 0.0193407554, 0.0006681237

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1113
rank avg (pred): 0.367 +- 0.273
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002201101, 0.0003005465

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1123
rank avg (pred): 0.329 +- 0.290
mrr vals (pred, true): 0.086, 0.000
batch losses (mrrl, rdl): 0.0129209403, 0.0005181435

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1019
rank avg (pred): 0.397 +- 0.299
mrr vals (pred, true): 0.056, 0.144
batch losses (mrrl, rdl): 0.0782614946, 0.000799273

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 376
rank avg (pred): 0.351 +- 0.272
mrr vals (pred, true): 0.059, 0.087
batch losses (mrrl, rdl): 0.0008193319, 0.0004591217

Epoch over!
epoch time: 55.206

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 83
rank avg (pred): 0.315 +- 0.245
mrr vals (pred, true): 0.068, 0.065
batch losses (mrrl, rdl): 0.003186845, 0.0004153793

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.383 +- 0.283
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0008484609, 7.83886e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 933
rank avg (pred): 0.339 +- 0.237
mrr vals (pred, true): 0.069, 0.144
batch losses (mrrl, rdl): 0.056163013, 0.0003569817

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 224
rank avg (pred): 0.358 +- 0.280
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0110633262, 0.0003256351

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 975
rank avg (pred): 0.081 +- 0.125
mrr vals (pred, true): 0.288, 0.278
batch losses (mrrl, rdl): 0.0009183211, 2.21569e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 597
rank avg (pred): 0.302 +- 0.164
mrr vals (pred, true): 0.045, 0.005
batch losses (mrrl, rdl): 0.0002797915, 8.54105e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 737
rank avg (pred): 0.115 +- 0.136
mrr vals (pred, true): 0.129, 0.085
batch losses (mrrl, rdl): 0.0621365979, 2.9626e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 476
rank avg (pred): 0.302 +- 0.262
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.0104081612, 0.0005813194

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 635
rank avg (pred): 0.319 +- 0.206
mrr vals (pred, true): 0.051, 0.032
batch losses (mrrl, rdl): 2.5398e-06, 5.27208e-05

Epoch over!
epoch time: 55.38

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 696
rank avg (pred): 0.326 +- 0.218
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.002729204, 0.0004093853

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.312 +- 0.202
mrr vals (pred, true): 0.062, 0.006
batch losses (mrrl, rdl): 0.0015354687, 3.95864e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1081
rank avg (pred): 0.374 +- 0.318
mrr vals (pred, true): 0.075, 0.170
batch losses (mrrl, rdl): 0.0907089934, 0.000756731

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 55
rank avg (pred): 0.087 +- 0.115
mrr vals (pred, true): 0.116, 0.056
batch losses (mrrl, rdl): 0.0438129455, 1.80582e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 781
rank avg (pred): 0.354 +- 0.291
mrr vals (pred, true): 0.074, 0.164
batch losses (mrrl, rdl): 0.0796854421, 0.0006012646

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.368 +- 0.308
mrr vals (pred, true): 0.067, 0.002
batch losses (mrrl, rdl): 0.0027970618, 0.0003114318

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.389 +- 0.326
mrr vals (pred, true): 0.073, 0.000
batch losses (mrrl, rdl): 0.00532474, 0.000167044

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.343 +- 0.314
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0057008187, 0.0003510403

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.389 +- 0.290
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002366353, 0.0001108582

Epoch over!
epoch time: 54.711

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 441
rank avg (pred): 0.398 +- 0.296
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006334225, 2.99764e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 986
rank avg (pred): 0.086 +- 0.118
mrr vals (pred, true): 0.233, 0.286
batch losses (mrrl, rdl): 0.0275786892, 3.18413e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 791
rank avg (pred): 0.357 +- 0.260
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0012843858, 0.0002230205

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 74
rank avg (pred): 0.055 +- 0.082
mrr vals (pred, true): 0.246, 0.193
batch losses (mrrl, rdl): 0.0278897937, 1.59147e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1133
rank avg (pred): 0.341 +- 0.332
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0061266972, 0.0002381561

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.394 +- 0.315
mrr vals (pred, true): 0.070, 0.038
batch losses (mrrl, rdl): 0.0041450881, 0.0004402096

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 827
rank avg (pred): 0.127 +- 0.152
mrr vals (pred, true): 0.188, 0.222
batch losses (mrrl, rdl): 0.0112737957, 0.0001241868

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 763
rank avg (pred): 0.369 +- 0.286
mrr vals (pred, true): 0.085, 0.108
batch losses (mrrl, rdl): 0.0056083878, 0.0003262619

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.048 +- 0.081
mrr vals (pred, true): 0.176, 0.217
batch losses (mrrl, rdl): 0.0168654174, 5.141e-05

Epoch over!
epoch time: 56.495

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 871
rank avg (pred): 0.354 +- 0.263
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0019440076, 0.0001900892

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 398
rank avg (pred): 0.300 +- 0.219
mrr vals (pred, true): 0.065, 0.130
batch losses (mrrl, rdl): 0.0414981656, 0.0002634605

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1204
rank avg (pred): 0.378 +- 0.278
mrr vals (pred, true): 0.071, 0.001
batch losses (mrrl, rdl): 0.0045476276, 0.0001307798

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1171
rank avg (pred): 0.423 +- 0.310
mrr vals (pred, true): 0.064, 0.049
batch losses (mrrl, rdl): 0.0020595673, 0.0003181758

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.396 +- 0.282
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.39523e-05, 8.37435e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 581
rank avg (pred): 0.372 +- 0.277
mrr vals (pred, true): 0.049, 0.035
batch losses (mrrl, rdl): 2.9669e-06, 0.0001165959

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 290
rank avg (pred): 0.057 +- 0.090
mrr vals (pred, true): 0.197, 0.202
batch losses (mrrl, rdl): 0.0001920531, 7.53765e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 482
rank avg (pred): 0.391 +- 0.314
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0011865896, 0.000108325

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 386
rank avg (pred): 0.451 +- 0.339
mrr vals (pred, true): 0.071, 0.104
batch losses (mrrl, rdl): 0.0109522985, 0.0012518493

Epoch over!
epoch time: 55.993

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 870
rank avg (pred): 0.390 +- 0.300
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001595545, 0.0001093706

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 647
rank avg (pred): 0.399 +- 0.288
mrr vals (pred, true): 0.057, 0.036
batch losses (mrrl, rdl): 0.0004556772, 0.0003787761

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 987
rank avg (pred): 0.106 +- 0.138
mrr vals (pred, true): 0.303, 0.310
batch losses (mrrl, rdl): 0.0004828042, 8.09817e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 61
rank avg (pred): 0.099 +- 0.137
mrr vals (pred, true): 0.117, 0.073
batch losses (mrrl, rdl): 0.0453031994, 1.6058e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.339 +- 0.250
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0016664879, 0.0002648078

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 841
rank avg (pred): 0.432 +- 0.315
mrr vals (pred, true): 0.069, 0.157
batch losses (mrrl, rdl): 0.0760968924, 0.0009720259

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 89
rank avg (pred): 0.331 +- 0.256
mrr vals (pred, true): 0.061, 0.055
batch losses (mrrl, rdl): 0.0011805792, 0.0004440256

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 252
rank avg (pred): 0.155 +- 0.194
mrr vals (pred, true): 0.088, 0.076
batch losses (mrrl, rdl): 0.0146030765, 3.78956e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1158
rank avg (pred): 0.128 +- 0.102
mrr vals (pred, true): 0.057, 0.104
batch losses (mrrl, rdl): 0.0222954433, 0.0002489562

Epoch over!
epoch time: 54.819

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 390
rank avg (pred): 0.375 +- 0.271
mrr vals (pred, true): 0.044, 0.050
batch losses (mrrl, rdl): 0.0003154722, 0.000584877

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 297
rank avg (pred): 0.125 +- 0.147
mrr vals (pred, true): 0.090, 0.057
batch losses (mrrl, rdl): 0.0159925167, 4.48982e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 289
rank avg (pred): 0.065 +- 0.098
mrr vals (pred, true): 0.134, 0.137
batch losses (mrrl, rdl): 7.38842e-05, 2.68092e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 759
rank avg (pred): 0.394 +- 0.299
mrr vals (pred, true): 0.065, 0.146
batch losses (mrrl, rdl): 0.0655588582, 0.0006621612

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 825
rank avg (pred): 0.061 +- 0.076
mrr vals (pred, true): 0.184, 0.219
batch losses (mrrl, rdl): 0.0122513846, 1.37729e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 867
rank avg (pred): 0.313 +- 0.277
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0194529034, 0.0004727704

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.079 +- 0.090
mrr vals (pred, true): 0.211, 0.209
batch losses (mrrl, rdl): 6.1952e-05, 2.30279e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 423
rank avg (pred): 0.338 +- 0.292
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.85712e-05, 0.0005326237

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 340
rank avg (pred): 0.323 +- 0.286
mrr vals (pred, true): 0.069, 0.052
batch losses (mrrl, rdl): 0.0037119619, 0.0003602316

Epoch over!
epoch time: 55.424

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 93
rank avg (pred): 0.326 +- 0.255
mrr vals (pred, true): 0.048, 0.008
batch losses (mrrl, rdl): 3.49145e-05, 0.000231803

running batch: 500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.362 +- 0.302
mrr vals (pred, true): 0.065, 0.047
batch losses (mrrl, rdl): 0.002370798, 0.00021657

running batch: 1000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 557
rank avg (pred): 0.106 +- 0.064
mrr vals (pred, true): 0.061, 0.082
batch losses (mrrl, rdl): 0.0011050398, 0.0005737523

running batch: 1500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 249
rank avg (pred): 0.107 +- 0.142
mrr vals (pred, true): 0.125, 0.086
batch losses (mrrl, rdl): 0.0564692393, 1.99196e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 477
rank avg (pred): 0.403 +- 0.338
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.322e-06, 3.63725e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1208
rank avg (pred): 0.349 +- 0.289
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0015006447, 0.0004283508

running batch: 3000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 420
rank avg (pred): 0.393 +- 0.333
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 6.06185e-05, 0.0001583358

running batch: 3500 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.399 +- 0.350
mrr vals (pred, true): 0.079, 0.162
batch losses (mrrl, rdl): 0.0697054863, 0.0008008202

running batch: 4000 / 4376 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 569
rank avg (pred): 0.357 +- 0.308
mrr vals (pred, true): 0.069, 0.026
batch losses (mrrl, rdl): 0.0034693624, 1.28115e-05

Epoch over!
epoch time: 53.683

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.390 +- 0.352
mrr vals (pred, true): 0.071, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   99 	     0 	 0.09800 	 0.00016 	 m..s
   20 	     1 	 0.06384 	 0.00019 	 m..s
  102 	     2 	 0.10037 	 0.00019 	 MISS
   37 	     3 	 0.06618 	 0.00020 	 m..s
   94 	     4 	 0.09352 	 0.00020 	 m..s
   93 	     5 	 0.09332 	 0.00021 	 m..s
   23 	     6 	 0.06407 	 0.00021 	 m..s
    1 	     7 	 0.06066 	 0.00022 	 m..s
   12 	     8 	 0.06231 	 0.00023 	 m..s
   95 	     9 	 0.09393 	 0.00023 	 m..s
    3 	    10 	 0.06186 	 0.00023 	 m..s
   48 	    11 	 0.06756 	 0.00024 	 m..s
    6 	    12 	 0.06209 	 0.00024 	 m..s
   60 	    13 	 0.07078 	 0.00024 	 m..s
   24 	    14 	 0.06415 	 0.00025 	 m..s
   52 	    15 	 0.06910 	 0.00026 	 m..s
    2 	    16 	 0.06078 	 0.00026 	 m..s
   32 	    17 	 0.06497 	 0.00026 	 m..s
   29 	    18 	 0.06462 	 0.00027 	 m..s
   54 	    19 	 0.06967 	 0.00028 	 m..s
   73 	    20 	 0.08194 	 0.00028 	 m..s
   77 	    21 	 0.08681 	 0.00028 	 m..s
   27 	    22 	 0.06438 	 0.00028 	 m..s
   13 	    23 	 0.06232 	 0.00030 	 m..s
   26 	    24 	 0.06431 	 0.00031 	 m..s
   78 	    25 	 0.08716 	 0.00031 	 m..s
   14 	    26 	 0.06257 	 0.00032 	 m..s
   39 	    27 	 0.06645 	 0.00032 	 m..s
   18 	    28 	 0.06363 	 0.00032 	 m..s
   25 	    29 	 0.06424 	 0.00032 	 m..s
   21 	    30 	 0.06387 	 0.00032 	 m..s
   91 	    31 	 0.09325 	 0.00033 	 m..s
   36 	    32 	 0.06551 	 0.00033 	 m..s
   28 	    33 	 0.06458 	 0.00033 	 m..s
   92 	    34 	 0.09331 	 0.00033 	 m..s
    0 	    35 	 0.06065 	 0.00034 	 m..s
   35 	    36 	 0.06527 	 0.00035 	 m..s
   85 	    37 	 0.09188 	 0.00036 	 m..s
   31 	    38 	 0.06483 	 0.00036 	 m..s
   45 	    39 	 0.06719 	 0.00041 	 m..s
   62 	    40 	 0.07288 	 0.00042 	 m..s
   16 	    41 	 0.06283 	 0.00045 	 m..s
   51 	    42 	 0.06825 	 0.00087 	 m..s
   33 	    43 	 0.06507 	 0.00088 	 m..s
   81 	    44 	 0.09107 	 0.00136 	 m..s
   63 	    45 	 0.07299 	 0.00194 	 m..s
    7 	    46 	 0.06214 	 0.00217 	 m..s
   55 	    47 	 0.06989 	 0.00246 	 m..s
    5 	    48 	 0.06197 	 0.00260 	 m..s
    8 	    49 	 0.06219 	 0.00512 	 m..s
   11 	    50 	 0.06231 	 0.00908 	 m..s
    4 	    51 	 0.06195 	 0.01079 	 m..s
   10 	    52 	 0.06230 	 0.02332 	 m..s
   15 	    53 	 0.06278 	 0.02900 	 m..s
   22 	    54 	 0.06400 	 0.02952 	 m..s
    9 	    55 	 0.06223 	 0.03364 	 ~...
   19 	    56 	 0.06366 	 0.03716 	 ~...
   44 	    57 	 0.06707 	 0.04668 	 ~...
   30 	    58 	 0.06471 	 0.05032 	 ~...
   41 	    59 	 0.06669 	 0.05294 	 ~...
   17 	    60 	 0.06329 	 0.05391 	 ~...
   98 	    61 	 0.09782 	 0.05393 	 m..s
   43 	    62 	 0.06693 	 0.05686 	 ~...
   96 	    63 	 0.09607 	 0.05707 	 m..s
   40 	    64 	 0.06650 	 0.06079 	 ~...
   53 	    65 	 0.06933 	 0.06196 	 ~...
   42 	    66 	 0.06672 	 0.06213 	 ~...
  100 	    67 	 0.09932 	 0.06577 	 m..s
  107 	    68 	 0.10729 	 0.06802 	 m..s
   49 	    69 	 0.06768 	 0.06807 	 ~...
   38 	    70 	 0.06643 	 0.06858 	 ~...
   50 	    71 	 0.06799 	 0.07151 	 ~...
   64 	    72 	 0.07371 	 0.07259 	 ~...
   58 	    73 	 0.07002 	 0.07672 	 ~...
   56 	    74 	 0.06992 	 0.07741 	 ~...
   57 	    75 	 0.06995 	 0.07774 	 ~...
  101 	    76 	 0.10002 	 0.08037 	 ~...
   34 	    77 	 0.06515 	 0.08123 	 ~...
  105 	    78 	 0.10689 	 0.08147 	 ~...
   47 	    79 	 0.06754 	 0.08249 	 ~...
   46 	    80 	 0.06733 	 0.08581 	 ~...
   66 	    81 	 0.07478 	 0.08771 	 ~...
  109 	    82 	 0.11290 	 0.09117 	 ~...
   83 	    83 	 0.09122 	 0.09280 	 ~...
  112 	    84 	 0.14677 	 0.09285 	 m..s
  104 	    85 	 0.10628 	 0.09490 	 ~...
  103 	    86 	 0.10590 	 0.09562 	 ~...
   59 	    87 	 0.07012 	 0.09668 	 ~...
  106 	    88 	 0.10712 	 0.09734 	 ~...
  113 	    89 	 0.14724 	 0.09745 	 m..s
   83 	    90 	 0.09122 	 0.10138 	 ~...
   80 	    91 	 0.09101 	 0.10204 	 ~...
   72 	    92 	 0.07878 	 0.10243 	 ~...
   68 	    93 	 0.07589 	 0.10485 	 ~...
   65 	    94 	 0.07373 	 0.10666 	 m..s
   69 	    95 	 0.07659 	 0.10673 	 m..s
  115 	    96 	 0.18612 	 0.10857 	 m..s
  114 	    97 	 0.15195 	 0.10961 	 m..s
   67 	    98 	 0.07555 	 0.11128 	 m..s
   61 	    99 	 0.07270 	 0.12731 	 m..s
   82 	   100 	 0.09107 	 0.12979 	 m..s
  108 	   101 	 0.11019 	 0.13179 	 ~...
   70 	   102 	 0.07780 	 0.13857 	 m..s
  111 	   103 	 0.12510 	 0.13901 	 ~...
  110 	   104 	 0.11635 	 0.14057 	 ~...
   76 	   105 	 0.08411 	 0.15698 	 m..s
   88 	   106 	 0.09285 	 0.15989 	 m..s
   79 	   107 	 0.09064 	 0.16282 	 m..s
   86 	   108 	 0.09272 	 0.16283 	 m..s
   74 	   109 	 0.08259 	 0.16364 	 m..s
   97 	   110 	 0.09720 	 0.16480 	 m..s
   75 	   111 	 0.08406 	 0.16986 	 m..s
   71 	   112 	 0.07876 	 0.17079 	 m..s
   90 	   113 	 0.09307 	 0.17132 	 m..s
   89 	   114 	 0.09292 	 0.18166 	 m..s
   87 	   115 	 0.09275 	 0.19866 	 MISS
  117 	   116 	 0.26265 	 0.20920 	 m..s
  116 	   117 	 0.25458 	 0.22383 	 m..s
  119 	   118 	 0.29070 	 0.26518 	 ~...
  120 	   119 	 0.30465 	 0.26738 	 m..s
  118 	   120 	 0.27410 	 0.28630 	 ~...
==========================================
r_mrr = 0.6826278567314148
r2_mrr = 0.323678195476532
spearmanr_mrr@5 = 0.9396758079528809
spearmanr_mrr@10 = 0.9433242678642273
spearmanr_mrr@50 = 0.9140487313270569
spearmanr_mrr@100 = 0.8565628528594971
spearmanr_mrr@All = 0.8614494800567627
==========================================
test time: 0.446
Done Testing dataset DBpedia50
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.337 +- 0.209
mrr vals (pred, true): 0.069, 0.039

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.05415 	 0.03433 	 ~...
   30 	     1 	 0.05640 	 0.03464 	 ~...
   53 	     2 	 0.06177 	 0.03589 	 ~...
   26 	     3 	 0.05532 	 0.03662 	 ~...
   41 	     4 	 0.05986 	 0.03668 	 ~...
   60 	     5 	 0.06486 	 0.03672 	 ~...
   14 	     6 	 0.05356 	 0.03692 	 ~...
   70 	     7 	 0.06904 	 0.03702 	 m..s
   43 	     8 	 0.06003 	 0.03703 	 ~...
   18 	     9 	 0.05445 	 0.03706 	 ~...
   44 	    10 	 0.06004 	 0.03713 	 ~...
    1 	    11 	 0.04945 	 0.03741 	 ~...
   46 	    12 	 0.06011 	 0.03743 	 ~...
   19 	    13 	 0.05488 	 0.03757 	 ~...
   15 	    14 	 0.05385 	 0.03759 	 ~...
   20 	    15 	 0.05492 	 0.03762 	 ~...
   10 	    16 	 0.05296 	 0.03766 	 ~...
   72 	    17 	 0.06924 	 0.03809 	 m..s
   24 	    18 	 0.05523 	 0.03811 	 ~...
    4 	    19 	 0.05180 	 0.03843 	 ~...
    6 	    20 	 0.05187 	 0.03874 	 ~...
   69 	    21 	 0.06874 	 0.03880 	 ~...
   12 	    22 	 0.05330 	 0.03882 	 ~...
   13 	    23 	 0.05342 	 0.03884 	 ~...
   73 	    24 	 0.06927 	 0.03886 	 m..s
    0 	    25 	 0.04929 	 0.03897 	 ~...
   55 	    26 	 0.06259 	 0.03920 	 ~...
    8 	    27 	 0.05238 	 0.03935 	 ~...
   68 	    28 	 0.06767 	 0.03942 	 ~...
   40 	    29 	 0.05979 	 0.03943 	 ~...
   27 	    30 	 0.05569 	 0.03954 	 ~...
   47 	    31 	 0.06046 	 0.03962 	 ~...
   31 	    32 	 0.05645 	 0.03972 	 ~...
   45 	    33 	 0.06004 	 0.03982 	 ~...
   16 	    34 	 0.05410 	 0.04002 	 ~...
   63 	    35 	 0.06554 	 0.04012 	 ~...
   59 	    36 	 0.06425 	 0.04013 	 ~...
   25 	    37 	 0.05526 	 0.04023 	 ~...
   36 	    38 	 0.05809 	 0.04036 	 ~...
   34 	    39 	 0.05668 	 0.04060 	 ~...
   71 	    40 	 0.06914 	 0.04078 	 ~...
   11 	    41 	 0.05305 	 0.04108 	 ~...
   42 	    42 	 0.05991 	 0.04119 	 ~...
   49 	    43 	 0.06079 	 0.04126 	 ~...
   57 	    44 	 0.06324 	 0.04143 	 ~...
   22 	    45 	 0.05498 	 0.04143 	 ~...
    7 	    46 	 0.05224 	 0.04156 	 ~...
    3 	    47 	 0.05142 	 0.04172 	 ~...
    5 	    48 	 0.05180 	 0.04186 	 ~...
   21 	    49 	 0.05496 	 0.04190 	 ~...
   23 	    50 	 0.05505 	 0.04191 	 ~...
   56 	    51 	 0.06323 	 0.04227 	 ~...
   39 	    52 	 0.05959 	 0.04593 	 ~...
   51 	    53 	 0.06094 	 0.04611 	 ~...
   28 	    54 	 0.05579 	 0.04648 	 ~...
   33 	    55 	 0.05654 	 0.04950 	 ~...
   80 	    56 	 0.08172 	 0.06644 	 ~...
   84 	    57 	 0.10628 	 0.06896 	 m..s
   81 	    58 	 0.08753 	 0.07231 	 ~...
   82 	    59 	 0.09499 	 0.07384 	 ~...
   83 	    60 	 0.09755 	 0.07553 	 ~...
   48 	    61 	 0.06048 	 0.08768 	 ~...
    9 	    62 	 0.05289 	 0.08934 	 m..s
   29 	    63 	 0.05621 	 0.08985 	 m..s
    2 	    64 	 0.05005 	 0.09072 	 m..s
   37 	    65 	 0.05843 	 0.09154 	 m..s
   65 	    66 	 0.06612 	 0.09271 	 ~...
   35 	    67 	 0.05673 	 0.09293 	 m..s
   32 	    68 	 0.05650 	 0.09637 	 m..s
   50 	    69 	 0.06088 	 0.09724 	 m..s
   58 	    70 	 0.06413 	 0.09818 	 m..s
   38 	    71 	 0.05859 	 0.09954 	 m..s
   54 	    72 	 0.06179 	 0.10238 	 m..s
   52 	    73 	 0.06176 	 0.10320 	 m..s
   62 	    74 	 0.06507 	 0.10944 	 m..s
   66 	    75 	 0.06671 	 0.11644 	 m..s
   64 	    76 	 0.06572 	 0.11872 	 m..s
   85 	    77 	 0.11043 	 0.12187 	 ~...
   86 	    78 	 0.11144 	 0.12289 	 ~...
   88 	    79 	 0.12728 	 0.12522 	 ~...
   87 	    80 	 0.12680 	 0.12528 	 ~...
   61 	    81 	 0.06500 	 0.12726 	 m..s
   67 	    82 	 0.06714 	 0.12771 	 m..s
   75 	    83 	 0.07044 	 0.12885 	 m..s
   77 	    84 	 0.07171 	 0.13087 	 m..s
   74 	    85 	 0.06975 	 0.13723 	 m..s
   76 	    86 	 0.07128 	 0.13895 	 m..s
   78 	    87 	 0.07270 	 0.14083 	 m..s
   90 	    88 	 0.15746 	 0.14348 	 ~...
   79 	    89 	 0.07490 	 0.14453 	 m..s
   94 	    90 	 0.19663 	 0.15235 	 m..s
   91 	    91 	 0.16468 	 0.15601 	 ~...
   89 	    92 	 0.14758 	 0.16535 	 ~...
   92 	    93 	 0.17865 	 0.16997 	 ~...
   92 	    94 	 0.17865 	 0.23331 	 m..s
   95 	    95 	 0.24446 	 0.25103 	 ~...
   96 	    96 	 0.27696 	 0.30753 	 m..s
   97 	    97 	 0.37007 	 0.31332 	 m..s
   98 	    98 	 0.44241 	 0.42704 	 ~...
  103 	    99 	 0.55697 	 0.43552 	 MISS
  107 	   100 	 0.56429 	 0.44884 	 MISS
   99 	   101 	 0.48829 	 0.48233 	 ~...
  104 	   102 	 0.55961 	 0.50669 	 m..s
  101 	   103 	 0.53714 	 0.51624 	 ~...
  100 	   104 	 0.52059 	 0.51696 	 ~...
  105 	   105 	 0.56073 	 0.51726 	 m..s
  102 	   106 	 0.54056 	 0.53031 	 ~...
  110 	   107 	 0.57671 	 0.54688 	 ~...
  106 	   108 	 0.56419 	 0.54800 	 ~...
  111 	   109 	 0.57900 	 0.55497 	 ~...
  108 	   110 	 0.56558 	 0.56975 	 ~...
  109 	   111 	 0.56670 	 0.57994 	 ~...
  116 	   112 	 0.63161 	 0.58855 	 m..s
  114 	   113 	 0.60359 	 0.60053 	 ~...
  113 	   114 	 0.60322 	 0.60193 	 ~...
  119 	   115 	 0.64729 	 0.60297 	 m..s
  118 	   116 	 0.63846 	 0.61320 	 ~...
  117 	   117 	 0.63542 	 0.61930 	 ~...
  120 	   118 	 0.64841 	 0.62441 	 ~...
  115 	   119 	 0.62027 	 0.62817 	 ~...
  112 	   120 	 0.60247 	 0.62963 	 ~...
==========================================
r_mrr = 0.9869411587715149
r2_mrr = 0.97103351354599
spearmanr_mrr@5 = 0.9569292068481445
spearmanr_mrr@10 = 0.9702156186103821
spearmanr_mrr@50 = 0.9951220154762268
spearmanr_mrr@100 = 0.9931245446205139
spearmanr_mrr@All = 0.9930315017700195
==========================================
test time: 0.449
Done Testing dataset UMLS
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.359 +- 0.275
mrr vals (pred, true): 0.044, 0.046

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   69 	     0 	 0.04289 	 0.03862 	 ~...
   35 	     1 	 0.03805 	 0.04155 	 ~...
    0 	     2 	 0.03107 	 0.04160 	 ~...
    3 	     3 	 0.03175 	 0.04162 	 ~...
    1 	     4 	 0.03120 	 0.04173 	 ~...
   62 	     5 	 0.04170 	 0.04206 	 ~...
   59 	     6 	 0.04140 	 0.04225 	 ~...
   34 	     7 	 0.03785 	 0.04225 	 ~...
    5 	     8 	 0.03196 	 0.04238 	 ~...
   30 	     9 	 0.03703 	 0.04244 	 ~...
    9 	    10 	 0.03302 	 0.04257 	 ~...
   41 	    11 	 0.03896 	 0.04265 	 ~...
   15 	    12 	 0.03374 	 0.04276 	 ~...
   14 	    13 	 0.03352 	 0.04276 	 ~...
   12 	    14 	 0.03339 	 0.04286 	 ~...
   11 	    15 	 0.03322 	 0.04288 	 ~...
   48 	    16 	 0.03983 	 0.04312 	 ~...
   51 	    17 	 0.04047 	 0.04323 	 ~...
   71 	    18 	 0.04408 	 0.04330 	 ~...
   33 	    19 	 0.03768 	 0.04341 	 ~...
   58 	    20 	 0.04132 	 0.04343 	 ~...
   37 	    21 	 0.03841 	 0.04346 	 ~...
   65 	    22 	 0.04186 	 0.04348 	 ~...
   27 	    23 	 0.03639 	 0.04363 	 ~...
   21 	    24 	 0.03504 	 0.04364 	 ~...
   70 	    25 	 0.04343 	 0.04371 	 ~...
   31 	    26 	 0.03712 	 0.04384 	 ~...
   52 	    27 	 0.04050 	 0.04385 	 ~...
   10 	    28 	 0.03311 	 0.04387 	 ~...
   49 	    29 	 0.03994 	 0.04393 	 ~...
   20 	    30 	 0.03474 	 0.04406 	 ~...
   26 	    31 	 0.03629 	 0.04412 	 ~...
   53 	    32 	 0.04075 	 0.04421 	 ~...
    2 	    33 	 0.03124 	 0.04426 	 ~...
   57 	    34 	 0.04130 	 0.04426 	 ~...
    8 	    35 	 0.03277 	 0.04434 	 ~...
   47 	    36 	 0.03980 	 0.04435 	 ~...
   45 	    37 	 0.03958 	 0.04439 	 ~...
   22 	    38 	 0.03513 	 0.04453 	 ~...
   17 	    39 	 0.03419 	 0.04460 	 ~...
   50 	    40 	 0.03998 	 0.04462 	 ~...
   13 	    41 	 0.03350 	 0.04463 	 ~...
   39 	    42 	 0.03856 	 0.04498 	 ~...
   32 	    43 	 0.03744 	 0.04498 	 ~...
   19 	    44 	 0.03464 	 0.04514 	 ~...
   43 	    45 	 0.03925 	 0.04515 	 ~...
    6 	    46 	 0.03203 	 0.04537 	 ~...
   36 	    47 	 0.03824 	 0.04541 	 ~...
   64 	    48 	 0.04186 	 0.04542 	 ~...
   38 	    49 	 0.03854 	 0.04569 	 ~...
   73 	    50 	 0.04441 	 0.04593 	 ~...
   40 	    51 	 0.03860 	 0.04596 	 ~...
   44 	    52 	 0.03954 	 0.04602 	 ~...
   42 	    53 	 0.03916 	 0.04622 	 ~...
   16 	    54 	 0.03413 	 0.04642 	 ~...
   24 	    55 	 0.03566 	 0.04656 	 ~...
   54 	    56 	 0.04096 	 0.04694 	 ~...
   23 	    57 	 0.03566 	 0.04726 	 ~...
   46 	    58 	 0.03962 	 0.04733 	 ~...
   29 	    59 	 0.03701 	 0.04760 	 ~...
    7 	    60 	 0.03270 	 0.04791 	 ~...
    4 	    61 	 0.03190 	 0.04826 	 ~...
   78 	    62 	 0.04557 	 0.04871 	 ~...
   68 	    63 	 0.04282 	 0.04917 	 ~...
   63 	    64 	 0.04171 	 0.05011 	 ~...
   67 	    65 	 0.04254 	 0.05014 	 ~...
   60 	    66 	 0.04153 	 0.05039 	 ~...
   61 	    67 	 0.04169 	 0.05184 	 ~...
   55 	    68 	 0.04099 	 0.05198 	 ~...
   28 	    69 	 0.03678 	 0.05220 	 ~...
   76 	    70 	 0.04490 	 0.05291 	 ~...
   18 	    71 	 0.03444 	 0.05405 	 ~...
   25 	    72 	 0.03601 	 0.05416 	 ~...
   79 	    73 	 0.04847 	 0.05427 	 ~...
   75 	    74 	 0.04480 	 0.05460 	 ~...
   66 	    75 	 0.04211 	 0.05472 	 ~...
   74 	    76 	 0.04466 	 0.05514 	 ~...
   77 	    77 	 0.04523 	 0.05586 	 ~...
   72 	    78 	 0.04428 	 0.05612 	 ~...
   56 	    79 	 0.04122 	 0.05665 	 ~...
   81 	    80 	 0.16181 	 0.13306 	 ~...
   83 	    81 	 0.18062 	 0.16203 	 ~...
   80 	    82 	 0.14469 	 0.17534 	 m..s
   86 	    83 	 0.20708 	 0.17873 	 ~...
   81 	    84 	 0.16181 	 0.18038 	 ~...
   91 	    85 	 0.22940 	 0.19216 	 m..s
   89 	    86 	 0.22090 	 0.19222 	 ~...
   84 	    87 	 0.18932 	 0.19718 	 ~...
   88 	    88 	 0.21858 	 0.20108 	 ~...
   90 	    89 	 0.22354 	 0.20417 	 ~...
   95 	    90 	 0.24310 	 0.21694 	 ~...
   85 	    91 	 0.20090 	 0.21728 	 ~...
   93 	    92 	 0.23538 	 0.21787 	 ~...
   99 	    93 	 0.25086 	 0.22013 	 m..s
  107 	    94 	 0.26912 	 0.22245 	 m..s
  106 	    95 	 0.26853 	 0.22748 	 m..s
   87 	    96 	 0.20870 	 0.23013 	 ~...
  101 	    97 	 0.25225 	 0.24195 	 ~...
   94 	    98 	 0.24155 	 0.26184 	 ~...
  102 	    99 	 0.26211 	 0.26262 	 ~...
  105 	   100 	 0.26779 	 0.26491 	 ~...
   92 	   101 	 0.23270 	 0.26520 	 m..s
  100 	   102 	 0.25117 	 0.26647 	 ~...
  108 	   103 	 0.27250 	 0.27037 	 ~...
  111 	   104 	 0.28633 	 0.27297 	 ~...
  103 	   105 	 0.26530 	 0.27588 	 ~...
  110 	   106 	 0.28214 	 0.28181 	 ~...
  112 	   107 	 0.29382 	 0.28547 	 ~...
  115 	   108 	 0.29709 	 0.28704 	 ~...
   97 	   109 	 0.24943 	 0.28917 	 m..s
  109 	   110 	 0.28156 	 0.28964 	 ~...
  116 	   111 	 0.31353 	 0.29096 	 ~...
  113 	   112 	 0.29482 	 0.29543 	 ~...
  104 	   113 	 0.26728 	 0.29653 	 ~...
  118 	   114 	 0.31808 	 0.29805 	 ~...
   96 	   115 	 0.24939 	 0.29913 	 m..s
  114 	   116 	 0.29673 	 0.30314 	 ~...
   98 	   117 	 0.24950 	 0.30319 	 m..s
  120 	   118 	 0.32282 	 0.30577 	 ~...
  117 	   119 	 0.31488 	 0.32833 	 ~...
  119 	   120 	 0.32106 	 0.36279 	 m..s
==========================================
r_mrr = 0.9888233542442322
r2_mrr = 0.9737630486488342
spearmanr_mrr@5 = 0.8785160183906555
spearmanr_mrr@10 = 0.7307199835777283
spearmanr_mrr@50 = 0.9903923273086548
spearmanr_mrr@100 = 0.9965969324111938
spearmanr_mrr@All = 0.9969186782836914
==========================================
test time: 0.48
Done Testing dataset Kinships
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.368 +- 0.295
mrr vals (pred, true): 0.072, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   78 	     0 	 0.07190 	 0.00594 	 m..s
   22 	     1 	 0.06409 	 0.00600 	 m..s
   35 	     2 	 0.06570 	 0.00605 	 m..s
   85 	     3 	 0.07389 	 0.00608 	 m..s
   44 	     4 	 0.06763 	 0.00610 	 m..s
   37 	     5 	 0.06611 	 0.00613 	 m..s
   14 	     6 	 0.06195 	 0.00615 	 m..s
   21 	     7 	 0.06403 	 0.00617 	 m..s
   73 	     8 	 0.07105 	 0.00619 	 m..s
    2 	     9 	 0.05868 	 0.00620 	 m..s
   13 	    10 	 0.06155 	 0.00620 	 m..s
   89 	    11 	 0.07611 	 0.00624 	 m..s
   12 	    12 	 0.06153 	 0.00624 	 m..s
   30 	    13 	 0.06513 	 0.00625 	 m..s
    0 	    14 	 0.05841 	 0.00625 	 m..s
    1 	    15 	 0.05844 	 0.00630 	 m..s
   31 	    16 	 0.06519 	 0.00631 	 m..s
   66 	    17 	 0.06970 	 0.00633 	 m..s
   29 	    18 	 0.06482 	 0.00633 	 m..s
   16 	    19 	 0.06239 	 0.00634 	 m..s
   33 	    20 	 0.06549 	 0.00635 	 m..s
   38 	    21 	 0.06640 	 0.00636 	 m..s
   60 	    22 	 0.06894 	 0.00636 	 m..s
    6 	    23 	 0.06113 	 0.00637 	 m..s
   49 	    24 	 0.06789 	 0.00638 	 m..s
   28 	    25 	 0.06474 	 0.00638 	 m..s
   43 	    26 	 0.06729 	 0.00641 	 m..s
   93 	    27 	 0.07820 	 0.00642 	 m..s
    3 	    28 	 0.06072 	 0.00642 	 m..s
   57 	    29 	 0.06857 	 0.00644 	 m..s
   27 	    30 	 0.06464 	 0.00644 	 m..s
   72 	    31 	 0.07096 	 0.00646 	 m..s
   48 	    32 	 0.06785 	 0.00647 	 m..s
   96 	    33 	 0.07926 	 0.00648 	 m..s
   98 	    34 	 0.07935 	 0.00648 	 m..s
   24 	    35 	 0.06439 	 0.00651 	 m..s
   92 	    36 	 0.07802 	 0.00653 	 m..s
   26 	    37 	 0.06448 	 0.00653 	 m..s
   19 	    38 	 0.06368 	 0.00656 	 m..s
   51 	    39 	 0.06810 	 0.00663 	 m..s
   69 	    40 	 0.07017 	 0.00666 	 m..s
   59 	    41 	 0.06891 	 0.00668 	 m..s
   56 	    42 	 0.06855 	 0.00668 	 m..s
   34 	    43 	 0.06565 	 0.00670 	 m..s
   25 	    44 	 0.06440 	 0.00673 	 m..s
   70 	    45 	 0.07056 	 0.00675 	 m..s
   84 	    46 	 0.07381 	 0.00681 	 m..s
    7 	    47 	 0.06123 	 0.01052 	 m..s
    5 	    48 	 0.06092 	 0.01120 	 m..s
    4 	    49 	 0.06087 	 0.01263 	 m..s
   11 	    50 	 0.06151 	 0.01398 	 m..s
    8 	    51 	 0.06131 	 0.01482 	 m..s
   20 	    52 	 0.06373 	 0.01965 	 m..s
   23 	    53 	 0.06428 	 0.02307 	 m..s
   53 	    54 	 0.06820 	 0.02347 	 m..s
   10 	    55 	 0.06150 	 0.02570 	 m..s
   46 	    56 	 0.06777 	 0.02736 	 m..s
   45 	    57 	 0.06770 	 0.02782 	 m..s
   15 	    58 	 0.06230 	 0.03479 	 ~...
   75 	    59 	 0.07128 	 0.03515 	 m..s
   50 	    60 	 0.06796 	 0.03581 	 m..s
   74 	    61 	 0.07117 	 0.03700 	 m..s
   62 	    62 	 0.06909 	 0.04005 	 ~...
   71 	    63 	 0.07079 	 0.04205 	 ~...
    9 	    64 	 0.06137 	 0.05011 	 ~...
   80 	    65 	 0.07280 	 0.05350 	 ~...
   81 	    66 	 0.07329 	 0.05706 	 ~...
   40 	    67 	 0.06705 	 0.05761 	 ~...
   41 	    68 	 0.06729 	 0.05813 	 ~...
   47 	    69 	 0.06780 	 0.05950 	 ~...
   83 	    70 	 0.07376 	 0.06318 	 ~...
   68 	    71 	 0.07010 	 0.06335 	 ~...
  100 	    72 	 0.08921 	 0.06497 	 ~...
   18 	    73 	 0.06323 	 0.06534 	 ~...
   64 	    74 	 0.06927 	 0.06588 	 ~...
   82 	    75 	 0.07347 	 0.07065 	 ~...
   99 	    76 	 0.08369 	 0.07302 	 ~...
  103 	    77 	 0.10418 	 0.07343 	 m..s
  104 	    78 	 0.10476 	 0.07721 	 ~...
   95 	    79 	 0.07849 	 0.07887 	 ~...
   36 	    80 	 0.06611 	 0.07904 	 ~...
   52 	    81 	 0.06817 	 0.07958 	 ~...
  110 	    82 	 0.13251 	 0.08407 	 m..s
   41 	    83 	 0.06729 	 0.08468 	 ~...
  109 	    84 	 0.12518 	 0.08724 	 m..s
   77 	    85 	 0.07183 	 0.08823 	 ~...
   32 	    86 	 0.06546 	 0.09192 	 ~...
   39 	    87 	 0.06697 	 0.09456 	 ~...
   54 	    88 	 0.06843 	 0.09492 	 ~...
   79 	    89 	 0.07190 	 0.09495 	 ~...
   76 	    90 	 0.07134 	 0.09507 	 ~...
   67 	    91 	 0.07000 	 0.09526 	 ~...
   65 	    92 	 0.06957 	 0.09660 	 ~...
   55 	    93 	 0.06850 	 0.09679 	 ~...
   17 	    94 	 0.06264 	 0.09721 	 m..s
   58 	    95 	 0.06858 	 0.09787 	 ~...
  102 	    96 	 0.09444 	 0.09837 	 ~...
   61 	    97 	 0.06897 	 0.10294 	 m..s
   63 	    98 	 0.06921 	 0.10484 	 m..s
   88 	    99 	 0.07566 	 0.10631 	 m..s
   86 	   100 	 0.07457 	 0.10766 	 m..s
   94 	   101 	 0.07838 	 0.10837 	 ~...
   90 	   102 	 0.07636 	 0.11051 	 m..s
  101 	   103 	 0.09342 	 0.11092 	 ~...
   87 	   104 	 0.07466 	 0.11815 	 m..s
   97 	   105 	 0.07934 	 0.12089 	 m..s
  113 	   106 	 0.16239 	 0.12209 	 m..s
   91 	   107 	 0.07743 	 0.12399 	 m..s
  108 	   108 	 0.11443 	 0.12784 	 ~...
  112 	   109 	 0.16189 	 0.12940 	 m..s
  106 	   110 	 0.10952 	 0.14556 	 m..s
  105 	   111 	 0.10671 	 0.14815 	 m..s
  107 	   112 	 0.11071 	 0.15744 	 m..s
  115 	   113 	 0.20072 	 0.16005 	 m..s
  111 	   114 	 0.13825 	 0.16380 	 ~...
  114 	   115 	 0.16914 	 0.19480 	 ~...
  117 	   116 	 0.27018 	 0.21513 	 m..s
  116 	   117 	 0.26462 	 0.21936 	 m..s
  119 	   118 	 0.29057 	 0.24199 	 m..s
  118 	   119 	 0.27943 	 0.26468 	 ~...
  120 	   120 	 0.30050 	 0.27828 	 ~...
==========================================
r_mrr = 0.7985418438911438
r2_mrr = 0.44437021017074585
spearmanr_mrr@5 = 0.9941369891166687
spearmanr_mrr@10 = 0.9674568772315979
spearmanr_mrr@50 = 0.9796373248100281
spearmanr_mrr@100 = 0.8638853430747986
spearmanr_mrr@All = 0.8635578751564026
==========================================
test time: 0.548
Done Testing dataset OpenEA
total time taken: 844.0220015048981
training time taken: 810.7183318138123
TWIG out ;))
=========================================================
---------------------------------------------------------
Running a TWIG experiment with tag: TransE-omit-DBpedia50
---------------------------------------------------------
=========================================================
Using random seed: 1278868043164098
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1077, 456, 1064, 272, 856, 893, 36, 750, 345, 353, 520, 33, 92, 1069, 708, 584, 127, 366, 829, 297, 874, 692, 655, 538, 508, 365, 136, 278, 43, 996, 315, 1089, 801, 484, 757, 1145, 349, 35, 835, 977, 567, 71, 1091, 131, 405, 841, 1005, 731, 806, 215, 906, 370, 362, 1079, 677, 1016, 441, 727, 209, 51, 732, 1198, 400, 882, 1210, 49, 1066, 803, 381, 66, 243, 627, 700, 336, 438, 328, 1052, 280, 292, 583, 1202, 1189, 377, 755, 28, 786, 1211, 1004, 901, 960, 702, 510, 957, 718, 1045, 178, 997, 836, 1036, 129, 1044, 282, 767, 247, 705, 502, 637, 176, 964, 859, 237, 16, 724, 313, 142, 134, 251, 1058, 428, 860, 1107]
valid_ids (0): []
train_ids (1094): [1158, 1169, 972, 380, 395, 20, 626, 808, 437, 917, 79, 97, 905, 975, 999, 787, 926, 891, 631, 982, 943, 476, 948, 48, 919, 154, 958, 312, 887, 244, 440, 717, 654, 711, 710, 415, 760, 39, 331, 1012, 661, 433, 858, 98, 895, 384, 148, 141, 1125, 200, 341, 847, 598, 201, 46, 832, 872, 612, 82, 928, 152, 232, 110, 1050, 1183, 1048, 942, 213, 296, 530, 1203, 898, 897, 159, 684, 485, 844, 431, 685, 956, 421, 843, 607, 116, 686, 1108, 223, 746, 1090, 991, 1018, 308, 451, 676, 563, 233, 984, 204, 719, 791, 785, 203, 890, 976, 61, 632, 749, 1126, 245, 465, 450, 1112, 810, 1017, 933, 123, 194, 651, 234, 1002, 173, 883, 963, 1156, 1041, 183, 556, 554, 115, 952, 19, 524, 398, 566, 62, 983, 44, 423, 1204, 950, 1021, 358, 348, 303, 821, 56, 889, 487, 111, 1185, 403, 343, 1136, 446, 1132, 751, 555, 1195, 161, 852, 1023, 401, 504, 739, 811, 1084, 240, 1056, 45, 252, 683, 752, 190, 13, 548, 792, 7, 419, 1165, 695, 1099, 564, 1065, 763, 69, 95, 89, 1034, 1182, 1006, 396, 884, 420, 253, 323, 828, 59, 927, 525, 747, 904, 211, 229, 553, 534, 1114, 372, 207, 162, 817, 578, 195, 1177, 261, 310, 47, 962, 288, 174, 562, 397, 863, 647, 1043, 542, 945, 778, 492, 81, 255, 196, 581, 756, 411, 662, 94, 1014, 518, 675, 987, 1072, 1209, 944, 772, 373, 981, 442, 230, 1015, 257, 712, 34, 389, 344, 720, 337, 585, 764, 1011, 128, 934, 593, 634, 42, 968, 1028, 638, 143, 483, 611, 726, 617, 316, 707, 793, 88, 241, 517, 369, 995, 391, 1110, 1144, 408, 1010, 325, 814, 1153, 112, 427, 494, 1194, 218, 624, 29, 846, 360, 40, 704, 840, 951, 873, 865, 322, 636, 864, 768, 998, 1059, 464, 586, 1181, 166, 779, 515, 132, 1171, 149, 460, 795, 561, 103, 813, 191, 65, 425, 37, 227, 837, 351, 382, 407, 293, 445, 644, 109, 368, 699, 155, 1067, 0, 1030, 72, 735, 925, 805, 985, 1027, 630, 758, 444, 477, 461, 966, 577, 305, 809, 892, 869, 979, 670, 11, 541, 663, 1139, 781, 216, 454, 214, 826, 1121, 224, 1193, 880, 182, 519, 1208, 394, 871, 1154, 5, 208, 1113, 1000, 822, 1201, 481, 387, 737, 1205, 447, 1062, 334, 575, 509, 1127, 475, 974, 1133, 163, 798, 993, 107, 1105, 250, 1117, 587, 1184, 716, 1039, 879, 1, 93, 482, 916, 907, 299, 1087, 658, 1157, 559, 924, 929, 1001, 558, 800, 357, 1199, 680, 198, 169, 187, 866, 320, 52, 903, 723, 572, 330, 777, 1150, 304, 101, 857, 742, 621, 1128, 994, 514, 913, 311, 1019, 113, 932, 610, 74, 936, 816, 953, 1046, 743, 961, 8, 753, 550, 1025, 347, 309, 1152, 546, 10, 946, 68, 529, 490, 802, 861, 930, 295, 231, 1190, 694, 86, 468, 902, 1123, 319, 759, 667, 744, 80, 935, 544, 1200, 193, 547, 1137, 1082, 1207, 126, 989, 881, 733, 402, 618, 55, 239, 140, 922, 83, 73, 1141, 506, 908, 286, 1103, 854, 486, 1040, 9, 267, 185, 376, 896, 290, 965, 469, 606, 573, 1085, 275, 512, 1192, 153, 682, 1120, 114, 691, 834, 910, 1003, 332, 1029, 1024, 549, 409, 839, 659, 12, 202, 698, 600, 681, 1078, 1135, 545, 527, 1106, 623, 1161, 1111, 850, 199, 1076, 696, 1075, 413, 246, 1055, 687, 992, 784, 281, 1168, 496, 748, 771, 669, 770, 361, 1213, 526, 1159, 796, 603, 1095, 740, 899, 645, 158, 78, 269, 734, 689, 762, 206, 1180, 1164, 1073, 1146, 471, 439, 327, 1131, 875, 41, 539, 1092, 417, 429, 633, 688, 643, 940, 1093, 601, 91, 713, 853, 87, 356, 706, 329, 909, 790, 279, 697, 430, 404, 885, 249, 969, 270, 1081, 58, 807, 378, 254, 709, 125, 501, 26, 1049, 628, 355, 500, 848, 1178, 815, 1088, 629, 414, 1102, 1037, 139, 99, 827, 474, 1109, 671, 399, 738, 938, 845, 574, 690, 77, 1149, 1175, 1008, 1160, 1061, 277, 170, 217, 57, 579, 1086, 335, 459, 1196, 678, 1033, 60, 978, 21, 67, 888, 168, 986, 238, 458, 375, 665, 851, 76, 1155, 350, 769, 736, 84, 102, 614, 1042, 1032, 674, 489, 640, 197, 971, 988, 393, 1206, 172, 877, 1173, 435, 184, 457, 1186, 17, 635, 639, 324, 609, 1031, 104, 722, 1129, 551, 745, 462, 536, 619, 271, 342, 205, 144, 434, 85, 595, 24, 649, 1140, 990, 1214, 521, 383, 818, 1054, 164, 268, 346, 1068, 392, 1070, 761, 838, 106, 825, 959, 222, 1166, 321, 511, 693, 780, 1013, 137, 18, 1167, 648, 352, 448, 63, 642, 937, 580, 799, 406, 568, 503, 565, 939, 2, 225, 6, 287, 620, 379, 354, 569, 1047, 923, 915, 765, 668, 105, 338, 75, 27, 497, 894, 657, 588, 513, 470, 452, 151, 165, 120, 495, 1176, 54, 412, 273, 70, 949, 422, 1188, 179, 443, 1151, 228, 22, 15, 449, 147, 650, 306, 1020, 1097, 31, 862, 1197, 118, 301, 911, 1080, 167, 124, 472, 463, 285, 842, 493, 171, 947, 613, 491, 1122, 1057, 582, 1162, 921, 266, 50, 479, 589, 596, 274, 878, 248, 100, 590, 703, 1053, 145, 783, 1130, 557, 867, 1026, 615, 507, 797, 1100, 122, 625, 941, 499, 339, 242, 236, 914, 931, 424, 775, 570, 25, 455, 725, 256, 1101, 473, 533, 653, 1163, 920, 284, 157, 1124, 535, 180, 886, 467, 1035, 410, 221, 390, 1074, 1212, 317, 367, 1104, 416, 622, 729, 824, 453, 973, 666, 597, 900, 300, 466, 150, 135, 794, 788, 488, 258, 672, 823, 776, 235, 571, 220, 108, 604, 870, 1094, 782, 160, 516, 505, 177, 291, 1174, 262, 773, 32, 849, 1060, 333, 226, 265, 594, 701, 1098, 820, 359, 14, 294, 138, 175, 664, 130, 386, 1063, 432, 1096, 646, 3, 591, 543, 1051, 219, 954, 1142, 1187, 364, 540, 537, 605, 812, 741, 188, 260, 673, 531, 1147, 560, 298, 599, 156, 1009, 314, 480, 592, 855, 340, 1172, 602, 189, 1071, 1138, 660, 912, 418, 146, 276, 789, 326, 259, 523, 528, 181, 819, 498, 652, 1143, 212, 754, 23, 876, 4, 385, 1191, 679, 1116, 1022, 307, 714, 371, 1134, 616, 1007, 1115, 53, 1119, 210, 1038, 302, 133, 552, 263, 730, 831, 1083, 388, 426, 656, 576, 728, 363, 186, 289, 117, 283, 980, 30, 955, 1170, 715, 967, 830, 64, 192, 522, 121, 374, 264, 119, 1148, 532, 833, 766, 868, 96, 1179, 90, 608, 970, 721, 918, 1118, 478, 318, 774, 641, 38, 804, 436]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6618359427635806
the save name prefix for this run is:  chkpt-ID_6618359427635806_tag_TransE-omit-DBpedia50
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 429
rank avg (pred): 0.470 +- 0.004
mrr vals (pred, true): 0.016, 0.038
batch losses (mrrl, rdl): 0.0, 8.71522e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 498
rank avg (pred): 0.255 +- 0.009
mrr vals (pred, true): 0.029, 0.077
batch losses (mrrl, rdl): 0.0, 0.0001790834

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 151
rank avg (pred): 0.397 +- 0.041
mrr vals (pred, true): 0.019, 0.101
batch losses (mrrl, rdl): 0.0, 0.0003578037

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1018
rank avg (pred): 0.374 +- 0.114
mrr vals (pred, true): 0.024, 0.111
batch losses (mrrl, rdl): 0.0, 0.000262071

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 815
rank avg (pred): 0.114 +- 0.051
mrr vals (pred, true): 0.093, 0.532
batch losses (mrrl, rdl): 0.0, 0.0001314474

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 559
rank avg (pred): 0.164 +- 0.107
mrr vals (pred, true): 0.116, 0.067
batch losses (mrrl, rdl): 0.0, 0.0008300071

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 781
rank avg (pred): 0.328 +- 0.204
mrr vals (pred, true): 0.111, 0.090
batch losses (mrrl, rdl): 0.0, 0.0001212152

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 531
rank avg (pred): 0.219 +- 0.163
mrr vals (pred, true): 0.169, 0.067
batch losses (mrrl, rdl): 0.0, 0.0003209643

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1099
rank avg (pred): 0.347 +- 0.234
mrr vals (pred, true): 0.143, 0.129
batch losses (mrrl, rdl): 0.0, 0.0003098694

Epoch over!
epoch time: 53.138

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 952
rank avg (pred): 0.400 +- 0.249
mrr vals (pred, true): 0.111, 0.043
batch losses (mrrl, rdl): 0.0, 3.9664e-06

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 444
rank avg (pred): 0.340 +- 0.230
mrr vals (pred, true): 0.161, 0.035
batch losses (mrrl, rdl): 0.0, 0.0001933674

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 580
rank avg (pred): 0.407 +- 0.296
mrr vals (pred, true): 0.186, 0.038
batch losses (mrrl, rdl): 0.0, 1.58029e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 469
rank avg (pred): 0.379 +- 0.256
mrr vals (pred, true): 0.151, 0.039
batch losses (mrrl, rdl): 0.0, 3.53659e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1206
rank avg (pred): 0.414 +- 0.286
mrr vals (pred, true): 0.170, 0.041
batch losses (mrrl, rdl): 0.0, 1.35044e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 225
rank avg (pred): 0.323 +- 0.265
mrr vals (pred, true): 0.249, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001562504

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 851
rank avg (pred): 0.295 +- 0.270
mrr vals (pred, true): 0.279, 0.088
batch losses (mrrl, rdl): 0.0, 2.8715e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 821
rank avg (pred): 0.095 +- 0.081
mrr vals (pred, true): 0.319, 0.430
batch losses (mrrl, rdl): 0.0, 4.40081e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 466
rank avg (pred): 0.315 +- 0.261
mrr vals (pred, true): 0.242, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001392466

Epoch over!
epoch time: 54.157

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 499
rank avg (pred): 0.213 +- 0.184
mrr vals (pred, true): 0.250, 0.076
batch losses (mrrl, rdl): 0.0, 0.0002426664

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 368
rank avg (pred): 0.343 +- 0.244
mrr vals (pred, true): 0.140, 0.135
batch losses (mrrl, rdl): 0.0, 0.0003042321

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 113
rank avg (pred): 0.352 +- 0.229
mrr vals (pred, true): 0.116, 0.122
batch losses (mrrl, rdl): 0.0, 0.0003522712

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 994
rank avg (pred): 0.048 +- 0.045
mrr vals (pred, true): 0.409, 0.623
batch losses (mrrl, rdl): 0.0, 1.49507e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 475
rank avg (pred): 0.342 +- 0.262
mrr vals (pred, true): 0.178, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001150673

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 722
rank avg (pred): 0.465 +- 0.290
mrr vals (pred, true): 0.095, 0.043
batch losses (mrrl, rdl): 0.0, 3.54062e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 540
rank avg (pred): 0.241 +- 0.212
mrr vals (pred, true): 0.192, 0.067
batch losses (mrrl, rdl): 0.0, 0.0001742435

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1000
rank avg (pred): 0.349 +- 0.258
mrr vals (pred, true): 0.128, 0.119
batch losses (mrrl, rdl): 0.0, 0.0002950222

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 952
rank avg (pred): 0.328 +- 0.267
mrr vals (pred, true): 0.168, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001066389

Epoch over!
epoch time: 54.312

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 816
rank avg (pred): 0.072 +- 0.062
mrr vals (pred, true): 0.302, 0.372
batch losses (mrrl, rdl): 0.0, 1.39572e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 930
rank avg (pred): 0.329 +- 0.241
mrr vals (pred, true): 0.167, 0.089
batch losses (mrrl, rdl): 0.0, 7.04683e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 445
rank avg (pred): 0.324 +- 0.260
mrr vals (pred, true): 0.169, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001414503

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 285
rank avg (pred): 0.042 +- 0.038
mrr vals (pred, true): 0.409, 0.512
batch losses (mrrl, rdl): 0.0, 3.054e-06

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 24
rank avg (pred): 0.086 +- 0.083
mrr vals (pred, true): 0.376, 0.495
batch losses (mrrl, rdl): 0.0, 7.76509e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 653
rank avg (pred): 0.406 +- 0.279
mrr vals (pred, true): 0.138, 0.037
batch losses (mrrl, rdl): 0.0, 9.7521e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 985
rank avg (pred): 0.034 +- 0.031
mrr vals (pred, true): 0.423, 0.619
batch losses (mrrl, rdl): 0.0, 3.2896e-06

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 547
rank avg (pred): 0.231 +- 0.207
mrr vals (pred, true): 0.234, 0.080
batch losses (mrrl, rdl): 0.0, 0.0001706466

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 457
rank avg (pred): 0.303 +- 0.278
mrr vals (pred, true): 0.271, 0.037
batch losses (mrrl, rdl): 0.0, 0.0002727663

Epoch over!
epoch time: 51.583

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 697
rank avg (pred): 0.401 +- 0.275
mrr vals (pred, true): 0.157, 0.039
batch losses (mrrl, rdl): 0.0, 1.17953e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1149
rank avg (pred): 0.117 +- 0.112
mrr vals (pred, true): 0.361, 0.141
batch losses (mrrl, rdl): 0.0, 0.0002016197

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 855
rank avg (pred): 0.331 +- 0.288
mrr vals (pred, true): 0.234, 0.087
batch losses (mrrl, rdl): 0.0, 0.0001375933

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 170
rank avg (pred): 0.348 +- 0.293
mrr vals (pred, true): 0.222, 0.041
batch losses (mrrl, rdl): 0.0, 8.5789e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 932
rank avg (pred): 0.351 +- 0.277
mrr vals (pred, true): 0.207, 0.090
batch losses (mrrl, rdl): 0.0, 0.000152687

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 666
rank avg (pred): 0.395 +- 0.274
mrr vals (pred, true): 0.199, 0.038
batch losses (mrrl, rdl): 0.0, 1.68845e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1055
rank avg (pred): 0.033 +- 0.030
mrr vals (pred, true): 0.451, 0.597
batch losses (mrrl, rdl): 0.0, 2.1687e-06

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 204
rank avg (pred): 0.336 +- 0.288
mrr vals (pred, true): 0.279, 0.037
batch losses (mrrl, rdl): 0.0, 0.0001444889

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 337
rank avg (pred): 0.324 +- 0.287
mrr vals (pred, true): 0.302, 0.112
batch losses (mrrl, rdl): 0.0, 0.0002925287

Epoch over!
epoch time: 51.024

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 661
rank avg (pred): 0.425 +- 0.300
mrr vals (pred, true): 0.224, 0.041
batch losses (mrrl, rdl): 0.3022870719, 1.11324e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 147
rank avg (pred): 0.362 +- 0.164
mrr vals (pred, true): 0.100, 0.077
batch losses (mrrl, rdl): 0.0250865165, 0.0001986146

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 402
rank avg (pred): 0.306 +- 0.127
mrr vals (pred, true): 0.083, 0.096
batch losses (mrrl, rdl): 0.0107048601, 5.73521e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 903
rank avg (pred): 0.088 +- 0.065
mrr vals (pred, true): 0.246, 0.308
batch losses (mrrl, rdl): 0.0383190773, 1.82599e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 226
rank avg (pred): 0.353 +- 0.146
mrr vals (pred, true): 0.087, 0.040
batch losses (mrrl, rdl): 0.0135023007, 0.0001575947

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 887
rank avg (pred): 0.364 +- 0.135
mrr vals (pred, true): 0.081, 0.038
batch losses (mrrl, rdl): 0.0094390055, 0.0001398176

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 358
rank avg (pred): 0.352 +- 0.144
mrr vals (pred, true): 0.087, 0.096
batch losses (mrrl, rdl): 0.0135469027, 0.0001168683

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 913
rank avg (pred): 0.084 +- 0.066
mrr vals (pred, true): 0.242, 0.330
batch losses (mrrl, rdl): 0.076793544, 2.09721e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1178
rank avg (pred): 0.358 +- 0.091
mrr vals (pred, true): 0.064, 0.037
batch losses (mrrl, rdl): 0.0020012883, 0.0001041087

Epoch over!
epoch time: 51.413

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 915
rank avg (pred): 0.078 +- 0.051
mrr vals (pred, true): 0.224, 0.335
batch losses (mrrl, rdl): 0.1232740879, 1.56331e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1171
rank avg (pred): 0.395 +- 0.097
mrr vals (pred, true): 0.054, 0.036
batch losses (mrrl, rdl): 0.0001363016, 6.03353e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 746
rank avg (pred): 0.014 +- 0.013
mrr vals (pred, true): 0.503, 0.566
batch losses (mrrl, rdl): 0.0397532135, 3.2682e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 61
rank avg (pred): 0.009 +- 0.009
mrr vals (pred, true): 0.601, 0.563
batch losses (mrrl, rdl): 0.014651401, 5.4931e-06

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 854
rank avg (pred): 0.372 +- 0.107
mrr vals (pred, true): 0.059, 0.091
batch losses (mrrl, rdl): 0.0007808927, 0.0002341392

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1126
rank avg (pred): 0.325 +- 0.138
mrr vals (pred, true): 0.079, 0.038
batch losses (mrrl, rdl): 0.008246203, 0.0003129095

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 597
rank avg (pred): 0.387 +- 0.079
mrr vals (pred, true): 0.043, 0.036
batch losses (mrrl, rdl): 0.0004603099, 8.49118e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 592
rank avg (pred): 0.348 +- 0.098
mrr vals (pred, true): 0.044, 0.038
batch losses (mrrl, rdl): 0.0003478019, 0.0002044817

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 344
rank avg (pred): 0.314 +- 0.142
mrr vals (pred, true): 0.085, 0.132
batch losses (mrrl, rdl): 0.0219047256, 0.0001351118

Epoch over!
epoch time: 51.435

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 245
rank avg (pred): 0.006 +- 0.006
mrr vals (pred, true): 0.650, 0.612
batch losses (mrrl, rdl): 0.0143311154, 5.3957e-06

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 93
rank avg (pred): 0.356 +- 0.103
mrr vals (pred, true): 0.057, 0.089
batch losses (mrrl, rdl): 0.0005305617, 0.0001975055

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 763
rank avg (pred): 0.341 +- 0.118
mrr vals (pred, true): 0.063, 0.067
batch losses (mrrl, rdl): 0.0016038731, 4.0858e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 811
rank avg (pred): 0.035 +- 0.038
mrr vals (pred, true): 0.422, 0.430
batch losses (mrrl, rdl): 0.0006135595, 8.1604e-06

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 875
rank avg (pred): 0.325 +- 0.118
mrr vals (pred, true): 0.062, 0.041
batch losses (mrrl, rdl): 0.0015175524, 0.0003101779

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 931
rank avg (pred): 0.324 +- 0.120
mrr vals (pred, true): 0.070, 0.093
batch losses (mrrl, rdl): 0.004118056, 6.70421e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 308
rank avg (pred): 0.018 +- 0.017
mrr vals (pred, true): 0.519, 0.619
batch losses (mrrl, rdl): 0.1011084095, 1.186e-07

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1060
rank avg (pred): 0.009 +- 0.009
mrr vals (pred, true): 0.635, 0.621
batch losses (mrrl, rdl): 0.0021970423, 3.2986e-06

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 466
rank avg (pred): 0.321 +- 0.120
mrr vals (pred, true): 0.068, 0.038
batch losses (mrrl, rdl): 0.0033734462, 0.0002545506

Epoch over!
epoch time: 51.398

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 706
rank avg (pred): 0.344 +- 0.102
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 3.15385e-05, 0.0002187332

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 433
rank avg (pred): 0.286 +- 0.124
mrr vals (pred, true): 0.074, 0.041
batch losses (mrrl, rdl): 0.0058297794, 0.0005619094

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1088
rank avg (pred): 0.257 +- 0.132
mrr vals (pred, true): 0.101, 0.134
batch losses (mrrl, rdl): 0.0111665111, 3.18989e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 126
rank avg (pred): 0.336 +- 0.104
mrr vals (pred, true): 0.056, 0.088
batch losses (mrrl, rdl): 0.0004113997, 9.57392e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 491
rank avg (pred): 0.115 +- 0.056
mrr vals (pred, true): 0.113, 0.131
batch losses (mrrl, rdl): 0.0032360046, 0.0006741281

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 988
rank avg (pred): 0.010 +- 0.010
mrr vals (pred, true): 0.635, 0.621
batch losses (mrrl, rdl): 0.0018278884, 4.7008e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 629
rank avg (pred): 0.298 +- 0.104
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 2.4333e-06, 0.0004365809

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1008
rank avg (pred): 0.266 +- 0.137
mrr vals (pred, true): 0.096, 0.122
batch losses (mrrl, rdl): 0.0066773333, 5.96421e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 464
rank avg (pred): 0.301 +- 0.116
mrr vals (pred, true): 0.065, 0.042
batch losses (mrrl, rdl): 0.002245703, 0.0003423947

Epoch over!
epoch time: 50.652

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 140
rank avg (pred): 0.308 +- 0.112
mrr vals (pred, true): 0.059, 0.110
batch losses (mrrl, rdl): 0.0266182683, 8.55657e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 682
rank avg (pred): 0.328 +- 0.102
mrr vals (pred, true): 0.050, 0.039
batch losses (mrrl, rdl): 4.104e-07, 0.0003135127

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 390
rank avg (pred): 0.288 +- 0.120
mrr vals (pred, true): 0.071, 0.098
batch losses (mrrl, rdl): 0.0044888062, 5.08842e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 348
rank avg (pred): 0.286 +- 0.114
mrr vals (pred, true): 0.065, 0.107
batch losses (mrrl, rdl): 0.0180348586, 4.03158e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 589
rank avg (pred): 0.314 +- 0.092
mrr vals (pred, true): 0.046, 0.038
batch losses (mrrl, rdl): 0.0001495021, 0.0003542694

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 706
rank avg (pred): 0.319 +- 0.093
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.0001562284, 0.0003351122

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1191
rank avg (pred): 0.253 +- 0.104
mrr vals (pred, true): 0.054, 0.036
batch losses (mrrl, rdl): 0.0001494064, 0.0008318353

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1152
rank avg (pred): 0.098 +- 0.065
mrr vals (pred, true): 0.146, 0.150
batch losses (mrrl, rdl): 0.0001037708, 0.0005522493

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 681
rank avg (pred): 0.298 +- 0.094
mrr vals (pred, true): 0.045, 0.040
batch losses (mrrl, rdl): 0.0002218666, 0.0004848461

Epoch over!
epoch time: 51.818

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 570
rank avg (pred): 0.292 +- 0.104
mrr vals (pred, true): 0.044, 0.036
batch losses (mrrl, rdl): 0.0003929359, 0.0004975908

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 352
rank avg (pred): 0.299 +- 0.119
mrr vals (pred, true): 0.073, 0.114
batch losses (mrrl, rdl): 0.0169365648, 9.88392e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 717
rank avg (pred): 0.318 +- 0.093
mrr vals (pred, true): 0.042, 0.040
batch losses (mrrl, rdl): 0.0006264807, 0.0003623223

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 81
rank avg (pred): 0.295 +- 0.087
mrr vals (pred, true): 0.050, 0.089
batch losses (mrrl, rdl): 1.678e-07, 4.71433e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 462
rank avg (pred): 0.268 +- 0.114
mrr vals (pred, true): 0.074, 0.046
batch losses (mrrl, rdl): 0.005991349, 0.0005760864

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 654
rank avg (pred): 0.297 +- 0.085
mrr vals (pred, true): 0.038, 0.042
batch losses (mrrl, rdl): 0.001340866, 0.0004693128

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 115
rank avg (pred): 0.278 +- 0.093
mrr vals (pred, true): 0.054, 0.089
batch losses (mrrl, rdl): 0.0001534462, 3.87567e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 815
rank avg (pred): 0.031 +- 0.049
mrr vals (pred, true): 0.471, 0.532
batch losses (mrrl, rdl): 0.0378114097, 3.094e-07

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 395
rank avg (pred): 0.248 +- 0.124
mrr vals (pred, true): 0.096, 0.135
batch losses (mrrl, rdl): 0.0153187737, 3.98273e-05

Epoch over!
epoch time: 54.477

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 969
rank avg (pred): 0.292 +- 0.098
mrr vals (pred, true): 0.065, 0.046
batch losses (mrrl, rdl): 0.0021317299, 0.0004573559

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 880
rank avg (pred): 0.285 +- 0.101
mrr vals (pred, true): 0.062, 0.040
batch losses (mrrl, rdl): 0.0014442191, 0.0004995296

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 539
rank avg (pred): 0.164 +- 0.095
mrr vals (pred, true): 0.094, 0.125
batch losses (mrrl, rdl): 0.0095078172, 0.0003514461

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1131
rank avg (pred): 0.259 +- 0.117
mrr vals (pred, true): 0.092, 0.037
batch losses (mrrl, rdl): 0.0178715046, 0.0007779666

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1146
rank avg (pred): 0.138 +- 0.085
mrr vals (pred, true): 0.104, 0.150
batch losses (mrrl, rdl): 0.02079257, 0.0001834097

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 663
rank avg (pred): 0.277 +- 0.094
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.000171558, 0.0006750842

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1177
rank avg (pred): 0.202 +- 0.071
mrr vals (pred, true): 0.064, 0.042
batch losses (mrrl, rdl): 0.0020284276, 0.0008399676

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 989
rank avg (pred): 0.006 +- 0.006
mrr vals (pred, true): 0.696, 0.620
batch losses (mrrl, rdl): 0.0583450794, 6.7107e-06

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 849
rank avg (pred): 0.296 +- 0.084
mrr vals (pred, true): 0.056, 0.091
batch losses (mrrl, rdl): 0.0003568376, 4.38042e-05

Epoch over!
epoch time: 52.13

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 267
rank avg (pred): 0.046 +- 0.077
mrr vals (pred, true): 0.526, 0.517
batch losses (mrrl, rdl): 0.0007201781, 1.06934e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 338
rank avg (pred): 0.227 +- 0.114
mrr vals (pred, true): 0.099, 0.136
batch losses (mrrl, rdl): 0.0136727272, 4.16331e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1175
rank avg (pred): 0.310 +- 0.076
mrr vals (pred, true): 0.047, 0.038
batch losses (mrrl, rdl): 9.12159e-05, 0.0002662393

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 321
rank avg (pred): 0.028 +- 0.049
mrr vals (pred, true): 0.519, 0.503
batch losses (mrrl, rdl): 0.0024199586, 2.89e-08

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 939
rank avg (pred): 0.289 +- 0.088
mrr vals (pred, true): 0.052, 0.088
batch losses (mrrl, rdl): 5.90236e-05, 3.91543e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 263
rank avg (pred): 0.013 +- 0.013
mrr vals (pred, true): 0.617, 0.622
batch losses (mrrl, rdl): 0.0003010155, 1.0482e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 553
rank avg (pred): 0.175 +- 0.104
mrr vals (pred, true): 0.089, 0.073
batch losses (mrrl, rdl): 0.0154450741, 0.0006517689

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 952
rank avg (pred): 0.266 +- 0.083
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 8.35274e-05, 0.0006082585

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 11
rank avg (pred): 0.017 +- 0.019
mrr vals (pred, true): 0.586, 0.606
batch losses (mrrl, rdl): 0.004065922, 3.032e-07

Epoch over!
epoch time: 52.769

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 568
rank avg (pred): 0.283 +- 0.069
mrr vals (pred, true): 0.035, 0.036
batch losses (mrrl, rdl): 0.0021154017, 0.0006883612

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1100
rank avg (pred): 0.225 +- 0.108
mrr vals (pred, true): 0.087, 0.117
batch losses (mrrl, rdl): 0.0090258978, 5.69202e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 227
rank avg (pred): 0.240 +- 0.095
mrr vals (pred, true): 0.071, 0.042
batch losses (mrrl, rdl): 0.0042958278, 0.0009118984

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1038
rank avg (pred): 0.252 +- 0.101
mrr vals (pred, true): 0.070, 0.038
batch losses (mrrl, rdl): 0.0041509913, 0.0008338536

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 588
rank avg (pred): 0.316 +- 0.053
mrr vals (pred, true): 0.043, 0.037
batch losses (mrrl, rdl): 0.0004330511, 0.000370496

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 172
rank avg (pred): 0.256 +- 0.100
mrr vals (pred, true): 0.074, 0.039
batch losses (mrrl, rdl): 0.0059732678, 0.0007508249

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 145
rank avg (pred): 0.273 +- 0.078
mrr vals (pred, true): 0.058, 0.078
batch losses (mrrl, rdl): 0.0006743034, 7.03101e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 225
rank avg (pred): 0.286 +- 0.068
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 7.156e-07, 0.0005796098

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 283
rank avg (pred): 0.033 +- 0.058
mrr vals (pred, true): 0.548, 0.570
batch losses (mrrl, rdl): 0.0047398484, 2.5978e-06

Epoch over!
epoch time: 53.342

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 426
rank avg (pred): 0.283 +- 0.077
mrr vals (pred, true): 0.060, 0.040
batch losses (mrrl, rdl): 0.0010606605, 0.0005318794

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1188
rank avg (pred): 0.251 +- 0.091
mrr vals (pred, true): 0.055, 0.042
batch losses (mrrl, rdl): 0.0002079789, 0.0007561314

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 529
rank avg (pred): 0.231 +- 0.139
mrr vals (pred, true): 0.097, 0.073
batch losses (mrrl, rdl): 0.0221986696, 0.0002394629

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 950
rank avg (pred): 0.263 +- 0.093
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001790863, 0.0006636568

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 963
rank avg (pred): 0.248 +- 0.098
mrr vals (pred, true): 0.067, 0.038
batch losses (mrrl, rdl): 0.0029939229, 0.0008334391

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 526
rank avg (pred): 0.214 +- 0.146
mrr vals (pred, true): 0.102, 0.073
batch losses (mrrl, rdl): 0.0270842128, 0.0003936189

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 889
rank avg (pred): 0.279 +- 0.078
mrr vals (pred, true): 0.056, 0.039
batch losses (mrrl, rdl): 0.0003063894, 0.0006609313

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 453
rank avg (pred): 0.283 +- 0.078
mrr vals (pred, true): 0.056, 0.040
batch losses (mrrl, rdl): 0.0004216628, 0.0005577582

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 533
rank avg (pred): 0.257 +- 0.124
mrr vals (pred, true): 0.076, 0.121
batch losses (mrrl, rdl): 0.0207351614, 7.1729e-05

Epoch over!
epoch time: 53.417

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.011 +- 0.013
mrr vals (pred, true): 0.638, 0.615

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04588 	 0.02982 	 ~...
   29 	     1 	 0.06428 	 0.03543 	 ~...
   45 	     2 	 0.06944 	 0.03554 	 m..s
    2 	     3 	 0.04775 	 0.03572 	 ~...
    8 	     4 	 0.04982 	 0.03604 	 ~...
    4 	     5 	 0.04798 	 0.03645 	 ~...
   62 	     6 	 0.07814 	 0.03659 	 m..s
   36 	     7 	 0.06615 	 0.03665 	 ~...
   53 	     8 	 0.07167 	 0.03672 	 m..s
   35 	     9 	 0.06595 	 0.03699 	 ~...
   18 	    10 	 0.05438 	 0.03741 	 ~...
   11 	    11 	 0.05084 	 0.03754 	 ~...
    5 	    12 	 0.04887 	 0.03754 	 ~...
   13 	    13 	 0.05184 	 0.03768 	 ~...
   67 	    14 	 0.08614 	 0.03795 	 m..s
   43 	    15 	 0.06848 	 0.03801 	 m..s
   54 	    16 	 0.07220 	 0.03806 	 m..s
   12 	    17 	 0.05170 	 0.03811 	 ~...
   19 	    18 	 0.05448 	 0.03817 	 ~...
   38 	    19 	 0.06721 	 0.03818 	 ~...
    6 	    20 	 0.04936 	 0.03824 	 ~...
   20 	    21 	 0.05754 	 0.03827 	 ~...
    3 	    22 	 0.04781 	 0.03843 	 ~...
   64 	    23 	 0.08283 	 0.03845 	 m..s
   40 	    24 	 0.06800 	 0.03866 	 ~...
   47 	    25 	 0.06996 	 0.03880 	 m..s
    1 	    26 	 0.04759 	 0.03917 	 ~...
   73 	    27 	 0.09982 	 0.03922 	 m..s
   17 	    28 	 0.05436 	 0.03945 	 ~...
   68 	    29 	 0.08800 	 0.03963 	 m..s
    7 	    30 	 0.04950 	 0.03977 	 ~...
   60 	    31 	 0.07643 	 0.04046 	 m..s
   16 	    32 	 0.05409 	 0.04116 	 ~...
   69 	    33 	 0.08810 	 0.04136 	 m..s
   30 	    34 	 0.06460 	 0.04136 	 ~...
   41 	    35 	 0.06819 	 0.04142 	 ~...
   10 	    36 	 0.05081 	 0.04156 	 ~...
   14 	    37 	 0.05339 	 0.04163 	 ~...
   23 	    38 	 0.06071 	 0.04190 	 ~...
    9 	    39 	 0.05079 	 0.04263 	 ~...
   44 	    40 	 0.06870 	 0.04284 	 ~...
   15 	    41 	 0.05381 	 0.04397 	 ~...
   70 	    42 	 0.09033 	 0.04445 	 m..s
   28 	    43 	 0.06367 	 0.04593 	 ~...
   31 	    44 	 0.06468 	 0.04661 	 ~...
   78 	    45 	 0.10457 	 0.07071 	 m..s
   75 	    46 	 0.10029 	 0.07231 	 ~...
   72 	    47 	 0.09912 	 0.07276 	 ~...
   77 	    48 	 0.10338 	 0.07324 	 m..s
   74 	    49 	 0.10014 	 0.07834 	 ~...
   22 	    50 	 0.05953 	 0.08011 	 ~...
   25 	    51 	 0.06142 	 0.08103 	 ~...
   39 	    52 	 0.06767 	 0.08401 	 ~...
   52 	    53 	 0.07146 	 0.08775 	 ~...
   21 	    54 	 0.05899 	 0.08797 	 ~...
   24 	    55 	 0.06101 	 0.08956 	 ~...
   49 	    56 	 0.07053 	 0.09154 	 ~...
   26 	    57 	 0.06227 	 0.09155 	 ~...
   42 	    58 	 0.06836 	 0.09245 	 ~...
   37 	    59 	 0.06720 	 0.09723 	 m..s
   33 	    60 	 0.06515 	 0.09782 	 m..s
   32 	    61 	 0.06499 	 0.09999 	 m..s
   34 	    62 	 0.06568 	 0.10161 	 m..s
   27 	    63 	 0.06236 	 0.10210 	 m..s
   51 	    64 	 0.07083 	 0.10238 	 m..s
   63 	    65 	 0.08233 	 0.10850 	 ~...
   46 	    66 	 0.06991 	 0.11467 	 m..s
   50 	    67 	 0.07058 	 0.11853 	 m..s
   66 	    68 	 0.08576 	 0.11896 	 m..s
   71 	    69 	 0.09369 	 0.12290 	 ~...
   61 	    70 	 0.07747 	 0.12310 	 m..s
   48 	    71 	 0.07042 	 0.12672 	 m..s
   55 	    72 	 0.07391 	 0.12945 	 m..s
   59 	    73 	 0.07601 	 0.13066 	 m..s
   58 	    74 	 0.07573 	 0.13357 	 m..s
   76 	    75 	 0.10214 	 0.13662 	 m..s
   56 	    76 	 0.07408 	 0.13797 	 m..s
   79 	    77 	 0.10628 	 0.13812 	 m..s
   57 	    78 	 0.07477 	 0.14083 	 m..s
   65 	    79 	 0.08366 	 0.14938 	 m..s
   80 	    80 	 0.13361 	 0.15500 	 ~...
   81 	    81 	 0.14697 	 0.20235 	 m..s
   82 	    82 	 0.26062 	 0.24515 	 ~...
   83 	    83 	 0.32346 	 0.29373 	 ~...
   85 	    84 	 0.46858 	 0.41053 	 m..s
   87 	    85 	 0.53121 	 0.43062 	 MISS
   91 	    86 	 0.53816 	 0.50333 	 m..s
   94 	    87 	 0.53831 	 0.50617 	 m..s
   84 	    88 	 0.41145 	 0.51269 	 MISS
   92 	    89 	 0.53817 	 0.51740 	 ~...
  103 	    90 	 0.54984 	 0.51757 	 m..s
  101 	    91 	 0.54937 	 0.52695 	 ~...
   98 	    92 	 0.54494 	 0.53015 	 ~...
  102 	    93 	 0.54940 	 0.53417 	 ~...
   86 	    94 	 0.52168 	 0.53743 	 ~...
   90 	    95 	 0.53790 	 0.54013 	 ~...
   99 	    96 	 0.54615 	 0.54533 	 ~...
   95 	    97 	 0.53918 	 0.54800 	 ~...
   93 	    98 	 0.53820 	 0.55278 	 ~...
   89 	    99 	 0.53508 	 0.55616 	 ~...
  105 	   100 	 0.55781 	 0.56450 	 ~...
  104 	   101 	 0.55048 	 0.56505 	 ~...
   96 	   102 	 0.53947 	 0.56964 	 m..s
  106 	   103 	 0.56677 	 0.57114 	 ~...
   97 	   104 	 0.54409 	 0.57618 	 m..s
  100 	   105 	 0.54777 	 0.57648 	 ~...
   88 	   106 	 0.53162 	 0.58131 	 m..s
  114 	   107 	 0.62243 	 0.60247 	 ~...
  116 	   108 	 0.62797 	 0.60425 	 ~...
  118 	   109 	 0.63519 	 0.60458 	 m..s
  119 	   110 	 0.63761 	 0.60513 	 m..s
  115 	   111 	 0.62698 	 0.60871 	 ~...
  109 	   112 	 0.58806 	 0.61212 	 ~...
  107 	   113 	 0.58331 	 0.61400 	 m..s
  120 	   114 	 0.63849 	 0.61509 	 ~...
  110 	   115 	 0.59210 	 0.61739 	 ~...
  112 	   116 	 0.61031 	 0.61949 	 ~...
  111 	   117 	 0.59578 	 0.62041 	 ~...
  117 	   118 	 0.62883 	 0.62336 	 ~...
  113 	   119 	 0.62017 	 0.62441 	 ~...
  108 	   120 	 0.58586 	 0.62963 	 m..s
==========================================
r_mrr = 0.9895110130310059
r2_mrr = 0.9791262745857239
spearmanr_mrr@5 = 0.8974744081497192
spearmanr_mrr@10 = 0.9256598353385925
spearmanr_mrr@50 = 0.9959028363227844
spearmanr_mrr@100 = 0.9956998825073242
spearmanr_mrr@All = 0.9959192276000977
==========================================
test time: 0.48
Done Testing dataset UMLS
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.013 +- 0.013
mrr vals (pred, true): 0.268, 0.263

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.06896 	 0.00327 	 m..s
   72 	     1 	 0.12952 	 0.00338 	 MISS
    7 	     2 	 0.05417 	 0.00338 	 m..s
   13 	     3 	 0.05945 	 0.00347 	 m..s
   50 	     4 	 0.11837 	 0.00347 	 MISS
    9 	     5 	 0.05797 	 0.00349 	 m..s
   15 	     6 	 0.06387 	 0.00349 	 m..s
   18 	     7 	 0.06841 	 0.00354 	 m..s
   37 	     8 	 0.11416 	 0.00356 	 MISS
   17 	     9 	 0.06840 	 0.00362 	 m..s
   52 	    10 	 0.11928 	 0.00363 	 MISS
   12 	    11 	 0.05901 	 0.00364 	 m..s
   16 	    12 	 0.06749 	 0.00364 	 m..s
   74 	    13 	 0.13011 	 0.00365 	 MISS
   10 	    14 	 0.05851 	 0.00365 	 m..s
   26 	    15 	 0.10194 	 0.00368 	 m..s
   29 	    16 	 0.10698 	 0.00370 	 MISS
   62 	    17 	 0.12342 	 0.00375 	 MISS
   36 	    18 	 0.11371 	 0.00380 	 MISS
   11 	    19 	 0.05899 	 0.00382 	 m..s
    3 	    20 	 0.05178 	 0.00383 	 m..s
   77 	    21 	 0.13053 	 0.00386 	 MISS
   79 	    22 	 0.13658 	 0.00392 	 MISS
    4 	    23 	 0.05178 	 0.00397 	 m..s
    1 	    24 	 0.05130 	 0.00400 	 m..s
   35 	    25 	 0.11318 	 0.00400 	 MISS
   67 	    26 	 0.12815 	 0.00406 	 MISS
   59 	    27 	 0.12127 	 0.00412 	 MISS
   41 	    28 	 0.11466 	 0.00415 	 MISS
   34 	    29 	 0.11291 	 0.00417 	 MISS
   71 	    30 	 0.12843 	 0.00422 	 MISS
    2 	    31 	 0.05166 	 0.00429 	 m..s
   46 	    32 	 0.11706 	 0.00442 	 MISS
   38 	    33 	 0.11452 	 0.00445 	 MISS
   43 	    34 	 0.11568 	 0.00456 	 MISS
   65 	    35 	 0.12747 	 0.00460 	 MISS
   48 	    36 	 0.11814 	 0.00464 	 MISS
   51 	    37 	 0.11856 	 0.00494 	 MISS
   73 	    38 	 0.13006 	 0.00499 	 MISS
   44 	    39 	 0.11589 	 0.00522 	 MISS
    5 	    40 	 0.05182 	 0.00672 	 m..s
    0 	    41 	 0.04765 	 0.00816 	 m..s
    6 	    42 	 0.05268 	 0.01032 	 m..s
    8 	    43 	 0.05464 	 0.01164 	 m..s
   22 	    44 	 0.07624 	 0.01387 	 m..s
   24 	    45 	 0.07942 	 0.01578 	 m..s
   20 	    46 	 0.07303 	 0.01719 	 m..s
   23 	    47 	 0.07917 	 0.01946 	 m..s
   14 	    48 	 0.05951 	 0.02006 	 m..s
   21 	    49 	 0.07324 	 0.02168 	 m..s
   25 	    50 	 0.08153 	 0.03033 	 m..s
   80 	    51 	 0.13718 	 0.03527 	 MISS
   83 	    52 	 0.16301 	 0.07990 	 m..s
   31 	    53 	 0.10825 	 0.11554 	 ~...
   82 	    54 	 0.15486 	 0.11816 	 m..s
   39 	    55 	 0.11452 	 0.11933 	 ~...
   98 	    56 	 0.19432 	 0.12102 	 m..s
   42 	    57 	 0.11475 	 0.12269 	 ~...
   30 	    58 	 0.10704 	 0.12923 	 ~...
   33 	    59 	 0.11109 	 0.13392 	 ~...
   32 	    60 	 0.10986 	 0.13455 	 ~...
   85 	    61 	 0.18335 	 0.13475 	 m..s
   45 	    62 	 0.11706 	 0.13770 	 ~...
   96 	    63 	 0.19402 	 0.14231 	 m..s
   40 	    64 	 0.11457 	 0.14286 	 ~...
   57 	    65 	 0.12055 	 0.14401 	 ~...
  100 	    66 	 0.19478 	 0.14463 	 m..s
   88 	    67 	 0.19357 	 0.15051 	 m..s
   28 	    68 	 0.10643 	 0.15295 	 m..s
   84 	    69 	 0.17431 	 0.15419 	 ~...
   91 	    70 	 0.19374 	 0.15441 	 m..s
   49 	    71 	 0.11825 	 0.15816 	 m..s
   27 	    72 	 0.10531 	 0.15860 	 m..s
   58 	    73 	 0.12109 	 0.15971 	 m..s
   94 	    74 	 0.19401 	 0.16001 	 m..s
   47 	    75 	 0.11754 	 0.16487 	 m..s
   55 	    76 	 0.12028 	 0.16538 	 m..s
   54 	    77 	 0.12015 	 0.16573 	 m..s
   68 	    78 	 0.12818 	 0.16773 	 m..s
   95 	    79 	 0.19401 	 0.16869 	 ~...
   90 	    80 	 0.19369 	 0.16954 	 ~...
   60 	    81 	 0.12284 	 0.16961 	 m..s
   56 	    82 	 0.12031 	 0.17104 	 m..s
   53 	    83 	 0.11971 	 0.17169 	 m..s
   99 	    84 	 0.19458 	 0.17242 	 ~...
   66 	    85 	 0.12789 	 0.17481 	 m..s
   76 	    86 	 0.13043 	 0.17628 	 m..s
   70 	    87 	 0.12826 	 0.17788 	 m..s
   64 	    88 	 0.12479 	 0.17983 	 m..s
   93 	    89 	 0.19401 	 0.18392 	 ~...
  112 	    90 	 0.23620 	 0.18393 	 m..s
   63 	    91 	 0.12401 	 0.18476 	 m..s
  104 	    92 	 0.20185 	 0.18518 	 ~...
   75 	    93 	 0.13029 	 0.18552 	 m..s
  103 	    94 	 0.19997 	 0.18566 	 ~...
  111 	    95 	 0.23089 	 0.18871 	 m..s
   61 	    96 	 0.12333 	 0.19017 	 m..s
  108 	    97 	 0.22617 	 0.19071 	 m..s
  102 	    98 	 0.19965 	 0.19419 	 ~...
  107 	    99 	 0.22562 	 0.19569 	 ~...
   89 	   100 	 0.19357 	 0.19856 	 ~...
   97 	   101 	 0.19415 	 0.19914 	 ~...
   78 	   102 	 0.13526 	 0.20081 	 m..s
   92 	   103 	 0.19397 	 0.20601 	 ~...
  109 	   104 	 0.22670 	 0.20723 	 ~...
  106 	   105 	 0.20564 	 0.20744 	 ~...
   69 	   106 	 0.12824 	 0.21423 	 m..s
   86 	   107 	 0.19355 	 0.21680 	 ~...
   81 	   108 	 0.13831 	 0.21777 	 m..s
   87 	   109 	 0.19356 	 0.21972 	 ~...
  101 	   110 	 0.19607 	 0.22076 	 ~...
  110 	   111 	 0.22993 	 0.22900 	 ~...
  113 	   112 	 0.23921 	 0.23645 	 ~...
  114 	   113 	 0.23984 	 0.23887 	 ~...
  115 	   114 	 0.24409 	 0.24075 	 ~...
  105 	   115 	 0.20529 	 0.25607 	 m..s
  120 	   116 	 0.26931 	 0.25834 	 ~...
  118 	   117 	 0.26743 	 0.26202 	 ~...
  119 	   118 	 0.26841 	 0.26317 	 ~...
  116 	   119 	 0.26114 	 0.26416 	 ~...
  117 	   120 	 0.26654 	 0.26979 	 ~...
==========================================
r_mrr = 0.781947672367096
r2_mrr = 0.4751855731010437
spearmanr_mrr@5 = 0.8687678575515747
spearmanr_mrr@10 = 0.9509217739105225
spearmanr_mrr@50 = 0.9404799938201904
spearmanr_mrr@100 = 0.8555206060409546
spearmanr_mrr@All = 0.893494725227356
==========================================
test time: 0.457
Done Testing dataset CoDExSmall
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.017 +- 0.000
mrr vals (pred, true): 0.370, 0.337

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.04593 	 0.03950 	 ~...
   46 	     1 	 0.05076 	 0.04162 	 ~...
   63 	     2 	 0.05715 	 0.04173 	 ~...
   67 	     3 	 0.05823 	 0.04202 	 ~...
    2 	     4 	 0.03993 	 0.04216 	 ~...
   51 	     5 	 0.05137 	 0.04217 	 ~...
   69 	     6 	 0.06344 	 0.04251 	 ~...
   73 	     7 	 0.06818 	 0.04258 	 ~...
   50 	     8 	 0.05125 	 0.04271 	 ~...
    9 	     9 	 0.04292 	 0.04281 	 ~...
   70 	    10 	 0.06346 	 0.04286 	 ~...
   38 	    11 	 0.04853 	 0.04290 	 ~...
   32 	    12 	 0.04771 	 0.04307 	 ~...
   39 	    13 	 0.04885 	 0.04310 	 ~...
   52 	    14 	 0.05149 	 0.04312 	 ~...
    7 	    15 	 0.04213 	 0.04325 	 ~...
   41 	    16 	 0.04930 	 0.04326 	 ~...
   34 	    17 	 0.04805 	 0.04341 	 ~...
   13 	    18 	 0.04354 	 0.04343 	 ~...
   42 	    19 	 0.04937 	 0.04345 	 ~...
   18 	    20 	 0.04579 	 0.04348 	 ~...
   60 	    21 	 0.05672 	 0.04354 	 ~...
    1 	    22 	 0.03938 	 0.04356 	 ~...
   44 	    23 	 0.05007 	 0.04362 	 ~...
   24 	    24 	 0.04660 	 0.04362 	 ~...
   11 	    25 	 0.04300 	 0.04363 	 ~...
   62 	    26 	 0.05710 	 0.04363 	 ~...
   25 	    27 	 0.04664 	 0.04370 	 ~...
    8 	    28 	 0.04253 	 0.04370 	 ~...
    5 	    29 	 0.04158 	 0.04372 	 ~...
   65 	    30 	 0.05726 	 0.04387 	 ~...
   66 	    31 	 0.05737 	 0.04392 	 ~...
   31 	    32 	 0.04753 	 0.04404 	 ~...
   27 	    33 	 0.04680 	 0.04409 	 ~...
   68 	    34 	 0.06076 	 0.04436 	 ~...
    4 	    35 	 0.04137 	 0.04447 	 ~...
   33 	    36 	 0.04800 	 0.04448 	 ~...
   15 	    37 	 0.04475 	 0.04460 	 ~...
   59 	    38 	 0.05630 	 0.04500 	 ~...
   57 	    39 	 0.05531 	 0.04519 	 ~...
   53 	    40 	 0.05186 	 0.04563 	 ~...
    6 	    41 	 0.04196 	 0.04578 	 ~...
   10 	    42 	 0.04298 	 0.04597 	 ~...
   43 	    43 	 0.04960 	 0.04624 	 ~...
   20 	    44 	 0.04621 	 0.04644 	 ~...
   22 	    45 	 0.04632 	 0.04700 	 ~...
   61 	    46 	 0.05707 	 0.04700 	 ~...
   23 	    47 	 0.04658 	 0.04713 	 ~...
   64 	    48 	 0.05718 	 0.04715 	 ~...
   30 	    49 	 0.04725 	 0.04748 	 ~...
   17 	    50 	 0.04560 	 0.04760 	 ~...
   35 	    51 	 0.04806 	 0.04772 	 ~...
   16 	    52 	 0.04477 	 0.04874 	 ~...
   29 	    53 	 0.04690 	 0.04884 	 ~...
   58 	    54 	 0.05615 	 0.04890 	 ~...
   36 	    55 	 0.04847 	 0.04895 	 ~...
   37 	    56 	 0.04849 	 0.04938 	 ~...
   72 	    57 	 0.06771 	 0.04955 	 ~...
   28 	    58 	 0.04687 	 0.04984 	 ~...
   47 	    59 	 0.05077 	 0.05148 	 ~...
   26 	    60 	 0.04667 	 0.05151 	 ~...
   74 	    61 	 0.08072 	 0.05154 	 ~...
   40 	    62 	 0.04922 	 0.05187 	 ~...
   49 	    63 	 0.05091 	 0.05232 	 ~...
   45 	    64 	 0.05041 	 0.05335 	 ~...
   56 	    65 	 0.05476 	 0.05367 	 ~...
    0 	    66 	 0.03310 	 0.05376 	 ~...
   54 	    67 	 0.05391 	 0.05389 	 ~...
   14 	    68 	 0.04468 	 0.05418 	 ~...
    3 	    69 	 0.04058 	 0.05448 	 ~...
   71 	    70 	 0.06616 	 0.05552 	 ~...
   55 	    71 	 0.05431 	 0.05586 	 ~...
   48 	    72 	 0.05082 	 0.05610 	 ~...
   21 	    73 	 0.04629 	 0.05696 	 ~...
   12 	    74 	 0.04307 	 0.06284 	 ~...
   82 	    75 	 0.22610 	 0.15183 	 m..s
   75 	    76 	 0.19746 	 0.15474 	 m..s
   84 	    77 	 0.25288 	 0.17582 	 m..s
   94 	    78 	 0.31300 	 0.19716 	 MISS
   79 	    79 	 0.21259 	 0.19838 	 ~...
   78 	    80 	 0.20839 	 0.20108 	 ~...
   80 	    81 	 0.21610 	 0.20539 	 ~...
   77 	    82 	 0.20794 	 0.20813 	 ~...
   76 	    83 	 0.20166 	 0.21326 	 ~...
   81 	    84 	 0.22514 	 0.21544 	 ~...
   83 	    85 	 0.24147 	 0.22266 	 ~...
   86 	    86 	 0.27858 	 0.23529 	 m..s
   91 	    87 	 0.30618 	 0.24573 	 m..s
   98 	    88 	 0.33198 	 0.26046 	 m..s
  110 	    89 	 0.36495 	 0.26618 	 m..s
  109 	    90 	 0.36080 	 0.26747 	 m..s
  102 	    91 	 0.33748 	 0.26893 	 m..s
  106 	    92 	 0.34098 	 0.27037 	 m..s
  116 	    93 	 0.37595 	 0.27120 	 MISS
   87 	    94 	 0.28974 	 0.27208 	 ~...
   92 	    95 	 0.30723 	 0.27473 	 m..s
   85 	    96 	 0.26461 	 0.27723 	 ~...
  108 	    97 	 0.34729 	 0.28228 	 m..s
  118 	    98 	 0.37928 	 0.28427 	 m..s
   89 	    99 	 0.29226 	 0.28547 	 ~...
   88 	   100 	 0.29177 	 0.28575 	 ~...
   95 	   101 	 0.32029 	 0.28723 	 m..s
   97 	   102 	 0.32669 	 0.28991 	 m..s
   93 	   103 	 0.31244 	 0.29005 	 ~...
  100 	   104 	 0.33691 	 0.29011 	 m..s
   96 	   105 	 0.32317 	 0.29257 	 m..s
   99 	   106 	 0.33656 	 0.29328 	 m..s
   90 	   107 	 0.29420 	 0.29412 	 ~...
  103 	   108 	 0.33798 	 0.29455 	 m..s
  101 	   109 	 0.33745 	 0.31448 	 ~...
  117 	   110 	 0.37745 	 0.31588 	 m..s
  115 	   111 	 0.37586 	 0.31651 	 m..s
  120 	   112 	 0.38699 	 0.32126 	 m..s
  113 	   113 	 0.37293 	 0.32833 	 m..s
  105 	   114 	 0.33954 	 0.33103 	 ~...
  104 	   115 	 0.33880 	 0.33701 	 ~...
  111 	   116 	 0.36999 	 0.33730 	 m..s
  107 	   117 	 0.34161 	 0.34644 	 ~...
  119 	   118 	 0.38228 	 0.34698 	 m..s
  114 	   119 	 0.37309 	 0.35056 	 ~...
  112 	   120 	 0.37065 	 0.35185 	 ~...
==========================================
r_mrr = 0.9853132367134094
r2_mrr = 0.9197419881820679
spearmanr_mrr@5 = 0.8363782167434692
spearmanr_mrr@10 = 0.9280110001564026
spearmanr_mrr@50 = 0.9934114813804626
spearmanr_mrr@100 = 0.9984872937202454
spearmanr_mrr@All = 0.9986060857772827
==========================================
test time: 0.545
Done Testing dataset Kinships
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.013 +- 0.017
mrr vals (pred, true): 0.223, 0.293

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   73 	     0 	 0.05963 	 0.00597 	 m..s
   62 	     1 	 0.05922 	 0.00602 	 m..s
   50 	     2 	 0.05894 	 0.00603 	 m..s
   67 	     3 	 0.05951 	 0.00612 	 m..s
    2 	     4 	 0.04535 	 0.00613 	 m..s
   15 	     5 	 0.05468 	 0.00621 	 m..s
    4 	     6 	 0.04549 	 0.00624 	 m..s
   11 	     7 	 0.05273 	 0.00626 	 m..s
   77 	     8 	 0.05966 	 0.00628 	 m..s
   10 	     9 	 0.05243 	 0.00629 	 m..s
    7 	    10 	 0.04838 	 0.00631 	 m..s
   17 	    11 	 0.05566 	 0.00632 	 m..s
    3 	    12 	 0.04548 	 0.00632 	 m..s
    9 	    13 	 0.05207 	 0.00634 	 m..s
   59 	    14 	 0.05910 	 0.00634 	 m..s
    1 	    15 	 0.04494 	 0.00634 	 m..s
   34 	    16 	 0.05868 	 0.00635 	 m..s
   71 	    17 	 0.05952 	 0.00636 	 m..s
   29 	    18 	 0.05844 	 0.00638 	 m..s
   41 	    19 	 0.05876 	 0.00638 	 m..s
   74 	    20 	 0.05963 	 0.00638 	 m..s
   35 	    21 	 0.05869 	 0.00640 	 m..s
   79 	    22 	 0.06010 	 0.00641 	 m..s
   12 	    23 	 0.05274 	 0.00642 	 m..s
   43 	    24 	 0.05881 	 0.00647 	 m..s
   46 	    25 	 0.05887 	 0.00649 	 m..s
   52 	    26 	 0.05899 	 0.00649 	 m..s
   13 	    27 	 0.05299 	 0.00652 	 m..s
   36 	    28 	 0.05871 	 0.00654 	 m..s
   37 	    29 	 0.05873 	 0.00654 	 m..s
   65 	    30 	 0.05946 	 0.00655 	 m..s
   72 	    31 	 0.05959 	 0.00656 	 m..s
   26 	    32 	 0.05826 	 0.00659 	 m..s
   19 	    33 	 0.05577 	 0.00659 	 m..s
   16 	    34 	 0.05550 	 0.00660 	 m..s
   48 	    35 	 0.05893 	 0.00664 	 m..s
   38 	    36 	 0.05875 	 0.00666 	 m..s
   18 	    37 	 0.05567 	 0.00670 	 m..s
   51 	    38 	 0.05895 	 0.00670 	 m..s
   44 	    39 	 0.05882 	 0.00675 	 m..s
    0 	    40 	 0.04146 	 0.00867 	 m..s
    5 	    41 	 0.04553 	 0.00885 	 m..s
    8 	    42 	 0.04893 	 0.01356 	 m..s
    6 	    43 	 0.04654 	 0.01453 	 m..s
   31 	    44 	 0.05849 	 0.01539 	 m..s
   30 	    45 	 0.05844 	 0.01671 	 m..s
   32 	    46 	 0.05855 	 0.01695 	 m..s
   42 	    47 	 0.05876 	 0.01857 	 m..s
   40 	    48 	 0.05875 	 0.02136 	 m..s
   39 	    49 	 0.05875 	 0.02222 	 m..s
   45 	    50 	 0.05887 	 0.02255 	 m..s
   33 	    51 	 0.05860 	 0.02327 	 m..s
   57 	    52 	 0.05906 	 0.02538 	 m..s
   14 	    53 	 0.05301 	 0.03492 	 ~...
   54 	    54 	 0.05904 	 0.03526 	 ~...
   53 	    55 	 0.05901 	 0.03818 	 ~...
   56 	    56 	 0.05904 	 0.03876 	 ~...
   66 	    57 	 0.05949 	 0.03893 	 ~...
   68 	    58 	 0.05951 	 0.04039 	 ~...
   64 	    59 	 0.05930 	 0.04417 	 ~...
   60 	    60 	 0.05919 	 0.05244 	 ~...
   85 	    61 	 0.08450 	 0.05635 	 ~...
   80 	    62 	 0.06015 	 0.05662 	 ~...
   88 	    63 	 0.09843 	 0.06170 	 m..s
   61 	    64 	 0.05921 	 0.06318 	 ~...
   75 	    65 	 0.05964 	 0.06528 	 ~...
   21 	    66 	 0.05646 	 0.06534 	 ~...
   82 	    67 	 0.06301 	 0.06558 	 ~...
   96 	    68 	 0.09918 	 0.06559 	 m..s
   63 	    69 	 0.05925 	 0.06586 	 ~...
   76 	    70 	 0.05965 	 0.07131 	 ~...
   84 	    71 	 0.07416 	 0.07267 	 ~...
   22 	    72 	 0.05688 	 0.07412 	 ~...
   70 	    73 	 0.05951 	 0.07477 	 ~...
   23 	    74 	 0.05719 	 0.07603 	 ~...
   20 	    75 	 0.05643 	 0.07875 	 ~...
   98 	    76 	 0.09966 	 0.08035 	 ~...
   78 	    77 	 0.05999 	 0.08277 	 ~...
   91 	    78 	 0.09872 	 0.08285 	 ~...
   99 	    79 	 0.10003 	 0.08425 	 ~...
   86 	    80 	 0.09840 	 0.08904 	 ~...
   89 	    81 	 0.09843 	 0.08906 	 ~...
   28 	    82 	 0.05842 	 0.09004 	 m..s
   24 	    83 	 0.05721 	 0.09252 	 m..s
   27 	    84 	 0.05838 	 0.09611 	 m..s
   55 	    85 	 0.05904 	 0.09679 	 m..s
   47 	    86 	 0.05890 	 0.09695 	 m..s
   58 	    87 	 0.05909 	 0.09713 	 m..s
   49 	    88 	 0.05893 	 0.09868 	 m..s
   81 	    89 	 0.06025 	 0.09922 	 m..s
   95 	    90 	 0.09917 	 0.10189 	 ~...
   25 	    91 	 0.05736 	 0.10274 	 m..s
   90 	    92 	 0.09864 	 0.10352 	 ~...
   94 	    93 	 0.09917 	 0.10629 	 ~...
  108 	    94 	 0.15263 	 0.11116 	 m..s
   69 	    95 	 0.05951 	 0.11240 	 m..s
   83 	    96 	 0.06606 	 0.11297 	 m..s
  111 	    97 	 0.16085 	 0.12010 	 m..s
  101 	    98 	 0.10228 	 0.12039 	 ~...
   87 	    99 	 0.09841 	 0.12726 	 ~...
  109 	   100 	 0.15356 	 0.12892 	 ~...
   97 	   101 	 0.09938 	 0.13613 	 m..s
   93 	   102 	 0.09916 	 0.13739 	 m..s
  100 	   103 	 0.10041 	 0.14725 	 m..s
   92 	   104 	 0.09910 	 0.14815 	 m..s
  105 	   105 	 0.11699 	 0.15132 	 m..s
  107 	   106 	 0.15169 	 0.15947 	 ~...
  106 	   107 	 0.11760 	 0.17938 	 m..s
  110 	   108 	 0.15918 	 0.19480 	 m..s
  102 	   109 	 0.10795 	 0.20702 	 m..s
  104 	   110 	 0.11144 	 0.21150 	 MISS
  112 	   111 	 0.17002 	 0.21253 	 m..s
  103 	   112 	 0.10847 	 0.21399 	 MISS
  114 	   113 	 0.17632 	 0.27178 	 m..s
  115 	   114 	 0.18357 	 0.27253 	 m..s
  113 	   115 	 0.17523 	 0.27828 	 MISS
  120 	   116 	 0.22452 	 0.28694 	 m..s
  118 	   117 	 0.22161 	 0.28843 	 m..s
  117 	   118 	 0.22026 	 0.28853 	 m..s
  119 	   119 	 0.22317 	 0.29280 	 m..s
  116 	   120 	 0.21174 	 0.29476 	 m..s
==========================================
r_mrr = 0.891351580619812
r2_mrr = 0.664851188659668
spearmanr_mrr@5 = 0.7876582145690918
spearmanr_mrr@10 = 0.8116453886032104
spearmanr_mrr@50 = 0.9752426147460938
spearmanr_mrr@100 = 0.9485099911689758
spearmanr_mrr@All = 0.9512947797775269
==========================================
test time: 0.455
Done Testing dataset OpenEA
total time taken: 828.3487331867218
training time taken: 789.1962468624115
TWIG out ;))
========================================================
--------------------------------------------------------
Running a TWIG experiment with tag: TransE-omit-Kinships
--------------------------------------------------------
========================================================
Using random seed: 8979582381977589
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [550, 679, 1124, 1211, 699, 347, 989, 958, 896, 1044, 414, 480, 123, 1032, 1002, 9, 532, 891, 1109, 590, 308, 1184, 761, 1066, 633, 211, 152, 167, 87, 231, 1117, 193, 14, 522, 905, 651, 1156, 269, 704, 985, 592, 456, 773, 112, 436, 1008, 163, 253, 715, 1058, 580, 93, 931, 635, 917, 1167, 956, 788, 714, 986, 908, 929, 486, 469, 192, 447, 1214, 109, 585, 1102, 66, 754, 667, 462, 593, 544, 766, 47, 127, 980, 1022, 26, 946, 583, 736, 313, 543, 529, 814, 560, 533, 1098, 757, 246, 831, 574, 11, 1182, 52, 774, 297, 78, 662, 735, 1110, 625, 1090, 1087, 733, 191, 95, 648, 725, 247, 718, 973, 1021, 457, 647, 350, 804]
valid_ids (0): []
train_ids (1094): [536, 72, 394, 397, 708, 319, 445, 869, 1132, 663, 504, 219, 55, 70, 873, 49, 1000, 139, 90, 32, 134, 1176, 1063, 960, 470, 731, 1094, 467, 997, 681, 1041, 894, 351, 499, 1054, 578, 652, 318, 861, 514, 454, 1212, 118, 450, 1161, 975, 630, 232, 759, 342, 954, 419, 468, 69, 463, 998, 815, 184, 924, 680, 312, 879, 1018, 439, 1147, 270, 660, 912, 51, 1005, 16, 172, 106, 767, 289, 311, 212, 1010, 38, 685, 4, 179, 654, 965, 791, 101, 1053, 1003, 594, 780, 813, 88, 117, 569, 772, 957, 858, 497, 372, 1192, 1141, 863, 709, 841, 1190, 205, 1116, 274, 115, 1180, 54, 1142, 655, 640, 28, 471, 868, 833, 613, 227, 1089, 830, 186, 793, 1015, 944, 910, 283, 503, 200, 854, 77, 61, 996, 208, 97, 886, 885, 22, 1175, 196, 723, 1194, 455, 615, 895, 687, 1204, 525, 310, 1049, 494, 63, 173, 161, 304, 1173, 346, 597, 806, 595, 398, 789, 727, 1091, 417, 126, 794, 900, 860, 103, 775, 666, 244, 336, 348, 981, 183, 104, 803, 745, 627, 327, 194, 160, 817, 538, 151, 534, 966, 1188, 601, 296, 427, 690, 618, 540, 872, 491, 816, 1144, 306, 716, 338, 121, 555, 226, 378, 672, 265, 1006, 68, 599, 665, 137, 158, 935, 992, 44, 874, 344, 696, 382, 57, 1017, 129, 8, 340, 145, 159, 1200, 484, 328, 189, 549, 403, 669, 703, 528, 675, 656, 508, 190, 906, 691, 125, 195, 1060, 693, 323, 942, 1011, 878, 255, 45, 620, 36, 220, 991, 769, 430, 423, 634, 983, 12, 582, 307, 1016, 294, 710, 1071, 1007, 412, 784, 519, 20, 301, 120, 132, 588, 632, 433, 753, 479, 76, 567, 326, 875, 1185, 25, 320, 472, 488, 1079, 82, 805, 855, 1073, 545, 851, 481, 1076, 923, 157, 376, 705, 807, 233, 402, 1163, 712, 421, 1096, 933, 1170, 19, 629, 239, 798, 930, 130, 418, 719, 18, 383, 420, 742, 1072, 74, 1202, 786, 379, 119, 1052, 238, 3, 535, 1140, 413, 722, 438, 330, 493, 116, 209, 553, 1055, 995, 1157, 100, 927, 598, 562, 758, 1131, 644, 29, 1080, 547, 460, 154, 676, 1134, 619, 395, 1160, 1166, 80, 778, 442, 556, 824, 240, 746, 967, 988, 926, 844, 809, 1128, 245, 261, 950, 506, 628, 437, 406, 584, 1048, 1101, 862, 361, 867, 1025, 501, 410, 1078, 302, 174, 1012, 102, 1196, 235, 64, 565, 81, 262, 837, 822, 1026, 367, 1033, 1046, 1119, 557, 744, 636, 1097, 877, 938, 1105, 89, 846, 739, 75, 899, 138, 495, 476, 994, 65, 10, 1057, 852, 554, 624, 1177, 974, 934, 1111, 968, 1179, 6, 489, 162, 771, 1209, 1146, 339, 399, 591, 298, 820, 1036, 474, 368, 229, 366, 537, 67, 1171, 864, 589, 242, 943, 686, 548, 916, 505, 1154, 579, 949, 43, 795, 2, 768, 444, 291, 800, 1198, 181, 1148, 292, 135, 937, 659, 642, 426, 637, 1103, 849, 459, 825, 842, 236, 185, 411, 210, 215, 1135, 993, 1039, 743, 581, 730, 558, 360, 790, 1034, 568, 963, 1118, 779, 371, 321, 317, 964, 452, 1029, 507, 428, 1207, 1174, 1085, 271, 432, 1189, 559, 358, 509, 293, 682, 1197, 1030, 113, 903, 728, 600, 1193, 252, 828, 770, 1165, 724, 810, 41, 415, 999, 288, 234, 928, 511, 440, 356, 177, 40, 1093, 913, 678, 1137, 969, 940, 280, 53, 85, 1037, 267, 1027, 385, 1130, 801, 902, 264, 740, 1020, 711, 1056, 979, 748, 94, 31, 990, 448, 254, 1059, 1151, 352, 1123, 747, 683, 889, 39, 566, 517, 502, 782, 96, 684, 984, 92, 354, 843, 586, 224, 441, 673, 1164, 153, 375, 520, 5, 977, 400, 871, 948, 230, 945, 273, 329, 305, 35, 1038, 407, 282, 384, 197, 475, 970, 309, 765, 1168, 325, 570, 250, 901, 701, 936, 355, 482, 110, 838, 107, 1074, 425, 142, 449, 290, 180, 865, 882, 571, 741, 564, 169, 892, 1050, 1100, 546, 904, 202, 608, 978, 689, 835, 1120, 552, 1, 435, 1043, 155, 111, 62, 941, 34, 277, 386, 37, 829, 364, 617, 201, 1115, 962, 60, 424, 148, 959, 1031, 492, 1213, 141, 259, 1088, 1028, 898, 812, 1023, 487, 490, 925, 561, 136, 334, 393, 108, 953, 1013, 30, 256, 1051, 182, 84, 832, 972, 316, 848, 343, 187, 337, 918, 1077, 604, 258, 1187, 671, 539, 363, 650, 1143, 91, 33, 808, 976, 58, 587, 688, 48, 612, 880, 390, 866, 341, 353, 515, 1068, 626, 396, 607, 7, 887, 677, 853, 614, 575, 146, 50, 404, 883, 1042, 408, 762, 485, 541, 907, 888, 299, 237, 653, 606, 643, 333, 1126, 381, 631, 698, 496, 836, 23, 621, 1095, 149, 1136, 369, 542, 1162, 287, 641, 216, 188, 921, 911, 1159, 295, 802, 21, 609, 847, 756, 98, 781, 884, 374, 1112, 670, 278, 150, 1152, 694, 331, 777, 1122, 332, 702, 222, 658, 458, 228, 225, 1083, 171, 1206, 1061, 56, 920, 144, 42, 839, 1114, 175, 1067, 818, 530, 573, 498, 707, 623, 516, 797, 1019, 1172, 218, 1201, 876, 221, 1099, 1113, 464, 523, 697, 596, 721, 881, 59, 859, 751, 939, 932, 834, 71, 732, 1205, 165, 13, 285, 300, 726, 214, 856, 178, 199, 105, 1035, 1169, 616, 776, 729, 783, 692, 796, 823, 961, 176, 206, 198, 453, 987, 1155, 204, 611, 477, 870, 915, 345, 99, 700, 720, 1125, 73, 909, 251, 1138, 286, 478, 1129, 1145, 279, 349, 263, 83, 713, 919, 622, 466, 513, 389, 982, 563, 787, 1081, 1107, 431, 276, 1075, 1001, 951, 223, 166, 1178, 785, 409, 315, 639, 649, 755, 955, 1104, 577, 638, 377, 1133, 646, 314, 133, 1139, 668, 1024, 122, 760, 850, 1127, 897, 443, 527, 521, 890, 335, 1014, 391, 1040, 1009, 213, 840, 827, 373, 128, 602, 131, 717, 1084, 17, 750, 749, 657, 610, 272, 217, 124, 524, 1065, 1195, 893, 416, 207, 947, 1149, 819, 1150, 531, 387, 303, 1121, 1183, 241, 1064, 512, 281, 526, 1208, 249, 792, 388, 661, 1186, 170, 1199, 1158, 1045, 674, 1092, 260, 845, 734, 1069, 1191, 357, 114, 603, 0, 645, 465, 392, 46, 510, 446, 168, 203, 461, 257, 79, 322, 763, 1181, 1070, 752, 429, 1082, 922, 664, 826, 695, 518, 1062, 268, 248, 737, 914, 1004, 380, 451, 434, 811, 284, 1047, 27, 422, 365, 1153, 551, 764, 401, 324, 1106, 857, 370, 143, 500, 275, 706, 483, 738, 1086, 799, 1210, 1108, 952, 86, 576, 405, 266, 140, 473, 147, 572, 243, 15, 971, 24, 605, 821, 359, 164, 1203, 156, 362]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3665101435926428
the save name prefix for this run is:  chkpt-ID_3665101435926428_tag_TransE-omit-Kinships
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1'], 'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 134
rank avg (pred): 0.433 +- 0.004
mrr vals (pred, true): 0.017, 0.129
batch losses (mrrl, rdl): 0.0, 0.0008955126

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 924
rank avg (pred): 0.375 +- 0.003
mrr vals (pred, true): 0.020, 0.069
batch losses (mrrl, rdl): 0.0, 0.0001188062

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 48
rank avg (pred): 0.070 +- 0.006
mrr vals (pred, true): 0.098, 0.505
batch losses (mrrl, rdl): 0.0, 3.06378e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 148
rank avg (pred): 0.385 +- 0.101
mrr vals (pred, true): 0.022, 0.099
batch losses (mrrl, rdl): 0.0, 0.0003246836

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 841
rank avg (pred): 0.278 +- 0.210
mrr vals (pred, true): 0.269, 0.080
batch losses (mrrl, rdl): 0.0, 5.2293e-06

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 894
rank avg (pred): 0.106 +- 0.079
mrr vals (pred, true): 0.320, 0.170
batch losses (mrrl, rdl): 0.0, 1.03936e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 293
rank avg (pred): 0.054 +- 0.049
mrr vals (pred, true): 0.418, 0.628
batch losses (mrrl, rdl): 0.0, 2.57814e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 864
rank avg (pred): 0.316 +- 0.240
mrr vals (pred, true): 0.316, 0.041
batch losses (mrrl, rdl): 0.0, 0.0002227781

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 852
rank avg (pred): 0.344 +- 0.248
mrr vals (pred, true): 0.291, 0.093
batch losses (mrrl, rdl): 0.0, 0.0001872815

Epoch over!
epoch time: 51.036

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 250
rank avg (pred): 0.050 +- 0.046
mrr vals (pred, true): 0.422, 0.546
batch losses (mrrl, rdl): 0.0, 1.41304e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 981
rank avg (pred): 0.031 +- 0.030
mrr vals (pred, true): 0.461, 0.613
batch losses (mrrl, rdl): 0.0, 2.4273e-06

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 369
rank avg (pred): 0.293 +- 0.252
mrr vals (pred, true): 0.371, 0.106
batch losses (mrrl, rdl): 0.0, 0.0001196479

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1054
rank avg (pred): 0.037 +- 0.036
mrr vals (pred, true): 0.457, 0.581
batch losses (mrrl, rdl): 0.0, 4.5684e-06

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 759
rank avg (pred): 0.340 +- 0.274
mrr vals (pred, true): 0.344, 0.090
batch losses (mrrl, rdl): 0.0, 0.0001620869

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 909
rank avg (pred): 0.088 +- 0.081
mrr vals (pred, true): 0.398, 0.239
batch losses (mrrl, rdl): 0.0, 7.827e-07

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1132
rank avg (pred): 0.339 +- 0.255
mrr vals (pred, true): 0.300, 0.039
batch losses (mrrl, rdl): 0.0, 0.000110513

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 116
rank avg (pred): 0.254 +- 0.230
mrr vals (pred, true): 0.405, 0.107
batch losses (mrrl, rdl): 0.0, 1.03238e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1130
rank avg (pred): 0.298 +- 0.249
mrr vals (pred, true): 0.374, 0.044
batch losses (mrrl, rdl): 0.0, 0.000204432

Epoch over!
epoch time: 52.511

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 497
rank avg (pred): 0.275 +- 0.204
mrr vals (pred, true): 0.305, 0.112
batch losses (mrrl, rdl): 0.0, 1.83649e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 274
rank avg (pred): 0.046 +- 0.049
mrr vals (pred, true): 0.456, 0.576
batch losses (mrrl, rdl): 0.0, 1.08386e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 402
rank avg (pred): 0.320 +- 0.257
mrr vals (pred, true): 0.350, 0.096
batch losses (mrrl, rdl): 0.0, 0.000118084

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 380
rank avg (pred): 0.330 +- 0.260
mrr vals (pred, true): 0.333, 0.131
batch losses (mrrl, rdl): 0.0, 0.000254419

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 936
rank avg (pred): 0.332 +- 0.250
mrr vals (pred, true): 0.313, 0.088
batch losses (mrrl, rdl): 0.0, 9.02442e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 745
rank avg (pred): 0.017 +- 0.022
mrr vals (pred, true): 0.533, 0.584
batch losses (mrrl, rdl): 0.0, 1.6414e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 906
rank avg (pred): 0.140 +- 0.128
mrr vals (pred, true): 0.407, 0.294
batch losses (mrrl, rdl): 0.0, 0.000154421

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 790
rank avg (pred): 0.327 +- 0.259
mrr vals (pred, true): 0.321, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001193342

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1103
rank avg (pred): 0.310 +- 0.253
mrr vals (pred, true): 0.349, 0.149
batch losses (mrrl, rdl): 0.0, 0.000237716

Epoch over!
epoch time: 52.787

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 719
rank avg (pred): 0.374 +- 0.278
mrr vals (pred, true): 0.283, 0.039
batch losses (mrrl, rdl): 0.0, 4.02386e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 661
rank avg (pred): 0.423 +- 0.296
mrr vals (pred, true): 0.251, 0.041
batch losses (mrrl, rdl): 0.0, 1.17945e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 680
rank avg (pred): 0.447 +- 0.282
mrr vals (pred, true): 0.200, 0.039
batch losses (mrrl, rdl): 0.0, 7.8703e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1086
rank avg (pred): 0.290 +- 0.240
mrr vals (pred, true): 0.362, 0.142
batch losses (mrrl, rdl): 0.0, 0.0001499735

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 844
rank avg (pred): 0.338 +- 0.262
mrr vals (pred, true): 0.295, 0.079
batch losses (mrrl, rdl): 0.0, 7.79726e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 319
rank avg (pred): 0.025 +- 0.043
mrr vals (pred, true): 0.538, 0.562
batch losses (mrrl, rdl): 0.0, 7.34e-08

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 654
rank avg (pred): 0.432 +- 0.278
mrr vals (pred, true): 0.203, 0.042
batch losses (mrrl, rdl): 0.0, 1.78543e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 497
rank avg (pred): 0.261 +- 0.224
mrr vals (pred, true): 0.350, 0.112
batch losses (mrrl, rdl): 0.0, 1.33711e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 312
rank avg (pred): 0.027 +- 0.038
mrr vals (pred, true): 0.506, 0.523
batch losses (mrrl, rdl): 0.0, 1.491e-07

Epoch over!
epoch time: 52.49

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 460
rank avg (pred): 0.332 +- 0.253
mrr vals (pred, true): 0.293, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001132881

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 395
rank avg (pred): 0.325 +- 0.245
mrr vals (pred, true): 0.311, 0.135
batch losses (mrrl, rdl): 0.0, 0.0003207378

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1057
rank avg (pred): 0.033 +- 0.048
mrr vals (pred, true): 0.498, 0.617
batch losses (mrrl, rdl): 0.0, 4.0593e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 321
rank avg (pred): 0.024 +- 0.037
mrr vals (pred, true): 0.538, 0.503
batch losses (mrrl, rdl): 0.0, 6.519e-07

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 687
rank avg (pred): 0.388 +- 0.286
mrr vals (pred, true): 0.270, 0.041
batch losses (mrrl, rdl): 0.0, 1.80965e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 373
rank avg (pred): 0.332 +- 0.245
mrr vals (pred, true): 0.287, 0.108
batch losses (mrrl, rdl): 0.0, 0.000222566

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 294
rank avg (pred): 0.065 +- 0.078
mrr vals (pred, true): 0.450, 0.513
batch losses (mrrl, rdl): 0.0, 3.20526e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 34
rank avg (pred): 0.032 +- 0.054
mrr vals (pred, true): 0.528, 0.561
batch losses (mrrl, rdl): 0.0, 8.528e-07

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 176
rank avg (pred): 0.314 +- 0.250
mrr vals (pred, true): 0.335, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001986546

Epoch over!
epoch time: 51.278

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 957
rank avg (pred): 0.344 +- 0.244
mrr vals (pred, true): 0.248, 0.039
batch losses (mrrl, rdl): 0.3906674683, 8.12819e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 51
rank avg (pred): 0.344 +- 0.463
mrr vals (pred, true): 0.531, 0.506
batch losses (mrrl, rdl): 0.006031719, 0.0019347223

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1006
rank avg (pred): 0.431 +- 0.456
mrr vals (pred, true): 0.127, 0.145
batch losses (mrrl, rdl): 0.0032034991, 0.0007606016

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 353
rank avg (pred): 0.461 +- 0.407
mrr vals (pred, true): 0.068, 0.138
batch losses (mrrl, rdl): 0.0488382913, 0.0007529408

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1014
rank avg (pred): 0.431 +- 0.451
mrr vals (pred, true): 0.112, 0.138
batch losses (mrrl, rdl): 0.006518499, 0.0006778516

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 270
rank avg (pred): 0.329 +- 0.455
mrr vals (pred, true): 0.544, 0.526
batch losses (mrrl, rdl): 0.0033180579, 0.0017770405

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 789
rank avg (pred): 0.504 +- 0.372
mrr vals (pred, true): 0.059, 0.039
batch losses (mrrl, rdl): 0.0007614939, 7.69377e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 428
rank avg (pred): 0.412 +- 0.417
mrr vals (pred, true): 0.094, 0.037
batch losses (mrrl, rdl): 0.0194058791, 0.0002331217

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1147
rank avg (pred): 0.414 +- 0.465
mrr vals (pred, true): 0.158, 0.159
batch losses (mrrl, rdl): 1.16635e-05, 0.0007348289

Epoch over!
epoch time: 53.802

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 116
rank avg (pred): 0.414 +- 0.345
mrr vals (pred, true): 0.069, 0.107
batch losses (mrrl, rdl): 0.014162086, 0.0003651157

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 565
rank avg (pred): 0.416 +- 0.396
mrr vals (pred, true): 0.073, 0.072
batch losses (mrrl, rdl): 0.0053199376, 0.000131954

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 824
rank avg (pred): 0.176 +- 0.258
mrr vals (pred, true): 0.551, 0.544
batch losses (mrrl, rdl): 0.0004770481, 0.0005863432

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 586
rank avg (pred): 0.677 +- 0.255
mrr vals (pred, true): 0.044, 0.036
batch losses (mrrl, rdl): 0.0003116872, 0.001109013

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 888
rank avg (pred): 0.414 +- 0.327
mrr vals (pred, true): 0.075, 0.037
batch losses (mrrl, rdl): 0.0063307034, 7.15482e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 438
rank avg (pred): 0.398 +- 0.227
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0014120095, 8.05214e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 876
rank avg (pred): 0.308 +- 0.186
mrr vals (pred, true): 0.073, 0.037
batch losses (mrrl, rdl): 0.0053195492, 0.0004249998

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 258
rank avg (pred): 0.066 +- 0.086
mrr vals (pred, true): 0.560, 0.509
batch losses (mrrl, rdl): 0.0257836618, 3.438e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1186
rank avg (pred): 0.419 +- 0.207
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0013473893, 1.58578e-05

Epoch over!
epoch time: 52.929

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 871
rank avg (pred): 0.325 +- 0.160
mrr vals (pred, true): 0.066, 0.040
batch losses (mrrl, rdl): 0.0026196497, 0.0003056862

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 136
rank avg (pred): 0.357 +- 0.126
mrr vals (pred, true): 0.058, 0.092
batch losses (mrrl, rdl): 0.0006633825, 0.0001634086

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 790
rank avg (pred): 0.298 +- 0.134
mrr vals (pred, true): 0.065, 0.042
batch losses (mrrl, rdl): 0.0022662468, 0.0003962914

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 615
rank avg (pred): 0.452 +- 0.131
mrr vals (pred, true): 0.052, 0.034
batch losses (mrrl, rdl): 5.63238e-05, 5.02244e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 223
rank avg (pred): 0.382 +- 0.112
mrr vals (pred, true): 0.061, 0.039
batch losses (mrrl, rdl): 0.0011156559, 0.000137832

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 36
rank avg (pred): 0.131 +- 0.176
mrr vals (pred, true): 0.554, 0.517
batch losses (mrrl, rdl): 0.0135785919, 0.0002971788

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1151
rank avg (pred): 0.281 +- 0.194
mrr vals (pred, true): 0.090, 0.162
batch losses (mrrl, rdl): 0.0506151505, 9.03842e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 576
rank avg (pred): 0.482 +- 0.124
mrr vals (pred, true): 0.043, 0.036
batch losses (mrrl, rdl): 0.0004710908, 0.000113789

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 316
rank avg (pred): 0.071 +- 0.095
mrr vals (pred, true): 0.576, 0.559
batch losses (mrrl, rdl): 0.0028829789, 5.45032e-05

Epoch over!
epoch time: 51.425

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 387
rank avg (pred): 0.362 +- 0.122
mrr vals (pred, true): 0.066, 0.099
batch losses (mrrl, rdl): 0.002701286, 0.0002605729

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 251
rank avg (pred): 0.068 +- 0.097
mrr vals (pred, true): 0.602, 0.612
batch losses (mrrl, rdl): 0.0011105032, 5.86223e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 223
rank avg (pred): 0.387 +- 0.107
mrr vals (pred, true): 0.055, 0.039
batch losses (mrrl, rdl): 0.0002107844, 0.0001286606

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1015
rank avg (pred): 0.295 +- 0.169
mrr vals (pred, true): 0.080, 0.141
batch losses (mrrl, rdl): 0.0371672511, 0.0001158369

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 778
rank avg (pred): 0.295 +- 0.145
mrr vals (pred, true): 0.077, 0.088
batch losses (mrrl, rdl): 0.0072096391, 2.427e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 8
rank avg (pred): 0.078 +- 0.117
mrr vals (pred, true): 0.604, 0.612
batch losses (mrrl, rdl): 0.0006243354, 9.36218e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1145
rank avg (pred): 0.232 +- 0.206
mrr vals (pred, true): 0.118, 0.155
batch losses (mrrl, rdl): 0.0138779879, 1.18278e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 425
rank avg (pred): 0.223 +- 0.160
mrr vals (pred, true): 0.092, 0.038
batch losses (mrrl, rdl): 0.0177371763, 0.0007785564

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 977
rank avg (pred): 0.090 +- 0.132
mrr vals (pred, true): 0.602, 0.619
batch losses (mrrl, rdl): 0.0030548635, 0.0001344716

Epoch over!
epoch time: 52.678

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 151
rank avg (pred): 0.355 +- 0.128
mrr vals (pred, true): 0.053, 0.101
batch losses (mrrl, rdl): 0.0229692943, 0.0001687087

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1133
rank avg (pred): 0.251 +- 0.176
mrr vals (pred, true): 0.106, 0.039
batch losses (mrrl, rdl): 0.0316366628, 0.0007461327

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 451
rank avg (pred): 0.268 +- 0.129
mrr vals (pred, true): 0.073, 0.040
batch losses (mrrl, rdl): 0.005449343, 0.0006363499

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 882
rank avg (pred): 0.359 +- 0.131
mrr vals (pred, true): 0.054, 0.037
batch losses (mrrl, rdl): 0.000197584, 0.000222609

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 262
rank avg (pred): 0.117 +- 0.147
mrr vals (pred, true): 0.548, 0.550
batch losses (mrrl, rdl): 2.71201e-05, 0.0002305978

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 609
rank avg (pred): 0.472 +- 0.151
mrr vals (pred, true): 0.053, 0.032
batch losses (mrrl, rdl): 8.19535e-05, 4.30336e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 38
rank avg (pred): 0.117 +- 0.147
mrr vals (pred, true): 0.541, 0.618
batch losses (mrrl, rdl): 0.0594986752, 0.0002583782

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 88
rank avg (pred): 0.287 +- 0.100
mrr vals (pred, true): 0.056, 0.096
batch losses (mrrl, rdl): 0.0003461654, 5.21037e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 923
rank avg (pred): 0.509 +- 0.188
mrr vals (pred, true): 0.055, 0.090
batch losses (mrrl, rdl): 0.0002190641, 0.0010818621

Epoch over!
epoch time: 53.401

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 906
rank avg (pred): 0.165 +- 0.197
mrr vals (pred, true): 0.383, 0.294
batch losses (mrrl, rdl): 0.0789679438, 0.000301174

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 20
rank avg (pred): 0.075 +- 0.112
mrr vals (pred, true): 0.613, 0.608
batch losses (mrrl, rdl): 0.00025779, 8.15091e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 185
rank avg (pred): 0.288 +- 0.111
mrr vals (pred, true): 0.073, 0.044
batch losses (mrrl, rdl): 0.0050684246, 0.000486968

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 499
rank avg (pred): 0.270 +- 0.110
mrr vals (pred, true): 0.068, 0.076
batch losses (mrrl, rdl): 0.0032539289, 0.0001371339

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 7
rank avg (pred): 0.076 +- 0.101
mrr vals (pred, true): 0.563, 0.544
batch losses (mrrl, rdl): 0.0033516262, 6.01732e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 162
rank avg (pred): 0.374 +- 0.126
mrr vals (pred, true): 0.055, 0.036
batch losses (mrrl, rdl): 0.0002057063, 0.0001876685

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 306
rank avg (pred): 0.092 +- 0.122
mrr vals (pred, true): 0.554, 0.532
batch losses (mrrl, rdl): 0.0047185645, 0.0001129971

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 380
rank avg (pred): 0.203 +- 0.135
mrr vals (pred, true): 0.089, 0.131
batch losses (mrrl, rdl): 0.0176121127, 4.98667e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 83
rank avg (pred): 0.291 +- 0.096
mrr vals (pred, true): 0.061, 0.116
batch losses (mrrl, rdl): 0.0306758825, 8.541e-05

Epoch over!
epoch time: 53.103

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 370
rank avg (pred): 0.265 +- 0.109
mrr vals (pred, true): 0.071, 0.115
batch losses (mrrl, rdl): 0.0187188592, 3.67326e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1207
rank avg (pred): 0.491 +- 0.169
mrr vals (pred, true): 0.046, 0.040
batch losses (mrrl, rdl): 0.0001242245, 8.68022e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 991
rank avg (pred): 0.069 +- 0.114
mrr vals (pred, true): 0.635, 0.614
batch losses (mrrl, rdl): 0.0045038825, 6.51202e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 76
rank avg (pred): 0.105 +- 0.140
mrr vals (pred, true): 0.563, 0.551
batch losses (mrrl, rdl): 0.0014670106, 0.0001734434

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 656
rank avg (pred): 0.380 +- 0.133
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 1.521e-07, 0.0001574521

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1213
rank avg (pred): 0.548 +- 0.200
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.0002023329, 0.000241716

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 220
rank avg (pred): 0.357 +- 0.131
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0014475895, 0.0002178595

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 616
rank avg (pred): 0.553 +- 0.216
mrr vals (pred, true): 0.044, 0.038
batch losses (mrrl, rdl): 0.0003050723, 0.0002462486

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 713
rank avg (pred): 0.422 +- 0.144
mrr vals (pred, true): 0.054, 0.037
batch losses (mrrl, rdl): 0.0001434478, 6.04302e-05

Epoch over!
epoch time: 53.105

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1166
rank avg (pred): 0.632 +- 0.273
mrr vals (pred, true): 0.044, 0.037
batch losses (mrrl, rdl): 0.0003870997, 0.0009173428

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 721
rank avg (pred): 0.485 +- 0.184
mrr vals (pred, true): 0.051, 0.040
batch losses (mrrl, rdl): 1.25287e-05, 5.81997e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 121
rank avg (pred): 0.317 +- 0.117
mrr vals (pred, true): 0.058, 0.095
batch losses (mrrl, rdl): 0.000601205, 9.29604e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 795
rank avg (pred): 0.379 +- 0.204
mrr vals (pred, true): 0.066, 0.037
batch losses (mrrl, rdl): 0.0026870219, 0.000130176

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 259
rank avg (pred): 0.101 +- 0.131
mrr vals (pred, true): 0.561, 0.546
batch losses (mrrl, rdl): 0.0021303061, 0.0001514539

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1164
rank avg (pred): 0.552 +- 0.240
mrr vals (pred, true): 0.053, 0.040
batch losses (mrrl, rdl): 6.60741e-05, 0.0003849889

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 792
rank avg (pred): 0.322 +- 0.203
mrr vals (pred, true): 0.066, 0.040
batch losses (mrrl, rdl): 0.0025481903, 0.0003688753

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 565
rank avg (pred): 0.231 +- 0.150
mrr vals (pred, true): 0.074, 0.072
batch losses (mrrl, rdl): 0.0058163563, 0.0004042976

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 398
rank avg (pred): 0.195 +- 0.174
mrr vals (pred, true): 0.099, 0.139
batch losses (mrrl, rdl): 0.0163164698, 3.29425e-05

Epoch over!
epoch time: 52.663

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 25
rank avg (pred): 0.089 +- 0.117
mrr vals (pred, true): 0.602, 0.544
batch losses (mrrl, rdl): 0.0335573889, 0.0001114313

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 348
rank avg (pred): 0.331 +- 0.170
mrr vals (pred, true): 0.060, 0.107
batch losses (mrrl, rdl): 0.0222164355, 8.545e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 143
rank avg (pred): 0.221 +- 0.155
mrr vals (pred, true): 0.074, 0.101
batch losses (mrrl, rdl): 0.0074948641, 7.74885e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 674
rank avg (pred): 0.533 +- 0.224
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 5.7406e-06, 0.0001828296

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 730
rank avg (pred): 0.119 +- 0.152
mrr vals (pred, true): 0.450, 0.444
batch losses (mrrl, rdl): 0.0003895261, 0.0001237664

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 820
rank avg (pred): 0.133 +- 0.193
mrr vals (pred, true): 0.507, 0.427
batch losses (mrrl, rdl): 0.0636957809, 0.0001615471

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 742
rank avg (pred): 0.138 +- 0.185
mrr vals (pred, true): 0.482, 0.512
batch losses (mrrl, rdl): 0.0089851655, 0.0002658937

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 700
rank avg (pred): 0.491 +- 0.223
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 2.6269e-06, 4.53316e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 359
rank avg (pred): 0.242 +- 0.237
mrr vals (pred, true): 0.089, 0.129
batch losses (mrrl, rdl): 0.0162174385, 1.3443e-06

Epoch over!
epoch time: 52.675

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 822
rank avg (pred): 0.135 +- 0.195
mrr vals (pred, true): 0.541, 0.538
batch losses (mrrl, rdl): 0.0001353707, 0.0002728284

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 16
rank avg (pred): 0.111 +- 0.160
mrr vals (pred, true): 0.538, 0.556
batch losses (mrrl, rdl): 0.0034229523, 0.0001876875

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 587
rank avg (pred): 0.511 +- 0.243
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0001028013, 8.22534e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 158
rank avg (pred): 0.232 +- 0.247
mrr vals (pred, true): 0.095, 0.117
batch losses (mrrl, rdl): 0.0047637485, 1.22781e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1126
rank avg (pred): 0.256 +- 0.247
mrr vals (pred, true): 0.085, 0.038
batch losses (mrrl, rdl): 0.0120095983, 0.0007291581

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 708
rank avg (pred): 0.446 +- 0.246
mrr vals (pred, true): 0.048, 0.039
batch losses (mrrl, rdl): 3.81768e-05, 7.6123e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 358
rank avg (pred): 0.331 +- 0.264
mrr vals (pred, true): 0.065, 0.096
batch losses (mrrl, rdl): 0.0021485677, 2.00463e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 983
rank avg (pred): 0.088 +- 0.124
mrr vals (pred, true): 0.595, 0.589
batch losses (mrrl, rdl): 0.0004670886, 0.000117521

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 963
rank avg (pred): 0.378 +- 0.264
mrr vals (pred, true): 0.056, 0.038
batch losses (mrrl, rdl): 0.0003472885, 0.0001347316

Epoch over!
epoch time: 54.134

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.317 +- 0.229
mrr vals (pred, true): 0.054, 0.081

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.04573 	 0.03422 	 ~...
    4 	     1 	 0.04195 	 0.03423 	 ~...
   21 	     2 	 0.04583 	 0.03530 	 ~...
    0 	     3 	 0.03804 	 0.03532 	 ~...
   10 	     4 	 0.04368 	 0.03555 	 ~...
   53 	     5 	 0.05463 	 0.03597 	 ~...
    8 	     6 	 0.04360 	 0.03637 	 ~...
   28 	     7 	 0.04747 	 0.03662 	 ~...
   61 	     8 	 0.06209 	 0.03672 	 ~...
   34 	     9 	 0.04907 	 0.03678 	 ~...
   50 	    10 	 0.05446 	 0.03692 	 ~...
   77 	    11 	 0.07323 	 0.03757 	 m..s
    1 	    12 	 0.04086 	 0.03760 	 ~...
   15 	    13 	 0.04430 	 0.03774 	 ~...
   68 	    14 	 0.06716 	 0.03795 	 ~...
   61 	    15 	 0.06209 	 0.03801 	 ~...
   74 	    16 	 0.07102 	 0.03810 	 m..s
    7 	    17 	 0.04356 	 0.03824 	 ~...
   17 	    18 	 0.04482 	 0.03828 	 ~...
    3 	    19 	 0.04184 	 0.03833 	 ~...
   58 	    20 	 0.05955 	 0.03854 	 ~...
    5 	    21 	 0.04287 	 0.03862 	 ~...
    9 	    22 	 0.04367 	 0.03872 	 ~...
   70 	    23 	 0.06843 	 0.03883 	 ~...
   19 	    24 	 0.04571 	 0.03911 	 ~...
   11 	    25 	 0.04383 	 0.03915 	 ~...
   44 	    26 	 0.05275 	 0.03915 	 ~...
   40 	    27 	 0.05166 	 0.03917 	 ~...
   32 	    28 	 0.04870 	 0.03928 	 ~...
    6 	    29 	 0.04351 	 0.03945 	 ~...
   26 	    30 	 0.04668 	 0.03948 	 ~...
   83 	    31 	 0.09823 	 0.03961 	 m..s
   61 	    32 	 0.06209 	 0.03964 	 ~...
   18 	    33 	 0.04570 	 0.03977 	 ~...
   36 	    34 	 0.04973 	 0.03992 	 ~...
   24 	    35 	 0.04664 	 0.04000 	 ~...
   23 	    36 	 0.04652 	 0.04012 	 ~...
   67 	    37 	 0.06625 	 0.04013 	 ~...
   14 	    38 	 0.04424 	 0.04070 	 ~...
   22 	    39 	 0.04641 	 0.04113 	 ~...
   46 	    40 	 0.05394 	 0.04119 	 ~...
   31 	    41 	 0.04826 	 0.04210 	 ~...
   38 	    42 	 0.05058 	 0.04217 	 ~...
   61 	    43 	 0.06209 	 0.04227 	 ~...
   16 	    44 	 0.04466 	 0.04243 	 ~...
   61 	    45 	 0.06209 	 0.04254 	 ~...
   12 	    46 	 0.04392 	 0.04328 	 ~...
   82 	    47 	 0.09405 	 0.04329 	 m..s
   49 	    48 	 0.05427 	 0.04408 	 ~...
   39 	    49 	 0.05063 	 0.04457 	 ~...
   25 	    50 	 0.04664 	 0.04459 	 ~...
   30 	    51 	 0.04826 	 0.04598 	 ~...
   55 	    52 	 0.05688 	 0.04950 	 ~...
    2 	    53 	 0.04126 	 0.06644 	 ~...
   13 	    54 	 0.04394 	 0.06766 	 ~...
   60 	    55 	 0.06192 	 0.06896 	 ~...
   51 	    56 	 0.05453 	 0.07306 	 ~...
   29 	    57 	 0.04813 	 0.07645 	 ~...
   27 	    58 	 0.04709 	 0.07992 	 m..s
   48 	    59 	 0.05412 	 0.08144 	 ~...
   33 	    60 	 0.04874 	 0.08385 	 m..s
   37 	    61 	 0.05048 	 0.08450 	 m..s
   42 	    62 	 0.05195 	 0.08483 	 m..s
   43 	    63 	 0.05241 	 0.08797 	 m..s
   54 	    64 	 0.05522 	 0.08873 	 m..s
   41 	    65 	 0.05175 	 0.08880 	 m..s
   45 	    66 	 0.05288 	 0.08924 	 m..s
   47 	    67 	 0.05403 	 0.09253 	 m..s
   52 	    68 	 0.05455 	 0.09304 	 m..s
   56 	    69 	 0.05717 	 0.09496 	 m..s
   57 	    70 	 0.05748 	 0.09651 	 m..s
   35 	    71 	 0.04956 	 0.10136 	 m..s
   61 	    72 	 0.06209 	 0.10161 	 m..s
   86 	    73 	 0.10441 	 0.10634 	 ~...
   80 	    74 	 0.07778 	 0.10944 	 m..s
   59 	    75 	 0.06137 	 0.11491 	 m..s
   73 	    76 	 0.06914 	 0.11767 	 m..s
   78 	    77 	 0.07454 	 0.11779 	 m..s
   85 	    78 	 0.10412 	 0.12112 	 ~...
   71 	    79 	 0.06867 	 0.12201 	 m..s
   76 	    80 	 0.07245 	 0.12305 	 m..s
   69 	    81 	 0.06806 	 0.12364 	 m..s
   72 	    82 	 0.06875 	 0.12771 	 m..s
   84 	    83 	 0.10377 	 0.13723 	 m..s
   75 	    84 	 0.07180 	 0.14185 	 m..s
   87 	    85 	 0.11712 	 0.14338 	 ~...
   81 	    86 	 0.08357 	 0.14453 	 m..s
   79 	    87 	 0.07648 	 0.14459 	 m..s
   89 	    88 	 0.18443 	 0.17167 	 ~...
   88 	    89 	 0.16666 	 0.21459 	 m..s
   92 	    90 	 0.33794 	 0.31332 	 ~...
   90 	    91 	 0.26979 	 0.31539 	 m..s
   91 	    92 	 0.33565 	 0.32864 	 ~...
   94 	    93 	 0.42474 	 0.34052 	 m..s
   93 	    94 	 0.42060 	 0.36050 	 m..s
  104 	    95 	 0.53088 	 0.49255 	 m..s
  100 	    96 	 0.52141 	 0.50333 	 ~...
   98 	    97 	 0.49789 	 0.50860 	 ~...
   99 	    98 	 0.50867 	 0.51304 	 ~...
   95 	    99 	 0.43132 	 0.52124 	 m..s
   96 	   100 	 0.44115 	 0.52523 	 m..s
  107 	   101 	 0.55578 	 0.53235 	 ~...
  101 	   102 	 0.52206 	 0.53417 	 ~...
  103 	   103 	 0.53086 	 0.54533 	 ~...
   97 	   104 	 0.48258 	 0.55518 	 m..s
  105 	   105 	 0.53129 	 0.55974 	 ~...
  106 	   106 	 0.55125 	 0.56505 	 ~...
  102 	   107 	 0.53015 	 0.57161 	 m..s
  108 	   108 	 0.57806 	 0.60595 	 ~...
  109 	   109 	 0.57883 	 0.60653 	 ~...
  118 	   110 	 0.60475 	 0.60871 	 ~...
  116 	   111 	 0.60221 	 0.61698 	 ~...
  114 	   112 	 0.60050 	 0.61885 	 ~...
  111 	   113 	 0.59581 	 0.61906 	 ~...
  115 	   114 	 0.60166 	 0.61930 	 ~...
  120 	   115 	 0.61545 	 0.61970 	 ~...
  112 	   116 	 0.59801 	 0.62092 	 ~...
  113 	   117 	 0.59977 	 0.62127 	 ~...
  110 	   118 	 0.58627 	 0.62257 	 m..s
  119 	   119 	 0.60842 	 0.62441 	 ~...
  117 	   120 	 0.60469 	 0.62896 	 ~...
==========================================
r_mrr = 0.9897915124893188
r2_mrr = 0.9776699542999268
spearmanr_mrr@5 = 0.9879820942878723
spearmanr_mrr@10 = 0.9897986054420471
spearmanr_mrr@50 = 0.9939853549003601
spearmanr_mrr@100 = 0.9946168065071106
spearmanr_mrr@All = 0.9947668313980103
==========================================
test time: 0.478
Done Testing dataset UMLS
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.248 +- 0.273
mrr vals (pred, true): 0.053, 0.055

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  104 	     0 	 0.14153 	 0.00017 	 MISS
   49 	     1 	 0.07338 	 0.00018 	 m..s
   49 	     2 	 0.07338 	 0.00018 	 m..s
   85 	     3 	 0.10379 	 0.00018 	 MISS
    9 	     4 	 0.05187 	 0.00019 	 m..s
   57 	     5 	 0.07543 	 0.00019 	 m..s
   75 	     6 	 0.09595 	 0.00020 	 m..s
   46 	     7 	 0.07128 	 0.00020 	 m..s
   24 	     8 	 0.05763 	 0.00021 	 m..s
   10 	     9 	 0.05193 	 0.00021 	 m..s
   14 	    10 	 0.05306 	 0.00022 	 m..s
   37 	    11 	 0.06009 	 0.00022 	 m..s
   16 	    12 	 0.05316 	 0.00023 	 m..s
   49 	    13 	 0.07338 	 0.00024 	 m..s
   82 	    14 	 0.10292 	 0.00024 	 MISS
   28 	    15 	 0.05812 	 0.00024 	 m..s
   12 	    16 	 0.05242 	 0.00024 	 m..s
   40 	    17 	 0.06134 	 0.00024 	 m..s
   36 	    18 	 0.06008 	 0.00025 	 m..s
   20 	    19 	 0.05589 	 0.00026 	 m..s
   41 	    20 	 0.06412 	 0.00028 	 m..s
   71 	    21 	 0.09387 	 0.00031 	 m..s
   32 	    22 	 0.05916 	 0.00032 	 m..s
   23 	    23 	 0.05741 	 0.00032 	 m..s
   96 	    24 	 0.11758 	 0.00036 	 MISS
   95 	    25 	 0.11493 	 0.00036 	 MISS
   19 	    26 	 0.05588 	 0.00038 	 m..s
   92 	    27 	 0.11361 	 0.00040 	 MISS
   22 	    28 	 0.05611 	 0.00041 	 m..s
   76 	    29 	 0.09630 	 0.00052 	 m..s
   77 	    30 	 0.09636 	 0.00053 	 m..s
    4 	    31 	 0.04687 	 0.00061 	 m..s
   48 	    32 	 0.07311 	 0.00073 	 m..s
  101 	    33 	 0.13397 	 0.00083 	 MISS
   49 	    34 	 0.07338 	 0.00086 	 m..s
    5 	    35 	 0.04813 	 0.00089 	 m..s
   45 	    36 	 0.07060 	 0.00100 	 m..s
   33 	    37 	 0.05924 	 0.00111 	 m..s
   21 	    38 	 0.05593 	 0.00195 	 m..s
   49 	    39 	 0.07338 	 0.00246 	 m..s
    8 	    40 	 0.05170 	 0.00394 	 m..s
   11 	    41 	 0.05195 	 0.00451 	 m..s
   34 	    42 	 0.05930 	 0.00530 	 m..s
   26 	    43 	 0.05772 	 0.00584 	 m..s
   42 	    44 	 0.06667 	 0.00597 	 m..s
   31 	    45 	 0.05913 	 0.00743 	 m..s
   44 	    46 	 0.06807 	 0.00834 	 m..s
   27 	    47 	 0.05793 	 0.00932 	 m..s
   17 	    48 	 0.05317 	 0.01115 	 m..s
    6 	    49 	 0.05119 	 0.02180 	 ~...
    7 	    50 	 0.05166 	 0.02774 	 ~...
   13 	    51 	 0.05283 	 0.02804 	 ~...
   49 	    52 	 0.07338 	 0.02935 	 m..s
   39 	    53 	 0.06057 	 0.03155 	 ~...
    0 	    54 	 0.03816 	 0.03364 	 ~...
   25 	    55 	 0.05764 	 0.03621 	 ~...
    2 	    56 	 0.04396 	 0.03801 	 ~...
   30 	    57 	 0.05900 	 0.03948 	 ~...
   29 	    58 	 0.05855 	 0.03972 	 ~...
   38 	    59 	 0.06040 	 0.04655 	 ~...
    3 	    60 	 0.04663 	 0.04716 	 ~...
   18 	    61 	 0.05496 	 0.05213 	 ~...
   47 	    62 	 0.07213 	 0.05294 	 ~...
   15 	    63 	 0.05313 	 0.05536 	 ~...
    1 	    64 	 0.04254 	 0.05661 	 ~...
   58 	    65 	 0.07689 	 0.06196 	 ~...
   55 	    66 	 0.07341 	 0.06684 	 ~...
   56 	    67 	 0.07525 	 0.06746 	 ~...
   35 	    68 	 0.05958 	 0.06756 	 ~...
   61 	    69 	 0.07775 	 0.07030 	 ~...
   63 	    70 	 0.08461 	 0.07377 	 ~...
   59 	    71 	 0.07699 	 0.08153 	 ~...
   65 	    72 	 0.08534 	 0.08693 	 ~...
   88 	    73 	 0.10516 	 0.08856 	 ~...
   86 	    74 	 0.10431 	 0.09016 	 ~...
   60 	    75 	 0.07757 	 0.09158 	 ~...
   66 	    76 	 0.08537 	 0.09182 	 ~...
   67 	    77 	 0.08555 	 0.09504 	 ~...
  103 	    78 	 0.14043 	 0.09781 	 m..s
   69 	    79 	 0.09009 	 0.09877 	 ~...
  102 	    80 	 0.13935 	 0.10094 	 m..s
   43 	    81 	 0.06791 	 0.10197 	 m..s
   87 	    82 	 0.10459 	 0.11095 	 ~...
  100 	    83 	 0.12832 	 0.11742 	 ~...
   62 	    84 	 0.08348 	 0.12107 	 m..s
   68 	    85 	 0.08654 	 0.12731 	 m..s
   64 	    86 	 0.08493 	 0.12816 	 m..s
  108 	    87 	 0.15035 	 0.12964 	 ~...
   89 	    88 	 0.10630 	 0.13543 	 ~...
   79 	    89 	 0.09695 	 0.13907 	 m..s
   97 	    90 	 0.11854 	 0.14879 	 m..s
   94 	    91 	 0.11483 	 0.15352 	 m..s
   83 	    92 	 0.10347 	 0.15654 	 m..s
   73 	    93 	 0.09556 	 0.15681 	 m..s
   93 	    94 	 0.11381 	 0.15702 	 m..s
   70 	    95 	 0.09351 	 0.15812 	 m..s
   74 	    96 	 0.09560 	 0.15872 	 m..s
   98 	    97 	 0.11927 	 0.16282 	 m..s
   80 	    98 	 0.10114 	 0.16341 	 m..s
   78 	    99 	 0.09676 	 0.16480 	 m..s
   81 	   100 	 0.10220 	 0.17039 	 m..s
   99 	   101 	 0.12669 	 0.17157 	 m..s
  110 	   102 	 0.20453 	 0.17161 	 m..s
   91 	   103 	 0.11148 	 0.17428 	 m..s
  107 	   104 	 0.14643 	 0.17815 	 m..s
   90 	   105 	 0.10675 	 0.17977 	 m..s
   72 	   106 	 0.09509 	 0.18136 	 m..s
   84 	   107 	 0.10349 	 0.19066 	 m..s
  105 	   108 	 0.14194 	 0.19287 	 m..s
  106 	   109 	 0.14552 	 0.19866 	 m..s
  112 	   110 	 0.21747 	 0.20138 	 ~...
  109 	   111 	 0.16527 	 0.20346 	 m..s
  116 	   112 	 0.23389 	 0.22896 	 ~...
  113 	   113 	 0.21801 	 0.22899 	 ~...
  111 	   114 	 0.21523 	 0.24817 	 m..s
  115 	   115 	 0.23249 	 0.26738 	 m..s
  118 	   116 	 0.24132 	 0.28559 	 m..s
  117 	   117 	 0.23490 	 0.28630 	 m..s
  120 	   118 	 0.29237 	 0.29184 	 ~...
  114 	   119 	 0.22890 	 0.31001 	 m..s
  119 	   120 	 0.24851 	 0.31534 	 m..s
==========================================
r_mrr = 0.8151904940605164
r2_mrr = 0.5759921073913574
spearmanr_mrr@5 = 0.8488813638687134
spearmanr_mrr@10 = 0.8111814856529236
spearmanr_mrr@50 = 0.949435830116272
spearmanr_mrr@100 = 0.9460364580154419
spearmanr_mrr@All = 0.9542993307113647
==========================================
test time: 0.432
Done Testing dataset DBpedia50
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.266 +- 0.158
mrr vals (pred, true): 0.051, 0.021

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.05031 	 0.00286 	 m..s
   24 	     1 	 0.05287 	 0.00327 	 m..s
    8 	     2 	 0.05003 	 0.00329 	 m..s
   26 	     3 	 0.05298 	 0.00330 	 m..s
   29 	     4 	 0.05428 	 0.00335 	 m..s
   73 	     5 	 0.10593 	 0.00338 	 MISS
   13 	     6 	 0.05151 	 0.00338 	 m..s
   22 	     7 	 0.05236 	 0.00340 	 m..s
   67 	     8 	 0.10465 	 0.00347 	 MISS
   65 	     9 	 0.10296 	 0.00357 	 m..s
   16 	    10 	 0.05161 	 0.00357 	 m..s
   30 	    11 	 0.05587 	 0.00362 	 m..s
   14 	    12 	 0.05152 	 0.00364 	 m..s
    6 	    13 	 0.04981 	 0.00370 	 m..s
   19 	    14 	 0.05224 	 0.00377 	 m..s
   54 	    15 	 0.09954 	 0.00385 	 m..s
   67 	    16 	 0.10465 	 0.00393 	 MISS
   41 	    17 	 0.08858 	 0.00397 	 m..s
   64 	    18 	 0.10291 	 0.00399 	 m..s
   45 	    19 	 0.09775 	 0.00400 	 m..s
   47 	    20 	 0.09869 	 0.00408 	 m..s
   81 	    21 	 0.10911 	 0.00410 	 MISS
   67 	    22 	 0.10465 	 0.00413 	 MISS
   51 	    23 	 0.09918 	 0.00415 	 m..s
   46 	    24 	 0.09857 	 0.00415 	 m..s
   42 	    25 	 0.08944 	 0.00416 	 m..s
   57 	    26 	 0.10087 	 0.00416 	 m..s
   18 	    27 	 0.05215 	 0.00424 	 m..s
   67 	    28 	 0.10465 	 0.00425 	 MISS
   76 	    29 	 0.10631 	 0.00426 	 MISS
   62 	    30 	 0.10199 	 0.00427 	 m..s
   59 	    31 	 0.10138 	 0.00442 	 m..s
   90 	    32 	 0.11982 	 0.00442 	 MISS
   48 	    33 	 0.09874 	 0.00447 	 m..s
   44 	    34 	 0.09699 	 0.00448 	 m..s
   67 	    35 	 0.10465 	 0.00453 	 MISS
   60 	    36 	 0.10163 	 0.00497 	 m..s
   39 	    37 	 0.08765 	 0.00502 	 m..s
   83 	    38 	 0.10967 	 0.00513 	 MISS
   17 	    39 	 0.05172 	 0.00668 	 m..s
   15 	    40 	 0.05153 	 0.00672 	 m..s
   31 	    41 	 0.05591 	 0.00772 	 m..s
    3 	    42 	 0.04903 	 0.00942 	 m..s
   32 	    43 	 0.05724 	 0.00950 	 m..s
    4 	    44 	 0.04971 	 0.01032 	 m..s
   11 	    45 	 0.05131 	 0.01043 	 m..s
   21 	    46 	 0.05228 	 0.01044 	 m..s
    7 	    47 	 0.04982 	 0.01118 	 m..s
   10 	    48 	 0.05036 	 0.01188 	 m..s
    5 	    49 	 0.04981 	 0.01493 	 m..s
    1 	    50 	 0.04678 	 0.01652 	 m..s
    0 	    51 	 0.04388 	 0.01790 	 ~...
   23 	    52 	 0.05237 	 0.01879 	 m..s
   20 	    53 	 0.05224 	 0.01944 	 m..s
   25 	    54 	 0.05288 	 0.01991 	 m..s
   27 	    55 	 0.05305 	 0.02011 	 m..s
   33 	    56 	 0.05827 	 0.02042 	 m..s
    2 	    57 	 0.04842 	 0.02094 	 ~...
   12 	    58 	 0.05136 	 0.02142 	 ~...
   28 	    59 	 0.05378 	 0.02269 	 m..s
   37 	    60 	 0.06667 	 0.02552 	 m..s
   35 	    61 	 0.06642 	 0.03538 	 m..s
   87 	    62 	 0.11155 	 0.03870 	 m..s
   84 	    63 	 0.11082 	 0.03956 	 m..s
   36 	    64 	 0.06652 	 0.04125 	 ~...
   96 	    65 	 0.14854 	 0.05106 	 m..s
   95 	    66 	 0.14720 	 0.05558 	 m..s
   89 	    67 	 0.11968 	 0.07709 	 m..s
   34 	    68 	 0.06356 	 0.09528 	 m..s
   38 	    69 	 0.07010 	 0.10533 	 m..s
   40 	    70 	 0.08852 	 0.10873 	 ~...
   53 	    71 	 0.09944 	 0.12846 	 ~...
   50 	    72 	 0.09899 	 0.13096 	 m..s
   88 	    73 	 0.11501 	 0.13148 	 ~...
   67 	    74 	 0.10465 	 0.14286 	 m..s
   98 	    75 	 0.17746 	 0.15051 	 ~...
   91 	    76 	 0.11990 	 0.15121 	 m..s
  101 	    77 	 0.18504 	 0.15311 	 m..s
   93 	    78 	 0.13943 	 0.15408 	 ~...
   94 	    79 	 0.14119 	 0.15561 	 ~...
   56 	    80 	 0.10071 	 0.15688 	 m..s
   97 	    81 	 0.15214 	 0.15715 	 ~...
   49 	    82 	 0.09890 	 0.15745 	 m..s
   55 	    83 	 0.09999 	 0.15860 	 m..s
   63 	    84 	 0.10204 	 0.16306 	 m..s
   66 	    85 	 0.10379 	 0.16593 	 m..s
   58 	    86 	 0.10121 	 0.16632 	 m..s
   75 	    87 	 0.10631 	 0.16812 	 m..s
  100 	    88 	 0.18438 	 0.16869 	 ~...
  103 	    89 	 0.19860 	 0.16954 	 ~...
   99 	    90 	 0.18268 	 0.17536 	 ~...
   78 	    91 	 0.10684 	 0.17898 	 m..s
   61 	    92 	 0.10170 	 0.17903 	 m..s
   43 	    93 	 0.09172 	 0.18103 	 m..s
  106 	    94 	 0.21147 	 0.18173 	 ~...
   77 	    95 	 0.10647 	 0.18219 	 m..s
   74 	    96 	 0.10619 	 0.18503 	 m..s
  102 	    97 	 0.19846 	 0.18784 	 ~...
   86 	    98 	 0.11151 	 0.18910 	 m..s
   82 	    99 	 0.10951 	 0.19443 	 m..s
   85 	   100 	 0.11094 	 0.19925 	 m..s
   79 	   101 	 0.10871 	 0.20043 	 m..s
   52 	   102 	 0.09923 	 0.20108 	 MISS
   80 	   103 	 0.10898 	 0.20159 	 m..s
  115 	   104 	 0.24989 	 0.20636 	 m..s
  108 	   105 	 0.22453 	 0.20850 	 ~...
   92 	   106 	 0.12035 	 0.21109 	 m..s
  104 	   107 	 0.20146 	 0.21972 	 ~...
  105 	   108 	 0.20175 	 0.22583 	 ~...
  110 	   109 	 0.22960 	 0.22604 	 ~...
  117 	   110 	 0.25095 	 0.23011 	 ~...
  109 	   111 	 0.22831 	 0.23096 	 ~...
  119 	   112 	 0.25559 	 0.23645 	 ~...
  113 	   113 	 0.24379 	 0.23952 	 ~...
  112 	   114 	 0.24302 	 0.24075 	 ~...
  111 	   115 	 0.24259 	 0.24540 	 ~...
  107 	   116 	 0.22169 	 0.24547 	 ~...
  114 	   117 	 0.24734 	 0.25288 	 ~...
  120 	   118 	 0.26325 	 0.25940 	 ~...
  116 	   119 	 0.25040 	 0.26101 	 ~...
  118 	   120 	 0.25451 	 0.26816 	 ~...
==========================================
r_mrr = 0.783970057964325
r2_mrr = 0.5340779423713684
spearmanr_mrr@5 = 0.9204044342041016
spearmanr_mrr@10 = 0.9657291173934937
spearmanr_mrr@50 = 0.9679441452026367
spearmanr_mrr@100 = 0.8652520775794983
spearmanr_mrr@All = 0.89103764295578
==========================================
test time: 0.447
Done Testing dataset CoDExSmall
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.292 +- 0.290
mrr vals (pred, true): 0.039, 0.076

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   11 	     0 	 0.03839 	 0.00600 	 m..s
   47 	     1 	 0.05269 	 0.00603 	 m..s
   34 	     2 	 0.04343 	 0.00604 	 m..s
   69 	     3 	 0.06835 	 0.00607 	 m..s
   13 	     4 	 0.03883 	 0.00611 	 m..s
   32 	     5 	 0.04301 	 0.00615 	 m..s
    8 	     6 	 0.03806 	 0.00616 	 m..s
    4 	     7 	 0.03550 	 0.00618 	 ~...
   39 	     8 	 0.04641 	 0.00619 	 m..s
    3 	     9 	 0.03466 	 0.00620 	 ~...
   18 	    10 	 0.04077 	 0.00626 	 m..s
   20 	    11 	 0.04092 	 0.00630 	 m..s
   17 	    12 	 0.04076 	 0.00631 	 m..s
   54 	    13 	 0.05273 	 0.00632 	 m..s
   96 	    14 	 0.09523 	 0.00634 	 m..s
   35 	    15 	 0.04365 	 0.00636 	 m..s
   47 	    16 	 0.05269 	 0.00636 	 m..s
   41 	    17 	 0.04848 	 0.00637 	 m..s
   65 	    18 	 0.06655 	 0.00638 	 m..s
   43 	    19 	 0.05079 	 0.00638 	 m..s
   38 	    20 	 0.04449 	 0.00640 	 m..s
   27 	    21 	 0.04232 	 0.00640 	 m..s
   47 	    22 	 0.05269 	 0.00642 	 m..s
   47 	    23 	 0.05269 	 0.00646 	 m..s
   33 	    24 	 0.04307 	 0.00646 	 m..s
   22 	    25 	 0.04181 	 0.00648 	 m..s
   24 	    26 	 0.04197 	 0.00651 	 m..s
   45 	    27 	 0.05250 	 0.00651 	 m..s
    9 	    28 	 0.03807 	 0.00652 	 m..s
   86 	    29 	 0.08045 	 0.00656 	 m..s
   70 	    30 	 0.06839 	 0.00656 	 m..s
   74 	    31 	 0.07137 	 0.00658 	 m..s
   16 	    32 	 0.03895 	 0.00659 	 m..s
   97 	    33 	 0.10088 	 0.00660 	 m..s
   89 	    34 	 0.08138 	 0.00667 	 m..s
   90 	    35 	 0.08231 	 0.00668 	 m..s
   47 	    36 	 0.05269 	 0.00668 	 m..s
   68 	    37 	 0.06789 	 0.00668 	 m..s
   76 	    38 	 0.07233 	 0.00670 	 m..s
   19 	    39 	 0.04080 	 0.00995 	 m..s
   29 	    40 	 0.04277 	 0.01084 	 m..s
   10 	    41 	 0.03807 	 0.01134 	 ~...
   40 	    42 	 0.04812 	 0.01147 	 m..s
   26 	    43 	 0.04202 	 0.01184 	 m..s
   42 	    44 	 0.04907 	 0.01345 	 m..s
   15 	    45 	 0.03891 	 0.01413 	 ~...
    7 	    46 	 0.03790 	 0.01453 	 ~...
   31 	    47 	 0.04299 	 0.01501 	 ~...
   23 	    48 	 0.04189 	 0.01585 	 ~...
    5 	    49 	 0.03761 	 0.01698 	 ~...
   12 	    50 	 0.03872 	 0.01709 	 ~...
   47 	    51 	 0.05269 	 0.02136 	 m..s
   37 	    52 	 0.04397 	 0.03212 	 ~...
   25 	    53 	 0.04197 	 0.03397 	 ~...
   30 	    54 	 0.04280 	 0.03491 	 ~...
   36 	    55 	 0.04379 	 0.03734 	 ~...
   56 	    56 	 0.05509 	 0.04205 	 ~...
   53 	    57 	 0.05270 	 0.04467 	 ~...
    0 	    58 	 0.02857 	 0.05011 	 ~...
   72 	    59 	 0.06880 	 0.05188 	 ~...
    6 	    60 	 0.03790 	 0.05202 	 ~...
   58 	    61 	 0.06043 	 0.05705 	 ~...
   55 	    62 	 0.05469 	 0.05917 	 ~...
   46 	    63 	 0.05268 	 0.05956 	 ~...
    1 	    64 	 0.03261 	 0.05959 	 ~...
   82 	    65 	 0.07602 	 0.06170 	 ~...
    2 	    66 	 0.03445 	 0.06519 	 m..s
   21 	    67 	 0.04099 	 0.06589 	 ~...
   83 	    68 	 0.07614 	 0.06889 	 ~...
   80 	    69 	 0.07572 	 0.06966 	 ~...
   62 	    70 	 0.06169 	 0.07065 	 ~...
   67 	    71 	 0.06784 	 0.07339 	 ~...
   57 	    72 	 0.05958 	 0.07573 	 ~...
   28 	    73 	 0.04261 	 0.07586 	 m..s
   14 	    74 	 0.03891 	 0.07593 	 m..s
   71 	    75 	 0.06867 	 0.07887 	 ~...
   95 	    76 	 0.08907 	 0.07911 	 ~...
   44 	    77 	 0.05184 	 0.07958 	 ~...
   61 	    78 	 0.06110 	 0.08502 	 ~...
   88 	    79 	 0.08131 	 0.08615 	 ~...
   60 	    80 	 0.06090 	 0.08746 	 ~...
   78 	    81 	 0.07334 	 0.08855 	 ~...
   59 	    82 	 0.06087 	 0.09171 	 m..s
   75 	    83 	 0.07220 	 0.09181 	 ~...
   66 	    84 	 0.06767 	 0.09363 	 ~...
   81 	    85 	 0.07595 	 0.09385 	 ~...
   64 	    86 	 0.06632 	 0.09396 	 ~...
   98 	    87 	 0.10356 	 0.09441 	 ~...
   87 	    88 	 0.08081 	 0.09443 	 ~...
   91 	    89 	 0.08564 	 0.09470 	 ~...
   73 	    90 	 0.07103 	 0.09611 	 ~...
  100 	    91 	 0.10462 	 0.09631 	 ~...
   84 	    92 	 0.07852 	 0.09662 	 ~...
   85 	    93 	 0.08009 	 0.09801 	 ~...
   77 	    94 	 0.07245 	 0.10170 	 ~...
   92 	    95 	 0.08596 	 0.10189 	 ~...
   79 	    96 	 0.07563 	 0.10220 	 ~...
  102 	    97 	 0.10841 	 0.10352 	 ~...
   63 	    98 	 0.06414 	 0.10736 	 m..s
   93 	    99 	 0.08620 	 0.11133 	 ~...
   94 	   100 	 0.08684 	 0.11815 	 m..s
   99 	   101 	 0.10391 	 0.12399 	 ~...
  105 	   102 	 0.14156 	 0.12480 	 ~...
  101 	   103 	 0.10671 	 0.12726 	 ~...
  104 	   104 	 0.12852 	 0.14016 	 ~...
  106 	   105 	 0.14778 	 0.14759 	 ~...
  103 	   106 	 0.11538 	 0.18004 	 m..s
  108 	   107 	 0.17964 	 0.18723 	 ~...
  107 	   108 	 0.17727 	 0.18730 	 ~...
  109 	   109 	 0.18482 	 0.20183 	 ~...
  110 	   110 	 0.20508 	 0.21743 	 ~...
  113 	   111 	 0.25102 	 0.24106 	 ~...
  116 	   112 	 0.25360 	 0.24189 	 ~...
  114 	   113 	 0.25287 	 0.26468 	 ~...
  117 	   114 	 0.26019 	 0.26756 	 ~...
  112 	   115 	 0.24919 	 0.27253 	 ~...
  111 	   116 	 0.23774 	 0.27292 	 m..s
  118 	   117 	 0.26700 	 0.27828 	 ~...
  120 	   118 	 0.30586 	 0.29009 	 ~...
  119 	   119 	 0.26959 	 0.29220 	 ~...
  115 	   120 	 0.25312 	 0.29259 	 m..s
==========================================
r_mrr = 0.9189918041229248
r2_mrr = 0.8093615770339966
spearmanr_mrr@5 = 0.7036972045898438
spearmanr_mrr@10 = 0.7627708315849304
spearmanr_mrr@50 = 0.9952002167701721
spearmanr_mrr@100 = 0.9739917516708374
spearmanr_mrr@All = 0.9755210876464844
==========================================
test time: 0.492
Done Testing dataset OpenEA
total time taken: 831.7091159820557
training time taken: 792.0547585487366
TWIG out ;))
======================================================
------------------------------------------------------
Running a TWIG experiment with tag: TransE-omit-OpenEA
------------------------------------------------------
======================================================
Using random seed: 6050225476865368
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Loading DBpedia50...
- loading run 2.1...
Loading CoDExSmall...
- loading run 2.1...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1141, 455, 708, 129, 247, 401, 375, 830, 677, 530, 544, 110, 853, 645, 651, 280, 633, 1201, 1162, 1017, 134, 936, 378, 878, 376, 210, 130, 99, 486, 178, 1123, 116, 897, 722, 974, 215, 137, 747, 220, 232, 296, 21, 719, 759, 139, 199, 69, 1071, 463, 292, 865, 206, 1061, 547, 805, 534, 629, 268, 289, 95, 49, 795, 228, 889, 422, 47, 214, 743, 557, 60, 1184, 285, 794, 186, 516, 358, 798, 942, 354, 511, 165, 245, 733, 660, 1011, 641, 1192, 826, 655, 1021, 927, 1103, 1181, 1139, 1031, 777, 787, 201, 72, 492, 109, 786, 190, 734, 465, 237, 861, 1196, 106, 367, 1001, 127, 478, 337, 993, 576, 509, 480, 154, 804, 498]
valid_ids (0): []
train_ids (1094): [556, 778, 1006, 813, 4, 906, 474, 1056, 683, 62, 538, 954, 637, 943, 242, 497, 17, 36, 6, 101, 16, 437, 125, 2, 668, 56, 689, 208, 590, 896, 886, 782, 355, 30, 425, 1185, 246, 1183, 643, 230, 490, 716, 710, 775, 620, 908, 1114, 424, 123, 440, 48, 572, 114, 31, 764, 239, 1146, 1182, 1073, 299, 1213, 435, 256, 403, 450, 602, 561, 144, 952, 843, 198, 334, 742, 1101, 181, 713, 118, 304, 442, 103, 627, 891, 581, 510, 814, 679, 987, 1019, 501, 744, 397, 1048, 469, 44, 753, 75, 419, 508, 842, 466, 723, 763, 45, 1157, 848, 1188, 675, 1054, 235, 644, 132, 894, 38, 229, 281, 135, 322, 691, 761, 169, 37, 985, 1144, 170, 70, 859, 441, 481, 368, 260, 185, 19, 102, 32, 1098, 888, 992, 1027, 827, 681, 63, 396, 377, 883, 512, 731, 666, 809, 1046, 431, 1133, 1127, 473, 54, 50, 1176, 741, 532, 570, 1004, 815, 672, 599, 155, 638, 253, 471, 552, 687, 907, 507, 751, 1084, 1088, 35, 669, 941, 810, 262, 880, 770, 1195, 737, 487, 203, 933, 279, 104, 290, 1189, 851, 535, 944, 678, 1126, 688, 902, 223, 1013, 571, 146, 332, 388, 26, 740, 1155, 394, 1095, 867, 136, 1093, 1109, 650, 1117, 145, 721, 1165, 977, 124, 824, 536, 115, 252, 94, 653, 959, 1051, 866, 46, 665, 654, 939, 947, 553, 1039, 1064, 189, 789, 1052, 305, 274, 937, 9, 85, 774, 207, 856, 1035, 752, 667, 3, 533, 749, 266, 542, 12, 173, 714, 1166, 837, 526, 1090, 287, 382, 618, 257, 567, 196, 596, 515, 686, 546, 621, 844, 434, 836, 1012, 219, 18, 1096, 958, 949, 193, 899, 525, 100, 489, 736, 771, 914, 347, 1081, 802, 520, 595, 1079, 574, 191, 331, 808, 226, 108, 1199, 416, 1149, 768, 1212, 648, 998, 1118, 1207, 605, 849, 52, 834, 1138, 1150, 960, 756, 81, 632, 635, 1042, 1020, 863, 934, 1180, 167, 162, 823, 634, 1173, 1171, 429, 783, 1153, 562, 216, 607, 673, 839, 704, 82, 238, 652, 314, 807, 244, 746, 1104, 935, 259, 1159, 472, 278, 701, 447, 738, 1187, 948, 1115, 84, 518, 1075, 5, 133, 329, 454, 324, 979, 61, 195, 1049, 1038, 623, 580, 566, 890, 79, 366, 372, 923, 446, 7, 1080, 357, 680, 1175, 1137, 89, 456, 619, 792, 302, 903, 901, 565, 529, 1106, 1076, 674, 1110, 978, 758, 873, 1007, 1211, 1028, 1154, 569, 42, 27, 408, 364, 981, 639, 53, 968, 433, 122, 414, 194, 930, 820, 539, 344, 436, 250, 80, 656, 559, 905, 812, 213, 270, 348, 640, 592, 386, 975, 1214, 141, 584, 117, 895, 755, 243, 702, 33, 221, 41, 1047, 788, 187, 420, 8, 549, 904, 200, 657, 225, 126, 177, 745, 410, 1089, 918, 161, 288, 1168, 286, 1204, 183, 765, 852, 359, 828, 1003, 336, 236, 972, 88, 577, 869, 107, 969, 295, 91, 550, 838, 913, 642, 78, 573, 448, 670, 720, 513, 874, 179, 1125, 609, 1205, 1105, 545, 521, 554, 86, 363, 940, 991, 1058, 1043, 277, 612, 696, 338, 1169, 64, 685, 857, 157, 784, 712, 610, 77, 400, 1102, 662, 494, 791, 1041, 767, 626, 898, 10, 928, 1010, 962, 254, 608, 34, 514, 209, 1009, 360, 551, 822, 503, 188, 464, 24, 15, 96, 984, 1111, 829, 197, 879, 415, 1160, 797, 631, 147, 882, 319, 806, 505, 283, 13, 1045, 40, 168, 762, 1099, 211, 825, 929, 477, 630, 458, 23, 174, 506, 854, 175, 579, 945, 143, 832, 97, 1077, 728, 373, 138, 417, 1100, 604, 333, 711, 729, 862, 39, 833, 950, 371, 800, 524, 989, 172, 1063, 999, 773, 564, 343, 965, 412, 263, 349, 1200, 248, 326, 676, 976, 578, 1015, 453, 1158, 1142, 233, 847, 58, 76, 461, 785, 457, 694, 152, 1130, 603, 1078, 476, 841, 963, 381, 65, 428, 1194, 83, 318, 488, 1, 628, 1108, 531, 801, 726, 407, 402, 793, 1124, 159, 885, 313, 1116, 28, 956, 1040, 816, 779, 1025, 275, 298, 59, 406, 113, 884, 120, 468, 1074, 1128, 1044, 427, 1120, 555, 739, 1179, 352, 43, 353, 964, 389, 769, 380, 1057, 659, 523, 467, 1008, 327, 365, 1163, 150, 218, 997, 709, 790, 1143, 261, 1083, 391, 706, 994, 180, 460, 705, 543, 647, 568, 591, 443, 320, 316, 1167, 51, 846, 14, 444, 404, 919, 986, 451, 625, 988, 926, 698, 1033, 594, 1068, 142, 695, 423, 1197, 917, 750, 87, 339, 312, 500, 982, 916, 718, 1086, 379, 224, 522, 475, 1191, 293, 582, 1016, 315, 699, 1113, 996, 1135, 66, 700, 227, 598, 350, 957, 241, 663, 1206, 1067, 1186, 272, 1152, 361, 593, 413, 321, 393, 405, 583, 182, 585, 0, 470, 20, 558, 715, 671, 74, 658, 1131, 249, 1029, 255, 754, 548, 955, 872, 622, 1036, 649, 1059, 57, 1140, 586, 541, 163, 845, 860, 140, 1147, 589, 112, 171, 661, 309, 611, 121, 303, 93, 340, 55, 409, 271, 202, 921, 717, 495, 291, 614, 819, 835, 871, 264, 240, 1022, 267, 1145, 1210, 399, 616, 748, 875, 192, 426, 418, 540, 1190, 1122, 411, 697, 98, 369, 690, 1208, 601, 850, 1134, 967, 301, 684, 504, 1132, 149, 335, 273, 817, 990, 724, 370, 864, 1018, 887, 25, 613, 803, 693, 796, 995, 1119, 938, 1070, 307, 725, 398, 925, 479, 855, 1178, 68, 445, 119, 1055, 158, 1136, 951, 105, 342, 1172, 1053, 587, 1156, 284, 971, 1121, 1032, 330, 308, 870, 912, 772, 931, 1193, 1072, 67, 1203, 932, 1129, 176, 1014, 911, 877, 384, 22, 973, 1198, 1050, 1094, 881, 430, 392, 449, 1060, 390, 615, 483, 636, 148, 893, 664, 537, 1092, 362, 90, 493, 502, 499, 1177, 946, 600, 1174, 131, 383, 909, 128, 222, 776, 1097, 1023, 528, 204, 527, 757, 212, 1062, 1082, 1091, 563, 1000, 300, 1066, 160, 760, 1024, 323, 345, 452, 953, 73, 858, 983, 1209, 328, 575, 1085, 910, 29, 71, 876, 317, 915, 1148, 924, 387, 153, 840, 703, 560, 799, 617, 1161, 462, 597, 496, 432, 966, 780, 1034, 395, 459, 821, 92, 1065, 730, 1005, 692, 351, 484, 1037, 646, 385, 282, 217, 900, 151, 491, 831, 269, 311, 519, 439, 276, 707, 766, 11, 818, 1069, 980, 922, 164, 517, 234, 961, 111, 606, 438, 868, 682, 374, 624, 781, 1170, 1112, 306, 1026, 482, 1002, 1202, 485, 920, 325, 1151, 421, 1030, 588, 346, 892, 1164, 735, 356, 265, 1107, 297, 294, 231, 205, 310, 156, 251, 732, 184, 341, 811, 727, 258, 166, 970, 1087]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9130376695746116
the save name prefix for this run is:  chkpt-ID_9130376695746116_tag_TransE-omit-OpenEA
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1'], 'DBpedia50': ['2.1'], 'CoDExSmall': ['2.1'], 'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 91
rank avg (pred): 0.463 +- 0.004
mrr vals (pred, true): 0.016, 0.102
batch losses (mrrl, rdl): 0.0, 0.0010044752

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 458
rank avg (pred): 0.395 +- 0.044
mrr vals (pred, true): 0.019, 0.046
batch losses (mrrl, rdl): 0.0, 9.89134e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 676
rank avg (pred): 0.420 +- 0.055
mrr vals (pred, true): 0.018, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001072163

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 416
rank avg (pred): 0.373 +- 0.119
mrr vals (pred, true): 0.022, 0.039
batch losses (mrrl, rdl): 0.0, 0.0002047618

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 212
rank avg (pred): 0.335 +- 0.214
mrr vals (pred, true): 0.055, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001311804

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1189
rank avg (pred): 0.370 +- 0.237
mrr vals (pred, true): 0.062, 0.044
batch losses (mrrl, rdl): 0.0, 4.04599e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 430
rank avg (pred): 0.326 +- 0.256
mrr vals (pred, true): 0.131, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001889135

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 960
rank avg (pred): 0.355 +- 0.264
mrr vals (pred, true): 0.081, 0.039
batch losses (mrrl, rdl): 0.0, 7.64792e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 832
rank avg (pred): 0.077 +- 0.077
mrr vals (pred, true): 0.425, 0.528
batch losses (mrrl, rdl): 0.0, 4.5065e-05

Epoch over!
epoch time: 51.715

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 437
rank avg (pred): 0.353 +- 0.274
mrr vals (pred, true): 0.084, 0.040
batch losses (mrrl, rdl): 0.0, 7.92928e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 279
rank avg (pred): 0.084 +- 0.077
mrr vals (pred, true): 0.315, 0.531
batch losses (mrrl, rdl): 0.0, 7.43962e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 209
rank avg (pred): 0.343 +- 0.273
mrr vals (pred, true): 0.090, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001000166

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 352
rank avg (pred): 0.323 +- 0.260
mrr vals (pred, true): 0.094, 0.114
batch losses (mrrl, rdl): 0.0, 0.0002241574

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1055
rank avg (pred): 0.109 +- 0.098
mrr vals (pred, true): 0.221, 0.597
batch losses (mrrl, rdl): 0.0, 0.0001793061

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 709
rank avg (pred): 0.395 +- 0.277
mrr vals (pred, true): 0.047, 0.039
batch losses (mrrl, rdl): 0.0, 3.64686e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 548
rank avg (pred): 0.256 +- 0.189
mrr vals (pred, true): 0.076, 0.127
batch losses (mrrl, rdl): 0.0, 4.18705e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 924
rank avg (pred): 0.318 +- 0.276
mrr vals (pred, true): 0.118, 0.069
batch losses (mrrl, rdl): 0.0, 3.24058e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 987
rank avg (pred): 0.037 +- 0.036
mrr vals (pred, true): 0.425, 0.632
batch losses (mrrl, rdl): 0.0, 4.7641e-06

Epoch over!
epoch time: 51.158

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 50
rank avg (pred): 0.062 +- 0.058
mrr vals (pred, true): 0.336, 0.612
batch losses (mrrl, rdl): 0.0, 3.92279e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 658
rank avg (pred): 0.420 +- 0.257
mrr vals (pred, true): 0.033, 0.042
batch losses (mrrl, rdl): 0.0, 1.0334e-06

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 221
rank avg (pred): 0.340 +- 0.261
mrr vals (pred, true): 0.076, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001341584

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 346
rank avg (pred): 0.321 +- 0.254
mrr vals (pred, true): 0.092, 0.126
batch losses (mrrl, rdl): 0.0, 0.0002464603

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 524
rank avg (pred): 0.199 +- 0.171
mrr vals (pred, true): 0.163, 0.117
batch losses (mrrl, rdl): 0.0, 0.0001425718

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 885
rank avg (pred): 0.317 +- 0.249
mrr vals (pred, true): 0.092, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001608903

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 468
rank avg (pred): 0.336 +- 0.267
mrr vals (pred, true): 0.088, 0.044
batch losses (mrrl, rdl): 0.0, 0.0001139986

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 545
rank avg (pred): 0.292 +- 0.255
mrr vals (pred, true): 0.116, 0.135
batch losses (mrrl, rdl): 0.0, 1.9227e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1083
rank avg (pred): 0.337 +- 0.254
mrr vals (pred, true): 0.086, 0.134
batch losses (mrrl, rdl): 0.0, 0.0002386148

Epoch over!
epoch time: 51.44

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 322
rank avg (pred): 0.063 +- 0.068
mrr vals (pred, true): 0.499, 0.559
batch losses (mrrl, rdl): 0.0, 3.46155e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 496
rank avg (pred): 0.294 +- 0.246
mrr vals (pred, true): 0.099, 0.075
batch losses (mrrl, rdl): 0.0, 1.20247e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 870
rank avg (pred): 0.327 +- 0.269
mrr vals (pred, true): 0.124, 0.036
batch losses (mrrl, rdl): 0.0, 0.0002309037

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 481
rank avg (pred): 0.346 +- 0.277
mrr vals (pred, true): 0.122, 0.039
batch losses (mrrl, rdl): 0.0, 6.42022e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 93
rank avg (pred): 0.340 +- 0.254
mrr vals (pred, true): 0.095, 0.089
batch losses (mrrl, rdl): 0.0, 0.000178994

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 940
rank avg (pred): 0.329 +- 0.291
mrr vals (pred, true): 0.206, 0.090
batch losses (mrrl, rdl): 0.0, 7.59589e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 850
rank avg (pred): 0.325 +- 0.259
mrr vals (pred, true): 0.095, 0.097
batch losses (mrrl, rdl): 0.0, 0.0001508725

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 744
rank avg (pred): 0.040 +- 0.050
mrr vals (pred, true): 0.523, 0.596
batch losses (mrrl, rdl): 0.0, 5.1836e-06

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 562
rank avg (pred): 0.292 +- 0.280
mrr vals (pred, true): 0.270, 0.072
batch losses (mrrl, rdl): 0.0, 2.71733e-05

Epoch over!
epoch time: 51.515

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 581
rank avg (pred): 0.440 +- 0.247
mrr vals (pred, true): 0.028, 0.040
batch losses (mrrl, rdl): 0.0, 4.6914e-06

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 881
rank avg (pred): 0.318 +- 0.262
mrr vals (pred, true): 0.196, 0.042
batch losses (mrrl, rdl): 0.0, 0.000170627

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 4
rank avg (pred): 0.038 +- 0.049
mrr vals (pred, true): 0.544, 0.544
batch losses (mrrl, rdl): 0.0, 2.5781e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 986
rank avg (pred): 0.017 +- 0.021
mrr vals (pred, true): 0.594, 0.629
batch losses (mrrl, rdl): 0.0, 7.695e-07

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 65
rank avg (pred): 0.021 +- 0.034
mrr vals (pred, true): 0.588, 0.619
batch losses (mrrl, rdl): 0.0, 1.94e-08

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 617
rank avg (pred): 0.430 +- 0.250
mrr vals (pred, true): 0.030, 0.040
batch losses (mrrl, rdl): 0.0, 2.6484e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 48
rank avg (pred): 0.041 +- 0.054
mrr vals (pred, true): 0.528, 0.505
batch losses (mrrl, rdl): 0.0, 2.7447e-06

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1214
rank avg (pred): 0.419 +- 0.266
mrr vals (pred, true): 0.035, 0.038
batch losses (mrrl, rdl): 0.0, 4.0857e-06

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 346
rank avg (pred): 0.350 +- 0.269
mrr vals (pred, true): 0.161, 0.126
batch losses (mrrl, rdl): 0.0, 0.0003945691

Epoch over!
epoch time: 51.801

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1041
rank avg (pred): 0.313 +- 0.249
mrr vals (pred, true): 0.186, 0.037
batch losses (mrrl, rdl): 0.1837466061, 0.0002858706

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 767
rank avg (pred): 0.159 +- 0.062
mrr vals (pred, true): 0.066, 0.092
batch losses (mrrl, rdl): 0.0025615257, 0.0004091134

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 332
rank avg (pred): 0.248 +- 0.201
mrr vals (pred, true): 0.088, 0.133
batch losses (mrrl, rdl): 0.0202837102, 8.3183e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 169
rank avg (pred): 0.276 +- 0.203
mrr vals (pred, true): 0.064, 0.043
batch losses (mrrl, rdl): 0.0019943945, 0.0003989622

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 789
rank avg (pred): 0.312 +- 0.215
mrr vals (pred, true): 0.051, 0.039
batch losses (mrrl, rdl): 6.8901e-06, 0.0003424781

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 643
rank avg (pred): 0.339 +- 0.176
mrr vals (pred, true): 0.036, 0.034
batch losses (mrrl, rdl): 0.0018230889, 0.0002136419

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 270
rank avg (pred): 0.020 +- 0.026
mrr vals (pred, true): 0.537, 0.526
batch losses (mrrl, rdl): 0.0012645165, 1.9857e-06

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 59
rank avg (pred): 0.016 +- 0.027
mrr vals (pred, true): 0.603, 0.602
batch losses (mrrl, rdl): 1.03315e-05, 6.97e-07

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 198
rank avg (pred): 0.303 +- 0.246
mrr vals (pred, true): 0.079, 0.041
batch losses (mrrl, rdl): 0.0083046556, 0.0002571501

Epoch over!
epoch time: 51.723

Epoch 2 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 788
rank avg (pred): 0.317 +- 0.237
mrr vals (pred, true): 0.070, 0.039
batch losses (mrrl, rdl): 0.0038426574, 0.0002510076

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 386
rank avg (pred): 0.343 +- 0.242
mrr vals (pred, true): 0.056, 0.139
batch losses (mrrl, rdl): 0.0695947856, 0.0002264654

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 371
rank avg (pred): 0.312 +- 0.260
mrr vals (pred, true): 0.094, 0.146
batch losses (mrrl, rdl): 0.0273938831, 0.0001924155

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 709
rank avg (pred): 0.428 +- 0.166
mrr vals (pred, true): 0.028, 0.039
batch losses (mrrl, rdl): 0.0047475635, 4.03836e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 438
rank avg (pred): 0.388 +- 0.212
mrr vals (pred, true): 0.046, 0.037
batch losses (mrrl, rdl): 0.0001504103, 8.24561e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 659
rank avg (pred): 0.444 +- 0.148
mrr vals (pred, true): 0.023, 0.040
batch losses (mrrl, rdl): 0.0073024528, 3.74692e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 226
rank avg (pred): 0.358 +- 0.225
mrr vals (pred, true): 0.057, 0.040
batch losses (mrrl, rdl): 0.0005465192, 8.52639e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 222
rank avg (pred): 0.377 +- 0.212
mrr vals (pred, true): 0.051, 0.039
batch losses (mrrl, rdl): 1.01232e-05, 8.54251e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 39
rank avg (pred): 0.080 +- 0.147
mrr vals (pred, true): 0.539, 0.516
batch losses (mrrl, rdl): 0.0053502442, 8.91813e-05

Epoch over!
epoch time: 53.156

Epoch 3 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 730
rank avg (pred): 0.097 +- 0.169
mrr vals (pred, true): 0.494, 0.444
batch losses (mrrl, rdl): 0.024848666, 7.56891e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 246
rank avg (pred): 0.095 +- 0.163
mrr vals (pred, true): 0.501, 0.513
batch losses (mrrl, rdl): 0.0014409214, 0.0001327004

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 198
rank avg (pred): 0.344 +- 0.211
mrr vals (pred, true): 0.068, 0.041
batch losses (mrrl, rdl): 0.0031465581, 0.0001346919

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 166
rank avg (pred): 0.360 +- 0.197
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.000154244, 0.0001176243

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 341
rank avg (pred): 0.314 +- 0.213
mrr vals (pred, true): 0.073, 0.133
batch losses (mrrl, rdl): 0.0357353725, 0.0001672444

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 757
rank avg (pred): 0.326 +- 0.206
mrr vals (pred, true): 0.056, 0.088
batch losses (mrrl, rdl): 0.0003285604, 5.70106e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 955
rank avg (pred): 0.348 +- 0.165
mrr vals (pred, true): 0.044, 0.041
batch losses (mrrl, rdl): 0.0003215448, 0.0001432295

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 893
rank avg (pred): 0.128 +- 0.183
mrr vals (pred, true): 0.329, 0.202
batch losses (mrrl, rdl): 0.1610785723, 3.22517e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1050
rank avg (pred): 0.295 +- 0.210
mrr vals (pred, true): 0.097, 0.041
batch losses (mrrl, rdl): 0.0220602043, 0.0003392031

Epoch over!
epoch time: 51.765

Epoch 4 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 515
rank avg (pred): 0.168 +- 0.183
mrr vals (pred, true): 0.113, 0.121
batch losses (mrrl, rdl): 0.0005566424, 0.0002688115

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 578
rank avg (pred): 0.390 +- 0.120
mrr vals (pred, true): 0.028, 0.041
batch losses (mrrl, rdl): 0.0047695884, 7.26588e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 399
rank avg (pred): 0.304 +- 0.197
mrr vals (pred, true): 0.066, 0.112
batch losses (mrrl, rdl): 0.0205889791, 8.97306e-05

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 64
rank avg (pred): 0.065 +- 0.120
mrr vals (pred, true): 0.581, 0.558
batch losses (mrrl, rdl): 0.0055729682, 4.93589e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 241
rank avg (pred): 0.321 +- 0.176
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 8.3576e-06, 0.0002177062

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 256
rank avg (pred): 0.090 +- 0.151
mrr vals (pred, true): 0.523, 0.556
batch losses (mrrl, rdl): 0.0106393304, 0.0001243812

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 752
rank avg (pred): 0.112 +- 0.168
mrr vals (pred, true): 0.458, 0.511
batch losses (mrrl, rdl): 0.0278161839, 0.0001806529

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 9
rank avg (pred): 0.076 +- 0.129
mrr vals (pred, true): 0.567, 0.509
batch losses (mrrl, rdl): 0.0340040736, 6.96654e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 855
rank avg (pred): 0.329 +- 0.167
mrr vals (pred, true): 0.049, 0.087
batch losses (mrrl, rdl): 1.81737e-05, 7.3121e-05

Epoch over!
epoch time: 50.603

Epoch 5 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 513
rank avg (pred): 0.178 +- 0.156
mrr vals (pred, true): 0.076, 0.065
batch losses (mrrl, rdl): 0.0067705712, 0.0004850442

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 819
rank avg (pred): 0.094 +- 0.141
mrr vals (pred, true): 0.488, 0.440
batch losses (mrrl, rdl): 0.0236770101, 6.47875e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 535
rank avg (pred): 0.165 +- 0.162
mrr vals (pred, true): 0.091, 0.072
batch losses (mrrl, rdl): 0.016543068, 0.0005222706

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 981
rank avg (pred): 0.034 +- 0.059
mrr vals (pred, true): 0.626, 0.613
batch losses (mrrl, rdl): 0.0017634347, 5.2743e-06

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 373
rank avg (pred): 0.302 +- 0.165
mrr vals (pred, true): 0.055, 0.108
batch losses (mrrl, rdl): 0.0277801938, 8.02301e-05

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 877
rank avg (pred): 0.299 +- 0.163
mrr vals (pred, true): 0.058, 0.042
batch losses (mrrl, rdl): 0.0005762393, 0.0003364491

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 264
rank avg (pred): 0.048 +- 0.075
mrr vals (pred, true): 0.566, 0.507
batch losses (mrrl, rdl): 0.0355538577, 1.04324e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 304
rank avg (pred): 0.051 +- 0.076
mrr vals (pred, true): 0.535, 0.580
batch losses (mrrl, rdl): 0.0200325921, 1.88372e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 863
rank avg (pred): 0.293 +- 0.157
mrr vals (pred, true): 0.060, 0.094
batch losses (mrrl, rdl): 0.0009427872, 2.82891e-05

Epoch over!
epoch time: 51.36

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 334
rank avg (pred): 0.276 +- 0.163
mrr vals (pred, true): 0.067, 0.127
batch losses (mrrl, rdl): 0.0359656177, 0.0001095367

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 344
rank avg (pred): 0.245 +- 0.166
mrr vals (pred, true): 0.086, 0.132
batch losses (mrrl, rdl): 0.0211699158, 1.55831e-05

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1063
rank avg (pred): 0.032 +- 0.054
mrr vals (pred, true): 0.631, 0.605
batch losses (mrrl, rdl): 0.0064823614, 3.0958e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 9
rank avg (pred): 0.056 +- 0.081
mrr vals (pred, true): 0.508, 0.509
batch losses (mrrl, rdl): 9.6676e-06, 1.95316e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 446
rank avg (pred): 0.239 +- 0.160
mrr vals (pred, true): 0.086, 0.035
batch losses (mrrl, rdl): 0.0126912091, 0.0008748202

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 458
rank avg (pred): 0.229 +- 0.153
mrr vals (pred, true): 0.087, 0.046
batch losses (mrrl, rdl): 0.0134189511, 0.0007259196

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 707
rank avg (pred): 0.340 +- 0.077
mrr vals (pred, true): 0.032, 0.038
batch losses (mrrl, rdl): 0.0031361044, 0.0002568426

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 491
rank avg (pred): 0.145 +- 0.132
mrr vals (pred, true): 0.094, 0.131
batch losses (mrrl, rdl): 0.01342801, 0.000402043

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 112
rank avg (pred): 0.311 +- 0.128
mrr vals (pred, true): 0.054, 0.101
batch losses (mrrl, rdl): 0.0228062049, 8.63895e-05

Epoch over!
epoch time: 52.541

Epoch 7 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 589
rank avg (pred): 0.340 +- 0.071
mrr vals (pred, true): 0.033, 0.038
batch losses (mrrl, rdl): 0.0030415368, 0.0002521662

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 483
rank avg (pred): 0.339 +- 0.106
mrr vals (pred, true): 0.045, 0.039
batch losses (mrrl, rdl): 0.0002706455, 0.000309252

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 606
rank avg (pred): 0.334 +- 0.066
mrr vals (pred, true): 0.032, 0.030
batch losses (mrrl, rdl): 0.0033410813, 0.0003581092

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 668
rank avg (pred): 0.294 +- 0.084
mrr vals (pred, true): 0.037, 0.037
batch losses (mrrl, rdl): 0.0016975111, 0.0006203294

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 595
rank avg (pred): 0.299 +- 0.085
mrr vals (pred, true): 0.038, 0.034
batch losses (mrrl, rdl): 0.0014106738, 0.0005682876

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 835
rank avg (pred): 0.041 +- 0.050
mrr vals (pred, true): 0.464, 0.576
batch losses (mrrl, rdl): 0.1264849007, 5.2954e-06

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 412
rank avg (pred): 0.313 +- 0.103
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 2.23841e-05, 0.0004301365

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 322
rank avg (pred): 0.029 +- 0.037
mrr vals (pred, true): 0.547, 0.559
batch losses (mrrl, rdl): 0.0013186781, 4.016e-07

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 149
rank avg (pred): 0.252 +- 0.127
mrr vals (pred, true): 0.072, 0.120
batch losses (mrrl, rdl): 0.0225974023, 3.19103e-05

Epoch over!
epoch time: 52.038

Epoch 8 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1156
rank avg (pred): 0.120 +- 0.108
mrr vals (pred, true): 0.103, 0.143
batch losses (mrrl, rdl): 0.016191585, 0.000210841

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 205
rank avg (pred): 0.299 +- 0.103
mrr vals (pred, true): 0.055, 0.039
batch losses (mrrl, rdl): 0.0002525972, 0.0005074325

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1132
rank avg (pred): 0.256 +- 0.120
mrr vals (pred, true): 0.072, 0.039
batch losses (mrrl, rdl): 0.0047296202, 0.0007215279

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 951
rank avg (pred): 0.279 +- 0.089
mrr vals (pred, true): 0.064, 0.043
batch losses (mrrl, rdl): 0.0020795851, 0.0005596218

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 867
rank avg (pred): 0.250 +- 0.121
mrr vals (pred, true): 0.080, 0.040
batch losses (mrrl, rdl): 0.0089078918, 0.0008612825

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 319
rank avg (pred): 0.026 +- 0.031
mrr vals (pred, true): 0.534, 0.562
batch losses (mrrl, rdl): 0.0079953028, 6.33e-08

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 934
rank avg (pred): 0.279 +- 0.086
mrr vals (pred, true): 0.077, 0.082
batch losses (mrrl, rdl): 0.0071142232, 7.14285e-05

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1000
rank avg (pred): 0.259 +- 0.106
mrr vals (pred, true): 0.072, 0.119
batch losses (mrrl, rdl): 0.0224925503, 3.49981e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 772
rank avg (pred): 0.267 +- 0.100
mrr vals (pred, true): 0.064, 0.089
batch losses (mrrl, rdl): 0.0020117746, 4.26566e-05

Epoch over!
epoch time: 52.636

Epoch 9 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 332
rank avg (pred): 0.240 +- 0.115
mrr vals (pred, true): 0.077, 0.133
batch losses (mrrl, rdl): 0.0319657065, 3.34819e-05

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 258
rank avg (pred): 0.031 +- 0.037
mrr vals (pred, true): 0.510, 0.509
batch losses (mrrl, rdl): 1.27014e-05, 6.28e-08

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 39
rank avg (pred): 0.035 +- 0.043
mrr vals (pred, true): 0.514, 0.516
batch losses (mrrl, rdl): 6.4328e-05, 1.0776e-06

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 931
rank avg (pred): 0.250 +- 0.087
mrr vals (pred, true): 0.080, 0.093
batch losses (mrrl, rdl): 0.0091661345, 7.92079e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 437
rank avg (pred): 0.232 +- 0.117
mrr vals (pred, true): 0.083, 0.040
batch losses (mrrl, rdl): 0.0106514292, 0.0009048221

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1134
rank avg (pred): 0.126 +- 0.109
mrr vals (pred, true): 0.106, 0.160
batch losses (mrrl, rdl): 0.0293657258, 0.0001652435

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 51
rank avg (pred): 0.024 +- 0.029
mrr vals (pred, true): 0.554, 0.506
batch losses (mrrl, rdl): 0.0232648291, 6.576e-07

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 329
rank avg (pred): 0.280 +- 0.086
mrr vals (pred, true): 0.061, 0.121
batch losses (mrrl, rdl): 0.0363183431, 5.01893e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 728
rank avg (pred): 0.238 +- 0.059
mrr vals (pred, true): 0.052, 0.040
batch losses (mrrl, rdl): 3.0943e-05, 0.0009748248

Epoch over!
epoch time: 52.571

Epoch 10 -- 
running batch: 0 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 652
rank avg (pred): 0.309 +- 0.037
mrr vals (pred, true): 0.036, 0.040
batch losses (mrrl, rdl): 0.0019894985, 0.0004784105

running batch: 500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 443
rank avg (pred): 0.225 +- 0.108
mrr vals (pred, true): 0.083, 0.044
batch losses (mrrl, rdl): 0.0109974062, 0.0009877335

running batch: 1000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 39
rank avg (pred): 0.034 +- 0.041
mrr vals (pred, true): 0.509, 0.516
batch losses (mrrl, rdl): 0.0005743174, 8.933e-07

running batch: 1500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 908
rank avg (pred): 0.106 +- 0.117
mrr vals (pred, true): 0.247, 0.329
batch losses (mrrl, rdl): 0.0660369545, 6.21617e-05

running batch: 2000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 832
rank avg (pred): 0.035 +- 0.039
mrr vals (pred, true): 0.464, 0.528
batch losses (mrrl, rdl): 0.0403504595, 1.997e-07

running batch: 2500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 400
rank avg (pred): 0.242 +- 0.088
mrr vals (pred, true): 0.071, 0.119
batch losses (mrrl, rdl): 0.0221546944, 3.49755e-05

running batch: 3000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 720
rank avg (pred): 0.286 +- 0.032
mrr vals (pred, true): 0.036, 0.045
batch losses (mrrl, rdl): 0.0019402872, 0.0005488421

running batch: 3500 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 931
rank avg (pred): 0.256 +- 0.069
mrr vals (pred, true): 0.075, 0.093
batch losses (mrrl, rdl): 0.0063614817, 7.91358e-05

running batch: 4000 / 4376 and superbatch(1); data from TransE, UMLS, run 2.1, exp 15
rank avg (pred): 0.030 +- 0.035
mrr vals (pred, true): 0.504, 0.488
batch losses (mrrl, rdl): 0.0027287179, 5.5e-09

Epoch over!
epoch time: 51.627

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.089 +- 0.099
mrr vals (pred, true): 0.169, 0.158

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   41 	     0 	 0.06646 	 0.03261 	 m..s
   30 	     1 	 0.06456 	 0.03363 	 m..s
    3 	     2 	 0.04426 	 0.03422 	 ~...
   49 	     3 	 0.07293 	 0.03449 	 m..s
    5 	     4 	 0.04608 	 0.03550 	 ~...
    0 	     5 	 0.04147 	 0.03584 	 ~...
   15 	     6 	 0.05800 	 0.03617 	 ~...
   45 	     7 	 0.06764 	 0.03637 	 m..s
   29 	     8 	 0.06406 	 0.03642 	 ~...
   16 	     9 	 0.05844 	 0.03672 	 ~...
   42 	    10 	 0.06664 	 0.03674 	 ~...
   55 	    11 	 0.07564 	 0.03702 	 m..s
   32 	    12 	 0.06461 	 0.03706 	 ~...
   60 	    13 	 0.07868 	 0.03735 	 m..s
    6 	    14 	 0.04613 	 0.03754 	 ~...
   17 	    15 	 0.05958 	 0.03757 	 ~...
   22 	    16 	 0.06131 	 0.03762 	 ~...
   64 	    17 	 0.08351 	 0.03783 	 m..s
   74 	    18 	 0.08781 	 0.03806 	 m..s
    9 	    19 	 0.04883 	 0.03811 	 ~...
   20 	    20 	 0.06027 	 0.03823 	 ~...
   50 	    21 	 0.07308 	 0.03827 	 m..s
    8 	    22 	 0.04767 	 0.03848 	 ~...
   65 	    23 	 0.08366 	 0.03852 	 m..s
   10 	    24 	 0.05219 	 0.03862 	 ~...
    1 	    25 	 0.04156 	 0.03872 	 ~...
   51 	    26 	 0.07440 	 0.03913 	 m..s
    7 	    27 	 0.04666 	 0.03917 	 ~...
   31 	    28 	 0.06458 	 0.03935 	 ~...
   27 	    29 	 0.06339 	 0.03964 	 ~...
   53 	    30 	 0.07552 	 0.04017 	 m..s
    2 	    31 	 0.04166 	 0.04024 	 ~...
   43 	    32 	 0.06681 	 0.04036 	 ~...
   47 	    33 	 0.06962 	 0.04050 	 ~...
   91 	    34 	 0.12156 	 0.04107 	 m..s
   19 	    35 	 0.06021 	 0.04109 	 ~...
   54 	    36 	 0.07558 	 0.04112 	 m..s
   25 	    37 	 0.06294 	 0.04136 	 ~...
   63 	    38 	 0.08268 	 0.04143 	 m..s
   14 	    39 	 0.05755 	 0.04149 	 ~...
   13 	    40 	 0.05491 	 0.04153 	 ~...
   18 	    41 	 0.05963 	 0.04190 	 ~...
   81 	    42 	 0.09438 	 0.04235 	 m..s
   33 	    43 	 0.06465 	 0.04302 	 ~...
    4 	    44 	 0.04587 	 0.04311 	 ~...
   11 	    45 	 0.05260 	 0.04459 	 ~...
   36 	    46 	 0.06480 	 0.04465 	 ~...
   62 	    47 	 0.07931 	 0.04554 	 m..s
   82 	    48 	 0.09694 	 0.04715 	 m..s
   56 	    49 	 0.07567 	 0.04950 	 ~...
   78 	    50 	 0.09041 	 0.06341 	 ~...
   79 	    51 	 0.09044 	 0.07102 	 ~...
   76 	    52 	 0.08909 	 0.07290 	 ~...
   80 	    53 	 0.09361 	 0.07645 	 ~...
   61 	    54 	 0.07913 	 0.07655 	 ~...
   59 	    55 	 0.07788 	 0.07992 	 ~...
   24 	    56 	 0.06216 	 0.08032 	 ~...
   85 	    57 	 0.10093 	 0.08036 	 ~...
   72 	    58 	 0.08601 	 0.08064 	 ~...
   21 	    59 	 0.06031 	 0.08103 	 ~...
   70 	    60 	 0.08495 	 0.08279 	 ~...
   23 	    61 	 0.06211 	 0.08372 	 ~...
   38 	    62 	 0.06544 	 0.08385 	 ~...
   67 	    63 	 0.08407 	 0.08426 	 ~...
   58 	    64 	 0.07719 	 0.08605 	 ~...
   75 	    65 	 0.08783 	 0.08764 	 ~...
   52 	    66 	 0.07449 	 0.08962 	 ~...
   37 	    67 	 0.06538 	 0.08993 	 ~...
   73 	    68 	 0.08766 	 0.09161 	 ~...
   69 	    69 	 0.08445 	 0.09271 	 ~...
   35 	    70 	 0.06478 	 0.09360 	 ~...
   40 	    71 	 0.06630 	 0.09433 	 ~...
   12 	    72 	 0.05466 	 0.09458 	 m..s
   44 	    73 	 0.06731 	 0.09556 	 ~...
   71 	    74 	 0.08575 	 0.09637 	 ~...
   26 	    75 	 0.06301 	 0.10079 	 m..s
   34 	    76 	 0.06470 	 0.10161 	 m..s
   28 	    77 	 0.06391 	 0.10238 	 m..s
   84 	    78 	 0.09813 	 0.10322 	 ~...
   66 	    79 	 0.08371 	 0.10674 	 ~...
   96 	    80 	 0.25842 	 0.10788 	 MISS
   46 	    81 	 0.06778 	 0.10885 	 m..s
   48 	    82 	 0.07035 	 0.10902 	 m..s
   68 	    83 	 0.08441 	 0.11071 	 ~...
   86 	    84 	 0.10531 	 0.11099 	 ~...
   39 	    85 	 0.06611 	 0.11221 	 m..s
   89 	    86 	 0.10884 	 0.11450 	 ~...
   93 	    87 	 0.12527 	 0.11473 	 ~...
   57 	    88 	 0.07605 	 0.11491 	 m..s
   92 	    89 	 0.12501 	 0.12289 	 ~...
   87 	    90 	 0.10686 	 0.12305 	 ~...
   77 	    91 	 0.09012 	 0.12945 	 m..s
   88 	    92 	 0.10759 	 0.13367 	 ~...
   83 	    93 	 0.09708 	 0.14825 	 m..s
   90 	    94 	 0.11077 	 0.14897 	 m..s
   95 	    95 	 0.16851 	 0.15811 	 ~...
   94 	    96 	 0.15497 	 0.16705 	 ~...
   99 	    97 	 0.54540 	 0.43219 	 MISS
  101 	    98 	 0.55321 	 0.44759 	 MISS
  103 	    99 	 0.57760 	 0.49835 	 m..s
  108 	   100 	 0.59093 	 0.51229 	 m..s
   98 	   101 	 0.53638 	 0.51239 	 ~...
  104 	   102 	 0.58253 	 0.51696 	 m..s
  106 	   103 	 0.58846 	 0.52055 	 m..s
   97 	   104 	 0.53495 	 0.52124 	 ~...
  105 	   105 	 0.58400 	 0.52169 	 m..s
  100 	   106 	 0.54970 	 0.53107 	 ~...
  107 	   107 	 0.58852 	 0.54533 	 m..s
  109 	   108 	 0.59968 	 0.55704 	 m..s
  102 	   109 	 0.56514 	 0.56109 	 ~...
  112 	   110 	 0.60838 	 0.56179 	 m..s
  111 	   111 	 0.60194 	 0.56450 	 m..s
  113 	   112 	 0.60912 	 0.57114 	 m..s
  110 	   113 	 0.60084 	 0.57648 	 ~...
  117 	   114 	 0.63735 	 0.58207 	 m..s
  115 	   115 	 0.62442 	 0.60723 	 ~...
  118 	   116 	 0.63863 	 0.60827 	 m..s
  114 	   117 	 0.61376 	 0.61199 	 ~...
  120 	   118 	 0.64520 	 0.61521 	 ~...
  116 	   119 	 0.63383 	 0.62092 	 ~...
  119 	   120 	 0.64020 	 0.62143 	 ~...
==========================================
r_mrr = 0.9882100224494934
r2_mrr = 0.9651802778244019
spearmanr_mrr@5 = 0.9132677316665649
spearmanr_mrr@10 = 0.9818336963653564
spearmanr_mrr@50 = 0.9957659244537354
spearmanr_mrr@100 = 0.9954557418823242
spearmanr_mrr@All = 0.9956216216087341
==========================================
test time: 0.438
Done Testing dataset UMLS
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.152 +- 0.104
mrr vals (pred, true): 0.084, 0.108

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.05121 	 0.00014 	 m..s
   11 	     1 	 0.04568 	 0.00015 	 m..s
   78 	     2 	 0.08057 	 0.00017 	 m..s
   21 	     3 	 0.04936 	 0.00019 	 m..s
   44 	     4 	 0.06709 	 0.00019 	 m..s
   47 	     5 	 0.06806 	 0.00020 	 m..s
   72 	     6 	 0.07787 	 0.00021 	 m..s
    1 	     7 	 0.03985 	 0.00021 	 m..s
   27 	     8 	 0.05574 	 0.00022 	 m..s
   69 	     9 	 0.07773 	 0.00022 	 m..s
   68 	    10 	 0.07767 	 0.00023 	 m..s
   51 	    11 	 0.06854 	 0.00024 	 m..s
   96 	    12 	 0.09707 	 0.00025 	 m..s
   32 	    13 	 0.06090 	 0.00025 	 m..s
   63 	    14 	 0.07631 	 0.00026 	 m..s
   64 	    15 	 0.07669 	 0.00026 	 m..s
   90 	    16 	 0.09329 	 0.00026 	 m..s
   61 	    17 	 0.07548 	 0.00027 	 m..s
   77 	    18 	 0.08032 	 0.00027 	 m..s
   13 	    19 	 0.04599 	 0.00027 	 m..s
   56 	    20 	 0.07004 	 0.00028 	 m..s
   62 	    21 	 0.07559 	 0.00028 	 m..s
   67 	    22 	 0.07760 	 0.00029 	 m..s
   79 	    23 	 0.08081 	 0.00029 	 m..s
   29 	    24 	 0.05600 	 0.00031 	 m..s
   31 	    25 	 0.06032 	 0.00032 	 m..s
   28 	    26 	 0.05588 	 0.00032 	 m..s
   46 	    27 	 0.06794 	 0.00033 	 m..s
   71 	    28 	 0.07780 	 0.00036 	 m..s
   82 	    29 	 0.08380 	 0.00036 	 m..s
   43 	    30 	 0.06707 	 0.00042 	 m..s
   40 	    31 	 0.06669 	 0.00044 	 m..s
   17 	    32 	 0.04772 	 0.00045 	 m..s
   26 	    33 	 0.05557 	 0.00046 	 m..s
   55 	    34 	 0.06987 	 0.00054 	 m..s
   38 	    35 	 0.06519 	 0.00056 	 m..s
   19 	    36 	 0.04854 	 0.00062 	 m..s
   14 	    37 	 0.04621 	 0.00063 	 m..s
   45 	    38 	 0.06730 	 0.00064 	 m..s
   70 	    39 	 0.07778 	 0.00066 	 m..s
    2 	    40 	 0.04008 	 0.00066 	 m..s
   36 	    41 	 0.06412 	 0.00086 	 m..s
    9 	    42 	 0.04451 	 0.00195 	 m..s
   16 	    43 	 0.04702 	 0.00277 	 m..s
    0 	    44 	 0.03979 	 0.00326 	 m..s
   20 	    45 	 0.04906 	 0.01222 	 m..s
   30 	    46 	 0.05826 	 0.01438 	 m..s
   48 	    47 	 0.06806 	 0.01740 	 m..s
   49 	    48 	 0.06819 	 0.01774 	 m..s
   22 	    49 	 0.04975 	 0.02020 	 ~...
    6 	    50 	 0.04323 	 0.02036 	 ~...
    7 	    51 	 0.04368 	 0.02136 	 ~...
   50 	    52 	 0.06825 	 0.02180 	 m..s
   42 	    53 	 0.06681 	 0.02723 	 m..s
    3 	    54 	 0.04013 	 0.02774 	 ~...
   39 	    55 	 0.06520 	 0.02783 	 m..s
   41 	    56 	 0.06673 	 0.02935 	 m..s
   35 	    57 	 0.06266 	 0.03439 	 ~...
    5 	    58 	 0.04242 	 0.03447 	 ~...
   58 	    59 	 0.07363 	 0.03642 	 m..s
   12 	    60 	 0.04578 	 0.04199 	 ~...
   33 	    61 	 0.06121 	 0.04366 	 ~...
    4 	    62 	 0.04055 	 0.04427 	 ~...
   37 	    63 	 0.06465 	 0.04668 	 ~...
   10 	    64 	 0.04529 	 0.04716 	 ~...
   73 	    65 	 0.07816 	 0.04886 	 ~...
  100 	    66 	 0.11208 	 0.05393 	 m..s
   66 	    67 	 0.07714 	 0.05580 	 ~...
   54 	    68 	 0.06925 	 0.05605 	 ~...
    8 	    69 	 0.04374 	 0.05634 	 ~...
   34 	    70 	 0.06256 	 0.05842 	 ~...
   18 	    71 	 0.04809 	 0.05908 	 ~...
   52 	    72 	 0.06885 	 0.06574 	 ~...
   98 	    73 	 0.11182 	 0.06587 	 m..s
   75 	    74 	 0.07909 	 0.06609 	 ~...
   60 	    75 	 0.07503 	 0.06684 	 ~...
   59 	    76 	 0.07371 	 0.06746 	 ~...
   53 	    77 	 0.06907 	 0.06900 	 ~...
  101 	    78 	 0.11243 	 0.07213 	 m..s
   74 	    79 	 0.07816 	 0.07741 	 ~...
   25 	    80 	 0.05439 	 0.08200 	 ~...
   24 	    81 	 0.05434 	 0.08249 	 ~...
  104 	    82 	 0.11369 	 0.08429 	 ~...
   57 	    83 	 0.07015 	 0.08685 	 ~...
   99 	    84 	 0.11206 	 0.08856 	 ~...
   97 	    85 	 0.11172 	 0.08889 	 ~...
  108 	    86 	 0.11563 	 0.08908 	 ~...
   15 	    87 	 0.04696 	 0.09751 	 m..s
   83 	    88 	 0.08448 	 0.10106 	 ~...
  102 	    89 	 0.11348 	 0.10350 	 ~...
   86 	    90 	 0.09038 	 0.10701 	 ~...
   81 	    91 	 0.08359 	 0.10806 	 ~...
  103 	    92 	 0.11361 	 0.12816 	 ~...
   80 	    93 	 0.08184 	 0.12885 	 m..s
  113 	    94 	 0.12108 	 0.13345 	 ~...
  109 	    95 	 0.11568 	 0.13370 	 ~...
  105 	    96 	 0.11394 	 0.13518 	 ~...
  111 	    97 	 0.11710 	 0.13650 	 ~...
  110 	    98 	 0.11584 	 0.14340 	 ~...
   65 	    99 	 0.07677 	 0.14567 	 m..s
   93 	   100 	 0.09527 	 0.14721 	 m..s
   91 	   101 	 0.09332 	 0.15098 	 m..s
   88 	   102 	 0.09268 	 0.15099 	 m..s
  112 	   103 	 0.11727 	 0.15316 	 m..s
   76 	   104 	 0.07913 	 0.15452 	 m..s
  106 	   105 	 0.11403 	 0.15571 	 m..s
   87 	   106 	 0.09268 	 0.16298 	 m..s
   89 	   107 	 0.09272 	 0.16715 	 m..s
   85 	   108 	 0.08517 	 0.16959 	 m..s
   84 	   109 	 0.08503 	 0.16986 	 m..s
  115 	   110 	 0.13456 	 0.17161 	 m..s
   95 	   111 	 0.09581 	 0.18732 	 m..s
  118 	   112 	 0.24595 	 0.18863 	 m..s
   94 	   113 	 0.09562 	 0.19066 	 m..s
   92 	   114 	 0.09446 	 0.19752 	 MISS
  116 	   115 	 0.13514 	 0.20392 	 m..s
  107 	   116 	 0.11555 	 0.20896 	 m..s
  114 	   117 	 0.12423 	 0.23021 	 MISS
  119 	   118 	 0.25303 	 0.26695 	 ~...
  117 	   119 	 0.24385 	 0.28002 	 m..s
  120 	   120 	 0.25878 	 0.30290 	 m..s
==========================================
r_mrr = 0.747310221195221
r2_mrr = 0.45845991373062134
spearmanr_mrr@5 = 0.7882182598114014
spearmanr_mrr@10 = 0.9290265440940857
spearmanr_mrr@50 = 0.8916428685188293
spearmanr_mrr@100 = 0.8935294151306152
spearmanr_mrr@All = 0.9111489653587341
==========================================
test time: 0.521
Done Testing dataset DBpedia50
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.138 +- 0.107
mrr vals (pred, true): 0.070, 0.022

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   39 	     0 	 0.09182 	 0.00349 	 m..s
   11 	     1 	 0.04717 	 0.00349 	 m..s
   82 	     2 	 0.11244 	 0.00359 	 MISS
   14 	     3 	 0.05074 	 0.00365 	 m..s
   66 	     4 	 0.10386 	 0.00368 	 MISS
    1 	     5 	 0.03876 	 0.00370 	 m..s
   35 	     6 	 0.08769 	 0.00370 	 m..s
   46 	     7 	 0.09701 	 0.00372 	 m..s
   86 	     8 	 0.11592 	 0.00375 	 MISS
   77 	     9 	 0.10799 	 0.00377 	 MISS
   70 	    10 	 0.10557 	 0.00378 	 MISS
   71 	    11 	 0.10561 	 0.00380 	 MISS
    4 	    12 	 0.04411 	 0.00383 	 m..s
   64 	    13 	 0.10323 	 0.00386 	 m..s
    2 	    14 	 0.03891 	 0.00387 	 m..s
   61 	    15 	 0.10048 	 0.00387 	 m..s
   34 	    16 	 0.08754 	 0.00390 	 m..s
   81 	    17 	 0.11239 	 0.00392 	 MISS
    9 	    18 	 0.04575 	 0.00392 	 m..s
   44 	    19 	 0.09580 	 0.00393 	 m..s
   96 	    20 	 0.12446 	 0.00395 	 MISS
    5 	    21 	 0.04465 	 0.00400 	 m..s
   42 	    22 	 0.09437 	 0.00400 	 m..s
   87 	    23 	 0.11636 	 0.00400 	 MISS
   72 	    24 	 0.10565 	 0.00404 	 MISS
   18 	    25 	 0.05182 	 0.00404 	 m..s
   76 	    26 	 0.10761 	 0.00405 	 MISS
   50 	    27 	 0.09763 	 0.00406 	 m..s
   47 	    28 	 0.09703 	 0.00406 	 m..s
   73 	    29 	 0.10567 	 0.00408 	 MISS
   20 	    30 	 0.05423 	 0.00411 	 m..s
   78 	    31 	 0.11047 	 0.00421 	 MISS
   33 	    32 	 0.08226 	 0.00422 	 m..s
   30 	    33 	 0.07572 	 0.00422 	 m..s
   48 	    34 	 0.09706 	 0.00425 	 m..s
   22 	    35 	 0.05598 	 0.00427 	 m..s
   57 	    36 	 0.09987 	 0.00433 	 m..s
   68 	    37 	 0.10478 	 0.00452 	 MISS
   65 	    38 	 0.10376 	 0.00472 	 m..s
   36 	    39 	 0.08838 	 0.00486 	 m..s
   37 	    40 	 0.08854 	 0.00493 	 m..s
   93 	    41 	 0.12013 	 0.00508 	 MISS
    3 	    42 	 0.04285 	 0.00672 	 m..s
   24 	    43 	 0.05906 	 0.00772 	 m..s
   25 	    44 	 0.05918 	 0.00884 	 m..s
    0 	    45 	 0.03855 	 0.00943 	 ~...
    7 	    46 	 0.04501 	 0.00967 	 m..s
   19 	    47 	 0.05216 	 0.01105 	 m..s
    6 	    48 	 0.04481 	 0.01493 	 ~...
    8 	    49 	 0.04566 	 0.01532 	 m..s
   17 	    50 	 0.05175 	 0.01566 	 m..s
   13 	    51 	 0.05055 	 0.01584 	 m..s
   12 	    52 	 0.04904 	 0.01678 	 m..s
   15 	    53 	 0.05115 	 0.01756 	 m..s
   10 	    54 	 0.04591 	 0.02020 	 ~...
   21 	    55 	 0.05528 	 0.02094 	 m..s
   26 	    56 	 0.06123 	 0.02133 	 m..s
   16 	    57 	 0.05143 	 0.02173 	 ~...
   28 	    58 	 0.07404 	 0.02189 	 m..s
   27 	    59 	 0.06989 	 0.02199 	 m..s
   23 	    60 	 0.05625 	 0.02739 	 ~...
   60 	    61 	 0.10045 	 0.02829 	 m..s
   31 	    62 	 0.07599 	 0.03596 	 m..s
   32 	    63 	 0.07613 	 0.03752 	 m..s
   29 	    64 	 0.07562 	 0.10802 	 m..s
   38 	    65 	 0.08946 	 0.11554 	 ~...
  101 	    66 	 0.18082 	 0.11814 	 m..s
   41 	    67 	 0.09300 	 0.12155 	 ~...
   40 	    68 	 0.09292 	 0.12516 	 m..s
   53 	    69 	 0.09798 	 0.12904 	 m..s
   52 	    70 	 0.09794 	 0.13096 	 m..s
   99 	    71 	 0.16925 	 0.13290 	 m..s
   45 	    72 	 0.09673 	 0.13521 	 m..s
   43 	    73 	 0.09472 	 0.13539 	 m..s
  100 	    74 	 0.17474 	 0.14063 	 m..s
   51 	    75 	 0.09773 	 0.14112 	 m..s
   56 	    76 	 0.09963 	 0.14271 	 m..s
   49 	    77 	 0.09762 	 0.14286 	 m..s
   62 	    78 	 0.10075 	 0.14852 	 m..s
  103 	    79 	 0.18484 	 0.15206 	 m..s
   54 	    80 	 0.09845 	 0.15265 	 m..s
   97 	    81 	 0.16309 	 0.15408 	 ~...
   69 	    82 	 0.10485 	 0.15438 	 m..s
   84 	    83 	 0.11292 	 0.15609 	 m..s
   98 	    84 	 0.16541 	 0.15806 	 ~...
   63 	    85 	 0.10131 	 0.16139 	 m..s
   67 	    86 	 0.10409 	 0.16157 	 m..s
   74 	    87 	 0.10669 	 0.16385 	 m..s
   85 	    88 	 0.11295 	 0.16434 	 m..s
   83 	    89 	 0.11281 	 0.16486 	 m..s
   58 	    90 	 0.10014 	 0.16538 	 m..s
   75 	    91 	 0.10750 	 0.16593 	 m..s
  102 	    92 	 0.18447 	 0.16779 	 ~...
   59 	    93 	 0.10023 	 0.16789 	 m..s
  108 	    94 	 0.19671 	 0.16954 	 ~...
  104 	    95 	 0.18700 	 0.17290 	 ~...
   55 	    96 	 0.09903 	 0.17394 	 m..s
   90 	    97 	 0.11922 	 0.17700 	 m..s
   80 	    98 	 0.11175 	 0.17801 	 m..s
   89 	    99 	 0.11723 	 0.17983 	 m..s
  106 	   100 	 0.19436 	 0.18107 	 ~...
   88 	   101 	 0.11680 	 0.18549 	 m..s
   79 	   102 	 0.11160 	 0.18632 	 m..s
   94 	   103 	 0.12168 	 0.18871 	 m..s
  105 	   104 	 0.19120 	 0.18990 	 ~...
  115 	   105 	 0.23987 	 0.19522 	 m..s
  117 	   106 	 0.25228 	 0.19574 	 m..s
  107 	   107 	 0.19447 	 0.19639 	 ~...
   95 	   108 	 0.12278 	 0.19838 	 m..s
  114 	   109 	 0.23534 	 0.20114 	 m..s
   92 	   110 	 0.11971 	 0.20159 	 m..s
   91 	   111 	 0.11970 	 0.20379 	 m..s
  111 	   112 	 0.21363 	 0.20744 	 ~...
  109 	   113 	 0.20904 	 0.22076 	 ~...
  116 	   114 	 0.24598 	 0.23410 	 ~...
  118 	   115 	 0.25901 	 0.23952 	 ~...
  110 	   116 	 0.21313 	 0.24444 	 m..s
  112 	   117 	 0.22216 	 0.25210 	 ~...
  113 	   118 	 0.22327 	 0.25607 	 m..s
  119 	   119 	 0.26366 	 0.26222 	 ~...
  120 	   120 	 0.28167 	 0.27211 	 ~...
==========================================
r_mrr = 0.7477728724479675
r2_mrr = 0.46999865770339966
spearmanr_mrr@5 = 0.9857252240180969
spearmanr_mrr@10 = 0.9768493175506592
spearmanr_mrr@50 = 0.962438702583313
spearmanr_mrr@100 = 0.8340756297111511
spearmanr_mrr@All = 0.867730438709259
==========================================
test time: 0.488
Done Testing dataset CoDExSmall
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.027 +- 0.007
mrr vals (pred, true): 0.271, 0.283

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.04320 	 0.03957 	 ~...
   69 	     1 	 0.05108 	 0.04134 	 ~...
   64 	     2 	 0.05009 	 0.04155 	 ~...
   37 	     3 	 0.04664 	 0.04160 	 ~...
   68 	     4 	 0.05106 	 0.04173 	 ~...
   45 	     5 	 0.04698 	 0.04205 	 ~...
   53 	     6 	 0.04824 	 0.04207 	 ~...
   46 	     7 	 0.04704 	 0.04216 	 ~...
   23 	     8 	 0.04493 	 0.04224 	 ~...
   34 	     9 	 0.04643 	 0.04225 	 ~...
   66 	    10 	 0.05044 	 0.04244 	 ~...
   14 	    11 	 0.04242 	 0.04280 	 ~...
   67 	    12 	 0.05071 	 0.04281 	 ~...
   30 	    13 	 0.04615 	 0.04282 	 ~...
   59 	    14 	 0.04951 	 0.04290 	 ~...
   80 	    15 	 0.06548 	 0.04298 	 ~...
   20 	    16 	 0.04398 	 0.04312 	 ~...
   73 	    17 	 0.05425 	 0.04316 	 ~...
   74 	    18 	 0.05428 	 0.04321 	 ~...
   40 	    19 	 0.04666 	 0.04322 	 ~...
   76 	    20 	 0.05470 	 0.04349 	 ~...
   70 	    21 	 0.05140 	 0.04362 	 ~...
   26 	    22 	 0.04546 	 0.04364 	 ~...
   56 	    23 	 0.04841 	 0.04367 	 ~...
   47 	    24 	 0.04707 	 0.04369 	 ~...
   48 	    25 	 0.04787 	 0.04371 	 ~...
   18 	    26 	 0.04322 	 0.04377 	 ~...
   75 	    27 	 0.05458 	 0.04383 	 ~...
   78 	    28 	 0.05877 	 0.04384 	 ~...
   55 	    29 	 0.04838 	 0.04384 	 ~...
   82 	    30 	 0.06946 	 0.04387 	 ~...
   27 	    31 	 0.04567 	 0.04389 	 ~...
    2 	    32 	 0.04007 	 0.04390 	 ~...
   60 	    33 	 0.04954 	 0.04409 	 ~...
   33 	    34 	 0.04642 	 0.04415 	 ~...
   79 	    35 	 0.06365 	 0.04415 	 ~...
   41 	    36 	 0.04682 	 0.04418 	 ~...
   81 	    37 	 0.06616 	 0.04427 	 ~...
   42 	    38 	 0.04683 	 0.04443 	 ~...
    8 	    39 	 0.04096 	 0.04460 	 ~...
    0 	    40 	 0.03886 	 0.04475 	 ~...
   24 	    41 	 0.04536 	 0.04484 	 ~...
   38 	    42 	 0.04664 	 0.04490 	 ~...
   58 	    43 	 0.04906 	 0.04495 	 ~...
   31 	    44 	 0.04622 	 0.04522 	 ~...
   39 	    45 	 0.04665 	 0.04575 	 ~...
   16 	    46 	 0.04283 	 0.04582 	 ~...
   11 	    47 	 0.04211 	 0.04602 	 ~...
   28 	    48 	 0.04568 	 0.04611 	 ~...
   71 	    49 	 0.05151 	 0.04648 	 ~...
   77 	    50 	 0.05740 	 0.04659 	 ~...
   15 	    51 	 0.04272 	 0.04659 	 ~...
   57 	    52 	 0.04847 	 0.04666 	 ~...
   29 	    53 	 0.04587 	 0.04680 	 ~...
   63 	    54 	 0.05005 	 0.04700 	 ~...
   21 	    55 	 0.04472 	 0.04723 	 ~...
   65 	    56 	 0.05041 	 0.04760 	 ~...
   72 	    57 	 0.05217 	 0.04812 	 ~...
    5 	    58 	 0.04076 	 0.04831 	 ~...
   25 	    59 	 0.04543 	 0.04908 	 ~...
   62 	    60 	 0.05003 	 0.04949 	 ~...
   61 	    61 	 0.04997 	 0.05004 	 ~...
   50 	    62 	 0.04807 	 0.05004 	 ~...
   12 	    63 	 0.04231 	 0.05011 	 ~...
    9 	    64 	 0.04126 	 0.05029 	 ~...
   51 	    65 	 0.04816 	 0.05037 	 ~...
   52 	    66 	 0.04817 	 0.05054 	 ~...
   54 	    67 	 0.04837 	 0.05086 	 ~...
   22 	    68 	 0.04486 	 0.05093 	 ~...
   44 	    69 	 0.04687 	 0.05187 	 ~...
   49 	    70 	 0.04792 	 0.05198 	 ~...
   35 	    71 	 0.04652 	 0.05264 	 ~...
    3 	    72 	 0.04017 	 0.05321 	 ~...
   43 	    73 	 0.04684 	 0.05373 	 ~...
    4 	    74 	 0.04073 	 0.05418 	 ~...
   19 	    75 	 0.04382 	 0.05435 	 ~...
   32 	    76 	 0.04624 	 0.05607 	 ~...
   13 	    77 	 0.04232 	 0.05666 	 ~...
   36 	    78 	 0.04655 	 0.05668 	 ~...
   10 	    79 	 0.04211 	 0.05720 	 ~...
    1 	    80 	 0.03985 	 0.05950 	 ~...
    7 	    81 	 0.04093 	 0.05973 	 ~...
    6 	    82 	 0.04089 	 0.06136 	 ~...
   87 	    83 	 0.16967 	 0.09735 	 m..s
   85 	    84 	 0.16662 	 0.18444 	 ~...
   88 	    85 	 0.17050 	 0.18775 	 ~...
   86 	    86 	 0.16916 	 0.18777 	 ~...
   83 	    87 	 0.16471 	 0.19000 	 ~...
   98 	    88 	 0.20329 	 0.19455 	 ~...
   97 	    89 	 0.20300 	 0.19653 	 ~...
   84 	    90 	 0.16595 	 0.20910 	 m..s
   95 	    91 	 0.18639 	 0.20997 	 ~...
   89 	    92 	 0.17095 	 0.21345 	 m..s
   96 	    93 	 0.18757 	 0.21355 	 ~...
   92 	    94 	 0.17692 	 0.21694 	 m..s
   94 	    95 	 0.18272 	 0.22144 	 m..s
   91 	    96 	 0.17461 	 0.22306 	 m..s
   93 	    97 	 0.17708 	 0.22373 	 m..s
   90 	    98 	 0.17408 	 0.22913 	 m..s
   99 	    99 	 0.20568 	 0.23470 	 ~...
  101 	   100 	 0.24212 	 0.25665 	 ~...
  105 	   101 	 0.27618 	 0.26262 	 ~...
  104 	   102 	 0.27271 	 0.26618 	 ~...
  108 	   103 	 0.27930 	 0.27130 	 ~...
  110 	   104 	 0.28507 	 0.27473 	 ~...
  116 	   105 	 0.33301 	 0.27780 	 m..s
  100 	   106 	 0.22396 	 0.27948 	 m..s
  111 	   107 	 0.28862 	 0.28228 	 ~...
  107 	   108 	 0.27676 	 0.28279 	 ~...
  103 	   109 	 0.27102 	 0.28336 	 ~...
  120 	   110 	 0.42017 	 0.28439 	 MISS
  113 	   111 	 0.29254 	 0.28647 	 ~...
  102 	   112 	 0.26199 	 0.29004 	 ~...
  114 	   113 	 0.29334 	 0.29011 	 ~...
  112 	   114 	 0.29195 	 0.29094 	 ~...
  106 	   115 	 0.27670 	 0.29285 	 ~...
  115 	   116 	 0.32087 	 0.29730 	 ~...
  117 	   117 	 0.34785 	 0.30148 	 m..s
  109 	   118 	 0.28411 	 0.32242 	 m..s
  119 	   119 	 0.36201 	 0.32456 	 m..s
  118 	   120 	 0.36045 	 0.34654 	 ~...
==========================================
r_mrr = 0.9738416075706482
r2_mrr = 0.9470670223236084
spearmanr_mrr@5 = 0.9427852630615234
spearmanr_mrr@10 = 0.9545319676399231
spearmanr_mrr@50 = 0.9676086902618408
spearmanr_mrr@100 = 0.9848702549934387
spearmanr_mrr@All = 0.9863665103912354
==========================================
test time: 0.462
Done Testing dataset Kinships
total time taken: 806.8815288543701
training time taken: 779.7390370368958
TWIG out ;))
