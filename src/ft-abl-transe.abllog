===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 148
rank avg (pred): 0.467 +- 0.004
mrr vals (pred, true): 0.001, 0.145
batch losses (mrrl, rdl): 0.0, 0.0027445373

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 404
rank avg (pred): 0.291 +- 0.192
mrr vals (pred, true): 0.080, 0.190
batch losses (mrrl, rdl): 0.0, 0.000807978

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1198
rank avg (pred): 0.310 +- 0.240
mrr vals (pred, true): 0.186, 0.004
batch losses (mrrl, rdl): 0.0, 0.000359351

Epoch over!
epoch time: 13.083

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.270 +- 0.234
mrr vals (pred, true): 0.262, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004475864

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 58
rank avg (pred): 0.058 +- 0.055
mrr vals (pred, true): 0.345, 0.179
batch losses (mrrl, rdl): 0.0, 2.3879e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 898
rank avg (pred): 0.132 +- 0.125
mrr vals (pred, true): 0.335, 0.028
batch losses (mrrl, rdl): 0.0, 1.71317e-05

Epoch over!
epoch time: 11.942

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.313 +- 0.258
mrr vals (pred, true): 0.210, 0.199
batch losses (mrrl, rdl): 0.0, 0.0012575308

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1206
rank avg (pred): 0.298 +- 0.274
mrr vals (pred, true): 0.331, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003666629

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 970
rank avg (pred): 0.313 +- 0.283
mrr vals (pred, true): 0.325, 0.004
batch losses (mrrl, rdl): 0.0, 0.000246307

Epoch over!
epoch time: 13.436

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 417
rank avg (pred): 0.280 +- 0.276
mrr vals (pred, true): 0.347, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003890363

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 16
rank avg (pred): 0.052 +- 0.056
mrr vals (pred, true): 0.413, 0.199
batch losses (mrrl, rdl): 0.0, 2.2215e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 170
rank avg (pred): 0.268 +- 0.287
mrr vals (pred, true): 0.416, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004597694

Epoch over!
epoch time: 13.769

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 450
rank avg (pred): 0.240 +- 0.265
mrr vals (pred, true): 0.410, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005703556

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.287 +- 0.288
mrr vals (pred, true): 0.356, 0.137
batch losses (mrrl, rdl): 0.0, 0.0009028268

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 797
rank avg (pred): 0.268 +- 0.284
mrr vals (pred, true): 0.375, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004624347

Epoch over!
epoch time: 13.674

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1157
rank avg (pred): 0.115 +- 0.130
mrr vals (pred, true): 0.454, 0.027
batch losses (mrrl, rdl): 1.6314854622, 5.0282e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1130
rank avg (pred): 0.456 +- 0.238
mrr vals (pred, true): 0.115, 0.004
batch losses (mrrl, rdl): 0.0423840396, 1.96864e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 404
rank avg (pred): 0.375 +- 0.212
mrr vals (pred, true): 0.116, 0.190
batch losses (mrrl, rdl): 0.0541543029, 0.0016256813

Epoch over!
epoch time: 13.789

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 484
rank avg (pred): 0.382 +- 0.212
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.0370881706, 0.0001186349

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.362 +- 0.200
mrr vals (pred, true): 0.123, 0.004
batch losses (mrrl, rdl): 0.0535611585, 0.0001350592

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 875
rank avg (pred): 0.379 +- 0.199
mrr vals (pred, true): 0.127, 0.004
batch losses (mrrl, rdl): 0.0589421988, 0.000141299

Epoch over!
epoch time: 13.816

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 205
rank avg (pred): 0.396 +- 0.194
mrr vals (pred, true): 0.105, 0.005
batch losses (mrrl, rdl): 0.0302826967, 7.63225e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1073
rank avg (pred): 0.078 +- 0.045
mrr vals (pred, true): 0.155, 0.187
batch losses (mrrl, rdl): 0.0104426313, 6.6097e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 811
rank avg (pred): 0.053 +- 0.031
mrr vals (pred, true): 0.182, 0.120
batch losses (mrrl, rdl): 0.0385900326, 0.0001118608

Epoch over!
epoch time: 13.233

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 144
rank avg (pred): 0.368 +- 0.168
mrr vals (pred, true): 0.088, 0.109
batch losses (mrrl, rdl): 0.0046837293, 0.0013406514

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 320
rank avg (pred): 0.012 +- 0.007
mrr vals (pred, true): 0.220, 0.267
batch losses (mrrl, rdl): 0.0218717717, 1.5016e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 866
rank avg (pred): 0.337 +- 0.163
mrr vals (pred, true): 0.111, 0.005
batch losses (mrrl, rdl): 0.0369205587, 0.0002572683

Epoch over!
epoch time: 13.255

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 865
rank avg (pred): 0.332 +- 0.161
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.0371175185, 0.0003526762

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 284
rank avg (pred): 0.037 +- 0.021
mrr vals (pred, true): 0.197, 0.243
batch losses (mrrl, rdl): 0.0209141113, 3.7674e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1032
rank avg (pred): 0.318 +- 0.152
mrr vals (pred, true): 0.113, 0.004
batch losses (mrrl, rdl): 0.0396271385, 0.0004516545

Epoch over!
epoch time: 12.486

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 806
rank avg (pred): 0.320 +- 0.148
mrr vals (pred, true): 0.101, 0.005
batch losses (mrrl, rdl): 0.026363641, 0.0003572645

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.348 +- 0.112
mrr vals (pred, true): 0.053, 0.011
batch losses (mrrl, rdl): 7.81602e-05, 6.88051e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 268
rank avg (pred): 0.013 +- 0.007
mrr vals (pred, true): 0.224, 0.244
batch losses (mrrl, rdl): 0.0039875684, 1.99924e-05

Epoch over!
epoch time: 13.603

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 178
rank avg (pred): 0.319 +- 0.134
mrr vals (pred, true): 0.089, 0.004
batch losses (mrrl, rdl): 0.0148324557, 0.0004595481

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 843
rank avg (pred): 0.305 +- 0.140
mrr vals (pred, true): 0.104, 0.044
batch losses (mrrl, rdl): 0.0289231651, 0.0003755689

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 660
rank avg (pred): 0.328 +- 0.109
mrr vals (pred, true): 0.061, 0.004
batch losses (mrrl, rdl): 0.0012557012, 0.0004313484

Epoch over!
epoch time: 13.597

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.056 +- 0.031
mrr vals (pred, true): 0.191, 0.153
batch losses (mrrl, rdl): 0.0144148944, 6.6044e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.297 +- 0.134
mrr vals (pred, true): 0.106, 0.004
batch losses (mrrl, rdl): 0.0315608643, 0.0006544858

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.291 +- 0.135
mrr vals (pred, true): 0.111, 0.114
batch losses (mrrl, rdl): 0.0001306176, 0.0006289514

Epoch over!
epoch time: 13.352

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 485
rank avg (pred): 0.290 +- 0.139
mrr vals (pred, true): 0.116, 0.004
batch losses (mrrl, rdl): 0.0439669043, 0.0005737268

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.052 +- 0.028
mrr vals (pred, true): 0.185, 0.205
batch losses (mrrl, rdl): 0.0038500642, 4.8663e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1187
rank avg (pred): 0.333 +- 0.114
mrr vals (pred, true): 0.065, 0.011
batch losses (mrrl, rdl): 0.0022058785, 5.15719e-05

Epoch over!
epoch time: 14.042

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 186
rank avg (pred): 0.311 +- 0.131
mrr vals (pred, true): 0.095, 0.004
batch losses (mrrl, rdl): 0.0199999623, 0.0004758934

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 329
rank avg (pred): 0.303 +- 0.133
mrr vals (pred, true): 0.101, 0.167
batch losses (mrrl, rdl): 0.0429006629, 0.0006331125

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 769
rank avg (pred): 0.291 +- 0.135
mrr vals (pred, true): 0.117, 0.166
batch losses (mrrl, rdl): 0.0234229881, 0.0008172649

Epoch over!
epoch time: 13.235

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.017 +- 0.009
mrr vals (pred, true): 0.213, 0.258

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   14 	     0 	 0.05087 	 0.00315 	 m..s
   15 	     1 	 0.05113 	 0.00338 	 m..s
   86 	     2 	 0.10839 	 0.00347 	 MISS
    0 	     3 	 0.04482 	 0.00350 	 m..s
   60 	     4 	 0.09868 	 0.00351 	 m..s
   64 	     5 	 0.10124 	 0.00352 	 m..s
   82 	     6 	 0.10575 	 0.00353 	 MISS
    1 	     7 	 0.04549 	 0.00356 	 m..s
   63 	     8 	 0.10119 	 0.00366 	 m..s
   58 	     9 	 0.09746 	 0.00370 	 m..s
   50 	    10 	 0.09349 	 0.00371 	 m..s
   12 	    11 	 0.05014 	 0.00378 	 m..s
   68 	    12 	 0.10244 	 0.00380 	 m..s
   62 	    13 	 0.09979 	 0.00391 	 m..s
   45 	    14 	 0.08835 	 0.00393 	 m..s
   91 	    15 	 0.11136 	 0.00394 	 MISS
   59 	    16 	 0.09747 	 0.00398 	 m..s
   18 	    17 	 0.05215 	 0.00409 	 m..s
   93 	    18 	 0.11490 	 0.00412 	 MISS
   75 	    19 	 0.10349 	 0.00416 	 m..s
   81 	    20 	 0.10463 	 0.00420 	 MISS
   17 	    21 	 0.05207 	 0.00429 	 m..s
    8 	    22 	 0.04849 	 0.00430 	 m..s
   87 	    23 	 0.10853 	 0.00435 	 MISS
   94 	    24 	 0.11595 	 0.00441 	 MISS
    7 	    25 	 0.04748 	 0.00442 	 m..s
   70 	    26 	 0.10287 	 0.00442 	 m..s
   28 	    27 	 0.05615 	 0.00464 	 m..s
   92 	    28 	 0.11473 	 0.00468 	 MISS
    4 	    29 	 0.04607 	 0.00472 	 m..s
   23 	    30 	 0.05398 	 0.00488 	 m..s
   73 	    31 	 0.10343 	 0.00492 	 m..s
   71 	    32 	 0.10313 	 0.00522 	 m..s
   16 	    33 	 0.05201 	 0.00724 	 m..s
   20 	    34 	 0.05340 	 0.00750 	 m..s
    5 	    35 	 0.04706 	 0.00972 	 m..s
   24 	    36 	 0.05403 	 0.01020 	 m..s
    3 	    37 	 0.04600 	 0.01032 	 m..s
   13 	    38 	 0.05031 	 0.01044 	 m..s
   22 	    39 	 0.05397 	 0.01077 	 m..s
    2 	    40 	 0.04587 	 0.01118 	 m..s
   11 	    41 	 0.05000 	 0.01207 	 m..s
   31 	    42 	 0.05889 	 0.01271 	 m..s
   37 	    43 	 0.06348 	 0.01713 	 m..s
   35 	    44 	 0.06124 	 0.01976 	 m..s
   34 	    45 	 0.06117 	 0.01991 	 m..s
   33 	    46 	 0.05979 	 0.02094 	 m..s
   32 	    47 	 0.05942 	 0.02142 	 m..s
   40 	    48 	 0.06582 	 0.02394 	 m..s
   41 	    49 	 0.06805 	 0.02424 	 m..s
   42 	    50 	 0.06811 	 0.02452 	 m..s
   36 	    51 	 0.06273 	 0.02471 	 m..s
   21 	    52 	 0.05360 	 0.02540 	 ~...
    6 	    53 	 0.04734 	 0.02563 	 ~...
   39 	    54 	 0.06544 	 0.02681 	 m..s
   26 	    55 	 0.05480 	 0.02777 	 ~...
   19 	    56 	 0.05320 	 0.02949 	 ~...
   38 	    57 	 0.06391 	 0.02953 	 m..s
   30 	    58 	 0.05666 	 0.04216 	 ~...
  107 	    59 	 0.17942 	 0.05027 	 MISS
   51 	    60 	 0.09371 	 0.09528 	 ~...
   67 	    61 	 0.10240 	 0.10008 	 ~...
   47 	    62 	 0.09176 	 0.10026 	 ~...
   48 	    63 	 0.09201 	 0.10045 	 ~...
   49 	    64 	 0.09250 	 0.10533 	 ~...
   66 	    65 	 0.10230 	 0.10834 	 ~...
   80 	    66 	 0.10455 	 0.10903 	 ~...
   56 	    67 	 0.09711 	 0.11933 	 ~...
   54 	    68 	 0.09657 	 0.12266 	 ~...
   61 	    69 	 0.09899 	 0.13455 	 m..s
   96 	    70 	 0.16495 	 0.13475 	 m..s
  100 	    71 	 0.16903 	 0.13483 	 m..s
   88 	    72 	 0.10903 	 0.13521 	 ~...
   52 	    73 	 0.09461 	 0.13539 	 m..s
  103 	    74 	 0.17329 	 0.14879 	 ~...
   53 	    75 	 0.09587 	 0.15392 	 m..s
   72 	    76 	 0.10337 	 0.15816 	 m..s
  104 	    77 	 0.17464 	 0.16218 	 ~...
    9 	    78 	 0.04860 	 0.16279 	 MISS
   74 	    79 	 0.10343 	 0.16373 	 m..s
   83 	    80 	 0.10633 	 0.16385 	 m..s
   69 	    81 	 0.10280 	 0.16467 	 m..s
   76 	    82 	 0.10366 	 0.16469 	 m..s
   78 	    83 	 0.10392 	 0.16489 	 m..s
   10 	    84 	 0.04891 	 0.16626 	 MISS
   84 	    85 	 0.10650 	 0.16632 	 m..s
   43 	    86 	 0.08656 	 0.16773 	 m..s
   55 	    87 	 0.09711 	 0.17114 	 m..s
   46 	    88 	 0.09106 	 0.17175 	 m..s
   44 	    89 	 0.08752 	 0.17219 	 m..s
   85 	    90 	 0.10731 	 0.17260 	 m..s
  106 	    91 	 0.17897 	 0.17536 	 ~...
   89 	    92 	 0.10923 	 0.17544 	 m..s
   29 	    93 	 0.05624 	 0.17614 	 MISS
   90 	    94 	 0.11038 	 0.17829 	 m..s
   65 	    95 	 0.10142 	 0.18105 	 m..s
   27 	    96 	 0.05560 	 0.18122 	 MISS
   25 	    97 	 0.05417 	 0.18550 	 MISS
  113 	    98 	 0.20873 	 0.18788 	 ~...
   99 	    99 	 0.16817 	 0.18868 	 ~...
   79 	   100 	 0.10439 	 0.18910 	 m..s
  119 	   101 	 0.22543 	 0.19419 	 m..s
  108 	   102 	 0.18201 	 0.19721 	 ~...
   77 	   103 	 0.10377 	 0.19838 	 m..s
  102 	   104 	 0.17058 	 0.20033 	 ~...
   57 	   105 	 0.09737 	 0.20081 	 MISS
   98 	   106 	 0.16805 	 0.20093 	 m..s
  101 	   107 	 0.17037 	 0.20331 	 m..s
  112 	   108 	 0.20583 	 0.20554 	 ~...
  111 	   109 	 0.20063 	 0.20723 	 ~...
  118 	   110 	 0.21826 	 0.21020 	 ~...
   95 	   111 	 0.11666 	 0.21971 	 MISS
  114 	   112 	 0.21023 	 0.23008 	 ~...
  109 	   113 	 0.18241 	 0.23807 	 m..s
   97 	   114 	 0.16728 	 0.23887 	 m..s
  116 	   115 	 0.21057 	 0.24137 	 m..s
  115 	   116 	 0.21053 	 0.24340 	 m..s
  105 	   117 	 0.17569 	 0.24540 	 m..s
  120 	   118 	 0.24025 	 0.25626 	 ~...
  117 	   119 	 0.21291 	 0.25834 	 m..s
  110 	   120 	 0.19300 	 0.26909 	 m..s
==========================================
r_mrr = 0.6889808773994446
r2_mrr = 0.451246440410614
spearmanr_mrr@5 = 0.966301441192627
spearmanr_mrr@10 = 0.9444057941436768
spearmanr_mrr@50 = 0.9407904744148254
spearmanr_mrr@100 = 0.8757395148277283
spearmanr_mrr@All = 0.9041807651519775
==========================================
test time: 0.45
Done Testing dataset CoDExSmall
total time taken: 212.22255945205688
training time taken: 200.86481761932373
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.6890)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4512)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9663)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9444)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9408)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8757)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.9042)}}, 'test_loss': {'TransE': {'CoDExSmall': 3.105165259923524}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 508425322870164
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 1122, 973, 460, 838, 863, 909, 21, 504, 464, 89, 865, 842, 266, 1184, 609, 899, 1110, 285, 455, 921, 748, 52, 550, 604, 328, 224, 633, 193, 763, 299, 990, 969, 1019, 1138, 272, 1171, 110, 497, 1146, 42, 696, 1046, 46, 645, 56, 300, 941, 77, 76, 954, 124, 6, 232, 32, 848, 326, 1153, 823, 364, 815, 225, 600, 576, 182, 914, 1173, 797, 1035, 854, 1113, 729, 454, 11, 427, 394, 896, 1126, 939, 140, 857, 416, 866, 214, 817, 279, 1069, 593, 543, 937, 793, 762, 1038, 432, 1060, 888, 711, 362, 818, 956, 1, 469, 142, 625, 377, 578, 506, 329, 839, 895, 929, 556, 101, 1005, 1196, 352, 260, 970, 1116, 322, 55]
valid_ids (0): []
train_ids (1094): [388, 230, 437, 675, 201, 383, 579, 862, 400, 280, 1054, 35, 10, 978, 1157, 652, 1158, 655, 268, 1088, 286, 1037, 367, 639, 318, 674, 515, 241, 1057, 591, 778, 940, 636, 1130, 713, 136, 715, 475, 619, 67, 430, 531, 8, 209, 632, 130, 786, 15, 755, 692, 319, 252, 664, 611, 424, 47, 943, 583, 827, 462, 1174, 294, 491, 1109, 363, 1197, 841, 1103, 790, 338, 1034, 78, 907, 1125, 386, 648, 912, 988, 852, 289, 29, 812, 45, 40, 856, 379, 723, 757, 777, 119, 301, 754, 568, 679, 700, 274, 1190, 993, 1089, 672, 980, 1042, 606, 544, 629, 349, 710, 314, 706, 760, 867, 221, 1136, 66, 61, 396, 1043, 667, 530, 484, 994, 1133, 596, 638, 135, 282, 313, 802, 642, 792, 307, 139, 1049, 137, 1008, 803, 335, 305, 1094, 433, 850, 1092, 602, 936, 378, 178, 492, 255, 1102, 534, 389, 87, 80, 60, 1191, 1093, 1023, 324, 779, 933, 100, 890, 112, 1058, 86, 871, 858, 958, 787, 191, 785, 949, 1017, 740, 668, 566, 85, 1121, 177, 357, 218, 238, 269, 479, 93, 687, 630, 561, 676, 744, 143, 846, 1118, 1135, 722, 292, 864, 188, 97, 892, 1072, 885, 25, 567, 102, 799, 820, 613, 637, 935, 1032, 254, 64, 964, 179, 747, 104, 195, 938, 242, 308, 1131, 944, 624, 985, 31, 81, 62, 804, 49, 735, 989, 107, 1096, 623, 580, 123, 371, 98, 798, 53, 183, 155, 312, 925, 1020, 945, 159, 283, 204, 75, 582, 1140, 766, 16, 950, 444, 1183, 196, 947, 855, 502, 1181, 805, 1214, 540, 281, 1167, 880, 331, 1067, 213, 1175, 273, 733, 824, 96, 3, 800, 791, 458, 1178, 343, 987, 374, 553, 983, 592, 247, 789, 971, 658, 1134, 441, 836, 1112, 1064, 886, 277, 918, 1154, 756, 222, 457, 346, 927, 720, 51, 170, 708, 320, 494, 468, 446, 495, 256, 486, 205, 752, 295, 1099, 878, 659, 702, 961, 761, 897, 948, 923, 1015, 117, 459, 765, 995, 354, 511, 1114, 194, 879, 290, 536, 407, 519, 1198, 192, 0, 125, 1106, 82, 37, 1039, 784, 471, 1193, 1009, 1036, 197, 1212, 529, 133, 1048, 439, 569, 651, 169, 877, 443, 122, 1194, 448, 239, 1172, 926, 38, 922, 1045, 816, 180, 1151, 1074, 1016, 649, 1024, 563, 910, 185, 425, 1101, 50, 677, 783, 128, 772, 418, 919, 1162, 693, 889, 1195, 808, 234, 873, 617, 120, 605, 369, 1176, 208, 1021, 429, 111, 271, 1168, 54, 361, 391, 891, 574, 333, 2, 828, 210, 1003, 344, 1029, 717, 549, 105, 245, 175, 533, 1161, 1170, 421, 240, 1090, 203, 265, 1111, 28, 481, 216, 1031, 138, 882, 874, 43, 1203, 1200, 17, 951, 588, 287, 339, 1041, 202, 235, 554, 1079, 164, 1145, 1210, 671, 466, 653, 745, 814, 1075, 310, 643, 1115, 1071, 911, 1185, 36, 199, 276, 641, 884, 870, 1001, 1107, 275, 351, 48, 616, 689, 33, 631, 325, 734, 622, 463, 115, 152, 306, 1091, 607, 837, 1123, 678, 782, 695, 423, 894, 682, 72, 90, 413, 1025, 825, 581, 560, 957, 1026, 844, 603, 1018, 399, 144, 974, 417, 1211, 171, 380, 258, 392, 725, 296, 703, 132, 24, 20, 627, 924, 1143, 573, 709, 381, 541, 806, 612, 408, 522, 771, 411, 490, 598, 634, 79, 893, 472, 1076, 236, 930, 1055, 1155, 160, 39, 226, 750, 847, 705, 1084, 434, 215, 341, 151, 595, 207, 1204, 116, 71, 304, 385, 233, 996, 704, 154, 906, 517, 438, 1177, 661, 356, 44, 728, 801, 770, 336, 1053, 575, 251, 382, 1105, 330, 1108, 982, 565, 913, 499, 449, 965, 521, 410, 1040, 902, 683, 1068, 545, 719, 572, 716, 810, 347, 1059, 1147, 172, 1010, 998, 262, 74, 966, 30, 1186, 309, 348, 167, 1202, 58, 376, 1061, 881, 165, 1165, 1002, 190, 920, 610, 718, 426, 398, 350, 153, 714, 302, 253, 900, 738, 1169, 422, 366, 131, 397, 781, 1033, 483, 477, 916, 688, 932, 586, 451, 764, 1201, 181, 860, 654, 480, 570, 503, 470, 849, 908, 1156, 833, 872, 528, 141, 1166, 963, 1137, 650, 539, 150, 547, 1150, 405, 1132, 999, 1187, 915, 1189, 514, 493, 1100, 984, 876, 246, 1117, 327, 70, 795, 577, 1004, 200, 7, 1087, 168, 261, 1022, 509, 1163, 1073, 690, 507, 134, 946, 12, 428, 524, 1097, 84, 206, 1207, 826, 903, 635, 473, 186, 518, 928, 173, 489, 1065, 13, 1160, 597, 628, 1027, 843, 157, 217, 1119, 419, 680, 291, 359, 584, 146, 587, 420, 952, 219, 9, 440, 527, 263, 1047, 1179, 1028, 303, 409, 1006, 450, 546, 829, 402, 721, 739, 775, 1014, 780, 1013, 231, 108, 1164, 293, 774, 482, 751, 1124, 220, 997, 284, 1213, 726, 942, 736, 384, 851, 737, 288, 360, 496, 753, 727, 807, 663, 788, 614, 227, 1192, 129, 883, 1141, 859, 387, 340, 749, 564, 662, 979, 834, 986, 353, 732, 1208, 478, 505, 187, 542, 311, 510, 68, 250, 59, 229, 730, 620, 212, 594, 315, 666, 644, 435, 237, 759, 372, 665, 548, 373, 794, 538, 65, 887, 41, 685, 647, 1030, 796, 1083, 121, 724, 488, 1188, 223, 114, 768, 370, 968, 1159, 508, 1152, 467, 975, 345, 163, 532, 853, 1062, 249, 758, 525, 162, 189, 684, 537, 321, 278, 904, 447, 332, 1086, 453, 355, 1056, 57, 557, 149, 337, 63, 585, 699, 516, 198, 211, 821, 742, 1104, 822, 590, 94, 1149, 166, 599, 819, 656, 618, 415, 741, 694, 589, 615, 1205, 646, 452, 868, 145, 1120, 512, 243, 1066, 669, 840, 19, 1199, 1052, 126, 1011, 316, 811, 869, 861, 358, 158, 555, 4, 334, 1078, 601, 395, 967, 776, 27, 686, 393, 640, 767, 461, 88, 436, 526, 501, 270, 500, 1128, 69, 513, 1098, 977, 1209, 562, 298, 1129, 1095, 401, 1070, 498, 1182, 342, 1080, 962, 73, 773, 670, 1012, 1139, 832, 485, 26, 976, 571, 259, 1082, 14, 551, 520, 113, 559, 106, 960, 184, 552, 845, 118, 442, 34, 691, 487, 905, 412, 174, 109, 1144, 813, 608, 707, 712, 22, 898, 465, 390, 673, 558, 746, 1081, 375, 323, 92, 267, 953, 23, 414, 228, 95, 5, 406, 698, 981, 476, 147, 403, 626, 1180, 835, 1148, 992, 18, 264, 1077, 445, 1127, 1007, 148, 523, 959, 769, 955, 1044, 809, 535, 176, 1206, 934, 456, 697, 127, 660, 156, 1000, 431, 297, 1050, 991, 875, 99, 731, 621, 83, 161, 244, 404, 317, 103, 681, 701, 368, 474, 931, 91, 365, 917, 830, 1063, 972, 901, 743, 1051, 1085, 257, 1142, 657, 248]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4810767613836557
the save name prefix for this run is:  chkpt-ID_4810767613836557_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.543 +- 0.002
mrr vals (pred, true): 0.001, 0.019
batch losses (mrrl, rdl): 0.0, 0.0023035528

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1106
rank avg (pred): 0.269 +- 0.261
mrr vals (pred, true): 0.137, 0.212
batch losses (mrrl, rdl): 0.0, 0.000935186

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 720
rank avg (pred): 0.316 +- 0.290
mrr vals (pred, true): 0.142, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003048663

Epoch over!
epoch time: 12.906

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 837
rank avg (pred): 0.281 +- 0.282
mrr vals (pred, true): 0.213, 0.158
batch losses (mrrl, rdl): 0.0, 0.0008384079

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.297 +- 0.291
mrr vals (pred, true): 0.202, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002985254

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.290 +- 0.301
mrr vals (pred, true): 0.277, 0.169
batch losses (mrrl, rdl): 0.0, 0.0009463052

Epoch over!
epoch time: 12.909

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.090 +- 0.099
mrr vals (pred, true): 0.339, 0.149
batch losses (mrrl, rdl): 0.0, 1.44315e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 305
rank avg (pred): 0.098 +- 0.109
mrr vals (pred, true): 0.354, 0.204
batch losses (mrrl, rdl): 0.0, 2.91572e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 631
rank avg (pred): 0.267 +- 0.291
mrr vals (pred, true): 0.345, 0.011
batch losses (mrrl, rdl): 0.0, 1.83982e-05

Epoch over!
epoch time: 13.196

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 125
rank avg (pred): 0.338 +- 0.318
mrr vals (pred, true): 0.203, 0.174
batch losses (mrrl, rdl): 0.0, 0.0014977599

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.212 +- 0.233
mrr vals (pred, true): 0.348, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009359238

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 524
rank avg (pred): 0.056 +- 0.065
mrr vals (pred, true): 0.460, 0.036
batch losses (mrrl, rdl): 0.0, 0.0002092918

Epoch over!
epoch time: 12.456

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 891
rank avg (pred): 0.066 +- 0.078
mrr vals (pred, true): 0.484, 0.040
batch losses (mrrl, rdl): 0.0, 7.437e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.292 +- 0.302
mrr vals (pred, true): 0.311, 0.142
batch losses (mrrl, rdl): 0.0, 0.0009165649

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.278 +- 0.296
mrr vals (pred, true): 0.355, 0.009
batch losses (mrrl, rdl): 0.0, 2.0207e-05

Epoch over!
epoch time: 12.86

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.083 +- 0.092
mrr vals (pred, true): 0.422, 0.220
batch losses (mrrl, rdl): 0.4070174992, 2.67095e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 889
rank avg (pred): 0.433 +- 0.223
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0345225856, 2.29143e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 928
rank avg (pred): 0.436 +- 0.192
mrr vals (pred, true): 0.088, 0.167
batch losses (mrrl, rdl): 0.0615628846, 0.0024616313

Epoch over!
epoch time: 12.858

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.139 +- 0.089
mrr vals (pred, true): 0.144, 0.152
batch losses (mrrl, rdl): 0.0006161628, 0.0001337019

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.422 +- 0.176
mrr vals (pred, true): 0.078, 0.176
batch losses (mrrl, rdl): 0.094758302, 0.0019054024

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.129 +- 0.076
mrr vals (pred, true): 0.146, 0.021
batch losses (mrrl, rdl): 0.0914839581, 0.0001488971

Epoch over!
epoch time: 13.304

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.418 +- 0.161
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0073724072, 9.08139e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 856
rank avg (pred): 0.389 +- 0.168
mrr vals (pred, true): 0.095, 0.158
batch losses (mrrl, rdl): 0.0401262566, 0.0018273744

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.387 +- 0.154
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0127505697, 0.0001504863

Epoch over!
epoch time: 12.745

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.123 +- 0.069
mrr vals (pred, true): 0.140, 0.024
batch losses (mrrl, rdl): 0.0810638815, 0.0001433219

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.243 +- 0.132
mrr vals (pred, true): 0.142, 0.040
batch losses (mrrl, rdl): 0.0840840638, 0.0004517715

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.375 +- 0.138
mrr vals (pred, true): 0.076, 0.003
batch losses (mrrl, rdl): 0.0067039449, 0.0002235842

Epoch over!
epoch time: 12.778

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 647
rank avg (pred): 0.353 +- 0.153
mrr vals (pred, true): 0.105, 0.019
batch losses (mrrl, rdl): 0.0306430906, 0.0001254588

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 97
rank avg (pred): 0.359 +- 0.139
mrr vals (pred, true): 0.092, 0.143
batch losses (mrrl, rdl): 0.0258410387, 0.0014010246

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.349 +- 0.137
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0127391377, 0.0003082156

Epoch over!
epoch time: 12.366

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.343 +- 0.134
mrr vals (pred, true): 0.088, 0.004
batch losses (mrrl, rdl): 0.014157502, 0.0003028753

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 401
rank avg (pred): 0.341 +- 0.134
mrr vals (pred, true): 0.091, 0.189
batch losses (mrrl, rdl): 0.0963291079, 0.0011908063

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.321 +- 0.139
mrr vals (pred, true): 0.105, 0.004
batch losses (mrrl, rdl): 0.0304586049, 0.0004436694

Epoch over!
epoch time: 13.218

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.061 +- 0.034
mrr vals (pred, true): 0.156, 0.189
batch losses (mrrl, rdl): 0.0109113958, 1.10661e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1097
rank avg (pred): 0.326 +- 0.148
mrr vals (pred, true): 0.107, 0.212
batch losses (mrrl, rdl): 0.1104956195, 0.0014199589

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 475
rank avg (pred): 0.339 +- 0.124
mrr vals (pred, true): 0.079, 0.004
batch losses (mrrl, rdl): 0.0081954719, 0.0003095165

Epoch over!
epoch time: 12.804

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.202 +- 0.109
mrr vals (pred, true): 0.150, 0.152
batch losses (mrrl, rdl): 3.42573e-05, 0.0004368447

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.338 +- 0.122
mrr vals (pred, true): 0.071, 0.023
batch losses (mrrl, rdl): 0.0045055319, 0.0001064514

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.316 +- 0.119
mrr vals (pred, true): 0.089, 0.003
batch losses (mrrl, rdl): 0.0153822312, 0.0006185899

Epoch over!
epoch time: 13.404

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.306 +- 0.135
mrr vals (pred, true): 0.105, 0.004
batch losses (mrrl, rdl): 0.029868681, 0.0004986436

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.079 +- 0.041
mrr vals (pred, true): 0.148, 0.027
batch losses (mrrl, rdl): 0.0961718634, 0.000209824

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1177
rank avg (pred): 0.299 +- 0.141
mrr vals (pred, true): 0.128, 0.008
batch losses (mrrl, rdl): 0.0613193773, 4.97726e-05

Epoch over!
epoch time: 13.056

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1001
rank avg (pred): 0.321 +- 0.125
mrr vals (pred, true): 0.089, 0.185
batch losses (mrrl, rdl): 0.0924800187, 0.001073383

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 639
rank avg (pred): 0.337 +- 0.113
mrr vals (pred, true): 0.057, 0.007
batch losses (mrrl, rdl): 0.0004964253, 4.1655e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.321 +- 0.119
mrr vals (pred, true): 0.091, 0.137
batch losses (mrrl, rdl): 0.0214928295, 0.0010335331

Epoch over!
epoch time: 14.385

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.229 +- 0.119
mrr vals (pred, true): 0.147, 0.157

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.08999 	 0.00329 	 m..s
    9 	     1 	 0.09040 	 0.00339 	 m..s
   52 	     2 	 0.10420 	 0.00350 	 MISS
   67 	     3 	 0.10685 	 0.00354 	 MISS
   20 	     4 	 0.09266 	 0.00379 	 m..s
   41 	     5 	 0.10032 	 0.00380 	 m..s
   17 	     6 	 0.09205 	 0.00385 	 m..s
   16 	     7 	 0.09161 	 0.00387 	 m..s
   53 	     8 	 0.10424 	 0.00394 	 MISS
   64 	     9 	 0.10627 	 0.00402 	 MISS
   11 	    10 	 0.09045 	 0.00406 	 m..s
    8 	    11 	 0.09032 	 0.00406 	 m..s
   26 	    12 	 0.09319 	 0.00407 	 m..s
   40 	    13 	 0.10000 	 0.00410 	 m..s
   45 	    14 	 0.10144 	 0.00411 	 m..s
   49 	    15 	 0.10276 	 0.00415 	 m..s
   72 	    16 	 0.10807 	 0.00415 	 MISS
   21 	    17 	 0.09270 	 0.00416 	 m..s
   31 	    18 	 0.09690 	 0.00422 	 m..s
    9 	    19 	 0.09040 	 0.00423 	 m..s
   23 	    20 	 0.09289 	 0.00425 	 m..s
   51 	    21 	 0.10419 	 0.00427 	 m..s
   72 	    22 	 0.10807 	 0.00427 	 MISS
   68 	    23 	 0.10714 	 0.00428 	 MISS
   62 	    24 	 0.10532 	 0.00433 	 MISS
   30 	    25 	 0.09607 	 0.00437 	 m..s
   66 	    26 	 0.10668 	 0.00445 	 MISS
   64 	    27 	 0.10627 	 0.00458 	 MISS
    2 	    28 	 0.08831 	 0.00465 	 m..s
   75 	    29 	 0.11262 	 0.00468 	 MISS
   35 	    30 	 0.09825 	 0.00485 	 m..s
   71 	    31 	 0.10760 	 0.00487 	 MISS
   33 	    32 	 0.09762 	 0.00490 	 m..s
   48 	    33 	 0.10229 	 0.00508 	 m..s
    3 	    34 	 0.08853 	 0.00643 	 m..s
    7 	    35 	 0.09026 	 0.00672 	 m..s
    4 	    36 	 0.08857 	 0.00696 	 m..s
   54 	    37 	 0.10429 	 0.00772 	 m..s
    0 	    38 	 0.08631 	 0.00943 	 m..s
    5 	    39 	 0.08892 	 0.00967 	 m..s
   70 	    40 	 0.10758 	 0.00999 	 m..s
   25 	    41 	 0.09308 	 0.01021 	 m..s
   19 	    42 	 0.09252 	 0.01044 	 m..s
   63 	    43 	 0.10592 	 0.01204 	 m..s
   99 	    44 	 0.15789 	 0.01310 	 MISS
   86 	    45 	 0.14771 	 0.01790 	 MISS
   96 	    46 	 0.15481 	 0.01807 	 MISS
   29 	    47 	 0.09522 	 0.01879 	 m..s
   28 	    48 	 0.09366 	 0.01997 	 m..s
   97 	    49 	 0.15491 	 0.02142 	 MISS
  115 	    50 	 0.16981 	 0.02369 	 MISS
  111 	    51 	 0.16704 	 0.02423 	 MISS
  108 	    52 	 0.16682 	 0.02608 	 MISS
  117 	    53 	 0.18309 	 0.02667 	 MISS
   93 	    54 	 0.15419 	 0.02798 	 MISS
  106 	    55 	 0.16576 	 0.02798 	 MISS
   76 	    56 	 0.14516 	 0.03683 	 MISS
   44 	    57 	 0.10116 	 0.03749 	 m..s
   80 	    58 	 0.14604 	 0.03870 	 MISS
   47 	    59 	 0.10189 	 0.04243 	 m..s
   93 	    60 	 0.15419 	 0.11143 	 m..s
   95 	    61 	 0.15419 	 0.11238 	 m..s
   13 	    62 	 0.09084 	 0.12923 	 m..s
   79 	    63 	 0.14587 	 0.12969 	 ~...
   78 	    64 	 0.14561 	 0.13270 	 ~...
   76 	    65 	 0.14516 	 0.13483 	 ~...
   85 	    66 	 0.14727 	 0.13529 	 ~...
   12 	    67 	 0.09083 	 0.13899 	 m..s
    1 	    68 	 0.08794 	 0.14401 	 m..s
   86 	    69 	 0.14771 	 0.14500 	 ~...
   38 	    70 	 0.09881 	 0.14746 	 m..s
   24 	    71 	 0.09304 	 0.14797 	 m..s
   82 	    72 	 0.14640 	 0.14854 	 ~...
   83 	    73 	 0.14645 	 0.15133 	 ~...
   80 	    74 	 0.14604 	 0.15323 	 ~...
   36 	    75 	 0.09865 	 0.15609 	 m..s
   84 	    76 	 0.14679 	 0.15715 	 ~...
   57 	    77 	 0.10485 	 0.15765 	 m..s
   46 	    78 	 0.10164 	 0.15918 	 m..s
   27 	    79 	 0.09326 	 0.15954 	 m..s
   14 	    80 	 0.09088 	 0.16042 	 m..s
   42 	    81 	 0.10086 	 0.16158 	 m..s
   15 	    82 	 0.09153 	 0.16167 	 m..s
   55 	    83 	 0.10478 	 0.16306 	 m..s
   55 	    84 	 0.10478 	 0.16467 	 m..s
   18 	    85 	 0.09251 	 0.16489 	 m..s
   69 	    86 	 0.10745 	 0.16565 	 m..s
   91 	    87 	 0.15104 	 0.16630 	 ~...
   22 	    88 	 0.09282 	 0.16650 	 m..s
   98 	    89 	 0.15787 	 0.16779 	 ~...
   37 	    90 	 0.09876 	 0.17107 	 m..s
   60 	    91 	 0.10512 	 0.17129 	 m..s
   32 	    92 	 0.09746 	 0.17175 	 m..s
   50 	    93 	 0.10340 	 0.17516 	 m..s
   34 	    94 	 0.09817 	 0.17727 	 m..s
   61 	    95 	 0.10519 	 0.17829 	 m..s
   88 	    96 	 0.14945 	 0.17866 	 ~...
   92 	    97 	 0.15285 	 0.17869 	 ~...
   74 	    98 	 0.10846 	 0.18067 	 m..s
   59 	    99 	 0.10491 	 0.18122 	 m..s
   43 	   100 	 0.10095 	 0.18552 	 m..s
  102 	   101 	 0.16552 	 0.18871 	 ~...
   39 	   102 	 0.09933 	 0.19017 	 m..s
  103 	   103 	 0.16554 	 0.19110 	 ~...
   90 	   104 	 0.14955 	 0.19379 	 m..s
   88 	   105 	 0.14945 	 0.19639 	 m..s
  113 	   106 	 0.16714 	 0.20334 	 m..s
  103 	   107 	 0.16554 	 0.20455 	 m..s
  100 	   108 	 0.16426 	 0.20636 	 m..s
  101 	   109 	 0.16549 	 0.20774 	 m..s
  110 	   110 	 0.16702 	 0.20850 	 m..s
  109 	   111 	 0.16685 	 0.21060 	 m..s
  114 	   112 	 0.16715 	 0.21220 	 m..s
   58 	   113 	 0.10487 	 0.21423 	 MISS
  106 	   114 	 0.16576 	 0.23096 	 m..s
  119 	   115 	 0.18724 	 0.23160 	 m..s
  105 	   116 	 0.16563 	 0.23835 	 m..s
  112 	   117 	 0.16704 	 0.24709 	 m..s
  118 	   118 	 0.18510 	 0.26223 	 m..s
  116 	   119 	 0.18304 	 0.26512 	 m..s
  120 	   120 	 0.18946 	 0.26979 	 m..s
==========================================
r_mrr = 0.47377461194992065
r2_mrr = 0.11817789077758789
spearmanr_mrr@5 = 0.9038974642753601
spearmanr_mrr@10 = 0.9299733638763428
spearmanr_mrr@50 = 0.8484422564506531
spearmanr_mrr@100 = 0.9040039777755737
spearmanr_mrr@All = 0.924630880355835
==========================================
test time: 0.532
Done Testing dataset CoDExSmall
total time taken: 205.3663890361786
training time taken: 195.89825201034546
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.4738)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.1182)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9039)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9300)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.8484)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.9040)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.9246)}}, 'test_loss': {'TransE': {'CoDExSmall': 4.564747539814562}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 2733616111716865
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [903, 328, 1167, 1212, 540, 1022, 856, 1148, 911, 593, 904, 377, 984, 228, 305, 281, 423, 436, 545, 162, 409, 268, 1024, 958, 349, 836, 577, 65, 772, 1197, 400, 1161, 70, 846, 831, 1187, 462, 1067, 266, 499, 914, 1119, 805, 1004, 670, 1068, 589, 101, 672, 467, 827, 79, 700, 699, 418, 659, 693, 171, 1056, 717, 394, 466, 1175, 117, 718, 665, 148, 1147, 351, 439, 1177, 966, 433, 724, 707, 196, 876, 848, 775, 762, 896, 592, 1214, 123, 729, 661, 306, 238, 89, 734, 746, 668, 711, 226, 515, 310, 100, 785, 1211, 821, 907, 562, 802, 921, 617, 404, 1111, 141, 820, 71, 233, 736, 833, 694, 360, 789, 814, 615, 382, 142, 760]
valid_ids (0): []
train_ids (1094): [894, 739, 526, 732, 764, 556, 154, 1210, 580, 634, 538, 504, 383, 235, 29, 946, 458, 867, 631, 1162, 863, 222, 1114, 888, 362, 1174, 189, 850, 774, 223, 509, 293, 1133, 2, 949, 627, 242, 740, 1059, 520, 583, 187, 103, 939, 152, 883, 697, 1087, 131, 837, 1052, 1120, 1095, 389, 77, 422, 1163, 183, 744, 195, 243, 737, 380, 301, 962, 988, 1014, 150, 960, 190, 209, 352, 551, 490, 109, 120, 172, 408, 289, 261, 182, 929, 318, 658, 572, 1008, 258, 374, 844, 241, 447, 630, 1016, 376, 1077, 229, 806, 657, 719, 927, 208, 449, 86, 517, 157, 344, 868, 399, 345, 1091, 1100, 249, 1176, 60, 675, 1043, 444, 877, 265, 855, 508, 749, 484, 1060, 88, 95, 923, 287, 59, 563, 935, 616, 1019, 225, 678, 1080, 521, 933, 144, 406, 568, 1153, 192, 854, 692, 781, 481, 1013, 165, 275, 290, 129, 721, 731, 518, 198, 1191, 1116, 1122, 548, 555, 413, 31, 512, 75, 626, 849, 606, 424, 82, 671, 611, 158, 941, 928, 695, 69, 917, 1118, 149, 940, 603, 1128, 432, 1126, 346, 943, 535, 981, 197, 461, 838, 994, 255, 800, 498, 1127, 354, 456, 916, 956, 1033, 708, 893, 1018, 1020, 1149, 401, 500, 507, 1173, 777, 277, 194, 751, 472, 353, 78, 703, 825, 14, 459, 1090, 973, 15, 899, 1089, 359, 111, 106, 664, 1182, 476, 696, 793, 451, 487, 858, 188, 704, 370, 163, 272, 743, 332, 1107, 81, 145, 473, 912, 479, 337, 440, 236, 677, 442, 944, 385, 340, 167, 19, 263, 922, 1085, 1048, 529, 674, 248, 130, 652, 17, 810, 1146, 395, 1104, 201, 125, 232, 537, 35, 990, 987, 1097, 710, 480, 327, 90, 1134, 219, 974, 733, 202, 807, 861, 181, 553, 477, 783, 947, 860, 112, 56, 1179, 654, 215, 685, 558, 1155, 586, 522, 726, 1201, 778, 748, 1037, 333, 516, 794, 124, 533, 722, 115, 690, 1154, 214, 283, 673, 906, 823, 393, 87, 411, 788, 497, 121, 930, 813, 792, 1203, 1072, 396, 51, 1031, 667, 909, 895, 811, 330, 107, 637, 884, 683, 1124, 342, 495, 104, 632, 527, 879, 567, 1098, 1140, 1054, 1075, 299, 590, 768, 308, 887, 1010, 218, 1049, 460, 367, 581, 1164, 605, 742, 761, 598, 464, 254, 375, 262, 766, 1145, 386, 1115, 513, 528, 231, 1135, 392, 865, 828, 1186, 660, 96, 274, 534, 983, 1021, 321, 547, 278, 1199, 804, 1026, 93, 797, 1006, 217, 482, 1017, 246, 1045, 1032, 784, 997, 882, 795, 334, 485, 641, 253, 610, 763, 63, 1074, 878, 76, 478, 1058, 584, 607, 859, 84, 869, 1168, 791, 118, 434, 519, 44, 381, 184, 213, 453, 622, 443, 55, 206, 651, 138, 292, 315, 959, 576, 320, 506, 132, 1151, 998, 1169, 986, 28, 525, 952, 669, 613, 957, 801, 852, 969, 752, 323, 829, 1065, 716, 220, 560, 276, 224, 474, 1166, 866, 91, 738, 595, 798, 297, 625, 180, 816, 938, 350, 999, 1159, 176, 684, 635, 174, 643, 1180, 1012, 1213, 1188, 809, 566, 133, 578, 1025, 343, 357, 1200, 597, 1108, 951, 99, 1204, 273, 483, 647, 1183, 623, 339, 361, 531, 412, 552, 68, 715, 62, 1202, 614, 758, 620, 1207, 680, 417, 10, 491, 698, 1195, 1123, 1030, 720, 913, 264, 295, 296, 1005, 179, 471, 54, 1046, 812, 369, 757, 32, 853, 1142, 542, 842, 146, 11, 286, 1079, 5, 961, 559, 1205, 965, 322, 1157, 1036, 972, 116, 1064, 985, 329, 702, 16, 221, 20, 1047, 771, 937, 830, 1138, 953, 1081, 178, 366, 1156, 435, 74, 18, 80, 39, 1063, 786, 257, 834, 430, 618, 250, 1039, 514, 288, 1112, 968, 505, 587, 872, 588, 915, 964, 431, 502, 714, 730, 799, 845, 390, 139, 204, 324, 1040, 511, 1196, 437, 642, 1035, 1160, 1184, 582, 363, 410, 532, 624, 503, 463, 325, 1076, 1129, 832, 835, 1034, 12, 446, 136, 256, 92, 151, 608, 676, 628, 317, 1002, 991, 1071, 919, 1105, 1069, 979, 585, 926, 457, 862, 523, 319, 338, 892, 594, 307, 57, 1093, 689, 1178, 279, 127, 1094, 621, 452, 114, 554, 425, 205, 160, 967, 83, 1062, 314, 619, 175, 638, 1132, 741, 280, 686, 387, 488, 169, 874, 420, 186, 230, 216, 924, 1101, 97, 438, 298, 122, 1209, 415, 723, 779, 609, 826, 153, 602, 954, 7, 364, 493, 687, 22, 161, 1165, 108, 889, 137, 1103, 536, 428, 948, 128, 931, 355, 384, 403, 237, 767, 1096, 85, 126, 379, 311, 787, 1029, 113, 886, 701, 942, 147, 441, 640, 600, 1110, 1003, 421, 549, 977, 770, 429, 46, 1023, 414, 819, 891, 378, 285, 870, 890, 1001, 776, 207, 469, 170, 840, 570, 747, 24, 98, 655, 579, 544, 902, 416, 918, 191, 901, 227, 936, 43, 1027, 454, 135, 291, 1137, 755, 1057, 1131, 662, 681, 982, 27, 591, 1051, 1009, 765, 302, 539, 405, 688, 1102, 489, 571, 569, 1136, 240, 1092, 910, 419, 709, 336, 300, 1088, 140, 851, 159, 1144, 313, 155, 649, 309, 1143, 134, 4, 934, 388, 975, 843, 30, 564, 575, 372, 646, 864, 546, 365, 871, 976, 1189, 282, 36, 727, 759, 105, 166, 45, 49, 1208, 873, 326, 510, 269, 725, 808, 34, 629, 1050, 37, 199, 1171, 407, 25, 494, 244, 21, 267, 348, 782, 356, 648, 1139, 251, 398, 465, 335, 650, 177, 978, 596, 885, 880, 501, 604, 847, 745, 612, 212, 818, 682, 304, 64, 245, 1078, 73, 200, 1172, 1194, 1028, 1185, 541, 817, 550, 316, 1117, 1130, 397, 955, 1121, 119, 1158, 271, 496, 1125, 557, 193, 6, 492, 42, 284, 639, 203, 897, 402, 920, 996, 824, 644, 445, 1, 66, 1055, 992, 713, 1099, 530, 450, 756, 347, 803, 1082, 260, 211, 391, 1073, 524, 1000, 875, 185, 995, 455, 259, 633, 656, 574, 50, 40, 1193, 247, 239, 773, 769, 67, 963, 1198, 1041, 728, 303, 565, 470, 796, 753, 1113, 23, 1141, 1152, 705, 371, 58, 1044, 48, 341, 1061, 426, 102, 358, 754, 38, 294, 599, 679, 706, 573, 252, 898, 841, 1192, 168, 1038, 9, 94, 691, 61, 1011, 905, 636, 857, 822, 1042, 486, 475, 663, 3, 210, 110, 52, 815, 1181, 945, 925, 8, 1066, 1150, 989, 173, 1109, 331, 645, 950, 653, 543, 735, 26, 270, 72, 0, 234, 1206, 750, 427, 881, 143, 993, 1015, 1053, 312, 1170, 1086, 790, 971, 900, 468, 47, 712, 164, 780, 1084, 839, 53, 1007, 448, 561, 970, 373, 666, 156, 368, 33, 1083, 601, 908, 1070, 1106, 41, 932, 980, 13, 1190]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3307245663304244
the save name prefix for this run is:  chkpt-ID_3307245663304244_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 461
rank avg (pred): 0.492 +- 0.004
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001073067

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 625
rank avg (pred): 0.261 +- 0.142
mrr vals (pred, true): 0.034, 0.010
batch losses (mrrl, rdl): 0.0, 0.0001122514

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.178 +- 0.153
mrr vals (pred, true): 0.216, 0.114
batch losses (mrrl, rdl): 0.0, 8.97322e-05

Epoch over!
epoch time: 13.522

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 33
rank avg (pred): 0.204 +- 0.180
mrr vals (pred, true): 0.230, 0.142
batch losses (mrrl, rdl): 0.0, 0.0005078297

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 784
rank avg (pred): 0.197 +- 0.190
mrr vals (pred, true): 0.325, 0.004
batch losses (mrrl, rdl): 0.0, 0.0011344565

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 658
rank avg (pred): 0.224 +- 0.251
mrr vals (pred, true): 0.426, 0.004
batch losses (mrrl, rdl): 0.0, 0.0008157944

Epoch over!
epoch time: 13.645

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 36
rank avg (pred): 0.190 +- 0.202
mrr vals (pred, true): 0.401, 0.160
batch losses (mrrl, rdl): 0.0, 0.0005113534

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.272 +- 0.300
mrr vals (pred, true): 0.432, 0.027
batch losses (mrrl, rdl): 0.0, 0.0004283576

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 172
rank avg (pred): 0.171 +- 0.201
mrr vals (pred, true): 0.469, 0.003
batch losses (mrrl, rdl): 0.0, 0.001389996

Epoch over!
epoch time: 12.687

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 749
rank avg (pred): 0.207 +- 0.231
mrr vals (pred, true): 0.426, 0.139
batch losses (mrrl, rdl): 0.0, 0.0005479008

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 219
rank avg (pred): 0.188 +- 0.221
mrr vals (pred, true): 0.463, 0.004
batch losses (mrrl, rdl): 0.0, 0.0011916077

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 218
rank avg (pred): 0.177 +- 0.215
mrr vals (pred, true): 0.455, 0.005
batch losses (mrrl, rdl): 0.0, 0.0011869802

Epoch over!
epoch time: 14.804

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 761
rank avg (pred): 0.209 +- 0.227
mrr vals (pred, true): 0.318, 0.157
batch losses (mrrl, rdl): 0.0, 0.0003819211

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 340
rank avg (pred): 0.214 +- 0.236
mrr vals (pred, true): 0.362, 0.149
batch losses (mrrl, rdl): 0.0, 0.0001926628

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 753
rank avg (pred): 0.188 +- 0.224
mrr vals (pred, true): 0.410, 0.188
batch losses (mrrl, rdl): 0.0, 0.000631832

Epoch over!
epoch time: 14.39

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 750
rank avg (pred): 0.196 +- 0.225
mrr vals (pred, true): 0.358, 0.145
batch losses (mrrl, rdl): 0.4531276226, 0.0005768731

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 683
rank avg (pred): 0.410 +- 0.168
mrr vals (pred, true): 0.088, 0.003
batch losses (mrrl, rdl): 0.0141178835, 9.86107e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 252
rank avg (pred): 0.119 +- 0.094
mrr vals (pred, true): 0.147, 0.189
batch losses (mrrl, rdl): 0.0175106842, 8.43684e-05

Epoch over!
epoch time: 13.841

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 325
rank avg (pred): 0.109 +- 0.085
mrr vals (pred, true): 0.143, 0.147
batch losses (mrrl, rdl): 0.000153669, 7.41586e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 429
rank avg (pred): 0.129 +- 0.090
mrr vals (pred, true): 0.140, 0.005
batch losses (mrrl, rdl): 0.0802166089, 0.0022695819

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 571
rank avg (pred): 0.397 +- 0.131
mrr vals (pred, true): 0.053, 0.011
batch losses (mrrl, rdl): 9.56739e-05, 0.0001635914

Epoch over!
epoch time: 14.278

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 212
rank avg (pred): 0.127 +- 0.081
mrr vals (pred, true): 0.144, 0.005
batch losses (mrrl, rdl): 0.0876326486, 0.002527243

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 732
rank avg (pred): 0.241 +- 0.147
mrr vals (pred, true): 0.137, 0.154
batch losses (mrrl, rdl): 0.0030999868, 0.0008241192

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 771
rank avg (pred): 0.262 +- 0.143
mrr vals (pred, true): 0.124, 0.182
batch losses (mrrl, rdl): 0.0331418887, 0.0007653896

Epoch over!
epoch time: 13.365

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.081 +- 0.048
mrr vals (pred, true): 0.147, 0.004
batch losses (mrrl, rdl): 0.093203783, 0.0028695562

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 317
rank avg (pred): 0.048 +- 0.029
mrr vals (pred, true): 0.155, 0.273
batch losses (mrrl, rdl): 0.1397973597, 3.2359e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1089
rank avg (pred): 0.077 +- 0.044
mrr vals (pred, true): 0.149, 0.178
batch losses (mrrl, rdl): 0.008122962, 7.78808e-05

Epoch over!
epoch time: 12.812

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1115
rank avg (pred): 0.070 +- 0.040
mrr vals (pred, true): 0.143, 0.004
batch losses (mrrl, rdl): 0.0857222155, 0.0030224929

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 776
rank avg (pred): 0.274 +- 0.147
mrr vals (pred, true): 0.130, 0.157
batch losses (mrrl, rdl): 0.007034787, 0.0006970108

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 245
rank avg (pred): 0.139 +- 0.072
mrr vals (pred, true): 0.130, 0.195
batch losses (mrrl, rdl): 0.0430098586, 0.000144433

Epoch over!
epoch time: 13.815

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1076
rank avg (pred): 0.084 +- 0.044
mrr vals (pred, true): 0.137, 0.234
batch losses (mrrl, rdl): 0.0956423432, 3.20885e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 569
rank avg (pred): 0.341 +- 0.102
mrr vals (pred, true): 0.054, 0.022
batch losses (mrrl, rdl): 0.000151678, 0.000103569

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.299 +- 0.144
mrr vals (pred, true): 0.116, 0.004
batch losses (mrrl, rdl): 0.0434969589, 0.000522426

Epoch over!
epoch time: 13.603

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 632
rank avg (pred): 0.342 +- 0.107
mrr vals (pred, true): 0.051, 0.021
batch losses (mrrl, rdl): 1.33053e-05, 0.0001040143

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1082
rank avg (pred): 0.067 +- 0.039
mrr vals (pred, true): 0.152, 0.177
batch losses (mrrl, rdl): 0.0063186763, 9.6511e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 115
rank avg (pred): 0.286 +- 0.141
mrr vals (pred, true): 0.133, 0.131
batch losses (mrrl, rdl): 4.14144e-05, 0.0006626804

Epoch over!
epoch time: 14.072

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 374
rank avg (pred): 0.052 +- 0.026
mrr vals (pred, true): 0.143, 0.197
batch losses (mrrl, rdl): 0.0283951536, 7.51782e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.329 +- 0.094
mrr vals (pred, true): 0.051, 0.024
batch losses (mrrl, rdl): 1.97015e-05, 0.0004039762

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 137
rank avg (pred): 0.093 +- 0.048
mrr vals (pred, true): 0.139, 0.164
batch losses (mrrl, rdl): 0.0064006192, 1.72224e-05

Epoch over!
epoch time: 13.256

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.272 +- 0.136
mrr vals (pred, true): 0.123, 0.005
batch losses (mrrl, rdl): 0.0526992157, 0.0007257794

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.277 +- 0.134
mrr vals (pred, true): 0.126, 0.112
batch losses (mrrl, rdl): 0.0017264395, 0.0003198668

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 792
rank avg (pred): 0.268 +- 0.132
mrr vals (pred, true): 0.129, 0.004
batch losses (mrrl, rdl): 0.0629514009, 0.000739416

Epoch over!
epoch time: 12.813

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.122 +- 0.062
mrr vals (pred, true): 0.141, 0.215
batch losses (mrrl, rdl): 0.0554778948, 0.0001123839

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 338
rank avg (pred): 0.070 +- 0.035
mrr vals (pred, true): 0.149, 0.181
batch losses (mrrl, rdl): 0.009980524, 7.87566e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 830
rank avg (pred): 0.281 +- 0.129
mrr vals (pred, true): 0.111, 0.118
batch losses (mrrl, rdl): 0.0005543424, 0.000549514

Epoch over!
epoch time: 12.674

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.292 +- 0.120
mrr vals (pred, true): 0.106, 0.129

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   14 	     0 	 0.05576 	 0.00286 	 m..s
   24 	     1 	 0.05844 	 0.00290 	 m..s
   16 	     2 	 0.05643 	 0.00312 	 m..s
   31 	     3 	 0.05973 	 0.00330 	 m..s
   15 	     4 	 0.05596 	 0.00338 	 m..s
   27 	     5 	 0.05914 	 0.00339 	 m..s
   17 	     6 	 0.05736 	 0.00347 	 m..s
    2 	     7 	 0.04811 	 0.00348 	 m..s
    6 	     8 	 0.04984 	 0.00350 	 m..s
   91 	     9 	 0.13102 	 0.00352 	 MISS
    3 	    10 	 0.04921 	 0.00356 	 m..s
  105 	    11 	 0.14321 	 0.00360 	 MISS
   22 	    12 	 0.05795 	 0.00361 	 m..s
   29 	    13 	 0.05921 	 0.00362 	 m..s
   13 	    14 	 0.05528 	 0.00364 	 m..s
   90 	    15 	 0.13102 	 0.00366 	 MISS
    8 	    16 	 0.05018 	 0.00373 	 m..s
   55 	    17 	 0.12398 	 0.00376 	 MISS
   75 	    18 	 0.12435 	 0.00378 	 MISS
   36 	    19 	 0.06234 	 0.00383 	 m..s
   84 	    20 	 0.13056 	 0.00384 	 MISS
   83 	    21 	 0.12801 	 0.00385 	 MISS
   47 	    22 	 0.11354 	 0.00386 	 MISS
    1 	    23 	 0.04795 	 0.00389 	 m..s
   35 	    24 	 0.06200 	 0.00399 	 m..s
   79 	    25 	 0.12535 	 0.00400 	 MISS
   97 	    26 	 0.13625 	 0.00414 	 MISS
   87 	    27 	 0.13077 	 0.00415 	 MISS
   71 	    28 	 0.12419 	 0.00419 	 MISS
  110 	    29 	 0.14471 	 0.00420 	 MISS
   10 	    30 	 0.05151 	 0.00421 	 m..s
   86 	    31 	 0.13069 	 0.00422 	 MISS
   93 	    32 	 0.13203 	 0.00426 	 MISS
   52 	    33 	 0.12128 	 0.00427 	 MISS
   96 	    34 	 0.13339 	 0.00451 	 MISS
   48 	    35 	 0.11414 	 0.00453 	 MISS
  108 	    36 	 0.14367 	 0.00454 	 MISS
   55 	    37 	 0.12398 	 0.00466 	 MISS
   19 	    38 	 0.05746 	 0.00472 	 m..s
   45 	    39 	 0.10604 	 0.00488 	 MISS
   50 	    40 	 0.11754 	 0.00493 	 MISS
   42 	    41 	 0.10517 	 0.00497 	 MISS
   73 	    42 	 0.12430 	 0.00516 	 MISS
  113 	    43 	 0.14487 	 0.00519 	 MISS
   21 	    44 	 0.05776 	 0.00540 	 m..s
   34 	    45 	 0.06160 	 0.00809 	 m..s
   12 	    46 	 0.05347 	 0.00942 	 m..s
   11 	    47 	 0.05332 	 0.00973 	 m..s
    0 	    48 	 0.04766 	 0.01043 	 m..s
    9 	    49 	 0.05038 	 0.01083 	 m..s
   31 	    50 	 0.05973 	 0.01098 	 m..s
    3 	    51 	 0.04921 	 0.01105 	 m..s
   25 	    52 	 0.05912 	 0.01162 	 m..s
    6 	    53 	 0.04984 	 0.01683 	 m..s
    5 	    54 	 0.04944 	 0.01879 	 m..s
   30 	    55 	 0.05927 	 0.01896 	 m..s
   17 	    56 	 0.05736 	 0.01924 	 m..s
   28 	    57 	 0.05919 	 0.02257 	 m..s
   25 	    58 	 0.05912 	 0.02394 	 m..s
   33 	    59 	 0.06119 	 0.02559 	 m..s
   19 	    60 	 0.05746 	 0.03512 	 ~...
   23 	    61 	 0.05800 	 0.03662 	 ~...
   37 	    62 	 0.09473 	 0.03870 	 m..s
   71 	    63 	 0.12419 	 0.04243 	 m..s
   70 	    64 	 0.12410 	 0.05106 	 m..s
   41 	    65 	 0.10451 	 0.07318 	 m..s
   46 	    66 	 0.11353 	 0.09814 	 ~...
   51 	    67 	 0.11865 	 0.10834 	 ~...
   49 	    68 	 0.11430 	 0.10873 	 ~...
   55 	    69 	 0.12398 	 0.11691 	 ~...
   78 	    70 	 0.12528 	 0.12016 	 ~...
   42 	    71 	 0.10517 	 0.12191 	 ~...
   55 	    72 	 0.12398 	 0.12241 	 ~...
   40 	    73 	 0.10320 	 0.12840 	 ~...
   44 	    74 	 0.10594 	 0.12862 	 ~...
   53 	    75 	 0.12133 	 0.12923 	 ~...
   80 	    76 	 0.12651 	 0.13404 	 ~...
   55 	    77 	 0.12398 	 0.13483 	 ~...
   89 	    78 	 0.13082 	 0.13693 	 ~...
   84 	    79 	 0.13056 	 0.14401 	 ~...
   82 	    80 	 0.12664 	 0.14465 	 ~...
   39 	    81 	 0.10307 	 0.14854 	 m..s
   88 	    82 	 0.13078 	 0.14963 	 ~...
   55 	    83 	 0.12398 	 0.15215 	 ~...
   55 	    84 	 0.12398 	 0.15392 	 ~...
   55 	    85 	 0.12398 	 0.15561 	 m..s
   55 	    86 	 0.12398 	 0.15715 	 m..s
   55 	    87 	 0.12398 	 0.15806 	 m..s
   55 	    88 	 0.12398 	 0.15816 	 m..s
   38 	    89 	 0.09716 	 0.15918 	 m..s
   95 	    90 	 0.13251 	 0.16042 	 ~...
   98 	    91 	 0.13686 	 0.16167 	 ~...
   73 	    92 	 0.12430 	 0.16294 	 m..s
   55 	    93 	 0.12398 	 0.16373 	 m..s
   55 	    94 	 0.12398 	 0.16467 	 m..s
  101 	    95 	 0.14046 	 0.16573 	 ~...
  100 	    96 	 0.14043 	 0.17104 	 m..s
  115 	    97 	 0.15328 	 0.17175 	 ~...
   76 	    98 	 0.12437 	 0.18399 	 m..s
  119 	    99 	 0.15899 	 0.18552 	 ~...
   77 	   100 	 0.12464 	 0.18871 	 m..s
  120 	   101 	 0.15899 	 0.19002 	 m..s
   69 	   102 	 0.12406 	 0.19419 	 m..s
   68 	   103 	 0.12406 	 0.19424 	 m..s
  104 	   104 	 0.14312 	 0.19925 	 m..s
   81 	   105 	 0.12651 	 0.20029 	 m..s
   99 	   106 	 0.13930 	 0.20081 	 m..s
  110 	   107 	 0.14471 	 0.20365 	 m..s
   54 	   108 	 0.12284 	 0.20832 	 m..s
   92 	   109 	 0.13137 	 0.20874 	 m..s
  116 	   110 	 0.15484 	 0.21630 	 m..s
   94 	   111 	 0.13225 	 0.22028 	 m..s
  103 	   112 	 0.14278 	 0.22861 	 m..s
  105 	   113 	 0.14321 	 0.22900 	 m..s
  109 	   114 	 0.14370 	 0.23291 	 m..s
  112 	   115 	 0.14473 	 0.23807 	 m..s
  114 	   116 	 0.14628 	 0.24193 	 m..s
  101 	   117 	 0.14046 	 0.24444 	 MISS
  107 	   118 	 0.14324 	 0.24580 	 MISS
  117 	   119 	 0.15707 	 0.25687 	 m..s
  118 	   120 	 0.15808 	 0.26512 	 MISS
==========================================
r_mrr = 0.636978268623352
r2_mrr = 0.29309046268463135
spearmanr_mrr@5 = 0.7899551391601562
spearmanr_mrr@10 = 0.9039245843887329
spearmanr_mrr@50 = 0.9841400384902954
spearmanr_mrr@100 = 0.7939808964729309
spearmanr_mrr@All = 0.8142509460449219
==========================================
test time: 0.562
Done Testing dataset CoDExSmall
total time taken: 215.47443199157715
training time taken: 204.21352744102478
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.6370)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.2931)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.7900)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9039)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9841)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.7940)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8143)}}, 'test_loss': {'TransE': {'CoDExSmall': 3.481672078734846}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 2660138701403651
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [881, 274, 983, 1157, 1150, 555, 1015, 791, 419, 719, 757, 921, 36, 364, 1188, 941, 558, 901, 198, 832, 696, 40, 61, 281, 1194, 59, 739, 378, 53, 607, 1139, 814, 675, 984, 353, 928, 1039, 255, 460, 846, 43, 362, 903, 46, 210, 375, 957, 1169, 544, 823, 310, 371, 651, 193, 27, 864, 501, 120, 751, 1115, 89, 792, 550, 882, 621, 899, 926, 620, 736, 880, 775, 594, 730, 166, 110, 496, 291, 139, 1027, 1108, 1103, 160, 799, 724, 701, 737, 654, 249, 1081, 233, 269, 156, 12, 435, 1098, 686, 441, 179, 1154, 1023, 1173, 996, 294, 908, 973, 282, 968, 1145, 333, 532, 706, 712, 655, 127, 841, 1160, 162, 184, 1, 749, 222]
valid_ids (0): []
train_ids (1094): [617, 334, 562, 1105, 1043, 844, 1159, 138, 1198, 509, 429, 24, 994, 224, 497, 627, 998, 1199, 257, 833, 802, 1085, 1165, 785, 566, 180, 860, 952, 1028, 22, 384, 403, 443, 695, 35, 810, 761, 200, 936, 77, 723, 411, 287, 347, 631, 482, 766, 100, 277, 672, 1211, 163, 355, 689, 504, 146, 1013, 48, 381, 315, 207, 949, 1017, 18, 1210, 1084, 23, 1118, 454, 769, 214, 1053, 679, 1111, 746, 389, 143, 392, 405, 913, 907, 358, 1186, 455, 918, 56, 342, 728, 506, 1022, 537, 1079, 895, 643, 1185, 244, 794, 1060, 388, 929, 866, 1051, 336, 352, 426, 1163, 106, 545, 820, 540, 524, 1208, 438, 1152, 767, 440, 58, 990, 759, 923, 155, 965, 800, 979, 126, 173, 1175, 569, 889, 1137, 112, 1004, 1126, 638, 174, 124, 644, 21, 565, 1038, 781, 424, 851, 930, 363, 39, 859, 343, 900, 571, 1046, 745, 650, 945, 665, 1029, 452, 253, 1148, 1113, 189, 959, 597, 60, 987, 1156, 290, 1026, 408, 1136, 326, 483, 891, 836, 232, 57, 539, 639, 31, 771, 1166, 503, 584, 1068, 177, 83, 659, 365, 312, 956, 523, 190, 788, 87, 19, 946, 511, 1037, 1178, 436, 932, 1020, 669, 181, 54, 783, 1000, 938, 579, 1155, 768, 196, 811, 288, 718, 185, 71, 286, 1171, 1146, 492, 714, 611, 962, 250, 808, 25, 741, 300, 1124, 670, 176, 1003, 1087, 916, 893, 47, 314, 321, 301, 734, 81, 777, 444, 434, 246, 868, 495, 848, 619, 13, 374, 450, 839, 733, 103, 20, 773, 822, 380, 711, 824, 1168, 512, 266, 637, 464, 1086, 15, 1007, 1102, 829, 385, 320, 349, 552, 653, 95, 323, 298, 105, 753, 69, 813, 704, 580, 1054, 1089, 499, 311, 1128, 152, 633, 459, 489, 169, 886, 217, 357, 1011, 747, 468, 226, 593, 279, 414, 939, 821, 570, 449, 165, 1164, 313, 517, 150, 977, 806, 26, 239, 604, 616, 234, 2, 416, 534, 585, 850, 219, 225, 588, 835, 1096, 167, 758, 583, 168, 568, 80, 855, 1106, 694, 284, 1042, 73, 919, 33, 367, 231, 656, 115, 1073, 340, 394, 1205, 433, 681, 406, 682, 360, 243, 680, 674, 911, 431, 1138, 985, 954, 151, 238, 904, 386, 1062, 710, 1200, 1101, 1070, 809, 914, 327, 272, 790, 1192, 1177, 116, 400, 215, 251, 328, 37, 415, 119, 576, 74, 410, 157, 4, 636, 687, 975, 465, 445, 1063, 1083, 1076, 1010, 149, 1024, 1197, 1190, 472, 346, 123, 974, 992, 595, 413, 471, 1080, 581, 887, 425, 778, 700, 667, 154, 872, 108, 283, 756, 892, 807, 1123, 615, 254, 1184, 264, 563, 873, 890, 303, 601, 144, 209, 285, 463, 256, 493, 995, 292, 178, 211, 396, 1117, 986, 182, 359, 270, 484, 79, 553, 705, 731, 1120, 1040, 1045, 518, 796, 235, 366, 933, 609, 755, 856, 409, 1014, 516, 369, 68, 260, 688, 332, 519, 572, 707, 319, 1181, 629, 480, 826, 828, 513, 1100, 600, 951, 420, 692, 554, 671, 625, 1092, 129, 1134, 547, 612, 344, 685, 750, 922, 10, 634, 1041, 339, 1202, 804, 118, 107, 30, 159, 702, 1201, 527, 953, 191, 469, 1167, 812, 218, 462, 838, 780, 722, 556, 86, 764, 302, 915, 248, 491, 421, 842, 652, 1064, 628, 442, 1074, 748, 430, 765, 646, 1077, 978, 970, 699, 1109, 1203, 803, 1075, 658, 894, 883, 293, 567, 502, 220, 797, 991, 7, 927, 1091, 446, 75, 795, 242, 691, 587, 976, 1093, 879, 849, 909, 11, 1110, 478, 265, 397, 370, 1135, 510, 236, 898, 542, 88, 267, 308, 716, 192, 1183, 213, 1095, 330, 819, 17, 521, 221, 131, 676, 94, 316, 278, 1125, 439, 1132, 498, 1180, 863, 546, 295, 522, 1056, 1147, 865, 66, 1078, 1055, 988, 140, 752, 657, 935, 65, 418, 677, 275, 391, 708, 531, 526, 660, 1191, 132, 473, 377, 456, 717, 51, 354, 114, 910, 982, 825, 1033, 206, 1212, 461, 479, 348, 559, 6, 1019, 268, 404, 477, 1172, 997, 96, 869, 774, 372, 194, 437, 1001, 432, 673, 98, 818, 14, 989, 111, 229, 309, 787, 241, 172, 350, 770, 128, 1129, 878, 325, 1072, 1008, 395, 1044, 683, 16, 470, 448, 548, 104, 399, 713, 1195, 912, 137, 383, 661, 62, 720, 276, 917, 195, 740, 582, 613, 3, 1193, 259, 1196, 457, 801, 1158, 1187, 1144, 549, 475, 458, 102, 34, 101, 870, 398, 109, 1067, 738, 1097, 1130, 317, 390, 122, 877, 345, 575, 201, 228, 148, 742, 133, 1149, 170, 204, 950, 1142, 862, 599, 837, 827, 525, 401, 97, 1050, 605, 205, 1127, 1006, 161, 1209, 1104, 164, 368, 42, 338, 63, 1009, 197, 341, 183, 666, 993, 817, 64, 72, 488, 533, 942, 273, 560, 754, 626, 972, 38, 32, 188, 1021, 41, 28, 980, 387, 306, 1088, 529, 793, 1030, 447, 263, 888, 958, 1107, 335, 84, 379, 136, 861, 622, 487, 937, 322, 361, 1025, 948, 896, 782, 125, 564, 1094, 481, 373, 1018, 1161, 141, 91, 29, 67, 635, 964, 1002, 649, 299, 760, 647, 1065, 476, 494, 258, 230, 684, 55, 1189, 514, 624, 763, 905, 925, 171, 1143, 591, 186, 175, 331, 981, 725, 135, 1119, 1213, 1012, 721, 417, 735, 645, 577, 535, 1153, 590, 606, 871, 407, 117, 280, 324, 93, 902, 840, 1214, 697, 1071, 955, 76, 853, 920, 474, 632, 831, 508, 969, 153, 662, 1151, 262, 45, 208, 453, 779, 1049, 1179, 515, 1058, 541, 589, 147, 451, 1057, 610, 1170, 0, 538, 726, 1052, 82, 199, 356, 145, 1112, 428, 351, 247, 121, 943, 237, 1059, 602, 1204, 252, 947, 561, 727, 158, 924, 897, 203, 1061, 1114, 52, 1162, 1016, 240, 1176, 543, 875, 640, 49, 641, 618, 729, 423, 858, 971, 467, 776, 536, 668, 815, 130, 427, 1082, 505, 642, 1034, 92, 961, 297, 1116, 608, 485, 854, 1099, 99, 9, 402, 78, 1174, 960, 5, 586, 663, 678, 134, 289, 592, 216, 847, 70, 304, 834, 296, 393, 1122, 271, 693, 551, 805, 772, 490, 212, 1047, 934, 422, 843, 1207, 857, 967, 318, 113, 466, 1005, 867, 1031, 744, 798, 614, 223, 1066, 1090, 885, 1032, 329, 376, 709, 963, 732, 245, 623, 1206, 578, 698, 966, 1131, 520, 664, 500, 816, 789, 90, 261, 142, 648, 884, 573, 762, 940, 412, 1036, 1069, 382, 598, 715, 931, 85, 703, 830, 845, 337, 307, 507, 1048, 852, 1182, 906, 530, 876, 305, 786, 1140, 8, 630, 1035, 44, 187, 743, 486, 574, 596, 690, 202, 557, 528, 1133, 603, 784, 1121, 874, 999, 50, 227, 1141, 944]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6195565893011902
the save name prefix for this run is:  chkpt-ID_6195565893011902_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 912
rank avg (pred): 0.529 +- 0.004
mrr vals (pred, true): 0.001, 0.142
batch losses (mrrl, rdl): 0.0, 0.0048762723

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 473
rank avg (pred): 0.324 +- 0.213
mrr vals (pred, true): 0.024, 0.005
batch losses (mrrl, rdl): 0.0, 0.0002924812

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 81
rank avg (pred): 0.276 +- 0.254
mrr vals (pred, true): 0.142, 0.100
batch losses (mrrl, rdl): 0.0, 0.0005213618

Epoch over!
epoch time: 13.462

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 687
rank avg (pred): 0.290 +- 0.260
mrr vals (pred, true): 0.142, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003958591

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 432
rank avg (pred): 0.286 +- 0.269
mrr vals (pred, true): 0.193, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003319157

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.070 +- 0.073
mrr vals (pred, true): 0.377, 0.168
batch losses (mrrl, rdl): 0.0, 5.0969e-06

Epoch over!
epoch time: 12.648

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 704
rank avg (pred): 0.303 +- 0.301
mrr vals (pred, true): 0.298, 0.003
batch losses (mrrl, rdl): 0.0, 0.0004460372

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 230
rank avg (pred): 0.253 +- 0.263
mrr vals (pred, true): 0.340, 0.003
batch losses (mrrl, rdl): 0.0, 0.0006513646

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 865
rank avg (pred): 0.244 +- 0.258
mrr vals (pred, true): 0.372, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006944781

Epoch over!
epoch time: 12.753

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1119
rank avg (pred): 0.263 +- 0.269
mrr vals (pred, true): 0.324, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004829315

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.028 +- 0.032
mrr vals (pred, true): 0.474, 0.147
batch losses (mrrl, rdl): 0.0, 1.72367e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.342 +- 0.310
mrr vals (pred, true): 0.198, 0.004
batch losses (mrrl, rdl): 0.0, 0.000190456

Epoch over!
epoch time: 13.469

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 471
rank avg (pred): 0.274 +- 0.273
mrr vals (pred, true): 0.250, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004492762

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 499
rank avg (pred): 0.190 +- 0.207
mrr vals (pred, true): 0.368, 0.017
batch losses (mrrl, rdl): 0.0, 2.8233e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1174
rank avg (pred): 0.356 +- 0.322
mrr vals (pred, true): 0.202, 0.009
batch losses (mrrl, rdl): 0.0, 0.0001167255

Epoch over!
epoch time: 14.39

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 324
rank avg (pred): 0.262 +- 0.270
mrr vals (pred, true): 0.297, 0.116
batch losses (mrrl, rdl): 0.3277010322, 0.0003064903

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 154
rank avg (pred): 0.342 +- 0.198
mrr vals (pred, true): 0.103, 0.141
batch losses (mrrl, rdl): 0.0144148944, 0.0013397882

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 563
rank avg (pred): 0.323 +- 0.149
mrr vals (pred, true): 0.075, 0.040
batch losses (mrrl, rdl): 0.0063605984, 0.0006775866

Epoch over!
epoch time: 13.006

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 355
rank avg (pred): 0.335 +- 0.174
mrr vals (pred, true): 0.101, 0.142
batch losses (mrrl, rdl): 0.0171797778, 0.0007724824

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 405
rank avg (pred): 0.362 +- 0.173
mrr vals (pred, true): 0.082, 0.004
batch losses (mrrl, rdl): 0.0105320523, 0.0001753745

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 192
rank avg (pred): 0.302 +- 0.156
mrr vals (pred, true): 0.105, 0.004
batch losses (mrrl, rdl): 0.0302388333, 0.0004407048

Epoch over!
epoch time: 12.559

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.301 +- 0.167
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.036713291, 0.0004992984

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 807
rank avg (pred): 0.310 +- 0.171
mrr vals (pred, true): 0.111, 0.005
batch losses (mrrl, rdl): 0.0374328047, 0.0003924072

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 693
rank avg (pred): 0.414 +- 0.212
mrr vals (pred, true): 0.047, 0.003
batch losses (mrrl, rdl): 7.53893e-05, 6.834e-05

Epoch over!
epoch time: 13.182

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.350 +- 0.174
mrr vals (pred, true): 0.076, 0.132
batch losses (mrrl, rdl): 0.0308706686, 0.0012147357

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.028 +- 0.017
mrr vals (pred, true): 0.184, 0.179
batch losses (mrrl, rdl): 0.0002605011, 1.97825e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 977
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.215, 0.239
batch losses (mrrl, rdl): 0.0055500208, 2.17003e-05

Epoch over!
epoch time: 13.205

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 42
rank avg (pred): 0.046 +- 0.027
mrr vals (pred, true): 0.151, 0.179
batch losses (mrrl, rdl): 0.0078465585, 2.985e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.297 +- 0.149
mrr vals (pred, true): 0.084, 0.004
batch losses (mrrl, rdl): 0.011844838, 0.0005074824

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 114
rank avg (pred): 0.252 +- 0.139
mrr vals (pred, true): 0.093, 0.110
batch losses (mrrl, rdl): 0.0029934836, 0.0003522924

Epoch over!
epoch time: 14.992

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 238
rank avg (pred): 0.290 +- 0.149
mrr vals (pred, true): 0.102, 0.004
batch losses (mrrl, rdl): 0.0271846242, 0.0006703479

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 425
rank avg (pred): 0.248 +- 0.137
mrr vals (pred, true): 0.115, 0.004
batch losses (mrrl, rdl): 0.0428268015, 0.000815752

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 798
rank avg (pred): 0.264 +- 0.142
mrr vals (pred, true): 0.113, 0.004
batch losses (mrrl, rdl): 0.0403168052, 0.0007521816

Epoch over!
epoch time: 14.695

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 178
rank avg (pred): 0.276 +- 0.137
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0072102351, 0.0007424025

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 149
rank avg (pred): 0.303 +- 0.160
mrr vals (pred, true): 0.108, 0.177
batch losses (mrrl, rdl): 0.0466857627, 0.0010149663

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 483
rank avg (pred): 0.287 +- 0.141
mrr vals (pred, true): 0.093, 0.004
batch losses (mrrl, rdl): 0.0183386095, 0.0005015257

Epoch over!
epoch time: 14.018

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 579
rank avg (pred): 0.320 +- 0.180
mrr vals (pred, true): 0.055, 0.010
batch losses (mrrl, rdl): 0.0003015897, 1.88651e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.264 +- 0.135
mrr vals (pred, true): 0.096, 0.005
batch losses (mrrl, rdl): 0.0213930905, 0.0007037034

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1036
rank avg (pred): 0.273 +- 0.150
mrr vals (pred, true): 0.119, 0.004
batch losses (mrrl, rdl): 0.0479229465, 0.0008142392

Epoch over!
epoch time: 13.15

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 237
rank avg (pred): 0.286 +- 0.141
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0128886886, 0.0006927861

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1071
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.224, 0.196
batch losses (mrrl, rdl): 0.0082513066, 7.56444e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 947
rank avg (pred): 0.251 +- 0.142
mrr vals (pred, true): 0.089, 0.004
batch losses (mrrl, rdl): 0.0155840069, 0.0009921959

Epoch over!
epoch time: 12.295

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 212
rank avg (pred): 0.404 +- 0.215
mrr vals (pred, true): 0.103, 0.005
batch losses (mrrl, rdl): 0.0279151108, 8.1071e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1050
rank avg (pred): 0.157 +- 0.091
mrr vals (pred, true): 0.141, 0.005
batch losses (mrrl, rdl): 0.0827503055, 0.0018689296

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 24
rank avg (pred): 0.041 +- 0.024
mrr vals (pred, true): 0.166, 0.178
batch losses (mrrl, rdl): 0.0013742481, 6.0392e-06

Epoch over!
epoch time: 12.713

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.267 +- 0.148
mrr vals (pred, true): 0.101, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   10 	     0 	 0.04690 	 0.00307 	 m..s
   21 	     1 	 0.05337 	 0.00322 	 m..s
    1 	     2 	 0.04295 	 0.00329 	 m..s
    7 	     3 	 0.04533 	 0.00347 	 m..s
   35 	     4 	 0.08665 	 0.00349 	 m..s
    0 	     5 	 0.04221 	 0.00360 	 m..s
   67 	     6 	 0.10473 	 0.00360 	 MISS
   15 	     7 	 0.04964 	 0.00365 	 m..s
   60 	     8 	 0.10114 	 0.00368 	 m..s
    2 	     9 	 0.04375 	 0.00370 	 m..s
   32 	    10 	 0.08430 	 0.00377 	 m..s
   81 	    11 	 0.11901 	 0.00378 	 MISS
   57 	    12 	 0.10033 	 0.00379 	 m..s
   46 	    13 	 0.09455 	 0.00380 	 m..s
   39 	    14 	 0.09173 	 0.00381 	 m..s
   17 	    15 	 0.05165 	 0.00381 	 m..s
    6 	    16 	 0.04450 	 0.00383 	 m..s
   59 	    17 	 0.10095 	 0.00384 	 m..s
   36 	    18 	 0.09079 	 0.00385 	 m..s
   34 	    19 	 0.08644 	 0.00386 	 m..s
   41 	    20 	 0.09200 	 0.00390 	 m..s
    3 	    21 	 0.04427 	 0.00390 	 m..s
   55 	    22 	 0.09910 	 0.00390 	 m..s
   79 	    23 	 0.11554 	 0.00393 	 MISS
   36 	    24 	 0.09079 	 0.00393 	 m..s
   56 	    25 	 0.10009 	 0.00394 	 m..s
    8 	    26 	 0.04610 	 0.00409 	 m..s
   88 	    27 	 0.13385 	 0.00414 	 MISS
   49 	    28 	 0.09510 	 0.00415 	 m..s
   77 	    29 	 0.11450 	 0.00415 	 MISS
   13 	    30 	 0.04920 	 0.00416 	 m..s
   51 	    31 	 0.09612 	 0.00422 	 m..s
   67 	    32 	 0.10473 	 0.00424 	 MISS
   80 	    33 	 0.11829 	 0.00426 	 MISS
   45 	    34 	 0.09410 	 0.00446 	 m..s
   18 	    35 	 0.05200 	 0.00455 	 m..s
   52 	    36 	 0.09692 	 0.00457 	 m..s
   71 	    37 	 0.10550 	 0.00519 	 MISS
   71 	    38 	 0.10550 	 0.00522 	 MISS
    4 	    39 	 0.04433 	 0.00724 	 m..s
    4 	    40 	 0.04433 	 0.00863 	 m..s
   16 	    41 	 0.05103 	 0.00888 	 m..s
    9 	    42 	 0.04642 	 0.00898 	 m..s
   19 	    43 	 0.05200 	 0.00999 	 m..s
   11 	    44 	 0.04884 	 0.01431 	 m..s
   11 	    45 	 0.04884 	 0.01504 	 m..s
   22 	    46 	 0.05365 	 0.01713 	 m..s
   23 	    47 	 0.05383 	 0.01731 	 m..s
   27 	    48 	 0.05704 	 0.02042 	 m..s
   20 	    49 	 0.05294 	 0.02094 	 m..s
   23 	    50 	 0.05383 	 0.02142 	 m..s
   28 	    51 	 0.05785 	 0.02189 	 m..s
   14 	    52 	 0.04932 	 0.02328 	 ~...
   30 	    53 	 0.06463 	 0.02452 	 m..s
   31 	    54 	 0.06696 	 0.02514 	 m..s
   28 	    55 	 0.05785 	 0.02655 	 m..s
   25 	    56 	 0.05677 	 0.02681 	 ~...
   65 	    57 	 0.10469 	 0.02798 	 m..s
   43 	    58 	 0.09266 	 0.02867 	 m..s
   25 	    59 	 0.05677 	 0.03033 	 ~...
   97 	    60 	 0.16570 	 0.05027 	 MISS
   96 	    61 	 0.16440 	 0.05106 	 MISS
   65 	    62 	 0.10469 	 0.07709 	 ~...
   42 	    63 	 0.09256 	 0.10685 	 ~...
   33 	    64 	 0.08628 	 0.11421 	 ~...
   48 	    65 	 0.09508 	 0.11742 	 ~...
   69 	    66 	 0.10485 	 0.11816 	 ~...
   47 	    67 	 0.09487 	 0.12516 	 m..s
   90 	    68 	 0.14459 	 0.12564 	 ~...
   74 	    69 	 0.10637 	 0.12862 	 ~...
   38 	    70 	 0.09136 	 0.12904 	 m..s
   90 	    71 	 0.14459 	 0.13286 	 ~...
   54 	    72 	 0.09866 	 0.13521 	 m..s
   89 	    73 	 0.14370 	 0.13924 	 ~...
   53 	    74 	 0.09735 	 0.14233 	 m..s
   99 	    75 	 0.17371 	 0.14255 	 m..s
   58 	    76 	 0.10051 	 0.14286 	 m..s
   95 	    77 	 0.14835 	 0.14747 	 ~...
   92 	    78 	 0.14749 	 0.15133 	 ~...
   92 	    79 	 0.14749 	 0.15274 	 ~...
   70 	    80 	 0.10491 	 0.15295 	 m..s
   92 	    81 	 0.14749 	 0.15561 	 ~...
   75 	    82 	 0.10739 	 0.15609 	 m..s
   98 	    83 	 0.17125 	 0.15845 	 ~...
   62 	    84 	 0.10152 	 0.15860 	 m..s
  100 	    85 	 0.17450 	 0.15914 	 ~...
   50 	    86 	 0.09520 	 0.15918 	 m..s
  101 	    87 	 0.17803 	 0.16001 	 ~...
   64 	    88 	 0.10383 	 0.16167 	 m..s
   62 	    89 	 0.10152 	 0.16294 	 m..s
   73 	    90 	 0.10559 	 0.16373 	 m..s
   61 	    91 	 0.10145 	 0.16489 	 m..s
  107 	    92 	 0.19231 	 0.16630 	 ~...
   44 	    93 	 0.09394 	 0.16692 	 m..s
   76 	    94 	 0.11382 	 0.16961 	 m..s
  106 	    95 	 0.19037 	 0.17444 	 ~...
   82 	    96 	 0.11939 	 0.17568 	 m..s
  102 	    97 	 0.18034 	 0.18087 	 ~...
   40 	    98 	 0.09182 	 0.18122 	 m..s
   83 	    99 	 0.11953 	 0.18503 	 m..s
   78 	   100 	 0.11495 	 0.19017 	 m..s
  103 	   101 	 0.18572 	 0.19078 	 ~...
   84 	   102 	 0.11990 	 0.19210 	 m..s
   85 	   103 	 0.12106 	 0.19838 	 m..s
  103 	   104 	 0.18572 	 0.19856 	 ~...
  110 	   105 	 0.21717 	 0.20554 	 ~...
  114 	   106 	 0.22412 	 0.20595 	 ~...
  105 	   107 	 0.18783 	 0.20601 	 ~...
  116 	   108 	 0.22627 	 0.20636 	 ~...
  109 	   109 	 0.19839 	 0.20692 	 ~...
  111 	   110 	 0.21738 	 0.21220 	 ~...
   87 	   111 	 0.12892 	 0.21567 	 m..s
   86 	   112 	 0.12882 	 0.21696 	 m..s
  108 	   113 	 0.19687 	 0.22028 	 ~...
  112 	   114 	 0.21933 	 0.22095 	 ~...
  113 	   115 	 0.22139 	 0.22172 	 ~...
  118 	   116 	 0.25650 	 0.23706 	 ~...
  115 	   117 	 0.22443 	 0.24193 	 ~...
  117 	   118 	 0.23637 	 0.24580 	 ~...
  120 	   119 	 0.29848 	 0.25834 	 m..s
  119 	   120 	 0.27514 	 0.26101 	 ~...
==========================================
r_mrr = 0.7766134738922119
r2_mrr = 0.5162794589996338
spearmanr_mrr@5 = 0.9728685617446899
spearmanr_mrr@10 = 0.9266690611839294
spearmanr_mrr@50 = 0.9891612529754639
spearmanr_mrr@100 = 0.878082811832428
spearmanr_mrr@All = 0.9045815467834473
==========================================
test time: 0.385
Done Testing dataset CoDExSmall
total time taken: 210.2635612487793
training time taken: 200.9918565750122
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7766)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5163)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9729)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9267)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9892)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8781)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.9046)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.111447981202218}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 5759413499077296
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [889, 243, 476, 458, 249, 977, 351, 1060, 1164, 17, 1128, 558, 772, 321, 1105, 596, 763, 127, 732, 289, 1029, 11, 136, 99, 780, 1062, 610, 710, 1175, 384, 232, 154, 517, 456, 980, 67, 427, 626, 203, 948, 568, 997, 223, 676, 753, 87, 313, 540, 276, 174, 387, 500, 843, 1159, 163, 861, 656, 85, 139, 350, 200, 1048, 931, 643, 1106, 20, 305, 408, 273, 646, 109, 511, 1025, 1005, 740, 425, 903, 1138, 78, 96, 1211, 797, 1061, 378, 83, 590, 1134, 149, 1142, 267, 449, 1201, 651, 1031, 112, 492, 825, 1047, 1037, 312, 1178, 735, 451, 247, 585, 399, 8, 158, 370, 573, 445, 1091, 34, 468, 1024, 159, 33, 617, 201, 840, 799]
valid_ids (0): []
train_ids (1094): [148, 81, 57, 721, 877, 452, 13, 1087, 719, 496, 360, 199, 1191, 786, 100, 157, 386, 960, 569, 895, 762, 802, 282, 1127, 416, 288, 850, 238, 853, 874, 992, 406, 114, 471, 541, 742, 463, 1143, 743, 91, 553, 811, 869, 973, 804, 867, 383, 993, 356, 1054, 302, 1093, 837, 325, 286, 3, 1194, 146, 778, 432, 911, 892, 213, 1171, 1103, 974, 1075, 1006, 228, 347, 1000, 420, 579, 366, 951, 1129, 301, 74, 947, 441, 1189, 418, 934, 1044, 1161, 619, 1072, 300, 284, 152, 145, 258, 9, 698, 989, 448, 1140, 882, 1003, 217, 536, 412, 110, 222, 757, 578, 1108, 1065, 657, 730, 981, 1056, 926, 293, 134, 696, 95, 31, 439, 51, 784, 856, 202, 263, 140, 1130, 684, 443, 264, 566, 75, 1212, 25, 188, 311, 944, 1050, 830, 922, 713, 489, 1034, 28, 41, 173, 580, 277, 212, 129, 298, 822, 1079, 252, 1176, 116, 563, 625, 779, 891, 679, 642, 333, 354, 829, 470, 1173, 589, 587, 894, 235, 1022, 402, 688, 885, 1098, 820, 1213, 475, 1137, 21, 358, 582, 921, 570, 380, 1084, 411, 707, 169, 245, 197, 985, 905, 1193, 604, 1036, 618, 120, 521, 485, 22, 90, 852, 562, 331, 118, 430, 318, 1162, 237, 1059, 652, 674, 689, 828, 549, 1168, 435, 798, 600, 268, 682, 552, 988, 530, 875, 53, 224, 107, 594, 806, 858, 673, 501, 43, 183, 1184, 663, 916, 991, 942, 628, 1070, 497, 841, 122, 680, 389, 827, 1073, 274, 352, 1170, 623, 936, 789, 369, 690, 156, 132, 1158, 906, 986, 904, 966, 662, 0, 510, 297, 1206, 72, 764, 253, 338, 121, 182, 290, 720, 564, 812, 636, 700, 736, 1192, 1163, 193, 632, 315, 65, 454, 415, 442, 979, 575, 678, 395, 631, 896, 1145, 381, 505, 444, 1039, 543, 433, 927, 294, 859, 595, 1207, 768, 349, 467, 893, 461, 39, 266, 1019, 603, 1203, 512, 401, 241, 307, 1141, 211, 106, 667, 453, 538, 1012, 561, 1081, 838, 1013, 609, 635, 715, 557, 1139, 520, 1051, 447, 382, 773, 147, 781, 949, 1132, 473, 488, 373, 932, 332, 965, 704, 125, 82, 255, 162, 1126, 259, 460, 40, 180, 920, 756, 972, 377, 654, 883, 56, 160, 1076, 925, 353, 144, 117, 170, 1041, 1069, 375, 161, 546, 345, 308, 322, 250, 613, 469, 555, 30, 355, 554, 529, 1110, 910, 758, 124, 390, 964, 372, 1104, 576, 55, 751, 1026, 835, 479, 647, 598, 952, 45, 583, 18, 419, 687, 890, 1007, 1092, 697, 128, 240, 36, 24, 1052, 655, 648, 176, 192, 509, 309, 115, 634, 365, 887, 428, 348, 1205, 519, 913, 943, 769, 607, 774, 292, 1099, 990, 659, 847, 363, 760, 206, 1004, 1096, 782, 1017, 694, 560, 574, 586, 164, 14, 438, 337, 846, 186, 898, 1038, 219, 1014, 1027, 1016, 665, 103, 50, 172, 204, 477, 998, 296, 1117, 54, 744, 391, 44, 622, 481, 866, 670, 257, 1174, 915, 248, 108, 508, 591, 620, 878, 1209, 1020, 545, 1124, 946, 319, 346, 98, 930, 593, 767, 978, 84, 788, 111, 155, 503, 1177, 606, 602, 516, 12, 417, 660, 48, 1121, 15, 1066, 870, 94, 711, 261, 410, 480, 959, 1118, 1214, 32, 171, 938, 699, 340, 971, 533, 1107, 1144, 967, 1102, 1045, 937, 907, 929, 283, 761, 462, 681, 1119, 873, 539, 166, 945, 7, 275, 1077, 398, 336, 271, 181, 69, 436, 1010, 914, 734, 547, 137, 765, 1160, 216, 1035, 709, 849, 123, 615, 190, 649, 1190, 368, 816, 731, 234, 361, 499, 330, 982, 727, 198, 969, 483, 1182, 422, 209, 324, 683, 59, 1187, 1148, 800, 577, 824, 537, 150, 737, 526, 650, 785, 722, 388, 995, 611, 374, 624, 1101, 664, 712, 227, 1064, 490, 1172, 999, 983, 823, 47, 738, 379, 272, 572, 52, 633, 280, 178, 601, 1030, 175, 1149, 919, 376, 630, 437, 334, 242, 902, 42, 269, 295, 855, 138, 246, 49, 1135, 984, 826, 194, 299, 627, 933, 876, 1115, 1009, 975, 316, 868, 306, 794, 729, 1146, 1196, 431, 1071, 105, 528, 230, 565, 961, 532, 225, 950, 86, 524, 498, 394, 571, 714, 879, 637, 534, 113, 329, 702, 518, 783, 567, 434, 621, 327, 484, 871, 776, 392, 923, 502, 770, 1136, 1085, 807, 486, 1074, 1185, 66, 371, 940, 404, 35, 705, 405, 478, 515, 897, 1113, 385, 909, 1008, 359, 792, 1042, 793, 218, 1090, 1111, 527, 834, 426, 1199, 1181, 808, 513, 1, 1088, 831, 1154, 1195, 92, 917, 1001, 1100, 1002, 455, 270, 281, 341, 2, 440, 1021, 706, 599, 326, 76, 265, 658, 278, 29, 185, 865, 364, 1198, 304, 1089, 1080, 262, 725, 1125, 1028, 941, 1015, 523, 775, 80, 1040, 968, 1208, 189, 796, 605, 215, 1067, 1116, 733, 135, 862, 424, 987, 101, 953, 935, 403, 328, 1152, 863, 771, 46, 177, 343, 491, 195, 814, 133, 1197, 1167, 357, 126, 260, 413, 857, 795, 525, 588, 396, 168, 68, 1150, 550, 612, 1023, 414, 752, 1165, 1063, 19, 556, 256, 196, 141, 726, 93, 1049, 955, 494, 832, 323, 597, 535, 747, 464, 142, 482, 717, 507, 1097, 749, 970, 746, 872, 16, 908, 836, 344, 239, 754, 695, 104, 686, 208, 1078, 912, 638, 220, 291, 880, 421, 818, 1114, 860, 787, 423, 1057, 581, 817, 207, 692, 187, 231, 884, 1083, 317, 693, 342, 1094, 833, 244, 335, 644, 976, 1153, 221, 38, 559, 639, 1131, 367, 143, 407, 888, 1055, 1043, 339, 487, 668, 1179, 544, 1169, 285, 531, 254, 958, 73, 718, 89, 810, 102, 88, 167, 493, 584, 1032, 755, 362, 645, 504, 962, 27, 750, 1053, 64, 954, 70, 899, 614, 4, 10, 629, 815, 1068, 1082, 675, 691, 821, 1018, 71, 472, 446, 803, 514, 474, 506, 640, 400, 608, 708, 466, 1109, 801, 766, 723, 1202, 233, 97, 844, 842, 994, 314, 131, 918, 63, 724, 62, 939, 6, 205, 854, 809, 845, 320, 303, 79, 393, 1204, 1046, 23, 459, 457, 1011, 839, 409, 661, 522, 957, 901, 741, 790, 677, 465, 1112, 1086, 791, 1186, 881, 813, 251, 130, 928, 151, 819, 748, 701, 191, 1133, 666, 1122, 1210, 672, 61, 184, 1166, 1120, 1058, 214, 210, 1188, 777, 685, 848, 616, 1095, 963, 956, 716, 37, 551, 805, 1183, 287, 226, 1147, 77, 279, 669, 864, 641, 739, 310, 26, 900, 153, 58, 542, 119, 1151, 759, 886, 1180, 1156, 996, 924, 592, 703, 60, 236, 653, 1157, 728, 429, 397, 1155, 5, 165, 745, 450, 548, 851, 495, 1033, 179, 229, 671, 1200, 1123]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3496520609485006
the save name prefix for this run is:  chkpt-ID_3496520609485006_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 352
rank avg (pred): 0.445 +- 0.005
mrr vals (pred, true): 0.001, 0.148
batch losses (mrrl, rdl): 0.0, 0.0018040616

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.057 +- 0.041
mrr vals (pred, true): 0.171, 0.243
batch losses (mrrl, rdl): 0.0, 4.5082e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.278 +- 0.225
mrr vals (pred, true): 0.286, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004834133

Epoch over!
epoch time: 12.615

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 917
rank avg (pred): 0.121 +- 0.101
mrr vals (pred, true): 0.329, 0.151
batch losses (mrrl, rdl): 0.0, 0.0001426

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 209
rank avg (pred): 0.303 +- 0.251
mrr vals (pred, true): 0.281, 0.005
batch losses (mrrl, rdl): 0.0, 0.0002956261

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1160
rank avg (pred): 0.148 +- 0.135
mrr vals (pred, true): 0.364, 0.025
batch losses (mrrl, rdl): 0.0, 9.9232e-06

Epoch over!
epoch time: 12.028

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.253 +- 0.232
mrr vals (pred, true): 0.354, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005911537

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 481
rank avg (pred): 0.255 +- 0.237
mrr vals (pred, true): 0.360, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005959864

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 234
rank avg (pred): 0.272 +- 0.247
mrr vals (pred, true): 0.339, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005621081

Epoch over!
epoch time: 11.958

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.182 +- 0.170
mrr vals (pred, true): 0.385, 0.021
batch losses (mrrl, rdl): 0.0, 9.2807e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 202
rank avg (pred): 0.261 +- 0.244
mrr vals (pred, true): 0.372, 0.003
batch losses (mrrl, rdl): 0.0, 0.000587119

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 600
rank avg (pred): 0.339 +- 0.305
mrr vals (pred, true): 0.347, 0.006
batch losses (mrrl, rdl): 0.0, 5.70868e-05

Epoch over!
epoch time: 11.776

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 634
rank avg (pred): 0.336 +- 0.301
mrr vals (pred, true): 0.346, 0.013
batch losses (mrrl, rdl): 0.0, 9.04384e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.067 +- 0.062
mrr vals (pred, true): 0.377, 0.147
batch losses (mrrl, rdl): 0.0, 4.052e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 531
rank avg (pred): 0.189 +- 0.184
mrr vals (pred, true): 0.405, 0.015
batch losses (mrrl, rdl): 0.0, 6.1298e-06

Epoch over!
epoch time: 11.783

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 723
rank avg (pred): 0.375 +- 0.325
mrr vals (pred, true): 0.322, 0.004
batch losses (mrrl, rdl): 0.737745285, 7.93104e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 464
rank avg (pred): 0.190 +- 0.121
mrr vals (pred, true): 0.118, 0.005
batch losses (mrrl, rdl): 0.0466550589, 0.001444492

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 976
rank avg (pred): 0.014 +- 0.008
mrr vals (pred, true): 0.182, 0.239
batch losses (mrrl, rdl): 0.0328394771, 1.80988e-05

Epoch over!
epoch time: 18.441

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 252
rank avg (pred): 0.043 +- 0.027
mrr vals (pred, true): 0.154, 0.189
batch losses (mrrl, rdl): 0.0124133108, 1.07496e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 104
rank avg (pred): 0.276 +- 0.171
mrr vals (pred, true): 0.135, 0.177
batch losses (mrrl, rdl): 0.0175029766, 0.0008056258

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.317 +- 0.187
mrr vals (pred, true): 0.110, 0.004
batch losses (mrrl, rdl): 0.0355094746, 0.0003678928

Epoch over!
epoch time: 12.664

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 21
rank avg (pred): 0.033 +- 0.021
mrr vals (pred, true): 0.192, 0.168
batch losses (mrrl, rdl): 0.0060315719, 1.38356e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 957
rank avg (pred): 0.353 +- 0.194
mrr vals (pred, true): 0.101, 0.004
batch losses (mrrl, rdl): 0.0257001165, 0.0001850185

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.426 +- 0.119
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.000134654, 0.0001047919

Epoch over!
epoch time: 12.733

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.016 +- 0.012
mrr vals (pred, true): 0.202, 0.168
batch losses (mrrl, rdl): 0.0119039826, 8.15069e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.413 +- 0.110
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001580507, 0.0001657727

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 841
rank avg (pred): 0.323 +- 0.180
mrr vals (pred, true): 0.107, 0.153
batch losses (mrrl, rdl): 0.0207149722, 0.0011719184

Epoch over!
epoch time: 12.621

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1
rank avg (pred): 0.035 +- 0.050
mrr vals (pred, true): 0.208, 0.166
batch losses (mrrl, rdl): 0.0172981527, 8.0888e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 921
rank avg (pred): 0.339 +- 0.166
mrr vals (pred, true): 0.088, 0.159
batch losses (mrrl, rdl): 0.0509213991, 0.0013055485

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1147
rank avg (pred): 0.371 +- 0.123
mrr vals (pred, true): 0.058, 0.026
batch losses (mrrl, rdl): 0.0006356594, 0.0009031548

Epoch over!
epoch time: 12.382

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1197
rank avg (pred): 0.380 +- 0.116
mrr vals (pred, true): 0.056, 0.004
batch losses (mrrl, rdl): 0.0003378052, 0.0001967587

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 317
rank avg (pred): 0.024 +- 0.051
mrr vals (pred, true): 0.240, 0.273
batch losses (mrrl, rdl): 0.0112081086, 3.4774e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 113
rank avg (pred): 0.301 +- 0.157
mrr vals (pred, true): 0.102, 0.161
batch losses (mrrl, rdl): 0.0345158279, 0.0009186416

Epoch over!
epoch time: 12.824

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 745
rank avg (pred): 0.145 +- 0.105
mrr vals (pred, true): 0.143, 0.193
batch losses (mrrl, rdl): 0.0253324676, 0.0002567423

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 982
rank avg (pred): 0.044 +- 0.069
mrr vals (pred, true): 0.225, 0.203
batch losses (mrrl, rdl): 0.0045268647, 6.761e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 939
rank avg (pred): 0.261 +- 0.141
mrr vals (pred, true): 0.106, 0.181
batch losses (mrrl, rdl): 0.0557913594, 0.0006964094

Epoch over!
epoch time: 12.951

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 124
rank avg (pred): 0.281 +- 0.144
mrr vals (pred, true): 0.099, 0.139
batch losses (mrrl, rdl): 0.0162817817, 0.0006788216

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.236 +- 0.082
mrr vals (pred, true): 0.057, 0.020
batch losses (mrrl, rdl): 0.0005015436, 5.75523e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1131
rank avg (pred): 0.251 +- 0.148
mrr vals (pred, true): 0.132, 0.004
batch losses (mrrl, rdl): 0.0669486299, 0.0008066766

Epoch over!
epoch time: 11.951

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.036 +- 0.053
mrr vals (pred, true): 0.251, 0.238
batch losses (mrrl, rdl): 0.0016448629, 2.5795e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 823
rank avg (pred): 0.221 +- 0.145
mrr vals (pred, true): 0.164, 0.151
batch losses (mrrl, rdl): 0.0016277992, 0.0005400941

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1203
rank avg (pred): 0.293 +- 0.098
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.000273905, 0.000715406

Epoch over!
epoch time: 12.207

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.265 +- 0.142
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.0366837457, 0.0007693229

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.104 +- 0.112
mrr vals (pred, true): 0.230, 0.220
batch losses (mrrl, rdl): 0.0010043838, 7.14391e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 263
rank avg (pred): 0.100 +- 0.114
mrr vals (pred, true): 0.236, 0.258
batch losses (mrrl, rdl): 0.0049001449, 0.000104518

Epoch over!
epoch time: 13.125

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.251 +- 0.135
mrr vals (pred, true): 0.108, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.05328 	 0.00333 	 m..s
   51 	     1 	 0.09829 	 0.00347 	 m..s
    1 	     2 	 0.04336 	 0.00358 	 m..s
   69 	     3 	 0.11054 	 0.00360 	 MISS
   23 	     4 	 0.05542 	 0.00362 	 m..s
    3 	     5 	 0.04488 	 0.00370 	 m..s
   56 	     6 	 0.10411 	 0.00370 	 MISS
   74 	     7 	 0.11969 	 0.00378 	 MISS
   86 	     8 	 0.13494 	 0.00388 	 MISS
   43 	     9 	 0.09510 	 0.00391 	 m..s
   59 	    10 	 0.10789 	 0.00394 	 MISS
   14 	    11 	 0.05197 	 0.00397 	 m..s
   82 	    12 	 0.13375 	 0.00400 	 MISS
   75 	    13 	 0.12148 	 0.00400 	 MISS
   81 	    14 	 0.13340 	 0.00404 	 MISS
   45 	    15 	 0.09543 	 0.00406 	 m..s
   43 	    16 	 0.09510 	 0.00406 	 m..s
   84 	    17 	 0.13480 	 0.00413 	 MISS
   30 	    18 	 0.08901 	 0.00414 	 m..s
   59 	    19 	 0.10789 	 0.00421 	 MISS
   68 	    20 	 0.11010 	 0.00422 	 MISS
   59 	    21 	 0.10789 	 0.00422 	 MISS
   84 	    22 	 0.13480 	 0.00426 	 MISS
   27 	    23 	 0.05746 	 0.00427 	 m..s
   52 	    24 	 0.09840 	 0.00430 	 m..s
   79 	    25 	 0.13226 	 0.00437 	 MISS
   46 	    26 	 0.09664 	 0.00452 	 m..s
   89 	    27 	 0.13878 	 0.00457 	 MISS
   76 	    28 	 0.12265 	 0.00458 	 MISS
   54 	    29 	 0.10040 	 0.00465 	 m..s
   31 	    30 	 0.08944 	 0.00486 	 m..s
   57 	    31 	 0.10446 	 0.00492 	 m..s
   35 	    32 	 0.09280 	 0.00502 	 m..s
    5 	    33 	 0.04559 	 0.00668 	 m..s
    0 	    34 	 0.04269 	 0.00725 	 m..s
   21 	    35 	 0.05530 	 0.00876 	 m..s
   10 	    36 	 0.05110 	 0.00886 	 m..s
   13 	    37 	 0.05166 	 0.01122 	 m..s
   21 	    38 	 0.05530 	 0.01162 	 m..s
   16 	    39 	 0.05291 	 0.01188 	 m..s
    7 	    40 	 0.04581 	 0.01283 	 m..s
   12 	    41 	 0.05159 	 0.01418 	 m..s
    6 	    42 	 0.04578 	 0.01566 	 m..s
    2 	    43 	 0.04352 	 0.01678 	 ~...
   24 	    44 	 0.05574 	 0.01713 	 m..s
   20 	    45 	 0.05357 	 0.01807 	 m..s
   24 	    46 	 0.05574 	 0.01896 	 m..s
    4 	    47 	 0.04553 	 0.02011 	 ~...
    8 	    48 	 0.04943 	 0.02127 	 ~...
   15 	    49 	 0.05213 	 0.02247 	 ~...
   17 	    50 	 0.05314 	 0.02257 	 m..s
   28 	    51 	 0.06126 	 0.02330 	 m..s
   26 	    52 	 0.05678 	 0.02351 	 m..s
   11 	    53 	 0.05152 	 0.02423 	 ~...
   18 	    54 	 0.05320 	 0.02592 	 ~...
    9 	    55 	 0.05084 	 0.02915 	 ~...
   59 	    56 	 0.10789 	 0.03749 	 m..s
   59 	    57 	 0.10789 	 0.04388 	 m..s
   91 	    58 	 0.14676 	 0.05558 	 m..s
   32 	    59 	 0.09004 	 0.09528 	 ~...
   34 	    60 	 0.09046 	 0.10711 	 ~...
   29 	    61 	 0.08896 	 0.10802 	 ~...
   32 	    62 	 0.09004 	 0.11016 	 ~...
   47 	    63 	 0.09704 	 0.12016 	 ~...
   53 	    64 	 0.09862 	 0.12419 	 ~...
   48 	    65 	 0.09709 	 0.12516 	 ~...
   41 	    66 	 0.09479 	 0.12846 	 m..s
   73 	    67 	 0.11928 	 0.12862 	 ~...
   42 	    68 	 0.09483 	 0.12904 	 m..s
   48 	    69 	 0.09709 	 0.13014 	 m..s
   37 	    70 	 0.09456 	 0.13096 	 m..s
   36 	    71 	 0.09312 	 0.13204 	 m..s
   93 	    72 	 0.16284 	 0.13253 	 m..s
   39 	    73 	 0.09460 	 0.13455 	 m..s
   39 	    74 	 0.09460 	 0.14112 	 m..s
   96 	    75 	 0.19343 	 0.14231 	 m..s
   98 	    76 	 0.19613 	 0.14255 	 m..s
   50 	    77 	 0.09734 	 0.14285 	 m..s
   37 	    78 	 0.09456 	 0.14286 	 m..s
  101 	    79 	 0.21034 	 0.15109 	 m..s
   59 	    80 	 0.10789 	 0.15161 	 m..s
  105 	    81 	 0.21151 	 0.15188 	 m..s
   91 	    82 	 0.14676 	 0.15419 	 ~...
  100 	    83 	 0.19779 	 0.15441 	 m..s
   71 	    84 	 0.11143 	 0.16878 	 m..s
  104 	    85 	 0.21067 	 0.16954 	 m..s
   55 	    86 	 0.10376 	 0.16969 	 m..s
   58 	    87 	 0.10474 	 0.17169 	 m..s
  107 	    88 	 0.21601 	 0.17605 	 m..s
   87 	    89 	 0.13821 	 0.17628 	 m..s
   71 	    90 	 0.11143 	 0.17666 	 m..s
   59 	    91 	 0.10789 	 0.17801 	 m..s
   70 	    92 	 0.11064 	 0.17903 	 m..s
   97 	    93 	 0.19366 	 0.18173 	 ~...
   94 	    94 	 0.16346 	 0.18782 	 ~...
   98 	    95 	 0.19613 	 0.18788 	 ~...
   59 	    96 	 0.10789 	 0.18871 	 m..s
   59 	    97 	 0.10789 	 0.18895 	 m..s
   77 	    98 	 0.12345 	 0.18910 	 m..s
  102 	    99 	 0.21048 	 0.18998 	 ~...
   95 	   100 	 0.16625 	 0.19334 	 ~...
  120 	   101 	 0.25115 	 0.19634 	 m..s
  111 	   102 	 0.22885 	 0.19717 	 m..s
  118 	   103 	 0.24740 	 0.20365 	 m..s
  106 	   104 	 0.21276 	 0.20509 	 ~...
   88 	   105 	 0.13833 	 0.21182 	 m..s
   78 	   106 	 0.12347 	 0.21423 	 m..s
   83 	   107 	 0.13436 	 0.21630 	 m..s
   90 	   108 	 0.13942 	 0.21663 	 m..s
   80 	   109 	 0.13332 	 0.21971 	 m..s
  109 	   110 	 0.22313 	 0.21972 	 ~...
  115 	   111 	 0.23023 	 0.22640 	 ~...
  111 	   112 	 0.22885 	 0.22825 	 ~...
  102 	   113 	 0.21048 	 0.23043 	 ~...
  115 	   114 	 0.23023 	 0.23096 	 ~...
  113 	   115 	 0.22989 	 0.23887 	 ~...
  110 	   116 	 0.22443 	 0.25210 	 ~...
  119 	   117 	 0.24975 	 0.26202 	 ~...
  108 	   118 	 0.22213 	 0.26222 	 m..s
  117 	   119 	 0.23620 	 0.26223 	 ~...
  113 	   120 	 0.22989 	 0.26816 	 m..s
==========================================
r_mrr = 0.7392944693565369
r2_mrr = 0.46781355142593384
spearmanr_mrr@5 = 0.8374084830284119
spearmanr_mrr@10 = 0.8527171015739441
spearmanr_mrr@50 = 0.9516036510467529
spearmanr_mrr@100 = 0.9039942026138306
spearmanr_mrr@All = 0.9254995584487915
==========================================
test time: 0.393
Done Testing dataset CoDExSmall
total time taken: 202.98614478111267
training time taken: 192.53044867515564
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7393)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4678)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.8374)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.8527)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9516)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.9040)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.9255)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.6250580814812565}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 2420341342983926
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [679, 990, 369, 661, 553, 1146, 84, 284, 904, 747, 780, 460, 738, 26, 809, 230, 576, 354, 70, 366, 228, 1148, 482, 919, 439, 17, 473, 501, 942, 424, 529, 1018, 465, 160, 364, 1073, 438, 595, 155, 36, 678, 640, 630, 244, 798, 719, 98, 1103, 820, 539, 892, 340, 1039, 64, 229, 750, 390, 1205, 289, 1017, 1186, 884, 819, 388, 619, 607, 523, 475, 412, 433, 227, 664, 181, 535, 158, 1206, 561, 857, 73, 916, 178, 177, 66, 1072, 562, 866, 319, 1139, 832, 927, 1036, 614, 579, 941, 736, 908, 347, 620, 20, 715, 874, 1150, 127, 945, 1050, 195, 437, 694, 1159, 1091, 700, 1200, 257, 786, 1001, 954, 647, 291, 1154, 662, 898]
valid_ids (0): []
train_ids (1094): [2, 889, 274, 1211, 546, 888, 45, 303, 756, 410, 506, 573, 342, 753, 850, 211, 235, 329, 324, 1048, 762, 8, 140, 758, 191, 1096, 174, 748, 1006, 415, 852, 43, 102, 687, 1095, 720, 1214, 1174, 639, 253, 1077, 1011, 914, 1057, 759, 844, 1162, 1031, 1142, 681, 30, 906, 938, 723, 24, 785, 1136, 494, 571, 1016, 853, 701, 418, 318, 478, 778, 1168, 7, 769, 655, 180, 1010, 430, 245, 1172, 481, 685, 445, 559, 240, 721, 332, 400, 466, 952, 810, 830, 1093, 575, 602, 1165, 773, 962, 497, 770, 987, 746, 622, 48, 802, 527, 1191, 569, 761, 991, 827, 627, 254, 1177, 733, 507, 674, 1185, 368, 1104, 22, 122, 725, 1198, 760, 335, 545, 842, 416, 57, 1175, 406, 200, 1082, 262, 1084, 428, 273, 1098, 534, 966, 757, 735, 979, 896, 429, 551, 833, 40, 683, 548, 493, 584, 514, 741, 372, 714, 610, 643, 359, 212, 525, 1028, 978, 1060, 395, 787, 293, 1021, 246, 305, 742, 768, 170, 310, 1130, 440, 772, 444, 54, 1117, 864, 880, 811, 1065, 1047, 463, 642, 729, 184, 104, 456, 164, 280, 1079, 782, 248, 986, 1080, 1081, 617, 210, 929, 153, 91, 1045, 1128, 116, 1101, 763, 500, 988, 146, 74, 901, 88, 1030, 172, 931, 960, 333, 1090, 670, 969, 509, 25, 206, 781, 1199, 1135, 1023, 666, 32, 1094, 358, 1149, 265, 251, 441, 696, 790, 499, 949, 590, 600, 895, 570, 1169, 963, 824, 147, 636, 189, 691, 1078, 225, 276, 847, 815, 800, 615, 414, 812, 886, 572, 76, 495, 933, 68, 1127, 789, 498, 910, 754, 1153, 130, 269, 951, 1112, 1055, 258, 699, 1062, 612, 1190, 867, 281, 185, 11, 726, 169, 1075, 709, 261, 677, 50, 1015, 637, 110, 621, 817, 651, 601, 337, 652, 260, 672, 1120, 134, 1115, 932, 14, 542, 379, 151, 512, 123, 879, 85, 1170, 698, 1070, 788, 654, 835, 86, 304, 256, 975, 377, 1167, 518, 1107, 111, 634, 183, 771, 394, 710, 680, 313, 1033, 435, 79, 314, 344, 957, 426, 224, 1106, 1187, 31, 152, 108, 290, 90, 1194, 922, 327, 878, 505, 541, 1020, 234, 799, 450, 718, 480, 249, 1183, 80, 479, 974, 154, 1197, 1076, 383, 939, 851, 336, 722, 21, 131, 427, 792, 1019, 618, 166, 490, 1192, 483, 97, 774, 531, 401, 425, 357, 51, 209, 893, 259, 1181, 971, 688, 900, 202, 348, 1066, 1176, 46, 156, 215, 955, 667, 606, 1126, 1184, 221, 326, 382, 536, 204, 716, 1042, 1195, 83, 538, 398, 1013, 352, 838, 345, 924, 232, 380, 496, 1118, 1067, 845, 145, 135, 568, 766, 1124, 320, 1151, 608, 707, 386, 555, 1038, 985, 243, 1007, 277, 485, 133, 403, 476, 1009, 669, 373, 176, 1171, 409, 1051, 854, 549, 188, 1212, 1125, 744, 378, 141, 157, 190, 411, 1123, 375, 219, 961, 948, 998, 532, 72, 58, 457, 795, 580, 885, 784, 1049, 1041, 983, 624, 99, 384, 605, 75, 791, 558, 508, 165, 1032, 641, 448, 503, 374, 724, 905, 77, 980, 93, 663, 537, 404, 585, 238, 587, 162, 935, 1110, 717, 711, 182, 764, 12, 826, 692, 462, 848, 192, 873, 899, 363, 511, 139, 890, 689, 3, 422, 861, 1012, 806, 338, 739, 740, 37, 656, 446, 413, 323, 168, 351, 282, 1178, 855, 1054, 252, 1140, 526, 61, 1005, 126, 632, 604, 519, 350, 515, 1014, 603, 339, 517, 442, 1069, 81, 330, 150, 793, 1022, 920, 592, 921, 676, 103, 1152, 33, 829, 510, 504, 649, 78, 263, 207, 272, 217, 704, 436, 860, 1074, 299, 897, 596, 28, 887, 241, 682, 846, 658, 408, 1046, 447, 836, 956, 566, 593, 599, 469, 684, 730, 994, 316, 312, 965, 950, 63, 356, 5, 1003, 1133, 461, 1026, 959, 560, 1143, 449, 114, 19, 106, 367, 278, 918, 745, 397, 849, 1111, 673, 911, 236, 547, 1088, 361, 1155, 301, 112, 453, 1008, 266, 831, 113, 1083, 226, 977, 638, 218, 891, 626, 10, 591, 633, 1043, 1132, 311, 371, 894, 296, 1207, 431, 976, 1040, 13, 1037, 743, 841, 92, 69, 1109, 513, 34, 734, 247, 1059, 993, 392, 953, 283, 279, 486, 1163, 47, 816, 389, 87, 923, 35, 59, 370, 958, 1138, 402, 109, 628, 242, 376, 1134, 455, 1092, 161, 1102, 1158, 477, 1114, 65, 623, 233, 1085, 946, 871, 173, 343, 583, 671, 997, 749, 727, 15, 999, 365, 285, 1161, 9, 1052, 1208, 464, 972, 752, 470, 645, 1156, 533, 44, 930, 95, 737, 903, 391, 292, 875, 1193, 1, 1204, 52, 399, 909, 947, 657, 62, 1210, 474, 865, 297, 82, 144, 563, 487, 616, 41, 859, 1116, 697, 804, 288, 1027, 1113, 915, 71, 137, 564, 100, 237, 712, 471, 163, 231, 1119, 668, 353, 936, 708, 271, 216, 315, 765, 1145, 635, 322, 355, 360, 222, 925, 205, 818, 713, 796, 143, 825, 167, 629, 834, 94, 298, 1100, 89, 1188, 443, 201, 877, 522, 1201, 186, 872, 268, 926, 928, 1108, 912, 937, 4, 843, 705, 943, 1180, 970, 828, 732, 863, 693, 862, 597, 837, 489, 107, 49, 1000, 1196, 124, 574, 451, 528, 286, 115, 60, 581, 554, 309, 213, 128, 1122, 1173, 989, 16, 840, 1129, 516, 1087, 609, 302, 194, 492, 653, 944, 706, 675, 992, 940, 644, 776, 856, 1157, 419, 432, 1137, 660, 421, 270, 405, 813, 565, 755, 808, 917, 396, 148, 582, 648, 393, 530, 119, 779, 349, 613, 488, 196, 1179, 540, 132, 556, 594, 794, 346, 598, 381, 870, 702, 120, 807, 821, 417, 6, 686, 220, 27, 387, 214, 868, 287, 611, 179, 967, 934, 858, 331, 902, 588, 1189, 423, 193, 458, 117, 1035, 39, 767, 1029, 1097, 1024, 552, 121, 472, 0, 665, 198, 1121, 484, 996, 783, 175, 334, 1002, 23, 881, 321, 29, 1147, 275, 454, 203, 308, 1166, 521, 984, 1203, 1064, 328, 728, 325, 1141, 1089, 317, 208, 307, 982, 159, 1044, 839, 913, 199, 968, 407, 907, 101, 125, 1144, 1131, 187, 543, 520, 797, 129, 149, 544, 524, 577, 1058, 300, 171, 142, 589, 197, 650, 659, 690, 468, 306, 239, 491, 1099, 459, 995, 695, 805, 105, 1071, 646, 751, 56, 1004, 42, 586, 1025, 731, 67, 775, 38, 703, 1034, 420, 250, 964, 876, 1182, 294, 801, 822, 1068, 1209, 578, 467, 557, 18, 452, 1160, 823, 803, 118, 981, 1086, 362, 567, 96, 814, 53, 1105, 264, 136, 1056, 55, 502, 1053, 973, 295, 434, 267, 625, 1202, 255, 550, 1213, 1061, 1063, 777, 882, 385, 223, 341, 1164, 138, 631, 883, 869]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5994404721095290
the save name prefix for this run is:  chkpt-ID_5994404721095290_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 44
rank avg (pred): 0.520 +- 0.012
mrr vals (pred, true): 0.001, 0.229
batch losses (mrrl, rdl): 0.0, 0.0049127378

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.279 +- 0.225
mrr vals (pred, true): 0.156, 0.178
batch losses (mrrl, rdl): 0.0, 0.0008324702

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 316
rank avg (pred): 0.042 +- 0.038
mrr vals (pred, true): 0.359, 0.253
batch losses (mrrl, rdl): 0.0, 1.9406e-06

Epoch over!
epoch time: 12.669

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.325 +- 0.263
mrr vals (pred, true): 0.258, 0.003
batch losses (mrrl, rdl): 0.0, 0.000269416

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 601
rank avg (pred): 0.321 +- 0.271
mrr vals (pred, true): 0.319, 0.011
batch losses (mrrl, rdl): 0.0, 1.14051e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 716
rank avg (pred): 0.352 +- 0.272
mrr vals (pred, true): 0.251, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001718454

Epoch over!
epoch time: 11.871

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.039 +- 0.044
mrr vals (pred, true): 0.460, 0.205
batch losses (mrrl, rdl): 0.0, 1.04522e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 862
rank avg (pred): 0.236 +- 0.261
mrr vals (pred, true): 0.396, 0.181
batch losses (mrrl, rdl): 0.0, 0.0005633845

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1089
rank avg (pred): 0.259 +- 0.277
mrr vals (pred, true): 0.392, 0.178
batch losses (mrrl, rdl): 0.0, 0.0005456927

Epoch over!
epoch time: 12.69

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1111
rank avg (pred): 0.249 +- 0.273
mrr vals (pred, true): 0.398, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004781547

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.228 +- 0.269
mrr vals (pred, true): 0.430, 0.101
batch losses (mrrl, rdl): 0.0, 0.0002750855

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 961
rank avg (pred): 0.258 +- 0.285
mrr vals (pred, true): 0.381, 0.003
batch losses (mrrl, rdl): 0.0, 0.0005327493

Epoch over!
epoch time: 12.993

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 269
rank avg (pred): 0.037 +- 0.045
mrr vals (pred, true): 0.521, 0.261
batch losses (mrrl, rdl): 0.0, 8.173e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.260 +- 0.283
mrr vals (pred, true): 0.381, 0.176
batch losses (mrrl, rdl): 0.0, 0.0005150265

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 706
rank avg (pred): 0.332 +- 0.278
mrr vals (pred, true): 0.360, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001826408

Epoch over!
epoch time: 12.468

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 48
rank avg (pred): 0.027 +- 0.030
mrr vals (pred, true): 0.482, 0.192
batch losses (mrrl, rdl): 0.8411076665, 1.48214e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.084 +- 0.071
mrr vals (pred, true): 0.165, 0.186
batch losses (mrrl, rdl): 0.0043401765, 4.70825e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 41
rank avg (pred): 0.060 +- 0.042
mrr vals (pred, true): 0.175, 0.227
batch losses (mrrl, rdl): 0.0274904277, 9.8815e-06

Epoch over!
epoch time: 13.279

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 410
rank avg (pred): 0.400 +- 0.196
mrr vals (pred, true): 0.102, 0.004
batch losses (mrrl, rdl): 0.0275265537, 7.40017e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 341
rank avg (pred): 0.376 +- 0.181
mrr vals (pred, true): 0.100, 0.185
batch losses (mrrl, rdl): 0.0729734227, 0.0015708224

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.417 +- 0.124
mrr vals (pred, true): 0.054, 0.027
batch losses (mrrl, rdl): 0.0001260787, 0.0013696332

Epoch over!
epoch time: 13.82

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1080
rank avg (pred): 0.334 +- 0.179
mrr vals (pred, true): 0.113, 0.175
batch losses (mrrl, rdl): 0.0391111299, 0.0010267344

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.391 +- 0.123
mrr vals (pred, true): 0.059, 0.019
batch losses (mrrl, rdl): 0.0008306359, 0.0007113157

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.325 +- 0.156
mrr vals (pred, true): 0.101, 0.101
batch losses (mrrl, rdl): 1.2343e-06, 0.0007815795

Epoch over!
epoch time: 12.916

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 193
rank avg (pred): 0.329 +- 0.160
mrr vals (pred, true): 0.101, 0.004
batch losses (mrrl, rdl): 0.0264854822, 0.0003587196

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.366 +- 0.105
mrr vals (pred, true): 0.051, 0.011
batch losses (mrrl, rdl): 2.6461e-06, 9.60901e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1094
rank avg (pred): 0.284 +- 0.157
mrr vals (pred, true): 0.133, 0.209
batch losses (mrrl, rdl): 0.0576352291, 0.000879933

Epoch over!
epoch time: 12.774

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.015 +- 0.010
mrr vals (pred, true): 0.234, 0.215
batch losses (mrrl, rdl): 0.0036144271, 2.97481e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1172
rank avg (pred): 0.339 +- 0.107
mrr vals (pred, true): 0.062, 0.011
batch losses (mrrl, rdl): 0.0013605175, 6.2484e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 654
rank avg (pred): 0.343 +- 0.093
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.0009800211, 0.0004325999

Epoch over!
epoch time: 14.827

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 69
rank avg (pred): 0.052 +- 0.031
mrr vals (pred, true): 0.186, 0.173
batch losses (mrrl, rdl): 0.0016132346, 3.6298e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 769
rank avg (pred): 0.289 +- 0.140
mrr vals (pred, true): 0.108, 0.166
batch losses (mrrl, rdl): 0.0335462317, 0.0008078963

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 705
rank avg (pred): 0.332 +- 0.094
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 2.86298e-05, 0.000452329

Epoch over!
epoch time: 13.436

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 542
rank avg (pred): 0.328 +- 0.095
mrr vals (pred, true): 0.054, 0.034
batch losses (mrrl, rdl): 0.0001459301, 0.000651918

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.329 +- 0.083
mrr vals (pred, true): 0.046, 0.003
batch losses (mrrl, rdl): 0.0001688256, 0.0005124154

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 492
rank avg (pred): 0.332 +- 0.083
mrr vals (pred, true): 0.046, 0.017
batch losses (mrrl, rdl): 0.000150373, 0.0002574399

Epoch over!
epoch time: 13.158

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 294
rank avg (pred): 0.017 +- 0.010
mrr vals (pred, true): 0.233, 0.221
batch losses (mrrl, rdl): 0.0015182969, 2.64319e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 466
rank avg (pred): 0.289 +- 0.124
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0213941298, 0.0005891356

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1211
rank avg (pred): 0.317 +- 0.092
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002461291, 0.0005907839

Epoch over!
epoch time: 12.378

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.143 +- 0.088
mrr vals (pred, true): 0.191, 0.188
batch losses (mrrl, rdl): 0.0001247718, 0.0001812377

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 250
rank avg (pred): 0.090 +- 0.054
mrr vals (pred, true): 0.188, 0.180
batch losses (mrrl, rdl): 0.0005926837, 1.4157e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 485
rank avg (pred): 0.286 +- 0.119
mrr vals (pred, true): 0.094, 0.004
batch losses (mrrl, rdl): 0.0192634687, 0.00064177

Epoch over!
epoch time: 12.059

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 295
rank avg (pred): 0.016 +- 0.010
mrr vals (pred, true): 0.234, 0.247
batch losses (mrrl, rdl): 0.0016103478, 1.69579e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 142
rank avg (pred): 0.271 +- 0.125
mrr vals (pred, true): 0.116, 0.129
batch losses (mrrl, rdl): 0.0018302121, 0.0005316149

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 122
rank avg (pred): 0.288 +- 0.117
mrr vals (pred, true): 0.095, 0.175
batch losses (mrrl, rdl): 0.0655200258, 0.0008468769

Epoch over!
epoch time: 13.331

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.314 +- 0.083
mrr vals (pred, true): 0.051, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.05131 	 0.00329 	 m..s
   53 	     1 	 0.09752 	 0.00336 	 m..s
   52 	     2 	 0.09747 	 0.00349 	 m..s
   47 	     3 	 0.09672 	 0.00349 	 m..s
   19 	     4 	 0.05191 	 0.00350 	 m..s
   19 	     5 	 0.05191 	 0.00357 	 m..s
   10 	     6 	 0.05134 	 0.00361 	 m..s
    0 	     7 	 0.05104 	 0.00364 	 m..s
    1 	     8 	 0.05105 	 0.00365 	 m..s
   90 	     9 	 0.11124 	 0.00365 	 MISS
   50 	    10 	 0.09746 	 0.00366 	 m..s
   40 	    11 	 0.09527 	 0.00368 	 m..s
   41 	    12 	 0.09538 	 0.00371 	 m..s
   38 	    13 	 0.09322 	 0.00373 	 m..s
   82 	    14 	 0.10218 	 0.00377 	 m..s
    8 	    15 	 0.05131 	 0.00377 	 m..s
    5 	    16 	 0.05121 	 0.00378 	 m..s
   68 	    17 	 0.09870 	 0.00379 	 m..s
   49 	    18 	 0.09742 	 0.00382 	 m..s
   22 	    19 	 0.05199 	 0.00391 	 m..s
   58 	    20 	 0.09781 	 0.00397 	 m..s
   41 	    21 	 0.09538 	 0.00400 	 m..s
   62 	    22 	 0.09837 	 0.00401 	 m..s
   26 	    23 	 0.05232 	 0.00404 	 m..s
   57 	    24 	 0.09777 	 0.00405 	 m..s
   44 	    25 	 0.09614 	 0.00412 	 m..s
   67 	    26 	 0.09869 	 0.00422 	 m..s
    7 	    27 	 0.05127 	 0.00424 	 m..s
   92 	    28 	 0.11294 	 0.00426 	 MISS
   74 	    29 	 0.09942 	 0.00441 	 m..s
   85 	    30 	 0.10365 	 0.00442 	 m..s
   70 	    31 	 0.09899 	 0.00451 	 m..s
   77 	    32 	 0.10027 	 0.00454 	 m..s
   50 	    33 	 0.09746 	 0.00456 	 m..s
   53 	    34 	 0.09752 	 0.00461 	 m..s
   81 	    35 	 0.10209 	 0.00463 	 m..s
   60 	    36 	 0.09832 	 0.00464 	 m..s
   96 	    37 	 0.12480 	 0.00468 	 MISS
   25 	    38 	 0.05230 	 0.00479 	 m..s
   39 	    39 	 0.09432 	 0.00485 	 m..s
   79 	    40 	 0.10131 	 0.00487 	 m..s
   53 	    41 	 0.09752 	 0.00493 	 m..s
   15 	    42 	 0.05142 	 0.00741 	 m..s
   12 	    43 	 0.05138 	 0.00898 	 m..s
   23 	    44 	 0.05220 	 0.00943 	 m..s
   21 	    45 	 0.05197 	 0.00972 	 m..s
   24 	    46 	 0.05226 	 0.01064 	 m..s
   13 	    47 	 0.05139 	 0.01073 	 m..s
    3 	    48 	 0.05111 	 0.01184 	 m..s
   16 	    49 	 0.05146 	 0.01230 	 m..s
   30 	    50 	 0.05282 	 0.01431 	 m..s
   34 	    51 	 0.05405 	 0.01763 	 m..s
   36 	    52 	 0.05405 	 0.01782 	 m..s
   34 	    53 	 0.05405 	 0.01924 	 m..s
    2 	    54 	 0.05109 	 0.01944 	 m..s
   27 	    55 	 0.05277 	 0.01991 	 m..s
   32 	    56 	 0.05302 	 0.02009 	 m..s
   14 	    57 	 0.05142 	 0.02077 	 m..s
   31 	    58 	 0.05296 	 0.02079 	 m..s
   18 	    59 	 0.05152 	 0.02189 	 ~...
    3 	    60 	 0.05111 	 0.02328 	 ~...
   28 	    61 	 0.05277 	 0.02330 	 ~...
   11 	    62 	 0.05135 	 0.02369 	 ~...
   17 	    63 	 0.05147 	 0.02394 	 ~...
   29 	    64 	 0.05277 	 0.02514 	 ~...
    6 	    65 	 0.05124 	 0.02681 	 ~...
   48 	    66 	 0.09704 	 0.02788 	 m..s
   63 	    67 	 0.09854 	 0.03390 	 m..s
   33 	    68 	 0.05388 	 0.03904 	 ~...
   97 	    69 	 0.13029 	 0.05106 	 m..s
   86 	    70 	 0.10519 	 0.07709 	 ~...
   46 	    71 	 0.09644 	 0.09733 	 ~...
  102 	    72 	 0.16442 	 0.11533 	 m..s
   66 	    73 	 0.09861 	 0.12155 	 ~...
   89 	    74 	 0.11113 	 0.12191 	 ~...
  100 	    75 	 0.16247 	 0.12241 	 m..s
   99 	    76 	 0.16112 	 0.13290 	 ~...
   65 	    77 	 0.09856 	 0.13506 	 m..s
   56 	    78 	 0.09762 	 0.13770 	 m..s
   98 	    79 	 0.16066 	 0.13844 	 ~...
   59 	    80 	 0.09831 	 0.14233 	 m..s
   75 	    81 	 0.09951 	 0.14286 	 m..s
  101 	    82 	 0.16342 	 0.14463 	 ~...
   78 	    83 	 0.10099 	 0.14866 	 m..s
   45 	    84 	 0.09629 	 0.14909 	 m..s
   88 	    85 	 0.10758 	 0.15097 	 m..s
  103 	    86 	 0.16525 	 0.15274 	 ~...
   83 	    87 	 0.10247 	 0.15765 	 m..s
  106 	    88 	 0.20473 	 0.16001 	 m..s
   69 	    89 	 0.09889 	 0.16024 	 m..s
   64 	    90 	 0.09855 	 0.16489 	 m..s
   80 	    91 	 0.10161 	 0.16538 	 m..s
   37 	    92 	 0.09285 	 0.16626 	 m..s
  107 	    93 	 0.20490 	 0.16869 	 m..s
   72 	    94 	 0.09921 	 0.16878 	 m..s
  104 	    95 	 0.20366 	 0.17211 	 m..s
   43 	    96 	 0.09558 	 0.17219 	 m..s
   76 	    97 	 0.09953 	 0.17295 	 m..s
   73 	    98 	 0.09932 	 0.17394 	 m..s
   94 	    99 	 0.11330 	 0.17628 	 m..s
   93 	   100 	 0.11324 	 0.17700 	 m..s
   61 	   101 	 0.09835 	 0.18122 	 m..s
   87 	   102 	 0.10591 	 0.18549 	 m..s
   91 	   103 	 0.11225 	 0.18624 	 m..s
  114 	   104 	 0.21985 	 0.18686 	 m..s
   84 	   105 	 0.10258 	 0.18895 	 m..s
  105 	   106 	 0.20472 	 0.19022 	 ~...
   71 	   107 	 0.09900 	 0.19443 	 m..s
   95 	   108 	 0.11519 	 0.19838 	 m..s
  113 	   109 	 0.21925 	 0.19870 	 ~...
  108 	   110 	 0.20537 	 0.20832 	 ~...
  117 	   111 	 0.23017 	 0.21020 	 ~...
  112 	   112 	 0.21585 	 0.21060 	 ~...
  120 	   113 	 0.23419 	 0.22172 	 ~...
  116 	   114 	 0.23005 	 0.22640 	 ~...
  109 	   115 	 0.20571 	 0.22825 	 ~...
  115 	   116 	 0.21989 	 0.23011 	 ~...
  110 	   117 	 0.20786 	 0.24311 	 m..s
  118 	   118 	 0.23395 	 0.25210 	 ~...
  111 	   119 	 0.20850 	 0.25289 	 m..s
  119 	   120 	 0.23418 	 0.25626 	 ~...
==========================================
r_mrr = 0.7531763315200806
r2_mrr = 0.45497047901153564
spearmanr_mrr@5 = 0.9003105759620667
spearmanr_mrr@10 = 0.9389479756355286
spearmanr_mrr@50 = 0.9196819067001343
spearmanr_mrr@100 = 0.8369614481925964
spearmanr_mrr@All = 0.8662901520729065
==========================================
test time: 0.419
Done Testing dataset CoDExSmall
total time taken: 206.18667912483215
training time taken: 195.16101217269897
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7532)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4550)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9003)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9389)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9197)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8370)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8663)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.2205579031506204}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 4674981592856998
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1094, 455, 634, 32, 805, 730, 1050, 521, 278, 548, 268, 582, 594, 1174, 282, 985, 835, 649, 437, 457, 247, 188, 16, 952, 321, 855, 839, 1056, 734, 223, 930, 957, 1128, 625, 754, 1171, 945, 253, 1061, 80, 1175, 490, 655, 688, 811, 436, 320, 769, 452, 748, 774, 23, 1083, 1199, 825, 962, 991, 1054, 210, 1036, 19, 803, 653, 54, 943, 219, 259, 164, 102, 1135, 326, 145, 135, 1091, 143, 648, 37, 737, 224, 402, 958, 609, 872, 875, 533, 194, 349, 412, 818, 876, 144, 126, 1210, 715, 520, 561, 48, 979, 44, 1071, 617, 392, 1129, 106, 1102, 687, 168, 63, 492, 611, 502, 200, 903, 67, 1108, 1185, 824, 963, 986, 852, 701]
valid_ids (0): []
train_ids (1094): [242, 505, 564, 707, 435, 186, 311, 937, 865, 211, 421, 927, 178, 571, 418, 801, 753, 451, 526, 131, 196, 886, 447, 104, 1213, 971, 1113, 1006, 516, 692, 420, 816, 1131, 263, 355, 397, 14, 81, 1153, 831, 430, 140, 553, 411, 303, 925, 458, 351, 363, 93, 929, 1138, 1205, 432, 993, 907, 842, 42, 125, 1028, 1200, 1162, 751, 236, 433, 550, 21, 1154, 951, 348, 319, 1012, 1003, 441, 731, 239, 959, 75, 1074, 1045, 1209, 1164, 938, 503, 598, 1189, 782, 644, 174, 454, 255, 1151, 820, 563, 86, 627, 25, 271, 884, 95, 1157, 328, 39, 325, 1019, 1063, 1041, 79, 777, 861, 274, 2, 926, 470, 792, 132, 877, 916, 269, 826, 409, 403, 758, 723, 1110, 1156, 551, 356, 542, 401, 335, 700, 58, 92, 755, 440, 148, 66, 983, 973, 1053, 127, 1097, 515, 17, 666, 339, 620, 994, 336, 950, 804, 786, 394, 874, 49, 797, 785, 672, 531, 491, 212, 36, 696, 918, 873, 361, 374, 385, 89, 362, 749, 84, 391, 1166, 756, 152, 1121, 765, 301, 1069, 387, 159, 915, 346, 76, 1072, 989, 449, 988, 133, 156, 752, 3, 358, 583, 848, 545, 1000, 1095, 966, 379, 60, 371, 307, 844, 225, 608, 718, 243, 373, 1105, 7, 558, 287, 750, 539, 911, 31, 171, 395, 712, 262, 713, 1120, 234, 899, 427, 389, 469, 162, 535, 249, 673, 509, 1186, 961, 96, 431, 297, 182, 1158, 261, 726, 497, 97, 784, 871, 709, 15, 475, 536, 1117, 512, 357, 1150, 773, 1109, 540, 187, 960, 575, 496, 322, 481, 460, 87, 300, 304, 645, 1192, 290, 313, 256, 474, 632, 1112, 921, 172, 283, 1184, 552, 935, 1015, 947, 114, 725, 849, 612, 997, 870, 13, 312, 549, 1084, 1143, 1134, 103, 1139, 710, 636, 1132, 1089, 791, 264, 588, 1022, 488, 352, 1170, 9, 590, 891, 1197, 1020, 589, 577, 316, 101, 484, 689, 837, 417, 879, 847, 888, 864, 614, 1191, 585, 123, 284, 1034, 828, 708, 621, 459, 1079, 508, 288, 91, 1060, 822, 821, 670, 980, 604, 694, 560, 354, 344, 579, 112, 939, 154, 191, 28, 906, 534, 965, 856, 982, 1073, 578, 1206, 808, 350, 453, 257, 1093, 142, 615, 177, 169, 942, 173, 1203, 1127, 158, 829, 424, 82, 368, 817, 833, 461, 838, 720, 221, 794, 668, 139, 525, 775, 439, 895, 1011, 1141, 55, 853, 832, 678, 912, 57, 719, 1031, 203, 867, 622, 468, 727, 429, 841, 428, 1090, 562, 1165, 665, 650, 546, 176, 630, 381, 664, 109, 823, 228, 592, 423, 796, 1013, 56, 1133, 862, 232, 308, 364, 717, 799, 1049, 43, 1194, 506, 721, 631, 359, 1195, 334, 778, 149, 170, 651, 759, 776, 64, 414, 969, 462, 372, 111, 1122, 834, 685, 98, 11, 105, 1123, 541, 595, 586, 479, 1046, 342, 519, 619, 568, 810, 52, 1145, 556, 1002, 527, 341, 944, 110, 1008, 207, 466, 1027, 624, 998, 587, 1208, 760, 296, 663, 972, 260, 956, 473, 347, 254, 258, 222, 893, 779, 332, 883, 857, 1116, 377, 854, 602, 978, 216, 107, 1004, 443, 868, 416, 0, 742, 405, 662, 370, 33, 375, 714, 680, 1037, 26, 1106, 528, 738, 465, 1100, 908, 291, 547, 1058, 1142, 641, 941, 442, 252, 46, 1052, 18, 380, 1182, 658, 62, 798, 900, 121, 1086, 669, 1196, 1099, 345, 317, 1017, 529, 314, 1029, 30, 1068, 119, 554, 74, 635, 330, 887, 286, 147, 559, 65, 1025, 472, 984, 1009, 596, 1059, 281, 613, 1026, 677, 691, 338, 1092, 600, 892, 740, 360, 426, 241, 607, 530, 728, 1124, 859, 767, 977, 309, 122, 165, 1207, 1001, 61, 1169, 995, 836, 100, 647, 1148, 659, 230, 1188, 815, 1152, 214, 298, 1146, 279, 967, 1064, 1, 894, 217, 250, 643, 711, 12, 914, 642, 686, 498, 1103, 456, 922, 936, 202, 757, 1193, 408, 819, 679, 201, 406, 783, 369, 494, 40, 59, 398, 523, 4, 761, 576, 266, 566, 276, 766, 1160, 486, 538, 1168, 806, 476, 1038, 990, 763, 681, 499, 167, 569, 273, 1030, 981, 137, 231, 999, 795, 809, 199, 1167, 1107, 869, 606, 1098, 206, 716, 628, 1062, 388, 415, 733, 1190, 160, 118, 20, 337, 1178, 108, 640, 293, 878, 964, 99, 1136, 860, 190, 919, 882, 928, 1075, 1078, 1126, 544, 968, 1048, 299, 573, 386, 504, 400, 524, 846, 146, 1085, 580, 830, 248, 514, 904, 1005, 446, 1163, 850, 574, 1070, 845, 932, 764, 204, 365, 73, 41, 699, 690, 1111, 889, 781, 34, 1080, 975, 53, 866, 557, 485, 584, 272, 205, 471, 1082, 616, 376, 445, 917, 450, 136, 5, 1101, 772, 565, 195, 1076, 949, 1179, 150, 789, 605, 1137, 88, 1014, 923, 1202, 1214, 94, 741, 410, 909, 657, 814, 323, 863, 682, 209, 555, 438, 378, 27, 702, 115, 185, 157, 275, 183, 277, 623, 141, 1155, 310, 1043, 601, 218, 396, 1119, 116, 517, 913, 1147, 654, 652, 638, 50, 639, 735, 1066, 593, 1176, 629, 464, 413, 384, 113, 744, 1055, 366, 898, 992, 47, 955, 610, 1149, 1067, 189, 1183, 480, 151, 1039, 745, 353, 77, 184, 934, 463, 896, 1010, 280, 245, 885, 793, 770, 390, 138, 38, 180, 404, 974, 467, 768, 1024, 1042, 603, 237, 10, 448, 661, 851, 697, 787, 129, 289, 425, 724, 1035, 626, 591, 1130, 567, 1087, 161, 1161, 128, 570, 705, 843, 513, 1114, 1177, 996, 208, 483, 572, 660, 324, 240, 532, 407, 1051, 1104, 698, 71, 905, 45, 294, 920, 130, 790, 1172, 646, 1187, 124, 1023, 1140, 743, 1032, 444, 477, 285, 70, 671, 193, 1096, 976, 722, 746, 970, 175, 306, 72, 198, 1040, 265, 840, 802, 500, 1088, 1057, 134, 8, 902, 910, 736, 305, 315, 890, 238, 597, 478, 85, 693, 543, 181, 1159, 729, 827, 215, 226, 762, 1144, 90, 434, 333, 518, 1212, 51, 192, 706, 1081, 329, 327, 501, 674, 940, 1016, 270, 618, 511, 489, 1118, 780, 788, 897, 179, 1033, 880, 1181, 487, 704, 771, 522, 24, 399, 419, 343, 302, 68, 1211, 220, 382, 924, 684, 1047, 695, 656, 267, 166, 1018, 675, 1007, 197, 954, 739, 948, 931, 495, 510, 295, 858, 69, 1044, 812, 229, 732, 340, 1065, 1021, 227, 244, 683, 537, 987, 953, 637, 1180, 251, 881, 29, 235, 83, 813, 393, 581, 901, 383, 1077, 482, 507, 747, 946, 676, 667, 292, 633, 120, 800, 1201, 6, 331, 22, 1125, 153, 318, 422, 933, 703, 233, 213, 493, 78, 163, 1115, 367, 1204, 246, 35, 155, 807, 599, 1173, 117, 1198]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4119288082181835
the save name prefix for this run is:  chkpt-ID_4119288082181835_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.522 +- 0.007
mrr vals (pred, true): 0.001, 0.238
batch losses (mrrl, rdl): 0.0, 0.0048692091

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1141
rank avg (pred): 0.122 +- 0.107
mrr vals (pred, true): 0.220, 0.022
batch losses (mrrl, rdl): 0.0, 6.45454e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.291 +- 0.252
mrr vals (pred, true): 0.206, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003874188

Epoch over!
epoch time: 12.963

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 206
rank avg (pred): 0.291 +- 0.260
mrr vals (pred, true): 0.260, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004447312

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.246 +- 0.243
mrr vals (pred, true): 0.344, 0.130
batch losses (mrrl, rdl): 0.0, 0.0003722447

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 272
rank avg (pred): 0.056 +- 0.057
mrr vals (pred, true): 0.377, 0.189
batch losses (mrrl, rdl): 0.0, 6.7491e-06

Epoch over!
epoch time: 12.49

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 580
rank avg (pred): 0.334 +- 0.317
mrr vals (pred, true): 0.319, 0.012
batch losses (mrrl, rdl): 0.0, 9.04019e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.266 +- 0.268
mrr vals (pred, true): 0.357, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004812184

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 960
rank avg (pred): 0.254 +- 0.261
mrr vals (pred, true): 0.332, 0.005
batch losses (mrrl, rdl): 0.0, 0.0006173725

Epoch over!
epoch time: 13.281

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.227 +- 0.248
mrr vals (pred, true): 0.352, 0.174
batch losses (mrrl, rdl): 0.0, 0.000526806

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.254 +- 0.260
mrr vals (pred, true): 0.333, 0.199
batch losses (mrrl, rdl): 0.0, 0.0007577292

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.247 +- 0.260
mrr vals (pred, true): 0.340, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005442846

Epoch over!
epoch time: 12.264

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.253 +- 0.265
mrr vals (pred, true): 0.357, 0.163
batch losses (mrrl, rdl): 0.0, 0.0006592032

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 332
rank avg (pred): 0.250 +- 0.270
mrr vals (pred, true): 0.378, 0.168
batch losses (mrrl, rdl): 0.0, 0.0003783539

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.242 +- 0.264
mrr vals (pred, true): 0.402, 0.125
batch losses (mrrl, rdl): 0.0, 0.0002575319

Epoch over!
epoch time: 13.974

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.063 +- 0.068
mrr vals (pred, true): 0.407, 0.186
batch losses (mrrl, rdl): 0.4887103438, 1.39395e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.296, 0.244
batch losses (mrrl, rdl): 0.0265026204, 3.37515e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.451 +- 0.133
mrr vals (pred, true): 0.028, 0.014
batch losses (mrrl, rdl): 0.004951328, 0.0004835462

Epoch over!
epoch time: 13.057

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 827
rank avg (pred): 0.080 +- 0.041
mrr vals (pred, true): 0.151, 0.194
batch losses (mrrl, rdl): 0.0188723691, 2.08e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.355 +- 0.158
mrr vals (pred, true): 0.092, 0.004
batch losses (mrrl, rdl): 0.0175216869, 0.0002364611

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.379 +- 0.124
mrr vals (pred, true): 0.053, 0.022
batch losses (mrrl, rdl): 6.35963e-05, 0.0002186448

Epoch over!
epoch time: 13.647

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.360 +- 0.148
mrr vals (pred, true): 0.089, 0.098
batch losses (mrrl, rdl): 0.0148895886, 0.0011000946

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.275, 0.230
batch losses (mrrl, rdl): 0.0198312439, 2.39516e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.303 +- 0.146
mrr vals (pred, true): 0.114, 0.004
batch losses (mrrl, rdl): 0.0407431684, 0.0005244037

Epoch over!
epoch time: 13.292

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 940
rank avg (pred): 0.303 +- 0.144
mrr vals (pred, true): 0.119, 0.179
batch losses (mrrl, rdl): 0.0355677716, 0.001014548

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 659
rank avg (pred): 0.384 +- 0.140
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0005556429, 0.0001659714

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 810
rank avg (pred): 0.287 +- 0.148
mrr vals (pred, true): 0.118, 0.123
batch losses (mrrl, rdl): 0.0001922791, 0.0006251343

Epoch over!
epoch time: 12.557

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 244
rank avg (pred): 0.046 +- 0.025
mrr vals (pred, true): 0.188, 0.172
batch losses (mrrl, rdl): 0.0025915306, 1.98218e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.350 +- 0.123
mrr vals (pred, true): 0.047, 0.018
batch losses (mrrl, rdl): 7.33911e-05, 0.0003808563

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.337 +- 0.125
mrr vals (pred, true): 0.055, 0.020
batch losses (mrrl, rdl): 0.0002713138, 0.0004042862

Epoch over!
epoch time: 12.291

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 35
rank avg (pred): 0.012 +- 0.006
mrr vals (pred, true): 0.220, 0.207
batch losses (mrrl, rdl): 0.0016965452, 2.54281e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.326 +- 0.106
mrr vals (pred, true): 0.047, 0.023
batch losses (mrrl, rdl): 0.0001010258, 9.31914e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.327 +- 0.114
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 3.6094e-06, 0.0005725319

Epoch over!
epoch time: 12.314

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1152
rank avg (pred): 0.328 +- 0.129
mrr vals (pred, true): 0.059, 0.026
batch losses (mrrl, rdl): 0.0007678463, 0.0005756729

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.311 +- 0.097
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 0.000106872, 0.0006524181

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.265 +- 0.129
mrr vals (pred, true): 0.120, 0.005
batch losses (mrrl, rdl): 0.0488309264, 0.0007611673

Epoch over!
epoch time: 11.931

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 78
rank avg (pred): 0.023 +- 0.012
mrr vals (pred, true): 0.192, 0.182
batch losses (mrrl, rdl): 0.0009667105, 2.71607e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.330 +- 0.131
mrr vals (pred, true): 0.052, 0.009
batch losses (mrrl, rdl): 4.15692e-05, 3.53947e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 334
rank avg (pred): 0.288 +- 0.124
mrr vals (pred, true): 0.105, 0.153
batch losses (mrrl, rdl): 0.0230922159, 0.0006074411

Epoch over!
epoch time: 12.842

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.269 +- 0.123
mrr vals (pred, true): 0.116, 0.174
batch losses (mrrl, rdl): 0.0337883569, 0.0006991664

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1021
rank avg (pred): 0.259 +- 0.131
mrr vals (pred, true): 0.122, 0.202
batch losses (mrrl, rdl): 0.0627874285, 0.0006434715

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.284 +- 0.113
mrr vals (pred, true): 0.096, 0.026
batch losses (mrrl, rdl): 0.0211297069, 0.000206466

Epoch over!
epoch time: 12.6

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.345 +- 0.168
mrr vals (pred, true): 0.050, 0.018
batch losses (mrrl, rdl): 1.1676e-06, 0.0003199761

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.297 +- 0.136
mrr vals (pred, true): 0.090, 0.005
batch losses (mrrl, rdl): 0.0160035193, 0.0005875421

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 619
rank avg (pred): 0.338 +- 0.169
mrr vals (pred, true): 0.044, 0.012
batch losses (mrrl, rdl): 0.0003089628, 3.30677e-05

Epoch over!
epoch time: 12.596

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.252 +- 0.122
mrr vals (pred, true): 0.122, 0.209

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04925 	 0.00327 	 m..s
   26 	     1 	 0.06151 	 0.00327 	 m..s
    1 	     2 	 0.04661 	 0.00329 	 m..s
   28 	     3 	 0.06274 	 0.00352 	 m..s
   72 	     4 	 0.12142 	 0.00363 	 MISS
   68 	     5 	 0.12041 	 0.00363 	 MISS
   79 	     6 	 0.12511 	 0.00365 	 MISS
   58 	     7 	 0.11580 	 0.00372 	 MISS
   49 	     8 	 0.10793 	 0.00373 	 MISS
   55 	     9 	 0.11436 	 0.00376 	 MISS
   51 	    10 	 0.11125 	 0.00377 	 MISS
   53 	    11 	 0.11271 	 0.00378 	 MISS
   66 	    12 	 0.11880 	 0.00378 	 MISS
   13 	    13 	 0.05210 	 0.00378 	 m..s
    2 	    14 	 0.04748 	 0.00383 	 m..s
   81 	    15 	 0.12874 	 0.00383 	 MISS
   31 	    16 	 0.08376 	 0.00387 	 m..s
   34 	    17 	 0.08627 	 0.00389 	 m..s
   80 	    18 	 0.12694 	 0.00389 	 MISS
   32 	    19 	 0.08397 	 0.00390 	 m..s
   43 	    20 	 0.09847 	 0.00391 	 m..s
   64 	    21 	 0.11843 	 0.00401 	 MISS
   37 	    22 	 0.09067 	 0.00409 	 m..s
   17 	    23 	 0.05256 	 0.00409 	 m..s
   42 	    24 	 0.09800 	 0.00412 	 m..s
   48 	    25 	 0.10781 	 0.00413 	 MISS
   77 	    26 	 0.12440 	 0.00415 	 MISS
   46 	    27 	 0.10524 	 0.00415 	 MISS
    5 	    28 	 0.04815 	 0.00416 	 m..s
   60 	    29 	 0.11773 	 0.00422 	 MISS
   15 	    30 	 0.05219 	 0.00424 	 m..s
   63 	    31 	 0.11805 	 0.00437 	 MISS
    2 	    32 	 0.04748 	 0.00437 	 m..s
   62 	    33 	 0.11797 	 0.00446 	 MISS
   50 	    34 	 0.10920 	 0.00453 	 MISS
   72 	    35 	 0.12142 	 0.00457 	 MISS
   83 	    36 	 0.13469 	 0.00468 	 MISS
   40 	    37 	 0.09336 	 0.00471 	 m..s
   56 	    38 	 0.11503 	 0.00486 	 MISS
   77 	    39 	 0.12440 	 0.00497 	 MISS
   81 	    40 	 0.12874 	 0.00508 	 MISS
   16 	    41 	 0.05251 	 0.00696 	 m..s
    0 	    42 	 0.04620 	 0.00822 	 m..s
   19 	    43 	 0.05340 	 0.00863 	 m..s
   24 	    44 	 0.06129 	 0.00892 	 m..s
   22 	    45 	 0.05560 	 0.00995 	 m..s
   18 	    46 	 0.05300 	 0.01044 	 m..s
   24 	    47 	 0.06129 	 0.01162 	 m..s
   27 	    48 	 0.06266 	 0.01204 	 m..s
   14 	    49 	 0.05215 	 0.01264 	 m..s
   10 	    50 	 0.05002 	 0.01630 	 m..s
    7 	    51 	 0.04888 	 0.01678 	 m..s
    9 	    52 	 0.04980 	 0.01719 	 m..s
    6 	    53 	 0.04852 	 0.01763 	 m..s
   11 	    54 	 0.05062 	 0.02168 	 ~...
    4 	    55 	 0.04812 	 0.02257 	 ~...
   29 	    56 	 0.06278 	 0.02427 	 m..s
   12 	    57 	 0.05127 	 0.02563 	 ~...
   23 	    58 	 0.05780 	 0.03538 	 ~...
   20 	    59 	 0.05451 	 0.03688 	 ~...
   21 	    60 	 0.05452 	 0.03764 	 ~...
   88 	    61 	 0.15754 	 0.05027 	 MISS
   38 	    62 	 0.09089 	 0.09818 	 ~...
   36 	    63 	 0.08921 	 0.10935 	 ~...
   87 	    64 	 0.15189 	 0.11143 	 m..s
   30 	    65 	 0.07866 	 0.11325 	 m..s
   33 	    66 	 0.08456 	 0.11951 	 m..s
   84 	    67 	 0.13668 	 0.11981 	 ~...
   61 	    68 	 0.11785 	 0.12862 	 ~...
   90 	    69 	 0.16382 	 0.13270 	 m..s
   85 	    70 	 0.14504 	 0.13286 	 ~...
   35 	    71 	 0.08812 	 0.13539 	 m..s
   44 	    72 	 0.09938 	 0.13746 	 m..s
   41 	    73 	 0.09703 	 0.14083 	 m..s
   89 	    74 	 0.16220 	 0.14165 	 ~...
   59 	    75 	 0.11625 	 0.15261 	 m..s
   93 	    76 	 0.18072 	 0.15280 	 ~...
   69 	    77 	 0.12063 	 0.15688 	 m..s
   86 	    78 	 0.14697 	 0.15806 	 ~...
   52 	    79 	 0.11196 	 0.15954 	 m..s
   65 	    80 	 0.11874 	 0.16554 	 m..s
   45 	    81 	 0.10012 	 0.16573 	 m..s
   67 	    82 	 0.11881 	 0.16854 	 m..s
  102 	    83 	 0.21302 	 0.16954 	 m..s
   39 	    84 	 0.09204 	 0.17107 	 m..s
   71 	    85 	 0.12070 	 0.17628 	 m..s
   95 	    86 	 0.18369 	 0.17631 	 ~...
   57 	    87 	 0.11549 	 0.17696 	 m..s
   47 	    88 	 0.10756 	 0.17811 	 m..s
  107 	    89 	 0.23597 	 0.17958 	 m..s
   70 	    90 	 0.12067 	 0.18278 	 m..s
   92 	    91 	 0.17535 	 0.18518 	 ~...
   76 	    92 	 0.12434 	 0.18550 	 m..s
   94 	    93 	 0.18293 	 0.18784 	 ~...
  111 	    94 	 0.24112 	 0.19071 	 m..s
   96 	    95 	 0.19492 	 0.19186 	 ~...
   91 	    96 	 0.17509 	 0.19334 	 ~...
  110 	    97 	 0.23984 	 0.19574 	 m..s
  101 	    98 	 0.21246 	 0.19856 	 ~...
   97 	    99 	 0.19923 	 0.19914 	 ~...
   54 	   100 	 0.11415 	 0.20016 	 m..s
  108 	   101 	 0.23663 	 0.20033 	 m..s
   75 	   102 	 0.12223 	 0.20043 	 m..s
   98 	   103 	 0.20465 	 0.20488 	 ~...
  100 	   104 	 0.20474 	 0.20509 	 ~...
   99 	   105 	 0.20469 	 0.20672 	 ~...
  109 	   106 	 0.23791 	 0.20774 	 m..s
   74 	   107 	 0.12213 	 0.20861 	 m..s
  103 	   108 	 0.22025 	 0.21490 	 ~...
  103 	   109 	 0.22025 	 0.22583 	 ~...
  117 	   110 	 0.26724 	 0.22892 	 m..s
  115 	   111 	 0.24593 	 0.22941 	 ~...
  105 	   112 	 0.23327 	 0.23043 	 ~...
  112 	   113 	 0.24381 	 0.23291 	 ~...
  116 	   114 	 0.25787 	 0.23323 	 ~...
  106 	   115 	 0.23334 	 0.24444 	 ~...
  113 	   116 	 0.24454 	 0.24540 	 ~...
  113 	   117 	 0.24454 	 0.25288 	 ~...
  118 	   118 	 0.29558 	 0.25993 	 m..s
  119 	   119 	 0.30203 	 0.26222 	 m..s
  120 	   120 	 0.31378 	 0.26713 	 m..s
==========================================
r_mrr = 0.7903177738189697
r2_mrr = 0.48592984676361084
spearmanr_mrr@5 = 0.9833868741989136
spearmanr_mrr@10 = 0.9489412307739258
spearmanr_mrr@50 = 0.972775936126709
spearmanr_mrr@100 = 0.8873422741889954
spearmanr_mrr@All = 0.9103894829750061
==========================================
test time: 0.393
Done Testing dataset CoDExSmall
total time taken: 201.95422863960266
training time taken: 192.56485223770142
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7903)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4859)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9834)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9489)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9728)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8873)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.9104)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.395153870286549}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 782163540610260
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [961, 1011, 423, 1214, 1134, 383, 731, 1088, 1033, 394, 405, 269, 288, 898, 284, 795, 57, 586, 1017, 715, 505, 957, 519, 188, 707, 363, 848, 1132, 655, 403, 1019, 964, 670, 354, 913, 679, 14, 1199, 246, 613, 75, 178, 609, 139, 53, 52, 420, 976, 607, 908, 63, 339, 28, 1168, 989, 148, 979, 1201, 1111, 1167, 1122, 765, 1094, 949, 545, 1062, 1014, 435, 897, 772, 820, 793, 769, 82, 464, 1182, 1200, 250, 201, 697, 636, 278, 554, 238, 174, 1181, 470, 382, 1048, 991, 198, 583, 921, 945, 969, 786, 350, 1086, 774, 604, 199, 813, 1056, 140, 404, 630, 1003, 313, 1096, 451, 736, 422, 168, 682, 1149, 625, 664, 1142, 584, 181, 928]
valid_ids (0): []
train_ids (1094): [778, 955, 428, 34, 706, 13, 341, 1150, 296, 657, 230, 860, 98, 388, 1117, 766, 1080, 113, 1020, 623, 321, 80, 754, 1040, 292, 656, 216, 557, 639, 177, 1126, 789, 650, 1022, 1084, 40, 1043, 783, 755, 833, 749, 1119, 771, 830, 709, 193, 906, 693, 540, 995, 610, 454, 853, 516, 775, 56, 746, 959, 315, 15, 640, 626, 575, 307, 819, 1058, 366, 926, 294, 910, 1068, 668, 1093, 1118, 1206, 184, 498, 131, 938, 161, 412, 525, 1202, 924, 834, 179, 305, 561, 94, 1187, 227, 1103, 143, 993, 396, 816, 149, 1038, 169, 548, 432, 476, 934, 1016, 565, 990, 433, 1175, 351, 543, 718, 716, 306, 888, 665, 605, 24, 727, 735, 681, 1082, 417, 68, 799, 20, 91, 779, 598, 880, 37, 580, 197, 239, 153, 1045, 488, 266, 323, 806, 458, 868, 684, 62, 588, 751, 532, 45, 962, 343, 745, 1140, 145, 67, 603, 915, 480, 978, 1095, 495, 539, 358, 517, 499, 787, 1049, 105, 47, 332, 1107, 276, 566, 494, 1197, 36, 1083, 801, 644, 634, 1037, 275, 429, 7, 667, 688, 1015, 64, 1211, 933, 975, 815, 373, 1108, 1114, 151, 570, 1030, 289, 8, 130, 245, 175, 431, 837, 265, 96, 791, 273, 418, 1063, 796, 846, 861, 182, 32, 556, 214, 948, 981, 881, 914, 1021, 4, 911, 828, 1079, 522, 643, 2, 160, 974, 364, 812, 48, 792, 564, 460, 635, 869, 1113, 1121, 591, 42, 274, 1059, 1002, 734, 61, 982, 695, 112, 425, 389, 615, 1141, 807, 1099, 106, 618, 845, 43, 1191, 484, 549, 259, 11, 980, 157, 407, 300, 89, 1067, 713, 426, 362, 1208, 9, 16, 1024, 1209, 672, 243, 1018, 21, 659, 1129, 507, 577, 205, 691, 673, 491, 46, 864, 10, 996, 967, 386, 1055, 1213, 58, 529, 568, 109, 817, 527, 559, 666, 1092, 99, 100, 633, 882, 759, 78, 1130, 704, 855, 165, 747, 919, 1210, 832, 41, 1013, 600, 946, 1073, 760, 825, 541, 1075, 122, 395, 23, 553, 249, 999, 1098, 1177, 863, 497, 866, 329, 1158, 838, 937, 973, 1179, 271, 279, 1065, 1131, 803, 741, 459, 73, 349, 1186, 352, 381, 1036, 430, 805, 263, 689, 638, 87, 1171, 652, 1029, 518, 208, 1138, 1166, 572, 137, 827, 537, 456, 224, 101, 524, 753, 714, 449, 84, 374, 965, 1164, 761, 858, 242, 879, 69, 687, 282, 737, 573, 1178, 852, 79, 406, 324, 642, 723, 1051, 653, 763, 378, 947, 475, 55, 984, 22, 490, 253, 277, 876, 1154, 597, 357, 302, 1101, 71, 171, 38, 121, 1185, 331, 361, 896, 1105, 1143, 654, 1136, 402, 411, 611, 337, 146, 486, 399, 647, 255, 267, 283, 936, 297, 1120, 878, 442, 445, 930, 211, 994, 322, 1147, 700, 508, 138, 225, 685, 206, 166, 1115, 5, 602, 1035, 671, 83, 338, 1112, 397, 25, 1031, 463, 546, 385, 1026, 851, 943, 85, 295, 1077, 788, 972, 1100, 50, 489, 621, 971, 773, 680, 1046, 849, 1133, 103, 708, 986, 455, 675, 348, 102, 187, 1041, 528, 599, 628, 1137, 86, 1123, 590, 400, 1050, 207, 1076, 234, 334, 54, 530, 578, 1102, 195, 30, 1006, 220, 501, 1146, 236, 790, 892, 493, 416, 823, 927, 387, 506, 172, 200, 309, 514, 1189, 123, 762, 562, 438, 932, 756, 164, 465, 900, 658, 439, 1023, 750, 93, 134, 632, 854, 421, 126, 478, 369, 1170, 764, 916, 931, 585, 108, 370, 677, 631, 894, 612, 446, 574, 1087, 344, 468, 739, 88, 1061, 136, 159, 152, 19, 509, 39, 1160, 441, 885, 325, 60, 147, 1151, 533, 1124, 992, 721, 162, 1116, 717, 390, 440, 641, 857, 1004, 257, 340, 701, 163, 90, 368, 535, 232, 595, 998, 794, 606, 479, 264, 76, 678, 258, 954, 436, 1057, 719, 191, 204, 808, 326, 744, 142, 1139, 784, 150, 115, 1027, 167, 536, 218, 301, 840, 346, 119, 821, 608, 738, 569, 434, 987, 550, 1091, 733, 170, 596, 49, 720, 1, 1128, 120, 1001, 155, 593, 558, 376, 538, 291, 1097, 31, 699, 237, 829, 614, 192, 674, 124, 582, 877, 1145, 551, 342, 314, 758, 29, 345, 408, 127, 547, 141, 1039, 213, 690, 781, 710, 809, 893, 144, 229, 1012, 33, 116, 917, 703, 627, 272, 859, 1205, 299, 59, 251, 355, 335, 1053, 477, 1127, 645, 1165, 1009, 377, 929, 114, 902, 310, 826, 26, 409, 871, 901, 183, 462, 443, 353, 581, 712, 474, 552, 622, 380, 414, 316, 576, 180, 542, 132, 104, 483, 1204, 240, 1194, 466, 843, 190, 447, 1081, 1078, 1109, 392, 1176, 970, 173, 359, 696, 44, 886, 905, 317, 1106, 874, 1089, 887, 95, 1044, 722, 333, 1025, 1148, 209, 1192, 17, 935, 1000, 960, 336, 944, 918, 70, 1007, 320, 770, 850, 616, 752, 776, 1005, 940, 836, 285, 814, 555, 117, 873, 268, 683, 521, 624, 379, 473, 822, 244, 217, 247, 1196, 308, 966, 0, 711, 968, 281, 1161, 619, 223, 798, 1157, 202, 452, 1174, 444, 215, 904, 424, 110, 780, 222, 286, 523, 365, 154, 1054, 496, 977, 469, 1028, 895, 694, 676, 492, 398, 811, 637, 129, 221, 865, 298, 920, 958, 629, 92, 419, 330, 1090, 1184, 18, 125, 740, 729, 824, 1085, 233, 728, 939, 504, 1042, 1069, 328, 768, 360, 1070, 1010, 617, 601, 802, 724, 1104, 472, 1032, 1212, 810, 1066, 72, 620, 692, 81, 1159, 111, 1125, 1110, 903, 841, 1173, 950, 27, 482, 312, 196, 587, 800, 923, 1153, 280, 189, 467, 579, 1047, 203, 226, 212, 1072, 77, 371, 186, 648, 883, 804, 726, 231, 662, 1064, 3, 870, 1155, 646, 1188, 907, 74, 66, 391, 453, 797, 875, 862, 515, 97, 256, 592, 777, 1183, 303, 649, 457, 210, 651, 367, 318, 393, 767, 988, 1071, 1163, 372, 481, 867, 589, 663, 531, 1207, 1172, 831, 942, 261, 241, 65, 35, 1074, 884, 563, 705, 951, 1060, 953, 660, 725, 952, 133, 912, 156, 889, 135, 6, 842, 571, 560, 375, 327, 856, 1008, 1203, 304, 311, 260, 839, 118, 743, 963, 1169, 997, 427, 520, 899, 262, 384, 526, 248, 270, 835, 235, 544, 983, 922, 485, 128, 1052, 12, 410, 941, 1162, 185, 500, 785, 502, 293, 252, 319, 219, 461, 290, 1144, 513, 1198, 487, 471, 511, 534, 698, 702, 107, 450, 872, 512, 158, 1180, 415, 1190, 818, 661, 730, 1152, 228, 356, 401, 1156, 347, 891, 985, 890, 757, 567, 254, 956, 686, 194, 503, 413, 448, 287, 844, 742, 925, 782, 1135, 51, 748, 1195, 1034, 1193, 669, 510, 847, 594, 732, 437, 176, 909]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5560990120470893
the save name prefix for this run is:  chkpt-ID_5560990120470893_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 573
rank avg (pred): 0.529 +- 0.009
mrr vals (pred, true): 0.001, 0.007
batch losses (mrrl, rdl): 0.0, 0.0007597

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 826
rank avg (pred): 0.103 +- 0.062
mrr vals (pred, true): 0.106, 0.190
batch losses (mrrl, rdl): 0.0, 5.90358e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1043
rank avg (pred): 0.297 +- 0.235
mrr vals (pred, true): 0.261, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003037088

Epoch over!
epoch time: 12.769

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 424
rank avg (pred): 0.271 +- 0.218
mrr vals (pred, true): 0.292, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005542189

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.078 +- 0.067
mrr vals (pred, true): 0.372, 0.179
batch losses (mrrl, rdl): 0.0, 1.4167e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.277 +- 0.243
mrr vals (pred, true): 0.375, 0.204
batch losses (mrrl, rdl): 0.0, 0.0009043427

Epoch over!
epoch time: 12.761

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1125
rank avg (pred): 0.277 +- 0.244
mrr vals (pred, true): 0.378, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005008407

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.339 +- 0.303
mrr vals (pred, true): 0.388, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001848795

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 965
rank avg (pred): 0.275 +- 0.249
mrr vals (pred, true): 0.401, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004646659

Epoch over!
epoch time: 11.867

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 917
rank avg (pred): 0.085 +- 0.077
mrr vals (pred, true): 0.415, 0.151
batch losses (mrrl, rdl): 0.0, 4.05312e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.274 +- 0.250
mrr vals (pred, true): 0.407, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004266712

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1150
rank avg (pred): 0.162 +- 0.154
mrr vals (pred, true): 0.425, 0.025
batch losses (mrrl, rdl): 0.0, 7.8599e-06

Epoch over!
epoch time: 12.205

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.256 +- 0.243
mrr vals (pred, true): 0.414, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005510522

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 331
rank avg (pred): 0.275 +- 0.258
mrr vals (pred, true): 0.414, 0.137
batch losses (mrrl, rdl): 0.0, 0.0004384142

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 321
rank avg (pred): 0.056 +- 0.059
mrr vals (pred, true): 0.471, 0.230
batch losses (mrrl, rdl): 0.0, 2.3557e-06

Epoch over!
epoch time: 12.432

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 846
rank avg (pred): 0.270 +- 0.251
mrr vals (pred, true): 0.380, 0.164
batch losses (mrrl, rdl): 0.4689724743, 0.0007914156

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 552
rank avg (pred): 0.563 +- 0.229
mrr vals (pred, true): 0.050, 0.018
batch losses (mrrl, rdl): 3.727e-07, 0.0025161016

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1002
rank avg (pred): 0.314 +- 0.188
mrr vals (pred, true): 0.106, 0.201
batch losses (mrrl, rdl): 0.0904777795, 0.0011435931

Epoch over!
epoch time: 12.0

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1008
rank avg (pred): 0.341 +- 0.190
mrr vals (pred, true): 0.092, 0.182
batch losses (mrrl, rdl): 0.0814129785, 0.0012960073

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.260 +- 0.153
mrr vals (pred, true): 0.101, 0.205
batch losses (mrrl, rdl): 0.108252883, 0.0006420838

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1018
rank avg (pred): 0.232 +- 0.136
mrr vals (pred, true): 0.107, 0.186
batch losses (mrrl, rdl): 0.0625103265, 0.0004088648

Epoch over!
epoch time: 13.156

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 869
rank avg (pred): 0.313 +- 0.173
mrr vals (pred, true): 0.082, 0.004
batch losses (mrrl, rdl): 0.0100675886, 0.0004360949

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 180
rank avg (pred): 0.272 +- 0.156
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.0241894759, 0.0007098457

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 620
rank avg (pred): 0.519 +- 0.213
mrr vals (pred, true): 0.057, 0.023
batch losses (mrrl, rdl): 0.0005261372, 0.0009880571

Epoch over!
epoch time: 13.103

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1190
rank avg (pred): 0.536 +- 0.224
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 6.1952e-05, 9.35401e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1059
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.274, 0.269
batch losses (mrrl, rdl): 0.0002298274, 2.54042e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 686
rank avg (pred): 0.511 +- 0.212
mrr vals (pred, true): 0.044, 0.005
batch losses (mrrl, rdl): 0.0003171983, 5.53345e-05

Epoch over!
epoch time: 12.162

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1031
rank avg (pred): 0.273 +- 0.154
mrr vals (pred, true): 0.097, 0.004
batch losses (mrrl, rdl): 0.0219754335, 0.0007701633

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.338 +- 0.189
mrr vals (pred, true): 0.114, 0.114
batch losses (mrrl, rdl): 4.5834e-06, 0.0010499347

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 776
rank avg (pred): 0.341 +- 0.182
mrr vals (pred, true): 0.099, 0.157
batch losses (mrrl, rdl): 0.0339106508, 0.001334518

Epoch over!
epoch time: 12.459

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 101
rank avg (pred): 0.209 +- 0.123
mrr vals (pred, true): 0.124, 0.172
batch losses (mrrl, rdl): 0.022737544, 0.0003071201

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.017 +- 0.010
mrr vals (pred, true): 0.189, 0.188
batch losses (mrrl, rdl): 1.21694e-05, 3.24015e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 656
rank avg (pred): 0.475 +- 0.223
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004225342, 1.96469e-05

Epoch over!
epoch time: 12.795

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 570
rank avg (pred): 0.457 +- 0.221
mrr vals (pred, true): 0.049, 0.007
batch losses (mrrl, rdl): 1.4043e-05, 0.0001785589

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 559
rank avg (pred): 0.401 +- 0.173
mrr vals (pred, true): 0.065, 0.020
batch losses (mrrl, rdl): 0.0022292933, 0.0008136866

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 968
rank avg (pred): 0.303 +- 0.162
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0211064052, 0.0005150519

Epoch over!
epoch time: 12.69

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 557
rank avg (pred): 0.397 +- 0.169
mrr vals (pred, true): 0.061, 0.038
batch losses (mrrl, rdl): 0.0011237849, 0.0012614724

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 129
rank avg (pred): 0.304 +- 0.155
mrr vals (pred, true): 0.089, 0.116
batch losses (mrrl, rdl): 0.00697017, 0.0007866794

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1169
rank avg (pred): 0.471 +- 0.251
mrr vals (pred, true): 0.033, 0.009
batch losses (mrrl, rdl): 0.0028296113, 0.0003123559

Epoch over!
epoch time: 13.224

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 608
rank avg (pred): 0.416 +- 0.221
mrr vals (pred, true): 0.057, 0.023
batch losses (mrrl, rdl): 0.0004282493, 0.0002463376

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 900
rank avg (pred): 0.285 +- 0.154
mrr vals (pred, true): 0.108, 0.115
batch losses (mrrl, rdl): 0.0005030131, 0.0010196086

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.396 +- 0.205
mrr vals (pred, true): 0.059, 0.009
batch losses (mrrl, rdl): 0.0008874665, 8.74365e-05

Epoch over!
epoch time: 14.079

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 103
rank avg (pred): 0.283 +- 0.151
mrr vals (pred, true): 0.101, 0.138
batch losses (mrrl, rdl): 0.0137092648, 0.0007264785

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.286 +- 0.144
mrr vals (pred, true): 0.097, 0.132
batch losses (mrrl, rdl): 0.0120687904, 0.0006812654

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.396 +- 0.207
mrr vals (pred, true): 0.052, 0.026
batch losses (mrrl, rdl): 2.7632e-05, 0.0001729233

Epoch over!
epoch time: 13.317

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.280 +- 0.142
mrr vals (pred, true): 0.100, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   26 	     0 	 0.05461 	 0.00327 	 m..s
   23 	     1 	 0.05422 	 0.00330 	 m..s
   38 	     2 	 0.10045 	 0.00340 	 m..s
   38 	     3 	 0.10045 	 0.00349 	 m..s
   28 	     4 	 0.05605 	 0.00352 	 m..s
    1 	     5 	 0.04641 	 0.00356 	 m..s
   77 	     6 	 0.11686 	 0.00359 	 MISS
   37 	     7 	 0.10000 	 0.00364 	 m..s
   38 	     8 	 0.10045 	 0.00368 	 m..s
   78 	     9 	 0.11701 	 0.00370 	 MISS
   56 	    10 	 0.10116 	 0.00372 	 m..s
   36 	    11 	 0.09985 	 0.00373 	 m..s
   17 	    12 	 0.05299 	 0.00377 	 m..s
    8 	    13 	 0.05018 	 0.00378 	 m..s
   10 	    14 	 0.05062 	 0.00383 	 m..s
   84 	    15 	 0.12053 	 0.00383 	 MISS
   38 	    16 	 0.10045 	 0.00384 	 m..s
   52 	    17 	 0.10055 	 0.00387 	 m..s
    9 	    18 	 0.05032 	 0.00392 	 m..s
   88 	    19 	 0.12777 	 0.00399 	 MISS
   57 	    20 	 0.10123 	 0.00400 	 m..s
   66 	    21 	 0.10305 	 0.00404 	 m..s
   82 	    22 	 0.11968 	 0.00404 	 MISS
   19 	    23 	 0.05378 	 0.00404 	 m..s
   71 	    24 	 0.10843 	 0.00405 	 MISS
   72 	    25 	 0.11007 	 0.00414 	 MISS
   38 	    26 	 0.10045 	 0.00414 	 m..s
   38 	    27 	 0.10045 	 0.00415 	 m..s
   38 	    28 	 0.10045 	 0.00415 	 m..s
   38 	    29 	 0.10045 	 0.00417 	 m..s
   15 	    30 	 0.05257 	 0.00424 	 m..s
   22 	    31 	 0.05409 	 0.00427 	 m..s
   95 	    32 	 0.14091 	 0.00435 	 MISS
   38 	    33 	 0.10045 	 0.00445 	 m..s
   59 	    34 	 0.10164 	 0.00445 	 m..s
   53 	    35 	 0.10067 	 0.00446 	 m..s
   68 	    36 	 0.10566 	 0.00451 	 MISS
   86 	    37 	 0.12296 	 0.00453 	 MISS
   83 	    38 	 0.12008 	 0.00454 	 MISS
   96 	    39 	 0.14151 	 0.00468 	 MISS
   12 	    40 	 0.05217 	 0.00472 	 m..s
   38 	    41 	 0.10045 	 0.00486 	 m..s
   79 	    42 	 0.11709 	 0.00490 	 MISS
   24 	    43 	 0.05450 	 0.00696 	 m..s
   13 	    44 	 0.05241 	 0.00741 	 m..s
   25 	    45 	 0.05455 	 0.00750 	 m..s
   27 	    46 	 0.05601 	 0.00884 	 m..s
   14 	    47 	 0.05246 	 0.00898 	 m..s
    3 	    48 	 0.04769 	 0.00916 	 m..s
    0 	    49 	 0.04577 	 0.00922 	 m..s
    2 	    50 	 0.04717 	 0.00942 	 m..s
   20 	    51 	 0.05385 	 0.00950 	 m..s
   11 	    52 	 0.05194 	 0.01021 	 m..s
    7 	    53 	 0.05011 	 0.01032 	 m..s
   16 	    54 	 0.05296 	 0.01044 	 m..s
    6 	    55 	 0.04976 	 0.01162 	 m..s
    4 	    56 	 0.04858 	 0.01568 	 m..s
   21 	    57 	 0.05392 	 0.01910 	 m..s
    5 	    58 	 0.04930 	 0.02006 	 ~...
   31 	    59 	 0.05738 	 0.02127 	 m..s
   32 	    60 	 0.06305 	 0.02188 	 m..s
   18 	    61 	 0.05326 	 0.02247 	 m..s
   34 	    62 	 0.09025 	 0.02788 	 m..s
   33 	    63 	 0.08911 	 0.02829 	 m..s
   29 	    64 	 0.05670 	 0.03512 	 ~...
   30 	    65 	 0.05724 	 0.04048 	 ~...
  100 	    66 	 0.16314 	 0.05106 	 MISS
   70 	    67 	 0.10809 	 0.07709 	 m..s
   55 	    68 	 0.10084 	 0.12155 	 ~...
   99 	    69 	 0.15342 	 0.12241 	 m..s
   38 	    70 	 0.10045 	 0.12266 	 ~...
   38 	    71 	 0.10045 	 0.12904 	 ~...
   38 	    72 	 0.10045 	 0.12913 	 ~...
   58 	    73 	 0.10161 	 0.13442 	 m..s
   92 	    74 	 0.13484 	 0.13475 	 ~...
   54 	    75 	 0.10083 	 0.14465 	 m..s
  101 	    76 	 0.17675 	 0.14610 	 m..s
   67 	    77 	 0.10357 	 0.14963 	 m..s
   64 	    78 	 0.10279 	 0.15094 	 m..s
   98 	    79 	 0.14961 	 0.15206 	 ~...
  102 	    80 	 0.18085 	 0.15311 	 ~...
   62 	    81 	 0.10251 	 0.15688 	 m..s
   61 	    82 	 0.10244 	 0.15694 	 m..s
   35 	    83 	 0.09855 	 0.15918 	 m..s
   65 	    84 	 0.10283 	 0.16042 	 m..s
   60 	    85 	 0.10191 	 0.16467 	 m..s
   63 	    86 	 0.10262 	 0.16554 	 m..s
   38 	    87 	 0.10045 	 0.16692 	 m..s
   80 	    88 	 0.11771 	 0.17181 	 m..s
  104 	    89 	 0.19101 	 0.17242 	 ~...
   76 	    90 	 0.11533 	 0.17516 	 m..s
   75 	    91 	 0.11508 	 0.17544 	 m..s
  103 	    92 	 0.18254 	 0.17631 	 ~...
   81 	    93 	 0.11849 	 0.17700 	 m..s
   73 	    94 	 0.11101 	 0.17727 	 m..s
  105 	    95 	 0.19610 	 0.18006 	 ~...
   69 	    96 	 0.10691 	 0.18871 	 m..s
   89 	    97 	 0.12899 	 0.18910 	 m..s
   91 	    98 	 0.13405 	 0.19002 	 m..s
  115 	    99 	 0.24034 	 0.19071 	 m..s
  107 	   100 	 0.21394 	 0.19264 	 ~...
  114 	   101 	 0.23609 	 0.19634 	 m..s
  111 	   102 	 0.22823 	 0.20033 	 ~...
   85 	   103 	 0.12110 	 0.20379 	 m..s
   74 	   104 	 0.11370 	 0.20818 	 m..s
  113 	   105 	 0.23369 	 0.20850 	 ~...
   87 	   106 	 0.12542 	 0.20861 	 m..s
   94 	   107 	 0.13513 	 0.21079 	 m..s
   90 	   108 	 0.13338 	 0.21451 	 m..s
   93 	   109 	 0.13509 	 0.21492 	 m..s
  110 	   110 	 0.22761 	 0.21514 	 ~...
   97 	   111 	 0.14152 	 0.21807 	 m..s
  106 	   112 	 0.21132 	 0.21972 	 ~...
  108 	   113 	 0.22019 	 0.22604 	 ~...
  112 	   114 	 0.23353 	 0.23291 	 ~...
  118 	   115 	 0.28259 	 0.23706 	 m..s
  109 	   116 	 0.22575 	 0.23892 	 ~...
  116 	   117 	 0.24725 	 0.24311 	 ~...
  119 	   118 	 0.28431 	 0.25940 	 ~...
  117 	   119 	 0.26918 	 0.25993 	 ~...
  120 	   120 	 0.29020 	 0.26101 	 ~...
==========================================
r_mrr = 0.7418630123138428
r2_mrr = 0.43923503160476685
spearmanr_mrr@5 = 0.9391516447067261
spearmanr_mrr@10 = 0.9591478705406189
spearmanr_mrr@50 = 0.9637627005577087
spearmanr_mrr@100 = 0.841984212398529
spearmanr_mrr@All = 0.8714139461517334
==========================================
test time: 0.418
Done Testing dataset CoDExSmall
total time taken: 201.23853302001953
training time taken: 191.51745653152466
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7419)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4392)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9392)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9591)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9638)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8420)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8714)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.6511362023229594}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 4865937335602121
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1055, 1161, 0, 268, 55, 615, 1147, 285, 127, 1090, 418, 22, 313, 650, 480, 701, 484, 441, 751, 174, 910, 329, 711, 217, 662, 82, 1195, 1023, 1006, 457, 599, 558, 597, 353, 1018, 808, 271, 159, 476, 1100, 979, 557, 863, 538, 1191, 459, 641, 181, 422, 88, 300, 167, 496, 410, 115, 594, 371, 477, 1002, 565, 664, 344, 519, 391, 355, 109, 897, 70, 393, 1099, 499, 833, 603, 1039, 66, 1182, 535, 868, 908, 1091, 569, 302, 735, 988, 673, 246, 261, 307, 1128, 705, 258, 166, 502, 1185, 706, 514, 913, 992, 560, 1122, 610, 479, 63, 928, 243, 583, 1125, 952, 1050, 368, 450, 1213, 57, 256, 481, 561, 663, 257, 354, 1146, 428]
valid_ids (0): []
train_ids (1094): [80, 1035, 96, 634, 85, 782, 624, 43, 1202, 58, 991, 417, 239, 1065, 99, 28, 842, 339, 1133, 629, 3, 759, 554, 687, 406, 674, 32, 856, 654, 986, 971, 377, 1109, 154, 728, 815, 215, 1017, 400, 171, 108, 501, 273, 244, 320, 145, 446, 780, 505, 537, 464, 289, 297, 439, 581, 675, 164, 125, 282, 420, 52, 325, 1079, 158, 409, 324, 331, 838, 390, 413, 970, 186, 528, 612, 685, 704, 852, 295, 846, 608, 926, 357, 267, 894, 1108, 813, 298, 596, 632, 427, 882, 646, 720, 322, 493, 890, 1, 767, 648, 434, 198, 670, 948, 475, 415, 817, 911, 684, 1036, 254, 1180, 200, 68, 693, 540, 497, 916, 488, 382, 136, 1139, 772, 671, 487, 380, 350, 1003, 627, 474, 59, 930, 1208, 630, 750, 823, 379, 990, 375, 722, 702, 1145, 791, 1151, 444, 1193, 749, 605, 492, 53, 721, 1165, 549, 114, 276, 442, 881, 809, 618, 311, 511, 64, 983, 1038, 1061, 414, 907, 33, 194, 843, 1166, 253, 423, 788, 521, 977, 151, 31, 958, 678, 438, 577, 29, 1105, 279, 1027, 572, 367, 349, 814, 1007, 452, 697, 770, 607, 252, 707, 359, 1093, 968, 310, 1075, 250, 864, 216, 448, 5, 466, 1067, 1062, 976, 133, 898, 242, 424, 939, 544, 529, 604, 730, 1110, 163, 914, 639, 614, 854, 384, 912, 161, 1088, 628, 700, 798, 471, 736, 828, 709, 978, 202, 71, 472, 765, 555, 328, 1121, 389, 361, 517, 113, 110, 531, 48, 1092, 433, 606, 436, 146, 1207, 1011, 224, 197, 314, 695, 1179, 431, 399, 1203, 1005, 786, 762, 613, 92, 682, 394, 89, 713, 338, 90, 1137, 525, 965, 666, 345, 625, 232, 533, 172, 228, 46, 1021, 262, 1031, 130, 981, 7, 333, 909, 102, 223, 959, 1184, 771, 588, 1154, 879, 160, 369, 378, 611, 238, 951, 327, 1004, 1096, 12, 787, 547, 1160, 1149, 392, 1214, 723, 1025, 539, 1186, 315, 779, 822, 1008, 1115, 1032, 845, 857, 542, 974, 373, 653, 1033, 935, 587, 1010, 901, 1177, 1117, 938, 699, 348, 1116, 647, 893, 155, 1168, 1014, 601, 347, 1152, 411, 849, 11, 342, 169, 1150, 1054, 218, 995, 245, 803, 530, 138, 876, 177, 75, 884, 1140, 1045, 123, 463, 508, 1173, 1157, 1012, 826, 582, 103, 1049, 1174, 426, 987, 874, 1169, 638, 340, 287, 211, 567, 676, 408, 906, 1101, 1175, 84, 1148, 954, 1042, 686, 147, 1124, 343, 1172, 545, 150, 24, 17, 1070, 1041, 326, 967, 729, 21, 1136, 960, 78, 1197, 226, 1053, 972, 124, 1131, 510, 39, 1094, 998, 792, 235, 1156, 407, 1126, 563, 1026, 396, 225, 626, 805, 67, 458, 689, 1028, 144, 858, 934, 1142, 73, 131, 398, 830, 72, 1013, 896, 1051, 550, 937, 278, 506, 139, 187, 461, 299, 86, 421, 260, 1196, 366, 334, 173, 552, 395, 887, 34, 251, 30, 973, 1058, 1081, 571, 850, 284, 559, 761, 292, 275, 281, 1107, 575, 230, 1123, 790, 162, 1127, 157, 305, 668, 41, 240, 220, 236, 241, 500, 209, 925, 248, 207, 490, 467, 799, 963, 853, 383, 140, 49, 920, 1144, 1106, 744, 265, 755, 45, 1016, 919, 62, 902, 860, 997, 796, 401, 129, 742, 593, 91, 435, 941, 1155, 1095, 516, 358, 824, 376, 756, 494, 93, 316, 453, 1052, 523, 1143, 365, 25, 219, 1046, 969, 1057, 306, 449, 797, 694, 922, 107, 522, 135, 1076, 97, 768, 203, 753, 231, 121, 915, 622, 763, 748, 949, 984, 374, 69, 1135, 760, 841, 955, 179, 861, 489, 994, 943, 795, 385, 222, 623, 870, 586, 579, 512, 769, 35, 156, 341, 2, 364, 54, 717, 644, 176, 964, 255, 1212, 335, 180, 1060, 1112, 65, 775, 1114, 1043, 1086, 1190, 178, 580, 867, 405, 498, 134, 50, 1059, 773, 524, 945, 989, 195, 620, 76, 381, 658, 81, 142, 503, 1069, 286, 532, 570, 272, 309, 42, 738, 1118, 437, 456, 362, 18, 921, 486, 642, 1178, 621, 892, 757, 740, 865, 903, 714, 175, 266, 589, 568, 553, 1200, 1083, 304, 683, 840, 1015, 551, 1097, 478, 877, 672, 754, 468, 889, 708, 848, 534, 19, 527, 221, 346, 541, 895, 318, 206, 677, 758, 564, 932, 982, 386, 504, 105, 1111, 847, 283, 962, 412, 462, 372, 834, 189, 880, 578, 183, 126, 337, 652, 835, 1119, 227, 878, 56, 637, 746, 141, 1211, 270, 118, 387, 691, 120, 485, 844, 811, 592, 669, 851, 430, 839, 77, 956, 293, 1198, 831, 660, 402, 111, 429, 388, 1064, 659, 696, 731, 469, 900, 233, 716, 1044, 509, 192, 301, 872, 330, 360, 741, 518, 112, 184, 100, 36, 122, 931, 1089, 715, 781, 148, 303, 432, 288, 832, 1192, 739, 15, 566, 598, 319, 927, 836, 264, 980, 404, 536, 947, 1134, 13, 263, 869, 454, 196, 645, 655, 871, 837, 886, 212, 665, 4, 810, 1205, 933, 595, 576, 923, 515, 859, 445, 204, 710, 143, 1080, 363, 40, 294, 27, 680, 513, 416, 205, 208, 1162, 918, 117, 210, 229, 866, 1153, 74, 996, 60, 460, 829, 600, 1138, 734, 789, 774, 875, 153, 26, 1085, 336, 1048, 946, 280, 1072, 419, 1063, 816, 801, 95, 317, 1171, 584, 785, 985, 312, 1030, 793, 507, 104, 820, 899, 616, 905, 827, 885, 1037, 891, 1199, 649, 1129, 784, 1001, 259, 688, 602, 732, 247, 690, 548, 352, 1077, 465, 573, 1034, 745, 888, 291, 1098, 1141, 116, 679, 719, 1047, 1159, 802, 188, 783, 1084, 633, 296, 546, 79, 1024, 590, 447, 966, 617, 37, 483, 764, 942, 777, 993, 961, 778, 370, 149, 8, 323, 1040, 1000, 1074, 440, 718, 562, 574, 1188, 1164, 403, 940, 269, 185, 98, 168, 692, 1029, 950, 609, 1181, 543, 1102, 128, 1056, 152, 165, 234, 51, 862, 491, 87, 308, 635, 1163, 455, 743, 667, 94, 1130, 924, 20, 356, 277, 1078, 1113, 1132, 1183, 636, 1167, 591, 193, 1009, 1073, 651, 332, 213, 199, 556, 495, 137, 482, 643, 725, 526, 191, 657, 703, 61, 1066, 656, 201, 776, 470, 1120, 1189, 681, 6, 443, 47, 812, 727, 1206, 904, 807, 917, 1201, 747, 16, 726, 1104, 724, 1176, 1158, 975, 936, 1071, 1022, 818, 619, 1170, 14, 9, 929, 873, 821, 794, 883, 473, 1187, 451, 38, 855, 190, 766, 640, 1020, 804, 274, 712, 1019, 132, 397, 1209, 999, 10, 119, 520, 1210, 800, 752, 290, 170, 83, 351, 698, 631, 737, 214, 1194, 237, 953, 425, 23, 106, 944, 182, 1103, 585, 1087, 733, 321, 806, 1204, 825, 957, 44, 819, 1082, 661, 249, 1068, 101]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3234694198400357
the save name prefix for this run is:  chkpt-ID_3234694198400357_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 864
rank avg (pred): 0.562 +- 0.003
mrr vals (pred, true): 0.001, 0.005
batch losses (mrrl, rdl): 0.0, 0.000319623

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1070
rank avg (pred): 0.069 +- 0.045
mrr vals (pred, true): 0.147, 0.264
batch losses (mrrl, rdl): 0.0, 2.13077e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 443
rank avg (pred): 0.275 +- 0.207
mrr vals (pred, true): 0.203, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005613648

Epoch over!
epoch time: 13.018

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 340
rank avg (pred): 0.256 +- 0.209
mrr vals (pred, true): 0.258, 0.149
batch losses (mrrl, rdl): 0.0, 0.0003726531

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 140
rank avg (pred): 0.288 +- 0.239
mrr vals (pred, true): 0.275, 0.177
batch losses (mrrl, rdl): 0.0, 0.0009730625

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 789
rank avg (pred): 0.206 +- 0.199
mrr vals (pred, true): 0.353, 0.004
batch losses (mrrl, rdl): 0.0, 0.0011120506

Epoch over!
epoch time: 12.286

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.160 +- 0.138
mrr vals (pred, true): 0.313, 0.040
batch losses (mrrl, rdl): 0.0, 9.31577e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 107
rank avg (pred): 0.249 +- 0.229
mrr vals (pred, true): 0.350, 0.176
batch losses (mrrl, rdl): 0.0, 0.0006643604

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.267 +- 0.246
mrr vals (pred, true): 0.356, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004977499

Epoch over!
epoch time: 12.052

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.320 +- 0.295
mrr vals (pred, true): 0.361, 0.004
batch losses (mrrl, rdl): 0.0, 0.000238658

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 170
rank avg (pred): 0.213 +- 0.204
mrr vals (pred, true): 0.381, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009953839

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 644
rank avg (pred): 0.338 +- 0.300
mrr vals (pred, true): 0.349, 0.026
batch losses (mrrl, rdl): 0.0, 0.0001095943

Epoch over!
epoch time: 12.208

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.023 +- 0.023
mrr vals (pred, true): 0.437, 0.244
batch losses (mrrl, rdl): 0.0, 8.7194e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 468
rank avg (pred): 0.288 +- 0.269
mrr vals (pred, true): 0.360, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003725002

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 48
rank avg (pred): 0.042 +- 0.042
mrr vals (pred, true): 0.421, 0.192
batch losses (mrrl, rdl): 0.0, 3.7706e-06

Epoch over!
epoch time: 12.537

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 530
rank avg (pred): 0.153 +- 0.152
mrr vals (pred, true): 0.406, 0.036
batch losses (mrrl, rdl): 1.2704104185, 8.5731e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.275, 0.260
batch losses (mrrl, rdl): 0.0019888859, 2.77281e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.233 +- 0.151
mrr vals (pred, true): 0.108, 0.204
batch losses (mrrl, rdl): 0.0921196193, 0.0004797977

Epoch over!
epoch time: 13.189

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 394
rank avg (pred): 0.382 +- 0.229
mrr vals (pred, true): 0.101, 0.160
batch losses (mrrl, rdl): 0.0353564024, 0.0015002565

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.372 +- 0.219
mrr vals (pred, true): 0.091, 0.138
batch losses (mrrl, rdl): 0.0228443947, 0.0015663895

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 676
rank avg (pred): 0.380 +- 0.183
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0002835162, 0.0001750353

Epoch over!
epoch time: 13.236

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 803
rank avg (pred): 0.305 +- 0.171
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0128514078, 0.000491336

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 333
rank avg (pred): 0.334 +- 0.194
mrr vals (pred, true): 0.112, 0.117
batch losses (mrrl, rdl): 0.0002539324, 0.0008845702

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 926
rank avg (pred): 0.334 +- 0.174
mrr vals (pred, true): 0.077, 0.029
batch losses (mrrl, rdl): 0.0074270763, 0.0004540851

Epoch over!
epoch time: 12.932

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 855
rank avg (pred): 0.324 +- 0.178
mrr vals (pred, true): 0.099, 0.153
batch losses (mrrl, rdl): 0.02893693, 0.0011115882

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 510
rank avg (pred): 0.378 +- 0.199
mrr vals (pred, true): 0.030, 0.014
batch losses (mrrl, rdl): 0.003864564, 0.0004928415

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 996
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.271, 0.258
batch losses (mrrl, rdl): 0.0016165409, 2.81644e-05

Epoch over!
epoch time: 12.287

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.014 +- 0.009
mrr vals (pred, true): 0.232, 0.215
batch losses (mrrl, rdl): 0.0026832384, 3.27146e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 469
rank avg (pred): 0.304 +- 0.174
mrr vals (pred, true): 0.102, 0.004
batch losses (mrrl, rdl): 0.0267042518, 0.0004460303

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 681
rank avg (pred): 0.370 +- 0.181
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 1.9444e-05, 0.0002575944

Epoch over!
epoch time: 12.933

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 463
rank avg (pred): 0.329 +- 0.178
mrr vals (pred, true): 0.089, 0.004
batch losses (mrrl, rdl): 0.0152212707, 0.0002694242

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.201 +- 0.122
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0581941791, 0.0013484675

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 896
rank avg (pred): 0.247 +- 0.129
mrr vals (pred, true): 0.092, 0.039
batch losses (mrrl, rdl): 0.0174227655, 0.0004634795

Epoch over!
epoch time: 13.155

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1011
rank avg (pred): 0.236 +- 0.138
mrr vals (pred, true): 0.112, 0.204
batch losses (mrrl, rdl): 0.0848441273, 0.000460939

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 922
rank avg (pred): 0.299 +- 0.156
mrr vals (pred, true): 0.092, 0.166
batch losses (mrrl, rdl): 0.0553245023, 0.0009323191

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 11
rank avg (pred): 0.020 +- 0.013
mrr vals (pred, true): 0.211, 0.231
batch losses (mrrl, rdl): 0.0041165212, 7.4685e-06

Epoch over!
epoch time: 12.932

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.369 +- 0.194
mrr vals (pred, true): 0.048, 0.003
batch losses (mrrl, rdl): 2.61721e-05, 0.0002881939

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 874
rank avg (pred): 0.265 +- 0.138
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.0171871874, 0.0007490303

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1150
rank avg (pred): 0.244 +- 0.155
mrr vals (pred, true): 0.063, 0.025
batch losses (mrrl, rdl): 0.0017572752, 0.0001190558

Epoch over!
epoch time: 13.417

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 316
rank avg (pred): 0.013 +- 0.009
mrr vals (pred, true): 0.219, 0.253
batch losses (mrrl, rdl): 0.0116726933, 2.37431e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 84
rank avg (pred): 0.288 +- 0.153
mrr vals (pred, true): 0.096, 0.097
batch losses (mrrl, rdl): 0.0210472289, 0.0005217682

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 517
rank avg (pred): 0.235 +- 0.206
mrr vals (pred, true): 0.049, 0.024
batch losses (mrrl, rdl): 5.3134e-06, 3.46941e-05

Epoch over!
epoch time: 13.679

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 429
rank avg (pred): 0.338 +- 0.196
mrr vals (pred, true): 0.110, 0.005
batch losses (mrrl, rdl): 0.0361839645, 0.0002567055

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.318 +- 0.174
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 9.89807e-05, 0.0004626488

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 374
rank avg (pred): 0.228 +- 0.145
mrr vals (pred, true): 0.138, 0.197
batch losses (mrrl, rdl): 0.0341432802, 0.0003838727

Epoch over!
epoch time: 13.131

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.015 +- 0.010
mrr vals (pred, true): 0.231, 0.189

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   28 	     0 	 0.05231 	 0.00339 	 m..s
    5 	     1 	 0.04247 	 0.00357 	 m..s
   47 	     2 	 0.09059 	 0.00359 	 m..s
   63 	     3 	 0.10489 	 0.00359 	 MISS
    8 	     4 	 0.04330 	 0.00362 	 m..s
   85 	     5 	 0.12655 	 0.00364 	 MISS
   11 	     6 	 0.04418 	 0.00378 	 m..s
    7 	     7 	 0.04274 	 0.00379 	 m..s
   51 	     8 	 0.09210 	 0.00380 	 m..s
   40 	     9 	 0.08919 	 0.00382 	 m..s
   29 	    10 	 0.05258 	 0.00383 	 m..s
    4 	    11 	 0.04246 	 0.00388 	 m..s
   64 	    12 	 0.10503 	 0.00388 	 MISS
   40 	    13 	 0.08919 	 0.00390 	 m..s
   46 	    14 	 0.09042 	 0.00393 	 m..s
   68 	    15 	 0.10940 	 0.00393 	 MISS
   52 	    16 	 0.09252 	 0.00405 	 m..s
   65 	    17 	 0.10560 	 0.00406 	 MISS
   78 	    18 	 0.12149 	 0.00406 	 MISS
   26 	    19 	 0.05175 	 0.00409 	 m..s
   74 	    20 	 0.11383 	 0.00412 	 MISS
   73 	    21 	 0.11372 	 0.00413 	 MISS
   75 	    22 	 0.11496 	 0.00414 	 MISS
   40 	    23 	 0.08919 	 0.00414 	 m..s
   14 	    24 	 0.04710 	 0.00416 	 m..s
   70 	    25 	 0.10976 	 0.00422 	 MISS
   60 	    26 	 0.10209 	 0.00422 	 m..s
   54 	    27 	 0.09281 	 0.00426 	 m..s
   82 	    28 	 0.12530 	 0.00426 	 MISS
   33 	    29 	 0.05311 	 0.00429 	 m..s
   58 	    30 	 0.09967 	 0.00453 	 m..s
   12 	    31 	 0.04496 	 0.00455 	 m..s
   90 	    32 	 0.13070 	 0.00457 	 MISS
   91 	    33 	 0.13799 	 0.00468 	 MISS
   96 	    34 	 0.15116 	 0.00468 	 MISS
   39 	    35 	 0.08846 	 0.00471 	 m..s
   69 	    36 	 0.10966 	 0.00474 	 MISS
   88 	    37 	 0.12763 	 0.00475 	 MISS
    0 	    38 	 0.04019 	 0.00506 	 m..s
   18 	    39 	 0.04940 	 0.00540 	 m..s
   31 	    40 	 0.05287 	 0.00689 	 m..s
   27 	    41 	 0.05207 	 0.00756 	 m..s
   32 	    42 	 0.05292 	 0.00863 	 m..s
   34 	    43 	 0.05428 	 0.00950 	 m..s
    9 	    44 	 0.04386 	 0.00973 	 m..s
   30 	    45 	 0.05270 	 0.00995 	 m..s
   10 	    46 	 0.04411 	 0.01032 	 m..s
   19 	    47 	 0.05092 	 0.01418 	 m..s
    2 	    48 	 0.04053 	 0.01683 	 ~...
   21 	    49 	 0.05113 	 0.01713 	 m..s
    3 	    50 	 0.04079 	 0.01719 	 ~...
    1 	    51 	 0.04044 	 0.01731 	 ~...
   20 	    52 	 0.05110 	 0.01763 	 m..s
   23 	    53 	 0.05143 	 0.01782 	 m..s
   15 	    54 	 0.04880 	 0.01910 	 ~...
   24 	    55 	 0.05162 	 0.01946 	 m..s
   13 	    56 	 0.04675 	 0.02020 	 ~...
   16 	    57 	 0.04919 	 0.02046 	 ~...
   17 	    58 	 0.04919 	 0.02232 	 ~...
    6 	    59 	 0.04266 	 0.02242 	 ~...
   37 	    60 	 0.06111 	 0.02369 	 m..s
   25 	    61 	 0.05172 	 0.02471 	 ~...
   36 	    62 	 0.06106 	 0.02559 	 m..s
   38 	    63 	 0.07823 	 0.02829 	 m..s
   22 	    64 	 0.05136 	 0.03752 	 ~...
   35 	    65 	 0.05431 	 0.04125 	 ~...
   94 	    66 	 0.15034 	 0.05558 	 m..s
   72 	    67 	 0.11127 	 0.07709 	 m..s
   55 	    68 	 0.09291 	 0.10711 	 ~...
   48 	    69 	 0.09062 	 0.12155 	 m..s
   45 	    70 	 0.09038 	 0.12913 	 m..s
   50 	    71 	 0.09150 	 0.13081 	 m..s
   40 	    72 	 0.08919 	 0.13089 	 m..s
   40 	    73 	 0.08919 	 0.13096 	 m..s
   77 	    74 	 0.11734 	 0.13114 	 ~...
   49 	    75 	 0.09129 	 0.13156 	 m..s
   53 	    76 	 0.09252 	 0.14249 	 m..s
   57 	    77 	 0.09673 	 0.14286 	 m..s
   97 	    78 	 0.15683 	 0.14355 	 ~...
  105 	    79 	 0.17682 	 0.14500 	 m..s
   98 	    80 	 0.15973 	 0.14610 	 ~...
   93 	    81 	 0.14277 	 0.14747 	 ~...
   76 	    82 	 0.11639 	 0.15094 	 m..s
   95 	    83 	 0.15112 	 0.15215 	 ~...
  103 	    84 	 0.17469 	 0.15311 	 ~...
  104 	    85 	 0.17475 	 0.15441 	 ~...
   56 	    86 	 0.09585 	 0.15881 	 m..s
   62 	    87 	 0.10271 	 0.16650 	 m..s
   67 	    88 	 0.10665 	 0.16692 	 m..s
   99 	    89 	 0.16410 	 0.16869 	 ~...
   61 	    90 	 0.10228 	 0.16961 	 m..s
  108 	    91 	 0.18839 	 0.17159 	 ~...
   83 	    92 	 0.12574 	 0.17628 	 m..s
  100 	    93 	 0.16415 	 0.17631 	 ~...
   59 	    94 	 0.09972 	 0.17829 	 m..s
   87 	    95 	 0.12691 	 0.17845 	 m..s
  101 	    96 	 0.16985 	 0.17869 	 ~...
   86 	    97 	 0.12682 	 0.17898 	 m..s
   66 	    98 	 0.10592 	 0.17973 	 m..s
   84 	    99 	 0.12583 	 0.17989 	 m..s
  113 	   100 	 0.20609 	 0.18212 	 ~...
  106 	   101 	 0.17939 	 0.18430 	 ~...
  114 	   102 	 0.21397 	 0.18523 	 ~...
   80 	   103 	 0.12477 	 0.18624 	 m..s
  118 	   104 	 0.23117 	 0.18868 	 m..s
   79 	   105 	 0.12274 	 0.19063 	 m..s
   89 	   106 	 0.12856 	 0.19210 	 m..s
  107 	   107 	 0.18154 	 0.19639 	 ~...
  117 	   108 	 0.22350 	 0.20093 	 ~...
   71 	   109 	 0.11025 	 0.20108 	 m..s
  102 	   110 	 0.17418 	 0.20832 	 m..s
   81 	   111 	 0.12510 	 0.21011 	 m..s
  112 	   112 	 0.19701 	 0.21220 	 ~...
   92 	   113 	 0.13861 	 0.21696 	 m..s
  111 	   114 	 0.19365 	 0.21757 	 ~...
  110 	   115 	 0.19340 	 0.21972 	 ~...
  109 	   116 	 0.19127 	 0.22592 	 m..s
  116 	   117 	 0.21963 	 0.24444 	 ~...
  115 	   118 	 0.21741 	 0.25289 	 m..s
  119 	   119 	 0.24663 	 0.25993 	 ~...
  120 	   120 	 0.26446 	 0.26245 	 ~...
==========================================
r_mrr = 0.7555398941040039
r2_mrr = 0.47876596450805664
spearmanr_mrr@5 = 0.8396775722503662
spearmanr_mrr@10 = 0.9286194443702698
spearmanr_mrr@50 = 0.9766297340393066
spearmanr_mrr@100 = 0.9165064096450806
spearmanr_mrr@All = 0.9286999702453613
==========================================
test time: 0.635
Done Testing dataset CoDExSmall
total time taken: 204.01795768737793
training time taken: 193.70777082443237
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7555)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4788)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.8397)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9286)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9766)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.9165)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.9287)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.2025038781566764}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7725030105057986
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [301, 116, 545, 655, 507, 808, 279, 620, 934, 531, 306, 774, 679, 510, 275, 336, 891, 85, 1150, 501, 63, 742, 1038, 413, 985, 441, 1044, 21, 383, 331, 417, 361, 1065, 257, 1119, 897, 863, 644, 15, 781, 1204, 427, 1132, 928, 231, 22, 740, 1072, 238, 1166, 31, 323, 834, 726, 298, 590, 734, 838, 699, 70, 552, 573, 93, 661, 146, 340, 927, 233, 641, 457, 779, 804, 20, 80, 468, 1196, 990, 518, 259, 855, 443, 579, 872, 302, 307, 540, 470, 90, 1016, 71, 596, 769, 1013, 1086, 992, 577, 1155, 1170, 627, 585, 938, 1, 646, 47, 462, 162, 278, 519, 756, 780, 473, 1001, 1141, 542, 304, 2, 913, 593, 1080, 961, 1022]
valid_ids (0): []
train_ids (1094): [55, 1151, 1164, 114, 725, 1110, 23, 430, 980, 280, 270, 757, 1039, 5, 475, 1193, 294, 1210, 1199, 467, 956, 801, 944, 782, 669, 532, 74, 1056, 724, 404, 1098, 563, 662, 766, 929, 344, 589, 460, 486, 712, 647, 185, 155, 1051, 17, 795, 166, 535, 628, 262, 654, 168, 131, 741, 14, 823, 1172, 702, 154, 525, 978, 659, 521, 458, 35, 260, 1178, 365, 342, 435, 497, 949, 343, 377, 536, 952, 1078, 369, 1027, 140, 332, 1055, 480, 715, 1106, 748, 642, 1198, 713, 558, 771, 72, 201, 737, 1123, 300, 657, 1111, 937, 1108, 833, 49, 348, 445, 656, 173, 447, 223, 451, 328, 407, 1003, 33, 264, 122, 719, 1160, 1149, 842, 547, 236, 248, 1100, 1168, 682, 494, 1214, 723, 574, 706, 971, 514, 106, 743, 676, 1006, 172, 1125, 539, 50, 947, 1101, 61, 925, 123, 1159, 568, 580, 455, 720, 45, 1138, 548, 556, 819, 469, 733, 716, 1209, 511, 115, 412, 605, 625, 767, 885, 1020, 1174, 1034, 516, 993, 564, 1052, 835, 75, 89, 1126, 1112, 1103, 479, 354, 253, 826, 637, 895, 129, 700, 554, 148, 82, 705, 1181, 493, 128, 1068, 1029, 915, 936, 1036, 1095, 143, 919, 698, 452, 324, 425, 630, 889, 973, 920, 738, 488, 345, 350, 164, 822, 42, 884, 764, 603, 388, 793, 492, 672, 239, 1122, 753, 1114, 214, 965, 312, 871, 401, 687, 308, 87, 193, 418, 776, 629, 138, 689, 101, 176, 600, 466, 640, 831, 946, 349, 330, 530, 960, 1010, 792, 69, 954, 1124, 703, 1066, 261, 760, 235, 569, 91, 775, 572, 132, 1088, 81, 180, 648, 1075, 597, 945, 761, 496, 645, 426, 1041, 843, 839, 228, 18, 245, 395, 456, 1045, 896, 989, 1043, 942, 582, 650, 1211, 249, 1037, 1182, 744, 181, 205, 732, 446, 624, 751, 272, 777, 914, 890, 635, 282, 633, 1208, 9, 99, 52, 88, 608, 851, 459, 288, 192, 387, 133, 1069, 537, 364, 359, 865, 51, 1186, 297, 1202, 873, 828, 182, 1053, 7, 799, 951, 158, 394, 184, 610, 1049, 905, 113, 218, 1116, 136, 888, 832, 190, 338, 968, 156, 159, 841, 255, 611, 879, 752, 199, 322, 675, 534, 24, 296, 111, 487, 878, 500, 363, 440, 506, 1063, 187, 119, 745, 276, 40, 754, 824, 68, 416, 287, 329, 232, 202, 333, 442, 549, 759, 571, 1131, 104, 43, 478, 722, 1105, 1017, 830, 1189, 439, 1008, 805, 415, 314, 1213, 988, 763, 846, 341, 327, 912, 1176, 875, 1084, 319, 709, 1067, 1060, 1071, 1096, 73, 996, 810, 584, 224, 557, 243, 286, 449, 1000, 592, 357, 171, 433, 562, 247, 1152, 409, 677, 267, 30, 38, 615, 870, 292, 790, 718, 538, 124, 178, 815, 921, 299, 567, 303, 482, 1021, 188, 693, 665, 1093, 750, 371, 317, 546, 150, 170, 561, 1135, 550, 398, 269, 1033, 707, 880, 935, 27, 126, 410, 1062, 768, 576, 903, 34, 599, 714, 421, 898, 522, 32, 673, 681, 972, 337, 382, 1073, 786, 432, 555, 708, 477, 678, 392, 553, 614, 1154, 200, 339, 1127, 785, 213, 840, 66, 623, 334, 1167, 103, 1087, 56, 802, 1134, 4, 1032, 220, 854, 244, 1142, 861, 6, 773, 1083, 995, 836, 606, 1023, 520, 64, 789, 523, 619, 1057, 1156, 121, 450, 651, 79, 994, 1185, 1194, 77, 1205, 977, 591, 1070, 618, 882, 403, 818, 1201, 527, 739, 108, 356, 899, 551, 174, 194, 736, 886, 1207, 290, 483, 429, 1148, 1143, 526, 755, 622, 1146, 346, 969, 1091, 1175, 1158, 222, 953, 1012, 820, 437, 316, 1042, 436, 320, 62, 251, 428, 959, 1104, 175, 1019, 313, 894, 691, 643, 83, 283, 1030, 604, 144, 422, 153, 680, 503, 1117, 731, 911, 1007, 25, 293, 97, 1128, 1092, 36, 1177, 533, 237, 474, 86, 570, 701, 987, 142, 632, 67, 495, 234, 844, 671, 966, 12, 216, 370, 970, 240, 660, 710, 916, 1015, 794, 639, 504, 940, 29, 607, 226, 609, 408, 102, 274, 950, 1058, 1097, 1048, 207, 285, 225, 1031, 717, 860, 509, 814, 221, 845, 380, 310, 908, 747, 1165, 362, 1139, 943, 37, 1035, 529, 1183, 137, 683, 490, 800, 857, 812, 867, 909, 163, 694, 204, 258, 697, 784, 246, 157, 385, 1157, 1147, 1018, 1107, 1144, 979, 621, 612, 256, 1094, 758, 997, 378, 864, 360, 1187, 1133, 183, 877, 721, 165, 930, 636, 829, 368, 263, 729, 1140, 1191, 100, 809, 684, 791, 229, 352, 1180, 438, 668, 1171, 955, 664, 1005, 57, 983, 400, 465, 197, 321, 666, 981, 453, 986, 1009, 1089, 134, 727, 964, 711, 230, 289, 852, 481, 1195, 749, 8, 1184, 318, 464, 1120, 783, 1059, 375, 167, 746, 1203, 941, 271, 856, 900, 817, 3, 1173, 76, 849, 141, 1077, 663, 414, 347, 59, 517, 353, 991, 1099, 16, 1047, 396, 811, 351, 859, 39, 196, 858, 893, 638, 728, 98, 94, 0, 125, 384, 420, 367, 444, 179, 847, 389, 219, 528, 393, 513, 565, 411, 907, 118, 982, 1024, 19, 373, 594, 1061, 797, 762, 1206, 559, 881, 1074, 78, 212, 962, 1102, 117, 160, 848, 177, 787, 967, 11, 53, 151, 386, 44, 311, 887, 406, 215, 381, 120, 397, 512, 827, 1130, 670, 326, 1064, 471, 1082, 203, 999, 1054, 379, 975, 883, 335, 1136, 273, 152, 112, 423, 926, 601, 26, 617, 575, 498, 765, 1081, 1161, 46, 515, 974, 667, 1090, 1169, 472, 1200, 135, 948, 505, 92, 1026, 252, 431, 217, 524, 130, 268, 1014, 1145, 209, 544, 807, 1115, 798, 54, 543, 295, 1076, 206, 391, 692, 602, 837, 1050, 109, 1011, 189, 541, 901, 254, 695, 476, 616, 566, 191, 922, 374, 910, 1153, 735, 850, 902, 658, 1197, 147, 690, 1046, 918, 1025, 1113, 1190, 1109, 778, 399, 502, 281, 816, 1163, 366, 241, 208, 957, 613, 402, 485, 499, 1085, 923, 508, 862, 95, 931, 1137, 376, 932, 419, 315, 250, 491, 372, 355, 998, 41, 578, 1188, 704, 560, 463, 1129, 906, 586, 796, 358, 461, 581, 853, 211, 96, 60, 917, 48, 107, 813, 868, 291, 127, 730, 325, 821, 933, 424, 65, 28, 110, 186, 806, 688, 169, 1004, 984, 454, 772, 788, 242, 1121, 198, 145, 595, 284, 869, 1162, 652, 305, 434, 924, 265, 1028, 583, 976, 277, 649, 390, 825, 1192, 803, 13, 195, 405, 770, 588, 161, 139, 266, 105, 587, 1002, 876, 674, 598, 1040, 1179, 1118, 149, 686, 634, 1212, 58, 84, 484, 210, 963, 874, 653, 10, 448, 892, 489, 631, 685, 866, 904, 1079, 696, 958, 227, 626, 309, 939]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8226097263966443
the save name prefix for this run is:  chkpt-ID_8226097263966443_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1127
rank avg (pred): 0.473 +- 0.004
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 9.63715e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 310
rank avg (pred): 0.074 +- 0.050
mrr vals (pred, true): 0.070, 0.220
batch losses (mrrl, rdl): 0.0, 1.29556e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.288 +- 0.226
mrr vals (pred, true): 0.229, 0.178
batch losses (mrrl, rdl): 0.0, 0.0009154408

Epoch over!
epoch time: 12.767

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 980
rank avg (pred): 0.059 +- 0.049
mrr vals (pred, true): 0.313, 0.268
batch losses (mrrl, rdl): 0.0, 9.6634e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 268
rank avg (pred): 0.054 +- 0.048
mrr vals (pred, true): 0.353, 0.244
batch losses (mrrl, rdl): 0.0, 4.978e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 384
rank avg (pred): 0.248 +- 0.225
mrr vals (pred, true): 0.351, 0.124
batch losses (mrrl, rdl): 0.0, 0.0002481503

Epoch over!
epoch time: 13.657

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1169
rank avg (pred): 0.383 +- 0.322
mrr vals (pred, true): 0.320, 0.009
batch losses (mrrl, rdl): 0.0, 0.0001985637

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1195
rank avg (pred): 0.392 +- 0.331
mrr vals (pred, true): 0.323, 0.005
batch losses (mrrl, rdl): 0.0, 4.08776e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.261 +- 0.242
mrr vals (pred, true): 0.361, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005973441

Epoch over!
epoch time: 13.774

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 313
rank avg (pred): 0.046 +- 0.044
mrr vals (pred, true): 0.409, 0.220
batch losses (mrrl, rdl): 0.0, 3.0582e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1156
rank avg (pred): 0.166 +- 0.157
mrr vals (pred, true): 0.379, 0.026
batch losses (mrrl, rdl): 0.0, 8.6233e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.049 +- 0.048
mrr vals (pred, true): 0.426, 0.238
batch losses (mrrl, rdl): 0.0, 1.4635e-06

Epoch over!
epoch time: 13.084

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 993
rank avg (pred): 0.045 +- 0.044
mrr vals (pred, true): 0.421, 0.234
batch losses (mrrl, rdl): 0.0, 1.039e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 4
rank avg (pred): 0.063 +- 0.059
mrr vals (pred, true): 0.370, 0.180
batch losses (mrrl, rdl): 0.0, 5.404e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 16
rank avg (pred): 0.042 +- 0.042
mrr vals (pred, true): 0.441, 0.199
batch losses (mrrl, rdl): 0.0, 1.6533e-06

Epoch over!
epoch time: 13.283

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 145
rank avg (pred): 0.262 +- 0.252
mrr vals (pred, true): 0.363, 0.141
batch losses (mrrl, rdl): 0.4932487011, 0.0006604463

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 747
rank avg (pred): 0.120 +- 0.062
mrr vals (pred, true): 0.135, 0.133
batch losses (mrrl, rdl): 3.41399e-05, 4.93683e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 648
rank avg (pred): 0.449 +- 0.212
mrr vals (pred, true): 0.056, 0.003
batch losses (mrrl, rdl): 0.0003420409, 2.6083e-05

Epoch over!
epoch time: 13.226

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.367 +- 0.176
mrr vals (pred, true): 0.061, 0.004
batch losses (mrrl, rdl): 0.0012118628, 0.0002024561

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 772
rank avg (pred): 0.304 +- 0.151
mrr vals (pred, true): 0.102, 0.189
batch losses (mrrl, rdl): 0.0748300701, 0.0011151292

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 494
rank avg (pred): 0.326 +- 0.158
mrr vals (pred, true): 0.064, 0.028
batch losses (mrrl, rdl): 0.002101613, 0.000475699

Epoch over!
epoch time: 13.377

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 68
rank avg (pred): 0.008 +- 0.004
mrr vals (pred, true): 0.222, 0.230
batch losses (mrrl, rdl): 0.0006030395, 2.667e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 242
rank avg (pred): 0.210 +- 0.107
mrr vals (pred, true): 0.123, 0.004
batch losses (mrrl, rdl): 0.0539105237, 0.0013655805

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1115
rank avg (pred): 0.234 +- 0.120
mrr vals (pred, true): 0.103, 0.004
batch losses (mrrl, rdl): 0.0281466842, 0.0009820474

Epoch over!
epoch time: 13.225

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1110
rank avg (pred): 0.262 +- 0.138
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0443042032, 0.0008304755

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1069
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.270, 0.270
batch losses (mrrl, rdl): 4.91e-08, 2.42126e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 879
rank avg (pred): 0.322 +- 0.157
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0213646349, 0.0003983294

Epoch over!
epoch time: 12.983

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 811
rank avg (pred): 0.097 +- 0.053
mrr vals (pred, true): 0.154, 0.120
batch losses (mrrl, rdl): 0.011903042, 2.77262e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.388 +- 0.161
mrr vals (pred, true): 0.034, 0.003
batch losses (mrrl, rdl): 0.0024392391, 0.0002006541

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.381 +- 0.163
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 0.0001002031, 0.0002003449

Epoch over!
epoch time: 13.187

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1191
rank avg (pred): 0.325 +- 0.151
mrr vals (pred, true): 0.069, 0.004
batch losses (mrrl, rdl): 0.0034734465, 0.0004068693

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 893
rank avg (pred): 0.338 +- 0.161
mrr vals (pred, true): 0.084, 0.035
batch losses (mrrl, rdl): 0.011822232, 0.0009895996

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 955
rank avg (pred): 0.303 +- 0.141
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.007267145, 0.0004797817

Epoch over!
epoch time: 13.333

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.008 +- 0.004
mrr vals (pred, true): 0.217, 0.252
batch losses (mrrl, rdl): 0.0124624269, 3.05083e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 352
rank avg (pred): 0.297 +- 0.144
mrr vals (pred, true): 0.104, 0.148
batch losses (mrrl, rdl): 0.0193410628, 0.0004941876

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.063 +- 0.032
mrr vals (pred, true): 0.141, 0.186
batch losses (mrrl, rdl): 0.0197767075, 1.20189e-05

Epoch over!
epoch time: 12.804

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 829
rank avg (pred): 0.108 +- 0.058
mrr vals (pred, true): 0.159, 0.121
batch losses (mrrl, rdl): 0.0145996911, 2.39311e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.197 +- 0.104
mrr vals (pred, true): 0.137, 0.005
batch losses (mrrl, rdl): 0.0761985704, 0.0013964053

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 839
rank avg (pred): 0.335 +- 0.161
mrr vals (pred, true): 0.097, 0.171
batch losses (mrrl, rdl): 0.0555023476, 0.0012893934

Epoch over!
epoch time: 12.825

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 701
rank avg (pred): 0.359 +- 0.154
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004845058, 0.0003168471

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 123
rank avg (pred): 0.330 +- 0.155
mrr vals (pred, true): 0.092, 0.109
batch losses (mrrl, rdl): 0.0028020944, 0.0009737327

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.351 +- 0.147
mrr vals (pred, true): 0.049, 0.015
batch losses (mrrl, rdl): 7.5365e-06, 0.0003272402

Epoch over!
epoch time: 12.885

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.292 +- 0.145
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0350292176, 0.0005143161

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 850
rank avg (pred): 0.306 +- 0.148
mrr vals (pred, true): 0.105, 0.167
batch losses (mrrl, rdl): 0.0377585962, 0.0009546597

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 587
rank avg (pred): 0.349 +- 0.149
mrr vals (pred, true): 0.048, 0.021
batch losses (mrrl, rdl): 4.36964e-05, 0.0001187101

Epoch over!
epoch time: 12.36

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.016 +- 0.008
mrr vals (pred, true): 0.181, 0.168

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03153 	 0.00286 	 ~...
   24 	     1 	 0.04152 	 0.00336 	 m..s
   56 	     2 	 0.08666 	 0.00338 	 m..s
   71 	     3 	 0.09442 	 0.00340 	 m..s
   11 	     4 	 0.03366 	 0.00350 	 m..s
   66 	     5 	 0.09204 	 0.00351 	 m..s
   68 	     6 	 0.09213 	 0.00354 	 m..s
   64 	     7 	 0.09144 	 0.00360 	 m..s
   18 	     8 	 0.03721 	 0.00377 	 m..s
   45 	     9 	 0.07779 	 0.00380 	 m..s
   13 	    10 	 0.03430 	 0.00383 	 m..s
   80 	    11 	 0.10312 	 0.00383 	 m..s
   34 	    12 	 0.06532 	 0.00386 	 m..s
   51 	    13 	 0.08344 	 0.00389 	 m..s
   37 	    14 	 0.06927 	 0.00397 	 m..s
   42 	    15 	 0.07549 	 0.00400 	 m..s
    2 	    16 	 0.03153 	 0.00405 	 ~...
   77 	    17 	 0.09955 	 0.00406 	 m..s
   73 	    18 	 0.09491 	 0.00408 	 m..s
   20 	    19 	 0.03934 	 0.00411 	 m..s
   58 	    20 	 0.08788 	 0.00413 	 m..s
   44 	    21 	 0.07652 	 0.00414 	 m..s
   87 	    22 	 0.10723 	 0.00435 	 MISS
   46 	    23 	 0.07782 	 0.00452 	 m..s
   41 	    24 	 0.07507 	 0.00459 	 m..s
   81 	    25 	 0.10315 	 0.00464 	 m..s
   48 	    26 	 0.08205 	 0.00465 	 m..s
   70 	    27 	 0.09418 	 0.00519 	 m..s
   79 	    28 	 0.10307 	 0.00528 	 m..s
    0 	    29 	 0.03063 	 0.00668 	 ~...
   14 	    30 	 0.03494 	 0.00672 	 ~...
   10 	    31 	 0.03358 	 0.00725 	 ~...
    8 	    32 	 0.03330 	 0.00972 	 ~...
   30 	    33 	 0.04870 	 0.01077 	 m..s
    9 	    34 	 0.03355 	 0.01083 	 ~...
    4 	    35 	 0.03191 	 0.01122 	 ~...
   26 	    36 	 0.04395 	 0.01230 	 m..s
   15 	    37 	 0.03516 	 0.01271 	 ~...
   17 	    38 	 0.03567 	 0.01387 	 ~...
   21 	    39 	 0.03946 	 0.01431 	 ~...
   16 	    40 	 0.03560 	 0.01528 	 ~...
   22 	    41 	 0.03996 	 0.01761 	 ~...
   19 	    42 	 0.03842 	 0.01807 	 ~...
    5 	    43 	 0.03214 	 0.01879 	 ~...
   25 	    44 	 0.04156 	 0.01896 	 ~...
   23 	    45 	 0.04085 	 0.01910 	 ~...
    3 	    46 	 0.03176 	 0.02011 	 ~...
   12 	    47 	 0.03368 	 0.02020 	 ~...
   31 	    48 	 0.04995 	 0.02199 	 ~...
   33 	    49 	 0.05467 	 0.02299 	 m..s
    6 	    50 	 0.03259 	 0.02328 	 ~...
   32 	    51 	 0.05235 	 0.02514 	 ~...
    7 	    52 	 0.03312 	 0.02572 	 ~...
   51 	    53 	 0.08344 	 0.02829 	 m..s
   27 	    54 	 0.04440 	 0.03418 	 ~...
   29 	    55 	 0.04471 	 0.03427 	 ~...
   28 	    56 	 0.04442 	 0.03512 	 ~...
   53 	    57 	 0.08401 	 0.03956 	 m..s
   35 	    58 	 0.06724 	 0.10045 	 m..s
   36 	    59 	 0.06735 	 0.10533 	 m..s
   43 	    60 	 0.07550 	 0.11933 	 m..s
   38 	    61 	 0.06932 	 0.13204 	 m..s
   89 	    62 	 0.12547 	 0.13253 	 ~...
   47 	    63 	 0.07905 	 0.13717 	 m..s
   90 	    64 	 0.14239 	 0.14560 	 ~...
   49 	    65 	 0.08273 	 0.14909 	 m..s
   82 	    66 	 0.10398 	 0.15094 	 m..s
   60 	    67 	 0.08861 	 0.15261 	 m..s
   54 	    68 	 0.08618 	 0.15622 	 m..s
   61 	    69 	 0.09046 	 0.15688 	 m..s
   40 	    70 	 0.07348 	 0.15721 	 m..s
   88 	    71 	 0.12462 	 0.15806 	 m..s
   39 	    72 	 0.07312 	 0.16158 	 m..s
   92 	    73 	 0.16950 	 0.16218 	 ~...
   99 	    74 	 0.18238 	 0.16384 	 ~...
   59 	    75 	 0.08813 	 0.16486 	 m..s
   86 	    76 	 0.10627 	 0.16538 	 m..s
   69 	    77 	 0.09280 	 0.16554 	 m..s
   93 	    78 	 0.17067 	 0.16630 	 ~...
   83 	    79 	 0.10421 	 0.16692 	 m..s
   95 	    80 	 0.17627 	 0.16779 	 ~...
   98 	    81 	 0.18091 	 0.16793 	 ~...
   57 	    82 	 0.08755 	 0.17176 	 m..s
   74 	    83 	 0.09670 	 0.17181 	 m..s
   96 	    84 	 0.17667 	 0.17282 	 ~...
   75 	    85 	 0.09834 	 0.17298 	 m..s
   65 	    86 	 0.09153 	 0.17389 	 m..s
   55 	    87 	 0.08632 	 0.17519 	 m..s
   97 	    88 	 0.18024 	 0.17631 	 ~...
   67 	    89 	 0.09204 	 0.17829 	 m..s
   72 	    90 	 0.09454 	 0.17979 	 m..s
   94 	    91 	 0.17454 	 0.18073 	 ~...
   76 	    92 	 0.09950 	 0.18195 	 m..s
  114 	    93 	 0.23219 	 0.18523 	 m..s
   50 	    94 	 0.08277 	 0.18549 	 MISS
   78 	    95 	 0.10116 	 0.18895 	 m..s
  111 	    96 	 0.22448 	 0.19071 	 m..s
   91 	    97 	 0.16921 	 0.19182 	 ~...
  102 	    98 	 0.18704 	 0.19379 	 ~...
  113 	    99 	 0.23208 	 0.19682 	 m..s
  105 	   100 	 0.19409 	 0.19870 	 ~...
   63 	   101 	 0.09127 	 0.19925 	 MISS
  103 	   102 	 0.18710 	 0.20029 	 ~...
  100 	   103 	 0.18259 	 0.20093 	 ~...
  109 	   104 	 0.21403 	 0.20160 	 ~...
   62 	   105 	 0.09125 	 0.20560 	 MISS
  106 	   106 	 0.19461 	 0.20832 	 ~...
  101 	   107 	 0.18652 	 0.21060 	 ~...
  104 	   108 	 0.19347 	 0.21220 	 ~...
  107 	   109 	 0.19548 	 0.21490 	 ~...
   84 	   110 	 0.10554 	 0.21492 	 MISS
  108 	   111 	 0.21103 	 0.21757 	 ~...
   85 	   112 	 0.10611 	 0.21777 	 MISS
  115 	   113 	 0.24073 	 0.22640 	 ~...
  119 	   114 	 0.28286 	 0.22892 	 m..s
  117 	   115 	 0.24753 	 0.22900 	 ~...
  112 	   116 	 0.22784 	 0.23882 	 ~...
  118 	   117 	 0.26313 	 0.23952 	 ~...
  110 	   118 	 0.21499 	 0.24540 	 m..s
  116 	   119 	 0.24319 	 0.25289 	 ~...
  120 	   120 	 0.30114 	 0.26566 	 m..s
==========================================
r_mrr = 0.7761034965515137
r2_mrr = 0.5988175272941589
spearmanr_mrr@5 = 0.9865883588790894
spearmanr_mrr@10 = 0.9687573909759521
spearmanr_mrr@50 = 0.9743192195892334
spearmanr_mrr@100 = 0.8427320122718811
spearmanr_mrr@All = 0.8811438083648682
==========================================
test time: 0.4
Done Testing dataset CoDExSmall
total time taken: 208.18003678321838
training time taken: 197.25849294662476
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7761)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5988)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9866)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9688)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9743)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8427)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8811)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.5065531315121916}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4486418957022485
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [781, 336, 1017, 486, 670, 353, 512, 880, 26, 296, 237, 813, 615, 1012, 567, 6, 1152, 41, 613, 653, 1010, 1026, 110, 927, 471, 689, 423, 298, 162, 439, 33, 765, 735, 150, 87, 273, 1071, 342, 306, 1110, 349, 231, 258, 105, 165, 27, 483, 704, 28, 370, 576, 648, 164, 404, 772, 827, 234, 1135, 601, 973, 1167, 149, 843, 1041, 818, 35, 681, 588, 1128, 718, 773, 147, 1196, 1168, 130, 756, 794, 538, 351, 767, 463, 55, 989, 1137, 388, 760, 786, 1008, 1183, 882, 184, 350, 699, 762, 895, 1164, 253, 341, 856, 1073, 684, 1134, 53, 1015, 698, 94, 708, 933, 1214, 870, 865, 69, 955, 346, 1057, 326, 1087, 811, 1132, 330, 261]
valid_ids (0): []
train_ids (1094): [858, 700, 1031, 707, 995, 706, 731, 355, 380, 469, 1066, 948, 751, 280, 91, 36, 182, 50, 76, 254, 673, 745, 730, 826, 966, 978, 289, 920, 195, 339, 1170, 92, 444, 1011, 119, 474, 1004, 901, 816, 1206, 787, 20, 1051, 835, 657, 557, 450, 1160, 558, 8, 726, 935, 961, 1146, 153, 801, 930, 623, 1081, 80, 1093, 982, 410, 1121, 859, 1068, 1104, 470, 593, 1097, 1101, 2, 9, 1193, 127, 467, 722, 112, 603, 912, 664, 286, 74, 671, 1161, 759, 1033, 386, 154, 403, 609, 190, 903, 1151, 125, 238, 1067, 727, 16, 1117, 608, 445, 392, 947, 1118, 1094, 161, 625, 1116, 244, 1016, 540, 270, 179, 146, 646, 391, 1169, 305, 301, 831, 38, 495, 304, 285, 923, 54, 579, 577, 299, 916, 95, 658, 318, 728, 227, 878, 365, 72, 778, 398, 1034, 1083, 1032, 1047, 115, 93, 451, 476, 57, 313, 390, 884, 1019, 14, 929, 1158, 1058, 846, 531, 986, 460, 1143, 458, 205, 833, 1150, 457, 120, 368, 260, 1029, 847, 823, 627, 564, 712, 491, 784, 523, 957, 186, 377, 893, 854, 62, 268, 48, 743, 331, 1042, 675, 1106, 275, 357, 199, 369, 128, 871, 586, 639, 534, 307, 433, 178, 183, 171, 1014, 716, 951, 672, 240, 714, 894, 347, 628, 266, 739, 156, 630, 425, 886, 66, 96, 409, 375, 1197, 1124, 315, 605, 747, 251, 552, 524, 60, 659, 908, 405, 416, 785, 1075, 1200, 780, 1095, 553, 1102, 49, 389, 802, 513, 544, 562, 983, 909, 230, 505, 1122, 725, 1157, 761, 245, 382, 817, 1105, 1192, 427, 1063, 809, 497, 1202, 311, 876, 533, 116, 99, 297, 1030, 434, 1089, 796, 1074, 293, 309, 791, 898, 148, 566, 1144, 1207, 848, 507, 600, 447, 272, 89, 1209, 666, 1055, 111, 1006, 257, 776, 192, 461, 1018, 489, 738, 994, 1025, 656, 713, 680, 132, 749, 873, 508, 494, 742, 555, 1085, 674, 842, 526, 988, 1082, 103, 345, 175, 967, 1153, 78, 7, 492, 201, 650, 549, 1145, 734, 34, 705, 755, 1024, 965, 741, 757, 4, 678, 883, 70, 866, 372, 560, 71, 233, 314, 117, 750, 599, 852, 1129, 188, 431, 0, 753, 770, 550, 922, 950, 721, 44, 595, 11, 321, 928, 194, 424, 931, 596, 411, 343, 1187, 1127, 122, 1096, 335, 1072, 906, 374, 527, 913, 956, 276, 793, 782, 732, 442, 294, 1115, 1022, 572, 998, 107, 621, 774, 582, 277, 319, 963, 585, 75, 815, 529, 992, 643, 212, 701, 807, 64, 200, 252, 652, 869, 626, 1023, 758, 530, 979, 1046, 185, 402, 547, 729, 918, 970, 812, 10, 262, 581, 971, 209, 1159, 422, 810, 1125, 239, 669, 337, 56, 449, 1060, 975, 287, 717, 429, 647, 622, 768, 448, 216, 136, 518, 393, 406, 151, 720, 1009, 946, 159, 620, 528, 24, 584, 651, 248, 949, 267, 302, 515, 84, 500, 464, 934, 366, 889, 269, 378, 493, 1007, 954, 490, 17, 855, 207, 271, 316, 358, 303, 635, 911, 1103, 1002, 475, 30, 215, 376, 1162, 644, 247, 1027, 872, 710, 224, 925, 1163, 535, 1155, 541, 1133, 479, 545, 839, 536, 228, 1079, 81, 754, 636, 687, 1107, 897, 959, 452, 612, 1140, 1204, 373, 462, 829, 59, 259, 875, 282, 969, 520, 210, 624, 568, 677, 556, 1005, 891, 1194, 1184, 783, 487, 1199, 438, 1111, 703, 1176, 841, 384, 142, 940, 400, 1208, 638, 395, 25, 551, 1044, 362, 225, 29, 432, 332, 481, 418, 12, 591, 1165, 173, 58, 454, 915, 1172, 1003, 22, 746, 1203, 1213, 1136, 124, 804, 850, 694, 607, 459, 1077, 740, 942, 715, 455, 1195, 408, 498, 300, 667, 999, 905, 921, 900, 141, 68, 163, 435, 1039, 691, 800, 598, 21, 1108, 39, 98, 805, 480, 436, 143, 288, 137, 851, 256, 86, 232, 795, 764, 665, 616, 1040, 594, 709, 1201, 291, 1109, 574, 61, 797, 501, 168, 695, 32, 1182, 340, 844, 597, 1064, 140, 690, 1084, 939, 77, 104, 121, 421, 1054, 611, 539, 1001, 155, 981, 1123, 13, 655, 1174, 509, 697, 619, 100, 937, 821, 219, 649, 176, 1099, 1154, 1038, 338, 1112, 868, 102, 144, 82, 881, 312, 885, 354, 532, 634, 85, 985, 663, 478, 443, 348, 1139, 437, 1088, 170, 18, 236, 792, 902, 1037, 836, 968, 246, 158, 419, 67, 990, 504, 208, 676, 1185, 468, 1119, 367, 472, 242, 1070, 222, 97, 838, 662, 1212, 1211, 641, 352, 633, 223, 977, 5, 590, 417, 1062, 1086, 363, 1198, 693, 1180, 1156, 629, 583, 737, 592, 135, 604, 860, 323, 456, 134, 679, 385, 642, 1148, 941, 1126, 537, 1113, 1028, 887, 840, 514, 265, 571, 1191, 295, 322, 221, 867, 484, 1050, 334, 640, 580, 845, 453, 1065, 1013, 814, 888, 379, 991, 1061, 565, 465, 283, 861, 711, 987, 788, 919, 775, 241, 51, 229, 145, 1171, 356, 863, 290, 1020, 779, 129, 837, 1166, 1090, 522, 736, 19, 381, 953, 668, 413, 1053, 849, 569, 958, 325, 777, 575, 570, 281, 654, 561, 879, 926, 101, 1076, 1138, 972, 213, 1059, 1120, 139, 503, 440, 399, 692, 198, 521, 15, 1181, 359, 106, 1098, 853, 748, 789, 415, 420, 324, 752, 798, 892, 1078, 719, 1147, 177, 1175, 563, 974, 573, 1210, 542, 361, 525, 226, 733, 278, 546, 292, 167, 364, 1186, 255, 686, 519, 360, 1036, 264, 45, 138, 396, 412, 1049, 83, 114, 1205, 1021, 407, 46, 499, 799, 90, 133, 661, 830, 113, 40, 63, 511, 877, 993, 214, 1100, 485, 1069, 907, 344, 310, 1091, 702, 820, 169, 160, 824, 263, 328, 73, 47, 1092, 414, 174, 506, 517, 862, 1048, 126, 790, 944, 825, 31, 473, 1177, 217, 203, 181, 88, 193, 383, 936, 187, 397, 166, 211, 960, 688, 932, 744, 803, 52, 1190, 543, 724, 284, 482, 890, 249, 202, 945, 152, 387, 682, 606, 1179, 488, 1000, 819, 118, 874, 502, 279, 317, 578, 832, 371, 206, 610, 614, 723, 1056, 632, 996, 763, 769, 980, 131, 822, 1130, 976, 172, 917, 1178, 696, 308, 1052, 1149, 997, 554, 1142, 466, 196, 65, 864, 510, 3, 618, 899, 548, 394, 37, 204, 1189, 1114, 857, 329, 645, 430, 43, 587, 631, 1188, 617, 962, 180, 683, 426, 896, 952, 984, 766, 964, 250, 943, 191, 559, 660, 1131, 637, 428, 218, 1045, 516, 157, 23, 108, 477, 924, 320, 243, 42, 1035, 1, 1080, 327, 806, 197, 401, 274, 589, 685, 1173, 1141, 446, 441, 834, 904, 828, 109, 914, 496, 808, 189, 1043, 333, 79, 938, 235, 771, 123, 602, 220, 910]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7675666536263713
the save name prefix for this run is:  chkpt-ID_7675666536263713_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.568 +- 0.006
mrr vals (pred, true): 0.001, 0.173
batch losses (mrrl, rdl): 0.0, 0.0047350102

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.219 +- 0.172
mrr vals (pred, true): 0.114, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009491958

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.254 +- 0.220
mrr vals (pred, true): 0.231, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006594507

Epoch over!
epoch time: 12.246

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 584
rank avg (pred): 0.337 +- 0.285
mrr vals (pred, true): 0.219, 0.020
batch losses (mrrl, rdl): 0.0, 6.67882e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 740
rank avg (pred): 0.032 +- 0.034
mrr vals (pred, true): 0.394, 0.133
batch losses (mrrl, rdl): 0.0, 3.90711e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 4
rank avg (pred): 0.055 +- 0.058
mrr vals (pred, true): 0.368, 0.180
batch losses (mrrl, rdl): 0.0, 2.1071e-06

Epoch over!
epoch time: 12.174

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 376
rank avg (pred): 0.284 +- 0.268
mrr vals (pred, true): 0.259, 0.162
batch losses (mrrl, rdl): 0.0, 0.000726524

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1121
rank avg (pred): 0.270 +- 0.271
mrr vals (pred, true): 0.288, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004327131

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 717
rank avg (pred): 0.410 +- 0.310
mrr vals (pred, true): 0.144, 0.003
batch losses (mrrl, rdl): 0.0, 4.85994e-05

Epoch over!
epoch time: 12.669

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.256 +- 0.271
mrr vals (pred, true): 0.326, 0.169
batch losses (mrrl, rdl): 0.0, 0.000665753

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.315 +- 0.301
mrr vals (pred, true): 0.296, 0.026
batch losses (mrrl, rdl): 0.0, 4.56214e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1038
rank avg (pred): 0.258 +- 0.277
mrr vals (pred, true): 0.331, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006695922

Epoch over!
epoch time: 11.849

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.056 +- 0.063
mrr vals (pred, true): 0.395, 0.168
batch losses (mrrl, rdl): 0.0, 1.13695e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1103
rank avg (pred): 0.260 +- 0.276
mrr vals (pred, true): 0.337, 0.198
batch losses (mrrl, rdl): 0.0, 0.0007593671

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 278
rank avg (pred): 0.037 +- 0.044
mrr vals (pred, true): 0.455, 0.191
batch losses (mrrl, rdl): 0.0, 2.09243e-05

Epoch over!
epoch time: 13.329

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 122
rank avg (pred): 0.260 +- 0.280
mrr vals (pred, true): 0.342, 0.175
batch losses (mrrl, rdl): 0.2772646546, 0.000770405

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 883
rank avg (pred): 0.350 +- 0.225
mrr vals (pred, true): 0.119, 0.004
batch losses (mrrl, rdl): 0.0472257957, 0.0001687738

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.412 +- 0.166
mrr vals (pred, true): 0.079, 0.025
batch losses (mrrl, rdl): 0.0084610451, 0.0011988073

Epoch over!
epoch time: 12.785

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 420
rank avg (pred): 0.335 +- 0.207
mrr vals (pred, true): 0.106, 0.004
batch losses (mrrl, rdl): 0.0315669104, 0.0002138491

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 173
rank avg (pred): 0.403 +- 0.243
mrr vals (pred, true): 0.114, 0.004
batch losses (mrrl, rdl): 0.041177243, 3.36317e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 453
rank avg (pred): 0.330 +- 0.188
mrr vals (pred, true): 0.094, 0.004
batch losses (mrrl, rdl): 0.01892405, 0.000291049

Epoch over!
epoch time: 12.361

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.337 +- 0.124
mrr vals (pred, true): 0.080, 0.027
batch losses (mrrl, rdl): 0.0088042943, 0.0006548673

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 142
rank avg (pred): 0.420 +- 0.233
mrr vals (pred, true): 0.107, 0.129
batch losses (mrrl, rdl): 0.0047760801, 0.0020491991

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.039 +- 0.023
mrr vals (pred, true): 0.144, 0.190
batch losses (mrrl, rdl): 0.0211732909, 2.51e-06

Epoch over!
epoch time: 12.877

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 596
rank avg (pred): 0.339 +- 0.103
mrr vals (pred, true): 0.048, 0.018
batch losses (mrrl, rdl): 5.21498e-05, 8.9711e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 403
rank avg (pred): 0.355 +- 0.209
mrr vals (pred, true): 0.112, 0.175
batch losses (mrrl, rdl): 0.040086709, 0.0013849164

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.333, 0.260
batch losses (mrrl, rdl): 0.0529748984, 3.0022e-05

Epoch over!
epoch time: 12.287

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 284
rank avg (pred): 0.011 +- 0.007
mrr vals (pred, true): 0.227, 0.243
batch losses (mrrl, rdl): 0.0026730741, 2.84821e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 223
rank avg (pred): 0.409 +- 0.218
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0214718282, 5.39569e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 313
rank avg (pred): 0.023 +- 0.014
mrr vals (pred, true): 0.176, 0.220
batch losses (mrrl, rdl): 0.0195303578, 2.17445e-05

Epoch over!
epoch time: 12.697

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.360 +- 0.211
mrr vals (pred, true): 0.114, 0.125
batch losses (mrrl, rdl): 0.0013013161, 0.0009837555

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.235, 0.252
batch losses (mrrl, rdl): 0.0027788803, 2.86921e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.329 +- 0.105
mrr vals (pred, true): 0.059, 0.011
batch losses (mrrl, rdl): 0.0008896008, 5.76081e-05

Epoch over!
epoch time: 12.26

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 503
rank avg (pred): 0.309 +- 0.102
mrr vals (pred, true): 0.062, 0.030
batch losses (mrrl, rdl): 0.0013860918, 0.0004450483

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 967
rank avg (pred): 0.394 +- 0.199
mrr vals (pred, true): 0.093, 0.004
batch losses (mrrl, rdl): 0.0181314126, 9.19372e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 447
rank avg (pred): 0.363 +- 0.207
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.0370411053, 0.0001217487

Epoch over!
epoch time: 13.349

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 182
rank avg (pred): 0.373 +- 0.200
mrr vals (pred, true): 0.105, 0.004
batch losses (mrrl, rdl): 0.0299734529, 0.0001428691

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 67
rank avg (pred): 0.020 +- 0.012
mrr vals (pred, true): 0.189, 0.205
batch losses (mrrl, rdl): 0.002599102, 1.7655e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.326, 0.267
batch losses (mrrl, rdl): 0.0354109518, 2.71913e-05

Epoch over!
epoch time: 13.338

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.349 +- 0.199
mrr vals (pred, true): 0.113, 0.004
batch losses (mrrl, rdl): 0.0390858985, 0.0002123577

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.364 +- 0.187
mrr vals (pred, true): 0.092, 0.004
batch losses (mrrl, rdl): 0.0179585051, 0.0001825261

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 942
rank avg (pred): 0.352 +- 0.187
mrr vals (pred, true): 0.100, 0.174
batch losses (mrrl, rdl): 0.0549010374, 0.0016838033

Epoch over!
epoch time: 13.99

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.322 +- 0.093
mrr vals (pred, true): 0.051, 0.019
batch losses (mrrl, rdl): 7.7012e-06, 0.0002716439

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1188
rank avg (pred): 0.327 +- 0.104
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.000667891, 0.0004826548

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 971
rank avg (pred): 0.346 +- 0.179
mrr vals (pred, true): 0.098, 0.004
batch losses (mrrl, rdl): 0.0234348215, 0.0002544242

Epoch over!
epoch time: 12.556

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.351 +- 0.181
mrr vals (pred, true): 0.092, 0.182

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04293 	 0.00286 	 m..s
    5 	     1 	 0.04276 	 0.00304 	 m..s
   12 	     2 	 0.04423 	 0.00327 	 m..s
    7 	     3 	 0.04288 	 0.00329 	 m..s
   22 	     4 	 0.05006 	 0.00330 	 m..s
   10 	     5 	 0.04392 	 0.00335 	 m..s
   15 	     6 	 0.04441 	 0.00338 	 m..s
   11 	     7 	 0.04402 	 0.00348 	 m..s
    1 	     8 	 0.04094 	 0.00356 	 m..s
   32 	     9 	 0.08211 	 0.00362 	 m..s
   64 	    10 	 0.09383 	 0.00366 	 m..s
   45 	    11 	 0.08639 	 0.00368 	 m..s
   34 	    12 	 0.08284 	 0.00370 	 m..s
   47 	    13 	 0.08717 	 0.00377 	 m..s
   54 	    14 	 0.09018 	 0.00380 	 m..s
   33 	    15 	 0.08263 	 0.00386 	 m..s
   31 	    16 	 0.08153 	 0.00390 	 m..s
   80 	    17 	 0.09975 	 0.00392 	 m..s
   75 	    18 	 0.09617 	 0.00394 	 m..s
   41 	    19 	 0.08551 	 0.00397 	 m..s
   14 	    20 	 0.04433 	 0.00400 	 m..s
   85 	    21 	 0.10355 	 0.00410 	 m..s
   20 	    22 	 0.04882 	 0.00411 	 m..s
    6 	    23 	 0.04285 	 0.00413 	 m..s
   59 	    24 	 0.09381 	 0.00414 	 m..s
   13 	    25 	 0.04433 	 0.00415 	 m..s
   66 	    26 	 0.09417 	 0.00422 	 m..s
   36 	    27 	 0.08327 	 0.00422 	 m..s
   91 	    28 	 0.11063 	 0.00427 	 MISS
   67 	    29 	 0.09423 	 0.00433 	 m..s
   93 	    30 	 0.11249 	 0.00435 	 MISS
   77 	    31 	 0.09767 	 0.00435 	 m..s
   71 	    32 	 0.09520 	 0.00451 	 m..s
   58 	    33 	 0.09302 	 0.00452 	 m..s
   88 	    34 	 0.10683 	 0.00457 	 MISS
   69 	    35 	 0.09474 	 0.00522 	 m..s
    3 	    36 	 0.04197 	 0.00540 	 m..s
   48 	    37 	 0.08755 	 0.00551 	 m..s
    8 	    38 	 0.04292 	 0.00816 	 m..s
    0 	    39 	 0.04027 	 0.00882 	 m..s
   21 	    40 	 0.04993 	 0.00886 	 m..s
   19 	    41 	 0.04872 	 0.00916 	 m..s
   18 	    42 	 0.04868 	 0.00942 	 m..s
    4 	    43 	 0.04246 	 0.00943 	 m..s
   23 	    44 	 0.05024 	 0.01006 	 m..s
   16 	    45 	 0.04468 	 0.01066 	 m..s
    2 	    46 	 0.04195 	 0.01162 	 m..s
   17 	    47 	 0.04755 	 0.01493 	 m..s
   25 	    48 	 0.05150 	 0.01946 	 m..s
   26 	    49 	 0.05942 	 0.02247 	 m..s
   27 	    50 	 0.05942 	 0.02334 	 m..s
   28 	    51 	 0.05982 	 0.02427 	 m..s
   24 	    52 	 0.05138 	 0.02613 	 ~...
   29 	    53 	 0.06061 	 0.02632 	 m..s
   40 	    54 	 0.08529 	 0.03683 	 m..s
   37 	    55 	 0.08334 	 0.04243 	 m..s
   49 	    56 	 0.08761 	 0.04388 	 m..s
   94 	    57 	 0.11348 	 0.05558 	 m..s
   38 	    58 	 0.08346 	 0.09528 	 ~...
   43 	    59 	 0.08569 	 0.10903 	 ~...
   95 	    60 	 0.11463 	 0.11143 	 ~...
   30 	    61 	 0.07842 	 0.11214 	 m..s
   59 	    62 	 0.09381 	 0.11579 	 ~...
   35 	    63 	 0.08304 	 0.11669 	 m..s
   59 	    64 	 0.09381 	 0.11933 	 ~...
   96 	    65 	 0.11591 	 0.11981 	 ~...
   65 	    66 	 0.09414 	 0.12016 	 ~...
   72 	    67 	 0.09544 	 0.13184 	 m..s
   98 	    68 	 0.13347 	 0.13529 	 ~...
   39 	    69 	 0.08375 	 0.13774 	 m..s
  101 	    70 	 0.15679 	 0.14231 	 ~...
   44 	    71 	 0.08604 	 0.14271 	 m..s
  103 	    72 	 0.16020 	 0.15188 	 ~...
   97 	    73 	 0.11864 	 0.15206 	 m..s
   42 	    74 	 0.08560 	 0.15392 	 m..s
   52 	    75 	 0.08874 	 0.15609 	 m..s
   59 	    76 	 0.09381 	 0.15694 	 m..s
   46 	    77 	 0.08702 	 0.15721 	 m..s
   68 	    78 	 0.09467 	 0.15816 	 m..s
  102 	    79 	 0.15883 	 0.15845 	 ~...
   74 	    80 	 0.09611 	 0.15954 	 m..s
   70 	    81 	 0.09514 	 0.16024 	 m..s
  109 	    82 	 0.17108 	 0.16384 	 ~...
   59 	    83 	 0.09381 	 0.16538 	 m..s
   57 	    84 	 0.09301 	 0.16538 	 m..s
   73 	    85 	 0.09570 	 0.16573 	 m..s
   79 	    86 	 0.09838 	 0.16961 	 m..s
   78 	    87 	 0.09813 	 0.17084 	 m..s
   82 	    88 	 0.10218 	 0.17169 	 m..s
  107 	    89 	 0.16889 	 0.17242 	 ~...
  104 	    90 	 0.16187 	 0.17290 	 ~...
   50 	    91 	 0.08781 	 0.17614 	 m..s
   53 	    92 	 0.09003 	 0.17666 	 m..s
   84 	    93 	 0.10303 	 0.17700 	 m..s
  108 	    94 	 0.16900 	 0.17869 	 ~...
   81 	    95 	 0.10156 	 0.18074 	 m..s
   51 	    96 	 0.08871 	 0.18103 	 m..s
   56 	    97 	 0.09203 	 0.18195 	 m..s
  110 	    98 	 0.17995 	 0.18212 	 ~...
   83 	    99 	 0.10299 	 0.18219 	 m..s
  100 	   100 	 0.15141 	 0.18430 	 m..s
   76 	   101 	 0.09716 	 0.18507 	 m..s
  115 	   102 	 0.19645 	 0.18686 	 ~...
   55 	   103 	 0.09191 	 0.18871 	 m..s
   87 	   104 	 0.10479 	 0.18910 	 m..s
   90 	   105 	 0.10966 	 0.19002 	 m..s
   99 	   106 	 0.15090 	 0.19424 	 m..s
  117 	   107 	 0.19934 	 0.19574 	 ~...
  106 	   108 	 0.16599 	 0.20029 	 m..s
   86 	   109 	 0.10365 	 0.20390 	 MISS
  111 	   110 	 0.18192 	 0.20636 	 ~...
  112 	   111 	 0.18939 	 0.20723 	 ~...
   89 	   112 	 0.10915 	 0.21109 	 MISS
   92 	   113 	 0.11100 	 0.21567 	 MISS
  105 	   114 	 0.16470 	 0.22583 	 m..s
  116 	   115 	 0.19819 	 0.22743 	 ~...
  114 	   116 	 0.19193 	 0.23011 	 m..s
  118 	   117 	 0.23597 	 0.23706 	 ~...
  113 	   118 	 0.19060 	 0.24214 	 m..s
  120 	   119 	 0.24232 	 0.25940 	 ~...
  119 	   120 	 0.23957 	 0.27211 	 m..s
==========================================
r_mrr = 0.7361093163490295
r2_mrr = 0.5004346966743469
spearmanr_mrr@5 = 0.8341025114059448
spearmanr_mrr@10 = 0.9223588705062866
spearmanr_mrr@50 = 0.9606286883354187
spearmanr_mrr@100 = 0.8060808777809143
spearmanr_mrr@All = 0.8552491664886475
==========================================
test time: 0.509
Done Testing dataset CoDExSmall
total time taken: 200.73303198814392
training time taken: 191.3600730895996
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7361)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5004)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.8341)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9224)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9606)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8061)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8552)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.652029321741793}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4303420726650775
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 421, 392, 1014, 319, 547, 417, 654, 855, 925, 1168, 371, 224, 714, 355, 213, 484, 955, 1196, 804, 409, 807, 673, 1089, 828, 62, 154, 341, 125, 27, 576, 1151, 121, 84, 1160, 435, 759, 304, 874, 259, 1157, 134, 635, 607, 272, 1099, 385, 32, 580, 598, 796, 459, 1106, 527, 321, 267, 1011, 143, 1052, 867, 591, 716, 1188, 914, 870, 227, 1204, 275, 300, 447, 68, 751, 130, 210, 410, 615, 443, 157, 348, 2, 972, 60, 736, 913, 104, 625, 480, 884, 1060, 781, 922, 787, 1091, 329, 299, 370, 303, 155, 382, 798, 425, 78, 1042, 1173, 406, 959, 659, 1067, 1203, 715, 1122, 767, 1087, 556, 400, 1057, 241, 9, 706, 856, 937]
valid_ids (0): []
train_ids (1094): [95, 353, 948, 509, 273, 1199, 644, 992, 1137, 1094, 703, 82, 456, 269, 776, 1023, 915, 649, 129, 719, 1181, 575, 243, 1053, 826, 908, 761, 971, 74, 483, 772, 5, 1019, 879, 753, 1070, 90, 747, 667, 562, 1013, 426, 102, 601, 144, 819, 839, 458, 434, 1071, 295, 287, 708, 848, 454, 1075, 850, 639, 73, 825, 558, 260, 1100, 599, 829, 296, 248, 424, 712, 1142, 919, 658, 460, 1171, 857, 211, 803, 450, 1051, 18, 1128, 1004, 895, 905, 617, 12, 939, 1193, 966, 133, 1135, 492, 1195, 1167, 301, 120, 1103, 1098, 1049, 502, 1180, 208, 713, 111, 627, 324, 583, 1007, 618, 445, 1048, 1205, 632, 318, 735, 372, 277, 265, 1147, 845, 545, 270, 284, 640, 528, 985, 1101, 264, 452, 574, 100, 860, 89, 597, 595, 784, 239, 399, 1086, 186, 451, 950, 779, 970, 503, 1012, 340, 317, 87, 175, 488, 739, 205, 431, 550, 361, 1090, 26, 1170, 397, 518, 793, 231, 590, 989, 298, 114, 201, 69, 778, 813, 721, 696, 105, 1113, 37, 81, 1159, 469, 866, 854, 165, 1043, 262, 810, 203, 57, 475, 977, 740, 873, 572, 365, 827, 1024, 1092, 332, 229, 338, 695, 146, 1214, 947, 209, 629, 853, 552, 729, 1114, 903, 1134, 1208, 790, 918, 336, 279, 46, 233, 702, 670, 80, 872, 1115, 660, 136, 1149, 64, 215, 268, 156, 592, 40, 762, 543, 768, 52, 325, 880, 920, 21, 886, 717, 1213, 608, 328, 620, 1136, 1130, 403, 1123, 780, 110, 996, 374, 823, 30, 1107, 147, 398, 221, 153, 407, 891, 25, 305, 569, 145, 1045, 91, 816, 108, 47, 333, 344, 515, 228, 22, 184, 169, 1001, 815, 968, 331, 805, 643, 648, 230, 573, 88, 560, 489, 487, 529, 737, 34, 167, 979, 782, 15, 1097, 1041, 1076, 77, 1082, 85, 462, 878, 974, 605, 500, 883, 168, 662, 347, 1059, 1210, 570, 491, 422, 1073, 466, 252, 137, 987, 170, 63, 923, 54, 844, 628, 159, 1035, 468, 537, 70, 687, 728, 775, 148, 432, 724, 1186, 764, 461, 470, 257, 151, 393, 376, 285, 638, 1132, 245, 464, 603, 1172, 471, 748, 909, 172, 668, 894, 732, 115, 786, 166, 244, 140, 783, 292, 549, 359, 619, 741, 306, 669, 742, 478, 904, 838, 534, 991, 1069, 678, 930, 297, 103, 954, 967, 119, 652, 362, 181, 189, 725, 927, 463, 282, 1085, 395, 630, 745, 1175, 794, 578, 593, 997, 865, 1009, 36, 342, 437, 536, 692, 1145, 770, 830, 1058, 897, 164, 283, 750, 1003, 664, 1184, 316, 349, 817, 932, 1006, 413, 636, 1072, 986, 360, 238, 936, 928, 423, 150, 681, 935, 7, 131, 963, 694, 1032, 246, 579, 49, 1064, 634, 477, 1118, 219, 777, 582, 98, 1080, 862, 193, 101, 863, 266, 28, 523, 522, 621, 797, 236, 709, 554, 13, 419, 943, 93, 861, 834, 1183, 482, 1, 812, 1112, 526, 727, 1095, 387, 841, 840, 56, 1044, 3, 1040, 187, 611, 92, 6, 94, 1126, 436, 960, 14, 1198, 564, 357, 586, 1150, 594, 66, 48, 614, 314, 1127, 1133, 637, 646, 1108, 308, 258, 738, 944, 377, 1038, 1111, 1010, 521, 1125, 44, 495, 501, 898, 23, 188, 726, 773, 1081, 984, 1000, 1102, 366, 749, 1143, 311, 831, 29, 1055, 286, 563, 892, 496, 588, 519, 179, 290, 253, 567, 524, 746, 525, 921, 38, 19, 1015, 888, 722, 900, 135, 814, 20, 1025, 97, 206, 415, 472, 138, 975, 799, 404, 1185, 765, 4, 194, 256, 851, 806, 774, 112, 0, 758, 565, 139, 1165, 697, 881, 345, 1212, 788, 1169, 952, 1020, 242, 679, 811, 938, 1031, 142, 988, 384, 320, 995, 1202, 1131, 680, 339, 671, 1152, 312, 71, 688, 197, 1034, 438, 976, 1008, 192, 467, 1189, 53, 416, 542, 657, 17, 514, 1206, 280, 731, 852, 655, 11, 907, 118, 96, 1046, 113, 538, 1084, 212, 801, 755, 1109, 202, 1088, 733, 641, 160, 367, 39, 754, 42, 1117, 1017, 465, 394, 504, 389, 789, 929, 1029, 10, 1047, 497, 1036, 689, 401, 67, 1096, 973, 1192, 288, 182, 122, 771, 541, 158, 785, 375, 994, 388, 58, 833, 962, 396, 1187, 978, 566, 255, 561, 875, 1139, 520, 220, 693, 141, 899, 51, 335, 822, 1021, 479, 1116, 546, 585, 701, 949, 998, 1074, 234, 1141, 846, 613, 276, 176, 43, 232, 677, 132, 1144, 1176, 808, 414, 356, 75, 183, 358, 1056, 281, 707, 1062, 683, 942, 906, 1154, 261, 204, 61, 983, 832, 704, 16, 405, 408, 1028, 45, 982, 964, 718, 1140, 474, 499, 559, 555, 1018, 606, 124, 1153, 596, 612, 802, 951, 383, 455, 730, 792, 278, 1093, 294, 418, 1191, 247, 1120, 577, 173, 289, 123, 1066, 651, 800, 548, 847, 1039, 161, 217, 1065, 882, 616, 493, 476, 207, 584, 511, 969, 1002, 581, 354, 274, 896, 626, 291, 330, 149, 1124, 763, 836, 35, 877, 390, 924, 931, 791, 1121, 1026, 126, 917, 642, 322, 869, 876, 216, 1110, 198, 1200, 177, 843, 363, 871, 720, 199, 532, 531, 1146, 1119, 1027, 901, 911, 185, 271, 1050, 956, 196, 128, 1201, 310, 859, 557, 958, 59, 430, 442, 744, 1174, 412, 498, 65, 544, 946, 218, 535, 1162, 505, 868, 481, 1077, 622, 86, 999, 449, 539, 824, 571, 698, 842, 600, 76, 350, 705, 448, 1030, 820, 178, 510, 485, 690, 490, 663, 1194, 551, 609, 1016, 666, 1022, 33, 195, 427, 457, 902, 858, 700, 769, 351, 240, 1079, 428, 486, 610, 337, 473, 1163, 734, 650, 981, 379, 107, 225, 116, 24, 1129, 334, 106, 8, 1177, 887, 645, 849, 661, 302, 1158, 453, 433, 1033, 254, 1104, 1164, 589, 684, 711, 961, 1182, 893, 1138, 55, 604, 1054, 364, 665, 533, 152, 127, 190, 309, 647, 293, 313, 352, 263, 623, 685, 682, 315, 378, 980, 200, 916, 674, 327, 933, 223, 953, 323, 391, 250, 1063, 699, 835, 889, 926, 766, 890, 381, 631, 1083, 587, 1037, 99, 540, 760, 993, 117, 516, 691, 513, 818, 1105, 343, 506, 174, 1190, 1179, 1068, 1197, 251, 1166, 72, 934, 743, 508, 180, 214, 568, 885, 1209, 386, 965, 402, 757, 672, 440, 171, 109, 517, 553, 653, 941, 368, 945, 420, 710, 1156, 1148, 723, 235, 864, 226, 249, 444, 624, 990, 346, 633, 530, 957, 676, 41, 1078, 446, 795, 373, 163, 1161, 940, 307, 1211, 222, 752, 686, 1207, 50, 162, 1061, 31, 494, 79, 512, 1005, 83, 191, 675, 821, 237, 910, 326, 369, 837, 411, 439, 1155, 912, 809, 756, 441, 380, 602, 429, 656, 507]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2464276625823700
the save name prefix for this run is:  chkpt-ID_2464276625823700_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.439 +- 0.006
mrr vals (pred, true): 0.001, 0.267
batch losses (mrrl, rdl): 0.0, 0.0034673251

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.278 +- 0.172
mrr vals (pred, true): 0.076, 0.138
batch losses (mrrl, rdl): 0.0, 0.0006682104

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 199
rank avg (pred): 0.264 +- 0.197
mrr vals (pred, true): 0.205, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007074255

Epoch over!
epoch time: 13.274

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.329 +- 0.213
mrr vals (pred, true): 0.156, 0.003
batch losses (mrrl, rdl): 0.0, 0.0003180785

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.295 +- 0.235
mrr vals (pred, true): 0.275, 0.003
batch losses (mrrl, rdl): 0.0, 0.0005209928

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 457
rank avg (pred): 0.290 +- 0.248
mrr vals (pred, true): 0.287, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003886834

Epoch over!
epoch time: 13.338

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1048
rank avg (pred): 0.239 +- 0.241
mrr vals (pred, true): 0.334, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007326569

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.268 +- 0.259
mrr vals (pred, true): 0.328, 0.205
batch losses (mrrl, rdl): 0.0, 0.0008174174

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 538
rank avg (pred): 0.168 +- 0.171
mrr vals (pred, true): 0.357, 0.019
batch losses (mrrl, rdl): 0.0, 1.49064e-05

Epoch over!
epoch time: 13.764

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.047 +- 0.054
mrr vals (pred, true): 0.433, 0.243
batch losses (mrrl, rdl): 0.0, 1.0567e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.246 +- 0.272
mrr vals (pred, true): 0.409, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006223701

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 379
rank avg (pred): 0.252 +- 0.273
mrr vals (pred, true): 0.376, 0.152
batch losses (mrrl, rdl): 0.0, 0.0003590891

Epoch over!
epoch time: 13.107

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1066
rank avg (pred): 0.069 +- 0.087
mrr vals (pred, true): 0.494, 0.241
batch losses (mrrl, rdl): 0.0, 1.30114e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 6
rank avg (pred): 0.047 +- 0.060
mrr vals (pred, true): 0.500, 0.135
batch losses (mrrl, rdl): 0.0, 8.4138e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.251 +- 0.283
mrr vals (pred, true): 0.437, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005452586

Epoch over!
epoch time: 12.715

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 564
rank avg (pred): 0.183 +- 0.200
mrr vals (pred, true): 0.420, 0.018
batch losses (mrrl, rdl): 1.368929863, 3.883e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.462 +- 0.227
mrr vals (pred, true): 0.060, 0.022
batch losses (mrrl, rdl): 0.0010354482, 0.0006601612

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 766
rank avg (pred): 0.214 +- 0.187
mrr vals (pred, true): 0.117, 0.166
batch losses (mrrl, rdl): 0.0241948087, 0.0003163036

Epoch over!
epoch time: 12.894

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 567
rank avg (pred): 0.503 +- 0.211
mrr vals (pred, true): 0.045, 0.008
batch losses (mrrl, rdl): 0.0002390881, 0.0005514401

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 642
rank avg (pred): 0.510 +- 0.200
mrr vals (pred, true): 0.039, 0.006
batch losses (mrrl, rdl): 0.0012922047, 0.0007105349

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1003
rank avg (pred): 0.231 +- 0.187
mrr vals (pred, true): 0.125, 0.208
batch losses (mrrl, rdl): 0.0683772266, 0.0004869461

Epoch over!
epoch time: 13.311

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 166
rank avg (pred): 0.338 +- 0.209
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0133139752, 0.000204358

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1019
rank avg (pred): 0.209 +- 0.164
mrr vals (pred, true): 0.115, 0.175
batch losses (mrrl, rdl): 0.0361583196, 0.0002945907

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.162 +- 0.129
mrr vals (pred, true): 0.125, 0.204
batch losses (mrrl, rdl): 0.0624438561, 0.0001273373

Epoch over!
epoch time: 13.444

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1102
rank avg (pred): 0.216 +- 0.159
mrr vals (pred, true): 0.111, 0.200
batch losses (mrrl, rdl): 0.0794418752, 0.000352424

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.372 +- 0.214
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.0168616734, 0.0001634349

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.414 +- 0.210
mrr vals (pred, true): 0.063, 0.027
batch losses (mrrl, rdl): 0.0016252687, 0.0014360618

Epoch over!
epoch time: 13.397

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 958
rank avg (pred): 0.358 +- 0.205
mrr vals (pred, true): 0.083, 0.005
batch losses (mrrl, rdl): 0.0111252479, 0.0001567632

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.430 +- 0.206
mrr vals (pred, true): 0.059, 0.025
batch losses (mrrl, rdl): 0.0008777078, 0.0014037509

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 76
rank avg (pred): 0.049 +- 0.036
mrr vals (pred, true): 0.162, 0.203
batch losses (mrrl, rdl): 0.017365735, 1.9031e-06

Epoch over!
epoch time: 13.122

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1061
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.262, 0.262
batch losses (mrrl, rdl): 2e-10, 2.07877e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1120
rank avg (pred): 0.168 +- 0.123
mrr vals (pred, true): 0.130, 0.004
batch losses (mrrl, rdl): 0.0638569593, 0.0017507492

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.294 +- 0.191
mrr vals (pred, true): 0.110, 0.005
batch losses (mrrl, rdl): 0.0362086445, 0.0004095445

Epoch over!
epoch time: 12.561

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1194
rank avg (pred): 0.421 +- 0.185
mrr vals (pred, true): 0.060, 0.003
batch losses (mrrl, rdl): 0.0010599861, 5.90804e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.416 +- 0.180
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.000269228, 6.82771e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.278 +- 0.171
mrr vals (pred, true): 0.101, 0.004
batch losses (mrrl, rdl): 0.0264540948, 0.0005324116

Epoch over!
epoch time: 13.316

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.033 +- 0.023
mrr vals (pred, true): 0.164, 0.190
batch losses (mrrl, rdl): 0.0065320423, 4.5979e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.046 +- 0.033
mrr vals (pred, true): 0.160, 0.179
batch losses (mrrl, rdl): 0.0036682193, 4.4379e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 974
rank avg (pred): 0.018 +- 0.013
mrr vals (pred, true): 0.216, 0.201
batch losses (mrrl, rdl): 0.0021709076, 1.8011e-05

Epoch over!
epoch time: 13.205

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.147 +- 0.099
mrr vals (pred, true): 0.134, 0.153
batch losses (mrrl, rdl): 0.0035302066, 0.0001487274

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.393 +- 0.163
mrr vals (pred, true): 0.060, 0.025
batch losses (mrrl, rdl): 0.0009596332, 0.0010163983

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.354 +- 0.184
mrr vals (pred, true): 0.081, 0.004
batch losses (mrrl, rdl): 0.0093729086, 0.0002139745

Epoch over!
epoch time: 13.218

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 91
rank avg (pred): 0.357 +- 0.179
mrr vals (pred, true): 0.078, 0.140
batch losses (mrrl, rdl): 0.0377328061, 0.0013965748

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.389 +- 0.150
mrr vals (pred, true): 0.054, 0.015
batch losses (mrrl, rdl): 0.0001665924, 0.0005472717

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.311 +- 0.184
mrr vals (pred, true): 0.107, 0.004
batch losses (mrrl, rdl): 0.0323508084, 0.0004177513

Epoch over!
epoch time: 13.183

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.389 +- 0.148
mrr vals (pred, true): 0.050, 0.009

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   32 	     0 	 0.08865 	 0.00317 	 m..s
   14 	     1 	 0.04801 	 0.00334 	 m..s
   19 	     2 	 0.04944 	 0.00336 	 m..s
   44 	     3 	 0.09362 	 0.00353 	 m..s
   39 	     4 	 0.09305 	 0.00359 	 m..s
    2 	     5 	 0.04330 	 0.00360 	 m..s
    6 	     6 	 0.04414 	 0.00364 	 m..s
   18 	     7 	 0.04924 	 0.00366 	 m..s
   58 	     8 	 0.10141 	 0.00370 	 m..s
   12 	     9 	 0.04626 	 0.00373 	 m..s
   58 	    10 	 0.10141 	 0.00377 	 m..s
   94 	    11 	 0.11707 	 0.00378 	 MISS
   16 	    12 	 0.04827 	 0.00381 	 m..s
   48 	    13 	 0.09683 	 0.00384 	 m..s
   36 	    14 	 0.09205 	 0.00387 	 m..s
    4 	    15 	 0.04391 	 0.00388 	 m..s
   31 	    16 	 0.08556 	 0.00390 	 m..s
   93 	    17 	 0.11679 	 0.00392 	 MISS
   45 	    18 	 0.09378 	 0.00393 	 m..s
   74 	    19 	 0.10286 	 0.00397 	 m..s
   58 	    20 	 0.10141 	 0.00408 	 m..s
    9 	    21 	 0.04548 	 0.00409 	 m..s
   17 	    22 	 0.04833 	 0.00411 	 m..s
   55 	    23 	 0.10059 	 0.00412 	 m..s
   91 	    24 	 0.11474 	 0.00412 	 MISS
   34 	    25 	 0.09041 	 0.00414 	 m..s
   58 	    26 	 0.10141 	 0.00419 	 m..s
   83 	    27 	 0.10994 	 0.00422 	 MISS
    7 	    28 	 0.04538 	 0.00424 	 m..s
   72 	    29 	 0.10198 	 0.00437 	 m..s
   40 	    30 	 0.09314 	 0.00442 	 m..s
   58 	    31 	 0.10141 	 0.00442 	 m..s
   37 	    32 	 0.09298 	 0.00446 	 m..s
   58 	    33 	 0.10141 	 0.00450 	 m..s
   52 	    34 	 0.09735 	 0.00451 	 m..s
   38 	    35 	 0.09303 	 0.00459 	 m..s
   58 	    36 	 0.10141 	 0.00463 	 m..s
   89 	    37 	 0.11272 	 0.00468 	 MISS
   58 	    38 	 0.10141 	 0.00472 	 m..s
   47 	    39 	 0.09609 	 0.00498 	 m..s
   85 	    40 	 0.11183 	 0.00528 	 MISS
    3 	    41 	 0.04366 	 0.00540 	 m..s
   58 	    42 	 0.10141 	 0.00551 	 m..s
   23 	    43 	 0.05007 	 0.00876 	 m..s
    8 	    44 	 0.04539 	 0.00898 	 m..s
   13 	    45 	 0.04780 	 0.00916 	 m..s
    1 	    46 	 0.04275 	 0.00921 	 m..s
    0 	    47 	 0.04260 	 0.00943 	 m..s
   22 	    48 	 0.04989 	 0.00999 	 m..s
   11 	    49 	 0.04551 	 0.01044 	 m..s
    5 	    50 	 0.04395 	 0.01188 	 m..s
   10 	    51 	 0.04550 	 0.01274 	 m..s
   21 	    52 	 0.04963 	 0.01807 	 m..s
   20 	    53 	 0.04958 	 0.02133 	 ~...
   15 	    54 	 0.04804 	 0.02269 	 ~...
   26 	    55 	 0.05872 	 0.02424 	 m..s
   27 	    56 	 0.05875 	 0.02452 	 m..s
   29 	    57 	 0.08199 	 0.02530 	 m..s
   25 	    58 	 0.05687 	 0.02655 	 m..s
   24 	    59 	 0.05474 	 0.04148 	 ~...
   99 	    60 	 0.13424 	 0.05106 	 m..s
   28 	    61 	 0.07941 	 0.09733 	 ~...
   96 	    62 	 0.11937 	 0.12113 	 ~...
   46 	    63 	 0.09406 	 0.12756 	 m..s
   33 	    64 	 0.08982 	 0.13809 	 m..s
   41 	    65 	 0.09352 	 0.14112 	 m..s
   49 	    66 	 0.09702 	 0.14249 	 m..s
   42 	    67 	 0.09353 	 0.14271 	 m..s
  102 	    68 	 0.15077 	 0.14500 	 ~...
   51 	    69 	 0.09721 	 0.14642 	 m..s
   97 	    70 	 0.12363 	 0.14747 	 ~...
   54 	    71 	 0.10032 	 0.14854 	 m..s
   43 	    72 	 0.09354 	 0.14934 	 m..s
   50 	    73 	 0.09702 	 0.14963 	 m..s
  103 	    74 	 0.15144 	 0.15089 	 ~...
   53 	    75 	 0.09745 	 0.15094 	 m..s
  101 	    76 	 0.14818 	 0.15206 	 ~...
   58 	    77 	 0.10141 	 0.15261 	 m..s
   58 	    78 	 0.10141 	 0.15438 	 m..s
   58 	    79 	 0.10141 	 0.15816 	 m..s
  100 	    80 	 0.14704 	 0.15845 	 ~...
   58 	    81 	 0.10141 	 0.16538 	 m..s
   35 	    82 	 0.09061 	 0.16565 	 m..s
   30 	    83 	 0.08422 	 0.16607 	 m..s
   84 	    84 	 0.11054 	 0.16650 	 m..s
   73 	    85 	 0.10254 	 0.16854 	 m..s
   57 	    86 	 0.10089 	 0.17104 	 m..s
   56 	    87 	 0.10080 	 0.17169 	 m..s
  105 	    88 	 0.16085 	 0.17282 	 ~...
   81 	    89 	 0.10874 	 0.17295 	 m..s
   75 	    90 	 0.10352 	 0.17355 	 m..s
   98 	    91 	 0.13250 	 0.17536 	 m..s
   78 	    92 	 0.10616 	 0.17628 	 m..s
   80 	    93 	 0.10794 	 0.17681 	 m..s
   76 	    94 	 0.10425 	 0.17788 	 m..s
   77 	    95 	 0.10464 	 0.17845 	 m..s
   82 	    96 	 0.10885 	 0.17983 	 m..s
  104 	    97 	 0.15927 	 0.18173 	 ~...
   58 	    98 	 0.10141 	 0.18195 	 m..s
   88 	    99 	 0.11255 	 0.18278 	 m..s
   86 	   100 	 0.11195 	 0.18507 	 m..s
  110 	   101 	 0.16853 	 0.18788 	 ~...
  112 	   102 	 0.18096 	 0.18871 	 ~...
  114 	   103 	 0.18098 	 0.19110 	 ~...
   95 	   104 	 0.11876 	 0.19210 	 m..s
  115 	   105 	 0.18124 	 0.19682 	 ~...
  116 	   106 	 0.18186 	 0.20160 	 ~...
   79 	   107 	 0.10674 	 0.20379 	 m..s
  107 	   108 	 0.16365 	 0.20703 	 m..s
  118 	   109 	 0.18461 	 0.20774 	 ~...
  119 	   110 	 0.18553 	 0.20816 	 ~...
   90 	   111 	 0.11466 	 0.21079 	 m..s
   87 	   112 	 0.11222 	 0.21109 	 m..s
   92 	   113 	 0.11558 	 0.21182 	 m..s
  106 	   114 	 0.16200 	 0.21490 	 m..s
  120 	   115 	 0.18862 	 0.23008 	 m..s
  111 	   116 	 0.17075 	 0.23043 	 m..s
  109 	   117 	 0.16720 	 0.23807 	 m..s
  108 	   118 	 0.16432 	 0.24214 	 m..s
  113 	   119 	 0.18097 	 0.25626 	 m..s
  117 	   120 	 0.18208 	 0.26223 	 m..s
==========================================
r_mrr = 0.7090023756027222
r2_mrr = 0.4252926707267761
spearmanr_mrr@5 = 0.9418042898178101
spearmanr_mrr@10 = 0.7742561101913452
spearmanr_mrr@50 = 0.9429675340652466
spearmanr_mrr@100 = 0.8113577961921692
spearmanr_mrr@All = 0.8483352065086365
==========================================
test time: 0.398
Done Testing dataset CoDExSmall
total time taken: 208.91503810882568
training time taken: 198.31955862045288
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7090)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4253)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9418)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.7743)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9430)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8114)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8483)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.9479292276591877}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7939117544682113
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [217, 855, 233, 256, 765, 447, 1037, 275, 276, 620, 640, 675, 1096, 401, 691, 722, 754, 1141, 1051, 880, 772, 145, 93, 1136, 290, 658, 917, 630, 1110, 146, 1039, 934, 689, 1126, 367, 926, 969, 1069, 85, 415, 462, 1142, 122, 719, 512, 711, 965, 324, 104, 112, 237, 195, 663, 509, 731, 767, 1212, 380, 713, 966, 467, 779, 537, 431, 971, 545, 204, 437, 720, 438, 956, 466, 983, 316, 1165, 29, 590, 1073, 894, 960, 959, 309, 662, 297, 653, 1124, 73, 441, 541, 449, 247, 1055, 1095, 191, 807, 674, 1034, 633, 834, 34, 420, 1076, 234, 206, 1214, 1199, 756, 829, 845, 1100, 232, 16, 686, 1001, 858, 886, 814, 626, 933, 357, 425]
valid_ids (0): []
train_ids (1094): [870, 957, 332, 1213, 666, 1115, 141, 750, 546, 391, 874, 615, 113, 348, 681, 169, 847, 744, 660, 723, 190, 1133, 248, 273, 1102, 1166, 948, 253, 1081, 1181, 444, 579, 820, 548, 595, 1083, 883, 343, 592, 538, 598, 669, 216, 1056, 943, 564, 827, 972, 696, 843, 386, 114, 840, 241, 508, 264, 850, 761, 240, 892, 821, 193, 946, 360, 314, 902, 993, 359, 143, 27, 24, 982, 1109, 514, 6, 504, 296, 735, 510, 1200, 1030, 1091, 947, 135, 147, 1107, 261, 398, 171, 1029, 23, 1047, 1175, 453, 1108, 745, 1120, 333, 460, 501, 1103, 35, 841, 542, 924, 1122, 547, 805, 131, 138, 250, 12, 102, 995, 334, 823, 494, 417, 846, 258, 1007, 1009, 1008, 422, 281, 1044, 1066, 793, 970, 656, 382, 931, 288, 629, 1, 279, 992, 238, 635, 646, 739, 1159, 305, 1211, 543, 100, 887, 601, 1035, 428, 589, 743, 519, 1012, 223, 1157, 1087, 411, 885, 303, 539, 586, 1036, 600, 1094, 351, 710, 154, 574, 647, 746, 553, 347, 918, 106, 219, 70, 738, 904, 407, 158, 403, 523, 285, 1179, 624, 955, 607, 1038, 860, 618, 1135, 1052, 912, 962, 440, 783, 818, 188, 413, 1152, 869, 1114, 1154, 94, 628, 19, 163, 676, 1129, 617, 211, 134, 162, 1049, 474, 976, 636, 725, 139, 0, 242, 377, 341, 1143, 721, 176, 389, 803, 121, 550, 436, 1150, 571, 133, 412, 639, 62, 175, 654, 697, 1188, 1067, 84, 220, 1192, 529, 762, 96, 310, 130, 1137, 554, 490, 1176, 1074, 796, 520, 1170, 435, 246, 637, 954, 363, 419, 185, 205, 737, 632, 372, 1182, 717, 1060, 930, 361, 985, 832, 263, 809, 1146, 1050, 464, 940, 362, 1209, 1167, 459, 115, 1169, 945, 173, 74, 319, 67, 648, 395, 790, 1015, 344, 1162, 891, 485, 254, 1097, 489, 1042, 667, 393, 371, 914, 14, 493, 906, 758, 222, 312, 784, 25, 1032, 741, 838, 328, 424, 968, 798, 1197, 578, 998, 495, 157, 612, 879, 320, 819, 480, 1006, 218, 552, 919, 699, 517, 476, 161, 1040, 944, 751, 602, 534, 788, 975, 1145, 349, 680, 565, 8, 203, 32, 1180, 1138, 340, 597, 842, 142, 17, 208, 409, 764, 549, 535, 69, 257, 272, 352, 268, 473, 786, 716, 5, 198, 625, 777, 921, 148, 1084, 861, 339, 346, 862, 483, 81, 949, 265, 186, 399, 575, 318, 189, 117, 868, 621, 937, 698, 321, 1045, 43, 997, 544, 336, 1104, 749, 503, 1207, 307, 655, 661, 325, 563, 867, 83, 68, 785, 373, 151, 323, 350, 540, 82, 243, 1206, 1003, 518, 48, 77, 354, 292, 181, 406, 1088, 651, 472, 236, 950, 1183, 752, 524, 172, 329, 127, 729, 753, 266, 837, 463, 470, 1160, 613, 1031, 38, 769, 967, 802, 72, 889, 442, 980, 614, 53, 446, 1054, 913, 604, 155, 649, 1119, 1057, 212, 1064, 863, 1189, 616, 136, 402, 728, 747, 706, 566, 327, 262, 877, 202, 168, 384, 1061, 366, 1093, 907, 991, 1195, 668, 1128, 488, 815, 1011, 734, 1023, 817, 408, 455, 694, 313, 915, 928, 1098, 252, 922, 562, 194, 300, 810, 789, 561, 294, 47, 1024, 787, 140, 811, 491, 1111, 559, 671, 376, 828, 245, 797, 650, 499, 461, 677, 866, 782, 199, 851, 123, 457, 1078, 610, 1046, 576, 989, 98, 707, 1193, 603, 228, 469, 652, 804, 505, 197, 718, 1149, 1071, 445, 683, 63, 1210, 153, 1041, 701, 1184, 1168, 996, 942, 1065, 911, 414, 558, 430, 80, 315, 700, 1112, 129, 831, 726, 1090, 201, 1043, 326, 283, 452, 355, 702, 584, 99, 835, 964, 583, 778, 308, 1163, 51, 688, 196, 1132, 854, 1144, 118, 670, 330, 800, 1021, 78, 56, 594, 267, 511, 378, 383, 174, 665, 591, 977, 1186, 277, 59, 1058, 105, 156, 311, 875, 1020, 42, 760, 410, 54, 251, 231, 712, 1164, 306, 1191, 1068, 335, 580, 1082, 528, 76, 732, 21, 293, 882, 773, 836, 641, 183, 214, 434, 97, 484, 95, 150, 200, 896, 923, 1022, 65, 57, 577, 905, 45, 433, 878, 496, 890, 953, 759, 31, 60, 556, 768, 79, 342, 92, 1201, 659, 605, 90, 1148, 481, 259, 1118, 269, 400, 299, 37, 727, 780, 421, 672, 572, 1053, 755, 1086, 46, 30, 1158, 492, 551, 274, 184, 774, 187, 555, 1077, 864, 15, 26, 36, 249, 479, 515, 375, 111, 695, 390, 908, 179, 581, 984, 766, 587, 1178, 465, 392, 872, 536, 1204, 533, 833, 66, 678, 244, 405, 448, 317, 3, 478, 994, 730, 939, 852, 456, 1025, 1113, 839, 423, 1147, 159, 132, 903, 282, 1048, 925, 13, 1177, 693, 963, 39, 137, 987, 52, 812, 374, 568, 429, 1187, 387, 1208, 298, 644, 951, 859, 596, 608, 876, 881, 144, 213, 426, 830, 981, 952, 1026, 11, 623, 813, 160, 1134, 9, 271, 638, 1194, 560, 385, 125, 475, 1017, 128, 1027, 192, 941, 973, 439, 1174, 215, 149, 531, 1010, 657, 1079, 126, 525, 826, 1196, 55, 497, 388, 301, 1072, 418, 1205, 679, 381, 1028, 416, 103, 61, 532, 748, 255, 848, 853, 687, 709, 642, 1203, 808, 20, 1014, 107, 432, 41, 740, 775, 229, 1019, 1127, 365, 856, 582, 781, 1105, 44, 498, 1059, 235, 631, 89, 379, 715, 221, 888, 1002, 569, 284, 487, 353, 799, 209, 794, 801, 978, 295, 606, 593, 230, 108, 337, 116, 207, 356, 120, 506, 1153, 101, 645, 958, 736, 1130, 500, 1013, 705, 88, 1116, 109, 527, 18, 1106, 124, 521, 322, 338, 304, 1016, 451, 87, 792, 450, 1198, 643, 49, 684, 516, 7, 901, 166, 673, 152, 1092, 824, 1202, 110, 929, 28, 369, 961, 742, 1151, 884, 302, 526, 1117, 227, 522, 585, 893, 471, 622, 331, 898, 1173, 427, 692, 895, 1121, 260, 397, 619, 557, 865, 1101, 170, 86, 776, 58, 177, 627, 1123, 224, 708, 4, 979, 770, 816, 1070, 900, 71, 443, 10, 871, 988, 482, 910, 849, 1155, 920, 690, 1063, 507, 573, 370, 567, 795, 822, 724, 1080, 477, 986, 599, 2, 935, 873, 1161, 1139, 1075, 33, 178, 345, 486, 1171, 167, 1085, 791, 1033, 180, 404, 1140, 91, 1125, 1190, 936, 75, 806, 825, 226, 771, 502, 733, 844, 165, 897, 899, 703, 1089, 368, 364, 999, 685, 990, 280, 182, 291, 932, 1018, 396, 1004, 164, 974, 916, 1131, 664, 704, 1172, 609, 1005, 530, 289, 278, 714, 458, 927, 468, 287, 394, 938, 22, 611, 1156, 1062, 358, 857, 513, 634, 1000, 239, 909, 588, 454, 570, 64, 270, 50, 286, 1185, 763, 757, 210, 119, 225, 1099, 682, 40]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1561269977600664
the save name prefix for this run is:  chkpt-ID_1561269977600664_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 124
rank avg (pred): 0.426 +- 0.010
mrr vals (pred, true): 0.001, 0.139
batch losses (mrrl, rdl): 0.0, 0.0021223007

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.260 +- 0.168
mrr vals (pred, true): 0.159, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007445986

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 824
rank avg (pred): 0.072 +- 0.061
mrr vals (pred, true): 0.331, 0.142
batch losses (mrrl, rdl): 0.0, 2.4504e-06

Epoch over!
epoch time: 12.955

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.253 +- 0.213
mrr vals (pred, true): 0.313, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006633404

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.367 +- 0.315
mrr vals (pred, true): 0.338, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001315867

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 890
rank avg (pred): 0.250 +- 0.228
mrr vals (pred, true): 0.379, 0.005
batch losses (mrrl, rdl): 0.0, 0.00066377

Epoch over!
epoch time: 13.185

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.297 +- 0.257
mrr vals (pred, true): 0.352, 0.173
batch losses (mrrl, rdl): 0.0, 0.0010480108

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 7
rank avg (pred): 0.065 +- 0.062
mrr vals (pred, true): 0.436, 0.161
batch losses (mrrl, rdl): 0.0, 4.8465e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 68
rank avg (pred): 0.031 +- 0.029
mrr vals (pred, true): 0.438, 0.230
batch losses (mrrl, rdl): 0.0, 3.788e-06

Epoch over!
epoch time: 12.253

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 349
rank avg (pred): 0.252 +- 0.240
mrr vals (pred, true): 0.407, 0.166
batch losses (mrrl, rdl): 0.0, 0.0005514238

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 246
rank avg (pred): 0.088 +- 0.087
mrr vals (pred, true): 0.444, 0.153
batch losses (mrrl, rdl): 0.0, 7.3548e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 490
rank avg (pred): 0.205 +- 0.194
mrr vals (pred, true): 0.414, 0.016
batch losses (mrrl, rdl): 0.0, 6.8208e-06

Epoch over!
epoch time: 14.413

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1197
rank avg (pred): 0.323 +- 0.298
mrr vals (pred, true): 0.395, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002050117

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 907
rank avg (pred): 0.064 +- 0.063
mrr vals (pred, true): 0.443, 0.073
batch losses (mrrl, rdl): 0.0, 3.0139e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 838
rank avg (pred): 0.240 +- 0.243
mrr vals (pred, true): 0.442, 0.162
batch losses (mrrl, rdl): 0.0, 0.0005516434

Epoch over!
epoch time: 13.27

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.053 +- 0.051
mrr vals (pred, true): 0.439, 0.111
batch losses (mrrl, rdl): 1.0717198849, 3.0953e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.188 +- 0.112
mrr vals (pred, true): 0.095, 0.004
batch losses (mrrl, rdl): 0.0203291532, 0.0014232221

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 472
rank avg (pred): 0.256 +- 0.150
mrr vals (pred, true): 0.092, 0.004
batch losses (mrrl, rdl): 0.0176608581, 0.0008854578

Epoch over!
epoch time: 14.348

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1032
rank avg (pred): 0.133 +- 0.081
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0442235395, 0.0023223753

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 487
rank avg (pred): 0.320 +- 0.121
mrr vals (pred, true): 0.058, 0.019
batch losses (mrrl, rdl): 0.0005876326, 0.0003071658

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 522
rank avg (pred): 0.329 +- 0.120
mrr vals (pred, true): 0.052, 0.017
batch losses (mrrl, rdl): 6.17339e-05, 0.000293495

Epoch over!
epoch time: 13.357

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 854
rank avg (pred): 0.264 +- 0.156
mrr vals (pred, true): 0.109, 0.171
batch losses (mrrl, rdl): 0.0390408225, 0.0006822606

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 203
rank avg (pred): 0.247 +- 0.147
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0442861915, 0.0008729299

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 833
rank avg (pred): 0.070 +- 0.042
mrr vals (pred, true): 0.134, 0.152
batch losses (mrrl, rdl): 0.0031610425, 3.3996e-06

Epoch over!
epoch time: 13.364

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 239
rank avg (pred): 0.251 +- 0.153
mrr vals (pred, true): 0.125, 0.004
batch losses (mrrl, rdl): 0.05603626, 0.0008558492

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 13
rank avg (pred): 0.027 +- 0.017
mrr vals (pred, true): 0.178, 0.201
batch losses (mrrl, rdl): 0.0051815137, 1.02033e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 236
rank avg (pred): 0.291 +- 0.168
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0343834385, 0.0005301687

Epoch over!
epoch time: 13.241

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 188
rank avg (pred): 0.326 +- 0.190
mrr vals (pred, true): 0.107, 0.005
batch losses (mrrl, rdl): 0.031939052, 0.0003039318

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 612
rank avg (pred): 0.431 +- 0.175
mrr vals (pred, true): 0.046, 0.007
batch losses (mrrl, rdl): 0.0001352276, 0.0002763629

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.187 +- 0.067
mrr vals (pred, true): 0.058, 0.020
batch losses (mrrl, rdl): 0.000698798, 4.4365e-05

Epoch over!
epoch time: 12.521

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 353
rank avg (pred): 0.184 +- 0.113
mrr vals (pred, true): 0.125, 0.170
batch losses (mrrl, rdl): 0.0198448747, 7.89568e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1173
rank avg (pred): 0.349 +- 0.136
mrr vals (pred, true): 0.057, 0.010
batch losses (mrrl, rdl): 0.0004298197, 6.06676e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 785
rank avg (pred): 0.280 +- 0.160
mrr vals (pred, true): 0.103, 0.005
batch losses (mrrl, rdl): 0.0283423066, 0.0005160007

Epoch over!
epoch time: 13.655

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 577
rank avg (pred): 0.432 +- 0.195
mrr vals (pred, true): 0.051, 0.011
batch losses (mrrl, rdl): 1.67651e-05, 0.0002850572

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1205
rank avg (pred): 0.387 +- 0.155
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0004855322, 0.0001751947

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 728
rank avg (pred): 0.411 +- 0.182
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0008720727, 0.000103203

Epoch over!
epoch time: 13.366

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 365
rank avg (pred): 0.243 +- 0.144
mrr vals (pred, true): 0.110, 0.185
batch losses (mrrl, rdl): 0.0555966459, 0.0003989519

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 875
rank avg (pred): 0.280 +- 0.151
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.0168931596, 0.0007229952

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 220
rank avg (pred): 0.263 +- 0.146
mrr vals (pred, true): 0.104, 0.004
batch losses (mrrl, rdl): 0.0286903996, 0.000706015

Epoch over!
epoch time: 12.811

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 427
rank avg (pred): 0.223 +- 0.136
mrr vals (pred, true): 0.119, 0.005
batch losses (mrrl, rdl): 0.0474832766, 0.0011342325

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1173
rank avg (pred): 0.387 +- 0.167
mrr vals (pred, true): 0.051, 0.010
batch losses (mrrl, rdl): 1.55907e-05, 0.0001389739

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.409 +- 0.181
mrr vals (pred, true): 0.051, 0.004
batch losses (mrrl, rdl): 3.6315e-06, 9.26192e-05

Epoch over!
epoch time: 13.226

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 739
rank avg (pred): 0.256 +- 0.158
mrr vals (pred, true): 0.132, 0.126
batch losses (mrrl, rdl): 0.0004306466, 0.0007948716

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 511
rank avg (pred): 0.178 +- 0.058
mrr vals (pred, true): 0.052, 0.016
batch losses (mrrl, rdl): 5.99228e-05, 5.87205e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 536
rank avg (pred): 0.196 +- 0.088
mrr vals (pred, true): 0.065, 0.035
batch losses (mrrl, rdl): 0.0022023742, 5.85112e-05

Epoch over!
epoch time: 12.971

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.268 +- 0.134
mrr vals (pred, true): 0.094, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.05075 	 0.00327 	 m..s
   22 	     1 	 0.05328 	 0.00330 	 m..s
    4 	     2 	 0.04788 	 0.00339 	 m..s
   17 	     3 	 0.05163 	 0.00348 	 m..s
   37 	     4 	 0.09044 	 0.00349 	 m..s
   25 	     5 	 0.05564 	 0.00352 	 m..s
   48 	     6 	 0.09857 	 0.00352 	 m..s
   77 	     7 	 0.10444 	 0.00357 	 MISS
   13 	     8 	 0.05033 	 0.00357 	 m..s
   84 	     9 	 0.11017 	 0.00360 	 MISS
   33 	    10 	 0.08839 	 0.00362 	 m..s
    0 	    11 	 0.04550 	 0.00363 	 m..s
   16 	    12 	 0.05160 	 0.00365 	 m..s
    3 	    13 	 0.04775 	 0.00369 	 m..s
   34 	    14 	 0.08875 	 0.00370 	 m..s
    7 	    15 	 0.04808 	 0.00371 	 m..s
   70 	    16 	 0.09865 	 0.00376 	 m..s
   88 	    17 	 0.11897 	 0.00378 	 MISS
   48 	    18 	 0.09857 	 0.00380 	 m..s
   40 	    19 	 0.09419 	 0.00382 	 m..s
   23 	    20 	 0.05334 	 0.00383 	 m..s
   48 	    21 	 0.09857 	 0.00384 	 m..s
   19 	    22 	 0.05166 	 0.00384 	 m..s
   48 	    23 	 0.09857 	 0.00387 	 m..s
   92 	    24 	 0.12392 	 0.00389 	 MISS
    9 	    25 	 0.04833 	 0.00390 	 m..s
   83 	    26 	 0.11014 	 0.00392 	 MISS
   11 	    27 	 0.04957 	 0.00392 	 m..s
   72 	    28 	 0.10139 	 0.00394 	 m..s
   48 	    29 	 0.09857 	 0.00397 	 m..s
   10 	    30 	 0.04913 	 0.00398 	 m..s
   48 	    31 	 0.09857 	 0.00400 	 m..s
   76 	    32 	 0.10439 	 0.00401 	 MISS
   43 	    33 	 0.09832 	 0.00403 	 m..s
   42 	    34 	 0.09466 	 0.00406 	 m..s
   85 	    35 	 0.11556 	 0.00410 	 MISS
   82 	    36 	 0.10973 	 0.00413 	 MISS
   46 	    37 	 0.09851 	 0.00415 	 m..s
   78 	    38 	 0.10700 	 0.00420 	 MISS
   48 	    39 	 0.09857 	 0.00420 	 m..s
   90 	    40 	 0.12133 	 0.00426 	 MISS
   48 	    41 	 0.09857 	 0.00427 	 m..s
   86 	    42 	 0.11710 	 0.00433 	 MISS
   91 	    43 	 0.12152 	 0.00437 	 MISS
   35 	    44 	 0.08922 	 0.00438 	 m..s
   48 	    45 	 0.09857 	 0.00442 	 m..s
  101 	    46 	 0.14314 	 0.00442 	 MISS
   73 	    47 	 0.10191 	 0.00450 	 m..s
    1 	    48 	 0.04631 	 0.00455 	 m..s
   18 	    49 	 0.05165 	 0.00455 	 m..s
   48 	    50 	 0.09857 	 0.00456 	 m..s
   44 	    51 	 0.09847 	 0.00464 	 m..s
   48 	    52 	 0.09857 	 0.00488 	 m..s
   99 	    53 	 0.13972 	 0.00493 	 MISS
  100 	    54 	 0.14020 	 0.00563 	 MISS
    5 	    55 	 0.04789 	 0.00672 	 m..s
    6 	    56 	 0.04791 	 0.00741 	 m..s
   24 	    57 	 0.05407 	 0.00802 	 m..s
    2 	    58 	 0.04691 	 0.01073 	 m..s
   14 	    59 	 0.05068 	 0.01552 	 m..s
    8 	    60 	 0.04812 	 0.02011 	 ~...
   31 	    61 	 0.06460 	 0.02127 	 m..s
   29 	    62 	 0.06361 	 0.02199 	 m..s
   12 	    63 	 0.04969 	 0.02328 	 ~...
   30 	    64 	 0.06406 	 0.02333 	 m..s
   21 	    65 	 0.05291 	 0.02408 	 ~...
   20 	    66 	 0.05208 	 0.02592 	 ~...
   28 	    67 	 0.05695 	 0.02613 	 m..s
   26 	    68 	 0.05586 	 0.02739 	 ~...
   41 	    69 	 0.09448 	 0.02867 	 m..s
   27 	    70 	 0.05662 	 0.03512 	 ~...
   48 	    71 	 0.09857 	 0.03959 	 m..s
   48 	    72 	 0.09857 	 0.04802 	 m..s
   32 	    73 	 0.08564 	 0.10533 	 ~...
   48 	    74 	 0.09857 	 0.11623 	 ~...
   48 	    75 	 0.09857 	 0.11872 	 ~...
   95 	    76 	 0.12858 	 0.12102 	 ~...
   39 	    77 	 0.09418 	 0.12846 	 m..s
   36 	    78 	 0.09036 	 0.13204 	 m..s
   96 	    79 	 0.12954 	 0.13475 	 ~...
   38 	    80 	 0.09386 	 0.14083 	 m..s
  104 	    81 	 0.15885 	 0.15051 	 ~...
  105 	    82 	 0.16656 	 0.15109 	 ~...
   48 	    83 	 0.09857 	 0.15121 	 m..s
   48 	    84 	 0.09857 	 0.15261 	 m..s
   94 	    85 	 0.12835 	 0.15561 	 ~...
   48 	    86 	 0.09857 	 0.15694 	 m..s
   48 	    87 	 0.09857 	 0.15721 	 m..s
   71 	    88 	 0.10011 	 0.16139 	 m..s
   48 	    89 	 0.09857 	 0.16538 	 m..s
   48 	    90 	 0.09857 	 0.16581 	 m..s
  107 	    91 	 0.16802 	 0.16954 	 ~...
   48 	    92 	 0.09857 	 0.17176 	 m..s
   75 	    93 	 0.10429 	 0.17260 	 m..s
   80 	    94 	 0.10722 	 0.17389 	 m..s
   81 	    95 	 0.10804 	 0.17550 	 m..s
  110 	    96 	 0.17573 	 0.17605 	 ~...
   47 	    97 	 0.09855 	 0.17614 	 m..s
   79 	    98 	 0.10719 	 0.17681 	 m..s
   45 	    99 	 0.09850 	 0.17979 	 m..s
   89 	   100 	 0.12103 	 0.17989 	 m..s
   87 	   101 	 0.11852 	 0.18549 	 m..s
  116 	   102 	 0.22030 	 0.18686 	 m..s
  103 	   103 	 0.15611 	 0.18784 	 m..s
  115 	   104 	 0.21615 	 0.18868 	 ~...
   74 	   105 	 0.10211 	 0.18871 	 m..s
   93 	   106 	 0.12409 	 0.18871 	 m..s
  102 	   107 	 0.15589 	 0.19182 	 m..s
  113 	   108 	 0.19887 	 0.19682 	 ~...
  108 	   109 	 0.17126 	 0.19914 	 ~...
  106 	   110 	 0.16673 	 0.20482 	 m..s
  111 	   111 	 0.19315 	 0.20485 	 ~...
  117 	   112 	 0.22133 	 0.20595 	 ~...
  112 	   113 	 0.19361 	 0.21020 	 ~...
   97 	   114 	 0.13798 	 0.21271 	 m..s
   98 	   115 	 0.13946 	 0.21807 	 m..s
  109 	   116 	 0.17419 	 0.22592 	 m..s
  118 	   117 	 0.22523 	 0.23441 	 ~...
  114 	   118 	 0.20896 	 0.25298 	 m..s
  119 	   119 	 0.23139 	 0.26667 	 m..s
  120 	   120 	 0.24176 	 0.26979 	 ~...
==========================================
r_mrr = 0.7077886462211609
r2_mrr = 0.35687702894210815
spearmanr_mrr@5 = 0.890876829624176
spearmanr_mrr@10 = 0.9294409155845642
spearmanr_mrr@50 = 0.8748039603233337
spearmanr_mrr@100 = 0.8528885245323181
spearmanr_mrr@All = 0.8685427904129028
==========================================
test time: 0.429
Done Testing dataset CoDExSmall
total time taken: 210.5949649810791
training time taken: 199.44766402244568
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7078)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.3569)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.8909)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9294)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.8748)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8529)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8685)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.5242039598197152}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 9648260179318608
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [256, 342, 294, 785, 881, 37, 637, 616, 430, 644, 182, 1120, 1108, 309, 1078, 336, 764, 676, 333, 349, 878, 316, 1043, 613, 949, 494, 400, 908, 150, 1056, 86, 912, 445, 966, 1168, 1018, 391, 310, 573, 847, 850, 468, 1027, 280, 1156, 653, 264, 816, 36, 326, 241, 433, 1014, 94, 950, 1214, 49, 985, 904, 725, 906, 515, 188, 1034, 499, 782, 504, 215, 772, 1064, 399, 22, 1097, 105, 810, 74, 665, 187, 190, 2, 582, 893, 814, 419, 939, 781, 550, 386, 868, 12, 962, 943, 214, 638, 829, 1066, 734, 864, 640, 561, 347, 1204, 965, 1000, 558, 983, 989, 129, 45, 208, 1158, 461, 1181, 451, 397, 262, 340, 223, 954, 120, 917]
valid_ids (0): []
train_ids (1094): [857, 650, 719, 923, 1058, 625, 783, 945, 351, 125, 1136, 25, 136, 516, 395, 691, 1118, 102, 1001, 594, 1209, 549, 1103, 147, 258, 581, 1199, 5, 1153, 35, 1009, 1207, 1096, 563, 952, 1161, 465, 463, 1134, 460, 514, 675, 371, 958, 293, 658, 385, 1157, 849, 790, 259, 799, 177, 79, 875, 517, 405, 976, 897, 696, 1200, 843, 126, 513, 181, 348, 570, 163, 388, 90, 235, 431, 830, 502, 1184, 584, 471, 910, 10, 553, 443, 1203, 862, 278, 702, 576, 731, 46, 1174, 84, 577, 376, 357, 931, 598, 145, 140, 1149, 413, 325, 1054, 915, 886, 1030, 447, 331, 678, 263, 444, 104, 52, 751, 792, 905, 663, 851, 381, 979, 526, 436, 1042, 439, 353, 291, 987, 1127, 17, 760, 980, 870, 1115, 483, 184, 1109, 416, 191, 272, 1023, 143, 818, 590, 968, 685, 379, 592, 523, 64, 845, 1143, 464, 510, 162, 547, 605, 901, 610, 42, 667, 1082, 168, 527, 362, 1186, 883, 528, 882, 710, 1048, 865, 884, 1090, 559, 1057, 377, 47, 647, 787, 78, 423, 1104, 179, 932, 578, 437, 543, 1016, 1035, 642, 1210, 257, 282, 890, 560, 902, 833, 396, 960, 273, 1017, 213, 947, 113, 18, 1179, 40, 364, 251, 662, 601, 101, 686, 648, 1020, 791, 564, 1197, 995, 707, 606, 836, 290, 1132, 859, 59, 1019, 1125, 977, 1061, 72, 828, 621, 587, 343, 1065, 300, 1085, 169, 874, 307, 769, 1113, 612, 71, 363, 654, 729, 61, 1122, 321, 212, 796, 283, 478, 82, 1005, 914, 271, 1183, 426, 111, 160, 715, 540, 754, 242, 1053, 860, 572, 450, 319, 721, 335, 304, 723, 387, 34, 519, 909, 733, 53, 265, 172, 422, 903, 132, 110, 414, 420, 951, 732, 1089, 824, 207, 1013, 529, 334, 750, 858, 997, 763, 596, 1160, 604, 938, 926, 580, 1032, 359, 531, 407, 56, 1111, 755, 503, 619, 77, 557, 848, 689, 512, 1102, 372, 724, 925, 1133, 13, 934, 1198, 907, 63, 861, 520, 548, 1169, 1110, 497, 737, 27, 509, 1081, 1114, 623, 652, 155, 631, 19, 330, 122, 314, 770, 1041, 81, 591, 834, 268, 953, 891, 1182, 133, 154, 449, 482, 1140, 922, 1116, 714, 575, 244, 198, 1010, 350, 673, 773, 608, 226, 195, 96, 706, 567, 446, 490, 511, 927, 885, 602, 1155, 275, 609, 894, 67, 73, 1193, 31, 197, 793, 963, 1202, 305, 853, 969, 481, 562, 76, 116, 837, 91, 1059, 742, 127, 417, 759, 368, 1038, 972, 554, 1173, 831, 852, 970, 356, 641, 473, 588, 425, 1093, 301, 1080, 38, 1025, 981, 1172, 541, 660, 1029, 877, 1105, 856, 1098, 217, 332, 778, 1100, 599, 495, 286, 297, 645, 1063, 7, 83, 618, 863, 937, 924, 1159, 919, 33, 827, 178, 205, 338, 216, 752, 615, 85, 539, 80, 532, 930, 477, 717, 21, 1185, 185, 462, 1092, 60, 803, 720, 672, 303, 626, 1175, 284, 109, 99, 780, 323, 1086, 398, 392, 786, 175, 346, 159, 961, 703, 586, 1037, 889, 209, 700, 839, 1189, 1021, 239, 669, 1124, 199, 248, 1040, 260, 747, 1167, 967, 1206, 739, 921, 899, 681, 176, 88, 620, 518, 245, 229, 118, 448, 164, 276, 643, 123, 51, 804, 730, 1144, 629, 1045, 138, 311, 1046, 639, 234, 811, 459, 896, 928, 825, 93, 249, 538, 753, 946, 43, 1131, 821, 1074, 95, 378, 551, 100, 141, 1015, 1117, 892, 498, 777, 535, 170, 880, 156, 112, 670, 20, 406, 920, 1135, 354, 1026, 992, 666, 222, 250, 26, 867, 728, 767, 708, 32, 148, 210, 956, 1073, 1212, 1055, 589, 131, 299, 408, 466, 795, 1107, 75, 651, 668, 194, 1084, 888, 775, 193, 579, 410, 913, 281, 69, 1163, 534, 617, 383, 274, 1091, 738, 54, 990, 1130, 1094, 634, 1024, 798, 30, 722, 48, 1075, 955, 911, 237, 55, 988, 1060, 530, 761, 552, 1004, 611, 964, 743, 657, 716, 664, 1147, 1171, 114, 491, 487, 756, 569, 1049, 871, 165, 1142, 533, 595, 522, 823, 900, 467, 687, 202, 1069, 228, 1011, 66, 603, 1076, 14, 784, 1067, 434, 693, 712, 704, 339, 556, 480, 699, 566, 1083, 1195, 246, 774, 986, 412, 374, 11, 916, 475, 600, 324, 835, 65, 684, 1201, 971, 402, 959, 500, 139, 382, 656, 1148, 384, 158, 812, 735, 415, 1154, 635, 1192, 501, 671, 855, 1176, 525, 1178, 994, 367, 121, 933, 749, 16, 805, 1051, 456, 380, 876, 255, 24, 661, 6, 745, 701, 1187, 130, 794, 269, 973, 329, 211, 746, 627, 1146, 457, 1180, 758, 674, 289, 369, 762, 574, 679, 630, 1208, 768, 128, 683, 1039, 1028, 9, 1126, 157, 295, 1141, 1121, 840, 524, 1031, 203, 935, 108, 243, 1044, 233, 292, 1079, 124, 427, 469, 236, 225, 998, 1165, 313, 1101, 887, 390, 315, 173, 8, 1, 705, 555, 328, 872, 1128, 841, 993, 942, 152, 428, 948, 394, 607, 44, 624, 3, 403, 401, 1003, 779, 842, 438, 545, 161, 221, 1205, 58, 489, 1008, 736, 655, 373, 219, 895, 404, 1006, 748, 1022, 151, 320, 247, 832, 393, 149, 117, 485, 429, 646, 200, 50, 298, 727, 39, 941, 166, 1139, 238, 869, 991, 296, 713, 789, 484, 936, 375, 288, 788, 218, 424, 680, 695, 593, 492, 317, 999, 632, 1213, 496, 366, 365, 41, 153, 822, 361, 1072, 726, 370, 231, 940, 508, 254, 62, 411, 106, 201, 192, 435, 694, 1070, 622, 698, 806, 421, 227, 279, 224, 1152, 776, 659, 565, 455, 807, 1190, 677, 232, 1099, 29, 1150, 302, 134, 312, 189, 28, 813, 771, 1106, 493, 978, 809, 546, 802, 355, 929, 252, 975, 797, 135, 360, 285, 1164, 476, 697, 144, 119, 89, 984, 1062, 472, 453, 741, 454, 568, 996, 103, 571, 918, 196, 167, 1211, 597, 479, 341, 506, 441, 765, 1052, 261, 137, 800, 146, 536, 866, 23, 186, 352, 389, 452, 633, 718, 1047, 766, 614, 521, 1138, 230, 1137, 544, 344, 220, 1123, 507, 1166, 879, 1071, 474, 740, 682, 838, 1033, 4, 345, 171, 1077, 1162, 174, 418, 583, 690, 898, 97, 488, 505, 1007, 70, 327, 92, 253, 358, 0, 711, 1170, 409, 826, 308, 266, 1129, 1068, 318, 944, 1088, 974, 57, 1012, 815, 628, 1177, 306, 982, 432, 844, 636, 87, 277, 442, 846, 458, 270, 287, 1087, 1145, 692, 470, 142, 107, 98, 649, 240, 542, 180, 801, 1036, 1151, 585, 1188, 1119, 808, 537, 957, 204, 1191, 183, 337, 1194, 1050, 1112, 322, 1002, 15, 709, 1196, 1095, 819, 440, 688, 115, 757, 486, 267, 820, 206, 873, 68, 854, 744, 817]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2110502777337082
the save name prefix for this run is:  chkpt-ID_2110502777337082_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 296
rank avg (pred): 0.466 +- 0.014
mrr vals (pred, true): 0.001, 0.272
batch losses (mrrl, rdl): 0.0, 0.0039232648

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 905
rank avg (pred): 0.131 +- 0.080
mrr vals (pred, true): 0.016, 0.131
batch losses (mrrl, rdl): 0.0, 0.0001234332

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 117
rank avg (pred): 0.301 +- 0.214
mrr vals (pred, true): 0.090, 0.108
batch losses (mrrl, rdl): 0.0, 0.0007098537

Epoch over!
epoch time: 13.059

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.045 +- 0.041
mrr vals (pred, true): 0.286, 0.188
batch losses (mrrl, rdl): 0.0, 4.4774e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 663
rank avg (pred): 0.334 +- 0.297
mrr vals (pred, true): 0.242, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001668533

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 402
rank avg (pred): 0.256 +- 0.259
mrr vals (pred, true): 0.325, 0.137
batch losses (mrrl, rdl): 0.0, 0.0004335708

Epoch over!
epoch time: 13.607

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 181
rank avg (pred): 0.254 +- 0.258
mrr vals (pred, true): 0.321, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005864439

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 15
rank avg (pred): 0.061 +- 0.067
mrr vals (pred, true): 0.411, 0.162
batch losses (mrrl, rdl): 0.0, 3.236e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 431
rank avg (pred): 0.236 +- 0.243
mrr vals (pred, true): 0.326, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006745853

Epoch over!
epoch time: 13.384

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1049
rank avg (pred): 0.240 +- 0.258
mrr vals (pred, true): 0.368, 0.004
batch losses (mrrl, rdl): 0.0, 0.0008312489

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1059
rank avg (pred): 0.053 +- 0.057
mrr vals (pred, true): 0.401, 0.269
batch losses (mrrl, rdl): 0.0, 5.9641e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1031
rank avg (pred): 0.240 +- 0.261
mrr vals (pred, true): 0.401, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007568303

Epoch over!
epoch time: 14.272

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.262 +- 0.268
mrr vals (pred, true): 0.320, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005134019

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1178
rank avg (pred): 0.290 +- 0.287
mrr vals (pred, true): 0.344, 0.009
batch losses (mrrl, rdl): 0.0, 1.35032e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 678
rank avg (pred): 0.361 +- 0.307
mrr vals (pred, true): 0.275, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001692879

Epoch over!
epoch time: 13.242

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 751
rank avg (pred): 0.062 +- 0.069
mrr vals (pred, true): 0.454, 0.147
batch losses (mrrl, rdl): 0.9373706579, 2.3988e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 594
rank avg (pred): 0.413 +- 0.197
mrr vals (pred, true): 0.035, 0.009
batch losses (mrrl, rdl): 0.0021455633, 0.000136628

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 531
rank avg (pred): 0.376 +- 0.209
mrr vals (pred, true): 0.068, 0.015
batch losses (mrrl, rdl): 0.0031018564, 0.0006280753

Epoch over!
epoch time: 13.394

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1198
rank avg (pred): 0.407 +- 0.189
mrr vals (pred, true): 0.032, 0.004
batch losses (mrrl, rdl): 0.0033789482, 7.85545e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.286 +- 0.137
mrr vals (pred, true): 0.048, 0.009
batch losses (mrrl, rdl): 3.51296e-05, 4.85741e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 583
rank avg (pred): 0.365 +- 0.190
mrr vals (pred, true): 0.073, 0.010
batch losses (mrrl, rdl): 0.005292532, 0.0001242049

Epoch over!
epoch time: 13.539

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 255
rank avg (pred): 0.067 +- 0.054
mrr vals (pred, true): 0.185, 0.191
batch losses (mrrl, rdl): 0.0003637322, 4.843e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 777
rank avg (pred): 0.330 +- 0.218
mrr vals (pred, true): 0.084, 0.164
batch losses (mrrl, rdl): 0.0636794418, 0.001280833

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 770
rank avg (pred): 0.320 +- 0.218
mrr vals (pred, true): 0.095, 0.170
batch losses (mrrl, rdl): 0.0559241325, 0.0011859087

Epoch over!
epoch time: 14.362

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 5
rank avg (pred): 0.039 +- 0.031
mrr vals (pred, true): 0.201, 0.197
batch losses (mrrl, rdl): 0.0001375991, 1.5793e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.348 +- 0.211
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0131855514, 0.0001647676

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 775
rank avg (pred): 0.326 +- 0.213
mrr vals (pred, true): 0.092, 0.163
batch losses (mrrl, rdl): 0.050567966, 0.0011314498

Epoch over!
epoch time: 15.24

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 576
rank avg (pred): 0.406 +- 0.172
mrr vals (pred, true): 0.043, 0.009
batch losses (mrrl, rdl): 0.0005484569, 0.0001376572

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1091
rank avg (pred): 0.293 +- 0.216
mrr vals (pred, true): 0.130, 0.176
batch losses (mrrl, rdl): 0.0211375393, 0.0006792907

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 186
rank avg (pred): 0.339 +- 0.205
mrr vals (pred, true): 0.093, 0.004
batch losses (mrrl, rdl): 0.0189120285, 0.0002282714

Epoch over!
epoch time: 15.09

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 968
rank avg (pred): 0.278 +- 0.203
mrr vals (pred, true): 0.114, 0.004
batch losses (mrrl, rdl): 0.0413512252, 0.0005905949

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 15
rank avg (pred): 0.075 +- 0.058
mrr vals (pred, true): 0.183, 0.162
batch losses (mrrl, rdl): 0.0045322366, 1.2404e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 229
rank avg (pred): 0.334 +- 0.201
mrr vals (pred, true): 0.092, 0.005
batch losses (mrrl, rdl): 0.0179722924, 0.0002273329

Epoch over!
epoch time: 14.044

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1029
rank avg (pred): 0.322 +- 0.206
mrr vals (pred, true): 0.097, 0.005
batch losses (mrrl, rdl): 0.0220108144, 0.0003442307

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 446
rank avg (pred): 0.311 +- 0.209
mrr vals (pred, true): 0.104, 0.004
batch losses (mrrl, rdl): 0.0291402489, 0.0002724854

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 752
rank avg (pred): 0.190 +- 0.140
mrr vals (pred, true): 0.152, 0.149
batch losses (mrrl, rdl): 7.41185e-05, 0.0004468556

Epoch over!
epoch time: 13.873

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.335 +- 0.198
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0215450972, 0.0002656487

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 260
rank avg (pred): 0.046 +- 0.035
mrr vals (pred, true): 0.215, 0.238
batch losses (mrrl, rdl): 0.0053700265, 1.7248e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 172
rank avg (pred): 0.338 +- 0.190
mrr vals (pred, true): 0.098, 0.003
batch losses (mrrl, rdl): 0.022632055, 0.0002523206

Epoch over!
epoch time: 13.802

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.076 +- 0.053
mrr vals (pred, true): 0.155, 0.220
batch losses (mrrl, rdl): 0.0425213017, 1.53369e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 96
rank avg (pred): 0.340 +- 0.185
mrr vals (pred, true): 0.097, 0.110
batch losses (mrrl, rdl): 0.0018561534, 0.0010602493

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 110
rank avg (pred): 0.315 +- 0.194
mrr vals (pred, true): 0.113, 0.156
batch losses (mrrl, rdl): 0.0188820269, 0.0010191586

Epoch over!
epoch time: 12.548

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 882
rank avg (pred): 0.330 +- 0.189
mrr vals (pred, true): 0.095, 0.005
batch losses (mrrl, rdl): 0.0202889405, 0.0002427389

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 453
rank avg (pred): 0.348 +- 0.174
mrr vals (pred, true): 0.073, 0.004
batch losses (mrrl, rdl): 0.0053363517, 0.00023462

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 981
rank avg (pred): 0.025 +- 0.019
mrr vals (pred, true): 0.241, 0.204
batch losses (mrrl, rdl): 0.013841385, 1.3103e-05

Epoch over!
epoch time: 14.054

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.108 +- 0.078
mrr vals (pred, true): 0.183, 0.226

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    3 	     0 	 0.04154 	 0.00327 	 m..s
   11 	     1 	 0.04651 	 0.00327 	 m..s
   16 	     2 	 0.05110 	 0.00330 	 m..s
   20 	     3 	 0.05226 	 0.00333 	 m..s
   13 	     4 	 0.04846 	 0.00336 	 m..s
    4 	     5 	 0.04250 	 0.00348 	 m..s
   69 	     6 	 0.11236 	 0.00353 	 MISS
   34 	     7 	 0.09747 	 0.00364 	 m..s
   81 	     8 	 0.12465 	 0.00368 	 MISS
   72 	     9 	 0.11659 	 0.00370 	 MISS
   94 	    10 	 0.14013 	 0.00375 	 MISS
   71 	    11 	 0.11502 	 0.00378 	 MISS
   49 	    12 	 0.10431 	 0.00384 	 MISS
   67 	    13 	 0.11226 	 0.00387 	 MISS
   54 	    14 	 0.10584 	 0.00390 	 MISS
   38 	    15 	 0.09939 	 0.00391 	 m..s
   66 	    16 	 0.11159 	 0.00393 	 MISS
   84 	    17 	 0.12733 	 0.00398 	 MISS
   60 	    18 	 0.10960 	 0.00405 	 MISS
   42 	    19 	 0.10124 	 0.00406 	 m..s
   46 	    20 	 0.10244 	 0.00414 	 m..s
   79 	    21 	 0.11991 	 0.00415 	 MISS
   82 	    22 	 0.12524 	 0.00420 	 MISS
   83 	    23 	 0.12730 	 0.00422 	 MISS
   57 	    24 	 0.10709 	 0.00422 	 MISS
   41 	    25 	 0.10018 	 0.00452 	 m..s
   87 	    26 	 0.13061 	 0.00453 	 MISS
   62 	    27 	 0.10997 	 0.00455 	 MISS
   61 	    28 	 0.10964 	 0.00457 	 MISS
   50 	    29 	 0.10488 	 0.00464 	 MISS
   52 	    30 	 0.10565 	 0.00466 	 MISS
   93 	    31 	 0.13965 	 0.00473 	 MISS
   63 	    32 	 0.11011 	 0.00486 	 MISS
   80 	    33 	 0.12124 	 0.00487 	 MISS
   77 	    34 	 0.11832 	 0.00488 	 MISS
   56 	    35 	 0.10709 	 0.00492 	 MISS
   86 	    36 	 0.13015 	 0.00493 	 MISS
   55 	    37 	 0.10657 	 0.00519 	 MISS
   10 	    38 	 0.04583 	 0.00725 	 m..s
    8 	    39 	 0.04455 	 0.00822 	 m..s
   24 	    40 	 0.06115 	 0.00884 	 m..s
    0 	    41 	 0.03902 	 0.00916 	 ~...
    7 	    42 	 0.04430 	 0.01073 	 m..s
    9 	    43 	 0.04496 	 0.01088 	 m..s
    6 	    44 	 0.04420 	 0.01162 	 m..s
   23 	    45 	 0.05457 	 0.01164 	 m..s
    1 	    46 	 0.04095 	 0.01310 	 ~...
    2 	    47 	 0.04114 	 0.01683 	 ~...
   18 	    48 	 0.05152 	 0.01713 	 m..s
   19 	    49 	 0.05197 	 0.01763 	 m..s
   17 	    50 	 0.05152 	 0.02142 	 m..s
   21 	    51 	 0.05295 	 0.02224 	 m..s
   22 	    52 	 0.05441 	 0.02529 	 ~...
   14 	    53 	 0.05027 	 0.02552 	 ~...
   12 	    54 	 0.04666 	 0.02572 	 ~...
    5 	    55 	 0.04301 	 0.02777 	 ~...
   40 	    56 	 0.09990 	 0.03517 	 m..s
   25 	    57 	 0.09075 	 0.03527 	 m..s
   15 	    58 	 0.05049 	 0.03662 	 ~...
   35 	    59 	 0.09829 	 0.07709 	 ~...
   26 	    60 	 0.09312 	 0.07990 	 ~...
   97 	    61 	 0.16355 	 0.10720 	 m..s
   32 	    62 	 0.09514 	 0.10903 	 ~...
   29 	    63 	 0.09437 	 0.11214 	 ~...
   30 	    64 	 0.09462 	 0.11421 	 ~...
   39 	    65 	 0.09945 	 0.11554 	 ~...
   28 	    66 	 0.09378 	 0.11742 	 ~...
   27 	    67 	 0.09373 	 0.11933 	 ~...
   95 	    68 	 0.14207 	 0.12102 	 ~...
   48 	    69 	 0.10306 	 0.12191 	 ~...
   88 	    70 	 0.13559 	 0.12251 	 ~...
   33 	    71 	 0.09653 	 0.13184 	 m..s
   31 	    72 	 0.09509 	 0.13774 	 m..s
   45 	    73 	 0.10188 	 0.14162 	 m..s
   53 	    74 	 0.10569 	 0.14285 	 m..s
   37 	    75 	 0.09898 	 0.14909 	 m..s
   36 	    76 	 0.09847 	 0.15121 	 m..s
   91 	    77 	 0.13798 	 0.15561 	 ~...
   70 	    78 	 0.11361 	 0.15768 	 m..s
   92 	    79 	 0.13851 	 0.15806 	 ~...
   58 	    80 	 0.10774 	 0.15881 	 m..s
   98 	    81 	 0.16415 	 0.15914 	 ~...
   73 	    82 	 0.11749 	 0.15954 	 m..s
   64 	    83 	 0.11013 	 0.15961 	 m..s
   99 	    84 	 0.17185 	 0.16001 	 ~...
   85 	    85 	 0.12809 	 0.16268 	 m..s
   65 	    86 	 0.11048 	 0.16573 	 m..s
   59 	    87 	 0.10905 	 0.16674 	 m..s
   74 	    88 	 0.11755 	 0.17072 	 m..s
   75 	    89 	 0.11814 	 0.17104 	 m..s
  108 	    90 	 0.20174 	 0.17472 	 ~...
   51 	    91 	 0.10527 	 0.17696 	 m..s
   68 	    92 	 0.11229 	 0.18062 	 m..s
   78 	    93 	 0.11852 	 0.18067 	 m..s
   44 	    94 	 0.10165 	 0.18195 	 m..s
  111 	    95 	 0.20646 	 0.18393 	 ~...
   76 	    96 	 0.11819 	 0.18624 	 m..s
   47 	    97 	 0.10259 	 0.18821 	 m..s
   43 	    98 	 0.10162 	 0.18871 	 m..s
  109 	    99 	 0.20383 	 0.19254 	 ~...
   89 	   100 	 0.13605 	 0.19443 	 m..s
  105 	   101 	 0.19937 	 0.20160 	 ~...
  100 	   102 	 0.17877 	 0.20482 	 ~...
  106 	   103 	 0.20006 	 0.20595 	 ~...
  102 	   104 	 0.18542 	 0.20672 	 ~...
  116 	   105 	 0.22010 	 0.20744 	 ~...
   90 	   106 	 0.13708 	 0.21079 	 m..s
  112 	   107 	 0.21073 	 0.21220 	 ~...
   96 	   108 	 0.14837 	 0.21234 	 m..s
  104 	   109 	 0.19239 	 0.22028 	 ~...
  103 	   110 	 0.19185 	 0.22076 	 ~...
  114 	   111 	 0.21309 	 0.22095 	 ~...
  101 	   112 	 0.18345 	 0.22592 	 m..s
  107 	   113 	 0.20148 	 0.23291 	 m..s
  120 	   114 	 0.25026 	 0.23428 	 ~...
  113 	   115 	 0.21129 	 0.24075 	 ~...
  115 	   116 	 0.21649 	 0.24327 	 ~...
  110 	   117 	 0.20481 	 0.24540 	 m..s
  117 	   118 	 0.22624 	 0.25298 	 ~...
  119 	   119 	 0.24706 	 0.25877 	 ~...
  118 	   120 	 0.24283 	 0.25940 	 ~...
==========================================
r_mrr = 0.723899781703949
r2_mrr = 0.4384682774543762
spearmanr_mrr@5 = 0.9838590025901794
spearmanr_mrr@10 = 0.951667845249176
spearmanr_mrr@50 = 0.9800122976303101
spearmanr_mrr@100 = 0.8671583533287048
spearmanr_mrr@All = 0.8904247283935547
==========================================
test time: 0.449
Done Testing dataset CoDExSmall
total time taken: 218.8988013267517
training time taken: 208.0367636680603
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7239)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4385)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9839)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9517)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9800)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8672)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8904)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.5528970413324714}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 4586819324266094
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [284, 831, 246, 71, 19, 361, 1166, 1134, 1201, 722, 816, 3, 731, 364, 81, 1138, 112, 931, 782, 296, 691, 935, 608, 69, 336, 923, 894, 992, 297, 38, 1068, 1186, 370, 51, 735, 42, 910, 405, 305, 1167, 554, 880, 646, 221, 308, 456, 355, 248, 399, 867, 198, 555, 602, 957, 786, 34, 23, 1038, 885, 961, 377, 228, 14, 624, 876, 1070, 408, 373, 897, 779, 1094, 217, 651, 604, 467, 1159, 161, 1132, 33, 1026, 850, 466, 956, 113, 421, 1183, 1149, 703, 349, 785, 881, 643, 639, 411, 896, 201, 844, 803, 443, 520, 548, 1205, 274, 930, 967, 92, 236, 1071, 149, 693, 671, 1187, 243, 969, 622, 630, 748, 613, 777, 383, 676]
valid_ids (0): []
train_ids (1094): [1088, 1192, 629, 1098, 823, 1120, 921, 534, 186, 718, 62, 524, 340, 57, 801, 889, 258, 828, 345, 18, 1154, 1007, 26, 440, 1180, 591, 1066, 389, 479, 1050, 714, 331, 721, 496, 679, 1073, 756, 1041, 561, 668, 323, 354, 487, 346, 556, 793, 747, 653, 356, 552, 610, 615, 887, 271, 214, 877, 379, 189, 917, 893, 859, 235, 905, 1001, 860, 819, 725, 128, 631, 821, 787, 1080, 759, 483, 315, 589, 448, 523, 1165, 337, 1122, 978, 713, 412, 1004, 338, 1097, 133, 1058, 266, 974, 299, 1000, 868, 277, 1016, 963, 663, 288, 376, 550, 1101, 352, 832, 1121, 773, 400, 115, 1191, 1012, 754, 446, 962, 543, 68, 451, 817, 66, 162, 290, 1190, 692, 1059, 110, 429, 1052, 792, 928, 80, 570, 966, 845, 410, 234, 511, 1175, 392, 253, 566, 30, 260, 362, 229, 951, 997, 197, 517, 879, 884, 557, 477, 150, 560, 233, 933, 1048, 1025, 1130, 372, 494, 763, 551, 1072, 1053, 344, 1092, 934, 999, 584, 125, 837, 899, 24, 929, 525, 460, 797, 160, 285, 690, 680, 869, 1086, 843, 132, 380, 1140, 141, 302, 397, 681, 450, 1013, 516, 1069, 914, 1185, 170, 1079, 158, 537, 533, 387, 851, 509, 12, 1179, 423, 901, 968, 965, 518, 1199, 28, 343, 497, 226, 848, 758, 637, 508, 324, 209, 946, 134, 1145, 638, 925, 611, 438, 120, 307, 49, 7, 1116, 757, 665, 1177, 402, 163, 1198, 15, 872, 1125, 598, 122, 1207, 375, 108, 1168, 334, 52, 585, 1163, 804, 4, 368, 526, 401, 640, 842, 127, 318, 1024, 1114, 712, 1084, 871, 328, 977, 1076, 737, 949, 674, 64, 454, 199, 168, 195, 1011, 357, 242, 781, 565, 1184, 240, 633, 687, 612, 41, 959, 1210, 601, 67, 683, 393, 43, 970, 507, 1206, 776, 701, 188, 1081, 1054, 1003, 280, 1214, 1027, 1146, 841, 16, 21, 652, 82, 22, 491, 634, 689, 648, 856, 529, 60, 157, 97, 603, 211, 891, 79, 427, 874, 780, 164, 461, 70, 101, 586, 490, 498, 220, 1049, 655, 771, 495, 519, 251, 169, 707, 1115, 1082, 826, 558, 109, 244, 219, 590, 532, 1144, 1010, 635, 366, 938, 8, 10, 1119, 784, 1062, 121, 861, 892, 1161, 1074, 697, 649, 762, 728, 350, 453, 87, 203, 594, 696, 1113, 814, 417, 979, 107, 1006, 1089, 265, 666, 911, 796, 1176, 314, 751, 138, 862, 213, 709, 184, 878, 1021, 670, 1141, 29, 732, 47, 353, 789, 93, 1103, 937, 906, 800, 152, 866, 650, 1112, 890, 514, 208, 1015, 886, 123, 472, 455, 846, 1102, 40, 326, 499, 176, 702, 644, 799, 404, 470, 27, 658, 73, 738, 282, 1008, 645, 147, 617, 912, 990, 175, 1031, 1211, 1139, 659, 1203, 319, 824, 247, 972, 291, 1106, 656, 1197, 950, 895, 434, 593, 310, 1036, 794, 898, 1075, 1160, 11, 863, 1034, 167, 119, 976, 452, 1174, 528, 694, 322, 873, 941, 486, 984, 54, 853, 301, 1104, 1057, 717, 420, 268, 818, 135, 306, 165, 1035, 657, 954, 58, 682, 245, 185, 753, 53, 791, 191, 447, 330, 231, 730, 916, 1150, 1032, 178, 812, 1117, 143, 1039, 329, 388, 945, 286, 813, 918, 685, 391, 1093, 182, 936, 283, 1123, 85, 578, 744, 750, 1044, 287, 430, 348, 426, 275, 17, 218, 742, 926, 259, 261, 1213, 1204, 215, 146, 562, 339, 173, 545, 567, 541, 815, 705, 729, 332, 1195, 662, 437, 413, 571, 765, 1, 398, 367, 710, 588, 641, 192, 210, 1143, 661, 991, 964, 445, 1051, 194, 327, 838, 269, 196, 1037, 900, 95, 960, 320, 289, 1046, 238, 549, 431, 1107, 313, 1009, 985, 432, 493, 1083, 227, 971, 428, 190, 772, 806, 124, 711, 471, 761, 769, 920, 475, 1078, 395, 875, 913, 607, 241, 358, 580, 363, 1042, 544, 1126, 409, 442, 1200, 542, 317, 1018, 174, 333, 86, 422, 205, 749, 1047, 888, 369, 577, 778, 1181, 232, 582, 940, 1189, 1171, 724, 870, 677, 419, 530, 106, 942, 72, 131, 56, 74, 745, 802, 156, 278, 1096, 783, 407, 767, 276, 988, 335, 1055, 994, 204, 118, 609, 351, 180, 1040, 298, 256, 396, 177, 148, 114, 84, 715, 136, 295, 484, 249, 482, 1148, 675, 788, 425, 1194, 883, 1085, 102, 104, 600, 1129, 130, 1155, 347, 907, 987, 144, 932, 250, 535, 489, 212, 193, 669, 621, 740, 1100, 839, 1182, 605, 441, 513, 1014, 78, 581, 309, 625, 538, 480, 596, 223, 465, 599, 378, 628, 614, 1109, 667, 45, 371, 374, 183, 1063, 569, 111, 1162, 385, 706, 834, 65, 833, 1188, 512, 61, 1212, 312, 515, 684, 126, 840, 947, 381, 501, 727, 695, 755, 25, 403, 958, 230, 574, 903, 943, 1169, 980, 116, 986, 636, 808, 1208, 996, 1135, 536, 416, 88, 1105, 647, 311, 207, 36, 733, 1118, 811, 583, 390, 83, 764, 632, 272, 592, 418, 865, 1172, 1017, 1170, 564, 944, 807, 500, 1147, 224, 1045, 449, 1124, 618, 187, 5, 13, 1164, 506, 222, 94, 382, 1196, 365, 1110, 664, 855, 206, 281, 678, 504, 904, 1023, 252, 836, 485, 1173, 492, 1091, 273, 1202, 98, 181, 716, 1002, 809, 955, 626, 522, 540, 909, 129, 768, 760, 1028, 468, 546, 46, 37, 736, 752, 76, 316, 539, 464, 708, 798, 620, 686, 444, 90, 386, 1033, 830, 1152, 59, 1178, 254, 654, 852, 154, 159, 563, 166, 827, 1151, 939, 321, 32, 975, 433, 1137, 835, 1099, 1136, 1193, 559, 1153, 502, 137, 510, 521, 1020, 153, 304, 6, 790, 117, 435, 673, 952, 202, 478, 1064, 257, 1061, 1056, 48, 527, 847, 948, 766, 1029, 1087, 384, 103, 595, 726, 998, 225, 1127, 457, 462, 0, 20, 1043, 770, 424, 704, 849, 100, 39, 96, 924, 267, 75, 415, 91, 294, 805, 660, 989, 476, 774, 915, 89, 142, 463, 829, 35, 179, 77, 406, 575, 1133, 1157, 739, 746, 1108, 1111, 488, 723, 9, 474, 151, 719, 439, 459, 579, 572, 50, 698, 503, 775, 239, 44, 973, 858, 810, 360, 2, 325, 993, 394, 642, 1128, 469, 155, 882, 436, 1158, 795, 1060, 619, 820, 741, 1019, 140, 927, 237, 1077, 700, 31, 293, 481, 1090, 1156, 587, 292, 341, 264, 983, 99, 473, 1209, 981, 262, 105, 919, 606, 505, 139, 743, 145, 922, 864, 255, 359, 300, 1095, 200, 672, 1022, 263, 627, 573, 279, 902, 568, 688, 63, 531, 982, 576, 995, 270, 547, 171, 822, 908, 953, 1065, 616, 458, 699, 1142, 342, 597, 1067, 1131, 553, 414, 734, 1005, 825, 720, 857, 172, 623, 854, 303, 1030, 216, 55]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9851026077322748
the save name prefix for this run is:  chkpt-ID_9851026077322748_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.414 +- 0.006
mrr vals (pred, true): 0.001, 0.163
batch losses (mrrl, rdl): 0.0, 0.0020575933

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 987
rank avg (pred): 0.114 +- 0.098
mrr vals (pred, true): 0.191, 0.263
batch losses (mrrl, rdl): 0.0, 0.0001326285

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1064
rank avg (pred): 0.070 +- 0.073
mrr vals (pred, true): 0.387, 0.184
batch losses (mrrl, rdl): 0.0, 3.1535e-06

Epoch over!
epoch time: 13.969

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 103
rank avg (pred): 0.262 +- 0.257
mrr vals (pred, true): 0.325, 0.138
batch losses (mrrl, rdl): 0.0, 0.0006731153

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 53
rank avg (pred): 0.052 +- 0.052
mrr vals (pred, true): 0.365, 0.237
batch losses (mrrl, rdl): 0.0, 4.6379e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 709
rank avg (pred): 0.277 +- 0.284
mrr vals (pred, true): 0.389, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004671082

Epoch over!
epoch time: 13.275

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 533
rank avg (pred): 0.130 +- 0.145
mrr vals (pred, true): 0.434, 0.035
batch losses (mrrl, rdl): 0.0, 8.2791e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 851
rank avg (pred): 0.229 +- 0.238
mrr vals (pred, true): 0.348, 0.165
batch losses (mrrl, rdl): 0.0, 0.0004927788

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 83
rank avg (pred): 0.262 +- 0.268
mrr vals (pred, true): 0.384, 0.170
batch losses (mrrl, rdl): 0.0, 0.0007203301

Epoch over!
epoch time: 12.521

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.377 +- 0.330
mrr vals (pred, true): 0.228, 0.003
batch losses (mrrl, rdl): 0.0, 9.20336e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 813
rank avg (pred): 0.067 +- 0.072
mrr vals (pred, true): 0.430, 0.152
batch losses (mrrl, rdl): 0.0, 1.4192e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 747
rank avg (pred): 0.055 +- 0.060
mrr vals (pred, true): 0.430, 0.133
batch losses (mrrl, rdl): 0.0, 9.0712e-06

Epoch over!
epoch time: 13.183

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1100
rank avg (pred): 0.240 +- 0.255
mrr vals (pred, true): 0.415, 0.180
batch losses (mrrl, rdl): 0.0, 0.0004368163

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1008
rank avg (pred): 0.250 +- 0.260
mrr vals (pred, true): 0.384, 0.182
batch losses (mrrl, rdl): 0.0, 0.0006288404

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 169
rank avg (pred): 0.257 +- 0.265
mrr vals (pred, true): 0.386, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004803544

Epoch over!
epoch time: 13.258

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 406
rank avg (pred): 0.278 +- 0.275
mrr vals (pred, true): 0.338, 0.005
batch losses (mrrl, rdl): 0.8286314607, 0.0003944119

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.394 +- 0.173
mrr vals (pred, true): 0.090, 0.173
batch losses (mrrl, rdl): 0.0696501359, 0.0019777117

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.052 +- 0.031
mrr vals (pred, true): 0.185, 0.189
batch losses (mrrl, rdl): 0.0001473061, 4.1207e-06

Epoch over!
epoch time: 13.554

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 939
rank avg (pred): 0.387 +- 0.139
mrr vals (pred, true): 0.073, 0.181
batch losses (mrrl, rdl): 0.1150125414, 0.001980368

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 303
rank avg (pred): 0.040 +- 0.022
mrr vals (pred, true): 0.174, 0.151
batch losses (mrrl, rdl): 0.0052560298, 5.09659e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 333
rank avg (pred): 0.336 +- 0.148
mrr vals (pred, true): 0.094, 0.117
batch losses (mrrl, rdl): 0.0054612234, 0.0008744141

Epoch over!
epoch time: 13.591

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 877
rank avg (pred): 0.322 +- 0.158
mrr vals (pred, true): 0.107, 0.004
batch losses (mrrl, rdl): 0.0326147303, 0.0003565242

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 253
rank avg (pred): 0.019 +- 0.011
mrr vals (pred, true): 0.208, 0.226
batch losses (mrrl, rdl): 0.003200134, 2.32705e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.353 +- 0.109
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0008765271, 0.0003623618

Epoch over!
epoch time: 12.973

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 698
rank avg (pred): 0.357 +- 0.102
mrr vals (pred, true): 0.047, 0.004
batch losses (mrrl, rdl): 8.5996e-05, 0.0003389831

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 686
rank avg (pred): 0.346 +- 0.103
mrr vals (pred, true): 0.056, 0.005
batch losses (mrrl, rdl): 0.0003270255, 0.0003459526

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.338 +- 0.104
mrr vals (pred, true): 0.058, 0.009
batch losses (mrrl, rdl): 0.0006699361, 5.4464e-05

Epoch over!
epoch time: 13.183

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 583
rank avg (pred): 0.340 +- 0.100
mrr vals (pred, true): 0.052, 0.010
batch losses (mrrl, rdl): 6.19225e-05, 8.05703e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 713
rank avg (pred): 0.340 +- 0.094
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.0007240446, 0.0003554128

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1084
rank avg (pred): 0.290 +- 0.138
mrr vals (pred, true): 0.105, 0.203
batch losses (mrrl, rdl): 0.0949012786, 0.0008949703

Epoch over!
epoch time: 12.275

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.329 +- 0.095
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 5.074e-07, 0.0005067118

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 390
rank avg (pred): 0.305 +- 0.120
mrr vals (pred, true): 0.080, 0.135
batch losses (mrrl, rdl): 0.0306248255, 0.000653965

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1153
rank avg (pred): 0.320 +- 0.102
mrr vals (pred, true): 0.066, 0.026
batch losses (mrrl, rdl): 0.0026667644, 0.0004992574

Epoch over!
epoch time: 13.457

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1023
rank avg (pred): 0.271 +- 0.145
mrr vals (pred, true): 0.126, 0.217
batch losses (mrrl, rdl): 0.0836385041, 0.0008051813

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 849
rank avg (pred): 0.279 +- 0.136
mrr vals (pred, true): 0.118, 0.159
batch losses (mrrl, rdl): 0.0166273192, 0.0006760842

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1040
rank avg (pred): 0.279 +- 0.134
mrr vals (pred, true): 0.115, 0.004
batch losses (mrrl, rdl): 0.041744411, 0.0007112392

Epoch over!
epoch time: 14.056

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 52
rank avg (pred): 0.015 +- 0.009
mrr vals (pred, true): 0.202, 0.208
batch losses (mrrl, rdl): 0.0003856835, 1.91497e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 384
rank avg (pred): 0.300 +- 0.116
mrr vals (pred, true): 0.084, 0.124
batch losses (mrrl, rdl): 0.0161934979, 0.0004639514

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 430
rank avg (pred): 0.287 +- 0.126
mrr vals (pred, true): 0.103, 0.005
batch losses (mrrl, rdl): 0.027848836, 0.0005652552

Epoch over!
epoch time: 14.903

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 913
rank avg (pred): 0.273 +- 0.132
mrr vals (pred, true): 0.122, 0.151
batch losses (mrrl, rdl): 0.0085367784, 0.0011237361

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 307
rank avg (pred): 0.027 +- 0.016
mrr vals (pred, true): 0.203, 0.218
batch losses (mrrl, rdl): 0.0019898864, 1.57797e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1164
rank avg (pred): 0.322 +- 0.085
mrr vals (pred, true): 0.042, 0.009
batch losses (mrrl, rdl): 0.0007148451, 5.37386e-05

Epoch over!
epoch time: 13.307

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 277
rank avg (pred): 0.024 +- 0.014
mrr vals (pred, true): 0.193, 0.166
batch losses (mrrl, rdl): 0.0071110739, 6.38314e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 563
rank avg (pred): 0.318 +- 0.093
mrr vals (pred, true): 0.053, 0.040
batch losses (mrrl, rdl): 6.77405e-05, 0.0006265797

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1129
rank avg (pred): 0.268 +- 0.134
mrr vals (pred, true): 0.123, 0.004
batch losses (mrrl, rdl): 0.0537798591, 0.00082787

Epoch over!
epoch time: 14.017

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.014 +- 0.008
mrr vals (pred, true): 0.224, 0.243

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04745 	 0.00290 	 m..s
   25 	     1 	 0.05177 	 0.00333 	 m..s
   68 	     2 	 0.10177 	 0.00340 	 m..s
   44 	     3 	 0.09122 	 0.00347 	 m..s
   40 	     4 	 0.08825 	 0.00349 	 m..s
   47 	     5 	 0.09253 	 0.00352 	 m..s
   71 	     6 	 0.10207 	 0.00352 	 m..s
   87 	     7 	 0.11402 	 0.00354 	 MISS
   59 	     8 	 0.09828 	 0.00363 	 m..s
   56 	     9 	 0.09739 	 0.00368 	 m..s
   61 	    10 	 0.09878 	 0.00370 	 m..s
    1 	    11 	 0.04126 	 0.00370 	 m..s
   70 	    12 	 0.10203 	 0.00376 	 m..s
   46 	    13 	 0.09238 	 0.00382 	 m..s
   62 	    14 	 0.09909 	 0.00384 	 m..s
   13 	    15 	 0.04825 	 0.00391 	 m..s
   81 	    16 	 0.10838 	 0.00392 	 MISS
   11 	    17 	 0.04787 	 0.00392 	 m..s
   66 	    18 	 0.10089 	 0.00394 	 m..s
    7 	    19 	 0.04710 	 0.00398 	 m..s
   77 	    20 	 0.10659 	 0.00398 	 MISS
   34 	    21 	 0.08697 	 0.00406 	 m..s
   74 	    22 	 0.10595 	 0.00412 	 MISS
   90 	    23 	 0.11777 	 0.00412 	 MISS
   26 	    24 	 0.05187 	 0.00413 	 m..s
   79 	    25 	 0.10777 	 0.00415 	 MISS
   72 	    26 	 0.10328 	 0.00415 	 m..s
   35 	    27 	 0.08731 	 0.00417 	 m..s
   73 	    28 	 0.10581 	 0.00420 	 MISS
    0 	    29 	 0.03901 	 0.00422 	 m..s
   63 	    30 	 0.09958 	 0.00427 	 m..s
   28 	    31 	 0.05345 	 0.00427 	 m..s
   91 	    32 	 0.12482 	 0.00435 	 MISS
   53 	    33 	 0.09532 	 0.00466 	 m..s
   32 	    34 	 0.08597 	 0.00471 	 m..s
   38 	    35 	 0.08780 	 0.00486 	 m..s
   39 	    36 	 0.08782 	 0.00493 	 m..s
   43 	    37 	 0.09103 	 0.00498 	 m..s
   85 	    38 	 0.11024 	 0.00528 	 MISS
   10 	    39 	 0.04768 	 0.00700 	 m..s
   21 	    40 	 0.05031 	 0.00700 	 m..s
   22 	    41 	 0.05073 	 0.00741 	 m..s
    2 	    42 	 0.04222 	 0.00942 	 m..s
   29 	    43 	 0.05354 	 0.01006 	 m..s
   24 	    44 	 0.05129 	 0.01021 	 m..s
   19 	    45 	 0.05023 	 0.01064 	 m..s
   14 	    46 	 0.04833 	 0.01098 	 m..s
    4 	    47 	 0.04451 	 0.01122 	 m..s
   12 	    48 	 0.04821 	 0.01162 	 m..s
    8 	    49 	 0.04732 	 0.01188 	 m..s
   27 	    50 	 0.05204 	 0.01207 	 m..s
    3 	    51 	 0.04275 	 0.01230 	 m..s
   15 	    52 	 0.04891 	 0.01504 	 m..s
   16 	    53 	 0.04897 	 0.01915 	 ~...
   17 	    54 	 0.04909 	 0.02168 	 ~...
   31 	    55 	 0.05495 	 0.02188 	 m..s
    6 	    56 	 0.04546 	 0.02247 	 ~...
   20 	    57 	 0.05026 	 0.02290 	 ~...
   30 	    58 	 0.05435 	 0.02330 	 m..s
    5 	    59 	 0.04470 	 0.02423 	 ~...
   41 	    60 	 0.08871 	 0.02829 	 m..s
   18 	    61 	 0.04948 	 0.03764 	 ~...
   42 	    62 	 0.08949 	 0.03870 	 m..s
   48 	    63 	 0.09311 	 0.03959 	 m..s
   23 	    64 	 0.05100 	 0.04048 	 ~...
   58 	    65 	 0.09816 	 0.04331 	 m..s
   97 	    66 	 0.16868 	 0.05558 	 MISS
   33 	    67 	 0.08633 	 0.10026 	 ~...
   96 	    68 	 0.16750 	 0.10720 	 m..s
   37 	    69 	 0.08774 	 0.11933 	 m..s
   45 	    70 	 0.09219 	 0.12846 	 m..s
   84 	    71 	 0.10997 	 0.13114 	 ~...
   92 	    72 	 0.14365 	 0.13270 	 ~...
   93 	    73 	 0.14421 	 0.13475 	 ~...
   95 	    74 	 0.16260 	 0.14129 	 ~...
   98 	    75 	 0.17117 	 0.14231 	 ~...
   50 	    76 	 0.09412 	 0.14249 	 m..s
   49 	    77 	 0.09320 	 0.14285 	 m..s
  103 	    78 	 0.17911 	 0.15051 	 ~...
  102 	    79 	 0.17863 	 0.15311 	 ~...
   36 	    80 	 0.08738 	 0.15364 	 m..s
  101 	    81 	 0.17829 	 0.15441 	 ~...
   52 	    82 	 0.09496 	 0.15622 	 m..s
   94 	    83 	 0.14621 	 0.15715 	 ~...
   78 	    84 	 0.10729 	 0.16065 	 m..s
   67 	    85 	 0.10102 	 0.16385 	 m..s
   51 	    86 	 0.09456 	 0.16489 	 m..s
   54 	    87 	 0.09572 	 0.16573 	 m..s
   69 	    88 	 0.10188 	 0.16674 	 m..s
   75 	    89 	 0.10597 	 0.16773 	 m..s
   65 	    90 	 0.10077 	 0.17169 	 m..s
   60 	    91 	 0.09863 	 0.17176 	 m..s
   82 	    92 	 0.10866 	 0.17181 	 m..s
   55 	    93 	 0.09677 	 0.17275 	 m..s
  100 	    94 	 0.17572 	 0.17290 	 ~...
   88 	    95 	 0.11529 	 0.17310 	 m..s
  104 	    96 	 0.18983 	 0.17444 	 ~...
  105 	    97 	 0.19144 	 0.17605 	 ~...
   83 	    98 	 0.10893 	 0.17666 	 m..s
   64 	    99 	 0.10018 	 0.17785 	 m..s
   99 	   100 	 0.17563 	 0.17866 	 ~...
   76 	   101 	 0.10624 	 0.17903 	 m..s
  106 	   102 	 0.20134 	 0.18392 	 ~...
   80 	   103 	 0.10789 	 0.18550 	 m..s
   89 	   104 	 0.11681 	 0.18552 	 m..s
  111 	   105 	 0.21680 	 0.18788 	 ~...
   57 	   106 	 0.09815 	 0.18821 	 m..s
  107 	   107 	 0.20734 	 0.19574 	 ~...
  108 	   108 	 0.20938 	 0.20093 	 ~...
  110 	   109 	 0.21593 	 0.20365 	 ~...
  109 	   110 	 0.21196 	 0.20488 	 ~...
   86 	   111 	 0.11317 	 0.20861 	 m..s
  112 	   112 	 0.21683 	 0.22604 	 ~...
  117 	   113 	 0.22632 	 0.22900 	 ~...
  116 	   114 	 0.22601 	 0.22952 	 ~...
  113 	   115 	 0.22114 	 0.23323 	 ~...
  115 	   116 	 0.22389 	 0.24311 	 ~...
  114 	   117 	 0.22380 	 0.24547 	 ~...
  118 	   118 	 0.26635 	 0.25687 	 ~...
  119 	   119 	 0.26686 	 0.26384 	 ~...
  120 	   120 	 0.28377 	 0.27211 	 ~...
==========================================
r_mrr = 0.7646281719207764
r2_mrr = 0.4924916625022888
spearmanr_mrr@5 = 0.968792200088501
spearmanr_mrr@10 = 0.9055423140525818
spearmanr_mrr@50 = 0.9433892369270325
spearmanr_mrr@100 = 0.8602734804153442
spearmanr_mrr@All = 0.887365460395813
==========================================
test time: 0.541
Done Testing dataset CoDExSmall
total time taken: 211.45081424713135
training time taken: 202.15356945991516
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7646)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4925)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9688)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9055)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9434)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8603)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8874)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.226603550629079}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 5557917464467117
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [536, 776, 661, 450, 1058, 515, 1159, 9, 545, 944, 152, 91, 681, 403, 281, 639, 694, 237, 885, 192, 758, 1199, 997, 980, 747, 43, 1163, 331, 894, 358, 292, 527, 990, 61, 993, 433, 383, 273, 507, 1138, 957, 1100, 1157, 1214, 122, 905, 73, 808, 249, 924, 219, 1107, 898, 766, 832, 820, 398, 550, 1172, 24, 922, 880, 189, 340, 1090, 367, 795, 893, 951, 900, 113, 979, 706, 910, 623, 1034, 402, 1054, 723, 860, 998, 368, 1030, 174, 471, 1061, 597, 326, 197, 62, 244, 33, 265, 927, 289, 742, 1140, 1213, 755, 803, 875, 656, 225, 140, 921, 1057, 72, 884, 391, 194, 112, 557, 274, 794, 253, 59, 1044, 212, 878, 58, 60]
valid_ids (0): []
train_ids (1094): [185, 401, 912, 609, 526, 941, 102, 588, 355, 1023, 1152, 704, 790, 210, 310, 566, 770, 655, 240, 36, 147, 809, 46, 1046, 899, 1015, 207, 458, 1069, 1207, 211, 916, 182, 939, 302, 975, 428, 1062, 290, 188, 642, 1070, 1174, 1202, 276, 1007, 709, 449, 77, 695, 378, 343, 381, 646, 901, 562, 1173, 834, 89, 1137, 1043, 1001, 598, 178, 807, 640, 11, 915, 349, 455, 481, 88, 464, 114, 1098, 115, 432, 670, 828, 931, 32, 144, 238, 1119, 316, 602, 518, 519, 744, 421, 865, 802, 1200, 845, 54, 215, 148, 506, 227, 153, 1197, 680, 388, 959, 844, 788, 821, 826, 982, 1005, 1047, 1193, 141, 914, 1056, 732, 396, 136, 259, 283, 389, 1110, 1080, 171, 517, 1032, 604, 40, 1167, 676, 621, 1180, 745, 569, 82, 218, 260, 606, 440, 360, 940, 962, 395, 35, 964, 778, 772, 1150, 119, 567, 268, 1022, 529, 128, 241, 970, 654, 105, 1203, 243, 356, 1106, 41, 352, 801, 1081, 612, 50, 181, 484, 1004, 1008, 262, 1040, 669, 1039, 1155, 137, 172, 15, 601, 864, 416, 693, 994, 477, 282, 871, 354, 846, 781, 607, 819, 516, 1149, 671, 659, 441, 415, 904, 638, 591, 474, 1093, 165, 126, 85, 1078, 973, 1088, 1195, 1136, 1189, 318, 1184, 969, 261, 1094, 525, 836, 103, 304, 1059, 540, 359, 193, 184, 689, 1082, 495, 869, 311, 816, 1158, 1120, 56, 721, 399, 462, 1160, 374, 1052, 867, 1182, 690, 1037, 67, 677, 216, 626, 332, 135, 955, 662, 339, 53, 929, 938, 874, 424, 1031, 280, 3, 502, 1161, 762, 1102, 1141, 538, 317, 173, 1049, 574, 838, 1143, 371, 635, 1133, 503, 786, 937, 157, 926, 622, 556, 907, 409, 7, 568, 175, 658, 101, 156, 314, 966, 97, 812, 925, 267, 217, 443, 668, 445, 30, 734, 1194, 201, 579, 902, 1072, 978, 51, 501, 679, 1105, 263, 228, 315, 410, 999, 37, 120, 111, 553, 949, 943, 1071, 633, 1186, 542, 1051, 544, 168, 223, 796, 489, 329, 511, 44, 585, 96, 775, 202, 13, 861, 420, 1073, 858, 664, 1027, 710, 840, 996, 851, 1122, 774, 342, 463, 151, 444, 1118, 269, 950, 752, 10, 341, 663, 648, 1166, 84, 879, 883, 909, 124, 737, 876, 531, 643, 68, 20, 595, 429, 627, 967, 337, 473, 974, 308, 888, 1142, 896, 815, 652, 408, 320, 992, 230, 465, 348, 75, 221, 892, 561, 232, 1135, 573, 347, 1038, 453, 14, 678, 270, 667, 239, 327, 490, 98, 437, 716, 376, 1124, 948, 499, 1154, 653, 920, 749, 1020, 229, 350, 482, 986, 1201, 512, 684, 582, 1003, 850, 264, 246, 1025, 1128, 149, 130, 1171, 1036, 176, 1178, 1168, 1147, 351, 988, 251, 647, 400, 767, 711, 377, 1177, 972, 822, 768, 254, 129, 200, 434, 1029, 613, 1095, 305, 422, 452, 746, 641, 522, 146, 323, 1017, 952, 863, 196, 719, 603, 1132, 509, 1156, 31, 100, 1145, 514, 1188, 995, 143, 1033, 636, 150, 1055, 703, 5, 468, 220, 319, 806, 780, 438, 810, 583, 480, 303, 106, 682, 700, 696, 1169, 1097, 565, 117, 1101, 38, 321, 632, 797, 532, 406, 523, 1196, 439, 71, 571, 279, 45, 191, 123, 1086, 764, 372, 908, 1125, 322, 166, 580, 508, 1117, 785, 163, 811, 451, 418, 387, 1079, 483, 1096, 170, 817, 284, 498, 42, 79, 599, 835, 674, 492, 397, 0, 1181, 563, 1065, 467, 127, 724, 25, 338, 447, 584, 634, 868, 208, 824, 513, 618, 738, 890, 843, 702, 945, 87, 534, 345, 686, 1076, 575, 707, 392, 739, 86, 205, 167, 855, 257, 1010, 731, 384, 57, 611, 177, 277, 1103, 1068, 1146, 328, 1139, 295, 954, 1091, 946, 160, 300, 18, 1075, 688, 1115, 620, 159, 161, 1035, 90, 625, 34, 287, 779, 1041, 162, 369, 1179, 630, 541, 761, 558, 4, 976, 906, 547, 554, 385, 947, 521, 829, 769, 1151, 1165, 154, 248, 935, 334, 1191, 722, 985, 226, 981, 80, 255, 754, 740, 759, 818, 577, 27, 930, 672, 109, 203, 1176, 793, 644, 827, 1011, 1063, 309, 751, 81, 714, 831, 1108, 407, 520, 206, 551, 275, 330, 637, 903, 792, 617, 825, 1074, 991, 692, 842, 76, 1016, 297, 862, 1208, 891, 699, 49, 23, 375, 121, 837, 728, 530, 475, 727, 675, 799, 581, 1013, 139, 460, 1121, 555, 1092, 570, 390, 546, 266, 1112, 1024, 17, 22, 705, 614, 748, 199, 272, 1014, 771, 763, 1009, 1048, 685, 1162, 872, 715, 1019, 19, 989, 628, 804, 800, 134, 735, 419, 236, 1116, 446, 650, 537, 870, 222, 918, 660, 651, 645, 426, 494, 491, 971, 294, 605, 805, 179, 857, 708, 1066, 487, 183, 589, 234, 125, 1083, 64, 1002, 743, 886, 26, 431, 718, 958, 65, 132, 1211, 1175, 336, 687, 936, 1, 344, 984, 209, 466, 1085, 213, 1111, 285, 1131, 756, 848, 859, 928, 1198, 497, 1134, 1153, 164, 454, 629, 960, 853, 169, 965, 1064, 942, 363, 92, 977, 765, 1164, 247, 730, 1185, 469, 791, 48, 413, 291, 624, 52, 543, 1123, 312, 63, 757, 839, 616, 733, 934, 913, 968, 1050, 78, 698, 1099, 911, 505, 1109, 430, 987, 94, 95, 535, 457, 560, 190, 887, 983, 590, 296, 1087, 823, 919, 787, 528, 673, 138, 414, 683, 373, 1067, 596, 155, 1127, 1130, 881, 877, 564, 353, 1053, 70, 252, 404, 773, 932, 852, 313, 539, 488, 195, 1190, 783, 572, 107, 66, 760, 435, 1006, 1104, 849, 666, 28, 953, 286, 245, 665, 578, 1114, 293, 559, 1206, 720, 364, 346, 83, 470, 963, 224, 600, 1028, 256, 923, 729, 961, 8, 833, 895, 99, 366, 423, 459, 69, 1187, 649, 592, 370, 301, 242, 1060, 631, 496, 1126, 712, 250, 158, 713, 576, 889, 549, 231, 610, 448, 436, 417, 21, 1113, 472, 586, 55, 411, 214, 116, 74, 271, 753, 1192, 782, 93, 278, 306, 1084, 405, 789, 1000, 933, 258, 1077, 813, 118, 362, 479, 16, 608, 1209, 1183, 594, 717, 382, 701, 361, 476, 427, 180, 814, 741, 504, 288, 1212, 548, 394, 1089, 847, 298, 204, 917, 2, 1045, 593, 524, 510, 299, 307, 412, 726, 866, 533, 854, 1026, 187, 873, 1144, 325, 365, 442, 750, 145, 697, 882, 29, 6, 47, 841, 380, 357, 335, 1148, 478, 956, 133, 198, 233, 615, 386, 830, 856, 1210, 784, 485, 552, 1018, 1204, 12, 1205, 104, 108, 1129, 777, 379, 461, 333, 657, 1042, 131, 691, 1021, 493, 110, 393, 619, 456, 425, 486, 186, 725, 1170, 500, 39, 324, 235, 897, 142, 798, 587, 736, 1012]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3165434729588150
the save name prefix for this run is:  chkpt-ID_3165434729588150_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1181
rank avg (pred): 0.601 +- 0.002
mrr vals (pred, true): 0.001, 0.009
batch losses (mrrl, rdl): 0.0, 0.0016085908

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1047
rank avg (pred): 0.258 +- 0.217
mrr vals (pred, true): 0.179, 0.004
batch losses (mrrl, rdl): 0.0, 0.000712921

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 476
rank avg (pred): 0.244 +- 0.220
mrr vals (pred, true): 0.283, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006840386

Epoch over!
epoch time: 14.965

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 522
rank avg (pred): 0.151 +- 0.140
mrr vals (pred, true): 0.312, 0.017
batch losses (mrrl, rdl): 0.0, 8.24305e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 614
rank avg (pred): 0.322 +- 0.275
mrr vals (pred, true): 0.241, 0.021
batch losses (mrrl, rdl): 0.0, 6.3807e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 57
rank avg (pred): 0.070 +- 0.069
mrr vals (pred, true): 0.363, 0.146
batch losses (mrrl, rdl): 0.0, 3.4651e-06

Epoch over!
epoch time: 14.78

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.181 +- 0.180
mrr vals (pred, true): 0.356, 0.019
batch losses (mrrl, rdl): 0.0, 1.1645e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.295 +- 0.285
mrr vals (pred, true): 0.368, 0.009
batch losses (mrrl, rdl): 0.0, 1.62698e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 702
rank avg (pred): 0.330 +- 0.291
mrr vals (pred, true): 0.264, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002573974

Epoch over!
epoch time: 14.612

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.289 +- 0.281
mrr vals (pred, true): 0.344, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003251868

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.056 +- 0.056
mrr vals (pred, true): 0.359, 0.147
batch losses (mrrl, rdl): 0.0, 9.393e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 96
rank avg (pred): 0.251 +- 0.263
mrr vals (pred, true): 0.392, 0.110
batch losses (mrrl, rdl): 0.0, 0.0004605067

Epoch over!
epoch time: 14.016

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 874
rank avg (pred): 0.237 +- 0.257
mrr vals (pred, true): 0.415, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006841387

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 970
rank avg (pred): 0.312 +- 0.286
mrr vals (pred, true): 0.324, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002511966

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 106
rank avg (pred): 0.240 +- 0.265
mrr vals (pred, true): 0.438, 0.135
batch losses (mrrl, rdl): 0.0, 0.000508969

Epoch over!
epoch time: 13.479

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1103
rank avg (pred): 0.257 +- 0.277
mrr vals (pred, true): 0.421, 0.198
batch losses (mrrl, rdl): 0.4976011813, 0.0007456117

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.422 +- 0.260
mrr vals (pred, true): 0.078, 0.027
batch losses (mrrl, rdl): 0.0078411894, 0.0014483657

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 306
rank avg (pred): 0.070 +- 0.055
mrr vals (pred, true): 0.170, 0.200
batch losses (mrrl, rdl): 0.0090173986, 5.5476e-06

Epoch over!
epoch time: 14.448

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.400 +- 0.253
mrr vals (pred, true): 0.095, 0.138
batch losses (mrrl, rdl): 0.0193012059, 0.0019081273

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 11
rank avg (pred): 0.028 +- 0.022
mrr vals (pred, true): 0.208, 0.231
batch losses (mrrl, rdl): 0.0052250493, 2.4316e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 195
rank avg (pred): 0.376 +- 0.244
mrr vals (pred, true): 0.116, 0.003
batch losses (mrrl, rdl): 0.0442036465, 7.51716e-05

Epoch over!
epoch time: 13.884

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 617
rank avg (pred): 0.477 +- 0.187
mrr vals (pred, true): 0.041, 0.023
batch losses (mrrl, rdl): 0.0007340481, 0.0008020466

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 724
rank avg (pred): 0.453 +- 0.192
mrr vals (pred, true): 0.054, 0.003
batch losses (mrrl, rdl): 0.0001363728, 3.99205e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 528
rank avg (pred): 0.427 +- 0.198
mrr vals (pred, true): 0.064, 0.016
batch losses (mrrl, rdl): 0.0019765536, 0.0010080072

Epoch over!
epoch time: 15.118

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 51
rank avg (pred): 0.028 +- 0.022
mrr vals (pred, true): 0.208, 0.184
batch losses (mrrl, rdl): 0.0056256116, 1.6559e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 179
rank avg (pred): 0.324 +- 0.224
mrr vals (pred, true): 0.133, 0.004
batch losses (mrrl, rdl): 0.0690452605, 0.0002608265

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1183
rank avg (pred): 0.420 +- 0.182
mrr vals (pred, true): 0.052, 0.010
batch losses (mrrl, rdl): 4.63469e-05, 0.0002757664

Epoch over!
epoch time: 13.363

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1035
rank avg (pred): 0.252 +- 0.176
mrr vals (pred, true): 0.124, 0.005
batch losses (mrrl, rdl): 0.0552426688, 0.0008271434

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 52
rank avg (pred): 0.036 +- 0.026
mrr vals (pred, true): 0.193, 0.208
batch losses (mrrl, rdl): 0.0023609526, 2.9724e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 246
rank avg (pred): 0.047 +- 0.035
mrr vals (pred, true): 0.196, 0.153
batch losses (mrrl, rdl): 0.0183821134, 3.23019e-05

Epoch over!
epoch time: 13.57

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 857
rank avg (pred): 0.324 +- 0.201
mrr vals (pred, true): 0.094, 0.158
batch losses (mrrl, rdl): 0.0404650904, 0.0010898291

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 948
rank avg (pred): 0.359 +- 0.189
mrr vals (pred, true): 0.075, 0.004
batch losses (mrrl, rdl): 0.0065009817, 0.0001778419

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 46
rank avg (pred): 0.014 +- 0.010
mrr vals (pred, true): 0.237, 0.212
batch losses (mrrl, rdl): 0.0061391131, 2.39289e-05

Epoch over!
epoch time: 13.506

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 495
rank avg (pred): 0.394 +- 0.154
mrr vals (pred, true): 0.043, 0.015
batch losses (mrrl, rdl): 0.0004925133, 0.0006966025

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.161 +- 0.110
mrr vals (pred, true): 0.141, 0.111
batch losses (mrrl, rdl): 0.0085838456, 0.0002307051

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1117
rank avg (pred): 0.314 +- 0.188
mrr vals (pred, true): 0.107, 0.004
batch losses (mrrl, rdl): 0.0324029326, 0.0003806221

Epoch over!
epoch time: 14.185

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.334 +- 0.181
mrr vals (pred, true): 0.089, 0.005
batch losses (mrrl, rdl): 0.0152354417, 0.0002909017

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 319
rank avg (pred): 0.016 +- 0.011
mrr vals (pred, true): 0.215, 0.256
batch losses (mrrl, rdl): 0.0173530821, 1.81993e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.330 +- 0.175
mrr vals (pred, true): 0.078, 0.138
batch losses (mrrl, rdl): 0.0366935879, 0.0011108376

Epoch over!
epoch time: 14.604

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 585
rank avg (pred): 0.395 +- 0.124
mrr vals (pred, true): 0.019, 0.007
batch losses (mrrl, rdl): 0.0094144735, 0.0001285628

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 722
rank avg (pred): 0.362 +- 0.145
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002583051, 0.0002832313

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 343
rank avg (pred): 0.322 +- 0.172
mrr vals (pred, true): 0.086, 0.172
batch losses (mrrl, rdl): 0.0752375722, 0.0010164649

Epoch over!
epoch time: 14.37

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 991
rank avg (pred): 0.027 +- 0.018
mrr vals (pred, true): 0.191, 0.200
batch losses (mrrl, rdl): 0.0009653104, 1.15306e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.305 +- 0.176
mrr vals (pred, true): 0.109, 0.130
batch losses (mrrl, rdl): 0.0045254948, 0.0006903794

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 295
rank avg (pred): 0.014 +- 0.009
mrr vals (pred, true): 0.229, 0.247
batch losses (mrrl, rdl): 0.0034063151, 1.94153e-05

Epoch over!
epoch time: 13.699

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.343 +- 0.148
mrr vals (pred, true): 0.061, 0.035

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   12 	     0 	 0.05017 	 0.00330 	 m..s
   80 	     1 	 0.11225 	 0.00338 	 MISS
    0 	     2 	 0.03584 	 0.00350 	 m..s
   10 	     3 	 0.04959 	 0.00352 	 m..s
   63 	     4 	 0.10804 	 0.00352 	 MISS
    7 	     5 	 0.04131 	 0.00358 	 m..s
    8 	     6 	 0.04135 	 0.00361 	 m..s
   63 	     7 	 0.10804 	 0.00363 	 MISS
   53 	     8 	 0.09088 	 0.00370 	 m..s
   63 	     9 	 0.10804 	 0.00372 	 MISS
   41 	    10 	 0.08522 	 0.00373 	 m..s
   37 	    11 	 0.08359 	 0.00374 	 m..s
    4 	    12 	 0.04058 	 0.00376 	 m..s
   13 	    13 	 0.05173 	 0.00383 	 m..s
   83 	    14 	 0.12018 	 0.00386 	 MISS
   23 	    15 	 0.07377 	 0.00387 	 m..s
   63 	    16 	 0.10804 	 0.00394 	 MISS
   63 	    17 	 0.10804 	 0.00404 	 MISS
   63 	    18 	 0.10804 	 0.00405 	 MISS
   63 	    19 	 0.10804 	 0.00406 	 MISS
   33 	    20 	 0.08279 	 0.00409 	 m..s
    3 	    21 	 0.04015 	 0.00409 	 m..s
   26 	    22 	 0.07635 	 0.00414 	 m..s
   60 	    23 	 0.09878 	 0.00415 	 m..s
    1 	    24 	 0.03832 	 0.00415 	 m..s
   47 	    25 	 0.08846 	 0.00422 	 m..s
   43 	    26 	 0.08737 	 0.00422 	 m..s
   38 	    27 	 0.08370 	 0.00423 	 m..s
   84 	    28 	 0.12133 	 0.00435 	 MISS
   48 	    29 	 0.08863 	 0.00446 	 m..s
   34 	    30 	 0.08282 	 0.00448 	 m..s
   63 	    31 	 0.10804 	 0.00452 	 MISS
   79 	    32 	 0.10990 	 0.00455 	 MISS
   63 	    33 	 0.10804 	 0.00463 	 MISS
   59 	    34 	 0.09793 	 0.00474 	 m..s
   85 	    35 	 0.13736 	 0.00493 	 MISS
    2 	    36 	 0.04010 	 0.00689 	 m..s
    6 	    37 	 0.04126 	 0.00700 	 m..s
   11 	    38 	 0.04971 	 0.01072 	 m..s
   15 	    39 	 0.05197 	 0.01080 	 m..s
    9 	    40 	 0.04224 	 0.01271 	 ~...
    5 	    41 	 0.04124 	 0.02019 	 ~...
   18 	    42 	 0.05349 	 0.02142 	 m..s
   30 	    43 	 0.07992 	 0.02264 	 m..s
   25 	    44 	 0.07522 	 0.02330 	 m..s
   22 	    45 	 0.06734 	 0.02423 	 m..s
   24 	    46 	 0.07393 	 0.02572 	 m..s
   21 	    47 	 0.06251 	 0.02655 	 m..s
   27 	    48 	 0.07670 	 0.02788 	 m..s
   20 	    49 	 0.06061 	 0.03482 	 ~...
   16 	    50 	 0.05326 	 0.03512 	 ~...
   31 	    51 	 0.08252 	 0.03527 	 m..s
   19 	    52 	 0.05380 	 0.03662 	 ~...
   14 	    53 	 0.05179 	 0.03752 	 ~...
   32 	    54 	 0.08254 	 0.03959 	 m..s
   17 	    55 	 0.05338 	 0.04148 	 ~...
   63 	    56 	 0.10804 	 0.11541 	 ~...
   86 	    57 	 0.14773 	 0.12241 	 ~...
   36 	    58 	 0.08329 	 0.12846 	 m..s
   63 	    59 	 0.10804 	 0.13114 	 ~...
   63 	    60 	 0.10804 	 0.13148 	 ~...
   87 	    61 	 0.15110 	 0.13290 	 ~...
   46 	    62 	 0.08833 	 0.13717 	 m..s
   52 	    63 	 0.09071 	 0.13746 	 m..s
   35 	    64 	 0.08311 	 0.13956 	 m..s
   91 	    65 	 0.16440 	 0.14231 	 ~...
   92 	    66 	 0.16890 	 0.14255 	 ~...
   89 	    67 	 0.15489 	 0.14560 	 ~...
   39 	    68 	 0.08487 	 0.14852 	 m..s
   44 	    69 	 0.08764 	 0.14909 	 m..s
   97 	    70 	 0.17806 	 0.15188 	 ~...
   90 	    71 	 0.16430 	 0.15206 	 ~...
   88 	    72 	 0.15260 	 0.15274 	 ~...
   63 	    73 	 0.10804 	 0.15682 	 m..s
   49 	    74 	 0.08866 	 0.15881 	 m..s
   29 	    75 	 0.07856 	 0.15918 	 m..s
   62 	    76 	 0.09967 	 0.15954 	 m..s
   63 	    77 	 0.10804 	 0.15971 	 m..s
   51 	    78 	 0.08941 	 0.16065 	 m..s
   40 	    79 	 0.08502 	 0.16139 	 m..s
   61 	    80 	 0.09882 	 0.16538 	 m..s
   28 	    81 	 0.07757 	 0.16607 	 m..s
   63 	    82 	 0.10804 	 0.16632 	 m..s
   56 	    83 	 0.09292 	 0.16633 	 m..s
   45 	    84 	 0.08791 	 0.16812 	 m..s
   42 	    85 	 0.08635 	 0.17114 	 m..s
   57 	    86 	 0.09408 	 0.17181 	 m..s
  101 	    87 	 0.19452 	 0.17211 	 ~...
  100 	    88 	 0.18898 	 0.17444 	 ~...
   93 	    89 	 0.17133 	 0.17536 	 ~...
   58 	    90 	 0.09541 	 0.17544 	 m..s
   55 	    91 	 0.09270 	 0.17550 	 m..s
   50 	    92 	 0.08936 	 0.17727 	 m..s
   95 	    93 	 0.17641 	 0.17804 	 ~...
   99 	    94 	 0.18388 	 0.17881 	 ~...
   81 	    95 	 0.11553 	 0.17898 	 m..s
  108 	    96 	 0.22895 	 0.17958 	 m..s
   54 	    97 	 0.09102 	 0.17973 	 m..s
   78 	    98 	 0.10890 	 0.17989 	 m..s
   94 	    99 	 0.17471 	 0.18087 	 ~...
  105 	   100 	 0.21912 	 0.18107 	 m..s
   96 	   101 	 0.17798 	 0.18566 	 ~...
   82 	   102 	 0.11613 	 0.19259 	 m..s
  104 	   103 	 0.20962 	 0.20554 	 ~...
   98 	   104 	 0.18100 	 0.20601 	 ~...
  102 	   105 	 0.19836 	 0.20816 	 ~...
  112 	   106 	 0.23600 	 0.21020 	 ~...
  109 	   107 	 0.22982 	 0.21060 	 ~...
  103 	   108 	 0.20145 	 0.22583 	 ~...
  115 	   109 	 0.23937 	 0.23410 	 ~...
  110 	   110 	 0.23134 	 0.23645 	 ~...
  107 	   111 	 0.22486 	 0.24193 	 ~...
  114 	   112 	 0.23843 	 0.24214 	 ~...
  106 	   113 	 0.22317 	 0.24363 	 ~...
  113 	   114 	 0.23828 	 0.25210 	 ~...
  111 	   115 	 0.23588 	 0.25607 	 ~...
  120 	   116 	 0.30747 	 0.25993 	 m..s
  116 	   117 	 0.28719 	 0.26045 	 ~...
  119 	   118 	 0.29806 	 0.26202 	 m..s
  117 	   119 	 0.28935 	 0.26222 	 ~...
  118 	   120 	 0.29598 	 0.26816 	 ~...
==========================================
r_mrr = 0.7626132369041443
r2_mrr = 0.5453755259513855
spearmanr_mrr@5 = 0.9570485949516296
spearmanr_mrr@10 = 0.896006166934967
spearmanr_mrr@50 = 0.953697681427002
spearmanr_mrr@100 = 0.8544870615005493
spearmanr_mrr@All = 0.8910171985626221
==========================================
test time: 0.55
Done Testing dataset CoDExSmall
total time taken: 223.89431166648865
training time taken: 213.2411231994629
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7626)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5454)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9570)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.8960)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9537)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8545)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8910)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.368961336663233}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 3827817496515613
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [226, 23, 1069, 184, 279, 627, 408, 409, 767, 595, 272, 106, 421, 938, 362, 207, 629, 598, 888, 415, 55, 12, 1080, 430, 585, 677, 248, 703, 200, 165, 856, 394, 1133, 539, 616, 914, 471, 127, 423, 407, 607, 203, 138, 543, 624, 974, 35, 779, 315, 561, 164, 747, 643, 1102, 71, 834, 395, 630, 224, 366, 281, 962, 67, 403, 546, 956, 817, 797, 680, 1023, 209, 28, 457, 427, 850, 617, 660, 141, 249, 709, 835, 126, 13, 620, 1091, 965, 515, 945, 379, 845, 812, 61, 582, 1022, 656, 462, 855, 72, 241, 433, 724, 406, 913, 580, 1114, 22, 45, 1166, 800, 1040, 339, 1213, 311, 1188, 280, 1161, 712, 973, 182, 1032, 548]
valid_ids (0): []
train_ids (1094): [128, 720, 640, 42, 1155, 260, 1158, 46, 443, 648, 437, 678, 794, 981, 1143, 826, 1063, 1202, 1013, 1151, 189, 438, 1025, 301, 450, 979, 823, 1167, 635, 190, 150, 988, 1203, 738, 731, 51, 94, 244, 198, 560, 771, 840, 626, 987, 1092, 773, 518, 488, 901, 912, 802, 1019, 1137, 98, 199, 684, 966, 692, 204, 59, 179, 833, 116, 392, 376, 1131, 1126, 1189, 468, 781, 950, 711, 188, 69, 15, 1109, 612, 1101, 628, 1116, 65, 266, 1209, 536, 553, 594, 155, 556, 1008, 633, 479, 131, 1174, 934, 611, 578, 830, 389, 192, 780, 355, 228, 1185, 107, 665, 646, 565, 424, 551, 1169, 976, 986, 191, 1181, 60, 236, 879, 1160, 1094, 1165, 210, 841, 849, 1117, 1083, 717, 208, 1049, 157, 809, 1175, 89, 744, 168, 805, 1195, 385, 922, 118, 1200, 1146, 202, 66, 520, 801, 730, 186, 796, 1029, 253, 920, 1003, 225, 445, 25, 785, 926, 872, 26, 1010, 770, 1118, 664, 758, 234, 881, 977, 32, 975, 1128, 402, 152, 31, 848, 832, 348, 869, 610, 492, 143, 896, 533, 196, 337, 153, 1211, 768, 718, 64, 156, 482, 854, 217, 992, 563, 1072, 372, 916, 282, 729, 119, 255, 755, 857, 306, 144, 527, 1140, 838, 490, 81, 398, 417, 1021, 792, 439, 762, 302, 336, 714, 657, 695, 806, 935, 275, 122, 899, 478, 382, 844, 432, 137, 257, 748, 774, 605, 606, 903, 171, 230, 690, 1179, 1110, 444, 795, 276, 428, 353, 685, 504, 526, 906, 475, 175, 27, 231, 290, 1144, 333, 882, 622, 1024, 516, 220, 951, 619, 1205, 426, 608, 1051, 791, 908, 895, 694, 4, 52, 365, 623, 486, 215, 24, 592, 753, 1170, 999, 318, 1104, 851, 383, 1044, 63, 1000, 335, 1120, 875, 1136, 1097, 452, 419, 638, 1016, 9, 980, 810, 710, 998, 615, 1050, 238, 573, 500, 82, 943, 289, 173, 741, 263, 864, 265, 575, 1046, 530, 581, 227, 245, 989, 347, 380, 877, 522, 1129, 1027, 216, 708, 576, 632, 567, 890, 268, 158, 867, 1065, 1015, 698, 1036, 101, 1115, 892, 787, 219, 944, 736, 964, 324, 146, 113, 212, 545, 273, 596, 982, 30, 889, 507, 843, 497, 505, 572, 878, 859, 474, 679, 1178, 43, 672, 370, 1020, 374, 411, 993, 343, 528, 655, 557, 295, 47, 930, 1154, 498, 40, 270, 682, 509, 1043, 304, 1100, 501, 559, 529, 1005, 50, 776, 961, 955, 1098, 931, 317, 1111, 136, 900, 727, 940, 274, 1041, 466, 880, 681, 29, 142, 719, 166, 658, 554, 434, 360, 740, 1075, 871, 1055, 994, 341, 897, 1150, 702, 465, 405, 669, 7, 705, 183, 893, 163, 332, 907, 316, 176, 853, 455, 958, 707, 991, 499, 1196, 733, 358, 1107, 160, 614, 1142, 48, 821, 641, 804, 418, 1141, 870, 330, 1124, 997, 667, 1152, 858, 798, 932, 1033, 891, 1085, 1047, 177, 484, 344, 722, 86, 11, 1009, 267, 53, 1077, 873, 162, 1156, 754, 827, 359, 493, 93, 378, 923, 625, 1157, 194, 464, 299, 750, 847, 777, 1093, 898, 503, 735, 259, 1186, 250, 495, 233, 1004, 591, 33, 613, 132, 1037, 454, 320, 531, 334, 278, 577, 808, 721, 1076, 73, 970, 953, 621, 1028, 745, 579, 1187, 960, 58, 18, 1180, 167, 201, 1149, 675, 397, 583, 76, 697, 769, 1011, 996, 502, 728, 1132, 972, 886, 1201, 524, 558, 569, 704, 737, 534, 0, 477, 414, 393, 37, 910, 793, 125, 822, 1038, 1057, 815, 1064, 453, 1067, 839, 661, 751, 1066, 663, 757, 968, 590, 650, 924, 340, 480, 723, 642, 1190, 1173, 725, 352, 1212, 799, 766, 170, 458, 990, 361, 151, 784, 172, 456, 1054, 699, 921, 825, 284, 1007, 446, 121, 863, 1134, 514, 19, 749, 952, 555, 243, 1127, 1030, 743, 1035, 283, 928, 390, 1122, 83, 496, 1073, 399, 510, 541, 1139, 49, 1006, 1042, 396, 84, 90, 254, 102, 1171, 92, 1135, 44, 971, 235, 742, 463, 1058, 140, 451, 20, 1084, 447, 364, 287, 788, 782, 659, 297, 885, 1199, 852, 133, 1145, 865, 485, 519, 1061, 1113, 95, 936, 846, 862, 367, 139, 746, 91, 647, 87, 726, 688, 21, 75, 917, 1026, 652, 1088, 1108, 760, 149, 947, 193, 772, 237, 691, 288, 1159, 902, 487, 820, 1089, 384, 1214, 449, 1204, 512, 161, 756, 828, 1130, 294, 459, 74, 425, 300, 586, 247, 790, 108, 568, 310, 670, 978, 36, 1034, 1182, 134, 588, 811, 1078, 124, 338, 494, 242, 508, 148, 765, 874, 346, 412, 1060, 214, 431, 544, 313, 861, 481, 1002, 589, 1177, 593, 473, 807, 195, 542, 789, 483, 400, 537, 5, 696, 1172, 54, 1017, 205, 375, 70, 314, 959, 1138, 1192, 229, 1095, 1070, 130, 783, 734, 239, 371, 683, 666, 538, 387, 946, 829, 887, 328, 420, 68, 422, 511, 1031, 868, 949, 566, 716, 99, 256, 57, 1062, 178, 842, 1087, 662, 114, 356, 814, 476, 246, 540, 824, 645, 429, 600, 1184, 221, 251, 836, 327, 331, 644, 404, 700, 1059, 564, 547, 584, 187, 112, 1208, 469, 277, 1148, 948, 876, 676, 1106, 1045, 915, 321, 639, 884, 1099, 883, 1123, 919, 816, 574, 96, 837, 587, 213, 441, 345, 1053, 103, 38, 1163, 1194, 115, 232, 1183, 467, 303, 312, 671, 100, 1048, 894, 14, 174, 941, 41, 686, 440, 381, 752, 206, 8, 298, 147, 985, 286, 363, 1210, 562, 532, 604, 957, 135, 775, 223, 88, 34, 939, 759, 552, 602, 351, 933, 995, 293, 1071, 307, 62, 984, 85, 687, 523, 104, 571, 342, 309, 368, 601, 109, 927, 218, 1018, 1103, 6, 386, 435, 1090, 1207, 350, 1112, 525, 271, 1039, 597, 145, 818, 325, 1164, 786, 1074, 354, 929, 261, 159, 111, 963, 105, 517, 636, 258, 262, 550, 79, 911, 909, 472, 78, 706, 1056, 388, 654, 56, 715, 269, 17, 461, 983, 442, 819, 673, 10, 954, 240, 1125, 308, 319, 506, 180, 570, 739, 129, 860, 296, 436, 1081, 413, 918, 535, 323, 1193, 1147, 448, 460, 491, 1206, 373, 377, 470, 693, 329, 1121, 1012, 80, 181, 1176, 764, 1153, 120, 322, 1068, 401, 264, 649, 1191, 357, 222, 937, 369, 252, 904, 77, 185, 110, 1197, 813, 599, 778, 1014, 1001, 609, 631, 154, 618, 549, 1096, 291, 1052, 803, 211, 1, 1198, 701, 653, 761, 117, 489, 1119, 831, 39, 416, 967, 169, 637, 2, 521, 942, 763, 603, 925, 285, 349, 905, 97, 326, 634, 732, 16, 1105, 410, 391, 668, 713, 1086, 651, 1168, 123, 689, 674, 969, 1162, 866, 1082, 197, 1079, 305, 292, 3, 513]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1767226810537736
the save name prefix for this run is:  chkpt-ID_1767226810537736_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.498 +- 0.005
mrr vals (pred, true): 0.001, 0.260
batch losses (mrrl, rdl): 0.0, 0.0045190649

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 266
rank avg (pred): 0.045 +- 0.037
mrr vals (pred, true): 0.302, 0.265
batch losses (mrrl, rdl): 0.0, 2.3336e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 505
rank avg (pred): 0.178 +- 0.145
mrr vals (pred, true): 0.310, 0.016
batch losses (mrrl, rdl): 0.0, 2.36958e-05

Epoch over!
epoch time: 14.681

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.199 +- 0.169
mrr vals (pred, true): 0.333, 0.019
batch losses (mrrl, rdl): 0.0, 8.663e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 185
rank avg (pred): 0.270 +- 0.228
mrr vals (pred, true): 0.335, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005343391

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1191
rank avg (pred): 0.303 +- 0.278
mrr vals (pred, true): 0.362, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003163703

Epoch over!
epoch time: 12.795

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 792
rank avg (pred): 0.234 +- 0.224
mrr vals (pred, true): 0.382, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007589907

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 545
rank avg (pred): 0.156 +- 0.144
mrr vals (pred, true): 0.387, 0.035
batch losses (mrrl, rdl): 0.0, 1.16929e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 110
rank avg (pred): 0.226 +- 0.219
mrr vals (pred, true): 0.417, 0.156
batch losses (mrrl, rdl): 0.0, 0.0004085406

Epoch over!
epoch time: 13.056

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.233 +- 0.229
mrr vals (pred, true): 0.423, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007084792

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1017
rank avg (pred): 0.253 +- 0.247
mrr vals (pred, true): 0.406, 0.177
batch losses (mrrl, rdl): 0.0, 0.0006824095

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 262
rank avg (pred): 0.042 +- 0.043
mrr vals (pred, true): 0.464, 0.243
batch losses (mrrl, rdl): 0.0, 1.5332e-06

Epoch over!
epoch time: 13.366

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 926
rank avg (pred): 0.268 +- 0.256
mrr vals (pred, true): 0.343, 0.029
batch losses (mrrl, rdl): 0.0, 0.0001889721

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 859
rank avg (pred): 0.270 +- 0.256
mrr vals (pred, true): 0.374, 0.165
batch losses (mrrl, rdl): 0.0, 0.000792894

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 490
rank avg (pred): 0.190 +- 0.186
mrr vals (pred, true): 0.382, 0.016
batch losses (mrrl, rdl): 0.0, 4.8426e-06

Epoch over!
epoch time: 14.176

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.260 +- 0.257
mrr vals (pred, true): 0.385, 0.114
batch losses (mrrl, rdl): 0.735740304, 0.0005309486

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 132
rank avg (pred): 0.270 +- 0.168
mrr vals (pred, true): 0.112, 0.120
batch losses (mrrl, rdl): 0.0006941488, 0.0005491714

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 636
rank avg (pred): 0.370 +- 0.115
mrr vals (pred, true): 0.043, 0.007
batch losses (mrrl, rdl): 0.000561426, 8.08299e-05

Epoch over!
epoch time: 12.859

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 134
rank avg (pred): 0.217 +- 0.149
mrr vals (pred, true): 0.096, 0.180
batch losses (mrrl, rdl): 0.0706720054, 0.0003701511

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1060
rank avg (pred): 0.005 +- 0.004
mrr vals (pred, true): 0.287, 0.262
batch losses (mrrl, rdl): 0.0061885086, 2.66789e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1071
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.240, 0.196
batch losses (mrrl, rdl): 0.0194648616, 7.8391e-05

Epoch over!
epoch time: 12.585

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 869
rank avg (pred): 0.224 +- 0.151
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.0375927016, 0.0011146988

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.240, 0.252
batch losses (mrrl, rdl): 0.0015751027, 2.98629e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 130
rank avg (pred): 0.302 +- 0.152
mrr vals (pred, true): 0.090, 0.143
batch losses (mrrl, rdl): 0.0279472861, 0.0008975561

Epoch over!
epoch time: 13.009

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 155
rank avg (pred): 0.136 +- 0.088
mrr vals (pred, true): 0.114, 0.173
batch losses (mrrl, rdl): 0.0352295302, 5.21534e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 25
rank avg (pred): 0.013 +- 0.008
mrr vals (pred, true): 0.195, 0.210
batch losses (mrrl, rdl): 0.0019773487, 2.31739e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 511
rank avg (pred): 0.356 +- 0.101
mrr vals (pred, true): 0.048, 0.016
batch losses (mrrl, rdl): 5.11514e-05, 0.0004920918

Epoch over!
epoch time: 12.581

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 157
rank avg (pred): 0.299 +- 0.149
mrr vals (pred, true): 0.090, 0.149
batch losses (mrrl, rdl): 0.0355505534, 0.0008554418

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 398
rank avg (pred): 0.118 +- 0.074
mrr vals (pred, true): 0.121, 0.193
batch losses (mrrl, rdl): 0.0507805087, 2.26416e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 400
rank avg (pred): 0.273 +- 0.154
mrr vals (pred, true): 0.103, 0.171
batch losses (mrrl, rdl): 0.0456767939, 0.0006224774

Epoch over!
epoch time: 13.059

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.053 +- 0.034
mrr vals (pred, true): 0.147, 0.111
batch losses (mrrl, rdl): 0.0128839463, 3.847e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 21
rank avg (pred): 0.017 +- 0.011
mrr vals (pred, true): 0.193, 0.168
batch losses (mrrl, rdl): 0.0063236812, 3.38969e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 369
rank avg (pred): 0.292 +- 0.148
mrr vals (pred, true): 0.095, 0.149
batch losses (mrrl, rdl): 0.0288465992, 0.0006505583

Epoch over!
epoch time: 13.843

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 983
rank avg (pred): 0.019 +- 0.013
mrr vals (pred, true): 0.203, 0.206
batch losses (mrrl, rdl): 0.0001080634, 1.8802e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.307 +- 0.140
mrr vals (pred, true): 0.088, 0.130
batch losses (mrrl, rdl): 0.0174809657, 0.0006831382

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 402
rank avg (pred): 0.285 +- 0.145
mrr vals (pred, true): 0.096, 0.137
batch losses (mrrl, rdl): 0.0167921353, 0.0005316117

Epoch over!
epoch time: 13.957

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 507
rank avg (pred): 0.344 +- 0.097
mrr vals (pred, true): 0.054, 0.013
batch losses (mrrl, rdl): 0.0001513732, 0.0003256542

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 832
rank avg (pred): 0.079 +- 0.052
mrr vals (pred, true): 0.140, 0.153
batch losses (mrrl, rdl): 0.001542845, 4.6185e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 941
rank avg (pred): 0.285 +- 0.150
mrr vals (pred, true): 0.106, 0.181
batch losses (mrrl, rdl): 0.0572928637, 0.0009058145

Epoch over!
epoch time: 14.955

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 623
rank avg (pred): 0.342 +- 0.102
mrr vals (pred, true): 0.051, 0.020
batch losses (mrrl, rdl): 1.9018e-05, 9.68372e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 733
rank avg (pred): 0.140 +- 0.094
mrr vals (pred, true): 0.135, 0.154
batch losses (mrrl, rdl): 0.003470635, 0.0001706454

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1131
rank avg (pred): 0.201 +- 0.130
mrr vals (pred, true): 0.123, 0.004
batch losses (mrrl, rdl): 0.0535420515, 0.0012819521

Epoch over!
epoch time: 13.103

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 745
rank avg (pred): 0.074 +- 0.060
mrr vals (pred, true): 0.166, 0.193
batch losses (mrrl, rdl): 0.0071395217, 2.67283e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 211
rank avg (pred): 0.283 +- 0.144
mrr vals (pred, true): 0.108, 0.004
batch losses (mrrl, rdl): 0.0331534222, 0.0006512846

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 592
rank avg (pred): 0.341 +- 0.080
mrr vals (pred, true): 0.040, 0.010
batch losses (mrrl, rdl): 0.0009240175, 7.58079e-05

Epoch over!
epoch time: 13.735

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.296 +- 0.132
mrr vals (pred, true): 0.094, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.05193 	 0.00307 	 m..s
    7 	     1 	 0.04844 	 0.00347 	 m..s
   25 	     2 	 0.05296 	 0.00349 	 m..s
   16 	     3 	 0.05179 	 0.00353 	 m..s
   43 	     4 	 0.09794 	 0.00353 	 m..s
    4 	     5 	 0.04543 	 0.00358 	 m..s
   80 	     6 	 0.11855 	 0.00360 	 MISS
   23 	     7 	 0.05273 	 0.00364 	 m..s
   50 	     8 	 0.10491 	 0.00373 	 MISS
   55 	     9 	 0.10833 	 0.00376 	 MISS
   82 	    10 	 0.11881 	 0.00377 	 MISS
   58 	    11 	 0.11125 	 0.00380 	 MISS
   11 	    12 	 0.05005 	 0.00381 	 m..s
   24 	    13 	 0.05275 	 0.00383 	 m..s
   51 	    14 	 0.10571 	 0.00384 	 MISS
   42 	    15 	 0.09440 	 0.00385 	 m..s
    2 	    16 	 0.04477 	 0.00387 	 m..s
   87 	    17 	 0.13223 	 0.00388 	 MISS
   46 	    18 	 0.10090 	 0.00390 	 m..s
   58 	    19 	 0.11125 	 0.00394 	 MISS
   39 	    20 	 0.09274 	 0.00400 	 m..s
   79 	    21 	 0.11819 	 0.00405 	 MISS
   41 	    22 	 0.09309 	 0.00406 	 m..s
   57 	    23 	 0.11068 	 0.00413 	 MISS
   22 	    24 	 0.05266 	 0.00413 	 m..s
   91 	    25 	 0.13465 	 0.00415 	 MISS
   83 	    26 	 0.11897 	 0.00417 	 MISS
   58 	    27 	 0.11125 	 0.00420 	 MISS
   84 	    28 	 0.11906 	 0.00422 	 MISS
   89 	    29 	 0.13296 	 0.00422 	 MISS
   52 	    30 	 0.10577 	 0.00422 	 MISS
   40 	    31 	 0.09303 	 0.00422 	 m..s
   34 	    32 	 0.08181 	 0.00422 	 m..s
   58 	    33 	 0.11125 	 0.00427 	 MISS
   58 	    34 	 0.11125 	 0.00427 	 MISS
   75 	    35 	 0.11491 	 0.00437 	 MISS
   88 	    36 	 0.13270 	 0.00443 	 MISS
   54 	    37 	 0.10714 	 0.00451 	 MISS
   45 	    38 	 0.09921 	 0.00451 	 m..s
   56 	    39 	 0.11020 	 0.00457 	 MISS
   90 	    40 	 0.13461 	 0.00460 	 MISS
   74 	    41 	 0.11460 	 0.00465 	 MISS
   58 	    42 	 0.11125 	 0.00486 	 MISS
   48 	    43 	 0.10275 	 0.00498 	 m..s
   35 	    44 	 0.08850 	 0.00499 	 m..s
    0 	    45 	 0.04012 	 0.00668 	 m..s
   13 	    46 	 0.05162 	 0.00672 	 m..s
   19 	    47 	 0.05218 	 0.00700 	 m..s
   15 	    48 	 0.05176 	 0.00741 	 m..s
    1 	    49 	 0.04402 	 0.00822 	 m..s
   14 	    50 	 0.05165 	 0.00898 	 m..s
   12 	    51 	 0.05005 	 0.00973 	 m..s
    6 	    52 	 0.04838 	 0.01088 	 m..s
    8 	    53 	 0.04844 	 0.01188 	 m..s
    3 	    54 	 0.04497 	 0.01188 	 m..s
   21 	    55 	 0.05259 	 0.01230 	 m..s
    9 	    56 	 0.04866 	 0.01230 	 m..s
   20 	    57 	 0.05237 	 0.01274 	 m..s
   26 	    58 	 0.05328 	 0.01763 	 m..s
   29 	    59 	 0.05498 	 0.01790 	 m..s
   27 	    60 	 0.05399 	 0.01938 	 m..s
   18 	    61 	 0.05215 	 0.02173 	 m..s
   10 	    62 	 0.04868 	 0.02257 	 ~...
    5 	    63 	 0.04813 	 0.02328 	 ~...
   31 	    64 	 0.05749 	 0.03662 	 ~...
   30 	    65 	 0.05714 	 0.03764 	 ~...
   28 	    66 	 0.05463 	 0.03904 	 ~...
   58 	    67 	 0.11125 	 0.04802 	 m..s
   32 	    68 	 0.07941 	 0.09814 	 ~...
   33 	    69 	 0.08163 	 0.10076 	 ~...
   96 	    70 	 0.17005 	 0.11238 	 m..s
   36 	    71 	 0.08903 	 0.11951 	 m..s
   93 	    72 	 0.13868 	 0.12023 	 ~...
   38 	    73 	 0.09143 	 0.12266 	 m..s
   94 	    74 	 0.14537 	 0.13290 	 ~...
   44 	    75 	 0.09814 	 0.13539 	 m..s
   37 	    76 	 0.09108 	 0.13770 	 m..s
   99 	    77 	 0.17898 	 0.14255 	 m..s
   49 	    78 	 0.10304 	 0.14286 	 m..s
   58 	    79 	 0.11125 	 0.14854 	 m..s
   58 	    80 	 0.11125 	 0.15094 	 m..s
   53 	    81 	 0.10621 	 0.15171 	 m..s
   58 	    82 	 0.11125 	 0.15261 	 m..s
   58 	    83 	 0.11125 	 0.15816 	 m..s
   95 	    84 	 0.16231 	 0.15914 	 ~...
   47 	    85 	 0.10208 	 0.16042 	 m..s
   58 	    86 	 0.11125 	 0.16538 	 m..s
   58 	    87 	 0.11125 	 0.16674 	 m..s
   58 	    88 	 0.11125 	 0.17176 	 m..s
  102 	    89 	 0.19866 	 0.17242 	 ~...
   58 	    90 	 0.11125 	 0.17298 	 m..s
   77 	    91 	 0.11623 	 0.17519 	 m..s
   73 	    92 	 0.11143 	 0.17544 	 m..s
   76 	    93 	 0.11541 	 0.17628 	 m..s
  103 	    94 	 0.19869 	 0.17869 	 ~...
  101 	    95 	 0.19253 	 0.18087 	 ~...
  105 	    96 	 0.19916 	 0.18107 	 ~...
   78 	    97 	 0.11678 	 0.18276 	 m..s
   97 	    98 	 0.17239 	 0.18518 	 ~...
  109 	    99 	 0.21692 	 0.18788 	 ~...
  112 	   100 	 0.22184 	 0.18871 	 m..s
   85 	   101 	 0.11909 	 0.19017 	 m..s
   98 	   102 	 0.17341 	 0.19182 	 ~...
  104 	   103 	 0.19914 	 0.19254 	 ~...
  100 	   104 	 0.19166 	 0.19379 	 ~...
   81 	   105 	 0.11878 	 0.19925 	 m..s
   86 	   106 	 0.11943 	 0.20043 	 m..s
  110 	   107 	 0.21829 	 0.20114 	 ~...
  106 	   108 	 0.20103 	 0.20124 	 ~...
  108 	   109 	 0.20395 	 0.20509 	 ~...
  113 	   110 	 0.22292 	 0.20636 	 ~...
  111 	   111 	 0.21831 	 0.20723 	 ~...
  119 	   112 	 0.24001 	 0.21220 	 ~...
  115 	   113 	 0.22684 	 0.21680 	 ~...
   92 	   114 	 0.13724 	 0.21696 	 m..s
  107 	   115 	 0.20300 	 0.22076 	 ~...
  114 	   116 	 0.22632 	 0.22900 	 ~...
  118 	   117 	 0.23238 	 0.23323 	 ~...
  116 	   118 	 0.22846 	 0.23903 	 ~...
  117 	   119 	 0.23009 	 0.24193 	 ~...
  120 	   120 	 0.28623 	 0.26979 	 ~...
==========================================
r_mrr = 0.755511999130249
r2_mrr = 0.41626816987991333
spearmanr_mrr@5 = 0.9846091866493225
spearmanr_mrr@10 = 0.9261086583137512
spearmanr_mrr@50 = 0.9495630264282227
spearmanr_mrr@100 = 0.8639143705368042
spearmanr_mrr@All = 0.885302722454071
==========================================
test time: 0.442
Done Testing dataset CoDExSmall
total time taken: 213.3030834197998
training time taken: 202.27683973312378
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7555)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4163)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9846)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9261)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9496)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8639)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8853)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.2775064503775866}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 4507206163146030
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [154, 1135, 347, 603, 1085, 1070, 295, 322, 944, 70, 913, 1035, 261, 956, 905, 1061, 58, 1186, 147, 827, 834, 179, 235, 458, 724, 1138, 768, 5, 38, 41, 1031, 208, 921, 90, 614, 506, 1117, 927, 938, 126, 205, 805, 312, 568, 107, 611, 939, 1154, 895, 1043, 1174, 1143, 527, 277, 909, 888, 36, 176, 986, 908, 541, 863, 646, 222, 64, 296, 139, 902, 947, 280, 532, 875, 173, 548, 362, 401, 729, 994, 1180, 448, 82, 612, 537, 122, 831, 764, 1083, 457, 288, 1013, 371, 644, 454, 728, 639, 213, 406, 412, 186, 622, 198, 211, 273, 57, 1003, 376, 855, 730, 326, 110, 967, 1077, 364, 459, 15, 502, 742, 890, 44, 1093, 543]
valid_ids (0): []
train_ids (1094): [142, 521, 1209, 499, 311, 279, 1184, 387, 814, 571, 1152, 703, 830, 467, 1165, 1214, 677, 252, 1158, 1131, 940, 105, 1028, 716, 1212, 982, 175, 518, 904, 577, 554, 14, 931, 225, 1058, 970, 9, 508, 236, 101, 451, 823, 610, 534, 748, 444, 717, 722, 32, 625, 52, 941, 873, 293, 820, 950, 1029, 1102, 455, 690, 1032, 1192, 1056, 328, 196, 664, 308, 1100, 493, 148, 576, 617, 75, 708, 230, 1161, 1006, 1210, 172, 869, 1170, 860, 245, 1207, 170, 1183, 813, 1189, 272, 128, 601, 849, 751, 232, 977, 866, 427, 1022, 268, 410, 247, 264, 450, 976, 885, 765, 251, 200, 501, 899, 123, 1160, 609, 659, 250, 95, 758, 795, 192, 1046, 290, 197, 828, 204, 332, 1109, 1016, 178, 484, 528, 442, 590, 1082, 903, 1203, 190, 431, 341, 731, 707, 565, 1208, 689, 642, 74, 524, 536, 960, 1, 18, 556, 12, 157, 253, 242, 1057, 384, 784, 98, 763, 535, 1126, 740, 397, 954, 645, 803, 483, 519, 476, 637, 1125, 1071, 934, 386, 833, 604, 846, 928, 632, 462, 901, 1063, 530, 94, 523, 88, 415, 775, 700, 973, 145, 948, 854, 265, 1167, 259, 127, 394, 143, 929, 21, 1145, 370, 393, 714, 550, 1147, 692, 355, 24, 298, 369, 923, 1047, 980, 1037, 491, 656, 1111, 1087, 61, 563, 267, 301, 806, 788, 162, 132, 774, 494, 755, 158, 999, 136, 206, 497, 1086, 1188, 801, 270, 479, 434, 782, 1053, 626, 783, 516, 238, 353, 1033, 171, 607, 338, 1042, 413, 500, 26, 68, 691, 77, 574, 544, 1140, 972, 195, 343, 822, 379, 920, 702, 630, 678, 1079, 942, 667, 358, 414, 1023, 636, 137, 80, 35, 354, 1201, 1144, 465, 850, 1090, 76, 837, 19, 1012, 449, 683, 351, 817, 649, 1052, 786, 693, 861, 1055, 865, 615, 1009, 69, 121, 481, 660, 134, 891, 1097, 769, 108, 505, 349, 894, 666, 557, 29, 1114, 221, 741, 963, 681, 496, 997, 825, 50, 53, 686, 529, 749, 1198, 174, 435, 234, 229, 7, 447, 581, 1081, 373, 320, 1091, 1162, 463, 658, 1094, 314, 1044, 1089, 971, 968, 1115, 503, 260, 654, 486, 840, 405, 1092, 1171, 974, 853, 628, 721, 1020, 780, 572, 309, 949, 859, 487, 1132, 672, 969, 843, 1200, 372, 150, 1168, 633, 1026, 348, 794, 73, 851, 857, 93, 1015, 662, 48, 597, 228, 911, 470, 687, 793, 163, 804, 83, 791, 1064, 709, 1206, 657, 648, 11, 1112, 1182, 433, 640, 1080, 701, 958, 1129, 278, 325, 870, 591, 684, 821, 383, 37, 1088, 34, 634, 316, 89, 498, 233, 871, 1039, 785, 1098, 424, 318, 220, 471, 81, 452, 824, 66, 907, 138, 995, 1050, 771, 111, 1001, 618, 647, 305, 428, 743, 1191, 365, 43, 395, 513, 438, 984, 161, 224, 345, 336, 898, 797, 990, 20, 104, 867, 943, 97, 883, 416, 564, 1038, 274, 167, 669, 916, 461, 623, 886, 436, 62, 256, 992, 605, 1155, 218, 239, 738, 51, 808, 202, 1149, 340, 1169, 85, 443, 131, 1025, 1068, 408, 652, 212, 1177, 1196, 331, 868, 919, 720, 248, 893, 72, 514, 177, 1076, 464, 802, 578, 661, 1008, 439, 790, 249, 217, 324, 596, 141, 575, 877, 1119, 246, 352, 400, 533, 297, 469, 812, 600, 113, 112, 522, 342, 650, 796, 411, 133, 219, 747, 130, 836, 776, 896, 333, 1051, 159, 166, 1130, 165, 711, 25, 1096, 598, 619, 1122, 746, 673, 965, 359, 446, 432, 892, 712, 935, 92, 1095, 350, 900, 629, 975, 194, 725, 392, 933, 989, 588, 109, 207, 185, 106, 385, 1069, 339, 398, 153, 862, 787, 382, 237, 835, 86, 540, 55, 1193, 1156, 573, 924, 203, 887, 697, 932, 732, 983, 872, 118, 284, 509, 847, 183, 13, 715, 1141, 695, 191, 389, 189, 560, 63, 811, 1116, 705, 42, 1190, 71, 149, 344, 979, 1150, 396, 30, 582, 889, 651, 511, 897, 1127, 1118, 429, 704, 445, 276, 1073, 674, 1005, 54, 262, 1011, 579, 809, 275, 475, 215, 156, 0, 1204, 880, 1041, 832, 841, 1197, 180, 1139, 1178, 488, 1120, 569, 146, 1175, 103, 752, 282, 754, 378, 45, 546, 271, 733, 1136, 1078, 594, 285, 1065, 987, 226, 879, 363, 419, 1040, 1105, 961, 243, 937, 299, 584, 231, 266, 884, 621, 981, 829, 1142, 766, 675, 99, 140, 417, 335, 144, 482, 466, 922, 957, 1000, 781, 792, 1213, 329, 1036, 1187, 561, 799, 478, 33, 539, 214, 407, 485, 551, 1004, 441, 47, 549, 10, 966, 426, 915, 882, 1179, 100, 366, 679, 1014, 437, 28, 374, 303, 1007, 515, 504, 761, 670, 507, 120, 744, 1159, 404, 468, 583, 84, 269, 114, 91, 193, 490, 79, 294, 472, 188, 606, 1067, 635, 49, 735, 914, 777, 753, 129, 17, 845, 826, 368, 698, 78, 1019, 418, 1017, 1099, 718, 608, 918, 1134, 525, 27, 223, 480, 263, 912, 489, 283, 1173, 473, 858, 337, 425, 542, 545, 517, 1137, 570, 964, 770, 737, 388, 291, 538, 696, 1172, 377, 688, 955, 685, 710, 682, 1164, 599, 1034, 254, 135, 653, 800, 310, 750, 559, 59, 878, 1048, 124, 1066, 1059, 510, 1124, 655, 164, 119, 380, 403, 399, 643, 65, 255, 391, 216, 1195, 1010, 1133, 474, 638, 988, 1166, 313, 456, 56, 745, 567, 1146, 586, 665, 819, 381, 874, 713, 760, 258, 1027, 789, 210, 402, 60, 1148, 421, 580, 360, 613, 547, 512, 936, 181, 996, 1060, 998, 1074, 945, 520, 1202, 187, 668, 592, 917, 624, 22, 553, 962, 818, 566, 453, 1045, 993, 757, 589, 807, 767, 4, 1084, 327, 201, 1104, 1002, 46, 8, 209, 727, 739, 1103, 117, 759, 810, 838, 492, 422, 773, 779, 881, 593, 816, 1018, 306, 680, 953, 951, 723, 676, 959, 925, 125, 169, 281, 1072, 23, 842, 361, 1176, 257, 699, 160, 1121, 420, 319, 526, 330, 844, 155, 357, 115, 641, 1101, 315, 555, 726, 423, 627, 16, 985, 1185, 244, 856, 1199, 1107, 663, 1049, 1153, 798, 102, 1108, 1021, 839, 1123, 1163, 671, 227, 587, 1151, 562, 734, 991, 116, 286, 168, 39, 460, 40, 151, 495, 1211, 430, 1054, 719, 558, 815, 375, 864, 910, 926, 952, 302, 184, 87, 1062, 323, 1113, 346, 1194, 1075, 304, 287, 96, 321, 300, 1205, 356, 762, 585, 1157, 620, 240, 317, 1181, 876, 706, 199, 31, 631, 334, 552, 756, 307, 367, 440, 848, 1128, 2, 182, 3, 1106, 930, 602, 1030, 736, 1024, 152, 772, 595, 616, 390, 852, 6, 978, 531, 409, 292, 241, 946, 1110, 906, 289, 477, 67, 694, 778]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2416535086208089
the save name prefix for this run is:  chkpt-ID_2416535086208089_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 397
rank avg (pred): 0.489 +- 0.003
mrr vals (pred, true): 0.001, 0.171
batch losses (mrrl, rdl): 0.0, 0.0030292876

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.300 +- 0.209
mrr vals (pred, true): 0.046, 0.101
batch losses (mrrl, rdl): 0.0, 0.0006327872

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 243
rank avg (pred): 0.081 +- 0.065
mrr vals (pred, true): 0.326, 0.154
batch losses (mrrl, rdl): 0.0, 7.148e-06

Epoch over!
epoch time: 12.923

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 943
rank avg (pred): 0.305 +- 0.244
mrr vals (pred, true): 0.291, 0.177
batch losses (mrrl, rdl): 0.0, 0.0012401172

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 396
rank avg (pred): 0.276 +- 0.229
mrr vals (pred, true): 0.354, 0.148
batch losses (mrrl, rdl): 0.0, 0.0006094203

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1084
rank avg (pred): 0.316 +- 0.261
mrr vals (pred, true): 0.356, 0.203
batch losses (mrrl, rdl): 0.0, 0.0012808663

Epoch over!
epoch time: 12.954

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 883
rank avg (pred): 0.245 +- 0.211
mrr vals (pred, true): 0.381, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007348304

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.311 +- 0.275
mrr vals (pred, true): 0.395, 0.026
batch losses (mrrl, rdl): 0.0, 3.84557e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 427
rank avg (pred): 0.277 +- 0.240
mrr vals (pred, true): 0.393, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004788198

Epoch over!
epoch time: 12.264

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 331
rank avg (pred): 0.279 +- 0.243
mrr vals (pred, true): 0.398, 0.137
batch losses (mrrl, rdl): 0.0, 0.0004513692

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.297 +- 0.281
mrr vals (pred, true): 0.430, 0.026
batch losses (mrrl, rdl): 0.0, 0.0003427103

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 135
rank avg (pred): 0.272 +- 0.255
mrr vals (pred, true): 0.420, 0.098
batch losses (mrrl, rdl): 0.0, 0.0005320953

Epoch over!
epoch time: 13.832

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 997
rank avg (pred): 0.058 +- 0.054
mrr vals (pred, true): 0.439, 0.262
batch losses (mrrl, rdl): 0.0, 6.6146e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 415
rank avg (pred): 0.263 +- 0.253
mrr vals (pred, true): 0.439, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004929133

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 137
rank avg (pred): 0.258 +- 0.250
mrr vals (pred, true): 0.440, 0.164
batch losses (mrrl, rdl): 0.0, 0.0007147265

Epoch over!
epoch time: 12.753

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 948
rank avg (pred): 0.271 +- 0.261
mrr vals (pred, true): 0.426, 0.004
batch losses (mrrl, rdl): 1.4153575897, 0.0004537861

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 606
rank avg (pred): 0.388 +- 0.229
mrr vals (pred, true): 0.092, 0.008
batch losses (mrrl, rdl): 0.0176242702, 0.0001118394

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 275
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.256, 0.197
batch losses (mrrl, rdl): 0.0350358188, 8.11464e-05

Epoch over!
epoch time: 13.723

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.456 +- 0.182
mrr vals (pred, true): 0.055, 0.004
batch losses (mrrl, rdl): 0.0002984315, 3.7848e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 45
rank avg (pred): 0.040 +- 0.025
mrr vals (pred, true): 0.150, 0.193
batch losses (mrrl, rdl): 0.0182201322, 6.6539e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.057 +- 0.037
mrr vals (pred, true): 0.149, 0.147
batch losses (mrrl, rdl): 3.97164e-05, 1.6248e-06

Epoch over!
epoch time: 13.715

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 711
rank avg (pred): 0.449 +- 0.162
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 3.7897e-06, 5.68236e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 536
rank avg (pred): 0.430 +- 0.165
mrr vals (pred, true): 0.062, 0.035
batch losses (mrrl, rdl): 0.0015235315, 0.0016819385

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 736
rank avg (pred): 0.047 +- 0.028
mrr vals (pred, true): 0.138, 0.051
batch losses (mrrl, rdl): 0.0773414969, 2.77749e-05

Epoch over!
epoch time: 12.613

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 50
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.297, 0.233
batch losses (mrrl, rdl): 0.0398675054, 3.21592e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 762
rank avg (pred): 0.330 +- 0.193
mrr vals (pred, true): 0.101, 0.042
batch losses (mrrl, rdl): 0.0259075016, 0.000644994

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 712
rank avg (pred): 0.395 +- 0.161
mrr vals (pred, true): 0.080, 0.003
batch losses (mrrl, rdl): 0.0088944472, 0.0001388249

Epoch over!
epoch time: 13.374

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 707
rank avg (pred): 0.405 +- 0.149
mrr vals (pred, true): 0.060, 0.005
batch losses (mrrl, rdl): 0.0009304917, 0.000129915

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 24
rank avg (pred): 0.017 +- 0.011
mrr vals (pred, true): 0.207, 0.178
batch losses (mrrl, rdl): 0.0081600631, 3.07744e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 725
rank avg (pred): 0.413 +- 0.125
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0003727073, 0.0001643141

Epoch over!
epoch time: 13.129

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1058
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.268, 0.236
batch losses (mrrl, rdl): 0.0101132262, 3.28469e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.399 +- 0.126
mrr vals (pred, true): 0.045, 0.004
batch losses (mrrl, rdl): 0.0002150014, 0.0001664604

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 857
rank avg (pred): 0.317 +- 0.177
mrr vals (pred, true): 0.108, 0.158
batch losses (mrrl, rdl): 0.024852179, 0.0010060783

Epoch over!
epoch time: 13.655

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 174
rank avg (pred): 0.331 +- 0.169
mrr vals (pred, true): 0.085, 0.004
batch losses (mrrl, rdl): 0.0125631057, 0.0003206315

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 30
rank avg (pred): 0.077 +- 0.073
mrr vals (pred, true): 0.159, 0.149
batch losses (mrrl, rdl): 0.0010390829, 7.166e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 748
rank avg (pred): 0.158 +- 0.141
mrr vals (pred, true): 0.159, 0.133
batch losses (mrrl, rdl): 0.006865507, 0.0001762676

Epoch over!
epoch time: 13.336

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 816
rank avg (pred): 0.157 +- 0.144
mrr vals (pred, true): 0.153, 0.107
batch losses (mrrl, rdl): 0.0208520405, 0.000228061

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 819
rank avg (pred): 0.198 +- 0.165
mrr vals (pred, true): 0.140, 0.115
batch losses (mrrl, rdl): 0.0061690812, 0.0001434154

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 656
rank avg (pred): 0.369 +- 0.103
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004916707, 0.0002828973

Epoch over!
epoch time: 13.203

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 423
rank avg (pred): 0.314 +- 0.159
mrr vals (pred, true): 0.093, 0.005
batch losses (mrrl, rdl): 0.018377101, 0.0004848334

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 410
rank avg (pred): 0.295 +- 0.156
mrr vals (pred, true): 0.085, 0.004
batch losses (mrrl, rdl): 0.0121560739, 0.0005466176

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 389
rank avg (pred): 0.293 +- 0.157
mrr vals (pred, true): 0.103, 0.187
batch losses (mrrl, rdl): 0.070584774, 0.0006958933

Epoch over!
epoch time: 13.749

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 319
rank avg (pred): 0.079 +- 0.116
mrr vals (pred, true): 0.230, 0.256
batch losses (mrrl, rdl): 0.006682856, 3.7301e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 249
rank avg (pred): 0.179 +- 0.167
mrr vals (pred, true): 0.164, 0.143
batch losses (mrrl, rdl): 0.0046237074, 0.0002499423

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.284 +- 0.153
mrr vals (pred, true): 0.102, 0.005
batch losses (mrrl, rdl): 0.0273261853, 0.0006193417

Epoch over!
epoch time: 13.278

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.289 +- 0.151
mrr vals (pred, true): 0.099, 0.141

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   44 	     0 	 0.09618 	 0.00317 	 m..s
   10 	     1 	 0.04722 	 0.00347 	 m..s
   31 	     2 	 0.09182 	 0.00349 	 m..s
   33 	     3 	 0.09219 	 0.00359 	 m..s
   60 	     4 	 0.10166 	 0.00372 	 m..s
   46 	     5 	 0.09854 	 0.00374 	 m..s
   29 	     6 	 0.09172 	 0.00377 	 m..s
   28 	     7 	 0.08642 	 0.00377 	 m..s
   51 	     8 	 0.09962 	 0.00378 	 m..s
    5 	     9 	 0.04510 	 0.00393 	 m..s
   45 	    10 	 0.09818 	 0.00399 	 m..s
   74 	    11 	 0.10830 	 0.00400 	 MISS
   38 	    12 	 0.09459 	 0.00409 	 m..s
   65 	    13 	 0.10327 	 0.00411 	 m..s
   53 	    14 	 0.10023 	 0.00412 	 m..s
   41 	    15 	 0.09554 	 0.00412 	 m..s
   62 	    16 	 0.10278 	 0.00413 	 m..s
   39 	    17 	 0.09459 	 0.00422 	 m..s
   63 	    18 	 0.10292 	 0.00422 	 m..s
   61 	    19 	 0.10235 	 0.00424 	 m..s
   64 	    20 	 0.10302 	 0.00425 	 m..s
   84 	    21 	 0.11326 	 0.00426 	 MISS
   85 	    22 	 0.11373 	 0.00426 	 MISS
   55 	    23 	 0.10056 	 0.00427 	 m..s
   75 	    24 	 0.10871 	 0.00427 	 MISS
   43 	    25 	 0.09591 	 0.00451 	 m..s
   47 	    26 	 0.09855 	 0.00455 	 m..s
   82 	    27 	 0.11245 	 0.00458 	 MISS
   35 	    28 	 0.09324 	 0.00462 	 m..s
   78 	    29 	 0.11017 	 0.00464 	 MISS
   92 	    30 	 0.12262 	 0.00473 	 MISS
   13 	    31 	 0.04846 	 0.00650 	 m..s
   14 	    32 	 0.04855 	 0.00700 	 m..s
   23 	    33 	 0.05085 	 0.00756 	 m..s
   12 	    34 	 0.04821 	 0.00892 	 m..s
   15 	    35 	 0.04861 	 0.00931 	 m..s
    9 	    36 	 0.04690 	 0.01064 	 m..s
   11 	    37 	 0.04793 	 0.01122 	 m..s
   18 	    38 	 0.04962 	 0.01207 	 m..s
    2 	    39 	 0.04443 	 0.01283 	 m..s
   20 	    40 	 0.05017 	 0.01552 	 m..s
    4 	    41 	 0.04491 	 0.01719 	 ~...
   21 	    42 	 0.05048 	 0.01790 	 m..s
   17 	    43 	 0.04948 	 0.02042 	 ~...
    1 	    44 	 0.04429 	 0.02077 	 ~...
   16 	    45 	 0.04926 	 0.02408 	 ~...
    6 	    46 	 0.04648 	 0.02423 	 ~...
    7 	    47 	 0.04661 	 0.02427 	 ~...
    8 	    48 	 0.04685 	 0.02563 	 ~...
    3 	    49 	 0.04453 	 0.02572 	 ~...
    0 	    50 	 0.04398 	 0.02667 	 ~...
   24 	    51 	 0.05416 	 0.02676 	 ~...
   25 	    52 	 0.05500 	 0.02681 	 ~...
   36 	    53 	 0.09384 	 0.03517 	 m..s
   26 	    54 	 0.08060 	 0.03683 	 m..s
   22 	    55 	 0.05065 	 0.03764 	 ~...
   19 	    56 	 0.04984 	 0.04148 	 ~...
   71 	    57 	 0.10563 	 0.07709 	 ~...
   30 	    58 	 0.09176 	 0.10045 	 ~...
   32 	    59 	 0.09197 	 0.11669 	 ~...
   42 	    60 	 0.09575 	 0.11951 	 ~...
   68 	    61 	 0.10452 	 0.12278 	 ~...
   34 	    62 	 0.09305 	 0.12904 	 m..s
   37 	    63 	 0.09409 	 0.12913 	 m..s
   67 	    64 	 0.10342 	 0.12969 	 ~...
   73 	    65 	 0.10588 	 0.13148 	 ~...
   90 	    66 	 0.11842 	 0.13286 	 ~...
   91 	    67 	 0.11904 	 0.13483 	 ~...
   48 	    68 	 0.09902 	 0.14112 	 m..s
   93 	    69 	 0.13087 	 0.14560 	 ~...
   95 	    70 	 0.15282 	 0.14610 	 ~...
   70 	    71 	 0.10507 	 0.15094 	 m..s
   98 	    72 	 0.16099 	 0.15188 	 ~...
   57 	    73 	 0.10086 	 0.15261 	 m..s
   54 	    74 	 0.10051 	 0.15609 	 m..s
   94 	    75 	 0.13946 	 0.15715 	 ~...
   27 	    76 	 0.08376 	 0.15918 	 m..s
   72 	    77 	 0.10568 	 0.15954 	 m..s
   97 	    78 	 0.15879 	 0.16001 	 ~...
   66 	    79 	 0.10331 	 0.16157 	 m..s
   96 	    80 	 0.15720 	 0.16218 	 ~...
   40 	    81 	 0.09499 	 0.16489 	 m..s
   49 	    82 	 0.09931 	 0.16538 	 m..s
  104 	    83 	 0.17286 	 0.16592 	 ~...
   59 	    84 	 0.10163 	 0.16633 	 m..s
   50 	    85 	 0.09940 	 0.16898 	 m..s
   56 	    86 	 0.10066 	 0.17298 	 m..s
   58 	    87 	 0.10133 	 0.17550 	 m..s
   80 	    88 	 0.11102 	 0.17563 	 m..s
   79 	    89 	 0.11045 	 0.17829 	 m..s
  100 	    90 	 0.16706 	 0.17881 	 ~...
   52 	    91 	 0.09992 	 0.18067 	 m..s
  107 	    92 	 0.20050 	 0.18212 	 ~...
   86 	    93 	 0.11488 	 0.18871 	 m..s
   99 	    94 	 0.16693 	 0.18998 	 ~...
   69 	    95 	 0.10466 	 0.19017 	 m..s
  105 	    96 	 0.17372 	 0.19022 	 ~...
  103 	    97 	 0.17037 	 0.19182 	 ~...
   87 	    98 	 0.11491 	 0.19210 	 m..s
  102 	    99 	 0.16922 	 0.19424 	 ~...
   89 	   100 	 0.11695 	 0.19443 	 m..s
  109 	   101 	 0.20432 	 0.19721 	 ~...
   76 	   102 	 0.10979 	 0.20000 	 m..s
   81 	   103 	 0.11131 	 0.20016 	 m..s
   88 	   104 	 0.11554 	 0.20292 	 m..s
   83 	   105 	 0.11304 	 0.20560 	 m..s
   77 	   106 	 0.10979 	 0.20818 	 m..s
  101 	   107 	 0.16903 	 0.20832 	 m..s
  110 	   108 	 0.20675 	 0.21514 	 ~...
  106 	   109 	 0.18330 	 0.22076 	 m..s
  111 	   110 	 0.20758 	 0.22743 	 ~...
  108 	   111 	 0.20321 	 0.22941 	 ~...
  112 	   112 	 0.20906 	 0.22952 	 ~...
  114 	   113 	 0.21433 	 0.24408 	 ~...
  115 	   114 	 0.22319 	 0.24708 	 ~...
  116 	   115 	 0.22356 	 0.24709 	 ~...
  113 	   116 	 0.20981 	 0.25288 	 m..s
  118 	   117 	 0.27104 	 0.26222 	 ~...
  120 	   118 	 0.28166 	 0.26317 	 ~...
  119 	   119 	 0.27938 	 0.26384 	 ~...
  117 	   120 	 0.26557 	 0.27211 	 ~...
==========================================
r_mrr = 0.7559342384338379
r2_mrr = 0.5322474241256714
spearmanr_mrr@5 = 0.902942955493927
spearmanr_mrr@10 = 0.9102250933647156
spearmanr_mrr@50 = 0.97667396068573
spearmanr_mrr@100 = 0.8108914494514465
spearmanr_mrr@All = 0.8603103160858154
==========================================
test time: 0.403
Done Testing dataset CoDExSmall
total time taken: 209.863840341568
training time taken: 198.98626804351807
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7559)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5322)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9029)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9102)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9767)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8109)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8603)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.4073210370079323}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 677895851252848
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [43, 1152, 474, 359, 658, 307, 587, 225, 178, 562, 1057, 774, 396, 715, 962, 502, 667, 1112, 1026, 1140, 879, 598, 959, 1018, 1144, 723, 166, 700, 883, 790, 676, 516, 85, 650, 939, 654, 175, 1039, 152, 901, 342, 1184, 12, 985, 770, 425, 57, 660, 842, 246, 923, 24, 54, 1071, 311, 276, 483, 729, 711, 366, 1133, 362, 335, 818, 347, 389, 809, 1110, 45, 837, 436, 1185, 941, 46, 343, 1101, 791, 482, 557, 549, 242, 255, 1085, 627, 219, 196, 87, 238, 61, 890, 74, 1012, 107, 885, 563, 360, 111, 397, 164, 142, 968, 892, 18, 174, 361, 162, 1132, 303, 1073, 780, 489, 220, 50, 843, 689, 236, 1115, 1202, 100, 656, 77]
valid_ids (0): []
train_ids (1094): [1056, 115, 405, 1176, 1168, 331, 772, 1000, 559, 432, 600, 556, 896, 752, 351, 1034, 792, 836, 535, 773, 767, 679, 775, 928, 7, 468, 1207, 1007, 499, 873, 903, 838, 847, 364, 233, 925, 285, 165, 584, 1019, 376, 534, 149, 292, 1035, 1206, 201, 487, 1004, 112, 727, 339, 41, 599, 607, 1155, 810, 0, 995, 446, 1067, 553, 608, 926, 145, 706, 66, 915, 1055, 1008, 63, 764, 65, 900, 691, 504, 289, 566, 506, 1211, 777, 481, 308, 105, 5, 987, 554, 8, 241, 651, 1064, 633, 296, 1143, 1043, 863, 163, 11, 78, 212, 301, 853, 393, 156, 756, 765, 944, 547, 256, 619, 907, 789, 615, 617, 804, 932, 294, 438, 4, 263, 1014, 284, 946, 442, 522, 337, 315, 686, 641, 420, 1183, 702, 128, 746, 418, 1051, 526, 492, 189, 856, 931, 922, 1096, 251, 322, 431, 623, 496, 97, 704, 755, 1038, 1082, 1029, 829, 203, 858, 788, 319, 58, 456, 701, 1186, 986, 864, 213, 579, 929, 832, 462, 473, 271, 973, 494, 695, 918, 758, 391, 747, 34, 824, 927, 28, 528, 1213, 517, 320, 787, 1161, 914, 889, 867, 611, 993, 227, 971, 527, 124, 1188, 582, 371, 1127, 458, 694, 576, 813, 674, 1125, 975, 493, 1083, 464, 561, 440, 413, 693, 514, 216, 610, 763, 120, 352, 349, 757, 109, 146, 782, 326, 317, 1098, 861, 476, 1006, 750, 421, 350, 976, 524, 110, 505, 834, 1003, 934, 471, 234, 899, 1002, 816, 161, 1158, 855, 232, 510, 139, 388, 382, 1146, 327, 583, 961, 806, 395, 625, 930, 872, 515, 414, 123, 1024, 970, 194, 1015, 906, 898, 448, 739, 1058, 148, 935, 1205, 338, 798, 1059, 274, 606, 947, 205, 1154, 681, 445, 978, 325, 759, 741, 353, 247, 191, 55, 845, 1052, 79, 1103, 605, 1089, 678, 297, 620, 616, 707, 1061, 609, 891, 383, 449, 844, 684, 210, 595, 193, 1027, 1032, 984, 558, 137, 722, 749, 118, 687, 675, 400, 1040, 507, 1028, 1163, 416, 511, 1126, 917, 1107, 158, 539, 1134, 119, 720, 379, 828, 603, 1138, 575, 893, 800, 974, 281, 187, 136, 1001, 1075, 272, 1078, 640, 306, 848, 1137, 1159, 182, 444, 384, 952, 439, 910, 530, 169, 321, 1214, 89, 718, 589, 126, 1076, 571, 200, 198, 1106, 724, 1050, 1066, 820, 332, 268, 447, 1088, 730, 104, 646, 812, 370, 1118, 868, 762, 134, 634, 390, 938, 1091, 245, 748, 799, 884, 1025, 257, 121, 1189, 463, 785, 288, 578, 531, 70, 101, 1121, 1196, 1179, 570, 839, 356, 497, 433, 406, 786, 1187, 644, 1141, 869, 409, 614, 86, 555, 967, 99, 500, 30, 1175, 44, 15, 424, 460, 354, 664, 387, 830, 717, 275, 503, 217, 1113, 1095, 969, 39, 1102, 631, 10, 1195, 696, 513, 983, 592, 685, 269, 344, 754, 870, 865, 1072, 223, 1149, 280, 1097, 62, 265, 159, 206, 170, 1120, 1047, 738, 626, 94, 22, 290, 690, 113, 851, 1191, 430, 1100, 532, 14, 710, 160, 154, 51, 1104, 441, 734, 1109, 286, 846, 260, 543, 179, 807, 1169, 733, 1124, 237, 398, 683, 283, 888, 254, 886, 1190, 977, 48, 1164, 1165, 1208, 68, 373, 668, 692, 417, 784, 211, 380, 989, 793, 150, 334, 745, 955, 1045, 1192, 33, 208, 1197, 950, 1081, 732, 466, 184, 850, 336, 453, 138, 147, 712, 966, 565, 407, 803, 355, 1212, 630, 912, 38, 725, 643, 357, 744, 1150, 92, 825, 218, 73, 1209, 188, 699, 484, 731, 1172, 860, 753, 346, 647, 310, 1063, 1181, 1011, 302, 612, 20, 551, 776, 751, 341, 1060, 680, 708, 996, 192, 958, 1044, 88, 659, 1177, 300, 1068, 963, 548, 207, 84, 540, 1135, 21, 141, 480, 450, 766, 714, 655, 64, 1079, 1178, 849, 314, 854, 1031, 429, 811, 895, 378, 728, 957, 330, 852, 239, 377, 226, 47, 1037, 1204, 999, 305, 954, 478, 1142, 1166, 185, 71, 177, 90, 1092, 1129, 519, 721, 385, 454, 833, 117, 560, 1022, 533, 264, 1156, 358, 956, 1116, 40, 601, 1194, 410, 180, 318, 726, 636, 670, 742, 943, 911, 964, 495, 1200, 657, 652, 381, 1053, 716, 965, 1123, 278, 13, 573, 541, 72, 538, 509, 1020, 1148, 399, 878, 369, 697, 781, 988, 243, 577, 1062, 221, 287, 486, 1203, 365, 114, 122, 457, 282, 1048, 933, 508, 1198, 768, 1077, 669, 876, 597, 329, 230, 737, 709, 949, 760, 31, 887, 894, 897, 719, 98, 498, 1160, 167, 1199, 427, 794, 401, 796, 544, 459, 299, 374, 713, 29, 224, 997, 545, 518, 23, 743, 593, 525, 761, 1173, 661, 95, 479, 437, 80, 435, 1167, 1005, 1023, 624, 1139, 248, 1130, 979, 677, 877, 273, 35, 443, 25, 229, 404, 937, 470, 1090, 942, 992, 1108, 1013, 3, 249, 1128, 82, 59, 982, 291, 862, 904, 1171, 157, 621, 130, 1170, 9, 635, 155, 808, 882, 452, 412, 1094, 2, 304, 1099, 270, 536, 408, 1210, 663, 313, 916, 638, 1122, 1010, 67, 96, 596, 841, 821, 568, 673, 920, 469, 204, 1182, 258, 402, 1153, 981, 666, 980, 83, 26, 222, 228, 831, 235, 994, 585, 16, 1136, 244, 665, 151, 945, 171, 426, 736, 316, 1086, 703, 771, 1174, 740, 783, 143, 1147, 586, 279, 653, 214, 1030, 682, 990, 475, 153, 490, 106, 801, 415, 467, 76, 19, 428, 613, 173, 779, 642, 594, 1084, 632, 827, 671, 688, 1021, 1131, 116, 521, 546, 924, 176, 637, 1201, 564, 199, 622, 1070, 880, 345, 902, 960, 1087, 698, 197, 202, 434, 795, 881, 940, 1151, 323, 181, 231, 909, 183, 1065, 132, 375, 1069, 125, 645, 567, 523, 835, 672, 1111, 953, 913, 875, 569, 529, 948, 580, 6, 936, 75, 419, 814, 951, 309, 572, 628, 423, 368, 705, 552, 602, 1162, 908, 991, 1, 394, 1180, 998, 649, 91, 403, 919, 485, 267, 215, 127, 253, 465, 581, 1119, 542, 312, 1033, 386, 455, 1093, 168, 53, 27, 822, 840, 819, 240, 1080, 550, 133, 293, 1016, 392, 1157, 859, 1017, 348, 735, 1049, 871, 1145, 32, 537, 491, 340, 1074, 37, 422, 472, 866, 298, 69, 817, 259, 372, 477, 857, 648, 1041, 769, 56, 1114, 144, 618, 778, 1054, 102, 277, 295, 108, 140, 823, 588, 42, 1193, 1046, 874, 261, 1105, 921, 93, 172, 190, 328, 1009, 972, 52, 250, 131, 905, 129, 802, 17, 135, 629, 662, 461, 451, 60, 324, 209, 266, 639, 103, 195, 252, 1036, 805, 590, 591, 826, 333, 363, 512, 488, 411, 574, 604, 186, 1042, 367, 49, 36, 81, 1117, 520, 501, 815, 262, 797]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9137863676932640
the save name prefix for this run is:  chkpt-ID_9137863676932640_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 990
rank avg (pred): 0.506 +- 0.008
mrr vals (pred, true): 0.001, 0.211
batch losses (mrrl, rdl): 0.0, 0.004467099

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 237
rank avg (pred): 0.288 +- 0.231
mrr vals (pred, true): 0.081, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004817993

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 464
rank avg (pred): 0.252 +- 0.226
mrr vals (pred, true): 0.277, 0.005
batch losses (mrrl, rdl): 0.0, 0.0006342859

Epoch over!
epoch time: 14.015

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.044 +- 0.041
mrr vals (pred, true): 0.377, 0.230
batch losses (mrrl, rdl): 0.0, 1.9131e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.261 +- 0.241
mrr vals (pred, true): 0.332, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004828406

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 254
rank avg (pred): 0.070 +- 0.066
mrr vals (pred, true): 0.389, 0.247
batch losses (mrrl, rdl): 0.0, 1.71217e-05

Epoch over!
epoch time: 12.824

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 663
rank avg (pred): 0.358 +- 0.323
mrr vals (pred, true): 0.333, 0.005
batch losses (mrrl, rdl): 0.0, 9.35813e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 526
rank avg (pred): 0.177 +- 0.174
mrr vals (pred, true): 0.388, 0.019
batch losses (mrrl, rdl): 0.0, 1.05893e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.241 +- 0.237
mrr vals (pred, true): 0.398, 0.005
batch losses (mrrl, rdl): 0.0, 0.0007081598

Epoch over!
epoch time: 12.78

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1130
rank avg (pred): 0.262 +- 0.252
mrr vals (pred, true): 0.380, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005240142

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 187
rank avg (pred): 0.269 +- 0.251
mrr vals (pred, true): 0.369, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004789566

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 772
rank avg (pred): 0.250 +- 0.246
mrr vals (pred, true): 0.412, 0.189
batch losses (mrrl, rdl): 0.0, 0.0007823927

Epoch over!
epoch time: 14.151

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 264
rank avg (pred): 0.051 +- 0.051
mrr vals (pred, true): 0.441, 0.175
batch losses (mrrl, rdl): 0.0, 2.8392e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 561
rank avg (pred): 0.186 +- 0.182
mrr vals (pred, true): 0.411, 0.018
batch losses (mrrl, rdl): 0.0, 5.4305e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 101
rank avg (pred): 0.265 +- 0.251
mrr vals (pred, true): 0.394, 0.172
batch losses (mrrl, rdl): 0.0, 0.0007892326

Epoch over!
epoch time: 14.264

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.028 +- 0.027
mrr vals (pred, true): 0.435, 0.244
batch losses (mrrl, rdl): 0.3639453053, 5.6167e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 277
rank avg (pred): 0.024 +- 0.015
mrr vals (pred, true): 0.186, 0.166
batch losses (mrrl, rdl): 0.003961456, 6.43088e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 95
rank avg (pred): 0.327 +- 0.181
mrr vals (pred, true): 0.102, 0.166
batch losses (mrrl, rdl): 0.0408424363, 0.001231546

Epoch over!
epoch time: 13.792

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 182
rank avg (pred): 0.328 +- 0.174
mrr vals (pred, true): 0.089, 0.004
batch losses (mrrl, rdl): 0.0155506423, 0.0003516649

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1125
rank avg (pred): 0.326 +- 0.196
mrr vals (pred, true): 0.118, 0.004
batch losses (mrrl, rdl): 0.0462836847, 0.0003282771

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 501
rank avg (pred): 0.285 +- 0.110
mrr vals (pred, true): 0.029, 0.014
batch losses (mrrl, rdl): 0.0045638713, 0.0001104881

Epoch over!
epoch time: 13.773

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 595
rank avg (pred): 0.332 +- 0.126
mrr vals (pred, true): 0.045, 0.012
batch losses (mrrl, rdl): 0.0002118177, 5.4462e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 293
rank avg (pred): 0.008 +- 0.005
mrr vals (pred, true): 0.250, 0.267
batch losses (mrrl, rdl): 0.0030135156, 2.11236e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 371
rank avg (pred): 0.227 +- 0.135
mrr vals (pred, true): 0.127, 0.192
batch losses (mrrl, rdl): 0.0422680937, 0.0003963666

Epoch over!
epoch time: 13.79

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1037
rank avg (pred): 0.320 +- 0.194
mrr vals (pred, true): 0.134, 0.004
batch losses (mrrl, rdl): 0.0708767176, 0.0003458247

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 109
rank avg (pred): 0.341 +- 0.174
mrr vals (pred, true): 0.101, 0.131
batch losses (mrrl, rdl): 0.0086983424, 0.0011163453

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 472
rank avg (pred): 0.329 +- 0.171
mrr vals (pred, true): 0.107, 0.004
batch losses (mrrl, rdl): 0.0330522805, 0.0003663225

Epoch over!
epoch time: 13.596

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 248
rank avg (pred): 0.016 +- 0.010
mrr vals (pred, true): 0.214, 0.188
batch losses (mrrl, rdl): 0.0066558872, 4.57558e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 129
rank avg (pred): 0.334 +- 0.161
mrr vals (pred, true): 0.083, 0.116
batch losses (mrrl, rdl): 0.0105933174, 0.0010463519

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 814
rank avg (pred): 0.155 +- 0.095
mrr vals (pred, true): 0.163, 0.156
batch losses (mrrl, rdl): 0.0005658343, 0.0001801752

Epoch over!
epoch time: 13.612

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.324 +- 0.152
mrr vals (pred, true): 0.087, 0.098
batch losses (mrrl, rdl): 0.0137741957, 0.0007849437

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.132 +- 0.075
mrr vals (pred, true): 0.127, 0.149
batch losses (mrrl, rdl): 0.004424606, 8.98761e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 167
rank avg (pred): 0.326 +- 0.167
mrr vals (pred, true): 0.098, 0.005
batch losses (mrrl, rdl): 0.0231701862, 0.0002715883

Epoch over!
epoch time: 13.929

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 248
rank avg (pred): 0.012 +- 0.007
mrr vals (pred, true): 0.251, 0.188
batch losses (mrrl, rdl): 0.0397274233, 5.40659e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 207
rank avg (pred): 0.318 +- 0.160
mrr vals (pred, true): 0.111, 0.005
batch losses (mrrl, rdl): 0.0374094322, 0.0004254041

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 350
rank avg (pred): 0.340 +- 0.165
mrr vals (pred, true): 0.085, 0.189
batch losses (mrrl, rdl): 0.1078893617, 0.0012597071

Epoch over!
epoch time: 13.238

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 51
rank avg (pred): 0.037 +- 0.022
mrr vals (pred, true): 0.186, 0.184
batch losses (mrrl, rdl): 3.397e-05, 8.9012e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 692
rank avg (pred): 0.292 +- 0.111
mrr vals (pred, true): 0.048, 0.004
batch losses (mrrl, rdl): 3.09752e-05, 0.0007479668

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.327 +- 0.153
mrr vals (pred, true): 0.092, 0.004
batch losses (mrrl, rdl): 0.017908603, 0.0003559823

Epoch over!
epoch time: 12.526

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 630
rank avg (pred): 0.357 +- 0.125
mrr vals (pred, true): 0.041, 0.007
batch losses (mrrl, rdl): 0.0007354794, 4.51647e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 75
rank avg (pred): 0.015 +- 0.009
mrr vals (pred, true): 0.223, 0.193
batch losses (mrrl, rdl): 0.0093581816, 3.60212e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 310
rank avg (pred): 0.030 +- 0.017
mrr vals (pred, true): 0.170, 0.220
batch losses (mrrl, rdl): 0.0249216035, 1.34284e-05

Epoch over!
epoch time: 13.171

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1207
rank avg (pred): 0.366 +- 0.131
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 2.59282e-05, 0.0002614881

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 789
rank avg (pred): 0.281 +- 0.144
mrr vals (pred, true): 0.115, 0.004
batch losses (mrrl, rdl): 0.0423948467, 0.0006678609

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1043
rank avg (pred): 0.299 +- 0.164
mrr vals (pred, true): 0.116, 0.005
batch losses (mrrl, rdl): 0.0430798009, 0.0004179952

Epoch over!
epoch time: 12.742

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.039 +- 0.022
mrr vals (pred, true): 0.170, 0.206

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.03975 	 0.00333 	 m..s
   14 	     1 	 0.03729 	 0.00339 	 m..s
    2 	     2 	 0.03149 	 0.00340 	 ~...
   45 	     3 	 0.08186 	 0.00344 	 m..s
   22 	     4 	 0.04190 	 0.00348 	 m..s
   68 	     5 	 0.09593 	 0.00350 	 m..s
   64 	     6 	 0.09324 	 0.00352 	 m..s
    1 	     7 	 0.03137 	 0.00354 	 ~...
   19 	     8 	 0.04024 	 0.00358 	 m..s
   10 	     9 	 0.03555 	 0.00360 	 m..s
    5 	    10 	 0.03338 	 0.00364 	 ~...
   13 	    11 	 0.03666 	 0.00369 	 m..s
    4 	    12 	 0.03323 	 0.00376 	 ~...
   71 	    13 	 0.09637 	 0.00377 	 m..s
   84 	    14 	 0.10523 	 0.00378 	 MISS
   21 	    15 	 0.04162 	 0.00379 	 m..s
   73 	    16 	 0.09789 	 0.00381 	 m..s
   30 	    17 	 0.06948 	 0.00386 	 m..s
   75 	    18 	 0.09821 	 0.00387 	 m..s
    9 	    19 	 0.03509 	 0.00387 	 m..s
   93 	    20 	 0.11306 	 0.00388 	 MISS
   79 	    21 	 0.10151 	 0.00392 	 m..s
   41 	    22 	 0.07853 	 0.00393 	 m..s
   94 	    23 	 0.11327 	 0.00394 	 MISS
   44 	    24 	 0.08173 	 0.00398 	 m..s
   40 	    25 	 0.07827 	 0.00400 	 m..s
   88 	    26 	 0.10839 	 0.00406 	 MISS
   33 	    27 	 0.07189 	 0.00409 	 m..s
   82 	    28 	 0.10434 	 0.00410 	 MISS
   89 	    29 	 0.10877 	 0.00412 	 MISS
   56 	    30 	 0.08896 	 0.00414 	 m..s
   95 	    31 	 0.11552 	 0.00414 	 MISS
   29 	    32 	 0.06896 	 0.00414 	 m..s
   53 	    33 	 0.08556 	 0.00415 	 m..s
   31 	    34 	 0.07079 	 0.00423 	 m..s
   15 	    35 	 0.03842 	 0.00424 	 m..s
   38 	    36 	 0.07780 	 0.00424 	 m..s
   43 	    37 	 0.07926 	 0.00425 	 m..s
   63 	    38 	 0.09322 	 0.00426 	 m..s
   46 	    39 	 0.08186 	 0.00427 	 m..s
   67 	    40 	 0.09448 	 0.00431 	 m..s
   92 	    41 	 0.11296 	 0.00435 	 MISS
   57 	    42 	 0.08933 	 0.00435 	 m..s
   66 	    43 	 0.09434 	 0.00441 	 m..s
   97 	    44 	 0.11758 	 0.00454 	 MISS
   50 	    45 	 0.08342 	 0.00457 	 m..s
   80 	    46 	 0.10201 	 0.00464 	 m..s
   69 	    47 	 0.09595 	 0.00486 	 m..s
   12 	    48 	 0.03639 	 0.00672 	 ~...
    0 	    49 	 0.03133 	 0.00772 	 ~...
    3 	    50 	 0.03186 	 0.00995 	 ~...
   17 	    51 	 0.03940 	 0.01274 	 ~...
   16 	    52 	 0.03883 	 0.01719 	 ~...
   24 	    53 	 0.04485 	 0.01756 	 ~...
   25 	    54 	 0.04520 	 0.01759 	 ~...
   23 	    55 	 0.04199 	 0.01823 	 ~...
   26 	    56 	 0.04688 	 0.01924 	 ~...
    8 	    57 	 0.03439 	 0.02063 	 ~...
   11 	    58 	 0.03625 	 0.02264 	 ~...
    7 	    59 	 0.03364 	 0.02632 	 ~...
    6 	    60 	 0.03338 	 0.02953 	 ~...
   28 	    61 	 0.06789 	 0.03390 	 m..s
   20 	    62 	 0.04082 	 0.03752 	 ~...
   27 	    63 	 0.04691 	 0.04015 	 ~...
   51 	    64 	 0.08528 	 0.04388 	 m..s
   34 	    65 	 0.07203 	 0.09528 	 ~...
   35 	    66 	 0.07219 	 0.10008 	 ~...
   98 	    67 	 0.14099 	 0.11143 	 ~...
   72 	    68 	 0.09728 	 0.11816 	 ~...
   48 	    69 	 0.08263 	 0.12923 	 m..s
   32 	    70 	 0.07132 	 0.13184 	 m..s
   42 	    71 	 0.07908 	 0.13204 	 m..s
   37 	    72 	 0.07623 	 0.13404 	 m..s
   96 	    73 	 0.11676 	 0.13483 	 ~...
   36 	    74 	 0.07521 	 0.13693 	 m..s
   47 	    75 	 0.08232 	 0.13770 	 m..s
  101 	    76 	 0.15306 	 0.14610 	 ~...
   60 	    77 	 0.09101 	 0.14746 	 m..s
   52 	    78 	 0.08548 	 0.14840 	 m..s
  107 	    79 	 0.16349 	 0.15089 	 ~...
  106 	    80 	 0.16343 	 0.15109 	 ~...
  100 	    81 	 0.15212 	 0.15280 	 ~...
  103 	    82 	 0.15360 	 0.15311 	 ~...
   39 	    83 	 0.07814 	 0.15364 	 m..s
   54 	    84 	 0.08694 	 0.15622 	 m..s
   59 	    85 	 0.09058 	 0.15688 	 m..s
   58 	    86 	 0.08947 	 0.15802 	 m..s
   99 	    87 	 0.14344 	 0.15914 	 ~...
   83 	    88 	 0.10503 	 0.16812 	 m..s
   62 	    89 	 0.09207 	 0.16959 	 m..s
   70 	    90 	 0.09607 	 0.17072 	 m..s
   49 	    91 	 0.08297 	 0.17231 	 m..s
   91 	    92 	 0.11058 	 0.17435 	 m..s
   81 	    93 	 0.10283 	 0.17563 	 m..s
  105 	    94 	 0.16134 	 0.17804 	 ~...
  102 	    95 	 0.15325 	 0.17919 	 ~...
   78 	    96 	 0.10057 	 0.18067 	 m..s
  108 	    97 	 0.16528 	 0.18087 	 ~...
   74 	    98 	 0.09800 	 0.18122 	 m..s
   61 	    99 	 0.09154 	 0.18624 	 m..s
   87 	   100 	 0.10804 	 0.18659 	 m..s
  112 	   101 	 0.17971 	 0.18686 	 ~...
   55 	   102 	 0.08893 	 0.18895 	 MISS
   86 	   103 	 0.10790 	 0.19017 	 m..s
  104 	   104 	 0.15614 	 0.19078 	 m..s
  109 	   105 	 0.16888 	 0.19254 	 ~...
   85 	   106 	 0.10711 	 0.19443 	 m..s
   76 	   107 	 0.09975 	 0.19491 	 m..s
  114 	   108 	 0.18508 	 0.19574 	 ~...
   90 	   109 	 0.10918 	 0.20000 	 m..s
   65 	   110 	 0.09412 	 0.20390 	 MISS
   77 	   111 	 0.10047 	 0.20519 	 MISS
  110 	   112 	 0.16964 	 0.20601 	 m..s
  115 	   113 	 0.18988 	 0.21220 	 ~...
  111 	   114 	 0.17364 	 0.21757 	 m..s
  120 	   115 	 0.22236 	 0.23160 	 ~...
  119 	   116 	 0.22224 	 0.23346 	 ~...
  118 	   117 	 0.22036 	 0.23428 	 ~...
  117 	   118 	 0.20027 	 0.23903 	 m..s
  116 	   119 	 0.19005 	 0.24214 	 m..s
  113 	   120 	 0.18459 	 0.24540 	 m..s
==========================================
r_mrr = 0.6916223764419556
r2_mrr = 0.4422800540924072
spearmanr_mrr@5 = 0.9019982218742371
spearmanr_mrr@10 = 0.8932624459266663
spearmanr_mrr@50 = 0.9581040143966675
spearmanr_mrr@100 = 0.858156144618988
spearmanr_mrr@All = 0.8807181119918823
==========================================
test time: 0.415
Done Testing dataset CoDExSmall
total time taken: 213.13234186172485
training time taken: 202.70154428482056
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.6916)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.4423)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.9020)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.8933)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9581)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8582)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8807)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.739465906467558}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 6418307216144532
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [46, 326, 723, 811, 290, 1178, 109, 1100, 1060, 303, 50, 543, 725, 920, 1139, 137, 903, 1165, 1194, 992, 571, 1051, 48, 864, 891, 11, 269, 1007, 884, 1212, 504, 136, 837, 821, 923, 173, 408, 847, 61, 745, 558, 387, 602, 419, 889, 135, 369, 848, 793, 226, 159, 235, 366, 1062, 738, 749, 676, 353, 89, 521, 555, 939, 951, 812, 873, 1031, 1048, 542, 20, 122, 119, 431, 934, 1124, 653, 704, 927, 43, 896, 1199, 118, 563, 700, 1008, 948, 570, 150, 899, 1173, 625, 806, 639, 836, 207, 672, 286, 794, 133, 1113, 442, 1039, 196, 550, 75, 1163, 1032, 935, 1092, 892, 260, 329, 245, 1184, 1013, 351, 199, 6, 1059, 142, 265, 70]
valid_ids (0): []
train_ids (1094): [604, 1014, 804, 667, 987, 330, 529, 217, 396, 844, 409, 979, 1130, 340, 106, 1111, 575, 190, 128, 695, 721, 252, 669, 546, 318, 36, 1118, 824, 673, 562, 335, 921, 461, 1135, 78, 635, 473, 490, 394, 12, 397, 430, 141, 251, 1054, 463, 576, 455, 1112, 172, 516, 194, 1037, 1017, 210, 153, 904, 460, 828, 417, 779, 736, 377, 1190, 495, 789, 716, 1192, 805, 285, 35, 401, 241, 1144, 719, 249, 748, 827, 483, 1157, 262, 19, 801, 950, 1063, 1169, 263, 1105, 395, 858, 389, 544, 915, 445, 759, 780, 317, 724, 434, 16, 1061, 888, 204, 893, 169, 654, 877, 712, 1171, 64, 886, 1129, 810, 341, 358, 642, 706, 1127, 1027, 626, 32, 770, 1053, 280, 671, 545, 548, 485, 55, 375, 115, 90, 803, 972, 1128, 524, 675, 412, 658, 853, 502, 404, 300, 863, 945, 261, 1042, 819, 1045, 1211, 613, 197, 29, 938, 478, 616, 1133, 424, 641, 1016, 800, 458, 222, 1078, 259, 1162, 52, 418, 305, 383, 767, 778, 1121, 1015, 677, 955, 632, 782, 792, 906, 1168, 465, 1172, 435, 567, 438, 525, 559, 292, 699, 80, 357, 208, 1088, 323, 1142, 22, 650, 250, 708, 423, 228, 386, 857, 345, 468, 769, 611, 762, 1012, 557, 580, 1196, 662, 2, 963, 38, 1122, 1099, 855, 339, 157, 711, 72, 714, 739, 909, 175, 707, 615, 509, 1176, 931, 640, 1208, 471, 97, 1115, 574, 826, 1026, 645, 124, 1207, 312, 753, 722, 533, 678, 894, 1025, 825, 110, 551, 954, 385, 538, 220, 1181, 818, 701, 474, 620, 974, 58, 937, 247, 1066, 517, 216, 506, 333, 783, 332, 534, 1152, 777, 462, 60, 907, 776, 941, 831, 564, 693, 988, 997, 617, 809, 475, 713, 221, 510, 202, 1041, 796, 165, 496, 67, 230, 1154, 1095, 3, 952, 644, 99, 930, 1134, 741, 940, 597, 936, 663, 599, 96, 160, 1087, 561, 420, 198, 1109, 470, 117, 751, 4, 244, 407, 1147, 1065, 897, 63, 1188, 347, 156, 93, 39, 528, 768, 308, 1110, 609, 443, 114, 336, 761, 1179, 648, 505, 1164, 270, 912, 182, 598, 1097, 132, 715, 448, 682, 984, 28, 464, 859, 472, 594, 188, 231, 772, 1046, 88, 910, 1123, 568, 815, 275, 742, 7, 1050, 94, 279, 512, 62, 983, 54, 629, 382, 27, 282, 926, 469, 1206, 276, 755, 612, 554, 999, 246, 40, 343, 766, 69, 1033, 588, 1000, 991, 187, 25, 1005, 816, 685, 21, 327, 1209, 1098, 624, 271, 201, 497, 92, 1019, 1150, 606, 319, 998, 577, 289, 466, 807, 905, 838, 1058, 86, 1107, 870, 879, 350, 968, 1156, 586, 1070, 515, 850, 477, 833, 342, 146, 1205, 321, 985, 994, 253, 1117, 619, 1202, 348, 487, 664, 900, 492, 140, 530, 582, 17, 1187, 593, 84, 255, 949, 24, 539, 33, 774, 1177, 1167, 503, 151, 784, 652, 9, 584, 797, 508, 163, 964, 493, 414, 1114, 1201, 234, 371, 1116, 933, 149, 822, 278, 924, 861, 638, 718, 911, 908, 355, 41, 334, 1, 489, 283, 552, 608, 730, 1145, 1086, 381, 922, 928, 610, 876, 914, 10, 301, 898, 986, 447, 764, 102, 237, 970, 656, 744, 883, 291, 364, 467, 95, 829, 398, 882, 1140, 436, 630, 657, 233, 481, 281, 363, 171, 978, 823, 126, 1028, 164, 942, 752, 304, 919, 499, 307, 30, 618, 852, 129, 595, 969, 689, 1074, 378, 456, 1198, 439, 660, 953, 680, 359, 960, 154, 560, 476, 392, 81, 1020, 569, 116, 540, 874, 781, 679, 413, 1094, 865, 162, 756, 184, 798, 621, 147, 665, 243, 268, 433, 236, 393, 692, 399, 388, 296, 479, 651, 814, 684, 1214, 549, 754, 287, 183, 158, 785, 881, 1166, 846, 491, 703, 34, 367, 842, 771, 179, 82, 349, 446, 494, 537, 1149, 1071, 1126, 990, 71, 337, 91, 1079, 362, 138, 1101, 519, 743, 961, 808, 23, 871, 121, 123, 215, 709, 181, 788, 205, 1072, 536, 839, 1119, 1189, 1055, 627, 143, 1160, 573, 298, 728, 916, 178, 227, 646, 1075, 813, 1077, 170, 565, 313, 108, 180, 670, 740, 637, 1106, 134, 45, 450, 603, 391, 1186, 1175, 688, 622, 1040, 191, 498, 895, 66, 1146, 995, 384, 729, 426, 1151, 1009, 541, 229, 177, 757, 860, 176, 583, 320, 405, 1023, 1203, 103, 655, 1081, 238, 929, 242, 687, 572, 832, 589, 294, 1161, 74, 746, 0, 79, 578, 760, 1200, 862, 85, 125, 167, 1180, 284, 726, 1102, 293, 710, 288, 309, 451, 1057, 1035, 918, 1030, 522, 267, 297, 634, 1103, 449, 239, 790, 277, 354, 1043, 107, 139, 731, 315, 1155, 148, 820, 956, 403, 352, 212, 592, 659, 225, 691, 845, 325, 971, 717, 758, 390, 511, 374, 453, 834, 1143, 13, 1004, 1089, 787, 866, 1159, 735, 324, 376, 1029, 902, 1080, 100, 579, 189, 87, 444, 224, 101, 486, 869, 254, 480, 195, 1210, 406, 943, 967, 37, 733, 437, 843, 702, 1213, 981, 429, 817, 957, 131, 786, 977, 1069, 631, 311, 1001, 185, 200, 737, 240, 1131, 1064, 636, 696, 365, 1108, 65, 1148, 1052, 402, 59, 913, 454, 1174, 989, 161, 440, 1195, 856, 1022, 174, 1104, 614, 1010, 15, 885, 416, 218, 368, 791, 53, 605, 314, 683, 1068, 973, 1082, 750, 596, 1096, 57, 232, 841, 880, 373, 152, 500, 1158, 1132, 727, 425, 890, 166, 356, 47, 1021, 49, 77, 211, 993, 944, 104, 338, 1018, 306, 1036, 219, 547, 206, 1083, 531, 698, 299, 1170, 996, 372, 76, 532, 257, 1138, 773, 872, 56, 328, 581, 273, 379, 601, 26, 720, 213, 331, 527, 591, 507, 681, 428, 5, 1034, 18, 31, 623, 484, 192, 441, 1011, 1125, 1049, 674, 647, 264, 73, 459, 1093, 965, 8, 120, 1185, 1067, 258, 44, 966, 666, 1044, 501, 272, 628, 830, 849, 410, 1137, 607, 452, 370, 421, 223, 346, 302, 1193, 868, 42, 130, 975, 145, 932, 1076, 432, 1141, 566, 214, 1136, 1090, 1047, 400, 633, 947, 1120, 203, 193, 360, 887, 83, 982, 457, 690, 51, 686, 556, 316, 1153, 668, 14, 835, 361, 946, 705, 1073, 248, 105, 851, 144, 274, 1002, 734, 344, 514, 962, 427, 1091, 186, 310, 553, 765, 518, 1191, 775, 643, 959, 867, 1197, 1183, 113, 209, 322, 523, 747, 266, 98, 513, 763, 980, 917, 1024, 411, 415, 112, 68, 697, 482, 1084, 649, 1003, 1056, 422, 526, 520, 111, 295, 976, 958, 802, 840, 1038, 732, 901, 878, 925, 256, 168, 694, 1182, 854, 799, 127, 875, 587, 1204, 380, 795, 1006, 1085, 155, 600, 585, 488, 661, 535, 590]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6254364806297936
the save name prefix for this run is:  chkpt-ID_6254364806297936_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.442 +- 0.006
mrr vals (pred, true): 0.001, 0.009
batch losses (mrrl, rdl): 0.0, 0.0003184165

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.268 +- 0.197
mrr vals (pred, true): 0.084, 0.174
batch losses (mrrl, rdl): 0.0, 0.0007665057

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.057 +- 0.047
mrr vals (pred, true): 0.332, 0.111
batch losses (mrrl, rdl): 0.0, 2.1677e-06

Epoch over!
epoch time: 13.414

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1181
rank avg (pred): 0.347 +- 0.285
mrr vals (pred, true): 0.281, 0.009
batch losses (mrrl, rdl): 0.0, 5.89306e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 300
rank avg (pred): 0.059 +- 0.051
mrr vals (pred, true): 0.343, 0.145
batch losses (mrrl, rdl): 0.0, 3.05731e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.058 +- 0.055
mrr vals (pred, true): 0.401, 0.188
batch losses (mrrl, rdl): 0.0, 3.1943e-06

Epoch over!
epoch time: 12.693

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 608
rank avg (pred): 0.328 +- 0.303
mrr vals (pred, true): 0.355, 0.023
batch losses (mrrl, rdl): 0.0, 8.64056e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 188
rank avg (pred): 0.253 +- 0.238
mrr vals (pred, true): 0.361, 0.005
batch losses (mrrl, rdl): 0.0, 0.0006119157

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.254 +- 0.240
mrr vals (pred, true): 0.373, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006500505

Epoch over!
epoch time: 12.042

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 870
rank avg (pred): 0.255 +- 0.242
mrr vals (pred, true): 0.380, 0.006
batch losses (mrrl, rdl): 0.0, 0.0005947215

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1149
rank avg (pred): 0.162 +- 0.159
mrr vals (pred, true): 0.420, 0.022
batch losses (mrrl, rdl): 0.0, 5.4426e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 584
rank avg (pred): 0.328 +- 0.309
mrr vals (pred, true): 0.365, 0.020
batch losses (mrrl, rdl): 0.0, 7.16383e-05

Epoch over!
epoch time: 13.799

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 748
rank avg (pred): 0.057 +- 0.056
mrr vals (pred, true): 0.426, 0.133
batch losses (mrrl, rdl): 0.0, 9.2944e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 554
rank avg (pred): 0.160 +- 0.154
mrr vals (pred, true): 0.385, 0.040
batch losses (mrrl, rdl): 0.0, 1.50946e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.243 +- 0.239
mrr vals (pred, true): 0.383, 0.005
batch losses (mrrl, rdl): 0.0, 0.0006021599

Epoch over!
epoch time: 13.464

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1175
rank avg (pred): 0.315 +- 0.311
mrr vals (pred, true): 0.379, 0.012
batch losses (mrrl, rdl): 1.0833297968, 5.95498e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 161
rank avg (pred): 0.348 +- 0.188
mrr vals (pred, true): 0.097, 0.173
batch losses (mrrl, rdl): 0.0579545721, 0.0014709284

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 184
rank avg (pred): 0.338 +- 0.183
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.0243118759, 0.0002484488

Epoch over!
epoch time: 13.837

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 585
rank avg (pred): 0.411 +- 0.133
mrr vals (pred, true): 0.051, 0.007
batch losses (mrrl, rdl): 1.73896e-05, 0.0001802725

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 944
rank avg (pred): 0.355 +- 0.164
mrr vals (pred, true): 0.086, 0.166
batch losses (mrrl, rdl): 0.0645145699, 0.0016725046

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 117
rank avg (pred): 0.339 +- 0.165
mrr vals (pred, true): 0.093, 0.108
batch losses (mrrl, rdl): 0.0024187581, 0.0009784665

Epoch over!
epoch time: 12.75

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 807
rank avg (pred): 0.313 +- 0.177
mrr vals (pred, true): 0.109, 0.005
batch losses (mrrl, rdl): 0.0351787768, 0.0003670263

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 5
rank avg (pred): 0.026 +- 0.016
mrr vals (pred, true): 0.177, 0.197
batch losses (mrrl, rdl): 0.003955408, 8.2227e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 189
rank avg (pred): 0.341 +- 0.145
mrr vals (pred, true): 0.075, 0.004
batch losses (mrrl, rdl): 0.0060177194, 0.0003501738

Epoch over!
epoch time: 12.85

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.019 +- 0.012
mrr vals (pred, true): 0.204, 0.238
batch losses (mrrl, rdl): 0.0116323456, 1.79842e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 200
rank avg (pred): 0.308 +- 0.161
mrr vals (pred, true): 0.104, 0.004
batch losses (mrrl, rdl): 0.0294676255, 0.000450597

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 880
rank avg (pred): 0.294 +- 0.165
mrr vals (pred, true): 0.122, 0.004
batch losses (mrrl, rdl): 0.0513547994, 0.0004904012

Epoch over!
epoch time: 13.2

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 867
rank avg (pred): 0.298 +- 0.161
mrr vals (pred, true): 0.107, 0.004
batch losses (mrrl, rdl): 0.0327239782, 0.0005285564

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 729
rank avg (pred): 0.254 +- 0.156
mrr vals (pred, true): 0.119, 0.135
batch losses (mrrl, rdl): 0.0024276096, 0.0007976358

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.350 +- 0.097
mrr vals (pred, true): 0.055, 0.014
batch losses (mrrl, rdl): 0.0002280189, 9.7052e-05

Epoch over!
epoch time: 12.259

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 919
rank avg (pred): 0.310 +- 0.142
mrr vals (pred, true): 0.095, 0.166
batch losses (mrrl, rdl): 0.0511468127, 0.0009719161

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.303 +- 0.140
mrr vals (pred, true): 0.099, 0.132
batch losses (mrrl, rdl): 0.0106281247, 0.0008114363

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.344 +- 0.097
mrr vals (pred, true): 0.053, 0.025
batch losses (mrrl, rdl): 9.00122e-05, 0.0005829762

Epoch over!
epoch time: 13.603

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.307 +- 0.134
mrr vals (pred, true): 0.091, 0.112
batch losses (mrrl, rdl): 0.004702488, 0.0004801224

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 668
rank avg (pred): 0.340 +- 0.086
mrr vals (pred, true): 0.050, 0.004
batch losses (mrrl, rdl): 2.6e-08, 0.0004358761

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1006
rank avg (pred): 0.264 +- 0.157
mrr vals (pred, true): 0.132, 0.210
batch losses (mrrl, rdl): 0.0605194829, 0.0007172159

Epoch over!
epoch time: 13.041

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 844
rank avg (pred): 0.279 +- 0.144
mrr vals (pred, true): 0.111, 0.043
batch losses (mrrl, rdl): 0.0375137143, 0.0002764923

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 551
rank avg (pred): 0.323 +- 0.102
mrr vals (pred, true): 0.065, 0.042
batch losses (mrrl, rdl): 0.0021144361, 0.0006772084

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 978
rank avg (pred): 0.012 +- 0.007
mrr vals (pred, true): 0.262, 0.265
batch losses (mrrl, rdl): 7.60572e-05, 1.78741e-05

Epoch over!
epoch time: 13.541

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 568
rank avg (pred): 0.386 +- 0.105
mrr vals (pred, true): 0.051, 0.013
batch losses (mrrl, rdl): 1.33329e-05, 0.0001240877

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.260 +- 0.138
mrr vals (pred, true): 0.131, 0.178
batch losses (mrrl, rdl): 0.0224298816, 0.0006026802

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 730
rank avg (pred): 0.248 +- 0.142
mrr vals (pred, true): 0.157, 0.133
batch losses (mrrl, rdl): 0.0057110116, 0.0007182671

Epoch over!
epoch time: 14.586

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 605
rank avg (pred): 0.372 +- 0.101
mrr vals (pred, true): 0.050, 0.025
batch losses (mrrl, rdl): 2.965e-07, 0.0001884965

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1044
rank avg (pred): 0.254 +- 0.135
mrr vals (pred, true): 0.113, 0.003
batch losses (mrrl, rdl): 0.0402641259, 0.001006384

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 866
rank avg (pred): 0.262 +- 0.128
mrr vals (pred, true): 0.106, 0.005
batch losses (mrrl, rdl): 0.0309428107, 0.0007427253

Epoch over!
epoch time: 14.477

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.053 +- 0.033
mrr vals (pred, true): 0.217, 0.212

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04936 	 0.00322 	 m..s
    5 	     1 	 0.04919 	 0.00327 	 m..s
   11 	     2 	 0.05116 	 0.00327 	 m..s
   15 	     3 	 0.05297 	 0.00333 	 m..s
   17 	     4 	 0.05422 	 0.00335 	 m..s
   90 	     5 	 0.12001 	 0.00350 	 MISS
   22 	     6 	 0.05561 	 0.00352 	 m..s
    3 	     7 	 0.04860 	 0.00364 	 m..s
   40 	     8 	 0.08632 	 0.00372 	 m..s
   55 	     9 	 0.09496 	 0.00374 	 m..s
    4 	    10 	 0.04908 	 0.00376 	 m..s
   18 	    11 	 0.05439 	 0.00383 	 m..s
   41 	    12 	 0.08634 	 0.00385 	 m..s
   30 	    13 	 0.07711 	 0.00387 	 m..s
    0 	    14 	 0.04031 	 0.00389 	 m..s
   97 	    15 	 0.12955 	 0.00389 	 MISS
   80 	    16 	 0.10689 	 0.00400 	 MISS
   85 	    17 	 0.11033 	 0.00404 	 MISS
   42 	    18 	 0.08684 	 0.00406 	 m..s
   59 	    19 	 0.10050 	 0.00411 	 m..s
   81 	    20 	 0.10720 	 0.00415 	 MISS
   88 	    21 	 0.11524 	 0.00415 	 MISS
   62 	    22 	 0.10251 	 0.00415 	 m..s
   52 	    23 	 0.09104 	 0.00417 	 m..s
   62 	    24 	 0.10251 	 0.00421 	 m..s
   84 	    25 	 0.11029 	 0.00426 	 MISS
   45 	    26 	 0.08714 	 0.00427 	 m..s
   48 	    27 	 0.08844 	 0.00430 	 m..s
   93 	    28 	 0.12577 	 0.00442 	 MISS
   62 	    29 	 0.10251 	 0.00445 	 m..s
   62 	    30 	 0.10251 	 0.00452 	 m..s
   62 	    31 	 0.10251 	 0.00463 	 m..s
   62 	    32 	 0.10251 	 0.00494 	 m..s
   44 	    33 	 0.08691 	 0.00499 	 m..s
   62 	    34 	 0.10251 	 0.00519 	 m..s
   95 	    35 	 0.12732 	 0.00563 	 MISS
    7 	    36 	 0.04951 	 0.00700 	 m..s
    1 	    37 	 0.04677 	 0.00707 	 m..s
   20 	    38 	 0.05523 	 0.00772 	 m..s
   12 	    39 	 0.05180 	 0.00802 	 m..s
   13 	    40 	 0.05253 	 0.00876 	 m..s
   23 	    41 	 0.05645 	 0.00999 	 m..s
   14 	    42 	 0.05275 	 0.01044 	 m..s
    9 	    43 	 0.05074 	 0.01080 	 m..s
    2 	    44 	 0.04763 	 0.01133 	 m..s
    8 	    45 	 0.04992 	 0.01310 	 m..s
   21 	    46 	 0.05526 	 0.01504 	 m..s
   19 	    47 	 0.05470 	 0.01713 	 m..s
   24 	    48 	 0.05664 	 0.01790 	 m..s
   16 	    49 	 0.05298 	 0.01915 	 m..s
   26 	    50 	 0.05723 	 0.02142 	 m..s
   10 	    51 	 0.05101 	 0.02189 	 ~...
   29 	    52 	 0.07075 	 0.02798 	 m..s
   51 	    53 	 0.08943 	 0.03390 	 m..s
   28 	    54 	 0.05922 	 0.03427 	 ~...
   25 	    55 	 0.05717 	 0.03688 	 ~...
   33 	    56 	 0.08090 	 0.03870 	 m..s
   53 	    57 	 0.09143 	 0.03956 	 m..s
   27 	    58 	 0.05854 	 0.04015 	 ~...
   31 	    59 	 0.07832 	 0.09818 	 ~...
   50 	    60 	 0.08922 	 0.10711 	 ~...
   34 	    61 	 0.08219 	 0.10903 	 ~...
   92 	    62 	 0.12550 	 0.11691 	 ~...
   89 	    63 	 0.11983 	 0.11981 	 ~...
   36 	    64 	 0.08428 	 0.12016 	 m..s
   91 	    65 	 0.12004 	 0.12023 	 ~...
   62 	    66 	 0.10251 	 0.12862 	 ~...
   46 	    67 	 0.08771 	 0.12923 	 m..s
   39 	    68 	 0.08462 	 0.13014 	 m..s
   37 	    69 	 0.08438 	 0.13096 	 m..s
   38 	    70 	 0.08439 	 0.13455 	 m..s
  100 	    71 	 0.15951 	 0.13529 	 ~...
   47 	    72 	 0.08779 	 0.13770 	 m..s
   94 	    73 	 0.12728 	 0.13844 	 ~...
   43 	    74 	 0.08689 	 0.13845 	 m..s
   96 	    75 	 0.12766 	 0.13924 	 ~...
   56 	    76 	 0.09584 	 0.14866 	 m..s
  101 	    77 	 0.17574 	 0.15089 	 ~...
   57 	    78 	 0.09708 	 0.15248 	 m..s
   32 	    79 	 0.08009 	 0.15364 	 m..s
   62 	    80 	 0.10251 	 0.15802 	 m..s
   76 	    81 	 0.10396 	 0.15954 	 m..s
   62 	    82 	 0.10251 	 0.15961 	 m..s
   60 	    83 	 0.10179 	 0.16167 	 m..s
   35 	    84 	 0.08238 	 0.16317 	 m..s
   58 	    85 	 0.09985 	 0.16434 	 m..s
   62 	    86 	 0.10251 	 0.16467 	 m..s
   62 	    87 	 0.10251 	 0.16538 	 m..s
   77 	    88 	 0.10493 	 0.16650 	 m..s
   78 	    89 	 0.10503 	 0.16961 	 m..s
   49 	    90 	 0.08890 	 0.17275 	 m..s
   61 	    91 	 0.10243 	 0.17299 	 m..s
   75 	    92 	 0.10336 	 0.17550 	 m..s
   54 	    93 	 0.09486 	 0.17979 	 m..s
   79 	    94 	 0.10687 	 0.17989 	 m..s
   62 	    95 	 0.10251 	 0.18067 	 m..s
  104 	    96 	 0.19253 	 0.18087 	 ~...
   82 	    97 	 0.10776 	 0.18219 	 m..s
  102 	    98 	 0.19047 	 0.19186 	 ~...
  103 	    99 	 0.19062 	 0.19264 	 ~...
   99 	   100 	 0.15488 	 0.19300 	 m..s
   98 	   101 	 0.15477 	 0.19419 	 m..s
  110 	   102 	 0.21069 	 0.19522 	 ~...
  106 	   103 	 0.19656 	 0.19634 	 ~...
  109 	   104 	 0.20236 	 0.20093 	 ~...
   83 	   105 	 0.10930 	 0.20369 	 m..s
   86 	   106 	 0.11098 	 0.20560 	 m..s
   87 	   107 	 0.11389 	 0.20578 	 m..s
  107 	   108 	 0.20056 	 0.20601 	 ~...
  108 	   109 	 0.20058 	 0.20832 	 ~...
  111 	   110 	 0.21712 	 0.21220 	 ~...
  105 	   111 	 0.19646 	 0.22588 	 ~...
  117 	   112 	 0.23310 	 0.22640 	 ~...
  112 	   113 	 0.21873 	 0.23096 	 ~...
  119 	   114 	 0.25828 	 0.23346 	 ~...
  113 	   115 	 0.21902 	 0.23835 	 ~...
  114 	   116 	 0.22076 	 0.24363 	 ~...
  118 	   117 	 0.25355 	 0.26101 	 ~...
  116 	   118 	 0.23294 	 0.26223 	 ~...
  120 	   119 	 0.26946 	 0.26667 	 ~...
  115 	   120 	 0.23268 	 0.26909 	 m..s
==========================================
r_mrr = 0.7447080016136169
r2_mrr = 0.5124173164367676
spearmanr_mrr@5 = 0.7828935384750366
spearmanr_mrr@10 = 0.9218358993530273
spearmanr_mrr@50 = 0.9641531109809875
spearmanr_mrr@100 = 0.8509724736213684
spearmanr_mrr@All = 0.8852075934410095
==========================================
test time: 0.392
Done Testing dataset CoDExSmall
total time taken: 209.748920917511
training time taken: 200.0389323234558
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7447)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5124)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.7829)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.9218)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9642)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8510)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8852)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.3216437707305886}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 602161709425367
Starting TWIG!
Loading datasets
Loading TransE...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [583, 551, 866, 657, 259, 465, 659, 58, 320, 1064, 78, 598, 424, 143, 812, 427, 715, 743, 716, 119, 476, 1045, 1095, 1201, 250, 818, 964, 568, 1123, 532, 742, 218, 815, 1147, 305, 601, 655, 1207, 42, 709, 264, 967, 989, 245, 711, 787, 806, 908, 53, 690, 70, 483, 50, 983, 1039, 92, 595, 828, 564, 1113, 339, 641, 438, 98, 539, 674, 757, 304, 1055, 534, 834, 960, 96, 650, 1140, 21, 1132, 826, 451, 361, 1187, 199, 47, 45, 766, 25, 379, 443, 224, 1189, 410, 139, 938, 1071, 557, 87, 1003, 767, 689, 425, 499, 852, 280, 94, 240, 306, 229, 404, 769, 615, 809, 848, 315, 553, 393, 880, 286, 473, 55, 700, 981]
valid_ids (0): []
train_ids (1094): [118, 1198, 1067, 755, 801, 11, 990, 687, 204, 108, 731, 1088, 608, 719, 1162, 833, 955, 811, 399, 1195, 431, 249, 1041, 508, 1112, 61, 524, 63, 822, 105, 185, 799, 442, 581, 756, 279, 82, 153, 702, 906, 19, 737, 708, 1120, 631, 1202, 879, 114, 323, 459, 786, 537, 299, 441, 497, 391, 490, 471, 390, 696, 46, 800, 322, 588, 673, 248, 904, 40, 289, 1103, 751, 1186, 444, 661, 559, 946, 721, 781, 179, 995, 999, 850, 837, 470, 593, 883, 717, 857, 1065, 232, 398, 209, 831, 969, 1079, 182, 876, 1094, 243, 353, 916, 724, 985, 206, 1153, 710, 88, 293, 996, 734, 336, 356, 90, 918, 591, 177, 1155, 3, 522, 1044, 1058, 176, 513, 663, 518, 874, 448, 408, 738, 367, 622, 6, 872, 360, 932, 447, 630, 437, 1209, 359, 713, 378, 1139, 1200, 155, 491, 1030, 362, 963, 382, 258, 901, 197, 34, 1072, 211, 265, 685, 17, 1046, 327, 516, 162, 853, 397, 798, 373, 902, 1026, 191, 352, 1061, 1125, 1069, 794, 541, 183, 174, 1119, 457, 1012, 4, 529, 693, 222, 219, 54, 52, 129, 103, 914, 36, 939, 157, 1197, 954, 774, 619, 558, 1036, 648, 554, 260, 384, 125, 607, 1062, 924, 1097, 933, 749, 288, 194, 582, 1037, 504, 664, 763, 1118, 329, 682, 894, 579, 484, 808, 1019, 1089, 328, 500, 1051, 1056, 184, 779, 201, 796, 950, 928, 1060, 15, 546, 851, 10, 406, 86, 152, 235, 686, 489, 1034, 1165, 727, 220, 670, 349, 364, 1005, 308, 1134, 758, 32, 523, 1059, 1110, 1031, 1160, 333, 978, 71, 186, 895, 462, 810, 135, 1068, 44, 163, 775, 1048, 110, 403, 703, 1047, 1106, 945, 1188, 324, 345, 458, 449, 885, 549, 262, 1133, 621, 411, 600, 547, 846, 231, 120, 417, 1091, 440, 246, 91, 1029, 464, 318, 1050, 535, 344, 971, 1137, 509, 861, 421, 791, 991, 538, 841, 838, 1028, 637, 1111, 503, 639, 651, 966, 266, 886, 590, 980, 530, 1013, 839, 498, 836, 854, 375, 132, 1211, 432, 667, 1178, 772, 512, 175, 270, 370, 354, 587, 22, 131, 330, 691, 705, 455, 226, 863, 43, 620, 893, 1168, 698, 1146, 1022, 975, 221, 979, 962, 366, 272, 446, 627, 778, 435, 241, 493, 931, 433, 677, 1135, 783, 1169, 909, 23, 877, 217, 422, 884, 141, 927, 343, 1194, 819, 407, 1175, 169, 212, 575, 35, 80, 935, 1156, 301, 342, 987, 488, 254, 242, 227, 198, 905, 1126, 79, 126, 122, 26, 671, 142, 210, 511, 515, 656, 74, 452, 357, 507, 603, 552, 389, 436, 665, 792, 797, 445, 1163, 602, 430, 1070, 1117, 392, 236, 1042, 643, 1124, 773, 121, 947, 616, 612, 1021, 128, 744, 1171, 707, 666, 481, 675, 1170, 1205, 255, 423, 64, 247, 816, 316, 1084, 681, 84, 735, 982, 326, 171, 596, 1213, 1087, 574, 725, 31, 1075, 127, 1074, 771, 1199, 130, 144, 37, 67, 823, 560, 1172, 635, 1129, 624, 526, 626, 911, 412, 291, 814, 913, 992, 533, 146, 625, 542, 334, 202, 337, 269, 563, 18, 567, 521, 1001, 1007, 2, 594, 62, 986, 213, 369, 623, 1157, 561, 290, 654, 660, 109, 943, 907, 573, 386, 517, 672, 576, 496, 368, 81, 860, 495, 1014, 405, 428, 817, 388, 695, 889, 592, 73, 477, 387, 613, 303, 56, 479, 60, 844, 1142, 658, 494, 365, 68, 51, 180, 85, 520, 281, 33, 788, 1138, 562, 486, 414, 543, 694, 253, 99, 413, 1077, 793, 1174, 314, 920, 346, 688, 145, 380, 205, 1158, 1073, 977, 636, 865, 1090, 502, 418, 1109, 903, 728, 482, 284, 570, 394, 454, 113, 1141, 974, 456, 1104, 1011, 569, 917, 1009, 475, 385, 97, 1183, 891, 402, 873, 528, 878, 89, 138, 1144, 506, 765, 605, 460, 115, 803, 27, 732, 642, 300, 965, 921, 958, 1114, 325, 1150, 611, 919, 813, 168, 669, 1105, 736, 57, 1159, 701, 915, 1057, 1086, 739, 1085, 1035, 401, 332, 776, 167, 1016, 1099, 1180, 505, 759, 697, 790, 1098, 1025, 28, 453, 1167, 652, 887, 133, 572, 780, 159, 1015, 429, 720, 548, 383, 825, 203, 501, 287, 712, 173, 1116, 519, 948, 647, 148, 514, 937, 868, 59, 376, 550, 770, 24, 1093, 1152, 1040, 396, 644, 633, 589, 942, 347, 117, 225, 922, 750, 1204, 678, 555, 485, 72, 899, 843, 39, 892, 76, 820, 1032, 338, 492, 350, 1149, 156, 953, 525, 718, 1179, 976, 381, 1143, 1006, 251, 733, 1131, 363, 1184, 341, 1190, 706, 134, 295, 1148, 30, 944, 746, 881, 545, 1206, 832, 102, 261, 1101, 752, 745, 1128, 1010, 923, 890, 420, 930, 1121, 150, 480, 164, 450, 586, 859, 1063, 302, 351, 1177, 298, 1161, 1027, 1182, 1164, 1130, 956, 910, 882, 319, 434, 870, 994, 1052, 277, 740, 649, 556, 107, 310, 1033, 730, 584, 181, 867, 187, 668, 1078, 604, 292, 172, 154, 215, 1049, 1108, 14, 845, 638, 577, 871, 760, 234, 925, 566, 158, 69, 609, 565, 223, 439, 1081, 1193, 311, 415, 1127, 645, 161, 170, 1122, 748, 864, 137, 101, 926, 862, 340, 1203, 267, 123, 487, 1176, 20, 952, 1004, 377, 274, 789, 296, 200, 355, 228, 140, 936, 683, 540, 847, 1192, 371, 961, 1000, 722, 723, 805, 75, 188, 151, 1020, 729, 1023, 988, 400, 632, 165, 273, 951, 1173, 472, 940, 634, 41, 1100, 777, 824, 466, 1080, 941, 1038, 1, 684, 900, 1181, 680, 48, 196, 1136, 348, 1066, 16, 1151, 1107, 256, 840, 510, 761, 571, 29, 998, 929, 136, 317, 1008, 233, 959, 312, 9, 409, 536, 762, 704, 276, 544, 372, 830, 193, 469, 106, 416, 527, 160, 331, 244, 842, 973, 897, 1102, 802, 753, 395, 124, 83, 214, 309, 934, 849, 238, 285, 461, 804, 997, 888, 100, 699, 679, 912, 653, 147, 896, 640, 1196, 112, 77, 149, 467, 252, 856, 785, 104, 617, 13, 297, 1092, 321, 95, 768, 578, 764, 307, 374, 629, 463, 784, 949, 257, 875, 858, 1096, 178, 898, 66, 335, 271, 65, 782, 662, 599, 192, 8, 754, 474, 237, 468, 827, 1017, 957, 294, 618, 313, 1191, 1212, 426, 968, 1214, 195, 597, 49, 606, 676, 610, 1210, 116, 1208, 614, 646, 263, 239, 970, 726, 747, 478, 1145, 283, 984, 628, 12, 580, 216, 190, 38, 692, 275, 714, 585, 1002, 7, 1185, 207, 972, 278, 93, 268, 111, 855, 807, 531, 1115, 835, 1018, 1166, 0, 1054, 1082, 5, 230, 189, 829, 419, 869, 166, 741, 1024, 1154, 795, 282, 1043, 208, 1076, 358, 993, 1083, 1053, 821]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  599702261446070
the save name prefix for this run is:  chkpt-ID_599702261446070_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'TransE': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1065
rank avg (pred): 0.514 +- 0.005
mrr vals (pred, true): 0.001, 0.239
batch losses (mrrl, rdl): 0.0, 0.0046864422

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 560
rank avg (pred): 0.156 +- 0.087
mrr vals (pred, true): 0.017, 0.041
batch losses (mrrl, rdl): 0.0, 2.36625e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 587
rank avg (pred): 0.290 +- 0.245
mrr vals (pred, true): 0.253, 0.021
batch losses (mrrl, rdl): 0.0, 1.54249e-05

Epoch over!
epoch time: 13.25

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1016
rank avg (pred): 0.278 +- 0.241
mrr vals (pred, true): 0.280, 0.218
batch losses (mrrl, rdl): 0.0, 0.000971391

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 153
rank avg (pred): 0.280 +- 0.253
mrr vals (pred, true): 0.346, 0.120
batch losses (mrrl, rdl): 0.0, 0.0007231446

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 956
rank avg (pred): 0.273 +- 0.263
mrr vals (pred, true): 0.398, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004401438

Epoch over!
epoch time: 13.152

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 530
rank avg (pred): 0.145 +- 0.141
mrr vals (pred, true): 0.412, 0.036
batch losses (mrrl, rdl): 0.0, 7.1347e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 415
rank avg (pred): 0.256 +- 0.251
mrr vals (pred, true): 0.418, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005389191

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 242
rank avg (pred): 0.236 +- 0.234
mrr vals (pred, true): 0.420, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007948396

Epoch over!
epoch time: 14.051

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 371
rank avg (pred): 0.249 +- 0.242
mrr vals (pred, true): 0.410, 0.192
batch losses (mrrl, rdl): 0.0, 0.0006363427

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 210
rank avg (pred): 0.254 +- 0.244
mrr vals (pred, true): 0.403, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006650133

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 336
rank avg (pred): 0.272 +- 0.257
mrr vals (pred, true): 0.399, 0.119
batch losses (mrrl, rdl): 0.0, 0.0004880544

Epoch over!
epoch time: 13.343

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 209
rank avg (pred): 0.257 +- 0.253
mrr vals (pred, true): 0.420, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005339946

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.307 +- 0.300
mrr vals (pred, true): 0.419, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003015419

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 611
rank avg (pred): 0.324 +- 0.303
mrr vals (pred, true): 0.399, 0.026
batch losses (mrrl, rdl): 0.0, 8.53655e-05

Epoch over!
epoch time: 13.253

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.258 +- 0.251
mrr vals (pred, true): 0.412, 0.004
batch losses (mrrl, rdl): 1.3085289001, 0.000568611

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 750
rank avg (pred): 0.134 +- 0.110
mrr vals (pred, true): 0.128, 0.145
batch losses (mrrl, rdl): 0.0028257361, 0.0001526594

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 630
rank avg (pred): 0.441 +- 0.193
mrr vals (pred, true): 0.047, 0.007
batch losses (mrrl, rdl): 7.20152e-05, 0.0002978506

Epoch over!
epoch time: 14.305

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1170
rank avg (pred): 0.428 +- 0.199
mrr vals (pred, true): 0.054, 0.011
batch losses (mrrl, rdl): 0.0001954639, 0.0003184346

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 693
rank avg (pred): 0.428 +- 0.187
mrr vals (pred, true): 0.053, 0.003
batch losses (mrrl, rdl): 8.37771e-05, 4.18268e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 882
rank avg (pred): 0.322 +- 0.215
mrr vals (pred, true): 0.103, 0.005
batch losses (mrrl, rdl): 0.0284311548, 0.0002414532

Epoch over!
epoch time: 14.055

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 914
rank avg (pred): 0.331 +- 0.214
mrr vals (pred, true): 0.094, 0.149
batch losses (mrrl, rdl): 0.0296656489, 0.0018563629

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.146 +- 0.121
mrr vals (pred, true): 0.136, 0.205
batch losses (mrrl, rdl): 0.0474830121, 0.0001906184

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1041
rank avg (pred): 0.171 +- 0.138
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0447934307, 0.001612416

Epoch over!
epoch time: 12.596

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.308 +- 0.207
mrr vals (pred, true): 0.103, 0.142
batch losses (mrrl, rdl): 0.0157300532, 0.0009536572

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 705
rank avg (pred): 0.409 +- 0.160
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.0010585514, 9.4684e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 895
rank avg (pred): 0.334 +- 0.193
mrr vals (pred, true): 0.087, 0.037
batch losses (mrrl, rdl): 0.0133834332, 0.0012596494

Epoch over!
epoch time: 12.106

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 273
rank avg (pred): 0.130 +- 0.110
mrr vals (pred, true): 0.171, 0.152
batch losses (mrrl, rdl): 0.0037439335, 6.61672e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 796
rank avg (pred): 0.317 +- 0.190
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.0164584052, 0.0003443154

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 897
rank avg (pred): 0.328 +- 0.184
mrr vals (pred, true): 0.085, 0.028
batch losses (mrrl, rdl): 0.0119305439, 0.0010875631

Epoch over!
epoch time: 12.511

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1028
rank avg (pred): 0.270 +- 0.191
mrr vals (pred, true): 0.127, 0.004
batch losses (mrrl, rdl): 0.0588628724, 0.0006392653

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 718
rank avg (pred): 0.381 +- 0.141
mrr vals (pred, true): 0.038, 0.003
batch losses (mrrl, rdl): 0.0013964644, 0.0001992121

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 354
rank avg (pred): 0.295 +- 0.185
mrr vals (pred, true): 0.117, 0.122
batch losses (mrrl, rdl): 0.000190333, 0.0004901936

Epoch over!
epoch time: 12.98

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 604
rank avg (pred): 0.356 +- 0.149
mrr vals (pred, true): 0.065, 0.010
batch losses (mrrl, rdl): 0.0023525264, 8.5526e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.310 +- 0.177
mrr vals (pred, true): 0.092, 0.112
batch losses (mrrl, rdl): 0.0041158199, 0.0005181651

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 1094
rank avg (pred): 0.256 +- 0.167
mrr vals (pred, true): 0.110, 0.209
batch losses (mrrl, rdl): 0.0974696949, 0.0006645847

Epoch over!
epoch time: 12.94

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 915
rank avg (pred): 0.285 +- 0.177
mrr vals (pred, true): 0.105, 0.153
batch losses (mrrl, rdl): 0.0225562584, 0.0013622543

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 204
rank avg (pred): 0.289 +- 0.171
mrr vals (pred, true): 0.110, 0.004
batch losses (mrrl, rdl): 0.0365133509, 0.0005897845

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.334 +- 0.142
mrr vals (pred, true): 0.065, 0.027
batch losses (mrrl, rdl): 0.0021365755, 0.0006435391

Epoch over!
epoch time: 12.411

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 121
rank avg (pred): 0.292 +- 0.168
mrr vals (pred, true): 0.103, 0.138
batch losses (mrrl, rdl): 0.0123659428, 0.0008246197

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 833
rank avg (pred): 0.161 +- 0.109
mrr vals (pred, true): 0.117, 0.152
batch losses (mrrl, rdl): 0.0121600777, 0.0001970949

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 344
rank avg (pred): 0.246 +- 0.157
mrr vals (pred, true): 0.122, 0.191
batch losses (mrrl, rdl): 0.0470514409, 0.0005080793

Epoch over!
epoch time: 12.12

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 480
rank avg (pred): 0.306 +- 0.157
mrr vals (pred, true): 0.082, 0.004
batch losses (mrrl, rdl): 0.0100363512, 0.0004776419

running batch: 500 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 30
rank avg (pred): 0.132 +- 0.096
mrr vals (pred, true): 0.167, 0.149
batch losses (mrrl, rdl): 0.0032958216, 0.0001102568

running batch: 1000 / 1094 and superbatch(1); data from TransE, CoDExSmall, run 2.1, exp 962
rank avg (pred): 0.280 +- 0.160
mrr vals (pred, true): 0.100, 0.005
batch losses (mrrl, rdl): 0.0251720175, 0.0005861705

Epoch over!
epoch time: 12.191

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.340 +- 0.118
mrr vals (pred, true): 0.036, 0.010

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.03370 	 0.00327 	 m..s
    2 	     1 	 0.03182 	 0.00334 	 ~...
   22 	     2 	 0.03809 	 0.00339 	 m..s
    7 	     3 	 0.03428 	 0.00340 	 m..s
   28 	     4 	 0.04167 	 0.00347 	 m..s
    3 	     5 	 0.03186 	 0.00348 	 ~...
   40 	     6 	 0.09072 	 0.00349 	 m..s
    6 	     7 	 0.03419 	 0.00349 	 m..s
   83 	     8 	 0.12164 	 0.00350 	 MISS
   24 	     9 	 0.03900 	 0.00353 	 m..s
   19 	    10 	 0.03739 	 0.00364 	 m..s
   69 	    11 	 0.10155 	 0.00370 	 m..s
    0 	    12 	 0.02896 	 0.00371 	 ~...
   40 	    13 	 0.09072 	 0.00372 	 m..s
    8 	    14 	 0.03435 	 0.00373 	 m..s
   82 	    15 	 0.11941 	 0.00378 	 MISS
   13 	    16 	 0.03590 	 0.00379 	 m..s
   18 	    17 	 0.03725 	 0.00383 	 m..s
   68 	    18 	 0.10122 	 0.00388 	 m..s
   49 	    19 	 0.09285 	 0.00390 	 m..s
   59 	    20 	 0.09727 	 0.00394 	 m..s
   84 	    21 	 0.12341 	 0.00395 	 MISS
   62 	    22 	 0.09783 	 0.00412 	 m..s
   77 	    23 	 0.10761 	 0.00422 	 MISS
   15 	    24 	 0.03619 	 0.00424 	 m..s
   76 	    25 	 0.10649 	 0.00426 	 MISS
    5 	    26 	 0.03412 	 0.00427 	 ~...
   85 	    27 	 0.12343 	 0.00435 	 MISS
   47 	    28 	 0.09228 	 0.00435 	 m..s
   52 	    29 	 0.09517 	 0.00437 	 m..s
   72 	    30 	 0.10418 	 0.00441 	 m..s
   60 	    31 	 0.09770 	 0.00445 	 m..s
   55 	    32 	 0.09597 	 0.00451 	 m..s
   40 	    33 	 0.09072 	 0.00456 	 m..s
   40 	    34 	 0.09072 	 0.00461 	 m..s
   63 	    35 	 0.09833 	 0.00464 	 m..s
   70 	    36 	 0.10357 	 0.00464 	 m..s
   51 	    37 	 0.09507 	 0.00465 	 m..s
   33 	    38 	 0.07757 	 0.00472 	 m..s
   37 	    39 	 0.08218 	 0.00485 	 m..s
   64 	    40 	 0.09879 	 0.00494 	 m..s
   71 	    41 	 0.10380 	 0.00499 	 m..s
   58 	    42 	 0.09672 	 0.00506 	 m..s
   74 	    43 	 0.10444 	 0.00528 	 m..s
   20 	    44 	 0.03751 	 0.00540 	 m..s
   11 	    45 	 0.03557 	 0.01032 	 ~...
   23 	    46 	 0.03893 	 0.01066 	 ~...
   26 	    47 	 0.04028 	 0.01098 	 ~...
   14 	    48 	 0.03599 	 0.01230 	 ~...
   16 	    49 	 0.03650 	 0.01274 	 ~...
   12 	    50 	 0.03578 	 0.01283 	 ~...
   29 	    51 	 0.04293 	 0.01584 	 ~...
    9 	    52 	 0.03479 	 0.01683 	 ~...
   30 	    53 	 0.04694 	 0.01762 	 ~...
    1 	    54 	 0.02992 	 0.02020 	 ~...
   27 	    55 	 0.04141 	 0.02042 	 ~...
   17 	    56 	 0.03673 	 0.02079 	 ~...
   32 	    57 	 0.04924 	 0.02264 	 ~...
   21 	    58 	 0.03760 	 0.02559 	 ~...
   25 	    59 	 0.03975 	 0.03752 	 ~...
   31 	    60 	 0.04887 	 0.03904 	 ~...
   10 	    61 	 0.03523 	 0.04216 	 ~...
   81 	    62 	 0.11862 	 0.07709 	 m..s
   36 	    63 	 0.08190 	 0.09528 	 ~...
   35 	    64 	 0.08041 	 0.11016 	 ~...
   87 	    65 	 0.13141 	 0.11143 	 ~...
   79 	    66 	 0.11547 	 0.12023 	 ~...
   88 	    67 	 0.13392 	 0.12113 	 ~...
   38 	    68 	 0.08405 	 0.12266 	 m..s
   40 	    69 	 0.09072 	 0.12904 	 m..s
   40 	    70 	 0.09072 	 0.13081 	 m..s
   39 	    71 	 0.08479 	 0.13774 	 m..s
   90 	    72 	 0.13853 	 0.14063 	 ~...
   89 	    73 	 0.13820 	 0.14560 	 ~...
   46 	    74 	 0.09178 	 0.15171 	 m..s
   80 	    75 	 0.11784 	 0.15323 	 m..s
   48 	    76 	 0.09246 	 0.15622 	 m..s
   34 	    77 	 0.07911 	 0.15860 	 m..s
   50 	    78 	 0.09420 	 0.16467 	 m..s
   61 	    79 	 0.09779 	 0.16538 	 m..s
   56 	    80 	 0.09647 	 0.16554 	 m..s
   54 	    81 	 0.09548 	 0.16632 	 m..s
   73 	    82 	 0.10438 	 0.16773 	 m..s
   92 	    83 	 0.16607 	 0.16779 	 ~...
   53 	    84 	 0.09537 	 0.16854 	 m..s
   67 	    85 	 0.10036 	 0.17219 	 m..s
   98 	    86 	 0.17793 	 0.17282 	 ~...
   65 	    87 	 0.09880 	 0.17298 	 m..s
   66 	    88 	 0.09951 	 0.17299 	 m..s
  101 	    89 	 0.18314 	 0.17472 	 ~...
   57 	    90 	 0.09663 	 0.17811 	 m..s
   91 	    91 	 0.16286 	 0.17866 	 ~...
   95 	    92 	 0.17424 	 0.17869 	 ~...
   94 	    93 	 0.17345 	 0.17881 	 ~...
   99 	    94 	 0.17833 	 0.18006 	 ~...
  106 	    95 	 0.20473 	 0.18173 	 ~...
  113 	    96 	 0.21754 	 0.18393 	 m..s
  112 	    97 	 0.21643 	 0.18868 	 ~...
   97 	    98 	 0.17721 	 0.18990 	 ~...
   78 	    99 	 0.11508 	 0.19002 	 m..s
   96 	   100 	 0.17719 	 0.19182 	 ~...
  107 	   101 	 0.20578 	 0.19254 	 ~...
  115 	   102 	 0.22291 	 0.19522 	 ~...
  114 	   103 	 0.21878 	 0.19574 	 ~...
   93 	   104 	 0.17211 	 0.20029 	 ~...
  109 	   105 	 0.21356 	 0.20365 	 ~...
  110 	   106 	 0.21429 	 0.20396 	 ~...
  111 	   107 	 0.21484 	 0.20595 	 ~...
   75 	   108 	 0.10545 	 0.20818 	 MISS
  100 	   109 	 0.17901 	 0.20832 	 ~...
  104 	   110 	 0.18778 	 0.20951 	 ~...
   86 	   111 	 0.12444 	 0.21271 	 m..s
  102 	   112 	 0.18328 	 0.21490 	 m..s
  108 	   113 	 0.21310 	 0.21680 	 ~...
  105 	   114 	 0.18941 	 0.22076 	 m..s
  103 	   115 	 0.18610 	 0.22588 	 m..s
  117 	   116 	 0.25428 	 0.23346 	 ~...
  116 	   117 	 0.25196 	 0.23706 	 ~...
  118 	   118 	 0.25506 	 0.23952 	 ~...
  119 	   119 	 0.25529 	 0.25940 	 ~...
  120 	   120 	 0.26092 	 0.26713 	 ~...
==========================================
r_mrr = 0.7897294759750366
r2_mrr = 0.5741276741027832
spearmanr_mrr@5 = 0.8585942387580872
spearmanr_mrr@10 = 0.8837388753890991
spearmanr_mrr@50 = 0.9634291529655457
spearmanr_mrr@100 = 0.8740105628967285
spearmanr_mrr@All = 0.8996791243553162
==========================================
test time: 0.484
Done Testing dataset CoDExSmall
total time taken: 206.4343822002411
training time taken: 195.83586931228638
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'CoDExSmall': tensor(0.7897)}}, 'r2_mrr': {'TransE': {'CoDExSmall': tensor(0.5741)}}, 'spearmanr_mrr@5': {'TransE': {'CoDExSmall': tensor(0.8586)}}, 'spearmanr_mrr@10': {'TransE': {'CoDExSmall': tensor(0.8837)}}, 'spearmanr_mrr@50': {'TransE': {'CoDExSmall': tensor(0.9634)}}, 'spearmanr_mrr@100': {'TransE': {'CoDExSmall': tensor(0.8740)}}, 'spearmanr_mrr@All': {'TransE': {'CoDExSmall': tensor(0.8997)}}, 'test_loss': {'TransE': {'CoDExSmall': 2.0384148394223303}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 8004585941334529
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [269, 920, 1178, 370, 129, 596, 853, 957, 908, 422, 861, 156, 754, 702, 891, 55, 765, 731, 209, 670, 514, 332, 1128, 1018, 1043, 941, 955, 397, 1202, 16, 516, 163, 432, 1076, 1097, 838, 198, 474, 986, 544, 638, 415, 231, 818, 686, 36, 712, 895, 915, 41, 546, 182, 1013, 61, 216, 305, 453, 962, 880, 535, 19, 537, 420, 43, 777, 902, 242, 1160, 1056, 600, 555, 884, 82, 945, 229, 1195, 877, 1136, 1015, 191, 612, 1085, 899, 931, 22, 873, 463, 746, 718, 570, 1193, 1102, 926, 1000, 1208, 339, 306, 490, 890, 134, 609, 121, 855, 476, 414, 888, 970, 426, 495, 572, 462, 882, 693, 1164, 952, 905, 684, 1007, 510, 1016, 930]
valid_ids (0): []
train_ids (1094): [1155, 131, 714, 312, 831, 192, 646, 508, 97, 1059, 918, 656, 440, 213, 876, 1023, 579, 985, 742, 423, 90, 30, 141, 640, 214, 975, 906, 469, 536, 1072, 1051, 849, 342, 1074, 1024, 889, 762, 1166, 732, 886, 629, 1048, 867, 10, 841, 392, 524, 266, 408, 78, 1009, 525, 825, 735, 438, 3, 999, 369, 815, 429, 784, 1040, 206, 433, 1036, 1100, 786, 11, 268, 1163, 1006, 154, 1148, 947, 284, 446, 245, 195, 382, 690, 388, 1168, 790, 792, 959, 736, 310, 1189, 549, 107, 625, 256, 393, 218, 193, 1027, 1196, 152, 262, 692, 604, 79, 713, 897, 738, 817, 733, 44, 40, 145, 324, 1144, 487, 606, 630, 522, 329, 892, 1068, 402, 724, 1146, 361, 1050, 334, 641, 1135, 364, 404, 1127, 751, 1037, 345, 101, 1021, 359, 576, 375, 607, 940, 354, 343, 139, 326, 367, 628, 937, 740, 111, 563, 1083, 114, 605, 1025, 688, 834, 568, 1080, 37, 898, 548, 1062, 1124, 616, 936, 878, 1120, 559, 452, 865, 956, 1, 710, 466, 1147, 648, 62, 618, 248, 827, 530, 990, 587, 1042, 309, 781, 1153, 128, 125, 564, 323, 390, 864, 919, 223, 34, 622, 1125, 814, 527, 807, 396, 140, 308, 866, 743, 602, 1031, 627, 1150, 1105, 672, 1093, 31, 1030, 820, 974, 418, 1142, 319, 796, 639, 1075, 830, 448, 1098, 939, 357, 562, 874, 1038, 389, 798, 922, 431, 540, 1212, 595, 86, 964, 542, 836, 811, 144, 589, 489, 557, 178, 511, 1179, 179, 727, 1129, 270, 954, 381, 938, 1122, 1114, 21, 1033, 806, 913, 1101, 185, 1073, 38, 430, 943, 519, 997, 721, 681, 631, 149, 437, 25, 227, 1133, 633, 824, 552, 881, 745, 483, 278, 6, 518, 108, 917, 1069, 482, 935, 445, 887, 528, 1061, 328, 60, 1077, 346, 497, 53, 135, 585, 934, 24, 479, 93, 1206, 984, 809, 883, 805, 680, 23, 5, 74, 427, 653, 228, 117, 51, 142, 478, 1175, 603, 837, 744, 9, 520, 289, 584, 54, 950, 571, 1032, 502, 973, 812, 789, 987, 443, 250, 33, 333, 835, 484, 927, 683, 772, 671, 200, 813, 492, 455, 1191, 706, 281, 729, 1003, 1165, 1065, 588, 283, 451, 1194, 1019, 1063, 803, 1113, 301, 726, 1204, 1055, 391, 59, 398, 267, 66, 533, 1103, 1197, 980, 608, 184, 907, 504, 948, 8, 1167, 1177, 791, 863, 362, 1078, 105, 903, 532, 788, 459, 292, 719, 251, 254, 293, 313, 1188, 802, 447, 635, 48, 481, 197, 655, 493, 660, 138, 341, 435, 320, 1034, 816, 57, 1108, 946, 350, 444, 285, 264, 158, 4, 159, 127, 541, 750, 967, 795, 371, 1205, 661, 1054, 110, 1190, 663, 132, 794, 632, 299, 1119, 253, 1200, 598, 868, 870, 993, 103, 473, 260, 160, 871, 842, 348, 1207, 989, 1181, 860, 416, 1180, 146, 783, 207, 829, 92, 747, 1084, 1210, 316, 166, 238, 1141, 574, 573, 409, 439, 210, 172, 924, 850, 591, 360, 475, 983, 569, 679, 1159, 928, 691, 302, 244, 896, 39, 538, 387, 566, 933, 1086, 912, 1071, 50, 304, 471, 161, 225, 311, 1096, 28, 190, 774, 116, 1092, 958, 164, 804, 95, 1070, 384, 336, 349, 383, 196, 626, 1186, 1154, 965, 461, 668, 64, 425, 77, 441, 823, 1058, 1104, 1214, 1213, 273, 534, 249, 279, 467, 45, 623, 219, 1192, 212, 720, 509, 1156, 748, 711, 406, 529, 979, 599, 852, 186, 872, 65, 1052, 428, 303, 122, 233, 1047, 1049, 230, 1008, 2, 1045, 317, 69, 457, 846, 488, 581, 1044, 1118, 298, 554, 124, 1134, 780, 717, 403, 331, 526, 921, 998, 202, 366, 1064, 769, 689, 1028, 910, 417, 685, 1137, 314, 464, 63, 297, 373, 75, 468, 1017, 173, 1089, 201, 372, 84, 1183, 87, 793, 1111, 81, 88, 678, 203, 971, 358, 755, 1138, 844, 330, 862, 1149, 171, 12, 1046, 505, 470, 47, 649, 737, 322, 465, 344, 904, 699, 376, 1117, 664, 168, 1131, 300, 176, 676, 611, 261, 411, 634, 153, 620, 76, 338, 85, 335, 966, 756, 222, 923, 286, 687, 725, 91, 352, 123, 673, 1012, 723, 550, 162, 1173, 799, 294, 593, 766, 205, 32, 1095, 1123, 840, 1203, 137, 610, 96, 650, 424, 378, 759, 401, 458, 234, 1187, 617, 102, 258, 377, 199, 115, 106, 925, 1109, 421, 1087, 1132, 1022, 208, 543, 592, 407, 480, 578, 991, 703, 645, 845, 859, 951, 94, 953, 911, 771, 356, 259, 399, 708, 477, 1116, 18, 636, 287, 1011, 1157, 739, 49, 1209, 553, 436, 130, 705, 819, 120, 1088, 558, 147, 1005, 169, 450, 151, 575, 113, 290, 1039, 778, 68, 288, 52, 263, 136, 275, 988, 1161, 770, 642, 1170, 696, 669, 856, 992, 561, 916, 583, 843, 1020, 385, 296, 380, 355, 647, 280, 1029, 351, 909, 282, 615, 722, 826, 315, 704, 507, 1099, 100, 460, 157, 978, 1106, 551, 839, 942, 1162, 80, 1041, 494, 307, 758, 1139, 71, 851, 15, 1145, 99, 1198, 601, 665, 1185, 728, 614, 810, 491, 741, 104, 694, 832, 1094, 594, 961, 565, 547, 662, 1090, 701, 26, 506, 1110, 700, 539, 972, 58, 217, 215, 67, 963, 885, 1014, 73, 1010, 340, 854, 624, 72, 667, 1152, 1171, 255, 347, 20, 1001, 666, 109, 70, 652, 764, 545, 513, 501, 413, 272, 1201, 211, 220, 232, 112, 932, 1057, 325, 643, 734, 808, 675, 879, 126, 98, 1199, 847, 405, 17, 695, 1053, 613, 83, 189, 637, 243, 386, 857, 150, 257, 621, 175, 716, 247, 496, 801, 177, 252, 1174, 1067, 1081, 761, 419, 658, 833, 27, 1107, 619, 914, 353, 828, 265, 194, 119, 749, 400, 800, 763, 240, 760, 869, 118, 1112, 976, 374, 982, 1182, 1115, 379, 531, 1176, 35, 821, 485, 944, 368, 13, 773, 89, 994, 960, 929, 295, 521, 556, 410, 500, 170, 567, 224, 752, 597, 442, 768, 682, 221, 365, 456, 776, 454, 472, 29, 582, 486, 499, 143, 204, 0, 894, 580, 654, 1151, 1184, 241, 321, 644, 785, 165, 981, 188, 657, 291, 767, 1121, 274, 226, 996, 7, 1026, 515, 995, 1169, 715, 858, 174, 394, 1140, 46, 586, 498, 56, 1035, 1158, 1082, 1079, 523, 14, 782, 787, 412, 677, 434, 797, 674, 512, 1130, 757, 42, 730, 155, 181, 183, 698, 327, 277, 1143, 276, 246, 1004, 1002, 337, 318, 822, 167, 949, 239, 659, 1126, 893, 503, 1172, 449, 1211, 1066, 709, 697, 779, 590, 363, 235, 1060, 395, 707, 577, 517, 753, 133, 901, 187, 271, 969, 900, 848, 1091, 236, 968, 775, 875, 560, 180, 237, 148, 977, 651]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9006216755560088
the save name prefix for this run is:  chkpt-ID_9006216755560088_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 85
rank avg (pred): 0.588 +- 0.002
mrr vals (pred, true): 0.000, 0.021
batch losses (mrrl, rdl): 0.0, 0.0022906614

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.307 +- 0.242
mrr vals (pred, true): 0.153, 0.000
batch losses (mrrl, rdl): 0.0, 0.0006693036

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 779
rank avg (pred): 0.348 +- 0.281
mrr vals (pred, true): 0.265, 0.153
batch losses (mrrl, rdl): 0.0, 0.0005790226

Epoch over!
epoch time: 12.061

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.315 +- 0.262
mrr vals (pred, true): 0.308, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003147053

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.342 +- 0.287
mrr vals (pred, true): 0.352, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002332335

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 533
rank avg (pred): 0.260 +- 0.225
mrr vals (pred, true): 0.376, 0.087
batch losses (mrrl, rdl): 0.0, 7.35519e-05

Epoch over!
epoch time: 11.914

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 146
rank avg (pred): 0.347 +- 0.300
mrr vals (pred, true): 0.373, 0.078
batch losses (mrrl, rdl): 0.0, 0.0006766126

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 447
rank avg (pred): 0.357 +- 0.308
mrr vals (pred, true): 0.378, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001249132

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 857
rank avg (pred): 0.325 +- 0.283
mrr vals (pred, true): 0.384, 0.150
batch losses (mrrl, rdl): 0.0, 0.0003835132

Epoch over!
epoch time: 12.569

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.342 +- 0.298
mrr vals (pred, true): 0.388, 0.003
batch losses (mrrl, rdl): 0.0, 5.81952e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.312 +- 0.272
mrr vals (pred, true): 0.391, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001799576

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.309 +- 0.269
mrr vals (pred, true): 0.393, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005494228

Epoch over!
epoch time: 12.158

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 433
rank avg (pred): 0.319 +- 0.278
mrr vals (pred, true): 0.393, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002182449

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.324 +- 0.282
mrr vals (pred, true): 0.395, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004313852

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 934
rank avg (pred): 0.346 +- 0.301
mrr vals (pred, true): 0.386, 0.146
batch losses (mrrl, rdl): 0.0, 0.0004955588

Epoch over!
epoch time: 11.796

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.349 +- 0.304
mrr vals (pred, true): 0.387, 0.000
batch losses (mrrl, rdl): 1.133960247, 0.0003255571

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 553
rank avg (pred): 0.272 +- 0.182
mrr vals (pred, true): 0.053, 0.059
batch losses (mrrl, rdl): 9.58558e-05, 9.24962e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.429 +- 0.187
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.000296423, 0.0001948739

Epoch over!
epoch time: 12.215

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.184 +- 0.156
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0259203445, 0.0016491732

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 738
rank avg (pred): 0.172 +- 0.185
mrr vals (pred, true): 0.124, 0.157
batch losses (mrrl, rdl): 0.0108209169, 0.0001359635

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.313 +- 0.173
mrr vals (pred, true): 0.099, 0.000
batch losses (mrrl, rdl): 0.0244849399, 0.0005355242

Epoch over!
epoch time: 11.922

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 841
rank avg (pred): 0.387 +- 0.178
mrr vals (pred, true): 0.134, 0.157
batch losses (mrrl, rdl): 0.0049278787, 0.0007450064

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.369 +- 0.175
mrr vals (pred, true): 0.077, 0.047
batch losses (mrrl, rdl): 0.0073348116, 0.0003167888

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 590
rank avg (pred): 0.404 +- 0.212
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 1.257e-07, 0.0002334299

Epoch over!
epoch time: 11.965

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 263
rank avg (pred): 0.162 +- 0.191
mrr vals (pred, true): 0.131, 0.271
batch losses (mrrl, rdl): 0.1946139485, 0.0001656079

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.428 +- 0.234
mrr vals (pred, true): 0.049, 0.072
batch losses (mrrl, rdl): 8.1308e-06, 0.0003022107

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1113
rank avg (pred): 0.315 +- 0.164
mrr vals (pred, true): 0.121, 0.001
batch losses (mrrl, rdl): 0.0499513038, 0.0006914733

Epoch over!
epoch time: 12.295

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1107
rank avg (pred): 0.357 +- 0.170
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0210393406, 9.57122e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.416 +- 0.204
mrr vals (pred, true): 0.070, 0.057
batch losses (mrrl, rdl): 0.004195468, 0.0005380377

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 87
rank avg (pred): 0.383 +- 0.173
mrr vals (pred, true): 0.068, 0.006
batch losses (mrrl, rdl): 0.0032350328, 0.0004112599

Epoch over!
epoch time: 12.139

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.355 +- 0.168
mrr vals (pred, true): 0.114, 0.000
batch losses (mrrl, rdl): 0.040548414, 0.0003166127

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.207 +- 0.147
mrr vals (pred, true): 0.095, 0.193
batch losses (mrrl, rdl): 0.0961911827, 0.0004306741

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 542
rank avg (pred): 0.352 +- 0.188
mrr vals (pred, true): 0.049, 0.095
batch losses (mrrl, rdl): 1.10814e-05, 0.0002713576

Epoch over!
epoch time: 12.2

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.420 +- 0.187
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 9.0269e-06, 0.0003155699

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.218 +- 0.172
mrr vals (pred, true): 0.118, 0.204
batch losses (mrrl, rdl): 0.0739848018, 0.0004717014

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.276 +- 0.166
mrr vals (pred, true): 0.081, 0.001
batch losses (mrrl, rdl): 0.0095318425, 0.0007049149

Epoch over!
epoch time: 12.181

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 759
rank avg (pred): 0.357 +- 0.176
mrr vals (pred, true): 0.102, 0.146
batch losses (mrrl, rdl): 0.0194023717, 0.0005162738

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.424 +- 0.214
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0005874367, 9.49668e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 52
rank avg (pred): 0.099 +- 0.178
mrr vals (pred, true): 0.144, 0.130
batch losses (mrrl, rdl): 0.0019814621, 5.696e-06

Epoch over!
epoch time: 12.127

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 556
rank avg (pred): 0.358 +- 0.203
mrr vals (pred, true): 0.057, 0.048
batch losses (mrrl, rdl): 0.000456669, 0.0002211245

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 223
rank avg (pred): 0.336 +- 0.170
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004636816, 0.000363607

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.218 +- 0.176
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0376011617, 0.0013210354

Epoch over!
epoch time: 11.924

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 657
rank avg (pred): 0.492 +- 0.233
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002074356, 2.45387e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 811
rank avg (pred): 0.127 +- 0.200
mrr vals (pred, true): 0.108, 0.087
batch losses (mrrl, rdl): 0.0342150591, 2.36162e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 984
rank avg (pred): 0.107 +- 0.189
mrr vals (pred, true): 0.153, 0.287
batch losses (mrrl, rdl): 0.1792149395, 6.08849e-05

Epoch over!
epoch time: 12.093

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.361 +- 0.176
mrr vals (pred, true): 0.107, 0.229

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   60 	     0 	 0.05830 	 0.00018 	 m..s
   13 	     1 	 0.05709 	 0.00018 	 m..s
  104 	     2 	 0.10920 	 0.00020 	 MISS
   90 	     3 	 0.10670 	 0.00021 	 MISS
   20 	     4 	 0.05711 	 0.00021 	 m..s
   26 	     5 	 0.05712 	 0.00021 	 m..s
   67 	     6 	 0.08230 	 0.00022 	 m..s
   18 	     7 	 0.05711 	 0.00022 	 m..s
   93 	     8 	 0.10724 	 0.00023 	 MISS
   21 	     9 	 0.05711 	 0.00023 	 m..s
  105 	    10 	 0.10954 	 0.00024 	 MISS
   98 	    11 	 0.10776 	 0.00024 	 MISS
   37 	    12 	 0.05724 	 0.00024 	 m..s
   63 	    13 	 0.05957 	 0.00025 	 m..s
    5 	    14 	 0.05708 	 0.00026 	 m..s
  107 	    15 	 0.10971 	 0.00027 	 MISS
    8 	    16 	 0.05708 	 0.00027 	 m..s
   17 	    17 	 0.05710 	 0.00027 	 m..s
   44 	    18 	 0.05729 	 0.00028 	 m..s
   32 	    19 	 0.05715 	 0.00028 	 m..s
   10 	    20 	 0.05708 	 0.00029 	 m..s
   69 	    21 	 0.08497 	 0.00029 	 m..s
   40 	    22 	 0.05728 	 0.00031 	 m..s
   38 	    23 	 0.05727 	 0.00036 	 m..s
   64 	    24 	 0.05989 	 0.00036 	 m..s
    4 	    25 	 0.05708 	 0.00036 	 m..s
   53 	    26 	 0.05757 	 0.00037 	 m..s
   33 	    27 	 0.05715 	 0.00038 	 m..s
    1 	    28 	 0.05708 	 0.00039 	 m..s
   41 	    29 	 0.05728 	 0.00039 	 m..s
   52 	    30 	 0.05746 	 0.00040 	 m..s
  100 	    31 	 0.10779 	 0.00041 	 MISS
   55 	    32 	 0.05765 	 0.00042 	 m..s
    1 	    33 	 0.05708 	 0.00042 	 m..s
   24 	    34 	 0.05712 	 0.00042 	 m..s
   14 	    35 	 0.05709 	 0.00051 	 m..s
   66 	    36 	 0.06039 	 0.00061 	 m..s
   95 	    37 	 0.10770 	 0.00063 	 MISS
   65 	    38 	 0.06023 	 0.00071 	 m..s
   61 	    39 	 0.05857 	 0.00074 	 m..s
   50 	    40 	 0.05738 	 0.00076 	 m..s
   49 	    41 	 0.05734 	 0.00087 	 m..s
   12 	    42 	 0.05709 	 0.00088 	 m..s
   28 	    43 	 0.05712 	 0.00100 	 m..s
   51 	    44 	 0.05745 	 0.00111 	 m..s
   22 	    45 	 0.05711 	 0.00195 	 m..s
   16 	    46 	 0.05710 	 0.00217 	 m..s
   15 	    47 	 0.05710 	 0.00221 	 m..s
   86 	    48 	 0.10577 	 0.00227 	 MISS
   23 	    49 	 0.05711 	 0.00479 	 m..s
   29 	    50 	 0.05712 	 0.01381 	 m..s
   56 	    51 	 0.05775 	 0.01438 	 m..s
   25 	    52 	 0.05712 	 0.01678 	 m..s
   57 	    53 	 0.05776 	 0.01959 	 m..s
   47 	    54 	 0.05732 	 0.02136 	 m..s
   42 	    55 	 0.05729 	 0.02537 	 m..s
   62 	    56 	 0.05884 	 0.02825 	 m..s
   48 	    57 	 0.05733 	 0.02901 	 ~...
   39 	    58 	 0.05727 	 0.03157 	 ~...
   46 	    59 	 0.05731 	 0.03172 	 ~...
   54 	    60 	 0.05762 	 0.03392 	 ~...
   45 	    61 	 0.05731 	 0.03972 	 ~...
   36 	    62 	 0.05722 	 0.04193 	 ~...
   27 	    63 	 0.05712 	 0.04273 	 ~...
   30 	    64 	 0.05713 	 0.04716 	 ~...
  102 	    65 	 0.10834 	 0.05573 	 m..s
   31 	    66 	 0.05713 	 0.05919 	 ~...
   35 	    67 	 0.05722 	 0.05996 	 ~...
   34 	    68 	 0.05716 	 0.06161 	 ~...
   43 	    69 	 0.05729 	 0.06609 	 ~...
    3 	    70 	 0.05708 	 0.06858 	 ~...
  112 	    71 	 0.12764 	 0.07021 	 m..s
  114 	    72 	 0.13921 	 0.07222 	 m..s
   97 	    73 	 0.10771 	 0.07253 	 m..s
   70 	    74 	 0.08652 	 0.07377 	 ~...
   19 	    75 	 0.05711 	 0.07437 	 ~...
   59 	    76 	 0.05789 	 0.08053 	 ~...
   58 	    77 	 0.05789 	 0.08177 	 ~...
  113 	    78 	 0.13785 	 0.08491 	 m..s
  103 	    79 	 0.10840 	 0.09562 	 ~...
  118 	    80 	 0.17687 	 0.09708 	 m..s
   71 	    81 	 0.08677 	 0.09822 	 ~...
   68 	    82 	 0.08413 	 0.10319 	 ~...
   87 	    83 	 0.10581 	 0.10483 	 ~...
   75 	    84 	 0.09620 	 0.10630 	 ~...
   73 	    85 	 0.09451 	 0.10651 	 ~...
    0 	    86 	 0.05708 	 0.10829 	 m..s
   80 	    87 	 0.10230 	 0.11054 	 ~...
   72 	    88 	 0.08693 	 0.11124 	 ~...
   88 	    89 	 0.10640 	 0.11589 	 ~...
   79 	    90 	 0.10157 	 0.12249 	 ~...
   76 	    91 	 0.09707 	 0.12497 	 ~...
  110 	    92 	 0.11193 	 0.12683 	 ~...
   77 	    93 	 0.09797 	 0.13543 	 m..s
  111 	    94 	 0.11199 	 0.13901 	 ~...
    6 	    95 	 0.05708 	 0.14133 	 m..s
   91 	    96 	 0.10690 	 0.14154 	 m..s
   94 	    97 	 0.10732 	 0.14493 	 m..s
   85 	    98 	 0.10572 	 0.14879 	 m..s
   83 	    99 	 0.10346 	 0.15452 	 m..s
   11 	   100 	 0.05709 	 0.15753 	 MISS
    9 	   101 	 0.05708 	 0.15872 	 MISS
  101 	   102 	 0.10824 	 0.16078 	 m..s
    7 	   103 	 0.05708 	 0.16084 	 MISS
   82 	   104 	 0.10316 	 0.16627 	 m..s
   84 	   105 	 0.10460 	 0.16766 	 m..s
  109 	   106 	 0.11012 	 0.16959 	 m..s
  108 	   107 	 0.10997 	 0.16986 	 m..s
   99 	   108 	 0.10778 	 0.17039 	 m..s
   78 	   109 	 0.10090 	 0.18083 	 m..s
   81 	   110 	 0.10296 	 0.18365 	 m..s
  106 	   111 	 0.10964 	 0.19204 	 m..s
   96 	   112 	 0.10770 	 0.20264 	 m..s
   74 	   113 	 0.09608 	 0.20499 	 MISS
   89 	   114 	 0.10658 	 0.20735 	 MISS
  119 	   115 	 0.21055 	 0.22405 	 ~...
  120 	   116 	 0.21102 	 0.22896 	 ~...
   92 	   117 	 0.10704 	 0.22899 	 MISS
  117 	   118 	 0.17556 	 0.27102 	 m..s
  116 	   119 	 0.16697 	 0.28559 	 MISS
  115 	   120 	 0.15116 	 0.28705 	 MISS
==========================================
r_mrr = 0.6642389297485352
r2_mrr = 0.3502320647239685
spearmanr_mrr@5 = 0.8607932329177856
spearmanr_mrr@10 = 0.9513048529624939
spearmanr_mrr@50 = 0.8792869448661804
spearmanr_mrr@100 = 0.9546247124671936
spearmanr_mrr@All = 0.9594462513923645
==========================================
test time: 0.433
Done Testing dataset DBpedia50
total time taken: 187.56648969650269
training time taken: 182.08083033561707
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.6642)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.3502)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8608)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9513)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.8793)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9546)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9594)}}, 'test_loss': {'TransE': {'DBpedia50': 2.736764254019363}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 8817225249415981
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [161, 652, 173, 1056, 1201, 314, 832, 996, 138, 784, 79, 845, 523, 443, 991, 1029, 1195, 429, 574, 400, 1093, 497, 422, 992, 631, 880, 467, 1014, 872, 979, 968, 642, 1059, 1178, 888, 109, 344, 938, 1209, 1140, 943, 415, 302, 685, 1210, 1211, 77, 883, 1185, 794, 167, 636, 150, 827, 260, 767, 183, 772, 263, 1196, 780, 17, 902, 1146, 1062, 462, 729, 629, 1025, 549, 106, 112, 180, 802, 848, 757, 325, 997, 446, 281, 1081, 1067, 825, 507, 160, 187, 399, 635, 1179, 1212, 364, 812, 663, 411, 605, 884, 60, 124, 897, 1006, 360, 756, 1145, 492, 486, 732, 1085, 701, 604, 191, 395, 807, 569, 1105, 499, 433, 834, 117, 268, 864, 847]
valid_ids (0): []
train_ids (1094): [894, 148, 632, 954, 1053, 105, 1205, 46, 1020, 749, 207, 623, 867, 939, 1176, 722, 73, 913, 937, 509, 273, 692, 127, 1090, 179, 21, 978, 837, 555, 495, 898, 1134, 971, 45, 822, 252, 511, 1169, 1151, 831, 266, 1009, 149, 1018, 447, 1136, 93, 798, 655, 739, 724, 985, 649, 505, 582, 346, 1167, 1017, 1075, 571, 377, 34, 809, 485, 836, 74, 1064, 1011, 542, 348, 965, 489, 1164, 192, 496, 609, 349, 977, 792, 1034, 391, 607, 1104, 50, 493, 861, 1102, 689, 806, 826, 398, 987, 471, 241, 1197, 638, 1092, 1086, 1010, 853, 472, 515, 1168, 345, 1155, 423, 242, 257, 615, 855, 564, 915, 775, 830, 730, 1036, 139, 146, 1070, 580, 76, 1199, 553, 1202, 125, 208, 1039, 4, 95, 860, 375, 768, 551, 821, 70, 567, 682, 1181, 63, 1173, 116, 779, 370, 1040, 115, 586, 336, 514, 357, 30, 49, 935, 251, 300, 189, 25, 218, 870, 829, 53, 1148, 1157, 1082, 926, 673, 328, 601, 487, 243, 481, 1061, 843, 3, 587, 64, 714, 740, 1016, 1200, 442, 734, 1187, 548, 1072, 174, 1193, 1182, 1007, 285, 537, 312, 478, 665, 760, 96, 184, 130, 136, 380, 534, 698, 1089, 246, 32, 288, 828, 428, 256, 340, 190, 810, 52, 742, 1186, 1172, 330, 650, 910, 612, 1131, 1125, 129, 236, 134, 62, 568, 132, 1084, 535, 522, 808, 240, 316, 451, 85, 172, 37, 185, 875, 570, 944, 1055, 128, 824, 221, 122, 547, 407, 223, 1088, 157, 466, 710, 378, 1139, 1019, 437, 811, 877, 168, 691, 613, 849, 833, 332, 813, 386, 584, 277, 1129, 627, 475, 700, 92, 750, 169, 1076, 929, 480, 1153, 239, 624, 1184, 284, 1191, 986, 1027, 512, 800, 600, 65, 166, 674, 859, 653, 726, 1124, 751, 140, 483, 137, 804, 508, 75, 66, 1094, 389, 118, 554, 656, 404, 31, 372, 1174, 980, 660, 541, 200, 215, 470, 626, 602, 871, 502, 900, 787, 575, 367, 408, 941, 394, 320, 983, 677, 1203, 126, 886, 783, 1012, 182, 1095, 297, 453, 1120, 560, 538, 101, 181, 588, 625, 435, 863, 773, 752, 29, 697, 317, 896, 339, 86, 1043, 718, 572, 1047, 865, 606, 211, 721, 371, 1050, 1154, 479, 41, 102, 222, 620, 272, 696, 556, 1180, 313, 1096, 755, 1114, 550, 1115, 577, 999, 158, 1065, 20, 425, 392, 895, 97, 924, 513, 163, 1194, 662, 347, 283, 338, 225, 956, 282, 488, 646, 1099, 230, 846, 100, 664, 790, 321, 889, 322, 503, 258, 290, 579, 921, 374, 786, 264, 562, 1177, 852, 279, 679, 934, 213, 634, 110, 1198, 177, 269, 705, 7, 1005, 528, 795, 817, 1189, 153, 706, 342, 1058, 789, 869, 412, 1004, 899, 68, 477, 753, 839, 1204, 707, 9, 71, 456, 526, 231, 165, 510, 1206, 1038, 51, 48, 844, 401, 928, 254, 585, 13, 310, 33, 379, 227, 1071, 1112, 737, 1190, 265, 91, 741, 362, 1, 1126, 617, 1080, 57, 527, 891, 1144, 44, 459, 976, 1117, 142, 666, 936, 5, 524, 686, 1165, 879, 595, 10, 1111, 94, 969, 1214, 881, 123, 276, 205, 640, 301, 1107, 1057, 69, 908, 176, 1028, 350, 1158, 280, 1054, 120, 1013, 680, 621, 1091, 893, 611, 820, 641, 530, 521, 6, 1128, 858, 838, 1101, 1188, 684, 464, 616, 14, 1152, 558, 43, 566, 363, 1192, 973, 444, 657, 368, 121, 520, 933, 426, 953, 1078, 501, 335, 195, 396, 274, 1031, 922, 206, 341, 962, 430, 669, 704, 287, 993, 1023, 785, 1132, 295, 862, 402, 355, 418, 197, 1097, 723, 591, 1063, 1073, 99, 961, 1123, 463, 630, 659, 393, 536, 690, 498, 383, 907, 289, 532, 544, 175, 735, 914, 133, 298, 857, 376, 543, 354, 919, 1116, 1147, 226, 781, 959, 436, 967, 648, 525, 958, 531, 713, 1024, 319, 856, 381, 271, 416, 628, 709, 193, 352, 1069, 59, 717, 80, 963, 186, 388, 1100, 1208, 876, 54, 334, 927, 1142, 818, 942, 1160, 770, 874, 597, 61, 1106, 590, 1098, 947, 683, 397, 619, 1026, 776, 940, 712, 1213, 108, 318, 27, 250, 930, 596, 454, 716, 974, 949, 103, 356, 589, 687, 610, 82, 155, 309, 439, 676, 1046, 410, 851, 1161, 233, 711, 299, 162, 545, 307, 22, 291, 819, 141, 736, 835, 728, 988, 373, 461, 18, 998, 733, 214, 267, 984, 743, 643, 529, 1022, 406, 671, 23, 762, 111, 970, 678, 1051, 196, 28, 131, 1207, 905, 667, 758, 946, 668, 995, 468, 194, 306, 892, 911, 912, 906, 925, 1127, 592, 622, 720, 235, 769, 778, 1037, 1162, 761, 1138, 972, 405, 114, 490, 255, 1002, 552, 931, 15, 78, 143, 159, 637, 916, 245, 576, 26, 1159, 1156, 188, 981, 540, 771, 315, 982, 202, 854, 670, 727, 932, 1183, 945, 199, 294, 278, 702, 286, 441, 403, 901, 209, 1049, 1166, 815, 438, 661, 504, 305, 1103, 694, 1032, 87, 1030, 343, 11, 1044, 329, 293, 39, 164, 573, 296, 765, 469, 1052, 887, 1143, 764, 217, 675, 337, 1000, 868, 873, 427, 238, 744, 420, 639, 850, 1066, 658, 842, 955, 67, 424, 12, 866, 960, 715, 1118, 840, 58, 878, 517, 465, 473, 19, 951, 719, 681, 903, 748, 904, 262, 688, 1083, 647, 382, 210, 458, 559, 449, 1035, 608, 358, 593, 390, 234, 457, 38, 793, 1045, 693, 56, 308, 1068, 506, 645, 326, 156, 353, 249, 738, 259, 1133, 434, 331, 1041, 365, 203, 1021, 224, 212, 90, 89, 651, 2, 699, 654, 40, 598, 384, 614, 703, 731, 445, 603, 474, 782, 759, 918, 557, 324, 460, 801, 323, 500, 516, 599, 746, 747, 816, 1110, 275, 948, 237, 909, 1163, 539, 989, 644, 494, 232, 450, 154, 1015, 882, 1171, 533, 303, 885, 1130, 333, 1121, 431, 229, 578, 952, 359, 24, 482, 35, 563, 151, 990, 796, 1150, 432, 994, 565, 1137, 519, 920, 1001, 1077, 1033, 204, 766, 917, 448, 745, 1074, 228, 763, 244, 366, 248, 1060, 419, 369, 413, 198, 618, 561, 220, 81, 594, 440, 219, 178, 107, 361, 421, 774, 135, 546, 814, 1109, 201, 113, 387, 261, 788, 964, 119, 581, 1122, 409, 152, 144, 311, 216, 754, 16, 84, 1079, 0, 247, 1108, 170, 452, 476, 890, 1149, 484, 292, 8, 491, 47, 1170, 823, 83, 327, 777, 1042, 104, 88, 708, 1003, 1113, 803, 1135, 1141, 791, 841, 672, 633, 72, 414, 923, 417, 583, 797, 36, 799, 1087, 966, 1175, 253, 1048, 957, 42, 351, 950, 385, 975, 1119, 98, 518, 805, 145, 147, 171, 455, 55, 270, 1008, 725, 304, 695]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3863470700813678
the save name prefix for this run is:  chkpt-ID_3863470700813678_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.554 +- 0.007
mrr vals (pred, true): 0.000, 0.035
batch losses (mrrl, rdl): 0.0, 0.0016071

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.155 +- 0.148
mrr vals (pred, true): 0.011, 0.159
batch losses (mrrl, rdl): 0.0, 9.42234e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.337 +- 0.295
mrr vals (pred, true): 0.007, 0.000
batch losses (mrrl, rdl): 0.0, 7.68808e-05

Epoch over!
epoch time: 12.426

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 736
rank avg (pred): 0.069 +- 0.065
mrr vals (pred, true): 0.063, 0.101
batch losses (mrrl, rdl): 0.0, 5.37317e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.099 +- 0.095
mrr vals (pred, true): 0.086, 0.089
batch losses (mrrl, rdl): 0.0, 4.43742e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.331 +- 0.305
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003292747

Epoch over!
epoch time: 11.989

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 476
rank avg (pred): 0.329 +- 0.306
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002949382

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 113
rank avg (pred): 0.300 +- 0.302
mrr vals (pred, true): 0.170, 0.056
batch losses (mrrl, rdl): 0.0, 0.0003388683

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 331
rank avg (pred): 0.330 +- 0.312
mrr vals (pred, true): 0.065, 0.056
batch losses (mrrl, rdl): 0.0, 0.0003457326

Epoch over!
epoch time: 12.087

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 94
rank avg (pred): 0.334 +- 0.309
mrr vals (pred, true): 0.046, 0.025
batch losses (mrrl, rdl): 0.0, 0.000307099

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 935
rank avg (pred): 0.310 +- 0.307
mrr vals (pred, true): 0.160, 0.141
batch losses (mrrl, rdl): 0.0, 0.0002705589

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 201
rank avg (pred): 0.319 +- 0.312
mrr vals (pred, true): 0.152, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003548461

Epoch over!
epoch time: 12.81

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.325 +- 0.314
mrr vals (pred, true): 0.111, 0.011
batch losses (mrrl, rdl): 0.0, 8.5605e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.324 +- 0.313
mrr vals (pred, true): 0.120, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001259317

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 479
rank avg (pred): 0.314 +- 0.313
mrr vals (pred, true): 0.150, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002022356

Epoch over!
epoch time: 12.658

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.332 +- 0.312
mrr vals (pred, true): 0.081, 0.033
batch losses (mrrl, rdl): 0.0098796356, 0.0002981367

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 688
rank avg (pred): 0.489 +- 0.255
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0261320285, 1.97848e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.506 +- 0.217
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 2.44723e-05, 4.12879e-05

Epoch over!
epoch time: 12.349

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.469 +- 0.242
mrr vals (pred, true): 0.080, 0.139
batch losses (mrrl, rdl): 0.0348823667, 0.0016903712

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 818
rank avg (pred): 0.060 +- 0.032
mrr vals (pred, true): 0.119, 0.097
batch losses (mrrl, rdl): 0.0469320491, 0.0001077712

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 19
rank avg (pred): 0.139 +- 0.075
mrr vals (pred, true): 0.113, 0.139
batch losses (mrrl, rdl): 0.0069056246, 8.53953e-05

Epoch over!
epoch time: 12.02

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.466 +- 0.214
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0015757208, 6.07615e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 98
rank avg (pred): 0.468 +- 0.204
mrr vals (pred, true): 0.063, 0.081
batch losses (mrrl, rdl): 0.0015689679, 0.0015373238

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 683
rank avg (pred): 0.455 +- 0.199
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0011420602, 3.66218e-05

Epoch over!
epoch time: 12.941

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 455
rank avg (pred): 0.449 +- 0.201
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0010735822, 4.28242e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1156
rank avg (pred): 0.006 +- 0.003
mrr vals (pred, true): 0.176, 0.099
batch losses (mrrl, rdl): 0.1595858932, 0.0008426004

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 947
rank avg (pred): 0.343 +- 0.179
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0153122451, 0.0004816607

Epoch over!
epoch time: 12.804

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.204, 0.224
batch losses (mrrl, rdl): 0.003978088, 9.01991e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.005 +- 0.002
mrr vals (pred, true): 0.182, 0.210
batch losses (mrrl, rdl): 0.0075108763, 7.40917e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.065 +- 0.034
mrr vals (pred, true): 0.144, 0.121
batch losses (mrrl, rdl): 0.0055955858, 8.29046e-05

Epoch over!
epoch time: 12.202

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.433 +- 0.192
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0010181797, 5.61925e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.313 +- 0.164
mrr vals (pred, true): 0.087, 0.099
batch losses (mrrl, rdl): 0.0133531522, 0.001003566

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.046 +- 0.023
mrr vals (pred, true): 0.128, 0.121
batch losses (mrrl, rdl): 0.0005871912, 0.0001181466

Epoch over!
epoch time: 13.129

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.389 +- 0.194
mrr vals (pred, true): 0.098, 0.136
batch losses (mrrl, rdl): 0.0145703498, 0.0020484873

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.272 +- 0.137
mrr vals (pred, true): 0.095, 0.174
batch losses (mrrl, rdl): 0.0630482659, 0.0001782469

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.417 +- 0.126
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0011411296, 0.000114988

Epoch over!
epoch time: 12.277

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 643
rank avg (pred): 0.408 +- 0.146
mrr vals (pred, true): 0.056, 0.005
batch losses (mrrl, rdl): 0.0003702696, 0.0001984355

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 627
rank avg (pred): 0.404 +- 0.125
mrr vals (pred, true): 0.046, 0.002
batch losses (mrrl, rdl): 0.0001917806, 0.0001201009

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 287
rank avg (pred): 0.347 +- 0.169
mrr vals (pred, true): 0.081, 0.111
batch losses (mrrl, rdl): 0.009311853, 0.0013434199

Epoch over!
epoch time: 11.963

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.073 +- 0.037
mrr vals (pred, true): 0.108, 0.098
batch losses (mrrl, rdl): 0.0333168283, 2.96449e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 543
rank avg (pred): 0.372 +- 0.167
mrr vals (pred, true): 0.066, 0.034
batch losses (mrrl, rdl): 0.0024842436, 0.0002820509

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.268 +- 0.139
mrr vals (pred, true): 0.114, 0.144
batch losses (mrrl, rdl): 0.0090162177, 0.0005614795

Epoch over!
epoch time: 12.368

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 249
rank avg (pred): 0.352 +- 0.168
mrr vals (pred, true): 0.084, 0.086
batch losses (mrrl, rdl): 0.0114593171, 0.0010983847

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.021 +- 0.012
mrr vals (pred, true): 0.176, 0.209
batch losses (mrrl, rdl): 0.0105668381, 9.6248e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 129
rank avg (pred): 0.383 +- 0.120
mrr vals (pred, true): 0.047, 0.014
batch losses (mrrl, rdl): 6.62347e-05, 0.0004576266

Epoch over!
epoch time: 12.263

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.359 +- 0.131
mrr vals (pred, true): 0.056, 0.092

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   81 	     0 	 0.08705 	 0.00019 	 m..s
   53 	     1 	 0.08135 	 0.00021 	 m..s
   20 	     2 	 0.05074 	 0.00021 	 m..s
   18 	     3 	 0.04973 	 0.00022 	 m..s
   33 	     4 	 0.05385 	 0.00023 	 m..s
   24 	     5 	 0.05229 	 0.00023 	 m..s
   42 	     6 	 0.06077 	 0.00024 	 m..s
   88 	     7 	 0.09145 	 0.00024 	 m..s
   21 	     8 	 0.05144 	 0.00025 	 m..s
   29 	     9 	 0.05350 	 0.00026 	 m..s
   52 	    10 	 0.08132 	 0.00026 	 m..s
   92 	    11 	 0.09301 	 0.00027 	 m..s
   76 	    12 	 0.08620 	 0.00027 	 m..s
   30 	    13 	 0.05363 	 0.00028 	 m..s
   67 	    14 	 0.08538 	 0.00028 	 m..s
   67 	    15 	 0.08538 	 0.00028 	 m..s
   60 	    16 	 0.08242 	 0.00028 	 m..s
   40 	    17 	 0.05867 	 0.00029 	 m..s
   92 	    18 	 0.09301 	 0.00029 	 m..s
   69 	    19 	 0.08564 	 0.00030 	 m..s
   44 	    20 	 0.06196 	 0.00032 	 m..s
   65 	    21 	 0.08456 	 0.00033 	 m..s
   13 	    22 	 0.04838 	 0.00033 	 m..s
   17 	    23 	 0.04952 	 0.00035 	 m..s
   84 	    24 	 0.08778 	 0.00038 	 m..s
   36 	    25 	 0.05462 	 0.00039 	 m..s
   71 	    26 	 0.08592 	 0.00039 	 m..s
   11 	    27 	 0.04816 	 0.00041 	 m..s
   62 	    28 	 0.08370 	 0.00042 	 m..s
   32 	    29 	 0.05383 	 0.00042 	 m..s
   26 	    30 	 0.05283 	 0.00044 	 m..s
   61 	    31 	 0.08368 	 0.00049 	 m..s
   75 	    32 	 0.08618 	 0.00054 	 m..s
   38 	    33 	 0.05598 	 0.00061 	 m..s
    2 	    34 	 0.04406 	 0.00061 	 m..s
   27 	    35 	 0.05330 	 0.00074 	 m..s
   85 	    36 	 0.08781 	 0.00078 	 m..s
   50 	    37 	 0.08097 	 0.00081 	 m..s
    7 	    38 	 0.04725 	 0.00349 	 m..s
    0 	    39 	 0.04214 	 0.00395 	 m..s
   25 	    40 	 0.05238 	 0.00451 	 m..s
   16 	    41 	 0.04856 	 0.00793 	 m..s
    3 	    42 	 0.04432 	 0.01117 	 m..s
   14 	    43 	 0.04850 	 0.01300 	 m..s
    6 	    44 	 0.04623 	 0.01739 	 ~...
    0 	    45 	 0.04214 	 0.01768 	 ~...
   22 	    46 	 0.05162 	 0.02020 	 m..s
    4 	    47 	 0.04542 	 0.02032 	 ~...
   12 	    48 	 0.04834 	 0.02180 	 ~...
   15 	    49 	 0.04852 	 0.02518 	 ~...
   43 	    50 	 0.06190 	 0.02604 	 m..s
   48 	    51 	 0.07040 	 0.02774 	 m..s
   31 	    52 	 0.05369 	 0.02783 	 ~...
    9 	    53 	 0.04779 	 0.02804 	 ~...
   28 	    54 	 0.05342 	 0.03155 	 ~...
   34 	    55 	 0.05404 	 0.03183 	 ~...
   45 	    56 	 0.06412 	 0.03447 	 ~...
   41 	    57 	 0.05904 	 0.04220 	 ~...
    4 	    58 	 0.04542 	 0.04785 	 ~...
   78 	    59 	 0.08644 	 0.05005 	 m..s
   37 	    60 	 0.05538 	 0.05393 	 ~...
   74 	    61 	 0.08617 	 0.05415 	 m..s
    7 	    62 	 0.04725 	 0.05453 	 ~...
   46 	    63 	 0.06421 	 0.05932 	 ~...
   35 	    64 	 0.05458 	 0.06079 	 ~...
   10 	    65 	 0.04801 	 0.06345 	 ~...
   96 	    66 	 0.09710 	 0.07259 	 ~...
   95 	    67 	 0.09692 	 0.07322 	 ~...
   82 	    68 	 0.08713 	 0.07437 	 ~...
   19 	    69 	 0.05031 	 0.07569 	 ~...
   73 	    70 	 0.08603 	 0.07652 	 ~...
   49 	    71 	 0.08074 	 0.08123 	 ~...
   99 	    72 	 0.09749 	 0.09149 	 ~...
   39 	    73 	 0.05627 	 0.09247 	 m..s
  113 	    74 	 0.18395 	 0.09392 	 m..s
  100 	    75 	 0.10497 	 0.09486 	 ~...
   58 	    76 	 0.08195 	 0.10013 	 ~...
  116 	    77 	 0.19438 	 0.10485 	 m..s
  112 	    78 	 0.18387 	 0.10511 	 m..s
  103 	    79 	 0.12555 	 0.10630 	 ~...
  101 	    80 	 0.10757 	 0.10701 	 ~...
   80 	    81 	 0.08665 	 0.11157 	 ~...
   59 	    82 	 0.08231 	 0.11949 	 m..s
   51 	    83 	 0.08125 	 0.12444 	 m..s
   23 	    84 	 0.05182 	 0.12782 	 m..s
   54 	    85 	 0.08161 	 0.12905 	 m..s
   47 	    86 	 0.06898 	 0.13182 	 m..s
   91 	    87 	 0.09233 	 0.13370 	 m..s
   90 	    88 	 0.09195 	 0.14838 	 m..s
   55 	    89 	 0.08177 	 0.16173 	 m..s
   87 	    90 	 0.08807 	 0.16341 	 m..s
   89 	    91 	 0.09192 	 0.16364 	 m..s
   57 	    92 	 0.08180 	 0.16796 	 m..s
   64 	    93 	 0.08442 	 0.17044 	 m..s
   86 	    94 	 0.08804 	 0.17079 	 m..s
   55 	    95 	 0.08177 	 0.17132 	 m..s
   94 	    96 	 0.09352 	 0.17136 	 m..s
  102 	    97 	 0.11825 	 0.17390 	 m..s
   97 	    98 	 0.09715 	 0.17817 	 m..s
   70 	    99 	 0.08577 	 0.17849 	 m..s
   66 	   100 	 0.08522 	 0.18365 	 m..s
   63 	   101 	 0.08409 	 0.18628 	 MISS
  114 	   102 	 0.18409 	 0.18766 	 ~...
   83 	   103 	 0.08715 	 0.18783 	 MISS
  111 	   104 	 0.18051 	 0.19456 	 ~...
  110 	   105 	 0.17920 	 0.19797 	 ~...
   77 	   106 	 0.08631 	 0.20007 	 MISS
   79 	   107 	 0.08657 	 0.20429 	 MISS
   72 	   108 	 0.08601 	 0.20862 	 MISS
  104 	   109 	 0.13893 	 0.21654 	 m..s
  107 	   110 	 0.17424 	 0.21929 	 m..s
   97 	   111 	 0.09715 	 0.22092 	 MISS
  109 	   112 	 0.17617 	 0.22192 	 m..s
  108 	   113 	 0.17493 	 0.24545 	 m..s
  115 	   114 	 0.18875 	 0.27049 	 m..s
  105 	   115 	 0.14504 	 0.27084 	 MISS
  106 	   116 	 0.17057 	 0.28705 	 MISS
  120 	   117 	 0.21332 	 0.29333 	 m..s
  119 	   118 	 0.21154 	 0.29621 	 m..s
  118 	   119 	 0.19572 	 0.30066 	 MISS
  116 	   120 	 0.19438 	 0.30997 	 MISS
==========================================
r_mrr = 0.7317129969596863
r2_mrr = 0.46989327669143677
spearmanr_mrr@5 = 0.8765363097190857
spearmanr_mrr@10 = 0.8322531580924988
spearmanr_mrr@50 = 0.9073589444160461
spearmanr_mrr@100 = 0.9123333692550659
spearmanr_mrr@All = 0.9280046820640564
==========================================
test time: 0.42
Done Testing dataset DBpedia50
total time taken: 192.46806693077087
training time taken: 186.7926733493805
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7317)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4699)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8765)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8323)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9074)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9123)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9280)}}, 'test_loss': {'TransE': {'DBpedia50': 3.4960354999784613}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 1928397725351642
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [605, 611, 395, 444, 198, 1066, 927, 620, 182, 130, 570, 493, 954, 777, 1070, 699, 62, 664, 291, 1139, 246, 345, 505, 456, 1009, 808, 735, 740, 795, 635, 1015, 308, 2, 815, 972, 970, 663, 916, 579, 950, 303, 170, 276, 797, 1098, 945, 754, 806, 1050, 44, 534, 765, 1182, 37, 233, 968, 1210, 903, 452, 118, 172, 39, 1202, 573, 1021, 1102, 1023, 84, 714, 459, 547, 898, 1200, 421, 786, 1141, 289, 1147, 528, 1165, 1175, 935, 235, 1006, 501, 618, 438, 256, 1159, 624, 396, 461, 778, 789, 128, 1053, 5, 532, 382, 918, 403, 1164, 4, 897, 828, 856, 522, 716, 650, 1160, 14, 725, 728, 937, 915, 210, 830, 794, 173, 1113, 906]
valid_ids (0): []
train_ids (1094): [388, 774, 283, 77, 65, 689, 428, 484, 30, 1004, 615, 437, 34, 15, 324, 802, 567, 639, 378, 1156, 1157, 871, 991, 106, 647, 1135, 224, 900, 366, 936, 634, 1114, 1074, 318, 1137, 599, 361, 446, 263, 1196, 642, 422, 0, 1012, 841, 1034, 691, 178, 372, 367, 304, 538, 632, 28, 578, 562, 253, 1091, 231, 593, 307, 869, 189, 25, 383, 1024, 1128, 81, 631, 103, 751, 454, 807, 341, 1010, 498, 472, 317, 107, 633, 78, 843, 328, 64, 677, 196, 957, 833, 524, 286, 840, 545, 842, 537, 565, 944, 949, 45, 1038, 379, 810, 158, 169, 893, 1062, 924, 120, 697, 575, 1203, 1105, 503, 790, 1127, 696, 596, 1185, 1179, 448, 1011, 70, 679, 550, 1000, 419, 997, 526, 203, 1151, 912, 803, 29, 159, 832, 250, 992, 595, 115, 658, 239, 274, 877, 582, 380, 1187, 137, 717, 184, 836, 544, 1126, 127, 306, 354, 260, 757, 373, 813, 1209, 652, 878, 743, 363, 226, 564, 694, 1018, 1168, 192, 775, 365, 59, 755, 666, 53, 1108, 207, 1145, 55, 408, 1041, 301, 800, 951, 335, 353, 1104, 48, 1194, 637, 710, 1, 988, 171, 102, 441, 1110, 1022, 932, 216, 707, 58, 749, 1090, 721, 905, 453, 124, 1030, 586, 236, 901, 101, 996, 278, 331, 433, 385, 202, 649, 215, 38, 180, 1013, 393, 571, 109, 657, 1111, 1086, 72, 1020, 265, 623, 139, 1028, 870, 1082, 121, 431, 469, 722, 931, 926, 849, 352, 763, 894, 166, 476, 1097, 561, 69, 746, 907, 730, 928, 629, 1014, 850, 958, 960, 425, 54, 1036, 733, 726, 232, 976, 117, 684, 1027, 1155, 1103, 1214, 785, 105, 160, 351, 113, 26, 218, 520, 356, 164, 1007, 298, 1071, 183, 13, 1150, 872, 370, 739, 93, 793, 132, 126, 280, 964, 560, 16, 1084, 685, 781, 851, 1115, 1047, 585, 320, 604, 32, 859, 555, 475, 779, 332, 762, 252, 829, 219, 1096, 626, 470, 259, 947, 138, 262, 85, 756, 973, 603, 440, 266, 863, 540, 704, 668, 1199, 917, 497, 49, 899, 411, 277, 720, 742, 12, 1109, 548, 831, 701, 1055, 412, 142, 816, 398, 1146, 330, 375, 400, 1019, 199, 427, 804, 457, 546, 750, 188, 434, 921, 168, 97, 1046, 934, 153, 1032, 536, 284, 981, 110, 129, 1143, 1033, 297, 323, 1189, 513, 712, 402, 614, 598, 641, 930, 695, 211, 9, 237, 868, 875, 819, 350, 1134, 977, 1044, 682, 788, 442, 381, 698, 702, 1180, 409, 768, 1026, 212, 247, 860, 688, 541, 100, 884, 507, 709, 342, 1121, 622, 116, 499, 1017, 264, 11, 1130, 334, 1054, 66, 348, 52, 206, 74, 591, 47, 881, 1186, 150, 435, 602, 50, 776, 673, 193, 176, 1077, 390, 998, 1058, 509, 825, 1089, 1129, 812, 413, 465, 485, 389, 764, 1008, 423, 339, 123, 769, 619, 386, 606, 636, 455, 848, 952, 693, 601, 1193, 282, 1174, 3, 194, 191, 919, 61, 1191, 759, 99, 261, 943, 374, 993, 805, 494, 886, 576, 512, 240, 122, 466, 295, 1136, 731, 271, 18, 681, 293, 204, 149, 683, 834, 533, 1076, 521, 478, 686, 157, 787, 299, 473, 1072, 1079, 177, 706, 296, 847, 854, 1125, 975, 167, 890, 858, 165, 19, 60, 867, 1120, 185, 529, 94, 21, 445, 515, 782, 760, 201, 140, 96, 510, 135, 145, 978, 111, 747, 391, 417, 6, 86, 221, 338, 316, 865, 922, 824, 144, 588, 79, 443, 486, 880, 1178, 1201, 346, 674, 488, 439, 814, 753, 963, 1016, 1162, 518, 1207, 700, 772, 312, 883, 22, 1132, 305, 217, 965, 429, 40, 1075, 715, 979, 451, 962, 559, 961, 337, 783, 967, 392, 527, 1052, 502, 359, 1064, 162, 1059, 36, 827, 220, 490, 516, 430, 197, 941, 826, 584, 517, 761, 57, 630, 82, 1063, 251, 290, 670, 1197, 131, 895, 112, 556, 321, 450, 287, 1152, 344, 566, 1176, 1173, 491, 597, 1094, 986, 1085, 617, 577, 468, 24, 1212, 73, 410, 608, 729, 1095, 1065, 909, 672, 1166, 891, 773, 969, 487, 1068, 496, 267, 908, 1040, 1087, 879, 852, 643, 1213, 724, 424, 329, 671, 294, 362, 799, 432, 553, 325, 416, 270, 770, 938, 771, 889, 460, 1069, 1093, 874, 8, 855, 920, 156, 405, 594, 42, 758, 718, 543, 17, 1119, 35, 1045, 1100, 948, 1154, 662, 1123, 242, 228, 1122, 1167, 589, 63, 200, 745, 551, 939, 268, 360, 143, 656, 243, 492, 911, 857, 713, 358, 680, 414, 811, 736, 539, 92, 554, 690, 195, 798, 474, 214, 1083, 241, 613, 349, 319, 397, 853, 581, 477, 896, 481, 1158, 1170, 51, 1140, 1092, 272, 355, 792, 1148, 91, 519, 309, 667, 796, 181, 552, 213, 1043, 904, 186, 1124, 862, 1177, 467, 480, 506, 1190, 244, 1039, 801, 269, 114, 141, 719, 966, 33, 959, 839, 1029, 147, 987, 845, 844, 942, 946, 1035, 511, 655, 347, 404, 654, 27, 542, 646, 1060, 67, 653, 401, 310, 1107, 530, 563, 675, 384, 371, 767, 187, 1106, 300, 87, 7, 95, 1208, 621, 227, 583, 10, 676, 665, 504, 648, 245, 910, 837, 659, 368, 549, 31, 1211, 766, 154, 580, 523, 1116, 953, 41, 489, 846, 175, 956, 415, 1037, 835, 343, 1169, 1131, 406, 1056, 1138, 327, 645, 933, 133, 1206, 155, 887, 151, 902, 279, 1003, 376, 1183, 273, 995, 458, 651, 669, 838, 1101, 885, 23, 88, 678, 134, 861, 607, 703, 610, 1118, 738, 394, 1031, 1005, 90, 557, 531, 612, 660, 464, 1195, 1112, 1198, 238, 482, 864, 587, 230, 322, 146, 125, 982, 1048, 479, 734, 436, 708, 377, 98, 1081, 1001, 462, 336, 821, 744, 302, 687, 1142, 1149, 1002, 1192, 1153, 311, 223, 999, 985, 254, 179, 940, 46, 357, 640, 984, 20, 1163, 292, 471, 208, 340, 644, 866, 387, 705, 1067, 463, 1188, 369, 1133, 791, 1051, 447, 990, 108, 873, 508, 818, 809, 315, 723, 89, 711, 1184, 1057, 913, 229, 222, 1061, 234, 525, 600, 625, 737, 980, 971, 732, 314, 255, 616, 882, 119, 820, 68, 418, 592, 1049, 495, 275, 281, 257, 1088, 152, 1204, 407, 249, 258, 75, 1080, 627, 288, 364, 449, 994, 1205, 569, 568, 780, 558, 161, 190, 326, 974, 748, 483, 399, 1171, 1144, 56, 609, 955, 163, 285, 876, 174, 817, 1073, 225, 1117, 43, 923, 1042, 1099, 1161, 929, 822, 572, 426, 248, 83, 205, 661, 692, 741, 80, 104, 1025, 784, 333, 209, 313, 76, 914, 574, 752, 500, 823, 628, 1078, 1172, 925, 148, 590, 1181, 71, 888, 638, 136, 727, 514, 420, 535, 989, 983, 892]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9078661167181808
the save name prefix for this run is:  chkpt-ID_9078661167181808_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.484 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001067184

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.261 +- 0.206
mrr vals (pred, true): 0.193, 0.230
batch losses (mrrl, rdl): 0.0, 0.0008867499

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.307 +- 0.251
mrr vals (pred, true): 0.278, 0.121
batch losses (mrrl, rdl): 0.0, 0.0010121902

Epoch over!
epoch time: 12.356

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 919
rank avg (pred): 0.282 +- 0.232
mrr vals (pred, true): 0.280, 0.150
batch losses (mrrl, rdl): 0.0, 0.0002163574

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.271 +- 0.226
mrr vals (pred, true): 0.329, 0.001
batch losses (mrrl, rdl): 0.0, 0.0007239958

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1127
rank avg (pred): 0.235 +- 0.209
mrr vals (pred, true): 0.379, 0.001
batch losses (mrrl, rdl): 0.0, 0.0009622403

Epoch over!
epoch time: 12.087

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.225 +- 0.204
mrr vals (pred, true): 0.391, 0.098
batch losses (mrrl, rdl): 0.0, 0.0004652237

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.281 +- 0.247
mrr vals (pred, true): 0.388, 0.193
batch losses (mrrl, rdl): 0.0, 0.0011331103

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 422
rank avg (pred): 0.243 +- 0.253
mrr vals (pred, true): 0.413, 0.000
batch losses (mrrl, rdl): 0.0, 0.0006173837

Epoch over!
epoch time: 12.676

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 105
rank avg (pred): 0.244 +- 0.253
mrr vals (pred, true): 0.412, 0.019
batch losses (mrrl, rdl): 0.0, 5.8341e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 775
rank avg (pred): 0.241 +- 0.252
mrr vals (pred, true): 0.438, 0.164
batch losses (mrrl, rdl): 0.0, 0.0001478155

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.242 +- 0.254
mrr vals (pred, true): 0.430, 0.201
batch losses (mrrl, rdl): 0.0, 0.000148835

Epoch over!
epoch time: 12.358

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 161
rank avg (pred): 0.246 +- 0.258
mrr vals (pred, true): 0.441, 0.092
batch losses (mrrl, rdl): 0.0, 0.0001224632

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 268
rank avg (pred): 0.241 +- 0.253
mrr vals (pred, true): 0.436, 0.134
batch losses (mrrl, rdl): 0.0, 0.000621091

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 721
rank avg (pred): 0.300 +- 0.315
mrr vals (pred, true): 0.439, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005020328

Epoch over!
epoch time: 11.851

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1185
rank avg (pred): 0.303 +- 0.318
mrr vals (pred, true): 0.435, 0.077
batch losses (mrrl, rdl): 1.484465003, 4.34682e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 680
rank avg (pred): 0.477 +- 0.301
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0248894673, 7.52705e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 520
rank avg (pred): 0.396 +- 0.344
mrr vals (pred, true): 0.000, 0.054
batch losses (mrrl, rdl): 0.0248365942, 0.0002049364

Epoch over!
epoch time: 13.233

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.383 +- 0.343
mrr vals (pred, true): 0.000, 0.209
batch losses (mrrl, rdl): 0.4359139204, 0.0012964255

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 672
rank avg (pred): 0.336 +- 0.360
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0247612055, 0.0006290764

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 319
rank avg (pred): 0.285 +- 0.377
mrr vals (pred, true): 0.000, 0.114
batch losses (mrrl, rdl): 0.1301060319, 0.0004404328

Epoch over!
epoch time: 12.393

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.276 +- 0.381
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0245544594, 0.0010625318

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 103
rank avg (pred): 0.186 +- 0.377
mrr vals (pred, true): 0.006, 0.030
batch losses (mrrl, rdl): 0.0192437712, 0.0001347664

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1086
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.159, 0.200
batch losses (mrrl, rdl): 0.0172682554, 0.0006286184

Epoch over!
epoch time: 12.425

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.063, 0.013
batch losses (mrrl, rdl): 0.0017208298, 0.0012550707

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 755
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.100, 0.236
batch losses (mrrl, rdl): 0.1848968863, 0.0001202886

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.077, 0.000
batch losses (mrrl, rdl): 0.0074056624, 0.005292383

Epoch over!
epoch time: 11.994

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1103
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.141, 0.198
batch losses (mrrl, rdl): 0.0316777416, 0.0006600062

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 555
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.044, 0.029
batch losses (mrrl, rdl): 0.0003108251, 0.0015561501

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.208, 0.301
batch losses (mrrl, rdl): 0.0857149065, 7.40389e-05

Epoch over!
epoch time: 11.796

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 497
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.057, 0.095
batch losses (mrrl, rdl): 0.0005330176, 0.0011804494

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 132
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.063, 0.012
batch losses (mrrl, rdl): 0.0016130843, 0.0012856316

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1099
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.145, 0.159
batch losses (mrrl, rdl): 0.0020641838, 0.000830291

Epoch over!
epoch time: 12.456

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.106, 0.204
batch losses (mrrl, rdl): 0.0952490419, 0.0001194812

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.134, 0.192
batch losses (mrrl, rdl): 0.0334363058, 0.0006234261

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0030736693, 0.0045671258

Epoch over!
epoch time: 12.079

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1170
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.060, 0.050
batch losses (mrrl, rdl): 0.0009860245, 0.001904215

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 467
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0039674714, 0.0047445754

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 483
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0084581934, 0.0037251136

Epoch over!
epoch time: 13.602

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.132, 0.000
batch losses (mrrl, rdl): 0.0674215034, 0.0042686397

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.168, 0.000
batch losses (mrrl, rdl): 0.1395215094, 0.0051584374

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 426
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0067125792, 0.004817165

Epoch over!
epoch time: 13.962

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 790
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.117, 0.001
batch losses (mrrl, rdl): 0.0453817062, 0.0042822021

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1051
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.221, 0.000
batch losses (mrrl, rdl): 0.2938889563, 0.0052112662

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 63
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.046, 0.080
batch losses (mrrl, rdl): 0.0001667585, 0.0003116265

Epoch over!
epoch time: 13.229

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.050, 0.032

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   65 	     0 	 0.08320 	 0.00018 	 m..s
   47 	     1 	 0.06073 	 0.00019 	 m..s
   18 	     2 	 0.04732 	 0.00020 	 m..s
   79 	     3 	 0.12028 	 0.00021 	 MISS
   14 	     4 	 0.04612 	 0.00022 	 m..s
    1 	     5 	 0.04249 	 0.00022 	 m..s
    9 	     6 	 0.04449 	 0.00022 	 m..s
  118 	     7 	 0.22187 	 0.00023 	 MISS
  105 	     8 	 0.15561 	 0.00024 	 MISS
   98 	     9 	 0.15113 	 0.00026 	 MISS
    7 	    10 	 0.04410 	 0.00026 	 m..s
   46 	    11 	 0.06003 	 0.00027 	 m..s
   34 	    12 	 0.05562 	 0.00028 	 m..s
   41 	    13 	 0.05752 	 0.00028 	 m..s
   78 	    14 	 0.11912 	 0.00028 	 MISS
   86 	    15 	 0.12729 	 0.00028 	 MISS
   44 	    16 	 0.05803 	 0.00028 	 m..s
   72 	    17 	 0.09493 	 0.00029 	 m..s
   63 	    18 	 0.07994 	 0.00032 	 m..s
   23 	    19 	 0.04896 	 0.00032 	 m..s
   50 	    20 	 0.06425 	 0.00032 	 m..s
   76 	    21 	 0.10891 	 0.00033 	 MISS
   87 	    22 	 0.14100 	 0.00034 	 MISS
   73 	    23 	 0.09551 	 0.00036 	 m..s
   24 	    24 	 0.04969 	 0.00037 	 m..s
   58 	    25 	 0.07703 	 0.00038 	 m..s
    3 	    26 	 0.04355 	 0.00040 	 m..s
   54 	    27 	 0.07457 	 0.00040 	 m..s
  108 	    28 	 0.17467 	 0.00043 	 MISS
   28 	    29 	 0.05196 	 0.00046 	 m..s
   61 	    30 	 0.07777 	 0.00050 	 m..s
   92 	    31 	 0.14346 	 0.00051 	 MISS
  116 	    32 	 0.19870 	 0.00055 	 MISS
   56 	    33 	 0.07591 	 0.00061 	 m..s
  100 	    34 	 0.15180 	 0.00066 	 MISS
  104 	    35 	 0.15524 	 0.00080 	 MISS
  102 	    36 	 0.15266 	 0.00083 	 MISS
   75 	    37 	 0.10563 	 0.00087 	 MISS
   42 	    38 	 0.05780 	 0.00088 	 m..s
   37 	    39 	 0.05580 	 0.00089 	 m..s
   11 	    40 	 0.04462 	 0.00108 	 m..s
    5 	    41 	 0.04378 	 0.00121 	 m..s
   16 	    42 	 0.04619 	 0.00221 	 m..s
   13 	    43 	 0.04605 	 0.00270 	 m..s
    1 	    44 	 0.04249 	 0.00351 	 m..s
   20 	    45 	 0.04751 	 0.01467 	 m..s
   57 	    46 	 0.07679 	 0.01774 	 m..s
   29 	    47 	 0.05236 	 0.01915 	 m..s
    6 	    48 	 0.04399 	 0.02036 	 ~...
   22 	    49 	 0.04805 	 0.02359 	 ~...
   62 	    50 	 0.07822 	 0.02510 	 m..s
   24 	    51 	 0.04969 	 0.03155 	 ~...
   26 	    52 	 0.05027 	 0.03183 	 ~...
   19 	    53 	 0.04734 	 0.03464 	 ~...
   67 	    54 	 0.08348 	 0.03767 	 m..s
   12 	    55 	 0.04470 	 0.03801 	 ~...
    0 	    56 	 0.04249 	 0.04061 	 ~...
    9 	    57 	 0.04449 	 0.04983 	 ~...
   37 	    58 	 0.05580 	 0.04998 	 ~...
   21 	    59 	 0.04756 	 0.05032 	 ~...
   15 	    60 	 0.04613 	 0.05294 	 ~...
   51 	    61 	 0.06557 	 0.05456 	 ~...
   17 	    62 	 0.04626 	 0.05733 	 ~...
   36 	    63 	 0.05574 	 0.05784 	 ~...
    8 	    64 	 0.04411 	 0.05908 	 ~...
   33 	    65 	 0.05502 	 0.06036 	 ~...
    4 	    66 	 0.04357 	 0.06577 	 ~...
   77 	    67 	 0.10980 	 0.06844 	 m..s
   35 	    68 	 0.05565 	 0.06858 	 ~...
   48 	    69 	 0.06077 	 0.07030 	 ~...
   42 	    70 	 0.05780 	 0.07299 	 ~...
   29 	    71 	 0.05236 	 0.07602 	 ~...
   82 	    72 	 0.12158 	 0.08498 	 m..s
   45 	    73 	 0.05999 	 0.09117 	 m..s
   40 	    74 	 0.05628 	 0.09156 	 m..s
   69 	    75 	 0.09227 	 0.09668 	 ~...
   87 	    76 	 0.14100 	 0.09781 	 m..s
   39 	    77 	 0.05588 	 0.10002 	 m..s
   31 	    78 	 0.05317 	 0.10106 	 m..s
   52 	    79 	 0.06933 	 0.10123 	 m..s
   53 	    80 	 0.07450 	 0.10154 	 ~...
   49 	    81 	 0.06238 	 0.10197 	 m..s
   32 	    82 	 0.05416 	 0.10483 	 m..s
   71 	    83 	 0.09303 	 0.10701 	 ~...
   27 	    84 	 0.05118 	 0.10806 	 m..s
   83 	    85 	 0.12267 	 0.11095 	 ~...
   70 	    86 	 0.09298 	 0.11545 	 ~...
   85 	    87 	 0.12646 	 0.12782 	 ~...
   98 	    88 	 0.15113 	 0.12878 	 ~...
   84 	    89 	 0.12403 	 0.12979 	 ~...
   66 	    90 	 0.08343 	 0.13225 	 m..s
   59 	    91 	 0.07760 	 0.13455 	 m..s
   97 	    92 	 0.15109 	 0.13518 	 ~...
  110 	    93 	 0.18216 	 0.13593 	 m..s
   68 	    94 	 0.09164 	 0.13650 	 m..s
   60 	    95 	 0.07764 	 0.13795 	 m..s
   94 	    96 	 0.14773 	 0.14057 	 ~...
   89 	    97 	 0.14189 	 0.14086 	 ~...
   55 	    98 	 0.07579 	 0.14257 	 m..s
   91 	    99 	 0.14292 	 0.14493 	 ~...
   90 	   100 	 0.14259 	 0.14536 	 ~...
   73 	   101 	 0.09551 	 0.15056 	 m..s
   92 	   102 	 0.14346 	 0.15287 	 ~...
  101 	   103 	 0.15217 	 0.15452 	 ~...
  111 	   104 	 0.18455 	 0.15681 	 ~...
   81 	   105 	 0.12150 	 0.16211 	 m..s
   79 	   106 	 0.12028 	 0.16298 	 m..s
   96 	   107 	 0.14856 	 0.16581 	 ~...
  103 	   108 	 0.15302 	 0.16589 	 ~...
   95 	   109 	 0.14777 	 0.16627 	 ~...
  114 	   110 	 0.19015 	 0.17039 	 ~...
   64 	   111 	 0.07998 	 0.17815 	 m..s
  115 	   112 	 0.19607 	 0.18628 	 ~...
  112 	   113 	 0.18832 	 0.19066 	 ~...
  118 	   114 	 0.22187 	 0.19776 	 ~...
  117 	   115 	 0.22108 	 0.20264 	 ~...
  106 	   116 	 0.15969 	 0.22048 	 m..s
  108 	   117 	 0.17467 	 0.22896 	 m..s
  107 	   118 	 0.16152 	 0.26518 	 MISS
  113 	   119 	 0.18989 	 0.31001 	 MISS
  120 	   120 	 0.22187 	 0.32098 	 m..s
==========================================
r_mrr = 0.5471264719963074
r2_mrr = 0.1663910150527954
spearmanr_mrr@5 = 0.6164366006851196
spearmanr_mrr@10 = 0.8833343386650085
spearmanr_mrr@50 = 0.9487568736076355
spearmanr_mrr@100 = 0.9848076105117798
spearmanr_mrr@All = 0.9871940612792969
==========================================
test time: 0.389
Done Testing dataset DBpedia50
total time taken: 194.1668140888214
training time taken: 188.96869707107544
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.5471)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.1664)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.6164)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8833)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9488)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9848)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9872)}}, 'test_loss': {'TransE': {'DBpedia50': 3.11403592073475}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 2519578377623106
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [946, 1115, 365, 374, 1125, 559, 777, 1046, 5, 460, 669, 927, 1156, 847, 453, 533, 565, 487, 479, 1160, 283, 502, 948, 394, 599, 953, 231, 1071, 773, 807, 687, 858, 1053, 952, 797, 1092, 302, 315, 1173, 177, 409, 14, 708, 232, 132, 430, 1129, 1209, 892, 134, 793, 1081, 623, 918, 1158, 543, 399, 75, 690, 261, 766, 1002, 654, 1, 703, 494, 506, 850, 433, 735, 792, 72, 184, 814, 505, 9, 836, 8, 1033, 758, 267, 92, 595, 928, 384, 611, 775, 45, 320, 33, 1098, 545, 345, 491, 661, 1001, 206, 883, 391, 373, 1161, 864, 974, 975, 239, 606, 523, 155, 1166, 389, 1049, 678, 934, 70, 589, 925, 939, 1057, 1176, 715, 1143]
valid_ids (0): []
train_ids (1094): [252, 720, 323, 564, 825, 423, 1184, 450, 326, 10, 577, 891, 106, 557, 629, 995, 916, 21, 1153, 637, 973, 36, 878, 930, 1084, 718, 726, 461, 738, 1132, 1083, 113, 124, 1059, 949, 586, 18, 635, 1089, 674, 1127, 1126, 982, 47, 406, 67, 520, 485, 750, 489, 879, 377, 30, 376, 762, 516, 943, 1031, 555, 986, 861, 828, 295, 388, 756, 319, 172, 412, 515, 658, 429, 572, 272, 795, 769, 696, 684, 811, 1061, 761, 273, 622, 193, 714, 960, 812, 787, 870, 278, 392, 662, 324, 655, 154, 783, 385, 857, 809, 26, 481, 932, 349, 1114, 573, 356, 846, 958, 378, 1137, 439, 138, 675, 888, 774, 298, 346, 43, 996, 819, 933, 1020, 474, 162, 544, 615, 260, 670, 1155, 170, 894, 331, 563, 192, 994, 82, 619, 826, 620, 277, 367, 1171, 390, 576, 69, 281, 1019, 317, 947, 660, 935, 709, 929, 532, 1067, 587, 386, 1186, 236, 626, 628, 1111, 173, 202, 621, 194, 1204, 122, 652, 227, 890, 411, 148, 276, 362, 1009, 457, 404, 369, 395, 601, 695, 881, 127, 845, 63, 956, 446, 496, 957, 1110, 136, 581, 452, 294, 1212, 503, 116, 51, 1203, 471, 665, 803, 765, 663, 683, 197, 62, 689, 1139, 309, 393, 1193, 451, 344, 251, 142, 189, 259, 596, 174, 329, 217, 796, 548, 989, 722, 977, 966, 1178, 438, 799, 1121, 478, 978, 1174, 490, 1076, 435, 1105, 729, 422, 732, 1078, 872, 467, 17, 185, 1116, 743, 1042, 1024, 530, 906, 649, 582, 1068, 226, 816, 269, 328, 552, 509, 354, 1179, 1128, 744, 131, 483, 1006, 1198, 125, 920, 224, 262, 425, 728, 874, 211, 27, 876, 180, 1181, 691, 806, 643, 440, 352, 782, 198, 396, 558, 991, 99, 144, 476, 482, 274, 46, 215, 962, 228, 257, 1085, 1087, 889, 468, 417, 1054, 473, 838, 1072, 550, 877, 1051, 264, 980, 831, 848, 519, 335, 1063, 1135, 221, 379, 118, 119, 1044, 1007, 1148, 157, 98, 321, 907, 1034, 723, 68, 554, 727, 48, 188, 94, 898, 842, 244, 421, 455, 786, 223, 77, 712, 763, 1177, 107, 896, 165, 528, 922, 1010, 618, 1022, 357, 80, 39, 105, 1101, 993, 470, 1090, 103, 616, 149, 753, 679, 1069, 527, 495, 608, 199, 97, 242, 746, 330, 686, 488, 900, 710, 1107, 290, 271, 1207, 1104, 535, 921, 359, 327, 759, 380, 751, 1138, 1168, 268, 146, 713, 778, 940, 549, 1014, 614, 969, 711, 1048, 147, 871, 1106, 1093, 1065, 432, 585, 694, 0, 1201, 431, 1151, 1088, 1102, 574, 229, 992, 768, 987, 682, 540, 292, 1062, 464, 1025, 334, 204, 1169, 1142, 1191, 1208, 50, 513, 602, 1013, 1058, 914, 1146, 37, 307, 954, 1008, 126, 1205, 1074, 944, 187, 212, 560, 286, 1196, 284, 141, 1035, 854, 499, 917, 588, 398, 737, 1199, 332, 716, 59, 76, 990, 901, 238, 810, 899, 931, 104, 444, 312, 873, 772, 1124, 347, 688, 979, 905, 667, 1036, 339, 869, 463, 818, 692, 536, 3, 32, 893, 225, 780, 627, 58, 579, 859, 158, 1012, 1206, 531, 524, 400, 297, 537, 760, 1170, 604, 145, 521, 650, 266, 484, 243, 325, 466, 465, 591, 827, 764, 1144, 644, 458, 1150, 970, 366, 755, 437, 419, 1162, 401, 701, 741, 1103, 843, 1094, 371, 776, 1005, 407, 168, 190, 1082, 784, 833, 52, 634, 91, 74, 250, 1004, 805, 11, 603, 348, 672, 717, 340, 265, 213, 361, 160, 849, 724, 1192, 997, 1149, 299, 248, 733, 964, 856, 959, 676, 1100, 253, 245, 1141, 343, 510, 90, 834, 112, 133, 196, 641, 525, 441, 707, 166, 924, 1163, 742, 815, 538, 280, 191, 287, 651, 445, 44, 84, 875, 129, 1038, 779, 61, 913, 1167, 121, 613, 341, 1189, 291, 195, 1064, 951, 370, 1079, 424, 1214, 115, 255, 685, 1211, 1195, 167, 1152, 1190, 454, 1086, 1210, 645, 653, 547, 919, 153, 950, 405, 29, 642, 342, 853, 6, 1077, 447, 83, 42, 813, 34, 546, 85, 902, 353, 504, 820, 486, 397, 1052, 1200, 590, 114, 702, 1165, 556, 804, 183, 443, 462, 798, 78, 1164, 408, 976, 222, 275, 1140, 955, 566, 680, 1113, 985, 88, 182, 971, 886, 968, 247, 288, 333, 1029, 794, 562, 740, 1017, 909, 988, 673, 410, 830, 817, 270, 128, 1117, 1123, 575, 64, 1073, 387, 498, 1157, 338, 363, 171, 705, 150, 730, 71, 840, 865, 426, 305, 942, 617, 293, 402, 625, 829, 1131, 719, 910, 638, 569, 984, 1041, 57, 789, 35, 375, 164, 159, 1091, 304, 12, 844, 624, 256, 96, 936, 220, 66, 1112, 1011, 648, 1095, 529, 81, 175, 137, 882, 55, 120, 434, 38, 1183, 203, 923, 631, 73, 1136, 911, 868, 1023, 210, 551, 216, 981, 143, 668, 594, 657, 785, 736, 1075, 414, 561, 235, 757, 837, 1108, 54, 28, 15, 108, 233, 40, 355, 1147, 938, 181, 771, 522, 605, 130, 704, 511, 926, 1045, 553, 1120, 752, 915, 135, 823, 459, 310, 318, 767, 500, 632, 904, 863, 246, 22, 1070, 2, 20, 381, 65, 788, 903, 580, 95, 436, 416, 748, 791, 1056, 636, 1000, 102, 633, 1040, 963, 855, 607, 364, 477, 731, 699, 802, 301, 230, 497, 442, 1021, 420, 1039, 1026, 646, 163, 140, 912, 79, 1175, 311, 1109, 967, 539, 214, 571, 427, 972, 862, 880, 475, 1188, 139, 492, 101, 822, 998, 1030, 1197, 93, 178, 205, 13, 1122, 1015, 570, 49, 337, 698, 241, 1187, 584, 1060, 860, 508, 583, 1003, 983, 999, 598, 739, 1159, 1066, 200, 1213, 382, 89, 351, 24, 87, 111, 1194, 677, 413, 821, 218, 117, 156, 1018, 313, 965, 169, 700, 1097, 824, 1037, 542, 885, 808, 518, 835, 1202, 693, 1185, 428, 1016, 1047, 541, 152, 472, 945, 176, 306, 801, 110, 469, 53, 1080, 161, 285, 360, 1096, 866, 639, 725, 1145, 754, 884, 526, 908, 578, 841, 1099, 832, 418, 659, 350, 449, 186, 41, 507, 237, 514, 597, 1130, 60, 1154, 656, 19, 852, 316, 151, 372, 358, 1050, 512, 610, 480, 109, 1043, 314, 448, 207, 895, 1119, 282, 612, 1172, 263, 897, 336, 567, 415, 16, 600, 630, 697, 201, 517, 56, 1028, 7, 368, 289, 867, 1027, 403, 279, 208, 593, 25, 300, 800, 706, 209, 456, 249, 745, 1055, 219, 887, 1134, 681, 308, 1032, 1118, 23, 1182, 671, 734, 179, 322, 839, 640, 609, 4, 501, 86, 749, 961, 383, 721, 647, 234, 790, 254, 664, 747, 1180, 123, 666, 100, 1133, 240, 770, 851, 568, 296, 534, 941, 592, 493, 303, 31, 781, 937, 258]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1252934476557140
the save name prefix for this run is:  chkpt-ID_1252934476557140_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 567
rank avg (pred): 0.588 +- 0.002
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0011535508

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 997
rank avg (pred): 0.086 +- 0.069
mrr vals (pred, true): 0.212, 0.293
batch losses (mrrl, rdl): 0.0, 2.60644e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 293
rank avg (pred): 0.078 +- 0.069
mrr vals (pred, true): 0.354, 0.209
batch losses (mrrl, rdl): 0.0, 3.22405e-05

Epoch over!
epoch time: 11.852

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.131 +- 0.123
mrr vals (pred, true): 0.353, 0.139
batch losses (mrrl, rdl): 0.0, 4.69162e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 225
rank avg (pred): 0.344 +- 0.286
mrr vals (pred, true): 0.340, 0.000
batch losses (mrrl, rdl): 0.0, 0.00036741

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.073 +- 0.069
mrr vals (pred, true): 0.386, 0.159
batch losses (mrrl, rdl): 0.0, 4.88417e-05

Epoch over!
epoch time: 12.363

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 165
rank avg (pred): 0.312 +- 0.285
mrr vals (pred, true): 0.374, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002947701

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.307 +- 0.290
mrr vals (pred, true): 0.382, 0.001
batch losses (mrrl, rdl): 0.0, 0.000237831

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.325 +- 0.296
mrr vals (pred, true): 0.370, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001651801

Epoch over!
epoch time: 12.25

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 510
rank avg (pred): 0.236 +- 0.227
mrr vals (pred, true): 0.386, 0.060
batch losses (mrrl, rdl): 0.0, 4.77535e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.077 +- 0.075
mrr vals (pred, true): 0.399, 0.156
batch losses (mrrl, rdl): 0.0, 3.58626e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1168
rank avg (pred): 0.354 +- 0.326
mrr vals (pred, true): 0.373, 0.082
batch losses (mrrl, rdl): 0.0, 9.99366e-05

Epoch over!
epoch time: 12.13

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 131
rank avg (pred): 0.306 +- 0.296
mrr vals (pred, true): 0.391, 0.067
batch losses (mrrl, rdl): 0.0, 0.000423534

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.320 +- 0.299
mrr vals (pred, true): 0.369, 0.174
batch losses (mrrl, rdl): 0.0, 0.000414671

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 6
rank avg (pred): 0.137 +- 0.136
mrr vals (pred, true): 0.395, 0.092
batch losses (mrrl, rdl): 0.0, 4.85255e-05

Epoch over!
epoch time: 12.463

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 782
rank avg (pred): 0.321 +- 0.304
mrr vals (pred, true): 0.375, 0.169
batch losses (mrrl, rdl): 0.4238179624, 0.0002952966

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1134
rank avg (pred): 0.406 +- 0.256
mrr vals (pred, true): 0.106, 0.097
batch losses (mrrl, rdl): 0.0315683484, 0.0005630384

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.462 +- 0.168
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.000224348, 0.0005617204

Epoch over!
epoch time: 12.242

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 161
rank avg (pred): 0.426 +- 0.187
mrr vals (pred, true): 0.063, 0.092
batch losses (mrrl, rdl): 0.0016773161, 0.0011650543

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.399 +- 0.171
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001141854, 0.00016826

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 602
rank avg (pred): 0.402 +- 0.146
mrr vals (pred, true): 0.050, 0.023
batch losses (mrrl, rdl): 1.6878e-06, 0.0002509706

Epoch over!
epoch time: 12.761

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.372 +- 0.165
mrr vals (pred, true): 0.067, 0.125
batch losses (mrrl, rdl): 0.033899691, 0.0009515788

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 415
rank avg (pred): 0.369 +- 0.150
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003844523, 0.0003364474

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 400
rank avg (pred): 0.347 +- 0.153
mrr vals (pred, true): 0.073, 0.076
batch losses (mrrl, rdl): 0.005151351, 0.000440488

Epoch over!
epoch time: 13.645

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 805
rank avg (pred): 0.336 +- 0.164
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0146941105, 0.0003833522

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.363 +- 0.142
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.65981e-05, 0.0003245396

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.315 +- 0.161
mrr vals (pred, true): 0.084, 0.167
batch losses (mrrl, rdl): 0.0695896819, 0.0003731268

Epoch over!
epoch time: 12.89

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1022
rank avg (pred): 0.304 +- 0.172
mrr vals (pred, true): 0.105, 0.180
batch losses (mrrl, rdl): 0.0566271171, 0.0005121708

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 22
rank avg (pred): 0.180 +- 0.113
mrr vals (pred, true): 0.119, 0.127
batch losses (mrrl, rdl): 0.0005951228, 0.0001846763

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 829
rank avg (pred): 0.208 +- 0.128
mrr vals (pred, true): 0.110, 0.142
batch losses (mrrl, rdl): 0.0104837222, 0.0003381871

Epoch over!
epoch time: 12.853

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 429
rank avg (pred): 0.352 +- 0.124
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 9.0984e-06, 0.0004637033

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 36
rank avg (pred): 0.293 +- 0.162
mrr vals (pred, true): 0.061, 0.070
batch losses (mrrl, rdl): 0.0012264452, 0.0006969264

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 445
rank avg (pred): 0.345 +- 0.137
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001540556, 0.0002979221

Epoch over!
epoch time: 12.659

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 78
rank avg (pred): 0.259 +- 0.155
mrr vals (pred, true): 0.102, 0.117
batch losses (mrrl, rdl): 0.0025140415, 0.0006689054

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.075 +- 0.049
mrr vals (pred, true): 0.175, 0.204
batch losses (mrrl, rdl): 0.0085424734, 2.14172e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1144
rank avg (pred): 0.346 +- 0.139
mrr vals (pred, true): 0.064, 0.093
batch losses (mrrl, rdl): 0.0018681679, 0.000475701

Epoch over!
epoch time: 13.439

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 377
rank avg (pred): 0.313 +- 0.148
mrr vals (pred, true): 0.104, 0.162
batch losses (mrrl, rdl): 0.0342034549, 0.0003501592

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 361
rank avg (pred): 0.335 +- 0.134
mrr vals (pred, true): 0.058, 0.063
batch losses (mrrl, rdl): 0.0007177679, 0.0003085201

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1155
rank avg (pred): 0.383 +- 0.178
mrr vals (pred, true): 0.064, 0.087
batch losses (mrrl, rdl): 0.0018354825, 0.0007855706

Epoch over!
epoch time: 12.948

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.397 +- 0.194
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0087029021, 0.0001518828

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 457
rank avg (pred): 0.333 +- 0.133
mrr vals (pred, true): 0.072, 0.002
batch losses (mrrl, rdl): 0.0048276079, 0.0004169467

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.310 +- 0.146
mrr vals (pred, true): 0.120, 0.000
batch losses (mrrl, rdl): 0.0492320769, 0.0005287635

Epoch over!
epoch time: 13.156

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.324 +- 0.160
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0333801284, 0.0004305373

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 855
rank avg (pred): 0.309 +- 0.160
mrr vals (pred, true): 0.108, 0.161
batch losses (mrrl, rdl): 0.0279299021, 0.0004818254

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.271 +- 0.172
mrr vals (pred, true): 0.136, 0.115
batch losses (mrrl, rdl): 0.0042794575, 0.0004907453

Epoch over!
epoch time: 11.948

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.298 +- 0.165
mrr vals (pred, true): 0.102, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  101 	     0 	 0.12164 	 0.00019 	 MISS
    3 	     1 	 0.04886 	 0.00020 	 m..s
  109 	     2 	 0.13560 	 0.00021 	 MISS
   71 	     3 	 0.10181 	 0.00024 	 MISS
  112 	     4 	 0.14466 	 0.00024 	 MISS
    0 	     5 	 0.04827 	 0.00025 	 m..s
   81 	     6 	 0.10960 	 0.00025 	 MISS
   33 	     7 	 0.05019 	 0.00026 	 m..s
   14 	     8 	 0.04923 	 0.00027 	 m..s
   64 	     9 	 0.09405 	 0.00028 	 m..s
   80 	    10 	 0.10907 	 0.00029 	 MISS
  102 	    11 	 0.12261 	 0.00029 	 MISS
   23 	    12 	 0.04987 	 0.00029 	 m..s
  105 	    13 	 0.12936 	 0.00030 	 MISS
    9 	    14 	 0.04910 	 0.00030 	 m..s
   97 	    15 	 0.11393 	 0.00032 	 MISS
    9 	    16 	 0.04910 	 0.00032 	 m..s
   72 	    17 	 0.10225 	 0.00034 	 MISS
    1 	    18 	 0.04866 	 0.00034 	 m..s
   32 	    19 	 0.05016 	 0.00036 	 m..s
   48 	    20 	 0.05951 	 0.00038 	 m..s
   63 	    21 	 0.09332 	 0.00038 	 m..s
   84 	    22 	 0.11111 	 0.00039 	 MISS
   38 	    23 	 0.05079 	 0.00039 	 m..s
    7 	    24 	 0.04901 	 0.00040 	 m..s
   21 	    25 	 0.04985 	 0.00041 	 m..s
   61 	    26 	 0.08713 	 0.00042 	 m..s
   21 	    27 	 0.04985 	 0.00043 	 m..s
   17 	    28 	 0.04934 	 0.00044 	 m..s
   15 	    29 	 0.04932 	 0.00051 	 m..s
  104 	    30 	 0.12802 	 0.00053 	 MISS
   40 	    31 	 0.05121 	 0.00053 	 m..s
   98 	    32 	 0.11759 	 0.00054 	 MISS
    1 	    33 	 0.04866 	 0.00063 	 m..s
    5 	    34 	 0.04894 	 0.00067 	 m..s
   96 	    35 	 0.11371 	 0.00080 	 MISS
   15 	    36 	 0.04932 	 0.00111 	 m..s
    4 	    37 	 0.04891 	 0.00157 	 m..s
   52 	    38 	 0.06329 	 0.00194 	 m..s
    6 	    39 	 0.04898 	 0.00571 	 m..s
   18 	    40 	 0.04965 	 0.01248 	 m..s
   13 	    41 	 0.04921 	 0.02041 	 ~...
    8 	    42 	 0.04909 	 0.02194 	 ~...
   12 	    43 	 0.04915 	 0.02359 	 ~...
   35 	    44 	 0.05028 	 0.02510 	 ~...
   19 	    45 	 0.04980 	 0.03364 	 ~...
   11 	    46 	 0.04910 	 0.03425 	 ~...
   41 	    47 	 0.05153 	 0.04070 	 ~...
   20 	    48 	 0.04981 	 0.04262 	 ~...
   43 	    49 	 0.05203 	 0.04743 	 ~...
   37 	    50 	 0.05067 	 0.05024 	 ~...
   41 	    51 	 0.05153 	 0.05032 	 ~...
   47 	    52 	 0.05944 	 0.05450 	 ~...
   35 	    53 	 0.05028 	 0.05453 	 ~...
   24 	    54 	 0.04991 	 0.05722 	 ~...
   25 	    55 	 0.04996 	 0.05784 	 ~...
   30 	    56 	 0.05008 	 0.05932 	 ~...
   31 	    57 	 0.05009 	 0.06330 	 ~...
   54 	    58 	 0.06618 	 0.06609 	 ~...
   29 	    59 	 0.05008 	 0.06811 	 ~...
   39 	    60 	 0.05106 	 0.06864 	 ~...
   51 	    61 	 0.06313 	 0.06992 	 ~...
   45 	    62 	 0.05512 	 0.07063 	 ~...
   26 	    63 	 0.04999 	 0.07108 	 ~...
   34 	    64 	 0.05025 	 0.07218 	 ~...
   44 	    65 	 0.05315 	 0.07478 	 ~...
   59 	    66 	 0.07515 	 0.07523 	 ~...
   46 	    67 	 0.05934 	 0.07562 	 ~...
   28 	    68 	 0.04999 	 0.08064 	 m..s
   53 	    69 	 0.06528 	 0.08153 	 ~...
   94 	    70 	 0.11324 	 0.08564 	 ~...
   49 	    71 	 0.06023 	 0.08693 	 ~...
   26 	    72 	 0.04999 	 0.08933 	 m..s
  111 	    73 	 0.14231 	 0.09016 	 m..s
   76 	    74 	 0.10323 	 0.09280 	 ~...
   67 	    75 	 0.09879 	 0.09472 	 ~...
   91 	    76 	 0.11224 	 0.09734 	 ~...
   57 	    77 	 0.06958 	 0.09766 	 ~...
   58 	    78 	 0.07046 	 0.09770 	 ~...
  115 	    79 	 0.23212 	 0.09781 	 MISS
   60 	    80 	 0.07622 	 0.09877 	 ~...
   87 	    81 	 0.11170 	 0.09974 	 ~...
  110 	    82 	 0.13785 	 0.10013 	 m..s
   85 	    83 	 0.11148 	 0.10350 	 ~...
   78 	    84 	 0.10760 	 0.10377 	 ~...
   79 	    85 	 0.10882 	 0.10483 	 ~...
   89 	    86 	 0.11180 	 0.10908 	 ~...
   49 	    87 	 0.06023 	 0.10944 	 m..s
   85 	    88 	 0.11148 	 0.10984 	 ~...
   94 	    89 	 0.11324 	 0.11143 	 ~...
   56 	    90 	 0.06830 	 0.11594 	 m..s
   77 	    91 	 0.10380 	 0.12037 	 ~...
   62 	    92 	 0.08956 	 0.13458 	 m..s
   73 	    93 	 0.10271 	 0.13795 	 m..s
   55 	    94 	 0.06812 	 0.14443 	 m..s
   89 	    95 	 0.11180 	 0.14636 	 m..s
   69 	    96 	 0.10144 	 0.15056 	 m..s
   99 	    97 	 0.11892 	 0.15098 	 m..s
  100 	    98 	 0.11898 	 0.15352 	 m..s
   92 	    99 	 0.11234 	 0.15452 	 m..s
   75 	   100 	 0.10322 	 0.15482 	 m..s
  106 	   101 	 0.12998 	 0.15681 	 ~...
  114 	   102 	 0.22365 	 0.15977 	 m..s
   68 	   103 	 0.09906 	 0.15989 	 m..s
   69 	   104 	 0.10144 	 0.16298 	 m..s
   82 	   105 	 0.11022 	 0.16360 	 m..s
   74 	   106 	 0.10279 	 0.16519 	 m..s
   88 	   107 	 0.11173 	 0.16914 	 m..s
  107 	   108 	 0.13059 	 0.17044 	 m..s
  103 	   109 	 0.12627 	 0.17157 	 m..s
  108 	   110 	 0.13078 	 0.17200 	 m..s
   66 	   111 	 0.09863 	 0.17251 	 m..s
   82 	   112 	 0.11022 	 0.17428 	 m..s
   93 	   113 	 0.11300 	 0.17815 	 m..s
   65 	   114 	 0.09490 	 0.17849 	 m..s
  117 	   115 	 0.28977 	 0.18863 	 MISS
  113 	   116 	 0.21108 	 0.24065 	 ~...
  117 	   117 	 0.28977 	 0.26518 	 ~...
  119 	   118 	 0.29276 	 0.27822 	 ~...
  116 	   119 	 0.28520 	 0.28002 	 ~...
  120 	   120 	 0.29406 	 0.28691 	 ~...
==========================================
r_mrr = 0.6631773710250854
r2_mrr = 0.36964601278305054
spearmanr_mrr@5 = 0.9437088370323181
spearmanr_mrr@10 = 0.8854978084564209
spearmanr_mrr@50 = 0.8978603482246399
spearmanr_mrr@100 = 0.9242969751358032
spearmanr_mrr@All = 0.930557906627655
==========================================
test time: 0.389
Done Testing dataset DBpedia50
total time taken: 195.17299509048462
training time taken: 190.07464504241943
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.6632)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.3696)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9437)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8855)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.8979)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9243)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9306)}}, 'test_loss': {'TransE': {'DBpedia50': 2.2529766874940833}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 7032145443614279
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [866, 283, 496, 993, 941, 331, 1098, 131, 4, 740, 483, 908, 935, 774, 222, 221, 1114, 1155, 600, 112, 860, 629, 1067, 955, 217, 676, 85, 146, 1206, 270, 800, 594, 1182, 1167, 70, 767, 1143, 1009, 28, 339, 721, 44, 799, 356, 1141, 1078, 525, 9, 446, 148, 858, 406, 267, 651, 1148, 1005, 96, 1127, 383, 63, 364, 1178, 640, 705, 790, 957, 154, 23, 40, 390, 313, 1196, 759, 744, 350, 542, 1164, 598, 487, 426, 990, 887, 209, 127, 527, 752, 116, 1139, 902, 48, 820, 579, 1061, 129, 560, 715, 341, 913, 258, 241, 601, 1187, 208, 596, 829, 1157, 753, 1181, 295, 176, 193, 535, 617, 353, 65, 377, 561, 371, 999, 747, 891]
valid_ids (0): []
train_ids (1094): [852, 95, 595, 287, 338, 1109, 1211, 639, 569, 1100, 796, 500, 842, 1126, 683, 1119, 157, 87, 66, 697, 1016, 247, 1194, 907, 567, 1068, 144, 585, 630, 896, 155, 672, 1184, 196, 714, 756, 704, 628, 572, 304, 1004, 14, 1083, 815, 284, 865, 300, 387, 5, 205, 423, 449, 43, 194, 47, 97, 12, 849, 899, 180, 795, 807, 895, 279, 666, 855, 430, 1207, 1199, 361, 202, 987, 84, 848, 1104, 975, 973, 413, 1147, 691, 395, 792, 653, 1146, 174, 71, 164, 69, 564, 171, 328, 644, 877, 299, 39, 459, 810, 804, 605, 75, 956, 805, 86, 745, 1041, 937, 916, 537, 352, 1069, 1160, 929, 1195, 159, 893, 342, 710, 280, 102, 1032, 1081, 1163, 850, 519, 1076, 1172, 490, 551, 522, 187, 61, 699, 185, 769, 844, 200, 871, 107, 961, 124, 610, 351, 290, 378, 126, 737, 435, 1180, 302, 638, 686, 264, 190, 494, 1120, 1192, 186, 754, 637, 461, 477, 379, 559, 599, 1028, 183, 195, 1095, 854, 189, 346, 121, 1070, 897, 818, 1073, 108, 656, 322, 199, 411, 1110, 237, 486, 802, 372, 1091, 294, 1125, 192, 405, 997, 1133, 979, 1088, 969, 276, 659, 316, 1080, 1118, 652, 967, 1161, 349, 1062, 74, 924, 198, 832, 881, 265, 32, 388, 864, 689, 281, 252, 523, 964, 396, 1193, 197, 616, 803, 172, 938, 417, 528, 433, 974, 568, 1189, 354, 434, 1138, 562, 89, 251, 458, 288, 20, 1013, 268, 685, 647, 983, 761, 1102, 718, 272, 755, 340, 507, 847, 416, 584, 624, 45, 1071, 843, 403, 463, 91, 654, 243, 103, 622, 687, 158, 306, 55, 13, 64, 462, 3, 690, 359, 798, 440, 702, 720, 382, 986, 1027, 33, 249, 822, 1018, 481, 760, 1175, 976, 474, 1074, 374, 473, 1035, 757, 1144, 228, 188, 811, 1090, 750, 468, 256, 18, 266, 883, 489, 540, 0, 471, 46, 869, 840, 1152, 257, 904, 733, 150, 213, 1024, 1129, 175, 401, 472, 278, 381, 611, 1176, 402, 120, 1166, 936, 99, 557, 1072, 424, 688, 779, 271, 425, 436, 138, 137, 118, 214, 545, 1047, 1151, 1149, 1185, 565, 318, 447, 900, 602, 1077, 867, 1030, 49, 526, 26, 515, 1103, 890, 248, 1121, 548, 245, 778, 856, 708, 531, 1135, 1200, 1075, 587, 1170, 54, 771, 948, 207, 36, 173, 942, 370, 764, 879, 109, 15, 603, 658, 1053, 1064, 923, 1212, 94, 319, 184, 1036, 476, 1011, 736, 427, 100, 1168, 636, 231, 422, 835, 591, 1162, 563, 274, 674, 34, 825, 996, 119, 978, 376, 994, 170, 211, 1169, 1079, 592, 1115, 1165, 139, 791, 570, 857, 963, 934, 730, 586, 7, 442, 589, 648, 546, 24, 952, 226, 614, 597, 437, 577, 831, 541, 1210, 335, 38, 549, 385, 244, 125, 814, 837, 786, 503, 580, 882, 412, 410, 414, 1153, 552, 236, 375, 627, 136, 1038, 79, 166, 400, 1204, 457, 419, 1108, 880, 460, 10, 50, 1156, 52, 669, 218, 841, 408, 729, 1046, 1014, 233, 536, 1044, 497, 1128, 254, 1087, 431, 716, 661, 836, 315, 409, 875, 1101, 909, 991, 1105, 140, 1049, 163, 429, 1019, 332, 78, 393, 646, 132, 821, 817, 1150, 806, 1209, 445, 293, 113, 1132, 933, 928, 178, 360, 479, 111, 269, 1065, 451, 670, 945, 1171, 21, 1043, 235, 566, 1026, 41, 1066, 81, 210, 544, 513, 297, 492, 1015, 1099, 989, 1054, 839, 362, 859, 944, 1086, 777, 114, 134, 98, 732, 775, 225, 677, 524, 135, 246, 305, 530, 482, 301, 260, 147, 706, 1085, 1050, 925, 478, 1202, 62, 1021, 450, 262, 1116, 491, 863, 532, 1060, 1097, 418, 1145, 212, 1006, 1029, 696, 443, 72, 998, 204, 995, 366, 1158, 149, 152, 1039, 1188, 303, 992, 469, 224, 1117, 141, 607, 498, 980, 680, 724, 845, 735, 58, 327, 741, 663, 220, 191, 618, 499, 1023, 444, 684, 671, 939, 898, 53, 613, 1213, 931, 389, 165, 323, 307, 915, 312, 878, 1186, 593, 25, 123, 1, 60, 830, 355, 533, 398, 508, 739, 668, 1123, 1048, 678, 940, 160, 679, 550, 285, 731, 80, 358, 828, 1003, 884, 781, 665, 962, 1130, 438, 348, 960, 770, 547, 723, 971, 455, 1177, 619, 1051, 693, 620, 179, 151, 324, 277, 773, 784, 1063, 675, 454, 145, 694, 30, 888, 529, 329, 625, 512, 667, 325, 673, 853, 510, 922, 448, 35, 106, 206, 988, 682, 337, 475, 168, 711, 432, 851, 834, 167, 238, 788, 467, 465, 643, 582, 240, 1198, 912, 1179, 1010, 862, 576, 719, 16, 161, 368, 914, 615, 521, 110, 951, 336, 981, 727, 1058, 657, 292, 631, 1055, 1159, 1084, 1008, 1052, 762, 330, 230, 517, 894, 1089, 801, 509, 954, 259, 919, 573, 782, 789, 29, 984, 397, 484, 273, 751, 1096, 1190, 367, 365, 308, 612, 441, 1034, 1112, 493, 608, 333, 255, 950, 920, 6, 712, 1214, 793, 905, 311, 1092, 953, 321, 642, 581, 946, 326, 588, 664, 275, 1131, 972, 660, 17, 766, 809, 578, 813, 728, 345, 117, 816, 947, 886, 37, 1136, 1124, 966, 700, 373, 239, 558, 968, 142, 1033, 604, 115, 749, 504, 51, 932, 725, 452, 384, 1037, 310, 59, 68, 88, 1107, 153, 819, 133, 713, 1173, 420, 334, 1174, 317, 56, 758, 1042, 910, 1040, 2, 82, 538, 506, 609, 763, 543, 783, 1056, 11, 485, 965, 470, 776, 1022, 215, 949, 701, 906, 590, 872, 958, 917, 1106, 633, 539, 130, 520, 456, 812, 1017, 556, 380, 1183, 101, 911, 182, 76, 641, 219, 901, 386, 726, 128, 709, 19, 1059, 838, 394, 787, 514, 1201, 982, 282, 870, 343, 286, 428, 1093, 1007, 253, 407, 391, 1082, 742, 229, 927, 826, 505, 943, 42, 734, 681, 748, 583, 1140, 698, 518, 242, 27, 415, 892, 645, 309, 404, 1045, 466, 298, 808, 768, 861, 649, 743, 921, 31, 765, 703, 223, 846, 369, 553, 722, 344, 772, 291, 1025, 421, 480, 555, 626, 143, 1094, 738, 632, 1057, 511, 92, 650, 621, 985, 1197, 232, 216, 780, 73, 635, 623, 794, 250, 873, 692, 90, 464, 320, 717, 889, 868, 1001, 662, 105, 347, 314, 707, 575, 1002, 177, 169, 516, 823, 296, 1012, 1134, 234, 439, 57, 67, 574, 903, 162, 357, 824, 918, 930, 201, 1203, 1122, 1020, 488, 122, 1191, 926, 959, 501, 453, 104, 261, 797, 785, 363, 876, 289, 874, 606, 203, 1154, 970, 634, 1000, 263, 495, 1208, 156, 1205, 1113, 833, 227, 655, 695, 8, 1142, 885, 977, 1137, 83, 502, 571, 1111, 22, 392, 181, 827, 77, 93, 746, 399, 534, 1031, 554]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9375447042018908
the save name prefix for this run is:  chkpt-ID_9375447042018908_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 854
rank avg (pred): 0.513 +- 0.003
mrr vals (pred, true): 0.000, 0.162
batch losses (mrrl, rdl): 0.0, 0.0016997298

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 700
rank avg (pred): 0.339 +- 0.238
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005402791

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1214
rank avg (pred): 0.329 +- 0.283
mrr vals (pred, true): 0.006, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004139227

Epoch over!
epoch time: 13.0

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 133
rank avg (pred): 0.331 +- 0.288
mrr vals (pred, true): 0.006, 0.022
batch losses (mrrl, rdl): 0.0, 0.0003953385

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.331 +- 0.299
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003846608

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 314
rank avg (pred): 0.106 +- 0.157
mrr vals (pred, true): 0.240, 0.119
batch losses (mrrl, rdl): 0.0, 1.59562e-05

Epoch over!
epoch time: 12.424

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1013
rank avg (pred): 0.325 +- 0.311
mrr vals (pred, true): 0.102, 0.181
batch losses (mrrl, rdl): 0.0, 0.0006472985

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 86
rank avg (pred): 0.310 +- 0.309
mrr vals (pred, true): 0.237, 0.082
batch losses (mrrl, rdl): 0.0, 0.0005167

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.291 +- 0.306
mrr vals (pred, true): 0.331, 0.066
batch losses (mrrl, rdl): 0.0, 8.04288e-05

Epoch over!
epoch time: 11.813

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 534
rank avg (pred): 0.247 +- 0.270
mrr vals (pred, true): 0.215, 0.020
batch losses (mrrl, rdl): 0.0, 5.12371e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 538
rank avg (pred): 0.266 +- 0.287
mrr vals (pred, true): 0.313, 0.049
batch losses (mrrl, rdl): 0.0, 2.1897e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1045
rank avg (pred): 0.314 +- 0.312
mrr vals (pred, true): 0.260, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004343376

Epoch over!
epoch time: 12.223

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.097 +- 0.195
mrr vals (pred, true): 0.427, 0.139
batch losses (mrrl, rdl): 0.0, 7.796e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.075 +- 0.176
mrr vals (pred, true): 0.444, 0.217
batch losses (mrrl, rdl): 0.0, 2.1592e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 320
rank avg (pred): 0.099 +- 0.205
mrr vals (pred, true): 0.443, 0.160
batch losses (mrrl, rdl): 0.0, 1.91423e-05

Epoch over!
epoch time: 12.152

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 382
rank avg (pred): 0.315 +- 0.311
mrr vals (pred, true): 0.232, 0.055
batch losses (mrrl, rdl): 0.3304471076, 0.0001389342

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.071 +- 0.050
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0309113692, 0.0033662049

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 232
rank avg (pred): 0.353 +- 0.231
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.77141e-05, 0.0001811956

Epoch over!
epoch time: 12.678

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 306
rank avg (pred): 0.059 +- 0.040
mrr vals (pred, true): 0.090, 0.072
batch losses (mrrl, rdl): 0.0156651028, 0.0001630311

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 325
rank avg (pred): 0.375 +- 0.255
mrr vals (pred, true): 0.047, 0.061
batch losses (mrrl, rdl): 7.8657e-05, 0.0003564805

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 20
rank avg (pred): 0.054 +- 0.105
mrr vals (pred, true): 0.157, 0.219
batch losses (mrrl, rdl): 0.0392339937, 3.7136e-06

Epoch over!
epoch time: 11.902

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 223
rank avg (pred): 0.427 +- 0.316
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004920391, 4.48719e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.271 +- 0.342
mrr vals (pred, true): 0.085, 0.101
batch losses (mrrl, rdl): 0.0026815201, 0.0003464076

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 639
rank avg (pred): 0.363 +- 0.277
mrr vals (pred, true): 0.054, 0.002
batch losses (mrrl, rdl): 0.0001387707, 2.06858e-05

Epoch over!
epoch time: 11.795

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.018 +- 0.039
mrr vals (pred, true): 0.206, 0.301
batch losses (mrrl, rdl): 0.0900139511, 3.82178e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.352 +- 0.374
mrr vals (pred, true): 0.116, 0.174
batch losses (mrrl, rdl): 0.0335242674, 0.000354978

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.296 +- 0.346
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0268367864, 0.0008121143

Epoch over!
epoch time: 12.584

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1000
rank avg (pred): 0.195 +- 0.267
mrr vals (pred, true): 0.109, 0.122
batch losses (mrrl, rdl): 0.0018750777, 7.0132e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 952
rank avg (pred): 0.447 +- 0.356
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0062283087, 2.96635e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 877
rank avg (pred): 0.316 +- 0.352
mrr vals (pred, true): 0.120, 0.000
batch losses (mrrl, rdl): 0.0487552546, 0.0006037804

Epoch over!
epoch time: 12.216

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 739
rank avg (pred): 0.170 +- 0.292
mrr vals (pred, true): 0.139, 0.132
batch losses (mrrl, rdl): 0.000463326, 0.0001342092

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 485
rank avg (pred): 0.269 +- 0.326
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0011584947, 0.0005728302

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 492
rank avg (pred): 0.516 +- 0.284
mrr vals (pred, true): 0.048, 0.034
batch losses (mrrl, rdl): 4.31242e-05, 0.0014239602

Epoch over!
epoch time: 12.801

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 619
rank avg (pred): 0.478 +- 0.280
mrr vals (pred, true): 0.048, 0.011
batch losses (mrrl, rdl): 5.49399e-05, 0.0004584683

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 731
rank avg (pred): 0.293 +- 0.323
mrr vals (pred, true): 0.105, 0.085
batch losses (mrrl, rdl): 0.0306418519, 0.0005530761

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 827
rank avg (pred): 0.104 +- 0.179
mrr vals (pred, true): 0.186, 0.222
batch losses (mrrl, rdl): 0.0130783711, 5.71527e-05

Epoch over!
epoch time: 12.022

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 775
rank avg (pred): 0.323 +- 0.320
mrr vals (pred, true): 0.118, 0.164
batch losses (mrrl, rdl): 0.0212459434, 0.0003609233

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1088
rank avg (pred): 0.138 +- 0.212
mrr vals (pred, true): 0.137, 0.192
batch losses (mrrl, rdl): 0.0304517597, 1.0811e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.282 +- 0.277
mrr vals (pred, true): 0.113, 0.000
batch losses (mrrl, rdl): 0.0396018326, 0.0005036782

Epoch over!
epoch time: 13.235

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.313 +- 0.266
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0256675668, 0.0004218405

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.296 +- 0.293
mrr vals (pred, true): 0.118, 0.191
batch losses (mrrl, rdl): 0.0520746559, 0.0003815649

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1027
rank avg (pred): 0.230 +- 0.262
mrr vals (pred, true): 0.132, 0.000
batch losses (mrrl, rdl): 0.0673794672, 0.000934008

Epoch over!
epoch time: 13.556

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.048 +- 0.079
mrr vals (pred, true): 0.249, 0.189
batch losses (mrrl, rdl): 0.0368295424, 5.26668e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1066
rank avg (pred): 0.061 +- 0.093
mrr vals (pred, true): 0.247, 0.310
batch losses (mrrl, rdl): 0.0400310233, 6.7268e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 5
rank avg (pred): 0.060 +- 0.070
mrr vals (pred, true): 0.169, 0.138
batch losses (mrrl, rdl): 0.0093491673, 1.2152e-05

Epoch over!
epoch time: 12.639

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.358 +- 0.320
mrr vals (pred, true): 0.089, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.05147 	 0.00019 	 m..s
   19 	     1 	 0.05575 	 0.00021 	 m..s
    1 	     2 	 0.04934 	 0.00021 	 m..s
    6 	     3 	 0.05150 	 0.00021 	 m..s
   33 	     4 	 0.05780 	 0.00023 	 m..s
   41 	     5 	 0.06002 	 0.00026 	 m..s
   93 	     6 	 0.10555 	 0.00026 	 MISS
   94 	     7 	 0.10558 	 0.00027 	 MISS
   51 	     8 	 0.06339 	 0.00027 	 m..s
   91 	     9 	 0.10533 	 0.00029 	 MISS
   29 	    10 	 0.05772 	 0.00029 	 m..s
  103 	    11 	 0.12834 	 0.00030 	 MISS
   23 	    12 	 0.05610 	 0.00031 	 m..s
   40 	    13 	 0.05962 	 0.00032 	 m..s
    8 	    14 	 0.05156 	 0.00032 	 m..s
   14 	    15 	 0.05511 	 0.00033 	 m..s
   87 	    16 	 0.10204 	 0.00035 	 MISS
   36 	    17 	 0.05806 	 0.00036 	 m..s
   92 	    18 	 0.10535 	 0.00038 	 MISS
   31 	    19 	 0.05775 	 0.00039 	 m..s
   45 	    20 	 0.06116 	 0.00042 	 m..s
   24 	    21 	 0.05685 	 0.00042 	 m..s
   79 	    22 	 0.08880 	 0.00044 	 m..s
   76 	    23 	 0.08751 	 0.00055 	 m..s
   57 	    24 	 0.07079 	 0.00074 	 m..s
   46 	    25 	 0.06288 	 0.00076 	 m..s
  107 	    26 	 0.13427 	 0.00092 	 MISS
   50 	    27 	 0.06338 	 0.00153 	 m..s
   15 	    28 	 0.05531 	 0.00262 	 m..s
    0 	    29 	 0.04932 	 0.00270 	 m..s
   10 	    30 	 0.05237 	 0.00323 	 m..s
   26 	    31 	 0.05741 	 0.00479 	 m..s
    7 	    32 	 0.05152 	 0.00612 	 m..s
   35 	    33 	 0.05787 	 0.01178 	 m..s
    5 	    34 	 0.05147 	 0.01397 	 m..s
   37 	    35 	 0.05892 	 0.01438 	 m..s
    9 	    36 	 0.05193 	 0.02020 	 m..s
   27 	    37 	 0.05748 	 0.02069 	 m..s
   34 	    38 	 0.05780 	 0.02495 	 m..s
    2 	    39 	 0.05112 	 0.02537 	 ~...
   13 	    40 	 0.05329 	 0.02710 	 ~...
   30 	    41 	 0.05772 	 0.02723 	 m..s
   32 	    42 	 0.05777 	 0.02804 	 ~...
   25 	    43 	 0.05692 	 0.02825 	 ~...
   28 	    44 	 0.05770 	 0.02935 	 ~...
    3 	    45 	 0.05119 	 0.03335 	 ~...
   12 	    46 	 0.05313 	 0.03357 	 ~...
   17 	    47 	 0.05556 	 0.03642 	 ~...
   11 	    48 	 0.05258 	 0.04193 	 ~...
   63 	    49 	 0.07895 	 0.04915 	 ~...
   39 	    50 	 0.05962 	 0.05021 	 ~...
   54 	    51 	 0.06524 	 0.05580 	 ~...
   44 	    52 	 0.06092 	 0.05599 	 ~...
   42 	    53 	 0.06009 	 0.05722 	 ~...
   38 	    54 	 0.05951 	 0.06345 	 ~...
   53 	    55 	 0.06371 	 0.06728 	 ~...
   20 	    56 	 0.05576 	 0.06756 	 ~...
   52 	    57 	 0.06342 	 0.06847 	 ~...
   20 	    58 	 0.05576 	 0.06858 	 ~...
   42 	    59 	 0.06009 	 0.06984 	 ~...
   22 	    60 	 0.05602 	 0.07030 	 ~...
   17 	    61 	 0.05556 	 0.07127 	 ~...
   74 	    62 	 0.08706 	 0.07284 	 ~...
   66 	    63 	 0.08233 	 0.07377 	 ~...
   16 	    64 	 0.05556 	 0.07437 	 ~...
   72 	    65 	 0.08688 	 0.07523 	 ~...
   47 	    66 	 0.06320 	 0.07838 	 ~...
   65 	    67 	 0.08177 	 0.08037 	 ~...
   84 	    68 	 0.09151 	 0.08153 	 ~...
   75 	    69 	 0.08711 	 0.08336 	 ~...
   70 	    70 	 0.08671 	 0.08617 	 ~...
   80 	    71 	 0.08931 	 0.08690 	 ~...
   78 	    72 	 0.08871 	 0.08771 	 ~...
   68 	    73 	 0.08455 	 0.09158 	 ~...
   48 	    74 	 0.06323 	 0.09182 	 ~...
   67 	    75 	 0.08450 	 0.09472 	 ~...
   48 	    76 	 0.06323 	 0.09506 	 m..s
   70 	    77 	 0.08671 	 0.09734 	 ~...
   58 	    78 	 0.07110 	 0.09758 	 ~...
   81 	    79 	 0.08932 	 0.09770 	 ~...
   85 	    80 	 0.09869 	 0.10002 	 ~...
   82 	    81 	 0.09041 	 0.10106 	 ~...
   99 	    82 	 0.11859 	 0.10154 	 ~...
   77 	    83 	 0.08870 	 0.10160 	 ~...
  104 	    84 	 0.13131 	 0.10630 	 ~...
   83 	    85 	 0.09103 	 0.10806 	 ~...
   55 	    86 	 0.07053 	 0.10888 	 m..s
   60 	    87 	 0.07157 	 0.11194 	 m..s
   64 	    88 	 0.08170 	 0.12003 	 m..s
   97 	    89 	 0.11405 	 0.12259 	 ~...
   59 	    90 	 0.07112 	 0.12473 	 m..s
   60 	    91 	 0.07157 	 0.12731 	 m..s
   55 	    92 	 0.07053 	 0.12804 	 m..s
  114 	    93 	 0.21096 	 0.12878 	 m..s
  100 	    94 	 0.12170 	 0.13242 	 ~...
   98 	    95 	 0.11491 	 0.13593 	 ~...
   89 	    96 	 0.10492 	 0.14086 	 m..s
  109 	    97 	 0.20006 	 0.14115 	 m..s
  111 	    98 	 0.20129 	 0.14189 	 m..s
   73 	    99 	 0.08703 	 0.14567 	 m..s
  102 	   100 	 0.12551 	 0.14767 	 ~...
  104 	   101 	 0.13131 	 0.14879 	 ~...
   69 	   102 	 0.08464 	 0.15351 	 m..s
  112 	   103 	 0.20163 	 0.15571 	 m..s
  106 	   104 	 0.13397 	 0.15681 	 ~...
   95 	   105 	 0.10562 	 0.15702 	 m..s
   90 	   106 	 0.10506 	 0.15753 	 m..s
   62 	   107 	 0.07285 	 0.16219 	 m..s
   88 	   108 	 0.10226 	 0.16883 	 m..s
   96 	   109 	 0.10582 	 0.17132 	 m..s
   86 	   110 	 0.10202 	 0.17251 	 m..s
  100 	   111 	 0.12170 	 0.19645 	 m..s
  115 	   112 	 0.21211 	 0.20305 	 ~...
  108 	   113 	 0.15841 	 0.21903 	 m..s
  119 	   114 	 0.27795 	 0.22181 	 m..s
  112 	   115 	 0.20163 	 0.22876 	 ~...
  110 	   116 	 0.20043 	 0.22960 	 ~...
  119 	   117 	 0.27795 	 0.26695 	 ~...
  118 	   118 	 0.27536 	 0.27049 	 ~...
  117 	   119 	 0.27475 	 0.28576 	 ~...
  116 	   120 	 0.26868 	 0.30290 	 m..s
==========================================
r_mrr = 0.7741501927375793
r2_mrr = 0.5567970275878906
spearmanr_mrr@5 = 0.968782901763916
spearmanr_mrr@10 = 0.8771057724952698
spearmanr_mrr@50 = 0.967555582523346
spearmanr_mrr@100 = 0.9368633031845093
spearmanr_mrr@All = 0.9349984526634216
==========================================
test time: 0.394
Done Testing dataset DBpedia50
total time taken: 192.10105347633362
training time taken: 187.51740169525146
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7742)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.5568)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9688)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8771)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9676)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9369)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9350)}}, 'test_loss': {'TransE': {'DBpedia50': 1.4213563397861435}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 5928191506465951
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1135, 1199, 168, 429, 1120, 827, 387, 1206, 760, 331, 852, 1108, 686, 344, 118, 819, 573, 198, 963, 935, 650, 1067, 718, 40, 841, 223, 643, 422, 156, 237, 67, 329, 1211, 473, 1064, 84, 1082, 224, 350, 663, 17, 1087, 561, 1142, 96, 332, 315, 811, 971, 375, 1198, 1008, 29, 820, 221, 220, 5, 148, 1066, 238, 404, 201, 778, 232, 688, 144, 1146, 165, 1012, 1157, 879, 873, 709, 536, 277, 882, 1197, 498, 524, 1192, 63, 788, 931, 653, 1193, 771, 388, 151, 670, 863, 1007, 636, 314, 776, 728, 496, 349, 185, 124, 494, 732, 154, 541, 1075, 1098, 533, 504, 1138, 587, 393, 607, 1046, 854, 608, 1116, 825, 1140, 469, 559, 463, 929]
valid_ids (0): []
train_ids (1094): [1154, 1158, 697, 1004, 91, 359, 453, 632, 545, 209, 822, 327, 1043, 896, 952, 522, 680, 932, 729, 638, 207, 1041, 720, 48, 1048, 659, 321, 1028, 250, 475, 691, 791, 217, 481, 1065, 526, 601, 540, 745, 304, 438, 295, 600, 1056, 972, 391, 1024, 953, 99, 702, 976, 574, 1015, 73, 960, 141, 779, 907, 1068, 758, 560, 282, 310, 877, 1175, 131, 136, 507, 108, 994, 384, 80, 754, 377, 285, 137, 228, 83, 525, 354, 342, 252, 229, 780, 1152, 334, 748, 895, 642, 1011, 950, 455, 208, 848, 657, 180, 172, 1190, 757, 707, 1052, 139, 58, 916, 996, 117, 894, 1039, 385, 1110, 1131, 843, 1059, 170, 286, 1061, 736, 737, 428, 549, 1009, 472, 1047, 322, 248, 263, 784, 613, 122, 1148, 1074, 528, 818, 869, 126, 551, 897, 200, 20, 598, 694, 586, 785, 1072, 700, 685, 970, 977, 660, 1101, 1117, 408, 159, 288, 502, 793, 1111, 75, 775, 566, 509, 905, 888, 458, 519, 406, 413, 734, 465, 437, 770, 163, 986, 973, 520, 471, 581, 743, 340, 205, 495, 503, 347, 805, 910, 1121, 814, 476, 182, 324, 213, 405, 801, 564, 530, 28, 606, 1194, 578, 382, 262, 731, 13, 176, 677, 287, 576, 462, 77, 113, 152, 56, 162, 1070, 202, 194, 865, 920, 762, 817, 1210, 1164, 468, 21, 480, 1174, 1038, 962, 1079, 904, 641, 1153, 542, 240, 585, 426, 487, 478, 662, 49, 204, 22, 257, 753, 150, 1163, 491, 749, 1034, 701, 403, 1005, 992, 348, 328, 1107, 1205, 1088, 974, 192, 810, 596, 695, 1027, 42, 925, 1159, 772, 269, 889, 1080, 605, 312, 673, 804, 206, 203, 357, 837, 103, 956, 1089, 866, 450, 553, 712, 816, 308, 787, 957, 272, 386, 571, 371, 740, 742, 1172, 1014, 179, 330, 409, 23, 547, 16, 730, 158, 868, 421, 27, 146, 281, 72, 693, 361, 190, 1026, 548, 479, 1203, 1020, 713, 0, 358, 1130, 510, 1053, 1186, 696, 254, 887, 432, 947, 672, 452, 618, 768, 592, 32, 993, 639, 698, 111, 107, 355, 412, 800, 690, 1050, 1109, 591, 912, 433, 1042, 774, 764, 270, 726, 903, 1195, 50, 214, 362, 682, 197, 565, 95, 1171, 915, 186, 86, 24, 485, 537, 517, 934, 1168, 937, 69, 187, 1181, 265, 338, 374, 913, 647, 215, 1045, 946, 750, 497, 477, 1165, 18, 630, 756, 216, 1208, 851, 1114, 85, 292, 160, 789, 516, 134, 1123, 1118, 3, 1188, 30, 311, 886, 420, 1051, 918, 961, 241, 500, 181, 1083, 944, 235, 460, 449, 558, 634, 142, 431, 101, 246, 759, 346, 948, 501, 105, 883, 773, 893, 747, 554, 114, 400, 424, 876, 430, 583, 1139, 684, 703, 880, 562, 674, 279, 615, 809, 802, 299, 555, 881, 710, 267, 398, 1169, 864, 383, 389, 1161, 1078, 826, 378, 445, 43, 301, 1115, 1113, 425, 951, 518, 620, 579, 751, 723, 699, 922, 7, 37, 1173, 942, 652, 411, 644, 47, 336, 130, 1177, 651, 39, 1071, 1006, 1129, 623, 563, 1150, 390, 899, 1000, 399, 352, 489, 923, 293, 927, 26, 439, 323, 326, 178, 123, 1094, 557, 648, 884, 1214, 668, 964, 175, 1097, 546, 514, 829, 1081, 132, 366, 1049, 177, 872, 844, 830, 459, 975, 924, 183, 997, 980, 78, 61, 813, 1187, 998, 376, 656, 834, 954, 654, 1044, 264, 637, 544, 102, 958, 898, 319, 523, 681, 309, 965, 725, 616, 1084, 812, 933, 236, 1145, 906, 320, 368, 276, 531, 1162, 949, 10, 715, 1143, 741, 1191, 402, 512, 1076, 506, 333, 850, 414, 143, 629, 885, 38, 360, 856, 291, 1010, 1054, 1144, 225, 917, 149, 717, 396, 33, 456, 1096, 278, 302, 714, 840, 110, 767, 233, 724, 138, 577, 945, 534, 938, 766, 679, 369, 646, 1037, 247, 870, 335, 53, 966, 230, 318, 987, 835, 461, 129, 1207, 515, 658, 727, 1141, 892, 832, 1085, 1212, 849, 188, 256, 82, 51, 307, 604, 484, 664, 511, 853, 66, 54, 1147, 397, 1002, 116, 249, 457, 539, 88, 1178, 704, 735, 1092, 1030, 1200, 259, 943, 443, 1077, 1156, 394, 763, 1196, 245, 955, 807, 1183, 164, 752, 1023, 226, 171, 687, 1179, 588, 255, 1204, 610, 365, 45, 1073, 1100, 446, 31, 1069, 1060, 621, 100, 273, 676, 55, 1003, 87, 499, 9, 434, 1032, 612, 928, 984, 353, 550, 1018, 1149, 1151, 1189, 675, 625, 4, 1036, 106, 379, 859, 372, 417, 678, 441, 985, 351, 380, 769, 593, 59, 570, 218, 999, 968, 527, 8, 1033, 1134, 661, 930, 711, 1025, 871, 283, 891, 597, 1106, 153, 1063, 846, 665, 683, 855, 862, 76, 614, 967, 483, 602, 874, 305, 275, 1013, 1155, 983, 25, 234, 645, 1095, 568, 341, 211, 532, 622, 538, 1160, 911, 133, 173, 444, 1091, 231, 300, 1119, 782, 339, 721, 556, 251, 1176, 959, 15, 505, 990, 823, 890, 161, 940, 1167, 435, 1132, 492, 68, 781, 189, 1090, 392, 89, 222, 41, 803, 655, 1099, 258, 914, 635, 35, 795, 1055, 482, 464, 169, 640, 765, 529, 589, 1209, 120, 119, 988, 594, 611, 590, 11, 1062, 135, 689, 628, 1105, 12, 280, 466, 582, 706, 839, 261, 979, 196, 1125, 1136, 343, 1133, 57, 70, 857, 908, 325, 777, 6, 109, 289, 1126, 395, 410, 284, 867, 239, 794, 166, 381, 62, 290, 79, 244, 761, 294, 666, 833, 1127, 790, 858, 808, 436, 847, 861, 242, 1180, 755, 978, 824, 36, 508, 671, 1137, 991, 609, 364, 253, 1166, 448, 1016, 274, 367, 692, 792, 93, 191, 902, 783, 838, 1201, 667, 2, 617, 467, 34, 941, 981, 580, 939, 266, 901, 603, 212, 1040, 535, 451, 271, 454, 1035, 155, 97, 969, 370, 174, 52, 313, 297, 1029, 14, 722, 926, 401, 631, 81, 786, 633, 1001, 71, 860, 875, 316, 112, 486, 1019, 1093, 1, 921, 799, 64, 806, 317, 1104, 490, 104, 1086, 708, 982, 1124, 1031, 418, 828, 243, 145, 716, 419, 298, 909, 919, 1185, 738, 74, 744, 157, 575, 94, 797, 626, 552, 669, 845, 989, 1170, 1057, 219, 831, 337, 878, 569, 427, 1128, 345, 268, 733, 415, 798, 140, 46, 193, 65, 649, 842, 184, 1184, 115, 199, 296, 373, 900, 227, 1103, 260, 1182, 167, 1017, 1022, 44, 440, 1112, 1021, 493, 407, 599, 739, 521, 543, 1122, 125, 121, 1213, 719, 619, 513, 936, 836, 1202, 705, 1102, 92, 470, 447, 60, 442, 595, 363, 356, 416, 584, 474, 567, 147, 796, 1058, 19, 572, 90, 815, 488, 627, 127, 624, 210, 306, 746, 98, 128, 195, 995, 303, 821, 423]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8921355145104759
the save name prefix for this run is:  chkpt-ID_8921355145104759_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 467
rank avg (pred): 0.527 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001169806

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1011
rank avg (pred): 0.348 +- 0.261
mrr vals (pred, true): 0.211, 0.187
batch losses (mrrl, rdl): 0.0, 0.0005964384

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.058 +- 0.050
mrr vals (pred, true): 0.331, 0.156
batch losses (mrrl, rdl): 0.0, 5.94576e-05

Epoch over!
epoch time: 13.565

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 321
rank avg (pred): 0.087 +- 0.086
mrr vals (pred, true): 0.352, 0.110
batch losses (mrrl, rdl): 0.0, 7.52176e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.246 +- 0.219
mrr vals (pred, true): 0.331, 0.082
batch losses (mrrl, rdl): 0.0, 5.97832e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 74
rank avg (pred): 0.090 +- 0.084
mrr vals (pred, true): 0.382, 0.193
batch losses (mrrl, rdl): 0.0, 2.91973e-05

Epoch over!
epoch time: 13.687

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 42
rank avg (pred): 0.116 +- 0.106
mrr vals (pred, true): 0.364, 0.069
batch losses (mrrl, rdl): 0.0, 3.52167e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 700
rank avg (pred): 0.337 +- 0.304
mrr vals (pred, true): 0.280, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004351902

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.102 +- 0.103
mrr vals (pred, true): 0.380, 0.156
batch losses (mrrl, rdl): 0.0, 3.10647e-05

Epoch over!
epoch time: 12.469

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1190
rank avg (pred): 0.327 +- 0.309
mrr vals (pred, true): 0.321, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002584959

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 22
rank avg (pred): 0.075 +- 0.077
mrr vals (pred, true): 0.389, 0.127
batch losses (mrrl, rdl): 0.0, 2.49878e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 691
rank avg (pred): 0.346 +- 0.307
mrr vals (pred, true): 0.275, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002778627

Epoch over!
epoch time: 11.891

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 600
rank avg (pred): 0.352 +- 0.309
mrr vals (pred, true): 0.261, 0.005
batch losses (mrrl, rdl): 0.0, 5.16661e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.335 +- 0.317
mrr vals (pred, true): 0.296, 0.192
batch losses (mrrl, rdl): 0.0, 0.0007122721

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 619
rank avg (pred): 0.341 +- 0.308
mrr vals (pred, true): 0.312, 0.011
batch losses (mrrl, rdl): 0.0, 3.14263e-05

Epoch over!
epoch time: 11.798

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1038
rank avg (pred): 0.284 +- 0.293
mrr vals (pred, true): 0.364, 0.000
batch losses (mrrl, rdl): 0.984839499, 0.0006852245

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 699
rank avg (pred): 0.552 +- 0.180
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002045424, 0.0001515913

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 865
rank avg (pred): 0.447 +- 0.242
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0014813379, 1.86831e-05

Epoch over!
epoch time: 12.063

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 578
rank avg (pred): 0.544 +- 0.174
mrr vals (pred, true): 0.051, 0.037
batch losses (mrrl, rdl): 1.80529e-05, 0.0013633396

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 900
rank avg (pred): 0.082 +- 0.057
mrr vals (pred, true): 0.124, 0.107
batch losses (mrrl, rdl): 0.0029662764, 8.93799e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 649
rank avg (pred): 0.495 +- 0.175
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 3.1103e-06, 7.2174e-05

Epoch over!
epoch time: 12.366

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.503 +- 0.168
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.000342759, 5.92781e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 68
rank avg (pred): 0.260 +- 0.173
mrr vals (pred, true): 0.083, 0.101
batch losses (mrrl, rdl): 0.0030936359, 0.0007498811

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 212
rank avg (pred): 0.428 +- 0.202
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0006702458, 5.36305e-05

Epoch over!
epoch time: 12.056

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 881
rank avg (pred): 0.255 +- 0.169
mrr vals (pred, true): 0.092, 0.001
batch losses (mrrl, rdl): 0.0179246161, 0.0007098072

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.427 +- 0.191
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.82618e-05, 0.0001073842

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.258 +- 0.172
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0198268145, 0.0011674479

Epoch over!
epoch time: 13.025

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 402
rank avg (pred): 0.397 +- 0.206
mrr vals (pred, true): 0.060, 0.054
batch losses (mrrl, rdl): 0.0010100645, 0.0007985716

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.353 +- 0.235
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0225601532, 0.0003970176

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 206
rank avg (pred): 0.409 +- 0.163
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001915878, 0.0002640017

Epoch over!
epoch time: 12.275

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.290 +- 0.192
mrr vals (pred, true): 0.094, 0.191
batch losses (mrrl, rdl): 0.094143942, 0.0004239308

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 576
rank avg (pred): 0.429 +- 0.130
mrr vals (pred, true): 0.045, 0.003
batch losses (mrrl, rdl): 0.0002607898, 0.0003286022

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 163
rank avg (pred): 0.364 +- 0.165
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0006345224, 0.0003075404

Epoch over!
epoch time: 13.544

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 705
rank avg (pred): 0.402 +- 0.132
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 0.0001145116, 0.00022637

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.395 +- 0.131
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003454508, 0.0001627964

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 704
rank avg (pred): 0.390 +- 0.122
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002966876, 0.0002657299

Epoch over!
epoch time: 12.192

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1102
rank avg (pred): 0.176 +- 0.123
mrr vals (pred, true): 0.116, 0.170
batch losses (mrrl, rdl): 0.0300514605, 6.31926e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 347
rank avg (pred): 0.426 +- 0.189
mrr vals (pred, true): 0.057, 0.121
batch losses (mrrl, rdl): 0.040946871, 0.0010619452

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 240
rank avg (pred): 0.367 +- 0.123
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001522237, 0.0003593475

Epoch over!
epoch time: 12.005

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 14
rank avg (pred): 0.211 +- 0.140
mrr vals (pred, true): 0.105, 0.178
batch losses (mrrl, rdl): 0.0539310984, 0.000437302

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.342 +- 0.233
mrr vals (pred, true): 0.106, 0.207
batch losses (mrrl, rdl): 0.1018188968, 0.0006397325

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1092
rank avg (pred): 0.100 +- 0.071
mrr vals (pred, true): 0.158, 0.172
batch losses (mrrl, rdl): 0.0019737002, 0.0002101469

Epoch over!
epoch time: 13.503

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 376
rank avg (pred): 0.359 +- 0.118
mrr vals (pred, true): 0.055, 0.087
batch losses (mrrl, rdl): 0.0002978311, 0.0005831055

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 548
rank avg (pred): 0.353 +- 0.115
mrr vals (pred, true): 0.056, 0.080
batch losses (mrrl, rdl): 0.0003755702, 0.000397982

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 6
rank avg (pred): 0.264 +- 0.162
mrr vals (pred, true): 0.088, 0.092
batch losses (mrrl, rdl): 0.0142163616, 0.000611801

Epoch over!
epoch time: 13.9

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.311 +- 0.145
mrr vals (pred, true): 0.067, 0.101

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   81 	     0 	 0.08506 	 0.00018 	 m..s
   30 	     1 	 0.05492 	 0.00019 	 m..s
   14 	     2 	 0.05251 	 0.00019 	 m..s
   16 	     3 	 0.05269 	 0.00020 	 m..s
    3 	     4 	 0.04374 	 0.00021 	 m..s
   12 	     5 	 0.05230 	 0.00021 	 m..s
   55 	     6 	 0.05883 	 0.00021 	 m..s
   33 	     7 	 0.05523 	 0.00021 	 m..s
   28 	     8 	 0.05454 	 0.00021 	 m..s
   65 	     9 	 0.05907 	 0.00022 	 m..s
   51 	    10 	 0.05778 	 0.00022 	 m..s
   11 	    11 	 0.05176 	 0.00022 	 m..s
   37 	    12 	 0.05628 	 0.00024 	 m..s
   91 	    13 	 0.09609 	 0.00024 	 m..s
   34 	    14 	 0.05598 	 0.00024 	 m..s
  108 	    15 	 0.10421 	 0.00025 	 MISS
   79 	    16 	 0.08466 	 0.00025 	 m..s
   41 	    17 	 0.05685 	 0.00025 	 m..s
  102 	    18 	 0.10013 	 0.00026 	 m..s
   40 	    19 	 0.05653 	 0.00026 	 m..s
   31 	    20 	 0.05504 	 0.00026 	 m..s
    7 	    21 	 0.05003 	 0.00027 	 m..s
   14 	    22 	 0.05251 	 0.00027 	 m..s
  107 	    23 	 0.10275 	 0.00028 	 MISS
   66 	    24 	 0.05914 	 0.00029 	 m..s
    0 	    25 	 0.04193 	 0.00031 	 m..s
   43 	    26 	 0.05720 	 0.00031 	 m..s
   32 	    27 	 0.05520 	 0.00031 	 m..s
   43 	    28 	 0.05720 	 0.00035 	 m..s
  111 	    29 	 0.10629 	 0.00035 	 MISS
    4 	    30 	 0.04382 	 0.00036 	 m..s
    1 	    31 	 0.04350 	 0.00038 	 m..s
   23 	    32 	 0.05400 	 0.00040 	 m..s
   90 	    33 	 0.09607 	 0.00041 	 m..s
   19 	    34 	 0.05375 	 0.00044 	 m..s
    8 	    35 	 0.05006 	 0.00046 	 m..s
   22 	    36 	 0.05394 	 0.00046 	 m..s
   36 	    37 	 0.05623 	 0.00047 	 m..s
   83 	    38 	 0.08790 	 0.00054 	 m..s
   98 	    39 	 0.09887 	 0.00058 	 m..s
   52 	    40 	 0.05780 	 0.00062 	 m..s
   27 	    41 	 0.05451 	 0.00062 	 m..s
   12 	    42 	 0.05230 	 0.00072 	 m..s
    9 	    43 	 0.05017 	 0.00108 	 m..s
    2 	    44 	 0.04366 	 0.00395 	 m..s
   18 	    45 	 0.05304 	 0.00512 	 m..s
    5 	    46 	 0.04391 	 0.00908 	 m..s
   54 	    47 	 0.05851 	 0.01178 	 m..s
   25 	    48 	 0.05410 	 0.01251 	 m..s
   53 	    49 	 0.05788 	 0.01467 	 m..s
   21 	    50 	 0.05389 	 0.01739 	 m..s
   26 	    51 	 0.05411 	 0.01915 	 m..s
   45 	    52 	 0.05727 	 0.01959 	 m..s
   20 	    53 	 0.05388 	 0.02299 	 m..s
   24 	    54 	 0.05404 	 0.02495 	 ~...
   61 	    55 	 0.05894 	 0.02710 	 m..s
   46 	    56 	 0.05732 	 0.02723 	 m..s
   29 	    57 	 0.05475 	 0.03613 	 ~...
   63 	    58 	 0.05897 	 0.04070 	 ~...
    5 	    59 	 0.04391 	 0.04198 	 ~...
   56 	    60 	 0.05890 	 0.04305 	 ~...
   38 	    61 	 0.05634 	 0.04325 	 ~...
   48 	    62 	 0.05743 	 0.04427 	 ~...
   49 	    63 	 0.05745 	 0.04668 	 ~...
   68 	    64 	 0.06045 	 0.05334 	 ~...
   58 	    65 	 0.05890 	 0.05599 	 ~...
   38 	    66 	 0.05634 	 0.05687 	 ~...
   35 	    67 	 0.05616 	 0.05808 	 ~...
   10 	    68 	 0.05044 	 0.06098 	 ~...
   47 	    69 	 0.05742 	 0.06984 	 ~...
   74 	    70 	 0.06598 	 0.07284 	 ~...
   72 	    71 	 0.06539 	 0.08037 	 ~...
   42 	    72 	 0.05706 	 0.08064 	 ~...
   76 	    73 	 0.07086 	 0.08147 	 ~...
   17 	    74 	 0.05302 	 0.08185 	 ~...
   88 	    75 	 0.09353 	 0.08669 	 ~...
   62 	    76 	 0.05894 	 0.08693 	 ~...
   73 	    77 	 0.06568 	 0.08777 	 ~...
   97 	    78 	 0.09874 	 0.09149 	 ~...
   69 	    79 	 0.06093 	 0.09745 	 m..s
   60 	    80 	 0.05891 	 0.09750 	 m..s
   77 	    81 	 0.07287 	 0.10028 	 ~...
   75 	    82 	 0.06693 	 0.10101 	 m..s
   70 	    83 	 0.06332 	 0.10160 	 m..s
   64 	    84 	 0.05900 	 0.10328 	 m..s
  106 	    85 	 0.10250 	 0.10485 	 ~...
   71 	    86 	 0.06395 	 0.10511 	 m..s
  104 	    87 	 0.10154 	 0.11003 	 ~...
   58 	    88 	 0.05890 	 0.11124 	 m..s
  112 	    89 	 0.13008 	 0.11143 	 ~...
   78 	    90 	 0.07871 	 0.11949 	 m..s
   50 	    91 	 0.05746 	 0.12511 	 m..s
   56 	    92 	 0.05890 	 0.12731 	 m..s
   67 	    93 	 0.05941 	 0.13182 	 m..s
   89 	    94 	 0.09390 	 0.13795 	 m..s
   85 	    95 	 0.09040 	 0.13907 	 m..s
   99 	    96 	 0.09912 	 0.14086 	 m..s
  114 	    97 	 0.13424 	 0.14115 	 ~...
  101 	    98 	 0.09998 	 0.14586 	 m..s
   84 	    99 	 0.08799 	 0.14672 	 m..s
   82 	   100 	 0.08636 	 0.15655 	 m..s
  110 	   101 	 0.10504 	 0.15681 	 m..s
  103 	   102 	 0.10054 	 0.15698 	 m..s
   80 	   103 	 0.08477 	 0.15812 	 m..s
   94 	   104 	 0.09710 	 0.15868 	 m..s
   86 	   105 	 0.09126 	 0.15872 	 m..s
  105 	   106 	 0.10246 	 0.15902 	 m..s
  113 	   107 	 0.13325 	 0.15925 	 ~...
   93 	   108 	 0.09683 	 0.16243 	 m..s
   92 	   109 	 0.09636 	 0.16589 	 m..s
   96 	   110 	 0.09859 	 0.16715 	 m..s
   87 	   111 	 0.09288 	 0.17400 	 m..s
  100 	   112 	 0.09992 	 0.19866 	 m..s
   95 	   113 	 0.09792 	 0.20499 	 MISS
  116 	   114 	 0.20136 	 0.21929 	 ~...
  109 	   115 	 0.10443 	 0.22092 	 MISS
  115 	   116 	 0.19199 	 0.22192 	 ~...
  117 	   117 	 0.24278 	 0.22501 	 ~...
  118 	   118 	 0.25342 	 0.27049 	 ~...
  120 	   119 	 0.25773 	 0.27160 	 ~...
  119 	   120 	 0.25765 	 0.31001 	 m..s
==========================================
r_mrr = 0.7413738965988159
r2_mrr = 0.49826472997665405
spearmanr_mrr@5 = 0.7432277202606201
spearmanr_mrr@10 = 0.8599318265914917
spearmanr_mrr@50 = 0.9119065403938293
spearmanr_mrr@100 = 0.8811778426170349
spearmanr_mrr@All = 0.8908902406692505
==========================================
test time: 0.502
Done Testing dataset DBpedia50
total time taken: 196.4026026725769
training time taken: 190.93037390708923
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7414)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4983)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.7432)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8599)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9119)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.8812)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.8909)}}, 'test_loss': {'TransE': {'DBpedia50': 1.6129618419872713}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 3656779935486475
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [710, 466, 94, 638, 630, 732, 1135, 12, 485, 917, 27, 567, 415, 798, 830, 430, 958, 805, 660, 3, 521, 883, 327, 1130, 160, 729, 375, 1039, 557, 525, 494, 1067, 149, 661, 127, 357, 67, 680, 559, 666, 909, 442, 98, 1208, 433, 482, 65, 329, 460, 389, 89, 1085, 891, 642, 37, 1156, 617, 134, 48, 1174, 1004, 514, 93, 236, 1012, 137, 202, 694, 565, 508, 1209, 586, 759, 571, 69, 1066, 368, 1010, 331, 185, 701, 427, 859, 676, 394, 1098, 1026, 214, 738, 980, 689, 218, 831, 74, 1038, 566, 1069, 481, 601, 937, 151, 1163, 843, 1139, 355, 795, 869, 714, 32, 991, 363, 188, 632, 440, 416, 510, 952, 600, 498, 285, 83]
valid_ids (0): []
train_ids (1094): [1161, 682, 432, 106, 1164, 652, 441, 1013, 1118, 204, 112, 421, 1158, 988, 615, 1045, 283, 408, 211, 298, 978, 428, 92, 708, 944, 20, 742, 890, 913, 237, 1192, 324, 999, 108, 1047, 84, 1078, 1180, 463, 696, 450, 813, 266, 1068, 634, 70, 195, 295, 502, 374, 380, 531, 376, 971, 217, 471, 6, 29, 72, 781, 1083, 183, 1091, 912, 116, 81, 523, 1058, 301, 1185, 91, 670, 597, 168, 45, 964, 386, 848, 246, 648, 306, 656, 575, 817, 960, 1189, 393, 297, 1015, 947, 626, 14, 262, 101, 359, 837, 613, 325, 612, 923, 123, 943, 1082, 270, 103, 28, 1030, 500, 51, 370, 852, 1213, 871, 728, 649, 849, 725, 529, 975, 570, 910, 286, 665, 470, 903, 130, 469, 900, 867, 250, 113, 125, 782, 518, 820, 167, 233, 931, 1061, 724, 1207, 562, 1119, 181, 981, 955, 114, 86, 175, 267, 1194, 274, 886, 780, 135, 68, 1152, 300, 1172, 802, 213, 414, 207, 377, 655, 110, 778, 599, 1166, 367, 1142, 396, 606, 1199, 221, 703, 1170, 815, 954, 10, 446, 503, 877, 438, 1132, 337, 951, 646, 1196, 1147, 695, 138, 902, 105, 866, 834, 678, 497, 1086, 429, 119, 49, 953, 273, 618, 147, 206, 8, 1020, 1198, 1022, 894, 171, 602, 1044, 82, 225, 657, 673, 1017, 773, 833, 255, 174, 946, 145, 771, 814, 131, 46, 540, 179, 459, 102, 115, 53, 1033, 777, 737, 1094, 397, 1205, 1137, 687, 893, 936, 664, 604, 263, 1141, 581, 691, 487, 475, 1001, 406, 969, 751, 766, 230, 328, 1165, 749, 1120, 767, 579, 854, 1202, 240, 228, 349, 58, 924, 361, 139, 700, 530, 304, 1167, 38, 161, 2, 884, 824, 1053, 208, 986, 685, 410, 919, 22, 823, 532, 323, 401, 1123, 451, 847, 260, 807, 774, 492, 1128, 249, 922, 876, 744, 80, 305, 758, 23, 461, 107, 133, 775, 1150, 472, 353, 1063, 940, 42, 752, 1071, 637, 519, 235, 855, 1028, 520, 914, 573, 743, 57, 607, 403, 874, 704, 1102, 157, 1089, 1134, 1140, 662, 76, 166, 572, 790, 13, 395, 611, 568, 111, 636, 173, 1182, 289, 1151, 897, 987, 265, 199, 243, 278, 330, 495, 30, 287, 765, 473, 968, 339, 1073, 276, 580, 832, 784, 535, 934, 484, 281, 735, 383, 1116, 1023, 945, 681, 534, 811, 995, 268, 1007, 818, 574, 1181, 994, 851, 242, 616, 949, 513, 1168, 335, 210, 1065, 610, 245, 50, 804, 512, 722, 1059, 939, 545, 162, 789, 479, 1169, 41, 576, 1201, 1077, 344, 658, 1084, 1060, 384, 85, 1197, 311, 966, 378, 194, 750, 192, 1186, 358, 881, 672, 24, 476, 1131, 1173, 762, 862, 159, 983, 925, 589, 164, 794, 850, 785, 120, 290, 153, 1040, 15, 957, 1112, 241, 1021, 400, 1187, 1179, 1111, 1100, 1117, 382, 619, 60, 596, 1121, 223, 148, 1088, 836, 1153, 587, 1109, 1008, 78, 727, 561, 187, 603, 44, 803, 317, 522, 464, 756, 62, 822, 956, 845, 180, 716, 352, 643, 748, 984, 1171, 746, 1200, 783, 515, 592, 1075, 1003, 719, 1043, 840, 552, 184, 439, 1029, 40, 763, 392, 918, 261, 1037, 190, 163, 1146, 216, 1096, 404, 941, 282, 764, 313, 1154, 950, 52, 549, 399, 583, 713, 810, 829, 588, 935, 741, 129, 537, 55, 435, 1035, 828, 827, 799, 277, 996, 788, 684, 77, 248, 809, 244, 547, 1203, 598, 419, 585, 1122, 875, 1024, 226, 591, 257, 819, 594, 1042, 1124, 1108, 930, 455, 373, 64, 25, 620, 683, 1177, 1025, 916, 388, 315, 1101, 516, 635, 538, 808, 1188, 99, 787, 739, 895, 1211, 1110, 972, 674, 911, 640, 232, 797, 870, 677, 962, 431, 791, 1148, 842, 318, 959, 271, 170, 1133, 772, 224, 563, 34, 178, 320, 141, 452, 1081, 543, 668, 75, 182, 730, 448, 348, 4, 292, 1032, 1155, 443, 1127, 307, 189, 745, 1178, 191, 288, 905, 556, 321, 627, 709, 1079, 360, 66, 21, 504, 1074, 711, 1175, 857, 546, 569, 761, 932, 928, 1070, 254, 5, 688, 888, 693, 346, 861, 736, 126, 39, 1019, 593, 18, 553, 345, 391, 542, 731, 172, 467, 885, 1097, 524, 528, 308, 926, 155, 539, 247, 229, 825, 338, 302, 663, 31, 1064, 907, 47, 990, 631, 507, 898, 222, 333, 754, 623, 1195, 251, 614, 326, 203, 734, 465, 1095, 284, 63, 872, 793, 985, 712, 417, 906, 558, 239, 921, 517, 118, 371, 993, 365, 56, 1105, 733, 920, 154, 1184, 456, 444, 858, 555, 1190, 1036, 16, 671, 422, 364, 1176, 136, 122, 35, 454, 316, 697, 977, 908, 659, 1093, 294, 839, 757, 800, 54, 720, 96, 1062, 550, 901, 806, 641, 197, 915, 132, 760, 369, 879, 590, 629, 965, 1055, 645, 90, 156, 755, 970, 457, 622, 87, 434, 835, 411, 212, 379, 1113, 165, 420, 948, 124, 489, 726, 499, 1049, 989, 342, 973, 979, 385, 1092, 1183, 405, 816, 334, 942, 253, 200, 398, 1072, 275, 1002, 490, 846, 486, 477, 356, 1099, 1005, 1051, 279, 699, 1149, 625, 1126, 340, 209, 312, 605, 462, 1076, 653, 1048, 483, 844, 1046, 740, 792, 582, 838, 826, 201, 496, 1212, 480, 1103, 595, 474, 647, 882, 1125, 453, 309, 97, 1057, 1162, 1204, 564, 33, 584, 1, 707, 747, 9, 150, 272, 675, 1157, 215, 753, 887, 578, 1018, 1006, 445, 227, 544, 468, 1052, 413, 390, 1210, 1136, 896, 639, 258, 491, 144, 967, 280, 177, 776, 424, 865, 982, 880, 821, 624, 169, 650, 501, 142, 853, 95, 702, 205, 193, 362, 1000, 577, 1114, 1016, 1206, 259, 158, 873, 61, 963, 0, 878, 718, 447, 1193, 768, 1145, 992, 293, 933, 196, 1009, 509, 402, 997, 899, 79, 644, 234, 140, 73, 238, 252, 88, 1214, 43, 651, 864, 366, 146, 1143, 36, 104, 109, 1191, 1027, 927, 426, 381, 17, 770, 291, 1087, 220, 667, 387, 351, 723, 998, 621, 449, 863, 929, 121, 974, 299, 341, 198, 548, 11, 319, 526, 264, 1144, 536, 860, 554, 633, 628, 1138, 889, 1014, 527, 303, 609, 705, 493, 186, 1011, 7, 1104, 1080, 1159, 436, 706, 219, 343, 541, 892, 269, 347, 698, 296, 100, 856, 1107, 769, 1106, 1031, 721, 176, 412, 314, 407, 715, 505, 812, 1054, 152, 1115, 425, 117, 1050, 322, 654, 551, 26, 801, 437, 717, 256, 786, 59, 332, 1034, 533, 868, 904, 143, 669, 1090, 478, 372, 679, 418, 690, 511, 1056, 841, 938, 336, 458, 19, 350, 796, 71, 560, 310, 354, 961, 779, 231, 1160, 423, 409, 128, 1129, 608, 488, 506, 692, 1041, 686, 976]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3262413142098884
the save name prefix for this run is:  chkpt-ID_3262413142098884_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 976
rank avg (pred): 0.535 +- 0.006
mrr vals (pred, true): 0.000, 0.308
batch losses (mrrl, rdl): 0.0, 0.0044972799

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1195
rank avg (pred): 0.371 +- 0.074
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004634013

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.369 +- 0.239
mrr vals (pred, true): 0.103, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001115543

Epoch over!
epoch time: 12.609

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.351 +- 0.240
mrr vals (pred, true): 0.130, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001777875

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.291 +- 0.265
mrr vals (pred, true): 0.234, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003727151

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.104 +- 0.097
mrr vals (pred, true): 0.268, 0.106
batch losses (mrrl, rdl): 0.0, 2.85293e-05

Epoch over!
epoch time: 12.734

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 677
rank avg (pred): 0.380 +- 0.342
mrr vals (pred, true): 0.222, 0.000
batch losses (mrrl, rdl): 0.0, 6.79709e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 232
rank avg (pred): 0.323 +- 0.299
mrr vals (pred, true): 0.245, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002046682

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 750
rank avg (pred): 0.057 +- 0.054
mrr vals (pred, true): 0.309, 0.212
batch losses (mrrl, rdl): 0.0, 1.93045e-05

Epoch over!
epoch time: 11.999

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.409 +- 0.344
mrr vals (pred, true): 0.210, 0.000
batch losses (mrrl, rdl): 0.0, 8.65452e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.322 +- 0.294
mrr vals (pred, true): 0.231, 0.081
batch losses (mrrl, rdl): 0.0, 0.0005249878

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1122
rank avg (pred): 0.305 +- 0.284
mrr vals (pred, true): 0.267, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005029349

Epoch over!
epoch time: 11.952

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1162
rank avg (pred): 0.342 +- 0.313
mrr vals (pred, true): 0.256, 0.044
batch losses (mrrl, rdl): 0.0, 0.0001533551

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 840
rank avg (pred): 0.311 +- 0.292
mrr vals (pred, true): 0.281, 0.151
batch losses (mrrl, rdl): 0.0, 0.0003208412

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 109
rank avg (pred): 0.313 +- 0.293
mrr vals (pred, true): 0.282, 0.022
batch losses (mrrl, rdl): 0.0, 0.0001848837

Epoch over!
epoch time: 12.167

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 605
rank avg (pred): 0.360 +- 0.327
mrr vals (pred, true): 0.268, 0.032
batch losses (mrrl, rdl): 0.4732747078, 0.0001168028

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 352
rank avg (pred): 0.413 +- 0.235
mrr vals (pred, true): 0.077, 0.066
batch losses (mrrl, rdl): 0.0075313561, 0.0007295649

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1165
rank avg (pred): 0.370 +- 0.179
mrr vals (pred, true): 0.065, 0.060
batch losses (mrrl, rdl): 0.0023833397, 0.0001720893

Epoch over!
epoch time: 12.163

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 462
rank avg (pred): 0.430 +- 0.224
mrr vals (pred, true): 0.080, 0.001
batch losses (mrrl, rdl): 0.0091212913, 3.66382e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 705
rank avg (pred): 0.400 +- 0.155
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 4.9194e-06, 0.0002065415

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 923
rank avg (pred): 0.389 +- 0.231
mrr vals (pred, true): 0.081, 0.151
batch losses (mrrl, rdl): 0.0492443666, 0.0007747887

Epoch over!
epoch time: 12.026

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 766
rank avg (pred): 0.309 +- 0.194
mrr vals (pred, true): 0.083, 0.174
batch losses (mrrl, rdl): 0.0839183778, 0.0004532556

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 16
rank avg (pred): 0.228 +- 0.143
mrr vals (pred, true): 0.100, 0.142
batch losses (mrrl, rdl): 0.0173212215, 0.0004060885

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.372 +- 0.215
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0146383122, 0.0003255586

Epoch over!
epoch time: 12.004

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1055
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.299, 0.257
batch losses (mrrl, rdl): 0.0175487418, 0.0001273487

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1096
rank avg (pred): 0.038 +- 0.026
mrr vals (pred, true): 0.160, 0.218
batch losses (mrrl, rdl): 0.0340552703, 0.000401899

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.335 +- 0.104
mrr vals (pred, true): 0.040, 0.002
batch losses (mrrl, rdl): 0.0010129028, 8.64245e-05

Epoch over!
epoch time: 12.019

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 233
rank avg (pred): 0.348 +- 0.157
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0015672289, 0.0004051618

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 789
rank avg (pred): 0.383 +- 0.223
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0147392936, 7.66993e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 719
rank avg (pred): 0.355 +- 0.128
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 5.4298e-06, 0.0004477608

Epoch over!
epoch time: 12.342

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1055
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.229, 0.257
batch losses (mrrl, rdl): 0.0075990986, 0.0001124202

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 248
rank avg (pred): 0.227 +- 0.150
mrr vals (pred, true): 0.114, 0.109
batch losses (mrrl, rdl): 0.0003136924, 0.0003573316

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.389 +- 0.161
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005314332, 0.0003902398

Epoch over!
epoch time: 12.638

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 730
rank avg (pred): 0.277 +- 0.186
mrr vals (pred, true): 0.128, 0.080
batch losses (mrrl, rdl): 0.0608213954, 0.0005786157

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.283 +- 0.192
mrr vals (pred, true): 0.128, 0.128
batch losses (mrrl, rdl): 2.0372e-06, 0.0008033697

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.357 +- 0.201
mrr vals (pred, true): 0.092, 0.001
batch losses (mrrl, rdl): 0.017413877, 0.0001706556

Epoch over!
epoch time: 12.516

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 153
rank avg (pred): 0.388 +- 0.139
mrr vals (pred, true): 0.048, 0.013
batch losses (mrrl, rdl): 4.40862e-05, 0.0005925199

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 123
rank avg (pred): 0.352 +- 0.121
mrr vals (pred, true): 0.048, 0.007
batch losses (mrrl, rdl): 5.1389e-05, 0.0003673191

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 392
rank avg (pred): 0.309 +- 0.133
mrr vals (pred, true): 0.056, 0.129
batch losses (mrrl, rdl): 0.0527615771, 0.0003703644

Epoch over!
epoch time: 12.37

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 335
rank avg (pred): 0.355 +- 0.185
mrr vals (pred, true): 0.076, 0.102
batch losses (mrrl, rdl): 0.0066992724, 0.0006374571

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.322 +- 0.202
mrr vals (pred, true): 0.123, 0.000
batch losses (mrrl, rdl): 0.053778287, 0.0003623982

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.380 +- 0.119
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.000498682, 0.0003043818

Epoch over!
epoch time: 13.014

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1146
rank avg (pred): 0.241 +- 0.116
mrr vals (pred, true): 0.065, 0.105
batch losses (mrrl, rdl): 0.0159786325, 0.0001309767

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1202
rank avg (pred): 0.343 +- 0.128
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001687677, 0.0003584849

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 125
rank avg (pred): 0.327 +- 0.129
mrr vals (pred, true): 0.057, 0.073
batch losses (mrrl, rdl): 0.0005577314, 0.000328885

Epoch over!
epoch time: 12.178

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.362 +- 0.124
mrr vals (pred, true): 0.045, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   92 	     0 	 0.10582 	 0.00016 	 MISS
   36 	     1 	 0.05220 	 0.00018 	 m..s
    3 	     2 	 0.04180 	 0.00019 	 m..s
   40 	     3 	 0.05352 	 0.00020 	 m..s
   85 	     4 	 0.09773 	 0.00020 	 m..s
   95 	     5 	 0.10760 	 0.00020 	 MISS
   20 	     6 	 0.04846 	 0.00021 	 m..s
   79 	     7 	 0.09096 	 0.00022 	 m..s
   81 	     8 	 0.09519 	 0.00022 	 m..s
   64 	     9 	 0.07390 	 0.00022 	 m..s
  109 	    10 	 0.11895 	 0.00023 	 MISS
   11 	    11 	 0.04553 	 0.00023 	 m..s
   92 	    12 	 0.10582 	 0.00024 	 MISS
   81 	    13 	 0.09519 	 0.00025 	 m..s
   33 	    14 	 0.05194 	 0.00025 	 m..s
    1 	    15 	 0.04168 	 0.00026 	 m..s
    8 	    16 	 0.04424 	 0.00028 	 m..s
  100 	    17 	 0.10952 	 0.00028 	 MISS
   62 	    18 	 0.06793 	 0.00029 	 m..s
  111 	    19 	 0.12191 	 0.00029 	 MISS
   10 	    20 	 0.04546 	 0.00029 	 m..s
   31 	    21 	 0.05059 	 0.00030 	 m..s
   14 	    22 	 0.04666 	 0.00035 	 m..s
   88 	    23 	 0.10326 	 0.00036 	 MISS
   47 	    24 	 0.05728 	 0.00037 	 m..s
   64 	    25 	 0.07390 	 0.00038 	 m..s
   95 	    26 	 0.10760 	 0.00039 	 MISS
   59 	    27 	 0.06467 	 0.00039 	 m..s
   53 	    28 	 0.06010 	 0.00039 	 m..s
   38 	    29 	 0.05261 	 0.00039 	 m..s
   79 	    30 	 0.09096 	 0.00041 	 m..s
   26 	    31 	 0.04902 	 0.00041 	 m..s
   24 	    32 	 0.04897 	 0.00043 	 m..s
  104 	    33 	 0.11382 	 0.00043 	 MISS
   28 	    34 	 0.04930 	 0.00059 	 m..s
   68 	    35 	 0.07629 	 0.00060 	 m..s
   17 	    36 	 0.04751 	 0.00061 	 m..s
  108 	    37 	 0.11873 	 0.00066 	 MISS
   19 	    38 	 0.04828 	 0.00066 	 m..s
   15 	    39 	 0.04701 	 0.00136 	 m..s
   24 	    40 	 0.04897 	 0.00171 	 m..s
   86 	    41 	 0.09959 	 0.00203 	 m..s
    2 	    42 	 0.04169 	 0.00237 	 m..s
    4 	    43 	 0.04180 	 0.00323 	 m..s
    7 	    44 	 0.04267 	 0.00349 	 m..s
    0 	    45 	 0.03969 	 0.00479 	 m..s
   29 	    46 	 0.04953 	 0.00756 	 m..s
   46 	    47 	 0.05645 	 0.00834 	 m..s
   32 	    48 	 0.05183 	 0.01141 	 m..s
   21 	    49 	 0.04851 	 0.02299 	 ~...
   55 	    50 	 0.06050 	 0.02512 	 m..s
   41 	    51 	 0.05366 	 0.02518 	 ~...
   48 	    52 	 0.05772 	 0.02802 	 ~...
   12 	    53 	 0.04664 	 0.02900 	 ~...
   42 	    54 	 0.05371 	 0.02935 	 ~...
   43 	    55 	 0.05522 	 0.03055 	 ~...
   12 	    56 	 0.04664 	 0.03172 	 ~...
   18 	    57 	 0.04756 	 0.03335 	 ~...
    5 	    58 	 0.04228 	 0.03357 	 ~...
    9 	    59 	 0.04513 	 0.03716 	 ~...
   22 	    60 	 0.04881 	 0.04070 	 ~...
    6 	    61 	 0.04247 	 0.04273 	 ~...
   30 	    62 	 0.05019 	 0.04427 	 ~...
   34 	    63 	 0.05203 	 0.04668 	 ~...
   22 	    64 	 0.04881 	 0.04743 	 ~...
   49 	    65 	 0.05818 	 0.05285 	 ~...
   66 	    66 	 0.07533 	 0.05469 	 ~...
   51 	    67 	 0.05971 	 0.05599 	 ~...
   58 	    68 	 0.06379 	 0.05707 	 ~...
   45 	    69 	 0.05585 	 0.05996 	 ~...
   50 	    70 	 0.05893 	 0.06345 	 ~...
   16 	    71 	 0.04707 	 0.06385 	 ~...
   66 	    72 	 0.07533 	 0.06476 	 ~...
   71 	    73 	 0.07722 	 0.06609 	 ~...
   27 	    74 	 0.04917 	 0.06654 	 ~...
   60 	    75 	 0.06538 	 0.06687 	 ~...
   56 	    76 	 0.06201 	 0.07213 	 ~...
   35 	    77 	 0.05206 	 0.07218 	 ~...
   99 	    78 	 0.10868 	 0.07322 	 m..s
   87 	    79 	 0.10318 	 0.07377 	 ~...
   74 	    80 	 0.07797 	 0.07602 	 ~...
   57 	    81 	 0.06295 	 0.07672 	 ~...
   54 	    82 	 0.06035 	 0.07741 	 ~...
   39 	    83 	 0.05275 	 0.07914 	 ~...
   37 	    84 	 0.05225 	 0.07950 	 ~...
   52 	    85 	 0.05994 	 0.08064 	 ~...
   77 	    86 	 0.08578 	 0.08101 	 ~...
   44 	    87 	 0.05541 	 0.08200 	 ~...
   61 	    88 	 0.06790 	 0.08429 	 ~...
  101 	    89 	 0.11077 	 0.08442 	 ~...
   73 	    90 	 0.07795 	 0.08777 	 ~...
  107 	    91 	 0.11714 	 0.09149 	 ~...
   89 	    92 	 0.10369 	 0.09525 	 ~...
   76 	    93 	 0.08065 	 0.09877 	 ~...
   72 	    94 	 0.07729 	 0.10101 	 ~...
   75 	    95 	 0.07883 	 0.10106 	 ~...
   78 	    96 	 0.09074 	 0.10328 	 ~...
   63 	    97 	 0.07101 	 0.11232 	 m..s
   95 	    98 	 0.10760 	 0.11391 	 ~...
   70 	    99 	 0.07674 	 0.11817 	 m..s
   83 	   100 	 0.09672 	 0.12003 	 ~...
  110 	   101 	 0.12023 	 0.12259 	 ~...
   91 	   102 	 0.10523 	 0.13466 	 ~...
  113 	   103 	 0.17040 	 0.13518 	 m..s
   69 	   104 	 0.07652 	 0.14443 	 m..s
   95 	   105 	 0.10760 	 0.14567 	 m..s
  103 	   106 	 0.11313 	 0.15681 	 m..s
  112 	   107 	 0.15762 	 0.15703 	 ~...
  105 	   108 	 0.11495 	 0.15744 	 m..s
   84 	   109 	 0.09681 	 0.16211 	 m..s
  106 	   110 	 0.11541 	 0.16282 	 m..s
   94 	   111 	 0.10650 	 0.16715 	 m..s
   90 	   112 	 0.10510 	 0.18320 	 m..s
  102 	   113 	 0.11259 	 0.18365 	 m..s
  115 	   114 	 0.19827 	 0.19344 	 ~...
  116 	   115 	 0.25184 	 0.19456 	 m..s
  114 	   116 	 0.19782 	 0.20346 	 ~...
  118 	   117 	 0.28899 	 0.27049 	 ~...
  120 	   118 	 0.31590 	 0.30536 	 ~...
  118 	   119 	 0.28899 	 0.31001 	 ~...
  117 	   120 	 0.27290 	 0.31534 	 m..s
==========================================
r_mrr = 0.766882598400116
r2_mrr = 0.4768221974372864
spearmanr_mrr@5 = 0.90092533826828
spearmanr_mrr@10 = 0.9258683323860168
spearmanr_mrr@50 = 0.9437663555145264
spearmanr_mrr@100 = 0.9519408941268921
spearmanr_mrr@All = 0.9572975635528564
==========================================
test time: 0.391
Done Testing dataset DBpedia50
total time taken: 190.21133637428284
training time taken: 185.20670294761658
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7669)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4768)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9009)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9259)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9438)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9519)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9573)}}, 'test_loss': {'TransE': {'DBpedia50': 1.2764542796358}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 7189001384334985
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [286, 499, 702, 198, 786, 861, 789, 472, 1205, 800, 442, 379, 217, 338, 311, 754, 840, 365, 844, 941, 1098, 817, 386, 17, 366, 1115, 842, 832, 1033, 279, 234, 874, 78, 791, 619, 1050, 1010, 2, 109, 190, 1153, 872, 502, 742, 596, 244, 966, 169, 278, 435, 600, 873, 245, 507, 820, 346, 35, 1077, 777, 798, 1060, 439, 945, 342, 810, 1066, 494, 511, 755, 657, 233, 188, 426, 257, 1003, 979, 246, 912, 802, 28, 371, 95, 37, 1099, 1161, 505, 156, 36, 911, 117, 637, 749, 866, 849, 239, 202, 18, 667, 421, 20, 703, 121, 385, 1145, 748, 305, 1150, 273, 538, 503, 16, 335, 498, 219, 449, 42, 83, 693, 1008, 675, 143]
valid_ids (0): []
train_ids (1094): [22, 753, 1122, 545, 106, 750, 793, 1124, 122, 528, 814, 68, 668, 450, 1016, 537, 192, 1015, 404, 915, 304, 65, 1088, 145, 1168, 124, 323, 319, 1183, 553, 1146, 515, 986, 130, 1030, 1097, 578, 235, 1005, 645, 510, 374, 76, 1013, 1159, 97, 142, 819, 669, 158, 1007, 63, 953, 367, 815, 411, 210, 249, 926, 184, 100, 1172, 473, 344, 957, 173, 1086, 361, 375, 427, 956, 665, 62, 828, 659, 529, 1190, 1114, 774, 90, 1069, 821, 1207, 620, 80, 712, 1194, 289, 110, 882, 640, 564, 913, 87, 895, 66, 1211, 53, 1052, 119, 805, 86, 64, 70, 928, 977, 635, 1213, 557, 720, 1012, 247, 783, 952, 818, 642, 707, 512, 934, 322, 277, 958, 519, 463, 662, 1078, 334, 560, 1163, 614, 162, 136, 1079, 325, 443, 103, 829, 1027, 987, 296, 393, 146, 933, 845, 524, 594, 530, 718, 376, 909, 392, 13, 1164, 1141, 1109, 458, 1018, 281, 1076, 582, 1193, 220, 369, 1151, 788, 328, 1001, 1180, 556, 853, 276, 1212, 1186, 629, 149, 682, 1158, 1203, 1189, 332, 937, 506, 569, 714, 163, 388, 67, 568, 884, 464, 608, 255, 213, 1110, 520, 1177, 617, 237, 51, 406, 923, 745, 49, 701, 743, 150, 521, 484, 756, 248, 359, 253, 650, 1031, 540, 787, 459, 431, 809, 983, 1200, 587, 420, 558, 516, 187, 1123, 713, 903, 836, 161, 867, 155, 308, 761, 216, 673, 1119, 525, 422, 985, 726, 1139, 148, 626, 847, 839, 661, 355, 887, 964, 965, 630, 348, 287, 127, 32, 317, 921, 653, 275, 779, 813, 576, 40, 1026, 271, 852, 1160, 674, 708, 272, 846, 77, 310, 280, 889, 491, 1108, 180, 988, 445, 1167, 925, 114, 166, 981, 333, 434, 606, 9, 605, 960, 522, 205, 816, 1091, 211, 590, 1191, 961, 141, 92, 11, 835, 414, 1131, 838, 888, 973, 1116, 303, 942, 906, 684, 1043, 232, 687, 46, 260, 685, 274, 1063, 5, 639, 946, 984, 209, 899, 1000, 625, 559, 757, 615, 824, 944, 1198, 480, 948, 997, 695, 706, 1017, 603, 1154, 378, 900, 175, 765, 947, 574, 288, 331, 803, 597, 467, 518, 3, 1201, 879, 896, 633, 696, 1023, 416, 740, 241, 446, 1125, 768, 638, 976, 691, 466, 717, 1083, 865, 716, 972, 770, 394, 643, 655, 711, 428, 377, 1051, 381, 52, 55, 1175, 1082, 980, 739, 157, 171, 283, 834, 226, 531, 709, 1181, 1021, 1196, 1032, 284, 330, 858, 797, 101, 671, 1127, 681, 1130, 1081, 172, 399, 949, 609, 993, 179, 82, 592, 168, 723, 932, 351, 1040, 1038, 841, 475, 56, 1187, 462, 881, 290, 562, 360, 804, 527, 830, 694, 1002, 1162, 362, 892, 808, 517, 6, 267, 251, 1140, 300, 402, 876, 737, 1155, 893, 730, 151, 352, 825, 236, 566, 8, 307, 1134, 589, 1132, 1045, 490, 26, 29, 1195, 316, 93, 391, 258, 991, 123, 862, 405, 508, 759, 664, 1152, 54, 700, 120, 387, 919, 214, 1, 61, 622, 128, 731, 570, 1144, 1173, 869, 766, 543, 898, 343, 1009, 567, 357, 951, 575, 513, 955, 349, 196, 752, 623, 794, 697, 1148, 14, 546, 10, 0, 1058, 690, 591, 409, 535, 160, 1101, 48, 710, 69, 154, 137, 1209, 699, 185, 1137, 1102, 831, 1184, 91, 12, 922, 914, 182, 767, 195, 1165, 1072, 1113, 1014, 641, 950, 719, 721, 126, 165, 390, 403, 773, 34, 1179, 850, 238, 968, 380, 539, 324, 680, 261, 1118, 479, 221, 396, 910, 610, 104, 920, 1105, 400, 468, 492, 410, 1156, 859, 894, 1182, 1202, 1073, 544, 938, 250, 792, 660, 417, 183, 301, 1090, 57, 489, 908, 313, 452, 457, 384, 222, 998, 85, 782, 975, 1065, 654, 135, 1157, 45, 321, 848, 880, 7, 1025, 1138, 612, 50, 868, 59, 153, 1047, 1074, 526, 326, 465, 666, 481, 407, 886, 573, 648, 263, 176, 456, 1129, 438, 649, 1111, 131, 837, 547, 230, 1029, 495, 1034, 644, 129, 268, 159, 616, 1096, 164, 577, 771, 1093, 107, 971, 125, 140, 561, 1126, 598, 1075, 415, 593, 363, 353, 736, 939, 1054, 1024, 509, 1062, 533, 851, 181, 73, 856, 432, 84, 223, 907, 995, 580, 297, 1143, 1094, 1192, 397, 373, 532, 1176, 105, 563, 1112, 646, 240, 801, 299, 552, 74, 38, 963, 663, 19, 652, 320, 843, 218, 347, 382, 254, 71, 1039, 550, 927, 1055, 1042, 193, 1185, 747, 1019, 1061, 208, 764, 327, 1197, 424, 108, 584, 586, 312, 500, 1067, 536, 1006, 448, 823, 744, 461, 604, 493, 732, 1174, 565, 132, 588, 1178, 627, 1199, 227, 781, 822, 293, 113, 954, 1210, 336, 1107, 1092, 785, 962, 476, 918, 585, 790, 1041, 102, 796, 722, 167, 24, 437, 370, 72, 878, 1169, 116, 81, 225, 891, 425, 430, 285, 613, 47, 1049, 996, 401, 689, 534, 454, 496, 302, 408, 776, 354, 1171, 677, 1087, 812, 1080, 294, 262, 469, 778, 583, 940, 270, 902, 1103, 1044, 890, 1120, 256, 204, 970, 144, 295, 672, 571, 741, 990, 191, 88, 870, 883, 651, 133, 1133, 345, 96, 658, 337, 885, 31, 857, 318, 692, 1206, 989, 854, 98, 1037, 860, 41, 30, 523, 618, 924, 1064, 189, 775, 398, 43, 372, 292, 487, 44, 772, 197, 994, 599, 905, 1135, 967, 266, 769, 935, 215, 1106, 39, 855, 1028, 1100, 551, 784, 746, 1004, 207, 548, 554, 75, 1057, 89, 470, 264, 341, 607, 504, 314, 383, 1170, 725, 203, 486, 799, 904, 871, 897, 212, 231, 542, 572, 943, 315, 760, 864, 94, 875, 356, 99, 581, 1022, 916, 174, 1035, 632, 177, 265, 1056, 488, 4, 833, 25, 762, 482, 440, 999, 460, 350, 978, 795, 1204, 601, 200, 595, 1149, 433, 474, 229, 602, 478, 1117, 901, 471, 1095, 982, 194, 15, 826, 549, 959, 178, 364, 624, 611, 329, 930, 1048, 698, 340, 621, 1166, 735, 206, 807, 441, 634, 224, 444, 58, 483, 170, 678, 992, 1104, 79, 429, 21, 60, 112, 33, 269, 419, 827, 728, 514, 453, 368, 1020, 738, 389, 936, 1011, 477, 1214, 27, 729, 917, 1059, 111, 541, 298, 656, 199, 497, 1208, 679, 1142, 676, 152, 242, 228, 1089, 1147, 118, 631, 1071, 734, 1136, 758, 688, 751, 715, 806, 863, 1070, 1121, 186, 138, 243, 1068, 1053, 705, 727, 412, 451, 485, 115, 724, 670, 686, 733, 501, 418, 931, 139, 252, 647, 811, 1188, 395, 201, 763, 555, 306, 447, 134, 413, 780, 358, 1036, 339, 628, 969, 1046, 1084, 436, 683, 877, 282, 291, 309, 1128, 1085, 259, 636, 974, 579, 455, 929, 23, 423, 147, 704]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6331642107274654
the save name prefix for this run is:  chkpt-ID_6331642107274654_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 647
rank avg (pred): 0.519 +- 0.002
mrr vals (pred, true): 0.000, 0.036
batch losses (mrrl, rdl): 0.0, 0.0013243691

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 940
rank avg (pred): 0.372 +- 0.233
mrr vals (pred, true): 0.016, 0.172
batch losses (mrrl, rdl): 0.0, 0.0007796181

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 627
rank avg (pred): 0.340 +- 0.273
mrr vals (pred, true): 0.240, 0.002
batch losses (mrrl, rdl): 0.0, 8.0861e-06

Epoch over!
epoch time: 12.221

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.337 +- 0.282
mrr vals (pred, true): 0.258, 0.093
batch losses (mrrl, rdl): 0.0, 0.0005570074

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1118
rank avg (pred): 0.327 +- 0.288
mrr vals (pred, true): 0.297, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001889969

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.300 +- 0.268
mrr vals (pred, true): 0.306, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004589218

Epoch over!
epoch time: 11.917

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 606
rank avg (pred): 0.359 +- 0.319
mrr vals (pred, true): 0.307, 0.002
batch losses (mrrl, rdl): 0.0, 4.46886e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.081 +- 0.076
mrr vals (pred, true): 0.352, 0.106
batch losses (mrrl, rdl): 0.0, 1.75294e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 732
rank avg (pred): 0.077 +- 0.075
mrr vals (pred, true): 0.356, 0.091
batch losses (mrrl, rdl): 0.0, 4.80266e-05

Epoch over!
epoch time: 11.935

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 845
rank avg (pred): 0.312 +- 0.301
mrr vals (pred, true): 0.342, 0.124
batch losses (mrrl, rdl): 0.0, 0.0001288034

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1069
rank avg (pred): 0.064 +- 0.064
mrr vals (pred, true): 0.375, 0.305
batch losses (mrrl, rdl): 0.0, 1.36836e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 466
rank avg (pred): 0.306 +- 0.300
mrr vals (pred, true): 0.358, 0.002
batch losses (mrrl, rdl): 0.0, 0.0002831042

Epoch over!
epoch time: 11.885

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.358 +- 0.317
mrr vals (pred, true): 0.322, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002959997

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.085 +- 0.084
mrr vals (pred, true): 0.363, 0.144
batch losses (mrrl, rdl): 0.0, 5.66763e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.355 +- 0.324
mrr vals (pred, true): 0.335, 0.011
batch losses (mrrl, rdl): 0.0, 3.20119e-05

Epoch over!
epoch time: 12.239

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1201
rank avg (pred): 0.343 +- 0.321
mrr vals (pred, true): 0.337, 0.001
batch losses (mrrl, rdl): 0.8221029639, 0.0001815863

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 148
rank avg (pred): 0.378 +- 0.216
mrr vals (pred, true): 0.067, 0.025
batch losses (mrrl, rdl): 0.0029881168, 0.0004186397

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.392 +- 0.238
mrr vals (pred, true): 0.078, 0.028
batch losses (mrrl, rdl): 0.0079487553, 0.0006528624

Epoch over!
epoch time: 11.988

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.132 +- 0.086
mrr vals (pred, true): 0.099, 0.101
batch losses (mrrl, rdl): 4.53091e-05, 7.36865e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 193
rank avg (pred): 0.400 +- 0.202
mrr vals (pred, true): 0.042, 0.000
batch losses (mrrl, rdl): 0.0006997038, 0.0002668334

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 989
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.276, 0.292
batch losses (mrrl, rdl): 0.0023591672, 5.78862e-05

Epoch over!
epoch time: 12.311

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 488
rank avg (pred): 0.390 +- 0.210
mrr vals (pred, true): 0.051, 0.094
batch losses (mrrl, rdl): 1.46132e-05, 0.0003805613

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 227
rank avg (pred): 0.394 +- 0.219
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0026283178, 0.0001037978

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 752
rank avg (pred): 0.031 +- 0.022
mrr vals (pred, true): 0.117, 0.203
batch losses (mrrl, rdl): 0.0733203143, 7.86513e-05

Epoch over!
epoch time: 12.403

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 729
rank avg (pred): 0.055 +- 0.040
mrr vals (pred, true): 0.099, 0.073
batch losses (mrrl, rdl): 0.0238532051, 0.0001634114

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 34
rank avg (pred): 0.336 +- 0.214
mrr vals (pred, true): 0.085, 0.073
batch losses (mrrl, rdl): 0.0122064333, 0.0010851513

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 317
rank avg (pred): 0.013 +- 0.010
mrr vals (pred, true): 0.195, 0.180
batch losses (mrrl, rdl): 0.0020525111, 9.39691e-05

Epoch over!
epoch time: 12.537

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 405
rank avg (pred): 0.382 +- 0.194
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.18787e-05, 0.0001496539

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.317 +- 0.152
mrr vals (pred, true): 0.055, 0.082
batch losses (mrrl, rdl): 0.0003004072, 0.0001974012

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 828
rank avg (pred): 0.045 +- 0.032
mrr vals (pred, true): 0.132, 0.141
batch losses (mrrl, rdl): 0.000727338, 0.0001136108

Epoch over!
epoch time: 12.476

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 0
rank avg (pred): 0.358 +- 0.198
mrr vals (pred, true): 0.073, 0.088
batch losses (mrrl, rdl): 0.0054227649, 0.0015119514

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.306 +- 0.157
mrr vals (pred, true): 0.022, 0.001
batch losses (mrrl, rdl): 0.0077468539, 0.0005232784

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 56
rank avg (pred): 0.282 +- 0.182
mrr vals (pred, true): 0.105, 0.099
batch losses (mrrl, rdl): 0.0306776017, 0.0008521834

Epoch over!
epoch time: 12.487

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 916
rank avg (pred): 0.314 +- 0.200
mrr vals (pred, true): 0.095, 0.153
batch losses (mrrl, rdl): 0.0339336917, 0.0009067594

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 315
rank avg (pred): 0.315 +- 0.192
mrr vals (pred, true): 0.095, 0.111
batch losses (mrrl, rdl): 0.0026172614, 0.0008629207

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.335 +- 0.174
mrr vals (pred, true): 0.061, 0.033
batch losses (mrrl, rdl): 0.0011901568, 0.0002625212

Epoch over!
epoch time: 12.479

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 345
rank avg (pred): 0.288 +- 0.169
mrr vals (pred, true): 0.077, 0.025
batch losses (mrrl, rdl): 0.007245305, 0.0001267175

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.223 +- 0.151
mrr vals (pred, true): 0.126, 0.125
batch losses (mrrl, rdl): 2.23557e-05, 0.0004812099

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 688
rank avg (pred): 0.344 +- 0.177
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.0013334265, 0.0005081006

Epoch over!
epoch time: 12.05

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 593
rank avg (pred): 0.352 +- 0.174
mrr vals (pred, true): 0.063, 0.039
batch losses (mrrl, rdl): 0.0017329077, 0.0001251929

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 106
rank avg (pred): 0.341 +- 0.182
mrr vals (pred, true): 0.063, 0.028
batch losses (mrrl, rdl): 0.001742932, 0.0002923158

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.321 +- 0.176
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0008571564, 0.0007962891

Epoch over!
epoch time: 12.394

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 570
rank avg (pred): 0.352 +- 0.173
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 9.317e-05, 6.06363e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.268 +- 0.170
mrr vals (pred, true): 0.077, 0.000
batch losses (mrrl, rdl): 0.007501517, 0.0011159359

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 801
rank avg (pred): 0.282 +- 0.195
mrr vals (pred, true): 0.099, 0.000
batch losses (mrrl, rdl): 0.0237459801, 0.0008172888

Epoch over!
epoch time: 12.432

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.289 +- 0.181
mrr vals (pred, true): 0.069, 0.088

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   30 	     0 	 0.05303 	 0.00018 	 m..s
   13 	     1 	 0.04855 	 0.00019 	 m..s
   72 	     2 	 0.08134 	 0.00021 	 m..s
  102 	     3 	 0.10263 	 0.00021 	 MISS
   18 	     4 	 0.04940 	 0.00021 	 m..s
   14 	     5 	 0.04862 	 0.00022 	 m..s
    0 	     6 	 0.03570 	 0.00023 	 m..s
  100 	     7 	 0.09954 	 0.00023 	 m..s
  103 	     8 	 0.10645 	 0.00024 	 MISS
   79 	     9 	 0.09112 	 0.00024 	 m..s
   20 	    10 	 0.04950 	 0.00024 	 m..s
   66 	    11 	 0.07310 	 0.00025 	 m..s
   22 	    12 	 0.04997 	 0.00025 	 m..s
   99 	    13 	 0.09736 	 0.00026 	 m..s
    6 	    14 	 0.04269 	 0.00027 	 m..s
    1 	    15 	 0.03580 	 0.00028 	 m..s
   97 	    16 	 0.09523 	 0.00028 	 m..s
   80 	    17 	 0.09121 	 0.00028 	 m..s
   95 	    18 	 0.09436 	 0.00029 	 m..s
    9 	    19 	 0.04766 	 0.00031 	 m..s
   80 	    20 	 0.09121 	 0.00034 	 m..s
   80 	    21 	 0.09121 	 0.00036 	 m..s
   80 	    22 	 0.09121 	 0.00037 	 m..s
   10 	    23 	 0.04804 	 0.00039 	 m..s
   15 	    24 	 0.04872 	 0.00040 	 m..s
    8 	    25 	 0.04460 	 0.00040 	 m..s
   37 	    26 	 0.05442 	 0.00040 	 m..s
   51 	    27 	 0.06138 	 0.00042 	 m..s
   80 	    28 	 0.09121 	 0.00044 	 m..s
   28 	    29 	 0.05264 	 0.00045 	 m..s
   36 	    30 	 0.05431 	 0.00045 	 m..s
   39 	    31 	 0.05502 	 0.00048 	 m..s
   43 	    32 	 0.05765 	 0.00061 	 m..s
   11 	    33 	 0.04812 	 0.00064 	 m..s
    2 	    34 	 0.03636 	 0.00067 	 m..s
   92 	    35 	 0.09317 	 0.00074 	 m..s
   80 	    36 	 0.09121 	 0.00081 	 m..s
   23 	    37 	 0.05002 	 0.00160 	 m..s
   60 	    38 	 0.06560 	 0.00194 	 m..s
    3 	    39 	 0.03655 	 0.00479 	 m..s
    7 	    40 	 0.04343 	 0.01078 	 m..s
    5 	    41 	 0.03857 	 0.01110 	 ~...
   19 	    42 	 0.04947 	 0.01678 	 m..s
   29 	    43 	 0.05297 	 0.01959 	 m..s
   16 	    44 	 0.04877 	 0.02032 	 ~...
   12 	    45 	 0.04817 	 0.02180 	 ~...
    4 	    46 	 0.03786 	 0.02537 	 ~...
   54 	    47 	 0.06173 	 0.02906 	 m..s
   32 	    48 	 0.05318 	 0.04363 	 ~...
   25 	    49 	 0.05021 	 0.04427 	 ~...
   24 	    50 	 0.05002 	 0.04863 	 ~...
   33 	    51 	 0.05327 	 0.04931 	 ~...
   38 	    52 	 0.05442 	 0.05024 	 ~...
   41 	    53 	 0.05702 	 0.05032 	 ~...
   35 	    54 	 0.05368 	 0.05415 	 ~...
   40 	    55 	 0.05648 	 0.05634 	 ~...
   17 	    56 	 0.04889 	 0.05686 	 ~...
   42 	    57 	 0.05739 	 0.05840 	 ~...
   58 	    58 	 0.06383 	 0.06100 	 ~...
   26 	    59 	 0.05212 	 0.06330 	 ~...
   45 	    60 	 0.05818 	 0.06476 	 ~...
   21 	    61 	 0.04951 	 0.06677 	 ~...
   49 	    62 	 0.06117 	 0.06684 	 ~...
   53 	    63 	 0.06165 	 0.06856 	 ~...
   48 	    64 	 0.05968 	 0.07021 	 ~...
  101 	    65 	 0.09998 	 0.07414 	 ~...
   61 	    66 	 0.06655 	 0.07602 	 ~...
   44 	    67 	 0.05768 	 0.08064 	 ~...
   27 	    68 	 0.05236 	 0.08123 	 ~...
   89 	    69 	 0.09192 	 0.08234 	 ~...
   56 	    70 	 0.06305 	 0.08336 	 ~...
   47 	    71 	 0.05917 	 0.08501 	 ~...
   46 	    72 	 0.05874 	 0.08581 	 ~...
   75 	    73 	 0.08850 	 0.08630 	 ~...
   62 	    74 	 0.06888 	 0.08798 	 ~...
   57 	    75 	 0.06326 	 0.09392 	 m..s
   94 	    76 	 0.09404 	 0.09594 	 ~...
  104 	    77 	 0.11267 	 0.09849 	 ~...
   77 	    78 	 0.09018 	 0.10130 	 ~...
   50 	    79 	 0.06128 	 0.10178 	 m..s
   63 	    80 	 0.06891 	 0.10197 	 m..s
   98 	    81 	 0.09637 	 0.10204 	 ~...
   73 	    82 	 0.08169 	 0.10243 	 ~...
   31 	    83 	 0.05311 	 0.10378 	 m..s
   55 	    84 	 0.06248 	 0.10468 	 m..s
   71 	    85 	 0.08054 	 0.10539 	 ~...
   80 	    86 	 0.09121 	 0.11379 	 ~...
   76 	    87 	 0.08988 	 0.11589 	 ~...
   34 	    88 	 0.05358 	 0.11594 	 m..s
   70 	    89 	 0.07965 	 0.11742 	 m..s
   52 	    90 	 0.06156 	 0.12692 	 m..s
   59 	    91 	 0.06460 	 0.12804 	 m..s
  107 	    92 	 0.12748 	 0.13345 	 ~...
  108 	    93 	 0.13154 	 0.13455 	 ~...
   68 	    94 	 0.07554 	 0.13466 	 m..s
   93 	    95 	 0.09343 	 0.13498 	 m..s
   64 	    96 	 0.07299 	 0.13907 	 m..s
  105 	    97 	 0.12035 	 0.14115 	 ~...
   74 	    98 	 0.08567 	 0.14154 	 m..s
  111 	    99 	 0.14027 	 0.14882 	 ~...
   80 	   100 	 0.09121 	 0.15148 	 m..s
   96 	   101 	 0.09461 	 0.15452 	 m..s
   65 	   102 	 0.07305 	 0.15681 	 m..s
   69 	   103 	 0.07929 	 0.15753 	 m..s
   67 	   104 	 0.07409 	 0.15935 	 m..s
   80 	   105 	 0.09121 	 0.15970 	 m..s
  109 	   106 	 0.13622 	 0.16166 	 ~...
   90 	   107 	 0.09257 	 0.16753 	 m..s
   91 	   108 	 0.09259 	 0.16959 	 m..s
  110 	   109 	 0.14023 	 0.18310 	 m..s
   78 	   110 	 0.09076 	 0.19120 	 MISS
  112 	   111 	 0.15373 	 0.21520 	 m..s
  106 	   112 	 0.12680 	 0.21654 	 m..s
  119 	   113 	 0.23679 	 0.21928 	 ~...
  113 	   114 	 0.16387 	 0.22092 	 m..s
  114 	   115 	 0.16487 	 0.22896 	 m..s
  115 	   116 	 0.16957 	 0.23564 	 m..s
  118 	   117 	 0.23126 	 0.30066 	 m..s
  120 	   118 	 0.25622 	 0.30943 	 m..s
  117 	   119 	 0.21960 	 0.31001 	 m..s
  116 	   120 	 0.21512 	 0.32792 	 MISS
==========================================
r_mrr = 0.7643291354179382
r2_mrr = 0.531147837638855
spearmanr_mrr@5 = 0.7781586050987244
spearmanr_mrr@10 = 0.9284483790397644
spearmanr_mrr@50 = 0.9654704332351685
spearmanr_mrr@100 = 0.9536040425300598
spearmanr_mrr@All = 0.95903480052948
==========================================
test time: 0.39
Done Testing dataset DBpedia50
total time taken: 190.18183636665344
training time taken: 184.23132729530334
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7643)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.5311)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.7782)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9284)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9655)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9536)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9590)}}, 'test_loss': {'TransE': {'DBpedia50': 1.9761654433168587}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 7858386549971944
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1076, 613, 1131, 1066, 947, 526, 42, 161, 313, 914, 167, 137, 116, 668, 976, 4, 949, 1012, 1188, 735, 466, 30, 1144, 44, 225, 688, 1083, 724, 1135, 299, 1210, 749, 1178, 1057, 460, 765, 1096, 692, 933, 1119, 276, 954, 939, 1200, 3, 863, 521, 1133, 322, 705, 113, 1141, 1079, 811, 788, 575, 1168, 817, 1120, 861, 1146, 836, 58, 506, 1147, 301, 772, 881, 507, 306, 827, 1091, 345, 1016, 887, 775, 154, 391, 367, 1193, 434, 809, 532, 789, 1151, 942, 132, 839, 390, 92, 734, 852, 814, 385, 16, 717, 866, 898, 961, 439, 1173, 872, 502, 186, 401, 581, 1153, 1011, 339, 270, 807, 43, 860, 1166, 547, 1054, 41, 52, 925, 266, 1084]
valid_ids (0): []
train_ids (1094): [54, 179, 35, 321, 127, 689, 777, 833, 531, 255, 412, 729, 1212, 955, 202, 642, 570, 600, 68, 163, 617, 300, 672, 361, 386, 1183, 1094, 727, 124, 259, 125, 372, 597, 304, 612, 810, 338, 84, 184, 665, 1180, 143, 793, 366, 985, 271, 588, 1031, 134, 986, 1070, 260, 249, 712, 1127, 61, 393, 806, 957, 148, 611, 967, 1179, 980, 834, 1034, 24, 273, 441, 471, 221, 926, 737, 655, 150, 217, 624, 1080, 1020, 326, 522, 63, 1078, 65, 1152, 508, 750, 327, 91, 493, 50, 406, 956, 1014, 349, 17, 759, 1206, 1181, 1081, 101, 529, 907, 844, 258, 801, 855, 356, 1051, 1205, 910, 212, 485, 346, 920, 1007, 427, 1087, 886, 697, 649, 469, 1209, 573, 571, 880, 408, 364, 250, 122, 395, 824, 199, 274, 1041, 864, 1194, 165, 1105, 1021, 974, 781, 703, 607, 286, 940, 1088, 982, 1053, 465, 481, 213, 1177, 1037, 608, 19, 676, 494, 117, 319, 643, 281, 194, 902, 865, 563, 491, 236, 858, 470, 656, 201, 943, 103, 399, 1207, 1211, 411, 1050, 1075, 670, 287, 435, 214, 1059, 592, 572, 661, 984, 680, 82, 468, 1157, 558, 561, 244, 690, 776, 472, 604, 229, 1042, 1138, 83, 990, 368, 1114, 1160, 423, 1095, 1000, 894, 669, 882, 200, 38, 1113, 622, 12, 952, 602, 456, 365, 378, 1004, 135, 267, 1018, 93, 348, 537, 1140, 681, 293, 486, 362, 407, 1009, 1162, 1174, 331, 979, 631, 544, 657, 845, 305, 787, 13, 564, 355, 700, 492, 487, 178, 1150, 972, 1032, 422, 543, 619, 428, 1203, 342, 527, 53, 838, 144, 1202, 847, 80, 282, 536, 796, 1159, 1027, 256, 1143, 1071, 1122, 187, 679, 1099, 822, 708, 1052, 704, 999, 755, 517, 351, 591, 220, 849, 523, 535, 1136, 398, 792, 195, 662, 36, 285, 1033, 1077, 1139, 463, 231, 474, 962, 153, 303, 66, 90, 691, 936, 1171, 918, 400, 869, 946, 462, 757, 639, 81, 525, 311, 443, 577, 1074, 1030, 1008, 879, 482, 278, 911, 197, 1109, 97, 823, 515, 234, 1068, 114, 1126, 20, 627, 871, 39, 634, 761, 6, 175, 419, 878, 731, 519, 442, 170, 610, 971, 238, 929, 237, 461, 915, 1165, 753, 959, 1, 257, 752, 1115, 138, 222, 747, 994, 953, 965, 207, 1169, 504, 1060, 510, 841, 644, 152, 966, 315, 1124, 916, 94, 223, 454, 945, 298, 32, 516, 897, 1149, 867, 173, 1145, 585, 678, 742, 336, 296, 130, 785, 900, 436, 478, 713, 848, 495, 464, 176, 601, 599, 477, 387, 447, 1130, 433, 151, 1154, 183, 48, 859, 744, 1046, 695, 1061, 264, 964, 26, 873, 343, 1056, 648, 177, 459, 722, 1204, 297, 190, 254, 206, 560, 1161, 128, 181, 973, 228, 392, 913, 1103, 620, 751, 720, 371, 795, 72, 551, 760, 635, 1199, 131, 798, 1155, 14, 246, 1025, 565, 567, 158, 1110, 562, 596, 302, 198, 1006, 709, 698, 45, 628, 1187, 969, 715, 557, 1112, 1090, 808, 832, 295, 766, 556, 748, 344, 542, 226, 951, 329, 509, 889, 1055, 584, 586, 893, 388, 1176, 292, 1156, 938, 651, 437, 721, 374, 1047, 802, 22, 714, 870, 330, 379, 819, 505, 921, 800, 147, 904, 224, 1085, 931, 677, 180, 699, 598, 937, 738, 112, 272, 252, 353, 970, 603, 812, 284, 1064, 778, 280, 856, 7, 328, 1003, 786, 583, 647, 56, 782, 927, 595, 559, 307, 333, 701, 1118, 896, 1163, 968, 934, 1035, 790, 501, 323, 430, 1197, 1101, 813, 768, 1010, 998, 1015, 290, 182, 576, 324, 1023, 265, 139, 730, 696, 1190, 769, 404, 582, 318, 710, 34, 95, 1134, 410, 587, 370, 574, 1132, 928, 770, 409, 1049, 736, 1058, 1116, 449, 189, 837, 230, 804, 1129, 108, 652, 1073, 1017, 247, 1201, 1189, 241, 623, 105, 901, 233, 51, 484, 106, 205, 376, 605, 988, 917, 743, 741, 85, 636, 763, 394, 415, 767, 983, 1063, 740, 263, 851, 1102, 658, 15, 829, 268, 18, 209, 1100, 1106, 363, 73, 541, 686, 733, 1038, 118, 377, 357, 496, 438, 923, 640, 123, 453, 883, 473, 550, 253, 853, 846, 444, 615, 164, 566, 79, 102, 549, 666, 421, 1024, 702, 291, 1195, 126, 402, 745, 107, 86, 1022, 684, 219, 978, 193, 1044, 554, 876, 136, 764, 310, 70, 590, 2, 431, 350, 381, 895, 641, 129, 794, 835, 5, 892, 726, 960, 815, 868, 448, 490, 555, 172, 25, 831, 1086, 1048, 1013, 1028, 633, 28, 1184, 62, 76, 728, 716, 314, 997, 771, 87, 1137, 1158, 524, 625, 162, 308, 98, 1072, 29, 354, 141, 885, 663, 160, 779, 445, 816, 963, 110, 513, 146, 1191, 373, 739, 384, 630, 891, 498, 185, 240, 1182, 483, 414, 426, 606, 944, 653, 579, 987, 1067, 279, 1128, 580, 78, 168, 1172, 991, 1107, 958, 100, 1089, 1148, 996, 218, 204, 60, 906, 432, 1123, 251, 159, 948, 166, 930, 440, 111, 479, 1111, 216, 403, 548, 552, 1026, 553, 539, 499, 512, 667, 1192, 488, 341, 242, 694, 188, 21, 171, 719, 75, 992, 416, 683, 1029, 375, 843, 756, 1043, 840, 905, 1198, 55, 88, 289, 208, 774, 674, 1092, 783, 121, 874, 1121, 578, 10, 358, 1186, 451, 33, 57, 71, 145, 780, 1097, 142, 37, 1170, 618, 120, 500, 211, 616, 545, 903, 830, 425, 540, 11, 784, 334, 732, 1104, 59, 803, 725, 169, 196, 671, 1213, 104, 288, 380, 1082, 818, 382, 977, 935, 418, 489, 283, 932, 174, 1040, 594, 1069, 232, 269, 389, 758, 480, 693, 317, 646, 47, 654, 660, 950, 1098, 450, 664, 149, 316, 530, 850, 325, 69, 629, 40, 922, 239, 762, 995, 396, 458, 191, 320, 1108, 569, 589, 1164, 294, 993, 675, 919, 975, 452, 405, 133, 799, 685, 413, 821, 842, 888, 546, 511, 335, 707, 650, 1142, 1125, 912, 74, 797, 1036, 746, 31, 262, 687, 156, 1005, 826, 862, 534, 875, 337, 261, 533, 609, 369, 245, 27, 64, 791, 514, 397, 192, 820, 989, 115, 420, 275, 773, 429, 825, 119, 638, 340, 659, 243, 924, 909, 109, 890, 140, 215, 0, 497, 614, 1062, 857, 1039, 1185, 626, 67, 8, 89, 1214, 711, 77, 854, 1208, 347, 1175, 49, 1093, 1167, 417, 96, 475, 235, 1117, 99, 706, 424, 621, 248, 1002, 157, 1065, 210, 682, 277, 332, 568, 941, 673, 877, 312, 227, 360, 455, 593, 1001, 203, 446, 23, 309, 467, 457, 503, 723, 476, 908, 520, 637, 528, 155, 383, 981, 754, 899, 632, 1196, 718, 884, 359, 645, 352, 518, 46, 538, 1045, 805, 9, 1019, 828]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4545665955189231
the save name prefix for this run is:  chkpt-ID_4545665955189231_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 527
rank avg (pred): 0.390 +- 0.005
mrr vals (pred, true): 0.000, 0.068
batch losses (mrrl, rdl): 0.0, 0.0003985333

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 436
rank avg (pred): 0.342 +- 0.213
mrr vals (pred, true): 0.125, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003859597

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1198
rank avg (pred): 0.332 +- 0.303
mrr vals (pred, true): 0.278, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002252551

Epoch over!
epoch time: 12.505

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 642
rank avg (pred): 0.374 +- 0.340
mrr vals (pred, true): 0.269, 0.003
batch losses (mrrl, rdl): 0.0, 9.41629e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.053 +- 0.049
mrr vals (pred, true): 0.352, 0.210
batch losses (mrrl, rdl): 0.0, 1.57027e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.333 +- 0.305
mrr vals (pred, true): 0.276, 0.191
batch losses (mrrl, rdl): 0.0, 0.0007795327

Epoch over!
epoch time: 11.789

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.134 +- 0.124
mrr vals (pred, true): 0.307, 0.077
batch losses (mrrl, rdl): 0.0, 3.45282e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.310 +- 0.288
mrr vals (pred, true): 0.347, 0.128
batch losses (mrrl, rdl): 0.0, 0.0003856324

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.315 +- 0.292
mrr vals (pred, true): 0.319, 0.135
batch losses (mrrl, rdl): 0.0, 0.0004393367

Epoch over!
epoch time: 12.269

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 801
rank avg (pred): 0.321 +- 0.297
mrr vals (pred, true): 0.289, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003669781

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 638
rank avg (pred): 0.367 +- 0.321
mrr vals (pred, true): 0.210, 0.032
batch losses (mrrl, rdl): 0.0, 0.0001346221

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1082
rank avg (pred): 0.335 +- 0.305
mrr vals (pred, true): 0.259, 0.146
batch losses (mrrl, rdl): 0.0, 0.000467595

Epoch over!
epoch time: 11.89

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 768
rank avg (pred): 0.321 +- 0.296
mrr vals (pred, true): 0.307, 0.170
batch losses (mrrl, rdl): 0.0, 0.0002803552

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.236 +- 0.219
mrr vals (pred, true): 0.351, 0.035
batch losses (mrrl, rdl): 0.0, 6.33272e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.177 +- 0.165
mrr vals (pred, true): 0.378, 0.098
batch losses (mrrl, rdl): 0.0, 6.45436e-05

Epoch over!
epoch time: 12.649

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 67
rank avg (pred): 0.115 +- 0.107
mrr vals (pred, true): 0.379, 0.088
batch losses (mrrl, rdl): 1.0804347992, 4.02274e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.345 +- 0.187
mrr vals (pred, true): 0.071, 0.099
batch losses (mrrl, rdl): 0.0045189685, 0.0013167007

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.084 +- 0.051
mrr vals (pred, true): 0.113, 0.134
batch losses (mrrl, rdl): 0.0040765419, 3.51957e-05

Epoch over!
epoch time: 12.626

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 525
rank avg (pred): 0.332 +- 0.125
mrr vals (pred, true): 0.059, 0.034
batch losses (mrrl, rdl): 0.0008627943, 0.0002027522

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 726
rank avg (pred): 0.449 +- 0.118
mrr vals (pred, true): 0.042, 0.001
batch losses (mrrl, rdl): 0.0007207396, 0.0001026439

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 487
rank avg (pred): 0.318 +- 0.106
mrr vals (pred, true): 0.055, 0.057
batch losses (mrrl, rdl): 0.000257969, 0.0001952002

Epoch over!
epoch time: 12.394

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 846
rank avg (pred): 0.363 +- 0.223
mrr vals (pred, true): 0.101, 0.149
batch losses (mrrl, rdl): 0.0226122141, 0.0005046937

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 996
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.249, 0.296
batch losses (mrrl, rdl): 0.0227195937, 7.4604e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 309
rank avg (pred): 0.270 +- 0.143
mrr vals (pred, true): 0.075, 0.070
batch losses (mrrl, rdl): 0.0061062328, 0.00045403

Epoch over!
epoch time: 11.893

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 199
rank avg (pred): 0.345 +- 0.116
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0005875629, 0.0004005052

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 849
rank avg (pred): 0.315 +- 0.199
mrr vals (pred, true): 0.115, 0.168
batch losses (mrrl, rdl): 0.0274681281, 0.0003472331

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 557
rank avg (pred): 0.326 +- 0.129
mrr vals (pred, true): 0.056, 0.082
batch losses (mrrl, rdl): 0.0004181192, 0.00023805

Epoch over!
epoch time: 12.704

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.333 +- 0.133
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0014524334, 0.0004474546

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1050
rank avg (pred): 0.311 +- 0.198
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0316563621, 0.0004486387

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 699
rank avg (pred): 0.351 +- 0.132
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.0386e-06, 0.0004018575

Epoch over!
epoch time: 12.251

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.066 +- 0.042
mrr vals (pred, true): 0.103, 0.085
batch losses (mrrl, rdl): 0.0285300761, 0.0001081914

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1050
rank avg (pred): 0.387 +- 0.216
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0178075545, 0.0001161474

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.305 +- 0.173
mrr vals (pred, true): 0.081, 0.128
batch losses (mrrl, rdl): 0.0224426221, 0.0004389284

Epoch over!
epoch time: 12.6

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 778
rank avg (pred): 0.296 +- 0.188
mrr vals (pred, true): 0.092, 0.166
batch losses (mrrl, rdl): 0.0546678342, 0.0003288545

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 550
rank avg (pred): 0.377 +- 0.138
mrr vals (pred, true): 0.056, 0.055
batch losses (mrrl, rdl): 0.0003427952, 0.0003056852

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.012 +- 0.009
mrr vals (pred, true): 0.242, 0.185
batch losses (mrrl, rdl): 0.0324900486, 0.0001683753

Epoch over!
epoch time: 12.411

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 278
rank avg (pred): 0.174 +- 0.109
mrr vals (pred, true): 0.095, 0.101
batch losses (mrrl, rdl): 0.0004103266, 0.0001182929

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1204
rank avg (pred): 0.376 +- 0.148
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005989351, 0.0002536501

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1139
rank avg (pred): 0.331 +- 0.206
mrr vals (pred, true): 0.088, 0.101
batch losses (mrrl, rdl): 0.0016092324, 0.0004605604

Epoch over!
epoch time: 12.478

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 902
rank avg (pred): 0.269 +- 0.188
mrr vals (pred, true): 0.132, 0.106
batch losses (mrrl, rdl): 0.0067234375, 0.000620892

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 201
rank avg (pred): 0.364 +- 0.150
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005040901, 0.0003628619

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 617
rank avg (pred): 0.421 +- 0.142
mrr vals (pred, true): 0.057, 0.033
batch losses (mrrl, rdl): 0.000500121, 0.0002884112

Epoch over!
epoch time: 13.214

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 897
rank avg (pred): 0.354 +- 0.221
mrr vals (pred, true): 0.087, 0.107
batch losses (mrrl, rdl): 0.0039775176, 0.0013154275

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 924
rank avg (pred): 0.352 +- 0.219
mrr vals (pred, true): 0.093, 0.106
batch losses (mrrl, rdl): 0.0018321318, 0.0004166375

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 563
rank avg (pred): 0.353 +- 0.155
mrr vals (pred, true): 0.058, 0.087
batch losses (mrrl, rdl): 0.0006380282, 0.0003081587

Epoch over!
epoch time: 12.411

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.210, 0.271

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   32 	     0 	 0.05721 	 0.00014 	 m..s
   76 	     1 	 0.08712 	 0.00018 	 m..s
   18 	     2 	 0.05691 	 0.00019 	 m..s
  106 	     3 	 0.10809 	 0.00021 	 MISS
    2 	     4 	 0.04378 	 0.00021 	 m..s
   15 	     5 	 0.05685 	 0.00021 	 m..s
   74 	     6 	 0.08366 	 0.00021 	 m..s
   35 	     7 	 0.05746 	 0.00022 	 m..s
   47 	     8 	 0.06041 	 0.00024 	 m..s
   13 	     9 	 0.05657 	 0.00024 	 m..s
    6 	    10 	 0.05230 	 0.00024 	 m..s
   23 	    11 	 0.05705 	 0.00024 	 m..s
   33 	    12 	 0.05722 	 0.00026 	 m..s
   17 	    13 	 0.05690 	 0.00027 	 m..s
  101 	    14 	 0.09614 	 0.00029 	 m..s
   92 	    15 	 0.09181 	 0.00029 	 m..s
  103 	    16 	 0.10052 	 0.00030 	 MISS
    1 	    17 	 0.03769 	 0.00030 	 m..s
    0 	    18 	 0.03527 	 0.00031 	 m..s
   69 	    19 	 0.07995 	 0.00034 	 m..s
   76 	    20 	 0.08712 	 0.00034 	 m..s
   76 	    21 	 0.08712 	 0.00035 	 m..s
  102 	    22 	 0.09849 	 0.00035 	 m..s
   61 	    23 	 0.07536 	 0.00036 	 m..s
   41 	    24 	 0.05794 	 0.00039 	 m..s
   26 	    25 	 0.05708 	 0.00043 	 m..s
   68 	    26 	 0.07977 	 0.00044 	 m..s
   88 	    27 	 0.09142 	 0.00054 	 m..s
   76 	    28 	 0.08712 	 0.00081 	 m..s
   76 	    29 	 0.08712 	 0.00082 	 m..s
    4 	    30 	 0.04939 	 0.00096 	 m..s
   22 	    31 	 0.05705 	 0.00171 	 m..s
   86 	    32 	 0.09029 	 0.00226 	 m..s
    7 	    33 	 0.05250 	 0.00611 	 m..s
   30 	    34 	 0.05712 	 0.01248 	 m..s
    8 	    35 	 0.05383 	 0.02183 	 m..s
   42 	    36 	 0.05908 	 0.02510 	 m..s
   37 	    37 	 0.05753 	 0.02723 	 m..s
   36 	    38 	 0.05750 	 0.02825 	 ~...
   10 	    39 	 0.05544 	 0.03475 	 ~...
   48 	    40 	 0.06116 	 0.04915 	 ~...
   44 	    41 	 0.05947 	 0.04998 	 ~...
   21 	    42 	 0.05704 	 0.05021 	 ~...
   16 	    43 	 0.05685 	 0.05294 	 ~...
   12 	    44 	 0.05654 	 0.05415 	 ~...
   57 	    45 	 0.06574 	 0.05436 	 ~...
   20 	    46 	 0.05703 	 0.05580 	 ~...
   25 	    47 	 0.05706 	 0.05609 	 ~...
   56 	    48 	 0.06566 	 0.05707 	 ~...
   49 	    49 	 0.06162 	 0.05784 	 ~...
    3 	    50 	 0.04843 	 0.05908 	 ~...
   40 	    51 	 0.05784 	 0.06045 	 ~...
    5 	    52 	 0.05146 	 0.06066 	 ~...
    9 	    53 	 0.05482 	 0.06330 	 ~...
   24 	    54 	 0.05705 	 0.06677 	 ~...
   45 	    55 	 0.05971 	 0.06731 	 ~...
   31 	    56 	 0.05715 	 0.06811 	 ~...
   39 	    57 	 0.05778 	 0.06856 	 ~...
   19 	    58 	 0.05692 	 0.06864 	 ~...
   29 	    59 	 0.05712 	 0.06900 	 ~...
   51 	    60 	 0.06373 	 0.07222 	 ~...
   14 	    61 	 0.05672 	 0.07437 	 ~...
   34 	    62 	 0.05723 	 0.07478 	 ~...
   28 	    63 	 0.05708 	 0.07741 	 ~...
   11 	    64 	 0.05581 	 0.07950 	 ~...
   27 	    65 	 0.05708 	 0.08176 	 ~...
   96 	    66 	 0.09385 	 0.08669 	 ~...
   95 	    67 	 0.09371 	 0.08889 	 ~...
  104 	    68 	 0.10519 	 0.09016 	 ~...
   55 	    69 	 0.06541 	 0.09117 	 ~...
   59 	    70 	 0.06764 	 0.09158 	 ~...
   38 	    71 	 0.05755 	 0.09247 	 m..s
   54 	    72 	 0.06516 	 0.09275 	 ~...
  107 	    73 	 0.11862 	 0.09521 	 ~...
   46 	    74 	 0.06005 	 0.09562 	 m..s
  110 	    75 	 0.15309 	 0.09781 	 m..s
  111 	    76 	 0.15612 	 0.09849 	 m..s
   50 	    77 	 0.06343 	 0.09867 	 m..s
   71 	    78 	 0.08217 	 0.10002 	 ~...
   73 	    79 	 0.08278 	 0.10101 	 ~...
   75 	    80 	 0.08528 	 0.10154 	 ~...
   53 	    81 	 0.06513 	 0.10468 	 m..s
   58 	    82 	 0.06663 	 0.10511 	 m..s
   70 	    83 	 0.08058 	 0.10806 	 ~...
   76 	    84 	 0.08712 	 0.10908 	 ~...
   43 	    85 	 0.05946 	 0.10944 	 m..s
   87 	    86 	 0.09072 	 0.11545 	 ~...
   97 	    87 	 0.09399 	 0.12497 	 m..s
  108 	    88 	 0.12196 	 0.12847 	 ~...
   52 	    89 	 0.06374 	 0.12885 	 m..s
   85 	    90 	 0.08793 	 0.12964 	 m..s
   64 	    91 	 0.07776 	 0.14154 	 m..s
   98 	    92 	 0.09401 	 0.14335 	 m..s
   63 	    93 	 0.07621 	 0.14422 	 m..s
   90 	    94 	 0.09156 	 0.14530 	 m..s
  109 	    95 	 0.12485 	 0.14882 	 ~...
   62 	    96 	 0.07604 	 0.15099 	 m..s
   72 	    97 	 0.08247 	 0.15482 	 m..s
   93 	    98 	 0.09240 	 0.15698 	 m..s
   91 	    99 	 0.09174 	 0.15868 	 m..s
   83 	   100 	 0.08718 	 0.16360 	 m..s
   89 	   101 	 0.09144 	 0.16364 	 m..s
   84 	   102 	 0.08766 	 0.16627 	 m..s
   67 	   103 	 0.07962 	 0.16710 	 m..s
   65 	   104 	 0.07787 	 0.16715 	 m..s
   76 	   105 	 0.08712 	 0.16883 	 m..s
   94 	   106 	 0.09253 	 0.16959 	 m..s
   66 	   107 	 0.07918 	 0.18732 	 MISS
   99 	   108 	 0.09532 	 0.18892 	 m..s
  100 	   109 	 0.09611 	 0.20077 	 MISS
  117 	   110 	 0.24171 	 0.20501 	 m..s
   60 	   111 	 0.07427 	 0.20735 	 MISS
  105 	   112 	 0.10613 	 0.21796 	 MISS
  112 	   113 	 0.18362 	 0.22192 	 m..s
  113 	   114 	 0.18404 	 0.24065 	 m..s
  120 	   115 	 0.33650 	 0.24472 	 m..s
  114 	   116 	 0.20994 	 0.27102 	 m..s
  118 	   117 	 0.24266 	 0.28691 	 m..s
  119 	   118 	 0.24350 	 0.29489 	 m..s
  116 	   119 	 0.23226 	 0.30790 	 m..s
  115 	   120 	 0.22577 	 0.31001 	 m..s
==========================================
r_mrr = 0.6916012763977051
r2_mrr = 0.47203683853149414
spearmanr_mrr@5 = 0.6282535195350647
spearmanr_mrr@10 = 0.8653603196144104
spearmanr_mrr@50 = 0.9258267879486084
spearmanr_mrr@100 = 0.8951511383056641
spearmanr_mrr@All = 0.8931087255477905
==========================================
test time: 0.444
Done Testing dataset DBpedia50
total time taken: 192.48960256576538
training time taken: 186.61526226997375
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.6916)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4720)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.6283)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8654)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9258)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.8952)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.8931)}}, 'test_loss': {'TransE': {'DBpedia50': 2.6138099902891554}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7325289107363570
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [920, 502, 184, 1133, 500, 592, 668, 1148, 258, 1174, 767, 1046, 887, 70, 295, 487, 1175, 652, 120, 16, 516, 321, 1099, 508, 1113, 1053, 52, 970, 530, 133, 1045, 325, 243, 797, 438, 1152, 1176, 271, 1102, 608, 1170, 625, 1210, 597, 1181, 369, 851, 994, 273, 383, 89, 496, 968, 181, 1038, 969, 84, 939, 698, 929, 99, 653, 1011, 460, 177, 973, 620, 82, 1031, 977, 840, 503, 1199, 832, 279, 1060, 548, 68, 991, 439, 1016, 450, 408, 388, 87, 341, 606, 1091, 1186, 126, 563, 69, 256, 728, 1139, 251, 556, 922, 3, 14, 434, 1026, 714, 482, 864, 941, 1125, 918, 658, 716, 230, 945, 749, 574, 687, 1171, 1049, 1012, 18, 37, 580]
valid_ids (0): []
train_ids (1094): [1094, 418, 927, 400, 1054, 201, 878, 111, 550, 289, 246, 39, 40, 986, 389, 75, 506, 925, 440, 380, 891, 898, 744, 857, 119, 513, 776, 821, 672, 953, 1119, 818, 946, 691, 303, 476, 0, 909, 268, 8, 285, 1015, 199, 865, 419, 727, 1190, 223, 729, 1043, 182, 480, 924, 1201, 1197, 297, 537, 45, 1112, 573, 517, 560, 957, 1137, 551, 93, 240, 631, 1041, 338, 789, 451, 1184, 628, 1179, 769, 1040, 614, 307, 1081, 562, 1114, 1150, 889, 838, 700, 121, 585, 740, 913, 812, 187, 1159, 591, 1183, 527, 932, 993, 558, 493, 456, 237, 1117, 160, 1172, 895, 967, 42, 534, 190, 661, 130, 1059, 290, 1071, 762, 155, 208, 207, 481, 141, 1196, 826, 484, 168, 478, 1212, 218, 495, 755, 626, 124, 754, 235, 162, 224, 173, 464, 1187, 324, 463, 978, 422, 647, 1193, 1124, 809, 611, 618, 100, 28, 853, 371, 71, 35, 686, 676, 645, 702, 954, 1028, 656, 813, 346, 919, 535, 515, 526, 955, 671, 747, 486, 869, 989, 455, 533, 900, 917, 304, 145, 417, 90, 518, 1143, 1009, 961, 241, 83, 431, 1032, 1200, 884, 221, 1214, 314, 217, 1021, 1083, 349, 1204, 1065, 1019, 51, 106, 31, 879, 1131, 1156, 854, 972, 1169, 971, 326, 801, 1127, 651, 416, 567, 347, 858, 806, 191, 1108, 600, 62, 699, 421, 681, 483, 196, 603, 437, 873, 405, 77, 219, 741, 281, 781, 866, 1055, 657, 856, 97, 169, 947, 188, 981, 916, 1023, 1084, 316, 1090, 992, 150, 339, 568, 570, 1182, 212, 452, 860, 862, 791, 29, 844, 161, 1085, 475, 876, 209, 733, 469, 498, 1030, 1157, 1118, 708, 514, 612, 435, 679, 1014, 137, 158, 807, 61, 306, 1158, 131, 828, 760, 379, 151, 78, 286, 1105, 587, 1213, 931, 710, 646, 1198, 522, 731, 673, 139, 1077, 1062, 852, 694, 870, 332, 311, 110, 787, 649, 38, 565, 943, 63, 892, 144, 512, 538, 665, 842, 904, 394, 914, 291, 270, 950, 32, 178, 340, 393, 391, 1075, 489, 377, 525, 1069, 1086, 982, 1001, 1202, 56, 157, 354, 693, 1138, 1206, 829, 704, 72, 319, 677, 350, 59, 1130, 888, 598, 1164, 125, 765, 963, 983, 468, 453, 1111, 24, 644, 761, 1154, 547, 477, 293, 1189, 976, 539, 1002, 320, 616, 604, 578, 793, 890, 774, 262, 1078, 461, 726, 85, 413, 1136, 175, 1050, 770, 819, 472, 1110, 855, 752, 707, 328, 984, 1155, 544, 1101, 905, 875, 798, 272, 1057, 1018, 406, 1058, 176, 689, 105, 443, 1024, 259, 613, 846, 910, 1096, 488, 206, 269, 352, 923, 822, 25, 666, 283, 334, 609, 524, 893, 959, 412, 494, 763, 232, 88, 22, 764, 81, 958, 997, 127, 784, 936, 66, 129, 385, 12, 933, 835, 794, 238, 885, 778, 519, 331, 715, 874, 1140, 1017, 1104, 777, 629, 1191, 1100, 799, 47, 724, 1185, 542, 872, 205, 610, 594, 590, 101, 280, 662, 115, 210, 718, 951, 1020, 965, 935, 785, 630, 429, 847, 395, 287, 186, 588, 103, 617, 359, 366, 743, 102, 20, 449, 198, 593, 1177, 678, 1160, 323, 58, 147, 192, 664, 696, 1116, 109, 409, 607, 757, 897, 1064, 674, 226, 949, 639, 682, 344, 841, 446, 814, 154, 557, 583, 333, 1163, 1173, 112, 605, 786, 420, 690, 1072, 466, 479, 1033, 701, 995, 685, 54, 403, 255, 312, 149, 589, 436, 41, 734, 355, 447, 263, 723, 859, 353, 579, 153, 92, 1, 529, 128, 30, 899, 1192, 203, 523, 1147, 411, 1103, 926, 309, 908, 123, 156, 824, 660, 684, 1098, 335, 980, 596, 930, 117, 257, 601, 1207, 302, 402, 720, 17, 1109, 880, 5, 632, 1178, 10, 247, 337, 571, 1141, 64, 1037, 1006, 136, 228, 470, 737, 705, 680, 1082, 575, 713, 1208, 1079, 261, 284, 572, 670, 780, 148, 467, 135, 638, 397, 848, 342, 759, 559, 396, 751, 43, 390, 771, 552, 1162, 634, 398, 27, 800, 877, 937, 944, 962, 564, 9, 107, 1151, 881, 1180, 654, 387, 619, 810, 627, 1047, 275, 1056, 602, 26, 50, 1004, 820, 717, 911, 278, 633, 239, 849, 21, 811, 663, 1027, 1129, 473, 975, 803, 425, 1035, 659, 1097, 211, 351, 566, 985, 301, 772, 831, 492, 1168, 637, 1115, 688, 471, 202, 432, 942, 1165, 1088, 1003, 692, 640, 655, 882, 553, 1123, 775, 368, 44, 541, 528, 845, 505, 1153, 850, 1106, 624, 706, 252, 166, 372, 225, 367, 172, 966, 348, 53, 1121, 894, 424, 376, 448, 1005, 1188, 444, 902, 1194, 1205, 214, 915, 536, 370, 248, 292, 648, 134, 315, 282, 1070, 996, 180, 531, 1211, 990, 96, 392, 883, 1022, 906, 796, 98, 987, 1080, 57, 364, 94, 756, 712, 622, 839, 1034, 782, 1013, 511, 213, 1052, 374, 277, 790, 220, 222, 329, 65, 116, 545, 1042, 825, 91, 561, 896, 549, 465, 288, 1134, 48, 318, 871, 104, 650, 159, 675, 861, 520, 1149, 276, 1128, 1107, 697, 490, 76, 683, 948, 501, 298, 1063, 802, 378, 15, 363, 742, 532, 423, 1008, 95, 132, 138, 294, 499, 709, 474, 1036, 867, 365, 382, 783, 868, 952, 46, 739, 974, 113, 928, 1120, 204, 249, 736, 510, 122, 491, 921, 236, 361, 1048, 635, 384, 1051, 183, 497, 843, 555, 576, 414, 903, 362, 938, 1068, 641, 231, 1209, 360, 459, 722, 912, 80, 410, 4, 1122, 34, 695, 457, 1195, 1166, 901, 266, 1073, 586, 23, 823, 74, 67, 1000, 1087, 299, 441, 234, 956, 1132, 768, 415, 725, 152, 711, 358, 540, 750, 313, 194, 442, 357, 250, 274, 336, 1029, 356, 322, 907, 433, 13, 373, 426, 485, 507, 1135, 817, 1144, 171, 521, 254, 317, 79, 310, 330, 758, 140, 343, 1142, 1095, 253, 185, 73, 49, 33, 804, 264, 833, 595, 827, 546, 245, 195, 399, 1066, 830, 1039, 643, 19, 345, 216, 816, 454, 577, 375, 667, 584, 999, 703, 428, 233, 621, 773, 581, 163, 863, 1010, 458, 197, 308, 242, 11, 1146, 6, 427, 2, 504, 227, 215, 960, 193, 748, 179, 988, 746, 623, 836, 1161, 260, 1061, 1126, 719, 170, 1093, 615, 738, 404, 636, 554, 805, 114, 167, 244, 886, 1067, 1145, 164, 934, 60, 174, 732, 795, 582, 1074, 669, 265, 300, 642, 964, 108, 1092, 753, 1044, 1007, 1089, 305, 808, 792, 543, 407, 430, 998, 1203, 142, 599, 36, 779, 788, 55, 569, 745, 86, 267, 815, 146, 766, 979, 735, 229, 445, 327, 1167, 509, 200, 401, 143, 1025, 462, 837, 940, 834, 381, 7, 1076, 721, 730, 189, 296, 386, 165, 118]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3975236193949154
the save name prefix for this run is:  chkpt-ID_3975236193949154_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 713
rank avg (pred): 0.592 +- 0.008
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003822617

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.355 +- 0.277
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001170754

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 475
rank avg (pred): 0.320 +- 0.288
mrr vals (pred, true): 0.207, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002028858

Epoch over!
epoch time: 12.997

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 422
rank avg (pred): 0.338 +- 0.290
mrr vals (pred, true): 0.161, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001351421

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 39
rank avg (pred): 0.118 +- 0.112
mrr vals (pred, true): 0.220, 0.066
batch losses (mrrl, rdl): 0.0, 3.89136e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.306 +- 0.302
mrr vals (pred, true): 0.353, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004050429

Epoch over!
epoch time: 11.937

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 568
rank avg (pred): 0.325 +- 0.303
mrr vals (pred, true): 0.290, 0.006
batch losses (mrrl, rdl): 0.0, 1.79889e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1207
rank avg (pred): 0.310 +- 0.305
mrr vals (pred, true): 0.329, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002200892

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1111
rank avg (pred): 0.324 +- 0.312
mrr vals (pred, true): 0.333, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003692103

Epoch over!
epoch time: 12.044

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.317 +- 0.310
mrr vals (pred, true): 0.369, 0.192
batch losses (mrrl, rdl): 0.0, 0.0005995197

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 575
rank avg (pred): 0.363 +- 0.306
mrr vals (pred, true): 0.187, 0.022
batch losses (mrrl, rdl): 0.0, 5.93635e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.309 +- 0.312
mrr vals (pred, true): 0.332, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001218706

Epoch over!
epoch time: 12.046

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1150
rank avg (pred): 0.200 +- 0.209
mrr vals (pred, true): 0.381, 0.102
batch losses (mrrl, rdl): 0.0, 3.19973e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.340 +- 0.311
mrr vals (pred, true): 0.241, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002140569

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1081
rank avg (pred): 0.279 +- 0.289
mrr vals (pred, true): 0.411, 0.170
batch losses (mrrl, rdl): 0.0, 0.0003875208

Epoch over!
epoch time: 11.891

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1183
rank avg (pred): 0.353 +- 0.312
mrr vals (pred, true): 0.271, 0.077
batch losses (mrrl, rdl): 0.4889449477, 0.0001176812

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.589 +- 0.282
mrr vals (pred, true): 0.061, 0.008
batch losses (mrrl, rdl): 0.0011138135, 0.0020366458

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 679
rank avg (pred): 0.592 +- 0.257
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 2.89675e-05, 0.0001253846

Epoch over!
epoch time: 12.321

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1073
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.191, 0.209
batch losses (mrrl, rdl): 0.0034000038, 0.0001286327

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 839
rank avg (pred): 0.353 +- 0.176
mrr vals (pred, true): 0.088, 0.167
batch losses (mrrl, rdl): 0.0623430759, 0.0004896241

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1163
rank avg (pred): 0.556 +- 0.242
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 1.9063e-06, 0.0011940887

Epoch over!
epoch time: 12.057

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 417
rank avg (pred): 0.567 +- 0.237
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.72125e-05, 0.0001737197

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 978
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.239, 0.302
batch losses (mrrl, rdl): 0.0396108553, 4.80461e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.291 +- 0.153
mrr vals (pred, true): 0.122, 0.001
batch losses (mrrl, rdl): 0.0514990389, 0.0007780284

Epoch over!
epoch time: 12.179

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.312 +- 0.160
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0338783897, 0.0008051627

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 855
rank avg (pred): 0.325 +- 0.167
mrr vals (pred, true): 0.100, 0.161
batch losses (mrrl, rdl): 0.0370915271, 0.0005887156

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 100
rank avg (pred): 0.475 +- 0.228
mrr vals (pred, true): 0.057, 0.032
batch losses (mrrl, rdl): 0.0004230952, 0.0016243265

Epoch over!
epoch time: 12.335

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 182
rank avg (pred): 0.462 +- 0.229
mrr vals (pred, true): 0.084, 0.001
batch losses (mrrl, rdl): 0.0117332861, 2.91569e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.514 +- 0.164
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 1.4706e-05, 0.0007429019

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 582
rank avg (pred): 0.496 +- 0.167
mrr vals (pred, true): 0.049, 0.008
batch losses (mrrl, rdl): 1.11279e-05, 0.0005614537

Epoch over!
epoch time: 11.919

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.449 +- 0.211
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 2.29457e-05, 2.94266e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.460 +- 0.160
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 7.8266e-06, 0.0005588972

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 102
rank avg (pred): 0.424 +- 0.183
mrr vals (pred, true): 0.051, 0.014
batch losses (mrrl, rdl): 3.1398e-06, 0.0008530167

Epoch over!
epoch time: 12.02

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.417 +- 0.185
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 6.07673e-05, 8.34582e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 876
rank avg (pred): 0.266 +- 0.145
mrr vals (pred, true): 0.112, 0.000
batch losses (mrrl, rdl): 0.0386916511, 0.0008666933

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1195
rank avg (pred): 0.397 +- 0.176
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 6.2162e-06, 0.000225651

Epoch over!
epoch time: 12.293

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1066
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.216, 0.310
batch losses (mrrl, rdl): 0.0885012671, 9.24086e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 144
rank avg (pred): 0.425 +- 0.139
mrr vals (pred, true): 0.048, 0.013
batch losses (mrrl, rdl): 2.99424e-05, 0.0007975596

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1072
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.202, 0.219
batch losses (mrrl, rdl): 0.0027178735, 0.000136592

Epoch over!
epoch time: 12.588

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 514
rank avg (pred): 0.424 +- 0.137
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.30395e-05, 0.0005751695

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 75
rank avg (pred): 0.286 +- 0.147
mrr vals (pred, true): 0.087, 0.100
batch losses (mrrl, rdl): 0.0134352054, 0.0008766095

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 697
rank avg (pred): 0.411 +- 0.132
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.23759e-05, 0.0001121216

Epoch over!
epoch time: 12.734

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.249 +- 0.135
mrr vals (pred, true): 0.091, 0.000
batch losses (mrrl, rdl): 0.017091807, 0.0013785204

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.402 +- 0.142
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 6.1676e-06, 9.74335e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 494
rank avg (pred): 0.366 +- 0.162
mrr vals (pred, true): 0.051, 0.081
batch losses (mrrl, rdl): 1.72567e-05, 0.0004463946

Epoch over!
epoch time: 14.099

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.337 +- 0.181
mrr vals (pred, true): 0.106, 0.141

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   86 	     0 	 0.11085 	 0.00019 	 MISS
   15 	     1 	 0.05052 	 0.00020 	 m..s
    2 	     2 	 0.04900 	 0.00020 	 m..s
   10 	     3 	 0.04975 	 0.00021 	 m..s
   29 	     4 	 0.05418 	 0.00021 	 m..s
   69 	     5 	 0.09683 	 0.00022 	 m..s
   91 	     6 	 0.11444 	 0.00023 	 MISS
   92 	     7 	 0.11487 	 0.00024 	 MISS
   20 	     8 	 0.05166 	 0.00024 	 m..s
    1 	     9 	 0.04900 	 0.00026 	 m..s
   39 	    10 	 0.05641 	 0.00026 	 m..s
   87 	    11 	 0.11109 	 0.00026 	 MISS
   14 	    12 	 0.05046 	 0.00027 	 m..s
   48 	    13 	 0.06600 	 0.00027 	 m..s
   37 	    14 	 0.05578 	 0.00028 	 m..s
   82 	    15 	 0.10873 	 0.00028 	 MISS
  110 	    16 	 0.16440 	 0.00029 	 MISS
   84 	    17 	 0.11006 	 0.00031 	 MISS
   13 	    18 	 0.05039 	 0.00033 	 m..s
   31 	    19 	 0.05427 	 0.00034 	 m..s
   96 	    20 	 0.12438 	 0.00035 	 MISS
   88 	    21 	 0.11233 	 0.00036 	 MISS
   77 	    22 	 0.10684 	 0.00036 	 MISS
    5 	    23 	 0.04928 	 0.00037 	 m..s
   96 	    24 	 0.12438 	 0.00038 	 MISS
   33 	    25 	 0.05525 	 0.00039 	 m..s
    8 	    26 	 0.04960 	 0.00042 	 m..s
   19 	    27 	 0.05128 	 0.00043 	 m..s
   59 	    28 	 0.07895 	 0.00046 	 m..s
   89 	    29 	 0.11406 	 0.00051 	 MISS
   34 	    30 	 0.05535 	 0.00051 	 m..s
  104 	    31 	 0.12612 	 0.00053 	 MISS
   60 	    32 	 0.07982 	 0.00053 	 m..s
   79 	    33 	 0.10720 	 0.00054 	 MISS
  109 	    34 	 0.14948 	 0.00055 	 MISS
   43 	    35 	 0.06433 	 0.00062 	 m..s
  103 	    36 	 0.12563 	 0.00080 	 MISS
   18 	    37 	 0.05084 	 0.00089 	 m..s
    3 	    38 	 0.04900 	 0.00157 	 m..s
    9 	    39 	 0.04970 	 0.00451 	 m..s
    0 	    40 	 0.04896 	 0.00451 	 m..s
    4 	    41 	 0.04901 	 0.00584 	 m..s
   32 	    42 	 0.05432 	 0.00597 	 m..s
   28 	    43 	 0.05407 	 0.00886 	 m..s
   21 	    44 	 0.05204 	 0.00932 	 m..s
   11 	    45 	 0.04980 	 0.01115 	 m..s
   12 	    46 	 0.05033 	 0.01139 	 m..s
   53 	    47 	 0.06957 	 0.01222 	 m..s
   36 	    48 	 0.05561 	 0.01381 	 m..s
   27 	    49 	 0.05383 	 0.01467 	 m..s
    7 	    50 	 0.04955 	 0.02136 	 ~...
   40 	    51 	 0.05871 	 0.02153 	 m..s
   17 	    52 	 0.05057 	 0.03464 	 ~...
   44 	    53 	 0.06459 	 0.03642 	 ~...
    6 	    54 	 0.04929 	 0.04198 	 ~...
   38 	    55 	 0.05589 	 0.04449 	 ~...
   16 	    56 	 0.05054 	 0.04797 	 ~...
   46 	    57 	 0.06544 	 0.04913 	 ~...
   47 	    58 	 0.06574 	 0.04971 	 ~...
   51 	    59 	 0.06858 	 0.05469 	 ~...
   22 	    60 	 0.05205 	 0.05687 	 ~...
   61 	    61 	 0.08130 	 0.05707 	 ~...
   24 	    62 	 0.05283 	 0.05722 	 ~...
   42 	    63 	 0.06216 	 0.05840 	 ~...
   41 	    64 	 0.05898 	 0.06079 	 ~...
   30 	    65 	 0.05425 	 0.06330 	 ~...
   58 	    66 	 0.07695 	 0.06345 	 ~...
   56 	    67 	 0.07382 	 0.06936 	 ~...
   25 	    68 	 0.05378 	 0.06984 	 ~...
   57 	    69 	 0.07462 	 0.07063 	 ~...
   62 	    70 	 0.08351 	 0.07101 	 ~...
   50 	    71 	 0.06855 	 0.07213 	 ~...
   45 	    72 	 0.06534 	 0.07299 	 ~...
   66 	    73 	 0.08912 	 0.07523 	 ~...
   64 	    74 	 0.08729 	 0.07602 	 ~...
   49 	    75 	 0.06623 	 0.07914 	 ~...
   23 	    76 	 0.05249 	 0.08016 	 ~...
   26 	    77 	 0.05383 	 0.08249 	 ~...
   54 	    78 	 0.07034 	 0.08501 	 ~...
   55 	    79 	 0.07059 	 0.08581 	 ~...
   67 	    80 	 0.09253 	 0.08617 	 ~...
   94 	    81 	 0.11685 	 0.08630 	 m..s
   68 	    82 	 0.09263 	 0.08722 	 ~...
   76 	    83 	 0.10658 	 0.08771 	 ~...
   75 	    84 	 0.10648 	 0.09591 	 ~...
  102 	    85 	 0.12527 	 0.10051 	 ~...
   70 	    86 	 0.09999 	 0.10106 	 ~...
   52 	    87 	 0.06940 	 0.11007 	 m..s
   72 	    88 	 0.10450 	 0.11015 	 ~...
  111 	    89 	 0.17562 	 0.11026 	 m..s
   65 	    90 	 0.08798 	 0.11194 	 ~...
   63 	    91 	 0.08535 	 0.11374 	 ~...
   35 	    92 	 0.05542 	 0.12473 	 m..s
  105 	    93 	 0.12947 	 0.12964 	 ~...
   74 	    94 	 0.10602 	 0.14133 	 m..s
   71 	    95 	 0.10261 	 0.14154 	 m..s
   73 	    96 	 0.10453 	 0.14257 	 m..s
   95 	    97 	 0.11850 	 0.14335 	 ~...
  113 	    98 	 0.19624 	 0.14882 	 m..s
   80 	    99 	 0.10751 	 0.14967 	 m..s
   78 	   100 	 0.10706 	 0.15056 	 m..s
   96 	   101 	 0.12438 	 0.15148 	 ~...
  107 	   102 	 0.13743 	 0.15351 	 ~...
   85 	   103 	 0.11020 	 0.15482 	 m..s
   83 	   104 	 0.10898 	 0.15753 	 m..s
   81 	   105 	 0.10819 	 0.15812 	 m..s
  101 	   106 	 0.12500 	 0.15935 	 m..s
   96 	   107 	 0.12438 	 0.16283 	 m..s
   90 	   108 	 0.11443 	 0.16715 	 m..s
  106 	   109 	 0.13093 	 0.17039 	 m..s
  100 	   110 	 0.12451 	 0.17132 	 m..s
  112 	   111 	 0.17938 	 0.17815 	 ~...
   93 	   112 	 0.11571 	 0.18732 	 m..s
  118 	   113 	 0.25980 	 0.19456 	 m..s
  108 	   114 	 0.14644 	 0.20735 	 m..s
  114 	   115 	 0.20376 	 0.21654 	 ~...
  115 	   116 	 0.23236 	 0.24817 	 ~...
  119 	   117 	 0.27395 	 0.26253 	 ~...
  117 	   118 	 0.23455 	 0.26518 	 m..s
  116 	   119 	 0.23420 	 0.29917 	 m..s
  120 	   120 	 0.28363 	 0.32792 	 m..s
==========================================
r_mrr = 0.7261719107627869
r2_mrr = 0.4141295552253723
spearmanr_mrr@5 = 0.917199969291687
spearmanr_mrr@10 = 0.9617353677749634
spearmanr_mrr@50 = 0.9450005292892456
spearmanr_mrr@100 = 0.9678613543510437
spearmanr_mrr@All = 0.9731591939926147
==========================================
test time: 0.411
Done Testing dataset DBpedia50
total time taken: 190.76481866836548
training time taken: 185.95160484313965
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7262)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4141)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9172)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9617)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9450)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9679)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9732)}}, 'test_loss': {'TransE': {'DBpedia50': 1.7930054638636648}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 3732072874912622
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [83, 176, 591, 121, 1054, 745, 993, 164, 461, 1061, 847, 194, 599, 436, 430, 362, 893, 1075, 848, 914, 1109, 119, 33, 1202, 846, 337, 1148, 629, 115, 509, 1071, 539, 311, 871, 1116, 153, 534, 936, 426, 257, 756, 37, 243, 887, 1151, 963, 958, 903, 374, 608, 308, 1070, 476, 175, 763, 899, 333, 760, 1145, 54, 603, 246, 794, 519, 796, 313, 635, 880, 56, 474, 6, 823, 829, 195, 1101, 1063, 878, 144, 790, 689, 475, 331, 815, 356, 435, 777, 491, 585, 497, 697, 465, 909, 1194, 464, 1083, 456, 989, 981, 412, 406, 930, 261, 172, 390, 378, 405, 90, 167, 384, 844, 233, 1181, 399, 531, 336, 758, 85, 163, 761, 286, 984]
valid_ids (0): []
train_ids (1094): [875, 62, 1066, 877, 741, 990, 417, 343, 1125, 929, 1014, 1123, 955, 403, 671, 1146, 801, 285, 791, 256, 792, 23, 684, 1019, 650, 35, 1126, 806, 386, 649, 938, 583, 563, 705, 87, 713, 92, 1105, 391, 607, 548, 573, 1168, 483, 707, 822, 12, 470, 965, 107, 1113, 303, 595, 1093, 126, 422, 641, 982, 493, 632, 22, 368, 67, 602, 1049, 340, 811, 578, 716, 288, 553, 111, 864, 881, 423, 214, 621, 1004, 654, 244, 189, 506, 554, 728, 751, 890, 110, 1047, 49, 407, 65, 701, 1144, 1154, 197, 98, 10, 1090, 351, 879, 326, 917, 1082, 415, 746, 648, 853, 510, 275, 320, 316, 322, 503, 365, 287, 652, 21, 865, 1053, 535, 136, 895, 1176, 278, 432, 201, 1128, 307, 885, 540, 1097, 622, 862, 1201, 112, 724, 170, 225, 185, 514, 805, 1196, 749, 886, 1078, 849, 729, 868, 342, 610, 279, 1058, 492, 84, 1163, 158, 1141, 488, 66, 908, 284, 447, 251, 772, 1080, 884, 904, 200, 557, 769, 755, 827, 81, 1212, 830, 147, 773, 1206, 670, 485, 413, 951, 562, 1189, 20, 105, 36, 738, 788, 859, 148, 262, 516, 224, 103, 732, 655, 481, 851, 935, 1012, 269, 1150, 690, 766, 370, 517, 924, 797, 236, 582, 4, 241, 734, 712, 381, 633, 80, 611, 1197, 344, 379, 874, 783, 1118, 99, 888, 957, 1167, 1110, 778, 559, 742, 793, 765, 397, 997, 387, 495, 994, 679, 389, 1041, 234, 180, 1074, 60, 845, 438, 710, 1121, 318, 151, 782, 525, 513, 623, 706, 544, 396, 134, 102, 979, 218, 42, 658, 94, 572, 926, 215, 202, 1002, 454, 346, 804, 714, 567, 424, 940, 267, 1022, 1065, 941, 187, 91, 717, 733, 678, 643, 928, 921, 1057, 1149, 32, 463, 767, 663, 31, 1133, 907, 479, 392, 905, 208, 293, 825, 1021, 77, 46, 837, 718, 58, 9, 486, 613, 520, 776, 645, 1205, 703, 1119, 532, 157, 677, 1036, 942, 1028, 1204, 211, 852, 1087, 821, 118, 814, 1214, 664, 524, 526, 143, 781, 694, 468, 1129, 931, 818, 274, 784, 489, 681, 1001, 372, 138, 108, 79, 128, 1084, 350, 1077, 660, 918, 740, 1011, 433, 832, 437, 297, 787, 428, 715, 268, 1031, 809, 150, 238, 15, 45, 161, 948, 836, 1102, 249, 76, 310, 956, 969, 1188, 100, 972, 722, 580, 527, 443, 1015, 579, 1136, 444, 1135, 89, 259, 181, 124, 7, 891, 523, 1076, 1023, 394, 575, 977, 358, 184, 473, 484, 600, 135, 114, 345, 371, 472, 744, 188, 642, 63, 634, 699, 619, 604, 26, 306, 348, 160, 858, 75, 1195, 1138, 298, 779, 537, 704, 248, 409, 449, 367, 518, 636, 292, 154, 1182, 727, 305, 589, 141, 2, 304, 70, 721, 925, 171, 1, 983, 383, 725, 496, 1186, 43, 330, 339, 651, 960, 1132, 68, 51, 477, 1112, 235, 975, 55, 731, 590, 228, 283, 120, 1172, 850, 726, 838, 876, 231, 263, 441, 624, 667, 301, 113, 500, 317, 177, 451, 946, 1127, 894, 762, 1098, 869, 460, 361, 452, 702, 295, 425, 1060, 616, 617, 291, 1020, 349, 687, 1104, 1055, 359, 676, 873, 536, 980, 166, 939, 369, 759, 352, 558, 270, 223, 471, 482, 584, 770, 834, 97, 934, 408, 14, 1180, 27, 1027, 1164, 419, 910, 1152, 854, 1140, 1016, 123, 1170, 179, 754, 954, 892, 596, 57, 1086, 82, 870, 1193, 668, 820, 1103, 469, 669, 817, 803, 637, 1203, 1029, 597, 711, 1190, 156, 1209, 686, 1174, 675, 656, 183, 101, 357, 467, 302, 199, 1183, 1072, 192, 819, 696, 962, 1124, 429, 1122, 618, 480, 866, 1159, 1198, 512, 137, 565, 324, 1211, 971, 1155, 511, 457, 528, 657, 533, 221, 780, 768, 842, 680, 896, 673, 321, 612, 59, 427, 142, 264, 872, 0, 568, 53, 901, 96, 1096, 626, 1179, 665, 190, 289, 117, 315, 764, 576, 882, 1046, 606, 1115, 1130, 1137, 753, 556, 366, 1120, 93, 266, 132, 1161, 219, 50, 808, 1067, 952, 216, 169, 442, 265, 462, 186, 1165, 355, 494, 395, 393, 335, 1039, 708, 639, 688, 205, 332, 230, 906, 341, 418, 564, 646, 478, 661, 440, 73, 1007, 691, 719, 198, 683, 282, 314, 593, 327, 798, 1013, 1114, 453, 191, 466, 529, 546, 674, 682, 382, 1079, 786, 1185, 541, 130, 571, 659, 843, 95, 976, 1160, 1037, 915, 71, 48, 598, 1009, 1038, 203, 182, 220, 226, 609, 168, 978, 44, 17, 968, 1095, 577, 61, 152, 252, 404, 1099, 334, 1200, 258, 961, 373, 106, 24, 434, 125, 1068, 775, 401, 855, 730, 1017, 1142, 146, 638, 363, 294, 19, 856, 949, 299, 222, 431, 698, 398, 995, 1005, 30, 757, 771, 992, 631, 319, 547, 1171, 1094, 5, 140, 947, 1035, 826, 416, 18, 1192, 933, 1073, 1143, 841, 1207, 986, 247, 347, 810, 227, 867, 700, 1089, 750, 239, 1106, 375, 421, 122, 1158, 1032, 490, 860, 210, 254, 34, 959, 1064, 807, 1059, 644, 902, 504, 1052, 897, 974, 1173, 1111, 196, 325, 709, 40, 816, 276, 88, 74, 64, 863, 28, 1162, 640, 991, 662, 155, 328, 988, 919, 666, 149, 920, 1033, 1048, 193, 912, 11, 605, 802, 385, 945, 414, 1157, 1026, 209, 104, 923, 109, 828, 795, 1006, 627, 999, 927, 739, 501, 1169, 1208, 857, 145, 970, 620, 883, 922, 237, 628, 439, 458, 785, 207, 515, 323, 647, 499, 950, 38, 1003, 566, 281, 581, 229, 833, 260, 543, 774, 1025, 1184, 1040, 507, 789, 300, 1091, 1024, 813, 1210, 212, 133, 521, 1050, 1177, 173, 1056, 1107, 653, 743, 13, 360, 551, 752, 296, 273, 388, 550, 290, 410, 1131, 129, 1000, 271, 39, 502, 380, 41, 800, 165, 735, 569, 420, 561, 450, 953, 1018, 1199, 987, 1117, 943, 522, 174, 1081, 1134, 25, 505, 402, 748, 3, 601, 720, 587, 549, 560, 861, 538, 250, 1139, 400, 376, 911, 630, 498, 985, 455, 240, 542, 998, 459, 162, 217, 116, 799, 206, 448, 232, 840, 329, 1043, 69, 615, 570, 1042, 139, 309, 354, 900, 1147, 487, 964, 52, 736, 1156, 545, 552, 1153, 1030, 625, 1051, 1008, 280, 508, 1213, 723, 824, 245, 16, 586, 835, 695, 672, 1085, 1088, 693, 1100, 78, 353, 692, 47, 932, 555, 204, 1034, 253, 812, 242, 1045, 1187, 446, 967, 29, 574, 839, 996, 1044, 445, 747, 685, 966, 588, 898, 159, 530, 213, 131, 277, 127, 614, 973, 594, 272, 592, 255, 8, 937, 889, 1108, 1010, 178, 916, 377, 737, 831, 913, 411, 944, 86, 338, 364, 1062, 1166, 1175, 1178, 1191, 1069, 1092, 312, 72]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4575526622048045
the save name prefix for this run is:  chkpt-ID_4575526622048045_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.545 +- 0.006
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002367467

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.380 +- 0.299
mrr vals (pred, true): 0.120, 0.072
batch losses (mrrl, rdl): 0.0, 0.0002110345

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.311 +- 0.278
mrr vals (pred, true): 0.282, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002791554

Epoch over!
epoch time: 12.274

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.098 +- 0.091
mrr vals (pred, true): 0.327, 0.093
batch losses (mrrl, rdl): 0.0, 2.55689e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.063 +- 0.060
mrr vals (pred, true): 0.356, 0.230
batch losses (mrrl, rdl): 0.0, 1.59726e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1166
rank avg (pred): 0.351 +- 0.320
mrr vals (pred, true): 0.272, 0.075
batch losses (mrrl, rdl): 0.0, 0.0001292763

Epoch over!
epoch time: 12.452

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.072 +- 0.068
mrr vals (pred, true): 0.388, 0.217
batch losses (mrrl, rdl): 0.0, 3.47046e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 950
rank avg (pred): 0.346 +- 0.317
mrr vals (pred, true): 0.312, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002201614

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 831
rank avg (pred): 0.079 +- 0.077
mrr vals (pred, true): 0.335, 0.203
batch losses (mrrl, rdl): 0.0, 2.93169e-05

Epoch over!
epoch time: 12.781

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1152
rank avg (pred): 0.187 +- 0.180
mrr vals (pred, true): 0.323, 0.096
batch losses (mrrl, rdl): 0.0, 7.83615e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.084 +- 0.081
mrr vals (pred, true): 0.350, 0.301
batch losses (mrrl, rdl): 0.0, 2.5989e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 793
rank avg (pred): 0.294 +- 0.284
mrr vals (pred, true): 0.361, 0.000
batch losses (mrrl, rdl): 0.0, 0.0006185703

Epoch over!
epoch time: 12.148

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1160
rank avg (pred): 0.181 +- 0.174
mrr vals (pred, true): 0.373, 0.105
batch losses (mrrl, rdl): 0.0, 4.28423e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 769
rank avg (pred): 0.320 +- 0.305
mrr vals (pred, true): 0.338, 0.146
batch losses (mrrl, rdl): 0.0, 0.000361835

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 344
rank avg (pred): 0.324 +- 0.307
mrr vals (pred, true): 0.324, 0.132
batch losses (mrrl, rdl): 0.0, 0.0005475976

Epoch over!
epoch time: 11.893

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.079 +- 0.078
mrr vals (pred, true): 0.413, 0.098
batch losses (mrrl, rdl): 1.3163746595, 1.89173e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 885
rank avg (pred): 0.204 +- 0.122
mrr vals (pred, true): 0.089, 0.002
batch losses (mrrl, rdl): 0.0154256206, 0.0016089346

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 67
rank avg (pred): 0.115 +- 0.063
mrr vals (pred, true): 0.081, 0.088
batch losses (mrrl, rdl): 0.0094272345, 5.48201e-05

Epoch over!
epoch time: 12.852

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 611
rank avg (pred): 0.321 +- 0.142
mrr vals (pred, true): 0.065, 0.024
batch losses (mrrl, rdl): 0.0021059853, 8.31937e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.153 +- 0.084
mrr vals (pred, true): 0.073, 0.128
batch losses (mrrl, rdl): 0.0297661051, 6.74258e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 31
rank avg (pred): 0.218 +- 0.118
mrr vals (pred, true): 0.088, 0.075
batch losses (mrrl, rdl): 0.0146691771, 0.0002572953

Epoch over!
epoch time: 13.612

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 952
rank avg (pred): 0.393 +- 0.178
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005110117, 8.8227e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 882
rank avg (pred): 0.302 +- 0.175
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.017939236, 0.0005941334

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 258
rank avg (pred): 0.241 +- 0.131
mrr vals (pred, true): 0.082, 0.086
batch losses (mrrl, rdl): 0.010139497, 0.0002986208

Epoch over!
epoch time: 13.316

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1042
rank avg (pred): 0.047 +- 0.029
mrr vals (pred, true): 0.123, 0.001
batch losses (mrrl, rdl): 0.0526486672, 0.0039858981

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.071 +- 0.047
mrr vals (pred, true): 0.120, 0.128
batch losses (mrrl, rdl): 0.0006033438, 4.41752e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.351 +- 0.131
mrr vals (pred, true): 0.055, 0.038
batch losses (mrrl, rdl): 0.0002196402, 0.0003552832

Epoch over!
epoch time: 12.253

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 235
rank avg (pred): 0.356 +- 0.138
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002791239, 0.0004623287

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 927
rank avg (pred): 0.442 +- 0.202
mrr vals (pred, true): 0.070, 0.163
batch losses (mrrl, rdl): 0.0868048668, 0.0010464349

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.398 +- 0.139
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0008036102, 0.0001456946

Epoch over!
epoch time: 13.383

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 193
rank avg (pred): 0.371 +- 0.141
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002044932, 0.0005159227

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 729
rank avg (pred): 0.019 +- 0.015
mrr vals (pred, true): 0.177, 0.073
batch losses (mrrl, rdl): 0.1621810347, 0.0002865329

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 285
rank avg (pred): 0.231 +- 0.124
mrr vals (pred, true): 0.084, 0.084
batch losses (mrrl, rdl): 0.011536614, 0.0002651478

Epoch over!
epoch time: 13.107

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.219 +- 0.129
mrr vals (pred, true): 0.083, 0.115
batch losses (mrrl, rdl): 0.010627687, 0.0002222158

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.217 +- 0.118
mrr vals (pred, true): 0.082, 0.089
batch losses (mrrl, rdl): 0.0100267567, 0.0002321055

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 273
rank avg (pred): 0.199 +- 0.100
mrr vals (pred, true): 0.074, 0.058
batch losses (mrrl, rdl): 0.0055596419, 0.0001225201

Epoch over!
epoch time: 13.152

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 684
rank avg (pred): 0.420 +- 0.147
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001960998, 0.0002478699

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 255
rank avg (pred): 0.194 +- 0.101
mrr vals (pred, true): 0.077, 0.096
batch losses (mrrl, rdl): 0.0074425484, 0.0001753296

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 499
rank avg (pred): 0.286 +- 0.114
mrr vals (pred, true): 0.051, 0.081
batch losses (mrrl, rdl): 4.0396e-06, 0.000169313

Epoch over!
epoch time: 13.206

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 470
rank avg (pred): 0.334 +- 0.127
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.00025869, 0.0005121569

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 216
rank avg (pred): 0.336 +- 0.117
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.40478e-05, 0.0005134679

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 342
rank avg (pred): 0.412 +- 0.156
mrr vals (pred, true): 0.064, 0.029
batch losses (mrrl, rdl): 0.0018448051, 0.0007449401

Epoch over!
epoch time: 13.47

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 443
rank avg (pred): 0.344 +- 0.138
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0034632725, 0.0006733778

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.259 +- 0.101
mrr vals (pred, true): 0.052, 0.062
batch losses (mrrl, rdl): 3.17375e-05, 9.32381e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.354 +- 0.131
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 6.0506e-06, 0.0003206102

Epoch over!
epoch time: 12.386

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.302 +- 0.115
mrr vals (pred, true): 0.055, 0.065

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   43 	     0 	 0.05336 	 0.00018 	 m..s
   12 	     1 	 0.05030 	 0.00019 	 m..s
    5 	     2 	 0.04961 	 0.00019 	 m..s
   84 	     3 	 0.08696 	 0.00020 	 m..s
   55 	     4 	 0.05847 	 0.00020 	 m..s
   41 	     5 	 0.05285 	 0.00021 	 m..s
   54 	     6 	 0.05663 	 0.00022 	 m..s
   53 	     7 	 0.05587 	 0.00024 	 m..s
   34 	     8 	 0.05165 	 0.00024 	 m..s
  105 	     9 	 0.11189 	 0.00024 	 MISS
   11 	    10 	 0.05019 	 0.00024 	 m..s
   14 	    11 	 0.05037 	 0.00025 	 m..s
   80 	    12 	 0.08450 	 0.00025 	 m..s
   81 	    13 	 0.08469 	 0.00025 	 m..s
   94 	    14 	 0.10332 	 0.00026 	 MISS
   30 	    15 	 0.05123 	 0.00026 	 m..s
   97 	    16 	 0.10467 	 0.00026 	 MISS
   98 	    17 	 0.10546 	 0.00027 	 MISS
   29 	    18 	 0.05106 	 0.00028 	 m..s
   32 	    19 	 0.05160 	 0.00032 	 m..s
    8 	    20 	 0.05005 	 0.00032 	 m..s
   47 	    21 	 0.05503 	 0.00033 	 m..s
   37 	    22 	 0.05240 	 0.00033 	 m..s
  100 	    23 	 0.10602 	 0.00035 	 MISS
   15 	    24 	 0.05042 	 0.00035 	 m..s
   31 	    25 	 0.05137 	 0.00036 	 m..s
   13 	    26 	 0.05032 	 0.00036 	 m..s
   61 	    27 	 0.06393 	 0.00038 	 m..s
   75 	    28 	 0.07994 	 0.00039 	 m..s
  104 	    29 	 0.11169 	 0.00040 	 MISS
   45 	    30 	 0.05390 	 0.00040 	 m..s
   46 	    31 	 0.05413 	 0.00042 	 m..s
   73 	    32 	 0.07977 	 0.00055 	 m..s
   38 	    33 	 0.05250 	 0.00062 	 m..s
   50 	    34 	 0.05514 	 0.00088 	 m..s
   19 	    35 	 0.05052 	 0.00100 	 m..s
    2 	    36 	 0.04948 	 0.00102 	 m..s
   57 	    37 	 0.05936 	 0.00153 	 m..s
    0 	    38 	 0.04904 	 0.00339 	 m..s
   22 	    39 	 0.05071 	 0.00367 	 m..s
    4 	    40 	 0.04955 	 0.00530 	 m..s
    1 	    41 	 0.04931 	 0.01251 	 m..s
   25 	    42 	 0.05083 	 0.01256 	 m..s
    7 	    43 	 0.04999 	 0.01610 	 m..s
   10 	    44 	 0.05012 	 0.01678 	 m..s
   24 	    45 	 0.05072 	 0.01695 	 m..s
   33 	    46 	 0.05165 	 0.02020 	 m..s
   27 	    47 	 0.05096 	 0.02036 	 m..s
   21 	    48 	 0.05070 	 0.02069 	 m..s
   28 	    49 	 0.05103 	 0.02818 	 ~...
   20 	    50 	 0.05057 	 0.03064 	 ~...
   16 	    51 	 0.05042 	 0.03155 	 ~...
   26 	    52 	 0.05087 	 0.03325 	 ~...
   18 	    53 	 0.05047 	 0.03425 	 ~...
   49 	    54 	 0.05505 	 0.03642 	 ~...
   23 	    55 	 0.05071 	 0.03931 	 ~...
   17 	    56 	 0.05044 	 0.04198 	 ~...
    9 	    57 	 0.05008 	 0.04262 	 ~...
   56 	    58 	 0.05898 	 0.04511 	 ~...
    6 	    59 	 0.04990 	 0.05021 	 ~...
   58 	    60 	 0.06196 	 0.05450 	 ~...
   35 	    61 	 0.05201 	 0.05453 	 ~...
   39 	    62 	 0.05255 	 0.05599 	 ~...
   40 	    63 	 0.05280 	 0.05605 	 ~...
    3 	    64 	 0.04948 	 0.05842 	 ~...
   51 	    65 	 0.05516 	 0.06476 	 ~...
   48 	    66 	 0.05505 	 0.07108 	 ~...
   70 	    67 	 0.07656 	 0.07602 	 ~...
   60 	    68 	 0.06265 	 0.07774 	 ~...
   79 	    69 	 0.08433 	 0.08498 	 ~...
   89 	    70 	 0.09454 	 0.08564 	 ~...
   36 	    71 	 0.05221 	 0.08693 	 m..s
   72 	    72 	 0.07932 	 0.08771 	 ~...
   78 	    73 	 0.08017 	 0.08798 	 ~...
   85 	    74 	 0.08727 	 0.08805 	 ~...
   77 	    75 	 0.08009 	 0.09158 	 ~...
   59 	    76 	 0.06222 	 0.09210 	 ~...
   71 	    77 	 0.07684 	 0.09392 	 ~...
   52 	    78 	 0.05570 	 0.09486 	 m..s
   92 	    79 	 0.09836 	 0.09525 	 ~...
   93 	    80 	 0.09959 	 0.09594 	 ~...
   64 	    81 	 0.07093 	 0.09751 	 ~...
   42 	    82 	 0.05299 	 0.09758 	 m..s
   90 	    83 	 0.09696 	 0.09867 	 ~...
   87 	    84 	 0.08831 	 0.09949 	 ~...
   63 	    85 	 0.06718 	 0.10197 	 m..s
   88 	    86 	 0.09279 	 0.10651 	 ~...
   74 	    87 	 0.07993 	 0.10827 	 ~...
   91 	    88 	 0.09808 	 0.11095 	 ~...
   62 	    89 	 0.06700 	 0.11374 	 m..s
   76 	    90 	 0.08004 	 0.11379 	 m..s
   95 	    91 	 0.10392 	 0.12979 	 ~...
   65 	    92 	 0.07135 	 0.13458 	 m..s
   44 	    93 	 0.05357 	 0.13857 	 m..s
  108 	    94 	 0.14955 	 0.14189 	 ~...
  106 	    95 	 0.11858 	 0.14530 	 ~...
   66 	    96 	 0.07507 	 0.14672 	 m..s
  103 	    97 	 0.10997 	 0.14879 	 m..s
   99 	    98 	 0.10574 	 0.15452 	 m..s
   67 	    99 	 0.07518 	 0.15654 	 m..s
   83 	   100 	 0.08548 	 0.16084 	 m..s
   96 	   101 	 0.10412 	 0.16173 	 m..s
   82 	   102 	 0.08480 	 0.16715 	 m..s
   69 	   103 	 0.07568 	 0.16914 	 m..s
   68 	   104 	 0.07553 	 0.17079 	 m..s
  101 	   105 	 0.10847 	 0.17849 	 m..s
   86 	   106 	 0.08786 	 0.18254 	 m..s
  112 	   107 	 0.23201 	 0.18863 	 m..s
  109 	   108 	 0.16011 	 0.19286 	 m..s
  102 	   109 	 0.10867 	 0.20077 	 m..s
  113 	   110 	 0.23834 	 0.20501 	 m..s
  110 	   111 	 0.21470 	 0.20969 	 ~...
  107 	   112 	 0.13577 	 0.21520 	 m..s
  111 	   113 	 0.22804 	 0.21557 	 ~...
  115 	   114 	 0.24863 	 0.21786 	 m..s
  116 	   115 	 0.26558 	 0.26695 	 ~...
  114 	   116 	 0.24148 	 0.27160 	 m..s
  117 	   117 	 0.26571 	 0.28659 	 ~...
  120 	   118 	 0.33505 	 0.29184 	 m..s
  119 	   119 	 0.33088 	 0.30290 	 ~...
  118 	   120 	 0.30072 	 0.32098 	 ~...
==========================================
r_mrr = 0.8018715381622314
r2_mrr = 0.6168742179870605
spearmanr_mrr@5 = 0.901708722114563
spearmanr_mrr@10 = 0.9079580903053284
spearmanr_mrr@50 = 0.9435909986495972
spearmanr_mrr@100 = 0.9136683940887451
spearmanr_mrr@All = 0.9141528606414795
==========================================
test time: 0.455
Done Testing dataset DBpedia50
total time taken: 198.9070839881897
training time taken: 192.82264971733093
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.8019)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.6169)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9017)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9080)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9436)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9137)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9142)}}, 'test_loss': {'TransE': {'DBpedia50': 1.538793495590653}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9307693417040594
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [117, 144, 1031, 1175, 455, 620, 352, 656, 737, 1129, 621, 540, 187, 790, 207, 809, 200, 675, 583, 1141, 186, 59, 211, 764, 416, 580, 1016, 14, 309, 506, 575, 626, 348, 9, 308, 770, 692, 42, 100, 678, 1065, 58, 490, 978, 461, 861, 337, 1027, 389, 820, 182, 629, 1144, 172, 424, 746, 945, 450, 953, 1102, 409, 791, 276, 62, 771, 273, 1049, 748, 1214, 1012, 862, 230, 520, 80, 1083, 990, 741, 1195, 1110, 195, 599, 381, 1077, 690, 439, 509, 388, 43, 333, 1101, 427, 848, 468, 1106, 932, 283, 923, 1021, 518, 435, 1191, 717, 394, 776, 54, 218, 78, 357, 395, 638, 1038, 217, 361, 478, 342, 341, 899, 174, 634, 683, 879]
valid_ids (0): []
train_ids (1094): [365, 827, 536, 71, 1114, 568, 609, 646, 11, 208, 481, 527, 456, 364, 666, 48, 1075, 153, 1043, 467, 814, 966, 938, 1179, 766, 561, 893, 147, 397, 137, 851, 566, 1131, 784, 1143, 617, 476, 782, 630, 661, 242, 929, 193, 1168, 65, 702, 511, 266, 1078, 607, 167, 586, 134, 808, 574, 832, 980, 697, 53, 1189, 1163, 1176, 1004, 963, 1182, 1067, 767, 448, 559, 724, 719, 838, 1007, 331, 943, 1030, 460, 500, 95, 407, 285, 290, 1089, 613, 300, 1044, 996, 973, 432, 886, 46, 839, 898, 852, 89, 1, 37, 408, 265, 974, 659, 769, 649, 1162, 327, 0, 1178, 805, 61, 225, 299, 1093, 155, 731, 1158, 521, 119, 437, 73, 594, 40, 1164, 142, 184, 1068, 856, 160, 720, 426, 721, 870, 271, 84, 760, 420, 335, 112, 1040, 732, 1050, 105, 837, 1074, 1213, 569, 336, 474, 751, 595, 457, 83, 314, 136, 952, 543, 651, 170, 1145, 874, 362, 123, 270, 818, 698, 565, 171, 1169, 502, 191, 246, 608, 1111, 637, 747, 522, 235, 558, 591, 849, 961, 1015, 56, 356, 722, 765, 18, 533, 835, 1193, 693, 802, 1133, 260, 900, 1206, 404, 1198, 446, 431, 992, 101, 801, 664, 910, 685, 1186, 998, 34, 32, 330, 366, 1072, 981, 325, 1104, 542, 178, 1196, 91, 1156, 45, 21, 673, 503, 470, 148, 994, 600, 190, 282, 858, 882, 449, 736, 645, 383, 27, 743, 627, 359, 438, 1070, 949, 551, 884, 1091, 510, 584, 247, 635, 24, 1076, 169, 578, 530, 227, 881, 799, 77, 162, 892, 368, 680, 322, 1146, 173, 164, 585, 433, 31, 1194, 794, 12, 1167, 991, 968, 87, 201, 491, 909, 412, 1166, 103, 97, 988, 582, 124, 922, 85, 632, 234, 405, 387, 777, 1032, 81, 399, 129, 1095, 1002, 597, 26, 122, 108, 1154, 1029, 1203, 339, 369, 867, 1035, 815, 278, 1197, 550, 819, 1121, 419, 50, 755, 749, 1116, 168, 1036, 555, 291, 706, 256, 1138, 1105, 414, 705, 219, 588, 343, 1118, 871, 710, 323, 13, 400, 1066, 338, 951, 311, 773, 854, 25, 66, 1071, 1019, 654, 1052, 798, 789, 263, 250, 917, 98, 465, 347, 1041, 507, 220, 708, 524, 442, 657, 891, 829, 203, 557, 403, 462, 681, 593, 386, 640, 1153, 964, 216, 684, 236, 935, 44, 340, 847, 213, 320, 307, 623, 869, 535, 28, 484, 872, 1113, 401, 729, 942, 233, 483, 677, 545, 243, 957, 810, 920, 804, 590, 324, 619, 1053, 1039, 228, 79, 596, 775, 662, 614, 1047, 946, 289, 113, 284, 612, 1125, 486, 49, 750, 1045, 668, 865, 644, 713, 926, 1211, 262, 1207, 57, 1201, 259, 948, 378, 570, 3, 598, 1134, 372, 127, 477, 1069, 116, 552, 223, 679, 908, 1185, 180, 384, 280, 1064, 444, 1155, 778, 275, 728, 60, 1092, 374, 529, 106, 703, 237, 96, 1199, 984, 822, 498, 989, 204, 35, 496, 759, 1088, 785, 650, 255, 346, 1120, 979, 377, 700, 655, 226, 985, 264, 1202, 699, 221, 671, 183, 1003, 198, 757, 315, 94, 241, 526, 367, 986, 1024, 1180, 826, 257, 140, 1136, 466, 911, 1037, 133, 1119, 1160, 660, 146, 316, 501, 279, 1152, 398, 936, 454, 135, 1073, 74, 306, 744, 1161, 855, 143, 993, 505, 1126, 312, 209, 453, 512, 956, 252, 413, 850, 761, 1135, 418, 913, 1117, 930, 92, 762, 915, 954, 360, 33, 197, 912, 834, 69, 1082, 725, 1063, 1097, 523, 674, 326, 166, 796, 430, 1107, 179, 298, 712, 756, 1025, 240, 504, 1017, 110, 396, 232, 1151, 379, 1057, 1165, 828, 669, 642, 977, 30, 303, 473, 1132, 845, 292, 695, 788, 628, 130, 317, 286, 962, 1139, 689, 139, 495, 423, 441, 254, 534, 159, 763, 1183, 803, 625, 181, 321, 853, 382, 1046, 17, 592, 924, 452, 406, 866, 999, 1042, 329, 488, 349, 1013, 1142, 258, 554, 294, 469, 1190, 745, 417, 863, 971, 873, 742, 206, 6, 676, 145, 643, 1188, 987, 480, 843, 824, 658, 781, 792, 916, 1200, 903, 967, 562, 287, 72, 519, 905, 1140, 813, 563, 840, 716, 188, 354, 1100, 1096, 29, 281, 102, 633, 571, 603, 1115, 1184, 780, 1130, 177, 268, 616, 1205, 128, 231, 548, 222, 370, 1137, 959, 1010, 768, 261, 459, 927, 772, 376, 375, 70, 515, 5, 1212, 895, 734, 1056, 463, 23, 269, 549, 831, 1210, 422, 7, 738, 154, 860, 618, 880, 138, 560, 20, 121, 955, 194, 39, 471, 931, 199, 189, 688, 429, 573, 727, 149, 421, 447, 411, 272, 983, 475, 544, 670, 876, 889, 440, 754, 914, 902, 758, 1085, 267, 812, 686, 508, 975, 1098, 328, 525, 950, 156, 1127, 779, 36, 251, 38, 783, 1173, 157, 940, 513, 126, 109, 682, 528, 553, 494, 825, 1094, 752, 1014, 539, 1108, 941, 1005, 648, 132, 64, 878, 921, 410, 19, 577, 380, 636, 93, 10, 332, 907, 1001, 479, 868, 1081, 472, 274, 1122, 833, 897, 1026, 937, 482, 497, 52, 1062, 492, 925, 538, 487, 687, 972, 353, 841, 224, 514, 906, 1000, 464, 718, 701, 704, 589, 141, 857, 176, 894, 1061, 391, 517, 939, 402, 205, 1124, 295, 358, 1192, 1059, 786, 76, 249, 229, 68, 672, 22, 816, 615, 443, 390, 118, 733, 919, 214, 1008, 665, 175, 890, 150, 373, 795, 541, 537, 753, 711, 896, 114, 653, 392, 104, 1034, 774, 2, 960, 901, 499, 944, 238, 807, 210, 1147, 1009, 904, 709, 844, 1051, 1187, 302, 192, 730, 1159, 288, 934, 1055, 877, 875, 1208, 248, 385, 107, 1099, 99, 239, 152, 564, 1209, 605, 445, 1090, 161, 696, 1174, 86, 41, 434, 610, 304, 1033, 918, 277, 1123, 888, 115, 305, 715, 516, 1181, 67, 1028, 567, 547, 997, 726, 624, 811, 691, 556, 344, 319, 371, 579, 485, 800, 318, 714, 165, 821, 393, 120, 842, 8, 297, 546, 125, 196, 75, 928, 245, 885, 576, 723, 707, 458, 16, 735, 363, 622, 965, 531, 253, 1006, 334, 1080, 995, 958, 1149, 587, 163, 976, 51, 667, 694, 641, 1204, 887, 131, 1157, 296, 451, 350, 493, 355, 55, 293, 606, 1171, 47, 1172, 740, 947, 572, 830, 90, 969, 1112, 4, 602, 82, 601, 817, 611, 1022, 806, 647, 215, 1058, 1020, 158, 836, 864, 1170, 982, 1048, 604, 1103, 652, 1054, 1148, 1150, 883, 425, 351, 15, 489, 797, 185, 202, 1018, 111, 1084, 631, 1109, 859, 532, 787, 415, 1177, 212, 1128, 846, 1023, 244, 1011, 933, 1086, 1060, 428, 1087, 345, 63, 88, 663, 1079, 310, 581, 739, 151, 436, 970, 301, 793, 823, 313, 639]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7228702395880010
the save name prefix for this run is:  chkpt-ID_7228702395880010_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 141
rank avg (pred): 0.508 +- 0.006
mrr vals (pred, true): 0.000, 0.011
batch losses (mrrl, rdl): 0.0, 0.0013860529

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.323 +- 0.203
mrr vals (pred, true): 0.180, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003238023

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 957
rank avg (pred): 0.347 +- 0.239
mrr vals (pred, true): 0.199, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002379768

Epoch over!
epoch time: 13.333

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1145
rank avg (pred): 0.193 +- 0.134
mrr vals (pred, true): 0.222, 0.094
batch losses (mrrl, rdl): 0.0, 9.82687e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.334 +- 0.265
mrr vals (pred, true): 0.268, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001271844

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 943
rank avg (pred): 0.334 +- 0.279
mrr vals (pred, true): 0.317, 0.148
batch losses (mrrl, rdl): 0.0, 0.0004909506

Epoch over!
epoch time: 13.573

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 4
rank avg (pred): 0.110 +- 0.096
mrr vals (pred, true): 0.377, 0.100
batch losses (mrrl, rdl): 0.0, 2.68502e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.308 +- 0.285
mrr vals (pred, true): 0.389, 0.013
batch losses (mrrl, rdl): 0.0, 0.0001637126

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 516
rank avg (pred): 0.280 +- 0.256
mrr vals (pred, true): 0.367, 0.021
batch losses (mrrl, rdl): 0.0, 3.16712e-05

Epoch over!
epoch time: 12.196

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 913
rank avg (pred): 0.173 +- 0.159
mrr vals (pred, true): 0.391, 0.148
batch losses (mrrl, rdl): 0.0, 0.0001518066

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.313 +- 0.293
mrr vals (pred, true): 0.405, 0.001
batch losses (mrrl, rdl): 0.0, 0.0005232233

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 460
rank avg (pred): 0.320 +- 0.301
mrr vals (pred, true): 0.388, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002381715

Epoch over!
epoch time: 12.644

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 166
rank avg (pred): 0.324 +- 0.302
mrr vals (pred, true): 0.380, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003483974

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 233
rank avg (pred): 0.319 +- 0.303
mrr vals (pred, true): 0.405, 0.000
batch losses (mrrl, rdl): 0.0, 0.000331452

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.315 +- 0.301
mrr vals (pred, true): 0.396, 0.000
batch losses (mrrl, rdl): 0.0, 0.000265044

Epoch over!
epoch time: 12.73

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 957
rank avg (pred): 0.325 +- 0.304
mrr vals (pred, true): 0.373, 0.000
batch losses (mrrl, rdl): 1.0421910286, 0.0002563357

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.029 +- 0.016
mrr vals (pred, true): 0.144, 0.229
batch losses (mrrl, rdl): 0.0724856406, 2.15713e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.311 +- 0.195
mrr vals (pred, true): 0.067, 0.043
batch losses (mrrl, rdl): 0.0027762069, 7.48523e-05

Epoch over!
epoch time: 12.311

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 180
rank avg (pred): 0.309 +- 0.200
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0060916385, 0.0004874548

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.145, 0.000
batch losses (mrrl, rdl): 0.089521192, 0.0041259527

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 850
rank avg (pred): 0.112 +- 0.074
mrr vals (pred, true): 0.076, 0.160
batch losses (mrrl, rdl): 0.0706784874, 0.0002803366

Epoch over!
epoch time: 12.881

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 493
rank avg (pred): 0.437 +- 0.244
mrr vals (pred, true): 0.057, 0.057
batch losses (mrrl, rdl): 0.000436776, 0.0007724861

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 586
rank avg (pred): 0.505 +- 0.255
mrr vals (pred, true): 0.039, 0.011
batch losses (mrrl, rdl): 0.0013009752, 0.0008173193

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1131
rank avg (pred): 0.053 +- 0.035
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0192998424, 0.0033196022

Epoch over!
epoch time: 13.131

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 444
rank avg (pred): 0.454 +- 0.268
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0029572942, 1.63009e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 972
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.235, 0.220
batch losses (mrrl, rdl): 0.001973131, 8.88195e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 956
rank avg (pred): 0.241 +- 0.158
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0027049144, 0.0010293224

Epoch over!
epoch time: 12.489

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.434 +- 0.260
mrr vals (pred, true): 0.063, 0.008
batch losses (mrrl, rdl): 0.0017802756, 0.0005771831

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 253
rank avg (pred): 0.067 +- 0.053
mrr vals (pred, true): 0.106, 0.181
batch losses (mrrl, rdl): 0.0571826659, 3.07459e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.016 +- 0.010
mrr vals (pred, true): 0.131, 0.229
batch losses (mrrl, rdl): 0.0963843018, 3.77829e-05

Epoch over!
epoch time: 13.32

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1151
rank avg (pred): 0.030 +- 0.020
mrr vals (pred, true): 0.099, 0.099
batch losses (mrrl, rdl): 0.0238131005, 0.0006349812

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.020 +- 0.015
mrr vals (pred, true): 0.180, 0.098
batch losses (mrrl, rdl): 0.1696667969, 0.0001784958

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.269 +- 0.163
mrr vals (pred, true): 0.067, 0.114
batch losses (mrrl, rdl): 0.0215694327, 7.5979e-05

Epoch over!
epoch time: 12.656

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 434
rank avg (pred): 0.343 +- 0.209
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009819258, 0.0003800663

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.110 +- 0.073
mrr vals (pred, true): 0.074, 0.106
batch losses (mrrl, rdl): 0.0100580566, 3.70948e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 826
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.215, 0.230
batch losses (mrrl, rdl): 0.002380312, 0.0001090067

Epoch over!
epoch time: 12.593

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.155 +- 0.098
mrr vals (pred, true): 0.076, 0.134
batch losses (mrrl, rdl): 0.0329160392, 9.04763e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.417 +- 0.252
mrr vals (pred, true): 0.072, 0.000
batch losses (mrrl, rdl): 0.0047564097, 7.20009e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 933
rank avg (pred): 0.370 +- 0.219
mrr vals (pred, true): 0.070, 0.144
batch losses (mrrl, rdl): 0.0549618825, 0.0005877349

Epoch over!
epoch time: 12.222

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.294 +- 0.189
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.010541915, 0.000711261

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1157
rank avg (pred): 0.118 +- 0.071
mrr vals (pred, true): 0.082, 0.102
batch losses (mrrl, rdl): 0.0036802073, 0.0003001059

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 752
rank avg (pred): 0.019 +- 0.012
mrr vals (pred, true): 0.184, 0.203
batch losses (mrrl, rdl): 0.0035237703, 0.0001035898

Epoch over!
epoch time: 12.473

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 111
rank avg (pred): 0.475 +- 0.210
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001510446, 0.0008439986

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 410
rank avg (pred): 0.384 +- 0.196
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0011572121, 0.0001762919

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 189
rank avg (pred): 0.438 +- 0.227
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0023010508, 3.88039e-05

Epoch over!
epoch time: 12.94

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.439 +- 0.221
mrr vals (pred, true): 0.059, 0.020

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   64 	     0 	 0.07130 	 0.00014 	 m..s
   67 	     1 	 0.07428 	 0.00017 	 m..s
   88 	     2 	 0.09329 	 0.00019 	 m..s
   25 	     3 	 0.05913 	 0.00019 	 m..s
   46 	     4 	 0.06633 	 0.00020 	 m..s
   63 	     5 	 0.07118 	 0.00022 	 m..s
   36 	     6 	 0.06142 	 0.00022 	 m..s
   85 	     7 	 0.08986 	 0.00024 	 m..s
   30 	     8 	 0.05977 	 0.00024 	 m..s
   22 	     9 	 0.05896 	 0.00024 	 m..s
    9 	    10 	 0.04886 	 0.00025 	 m..s
   58 	    11 	 0.06967 	 0.00026 	 m..s
  103 	    12 	 0.12943 	 0.00026 	 MISS
   99 	    13 	 0.11265 	 0.00026 	 MISS
   52 	    14 	 0.06747 	 0.00027 	 m..s
   40 	    15 	 0.06254 	 0.00028 	 m..s
    8 	    16 	 0.04848 	 0.00028 	 m..s
   45 	    17 	 0.06626 	 0.00028 	 m..s
   56 	    18 	 0.06852 	 0.00028 	 m..s
   34 	    19 	 0.06037 	 0.00028 	 m..s
   94 	    20 	 0.10794 	 0.00029 	 MISS
   48 	    21 	 0.06691 	 0.00029 	 m..s
   91 	    22 	 0.09419 	 0.00030 	 m..s
   11 	    23 	 0.05028 	 0.00030 	 m..s
   74 	    24 	 0.07797 	 0.00032 	 m..s
   23 	    25 	 0.05909 	 0.00033 	 m..s
   39 	    26 	 0.06184 	 0.00033 	 m..s
   38 	    27 	 0.06167 	 0.00034 	 m..s
   74 	    28 	 0.07797 	 0.00036 	 m..s
   95 	    29 	 0.10907 	 0.00036 	 MISS
   78 	    30 	 0.07962 	 0.00037 	 m..s
   16 	    31 	 0.05796 	 0.00039 	 m..s
   32 	    32 	 0.05984 	 0.00039 	 m..s
   12 	    33 	 0.05040 	 0.00040 	 m..s
   62 	    34 	 0.07071 	 0.00042 	 m..s
   17 	    35 	 0.05797 	 0.00050 	 m..s
    3 	    36 	 0.04566 	 0.00053 	 m..s
   80 	    37 	 0.07990 	 0.00055 	 m..s
   37 	    38 	 0.06164 	 0.00073 	 m..s
   35 	    39 	 0.06039 	 0.00073 	 m..s
   84 	    40 	 0.08760 	 0.00087 	 m..s
   61 	    41 	 0.07063 	 0.00090 	 m..s
    5 	    42 	 0.04613 	 0.00096 	 m..s
    7 	    43 	 0.04844 	 0.00194 	 m..s
   66 	    44 	 0.07149 	 0.00203 	 m..s
  105 	    45 	 0.13098 	 0.00226 	 MISS
   33 	    46 	 0.05994 	 0.00394 	 m..s
    6 	    47 	 0.04786 	 0.00871 	 m..s
   29 	    48 	 0.05933 	 0.01115 	 m..s
   26 	    49 	 0.05916 	 0.01251 	 m..s
    2 	    50 	 0.04562 	 0.02020 	 ~...
   28 	    51 	 0.05924 	 0.02032 	 m..s
    0 	    52 	 0.04439 	 0.02078 	 ~...
   18 	    53 	 0.05803 	 0.02183 	 m..s
   59 	    54 	 0.06968 	 0.02906 	 m..s
    4 	    55 	 0.04608 	 0.03172 	 ~...
   55 	    56 	 0.06840 	 0.03184 	 m..s
   50 	    57 	 0.06741 	 0.03325 	 m..s
    1 	    58 	 0.04454 	 0.03425 	 ~...
   10 	    59 	 0.04943 	 0.03464 	 ~...
   15 	    60 	 0.05773 	 0.03653 	 ~...
   31 	    61 	 0.05980 	 0.03716 	 ~...
   65 	    62 	 0.07137 	 0.03737 	 m..s
   49 	    63 	 0.06733 	 0.04511 	 ~...
   53 	    64 	 0.06760 	 0.04998 	 ~...
   24 	    65 	 0.05910 	 0.05078 	 ~...
   14 	    66 	 0.05705 	 0.05391 	 ~...
   51 	    67 	 0.06742 	 0.05605 	 ~...
   20 	    68 	 0.05830 	 0.05687 	 ~...
   47 	    69 	 0.06634 	 0.05840 	 ~...
   42 	    70 	 0.06373 	 0.05919 	 ~...
   21 	    71 	 0.05839 	 0.06283 	 ~...
   19 	    72 	 0.05812 	 0.06581 	 ~...
   60 	    73 	 0.07041 	 0.06731 	 ~...
   57 	    74 	 0.06894 	 0.06856 	 ~...
   54 	    75 	 0.06830 	 0.06977 	 ~...
   27 	    76 	 0.05919 	 0.07218 	 ~...
   43 	    77 	 0.06471 	 0.07299 	 ~...
   81 	    78 	 0.08374 	 0.08153 	 ~...
   13 	    79 	 0.05606 	 0.08492 	 ~...
  101 	    80 	 0.11864 	 0.08537 	 m..s
   73 	    81 	 0.07703 	 0.09275 	 ~...
   90 	    82 	 0.09396 	 0.09285 	 ~...
   72 	    83 	 0.07653 	 0.09472 	 ~...
   68 	    84 	 0.07438 	 0.09562 	 ~...
   71 	    85 	 0.07603 	 0.09751 	 ~...
   93 	    86 	 0.09797 	 0.10123 	 ~...
   74 	    87 	 0.07797 	 0.10651 	 ~...
   96 	    88 	 0.10967 	 0.10806 	 ~...
   70 	    89 	 0.07521 	 0.10944 	 m..s
   98 	    90 	 0.11100 	 0.11095 	 ~...
   69 	    91 	 0.07448 	 0.11194 	 m..s
   82 	    92 	 0.08508 	 0.11742 	 m..s
   79 	    93 	 0.07969 	 0.12008 	 m..s
   44 	    94 	 0.06489 	 0.12782 	 m..s
  115 	    95 	 0.19205 	 0.14115 	 m..s
   41 	    96 	 0.06362 	 0.14443 	 m..s
   74 	    97 	 0.07797 	 0.15126 	 m..s
  108 	    98 	 0.13264 	 0.15902 	 ~...
   83 	    99 	 0.08716 	 0.15968 	 m..s
  113 	   100 	 0.18766 	 0.16166 	 ~...
  107 	   101 	 0.13133 	 0.16173 	 m..s
   87 	   102 	 0.09119 	 0.16715 	 m..s
  106 	   103 	 0.13129 	 0.16959 	 m..s
  104 	   104 	 0.13090 	 0.16968 	 m..s
   92 	   105 	 0.09488 	 0.17039 	 m..s
  109 	   106 	 0.13507 	 0.17325 	 m..s
  110 	   107 	 0.13649 	 0.17400 	 m..s
  111 	   108 	 0.14839 	 0.17815 	 ~...
   89 	   109 	 0.09388 	 0.18254 	 m..s
  112 	   110 	 0.15703 	 0.18661 	 ~...
   86 	   111 	 0.09109 	 0.19066 	 m..s
  102 	   112 	 0.12146 	 0.19233 	 m..s
   97 	   113 	 0.11017 	 0.20077 	 m..s
  100 	   114 	 0.11568 	 0.20735 	 m..s
  114 	   115 	 0.19186 	 0.21684 	 ~...
  116 	   116 	 0.20146 	 0.22181 	 ~...
  117 	   117 	 0.21548 	 0.22405 	 ~...
  118 	   118 	 0.21613 	 0.28702 	 m..s
  120 	   119 	 0.27103 	 0.30234 	 m..s
  119 	   120 	 0.25089 	 0.30943 	 m..s
==========================================
r_mrr = 0.7633246779441833
r2_mrr = 0.47833991050720215
spearmanr_mrr@5 = 0.8312692642211914
spearmanr_mrr@10 = 0.9110487699508667
spearmanr_mrr@50 = 0.9594133496284485
spearmanr_mrr@100 = 0.9382949471473694
spearmanr_mrr@All = 0.9458253979682922
==========================================
test time: 0.393
Done Testing dataset DBpedia50
total time taken: 196.81759905815125
training time taken: 191.9705047607422
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7633)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4783)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8313)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9110)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9594)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9383)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9458)}}, 'test_loss': {'TransE': {'DBpedia50': 1.63273074596691}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 3921256419589720
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [399, 574, 506, 1103, 115, 858, 104, 1193, 167, 1017, 1001, 829, 631, 1005, 759, 1119, 211, 65, 670, 408, 1052, 43, 668, 12, 559, 720, 933, 589, 1135, 620, 490, 347, 8, 555, 313, 921, 1025, 711, 1123, 233, 214, 661, 40, 163, 1162, 449, 1107, 249, 545, 782, 866, 511, 1164, 689, 873, 862, 430, 524, 786, 883, 173, 746, 798, 1099, 1077, 371, 121, 142, 277, 113, 429, 865, 391, 1140, 1079, 475, 1171, 788, 634, 1058, 510, 47, 64, 1045, 22, 331, 324, 1097, 685, 617, 1157, 520, 361, 898, 1134, 908, 303, 360, 536, 771, 1152, 29, 963, 1118, 55, 1030, 400, 238, 389, 1089, 825, 340, 976, 310, 783, 83, 881, 292, 1197, 37, 281]
valid_ids (0): []
train_ids (1094): [997, 961, 1083, 778, 726, 33, 1056, 548, 345, 132, 151, 1055, 965, 824, 533, 1146, 137, 845, 958, 1150, 498, 1028, 892, 308, 236, 46, 651, 74, 494, 274, 515, 492, 1125, 1029, 593, 330, 57, 1080, 801, 1061, 861, 1194, 654, 931, 509, 990, 1143, 774, 305, 658, 476, 719, 587, 627, 1120, 1211, 306, 953, 1163, 678, 923, 280, 1068, 728, 342, 750, 1202, 193, 831, 1012, 171, 546, 714, 860, 848, 842, 1138, 378, 992, 346, 54, 69, 422, 95, 1003, 124, 791, 772, 272, 1203, 797, 1004, 607, 254, 158, 610, 255, 24, 443, 608, 298, 939, 1122, 832, 265, 872, 904, 1113, 96, 472, 401, 894, 48, 962, 1169, 98, 942, 522, 1188, 101, 650, 600, 384, 715, 1145, 1149, 936, 1092, 185, 501, 710, 767, 705, 275, 1112, 833, 276, 839, 296, 199, 882, 470, 179, 950, 804, 1106, 823, 416, 840, 790, 73, 332, 740, 516, 456, 438, 508, 322, 141, 1067, 135, 512, 1046, 270, 919, 623, 643, 16, 317, 737, 530, 1110, 63, 325, 223, 71, 279, 215, 949, 1090, 849, 291, 1011, 519, 835, 590, 450, 1167, 11, 252, 150, 977, 444, 673, 915, 375, 1021, 256, 155, 1050, 1210, 15, 484, 300, 42, 392, 133, 260, 647, 625, 597, 85, 760, 358, 735, 918, 326, 1085, 421, 229, 694, 59, 602, 1127, 556, 154, 49, 834, 1027, 1104, 567, 1002, 17, 1126, 709, 803, 126, 288, 66, 646, 1074, 1013, 906, 565, 102, 686, 907, 149, 208, 967, 766, 1142, 123, 145, 1192, 1036, 459, 1174, 161, 221, 217, 1032, 235, 286, 605, 722, 930, 1209, 471, 500, 432, 1070, 856, 1181, 122, 396, 912, 811, 5, 1180, 1088, 1084, 558, 578, 415, 899, 329, 493, 1153, 1116, 920, 189, 640, 411, 419, 232, 169, 946, 390, 674, 1024, 630, 177, 87, 26, 355, 1173, 114, 77, 38, 947, 250, 316, 1151, 79, 757, 847, 564, 730, 1075, 1176, 483, 1200, 662, 261, 1183, 478, 414, 940, 815, 995, 370, 1057, 1158, 991, 407, 120, 335, 97, 1214, 127, 642, 736, 339, 367, 480, 301, 34, 386, 1124, 911, 1064, 295, 615, 1066, 850, 352, 806, 21, 159, 863, 468, 176, 944, 240, 241, 219, 165, 1148, 143, 935, 821, 945, 999, 697, 903, 328, 594, 1054, 365, 1208, 451, 307, 209, 36, 534, 902, 304, 681, 107, 495, 1000, 542, 385, 916, 60, 1044, 30, 762, 636, 70, 676, 192, 588, 659, 554, 551, 639, 251, 527, 381, 813, 707, 532, 1100, 183, 799, 552, 1206, 802, 1129, 111, 175, 537, 1059, 56, 591, 571, 688, 604, 318, 800, 1130, 78, 644, 664, 934, 569, 789, 4, 1147, 50, 178, 382, 230, 112, 729, 403, 404, 1204, 751, 1189, 693, 553, 393, 436, 1137, 764, 248, 197, 455, 1187, 723, 987, 486, 988, 110, 129, 237, 846, 1010, 649, 100, 1114, 201, 628, 338, 666, 739, 932, 362, 570, 637, 822, 614, 462, 68, 733, 893, 271, 84, 1199, 805, 473, 1022, 748, 663, 586, 2, 613, 168, 187, 960, 1033, 246, 538, 1091, 576, 388, 529, 853, 889, 680, 423, 216, 891, 464, 162, 273, 1096, 181, 491, 1060, 830, 1105, 447, 203, 952, 982, 624, 417, 146, 80, 1047, 989, 787, 349, 285, 25, 878, 160, 1136, 45, 1082, 373, 540, 58, 466, 1019, 289, 776, 1161, 854, 195, 784, 732, 702, 562, 577, 336, 67, 202, 973, 844, 377, 460, 927, 671, 1049, 1179, 134, 1071, 76, 734, 290, 268, 53, 521, 809, 633, 453, 1048, 166, 721, 138, 877, 875, 619, 870, 156, 603, 10, 794, 925, 994, 264, 814, 836, 1207, 816, 327, 528, 425, 1023, 888, 356, 13, 851, 993, 855, 14, 539, 1212, 1111, 585, 247, 81, 712, 874, 632, 452, 704, 638, 957, 409, 828, 518, 348, 363, 259, 616, 596, 312, 1168, 364, 980, 1159, 153, 1039, 859, 765, 1098, 61, 966, 242, 1132, 320, 196, 488, 157, 1191, 575, 1, 1053, 890, 535, 550, 1062, 323, 601, 344, 941, 1177, 517, 213, 905, 924, 544, 692, 580, 956, 489, 1043, 951, 928, 612, 656, 948, 970, 598, 1195, 428, 1093, 418, 368, 1108, 549, 561, 467, 257, 376, 756, 448, 1063, 106, 763, 94, 481, 374, 1175, 968, 792, 985, 445, 118, 981, 996, 3, 62, 573, 435, 350, 366, 18, 1156, 1128, 913, 869, 568, 92, 23, 188, 282, 1131, 41, 769, 103, 758, 88, 818, 745, 566, 867, 139, 210, 682, 584, 563, 1065, 198, 387, 648, 926, 871, 477, 857, 212, 547, 1155, 82, 879, 205, 117, 9, 1031, 724, 180, 224, 1086, 463, 793, 172, 838, 278, 302, 1172, 984, 174, 144, 269, 140, 505, 796, 700, 1042, 1009, 807, 1201, 752, 379, 457, 207, 194, 541, 1109, 234, 959, 657, 1115, 44, 690, 204, 513, 897, 514, 299, 669, 1073, 442, 51, 1040, 1081, 572, 1186, 969, 218, 499, 503, 753, 731, 972, 439, 297, 1117, 446, 222, 487, 0, 937, 293, 655, 557, 372, 1095, 1185, 465, 283, 755, 699, 410, 454, 785, 19, 420, 75, 130, 186, 284, 964, 90, 1178, 200, 1160, 938, 253, 667, 32, 665, 35, 433, 321, 717, 979, 837, 405, 206, 653, 485, 812, 31, 152, 427, 727, 660, 618, 641, 698, 909, 334, 191, 245, 725, 136, 914, 1144, 742, 482, 1026, 971, 599, 900, 190, 105, 747, 72, 1008, 424, 1007, 1037, 496, 955, 779, 652, 1102, 887, 182, 267, 243, 703, 777, 1205, 86, 412, 978, 744, 1133, 239, 819, 319, 27, 1139, 402, 1087, 413, 91, 896, 109, 266, 497, 582, 220, 1213, 592, 6, 351, 380, 226, 507, 917, 258, 1038, 901, 458, 773, 125, 895, 164, 394, 749, 672, 474, 1041, 52, 1018, 695, 314, 579, 469, 1072, 706, 876, 1015, 687, 116, 606, 147, 99, 262, 311, 780, 383, 583, 713, 1076, 683, 431, 170, 611, 1190, 369, 1165, 479, 93, 609, 718, 635, 395, 621, 1014, 244, 1094, 679, 131, 543, 696, 353, 1166, 852, 406, 531, 1069, 398, 28, 1034, 359, 1035, 119, 7, 626, 677, 357, 743, 983, 986, 1006, 817, 341, 761, 1182, 354, 309, 622, 184, 701, 716, 263, 910, 39, 675, 1020, 868, 708, 287, 523, 434, 343, 645, 337, 1121, 89, 595, 108, 294, 826, 810, 225, 525, 1170, 333, 886, 775, 20, 461, 954, 922, 885, 1196, 943, 741, 526, 770, 998, 1101, 768, 1141, 884, 864, 1016, 975, 560, 148, 1198, 504, 820, 1051, 502, 440, 437, 827, 227, 397, 441, 754, 880, 1078, 808, 691, 974, 841, 843, 128, 629, 1154, 228, 684, 426, 315, 738, 581, 795, 929, 781, 1184, 231]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9629633605974734
the save name prefix for this run is:  chkpt-ID_9629633605974734_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 551
rank avg (pred): 0.545 +- 0.004
mrr vals (pred, true): 0.000, 0.092
batch losses (mrrl, rdl): 0.0, 0.0019038728

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 406
rank avg (pred): 0.347 +- 0.259
mrr vals (pred, true): 0.031, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002344616

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 997
rank avg (pred): 0.101 +- 0.084
mrr vals (pred, true): 0.338, 0.293
batch losses (mrrl, rdl): 0.0, 4.45463e-05

Epoch over!
epoch time: 12.736

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 367
rank avg (pred): 0.334 +- 0.275
mrr vals (pred, true): 0.293, 0.069
batch losses (mrrl, rdl): 0.0, 0.0004084053

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.285 +- 0.241
mrr vals (pred, true): 0.360, 0.043
batch losses (mrrl, rdl): 0.0, 3.29609e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 508
rank avg (pred): 0.276 +- 0.238
mrr vals (pred, true): 0.384, 0.063
batch losses (mrrl, rdl): 0.0, 6.20261e-05

Epoch over!
epoch time: 12.176

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1181
rank avg (pred): 0.340 +- 0.290
mrr vals (pred, true): 0.373, 0.036
batch losses (mrrl, rdl): 0.0, 0.0001222171

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 448
rank avg (pred): 0.318 +- 0.276
mrr vals (pred, true): 0.388, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003228354

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 128
rank avg (pred): 0.234 +- 0.237
mrr vals (pred, true): 0.400, 0.068
batch losses (mrrl, rdl): 0.0, 0.000115712

Epoch over!
epoch time: 12.0

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 315
rank avg (pred): 0.106 +- 0.092
mrr vals (pred, true): 0.397, 0.111
batch losses (mrrl, rdl): 0.0, 4.1352e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.314 +- 0.275
mrr vals (pred, true): 0.395, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003417199

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.099 +- 0.088
mrr vals (pred, true): 0.404, 0.217
batch losses (mrrl, rdl): 0.0, 3.21268e-05

Epoch over!
epoch time: 12.14

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1186
rank avg (pred): 0.332 +- 0.290
mrr vals (pred, true): 0.396, 0.069
batch losses (mrrl, rdl): 0.0, 0.0002321976

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.319 +- 0.282
mrr vals (pred, true): 0.399, 0.171
batch losses (mrrl, rdl): 0.0, 0.0003865791

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1175
rank avg (pred): 0.382 +- 0.333
mrr vals (pred, true): 0.396, 0.073
batch losses (mrrl, rdl): 0.0, 0.0001747171

Epoch over!
epoch time: 11.759

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.322 +- 0.288
mrr vals (pred, true): 0.403, 0.128
batch losses (mrrl, rdl): 0.7589386702, 0.0004492047

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.105 +- 0.053
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0154983066, 0.0026695926

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 302
rank avg (pred): 0.398 +- 0.205
mrr vals (pred, true): 0.108, 0.100
batch losses (mrrl, rdl): 0.0006706082, 0.0019011188

Epoch over!
epoch time: 12.176

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 314
rank avg (pred): 0.371 +- 0.190
mrr vals (pred, true): 0.103, 0.119
batch losses (mrrl, rdl): 0.0026015192, 0.0015641979

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 636
rank avg (pred): 0.330 +- 0.094
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.23948e-05, 9.95989e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.403 +- 0.157
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.14461e-05, 0.0003258656

Epoch over!
epoch time: 12.145

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 24
rank avg (pred): 0.241 +- 0.118
mrr vals (pred, true): 0.088, 0.082
batch losses (mrrl, rdl): 0.0144931562, 0.0003088005

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 219
rank avg (pred): 0.357 +- 0.147
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.61706e-05, 0.0001913143

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.435 +- 0.221
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0308462773, 3.27791e-05

Epoch over!
epoch time: 12.489

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1206
rank avg (pred): 0.423 +- 0.182
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.000329513, 0.000135274

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.335 +- 0.124
mrr vals (pred, true): 0.051, 0.038
batch losses (mrrl, rdl): 2.14e-05, 0.000292339

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.187 +- 0.099
mrr vals (pred, true): 0.120, 0.085
batch losses (mrrl, rdl): 0.0489870533, 0.0001249286

Epoch over!
epoch time: 12.404

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 486
rank avg (pred): 0.287 +- 0.099
mrr vals (pred, true): 0.049, 0.028
batch losses (mrrl, rdl): 7.8366e-06, 0.0001348302

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.247 +- 0.087
mrr vals (pred, true): 0.050, 0.057
batch losses (mrrl, rdl): 7.57e-07, 0.0001267667

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.288 +- 0.131
mrr vals (pred, true): 0.062, 0.001
batch losses (mrrl, rdl): 0.0014621299, 0.0004996767

Epoch over!
epoch time: 12.592

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1039
rank avg (pred): 0.386 +- 0.194
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0141359866, 0.0003063314

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.351 +- 0.131
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.88898e-05, 0.0003279766

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.384 +- 0.103
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.28246e-05, 0.0002973134

Epoch over!
epoch time: 12.536

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.344 +- 0.161
mrr vals (pred, true): 0.081, 0.135
batch losses (mrrl, rdl): 0.0291767456, 0.0005674633

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 498
rank avg (pred): 0.318 +- 0.105
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 7.8442e-06, 0.0001610662

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 330
rank avg (pred): 0.371 +- 0.123
mrr vals (pred, true): 0.049, 0.022
batch losses (mrrl, rdl): 9.2979e-06, 0.0003366197

Epoch over!
epoch time: 13.203

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1026
rank avg (pred): 0.340 +- 0.176
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0340022184, 0.000188716

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 297
rank avg (pred): 0.246 +- 0.096
mrr vals (pred, true): 0.051, 0.057
batch losses (mrrl, rdl): 2.7564e-06, 0.0002406622

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1006
rank avg (pred): 0.340 +- 0.174
mrr vals (pred, true): 0.109, 0.186
batch losses (mrrl, rdl): 0.0590376593, 0.0006287352

Epoch over!
epoch time: 12.821

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.176 +- 0.071
mrr vals (pred, true): 0.059, 0.104
batch losses (mrrl, rdl): 0.0196987949, 0.0001483619

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 951
rank avg (pred): 0.369 +- 0.168
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0178199671, 0.0002558304

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.401 +- 0.150
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 8.805e-07, 0.0003444426

Epoch over!
epoch time: 12.054

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 478
rank avg (pred): 0.380 +- 0.137
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 6.0929e-06, 0.0001194298

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.087 +- 0.041
mrr vals (pred, true): 0.091, 0.123
batch losses (mrrl, rdl): 0.010085674, 3.0317e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 91
rank avg (pred): 0.400 +- 0.118
mrr vals (pred, true): 0.049, 0.032
batch losses (mrrl, rdl): 1.23128e-05, 0.0008939682

Epoch over!
epoch time: 12.72

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.389 +- 0.130
mrr vals (pred, true): 0.050, 0.055

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   81 	     0 	 0.11529 	 0.00018 	 MISS
   32 	     1 	 0.05062 	 0.00020 	 m..s
   47 	     2 	 0.06582 	 0.00021 	 m..s
   12 	     3 	 0.04891 	 0.00021 	 m..s
   21 	     4 	 0.04915 	 0.00021 	 m..s
   96 	     5 	 0.13006 	 0.00022 	 MISS
    1 	     6 	 0.04887 	 0.00022 	 m..s
   44 	     7 	 0.06302 	 0.00024 	 m..s
  106 	     8 	 0.14047 	 0.00024 	 MISS
  104 	     9 	 0.13948 	 0.00024 	 MISS
   77 	    10 	 0.11157 	 0.00025 	 MISS
   98 	    11 	 0.13304 	 0.00025 	 MISS
   33 	    12 	 0.05064 	 0.00026 	 m..s
   35 	    13 	 0.05477 	 0.00026 	 m..s
  103 	    14 	 0.13917 	 0.00028 	 MISS
    7 	    15 	 0.04888 	 0.00028 	 m..s
   84 	    16 	 0.11920 	 0.00028 	 MISS
   80 	    17 	 0.11521 	 0.00029 	 MISS
  102 	    18 	 0.13888 	 0.00030 	 MISS
   90 	    19 	 0.12255 	 0.00030 	 MISS
    0 	    20 	 0.04887 	 0.00030 	 m..s
   97 	    21 	 0.13173 	 0.00031 	 MISS
   42 	    22 	 0.06161 	 0.00031 	 m..s
   49 	    23 	 0.07072 	 0.00032 	 m..s
   91 	    24 	 0.12508 	 0.00032 	 MISS
    5 	    25 	 0.04888 	 0.00033 	 m..s
   29 	    26 	 0.05041 	 0.00035 	 m..s
    3 	    27 	 0.04887 	 0.00035 	 m..s
   27 	    28 	 0.04982 	 0.00036 	 m..s
   83 	    29 	 0.11796 	 0.00036 	 MISS
   92 	    30 	 0.12583 	 0.00037 	 MISS
   57 	    31 	 0.08461 	 0.00038 	 m..s
   31 	    32 	 0.05059 	 0.00040 	 m..s
   94 	    33 	 0.12639 	 0.00044 	 MISS
   52 	    34 	 0.07850 	 0.00045 	 m..s
   87 	    35 	 0.11991 	 0.00054 	 MISS
   30 	    36 	 0.05047 	 0.00073 	 m..s
   26 	    37 	 0.04977 	 0.00100 	 m..s
    2 	    38 	 0.04887 	 0.00451 	 m..s
    4 	    39 	 0.04888 	 0.00871 	 m..s
    6 	    40 	 0.04888 	 0.01300 	 m..s
   10 	    41 	 0.04890 	 0.01619 	 m..s
   16 	    42 	 0.04893 	 0.01678 	 m..s
   11 	    43 	 0.04890 	 0.01695 	 m..s
   14 	    44 	 0.04891 	 0.02041 	 ~...
   34 	    45 	 0.05066 	 0.02693 	 ~...
   15 	    46 	 0.04891 	 0.02901 	 ~...
    9 	    47 	 0.04889 	 0.03335 	 ~...
    8 	    48 	 0.04888 	 0.03464 	 ~...
   22 	    49 	 0.04962 	 0.04070 	 ~...
   37 	    50 	 0.05897 	 0.04366 	 ~...
   18 	    51 	 0.04901 	 0.04785 	 ~...
   41 	    52 	 0.06063 	 0.04913 	 ~...
   43 	    53 	 0.06291 	 0.05241 	 ~...
   13 	    54 	 0.04891 	 0.05391 	 ~...
   28 	    55 	 0.05011 	 0.05453 	 ~...
   48 	    56 	 0.06888 	 0.05573 	 ~...
   38 	    57 	 0.05930 	 0.05599 	 ~...
   23 	    58 	 0.04965 	 0.05609 	 ~...
   50 	    59 	 0.07323 	 0.05634 	 ~...
   46 	    60 	 0.06513 	 0.05784 	 ~...
   19 	    61 	 0.04901 	 0.05919 	 ~...
   40 	    62 	 0.06055 	 0.05996 	 ~...
   24 	    63 	 0.04971 	 0.06283 	 ~...
   70 	    64 	 0.09849 	 0.06330 	 m..s
   45 	    65 	 0.06398 	 0.06476 	 ~...
   25 	    66 	 0.04973 	 0.06811 	 ~...
   39 	    67 	 0.05981 	 0.06858 	 ~...
   56 	    68 	 0.08452 	 0.07284 	 ~...
   36 	    69 	 0.05606 	 0.07569 	 ~...
   54 	    70 	 0.08234 	 0.07573 	 ~...
   55 	    71 	 0.08268 	 0.07602 	 ~...
   63 	    72 	 0.09354 	 0.08095 	 ~...
   61 	    73 	 0.09261 	 0.08147 	 ~...
   20 	    74 	 0.04906 	 0.08185 	 m..s
   62 	    75 	 0.09330 	 0.08571 	 ~...
   17 	    76 	 0.04897 	 0.08933 	 m..s
   68 	    77 	 0.09671 	 0.09158 	 ~...
   58 	    78 	 0.08928 	 0.09562 	 ~...
   66 	    79 	 0.09525 	 0.09591 	 ~...
   64 	    80 	 0.09511 	 0.09725 	 ~...
   71 	    81 	 0.10178 	 0.09745 	 ~...
   53 	    82 	 0.07923 	 0.09750 	 ~...
   65 	    83 	 0.09524 	 0.10101 	 ~...
   69 	    84 	 0.09742 	 0.10160 	 ~...
   73 	    85 	 0.10716 	 0.10485 	 ~...
   60 	    86 	 0.09259 	 0.10944 	 ~...
   95 	    87 	 0.12940 	 0.11157 	 ~...
   59 	    88 	 0.09215 	 0.11232 	 ~...
  111 	    89 	 0.15683 	 0.11545 	 m..s
  107 	    90 	 0.14101 	 0.12037 	 ~...
   72 	    91 	 0.10617 	 0.12107 	 ~...
   74 	    92 	 0.10815 	 0.12259 	 ~...
  112 	    93 	 0.15853 	 0.12683 	 m..s
   67 	    94 	 0.09590 	 0.12804 	 m..s
  113 	    95 	 0.18217 	 0.14189 	 m..s
   75 	    96 	 0.10896 	 0.14422 	 m..s
   51 	    97 	 0.07441 	 0.14443 	 m..s
   76 	    98 	 0.10994 	 0.14524 	 m..s
   86 	    99 	 0.11978 	 0.14567 	 ~...
  108 	   100 	 0.14139 	 0.14605 	 ~...
   99 	   101 	 0.13308 	 0.14721 	 ~...
  110 	   102 	 0.15545 	 0.14879 	 ~...
   85 	   103 	 0.11946 	 0.15098 	 m..s
   89 	   104 	 0.12248 	 0.15316 	 m..s
   82 	   105 	 0.11583 	 0.15902 	 m..s
  109 	   106 	 0.14140 	 0.15935 	 ~...
   78 	   107 	 0.11371 	 0.16866 	 m..s
   88 	   108 	 0.12132 	 0.16968 	 m..s
  114 	   109 	 0.19092 	 0.17161 	 ~...
  101 	   110 	 0.13704 	 0.17251 	 m..s
   93 	   111 	 0.12619 	 0.18783 	 m..s
  100 	   112 	 0.13386 	 0.19204 	 m..s
   79 	   113 	 0.11490 	 0.19645 	 m..s
  105 	   114 	 0.13976 	 0.19752 	 m..s
  115 	   115 	 0.20398 	 0.21929 	 ~...
  116 	   116 	 0.21118 	 0.22405 	 ~...
  118 	   117 	 0.25001 	 0.26738 	 ~...
  120 	   118 	 0.38953 	 0.29489 	 m..s
  117 	   119 	 0.24363 	 0.30790 	 m..s
  119 	   120 	 0.33872 	 0.30943 	 ~...
==========================================
r_mrr = 0.7038339972496033
r2_mrr = 0.3818991184234619
spearmanr_mrr@5 = 0.8137165307998657
spearmanr_mrr@10 = 0.9169878363609314
spearmanr_mrr@50 = 0.9235888123512268
spearmanr_mrr@100 = 0.9530707001686096
spearmanr_mrr@All = 0.9604416489601135
==========================================
test time: 0.443
Done Testing dataset DBpedia50
total time taken: 192.2901451587677
training time taken: 186.4817032814026
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7038)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.3819)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8137)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9170)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9236)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9531)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9604)}}, 'test_loss': {'TransE': {'DBpedia50': 1.9102111154788872}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 5053379673425983
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [800, 1052, 1043, 1054, 768, 651, 691, 1084, 683, 34, 620, 211, 492, 859, 1196, 1126, 630, 924, 21, 675, 1212, 1011, 597, 1195, 355, 32, 122, 280, 743, 215, 646, 1047, 290, 810, 640, 38, 1072, 848, 1159, 448, 971, 317, 1015, 842, 169, 1139, 637, 610, 218, 1009, 1211, 607, 545, 68, 142, 312, 285, 81, 793, 165, 92, 780, 99, 658, 553, 1204, 739, 727, 742, 30, 402, 192, 35, 459, 710, 901, 827, 1180, 1022, 925, 349, 1010, 990, 988, 778, 220, 219, 426, 457, 645, 240, 1182, 514, 340, 689, 540, 585, 1137, 1121, 580, 533, 467, 1016, 360, 469, 1166, 37, 1001, 807, 509, 1118, 511, 520, 363, 346, 470, 1109, 90, 906, 109, 797]
valid_ids (0): []
train_ids (1094): [1036, 89, 128, 113, 826, 102, 1002, 724, 328, 870, 16, 348, 1103, 832, 453, 939, 143, 368, 239, 868, 140, 843, 958, 1082, 301, 394, 1039, 475, 764, 1071, 833, 518, 766, 233, 666, 1153, 622, 435, 237, 389, 1151, 1097, 548, 762, 26, 369, 927, 119, 1148, 655, 98, 1164, 82, 305, 295, 555, 977, 272, 1006, 217, 704, 1154, 178, 54, 111, 126, 412, 461, 849, 480, 874, 77, 430, 907, 1147, 300, 468, 1163, 1141, 790, 275, 883, 471, 1037, 1077, 678, 390, 858, 523, 718, 1061, 1021, 973, 584, 604, 836, 158, 314, 1089, 188, 950, 999, 451, 947, 256, 1062, 884, 129, 558, 546, 401, 1023, 760, 519, 547, 798, 15, 1091, 767, 145, 302, 352, 96, 1060, 334, 1070, 106, 638, 43, 692, 56, 878, 134, 152, 0, 266, 1132, 788, 980, 1114, 605, 544, 498, 789, 375, 654, 717, 1183, 602, 433, 136, 484, 1158, 29, 700, 866, 254, 279, 516, 1134, 831, 792, 478, 315, 60, 248, 452, 773, 753, 146, 161, 1055, 327, 955, 802, 617, 304, 75, 157, 209, 1105, 853, 247, 329, 1129, 1020, 761, 672, 822, 486, 1069, 685, 567, 1075, 1116, 964, 936, 1179, 623, 52, 750, 706, 932, 417, 441, 1003, 482, 18, 811, 578, 715, 425, 1133, 214, 679, 744, 125, 823, 730, 1181, 131, 909, 855, 13, 1143, 3, 663, 1173, 1024, 912, 444, 1034, 660, 483, 938, 515, 163, 265, 156, 711, 370, 364, 1198, 371, 564, 28, 991, 825, 1115, 9, 404, 695, 752, 454, 432, 207, 48, 919, 147, 582, 552, 942, 103, 828, 418, 398, 108, 1112, 243, 899, 261, 2, 69, 589, 307, 725, 91, 565, 1149, 694, 488, 926, 719, 194, 373, 777, 893, 698, 1078, 1144, 759, 1057, 155, 366, 481, 861, 1168, 148, 687, 84, 269, 671, 335, 282, 223, 677, 245, 466, 1028, 525, 442, 324, 1202, 337, 189, 794, 130, 643, 1136, 179, 437, 244, 784, 495, 941, 359, 587, 407, 202, 900, 493, 70, 795, 913, 350, 374, 309, 477, 673, 510, 1162, 856, 965, 403, 676, 1111, 959, 204, 963, 127, 501, 592, 499, 343, 286, 436, 319, 318, 1110, 639, 1152, 1178, 979, 267, 763, 388, 379, 1209, 74, 1012, 517, 455, 386, 414, 1160, 581, 857, 494, 353, 174, 252, 550, 543, 253, 813, 8, 65, 839, 170, 97, 19, 1029, 1100, 181, 1117, 182, 88, 1190, 903, 1170, 358, 987, 1200, 365, 886, 313, 657, 124, 1176, 1146, 805, 908, 1203, 46, 838, 62, 532, 779, 186, 1085, 61, 708, 228, 729, 809, 341, 456, 342, 1130, 326, 434, 153, 86, 869, 1169, 931, 1045, 212, 902, 575, 166, 621, 748, 918, 1184, 820, 320, 123, 120, 115, 536, 577, 594, 624, 667, 198, 627, 167, 213, 180, 193, 234, 524, 139, 847, 920, 1096, 226, 885, 559, 246, 1007, 387, 816, 176, 141, 892, 659, 682, 749, 787, 238, 303, 774, 421, 11, 865, 940, 1076, 281, 1102, 1210, 45, 331, 662, 332, 489, 852, 14, 680, 615, 786, 626, 781, 996, 76, 230, 242, 1156, 427, 380, 367, 201, 573, 873, 1066, 1208, 824, 570, 521, 867, 150, 420, 845, 1026, 216, 197, 636, 1175, 556, 527, 507, 1005, 12, 1128, 905, 232, 411, 93, 205, 1088, 1090, 603, 1155, 250, 354, 296, 87, 737, 221, 249, 653, 720, 634, 311, 1092, 1142, 1068, 362, 406, 41, 967, 1051, 968, 723, 1185, 894, 735, 688, 731, 998, 535, 864, 702, 55, 396, 782, 686, 57, 339, 24, 51, 357, 628, 100, 1177, 1150, 67, 382, 665, 500, 395, 713, 400, 283, 44, 1013, 994, 879, 1098, 571, 601, 681, 1056, 850, 443, 460, 851, 512, 1119, 502, 1035, 871, 78, 701, 834, 22, 568, 803, 946, 1046, 172, 151, 289, 72, 133, 1213, 707, 1030, 1165, 928, 785, 566, 291, 721, 473, 330, 440, 64, 933, 953, 596, 422, 149, 1058, 837, 419, 47, 6, 1048, 895, 880, 1187, 554, 208, 229, 1083, 569, 185, 1042, 1101, 929, 1140, 1108, 496, 258, 95, 195, 712, 770, 40, 751, 661, 508, 983, 551, 522, 288, 345, 531, 668, 1113, 614, 1073, 992, 5, 664, 588, 670, 560, 796, 732, 1161, 1086, 1138, 644, 1120, 916, 196, 476, 593, 117, 1205, 974, 1079, 611, 981, 1080, 583, 392, 316, 598, 1008, 812, 53, 94, 1041, 458, 464, 714, 618, 1123, 922, 1053, 557, 572, 804, 1135, 1197, 372, 83, 815, 930, 251, 917, 306, 344, 632, 1017, 429, 791, 854, 4, 995, 829, 271, 1188, 758, 308, 222, 1131, 80, 1106, 1004, 79, 923, 321, 173, 224, 472, 25, 1050, 819, 817, 875, 107, 293, 408, 538, 112, 257, 969, 338, 287, 397, 42, 876, 203, 504, 674, 292, 503, 888, 1087, 746, 877, 262, 934, 647, 462, 1207, 137, 333, 911, 1040, 487, 641, 1192, 776, 277, 599, 600, 1081, 1201, 449, 951, 684, 491, 612, 187, 887, 890, 101, 956, 446, 294, 1019, 241, 73, 574, 297, 970, 726, 184, 264, 989, 649, 1027, 323, 479, 775, 1145, 898, 881, 897, 1099, 356, 738, 1063, 497, 1025, 561, 1033, 255, 818, 997, 168, 616, 528, 361, 1214, 872, 669, 159, 138, 910, 656, 549, 1104, 225, 736, 650, 1067, 579, 431, 808, 733, 745, 409, 1171, 539, 274, 754, 860, 438, 863, 413, 972, 190, 23, 526, 191, 696, 385, 896, 474, 423, 889, 276, 709, 1065, 914, 465, 943, 841, 105, 110, 1000, 948, 741, 591, 606, 259, 1122, 757, 952, 424, 27, 284, 39, 351, 1093, 962, 298, 821, 445, 299, 450, 199, 882, 613, 116, 1059, 1074, 576, 537, 586, 957, 1194, 530, 633, 63, 1193, 747, 393, 263, 391, 984, 415, 132, 236, 1049, 625, 541, 268, 162, 830, 278, 862, 921, 177, 447, 160, 1124, 1127, 1031, 1157, 635, 505, 1189, 801, 1, 485, 336, 978, 310, 33, 183, 175, 114, 949, 1172, 405, 383, 619, 976, 975, 58, 384, 399, 506, 135, 118, 20, 210, 835, 844, 121, 347, 322, 1174, 954, 891, 609, 652, 1018, 381, 697, 716, 71, 154, 325, 1014, 705, 960, 227, 631, 915, 171, 765, 1199, 642, 513, 36, 806, 799, 814, 542, 231, 982, 985, 376, 772, 728, 756, 563, 966, 273, 10, 840, 270, 1186, 416, 200, 1044, 769, 378, 629, 937, 608, 693, 699, 206, 7, 144, 846, 377, 703, 771, 463, 590, 783, 17, 104, 986, 1032, 722, 595, 534, 59, 755, 1167, 85, 66, 428, 1107, 529, 945, 1064, 690, 648, 1191, 1094, 1206, 1125, 260, 562, 490, 235, 410, 164, 944, 740, 993, 935, 439, 1038, 734, 31, 904, 50, 1095, 49, 961]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2265162395238766
the save name prefix for this run is:  chkpt-ID_2265162395238766_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 194
rank avg (pred): 0.546 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001524143

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 54
rank avg (pred): 0.112 +- 0.089
mrr vals (pred, true): 0.171, 0.045
batch losses (mrrl, rdl): 0.0, 4.67156e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.320 +- 0.256
mrr vals (pred, true): 0.212, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001398035

Epoch over!
epoch time: 12.23

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 308
rank avg (pred): 0.122 +- 0.097
mrr vals (pred, true): 0.265, 0.111
batch losses (mrrl, rdl): 0.0, 5.03458e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.365 +- 0.292
mrr vals (pred, true): 0.225, 0.000
batch losses (mrrl, rdl): 0.0, 3.74151e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.318 +- 0.254
mrr vals (pred, true): 0.272, 0.001
batch losses (mrrl, rdl): 0.0, 0.0005529131

Epoch over!
epoch time: 12.507

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 438
rank avg (pred): 0.329 +- 0.263
mrr vals (pred, true): 0.273, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002074438

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1135
rank avg (pred): 0.194 +- 0.155
mrr vals (pred, true): 0.313, 0.101
batch losses (mrrl, rdl): 0.0, 0.0001247821

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.094 +- 0.076
mrr vals (pred, true): 0.354, 0.093
batch losses (mrrl, rdl): 0.0, 2.08622e-05

Epoch over!
epoch time: 12.126

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 279
rank avg (pred): 0.119 +- 0.095
mrr vals (pred, true): 0.357, 0.085
batch losses (mrrl, rdl): 0.0, 5.0999e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1152
rank avg (pred): 0.164 +- 0.131
mrr vals (pred, true): 0.353, 0.096
batch losses (mrrl, rdl): 0.0, 0.0001654553

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 175
rank avg (pred): 0.327 +- 0.262
mrr vals (pred, true): 0.335, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002640059

Epoch over!
epoch time: 12.371

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.351 +- 0.281
mrr vals (pred, true): 0.319, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002483435

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 372
rank avg (pred): 0.326 +- 0.261
mrr vals (pred, true): 0.312, 0.054
batch losses (mrrl, rdl): 0.0, 0.0002376427

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.258 +- 0.206
mrr vals (pred, true): 0.354, 0.062
batch losses (mrrl, rdl): 0.0, 4.18127e-05

Epoch over!
epoch time: 12.414

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 174
rank avg (pred): 0.333 +- 0.267
mrr vals (pred, true): 0.317, 0.000
batch losses (mrrl, rdl): 0.7118983269, 0.0003098647

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.381 +- 0.176
mrr vals (pred, true): 0.051, 0.028
batch losses (mrrl, rdl): 2.951e-06, 0.0005622432

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 863
rank avg (pred): 0.061 +- 0.039
mrr vals (pred, true): 0.081, 0.159
batch losses (mrrl, rdl): 0.0598212555, 0.0006329822

Epoch over!
epoch time: 12.645

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.373 +- 0.151
mrr vals (pred, true): 0.049, 0.002
batch losses (mrrl, rdl): 1.31675e-05, 0.0001299972

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 75
rank avg (pred): 0.062 +- 0.039
mrr vals (pred, true): 0.114, 0.100
batch losses (mrrl, rdl): 0.0410925895, 3.43919e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 372
rank avg (pred): 0.346 +- 0.156
mrr vals (pred, true): 0.059, 0.054
batch losses (mrrl, rdl): 0.0007709119, 0.0003071467

Epoch over!
epoch time: 12.189

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 313
rank avg (pred): 0.137 +- 0.085
mrr vals (pred, true): 0.101, 0.092
batch losses (mrrl, rdl): 0.025993688, 6.55611e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1184
rank avg (pred): 0.413 +- 0.196
mrr vals (pred, true): 0.067, 0.067
batch losses (mrrl, rdl): 0.0030150742, 0.0003608364

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.294 +- 0.129
mrr vals (pred, true): 0.057, 0.041
batch losses (mrrl, rdl): 0.0005049077, 0.0001301284

Epoch over!
epoch time: 11.818

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.227, 0.195
batch losses (mrrl, rdl): 0.010524014, 9.53768e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.154, 0.139
batch losses (mrrl, rdl): 0.0021388272, 0.0002192538

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.032 +- 0.022
mrr vals (pred, true): 0.125, 0.085
batch losses (mrrl, rdl): 0.0566855147, 0.0001471475

Epoch over!
epoch time: 11.822

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.344 +- 0.273
mrr vals (pred, true): 0.049, 0.003
batch losses (mrrl, rdl): 1.02187e-05, 1.01089e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1141
rank avg (pred): 0.127 +- 0.082
mrr vals (pred, true): 0.104, 0.108
batch losses (mrrl, rdl): 0.0001835146, 0.0003515979

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.071 +- 0.045
mrr vals (pred, true): 0.116, 0.123
batch losses (mrrl, rdl): 0.000584953, 3.45065e-05

Epoch over!
epoch time: 11.773

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.281, 0.189
batch losses (mrrl, rdl): 0.0849567726, 0.0001840508

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 65
rank avg (pred): 0.046 +- 0.030
mrr vals (pred, true): 0.128, 0.123
batch losses (mrrl, rdl): 0.0003209371, 6.24564e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 160
rank avg (pred): 0.404 +- 0.299
mrr vals (pred, true): 0.049, 0.025
batch losses (mrrl, rdl): 1.01774e-05, 0.0004675865

Epoch over!
epoch time: 12.101

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 414
rank avg (pred): 0.376 +- 0.278
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.3732e-05, 0.000335667

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 342
rank avg (pred): 0.405 +- 0.246
mrr vals (pred, true): 0.054, 0.029
batch losses (mrrl, rdl): 0.0001333383, 0.0005206263

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.407 +- 0.322
mrr vals (pred, true): 0.049, 0.066
batch losses (mrrl, rdl): 1.28896e-05, 0.0002562461

Epoch over!
epoch time: 11.928

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 720
rank avg (pred): 0.370 +- 0.351
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.1252e-05, 0.0004434492

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1093
rank avg (pred): 0.307 +- 0.224
mrr vals (pred, true): 0.108, 0.204
batch losses (mrrl, rdl): 0.0935733765, 0.0004860272

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.045 +- 0.033
mrr vals (pred, true): 0.170, 0.133
batch losses (mrrl, rdl): 0.0137092602, 7.97556e-05

Epoch over!
epoch time: 12.003

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 896
rank avg (pred): 0.325 +- 0.208
mrr vals (pred, true): 0.100, 0.095
batch losses (mrrl, rdl): 0.0252764188, 0.0009962297

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 497
rank avg (pred): 0.311 +- 0.245
mrr vals (pred, true): 0.054, 0.095
batch losses (mrrl, rdl): 0.0001278039, 0.0001185504

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1192
rank avg (pred): 0.421 +- 0.228
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0027026308, 8.00195e-05

Epoch over!
epoch time: 11.982

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.079 +- 0.052
mrr vals (pred, true): 0.103, 0.098
batch losses (mrrl, rdl): 0.0285046194, 2.42715e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 548
rank avg (pred): 0.340 +- 0.314
mrr vals (pred, true): 0.049, 0.080
batch losses (mrrl, rdl): 9.2975e-06, 0.0001312398

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 734
rank avg (pred): 0.184 +- 0.118
mrr vals (pred, true): 0.098, 0.089
batch losses (mrrl, rdl): 0.0229792763, 0.0001816516

Epoch over!
epoch time: 12.034

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.250 +- 0.175
mrr vals (pred, true): 0.120, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   40 	     0 	 0.04976 	 0.00017 	 m..s
   52 	     1 	 0.05191 	 0.00019 	 m..s
   17 	     2 	 0.04896 	 0.00019 	 m..s
   91 	     3 	 0.09967 	 0.00020 	 m..s
   56 	     4 	 0.05318 	 0.00020 	 m..s
   65 	     5 	 0.06657 	 0.00021 	 m..s
   11 	     6 	 0.04894 	 0.00021 	 m..s
   12 	     7 	 0.04894 	 0.00021 	 m..s
   25 	     8 	 0.04899 	 0.00024 	 m..s
   87 	     9 	 0.09787 	 0.00024 	 m..s
   50 	    10 	 0.05172 	 0.00025 	 m..s
   99 	    11 	 0.10239 	 0.00025 	 MISS
  108 	    12 	 0.12038 	 0.00026 	 MISS
   28 	    13 	 0.04900 	 0.00027 	 m..s
   14 	    14 	 0.04895 	 0.00028 	 m..s
    0 	    15 	 0.04865 	 0.00028 	 m..s
  106 	    16 	 0.11302 	 0.00029 	 MISS
    5 	    17 	 0.04887 	 0.00029 	 m..s
   43 	    18 	 0.05012 	 0.00030 	 m..s
   22 	    19 	 0.04897 	 0.00031 	 m..s
   82 	    20 	 0.09474 	 0.00032 	 m..s
   89 	    21 	 0.09956 	 0.00032 	 m..s
   57 	    22 	 0.05335 	 0.00033 	 m..s
   30 	    23 	 0.04901 	 0.00035 	 m..s
    4 	    24 	 0.04885 	 0.00035 	 m..s
   41 	    25 	 0.04982 	 0.00039 	 m..s
   79 	    26 	 0.09438 	 0.00040 	 m..s
   54 	    27 	 0.05280 	 0.00042 	 m..s
   55 	    28 	 0.05301 	 0.00042 	 m..s
    7 	    29 	 0.04893 	 0.00047 	 m..s
   37 	    30 	 0.04928 	 0.00048 	 m..s
    6 	    31 	 0.04887 	 0.00053 	 m..s
   98 	    32 	 0.10214 	 0.00058 	 MISS
   44 	    33 	 0.05018 	 0.00060 	 m..s
   27 	    34 	 0.04900 	 0.00062 	 m..s
   42 	    35 	 0.04999 	 0.00073 	 m..s
  104 	    36 	 0.10834 	 0.00080 	 MISS
   21 	    37 	 0.04896 	 0.00089 	 m..s
   94 	    38 	 0.10041 	 0.00149 	 m..s
   90 	    39 	 0.09964 	 0.00227 	 m..s
    3 	    40 	 0.04883 	 0.00237 	 m..s
   51 	    41 	 0.05183 	 0.00246 	 m..s
   34 	    42 	 0.04905 	 0.00277 	 m..s
    1 	    43 	 0.04878 	 0.00451 	 m..s
   20 	    44 	 0.04896 	 0.00530 	 m..s
    2 	    45 	 0.04879 	 0.00908 	 m..s
   29 	    46 	 0.04901 	 0.00977 	 m..s
    9 	    47 	 0.04893 	 0.01079 	 m..s
   10 	    48 	 0.04894 	 0.01110 	 m..s
   13 	    49 	 0.04894 	 0.01115 	 m..s
   26 	    50 	 0.04899 	 0.01205 	 m..s
   46 	    51 	 0.05059 	 0.01222 	 m..s
   24 	    52 	 0.04898 	 0.01397 	 m..s
   33 	    53 	 0.04904 	 0.01610 	 m..s
   18 	    54 	 0.04896 	 0.01619 	 m..s
   19 	    55 	 0.04896 	 0.02180 	 ~...
   38 	    56 	 0.04932 	 0.03447 	 ~...
    8 	    57 	 0.04893 	 0.03464 	 ~...
   32 	    58 	 0.04904 	 0.03653 	 ~...
   53 	    59 	 0.05210 	 0.03688 	 ~...
   31 	    60 	 0.04901 	 0.04273 	 ~...
   63 	    61 	 0.06055 	 0.04305 	 ~...
   15 	    62 	 0.04895 	 0.04785 	 ~...
   47 	    63 	 0.05061 	 0.05241 	 ~...
   36 	    64 	 0.04910 	 0.05391 	 ~...
   45 	    65 	 0.05032 	 0.05418 	 ~...
   62 	    66 	 0.05972 	 0.05634 	 ~...
   61 	    67 	 0.05949 	 0.05784 	 ~...
   35 	    68 	 0.04907 	 0.05942 	 ~...
   58 	    69 	 0.05616 	 0.06045 	 ~...
   60 	    70 	 0.05936 	 0.06100 	 ~...
   16 	    71 	 0.04895 	 0.06385 	 ~...
   86 	    72 	 0.09732 	 0.06587 	 m..s
   23 	    73 	 0.04897 	 0.06654 	 ~...
   49 	    74 	 0.05160 	 0.07030 	 ~...
   68 	    75 	 0.07125 	 0.07300 	 ~...
   85 	    76 	 0.09556 	 0.07414 	 ~...
   59 	    77 	 0.05677 	 0.07478 	 ~...
   69 	    78 	 0.07224 	 0.07602 	 ~...
   67 	    79 	 0.06879 	 0.07688 	 ~...
  101 	    80 	 0.10292 	 0.08234 	 ~...
   66 	    81 	 0.06863 	 0.08429 	 ~...
   96 	    82 	 0.10087 	 0.08442 	 ~...
   64 	    83 	 0.06147 	 0.08693 	 ~...
   72 	    84 	 0.07649 	 0.08908 	 ~...
   39 	    85 	 0.04951 	 0.08933 	 m..s
   77 	    86 	 0.08904 	 0.09156 	 ~...
   48 	    87 	 0.05094 	 0.09317 	 m..s
  109 	    88 	 0.12441 	 0.09692 	 ~...
   74 	    89 	 0.08741 	 0.09751 	 ~...
  105 	    90 	 0.10859 	 0.10051 	 ~...
   75 	    91 	 0.08806 	 0.10106 	 ~...
  103 	    92 	 0.10801 	 0.10188 	 ~...
   70 	    93 	 0.07374 	 0.10647 	 m..s
   76 	    94 	 0.08824 	 0.10666 	 ~...
   71 	    95 	 0.07404 	 0.10908 	 m..s
  111 	    96 	 0.17242 	 0.13179 	 m..s
   78 	    97 	 0.09430 	 0.13466 	 m..s
   80 	    98 	 0.09444 	 0.13593 	 m..s
  110 	    99 	 0.13128 	 0.14536 	 ~...
   81 	   100 	 0.09470 	 0.15098 	 m..s
   92 	   101 	 0.09989 	 0.15744 	 m..s
   73 	   102 	 0.08026 	 0.15970 	 m..s
   97 	   103 	 0.10145 	 0.16173 	 m..s
  102 	   104 	 0.10461 	 0.16589 	 m..s
  100 	   105 	 0.10282 	 0.17008 	 m..s
  107 	   106 	 0.11316 	 0.17136 	 m..s
   83 	   107 	 0.09518 	 0.17977 	 m..s
  114 	   108 	 0.22553 	 0.18032 	 m..s
  112 	   109 	 0.18435 	 0.18310 	 ~...
   84 	   110 	 0.09544 	 0.18732 	 m..s
   88 	   111 	 0.09827 	 0.18892 	 m..s
  115 	   112 	 0.22630 	 0.20156 	 ~...
   95 	   113 	 0.10054 	 0.20264 	 MISS
  117 	   114 	 0.25398 	 0.20501 	 m..s
   93 	   115 	 0.10037 	 0.20735 	 MISS
  113 	   116 	 0.19095 	 0.20896 	 ~...
  118 	   117 	 0.25971 	 0.21882 	 m..s
  119 	   118 	 0.26081 	 0.22181 	 m..s
  116 	   119 	 0.23474 	 0.22192 	 ~...
  120 	   120 	 0.28582 	 0.29990 	 ~...
==========================================
r_mrr = 0.744353175163269
r2_mrr = 0.4807835817337036
spearmanr_mrr@5 = 0.8932080268859863
spearmanr_mrr@10 = 0.7688345313072205
spearmanr_mrr@50 = 0.8505257964134216
spearmanr_mrr@100 = 0.9051013588905334
spearmanr_mrr@All = 0.9115844368934631
==========================================
test time: 0.447
Done Testing dataset DBpedia50
total time taken: 188.48383259773254
training time taken: 182.47187161445618
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7444)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4808)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8932)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.7688)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.8505)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9051)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9116)}}, 'test_loss': {'TransE': {'DBpedia50': 1.5025064591391128}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 8254701711281827
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [154, 386, 877, 1189, 328, 761, 515, 496, 6, 417, 1018, 12, 1033, 671, 206, 249, 471, 999, 909, 112, 89, 638, 75, 1184, 880, 996, 774, 293, 850, 627, 308, 835, 180, 968, 236, 416, 211, 305, 1135, 639, 406, 862, 474, 665, 1047, 334, 756, 53, 590, 1170, 487, 262, 901, 817, 194, 146, 400, 752, 534, 144, 392, 810, 551, 189, 9, 191, 1174, 605, 65, 963, 162, 115, 1002, 138, 546, 848, 787, 269, 816, 579, 1140, 500, 934, 364, 466, 1071, 223, 209, 464, 547, 882, 1012, 998, 991, 222, 647, 291, 340, 67, 271, 818, 495, 472, 491, 673, 642, 512, 425, 1150, 911, 462, 321, 373, 616, 1034, 1139, 212, 780, 1053, 168, 1046]
valid_ids (0): []
train_ids (1094): [860, 728, 635, 387, 931, 325, 208, 185, 1205, 718, 976, 623, 600, 139, 316, 809, 971, 375, 362, 480, 771, 614, 233, 260, 733, 948, 588, 48, 871, 697, 651, 26, 423, 114, 634, 856, 957, 202, 747, 490, 894, 859, 332, 1011, 1029, 201, 785, 435, 949, 1025, 70, 475, 874, 580, 456, 514, 219, 975, 352, 503, 586, 1156, 1107, 297, 674, 175, 896, 1049, 961, 42, 479, 812, 273, 612, 331, 36, 1090, 955, 1072, 152, 108, 529, 64, 734, 722, 388, 68, 649, 928, 399, 796, 513, 284, 13, 380, 956, 765, 815, 511, 606, 1207, 696, 505, 1130, 251, 50, 253, 57, 577, 715, 327, 173, 229, 1144, 281, 492, 499, 239, 889, 313, 21, 710, 192, 439, 432, 47, 596, 763, 736, 188, 164, 536, 390, 252, 247, 721, 73, 445, 426, 141, 157, 844, 618, 368, 581, 1105, 1057, 1054, 32, 675, 15, 187, 942, 1093, 829, 419, 764, 1097, 78, 19, 620, 881, 83, 203, 37, 1142, 637, 1213, 170, 397, 1121, 870, 125, 1206, 1137, 692, 724, 919, 1035, 494, 555, 1041, 729, 1208, 183, 585, 689, 1045, 24, 940, 921, 558, 270, 77, 415, 145, 200, 1125, 684, 825, 453, 318, 640, 259, 1059, 179, 307, 824, 954, 895, 570, 709, 857, 1020, 1064, 843, 572, 0, 779, 1088, 1165, 604, 1056, 395, 486, 469, 652, 461, 836, 630, 452, 1154, 1087, 484, 1101, 661, 1147, 986, 535, 323, 1060, 498, 287, 1003, 356, 962, 195, 897, 906, 1082, 995, 1199, 993, 599, 607, 1179, 806, 278, 1043, 97, 1081, 1080, 660, 625, 396, 544, 129, 133, 754, 225, 904, 147, 242, 346, 506, 374, 681, 1079, 594, 703, 1068, 369, 990, 1136, 217, 258, 410, 370, 1172, 1098, 52, 95, 1133, 485, 899, 23, 584, 978, 842, 947, 459, 1197, 153, 977, 959, 778, 427, 436, 454, 643, 578, 730, 16, 176, 447, 371, 213, 104, 44, 3, 277, 353, 751, 122, 1118, 181, 644, 772, 907, 507, 174, 548, 120, 613, 320, 1168, 650, 553, 172, 658, 750, 98, 137, 974, 1074, 762, 1076, 267, 516, 646, 159, 304, 298, 552, 890, 1119, 411, 550, 935, 830, 302, 910, 1173, 20, 711, 448, 1116, 389, 178, 363, 407, 39, 227, 929, 63, 46, 94, 401, 539, 230, 922, 420, 915, 465, 177, 985, 303, 72, 256, 800, 1051, 1160, 1091, 339, 917, 254, 805, 1134, 372, 984, 636, 451, 130, 814, 964, 442, 127, 744, 1006, 1023, 405, 621, 408, 1026, 430, 926, 615, 1063, 134, 833, 377, 967, 82, 394, 979, 1127, 608, 686, 88, 568, 808, 840, 483, 276, 566, 35, 424, 945, 5, 330, 493, 807, 398, 312, 107, 379, 1048, 705, 1092, 927, 382, 790, 617, 508, 348, 165, 549, 1181, 821, 943, 1001, 306, 1145, 749, 759, 668, 804, 932, 786, 1159, 966, 972, 1203, 1095, 158, 559, 79, 1086, 654, 1042, 69, 788, 582, 845, 329, 90, 822, 603, 463, 873, 1039, 324, 336, 151, 864, 1157, 933, 414, 91, 866, 672, 467, 7, 433, 849, 1161, 556, 521, 10, 851, 587, 893, 1106, 4, 1084, 732, 629, 51, 1099, 231, 1198, 1028, 1062, 583, 279, 884, 601, 1152, 266, 27, 656, 1055, 746, 55, 244, 66, 981, 204, 1185, 669, 450, 589, 142, 1149, 611, 335, 1169, 357, 776, 470, 983, 941, 119, 131, 1073, 1194, 939, 272, 865, 913, 803, 1036, 1192, 662, 169, 898, 628, 567, 1005, 1113, 811, 846, 99, 1148, 103, 592, 1164, 56, 720, 124, 525, 33, 1167, 443, 690, 719, 265, 648, 240, 641, 920, 126, 685, 792, 1096, 852, 1178, 404, 338, 317, 659, 524, 92, 367, 431, 885, 886, 299, 1187, 753, 109, 770, 1022, 243, 337, 393, 241, 791, 1070, 1021, 758, 878, 887, 381, 228, 121, 969, 708, 858, 973, 872, 632, 1143, 171, 161, 1102, 602, 14, 923, 478, 701, 737, 366, 87, 994, 936, 837, 1141, 1, 892, 755, 688, 383, 326, 542, 437, 828, 801, 86, 1201, 528, 111, 93, 538, 702, 235, 855, 742, 1104, 214, 532, 930, 264, 1155, 802, 717, 793, 706, 609, 449, 422, 838, 221, 952, 598, 1153, 143, 767, 199, 234, 322, 123, 1114, 1019, 554, 128, 832, 924, 704, 925, 84, 1017, 545, 193, 560, 1013, 434, 248, 1032, 509, 1196, 726, 8, 591, 667, 1209, 768, 1108, 655, 795, 38, 745, 526, 1214, 136, 1117, 781, 100, 365, 679, 45, 766, 1038, 1067, 440, 879, 149, 1158, 997, 347, 682, 989, 1075, 354, 1129, 215, 160, 216, 557, 309, 274, 488, 155, 224, 841, 226, 1191, 847, 861, 1162, 96, 593, 85, 1190, 473, 595, 446, 245, 1030, 429, 914, 315, 1146, 413, 1200, 197, 62, 28, 992, 799, 41, 533, 1037, 716, 735, 541, 156, 1186, 1110, 1050, 378, 76, 1016, 1044, 489, 903, 286, 275, 1124, 982, 1007, 1183, 739, 1008, 645, 773, 1188, 1163, 1027, 723, 163, 54, 531, 797, 1138, 820, 1031, 412, 1195, 876, 740, 481, 71, 694, 1089, 946, 1100, 707, 633, 314, 569, 29, 918, 311, 457, 831, 198, 343, 563, 875, 207, 333, 190, 700, 900, 561, 677, 34, 1182, 246, 1193, 571, 458, 182, 619, 502, 912, 958, 349, 441, 823, 296, 166, 687, 1066, 537, 220, 691, 61, 819, 40, 1009, 1112, 775, 1065, 409, 186, 813, 1166, 520, 1132, 1078, 798, 1014, 105, 350, 714, 1058, 916, 664, 1212, 853, 250, 282, 1085, 1004, 1069, 150, 295, 218, 826, 30, 527, 342, 951, 1177, 418, 43, 908, 562, 1111, 827, 476, 713, 421, 1210, 135, 1120, 725, 784, 1151, 310, 1123, 110, 980, 268, 210, 1010, 610, 345, 760, 1052, 288, 80, 518, 49, 839, 769, 22, 902, 624, 292, 285, 888, 670, 676, 965, 1024, 257, 712, 2, 540, 953, 631, 438, 794, 460, 782, 237, 699, 238, 937, 1175, 678, 359, 738, 184, 106, 510, 403, 1000, 748, 1211, 905, 1109, 575, 74, 17, 868, 25, 1131, 60, 1083, 101, 1103, 960, 653, 294, 148, 1015, 869, 666, 289, 523, 522, 564, 1204, 834, 477, 757, 657, 574, 950, 384, 205, 622, 698, 58, 113, 597, 361, 1094, 683, 1180, 1061, 1202, 504, 731, 117, 261, 468, 501, 283, 301, 196, 543, 263, 376, 519, 355, 428, 573, 102, 987, 988, 140, 626, 444, 1115, 31, 743, 530, 680, 344, 517, 1128, 280, 358, 59, 944, 116, 1122, 455, 777, 1077, 854, 482, 741, 319, 11, 132, 695, 385, 1171, 891, 360, 867, 118, 341, 167, 290, 863, 351, 402, 232, 497, 789, 663, 18, 1040, 693, 81, 255, 1126, 970, 883, 727, 783, 1176, 300, 938, 565, 576, 391]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4413811063692282
the save name prefix for this run is:  chkpt-ID_4413811063692282_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 202
rank avg (pred): 0.455 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001641241

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 857
rank avg (pred): 0.332 +- 0.289
mrr vals (pred, true): 0.044, 0.150
batch losses (mrrl, rdl): 0.0, 0.0003665716

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.276 +- 0.284
mrr vals (pred, true): 0.127, 0.125
batch losses (mrrl, rdl): 0.0, 0.000333191

Epoch over!
epoch time: 11.694

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 820
rank avg (pred): 0.090 +- 0.088
mrr vals (pred, true): 0.146, 0.141
batch losses (mrrl, rdl): 0.0, 2.99138e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.344 +- 0.322
mrr vals (pred, true): 0.107, 0.013
batch losses (mrrl, rdl): 0.0, 0.000286346

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.333 +- 0.309
mrr vals (pred, true): 0.104, 0.192
batch losses (mrrl, rdl): 0.0, 0.0006694439

Epoch over!
epoch time: 11.701

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 884
rank avg (pred): 0.323 +- 0.304
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002405745

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.283 +- 0.295
mrr vals (pred, true): 0.195, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004875623

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 239
rank avg (pred): 0.341 +- 0.317
mrr vals (pred, true): 0.124, 0.002
batch losses (mrrl, rdl): 0.0, 0.0002153608

Epoch over!
epoch time: 11.666

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.079 +- 0.085
mrr vals (pred, true): 0.208, 0.229
batch losses (mrrl, rdl): 0.0, 2.11649e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 250
rank avg (pred): 0.093 +- 0.109
mrr vals (pred, true): 0.244, 0.082
batch losses (mrrl, rdl): 0.0, 2.85779e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.311 +- 0.318
mrr vals (pred, true): 0.212, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003384727

Epoch over!
epoch time: 11.647

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 788
rank avg (pred): 0.282 +- 0.299
mrr vals (pred, true): 0.244, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004170515

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 528
rank avg (pred): 0.260 +- 0.298
mrr vals (pred, true): 0.229, 0.041
batch losses (mrrl, rdl): 0.0, 5.6105e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.189 +- 0.264
mrr vals (pred, true): 0.256, 0.082
batch losses (mrrl, rdl): 0.0, 8.01968e-05

Epoch over!
epoch time: 11.788

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 139
rank avg (pred): 0.329 +- 0.327
mrr vals (pred, true): 0.196, 0.017
batch losses (mrrl, rdl): 0.2132511437, 0.0002534703

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1027
rank avg (pred): 0.255 +- 0.187
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0282736104, 0.0008470915

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 237
rank avg (pred): 0.407 +- 0.158
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 9.4984e-06, 0.0002129495

Epoch over!
epoch time: 12.006

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 710
rank avg (pred): 0.412 +- 0.155
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 6.20477e-05, 0.0001369941

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 542
rank avg (pred): 0.403 +- 0.167
mrr vals (pred, true): 0.054, 0.095
batch losses (mrrl, rdl): 0.0001314048, 0.0005610615

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 526
rank avg (pred): 0.406 +- 0.142
mrr vals (pred, true): 0.042, 0.061
batch losses (mrrl, rdl): 0.0006458387, 0.0005325692

Epoch over!
epoch time: 11.908

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.374 +- 0.164
mrr vals (pred, true): 0.056, 0.128
batch losses (mrrl, rdl): 0.0519434512, 0.0007169488

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.255 +- 0.257
mrr vals (pred, true): 0.120, 0.000
batch losses (mrrl, rdl): 0.0483269244, 0.0008745919

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 767
rank avg (pred): 0.393 +- 0.281
mrr vals (pred, true): 0.094, 0.171
batch losses (mrrl, rdl): 0.0594882481, 0.0007830583

Epoch over!
epoch time: 11.846

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.095 +- 0.229
mrr vals (pred, true): 0.219, 0.209
batch losses (mrrl, rdl): 0.0011060481, 1.09332e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1173
rank avg (pred): 0.362 +- 0.145
mrr vals (pred, true): 0.052, 0.069
batch losses (mrrl, rdl): 3.98088e-05, 0.000139717

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.334 +- 0.133
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 3.36e-08, 0.0006212593

Epoch over!
epoch time: 11.888

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.346 +- 0.138
mrr vals (pred, true): 0.050, 0.041
batch losses (mrrl, rdl): 3.343e-07, 0.0002721004

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 188
rank avg (pred): 0.336 +- 0.151
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0005331998, 0.0003084767

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.137 +- 0.137
mrr vals (pred, true): 0.191, 0.224
batch losses (mrrl, rdl): 0.0106084589, 0.0001353999

Epoch over!
epoch time: 11.889

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 216
rank avg (pred): 0.318 +- 0.131
mrr vals (pred, true): 0.035, 0.000
batch losses (mrrl, rdl): 0.0022912545, 0.0006031936

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1123
rank avg (pred): 0.191 +- 0.162
mrr vals (pred, true): 0.140, 0.000
batch losses (mrrl, rdl): 0.0805950165, 0.0018341321

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.389 +- 0.287
mrr vals (pred, true): 0.103, 0.001
batch losses (mrrl, rdl): 0.0280043129, 0.0001018045

Epoch over!
epoch time: 12.057

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 634
rank avg (pred): 0.324 +- 0.139
mrr vals (pred, true): 0.045, 0.009
batch losses (mrrl, rdl): 0.0002884629, 7.91443e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.276 +- 0.157
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004254338, 0.0009962721

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 473
rank avg (pred): 0.262 +- 0.145
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004361357, 0.0009300041

Epoch over!
epoch time: 11.967

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 509
rank avg (pred): 0.303 +- 0.200
mrr vals (pred, true): 0.066, 0.098
batch losses (mrrl, rdl): 0.0026690499, 0.000109381

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 675
rank avg (pred): 0.384 +- 0.218
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0007515344, 0.000301455

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 676
rank avg (pred): 0.390 +- 0.254
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.0013307376, 0.0005116307

Epoch over!
epoch time: 11.842

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1094
rank avg (pred): 0.253 +- 0.311
mrr vals (pred, true): 0.113, 0.190
batch losses (mrrl, rdl): 0.0590652227, 4.81918e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 870
rank avg (pred): 0.334 +- 0.309
mrr vals (pred, true): 0.095, 0.001
batch losses (mrrl, rdl): 0.0205061361, 0.0004418329

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 531
rank avg (pred): 0.239 +- 0.229
mrr vals (pred, true): 0.062, 0.039
batch losses (mrrl, rdl): 0.0013954098, 6.19536e-05

Epoch over!
epoch time: 11.98

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 457
rank avg (pred): 0.324 +- 0.273
mrr vals (pred, true): 0.056, 0.002
batch losses (mrrl, rdl): 0.0003853814, 0.0004021568

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.253 +- 0.310
mrr vals (pred, true): 0.105, 0.000
batch losses (mrrl, rdl): 0.0299353749, 0.000960856

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 28
rank avg (pred): 0.154 +- 0.130
mrr vals (pred, true): 0.064, 0.083
batch losses (mrrl, rdl): 0.0018569096, 7.19043e-05

Epoch over!
epoch time: 12.161

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.383 +- 0.241
mrr vals (pred, true): 0.042, 0.027

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   52 	     0 	 0.05395 	 0.00021 	 m..s
  101 	     1 	 0.09986 	 0.00022 	 m..s
   29 	     2 	 0.04461 	 0.00022 	 m..s
   92 	     3 	 0.09709 	 0.00023 	 m..s
   23 	     4 	 0.04338 	 0.00024 	 m..s
   89 	     5 	 0.09348 	 0.00024 	 m..s
   92 	     6 	 0.09709 	 0.00024 	 m..s
   18 	     7 	 0.04234 	 0.00025 	 m..s
   92 	     8 	 0.09709 	 0.00025 	 m..s
   32 	     9 	 0.04660 	 0.00026 	 m..s
    3 	    10 	 0.03797 	 0.00026 	 m..s
   40 	    11 	 0.04935 	 0.00026 	 m..s
   91 	    12 	 0.09589 	 0.00027 	 m..s
   21 	    13 	 0.04288 	 0.00028 	 m..s
   19 	    14 	 0.04234 	 0.00028 	 m..s
   47 	    15 	 0.05224 	 0.00028 	 m..s
   92 	    16 	 0.09709 	 0.00028 	 m..s
   25 	    17 	 0.04354 	 0.00029 	 m..s
   30 	    18 	 0.04577 	 0.00031 	 m..s
   78 	    19 	 0.08180 	 0.00032 	 m..s
   54 	    20 	 0.05529 	 0.00032 	 m..s
    0 	    21 	 0.03724 	 0.00034 	 m..s
   11 	    22 	 0.04076 	 0.00037 	 m..s
   92 	    23 	 0.09709 	 0.00041 	 m..s
   46 	    24 	 0.05162 	 0.00042 	 m..s
   27 	    25 	 0.04413 	 0.00044 	 m..s
   82 	    26 	 0.08524 	 0.00054 	 m..s
   44 	    27 	 0.05136 	 0.00060 	 m..s
   22 	    28 	 0.04308 	 0.00061 	 m..s
   59 	    29 	 0.05781 	 0.00061 	 m..s
    8 	    30 	 0.04044 	 0.00072 	 m..s
   20 	    31 	 0.04243 	 0.00073 	 m..s
   45 	    32 	 0.05140 	 0.00076 	 m..s
   26 	    33 	 0.04359 	 0.00088 	 m..s
   34 	    34 	 0.04795 	 0.00160 	 m..s
   31 	    35 	 0.04581 	 0.00171 	 m..s
    4 	    36 	 0.03828 	 0.00177 	 m..s
   14 	    37 	 0.04109 	 0.00186 	 m..s
   74 	    38 	 0.07476 	 0.00203 	 m..s
   33 	    39 	 0.04761 	 0.00270 	 m..s
   13 	    40 	 0.04106 	 0.00349 	 m..s
   15 	    41 	 0.04190 	 0.00535 	 m..s
    1 	    42 	 0.03744 	 0.01117 	 ~...
    2 	    43 	 0.03760 	 0.01251 	 ~...
    9 	    44 	 0.04058 	 0.01695 	 ~...
   49 	    45 	 0.05264 	 0.02036 	 m..s
   17 	    46 	 0.04219 	 0.02723 	 ~...
    7 	    47 	 0.03991 	 0.02804 	 ~...
   10 	    48 	 0.04074 	 0.03172 	 ~...
    5 	    49 	 0.03922 	 0.03183 	 ~...
   24 	    50 	 0.04339 	 0.03621 	 ~...
    6 	    51 	 0.03972 	 0.03972 	 ~...
   12 	    52 	 0.04081 	 0.04655 	 ~...
   53 	    53 	 0.05460 	 0.04846 	 ~...
   38 	    54 	 0.04869 	 0.04971 	 ~...
   50 	    55 	 0.05290 	 0.05241 	 ~...
   58 	    56 	 0.05719 	 0.05469 	 ~...
   55 	    57 	 0.05555 	 0.05722 	 ~...
   16 	    58 	 0.04196 	 0.05908 	 ~...
   48 	    59 	 0.05243 	 0.06161 	 ~...
   56 	    60 	 0.05593 	 0.06283 	 ~...
   35 	    61 	 0.04807 	 0.06345 	 ~...
   39 	    62 	 0.04891 	 0.06746 	 ~...
   60 	    63 	 0.05789 	 0.06984 	 ~...
   62 	    64 	 0.06072 	 0.07101 	 ~...
   63 	    65 	 0.06137 	 0.07108 	 ~...
  106 	    66 	 0.12574 	 0.07414 	 m..s
   43 	    67 	 0.05102 	 0.07562 	 ~...
   42 	    68 	 0.05095 	 0.07569 	 ~...
   28 	    69 	 0.04426 	 0.07838 	 m..s
   37 	    70 	 0.04866 	 0.07914 	 m..s
   36 	    71 	 0.04814 	 0.08120 	 m..s
   73 	    72 	 0.07341 	 0.08153 	 ~...
   64 	    73 	 0.06204 	 0.08155 	 ~...
   69 	    74 	 0.06726 	 0.08571 	 ~...
   61 	    75 	 0.06040 	 0.08777 	 ~...
  111 	    76 	 0.15251 	 0.09105 	 m..s
   41 	    77 	 0.05029 	 0.09210 	 m..s
   66 	    78 	 0.06357 	 0.09210 	 ~...
  105 	    79 	 0.11354 	 0.09525 	 ~...
  104 	    80 	 0.11306 	 0.09692 	 ~...
  109 	    81 	 0.14963 	 0.09708 	 m..s
  110 	    82 	 0.15177 	 0.09849 	 m..s
   75 	    83 	 0.07697 	 0.09974 	 ~...
   67 	    84 	 0.06604 	 0.10101 	 m..s
   68 	    85 	 0.06707 	 0.10106 	 m..s
  103 	    86 	 0.11225 	 0.10204 	 ~...
   72 	    87 	 0.07314 	 0.10243 	 ~...
   51 	    88 	 0.05294 	 0.10378 	 m..s
   71 	    89 	 0.07222 	 0.10485 	 m..s
   65 	    90 	 0.06319 	 0.11007 	 m..s
   77 	    91 	 0.08158 	 0.11015 	 ~...
   76 	    92 	 0.07902 	 0.11054 	 m..s
  102 	    93 	 0.10278 	 0.11095 	 ~...
   70 	    94 	 0.07127 	 0.11232 	 m..s
   87 	    95 	 0.08875 	 0.11589 	 ~...
   80 	    96 	 0.08248 	 0.12259 	 m..s
   57 	    97 	 0.05652 	 0.12913 	 m..s
   83 	    98 	 0.08533 	 0.13225 	 m..s
   86 	    99 	 0.08712 	 0.13242 	 m..s
  107 	   100 	 0.13000 	 0.13936 	 ~...
   92 	   101 	 0.09709 	 0.14636 	 m..s
   85 	   102 	 0.08703 	 0.15352 	 m..s
   88 	   103 	 0.09229 	 0.15654 	 m..s
   81 	   104 	 0.08281 	 0.15702 	 m..s
   92 	   105 	 0.09709 	 0.15989 	 m..s
   92 	   106 	 0.09709 	 0.16173 	 m..s
   84 	   107 	 0.08597 	 0.16715 	 m..s
   92 	   108 	 0.09709 	 0.16968 	 m..s
   90 	   109 	 0.09376 	 0.17079 	 m..s
   79 	   110 	 0.08223 	 0.17136 	 m..s
  112 	   111 	 0.15891 	 0.18278 	 ~...
  115 	   112 	 0.21386 	 0.18863 	 ~...
  116 	   113 	 0.21639 	 0.19456 	 ~...
  108 	   114 	 0.14158 	 0.20305 	 m..s
  113 	   115 	 0.19051 	 0.20934 	 ~...
  117 	   116 	 0.22239 	 0.22899 	 ~...
  114 	   117 	 0.19479 	 0.23952 	 m..s
  118 	   118 	 0.24382 	 0.26518 	 ~...
  119 	   119 	 0.28319 	 0.28660 	 ~...
  120 	   120 	 0.28411 	 0.29621 	 ~...
==========================================
r_mrr = 0.7668420076370239
r2_mrr = 0.5707564353942871
spearmanr_mrr@5 = 0.9821054935455322
spearmanr_mrr@10 = 0.9694703221321106
spearmanr_mrr@50 = 0.9718337655067444
spearmanr_mrr@100 = 0.9398773312568665
spearmanr_mrr@All = 0.9438138604164124
==========================================
test time: 0.391
Done Testing dataset DBpedia50
total time taken: 183.08179116249084
training time taken: 178.5161647796631
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7668)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.5708)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9821)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9695)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9718)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9399)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9438)}}, 'test_loss': {'TransE': {'DBpedia50': 1.562264735046483}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 9832682499863930
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [945, 705, 91, 353, 193, 70, 587, 172, 630, 552, 500, 1093, 675, 581, 634, 351, 1116, 1031, 742, 504, 1194, 1029, 629, 1052, 97, 13, 888, 325, 19, 916, 697, 1023, 726, 1190, 1103, 408, 813, 287, 334, 731, 188, 883, 282, 703, 1135, 436, 386, 1126, 223, 230, 419, 789, 520, 340, 971, 851, 624, 1098, 61, 656, 521, 935, 260, 750, 55, 1017, 574, 128, 565, 297, 593, 542, 298, 1200, 421, 202, 842, 969, 145, 235, 761, 155, 66, 1072, 740, 1136, 606, 1111, 544, 101, 1034, 1204, 40, 492, 1077, 528, 217, 153, 657, 641, 1125, 390, 288, 1189, 371, 860, 724, 157, 51, 470, 445, 557, 1076, 163, 98, 499, 835, 10, 941, 269, 931]
valid_ids (0): []
train_ids (1094): [1048, 130, 572, 181, 1130, 125, 48, 225, 812, 820, 1011, 613, 514, 320, 144, 948, 625, 843, 1144, 413, 618, 264, 678, 921, 1008, 978, 579, 147, 1012, 914, 263, 392, 929, 361, 1037, 1114, 1210, 957, 1172, 330, 135, 982, 592, 444, 471, 468, 988, 702, 122, 881, 26, 1181, 212, 938, 307, 209, 1146, 75, 876, 178, 598, 126, 901, 1080, 597, 161, 1120, 631, 1083, 224, 694, 1207, 633, 214, 18, 563, 554, 467, 1075, 693, 380, 576, 752, 1100, 808, 666, 176, 794, 335, 1184, 547, 1068, 517, 567, 676, 900, 966, 787, 821, 186, 459, 295, 1140, 551, 1159, 268, 838, 973, 1133, 1117, 302, 893, 252, 691, 715, 160, 979, 897, 614, 642, 932, 138, 1199, 706, 133, 928, 956, 99, 684, 543, 885, 823, 985, 937, 1063, 1058, 1145, 206, 824, 774, 939, 469, 1137, 560, 725, 1059, 387, 1016, 45, 241, 1128, 910, 874, 1175, 1055, 68, 204, 911, 365, 14, 964, 24, 665, 132, 924, 233, 171, 717, 853, 785, 936, 350, 513, 140, 426, 58, 783, 1148, 285, 1187, 1009, 695, 748, 33, 254, 1060, 1087, 478, 1170, 7, 942, 456, 990, 686, 718, 1209, 1105, 627, 1074, 819, 35, 137, 1078, 420, 457, 947, 1040, 292, 1169, 965, 773, 397, 71, 418, 1092, 707, 34, 1167, 652, 727, 227, 778, 191, 446, 603, 534, 411, 1026, 951, 1155, 170, 972, 617, 1025, 1102, 404, 741, 258, 1044, 738, 647, 103, 815, 505, 757, 1001, 485, 1165, 604, 1101, 210, 541, 185, 612, 635, 739, 42, 331, 1139, 770, 879, 219, 991, 753, 296, 247, 60, 531, 1020, 195, 816, 1010, 615, 17, 305, 602, 643, 799, 136, 466, 360, 359, 927, 904, 182, 248, 115, 1113, 747, 626, 189, 479, 1211, 197, 311, 493, 1065, 246, 120, 584, 700, 890, 638, 840, 655, 756, 352, 620, 699, 646, 388, 69, 555, 134, 149, 348, 1041, 414, 434, 121, 271, 46, 889, 274, 293, 681, 95, 396, 74, 958, 280, 150, 790, 600, 332, 3, 146, 165, 1050, 1153, 711, 765, 1066, 428, 215, 1115, 915, 1123, 423, 925, 1180, 976, 222, 784, 743, 515, 501, 522, 780, 39, 211, 461, 1182, 569, 312, 997, 830, 100, 272, 429, 668, 451, 205, 1166, 848, 416, 1193, 796, 409, 90, 253, 917, 758, 975, 124, 465, 1196, 127, 2, 628, 1108, 968, 301, 775, 1150, 594, 119, 381, 131, 619, 198, 1160, 868, 512, 431, 545, 59, 909, 277, 930, 558, 229, 37, 403, 410, 1195, 907, 173, 525, 810, 218, 207, 535, 1149, 1192, 113, 950, 1019, 800, 807, 1067, 480, 63, 719, 1110, 992, 452, 76, 107, 234, 52, 795, 1039, 483, 129, 1158, 304, 412, 494, 827, 22, 237, 993, 685, 832, 886, 637, 1062, 855, 463, 899, 1178, 856, 339, 393, 88, 401, 1038, 77, 803, 142, 1035, 549, 167, 1081, 802, 1082, 766, 1032, 326, 508, 376, 908, 310, 1164, 317, 1061, 148, 329, 363, 829, 786, 980, 266, 1006, 1201, 1179, 432, 828, 902, 961, 1099, 257, 817, 1047, 995, 861, 919, 220, 546, 1173, 1107, 994, 1198, 357, 105, 611, 1147, 1154, 203, 487, 384, 448, 873, 709, 474, 481, 806, 255, 1097, 1, 869, 940, 506, 284, 123, 996, 651, 85, 720, 162, 658, 291, 1057, 519, 156, 112, 1171, 400, 44, 1091, 183, 54, 364, 782, 383, 422, 184, 788, 654, 477, 850, 511, 745, 918, 151, 427, 455, 841, 349, 43, 308, 309, 509, 847, 1015, 722, 109, 454, 953, 779, 952, 366, 949, 962, 299, 346, 208, 49, 28, 649, 143, 623, 690, 755, 674, 496, 548, 1090, 116, 912, 1127, 894, 355, 36, 159, 267, 259, 1177, 289, 83, 67, 672, 529, 871, 0, 721, 273, 1138, 696, 1152, 84, 56, 645, 523, 677, 797, 347, 1106, 243, 1096, 878, 337, 687, 863, 596, 72, 472, 437, 844, 80, 379, 50, 590, 342, 903, 236, 497, 683, 564, 875, 1186, 805, 29, 462, 998, 370, 859, 663, 1124, 793, 729, 781, 977, 834, 967, 488, 1021, 846, 884, 1049, 1208, 226, 510, 194, 15, 516, 290, 591, 1094, 1079, 11, 92, 905, 1022, 23, 1112, 327, 21, 435, 764, 728, 944, 1161, 923, 368, 440, 1205, 760, 692, 250, 792, 450, 303, 751, 262, 744, 524, 216, 716, 417, 877, 689, 960, 502, 1122, 256, 460, 891, 880, 1007, 405, 1071, 710, 640, 1141, 837, 275, 385, 1191, 660, 659, 870, 5, 6, 179, 1004, 213, 804, 653, 582, 1003, 599, 391, 1176, 1163, 857, 491, 333, 527, 374, 96, 532, 141, 586, 698, 41, 959, 57, 1131, 322, 166, 441, 389, 324, 749, 1119, 244, 164, 589, 858, 550, 458, 825, 1156, 196, 946, 588, 1064, 369, 169, 152, 490, 769, 736, 553, 270, 1168, 906, 1212, 1045, 238, 540, 679, 1013, 1183, 1024, 737, 556, 814, 375, 809, 1143, 763, 754, 896, 605, 168, 664, 154, 27, 854, 12, 87, 73, 192, 1197, 1121, 1056, 1151, 622, 319, 714, 47, 704, 670, 449, 771, 776, 378, 447, 338, 580, 9, 433, 1042, 836, 86, 245, 583, 94, 595, 661, 970, 1089, 200, 314, 983, 306, 791, 538, 1018, 475, 733, 425, 251, 1028, 16, 1086, 31, 887, 818, 1188, 577, 453, 723, 673, 1084, 680, 1053, 1033, 495, 221, 872, 377, 682, 530, 328, 438, 845, 1206, 323, 486, 424, 954, 852, 568, 610, 621, 313, 316, 1174, 526, 484, 65, 104, 518, 507, 895, 669, 81, 402, 864, 53, 667, 406, 826, 866, 578, 231, 228, 345, 25, 32, 180, 442, 701, 1070, 943, 867, 1109, 430, 239, 616, 1142, 82, 240, 913, 158, 367, 934, 734, 399, 38, 114, 232, 798, 1185, 372, 648, 498, 111, 394, 585, 89, 632, 831, 1043, 1162, 1129, 1213, 920, 898, 759, 767, 639, 315, 892, 688, 671, 362, 1046, 281, 341, 566, 536, 283, 1069, 1000, 1014, 708, 443, 8, 30, 382, 1073, 561, 199, 318, 1134, 730, 539, 822, 772, 862, 321, 356, 1095, 833, 768, 571, 1036, 746, 981, 93, 415, 999, 102, 1088, 1002, 106, 358, 644, 732, 201, 955, 265, 1005, 713, 117, 575, 559, 190, 926, 1085, 1027, 489, 503, 1132, 62, 242, 300, 118, 482, 279, 294, 398, 762, 636, 989, 849, 984, 1118, 261, 1203, 662, 1202, 4, 476, 175, 79, 249, 974, 1051, 735, 64, 601, 354, 963, 407, 608, 286, 987, 336, 276, 865, 607, 609, 177, 1104, 712, 78, 801, 573, 933, 344, 562, 395, 986, 1030, 108, 570, 110, 464, 278, 882, 187, 777, 1054, 473, 1214, 20, 922, 533, 439, 373, 811, 174, 343, 839, 139, 1157, 537, 650]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2862054390058037
the save name prefix for this run is:  chkpt-ID_2862054390058037_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 563
rank avg (pred): 0.518 +- 0.002
mrr vals (pred, true): 0.000, 0.087
batch losses (mrrl, rdl): 0.0, 0.001476763

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 763
rank avg (pred): 0.346 +- 0.259
mrr vals (pred, true): 0.123, 0.108
batch losses (mrrl, rdl): 0.0, 0.0003243707

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 173
rank avg (pred): 0.351 +- 0.280
mrr vals (pred, true): 0.227, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002462477

Epoch over!
epoch time: 11.983

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.344 +- 0.275
mrr vals (pred, true): 0.232, 0.002
batch losses (mrrl, rdl): 0.0, 8.17863e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 261
rank avg (pred): 0.095 +- 0.077
mrr vals (pred, true): 0.324, 0.086
batch losses (mrrl, rdl): 0.0, 2.28471e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 906
rank avg (pred): 0.147 +- 0.119
mrr vals (pred, true): 0.286, 0.145
batch losses (mrrl, rdl): 0.0, 8.97021e-05

Epoch over!
epoch time: 11.72

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1154
rank avg (pred): 0.213 +- 0.172
mrr vals (pred, true): 0.291, 0.104
batch losses (mrrl, rdl): 0.0, 8.31363e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.354 +- 0.283
mrr vals (pred, true): 0.240, 0.003
batch losses (mrrl, rdl): 0.0, 8.01129e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.344 +- 0.275
mrr vals (pred, true): 0.296, 0.203
batch losses (mrrl, rdl): 0.0, 0.0007220696

Epoch over!
epoch time: 11.797

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 2
rank avg (pred): 0.094 +- 0.076
mrr vals (pred, true): 0.350, 0.135
batch losses (mrrl, rdl): 0.0, 3.35954e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1120
rank avg (pred): 0.334 +- 0.268
mrr vals (pred, true): 0.271, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001405952

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 160
rank avg (pred): 0.346 +- 0.277
mrr vals (pred, true): 0.332, 0.025
batch losses (mrrl, rdl): 0.0, 0.0004549136

Epoch over!
epoch time: 11.674

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 366
rank avg (pred): 0.335 +- 0.268
mrr vals (pred, true): 0.322, 0.049
batch losses (mrrl, rdl): 0.0, 0.0002908329

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 20
rank avg (pred): 0.080 +- 0.064
mrr vals (pred, true): 0.380, 0.219
batch losses (mrrl, rdl): 0.0, 1.34063e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.328 +- 0.263
mrr vals (pred, true): 0.306, 0.171
batch losses (mrrl, rdl): 0.0, 0.0004406092

Epoch over!
epoch time: 11.769

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 549
rank avg (pred): 0.256 +- 0.205
mrr vals (pred, true): 0.349, 0.042
batch losses (mrrl, rdl): 0.8912377954, 9.40415e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.273 +- 0.188
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0031916401, 0.0006792712

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 205
rank avg (pred): 0.430 +- 0.209
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0006042209, 8.98523e-05

Epoch over!
epoch time: 12.176

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 882
rank avg (pred): 0.311 +- 0.203
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0026784553, 0.0004898779

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.429 +- 0.197
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0010933431, 6.8705e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.284, 0.295
batch losses (mrrl, rdl): 0.0011805929, 7.15277e-05

Epoch over!
epoch time: 12.007

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.441 +- 0.170
mrr vals (pred, true): 0.059, 0.162
batch losses (mrrl, rdl): 0.1069670841, 0.0013368964

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 302
rank avg (pred): 0.092 +- 0.061
mrr vals (pred, true): 0.119, 0.100
batch losses (mrrl, rdl): 0.0036314619, 3.54602e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 251
rank avg (pred): 0.045 +- 0.029
mrr vals (pred, true): 0.123, 0.110
batch losses (mrrl, rdl): 0.0015316342, 0.0001314846

Epoch over!
epoch time: 11.978

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 506
rank avg (pred): 0.382 +- 0.201
mrr vals (pred, true): 0.062, 0.109
batch losses (mrrl, rdl): 0.0223104265, 0.0006220386

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.443 +- 0.127
mrr vals (pred, true): 0.053, 0.003
batch losses (mrrl, rdl): 6.55132e-05, 0.0003203524

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 250
rank avg (pred): 0.311 +- 0.183
mrr vals (pred, true): 0.080, 0.082
batch losses (mrrl, rdl): 0.0091870613, 0.0008442831

Epoch over!
epoch time: 11.941

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 772
rank avg (pred): 0.213 +- 0.133
mrr vals (pred, true): 0.093, 0.164
batch losses (mrrl, rdl): 0.0492916629, 8.97331e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.417 +- 0.136
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009762616, 0.0001571283

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.408 +- 0.130
mrr vals (pred, true): 0.058, 0.008
batch losses (mrrl, rdl): 0.0005937551, 0.0004283478

Epoch over!
epoch time: 12.1

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.326 +- 0.194
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0063246498, 0.0004976002

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.371 +- 0.145
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022377647, 0.0003362743

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.197 +- 0.156
mrr vals (pred, true): 0.113, 0.201
batch losses (mrrl, rdl): 0.0776034594, 6.05742e-05

Epoch over!
epoch time: 12.092

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.036 +- 0.030
mrr vals (pred, true): 0.170, 0.224
batch losses (mrrl, rdl): 0.0289615598, 6.70737e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 84
rank avg (pred): 0.409 +- 0.127
mrr vals (pred, true): 0.050, 0.015
batch losses (mrrl, rdl): 3.369e-07, 0.0003721411

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.379 +- 0.146
mrr vals (pred, true): 0.060, 0.116
batch losses (mrrl, rdl): 0.0315742418, 0.0007954712

Epoch over!
epoch time: 11.91

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 986
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.197, 0.286
batch losses (mrrl, rdl): 0.0777170882, 6.45264e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.198 +- 0.138
mrr vals (pred, true): 0.118, 0.098
batch losses (mrrl, rdl): 0.0463279784, 8.01074e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.072 +- 0.051
mrr vals (pred, true): 0.133, 0.207
batch losses (mrrl, rdl): 0.0559357777, 0.0003386165

Epoch over!
epoch time: 12.143

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 64
rank avg (pred): 0.341 +- 0.169
mrr vals (pred, true): 0.070, 0.076
batch losses (mrrl, rdl): 0.0041826251, 0.0011694815

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.388 +- 0.129
mrr vals (pred, true): 0.056, 0.093
batch losses (mrrl, rdl): 0.0003723372, 0.0008399169

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.311 +- 0.187
mrr vals (pred, true): 0.092, 0.139
batch losses (mrrl, rdl): 0.0222265739, 0.0004531356

Epoch over!
epoch time: 11.861

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 247
rank avg (pred): 0.309 +- 0.185
mrr vals (pred, true): 0.086, 0.128
batch losses (mrrl, rdl): 0.0176264923, 0.0007670742

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.404 +- 0.108
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.51903e-05, 0.0001849947

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1059
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.298, 0.310
batch losses (mrrl, rdl): 0.0015284786, 3.60503e-05

Epoch over!
epoch time: 11.982

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.302 +- 0.190
mrr vals (pred, true): 0.102, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   35 	     0 	 0.05368 	 0.00019 	 m..s
   37 	     1 	 0.05420 	 0.00019 	 m..s
  100 	     2 	 0.11580 	 0.00019 	 MISS
   81 	     3 	 0.09153 	 0.00020 	 m..s
   64 	     4 	 0.07064 	 0.00020 	 m..s
   58 	     5 	 0.06230 	 0.00020 	 m..s
   30 	     6 	 0.05263 	 0.00021 	 m..s
  104 	     7 	 0.12113 	 0.00022 	 MISS
   27 	     8 	 0.05172 	 0.00023 	 m..s
  110 	     9 	 0.13230 	 0.00023 	 MISS
   16 	    10 	 0.04908 	 0.00024 	 m..s
   84 	    11 	 0.09268 	 0.00025 	 m..s
   77 	    12 	 0.08607 	 0.00025 	 m..s
   43 	    13 	 0.05767 	 0.00026 	 m..s
   89 	    14 	 0.09944 	 0.00026 	 m..s
  109 	    15 	 0.13160 	 0.00027 	 MISS
    2 	    16 	 0.04842 	 0.00028 	 m..s
   48 	    17 	 0.05863 	 0.00028 	 m..s
   47 	    18 	 0.05863 	 0.00028 	 m..s
  108 	    19 	 0.12295 	 0.00028 	 MISS
    0 	    20 	 0.04831 	 0.00031 	 m..s
   95 	    21 	 0.10415 	 0.00032 	 MISS
   71 	    22 	 0.07448 	 0.00034 	 m..s
   46 	    23 	 0.05822 	 0.00034 	 m..s
   90 	    24 	 0.10166 	 0.00034 	 MISS
   90 	    25 	 0.10166 	 0.00036 	 MISS
   49 	    26 	 0.05867 	 0.00038 	 m..s
   28 	    27 	 0.05194 	 0.00039 	 m..s
   11 	    28 	 0.04899 	 0.00040 	 m..s
   38 	    29 	 0.05480 	 0.00041 	 m..s
   96 	    30 	 0.10544 	 0.00049 	 MISS
   17 	    31 	 0.04919 	 0.00050 	 m..s
   44 	    32 	 0.05774 	 0.00051 	 m..s
    8 	    33 	 0.04894 	 0.00052 	 m..s
   83 	    34 	 0.09254 	 0.00053 	 m..s
  105 	    35 	 0.12229 	 0.00058 	 MISS
   52 	    36 	 0.05888 	 0.00060 	 m..s
   53 	    37 	 0.05963 	 0.00061 	 m..s
    5 	    38 	 0.04865 	 0.00067 	 m..s
   24 	    39 	 0.05103 	 0.00072 	 m..s
   42 	    40 	 0.05757 	 0.00100 	 m..s
   15 	    41 	 0.04908 	 0.00102 	 m..s
    1 	    42 	 0.04833 	 0.00121 	 m..s
    3 	    43 	 0.04856 	 0.00157 	 m..s
    4 	    44 	 0.04864 	 0.00237 	 m..s
    9 	    45 	 0.04897 	 0.00451 	 m..s
    6 	    46 	 0.04874 	 0.00871 	 m..s
   40 	    47 	 0.05723 	 0.01256 	 m..s
   41 	    48 	 0.05751 	 0.01785 	 m..s
   33 	    49 	 0.05305 	 0.01837 	 m..s
    7 	    50 	 0.04880 	 0.02020 	 ~...
   45 	    51 	 0.05805 	 0.02041 	 m..s
   50 	    52 	 0.05869 	 0.03232 	 ~...
   13 	    53 	 0.04906 	 0.03317 	 ~...
   18 	    54 	 0.04927 	 0.03447 	 ~...
   19 	    55 	 0.04956 	 0.03475 	 ~...
   32 	    56 	 0.05294 	 0.03613 	 ~...
   29 	    57 	 0.05199 	 0.03948 	 ~...
   10 	    58 	 0.04898 	 0.04061 	 ~...
   20 	    59 	 0.04963 	 0.04199 	 ~...
   14 	    60 	 0.04907 	 0.04716 	 ~...
   26 	    61 	 0.05172 	 0.04743 	 ~...
   31 	    62 	 0.05291 	 0.04749 	 ~...
   34 	    63 	 0.05357 	 0.05021 	 ~...
   59 	    64 	 0.06369 	 0.05213 	 ~...
   54 	    65 	 0.05970 	 0.05241 	 ~...
   36 	    66 	 0.05386 	 0.05334 	 ~...
   12 	    67 	 0.04903 	 0.05391 	 ~...
   67 	    68 	 0.07191 	 0.05573 	 ~...
   62 	    69 	 0.06762 	 0.05661 	 ~...
   55 	    70 	 0.05985 	 0.06079 	 ~...
   57 	    71 	 0.06148 	 0.06283 	 ~...
   73 	    72 	 0.08024 	 0.06343 	 ~...
   80 	    73 	 0.08947 	 0.06519 	 ~...
   68 	    74 	 0.07386 	 0.06844 	 ~...
   69 	    75 	 0.07387 	 0.06992 	 ~...
   61 	    76 	 0.06756 	 0.07253 	 ~...
   72 	    77 	 0.07501 	 0.07284 	 ~...
   65 	    78 	 0.07149 	 0.07523 	 ~...
   63 	    79 	 0.07043 	 0.07718 	 ~...
   21 	    80 	 0.05048 	 0.07950 	 ~...
   60 	    81 	 0.06636 	 0.08101 	 ~...
   22 	    82 	 0.05066 	 0.08123 	 m..s
   25 	    83 	 0.05146 	 0.08200 	 m..s
  106 	    84 	 0.12268 	 0.08454 	 m..s
  101 	    85 	 0.11616 	 0.08491 	 m..s
   23 	    86 	 0.05097 	 0.09506 	 m..s
   70 	    87 	 0.07390 	 0.10101 	 ~...
   66 	    88 	 0.07180 	 0.10319 	 m..s
   51 	    89 	 0.05884 	 0.10378 	 m..s
   74 	    90 	 0.08065 	 0.10583 	 ~...
   56 	    91 	 0.06052 	 0.10888 	 m..s
   39 	    92 	 0.05604 	 0.11007 	 m..s
   97 	    93 	 0.10825 	 0.11102 	 ~...
   86 	    94 	 0.09409 	 0.11415 	 ~...
   75 	    95 	 0.08240 	 0.12804 	 m..s
  113 	    96 	 0.16303 	 0.12878 	 m..s
   79 	    97 	 0.08871 	 0.13405 	 m..s
   78 	    98 	 0.08799 	 0.13635 	 m..s
   99 	    99 	 0.11330 	 0.13901 	 ~...
  107 	   100 	 0.12271 	 0.14086 	 ~...
   76 	   101 	 0.08536 	 0.14721 	 m..s
  111 	   102 	 0.14197 	 0.15287 	 ~...
   90 	   103 	 0.10166 	 0.15654 	 m..s
   85 	   104 	 0.09292 	 0.15681 	 m..s
   88 	   105 	 0.09789 	 0.15753 	 m..s
   94 	   106 	 0.10220 	 0.15872 	 m..s
   90 	   107 	 0.10166 	 0.15970 	 m..s
  102 	   108 	 0.11969 	 0.16283 	 m..s
  103 	   109 	 0.12009 	 0.16883 	 m..s
  112 	   110 	 0.14617 	 0.17817 	 m..s
  114 	   111 	 0.17736 	 0.18310 	 ~...
   82 	   112 	 0.09242 	 0.19752 	 MISS
   98 	   113 	 0.10954 	 0.19776 	 m..s
   87 	   114 	 0.09561 	 0.20429 	 MISS
  115 	   115 	 0.18051 	 0.21184 	 m..s
  116 	   116 	 0.22265 	 0.21882 	 ~...
  119 	   117 	 0.25097 	 0.22899 	 ~...
  118 	   118 	 0.23615 	 0.23952 	 ~...
  117 	   119 	 0.23325 	 0.27102 	 m..s
  120 	   120 	 0.29276 	 0.30943 	 ~...
==========================================
r_mrr = 0.7012509703636169
r2_mrr = 0.41223591566085815
spearmanr_mrr@5 = 0.979762077331543
spearmanr_mrr@10 = 0.9405099749565125
spearmanr_mrr@50 = 0.9439805150032043
spearmanr_mrr@100 = 0.9491539001464844
spearmanr_mrr@All = 0.9546222686767578
==========================================
test time: 0.389
Done Testing dataset DBpedia50
total time taken: 184.0434045791626
training time taken: 179.60575151443481
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7013)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4122)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9798)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9405)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9440)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9492)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9546)}}, 'test_loss': {'TransE': {'DBpedia50': 1.5771053244970972}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 3734228699327612
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1124, 1039, 1162, 789, 659, 331, 984, 73, 991, 424, 313, 699, 399, 693, 988, 854, 1180, 583, 810, 241, 1131, 392, 879, 429, 362, 436, 1120, 1170, 1002, 498, 1202, 556, 599, 622, 236, 132, 630, 144, 1058, 323, 1074, 637, 408, 343, 896, 1038, 1052, 1005, 49, 844, 989, 378, 41, 434, 402, 94, 2, 356, 178, 613, 96, 1111, 517, 612, 518, 750, 1025, 671, 963, 45, 825, 771, 228, 882, 1021, 481, 28, 1193, 851, 916, 186, 880, 17, 698, 177, 85, 908, 315, 1018, 227, 371, 603, 775, 925, 194, 629, 365, 314, 296, 510, 48, 224, 105, 652, 935, 1023, 1033, 254, 232, 162, 419, 496, 1158, 1119, 61, 868, 134, 508, 700, 970, 618]
valid_ids (0): []
train_ids (1094): [1159, 417, 588, 468, 1118, 864, 731, 898, 772, 62, 67, 195, 871, 153, 798, 115, 119, 631, 244, 758, 274, 109, 621, 1097, 752, 1187, 780, 1214, 322, 739, 861, 1020, 423, 180, 34, 449, 648, 95, 470, 1044, 422, 1081, 139, 40, 509, 1157, 368, 151, 867, 168, 803, 413, 1104, 1205, 900, 366, 51, 68, 142, 229, 1008, 996, 530, 372, 179, 465, 310, 866, 678, 7, 824, 978, 1129, 494, 596, 522, 924, 972, 1156, 158, 110, 1026, 1101, 968, 176, 1206, 552, 12, 1079, 403, 802, 192, 607, 560, 728, 708, 947, 376, 815, 488, 27, 1169, 982, 389, 13, 764, 1068, 735, 471, 906, 873, 350, 456, 369, 1146, 975, 383, 619, 87, 609, 933, 171, 749, 328, 623, 409, 899, 240, 977, 1004, 674, 432, 938, 462, 535, 561, 1078, 155, 1096, 672, 837, 1168, 388, 922, 1128, 527, 1147, 285, 483, 99, 710, 594, 687, 580, 395, 295, 574, 292, 416, 1212, 251, 831, 234, 445, 796, 1133, 643, 71, 357, 847, 966, 932, 415, 452, 971, 732, 628, 1108, 602, 55, 1088, 394, 892, 10, 104, 913, 777, 1053, 526, 493, 321, 474, 489, 33, 1123, 579, 799, 1196, 1048, 459, 1009, 396, 967, 635, 894, 601, 44, 1042, 636, 420, 743, 499, 39, 785, 781, 288, 919, 543, 238, 980, 454, 725, 30, 114, 691, 277, 1019, 88, 248, 734, 647, 1164, 307, 828, 849, 450, 1056, 282, 1063, 1012, 696, 231, 675, 391, 1114, 808, 839, 990, 656, 11, 216, 272, 746, 354, 537, 91, 1036, 720, 727, 976, 787, 220, 495, 877, 206, 221, 412, 129, 327, 742, 472, 941, 89, 853, 353, 212, 547, 414, 278, 170, 548, 878, 1029, 198, 1076, 249, 640, 1102, 421, 1184, 401, 337, 1177, 956, 156, 318, 146, 512, 615, 1186, 715, 358, 386, 726, 721, 102, 223, 918, 529, 289, 143, 137, 536, 1207, 776, 895, 565, 239, 1121, 901, 1110, 3, 70, 111, 161, 576, 539, 1060, 345, 103, 1087, 1103, 572, 410, 586, 1150, 848, 347, 373, 437, 654, 397, 860, 805, 65, 50, 52, 1043, 163, 501, 148, 18, 165, 598, 273, 738, 1024, 657, 890, 411, 36, 820, 202, 1185, 4, 931, 205, 426, 768, 407, 1151, 923, 477, 1051, 390, 1034, 644, 6, 773, 9, 814, 874, 641, 507, 1073, 486, 279, 24, 910, 1006, 605, 361, 237, 521, 519, 766, 538, 1011, 965, 500, 1197, 127, 697, 181, 393, 545, 306, 993, 214, 107, 428, 505, 182, 729, 639, 430, 222, 1194, 1139, 642, 166, 688, 929, 242, 571, 569, 747, 582, 1213, 1199, 845, 774, 490, 1178, 942, 979, 1155, 324, 84, 595, 435, 66, 404, 934, 418, 191, 669, 463, 464, 998, 460, 377, 485, 1149, 722, 902, 308, 1046, 75, 573, 816, 524, 287, 597, 1064, 567, 949, 233, 438, 440, 250, 606, 193, 141, 1109, 427, 384, 855, 779, 1138, 341, 575, 1145, 1045, 937, 14, 112, 21, 487, 125, 1191, 994, 200, 138, 620, 349, 246, 670, 1144, 842, 1166, 568, 885, 1135, 1099, 564, 346, 562, 259, 1122, 56, 443, 338, 632, 317, 1037, 329, 484, 64, 650, 891, 786, 267, 225, 664, 829, 447, 754, 126, 905, 159, 823, 255, 1093, 625, 253, 16, 196, 1134, 912, 907, 930, 352, 888, 340, 86, 1090, 304, 767, 515, 325, 442, 265, 719, 686, 549, 1091, 190, 266, 275, 441, 1100, 917, 903, 1015, 865, 1094, 709, 600, 887, 1075, 841, 291, 1211, 832, 293, 332, 585, 541, 1116, 380, 32, 1016, 199, 1204, 525, 461, 281, 210, 467, 319, 724, 1141, 995, 1201, 544, 843, 433, 184, 744, 260, 948, 1203, 15, 681, 634, 116, 969, 733, 869, 791, 1027, 26, 852, 1165, 666, 90, 268, 245, 627, 473, 359, 577, 1028, 286, 876, 578, 794, 817, 59, 677, 1179, 339, 197, 624, 532, 957, 793, 150, 348, 638, 1054, 174, 381, 961, 145, 203, 718, 626, 661, 952, 312, 492, 1065, 1017, 633, 122, 884, 1085, 911, 542, 1126, 958, 1160, 782, 1195, 1210, 333, 737, 1080, 1092, 658, 169, 448, 1112, 795, 1106, 1163, 665, 757, 128, 962, 858, 207, 875, 835, 673, 425, 270, 1071, 469, 262, 247, 189, 1095, 106, 1188, 534, 97, 491, 503, 8, 904, 264, 797, 320, 859, 1001, 706, 130, 992, 936, 133, 1132, 1148, 589, 283, 19, 256, 723, 550, 1062, 187, 1125, 342, 608, 431, 311, 502, 58, 1003, 497, 1030, 451, 563, 294, 444, 959, 926, 98, 551, 712, 1041, 651, 778, 1113, 570, 707, 1183, 1209, 360, 72, 1175, 881, 684, 334, 1098, 974, 857, 1067, 759, 77, 705, 883, 63, 985, 685, 164, 1172, 1161, 692, 140, 755, 363, 1031, 690, 80, 172, 748, 617, 668, 57, 1049, 783, 31, 973, 475, 101, 188, 889, 351, 1072, 846, 587, 204, 297, 983, 1105, 1032, 316, 740, 124, 20, 921, 1153, 1069, 955, 379, 950, 834, 915, 893, 559, 1040, 219, 711, 42, 154, 25, 761, 717, 986, 714, 1171, 897, 610, 528, 1189, 482, 546, 400, 53, 850, 683, 1115, 1140, 792, 804, 997, 370, 581, 1136, 964, 1089, 590, 1154, 213, 82, 680, 1057, 953, 649, 784, 215, 29, 476, 653, 655, 1000, 290, 299, 682, 614, 466, 330, 593, 252, 157, 1050, 263, 821, 160, 78, 769, 611, 702, 217, 523, 108, 1200, 1082, 1083, 382, 121, 763, 201, 756, 591, 1137, 558, 838, 694, 47, 123, 1198, 800, 480, 939, 822, 208, 807, 504, 173, 584, 1066, 704, 1077, 147, 960, 136, 663, 269, 271, 981, 943, 37, 790, 553, 1035, 439, 284, 554, 385, 336, 801, 1176, 1, 741, 243, 1181, 1014, 730, 1143, 557, 185, 344, 1022, 940, 211, 1055, 826, 1007, 944, 149, 120, 819, 387, 300, 1142, 375, 516, 230, 1084, 280, 679, 809, 646, 592, 836, 1047, 533, 736, 46, 954, 676, 43, 226, 856, 1174, 478, 667, 261, 81, 60, 92, 302, 833, 513, 22, 751, 69, 364, 367, 1086, 209, 753, 540, 1117, 406, 1173, 616, 999, 830, 788, 945, 862, 74, 765, 1182, 301, 695, 812, 303, 1127, 131, 1107, 83, 1013, 914, 1010, 1167, 1208, 920, 827, 117, 305, 453, 660, 76, 183, 1190, 946, 806, 520, 928, 840, 716, 872, 506, 818, 175, 298, 927, 35, 762, 604, 811, 886, 326, 1059, 555, 0, 93, 100, 335, 1152, 258, 566, 398, 79, 405, 38, 458, 257, 446, 167, 703, 235, 662, 1070, 374, 455, 870, 135, 951, 1192, 152, 457, 23, 745, 863, 514, 909, 701, 276, 5, 813, 1061, 987, 113, 760, 355, 218, 531, 479, 1130, 713, 770, 118, 511, 309, 645, 54, 689]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5620701021019421
the save name prefix for this run is:  chkpt-ID_5620701021019421_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 616
rank avg (pred): 0.552 +- 0.005
mrr vals (pred, true): 0.000, 0.005
batch losses (mrrl, rdl): 0.0, 0.0010711829

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 360
rank avg (pred): 0.331 +- 0.263
mrr vals (pred, true): 0.145, 0.048
batch losses (mrrl, rdl): 0.0, 0.0003086609

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 863
rank avg (pred): 0.335 +- 0.274
mrr vals (pred, true): 0.219, 0.159
batch losses (mrrl, rdl): 0.0, 0.0003866541

Epoch over!
epoch time: 11.875

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1088
rank avg (pred): 0.341 +- 0.277
mrr vals (pred, true): 0.221, 0.192
batch losses (mrrl, rdl): 0.0, 0.0007940201

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 147
rank avg (pred): 0.336 +- 0.282
mrr vals (pred, true): 0.275, 0.010
batch losses (mrrl, rdl): 0.0, 0.000216567

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.327 +- 0.282
mrr vals (pred, true): 0.326, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001889569

Epoch over!
epoch time: 11.91

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 66
rank avg (pred): 0.126 +- 0.110
mrr vals (pred, true): 0.371, 0.052
batch losses (mrrl, rdl): 0.0, 3.946e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.341 +- 0.292
mrr vals (pred, true): 0.360, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003800007

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.341 +- 0.293
mrr vals (pred, true): 0.362, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002210508

Epoch over!
epoch time: 11.809

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 479
rank avg (pred): 0.303 +- 0.264
mrr vals (pred, true): 0.383, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002651443

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 545
rank avg (pred): 0.257 +- 0.222
mrr vals (pred, true): 0.388, 0.089
batch losses (mrrl, rdl): 0.0, 7.98288e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 127
rank avg (pred): 0.346 +- 0.300
mrr vals (pred, true): 0.390, 0.029
batch losses (mrrl, rdl): 0.0, 0.0003281169

Epoch over!
epoch time: 11.846

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 949
rank avg (pred): 0.358 +- 0.302
mrr vals (pred, true): 0.380, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001342023

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 417
rank avg (pred): 0.308 +- 0.277
mrr vals (pred, true): 0.403, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004239753

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 44
rank avg (pred): 0.085 +- 0.078
mrr vals (pred, true): 0.406, 0.102
batch losses (mrrl, rdl): 0.0, 1.87837e-05

Epoch over!
epoch time: 11.685

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 453
rank avg (pred): 0.319 +- 0.270
mrr vals (pred, true): 0.391, 0.000
batch losses (mrrl, rdl): 1.1613894701, 0.0001838997

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 614
rank avg (pred): 0.448 +- 0.241
mrr vals (pred, true): 0.043, 0.039
batch losses (mrrl, rdl): 0.000438663, 0.0006268017

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.393 +- 0.264
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0016923875, 0.0001210655

Epoch over!
epoch time: 12.325

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 39
rank avg (pred): 0.330 +- 0.254
mrr vals (pred, true): 0.094, 0.066
batch losses (mrrl, rdl): 0.0193467028, 0.0009180868

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1070
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.331, 0.321
batch losses (mrrl, rdl): 0.0009942604, 6.08962e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.348 +- 0.269
mrr vals (pred, true): 0.110, 0.136
batch losses (mrrl, rdl): 0.0070088799, 0.0016496291

Epoch over!
epoch time: 11.996

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.376 +- 0.258
mrr vals (pred, true): 0.065, 0.077
batch losses (mrrl, rdl): 0.0021222113, 0.0013028632

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 848
rank avg (pred): 0.325 +- 0.261
mrr vals (pred, true): 0.108, 0.162
batch losses (mrrl, rdl): 0.0283832122, 0.0004041751

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1043
rank avg (pred): 0.276 +- 0.215
mrr vals (pred, true): 0.087, 0.002
batch losses (mrrl, rdl): 0.0138456738, 0.0005682926

Epoch over!
epoch time: 11.947

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.432 +- 0.194
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.71515e-05, 9.03169e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 718
rank avg (pred): 0.462 +- 0.168
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0008012849, 4.54781e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.048 +- 0.038
mrr vals (pred, true): 0.155, 0.098
batch losses (mrrl, rdl): 0.11095763, 9.87172e-05

Epoch over!
epoch time: 12.087

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.349 +- 0.228
mrr vals (pred, true): 0.070, 0.072
batch losses (mrrl, rdl): 0.0040640822, 0.0012450033

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.011 +- 0.008
mrr vals (pred, true): 0.254, 0.287
batch losses (mrrl, rdl): 0.0109450603, 5.99275e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 983
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.220, 0.224
batch losses (mrrl, rdl): 0.0001708207, 0.0001169839

Epoch over!
epoch time: 11.956

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1110
rank avg (pred): 0.314 +- 0.232
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0154574681, 0.0004918747

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.426 +- 0.153
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0005087209, 8.70789e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 29
rank avg (pred): 0.322 +- 0.220
mrr vals (pred, true): 0.073, 0.097
batch losses (mrrl, rdl): 0.0050867144, 0.0013581367

Epoch over!
epoch time: 12.182

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 911
rank avg (pred): 0.297 +- 0.225
mrr vals (pred, true): 0.109, 0.102
batch losses (mrrl, rdl): 0.0004190867, 0.000763019

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 710
rank avg (pred): 0.405 +- 0.151
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 5.5698e-05, 0.0001616709

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 301
rank avg (pred): 0.333 +- 0.201
mrr vals (pred, true): 0.061, 0.054
batch losses (mrrl, rdl): 0.0011649569, 0.0009720332

Epoch over!
epoch time: 11.975

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.365 +- 0.177
mrr vals (pred, true): 0.049, 0.072
batch losses (mrrl, rdl): 1.6507e-05, 0.0013528507

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.022 +- 0.017
mrr vals (pred, true): 0.188, 0.230
batch losses (mrrl, rdl): 0.0176670309, 6.01945e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 145
rank avg (pred): 0.401 +- 0.137
mrr vals (pred, true): 0.048, 0.018
batch losses (mrrl, rdl): 3.98638e-05, 0.0007286226

Epoch over!
epoch time: 11.891

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 702
rank avg (pred): 0.418 +- 0.118
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0030292196, 0.0002713134

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.282 +- 0.211
mrr vals (pred, true): 0.109, 0.115
batch losses (mrrl, rdl): 0.0003950924, 0.0005802067

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.286 +- 0.207
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0271827206, 0.0008320966

Epoch over!
epoch time: 12.228

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.351 +- 0.167
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0012165415, 0.0003587055

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.322 +- 0.185
mrr vals (pred, true): 0.069, 0.002
batch losses (mrrl, rdl): 0.0036104126, 0.0005575021

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 90
rank avg (pred): 0.334 +- 0.167
mrr vals (pred, true): 0.061, 0.016
batch losses (mrrl, rdl): 0.0012324643, 0.0002586218

Epoch over!
epoch time: 12.086

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.073 +- 0.095
mrr vals (pred, true): 0.183, 0.001

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   47 	     0 	 0.05741 	 0.00014 	 m..s
   91 	     1 	 0.10435 	 0.00016 	 MISS
   17 	     2 	 0.04726 	 0.00019 	 m..s
  100 	     3 	 0.11526 	 0.00019 	 MISS
  110 	     4 	 0.17551 	 0.00021 	 MISS
   23 	     5 	 0.05037 	 0.00021 	 m..s
   62 	     6 	 0.06761 	 0.00021 	 m..s
   13 	     7 	 0.04141 	 0.00022 	 m..s
    6 	     8 	 0.03912 	 0.00022 	 m..s
    9 	     9 	 0.04015 	 0.00023 	 m..s
   82 	    10 	 0.10356 	 0.00024 	 MISS
   20 	    11 	 0.04951 	 0.00024 	 m..s
   94 	    12 	 0.10621 	 0.00024 	 MISS
   84 	    13 	 0.10420 	 0.00024 	 MISS
   81 	    14 	 0.10052 	 0.00025 	 MISS
   28 	    15 	 0.05158 	 0.00025 	 m..s
   84 	    16 	 0.10420 	 0.00026 	 MISS
   53 	    17 	 0.05812 	 0.00026 	 m..s
   52 	    18 	 0.05801 	 0.00026 	 m..s
    3 	    19 	 0.03824 	 0.00027 	 m..s
   34 	    20 	 0.05447 	 0.00029 	 m..s
  101 	    21 	 0.11544 	 0.00030 	 MISS
  105 	    22 	 0.14017 	 0.00032 	 MISS
   14 	    23 	 0.04157 	 0.00033 	 m..s
   68 	    24 	 0.08109 	 0.00034 	 m..s
   84 	    25 	 0.10420 	 0.00034 	 MISS
  102 	    26 	 0.11589 	 0.00035 	 MISS
   27 	    27 	 0.05100 	 0.00036 	 m..s
   22 	    28 	 0.04969 	 0.00037 	 m..s
   37 	    29 	 0.05546 	 0.00037 	 m..s
   35 	    30 	 0.05459 	 0.00039 	 m..s
   24 	    31 	 0.05043 	 0.00039 	 m..s
   75 	    32 	 0.09135 	 0.00041 	 m..s
   25 	    33 	 0.05071 	 0.00042 	 m..s
   16 	    34 	 0.04415 	 0.00044 	 m..s
   70 	    35 	 0.08427 	 0.00051 	 m..s
   40 	    36 	 0.05625 	 0.00051 	 m..s
   55 	    37 	 0.05821 	 0.00056 	 m..s
   58 	    38 	 0.06016 	 0.00060 	 m..s
  111 	    39 	 0.18298 	 0.00083 	 MISS
   51 	    40 	 0.05797 	 0.00088 	 m..s
   63 	    41 	 0.06782 	 0.00090 	 m..s
   84 	    42 	 0.10420 	 0.00136 	 MISS
    4 	    43 	 0.03832 	 0.00217 	 m..s
    0 	    44 	 0.03619 	 0.00237 	 m..s
    1 	    45 	 0.03632 	 0.00339 	 m..s
    7 	    46 	 0.03921 	 0.00351 	 m..s
   26 	    47 	 0.05086 	 0.00394 	 m..s
    2 	    48 	 0.03669 	 0.00450 	 m..s
    8 	    49 	 0.03931 	 0.00611 	 m..s
    5 	    50 	 0.03863 	 0.01110 	 ~...
   42 	    51 	 0.05635 	 0.01178 	 m..s
   19 	    52 	 0.04881 	 0.01248 	 m..s
   12 	    53 	 0.04130 	 0.01251 	 ~...
   48 	    54 	 0.05750 	 0.01930 	 m..s
   11 	    55 	 0.04124 	 0.02020 	 ~...
   41 	    56 	 0.05634 	 0.02069 	 m..s
   49 	    57 	 0.05762 	 0.02512 	 m..s
   10 	    58 	 0.04073 	 0.03425 	 ~...
   56 	    59 	 0.05821 	 0.03688 	 ~...
   18 	    60 	 0.04802 	 0.04060 	 ~...
   57 	    61 	 0.05869 	 0.04366 	 ~...
   38 	    62 	 0.05592 	 0.04427 	 ~...
   21 	    63 	 0.04952 	 0.04797 	 ~...
   54 	    64 	 0.05819 	 0.04971 	 ~...
   31 	    65 	 0.05383 	 0.05418 	 ~...
   30 	    66 	 0.05338 	 0.05453 	 ~...
   59 	    67 	 0.06258 	 0.05599 	 ~...
   15 	    68 	 0.04404 	 0.05842 	 ~...
   32 	    69 	 0.05422 	 0.05996 	 ~...
   64 	    70 	 0.06800 	 0.06213 	 ~...
   33 	    71 	 0.05446 	 0.06345 	 ~...
   60 	    72 	 0.06259 	 0.06609 	 ~...
   45 	    73 	 0.05664 	 0.06984 	 ~...
   50 	    74 	 0.05766 	 0.07253 	 ~...
   98 	    75 	 0.10935 	 0.07414 	 m..s
   46 	    76 	 0.05692 	 0.08336 	 ~...
   29 	    77 	 0.05167 	 0.08492 	 m..s
   61 	    78 	 0.06456 	 0.09158 	 ~...
   84 	    79 	 0.10420 	 0.09504 	 ~...
   36 	    80 	 0.05494 	 0.09758 	 m..s
   73 	    81 	 0.08717 	 0.10377 	 ~...
   71 	    82 	 0.08460 	 0.10908 	 ~...
   66 	    83 	 0.07887 	 0.10984 	 m..s
   79 	    84 	 0.09803 	 0.11054 	 ~...
   69 	    85 	 0.08363 	 0.11143 	 ~...
   84 	    86 	 0.10420 	 0.11379 	 ~...
   44 	    87 	 0.05658 	 0.11594 	 m..s
   83 	    88 	 0.10384 	 0.11949 	 ~...
   67 	    89 	 0.07945 	 0.12003 	 m..s
   77 	    90 	 0.09722 	 0.12322 	 ~...
   76 	    91 	 0.09578 	 0.12497 	 ~...
   65 	    92 	 0.07398 	 0.12804 	 m..s
   43 	    93 	 0.05646 	 0.12913 	 m..s
  103 	    94 	 0.12841 	 0.13455 	 ~...
   39 	    95 	 0.05620 	 0.13857 	 m..s
   72 	    96 	 0.08551 	 0.14086 	 m..s
   80 	    97 	 0.09845 	 0.14340 	 m..s
   97 	    98 	 0.10905 	 0.14879 	 m..s
   99 	    99 	 0.11035 	 0.15287 	 m..s
   96 	   100 	 0.10731 	 0.15352 	 m..s
   93 	   101 	 0.10585 	 0.15902 	 m..s
   84 	   102 	 0.10420 	 0.16243 	 m..s
   78 	   103 	 0.09763 	 0.16283 	 m..s
   74 	   104 	 0.09031 	 0.16360 	 m..s
  106 	   105 	 0.14184 	 0.18783 	 m..s
   92 	   106 	 0.10525 	 0.19066 	 m..s
  109 	   107 	 0.15214 	 0.19119 	 m..s
  115 	   108 	 0.23800 	 0.19456 	 m..s
   95 	   109 	 0.10674 	 0.19645 	 m..s
  104 	   110 	 0.13578 	 0.19776 	 m..s
  114 	   111 	 0.19341 	 0.20392 	 ~...
  108 	   112 	 0.14840 	 0.21184 	 m..s
  112 	   113 	 0.18987 	 0.21929 	 ~...
  107 	   114 	 0.14799 	 0.22092 	 m..s
  113 	   115 	 0.19318 	 0.22372 	 m..s
  116 	   116 	 0.24299 	 0.26738 	 ~...
  118 	   117 	 0.26318 	 0.27062 	 ~...
  117 	   118 	 0.26090 	 0.28659 	 ~...
  120 	   119 	 0.32801 	 0.29184 	 m..s
  119 	   120 	 0.32502 	 0.29990 	 ~...
==========================================
r_mrr = 0.7283656597137451
r2_mrr = 0.47715359926223755
spearmanr_mrr@5 = 0.8892993927001953
spearmanr_mrr@10 = 0.9288734197616577
spearmanr_mrr@50 = 0.9331263303756714
spearmanr_mrr@100 = 0.9375762343406677
spearmanr_mrr@All = 0.9463871121406555
==========================================
test time: 0.393
Done Testing dataset DBpedia50
total time taken: 185.09548354148865
training time taken: 180.2759051322937
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7284)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4772)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8893)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9289)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9331)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9376)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9464)}}, 'test_loss': {'TransE': {'DBpedia50': 1.8847915610094788}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 3189340799894055
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [476, 725, 279, 1212, 672, 468, 136, 87, 1139, 798, 1012, 718, 125, 346, 963, 274, 998, 618, 642, 801, 1123, 460, 708, 695, 740, 448, 86, 855, 1172, 39, 363, 861, 748, 1192, 171, 179, 189, 555, 329, 1027, 903, 217, 784, 742, 882, 233, 670, 1167, 1190, 473, 957, 680, 819, 284, 868, 23, 428, 612, 706, 240, 653, 914, 886, 175, 1135, 1104, 387, 1060, 771, 25, 324, 447, 785, 373, 185, 561, 434, 581, 431, 402, 27, 1151, 57, 536, 354, 501, 248, 783, 715, 107, 262, 1176, 1005, 1021, 101, 797, 49, 24, 730, 55, 1069, 1092, 283, 120, 551, 439, 1018, 356, 304, 121, 1020, 609, 961, 1195, 417, 401, 864, 863, 946, 529, 243]
valid_ids (0): []
train_ids (1094): [826, 344, 176, 955, 908, 590, 1137, 498, 711, 6, 635, 809, 143, 347, 647, 1130, 1207, 518, 859, 1025, 199, 168, 699, 106, 113, 471, 1083, 358, 104, 996, 412, 749, 1096, 720, 629, 66, 948, 641, 1174, 632, 1093, 381, 605, 133, 1183, 654, 818, 896, 28, 1067, 904, 312, 254, 218, 362, 227, 763, 893, 571, 221, 508, 958, 61, 1076, 466, 1082, 506, 1197, 816, 857, 765, 319, 562, 339, 1110, 997, 1024, 63, 511, 799, 585, 366, 731, 315, 210, 391, 70, 42, 625, 902, 991, 1211, 187, 167, 999, 737, 17, 340, 1208, 1053, 602, 645, 844, 65, 219, 122, 482, 1097, 1149, 169, 601, 463, 103, 913, 787, 456, 1187, 1014, 418, 1054, 1180, 172, 823, 528, 1048, 744, 349, 916, 207, 942, 323, 769, 833, 548, 1133, 1152, 1122, 390, 941, 901, 357, 490, 791, 1203, 845, 1040, 286, 541, 228, 622, 159, 610, 132, 1072, 435, 154, 52, 747, 659, 1049, 752, 1147, 751, 735, 922, 258, 364, 375, 962, 422, 293, 1154, 15, 608, 736, 297, 194, 1168, 396, 318, 420, 502, 865, 303, 881, 974, 778, 944, 986, 514, 37, 202, 67, 796, 455, 260, 626, 1036, 91, 496, 152, 880, 1199, 290, 762, 807, 971, 1171, 539, 794, 686, 29, 252, 690, 604, 178, 987, 732, 1071, 1042, 600, 268, 734, 1163, 838, 525, 559, 643, 126, 45, 560, 296, 805, 757, 385, 156, 879, 97, 712, 3, 369, 1001, 117, 166, 513, 36, 99, 376, 619, 848, 546, 73, 1050, 636, 872, 777, 361, 1189, 814, 1131, 575, 205, 570, 867, 452, 945, 332, 827, 287, 370, 979, 11, 704, 328, 223, 151, 313, 789, 956, 874, 241, 321, 83, 884, 69, 728, 1188, 384, 994, 627, 325, 800, 289, 840, 534, 266, 353, 96, 831, 374, 410, 1185, 1162, 445, 912, 141, 267, 270, 90, 1124, 13, 939, 419, 877, 593, 147, 760, 300, 19, 389, 1008, 889, 1209, 582, 953, 427, 1143, 1114, 1205, 758, 1175, 682, 1148, 249, 1103, 140, 825, 1144, 658, 395, 229, 444, 553, 239, 157, 54, 1078, 921, 1173, 927, 907, 691, 583, 843, 1140, 115, 367, 441, 992, 59, 12, 661, 183, 576, 500, 77, 1105, 433, 1142, 246, 713, 954, 111, 1030, 892, 257, 371, 208, 808, 359, 477, 1000, 196, 407, 421, 1108, 469, 1125, 1099, 72, 173, 674, 568, 523, 929, 1145, 1007, 31, 355, 786, 662, 432, 510, 965, 1102, 1010, 898, 360, 38, 972, 937, 835, 984, 891, 550, 545, 648, 552, 723, 191, 184, 85, 453, 1158, 1153, 348, 307, 563, 968, 917, 507, 1029, 212, 928, 493, 1126, 709, 793, 84, 1136, 1196, 253, 1128, 557, 309, 821, 727, 973, 540, 135, 119, 1134, 276, 398, 621, 1041, 533, 594, 337, 926, 788, 430, 35, 377, 530, 679, 237, 842, 930, 705, 683, 660, 1178, 1051, 822, 129, 1061, 726, 782, 403, 484, 1085, 767, 1002, 675, 1202, 301, 1118, 1022, 209, 465, 888, 657, 1016, 853, 564, 897, 651, 124, 885, 467, 1094, 1166, 1015, 98, 1095, 155, 62, 776, 1198, 516, 288, 588, 216, 676, 1181, 775, 646, 255, 900, 1013, 397, 764, 1193, 829, 768, 932, 1056, 515, 556, 160, 1032, 386, 399, 162, 614, 920, 995, 697, 1116, 481, 830, 480, 538, 224, 1037, 198, 193, 285, 81, 1100, 149, 137, 1011, 1146, 624, 543, 21, 940, 895, 710, 755, 203, 74, 770, 620, 446, 580, 128, 1023, 351, 589, 305, 565, 860, 392, 273, 1191, 1017, 1073, 694, 925, 0, 1112, 792, 41, 978, 404, 474, 628, 14, 673, 164, 192, 779, 1129, 1075, 1087, 804, 414, 330, 969, 733, 153, 1028, 109, 806, 180, 756, 1206, 869, 745, 245, 519, 425, 475, 630, 436, 242, 1160, 220, 382, 849, 871, 924, 302, 1089, 894, 668, 333, 990, 1106, 1090, 1081, 472, 43, 190, 1156, 754, 450, 933, 264, 483, 158, 1052, 46, 684, 815, 985, 437, 542, 975, 89, 80, 451, 1165, 750, 1127, 442, 1111, 322, 247, 611, 1066, 34, 130, 852, 949, 1077, 211, 1141, 459, 1026, 48, 547, 139, 909, 637, 1194, 161, 7, 108, 899, 1063, 342, 890, 774, 1138, 341, 280, 841, 458, 314, 102, 1120, 526, 1214, 846, 584, 685, 873, 824, 905, 1003, 298, 537, 1004, 934, 558, 380, 177, 817, 951, 163, 26, 438, 803, 486, 1098, 1046, 429, 423, 531, 544, 138, 617, 811, 967, 1117, 406, 906, 729, 910, 597, 938, 828, 1070, 664, 485, 204, 331, 520, 719, 365, 652, 79, 512, 400, 146, 470, 766, 100, 592, 1047, 292, 616, 408, 82, 966, 724, 114, 281, 1062, 578, 505, 1091, 32, 30, 1065, 277, 649, 116, 492, 613, 1169, 231, 9, 988, 688, 509, 1031, 982, 352, 1159, 1121, 5, 236, 234, 950, 338, 700, 586, 862, 813, 943, 275, 866, 1009, 20, 671, 213, 308, 105, 44, 295, 409, 1034, 772, 75, 1084, 1045, 131, 244, 717, 856, 1044, 294, 935, 1101, 1057, 591, 1170, 47, 810, 457, 850, 174, 640, 878, 1086, 76, 278, 667, 499, 1079, 887, 875, 687, 832, 549, 644, 144, 665, 573, 802, 781, 1119, 854, 993, 915, 56, 677, 424, 638, 393, 93, 454, 596, 195, 759, 698, 977, 372, 1213, 335, 918, 1074, 1055, 837, 88, 123, 820, 413, 443, 186, 150, 22, 1, 656, 1164, 773, 142, 1064, 587, 923, 1038, 595, 368, 68, 282, 440, 663, 693, 110, 976, 148, 1201, 336, 1035, 16, 1150, 981, 188, 464, 53, 847, 1043, 603, 678, 572, 345, 689, 461, 812, 60, 983, 1039, 623, 753, 554, 478, 33, 259, 320, 989, 222, 650, 479, 681, 606, 716, 8, 1107, 487, 1080, 58, 1068, 633, 599, 870, 655, 960, 145, 836, 1200, 834, 911, 95, 462, 201, 1161, 851, 112, 721, 1006, 494, 316, 271, 405, 1177, 64, 181, 504, 134, 1179, 291, 569, 1113, 524, 261, 1088, 722, 92, 118, 883, 235, 1182, 959, 18, 858, 1033, 269, 317, 334, 701, 182, 250, 50, 1059, 78, 327, 1157, 170, 383, 306, 2, 1058, 567, 94, 952, 197, 394, 790, 1019, 527, 350, 489, 615, 256, 707, 272, 936, 532, 503, 714, 1155, 839, 4, 214, 426, 415, 1204, 535, 761, 522, 326, 1115, 311, 488, 449, 343, 165, 692, 40, 127, 225, 743, 666, 795, 411, 741, 964, 703, 491, 1184, 634, 970, 598, 702, 931, 738, 299, 1109, 232, 379, 238, 10, 200, 263, 206, 739, 980, 1132, 497, 746, 378, 388, 574, 310, 607, 566, 517, 251, 1186, 631, 265, 495, 919, 71, 579, 230, 696, 1210, 876, 639, 416, 51, 947, 577, 669, 215, 780, 226, 521]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2341358330888383
the save name prefix for this run is:  chkpt-ID_2341358330888383_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 77
rank avg (pred): 0.478 +- 0.011
mrr vals (pred, true): 0.000, 0.174
batch losses (mrrl, rdl): 0.0, 0.0033650834

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1129
rank avg (pred): 0.326 +- 0.259
mrr vals (pred, true): 0.120, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002311573

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.298 +- 0.249
mrr vals (pred, true): 0.260, 0.125
batch losses (mrrl, rdl): 0.0, 0.0004878698

Epoch over!
epoch time: 11.895

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.304 +- 0.261
mrr vals (pred, true): 0.294, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003301569

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.098 +- 0.082
mrr vals (pred, true): 0.351, 0.224
batch losses (mrrl, rdl): 0.0, 3.0141e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 500
rank avg (pred): 0.248 +- 0.212
mrr vals (pred, true): 0.373, 0.110
batch losses (mrrl, rdl): 0.0, 7.80185e-05

Epoch over!
epoch time: 11.681

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 938
rank avg (pred): 0.332 +- 0.278
mrr vals (pred, true): 0.365, 0.168
batch losses (mrrl, rdl): 0.0, 0.0005208818

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1126
rank avg (pred): 0.313 +- 0.276
mrr vals (pred, true): 0.383, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004969659

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1153
rank avg (pred): 0.268 +- 0.246
mrr vals (pred, true): 0.392, 0.105
batch losses (mrrl, rdl): 0.0, 0.0001320469

Epoch over!
epoch time: 11.951

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.327 +- 0.291
mrr vals (pred, true): 0.386, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004357557

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.318 +- 0.287
mrr vals (pred, true): 0.389, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002855599

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.312 +- 0.294
mrr vals (pred, true): 0.404, 0.203
batch losses (mrrl, rdl): 0.0, 0.0005184929

Epoch over!
epoch time: 11.754

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 316
rank avg (pred): 0.100 +- 0.095
mrr vals (pred, true): 0.426, 0.109
batch losses (mrrl, rdl): 0.0, 2.90851e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 694
rank avg (pred): 0.357 +- 0.331
mrr vals (pred, true): 0.383, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001605208

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 63
rank avg (pred): 0.100 +- 0.095
mrr vals (pred, true): 0.422, 0.080
batch losses (mrrl, rdl): 0.0, 3.4067e-05

Epoch over!
epoch time: 11.867

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1001
rank avg (pred): 0.190 +- 0.179
mrr vals (pred, true): 0.398, 0.151
batch losses (mrrl, rdl): 0.6124659777, 3.88088e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 919
rank avg (pred): 0.353 +- 0.009
mrr vals (pred, true): 0.000, 0.150
batch losses (mrrl, rdl): 0.2245993316, 0.0005296037

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.349 +- 0.291
mrr vals (pred, true): 0.065, 0.167
batch losses (mrrl, rdl): 0.1044795737, 0.000356785

Epoch over!
epoch time: 12.15

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 942
rank avg (pred): 0.342 +- 0.290
mrr vals (pred, true): 0.077, 0.151
batch losses (mrrl, rdl): 0.0554488637, 0.0002360143

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.056 +- 0.167
mrr vals (pred, true): 0.268, 0.287
batch losses (mrrl, rdl): 0.0036400026, 6.008e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.084 +- 0.230
mrr vals (pred, true): 0.213, 0.248
batch losses (mrrl, rdl): 0.012472339, 3.0223e-05

Epoch over!
epoch time: 12.098

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.081 +- 0.220
mrr vals (pred, true): 0.174, 0.210
batch losses (mrrl, rdl): 0.0130321868, 2.33659e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1053
rank avg (pred): 0.119 +- 0.306
mrr vals (pred, true): 0.176, 0.265
batch losses (mrrl, rdl): 0.0801718831, 8.08188e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 503
rank avg (pred): 0.320 +- 0.296
mrr vals (pred, true): 0.067, 0.086
batch losses (mrrl, rdl): 0.0030157883, 9.56106e-05

Epoch over!
epoch time: 12.113

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 173
rank avg (pred): 0.333 +- 0.296
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001471872, 0.0005858548

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1087
rank avg (pred): 0.152 +- 0.329
mrr vals (pred, true): 0.124, 0.199
batch losses (mrrl, rdl): 0.0564590879, 3.65679e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 191
rank avg (pred): 0.360 +- 0.286
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 3.177e-07, 0.000279146

Epoch over!
epoch time: 11.947

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.359 +- 0.284
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 9.81e-07, 0.0005234696

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.375 +- 0.268
mrr vals (pred, true): 0.049, 0.011
batch losses (mrrl, rdl): 1.40158e-05, 3.99801e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 347
rank avg (pred): 0.169 +- 0.310
mrr vals (pred, true): 0.103, 0.121
batch losses (mrrl, rdl): 0.00344748, 5.7863e-05

Epoch over!
epoch time: 11.985

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.119 +- 0.297
mrr vals (pred, true): 0.197, 0.224
batch losses (mrrl, rdl): 0.0072303899, 9.77753e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1148
rank avg (pred): 0.260 +- 0.325
mrr vals (pred, true): 0.085, 0.088
batch losses (mrrl, rdl): 0.0120990025, 1.94157e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1078
rank avg (pred): 0.090 +- 0.254
mrr vals (pred, true): 0.299, 0.286
batch losses (mrrl, rdl): 0.0017486408, 4.67564e-05

Epoch over!
epoch time: 12.192

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1131
rank avg (pred): 0.155 +- 0.336
mrr vals (pred, true): 0.123, 0.000
batch losses (mrrl, rdl): 0.0535158664, 0.0018126869

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.405 +- 0.305
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003672785, 0.0001478414

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 8
rank avg (pred): 0.148 +- 0.332
mrr vals (pred, true): 0.131, 0.120
batch losses (mrrl, rdl): 0.0011245622, 0.0001177407

Epoch over!
epoch time: 12.221

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.396 +- 0.282
mrr vals (pred, true): 0.049, 0.037
batch losses (mrrl, rdl): 9.5993e-06, 0.0003003081

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.379 +- 0.337
mrr vals (pred, true): 0.101, 0.167
batch losses (mrrl, rdl): 0.0439249538, 0.0005360499

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 138
rank avg (pred): 0.392 +- 0.275
mrr vals (pred, true): 0.049, 0.011
batch losses (mrrl, rdl): 2.14619e-05, 0.0002453638

Epoch over!
epoch time: 12.222

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 838
rank avg (pred): 0.306 +- 0.333
mrr vals (pred, true): 0.089, 0.168
batch losses (mrrl, rdl): 0.0611351393, 0.0001044656

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 814
rank avg (pred): 0.175 +- 0.345
mrr vals (pred, true): 0.105, 0.090
batch losses (mrrl, rdl): 0.0300186835, 7.66334e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1034
rank avg (pred): 0.158 +- 0.344
mrr vals (pred, true): 0.109, 0.000
batch losses (mrrl, rdl): 0.0347809531, 0.0023258294

Epoch over!
epoch time: 12.085

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.395 +- 0.302
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 2.8272e-06, 0.0002592961

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 560
rank avg (pred): 0.318 +- 0.275
mrr vals (pred, true): 0.066, 0.092
batch losses (mrrl, rdl): 0.0026052953, 7.60694e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.135 +- 0.287
mrr vals (pred, true): 0.114, 0.125
batch losses (mrrl, rdl): 0.0012966117, 8.38179e-05

Epoch over!
epoch time: 11.944

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.356 +- 0.296
mrr vals (pred, true): 0.051, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   36 	     0 	 0.04935 	 0.00017 	 m..s
    8 	     1 	 0.04857 	 0.00020 	 m..s
  105 	     2 	 0.11201 	 0.00021 	 MISS
   22 	     3 	 0.04911 	 0.00021 	 m..s
   54 	     4 	 0.05109 	 0.00022 	 m..s
   78 	     5 	 0.07335 	 0.00022 	 m..s
    0 	     6 	 0.04590 	 0.00023 	 m..s
   88 	     7 	 0.08923 	 0.00024 	 m..s
   74 	     8 	 0.07177 	 0.00024 	 m..s
   71 	     9 	 0.06912 	 0.00024 	 m..s
   21 	    10 	 0.04908 	 0.00024 	 m..s
   33 	    11 	 0.04927 	 0.00025 	 m..s
   92 	    12 	 0.09757 	 0.00025 	 m..s
   96 	    13 	 0.09883 	 0.00025 	 m..s
   48 	    14 	 0.05084 	 0.00025 	 m..s
   38 	    15 	 0.04955 	 0.00026 	 m..s
   41 	    16 	 0.04999 	 0.00026 	 m..s
   13 	    17 	 0.04864 	 0.00027 	 m..s
   93 	    18 	 0.09769 	 0.00027 	 m..s
   85 	    19 	 0.08240 	 0.00029 	 m..s
  104 	    20 	 0.11052 	 0.00029 	 MISS
   77 	    21 	 0.07296 	 0.00030 	 m..s
   44 	    22 	 0.05006 	 0.00030 	 m..s
    7 	    23 	 0.04856 	 0.00032 	 m..s
    4 	    24 	 0.04703 	 0.00032 	 m..s
   30 	    25 	 0.04926 	 0.00033 	 m..s
   11 	    26 	 0.04862 	 0.00034 	 m..s
   26 	    27 	 0.04923 	 0.00035 	 m..s
  107 	    28 	 0.11419 	 0.00035 	 MISS
   94 	    29 	 0.09860 	 0.00036 	 m..s
    3 	    30 	 0.04686 	 0.00038 	 m..s
   72 	    31 	 0.07003 	 0.00038 	 m..s
   42 	    32 	 0.05002 	 0.00039 	 m..s
    9 	    33 	 0.04859 	 0.00039 	 m..s
    2 	    34 	 0.04631 	 0.00039 	 m..s
   25 	    35 	 0.04921 	 0.00040 	 m..s
  101 	    36 	 0.11018 	 0.00041 	 MISS
   49 	    37 	 0.05084 	 0.00042 	 m..s
   23 	    38 	 0.04913 	 0.00043 	 m..s
   87 	    39 	 0.08887 	 0.00044 	 m..s
   20 	    40 	 0.04905 	 0.00046 	 m..s
   47 	    41 	 0.05084 	 0.00047 	 m..s
   58 	    42 	 0.05273 	 0.00049 	 m..s
   46 	    43 	 0.05083 	 0.00051 	 m..s
   35 	    44 	 0.04935 	 0.00053 	 m..s
    1 	    45 	 0.04629 	 0.00063 	 m..s
   76 	    46 	 0.07266 	 0.00078 	 m..s
  108 	    47 	 0.11525 	 0.00080 	 MISS
   70 	    48 	 0.06909 	 0.00136 	 m..s
    5 	    49 	 0.04705 	 0.00195 	 m..s
   15 	    50 	 0.04867 	 0.00217 	 m..s
   14 	    51 	 0.04866 	 0.00349 	 m..s
   12 	    52 	 0.04863 	 0.00351 	 m..s
   29 	    53 	 0.04925 	 0.00597 	 m..s
   10 	    54 	 0.04859 	 0.01046 	 m..s
   18 	    55 	 0.04874 	 0.01139 	 m..s
   17 	    56 	 0.04871 	 0.01678 	 m..s
   39 	    57 	 0.04965 	 0.02693 	 ~...
   37 	    58 	 0.04943 	 0.02710 	 ~...
   53 	    59 	 0.05109 	 0.02802 	 ~...
    6 	    60 	 0.04855 	 0.02901 	 ~...
   24 	    61 	 0.04914 	 0.03439 	 ~...
   19 	    62 	 0.04876 	 0.03475 	 ~...
   55 	    63 	 0.05113 	 0.03665 	 ~...
   16 	    64 	 0.04869 	 0.03972 	 ~...
   31 	    65 	 0.04926 	 0.04325 	 ~...
   28 	    66 	 0.04925 	 0.04983 	 ~...
   51 	    67 	 0.05088 	 0.05418 	 ~...
   59 	    68 	 0.05386 	 0.05573 	 ~...
   40 	    69 	 0.04973 	 0.05932 	 ~...
   62 	    70 	 0.05621 	 0.06100 	 ~...
   32 	    71 	 0.04927 	 0.06385 	 ~...
   79 	    72 	 0.07432 	 0.06519 	 ~...
   57 	    73 	 0.05246 	 0.06577 	 ~...
   50 	    74 	 0.05087 	 0.06756 	 ~...
   66 	    75 	 0.05998 	 0.06802 	 ~...
   75 	    76 	 0.07235 	 0.06807 	 ~...
   43 	    77 	 0.05005 	 0.07063 	 ~...
   64 	    78 	 0.05802 	 0.07237 	 ~...
   27 	    79 	 0.04924 	 0.07302 	 ~...
   56 	    80 	 0.05201 	 0.07562 	 ~...
   90 	    81 	 0.09134 	 0.08014 	 ~...
   52 	    82 	 0.05108 	 0.08185 	 m..s
   97 	    83 	 0.10069 	 0.08236 	 ~...
   60 	    84 	 0.05424 	 0.08501 	 m..s
   34 	    85 	 0.04930 	 0.09210 	 m..s
   67 	    86 	 0.06177 	 0.09472 	 m..s
   61 	    87 	 0.05494 	 0.09750 	 m..s
   45 	    88 	 0.05014 	 0.09758 	 m..s
   81 	    89 	 0.07585 	 0.09867 	 ~...
   69 	    90 	 0.06892 	 0.10101 	 m..s
   73 	    91 	 0.07136 	 0.10106 	 ~...
   63 	    92 	 0.05674 	 0.10328 	 m..s
  112 	    93 	 0.13673 	 0.10857 	 ~...
   80 	    94 	 0.07457 	 0.11054 	 m..s
   65 	    95 	 0.05858 	 0.11374 	 m..s
  110 	    96 	 0.12553 	 0.12796 	 ~...
  114 	    97 	 0.16047 	 0.12878 	 m..s
   68 	    98 	 0.06695 	 0.12885 	 m..s
  109 	    99 	 0.11775 	 0.12979 	 ~...
  111 	   100 	 0.12844 	 0.13936 	 ~...
   89 	   101 	 0.09123 	 0.14340 	 m..s
  106 	   102 	 0.11273 	 0.14530 	 m..s
   99 	   103 	 0.10689 	 0.15451 	 m..s
   98 	   104 	 0.10515 	 0.15868 	 m..s
  103 	   105 	 0.11050 	 0.15902 	 m..s
  113 	   106 	 0.16003 	 0.15925 	 ~...
  102 	   107 	 0.11033 	 0.16078 	 m..s
  115 	   108 	 0.16290 	 0.16166 	 ~...
   84 	   109 	 0.07724 	 0.16715 	 m..s
  100 	   110 	 0.10949 	 0.16959 	 m..s
   86 	   111 	 0.08850 	 0.17200 	 m..s
   82 	   112 	 0.07708 	 0.17398 	 m..s
   95 	   113 	 0.09861 	 0.18166 	 m..s
  116 	   114 	 0.16712 	 0.18310 	 ~...
   83 	   115 	 0.07716 	 0.19066 	 MISS
   91 	   116 	 0.09631 	 0.19645 	 MISS
  117 	   117 	 0.19407 	 0.21903 	 ~...
  120 	   118 	 0.26283 	 0.28660 	 ~...
  119 	   119 	 0.24366 	 0.30536 	 m..s
  118 	   120 	 0.22919 	 0.32792 	 m..s
==========================================
r_mrr = 0.7179039120674133
r2_mrr = 0.4437630772590637
spearmanr_mrr@5 = 0.9923109412193298
spearmanr_mrr@10 = 0.9815703630447388
spearmanr_mrr@50 = 0.9751428961753845
spearmanr_mrr@100 = 0.965723991394043
spearmanr_mrr@All = 0.9674354195594788
==========================================
test time: 0.423
Done Testing dataset DBpedia50
total time taken: 185.10920357704163
training time taken: 180.6194453239441
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7179)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4438)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9923)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9816)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9751)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9657)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9674)}}, 'test_loss': {'TransE': {'DBpedia50': 1.4760873829873162}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 8400672867204421
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [374, 1058, 1145, 990, 193, 568, 396, 378, 426, 1154, 264, 818, 729, 88, 840, 130, 657, 695, 105, 371, 851, 859, 514, 157, 1116, 718, 33, 960, 306, 683, 131, 1025, 251, 619, 521, 79, 361, 752, 620, 462, 1171, 154, 236, 68, 273, 643, 202, 248, 419, 920, 969, 171, 135, 569, 697, 530, 473, 909, 638, 805, 1100, 149, 563, 40, 104, 1119, 849, 678, 420, 28, 1207, 1174, 993, 363, 1035, 89, 364, 1080, 8, 1061, 921, 584, 471, 1059, 414, 14, 1094, 976, 961, 791, 1147, 1208, 565, 498, 1107, 444, 702, 50, 933, 132, 962, 449, 467, 1111, 1148, 170, 545, 490, 764, 108, 926, 298, 188, 432, 773, 877, 1045, 65, 576, 1104, 581]
valid_ids (0): []
train_ids (1094): [74, 215, 628, 393, 373, 907, 813, 435, 777, 558, 26, 241, 31, 983, 939, 1105, 185, 716, 137, 503, 84, 536, 272, 277, 213, 945, 36, 586, 801, 63, 77, 892, 222, 1146, 886, 540, 328, 1041, 85, 144, 670, 973, 1087, 3, 51, 1067, 355, 101, 201, 445, 646, 693, 747, 800, 713, 45, 383, 1160, 881, 928, 914, 474, 1167, 1179, 763, 332, 9, 267, 1026, 1126, 602, 996, 817, 138, 1037, 1185, 541, 863, 308, 1036, 946, 366, 664, 382, 1005, 1199, 862, 1144, 134, 952, 145, 727, 225, 239, 571, 81, 487, 506, 564, 303, 1124, 483, 1106, 398, 196, 1130, 370, 807, 512, 493, 684, 410, 949, 866, 237, 532, 345, 738, 865, 288, 400, 221, 1153, 428, 189, 850, 433, 1193, 612, 1162, 523, 744, 526, 1180, 392, 825, 991, 812, 124, 685, 1161, 404, 495, 746, 481, 351, 421, 788, 413, 390, 263, 475, 964, 1093, 275, 1027, 879, 457, 30, 717, 326, 607, 810, 119, 466, 216, 13, 322, 641, 680, 146, 48, 174, 864, 1189, 992, 984, 627, 142, 38, 1170, 1023, 448, 1014, 955, 407, 446, 822, 874, 291, 86, 304, 782, 292, 334, 338, 1169, 671, 311, 197, 625, 293, 1006, 1099, 2, 22, 313, 721, 107, 310, 163, 460, 637, 968, 254, 977, 1200, 944, 917, 787, 1051, 1157, 468, 786, 942, 703, 1008, 184, 577, 461, 574, 299, 422, 258, 233, 724, 387, 1102, 96, 59, 261, 954, 1016, 868, 1182, 172, 114, 870, 1081, 205, 694, 87, 916, 122, 828, 324, 83, 547, 971, 855, 882, 312, 672, 452, 191, 103, 1084, 815, 1214, 270, 7, 479, 965, 836, 901, 280, 780, 700, 1017, 329, 484, 200, 1079, 922, 736, 876, 1003, 318, 340, 488, 601, 330, 230, 268, 406, 1156, 391, 554, 896, 190, 814, 517, 218, 408, 1120, 110, 1086, 1209, 556, 1134, 665, 651, 49, 835, 1138, 12, 994, 1152, 923, 42, 301, 918, 749, 54, 653, 504, 343, 710, 603, 938, 951, 1021, 203, 327, 1108, 911, 379, 244, 931, 177, 112, 284, 679, 295, 1060, 739, 953, 243, 529, 358, 416, 476, 1206, 1151, 783, 871, 41, 346, 1071, 600, 599, 535, 943, 897, 593, 15, 891, 159, 842, 656, 1150, 224, 596, 522, 531, 799, 210, 910, 639, 1089, 803, 765, 302, 1075, 560, 46, 1078, 186, 499, 1128, 957, 320, 769, 24, 1131, 285, 742, 1018, 947, 1048, 442, 903, 102, 394, 1069, 823, 676, 615, 555, 621, 539, 180, 307, 567, 271, 337, 644, 369, 940, 425, 1066, 55, 257, 988, 265, 469, 755, 1049, 353, 411, 809, 819, 199, 1028, 294, 92, 316, 632, 590, 533, 711, 794, 978, 1139, 274, 553, 1177, 21, 315, 1112, 436, 464, 972, 347, 608, 611, 552, 737, 360, 1164, 242, 491, 767, 17, 510, 357, 1072, 551, 956, 127, 349, 1022, 440, 1034, 509, 706, 797, 1204, 659, 654, 1110, 260, 645, 906, 572, 887, 431, 214, 595, 238, 838, 456, 450, 853, 206, 528, 78, 1065, 784, 587, 1095, 37, 1039, 384, 579, 802, 682, 158, 53, 719, 1118, 1201, 662, 772, 402, 898, 262, 278, 489, 147, 970, 751, 959, 152, 336, 1109, 527, 32, 1040, 666, 123, 833, 397, 1178, 543, 1012, 57, 27, 1195, 872, 16, 1155, 919, 908, 168, 515, 635, 562, 290, 894, 289, 1011, 376, 692, 708, 614, 542, 867, 844, 1033, 1085, 93, 1142, 1015, 377, 591, 924, 756, 497, 668, 1068, 129, 350, 578, 561, 1062, 235, 1175, 438, 252, 1097, 259, 121, 1186, 472, 486, 375, 624, 709, 795, 1031, 1053, 516, 255, 401, 1183, 1202, 176, 760, 443, 941, 372, 580, 834, 1159, 1212, 843, 240, 75, 227, 598, 966, 832, 893, 1090, 1042, 689, 231, 141, 1013, 661, 161, 548, 1001, 613, 90, 981, 1057, 649, 1173, 217, 1038, 470, 570, 91, 743, 1127, 155, 854, 730, 1082, 72, 537, 181, 70, 1073, 852, 250, 636, 1181, 309, 701, 839, 754, 913, 1019, 325, 691, 477, 386, 798, 1030, 806, 405, 211, 1136, 796, 133, 811, 113, 35, 998, 1083, 781, 451, 841, 575, 362, 677, 559, 588, 546, 732, 1125, 699, 359, 1121, 557, 365, 333, 1194, 331, 245, 761, 136, 856, 793, 592, 232, 344, 888, 589, 164, 912, 634, 1055, 830, 418, 633, 1122, 974, 424, 549, 858, 502, 118, 1096, 513, 857, 228, 1198, 1172, 905, 195, 895, 1046, 669, 594, 735, 1044, 1000, 1076, 789, 999, 380, 156, 148, 204, 447, 182, 335, 463, 688, 778, 437, 321, 658, 282, 696, 726, 1054, 352, 100, 757, 500, 690, 403, 597, 929, 861, 496, 935, 779, 734, 820, 453, 720, 655, 544, 501, 167, 837, 482, 770, 889, 899, 505, 925, 323, 1140, 846, 173, 44, 1113, 675, 731, 109, 52, 585, 276, 485, 209, 279, 1137, 963, 1088, 1056, 1188, 492, 1043, 616, 162, 674, 1070, 605, 247, 458, 305, 178, 39, 741, 98, 417, 175, 283, 847, 389, 785, 494, 723, 1092, 19, 550, 0, 967, 934, 1010, 704, 524, 663, 423, 1024, 80, 915, 1190, 111, 58, 223, 715, 1184, 745, 606, 23, 975, 890, 354, 816, 1103, 314, 774, 1064, 1047, 395, 1052, 341, 712, 194, 660, 519, 1077, 1114, 705, 883, 821, 986, 234, 1063, 725, 208, 880, 25, 429, 4, 356, 1149, 10, 869, 43, 626, 1135, 1168, 116, 507, 76, 686, 936, 212, 115, 511, 652, 958, 1205, 824, 220, 73, 1196, 792, 140, 179, 286, 18, 827, 459, 1197, 1115, 604, 478, 67, 622, 650, 1117, 995, 687, 1009, 1091, 1032, 388, 885, 1007, 399, 1020, 617, 151, 1141, 296, 904, 348, 1213, 826, 534, 95, 771, 927, 246, 768, 610, 1176, 66, 368, 776, 987, 1187, 1074, 165, 117, 1166, 1002, 520, 566, 1050, 1203, 518, 790, 62, 714, 804, 47, 207, 848, 583, 948, 187, 629, 430, 1004, 845, 748, 56, 989, 1143, 441, 253, 126, 427, 342, 69, 11, 150, 982, 29, 153, 1098, 1029, 219, 873, 667, 623, 950, 82, 61, 120, 609, 125, 269, 642, 573, 166, 900, 762, 385, 454, 775, 1132, 139, 1210, 681, 226, 733, 753, 618, 229, 297, 1192, 409, 1211, 758, 631, 465, 198, 128, 630, 884, 766, 902, 722, 143, 71, 582, 508, 256, 434, 937, 6, 980, 808, 648, 1133, 979, 480, 860, 647, 455, 1191, 249, 94, 183, 1165, 1, 831, 367, 740, 317, 319, 106, 750, 878, 728, 381, 439, 64, 985, 759, 1158, 169, 829, 698, 266, 932, 60, 707, 930, 1101, 673, 640, 415, 1129, 1163, 412, 875, 1123, 339, 281, 97, 5, 300, 538, 20, 287, 160, 192, 997, 34, 99, 525]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8129535089091037
the save name prefix for this run is:  chkpt-ID_8129535089091037_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.420 +- 0.003
mrr vals (pred, true): 0.000, 0.125
batch losses (mrrl, rdl): 0.0, 0.0012791224

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.119 +- 0.063
mrr vals (pred, true): 0.026, 0.104
batch losses (mrrl, rdl): 0.0, 5.23381e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 888
rank avg (pred): 0.322 +- 0.223
mrr vals (pred, true): 0.194, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003960739

Epoch over!
epoch time: 11.874

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.352 +- 0.243
mrr vals (pred, true): 0.192, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001424427

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.320 +- 0.227
mrr vals (pred, true): 0.218, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003150245

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.335 +- 0.284
mrr vals (pred, true): 0.254, 0.003
batch losses (mrrl, rdl): 0.0, 2.59864e-05

Epoch over!
epoch time: 11.805

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 418
rank avg (pred): 0.326 +- 0.280
mrr vals (pred, true): 0.260, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004462689

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 930
rank avg (pred): 0.311 +- 0.282
mrr vals (pred, true): 0.300, 0.161
batch losses (mrrl, rdl): 0.0, 0.0003679683

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 183
rank avg (pred): 0.328 +- 0.297
mrr vals (pred, true): 0.317, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003625055

Epoch over!
epoch time: 11.861

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 698
rank avg (pred): 0.347 +- 0.320
mrr vals (pred, true): 0.316, 0.000
batch losses (mrrl, rdl): 0.0, 0.000161802

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1141
rank avg (pred): 0.176 +- 0.169
mrr vals (pred, true): 0.384, 0.108
batch losses (mrrl, rdl): 0.0, 0.0001181439

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1005
rank avg (pred): 0.315 +- 0.298
mrr vals (pred, true): 0.332, 0.196
batch losses (mrrl, rdl): 0.0, 0.0006472568

Epoch over!
epoch time: 11.738

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 966
rank avg (pred): 0.313 +- 0.300
mrr vals (pred, true): 0.350, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002516017

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.290 +- 0.282
mrr vals (pred, true): 0.363, 0.139
batch losses (mrrl, rdl): 0.0, 0.0003971612

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 830
rank avg (pred): 0.093 +- 0.093
mrr vals (pred, true): 0.407, 0.135
batch losses (mrrl, rdl): 0.0, 3.90291e-05

Epoch over!
epoch time: 11.932

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.079 +- 0.078
mrr vals (pred, true): 0.416, 0.123
batch losses (mrrl, rdl): 0.8555732965, 2.43326e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 875
rank avg (pred): 0.377 +- 0.000
mrr vals (pred, true): 0.000, 0.002
batch losses (mrrl, rdl): 0.0248924289, 0.000363755

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1167
rank avg (pred): 0.311 +- 0.000
mrr vals (pred, true): 0.000, 0.068
batch losses (mrrl, rdl): 0.0248696506, 0.0001870185

Epoch over!
epoch time: 12.154

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 142
rank avg (pred): 0.349 +- 0.027
mrr vals (pred, true): 0.000, 0.016
batch losses (mrrl, rdl): 0.0248831343, 0.0002850245

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.249 +- 0.000
mrr vals (pred, true): 0.000, 0.133
batch losses (mrrl, rdl): 0.1776576936, 0.0004759725

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 896
rank avg (pred): 0.198 +- 0.000
mrr vals (pred, true): 0.000, 0.095
batch losses (mrrl, rdl): 0.0247954559, 0.0002024946

Epoch over!
epoch time: 11.981

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 446
rank avg (pred): 0.315 +- 0.124
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0248491541, 0.0004678332

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.354 +- 0.307
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0247946866, 0.0002909303

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.083 +- 0.030
mrr vals (pred, true): 0.001, 0.129
batch losses (mrrl, rdl): 0.1657608449, 4.02468e-05

Epoch over!
epoch time: 11.956

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 544
rank avg (pred): 0.267 +- 0.294
mrr vals (pred, true): 0.000, 0.047
batch losses (mrrl, rdl): 0.0245951898, 4.7855e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 82
rank avg (pred): 0.178 +- 0.342
mrr vals (pred, true): 0.007, 0.014
batch losses (mrrl, rdl): 0.018260181, 7.83863e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 460
rank avg (pred): 0.216 +- 0.342
mrr vals (pred, true): 0.065, 0.000
batch losses (mrrl, rdl): 0.0022708769, 0.0010126984

Epoch over!
epoch time: 11.944

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.345 +- 0.466
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0010942672, 0.0006341018

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1084
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.096, 0.189
batch losses (mrrl, rdl): 0.0862947926, 0.0007346636

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1117
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.111, 0.001
batch losses (mrrl, rdl): 0.0369336009, 0.0037520146

Epoch over!
epoch time: 11.979

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.338 +- 0.471
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0013559093, 0.0007266311

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.126, 0.159
batch losses (mrrl, rdl): 0.0109554352, 0.0002393505

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 38
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.109, 0.102
batch losses (mrrl, rdl): 0.0005239394, 0.0002209141

Epoch over!
epoch time: 12.121

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 259
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.094, 0.151
batch losses (mrrl, rdl): 0.0327461734, 0.0002154658

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1193
rank avg (pred): 0.398 +- 0.484
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002810999, 0.0005765965

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.292 +- 0.453
mrr vals (pred, true): 0.064, 0.139
batch losses (mrrl, rdl): 0.056176737, 0.0003317587

Epoch over!
epoch time: 12.038

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1115
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0292691998, 0.0042692935

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1062
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.189, 0.198
batch losses (mrrl, rdl): 0.0007783415, 0.0001487933

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.384 +- 0.478
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004611221, 0.0005281441

Epoch over!
epoch time: 11.884

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 611
rank avg (pred): 0.404 +- 0.488
mrr vals (pred, true): 0.053, 0.024
batch losses (mrrl, rdl): 7.43441e-05, 0.000404845

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 413
rank avg (pred): 0.279 +- 0.446
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0012974739, 0.0009054634

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 585
rank avg (pred): 0.408 +- 0.487
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 4.99299e-05, 0.0003930355

Epoch over!
epoch time: 11.899

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 373
rank avg (pred): 0.331 +- 0.449
mrr vals (pred, true): 0.059, 0.076
batch losses (mrrl, rdl): 0.0008513292, 0.0002044516

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.391 +- 0.486
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003290401, 0.0005689182

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 128
rank avg (pred): 0.470 +- 0.495
mrr vals (pred, true): 0.053, 0.068
batch losses (mrrl, rdl): 0.0001019101, 0.0015087842

Epoch over!
epoch time: 11.993

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.272 +- 0.443
mrr vals (pred, true): 0.068, 0.135

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   38 	     0 	 0.05708 	 0.00018 	 m..s
   60 	     1 	 0.06704 	 0.00018 	 m..s
  102 	     2 	 0.11318 	 0.00019 	 MISS
    2 	     3 	 0.05074 	 0.00020 	 m..s
   23 	     4 	 0.05487 	 0.00021 	 m..s
   72 	     5 	 0.07339 	 0.00022 	 m..s
   94 	     6 	 0.10161 	 0.00023 	 MISS
    5 	     7 	 0.05089 	 0.00023 	 m..s
   26 	     8 	 0.05513 	 0.00023 	 m..s
   66 	     9 	 0.07249 	 0.00023 	 m..s
   84 	    10 	 0.10142 	 0.00023 	 MISS
    6 	    11 	 0.05097 	 0.00025 	 m..s
  106 	    12 	 0.11935 	 0.00025 	 MISS
   27 	    13 	 0.05518 	 0.00025 	 m..s
   70 	    14 	 0.07318 	 0.00025 	 m..s
   84 	    15 	 0.10142 	 0.00026 	 MISS
   39 	    16 	 0.05738 	 0.00028 	 m..s
  104 	    17 	 0.11768 	 0.00030 	 MISS
   84 	    18 	 0.10142 	 0.00031 	 MISS
   46 	    19 	 0.06032 	 0.00032 	 m..s
   54 	    20 	 0.06408 	 0.00034 	 m..s
   69 	    21 	 0.07289 	 0.00036 	 m..s
   76 	    22 	 0.07461 	 0.00037 	 m..s
   99 	    23 	 0.11201 	 0.00037 	 MISS
    7 	    24 	 0.05158 	 0.00038 	 m..s
   10 	    25 	 0.05203 	 0.00040 	 m..s
   28 	    26 	 0.05568 	 0.00042 	 m..s
   73 	    27 	 0.07341 	 0.00045 	 m..s
   65 	    28 	 0.06923 	 0.00047 	 m..s
   34 	    29 	 0.05581 	 0.00050 	 m..s
   30 	    30 	 0.05568 	 0.00053 	 m..s
    8 	    31 	 0.05191 	 0.00053 	 m..s
   67 	    32 	 0.07252 	 0.00058 	 m..s
   29 	    33 	 0.05568 	 0.00060 	 m..s
   42 	    34 	 0.05933 	 0.00061 	 m..s
   43 	    35 	 0.05963 	 0.00071 	 m..s
   37 	    36 	 0.05642 	 0.00074 	 m..s
   40 	    37 	 0.05739 	 0.00088 	 m..s
    0 	    38 	 0.05073 	 0.00102 	 m..s
   11 	    39 	 0.05204 	 0.00326 	 m..s
    1 	    40 	 0.05073 	 0.00512 	 m..s
   17 	    41 	 0.05230 	 0.00584 	 m..s
   22 	    42 	 0.05477 	 0.00885 	 m..s
    3 	    43 	 0.05074 	 0.01078 	 m..s
   21 	    44 	 0.05475 	 0.01189 	 m..s
   18 	    45 	 0.05231 	 0.01248 	 m..s
   36 	    46 	 0.05586 	 0.01336 	 m..s
   15 	    47 	 0.05220 	 0.01774 	 m..s
   25 	    48 	 0.05509 	 0.01930 	 m..s
   14 	    49 	 0.05219 	 0.02041 	 m..s
   16 	    50 	 0.05227 	 0.02604 	 ~...
   12 	    51 	 0.05212 	 0.02723 	 ~...
    9 	    52 	 0.05200 	 0.03172 	 ~...
   19 	    53 	 0.05246 	 0.03416 	 ~...
    4 	    54 	 0.05077 	 0.03464 	 ~...
   13 	    55 	 0.05217 	 0.03475 	 ~...
   31 	    56 	 0.05568 	 0.03767 	 ~...
   20 	    57 	 0.05287 	 0.04273 	 ~...
   53 	    58 	 0.06388 	 0.04427 	 ~...
   61 	    59 	 0.06713 	 0.04743 	 ~...
   48 	    60 	 0.06101 	 0.04913 	 ~...
   59 	    61 	 0.06693 	 0.05450 	 ~...
   35 	    62 	 0.05585 	 0.05469 	 ~...
   56 	    63 	 0.06450 	 0.05840 	 ~...
   44 	    64 	 0.05989 	 0.05842 	 ~...
   47 	    65 	 0.06050 	 0.05919 	 ~...
   51 	    66 	 0.06147 	 0.06283 	 ~...
   24 	    67 	 0.05491 	 0.06330 	 ~...
   55 	    68 	 0.06411 	 0.06343 	 ~...
   49 	    69 	 0.06123 	 0.06345 	 ~...
   41 	    70 	 0.05771 	 0.06385 	 ~...
   33 	    71 	 0.05572 	 0.06687 	 ~...
   32 	    72 	 0.05569 	 0.06728 	 ~...
   78 	    73 	 0.07481 	 0.07222 	 ~...
   79 	    74 	 0.07521 	 0.07284 	 ~...
  103 	    75 	 0.11575 	 0.07322 	 m..s
   45 	    76 	 0.05996 	 0.07914 	 ~...
   52 	    77 	 0.06296 	 0.07950 	 ~...
   57 	    78 	 0.06487 	 0.08249 	 ~...
   58 	    79 	 0.06570 	 0.08336 	 ~...
   63 	    80 	 0.06823 	 0.08722 	 ~...
   84 	    81 	 0.10142 	 0.08771 	 ~...
   50 	    82 	 0.06132 	 0.08933 	 ~...
   84 	    83 	 0.10142 	 0.09117 	 ~...
   84 	    84 	 0.10142 	 0.09392 	 ~...
   80 	    85 	 0.07689 	 0.09490 	 ~...
   97 	    86 	 0.10895 	 0.09525 	 ~...
  113 	    87 	 0.15183 	 0.09708 	 m..s
   82 	    88 	 0.08665 	 0.10051 	 ~...
   84 	    89 	 0.10142 	 0.10353 	 ~...
   71 	    90 	 0.07321 	 0.10829 	 m..s
  100 	    91 	 0.11287 	 0.10857 	 ~...
  107 	    92 	 0.11949 	 0.11026 	 ~...
   77 	    93 	 0.07477 	 0.12008 	 m..s
  110 	    94 	 0.13029 	 0.12037 	 ~...
   81 	    95 	 0.08488 	 0.12259 	 m..s
   62 	    96 	 0.06772 	 0.12804 	 m..s
  105 	    97 	 0.11853 	 0.12905 	 ~...
   64 	    98 	 0.06844 	 0.13458 	 m..s
   74 	    99 	 0.07456 	 0.14133 	 m..s
  101 	   100 	 0.11298 	 0.14383 	 m..s
   68 	   101 	 0.07274 	 0.14422 	 m..s
   74 	   102 	 0.07456 	 0.14524 	 m..s
   83 	   103 	 0.09110 	 0.15148 	 m..s
  108 	   104 	 0.12187 	 0.15411 	 m..s
   95 	   105 	 0.10220 	 0.15744 	 m..s
   96 	   106 	 0.10405 	 0.16283 	 m..s
   93 	   107 	 0.10150 	 0.16753 	 m..s
   84 	   108 	 0.10142 	 0.17157 	 m..s
  112 	   109 	 0.14809 	 0.17597 	 ~...
  111 	   110 	 0.13580 	 0.17815 	 m..s
   98 	   111 	 0.11106 	 0.18166 	 m..s
   84 	   112 	 0.10142 	 0.18783 	 m..s
  109 	   113 	 0.12306 	 0.18981 	 m..s
  114 	   114 	 0.16143 	 0.20305 	 m..s
  115 	   115 	 0.20734 	 0.22181 	 ~...
  116 	   116 	 0.22711 	 0.26695 	 m..s
  119 	   117 	 0.26632 	 0.26738 	 ~...
  120 	   118 	 0.28522 	 0.30290 	 ~...
  117 	   119 	 0.25083 	 0.30790 	 m..s
  118 	   120 	 0.26609 	 0.30997 	 m..s
==========================================
r_mrr = 0.7897195816040039
r2_mrr = 0.5422480702400208
spearmanr_mrr@5 = 0.8770726919174194
spearmanr_mrr@10 = 0.9819018840789795
spearmanr_mrr@50 = 0.9670813083648682
spearmanr_mrr@100 = 0.9477775692939758
spearmanr_mrr@All = 0.9508672952651978
==========================================
test time: 0.39
Done Testing dataset DBpedia50
total time taken: 185.24248266220093
training time taken: 179.63891983032227
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7897)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.5422)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8771)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9819)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9671)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9478)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9509)}}, 'test_loss': {'TransE': {'DBpedia50': 1.5217699107452063}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 8808177105511363
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [964, 1087, 416, 1099, 657, 90, 684, 663, 817, 1169, 190, 1070, 319, 1111, 1203, 209, 239, 421, 33, 883, 414, 776, 980, 262, 961, 797, 1113, 874, 813, 483, 325, 343, 629, 408, 250, 760, 256, 213, 444, 745, 890, 535, 2, 1018, 237, 541, 151, 1125, 995, 973, 164, 1027, 25, 1119, 88, 895, 638, 635, 38, 192, 245, 484, 290, 649, 1038, 1137, 221, 210, 1127, 215, 795, 167, 1114, 244, 958, 199, 1154, 226, 246, 19, 716, 819, 990, 548, 576, 431, 391, 1120, 1044, 1147, 451, 1045, 118, 89, 1129, 562, 860, 114, 630, 282, 63, 608, 924, 591, 855, 488, 220, 264, 1207, 620, 551, 436, 181, 862, 524, 146, 438, 777, 217, 260, 233]
valid_ids (0): []
train_ids (1094): [191, 570, 276, 85, 969, 1157, 759, 1035, 678, 285, 266, 422, 564, 446, 120, 493, 922, 746, 149, 1002, 1021, 173, 143, 669, 23, 1104, 410, 946, 24, 65, 294, 1164, 1179, 1043, 230, 232, 747, 1067, 454, 404, 393, 1059, 389, 34, 778, 531, 370, 732, 470, 258, 131, 571, 876, 911, 1053, 1153, 102, 1130, 666, 328, 133, 615, 1123, 208, 227, 871, 763, 171, 744, 891, 672, 364, 752, 656, 533, 1210, 450, 506, 566, 914, 1115, 418, 1201, 494, 688, 1, 967, 539, 611, 921, 278, 989, 626, 1167, 1069, 786, 132, 117, 1178, 82, 1071, 257, 160, 129, 715, 322, 793, 312, 751, 485, 448, 851, 694, 1074, 501, 590, 505, 315, 1185, 675, 555, 1131, 648, 559, 104, 987, 428, 296, 400, 437, 378, 1031, 1181, 22, 612, 372, 558, 800, 712, 187, 527, 174, 1191, 497, 920, 298, 942, 81, 710, 1006, 417, 887, 103, 880, 898, 465, 303, 772, 976, 882, 96, 439, 91, 55, 1092, 650, 122, 481, 931, 424, 944, 1116, 354, 701, 981, 405, 196, 134, 1083, 836, 80, 1098, 1054, 1208, 1064, 664, 766, 469, 31, 427, 473, 1174, 7, 324, 479, 538, 1200, 830, 1042, 166, 161, 9, 368, 704, 938, 1065, 326, 721, 953, 1138, 818, 308, 696, 991, 731, 279, 791, 5, 476, 283, 682, 897, 486, 42, 457, 1202, 625, 1148, 634, 782, 547, 299, 660, 1088, 121, 288, 573, 603, 348, 186, 729, 185, 347, 846, 534, 273, 521, 1198, 908, 423, 894, 651, 532, 286, 147, 542, 1205, 610, 775, 201, 277, 821, 11, 472, 951, 1030, 184, 730, 402, 520, 750, 907, 932, 528, 578, 291, 115, 335, 107, 743, 593, 838, 1072, 642, 1101, 384, 126, 842, 927, 163, 893, 1095, 222, 119, 641, 823, 503, 840, 1066, 583, 175, 219, 339, 955, 130, 1041, 1050, 912, 1060, 74, 1106, 100, 810, 30, 464, 796, 736, 837, 902, 1008, 76, 43, 1142, 783, 605, 511, 12, 714, 866, 784, 255, 905, 628, 105, 556, 235, 455, 21, 724, 1193, 875, 655, 892, 833, 67, 646, 1015, 584, 674, 617, 639, 917, 26, 1197, 1012, 755, 433, 318, 193, 904, 975, 691, 329, 690, 557, 144, 827, 240, 203, 1028, 491, 913, 383, 1168, 519, 29, 574, 1204, 361, 498, 373, 73, 1158, 176, 859, 409, 1182, 419, 61, 1062, 1188, 306, 478, 153, 490, 300, 594, 848, 442, 792, 297, 108, 327, 399, 896, 916, 965, 530, 1194, 654, 477, 48, 711, 1105, 720, 474, 394, 362, 514, 801, 901, 139, 977, 6, 374, 28, 1150, 447, 706, 1199, 62, 77, 84, 495, 680, 613, 988, 756, 367, 839, 780, 607, 197, 434, 685, 900, 683, 344, 1033, 764, 158, 986, 540, 1117, 725, 459, 658, 183, 109, 864, 36, 877, 609, 334, 1022, 1102, 1046, 456, 336, 261, 1085, 66, 737, 323, 8, 377, 979, 466, 661, 413, 577, 263, 717, 211, 722, 145, 929, 188, 142, 124, 723, 1047, 218, 739, 934, 407, 482, 906, 753, 86, 71, 886, 333, 928, 568, 57, 668, 254, 1132, 489, 627, 94, 14, 53, 748, 598, 870, 1103, 305, 204, 371, 17, 398, 452, 350, 1016, 395, 676, 1001, 18, 1189, 804, 925, 695, 853, 386, 757, 622, 252, 194, 75, 978, 930, 1165, 808, 467, 1029, 125, 919, 1007, 259, 523, 289, 1146, 487, 1049, 95, 205, 1000, 513, 342, 165, 346, 1160, 652, 923, 420, 910, 758, 1133, 236, 59, 769, 970, 1048, 600, 579, 58, 170, 1186, 356, 858, 552, 353, 845, 637, 589, 596, 1162, 1139, 4, 512, 1052, 966, 618, 238, 453, 604, 352, 1023, 47, 915, 1143, 888, 1079, 1091, 1155, 1020, 251, 549, 950, 304, 265, 35, 1134, 677, 1126, 112, 432, 281, 141, 1024, 401, 844, 247, 69, 623, 726, 993, 854, 168, 234, 621, 380, 412, 1017, 98, 162, 645, 41, 475, 592, 231, 994, 274, 667, 1211, 37, 40, 492, 948, 903, 396, 225, 956, 317, 1176, 1080, 647, 773, 500, 809, 771, 403, 575, 15, 1108, 580, 770, 508, 560, 788, 313, 554, 673, 982, 1081, 572, 1145, 873, 83, 1213, 768, 546, 563, 735, 689, 1141, 155, 0, 799, 206, 180, 207, 458, 1056, 996, 70, 101, 10, 461, 949, 1019, 1151, 767, 522, 381, 172, 831, 754, 806, 807, 1183, 719, 345, 971, 1051, 1036, 865, 449, 765, 229, 516, 275, 952, 429, 614, 27, 707, 60, 309, 1073, 128, 443, 387, 947, 1177, 116, 20, 974, 781, 606, 195, 959, 496, 1003, 960, 659, 1058, 87, 1063, 709, 526, 692, 382, 529, 847, 441, 879, 1195, 228, 662, 1009, 341, 152, 507, 1034, 899, 1109, 242, 624, 565, 631, 1082, 742, 708, 1166, 79, 178, 738, 968, 1097, 1076, 543, 567, 582, 415, 869, 159, 390, 828, 307, 941, 1184, 933, 727, 820, 992, 861, 200, 214, 1144, 284, 700, 1171, 1128, 212, 138, 111, 595, 653, 177, 826, 1214, 1039, 587, 794, 957, 272, 1180, 1055, 665, 311, 599, 97, 863, 440, 72, 445, 602, 687, 1061, 216, 136, 49, 553, 832, 1057, 45, 636, 954, 1196, 198, 733, 517, 52, 1075, 985, 320, 1011, 388, 825, 1156, 113, 1090, 137, 878, 1124, 292, 425, 270, 295, 681, 363, 338, 705, 360, 223, 545, 741, 935, 632, 515, 110, 1192, 44, 156, 616, 1089, 150, 581, 337, 426, 824, 749, 1100, 937, 1206, 812, 1212, 375, 1170, 963, 510, 1037, 586, 644, 271, 406, 1112, 834, 885, 51, 302, 358, 321, 926, 881, 798, 1122, 561, 909, 179, 918, 1078, 1149, 939, 518, 430, 1010, 544, 1121, 502, 287, 940, 157, 1086, 702, 1004, 884, 537, 154, 843, 310, 779, 1163, 135, 850, 829, 349, 202, 728, 703, 679, 268, 1068, 93, 983, 46, 355, 106, 1118, 435, 248, 785, 999, 972, 1135, 822, 867, 1175, 340, 460, 619, 1005, 718, 787, 984, 330, 480, 525, 301, 868, 169, 789, 713, 945, 293, 499, 99, 856, 463, 68, 1161, 997, 774, 585, 39, 601, 790, 734, 1094, 1084, 852, 699, 1209, 314, 811, 369, 462, 504, 1013, 550, 1152, 280, 835, 92, 56, 253, 1136, 332, 802, 698, 267, 1110, 359, 243, 316, 468, 697, 670, 123, 411, 351, 815, 379, 643, 224, 936, 597, 397, 148, 331, 841, 943, 962, 536, 762, 509, 269, 54, 1025, 1096, 249, 998, 693, 385, 1173, 872, 392, 569, 1032, 1014, 140, 1107, 1093, 64, 376, 1026, 1140, 761, 803, 1077, 32, 16, 241, 366, 671, 805, 471, 50, 357, 640, 633, 1187, 3, 127, 1190, 849, 1172, 1159, 189, 182, 78, 13, 889, 365, 857, 686, 1040, 814, 816, 740, 588]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1330777416949727
the save name prefix for this run is:  chkpt-ID_1330777416949727_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1025
rank avg (pred): 0.532 +- 0.001
mrr vals (pred, true): 0.000, 0.188
batch losses (mrrl, rdl): 0.0, 0.0023365614

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.354 +- 0.178
mrr vals (pred, true): 0.035, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003788526

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.100 +- 0.063
mrr vals (pred, true): 0.223, 0.193
batch losses (mrrl, rdl): 0.0, 3.73037e-05

Epoch over!
epoch time: 11.873

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 501
rank avg (pred): 0.261 +- 0.161
mrr vals (pred, true): 0.214, 0.050
batch losses (mrrl, rdl): 0.0, 9.53361e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 686
rank avg (pred): 0.393 +- 0.272
mrr vals (pred, true): 0.243, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001318405

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 145
rank avg (pred): 0.313 +- 0.278
mrr vals (pred, true): 0.271, 0.018
batch losses (mrrl, rdl): 0.0, 0.0002747811

Epoch over!
epoch time: 11.75

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 708
rank avg (pred): 0.427 +- 0.331
mrr vals (pred, true): 0.257, 0.001
batch losses (mrrl, rdl): 0.0, 4.57605e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 267
rank avg (pred): 0.117 +- 0.107
mrr vals (pred, true): 0.304, 0.097
batch losses (mrrl, rdl): 0.0, 3.94911e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 597
rank avg (pred): 0.337 +- 0.314
mrr vals (pred, true): 0.302, 0.005
batch losses (mrrl, rdl): 0.0, 2.19668e-05

Epoch over!
epoch time: 11.879

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 721
rank avg (pred): 0.349 +- 0.317
mrr vals (pred, true): 0.294, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002658402

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.318 +- 0.290
mrr vals (pred, true): 0.298, 0.116
batch losses (mrrl, rdl): 0.0, 0.0004631135

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.334 +- 0.294
mrr vals (pred, true): 0.292, 0.033
batch losses (mrrl, rdl): 0.0, 0.0003028001

Epoch over!
epoch time: 11.859

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.294 +- 0.287
mrr vals (pred, true): 0.319, 0.128
batch losses (mrrl, rdl): 0.0, 0.000297265

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 112
rank avg (pred): 0.299 +- 0.293
mrr vals (pred, true): 0.304, 0.028
batch losses (mrrl, rdl): 0.0, 0.0002365301

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.303 +- 0.297
mrr vals (pred, true): 0.295, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001131324

Epoch over!
epoch time: 11.705

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 810
rank avg (pred): 0.078 +- 0.078
mrr vals (pred, true): 0.330, 0.074
batch losses (mrrl, rdl): 0.7829021215, 8.89267e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 706
rank avg (pred): 0.642 +- 0.267
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 9.784e-07, 0.0004570802

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 62
rank avg (pred): 0.227 +- 0.135
mrr vals (pred, true): 0.105, 0.101
batch losses (mrrl, rdl): 0.0001388562, 0.0004371307

Epoch over!
epoch time: 12.108

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.419 +- 0.232
mrr vals (pred, true): 0.076, 0.033
batch losses (mrrl, rdl): 0.0068386998, 0.0007874214

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.035 +- 0.022
mrr vals (pred, true): 0.137, 0.224
batch losses (mrrl, rdl): 0.0745433494, 7.01333e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 659
rank avg (pred): 0.447 +- 0.203
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 7.12248e-05, 0.0001149287

Epoch over!
epoch time: 12.008

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.290 +- 0.175
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.015132661, 0.0008930341

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 825
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.195, 0.219
batch losses (mrrl, rdl): 0.0057163546, 8.77534e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1001
rank avg (pred): 0.238 +- 0.167
mrr vals (pred, true): 0.090, 0.151
batch losses (mrrl, rdl): 0.0366159379, 7.83761e-05

Epoch over!
epoch time: 11.99

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1175
rank avg (pred): 0.434 +- 0.182
mrr vals (pred, true): 0.051, 0.073
batch losses (mrrl, rdl): 2.6257e-06, 0.0003735689

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.285 +- 0.197
mrr vals (pred, true): 0.084, 0.135
batch losses (mrrl, rdl): 0.0250831414, 0.0002579225

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 157
rank avg (pred): 0.424 +- 0.175
mrr vals (pred, true): 0.049, 0.020
batch losses (mrrl, rdl): 1.8363e-05, 0.0009969029

Epoch over!
epoch time: 12.009

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 558
rank avg (pred): 0.403 +- 0.192
mrr vals (pred, true): 0.056, 0.018
batch losses (mrrl, rdl): 0.0004079233, 0.0002729483

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 450
rank avg (pred): 0.373 +- 0.196
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001518536, 0.0001423625

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1006
rank avg (pred): 0.297 +- 0.217
mrr vals (pred, true): 0.110, 0.186
batch losses (mrrl, rdl): 0.0586290509, 0.0003809651

Epoch over!
epoch time: 12.086

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.090 +- 0.145
mrr vals (pred, true): 0.223, 0.195
batch losses (mrrl, rdl): 0.0081123989, 2.49249e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.211 +- 0.192
mrr vals (pred, true): 0.136, 0.192
batch losses (mrrl, rdl): 0.0311572812, 9.11891e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 939
rank avg (pred): 0.263 +- 0.187
mrr vals (pred, true): 0.080, 0.155
batch losses (mrrl, rdl): 0.0560506359, 0.0001568191

Epoch over!
epoch time: 12.155

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.282 +- 0.195
mrr vals (pred, true): 0.079, 0.114
batch losses (mrrl, rdl): 0.0124539165, 7.46595e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 925
rank avg (pred): 0.286 +- 0.190
mrr vals (pred, true): 0.073, 0.109
batch losses (mrrl, rdl): 0.0127359284, 6.3812e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 594
rank avg (pred): 0.391 +- 0.147
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 8.663e-07, 0.0001012847

Epoch over!
epoch time: 11.978

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.237 +- 0.190
mrr vals (pred, true): 0.068, 0.085
batch losses (mrrl, rdl): 0.0032220499, 0.0004418033

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.133 +- 0.167
mrr vals (pred, true): 0.152, 0.144
batch losses (mrrl, rdl): 0.0006737504, 3.75599e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.229 +- 0.197
mrr vals (pred, true): 0.108, 0.162
batch losses (mrrl, rdl): 0.0294510573, 8.26622e-05

Epoch over!
epoch time: 11.927

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.238 +- 0.201
mrr vals (pred, true): 0.119, 0.201
batch losses (mrrl, rdl): 0.0661359802, 0.0001240214

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1053
rank avg (pred): 0.106 +- 0.158
mrr vals (pred, true): 0.249, 0.265
batch losses (mrrl, rdl): 0.0026971158, 5.47236e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 482
rank avg (pred): 0.299 +- 0.177
mrr vals (pred, true): 0.062, 0.000
batch losses (mrrl, rdl): 0.0013644747, 0.0005885353

Epoch over!
epoch time: 12.12

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 435
rank avg (pred): 0.309 +- 0.174
mrr vals (pred, true): 0.063, 0.000
batch losses (mrrl, rdl): 0.0016050586, 0.0005381162

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 840
rank avg (pred): 0.282 +- 0.181
mrr vals (pred, true): 0.068, 0.151
batch losses (mrrl, rdl): 0.0700540319, 0.000172433

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 85
rank avg (pred): 0.371 +- 0.165
mrr vals (pred, true): 0.049, 0.021
batch losses (mrrl, rdl): 5.322e-06, 0.0003478967

Epoch over!
epoch time: 12.036

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.192 +- 0.173
mrr vals (pred, true): 0.113, 0.002

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   47 	     0 	 0.05009 	 0.00018 	 m..s
   45 	     1 	 0.04984 	 0.00019 	 m..s
   19 	     2 	 0.04849 	 0.00019 	 m..s
   90 	     3 	 0.10599 	 0.00019 	 MISS
  104 	     4 	 0.11147 	 0.00020 	 MISS
   94 	     5 	 0.10882 	 0.00020 	 MISS
   59 	     6 	 0.05093 	 0.00021 	 m..s
   24 	     7 	 0.04865 	 0.00021 	 m..s
    0 	     8 	 0.04474 	 0.00022 	 m..s
    3 	     9 	 0.04650 	 0.00023 	 m..s
   41 	    10 	 0.04966 	 0.00024 	 m..s
   75 	    11 	 0.08828 	 0.00024 	 m..s
   82 	    12 	 0.09650 	 0.00024 	 m..s
   50 	    13 	 0.05022 	 0.00026 	 m..s
   29 	    14 	 0.04891 	 0.00027 	 m..s
   99 	    15 	 0.10993 	 0.00028 	 MISS
   49 	    16 	 0.05019 	 0.00028 	 m..s
   73 	    17 	 0.08612 	 0.00029 	 m..s
   92 	    18 	 0.10815 	 0.00030 	 MISS
   93 	    19 	 0.10860 	 0.00030 	 MISS
  108 	    20 	 0.12926 	 0.00030 	 MISS
   78 	    21 	 0.09418 	 0.00031 	 m..s
   15 	    22 	 0.04844 	 0.00031 	 m..s
   13 	    23 	 0.04839 	 0.00032 	 m..s
   63 	    24 	 0.05165 	 0.00032 	 m..s
   42 	    25 	 0.04967 	 0.00033 	 m..s
   14 	    26 	 0.04844 	 0.00033 	 m..s
   91 	    27 	 0.10799 	 0.00035 	 MISS
   87 	    28 	 0.10552 	 0.00036 	 MISS
    7 	    29 	 0.04776 	 0.00037 	 m..s
   20 	    30 	 0.04849 	 0.00039 	 m..s
   11 	    31 	 0.04827 	 0.00039 	 m..s
    1 	    32 	 0.04480 	 0.00040 	 m..s
   32 	    33 	 0.04900 	 0.00040 	 m..s
   16 	    34 	 0.04844 	 0.00042 	 m..s
   39 	    35 	 0.04939 	 0.00042 	 m..s
   34 	    36 	 0.04920 	 0.00046 	 m..s
   89 	    37 	 0.10581 	 0.00053 	 MISS
   81 	    38 	 0.09480 	 0.00053 	 m..s
  110 	    39 	 0.13009 	 0.00055 	 MISS
   54 	    40 	 0.05031 	 0.00057 	 m..s
   56 	    41 	 0.05075 	 0.00061 	 m..s
   21 	    42 	 0.04849 	 0.00064 	 m..s
  102 	    43 	 0.11031 	 0.00066 	 MISS
   97 	    44 	 0.10974 	 0.00074 	 MISS
   58 	    45 	 0.05089 	 0.00076 	 m..s
   98 	    46 	 0.10983 	 0.00080 	 MISS
   28 	    47 	 0.04886 	 0.00089 	 m..s
   48 	    48 	 0.05017 	 0.00089 	 m..s
   86 	    49 	 0.10552 	 0.00092 	 MISS
   61 	    50 	 0.05129 	 0.00093 	 m..s
   52 	    51 	 0.05026 	 0.00111 	 m..s
   57 	    52 	 0.05088 	 0.00194 	 m..s
   70 	    53 	 0.06010 	 0.00203 	 m..s
  106 	    54 	 0.11288 	 0.00229 	 MISS
   25 	    55 	 0.04868 	 0.00237 	 m..s
    2 	    56 	 0.04488 	 0.00326 	 m..s
    4 	    57 	 0.04702 	 0.00367 	 m..s
   27 	    58 	 0.04876 	 0.00784 	 m..s
   26 	    59 	 0.04870 	 0.01336 	 m..s
   23 	    60 	 0.04861 	 0.01610 	 m..s
   18 	    61 	 0.04845 	 0.01915 	 ~...
    5 	    62 	 0.04754 	 0.02020 	 ~...
   17 	    63 	 0.04844 	 0.02299 	 ~...
    9 	    64 	 0.04779 	 0.03155 	 ~...
    8 	    65 	 0.04777 	 0.03172 	 ~...
   22 	    66 	 0.04856 	 0.03464 	 ~...
   30 	    67 	 0.04896 	 0.04190 	 ~...
   31 	    68 	 0.04896 	 0.04193 	 ~...
   10 	    69 	 0.04780 	 0.04198 	 ~...
   35 	    70 	 0.04921 	 0.05450 	 ~...
   43 	    71 	 0.04967 	 0.05469 	 ~...
   55 	    72 	 0.05049 	 0.06079 	 ~...
   12 	    73 	 0.04828 	 0.06098 	 ~...
   66 	    74 	 0.05403 	 0.06213 	 ~...
   44 	    75 	 0.04975 	 0.06811 	 ~...
    6 	    76 	 0.04774 	 0.07008 	 ~...
   51 	    77 	 0.05022 	 0.07718 	 ~...
   33 	    78 	 0.04903 	 0.07838 	 ~...
   36 	    79 	 0.04924 	 0.08016 	 m..s
   37 	    80 	 0.04924 	 0.08037 	 m..s
   40 	    81 	 0.04940 	 0.08185 	 m..s
   67 	    82 	 0.05531 	 0.08187 	 ~...
   60 	    83 	 0.05112 	 0.08454 	 m..s
   64 	    84 	 0.05292 	 0.09117 	 m..s
   38 	    85 	 0.04939 	 0.09210 	 m..s
   53 	    86 	 0.05029 	 0.09353 	 m..s
   76 	    87 	 0.08855 	 0.09490 	 ~...
   69 	    88 	 0.06009 	 0.09822 	 m..s
   72 	    89 	 0.06718 	 0.09849 	 m..s
   79 	    90 	 0.09426 	 0.10188 	 ~...
   46 	    91 	 0.04988 	 0.10197 	 m..s
   62 	    92 	 0.05148 	 0.10353 	 m..s
   68 	    93 	 0.05562 	 0.10539 	 m..s
   77 	    94 	 0.09247 	 0.10647 	 ~...
   65 	    95 	 0.05371 	 0.10666 	 m..s
   80 	    96 	 0.09471 	 0.11054 	 ~...
  105 	    97 	 0.11201 	 0.11446 	 ~...
   83 	    98 	 0.09810 	 0.12796 	 ~...
  107 	    99 	 0.11428 	 0.13345 	 ~...
   85 	   100 	 0.10258 	 0.13455 	 m..s
   84 	   101 	 0.09936 	 0.13901 	 m..s
  109 	   102 	 0.12958 	 0.13936 	 ~...
   71 	   103 	 0.06177 	 0.14257 	 m..s
   74 	   104 	 0.08768 	 0.14672 	 m..s
  103 	   105 	 0.11040 	 0.15452 	 m..s
  113 	   106 	 0.17789 	 0.15925 	 ~...
   88 	   107 	 0.10579 	 0.15935 	 m..s
  100 	   108 	 0.10994 	 0.16078 	 m..s
   96 	   109 	 0.10934 	 0.16883 	 m..s
   95 	   110 	 0.10907 	 0.16968 	 m..s
  101 	   111 	 0.11005 	 0.17400 	 m..s
  112 	   112 	 0.13103 	 0.17817 	 m..s
  111 	   113 	 0.13046 	 0.19866 	 m..s
  114 	   114 	 0.22168 	 0.20156 	 ~...
  115 	   115 	 0.22706 	 0.20969 	 ~...
  116 	   116 	 0.23440 	 0.22181 	 ~...
  118 	   117 	 0.24272 	 0.24817 	 ~...
  117 	   118 	 0.24092 	 0.27003 	 ~...
  120 	   119 	 0.31653 	 0.31534 	 ~...
  119 	   120 	 0.29744 	 0.32098 	 ~...
==========================================
r_mrr = 0.7114453911781311
r2_mrr = 0.4081173539161682
spearmanr_mrr@5 = 0.9377151727676392
spearmanr_mrr@10 = 0.9235866069793701
spearmanr_mrr@50 = 0.9360822439193726
spearmanr_mrr@100 = 0.9527324438095093
spearmanr_mrr@All = 0.9560737609863281
==========================================
test time: 0.389
Done Testing dataset DBpedia50
total time taken: 184.66409039497375
training time taken: 179.95439791679382
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.7114)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.4081)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.9377)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.9236)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9361)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9527)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9561)}}, 'test_loss': {'TransE': {'DBpedia50': 1.3060139580193209}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 5089547278025486
Starting TWIG!
Loading datasets
Loading TransE...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [787, 937, 29, 301, 1070, 635, 927, 196, 1115, 525, 300, 312, 736, 928, 889, 864, 298, 43, 52, 350, 1173, 535, 975, 554, 35, 817, 254, 463, 464, 1072, 417, 957, 800, 979, 452, 720, 209, 946, 863, 802, 1027, 398, 728, 1004, 219, 1006, 44, 12, 696, 263, 580, 100, 998, 530, 200, 467, 461, 865, 1016, 793, 965, 485, 1021, 1048, 950, 1212, 241, 949, 477, 476, 259, 275, 658, 792, 724, 1182, 282, 67, 997, 407, 506, 90, 95, 687, 992, 1064, 829, 905, 146, 977, 909, 166, 502, 544, 897, 171, 1189, 1158, 370, 754, 212, 152, 405, 118, 1118, 652, 1053, 679, 164, 1124, 468, 732, 240, 874, 859, 1194, 27, 1161, 1208, 547, 710]
valid_ids (0): []
train_ids (1094): [1008, 30, 264, 1040, 429, 1199, 83, 938, 511, 684, 879, 908, 1096, 1082, 235, 780, 339, 840, 1170, 430, 46, 790, 717, 126, 40, 640, 133, 990, 424, 1176, 986, 344, 584, 685, 92, 379, 285, 872, 267, 329, 594, 1002, 423, 1147, 167, 42, 862, 366, 17, 248, 1139, 626, 674, 1045, 903, 1063, 216, 120, 70, 1140, 1135, 910, 215, 712, 1203, 891, 58, 701, 314, 636, 286, 678, 1132, 1191, 898, 289, 776, 336, 822, 617, 706, 139, 884, 1019, 615, 586, 596, 1069, 165, 994, 272, 278, 1011, 236, 1037, 565, 311, 604, 351, 80, 831, 832, 338, 772, 1138, 522, 875, 1054, 136, 1177, 395, 188, 337, 915, 825, 1155, 1046, 322, 589, 1012, 28, 142, 764, 406, 471, 265, 1166, 99, 688, 116, 1156, 914, 475, 119, 361, 873, 644, 624, 537, 1101, 75, 797, 266, 576, 82, 552, 608, 693, 974, 297, 667, 437, 291, 761, 945, 107, 442, 33, 926, 602, 976, 184, 4, 447, 129, 571, 474, 1157, 354, 15, 1078, 1003, 357, 1030, 819, 1092, 854, 1085, 996, 32, 671, 560, 1091, 1029, 849, 161, 1175, 936, 655, 971, 381, 670, 798, 252, 697, 750, 1071, 203, 543, 597, 871, 402, 848, 568, 93, 513, 1039, 175, 1129, 587, 923, 751, 548, 741, 707, 450, 1133, 1007, 434, 162, 661, 1026, 440, 220, 823, 489, 108, 669, 1162, 987, 230, 1080, 851, 1214, 686, 494, 526, 607, 614, 238, 838, 705, 319, 247, 900, 359, 834, 155, 112, 746, 722, 193, 1009, 585, 813, 618, 555, 642, 677, 1094, 1160, 1035, 833, 714, 572, 1113, 492, 20, 190, 493, 951, 625, 1109, 699, 84, 270, 377, 481, 305, 341, 536, 31, 76, 364, 1130, 1184, 963, 540, 432, 887, 723, 153, 7, 791, 683, 13, 556, 721, 262, 308, 806, 1144, 426, 953, 1081, 61, 244, 783, 211, 1110, 404, 331, 105, 482, 490, 803, 656, 326, 205, 748, 156, 151, 726, 828, 258, 392, 878, 1057, 245, 564, 1017, 747, 738, 961, 389, 170, 1056, 1152, 601, 606, 553, 91, 892, 154, 634, 514, 1127, 869, 98, 342, 982, 866, 826, 113, 520, 901, 456, 1098, 470, 78, 1028, 56, 436, 518, 9, 1159, 824, 969, 427, 1151, 836, 808, 466, 478, 559, 956, 39, 1204, 373, 340, 49, 315, 692, 599, 8, 104, 302, 666, 74, 1117, 399, 804, 268, 630, 645, 1186, 72, 277, 89, 138, 899, 1126, 149, 16, 85, 296, 504, 144, 1076, 242, 307, 411, 130, 169, 1005, 433, 207, 659, 919, 657, 993, 197, 579, 206, 561, 1095, 739, 391, 959, 810, 71, 185, 1051, 1116, 922, 1104, 542, 1202, 11, 182, 766, 400, 1020, 249, 742, 283, 106, 1042, 501, 284, 718, 1099, 1043, 509, 88, 978, 1197, 725, 622, 1145, 348, 360, 558, 852, 676, 1185, 292, 94, 1136, 1024, 79, 893, 1150, 911, 274, 860, 327, 243, 228, 36, 480, 745, 1196, 256, 195, 1108, 439, 566, 1014, 1084, 970, 1163, 346, 163, 512, 650, 942, 664, 310, 168, 773, 68, 631, 369, 794, 421, 737, 1167, 66, 435, 647, 1119, 371, 653, 610, 223, 839, 309, 409, 985, 181, 1210, 469, 173, 455, 251, 306, 958, 21, 947, 1060, 287, 719, 498, 820, 855, 1062, 204, 416, 782, 499, 1088, 59, 397, 811, 96, 174, 713, 1077, 816, 763, 87, 330, 114, 1065, 1206, 966, 313, 731, 1190, 48, 867, 145, 62, 462, 18, 261, 1052, 1105, 562, 288, 532, 964, 807, 821, 883, 488, 1066, 902, 396, 637, 147, 1122, 765, 1154, 529, 385, 744, 131, 178, 229, 2, 378, 122, 609, 269, 38, 941, 708, 47, 743, 510, 595, 449, 621, 60, 524, 1025, 408, 213, 1022, 214, 1038, 293, 110, 1142, 1106, 1174, 629, 913, 519, 1180, 53, 809, 255, 387, 827, 487, 441, 375, 574, 704, 63, 128, 183, 186, 775, 380, 702, 843, 1153, 124, 210, 1183, 343, 534, 6, 1067, 567, 50, 727, 317, 1033, 231, 358, 237, 176, 217, 598, 944, 443, 1079, 202, 638, 208, 904, 955, 557, 25, 5, 1121, 845, 363, 1165, 374, 578, 785, 491, 691, 453, 115, 1141, 352, 1083, 222, 694, 273, 581, 1049, 14, 137, 22, 1172, 844, 316, 0, 225, 103, 189, 425, 10, 460, 523, 299, 495, 246, 479, 393, 592, 451, 973, 730, 484, 668, 778, 279, 716, 888, 508, 324, 1146, 454, 500, 709, 698, 117, 861, 1131, 199, 1086, 734, 984, 1050, 896, 980, 1198, 528, 431, 989, 916, 842, 349, 952, 332, 445, 1125, 224, 295, 880, 675, 158, 1111, 24, 894, 962, 760, 774, 386, 538, 740, 1213, 649, 260, 886, 157, 541, 1181, 895, 132, 715, 603, 383, 179, 907, 935, 77, 496, 280, 777, 700, 912, 682, 1100, 57, 367, 192, 931, 818, 34, 786, 125, 753, 943, 355, 1195, 1089, 1059, 680, 23, 323, 41, 665, 570, 353, 65, 384, 403, 801, 483, 198, 1075, 815, 805, 177, 102, 639, 218, 590, 835, 194, 917, 1207, 1, 444, 1044, 372, 1032, 140, 752, 1087, 465, 613, 1097, 1034, 1102, 660, 633, 616, 846, 939, 1192, 473, 45, 1010, 575, 521, 812, 516, 253, 159, 233, 967, 611, 779, 1178, 1058, 250, 303, 870, 876, 1187, 376, 632, 507, 853, 413, 881, 1164, 232, 1023, 81, 733, 160, 382, 134, 459, 769, 26, 191, 318, 438, 51, 837, 281, 290, 527, 1073, 918, 1090, 755, 428, 988, 294, 1000, 549, 54, 623, 172, 954, 583, 1001, 593, 410, 304, 968, 1068, 847, 588, 73, 1031, 711, 472, 930, 135, 868, 550, 109, 148, 651, 1103, 420, 619, 1201, 226, 781, 858, 789, 345, 187, 127, 995, 681, 690, 69, 768, 1209, 320, 1061, 759, 257, 394, 531, 486, 356, 920, 505, 627, 1211, 1123, 612, 1200, 271, 1137, 933, 448, 762, 150, 960, 757, 857, 663, 458, 551, 422, 758, 885, 546, 628, 227, 703, 457, 569, 515, 830, 1179, 221, 1193, 591, 418, 1047, 662, 934, 111, 850, 412, 1149, 121, 771, 648, 1055, 276, 695, 1143, 890, 201, 333, 1188, 925, 924, 999, 563, 335, 334, 365, 654, 981, 646, 643, 321, 1120, 795, 503, 767, 1205, 770, 388, 37, 1107, 991, 1036, 756, 882, 1171, 419, 673, 641, 972, 1013, 784, 414, 390, 180, 1015, 1128, 735, 796, 689, 1074, 906, 921, 573, 856, 19, 1093, 1041, 347, 814, 1168, 55, 841, 545, 940, 517, 788, 948, 729, 577, 368, 1148, 3, 86, 446, 605, 582, 97, 929, 141, 362, 415, 1018, 1134, 401, 239, 600, 328, 799, 1169, 234, 325, 497, 1114, 101, 983, 749, 1112, 932, 533, 539, 143, 620, 672, 123, 64, 877]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5143361267777201
the save name prefix for this run is:  chkpt-ID_5143361267777201_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'TransE': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 761
rank avg (pred): 0.516 +- 0.004
mrr vals (pred, true): 0.000, 0.157
batch losses (mrrl, rdl): 0.0, 0.0018680432

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 454
rank avg (pred): 0.369 +- 0.279
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001216975

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1171
rank avg (pred): 0.352 +- 0.284
mrr vals (pred, true): 0.227, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001361755

Epoch over!
epoch time: 11.843

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.123 +- 0.102
mrr vals (pred, true): 0.298, 0.104
batch losses (mrrl, rdl): 0.0, 4.76365e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 549
rank avg (pred): 0.270 +- 0.218
mrr vals (pred, true): 0.270, 0.042
batch losses (mrrl, rdl): 0.0, 6.96776e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 674
rank avg (pred): 0.352 +- 0.287
mrr vals (pred, true): 0.306, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002547232

Epoch over!
epoch time: 11.801

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 225
rank avg (pred): 0.338 +- 0.278
mrr vals (pred, true): 0.318, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004076168

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 852
rank avg (pred): 0.320 +- 0.269
mrr vals (pred, true): 0.349, 0.157
batch losses (mrrl, rdl): 0.0, 0.0003149333

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 441
rank avg (pred): 0.335 +- 0.282
mrr vals (pred, true): 0.345, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001109503

Epoch over!
epoch time: 11.805

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 560
rank avg (pred): 0.257 +- 0.216
mrr vals (pred, true): 0.365, 0.092
batch losses (mrrl, rdl): 0.0, 7.47403e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 513
rank avg (pred): 0.250 +- 0.212
mrr vals (pred, true): 0.388, 0.028
batch losses (mrrl, rdl): 0.0, 9.44259e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.317 +- 0.270
mrr vals (pred, true): 0.395, 0.081
batch losses (mrrl, rdl): 0.0, 0.000490403

Epoch over!
epoch time: 11.88

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.321 +- 0.274
mrr vals (pred, true): 0.398, 0.037
batch losses (mrrl, rdl): 0.0, 5.01717e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 980
rank avg (pred): 0.080 +- 0.068
mrr vals (pred, true): 0.405, 0.315
batch losses (mrrl, rdl): 0.0, 2.58846e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.305 +- 0.261
mrr vals (pred, true): 0.401, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003278596

Epoch over!
epoch time: 11.84

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.075 +- 0.064
mrr vals (pred, true): 0.407, 0.248
batch losses (mrrl, rdl): 0.2512143254, 1.13623e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 353
rank avg (pred): 0.449 +- 0.268
mrr vals (pred, true): 0.051, 0.109
batch losses (mrrl, rdl): 0.0334332585, 0.0012305747

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.102 +- 0.081
mrr vals (pred, true): 0.103, 0.129
batch losses (mrrl, rdl): 0.0068586525, 2.83091e-05

Epoch over!
epoch time: 12.255

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.184 +- 0.136
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0055986326, 0.0016686269

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 256
rank avg (pred): 0.073 +- 0.055
mrr vals (pred, true): 0.100, 0.143
batch losses (mrrl, rdl): 0.0178658757, 2.44487e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 183
rank avg (pred): 0.412 +- 0.278
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0009630388, 9.50567e-05

Epoch over!
epoch time: 11.917

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 551
rank avg (pred): 0.411 +- 0.277
mrr vals (pred, true): 0.065, 0.092
batch losses (mrrl, rdl): 0.0021424757, 0.0006393666

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.174, 0.185
batch losses (mrrl, rdl): 0.001211392, 0.0001963463

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 639
rank avg (pred): 0.444 +- 0.271
mrr vals (pred, true): 0.059, 0.002
batch losses (mrrl, rdl): 0.0008561372, 0.0003144664

Epoch over!
epoch time: 12.118

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 622
rank avg (pred): 0.442 +- 0.265
mrr vals (pred, true): 0.053, 0.005
batch losses (mrrl, rdl): 7.05573e-05, 0.0003609041

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 902
rank avg (pred): 0.268 +- 0.183
mrr vals (pred, true): 0.081, 0.106
batch losses (mrrl, rdl): 0.0064103492, 0.0006081632

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 328
rank avg (pred): 0.437 +- 0.265
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 1.3823e-05, 0.0010444107

Epoch over!
epoch time: 12.002

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.438 +- 0.273
mrr vals (pred, true): 0.065, 0.037
batch losses (mrrl, rdl): 0.0022191883, 0.0008482122

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 620
rank avg (pred): 0.442 +- 0.272
mrr vals (pred, true): 0.056, 0.035
batch losses (mrrl, rdl): 0.0004137288, 0.000479016

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.446 +- 0.275
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004239568, 1.46913e-05

Epoch over!
epoch time: 11.915

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 674
rank avg (pred): 0.393 +- 0.257
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0059760432, 0.0001524101

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 390
rank avg (pred): 0.440 +- 0.268
mrr vals (pred, true): 0.056, 0.050
batch losses (mrrl, rdl): 0.0003298317, 0.0012301739

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 234
rank avg (pred): 0.445 +- 0.265
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.45314e-05, 1.89381e-05

Epoch over!
epoch time: 12.379

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.331 +- 0.230
mrr vals (pred, true): 0.067, 0.101
batch losses (mrrl, rdl): 0.0118898228, 0.000953845

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 556
rank avg (pred): 0.430 +- 0.263
mrr vals (pred, true): 0.061, 0.048
batch losses (mrrl, rdl): 0.0012820293, 0.0005986768

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.063 +- 0.044
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0209682696, 0.0033109582

Epoch over!
epoch time: 12.095

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 587
rank avg (pred): 0.380 +- 0.242
mrr vals (pred, true): 0.064, 0.036
batch losses (mrrl, rdl): 0.0018360983, 0.0001554161

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 367
rank avg (pred): 0.424 +- 0.268
mrr vals (pred, true): 0.066, 0.069
batch losses (mrrl, rdl): 0.002476396, 0.0009979371

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 18
rank avg (pred): 0.066 +- 0.047
mrr vals (pred, true): 0.088, 0.086
batch losses (mrrl, rdl): 0.0148012405, 8.80318e-05

Epoch over!
epoch time: 12.15

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.275 +- 0.206
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0034012718, 0.0009803358

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 21
rank avg (pred): 0.092 +- 0.060
mrr vals (pred, true): 0.086, 0.066
batch losses (mrrl, rdl): 0.0130272238, 2.1035e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 960
rank avg (pred): 0.341 +- 0.237
mrr vals (pred, true): 0.071, 0.001
batch losses (mrrl, rdl): 0.0044019278, 0.0001414714

Epoch over!
epoch time: 12.284

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 791
rank avg (pred): 0.231 +- 0.169
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0155896191, 0.0010799401

running batch: 500 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 871
rank avg (pred): 0.284 +- 0.211
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0070204455, 0.0005364835

running batch: 1000 / 1094 and superbatch(1); data from TransE, DBpedia50, run 2.1, exp 572
rank avg (pred): 0.417 +- 0.253
mrr vals (pred, true): 0.047, 0.032
batch losses (mrrl, rdl): 7.91404e-05, 0.0003174673

Epoch over!
epoch time: 11.991

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.351 +- 0.255
mrr vals (pred, true): 0.082, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   29 	     0 	 0.05687 	 0.00017 	 m..s
   60 	     1 	 0.07605 	 0.00018 	 m..s
   74 	     2 	 0.08288 	 0.00019 	 m..s
   28 	     3 	 0.05624 	 0.00020 	 m..s
   23 	     4 	 0.05390 	 0.00020 	 m..s
   16 	     5 	 0.05207 	 0.00020 	 m..s
   14 	     6 	 0.05151 	 0.00021 	 m..s
   98 	     7 	 0.12088 	 0.00021 	 MISS
    0 	     8 	 0.05142 	 0.00021 	 m..s
   38 	     9 	 0.06057 	 0.00021 	 m..s
   37 	    10 	 0.06047 	 0.00022 	 m..s
   75 	    11 	 0.08463 	 0.00022 	 m..s
    0 	    12 	 0.05142 	 0.00022 	 m..s
   32 	    13 	 0.05782 	 0.00022 	 m..s
   45 	    14 	 0.06816 	 0.00024 	 m..s
    0 	    15 	 0.05142 	 0.00024 	 m..s
    0 	    16 	 0.05142 	 0.00024 	 m..s
   21 	    17 	 0.05330 	 0.00024 	 m..s
   88 	    18 	 0.10743 	 0.00024 	 MISS
   34 	    19 	 0.05880 	 0.00025 	 m..s
   87 	    20 	 0.10713 	 0.00025 	 MISS
   84 	    21 	 0.10390 	 0.00026 	 MISS
   99 	    22 	 0.12256 	 0.00026 	 MISS
    0 	    23 	 0.05142 	 0.00026 	 m..s
   73 	    24 	 0.08159 	 0.00027 	 m..s
   67 	    25 	 0.08007 	 0.00027 	 m..s
   42 	    26 	 0.06162 	 0.00028 	 m..s
   90 	    27 	 0.10748 	 0.00028 	 MISS
   81 	    28 	 0.10030 	 0.00029 	 MISS
   89 	    29 	 0.10744 	 0.00029 	 MISS
   46 	    30 	 0.06859 	 0.00029 	 m..s
    0 	    31 	 0.05142 	 0.00029 	 m..s
   30 	    32 	 0.05697 	 0.00030 	 m..s
   50 	    33 	 0.07100 	 0.00030 	 m..s
   19 	    34 	 0.05321 	 0.00031 	 m..s
   24 	    35 	 0.05420 	 0.00032 	 m..s
   61 	    36 	 0.07619 	 0.00033 	 m..s
   18 	    37 	 0.05280 	 0.00033 	 m..s
   39 	    38 	 0.06117 	 0.00033 	 m..s
    0 	    39 	 0.05142 	 0.00036 	 m..s
  101 	    40 	 0.12436 	 0.00036 	 MISS
   35 	    41 	 0.05959 	 0.00036 	 m..s
   76 	    42 	 0.08482 	 0.00038 	 m..s
   15 	    43 	 0.05195 	 0.00040 	 m..s
   59 	    44 	 0.07595 	 0.00041 	 m..s
   48 	    45 	 0.07017 	 0.00042 	 m..s
    0 	    46 	 0.05142 	 0.00042 	 m..s
    0 	    47 	 0.05142 	 0.00053 	 m..s
   69 	    48 	 0.08057 	 0.00069 	 m..s
   93 	    49 	 0.11351 	 0.00074 	 MISS
   47 	    50 	 0.06990 	 0.00076 	 m..s
   44 	    51 	 0.06731 	 0.00082 	 m..s
   55 	    52 	 0.07496 	 0.00083 	 m..s
  100 	    53 	 0.12260 	 0.00083 	 MISS
    0 	    54 	 0.05142 	 0.01115 	 m..s
    0 	    55 	 0.05142 	 0.01610 	 m..s
   13 	    56 	 0.05147 	 0.01915 	 m..s
   43 	    57 	 0.06694 	 0.02802 	 m..s
    0 	    58 	 0.05142 	 0.03155 	 ~...
   31 	    59 	 0.05753 	 0.03184 	 ~...
   17 	    60 	 0.05232 	 0.03357 	 ~...
   36 	    61 	 0.06008 	 0.04193 	 ~...
   12 	    62 	 0.05142 	 0.04716 	 ~...
   41 	    63 	 0.06141 	 0.05024 	 ~...
   65 	    64 	 0.07739 	 0.05436 	 ~...
   20 	    65 	 0.05322 	 0.05908 	 ~...
   49 	    66 	 0.07018 	 0.06095 	 ~...
   26 	    67 	 0.05577 	 0.06196 	 ~...
   25 	    68 	 0.05460 	 0.06330 	 ~...
   63 	    69 	 0.07712 	 0.06343 	 ~...
   62 	    70 	 0.07707 	 0.06684 	 ~...
   52 	    71 	 0.07336 	 0.06864 	 ~...
   53 	    72 	 0.07349 	 0.07030 	 ~...
   22 	    73 	 0.05362 	 0.07458 	 ~...
   54 	    74 	 0.07481 	 0.07688 	 ~...
   51 	    75 	 0.07317 	 0.07718 	 ~...
   33 	    76 	 0.05809 	 0.07838 	 ~...
   40 	    77 	 0.06138 	 0.08177 	 ~...
   97 	    78 	 0.11778 	 0.08234 	 m..s
   27 	    79 	 0.05589 	 0.08249 	 ~...
  103 	    80 	 0.12661 	 0.08483 	 m..s
   64 	    81 	 0.07735 	 0.08777 	 ~...
   91 	    82 	 0.11285 	 0.09149 	 ~...
   78 	    83 	 0.09454 	 0.09525 	 ~...
   66 	    84 	 0.07897 	 0.09562 	 ~...
   94 	    85 	 0.11446 	 0.09745 	 ~...
  106 	    86 	 0.13752 	 0.09849 	 m..s
  104 	    87 	 0.12769 	 0.10094 	 ~...
  102 	    88 	 0.12654 	 0.10154 	 ~...
   86 	    89 	 0.10553 	 0.10377 	 ~...
   58 	    90 	 0.07580 	 0.10701 	 m..s
   68 	    91 	 0.08036 	 0.10944 	 ~...
   57 	    92 	 0.07558 	 0.11232 	 m..s
   77 	    93 	 0.09197 	 0.12731 	 m..s
   82 	    94 	 0.10169 	 0.12964 	 ~...
   56 	    95 	 0.07540 	 0.12973 	 m..s
   79 	    96 	 0.09799 	 0.13543 	 m..s
  107 	    97 	 0.18545 	 0.14189 	 m..s
   80 	    98 	 0.09876 	 0.15134 	 m..s
   95 	    99 	 0.11557 	 0.15744 	 m..s
  105 	   100 	 0.13016 	 0.15868 	 ~...
   72 	   101 	 0.08154 	 0.16211 	 m..s
   70 	   102 	 0.08096 	 0.16298 	 m..s
   71 	   103 	 0.08141 	 0.16519 	 m..s
   83 	   104 	 0.10203 	 0.18320 	 m..s
   92 	   105 	 0.11292 	 0.18628 	 m..s
  110 	   106 	 0.23650 	 0.18766 	 m..s
   85 	   107 	 0.10426 	 0.19066 	 m..s
  108 	   108 	 0.19825 	 0.19119 	 ~...
   96 	   109 	 0.11770 	 0.20735 	 m..s
  111 	   110 	 0.23924 	 0.21882 	 ~...
  112 	   111 	 0.24724 	 0.22501 	 ~...
  109 	   112 	 0.21979 	 0.22896 	 ~...
  116 	   113 	 0.26478 	 0.26518 	 ~...
  115 	   114 	 0.26448 	 0.27084 	 ~...
  114 	   115 	 0.25290 	 0.27822 	 ~...
  118 	   116 	 0.29656 	 0.28660 	 ~...
  117 	   117 	 0.28341 	 0.29333 	 ~...
  113 	   118 	 0.25038 	 0.29917 	 m..s
  120 	   119 	 0.30709 	 0.30066 	 ~...
  119 	   120 	 0.30480 	 0.32098 	 ~...
==========================================
r_mrr = 0.8209225535392761
r2_mrr = 0.5592886209487915
spearmanr_mrr@5 = 0.8240881562232971
spearmanr_mrr@10 = 0.8973782658576965
spearmanr_mrr@50 = 0.9615654349327087
spearmanr_mrr@100 = 0.9469217658042908
spearmanr_mrr@All = 0.9532114267349243
==========================================
test time: 0.384
Done Testing dataset DBpedia50
total time taken: 185.44954347610474
training time taken: 180.7411983013153
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'DBpedia50': tensor(0.8209)}}, 'r2_mrr': {'TransE': {'DBpedia50': tensor(0.5593)}}, 'spearmanr_mrr@5': {'TransE': {'DBpedia50': tensor(0.8241)}}, 'spearmanr_mrr@10': {'TransE': {'DBpedia50': tensor(0.8974)}}, 'spearmanr_mrr@50': {'TransE': {'DBpedia50': tensor(0.9616)}}, 'spearmanr_mrr@100': {'TransE': {'DBpedia50': tensor(0.9469)}}, 'spearmanr_mrr@All': {'TransE': {'DBpedia50': tensor(0.9532)}}, 'test_loss': {'TransE': {'DBpedia50': 1.696169694820128}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 6439349733452592
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1115, 72, 489, 123, 780, 1133, 210, 1157, 831, 343, 192, 488, 872, 898, 274, 233, 736, 185, 29, 1177, 27, 885, 527, 33, 1087, 77, 868, 773, 995, 615, 850, 795, 818, 589, 522, 292, 925, 289, 573, 679, 973, 716, 713, 953, 971, 86, 164, 804, 314, 769, 1207, 17, 845, 1070, 1111, 191, 892, 665, 869, 179, 173, 1125, 705, 586, 680, 59, 268, 658, 1052, 864, 266, 1180, 767, 372, 799, 766, 1167, 67, 440, 649, 512, 463, 98, 295, 690, 190, 419, 281, 371, 442, 1056, 778, 65, 1124, 836, 1026, 265, 968, 507, 1140, 770, 249, 1050, 927, 46, 443, 1109, 1119, 808, 193, 553, 745, 1162, 975, 961, 917, 146, 1104, 1079, 722, 206]
valid_ids (0): []
train_ids (1094): [879, 866, 979, 653, 822, 636, 223, 1176, 1078, 834, 893, 151, 1200, 843, 92, 1066, 1130, 163, 1048, 558, 307, 1151, 39, 308, 108, 1016, 613, 83, 207, 538, 226, 221, 525, 1040, 88, 511, 1170, 135, 635, 1196, 122, 637, 389, 8, 776, 994, 639, 1146, 838, 115, 858, 403, 102, 552, 1181, 227, 729, 31, 1193, 378, 675, 867, 760, 1108, 177, 883, 1059, 454, 940, 165, 172, 283, 280, 313, 741, 820, 812, 789, 580, 242, 742, 677, 293, 980, 697, 394, 1159, 462, 490, 473, 505, 168, 340, 1047, 60, 1010, 365, 807, 588, 514, 286, 457, 1043, 315, 335, 258, 1017, 771, 988, 810, 777, 93, 417, 833, 536, 646, 1148, 398, 918, 1019, 100, 156, 548, 1014, 330, 161, 4, 1120, 861, 218, 182, 708, 853, 881, 1074, 285, 90, 1145, 1093, 414, 13, 328, 563, 184, 12, 565, 1055, 683, 784, 544, 621, 1039, 513, 517, 1057, 821, 309, 425, 167, 628, 663, 461, 215, 384, 523, 617, 64, 119, 147, 499, 668, 129, 400, 768, 32, 520, 718, 269, 578, 865, 254, 609, 974, 938, 397, 1175, 178, 405, 891, 1063, 797, 592, 754, 300, 1173, 312, 1185, 727, 350, 737, 992, 468, 390, 802, 319, 700, 627, 890, 556, 759, 691, 847, 45, 500, 744, 188, 684, 855, 431, 696, 753, 469, 775, 707, 1204, 44, 346, 51, 68, 368, 35, 271, 751, 96, 337, 862, 570, 367, 75, 608, 48, 395, 1008, 1073, 1123, 555, 1187, 385, 595, 248, 125, 1003, 447, 446, 287, 201, 998, 1006, 1144, 396, 124, 23, 967, 870, 426, 430, 428, 1156, 1032, 964, 983, 260, 1113, 393, 509, 943, 1077, 476, 839, 1172, 878, 453, 689, 720, 515, 25, 519, 765, 1142, 912, 1129, 761, 1199, 342, 949, 894, 101, 84, 450, 857, 937, 120, 416, 990, 671, 19, 590, 99, 966, 341, 415, 603, 977, 97, 171, 1081, 582, 9, 798, 692, 758, 533, 535, 436, 612, 852, 1054, 740, 1049, 1161, 1117, 638, 817, 209, 919, 1021, 541, 321, 434, 301, 537, 1209, 47, 1097, 1110, 1206, 939, 1174, 481, 131, 657, 724, 1096, 130, 1099, 1062, 1205, 1023, 1041, 726, 10, 1138, 53, 251, 1082, 1031, 574, 711, 986, 728, 731, 1083, 94, 222, 911, 1154, 375, 437, 352, 934, 1136, 28, 38, 152, 1195, 267, 903, 126, 503, 554, 928, 116, 264, 1036, 682, 965, 924, 856, 1095, 316, 475, 877, 985, 485, 549, 1131, 219, 651, 144, 634, 1210, 380, 166, 464, 1089, 725, 85, 569, 545, 596, 204, 568, 246, 1211, 876, 946, 757, 670, 1197, 1007, 357, 229, 234, 236, 501, 1164, 829, 104, 160, 1212, 214, 55, 333, 497, 551, 685, 791, 42, 957, 550, 14, 150, 63, 626, 752, 873, 1072, 654, 200, 618, 250, 591, 305, 1208, 747, 909, 1076, 87, 408, 1135, 564, 175, 622, 26, 978, 370, 95, 299, 231, 1139, 460, 1002, 56, 467, 411, 155, 240, 216, 243, 521, 133, 401, 73, 910, 1137, 921, 194, 253, 1046, 619, 926, 1033, 471, 423, 1000, 908, 805, 606, 455, 277, 382, 715, 801, 955, 960, 1068, 297, 859, 273, 294, 279, 374, 976, 623, 958, 792, 991, 734, 762, 34, 1100, 851, 772, 424, 811, 730, 76, 224, 458, 733, 688, 41, 298, 860, 334, 763, 470, 593, 914, 1203, 930, 114, 449, 399, 54, 79, 602, 571, 954, 902, 1, 291, 387, 945, 208, 647, 486, 1012, 504, 972, 1042, 1128, 1061, 52, 969, 656, 183, 493, 1116, 275, 498, 91, 388, 435, 413, 931, 539, 756, 629, 176, 1134, 153, 899, 364, 502, 577, 1213, 1192, 16, 529, 1155, 71, 296, 228, 37, 452, 981, 840, 562, 534, 532, 783, 186, 997, 290, 239, 1127, 559, 377, 721, 211, 941, 630, 1152, 659, 212, 257, 78, 989, 579, 664, 673, 1035, 687, 597, 996, 655, 474, 906, 247, 1011, 373, 1044, 1022, 1067, 1085, 681, 276, 49, 433, 703, 1029, 785, 794, 494, 441, 484, 323, 238, 1122, 1037, 339, 36, 530, 1092, 148, 451, 895, 360, 338, 897, 137, 1098, 510, 920, 439, 58, 318, 1064, 1182, 326, 616, 796, 848, 196, 356, 157, 459, 782, 409, 947, 444, 331, 379, 1020, 336, 1171, 984, 7, 3, 572, 524, 391, 50, 1163, 427, 706, 1198, 1141, 508, 607, 324, 107, 40, 650, 111, 429, 110, 203, 118, 672, 1025, 561, 678, 1015, 518, 472, 1169, 674, 963, 932, 560, 667, 587, 1086, 738, 935, 813, 788, 710, 698, 406, 149, 982, 793, 158, 660, 900, 1143, 66, 841, 863, 1105, 648, 835, 959, 492, 1045, 383, 542, 465, 610, 907, 232, 1132, 2, 213, 302, 882, 5, 594, 1202, 392, 112, 1034, 1114, 642, 901, 478, 174, 824, 1038, 1091, 252, 528, 1080, 1118, 815, 404, 482, 1009, 875, 169, 113, 109, 830, 82, 358, 922, 220, 506, 89, 288, 779, 106, 1201, 230, 1051, 1101, 1189, 632, 6, 633, 842, 162, 584, 774, 699, 1001, 1065, 583, 432, 410, 329, 11, 1106, 557, 128, 351, 546, 198, 412, 320, 601, 1153, 543, 694, 256, 823, 1053, 225, 1013, 828, 666, 1027, 614, 884, 418, 781, 235, 270, 1088, 480, 620, 790, 244, 456, 819, 755, 1178, 888, 844, 145, 359, 970, 871, 154, 1160, 719, 1103, 739, 948, 189, 746, 701, 1149, 723, 605, 714, 915, 325, 1112, 787, 929, 896, 887, 880, 272, 1158, 956, 1121, 886, 121, 662, 1194, 345, 1071, 304, 750, 278, 846, 139, 103, 62, 1166, 942, 962, 516, 263, 717, 702, 904, 611, 322, 575, 923, 826, 205, 202, 138, 1190, 748, 1102, 479, 849, 134, 1094, 21, 669, 1126, 241, 245, 354, 827, 1018, 1075, 282, 526, 366, 61, 709, 803, 951, 420, 363, 355, 1150, 933, 24, 117, 136, 70, 624, 143, 353, 448, 1165, 604, 255, 105, 676, 376, 284, 1084, 381, 1004, 438, 361, 576, 347, 644, 764, 732, 491, 43, 362, 567, 402, 181, 306, 643, 640, 141, 950, 531, 466, 1024, 695, 1030, 303, 600, 421, 261, 913, 652, 1214, 693, 814, 69, 585, 905, 22, 20, 547, 317, 311, 809, 348, 1179, 1147, 132, 993, 187, 445, 661, 496, 987, 310, 686, 631, 344, 735, 1028, 1191, 332, 1107, 566, 18, 936, 74, 180, 159, 1060, 645, 407, 477, 712, 217, 1058, 199, 1069, 80, 916, 349, 800, 57, 140, 327, 1186, 262, 816, 15, 1188, 142, 837, 0, 743, 999, 386, 749, 422, 641, 195, 599, 81, 581, 540, 944, 889, 369, 487, 786, 625, 483, 704, 197, 825, 30, 952, 259, 1005, 1184, 127, 874, 1168, 170, 1090, 854, 237, 1183, 598, 832, 806, 495]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4583656875995092
the save name prefix for this run is:  chkpt-ID_4583656875995092_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1023
rank avg (pred): 0.475 +- 0.005
mrr vals (pred, true): 0.020, 0.052
batch losses (mrrl, rdl): 0.0, 0.000107677

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1064
rank avg (pred): 0.134 +- 0.096
mrr vals (pred, true): 0.191, 0.321
batch losses (mrrl, rdl): 0.0, 3.12169e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 410
rank avg (pred): 0.408 +- 0.258
mrr vals (pred, true): 0.079, 0.045
batch losses (mrrl, rdl): 0.0, 2.95823e-05

Epoch over!
epoch time: 12.105

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 564
rank avg (pred): 0.126 +- 0.120
mrr vals (pred, true): 0.197, 0.220
batch losses (mrrl, rdl): 0.0, 5.6931e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 121
rank avg (pred): 0.451 +- 0.263
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0, 8.0702e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 119
rank avg (pred): 0.440 +- 0.267
mrr vals (pred, true): 0.061, 0.054
batch losses (mrrl, rdl): 0.0, 9.2176e-06

Epoch over!
epoch time: 11.849

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 232
rank avg (pred): 0.450 +- 0.263
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0, 5.1794e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 270
rank avg (pred): 0.135 +- 0.172
mrr vals (pred, true): 0.194, 0.290
batch losses (mrrl, rdl): 0.0, 2.82371e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1058
rank avg (pred): 0.132 +- 0.160
mrr vals (pred, true): 0.183, 0.328
batch losses (mrrl, rdl): 0.0, 2.31467e-05

Epoch over!
epoch time: 11.848

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 794
rank avg (pred): 0.421 +- 0.273
mrr vals (pred, true): 0.073, 0.044
batch losses (mrrl, rdl): 0.0, 2.10337e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 391
rank avg (pred): 0.454 +- 0.259
mrr vals (pred, true): 0.048, 0.059
batch losses (mrrl, rdl): 0.0, 5.35235e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 731
rank avg (pred): 0.129 +- 0.160
mrr vals (pred, true): 0.177, 0.176
batch losses (mrrl, rdl): 0.0, 0.0002713541

Epoch over!
epoch time: 11.92

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 884
rank avg (pred): 0.450 +- 0.263
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 0.0, 9.405e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 535
rank avg (pred): 0.124 +- 0.155
mrr vals (pred, true): 0.170, 0.199
batch losses (mrrl, rdl): 0.0, 1.2411e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 932
rank avg (pred): 0.467 +- 0.263
mrr vals (pred, true): 0.045, 0.046
batch losses (mrrl, rdl): 0.0, 3.1052e-06

Epoch over!
epoch time: 11.866

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 807
rank avg (pred): 0.462 +- 0.265
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.0002644878, 3.124e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 362
rank avg (pred): 0.456 +- 0.206
mrr vals (pred, true): 0.051, 0.056
batch losses (mrrl, rdl): 1.56079e-05, 5.09155e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 685
rank avg (pred): 0.464 +- 0.188
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 1.2371e-05, 2.24965e-05

Epoch over!
epoch time: 12.132

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 536
rank avg (pred): 0.261 +- 0.216
mrr vals (pred, true): 0.212, 0.231
batch losses (mrrl, rdl): 0.0035183793, 0.0004286449

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 839
rank avg (pred): 0.473 +- 0.169
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0004585212, 3.06865e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1082
rank avg (pred): 0.442 +- 0.142
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 1.0102e-06, 5.11676e-05

Epoch over!
epoch time: 12.199

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 569
rank avg (pred): 0.409 +- 0.134
mrr vals (pred, true): 0.060, 0.044
batch losses (mrrl, rdl): 0.0009682591, 7.93571e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 918
rank avg (pred): 0.443 +- 0.123
mrr vals (pred, true): 0.043, 0.042
batch losses (mrrl, rdl): 0.0004635959, 5.72165e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1152
rank avg (pred): 0.275 +- 0.201
mrr vals (pred, true): 0.239, 0.243
batch losses (mrrl, rdl): 0.0001879779, 0.0006474535

Epoch over!
epoch time: 12.012

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 466
rank avg (pred): 0.424 +- 0.137
mrr vals (pred, true): 0.061, 0.044
batch losses (mrrl, rdl): 0.0012656914, 0.0001116186

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 933
rank avg (pred): 0.431 +- 0.128
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 4.56792e-05, 6.8071e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 359
rank avg (pred): 0.442 +- 0.112
mrr vals (pred, true): 0.045, 0.056
batch losses (mrrl, rdl): 0.0002726594, 7.04654e-05

Epoch over!
epoch time: 12.189

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 267
rank avg (pred): 0.201 +- 0.163
mrr vals (pred, true): 0.270, 0.303
batch losses (mrrl, rdl): 0.0107040918, 0.0003157273

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 299
rank avg (pred): 0.189 +- 0.160
mrr vals (pred, true): 0.282, 0.291
batch losses (mrrl, rdl): 0.0007447035, 0.000162454

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 829
rank avg (pred): 0.222 +- 0.170
mrr vals (pred, true): 0.267, 0.197
batch losses (mrrl, rdl): 0.0482480004, 1.59738e-05

Epoch over!
epoch time: 12.011

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 505
rank avg (pred): 0.266 +- 0.191
mrr vals (pred, true): 0.236, 0.204
batch losses (mrrl, rdl): 0.0099153174, 0.0005681489

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 117
rank avg (pred): 0.439 +- 0.107
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 0.0001069122, 5.95552e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 487
rank avg (pred): 0.284 +- 0.186
mrr vals (pred, true): 0.219, 0.204
batch losses (mrrl, rdl): 0.002200003, 0.0006420496

Epoch over!
epoch time: 12.19

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 755
rank avg (pred): 0.190 +- 0.155
mrr vals (pred, true): 0.290, 0.277
batch losses (mrrl, rdl): 0.0015630932, 7.71992e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 396
rank avg (pred): 0.453 +- 0.079
mrr vals (pred, true): 0.037, 0.050
batch losses (mrrl, rdl): 0.0018025654, 7.3831e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 858
rank avg (pred): 0.435 +- 0.103
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 6.31011e-05, 7.66079e-05

Epoch over!
epoch time: 12.045

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 874
rank avg (pred): 0.434 +- 0.104
mrr vals (pred, true): 0.048, 0.049
batch losses (mrrl, rdl): 3.09272e-05, 7.15012e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 34
rank avg (pred): 0.176 +- 0.138
mrr vals (pred, true): 0.305, 0.272
batch losses (mrrl, rdl): 0.0110390456, 0.0001502364

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 744
rank avg (pred): 0.162 +- 0.135
mrr vals (pred, true): 0.321, 0.255
batch losses (mrrl, rdl): 0.0427411683, 1.62636e-05

Epoch over!
epoch time: 12.06

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 254
rank avg (pred): 0.206 +- 0.151
mrr vals (pred, true): 0.286, 0.290
batch losses (mrrl, rdl): 0.0001430153, 0.0002248323

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 432
rank avg (pred): 0.437 +- 0.090
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0002919443, 9.93361e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 709
rank avg (pred): 0.433 +- 0.091
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001535951, 9.01227e-05

Epoch over!
epoch time: 12.045

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 781
rank avg (pred): 0.431 +- 0.099
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 6.38516e-05, 8.09466e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 856
rank avg (pred): 0.429 +- 0.097
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 2.44977e-05, 0.0001004909

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 99
rank avg (pred): 0.432 +- 0.096
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 7.57551e-05, 6.48566e-05

Epoch over!
epoch time: 11.954

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.445 +- 0.074
mrr vals (pred, true): 0.038, 0.045

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   10 	     0 	 0.04068 	 0.04001 	 ~...
   51 	     1 	 0.04370 	 0.04160 	 ~...
   15 	     2 	 0.04156 	 0.04162 	 ~...
   19 	     3 	 0.04195 	 0.04205 	 ~...
    6 	     4 	 0.03908 	 0.04209 	 ~...
   17 	     5 	 0.04161 	 0.04238 	 ~...
   45 	     6 	 0.04348 	 0.04244 	 ~...
   30 	     7 	 0.04264 	 0.04245 	 ~...
   34 	     8 	 0.04281 	 0.04259 	 ~...
   54 	     9 	 0.04388 	 0.04263 	 ~...
   23 	    10 	 0.04218 	 0.04264 	 ~...
   66 	    11 	 0.04445 	 0.04269 	 ~...
   57 	    12 	 0.04389 	 0.04274 	 ~...
   72 	    13 	 0.04476 	 0.04275 	 ~...
   33 	    14 	 0.04275 	 0.04276 	 ~...
   48 	    15 	 0.04351 	 0.04288 	 ~...
    3 	    16 	 0.03898 	 0.04299 	 ~...
   26 	    17 	 0.04232 	 0.04315 	 ~...
   12 	    18 	 0.04110 	 0.04331 	 ~...
   20 	    19 	 0.04195 	 0.04344 	 ~...
    0 	    20 	 0.03814 	 0.04354 	 ~...
   74 	    21 	 0.04497 	 0.04355 	 ~...
   38 	    22 	 0.04298 	 0.04366 	 ~...
    2 	    23 	 0.03897 	 0.04369 	 ~...
   13 	    24 	 0.04111 	 0.04370 	 ~...
   76 	    25 	 0.04541 	 0.04377 	 ~...
   70 	    26 	 0.04472 	 0.04388 	 ~...
   49 	    27 	 0.04354 	 0.04401 	 ~...
   42 	    28 	 0.04322 	 0.04402 	 ~...
   65 	    29 	 0.04436 	 0.04412 	 ~...
   67 	    30 	 0.04455 	 0.04412 	 ~...
   11 	    31 	 0.04091 	 0.04414 	 ~...
   55 	    32 	 0.04388 	 0.04414 	 ~...
   16 	    33 	 0.04157 	 0.04418 	 ~...
   56 	    34 	 0.04389 	 0.04428 	 ~...
   32 	    35 	 0.04271 	 0.04437 	 ~...
   37 	    36 	 0.04296 	 0.04450 	 ~...
    7 	    37 	 0.04049 	 0.04460 	 ~...
   71 	    38 	 0.04474 	 0.04462 	 ~...
   61 	    39 	 0.04393 	 0.04463 	 ~...
   52 	    40 	 0.04374 	 0.04472 	 ~...
   69 	    41 	 0.04464 	 0.04473 	 ~...
   36 	    42 	 0.04285 	 0.04473 	 ~...
   46 	    43 	 0.04349 	 0.04476 	 ~...
    9 	    44 	 0.04049 	 0.04480 	 ~...
   47 	    45 	 0.04350 	 0.04484 	 ~...
   29 	    46 	 0.04260 	 0.04487 	 ~...
   50 	    47 	 0.04367 	 0.04490 	 ~...
    4 	    48 	 0.03898 	 0.04517 	 ~...
   58 	    49 	 0.04391 	 0.04536 	 ~...
    0 	    50 	 0.03814 	 0.04538 	 ~...
   28 	    51 	 0.04258 	 0.04561 	 ~...
   31 	    52 	 0.04265 	 0.04588 	 ~...
   59 	    53 	 0.04391 	 0.04597 	 ~...
   21 	    54 	 0.04196 	 0.04602 	 ~...
   44 	    55 	 0.04348 	 0.04607 	 ~...
   60 	    56 	 0.04393 	 0.04611 	 ~...
    5 	    57 	 0.03905 	 0.04628 	 ~...
   18 	    58 	 0.04189 	 0.04635 	 ~...
   63 	    59 	 0.04424 	 0.04659 	 ~...
   62 	    60 	 0.04422 	 0.04666 	 ~...
   64 	    61 	 0.04426 	 0.04676 	 ~...
   35 	    62 	 0.04282 	 0.04688 	 ~...
   22 	    63 	 0.04201 	 0.04694 	 ~...
   43 	    64 	 0.04345 	 0.04724 	 ~...
    7 	    65 	 0.04049 	 0.04726 	 ~...
   53 	    66 	 0.04384 	 0.04738 	 ~...
   75 	    67 	 0.04514 	 0.04798 	 ~...
   25 	    68 	 0.04223 	 0.04871 	 ~...
   24 	    69 	 0.04219 	 0.04890 	 ~...
   27 	    70 	 0.04233 	 0.04914 	 ~...
   68 	    71 	 0.04458 	 0.04945 	 ~...
   73 	    72 	 0.04495 	 0.05189 	 ~...
   14 	    73 	 0.04146 	 0.05220 	 ~...
   41 	    74 	 0.04318 	 0.05220 	 ~...
   39 	    75 	 0.04313 	 0.05416 	 ~...
   77 	    76 	 0.04558 	 0.05539 	 ~...
   40 	    77 	 0.04315 	 0.05818 	 ~...
   81 	    78 	 0.20873 	 0.10283 	 MISS
   93 	    79 	 0.26020 	 0.10667 	 MISS
  100 	    80 	 0.26899 	 0.11025 	 MISS
   78 	    81 	 0.20786 	 0.13306 	 m..s
   86 	    82 	 0.21174 	 0.21008 	 ~...
   88 	    83 	 0.21472 	 0.21053 	 ~...
   78 	    84 	 0.20786 	 0.21095 	 ~...
   83 	    85 	 0.20941 	 0.21573 	 ~...
   87 	    86 	 0.21200 	 0.22564 	 ~...
   85 	    87 	 0.21145 	 0.22697 	 ~...
   89 	    88 	 0.22256 	 0.23013 	 ~...
  102 	    89 	 0.26940 	 0.23489 	 m..s
   82 	    90 	 0.20918 	 0.23641 	 ~...
   84 	    91 	 0.21010 	 0.25812 	 m..s
  111 	    92 	 0.27311 	 0.26203 	 ~...
  120 	    93 	 0.28525 	 0.26225 	 ~...
   99 	    94 	 0.26706 	 0.26274 	 ~...
  114 	    95 	 0.27671 	 0.26409 	 ~...
  109 	    96 	 0.27285 	 0.26557 	 ~...
  112 	    97 	 0.27621 	 0.27130 	 ~...
   94 	    98 	 0.26098 	 0.27549 	 ~...
  115 	    99 	 0.27680 	 0.27906 	 ~...
  108 	   100 	 0.27249 	 0.28054 	 ~...
  113 	   101 	 0.27630 	 0.28089 	 ~...
  101 	   102 	 0.26924 	 0.28167 	 ~...
  119 	   103 	 0.28297 	 0.28279 	 ~...
   96 	   104 	 0.26449 	 0.28323 	 ~...
  107 	   105 	 0.27247 	 0.28497 	 ~...
  117 	   106 	 0.27921 	 0.28575 	 ~...
  105 	   107 	 0.27159 	 0.28647 	 ~...
  106 	   108 	 0.27210 	 0.28689 	 ~...
  104 	   109 	 0.27091 	 0.28704 	 ~...
   80 	   110 	 0.20859 	 0.28917 	 m..s
  103 	   111 	 0.27045 	 0.29011 	 ~...
   95 	   112 	 0.26357 	 0.29219 	 ~...
   97 	   113 	 0.26556 	 0.29328 	 ~...
  110 	   114 	 0.27296 	 0.30314 	 m..s
   90 	   115 	 0.23864 	 0.30675 	 m..s
   92 	   116 	 0.24679 	 0.33674 	 m..s
  116 	   117 	 0.27908 	 0.34190 	 m..s
   91 	   118 	 0.24184 	 0.34329 	 MISS
   98 	   119 	 0.26621 	 0.34777 	 m..s
  117 	   120 	 0.27921 	 0.35056 	 m..s
==========================================
r_mrr = 0.9572600722312927
r2_mrr = 0.9144228100776672
spearmanr_mrr@5 = 0.8954523205757141
spearmanr_mrr@10 = 0.8752923607826233
spearmanr_mrr@50 = 0.9266402125358582
spearmanr_mrr@100 = 0.9766896367073059
spearmanr_mrr@All = 0.979292631149292
==========================================
test time: 0.894
Done Testing dataset Kinships
total time taken: 188.51855969429016
training time taken: 181.40355610847473
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9573)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9144)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.8955)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.8753)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9266)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9767)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9793)}}, 'test_loss': {'TransE': {'Kinships': 1.274661569885211}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 6557380255459631
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1104, 175, 1011, 369, 1085, 137, 858, 802, 372, 915, 283, 653, 411, 214, 423, 1019, 231, 179, 966, 377, 625, 353, 338, 91, 1171, 192, 824, 74, 1072, 438, 367, 773, 946, 253, 1116, 1023, 701, 177, 68, 581, 104, 1109, 515, 394, 378, 380, 448, 19, 1183, 1103, 444, 401, 757, 522, 1043, 851, 328, 307, 1088, 379, 1140, 597, 229, 977, 775, 182, 1132, 1209, 166, 736, 784, 334, 938, 618, 1137, 1189, 586, 519, 698, 920, 95, 537, 774, 891, 1017, 545, 1196, 251, 965, 558, 3, 992, 210, 899, 1045, 959, 1068, 574, 462, 450, 1063, 791, 1033, 715, 278, 512, 943, 1035, 589, 937, 1139, 616, 156, 663, 551, 591, 92, 1151, 1012, 273, 439]
valid_ids (0): []
train_ids (1094): [163, 124, 1004, 576, 912, 559, 230, 391, 680, 244, 162, 381, 262, 44, 113, 553, 14, 948, 276, 1032, 612, 1027, 949, 489, 37, 1203, 960, 212, 514, 384, 473, 1200, 345, 234, 1054, 335, 905, 650, 108, 1041, 1212, 141, 189, 386, 483, 115, 390, 690, 629, 1076, 861, 485, 981, 282, 722, 1157, 704, 733, 1093, 1129, 20, 631, 643, 414, 604, 717, 542, 985, 955, 465, 794, 1053, 500, 1124, 803, 1042, 594, 787, 830, 582, 986, 25, 84, 795, 1156, 458, 1089, 297, 289, 410, 624, 630, 1111, 505, 461, 563, 118, 259, 355, 145, 647, 809, 490, 1090, 445, 1115, 164, 33, 874, 584, 106, 211, 1036, 777, 148, 1211, 729, 302, 1122, 357, 80, 467, 1049, 588, 223, 344, 838, 366, 460, 536, 286, 1168, 1021, 610, 573, 1006, 785, 205, 889, 840, 31, 711, 351, 1097, 1055, 1062, 2, 552, 879, 644, 257, 1001, 245, 1210, 796, 157, 1166, 863, 518, 430, 556, 83, 1110, 776, 914, 304, 429, 455, 929, 388, 408, 1119, 605, 991, 347, 779, 724, 590, 18, 281, 600, 1141, 934, 662, 857, 1113, 480, 602, 979, 1020, 1118, 0, 492, 562, 855, 731, 892, 620, 284, 924, 111, 1098, 557, 53, 484, 42, 199, 720, 51, 208, 767, 668, 8, 1087, 502, 261, 961, 425, 727, 1190, 907, 1079, 316, 1073, 633, 1213, 1134, 1080, 55, 1038, 815, 89, 361, 546, 491, 665, 1108, 744, 939, 782, 45, 255, 725, 543, 964, 702, 517, 716, 474, 1128, 318, 242, 5, 1074, 611, 159, 1167, 1069, 682, 693, 373, 607, 989, 511, 761, 1078, 56, 671, 585, 1114, 88, 703, 922, 726, 139, 1127, 947, 890, 99, 919, 403, 109, 61, 881, 407, 739, 471, 634, 613, 435, 326, 227, 859, 812, 468, 265, 856, 823, 399, 641, 238, 1188, 1135, 689, 925, 9, 415, 1092, 329, 478, 110, 1100, 495, 1164, 370, 486, 1174, 48, 1106, 1138, 126, 93, 930, 962, 13, 374, 990, 828, 412, 745, 1061, 799, 1152, 1182, 127, 549, 1126, 735, 7, 233, 292, 39, 246, 808, 184, 497, 475, 880, 849, 658, 1050, 382, 76, 201, 1075, 1037, 674, 237, 1084, 342, 138, 105, 526, 314, 888, 332, 1091, 453, 1178, 728, 548, 96, 902, 748, 747, 524, 967, 923, 875, 186, 942, 999, 493, 195, 375, 853, 151, 235, 645, 350, 870, 327, 1070, 165, 457, 279, 1191, 666, 1150, 30, 822, 617, 420, 877, 988, 299, 181, 107, 868, 975, 769, 886, 120, 596, 323, 94, 398, 885, 356, 308, 994, 160, 752, 1161, 734, 422, 71, 432, 1086, 538, 968, 476, 579, 1195, 1214, 406, 147, 116, 359, 1159, 798, 1013, 131, 606, 1169, 694, 664, 1179, 447, 1022, 1040, 1187, 405, 927, 393, 73, 1048, 368, 187, 354, 686, 206, 587, 781, 34, 216, 12, 1197, 341, 993, 431, 498, 27, 1123, 931, 362, 319, 599, 578, 1144, 434, 122, 443, 1056, 207, 240, 560, 213, 66, 534, 135, 1173, 452, 550, 1143, 687, 778, 275, 52, 81, 609, 1102, 309, 541, 760, 154, 149, 400, 528, 150, 1047, 692, 638, 532, 1206, 180, 958, 385, 753, 125, 63, 659, 842, 451, 882, 409, 417, 397, 887, 928, 893, 78, 732, 813, 950, 193, 232, 248, 811, 829, 315, 272, 869, 535, 1064, 392, 648, 952, 32, 1148, 903, 269, 908, 196, 984, 860, 1044, 1125, 864, 876, 667, 1099, 699, 320, 144, 190, 530, 1184, 661, 636, 695, 331, 168, 459, 140, 496, 305, 254, 170, 158, 69, 564, 848, 615, 250, 1082, 1025, 1175, 917, 1158, 568, 654, 831, 1162, 577, 1077, 264, 688, 383, 635, 996, 987, 487, 933, 936, 758, 871, 1, 161, 507, 1170, 681, 622, 194, 969, 271, 129, 567, 222, 1101, 755, 215, 70, 974, 50, 973, 134, 926, 221, 21, 569, 86, 696, 679, 814, 249, 873, 270, 516, 293, 247, 971, 554, 153, 913, 572, 178, 800, 169, 1147, 835, 43, 646, 477, 837, 626, 595, 311, 1002, 691, 296, 102, 176, 146, 520, 174, 570, 510, 709, 15, 705, 531, 1029, 963, 1000, 456, 1007, 191, 239, 119, 418, 352, 866, 1051, 426, 396, 501, 1008, 112, 509, 839, 441, 846, 527, 810, 555, 436, 404, 364, 1066, 64, 660, 185, 1131, 421, 442, 143, 883, 277, 363, 825, 343, 580, 117, 220, 130, 749, 395, 940, 513, 67, 1052, 1146, 103, 852, 675, 1058, 97, 1094, 260, 437, 1018, 252, 1142, 719, 575, 1154, 766, 770, 40, 298, 1003, 896, 449, 789, 805, 291, 1181, 983, 656, 427, 317, 878, 826, 816, 114, 503, 1185, 945, 598, 708, 1105, 1057, 676, 621, 547, 533, 583, 58, 142, 1016, 1096, 706, 730, 371, 980, 539, 1199, 780, 1028, 172, 499, 1120, 818, 28, 1059, 916, 197, 746, 628, 463, 183, 121, 79, 783, 561, 867, 592, 523, 865, 209, 918, 741, 1065, 204, 1177, 330, 951, 224, 217, 623, 754, 642, 290, 1155, 16, 333, 100, 285, 801, 566, 845, 832, 41, 77, 413, 358, 226, 797, 684, 982, 921, 714, 1149, 957, 995, 529, 339, 944, 128, 901, 1193, 488, 472, 57, 4, 521, 132, 266, 768, 713, 267, 525, 672, 935, 1095, 1071, 60, 683, 806, 26, 72, 772, 348, 895, 737, 756, 685, 155, 482, 508, 36, 469, 322, 1034, 203, 652, 750, 24, 152, 454, 325, 218, 1009, 900, 365, 854, 788, 11, 571, 1202, 1205, 1160, 1201, 87, 911, 198, 506, 65, 268, 1026, 673, 771, 274, 759, 640, 906, 1031, 910, 336, 228, 740, 295, 1107, 1067, 324, 904, 303, 1112, 35, 544, 402, 608, 862, 678, 909, 820, 428, 136, 243, 817, 346, 833, 54, 90, 970, 998, 494, 280, 932, 1136, 738, 313, 1046, 721, 416, 765, 263, 804, 424, 1005, 1204, 236, 337, 1024, 651, 38, 202, 1208, 6, 49, 956, 23, 1039, 619, 98, 301, 1194, 123, 897, 997, 1121, 884, 565, 792, 360, 85, 173, 649, 1186, 1172, 310, 637, 763, 954, 742, 200, 287, 376, 827, 639, 1081, 101, 718, 807, 601, 850, 1133, 59, 241, 171, 340, 836, 790, 657, 321, 446, 743, 62, 712, 841, 1153, 847, 479, 751, 188, 312, 75, 978, 29, 46, 22, 258, 941, 133, 953, 1130, 1014, 1083, 1010, 710, 898, 697, 723, 300, 389, 1176, 793, 540, 976, 306, 1030, 972, 762, 669, 1207, 466, 764, 593, 1015, 225, 844, 677, 627, 821, 819, 294, 288, 167, 670, 834, 1165, 1180, 1163, 1198, 614, 481, 707, 464, 433, 256, 82, 387, 1145, 1192, 47, 419, 603, 894, 504, 219, 470, 10, 349, 843, 17, 1060, 440, 655, 786, 632, 700, 1117, 872]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9211968346014076
the save name prefix for this run is:  chkpt-ID_9211968346014076_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 995
rank avg (pred): 0.460 +- 0.003
mrr vals (pred, true): 0.021, 0.348
batch losses (mrrl, rdl): 0.0, 0.0027309291

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 675
rank avg (pred): 0.471 +- 0.014
mrr vals (pred, true): 0.020, 0.044
batch losses (mrrl, rdl): 0.0, 8.3488e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 145
rank avg (pred): 0.446 +- 0.256
mrr vals (pred, true): 0.045, 0.052
batch losses (mrrl, rdl): 0.0, 1.51912e-05

Epoch over!
epoch time: 11.959

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 716
rank avg (pred): 0.446 +- 0.265
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 0.0, 4.7733e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 220
rank avg (pred): 0.456 +- 0.267
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.0, 9.6545e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 656
rank avg (pred): 0.472 +- 0.263
mrr vals (pred, true): 0.037, 0.047
batch losses (mrrl, rdl): 0.0, 8.894e-07

Epoch over!
epoch time: 11.801

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 904
rank avg (pred): 0.197 +- 0.187
mrr vals (pred, true): 0.112, 0.198
batch losses (mrrl, rdl): 0.0, 1.23608e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1059
rank avg (pred): 0.129 +- 0.154
mrr vals (pred, true): 0.175, 0.341
batch losses (mrrl, rdl): 0.0, 2.70918e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 906
rank avg (pred): 0.183 +- 0.182
mrr vals (pred, true): 0.131, 0.215
batch losses (mrrl, rdl): 0.0, 2.98232e-05

Epoch over!
epoch time: 11.752

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 244
rank avg (pred): 0.131 +- 0.155
mrr vals (pred, true): 0.165, 0.260
batch losses (mrrl, rdl): 0.0, 2.08541e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1025
rank avg (pred): 0.478 +- 0.274
mrr vals (pred, true): 0.044, 0.055
batch losses (mrrl, rdl): 0.0, 2.66189e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 947
rank avg (pred): 0.473 +- 0.261
mrr vals (pred, true): 0.042, 0.045
batch losses (mrrl, rdl): 0.0, 5.5025e-06

Epoch over!
epoch time: 11.996

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 814
rank avg (pred): 0.176 +- 0.178
mrr vals (pred, true): 0.149, 0.222
batch losses (mrrl, rdl): 0.0, 3.8862e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 483
rank avg (pred): 0.466 +- 0.265
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0, 8.42e-08

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 563
rank avg (pred): 0.140 +- 0.160
mrr vals (pred, true): 0.169, 0.226
batch losses (mrrl, rdl): 0.0, 8.244e-07

Epoch over!
epoch time: 11.98

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 125
rank avg (pred): 0.451 +- 0.266
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 7.55278e-05, 2.3184e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 493
rank avg (pred): 0.110 +- 0.131
mrr vals (pred, true): 0.275, 0.209
batch losses (mrrl, rdl): 0.0437851325, 4.459e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 819
rank avg (pred): 0.196 +- 0.180
mrr vals (pred, true): 0.177, 0.182
batch losses (mrrl, rdl): 0.0002217614, 1.81079e-05

Epoch over!
epoch time: 12.273

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 745
rank avg (pred): 0.182 +- 0.183
mrr vals (pred, true): 0.213, 0.262
batch losses (mrrl, rdl): 0.0243416149, 5.51355e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 73
rank avg (pred): 0.173 +- 0.177
mrr vals (pred, true): 0.253, 0.272
batch losses (mrrl, rdl): 0.0036215468, 0.0001552684

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1115
rank avg (pred): 0.483 +- 0.246
mrr vals (pred, true): 0.038, 0.045
batch losses (mrrl, rdl): 0.0014311592, 4.1607e-06

Epoch over!
epoch time: 12.046

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1047
rank avg (pred): 0.451 +- 0.223
mrr vals (pred, true): 0.043, 0.042
batch losses (mrrl, rdl): 0.0005554452, 2.86646e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 662
rank avg (pred): 0.500 +- 0.248
mrr vals (pred, true): 0.039, 0.045
batch losses (mrrl, rdl): 0.0011240329, 1.42517e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 711
rank avg (pred): 0.474 +- 0.239
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 1.404e-07, 5.2763e-06

Epoch over!
epoch time: 12.057

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 897
rank avg (pred): 0.284 +- 0.220
mrr vals (pred, true): 0.175, 0.097
batch losses (mrrl, rdl): 0.155594334, 8.5937e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 125
rank avg (pred): 0.460 +- 0.237
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 7.96688e-05, 7.9609e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 810
rank avg (pred): 0.283 +- 0.211
mrr vals (pred, true): 0.216, 0.179
batch losses (mrrl, rdl): 0.0131875072, 4.95325e-05

Epoch over!
epoch time: 11.971

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 373
rank avg (pred): 0.477 +- 0.234
mrr vals (pred, true): 0.052, 0.053
batch losses (mrrl, rdl): 4.02129e-05, 3.81303e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 31
rank avg (pred): 0.214 +- 0.181
mrr vals (pred, true): 0.262, 0.269
batch losses (mrrl, rdl): 0.0004370045, 0.0003145147

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 705
rank avg (pred): 0.499 +- 0.252
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 0.00011947, 1.52938e-05

Epoch over!
epoch time: 11.978

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1128
rank avg (pred): 0.407 +- 0.185
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 3.42748e-05, 0.0001041924

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1206
rank avg (pred): 0.377 +- 0.140
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001354118, 0.0002514112

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 994
rank avg (pred): 0.194 +- 0.175
mrr vals (pred, true): 0.310, 0.357
batch losses (mrrl, rdl): 0.022422133, 0.0002564433

Epoch over!
epoch time: 12.209

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 902
rank avg (pred): 0.303 +- 0.233
mrr vals (pred, true): 0.232, 0.145
batch losses (mrrl, rdl): 0.0757125542, 4.87365e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 546
rank avg (pred): 0.228 +- 0.166
mrr vals (pred, true): 0.258, 0.212
batch losses (mrrl, rdl): 0.0208980311, 0.0002914587

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 509
rank avg (pred): 0.228 +- 0.168
mrr vals (pred, true): 0.254, 0.221
batch losses (mrrl, rdl): 0.0102811391, 0.0002701398

Epoch over!
epoch time: 12.028

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 877
rank avg (pred): 0.480 +- 0.239
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0001324456, 6.5051e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1120
rank avg (pred): 0.383 +- 0.151
mrr vals (pred, true): 0.048, 0.042
batch losses (mrrl, rdl): 5.48727e-05, 0.0002117535

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 86
rank avg (pred): 0.440 +- 0.223
mrr vals (pred, true): 0.056, 0.052
batch losses (mrrl, rdl): 0.0004062343, 8.1862e-06

Epoch over!
epoch time: 12.064

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1091
rank avg (pred): 0.543 +- 0.263
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003190897, 0.0001807831

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 299
rank avg (pred): 0.224 +- 0.157
mrr vals (pred, true): 0.273, 0.291
batch losses (mrrl, rdl): 0.0032056863, 0.0003209712

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 40
rank avg (pred): 0.244 +- 0.159
mrr vals (pred, true): 0.255, 0.278
batch losses (mrrl, rdl): 0.0053679892, 0.0004930082

Epoch over!
epoch time: 12.199

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 276
rank avg (pred): 0.252 +- 0.158
mrr vals (pred, true): 0.240, 0.308
batch losses (mrrl, rdl): 0.0462229997, 0.0005891601

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 34
rank avg (pred): 0.240 +- 0.160
mrr vals (pred, true): 0.264, 0.272
batch losses (mrrl, rdl): 0.0006156763, 0.000478217

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 215
rank avg (pred): 0.414 +- 0.222
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 4.5848e-05, 6.87286e-05

Epoch over!
epoch time: 11.956

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.328 +- 0.083
mrr vals (pred, true): 0.048, 0.054

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   63 	     0 	 0.04851 	 0.03862 	 ~...
    9 	     1 	 0.04447 	 0.04035 	 ~...
   86 	     2 	 0.05107 	 0.04092 	 ~...
   35 	     3 	 0.04711 	 0.04108 	 ~...
   59 	     4 	 0.04847 	 0.04191 	 ~...
   39 	     5 	 0.04728 	 0.04200 	 ~...
   72 	     6 	 0.04970 	 0.04204 	 ~...
   73 	     7 	 0.04970 	 0.04206 	 ~...
    6 	     8 	 0.04387 	 0.04215 	 ~...
    7 	     9 	 0.04397 	 0.04217 	 ~...
   61 	    10 	 0.04847 	 0.04225 	 ~...
   84 	    11 	 0.05019 	 0.04225 	 ~...
   42 	    12 	 0.04764 	 0.04244 	 ~...
    3 	    13 	 0.04346 	 0.04251 	 ~...
   31 	    14 	 0.04702 	 0.04278 	 ~...
    4 	    15 	 0.04372 	 0.04286 	 ~...
   16 	    16 	 0.04528 	 0.04313 	 ~...
   57 	    17 	 0.04841 	 0.04315 	 ~...
   25 	    18 	 0.04648 	 0.04335 	 ~...
   26 	    19 	 0.04676 	 0.04338 	 ~...
    1 	    20 	 0.04281 	 0.04345 	 ~...
   12 	    21 	 0.04499 	 0.04350 	 ~...
   35 	    22 	 0.04711 	 0.04363 	 ~...
   48 	    23 	 0.04814 	 0.04367 	 ~...
   64 	    24 	 0.04852 	 0.04371 	 ~...
   47 	    25 	 0.04793 	 0.04372 	 ~...
   11 	    26 	 0.04496 	 0.04379 	 ~...
   22 	    27 	 0.04636 	 0.04387 	 ~...
    0 	    28 	 0.04251 	 0.04392 	 ~...
   26 	    29 	 0.04676 	 0.04392 	 ~...
   37 	    30 	 0.04712 	 0.04402 	 ~...
   30 	    31 	 0.04694 	 0.04404 	 ~...
   28 	    32 	 0.04684 	 0.04409 	 ~...
   81 	    33 	 0.04981 	 0.04412 	 ~...
   66 	    34 	 0.04854 	 0.04435 	 ~...
   23 	    35 	 0.04642 	 0.04463 	 ~...
   15 	    36 	 0.04511 	 0.04491 	 ~...
   68 	    37 	 0.04877 	 0.04496 	 ~...
   34 	    38 	 0.04710 	 0.04514 	 ~...
   59 	    39 	 0.04847 	 0.04515 	 ~...
   18 	    40 	 0.04533 	 0.04542 	 ~...
   44 	    41 	 0.04780 	 0.04546 	 ~...
   71 	    42 	 0.04923 	 0.04549 	 ~...
   43 	    43 	 0.04764 	 0.04553 	 ~...
   67 	    44 	 0.04858 	 0.04557 	 ~...
   32 	    45 	 0.04703 	 0.04570 	 ~...
   33 	    46 	 0.04707 	 0.04571 	 ~...
   12 	    47 	 0.04499 	 0.04584 	 ~...
   24 	    48 	 0.04646 	 0.04585 	 ~...
   38 	    49 	 0.04721 	 0.04596 	 ~...
   14 	    50 	 0.04507 	 0.04612 	 ~...
   87 	    51 	 0.05123 	 0.04625 	 ~...
    8 	    52 	 0.04413 	 0.04628 	 ~...
   54 	    53 	 0.04831 	 0.04629 	 ~...
   19 	    54 	 0.04534 	 0.04677 	 ~...
   41 	    55 	 0.04752 	 0.04694 	 ~...
   78 	    56 	 0.04978 	 0.04738 	 ~...
    1 	    57 	 0.04281 	 0.04785 	 ~...
   52 	    58 	 0.04830 	 0.04815 	 ~...
   65 	    59 	 0.04854 	 0.04829 	 ~...
   75 	    60 	 0.04975 	 0.04883 	 ~...
   79 	    61 	 0.04979 	 0.04890 	 ~...
   69 	    62 	 0.04901 	 0.04914 	 ~...
   70 	    63 	 0.04906 	 0.04915 	 ~...
   21 	    64 	 0.04623 	 0.04922 	 ~...
   53 	    65 	 0.04831 	 0.04949 	 ~...
   82 	    66 	 0.04983 	 0.04970 	 ~...
   49 	    67 	 0.04818 	 0.05011 	 ~...
    5 	    68 	 0.04386 	 0.05029 	 ~...
   56 	    69 	 0.04839 	 0.05039 	 ~...
   58 	    70 	 0.04843 	 0.05126 	 ~...
   62 	    71 	 0.04848 	 0.05199 	 ~...
   45 	    72 	 0.04785 	 0.05223 	 ~...
   80 	    73 	 0.04980 	 0.05232 	 ~...
   49 	    74 	 0.04818 	 0.05291 	 ~...
   29 	    75 	 0.04690 	 0.05321 	 ~...
   82 	    76 	 0.04983 	 0.05331 	 ~...
   74 	    77 	 0.04974 	 0.05359 	 ~...
   51 	    78 	 0.04818 	 0.05389 	 ~...
   20 	    79 	 0.04558 	 0.05401 	 ~...
   45 	    80 	 0.04785 	 0.05416 	 ~...
   17 	    81 	 0.04531 	 0.05424 	 ~...
   75 	    82 	 0.04975 	 0.05435 	 ~...
   75 	    83 	 0.04975 	 0.05522 	 ~...
   55 	    84 	 0.04837 	 0.05607 	 ~...
   88 	    85 	 0.05257 	 0.05696 	 ~...
   40 	    86 	 0.04746 	 0.05740 	 ~...
   85 	    87 	 0.05032 	 0.05950 	 ~...
   10 	    88 	 0.04473 	 0.05973 	 ~...
   90 	    89 	 0.18318 	 0.10667 	 m..s
   91 	    90 	 0.19169 	 0.10993 	 m..s
   89 	    91 	 0.18180 	 0.14657 	 m..s
   97 	    92 	 0.24166 	 0.21008 	 m..s
  103 	    93 	 0.24606 	 0.21721 	 ~...
  101 	    94 	 0.24444 	 0.22106 	 ~...
  107 	    95 	 0.24969 	 0.22159 	 ~...
   93 	    96 	 0.20750 	 0.22636 	 ~...
  106 	    97 	 0.24914 	 0.22898 	 ~...
  110 	    98 	 0.25407 	 0.23641 	 ~...
   92 	    99 	 0.20509 	 0.23675 	 m..s
  111 	   100 	 0.25826 	 0.24077 	 ~...
   96 	   101 	 0.24130 	 0.24195 	 ~...
  104 	   102 	 0.24697 	 0.24234 	 ~...
  117 	   103 	 0.29214 	 0.25665 	 m..s
   94 	   104 	 0.23182 	 0.26184 	 m..s
  114 	   105 	 0.28220 	 0.26520 	 ~...
   95 	   106 	 0.23265 	 0.27106 	 m..s
   99 	   107 	 0.24324 	 0.27297 	 ~...
  102 	   108 	 0.24547 	 0.27483 	 ~...
  120 	   109 	 0.30886 	 0.27845 	 m..s
   98 	   110 	 0.24259 	 0.28138 	 m..s
  105 	   111 	 0.24842 	 0.28240 	 m..s
  116 	   112 	 0.29067 	 0.28917 	 ~...
  100 	   113 	 0.24421 	 0.29005 	 m..s
  108 	   114 	 0.25325 	 0.29076 	 m..s
  109 	   115 	 0.25382 	 0.29257 	 m..s
  115 	   116 	 0.28651 	 0.30382 	 ~...
  112 	   117 	 0.27860 	 0.30903 	 m..s
  113 	   118 	 0.27865 	 0.31102 	 m..s
  117 	   119 	 0.29214 	 0.34644 	 m..s
  119 	   120 	 0.29892 	 0.35877 	 m..s
==========================================
r_mrr = 0.9802743792533875
r2_mrr = 0.9599823355674744
spearmanr_mrr@5 = 0.9532170295715332
spearmanr_mrr@10 = 0.8573629856109619
spearmanr_mrr@50 = 0.9793570041656494
spearmanr_mrr@100 = 0.9887260794639587
spearmanr_mrr@All = 0.9896425604820251
==========================================
test time: 0.415
Done Testing dataset Kinships
total time taken: 187.4359645843506
training time taken: 180.77215147018433
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9803)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9600)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9532)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.8574)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9794)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9887)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9896)}}, 'test_loss': {'TransE': {'Kinships': 0.4530643246598629}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 3888587108062485
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [753, 559, 878, 733, 355, 87, 345, 1027, 1128, 954, 919, 101, 1082, 107, 1057, 1109, 118, 569, 1040, 24, 205, 1149, 481, 626, 614, 1174, 329, 535, 365, 84, 805, 175, 834, 249, 900, 1004, 74, 890, 622, 658, 18, 867, 1014, 1134, 957, 599, 699, 125, 1133, 587, 346, 272, 662, 992, 741, 238, 56, 766, 65, 58, 140, 1018, 225, 142, 122, 486, 818, 700, 800, 1013, 304, 941, 709, 208, 273, 618, 690, 100, 1155, 885, 1093, 1168, 330, 512, 1052, 1201, 251, 10, 247, 912, 1098, 872, 385, 57, 593, 635, 918, 688, 36, 1106, 759, 513, 164, 422, 1167, 196, 1148, 201, 189, 197, 19, 491, 846, 322, 681, 421, 813, 103, 417, 143, 837]
valid_ids (0): []
train_ids (1094): [1047, 233, 182, 951, 96, 581, 851, 584, 206, 278, 1060, 174, 523, 54, 425, 881, 1058, 966, 342, 26, 1161, 651, 844, 351, 558, 446, 668, 1045, 905, 91, 1065, 848, 1062, 1159, 1108, 498, 1083, 1138, 1118, 420, 369, 432, 1136, 672, 1002, 702, 1, 1207, 312, 154, 703, 466, 437, 338, 590, 83, 531, 283, 217, 679, 831, 997, 1195, 293, 876, 689, 1172, 546, 743, 961, 817, 255, 773, 874, 270, 295, 179, 470, 479, 382, 629, 1008, 275, 379, 104, 9, 370, 548, 412, 218, 775, 552, 3, 471, 90, 282, 95, 1088, 23, 1102, 1030, 763, 782, 783, 27, 646, 522, 840, 1053, 343, 740, 788, 530, 1069, 257, 677, 404, 583, 33, 862, 325, 669, 792, 532, 52, 378, 187, 82, 698, 1039, 644, 441, 1011, 64, 996, 307, 211, 631, 311, 1132, 1094, 462, 728, 110, 1044, 1150, 215, 139, 945, 1055, 748, 588, 638, 440, 749, 585, 17, 1205, 704, 480, 572, 1092, 790, 1121, 1127, 413, 43, 429, 1012, 711, 1151, 579, 756, 279, 1203, 1214, 1208, 1105, 993, 931, 770, 673, 1183, 427, 924, 828, 750, 1163, 575, 258, 61, 297, 363, 73, 633, 285, 586, 37, 518, 316, 693, 300, 28, 871, 392, 942, 916, 712, 135, 1147, 294, 235, 1176, 248, 397, 974, 1194, 29, 76, 111, 78, 72, 445, 1070, 146, 1009, 331, 398, 1073, 129, 1107, 863, 1206, 987, 554, 746, 825, 1170, 994, 92, 320, 1007, 442, 0, 502, 598, 340, 469, 1061, 281, 415, 636, 390, 1096, 506, 1141, 1157, 31, 567, 776, 899, 594, 1140, 1111, 769, 845, 543, 906, 784, 648, 882, 461, 649, 647, 203, 1064, 214, 510, 77, 274, 39, 550, 290, 393, 683, 893, 639, 317, 11, 263, 789, 180, 1114, 624, 63, 541, 326, 276, 30, 376, 476, 949, 1129, 984, 595, 130, 403, 38, 611, 223, 986, 525, 477, 861, 86, 989, 226, 239, 229, 371, 1123, 822, 608, 1179, 50, 988, 25, 321, 373, 913, 1162, 847, 768, 798, 1085, 456, 952, 377, 287, 81, 927, 306, 944, 487, 888, 93, 640, 947, 898, 20, 496, 760, 1063, 684, 1144, 171, 973, 909, 920, 1035, 744, 910, 1017, 209, 484, 500, 141, 148, 617, 97, 1143, 864, 710, 547, 576, 515, 184, 1117, 727, 880, 88, 144, 938, 830, 368, 983, 692, 752, 765, 747, 1177, 85, 212, 34, 1178, 405, 613, 359, 123, 781, 578, 1022, 1019, 13, 682, 150, 105, 524, 1112, 811, 729, 384, 591, 1212, 678, 833, 127, 115, 691, 886, 237, 855, 106, 191, 1021, 620, 895, 45, 426, 1152, 787, 972, 1187, 625, 977, 922, 660, 250, 859, 298, 656, 832, 341, 925, 687, 244, 564, 835, 939, 870, 460, 616, 971, 894, 960, 301, 44, 120, 810, 438, 434, 779, 745, 902, 408, 483, 35, 537, 1180, 1025, 514, 328, 344, 1054, 303, 464, 334, 452, 1086, 200, 697, 252, 418, 216, 41, 519, 468, 566, 352, 738, 465, 654, 1029, 261, 520, 1185, 210, 1036, 1100, 607, 1046, 1120, 1079, 495, 1031, 169, 877, 126, 936, 731, 48, 713, 601, 1188, 675, 433, 632, 149, 1125, 315, 489, 131, 1010, 764, 75, 815, 1153, 400, 21, 387, 670, 231, 158, 161, 1192, 1066, 799, 402, 347, 1033, 956, 323, 780, 259, 475, 236, 264, 396, 665, 488, 948, 1142, 1181, 2, 641, 655, 198, 167, 178, 192, 243, 933, 1198, 501, 424, 791, 1146, 4, 1003, 609, 695, 60, 414, 1084, 70, 152, 269, 89, 319, 1184, 168, 793, 410, 542, 915, 926, 221, 853, 932, 109, 1038, 265, 778, 928, 652, 527, 254, 842, 145, 305, 367, 435, 472, 176, 1204, 1034, 119, 714, 245, 580, 562, 754, 637, 716, 8, 159, 207, 112, 540, 1091, 1197, 1126, 809, 896, 976, 589, 155, 1042, 416, 1131, 1130, 950, 940, 1196, 539, 336, 188, 386, 857, 364, 1015, 999, 722, 674, 545, 812, 721, 66, 797, 1145, 383, 645, 1154, 350, 1048, 67, 685, 1199, 302, 1202, 606, 409, 240, 114, 284, 602, 571, 138, 447, 734, 419, 664, 995, 353, 873, 998, 968, 958, 1101, 929, 1191, 934, 737, 124, 503, 1156, 478, 538, 814, 819, 544, 804, 563, 557, 356, 246, 785, 1056, 508, 241, 204, 574, 448, 1097, 310, 854, 1006, 1186, 357, 634, 268, 1165, 808, 561, 42, 715, 219, 883, 494, 102, 431, 935, 875, 962, 901, 493, 1164, 1041, 374, 1087, 267, 453, 959, 708, 505, 199, 801, 980, 761, 1075, 1166, 195, 375, 596, 868, 856, 277, 701, 40, 1016, 473, 911, 516, 865, 553, 430, 549, 965, 757, 361, 726, 193, 68, 1071, 360, 767, 917, 253, 904, 389, 395, 1175, 511, 406, 666, 348, 657, 157, 381, 884, 485, 132, 889, 492, 454, 14, 162, 923, 47, 153, 517, 723, 354, 339, 720, 930, 719, 1020, 1099, 597, 1051, 705, 556, 163, 482, 1135, 12, 394, 451, 185, 592, 843, 964, 852, 62, 286, 1119, 222, 450, 529, 953, 332, 568, 1037, 1001, 220, 170, 771, 612, 165, 509, 979, 1089, 173, 806, 291, 55, 628, 1032, 1078, 99, 1090, 457, 772, 603, 892, 1209, 16, 136, 181, 820, 725, 117, 650, 362, 1043, 1050, 113, 762, 985, 458, 299, 324, 774, 869, 850, 266, 327, 891, 1213, 718, 1049, 680, 280, 401, 582, 1139, 1113, 1171, 560, 823, 982, 391, 600, 841, 807, 735, 407, 1028, 667, 186, 838, 309, 573, 619, 160, 459, 969, 866, 1122, 615, 642, 751, 724, 630, 1110, 190, 1193, 455, 242, 333, 1210, 1104, 777, 786, 1023, 388, 829, 1158, 7, 313, 296, 46, 467, 32, 1115, 507, 227, 1103, 1124, 1005, 663, 202, 943, 289, 858, 755, 358, 49, 98, 604, 937, 366, 1137, 94, 1074, 166, 1081, 534, 839, 694, 803, 262, 423, 739, 794, 156, 288, 228, 372, 908, 661, 183, 439, 795, 80, 151, 1173, 526, 232, 1095, 730, 308, 1182, 1068, 849, 887, 975, 260, 623, 732, 497, 380, 736, 411, 69, 577, 51, 463, 1024, 551, 903, 314, 1190, 521, 318, 1076, 194, 836, 1116, 981, 133, 707, 349, 53, 536, 671, 860, 116, 1160, 271, 22, 827, 610, 490, 802, 256, 335, 970, 570, 108, 963, 555, 706, 474, 528, 676, 1059, 128, 121, 1211, 533, 172, 399, 758, 627, 1072, 1067, 914, 234, 879, 907, 821, 696, 1080, 621, 1200, 921, 71, 337, 213, 967, 444, 605, 449, 134, 1189, 991, 1026, 990, 147, 1169, 643, 717, 659, 946, 436, 1000, 230, 826, 978, 897, 796, 565, 504, 824, 224, 742, 499, 443, 1077, 955, 6, 292, 5, 428, 686, 59, 653, 177, 816, 79, 15, 137]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3516266246836159
the save name prefix for this run is:  chkpt-ID_3516266246836159_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 706
rank avg (pred): 0.567 +- 0.007
mrr vals (pred, true): 0.017, 0.044
batch losses (mrrl, rdl): 0.0, 0.0002788407

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 936
rank avg (pred): 0.332 +- 0.252
mrr vals (pred, true): 0.211, 0.040
batch losses (mrrl, rdl): 0.0, 0.0002175296

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1067
rank avg (pred): 0.318 +- 0.271
mrr vals (pred, true): 0.294, 0.336
batch losses (mrrl, rdl): 0.0, 0.0012444117

Epoch over!
epoch time: 12.118

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 819
rank avg (pred): 0.280 +- 0.266
mrr vals (pred, true): 0.373, 0.182
batch losses (mrrl, rdl): 0.0, 0.0001214802

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 545
rank avg (pred): 0.332 +- 0.276
mrr vals (pred, true): 0.284, 0.242
batch losses (mrrl, rdl): 0.0, 0.001145229

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 951
rank avg (pred): 0.342 +- 0.279
mrr vals (pred, true): 0.296, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001933734

Epoch over!
epoch time: 11.88

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 190
rank avg (pred): 0.294 +- 0.280
mrr vals (pred, true): 0.402, 0.047
batch losses (mrrl, rdl): 0.0, 0.0003340093

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 689
rank avg (pred): 0.308 +- 0.280
mrr vals (pred, true): 0.357, 0.044
batch losses (mrrl, rdl): 0.0, 0.0003141277

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 8
rank avg (pred): 0.329 +- 0.274
mrr vals (pred, true): 0.292, 0.283
batch losses (mrrl, rdl): 0.0, 0.0012222917

Epoch over!
epoch time: 12.038

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 127
rank avg (pred): 0.291 +- 0.278
mrr vals (pred, true): 0.413, 0.052
batch losses (mrrl, rdl): 0.0, 0.0002137251

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 979
rank avg (pred): 0.305 +- 0.279
mrr vals (pred, true): 0.379, 0.331
batch losses (mrrl, rdl): 0.0, 0.0010632648

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 869
rank avg (pred): 0.310 +- 0.286
mrr vals (pred, true): 0.397, 0.045
batch losses (mrrl, rdl): 0.0, 0.0002746503

Epoch over!
epoch time: 11.892

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 642
rank avg (pred): 0.348 +- 0.264
mrr vals (pred, true): 0.237, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001204819

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 781
rank avg (pred): 0.323 +- 0.289
mrr vals (pred, true): 0.375, 0.044
batch losses (mrrl, rdl): 0.0, 0.0002175207

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 740
rank avg (pred): 0.298 +- 0.282
mrr vals (pred, true): 0.415, 0.178
batch losses (mrrl, rdl): 0.0, 0.0001956667

Epoch over!
epoch time: 11.831

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 95
rank avg (pred): 0.333 +- 0.270
mrr vals (pred, true): 0.282, 0.054
batch losses (mrrl, rdl): 0.5370193124, 0.0001009944

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 293
rank avg (pred): 0.347 +- 0.182
mrr vals (pred, true): 0.145, 0.275
batch losses (mrrl, rdl): 0.1695891321, 0.0012439805

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 980
rank avg (pred): 0.347 +- 0.167
mrr vals (pred, true): 0.132, 0.347
batch losses (mrrl, rdl): 0.4636542201, 0.001404989

Epoch over!
epoch time: 12.567

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1178
rank avg (pred): 0.372 +- 0.154
mrr vals (pred, true): 0.112, 0.046
batch losses (mrrl, rdl): 0.0388637632, 0.000119182

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 631
rank avg (pred): 0.387 +- 0.132
mrr vals (pred, true): 0.101, 0.047
batch losses (mrrl, rdl): 0.0258426033, 0.0001017866

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 295
rank avg (pred): 0.334 +- 0.157
mrr vals (pred, true): 0.136, 0.263
batch losses (mrrl, rdl): 0.1599212289, 0.0011763023

Epoch over!
epoch time: 12.252

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 959
rank avg (pred): 0.380 +- 0.123
mrr vals (pred, true): 0.101, 0.043
batch losses (mrrl, rdl): 0.0261856224, 0.0002052243

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 609
rank avg (pred): 0.363 +- 0.126
mrr vals (pred, true): 0.109, 0.048
batch losses (mrrl, rdl): 0.0346151665, 0.0001826232

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 827
rank avg (pred): 0.343 +- 0.138
mrr vals (pred, true): 0.124, 0.277
batch losses (mrrl, rdl): 0.2349085361, 0.0010011412

Epoch over!
epoch time: 12.113

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 732
rank avg (pred): 0.366 +- 0.118
mrr vals (pred, true): 0.103, 0.223
batch losses (mrrl, rdl): 0.1435164064, 0.000732211

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1061
rank avg (pred): 0.296 +- 0.128
mrr vals (pred, true): 0.132, 0.325
batch losses (mrrl, rdl): 0.3705662191, 0.000867032

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 356
rank avg (pred): 0.323 +- 0.143
mrr vals (pred, true): 0.137, 0.054
batch losses (mrrl, rdl): 0.0756691471, 0.0001956937

Epoch over!
epoch time: 12.184

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 529
rank avg (pred): 0.363 +- 0.119
mrr vals (pred, true): 0.103, 0.211
batch losses (mrrl, rdl): 0.1171803921, 0.001366303

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 506
rank avg (pred): 0.353 +- 0.119
mrr vals (pred, true): 0.108, 0.221
batch losses (mrrl, rdl): 0.128597796, 0.0009923078

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 63
rank avg (pred): 0.346 +- 0.124
mrr vals (pred, true): 0.115, 0.297
batch losses (mrrl, rdl): 0.3299581707, 0.0014248863

Epoch over!
epoch time: 12.259

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1102
rank avg (pred): 0.247 +- 0.114
mrr vals (pred, true): 0.156, 0.057
batch losses (mrrl, rdl): 0.1113661379, 0.000551864

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 465
rank avg (pred): 0.321 +- 0.133
mrr vals (pred, true): 0.132, 0.042
batch losses (mrrl, rdl): 0.0679920167, 0.0004451752

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 374
rank avg (pred): 0.330 +- 0.129
mrr vals (pred, true): 0.125, 0.054
batch losses (mrrl, rdl): 0.0561775416, 0.0002220941

Epoch over!
epoch time: 12.32

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 838
rank avg (pred): 0.349 +- 0.110
mrr vals (pred, true): 0.104, 0.043
batch losses (mrrl, rdl): 0.0294212401, 0.0003766972

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 624
rank avg (pred): 0.345 +- 0.106
mrr vals (pred, true): 0.102, 0.041
batch losses (mrrl, rdl): 0.0270416178, 0.0004116746

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 172
rank avg (pred): 0.346 +- 0.111
mrr vals (pred, true): 0.104, 0.045
batch losses (mrrl, rdl): 0.0296878535, 0.0003587201

Epoch over!
epoch time: 12.155

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1180
rank avg (pred): 0.346 +- 0.110
mrr vals (pred, true): 0.104, 0.044
batch losses (mrrl, rdl): 0.0289748944, 0.0002935136

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1193
rank avg (pred): 0.325 +- 0.116
mrr vals (pred, true): 0.124, 0.045
batch losses (mrrl, rdl): 0.0541430339, 0.0004414233

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 436
rank avg (pred): 0.327 +- 0.122
mrr vals (pred, true): 0.124, 0.045
batch losses (mrrl, rdl): 0.0554788709, 0.000487124

Epoch over!
epoch time: 12.382

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1046
rank avg (pred): 0.289 +- 0.105
mrr vals (pred, true): 0.122, 0.041
batch losses (mrrl, rdl): 0.0513645187, 0.0008023143

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 388
rank avg (pred): 0.327 +- 0.122
mrr vals (pred, true): 0.126, 0.057
batch losses (mrrl, rdl): 0.0576448664, 0.0001891874

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 542
rank avg (pred): 0.341 +- 0.109
mrr vals (pred, true): 0.107, 0.233
batch losses (mrrl, rdl): 0.158824563, 0.0009791432

Epoch over!
epoch time: 12.118

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 377
rank avg (pred): 0.317 +- 0.125
mrr vals (pred, true): 0.135, 0.057
batch losses (mrrl, rdl): 0.0719448105, 0.000203079

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1191
rank avg (pred): 0.347 +- 0.105
mrr vals (pred, true): 0.101, 0.044
batch losses (mrrl, rdl): 0.0258245245, 0.0003257965

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 610
rank avg (pred): 0.333 +- 0.111
mrr vals (pred, true): 0.114, 0.045
batch losses (mrrl, rdl): 0.0405395739, 0.0003383006

Epoch over!
epoch time: 12.155

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.345 +- 0.107
mrr vals (pred, true): 0.104, 0.264

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
  115 	     0 	 0.13406 	 0.04004 	 m..s
   73 	     1 	 0.12126 	 0.04079 	 m..s
   82 	     2 	 0.12239 	 0.04134 	 m..s
   92 	     3 	 0.12620 	 0.04154 	 m..s
    0 	     4 	 0.09244 	 0.04198 	 m..s
   12 	     5 	 0.09956 	 0.04211 	 m..s
   30 	     6 	 0.10374 	 0.04216 	 m..s
   92 	     7 	 0.12620 	 0.04231 	 m..s
   14 	     8 	 0.09966 	 0.04232 	 m..s
   44 	     9 	 0.10683 	 0.04250 	 m..s
   56 	    10 	 0.11321 	 0.04270 	 m..s
  111 	    11 	 0.13102 	 0.04274 	 m..s
    8 	    12 	 0.09707 	 0.04275 	 m..s
   92 	    13 	 0.12620 	 0.04280 	 m..s
   17 	    14 	 0.10128 	 0.04284 	 m..s
   51 	    15 	 0.10971 	 0.04298 	 m..s
  118 	    16 	 0.13557 	 0.04299 	 m..s
   64 	    17 	 0.11684 	 0.04301 	 m..s
   32 	    18 	 0.10426 	 0.04322 	 m..s
   40 	    19 	 0.10532 	 0.04341 	 m..s
   21 	    20 	 0.10267 	 0.04354 	 m..s
  120 	    21 	 0.13713 	 0.04370 	 m..s
    4 	    22 	 0.09456 	 0.04387 	 m..s
   41 	    23 	 0.10585 	 0.04393 	 m..s
   84 	    24 	 0.12258 	 0.04399 	 m..s
    7 	    25 	 0.09641 	 0.04402 	 m..s
   72 	    26 	 0.12050 	 0.04412 	 m..s
   34 	    27 	 0.10445 	 0.04418 	 m..s
    9 	    28 	 0.09728 	 0.04418 	 m..s
   92 	    29 	 0.12620 	 0.04421 	 m..s
   20 	    30 	 0.10261 	 0.04422 	 m..s
   18 	    31 	 0.10185 	 0.04432 	 m..s
   91 	    32 	 0.12599 	 0.04444 	 m..s
   69 	    33 	 0.12014 	 0.04448 	 m..s
   26 	    34 	 0.10315 	 0.04469 	 m..s
   81 	    35 	 0.12223 	 0.04473 	 m..s
   45 	    36 	 0.10710 	 0.04487 	 m..s
   54 	    37 	 0.11130 	 0.04522 	 m..s
   61 	    38 	 0.11582 	 0.04542 	 m..s
   92 	    39 	 0.12620 	 0.04544 	 m..s
    1 	    40 	 0.09290 	 0.04576 	 m..s
    2 	    41 	 0.09454 	 0.04586 	 m..s
   53 	    42 	 0.11040 	 0.04611 	 m..s
   38 	    43 	 0.10486 	 0.04625 	 m..s
   89 	    44 	 0.12441 	 0.04628 	 m..s
   36 	    45 	 0.10464 	 0.04629 	 m..s
   92 	    46 	 0.12620 	 0.04653 	 m..s
   33 	    47 	 0.10439 	 0.04663 	 m..s
   28 	    48 	 0.10354 	 0.04704 	 m..s
   92 	    49 	 0.12620 	 0.04713 	 m..s
   31 	    50 	 0.10401 	 0.04724 	 m..s
   38 	    51 	 0.10486 	 0.04748 	 m..s
   50 	    52 	 0.10940 	 0.04753 	 m..s
   59 	    53 	 0.11513 	 0.04829 	 m..s
   37 	    54 	 0.10465 	 0.04852 	 m..s
   65 	    55 	 0.11705 	 0.04874 	 m..s
   19 	    56 	 0.10216 	 0.04910 	 m..s
   51 	    57 	 0.10971 	 0.04940 	 m..s
   16 	    58 	 0.10015 	 0.04945 	 m..s
   63 	    59 	 0.11629 	 0.04980 	 m..s
   35 	    60 	 0.10457 	 0.05020 	 m..s
  112 	    61 	 0.13140 	 0.05028 	 m..s
  113 	    62 	 0.13224 	 0.05077 	 m..s
   75 	    63 	 0.12143 	 0.05123 	 m..s
   67 	    64 	 0.11964 	 0.05144 	 m..s
  110 	    65 	 0.13049 	 0.05144 	 m..s
   86 	    66 	 0.12294 	 0.05171 	 m..s
   42 	    67 	 0.10598 	 0.05179 	 m..s
   88 	    68 	 0.12353 	 0.05188 	 m..s
   92 	    69 	 0.12620 	 0.05194 	 m..s
   92 	    70 	 0.12620 	 0.05294 	 m..s
   57 	    71 	 0.11362 	 0.05327 	 m..s
  115 	    72 	 0.13406 	 0.05336 	 m..s
   92 	    73 	 0.12620 	 0.05340 	 m..s
   92 	    74 	 0.12620 	 0.05367 	 m..s
   89 	    75 	 0.12441 	 0.05395 	 m..s
   62 	    76 	 0.11619 	 0.05472 	 m..s
   87 	    77 	 0.12349 	 0.05512 	 m..s
   92 	    78 	 0.12620 	 0.05513 	 m..s
   92 	    79 	 0.12620 	 0.05514 	 m..s
   76 	    80 	 0.12168 	 0.05552 	 m..s
  118 	    81 	 0.13557 	 0.05557 	 m..s
  117 	    82 	 0.13523 	 0.05711 	 m..s
   78 	    83 	 0.12202 	 0.06170 	 m..s
   21 	    84 	 0.10267 	 0.11025 	 ~...
    2 	    85 	 0.09454 	 0.14413 	 m..s
    5 	    86 	 0.09458 	 0.18570 	 m..s
    6 	    87 	 0.09553 	 0.18775 	 m..s
   27 	    88 	 0.10336 	 0.19920 	 m..s
   24 	    89 	 0.10276 	 0.19996 	 m..s
   13 	    90 	 0.09964 	 0.21070 	 MISS
   10 	    91 	 0.09807 	 0.21355 	 MISS
   25 	    92 	 0.10309 	 0.22278 	 MISS
   14 	    93 	 0.09966 	 0.22530 	 MISS
   42 	    94 	 0.10598 	 0.23641 	 MISS
   57 	    95 	 0.11362 	 0.24195 	 MISS
   11 	    96 	 0.09954 	 0.24312 	 MISS
   23 	    97 	 0.10271 	 0.24908 	 MISS
   55 	    98 	 0.11262 	 0.25575 	 MISS
   29 	    99 	 0.10372 	 0.26363 	 MISS
   46 	   100 	 0.10782 	 0.26493 	 MISS
   83 	   101 	 0.12243 	 0.26580 	 MISS
   74 	   102 	 0.12141 	 0.26618 	 MISS
   60 	   103 	 0.11533 	 0.26647 	 MISS
   47 	   104 	 0.10823 	 0.26778 	 MISS
   71 	   105 	 0.12036 	 0.26893 	 MISS
   49 	   106 	 0.10864 	 0.27017 	 MISS
   66 	   107 	 0.11777 	 0.27108 	 MISS
   85 	   108 	 0.12279 	 0.27505 	 MISS
   67 	   109 	 0.11964 	 0.27549 	 MISS
   92 	   110 	 0.12620 	 0.27558 	 MISS
   48 	   111 	 0.10834 	 0.27973 	 MISS
   92 	   112 	 0.12620 	 0.28138 	 MISS
   92 	   113 	 0.12620 	 0.28181 	 MISS
   80 	   114 	 0.12219 	 0.28601 	 MISS
   79 	   115 	 0.12216 	 0.28689 	 MISS
   92 	   116 	 0.12620 	 0.28723 	 MISS
   92 	   117 	 0.12620 	 0.29005 	 MISS
   77 	   118 	 0.12198 	 0.29076 	 MISS
  114 	   119 	 0.13351 	 0.30382 	 MISS
   70 	   120 	 0.12024 	 0.35540 	 MISS
==========================================
r_mrr = 0.009044445119798183
r2_mrr = -0.014811038970947266
spearmanr_mrr@5 = 0.8803953528404236
spearmanr_mrr@10 = 0.7421550750732422
spearmanr_mrr@50 = 0.8114508986473083
spearmanr_mrr@100 = 0.8246818780899048
spearmanr_mrr@All = 0.7994216680526733
==========================================
test time: 0.454
Done Testing dataset Kinships
total time taken: 189.62799048423767
training time taken: 182.8006992340088
TWIG out ;))
Ablation done!
The best results were: None
The best settings found were:

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 6596451614249046
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [40, 856, 354, 243, 1047, 930, 71, 444, 670, 545, 432, 717, 495, 1096, 151, 1064, 131, 17, 925, 316, 303, 464, 408, 699, 598, 787, 589, 659, 415, 861, 218, 199, 149, 29, 519, 380, 417, 1199, 429, 637, 472, 1013, 1062, 1011, 348, 312, 279, 987, 1092, 1145, 487, 225, 301, 998, 773, 179, 802, 986, 1114, 569, 1131, 591, 808, 1007, 819, 99, 448, 1132, 1044, 98, 1192, 30, 9, 664, 996, 751, 85, 441, 1112, 782, 1197, 70, 876, 244, 197, 19, 509, 570, 237, 35, 1141, 720, 726, 318, 1063, 542, 127, 496, 831, 1035, 872, 1214, 502, 11, 425, 280, 378, 812, 778, 1060, 638, 174, 484, 798, 1032, 1061, 1105, 739, 159, 248, 411]
valid_ids (0): []
train_ids (1094): [953, 363, 676, 1193, 1117, 758, 368, 1150, 173, 1140, 682, 546, 356, 1116, 592, 297, 619, 281, 193, 929, 427, 1118, 265, 379, 48, 603, 794, 599, 612, 57, 78, 374, 391, 941, 5, 737, 525, 473, 23, 962, 41, 718, 112, 167, 177, 918, 610, 1049, 658, 890, 486, 990, 416, 471, 55, 584, 1159, 491, 649, 1170, 952, 1058, 1201, 1026, 605, 656, 1079, 877, 435, 183, 1055, 246, 724, 561, 475, 403, 330, 668, 571, 433, 1110, 481, 103, 404, 1176, 974, 747, 498, 694, 826, 337, 774, 250, 544, 50, 991, 860, 1134, 67, 678, 940, 1078, 516, 950, 221, 172, 1172, 227, 674, 68, 869, 1037, 684, 314, 180, 109, 1187, 393, 1074, 326, 437, 198, 182, 617, 206, 597, 345, 700, 306, 479, 60, 508, 1208, 154, 257, 799, 1083, 290, 927, 882, 240, 847, 985, 184, 932, 145, 640, 108, 970, 390, 65, 25, 1143, 431, 106, 192, 1070, 1046, 646, 514, 234, 1059, 999, 335, 1207, 135, 976, 152, 83, 652, 552, 252, 606, 191, 84, 797, 870, 588, 818, 322, 258, 352, 625, 833, 719, 370, 650, 321, 816, 115, 777, 136, 144, 789, 942, 873, 853, 91, 107, 230, 1183, 129, 181, 697, 195, 755, 527, 1, 1169, 738, 984, 644, 517, 1137, 405, 412, 533, 308, 820, 710, 229, 434, 1088, 194, 90, 1165, 474, 1135, 439, 1202, 702, 513, 746, 1174, 743, 342, 236, 245, 754, 386, 249, 241, 1151, 982, 351, 66, 273, 259, 483, 854, 365, 1077, 862, 955, 537, 261, 510, 285, 949, 934, 1136, 223, 716, 965, 759, 1010, 232, 620, 500, 274, 213, 975, 834, 87, 141, 501, 642, 383, 1128, 707, 385, 613, 292, 1076, 943, 1147, 733, 900, 74, 752, 157, 62, 105, 395, 1095, 628, 1189, 456, 376, 667, 966, 634, 482, 842, 1041, 547, 884, 828, 864, 16, 536, 26, 800, 1023, 1001, 1084, 825, 226, 196, 735, 215, 1153, 1033, 58, 708, 443, 741, 165, 372, 212, 538, 608, 624, 27, 457, 647, 220, 898, 377, 729, 521, 373, 904, 219, 711, 305, 242, 643, 899, 12, 558, 394, 418, 761, 905, 211, 791, 1212, 333, 1210, 548, 45, 917, 32, 160, 309, 1109, 260, 1004, 1127, 187, 878, 426, 51, 288, 621, 458, 130, 75, 304, 1205, 1188, 399, 779, 703, 256, 604, 298, 132, 1206, 896, 117, 1133, 557, 323, 1085, 1175, 948, 360, 551, 630, 302, 1031, 398, 1091, 400, 979, 367, 1103, 564, 566, 410, 836, 559, 922, 217, 1022, 1106, 413, 786, 840, 291, 43, 671, 284, 1099, 122, 549, 848, 1009, 963, 375, 912, 936, 512, 645, 31, 283, 396, 865, 1016, 822, 506, 222, 307, 1098, 1005, 1052, 111, 964, 1177, 575, 419, 73, 666, 829, 504, 92, 734, 683, 147, 470, 207, 293, 857, 140, 728, 736, 845, 420, 315, 430, 709, 680, 1082, 185, 80, 654, 771, 919, 269, 926, 732, 933, 59, 202, 781, 233, 1158, 846, 1161, 4, 387, 171, 275, 1014, 715, 88, 121, 550, 524, 727, 823, 515, 300, 691, 169, 1086, 937, 636, 436, 344, 805, 289, 748, 881, 272, 995, 1071, 959, 837, 113, 447, 1051, 824, 915, 821, 332, 54, 21, 480, 69, 1045, 477, 595, 883, 1173, 287, 1018, 1093, 795, 572, 102, 453, 1190, 1015, 346, 499, 669, 95, 170, 1050, 1067, 200, 1113, 266, 247, 895, 282, 555, 578, 796, 347, 1209, 161, 1115, 449, 86, 189, 1182, 320, 1036, 908, 665, 1163, 1156, 815, 651, 693, 633, 493, 690, 407, 886, 1040, 138, 362, 263, 463, 1024, 879, 44, 166, 343, 583, 713, 104, 1102, 270, 685, 921, 830, 485, 123, 553, 692, 489, 327, 476, 579, 535, 1097, 208, 622, 1029, 851, 1198, 1017, 235, 698, 764, 63, 543, 858, 568, 397, 695, 409, 310, 349, 677, 540, 350, 772, 6, 231, 1101, 749, 0, 1185, 461, 20, 627, 1065, 971, 1171, 887, 462, 1119, 980, 522, 868, 497, 814, 655, 523, 1138, 1104, 641, 762, 13, 361, 389, 722, 175, 600, 745, 8, 1149, 954, 353, 909, 817, 278, 587, 973, 77, 1130, 148, 1000, 567, 961, 488, 601, 760, 89, 339, 712, 3, 1121, 1157, 364, 594, 39, 968, 186, 268, 892, 271, 33, 766, 139, 163, 1090, 1122, 96, 623, 616, 660, 530, 1139, 661, 1089, 945, 406, 1186, 15, 581, 730, 916, 388, 295, 1073, 1075, 534, 188, 381, 585, 835, 577, 672, 866, 903, 423, 146, 871, 255, 446, 1126, 1108, 783, 118, 1125, 1094, 355, 455, 1144, 276, 137, 478, 1006, 494, 371, 639, 635, 469, 1168, 317, 626, 528, 1012, 1019, 209, 596, 36, 859, 1148, 1008, 22, 704, 319, 1081, 422, 972, 827, 1068, 631, 262, 1087, 1053, 993, 775, 1162, 756, 459, 924, 392, 1021, 983, 253, 648, 366, 931, 565, 277, 532, 1043, 338, 1184, 744, 804, 205, 168, 341, 988, 757, 768, 384, 14, 451, 511, 238, 153, 1057, 855, 753, 47, 688, 843, 944, 541, 340, 880, 1200, 1027, 18, 1181, 28, 947, 267, 580, 442, 125, 1072, 1191, 239, 369, 1107, 440, 460, 867, 216, 554, 539, 1054, 811, 294, 844, 891, 382, 210, 679, 24, 1034, 1020, 37, 299, 629, 740, 1003, 618, 780, 958, 64, 529, 663, 1030, 526, 79, 1042, 1069, 852, 401, 214, 124, 706, 507, 1179, 531, 424, 923, 832, 42, 939, 7, 116, 997, 150, 989, 928, 334, 1100, 889, 1080, 849, 776, 264, 725, 902, 938, 742, 156, 34, 468, 901, 897, 76, 56, 602, 421, 142, 920, 143, 503, 492, 328, 910, 10, 311, 763, 93, 1160, 329, 82, 981, 1056, 49, 632, 935, 52, 977, 960, 801, 967, 445, 574, 1194, 957, 673, 1204, 1154, 53, 359, 38, 176, 101, 792, 978, 765, 615, 785, 1180, 784, 204, 1038, 696, 46, 114, 1039, 614, 788, 705, 653, 1025, 158, 358, 793, 863, 254, 1066, 992, 807, 1152, 573, 336, 1155, 1028, 164, 110, 563, 689, 190, 61, 951, 286, 582, 770, 134, 907, 590, 128, 809, 914, 296, 1146, 119, 1196, 803, 325, 875, 850, 100, 969, 838, 750, 414, 520, 609, 1124, 1164, 1178, 228, 888, 224, 810, 1111, 806, 402, 769, 723, 438, 576, 721, 675, 560, 155, 556, 994, 1048, 714, 518, 1123, 1166, 657, 133, 906, 81, 593, 1120, 586, 203, 790, 1142, 466, 251, 450, 162, 1167, 913, 894, 331, 731, 607, 452, 686, 94, 490, 662, 2, 687, 505, 841, 767, 956, 681, 893, 313, 178, 1129, 1203, 357, 946, 839, 97, 201, 611, 467, 465, 72, 911, 1211, 428, 1195, 885, 813, 562, 1002, 1213, 120, 701, 454, 324, 126, 874]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  578551382359514
the save name prefix for this run is:  chkpt-ID_578551382359514_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 177
rank avg (pred): 0.514 +- 0.002
mrr vals (pred, true): 0.019, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001079646

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 214
rank avg (pred): 0.465 +- 0.020
mrr vals (pred, true): 0.020, 0.042
batch losses (mrrl, rdl): 0.0, 7.66926e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 386
rank avg (pred): 0.413 +- 0.258
mrr vals (pred, true): 0.045, 0.055
batch losses (mrrl, rdl): 0.0, 2.561e-07

Epoch over!
epoch time: 12.138

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 447
rank avg (pred): 0.463 +- 0.266
mrr vals (pred, true): 0.036, 0.046
batch losses (mrrl, rdl): 0.0, 9.29e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 850
rank avg (pred): 0.475 +- 0.272
mrr vals (pred, true): 0.035, 0.043
batch losses (mrrl, rdl): 0.0, 2.3432e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 491
rank avg (pred): 0.126 +- 0.149
mrr vals (pred, true): 0.159, 0.223
batch losses (mrrl, rdl): 0.0, 3.852e-07

Epoch over!
epoch time: 11.9

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 926
rank avg (pred): 0.477 +- 0.271
mrr vals (pred, true): 0.036, 0.046
batch losses (mrrl, rdl): 0.0, 7.456e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 946
rank avg (pred): 0.459 +- 0.264
mrr vals (pred, true): 0.039, 0.044
batch losses (mrrl, rdl): 0.0, 9.9917e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1140
rank avg (pred): 0.135 +- 0.168
mrr vals (pred, true): 0.166, 0.289
batch losses (mrrl, rdl): 0.0, 1.76977e-05

Epoch over!
epoch time: 11.955

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 547
rank avg (pred): 0.112 +- 0.139
mrr vals (pred, true): 0.182, 0.184
batch losses (mrrl, rdl): 0.0, 3.6024e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 428
rank avg (pred): 0.444 +- 0.265
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0, 9.9545e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 848
rank avg (pred): 0.468 +- 0.277
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0, 1.6463e-06

Epoch over!
epoch time: 11.895

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1184
rank avg (pred): 0.456 +- 0.271
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0, 3.291e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 296
rank avg (pred): 0.115 +- 0.141
mrr vals (pred, true): 0.185, 0.284
batch losses (mrrl, rdl): 0.0, 2.202e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1164
rank avg (pred): 0.454 +- 0.276
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 0.0, 7.721e-07

Epoch over!
epoch time: 12.015

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 850
rank avg (pred): 0.451 +- 0.271
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 2.2565e-06, 1.539e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 761
rank avg (pred): 0.484 +- 0.258
mrr vals (pred, true): 0.044, 0.045
batch losses (mrrl, rdl): 0.0003872189, 3.4168e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 307
rank avg (pred): 0.114 +- 0.163
mrr vals (pred, true): 0.303, 0.273
batch losses (mrrl, rdl): 0.008786208, 4.1552e-06

Epoch over!
epoch time: 12.221

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 888
rank avg (pred): 0.460 +- 0.230
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0004087779, 6.6491e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 65
rank avg (pred): 0.155 +- 0.181
mrr vals (pred, true): 0.264, 0.287
batch losses (mrrl, rdl): 0.0052604582, 4.25541e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 511
rank avg (pred): 0.198 +- 0.196
mrr vals (pred, true): 0.222, 0.190
batch losses (mrrl, rdl): 0.0103520881, 0.0001280814

Epoch over!
epoch time: 12.061

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 995
rank avg (pred): 0.091 +- 0.127
mrr vals (pred, true): 0.340, 0.348
batch losses (mrrl, rdl): 0.0006149937, 4.847e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1207
rank avg (pred): 0.466 +- 0.214
mrr vals (pred, true): 0.044, 0.045
batch losses (mrrl, rdl): 0.000351137, 9.8755e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 153
rank avg (pred): 0.438 +- 0.208
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 0.0001218003, 1.398e-05

Epoch over!
epoch time: 12.152

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 286
rank avg (pred): 0.127 +- 0.144
mrr vals (pred, true): 0.290, 0.275
batch losses (mrrl, rdl): 0.0022704536, 2.57902e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 844
rank avg (pred): 0.466 +- 0.226
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 3.29634e-05, 7.9741e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 352
rank avg (pred): 0.456 +- 0.207
mrr vals (pred, true): 0.042, 0.052
batch losses (mrrl, rdl): 0.0006871484, 2.40346e-05

Epoch over!
epoch time: 12.238

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 245
rank avg (pred): 0.195 +- 0.173
mrr vals (pred, true): 0.243, 0.291
batch losses (mrrl, rdl): 0.0227002501, 0.0001642725

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 120
rank avg (pred): 0.435 +- 0.191
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 9.6123e-06, 2.13683e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1079
rank avg (pred): 0.085 +- 0.114
mrr vals (pred, true): 0.361, 0.351
batch losses (mrrl, rdl): 0.0009905612, 5.438e-07

Epoch over!
epoch time: 11.963

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 169
rank avg (pred): 0.467 +- 0.199
mrr vals (pred, true): 0.036, 0.044
batch losses (mrrl, rdl): 0.0020813227, 2.11855e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 956
rank avg (pred): 0.450 +- 0.220
mrr vals (pred, true): 0.054, 0.048
batch losses (mrrl, rdl): 0.0001290441, 1.05115e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1014
rank avg (pred): 0.394 +- 0.149
mrr vals (pred, true): 0.052, 0.057
batch losses (mrrl, rdl): 6.17411e-05, 5.77137e-05

Epoch over!
epoch time: 12.421

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1126
rank avg (pred): 0.413 +- 0.150
mrr vals (pred, true): 0.043, 0.049
batch losses (mrrl, rdl): 0.0004692098, 8.83867e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 577
rank avg (pred): 0.455 +- 0.217
mrr vals (pred, true): 0.050, 0.048
batch losses (mrrl, rdl): 9.39e-08, 1.23049e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 180
rank avg (pred): 0.412 +- 0.184
mrr vals (pred, true): 0.059, 0.050
batch losses (mrrl, rdl): 0.0007355982, 5.03408e-05

Epoch over!
epoch time: 12.206

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 721
rank avg (pred): 0.428 +- 0.197
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0001854022, 6.34049e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 62
rank avg (pred): 0.143 +- 0.147
mrr vals (pred, true): 0.271, 0.258
batch losses (mrrl, rdl): 0.0014786568, 1.81332e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 201
rank avg (pred): 0.437 +- 0.211
mrr vals (pred, true): 0.051, 0.041
batch losses (mrrl, rdl): 4.0962e-06, 5.15846e-05

Epoch over!
epoch time: 12.051

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 489
rank avg (pred): 0.301 +- 0.202
mrr vals (pred, true): 0.200, 0.211
batch losses (mrrl, rdl): 0.0011034607, 0.0006982352

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1008
rank avg (pred): 0.392 +- 0.151
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 1.0602e-05, 9.85205e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 755
rank avg (pred): 0.243 +- 0.196
mrr vals (pred, true): 0.227, 0.277
batch losses (mrrl, rdl): 0.0250868574, 0.0002544479

Epoch over!
epoch time: 12.099

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 24
rank avg (pred): 0.153 +- 0.151
mrr vals (pred, true): 0.276, 0.271
batch losses (mrrl, rdl): 0.0002919801, 9.3366e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 457
rank avg (pred): 0.408 +- 0.188
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002146167, 9.8228e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 138
rank avg (pred): 0.458 +- 0.235
mrr vals (pred, true): 0.048, 0.047
batch losses (mrrl, rdl): 2.97373e-05, 6.2714e-06

Epoch over!
epoch time: 11.909

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.155 +- 0.153
mrr vals (pred, true): 0.275, 0.278

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   24 	     0 	 0.04780 	 0.04205 	 ~...
    9 	     1 	 0.04570 	 0.04206 	 ~...
   31 	     2 	 0.04808 	 0.04207 	 ~...
   10 	     3 	 0.04585 	 0.04215 	 ~...
   34 	     4 	 0.04830 	 0.04217 	 ~...
   56 	     5 	 0.04959 	 0.04225 	 ~...
   70 	     6 	 0.05061 	 0.04227 	 ~...
   46 	     7 	 0.04913 	 0.04262 	 ~...
   67 	     8 	 0.05040 	 0.04263 	 ~...
   61 	     9 	 0.05019 	 0.04273 	 ~...
   30 	    10 	 0.04798 	 0.04282 	 ~...
    6 	    11 	 0.04513 	 0.04303 	 ~...
   32 	    12 	 0.04819 	 0.04309 	 ~...
   44 	    13 	 0.04897 	 0.04313 	 ~...
   57 	    14 	 0.04968 	 0.04315 	 ~...
   45 	    15 	 0.04901 	 0.04326 	 ~...
   13 	    16 	 0.04661 	 0.04345 	 ~...
    0 	    17 	 0.04262 	 0.04346 	 ~...
   54 	    18 	 0.04943 	 0.04354 	 ~...
   43 	    19 	 0.04896 	 0.04362 	 ~...
    7 	    20 	 0.04525 	 0.04363 	 ~...
   16 	    21 	 0.04688 	 0.04369 	 ~...
   29 	    22 	 0.04792 	 0.04371 	 ~...
    3 	    23 	 0.04385 	 0.04384 	 ~...
   52 	    24 	 0.04936 	 0.04406 	 ~...
   53 	    25 	 0.04937 	 0.04408 	 ~...
   21 	    26 	 0.04762 	 0.04412 	 ~...
   38 	    27 	 0.04842 	 0.04421 	 ~...
   33 	    28 	 0.04822 	 0.04432 	 ~...
   51 	    29 	 0.04934 	 0.04434 	 ~...
   13 	    30 	 0.04661 	 0.04436 	 ~...
   23 	    31 	 0.04774 	 0.04463 	 ~...
   27 	    32 	 0.04790 	 0.04475 	 ~...
   71 	    33 	 0.05078 	 0.04475 	 ~...
   26 	    34 	 0.04786 	 0.04476 	 ~...
   20 	    35 	 0.04757 	 0.04485 	 ~...
   19 	    36 	 0.04750 	 0.04486 	 ~...
   40 	    37 	 0.04849 	 0.04495 	 ~...
   39 	    38 	 0.04844 	 0.04506 	 ~...
    2 	    39 	 0.04310 	 0.04509 	 ~...
   49 	    40 	 0.04932 	 0.04514 	 ~...
   35 	    41 	 0.04833 	 0.04515 	 ~...
   28 	    42 	 0.04790 	 0.04536 	 ~...
   68 	    43 	 0.05040 	 0.04542 	 ~...
   25 	    44 	 0.04785 	 0.04551 	 ~...
   75 	    45 	 0.05128 	 0.04570 	 ~...
    4 	    46 	 0.04480 	 0.04592 	 ~...
   68 	    47 	 0.05040 	 0.04596 	 ~...
   22 	    48 	 0.04764 	 0.04622 	 ~...
   62 	    49 	 0.05023 	 0.04644 	 ~...
   60 	    50 	 0.05017 	 0.04644 	 ~...
    1 	    51 	 0.04286 	 0.04693 	 ~...
   40 	    52 	 0.04849 	 0.04694 	 ~...
   49 	    53 	 0.04932 	 0.04734 	 ~...
   11 	    54 	 0.04604 	 0.04738 	 ~...
    4 	    55 	 0.04480 	 0.04748 	 ~...
    8 	    56 	 0.04556 	 0.04760 	 ~...
   36 	    57 	 0.04837 	 0.04793 	 ~...
   12 	    58 	 0.04642 	 0.04812 	 ~...
   17 	    59 	 0.04723 	 0.04900 	 ~...
   76 	    60 	 0.05156 	 0.04908 	 ~...
   59 	    61 	 0.05012 	 0.04949 	 ~...
   63 	    62 	 0.05028 	 0.05004 	 ~...
   55 	    63 	 0.04948 	 0.05072 	 ~...
   47 	    64 	 0.04922 	 0.05167 	 ~...
   15 	    65 	 0.04664 	 0.05187 	 ~...
   72 	    66 	 0.05086 	 0.05220 	 ~...
   74 	    67 	 0.05120 	 0.05291 	 ~...
   48 	    68 	 0.04928 	 0.05336 	 ~...
   66 	    69 	 0.05039 	 0.05354 	 ~...
   58 	    70 	 0.05012 	 0.05445 	 ~...
   18 	    71 	 0.04726 	 0.05448 	 ~...
   64 	    72 	 0.05032 	 0.05531 	 ~...
   73 	    73 	 0.05089 	 0.05627 	 ~...
   64 	    74 	 0.05032 	 0.05789 	 ~...
   37 	    75 	 0.04842 	 0.05790 	 ~...
   42 	    76 	 0.04889 	 0.05973 	 ~...
   79 	    77 	 0.20535 	 0.17534 	 m..s
   77 	    78 	 0.19280 	 0.17873 	 ~...
   78 	    79 	 0.19545 	 0.18210 	 ~...
   81 	    80 	 0.21364 	 0.20450 	 ~...
   80 	    81 	 0.21314 	 0.20505 	 ~...
   82 	    82 	 0.21391 	 0.20570 	 ~...
   89 	    83 	 0.21977 	 0.20813 	 ~...
   86 	    84 	 0.21832 	 0.22144 	 ~...
   87 	    85 	 0.21868 	 0.22159 	 ~...
   83 	    86 	 0.21520 	 0.23255 	 ~...
   88 	    87 	 0.21956 	 0.23489 	 ~...
   95 	    88 	 0.27519 	 0.24195 	 m..s
   85 	    89 	 0.21665 	 0.24234 	 ~...
   90 	    90 	 0.25143 	 0.24573 	 ~...
   84 	    91 	 0.21637 	 0.25616 	 m..s
  100 	    92 	 0.27929 	 0.26028 	 ~...
   98 	    93 	 0.27802 	 0.26747 	 ~...
  103 	    94 	 0.28204 	 0.26867 	 ~...
  104 	    95 	 0.28237 	 0.26911 	 ~...
  102 	    96 	 0.28167 	 0.26958 	 ~...
   92 	    97 	 0.27179 	 0.27075 	 ~...
   93 	    98 	 0.27362 	 0.27126 	 ~...
   97 	    99 	 0.27546 	 0.27220 	 ~...
   96 	   100 	 0.27529 	 0.27795 	 ~...
  108 	   101 	 0.28499 	 0.28089 	 ~...
  101 	   102 	 0.27974 	 0.28228 	 ~...
   91 	   103 	 0.25880 	 0.28336 	 ~...
  108 	   104 	 0.28499 	 0.28547 	 ~...
   94 	   105 	 0.27502 	 0.28704 	 ~...
  110 	   106 	 0.28773 	 0.29129 	 ~...
  107 	   107 	 0.28452 	 0.29412 	 ~...
  106 	   108 	 0.28429 	 0.29543 	 ~...
  111 	   109 	 0.28822 	 0.29840 	 ~...
  112 	   110 	 0.32564 	 0.30463 	 ~...
  113 	   111 	 0.33192 	 0.31102 	 ~...
  116 	   112 	 0.35195 	 0.32126 	 m..s
  120 	   113 	 0.37068 	 0.32456 	 m..s
   99 	   114 	 0.27847 	 0.32651 	 m..s
  117 	   115 	 0.35619 	 0.33235 	 ~...
  118 	   116 	 0.35944 	 0.33250 	 ~...
  105 	   117 	 0.28316 	 0.33317 	 m..s
  119 	   118 	 0.35971 	 0.33373 	 ~...
  114 	   119 	 0.34543 	 0.33701 	 ~...
  114 	   120 	 0.34543 	 0.35205 	 ~...
==========================================
r_mrr = 0.9935733079910278
r2_mrr = 0.986573338508606
spearmanr_mrr@5 = 0.9416431784629822
spearmanr_mrr@10 = 0.9170721173286438
spearmanr_mrr@50 = 0.9900817275047302
spearmanr_mrr@100 = 0.9970840215682983
spearmanr_mrr@All = 0.99741131067276
==========================================
test time: 0.391
Done Testing dataset Kinships
total time taken: 189.33375191688538
training time taken: 181.70004987716675
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9936)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9866)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9416)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9171)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9901)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9971)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9974)}}, 'test_loss': {'TransE': {'Kinships': 0.19947614569628058}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 361029881349393
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [707, 613, 346, 664, 275, 657, 494, 813, 854, 1002, 552, 768, 885, 880, 434, 1062, 241, 1208, 121, 326, 772, 670, 962, 1182, 610, 624, 246, 1128, 353, 143, 738, 394, 558, 294, 176, 928, 1173, 888, 1109, 695, 411, 287, 537, 534, 719, 774, 1194, 1181, 256, 1000, 324, 487, 490, 953, 614, 510, 1185, 741, 12, 315, 43, 999, 119, 977, 312, 1151, 731, 1123, 302, 421, 548, 565, 580, 97, 730, 1057, 1207, 356, 110, 297, 536, 1069, 128, 775, 933, 420, 960, 301, 1096, 67, 833, 42, 277, 770, 157, 997, 828, 0, 892, 966, 656, 209, 156, 117, 369, 41, 239, 799, 1214, 1120, 9, 336, 561, 396, 1188, 1192, 599, 713, 520, 115, 460]
valid_ids (0): []
train_ids (1094): [89, 964, 437, 823, 107, 740, 556, 637, 788, 673, 846, 1099, 581, 518, 563, 442, 243, 1138, 877, 782, 1122, 1102, 20, 79, 1176, 427, 455, 908, 433, 108, 529, 562, 1124, 945, 866, 191, 881, 967, 88, 549, 444, 391, 845, 930, 8, 389, 824, 469, 559, 943, 347, 28, 650, 340, 293, 378, 635, 1061, 501, 1133, 475, 141, 778, 30, 1141, 408, 78, 517, 667, 284, 451, 450, 797, 1139, 644, 808, 162, 919, 690, 53, 173, 764, 926, 441, 271, 37, 607, 900, 158, 1156, 446, 697, 393, 986, 418, 39, 958, 1183, 1058, 848, 48, 811, 619, 55, 682, 899, 1082, 723, 1144, 186, 10, 230, 329, 934, 991, 995, 989, 202, 1121, 497, 478, 757, 283, 579, 684, 604, 820, 70, 605, 921, 413, 99, 814, 27, 1149, 780, 1046, 2, 118, 163, 319, 998, 608, 338, 449, 929, 876, 164, 412, 365, 803, 832, 270, 791, 793, 334, 868, 840, 879, 603, 102, 931, 678, 318, 1167, 722, 519, 133, 189, 134, 498, 373, 542, 474, 183, 153, 901, 776, 233, 816, 728, 858, 1084, 950, 992, 197, 33, 1041, 452, 533, 669, 90, 939, 1178, 1190, 1035, 14, 550, 258, 633, 508, 180, 454, 404, 1213, 909, 547, 165, 990, 1093, 1142, 827, 1119, 235, 526, 756, 961, 748, 458, 317, 303, 622, 1037, 61, 703, 335, 746, 292, 266, 225, 821, 431, 345, 954, 860, 267, 101, 686, 376, 1077, 993, 320, 676, 836, 1114, 211, 1177, 212, 568, 727, 1026, 658, 1199, 742, 1168, 996, 1078, 13, 250, 291, 123, 159, 1012, 11, 523, 1022, 514, 589, 1211, 92, 804, 190, 462, 1131, 1028, 111, 1174, 865, 511, 380, 609, 62, 630, 1201, 626, 916, 1075, 841, 896, 600, 257, 1005, 124, 308, 582, 951, 830, 717, 895, 787, 1202, 515, 300, 152, 709, 479, 1148, 863, 354, 109, 538, 1191, 273, 21, 231, 1184, 203, 72, 794, 743, 112, 305, 912, 675, 1029, 735, 440, 1008, 19, 617, 1147, 578, 228, 947, 1076, 710, 1157, 483, 274, 359, 665, 259, 443, 344, 834, 1083, 390, 1193, 486, 321, 1094, 503, 1170, 22, 588, 1016, 927, 1150, 779, 155, 182, 281, 445, 374, 980, 1080, 122, 316, 187, 56, 1063, 295, 894, 627, 146, 666, 923, 789, 463, 724, 1197, 532, 903, 1034, 785, 473, 1111, 1204, 435, 453, 629, 1169, 528, 504, 482, 96, 601, 60, 1198, 1098, 652, 516, 646, 557, 649, 1052, 306, 530, 83, 861, 1068, 426, 1060, 763, 288, 711, 137, 367, 1140, 1160, 1053, 554, 645, 965, 1115, 636, 1049, 181, 265, 974, 149, 310, 975, 366, 795, 248, 1166, 969, 535, 1087, 363, 736, 459, 642, 7, 105, 883, 843, 236, 1200, 429, 681, 737, 148, 428, 130, 219, 1206, 24, 17, 1010, 289, 871, 6, 715, 247, 1006, 151, 1023, 704, 1045, 113, 783, 331, 290, 1054, 539, 80, 1048, 705, 145, 1195, 655, 918, 298, 1205, 174, 1118, 936, 777, 963, 844, 734, 1047, 718, 185, 385, 1073, 269, 956, 643, 84, 870, 1004, 521, 647, 531, 74, 971, 223, 296, 261, 95, 1125, 4, 733, 819, 714, 1051, 77, 612, 253, 34, 278, 886, 69, 216, 911, 1001, 448, 688, 314, 712, 379, 387, 477, 807, 154, 973, 496, 595, 282, 752, 382, 867, 417, 1043, 598, 304, 749, 551, 628, 352, 169, 606, 217, 25, 683, 687, 52, 208, 1009, 75, 1130, 35, 251, 244, 196, 602, 1159, 957, 761, 560, 1107, 696, 940, 691, 555, 1163, 430, 576, 1055, 527, 689, 659, 1189, 1101, 583, 720, 1196, 1038, 160, 342, 567, 1032, 812, 855, 1209, 472, 392, 1153, 424, 869, 227, 835, 898, 221, 415, 822, 1097, 955, 199, 416, 674, 893, 500, 175, 859, 671, 1135, 574, 806, 200, 611, 406, 692, 76, 1027, 663, 1064, 397, 850, 461, 466, 839, 584, 23, 252, 1154, 120, 509, 935, 800, 773, 260, 596, 623, 809, 507, 632, 790, 862, 309, 910, 1020, 47, 932, 218, 238, 395, 481, 1108, 220, 214, 874, 132, 360, 201, 699, 470, 51, 802, 31, 349, 1039, 1155, 750, 1030, 661, 760, 480, 204, 1050, 229, 762, 126, 144, 313, 100, 409, 82, 668, 784, 988, 402, 1165, 586, 591, 268, 44, 371, 592, 543, 985, 73, 525, 907, 16, 195, 1013, 616, 81, 1106, 179, 457, 884, 328, 249, 36, 1100, 725, 272, 26, 1031, 577, 357, 590, 765, 685, 29, 491, 15, 341, 398, 362, 285, 45, 91, 593, 815, 887, 938, 255, 242, 1042, 597, 585, 370, 546, 873, 286, 837, 753, 38, 401, 438, 693, 553, 142, 838, 1074, 98, 66, 135, 825, 767, 1014, 1044, 541, 226, 698, 167, 63, 1180, 18, 1024, 330, 245, 1158, 136, 372, 410, 856, 116, 193, 1015, 93, 805, 213, 58, 1210, 68, 348, 172, 403, 240, 323, 651, 1017, 745, 299, 114, 170, 1127, 864, 388, 587, 327, 1136, 489, 983, 754, 1091, 1011, 944, 755, 361, 618, 184, 522, 234, 64, 801, 215, 276, 857, 981, 810, 355, 54, 85, 1164, 224, 50, 575, 471, 672, 680, 512, 467, 513, 891, 1070, 339, 842, 702, 524, 631, 59, 1066, 653, 1088, 968, 332, 206, 572, 648, 492, 726, 638, 786, 732, 634, 979, 5, 131, 982, 906, 280, 263, 1126, 882, 1137, 1212, 1081, 493, 177, 381, 758, 660, 1105, 976, 506, 1089, 615, 818, 1161, 166, 701, 1085, 890, 1145, 484, 364, 694, 739, 468, 350, 205, 545, 677, 566, 729, 1112, 621, 1175, 759, 407, 937, 502, 540, 207, 987, 654, 942, 386, 358, 897, 625, 140, 914, 423, 1056, 351, 679, 1092, 798, 399, 569, 949, 1067, 700, 913, 46, 639, 829, 708, 383, 1007, 1186, 125, 924, 237, 311, 464, 1203, 564, 769, 1079, 1110, 831, 781, 904, 662, 432, 878, 792, 279, 422, 570, 232, 849, 439, 465, 1019, 1146, 1, 1072, 852, 1021, 161, 377, 87, 127, 922, 1090, 817, 178, 325, 984, 796, 766, 71, 436, 1187, 495, 889, 400, 1025, 1103, 1040, 447, 139, 905, 1018, 505, 49, 959, 194, 1171, 333, 826, 1132, 948, 188, 192, 307, 456, 262, 641, 171, 952, 476, 1065, 594, 1129, 499, 57, 264, 368, 222, 322, 198, 1162, 104, 721, 1036, 337, 1003, 419, 853, 872, 384, 970, 343, 751, 978, 129, 425, 375, 847, 1104, 485, 972, 210, 488, 1179, 620, 925, 716, 902, 1059, 1143, 744, 405, 414, 1116, 103, 706, 40, 1152, 86, 544, 1134, 915, 1117, 994, 573, 1071, 941, 640, 747, 1113, 1086, 946, 168, 920, 571, 65, 917, 254, 94, 851, 1172, 147, 32, 771, 106, 1095, 138, 875, 1033, 150, 3]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8561331982638825
the save name prefix for this run is:  chkpt-ID_8561331982638825_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 802
rank avg (pred): 0.561 +- 0.006
mrr vals (pred, true): 0.017, 0.042
batch losses (mrrl, rdl): 0.0, 0.0002452667

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 307
rank avg (pred): 0.115 +- 0.117
mrr vals (pred, true): 0.177, 0.273
batch losses (mrrl, rdl): 0.0, 2.3596e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 14
rank avg (pred): 0.118 +- 0.142
mrr vals (pred, true): 0.192, 0.279
batch losses (mrrl, rdl): 0.0, 6.386e-07

Epoch over!
epoch time: 12.001

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 706
rank avg (pred): 0.458 +- 0.264
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0, 6.547e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 357
rank avg (pred): 0.455 +- 0.260
mrr vals (pred, true): 0.041, 0.047
batch losses (mrrl, rdl): 0.0, 4.6154e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 145
rank avg (pred): 0.459 +- 0.265
mrr vals (pred, true): 0.043, 0.052
batch losses (mrrl, rdl): 0.0, 2.61312e-05

Epoch over!
epoch time: 11.933

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 611
rank avg (pred): 0.425 +- 0.262
mrr vals (pred, true): 0.054, 0.049
batch losses (mrrl, rdl): 0.0, 1.28853e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 572
rank avg (pred): 0.450 +- 0.266
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 0.0, 4.5965e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 454
rank avg (pred): 0.438 +- 0.267
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 0.0, 1.76174e-05

Epoch over!
epoch time: 11.795

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 389
rank avg (pred): 0.427 +- 0.264
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0, 3.164e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 765
rank avg (pred): 0.455 +- 0.268
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 0.0, 2.218e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 470
rank avg (pred): 0.443 +- 0.265
mrr vals (pred, true): 0.052, 0.040
batch losses (mrrl, rdl): 0.0, 2.18804e-05

Epoch over!
epoch time: 11.79

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 14
rank avg (pred): 0.146 +- 0.170
mrr vals (pred, true): 0.154, 0.279
batch losses (mrrl, rdl): 0.0, 1.98397e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 496
rank avg (pred): 0.119 +- 0.143
mrr vals (pred, true): 0.184, 0.206
batch losses (mrrl, rdl): 0.0, 4.032e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 567
rank avg (pred): 0.472 +- 0.265
mrr vals (pred, true): 0.043, 0.046
batch losses (mrrl, rdl): 0.0, 1.9272e-06

Epoch over!
epoch time: 11.762

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1020
rank avg (pred): 0.421 +- 0.269
mrr vals (pred, true): 0.065, 0.057
batch losses (mrrl, rdl): 0.0021580488, 3.266e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 681
rank avg (pred): 0.473 +- 0.255
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 8.3687e-06, 2.6313e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 989
rank avg (pred): 0.095 +- 0.129
mrr vals (pred, true): 0.307, 0.342
batch losses (mrrl, rdl): 0.0128141027, 1.557e-07

Epoch over!
epoch time: 12.231

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 850
rank avg (pred): 0.461 +- 0.246
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 2.22389e-05, 1.4101e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 765
rank avg (pred): 0.450 +- 0.234
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 9.2583e-06, 5.0562e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 863
rank avg (pred): 0.470 +- 0.250
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 2.3552e-05, 1.66729e-05

Epoch over!
epoch time: 12.098

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 341
rank avg (pred): 0.447 +- 0.242
mrr vals (pred, true): 0.055, 0.060
batch losses (mrrl, rdl): 0.0002357131, 4.01095e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 512
rank avg (pred): 0.171 +- 0.173
mrr vals (pred, true): 0.243, 0.236
batch losses (mrrl, rdl): 0.0003779235, 7.1349e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1018
rank avg (pred): 0.414 +- 0.208
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0004711705, 3.76813e-05

Epoch over!
epoch time: 12.191

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1197
rank avg (pred): 0.470 +- 0.224
mrr vals (pred, true): 0.038, 0.047
batch losses (mrrl, rdl): 0.0014708908, 1.06153e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 596
rank avg (pred): 0.486 +- 0.245
mrr vals (pred, true): 0.038, 0.046
batch losses (mrrl, rdl): 0.0013415305, 2.35565e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 445
rank avg (pred): 0.474 +- 0.246
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0004845561, 2.4768e-06

Epoch over!
epoch time: 12.182

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 75
rank avg (pred): 0.163 +- 0.175
mrr vals (pred, true): 0.280, 0.301
batch losses (mrrl, rdl): 0.0043071406, 0.0001430371

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 687
rank avg (pred): 0.482 +- 0.263
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0001359272, 1.5092e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 554
rank avg (pred): 0.210 +- 0.183
mrr vals (pred, true): 0.218, 0.236
batch losses (mrrl, rdl): 0.0034661228, 0.0001791966

Epoch over!
epoch time: 12.073

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 281
rank avg (pred): 0.162 +- 0.176
mrr vals (pred, true): 0.295, 0.285
batch losses (mrrl, rdl): 0.0009574616, 9.37967e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 957
rank avg (pred): 0.475 +- 0.256
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.000369133, 8.586e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 652
rank avg (pred): 0.477 +- 0.260
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.000347443, 1.2852e-06

Epoch over!
epoch time: 12.512

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 18
rank avg (pred): 0.176 +- 0.180
mrr vals (pred, true): 0.277, 0.280
batch losses (mrrl, rdl): 8.40467e-05, 0.000201218

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 864
rank avg (pred): 0.464 +- 0.251
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 7.32007e-05, 2.6084e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 116
rank avg (pred): 0.450 +- 0.257
mrr vals (pred, true): 0.052, 0.057
batch losses (mrrl, rdl): 5.07712e-05, 1.80061e-05

Epoch over!
epoch time: 12.014

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 505
rank avg (pred): 0.221 +- 0.181
mrr vals (pred, true): 0.212, 0.204
batch losses (mrrl, rdl): 0.0005974817, 0.000291221

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 100
rank avg (pred): 0.455 +- 0.241
mrr vals (pred, true): 0.044, 0.053
batch losses (mrrl, rdl): 0.0003891825, 1.21514e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 513
rank avg (pred): 0.204 +- 0.178
mrr vals (pred, true): 0.236, 0.211
batch losses (mrrl, rdl): 0.0064293928, 0.0002021336

Epoch over!
epoch time: 12.29

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 357
rank avg (pred): 0.498 +- 0.272
mrr vals (pred, true): 0.038, 0.047
batch losses (mrrl, rdl): 0.0014382745, 4.18946e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 819
rank avg (pred): 0.208 +- 0.177
mrr vals (pred, true): 0.234, 0.182
batch losses (mrrl, rdl): 0.0270561669, 9.8368e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1160
rank avg (pred): 0.195 +- 0.177
mrr vals (pred, true): 0.271, 0.281
batch losses (mrrl, rdl): 0.0009378383, 0.0002310589

Epoch over!
epoch time: 12.019

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 827
rank avg (pred): 0.210 +- 0.174
mrr vals (pred, true): 0.229, 0.277
batch losses (mrrl, rdl): 0.0232702326, 0.000173913

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 56
rank avg (pred): 0.188 +- 0.176
mrr vals (pred, true): 0.258, 0.286
batch losses (mrrl, rdl): 0.0077310316, 0.0001611038

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 141
rank avg (pred): 0.451 +- 0.255
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 9.286e-07, 1.3309e-06

Epoch over!
epoch time: 11.979

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.454 +- 0.272
mrr vals (pred, true): 0.058, 0.047

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05686 	 0.04133 	 ~...
    0 	     1 	 0.05686 	 0.04184 	 ~...
   73 	     2 	 0.06457 	 0.04209 	 ~...
    0 	     3 	 0.05686 	 0.04215 	 ~...
   47 	     4 	 0.05693 	 0.04223 	 ~...
   71 	     5 	 0.06063 	 0.04224 	 ~...
   55 	     6 	 0.05782 	 0.04225 	 ~...
    0 	     7 	 0.05686 	 0.04257 	 ~...
    0 	     8 	 0.05686 	 0.04269 	 ~...
   72 	     9 	 0.06426 	 0.04274 	 ~...
    0 	    10 	 0.05686 	 0.04298 	 ~...
   49 	    11 	 0.05748 	 0.04303 	 ~...
    0 	    12 	 0.05686 	 0.04356 	 ~...
    0 	    13 	 0.05686 	 0.04379 	 ~...
   55 	    14 	 0.05782 	 0.04382 	 ~...
   63 	    15 	 0.05903 	 0.04384 	 ~...
    0 	    16 	 0.05686 	 0.04387 	 ~...
    0 	    17 	 0.05686 	 0.04389 	 ~...
   54 	    18 	 0.05780 	 0.04399 	 ~...
    0 	    19 	 0.05686 	 0.04405 	 ~...
    0 	    20 	 0.05686 	 0.04407 	 ~...
    0 	    21 	 0.05686 	 0.04414 	 ~...
    0 	    22 	 0.05686 	 0.04415 	 ~...
    0 	    23 	 0.05686 	 0.04426 	 ~...
   52 	    24 	 0.05766 	 0.04444 	 ~...
    0 	    25 	 0.05686 	 0.04463 	 ~...
    0 	    26 	 0.05686 	 0.04463 	 ~...
   48 	    27 	 0.05701 	 0.04475 	 ~...
   73 	    28 	 0.06457 	 0.04475 	 ~...
   64 	    29 	 0.05919 	 0.04485 	 ~...
    0 	    30 	 0.05686 	 0.04486 	 ~...
    0 	    31 	 0.05686 	 0.04487 	 ~...
   67 	    32 	 0.05943 	 0.04498 	 ~...
   60 	    33 	 0.05819 	 0.04500 	 ~...
    0 	    34 	 0.05686 	 0.04509 	 ~...
    0 	    35 	 0.05686 	 0.04516 	 ~...
    0 	    36 	 0.05686 	 0.04519 	 ~...
    0 	    37 	 0.05686 	 0.04523 	 ~...
    0 	    38 	 0.05686 	 0.04542 	 ~...
    0 	    39 	 0.05686 	 0.04542 	 ~...
    0 	    40 	 0.05686 	 0.04546 	 ~...
    0 	    41 	 0.05686 	 0.04584 	 ~...
    0 	    42 	 0.05686 	 0.04598 	 ~...
    0 	    43 	 0.05686 	 0.04607 	 ~...
   64 	    44 	 0.05919 	 0.04609 	 ~...
   66 	    45 	 0.05921 	 0.04628 	 ~...
   53 	    46 	 0.05776 	 0.04629 	 ~...
   50 	    47 	 0.05752 	 0.04631 	 ~...
    0 	    48 	 0.05686 	 0.04635 	 ~...
   51 	    49 	 0.05754 	 0.04653 	 ~...
    0 	    50 	 0.05686 	 0.04704 	 ~...
    0 	    51 	 0.05686 	 0.04741 	 ~...
    0 	    52 	 0.05686 	 0.04813 	 ~...
   69 	    53 	 0.06038 	 0.04852 	 ~...
   69 	    54 	 0.06038 	 0.04860 	 ~...
    0 	    55 	 0.05686 	 0.04915 	 ~...
    0 	    56 	 0.05686 	 0.04934 	 ~...
   57 	    57 	 0.05801 	 0.04935 	 ~...
   61 	    58 	 0.05834 	 0.04984 	 ~...
    0 	    59 	 0.05686 	 0.05008 	 ~...
    0 	    60 	 0.05686 	 0.05019 	 ~...
    0 	    61 	 0.05686 	 0.05077 	 ~...
    0 	    62 	 0.05686 	 0.05126 	 ~...
   59 	    63 	 0.05818 	 0.05171 	 ~...
    0 	    64 	 0.05686 	 0.05199 	 ~...
   58 	    65 	 0.05808 	 0.05245 	 ~...
   62 	    66 	 0.05850 	 0.05295 	 ~...
    0 	    67 	 0.05686 	 0.05389 	 ~...
    0 	    68 	 0.05686 	 0.05392 	 ~...
    0 	    69 	 0.05686 	 0.05402 	 ~...
    0 	    70 	 0.05686 	 0.05444 	 ~...
    0 	    71 	 0.05686 	 0.05513 	 ~...
   75 	    72 	 0.06631 	 0.05531 	 ~...
   68 	    73 	 0.06020 	 0.05566 	 ~...
    0 	    74 	 0.05686 	 0.05666 	 ~...
    0 	    75 	 0.05686 	 0.05728 	 ~...
   76 	    76 	 0.17897 	 0.13306 	 m..s
   78 	    77 	 0.22135 	 0.17582 	 m..s
   77 	    78 	 0.21923 	 0.17681 	 m..s
   92 	    79 	 0.24410 	 0.17772 	 m..s
   95 	    80 	 0.25292 	 0.19222 	 m..s
   88 	    81 	 0.24169 	 0.19225 	 m..s
   89 	    82 	 0.24189 	 0.19965 	 m..s
   87 	    83 	 0.24164 	 0.20108 	 m..s
   90 	    84 	 0.24201 	 0.20450 	 m..s
   84 	    85 	 0.24020 	 0.21326 	 ~...
   86 	    86 	 0.24163 	 0.21918 	 ~...
   82 	    87 	 0.23903 	 0.22418 	 ~...
   79 	    88 	 0.22561 	 0.22530 	 ~...
   80 	    89 	 0.23853 	 0.22898 	 ~...
   81 	    90 	 0.23878 	 0.22913 	 ~...
   94 	    91 	 0.24461 	 0.23010 	 ~...
   91 	    92 	 0.24283 	 0.23100 	 ~...
   82 	    93 	 0.23903 	 0.24035 	 ~...
   85 	    94 	 0.24029 	 0.24077 	 ~...
   96 	    95 	 0.25508 	 0.24660 	 ~...
   92 	    96 	 0.24410 	 0.24908 	 ~...
  102 	    97 	 0.29446 	 0.26278 	 m..s
  108 	    98 	 0.29652 	 0.26958 	 ~...
   99 	    99 	 0.28840 	 0.27037 	 ~...
  103 	   100 	 0.29489 	 0.27075 	 ~...
  105 	   101 	 0.29594 	 0.27120 	 ~...
  114 	   102 	 0.30140 	 0.27454 	 ~...
  103 	   103 	 0.29489 	 0.27591 	 ~...
   97 	   104 	 0.27851 	 0.27845 	 ~...
  101 	   105 	 0.28984 	 0.27906 	 ~...
  100 	   106 	 0.28871 	 0.28054 	 ~...
  107 	   107 	 0.29636 	 0.28517 	 ~...
  113 	   108 	 0.30111 	 0.28527 	 ~...
  110 	   109 	 0.29794 	 0.28964 	 ~...
  112 	   110 	 0.29965 	 0.29262 	 ~...
   98 	   111 	 0.28706 	 0.29300 	 ~...
  115 	   112 	 0.30214 	 0.29520 	 ~...
  109 	   113 	 0.29665 	 0.29840 	 ~...
  111 	   114 	 0.29936 	 0.30411 	 ~...
  119 	   115 	 0.34626 	 0.30463 	 m..s
  105 	   116 	 0.29594 	 0.31588 	 ~...
  116 	   117 	 0.33948 	 0.33103 	 ~...
  117 	   118 	 0.34417 	 0.34644 	 ~...
  118 	   119 	 0.34606 	 0.35185 	 ~...
  120 	   120 	 0.35104 	 0.35540 	 ~...
==========================================
r_mrr = 0.9928363561630249
r2_mrr = 0.9692537784576416
spearmanr_mrr@5 = 0.9303969740867615
spearmanr_mrr@10 = 0.929739236831665
spearmanr_mrr@50 = 0.9826109409332275
spearmanr_mrr@100 = 0.9952488541603088
spearmanr_mrr@All = 0.9958065748214722
==========================================
test time: 0.4
Done Testing dataset Kinships
total time taken: 188.4500002861023
training time taken: 181.34760665893555
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9928)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9693)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9304)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9297)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9826)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9952)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9958)}}, 'test_loss': {'TransE': {'Kinships': 0.3663319174411299}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 4375677851112455
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [766, 693, 118, 763, 911, 256, 964, 471, 1144, 803, 167, 137, 1209, 470, 609, 392, 613, 337, 1192, 238, 1107, 413, 204, 14, 1012, 788, 926, 1203, 274, 243, 192, 543, 946, 93, 621, 320, 864, 229, 833, 450, 524, 90, 948, 1199, 25, 1128, 91, 82, 252, 399, 1156, 357, 684, 67, 520, 319, 504, 844, 205, 460, 43, 1173, 949, 15, 677, 1093, 87, 745, 1179, 944, 170, 221, 950, 690, 233, 922, 441, 1070, 861, 1024, 440, 588, 374, 388, 415, 899, 641, 919, 521, 531, 110, 626, 900, 152, 642, 1044, 1150, 1050, 532, 558, 29, 1076, 659, 390, 97, 294, 56, 369, 828, 222, 8, 863, 1029, 234, 1153, 1064, 420, 115, 161, 476, 187]
valid_ids (0): []
train_ids (1094): [489, 116, 757, 462, 630, 910, 160, 789, 815, 13, 829, 315, 723, 683, 493, 191, 977, 1102, 992, 33, 104, 638, 831, 625, 1043, 172, 548, 434, 920, 1084, 656, 624, 77, 184, 74, 954, 961, 1213, 96, 571, 179, 506, 726, 879, 345, 32, 376, 658, 366, 296, 819, 542, 249, 1014, 875, 494, 153, 704, 851, 469, 27, 248, 1101, 417, 1211, 368, 26, 426, 1187, 837, 63, 1138, 956, 564, 724, 279, 147, 986, 303, 173, 485, 232, 46, 528, 208, 589, 181, 34, 874, 502, 927, 406, 79, 696, 50, 397, 869, 212, 935, 411, 144, 557, 1158, 0, 583, 446, 496, 1113, 495, 422, 44, 353, 1178, 908, 1034, 577, 231, 714, 99, 546, 673, 442, 1124, 391, 481, 563, 759, 663, 322, 982, 674, 1066, 362, 482, 53, 1004, 611, 628, 1016, 281, 377, 686, 227, 605, 891, 796, 1106, 744, 890, 808, 746, 848, 1055, 1205, 1017, 769, 710, 240, 403, 459, 916, 1176, 511, 360, 801, 1167, 1127, 750, 645, 1114, 551, 523, 675, 235, 777, 62, 270, 966, 479, 258, 639, 340, 824, 1149, 1202, 794, 351, 226, 718, 717, 1069, 720, 338, 687, 1189, 994, 145, 318, 702, 1090, 293, 995, 291, 58, 335, 1036, 45, 664, 66, 509, 568, 594, 703, 164, 384, 804, 467, 188, 355, 484, 739, 151, 847, 1117, 530, 514, 540, 1032, 218, 1181, 572, 533, 267, 885, 845, 122, 522, 197, 271, 1091, 253, 40, 342, 990, 195, 70, 387, 933, 76, 826, 289, 1033, 807, 1077, 1078, 903, 92, 461, 963, 284, 883, 1135, 297, 955, 1056, 945, 786, 823, 725, 858, 358, 535, 402, 134, 456, 250, 644, 1162, 449, 872, 1061, 326, 298, 17, 778, 57, 756, 21, 1018, 758, 580, 1147, 119, 660, 359, 854, 1040, 740, 55, 816, 817, 574, 1104, 860, 443, 925, 380, 1155, 216, 1054, 830, 220, 373, 561, 1109, 131, 1201, 1180, 203, 246, 810, 813, 1120, 892, 190, 427, 913, 1129, 12, 356, 841, 873, 593, 38, 219, 1152, 775, 1208, 202, 343, 884, 898, 47, 616, 587, 214, 404, 738, 1049, 959, 431, 19, 239, 755, 1000, 194, 1099, 301, 870, 310, 51, 676, 478, 283, 378, 230, 20, 1086, 1140, 1125, 410, 261, 632, 562, 1068, 1072, 748, 727, 1087, 1142, 712, 16, 513, 897, 1134, 1067, 608, 61, 312, 454, 1133, 553, 189, 398, 780, 527, 407, 773, 277, 555, 1071, 163, 1214, 401, 695, 706, 975, 694, 464, 1042, 365, 1110, 701, 762, 272, 567, 128, 818, 886, 1059, 856, 602, 811, 41, 610, 855, 107, 790, 490, 412, 1053, 405, 1019, 425, 1011, 947, 1141, 1168, 306, 6, 73, 688, 1123, 1, 336, 1210, 843, 1145, 953, 893, 499, 1166, 505, 581, 154, 123, 765, 809, 988, 680, 483, 1185, 622, 103, 1163, 466, 767, 734, 924, 868, 985, 732, 211, 1108, 733, 722, 1175, 165, 1160, 1206, 619, 106, 223, 741, 938, 849, 965, 242, 797, 941, 292, 199, 772, 132, 1157, 10, 127, 1001, 3, 595, 64, 1008, 1174, 550, 286, 636, 997, 1028, 1082, 547, 236, 314, 751, 842, 503, 175, 308, 182, 168, 633, 501, 1080, 263, 1195, 447, 853, 171, 709, 876, 95, 678, 1085, 381, 882, 1021, 386, 1146, 350, 515, 1052, 278, 918, 24, 259, 1047, 54, 200, 812, 105, 715, 670, 692, 668, 598, 582, 288, 993, 614, 244, 371, 299, 795, 254, 917, 112, 429, 266, 325, 183, 480, 48, 661, 928, 984, 685, 970, 423, 1184, 643, 549, 921, 649, 554, 782, 363, 1046, 1062, 737, 241, 474, 1188, 419, 237, 518, 1172, 1030, 108, 80, 42, 539, 75, 156, 1015, 1005, 68, 367, 541, 498, 689, 681, 721, 951, 418, 881, 409, 859, 455, 1137, 573, 9, 749, 623, 979, 383, 1190, 565, 344, 618, 408, 37, 719, 120, 827, 331, 1165, 300, 705, 143, 81, 617, 937, 389, 545, 177, 1177, 525, 421, 822, 586, 436, 728, 468, 743, 1026, 1207, 1079, 771, 711, 334, 1009, 761, 650, 973, 430, 193, 850, 129, 433, 1139, 840, 109, 754, 1048, 287, 372, 862, 89, 747, 393, 671, 1027, 396, 457, 1023, 285, 1025, 987, 814, 328, 902, 983, 329, 30, 176, 1170, 1031, 836, 1081, 534, 1045, 1111, 735, 126, 615, 834, 1075, 996, 731, 787, 276, 140, 437, 1089, 121, 569, 206, 1197, 472, 1094, 634, 185, 85, 273, 475, 354, 764, 1041, 867, 395, 969, 888, 364, 575, 217, 280, 665, 84, 71, 1095, 11, 631, 566, 215, 497, 1010, 784, 257, 774, 930, 7, 321, 957, 444, 800, 166, 915, 894, 22, 646, 652, 559, 976, 117, 225, 904, 1060, 648, 114, 83, 486, 439, 640, 552, 1191, 835, 31, 150, 463, 180, 1171, 607, 453, 962, 1154, 667, 700, 228, 1151, 72, 793, 591, 932, 972, 906, 912, 544, 346, 1115, 538, 247, 414, 1051, 1148, 1013, 201, 999, 260, 556, 139, 779, 776, 465, 596, 1161, 302, 579, 989, 560, 60, 488, 852, 791, 599, 698, 901, 1169, 317, 923, 783, 1037, 458, 1200, 473, 432, 36, 451, 162, 305, 785, 135, 939, 974, 752, 500, 174, 931, 313, 304, 604, 59, 1002, 895, 347, 39, 4, 207, 736, 213, 662, 603, 1121, 1204, 1182, 1196, 512, 600, 936, 635, 657, 1022, 86, 1136, 1007, 295, 590, 576, 877, 1100, 5, 282, 1198, 730, 379, 536, 978, 169, 487, 210, 352, 1088, 49, 341, 492, 517, 1035, 424, 1112, 529, 111, 307, 316, 245, 196, 713, 88, 510, 805, 375, 654, 349, 339, 35, 52, 1131, 914, 729, 311, 265, 1186, 753, 78, 142, 760, 209, 871, 971, 124, 940, 1116, 255, 846, 130, 102, 672, 98, 627, 133, 157, 100, 1126, 125, 839, 998, 620, 155, 251, 880, 23, 323, 327, 943, 991, 825, 348, 1130, 1098, 798, 178, 1183, 275, 262, 647, 802, 1003, 428, 1083, 1057, 448, 697, 1063, 101, 332, 186, 578, 768, 361, 968, 149, 682, 1159, 400, 1038, 585, 929, 597, 94, 669, 1122, 1006, 1074, 435, 1058, 570, 1096, 896, 1118, 1193, 980, 1039, 18, 1073, 1020, 1105, 832, 507, 148, 1065, 141, 519, 28, 651, 438, 820, 821, 606, 909, 477, 333, 416, 445, 792, 136, 981, 65, 866, 1164, 907, 770, 1194, 592, 264, 865, 290, 691, 526, 370, 806, 146, 699, 781, 666, 269, 878, 508, 934, 708, 838, 516, 1103, 612, 491, 679, 452, 742, 158, 537, 159, 138, 113, 889, 653, 637, 707, 1092, 584, 655, 601, 960, 324, 2, 952, 716, 198, 958, 905, 1097, 394, 385, 1119, 309, 1132, 857, 967, 1212, 799, 224, 629, 1143, 69, 382, 268, 887, 330, 942]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3655748739255678
the save name prefix for this run is:  chkpt-ID_3655748739255678_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 13
rank avg (pred): 0.405 +- 0.008
mrr vals (pred, true): 0.023, 0.255
batch losses (mrrl, rdl): 0.0, 0.0018404572

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 416
rank avg (pred): 0.440 +- 0.017
mrr vals (pred, true): 0.022, 0.044
batch losses (mrrl, rdl): 0.0, 0.000122343

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1151
rank avg (pred): 0.098 +- 0.132
mrr vals (pred, true): 0.211, 0.278
batch losses (mrrl, rdl): 0.0, 6.933e-07

Epoch over!
epoch time: 12.141

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 596
rank avg (pred): 0.468 +- 0.252
mrr vals (pred, true): 0.030, 0.046
batch losses (mrrl, rdl): 0.0, 3.2475e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 750
rank avg (pred): 0.201 +- 0.196
mrr vals (pred, true): 0.114, 0.235
batch losses (mrrl, rdl): 0.0, 3.00825e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 182
rank avg (pred): 0.451 +- 0.268
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0, 1.9697e-06

Epoch over!
epoch time: 11.887

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 372
rank avg (pred): 0.452 +- 0.264
mrr vals (pred, true): 0.041, 0.049
batch losses (mrrl, rdl): 0.0, 2.7905e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 449
rank avg (pred): 0.450 +- 0.262
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0, 3.3028e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 331
rank avg (pred): 0.449 +- 0.267
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 0.0, 2.9229e-06

Epoch over!
epoch time: 11.919

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1136
rank avg (pred): 0.125 +- 0.149
mrr vals (pred, true): 0.188, 0.245
batch losses (mrrl, rdl): 0.0, 5.2782e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 410
rank avg (pred): 0.444 +- 0.276
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0, 5.6406e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 272
rank avg (pred): 0.110 +- 0.141
mrr vals (pred, true): 0.201, 0.287
batch losses (mrrl, rdl): 0.0, 2.2302e-06

Epoch over!
epoch time: 11.998

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 826
rank avg (pred): 0.126 +- 0.151
mrr vals (pred, true): 0.186, 0.279
batch losses (mrrl, rdl): 0.0, 1.0785e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 561
rank avg (pred): 0.112 +- 0.140
mrr vals (pred, true): 0.207, 0.224
batch losses (mrrl, rdl): 0.0, 1.4443e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 302
rank avg (pred): 0.095 +- 0.114
mrr vals (pred, true): 0.230, 0.293
batch losses (mrrl, rdl): 0.0, 3.3742e-06

Epoch over!
epoch time: 11.897

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 395
rank avg (pred): 0.428 +- 0.269
mrr vals (pred, true): 0.068, 0.058
batch losses (mrrl, rdl): 0.0033018421, 1.19602e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 547
rank avg (pred): 0.146 +- 0.116
mrr vals (pred, true): 0.213, 0.184
batch losses (mrrl, rdl): 0.0081667565, 1.12895e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 708
rank avg (pred): 0.471 +- 0.220
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 1.681e-06, 7.8755e-06

Epoch over!
epoch time: 12.382

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1091
rank avg (pred): 0.422 +- 0.195
mrr vals (pred, true): 0.055, 0.052
batch losses (mrrl, rdl): 0.0002695764, 1.85189e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 406
rank avg (pred): 0.464 +- 0.219
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 3.7957e-06, 1.29188e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 345
rank avg (pred): 0.436 +- 0.205
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 8.41558e-05, 1.44906e-05

Epoch over!
epoch time: 12.224

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1148
rank avg (pred): 0.116 +- 0.112
mrr vals (pred, true): 0.264, 0.266
batch losses (mrrl, rdl): 5.50348e-05, 1.8954e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 643
rank avg (pred): 0.489 +- 0.235
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.0002428522, 1.11928e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 203
rank avg (pred): 0.459 +- 0.223
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001320039, 1.0636e-05

Epoch over!
epoch time: 12.07

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 257
rank avg (pred): 0.120 +- 0.126
mrr vals (pred, true): 0.272, 0.305
batch losses (mrrl, rdl): 0.0107910279, 1.2949e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 795
rank avg (pred): 0.468 +- 0.218
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0002235794, 1.33301e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1175
rank avg (pred): 0.497 +- 0.241
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 9.3598e-06, 3.00946e-05

Epoch over!
epoch time: 12.182

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 380
rank avg (pred): 0.450 +- 0.208
mrr vals (pred, true): 0.053, 0.053
batch losses (mrrl, rdl): 7.57696e-05, 1.82286e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 603
rank avg (pred): 0.486 +- 0.232
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 2.54684e-05, 2.3292e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 473
rank avg (pred): 0.494 +- 0.236
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 2.5625e-05, 1.71505e-05

Epoch over!
epoch time: 12.104

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 725
rank avg (pred): 0.457 +- 0.224
mrr vals (pred, true): 0.058, 0.043
batch losses (mrrl, rdl): 0.0005628292, 2.82581e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 428
rank avg (pred): 0.444 +- 0.190
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 6.27902e-05, 4.48542e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 92
rank avg (pred): 0.483 +- 0.241
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 8.5958e-05, 2.2008e-05

Epoch over!
epoch time: 12.26

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 660
rank avg (pred): 0.449 +- 0.210
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.0003319622, 2.40852e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 796
rank avg (pred): 0.489 +- 0.237
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 5.6912e-06, 7.8134e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 103
rank avg (pred): 0.457 +- 0.205
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 2.95461e-05, 3.155e-05

Epoch over!
epoch time: 11.934

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 567
rank avg (pred): 0.435 +- 0.184
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.22819e-05, 4.79152e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 193
rank avg (pred): 0.491 +- 0.230
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001830664, 9.6728e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 313
rank avg (pred): 0.099 +- 0.080
mrr vals (pred, true): 0.295, 0.284
batch losses (mrrl, rdl): 0.0012054883, 1.9127e-06

Epoch over!
epoch time: 12.229

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 438
rank avg (pred): 0.501 +- 0.240
mrr vals (pred, true): 0.048, 0.044
batch losses (mrrl, rdl): 5.79442e-05, 1.13904e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1123
rank avg (pred): 0.383 +- 0.112
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0003969341, 0.0001921925

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1078
rank avg (pred): 0.068 +- 0.054
mrr vals (pred, true): 0.344, 0.330
batch losses (mrrl, rdl): 0.0020748291, 2.5115e-05

Epoch over!
epoch time: 12.223

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 63
rank avg (pred): 0.125 +- 0.103
mrr vals (pred, true): 0.287, 0.297
batch losses (mrrl, rdl): 0.0009391144, 2.67799e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 50
rank avg (pred): 0.131 +- 0.111
mrr vals (pred, true): 0.285, 0.282
batch losses (mrrl, rdl): 0.0001276633, 6.66e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 83
rank avg (pred): 0.522 +- 0.260
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 3.41428e-05, 8.51291e-05

Epoch over!
epoch time: 12.195

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.538 +- 0.264
mrr vals (pred, true): 0.049, 0.047

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   38 	     0 	 0.04665 	 0.03973 	 ~...
    7 	     1 	 0.04452 	 0.04001 	 ~...
   30 	     2 	 0.04604 	 0.04108 	 ~...
   61 	     3 	 0.04893 	 0.04126 	 ~...
   15 	     4 	 0.04497 	 0.04154 	 ~...
   84 	     5 	 0.05176 	 0.04188 	 ~...
   76 	     6 	 0.05008 	 0.04200 	 ~...
   67 	     7 	 0.04950 	 0.04202 	 ~...
   58 	     8 	 0.04849 	 0.04206 	 ~...
   57 	     9 	 0.04847 	 0.04227 	 ~...
   15 	    10 	 0.04497 	 0.04247 	 ~...
   83 	    11 	 0.05124 	 0.04258 	 ~...
   13 	    12 	 0.04489 	 0.04260 	 ~...
   46 	    13 	 0.04720 	 0.04274 	 ~...
    1 	    14 	 0.04278 	 0.04276 	 ~...
    6 	    15 	 0.04450 	 0.04286 	 ~...
   33 	    16 	 0.04626 	 0.04289 	 ~...
   55 	    17 	 0.04812 	 0.04302 	 ~...
   27 	    18 	 0.04573 	 0.04312 	 ~...
   12 	    19 	 0.04487 	 0.04331 	 ~...
   14 	    20 	 0.04493 	 0.04344 	 ~...
    2 	    21 	 0.04285 	 0.04346 	 ~...
   54 	    22 	 0.04784 	 0.04362 	 ~...
   73 	    23 	 0.04981 	 0.04371 	 ~...
    4 	    24 	 0.04427 	 0.04381 	 ~...
   37 	    25 	 0.04662 	 0.04382 	 ~...
   60 	    26 	 0.04876 	 0.04384 	 ~...
   18 	    27 	 0.04525 	 0.04392 	 ~...
   66 	    28 	 0.04939 	 0.04399 	 ~...
   10 	    29 	 0.04470 	 0.04400 	 ~...
   23 	    30 	 0.04547 	 0.04402 	 ~...
   31 	    31 	 0.04605 	 0.04407 	 ~...
   29 	    32 	 0.04604 	 0.04418 	 ~...
   36 	    33 	 0.04650 	 0.04436 	 ~...
   24 	    34 	 0.04554 	 0.04462 	 ~...
   19 	    35 	 0.04528 	 0.04463 	 ~...
   17 	    36 	 0.04508 	 0.04463 	 ~...
    8 	    37 	 0.04457 	 0.04466 	 ~...
   48 	    38 	 0.04741 	 0.04473 	 ~...
   74 	    39 	 0.04982 	 0.04486 	 ~...
   11 	    40 	 0.04481 	 0.04503 	 ~...
   75 	    41 	 0.05000 	 0.04503 	 ~...
   51 	    42 	 0.04763 	 0.04515 	 ~...
    0 	    43 	 0.04238 	 0.04531 	 ~...
   32 	    44 	 0.04621 	 0.04556 	 ~...
   28 	    45 	 0.04599 	 0.04576 	 ~...
   43 	    46 	 0.04701 	 0.04593 	 ~...
   22 	    47 	 0.04543 	 0.04601 	 ~...
   71 	    48 	 0.04972 	 0.04606 	 ~...
    3 	    49 	 0.04313 	 0.04609 	 ~...
   40 	    50 	 0.04670 	 0.04644 	 ~...
    5 	    51 	 0.04449 	 0.04656 	 ~...
   78 	    52 	 0.05049 	 0.04676 	 ~...
   72 	    53 	 0.04976 	 0.04680 	 ~...
   35 	    54 	 0.04643 	 0.04708 	 ~...
   26 	    55 	 0.04571 	 0.04715 	 ~...
   65 	    56 	 0.04933 	 0.04724 	 ~...
   21 	    57 	 0.04534 	 0.04733 	 ~...
   77 	    58 	 0.05011 	 0.04780 	 ~...
   20 	    59 	 0.04530 	 0.04813 	 ~...
   42 	    60 	 0.04681 	 0.04822 	 ~...
    9 	    61 	 0.04463 	 0.04835 	 ~...
   34 	    62 	 0.04639 	 0.04852 	 ~...
   79 	    63 	 0.05063 	 0.04874 	 ~...
   70 	    64 	 0.04959 	 0.04908 	 ~...
   68 	    65 	 0.04953 	 0.04922 	 ~...
   68 	    66 	 0.04953 	 0.04970 	 ~...
   56 	    67 	 0.04835 	 0.04980 	 ~...
   39 	    68 	 0.04666 	 0.05011 	 ~...
   52 	    69 	 0.04774 	 0.05028 	 ~...
   62 	    70 	 0.04899 	 0.05054 	 ~...
   59 	    71 	 0.04870 	 0.05095 	 ~...
   47 	    72 	 0.04736 	 0.05123 	 ~...
   44 	    73 	 0.04714 	 0.05138 	 ~...
   82 	    74 	 0.05108 	 0.05199 	 ~...
   63 	    75 	 0.04905 	 0.05204 	 ~...
   53 	    76 	 0.04775 	 0.05245 	 ~...
   80 	    77 	 0.05068 	 0.05417 	 ~...
   50 	    78 	 0.04759 	 0.05424 	 ~...
   64 	    79 	 0.04909 	 0.05456 	 ~...
   44 	    80 	 0.04714 	 0.05494 	 ~...
   81 	    81 	 0.05098 	 0.05516 	 ~...
   25 	    82 	 0.04570 	 0.05665 	 ~...
   41 	    83 	 0.04671 	 0.05666 	 ~...
   49 	    84 	 0.04753 	 0.05722 	 ~...
   85 	    85 	 0.09048 	 0.10993 	 ~...
   86 	    86 	 0.18220 	 0.14413 	 m..s
   87 	    87 	 0.18374 	 0.16203 	 ~...
   94 	    88 	 0.22155 	 0.19222 	 ~...
   88 	    89 	 0.20699 	 0.20108 	 ~...
   91 	    90 	 0.21320 	 0.20170 	 ~...
   90 	    91 	 0.21187 	 0.21728 	 ~...
   95 	    92 	 0.22707 	 0.21787 	 ~...
   88 	    93 	 0.20699 	 0.22192 	 ~...
   95 	    94 	 0.22707 	 0.22751 	 ~...
   97 	    95 	 0.22710 	 0.22898 	 ~...
   92 	    96 	 0.21574 	 0.23195 	 ~...
   99 	    97 	 0.24650 	 0.24127 	 ~...
   93 	    98 	 0.21920 	 0.24660 	 ~...
  104 	    99 	 0.26404 	 0.25576 	 ~...
   98 	   100 	 0.24646 	 0.25579 	 ~...
  100 	   101 	 0.24883 	 0.26140 	 ~...
  112 	   102 	 0.27810 	 0.26203 	 ~...
  106 	   103 	 0.26837 	 0.26225 	 ~...
  102 	   104 	 0.26083 	 0.26491 	 ~...
  114 	   105 	 0.27957 	 0.26639 	 ~...
  113 	   106 	 0.27811 	 0.26747 	 ~...
  116 	   107 	 0.28710 	 0.26761 	 ~...
  105 	   108 	 0.26691 	 0.27037 	 ~...
  116 	   109 	 0.28710 	 0.27463 	 ~...
  107 	   110 	 0.26859 	 0.27942 	 ~...
  110 	   111 	 0.27246 	 0.28054 	 ~...
  101 	   112 	 0.26065 	 0.28280 	 ~...
  115 	   113 	 0.27987 	 0.28517 	 ~...
  109 	   114 	 0.27179 	 0.28601 	 ~...
  108 	   115 	 0.27176 	 0.28704 	 ~...
  111 	   116 	 0.27724 	 0.29520 	 ~...
  103 	   117 	 0.26313 	 0.29708 	 m..s
  118 	   118 	 0.30602 	 0.32126 	 ~...
  119 	   119 	 0.30776 	 0.33857 	 m..s
  120 	   120 	 0.31666 	 0.34190 	 ~...
==========================================
r_mrr = 0.9949465394020081
r2_mrr = 0.9894733428955078
spearmanr_mrr@5 = 0.9697084426879883
spearmanr_mrr@10 = 0.9872231483459473
spearmanr_mrr@50 = 0.994843602180481
spearmanr_mrr@100 = 0.9975874423980713
spearmanr_mrr@All = 0.9977982044219971
==========================================
test time: 0.392
Done Testing dataset Kinships
total time taken: 189.4134442806244
training time taken: 182.12001657485962
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9949)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9895)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9697)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9872)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9948)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9976)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9978)}}, 'test_loss': {'TransE': {'Kinships': 0.12391936012045335}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 7103102160345296
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [308, 400, 258, 1038, 728, 343, 1077, 815, 741, 870, 789, 49, 771, 172, 256, 92, 628, 98, 924, 743, 819, 222, 395, 833, 640, 567, 54, 1131, 662, 1031, 1035, 955, 500, 706, 476, 701, 788, 446, 173, 572, 1068, 451, 963, 947, 839, 1103, 139, 287, 317, 970, 578, 568, 398, 425, 87, 635, 782, 1189, 672, 683, 78, 834, 862, 613, 1181, 11, 390, 758, 916, 772, 365, 59, 43, 746, 176, 558, 803, 611, 295, 554, 522, 842, 826, 1139, 1098, 1022, 755, 1182, 273, 678, 379, 805, 145, 61, 666, 621, 345, 889, 1011, 203, 100, 363, 1057, 975, 501, 1119, 1095, 297, 417, 738, 253, 518, 1028, 1039, 385, 954, 602, 213, 670, 655, 749]
valid_ids (0): []
train_ids (1094): [1019, 813, 404, 1062, 966, 690, 882, 904, 936, 684, 171, 112, 52, 1006, 533, 650, 211, 366, 1199, 867, 46, 828, 458, 466, 778, 648, 1002, 221, 796, 111, 331, 940, 997, 424, 397, 534, 995, 802, 1097, 521, 653, 957, 888, 480, 1036, 871, 912, 1026, 270, 1202, 727, 474, 582, 456, 433, 1066, 877, 1116, 1105, 227, 422, 442, 1210, 384, 983, 162, 421, 1128, 119, 527, 207, 255, 248, 774, 1188, 483, 1088, 247, 6, 1111, 362, 461, 761, 1178, 1175, 1187, 619, 656, 902, 1118, 573, 1025, 581, 263, 1143, 339, 1185, 914, 1135, 1067, 376, 1212, 5, 937, 536, 751, 154, 449, 890, 1015, 801, 586, 160, 1048, 971, 905, 281, 498, 1213, 1186, 35, 195, 83, 360, 891, 488, 105, 852, 1046, 338, 1043, 1083, 876, 1075, 931, 1196, 561, 827, 816, 288, 753, 319, 570, 121, 37, 709, 926, 1134, 356, 323, 896, 283, 641, 1179, 149, 129, 490, 457, 57, 552, 750, 836, 1027, 1155, 134, 140, 123, 1209, 559, 235, 1152, 1141, 524, 1164, 158, 696, 908, 523, 642, 911, 267, 775, 622, 979, 516, 392, 695, 760, 864, 298, 620, 420, 445, 776, 617, 186, 90, 199, 76, 378, 925, 1140, 933, 18, 9, 486, 130, 494, 29, 783, 615, 375, 289, 39, 636, 732, 19, 754, 1051, 1171, 250, 1099, 718, 932, 825, 324, 597, 95, 103, 1032, 40, 1059, 785, 113, 106, 20, 10, 661, 699, 241, 848, 651, 885, 677, 440, 438, 1149, 909, 237, 810, 675, 631, 935, 806, 278, 1069, 557, 201, 795, 386, 1113, 1060, 503, 580, 21, 790, 497, 791, 41, 688, 634, 600, 934, 74, 192, 373, 807, 1120, 992, 135, 835, 799, 1165, 412, 1166, 668, 1151, 1021, 686, 450, 846, 623, 664, 282, 1170, 444, 47, 544, 820, 1056, 1124, 274, 707, 840, 152, 739, 873, 159, 469, 1096, 1130, 713, 426, 236, 1132, 232, 563, 564, 589, 1086, 372, 687, 332, 157, 437, 4, 251, 604, 850, 51, 949, 509, 944, 505, 859, 855, 126, 24, 1072, 962, 1144, 872, 549, 526, 577, 381, 710, 265, 735, 50, 1138, 34, 1109, 1195, 843, 562, 75, 942, 69, 880, 565, 973, 708, 1087, 318, 787, 1000, 837, 627, 187, 455, 124, 315, 487, 532, 797, 644, 296, 1040, 808, 689, 812, 311, 110, 674, 233, 210, 264, 368, 1091, 583, 541, 605, 733, 1052, 512, 148, 109, 822, 1012, 1044, 555, 1159, 704, 1017, 151, 430, 340, 599, 736, 71, 431, 120, 407, 1172, 155, 1089, 907, 1137, 294, 184, 1005, 79, 1016, 355, 1003, 609, 73, 535, 1049, 225, 181, 153, 238, 346, 491, 1125, 1024, 769, 1054, 624, 198, 272, 183, 127, 847, 520, 1200, 204, 218, 1147, 548, 143, 998, 857, 475, 322, 419, 245, 406, 328, 226, 798, 244, 734, 939, 17, 1142, 380, 762, 142, 652, 612, 530, 167, 132, 23, 1126, 1190, 402, 102, 547, 193, 506, 730, 344, 1127, 1009, 489, 646, 45, 128, 391, 354, 579, 694, 929, 504, 575, 767, 326, 206, 477, 1085, 770, 928, 818, 70, 436, 756, 671, 1122, 64, 1156, 405, 447, 747, 974, 780, 951, 7, 881, 175, 793, 551, 1154, 209, 163, 849, 84, 528, 657, 320, 1020, 492, 481, 587, 350, 310, 482, 886, 938, 744, 88, 133, 1110, 1146, 485, 731, 291, 721, 817, 208, 841, 189, 131, 742, 32, 725, 86, 510, 829, 1194, 341, 903, 484, 1102, 48, 58, 964, 603, 519, 453, 920, 1037, 408, 1045, 349, 465, 1047, 94, 1080, 680, 883, 923, 658, 147, 614, 632, 969, 85, 919, 369, 722, 538, 359, 856, 740, 507, 649, 1197, 921, 700, 667, 459, 993, 716, 388, 660, 276, 539, 830, 702, 773, 1081, 986, 108, 330, 1076, 313, 1, 540, 1004, 1090, 1214, 1001, 1121, 1148, 321, 953, 514, 705, 266, 1115, 1117, 495, 401, 220, 590, 115, 82, 1029, 249, 394, 66, 1100, 93, 1160, 303, 854, 268, 591, 996, 0, 36, 234, 224, 114, 1184, 389, 499, 104, 1153, 571, 616, 1041, 279, 1198, 608, 190, 682, 337, 529, 309, 712, 439, 174, 976, 335, 1050, 197, 1079, 887, 347, 462, 1074, 260, 779, 434, 768, 831, 306, 1204, 560, 821, 336, 863, 874, 286, 30, 358, 329, 606, 26, 1177, 960, 592, 205, 1106, 999, 333, 215, 794, 895, 101, 28, 804, 262, 383, 3, 415, 194, 418, 853, 765, 654, 511, 1192, 918, 697, 814, 228, 626, 387, 823, 824, 866, 1206, 427, 429, 989, 594, 637, 25, 685, 946, 737, 117, 1104, 1169, 361, 729, 645, 899, 868, 230, 243, 858, 1092, 703, 1173, 596, 435, 574, 991, 894, 89, 166, 342, 513, 432, 980, 811, 553, 146, 1163, 212, 1180, 1161, 164, 182, 792, 67, 965, 984, 1042, 96, 1071, 525, 312, 1065, 179, 219, 679, 663, 144, 277, 325, 200, 566, 851, 348, 307, 150, 576, 242, 1030, 454, 467, 630, 16, 169, 136, 414, 647, 598, 724, 1158, 994, 845, 1207, 748, 463, 659, 413, 27, 1183, 271, 80, 941, 165, 1201, 314, 15, 537, 443, 1133, 1058, 714, 959, 726, 869, 301, 676, 1053, 161, 542, 958, 441, 1203, 968, 53, 97, 493, 681, 1064, 202, 1193, 915, 191, 68, 906, 377, 838, 601, 766, 452, 217, 800, 141, 948, 900, 698, 374, 327, 809, 1174, 99, 122, 304, 786, 214, 1167, 8, 299, 1023, 62, 588, 137, 269, 14, 229, 1055, 1078, 673, 496, 22, 987, 893, 116, 107, 334, 55, 403, 952, 715, 517, 764, 865, 275, 546, 1033, 884, 1157, 917, 1205, 239, 464, 1123, 1093, 473, 879, 409, 943, 178, 1162, 364, 945, 56, 393, 44, 1094, 353, 633, 897, 502, 638, 285, 416, 292, 302, 1084, 910, 711, 988, 367, 1114, 898, 138, 1150, 399, 745, 460, 1136, 585, 717, 784, 618, 1061, 254, 1073, 930, 861, 471, 757, 231, 168, 470, 216, 396, 91, 280, 77, 13, 1018, 72, 478, 860, 752, 985, 531, 33, 972, 156, 42, 290, 81, 950, 595, 719, 196, 257, 1107, 1108, 723, 118, 1191, 982, 878, 692, 1176, 428, 1168, 927, 515, 720, 693, 351, 1007, 410, 643, 545, 901, 967, 293, 60, 259, 38, 550, 508, 1013, 832, 1101, 691, 316, 261, 305, 763, 370, 12, 125, 892, 556, 543, 669, 777, 629, 423, 875, 1014, 990, 1070, 584, 759, 31, 593, 610, 246, 1129, 180, 1112, 65, 352, 177, 977, 1008, 781, 411, 1145, 607, 1082, 1211, 1034, 665, 371, 625, 569, 170, 284, 2, 300, 1010, 185, 922, 1063, 382, 223, 252, 913, 844, 1208, 468, 961, 978, 472, 63, 188, 956, 357, 448, 240, 639, 479, 981]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7660804830617600
the save name prefix for this run is:  chkpt-ID_7660804830617600_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 985
rank avg (pred): 0.557 +- 0.003
mrr vals (pred, true): 0.017, 0.363
batch losses (mrrl, rdl): 0.0, 0.0044763573

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 383
rank avg (pred): 0.424 +- 0.191
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 0.0, 3.14591e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 223
rank avg (pred): 0.438 +- 0.263
mrr vals (pred, true): 0.107, 0.040
batch losses (mrrl, rdl): 0.0, 6.0218e-06

Epoch over!
epoch time: 12.035

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 66
rank avg (pred): 0.098 +- 0.071
mrr vals (pred, true): 0.246, 0.314
batch losses (mrrl, rdl): 0.0, 2.4408e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 423
rank avg (pred): 0.446 +- 0.260
mrr vals (pred, true): 0.089, 0.046
batch losses (mrrl, rdl): 0.0, 2.4647e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1041
rank avg (pred): 0.437 +- 0.264
mrr vals (pred, true): 0.090, 0.045
batch losses (mrrl, rdl): 0.0, 3.1394e-06

Epoch over!
epoch time: 11.981

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 906
rank avg (pred): 0.165 +- 0.127
mrr vals (pred, true): 0.196, 0.215
batch losses (mrrl, rdl): 0.0, 1.43815e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 715
rank avg (pred): 0.441 +- 0.267
mrr vals (pred, true): 0.094, 0.044
batch losses (mrrl, rdl): 0.0, 6.1152e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1120
rank avg (pred): 0.431 +- 0.266
mrr vals (pred, true): 0.085, 0.042
batch losses (mrrl, rdl): 0.0, 1.50161e-05

Epoch over!
epoch time: 11.85

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 857
rank avg (pred): 0.468 +- 0.263
mrr vals (pred, true): 0.063, 0.042
batch losses (mrrl, rdl): 0.0, 2.8114e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 886
rank avg (pred): 0.459 +- 0.261
mrr vals (pred, true): 0.064, 0.047
batch losses (mrrl, rdl): 0.0, 3.7692e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1128
rank avg (pred): 0.419 +- 0.276
mrr vals (pred, true): 0.103, 0.043
batch losses (mrrl, rdl): 0.0, 2.50918e-05

Epoch over!
epoch time: 11.936

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 675
rank avg (pred): 0.453 +- 0.262
mrr vals (pred, true): 0.074, 0.044
batch losses (mrrl, rdl): 0.0, 1.8433e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1183
rank avg (pred): 0.427 +- 0.264
mrr vals (pred, true): 0.093, 0.047
batch losses (mrrl, rdl): 0.0, 3.2097e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 84
rank avg (pred): 0.438 +- 0.263
mrr vals (pred, true): 0.078, 0.048
batch losses (mrrl, rdl): 0.0, 6.7554e-06

Epoch over!
epoch time: 11.867

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 563
rank avg (pred): 0.110 +- 0.104
mrr vals (pred, true): 0.297, 0.226
batch losses (mrrl, rdl): 0.049754139, 1.3265e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 242
rank avg (pred): 0.471 +- 0.211
mrr vals (pred, true): 0.061, 0.046
batch losses (mrrl, rdl): 0.0013118628, 1.49161e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 41
rank avg (pred): 0.072 +- 0.051
mrr vals (pred, true): 0.293, 0.279
batch losses (mrrl, rdl): 0.0020792445, 5.44408e-05

Epoch over!
epoch time: 12.429

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 270
rank avg (pred): 0.076 +- 0.052
mrr vals (pred, true): 0.274, 0.290
batch losses (mrrl, rdl): 0.0023097445, 1.63864e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1178
rank avg (pred): 0.457 +- 0.178
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.73606e-05, 2.96993e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 272
rank avg (pred): 0.079 +- 0.054
mrr vals (pred, true): 0.283, 0.287
batch losses (mrrl, rdl): 0.0001574479, 4.99833e-05

Epoch over!
epoch time: 12.054

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 25
rank avg (pred): 0.084 +- 0.058
mrr vals (pred, true): 0.280, 0.256
batch losses (mrrl, rdl): 0.005662322, 1.36701e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 779
rank avg (pred): 0.455 +- 0.170
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 0.0001128194, 2.92405e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 202
rank avg (pred): 0.466 +- 0.161
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 5.31676e-05, 3.32261e-05

Epoch over!
epoch time: 11.983

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 936
rank avg (pred): 0.453 +- 0.165
mrr vals (pred, true): 0.052, 0.040
batch losses (mrrl, rdl): 5.43504e-05, 2.59476e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 293
rank avg (pred): 0.080 +- 0.052
mrr vals (pred, true): 0.275, 0.275
batch losses (mrrl, rdl): 4.6272e-06, 3.17999e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 86
rank avg (pred): 0.447 +- 0.172
mrr vals (pred, true): 0.058, 0.052
batch losses (mrrl, rdl): 0.0006068577, 4.64288e-05

Epoch over!
epoch time: 12.023

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 447
rank avg (pred): 0.456 +- 0.163
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 0.000111135, 3.24294e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 843
rank avg (pred): 0.466 +- 0.151
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 0.0001062319, 4.06074e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 229
rank avg (pred): 0.471 +- 0.147
mrr vals (pred, true): 0.045, 0.041
batch losses (mrrl, rdl): 0.0002914385, 3.85659e-05

Epoch over!
epoch time: 12.003

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 998
rank avg (pred): 0.043 +- 0.027
mrr vals (pred, true): 0.349, 0.334
batch losses (mrrl, rdl): 0.002281568, 9.66883e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 574
rank avg (pred): 0.467 +- 0.155
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 1.7e-09, 3.26601e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 660
rank avg (pred): 0.452 +- 0.147
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 4.558e-07, 4.0029e-05

Epoch over!
epoch time: 12.165

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 677
rank avg (pred): 0.445 +- 0.155
mrr vals (pred, true): 0.055, 0.043
batch losses (mrrl, rdl): 0.0002786746, 4.58203e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 864
rank avg (pred): 0.463 +- 0.139
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001484656, 4.23049e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 357
rank avg (pred): 0.469 +- 0.139
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0001365586, 6.47619e-05

Epoch over!
epoch time: 12.026

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 156
rank avg (pred): 0.440 +- 0.147
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 0.0001042846, 3.98474e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 194
rank avg (pred): 0.448 +- 0.162
mrr vals (pred, true): 0.063, 0.044
batch losses (mrrl, rdl): 0.0018038498, 3.32524e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 863
rank avg (pred): 0.469 +- 0.136
mrr vals (pred, true): 0.048, 0.051
batch losses (mrrl, rdl): 4.12341e-05, 6.53168e-05

Epoch over!
epoch time: 12.141

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1206
rank avg (pred): 0.472 +- 0.132
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001326889, 4.50472e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 282
rank avg (pred): 0.084 +- 0.052
mrr vals (pred, true): 0.285, 0.317
batch losses (mrrl, rdl): 0.0101082316, 3.1703e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 42
rank avg (pred): 0.088 +- 0.054
mrr vals (pred, true): 0.284, 0.293
batch losses (mrrl, rdl): 0.000888573, 3.9744e-06

Epoch over!
epoch time: 11.982

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 939
rank avg (pred): 0.455 +- 0.145
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002551292, 4.3687e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 515
rank avg (pred): 0.222 +- 0.132
mrr vals (pred, true): 0.216, 0.221
batch losses (mrrl, rdl): 0.0002326003, 0.0001946212

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 972
rank avg (pred): 0.055 +- 0.032
mrr vals (pred, true): 0.313, 0.312
batch losses (mrrl, rdl): 3.228e-06, 5.16663e-05

Epoch over!
epoch time: 12.017

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.105 +- 0.062
mrr vals (pred, true): 0.263, 0.287

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   36 	     0 	 0.04848 	 0.04092 	 ~...
   29 	     1 	 0.04835 	 0.04152 	 ~...
   51 	     2 	 0.04887 	 0.04214 	 ~...
   16 	     3 	 0.04801 	 0.04222 	 ~...
   25 	     4 	 0.04831 	 0.04224 	 ~...
    9 	     5 	 0.04789 	 0.04234 	 ~...
   47 	     6 	 0.04874 	 0.04239 	 ~...
   10 	     7 	 0.04796 	 0.04257 	 ~...
   34 	     8 	 0.04843 	 0.04303 	 ~...
   78 	     9 	 0.05093 	 0.04319 	 ~...
    0 	    10 	 0.04750 	 0.04322 	 ~...
    7 	    11 	 0.04778 	 0.04345 	 ~...
   42 	    12 	 0.04866 	 0.04346 	 ~...
   21 	    13 	 0.04822 	 0.04352 	 ~...
   33 	    14 	 0.04838 	 0.04357 	 ~...
    1 	    15 	 0.04752 	 0.04362 	 ~...
    8 	    16 	 0.04781 	 0.04381 	 ~...
   30 	    17 	 0.04835 	 0.04383 	 ~...
    4 	    18 	 0.04768 	 0.04389 	 ~...
   23 	    19 	 0.04825 	 0.04392 	 ~...
    6 	    20 	 0.04777 	 0.04404 	 ~...
   64 	    21 	 0.04943 	 0.04406 	 ~...
   66 	    22 	 0.04960 	 0.04409 	 ~...
   32 	    23 	 0.04837 	 0.04409 	 ~...
   76 	    24 	 0.05080 	 0.04412 	 ~...
   16 	    25 	 0.04801 	 0.04414 	 ~...
    2 	    26 	 0.04759 	 0.04415 	 ~...
   12 	    27 	 0.04797 	 0.04416 	 ~...
   13 	    28 	 0.04797 	 0.04423 	 ~...
   20 	    29 	 0.04813 	 0.04436 	 ~...
   19 	    30 	 0.04809 	 0.04454 	 ~...
   72 	    31 	 0.05009 	 0.04455 	 ~...
   26 	    32 	 0.04832 	 0.04463 	 ~...
   71 	    33 	 0.05003 	 0.04469 	 ~...
   24 	    34 	 0.04828 	 0.04480 	 ~...
   43 	    35 	 0.04867 	 0.04486 	 ~...
   27 	    36 	 0.04833 	 0.04494 	 ~...
   15 	    37 	 0.04797 	 0.04495 	 ~...
   31 	    38 	 0.04836 	 0.04496 	 ~...
   76 	    39 	 0.05080 	 0.04500 	 ~...
   60 	    40 	 0.04919 	 0.04506 	 ~...
   10 	    41 	 0.04796 	 0.04532 	 ~...
   75 	    42 	 0.05068 	 0.04535 	 ~...
   58 	    43 	 0.04893 	 0.04537 	 ~...
   68 	    44 	 0.04993 	 0.04542 	 ~...
    3 	    45 	 0.04767 	 0.04546 	 ~...
   27 	    46 	 0.04833 	 0.04548 	 ~...
   62 	    47 	 0.04928 	 0.04560 	 ~...
   22 	    48 	 0.04822 	 0.04561 	 ~...
   73 	    49 	 0.05013 	 0.04578 	 ~...
   13 	    50 	 0.04797 	 0.04586 	 ~...
   74 	    51 	 0.05020 	 0.04616 	 ~...
   48 	    52 	 0.04880 	 0.04653 	 ~...
   18 	    53 	 0.04809 	 0.04656 	 ~...
   45 	    54 	 0.04872 	 0.04659 	 ~...
   35 	    55 	 0.04847 	 0.04665 	 ~...
    5 	    56 	 0.04775 	 0.04672 	 ~...
   54 	    57 	 0.04891 	 0.04694 	 ~...
   38 	    58 	 0.04862 	 0.04708 	 ~...
   63 	    59 	 0.04932 	 0.04871 	 ~...
   61 	    60 	 0.04926 	 0.04874 	 ~...
   80 	    61 	 0.05151 	 0.04890 	 ~...
   55 	    62 	 0.04892 	 0.04895 	 ~...
   57 	    63 	 0.04893 	 0.04910 	 ~...
   67 	    64 	 0.04972 	 0.04970 	 ~...
   79 	    65 	 0.05093 	 0.04980 	 ~...
   41 	    66 	 0.04866 	 0.05004 	 ~...
   44 	    67 	 0.04868 	 0.05012 	 ~...
   56 	    68 	 0.04893 	 0.05020 	 ~...
   48 	    69 	 0.04880 	 0.05039 	 ~...
   59 	    70 	 0.04895 	 0.05138 	 ~...
   39 	    71 	 0.04863 	 0.05143 	 ~...
   40 	    72 	 0.04863 	 0.05144 	 ~...
   80 	    73 	 0.05151 	 0.05220 	 ~...
   46 	    74 	 0.04873 	 0.05240 	 ~...
   50 	    75 	 0.04884 	 0.05321 	 ~...
   65 	    76 	 0.04946 	 0.05327 	 ~...
   52 	    77 	 0.04887 	 0.05367 	 ~...
   69 	    78 	 0.04993 	 0.05610 	 ~...
   70 	    79 	 0.05003 	 0.05723 	 ~...
   53 	    80 	 0.04889 	 0.05791 	 ~...
   37 	    81 	 0.04860 	 0.05973 	 ~...
   84 	    82 	 0.20170 	 0.17772 	 ~...
   88 	    83 	 0.20709 	 0.18210 	 ~...
   85 	    84 	 0.20209 	 0.18602 	 ~...
   90 	    85 	 0.21351 	 0.20271 	 ~...
   99 	    86 	 0.22662 	 0.20380 	 ~...
   91 	    87 	 0.21628 	 0.21008 	 ~...
   82 	    88 	 0.18872 	 0.22449 	 m..s
   83 	    89 	 0.19631 	 0.22671 	 m..s
   94 	    90 	 0.21861 	 0.22898 	 ~...
   96 	    91 	 0.21929 	 0.22906 	 ~...
   86 	    92 	 0.20410 	 0.23470 	 m..s
   95 	    93 	 0.21920 	 0.23632 	 ~...
   89 	    94 	 0.20998 	 0.24660 	 m..s
   86 	    95 	 0.20410 	 0.24908 	 m..s
  100 	    96 	 0.24938 	 0.25665 	 ~...
  108 	    97 	 0.26133 	 0.26274 	 ~...
   92 	    98 	 0.21669 	 0.26680 	 m..s
   98 	    99 	 0.22312 	 0.26778 	 m..s
  106 	   100 	 0.25997 	 0.27037 	 ~...
  103 	   101 	 0.25820 	 0.27106 	 ~...
  115 	   102 	 0.26926 	 0.27120 	 ~...
  116 	   103 	 0.27528 	 0.27220 	 ~...
  101 	   104 	 0.25556 	 0.27473 	 ~...
   93 	   105 	 0.21711 	 0.27723 	 m..s
  107 	   106 	 0.26099 	 0.27725 	 ~...
  102 	   107 	 0.25774 	 0.27807 	 ~...
   97 	   108 	 0.22267 	 0.27948 	 m..s
  109 	   109 	 0.26189 	 0.28089 	 ~...
  114 	   110 	 0.26914 	 0.28138 	 ~...
  110 	   111 	 0.26262 	 0.28527 	 ~...
  111 	   112 	 0.26265 	 0.28696 	 ~...
  105 	   113 	 0.25867 	 0.28742 	 ~...
  112 	   114 	 0.26315 	 0.29137 	 ~...
  103 	   115 	 0.25820 	 0.29520 	 m..s
  113 	   116 	 0.26592 	 0.30314 	 m..s
  119 	   117 	 0.30855 	 0.33730 	 ~...
  120 	   118 	 0.31023 	 0.34329 	 m..s
  117 	   119 	 0.30605 	 0.35540 	 m..s
  118 	   120 	 0.30852 	 0.35877 	 m..s
==========================================
r_mrr = 0.9925413727760315
r2_mrr = 0.9724318385124207
spearmanr_mrr@5 = 0.9489924907684326
spearmanr_mrr@10 = 0.9874269366264343
spearmanr_mrr@50 = 0.9923029541969299
spearmanr_mrr@100 = 0.9969936013221741
spearmanr_mrr@All = 0.9972559213638306
==========================================
test time: 0.391
Done Testing dataset Kinships
total time taken: 188.10563373565674
training time taken: 180.9705367088318
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9925)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9724)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9490)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9874)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9923)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9970)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9973)}}, 'test_loss': {'TransE': {'Kinships': 0.3573663632996613}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 1295534603164979
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [678, 1020, 592, 978, 386, 902, 585, 1161, 214, 990, 191, 370, 1166, 380, 583, 395, 1158, 96, 175, 412, 966, 205, 686, 426, 1114, 235, 816, 494, 268, 250, 152, 885, 1188, 556, 449, 824, 243, 776, 266, 1050, 455, 690, 1047, 137, 279, 771, 25, 622, 643, 1124, 1203, 1076, 843, 558, 723, 1026, 800, 194, 999, 798, 1136, 621, 100, 737, 540, 446, 66, 294, 321, 1196, 109, 86, 1160, 721, 952, 57, 1024, 20, 606, 389, 437, 105, 436, 1147, 653, 390, 892, 1112, 789, 64, 163, 871, 356, 942, 802, 150, 281, 433, 683, 672, 387, 1198, 363, 1098, 596, 792, 300, 787, 948, 668, 67, 1205, 856, 507, 283, 82, 808, 173, 1063, 719, 78]
valid_ids (0): []
train_ids (1094): [210, 724, 1058, 1057, 102, 170, 313, 471, 1127, 1039, 743, 1106, 398, 997, 377, 146, 320, 778, 341, 579, 936, 264, 311, 658, 400, 781, 200, 872, 1053, 1025, 584, 624, 142, 614, 806, 85, 330, 972, 60, 206, 807, 899, 947, 561, 325, 42, 1016, 312, 845, 211, 569, 930, 1212, 275, 399, 762, 691, 883, 1123, 533, 1162, 692, 479, 510, 361, 288, 472, 434, 756, 237, 1194, 117, 1169, 650, 637, 224, 1079, 1074, 1157, 983, 366, 37, 90, 1, 955, 661, 1038, 587, 880, 921, 416, 1113, 349, 480, 187, 877, 552, 758, 748, 249, 429, 616, 230, 984, 938, 543, 745, 524, 1060, 875, 933, 526, 810, 461, 707, 1061, 674, 575, 409, 1031, 5, 581, 1182, 302, 530, 148, 1207, 973, 53, 1164, 450, 218, 864, 701, 932, 574, 30, 945, 1011, 852, 413, 8, 1091, 931, 960, 1015, 729, 337, 1192, 115, 24, 1081, 741, 192, 744, 43, 270, 13, 430, 1036, 431, 508, 422, 958, 559, 594, 443, 1141, 1183, 139, 865, 512, 1144, 233, 154, 299, 1129, 445, 577, 404, 591, 750, 419, 623, 149, 48, 694, 156, 566, 444, 657, 582, 553, 849, 306, 628, 962, 95, 442, 38, 1101, 1134, 1199, 1149, 223, 988, 1208, 649, 597, 285, 1052, 937, 505, 768, 174, 916, 714, 1108, 71, 602, 760, 213, 1142, 318, 1121, 326, 121, 664, 659, 568, 976, 1146, 609, 166, 375, 919, 358, 1152, 1213, 588, 554, 4, 1201, 1209, 927, 333, 1180, 350, 542, 204, 483, 207, 231, 133, 92, 706, 309, 269, 995, 497, 626, 342, 642, 735, 457, 1200, 1033, 402, 420, 120, 681, 548, 992, 549, 365, 651, 459, 481, 1041, 406, 1156, 21, 844, 193, 1023, 343, 484, 335, 1004, 893, 1010, 1095, 1049, 850, 401, 454, 253, 267, 1174, 599, 696, 31, 814, 39, 730, 276, 226, 368, 232, 34, 1088, 767, 56, 963, 829, 91, 98, 272, 598, 169, 199, 357, 221, 384, 1068, 847, 72, 178, 1055, 439, 1165, 830, 669, 177, 1075, 879, 185, 1072, 151, 925, 315, 634, 1056, 532, 107, 469, 996, 751, 462, 728, 307, 636, 385, 407, 360, 1154, 926, 539, 1104, 161, 1100, 1048, 108, 424, 970, 673, 870, 125, 713, 815, 841, 1137, 1027, 527, 145, 134, 761, 564, 954, 241, 523, 456, 1155, 19, 184, 834, 410, 720, 7, 890, 906, 881, 1135, 1030, 1179, 159, 645, 9, 1019, 770, 1128, 935, 323, 212, 851, 833, 397, 991, 1195, 519, 55, 688, 47, 466, 801, 590, 652, 1046, 372, 557, 832, 703, 1006, 943, 1168, 939, 975, 1062, 1085, 853, 1028, 470, 274, 54, 334, 965, 817, 203, 418, 215, 709, 240, 520, 295, 260, 726, 739, 448, 842, 662, 998, 12, 352, 589, 114, 50, 489, 104, 1082, 563, 14, 929, 1120, 393, 1001, 23, 795, 619, 534, 89, 908, 482, 684, 18, 77, 994, 113, 980, 640, 826, 171, 1080, 347, 373, 1139, 780, 537, 1184, 52, 128, 670, 94, 256, 777, 1070, 766, 247, 515, 982, 202, 465, 560, 301, 1119, 181, 1151, 172, 676, 118, 578, 317, 51, 290, 1012, 1189, 747, 1064, 44, 509, 956, 345, 46, 251, 828, 130, 1066, 894, 961, 772, 1175, 1084, 1117, 793, 629, 705, 28, 974, 222, 873, 812, 123, 103, 284, 280, 485, 438, 40, 217, 93, 665, 804, 491, 242, 427, 435, 677, 562, 521, 234, 403, 600, 180, 49, 1008, 805, 1193, 898, 376, 884, 225, 654, 698, 124, 478, 374, 914, 1206, 1191, 87, 157, 1099, 666, 303, 825, 355, 62, 138, 944, 689, 371, 477, 625, 682, 164, 535, 378, 1122, 809, 69, 1132, 1177, 297, 704, 1211, 1014, 1021, 291, 1034, 106, 475, 501, 417, 601, 712, 112, 525, 917, 1044, 167, 239, 411, 354, 848, 388, 576, 1148, 58, 838, 603, 946, 785, 783, 1111, 153, 1037, 97, 971, 860, 143, 405, 261, 36, 660, 15, 336, 1002, 255, 278, 248, 529, 1202, 866, 887, 1045, 338, 263, 474, 511, 447, 1007, 620, 467, 219, 506, 332, 861, 132, 786, 903, 950, 904, 344, 254, 791, 820, 75, 1005, 362, 1109, 835, 551, 888, 891, 840, 220, 784, 855, 1172, 1040, 949, 201, 1204, 473, 76, 702, 453, 570, 141, 827, 1087, 981, 769, 65, 858, 45, 127, 1131, 101, 59, 1145, 126, 88, 293, 1029, 685, 84, 1126, 1077, 244, 905, 746, 32, 188, 1167, 314, 421, 147, 759, 740, 74, 464, 136, 863, 580, 440, 618, 1089, 1138, 339, 774, 382, 1115, 396, 79, 0, 878, 364, 16, 155, 895, 868, 964, 331, 639, 216, 733, 496, 1181, 238, 517, 763, 968, 168, 1018, 262, 1143, 876, 818, 708, 183, 498, 2, 874, 353, 198, 869, 451, 160, 502, 22, 189, 846, 381, 607, 1171, 492, 779, 452, 734, 1059, 1013, 907, 33, 176, 573, 277, 165, 1105, 1176, 901, 329, 1071, 236, 182, 836, 586, 1150, 595, 941, 1159, 918, 1022, 680, 367, 794, 228, 486, 928, 655, 923, 41, 17, 897, 940, 732, 531, 1009, 351, 641, 287, 711, 544, 319, 957, 752, 122, 35, 415, 1133, 782, 131, 764, 695, 328, 831, 83, 718, 715, 610, 271, 229, 819, 889, 259, 1093, 571, 555, 129, 327, 648, 742, 647, 111, 822, 1210, 208, 414, 162, 1054, 700, 1125, 631, 796, 140, 503, 909, 1116, 882, 1118, 839, 725, 550, 432, 773, 144, 273, 993, 1107, 1153, 1110, 26, 920, 493, 951, 316, 854, 186, 969, 310, 500, 1197, 209, 632, 286, 1187, 245, 1065, 633, 346, 1078, 886, 736, 246, 110, 967, 1102, 488, 545, 541, 565, 116, 468, 81, 190, 567, 857, 716, 348, 227, 383, 68, 513, 663, 627, 699, 304, 675, 755, 753, 679, 615, 986, 953, 296, 977, 989, 593, 135, 754, 915, 1163, 27, 1000, 959, 896, 1032, 460, 322, 924, 697, 803, 1103, 476, 499, 934, 656, 910, 617, 394, 922, 495, 757, 258, 11, 1073, 428, 308, 538, 536, 487, 305, 1173, 547, 504, 1185, 99, 738, 644, 1130, 289, 604, 749, 252, 80, 797, 1069, 837, 423, 369, 913, 1043, 458, 1097, 987, 257, 1035, 862, 1086, 6, 613, 195, 811, 514, 731, 788, 1140, 859, 635, 612, 867, 813, 1003, 611, 340, 1094, 693, 518, 823, 1096, 1042, 765, 197, 441, 646, 298, 687, 1178, 522, 546, 265, 179, 799, 821, 900, 630, 722, 463, 282, 1170, 667, 359, 985, 292, 73, 790, 158, 1051, 63, 727, 717, 1214, 1186, 608, 572, 912, 379, 490, 979, 911, 10, 1083, 516, 408, 671, 638, 1017, 1090, 775, 70, 119, 392, 391, 3, 29, 528, 61, 324, 1092, 710, 1190, 1067, 605, 425, 196]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4352851468419124
the save name prefix for this run is:  chkpt-ID_4352851468419124_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 981
rank avg (pred): 0.455 +- 0.006
mrr vals (pred, true): 0.021, 0.315
batch losses (mrrl, rdl): 0.0, 0.0026661437

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 336
rank avg (pred): 0.456 +- 0.005
mrr vals (pred, true): 0.021, 0.050
batch losses (mrrl, rdl): 0.0, 8.55461e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 7
rank avg (pred): 0.095 +- 0.013
mrr vals (pred, true): 0.095, 0.245
batch losses (mrrl, rdl): 0.0, 1.27492e-05

Epoch over!
epoch time: 12.233

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 811
rank avg (pred): 0.227 +- 0.031
mrr vals (pred, true): 0.042, 0.176
batch losses (mrrl, rdl): 0.0, 6.73788e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1145
rank avg (pred): 0.104 +- 0.070
mrr vals (pred, true): 0.303, 0.246
batch losses (mrrl, rdl): 0.0, 6.1678e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 192
rank avg (pred): 0.418 +- 0.270
mrr vals (pred, true): 0.208, 0.044
batch losses (mrrl, rdl): 0.0, 2.2086e-05

Epoch over!
epoch time: 11.972

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 456
rank avg (pred): 0.403 +- 0.263
mrr vals (pred, true): 0.210, 0.043
batch losses (mrrl, rdl): 0.0, 4.94932e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1080
rank avg (pred): 0.440 +- 0.268
mrr vals (pred, true): 0.166, 0.053
batch losses (mrrl, rdl): 0.0, 4.21604e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 512
rank avg (pred): 0.096 +- 0.069
mrr vals (pred, true): 0.292, 0.236
batch losses (mrrl, rdl): 0.0, 2.13219e-05

Epoch over!
epoch time: 11.836

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1039
rank avg (pred): 0.438 +- 0.264
mrr vals (pred, true): 0.144, 0.044
batch losses (mrrl, rdl): 0.0, 4.4068e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 60
rank avg (pred): 0.098 +- 0.072
mrr vals (pred, true): 0.288, 0.263
batch losses (mrrl, rdl): 0.0, 2.5029e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 754
rank avg (pred): 0.170 +- 0.125
mrr vals (pred, true): 0.230, 0.272
batch losses (mrrl, rdl): 0.0, 2.58962e-05

Epoch over!
epoch time: 11.903

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 255
rank avg (pred): 0.077 +- 0.065
mrr vals (pred, true): 0.356, 0.296
batch losses (mrrl, rdl): 0.0, 8.379e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 525
rank avg (pred): 0.132 +- 0.096
mrr vals (pred, true): 0.235, 0.221
batch losses (mrrl, rdl): 0.0, 1.17891e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1212
rank avg (pred): 0.443 +- 0.265
mrr vals (pred, true): 0.089, 0.041
batch losses (mrrl, rdl): 0.0, 1.35337e-05

Epoch over!
epoch time: 11.819

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 628
rank avg (pred): 0.448 +- 0.266
mrr vals (pred, true): 0.083, 0.043
batch losses (mrrl, rdl): 0.010669075, 2.7714e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 685
rank avg (pred): 0.467 +- 0.173
mrr vals (pred, true): 0.059, 0.047
batch losses (mrrl, rdl): 0.0007438459, 3.60067e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 543
rank avg (pred): 0.267 +- 0.167
mrr vals (pred, true): 0.195, 0.217
batch losses (mrrl, rdl): 0.0048612598, 0.0005464485

Epoch over!
epoch time: 12.392

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1168
rank avg (pred): 0.454 +- 0.137
mrr vals (pred, true): 0.043, 0.043
batch losses (mrrl, rdl): 0.0004438716, 4.01114e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 841
rank avg (pred): 0.480 +- 0.160
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 7.4432e-06, 3.57293e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 909
rank avg (pred): 0.333 +- 0.204
mrr vals (pred, true): 0.186, 0.139
batch losses (mrrl, rdl): 0.0212570373, 0.0002825618

Epoch over!
epoch time: 12.079

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 253
rank avg (pred): 0.137 +- 0.091
mrr vals (pred, true): 0.268, 0.271
batch losses (mrrl, rdl): 9.13131e-05, 3.69674e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 812
rank avg (pred): 0.270 +- 0.167
mrr vals (pred, true): 0.200, 0.175
batch losses (mrrl, rdl): 0.0058928169, 5.74358e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 901
rank avg (pred): 0.341 +- 0.210
mrr vals (pred, true): 0.204, 0.152
batch losses (mrrl, rdl): 0.027040327, 0.000248148

Epoch over!
epoch time: 12.154

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 403
rank avg (pred): 0.440 +- 0.153
mrr vals (pred, true): 0.061, 0.050
batch losses (mrrl, rdl): 0.0012720865, 3.70895e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 210
rank avg (pred): 0.446 +- 0.138
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 1.0481e-06, 5.83845e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 440
rank avg (pred): 0.458 +- 0.126
mrr vals (pred, true): 0.046, 0.040
batch losses (mrrl, rdl): 0.0001678837, 6.09502e-05

Epoch over!
epoch time: 12.148

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1118
rank avg (pred): 0.446 +- 0.134
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 6.0024e-06, 5.85901e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 829
rank avg (pred): 0.248 +- 0.150
mrr vals (pred, true): 0.214, 0.197
batch losses (mrrl, rdl): 0.0029639979, 4.48542e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 661
rank avg (pred): 0.444 +- 0.133
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001232869, 6.49907e-05

Epoch over!
epoch time: 12.13

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 486
rank avg (pred): 0.239 +- 0.143
mrr vals (pred, true): 0.201, 0.188
batch losses (mrrl, rdl): 0.0016829167, 0.0003096701

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1184
rank avg (pred): 0.464 +- 0.111
mrr vals (pred, true): 0.042, 0.044
batch losses (mrrl, rdl): 0.0006062508, 5.93451e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1117
rank avg (pred): 0.446 +- 0.122
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 3.3456e-05, 6.099e-05

Epoch over!
epoch time: 12.275

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 104
rank avg (pred): 0.453 +- 0.119
mrr vals (pred, true): 0.049, 0.054
batch losses (mrrl, rdl): 2.01987e-05, 6.80255e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 664
rank avg (pred): 0.447 +- 0.120
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 1.648e-07, 6.2685e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1204
rank avg (pred): 0.452 +- 0.114
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 8.3711e-06, 5.59293e-05

Epoch over!
epoch time: 12.037

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 420
rank avg (pred): 0.446 +- 0.120
mrr vals (pred, true): 0.053, 0.044
batch losses (mrrl, rdl): 0.0001010558, 5.5596e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 256
rank avg (pred): 0.166 +- 0.103
mrr vals (pred, true): 0.250, 0.295
batch losses (mrrl, rdl): 0.0203149486, 0.0001237872

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1133
rank avg (pred): 0.447 +- 0.111
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 3.9444e-06, 5.62847e-05

Epoch over!
epoch time: 12.131

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 682
rank avg (pred): 0.458 +- 0.096
mrr vals (pred, true): 0.042, 0.043
batch losses (mrrl, rdl): 0.0006293382, 6.06077e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1211
rank avg (pred): 0.451 +- 0.101
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 7.16105e-05, 7.64295e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 528
rank avg (pred): 0.272 +- 0.159
mrr vals (pred, true): 0.206, 0.217
batch losses (mrrl, rdl): 0.001297664, 0.0006167075

Epoch over!
epoch time: 12.222

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 593
rank avg (pred): 0.438 +- 0.102
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 6.3878e-06, 5.88901e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 254
rank avg (pred): 0.150 +- 0.096
mrr vals (pred, true): 0.279, 0.290
batch losses (mrrl, rdl): 0.0012040294, 3.68727e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 530
rank avg (pred): 0.276 +- 0.162
mrr vals (pred, true): 0.212, 0.217
batch losses (mrrl, rdl): 0.0002384302, 0.0004728818

Epoch over!
epoch time: 12.031

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.451 +- 0.099
mrr vals (pred, true): 0.049, 0.045

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   67 	     0 	 0.05060 	 0.03862 	 ~...
   32 	     1 	 0.04847 	 0.04134 	 ~...
   44 	     2 	 0.04919 	 0.04154 	 ~...
   35 	     3 	 0.04868 	 0.04154 	 ~...
   14 	     4 	 0.04744 	 0.04189 	 ~...
   78 	     5 	 0.05167 	 0.04197 	 ~...
   62 	     6 	 0.05034 	 0.04205 	 ~...
   31 	     7 	 0.04842 	 0.04214 	 ~...
   22 	     8 	 0.04794 	 0.04215 	 ~...
   16 	     9 	 0.04749 	 0.04224 	 ~...
   20 	    10 	 0.04789 	 0.04224 	 ~...
   69 	    11 	 0.05061 	 0.04225 	 ~...
   54 	    12 	 0.04978 	 0.04260 	 ~...
   38 	    13 	 0.04888 	 0.04282 	 ~...
   17 	    14 	 0.04752 	 0.04296 	 ~...
   57 	    15 	 0.04994 	 0.04302 	 ~...
   25 	    16 	 0.04811 	 0.04326 	 ~...
   60 	    17 	 0.05027 	 0.04328 	 ~...
    7 	    18 	 0.04679 	 0.04330 	 ~...
   26 	    19 	 0.04816 	 0.04333 	 ~...
    0 	    20 	 0.04452 	 0.04354 	 ~...
    5 	    21 	 0.04665 	 0.04357 	 ~...
   11 	    22 	 0.04716 	 0.04369 	 ~...
   85 	    23 	 0.05392 	 0.04369 	 ~...
   10 	    24 	 0.04692 	 0.04369 	 ~...
   36 	    25 	 0.04875 	 0.04381 	 ~...
   34 	    26 	 0.04862 	 0.04387 	 ~...
   29 	    27 	 0.04833 	 0.04390 	 ~...
   43 	    28 	 0.04907 	 0.04390 	 ~...
   49 	    29 	 0.04940 	 0.04399 	 ~...
   53 	    30 	 0.04975 	 0.04412 	 ~...
    9 	    31 	 0.04690 	 0.04415 	 ~...
   45 	    32 	 0.04926 	 0.04418 	 ~...
   50 	    33 	 0.04943 	 0.04418 	 ~...
   39 	    34 	 0.04890 	 0.04422 	 ~...
   21 	    35 	 0.04793 	 0.04438 	 ~...
   52 	    36 	 0.04953 	 0.04439 	 ~...
   33 	    37 	 0.04853 	 0.04454 	 ~...
   12 	    38 	 0.04722 	 0.04462 	 ~...
   13 	    39 	 0.04735 	 0.04462 	 ~...
   55 	    40 	 0.04978 	 0.04471 	 ~...
   46 	    41 	 0.04926 	 0.04476 	 ~...
    4 	    42 	 0.04660 	 0.04487 	 ~...
   41 	    43 	 0.04896 	 0.04494 	 ~...
    1 	    44 	 0.04598 	 0.04510 	 ~...
   72 	    45 	 0.05096 	 0.04517 	 ~...
   56 	    46 	 0.04992 	 0.04532 	 ~...
   75 	    47 	 0.05131 	 0.04533 	 ~...
   18 	    48 	 0.04754 	 0.04540 	 ~...
   65 	    49 	 0.05051 	 0.04542 	 ~...
   66 	    50 	 0.05060 	 0.04588 	 ~...
   15 	    51 	 0.04746 	 0.04609 	 ~...
   63 	    52 	 0.05036 	 0.04611 	 ~...
   82 	    53 	 0.05211 	 0.04644 	 ~...
   24 	    54 	 0.04809 	 0.04649 	 ~...
    3 	    55 	 0.04651 	 0.04672 	 ~...
   80 	    56 	 0.05184 	 0.04676 	 ~...
   51 	    57 	 0.04947 	 0.04708 	 ~...
   74 	    58 	 0.05129 	 0.04734 	 ~...
   76 	    59 	 0.05158 	 0.04748 	 ~...
   42 	    60 	 0.04904 	 0.04773 	 ~...
   71 	    61 	 0.05087 	 0.04780 	 ~...
   81 	    62 	 0.05201 	 0.04825 	 ~...
   48 	    63 	 0.04934 	 0.04829 	 ~...
   70 	    64 	 0.05073 	 0.04831 	 ~...
   40 	    65 	 0.04891 	 0.04832 	 ~...
   79 	    66 	 0.05178 	 0.04860 	 ~...
   64 	    67 	 0.05045 	 0.04895 	 ~...
   58 	    68 	 0.05015 	 0.04946 	 ~...
   23 	    69 	 0.04798 	 0.05011 	 ~...
   37 	    70 	 0.04876 	 0.05037 	 ~...
   77 	    71 	 0.05162 	 0.05129 	 ~...
   61 	    72 	 0.05034 	 0.05138 	 ~...
   28 	    73 	 0.04827 	 0.05144 	 ~...
   83 	    74 	 0.05280 	 0.05189 	 ~...
    8 	    75 	 0.04684 	 0.05291 	 ~...
   47 	    76 	 0.04930 	 0.05327 	 ~...
   59 	    77 	 0.05024 	 0.05335 	 ~...
   30 	    78 	 0.04839 	 0.05342 	 ~...
   68 	    79 	 0.05060 	 0.05402 	 ~...
    6 	    80 	 0.04674 	 0.05402 	 ~...
    2 	    81 	 0.04608 	 0.05451 	 ~...
   84 	    82 	 0.05355 	 0.05516 	 ~...
   27 	    83 	 0.04816 	 0.05665 	 ~...
   73 	    84 	 0.05125 	 0.05699 	 ~...
   19 	    85 	 0.04759 	 0.05791 	 ~...
   86 	    86 	 0.12500 	 0.10078 	 ~...
   87 	    87 	 0.13299 	 0.10354 	 ~...
   88 	    88 	 0.17424 	 0.13306 	 m..s
   88 	    89 	 0.17424 	 0.14461 	 ~...
   91 	    90 	 0.20914 	 0.20038 	 ~...
   92 	    91 	 0.21211 	 0.20918 	 ~...
   90 	    92 	 0.19426 	 0.21573 	 ~...
   95 	    93 	 0.23578 	 0.21918 	 ~...
   93 	    94 	 0.22517 	 0.22898 	 ~...
   94 	    95 	 0.23082 	 0.23675 	 ~...
   96 	    96 	 0.23764 	 0.24483 	 ~...
   99 	    97 	 0.26988 	 0.25490 	 ~...
  101 	    98 	 0.27851 	 0.25576 	 ~...
   97 	    99 	 0.25021 	 0.26302 	 ~...
  105 	   100 	 0.28198 	 0.26309 	 ~...
  103 	   101 	 0.28107 	 0.26580 	 ~...
  102 	   102 	 0.28100 	 0.26747 	 ~...
  108 	   103 	 0.28300 	 0.26891 	 ~...
  110 	   104 	 0.28562 	 0.27130 	 ~...
  113 	   105 	 0.28948 	 0.27483 	 ~...
  100 	   106 	 0.27692 	 0.27682 	 ~...
  107 	   107 	 0.28274 	 0.27807 	 ~...
  106 	   108 	 0.28221 	 0.28054 	 ~...
   98 	   109 	 0.26945 	 0.28112 	 ~...
  109 	   110 	 0.28494 	 0.28167 	 ~...
  116 	   111 	 0.29113 	 0.28497 	 ~...
  114 	   112 	 0.28999 	 0.28517 	 ~...
  112 	   113 	 0.28849 	 0.30791 	 ~...
  115 	   114 	 0.29006 	 0.30865 	 ~...
  119 	   115 	 0.32131 	 0.31102 	 ~...
  104 	   116 	 0.28113 	 0.31448 	 m..s
  117 	   117 	 0.31469 	 0.31732 	 ~...
  111 	   118 	 0.28833 	 0.32651 	 m..s
  120 	   119 	 0.32218 	 0.33857 	 ~...
  118 	   120 	 0.31982 	 0.34118 	 ~...
==========================================
r_mrr = 0.9947537779808044
r2_mrr = 0.9886301755905151
spearmanr_mrr@5 = 0.7599222660064697
spearmanr_mrr@10 = 0.8404271602630615
spearmanr_mrr@50 = 0.9930270314216614
spearmanr_mrr@100 = 0.9965086579322815
spearmanr_mrr@All = 0.9968017935752869
==========================================
test time: 0.404
Done Testing dataset Kinships
total time taken: 188.8169903755188
training time taken: 181.85165524482727
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9948)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9886)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.7599)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.8404)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9930)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9965)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9968)}}, 'test_loss': {'TransE': {'Kinships': 0.13016893740859814}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 948062385052008
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [52, 563, 1115, 45, 493, 121, 392, 1211, 732, 252, 113, 938, 804, 825, 316, 961, 372, 1046, 669, 998, 1142, 515, 75, 152, 109, 803, 66, 1044, 212, 1084, 56, 1026, 817, 810, 558, 20, 214, 882, 614, 1133, 446, 506, 885, 1078, 131, 1201, 51, 719, 657, 703, 267, 147, 780, 595, 649, 280, 712, 496, 687, 343, 42, 728, 61, 202, 776, 379, 1061, 1150, 1144, 327, 958, 888, 1207, 63, 1132, 996, 277, 3, 569, 620, 1176, 319, 122, 799, 1072, 824, 1036, 1168, 919, 291, 1148, 701, 352, 706, 502, 704, 869, 295, 254, 845, 1135, 371, 546, 838, 737, 135, 808, 668, 698, 555, 1100, 1165, 1068, 647, 491, 158, 30, 631, 178, 878, 950]
valid_ids (0): []
train_ids (1094): [500, 238, 453, 495, 169, 336, 643, 1017, 1178, 389, 835, 41, 822, 17, 222, 1103, 397, 46, 304, 809, 963, 854, 747, 76, 613, 1208, 307, 617, 261, 257, 1114, 263, 265, 849, 1092, 575, 659, 209, 628, 784, 11, 276, 1185, 433, 664, 33, 755, 576, 1000, 862, 1059, 997, 432, 60, 408, 978, 124, 1099, 931, 132, 739, 119, 157, 1164, 840, 912, 133, 847, 298, 440, 443, 801, 416, 449, 108, 35, 384, 800, 218, 635, 182, 421, 1153, 211, 904, 1121, 95, 74, 1094, 905, 383, 830, 1066, 522, 1199, 898, 557, 44, 989, 606, 237, 322, 105, 480, 125, 684, 516, 691, 476, 83, 346, 84, 577, 272, 895, 789, 1049, 964, 1110, 193, 565, 782, 40, 1005, 990, 299, 674, 87, 303, 475, 884, 920, 427, 771, 438, 472, 441, 1063, 781, 922, 1210, 976, 770, 354, 334, 466, 89, 1156, 163, 987, 350, 901, 320, 488, 189, 715, 1091, 879, 615, 127, 952, 451, 1180, 458, 72, 796, 155, 1206, 1183, 1116, 305, 481, 274, 399, 422, 842, 529, 1169, 477, 380, 536, 330, 654, 850, 177, 678, 638, 233, 827, 1031, 794, 924, 154, 752, 589, 370, 868, 607, 501, 112, 525, 1012, 543, 486, 936, 183, 791, 123, 85, 955, 857, 192, 130, 805, 1147, 911, 510, 297, 129, 767, 205, 559, 150, 411, 527, 623, 1124, 381, 311, 1122, 1024, 892, 420, 1073, 685, 361, 1071, 1111, 344, 957, 73, 413, 180, 772, 984, 253, 1161, 251, 754, 512, 511, 1038, 586, 347, 459, 484, 54, 1209, 1138, 726, 841, 889, 966, 926, 430, 220, 626, 1003, 677, 206, 795, 906, 262, 562, 896, 398, 1043, 1214, 1140, 593, 1011, 144, 1143, 673, 1196, 634, 695, 326, 191, 661, 1181, 848, 723, 1002, 391, 683, 540, 448, 549, 603, 315, 245, 627, 883, 203, 890, 282, 934, 337, 1014, 1001, 582, 579, 244, 533, 969, 139, 232, 0, 520, 531, 537, 294, 983, 743, 679, 705, 1193, 221, 975, 1204, 454, 1137, 948, 9, 710, 513, 368, 915, 14, 748, 839, 479, 1163, 769, 1159, 104, 436, 1055, 713, 418, 1057, 1170, 207, 289, 1083, 1076, 640, 844, 867, 736, 417, 1197, 229, 393, 259, 1009, 145, 1025, 786, 201, 619, 175, 483, 447, 338, 4, 1125, 914, 235, 973, 834, 1104, 339, 725, 28, 530, 716, 956, 226, 50, 1152, 1155, 463, 876, 55, 1128, 410, 128, 941, 474, 688, 714, 733, 67, 341, 740, 27, 700, 1139, 317, 57, 863, 377, 199, 1106, 407, 329, 310, 1096, 734, 991, 342, 1089, 821, 930, 746, 281, 699, 621, 762, 564, 1051, 351, 376, 1171, 137, 120, 903, 223, 916, 390, 273, 78, 553, 439, 107, 190, 751, 648, 65, 609, 18, 608, 897, 618, 818, 306, 1184, 1172, 856, 325, 694, 1020, 947, 25, 548, 697, 968, 764, 1047, 1157, 1162, 761, 13, 852, 759, 717, 159, 1175, 711, 395, 1149, 12, 424, 1117, 880, 219, 1119, 521, 881, 148, 596, 909, 833, 1056, 247, 798, 1041, 47, 605, 414, 806, 239, 32, 165, 1120, 940, 1213, 58, 836, 629, 318, 204, 581, 656, 749, 756, 541, 231, 1203, 887, 450, 434, 1205, 864, 375, 571, 331, 721, 652, 369, 362, 602, 665, 170, 918, 1015, 729, 181, 278, 1053, 210, 1032, 250, 526, 625, 1064, 524, 290, 19, 161, 270, 401, 302, 788, 610, 22, 374, 1037, 913, 323, 1190, 1018, 999, 15, 572, 16, 444, 508, 300, 758, 946, 1167, 1, 279, 1022, 213, 140, 1028, 927, 312, 256, 908, 875, 283, 689, 1045, 954, 452, 561, 168, 340, 872, 1195, 1200, 101, 1075, 172, 651, 470, 599, 1006, 1189, 1202, 959, 388, 773, 967, 765, 1013, 404, 349, 675, 23, 923, 1112, 591, 1188, 308, 935, 5, 1198, 523, 859, 972, 142, 921, 69, 632, 249, 598, 36, 425, 1040, 116, 81, 409, 179, 284, 38, 1035, 542, 151, 509, 636, 62, 663, 951, 473, 1007, 1130, 1192, 637, 702, 489, 1182, 1126, 566, 1146, 91, 241, 583, 240, 584, 456, 980, 387, 1050, 1131, 230, 217, 43, 1151, 853, 503, 24, 960, 680, 568, 939, 497, 1158, 587, 768, 787, 823, 21, 29, 738, 492, 1166, 1101, 365, 103, 811, 90, 266, 328, 394, 136, 953, 482, 932, 1033, 925, 39, 720, 594, 1085, 1090, 1023, 550, 670, 622, 80, 666, 471, 630, 271, 866, 832, 464, 658, 532, 692, 797, 578, 910, 945, 560, 1065, 505, 611, 356, 707, 146, 624, 828, 763, 1102, 248, 268, 469, 861, 1098, 774, 644, 186, 332, 727, 70, 184, 360, 173, 745, 1145, 645, 1093, 1008, 551, 402, 224, 1067, 153, 400, 461, 646, 79, 1019, 415, 367, 197, 164, 275, 816, 141, 1034, 815, 200, 92, 994, 777, 301, 544, 208, 855, 460, 48, 775, 364, 499, 264, 378, 676, 545, 26, 287, 366, 457, 236, 877, 288, 187, 313, 321, 837, 539, 353, 465, 196, 437, 1134, 1129, 333, 783, 7, 98, 445, 385, 1212, 977, 6, 345, 82, 494, 215, 985, 900, 633, 462, 406, 1021, 874, 528, 348, 292, 807, 94, 485, 731, 742, 382, 655, 53, 672, 616, 1187, 363, 138, 1095, 870, 242, 386, 110, 943, 753, 760, 735, 1179, 1113, 1127, 355, 100, 357, 696, 1029, 162, 1077, 194, 426, 309, 419, 650, 1173, 59, 722, 965, 405, 792, 143, 64, 937, 682, 156, 902, 744, 335, 1186, 1105, 612, 690, 1069, 126, 871, 8, 819, 592, 49, 31, 1088, 974, 1016, 653, 174, 1048, 258, 829, 894, 730, 97, 504, 1039, 1141, 814, 1107, 358, 917, 490, 435, 986, 1154, 99, 293, 865, 428, 724, 962, 891, 114, 68, 234, 933, 71, 641, 662, 296, 1010, 944, 547, 1062, 570, 979, 285, 802, 552, 228, 2, 766, 554, 1027, 149, 468, 535, 660, 77, 779, 899, 403, 1097, 324, 102, 314, 860, 1109, 246, 255, 517, 667, 1160, 34, 970, 111, 718, 1074, 167, 518, 873, 1030, 198, 995, 580, 507, 604, 227, 590, 442, 1082, 1080, 534, 498, 639, 686, 992, 971, 812, 118, 574, 1004, 750, 269, 431, 681, 597, 1194, 981, 826, 556, 86, 478, 519, 928, 1177, 982, 1087, 1136, 886, 243, 429, 1123, 1052, 37, 1054, 793, 708, 538, 93, 117, 588, 600, 396, 567, 893, 671, 1060, 693, 1081, 487, 757, 942, 843, 846, 929, 166, 216, 195, 188, 88, 790, 160, 949, 709, 907, 1058, 467, 831, 642, 601, 1042, 820, 373, 96, 10, 115, 785, 412, 514, 1118, 813, 359, 286, 455, 741, 993, 1079, 176, 171, 851, 1174, 423, 778, 1070, 988, 858, 134, 185, 1086, 1191, 106, 1108, 260, 225, 585, 573]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4941401439333306
the save name prefix for this run is:  chkpt-ID_4941401439333306_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1093
rank avg (pred): 0.500 +- 0.002
mrr vals (pred, true): 0.019, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001804779

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 928
rank avg (pred): 0.443 +- 0.266
mrr vals (pred, true): 0.096, 0.043
batch losses (mrrl, rdl): 0.0, 5.4799e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1067
rank avg (pred): 0.101 +- 0.079
mrr vals (pred, true): 0.257, 0.336
batch losses (mrrl, rdl): 0.0, 2.6317e-06

Epoch over!
epoch time: 12.114

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 145
rank avg (pred): 0.429 +- 0.266
mrr vals (pred, true): 0.103, 0.052
batch losses (mrrl, rdl): 0.0, 1.4066e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 360
rank avg (pred): 0.453 +- 0.262
mrr vals (pred, true): 0.084, 0.053
batch losses (mrrl, rdl): 0.0, 4.74277e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 986
rank avg (pred): 0.102 +- 0.078
mrr vals (pred, true): 0.250, 0.332
batch losses (mrrl, rdl): 0.0, 2.8959e-06

Epoch over!
epoch time: 11.721

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 992
rank avg (pred): 0.098 +- 0.075
mrr vals (pred, true): 0.250, 0.304
batch losses (mrrl, rdl): 0.0, 2.7657e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 250
rank avg (pred): 0.101 +- 0.079
mrr vals (pred, true): 0.260, 0.269
batch losses (mrrl, rdl): 0.0, 3.5815e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 322
rank avg (pred): 0.108 +- 0.082
mrr vals (pred, true): 0.246, 0.276
batch losses (mrrl, rdl): 0.0, 4.2953e-06

Epoch over!
epoch time: 12.011

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 242
rank avg (pred): 0.439 +- 0.260
mrr vals (pred, true): 0.089, 0.046
batch losses (mrrl, rdl): 0.0, 6.9993e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 741
rank avg (pred): 0.161 +- 0.125
mrr vals (pred, true): 0.215, 0.249
batch losses (mrrl, rdl): 0.0, 9.0328e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 285
rank avg (pred): 0.092 +- 0.071
mrr vals (pred, true): 0.256, 0.322
batch losses (mrrl, rdl): 0.0, 2.4914e-06

Epoch over!
epoch time: 11.855

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 175
rank avg (pred): 0.444 +- 0.266
mrr vals (pred, true): 0.085, 0.048
batch losses (mrrl, rdl): 0.0, 2.0874e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1003
rank avg (pred): 0.395 +- 0.274
mrr vals (pred, true): 0.125, 0.051
batch losses (mrrl, rdl): 0.0, 1.38305e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 206
rank avg (pred): 0.432 +- 0.259
mrr vals (pred, true): 0.081, 0.046
batch losses (mrrl, rdl): 0.0, 2.7575e-06

Epoch over!
epoch time: 11.891

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 229
rank avg (pred): 0.439 +- 0.258
mrr vals (pred, true): 0.076, 0.041
batch losses (mrrl, rdl): 0.006776385, 1.08548e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 33
rank avg (pred): 0.107 +- 0.082
mrr vals (pred, true): 0.252, 0.293
batch losses (mrrl, rdl): 0.0171034075, 3.3314e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 210
rank avg (pred): 0.526 +- 0.183
mrr vals (pred, true): 0.042, 0.042
batch losses (mrrl, rdl): 0.0006246875, 9.82344e-05

Epoch over!
epoch time: 12.322

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 596
rank avg (pred): 0.491 +- 0.184
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.22003e-05, 7.50435e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1081
rank avg (pred): 0.522 +- 0.181
mrr vals (pred, true): 0.044, 0.047
batch losses (mrrl, rdl): 0.0003331038, 0.0001510744

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 482
rank avg (pred): 0.487 +- 0.158
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 8.17275e-05, 3.87217e-05

Epoch over!
epoch time: 12.096

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 516
rank avg (pred): 0.158 +- 0.116
mrr vals (pred, true): 0.215, 0.223
batch losses (mrrl, rdl): 0.0006250115, 5.36438e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 538
rank avg (pred): 0.140 +- 0.107
mrr vals (pred, true): 0.251, 0.198
batch losses (mrrl, rdl): 0.0271671191, 1.16461e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 765
rank avg (pred): 0.462 +- 0.161
mrr vals (pred, true): 0.058, 0.047
batch losses (mrrl, rdl): 0.0006314643, 3.52221e-05

Epoch over!
epoch time: 11.941

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1184
rank avg (pred): 0.456 +- 0.143
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 7.8987e-06, 4.40071e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 87
rank avg (pred): 0.445 +- 0.144
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 2.472e-06, 4.21596e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 889
rank avg (pred): 0.493 +- 0.152
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001311329, 4.68957e-05

Epoch over!
epoch time: 11.943

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 175
rank avg (pred): 0.490 +- 0.121
mrr vals (pred, true): 0.038, 0.048
batch losses (mrrl, rdl): 0.0014338826, 7.70027e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 857
rank avg (pred): 0.464 +- 0.140
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 1.806e-07, 4.20464e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 707
rank avg (pred): 0.466 +- 0.123
mrr vals (pred, true): 0.044, 0.047
batch losses (mrrl, rdl): 0.0003908618, 5.29312e-05

Epoch over!
epoch time: 11.975

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 665
rank avg (pred): 0.434 +- 0.129
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 2.9207e-06, 6.36225e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1096
rank avg (pred): 0.449 +- 0.153
mrr vals (pred, true): 0.058, 0.055
batch losses (mrrl, rdl): 0.0006336924, 6.30729e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1188
rank avg (pred): 0.410 +- 0.122
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 9.65445e-05, 0.0001015417

Epoch over!
epoch time: 12.26

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 656
rank avg (pred): 0.443 +- 0.130
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 3.8195e-06, 5.34169e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 247
rank avg (pred): 0.103 +- 0.076
mrr vals (pred, true): 0.291, 0.266
batch losses (mrrl, rdl): 0.0059657772, 2.7015e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 688
rank avg (pred): 0.458 +- 0.130
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 1.8923e-06, 4.54679e-05

Epoch over!
epoch time: 11.999

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 312
rank avg (pred): 0.112 +- 0.082
mrr vals (pred, true): 0.278, 0.298
batch losses (mrrl, rdl): 0.0042547402, 1.53406e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 759
rank avg (pred): 0.475 +- 0.124
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.0002199022, 5.36832e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 820
rank avg (pred): 0.188 +- 0.134
mrr vals (pred, true): 0.222, 0.183
batch losses (mrrl, rdl): 0.0151531752, 9.69979e-05

Epoch over!
epoch time: 11.994

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 418
rank avg (pred): 0.429 +- 0.120
mrr vals (pred, true): 0.053, 0.044
batch losses (mrrl, rdl): 0.0001009618, 6.99798e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 916
rank avg (pred): 0.218 +- 0.147
mrr vals (pred, true): 0.204, 0.227
batch losses (mrrl, rdl): 0.0053345873, 0.0001310962

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 81
rank avg (pred): 0.401 +- 0.105
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 6.80937e-05, 0.0001212125

Epoch over!
epoch time: 12.149

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 347
rank avg (pred): 0.412 +- 0.111
mrr vals (pred, true): 0.054, 0.056
batch losses (mrrl, rdl): 0.0001449079, 5.64833e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 785
rank avg (pred): 0.473 +- 0.115
mrr vals (pred, true): 0.044, 0.043
batch losses (mrrl, rdl): 0.000420064, 5.6267e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 201
rank avg (pred): 0.429 +- 0.115
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 8.82072e-05, 0.0001012271

Epoch over!
epoch time: 12.021

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.113 +- 0.078
mrr vals (pred, true): 0.270, 0.257

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04411 	 0.04062 	 ~...
   10 	     1 	 0.04479 	 0.04092 	 ~...
   37 	     2 	 0.04732 	 0.04160 	 ~...
    1 	     3 	 0.04341 	 0.04209 	 ~...
   34 	     4 	 0.04696 	 0.04224 	 ~...
   17 	     5 	 0.04589 	 0.04224 	 ~...
   35 	     6 	 0.04696 	 0.04225 	 ~...
   69 	     7 	 0.05049 	 0.04233 	 ~...
   20 	     8 	 0.04604 	 0.04284 	 ~...
   47 	     9 	 0.04780 	 0.04285 	 ~...
    6 	    10 	 0.04415 	 0.04286 	 ~...
   38 	    11 	 0.04746 	 0.04290 	 ~...
   24 	    12 	 0.04644 	 0.04296 	 ~...
   23 	    13 	 0.04640 	 0.04298 	 ~...
   68 	    14 	 0.05039 	 0.04299 	 ~...
   50 	    15 	 0.04798 	 0.04307 	 ~...
   16 	    16 	 0.04586 	 0.04310 	 ~...
   60 	    17 	 0.04909 	 0.04328 	 ~...
   28 	    18 	 0.04661 	 0.04330 	 ~...
   36 	    19 	 0.04728 	 0.04343 	 ~...
   31 	    20 	 0.04667 	 0.04353 	 ~...
   65 	    21 	 0.04988 	 0.04355 	 ~...
   49 	    22 	 0.04795 	 0.04362 	 ~...
   32 	    23 	 0.04674 	 0.04363 	 ~...
    4 	    24 	 0.04388 	 0.04389 	 ~...
   61 	    25 	 0.04936 	 0.04399 	 ~...
   72 	    26 	 0.05194 	 0.04432 	 ~...
   27 	    27 	 0.04658 	 0.04433 	 ~...
   29 	    28 	 0.04662 	 0.04436 	 ~...
    7 	    29 	 0.04419 	 0.04436 	 ~...
   15 	    30 	 0.04579 	 0.04437 	 ~...
   30 	    31 	 0.04664 	 0.04463 	 ~...
   66 	    32 	 0.05027 	 0.04466 	 ~...
    3 	    33 	 0.04365 	 0.04476 	 ~...
   44 	    34 	 0.04771 	 0.04487 	 ~...
   12 	    35 	 0.04493 	 0.04496 	 ~...
   70 	    36 	 0.05122 	 0.04514 	 ~...
   18 	    37 	 0.04593 	 0.04517 	 ~...
   40 	    38 	 0.04752 	 0.04522 	 ~...
    0 	    39 	 0.04027 	 0.04538 	 ~...
   22 	    40 	 0.04631 	 0.04562 	 ~...
   71 	    41 	 0.05143 	 0.04576 	 ~...
   52 	    42 	 0.04820 	 0.04588 	 ~...
    2 	    43 	 0.04363 	 0.04607 	 ~...
   67 	    44 	 0.05035 	 0.04612 	 ~...
   26 	    45 	 0.04657 	 0.04616 	 ~...
   11 	    46 	 0.04480 	 0.04665 	 ~...
   46 	    47 	 0.04773 	 0.04677 	 ~...
   48 	    48 	 0.04782 	 0.04678 	 ~...
   55 	    49 	 0.04855 	 0.04680 	 ~...
   19 	    50 	 0.04597 	 0.04704 	 ~...
   39 	    51 	 0.04750 	 0.04748 	 ~...
   59 	    52 	 0.04905 	 0.04791 	 ~...
    9 	    53 	 0.04473 	 0.04798 	 ~...
   21 	    54 	 0.04621 	 0.04845 	 ~...
   13 	    55 	 0.04578 	 0.04858 	 ~...
    8 	    56 	 0.04470 	 0.04871 	 ~...
   62 	    57 	 0.04948 	 0.04905 	 ~...
   64 	    58 	 0.04973 	 0.04914 	 ~...
   53 	    59 	 0.04841 	 0.04973 	 ~...
   45 	    60 	 0.04773 	 0.05037 	 ~...
   54 	    61 	 0.04854 	 0.05039 	 ~...
   14 	    62 	 0.04578 	 0.05149 	 ~...
   58 	    63 	 0.04870 	 0.05214 	 ~...
   56 	    64 	 0.04855 	 0.05231 	 ~...
   41 	    65 	 0.04754 	 0.05265 	 ~...
   25 	    66 	 0.04657 	 0.05269 	 ~...
   33 	    67 	 0.04696 	 0.05392 	 ~...
   57 	    68 	 0.04858 	 0.05448 	 ~...
   51 	    69 	 0.04809 	 0.05494 	 ~...
   43 	    70 	 0.04763 	 0.05512 	 ~...
   42 	    71 	 0.04755 	 0.05665 	 ~...
   63 	    72 	 0.04963 	 0.05818 	 ~...
   74 	    73 	 0.07282 	 0.10280 	 ~...
   73 	    74 	 0.06102 	 0.10354 	 m..s
   79 	    75 	 0.20548 	 0.17928 	 ~...
   76 	    76 	 0.19600 	 0.20570 	 ~...
   77 	    77 	 0.19688 	 0.20813 	 ~...
   78 	    78 	 0.20037 	 0.20921 	 ~...
   80 	    79 	 0.20916 	 0.21232 	 ~...
   85 	    80 	 0.22211 	 0.22106 	 ~...
   83 	    81 	 0.21462 	 0.22128 	 ~...
   75 	    82 	 0.15996 	 0.22266 	 m..s
   81 	    83 	 0.21076 	 0.22278 	 ~...
   90 	    84 	 0.24408 	 0.22608 	 ~...
   86 	    85 	 0.22222 	 0.22618 	 ~...
   87 	    86 	 0.23202 	 0.22898 	 ~...
   82 	    87 	 0.21180 	 0.23396 	 ~...
   84 	    88 	 0.22068 	 0.23675 	 ~...
   88 	    89 	 0.23720 	 0.25579 	 ~...
  100 	    90 	 0.26977 	 0.25742 	 ~...
   91 	    91 	 0.25272 	 0.26184 	 ~...
  103 	    92 	 0.27483 	 0.26274 	 ~...
  111 	    93 	 0.28320 	 0.26491 	 ~...
   93 	    94 	 0.26211 	 0.26639 	 ~...
   89 	    95 	 0.23854 	 0.26647 	 ~...
  107 	    96 	 0.27830 	 0.26761 	 ~...
  109 	    97 	 0.27985 	 0.26867 	 ~...
   94 	    98 	 0.26469 	 0.26867 	 ~...
   97 	    99 	 0.26736 	 0.27126 	 ~...
   92 	   100 	 0.26197 	 0.27682 	 ~...
   99 	   101 	 0.26837 	 0.27725 	 ~...
  113 	   102 	 0.28925 	 0.27873 	 ~...
  108 	   103 	 0.27870 	 0.28228 	 ~...
  106 	   104 	 0.27696 	 0.28601 	 ~...
  105 	   105 	 0.27619 	 0.28964 	 ~...
  102 	   106 	 0.27087 	 0.28965 	 ~...
  114 	   107 	 0.29253 	 0.29265 	 ~...
   95 	   108 	 0.26513 	 0.29300 	 ~...
  101 	   109 	 0.27007 	 0.29354 	 ~...
  110 	   110 	 0.28291 	 0.29455 	 ~...
   98 	   111 	 0.26790 	 0.29653 	 ~...
  112 	   112 	 0.28794 	 0.30063 	 ~...
  104 	   113 	 0.27577 	 0.30319 	 ~...
  115 	   114 	 0.31477 	 0.30903 	 ~...
   96 	   115 	 0.26724 	 0.31448 	 m..s
  116 	   116 	 0.34129 	 0.32456 	 ~...
  119 	   117 	 0.36130 	 0.32959 	 m..s
  117 	   118 	 0.34844 	 0.33373 	 ~...
  118 	   119 	 0.35690 	 0.33701 	 ~...
  120 	   120 	 0.36308 	 0.35877 	 ~...
==========================================
r_mrr = 0.9931463003158569
r2_mrr = 0.9858104586601257
spearmanr_mrr@5 = 0.8027487397193909
spearmanr_mrr@10 = 0.9387876987457275
spearmanr_mrr@50 = 0.9902679324150085
spearmanr_mrr@100 = 0.996947169303894
spearmanr_mrr@All = 0.9972748160362244
==========================================
test time: 0.415
Done Testing dataset Kinships
total time taken: 187.80770254135132
training time taken: 180.78845977783203
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9931)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9858)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.8027)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9388)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9903)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9969)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9973)}}, 'test_loss': {'TransE': {'Kinships': 0.21856943739840062}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 9858336502325712
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [669, 902, 1097, 203, 713, 906, 518, 192, 608, 315, 1199, 391, 71, 875, 561, 501, 860, 486, 724, 1063, 722, 149, 1212, 502, 579, 631, 1075, 411, 202, 1057, 92, 265, 793, 781, 924, 765, 842, 858, 1117, 488, 225, 56, 616, 605, 434, 178, 673, 69, 309, 487, 297, 215, 206, 901, 218, 1038, 310, 1109, 721, 507, 459, 29, 878, 95, 1125, 515, 566, 460, 755, 195, 967, 868, 278, 792, 437, 657, 907, 954, 370, 670, 600, 876, 861, 83, 830, 469, 201, 412, 426, 895, 466, 1163, 963, 700, 144, 745, 809, 395, 814, 267, 828, 464, 243, 893, 462, 934, 946, 942, 645, 98, 142, 485, 1011, 1072, 472, 153, 748, 1150, 372, 30, 768]
valid_ids (0): []
train_ids (1094): [870, 899, 743, 692, 877, 448, 183, 815, 603, 774, 408, 479, 266, 14, 270, 163, 1160, 343, 523, 800, 1197, 898, 834, 879, 1034, 498, 617, 483, 984, 357, 1012, 1053, 24, 1171, 602, 286, 1052, 970, 232, 1189, 23, 275, 332, 799, 969, 950, 953, 449, 402, 806, 689, 513, 1149, 49, 885, 965, 42, 574, 628, 1143, 156, 709, 606, 188, 443, 1062, 543, 569, 549, 827, 551, 918, 496, 87, 222, 558, 582, 126, 846, 396, 429, 346, 1156, 362, 941, 504, 1136, 711, 1030, 5, 1067, 398, 481, 1024, 554, 474, 478, 880, 480, 161, 750, 1152, 935, 919, 2, 634, 982, 347, 445, 575, 340, 921, 1095, 511, 734, 703, 1081, 1180, 625, 1146, 604, 9, 1015, 1201, 1008, 116, 550, 720, 679, 832, 913, 1157, 714, 674, 307, 399, 761, 312, 678, 980, 304, 939, 118, 185, 824, 651, 512, 471, 923, 74, 263, 299, 430, 510, 912, 291, 88, 848, 620, 699, 650, 268, 432, 205, 955, 1035, 306, 4, 1124, 1004, 1089, 756, 50, 84, 695, 60, 936, 305, 335, 938, 859, 422, 123, 1202, 991, 1209, 593, 509, 311, 811, 331, 122, 820, 114, 843, 764, 138, 117, 476, 247, 903, 995, 482, 668, 500, 237, 612, 302, 53, 21, 294, 663, 998, 420, 562, 255, 889, 945, 572, 624, 327, 567, 1166, 1019, 1059, 719, 588, 986, 1168, 130, 872, 18, 613, 647, 896, 1006, 406, 216, 129, 882, 1190, 1003, 660, 1086, 388, 102, 694, 390, 590, 12, 580, 182, 837, 548, 1040, 696, 121, 254, 491, 922, 1206, 51, 367, 1056, 214, 1014, 345, 1179, 160, 497, 184, 1016, 319, 470, 702, 1042, 32, 869, 637, 57, 687, 1069, 55, 850, 988, 855, 1064, 823, 981, 1126, 654, 221, 968, 622, 990, 517, 1093, 38, 900, 704, 556, 63, 1214, 355, 667, 423, 808, 17, 379, 1147, 979, 169, 829, 72, 888, 971, 147, 145, 47, 1107, 812, 492, 839, 1151, 1060, 1017, 1049, 973, 65, 409, 905, 1174, 150, 235, 977, 863, 818, 1087, 997, 494, 931, 552, 1065, 1001, 256, 405, 1158, 1043, 547, 112, 13, 317, 174, 397, 104, 1205, 328, 26, 752, 228, 619, 585, 929, 797, 382, 61, 1007, 531, 926, 966, 508, 717, 181, 68, 887, 167, 538, 961, 801, 1113, 200, 521, 1144, 15, 441, 454, 289, 1025, 753, 760, 911, 1023, 959, 1132, 1196, 648, 48, 248, 1133, 707, 716, 97, 690, 366, 854, 323, 630, 639, 693, 665, 822, 394, 447, 640, 833, 1031, 34, 686, 28, 380, 1140, 140, 641, 1137, 164, 524, 136, 360, 656, 723, 344, 1118, 300, 1200, 1068, 27, 659, 1208, 350, 37, 1029, 363, 199, 1164, 1111, 826, 555, 632, 1120, 177, 279, 762, 77, 175, 996, 601, 1002, 1047, 573, 993, 1013, 975, 1092, 1198, 168, 36, 537, 303, 64, 577, 473, 635, 733, 281, 546, 985, 318, 728, 1080, 739, 751, 179, 1203, 428, 207, 1128, 949, 416, 952, 1050, 1055, 769, 100, 134, 359, 223, 599, 847, 614, 457, 607, 726, 333, 621, 227, 994, 269, 308, 1195, 514, 587, 103, 992, 789, 1175, 831, 1058, 414, 819, 892, 76, 239, 1121, 280, 1073, 782, 962, 701, 1061, 285, 948, 727, 598, 111, 718, 576, 381, 204, 1142, 940, 784, 146, 570, 532, 909, 356, 298, 11, 463, 45, 529, 377, 339, 1108, 1114, 475, 770, 410, 386, 749, 987, 85, 1018, 320, 871, 712, 817, 1022, 1182, 240, 874, 856, 807, 93, 638, 1032, 932, 224, 143, 341, 1103, 530, 626, 533, 964, 754, 920, 1210, 1041, 535, 106, 794, 1186, 385, 477, 219, 1167, 708, 86, 883, 226, 503, 453, 383, 506, 208, 737, 115, 771, 257, 245, 705, 70, 1194, 788, 795, 776, 132, 732, 729, 786, 220, 465, 780, 1123, 862, 908, 43, 425, 545, 39, 1110, 66, 615, 1161, 455, 1054, 1066, 611, 539, 337, 1187, 736, 779, 450, 1102, 1071, 231, 44, 119, 1094, 623, 804, 99, 542, 418, 387, 241, 1122, 461, 544, 260, 1207, 435, 958, 1099, 401, 1005, 564, 677, 1009, 857, 35, 59, 917, 933, 522, 1115, 91, 519, 54, 568, 1155, 353, 1173, 400, 1076, 1172, 1105, 681, 775, 864, 1048, 790, 125, 629, 277, 253, 609, 292, 419, 1010, 904, 1193, 415, 189, 1165, 886, 928, 944, 375, 351, 671, 661, 490, 565, 698, 664, 951, 1169, 421, 1159, 796, 747, 960, 259, 1176, 284, 595, 643, 1083, 155, 213, 338, 683, 676, 915, 162, 433, 436, 1039, 943, 326, 493, 772, 198, 738, 983, 282, 322, 1036, 691, 389, 1085, 1138, 252, 79, 1192, 652, 133, 778, 374, 499, 209, 293, 413, 1037, 427, 364, 1082, 1027, 1116, 314, 1026, 851, 31, 378, 742, 384, 1112, 440, 276, 894, 107, 816, 589, 321, 844, 658, 1045, 1070, 246, 316, 528, 1145, 777, 873, 516, 1020, 444, 1000, 417, 348, 1088, 1129, 75, 526, 685, 290, 947, 194, 1046, 810, 261, 890, 6, 8, 821, 586, 706, 120, 1183, 190, 358, 937, 594, 866, 697, 1101, 16, 1106, 110, 158, 1170, 791, 525, 557, 766, 249, 210, 881, 352, 62, 578, 897, 193, 196, 141, 757, 151, 197, 1211, 217, 1021, 655, 242, 1074, 78, 313, 258, 1091, 1077, 759, 113, 1188, 1104, 135, 1028, 489, 730, 541, 1134, 90, 58, 744, 914, 458, 369, 295, 803, 20, 646, 283, 105, 19, 715, 581, 725, 767, 81, 186, 1084, 956, 930, 1191, 735, 740, 927, 439, 584, 627, 244, 288, 468, 731, 3, 392, 999, 336, 1178, 1100, 233, 972, 989, 271, 1153, 52, 325, 324, 746, 108, 139, 636, 7, 211, 368, 234, 424, 925, 82, 22, 838, 349, 301, 451, 853, 171, 741, 484, 666, 505, 373, 137, 840, 10, 1162, 456, 152, 527, 33, 841, 559, 978, 553, 802, 371, 236, 1033, 642, 1130, 644, 1096, 852, 672, 773, 849, 296, 976, 212, 835, 1135, 1177, 495, 25, 407, 262, 73, 540, 404, 675, 124, 89, 273, 1204, 1098, 571, 758, 365, 618, 798, 610, 452, 287, 159, 393, 597, 264, 1119, 910, 446, 230, 891, 128, 633, 682, 127, 1127, 591, 1, 361, 191, 0, 520, 865, 1148, 131, 67, 354, 957, 251, 680, 867, 154, 274, 534, 172, 46, 109, 41, 166, 688, 1044, 825, 229, 176, 442, 836, 165, 653, 1139, 763, 157, 330, 974, 438, 560, 467, 101, 1131, 1079, 805, 329, 148, 334, 80, 649, 1154, 813, 170, 180, 563, 596, 884, 238, 250, 376, 536, 710, 173, 1181, 1051, 1185, 96, 592, 1184, 684, 845, 662, 787, 783, 403, 785, 916, 94, 1090, 583, 1078, 272, 431, 1213, 187, 40, 342, 1141]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2738235999805460
the save name prefix for this run is:  chkpt-ID_2738235999805460_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1110
rank avg (pred): 0.534 +- 0.003
mrr vals (pred, true): 0.018, 0.045
batch losses (mrrl, rdl): 0.0, 0.0001740195

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 778
rank avg (pred): 0.433 +- 0.269
mrr vals (pred, true): 0.125, 0.045
batch losses (mrrl, rdl): 0.0, 3.8785e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 345
rank avg (pred): 0.457 +- 0.255
mrr vals (pred, true): 0.089, 0.049
batch losses (mrrl, rdl): 0.0, 2.43162e-05

Epoch over!
epoch time: 11.883

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 461
rank avg (pred): 0.423 +- 0.260
mrr vals (pred, true): 0.109, 0.044
batch losses (mrrl, rdl): 0.0, 1.51185e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1174
rank avg (pred): 0.464 +- 0.264
mrr vals (pred, true): 0.074, 0.049
batch losses (mrrl, rdl): 0.0, 1.13688e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 0
rank avg (pred): 0.322 +- 0.248
mrr vals (pred, true): 0.142, 0.276
batch losses (mrrl, rdl): 0.0, 0.0011467274

Epoch over!
epoch time: 11.91

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1200
rank avg (pred): 0.446 +- 0.266
mrr vals (pred, true): 0.081, 0.042
batch losses (mrrl, rdl): 0.0, 2.3906e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 735
rank avg (pred): 0.231 +- 0.190
mrr vals (pred, true): 0.168, 0.107
batch losses (mrrl, rdl): 0.0, 6.02374e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 174
rank avg (pred): 0.436 +- 0.260
mrr vals (pred, true): 0.074, 0.044
batch losses (mrrl, rdl): 0.0, 1.11302e-05

Epoch over!
epoch time: 11.833

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 840
rank avg (pred): 0.456 +- 0.264
mrr vals (pred, true): 0.064, 0.045
batch losses (mrrl, rdl): 0.0, 8.365e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 801
rank avg (pred): 0.475 +- 0.255
mrr vals (pred, true): 0.048, 0.044
batch losses (mrrl, rdl): 0.0, 5.5219e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1091
rank avg (pred): 0.438 +- 0.263
mrr vals (pred, true): 0.071, 0.052
batch losses (mrrl, rdl): 0.0, 4.3203e-06

Epoch over!
epoch time: 11.774

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 911
rank avg (pred): 0.163 +- 0.162
mrr vals (pred, true): 0.234, 0.162
batch losses (mrrl, rdl): 0.0, 0.0001158019

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 784
rank avg (pred): 0.474 +- 0.269
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 0.0, 2.551e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 816
rank avg (pred): 0.223 +- 0.216
mrr vals (pred, true): 0.200, 0.101
batch losses (mrrl, rdl): 0.0, 0.000105943

Epoch over!
epoch time: 11.924

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 569
rank avg (pred): 0.439 +- 0.262
mrr vals (pred, true): 0.069, 0.044
batch losses (mrrl, rdl): 0.0035445099, 2.0668e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 393
rank avg (pred): 0.456 +- 0.184
mrr vals (pred, true): 0.050, 0.051
batch losses (mrrl, rdl): 2.3229e-06, 3.34596e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 91
rank avg (pred): 0.429 +- 0.144
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002280802, 4.53519e-05

Epoch over!
epoch time: 12.181

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 564
rank avg (pred): 0.229 +- 0.173
mrr vals (pred, true): 0.214, 0.220
batch losses (mrrl, rdl): 0.0003528154, 0.0003548462

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 490
rank avg (pred): 0.232 +- 0.169
mrr vals (pred, true): 0.215, 0.192
batch losses (mrrl, rdl): 0.0051959329, 0.0002954342

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1166
rank avg (pred): 0.451 +- 0.133
mrr vals (pred, true): 0.041, 0.042
batch losses (mrrl, rdl): 0.000806988, 5.02673e-05

Epoch over!
epoch time: 11.933

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 543
rank avg (pred): 0.246 +- 0.169
mrr vals (pred, true): 0.200, 0.217
batch losses (mrrl, rdl): 0.0028981999, 0.0004122862

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 970
rank avg (pred): 0.445 +- 0.143
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 7.5014e-06, 4.76636e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1142
rank avg (pred): 0.153 +- 0.110
mrr vals (pred, true): 0.258, 0.294
batch losses (mrrl, rdl): 0.0123649202, 5.31192e-05

Epoch over!
epoch time: 12.019

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 615
rank avg (pred): 0.447 +- 0.141
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 9.46e-08, 5.0013e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1180
rank avg (pred): 0.460 +- 0.127
mrr vals (pred, true): 0.037, 0.044
batch losses (mrrl, rdl): 0.0016823339, 4.56211e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 181
rank avg (pred): 0.464 +- 0.122
mrr vals (pred, true): 0.036, 0.046
batch losses (mrrl, rdl): 0.0020874562, 5.62323e-05

Epoch over!
epoch time: 11.979

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 147
rank avg (pred): 0.436 +- 0.150
mrr vals (pred, true): 0.059, 0.053
batch losses (mrrl, rdl): 0.0007995212, 4.42656e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 212
rank avg (pred): 0.451 +- 0.135
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 0.0001001932, 5.10104e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 306
rank avg (pred): 0.174 +- 0.125
mrr vals (pred, true): 0.287, 0.315
batch losses (mrrl, rdl): 0.0075607486, 0.000177222

Epoch over!
epoch time: 12.005

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1168
rank avg (pred): 0.444 +- 0.139
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 1.1967e-05, 4.06213e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1179
rank avg (pred): 0.451 +- 0.133
mrr vals (pred, true): 0.047, 0.048
batch losses (mrrl, rdl): 9.83579e-05, 5.07869e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 418
rank avg (pred): 0.449 +- 0.134
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 8.6011e-06, 4.87159e-05

Epoch over!
epoch time: 11.996

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 161
rank avg (pred): 0.450 +- 0.131
mrr vals (pred, true): 0.046, 0.055
batch losses (mrrl, rdl): 0.0001362136, 7.42041e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 710
rank avg (pred): 0.452 +- 0.127
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0003118282, 5.38016e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 756
rank avg (pred): 0.444 +- 0.134
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 2.23373e-05, 5.75162e-05

Epoch over!
epoch time: 12.029

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1062
rank avg (pred): 0.074 +- 0.051
mrr vals (pred, true): 0.353, 0.305
batch losses (mrrl, rdl): 0.0237338599, 1.97594e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 343
rank avg (pred): 0.446 +- 0.130
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 0.0001080617, 4.97714e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 369
rank avg (pred): 0.452 +- 0.124
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003259523, 6.17338e-05

Epoch over!
epoch time: 11.999

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 598
rank avg (pred): 0.447 +- 0.130
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 2.36e-08, 4.87191e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 28
rank avg (pred): 0.201 +- 0.132
mrr vals (pred, true): 0.254, 0.260
batch losses (mrrl, rdl): 0.0004511524, 0.000215202

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 207
rank avg (pred): 0.456 +- 0.119
mrr vals (pred, true): 0.042, 0.045
batch losses (mrrl, rdl): 0.0005634112, 5.37497e-05

Epoch over!
epoch time: 11.968

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 614
rank avg (pred): 0.442 +- 0.130
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.62234e-05, 4.73234e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1014
rank avg (pred): 0.445 +- 0.131
mrr vals (pred, true): 0.051, 0.057
batch losses (mrrl, rdl): 2.9951e-06, 5.56022e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 969
rank avg (pred): 0.443 +- 0.131
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 8.47992e-05, 4.6093e-05

Epoch over!
epoch time: 11.981

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.454 +- 0.117
mrr vals (pred, true): 0.046, 0.044

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   37 	     0 	 0.04730 	 0.04071 	 ~...
    6 	     1 	 0.04586 	 0.04133 	 ~...
   18 	     2 	 0.04648 	 0.04134 	 ~...
   65 	     3 	 0.04833 	 0.04222 	 ~...
    4 	     4 	 0.04573 	 0.04225 	 ~...
   34 	     5 	 0.04710 	 0.04239 	 ~...
   35 	     6 	 0.04711 	 0.04245 	 ~...
   73 	     7 	 0.04879 	 0.04264 	 ~...
   33 	     8 	 0.04708 	 0.04269 	 ~...
   61 	     9 	 0.04823 	 0.04271 	 ~...
   40 	    10 	 0.04731 	 0.04273 	 ~...
    8 	    11 	 0.04595 	 0.04290 	 ~...
   43 	    12 	 0.04739 	 0.04296 	 ~...
   72 	    13 	 0.04864 	 0.04302 	 ~...
   15 	    14 	 0.04631 	 0.04303 	 ~...
    9 	    15 	 0.04599 	 0.04310 	 ~...
   28 	    16 	 0.04692 	 0.04333 	 ~...
   71 	    17 	 0.04857 	 0.04335 	 ~...
   19 	    18 	 0.04650 	 0.04341 	 ~...
    5 	    19 	 0.04581 	 0.04346 	 ~...
   38 	    20 	 0.04730 	 0.04358 	 ~...
   29 	    21 	 0.04697 	 0.04383 	 ~...
   25 	    22 	 0.04676 	 0.04392 	 ~...
   27 	    23 	 0.04683 	 0.04399 	 ~...
   14 	    24 	 0.04630 	 0.04402 	 ~...
   22 	    25 	 0.04665 	 0.04407 	 ~...
   23 	    26 	 0.04666 	 0.04412 	 ~...
   59 	    27 	 0.04816 	 0.04416 	 ~...
    2 	    28 	 0.04543 	 0.04418 	 ~...
   36 	    29 	 0.04724 	 0.04426 	 ~...
   69 	    30 	 0.04845 	 0.04429 	 ~...
   12 	    31 	 0.04623 	 0.04433 	 ~...
   56 	    32 	 0.04782 	 0.04434 	 ~...
   11 	    33 	 0.04605 	 0.04434 	 ~...
   76 	    34 	 0.04944 	 0.04438 	 ~...
   52 	    35 	 0.04763 	 0.04451 	 ~...
    7 	    36 	 0.04595 	 0.04456 	 ~...
    3 	    37 	 0.04567 	 0.04460 	 ~...
   20 	    38 	 0.04662 	 0.04475 	 ~...
   60 	    39 	 0.04817 	 0.04495 	 ~...
    1 	    40 	 0.04510 	 0.04496 	 ~...
   41 	    41 	 0.04732 	 0.04510 	 ~...
   64 	    42 	 0.04831 	 0.04516 	 ~...
   13 	    43 	 0.04623 	 0.04517 	 ~...
   48 	    44 	 0.04747 	 0.04522 	 ~...
   10 	    45 	 0.04604 	 0.04526 	 ~...
   24 	    46 	 0.04674 	 0.04547 	 ~...
   50 	    47 	 0.04749 	 0.04549 	 ~...
   39 	    48 	 0.04731 	 0.04551 	 ~...
   32 	    49 	 0.04704 	 0.04570 	 ~...
   31 	    50 	 0.04699 	 0.04571 	 ~...
   58 	    51 	 0.04811 	 0.04586 	 ~...
   49 	    52 	 0.04749 	 0.04598 	 ~...
   17 	    53 	 0.04647 	 0.04602 	 ~...
   21 	    54 	 0.04663 	 0.04611 	 ~...
   44 	    55 	 0.04740 	 0.04627 	 ~...
   45 	    56 	 0.04741 	 0.04628 	 ~...
   16 	    57 	 0.04642 	 0.04713 	 ~...
   46 	    58 	 0.04743 	 0.04722 	 ~...
   63 	    59 	 0.04829 	 0.04723 	 ~...
    0 	    60 	 0.04482 	 0.04726 	 ~...
   30 	    61 	 0.04698 	 0.04728 	 ~...
   57 	    62 	 0.04809 	 0.04746 	 ~...
   51 	    63 	 0.04760 	 0.04748 	 ~...
   26 	    64 	 0.04682 	 0.04772 	 ~...
   78 	    65 	 0.05012 	 0.04809 	 ~...
   80 	    66 	 0.05027 	 0.04831 	 ~...
   68 	    67 	 0.04838 	 0.04890 	 ~...
   77 	    68 	 0.04974 	 0.04908 	 ~...
   74 	    69 	 0.04880 	 0.04914 	 ~...
   55 	    70 	 0.04778 	 0.05030 	 ~...
   70 	    71 	 0.04856 	 0.05034 	 ~...
   54 	    72 	 0.04775 	 0.05220 	 ~...
   66 	    73 	 0.04835 	 0.05280 	 ~...
   67 	    74 	 0.04837 	 0.05335 	 ~...
   53 	    75 	 0.04773 	 0.05354 	 ~...
   62 	    76 	 0.04823 	 0.05435 	 ~...
   47 	    77 	 0.04743 	 0.05791 	 ~...
   42 	    78 	 0.04737 	 0.05853 	 ~...
   75 	    79 	 0.04908 	 0.05973 	 ~...
   79 	    80 	 0.05025 	 0.06201 	 ~...
   84 	    81 	 0.18560 	 0.14461 	 m..s
   83 	    82 	 0.18316 	 0.15183 	 m..s
   81 	    83 	 0.17232 	 0.15474 	 ~...
   81 	    84 	 0.17232 	 0.17397 	 ~...
   91 	    85 	 0.21411 	 0.18190 	 m..s
   89 	    86 	 0.21133 	 0.18775 	 ~...
   97 	    87 	 0.22692 	 0.19222 	 m..s
   99 	    88 	 0.22789 	 0.19455 	 m..s
   86 	    89 	 0.20886 	 0.20271 	 ~...
   93 	    90 	 0.21789 	 0.20450 	 ~...
   92 	    91 	 0.21535 	 0.20813 	 ~...
   88 	    92 	 0.21064 	 0.21544 	 ~...
   87 	    93 	 0.20976 	 0.21573 	 ~...
   90 	    94 	 0.21154 	 0.21959 	 ~...
   96 	    95 	 0.22600 	 0.22106 	 ~...
   85 	    96 	 0.19366 	 0.22189 	 ~...
   94 	    97 	 0.22314 	 0.22245 	 ~...
   98 	    98 	 0.22711 	 0.22418 	 ~...
  100 	    99 	 0.23143 	 0.22564 	 ~...
   95 	   100 	 0.22530 	 0.22906 	 ~...
  101 	   101 	 0.24321 	 0.26225 	 ~...
  103 	   102 	 0.26729 	 0.26491 	 ~...
  108 	   103 	 0.29219 	 0.26557 	 ~...
  107 	   104 	 0.28922 	 0.26747 	 ~...
  111 	   105 	 0.29444 	 0.27120 	 ~...
  104 	   106 	 0.28216 	 0.27126 	 ~...
  102 	   107 	 0.24475 	 0.27723 	 m..s
  109 	   108 	 0.29293 	 0.28547 	 ~...
  110 	   109 	 0.29295 	 0.28573 	 ~...
  112 	   110 	 0.29544 	 0.28601 	 ~...
  113 	   111 	 0.29544 	 0.28704 	 ~...
  115 	   112 	 0.30053 	 0.29257 	 ~...
  106 	   113 	 0.28798 	 0.29285 	 ~...
  114 	   114 	 0.29855 	 0.30137 	 ~...
  105 	   115 	 0.28590 	 0.30319 	 ~...
  119 	   116 	 0.32261 	 0.30903 	 ~...
  118 	   117 	 0.32222 	 0.31102 	 ~...
  116 	   118 	 0.31415 	 0.31588 	 ~...
  120 	   119 	 0.32482 	 0.35525 	 m..s
  117 	   120 	 0.31998 	 0.35540 	 m..s
==========================================
r_mrr = 0.9937020540237427
r2_mrr = 0.9866459369659424
spearmanr_mrr@5 = 0.7271842360496521
spearmanr_mrr@10 = 0.8340972065925598
spearmanr_mrr@50 = 0.9908034205436707
spearmanr_mrr@100 = 0.9964644312858582
spearmanr_mrr@All = 0.9968060255050659
==========================================
test time: 0.387
Done Testing dataset Kinships
total time taken: 186.782484292984
training time taken: 179.88357830047607
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9937)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9866)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.7272)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.8341)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9908)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9965)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9968)}}, 'test_loss': {'TransE': {'Kinships': 0.1649926184072683}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 6359678256679415
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [156, 391, 36, 1155, 121, 1187, 999, 1034, 1181, 324, 188, 671, 462, 944, 611, 297, 1180, 982, 496, 447, 419, 1031, 838, 595, 120, 954, 701, 325, 186, 1154, 1020, 823, 454, 368, 437, 897, 131, 817, 689, 957, 431, 989, 1096, 376, 597, 607, 445, 500, 942, 15, 974, 0, 1116, 1205, 894, 330, 1092, 1179, 364, 946, 947, 92, 1213, 916, 220, 173, 669, 930, 414, 800, 489, 977, 223, 962, 66, 226, 1055, 876, 920, 562, 711, 196, 369, 1168, 713, 275, 696, 970, 1015, 477, 1138, 641, 510, 211, 442, 1069, 1152, 1114, 745, 1172, 287, 521, 776, 443, 598, 314, 83, 773, 584, 1073, 1178, 896, 106, 923, 51, 43, 491, 268, 108, 482, 812]
valid_ids (0): []
train_ids (1094): [241, 70, 996, 1170, 787, 664, 316, 245, 82, 1087, 453, 1021, 1029, 400, 506, 875, 224, 871, 841, 1214, 222, 501, 702, 603, 933, 55, 700, 516, 546, 918, 1177, 900, 811, 628, 261, 52, 682, 380, 1001, 1129, 310, 663, 1059, 929, 69, 980, 899, 184, 303, 1058, 960, 183, 1210, 1127, 113, 828, 752, 786, 76, 483, 834, 1191, 1119, 1186, 569, 735, 623, 435, 968, 1048, 104, 1078, 657, 71, 22, 1130, 789, 809, 588, 362, 547, 613, 356, 215, 97, 652, 1123, 550, 433, 1, 146, 136, 739, 886, 1153, 851, 227, 1192, 631, 599, 311, 925, 98, 724, 1079, 1085, 344, 940, 1109, 45, 803, 457, 539, 878, 1202, 848, 943, 733, 1124, 499, 480, 290, 1035, 978, 101, 881, 615, 1016, 677, 116, 185, 528, 128, 137, 570, 783, 446, 403, 819, 887, 888, 1063, 1131, 844, 243, 276, 401, 246, 214, 648, 775, 704, 873, 493, 201, 208, 794, 1195, 165, 206, 1097, 1062, 267, 958, 190, 632, 707, 20, 481, 537, 1004, 771, 279, 38, 198, 7, 1141, 1041, 80, 1006, 144, 62, 41, 193, 1198, 505, 110, 509, 1167, 544, 565, 530, 764, 636, 488, 59, 976, 1142, 814, 6, 908, 859, 93, 174, 740, 766, 922, 522, 541, 1188, 710, 622, 604, 4, 822, 567, 804, 605, 111, 549, 606, 469, 553, 868, 421, 114, 430, 529, 601, 757, 465, 1082, 308, 690, 1045, 955, 336, 647, 1189, 885, 312, 813, 1136, 913, 177, 396, 898, 1089, 969, 591, 381, 1158, 142, 837, 640, 1139, 354, 777, 127, 576, 479, 399, 200, 154, 1011, 272, 1060, 265, 755, 239, 164, 406, 869, 65, 721, 972, 355, 644, 466, 238, 596, 586, 1106, 321, 260, 1212, 133, 1022, 744, 797, 1080, 81, 346, 117, 827, 398, 722, 616, 438, 262, 367, 747, 1203, 357, 1057, 358, 750, 1171, 450, 1132, 404, 731, 984, 1113, 649, 788, 459, 1200, 1100, 29, 484, 155, 737, 329, 497, 793, 534, 274, 973, 449, 902, 618, 458, 248, 74, 691, 831, 33, 633, 934, 545, 37, 441, 1128, 542, 444, 273, 552, 202, 75, 759, 416, 327, 1013, 912, 17, 1145, 455, 170, 257, 407, 624, 180, 763, 12, 1046, 536, 351, 298, 1182, 194, 383, 19, 798, 289, 317, 687, 883, 256, 667, 609, 96, 994, 945, 335, 820, 983, 694, 374, 328, 627, 157, 953, 285, 219, 315, 293, 408, 1122, 1149, 385, 697, 1135, 593, 845, 100, 986, 134, 10, 1094, 746, 1039, 1077, 924, 866, 387, 1126, 612, 642, 11, 532, 94, 26, 575, 1017, 77, 284, 1206, 1076, 686, 28, 580, 270, 386, 212, 566, 191, 784, 1027, 1208, 1099, 761, 865, 574, 266, 130, 125, 698, 46, 1143, 1000, 343, 168, 361, 626, 388, 533, 91, 805, 1144, 394, 807, 931, 60, 742, 675, 296, 653, 993, 397, 334, 867, 1061, 205, 513, 429, 830, 1081, 34, 617, 720, 825, 518, 850, 332, 84, 228, 145, 204, 1169, 507, 1010, 579, 1091, 614, 1088, 756, 1014, 451, 171, 1108, 242, 112, 995, 1147, 234, 880, 600, 651, 122, 233, 1201, 309, 393, 427, 439, 941, 1024, 872, 678, 30, 555, 857, 1086, 322, 48, 86, 1042, 231, 709, 568, 487, 411, 105, 808, 864, 765, 326, 835, 950, 304, 910, 192, 281, 554, 475, 412, 723, 662, 585, 410, 935, 1185, 621, 774, 858, 870, 129, 1121, 217, 1117, 187, 163, 79, 514, 464, 1084, 693, 939, 1005, 695, 1023, 502, 107, 685, 743, 178, 492, 1211, 1043, 578, 990, 420, 258, 561, 1162, 89, 147, 1008, 556, 378, 716, 668, 1065, 402, 1018, 754, 1009, 625, 1047, 413, 829, 1196, 305, 405, 861, 213, 207, 688, 816, 959, 1066, 460, 810, 31, 1003, 132, 253, 997, 975, 49, 679, 660, 948, 684, 904, 998, 225, 1054, 821, 307, 1049, 99, 353, 581, 699, 151, 926, 758, 478, 467, 727, 166, 849, 463, 269, 892, 1007, 508, 801, 149, 21, 348, 791, 313, 1174, 473, 152, 118, 160, 889, 650, 1183, 981, 379, 956, 681, 1103, 349, 712, 719, 153, 543, 61, 254, 729, 879, 474, 1125, 1072, 515, 594, 1093, 717, 538, 85, 123, 815, 498, 619, 210, 189, 306, 674, 5, 1075, 854, 961, 434, 571, 655, 425, 703, 490, 862, 1151, 54, 748, 1019, 264, 162, 503, 159, 175, 842, 68, 418, 288, 179, 1071, 906, 963, 852, 964, 782, 736, 301, 58, 1115, 1107, 24, 139, 796, 1052, 422, 135, 181, 115, 47, 1118, 428, 161, 331, 371, 337, 790, 832, 979, 472, 1150, 610, 1012, 860, 339, 456, 384, 526, 250, 563, 903, 531, 1038, 582, 88, 72, 3, 914, 486, 732, 1209, 847, 646, 1051, 148, 1173, 436, 333, 893, 656, 389, 32, 523, 323, 1161, 666, 1184, 319, 665, 1175, 373, 318, 40, 286, 560, 35, 551, 63, 971, 221, 67, 767, 252, 768, 1050, 635, 240, 1159, 991, 350, 18, 1105, 937, 370, 527, 638, 440, 714, 1194, 919, 320, 158, 251, 395, 965, 87, 525, 377, 1193, 1090, 1204, 780, 199, 583, 952, 927, 520, 27, 1033, 1163, 928, 726, 366, 172, 1040, 392, 890, 590, 9, 1104, 209, 360, 363, 230, 683, 938, 1137, 124, 840, 706, 282, 278, 195, 654, 1032, 1056, 375, 915, 461, 751, 417, 1037, 573, 572, 494, 218, 372, 102, 470, 203, 728, 432, 874, 589, 42, 818, 352, 1101, 283, 577, 967, 517, 138, 907, 56, 448, 424, 833, 8, 1074, 291, 1025, 1166, 884, 645, 824, 895, 672, 150, 409, 16, 738, 1026, 1134, 1146, 1111, 216, 909, 390, 64, 1028, 778, 519, 843, 1067, 197, 587, 237, 236, 863, 1070, 345, 634, 753, 1064, 452, 620, 249, 1190, 1165, 13, 271, 504, 1036, 932, 770, 592, 167, 708, 103, 769, 839, 73, 921, 300, 176, 799, 426, 846, 548, 949, 95, 244, 1197, 856, 294, 643, 109, 44, 535, 725, 779, 53, 715, 1140, 608, 1030, 50, 1164, 781, 365, 302, 877, 232, 673, 966, 1083, 119, 1120, 1044, 90, 229, 559, 540, 676, 512, 1157, 141, 826, 718, 182, 806, 1199, 2, 39, 1068, 987, 476, 126, 468, 78, 341, 936, 734, 25, 415, 1160, 340, 485, 760, 658, 295, 901, 670, 1133, 347, 169, 853, 1148, 951, 382, 630, 855, 629, 692, 564, 342, 905, 785, 795, 988, 1053, 637, 1207, 495, 802, 1110, 259, 792, 1095, 1176, 1156, 772, 992, 1112, 680, 235, 639, 1098, 57, 661, 836, 292, 882, 762, 247, 705, 524, 557, 659, 140, 359, 255, 749, 280, 730, 299, 14, 143, 891, 911, 423, 558, 917, 741, 985, 263, 277, 511, 1102, 1002, 23, 471, 338, 602]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5698259655636195
the save name prefix for this run is:  chkpt-ID_5698259655636195_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 520
rank avg (pred): 0.497 +- 0.013
mrr vals (pred, true): 0.019, 0.201
batch losses (mrrl, rdl): 0.0, 0.0029286118

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 950
rank avg (pred): 0.474 +- 0.266
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 0.0, 7.1627e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 345
rank avg (pred): 0.457 +- 0.251
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0, 1.53149e-05

Epoch over!
epoch time: 11.833

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 272
rank avg (pred): 0.099 +- 0.095
mrr vals (pred, true): 0.208, 0.287
batch losses (mrrl, rdl): 0.0, 1.35918e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 703
rank avg (pred): 0.448 +- 0.263
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 0.0, 1.2226e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 201
rank avg (pred): 0.466 +- 0.263
mrr vals (pred, true): 0.043, 0.041
batch losses (mrrl, rdl): 0.0, 1.7906e-06

Epoch over!
epoch time: 11.824

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1010
rank avg (pred): 0.454 +- 0.260
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0, 9.1578e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 259
rank avg (pred): 0.104 +- 0.134
mrr vals (pred, true): 0.218, 0.263
batch losses (mrrl, rdl): 0.0, 2.683e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 935
rank avg (pred): 0.452 +- 0.264
mrr vals (pred, true): 0.048, 0.046
batch losses (mrrl, rdl): 0.0, 4.634e-07

Epoch over!
epoch time: 11.877

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 159
rank avg (pred): 0.472 +- 0.257
mrr vals (pred, true): 0.038, 0.049
batch losses (mrrl, rdl): 0.0, 2.04608e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 565
rank avg (pred): 0.110 +- 0.138
mrr vals (pred, true): 0.195, 0.200
batch losses (mrrl, rdl): 0.0, 3.8677e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 79
rank avg (pred): 0.095 +- 0.129
mrr vals (pred, true): 0.215, 0.266
batch losses (mrrl, rdl): 0.0, 2.552e-07

Epoch over!
epoch time: 11.828

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 924
rank avg (pred): 0.450 +- 0.260
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 0.0, 8.488e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1027
rank avg (pred): 0.428 +- 0.279
mrr vals (pred, true): 0.071, 0.043
batch losses (mrrl, rdl): 0.0, 1.22863e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 841
rank avg (pred): 0.487 +- 0.258
mrr vals (pred, true): 0.037, 0.043
batch losses (mrrl, rdl): 0.0, 3.8691e-06

Epoch over!
epoch time: 11.838

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 289
rank avg (pred): 0.100 +- 0.137
mrr vals (pred, true): 0.218, 0.286
batch losses (mrrl, rdl): 0.0465721227, 5.431e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1174
rank avg (pred): 0.497 +- 0.247
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 3.39497e-05, 6.59347e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 244
rank avg (pred): 0.055 +- 0.031
mrr vals (pred, true): 0.259, 0.260
batch losses (mrrl, rdl): 1.04548e-05, 5.73191e-05

Epoch over!
epoch time: 12.159

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 4
rank avg (pred): 0.055 +- 0.033
mrr vals (pred, true): 0.272, 0.262
batch losses (mrrl, rdl): 0.0010609881, 6.15072e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 84
rank avg (pred): 0.460 +- 0.212
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 1.82105e-05, 4.01825e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 887
rank avg (pred): 0.467 +- 0.203
mrr vals (pred, true): 0.045, 0.047
batch losses (mrrl, rdl): 0.0002450613, 2.43887e-05

Epoch over!
epoch time: 12.238

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 808
rank avg (pred): 0.503 +- 0.221
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 5.68281e-05, 4.55057e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 204
rank avg (pred): 0.425 +- 0.192
mrr vals (pred, true): 0.056, 0.042
batch losses (mrrl, rdl): 0.0003330371, 4.67728e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1113
rank avg (pred): 0.428 +- 0.176
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.000130518, 5.97231e-05

Epoch over!
epoch time: 12.059

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 174
rank avg (pred): 0.428 +- 0.179
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 1.8028e-06, 4.75921e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 847
rank avg (pred): 0.446 +- 0.183
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 2.14664e-05, 3.55671e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1140
rank avg (pred): 0.108 +- 0.067
mrr vals (pred, true): 0.242, 0.289
batch losses (mrrl, rdl): 0.0227149203, 3.4481e-06

Epoch over!
epoch time: 12.152

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 301
rank avg (pred): 0.077 +- 0.047
mrr vals (pred, true): 0.268, 0.270
batch losses (mrrl, rdl): 2.78991e-05, 1.9651e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 553
rank avg (pred): 0.126 +- 0.077
mrr vals (pred, true): 0.223, 0.211
batch losses (mrrl, rdl): 0.0015085116, 6.4676e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1197
rank avg (pred): 0.427 +- 0.161
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0001414272, 4.99422e-05

Epoch over!
epoch time: 12.118

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 835
rank avg (pred): 0.121 +- 0.076
mrr vals (pred, true): 0.241, 0.272
batch losses (mrrl, rdl): 0.0094401473, 1.91081e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 647
rank avg (pred): 0.524 +- 0.219
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 1.42654e-05, 0.0002339907

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1066
rank avg (pred): 0.065 +- 0.040
mrr vals (pred, true): 0.298, 0.347
batch losses (mrrl, rdl): 0.0244024545, 2.16843e-05

Epoch over!
epoch time: 12.244

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 405
rank avg (pred): 0.438 +- 0.167
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 6.4506e-06, 4.87981e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 249
rank avg (pred): 0.097 +- 0.060
mrr vals (pred, true): 0.258, 0.275
batch losses (mrrl, rdl): 0.003215448, 4.1107e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1021
rank avg (pred): 0.400 +- 0.163
mrr vals (pred, true): 0.062, 0.061
batch losses (mrrl, rdl): 0.0015035458, 3.5919e-05

Epoch over!
epoch time: 12.134

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 933
rank avg (pred): 0.431 +- 0.170
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001304448, 4.23769e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 792
rank avg (pred): 0.416 +- 0.167
mrr vals (pred, true): 0.063, 0.043
batch losses (mrrl, rdl): 0.0016526895, 6.8258e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 547
rank avg (pred): 0.206 +- 0.121
mrr vals (pred, true): 0.191, 0.184
batch losses (mrrl, rdl): 0.0004718812, 0.000152127

Epoch over!
epoch time: 12.098

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 560
rank avg (pred): 0.136 +- 0.083
mrr vals (pred, true): 0.236, 0.233
batch losses (mrrl, rdl): 0.0001016533, 6.8165e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 782
rank avg (pred): 0.416 +- 0.151
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 3.136e-05, 7.41683e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 343
rank avg (pred): 0.432 +- 0.150
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0002010824, 3.67624e-05

Epoch over!
epoch time: 12.268

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 609
rank avg (pred): 0.463 +- 0.173
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 1.64149e-05, 4.37101e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 747
rank avg (pred): 0.188 +- 0.116
mrr vals (pred, true): 0.230, 0.197
batch losses (mrrl, rdl): 0.0111024994, 4.75009e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 905
rank avg (pred): 0.364 +- 0.213
mrr vals (pred, true): 0.171, 0.197
batch losses (mrrl, rdl): 0.0065926677, 0.0008241173

Epoch over!
epoch time: 12.14

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.424 +- 0.160
mrr vals (pred, true): 0.055, 0.049

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.05164 	 0.03958 	 ~...
   51 	     1 	 0.05433 	 0.04075 	 ~...
   17 	     2 	 0.05151 	 0.04092 	 ~...
   34 	     3 	 0.05279 	 0.04100 	 ~...
   25 	     4 	 0.05199 	 0.04137 	 ~...
    1 	     5 	 0.04830 	 0.04151 	 ~...
    0 	     6 	 0.04764 	 0.04197 	 ~...
    9 	     7 	 0.05067 	 0.04207 	 ~...
   56 	     8 	 0.05507 	 0.04222 	 ~...
   78 	     9 	 0.05896 	 0.04223 	 ~...
   20 	    10 	 0.05167 	 0.04231 	 ~...
   80 	    11 	 0.05989 	 0.04234 	 ~...
    4 	    12 	 0.05052 	 0.04245 	 ~...
   46 	    13 	 0.05378 	 0.04265 	 ~...
   28 	    14 	 0.05230 	 0.04269 	 ~...
   30 	    15 	 0.05241 	 0.04284 	 ~...
   48 	    16 	 0.05398 	 0.04285 	 ~...
   21 	    17 	 0.05184 	 0.04306 	 ~...
   74 	    18 	 0.05663 	 0.04311 	 ~...
   40 	    19 	 0.05307 	 0.04328 	 ~...
   72 	    20 	 0.05624 	 0.04342 	 ~...
   29 	    21 	 0.05233 	 0.04362 	 ~...
   15 	    22 	 0.05134 	 0.04384 	 ~...
   18 	    23 	 0.05154 	 0.04385 	 ~...
   55 	    24 	 0.05476 	 0.04387 	 ~...
   12 	    25 	 0.05082 	 0.04388 	 ~...
   65 	    26 	 0.05564 	 0.04392 	 ~...
    7 	    27 	 0.05062 	 0.04393 	 ~...
   71 	    28 	 0.05604 	 0.04408 	 ~...
   42 	    29 	 0.05323 	 0.04412 	 ~...
    6 	    30 	 0.05062 	 0.04415 	 ~...
   26 	    31 	 0.05220 	 0.04432 	 ~...
   68 	    32 	 0.05578 	 0.04433 	 ~...
   44 	    33 	 0.05349 	 0.04434 	 ~...
   38 	    34 	 0.05296 	 0.04450 	 ~...
   47 	    35 	 0.05388 	 0.04463 	 ~...
    5 	    36 	 0.05053 	 0.04473 	 ~...
    3 	    37 	 0.04869 	 0.04474 	 ~...
   62 	    38 	 0.05538 	 0.04480 	 ~...
   37 	    39 	 0.05296 	 0.04485 	 ~...
   11 	    40 	 0.05080 	 0.04498 	 ~...
   82 	    41 	 0.06047 	 0.04503 	 ~...
   10 	    42 	 0.05072 	 0.04510 	 ~...
   75 	    43 	 0.05683 	 0.04553 	 ~...
   14 	    44 	 0.05109 	 0.04558 	 ~...
   16 	    45 	 0.05135 	 0.04571 	 ~...
   43 	    46 	 0.05333 	 0.04578 	 ~...
   45 	    47 	 0.05378 	 0.04586 	 ~...
    2 	    48 	 0.04863 	 0.04604 	 ~...
   76 	    49 	 0.05783 	 0.04611 	 ~...
   50 	    50 	 0.05417 	 0.04629 	 ~...
   60 	    51 	 0.05532 	 0.04639 	 ~...
   77 	    52 	 0.05808 	 0.04659 	 ~...
   49 	    53 	 0.05401 	 0.04677 	 ~...
   41 	    54 	 0.05316 	 0.04680 	 ~...
   79 	    55 	 0.05918 	 0.04694 	 ~...
   73 	    56 	 0.05632 	 0.04700 	 ~...
    8 	    57 	 0.05066 	 0.04704 	 ~...
   53 	    58 	 0.05461 	 0.04734 	 ~...
   35 	    59 	 0.05281 	 0.04747 	 ~...
   52 	    60 	 0.05454 	 0.04785 	 ~...
   83 	    61 	 0.06169 	 0.04831 	 ~...
   13 	    62 	 0.05089 	 0.04835 	 ~...
   23 	    63 	 0.05192 	 0.04858 	 ~...
   81 	    64 	 0.06000 	 0.04860 	 ~...
   54 	    65 	 0.05472 	 0.04890 	 ~...
   36 	    66 	 0.05296 	 0.04910 	 ~...
   61 	    67 	 0.05533 	 0.04915 	 ~...
   31 	    68 	 0.05245 	 0.05025 	 ~...
   59 	    69 	 0.05526 	 0.05034 	 ~...
   39 	    70 	 0.05304 	 0.05086 	 ~...
   32 	    71 	 0.05274 	 0.05093 	 ~...
   70 	    72 	 0.05602 	 0.05144 	 ~...
   66 	    73 	 0.05572 	 0.05199 	 ~...
   64 	    74 	 0.05551 	 0.05295 	 ~...
   27 	    75 	 0.05225 	 0.05392 	 ~...
   33 	    76 	 0.05275 	 0.05448 	 ~...
   58 	    77 	 0.05526 	 0.05460 	 ~...
   57 	    78 	 0.05509 	 0.05473 	 ~...
   67 	    79 	 0.05574 	 0.05531 	 ~...
   63 	    80 	 0.05548 	 0.05627 	 ~...
   69 	    81 	 0.05584 	 0.05699 	 ~...
   24 	    82 	 0.05193 	 0.05806 	 ~...
   22 	    83 	 0.05189 	 0.05853 	 ~...
   84 	    84 	 0.17276 	 0.09735 	 m..s
   87 	    85 	 0.19071 	 0.10280 	 m..s
   88 	    86 	 0.19220 	 0.17534 	 ~...
   84 	    87 	 0.17276 	 0.18038 	 ~...
   84 	    88 	 0.17276 	 0.18648 	 ~...
   95 	    89 	 0.22583 	 0.20380 	 ~...
   93 	    90 	 0.22023 	 0.20570 	 ~...
   91 	    91 	 0.21936 	 0.21095 	 ~...
   94 	    92 	 0.22507 	 0.21172 	 ~...
   92 	    93 	 0.21948 	 0.21326 	 ~...
   97 	    94 	 0.23451 	 0.22192 	 ~...
   96 	    95 	 0.22615 	 0.22278 	 ~...
   90 	    96 	 0.21462 	 0.22671 	 ~...
  100 	    97 	 0.25623 	 0.24331 	 ~...
  102 	    98 	 0.26014 	 0.24336 	 ~...
   89 	    99 	 0.21310 	 0.25569 	 m..s
   99 	   100 	 0.24947 	 0.26123 	 ~...
   98 	   101 	 0.24763 	 0.26225 	 ~...
  101 	   102 	 0.25633 	 0.26493 	 ~...
  105 	   103 	 0.27827 	 0.26893 	 ~...
  109 	   104 	 0.28730 	 0.27037 	 ~...
  108 	   105 	 0.28278 	 0.27120 	 ~...
  107 	   106 	 0.28255 	 0.27130 	 ~...
  103 	   107 	 0.26934 	 0.27591 	 ~...
  113 	   108 	 0.30041 	 0.27906 	 ~...
  112 	   109 	 0.30033 	 0.28527 	 ~...
  111 	   110 	 0.29929 	 0.29455 	 ~...
  104 	   111 	 0.27370 	 0.29708 	 ~...
  114 	   112 	 0.31431 	 0.29730 	 ~...
  116 	   113 	 0.31986 	 0.29789 	 ~...
  110 	   114 	 0.29682 	 0.30411 	 ~...
  119 	   115 	 0.33170 	 0.30577 	 ~...
  106 	   116 	 0.27947 	 0.31448 	 m..s
  117 	   117 	 0.32146 	 0.31464 	 ~...
  118 	   118 	 0.32658 	 0.34246 	 ~...
  115 	   119 	 0.31445 	 0.34644 	 m..s
  120 	   120 	 0.33419 	 0.35185 	 ~...
==========================================
r_mrr = 0.9898955225944519
r2_mrr = 0.9751940965652466
spearmanr_mrr@5 = 0.956203043460846
spearmanr_mrr@10 = 0.9245893359184265
spearmanr_mrr@50 = 0.9889135956764221
spearmanr_mrr@100 = 0.9949490427970886
spearmanr_mrr@All = 0.9954259991645813
==========================================
test time: 0.391
Done Testing dataset Kinships
total time taken: 188.03307127952576
training time taken: 181.2869312763214
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9899)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9752)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9562)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9246)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9889)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9949)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9954)}}, 'test_loss': {'TransE': {'Kinships': 0.3611857854957634}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7267501249025243
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [409, 989, 542, 1089, 29, 837, 1000, 475, 138, 510, 477, 1202, 1137, 1169, 1177, 842, 887, 1032, 61, 483, 909, 299, 1064, 32, 63, 437, 321, 208, 1161, 880, 1027, 1149, 69, 953, 1024, 531, 224, 638, 733, 1141, 1057, 450, 937, 121, 428, 978, 524, 71, 706, 936, 374, 160, 799, 869, 849, 882, 690, 166, 715, 794, 742, 554, 752, 237, 170, 357, 1185, 573, 1113, 425, 1056, 1038, 591, 585, 895, 784, 349, 761, 866, 952, 1168, 1028, 352, 637, 495, 80, 416, 21, 1104, 611, 502, 984, 558, 151, 547, 1069, 712, 839, 165, 728, 1191, 363, 302, 228, 371, 1001, 342, 249, 1016, 649, 360, 447, 1020, 835, 135, 39, 873, 243, 227, 615, 244]
valid_ids (0): []
train_ids (1094): [236, 1010, 1187, 605, 949, 461, 106, 161, 64, 904, 432, 1171, 197, 152, 626, 184, 944, 211, 529, 993, 894, 58, 812, 72, 256, 990, 1115, 140, 355, 384, 1204, 376, 169, 484, 382, 639, 49, 998, 478, 127, 261, 408, 481, 377, 291, 193, 846, 499, 657, 130, 44, 684, 254, 1125, 137, 651, 847, 277, 636, 857, 129, 1054, 1061, 164, 1142, 65, 233, 42, 830, 927, 640, 746, 633, 513, 517, 415, 1033, 298, 923, 1071, 1152, 192, 710, 441, 762, 36, 235, 87, 340, 431, 1110, 672, 661, 570, 27, 407, 465, 1196, 1040, 1014, 919, 167, 683, 146, 482, 420, 940, 454, 1074, 536, 621, 1037, 362, 430, 831, 442, 668, 346, 419, 618, 101, 1025, 539, 1164, 183, 94, 538, 1183, 186, 608, 195, 889, 1063, 412, 1085, 1138, 457, 860, 188, 963, 1013, 304, 527, 1068, 366, 319, 89, 1005, 1214, 563, 522, 17, 823, 229, 568, 663, 738, 653, 694, 231, 888, 134, 496, 589, 95, 815, 1081, 601, 399, 720, 1167, 398, 722, 1199, 541, 18, 604, 204, 607, 404, 345, 486, 520, 1151, 859, 213, 747, 516, 187, 631, 296, 60, 190, 281, 814, 1075, 418, 403, 181, 1174, 734, 1083, 1198, 617, 6, 834, 767, 995, 353, 246, 702, 93, 773, 323, 704, 198, 1072, 11, 22, 896, 1051, 74, 309, 964, 86, 874, 813, 981, 596, 361, 796, 406, 1150, 1143, 700, 469, 545, 932, 588, 1203, 664, 459, 307, 372, 1213, 1206, 1176, 400, 848, 79, 806, 120, 779, 907, 708, 965, 717, 276, 479, 263, 180, 1043, 753, 1107, 119, 928, 508, 88, 555, 1178, 199, 31, 878, 898, 1049, 348, 492, 646, 606, 776, 7, 111, 456, 1192, 367, 645, 840, 178, 232, 511, 280, 997, 850, 23, 444, 1184, 535, 1207, 600, 385, 1039, 1165, 438, 763, 1166, 157, 335, 1046, 343, 929, 396, 43, 284, 532, 206, 553, 1131, 45, 1078, 820, 1162, 811, 1026, 123, 504, 1084, 914, 743, 82, 916, 1029, 662, 863, 117, 819, 556, 359, 821, 1126, 689, 901, 967, 1134, 526, 133, 1004, 1153, 518, 401, 951, 537, 364, 507, 854, 283, 54, 1058, 194, 270, 789, 561, 546, 77, 1117, 226, 971, 1021, 463, 678, 8, 620, 792, 336, 1136, 287, 603, 326, 827, 458, 665, 316, 320, 402, 1062, 150, 567, 790, 1070, 3, 711, 464, 103, 1127, 525, 391, 991, 676, 448, 393, 841, 185, 241, 1065, 931, 55, 1017, 758, 1140, 451, 769, 800, 209, 1159, 817, 669, 629, 303, 200, 10, 1175, 798, 274, 992, 581, 337, 973, 565, 1066, 305, 153, 756, 1189, 487, 1181, 1077, 118, 1172, 159, 1109, 324, 1112, 78, 380, 574, 132, 619, 498, 1156, 551, 724, 1211, 476, 216, 282, 311, 667, 172, 333, 977, 713, 577, 644, 658, 675, 1086, 269, 81, 330, 144, 390, 543, 632, 1145, 899, 785, 612, 462, 957, 107, 552, 698, 754, 300, 9, 616, 13, 1098, 810, 48, 5, 341, 803, 654, 257, 740, 294, 687, 918, 92, 723, 915, 489, 755, 959, 347, 614, 271, 449, 350, 35, 397, 331, 179, 258, 125, 30, 622, 911, 84, 795, 259, 41, 719, 297, 46, 83, 783, 368, 576, 1108, 770, 736, 729, 884, 845, 480, 760, 435, 314, 627, 51, 423, 1023, 485, 774, 292, 560, 578, 1048, 956, 434, 413, 650, 703, 491, 688, 679, 515, 503, 745, 727, 968, 505, 905, 175, 136, 239, 50, 1095, 386, 1148, 217, 1092, 872, 85, 116, 59, 1007, 369, 858, 994, 1179, 922, 804, 540, 950, 925, 351, 987, 697, 171, 778, 643, 630, 176, 155, 920, 506, 960, 955, 721, 912, 334, 308, 590, 1186, 1106, 521, 264, 443, 338, 634, 1011, 856, 780, 674, 113, 749, 958, 1031, 290, 1130, 864, 56, 1052, 218, 935, 440, 822, 583, 234, 312, 381, 1067, 455, 983, 1123, 329, 327, 62, 908, 288, 1055, 66, 852, 879, 225, 921, 34, 429, 471, 202, 844, 1097, 876, 1111, 344, 699, 128, 726, 253, 223, 201, 248, 387, 1135, 500, 757, 979, 76, 544, 1041, 818, 751, 610, 1096, 933, 750, 974, 1154, 154, 735, 16, 718, 189, 392, 1205, 210, 286, 808, 569, 945, 976, 917, 897, 370, 777, 356, 575, 40, 594, 247, 737, 145, 468, 519, 53, 673, 881, 240, 938, 1047, 1212, 272, 528, 0, 628, 969, 114, 1120, 828, 1059, 293, 122, 685, 268, 37, 549, 802, 1146, 405, 453, 439, 705, 205, 262, 943, 289, 493, 177, 1100, 214, 473, 1139, 771, 962, 26, 75, 1195, 1105, 851, 252, 255, 70, 732, 1163, 625, 124, 947, 642, 358, 833, 1157, 867, 670, 548, 829, 445, 1088, 207, 982, 1030, 865, 427, 394, 238, 365, 975, 692, 861, 709, 701, 325, 14, 656, 267, 514, 1093, 474, 824, 212, 666, 395, 587, 1003, 1, 714, 1018, 433, 954, 1147, 1182, 488, 765, 660, 417, 768, 1133, 104, 424, 1036, 686, 1045, 1132, 422, 942, 215, 946, 446, 597, 1173, 173, 131, 913, 1160, 12, 677, 379, 1194, 807, 843, 295, 671, 375, 1079, 986, 571, 20, 652, 1129, 1015, 306, 572, 100, 797, 1035, 885, 102, 891, 550, 680, 793, 182, 265, 67, 655, 109, 490, 47, 787, 96, 731, 832, 1193, 1118, 91, 996, 436, 388, 584, 332, 278, 279, 222, 825, 245, 494, 1002, 641, 472, 1019, 602, 383, 57, 1201, 805, 648, 322, 251, 466, 1006, 149, 15, 707, 759, 559, 930, 497, 28, 999, 313, 826, 924, 781, 242, 966, 970, 624, 1121, 509, 838, 961, 739, 168, 1180, 1094, 853, 691, 1087, 1012, 647, 191, 142, 613, 196, 373, 534, 317, 452, 906, 1114, 1188, 741, 599, 250, 90, 174, 52, 426, 862, 609, 421, 275, 1076, 143, 870, 318, 467, 566, 115, 1200, 1034, 163, 695, 220, 635, 112, 883, 460, 301, 315, 1155, 203, 411, 110, 900, 744, 378, 99, 1053, 105, 716, 221, 1102, 557, 38, 926, 2, 1060, 592, 1101, 1082, 598, 579, 786, 24, 156, 748, 470, 108, 1158, 816, 875, 354, 523, 1122, 148, 725, 772, 266, 681, 230, 530, 730, 941, 682, 1128, 219, 988, 980, 310, 939, 595, 1042, 562, 1124, 273, 1044, 389, 1080, 791, 902, 141, 855, 533, 775, 414, 1022, 19, 25, 1073, 782, 285, 972, 801, 696, 1009, 659, 580, 33, 985, 98, 1144, 68, 73, 582, 147, 903, 836, 1090, 871, 877, 501, 328, 158, 788, 890, 162, 139, 126, 766, 4, 1091, 1099, 512, 1209, 1208, 764, 623, 910, 97, 593, 934, 893, 886, 1116, 1210, 868, 339, 586, 1119, 410, 260, 1103, 1190, 1170, 809, 564, 1050, 693, 948, 1008, 1197, 892]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2511527694940704
the save name prefix for this run is:  chkpt-ID_2511527694940704_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 78
rank avg (pred): 0.491 +- 0.008
mrr vals (pred, true): 0.019, 0.278
batch losses (mrrl, rdl): 0.0, 0.0032560294

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 144
rank avg (pred): 0.462 +- 0.004
mrr vals (pred, true): 0.021, 0.047
batch losses (mrrl, rdl): 0.0, 8.07698e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 619
rank avg (pred): 0.459 +- 0.005
mrr vals (pred, true): 0.021, 0.045
batch losses (mrrl, rdl): 0.0, 7.07261e-05

Epoch over!
epoch time: 11.923

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 310
rank avg (pred): 0.104 +- 0.002
mrr vals (pred, true): 0.086, 0.286
batch losses (mrrl, rdl): 0.0, 6.5518e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 175
rank avg (pred): 0.465 +- 0.007
mrr vals (pred, true): 0.020, 0.048
batch losses (mrrl, rdl): 0.0, 8.10228e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 334
rank avg (pred): 0.454 +- 0.008
mrr vals (pred, true): 0.021, 0.053
batch losses (mrrl, rdl): 0.0, 9.12689e-05

Epoch over!
epoch time: 11.861

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 92
rank avg (pred): 0.454 +- 0.006
mrr vals (pred, true): 0.021, 0.049
batch losses (mrrl, rdl): 0.0, 7.85987e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 403
rank avg (pred): 0.459 +- 0.009
mrr vals (pred, true): 0.021, 0.050
batch losses (mrrl, rdl): 0.0, 8.16057e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 805
rank avg (pred): 0.440 +- 0.263
mrr vals (pred, true): 0.074, 0.043
batch losses (mrrl, rdl): 0.0, 1.02299e-05

Epoch over!
epoch time: 11.909

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1063
rank avg (pred): 0.110 +- 0.066
mrr vals (pred, true): 0.161, 0.311
batch losses (mrrl, rdl): 0.0, 4.7695e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 147
rank avg (pred): 0.452 +- 0.272
mrr vals (pred, true): 0.064, 0.053
batch losses (mrrl, rdl): 0.0, 9.3609e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 879
rank avg (pred): 0.450 +- 0.269
mrr vals (pred, true): 0.059, 0.043
batch losses (mrrl, rdl): 0.0, 4.6385e-06

Epoch over!
epoch time: 11.852

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1153
rank avg (pred): 0.060 +- 0.067
mrr vals (pred, true): 0.284, 0.241
batch losses (mrrl, rdl): 0.0, 7.04202e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 26
rank avg (pred): 0.090 +- 0.124
mrr vals (pred, true): 0.220, 0.289
batch losses (mrrl, rdl): 0.0, 2.24839e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 253
rank avg (pred): 0.115 +- 0.144
mrr vals (pred, true): 0.201, 0.271
batch losses (mrrl, rdl): 0.0, 8.4985e-06

Epoch over!
epoch time: 11.705

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1176
rank avg (pred): 0.453 +- 0.272
mrr vals (pred, true): 0.062, 0.047
batch losses (mrrl, rdl): 0.0014601459, 2.1294e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 67
rank avg (pred): 0.039 +- 0.023
mrr vals (pred, true): 0.291, 0.281
batch losses (mrrl, rdl): 0.0010673889, 9.20593e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 678
rank avg (pred): 0.369 +- 0.127
mrr vals (pred, true): 0.055, 0.045
batch losses (mrrl, rdl): 0.0002740918, 0.0002245774

Epoch over!
epoch time: 12.173

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 748
rank avg (pred): 0.080 +- 0.050
mrr vals (pred, true): 0.225, 0.182
batch losses (mrrl, rdl): 0.0186290834, 0.0005310667

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 183
rank avg (pred): 0.387 +- 0.117
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 5.2762e-06, 0.0001650495

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 28
rank avg (pred): 0.063 +- 0.047
mrr vals (pred, true): 0.274, 0.260
batch losses (mrrl, rdl): 0.0017999444, 4.7361e-05

Epoch over!
epoch time: 12.276

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 325
rank avg (pred): 0.391 +- 0.123
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001996007, 6.93053e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 686
rank avg (pred): 0.408 +- 0.111
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001343711, 0.0001272539

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 658
rank avg (pred): 0.411 +- 0.108
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.000219166, 0.0001328647

Epoch over!
epoch time: 12.113

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 418
rank avg (pred): 0.414 +- 0.109
mrr vals (pred, true): 0.045, 0.044
batch losses (mrrl, rdl): 0.000202953, 9.93306e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 421
rank avg (pred): 0.429 +- 0.094
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.000889449, 9.52599e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 457
rank avg (pred): 0.415 +- 0.117
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 4.79599e-05, 0.0001114416

Epoch over!
epoch time: 11.919

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 44
rank avg (pred): 0.103 +- 0.098
mrr vals (pred, true): 0.264, 0.298
batch losses (mrrl, rdl): 0.0114302672, 2.1843e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 548
rank avg (pred): 0.181 +- 0.136
mrr vals (pred, true): 0.217, 0.230
batch losses (mrrl, rdl): 0.001696487, 8.86745e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 284
rank avg (pred): 0.090 +- 0.100
mrr vals (pred, true): 0.310, 0.280
batch losses (mrrl, rdl): 0.0088116862, 2.11727e-05

Epoch over!
epoch time: 12.081

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 907
rank avg (pred): 0.284 +- 0.182
mrr vals (pred, true): 0.190, 0.220
batch losses (mrrl, rdl): 0.0089367144, 0.0004399347

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 529
rank avg (pred): 0.196 +- 0.146
mrr vals (pred, true): 0.204, 0.211
batch losses (mrrl, rdl): 0.0006069407, 0.0001730084

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 124
rank avg (pred): 0.430 +- 0.112
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 3.84e-08, 5.16993e-05

Epoch over!
epoch time: 12.147

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 901
rank avg (pred): 0.313 +- 0.199
mrr vals (pred, true): 0.197, 0.152
batch losses (mrrl, rdl): 0.0203985572, 0.0001358949

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 134
rank avg (pred): 0.433 +- 0.112
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 7.0103e-06, 5.70138e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 807
rank avg (pred): 0.425 +- 0.120
mrr vals (pred, true): 0.056, 0.044
batch losses (mrrl, rdl): 0.0003409864, 8.39246e-05

Epoch over!
epoch time: 12.007

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 549
rank avg (pred): 0.202 +- 0.153
mrr vals (pred, true): 0.207, 0.206
batch losses (mrrl, rdl): 1.91877e-05, 0.0001915172

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 108
rank avg (pred): 0.435 +- 0.111
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 7.141e-07, 5.01024e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 25
rank avg (pred): 0.131 +- 0.146
mrr vals (pred, true): 0.291, 0.256
batch losses (mrrl, rdl): 0.012519178, 2.31096e-05

Epoch over!
epoch time: 11.958

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 990
rank avg (pred): 0.098 +- 0.138
mrr vals (pred, true): 0.354, 0.317
batch losses (mrrl, rdl): 0.0131512973, 4.783e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 13
rank avg (pred): 0.175 +- 0.156
mrr vals (pred, true): 0.254, 0.255
batch losses (mrrl, rdl): 8.8146e-06, 0.0001133901

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 217
rank avg (pred): 0.433 +- 0.107
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 5.1132e-06, 0.0001057193

Epoch over!
epoch time: 11.973

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 292
rank avg (pred): 0.139 +- 0.157
mrr vals (pred, true): 0.293, 0.290
batch losses (mrrl, rdl): 8.99995e-05, 5.15131e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 720
rank avg (pred): 0.437 +- 0.104
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 6.4124e-06, 7.21447e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1051
rank avg (pred): 0.436 +- 0.104
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 1.01501e-05, 8.90563e-05

Epoch over!
epoch time: 12.098

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.435 +- 0.107
mrr vals (pred, true): 0.051, 0.042

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   78 	     0 	 0.05173 	 0.03957 	 ~...
   65 	     1 	 0.05046 	 0.04075 	 ~...
   36 	     2 	 0.04869 	 0.04079 	 ~...
   19 	     3 	 0.04771 	 0.04134 	 ~...
   32 	     4 	 0.04849 	 0.04173 	 ~...
   52 	     5 	 0.04973 	 0.04173 	 ~...
   38 	     6 	 0.04908 	 0.04178 	 ~...
   30 	     7 	 0.04840 	 0.04184 	 ~...
   63 	     8 	 0.05044 	 0.04200 	 ~...
    1 	     9 	 0.04609 	 0.04206 	 ~...
   67 	    10 	 0.05056 	 0.04211 	 ~...
   46 	    11 	 0.04954 	 0.04214 	 ~...
   58 	    12 	 0.05008 	 0.04222 	 ~...
   70 	    13 	 0.05113 	 0.04232 	 ~...
   45 	    14 	 0.04950 	 0.04284 	 ~...
   25 	    15 	 0.04814 	 0.04284 	 ~...
   73 	    16 	 0.05126 	 0.04288 	 ~...
   29 	    17 	 0.04831 	 0.04292 	 ~...
   23 	    18 	 0.04786 	 0.04296 	 ~...
    6 	    19 	 0.04682 	 0.04301 	 ~...
   34 	    20 	 0.04857 	 0.04307 	 ~...
   53 	    21 	 0.04984 	 0.04313 	 ~...
   13 	    22 	 0.04713 	 0.04316 	 ~...
   50 	    23 	 0.04960 	 0.04338 	 ~...
   75 	    24 	 0.05143 	 0.04350 	 ~...
    9 	    25 	 0.04704 	 0.04355 	 ~...
   42 	    26 	 0.04941 	 0.04356 	 ~...
   17 	    27 	 0.04739 	 0.04362 	 ~...
   18 	    28 	 0.04764 	 0.04363 	 ~...
   37 	    29 	 0.04884 	 0.04369 	 ~...
    3 	    30 	 0.04640 	 0.04399 	 ~...
   24 	    31 	 0.04786 	 0.04401 	 ~...
   57 	    32 	 0.05006 	 0.04409 	 ~...
   11 	    33 	 0.04707 	 0.04414 	 ~...
   16 	    34 	 0.04726 	 0.04418 	 ~...
   80 	    35 	 0.05203 	 0.04422 	 ~...
   15 	    36 	 0.04720 	 0.04435 	 ~...
   21 	    37 	 0.04774 	 0.04436 	 ~...
   49 	    38 	 0.04957 	 0.04443 	 ~...
   71 	    39 	 0.05124 	 0.04447 	 ~...
   41 	    40 	 0.04937 	 0.04454 	 ~...
    5 	    41 	 0.04673 	 0.04473 	 ~...
   31 	    42 	 0.04844 	 0.04479 	 ~...
   22 	    43 	 0.04781 	 0.04484 	 ~...
    4 	    44 	 0.04651 	 0.04485 	 ~...
   56 	    45 	 0.05005 	 0.04496 	 ~...
   72 	    46 	 0.05125 	 0.04506 	 ~...
   59 	    47 	 0.05009 	 0.04510 	 ~...
   44 	    48 	 0.04949 	 0.04517 	 ~...
    2 	    49 	 0.04624 	 0.04537 	 ~...
   76 	    50 	 0.05161 	 0.04542 	 ~...
   39 	    51 	 0.04920 	 0.04558 	 ~...
   12 	    52 	 0.04710 	 0.04582 	 ~...
   54 	    53 	 0.04990 	 0.04607 	 ~...
    0 	    54 	 0.04604 	 0.04665 	 ~...
   55 	    55 	 0.04996 	 0.04678 	 ~...
   14 	    56 	 0.04716 	 0.04688 	 ~...
   26 	    57 	 0.04814 	 0.04717 	 ~...
   47 	    58 	 0.04955 	 0.04725 	 ~...
   51 	    59 	 0.04971 	 0.04733 	 ~...
   33 	    60 	 0.04851 	 0.04760 	 ~...
   35 	    61 	 0.04859 	 0.04830 	 ~...
   61 	    62 	 0.05030 	 0.04895 	 ~...
    7 	    63 	 0.04683 	 0.04910 	 ~...
   10 	    64 	 0.04705 	 0.04926 	 ~...
   43 	    65 	 0.04947 	 0.04955 	 ~...
    8 	    66 	 0.04685 	 0.05072 	 ~...
   68 	    67 	 0.05087 	 0.05148 	 ~...
   28 	    68 	 0.04827 	 0.05217 	 ~...
   66 	    69 	 0.05053 	 0.05231 	 ~...
   64 	    70 	 0.05046 	 0.05349 	 ~...
   40 	    71 	 0.04935 	 0.05392 	 ~...
   60 	    72 	 0.05025 	 0.05416 	 ~...
   77 	    73 	 0.05161 	 0.05417 	 ~...
   27 	    74 	 0.04825 	 0.05445 	 ~...
   69 	    75 	 0.05091 	 0.05516 	 ~...
   20 	    76 	 0.04774 	 0.05566 	 ~...
   74 	    77 	 0.05135 	 0.05699 	 ~...
   62 	    78 	 0.05043 	 0.05711 	 ~...
   79 	    79 	 0.05179 	 0.05818 	 ~...
   48 	    80 	 0.04956 	 0.06284 	 ~...
   82 	    81 	 0.19396 	 0.13946 	 m..s
   81 	    82 	 0.18870 	 0.17397 	 ~...
   87 	    83 	 0.22178 	 0.18444 	 m..s
   84 	    84 	 0.21251 	 0.20505 	 ~...
   86 	    85 	 0.22021 	 0.20813 	 ~...
   85 	    86 	 0.21804 	 0.21326 	 ~...
   83 	    87 	 0.20005 	 0.21355 	 ~...
   92 	    88 	 0.22861 	 0.22751 	 ~...
   94 	    89 	 0.22894 	 0.22844 	 ~...
   93 	    90 	 0.22893 	 0.22898 	 ~...
   89 	    91 	 0.22523 	 0.23195 	 ~...
   88 	    92 	 0.22440 	 0.23255 	 ~...
   90 	    93 	 0.22527 	 0.23632 	 ~...
   91 	    94 	 0.22842 	 0.24432 	 ~...
  106 	    95 	 0.28607 	 0.26028 	 ~...
   96 	    96 	 0.24524 	 0.26520 	 ~...
  100 	    97 	 0.28082 	 0.26747 	 ~...
   98 	    98 	 0.26308 	 0.27017 	 ~...
   95 	    99 	 0.23987 	 0.27208 	 m..s
  109 	   100 	 0.28842 	 0.27549 	 ~...
  103 	   101 	 0.28425 	 0.27588 	 ~...
  101 	   102 	 0.28302 	 0.27725 	 ~...
  112 	   103 	 0.29540 	 0.27859 	 ~...
   97 	   104 	 0.24680 	 0.28336 	 m..s
  108 	   105 	 0.28691 	 0.28547 	 ~...
  107 	   106 	 0.28639 	 0.28704 	 ~...
   99 	   107 	 0.27409 	 0.29004 	 ~...
  111 	   108 	 0.29324 	 0.29071 	 ~...
  105 	   109 	 0.28600 	 0.29207 	 ~...
  110 	   110 	 0.29283 	 0.29262 	 ~...
  102 	   111 	 0.28352 	 0.29285 	 ~...
  104 	   112 	 0.28508 	 0.29653 	 ~...
  113 	   113 	 0.29842 	 0.30865 	 ~...
  118 	   114 	 0.33835 	 0.32126 	 ~...
  116 	   115 	 0.33442 	 0.33199 	 ~...
  115 	   116 	 0.33193 	 0.33674 	 ~...
  114 	   117 	 0.32048 	 0.34118 	 ~...
  119 	   118 	 0.34318 	 0.34246 	 ~...
  120 	   119 	 0.35336 	 0.35185 	 ~...
  117 	   120 	 0.33445 	 0.35540 	 ~...
==========================================
r_mrr = 0.9953247904777527
r2_mrr = 0.9901646971702576
spearmanr_mrr@5 = 0.93329256772995
spearmanr_mrr@10 = 0.9858571887016296
spearmanr_mrr@50 = 0.9937921762466431
spearmanr_mrr@100 = 0.9976046085357666
spearmanr_mrr@All = 0.997829794883728
==========================================
test time: 0.388
Done Testing dataset Kinships
total time taken: 187.48309183120728
training time taken: 180.46767497062683
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9953)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9902)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9333)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9859)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9938)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9976)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9978)}}, 'test_loss': {'TransE': {'Kinships': 0.12958158415676735}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7729071668358768
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [940, 203, 865, 35, 586, 305, 1052, 419, 548, 80, 240, 1164, 398, 383, 702, 1187, 565, 280, 867, 416, 819, 928, 562, 194, 883, 846, 892, 651, 431, 687, 601, 860, 902, 391, 344, 648, 767, 255, 872, 525, 87, 544, 335, 1048, 1139, 530, 440, 371, 117, 366, 40, 558, 54, 731, 1085, 365, 813, 510, 1068, 467, 557, 88, 165, 1046, 368, 1161, 1038, 560, 1182, 1090, 1149, 321, 653, 64, 823, 997, 1200, 497, 1167, 498, 696, 382, 1, 930, 1184, 128, 439, 723, 286, 862, 1146, 839, 121, 785, 529, 931, 76, 956, 1002, 802, 506, 658, 1169, 966, 420, 200, 152, 859, 338, 869, 552, 969, 6, 612, 413, 108, 777, 1136, 942, 1011, 204]
valid_ids (0): []
train_ids (1094): [751, 1214, 825, 184, 810, 598, 1155, 597, 1008, 900, 1041, 492, 798, 306, 864, 932, 345, 980, 805, 991, 953, 316, 1088, 977, 201, 1080, 160, 409, 272, 263, 360, 71, 703, 830, 1121, 271, 357, 1020, 155, 1083, 1213, 58, 1058, 622, 124, 1174, 303, 748, 906, 285, 198, 1034, 786, 434, 691, 449, 727, 205, 313, 1079, 415, 1181, 1141, 7, 403, 279, 626, 397, 1007, 1197, 26, 837, 994, 555, 781, 477, 776, 968, 242, 17, 712, 1054, 761, 317, 610, 499, 500, 185, 589, 57, 1190, 646, 127, 1153, 988, 649, 833, 721, 367, 699, 215, 627, 1025, 514, 296, 47, 348, 676, 1006, 1159, 520, 34, 1138, 1180, 504, 941, 1107, 485, 701, 814, 879, 1113, 1099, 1028, 827, 179, 984, 591, 590, 315, 922, 898, 618, 484, 1069, 531, 332, 175, 1032, 461, 995, 588, 1017, 1075, 395, 104, 1142, 78, 599, 1122, 667, 1091, 818, 572, 45, 1133, 519, 443, 469, 351, 292, 1104, 644, 1108, 170, 86, 125, 1015, 172, 281, 301, 522, 384, 331, 302, 473, 871, 993, 592, 518, 278, 550, 491, 325, 1024, 526, 1019, 604, 1065, 532, 845, 105, 100, 650, 577, 523, 107, 425, 311, 1204, 706, 249, 689, 692, 442, 1119, 1071, 949, 634, 451, 771, 513, 16, 923, 1029, 568, 704, 438, 686, 231, 435, 934, 261, 606, 437, 433, 1070, 613, 947, 422, 1196, 732, 584, 624, 299, 161, 166, 1188, 101, 256, 1171, 1074, 1100, 943, 364, 83, 964, 878, 333, 809, 1208, 674, 148, 844, 11, 275, 326, 329, 996, 693, 388, 246, 475, 1117, 669, 489, 816, 801, 115, 393, 159, 234, 1072, 193, 1060, 411, 549, 390, 370, 1152, 149, 962, 911, 5, 169, 319, 600, 840, 441, 739, 874, 528, 65, 540, 73, 685, 652, 665, 736, 593, 547, 831, 153, 248, 406, 886, 855, 1157, 821, 251, 112, 517, 836, 710, 679, 917, 605, 69, 378, 225, 881, 486, 524, 18, 1059, 954, 1150, 647, 223, 921, 683, 468, 615, 9, 129, 14, 151, 150, 749, 747, 570, 664, 247, 1018, 488, 904, 354, 52, 873, 471, 1192, 973, 1145, 578, 1111, 4, 211, 210, 1147, 875, 111, 958, 268, 950, 807, 386, 1185, 655, 412, 1210, 483, 447, 190, 428, 545, 445, 182, 815, 330, 1044, 671, 938, 289, 267, 353, 675, 312, 1000, 998, 714, 120, 794, 1031, 960, 457, 707, 192, 1166, 567, 220, 754, 778, 294, 737, 374, 1125, 429, 630, 539, 654, 569, 717, 718, 156, 677, 269, 72, 899, 542, 979, 177, 1151, 551, 870, 766, 30, 908, 848, 640, 581, 27, 334, 829, 218, 94, 358, 561, 768, 136, 755, 607, 1120, 288, 629, 625, 336, 511, 895, 342, 162, 1179, 349, 868, 999, 487, 219, 913, 93, 834, 284, 402, 910, 207, 959, 70, 527, 765, 1093, 660, 682, 444, 705, 180, 42, 118, 183, 452, 1096, 614, 608, 1170, 213, 926, 1110, 905, 933, 1137, 828, 937, 1055, 282, 1033, 779, 735, 852, 594, 79, 657, 1005, 851, 253, 67, 887, 1203, 232, 1115, 427, 619, 505, 373, 355, 110, 708, 681, 1082, 307, 41, 265, 1012, 212, 1131, 1086, 1168, 1004, 257, 621, 208, 135, 450, 975, 951, 713, 939, 1212, 186, 96, 609, 448, 270, 154, 756, 308, 340, 863, 639, 23, 314, 1073, 603, 227, 479, 1039, 293, 19, 733, 1123, 623, 103, 1076, 343, 241, 888, 481, 273, 961, 812, 436, 290, 59, 1050, 66, 258, 1023, 760, 740, 1081, 1126, 1135, 659, 379, 1105, 199, 971, 698, 244, 596, 914, 493, 967, 1103, 1087, 396, 324, 983, 1143, 912, 889, 298, 700, 143, 206, 1097, 470, 401, 84, 1202, 730, 924, 709, 217, 237, 38, 502, 893, 318, 347, 482, 1027, 1209, 1040, 74, 1183, 1112, 896, 22, 15, 1030, 51, 806, 356, 501, 50, 919, 595, 952, 176, 456, 222, 935, 363, 670, 230, 1094, 1148, 885, 464, 804, 564, 1078, 62, 446, 235, 495, 800, 684, 453, 841, 12, 750, 1092, 187, 1207, 259, 459, 264, 116, 1114, 13, 337, 91, 793, 797, 916, 233, 915, 769, 454, 1211, 1009, 1140, 799, 33, 533, 1102, 1062, 1003, 0, 350, 109, 31, 95, 1061, 795, 417, 1134, 847, 628, 772, 89, 46, 328, 274, 792, 925, 1042, 1084, 1177, 465, 1035, 668, 410, 987, 133, 1056, 141, 796, 729, 817, 1014, 480, 327, 811, 8, 430, 784, 146, 835, 75, 780, 189, 37, 909, 400, 250, 385, 986, 516, 25, 891, 515, 787, 191, 310, 617, 641, 521, 380, 126, 556, 387, 277, 139, 1109, 85, 494, 1098, 339, 842, 876, 77, 982, 632, 746, 119, 509, 826, 1130, 68, 715, 214, 276, 60, 929, 972, 236, 662, 1205, 462, 466, 209, 672, 948, 254, 39, 738, 123, 61, 1124, 260, 164, 167, 1176, 195, 1066, 432, 36, 1156, 376, 304, 890, 178, 965, 56, 575, 508, 145, 507, 228, 957, 690, 587, 1144, 1132, 1198, 49, 266, 537, 541, 359, 1051, 2, 571, 579, 678, 585, 789, 322, 1010, 688, 245, 985, 99, 424, 936, 1045, 90, 770, 142, 476, 1047, 389, 55, 131, 680, 734, 122, 490, 10, 974, 392, 81, 1106, 1154, 144, 407, 976, 1206, 1013, 408, 773, 460, 21, 262, 631, 790, 130, 1101, 381, 1022, 775, 1089, 822, 138, 163, 858, 849, 861, 361, 728, 224, 1158, 758, 743, 323, 1162, 907, 1057, 866, 352, 1037, 132, 229, 216, 636, 291, 602, 643, 346, 820, 741, 496, 1178, 97, 573, 472, 1063, 1128, 989, 535, 638, 978, 238, 377, 1064, 1163, 574, 877, 300, 576, 764, 853, 757, 171, 1172, 1116, 620, 1195, 405, 503, 239, 752, 719, 421, 48, 404, 918, 137, 720, 1199, 763, 546, 854, 543, 1026, 903, 897, 1173, 534, 1043, 458, 20, 43, 226, 394, 536, 656, 1165, 637, 832, 1127, 106, 197, 478, 134, 1016, 243, 1021, 633, 295, 372, 399, 616, 894, 24, 694, 970, 181, 63, 955, 158, 1194, 663, 29, 563, 673, 666, 920, 744, 341, 742, 1201, 418, 695, 320, 173, 808, 375, 850, 252, 369, 287, 945, 856, 774, 1186, 1193, 157, 1189, 283, 963, 553, 583, 753, 1036, 726, 1118, 3, 28, 857, 642, 838, 645, 788, 1049, 414, 221, 990, 782, 697, 114, 455, 725, 147, 582, 1095, 1191, 362, 722, 538, 309, 98, 803, 92, 174, 901, 102, 1175, 566, 202, 759, 635, 1160, 711, 791, 944, 426, 44, 188, 661, 843, 53, 168, 512, 992, 611, 927, 32, 1129, 196, 1001, 882, 297, 423, 580, 474, 824, 554, 880, 884, 463, 1067, 981, 113, 745, 82, 140, 716, 559, 724, 1053, 762, 783, 946, 1077]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5963786955734398
the save name prefix for this run is:  chkpt-ID_5963786955734398_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 912
rank avg (pred): 0.496 +- 0.003
mrr vals (pred, true): 0.019, 0.186
batch losses (mrrl, rdl): 0.0, 0.0021186294

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1079
rank avg (pred): 0.116 +- 0.087
mrr vals (pred, true): 0.247, 0.351
batch losses (mrrl, rdl): 0.0, 1.27183e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 384
rank avg (pred): 0.432 +- 0.265
mrr vals (pred, true): 0.103, 0.047
batch losses (mrrl, rdl): 0.0, 2.4572e-06

Epoch over!
epoch time: 11.911

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 660
rank avg (pred): 0.430 +- 0.267
mrr vals (pred, true): 0.105, 0.043
batch losses (mrrl, rdl): 0.0, 7.6063e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 101
rank avg (pred): 0.442 +- 0.262
mrr vals (pred, true): 0.081, 0.062
batch losses (mrrl, rdl): 0.0, 2.9402e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 43
rank avg (pred): 0.100 +- 0.098
mrr vals (pred, true): 0.265, 0.270
batch losses (mrrl, rdl): 0.0, 6.878e-07

Epoch over!
epoch time: 11.866

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 492
rank avg (pred): 0.103 +- 0.097
mrr vals (pred, true): 0.249, 0.213
batch losses (mrrl, rdl): 0.0, 4.9662e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1026
rank avg (pred): 0.461 +- 0.257
mrr vals (pred, true): 0.059, 0.046
batch losses (mrrl, rdl): 0.0, 1.7253e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 112
rank avg (pred): 0.445 +- 0.266
mrr vals (pred, true): 0.066, 0.049
batch losses (mrrl, rdl): 0.0, 3.4121e-06

Epoch over!
epoch time: 11.846

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 682
rank avg (pred): 0.459 +- 0.262
mrr vals (pred, true): 0.057, 0.043
batch losses (mrrl, rdl): 0.0, 8.866e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 830
rank avg (pred): 0.154 +- 0.158
mrr vals (pred, true): 0.150, 0.195
batch losses (mrrl, rdl): 0.0, 7.60977e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 470
rank avg (pred): 0.459 +- 0.261
mrr vals (pred, true): 0.042, 0.040
batch losses (mrrl, rdl): 0.0, 8.0163e-06

Epoch over!
epoch time: 11.908

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 231
rank avg (pred): 0.420 +- 0.275
mrr vals (pred, true): 0.076, 0.043
batch losses (mrrl, rdl): 0.0, 2.46249e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 126
rank avg (pred): 0.458 +- 0.263
mrr vals (pred, true): 0.047, 0.049
batch losses (mrrl, rdl): 0.0, 3.1941e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 650
rank avg (pred): 0.447 +- 0.260
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 0.0, 2.1969e-06

Epoch over!
epoch time: 11.86

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 145
rank avg (pred): 0.468 +- 0.272
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 1.3773e-05, 4.01432e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1172
rank avg (pred): 0.457 +- 0.167
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.70719e-05, 3.45784e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 251
rank avg (pred): 0.117 +- 0.124
mrr vals (pred, true): 0.289, 0.290
batch losses (mrrl, rdl): 8.6886e-06, 1.8931e-06

Epoch over!
epoch time: 12.237

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1113
rank avg (pred): 0.445 +- 0.144
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 3.96482e-05, 5.75957e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 167
rank avg (pred): 0.444 +- 0.142
mrr vals (pred, true): 0.046, 0.042
batch losses (mrrl, rdl): 0.0001774163, 6.00182e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 410
rank avg (pred): 0.444 +- 0.154
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 1.52371e-05, 3.91111e-05

Epoch over!
epoch time: 12.086

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 518
rank avg (pred): 0.238 +- 0.189
mrr vals (pred, true): 0.229, 0.229
batch losses (mrrl, rdl): 1.593e-07, 0.0002792432

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 654
rank avg (pred): 0.464 +- 0.141
mrr vals (pred, true): 0.044, 0.046
batch losses (mrrl, rdl): 0.0003665307, 4.27902e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 403
rank avg (pred): 0.442 +- 0.155
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 8.20493e-05, 3.41579e-05

Epoch over!
epoch time: 11.894

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 499
rank avg (pred): 0.250 +- 0.187
mrr vals (pred, true): 0.200, 0.192
batch losses (mrrl, rdl): 0.0005904672, 0.0003279083

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 619
rank avg (pred): 0.459 +- 0.166
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 9.74417e-05, 2.7056e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 331
rank avg (pred): 0.457 +- 0.173
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 4.7001e-06, 3.72047e-05

Epoch over!
epoch time: 12.204

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 287
rank avg (pred): 0.127 +- 0.126
mrr vals (pred, true): 0.285, 0.285
batch losses (mrrl, rdl): 1.17e-08, 7.4264e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 48
rank avg (pred): 0.120 +- 0.112
mrr vals (pred, true): 0.280, 0.296
batch losses (mrrl, rdl): 0.0024975771, 1.65588e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 378
rank avg (pred): 0.441 +- 0.176
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 4.59401e-05, 2.79576e-05

Epoch over!
epoch time: 11.991

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 960
rank avg (pred): 0.438 +- 0.177
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0004207485, 3.41771e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 430
rank avg (pred): 0.427 +- 0.166
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 6.76887e-05, 6.61542e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 561
rank avg (pred): 0.192 +- 0.164
mrr vals (pred, true): 0.245, 0.224
batch losses (mrrl, rdl): 0.0044497987, 0.000149946

Epoch over!
epoch time: 12.198

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 86
rank avg (pred): 0.446 +- 0.185
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 1.1813e-06, 2.6006e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 242
rank avg (pred): 0.442 +- 0.174
mrr vals (pred, true): 0.045, 0.046
batch losses (mrrl, rdl): 0.0002880025, 5.95301e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 995
rank avg (pred): 0.064 +- 0.072
mrr vals (pred, true): 0.342, 0.348
batch losses (mrrl, rdl): 0.00034868, 3.08592e-05

Epoch over!
epoch time: 11.963

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 910
rank avg (pred): 0.373 +- 0.273
mrr vals (pred, true): 0.181, 0.163
batch losses (mrrl, rdl): 0.0030721084, 0.0003623145

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 736
rank avg (pred): 0.342 +- 0.205
mrr vals (pred, true): 0.165, 0.107
batch losses (mrrl, rdl): 0.0341660082, 5.89748e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 576
rank avg (pred): 0.468 +- 0.217
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 6.08528e-05, 1.35084e-05

Epoch over!
epoch time: 12.266

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 229
rank avg (pred): 0.482 +- 0.222
mrr vals (pred, true): 0.048, 0.041
batch losses (mrrl, rdl): 5.36735e-05, 1.10385e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 898
rank avg (pred): 0.387 +- 0.232
mrr vals (pred, true): 0.127, 0.103
batch losses (mrrl, rdl): 0.0059222546, 0.000158638

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 745
rank avg (pred): 0.184 +- 0.199
mrr vals (pred, true): 0.250, 0.262
batch losses (mrrl, rdl): 0.0014399129, 3.40982e-05

Epoch over!
epoch time: 11.95

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 780
rank avg (pred): 0.445 +- 0.192
mrr vals (pred, true): 0.045, 0.048
batch losses (mrrl, rdl): 0.0002542321, 3.30503e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 441
rank avg (pred): 0.466 +- 0.217
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 1.25648e-05, 1.33519e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 269
rank avg (pred): 0.134 +- 0.149
mrr vals (pred, true): 0.251, 0.287
batch losses (mrrl, rdl): 0.0132888919, 7.7153e-06

Epoch over!
epoch time: 12.036

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.487 +- 0.236
mrr vals (pred, true): 0.049, 0.044

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   71 	     0 	 0.04737 	 0.03862 	 ~...
    4 	     1 	 0.04208 	 0.04001 	 ~...
   20 	     2 	 0.04380 	 0.04053 	 ~...
    7 	     3 	 0.04255 	 0.04062 	 ~...
   35 	     4 	 0.04473 	 0.04137 	 ~...
    1 	     5 	 0.04180 	 0.04158 	 ~...
   73 	     6 	 0.04765 	 0.04162 	 ~...
   50 	     7 	 0.04598 	 0.04214 	 ~...
   59 	     8 	 0.04681 	 0.04215 	 ~...
   26 	     9 	 0.04428 	 0.04222 	 ~...
   18 	    10 	 0.04368 	 0.04232 	 ~...
    8 	    11 	 0.04278 	 0.04247 	 ~...
   64 	    12 	 0.04706 	 0.04250 	 ~...
   55 	    13 	 0.04645 	 0.04271 	 ~...
   57 	    14 	 0.04678 	 0.04275 	 ~...
   40 	    15 	 0.04522 	 0.04286 	 ~...
   80 	    16 	 0.04918 	 0.04298 	 ~...
   56 	    17 	 0.04659 	 0.04310 	 ~...
   46 	    18 	 0.04573 	 0.04311 	 ~...
   54 	    19 	 0.04624 	 0.04316 	 ~...
   30 	    20 	 0.04452 	 0.04320 	 ~...
   67 	    21 	 0.04715 	 0.04349 	 ~...
   27 	    22 	 0.04429 	 0.04352 	 ~...
    2 	    23 	 0.04185 	 0.04354 	 ~...
   36 	    24 	 0.04479 	 0.04363 	 ~...
    9 	    25 	 0.04298 	 0.04369 	 ~...
   43 	    26 	 0.04540 	 0.04370 	 ~...
   42 	    27 	 0.04532 	 0.04382 	 ~...
   12 	    28 	 0.04344 	 0.04383 	 ~...
   69 	    29 	 0.04723 	 0.04388 	 ~...
   47 	    30 	 0.04575 	 0.04390 	 ~...
    3 	    31 	 0.04204 	 0.04392 	 ~...
   19 	    32 	 0.04373 	 0.04401 	 ~...
   78 	    33 	 0.04870 	 0.04408 	 ~...
    5 	    34 	 0.04233 	 0.04409 	 ~...
   34 	    35 	 0.04472 	 0.04409 	 ~...
   39 	    36 	 0.04519 	 0.04412 	 ~...
   70 	    37 	 0.04736 	 0.04415 	 ~...
   13 	    38 	 0.04346 	 0.04418 	 ~...
   49 	    39 	 0.04593 	 0.04422 	 ~...
    0 	    40 	 0.04112 	 0.04427 	 ~...
   79 	    41 	 0.04870 	 0.04442 	 ~...
   48 	    42 	 0.04593 	 0.04450 	 ~...
   14 	    43 	 0.04353 	 0.04474 	 ~...
   22 	    44 	 0.04395 	 0.04476 	 ~...
   15 	    45 	 0.04356 	 0.04517 	 ~...
   77 	    46 	 0.04868 	 0.04522 	 ~...
   74 	    47 	 0.04769 	 0.04542 	 ~...
    6 	    48 	 0.04245 	 0.04546 	 ~...
   58 	    49 	 0.04680 	 0.04569 	 ~...
   23 	    50 	 0.04403 	 0.04575 	 ~...
   41 	    51 	 0.04527 	 0.04591 	 ~...
   52 	    52 	 0.04611 	 0.04592 	 ~...
   51 	    53 	 0.04603 	 0.04593 	 ~...
   31 	    54 	 0.04453 	 0.04616 	 ~...
   63 	    55 	 0.04702 	 0.04640 	 ~...
   44 	    56 	 0.04552 	 0.04642 	 ~...
   32 	    57 	 0.04455 	 0.04747 	 ~...
   76 	    58 	 0.04800 	 0.04826 	 ~...
   53 	    59 	 0.04613 	 0.04831 	 ~...
   75 	    60 	 0.04775 	 0.04852 	 ~...
   17 	    61 	 0.04368 	 0.04895 	 ~...
   38 	    62 	 0.04505 	 0.04934 	 ~...
   16 	    63 	 0.04364 	 0.04945 	 ~...
   65 	    64 	 0.04707 	 0.04970 	 ~...
   62 	    65 	 0.04695 	 0.04980 	 ~...
   25 	    66 	 0.04409 	 0.05086 	 ~...
   60 	    67 	 0.04685 	 0.05241 	 ~...
   28 	    68 	 0.04432 	 0.05367 	 ~...
   29 	    69 	 0.04451 	 0.05392 	 ~...
   72 	    70 	 0.04756 	 0.05401 	 ~...
   24 	    71 	 0.04408 	 0.05405 	 ~...
   21 	    72 	 0.04380 	 0.05480 	 ~...
   61 	    73 	 0.04686 	 0.05522 	 ~...
   10 	    74 	 0.04309 	 0.05665 	 ~...
   45 	    75 	 0.04563 	 0.05696 	 ~...
   68 	    76 	 0.04722 	 0.05728 	 ~...
   11 	    77 	 0.04325 	 0.05806 	 ~...
   66 	    78 	 0.04714 	 0.05818 	 ~...
   33 	    79 	 0.04458 	 0.05853 	 ~...
   37 	    80 	 0.04502 	 0.05973 	 ~...
   81 	    81 	 0.15511 	 0.13306 	 ~...
   83 	    82 	 0.18825 	 0.14461 	 m..s
   84 	    83 	 0.19431 	 0.17582 	 ~...
  100 	    84 	 0.22237 	 0.18210 	 m..s
   85 	    85 	 0.20384 	 0.18777 	 ~...
   95 	    86 	 0.21766 	 0.19965 	 ~...
   87 	    87 	 0.20814 	 0.20910 	 ~...
   91 	    88 	 0.21191 	 0.21130 	 ~...
   96 	    89 	 0.21810 	 0.21172 	 ~...
   86 	    90 	 0.20657 	 0.21191 	 ~...
   99 	    91 	 0.21880 	 0.21326 	 ~...
   93 	    92 	 0.21357 	 0.21694 	 ~...
   88 	    93 	 0.20976 	 0.22121 	 ~...
   94 	    94 	 0.21390 	 0.22128 	 ~...
   92 	    95 	 0.21353 	 0.22373 	 ~...
   82 	    96 	 0.16942 	 0.22530 	 m..s
   97 	    97 	 0.21817 	 0.22898 	 ~...
   90 	    98 	 0.21134 	 0.23010 	 ~...
   98 	    99 	 0.21850 	 0.23260 	 ~...
   89 	   100 	 0.20982 	 0.24035 	 m..s
  102 	   101 	 0.23171 	 0.24483 	 ~...
  106 	   102 	 0.24854 	 0.25503 	 ~...
  101 	   103 	 0.22609 	 0.25569 	 ~...
  103 	   104 	 0.23486 	 0.25665 	 ~...
  104 	   105 	 0.24025 	 0.26043 	 ~...
  110 	   106 	 0.25957 	 0.26309 	 ~...
  117 	   107 	 0.26996 	 0.26351 	 ~...
  105 	   108 	 0.24798 	 0.26817 	 ~...
  109 	   109 	 0.25767 	 0.27017 	 ~...
  115 	   110 	 0.26661 	 0.27470 	 ~...
  111 	   111 	 0.25999 	 0.27795 	 ~...
  116 	   112 	 0.26950 	 0.27859 	 ~...
  113 	   113 	 0.26449 	 0.28228 	 ~...
  107 	   114 	 0.25447 	 0.29137 	 m..s
  112 	   115 	 0.26065 	 0.29412 	 m..s
  108 	   116 	 0.25533 	 0.29612 	 m..s
  114 	   117 	 0.26578 	 0.30045 	 m..s
  118 	   118 	 0.27644 	 0.30865 	 m..s
  119 	   119 	 0.32814 	 0.33103 	 ~...
  120 	   120 	 0.33471 	 0.35877 	 ~...
==========================================
r_mrr = 0.991875946521759
r2_mrr = 0.9799225926399231
spearmanr_mrr@5 = 0.9448508620262146
spearmanr_mrr@10 = 0.9517995119094849
spearmanr_mrr@50 = 0.9908016324043274
spearmanr_mrr@100 = 0.9965542554855347
spearmanr_mrr@All = 0.9969102740287781
==========================================
test time: 0.564
Done Testing dataset Kinships
total time taken: 187.6784007549286
training time taken: 180.86765027046204
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9919)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9799)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9449)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9518)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9908)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9966)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9969)}}, 'test_loss': {'TransE': {'Kinships': 0.2349801466862118}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 2087341477706789
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1131, 719, 180, 534, 362, 582, 406, 49, 30, 260, 654, 533, 454, 668, 435, 136, 1038, 1115, 428, 573, 1059, 526, 1135, 1161, 1136, 446, 282, 1064, 786, 1175, 297, 448, 767, 660, 880, 672, 1182, 442, 445, 319, 1090, 188, 1104, 352, 1148, 381, 314, 749, 935, 639, 1058, 1033, 66, 870, 1114, 345, 1036, 1091, 1128, 731, 214, 389, 433, 1177, 588, 970, 845, 696, 353, 713, 962, 1032, 122, 1031, 889, 368, 738, 158, 1194, 185, 784, 558, 431, 360, 865, 43, 648, 16, 825, 739, 929, 727, 1169, 456, 1139, 882, 896, 1041, 186, 290, 421, 357, 472, 1189, 398, 513, 22, 1009, 135, 1086, 516, 1053, 225, 793, 597, 229, 1068, 56, 817, 333, 804]
valid_ids (0): []
train_ids (1094): [1127, 625, 438, 1003, 443, 688, 584, 647, 855, 768, 462, 670, 1039, 336, 169, 753, 805, 565, 58, 9, 591, 497, 24, 594, 969, 1142, 973, 227, 892, 808, 71, 423, 531, 1179, 750, 385, 177, 194, 971, 1082, 850, 467, 1055, 557, 34, 1207, 875, 261, 243, 702, 15, 479, 482, 1022, 154, 1019, 240, 107, 546, 709, 583, 1170, 26, 1203, 405, 394, 890, 1134, 128, 887, 1147, 239, 644, 88, 296, 1000, 867, 280, 488, 155, 139, 72, 40, 1085, 152, 20, 555, 697, 525, 972, 537, 1210, 790, 129, 824, 795, 1071, 359, 329, 550, 404, 545, 1073, 759, 674, 328, 919, 383, 208, 800, 690, 121, 1010, 1101, 838, 599, 407, 632, 311, 1046, 821, 595, 755, 1057, 102, 653, 903, 473, 118, 138, 737, 1008, 344, 918, 293, 905, 695, 572, 980, 444, 846, 1199, 891, 411, 106, 848, 560, 1049, 274, 922, 210, 209, 952, 150, 1099, 728, 54, 313, 600, 1097, 221, 1163, 763, 337, 272, 942, 267, 940, 590, 1196, 104, 1119, 37, 832, 553, 592, 593, 12, 163, 351, 307, 125, 736, 377, 187, 224, 1125, 914, 975, 89, 233, 1001, 748, 778, 758, 1020, 413, 801, 965, 735, 770, 162, 782, 91, 315, 103, 449, 602, 764, 202, 999, 205, 618, 977, 577, 363, 430, 387, 396, 953, 603, 938, 292, 252, 90, 245, 1109, 643, 773, 447, 478, 429, 923, 666, 141, 756, 571, 1074, 1080, 628, 716, 327, 989, 204, 528, 1, 646, 190, 957, 686, 101, 112, 524, 857, 461, 212, 655, 966, 1112, 258, 909, 217, 536, 1171, 98, 171, 564, 1056, 301, 403, 1120, 1117, 140, 495, 802, 1193, 1111, 123, 1197, 420, 500, 341, 343, 829, 853, 1105, 1072, 601, 28, 757, 788, 833, 1106, 60, 415, 623, 985, 798, 1116, 393, 651, 835, 97, 324, 694, 774, 146, 700, 634, 743, 1202, 1140, 105, 1152, 160, 562, 269, 111, 55, 687, 254, 250, 2, 1211, 32, 94, 742, 611, 199, 303, 943, 609, 399, 132, 51, 1156, 538, 80, 904, 263, 883, 656, 561, 1100, 249, 235, 412, 391, 510, 663, 598, 1123, 1002, 807, 35, 285, 1145, 417, 785, 416, 349, 616, 920, 876, 326, 69, 715, 636, 59, 554, 149, 885, 916, 578, 453, 509, 316, 499, 476, 242, 858, 1084, 166, 771, 745, 681, 703, 1153, 1167, 287, 680, 1006, 752, 300, 976, 939, 859, 1176, 77, 685, 7, 523, 791, 304, 535, 1209, 926, 1118, 1201, 419, 1204, 1208, 161, 854, 330, 514, 792, 1122, 126, 1113, 606, 1035, 946, 987, 959, 678, 4, 979, 127, 621, 286, 675, 871, 724, 789, 997, 271, 1077, 207, 388, 459, 995, 677, 33, 541, 455, 318, 803, 219, 153, 366, 458, 563, 1093, 8, 721, 321, 1029, 507, 1184, 119, 772, 908, 676, 947, 839, 506, 740, 305, 504, 799, 197, 57, 827, 130, 944, 134, 256, 951, 480, 559, 775, 813, 949, 860, 266, 568, 1137, 92, 819, 116, 452, 1070, 990, 376, 331, 502, 222, 1047, 378, 450, 1164, 355, 1124, 61, 3, 1166, 732, 1165, 607, 174, 75, 512, 604, 532, 822, 1052, 991, 338, 569, 640, 203, 556, 41, 373, 769, 983, 361, 165, 487, 1107, 131, 1158, 734, 856, 667, 426, 1027, 895, 884, 956, 936, 408, 705, 610, 1014, 191, 868, 626, 317, 661, 1198, 810, 996, 1045, 629, 701, 751, 86, 682, 501, 347, 761, 1069, 234, 852, 156, 649, 1007, 469, 549, 1185, 552, 907, 963, 196, 226, 124, 893, 994, 760, 707, 1013, 142, 729, 1205, 465, 284, 615, 605, 527, 818, 613, 236, 776, 79, 400, 877, 659, 1021, 230, 794, 878, 27, 539, 1102, 114, 746, 783, 1030, 25, 924, 109, 21, 517, 457, 974, 439, 570, 998, 812, 237, 1040, 145, 993, 567, 1061, 159, 503, 862, 650, 664, 1044, 830, 170, 1004, 281, 251, 372, 619, 950, 722, 11, 879, 726, 14, 1126, 382, 99, 652, 367, 836, 1149, 87, 780, 566, 576, 18, 706, 712, 1213, 179, 1011, 714, 520, 295, 365, 1138, 1129, 108, 247, 547, 933, 1181, 172, 1005, 580, 1043, 238, 23, 492, 183, 1130, 548, 978, 1172, 268, 522, 490, 1195, 496, 1180, 110, 491, 255, 216, 955, 948, 1078, 828, 181, 228, 1162, 837, 1186, 692, 189, 288, 1075, 100, 143, 115, 1095, 967, 861, 1168, 45, 277, 485, 954, 117, 386, 777, 1024, 323, 44, 1087, 515, 241, 635, 873, 657, 375, 0, 1025, 814, 466, 67, 894, 1191, 133, 63, 299, 289, 354, 587, 586, 913, 340, 74, 866, 992, 816, 711, 48, 1103, 310, 410, 708, 575, 1092, 941, 1212, 471, 463, 574, 96, 881, 42, 342, 542, 278, 1200, 718, 521, 662, 1094, 910, 70, 671, 1121, 725, 73, 358, 834, 38, 620, 425, 427, 397, 13, 52, 809, 275, 864, 374, 232, 62, 1155, 151, 231, 691, 630, 811, 698, 765, 246, 717, 787, 470, 669, 831, 1067, 346, 356, 432, 309, 164, 218, 322, 1060, 308, 1098, 257, 863, 380, 401, 53, 474, 1048, 930, 481, 744, 988, 508, 422, 928, 291, 283, 849, 120, 544, 518, 1026, 826, 184, 900, 684, 1141, 148, 897, 766, 76, 19, 982, 223, 505, 781, 451, 84, 902, 665, 434, 596, 402, 642, 981, 638, 1062, 1066, 1018, 530, 608, 741, 440, 392, 540, 325, 206, 872, 201, 658, 464, 641, 306, 175, 1174, 1081, 1012, 797, 475, 1160, 1042, 424, 1159, 1178, 178, 898, 81, 312, 1110, 5, 921, 710, 36, 371, 47, 248, 617, 441, 265, 543, 390, 1150, 198, 631, 915, 1015, 689, 511, 6, 418, 95, 262, 335, 484, 1076, 1079, 1146, 409, 730, 673, 82, 622, 612, 1051, 264, 843, 931, 984, 182, 961, 200, 320, 1023, 579, 78, 215, 193, 841, 779, 294, 699, 899, 173, 645, 364, 350, 85, 1037, 1206, 276, 958, 192, 494, 796, 901, 83, 437, 869, 820, 348, 436, 298, 934, 851, 1132, 806, 720, 723, 679, 815, 1054, 468, 302, 874, 906, 65, 1183, 1190, 1154, 31, 195, 384, 414, 633, 519, 379, 986, 489, 147, 113, 273, 220, 259, 1133, 581, 1143, 529, 964, 144, 176, 886, 683, 844, 339, 927, 1017, 1088, 733, 925, 213, 334, 370, 395, 483, 960, 211, 253, 1192, 627, 823, 244, 1151, 840, 460, 637, 68, 551, 754, 911, 168, 137, 64, 270, 493, 1034, 46, 704, 937, 167, 932, 29, 912, 968, 1187, 486, 157, 1083, 1096, 1065, 1016, 1144, 693, 847, 1050, 279, 50, 1063, 477, 498, 332, 1108, 93, 945, 842, 1173, 589, 624, 17, 1214, 917, 1028, 585, 10, 888, 614, 1157, 369, 39, 747, 762, 1188, 1089]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3736844726192935
the save name prefix for this run is:  chkpt-ID_3736844726192935_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 921
rank avg (pred): 0.578 +- 0.002
mrr vals (pred, true): 0.017, 0.041
batch losses (mrrl, rdl): 0.0, 0.000290518

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1191
rank avg (pred): 0.451 +- 0.204
mrr vals (pred, true): 0.075, 0.044
batch losses (mrrl, rdl): 0.0, 1.55407e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1075
rank avg (pred): 0.072 +- 0.051
mrr vals (pred, true): 0.319, 0.355
batch losses (mrrl, rdl): 0.0, 1.30548e-05

Epoch over!
epoch time: 11.846

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 123
rank avg (pred): 0.424 +- 0.255
mrr vals (pred, true): 0.154, 0.049
batch losses (mrrl, rdl): 0.0, 5.2537e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 195
rank avg (pred): 0.428 +- 0.263
mrr vals (pred, true): 0.159, 0.045
batch losses (mrrl, rdl): 0.0, 5.0219e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 924
rank avg (pred): 0.443 +- 0.258
mrr vals (pred, true): 0.138, 0.042
batch losses (mrrl, rdl): 0.0, 7.1374e-06

Epoch over!
epoch time: 11.718

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1171
rank avg (pred): 0.440 +- 0.258
mrr vals (pred, true): 0.130, 0.040
batch losses (mrrl, rdl): 0.0, 7.3952e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 257
rank avg (pred): 0.120 +- 0.086
mrr vals (pred, true): 0.267, 0.305
batch losses (mrrl, rdl): 0.0, 4.6708e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1138
rank avg (pred): 0.135 +- 0.099
mrr vals (pred, true): 0.260, 0.261
batch losses (mrrl, rdl): 0.0, 1.91214e-05

Epoch over!
epoch time: 11.78

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1062
rank avg (pred): 0.117 +- 0.084
mrr vals (pred, true): 0.255, 0.305
batch losses (mrrl, rdl): 0.0, 8.844e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 925
rank avg (pred): 0.464 +- 0.247
mrr vals (pred, true): 0.087, 0.043
batch losses (mrrl, rdl): 0.0, 5.2969e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 952
rank avg (pred): 0.471 +- 0.247
mrr vals (pred, true): 0.096, 0.041
batch losses (mrrl, rdl): 0.0, 9.9244e-06

Epoch over!
epoch time: 11.885

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 346
rank avg (pred): 0.419 +- 0.270
mrr vals (pred, true): 0.145, 0.052
batch losses (mrrl, rdl): 0.0, 8.8925e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1165
rank avg (pred): 0.439 +- 0.262
mrr vals (pred, true): 0.121, 0.042
batch losses (mrrl, rdl): 0.0, 4.3221e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 926
rank avg (pred): 0.449 +- 0.257
mrr vals (pred, true): 0.113, 0.046
batch losses (mrrl, rdl): 0.0, 3.958e-06

Epoch over!
epoch time: 11.931

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1198
rank avg (pred): 0.448 +- 0.258
mrr vals (pred, true): 0.111, 0.044
batch losses (mrrl, rdl): 0.0371014401, 4.8642e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 6
rank avg (pred): 0.092 +- 0.062
mrr vals (pred, true): 0.269, 0.255
batch losses (mrrl, rdl): 0.001876886, 7.9444e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 291
rank avg (pred): 0.073 +- 0.051
mrr vals (pred, true): 0.300, 0.293
batch losses (mrrl, rdl): 0.0005197531, 9.3106e-06

Epoch over!
epoch time: 12.07

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 760
rank avg (pred): 0.502 +- 0.186
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001926622, 6.0112e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 802
rank avg (pred): 0.469 +- 0.156
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 1.20179e-05, 3.6334e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1106
rank avg (pred): 0.452 +- 0.160
mrr vals (pred, true): 0.056, 0.056
batch losses (mrrl, rdl): 0.0003040773, 6.95117e-05

Epoch over!
epoch time: 12.125

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 31
rank avg (pred): 0.106 +- 0.072
mrr vals (pred, true): 0.266, 0.269
batch losses (mrrl, rdl): 0.0001059606, 2.6199e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 743
rank avg (pred): 0.191 +- 0.124
mrr vals (pred, true): 0.204, 0.235
batch losses (mrrl, rdl): 0.009632539, 2.76883e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1188
rank avg (pred): 0.479 +- 0.141
mrr vals (pred, true): 0.043, 0.046
batch losses (mrrl, rdl): 0.0004495149, 4.83101e-05

Epoch over!
epoch time: 11.97

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 641
rank avg (pred): 0.503 +- 0.146
mrr vals (pred, true): 0.042, 0.047
batch losses (mrrl, rdl): 0.0005721941, 0.0001074654

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 794
rank avg (pred): 0.453 +- 0.133
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001820792, 4.69871e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1187
rank avg (pred): 0.481 +- 0.138
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0002145694, 6.08208e-05

Epoch over!
epoch time: 12.064

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 349
rank avg (pred): 0.456 +- 0.146
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 0.0001036926, 7.0931e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 900
rank avg (pred): 0.348 +- 0.236
mrr vals (pred, true): 0.219, 0.144
batch losses (mrrl, rdl): 0.0557776354, 0.0002936155

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1133
rank avg (pred): 0.429 +- 0.147
mrr vals (pred, true): 0.059, 0.043
batch losses (mrrl, rdl): 0.0008089945, 5.56558e-05

Epoch over!
epoch time: 12.081

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 13
rank avg (pred): 0.155 +- 0.106
mrr vals (pred, true): 0.257, 0.255
batch losses (mrrl, rdl): 3.30773e-05, 5.38364e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 183
rank avg (pred): 0.457 +- 0.134
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 8.93783e-05, 4.46799e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 317
rank avg (pred): 0.114 +- 0.081
mrr vals (pred, true): 0.299, 0.281
batch losses (mrrl, rdl): 0.0032156936, 3.9045e-06

Epoch over!
epoch time: 12.228

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 806
rank avg (pred): 0.431 +- 0.133
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 2.4858e-06, 6.86546e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 776
rank avg (pred): 0.468 +- 0.138
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 2.05446e-05, 4.18161e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1049
rank avg (pred): 0.467 +- 0.148
mrr vals (pred, true): 0.055, 0.042
batch losses (mrrl, rdl): 0.0002349162, 4.06804e-05

Epoch over!
epoch time: 12.02

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 329
rank avg (pred): 0.439 +- 0.137
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 1.5577e-05, 4.59681e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1172
rank avg (pred): 0.446 +- 0.131
mrr vals (pred, true): 0.048, 0.047
batch losses (mrrl, rdl): 5.22437e-05, 5.13444e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 960
rank avg (pred): 0.427 +- 0.132
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 2.42893e-05, 5.83371e-05

Epoch over!
epoch time: 12.069

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 890
rank avg (pred): 0.455 +- 0.131
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 8.60705e-05, 5.23828e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 862
rank avg (pred): 0.472 +- 0.130
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001570021, 5.0122e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1132
rank avg (pred): 0.440 +- 0.142
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.0003848922, 6.22784e-05

Epoch over!
epoch time: 12.003

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 335
rank avg (pred): 0.452 +- 0.136
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 1.491e-07, 6.762e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 263
rank avg (pred): 0.069 +- 0.048
mrr vals (pred, true): 0.331, 0.276
batch losses (mrrl, rdl): 0.0299931783, 6.01314e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 453
rank avg (pred): 0.451 +- 0.129
mrr vals (pred, true): 0.048, 0.042
batch losses (mrrl, rdl): 4.75152e-05, 5.87699e-05

Epoch over!
epoch time: 12.116

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.409 +- 0.143
mrr vals (pred, true): 0.063, 0.044

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.05221 	 0.04100 	 ~...
   43 	     1 	 0.05456 	 0.04108 	 ~...
   16 	     2 	 0.05297 	 0.04137 	 ~...
   58 	     3 	 0.05544 	 0.04160 	 ~...
   77 	     4 	 0.05762 	 0.04162 	 ~...
   42 	     5 	 0.05455 	 0.04184 	 ~...
   36 	     6 	 0.05423 	 0.04189 	 ~...
   31 	     7 	 0.05384 	 0.04209 	 ~...
   37 	     8 	 0.05431 	 0.04214 	 ~...
   22 	     9 	 0.05329 	 0.04216 	 ~...
   73 	    10 	 0.05743 	 0.04222 	 ~...
   32 	    11 	 0.05389 	 0.04223 	 ~...
   25 	    12 	 0.05362 	 0.04224 	 ~...
   11 	    13 	 0.05237 	 0.04224 	 ~...
   47 	    14 	 0.05470 	 0.04225 	 ~...
   28 	    15 	 0.05366 	 0.04234 	 ~...
   60 	    16 	 0.05547 	 0.04245 	 ~...
   41 	    17 	 0.05452 	 0.04248 	 ~...
   17 	    18 	 0.05302 	 0.04269 	 ~...
   78 	    19 	 0.05764 	 0.04274 	 ~...
   61 	    20 	 0.05549 	 0.04286 	 ~...
   63 	    21 	 0.05558 	 0.04288 	 ~...
   38 	    22 	 0.05438 	 0.04291 	 ~...
   65 	    23 	 0.05574 	 0.04306 	 ~...
   70 	    24 	 0.05672 	 0.04307 	 ~...
    8 	    25 	 0.05233 	 0.04311 	 ~...
    3 	    26 	 0.05176 	 0.04321 	 ~...
    0 	    27 	 0.04959 	 0.04330 	 ~...
   29 	    28 	 0.05368 	 0.04338 	 ~...
   62 	    29 	 0.05556 	 0.04345 	 ~...
   66 	    30 	 0.05589 	 0.04358 	 ~...
    9 	    31 	 0.05234 	 0.04362 	 ~...
   12 	    32 	 0.05242 	 0.04366 	 ~...
    4 	    33 	 0.05197 	 0.04388 	 ~...
   45 	    34 	 0.05457 	 0.04389 	 ~...
   24 	    35 	 0.05341 	 0.04392 	 ~...
   51 	    36 	 0.05502 	 0.04393 	 ~...
   23 	    37 	 0.05331 	 0.04401 	 ~...
   13 	    38 	 0.05247 	 0.04404 	 ~...
   84 	    39 	 0.06256 	 0.04406 	 ~...
   64 	    40 	 0.05571 	 0.04412 	 ~...
   26 	    41 	 0.05363 	 0.04444 	 ~...
   20 	    42 	 0.05321 	 0.04447 	 ~...
   10 	    43 	 0.05234 	 0.04448 	 ~...
   49 	    44 	 0.05489 	 0.04475 	 ~...
   75 	    45 	 0.05756 	 0.04485 	 ~...
    7 	    46 	 0.05232 	 0.04485 	 ~...
   74 	    47 	 0.05746 	 0.04491 	 ~...
    1 	    48 	 0.05055 	 0.04494 	 ~...
   83 	    49 	 0.06096 	 0.04513 	 ~...
   80 	    50 	 0.05819 	 0.04538 	 ~...
   59 	    51 	 0.05544 	 0.04546 	 ~...
   30 	    52 	 0.05381 	 0.04570 	 ~...
   39 	    53 	 0.05442 	 0.04575 	 ~...
    6 	    54 	 0.05227 	 0.04622 	 ~...
   55 	    55 	 0.05527 	 0.04629 	 ~...
   18 	    56 	 0.05317 	 0.04632 	 ~...
   50 	    57 	 0.05493 	 0.04645 	 ~...
   79 	    58 	 0.05766 	 0.04659 	 ~...
   57 	    59 	 0.05534 	 0.04678 	 ~...
   19 	    60 	 0.05320 	 0.04688 	 ~...
    2 	    61 	 0.05119 	 0.04715 	 ~...
   35 	    62 	 0.05410 	 0.04733 	 ~...
   81 	    63 	 0.05851 	 0.04734 	 ~...
   15 	    64 	 0.05289 	 0.04802 	 ~...
   72 	    65 	 0.05729 	 0.04846 	 ~...
   40 	    66 	 0.05445 	 0.04874 	 ~...
   54 	    67 	 0.05526 	 0.04884 	 ~...
   34 	    68 	 0.05409 	 0.04917 	 ~...
   14 	    69 	 0.05263 	 0.04950 	 ~...
   71 	    70 	 0.05703 	 0.04970 	 ~...
   44 	    71 	 0.05457 	 0.05151 	 ~...
   56 	    72 	 0.05532 	 0.05154 	 ~...
   67 	    73 	 0.05596 	 0.05214 	 ~...
   52 	    74 	 0.05503 	 0.05231 	 ~...
   46 	    75 	 0.05467 	 0.05342 	 ~...
   76 	    76 	 0.05761 	 0.05349 	 ~...
   53 	    77 	 0.05510 	 0.05375 	 ~...
   27 	    78 	 0.05363 	 0.05389 	 ~...
   82 	    79 	 0.06018 	 0.05404 	 ~...
   69 	    80 	 0.05670 	 0.05405 	 ~...
   85 	    81 	 0.06463 	 0.05416 	 ~...
   33 	    82 	 0.05407 	 0.05512 	 ~...
   48 	    83 	 0.05479 	 0.05586 	 ~...
   68 	    84 	 0.05637 	 0.05642 	 ~...
   21 	    85 	 0.05323 	 0.05806 	 ~...
   86 	    86 	 0.15722 	 0.10280 	 m..s
   88 	    87 	 0.21915 	 0.17582 	 m..s
   95 	    88 	 0.23319 	 0.17772 	 m..s
   93 	    89 	 0.23191 	 0.17873 	 m..s
   92 	    90 	 0.22815 	 0.18602 	 m..s
   87 	    91 	 0.16597 	 0.18648 	 ~...
   91 	    92 	 0.22577 	 0.20159 	 ~...
   90 	    93 	 0.22328 	 0.21070 	 ~...
   89 	    94 	 0.22201 	 0.22306 	 ~...
  100 	    95 	 0.24565 	 0.22608 	 ~...
   96 	    96 	 0.23397 	 0.22898 	 ~...
   94 	    97 	 0.23266 	 0.22913 	 ~...
   97 	    98 	 0.24036 	 0.24023 	 ~...
   98 	    99 	 0.24164 	 0.24483 	 ~...
  112 	   100 	 0.28968 	 0.25649 	 m..s
   99 	   101 	 0.24381 	 0.25665 	 ~...
  101 	   102 	 0.24580 	 0.26647 	 ~...
  114 	   103 	 0.29653 	 0.26761 	 ~...
  102 	   104 	 0.24723 	 0.26867 	 ~...
  104 	   105 	 0.27557 	 0.27037 	 ~...
  107 	   106 	 0.28066 	 0.27120 	 ~...
  105 	   107 	 0.27567 	 0.27126 	 ~...
  113 	   108 	 0.29302 	 0.27473 	 ~...
  110 	   109 	 0.28546 	 0.27906 	 ~...
  111 	   110 	 0.28778 	 0.28601 	 ~...
  115 	   111 	 0.30641 	 0.28884 	 ~...
  103 	   112 	 0.27260 	 0.28991 	 ~...
  118 	   113 	 0.31386 	 0.29096 	 ~...
  109 	   114 	 0.28224 	 0.29469 	 ~...
  106 	   115 	 0.27792 	 0.31448 	 m..s
  108 	   116 	 0.28099 	 0.31651 	 m..s
  117 	   117 	 0.30830 	 0.32126 	 ~...
  116 	   118 	 0.30822 	 0.32833 	 ~...
  119 	   119 	 0.33606 	 0.34116 	 ~...
  120 	   120 	 0.33999 	 0.35877 	 ~...
==========================================
r_mrr = 0.9905843138694763
r2_mrr = 0.9741899967193604
spearmanr_mrr@5 = 0.9504083395004272
spearmanr_mrr@10 = 0.9771711230278015
spearmanr_mrr@50 = 0.9894086718559265
spearmanr_mrr@100 = 0.9948435425758362
spearmanr_mrr@All = 0.9952967166900635
==========================================
test time: 0.387
Done Testing dataset Kinships
total time taken: 187.25889348983765
training time taken: 180.38069677352905
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9906)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9742)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9504)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9772)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9894)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9948)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9953)}}, 'test_loss': {'TransE': {'Kinships': 0.2647197946862434}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 6157148552337508
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [461, 1151, 745, 655, 689, 447, 754, 1140, 653, 691, 631, 510, 527, 123, 704, 196, 1051, 377, 137, 1027, 672, 981, 198, 928, 1076, 81, 1085, 499, 1071, 925, 215, 860, 199, 651, 238, 944, 1160, 1026, 122, 954, 426, 1074, 213, 643, 821, 617, 1007, 281, 52, 747, 540, 544, 398, 249, 729, 416, 248, 216, 712, 474, 1134, 403, 911, 1182, 378, 739, 563, 306, 9, 484, 25, 124, 721, 857, 325, 1045, 702, 1165, 262, 1000, 898, 1192, 331, 280, 1036, 197, 1002, 918, 775, 1022, 705, 824, 355, 440, 6, 1070, 568, 60, 1052, 17, 768, 688, 1014, 1193, 489, 927, 781, 21, 869, 1078, 1171, 1167, 84, 567, 1159, 10, 33, 517, 538, 562, 906]
valid_ids (0): []
train_ids (1094): [1212, 289, 465, 236, 539, 592, 476, 243, 1062, 376, 276, 985, 962, 131, 1046, 1099, 140, 120, 365, 518, 706, 256, 723, 916, 995, 233, 532, 485, 318, 173, 0, 1179, 316, 397, 301, 1149, 175, 935, 576, 336, 91, 1166, 1094, 208, 480, 228, 763, 362, 624, 176, 473, 941, 969, 1198, 727, 838, 434, 1132, 1173, 542, 939, 1141, 1073, 1180, 564, 346, 1133, 1177, 913, 503, 107, 324, 839, 335, 1121, 493, 658, 580, 410, 246, 622, 895, 896, 265, 585, 467, 218, 508, 468, 843, 291, 1169, 350, 810, 61, 194, 639, 1191, 610, 322, 558, 652, 1195, 1164, 1131, 1033, 253, 628, 1185, 613, 247, 951, 159, 149, 433, 386, 823, 687, 412, 383, 504, 345, 1048, 222, 675, 487, 28, 908, 597, 1112, 746, 51, 909, 574, 674, 988, 699, 885, 1013, 118, 830, 369, 603, 219, 251, 1058, 531, 141, 1100, 1049, 758, 615, 195, 514, 250, 96, 894, 1186, 638, 300, 186, 1196, 840, 43, 817, 997, 36, 1130, 591, 812, 1043, 390, 919, 1200, 636, 446, 227, 498, 549, 171, 1162, 1135, 1189, 978, 1020, 99, 1136, 179, 464, 224, 308, 214, 442, 646, 1095, 1075, 828, 922, 799, 311, 633, 816, 825, 14, 22, 333, 849, 1090, 695, 833, 303, 697, 996, 707, 694, 522, 725, 12, 555, 525, 798, 806, 845, 73, 682, 654, 413, 1029, 1003, 621, 1152, 1109, 74, 770, 483, 35, 192, 618, 943, 1201, 548, 1187, 590, 854, 328, 145, 418, 802, 663, 1143, 1154, 183, 871, 642, 1142, 815, 1064, 327, 762, 1213, 423, 989, 551, 842, 665, 559, 41, 818, 611, 268, 891, 856, 181, 92, 637, 1103, 931, 286, 600, 341, 874, 1088, 62, 992, 809, 1093, 711, 866, 670, 1108, 797, 430, 1042, 69, 190, 737, 2, 143, 819, 1044, 582, 156, 160, 556, 698, 88, 728, 1032, 589, 1053, 226, 482, 38, 3, 696, 44, 1050, 547, 1174, 760, 307, 204, 112, 947, 506, 488, 1125, 632, 1190, 1101, 769, 20, 220, 945, 732, 822, 800, 391, 202, 952, 528, 279, 462, 870, 910, 1056, 258, 625, 701, 379, 740, 835, 964, 1117, 623, 382, 210, 949, 163, 406, 172, 1008, 553, 374, 557, 859, 298, 387, 608, 97, 594, 892, 667, 310, 1001, 715, 1170, 805, 960, 101, 209, 212, 776, 385, 130, 836, 515, 782, 1104, 450, 546, 27, 720, 753, 332, 139, 774, 417, 620, 785, 719, 779, 647, 358, 76, 933, 57, 1161, 887, 95, 765, 363, 158, 321, 834, 142, 1176, 545, 89, 968, 505, 135, 569, 889, 678, 463, 972, 873, 1006, 449, 1089, 102, 755, 428, 109, 133, 448, 1158, 366, 221, 552, 154, 673, 861, 491, 437, 263, 957, 241, 645, 1205, 852, 1054, 169, 207, 152, 104, 855, 1081, 686, 534, 7, 37, 578, 629, 472, 924, 607, 255, 877, 930, 876, 1031, 1037, 370, 1097, 1175, 605, 305, 445, 151, 312, 1040, 1098, 296, 994, 1012, 264, 693, 513, 477, 736, 486, 541, 40, 188, 973, 150, 231, 656, 260, 319, 1123, 79, 273, 676, 90, 648, 1069, 535, 804, 1015, 864, 167, 343, 177, 184, 759, 337, 659, 767, 259, 1066, 93, 731, 1178, 826, 794, 381, 404, 875, 113, 294, 752, 103, 814, 1111, 347, 537, 901, 1146, 1148, 1019, 614, 115, 352, 903, 1059, 166, 1197, 980, 235, 1030, 459, 217, 201, 1113, 297, 1153, 436, 117, 583, 66, 974, 902, 1087, 1110, 764, 269, 827, 971, 900, 1206, 134, 271, 274, 356, 703, 1, 1055, 612, 1168, 126, 929, 965, 1194, 1139, 444, 677, 384, 844, 791, 441, 396, 496, 1183, 389, 47, 23, 714, 1156, 257, 86, 1004, 455, 106, 1009, 1082, 1207, 394, 185, 923, 533, 886, 1079, 1010, 1086, 148, 1202, 494, 664, 329, 986, 627, 427, 439, 334, 948, 967, 888, 560, 75, 979, 914, 921, 717, 868, 174, 1118, 1096, 1188, 530, 741, 203, 30, 657, 58, 744, 1063, 966, 1210, 554, 272, 1077, 650, 982, 1028, 666, 983, 1204, 526, 1091, 111, 419, 1083, 932, 671, 561, 283, 573, 1181, 1129, 65, 180, 915, 851, 1126, 127, 116, 883, 309, 48, 1214, 619, 904, 681, 644, 1038, 285, 708, 593, 63, 626, 566, 351, 1145, 880, 315, 435, 338, 795, 722, 1211, 119, 270, 1025, 178, 684, 453, 354, 261, 314, 277, 225, 950, 425, 460, 862, 187, 1155, 1061, 409, 451, 232, 878, 771, 414, 1068, 1065, 371, 516, 32, 344, 4, 572, 847, 718, 683, 466, 304, 793, 478, 523, 479, 738, 1120, 407, 372, 586, 811, 481, 780, 831, 596, 15, 543, 938, 223, 846, 588, 302, 602, 278, 39, 205, 565, 164, 71, 500, 108, 920, 599, 807, 934, 54, 1209, 5, 1172, 367, 1163, 606, 733, 716, 373, 411, 709, 942, 520, 146, 778, 239, 400, 813, 1128, 136, 266, 211, 431, 955, 293, 13, 8, 317, 45, 457, 1060, 832, 288, 59, 340, 193, 157, 640, 368, 863, 495, 469, 679, 867, 42, 83, 240, 275, 342, 899, 155, 422, 31, 16, 926, 787, 395, 19, 990, 550, 501, 245, 1017, 710, 897, 443, 206, 796, 1016, 524, 380, 1057, 353, 730, 85, 529, 168, 144, 668, 128, 595, 244, 571, 584, 700, 907, 1021, 1092, 78, 641, 890, 1184, 1124, 841, 749, 375, 242, 536, 64, 53, 320, 1041, 46, 323, 977, 1023, 287, 72, 77, 121, 837, 349, 601, 748, 165, 1018, 853, 751, 1137, 507, 685, 937, 454, 200, 230, 415, 497, 575, 429, 829, 991, 326, 1127, 786, 777, 742, 401, 49, 267, 56, 884, 129, 34, 879, 132, 1157, 912, 850, 1034, 295, 872, 734, 392, 191, 458, 432, 959, 348, 773, 808, 313, 290, 987, 70, 865, 940, 1084, 452, 976, 635, 1105, 772, 361, 284, 254, 1147, 55, 399, 471, 357, 512, 456, 519, 669, 11, 609, 364, 750, 1047, 1005, 604, 713, 690, 125, 50, 661, 587, 660, 237, 956, 110, 388, 788, 946, 502, 958, 420, 511, 87, 438, 475, 735, 1138, 803, 80, 162, 1122, 1199, 790, 993, 402, 470, 67, 299, 234, 1102, 726, 252, 147, 998, 1144, 1119, 1072, 509, 858, 792, 936, 521, 492, 359, 917, 801, 24, 882, 68, 649, 598, 421, 393, 905, 292, 161, 581, 784, 229, 1115, 724, 1039, 360, 692, 761, 579, 26, 1116, 182, 616, 94, 138, 405, 100, 662, 408, 105, 1080, 783, 961, 1106, 282, 170, 1107, 634, 680, 570, 29, 330, 577, 881, 424, 953, 339, 1203, 970, 153, 1067, 848, 1035, 114, 757, 984, 1150, 82, 1114, 999, 18, 98, 766, 1024, 820, 789, 743, 1208, 756, 1011, 893, 630, 189, 963, 975, 490]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9636499047713794
the save name prefix for this run is:  chkpt-ID_9636499047713794_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 738
rank avg (pred): 0.501 +- 0.001
mrr vals (pred, true): 0.019, 0.178
batch losses (mrrl, rdl): 0.0, 0.001510283

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 414
rank avg (pred): 0.455 +- 0.265
mrr vals (pred, true): 0.066, 0.046
batch losses (mrrl, rdl): 0.0, 1.1437e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 202
rank avg (pred): 0.458 +- 0.272
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.0, 3.1225e-06

Epoch over!
epoch time: 11.986

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 779
rank avg (pred): 0.454 +- 0.264
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0, 3.822e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 73
rank avg (pred): 0.095 +- 0.114
mrr vals (pred, true): 0.221, 0.272
batch losses (mrrl, rdl): 0.0, 2.341e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 974
rank avg (pred): 0.115 +- 0.132
mrr vals (pred, true): 0.187, 0.297
batch losses (mrrl, rdl): 0.0, 2.1906e-06

Epoch over!
epoch time: 11.831

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 842
rank avg (pred): 0.472 +- 0.258
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0, 1.3933e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 670
rank avg (pred): 0.448 +- 0.265
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.0, 4.256e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 808
rank avg (pred): 0.470 +- 0.261
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 0.0, 4.16e-07

Epoch over!
epoch time: 11.865

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 924
rank avg (pred): 0.465 +- 0.263
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 0.0, 1.8348e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 433
rank avg (pred): 0.442 +- 0.272
mrr vals (pred, true): 0.060, 0.042
batch losses (mrrl, rdl): 0.0, 2.16782e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1059
rank avg (pred): 0.121 +- 0.151
mrr vals (pred, true): 0.186, 0.341
batch losses (mrrl, rdl): 0.0, 1.48854e-05

Epoch over!
epoch time: 11.861

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 641
rank avg (pred): 0.462 +- 0.266
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 0.0, 1.9153e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 87
rank avg (pred): 0.480 +- 0.268
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 0.0, 2.4238e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 782
rank avg (pred): 0.446 +- 0.269
mrr vals (pred, true): 0.063, 0.045
batch losses (mrrl, rdl): 0.0, 4.7959e-06

Epoch over!
epoch time: 11.8

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 791
rank avg (pred): 0.464 +- 0.267
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002310921, 5.02e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 64
rank avg (pred): 0.057 +- 0.039
mrr vals (pred, true): 0.282, 0.263
batch losses (mrrl, rdl): 0.0035769483, 4.5074e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 219
rank avg (pred): 0.470 +- 0.181
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0001482403, 2.52965e-05

Epoch over!
epoch time: 12.312

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 776
rank avg (pred): 0.474 +- 0.192
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.82872e-05, 1.7454e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1112
rank avg (pred): 0.461 +- 0.204
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002226984, 1.58561e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 577
rank avg (pred): 0.465 +- 0.196
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 6.63688e-05, 2.36986e-05

Epoch over!
epoch time: 12.064

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 733
rank avg (pred): 0.214 +- 0.198
mrr vals (pred, true): 0.193, 0.214
batch losses (mrrl, rdl): 0.0041590482, 1.55998e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 490
rank avg (pred): 0.220 +- 0.223
mrr vals (pred, true): 0.208, 0.192
batch losses (mrrl, rdl): 0.0025056421, 0.0002006807

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 48
rank avg (pred): 0.127 +- 0.148
mrr vals (pred, true): 0.260, 0.296
batch losses (mrrl, rdl): 0.0128020514, 2.77136e-05

Epoch over!
epoch time: 12.052

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1117
rank avg (pred): 0.440 +- 0.169
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 1.10788e-05, 5.26726e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1077
rank avg (pred): 0.081 +- 0.108
mrr vals (pred, true): 0.331, 0.337
batch losses (mrrl, rdl): 0.0003470558, 9.904e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1004
rank avg (pred): 0.469 +- 0.205
mrr vals (pred, true): 0.049, 0.056
batch losses (mrrl, rdl): 1.01213e-05, 4.24099e-05

Epoch over!
epoch time: 12.021

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 822
rank avg (pred): 0.189 +- 0.179
mrr vals (pred, true): 0.237, 0.259
batch losses (mrrl, rdl): 0.0047343867, 1.60518e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 598
rank avg (pred): 0.476 +- 0.220
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 5.39414e-05, 1.15251e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 212
rank avg (pred): 0.434 +- 0.172
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 7.5538e-06, 6.72127e-05

Epoch over!
epoch time: 11.999

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 854
rank avg (pred): 0.465 +- 0.211
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 9.657e-07, 1.86461e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 494
rank avg (pred): 0.250 +- 0.229
mrr vals (pred, true): 0.214, 0.219
batch losses (mrrl, rdl): 0.000253383, 0.0002364535

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 960
rank avg (pred): 0.488 +- 0.240
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 6.7637e-06, 1.19254e-05

Epoch over!
epoch time: 12.167

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 309
rank avg (pred): 0.133 +- 0.152
mrr vals (pred, true): 0.296, 0.301
batch losses (mrrl, rdl): 0.0002822427, 4.93092e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 169
rank avg (pred): 0.485 +- 0.226
mrr vals (pred, true): 0.044, 0.044
batch losses (mrrl, rdl): 0.0003592294, 1.35809e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 573
rank avg (pred): 0.517 +- 0.256
mrr vals (pred, true): 0.042, 0.047
batch losses (mrrl, rdl): 0.0005886194, 2.95346e-05

Epoch over!
epoch time: 11.995

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 603
rank avg (pred): 0.502 +- 0.256
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001549324, 3.17852e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 427
rank avg (pred): 0.472 +- 0.232
mrr vals (pred, true): 0.045, 0.041
batch losses (mrrl, rdl): 0.0002210707, 1.54292e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 3
rank avg (pred): 0.153 +- 0.162
mrr vals (pred, true): 0.279, 0.262
batch losses (mrrl, rdl): 0.0031054204, 6.74732e-05

Epoch over!
epoch time: 12.011

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 915
rank avg (pred): 0.323 +- 0.266
mrr vals (pred, true): 0.190, 0.226
batch losses (mrrl, rdl): 0.0129297506, 0.0006058794

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 418
rank avg (pred): 0.460 +- 0.233
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.00015461, 1.56867e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 471
rank avg (pred): 0.461 +- 0.232
mrr vals (pred, true): 0.043, 0.043
batch losses (mrrl, rdl): 0.000494246, 2.40785e-05

Epoch over!
epoch time: 11.958

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 76
rank avg (pred): 0.156 +- 0.163
mrr vals (pred, true): 0.291, 0.264
batch losses (mrrl, rdl): 0.0074738567, 8.21264e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 263
rank avg (pred): 0.151 +- 0.165
mrr vals (pred, true): 0.299, 0.276
batch losses (mrrl, rdl): 0.0050997613, 4.16371e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1068
rank avg (pred): 0.122 +- 0.160
mrr vals (pred, true): 0.388, 0.359
batch losses (mrrl, rdl): 0.0084696617, 3.66519e-05

Epoch over!
epoch time: 12.032

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.441 +- 0.243
mrr vals (pred, true): 0.052, 0.044

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   70 	     0 	 0.05701 	 0.03862 	 ~...
   49 	     1 	 0.05401 	 0.04001 	 ~...
    4 	     2 	 0.05087 	 0.04035 	 ~...
   63 	     3 	 0.05536 	 0.04198 	 ~...
    9 	     4 	 0.05194 	 0.04207 	 ~...
   33 	     5 	 0.05313 	 0.04209 	 ~...
   58 	     6 	 0.05490 	 0.04214 	 ~...
   17 	     7 	 0.05242 	 0.04216 	 ~...
   55 	     8 	 0.05467 	 0.04217 	 ~...
   44 	     9 	 0.05379 	 0.04225 	 ~...
   43 	    10 	 0.05377 	 0.04231 	 ~...
   27 	    11 	 0.05301 	 0.04233 	 ~...
   28 	    12 	 0.05304 	 0.04251 	 ~...
   37 	    13 	 0.05339 	 0.04263 	 ~...
   21 	    14 	 0.05279 	 0.04271 	 ~...
   26 	    15 	 0.05298 	 0.04286 	 ~...
   13 	    16 	 0.05206 	 0.04296 	 ~...
   60 	    17 	 0.05515 	 0.04298 	 ~...
   35 	    18 	 0.05321 	 0.04301 	 ~...
    3 	    19 	 0.05079 	 0.04333 	 ~...
   41 	    20 	 0.05364 	 0.04340 	 ~...
   67 	    21 	 0.05585 	 0.04349 	 ~...
   18 	    22 	 0.05247 	 0.04363 	 ~...
   57 	    23 	 0.05487 	 0.04370 	 ~...
   54 	    24 	 0.05463 	 0.04377 	 ~...
   12 	    25 	 0.05198 	 0.04377 	 ~...
   15 	    26 	 0.05222 	 0.04384 	 ~...
   39 	    27 	 0.05344 	 0.04399 	 ~...
   74 	    28 	 0.05799 	 0.04409 	 ~...
    2 	    29 	 0.05072 	 0.04418 	 ~...
   53 	    30 	 0.05462 	 0.04421 	 ~...
   48 	    31 	 0.05399 	 0.04422 	 ~...
   31 	    32 	 0.05309 	 0.04432 	 ~...
    5 	    33 	 0.05094 	 0.04438 	 ~...
    6 	    34 	 0.05185 	 0.04439 	 ~...
   11 	    35 	 0.05198 	 0.04453 	 ~...
   68 	    36 	 0.05674 	 0.04458 	 ~...
   56 	    37 	 0.05470 	 0.04460 	 ~...
   24 	    38 	 0.05295 	 0.04465 	 ~...
   14 	    39 	 0.05214 	 0.04479 	 ~...
   19 	    40 	 0.05256 	 0.04494 	 ~...
   51 	    41 	 0.05405 	 0.04503 	 ~...
    7 	    42 	 0.05193 	 0.04516 	 ~...
   16 	    43 	 0.05233 	 0.04517 	 ~...
   50 	    44 	 0.05402 	 0.04541 	 ~...
    1 	    45 	 0.05063 	 0.04546 	 ~...
   46 	    46 	 0.05394 	 0.04558 	 ~...
   32 	    47 	 0.05310 	 0.04562 	 ~...
   69 	    48 	 0.05681 	 0.04578 	 ~...
   20 	    49 	 0.05256 	 0.04584 	 ~...
   38 	    50 	 0.05343 	 0.04586 	 ~...
   25 	    51 	 0.05298 	 0.04588 	 ~...
   22 	    52 	 0.05285 	 0.04597 	 ~...
   71 	    53 	 0.05720 	 0.04616 	 ~...
   29 	    54 	 0.05305 	 0.04748 	 ~...
   72 	    55 	 0.05724 	 0.04753 	 ~...
   10 	    56 	 0.05197 	 0.04835 	 ~...
   34 	    57 	 0.05317 	 0.04852 	 ~...
   61 	    58 	 0.05521 	 0.04890 	 ~...
   59 	    59 	 0.05508 	 0.04906 	 ~...
   73 	    60 	 0.05761 	 0.04911 	 ~...
    0 	    61 	 0.04969 	 0.04945 	 ~...
   23 	    62 	 0.05293 	 0.04949 	 ~...
   42 	    63 	 0.05368 	 0.04970 	 ~...
   40 	    64 	 0.05345 	 0.05011 	 ~...
   66 	    65 	 0.05578 	 0.05014 	 ~...
   62 	    66 	 0.05526 	 0.05143 	 ~...
   30 	    67 	 0.05309 	 0.05194 	 ~...
   36 	    68 	 0.05333 	 0.05401 	 ~...
   65 	    69 	 0.05564 	 0.05460 	 ~...
   45 	    70 	 0.05384 	 0.05512 	 ~...
   47 	    71 	 0.05398 	 0.05566 	 ~...
   64 	    72 	 0.05562 	 0.05696 	 ~...
   52 	    73 	 0.05420 	 0.05711 	 ~...
    8 	    74 	 0.05193 	 0.05790 	 ~...
   75 	    75 	 0.18034 	 0.10283 	 m..s
   77 	    76 	 0.19274 	 0.16203 	 m..s
   86 	    77 	 0.22215 	 0.17873 	 m..s
   76 	    78 	 0.19160 	 0.18413 	 ~...
   79 	    79 	 0.21536 	 0.19216 	 ~...
   85 	    80 	 0.22184 	 0.19653 	 ~...
   92 	    81 	 0.23966 	 0.19838 	 m..s
   84 	    82 	 0.22124 	 0.20038 	 ~...
   87 	    83 	 0.22740 	 0.20646 	 ~...
   82 	    84 	 0.22018 	 0.20744 	 ~...
   83 	    85 	 0.22027 	 0.20910 	 ~...
   80 	    86 	 0.21614 	 0.21095 	 ~...
   91 	    87 	 0.23755 	 0.21172 	 ~...
   81 	    88 	 0.21752 	 0.21326 	 ~...
   78 	    89 	 0.20453 	 0.21544 	 ~...
   90 	    90 	 0.23688 	 0.22618 	 ~...
   88 	    91 	 0.22751 	 0.22697 	 ~...
   89 	    92 	 0.23040 	 0.23675 	 ~...
   95 	    93 	 0.24567 	 0.24312 	 ~...
   99 	    94 	 0.26653 	 0.24849 	 ~...
  100 	    95 	 0.26666 	 0.25503 	 ~...
   98 	    96 	 0.26585 	 0.25575 	 ~...
  103 	    97 	 0.26923 	 0.25576 	 ~...
  114 	    98 	 0.28367 	 0.25742 	 ~...
   93 	    99 	 0.24036 	 0.26225 	 ~...
  112 	   100 	 0.27985 	 0.26262 	 ~...
   97 	   101 	 0.26436 	 0.27075 	 ~...
  104 	   102 	 0.26940 	 0.27167 	 ~...
   94 	   103 	 0.24091 	 0.27200 	 m..s
  101 	   104 	 0.26705 	 0.27549 	 ~...
  105 	   105 	 0.27196 	 0.27845 	 ~...
  113 	   106 	 0.28279 	 0.28089 	 ~...
  106 	   107 	 0.27262 	 0.28112 	 ~...
  107 	   108 	 0.27544 	 0.28228 	 ~...
  110 	   109 	 0.27934 	 0.28497 	 ~...
   96 	   110 	 0.24981 	 0.28917 	 m..s
  102 	   111 	 0.26900 	 0.29004 	 ~...
  111 	   112 	 0.27971 	 0.29328 	 ~...
  108 	   113 	 0.27749 	 0.29543 	 ~...
  116 	   114 	 0.32971 	 0.30148 	 ~...
  109 	   115 	 0.27779 	 0.31490 	 m..s
  115 	   116 	 0.32897 	 0.31539 	 ~...
  119 	   117 	 0.33752 	 0.32959 	 ~...
  118 	   118 	 0.33525 	 0.33857 	 ~...
  120 	   119 	 0.34075 	 0.34190 	 ~...
  117 	   120 	 0.33081 	 0.34545 	 ~...
==========================================
r_mrr = 0.9915812015533447
r2_mrr = 0.9785696268081665
spearmanr_mrr@5 = 0.9194828271865845
spearmanr_mrr@10 = 0.9276185631752014
spearmanr_mrr@50 = 0.9793514609336853
spearmanr_mrr@100 = 0.9951441287994385
spearmanr_mrr@All = 0.9957616329193115
==========================================
test time: 0.384
Done Testing dataset Kinships
total time taken: 187.59378266334534
training time taken: 180.42175149917603
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9916)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9786)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9195)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9276)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9794)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9951)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9958)}}, 'test_loss': {'TransE': {'Kinships': 0.24954120449092443}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 4621304194246565
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [785, 1090, 60, 1127, 181, 612, 681, 70, 95, 1035, 720, 365, 643, 832, 935, 445, 391, 192, 983, 211, 1044, 604, 1171, 455, 875, 963, 637, 930, 1214, 901, 26, 1110, 504, 49, 1052, 484, 187, 372, 442, 179, 1049, 704, 1109, 629, 713, 255, 305, 1096, 805, 94, 837, 924, 244, 352, 56, 91, 534, 490, 960, 562, 953, 256, 625, 863, 1179, 480, 336, 765, 198, 556, 111, 771, 979, 974, 1108, 666, 1141, 467, 874, 748, 921, 1006, 133, 197, 632, 1012, 499, 932, 1071, 344, 191, 900, 552, 216, 972, 812, 1203, 119, 368, 335, 259, 544, 155, 396, 1060, 1069, 170, 193, 33, 1080, 9, 491, 428, 16, 184, 497, 645, 110, 672, 280, 1038]
valid_ids (0): []
train_ids (1094): [1002, 520, 728, 4, 112, 439, 833, 814, 618, 860, 636, 233, 297, 570, 324, 61, 517, 1200, 862, 320, 726, 53, 267, 686, 651, 703, 321, 715, 118, 742, 1021, 939, 716, 987, 648, 955, 652, 316, 1166, 376, 28, 298, 970, 1170, 12, 671, 760, 357, 262, 127, 587, 877, 63, 677, 403, 242, 898, 460, 196, 628, 1160, 471, 945, 413, 423, 1125, 740, 1158, 754, 880, 51, 1093, 503, 125, 1104, 857, 350, 64, 616, 1033, 1210, 478, 931, 1128, 241, 1190, 1173, 1199, 456, 190, 132, 876, 23, 816, 100, 710, 1068, 1204, 212, 400, 1087, 143, 554, 980, 157, 793, 234, 688, 917, 349, 370, 735, 1119, 992, 177, 355, 188, 565, 425, 801, 772, 724, 905, 270, 661, 576, 910, 1167, 543, 333, 1026, 58, 692, 340, 164, 301, 1151, 718, 328, 392, 257, 693, 856, 500, 285, 142, 165, 849, 1115, 828, 228, 701, 402, 887, 678, 360, 2, 1070, 904, 803, 751, 1001, 1092, 730, 1032, 437, 896, 123, 514, 535, 14, 1189, 548, 1123, 598, 1091, 238, 521, 865, 827, 1154, 382, 607, 727, 691, 844, 667, 773, 443, 1024, 419, 229, 343, 1000, 861, 501, 408, 135, 452, 1046, 753, 1034, 1113, 555, 258, 1121, 946, 202, 1043, 810, 302, 787, 1134, 537, 659, 83, 690, 226, 729, 733, 5, 362, 743, 454, 778, 545, 845, 1082, 398, 653, 568, 1106, 278, 951, 811, 22, 466, 487, 139, 796, 513, 140, 790, 920, 325, 182, 627, 81, 928, 32, 749, 231, 93, 783, 1019, 218, 1054, 39, 220, 882, 249, 1135, 683, 642, 42, 240, 43, 304, 567, 1163, 1099, 265, 154, 6, 1156, 999, 223, 731, 489, 289, 758, 410, 1076, 638, 55, 1029, 1027, 750, 989, 1097, 675, 558, 19, 994, 649, 1098, 136, 381, 1103, 1025, 871, 709, 872, 266, 462, 591, 1061, 44, 622, 434, 902, 213, 770, 746, 639, 640, 676, 650, 98, 75, 279, 1020, 1072, 86, 364, 848, 563, 912, 717, 571, 741, 103, 1102, 779, 829, 664, 797, 1150, 647, 294, 1051, 971, 283, 705, 464, 1036, 804, 1201, 474, 791, 475, 41, 243, 538, 1195, 878, 966, 152, 149, 371, 1062, 245, 909, 263, 996, 532, 852, 404, 137, 1015, 831, 699, 327, 359, 663, 855, 1172, 206, 407, 73, 1122, 172, 79, 1111, 1152, 588, 107, 163, 679, 1039, 1146, 405, 124, 582, 1011, 891, 919, 850, 236, 1066, 883, 511, 609, 1126, 732, 397, 502, 1013, 488, 892, 739, 67, 146, 834, 985, 208, 153, 766, 841, 899, 117, 313, 239, 941, 846, 401, 272, 318, 777, 62, 531, 134, 47, 174, 1153, 433, 395, 260, 10, 97, 505, 820, 200, 1009, 603, 988, 826, 696, 1048, 468, 818, 326, 385, 194, 299, 156, 424, 338, 224, 601, 312, 546, 203, 654, 431, 936, 367, 248, 291, 635, 600, 768, 626, 1074, 1053, 522, 387, 50, 1184, 269, 533, 261, 700, 903, 859, 209, 342, 390, 482, 1157, 662, 914, 998, 685, 300, 440, 755, 1145, 807, 674, 981, 416, 1144, 1007, 1088, 1116, 906, 160, 784, 351, 575, 354, 101, 281, 965, 519, 169, 725, 1081, 830, 331, 621, 293, 427, 762, 329, 623, 409, 374, 1185, 246, 120, 389, 615, 114, 252, 493, 1022, 54, 108, 1129, 1073, 895, 1124, 721, 1197, 232, 589, 290, 159, 961, 485, 737, 109, 967, 429, 633, 843, 1047, 1175, 84, 457, 126, 885, 795, 1107, 168, 578, 30, 130, 88, 559, 774, 864, 166, 1030, 1079, 553, 1089, 1014, 1064, 547, 470, 557, 449, 494, 524, 465, 509, 90, 995, 606, 1120, 913, 486, 1177, 610, 894, 453, 161, 45, 1114, 247, 761, 1174, 207, 1193, 1050, 377, 78, 923, 583, 836, 800, 426, 435, 492, 657, 958, 144, 947, 1085, 341, 162, 235, 46, 66, 148, 719, 682, 1202, 253, 1205, 210, 337, 145, 230, 916, 847, 776, 0, 1078, 96, 1100, 808, 375, 48, 1212, 890, 332, 711, 366, 420, 1065, 764, 254, 1147, 756, 436, 822, 68, 630, 527, 508, 788, 35, 937, 373, 65, 948, 602, 418, 798, 506, 838, 363, 1137, 1196, 323, 660, 1132, 498, 507, 215, 217, 964, 438, 978, 1213, 599, 167, 214, 706, 1056, 412, 512, 353, 879, 1182, 1136, 31, 881, 21, 824, 417, 221, 689, 89, 673, 594, 933, 668, 572, 461, 189, 1003, 379, 529, 282, 736, 817, 34, 430, 997, 934, 287, 813, 669, 646, 150, 141, 1040, 57, 451, 8, 80, 586, 292, 356, 1194, 712, 1028, 1055, 1140, 183, 1143, 178, 561, 631, 697, 1010, 617, 525, 542, 1207, 411, 929, 794, 339, 869, 528, 204, 694, 195, 85, 432, 311, 510, 707, 873, 515, 870, 1186, 317, 422, 954, 388, 968, 866, 446, 1117, 886, 665, 275, 1183, 227, 835, 199, 620, 472, 76, 539, 264, 71, 151, 481, 1178, 769, 180, 1005, 1211, 7, 477, 550, 1017, 819, 644, 399, 296, 1037, 1180, 113, 853, 977, 346, 1045, 1191, 131, 925, 969, 670, 634, 516, 450, 775, 483, 745, 786, 767, 823, 584, 944, 579, 908, 394, 911, 723, 1016, 1075, 976, 574, 752, 384, 893, 13, 59, 1181, 744, 369, 782, 448, 20, 698, 1004, 1008, 802, 868, 840, 611, 38, 815, 1176, 121, 361, 334, 176, 858, 943, 173, 271, 1165, 959, 286, 479, 92, 889, 1083, 496, 82, 102, 1162, 915, 1101, 115, 956, 447, 825, 577, 821, 406, 72, 122, 29, 1155, 77, 1059, 310, 476, 307, 458, 358, 596, 747, 597, 347, 421, 414, 780, 580, 128, 940, 473, 147, 684, 25, 36, 658, 990, 288, 982, 1094, 695, 763, 1133, 605, 984, 738, 15, 222, 295, 18, 1118, 308, 1149, 781, 87, 69, 927, 655, 581, 702, 806, 595, 759, 839, 27, 566, 185, 315, 348, 104, 734, 942, 592, 986, 809, 564, 792, 415, 129, 641, 938, 3, 1023, 303, 158, 1198, 17, 268, 888, 918, 624, 523, 495, 569, 1206, 273, 1105, 314, 251, 526, 319, 441, 1095, 799, 1084, 619, 378, 1168, 867, 277, 560, 991, 74, 897, 950, 1161, 907, 393, 383, 1057, 138, 593, 1208, 530, 722, 1164, 1042, 1131, 459, 40, 250, 205, 957, 613, 52, 276, 789, 1130, 1187, 1086, 1209, 309, 1138, 551, 1112, 1067, 949, 469, 1169, 585, 1018, 540, 757, 237, 175, 714, 973, 37, 284, 106, 518, 656, 219, 201, 975, 884, 330, 1188, 608, 274, 590, 962, 24, 463, 536, 922, 225, 1058, 851, 11, 116, 952, 842, 614, 1041, 1139, 708, 1142, 993, 687, 345, 1031, 444, 549, 541, 171, 380, 680, 306, 186, 1192, 1148, 573, 105, 1, 322, 386, 854, 99, 926, 1159, 1063, 1077]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9275023596512702
the save name prefix for this run is:  chkpt-ID_9275023596512702_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 105
rank avg (pred): 0.552 +- 0.004
mrr vals (pred, true): 0.017, 0.051
batch losses (mrrl, rdl): 0.0, 0.0002927619

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 313
rank avg (pred): 0.093 +- 0.052
mrr vals (pred, true): 0.187, 0.284
batch losses (mrrl, rdl): 0.0, 4.8321e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 436
rank avg (pred): 0.443 +- 0.261
mrr vals (pred, true): 0.080, 0.045
batch losses (mrrl, rdl): 0.0, 8.1797e-06

Epoch over!
epoch time: 11.952

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 175
rank avg (pred): 0.452 +- 0.263
mrr vals (pred, true): 0.074, 0.048
batch losses (mrrl, rdl): 0.0, 2.9604e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 565
rank avg (pred): 0.108 +- 0.069
mrr vals (pred, true): 0.188, 0.200
batch losses (mrrl, rdl): 0.0, 1.14072e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 340
rank avg (pred): 0.439 +- 0.259
mrr vals (pred, true): 0.066, 0.054
batch losses (mrrl, rdl): 0.0, 1.92689e-05

Epoch over!
epoch time: 11.893

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 392
rank avg (pred): 0.432 +- 0.246
mrr vals (pred, true): 0.062, 0.055
batch losses (mrrl, rdl): 0.0, 4.2021e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 911
rank avg (pred): 0.210 +- 0.125
mrr vals (pred, true): 0.109, 0.162
batch losses (mrrl, rdl): 0.0, 4.5535e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 968
rank avg (pred): 0.434 +- 0.258
mrr vals (pred, true): 0.063, 0.042
batch losses (mrrl, rdl): 0.0, 2.28055e-05

Epoch over!
epoch time: 11.807

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 236
rank avg (pred): 0.439 +- 0.264
mrr vals (pred, true): 0.059, 0.042
batch losses (mrrl, rdl): 0.0, 1.81225e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 237
rank avg (pred): 0.462 +- 0.279
mrr vals (pred, true): 0.058, 0.048
batch losses (mrrl, rdl): 0.0, 2.8946e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 512
rank avg (pred): 0.141 +- 0.086
mrr vals (pred, true): 0.139, 0.236
batch losses (mrrl, rdl): 0.0, 1.03912e-05

Epoch over!
epoch time: 11.835

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1193
rank avg (pred): 0.430 +- 0.270
mrr vals (pred, true): 0.064, 0.045
batch losses (mrrl, rdl): 0.0, 1.13112e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 933
rank avg (pred): 0.439 +- 0.263
mrr vals (pred, true): 0.057, 0.044
batch losses (mrrl, rdl): 0.0, 4.3285e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 45
rank avg (pred): 0.093 +- 0.063
mrr vals (pred, true): 0.205, 0.279
batch losses (mrrl, rdl): 0.0, 2.0516e-06

Epoch over!
epoch time: 11.828

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1188
rank avg (pred): 0.437 +- 0.266
mrr vals (pred, true): 0.057, 0.046
batch losses (mrrl, rdl): 0.0005000388, 5.9105e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1195
rank avg (pred): 0.425 +- 0.127
mrr vals (pred, true): 0.034, 0.043
batch losses (mrrl, rdl): 0.0026411456, 8.63127e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1022
rank avg (pred): 0.430 +- 0.232
mrr vals (pred, true): 0.070, 0.051
batch losses (mrrl, rdl): 0.0038434286, 5.959e-06

Epoch over!
epoch time: 12.225

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 573
rank avg (pred): 0.448 +- 0.204
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.442e-05, 1.56845e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 750
rank avg (pred): 0.052 +- 0.028
mrr vals (pred, true): 0.248, 0.235
batch losses (mrrl, rdl): 0.0015366987, 0.0003081975

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 108
rank avg (pred): 0.472 +- 0.234
mrr vals (pred, true): 0.061, 0.047
batch losses (mrrl, rdl): 0.0011131011, 4.48339e-05

Epoch over!
epoch time: 12.131

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 509
rank avg (pred): 0.062 +- 0.032
mrr vals (pred, true): 0.225, 0.221
batch losses (mrrl, rdl): 0.0001220706, 9.8218e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 998
rank avg (pred): 0.032 +- 0.016
mrr vals (pred, true): 0.313, 0.334
batch losses (mrrl, rdl): 0.0041857054, 0.0001282482

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 926
rank avg (pred): 0.410 +- 0.114
mrr vals (pred, true): 0.036, 0.046
batch losses (mrrl, rdl): 0.0019304611, 0.0001242367

Epoch over!
epoch time: 12.007

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 385
rank avg (pred): 0.491 +- 0.229
mrr vals (pred, true): 0.057, 0.047
batch losses (mrrl, rdl): 0.0004567722, 6.05606e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1165
rank avg (pred): 0.465 +- 0.195
mrr vals (pred, true): 0.050, 0.042
batch losses (mrrl, rdl): 4.908e-07, 1.70002e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 653
rank avg (pred): 0.458 +- 0.196
mrr vals (pred, true): 0.055, 0.039
batch losses (mrrl, rdl): 0.0002841927, 1.48304e-05

Epoch over!
epoch time: 11.985

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1094
rank avg (pred): 0.440 +- 0.182
mrr vals (pred, true): 0.056, 0.052
batch losses (mrrl, rdl): 0.0003282401, 3.08062e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 449
rank avg (pred): 0.502 +- 0.213
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 1.15e-06, 5.88857e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 239
rank avg (pred): 0.480 +- 0.199
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 9.2201e-06, 2.88345e-05

Epoch over!
epoch time: 12.117

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 257
rank avg (pred): 0.043 +- 0.022
mrr vals (pred, true): 0.283, 0.305
batch losses (mrrl, rdl): 0.0047994805, 0.0001228218

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1195
rank avg (pred): 0.425 +- 0.151
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 2.73e-08, 7.00076e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 419
rank avg (pred): 0.487 +- 0.209
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.000413863, 3.85078e-05

Epoch over!
epoch time: 12.104

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 895
rank avg (pred): 0.100 +- 0.049
mrr vals (pred, true): 0.183, 0.174
batch losses (mrrl, rdl): 0.0007598635, 0.0002387919

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1195
rank avg (pred): 0.394 +- 0.137
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 0.0001215261, 0.0001504811

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 262
rank avg (pred): 0.041 +- 0.020
mrr vals (pred, true): 0.288, 0.248
batch losses (mrrl, rdl): 0.0157691836, 9.79574e-05

Epoch over!
epoch time: 12.162

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1081
rank avg (pred): 0.449 +- 0.182
mrr vals (pred, true): 0.063, 0.047
batch losses (mrrl, rdl): 0.0018002493, 2.38749e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 269
rank avg (pred): 0.042 +- 0.020
mrr vals (pred, true): 0.285, 0.287
batch losses (mrrl, rdl): 3.63434e-05, 0.0001191872

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 459
rank avg (pred): 0.483 +- 0.174
mrr vals (pred, true): 0.048, 0.042
batch losses (mrrl, rdl): 3.90007e-05, 3.52619e-05

Epoch over!
epoch time: 12.078

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1014
rank avg (pred): 0.397 +- 0.147
mrr vals (pred, true): 0.056, 0.057
batch losses (mrrl, rdl): 0.0004185867, 5.43157e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1102
rank avg (pred): 0.475 +- 0.179
mrr vals (pred, true): 0.050, 0.057
batch losses (mrrl, rdl): 9.804e-07, 0.0001436415

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1011
rank avg (pred): 0.447 +- 0.160
mrr vals (pred, true): 0.049, 0.060
batch losses (mrrl, rdl): 6.2369e-06, 7.0746e-05

Epoch over!
epoch time: 11.962

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 687
rank avg (pred): 0.470 +- 0.167
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0001816484, 3.3955e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 563
rank avg (pred): 0.081 +- 0.041
mrr vals (pred, true): 0.230, 0.226
batch losses (mrrl, rdl): 0.0001436934, 7.71349e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 71
rank avg (pred): 0.056 +- 0.028
mrr vals (pred, true): 0.263, 0.285
batch losses (mrrl, rdl): 0.0049390527, 0.0001066866

Epoch over!
epoch time: 12.068

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.265 +- 0.098
mrr vals (pred, true): 0.050, 0.043

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04613 	 0.04035 	 ~...
   35 	     1 	 0.04864 	 0.04134 	 ~...
   12 	     2 	 0.04628 	 0.04202 	 ~...
   41 	     3 	 0.04900 	 0.04211 	 ~...
   55 	     4 	 0.05025 	 0.04217 	 ~...
   66 	     5 	 0.05146 	 0.04222 	 ~...
   45 	     6 	 0.04963 	 0.04231 	 ~...
   16 	     7 	 0.04712 	 0.04239 	 ~...
   82 	     8 	 0.05565 	 0.04245 	 ~...
   74 	     9 	 0.05276 	 0.04269 	 ~...
   39 	    10 	 0.04894 	 0.04289 	 ~...
   43 	    11 	 0.04937 	 0.04302 	 ~...
   60 	    12 	 0.05095 	 0.04315 	 ~...
   46 	    13 	 0.04966 	 0.04320 	 ~...
   17 	    14 	 0.04736 	 0.04322 	 ~...
   31 	    15 	 0.04811 	 0.04345 	 ~...
   56 	    16 	 0.05026 	 0.04362 	 ~...
   54 	    17 	 0.05020 	 0.04363 	 ~...
   48 	    18 	 0.04980 	 0.04367 	 ~...
   78 	    19 	 0.05481 	 0.04370 	 ~...
   61 	    20 	 0.05097 	 0.04385 	 ~...
   50 	    21 	 0.04984 	 0.04390 	 ~...
   40 	    22 	 0.04895 	 0.04392 	 ~...
   80 	    23 	 0.05513 	 0.04393 	 ~...
   34 	    24 	 0.04834 	 0.04397 	 ~...
   13 	    25 	 0.04641 	 0.04400 	 ~...
   27 	    26 	 0.04792 	 0.04401 	 ~...
   21 	    27 	 0.04767 	 0.04402 	 ~...
   18 	    28 	 0.04738 	 0.04408 	 ~...
   15 	    29 	 0.04705 	 0.04414 	 ~...
   32 	    30 	 0.04818 	 0.04416 	 ~...
   23 	    31 	 0.04773 	 0.04421 	 ~...
   29 	    32 	 0.04798 	 0.04426 	 ~...
   30 	    33 	 0.04804 	 0.04436 	 ~...
   59 	    34 	 0.05066 	 0.04439 	 ~...
    4 	    35 	 0.04574 	 0.04447 	 ~...
   47 	    36 	 0.04969 	 0.04453 	 ~...
   25 	    37 	 0.04783 	 0.04453 	 ~...
   36 	    38 	 0.04872 	 0.04462 	 ~...
   57 	    39 	 0.05032 	 0.04473 	 ~...
   71 	    40 	 0.05190 	 0.04479 	 ~...
   11 	    41 	 0.04627 	 0.04494 	 ~...
   62 	    42 	 0.05106 	 0.04495 	 ~...
   53 	    43 	 0.04998 	 0.04509 	 ~...
    2 	    44 	 0.04552 	 0.04519 	 ~...
   72 	    45 	 0.05208 	 0.04521 	 ~...
   63 	    46 	 0.05129 	 0.04528 	 ~...
    6 	    47 	 0.04587 	 0.04532 	 ~...
    7 	    48 	 0.04610 	 0.04548 	 ~...
   33 	    49 	 0.04829 	 0.04562 	 ~...
   58 	    50 	 0.05036 	 0.04569 	 ~...
   37 	    51 	 0.04872 	 0.04628 	 ~...
   10 	    52 	 0.04624 	 0.04644 	 ~...
    1 	    53 	 0.04552 	 0.04645 	 ~...
   20 	    54 	 0.04756 	 0.04648 	 ~...
   14 	    55 	 0.04689 	 0.04663 	 ~...
   19 	    56 	 0.04751 	 0.04723 	 ~...
   28 	    57 	 0.04794 	 0.04746 	 ~...
   22 	    58 	 0.04768 	 0.04815 	 ~...
    9 	    59 	 0.04621 	 0.04835 	 ~...
   24 	    60 	 0.04781 	 0.04862 	 ~...
   52 	    61 	 0.04995 	 0.04914 	 ~...
   26 	    62 	 0.04783 	 0.04938 	 ~...
   65 	    63 	 0.05142 	 0.04970 	 ~...
   67 	    64 	 0.05146 	 0.04984 	 ~...
   51 	    65 	 0.04992 	 0.05008 	 ~...
   69 	    66 	 0.05172 	 0.05011 	 ~...
    3 	    67 	 0.04573 	 0.05095 	 ~...
   75 	    68 	 0.05311 	 0.05184 	 ~...
   49 	    69 	 0.04981 	 0.05231 	 ~...
    0 	    70 	 0.04505 	 0.05237 	 ~...
   73 	    71 	 0.05227 	 0.05241 	 ~...
   44 	    72 	 0.04937 	 0.05277 	 ~...
   64 	    73 	 0.05134 	 0.05317 	 ~...
   83 	    74 	 0.05738 	 0.05367 	 ~...
   42 	    75 	 0.04929 	 0.05405 	 ~...
   68 	    76 	 0.05164 	 0.05424 	 ~...
   70 	    77 	 0.05172 	 0.05435 	 ~...
   77 	    78 	 0.05416 	 0.05444 	 ~...
   76 	    79 	 0.05351 	 0.05531 	 ~...
   38 	    80 	 0.04889 	 0.05666 	 ~...
    5 	    81 	 0.04577 	 0.05696 	 ~...
   79 	    82 	 0.05495 	 0.05806 	 ~...
   81 	    83 	 0.05534 	 0.05853 	 ~...
   85 	    84 	 0.14943 	 0.14413 	 ~...
   84 	    85 	 0.14586 	 0.15183 	 ~...
   86 	    86 	 0.18242 	 0.17534 	 ~...
   87 	    87 	 0.18824 	 0.18190 	 ~...
   90 	    88 	 0.21370 	 0.19216 	 ~...
   89 	    89 	 0.21325 	 0.19225 	 ~...
   93 	    90 	 0.21677 	 0.20170 	 ~...
   94 	    91 	 0.21735 	 0.20910 	 ~...
   92 	    92 	 0.21569 	 0.20918 	 ~...
   96 	    93 	 0.22013 	 0.21172 	 ~...
   91 	    94 	 0.21418 	 0.21191 	 ~...
   88 	    95 	 0.21304 	 0.22278 	 ~...
   97 	    96 	 0.22020 	 0.22913 	 ~...
   95 	    97 	 0.21792 	 0.24035 	 ~...
   99 	    98 	 0.26796 	 0.25272 	 ~...
  107 	    99 	 0.28056 	 0.26028 	 ~...
  109 	   100 	 0.28115 	 0.26262 	 ~...
  102 	   101 	 0.27723 	 0.26272 	 ~...
  110 	   102 	 0.28164 	 0.26911 	 ~...
  104 	   103 	 0.27993 	 0.27075 	 ~...
  114 	   104 	 0.28797 	 0.27473 	 ~...
  113 	   105 	 0.28714 	 0.28228 	 ~...
   98 	   106 	 0.25428 	 0.28336 	 ~...
  112 	   107 	 0.28473 	 0.28601 	 ~...
  103 	   108 	 0.27871 	 0.28894 	 ~...
  101 	   109 	 0.27581 	 0.28991 	 ~...
  108 	   110 	 0.28109 	 0.29328 	 ~...
  105 	   111 	 0.28021 	 0.29520 	 ~...
  106 	   112 	 0.28037 	 0.29612 	 ~...
  115 	   113 	 0.32853 	 0.29730 	 m..s
  100 	   114 	 0.27449 	 0.29805 	 ~...
  111 	   115 	 0.28213 	 0.30045 	 ~...
  119 	   116 	 0.34262 	 0.30148 	 m..s
  116 	   117 	 0.33344 	 0.31226 	 ~...
  117 	   118 	 0.33704 	 0.33146 	 ~...
  118 	   119 	 0.33886 	 0.33250 	 ~...
  120 	   120 	 0.34626 	 0.35185 	 ~...
==========================================
r_mrr = 0.9955641031265259
r2_mrr = 0.9902722835540771
spearmanr_mrr@5 = 0.9630668759346008
spearmanr_mrr@10 = 0.7628347277641296
spearmanr_mrr@50 = 0.9946129322052002
spearmanr_mrr@100 = 0.9974868297576904
spearmanr_mrr@All = 0.9977138638496399
==========================================
test time: 0.392
Done Testing dataset Kinships
total time taken: 187.57771396636963
training time taken: 180.63309526443481
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9956)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9903)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9631)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.7628)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9946)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9975)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9977)}}, 'test_loss': {'TransE': {'Kinships': 0.12345594001817517}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 6014342729436296
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [933, 451, 139, 565, 32, 681, 301, 768, 997, 275, 1043, 682, 1076, 122, 647, 318, 711, 162, 518, 806, 26, 365, 467, 1188, 950, 1034, 1047, 276, 1041, 816, 138, 56, 1071, 696, 893, 305, 661, 667, 354, 461, 500, 851, 508, 892, 241, 248, 874, 459, 680, 803, 598, 499, 1028, 780, 141, 1159, 358, 773, 417, 387, 59, 1068, 329, 889, 308, 93, 729, 287, 497, 293, 217, 822, 1183, 491, 595, 644, 231, 144, 456, 463, 443, 990, 257, 199, 811, 774, 92, 850, 900, 957, 703, 1194, 1101, 1114, 866, 920, 337, 984, 1210, 342, 447, 943, 264, 1207, 1038, 747, 812, 315, 1124, 614, 385, 743, 130, 1150, 759, 418, 529, 845, 549, 190, 12]
valid_ids (0): []
train_ids (1094): [243, 359, 637, 233, 131, 254, 906, 394, 140, 678, 937, 1088, 1001, 112, 170, 973, 1091, 1030, 291, 453, 925, 70, 121, 273, 945, 979, 1070, 183, 157, 1087, 401, 297, 16, 6, 319, 801, 80, 875, 498, 403, 0, 1048, 640, 1190, 786, 73, 707, 1072, 1147, 5, 128, 953, 18, 826, 955, 125, 502, 568, 570, 754, 655, 299, 507, 1064, 48, 536, 864, 970, 67, 999, 651, 259, 582, 163, 590, 578, 750, 1119, 348, 147, 912, 1138, 534, 23, 1002, 956, 402, 148, 914, 627, 201, 1152, 82, 1196, 361, 630, 441, 47, 340, 390, 731, 708, 3, 668, 562, 211, 616, 378, 589, 44, 135, 849, 1121, 242, 221, 799, 187, 234, 435, 2, 686, 936, 642, 971, 338, 533, 1016, 83, 844, 9, 830, 755, 993, 886, 30, 374, 1051, 477, 346, 884, 693, 1129, 302, 1090, 1184, 1049, 292, 913, 208, 751, 567, 336, 1069, 831, 876, 371, 865, 787, 895, 462, 127, 664, 1092, 908, 171, 68, 1052, 653, 734, 416, 530, 1199, 195, 193, 382, 891, 495, 232, 709, 757, 1073, 213, 594, 1165, 882, 662, 1213, 1094, 975, 656, 372, 724, 422, 603, 623, 256, 861, 215, 926, 289, 314, 120, 996, 457, 785, 89, 349, 99, 114, 690, 804, 972, 853, 353, 620, 17, 102, 540, 260, 1055, 397, 689, 1059, 209, 873, 174, 373, 172, 272, 1115, 294, 1050, 1127, 1025, 493, 312, 409, 295, 646, 878, 448, 322, 1180, 395, 426, 756, 558, 615, 216, 1093, 355, 388, 622, 519, 1200, 645, 576, 181, 1128, 1105, 184, 607, 1044, 452, 1098, 357, 261, 407, 597, 868, 444, 612, 596, 1172, 400, 1134, 657, 897, 194, 569, 995, 1163, 1102, 255, 420, 100, 1085, 325, 154, 634, 24, 1166, 165, 64, 415, 1078, 290, 688, 784, 320, 438, 362, 1181, 1133, 1136, 7, 899, 1029, 62, 846, 1193, 285, 1, 196, 699, 79, 252, 356, 909, 555, 847, 538, 676, 240, 1089, 1067, 161, 683, 1009, 776, 670, 915, 1131, 1169, 506, 949, 929, 631, 366, 944, 572, 1013, 760, 725, 283, 694, 212, 722, 470, 476, 697, 687, 798, 334, 133, 492, 251, 728, 81, 559, 692, 189, 503, 20, 1117, 1126, 496, 109, 857, 341, 303, 599, 939, 1061, 863, 191, 721, 951, 742, 718, 767, 982, 1053, 263, 1185, 421, 986, 542, 1208, 19, 898, 108, 547, 65, 455, 643, 1026, 1198, 442, 249, 11, 1007, 84, 408, 1111, 494, 737, 168, 250, 541, 137, 855, 186, 204, 560, 1173, 219, 654, 429, 910, 160, 980, 460, 1045, 546, 704, 126, 1122, 1060, 1178, 1176, 1182, 870, 509, 770, 228, 35, 1024, 4, 740, 489, 879, 669, 968, 685, 98, 1008, 524, 129, 245, 1022, 333, 40, 962, 766, 1201, 958, 152, 517, 931, 626, 428, 1135, 505, 1123, 544, 328, 71, 87, 611, 1132, 436, 466, 1209, 522, 1142, 1082, 270, 577, 632, 396, 1074, 1171, 1214, 155, 304, 749, 205, 76, 53, 856, 548, 324, 574, 928, 1189, 1112, 91, 807, 1162, 368, 46, 85, 1191, 1141, 575, 1042, 280, 862, 410, 226, 1021, 473, 1006, 298, 633, 1179, 829, 1158, 658, 179, 316, 1167, 758, 907, 814, 188, 1149, 300, 563, 977, 379, 1204, 472, 34, 535, 150, 921, 363, 941, 672, 665, 554, 641, 1003, 1096, 389, 119, 795, 278, 267, 413, 286, 331, 1153, 978, 386, 841, 539, 922, 296, 445, 824, 712, 384, 592, 1058, 309, 419, 75, 738, 713, 458, 335, 652, 182, 613, 714, 809, 802, 375, 449, 779, 1000, 684, 629, 989, 918, 424, 1108, 1205, 177, 1081, 720, 1192, 1027, 815, 1175, 919, 347, 1066, 867, 116, 63, 551, 229, 425, 156, 777, 1174, 51, 134, 207, 1168, 545, 214, 430, 671, 480, 404, 45, 1120, 1143, 1032, 800, 772, 946, 105, 789, 601, 588, 97, 464, 218, 871, 446, 377, 1097, 1106, 967, 606, 513, 1186, 872, 727, 468, 271, 571, 963, 57, 210, 192, 36, 166, 1139, 14, 532, 153, 790, 103, 167, 123, 159, 381, 310, 1160, 791, 778, 1077, 1063, 835, 832, 1140, 1014, 746, 501, 1037, 948, 96, 220, 974, 650, 332, 1020, 27, 432, 37, 515, 1005, 543, 317, 29, 719, 164, 723, 604, 307, 1054, 197, 732, 836, 427, 1056, 392, 769, 1109, 383, 823, 78, 149, 411, 618, 1211, 61, 1011, 819, 848, 247, 268, 843, 200, 556, 1107, 820, 206, 1206, 883, 700, 610, 808, 587, 602, 981, 930, 94, 414, 237, 203, 115, 833, 675, 142, 483, 330, 1099, 821, 639, 660, 579, 834, 940, 666, 752, 924, 469, 952, 564, 440, 1151, 399, 965, 903, 474, 258, 486, 960, 364, 1104, 434, 691, 817, 854, 225, 282, 516, 398, 465, 431, 736, 227, 514, 1125, 880, 605, 1018, 136, 994, 288, 175, 887, 894, 369, 1039, 584, 33, 230, 905, 178, 284, 72, 992, 762, 566, 698, 169, 730, 1031, 934, 744, 10, 1100, 1195, 1083, 1004, 679, 158, 748, 405, 145, 901, 837, 1144, 327, 521, 763, 69, 1035, 1116, 1046, 306, 964, 88, 1017, 281, 1177, 323, 797, 311, 705, 512, 151, 583, 608, 966, 617, 15, 13, 673, 1155, 380, 481, 479, 842, 916, 49, 343, 942, 827, 146, 31, 969, 710, 765, 412, 106, 593, 1040, 976, 561, 813, 695, 352, 360, 701, 902, 1075, 223, 244, 277, 1156, 52, 1023, 1161, 573, 1010, 246, 591, 321, 393, 706, 21, 781, 663, 520, 107, 628, 185, 1157, 1170, 235, 828, 43, 553, 433, 55, 796, 376, 74, 648, 132, 1062, 860, 54, 531, 537, 42, 659, 50, 624, 1154, 735, 726, 1203, 339, 1118, 581, 782, 58, 881, 1057, 266, 1145, 90, 124, 39, 585, 77, 274, 877, 677, 326, 988, 173, 557, 586, 528, 202, 482, 1015, 840, 715, 775, 888, 110, 1033, 771, 625, 238, 423, 262, 983, 527, 454, 985, 600, 702, 478, 609, 764, 488, 180, 961, 852, 253, 487, 101, 954, 923, 825, 635, 1113, 236, 370, 485, 41, 1080, 437, 510, 733, 959, 1148, 1187, 117, 525, 739, 987, 38, 367, 1084, 1146, 279, 741, 838, 344, 935, 1164, 858, 313, 717, 917, 938, 111, 471, 224, 619, 636, 947, 22, 1079, 1197, 550, 351, 761, 113, 406, 1065, 504, 998, 896, 674, 523, 345, 927, 788, 552, 176, 1110, 526, 66, 25, 753, 1103, 859, 1202, 269, 716, 511, 638, 450, 1095, 439, 885, 1130, 222, 911, 104, 28, 1137, 198, 904, 810, 621, 890, 794, 1019, 580, 869, 792, 839, 239, 475, 1036, 818, 95, 783, 391, 60, 991, 649, 793, 1012, 350, 805, 265, 1086, 490, 745, 118, 1212, 86, 8, 932, 484, 143]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5787634573844018
the save name prefix for this run is:  chkpt-ID_5787634573844018_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 46
rank avg (pred): 0.443 +- 0.004
mrr vals (pred, true): 0.021, 0.264
batch losses (mrrl, rdl): 0.0, 0.0024279773

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1051
rank avg (pred): 0.481 +- 0.213
mrr vals (pred, true): 0.023, 0.042
batch losses (mrrl, rdl): 0.0, 1.58759e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 225
rank avg (pred): 0.467 +- 0.275
mrr vals (pred, true): 0.034, 0.044
batch losses (mrrl, rdl): 0.0, 4.0449e-06

Epoch over!
epoch time: 12.149

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 733
rank avg (pred): 0.162 +- 0.158
mrr vals (pred, true): 0.110, 0.214
batch losses (mrrl, rdl): 0.0, 1.88939e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 3
rank avg (pred): 0.106 +- 0.133
mrr vals (pred, true): 0.183, 0.262
batch losses (mrrl, rdl): 0.0, 1.521e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 754
rank avg (pred): 0.177 +- 0.180
mrr vals (pred, true): 0.130, 0.272
batch losses (mrrl, rdl): 0.0, 2.36801e-05

Epoch over!
epoch time: 12.305

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 361
rank avg (pred): 0.455 +- 0.268
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0, 2.36978e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 555
rank avg (pred): 0.110 +- 0.133
mrr vals (pred, true): 0.181, 0.234
batch losses (mrrl, rdl): 0.0, 9.112e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 377
rank avg (pred): 0.469 +- 0.272
mrr vals (pred, true): 0.044, 0.057
batch losses (mrrl, rdl): 0.0, 6.13747e-05

Epoch over!
epoch time: 11.883

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1067
rank avg (pred): 0.096 +- 0.115
mrr vals (pred, true): 0.196, 0.336
batch losses (mrrl, rdl): 0.0, 7.12e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 996
rank avg (pred): 0.086 +- 0.113
mrr vals (pred, true): 0.223, 0.337
batch losses (mrrl, rdl): 0.0, 1.81956e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1156
rank avg (pred): 0.065 +- 0.074
mrr vals (pred, true): 0.277, 0.261
batch losses (mrrl, rdl): 0.0, 3.48711e-05

Epoch over!
epoch time: 12.985

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1192
rank avg (pred): 0.471 +- 0.270
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 0.0, 6.621e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 123
rank avg (pred): 0.457 +- 0.279
mrr vals (pred, true): 0.059, 0.049
batch losses (mrrl, rdl): 0.0, 5.5256e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 786
rank avg (pred): 0.466 +- 0.266
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.0, 3.32e-07

Epoch over!
epoch time: 14.188

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1110
rank avg (pred): 0.450 +- 0.267
mrr vals (pred, true): 0.060, 0.045
batch losses (mrrl, rdl): 0.0010336188, 2.7398e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 646
rank avg (pred): 0.462 +- 0.240
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002081203, 9.1265e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 922
rank avg (pred): 0.413 +- 0.210
mrr vals (pred, true): 0.046, 0.045
batch losses (mrrl, rdl): 0.0001693816, 8.115e-05

Epoch over!
epoch time: 15.584

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 492
rank avg (pred): 0.137 +- 0.227
mrr vals (pred, true): 0.231, 0.213
batch losses (mrrl, rdl): 0.0029595511, 2.2408e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 687
rank avg (pred): 0.479 +- 0.248
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0003014621, 5.7109e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 405
rank avg (pred): 0.463 +- 0.228
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 2.04364e-05, 1.32096e-05

Epoch over!
epoch time: 14.56

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 109
rank avg (pred): 0.504 +- 0.241
mrr vals (pred, true): 0.044, 0.050
batch losses (mrrl, rdl): 0.0003523787, 8.07039e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 488
rank avg (pred): 0.155 +- 0.226
mrr vals (pred, true): 0.209, 0.226
batch losses (mrrl, rdl): 0.0027264832, 2.00437e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 581
rank avg (pred): 0.483 +- 0.238
mrr vals (pred, true): 0.049, 0.049
batch losses (mrrl, rdl): 8.2288e-06, 2.94668e-05

Epoch over!
epoch time: 15.234

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1072
rank avg (pred): 0.098 +- 0.210
mrr vals (pred, true): 0.305, 0.309
batch losses (mrrl, rdl): 0.0001983216, 1.73919e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 625
rank avg (pred): 0.485 +- 0.245
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 7.20888e-05, 3.37444e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 922
rank avg (pred): 0.426 +- 0.218
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 9e-09, 5.0766e-05

Epoch over!
epoch time: 14.504

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1075
rank avg (pred): 0.088 +- 0.210
mrr vals (pred, true): 0.360, 0.355
batch losses (mrrl, rdl): 0.0002075361, 2.05342e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 724
rank avg (pred): 0.484 +- 0.236
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001531571, 1.46056e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 555
rank avg (pred): 0.156 +- 0.234
mrr vals (pred, true): 0.222, 0.234
batch losses (mrrl, rdl): 0.0013604515, 5.21264e-05

Epoch over!
epoch time: 15.233

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 94
rank avg (pred): 0.415 +- 0.219
mrr vals (pred, true): 0.050, 0.045
batch losses (mrrl, rdl): 3.394e-07, 3.37521e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 196
rank avg (pred): 0.509 +- 0.245
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 0.0001095165, 4.44596e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1214
rank avg (pred): 0.493 +- 0.245
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 2.02329e-05, 1.79869e-05

Epoch over!
epoch time: 15.336

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 707
rank avg (pred): 0.495 +- 0.246
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 4.60291e-05, 1.76589e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 862
rank avg (pred): 0.438 +- 0.213
mrr vals (pred, true): 0.043, 0.044
batch losses (mrrl, rdl): 0.0005357157, 3.83345e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 140
rank avg (pred): 0.509 +- 0.251
mrr vals (pred, true): 0.048, 0.052
batch losses (mrrl, rdl): 4.30235e-05, 0.0001001308

Epoch over!
epoch time: 14.755

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1176
rank avg (pred): 0.456 +- 0.235
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0001653528, 6.2468e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 369
rank avg (pred): 0.434 +- 0.234
mrr vals (pred, true): 0.049, 0.052
batch losses (mrrl, rdl): 9.438e-06, 9.0373e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1112
rank avg (pred): 0.462 +- 0.246
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 0.0001057284, 4.8113e-06

Epoch over!
epoch time: 15.077

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 85
rank avg (pred): 0.455 +- 0.236
mrr vals (pred, true): 0.045, 0.048
batch losses (mrrl, rdl): 0.0002405278, 1.02676e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 739
rank avg (pred): 0.184 +- 0.252
mrr vals (pred, true): 0.236, 0.179
batch losses (mrrl, rdl): 0.032729838, 0.000107563

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 527
rank avg (pred): 0.187 +- 0.250
mrr vals (pred, true): 0.220, 0.227
batch losses (mrrl, rdl): 0.0004239529, 5.31438e-05

Epoch over!
epoch time: 15.414

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 942
rank avg (pred): 0.427 +- 0.220
mrr vals (pred, true): 0.038, 0.048
batch losses (mrrl, rdl): 0.0015607419, 4.6018e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 835
rank avg (pred): 0.151 +- 0.254
mrr vals (pred, true): 0.261, 0.272
batch losses (mrrl, rdl): 0.0012958958, 2.06701e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 854
rank avg (pred): 0.393 +- 0.225
mrr vals (pred, true): 0.042, 0.050
batch losses (mrrl, rdl): 0.0005692205, 0.0001102959

Epoch over!
epoch time: 14.107

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.394 +- 0.238
mrr vals (pred, true): 0.053, 0.044

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04891 	 0.04205 	 ~...
    3 	     1 	 0.04943 	 0.04205 	 ~...
   23 	     2 	 0.04946 	 0.04207 	 ~...
   41 	     3 	 0.05136 	 0.04209 	 ~...
    1 	     4 	 0.04891 	 0.04222 	 ~...
    3 	     5 	 0.04943 	 0.04245 	 ~...
   31 	     6 	 0.04971 	 0.04256 	 ~...
   54 	     7 	 0.05206 	 0.04276 	 ~...
   24 	     8 	 0.04950 	 0.04278 	 ~...
   66 	     9 	 0.05428 	 0.04284 	 ~...
   71 	    10 	 0.05743 	 0.04308 	 ~...
   40 	    11 	 0.05130 	 0.04311 	 ~...
   65 	    12 	 0.05396 	 0.04345 	 ~...
   27 	    13 	 0.04963 	 0.04353 	 ~...
   44 	    14 	 0.05170 	 0.04362 	 ~...
   53 	    15 	 0.05203 	 0.04366 	 ~...
   75 	    16 	 0.06235 	 0.04367 	 ~...
   76 	    17 	 0.07204 	 0.04369 	 ~...
    3 	    18 	 0.04943 	 0.04377 	 ~...
   51 	    19 	 0.05194 	 0.04379 	 ~...
   64 	    20 	 0.05379 	 0.04385 	 ~...
    3 	    21 	 0.04943 	 0.04385 	 ~...
   49 	    22 	 0.05194 	 0.04387 	 ~...
   57 	    23 	 0.05257 	 0.04389 	 ~...
    3 	    24 	 0.04943 	 0.04392 	 ~...
   62 	    25 	 0.05332 	 0.04405 	 ~...
   67 	    26 	 0.05594 	 0.04418 	 ~...
   30 	    27 	 0.04970 	 0.04428 	 ~...
   47 	    28 	 0.05190 	 0.04433 	 ~...
    3 	    29 	 0.04943 	 0.04463 	 ~...
   52 	    30 	 0.05202 	 0.04463 	 ~...
   28 	    31 	 0.04965 	 0.04463 	 ~...
   70 	    32 	 0.05703 	 0.04466 	 ~...
   19 	    33 	 0.04945 	 0.04473 	 ~...
   32 	    34 	 0.04972 	 0.04485 	 ~...
   55 	    35 	 0.05242 	 0.04485 	 ~...
   73 	    36 	 0.05940 	 0.04513 	 ~...
   50 	    37 	 0.05194 	 0.04516 	 ~...
   42 	    38 	 0.05146 	 0.04523 	 ~...
   34 	    39 	 0.04992 	 0.04537 	 ~...
   60 	    40 	 0.05290 	 0.04542 	 ~...
   59 	    41 	 0.05289 	 0.04546 	 ~...
   68 	    42 	 0.05662 	 0.04553 	 ~...
   21 	    43 	 0.04945 	 0.04558 	 ~...
   63 	    44 	 0.05335 	 0.04560 	 ~...
   35 	    45 	 0.05025 	 0.04609 	 ~...
   46 	    46 	 0.05190 	 0.04624 	 ~...
   69 	    47 	 0.05691 	 0.04639 	 ~...
    3 	    48 	 0.04943 	 0.04653 	 ~...
   29 	    49 	 0.04968 	 0.04663 	 ~...
    3 	    50 	 0.04943 	 0.04666 	 ~...
   17 	    51 	 0.04944 	 0.04677 	 ~...
   26 	    52 	 0.04963 	 0.04677 	 ~...
   56 	    53 	 0.05246 	 0.04694 	 ~...
   37 	    54 	 0.05074 	 0.04704 	 ~...
   18 	    55 	 0.04944 	 0.04715 	 ~...
   20 	    56 	 0.04945 	 0.04722 	 ~...
    3 	    57 	 0.04943 	 0.04725 	 ~...
   74 	    58 	 0.06119 	 0.04734 	 ~...
   58 	    59 	 0.05261 	 0.04798 	 ~...
   36 	    60 	 0.05036 	 0.04890 	 ~...
   45 	    61 	 0.05188 	 0.04938 	 ~...
    3 	    62 	 0.04943 	 0.05004 	 ~...
    3 	    63 	 0.04943 	 0.05004 	 ~...
   48 	    64 	 0.05191 	 0.05054 	 ~...
    3 	    65 	 0.04943 	 0.05091 	 ~...
   39 	    66 	 0.05093 	 0.05149 	 ~...
   61 	    67 	 0.05327 	 0.05204 	 ~...
    3 	    68 	 0.04943 	 0.05264 	 ~...
   33 	    69 	 0.04983 	 0.05294 	 ~...
   38 	    70 	 0.05080 	 0.05320 	 ~...
   22 	    71 	 0.04946 	 0.05367 	 ~...
    3 	    72 	 0.04943 	 0.05402 	 ~...
   25 	    73 	 0.04951 	 0.05512 	 ~...
   43 	    74 	 0.05150 	 0.05668 	 ~...
   72 	    75 	 0.05828 	 0.05711 	 ~...
    0 	    76 	 0.04888 	 0.05723 	 ~...
   77 	    77 	 0.14755 	 0.10078 	 m..s
   78 	    78 	 0.15668 	 0.13306 	 ~...
   79 	    79 	 0.15891 	 0.14413 	 ~...
   80 	    80 	 0.15970 	 0.15474 	 ~...
   85 	    81 	 0.22108 	 0.17534 	 m..s
   83 	    82 	 0.21598 	 0.17560 	 m..s
   81 	    83 	 0.20462 	 0.18413 	 ~...
   84 	    84 	 0.22010 	 0.19216 	 ~...
   82 	    85 	 0.21485 	 0.19653 	 ~...
   95 	    86 	 0.23513 	 0.19965 	 m..s
   92 	    87 	 0.22823 	 0.20380 	 ~...
   93 	    88 	 0.23189 	 0.20539 	 ~...
   87 	    89 	 0.22133 	 0.20583 	 ~...
   89 	    90 	 0.22259 	 0.21130 	 ~...
   90 	    91 	 0.22675 	 0.21191 	 ~...
   88 	    92 	 0.22179 	 0.22278 	 ~...
   86 	    93 	 0.22121 	 0.22906 	 ~...
   91 	    94 	 0.22687 	 0.23470 	 ~...
   94 	    95 	 0.23256 	 0.25856 	 ~...
   99 	    96 	 0.26443 	 0.26278 	 ~...
  103 	    97 	 0.27234 	 0.26491 	 ~...
  104 	    98 	 0.27271 	 0.26958 	 ~...
  102 	    99 	 0.27213 	 0.27167 	 ~...
  113 	   100 	 0.29324 	 0.27474 	 ~...
  109 	   101 	 0.27926 	 0.28527 	 ~...
   96 	   102 	 0.25970 	 0.28601 	 ~...
  107 	   103 	 0.27467 	 0.28696 	 ~...
  111 	   104 	 0.29311 	 0.28894 	 ~...
   97 	   105 	 0.26075 	 0.29207 	 m..s
  100 	   106 	 0.26864 	 0.29543 	 ~...
  110 	   107 	 0.29188 	 0.29913 	 ~...
  105 	   108 	 0.27322 	 0.30045 	 ~...
  117 	   109 	 0.31725 	 0.30148 	 ~...
   98 	   110 	 0.26076 	 0.30314 	 m..s
  101 	   111 	 0.26985 	 0.30411 	 m..s
  106 	   112 	 0.27458 	 0.30518 	 m..s
  108 	   113 	 0.27757 	 0.30816 	 m..s
  112 	   114 	 0.29315 	 0.31588 	 ~...
  115 	   115 	 0.30579 	 0.31732 	 ~...
  119 	   116 	 0.34737 	 0.33103 	 ~...
  116 	   117 	 0.31295 	 0.33199 	 ~...
  114 	   118 	 0.29343 	 0.33317 	 m..s
  118 	   119 	 0.33061 	 0.33857 	 ~...
  120 	   120 	 0.36550 	 0.35877 	 ~...
==========================================
r_mrr = 0.990757405757904
r2_mrr = 0.9790684580802917
spearmanr_mrr@5 = 0.9127553701400757
spearmanr_mrr@10 = 0.9530979990959167
spearmanr_mrr@50 = 0.9832127094268799
spearmanr_mrr@100 = 0.994181215763092
spearmanr_mrr@All = 0.9948453903198242
==========================================
test time: 0.45
Done Testing dataset Kinships
total time taken: 221.099782705307
training time taken: 213.8456265926361
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9908)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9791)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9128)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9531)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9832)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9942)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9948)}}, 'test_loss': {'TransE': {'Kinships': 0.2623359402873575}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 8738157128374912
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [182, 208, 162, 276, 796, 1173, 926, 1183, 460, 514, 302, 93, 950, 581, 678, 630, 1016, 116, 757, 456, 880, 511, 213, 167, 835, 526, 212, 370, 468, 1209, 934, 873, 477, 1124, 1094, 656, 1053, 445, 444, 153, 466, 1205, 257, 764, 216, 991, 551, 1195, 336, 913, 859, 588, 156, 102, 576, 1002, 846, 516, 541, 774, 734, 317, 327, 498, 347, 1158, 717, 919, 505, 1000, 160, 847, 899, 256, 698, 751, 715, 562, 1161, 1089, 136, 537, 1153, 932, 333, 839, 821, 726, 411, 398, 719, 197, 958, 801, 422, 963, 268, 1047, 906, 418, 1159, 542, 828, 353, 1065, 777, 176, 38, 969, 490, 134, 352, 471, 528, 454, 1034, 791, 401, 391, 945, 785]
valid_ids (0): []
train_ids (1094): [818, 691, 739, 856, 14, 243, 1208, 95, 977, 356, 232, 824, 1181, 465, 1003, 907, 935, 980, 177, 651, 1101, 831, 1033, 1037, 683, 78, 961, 191, 1149, 558, 819, 278, 659, 224, 1118, 749, 1202, 339, 384, 1045, 1168, 185, 201, 499, 536, 1060, 974, 22, 138, 36, 1018, 643, 1160, 381, 475, 396, 290, 54, 627, 822, 1207, 931, 483, 82, 172, 282, 917, 793, 393, 882, 28, 410, 1138, 669, 649, 535, 273, 790, 1120, 1152, 794, 461, 611, 787, 673, 369, 463, 646, 120, 916, 539, 253, 187, 1070, 805, 608, 255, 569, 52, 557, 879, 12, 313, 744, 32, 436, 925, 591, 41, 840, 429, 311, 237, 448, 375, 103, 1106, 682, 488, 1199, 128, 293, 567, 1014, 766, 1054, 944, 56, 101, 83, 1164, 686, 681, 628, 540, 378, 1027, 264, 2, 148, 431, 632, 741, 110, 813, 631, 439, 481, 606, 929, 836, 583, 357, 784, 523, 877, 140, 768, 263, 13, 395, 1043, 890, 100, 587, 598, 416, 137, 8, 620, 275, 892, 662, 1114, 721, 655, 484, 420, 966, 850, 27, 44, 756, 1042, 1147, 638, 462, 61, 474, 188, 952, 1082, 214, 954, 616, 210, 555, 520, 159, 872, 15, 968, 79, 489, 1052, 529, 1174, 178, 692, 809, 563, 512, 392, 1194, 1049, 1004, 453, 179, 604, 509, 151, 970, 267, 192, 437, 500, 1007, 402, 1046, 600, 96, 697, 441, 106, 761, 299, 1040, 1105, 226, 808, 891, 578, 527, 885, 838, 782, 48, 610, 1171, 765, 139, 605, 1073, 1196, 113, 222, 355, 298, 860, 513, 1197, 458, 566, 957, 487, 451, 26, 123, 889, 60, 613, 920, 740, 586, 705, 1031, 675, 534, 430, 990, 670, 29, 1112, 1175, 181, 190, 132, 617, 320, 995, 17, 53, 90, 72, 478, 37, 579, 911, 1097, 1136, 144, 799, 387, 109, 309, 1129, 360, 364, 1186, 80, 773, 1140, 700, 225, 830, 564, 665, 1122, 130, 747, 811, 1125, 58, 108, 343, 1064, 1155, 946, 377, 825, 399, 236, 390, 359, 271, 1066, 903, 769, 806, 1063, 1096, 637, 1130, 728, 762, 574, 967, 888, 1109, 924, 1163, 729, 5, 57, 975, 440, 491, 152, 900, 400, 857, 205, 64, 1036, 1166, 997, 435, 476, 671, 374, 230, 914, 404, 1, 99, 571, 450, 875, 1087, 565, 994, 397, 20, 142, 170, 742, 778, 1024, 827, 350, 68, 304, 1115, 202, 1144, 125, 1151, 358, 10, 168, 973, 19, 81, 862, 989, 1139, 789, 259, 164, 547, 365, 624, 269, 549, 962, 603, 295, 1099, 300, 895, 442, 447, 217, 443, 707, 948, 292, 992, 65, 1090, 696, 972, 1178, 635, 652, 413, 690, 909, 55, 956, 797, 45, 1142, 1146, 876, 301, 609, 193, 1137, 1148, 708, 843, 59, 1029, 1069, 131, 905, 896, 570, 486, 965, 626, 296, 1201, 1211, 783, 897, 220, 702, 976, 1111, 874, 280, 941, 942, 1204, 864, 519, 1102, 1035, 883, 438, 325, 284, 1038, 363, 73, 869, 775, 89, 998, 676, 1019, 124, 265, 260, 940, 1015, 196, 452, 209, 1188, 1008, 324, 251, 544, 332, 129, 711, 1116, 126, 1131, 495, 650, 663, 999, 982, 479, 693, 723, 607, 661, 1020, 894, 158, 522, 1121, 211, 814, 1192, 955, 1126, 227, 127, 229, 653, 105, 657, 680, 303, 1072, 1132, 1079, 781, 845, 1128, 335, 703, 326, 908, 233, 200, 507, 85, 625, 281, 709, 1032, 904, 927, 618, 508, 199, 533, 1086, 798, 959, 532, 928, 91, 341, 660, 104, 936, 1156, 861, 473, 351, 832, 385, 780, 33, 244, 349, 94, 922, 97, 1172, 713, 346, 530, 1048, 433, 985, 867, 538, 379, 1076, 6, 901, 679, 594, 1010, 800, 504, 515, 169, 143, 1084, 858, 674, 270, 687, 561, 362, 286, 98, 572, 577, 1212, 344, 145, 1104, 7, 49, 1117, 701, 585, 853, 330, 758, 1154, 792, 596, 615, 389, 77, 1162, 415, 664, 634, 122, 219, 1210, 1057, 943, 1088, 645, 366, 834, 1206, 146, 1006, 316, 470, 964, 239, 1170, 868, 1044, 174, 854, 340, 111, 1193, 754, 246, 310, 795, 114, 1062, 844, 1214, 1067, 510, 672, 75, 367, 0, 175, 750, 878, 641, 1103, 469, 612, 107, 779, 371, 803, 403, 40, 710, 1189, 86, 1059, 724, 485, 614, 328, 658, 531, 88, 550, 988, 545, 1176, 893, 506, 767, 1184, 307, 863, 851, 221, 407, 306, 1141, 601, 321, 815, 714, 9, 112, 548, 971, 619, 1058, 589, 1203, 786, 329, 580, 373, 414, 186, 412, 590, 978, 1182, 1150, 1092, 69, 50, 247, 748, 331, 745, 1167, 71, 746, 382, 319, 983, 380, 46, 1026, 11, 1135, 426, 1023, 39, 1185, 250, 736, 277, 770, 812, 939, 147, 42, 599, 1200, 30, 92, 1012, 849, 31, 1078, 887, 755, 848, 287, 376, 16, 923, 737, 575, 194, 870, 622, 647, 63, 712, 171, 322, 1028, 241, 449, 1190, 223, 1110, 837, 621, 930, 492, 279, 1127, 1091, 644, 51, 3, 688, 231, 816, 315, 884, 249, 386, 871, 35, 235, 446, 423, 135, 738, 117, 464, 735, 1077, 866, 1061, 1083, 74, 1085, 141, 833, 1123, 157, 1022, 1039, 518, 368, 718, 274, 684, 1017, 953, 568, 546, 602, 685, 457, 1051, 266, 240, 501, 66, 689, 165, 1119, 759, 722, 155, 560, 184, 1165, 195, 732, 425, 636, 1005, 203, 354, 204, 502, 348, 639, 521, 118, 163, 459, 405, 1100, 543, 949, 4, 87, 248, 283, 996, 573, 342, 951, 121, 763, 305, 772, 1179, 323, 1071, 388, 297, 294, 261, 826, 553, 455, 150, 1133, 406, 1198, 133, 752, 372, 18, 215, 1108, 334, 1177, 595, 979, 1050, 47, 623, 947, 807, 198, 912, 1169, 654, 417, 480, 1098, 915, 23, 76, 960, 1113, 242, 582, 1011, 1055, 424, 394, 115, 427, 262, 694, 308, 720, 67, 337, 497, 1074, 21, 776, 1001, 993, 695, 706, 285, 312, 841, 743, 1095, 288, 593, 493, 938, 1134, 525, 318, 881, 207, 1107, 1068, 238, 408, 987, 161, 421, 716, 788, 731, 119, 43, 886, 206, 1056, 432, 668, 1030, 667, 218, 820, 467, 725, 252, 228, 1093, 291, 1041, 183, 189, 554, 842, 733, 428, 642, 503, 804, 254, 234, 817, 345, 154, 34, 802, 482, 1143, 1081, 597, 984, 727, 981, 918, 810, 434, 245, 855, 494, 1021, 552, 730, 1145, 419, 937, 314, 1075, 559, 62, 1009, 898, 556, 1187, 1080, 704, 771, 986, 1180, 338, 173, 760, 1157, 910, 823, 1013, 852, 648, 409, 84, 149, 166, 699, 666, 24, 383, 921, 180, 584, 592, 258, 361, 640, 496, 472, 524, 517, 629, 25, 933, 865, 289, 829, 70, 1191, 1025, 902, 677, 753, 633, 272, 1213]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1711305876456518
the save name prefix for this run is:  chkpt-ID_1711305876456518_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1107
rank avg (pred): 0.553 +- 0.007
mrr vals (pred, true): 0.017, 0.043
batch losses (mrrl, rdl): 0.0, 0.0002229934

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 569
rank avg (pred): 0.439 +- 0.266
mrr vals (pred, true): 0.123, 0.044
batch losses (mrrl, rdl): 0.0, 4.6588e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1134
rank avg (pred): 0.133 +- 0.101
mrr vals (pred, true): 0.250, 0.243
batch losses (mrrl, rdl): 0.0, 1.30362e-05

Epoch over!
epoch time: 14.384

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 848
rank avg (pred): 0.436 +- 0.262
mrr vals (pred, true): 0.107, 0.044
batch losses (mrrl, rdl): 0.0, 1.17732e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 683
rank avg (pred): 0.447 +- 0.262
mrr vals (pred, true): 0.085, 0.044
batch losses (mrrl, rdl): 0.0, 3.3361e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 507
rank avg (pred): 0.120 +- 0.089
mrr vals (pred, true): 0.216, 0.216
batch losses (mrrl, rdl): 0.0, 2.7228e-06

Epoch over!
epoch time: 14.833

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 260
rank avg (pred): 0.099 +- 0.081
mrr vals (pred, true): 0.261, 0.295
batch losses (mrrl, rdl): 0.0, 8.3954e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1090
rank avg (pred): 0.451 +- 0.255
mrr vals (pred, true): 0.065, 0.054
batch losses (mrrl, rdl): 0.0, 3.63459e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 713
rank avg (pred): 0.449 +- 0.256
mrr vals (pred, true): 0.063, 0.043
batch losses (mrrl, rdl): 0.0, 4.9144e-06

Epoch over!
epoch time: 13.648

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 803
rank avg (pred): 0.468 +- 0.255
mrr vals (pred, true): 0.058, 0.044
batch losses (mrrl, rdl): 0.0, 3.0863e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 766
rank avg (pred): 0.432 +- 0.269
mrr vals (pred, true): 0.081, 0.047
batch losses (mrrl, rdl): 0.0, 5.8529e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 96
rank avg (pred): 0.439 +- 0.265
mrr vals (pred, true): 0.079, 0.048
batch losses (mrrl, rdl): 0.0, 1.6329e-06

Epoch over!
epoch time: 14.851

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 55
rank avg (pred): 0.105 +- 0.091
mrr vals (pred, true): 0.271, 0.260
batch losses (mrrl, rdl): 0.0, 2.2726e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1067
rank avg (pred): 0.107 +- 0.097
mrr vals (pred, true): 0.274, 0.336
batch losses (mrrl, rdl): 0.0, 4.6331e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1131
rank avg (pred): 0.440 +- 0.259
mrr vals (pred, true): 0.066, 0.044
batch losses (mrrl, rdl): 0.0, 1.21484e-05

Epoch over!
epoch time: 13.77

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 794
rank avg (pred): 0.456 +- 0.259
mrr vals (pred, true): 0.059, 0.044
batch losses (mrrl, rdl): 0.0008500772, 7.13e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1060
rank avg (pred): 0.105 +- 0.083
mrr vals (pred, true): 0.314, 0.332
batch losses (mrrl, rdl): 0.0033878428, 4.8913e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 69
rank avg (pred): 0.142 +- 0.105
mrr vals (pred, true): 0.263, 0.293
batch losses (mrrl, rdl): 0.0086297737, 6.22754e-05

Epoch over!
epoch time: 13.872

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 754
rank avg (pred): 0.230 +- 0.168
mrr vals (pred, true): 0.225, 0.272
batch losses (mrrl, rdl): 0.0218308549, 0.0001943639

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 224
rank avg (pred): 0.474 +- 0.153
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 7.6446e-06, 5.72473e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 342
rank avg (pred): 0.472 +- 0.151
mrr vals (pred, true): 0.052, 0.057
batch losses (mrrl, rdl): 2.81871e-05, 0.0001006812

Epoch over!
epoch time: 15.375

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1130
rank avg (pred): 0.467 +- 0.150
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 4.12367e-05, 3.60193e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 905
rank avg (pred): 0.347 +- 0.228
mrr vals (pred, true): 0.196, 0.197
batch losses (mrrl, rdl): 1.51276e-05, 0.000701842

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 565
rank avg (pred): 0.220 +- 0.151
mrr vals (pred, true): 0.238, 0.200
batch losses (mrrl, rdl): 0.0148874987, 0.0002288915

Epoch over!
epoch time: 13.411

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 297
rank avg (pred): 0.145 +- 0.103
mrr vals (pred, true): 0.283, 0.271
batch losses (mrrl, rdl): 0.0014806434, 5.18041e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1179
rank avg (pred): 0.470 +- 0.135
mrr vals (pred, true): 0.051, 0.048
batch losses (mrrl, rdl): 8.7724e-06, 7.03616e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 769
rank avg (pred): 0.460 +- 0.141
mrr vals (pred, true): 0.057, 0.044
batch losses (mrrl, rdl): 0.0004422841, 4.26116e-05

Epoch over!
epoch time: 14.119

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 555
rank avg (pred): 0.278 +- 0.179
mrr vals (pred, true): 0.212, 0.234
batch losses (mrrl, rdl): 0.0050177965, 0.0007052466

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 252
rank avg (pred): 0.163 +- 0.111
mrr vals (pred, true): 0.273, 0.266
batch losses (mrrl, rdl): 0.0004918959, 0.0001044412

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1200
rank avg (pred): 0.462 +- 0.122
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 1.54333e-05, 5.12761e-05

Epoch over!
epoch time: 15.141

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 319
rank avg (pred): 0.233 +- 0.159
mrr vals (pred, true): 0.274, 0.268
batch losses (mrrl, rdl): 0.00035637, 0.0004413345

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 481
rank avg (pred): 0.463 +- 0.118
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 2.11382e-05, 5.31723e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 232
rank avg (pred): 0.466 +- 0.112
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 0.0001079799, 5.7888e-05

Epoch over!
epoch time: 15.473

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1024
rank avg (pred): 0.450 +- 0.133
mrr vals (pred, true): 0.057, 0.055
batch losses (mrrl, rdl): 0.0005008141, 6.2974e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 596
rank avg (pred): 0.454 +- 0.120
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.02455e-05, 5.12761e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 798
rank avg (pred): 0.456 +- 0.111
mrr vals (pred, true): 0.046, 0.043
batch losses (mrrl, rdl): 0.0001361729, 6.5314e-05

Epoch over!
epoch time: 14.709

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 964
rank avg (pred): 0.442 +- 0.130
mrr vals (pred, true): 0.055, 0.042
batch losses (mrrl, rdl): 0.0002708626, 6.32333e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 646
rank avg (pred): 0.454 +- 0.116
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.2379e-06, 5.37664e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 168
rank avg (pred): 0.451 +- 0.115
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 4.4779e-06, 5.85151e-05

Epoch over!
epoch time: 14.998

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 450
rank avg (pred): 0.451 +- 0.115
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 2.9391e-06, 5.84481e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 238
rank avg (pred): 0.451 +- 0.115
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 1.682e-07, 6.19754e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 641
rank avg (pred): 0.447 +- 0.116
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 2.9354e-05, 5.40989e-05

Epoch over!
epoch time: 14.279

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 479
rank avg (pred): 0.445 +- 0.118
mrr vals (pred, true): 0.052, 0.040
batch losses (mrrl, rdl): 3.86329e-05, 7.65818e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 559
rank avg (pred): 0.210 +- 0.134
mrr vals (pred, true): 0.252, 0.200
batch losses (mrrl, rdl): 0.0268893614, 0.0002064977

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 892
rank avg (pred): 0.369 +- 0.186
mrr vals (pred, true): 0.150, 0.133
batch losses (mrrl, rdl): 0.0030227988, 0.0003121967

Epoch over!
epoch time: 14.63

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.452 +- 0.105
mrr vals (pred, true): 0.048, 0.044

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   50 	     0 	 0.04872 	 0.04075 	 ~...
   19 	     1 	 0.04799 	 0.04079 	 ~...
   15 	     2 	 0.04789 	 0.04127 	 ~...
   27 	     3 	 0.04810 	 0.04162 	 ~...
   16 	     4 	 0.04793 	 0.04184 	 ~...
    7 	     5 	 0.04748 	 0.04197 	 ~...
    4 	     6 	 0.04736 	 0.04205 	 ~...
   86 	     7 	 0.05212 	 0.04206 	 ~...
   47 	     8 	 0.04865 	 0.04214 	 ~...
   29 	     9 	 0.04812 	 0.04214 	 ~...
   17 	    10 	 0.04793 	 0.04224 	 ~...
   77 	    11 	 0.05007 	 0.04225 	 ~...
    8 	    12 	 0.04760 	 0.04250 	 ~...
   82 	    13 	 0.05113 	 0.04280 	 ~...
   35 	    14 	 0.04826 	 0.04286 	 ~...
    2 	    15 	 0.04713 	 0.04286 	 ~...
   34 	    16 	 0.04820 	 0.04289 	 ~...
   42 	    17 	 0.04854 	 0.04304 	 ~...
   48 	    18 	 0.04865 	 0.04306 	 ~...
   13 	    19 	 0.04786 	 0.04310 	 ~...
   22 	    20 	 0.04803 	 0.04320 	 ~...
    0 	    21 	 0.04619 	 0.04320 	 ~...
   14 	    22 	 0.04788 	 0.04343 	 ~...
   51 	    23 	 0.04877 	 0.04345 	 ~...
   39 	    24 	 0.04842 	 0.04363 	 ~...
   78 	    25 	 0.05027 	 0.04366 	 ~...
   64 	    26 	 0.04929 	 0.04369 	 ~...
   18 	    27 	 0.04796 	 0.04370 	 ~...
   40 	    28 	 0.04843 	 0.04371 	 ~...
   33 	    29 	 0.04818 	 0.04372 	 ~...
   45 	    30 	 0.04861 	 0.04379 	 ~...
   71 	    31 	 0.04969 	 0.04383 	 ~...
   28 	    32 	 0.04811 	 0.04393 	 ~...
    9 	    33 	 0.04766 	 0.04407 	 ~...
    6 	    34 	 0.04747 	 0.04409 	 ~...
   44 	    35 	 0.04859 	 0.04415 	 ~...
   70 	    36 	 0.04969 	 0.04416 	 ~...
    3 	    37 	 0.04725 	 0.04421 	 ~...
   60 	    38 	 0.04916 	 0.04433 	 ~...
   68 	    39 	 0.04943 	 0.04433 	 ~...
   24 	    40 	 0.04807 	 0.04434 	 ~...
    5 	    41 	 0.04742 	 0.04453 	 ~...
   49 	    42 	 0.04871 	 0.04454 	 ~...
   32 	    43 	 0.04818 	 0.04459 	 ~...
   11 	    44 	 0.04781 	 0.04463 	 ~...
   73 	    45 	 0.04979 	 0.04466 	 ~...
   43 	    46 	 0.04858 	 0.04496 	 ~...
   84 	    47 	 0.05157 	 0.04500 	 ~...
   55 	    48 	 0.04881 	 0.04536 	 ~...
   75 	    49 	 0.04997 	 0.04576 	 ~...
   12 	    50 	 0.04786 	 0.04585 	 ~...
   38 	    51 	 0.04839 	 0.04592 	 ~...
   31 	    52 	 0.04817 	 0.04596 	 ~...
   79 	    53 	 0.05030 	 0.04609 	 ~...
   30 	    54 	 0.04813 	 0.04622 	 ~...
   61 	    55 	 0.04919 	 0.04639 	 ~...
   81 	    56 	 0.05104 	 0.04648 	 ~...
   10 	    57 	 0.04780 	 0.04677 	 ~...
   25 	    58 	 0.04808 	 0.04715 	 ~...
   85 	    59 	 0.05194 	 0.04741 	 ~...
   74 	    60 	 0.04982 	 0.04791 	 ~...
   57 	    61 	 0.04901 	 0.04809 	 ~...
    1 	    62 	 0.04689 	 0.04830 	 ~...
   58 	    63 	 0.04906 	 0.04852 	 ~...
   83 	    64 	 0.05137 	 0.04883 	 ~...
   76 	    65 	 0.05007 	 0.04905 	 ~...
   54 	    66 	 0.04881 	 0.04915 	 ~...
   23 	    67 	 0.04804 	 0.04955 	 ~...
   62 	    68 	 0.04924 	 0.04970 	 ~...
   69 	    69 	 0.04965 	 0.04984 	 ~...
   26 	    70 	 0.04809 	 0.05151 	 ~...
   59 	    71 	 0.04910 	 0.05172 	 ~...
   80 	    72 	 0.05075 	 0.05204 	 ~...
   53 	    73 	 0.04879 	 0.05217 	 ~...
   37 	    74 	 0.04834 	 0.05231 	 ~...
   52 	    75 	 0.04879 	 0.05280 	 ~...
   65 	    76 	 0.04934 	 0.05335 	 ~...
   67 	    77 	 0.04943 	 0.05375 	 ~...
   36 	    78 	 0.04833 	 0.05389 	 ~...
   21 	    79 	 0.04802 	 0.05390 	 ~...
   46 	    80 	 0.04864 	 0.05418 	 ~...
   72 	    81 	 0.04970 	 0.05566 	 ~...
   41 	    82 	 0.04853 	 0.05630 	 ~...
   20 	    83 	 0.04800 	 0.05720 	 ~...
   56 	    84 	 0.04884 	 0.05853 	 ~...
   63 	    85 	 0.04925 	 0.05950 	 ~...
   66 	    86 	 0.04935 	 0.06284 	 ~...
   87 	    87 	 0.14683 	 0.10993 	 m..s
   91 	    88 	 0.19764 	 0.18777 	 ~...
   94 	    89 	 0.21125 	 0.19000 	 ~...
  104 	    90 	 0.22714 	 0.19222 	 m..s
   98 	    91 	 0.21300 	 0.19225 	 ~...
   89 	    92 	 0.17843 	 0.20042 	 ~...
  100 	    93 	 0.21914 	 0.20159 	 ~...
   90 	    94 	 0.19409 	 0.20417 	 ~...
  106 	    95 	 0.23200 	 0.20646 	 ~...
   96 	    96 	 0.21155 	 0.20981 	 ~...
   88 	    97 	 0.17613 	 0.20997 	 m..s
   92 	    98 	 0.21099 	 0.21167 	 ~...
  105 	    99 	 0.22754 	 0.21172 	 ~...
   93 	   100 	 0.21118 	 0.21544 	 ~...
  102 	   101 	 0.22057 	 0.21705 	 ~...
  101 	   102 	 0.21936 	 0.21721 	 ~...
   95 	   103 	 0.21141 	 0.22306 	 ~...
   97 	   104 	 0.21170 	 0.23255 	 ~...
  103 	   105 	 0.22345 	 0.24077 	 ~...
  107 	   106 	 0.25576 	 0.24127 	 ~...
  115 	   107 	 0.28449 	 0.25490 	 ~...
   99 	   108 	 0.21573 	 0.25616 	 m..s
  117 	   109 	 0.28969 	 0.27130 	 ~...
  112 	   110 	 0.28248 	 0.27167 	 ~...
  108 	   111 	 0.26905 	 0.27208 	 ~...
  116 	   112 	 0.28961 	 0.28089 	 ~...
  119 	   113 	 0.31132 	 0.29096 	 ~...
  109 	   114 	 0.26968 	 0.29126 	 ~...
  113 	   115 	 0.28260 	 0.29262 	 ~...
  110 	   116 	 0.27160 	 0.29520 	 ~...
  118 	   117 	 0.30980 	 0.30151 	 ~...
  111 	   118 	 0.27671 	 0.30518 	 ~...
  114 	   119 	 0.28397 	 0.30816 	 ~...
  120 	   120 	 0.31207 	 0.34144 	 ~...
==========================================
r_mrr = 0.991669237613678
r2_mrr = 0.982986330986023
spearmanr_mrr@5 = 0.6460267305374146
spearmanr_mrr@10 = 0.8430868983268738
spearmanr_mrr@50 = 0.995512843132019
spearmanr_mrr@100 = 0.997511088848114
spearmanr_mrr@All = 0.9976188540458679
==========================================
test time: 0.556
Done Testing dataset Kinships
total time taken: 226.52728271484375
training time taken: 218.1296718120575
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9917)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9830)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.6460)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.8431)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9955)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9975)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9976)}}, 'test_loss': {'TransE': {'Kinships': 0.16736936548841186}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 9746375646397996
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1167, 329, 79, 739, 593, 723, 231, 545, 309, 845, 698, 418, 65, 606, 84, 46, 1128, 1048, 915, 1134, 943, 840, 1017, 669, 821, 381, 810, 987, 842, 477, 12, 668, 147, 205, 125, 294, 286, 1001, 434, 687, 129, 707, 4, 240, 60, 899, 1179, 53, 480, 8, 2, 499, 392, 85, 1089, 813, 190, 106, 1209, 939, 248, 311, 1140, 478, 218, 645, 543, 524, 647, 308, 704, 1163, 226, 442, 1051, 1143, 1211, 729, 276, 41, 1176, 1055, 555, 201, 1117, 1170, 140, 626, 1212, 378, 176, 568, 1027, 1103, 193, 516, 155, 721, 981, 1174, 886, 651, 1070, 763, 627, 650, 1138, 942, 1186, 825, 426, 73, 1189, 1120, 802, 55, 920, 28, 918, 123, 498]
valid_ids (0): []
train_ids (1094): [1214, 379, 624, 951, 561, 330, 642, 881, 115, 66, 401, 991, 214, 1037, 181, 1173, 590, 307, 796, 888, 1210, 277, 528, 353, 831, 163, 768, 388, 697, 210, 851, 678, 900, 209, 760, 709, 296, 1191, 1041, 877, 969, 549, 998, 23, 130, 14, 132, 879, 490, 634, 441, 1095, 520, 930, 482, 793, 649, 278, 718, 858, 811, 445, 394, 166, 578, 790, 346, 944, 1203, 1145, 622, 798, 803, 1151, 596, 563, 1091, 13, 458, 518, 570, 773, 658, 15, 1015, 198, 1164, 875, 786, 456, 252, 1102, 457, 454, 670, 86, 1193, 199, 931, 435, 154, 38, 247, 334, 841, 836, 238, 419, 871, 685, 323, 996, 592, 859, 466, 955, 267, 856, 400, 769, 1129, 1177, 903, 986, 883, 491, 523, 715, 157, 743, 961, 664, 756, 680, 179, 666, 64, 259, 728, 688, 1094, 101, 701, 373, 783, 994, 58, 504, 675, 356, 317, 800, 475, 682, 677, 325, 312, 1078, 94, 548, 372, 791, 459, 290, 933, 767, 200, 74, 78, 119, 156, 118, 162, 358, 742, 1202, 967, 422, 429, 185, 342, 1026, 249, 175, 661, 139, 173, 817, 1034, 148, 1108, 17, 1036, 88, 1, 6, 1004, 246, 905, 387, 510, 243, 1118, 174, 1074, 1086, 572, 988, 919, 801, 486, 260, 646, 1201, 1008, 272, 902, 618, 740, 684, 500, 546, 1049, 494, 241, 603, 3, 979, 1058, 288, 135, 391, 415, 211, 508, 759, 517, 982, 1052, 83, 1156, 1032, 1110, 313, 735, 892, 167, 228, 1038, 62, 713, 1077, 519, 109, 237, 522, 749, 965, 348, 389, 253, 124, 737, 122, 215, 245, 295, 702, 576, 908, 950, 142, 357, 133, 623, 619, 1047, 47, 1035, 533, 1013, 285, 302, 131, 421, 439, 127, 946, 361, 301, 1127, 750, 316, 1107, 872, 45, 54, 861, 385, 492, 582, 695, 693, 452, 44, 992, 474, 1132, 909, 481, 1093, 273, 239, 1040, 7, 540, 662, 1012, 380, 34, 714, 159, 1050, 894, 414, 635, 959, 51, 1072, 298, 692, 581, 970, 416, 826, 954, 878, 244, 852, 183, 844, 922, 332, 483, 1061, 613, 656, 374, 77, 300, 324, 962, 1161, 39, 1162, 972, 192, 912, 364, 819, 876, 397, 1084, 489, 1081, 1105, 462, 29, 496, 799, 997, 1206, 449, 539, 754, 553, 406, 639, 604, 1087, 1147, 460, 1188, 1064, 365, 591, 676, 863, 839, 691, 417, 487, 515, 37, 91, 168, 1204, 367, 161, 751, 87, 507, 432, 509, 1022, 617, 377, 68, 476, 1130, 112, 99, 1131, 1045, 121, 32, 734, 537, 279, 679, 512, 1075, 340, 868, 726, 804, 110, 443, 484, 690, 337, 1199, 319, 448, 806, 587, 1207, 59, 1180, 1005, 536, 874, 48, 1028, 755, 468, 1009, 706, 938, 990, 404, 424, 72, 832, 194, 1139, 322, 1079, 779, 1115, 621, 223, 1113, 144, 265, 502, 262, 164, 1141, 384, 667, 673, 315, 873, 1080, 327, 846, 923, 689, 889, 141, 440, 36, 1144, 648, 61, 203, 830, 978, 1200, 178, 146, 150, 565, 1205, 1178, 220, 1148, 453, 1184, 562, 501, 652, 571, 1057, 25, 1106, 824, 532, 18, 172, 386, 822, 1063, 197, 829, 229, 398, 616, 1160, 665, 632, 927, 344, 1194, 1016, 789, 747, 674, 864, 525, 848, 631, 281, 828, 347, 1066, 816, 446, 1175, 196, 409, 188, 977, 1185, 663, 93, 1071, 89, 940, 33, 369, 1100, 1169, 1062, 69, 823, 1114, 1116, 67, 960, 455, 352, 236, 827, 834, 1073, 1195, 165, 1125, 866, 580, 999, 584, 1025, 436, 808, 1136, 376, 463, 363, 552, 867, 686, 270, 535, 80, 1135, 608, 995, 897, 630, 683, 534, 22, 629, 941, 812, 907, 657, 271, 287, 493, 473, 1159, 741, 1183, 395, 503, 611, 855, 405, 904, 331, 306, 521, 513, 469, 428, 948, 1197, 202, 612, 92, 182, 338, 382, 1190, 564, 318, 814, 1146, 339, 153, 1150, 283, 1068, 105, 1152, 917, 76, 1192, 297, 497, 488, 470, 1056, 292, 5, 350, 1213, 1098, 722, 774, 1082, 560, 926, 1097, 833, 609, 314, 396, 929, 731, 880, 427, 780, 765, 542, 579, 42, 1096, 1003, 35, 56, 643, 974, 1119, 355, 711, 558, 547, 720, 901, 890, 180, 752, 100, 304, 408, 354, 43, 1208, 26, 934, 820, 70, 569, 11, 1023, 1067, 818, 640, 256, 310, 186, 605, 1033, 895, 264, 653, 506, 250, 1133, 976, 1149, 993, 230, 321, 225, 853, 138, 465, 328, 30, 705, 407, 869, 1030, 169, 1007, 431, 104, 968, 438, 75, 1109, 757, 937, 849, 1024, 600, 40, 654, 712, 402, 914, 594, 187, 343, 730, 255, 764, 207, 805, 589, 191, 1099, 1153, 116, 857, 781, 50, 766, 732, 1142, 550, 275, 413, 1172, 97, 703, 336, 1006, 785, 370, 637, 544, 947, 10, 699, 1122, 189, 217, 303, 251, 1171, 511, 126, 1069, 1124, 744, 838, 807, 433, 1053, 794, 966, 1154, 1076, 222, 538, 242, 410, 206, 924, 708, 160, 911, 753, 375, 27, 636, 117, 610, 633, 925, 280, 450, 120, 49, 1168, 1029, 566, 1059, 1181, 1000, 1002, 913, 102, 638, 655, 216, 556, 1137, 795, 696, 599, 447, 885, 158, 472, 351, 577, 149, 615, 531, 583, 284, 758, 57, 671, 644, 541, 326, 9, 854, 588, 1155, 882, 145, 906, 299, 1187, 896, 20, 865, 261, 921, 1090, 614, 461, 952, 1011, 305, 985, 527, 788, 430, 1092, 108, 1157, 776, 557, 607, 575, 898, 554, 221, 850, 574, 291, 1010, 710, 598, 916, 467, 98, 213, 371, 815, 771, 16, 893, 177, 748, 862, 514, 366, 204, 257, 551, 293, 641, 341, 320, 96, 843, 1198, 111, 597, 82, 601, 1101, 567, 208, 945, 887, 1043, 1018, 891, 234, 778, 24, 1021, 274, 953, 736, 681, 983, 333, 787, 949, 1060, 1166, 263, 1019, 1085, 1112, 725, 95, 266, 797, 761, 254, 775, 936, 727, 19, 719, 485, 971, 1031, 345, 349, 63, 784, 745, 103, 659, 471, 235, 1165, 772, 1044, 128, 700, 1039, 809, 151, 530, 1123, 1046, 137, 717, 479, 620, 383, 625, 733, 31, 770, 1083, 152, 269, 777, 90, 425, 1196, 444, 884, 660, 233, 762, 411, 1020, 989, 628, 975, 390, 170, 1126, 1121, 716, 694, 136, 224, 738, 107, 113, 282, 935, 505, 792, 1054, 268, 184, 52, 870, 362, 956, 360, 602, 420, 227, 335, 928, 359, 212, 289, 232, 910, 403, 81, 219, 782, 437, 0, 984, 393, 258, 526, 21, 964, 860, 1104, 195, 957, 134, 963, 585, 495, 835, 1158, 464, 114, 595, 399, 573, 171, 1065, 973, 1042, 451, 559, 746, 423, 412, 932, 837, 71, 1014, 672, 368, 586, 143, 529, 1111, 1088, 724, 847, 980, 1182, 958]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3625581143562470
the save name prefix for this run is:  chkpt-ID_3625581143562470_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 806
rank avg (pred): 0.542 +- 0.003
mrr vals (pred, true): 0.018, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001917012

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 604
rank avg (pred): 0.458 +- 0.258
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0, 1.483e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 109
rank avg (pred): 0.453 +- 0.265
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.0, 4.4006e-06

Epoch over!
epoch time: 14.039

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1062
rank avg (pred): 0.096 +- 0.114
mrr vals (pred, true): 0.249, 0.305
batch losses (mrrl, rdl): 0.0, 5.38e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 405
rank avg (pred): 0.444 +- 0.270
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 0.0, 1.41213e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 206
rank avg (pred): 0.447 +- 0.259
mrr vals (pred, true): 0.046, 0.046
batch losses (mrrl, rdl): 0.0, 3.467e-07

Epoch over!
epoch time: 13.611

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 137
rank avg (pred): 0.444 +- 0.262
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 0.0, 3.727e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 264
rank avg (pred): 0.089 +- 0.108
mrr vals (pred, true): 0.222, 0.299
batch losses (mrrl, rdl): 0.0, 1.077e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 515
rank avg (pred): 0.130 +- 0.153
mrr vals (pred, true): 0.179, 0.221
batch losses (mrrl, rdl): 0.0, 2.503e-07

Epoch over!
epoch time: 14.177

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 453
rank avg (pred): 0.444 +- 0.272
mrr vals (pred, true): 0.053, 0.042
batch losses (mrrl, rdl): 0.0, 1.51053e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 683
rank avg (pred): 0.456 +- 0.261
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.0, 2.6554e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1042
rank avg (pred): 0.459 +- 0.266
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0, 1.237e-07

Epoch over!
epoch time: 14.675

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 748
rank avg (pred): 0.217 +- 0.209
mrr vals (pred, true): 0.119, 0.182
batch losses (mrrl, rdl): 0.0, 5.8961e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 672
rank avg (pred): 0.468 +- 0.260
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0, 3.236e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1126
rank avg (pred): 0.431 +- 0.270
mrr vals (pred, true): 0.064, 0.049
batch losses (mrrl, rdl): 0.0, 1.33949e-05

Epoch over!
epoch time: 15.211

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 409
rank avg (pred): 0.447 +- 0.283
mrr vals (pred, true): 0.067, 0.042
batch losses (mrrl, rdl): 0.003032435, 1.17831e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 36
rank avg (pred): 0.120 +- 0.139
mrr vals (pred, true): 0.278, 0.269
batch losses (mrrl, rdl): 0.0009161461, 1.96931e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 847
rank avg (pred): 0.462 +- 0.198
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 1.30369e-05, 2.14698e-05

Epoch over!
epoch time: 14.907

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 635
rank avg (pred): 0.458 +- 0.210
mrr vals (pred, true): 0.058, 0.050
batch losses (mrrl, rdl): 0.0006515105, 2.23304e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 113
rank avg (pred): 0.473 +- 0.197
mrr vals (pred, true): 0.045, 0.050
batch losses (mrrl, rdl): 0.0002979705, 3.70229e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 214
rank avg (pred): 0.453 +- 0.194
mrr vals (pred, true): 0.053, 0.042
batch losses (mrrl, rdl): 9.52837e-05, 1.98094e-05

Epoch over!
epoch time: 14.169

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1157
rank avg (pred): 0.158 +- 0.150
mrr vals (pred, true): 0.242, 0.258
batch losses (mrrl, rdl): 0.0027319533, 6.67715e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 822
rank avg (pred): 0.227 +- 0.180
mrr vals (pred, true): 0.192, 0.259
batch losses (mrrl, rdl): 0.0444562212, 9.88014e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 285
rank avg (pred): 0.143 +- 0.136
mrr vals (pred, true): 0.283, 0.322
batch losses (mrrl, rdl): 0.01590636, 9.08657e-05

Epoch over!
epoch time: 14.342

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 890
rank avg (pred): 0.438 +- 0.177
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 5.11417e-05, 5.17088e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1047
rank avg (pred): 0.436 +- 0.158
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0002147765, 6.91791e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 618
rank avg (pred): 0.452 +- 0.198
mrr vals (pred, true): 0.054, 0.046
batch losses (mrrl, rdl): 0.0001817802, 1.89158e-05

Epoch over!
epoch time: 14.595

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 871
rank avg (pred): 0.473 +- 0.192
mrr vals (pred, true): 0.042, 0.044
batch losses (mrrl, rdl): 0.0006973849, 2.12652e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 794
rank avg (pred): 0.449 +- 0.184
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 6.95932e-05, 2.89599e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 22
rank avg (pred): 0.135 +- 0.107
mrr vals (pred, true): 0.266, 0.256
batch losses (mrrl, rdl): 0.0009826161, 2.41035e-05

Epoch over!
epoch time: 12.946

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 23
rank avg (pred): 0.139 +- 0.111
mrr vals (pred, true): 0.275, 0.277
batch losses (mrrl, rdl): 7.1263e-05, 1.49434e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 567
rank avg (pred): 0.463 +- 0.225
mrr vals (pred, true): 0.058, 0.046
batch losses (mrrl, rdl): 0.0007132719, 8.8985e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 202
rank avg (pred): 0.493 +- 0.229
mrr vals (pred, true): 0.044, 0.043
batch losses (mrrl, rdl): 0.0003387156, 9.2335e-06

Epoch over!
epoch time: 15.26

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 656
rank avg (pred): 0.433 +- 0.200
mrr vals (pred, true): 0.062, 0.047
batch losses (mrrl, rdl): 0.0013886271, 3.4943e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 500
rank avg (pred): 0.267 +- 0.169
mrr vals (pred, true): 0.201, 0.204
batch losses (mrrl, rdl): 9.3409e-05, 0.0004125366

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 631
rank avg (pred): 0.465 +- 0.233
mrr vals (pred, true): 0.058, 0.047
batch losses (mrrl, rdl): 0.0007204709, 1.02977e-05

Epoch over!
epoch time: 13.74

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 262
rank avg (pred): 0.137 +- 0.094
mrr vals (pred, true): 0.260, 0.248
batch losses (mrrl, rdl): 0.0012254716, 2.52129e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 160
rank avg (pred): 0.428 +- 0.195
mrr vals (pred, true): 0.057, 0.052
batch losses (mrrl, rdl): 0.0004328516, 2.11713e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 872
rank avg (pred): 0.456 +- 0.211
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 1.03375e-05, 3.10483e-05

Epoch over!
epoch time: 14.013

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 592
rank avg (pred): 0.462 +- 0.216
mrr vals (pred, true): 0.049, 0.048
batch losses (mrrl, rdl): 1.53902e-05, 1.41206e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 48
rank avg (pred): 0.120 +- 0.082
mrr vals (pred, true): 0.286, 0.296
batch losses (mrrl, rdl): 0.0008997213, 1.72456e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 159
rank avg (pred): 0.417 +- 0.186
mrr vals (pred, true): 0.057, 0.049
batch losses (mrrl, rdl): 0.0005386142, 3.52422e-05

Epoch over!
epoch time: 13.11

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 823
rank avg (pred): 0.189 +- 0.132
mrr vals (pred, true): 0.251, 0.256
batch losses (mrrl, rdl): 0.0001983003, 2.32769e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 465
rank avg (pred): 0.476 +- 0.228
mrr vals (pred, true): 0.043, 0.042
batch losses (mrrl, rdl): 0.0004763573, 8.6663e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1124
rank avg (pred): 0.404 +- 0.140
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 4.2689e-06, 0.000142096

Epoch over!
epoch time: 13.048

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.475 +- 0.234
mrr vals (pred, true): 0.046, 0.049

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.04721 	 0.04071 	 ~...
   32 	     1 	 0.04813 	 0.04075 	 ~...
   35 	     2 	 0.04832 	 0.04134 	 ~...
   33 	     3 	 0.04825 	 0.04154 	 ~...
   56 	     4 	 0.05003 	 0.04198 	 ~...
    2 	     5 	 0.04481 	 0.04209 	 ~...
   70 	     6 	 0.05132 	 0.04209 	 ~...
   44 	     7 	 0.04870 	 0.04215 	 ~...
   75 	     8 	 0.05235 	 0.04225 	 ~...
   47 	     9 	 0.04886 	 0.04245 	 ~...
   69 	    10 	 0.05126 	 0.04274 	 ~...
    0 	    11 	 0.04386 	 0.04276 	 ~...
   30 	    12 	 0.04809 	 0.04278 	 ~...
   11 	    13 	 0.04665 	 0.04286 	 ~...
   76 	    14 	 0.05412 	 0.04301 	 ~...
   54 	    15 	 0.04986 	 0.04302 	 ~...
    3 	    16 	 0.04537 	 0.04330 	 ~...
   17 	    17 	 0.04720 	 0.04333 	 ~...
   46 	    18 	 0.04879 	 0.04343 	 ~...
   62 	    19 	 0.05045 	 0.04349 	 ~...
    9 	    20 	 0.04625 	 0.04363 	 ~...
   41 	    21 	 0.04851 	 0.04367 	 ~...
   39 	    22 	 0.04846 	 0.04390 	 ~...
   57 	    23 	 0.05015 	 0.04392 	 ~...
   14 	    24 	 0.04704 	 0.04401 	 ~...
    1 	    25 	 0.04459 	 0.04418 	 ~...
    4 	    26 	 0.04548 	 0.04433 	 ~...
   36 	    27 	 0.04833 	 0.04433 	 ~...
   74 	    28 	 0.05193 	 0.04436 	 ~...
   67 	    29 	 0.05085 	 0.04476 	 ~...
   20 	    30 	 0.04747 	 0.04484 	 ~...
   61 	    31 	 0.05041 	 0.04495 	 ~...
   10 	    32 	 0.04636 	 0.04496 	 ~...
   23 	    33 	 0.04771 	 0.04496 	 ~...
   13 	    34 	 0.04701 	 0.04497 	 ~...
   37 	    35 	 0.04835 	 0.04498 	 ~...
   34 	    36 	 0.04825 	 0.04500 	 ~...
   64 	    37 	 0.05073 	 0.04533 	 ~...
   63 	    38 	 0.05061 	 0.04546 	 ~...
   16 	    39 	 0.04717 	 0.04551 	 ~...
   40 	    40 	 0.04850 	 0.04551 	 ~...
   66 	    41 	 0.05077 	 0.04553 	 ~...
   26 	    42 	 0.04793 	 0.04562 	 ~...
   77 	    43 	 0.05636 	 0.04582 	 ~...
   42 	    44 	 0.04852 	 0.04591 	 ~...
   22 	    45 	 0.04768 	 0.04598 	 ~...
    8 	    46 	 0.04615 	 0.04606 	 ~...
   60 	    47 	 0.05034 	 0.04615 	 ~...
   71 	    48 	 0.05135 	 0.04616 	 ~...
   55 	    49 	 0.04994 	 0.04616 	 ~...
   24 	    50 	 0.04782 	 0.04653 	 ~...
   15 	    51 	 0.04709 	 0.04666 	 ~...
   31 	    52 	 0.04813 	 0.04680 	 ~...
   38 	    53 	 0.04842 	 0.04700 	 ~...
   45 	    54 	 0.04876 	 0.04723 	 ~...
   72 	    55 	 0.05188 	 0.04728 	 ~...
   51 	    56 	 0.04961 	 0.04753 	 ~...
   53 	    57 	 0.04982 	 0.04793 	 ~...
   58 	    58 	 0.05029 	 0.04831 	 ~...
    7 	    59 	 0.04603 	 0.04835 	 ~...
   43 	    60 	 0.04853 	 0.04852 	 ~...
   28 	    61 	 0.04802 	 0.04884 	 ~...
   48 	    62 	 0.04903 	 0.04890 	 ~...
   19 	    63 	 0.04736 	 0.04940 	 ~...
    5 	    64 	 0.04580 	 0.04945 	 ~...
   29 	    65 	 0.04808 	 0.04949 	 ~...
   65 	    66 	 0.05075 	 0.04955 	 ~...
   59 	    67 	 0.05031 	 0.05029 	 ~...
    6 	    68 	 0.04597 	 0.05086 	 ~...
   27 	    69 	 0.04794 	 0.05149 	 ~...
   12 	    70 	 0.04697 	 0.05179 	 ~...
   25 	    71 	 0.04792 	 0.05188 	 ~...
   49 	    72 	 0.04904 	 0.05265 	 ~...
   68 	    73 	 0.05096 	 0.05294 	 ~...
   21 	    74 	 0.04758 	 0.05317 	 ~...
   73 	    75 	 0.05191 	 0.05321 	 ~...
   50 	    76 	 0.04927 	 0.05340 	 ~...
   52 	    77 	 0.04976 	 0.05494 	 ~...
   78 	    78 	 0.08949 	 0.10993 	 ~...
   84 	    79 	 0.20513 	 0.17873 	 ~...
   83 	    80 	 0.20067 	 0.17928 	 ~...
   80 	    81 	 0.19656 	 0.18413 	 ~...
   81 	    82 	 0.19787 	 0.18777 	 ~...
   82 	    83 	 0.20020 	 0.19216 	 ~...
   89 	    84 	 0.20951 	 0.20646 	 ~...
   86 	    85 	 0.20608 	 0.21728 	 ~...
   85 	    86 	 0.20600 	 0.22306 	 ~...
   79 	    87 	 0.19566 	 0.22530 	 ~...
   88 	    88 	 0.20931 	 0.22636 	 ~...
   91 	    89 	 0.21346 	 0.23195 	 ~...
   87 	    90 	 0.20667 	 0.23396 	 ~...
   95 	    91 	 0.24545 	 0.23600 	 ~...
   90 	    92 	 0.21323 	 0.24234 	 ~...
   92 	    93 	 0.23589 	 0.24312 	 ~...
  103 	    94 	 0.26549 	 0.25982 	 ~...
  102 	    95 	 0.26539 	 0.26046 	 ~...
   93 	    96 	 0.23639 	 0.26123 	 ~...
   97 	    97 	 0.25539 	 0.26194 	 ~...
  101 	    98 	 0.26306 	 0.26262 	 ~...
   96 	    99 	 0.25311 	 0.26278 	 ~...
  106 	   100 	 0.26814 	 0.26409 	 ~...
  109 	   101 	 0.26861 	 0.26563 	 ~...
   98 	   102 	 0.25647 	 0.26867 	 ~...
  108 	   103 	 0.26824 	 0.27160 	 ~...
   99 	   104 	 0.26081 	 0.27421 	 ~...
  110 	   105 	 0.27125 	 0.27470 	 ~...
  111 	   106 	 0.27160 	 0.27906 	 ~...
  115 	   107 	 0.27680 	 0.27984 	 ~...
  100 	   108 	 0.26117 	 0.28280 	 ~...
  113 	   109 	 0.27420 	 0.28516 	 ~...
  116 	   110 	 0.28524 	 0.28517 	 ~...
  112 	   111 	 0.27161 	 0.28689 	 ~...
  114 	   112 	 0.27670 	 0.28696 	 ~...
   94 	   113 	 0.23843 	 0.28917 	 m..s
  104 	   114 	 0.26587 	 0.29543 	 ~...
  117 	   115 	 0.30463 	 0.29789 	 ~...
  107 	   116 	 0.26824 	 0.30137 	 m..s
  105 	   117 	 0.26796 	 0.30816 	 m..s
  118 	   118 	 0.31030 	 0.31539 	 ~...
  120 	   119 	 0.32407 	 0.34190 	 ~...
  119 	   120 	 0.31348 	 0.35205 	 m..s
==========================================
r_mrr = 0.9950409531593323
r2_mrr = 0.9866459369659424
spearmanr_mrr@5 = 0.8678779602050781
spearmanr_mrr@10 = 0.9350746870040894
spearmanr_mrr@50 = 0.9927065372467041
spearmanr_mrr@100 = 0.997834324836731
spearmanr_mrr@All = 0.9980794787406921
==========================================
test time: 0.425
Done Testing dataset Kinships
total time taken: 221.2377257347107
training time taken: 212.35344529151917
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9950)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9866)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.8679)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9351)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9927)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9978)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9981)}}, 'test_loss': {'TransE': {'Kinships': 0.16926127596343576}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 7268825403311360
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [114, 464, 951, 167, 838, 264, 605, 883, 385, 362, 480, 257, 671, 399, 949, 635, 495, 267, 327, 434, 469, 232, 406, 796, 127, 903, 290, 386, 839, 205, 510, 1130, 522, 889, 943, 524, 247, 1104, 665, 870, 484, 725, 1165, 1131, 855, 530, 1142, 479, 997, 259, 142, 731, 1013, 965, 1173, 905, 1149, 502, 754, 1029, 825, 514, 305, 314, 994, 494, 1084, 897, 1139, 189, 75, 345, 584, 597, 970, 578, 273, 186, 1175, 105, 64, 1113, 860, 794, 518, 251, 645, 1001, 1186, 37, 433, 297, 979, 526, 1023, 1026, 1191, 66, 938, 262, 203, 892, 225, 1159, 335, 431, 1109, 587, 696, 644, 676, 161, 873, 1058, 109, 963, 588, 972, 1018, 298, 150]
valid_ids (0): []
train_ids (1094): [737, 1086, 712, 791, 470, 988, 374, 246, 1033, 413, 230, 284, 1208, 924, 913, 1079, 261, 1027, 556, 1020, 710, 1, 72, 48, 172, 577, 1138, 319, 581, 663, 133, 819, 307, 593, 455, 65, 682, 772, 210, 702, 801, 175, 25, 738, 888, 183, 170, 740, 332, 199, 858, 223, 638, 121, 705, 637, 35, 793, 811, 953, 786, 300, 289, 238, 893, 500, 1148, 920, 1016, 196, 700, 1071, 1164, 561, 436, 869, 643, 85, 1178, 709, 1017, 201, 437, 438, 106, 371, 1060, 912, 499, 70, 1065, 271, 124, 782, 1147, 533, 751, 56, 392, 887, 1050, 824, 166, 23, 727, 329, 389, 277, 6, 143, 572, 458, 564, 901, 1196, 1195, 285, 160, 188, 103, 67, 847, 917, 973, 342, 693, 1106, 129, 102, 1068, 1036, 1161, 357, 922, 0, 1085, 1019, 942, 844, 616, 685, 1102, 1205, 1028, 258, 803, 907, 789, 218, 746, 641, 989, 932, 1077, 153, 14, 854, 51, 785, 68, 842, 1140, 711, 465, 366, 435, 704, 720, 107, 417, 862, 809, 1108, 229, 573, 235, 1123, 1194, 1204, 352, 1074, 178, 214, 759, 835, 843, 180, 475, 274, 872, 453, 304, 978, 675, 1133, 110, 960, 269, 139, 539, 615, 781, 574, 805, 954, 804, 1037, 5, 36, 303, 381, 10, 909, 850, 351, 969, 29, 977, 1180, 940, 910, 1115, 89, 242, 836, 1078, 764, 719, 516, 292, 163, 1124, 1193, 123, 625, 830, 47, 513, 456, 1152, 380, 477, 856, 63, 322, 713, 1129, 1031, 220, 1064, 84, 370, 78, 55, 492, 760, 1080, 817, 598, 1177, 871, 1083, 1141, 12, 604, 411, 197, 840, 239, 278, 559, 545, 906, 1155, 523, 363, 1057, 250, 895, 648, 620, 831, 689, 1111, 1046, 46, 981, 387, 686, 761, 294, 155, 253, 118, 491, 1095, 1054, 899, 1039, 591, 945, 213, 86, 621, 485, 1048, 337, 962, 111, 557, 1214, 624, 313, 349, 445, 373, 651, 444, 4, 652, 777, 1171, 224, 308, 779, 99, 173, 410, 324, 569, 885, 424, 664, 276, 1120, 310, 1009, 355, 339, 657, 776, 54, 33, 995, 1000, 179, 254, 404, 890, 642, 1200, 863, 1066, 1192, 87, 402, 22, 1047, 1117, 369, 1024, 159, 553, 1088, 565, 286, 265, 359, 320, 612, 1097, 1090, 1010, 136, 575, 1127, 993, 724, 618, 818, 900, 291, 815, 134, 1059, 157, 698, 875, 971, 1153, 611, 3, 209, 991, 266, 535, 1005, 946, 808, 623, 1135, 681, 821, 916, 763, 1211, 1014, 877, 252, 45, 190, 1157, 34, 976, 586, 959, 1126, 145, 420, 62, 957, 1012, 443, 282, 472, 18, 1212, 745, 138, 741, 233, 521, 1032, 119, 736, 39, 659, 662, 739, 442, 1021, 93, 268, 1082, 212, 354, 542, 200, 439, 204, 228, 1052, 547, 688, 774, 857, 1184, 914, 462, 589, 16, 76, 1063, 832, 829, 1183, 636, 617, 83, 1101, 493, 390, 944, 592, 409, 629, 116, 128, 400, 783, 104, 987, 837, 21, 936, 653, 275, 600, 240, 628, 1201, 998, 607, 904, 185, 8, 926, 98, 807, 800, 488, 806, 376, 50, 596, 350, 1107, 911, 501, 1150, 930, 112, 551, 896, 100, 487, 429, 918, 227, 706, 130, 790, 19, 137, 146, 958, 115, 394, 94, 1174, 343, 852, 861, 208, 947, 1210, 234, 1007, 517, 1045, 53, 921, 428, 184, 672, 463, 418, 1125, 716, 9, 1143, 1176, 554, 568, 154, 723, 602, 1003, 1197, 865, 966, 646, 309, 288, 538, 627, 483, 610, 679, 216, 44, 1006, 982, 79, 407, 853, 1053, 243, 822, 415, 849, 563, 703, 721, 38, 732, 931, 1116, 140, 459, 941, 511, 1185, 1051, 631, 955, 583, 828, 735, 177, 622, 1092, 90, 207, 687, 1022, 722, 669, 714, 202, 255, 498, 1002, 915, 684, 579, 1011, 718, 908, 1154, 697, 670, 331, 325, 729, 169, 403, 353, 975, 1114, 473, 347, 509, 919, 476, 287, 874, 1035, 1076, 1030, 466, 1073, 762, 328, 384, 747, 851, 1158, 396, 1069, 1167, 999, 540, 1110, 823, 742, 626, 950, 367, 211, 414, 395, 1203, 421, 750, 765, 408, 531, 1156, 678, 787, 194, 30, 1137, 28, 97, 80, 534, 474, 377, 323, 548, 401, 74, 1041, 985, 486, 176, 650, 1100, 58, 92, 302, 933, 77, 1202, 40, 990, 489, 576, 496, 528, 24, 881, 749, 1199, 13, 582, 222, 773, 20, 15, 59, 263, 1144, 471, 879, 26, 144, 792, 1172, 1038, 1213, 364, 769, 683, 81, 968, 894, 859, 171, 427, 599, 507, 694, 231, 505, 1081, 346, 449, 802, 733, 440, 1207, 541, 752, 690, 344, 1187, 147, 101, 864, 668, 1061, 379, 468, 447, 326, 43, 1163, 91, 1189, 1128, 580, 237, 236, 775, 1098, 187, 1004, 378, 546, 1166, 317, 726, 937, 281, 544, 249, 245, 543, 397, 1087, 788, 673, 333, 241, 165, 193, 226, 368, 658, 120, 519, 1103, 318, 336, 192, 69, 221, 886, 656, 1008, 148, 164, 82, 748, 570, 640, 1099, 1043, 1062, 358, 1160, 206, 529, 95, 983, 734, 717, 608, 467, 934, 964, 191, 550, 778, 301, 609, 71, 884, 996, 168, 730, 757, 1067, 1198, 244, 256, 590, 61, 633, 1188, 867, 753, 1181, 283, 1040, 131, 135, 1056, 634, 866, 149, 992, 1162, 215, 299, 898, 7, 356, 1070, 695, 383, 96, 1132, 1146, 360, 306, 986, 929, 549, 701, 948, 571, 1136, 174, 503, 667, 552, 743, 1145, 1091, 31, 260, 446, 482, 388, 461, 1034, 41, 432, 27, 768, 60, 11, 606, 594, 295, 639, 460, 771, 152, 567, 699, 820, 1134, 655, 984, 848, 393, 311, 537, 452, 1096, 692, 923, 334, 691, 654, 448, 430, 677, 755, 756, 158, 845, 361, 382, 122, 661, 279, 614, 766, 770, 419, 902, 1209, 666, 195, 555, 296, 649, 497, 315, 1015, 57, 562, 88, 1072, 1093, 797, 1025, 481, 1118, 647, 833, 660, 372, 812, 113, 451, 536, 391, 151, 312, 141, 270, 1170, 891, 426, 454, 532, 952, 450, 1055, 365, 595, 2, 1119, 619, 156, 961, 841, 1094, 1049, 826, 585, 398, 272, 416, 882, 1112, 880, 967, 744, 527, 182, 707, 330, 217, 1105, 348, 425, 928, 125, 767, 506, 1151, 1075, 939, 248, 504, 799, 1121, 520, 32, 632, 52, 827, 1179, 1089, 876, 117, 49, 758, 1182, 674, 316, 108, 708, 956, 925, 375, 798, 478, 17, 1044, 566, 601, 980, 423, 321, 1168, 508, 613, 1042, 219, 715, 834, 868, 422, 603, 878, 515, 457, 814, 1206, 680, 795, 405, 132, 340, 1190, 441, 560, 280, 1169, 558, 73, 490, 412, 338, 816, 846, 341, 1122, 126, 512, 198, 813, 42, 630, 525, 728, 181, 162, 784, 780, 935, 293, 974, 810, 927]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1869797868787988
the save name prefix for this run is:  chkpt-ID_1869797868787988_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 312
rank avg (pred): 0.511 +- 0.006
mrr vals (pred, true): 0.019, 0.298
batch losses (mrrl, rdl): 0.0, 0.0038075915

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 465
rank avg (pred): 0.446 +- 0.267
mrr vals (pred, true): 0.132, 0.042
batch losses (mrrl, rdl): 0.0, 4.5816e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 44
rank avg (pred): 0.105 +- 0.082
mrr vals (pred, true): 0.301, 0.298
batch losses (mrrl, rdl): 0.0, 3.7131e-06

Epoch over!
epoch time: 12.77

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 768
rank avg (pred): 0.434 +- 0.267
mrr vals (pred, true): 0.144, 0.045
batch losses (mrrl, rdl): 0.0, 4.7485e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 178
rank avg (pred): 0.421 +- 0.261
mrr vals (pred, true): 0.135, 0.043
batch losses (mrrl, rdl): 0.0, 2.46187e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 10
rank avg (pred): 0.108 +- 0.085
mrr vals (pred, true): 0.286, 0.256
batch losses (mrrl, rdl): 0.0, 2.8931e-06

Epoch over!
epoch time: 11.926

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 748
rank avg (pred): 0.214 +- 0.161
mrr vals (pred, true): 0.210, 0.182
batch losses (mrrl, rdl): 0.0, 2.06215e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 48
rank avg (pred): 0.084 +- 0.067
mrr vals (pred, true): 0.304, 0.296
batch losses (mrrl, rdl): 0.0, 4.8333e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 564
rank avg (pred): 0.106 +- 0.084
mrr vals (pred, true): 0.275, 0.220
batch losses (mrrl, rdl): 0.0, 1.8402e-06

Epoch over!
epoch time: 11.829

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1012
rank avg (pred): 0.425 +- 0.264
mrr vals (pred, true): 0.104, 0.054
batch losses (mrrl, rdl): 0.0, 7.2318e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1212
rank avg (pred): 0.456 +- 0.265
mrr vals (pred, true): 0.080, 0.041
batch losses (mrrl, rdl): 0.0, 4.5968e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 859
rank avg (pred): 0.416 +- 0.275
mrr vals (pred, true): 0.125, 0.043
batch losses (mrrl, rdl): 0.0, 3.25896e-05

Epoch over!
epoch time: 11.827

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 208
rank avg (pred): 0.429 +- 0.267
mrr vals (pred, true): 0.103, 0.041
batch losses (mrrl, rdl): 0.0, 2.20443e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 737
rank avg (pred): 0.242 +- 0.183
mrr vals (pred, true): 0.164, 0.104
batch losses (mrrl, rdl): 0.0, 6.91151e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 937
rank avg (pred): 0.443 +- 0.272
mrr vals (pred, true): 0.083, 0.043
batch losses (mrrl, rdl): 0.0, 4.9017e-06

Epoch over!
epoch time: 11.984

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 62
rank avg (pred): 0.110 +- 0.095
mrr vals (pred, true): 0.290, 0.258
batch losses (mrrl, rdl): 0.0098232469, 4.3602e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 482
rank avg (pred): 0.408 +- 0.151
mrr vals (pred, true): 0.044, 0.042
batch losses (mrrl, rdl): 0.0003035935, 0.0001164114

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 611
rank avg (pred): 0.419 +- 0.149
mrr vals (pred, true): 0.044, 0.049
batch losses (mrrl, rdl): 0.000370354, 5.85961e-05

Epoch over!
epoch time: 12.359

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 299
rank avg (pred): 0.119 +- 0.093
mrr vals (pred, true): 0.276, 0.291
batch losses (mrrl, rdl): 0.0020810755, 4.6745e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 496
rank avg (pred): 0.208 +- 0.161
mrr vals (pred, true): 0.245, 0.206
batch losses (mrrl, rdl): 0.0156189362, 0.0002136685

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 499
rank avg (pred): 0.227 +- 0.162
mrr vals (pred, true): 0.201, 0.192
batch losses (mrrl, rdl): 0.0008170755, 0.0002333506

Epoch over!
epoch time: 12.2

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 672
rank avg (pred): 0.418 +- 0.156
mrr vals (pred, true): 0.049, 0.045
batch losses (mrrl, rdl): 1.15792e-05, 7.73626e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 909
rank avg (pred): 0.284 +- 0.195
mrr vals (pred, true): 0.182, 0.139
batch losses (mrrl, rdl): 0.0183825213, 9.62704e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 925
rank avg (pred): 0.443 +- 0.144
mrr vals (pred, true): 0.043, 0.043
batch losses (mrrl, rdl): 0.0004455703, 5.49359e-05

Epoch over!
epoch time: 12.157

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1067
rank avg (pred): 0.066 +- 0.049
mrr vals (pred, true): 0.324, 0.336
batch losses (mrrl, rdl): 0.0016118609, 2.52415e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 498
rank avg (pred): 0.239 +- 0.169
mrr vals (pred, true): 0.217, 0.188
batch losses (mrrl, rdl): 0.0088096242, 0.0003264691

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 907
rank avg (pred): 0.285 +- 0.202
mrr vals (pred, true): 0.222, 0.220
batch losses (mrrl, rdl): 4.8458e-05, 0.0004602484

Epoch over!
epoch time: 12.015

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 477
rank avg (pred): 0.439 +- 0.150
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 8.84951e-05, 5.99819e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 746
rank avg (pred): 0.177 +- 0.126
mrr vals (pred, true): 0.249, 0.267
batch losses (mrrl, rdl): 0.0031553649, 4.10222e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 528
rank avg (pred): 0.231 +- 0.161
mrr vals (pred, true): 0.229, 0.217
batch losses (mrrl, rdl): 0.001466498, 0.0003606932

Epoch over!
epoch time: 12.14

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1072
rank avg (pred): 0.063 +- 0.044
mrr vals (pred, true): 0.326, 0.309
batch losses (mrrl, rdl): 0.0027434574, 3.81774e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 187
rank avg (pred): 0.464 +- 0.126
mrr vals (pred, true): 0.035, 0.044
batch losses (mrrl, rdl): 0.0023444267, 5.20619e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1114
rank avg (pred): 0.436 +- 0.150
mrr vals (pred, true): 0.052, 0.047
batch losses (mrrl, rdl): 3.80564e-05, 4.94465e-05

Epoch over!
epoch time: 12.15

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 818
rank avg (pred): 0.385 +- 0.191
mrr vals (pred, true): 0.113, 0.110
batch losses (mrrl, rdl): 0.0001032942, 0.0002694806

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1093
rank avg (pred): 0.440 +- 0.147
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 9.1741e-06, 4.73979e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1008
rank avg (pred): 0.442 +- 0.142
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.000160309, 4.65746e-05

Epoch over!
epoch time: 12.161

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 501
rank avg (pred): 0.262 +- 0.176
mrr vals (pred, true): 0.224, 0.203
batch losses (mrrl, rdl): 0.0045483983, 0.0005361591

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 341
rank avg (pred): 0.433 +- 0.148
mrr vals (pred, true): 0.054, 0.060
batch losses (mrrl, rdl): 0.0001957971, 6.45553e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 821
rank avg (pred): 0.196 +- 0.136
mrr vals (pred, true): 0.256, 0.206
batch losses (mrrl, rdl): 0.0246367529, 2.28092e-05

Epoch over!
epoch time: 12.177

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1181
rank avg (pred): 0.452 +- 0.131
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.00083838, 4.72163e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 511
rank avg (pred): 0.289 +- 0.184
mrr vals (pred, true): 0.205, 0.190
batch losses (mrrl, rdl): 0.0022878116, 0.0006398241

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 789
rank avg (pred): 0.454 +- 0.129
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.0012980367, 4.61419e-05

Epoch over!
epoch time: 12.212

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 879
rank avg (pred): 0.436 +- 0.148
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 6.272e-07, 5.35344e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 703
rank avg (pred): 0.448 +- 0.135
mrr vals (pred, true): 0.042, 0.044
batch losses (mrrl, rdl): 0.0007217508, 4.64144e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 471
rank avg (pred): 0.427 +- 0.156
mrr vals (pred, true): 0.058, 0.043
batch losses (mrrl, rdl): 0.0006710116, 5.98922e-05

Epoch over!
epoch time: 12.115

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.433 +- 0.150
mrr vals (pred, true): 0.053, 0.049

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   34 	     0 	 0.05270 	 0.03961 	 ~...
   61 	     1 	 0.05537 	 0.03969 	 ~...
    0 	     2 	 0.04853 	 0.04137 	 ~...
   26 	     3 	 0.05197 	 0.04154 	 ~...
   63 	     4 	 0.05571 	 0.04162 	 ~...
   75 	     5 	 0.05859 	 0.04188 	 ~...
   27 	     6 	 0.05201 	 0.04189 	 ~...
   58 	     7 	 0.05498 	 0.04206 	 ~...
   55 	     8 	 0.05488 	 0.04217 	 ~...
    6 	     9 	 0.05024 	 0.04217 	 ~...
    9 	    10 	 0.05054 	 0.04222 	 ~...
   19 	    11 	 0.05136 	 0.04233 	 ~...
   22 	    12 	 0.05186 	 0.04234 	 ~...
   32 	    13 	 0.05264 	 0.04259 	 ~...
   28 	    14 	 0.05218 	 0.04261 	 ~...
   39 	    15 	 0.05307 	 0.04271 	 ~...
   10 	    16 	 0.05069 	 0.04273 	 ~...
   47 	    17 	 0.05401 	 0.04285 	 ~...
    1 	    18 	 0.04912 	 0.04291 	 ~...
   51 	    19 	 0.05447 	 0.04292 	 ~...
   20 	    20 	 0.05152 	 0.04311 	 ~...
   21 	    21 	 0.05172 	 0.04323 	 ~...
    4 	    22 	 0.04989 	 0.04362 	 ~...
   23 	    23 	 0.05187 	 0.04367 	 ~...
   13 	    24 	 0.05090 	 0.04369 	 ~...
   24 	    25 	 0.05188 	 0.04371 	 ~...
   41 	    26 	 0.05312 	 0.04383 	 ~...
    5 	    27 	 0.05003 	 0.04389 	 ~...
    2 	    28 	 0.04938 	 0.04404 	 ~...
   70 	    29 	 0.05661 	 0.04406 	 ~...
   31 	    30 	 0.05254 	 0.04409 	 ~...
   53 	    31 	 0.05477 	 0.04412 	 ~...
   71 	    32 	 0.05710 	 0.04416 	 ~...
   66 	    33 	 0.05596 	 0.04431 	 ~...
   56 	    34 	 0.05494 	 0.04443 	 ~...
   50 	    35 	 0.05434 	 0.04448 	 ~...
   12 	    36 	 0.05089 	 0.04463 	 ~...
   15 	    37 	 0.05110 	 0.04495 	 ~...
   49 	    38 	 0.05430 	 0.04503 	 ~...
   64 	    39 	 0.05571 	 0.04535 	 ~...
   37 	    40 	 0.05282 	 0.04546 	 ~...
   30 	    41 	 0.05241 	 0.04547 	 ~...
    7 	    42 	 0.05025 	 0.04551 	 ~...
   16 	    43 	 0.05120 	 0.04582 	 ~...
   73 	    44 	 0.05755 	 0.04582 	 ~...
   76 	    45 	 0.05907 	 0.04588 	 ~...
   48 	    46 	 0.05429 	 0.04592 	 ~...
   18 	    47 	 0.05135 	 0.04598 	 ~...
   67 	    48 	 0.05601 	 0.04612 	 ~...
   52 	    49 	 0.05454 	 0.04628 	 ~...
   68 	    50 	 0.05620 	 0.04629 	 ~...
    8 	    51 	 0.05044 	 0.04653 	 ~...
   44 	    52 	 0.05361 	 0.04700 	 ~...
   29 	    53 	 0.05228 	 0.04713 	 ~...
   17 	    54 	 0.05127 	 0.04715 	 ~...
   54 	    55 	 0.05481 	 0.04830 	 ~...
   38 	    56 	 0.05292 	 0.04874 	 ~...
   14 	    57 	 0.05094 	 0.04874 	 ~...
   45 	    58 	 0.05363 	 0.04905 	 ~...
   25 	    59 	 0.05190 	 0.04910 	 ~...
   43 	    60 	 0.05320 	 0.04917 	 ~...
   35 	    61 	 0.05271 	 0.04935 	 ~...
   42 	    62 	 0.05312 	 0.04946 	 ~...
   59 	    63 	 0.05505 	 0.05020 	 ~...
   65 	    64 	 0.05573 	 0.05030 	 ~...
   57 	    65 	 0.05497 	 0.05037 	 ~...
   69 	    66 	 0.05642 	 0.05077 	 ~...
   11 	    67 	 0.05089 	 0.05129 	 ~...
   62 	    68 	 0.05541 	 0.05187 	 ~...
   77 	    69 	 0.05974 	 0.05223 	 ~...
   46 	    70 	 0.05387 	 0.05241 	 ~...
   72 	    71 	 0.05715 	 0.05269 	 ~...
   33 	    72 	 0.05267 	 0.05320 	 ~...
   60 	    73 	 0.05508 	 0.05336 	 ~...
   74 	    74 	 0.05777 	 0.05416 	 ~...
    3 	    75 	 0.04982 	 0.05451 	 ~...
   40 	    76 	 0.05311 	 0.05456 	 ~...
   36 	    77 	 0.05272 	 0.05586 	 ~...
   78 	    78 	 0.12392 	 0.09735 	 ~...
   79 	    79 	 0.17481 	 0.13306 	 m..s
   84 	    80 	 0.21343 	 0.17582 	 m..s
   80 	    81 	 0.20166 	 0.19682 	 ~...
   80 	    82 	 0.20166 	 0.19718 	 ~...
   89 	    83 	 0.21747 	 0.20159 	 ~...
   85 	    84 	 0.21361 	 0.20505 	 ~...
   83 	    85 	 0.21092 	 0.20813 	 ~...
   90 	    86 	 0.21815 	 0.21008 	 ~...
   88 	    87 	 0.21653 	 0.21167 	 ~...
   92 	    88 	 0.21902 	 0.21326 	 ~...
   86 	    89 	 0.21538 	 0.21694 	 ~...
   82 	    90 	 0.20956 	 0.21918 	 ~...
   87 	    91 	 0.21595 	 0.22906 	 ~...
   91 	    92 	 0.21841 	 0.23195 	 ~...
  113 	    93 	 0.28802 	 0.24849 	 m..s
   95 	    94 	 0.25961 	 0.25665 	 ~...
   97 	    95 	 0.26980 	 0.26272 	 ~...
  108 	    96 	 0.27955 	 0.26309 	 ~...
   98 	    97 	 0.27066 	 0.26618 	 ~...
   94 	    98 	 0.24616 	 0.26867 	 ~...
  111 	    99 	 0.28381 	 0.27017 	 ~...
  107 	   100 	 0.27948 	 0.27108 	 ~...
  105 	   101 	 0.27882 	 0.27120 	 ~...
  109 	   102 	 0.28264 	 0.27167 	 ~...
   93 	   103 	 0.24365 	 0.27200 	 ~...
  106 	   104 	 0.27896 	 0.27458 	 ~...
  102 	   105 	 0.27713 	 0.27906 	 ~...
  103 	   106 	 0.27755 	 0.28138 	 ~...
  115 	   107 	 0.29628 	 0.28884 	 ~...
   96 	   108 	 0.26755 	 0.29005 	 ~...
   99 	   109 	 0.27183 	 0.29354 	 ~...
  112 	   110 	 0.28660 	 0.29913 	 ~...
  101 	   111 	 0.27450 	 0.30045 	 ~...
  114 	   112 	 0.29225 	 0.30063 	 ~...
  110 	   113 	 0.28285 	 0.30319 	 ~...
  100 	   114 	 0.27348 	 0.30518 	 m..s
  117 	   115 	 0.35191 	 0.31226 	 m..s
  104 	   116 	 0.27822 	 0.31448 	 m..s
  116 	   117 	 0.35188 	 0.32833 	 ~...
  120 	   118 	 0.38920 	 0.33103 	 m..s
  119 	   119 	 0.37623 	 0.33146 	 m..s
  118 	   120 	 0.36598 	 0.35734 	 ~...
==========================================
r_mrr = 0.9920650720596313
r2_mrr = 0.9807378649711609
spearmanr_mrr@5 = 0.8803490400314331
spearmanr_mrr@10 = 0.9173352718353271
spearmanr_mrr@50 = 0.9848889708518982
spearmanr_mrr@100 = 0.995215654373169
spearmanr_mrr@All = 0.9957501888275146
==========================================
test time: 0.467
Done Testing dataset Kinships
total time taken: 189.8112473487854
training time taken: 182.5805103778839
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9921)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9807)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.8803)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9173)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9849)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9952)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9958)}}, 'test_loss': {'TransE': {'Kinships': 0.2798335141415009}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 9679830691813768
Starting TWIG!
Loading datasets
Loading TransE...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1177, 1124, 1156, 596, 908, 968, 151, 288, 222, 320, 879, 71, 1097, 420, 1049, 904, 376, 670, 976, 59, 971, 146, 422, 822, 686, 805, 1179, 1046, 203, 902, 560, 574, 1213, 872, 115, 571, 375, 354, 147, 223, 857, 1188, 128, 67, 1070, 1130, 983, 60, 511, 544, 132, 250, 416, 276, 395, 27, 285, 568, 552, 591, 733, 549, 674, 703, 397, 638, 1161, 442, 78, 839, 349, 600, 569, 482, 374, 538, 326, 1146, 1037, 602, 874, 491, 1076, 835, 304, 832, 383, 1170, 1044, 300, 119, 371, 437, 96, 825, 720, 291, 993, 790, 318, 948, 414, 601, 542, 262, 550, 943, 745, 346, 32, 58, 637, 33, 193, 972, 764, 1073, 387, 396, 1022, 1157]
valid_ids (0): []
train_ids (1094): [951, 780, 900, 1103, 470, 333, 775, 294, 364, 880, 583, 767, 995, 366, 485, 1133, 803, 697, 402, 946, 595, 867, 1206, 689, 682, 1144, 1209, 1064, 233, 1152, 124, 1104, 380, 1061, 915, 415, 748, 743, 363, 1095, 357, 315, 413, 779, 118, 740, 1, 208, 650, 236, 24, 807, 737, 516, 1159, 1114, 473, 1172, 194, 990, 759, 950, 1155, 1138, 772, 960, 228, 1154, 590, 350, 1047, 1185, 688, 136, 17, 8, 980, 94, 786, 35, 478, 214, 79, 69, 890, 246, 488, 1027, 113, 384, 480, 706, 14, 244, 529, 710, 1084, 625, 114, 356, 88, 612, 1212, 741, 580, 582, 599, 752, 570, 417, 575, 0, 158, 160, 64, 1096, 75, 152, 234, 837, 373, 669, 907, 263, 271, 289, 514, 2, 418, 82, 506, 979, 455, 210, 99, 1067, 668, 1062, 445, 273, 827, 1151, 309, 257, 308, 46, 252, 566, 1071, 339, 921, 444, 731, 1136, 1189, 1182, 217, 524, 917, 186, 957, 1078, 57, 409, 66, 185, 1207, 451, 859, 896, 428, 45, 817, 765, 1134, 997, 665, 799, 588, 713, 351, 86, 781, 241, 213, 42, 1112, 852, 1186, 751, 1023, 385, 695, 794, 277, 715, 711, 44, 535, 942, 801, 1183, 704, 476, 1034, 259, 965, 808, 533, 792, 29, 988, 548, 267, 148, 1056, 172, 358, 28, 1092, 1148, 256, 1029, 1201, 831, 1098, 607, 547, 1202, 657, 238, 197, 165, 541, 581, 526, 848, 1033, 307, 1140, 265, 1042, 130, 104, 818, 83, 1164, 796, 986, 812, 592, 314, 646, 398, 632, 347, 1025, 609, 279, 709, 199, 435, 875, 945, 520, 92, 937, 360, 1091, 742, 378, 1171, 1055, 487, 40, 935, 73, 1142, 139, 1077, 1063, 405, 1032, 174, 303, 523, 787, 249, 474, 343, 429, 850, 809, 159, 873, 275, 778, 1020, 654, 206, 725, 851, 753, 446, 153, 441, 919, 348, 838, 135, 1173, 994, 826, 131, 1019, 763, 19, 954, 127, 532, 620, 386, 776, 1162, 565, 1191, 192, 112, 498, 530, 1153, 934, 1135, 618, 175, 209, 628, 286, 701, 20, 426, 1060, 608, 394, 678, 811, 564, 149, 297, 785, 157, 95, 166, 593, 388, 1126, 129, 885, 317, 1002, 758, 705, 109, 1143, 189, 1122, 352, 963, 52, 525, 50, 1106, 649, 31, 1163, 369, 522, 1200, 43, 920, 509, 856, 145, 598, 847, 683, 164, 707, 195, 992, 1053, 1093, 782, 1102, 510, 540, 829, 627, 125, 939, 179, 558, 681, 231, 264, 998, 450, 211, 278, 853, 693, 141, 658, 1030, 1058, 16, 866, 224, 1190, 777, 865, 431, 201, 749, 475, 138, 673, 340, 505, 621, 955, 298, 666, 361, 200, 739, 1045, 63, 15, 421, 68, 245, 163, 845, 773, 1088, 101, 1080, 708, 161, 48, 331, 1016, 1205, 98, 292, 918, 723, 393, 133, 824, 268, 1111, 1101, 302, 841, 727, 225, 329, 1011, 977, 334, 864, 221, 495, 282, 586, 814, 311, 270, 1079, 472, 1015, 528, 962, 483, 248, 6, 589, 551, 1039, 432, 54, 181, 1072, 806, 1013, 162, 176, 984, 1194, 77, 577, 1083, 928, 1038, 672, 458, 901, 755, 25, 36, 1204, 519, 468, 287, 447, 37, 750, 168, 377, 1131, 89, 1149, 513, 648, 359, 239, 1057, 337, 906, 1147, 486, 728, 736, 881, 336, 187, 182, 154, 1113, 970, 820, 605, 362, 675, 255, 55, 215, 382, 220, 655, 1166, 137, 714, 501, 1203, 408, 721, 912, 1005, 1006, 389, 462, 1109, 1160, 894, 93, 103, 878, 843, 345, 85, 372, 958, 461, 407, 1074, 11, 1196, 768, 830, 576, 1099, 585, 467, 332, 536, 891, 321, 512, 640, 893, 999, 554, 338, 633, 1043, 521, 121, 770, 367, 481, 527, 626, 110, 242, 207, 889, 793, 1007, 237, 761, 611, 296, 281, 330, 771, 218, 229, 471, 499, 272, 1187, 518, 579, 205, 464, 440, 927, 406, 30, 760, 1169, 293, 854, 1054, 253, 117, 892, 327, 717, 846, 313, 925, 634, 973, 235, 619, 660, 687, 274, 784, 517, 1116, 563, 41, 1125, 84, 684, 1107, 1021, 876, 493, 1208, 546, 269, 539, 614, 1059, 860, 622, 774, 623, 617, 243, 284, 1087, 804, 1090, 1100, 494, 1094, 310, 38, 430, 479, 819, 457, 21, 567, 810, 123, 1210, 1035, 4, 1069, 1178, 987, 72, 232, 1127, 813, 1004, 328, 23, 1017, 863, 1121, 802, 1012, 316, 9, 910, 1028, 1041, 12, 578, 718, 929, 691, 465, 365, 1105, 1132, 1108, 562, 322, 700, 769, 1214, 531, 676, 1003, 463, 490, 65, 1066, 449, 766, 884, 722, 1158, 306, 956, 883, 1031, 216, 643, 1120, 62, 433, 368, 100, 436, 381, 738, 923, 651, 855, 401, 754, 871, 434, 261, 212, 886, 1052, 1082, 677, 299, 438, 120, 897, 1137, 106, 858, 844, 1081, 325, 469, 941, 940, 319, 604, 1010, 91, 423, 989, 1180, 425, 692, 254, 1184, 1141, 730, 47, 1181, 90, 1050, 895, 107, 557, 641, 1008, 219, 823, 543, 51, 815, 150, 724, 190, 969, 134, 559, 170, 140, 1009, 1123, 797, 204, 370, 696, 1195, 290, 1117, 198, 644, 22, 694, 909, 453, 122, 905, 266, 639, 959, 922, 828, 645, 74, 1119, 18, 926, 656, 10, 964, 419, 967, 662, 553, 653, 353, 953, 126, 783, 1168, 484, 76, 791, 1075, 459, 762, 545, 400, 573, 903, 911, 610, 180, 877, 702, 312, 789, 1115, 301, 561, 944, 931, 240, 3, 636, 1192, 226, 492, 869, 230, 53, 335, 156, 833, 39, 975, 642, 251, 111, 454, 556, 1048, 982, 635, 404, 887, 26, 667, 1198, 795, 712, 1089, 424, 659, 816, 898, 1068, 344, 34, 504, 1040, 489, 7, 756, 439, 1175, 978, 836, 191, 477, 981, 116, 616, 924, 403, 143, 729, 1128, 1211, 991, 196, 914, 169, 1085, 584, 664, 679, 932, 448, 70, 515, 800, 930, 324, 183, 280, 821, 295, 81, 671, 690, 699, 1197, 1150, 173, 502, 882, 142, 500, 56, 508, 341, 1145, 757, 1051, 497, 355, 283, 849, 108, 735, 537, 615, 399, 507, 390, 933, 456, 177, 936, 961, 171, 572, 888, 716, 49, 13, 594, 105, 985, 952, 870, 555, 661, 260, 966, 1000, 938, 606, 410, 167, 247, 411, 613, 747, 460, 342, 1167, 746, 323, 842, 974, 1129, 102, 913, 534, 1165, 685, 680, 947, 496, 652, 834, 996, 188, 1001, 1193, 412, 178, 427, 1086, 597, 663, 1014, 734, 624, 452, 392, 788, 798, 1026, 726, 391, 5, 1176, 443, 1110, 629, 97, 899, 647, 80, 227, 202, 840, 305, 862, 630, 1018, 87, 1024, 1065, 184, 144, 258, 155, 868, 732, 1036, 1174, 587, 744, 861, 1199, 1139, 949, 503, 61, 379, 631, 698, 466, 603, 719, 1118, 916]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4791485652718769
the save name prefix for this run is:  chkpt-ID_4791485652718769_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'TransE': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 890
rank avg (pred): 0.533 +- 0.005
mrr vals (pred, true): 0.018, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001335256

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 229
rank avg (pred): 0.431 +- 0.241
mrr vals (pred, true): 0.142, 0.041
batch losses (mrrl, rdl): 0.0, 1.76954e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 164
rank avg (pred): 0.424 +- 0.261
mrr vals (pred, true): 0.150, 0.045
batch losses (mrrl, rdl): 0.0, 7.3118e-06

Epoch over!
epoch time: 12.223

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 263
rank avg (pred): 0.110 +- 0.079
mrr vals (pred, true): 0.265, 0.276
batch losses (mrrl, rdl): 0.0, 5.9741e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 20
rank avg (pred): 0.109 +- 0.084
mrr vals (pred, true): 0.270, 0.277
batch losses (mrrl, rdl): 0.0, 3.6765e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 924
rank avg (pred): 0.448 +- 0.276
mrr vals (pred, true): 0.112, 0.042
batch losses (mrrl, rdl): 0.0, 4.8988e-06

Epoch over!
epoch time: 11.866

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 950
rank avg (pred): 0.456 +- 0.268
mrr vals (pred, true): 0.101, 0.045
batch losses (mrrl, rdl): 0.0, 3.6756e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 837
rank avg (pred): 0.428 +- 0.267
mrr vals (pred, true): 0.105, 0.042
batch losses (mrrl, rdl): 0.0, 2.00565e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 840
rank avg (pred): 0.442 +- 0.261
mrr vals (pred, true): 0.081, 0.045
batch losses (mrrl, rdl): 0.0, 3.7126e-06

Epoch over!
epoch time: 11.838

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 436
rank avg (pred): 0.456 +- 0.255
mrr vals (pred, true): 0.067, 0.045
batch losses (mrrl, rdl): 0.0, 3.3284e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 186
rank avg (pred): 0.452 +- 0.274
mrr vals (pred, true): 0.077, 0.044
batch losses (mrrl, rdl): 0.0, 2.7763e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 160
rank avg (pred): 0.409 +- 0.269
mrr vals (pred, true): 0.093, 0.052
batch losses (mrrl, rdl): 0.0, 4.2046e-06

Epoch over!
epoch time: 11.855

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 401
rank avg (pred): 0.432 +- 0.259
mrr vals (pred, true): 0.069, 0.060
batch losses (mrrl, rdl): 0.0, 2.25142e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 231
rank avg (pred): 0.451 +- 0.256
mrr vals (pred, true): 0.054, 0.043
batch losses (mrrl, rdl): 0.0, 1.2544e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 118
rank avg (pred): 0.436 +- 0.268
mrr vals (pred, true): 0.062, 0.051
batch losses (mrrl, rdl): 0.0, 2.2103e-06

Epoch over!
epoch time: 11.96

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 251
rank avg (pred): 0.139 +- 0.115
mrr vals (pred, true): 0.205, 0.290
batch losses (mrrl, rdl): 0.0728701502, 2.17238e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 888
rank avg (pred): 0.510 +- 0.169
mrr vals (pred, true): 0.040, 0.044
batch losses (mrrl, rdl): 0.0009891439, 7.52211e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 648
rank avg (pred): 0.500 +- 0.161
mrr vals (pred, true): 0.047, 0.044
batch losses (mrrl, rdl): 0.000115377, 6.02009e-05

Epoch over!
epoch time: 12.294

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 373
rank avg (pred): 0.378 +- 0.127
mrr vals (pred, true): 0.058, 0.053
batch losses (mrrl, rdl): 0.0006686724, 0.0001031679

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 212
rank avg (pred): 0.441 +- 0.139
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 9.03485e-05, 5.86182e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 840
rank avg (pred): 0.552 +- 0.161
mrr vals (pred, true): 0.042, 0.045
batch losses (mrrl, rdl): 0.0006192359, 0.0001993813

Epoch over!
epoch time: 12.22

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 980
rank avg (pred): 0.106 +- 0.083
mrr vals (pred, true): 0.316, 0.347
batch losses (mrrl, rdl): 0.0098549752, 4.0126e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 784
rank avg (pred): 0.476 +- 0.158
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.0003880919, 3.66961e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 483
rank avg (pred): 0.447 +- 0.138
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.59071e-05, 4.3159e-05

Epoch over!
epoch time: 12.291

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 36
rank avg (pred): 0.114 +- 0.085
mrr vals (pred, true): 0.294, 0.269
batch losses (mrrl, rdl): 0.006062807, 8.5539e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1080
rank avg (pred): 0.444 +- 0.151
mrr vals (pred, true): 0.054, 0.053
batch losses (mrrl, rdl): 0.0001660655, 4.52675e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1178
rank avg (pred): 0.340 +- 0.123
mrr vals (pred, true): 0.063, 0.046
batch losses (mrrl, rdl): 0.0017943023, 0.0002670398

Epoch over!
epoch time: 11.988

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1113
rank avg (pred): 0.617 +- 0.182
mrr vals (pred, true): 0.044, 0.043
batch losses (mrrl, rdl): 0.0003256521, 0.0004257714

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 541
rank avg (pred): 0.267 +- 0.188
mrr vals (pred, true): 0.222, 0.210
batch losses (mrrl, rdl): 0.0014374534, 0.0005311606

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 394
rank avg (pred): 0.444 +- 0.155
mrr vals (pred, true): 0.053, 0.051
batch losses (mrrl, rdl): 0.000113348, 3.63082e-05

Epoch over!
epoch time: 12.006

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 257
rank avg (pred): 0.132 +- 0.094
mrr vals (pred, true): 0.273, 0.305
batch losses (mrrl, rdl): 0.010121786, 1.12079e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 35
rank avg (pred): 0.110 +- 0.079
mrr vals (pred, true): 0.295, 0.294
batch losses (mrrl, rdl): 1.23306e-05, 4.5427e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 391
rank avg (pred): 0.462 +- 0.160
mrr vals (pred, true): 0.043, 0.059
batch losses (mrrl, rdl): 0.0004903645, 8.87431e-05

Epoch over!
epoch time: 12.51

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 235
rank avg (pred): 0.546 +- 0.149
mrr vals (pred, true): 0.042, 0.048
batch losses (mrrl, rdl): 0.000625781, 0.0002036144

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 527
rank avg (pred): 0.295 +- 0.205
mrr vals (pred, true): 0.220, 0.227
batch losses (mrrl, rdl): 0.0004637095, 0.0006715629

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 467
rank avg (pred): 0.495 +- 0.162
mrr vals (pred, true): 0.042, 0.044
batch losses (mrrl, rdl): 0.0006879126, 4.96234e-05

Epoch over!
epoch time: 12.003

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 461
rank avg (pred): 0.471 +- 0.161
mrr vals (pred, true): 0.053, 0.044
batch losses (mrrl, rdl): 8.85228e-05, 3.16434e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 74
rank avg (pred): 0.142 +- 0.102
mrr vals (pred, true): 0.290, 0.291
batch losses (mrrl, rdl): 4.5461e-06, 2.84769e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 419
rank avg (pred): 0.414 +- 0.189
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 2.84369e-05, 7.49501e-05

Epoch over!
epoch time: 12.05

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 467
rank avg (pred): 0.528 +- 0.165
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.0008617475, 0.0001087787

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 133
rank avg (pred): 0.473 +- 0.199
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 1.709e-07, 2.9932e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 1212
rank avg (pred): 0.479 +- 0.269
mrr vals (pred, true): 0.042, 0.041
batch losses (mrrl, rdl): 0.0006109006, 1.47315e-05

Epoch over!
epoch time: 12.099

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 705
rank avg (pred): 0.514 +- 0.172
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 9.04601e-05, 8.10936e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 821
rank avg (pred): 0.229 +- 0.156
mrr vals (pred, true): 0.234, 0.206
batch losses (mrrl, rdl): 0.0076850774, 2.57137e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, Kinships, run 2.1, exp 102
rank avg (pred): 0.436 +- 0.267
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0001256495, 1.01313e-05

Epoch over!
epoch time: 12.183

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.469 +- 0.261
mrr vals (pred, true): 0.053, 0.043

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   26 	     0 	 0.05432 	 0.03958 	 ~...
   68 	     1 	 0.05923 	 0.04062 	 ~...
   62 	     2 	 0.05809 	 0.04150 	 ~...
    2 	     3 	 0.05218 	 0.04151 	 ~...
   25 	     4 	 0.05416 	 0.04204 	 ~...
   61 	     5 	 0.05806 	 0.04206 	 ~...
   15 	     6 	 0.05360 	 0.04207 	 ~...
   55 	     7 	 0.05728 	 0.04209 	 ~...
   49 	     8 	 0.05670 	 0.04214 	 ~...
   70 	     9 	 0.06058 	 0.04231 	 ~...
   57 	    10 	 0.05765 	 0.04238 	 ~...
   13 	    11 	 0.05350 	 0.04245 	 ~...
   53 	    12 	 0.05712 	 0.04260 	 ~...
   56 	    13 	 0.05728 	 0.04261 	 ~...
   21 	    14 	 0.05390 	 0.04280 	 ~...
    9 	    15 	 0.05310 	 0.04288 	 ~...
   41 	    16 	 0.05598 	 0.04296 	 ~...
   54 	    17 	 0.05716 	 0.04303 	 ~...
   23 	    18 	 0.05411 	 0.04304 	 ~...
   17 	    19 	 0.05378 	 0.04307 	 ~...
   66 	    20 	 0.05856 	 0.04322 	 ~...
    1 	    21 	 0.05213 	 0.04353 	 ~...
   27 	    22 	 0.05443 	 0.04354 	 ~...
   30 	    23 	 0.05463 	 0.04362 	 ~...
   52 	    24 	 0.05710 	 0.04363 	 ~...
   72 	    25 	 0.06403 	 0.04369 	 ~...
    4 	    26 	 0.05246 	 0.04381 	 ~...
   35 	    27 	 0.05509 	 0.04382 	 ~...
   24 	    28 	 0.05413 	 0.04383 	 ~...
   20 	    29 	 0.05379 	 0.04401 	 ~...
   60 	    30 	 0.05805 	 0.04409 	 ~...
   14 	    31 	 0.05351 	 0.04418 	 ~...
   11 	    32 	 0.05342 	 0.04422 	 ~...
   69 	    33 	 0.05924 	 0.04426 	 ~...
    7 	    34 	 0.05290 	 0.04432 	 ~...
   59 	    35 	 0.05777 	 0.04436 	 ~...
   12 	    36 	 0.05349 	 0.04452 	 ~...
   43 	    37 	 0.05617 	 0.04480 	 ~...
    0 	    38 	 0.05179 	 0.04510 	 ~...
   51 	    39 	 0.05700 	 0.04546 	 ~...
   22 	    40 	 0.05393 	 0.04570 	 ~...
   29 	    41 	 0.05460 	 0.04578 	 ~...
   39 	    42 	 0.05589 	 0.04606 	 ~...
   40 	    43 	 0.05594 	 0.04609 	 ~...
   16 	    44 	 0.05366 	 0.04616 	 ~...
    3 	    45 	 0.05222 	 0.04649 	 ~...
   31 	    46 	 0.05470 	 0.04656 	 ~...
    6 	    47 	 0.05269 	 0.04694 	 ~...
   37 	    48 	 0.05544 	 0.04813 	 ~...
   58 	    49 	 0.05774 	 0.04825 	 ~...
   38 	    50 	 0.05588 	 0.04835 	 ~...
   10 	    51 	 0.05317 	 0.04935 	 ~...
   42 	    52 	 0.05599 	 0.04938 	 ~...
   18 	    53 	 0.05378 	 0.05004 	 ~...
   50 	    54 	 0.05682 	 0.05008 	 ~...
   19 	    55 	 0.05378 	 0.05072 	 ~...
   64 	    56 	 0.05844 	 0.05093 	 ~...
   71 	    57 	 0.06232 	 0.05143 	 ~...
   65 	    58 	 0.05848 	 0.05148 	 ~...
   63 	    59 	 0.05812 	 0.05171 	 ~...
   67 	    60 	 0.05903 	 0.05198 	 ~...
   45 	    61 	 0.05636 	 0.05265 	 ~...
   32 	    62 	 0.05475 	 0.05402 	 ~...
   44 	    63 	 0.05627 	 0.05403 	 ~...
   36 	    64 	 0.05512 	 0.05417 	 ~...
   34 	    65 	 0.05497 	 0.05444 	 ~...
   47 	    66 	 0.05662 	 0.05445 	 ~...
    5 	    67 	 0.05264 	 0.05480 	 ~...
   48 	    68 	 0.05663 	 0.05481 	 ~...
   33 	    69 	 0.05496 	 0.05539 	 ~...
   46 	    70 	 0.05661 	 0.05728 	 ~...
    8 	    71 	 0.05296 	 0.05791 	 ~...
   28 	    72 	 0.05450 	 0.05818 	 ~...
   73 	    73 	 0.06687 	 0.06201 	 ~...
   74 	    74 	 0.20214 	 0.14461 	 m..s
   79 	    75 	 0.23314 	 0.19000 	 m..s
   75 	    76 	 0.20244 	 0.19813 	 ~...
   88 	    77 	 0.24299 	 0.19838 	 m..s
   80 	    78 	 0.23359 	 0.20583 	 ~...
   85 	    79 	 0.23809 	 0.20910 	 ~...
   76 	    80 	 0.20517 	 0.21355 	 ~...
   84 	    81 	 0.23601 	 0.21950 	 ~...
   77 	    82 	 0.22910 	 0.22278 	 ~...
   82 	    83 	 0.23516 	 0.22737 	 ~...
   87 	    84 	 0.24119 	 0.23255 	 ~...
   86 	    85 	 0.23944 	 0.23260 	 ~...
   83 	    86 	 0.23537 	 0.24035 	 ~...
   95 	    87 	 0.29331 	 0.24849 	 m..s
   81 	    88 	 0.23423 	 0.25272 	 ~...
   94 	    89 	 0.28428 	 0.25812 	 ~...
   78 	    90 	 0.23229 	 0.25856 	 ~...
   92 	    91 	 0.27636 	 0.26043 	 ~...
   93 	    92 	 0.27843 	 0.26140 	 ~...
   91 	    93 	 0.26726 	 0.26225 	 ~...
  105 	    94 	 0.30420 	 0.26262 	 m..s
   89 	    95 	 0.25094 	 0.26867 	 ~...
   97 	    96 	 0.29873 	 0.26891 	 ~...
   90 	    97 	 0.25235 	 0.27208 	 ~...
  109 	    98 	 0.30713 	 0.27463 	 m..s
  100 	    99 	 0.30095 	 0.27505 	 ~...
  108 	   100 	 0.30663 	 0.27807 	 ~...
   99 	   101 	 0.30040 	 0.28054 	 ~...
  112 	   102 	 0.31101 	 0.28181 	 ~...
   96 	   103 	 0.29604 	 0.28323 	 ~...
  113 	   104 	 0.31201 	 0.28547 	 ~...
  106 	   105 	 0.30644 	 0.29207 	 ~...
  101 	   106 	 0.30164 	 0.29265 	 ~...
  104 	   107 	 0.30408 	 0.29328 	 ~...
  117 	   108 	 0.35147 	 0.29805 	 m..s
  107 	   109 	 0.30656 	 0.30314 	 ~...
  119 	   110 	 0.35471 	 0.30577 	 m..s
  103 	   111 	 0.30226 	 0.30791 	 ~...
  111 	   112 	 0.30812 	 0.30816 	 ~...
   98 	   113 	 0.29963 	 0.30820 	 ~...
  114 	   114 	 0.33698 	 0.31226 	 ~...
  110 	   115 	 0.30754 	 0.32242 	 ~...
  102 	   116 	 0.30172 	 0.33317 	 m..s
  118 	   117 	 0.35383 	 0.33857 	 ~...
  115 	   118 	 0.34103 	 0.34148 	 ~...
  120 	   119 	 0.35870 	 0.34190 	 ~...
  116 	   120 	 0.34375 	 0.34654 	 ~...
==========================================
r_mrr = 0.9924378991127014
r2_mrr = 0.9749658107757568
spearmanr_mrr@5 = 0.9910449981689453
spearmanr_mrr@10 = 0.9347921013832092
spearmanr_mrr@50 = 0.9842185378074646
spearmanr_mrr@100 = 0.9972442984580994
spearmanr_mrr@All = 0.9975923895835876
==========================================
test time: 0.581
Done Testing dataset Kinships
total time taken: 188.94020795822144
training time taken: 182.0531029701233
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'Kinships': tensor(0.9924)}}, 'r2_mrr': {'TransE': {'Kinships': tensor(0.9750)}}, 'spearmanr_mrr@5': {'TransE': {'Kinships': tensor(0.9910)}}, 'spearmanr_mrr@10': {'TransE': {'Kinships': tensor(0.9348)}}, 'spearmanr_mrr@50': {'TransE': {'Kinships': tensor(0.9842)}}, 'spearmanr_mrr@100': {'TransE': {'Kinships': tensor(0.9972)}}, 'spearmanr_mrr@All': {'TransE': {'Kinships': tensor(0.9976)}}, 'test_loss': {'TransE': {'Kinships': 0.335629151849389}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 5627025037156091
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [680, 963, 757, 956, 260, 369, 615, 707, 155, 1042, 1158, 444, 35, 10, 483, 1195, 456, 278, 380, 676, 404, 361, 558, 595, 1096, 753, 824, 184, 302, 445, 821, 806, 1006, 491, 78, 919, 472, 339, 501, 393, 308, 1125, 955, 63, 170, 1035, 1082, 606, 416, 773, 1151, 1005, 494, 780, 1025, 81, 712, 44, 1003, 381, 697, 534, 981, 476, 493, 91, 1185, 1087, 812, 432, 610, 458, 427, 1144, 437, 1001, 276, 1032, 590, 1170, 167, 489, 301, 45, 435, 1047, 253, 677, 309, 896, 394, 180, 759, 957, 284, 6, 231, 843, 862, 341, 622, 270, 886, 724, 482, 609, 1190, 582, 249, 411, 1037, 300, 1046, 499, 777, 656, 541, 587, 150, 169, 888]
valid_ids (0): []
train_ids (1094): [876, 714, 1054, 836, 626, 735, 469, 705, 214, 200, 585, 219, 426, 275, 1147, 791, 1112, 321, 915, 316, 382, 880, 576, 783, 910, 1064, 351, 140, 916, 505, 235, 992, 313, 421, 303, 422, 671, 27, 689, 1074, 924, 850, 322, 979, 1045, 723, 368, 224, 594, 365, 277, 258, 1161, 165, 754, 34, 203, 75, 778, 845, 701, 1002, 447, 584, 646, 933, 33, 455, 428, 431, 917, 131, 147, 352, 363, 570, 878, 722, 550, 178, 904, 1167, 935, 841, 630, 747, 204, 12, 987, 1036, 751, 989, 185, 117, 190, 336, 130, 868, 79, 462, 503, 264, 729, 892, 991, 653, 825, 621, 182, 272, 359, 1092, 279, 1056, 1068, 1198, 920, 826, 375, 202, 1146, 866, 13, 1072, 1152, 134, 581, 823, 323, 406, 1097, 767, 732, 567, 781, 51, 1209, 758, 312, 1148, 733, 739, 121, 738, 177, 464, 583, 267, 1093, 632, 521, 254, 858, 737, 861, 822, 116, 288, 527, 1040, 771, 996, 223, 1019, 1207, 209, 251, 111, 776, 634, 601, 479, 954, 752, 402, 995, 710, 761, 335, 1041, 523, 1039, 439, 396, 210, 1077, 304, 135, 334, 233, 967, 171, 373, 236, 305, 354, 156, 95, 348, 693, 648, 160, 330, 695, 152, 1110, 1140, 545, 1066, 389, 488, 914, 813, 698, 1100, 946, 941, 1179, 1114, 28, 16, 810, 485, 403, 814, 317, 417, 337, 832, 803, 788, 438, 26, 531, 230, 1169, 237, 819, 326, 779, 283, 512, 148, 0, 942, 1023, 468, 103, 800, 597, 263, 1070, 1141, 529, 694, 86, 669, 683, 811, 287, 1192, 1214, 226, 285, 655, 645, 704, 1142, 159, 372, 378, 311, 64, 446, 978, 629, 608, 854, 681, 36, 708, 687, 1033, 1136, 959, 887, 212, 936, 672, 137, 30, 1199, 1095, 572, 419, 715, 938, 804, 1048, 997, 1120, 189, 833, 504, 232, 478, 205, 1171, 213, 520, 740, 480, 867, 763, 742, 589, 3, 430, 1126, 179, 186, 1085, 29, 643, 709, 153, 986, 901, 85, 486, 22, 969, 1067, 562, 123, 424, 731, 703, 201, 1, 115, 119, 515, 261, 197, 314, 384, 1017, 217, 631, 328, 21, 1015, 994, 926, 766, 647, 1084, 908, 506, 741, 295, 132, 1128, 87, 4, 498, 1108, 1210, 183, 1000, 1124, 559, 667, 619, 39, 1213, 97, 965, 195, 420, 674, 652, 43, 664, 72, 1094, 891, 168, 875, 1132, 101, 1078, 852, 1180, 98, 816, 1186, 641, 1183, 885, 1156, 442, 307, 657, 551, 857, 1104, 227, 640, 17, 221, 817, 452, 573, 93, 474, 487, 1020, 1043, 829, 906, 644, 569, 556, 557, 661, 149, 1168, 358, 222, 9, 53, 555, 749, 921, 1079, 42, 877, 496, 691, 495, 440, 787, 770, 789, 984, 273, 1212, 1118, 871, 141, 292, 471, 526, 400, 666, 423, 349, 1201, 614, 70, 1090, 47, 616, 259, 319, 864, 225, 1091, 847, 248, 174, 897, 870, 1065, 164, 364, 720, 173, 357, 542, 346, 196, 1160, 890, 490, 240, 124, 966, 846, 790, 792, 1007, 1098, 106, 41, 18, 947, 1172, 976, 638, 678, 772, 931, 88, 840, 660, 1049, 162, 815, 74, 988, 863, 566, 122, 983, 246, 1135, 1089, 528, 11, 768, 105, 869, 940, 786, 463, 552, 580, 1127, 762, 802, 510, 84, 252, 338, 320, 280, 112, 1069, 807, 443, 1044, 553, 533, 68, 860, 1071, 838, 848, 579, 554, 964, 166, 623, 310, 1202, 387, 407, 399, 1101, 827, 90, 470, 1165, 306, 668, 620, 588, 719, 107, 736, 208, 434, 408, 1163, 1184, 565, 903, 48, 1193, 289, 350, 66, 612, 1197, 899, 1189, 398, 1107, 952, 795, 1155, 1159, 71, 1164, 1031, 1004, 905, 1157, 1010, 1143, 568, 637, 120, 410, 799, 229, 663, 578, 519, 198, 2, 395, 716, 466, 522, 571, 675, 717, 59, 745, 118, 49, 73, 89, 1162, 379, 1175, 1024, 500, 414, 23, 713, 8, 61, 670, 513, 1061, 1038, 982, 839, 543, 256, 561, 937, 376, 879, 1153, 271, 1204, 5, 344, 797, 525, 893, 702, 243, 785, 535, 1028, 726, 925, 1026, 62, 922, 808, 126, 161, 467, 1057, 711, 889, 818, 748, 930, 586, 627, 325, 900, 356, 158, 939, 998, 146, 297, 592, 244, 782, 1055, 774, 730, 696, 682, 530, 650, 750, 1203, 1188, 798, 392, 281, 775, 546, 50, 928, 949, 1182, 52, 635, 67, 1021, 980, 1200, 176, 388, 38, 290, 865, 1133, 756, 274, 658, 1012, 977, 46, 1191, 327, 835, 1076, 602, 884, 207, 429, 517, 109, 607, 397, 331, 1060, 902, 157, 142, 507, 706, 460, 332, 138, 367, 538, 129, 206, 796, 25, 401, 599, 1109, 1119, 370, 728, 299, 968, 1149, 564, 990, 639, 1138, 918, 617, 37, 54, 518, 65, 481, 333, 1206, 642, 909, 451, 929, 970, 242, 673, 927, 92, 347, 769, 895, 366, 69, 958, 1130, 220, 537, 296, 1154, 593, 1115, 794, 113, 60, 1177, 600, 355, 743, 125, 257, 974, 139, 1117, 744, 1030, 524, 881, 801, 96, 127, 649, 151, 145, 191, 943, 324, 1173, 475, 40, 102, 1022, 15, 1121, 1122, 872, 549, 108, 1062, 172, 1137, 502, 286, 809, 842, 725, 293, 577, 1134, 82, 574, 234, 511, 1083, 1063, 686, 374, 187, 746, 611, 837, 547, 448, 973, 383, 291, 1008, 898, 405, 1102, 1086, 1211, 188, 1073, 1178, 651, 449, 32, 718, 932, 975, 266, 948, 856, 907, 1014, 993, 193, 509, 563, 7, 1080, 1081, 874, 175, 412, 684, 575, 596, 163, 318, 951, 853, 239, 265, 1181, 665, 19, 128, 282, 241, 883, 540, 459, 727, 1013, 1058, 913, 211, 247, 154, 1113, 960, 497, 536, 516, 418, 269, 1018, 961, 1099, 215, 685, 618, 972, 24, 385, 1088, 94, 945, 457, 484, 985, 734, 199, 1105, 688, 1050, 100, 342, 953, 544, 433, 76, 477, 934, 605, 1027, 923, 262, 450, 1145, 390, 514, 1174, 1029, 1011, 539, 58, 1111, 1051, 20, 454, 83, 834, 345, 636, 912, 465, 628, 755, 1053, 1116, 1150, 820, 844, 245, 218, 255, 461, 1131, 415, 492, 473, 268, 114, 360, 654, 603, 238, 353, 851, 699, 315, 894, 194, 453, 143, 441, 436, 1075, 250, 679, 1009, 136, 849, 692, 613, 1208, 1106, 944, 181, 662, 1123, 690, 329, 1205, 700, 31, 192, 598, 391, 216, 532, 764, 55, 377, 425, 873, 298, 1176, 591, 1139, 828, 1194, 1187, 228, 659, 144, 760, 294, 1034, 99, 971, 371, 859, 409, 1016, 77, 110, 560, 999, 1196, 56, 1052, 548, 721, 413, 805, 343, 604, 340, 1059, 133, 1166, 793, 911, 386, 625, 784, 104, 14, 508, 1129, 882, 765, 624, 950, 57, 80, 831, 855, 633, 830, 362, 1103, 962]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6318669839843700
the save name prefix for this run is:  chkpt-ID_6318669839843700_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1093
rank avg (pred): 0.531 +- 0.004
mrr vals (pred, true): 0.000, 0.110
batch losses (mrrl, rdl): 0.0, 0.0027079445

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 617
rank avg (pred): 0.359 +- 0.215
mrr vals (pred, true): 0.007, 0.037
batch losses (mrrl, rdl): 0.0, 0.000105165

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 906
rank avg (pred): 0.134 +- 0.107
mrr vals (pred, true): 0.212, 0.113
batch losses (mrrl, rdl): 0.0, 0.0001186254

Epoch over!
epoch time: 12.122

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 351
rank avg (pred): 0.325 +- 0.257
mrr vals (pred, true): 0.204, 0.020
batch losses (mrrl, rdl): 0.0, 0.0001585769

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 893
rank avg (pred): 0.133 +- 0.122
mrr vals (pred, true): 0.285, 0.057
batch losses (mrrl, rdl): 0.0, 5.70497e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 814
rank avg (pred): 0.056 +- 0.050
mrr vals (pred, true): 0.301, 0.070
batch losses (mrrl, rdl): 0.0, 3.4567e-06

Epoch over!
epoch time: 12.076

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 892
rank avg (pred): 0.174 +- 0.162
mrr vals (pred, true): 0.296, 0.058
batch losses (mrrl, rdl): 0.0, 0.0001903714

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1065
rank avg (pred): 0.055 +- 0.053
mrr vals (pred, true): 0.331, 0.275
batch losses (mrrl, rdl): 0.0, 1.09454e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 983
rank avg (pred): 0.055 +- 0.051
mrr vals (pred, true): 0.318, 0.219
batch losses (mrrl, rdl): 0.0, 6.2243e-06

Epoch over!
epoch time: 11.92

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 179
rank avg (pred): 0.317 +- 0.290
mrr vals (pred, true): 0.290, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003390819

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 504
rank avg (pred): 0.162 +- 0.151
mrr vals (pred, true): 0.303, 0.062
batch losses (mrrl, rdl): 0.0, 0.0001050475

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.319 +- 0.293
mrr vals (pred, true): 0.305, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003440728

Epoch over!
epoch time: 12.04

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 474
rank avg (pred): 0.331 +- 0.299
mrr vals (pred, true): 0.300, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002768966

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 87
rank avg (pred): 0.328 +- 0.298
mrr vals (pred, true): 0.311, 0.011
batch losses (mrrl, rdl): 0.0, 0.0001211879

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 105
rank avg (pred): 0.346 +- 0.310
mrr vals (pred, true): 0.306, 0.014
batch losses (mrrl, rdl): 0.0, 0.0002398801

Epoch over!
epoch time: 12.059

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 688
rank avg (pred): 0.356 +- 0.319
mrr vals (pred, true): 0.311, 0.007
batch losses (mrrl, rdl): 0.6794512272, 0.0002038419

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 511
rank avg (pred): 0.099 +- 0.049
mrr vals (pred, true): 0.100, 0.093
batch losses (mrrl, rdl): 0.0248068459, 0.0002636912

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 692
rank avg (pred): 0.407 +- 0.114
mrr vals (pred, true): 0.042, 0.006
batch losses (mrrl, rdl): 0.0006599528, 0.0001996925

Epoch over!
epoch time: 12.378

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 0
rank avg (pred): 0.417 +- 0.163
mrr vals (pred, true): 0.068, 0.077
batch losses (mrrl, rdl): 0.0031845577, 0.0021475386

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 755
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.207, 0.214
batch losses (mrrl, rdl): 0.0005563045, 1.09784e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 252
rank avg (pred): 0.011 +- 0.006
mrr vals (pred, true): 0.118, 0.109
batch losses (mrrl, rdl): 0.0007607861, 0.0001623438

Epoch over!
epoch time: 12.372

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 158
rank avg (pred): 0.461 +- 0.129
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0002304732, 0.0014431662

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 425
rank avg (pred): 0.438 +- 0.138
mrr vals (pred, true): 0.054, 0.007
batch losses (mrrl, rdl): 0.0001761682, 0.0001001043

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 850
rank avg (pred): 0.351 +- 0.112
mrr vals (pred, true): 0.045, 0.095
batch losses (mrrl, rdl): 0.0002829373, 0.0002604254

Epoch over!
epoch time: 12.196

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 894
rank avg (pred): 0.338 +- 0.144
mrr vals (pred, true): 0.070, 0.085
batch losses (mrrl, rdl): 0.004073367, 0.0016187264

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 508
rank avg (pred): 0.289 +- 0.132
mrr vals (pred, true): 0.083, 0.093
batch losses (mrrl, rdl): 0.0110347811, 0.0002816186

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 621
rank avg (pred): 0.367 +- 0.108
mrr vals (pred, true): 0.044, 0.009
batch losses (mrrl, rdl): 0.0004108675, 8.6426e-05

Epoch over!
epoch time: 12.235

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 364
rank avg (pred): 0.382 +- 0.114
mrr vals (pred, true): 0.049, 0.032
batch losses (mrrl, rdl): 1.30253e-05, 0.0005903463

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 525
rank avg (pred): 0.396 +- 0.153
mrr vals (pred, true): 0.066, 0.064
batch losses (mrrl, rdl): 0.0025467607, 0.0007994715

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 256
rank avg (pred): 0.031 +- 0.015
mrr vals (pred, true): 0.114, 0.178
batch losses (mrrl, rdl): 0.0411335304, 5.50628e-05

Epoch over!
epoch time: 12.361

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 354
rank avg (pred): 0.371 +- 0.110
mrr vals (pred, true): 0.046, 0.021
batch losses (mrrl, rdl): 0.0001293985, 0.0003437052

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 649
rank avg (pred): 0.377 +- 0.114
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 9.57684e-05, 0.0003203323

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 746
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.497, 0.217
batch losses (mrrl, rdl): 0.7832133174, 1.12217e-05

Epoch over!
epoch time: 12.461

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 446
rank avg (pred): 0.451 +- 0.142
mrr vals (pred, true): 0.056, 0.006
batch losses (mrrl, rdl): 0.0003970928, 8.64891e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 140
rank avg (pred): 0.381 +- 0.107
mrr vals (pred, true): 0.047, 0.036
batch losses (mrrl, rdl): 8.28472e-05, 0.0006790576

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 346
rank avg (pred): 0.387 +- 0.113
mrr vals (pred, true): 0.052, 0.034
batch losses (mrrl, rdl): 4.16251e-05, 0.0006686037

Epoch over!
epoch time: 12.179

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 786
rank avg (pred): 0.384 +- 0.108
mrr vals (pred, true): 0.047, 0.007
batch losses (mrrl, rdl): 7.1643e-05, 0.0002752374

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 199
rank avg (pred): 0.357 +- 0.105
mrr vals (pred, true): 0.048, 0.007
batch losses (mrrl, rdl): 2.74088e-05, 0.0004132972

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 255
rank avg (pred): 0.045 +- 0.025
mrr vals (pred, true): 0.140, 0.120
batch losses (mrrl, rdl): 0.0039547887, 6.37225e-05

Epoch over!
epoch time: 12.173

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 297
rank avg (pred): 0.020 +- 0.011
mrr vals (pred, true): 0.143, 0.062
batch losses (mrrl, rdl): 0.0866713226, 0.0002482664

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 343
rank avg (pred): 0.388 +- 0.114
mrr vals (pred, true): 0.054, 0.036
batch losses (mrrl, rdl): 0.0001635852, 0.0006793616

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1162
rank avg (pred): 0.352 +- 0.137
mrr vals (pred, true): 0.054, 0.037
batch losses (mrrl, rdl): 0.0001594778, 0.0001105177

Epoch over!
epoch time: 12.338

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 208
rank avg (pred): 0.394 +- 0.110
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 1.2393e-06, 0.0002624656

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 566
rank avg (pred): 0.109 +- 0.047
mrr vals (pred, true): 0.066, 0.095
batch losses (mrrl, rdl): 0.002430321, 0.0001885923

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 428
rank avg (pred): 0.379 +- 0.131
mrr vals (pred, true): 0.059, 0.006
batch losses (mrrl, rdl): 0.0007500463, 0.0002737569

Epoch over!
epoch time: 12.349

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.402 +- 0.150
mrr vals (pred, true): 0.048, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   45 	     0 	 0.05193 	 0.00572 	 m..s
   61 	     1 	 0.05337 	 0.00575 	 m..s
   31 	     2 	 0.05023 	 0.00598 	 m..s
   36 	     3 	 0.05091 	 0.00603 	 m..s
    1 	     4 	 0.04678 	 0.00605 	 m..s
    8 	     5 	 0.04761 	 0.00606 	 m..s
   39 	     6 	 0.05109 	 0.00613 	 m..s
   12 	     7 	 0.04808 	 0.00613 	 m..s
   37 	     8 	 0.05092 	 0.00624 	 m..s
   79 	     9 	 0.05511 	 0.00625 	 m..s
   59 	    10 	 0.05327 	 0.00627 	 m..s
   27 	    11 	 0.04968 	 0.00631 	 m..s
    5 	    12 	 0.04729 	 0.00631 	 m..s
   28 	    13 	 0.04975 	 0.00633 	 m..s
    9 	    14 	 0.04783 	 0.00634 	 m..s
   34 	    15 	 0.05077 	 0.00636 	 m..s
   49 	    16 	 0.05221 	 0.00638 	 m..s
   60 	    17 	 0.05331 	 0.00638 	 m..s
   29 	    18 	 0.04989 	 0.00639 	 m..s
   20 	    19 	 0.04917 	 0.00640 	 m..s
   50 	    20 	 0.05223 	 0.00641 	 m..s
    7 	    21 	 0.04760 	 0.00643 	 m..s
   11 	    22 	 0.04787 	 0.00645 	 m..s
   16 	    23 	 0.04865 	 0.00646 	 m..s
   73 	    24 	 0.05441 	 0.00646 	 m..s
   76 	    25 	 0.05485 	 0.00649 	 m..s
   58 	    26 	 0.05299 	 0.00649 	 m..s
   10 	    27 	 0.04786 	 0.00652 	 m..s
   81 	    28 	 0.05567 	 0.00653 	 m..s
   52 	    29 	 0.05235 	 0.00653 	 m..s
   66 	    30 	 0.05392 	 0.00653 	 m..s
   38 	    31 	 0.05092 	 0.00655 	 m..s
   62 	    32 	 0.05347 	 0.00656 	 m..s
    2 	    33 	 0.04699 	 0.00657 	 m..s
   13 	    34 	 0.04816 	 0.00658 	 m..s
   46 	    35 	 0.05201 	 0.00659 	 m..s
   64 	    36 	 0.05373 	 0.00659 	 m..s
   68 	    37 	 0.05405 	 0.00660 	 m..s
   53 	    38 	 0.05236 	 0.00660 	 m..s
   32 	    39 	 0.05061 	 0.00660 	 m..s
   40 	    40 	 0.05113 	 0.00661 	 m..s
   21 	    41 	 0.04917 	 0.00663 	 m..s
   56 	    42 	 0.05251 	 0.00665 	 m..s
   55 	    43 	 0.05244 	 0.00670 	 m..s
    6 	    44 	 0.04746 	 0.00971 	 m..s
   15 	    45 	 0.04855 	 0.00979 	 m..s
   25 	    46 	 0.04957 	 0.00990 	 m..s
   22 	    47 	 0.04924 	 0.01016 	 m..s
   18 	    48 	 0.04884 	 0.01096 	 m..s
    4 	    49 	 0.04707 	 0.01118 	 m..s
    3 	    50 	 0.04706 	 0.01171 	 m..s
    0 	    51 	 0.04595 	 0.01263 	 m..s
   19 	    52 	 0.04888 	 0.01338 	 m..s
   30 	    53 	 0.04989 	 0.01857 	 m..s
   24 	    54 	 0.04945 	 0.01990 	 ~...
   14 	    55 	 0.04852 	 0.02177 	 ~...
   23 	    56 	 0.04932 	 0.02283 	 ~...
   44 	    57 	 0.05176 	 0.02652 	 ~...
   26 	    58 	 0.04959 	 0.03191 	 ~...
   35 	    59 	 0.05080 	 0.03445 	 ~...
   17 	    60 	 0.04884 	 0.03489 	 ~...
   63 	    61 	 0.05352 	 0.03578 	 ~...
   33 	    62 	 0.05067 	 0.03734 	 ~...
   78 	    63 	 0.05510 	 0.04195 	 ~...
   86 	    64 	 0.05910 	 0.05157 	 ~...
   92 	    65 	 0.06923 	 0.05164 	 ~...
   43 	    66 	 0.05162 	 0.05240 	 ~...
   99 	    67 	 0.10595 	 0.05617 	 m..s
   91 	    68 	 0.06919 	 0.05645 	 ~...
   41 	    69 	 0.05135 	 0.05706 	 ~...
  100 	    70 	 0.10728 	 0.05750 	 m..s
  107 	    71 	 0.12078 	 0.05950 	 m..s
  101 	    72 	 0.11070 	 0.05962 	 m..s
   84 	    73 	 0.05758 	 0.06440 	 ~...
   67 	    74 	 0.05393 	 0.06559 	 ~...
   88 	    75 	 0.05998 	 0.06624 	 ~...
   75 	    76 	 0.05475 	 0.06701 	 ~...
   42 	    77 	 0.05146 	 0.06932 	 ~...
   85 	    78 	 0.05840 	 0.06963 	 ~...
  104 	    79 	 0.11380 	 0.06980 	 m..s
   98 	    80 	 0.10263 	 0.07296 	 ~...
   48 	    81 	 0.05208 	 0.07447 	 ~...
   87 	    82 	 0.05950 	 0.07904 	 ~...
   97 	    83 	 0.10172 	 0.07908 	 ~...
   77 	    84 	 0.05488 	 0.08307 	 ~...
  110 	    85 	 0.14652 	 0.08447 	 m..s
   93 	    86 	 0.07366 	 0.08502 	 ~...
   57 	    87 	 0.05267 	 0.09191 	 m..s
   54 	    88 	 0.05242 	 0.09215 	 m..s
   89 	    89 	 0.06009 	 0.09434 	 m..s
   90 	    90 	 0.06032 	 0.09469 	 m..s
   71 	    91 	 0.05434 	 0.09470 	 m..s
   65 	    92 	 0.05373 	 0.09500 	 m..s
   51 	    93 	 0.05233 	 0.09611 	 m..s
   47 	    94 	 0.05206 	 0.09630 	 m..s
  102 	    95 	 0.11163 	 0.09644 	 ~...
   83 	    96 	 0.05571 	 0.09825 	 m..s
  103 	    97 	 0.11265 	 0.09837 	 ~...
   80 	    98 	 0.05548 	 0.10022 	 m..s
   72 	    99 	 0.05435 	 0.10397 	 m..s
   69 	   100 	 0.05408 	 0.10491 	 m..s
   94 	   101 	 0.07945 	 0.10619 	 ~...
   95 	   102 	 0.09713 	 0.10641 	 ~...
   96 	   103 	 0.09728 	 0.10972 	 ~...
  114 	   104 	 0.15937 	 0.11116 	 m..s
   70 	   105 	 0.05428 	 0.11240 	 m..s
  112 	   106 	 0.15413 	 0.11600 	 m..s
   82 	   107 	 0.05570 	 0.12139 	 m..s
   74 	   108 	 0.05466 	 0.12399 	 m..s
  111 	   109 	 0.14940 	 0.12480 	 ~...
  115 	   110 	 0.16161 	 0.12892 	 m..s
  106 	   111 	 0.11899 	 0.13362 	 ~...
  113 	   112 	 0.15483 	 0.14115 	 ~...
  116 	   113 	 0.16413 	 0.14759 	 ~...
  117 	   114 	 0.16609 	 0.16702 	 ~...
  108 	   115 	 0.14061 	 0.16719 	 ~...
  105 	   116 	 0.11784 	 0.18004 	 m..s
  119 	   117 	 0.17509 	 0.19103 	 ~...
  120 	   118 	 0.21078 	 0.21849 	 ~...
  109 	   119 	 0.14190 	 0.21914 	 m..s
  118 	   120 	 0.17009 	 0.23381 	 m..s
==========================================
r_mrr = 0.7707050442695618
r2_mrr = 0.4957175850868225
spearmanr_mrr@5 = 0.775648295879364
spearmanr_mrr@10 = 0.850684642791748
spearmanr_mrr@50 = 0.9293577075004578
spearmanr_mrr@100 = 0.8944286108016968
spearmanr_mrr@All = 0.8991867303848267
==========================================
test time: 0.41
Done Testing dataset OpenEA
total time taken: 199.6945505142212
training time taken: 183.75398325920105
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.7707)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.4957)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.7756)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8507)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9294)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.8944)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.8992)}}, 'test_loss': {'TransE': {'OpenEA': 0.8991782367302221}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 9578538038609070
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [620, 1020, 329, 1171, 858, 249, 1115, 176, 966, 923, 215, 389, 623, 562, 302, 324, 229, 263, 419, 1203, 847, 880, 739, 998, 440, 888, 248, 710, 1106, 288, 396, 1127, 887, 808, 1206, 782, 631, 748, 1119, 769, 31, 678, 242, 1099, 184, 757, 260, 875, 694, 790, 610, 200, 1068, 776, 46, 1180, 702, 365, 1151, 788, 132, 959, 500, 659, 204, 523, 687, 817, 1175, 896, 539, 864, 940, 158, 379, 391, 559, 425, 439, 924, 581, 1103, 587, 648, 162, 371, 1212, 239, 763, 690, 749, 984, 1192, 466, 818, 795, 363, 475, 778, 452, 462, 540, 746, 23, 489, 871, 962, 698, 272, 988, 412, 291, 120, 1147, 1077, 553, 815, 618, 98, 609, 221]
valid_ids (0): []
train_ids (1094): [787, 181, 297, 147, 29, 281, 898, 450, 773, 846, 1056, 1148, 433, 568, 897, 807, 293, 919, 717, 1105, 464, 392, 93, 18, 490, 201, 920, 1154, 517, 1052, 624, 600, 792, 1143, 1118, 573, 820, 825, 238, 415, 634, 435, 222, 264, 980, 65, 469, 1022, 593, 252, 731, 234, 542, 22, 987, 431, 308, 340, 637, 766, 115, 1110, 527, 479, 52, 917, 35, 616, 1098, 432, 546, 455, 76, 67, 960, 567, 674, 833, 992, 45, 767, 764, 352, 1079, 999, 1201, 621, 355, 1094, 693, 1125, 974, 438, 643, 633, 884, 1161, 967, 822, 358, 1018, 564, 413, 592, 226, 1182, 666, 699, 575, 989, 971, 668, 774, 194, 997, 28, 232, 1091, 783, 1062, 118, 346, 47, 803, 740, 1166, 834, 382, 712, 544, 642, 685, 713, 359, 853, 571, 467, 253, 1021, 483, 1071, 1023, 797, 626, 629, 1093, 119, 1037, 1028, 647, 1202, 628, 481, 276, 1156, 401, 709, 1191, 294, 217, 1104, 1000, 859, 1015, 726, 470, 134, 986, 321, 497, 4, 225, 1160, 649, 196, 163, 169, 1186, 357, 160, 370, 289, 195, 1102, 918, 100, 381, 102, 138, 1038, 107, 1096, 451, 1090, 56, 211, 180, 977, 613, 762, 874, 170, 771, 420, 186, 1041, 233, 636, 561, 531, 916, 759, 137, 1111, 473, 1047, 255, 727, 758, 1117, 1025, 597, 730, 1165, 760, 150, 965, 903, 548, 877, 1139, 349, 237, 19, 70, 832, 1185, 198, 85, 1149, 216, 798, 388, 913, 1051, 1162, 973, 948, 251, 696, 104, 503, 1024, 259, 535, 656, 1060, 101, 1057, 686, 77, 208, 277, 84, 1109, 886, 1152, 632, 582, 1135, 1164, 437, 453, 484, 520, 1211, 824, 522, 5, 376, 703, 518, 968, 755, 579, 243, 446, 931, 635, 1014, 405, 284, 1138, 1133, 675, 625, 7, 1064, 143, 1004, 951, 835, 32, 779, 975, 614, 86, 323, 283, 384, 122, 105, 909, 1072, 958, 839, 867, 828, 729, 353, 1005, 1080, 250, 534, 41, 1045, 350, 1101, 129, 369, 1140, 905, 1214, 156, 397, 171, 873, 700, 203, 135, 943, 378, 953, 202, 494, 849, 280, 61, 870, 1061, 1126, 399, 715, 88, 59, 73, 861, 53, 262, 644, 44, 1034, 444, 268, 258, 1026, 768, 558, 509, 220, 338, 735, 1196, 1074, 994, 417, 716, 906, 947, 205, 848, 1008, 317, 780, 166, 1007, 505, 409, 14, 179, 476, 1054, 673, 21, 930, 765, 83, 336, 922, 69, 1176, 80, 360, 454, 136, 1011, 572, 1173, 142, 153, 1058, 366, 511, 615, 598, 602, 1153, 182, 372, 1210, 254, 826, 552, 275, 299, 63, 599, 1112, 172, 1084, 578, 944, 830, 996, 1078, 159, 55, 993, 810, 802, 537, 328, 528, 653, 1194, 236, 214, 510, 25, 227, 301, 1050, 364, 1204, 1035, 530, 1174, 210, 801, 175, 661, 719, 374, 213, 584, 942, 486, 811, 0, 459, 337, 292, 448, 168, 662, 287, 1006, 991, 402, 50, 133, 112, 646, 1159, 796, 403, 62, 167, 532, 1097, 271, 688, 718, 1137, 241, 821, 261, 844, 315, 1184, 1036, 79, 458, 152, 131, 639, 836, 1048, 747, 929, 508, 689, 926, 191, 894, 752, 295, 816, 447, 8, 212, 316, 725, 1046, 941, 630, 161, 309, 94, 68, 491, 851, 183, 1172, 722, 342, 895, 892, 445, 720, 770, 640, 1017, 524, 970, 326, 197, 1200, 13, 1145, 304, 144, 1086, 273, 576, 738, 536, 979, 334, 589, 1065, 485, 914, 1197, 854, 869, 850, 925, 612, 177, 488, 27, 937, 1095, 428, 300, 743, 165, 800, 728, 723, 857, 332, 981, 928, 495, 36, 130, 480, 734, 1042, 964, 1027, 704, 619, 1059, 956, 286, 591, 43, 375, 310, 889, 1029, 921, 1199, 1190, 827, 680, 560, 1142, 344, 513, 406, 9, 1040, 333, 706, 1167, 541, 487, 840, 306, 1085, 207, 141, 282, 805, 603, 125, 1, 985, 708, 188, 651, 679, 550, 1114, 1066, 411, 842, 976, 496, 878, 1136, 1113, 580, 588, 1129, 416, 1044, 1134, 1157, 51, 377, 116, 1187, 1002, 111, 912, 343, 2, 1116, 794, 590, 502, 347, 1012, 789, 348, 876, 872, 585, 775, 240, 1055, 1010, 1131, 934, 1189, 89, 705, 113, 1177, 1069, 506, 421, 732, 1087, 891, 461, 902, 911, 607, 809, 477, 314, 1130, 12, 1063, 331, 20, 148, 54, 193, 617, 1144, 66, 1195, 804, 395, 935, 269, 654, 6, 482, 952, 414, 145, 963, 1123, 777, 128, 430, 692, 110, 865, 187, 26, 38, 799, 504, 681, 278, 650, 424, 1132, 361, 711, 1049, 784, 1108, 669, 695, 1073, 881, 1188, 91, 772, 622, 37, 174, 545, 1208, 990, 1158, 521, 322, 551, 74, 831, 10, 390, 17, 744, 751, 761, 1053, 154, 982, 745, 1181, 157, 901, 185, 164, 933, 1183, 501, 684, 404, 515, 983, 676, 368, 367, 422, 586, 950, 427, 936, 596, 879, 472, 189, 474, 893, 721, 1088, 99, 638, 387, 742, 1169, 672, 190, 664, 554, 78, 463, 30, 298, 793, 1193, 307, 383, 219, 841, 611, 426, 883, 408, 516, 829, 493, 677, 34, 1009, 819, 311, 1120, 781, 97, 199, 750, 449, 663, 682, 945, 274, 478, 303, 58, 103, 1178, 39, 574, 305, 290, 319, 556, 512, 33, 1003, 436, 645, 256, 813, 423, 863, 538, 279, 791, 209, 407, 57, 658, 140, 24, 16, 223, 837, 1121, 498, 410, 946, 320, 670, 285, 978, 87, 492, 665, 296, 786, 957, 149, 547, 60, 855, 441, 1075, 660, 151, 72, 1107, 499, 1092, 40, 714, 1043, 124, 90, 915, 868, 139, 265, 1076, 106, 230, 885, 1198, 362, 1082, 457, 1122, 595, 386, 701, 555, 608, 244, 270, 1163, 96, 82, 533, 393, 121, 932, 456, 549, 318, 908, 11, 641, 525, 900, 529, 570, 860, 351, 224, 228, 961, 890, 114, 1030, 856, 1039, 206, 1019, 606, 838, 1141, 1124, 519, 117, 939, 1033, 460, 1170, 235, 927, 1205, 1150, 1100, 48, 81, 852, 1128, 812, 418, 335, 380, 92, 443, 652, 594, 691, 1067, 434, 247, 604, 109, 938, 907, 862, 465, 972, 514, 1179, 995, 339, 507, 569, 313, 42, 1213, 356, 823, 246, 655, 1146, 753, 904, 429, 1070, 442, 949, 1032, 123, 707, 1089, 1016, 266, 1081, 385, 15, 400, 736, 178, 95, 468, 267, 173, 325, 330, 1209, 601, 1001, 354, 192, 126, 394, 49, 557, 565, 583, 1031, 683, 231, 1168, 741, 667, 910, 845, 955, 1207, 1013, 373, 471, 754, 341, 806, 785, 345, 127, 526, 724, 64, 756, 954, 577, 327, 899, 1083, 657, 737, 882, 566, 969, 71, 3, 543, 697, 108, 155, 398, 257, 245, 605, 563, 866, 218, 75, 1155, 146, 814, 627, 733, 671, 312, 843]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  677701165176765
the save name prefix for this run is:  chkpt-ID_677701165176765_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 820
rank avg (pred): 0.491 +- 0.003
mrr vals (pred, true): 0.000, 0.086
batch losses (mrrl, rdl): 0.0, 0.0040005101

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 157
rank avg (pred): 0.394 +- 0.194
mrr vals (pred, true): 0.080, 0.022
batch losses (mrrl, rdl): 0.0, 0.0006475705

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 597
rank avg (pred): 0.366 +- 0.257
mrr vals (pred, true): 0.216, 0.008
batch losses (mrrl, rdl): 0.0, 2.67913e-05

Epoch over!
epoch time: 12.082

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 834
rank avg (pred): 0.087 +- 0.068
mrr vals (pred, true): 0.272, 0.208
batch losses (mrrl, rdl): 0.0, 9.55882e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 948
rank avg (pred): 0.405 +- 0.303
mrr vals (pred, true): 0.265, 0.007
batch losses (mrrl, rdl): 0.0, 5.51331e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 662
rank avg (pred): 0.341 +- 0.284
mrr vals (pred, true): 0.326, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002434681

Epoch over!
epoch time: 11.78

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 44
rank avg (pred): 0.099 +- 0.091
mrr vals (pred, true): 0.370, 0.191
batch losses (mrrl, rdl): 0.0, 5.68834e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 254
rank avg (pred): 0.090 +- 0.081
mrr vals (pred, true): 0.368, 0.245
batch losses (mrrl, rdl): 0.0, 4.63684e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 130
rank avg (pred): 0.366 +- 0.306
mrr vals (pred, true): 0.333, 0.020
batch losses (mrrl, rdl): 0.0, 0.0005496283

Epoch over!
epoch time: 11.938

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 39
rank avg (pred): 0.144 +- 0.128
mrr vals (pred, true): 0.364, 0.111
batch losses (mrrl, rdl): 0.0, 0.0001178134

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 992
rank avg (pred): 0.084 +- 0.075
mrr vals (pred, true): 0.370, 0.218
batch losses (mrrl, rdl): 0.0, 4.23275e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1029
rank avg (pred): 0.324 +- 0.294
mrr vals (pred, true): 0.373, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003066046

Epoch over!
epoch time: 11.92

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 62
rank avg (pred): 0.108 +- 0.096
mrr vals (pred, true): 0.367, 0.126
batch losses (mrrl, rdl): 0.0, 5.53304e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 42
rank avg (pred): 0.120 +- 0.109
mrr vals (pred, true): 0.377, 0.104
batch losses (mrrl, rdl): 0.0, 5.22188e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1107
rank avg (pred): 0.334 +- 0.303
mrr vals (pred, true): 0.379, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002505752

Epoch over!
epoch time: 11.849

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.078 +- 0.072
mrr vals (pred, true): 0.396, 0.241
batch losses (mrrl, rdl): 0.2409692258, 2.65626e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 270
rank avg (pred): 0.045 +- 0.032
mrr vals (pred, true): 0.109, 0.056
batch losses (mrrl, rdl): 0.0344678499, 0.000150014

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1161
rank avg (pred): 0.487 +- 0.202
mrr vals (pred, true): 0.053, 0.035
batch losses (mrrl, rdl): 9.3332e-05, 0.0006113065

Epoch over!
epoch time: 12.198

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1083
rank avg (pred): 0.474 +- 0.209
mrr vals (pred, true): 0.057, 0.107
batch losses (mrrl, rdl): 0.0244906079, 0.0020008644

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1196
rank avg (pred): 0.443 +- 0.212
mrr vals (pred, true): 0.057, 0.006
batch losses (mrrl, rdl): 0.0004553387, 5.97686e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 474
rank avg (pred): 0.487 +- 0.173
mrr vals (pred, true): 0.046, 0.007
batch losses (mrrl, rdl): 0.0001944369, 5.57723e-05

Epoch over!
epoch time: 11.956

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 555
rank avg (pred): 0.114 +- 0.068
mrr vals (pred, true): 0.098, 0.064
batch losses (mrrl, rdl): 0.0231726374, 0.0002827858

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 995
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.195, 0.260
batch losses (mrrl, rdl): 0.0426259413, 2.96924e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 2
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.142, 0.158
batch losses (mrrl, rdl): 0.0027754682, 5.32039e-05

Epoch over!
epoch time: 12.063

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 580
rank avg (pred): 0.445 +- 0.169
mrr vals (pred, true): 0.047, 0.014
batch losses (mrrl, rdl): 8.67271e-05, 0.0003275303

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 791
rank avg (pred): 0.449 +- 0.167
mrr vals (pred, true): 0.046, 0.007
batch losses (mrrl, rdl): 0.000125165, 6.58465e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 17
rank avg (pred): 0.013 +- 0.008
mrr vals (pred, true): 0.126, 0.187
batch losses (mrrl, rdl): 0.0363101773, 4.17534e-05

Epoch over!
epoch time: 12.085

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 441
rank avg (pred): 0.419 +- 0.184
mrr vals (pred, true): 0.057, 0.007
batch losses (mrrl, rdl): 0.0004330795, 0.0001032432

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 787
rank avg (pred): 0.436 +- 0.172
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 2.41719e-05, 9.85446e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 352
rank avg (pred): 0.409 +- 0.186
mrr vals (pred, true): 0.068, 0.029
batch losses (mrrl, rdl): 0.0032605042, 0.0007353768

Epoch over!
epoch time: 12.018

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 357
rank avg (pred): 0.437 +- 0.158
mrr vals (pred, true): 0.044, 0.020
batch losses (mrrl, rdl): 0.00037346, 0.0007557819

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1034
rank avg (pred): 0.369 +- 0.198
mrr vals (pred, true): 0.056, 0.006
batch losses (mrrl, rdl): 0.0003540281, 0.0002663516

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 131
rank avg (pred): 0.399 +- 0.164
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.0002002667, 0.0008704

Epoch over!
epoch time: 12.297

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 920
rank avg (pred): 0.411 +- 0.171
mrr vals (pred, true): 0.053, 0.096
batch losses (mrrl, rdl): 8.49055e-05, 0.0004799073

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 472
rank avg (pred): 0.374 +- 0.154
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 7.3442e-06, 0.0002925165

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1036
rank avg (pred): 0.378 +- 0.156
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001302594, 0.0002642391

Epoch over!
epoch time: 12.05

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 768
rank avg (pred): 0.368 +- 0.147
mrr vals (pred, true): 0.046, 0.097
batch losses (mrrl, rdl): 0.0001877661, 0.000314524

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 14
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.176, 0.202
batch losses (mrrl, rdl): 0.0066127623, 3.82795e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 626
rank avg (pred): 0.372 +- 0.144
mrr vals (pred, true): 0.046, 0.026
batch losses (mrrl, rdl): 0.0001867633, 0.0001434967

Epoch over!
epoch time: 12.001

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 504
rank avg (pred): 0.118 +- 0.069
mrr vals (pred, true): 0.082, 0.062
batch losses (mrrl, rdl): 0.0101477364, 0.0003156712

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 536
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.157, 0.088
batch losses (mrrl, rdl): 0.1150671989, 0.0007126674

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 507
rank avg (pred): 0.169 +- 0.099
mrr vals (pred, true): 0.083, 0.067
batch losses (mrrl, rdl): 0.0107589774, 0.0001312929

Epoch over!
epoch time: 12.465

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 555
rank avg (pred): 0.257 +- 0.159
mrr vals (pred, true): 0.118, 0.064
batch losses (mrrl, rdl): 0.045648329, 0.0001244612

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 392
rank avg (pred): 0.375 +- 0.151
mrr vals (pred, true): 0.050, 0.063
batch losses (mrrl, rdl): 7.704e-07, 0.0007961778

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1008
rank avg (pred): 0.362 +- 0.169
mrr vals (pred, true): 0.058, 0.052
batch losses (mrrl, rdl): 0.0006382103, 0.000557897

Epoch over!
epoch time: 11.984

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.359 +- 0.154
mrr vals (pred, true): 0.049, 0.034

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04553 	 0.00581 	 m..s
   26 	     1 	 0.04645 	 0.00583 	 m..s
   49 	     2 	 0.04769 	 0.00600 	 m..s
   60 	     3 	 0.04893 	 0.00602 	 m..s
   65 	     4 	 0.04939 	 0.00608 	 m..s
   22 	     5 	 0.04629 	 0.00620 	 m..s
   32 	     6 	 0.04659 	 0.00624 	 m..s
   14 	     7 	 0.04597 	 0.00626 	 m..s
   80 	     8 	 0.05154 	 0.00628 	 m..s
   72 	     9 	 0.05053 	 0.00629 	 m..s
    9 	    10 	 0.04558 	 0.00632 	 m..s
   85 	    11 	 0.05775 	 0.00633 	 m..s
   59 	    12 	 0.04891 	 0.00634 	 m..s
   58 	    13 	 0.04839 	 0.00635 	 m..s
   30 	    14 	 0.04656 	 0.00636 	 m..s
   23 	    15 	 0.04630 	 0.00637 	 m..s
   66 	    16 	 0.04941 	 0.00638 	 m..s
   49 	    17 	 0.04769 	 0.00639 	 m..s
    5 	    18 	 0.04512 	 0.00640 	 m..s
   84 	    19 	 0.05772 	 0.00644 	 m..s
   82 	    20 	 0.05295 	 0.00644 	 m..s
    0 	    21 	 0.04438 	 0.00644 	 m..s
   61 	    22 	 0.04902 	 0.00645 	 m..s
   67 	    23 	 0.04957 	 0.00647 	 m..s
   20 	    24 	 0.04621 	 0.00648 	 m..s
   30 	    25 	 0.04656 	 0.00650 	 m..s
   76 	    26 	 0.05099 	 0.00650 	 m..s
    9 	    27 	 0.04558 	 0.00652 	 m..s
    2 	    28 	 0.04496 	 0.00652 	 m..s
    3 	    29 	 0.04499 	 0.00653 	 m..s
   56 	    30 	 0.04815 	 0.00653 	 m..s
   54 	    31 	 0.04785 	 0.00653 	 m..s
   86 	    32 	 0.05895 	 0.00654 	 m..s
   61 	    33 	 0.04902 	 0.00655 	 m..s
   37 	    34 	 0.04693 	 0.00657 	 m..s
   70 	    35 	 0.04985 	 0.00658 	 m..s
   52 	    36 	 0.04779 	 0.00659 	 m..s
   16 	    37 	 0.04607 	 0.00661 	 m..s
   15 	    38 	 0.04602 	 0.00662 	 m..s
    1 	    39 	 0.04439 	 0.00662 	 m..s
   46 	    40 	 0.04740 	 0.00664 	 m..s
   35 	    41 	 0.04676 	 0.00664 	 m..s
    4 	    42 	 0.04500 	 0.00667 	 m..s
   45 	    43 	 0.04739 	 0.00668 	 m..s
   18 	    44 	 0.04620 	 0.00669 	 m..s
   38 	    45 	 0.04699 	 0.00669 	 m..s
   41 	    46 	 0.04729 	 0.00670 	 m..s
   53 	    47 	 0.04784 	 0.00673 	 m..s
   12 	    48 	 0.04580 	 0.00979 	 m..s
   28 	    49 	 0.04650 	 0.00988 	 m..s
    6 	    50 	 0.04534 	 0.01263 	 m..s
   33 	    51 	 0.04667 	 0.01320 	 m..s
   27 	    52 	 0.04648 	 0.01355 	 m..s
   28 	    53 	 0.04650 	 0.01523 	 m..s
   55 	    54 	 0.04792 	 0.01684 	 m..s
   33 	    55 	 0.04667 	 0.02143 	 ~...
    7 	    56 	 0.04542 	 0.02633 	 ~...
   24 	    57 	 0.04634 	 0.02736 	 ~...
   51 	    58 	 0.04770 	 0.02805 	 ~...
   17 	    59 	 0.04619 	 0.03222 	 ~...
   75 	    60 	 0.05078 	 0.03356 	 ~...
   36 	    61 	 0.04686 	 0.03429 	 ~...
   79 	    62 	 0.05141 	 0.03445 	 ~...
   64 	    63 	 0.04930 	 0.03449 	 ~...
   73 	    64 	 0.05076 	 0.03745 	 ~...
   25 	    65 	 0.04638 	 0.04091 	 ~...
   69 	    66 	 0.04978 	 0.04160 	 ~...
   87 	    67 	 0.07073 	 0.04880 	 ~...
   87 	    68 	 0.07073 	 0.05157 	 ~...
   19 	    69 	 0.04620 	 0.05180 	 ~...
   78 	    70 	 0.05124 	 0.05911 	 ~...
   11 	    71 	 0.04567 	 0.06586 	 ~...
   13 	    72 	 0.04581 	 0.06602 	 ~...
   71 	    73 	 0.04996 	 0.06655 	 ~...
   98 	    74 	 0.08765 	 0.06932 	 ~...
   73 	    75 	 0.05076 	 0.07223 	 ~...
  100 	    76 	 0.10212 	 0.07550 	 ~...
   97 	    77 	 0.08590 	 0.07687 	 ~...
   96 	    78 	 0.08519 	 0.07764 	 ~...
  103 	    79 	 0.10304 	 0.07777 	 ~...
   89 	    80 	 0.07163 	 0.07908 	 ~...
   92 	    81 	 0.08297 	 0.08267 	 ~...
   68 	    82 	 0.04961 	 0.08434 	 m..s
   63 	    83 	 0.04927 	 0.08456 	 m..s
   98 	    84 	 0.08765 	 0.08502 	 ~...
   95 	    85 	 0.08416 	 0.08517 	 ~...
   90 	    86 	 0.08210 	 0.08610 	 ~...
   91 	    87 	 0.08211 	 0.08724 	 ~...
  115 	    88 	 0.16218 	 0.08823 	 m..s
   81 	    89 	 0.05196 	 0.08825 	 m..s
  102 	    90 	 0.10246 	 0.09087 	 ~...
  105 	    91 	 0.10526 	 0.09178 	 ~...
   47 	    92 	 0.04752 	 0.09260 	 m..s
   94 	    93 	 0.08380 	 0.09268 	 ~...
   48 	    94 	 0.04762 	 0.09542 	 m..s
   40 	    95 	 0.04718 	 0.09563 	 m..s
   57 	    96 	 0.04830 	 0.09611 	 m..s
   39 	    97 	 0.04715 	 0.09648 	 m..s
   44 	    98 	 0.04733 	 0.09679 	 m..s
   42 	    99 	 0.04731 	 0.09713 	 m..s
   42 	   100 	 0.04731 	 0.09718 	 m..s
   93 	   101 	 0.08311 	 0.09751 	 ~...
   21 	   102 	 0.04628 	 0.09761 	 m..s
  109 	   103 	 0.13213 	 0.10369 	 ~...
   77 	   104 	 0.05122 	 0.10386 	 m..s
  120 	   105 	 0.19366 	 0.10641 	 m..s
  112 	   106 	 0.15665 	 0.10775 	 m..s
  107 	   107 	 0.12650 	 0.11600 	 ~...
  106 	   108 	 0.12537 	 0.12010 	 ~...
   83 	   109 	 0.05728 	 0.13100 	 m..s
  108 	   110 	 0.12726 	 0.16005 	 m..s
  101 	   111 	 0.10212 	 0.17049 	 m..s
  104 	   112 	 0.10465 	 0.21706 	 MISS
  110 	   113 	 0.13574 	 0.23381 	 m..s
  114 	   114 	 0.15889 	 0.24481 	 m..s
  111 	   115 	 0.15528 	 0.26451 	 MISS
  113 	   116 	 0.15750 	 0.27645 	 MISS
  119 	   117 	 0.19364 	 0.28232 	 m..s
  118 	   118 	 0.18838 	 0.28985 	 MISS
  116 	   119 	 0.18666 	 0.29280 	 MISS
  117 	   120 	 0.18667 	 0.29431 	 MISS
==========================================
r_mrr = 0.8315026760101318
r2_mrr = 0.5922375321388245
spearmanr_mrr@5 = 0.8702114224433899
spearmanr_mrr@10 = 0.8563259243965149
spearmanr_mrr@50 = 0.9316803216934204
spearmanr_mrr@100 = 0.9433308243751526
spearmanr_mrr@All = 0.9456587433815002
==========================================
test time: 0.432
Done Testing dataset OpenEA
total time taken: 196.7622447013855
training time taken: 181.2078981399536
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8315)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.5922)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.8702)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8563)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9317)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9433)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9457)}}, 'test_loss': {'TransE': {'OpenEA': 1.6364269897749182}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 8943215232102266
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1048, 1068, 1012, 496, 142, 80, 1104, 306, 226, 4, 799, 873, 84, 613, 1108, 55, 237, 83, 937, 738, 382, 1013, 296, 1161, 1061, 827, 1020, 479, 1030, 38, 90, 953, 797, 1203, 1034, 86, 133, 275, 268, 18, 1112, 831, 978, 821, 701, 312, 396, 12, 244, 230, 228, 642, 639, 885, 210, 984, 598, 49, 924, 121, 472, 119, 166, 505, 888, 1186, 176, 510, 1005, 656, 747, 155, 657, 162, 285, 617, 968, 484, 658, 489, 1166, 167, 1199, 838, 1143, 906, 216, 565, 494, 264, 1006, 1167, 135, 940, 7, 693, 486, 516, 807, 82, 542, 1040, 1037, 1052, 463, 1025, 746, 87, 735, 644, 1043, 626, 820, 730, 962, 1152, 409, 485, 1036, 917, 520]
valid_ids (0): []
train_ids (1094): [1007, 459, 1131, 305, 803, 804, 1063, 708, 1154, 1059, 842, 321, 1057, 234, 384, 905, 932, 106, 192, 1195, 32, 352, 614, 28, 127, 1046, 1078, 1111, 1180, 722, 488, 893, 497, 548, 713, 531, 1162, 871, 220, 1130, 91, 326, 99, 37, 1003, 443, 622, 624, 1023, 1065, 1120, 538, 118, 575, 1049, 883, 270, 715, 332, 233, 360, 442, 290, 76, 1002, 94, 753, 774, 79, 736, 213, 303, 519, 635, 0, 467, 178, 1188, 594, 462, 665, 1200, 775, 853, 951, 277, 828, 407, 1055, 802, 822, 1145, 426, 338, 660, 209, 530, 766, 1075, 1213, 574, 661, 963, 196, 337, 999, 131, 517, 336, 1193, 198, 362, 819, 1032, 100, 421, 719, 381, 535, 707, 501, 864, 895, 779, 177, 1054, 1207, 731, 525, 374, 544, 11, 325, 588, 65, 1092, 441, 430, 255, 1122, 30, 51, 1132, 604, 982, 621, 111, 95, 359, 239, 279, 1102, 561, 206, 385, 278, 1087, 638, 868, 700, 221, 683, 493, 651, 712, 414, 1156, 633, 972, 444, 402, 1026, 322, 636, 343, 720, 699, 431, 449, 840, 907, 58, 959, 768, 1045, 1204, 834, 474, 481, 53, 438, 1093, 1202, 796, 559, 1079, 364, 299, 434, 181, 23, 33, 781, 291, 283, 1191, 369, 584, 50, 218, 896, 1080, 227, 714, 350, 579, 400, 539, 97, 455, 85, 767, 954, 592, 541, 132, 750, 1, 199, 1085, 928, 1010, 377, 760, 180, 846, 787, 1201, 345, 1101, 1123, 667, 1174, 881, 17, 418, 757, 550, 504, 1091, 41, 892, 625, 25, 814, 1171, 419, 568, 911, 174, 1008, 785, 253, 1135, 694, 734, 356, 808, 1113, 789, 380, 1069, 589, 26, 406, 717, 778, 543, 829, 946, 355, 487, 113, 772, 643, 901, 903, 507, 263, 784, 690, 294, 971, 1173, 465, 231, 791, 521, 666, 1185, 36, 70, 692, 859, 123, 764, 894, 1172, 733, 185, 511, 897, 1077, 931, 391, 183, 751, 632, 117, 967, 170, 351, 1121, 383, 242, 1160, 677, 1175, 96, 1000, 674, 71, 1165, 347, 495, 593, 47, 1138, 62, 865, 482, 891, 756, 282, 152, 556, 211, 801, 102, 393, 115, 1021, 236, 532, 925, 59, 267, 841, 995, 104, 606, 970, 786, 798, 920, 620, 318, 52, 655, 728, 109, 609, 31, 1070, 882, 762, 450, 718, 676, 42, 288, 116, 250, 172, 836, 794, 949, 673, 314, 957, 223, 1129, 600, 912, 1164, 653, 1190, 522, 468, 1117, 114, 1194, 397, 145, 195, 862, 705, 867, 1009, 1187, 998, 46, 861, 508, 456, 358, 562, 184, 1004, 502, 1126, 551, 696, 1110, 554, 1184, 723, 628, 850, 825, 150, 961, 182, 1056, 466, 378, 256, 761, 1144, 165, 514, 874, 14, 63, 448, 533, 57, 153, 191, 260, 317, 737, 988, 523, 222, 390, 366, 682, 1064, 35, 73, 103, 681, 680, 577, 637, 134, 981, 387, 404, 721, 1103, 179, 1197, 685, 729, 289, 440, 432, 711, 411, 599, 149, 405, 884, 899, 190, 156, 433, 24, 607, 413, 204, 6, 212, 476, 765, 460, 706, 710, 194, 478, 395, 500, 560, 975, 889, 420, 1146, 458, 688, 107, 1050, 410, 408, 368, 1125, 547, 93, 439, 950, 399, 566, 980, 668, 740, 641, 146, 371, 812, 994, 852, 754, 866, 445, 926, 569, 748, 663, 634, 265, 157, 608, 858, 938, 1119, 1017, 1105, 331, 1139, 545, 1137, 66, 704, 847, 996, 349, 886, 365, 246, 716, 851, 1019, 136, 993, 943, 973, 702, 1183, 72, 528, 1076, 346, 1149, 983, 175, 88, 375, 1029, 755, 571, 247, 379, 776, 1095, 403, 258, 54, 936, 672, 709, 126, 143, 354, 570, 168, 208, 526, 363, 125, 1086, 915, 1016, 670, 171, 913, 469, 743, 164, 424, 647, 910, 373, 394, 724, 537, 1018, 138, 534, 45, 1177, 428, 811, 1060, 215, 447, 876, 1134, 266, 1053, 1163, 1044, 1028, 140, 939, 415, 572, 631, 902, 583, 969, 339, 1205, 1155, 646, 1074, 16, 344, 159, 74, 947, 112, 29, 1192, 879, 64, 612, 650, 348, 471, 13, 1157, 147, 129, 898, 238, 887, 1153, 202, 353, 506, 890, 578, 1118, 48, 1039, 576, 923, 948, 823, 1210, 284, 752, 652, 933, 1031, 934, 997, 503, 837, 451, 669, 271, 68, 161, 989, 619, 726, 813, 662, 582, 334, 308, 311, 1170, 108, 742, 872, 992, 990, 141, 67, 1116, 262, 40, 857, 122, 130, 557, 1182, 780, 187, 137, 1098, 1176, 1088, 596, 540, 173, 1022, 75, 914, 214, 78, 437, 1107, 464, 985, 77, 788, 844, 679, 329, 909, 916, 935, 1011, 1066, 965, 1089, 225, 512, 461, 5, 800, 56, 269, 741, 217, 310, 1099, 816, 515, 361, 292, 691, 684, 602, 388, 848, 229, 330, 1128, 877, 158, 645, 333, 3, 128, 429, 203, 648, 616, 454, 281, 945, 1096, 453, 900, 1179, 880, 553, 795, 34, 1109, 1001, 782, 1090, 860, 315, 929, 309, 1124, 524, 695, 1051, 219, 328, 659, 110, 991, 154, 790, 372, 398, 297, 603, 1140, 490, 1209, 151, 19, 295, 739, 1208, 197, 81, 640, 629, 649, 623, 9, 323, 878, 327, 697, 964, 758, 1206, 930, 1168, 416, 826, 769, 1115, 956, 1159, 1178, 849, 835, 986, 499, 324, 952, 839, 927, 8, 549, 342, 810, 870, 1094, 615, 245, 987, 200, 189, 427, 921, 105, 703, 139, 955, 186, 316, 376, 581, 918, 148, 1133, 818, 773, 357, 809, 480, 43, 483, 1035, 976, 725, 341, 386, 120, 1147, 1214, 22, 763, 272, 792, 830, 759, 770, 207, 1072, 611, 1047, 163, 425, 509, 518, 477, 919, 856, 473, 601, 273, 417, 1127, 298, 563, 1027, 261, 201, 678, 412, 44, 1151, 783, 144, 276, 513, 590, 340, 960, 302, 287, 254, 1073, 10, 1084, 942, 875, 301, 567, 1097, 98, 205, 843, 470, 1083, 664, 39, 1081, 529, 855, 689, 232, 854, 675, 304, 1038, 1014, 370, 1181, 1189, 313, 727, 597, 806, 1196, 1158, 1033, 280, 744, 536, 686, 160, 452, 241, 1141, 498, 558, 591, 817, 815, 491, 89, 698, 1100, 1150, 587, 527, 60, 286, 1106, 69, 805, 966, 446, 20, 457, 1148, 492, 193, 319, 15, 618, 586, 274, 555, 1136, 435, 908, 293, 941, 124, 21, 552, 61, 423, 845, 654, 101, 2, 1114, 1041, 904, 422, 595, 564, 1211, 367, 958, 610, 188, 732, 252, 824, 580, 248, 257, 1142, 863, 401, 251, 240, 1015, 1062, 793, 475, 389, 687, 1169, 1042, 259, 605, 944, 224, 436, 546, 27, 335, 243, 300, 92, 979, 671, 392, 627, 585, 777, 832, 573, 1198, 1212, 320, 1067, 833, 1071, 235, 1058, 1024, 1082, 749, 974, 745, 249, 977, 922, 169, 869, 630, 771, 307]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6536493115483800
the save name prefix for this run is:  chkpt-ID_6536493115483800_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 564
rank avg (pred): 0.481 +- 0.008
mrr vals (pred, true): 0.000, 0.048
batch losses (mrrl, rdl): 0.0, 0.0014958089

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 407
rank avg (pred): 0.242 +- 0.211
mrr vals (pred, true): 0.139, 0.006
batch losses (mrrl, rdl): 0.0, 0.0010427429

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 864
rank avg (pred): 0.245 +- 0.244
mrr vals (pred, true): 0.304, 0.006
batch losses (mrrl, rdl): 0.0, 0.0008282982

Epoch over!
epoch time: 12.047

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 977
rank avg (pred): 0.243 +- 0.252
mrr vals (pred, true): 0.346, 0.272
batch losses (mrrl, rdl): 0.0, 0.0011394105

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 596
rank avg (pred): 0.278 +- 0.274
mrr vals (pred, true): 0.320, 0.024
batch losses (mrrl, rdl): 0.0, 2.23361e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 30
rank avg (pred): 0.230 +- 0.267
mrr vals (pred, true): 0.437, 0.061
batch losses (mrrl, rdl): 0.0, 0.0004521487

Epoch over!
epoch time: 11.766

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 772
rank avg (pred): 0.220 +- 0.255
mrr vals (pred, true): 0.432, 0.095
batch losses (mrrl, rdl): 0.0, 1.3044e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 461
rank avg (pred): 0.214 +- 0.252
mrr vals (pred, true): 0.478, 0.006
batch losses (mrrl, rdl): 0.0, 0.001065177

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 840
rank avg (pred): 0.235 +- 0.276
mrr vals (pred, true): 0.468, 0.093
batch losses (mrrl, rdl): 0.0, 1.0817e-05

Epoch over!
epoch time: 11.921

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 10
rank avg (pred): 0.248 +- 0.284
mrr vals (pred, true): 0.469, 0.134
batch losses (mrrl, rdl): 0.0, 0.0008498441

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 0
rank avg (pred): 0.224 +- 0.286
mrr vals (pred, true): 0.528, 0.077
batch losses (mrrl, rdl): 0.0, 0.0004594469

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.270 +- 0.296
mrr vals (pred, true): 0.481, 0.006
batch losses (mrrl, rdl): 0.0, 0.0006070117

Epoch over!
epoch time: 11.979

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 864
rank avg (pred): 0.241 +- 0.290
mrr vals (pred, true): 0.501, 0.006
batch losses (mrrl, rdl): 0.0, 0.0007897581

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1125
rank avg (pred): 0.203 +- 0.259
mrr vals (pred, true): 0.539, 0.007
batch losses (mrrl, rdl): 0.0, 0.0012113504

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 839
rank avg (pred): 0.247 +- 0.294
mrr vals (pred, true): 0.479, 0.096
batch losses (mrrl, rdl): 0.0, 4.8228e-06

Epoch over!
epoch time: 11.863

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 913
rank avg (pred): 0.243 +- 0.295
mrr vals (pred, true): 0.523, 0.095
batch losses (mrrl, rdl): 2.2365987301, 0.0009229344

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 570
rank avg (pred): 0.428 +- 0.000
mrr vals (pred, true): 0.000, 0.009
batch losses (mrrl, rdl): 0.024844639, 0.0001957836

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 160
rank avg (pred): 0.314 +- 0.003
mrr vals (pred, true): 0.000, 0.022
batch losses (mrrl, rdl): 0.024788389, 0.0002128912

Epoch over!
epoch time: 12.091

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 620
rank avg (pred): 0.302 +- 0.000
mrr vals (pred, true): 0.000, 0.034
batch losses (mrrl, rdl): 0.0247800536, 0.000176804

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 788
rank avg (pred): 0.269 +- 0.002
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247532241, 0.0011710885

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 281
rank avg (pred): 0.252 +- 0.002
mrr vals (pred, true): 0.000, 0.149
batch losses (mrrl, rdl): 0.2203609794, 0.0007552685

Epoch over!
epoch time: 12.091

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.251 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247355774, 0.0013159522

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 568
rank avg (pred): 0.249 +- 0.000
mrr vals (pred, true): 0.000, 0.011
batch losses (mrrl, rdl): 0.0247334428, 0.0003804028

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 221
rank avg (pred): 0.243 +- 0.002
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247267801, 0.0013957812

Epoch over!
epoch time: 12.063

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 683
rank avg (pred): 0.242 +- 0.000
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247249007, 0.0014693412

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 14
rank avg (pred): 0.243 +- 0.001
mrr vals (pred, true): 0.000, 0.202
batch losses (mrrl, rdl): 0.4062525034, 0.0007925924

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 207
rank avg (pred): 0.236 +- 0.001
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247181002, 0.0015306411

Epoch over!
epoch time: 11.985

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 566
rank avg (pred): 0.249 +- 0.000
mrr vals (pred, true): 0.000, 0.095
batch losses (mrrl, rdl): 0.0247327965, 0.0002037625

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 725
rank avg (pred): 0.249 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247330666, 0.0013734178

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 332
rank avg (pred): 0.236 +- 0.001
mrr vals (pred, true): 0.000, 0.058
batch losses (mrrl, rdl): 0.0247179512, 0.0001147876

Epoch over!
epoch time: 12.088

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 871
rank avg (pred): 0.247 +- 0.001
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247307159, 0.0013654913

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1091
rank avg (pred): 0.256 +- 0.001
mrr vals (pred, true): 0.000, 0.071
batch losses (mrrl, rdl): 0.0247402024, 0.0001593689

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 616
rank avg (pred): 0.227 +- 0.000
mrr vals (pred, true): 0.000, 0.015
batch losses (mrrl, rdl): 0.0247069839, 0.0004416302

Epoch over!
epoch time: 12.238

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 664
rank avg (pred): 0.238 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247211289, 0.0014341194

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 707
rank avg (pred): 0.245 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247286484, 0.0015135135

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 825
rank avg (pred): 0.246 +- 0.000
mrr vals (pred, true): 0.000, 0.205
batch losses (mrrl, rdl): 0.4188488424, 0.0010607208

Epoch over!
epoch time: 12.271

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 587
rank avg (pred): 0.236 +- 0.000
mrr vals (pred, true): 0.000, 0.034
batch losses (mrrl, rdl): 0.0247189421, 0.0003069113

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.246 +- 0.000
mrr vals (pred, true): 0.000, 0.301
batch losses (mrrl, rdl): 0.9073709846, 0.0009643383

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1126
rank avg (pred): 0.227 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247076266, 0.0015819898

Epoch over!
epoch time: 11.953

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 201
rank avg (pred): 0.228 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247087423, 0.0015195974

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 293
rank avg (pred): 0.233 +- 0.000
mrr vals (pred, true): 0.000, 0.287
batch losses (mrrl, rdl): 0.8231452703, 0.0006985542

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 483
rank avg (pred): 0.234 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247157905, 0.0015681247

Epoch over!
epoch time: 12.063

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1176
rank avg (pred): 0.248 +- 0.000
mrr vals (pred, true): 0.000, 0.065
batch losses (mrrl, rdl): 0.0247317888, 0.0002874342

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 604
rank avg (pred): 0.238 +- 0.000
mrr vals (pred, true): 0.000, 0.014
batch losses (mrrl, rdl): 0.0247209296, 0.0004120772

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 397
rank avg (pred): 0.227 +- 0.000
mrr vals (pred, true): 0.000, 0.038
batch losses (mrrl, rdl): 0.0247071609, 9.36459e-05

Epoch over!
epoch time: 11.895

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.237 +- 0.000
mrr vals (pred, true): 0.000, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.00028 	 0.00600 	 ~...
    0 	     1 	 0.00028 	 0.00609 	 ~...
    0 	     2 	 0.00028 	 0.00617 	 ~...
    0 	     3 	 0.00028 	 0.00621 	 ~...
    0 	     4 	 0.00028 	 0.00622 	 ~...
    0 	     5 	 0.00028 	 0.00622 	 ~...
    0 	     6 	 0.00028 	 0.00625 	 ~...
    0 	     7 	 0.00028 	 0.00631 	 ~...
    0 	     8 	 0.00028 	 0.00633 	 ~...
    0 	     9 	 0.00028 	 0.00634 	 ~...
    0 	    10 	 0.00028 	 0.00634 	 ~...
    0 	    11 	 0.00028 	 0.00636 	 ~...
    0 	    12 	 0.00028 	 0.00636 	 ~...
    0 	    13 	 0.00028 	 0.00637 	 ~...
    0 	    14 	 0.00028 	 0.00638 	 ~...
    0 	    15 	 0.00028 	 0.00638 	 ~...
    0 	    16 	 0.00028 	 0.00638 	 ~...
    0 	    17 	 0.00028 	 0.00639 	 ~...
    0 	    18 	 0.00028 	 0.00639 	 ~...
    0 	    19 	 0.00028 	 0.00640 	 ~...
    0 	    20 	 0.00028 	 0.00641 	 ~...
    0 	    21 	 0.00028 	 0.00642 	 ~...
    0 	    22 	 0.00028 	 0.00644 	 ~...
    0 	    23 	 0.00028 	 0.00644 	 ~...
    0 	    24 	 0.00028 	 0.00644 	 ~...
    0 	    25 	 0.00028 	 0.00645 	 ~...
    0 	    26 	 0.00028 	 0.00647 	 ~...
    0 	    27 	 0.00028 	 0.00649 	 ~...
    0 	    28 	 0.00028 	 0.00652 	 ~...
    0 	    29 	 0.00028 	 0.00652 	 ~...
    0 	    30 	 0.00028 	 0.00653 	 ~...
    0 	    31 	 0.00028 	 0.00653 	 ~...
    0 	    32 	 0.00028 	 0.00653 	 ~...
    0 	    33 	 0.00028 	 0.00655 	 ~...
    0 	    34 	 0.00028 	 0.00657 	 ~...
    0 	    35 	 0.00028 	 0.00660 	 ~...
    0 	    36 	 0.00028 	 0.00660 	 ~...
    0 	    37 	 0.00028 	 0.00660 	 ~...
    0 	    38 	 0.00028 	 0.00668 	 ~...
    0 	    39 	 0.00028 	 0.00670 	 ~...
    0 	    40 	 0.00028 	 0.00671 	 ~...
    0 	    41 	 0.00028 	 0.00680 	 ~...
    0 	    42 	 0.00028 	 0.00967 	 ~...
    0 	    43 	 0.00028 	 0.01007 	 ~...
    0 	    44 	 0.00028 	 0.01063 	 ~...
    0 	    45 	 0.00028 	 0.01147 	 ~...
    0 	    46 	 0.00028 	 0.01194 	 ~...
    0 	    47 	 0.00028 	 0.01244 	 ~...
    0 	    48 	 0.00028 	 0.01346 	 ~...
    0 	    49 	 0.00028 	 0.01451 	 ~...
    0 	    50 	 0.00028 	 0.01671 	 ~...
    0 	    51 	 0.00028 	 0.01758 	 ~...
    0 	    52 	 0.00028 	 0.01894 	 ~...
    0 	    53 	 0.00028 	 0.01989 	 ~...
    0 	    54 	 0.00028 	 0.02637 	 ~...
    0 	    55 	 0.00028 	 0.02772 	 ~...
    0 	    56 	 0.00028 	 0.02805 	 ~...
    0 	    57 	 0.00028 	 0.03288 	 m..s
    0 	    58 	 0.00028 	 0.03526 	 m..s
    0 	    59 	 0.00028 	 0.03588 	 m..s
    0 	    60 	 0.00028 	 0.03682 	 m..s
    0 	    61 	 0.00028 	 0.03738 	 m..s
    0 	    62 	 0.00028 	 0.04121 	 m..s
    0 	    63 	 0.00028 	 0.04195 	 m..s
    0 	    64 	 0.00028 	 0.04971 	 m..s
    0 	    65 	 0.00028 	 0.05157 	 m..s
    0 	    66 	 0.00028 	 0.05170 	 m..s
    0 	    67 	 0.00028 	 0.05202 	 m..s
    0 	    68 	 0.00028 	 0.05713 	 m..s
    0 	    69 	 0.00028 	 0.06534 	 m..s
    0 	    70 	 0.00028 	 0.06589 	 m..s
    0 	    71 	 0.00028 	 0.06673 	 m..s
    0 	    72 	 0.00028 	 0.07412 	 m..s
    0 	    73 	 0.00028 	 0.08075 	 m..s
    0 	    74 	 0.00028 	 0.08095 	 m..s
    0 	    75 	 0.00028 	 0.08434 	 m..s
    0 	    76 	 0.00028 	 0.08447 	 m..s
    0 	    77 	 0.00028 	 0.08550 	 m..s
    0 	    78 	 0.00028 	 0.08613 	 m..s
    0 	    79 	 0.00028 	 0.08662 	 m..s
    0 	    80 	 0.00028 	 0.08716 	 m..s
    0 	    81 	 0.00028 	 0.08816 	 m..s
    0 	    82 	 0.00028 	 0.08825 	 m..s
    0 	    83 	 0.00028 	 0.08861 	 m..s
    0 	    84 	 0.00028 	 0.08938 	 m..s
    0 	    85 	 0.00028 	 0.08995 	 m..s
    0 	    86 	 0.00028 	 0.09192 	 m..s
    0 	    87 	 0.00028 	 0.09330 	 m..s
    0 	    88 	 0.00028 	 0.09453 	 m..s
    0 	    89 	 0.00028 	 0.09469 	 m..s
    0 	    90 	 0.00028 	 0.09616 	 m..s
    0 	    91 	 0.00028 	 0.09631 	 m..s
    0 	    92 	 0.00028 	 0.09713 	 m..s
    0 	    93 	 0.00028 	 0.09751 	 m..s
    0 	    94 	 0.00028 	 0.10270 	 MISS
    0 	    95 	 0.00028 	 0.10397 	 MISS
    0 	    96 	 0.00028 	 0.10479 	 MISS
    0 	    97 	 0.00028 	 0.10491 	 MISS
    0 	    98 	 0.00028 	 0.10534 	 MISS
    0 	    99 	 0.00028 	 0.10549 	 MISS
    0 	   100 	 0.00028 	 0.10791 	 MISS
    0 	   101 	 0.00028 	 0.11213 	 MISS
    0 	   102 	 0.00028 	 0.11240 	 MISS
    0 	   103 	 0.00028 	 0.11297 	 MISS
    0 	   104 	 0.00028 	 0.11573 	 MISS
    0 	   105 	 0.00028 	 0.11730 	 MISS
    0 	   106 	 0.00028 	 0.11815 	 MISS
    0 	   107 	 0.00028 	 0.12089 	 MISS
    0 	   108 	 0.00028 	 0.14016 	 MISS
    0 	   109 	 0.00028 	 0.14556 	 MISS
    0 	   110 	 0.00028 	 0.17938 	 MISS
    0 	   111 	 0.00028 	 0.19077 	 MISS
    0 	   112 	 0.00028 	 0.20024 	 MISS
    0 	   113 	 0.00028 	 0.20882 	 MISS
    0 	   114 	 0.00028 	 0.21706 	 MISS
    0 	   115 	 0.00028 	 0.26451 	 MISS
    0 	   116 	 0.00028 	 0.27685 	 MISS
    0 	   117 	 0.00028 	 0.28145 	 MISS
    0 	   118 	 0.00028 	 0.29075 	 MISS
    0 	   119 	 0.00028 	 0.29402 	 MISS
    0 	   120 	 0.00028 	 0.29431 	 MISS
==========================================
r_mrr = nan
r2_mrr = -0.7896822690963745
spearmanr_mrr@5 = nan
spearmanr_mrr@10 = nan
spearmanr_mrr@50 = -1.1127261956289658e-07
spearmanr_mrr@100 = 1.018657300733139e-07
spearmanr_mrr@All = nan
==========================================
test time: 0.387
Done Testing dataset OpenEA
total time taken: 196.34677362442017
training time taken: 180.78485870361328
TWIG out ;))
Ablation done!
The best results were: None
The best settings found were:

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 8649224008780068
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [944, 1089, 595, 390, 42, 649, 1207, 196, 515, 280, 768, 566, 484, 377, 696, 587, 1112, 1182, 1162, 1206, 347, 834, 364, 109, 983, 1023, 873, 557, 205, 240, 769, 1148, 640, 313, 780, 69, 1046, 284, 739, 679, 968, 841, 363, 234, 705, 775, 389, 576, 1075, 92, 233, 692, 228, 934, 898, 1141, 79, 182, 247, 1170, 597, 1122, 187, 1103, 999, 637, 848, 1050, 671, 41, 810, 1131, 645, 1150, 1078, 74, 37, 307, 3, 132, 817, 517, 842, 475, 1077, 913, 266, 838, 615, 106, 844, 967, 55, 312, 948, 904, 612, 231, 851, 641, 631, 764, 700, 716, 1049, 93, 894, 489, 925, 202, 1011, 658, 404, 620, 90, 607, 1191, 1124, 388, 476, 682]
valid_ids (0): []
train_ids (1094): [1203, 467, 334, 384, 580, 1095, 431, 353, 103, 283, 878, 57, 808, 602, 984, 865, 112, 1054, 168, 559, 793, 354, 500, 1094, 448, 456, 748, 308, 26, 507, 603, 840, 1096, 606, 117, 110, 237, 752, 830, 754, 585, 1193, 221, 563, 327, 734, 323, 293, 1066, 571, 1187, 742, 573, 674, 446, 230, 211, 1137, 1022, 1045, 782, 1163, 397, 115, 497, 877, 1083, 723, 922, 625, 963, 341, 1130, 525, 63, 686, 1149, 708, 709, 857, 905, 181, 490, 596, 920, 67, 383, 1060, 744, 798, 733, 1038, 111, 164, 1195, 14, 568, 550, 369, 335, 47, 521, 959, 660, 1012, 1169, 1108, 1192, 36, 470, 78, 1210, 81, 766, 207, 940, 486, 1194, 462, 243, 303, 1025, 324, 522, 320, 281, 439, 755, 222, 249, 860, 460, 425, 670, 888, 778, 264, 31, 511, 1073, 896, 1009, 415, 1070, 893, 1074, 491, 368, 1000, 217, 609, 836, 342, 652, 590, 238, 608, 478, 1185, 1111, 1105, 1015, 185, 884, 1091, 929, 84, 482, 450, 188, 720, 169, 150, 676, 982, 560, 1040, 627, 474, 819, 909, 882, 891, 794, 972, 965, 455, 405, 229, 866, 886, 427, 96, 361, 738, 225, 371, 424, 911, 776, 139, 166, 728, 367, 1199, 711, 788, 813, 214, 235, 91, 971, 492, 178, 773, 97, 791, 897, 287, 680, 1209, 402, 195, 310, 916, 72, 862, 823, 206, 879, 105, 444, 758, 1005, 158, 663, 726, 138, 691, 378, 694, 732, 128, 296, 21, 39, 787, 1160, 902, 989, 693, 80, 129, 531, 756, 582, 814, 820, 907, 262, 95, 970, 518, 969, 535, 272, 688, 433, 359, 1063, 703, 487, 642, 116, 58, 459, 120, 43, 113, 339, 1202, 761, 466, 1175, 244, 942, 1064, 19, 956, 662, 173, 1153, 108, 547, 251, 1028, 1177, 1126, 365, 509, 689, 978, 70, 148, 976, 118, 863, 122, 928, 1084, 99, 328, 953, 825, 549, 245, 598, 858, 849, 604, 1183, 1110, 165, 410, 100, 301, 561, 23, 512, 88, 1115, 765, 643, 73, 85, 736, 1180, 1161, 1188, 1184, 721, 695, 375, 917, 151, 623, 393, 1020, 534, 687, 685, 48, 322, 161, 937, 1013, 938, 520, 458, 387, 83, 56, 827, 713, 815, 29, 380, 204, 346, 718, 599, 437, 505, 629, 803, 302, 852, 1156, 741, 993, 605, 762, 174, 297, 579, 870, 943, 781, 628, 1007, 149, 701, 480, 140, 386, 853, 1057, 1154, 826, 831, 892, 715, 1164, 316, 868, 610, 331, 519, 213, 17, 880, 408, 1113, 1138, 481, 918, 20, 786, 259, 574, 276, 406, 15, 528, 1098, 179, 900, 1143, 832, 632, 44, 381, 1068, 966, 921, 1076, 50, 1213, 554, 919, 89, 270, 294, 667, 600, 472, 1071, 382, 309, 385, 1142, 1065, 1168, 923, 634, 1003, 184, 876, 1147, 747, 1128, 792, 170, 783, 398, 273, 952, 453, 946, 6, 1, 871, 532, 7, 552, 22, 134, 413, 958, 4, 1114, 1006, 1106, 1031, 49, 40, 278, 1211, 936, 931, 10, 712, 160, 1198, 617, 477, 318, 68, 469, 275, 30, 583, 290, 445, 355, 556, 332, 947, 435, 86, 1109, 45, 414, 60, 565, 1146, 291, 451, 465, 949, 881, 153, 906, 197, 760, 508, 1008, 1041, 344, 699, 1086, 855, 1033, 219, 770, 910, 636, 990, 471, 570, 669, 422, 429, 960, 601, 421, 653, 510, 730, 821, 927, 802, 659, 325, 975, 1201, 1061, 423, 1181, 933, 553, 194, 1107, 1117, 254, 357, 434, 32, 856, 449, 621, 416, 321, 227, 763, 183, 1173, 704, 403, 533, 717, 572, 145, 767, 1082, 1048, 499, 805, 498, 279, 784, 1099, 666, 1059, 0, 1152, 156, 209, 1055, 816, 930, 1030, 447, 282, 76, 980, 883, 162, 504, 119, 804, 33, 678, 167, 1212, 1190, 986, 376, 874, 11, 530, 1035, 647, 981, 955, 1039, 286, 1140, 267, 53, 991, 198, 845, 27, 724, 176, 1090, 1085, 330, 13, 630, 65, 1016, 352, 241, 391, 277, 903, 172, 319, 208, 51, 1021, 562, 463, 690, 295, 239, 646, 257, 861, 996, 1166, 464, 396, 624, 524, 224, 189, 885, 616, 345, 326, 289, 785, 260, 746, 300, 232, 1136, 985, 1157, 141, 1024, 593, 314, 664, 443, 180, 25, 697, 974, 255, 87, 142, 672, 650, 226, 835, 157, 250, 454, 1018, 348, 154, 581, 992, 797, 638, 75, 253, 268, 311, 867, 722, 392, 1139, 542, 926, 588, 263, 789, 483, 1134, 1127, 1056, 473, 774, 411, 35, 751, 107, 432, 753, 159, 513, 932, 875, 673, 529, 358, 9, 1125, 537, 1176, 743, 125, 1159, 1120, 997, 199, 1155, 1004, 133, 790, 190, 292, 1051, 24, 1135, 98, 5, 1123, 592, 683, 812, 379, 1072, 635, 135, 1104, 274, 757, 502, 644, 901, 555, 1034, 102, 681, 366, 759, 372, 962, 1208, 495, 941, 523, 727, 548, 258, 1119, 305, 1179, 2, 1037, 977, 661, 1133, 828, 771, 1174, 1052, 436, 419, 203, 514, 216, 218, 104, 137, 220, 236, 1043, 210, 1097, 38, 614, 987, 468, 362, 964, 847, 1116, 725, 114, 833, 503, 1027, 577, 698, 77, 401, 155, 737, 795, 1014, 527, 146, 915, 315, 144, 409, 1019, 546, 538, 558, 1214, 126, 806, 412, 945, 998, 749, 1204, 261, 796, 285, 957, 123, 200, 1165, 370, 655, 1069, 850, 54, 269, 924, 64, 488, 130, 740, 1145, 356, 1151, 418, 1017, 1081, 163, 540, 1087, 818, 1053, 622, 799, 706, 246, 594, 317, 651, 395, 1062, 611, 506, 995, 61, 545, 417, 714, 306, 543, 252, 399, 1158, 843, 526, 961, 908, 567, 299, 899, 101, 340, 811, 801, 750, 1029, 1196, 461, 338, 201, 1172, 62, 824, 152, 648, 349, 360, 452, 46, 973, 939, 177, 1100, 1079, 800, 854, 485, 677, 440, 1205, 829, 591, 864, 1186, 131, 1132, 1044, 654, 639, 979, 336, 1121, 1088, 428, 374, 869, 442, 1092, 337, 1001, 890, 1042, 59, 872, 496, 895, 304, 288, 516, 951, 719, 501, 1002, 626, 578, 575, 66, 1101, 430, 1189, 136, 988, 1058, 541, 400, 950, 256, 564, 121, 16, 589, 407, 1178, 657, 772, 1171, 889, 551, 777, 186, 1129, 668, 1026, 192, 729, 809, 242, 351, 1010, 675, 215, 420, 438, 143, 1144, 1093, 684, 1167, 613, 1197, 394, 994, 171, 618, 779, 707, 193, 954, 493, 457, 223, 586, 248, 147, 702, 731, 536, 18, 8, 656, 1032, 191, 569, 265, 887, 1067, 494, 373, 912, 822, 329, 271, 1200, 544, 12, 479, 665, 837, 1080, 1036, 584, 633, 82, 333, 94, 212, 846, 619, 350, 71, 34, 745, 441, 914, 807, 735, 127, 124, 1047, 52, 28, 426, 175, 298, 1102, 710, 839, 935, 343, 1118, 859, 539]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3889179129344006
the save name prefix for this run is:  chkpt-ID_3889179129344006_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 783
rank avg (pred): 0.547 +- 0.004
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001814733

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 903
rank avg (pred): 0.125 +- 0.088
mrr vals (pred, true): 0.023, 0.095
batch losses (mrrl, rdl): 0.0, 8.89776e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 33
rank avg (pred): 0.070 +- 0.061
mrr vals (pred, true): 0.156, 0.066
batch losses (mrrl, rdl): 0.0, 4.13463e-05

Epoch over!
epoch time: 12.123

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1156
rank avg (pred): 0.134 +- 0.120
mrr vals (pred, true): 0.134, 0.107
batch losses (mrrl, rdl): 0.0, 5.7744e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 918
rank avg (pred): 0.372 +- 0.319
mrr vals (pred, true): 0.072, 0.097
batch losses (mrrl, rdl): 0.0, 0.0003071961

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1081
rank avg (pred): 0.302 +- 0.274
mrr vals (pred, true): 0.094, 0.070
batch losses (mrrl, rdl): 0.0, 0.0003421796

Epoch over!
epoch time: 11.782

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 449
rank avg (pred): 0.347 +- 0.301
mrr vals (pred, true): 0.074, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002299957

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 632
rank avg (pred): 0.350 +- 0.321
mrr vals (pred, true): 0.107, 0.035
batch losses (mrrl, rdl): 0.0, 5.78014e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 453
rank avg (pred): 0.346 +- 0.303
mrr vals (pred, true): 0.081, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002266412

Epoch over!
epoch time: 11.812

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 487
rank avg (pred): 0.174 +- 0.159
mrr vals (pred, true): 0.118, 0.069
batch losses (mrrl, rdl): 0.0, 5.2763e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 800
rank avg (pred): 0.357 +- 0.317
mrr vals (pred, true): 0.077, 0.007
batch losses (mrrl, rdl): 0.0, 0.000173333

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 96
rank avg (pred): 0.361 +- 0.316
mrr vals (pred, true): 0.085, 0.013
batch losses (mrrl, rdl): 0.0, 0.0002819814

Epoch over!
epoch time: 12.025

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 653
rank avg (pred): 0.364 +- 0.323
mrr vals (pred, true): 0.092, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001893664

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 224
rank avg (pred): 0.324 +- 0.294
mrr vals (pred, true): 0.111, 0.006
batch losses (mrrl, rdl): 0.0, 0.000318264

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.368 +- 0.321
mrr vals (pred, true): 0.083, 0.037
batch losses (mrrl, rdl): 0.0, 8.92416e-05

Epoch over!
epoch time: 11.928

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.046 +- 0.044
mrr vals (pred, true): 0.191, 0.301
batch losses (mrrl, rdl): 0.1211752817, 6.4369e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 311
rank avg (pred): 0.026 +- 0.016
mrr vals (pred, true): 0.100, 0.158
batch losses (mrrl, rdl): 0.0336210206, 3.78283e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 966
rank avg (pred): 0.417 +- 0.231
mrr vals (pred, true): 0.043, 0.007
batch losses (mrrl, rdl): 0.0004692154, 6.38299e-05

Epoch over!
epoch time: 12.133

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 573
rank avg (pred): 0.387 +- 0.241
mrr vals (pred, true): 0.057, 0.008
batch losses (mrrl, rdl): 0.0005132623, 5.96181e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 874
rank avg (pred): 0.391 +- 0.233
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 3.5102e-05, 0.0001158334

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 837
rank avg (pred): 0.379 +- 0.241
mrr vals (pred, true): 0.049, 0.096
batch losses (mrrl, rdl): 7.0714e-06, 0.0003378612

Epoch over!
epoch time: 12.033

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 219
rank avg (pred): 0.367 +- 0.242
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 7.44087e-05, 0.000224678

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1099
rank avg (pred): 0.237 +- 0.156
mrr vals (pred, true): 0.055, 0.072
batch losses (mrrl, rdl): 0.0002145121, 9.9961e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 807
rank avg (pred): 0.394 +- 0.232
mrr vals (pred, true): 0.041, 0.006
batch losses (mrrl, rdl): 0.0007554374, 0.0001173007

Epoch over!
epoch time: 12.049

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.409 +- 0.238
mrr vals (pred, true): 0.039, 0.037
batch losses (mrrl, rdl): 0.0012982718, 0.0002132786

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 355
rank avg (pred): 0.390 +- 0.230
mrr vals (pred, true): 0.049, 0.028
batch losses (mrrl, rdl): 9.6733e-06, 0.000616021

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 880
rank avg (pred): 0.377 +- 0.211
mrr vals (pred, true): 0.048, 0.007
batch losses (mrrl, rdl): 3.64597e-05, 0.0002028262

Epoch over!
epoch time: 11.947

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 50
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.258, 0.275
batch losses (mrrl, rdl): 0.0030119587, 4.01857e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 104
rank avg (pred): 0.281 +- 0.190
mrr vals (pred, true): 0.059, 0.042
batch losses (mrrl, rdl): 0.0008706574, 0.0001661288

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 263
rank avg (pred): 0.008 +- 0.007
mrr vals (pred, true): 0.248, 0.276
batch losses (mrrl, rdl): 0.0080199279, 3.61258e-05

Epoch over!
epoch time: 12.04

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.044 +- 0.033
mrr vals (pred, true): 0.131, 0.109
batch losses (mrrl, rdl): 0.0048262868, 0.0003267505

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 352
rank avg (pred): 0.353 +- 0.196
mrr vals (pred, true): 0.056, 0.029
batch losses (mrrl, rdl): 0.0003983003, 0.0003716931

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 304
rank avg (pred): 0.078 +- 0.057
mrr vals (pred, true): 0.102, 0.077
batch losses (mrrl, rdl): 0.0269523934, 2.8834e-05

Epoch over!
epoch time: 12.231

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 299
rank avg (pred): 0.024 +- 0.018
mrr vals (pred, true): 0.162, 0.110
batch losses (mrrl, rdl): 0.0273311045, 7.95287e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 845
rank avg (pred): 0.373 +- 0.188
mrr vals (pred, true): 0.048, 0.085
batch losses (mrrl, rdl): 3.22372e-05, 0.0001822839

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 128
rank avg (pred): 0.246 +- 0.165
mrr vals (pred, true): 0.072, 0.043
batch losses (mrrl, rdl): 0.0048517482, 9.99875e-05

Epoch over!
epoch time: 12.207

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1213
rank avg (pred): 0.373 +- 0.191
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 5.6773e-06, 0.000273978

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 871
rank avg (pred): 0.378 +- 0.183
mrr vals (pred, true): 0.039, 0.007
batch losses (mrrl, rdl): 0.0012781769, 0.0002314747

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 805
rank avg (pred): 0.361 +- 0.178
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 3.61417e-05, 0.000311445

Epoch over!
epoch time: 12.258

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 100
rank avg (pred): 0.383 +- 0.207
mrr vals (pred, true): 0.058, 0.022
batch losses (mrrl, rdl): 0.0007102271, 0.0005229543

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 111
rank avg (pred): 0.376 +- 0.190
mrr vals (pred, true): 0.052, 0.012
batch losses (mrrl, rdl): 4.156e-05, 0.0002514648

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 491
rank avg (pred): 0.199 +- 0.102
mrr vals (pred, true): 0.052, 0.094
batch losses (mrrl, rdl): 5.44414e-05, 9.59764e-05

Epoch over!
epoch time: 12.141

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 899
rank avg (pred): 0.120 +- 0.069
mrr vals (pred, true): 0.071, 0.105
batch losses (mrrl, rdl): 0.0114457589, 5.2879e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1071
rank avg (pred): 0.036 +- 0.026
mrr vals (pred, true): 0.252, 0.216
batch losses (mrrl, rdl): 0.0128754647, 1.30948e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 519
rank avg (pred): 0.385 +- 0.203
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 1.45152e-05, 0.0006050512

Epoch over!
epoch time: 12.005

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.347 +- 0.195
mrr vals (pred, true): 0.046, 0.093

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04604 	 0.00600 	 m..s
    0 	     1 	 0.04604 	 0.00613 	 m..s
  104 	     2 	 0.08608 	 0.00619 	 m..s
    0 	     3 	 0.04604 	 0.00620 	 m..s
    0 	     4 	 0.04604 	 0.00625 	 m..s
    0 	     5 	 0.04604 	 0.00627 	 m..s
    0 	     6 	 0.04604 	 0.00629 	 m..s
    0 	     7 	 0.04604 	 0.00629 	 m..s
   78 	     8 	 0.05276 	 0.00633 	 m..s
    0 	     9 	 0.04604 	 0.00634 	 m..s
  101 	    10 	 0.08065 	 0.00634 	 m..s
   71 	    11 	 0.04858 	 0.00634 	 m..s
    0 	    12 	 0.04604 	 0.00634 	 m..s
    0 	    13 	 0.04604 	 0.00637 	 m..s
    0 	    14 	 0.04604 	 0.00640 	 m..s
    0 	    15 	 0.04604 	 0.00641 	 m..s
    0 	    16 	 0.04604 	 0.00642 	 m..s
    0 	    17 	 0.04604 	 0.00642 	 m..s
   67 	    18 	 0.04828 	 0.00644 	 m..s
    0 	    19 	 0.04604 	 0.00644 	 m..s
    0 	    20 	 0.04604 	 0.00644 	 m..s
    0 	    21 	 0.04604 	 0.00646 	 m..s
    0 	    22 	 0.04604 	 0.00647 	 m..s
   62 	    23 	 0.04723 	 0.00647 	 m..s
   88 	    24 	 0.06122 	 0.00648 	 m..s
   96 	    25 	 0.07944 	 0.00648 	 m..s
    0 	    26 	 0.04604 	 0.00651 	 m..s
   83 	    27 	 0.05788 	 0.00652 	 m..s
    0 	    28 	 0.04604 	 0.00652 	 m..s
    0 	    29 	 0.04604 	 0.00660 	 m..s
   96 	    30 	 0.07944 	 0.00660 	 m..s
   84 	    31 	 0.05797 	 0.00660 	 m..s
   68 	    32 	 0.04831 	 0.00660 	 m..s
    0 	    33 	 0.04604 	 0.00661 	 m..s
   69 	    34 	 0.04854 	 0.00663 	 m..s
   63 	    35 	 0.04735 	 0.00664 	 m..s
    0 	    36 	 0.04604 	 0.00680 	 m..s
    0 	    37 	 0.04604 	 0.00805 	 m..s
    0 	    38 	 0.04604 	 0.00961 	 m..s
    0 	    39 	 0.04604 	 0.00990 	 m..s
    0 	    40 	 0.04604 	 0.01013 	 m..s
    0 	    41 	 0.04604 	 0.01052 	 m..s
    0 	    42 	 0.04604 	 0.01118 	 m..s
    0 	    43 	 0.04604 	 0.01345 	 m..s
    0 	    44 	 0.04604 	 0.01346 	 m..s
    0 	    45 	 0.04604 	 0.01355 	 m..s
    0 	    46 	 0.04604 	 0.01356 	 m..s
    0 	    47 	 0.04604 	 0.01398 	 m..s
   64 	    48 	 0.04738 	 0.01523 	 m..s
    0 	    49 	 0.04604 	 0.01591 	 m..s
    0 	    50 	 0.04604 	 0.01698 	 ~...
    0 	    51 	 0.04604 	 0.02118 	 ~...
    0 	    52 	 0.04604 	 0.02143 	 ~...
   70 	    53 	 0.04858 	 0.02153 	 ~...
    0 	    54 	 0.04604 	 0.03177 	 ~...
    0 	    55 	 0.04604 	 0.03243 	 ~...
    0 	    56 	 0.04604 	 0.03290 	 ~...
    0 	    57 	 0.04604 	 0.03445 	 ~...
    0 	    58 	 0.04604 	 0.03449 	 ~...
    0 	    59 	 0.04604 	 0.03578 	 ~...
    0 	    60 	 0.04604 	 0.03717 	 ~...
   65 	    61 	 0.04819 	 0.04039 	 ~...
    0 	    62 	 0.04604 	 0.05157 	 ~...
   81 	    63 	 0.05752 	 0.05596 	 ~...
    0 	    64 	 0.04604 	 0.05917 	 ~...
   95 	    65 	 0.07837 	 0.06429 	 ~...
   76 	    66 	 0.05230 	 0.06528 	 ~...
   66 	    67 	 0.04822 	 0.06602 	 ~...
   61 	    68 	 0.04671 	 0.06688 	 ~...
   76 	    69 	 0.05230 	 0.06701 	 ~...
   89 	    70 	 0.06394 	 0.07302 	 ~...
   79 	    71 	 0.05412 	 0.07477 	 ~...
   73 	    72 	 0.05178 	 0.07573 	 ~...
    0 	    73 	 0.04604 	 0.08466 	 m..s
   72 	    74 	 0.05173 	 0.08468 	 m..s
   60 	    75 	 0.04618 	 0.08500 	 m..s
  100 	    76 	 0.07994 	 0.08613 	 ~...
   59 	    77 	 0.04613 	 0.08716 	 m..s
  103 	    78 	 0.08523 	 0.08724 	 ~...
   87 	    79 	 0.06117 	 0.08829 	 ~...
   90 	    80 	 0.06435 	 0.08861 	 ~...
    0 	    81 	 0.04604 	 0.09004 	 m..s
   74 	    82 	 0.05182 	 0.09078 	 m..s
  111 	    83 	 0.13035 	 0.09087 	 m..s
    0 	    84 	 0.04604 	 0.09122 	 m..s
    0 	    85 	 0.04604 	 0.09335 	 m..s
   74 	    86 	 0.05182 	 0.09453 	 m..s
   80 	    87 	 0.05664 	 0.09486 	 m..s
   93 	    88 	 0.07183 	 0.09495 	 ~...
    0 	    89 	 0.04604 	 0.09498 	 m..s
    0 	    90 	 0.04604 	 0.09646 	 m..s
    0 	    91 	 0.04604 	 0.09660 	 m..s
    0 	    92 	 0.04604 	 0.09671 	 m..s
    0 	    93 	 0.04604 	 0.09679 	 m..s
    0 	    94 	 0.04604 	 0.09697 	 m..s
    0 	    95 	 0.04604 	 0.09751 	 m..s
   58 	    96 	 0.04607 	 0.10022 	 m..s
   82 	    97 	 0.05761 	 0.10147 	 m..s
   94 	    98 	 0.07424 	 0.10352 	 ~...
   91 	    99 	 0.06907 	 0.10361 	 m..s
   86 	   100 	 0.05851 	 0.10384 	 m..s
   85 	   101 	 0.05835 	 0.10386 	 m..s
   91 	   102 	 0.06907 	 0.10414 	 m..s
  106 	   103 	 0.08735 	 0.10582 	 ~...
  107 	   104 	 0.09117 	 0.10631 	 ~...
  108 	   105 	 0.12650 	 0.10837 	 ~...
  108 	   106 	 0.12650 	 0.11189 	 ~...
   98 	   107 	 0.07964 	 0.12039 	 m..s
  102 	   108 	 0.08084 	 0.12726 	 m..s
   98 	   109 	 0.07964 	 0.12784 	 m..s
  105 	   110 	 0.08633 	 0.14474 	 m..s
  112 	   111 	 0.13561 	 0.15691 	 ~...
  113 	   112 	 0.13879 	 0.16702 	 ~...
  114 	   113 	 0.15349 	 0.17497 	 ~...
  110 	   114 	 0.12975 	 0.20766 	 m..s
  118 	   115 	 0.25467 	 0.21936 	 m..s
  115 	   116 	 0.22183 	 0.26458 	 m..s
  117 	   117 	 0.25258 	 0.27280 	 ~...
  116 	   118 	 0.23728 	 0.27746 	 m..s
  119 	   119 	 0.28610 	 0.29048 	 ~...
  120 	   120 	 0.28622 	 0.29280 	 ~...
==========================================
r_mrr = 0.8307464122772217
r2_mrr = 0.6736290454864502
spearmanr_mrr@5 = 0.9902173280715942
spearmanr_mrr@10 = 0.9739130139350891
spearmanr_mrr@50 = 0.9890096187591553
spearmanr_mrr@100 = 0.912511944770813
spearmanr_mrr@All = 0.9033453464508057
==========================================
test time: 0.465
Done Testing dataset OpenEA
total time taken: 196.7461268901825
training time taken: 181.26076531410217
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8307)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.6736)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9902)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9739)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9890)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9125)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9033)}}, 'test_loss': {'TransE': {'OpenEA': 0.5470869416458299}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 4406815375814997
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [761, 505, 562, 176, 275, 946, 1057, 65, 677, 435, 1003, 316, 629, 6, 493, 469, 1126, 973, 972, 662, 170, 1052, 547, 245, 63, 947, 260, 305, 796, 412, 873, 471, 130, 307, 1025, 88, 293, 1213, 67, 780, 958, 341, 579, 1087, 1013, 957, 552, 922, 621, 474, 857, 681, 371, 1118, 168, 504, 1082, 393, 320, 81, 620, 941, 935, 416, 915, 660, 657, 489, 464, 5, 1069, 968, 659, 672, 954, 224, 930, 1122, 34, 1092, 799, 899, 771, 337, 793, 348, 907, 249, 548, 124, 826, 1059, 154, 346, 928, 936, 1074, 196, 248, 906, 634, 481, 719, 51, 667, 312, 1047, 1184, 352, 292, 1155, 580, 1066, 46, 19, 955, 155, 298, 965, 537, 628]
valid_ids (0): []
train_ids (1094): [363, 429, 439, 769, 649, 66, 1120, 1166, 756, 752, 492, 15, 299, 171, 160, 706, 856, 134, 49, 209, 576, 1015, 453, 267, 318, 701, 656, 90, 210, 240, 705, 655, 982, 236, 456, 1199, 151, 21, 286, 809, 120, 324, 904, 676, 811, 712, 84, 38, 447, 863, 483, 0, 678, 404, 24, 496, 508, 1127, 482, 690, 526, 165, 1007, 396, 525, 45, 633, 715, 909, 1108, 1186, 114, 1170, 1147, 564, 903, 913, 1188, 700, 779, 397, 218, 709, 432, 823, 524, 1048, 441, 78, 478, 458, 843, 617, 116, 687, 1102, 1002, 1114, 201, 142, 688, 626, 446, 204, 50, 350, 250, 514, 355, 83, 306, 668, 268, 69, 1131, 152, 1138, 383, 836, 574, 1113, 1130, 1169, 644, 121, 82, 1104, 539, 786, 242, 625, 992, 635, 586, 553, 43, 994, 42, 716, 175, 686, 596, 1094, 459, 259, 444, 1124, 327, 911, 332, 815, 178, 421, 588, 842, 300, 787, 1111, 281, 797, 729, 875, 1078, 1075, 1037, 1010, 926, 816, 520, 311, 472, 1183, 859, 892, 463, 1070, 180, 379, 543, 764, 261, 732, 531, 1165, 94, 989, 422, 1099, 658, 365, 1021, 1091, 282, 96, 169, 1089, 238, 1054, 97, 174, 288, 987, 122, 889, 64, 228, 833, 960, 146, 618, 361, 362, 1152, 244, 399, 415, 713, 411, 727, 212, 1088, 473, 25, 452, 289, 74, 339, 794, 931, 841, 380, 791, 647, 751, 883, 358, 1171, 943, 87, 343, 623, 517, 369, 652, 241, 106, 101, 52, 1020, 527, 637, 516, 325, 825, 1022, 1158, 1036, 849, 3, 1204, 717, 1190, 986, 375, 1051, 840, 592, 254, 1045, 347, 129, 434, 227, 1, 1129, 486, 126, 322, 491, 17, 118, 860, 1016, 753, 185, 133, 247, 231, 874, 297, 726, 56, 378, 1205, 609, 253, 1042, 1153, 988, 1049, 419, 974, 949, 942, 30, 1145, 1001, 387, 739, 742, 758, 226, 923, 1109, 342, 448, 1163, 905, 285, 886, 290, 23, 590, 575, 683, 1062, 150, 394, 119, 179, 95, 561, 790, 140, 895, 1202, 141, 1136, 1083, 1008, 927, 788, 502, 803, 912, 865, 978, 1177, 748, 266, 445, 449, 582, 653, 530, 100, 230, 145, 1157, 1160, 512, 166, 398, 1115, 1116, 924, 798, 484, 1211, 319, 541, 725, 1096, 424, 898, 685, 692, 650, 206, 506, 772, 736, 466, 53, 565, 921, 390, 420, 59, 1105, 345, 832, 1189, 137, 1024, 814, 1072, 808, 645, 433, 349, 619, 465, 213, 567, 1060, 135, 85, 1123, 340, 409, 896, 359, 515, 536, 740, 861, 571, 568, 223, 44, 545, 741, 805, 270, 951, 746, 77, 707, 1173, 953, 829, 890, 884, 99, 559, 669, 1076, 12, 607, 894, 880, 916, 1065, 919, 1162, 551, 1207, 1140, 499, 812, 781, 197, 881, 403, 981, 1117, 377, 188, 354, 721, 125, 891, 211, 164, 893, 643, 1030, 161, 795, 1141, 792, 730, 876, 144, 1084, 388, 428, 785, 186, 670, 356, 554, 997, 112, 32, 1035, 985, 13, 71, 877, 544, 93, 522, 1133, 73, 127, 256, 374, 60, 589, 1101, 1208, 1164, 443, 996, 485, 975, 835, 674, 1156, 747, 11, 392, 866, 616, 538, 479, 631, 405, 494, 334, 869, 959, 804, 367, 1097, 593, 385, 990, 1100, 1142, 262, 837, 172, 711, 39, 333, 167, 57, 183, 901, 470, 878, 55, 979, 132, 980, 680, 689, 819, 1029, 279, 945, 1073, 540, 838, 423, 766, 182, 1139, 1014, 693, 759, 14, 651, 703, 817, 1005, 578, 386, 533, 1023, 9, 1172, 323, 274, 1154, 920, 495, 622, 854, 54, 222, 908, 939, 722, 148, 313, 535, 302, 630, 86, 743, 770, 971, 778, 871, 1004, 303, 1034, 219, 549, 31, 646, 202, 918, 1119, 400, 550, 455, 283, 738, 783, 364, 113, 1018, 1041, 220, 762, 846, 983, 1009, 1144, 440, 1110, 511, 1196, 128, 608, 1081, 239, 189, 357, 407, 143, 902, 984, 602, 47, 1159, 442, 1182, 1178, 696, 29, 910, 308, 20, 581, 4, 208, 207, 317, 115, 666, 867, 532, 572, 105, 1038, 1148, 139, 1193, 431, 569, 914, 7, 451, 627, 510, 757, 710, 1135, 271, 776, 675, 89, 566, 462, 699, 950, 818, 153, 373, 967, 138, 314, 595, 782, 521, 879, 111, 933, 425, 131, 603, 1214, 731, 1187, 682, 252, 760, 70, 612, 853, 698, 10, 92, 872, 828, 16, 488, 413, 868, 615, 583, 1080, 587, 1112, 557, 1149, 110, 370, 376, 654, 468, 372, 177, 735, 401, 845, 745, 149, 1128, 1071, 767, 601, 273, 221, 68, 336, 714, 107, 858, 763, 136, 265, 1200, 956, 1086, 27, 263, 534, 624, 414, 497, 855, 750, 187, 190, 1064, 234, 822, 704, 410, 1046, 1017, 232, 287, 599, 217, 503, 437, 246, 1012, 1137, 278, 614, 36, 807, 1077, 1168, 591, 1006, 847, 528, 870, 938, 391, 802, 203, 184, 368, 821, 1150, 594, 940, 1058, 665, 1151, 673, 691, 103, 108, 810, 1044, 467, 1212, 1180, 830, 834, 734, 944, 1174, 1028, 613, 560, 237, 330, 1167, 1103, 888, 755, 679, 610, 1146, 328, 934, 200, 461, 1203, 329, 296, 806, 156, 848, 585, 76, 784, 684, 215, 487, 917, 104, 948, 584, 258, 952, 475, 315, 885, 1192, 280, 18, 335, 887, 1206, 708, 642, 1053, 22, 661, 862, 418, 62, 162, 507, 850, 26, 1011, 326, 961, 999, 765, 40, 309, 563, 963, 600, 1085, 611, 301, 454, 205, 966, 577, 1161, 864, 546, 1055, 509, 598, 897, 460, 604, 395, 519, 80, 518, 264, 1068, 295, 33, 824, 523, 255, 638, 882, 1132, 243, 158, 664, 1195, 851, 194, 831, 269, 852, 1067, 79, 969, 75, 925, 501, 338, 1033, 389, 1056, 1098, 932, 417, 558, 820, 500, 1179, 789, 937, 498, 480, 1181, 632, 697, 195, 58, 353, 102, 35, 801, 157, 1185, 720, 477, 723, 304, 663, 733, 1191, 702, 277, 294, 37, 402, 1121, 384, 193, 991, 91, 737, 344, 61, 1176, 695, 276, 360, 1043, 1143, 774, 198, 970, 1000, 597, 1079, 408, 284, 671, 436, 28, 331, 718, 199, 694, 117, 381, 1090, 728, 1175, 844, 964, 192, 1027, 773, 1197, 406, 800, 1040, 768, 427, 272, 1063, 542, 827, 724, 555, 48, 775, 1125, 490, 438, 976, 382, 929, 977, 8, 123, 1095, 1210, 1050, 163, 1061, 1031, 556, 998, 321, 98, 1201, 310, 639, 457, 839, 173, 1093, 1026, 529, 214, 1032, 366, 233, 636, 426, 159, 181, 1039, 962, 900, 1194, 570, 640, 291, 41, 191, 2, 351, 1209, 573, 476, 749, 606, 235, 813, 216, 648, 450, 1198, 995, 993, 513, 430, 147, 1134, 72, 777, 1019, 1106, 229, 754, 641, 1107, 744, 109, 225, 257, 605, 251]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7580743448180963
the save name prefix for this run is:  chkpt-ID_7580743448180963_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 497
rank avg (pred): 0.414 +- 0.003
mrr vals (pred, true): 0.000, 0.105
batch losses (mrrl, rdl): 0.0, 0.001308439

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 545
rank avg (pred): 0.206 +- 0.058
mrr vals (pred, true): 0.020, 0.091
batch losses (mrrl, rdl): 0.0, 0.0001139094

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 792
rank avg (pred): 0.341 +- 0.199
mrr vals (pred, true): 0.169, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003545155

Epoch over!
epoch time: 12.05

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 932
rank avg (pred): 0.346 +- 0.207
mrr vals (pred, true): 0.182, 0.097
batch losses (mrrl, rdl): 0.0, 0.0002271399

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 999
rank avg (pred): 0.317 +- 0.229
mrr vals (pred, true): 0.252, 0.056
batch losses (mrrl, rdl): 0.0, 0.0003428089

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 175
rank avg (pred): 0.362 +- 0.259
mrr vals (pred, true): 0.271, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002086622

Epoch over!
epoch time: 11.832

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 26
rank avg (pred): 0.057 +- 0.044
mrr vals (pred, true): 0.312, 0.242
batch losses (mrrl, rdl): 0.0, 7.0603e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1002
rank avg (pred): 0.332 +- 0.278
mrr vals (pred, true): 0.337, 0.086
batch losses (mrrl, rdl): 0.0, 0.0005631812

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 357
rank avg (pred): 0.328 +- 0.290
mrr vals (pred, true): 0.360, 0.020
batch losses (mrrl, rdl): 0.0, 0.0002071815

Epoch over!
epoch time: 11.909

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 283
rank avg (pred): 0.079 +- 0.072
mrr vals (pred, true): 0.379, 0.131
batch losses (mrrl, rdl): 0.0, 1.16219e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 908
rank avg (pred): 0.087 +- 0.078
mrr vals (pred, true): 0.363, 0.111
batch losses (mrrl, rdl): 0.0, 1.8888e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 575
rank avg (pred): 0.335 +- 0.302
mrr vals (pred, true): 0.369, 0.027
batch losses (mrrl, rdl): 0.0, 3.49817e-05

Epoch over!
epoch time: 11.915

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 964
rank avg (pred): 0.348 +- 0.301
mrr vals (pred, true): 0.349, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001761817

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1194
rank avg (pred): 0.380 +- 0.302
mrr vals (pred, true): 0.302, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001273101

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 639
rank avg (pred): 0.369 +- 0.303
mrr vals (pred, true): 0.312, 0.010
batch losses (mrrl, rdl): 0.0, 4.92913e-05

Epoch over!
epoch time: 11.921

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 570
rank avg (pred): 0.372 +- 0.305
mrr vals (pred, true): 0.316, 0.009
batch losses (mrrl, rdl): 0.7078877687, 4.29896e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 543
rank avg (pred): 0.344 +- 0.193
mrr vals (pred, true): 0.066, 0.050
batch losses (mrrl, rdl): 0.0026205881, 0.0004169035

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1129
rank avg (pred): 0.398 +- 0.251
mrr vals (pred, true): 0.078, 0.006
batch losses (mrrl, rdl): 0.0076166098, 0.0001037955

Epoch over!
epoch time: 12.165

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 985
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.251, 0.265
batch losses (mrrl, rdl): 0.001882271, 2.55287e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 467
rank avg (pred): 0.452 +- 0.236
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.0001726696, 2.10116e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 383
rank avg (pred): 0.401 +- 0.241
mrr vals (pred, true): 0.055, 0.055
batch losses (mrrl, rdl): 0.0002761024, 0.0008808285

Epoch over!
epoch time: 12.038

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 287
rank avg (pred): 0.004 +- 0.004
mrr vals (pred, true): 0.194, 0.151
batch losses (mrrl, rdl): 0.018486781, 8.30755e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 256
rank avg (pred): 0.037 +- 0.033
mrr vals (pred, true): 0.124, 0.178
batch losses (mrrl, rdl): 0.0295746475, 4.39896e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1162
rank avg (pred): 0.426 +- 0.223
mrr vals (pred, true): 0.044, 0.037
batch losses (mrrl, rdl): 0.0003250924, 0.0002559987

Epoch over!
epoch time: 12.071

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 565
rank avg (pred): 0.284 +- 0.187
mrr vals (pred, true): 0.076, 0.081
batch losses (mrrl, rdl): 0.0066656037, 0.0002665686

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 58
rank avg (pred): 0.092 +- 0.069
mrr vals (pred, true): 0.097, 0.086
batch losses (mrrl, rdl): 0.0223225746, 1.3191e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 220
rank avg (pred): 0.480 +- 0.213
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001496643, 2.8498e-05

Epoch over!
epoch time: 11.844

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 334
rank avg (pred): 0.419 +- 0.225
mrr vals (pred, true): 0.056, 0.032
batch losses (mrrl, rdl): 0.0004164847, 0.0008690775

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 734
rank avg (pred): 0.093 +- 0.070
mrr vals (pred, true): 0.138, 0.070
batch losses (mrrl, rdl): 0.0776738226, 3.78965e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 998
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.201, 0.282
batch losses (mrrl, rdl): 0.0663572177, 2.20202e-05

Epoch over!
epoch time: 11.862

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 597
rank avg (pred): 0.432 +- 0.183
mrr vals (pred, true): 0.045, 0.008
batch losses (mrrl, rdl): 0.0002158448, 0.0001532791

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 625
rank avg (pred): 0.401 +- 0.170
mrr vals (pred, true): 0.047, 0.012
batch losses (mrrl, rdl): 0.0001122313, 0.0001327217

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 253
rank avg (pred): 0.055 +- 0.039
mrr vals (pred, true): 0.102, 0.180
batch losses (mrrl, rdl): 0.0611349903, 1.7643e-05

Epoch over!
epoch time: 12.082

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 574
rank avg (pred): 0.501 +- 0.181
mrr vals (pred, true): 0.039, 0.011
batch losses (mrrl, rdl): 0.0012217769, 0.0005806966

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 569
rank avg (pred): 0.449 +- 0.192
mrr vals (pred, true): 0.047, 0.024
batch losses (mrrl, rdl): 7.14226e-05, 0.0003497303

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1007
rank avg (pred): 0.211 +- 0.105
mrr vals (pred, true): 0.054, 0.115
batch losses (mrrl, rdl): 0.0372636504, 8.42835e-05

Epoch over!
epoch time: 12.105

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 94
rank avg (pred): 0.474 +- 0.199
mrr vals (pred, true): 0.050, 0.020
batch losses (mrrl, rdl): 2.87e-08, 0.0012373386

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 203
rank avg (pred): 0.286 +- 0.157
mrr vals (pred, true): 0.066, 0.006
batch losses (mrrl, rdl): 0.0024125106, 0.0008348803

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.208, 0.241
batch losses (mrrl, rdl): 0.0106942179, 4.97765e-05

Epoch over!
epoch time: 12.032

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 139
rank avg (pred): 0.446 +- 0.203
mrr vals (pred, true): 0.055, 0.016
batch losses (mrrl, rdl): 0.0002191433, 0.0008393196

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 254
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.301, 0.245
batch losses (mrrl, rdl): 0.0318404362, 5.22609e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1001
rank avg (pred): 0.193 +- 0.097
mrr vals (pred, true): 0.059, 0.052
batch losses (mrrl, rdl): 0.0007881657, 7.51778e-05

Epoch over!
epoch time: 11.95

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 567
rank avg (pred): 0.496 +- 0.145
mrr vals (pred, true): 0.044, 0.009
batch losses (mrrl, rdl): 0.000402995, 0.0004621024

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 444
rank avg (pred): 0.457 +- 0.180
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 9.18816e-05, 5.0921e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 541
rank avg (pred): 0.217 +- 0.105
mrr vals (pred, true): 0.057, 0.064
batch losses (mrrl, rdl): 0.0005287816, 9.39615e-05

Epoch over!
epoch time: 12.167

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.443 +- 0.176
mrr vals (pred, true): 0.051, 0.092

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   73 	     0 	 0.05935 	 0.00572 	 m..s
   46 	     1 	 0.05244 	 0.00597 	 m..s
   65 	     2 	 0.05615 	 0.00598 	 m..s
    5 	     3 	 0.04704 	 0.00604 	 m..s
   80 	     4 	 0.06456 	 0.00609 	 m..s
   78 	     5 	 0.06247 	 0.00618 	 m..s
    0 	     6 	 0.04664 	 0.00619 	 m..s
   44 	     7 	 0.05187 	 0.00619 	 m..s
   60 	     8 	 0.05470 	 0.00621 	 m..s
   20 	     9 	 0.05061 	 0.00627 	 m..s
    1 	    10 	 0.04667 	 0.00629 	 m..s
   13 	    11 	 0.04869 	 0.00630 	 m..s
   43 	    12 	 0.05186 	 0.00631 	 m..s
   17 	    13 	 0.04884 	 0.00633 	 m..s
    6 	    14 	 0.04844 	 0.00634 	 m..s
   67 	    15 	 0.05723 	 0.00636 	 m..s
    3 	    16 	 0.04673 	 0.00637 	 m..s
   31 	    17 	 0.05141 	 0.00638 	 m..s
   55 	    18 	 0.05391 	 0.00640 	 m..s
   47 	    19 	 0.05256 	 0.00640 	 m..s
    7 	    20 	 0.04849 	 0.00641 	 m..s
   75 	    21 	 0.06074 	 0.00641 	 m..s
   55 	    22 	 0.05391 	 0.00642 	 m..s
   19 	    23 	 0.05047 	 0.00642 	 m..s
   51 	    24 	 0.05338 	 0.00644 	 m..s
   24 	    25 	 0.05082 	 0.00647 	 m..s
   36 	    26 	 0.05151 	 0.00647 	 m..s
   36 	    27 	 0.05151 	 0.00648 	 m..s
   29 	    28 	 0.05138 	 0.00649 	 m..s
    9 	    29 	 0.04854 	 0.00653 	 m..s
   27 	    30 	 0.05133 	 0.00653 	 m..s
   31 	    31 	 0.05141 	 0.00654 	 m..s
   55 	    32 	 0.05391 	 0.00655 	 m..s
   79 	    33 	 0.06262 	 0.00656 	 m..s
   45 	    34 	 0.05236 	 0.00657 	 m..s
   10 	    35 	 0.04854 	 0.00658 	 m..s
   83 	    36 	 0.06536 	 0.00660 	 m..s
   49 	    37 	 0.05283 	 0.00662 	 m..s
   14 	    38 	 0.04878 	 0.00664 	 m..s
   29 	    39 	 0.05138 	 0.00668 	 m..s
   18 	    40 	 0.04894 	 0.00878 	 m..s
    2 	    41 	 0.04672 	 0.00961 	 m..s
   26 	    42 	 0.05128 	 0.01096 	 m..s
   16 	    43 	 0.04882 	 0.01120 	 m..s
   15 	    44 	 0.04881 	 0.01376 	 m..s
    4 	    45 	 0.04700 	 0.01413 	 m..s
   42 	    46 	 0.05179 	 0.01591 	 m..s
   28 	    47 	 0.05135 	 0.02041 	 m..s
   25 	    48 	 0.05118 	 0.02048 	 m..s
   22 	    49 	 0.05071 	 0.02126 	 ~...
   40 	    50 	 0.05157 	 0.02198 	 ~...
   33 	    51 	 0.05145 	 0.02283 	 ~...
   11 	    52 	 0.04855 	 0.02440 	 ~...
   48 	    53 	 0.05270 	 0.02938 	 ~...
   53 	    54 	 0.05370 	 0.03326 	 ~...
   53 	    55 	 0.05370 	 0.03365 	 ~...
    8 	    56 	 0.04853 	 0.03449 	 ~...
   52 	    57 	 0.05356 	 0.04195 	 ~...
   70 	    58 	 0.05879 	 0.04500 	 ~...
   61 	    59 	 0.05488 	 0.05157 	 ~...
   23 	    60 	 0.05082 	 0.05956 	 ~...
   62 	    61 	 0.05512 	 0.06178 	 ~...
   72 	    62 	 0.05928 	 0.06311 	 ~...
   71 	    63 	 0.05892 	 0.06472 	 ~...
   63 	    64 	 0.05520 	 0.06655 	 ~...
   68 	    65 	 0.05767 	 0.06932 	 ~...
   64 	    66 	 0.05568 	 0.06963 	 ~...
   99 	    67 	 0.10227 	 0.07296 	 ~...
   82 	    68 	 0.06513 	 0.07447 	 ~...
   74 	    69 	 0.05940 	 0.07777 	 ~...
   97 	    70 	 0.10038 	 0.07908 	 ~...
   94 	    71 	 0.09909 	 0.07981 	 ~...
   98 	    72 	 0.10210 	 0.08414 	 ~...
   90 	    73 	 0.08495 	 0.08861 	 ~...
   76 	    74 	 0.06116 	 0.08938 	 ~...
   12 	    75 	 0.04860 	 0.09121 	 m..s
   85 	    76 	 0.06752 	 0.09162 	 ~...
   21 	    77 	 0.05066 	 0.09181 	 m..s
   66 	    78 	 0.05697 	 0.09192 	 m..s
   81 	    79 	 0.06459 	 0.09215 	 ~...
   34 	    80 	 0.05146 	 0.09497 	 m..s
   39 	    81 	 0.05155 	 0.09585 	 m..s
   35 	    82 	 0.05149 	 0.09588 	 m..s
   41 	    83 	 0.05159 	 0.09687 	 m..s
   58 	    84 	 0.05421 	 0.09750 	 m..s
   38 	    85 	 0.05154 	 0.09790 	 m..s
   50 	    86 	 0.05286 	 0.09837 	 m..s
   92 	    87 	 0.09067 	 0.09837 	 ~...
   59 	    88 	 0.05428 	 0.10022 	 m..s
   77 	    89 	 0.06119 	 0.10397 	 m..s
   94 	    90 	 0.09909 	 0.10432 	 ~...
   69 	    91 	 0.05873 	 0.10469 	 m..s
   84 	    92 	 0.06599 	 0.10632 	 m..s
  103 	    93 	 0.11603 	 0.10640 	 ~...
  105 	    94 	 0.15391 	 0.10675 	 m..s
   88 	    95 	 0.06871 	 0.11297 	 m..s
   87 	    96 	 0.06803 	 0.11353 	 m..s
  107 	    97 	 0.15913 	 0.11730 	 m..s
   89 	    98 	 0.06883 	 0.11752 	 m..s
   86 	    99 	 0.06755 	 0.12399 	 m..s
   94 	   100 	 0.09909 	 0.12784 	 ~...
  100 	   101 	 0.10474 	 0.12970 	 ~...
   91 	   102 	 0.08862 	 0.13739 	 m..s
   93 	   103 	 0.09841 	 0.15132 	 m..s
  112 	   104 	 0.22069 	 0.15929 	 m..s
  111 	   105 	 0.21798 	 0.16005 	 m..s
  104 	   106 	 0.13545 	 0.16380 	 ~...
  113 	   107 	 0.22325 	 0.16628 	 m..s
  101 	   108 	 0.10575 	 0.17049 	 m..s
  109 	   109 	 0.17105 	 0.19590 	 ~...
  102 	   110 	 0.10922 	 0.20470 	 m..s
  110 	   111 	 0.21023 	 0.23381 	 ~...
  119 	   112 	 0.28227 	 0.24106 	 m..s
  120 	   113 	 0.28559 	 0.24897 	 m..s
  114 	   114 	 0.24030 	 0.27253 	 m..s
  106 	   115 	 0.15867 	 0.27598 	 MISS
  116 	   116 	 0.24213 	 0.27718 	 m..s
  117 	   117 	 0.27483 	 0.28350 	 ~...
  107 	   118 	 0.15913 	 0.28719 	 MISS
  114 	   119 	 0.24030 	 0.28853 	 m..s
  118 	   120 	 0.27761 	 0.29501 	 ~...
==========================================
r_mrr = 0.8511051535606384
r2_mrr = 0.7020581364631653
spearmanr_mrr@5 = 0.8947335481643677
spearmanr_mrr@10 = 0.9100144505500793
spearmanr_mrr@50 = 0.9910139441490173
spearmanr_mrr@100 = 0.9382121562957764
spearmanr_mrr@All = 0.9356472492218018
==========================================
test time: 0.39
Done Testing dataset OpenEA
total time taken: 195.43719291687012
training time taken: 180.40369200706482
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8511)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7021)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.8947)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9100)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9910)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9382)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9356)}}, 'test_loss': {'TransE': {'OpenEA': 1.1259895150142256}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 6861649783297940
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1050, 1113, 462, 279, 217, 75, 1074, 588, 986, 0, 624, 587, 204, 955, 861, 1110, 304, 923, 1056, 337, 158, 113, 522, 880, 692, 144, 80, 648, 724, 761, 371, 1132, 837, 66, 251, 447, 833, 355, 245, 260, 809, 179, 103, 511, 1037, 1196, 151, 816, 73, 97, 980, 82, 896, 405, 1012, 44, 237, 1082, 987, 764, 618, 660, 943, 438, 492, 708, 524, 1060, 222, 711, 731, 451, 652, 1199, 963, 948, 1160, 606, 209, 1011, 733, 244, 975, 77, 621, 49, 785, 556, 54, 302, 206, 116, 464, 993, 680, 569, 771, 376, 581, 367, 401, 1122, 124, 308, 67, 889, 122, 1191, 104, 1032, 659, 466, 544, 663, 180, 625, 1182, 669, 1112, 735, 315]
valid_ids (0): []
train_ids (1094): [358, 257, 594, 665, 386, 823, 1135, 770, 41, 309, 472, 318, 571, 129, 1129, 170, 820, 592, 699, 620, 391, 817, 615, 480, 292, 187, 893, 956, 351, 608, 1169, 233, 327, 970, 787, 593, 945, 1093, 1006, 561, 661, 64, 517, 1154, 742, 61, 1044, 372, 38, 235, 795, 579, 612, 1023, 636, 352, 545, 477, 395, 1007, 181, 573, 37, 788, 605, 1024, 381, 917, 457, 236, 433, 696, 397, 599, 991, 46, 275, 285, 446, 902, 469, 202, 713, 159, 421, 542, 633, 916, 726, 1005, 1202, 737, 555, 243, 877, 562, 239, 796, 547, 350, 149, 1141, 491, 1173, 228, 580, 1043, 1094, 954, 19, 1167, 941, 85, 740, 965, 707, 929, 627, 1038, 404, 939, 71, 838, 130, 961, 282, 90, 178, 39, 425, 1047, 390, 900, 1186, 388, 488, 199, 473, 51, 1021, 436, 422, 366, 1069, 1068, 1013, 600, 754, 514, 1017, 671, 250, 853, 74, 242, 1104, 1214, 842, 512, 115, 157, 389, 804, 1101, 437, 3, 818, 972, 62, 377, 769, 790, 306, 96, 319, 380, 345, 173, 453, 111, 1130, 373, 1073, 667, 474, 485, 368, 730, 1193, 746, 1097, 949, 1076, 523, 208, 591, 307, 238, 1162, 1146, 647, 224, 276, 280, 163, 506, 471, 392, 1161, 835, 28, 1095, 461, 489, 810, 914, 240, 765, 936, 828, 751, 1091, 253, 1036, 481, 942, 784, 1001, 136, 962, 622, 60, 69, 273, 768, 487, 529, 321, 637, 65, 91, 675, 172, 1055, 317, 48, 1121, 241, 1140, 1151, 767, 133, 921, 26, 774, 1170, 32, 753, 867, 234, 1210, 226, 1209, 881, 1086, 537, 1025, 1212, 1138, 155, 1148, 777, 799, 775, 803, 577, 297, 84, 118, 1134, 278, 286, 213, 52, 1123, 411, 50, 969, 63, 320, 901, 574, 834, 402, 1019, 468, 844, 840, 1072, 174, 14, 550, 518, 681, 20, 106, 776, 190, 448, 128, 1053, 9, 197, 1051, 1010, 576, 791, 503, 688, 254, 1163, 727, 349, 99, 734, 656, 16, 854, 335, 482, 836, 841, 990, 538, 1157, 1126, 974, 22, 897, 839, 56, 750, 15, 449, 132, 1045, 895, 1015, 246, 870, 634, 288, 638, 831, 979, 1195, 604, 499, 322, 676, 829, 903, 192, 100, 756, 684, 326, 1178, 695, 223, 105, 879, 1099, 348, 513, 258, 697, 1144, 568, 643, 365, 379, 406, 188, 6, 875, 31, 1035, 1083, 310, 305, 617, 1070, 398, 479, 685, 121, 454, 966, 189, 932, 443, 271, 1111, 1030, 229, 109, 868, 1064, 323, 314, 30, 778, 1149, 797, 748, 1071, 1198, 1018, 93, 23, 919, 525, 891, 533, 1197, 5, 1084, 832, 1100, 387, 1090, 1125, 885, 346, 1061, 344, 316, 720, 263, 215, 456, 140, 483, 535, 444, 1004, 1042, 642, 757, 40, 595, 1139, 802, 1033, 845, 938, 994, 114, 169, 1119, 982, 1203, 664, 603, 560, 721, 1128, 168, 430, 565, 300, 435, 898, 221, 1200, 17, 420, 510, 865, 1, 575, 646, 918, 888, 951, 700, 851, 1080, 295, 644, 364, 890, 112, 1040, 957, 682, 528, 551, 231, 850, 270, 291, 882, 156, 567, 922, 35, 557, 13, 908, 801, 983, 1192, 450, 752, 476, 267, 1171, 214, 42, 1114, 564, 811, 1046, 718, 290, 164, 1127, 1208, 826, 710, 704, 709, 1048, 1079, 911, 1176, 412, 907, 1002, 539, 905, 123, 334, 1153, 651, 1054, 184, 410, 1059, 928, 780, 670, 944, 356, 950, 301, 1087, 819, 862, 264, 1118, 583, 195, 86, 498, 857, 182, 497, 806, 312, 1031, 613, 920, 10, 460, 978, 18, 674, 741, 691, 493, 732, 937, 1088, 927, 629, 328, 33, 1065, 959, 72, 931, 393, 698, 200, 296, 101, 486, 1137, 988, 1034, 666, 378, 1143, 176, 1000, 747, 331, 201, 792, 382, 459, 1177, 4, 1014, 532, 715, 299, 230, 415, 1009, 601, 892, 1081, 507, 255, 654, 423, 1039, 755, 261, 343, 690, 141, 1052, 484, 269, 543, 559, 536, 871, 496, 909, 672, 521, 693, 728, 616, 495, 196, 655, 194, 678, 668, 1057, 878, 1179, 641, 409, 166, 925, 611, 793, 375, 662, 992, 960, 95, 626, 120, 717, 519, 177, 1159, 821, 1181, 773, 640, 714, 126, 183, 1098, 333, 632, 1205, 884, 455, 293, 1120, 763, 869, 143, 658, 984, 582, 849, 34, 1166, 650, 70, 1041, 1190, 400, 515, 154, 403, 798, 1109, 702, 428, 940, 653, 419, 274, 1106, 1003, 298, 998, 935, 995, 578, 502, 374, 976, 848, 930, 598, 813, 1066, 470, 794, 500, 369, 546, 29, 1085, 822, 883, 716, 501, 805, 967, 540, 520, 824, 899, 162, 738, 370, 701, 504, 89, 683, 1201, 610, 873, 441, 1105, 452, 687, 1078, 27, 527, 330, 153, 946, 973, 146, 1152, 160, 342, 915, 619, 1206, 703, 341, 766, 508, 530, 1062, 1150, 249, 859, 887, 87, 1027, 139, 552, 572, 294, 1168, 394, 458, 736, 142, 78, 516, 1096, 256, 906, 1063, 847, 262, 947, 723, 43, 1092, 989, 1175, 55, 566, 924, 399, 762, 781, 1108, 814, 505, 1089, 1174, 359, 589, 259, 825, 1029, 1115, 1147, 705, 1189, 1131, 706, 102, 36, 266, 968, 760, 864, 509, 597, 933, 219, 431, 161, 843, 440, 465, 800, 442, 759, 11, 874, 347, 332, 686, 852, 383, 24, 812, 541, 913, 904, 1020, 289, 1049, 628, 427, 807, 147, 88, 478, 602, 1008, 772, 872, 414, 212, 417, 165, 1124, 584, 463, 786, 729, 531, 152, 558, 248, 712, 98, 125, 635, 630, 150, 1156, 657, 1026, 117, 186, 277, 1067, 1016, 1107, 216, 338, 110, 860, 631, 272, 268, 1204, 999, 2, 745, 45, 743, 198, 434, 1077, 127, 283, 1022, 1158, 815, 1165, 1184, 808, 1117, 21, 47, 1207, 83, 353, 108, 886, 855, 287, 490, 1194, 689, 25, 357, 119, 220, 934, 876, 252, 76, 167, 549, 534, 467, 997, 1103, 426, 131, 175, 137, 385, 225, 719, 284, 863, 609, 185, 694, 590, 894, 445, 92, 1142, 1136, 1188, 526, 679, 408, 926, 1075, 649, 138, 134, 677, 971, 362, 191, 1145, 846, 247, 354, 645, 1180, 324, 858, 952, 1028, 1172, 311, 313, 1187, 339, 856, 673, 193, 1058, 59, 912, 303, 416, 1102, 8, 439, 432, 171, 1211, 548, 107, 782, 1133, 553, 12, 1164, 586, 148, 265, 1185, 953, 964, 361, 407, 739, 135, 360, 722, 1116, 207, 585, 639, 985, 384, 554, 218, 1155, 996, 145, 614, 827, 977, 475, 758, 623, 1183, 830, 607, 424, 79, 779, 227, 396, 232, 1213, 363, 725, 329, 749, 570, 783, 281, 210, 981, 789, 58, 413, 57, 68, 325, 958, 429, 336, 596, 7, 866, 81, 418, 211, 340, 203, 205, 563, 94, 744, 910, 494, 53]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7239210844666301
the save name prefix for this run is:  chkpt-ID_7239210844666301_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 514
rank avg (pred): 0.486 +- 0.004
mrr vals (pred, true): 0.000, 0.066
batch losses (mrrl, rdl): 0.0, 0.0017429889

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1016
rank avg (pred): 0.333 +- 0.217
mrr vals (pred, true): 0.001, 0.099
batch losses (mrrl, rdl): 0.0, 0.0005322424

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 255
rank avg (pred): 0.074 +- 0.112
mrr vals (pred, true): 0.183, 0.120
batch losses (mrrl, rdl): 0.0, 9.6553e-06

Epoch over!
epoch time: 12.114

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.137 +- 0.184
mrr vals (pred, true): 0.199, 0.106
batch losses (mrrl, rdl): 0.0, 1.79703e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 693
rank avg (pred): 0.358 +- 0.314
mrr vals (pred, true): 0.093, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002247955

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 811
rank avg (pred): 0.049 +- 0.159
mrr vals (pred, true): 0.330, 0.061
batch losses (mrrl, rdl): 0.0, 1.82138e-05

Epoch over!
epoch time: 11.93

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 211
rank avg (pred): 0.347 +- 0.309
mrr vals (pred, true): 0.079, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002546532

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 762
rank avg (pred): 0.360 +- 0.313
mrr vals (pred, true): 0.029, 0.084
batch losses (mrrl, rdl): 0.0, 7.54036e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.326 +- 0.308
mrr vals (pred, true): 0.087, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003428561

Epoch over!
epoch time: 11.97

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 884
rank avg (pred): 0.331 +- 0.309
mrr vals (pred, true): 0.098, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003145961

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 427
rank avg (pred): 0.366 +- 0.313
mrr vals (pred, true): 0.013, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001965086

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.338 +- 0.315
mrr vals (pred, true): 0.047, 0.104
batch losses (mrrl, rdl): 0.0, 0.0006390183

Epoch over!
epoch time: 11.989

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1161
rank avg (pred): 0.386 +- 0.325
mrr vals (pred, true): 0.039, 0.035
batch losses (mrrl, rdl): 0.0, 9.28889e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 997
rank avg (pred): 0.045 +- 0.153
mrr vals (pred, true): 0.316, 0.288
batch losses (mrrl, rdl): 0.0, 6.07e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 322
rank avg (pred): 0.059 +- 0.147
mrr vals (pred, true): 0.335, 0.094
batch losses (mrrl, rdl): 0.0, 3.3367e-06

Epoch over!
epoch time: 11.812

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 368
rank avg (pred): 0.339 +- 0.324
mrr vals (pred, true): 0.121, 0.065
batch losses (mrrl, rdl): 0.0507780351, 0.0005604126

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 513
rank avg (pred): 0.338 +- 0.218
mrr vals (pred, true): 0.068, 0.050
batch losses (mrrl, rdl): 0.0034120092, 0.0003812991

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 515
rank avg (pred): 0.402 +- 0.161
mrr vals (pred, true): 0.055, 0.095
batch losses (mrrl, rdl): 0.0002460711, 0.0011896082

Epoch over!
epoch time: 12.264

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 332
rank avg (pred): 0.439 +- 0.175
mrr vals (pred, true): 0.055, 0.058
batch losses (mrrl, rdl): 0.0002645499, 0.0012881994

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 783
rank avg (pred): 0.433 +- 0.134
mrr vals (pred, true): 0.044, 0.007
batch losses (mrrl, rdl): 0.0003138915, 0.0001157627

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1001
rank avg (pred): 0.375 +- 0.182
mrr vals (pred, true): 0.069, 0.052
batch losses (mrrl, rdl): 0.0034483722, 0.0006696224

Epoch over!
epoch time: 12.118

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 559
rank avg (pred): 0.371 +- 0.178
mrr vals (pred, true): 0.067, 0.075
batch losses (mrrl, rdl): 0.0029893355, 0.0007615783

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 390
rank avg (pred): 0.344 +- 0.131
mrr vals (pred, true): 0.058, 0.021
batch losses (mrrl, rdl): 0.0006446698, 0.0002870125

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 448
rank avg (pred): 0.325 +- 0.111
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 5.3438e-06, 0.0005705287

Epoch over!
epoch time: 12.103

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1173
rank avg (pred): 0.404 +- 0.131
mrr vals (pred, true): 0.050, 0.058
batch losses (mrrl, rdl): 1.1006e-06, 0.0002597186

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 589
rank avg (pred): 0.437 +- 0.135
mrr vals (pred, true): 0.046, 0.015
batch losses (mrrl, rdl): 0.0001871554, 0.0002984934

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 184
rank avg (pred): 0.367 +- 0.142
mrr vals (pred, true): 0.056, 0.007
batch losses (mrrl, rdl): 0.0003648029, 0.0003296646

Epoch over!
epoch time: 12.154

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 857
rank avg (pred): 0.394 +- 0.128
mrr vals (pred, true): 0.048, 0.098
batch losses (mrrl, rdl): 4.86358e-05, 0.0004541067

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 34
rank avg (pred): 0.245 +- 0.129
mrr vals (pred, true): 0.089, 0.084
batch losses (mrrl, rdl): 0.0155741731, 0.0005914188

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 526
rank avg (pred): 0.182 +- 0.078
mrr vals (pred, true): 0.060, 0.078
batch losses (mrrl, rdl): 0.0010972575, 9.53682e-05

Epoch over!
epoch time: 12.105

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 17
rank avg (pred): 0.201 +- 0.113
mrr vals (pred, true): 0.098, 0.187
batch losses (mrrl, rdl): 0.0781189129, 0.0004972479

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 230
rank avg (pred): 0.393 +- 0.193
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 3.49658e-05, 0.0001726069

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 391
rank avg (pred): 0.334 +- 0.167
mrr vals (pred, true): 0.052, 0.032
batch losses (mrrl, rdl): 2.29654e-05, 0.0003274483

Epoch over!
epoch time: 12.126

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 718
rank avg (pred): 0.445 +- 0.248
mrr vals (pred, true): 0.043, 0.006
batch losses (mrrl, rdl): 0.0005041599, 3.17059e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 870
rank avg (pred): 0.250 +- 0.133
mrr vals (pred, true): 0.057, 0.006
batch losses (mrrl, rdl): 0.0005391229, 0.0011540215

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 806
rank avg (pred): 0.431 +- 0.257
mrr vals (pred, true): 0.046, 0.007
batch losses (mrrl, rdl): 0.0001724127, 2.72716e-05

Epoch over!
epoch time: 12.054

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 86
rank avg (pred): 0.265 +- 0.170
mrr vals (pred, true): 0.047, 0.037
batch losses (mrrl, rdl): 6.29687e-05, 0.0001128944

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 487
rank avg (pred): 0.208 +- 0.104
mrr vals (pred, true): 0.058, 0.069
batch losses (mrrl, rdl): 0.0006661598, 8.35878e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 622
rank avg (pred): 0.342 +- 0.249
mrr vals (pred, true): 0.048, 0.012
batch losses (mrrl, rdl): 5.12836e-05, 1.96166e-05

Epoch over!
epoch time: 12.297

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 13
rank avg (pred): 0.160 +- 0.092
mrr vals (pred, true): 0.135, 0.138
batch losses (mrrl, rdl): 9.12969e-05, 0.0001486913

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 385
rank avg (pred): 0.374 +- 0.315
mrr vals (pred, true): 0.049, 0.026
batch losses (mrrl, rdl): 4.0197e-06, 0.0004669724

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 108
rank avg (pred): 0.347 +- 0.283
mrr vals (pred, true): 0.055, 0.012
batch losses (mrrl, rdl): 0.0002027346, 0.0001618018

Epoch over!
epoch time: 11.94

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 628
rank avg (pred): 0.355 +- 0.298
mrr vals (pred, true): 0.039, 0.011
batch losses (mrrl, rdl): 0.0013112485, 1.53201e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1150
rank avg (pred): 0.123 +- 0.070
mrr vals (pred, true): 0.131, 0.108
batch losses (mrrl, rdl): 0.0049286168, 8.36939e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 143
rank avg (pred): 0.313 +- 0.285
mrr vals (pred, true): 0.049, 0.036
batch losses (mrrl, rdl): 2.9808e-06, 0.0003046252

Epoch over!
epoch time: 12.092

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.357 +- 0.255
mrr vals (pred, true): 0.080, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04995 	 0.00600 	 m..s
    3 	     1 	 0.04986 	 0.00606 	 m..s
   77 	     2 	 0.06133 	 0.00613 	 m..s
   87 	     3 	 0.07996 	 0.00619 	 m..s
   29 	     4 	 0.05201 	 0.00620 	 m..s
   27 	     5 	 0.05200 	 0.00620 	 m..s
   27 	     6 	 0.05200 	 0.00621 	 m..s
   35 	     7 	 0.05215 	 0.00622 	 m..s
   10 	     8 	 0.05004 	 0.00627 	 m..s
   16 	     9 	 0.05058 	 0.00629 	 m..s
   12 	    10 	 0.05024 	 0.00629 	 m..s
   14 	    11 	 0.05032 	 0.00630 	 m..s
   43 	    12 	 0.05237 	 0.00632 	 m..s
   36 	    13 	 0.05222 	 0.00633 	 m..s
   13 	    14 	 0.05029 	 0.00633 	 m..s
   66 	    15 	 0.05640 	 0.00634 	 m..s
    0 	    16 	 0.04980 	 0.00634 	 m..s
   33 	    17 	 0.05209 	 0.00635 	 m..s
    6 	    18 	 0.04991 	 0.00636 	 m..s
   37 	    19 	 0.05224 	 0.00636 	 m..s
   53 	    20 	 0.05460 	 0.00636 	 m..s
   59 	    21 	 0.05586 	 0.00638 	 m..s
   90 	    22 	 0.10254 	 0.00640 	 m..s
   52 	    23 	 0.05287 	 0.00642 	 m..s
   55 	    24 	 0.05506 	 0.00645 	 m..s
   26 	    25 	 0.05191 	 0.00647 	 m..s
   83 	    26 	 0.07603 	 0.00648 	 m..s
   11 	    27 	 0.05010 	 0.00649 	 m..s
   80 	    28 	 0.06769 	 0.00652 	 m..s
   19 	    29 	 0.05136 	 0.00652 	 m..s
   25 	    30 	 0.05191 	 0.00653 	 m..s
   67 	    31 	 0.05640 	 0.00653 	 m..s
   73 	    32 	 0.06082 	 0.00653 	 m..s
   60 	    33 	 0.05593 	 0.00655 	 m..s
   94 	    34 	 0.10835 	 0.00660 	 MISS
   85 	    35 	 0.07618 	 0.00660 	 m..s
   18 	    36 	 0.05067 	 0.00664 	 m..s
   82 	    37 	 0.07026 	 0.00667 	 m..s
   21 	    38 	 0.05174 	 0.00668 	 m..s
   47 	    39 	 0.05244 	 0.00668 	 m..s
   22 	    40 	 0.05174 	 0.00671 	 m..s
   43 	    41 	 0.05237 	 0.00675 	 m..s
   65 	    42 	 0.05626 	 0.00678 	 m..s
    2 	    43 	 0.04986 	 0.00680 	 m..s
    4 	    44 	 0.04990 	 0.00801 	 m..s
    7 	    45 	 0.04994 	 0.00878 	 m..s
   20 	    46 	 0.05138 	 0.00988 	 m..s
   15 	    47 	 0.05035 	 0.01016 	 m..s
   39 	    48 	 0.05225 	 0.01046 	 m..s
    4 	    49 	 0.04990 	 0.01184 	 m..s
   51 	    50 	 0.05262 	 0.01464 	 m..s
   38 	    51 	 0.05224 	 0.01758 	 m..s
   49 	    52 	 0.05249 	 0.01851 	 m..s
   46 	    53 	 0.05242 	 0.02126 	 m..s
   45 	    54 	 0.05242 	 0.02253 	 ~...
   75 	    55 	 0.06130 	 0.02307 	 m..s
    9 	    56 	 0.05000 	 0.02448 	 ~...
   30 	    57 	 0.05203 	 0.02799 	 ~...
   48 	    58 	 0.05248 	 0.03326 	 ~...
   17 	    59 	 0.05066 	 0.03429 	 ~...
   40 	    60 	 0.05226 	 0.03445 	 ~...
   41 	    61 	 0.05227 	 0.03625 	 ~...
   31 	    62 	 0.05204 	 0.03671 	 ~...
   34 	    63 	 0.05214 	 0.03776 	 ~...
   54 	    64 	 0.05506 	 0.04096 	 ~...
   61 	    65 	 0.05599 	 0.04160 	 ~...
   75 	    66 	 0.06130 	 0.04196 	 ~...
   50 	    67 	 0.05256 	 0.04468 	 ~...
   71 	    68 	 0.05847 	 0.04886 	 ~...
   81 	    69 	 0.06960 	 0.05635 	 ~...
   24 	    70 	 0.05187 	 0.05917 	 ~...
   63 	    71 	 0.05616 	 0.05959 	 ~...
   58 	    72 	 0.05563 	 0.06519 	 ~...
   95 	    73 	 0.10857 	 0.06584 	 m..s
   57 	    74 	 0.05518 	 0.06655 	 ~...
   86 	    75 	 0.07922 	 0.06889 	 ~...
   56 	    76 	 0.05511 	 0.07115 	 ~...
   74 	    77 	 0.06127 	 0.07447 	 ~...
   97 	    78 	 0.11045 	 0.07661 	 m..s
   88 	    79 	 0.10123 	 0.07721 	 ~...
   62 	    80 	 0.05603 	 0.07905 	 ~...
   68 	    81 	 0.05667 	 0.08500 	 ~...
   72 	    82 	 0.05940 	 0.08502 	 ~...
   78 	    83 	 0.06330 	 0.08829 	 ~...
  107 	    84 	 0.16073 	 0.08904 	 m..s
   79 	    85 	 0.06334 	 0.08995 	 ~...
  106 	    86 	 0.15309 	 0.09061 	 m..s
   32 	    87 	 0.05205 	 0.09181 	 m..s
    1 	    88 	 0.04982 	 0.09260 	 m..s
   84 	    89 	 0.07613 	 0.09315 	 ~...
   42 	    90 	 0.05236 	 0.09355 	 m..s
  100 	    91 	 0.11671 	 0.09544 	 ~...
   23 	    92 	 0.05180 	 0.09614 	 m..s
  105 	    93 	 0.14550 	 0.09631 	 m..s
   70 	    94 	 0.05714 	 0.09750 	 m..s
   69 	    95 	 0.05678 	 0.09795 	 m..s
  103 	    96 	 0.12420 	 0.10189 	 ~...
   63 	    97 	 0.05616 	 0.10268 	 m..s
   96 	    98 	 0.10872 	 0.10937 	 ~...
   92 	    99 	 0.10612 	 0.11213 	 ~...
   91 	   100 	 0.10425 	 0.11600 	 ~...
  103 	   101 	 0.12420 	 0.12970 	 ~...
   98 	   102 	 0.11046 	 0.14026 	 ~...
  109 	   103 	 0.17156 	 0.14296 	 ~...
   99 	   104 	 0.11631 	 0.14759 	 m..s
  108 	   105 	 0.17136 	 0.15396 	 ~...
   92 	   106 	 0.10612 	 0.15929 	 m..s
   89 	   107 	 0.10202 	 0.15947 	 m..s
  112 	   108 	 0.17217 	 0.17938 	 ~...
  102 	   109 	 0.12236 	 0.19103 	 m..s
  101 	   110 	 0.11936 	 0.23381 	 MISS
  115 	   111 	 0.25281 	 0.26716 	 ~...
  117 	   112 	 0.25617 	 0.26756 	 ~...
  111 	   113 	 0.17215 	 0.27685 	 MISS
  116 	   114 	 0.25428 	 0.27718 	 ~...
  113 	   115 	 0.24809 	 0.27844 	 m..s
  109 	   116 	 0.17156 	 0.28084 	 MISS
  119 	   117 	 0.27375 	 0.28727 	 ~...
  114 	   118 	 0.25214 	 0.28765 	 m..s
  118 	   119 	 0.27286 	 0.29220 	 ~...
  120 	   120 	 0.27415 	 0.30150 	 ~...
==========================================
r_mrr = 0.8853382468223572
r2_mrr = 0.7082715034484863
spearmanr_mrr@5 = 0.7421139478683472
spearmanr_mrr@10 = 0.8377479910850525
spearmanr_mrr@50 = 0.9667733907699585
spearmanr_mrr@100 = 0.9630252122879028
spearmanr_mrr@All = 0.9635797739028931
==========================================
test time: 0.388
Done Testing dataset OpenEA
total time taken: 196.3007743358612
training time taken: 181.53400588035583
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8853)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7083)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.7421)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8377)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9668)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9630)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9636)}}, 'test_loss': {'TransE': {'OpenEA': 1.1869873573377845}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 6482657741887021
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 398, 756, 12, 874, 22, 364, 889, 1062, 726, 1200, 1190, 984, 1058, 329, 1061, 356, 1059, 109, 1201, 451, 169, 852, 151, 457, 103, 791, 1117, 462, 1086, 570, 622, 588, 904, 880, 70, 550, 671, 982, 466, 944, 277, 345, 1166, 524, 1076, 1060, 763, 698, 104, 97, 555, 408, 1033, 371, 782, 219, 840, 663, 812, 192, 1021, 851, 882, 148, 83, 404, 1210, 801, 688, 1131, 813, 1195, 773, 884, 420, 266, 611, 1146, 303, 646, 717, 665, 470, 708, 308, 856, 295, 247, 713, 75, 336, 686, 312, 896, 1179, 651, 358, 253, 768, 695, 87, 641, 1051, 1112, 675, 1066, 9, 43, 534, 347, 977, 218, 414, 48, 1073, 960, 117, 57, 324, 258]
valid_ids (0): []
train_ids (1094): [631, 888, 229, 521, 829, 1134, 283, 206, 758, 484, 116, 1014, 200, 4, 659, 839, 980, 734, 598, 415, 24, 58, 992, 183, 918, 681, 1083, 209, 93, 584, 576, 316, 13, 461, 184, 73, 737, 684, 822, 687, 966, 362, 112, 927, 624, 1142, 664, 105, 82, 1100, 507, 1071, 1042, 479, 139, 615, 548, 797, 897, 761, 513, 1137, 958, 542, 91, 581, 232, 394, 1095, 114, 627, 596, 667, 311, 418, 964, 163, 496, 1120, 429, 994, 54, 1046, 1212, 764, 1043, 1136, 1054, 1090, 231, 402, 1056, 842, 578, 477, 733, 640, 78, 893, 214, 634, 1070, 363, 1018, 288, 1079, 1211, 1129, 1160, 1025, 629, 1214, 201, 527, 31, 1057, 268, 707, 69, 493, 950, 728, 1085, 118, 1194, 385, 1186, 557, 208, 536, 1022, 185, 1209, 847, 305, 620, 350, 252, 915, 544, 282, 338, 241, 905, 817, 153, 1102, 121, 240, 62, 403, 1093, 929, 337, 518, 644, 571, 635, 1088, 907, 514, 1159, 648, 1135, 873, 849, 946, 926, 357, 98, 908, 271, 260, 895, 178, 628, 776, 986, 1154, 608, 603, 452, 1109, 310, 975, 77, 724, 807, 878, 181, 894, 1163, 868, 930, 826, 906, 770, 872, 1004, 538, 475, 870, 126, 509, 767, 999, 1035, 1064, 891, 696, 101, 1105, 552, 658, 775, 670, 561, 1080, 1157, 317, 1007, 769, 269, 690, 900, 787, 354, 1165, 832, 574, 600, 610, 784, 1067, 328, 39, 1144, 445, 860, 111, 861, 1203, 621, 431, 92, 215, 540, 592, 579, 931, 395, 469, 72, 1132, 94, 1001, 468, 341, 467, 259, 692, 494, 924, 210, 1050, 175, 106, 71, 1149, 941, 981, 1097, 254, 939, 963, 1124, 649, 18, 1138, 819, 545, 1111, 14, 704, 248, 437, 612, 327, 102, 745, 1016, 1072, 645, 459, 188, 837, 1030, 1032, 1099, 196, 361, 916, 711, 546, 373, 195, 23, 1172, 68, 700, 140, 221, 809, 323, 625, 935, 932, 705, 47, 865, 84, 938, 501, 1092, 454, 164, 146, 313, 572, 344, 771, 256, 6, 460, 383, 875, 198, 1063, 1037, 44, 549, 742, 1084, 1041, 177, 922, 693, 359, 1202, 883, 294, 482, 405, 1104, 388, 480, 987, 816, 859, 376, 1015, 430, 1204, 458, 249, 558, 1068, 1167, 564, 187, 224, 747, 993, 824, 848, 374, 709, 774, 1143, 1171, 142, 152, 413, 732, 792, 29, 340, 730, 890, 330, 261, 647, 1052, 1000, 162, 393, 1045, 235, 844, 1127, 63, 632, 971, 1110, 729, 715, 138, 560, 1173, 5, 506, 1213, 779, 159, 64, 320, 522, 504, 86, 191, 391, 786, 1011, 16, 650, 951, 34, 519, 27, 720, 1119, 973, 1147, 727, 161, 435, 302, 367, 407, 805, 562, 1118, 869, 1196, 854, 275, 1181, 307, 28, 585, 677, 2, 1180, 1019, 954, 296, 250, 702, 703, 274, 100, 793, 427, 197, 499, 474, 515, 10, 7, 265, 272, 952, 202, 446, 559, 51, 335, 236, 1148, 920, 390, 909, 943, 483, 1023, 488, 264, 988, 974, 193, 589, 478, 913, 953, 602, 934, 133, 1115, 1207, 113, 382, 147, 537, 877, 820, 565, 790, 910, 88, 61, 143, 19, 529, 136, 531, 36, 160, 1178, 1098, 948, 750, 473, 66, 788, 392, 230, 997, 237, 850, 59, 378, 638, 991, 76, 297, 587, 743, 901, 123, 95, 321, 1158, 290, 251, 270, 497, 516, 680, 438, 682, 656, 660, 1096, 52, 502, 301, 11, 1003, 1048, 962, 976, 676, 421, 735, 306, 1128, 879, 706, 278, 520, 485, 1155, 1103, 442, 755, 567, 845, 979, 389, 867, 315, 661, 45, 1024, 547, 360, 613, 1123, 158, 1133, 885, 168, 881, 553, 637, 242, 65, 1182, 956, 67, 503, 937, 1031, 970, 1141, 137, 871, 368, 490, 1038, 1193, 1187, 772, 481, 370, 190, 1161, 352, 666, 583, 996, 556, 808, 1125, 35, 551, 42, 287, 911, 381, 289, 498, 157, 463, 56, 510, 921, 1013, 377, 212, 149, 972, 380, 1005, 318, 796, 15, 967, 1208, 1049, 226, 834, 273, 1199, 194, 762, 49, 1027, 448, 293, 211, 606, 85, 436, 823, 836, 714, 495, 652, 601, 339, 419, 783, 286, 721, 145, 108, 748, 736, 366, 1055, 125, 351, 127, 505, 719, 657, 1106, 914, 955, 563, 186, 811, 853, 1188, 1184, 741, 617, 789, 607, 925, 653, 759, 20, 577, 55, 440, 525, 575, 623, 90, 299, 864, 375, 353, 279, 1008, 1034, 633, 46, 444, 990, 1140, 511, 122, 1108, 233, 60, 566, 616, 678, 1176, 766, 1183, 155, 841, 898, 120, 471, 1162, 862, 802, 1139, 795, 995, 372, 346, 441, 298, 285, 425, 618, 1151, 1192, 781, 985, 267, 857, 33, 412, 220, 863, 343, 928, 131, 411, 80, 1009, 892, 983, 489, 334, 207, 642, 167, 757, 1012, 417, 17, 590, 110, 227, 738, 32, 284, 81, 532, 1026, 238, 154, 1010, 225, 777, 754, 722, 234, 426, 1205, 639, 753, 683, 716, 300, 539, 173, 798, 619, 827, 1177, 500, 902, 456, 517, 838, 406, 569, 858, 593, 523, 333, 216, 434, 486, 189, 936, 785, 1126, 8, 855, 3, 673, 450, 203, 1197, 945, 476, 989, 1156, 443, 947, 1185, 1121, 1206, 40, 1130, 806, 1069, 379, 396, 1006, 141, 843, 26, 1002, 746, 53, 541, 156, 424, 150, 449, 751, 917, 1017, 401, 508, 968, 512, 694, 262, 739, 662, 778, 107, 205, 1020, 1116, 99, 731, 170, 314, 487, 416, 899, 554, 400, 1174, 528, 326, 174, 535, 228, 204, 1044, 50, 998, 369, 332, 573, 655, 876, 325, 213, 580, 712, 794, 543, 626, 526, 568, 594, 171, 799, 432, 166, 723, 978, 0, 800, 835, 1036, 397, 886, 699, 1153, 129, 725, 959, 830, 447, 679, 309, 1145, 933, 291, 1089, 472, 179, 1164, 1082, 1053, 292, 263, 1150, 255, 940, 132, 1101, 1191, 804, 409, 1175, 828, 691, 1077, 172, 217, 453, 582, 280, 833, 319, 199, 180, 919, 810, 144, 923, 239, 387, 365, 597, 942, 903, 386, 643, 1075, 1087, 79, 689, 384, 25, 244, 1122, 455, 718, 669, 355, 1198, 74, 410, 654, 1169, 614, 96, 399, 1078, 1107, 464, 222, 530, 331, 423, 38, 128, 428, 124, 740, 348, 182, 30, 349, 887, 697, 1170, 674, 165, 322, 949, 37, 599, 961, 591, 710, 765, 744, 492, 245, 1094, 1189, 130, 1065, 803, 89, 815, 821, 21, 912, 630, 1168, 965, 969, 609, 257, 223, 1047, 1091, 276, 701, 135, 604, 1039, 533, 41, 749, 246, 866, 586, 1028, 134, 439, 1074, 752, 176, 281, 846, 119, 685, 957, 818, 1113, 115, 814, 1040, 1114, 422, 491, 595, 1152, 1, 636, 243, 780, 465, 605, 825, 342, 668, 1081, 304, 672, 433, 760, 1029]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  48310380606809
the save name prefix for this run is:  chkpt-ID_48310380606809_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 536
rank avg (pred): 0.475 +- 0.006
mrr vals (pred, true): 0.000, 0.088
batch losses (mrrl, rdl): 0.0, 0.0018730669

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 377
rank avg (pred): 0.367 +- 0.206
mrr vals (pred, true): 0.070, 0.065
batch losses (mrrl, rdl): 0.0, 0.000769347

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1096
rank avg (pred): 0.308 +- 0.266
mrr vals (pred, true): 0.231, 0.121
batch losses (mrrl, rdl): 0.0, 0.000576754

Epoch over!
epoch time: 12.046

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 119
rank avg (pred): 0.347 +- 0.286
mrr vals (pred, true): 0.227, 0.041
batch losses (mrrl, rdl): 0.0, 0.0005719285

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 607
rank avg (pred): 0.362 +- 0.311
mrr vals (pred, true): 0.245, 0.014
batch losses (mrrl, rdl): 0.0, 3.9952e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 556
rank avg (pred): 0.181 +- 0.255
mrr vals (pred, true): 0.285, 0.079
batch losses (mrrl, rdl): 0.0, 1.8338e-06

Epoch over!
epoch time: 11.731

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 427
rank avg (pred): 0.334 +- 0.302
mrr vals (pred, true): 0.254, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002891161

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 728
rank avg (pred): 0.372 +- 0.318
mrr vals (pred, true): 0.245, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001680104

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 36
rank avg (pred): 0.083 +- 0.157
mrr vals (pred, true): 0.323, 0.106
batch losses (mrrl, rdl): 0.0, 4.7445e-06

Epoch over!
epoch time: 11.929

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 59
rank avg (pred): 0.066 +- 0.136
mrr vals (pred, true): 0.339, 0.129
batch losses (mrrl, rdl): 0.0, 4.4777e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 539
rank avg (pred): 0.163 +- 0.268
mrr vals (pred, true): 0.288, 0.088
batch losses (mrrl, rdl): 0.0, 7.97e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 795
rank avg (pred): 0.339 +- 0.308
mrr vals (pred, true): 0.241, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003243814

Epoch over!
epoch time: 11.983

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 282
rank avg (pred): 0.078 +- 0.151
mrr vals (pred, true): 0.323, 0.089
batch losses (mrrl, rdl): 0.0, 2.1212e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 443
rank avg (pred): 0.284 +- 0.299
mrr vals (pred, true): 0.273, 0.007
batch losses (mrrl, rdl): 0.0, 0.0006795675

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 275
rank avg (pred): 0.042 +- 0.086
mrr vals (pred, true): 0.328, 0.117
batch losses (mrrl, rdl): 0.0, 2.16208e-05

Epoch over!
epoch time: 11.936

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 67
rank avg (pred): 0.075 +- 0.150
mrr vals (pred, true): 0.316, 0.130
batch losses (mrrl, rdl): 0.3474363089, 9.502e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1173
rank avg (pred): 0.417 +- 0.202
mrr vals (pred, true): 0.044, 0.058
batch losses (mrrl, rdl): 0.0003118145, 0.0002752379

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 591
rank avg (pred): 0.436 +- 0.186
mrr vals (pred, true): 0.045, 0.011
batch losses (mrrl, rdl): 0.0002221861, 0.0002283494

Epoch over!
epoch time: 12.205

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 498
rank avg (pred): 0.348 +- 0.220
mrr vals (pred, true): 0.053, 0.069
batch losses (mrrl, rdl): 9.45036e-05, 0.0004865135

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 8
rank avg (pred): 0.026 +- 0.022
mrr vals (pred, true): 0.126, 0.153
batch losses (mrrl, rdl): 0.007086236, 2.56456e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 150
rank avg (pred): 0.381 +- 0.196
mrr vals (pred, true): 0.055, 0.013
batch losses (mrrl, rdl): 0.0002538156, 0.0003059335

Epoch over!
epoch time: 12.034

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 199
rank avg (pred): 0.381 +- 0.196
mrr vals (pred, true): 0.051, 0.007
batch losses (mrrl, rdl): 1.25072e-05, 0.0001902526

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 454
rank avg (pred): 0.365 +- 0.248
mrr vals (pred, true): 0.056, 0.006
batch losses (mrrl, rdl): 0.0003789355, 0.000200288

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1043
rank avg (pred): 0.155 +- 0.134
mrr vals (pred, true): 0.089, 0.007
batch losses (mrrl, rdl): 0.0153748039, 0.0020721997

Epoch over!
epoch time: 11.966

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 638
rank avg (pred): 0.404 +- 0.177
mrr vals (pred, true): 0.042, 0.032
batch losses (mrrl, rdl): 0.00056411, 0.0002098166

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 241
rank avg (pred): 0.373 +- 0.205
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.000147333, 0.0002481046

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 576
rank avg (pred): 0.412 +- 0.164
mrr vals (pred, true): 0.043, 0.010
batch losses (mrrl, rdl): 0.0004892194, 0.0001494763

Epoch over!
epoch time: 12.068

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 875
rank avg (pred): 0.385 +- 0.184
mrr vals (pred, true): 0.048, 0.007
batch losses (mrrl, rdl): 3.08815e-05, 0.0002100011

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 293
rank avg (pred): 0.051 +- 0.044
mrr vals (pred, true): 0.183, 0.287
batch losses (mrrl, rdl): 0.109122403, 6.0228e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 417
rank avg (pred): 0.349 +- 0.196
mrr vals (pred, true): 0.059, 0.006
batch losses (mrrl, rdl): 0.0007922442, 0.0003240869

Epoch over!
epoch time: 12.01

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 89
rank avg (pred): 0.377 +- 0.179
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 3.0914e-05, 0.0006618021

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1204
rank avg (pred): 0.391 +- 0.163
mrr vals (pred, true): 0.047, 0.007
batch losses (mrrl, rdl): 7.61791e-05, 0.0002212286

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1197
rank avg (pred): 0.406 +- 0.145
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.0002193306, 0.0001801724

Epoch over!
epoch time: 12.245

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 846
rank avg (pred): 0.407 +- 0.146
mrr vals (pred, true): 0.047, 0.095
batch losses (mrrl, rdl): 0.0001061899, 0.0005207136

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 412
rank avg (pred): 0.371 +- 0.164
mrr vals (pred, true): 0.060, 0.007
batch losses (mrrl, rdl): 0.0009641314, 0.0003049858

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 934
rank avg (pred): 0.368 +- 0.165
mrr vals (pred, true): 0.057, 0.095
batch losses (mrrl, rdl): 0.000509077, 0.0002783303

Epoch over!
epoch time: 12.047

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 499
rank avg (pred): 0.333 +- 0.186
mrr vals (pred, true): 0.056, 0.079
batch losses (mrrl, rdl): 0.0003080888, 0.0004985253

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 488
rank avg (pred): 0.200 +- 0.133
mrr vals (pred, true): 0.063, 0.095
batch losses (mrrl, rdl): 0.0017453252, 8.1372e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 479
rank avg (pred): 0.268 +- 0.160
mrr vals (pred, true): 0.062, 0.006
batch losses (mrrl, rdl): 0.0014185109, 0.0009204507

Epoch over!
epoch time: 12.001

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 922
rank avg (pred): 0.367 +- 0.160
mrr vals (pred, true): 0.053, 0.091
batch losses (mrrl, rdl): 7.13145e-05, 0.0002329288

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 385
rank avg (pred): 0.401 +- 0.135
mrr vals (pred, true): 0.051, 0.026
batch losses (mrrl, rdl): 3.1771e-06, 0.0006093535

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 232
rank avg (pred): 0.396 +- 0.130
mrr vals (pred, true): 0.047, 0.007
batch losses (mrrl, rdl): 7.59126e-05, 0.0002386217

Epoch over!
epoch time: 12.086

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 859
rank avg (pred): 0.397 +- 0.129
mrr vals (pred, true): 0.048, 0.097
batch losses (mrrl, rdl): 4.9567e-05, 0.0004756784

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 416
rank avg (pred): 0.362 +- 0.143
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.0001992976, 0.0003436503

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 766
rank avg (pred): 0.388 +- 0.118
mrr vals (pred, true): 0.046, 0.097
batch losses (mrrl, rdl): 0.0001711032, 0.0004080154

Epoch over!
epoch time: 11.947

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.098 +- 0.084
mrr vals (pred, true): 0.082, 0.140

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   24 	     0 	 0.04504 	 0.00590 	 m..s
   11 	     1 	 0.04295 	 0.00600 	 m..s
   58 	     2 	 0.05181 	 0.00607 	 m..s
   13 	     3 	 0.04333 	 0.00613 	 m..s
   15 	     4 	 0.04431 	 0.00616 	 m..s
   27 	     5 	 0.04537 	 0.00618 	 m..s
   30 	     6 	 0.04557 	 0.00620 	 m..s
    7 	     7 	 0.04208 	 0.00628 	 m..s
   26 	     8 	 0.04534 	 0.00630 	 m..s
   33 	     9 	 0.04593 	 0.00630 	 m..s
   75 	    10 	 0.06177 	 0.00634 	 m..s
    1 	    11 	 0.03977 	 0.00634 	 m..s
    8 	    12 	 0.04210 	 0.00638 	 m..s
    8 	    13 	 0.04210 	 0.00640 	 m..s
   32 	    14 	 0.04587 	 0.00641 	 m..s
    3 	    15 	 0.04005 	 0.00641 	 m..s
    4 	    16 	 0.04010 	 0.00643 	 m..s
   61 	    17 	 0.05308 	 0.00645 	 m..s
   61 	    18 	 0.05308 	 0.00646 	 m..s
   35 	    19 	 0.04657 	 0.00646 	 m..s
    2 	    20 	 0.03982 	 0.00647 	 m..s
   94 	    21 	 0.08262 	 0.00648 	 m..s
   22 	    22 	 0.04472 	 0.00649 	 m..s
   95 	    23 	 0.08344 	 0.00651 	 m..s
   19 	    24 	 0.04469 	 0.00652 	 m..s
   84 	    25 	 0.06738 	 0.00652 	 m..s
   28 	    26 	 0.04542 	 0.00653 	 m..s
   75 	    27 	 0.06177 	 0.00653 	 m..s
   42 	    28 	 0.04756 	 0.00653 	 m..s
    6 	    29 	 0.04147 	 0.00654 	 m..s
   17 	    30 	 0.04446 	 0.00654 	 m..s
   48 	    31 	 0.04838 	 0.00658 	 m..s
   34 	    32 	 0.04654 	 0.00659 	 m..s
   90 	    33 	 0.07977 	 0.00660 	 m..s
   18 	    34 	 0.04449 	 0.00660 	 m..s
   47 	    35 	 0.04833 	 0.00661 	 m..s
   75 	    36 	 0.06177 	 0.00662 	 m..s
   35 	    37 	 0.04657 	 0.00663 	 m..s
   56 	    38 	 0.05168 	 0.00664 	 m..s
   64 	    39 	 0.05332 	 0.00668 	 m..s
   64 	    40 	 0.05332 	 0.00669 	 m..s
   51 	    41 	 0.04894 	 0.00673 	 m..s
   19 	    42 	 0.04469 	 0.00675 	 m..s
   16 	    43 	 0.04438 	 0.00882 	 m..s
   14 	    44 	 0.04413 	 0.01046 	 m..s
   50 	    45 	 0.04863 	 0.01147 	 m..s
    0 	    46 	 0.03917 	 0.01171 	 ~...
   12 	    47 	 0.04298 	 0.01313 	 ~...
   43 	    48 	 0.04766 	 0.01328 	 m..s
   52 	    49 	 0.04897 	 0.01684 	 m..s
   25 	    50 	 0.04525 	 0.01698 	 ~...
   59 	    51 	 0.05246 	 0.01851 	 m..s
   39 	    52 	 0.04704 	 0.02024 	 ~...
   66 	    53 	 0.05339 	 0.02222 	 m..s
   39 	    54 	 0.04704 	 0.02253 	 ~...
   63 	    55 	 0.05320 	 0.02307 	 m..s
   67 	    56 	 0.05612 	 0.02327 	 m..s
   29 	    57 	 0.04547 	 0.02954 	 ~...
    5 	    58 	 0.04018 	 0.03202 	 ~...
   44 	    59 	 0.04786 	 0.03243 	 ~...
   10 	    60 	 0.04216 	 0.03290 	 ~...
   23 	    61 	 0.04489 	 0.03581 	 ~...
   45 	    62 	 0.04807 	 0.03588 	 ~...
   69 	    63 	 0.05699 	 0.04196 	 ~...
   46 	    64 	 0.04833 	 0.05180 	 ~...
   31 	    65 	 0.04557 	 0.05596 	 ~...
   54 	    66 	 0.05017 	 0.05645 	 ~...
   70 	    67 	 0.05818 	 0.05695 	 ~...
   49 	    68 	 0.04847 	 0.05713 	 ~...
   89 	    69 	 0.07493 	 0.05950 	 ~...
   53 	    70 	 0.04949 	 0.06428 	 ~...
   71 	    71 	 0.05831 	 0.06655 	 ~...
   74 	    72 	 0.06030 	 0.06665 	 ~...
   72 	    73 	 0.05832 	 0.06701 	 ~...
   72 	    74 	 0.05832 	 0.06713 	 ~...
  101 	    75 	 0.09804 	 0.06971 	 ~...
   87 	    76 	 0.06927 	 0.07343 	 ~...
   68 	    77 	 0.05690 	 0.07573 	 ~...
   60 	    78 	 0.05265 	 0.07593 	 ~...
   75 	    79 	 0.06177 	 0.08456 	 ~...
   86 	    80 	 0.06887 	 0.08502 	 ~...
   83 	    81 	 0.06244 	 0.08855 	 ~...
   88 	    82 	 0.07107 	 0.08861 	 ~...
   82 	    83 	 0.06183 	 0.09078 	 ~...
   57 	    84 	 0.05175 	 0.09335 	 m..s
   55 	    85 	 0.05132 	 0.09350 	 m..s
   97 	    86 	 0.08833 	 0.09443 	 ~...
   97 	    87 	 0.08833 	 0.09453 	 ~...
   75 	    88 	 0.06177 	 0.09470 	 m..s
   75 	    89 	 0.06177 	 0.09648 	 m..s
   38 	    90 	 0.04697 	 0.09660 	 m..s
   37 	    91 	 0.04672 	 0.09697 	 m..s
   41 	    92 	 0.04745 	 0.09721 	 m..s
   75 	    93 	 0.06177 	 0.09787 	 m..s
   21 	    94 	 0.04472 	 0.09868 	 m..s
  108 	    95 	 0.17704 	 0.10163 	 m..s
   85 	    96 	 0.06804 	 0.10268 	 m..s
   92 	    97 	 0.08092 	 0.10352 	 ~...
  103 	    98 	 0.12809 	 0.10676 	 ~...
   91 	    99 	 0.08027 	 0.12157 	 m..s
   96 	   100 	 0.08357 	 0.12436 	 m..s
   93 	   101 	 0.08201 	 0.14016 	 m..s
  104 	   102 	 0.13545 	 0.14112 	 ~...
  105 	   103 	 0.13567 	 0.14296 	 ~...
   99 	   104 	 0.09413 	 0.14405 	 m..s
  106 	   105 	 0.15808 	 0.14759 	 ~...
  100 	   106 	 0.09429 	 0.14815 	 m..s
  107 	   107 	 0.16629 	 0.16654 	 ~...
  102 	   108 	 0.10721 	 0.18004 	 m..s
  110 	   109 	 0.23626 	 0.20588 	 m..s
  111 	   110 	 0.24919 	 0.21324 	 m..s
  109 	   111 	 0.23576 	 0.21513 	 ~...
  117 	   112 	 0.28547 	 0.26451 	 ~...
  113 	   113 	 0.27796 	 0.26458 	 ~...
  114 	   114 	 0.28140 	 0.27178 	 ~...
  116 	   115 	 0.28308 	 0.27253 	 ~...
  115 	   116 	 0.28293 	 0.27825 	 ~...
  112 	   117 	 0.27443 	 0.27828 	 ~...
  118 	   118 	 0.29092 	 0.29402 	 ~...
  118 	   119 	 0.29092 	 0.29501 	 ~...
  118 	   120 	 0.29092 	 0.30150 	 ~...
==========================================
r_mrr = 0.9103403687477112
r2_mrr = 0.7973021268844604
spearmanr_mrr@5 = 0.9377893209457397
spearmanr_mrr@10 = 0.9849249720573425
spearmanr_mrr@50 = 0.989144504070282
spearmanr_mrr@100 = 0.9531046152114868
spearmanr_mrr@All = 0.9526429772377014
==========================================
test time: 0.386
Done Testing dataset OpenEA
total time taken: 195.39324808120728
training time taken: 180.69806957244873
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.9103)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7973)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9378)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9849)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9891)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9531)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9526)}}, 'test_loss': {'TransE': {'OpenEA': 0.4664141675675637}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 6947424689162423
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [680, 187, 1195, 1209, 706, 634, 65, 1104, 1144, 354, 186, 20, 889, 956, 34, 437, 393, 13, 239, 163, 1121, 129, 1153, 100, 873, 1125, 640, 135, 519, 692, 1143, 281, 905, 702, 531, 795, 472, 767, 215, 805, 227, 765, 97, 310, 548, 755, 684, 182, 976, 323, 149, 555, 991, 1028, 433, 502, 119, 544, 931, 708, 448, 1064, 619, 188, 256, 56, 52, 642, 987, 730, 605, 726, 514, 184, 543, 985, 1006, 710, 462, 107, 960, 255, 403, 496, 970, 830, 290, 971, 452, 379, 880, 817, 820, 655, 404, 269, 1050, 371, 274, 953, 370, 1166, 131, 1089, 1155, 580, 174, 1113, 358, 537, 807, 938, 349, 1086, 77, 941, 159, 1182, 681, 739, 45]
valid_ids (0): []
train_ids (1094): [202, 920, 685, 483, 123, 503, 417, 1011, 471, 1184, 3, 615, 67, 900, 212, 746, 198, 291, 355, 745, 1021, 1061, 979, 485, 524, 36, 891, 977, 328, 944, 1134, 588, 788, 427, 989, 1053, 487, 796, 439, 850, 939, 731, 787, 528, 568, 579, 718, 943, 1042, 424, 23, 1157, 398, 1024, 272, 651, 654, 1025, 1140, 848, 99, 425, 832, 968, 1126, 165, 553, 141, 887, 1213, 972, 384, 352, 311, 564, 451, 459, 1117, 583, 1022, 614, 336, 88, 108, 645, 912, 895, 11, 828, 219, 922, 41, 539, 280, 855, 80, 675, 205, 673, 1000, 993, 647, 676, 683, 735, 742, 810, 857, 842, 203, 698, 901, 1099, 264, 266, 1069, 94, 789, 999, 570, 1062, 286, 1167, 587, 443, 183, 1014, 378, 1047, 1201, 672, 824, 136, 1199, 919, 854, 663, 407, 516, 2, 162, 1194, 969, 558, 390, 933, 566, 415, 1039, 102, 150, 966, 696, 846, 46, 319, 813, 497, 192, 156, 259, 1204, 493, 1015, 1004, 662, 701, 76, 986, 14, 885, 177, 721, 190, 74, 73, 5, 957, 500, 575, 597, 997, 888, 126, 1055, 581, 152, 1188, 1038, 760, 603, 703, 9, 118, 1116, 963, 779, 978, 722, 24, 83, 777, 438, 302, 171, 27, 340, 1138, 728, 1177, 1081, 1163, 1109, 418, 7, 373, 1192, 374, 321, 1072, 1070, 248, 940, 284, 312, 87, 643, 1026, 1149, 4, 535, 406, 526, 506, 909, 426, 271, 623, 567, 911, 392, 786, 1020, 545, 275, 1178, 447, 515, 372, 865, 902, 361, 952, 723, 649, 608, 444, 862, 1058, 430, 1123, 229, 1137, 1044, 875, 630, 877, 69, 611, 1068, 195, 737, 1093, 1196, 1171, 273, 324, 556, 921, 679, 343, 251, 552, 429, 756, 839, 12, 1135, 903, 385, 932, 666, 220, 317, 216, 750, 762, 523, 1175, 1100, 768, 278, 359, 1131, 1148, 169, 298, 831, 613, 856, 122, 660, 584, 946, 53, 859, 296, 646, 1008, 160, 185, 1054, 709, 241, 173, 480, 214, 1193, 419, 199, 441, 1190, 181, 127, 874, 422, 207, 360, 369, 1161, 780, 363, 775, 344, 1096, 51, 98, 814, 1165, 1059, 837, 314, 917, 664, 436, 794, 628, 232, 844, 306, 261, 833, 113, 61, 297, 235, 1033, 860, 711, 1208, 180, 863, 157, 39, 1010, 697, 717, 914, 293, 465, 973, 221, 1115, 670, 234, 784, 1052, 213, 937, 481, 428, 890, 823, 1187, 450, 130, 687, 620, 479, 599, 81, 208, 578, 827, 951, 1048, 573, 617, 460, 638, 759, 405, 276, 690, 521, 469, 641, 1073, 397, 766, 197, 283, 179, 456, 58, 476, 498, 959, 95, 154, 994, 294, 1211, 211, 16, 1120, 125, 1156, 246, 288, 849, 6, 408, 980, 674, 299, 534, 389, 82, 851, 870, 300, 204, 1160, 876, 111, 287, 494, 59, 618, 801, 734, 104, 1063, 560, 432, 1046, 380, 1210, 116, 1152, 106, 930, 467, 260, 342, 138, 604, 412, 729, 838, 505, 818, 791, 1019, 1037, 455, 414, 509, 861, 667, 1159, 749, 1075, 103, 333, 490, 461, 132, 1197, 339, 258, 413, 1207, 489, 650, 1023, 399, 947, 91, 542, 883, 1085, 879, 50, 804, 364, 1118, 326, 350, 338, 929, 279, 318, 893, 1092, 84, 96, 172, 1066, 665, 48, 852, 1007, 38, 764, 700, 598, 43, 247, 453, 693, 508, 231, 1133, 836, 945, 1082, 1173, 572, 1198, 44, 341, 1127, 602, 1017, 704, 401, 466, 1110, 1108, 377, 1150, 409, 715, 238, 1147, 420, 621, 562, 771, 716, 178, 847, 688, 411, 304, 1056, 529, 906, 520, 170, 327, 240, 538, 1076, 499, 536, 1087, 591, 858, 49, 1111, 25, 253, 1016, 151, 918, 153, 896, 206, 974, 86, 29, 934, 1067, 793, 0, 1060, 935, 797, 1083, 586, 478, 301, 1141, 164, 714, 292, 309, 1128, 686, 1130, 353, 201, 1049, 217, 228, 778, 774, 965, 637, 809, 593, 362, 601, 511, 1088, 594, 744, 431, 334, 1091, 267, 325, 936, 892, 223, 513, 30, 996, 741, 926, 470, 950, 31, 329, 886, 434, 772, 47, 899, 995, 904, 268, 908, 482, 845, 368, 386, 866, 316, 143, 798, 622, 610, 1124, 1158, 840, 659, 32, 134, 33, 961, 639, 725, 872, 561, 191, 829, 17, 803, 168, 585, 491, 501, 1095, 1103, 923, 492, 609, 592, 282, 1106, 1029, 1174, 694, 320, 28, 554, 115, 388, 55, 758, 525, 1119, 569, 1094, 120, 773, 982, 137, 488, 245, 616, 236, 913, 458, 1105, 332, 607, 563, 648, 92, 210, 1186, 967, 1214, 357, 313, 189, 224, 475, 815, 477, 954, 927, 295, 15, 747, 486, 1018, 990, 1036, 1102, 175, 60, 468, 233, 1202, 89, 1162, 330, 606, 770, 565, 1079, 93, 507, 1002, 816, 423, 416, 910, 834, 1146, 669, 148, 1098, 806, 75, 63, 1142, 376, 66, 656, 10, 1203, 166, 85, 551, 736, 547, 22, 769, 668, 121, 707, 652, 395, 1189, 345, 72, 557, 1090, 1077, 1041, 577, 8, 624, 699, 1057, 464, 1078, 161, 532, 691, 445, 924, 695, 209, 671, 315, 1145, 819, 644, 263, 1185, 517, 252, 79, 265, 719, 864, 595, 442, 661, 600, 1009, 1206, 367, 732, 351, 800, 992, 146, 1043, 915, 1114, 826, 303, 1170, 869, 811, 571, 783, 1132, 139, 394, 381, 867, 225, 196, 112, 155, 942, 712, 790, 193, 754, 1101, 218, 881, 907, 1176, 504, 981, 550, 365, 705, 449, 26, 230, 308, 955, 446, 21, 495, 1034, 249, 1191, 18, 743, 549, 1045, 257, 1084, 474, 1097, 1112, 402, 894, 871, 1169, 792, 176, 305, 761, 677, 738, 142, 101, 878, 117, 1001, 1, 105, 337, 625, 853, 90, 753, 988, 147, 78, 626, 62, 1151, 1212, 574, 612, 1003, 821, 812, 627, 1168, 133, 799, 140, 897, 391, 1179, 653, 1027, 421, 1030, 158, 410, 382, 1065, 958, 733, 802, 1032, 484, 882, 720, 916, 396, 678, 540, 1074, 243, 37, 454, 194, 242, 763, 400, 387, 109, 782, 1013, 1180, 64, 383, 1181, 322, 1035, 636, 658, 254, 40, 335, 463, 781, 262, 998, 713, 962, 657, 776, 682, 751, 727, 740, 114, 785, 757, 596, 331, 822, 35, 128, 57, 541, 632, 530, 843, 250, 1154, 589, 167, 510, 522, 226, 1164, 110, 346, 440, 576, 356, 222, 1012, 1200, 144, 590, 925, 949, 473, 457, 635, 983, 68, 54, 1139, 975, 1183, 200, 582, 347, 289, 868, 546, 42, 71, 270, 375, 285, 1205, 1071, 808, 984, 1107, 1040, 1136, 724, 884, 928, 559, 841, 825, 145, 689, 518, 948, 1051, 512, 1005, 19, 964, 1031, 527, 631, 629, 307, 237, 633, 533, 748, 277, 1129, 898, 752, 244, 70, 1122, 1080, 1172, 435, 348, 366, 835, 124]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8924187220196391
the save name prefix for this run is:  chkpt-ID_8924187220196391_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 579
rank avg (pred): 0.462 +- 0.005
mrr vals (pred, true): 0.000, 0.010
batch losses (mrrl, rdl): 0.0, 0.0003354997

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 901
rank avg (pred): 0.113 +- 0.067
mrr vals (pred, true): 0.124, 0.066
batch losses (mrrl, rdl): 0.0, 2.58137e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 446
rank avg (pred): 0.335 +- 0.243
mrr vals (pred, true): 0.242, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003043922

Epoch over!
epoch time: 11.976

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 737
rank avg (pred): 0.029 +- 0.023
mrr vals (pred, true): 0.307, 0.092
batch losses (mrrl, rdl): 0.0, 1.88005e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 855
rank avg (pred): 0.301 +- 0.268
mrr vals (pred, true): 0.346, 0.097
batch losses (mrrl, rdl): 0.0, 8.94288e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 285
rank avg (pred): 0.086 +- 0.076
mrr vals (pred, true): 0.373, 0.103
batch losses (mrrl, rdl): 0.0, 1.51527e-05

Epoch over!
epoch time: 11.654

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.325 +- 0.272
mrr vals (pred, true): 0.337, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003149209

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 896
rank avg (pred): 0.091 +- 0.085
mrr vals (pred, true): 0.394, 0.085
batch losses (mrrl, rdl): 0.0, 1.96918e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1185
rank avg (pred): 0.367 +- 0.297
mrr vals (pred, true): 0.321, 0.066
batch losses (mrrl, rdl): 0.0, 0.0001170757

Epoch over!
epoch time: 11.807

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 315
rank avg (pred): 0.075 +- 0.068
mrr vals (pred, true): 0.391, 0.089
batch losses (mrrl, rdl): 0.0, 1.46733e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 23
rank avg (pred): 0.057 +- 0.053
mrr vals (pred, true): 0.405, 0.245
batch losses (mrrl, rdl): 0.0, 6.8147e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 625
rank avg (pred): 0.362 +- 0.303
mrr vals (pred, true): 0.335, 0.012
batch losses (mrrl, rdl): 0.0, 5.04039e-05

Epoch over!
epoch time: 11.785

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 542
rank avg (pred): 0.167 +- 0.156
mrr vals (pred, true): 0.391, 0.093
batch losses (mrrl, rdl): 0.0, 4.63925e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 234
rank avg (pred): 0.343 +- 0.307
mrr vals (pred, true): 0.362, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002464245

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 695
rank avg (pred): 0.345 +- 0.309
mrr vals (pred, true): 0.370, 0.006
batch losses (mrrl, rdl): 0.0, 0.000236244

Epoch over!
epoch time: 11.759

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.351 +- 0.309
mrr vals (pred, true): 0.366, 0.007
batch losses (mrrl, rdl): 0.9996805191, 0.000234853

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 251
rank avg (pred): 0.017 +- 0.014
mrr vals (pred, true): 0.103, 0.159
batch losses (mrrl, rdl): 0.0313678235, 6.16424e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 264
rank avg (pred): 0.034 +- 0.023
mrr vals (pred, true): 0.103, 0.146
batch losses (mrrl, rdl): 0.017800767, 7.23766e-05

Epoch over!
epoch time: 12.132

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1119
rank avg (pred): 0.272 +- 0.167
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 4.44162e-05, 0.0008618385

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 624
rank avg (pred): 0.458 +- 0.239
mrr vals (pred, true): 0.046, 0.008
batch losses (mrrl, rdl): 0.0001261465, 0.000251779

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 625
rank avg (pred): 0.433 +- 0.248
mrr vals (pred, true): 0.054, 0.012
batch losses (mrrl, rdl): 0.0001309021, 0.0002215147

Epoch over!
epoch time: 11.897

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 704
rank avg (pred): 0.450 +- 0.235
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.000180388, 3.08243e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.472 +- 0.228
mrr vals (pred, true): 0.042, 0.007
batch losses (mrrl, rdl): 0.0006249396, 2.34154e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 810
rank avg (pred): 0.049 +- 0.036
mrr vals (pred, true): 0.106, 0.064
batch losses (mrrl, rdl): 0.0318751931, 2.6521e-05

Epoch over!
epoch time: 11.961

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1167
rank avg (pred): 0.428 +- 0.248
mrr vals (pred, true): 0.062, 0.066
batch losses (mrrl, rdl): 0.0015317337, 0.0003250984

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 200
rank avg (pred): 0.336 +- 0.194
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 2.986e-07, 0.0004024704

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 993
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.276, 0.267
batch losses (mrrl, rdl): 0.0007866643, 2.82217e-05

Epoch over!
epoch time: 11.922

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 76
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.179, 0.167
batch losses (mrrl, rdl): 0.0016236838, 6.50141e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 304
rank avg (pred): 0.053 +- 0.035
mrr vals (pred, true): 0.066, 0.077
batch losses (mrrl, rdl): 0.0026252577, 6.76063e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 836
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.202, 0.207
batch losses (mrrl, rdl): 0.000278358, 6.5062e-06

Epoch over!
epoch time: 11.908

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 973
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.217, 0.241
batch losses (mrrl, rdl): 0.0059556346, 3.2581e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 85
rank avg (pred): 0.406 +- 0.204
mrr vals (pred, true): 0.050, 0.017
batch losses (mrrl, rdl): 9.906e-07, 0.0005714342

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 381
rank avg (pred): 0.367 +- 0.207
mrr vals (pred, true): 0.053, 0.019
batch losses (mrrl, rdl): 7.21162e-05, 0.0003430454

Epoch over!
epoch time: 12.181

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.142 +- 0.096
mrr vals (pred, true): 0.096, 0.106
batch losses (mrrl, rdl): 0.0010524468, 6.38771e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 728
rank avg (pred): 0.410 +- 0.204
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001504701, 0.0001300057

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 362
rank avg (pred): 0.356 +- 0.191
mrr vals (pred, true): 0.047, 0.063
batch losses (mrrl, rdl): 9.80558e-05, 0.0006810524

Epoch over!
epoch time: 12.15

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 520
rank avg (pred): 0.333 +- 0.190
mrr vals (pred, true): 0.051, 0.065
batch losses (mrrl, rdl): 7.1118e-06, 0.0004426512

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 625
rank avg (pred): 0.393 +- 0.197
mrr vals (pred, true): 0.047, 0.012
batch losses (mrrl, rdl): 0.0001135342, 0.0001083717

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 715
rank avg (pred): 0.394 +- 0.195
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 1.40703e-05, 0.0001918182

Epoch over!
epoch time: 12.01

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 559
rank avg (pred): 0.326 +- 0.199
mrr vals (pred, true): 0.054, 0.075
batch losses (mrrl, rdl): 0.0001799146, 0.0004613658

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 995
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.266, 0.260
batch losses (mrrl, rdl): 0.0004166323, 3.02977e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 235
rank avg (pred): 0.368 +- 0.198
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 6.72639e-05, 0.0002172868

Epoch over!
epoch time: 12.046

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 574
rank avg (pred): 0.368 +- 0.198
mrr vals (pred, true): 0.058, 0.011
batch losses (mrrl, rdl): 0.0006349106, 6.88108e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 432
rank avg (pred): 0.369 +- 0.187
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 2.21362e-05, 0.0002443495

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 516
rank avg (pred): 0.356 +- 0.186
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 3.17024e-05, 0.0004672865

Epoch over!
epoch time: 12.029

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.366 +- 0.186
mrr vals (pred, true): 0.051, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   22 	     0 	 0.05057 	 0.00575 	 m..s
   84 	     1 	 0.06379 	 0.00577 	 m..s
   67 	     2 	 0.05439 	 0.00602 	 m..s
   54 	     3 	 0.05165 	 0.00605 	 m..s
   30 	     4 	 0.05071 	 0.00606 	 m..s
   72 	     5 	 0.05513 	 0.00608 	 m..s
   27 	     6 	 0.05065 	 0.00613 	 m..s
   89 	     7 	 0.07610 	 0.00619 	 m..s
   36 	     8 	 0.05093 	 0.00620 	 m..s
   36 	     9 	 0.05093 	 0.00621 	 m..s
   80 	    10 	 0.05832 	 0.00624 	 m..s
   12 	    11 	 0.05029 	 0.00624 	 m..s
   36 	    12 	 0.05093 	 0.00626 	 m..s
   25 	    13 	 0.05064 	 0.00629 	 m..s
   19 	    14 	 0.05050 	 0.00632 	 m..s
   17 	    15 	 0.05035 	 0.00633 	 m..s
   83 	    16 	 0.06321 	 0.00633 	 m..s
   59 	    17 	 0.05228 	 0.00634 	 m..s
   16 	    18 	 0.05035 	 0.00634 	 m..s
   11 	    19 	 0.05017 	 0.00634 	 m..s
   66 	    20 	 0.05430 	 0.00638 	 m..s
   36 	    21 	 0.05093 	 0.00638 	 m..s
    4 	    22 	 0.04858 	 0.00639 	 m..s
   87 	    23 	 0.07406 	 0.00640 	 m..s
   36 	    24 	 0.05093 	 0.00640 	 m..s
   10 	    25 	 0.05009 	 0.00641 	 m..s
   18 	    26 	 0.05044 	 0.00642 	 m..s
   36 	    27 	 0.05093 	 0.00644 	 m..s
   33 	    28 	 0.05076 	 0.00644 	 m..s
   79 	    29 	 0.05706 	 0.00645 	 m..s
   60 	    30 	 0.05324 	 0.00647 	 m..s
   71 	    31 	 0.05504 	 0.00649 	 m..s
   36 	    32 	 0.05093 	 0.00649 	 m..s
    5 	    33 	 0.04895 	 0.00649 	 m..s
   58 	    34 	 0.05218 	 0.00653 	 m..s
   24 	    35 	 0.05058 	 0.00653 	 m..s
   36 	    36 	 0.05093 	 0.00655 	 m..s
   28 	    37 	 0.05066 	 0.00659 	 m..s
   29 	    38 	 0.05070 	 0.00663 	 m..s
   77 	    39 	 0.05620 	 0.00664 	 m..s
    3 	    40 	 0.04854 	 0.00664 	 m..s
   63 	    41 	 0.05387 	 0.00669 	 m..s
    0 	    42 	 0.04363 	 0.00670 	 m..s
   78 	    43 	 0.05646 	 0.00672 	 m..s
   13 	    44 	 0.05032 	 0.00967 	 m..s
   36 	    45 	 0.05093 	 0.01244 	 m..s
   14 	    46 	 0.05033 	 0.01376 	 m..s
   15 	    47 	 0.05034 	 0.01413 	 m..s
   20 	    48 	 0.05052 	 0.01539 	 m..s
    8 	    49 	 0.04974 	 0.01591 	 m..s
   21 	    50 	 0.05056 	 0.01600 	 m..s
    9 	    51 	 0.04982 	 0.01613 	 m..s
   36 	    52 	 0.05093 	 0.01851 	 m..s
   36 	    53 	 0.05093 	 0.02062 	 m..s
   26 	    54 	 0.05065 	 0.02179 	 ~...
   36 	    55 	 0.05093 	 0.02283 	 ~...
   36 	    56 	 0.05093 	 0.02736 	 ~...
   36 	    57 	 0.05093 	 0.02954 	 ~...
   23 	    58 	 0.05058 	 0.03183 	 ~...
   35 	    59 	 0.05080 	 0.03526 	 ~...
   34 	    60 	 0.05079 	 0.03700 	 ~...
   31 	    61 	 0.05072 	 0.03818 	 ~...
   36 	    62 	 0.05093 	 0.03866 	 ~...
   75 	    63 	 0.05566 	 0.03893 	 ~...
   85 	    64 	 0.06445 	 0.04005 	 ~...
   36 	    65 	 0.05093 	 0.04121 	 ~...
   73 	    66 	 0.05537 	 0.04500 	 ~...
   53 	    67 	 0.05131 	 0.04907 	 ~...
   52 	    68 	 0.05103 	 0.05011 	 ~...
    7 	    69 	 0.04939 	 0.05170 	 ~...
   70 	    70 	 0.05499 	 0.05239 	 ~...
    6 	    71 	 0.04924 	 0.05713 	 ~...
   64 	    72 	 0.05391 	 0.05917 	 ~...
   57 	    73 	 0.05185 	 0.06428 	 ~...
   55 	    74 	 0.05171 	 0.06519 	 ~...
   56 	    75 	 0.05173 	 0.06597 	 ~...
   62 	    76 	 0.05347 	 0.06655 	 ~...
   61 	    77 	 0.05334 	 0.06701 	 ~...
   92 	    78 	 0.08176 	 0.07219 	 ~...
   32 	    79 	 0.05075 	 0.07477 	 ~...
   76 	    80 	 0.05581 	 0.07875 	 ~...
   74 	    81 	 0.05546 	 0.08075 	 ~...
   93 	    82 	 0.08283 	 0.08414 	 ~...
   96 	    83 	 0.08465 	 0.08550 	 ~...
   94 	    84 	 0.08285 	 0.08694 	 ~...
   90 	    85 	 0.07807 	 0.08724 	 ~...
   97 	    86 	 0.08490 	 0.09087 	 ~...
   82 	    87 	 0.06094 	 0.09162 	 m..s
   65 	    88 	 0.05427 	 0.09363 	 m..s
   81 	    89 	 0.05963 	 0.09385 	 m..s
    1 	    90 	 0.04780 	 0.09679 	 m..s
   69 	    91 	 0.05487 	 0.09687 	 m..s
    2 	    92 	 0.04812 	 0.09762 	 m..s
   68 	    93 	 0.05441 	 0.09940 	 m..s
   91 	    94 	 0.07974 	 0.10491 	 ~...
  101 	    95 	 0.09987 	 0.10549 	 ~...
  100 	    96 	 0.09981 	 0.10611 	 ~...
   99 	    97 	 0.09979 	 0.10619 	 ~...
  102 	    98 	 0.10471 	 0.10640 	 ~...
   98 	    99 	 0.08567 	 0.11969 	 m..s
   95 	   100 	 0.08321 	 0.11998 	 m..s
   86 	   101 	 0.07218 	 0.12089 	 m..s
   88 	   102 	 0.07514 	 0.12157 	 m..s
  108 	   103 	 0.16848 	 0.12687 	 m..s
  103 	   104 	 0.10588 	 0.13768 	 m..s
  107 	   105 	 0.16840 	 0.14871 	 ~...
  104 	   106 	 0.11037 	 0.16719 	 m..s
  105 	   107 	 0.12159 	 0.17832 	 m..s
  106 	   108 	 0.14945 	 0.18730 	 m..s
  110 	   109 	 0.18472 	 0.19590 	 ~...
  111 	   110 	 0.23061 	 0.21253 	 ~...
  109 	   111 	 0.18272 	 0.21399 	 m..s
  113 	   112 	 0.24456 	 0.21933 	 ~...
  120 	   113 	 0.29504 	 0.25442 	 m..s
  115 	   114 	 0.25526 	 0.26468 	 ~...
  114 	   115 	 0.24677 	 0.27237 	 ~...
  119 	   116 	 0.28520 	 0.27652 	 ~...
  116 	   117 	 0.25754 	 0.28084 	 ~...
  112 	   118 	 0.24108 	 0.28115 	 m..s
  117 	   119 	 0.26616 	 0.28727 	 ~...
  118 	   120 	 0.27497 	 0.29259 	 ~...
==========================================
r_mrr = 0.9133926033973694
r2_mrr = 0.7811453342437744
spearmanr_mrr@5 = 0.9768559336662292
spearmanr_mrr@10 = 0.8568572402000427
spearmanr_mrr@50 = 0.9928673505783081
spearmanr_mrr@100 = 0.9512115716934204
spearmanr_mrr@All = 0.9478563666343689
==========================================
test time: 0.394
Done Testing dataset OpenEA
total time taken: 195.04506063461304
training time taken: 179.68480610847473
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.9134)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7811)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9769)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8569)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9929)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9512)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9479)}}, 'test_loss': {'TransE': {'OpenEA': 0.3925687363735051}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 248854857392949
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1211, 1039, 1024, 1045, 346, 868, 1178, 486, 972, 559, 1005, 586, 345, 910, 240, 697, 1016, 473, 276, 544, 785, 923, 712, 422, 1091, 851, 191, 1193, 284, 156, 135, 1046, 788, 74, 857, 1190, 214, 650, 930, 85, 1022, 321, 366, 328, 37, 509, 558, 954, 1185, 208, 1047, 980, 1172, 225, 455, 695, 897, 320, 822, 1173, 853, 836, 304, 1195, 978, 783, 905, 453, 1034, 1114, 1101, 431, 901, 16, 1062, 1199, 1187, 298, 67, 680, 1205, 247, 339, 909, 57, 1023, 464, 59, 663, 975, 160, 662, 230, 55, 125, 743, 89, 1127, 1036, 831, 1021, 1043, 928, 686, 382, 1017, 1037, 361, 893, 171, 234, 206, 931, 823, 102, 362, 844, 1176, 1029, 1133, 848]
valid_ids (0): []
train_ids (1094): [229, 974, 802, 1140, 572, 766, 643, 891, 161, 46, 621, 35, 651, 342, 1074, 392, 1106, 1111, 458, 405, 219, 237, 1166, 879, 541, 854, 424, 23, 878, 472, 68, 791, 270, 924, 1148, 301, 294, 657, 18, 114, 1104, 146, 614, 71, 988, 251, 306, 113, 981, 62, 120, 631, 952, 263, 871, 655, 1159, 497, 772, 505, 460, 122, 1000, 86, 757, 996, 810, 1164, 186, 211, 202, 52, 1038, 1009, 606, 282, 167, 212, 319, 790, 737, 491, 373, 428, 725, 636, 1081, 136, 646, 872, 805, 729, 259, 960, 986, 1060, 1150, 384, 956, 970, 1126, 1213, 1189, 1082, 898, 756, 637, 1174, 285, 203, 222, 1144, 951, 730, 439, 530, 959, 581, 529, 315, 65, 1085, 358, 163, 781, 717, 661, 1069, 798, 40, 91, 469, 900, 421, 476, 704, 1160, 927, 890, 1063, 579, 850, 554, 795, 387, 971, 832, 0, 538, 42, 170, 95, 312, 349, 552, 946, 88, 393, 17, 296, 451, 741, 1117, 13, 847, 773, 876, 999, 896, 243, 417, 326, 1033, 944, 1067, 112, 934, 261, 1068, 1151, 852, 1170, 198, 1026, 985, 779, 565, 82, 601, 624, 875, 1027, 882, 334, 410, 752, 919, 61, 484, 111, 258, 808, 322, 341, 1025, 533, 204, 815, 244, 97, 1186, 997, 443, 188, 1102, 1110, 696, 286, 1049, 197, 589, 327, 635, 706, 902, 1059, 765, 1012, 277, 300, 1157, 682, 653, 1184, 746, 892, 1162, 31, 49, 307, 673, 96, 169, 827, 11, 659, 448, 691, 152, 364, 351, 649, 209, 271, 668, 739, 1212, 465, 1080, 1115, 179, 196, 137, 175, 652, 1031, 224, 47, 1096, 543, 1073, 189, 913, 354, 218, 969, 545, 596, 837, 709, 758, 498, 964, 488, 1154, 724, 966, 426, 54, 516, 1077, 15, 512, 19, 908, 1145, 178, 1122, 1105, 866, 671, 679, 1057, 575, 518, 616, 314, 423, 53, 444, 246, 76, 926, 963, 627, 1015, 289, 618, 1138, 1194, 904, 714, 887, 602, 45, 1191, 733, 845, 69, 195, 703, 1055, 140, 941, 654, 1072, 744, 490, 12, 800, 429, 28, 479, 617, 493, 953, 674, 480, 993, 22, 861, 990, 563, 194, 690, 399, 94, 1208, 1196, 587, 440, 406, 705, 580, 401, 1130, 1135, 478, 1088, 1116, 1070, 979, 281, 310, 80, 408, 1093, 846, 1206, 711, 376, 355, 98, 1120, 600, 238, 75, 605, 820, 461, 58, 819, 998, 26, 797, 93, 275, 403, 590, 814, 502, 976, 867, 522, 534, 164, 1078, 736, 60, 269, 1134, 253, 553, 430, 692, 681, 159, 548, 666, 1201, 391, 266, 531, 1066, 367, 172, 803, 526, 597, 1006, 335, 279, 557, 475, 368, 254, 940, 442, 914, 1020, 4, 574, 571, 105, 449, 148, 760, 241, 1041, 825, 1058, 280, 436, 166, 267, 1198, 829, 777, 684, 492, 297, 347, 402, 268, 1108, 793, 119, 48, 495, 496, 916, 374, 799, 1139, 27, 100, 955, 948, 239, 157, 344, 221, 231, 824, 363, 457, 155, 912, 886, 834, 1032, 1147, 371, 216, 332, 1118, 701, 550, 720, 1042, 745, 644, 419, 883, 806, 626, 794, 818, 884, 1103, 1065, 713, 353, 380, 710, 459, 856, 379, 608, 1119, 860, 501, 201, 456, 412, 523, 400, 610, 787, 607, 283, 370, 950, 865, 642, 639, 915, 356, 217, 468, 911, 200, 1136, 1094, 228, 1050, 1207, 124, 386, 775, 427, 292, 467, 1052, 994, 6, 1188, 8, 336, 716, 107, 390, 132, 835, 1149, 513, 236, 984, 641, 205, 761, 540, 176, 1123, 907, 723, 769, 658, 1087, 450, 973, 369, 29, 1161, 255, 1203, 735, 570, 32, 123, 629, 595, 982, 311, 957, 942, 562, 1053, 149, 1141, 708, 72, 1125, 154, 598, 308, 1056, 564, 372, 508, 784, 903, 360, 535, 287, 707, 1167, 494, 1156, 129, 647, 774, 759, 992, 260, 862, 226, 256, 1181, 223, 732, 1097, 395, 265, 1177, 1202, 833, 471, 274, 25, 1044, 740, 133, 210, 435, 329, 546, 452, 323, 539, 619, 889, 511, 1210, 117, 551, 66, 165, 375, 573, 536, 937, 388, 858, 676, 1165, 1197, 193, 965, 1183, 764, 272, 77, 173, 1086, 989, 295, 183, 1048, 299, 935, 1040, 63, 507, 863, 325, 670, 1168, 1018, 357, 625, 525, 843, 1004, 1002, 762, 138, 313, 103, 87, 1095, 153, 925, 591, 252, 192, 667, 394, 750, 151, 660, 5, 343, 599, 1214, 447, 1146, 318, 288, 104, 524, 411, 1008, 877, 177, 592, 36, 232, 885, 477, 1152, 213, 583, 144, 184, 734, 398, 264, 542, 101, 273, 338, 78, 895, 630, 73, 638, 1099, 977, 83, 51, 420, 1142, 888, 180, 500, 330, 1011, 147, 532, 1175, 121, 807, 90, 727, 849, 1155, 109, 547, 499, 503, 1171, 162, 881, 770, 158, 812, 1131, 967, 612, 816, 1019, 10, 396, 632, 126, 434, 813, 728, 1137, 1132, 39, 142, 1054, 463, 672, 894, 958, 1010, 873, 350, 482, 418, 483, 932, 506, 567, 220, 1128, 613, 722, 38, 603, 921, 995, 859, 731, 207, 1013, 1180, 754, 316, 584, 576, 1100, 780, 317, 1169, 1076, 604, 947, 675, 792, 474, 487, 1179, 1153, 755, 445, 811, 520, 748, 1192, 1084, 839, 446, 1089, 470, 1003, 917, 593, 648, 749, 181, 702, 880, 9, 961, 233, 519, 56, 462, 389, 348, 21, 383, 687, 128, 929, 409, 830, 416, 782, 767, 385, 127, 768, 987, 840, 1075, 585, 141, 397, 415, 333, 801, 577, 182, 809, 106, 1007, 771, 485, 249, 309, 116, 582, 262, 293, 664, 555, 185, 517, 804, 1035, 1083, 786, 70, 174, 1121, 1071, 404, 425, 763, 215, 568, 337, 1143, 150, 1051, 821, 623, 1124, 1079, 870, 515, 3, 700, 527, 227, 1028, 139, 1113, 906, 826, 1200, 698, 685, 939, 751, 689, 110, 556, 145, 566, 1129, 694, 44, 441, 1092, 79, 968, 413, 528, 433, 874, 1163, 796, 699, 1014, 949, 290, 628, 715, 721, 719, 108, 1001, 432, 678, 1109, 352, 594, 41, 250, 521, 438, 377, 381, 1064, 115, 1158, 669, 359, 130, 936, 869, 168, 726, 34, 609, 828, 945, 842, 1112, 278, 753, 560, 131, 365, 134, 1090, 578, 645, 7, 688, 81, 64, 437, 143, 569, 841, 615, 738, 1107, 303, 991, 43, 331, 864, 30, 481, 778, 305, 84, 665, 199, 33, 257, 414, 24, 245, 99, 187, 611, 817, 933, 838, 489, 789, 633, 1061, 855, 983, 190, 378, 514, 504, 454, 537, 291, 918, 588, 747, 302, 943, 340, 634, 920, 693, 50, 922, 324, 1204, 640, 656, 1182, 510, 407, 677, 118, 92, 20, 248, 14, 938, 242, 620, 1098, 1, 1030, 235, 899, 2, 742, 466, 962, 1209, 622, 549, 561, 683, 718, 776]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6506170420749743
the save name prefix for this run is:  chkpt-ID_6506170420749743_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 350
rank avg (pred): 0.554 +- 0.003
mrr vals (pred, true): 0.000, 0.071
batch losses (mrrl, rdl): 0.0, 0.0029655017

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 684
rank avg (pred): 0.363 +- 0.259
mrr vals (pred, true): 0.183, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001926273

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 732
rank avg (pred): 0.064 +- 0.052
mrr vals (pred, true): 0.300, 0.073
batch losses (mrrl, rdl): 0.0, 6.1661e-06

Epoch over!
epoch time: 11.941

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1074
rank avg (pred): 0.079 +- 0.065
mrr vals (pred, true): 0.311, 0.277
batch losses (mrrl, rdl): 0.0, 4.41844e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 267
rank avg (pred): 0.072 +- 0.064
mrr vals (pred, true): 0.359, 0.157
batch losses (mrrl, rdl): 0.0, 1.64299e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1078
rank avg (pred): 0.052 +- 0.045
mrr vals (pred, true): 0.362, 0.290
batch losses (mrrl, rdl): 0.0, 1.0547e-05

Epoch over!
epoch time: 11.771

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 248
rank avg (pred): 0.057 +- 0.053
mrr vals (pred, true): 0.390, 0.160
batch losses (mrrl, rdl): 0.0, 8.1333e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 517
rank avg (pred): 0.156 +- 0.151
mrr vals (pred, true): 0.403, 0.067
batch losses (mrrl, rdl): 0.0, 7.97426e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 688
rank avg (pred): 0.362 +- 0.309
mrr vals (pred, true): 0.337, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001764665

Epoch over!
epoch time: 11.752

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 375
rank avg (pred): 0.317 +- 0.300
mrr vals (pred, true): 0.392, 0.023
batch losses (mrrl, rdl): 0.0, 0.0002733245

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 780
rank avg (pred): 0.333 +- 0.300
mrr vals (pred, true): 0.363, 0.100
batch losses (mrrl, rdl): 0.0, 0.0001803979

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 409
rank avg (pred): 0.316 +- 0.296
mrr vals (pred, true): 0.388, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003562016

Epoch over!
epoch time: 11.711

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 920
rank avg (pred): 0.348 +- 0.313
mrr vals (pred, true): 0.377, 0.096
batch losses (mrrl, rdl): 0.0, 0.000199302

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 185
rank avg (pred): 0.323 +- 0.291
mrr vals (pred, true): 0.352, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002995831

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 311
rank avg (pred): 0.052 +- 0.049
mrr vals (pred, true): 0.397, 0.158
batch losses (mrrl, rdl): 0.0, 1.04159e-05

Epoch over!
epoch time: 12.013

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 374
rank avg (pred): 0.347 +- 0.302
mrr vals (pred, true): 0.327, 0.073
batch losses (mrrl, rdl): 0.7663735151, 0.000783049

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 671
rank avg (pred): 0.419 +- 0.197
mrr vals (pred, true): 0.063, 0.006
batch losses (mrrl, rdl): 0.0017733615, 0.0001042033

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 33
rank avg (pred): 0.081 +- 0.051
mrr vals (pred, true): 0.089, 0.066
batch losses (mrrl, rdl): 0.0150714815, 3.19702e-05

Epoch over!
epoch time: 12.029

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.400 +- 0.214
mrr vals (pred, true): 0.074, 0.104
batch losses (mrrl, rdl): 0.0086719012, 0.0012403483

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 803
rank avg (pred): 0.433 +- 0.170
mrr vals (pred, true): 0.044, 0.006
batch losses (mrrl, rdl): 0.0004185539, 0.0001008617

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 398
rank avg (pred): 0.375 +- 0.202
mrr vals (pred, true): 0.061, 0.067
batch losses (mrrl, rdl): 0.0012461289, 0.0009092945

Epoch over!
epoch time: 12.132

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 814
rank avg (pred): 0.047 +- 0.030
mrr vals (pred, true): 0.105, 0.070
batch losses (mrrl, rdl): 0.0301976725, 4.7131e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 565
rank avg (pred): 0.321 +- 0.190
mrr vals (pred, true): 0.061, 0.081
batch losses (mrrl, rdl): 0.0011156708, 0.0004674639

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 756
rank avg (pred): 0.407 +- 0.175
mrr vals (pred, true): 0.047, 0.097
batch losses (mrrl, rdl): 9.80663e-05, 0.0004717986

Epoch over!
epoch time: 12.025

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1192
rank avg (pred): 0.382 +- 0.197
mrr vals (pred, true): 0.061, 0.006
batch losses (mrrl, rdl): 0.0012013364, 0.0002130742

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1140
rank avg (pred): 0.033 +- 0.022
mrr vals (pred, true): 0.115, 0.111
batch losses (mrrl, rdl): 0.0001855238, 0.000396209

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 142
rank avg (pred): 0.402 +- 0.173
mrr vals (pred, true): 0.046, 0.017
batch losses (mrrl, rdl): 0.0001958158, 0.000525301

Epoch over!
epoch time: 12.034

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 64
rank avg (pred): 0.046 +- 0.031
mrr vals (pred, true): 0.114, 0.145
batch losses (mrrl, rdl): 0.0097800568, 1.64796e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 702
rank avg (pred): 0.388 +- 0.179
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 3.94365e-05, 0.0002055076

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 704
rank avg (pred): 0.400 +- 0.170
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.0002517437, 0.0001930354

Epoch over!
epoch time: 12.167

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 850
rank avg (pred): 0.388 +- 0.177
mrr vals (pred, true): 0.049, 0.095
batch losses (mrrl, rdl): 1.7892e-05, 0.0004198365

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 150
rank avg (pred): 0.387 +- 0.177
mrr vals (pred, true): 0.048, 0.013
batch losses (mrrl, rdl): 2.34287e-05, 0.0003509542

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 310
rank avg (pred): 0.048 +- 0.032
mrr vals (pred, true): 0.094, 0.120
batch losses (mrrl, rdl): 0.0066272048, 3.41704e-05

Epoch over!
epoch time: 12.145

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 285
rank avg (pred): 0.094 +- 0.064
mrr vals (pred, true): 0.080, 0.103
batch losses (mrrl, rdl): 0.0051991944, 1.73455e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 221
rank avg (pred): 0.397 +- 0.170
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002196343, 0.0001764534

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 281
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.213, 0.149
batch losses (mrrl, rdl): 0.0410209, 7.73158e-05

Epoch over!
epoch time: 11.976

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 865
rank avg (pred): 0.399 +- 0.167
mrr vals (pred, true): 0.040, 0.007
batch losses (mrrl, rdl): 0.0009393772, 0.0001780195

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 915
rank avg (pred): 0.283 +- 0.185
mrr vals (pred, true): 0.063, 0.118
batch losses (mrrl, rdl): 0.0297349282, 0.0011310669

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 499
rank avg (pred): 0.308 +- 0.201
mrr vals (pred, true): 0.079, 0.079
batch losses (mrrl, rdl): 0.0081552854, 0.0003697664

Epoch over!
epoch time: 11.887

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1059
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.291, 0.295
batch losses (mrrl, rdl): 0.0001243479, 2.07834e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 410
rank avg (pred): 0.368 +- 0.185
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 1.42359e-05, 0.0003276582

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 219
rank avg (pred): 0.413 +- 0.168
mrr vals (pred, true): 0.039, 0.006
batch losses (mrrl, rdl): 0.0012893884, 0.0001554415

Epoch over!
epoch time: 11.917

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.186 +- 0.125
mrr vals (pred, true): 0.087, 0.106
batch losses (mrrl, rdl): 0.0037292722, 7.88758e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 86
rank avg (pred): 0.385 +- 0.170
mrr vals (pred, true): 0.049, 0.037
batch losses (mrrl, rdl): 7.9396e-06, 0.0006993932

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.339 +- 0.193
mrr vals (pred, true): 0.057, 0.006
batch losses (mrrl, rdl): 0.0004894858, 0.0003917796

Epoch over!
epoch time: 12.188

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.377 +- 0.175
mrr vals (pred, true): 0.044, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   64 	     0 	 0.04825 	 0.00597 	 m..s
    0 	     1 	 0.03585 	 0.00606 	 ~...
   84 	     2 	 0.05850 	 0.00609 	 m..s
   16 	     3 	 0.03961 	 0.00616 	 m..s
    6 	     4 	 0.03765 	 0.00619 	 m..s
   42 	     5 	 0.04173 	 0.00621 	 m..s
   13 	     6 	 0.03924 	 0.00622 	 m..s
   93 	     7 	 0.06008 	 0.00627 	 m..s
   83 	     8 	 0.05668 	 0.00630 	 m..s
   46 	     9 	 0.04206 	 0.00630 	 m..s
    3 	    10 	 0.03608 	 0.00630 	 ~...
   56 	    11 	 0.04543 	 0.00631 	 m..s
   53 	    12 	 0.04424 	 0.00632 	 m..s
   28 	    13 	 0.04059 	 0.00635 	 m..s
   10 	    14 	 0.03878 	 0.00637 	 m..s
   95 	    15 	 0.06113 	 0.00637 	 m..s
   63 	    16 	 0.04824 	 0.00638 	 m..s
   61 	    17 	 0.04700 	 0.00638 	 m..s
    4 	    18 	 0.03684 	 0.00640 	 m..s
   41 	    19 	 0.04168 	 0.00641 	 m..s
   76 	    20 	 0.05216 	 0.00641 	 m..s
   18 	    21 	 0.03967 	 0.00641 	 m..s
   11 	    22 	 0.03885 	 0.00643 	 m..s
    2 	    23 	 0.03604 	 0.00643 	 ~...
   49 	    24 	 0.04336 	 0.00643 	 m..s
   29 	    25 	 0.04085 	 0.00645 	 m..s
   17 	    26 	 0.03961 	 0.00647 	 m..s
   47 	    27 	 0.04257 	 0.00648 	 m..s
    1 	    28 	 0.03594 	 0.00651 	 ~...
   67 	    29 	 0.04890 	 0.00653 	 m..s
   33 	    30 	 0.04102 	 0.00654 	 m..s
   86 	    31 	 0.05899 	 0.00655 	 m..s
   65 	    32 	 0.04848 	 0.00655 	 m..s
   69 	    33 	 0.04941 	 0.00656 	 m..s
    8 	    34 	 0.03782 	 0.00656 	 m..s
   24 	    35 	 0.04002 	 0.00657 	 m..s
   71 	    36 	 0.05001 	 0.00658 	 m..s
   70 	    37 	 0.04963 	 0.00658 	 m..s
   68 	    38 	 0.04891 	 0.00660 	 m..s
   27 	    39 	 0.04035 	 0.00661 	 m..s
   15 	    40 	 0.03952 	 0.00664 	 m..s
   51 	    41 	 0.04396 	 0.00668 	 m..s
    9 	    42 	 0.03790 	 0.00670 	 m..s
    7 	    43 	 0.03773 	 0.00671 	 m..s
   32 	    44 	 0.04089 	 0.00673 	 m..s
   14 	    45 	 0.03941 	 0.00675 	 m..s
   12 	    46 	 0.03889 	 0.01244 	 ~...
   26 	    47 	 0.04029 	 0.01393 	 ~...
   22 	    48 	 0.03987 	 0.01485 	 ~...
   35 	    49 	 0.04123 	 0.01575 	 ~...
    5 	    50 	 0.03695 	 0.01653 	 ~...
   45 	    51 	 0.04200 	 0.02177 	 ~...
   25 	    52 	 0.04004 	 0.02182 	 ~...
   43 	    53 	 0.04193 	 0.02255 	 ~...
   52 	    54 	 0.04410 	 0.02327 	 ~...
   20 	    55 	 0.03984 	 0.02538 	 ~...
   19 	    56 	 0.03977 	 0.02772 	 ~...
   23 	    57 	 0.03997 	 0.03191 	 ~...
   62 	    58 	 0.04722 	 0.03365 	 ~...
   40 	    59 	 0.04156 	 0.03485 	 ~...
   54 	    60 	 0.04429 	 0.03739 	 ~...
   21 	    61 	 0.03986 	 0.03990 	 ~...
   77 	    62 	 0.05613 	 0.05164 	 ~...
   77 	    63 	 0.05613 	 0.05202 	 ~...
   66 	    64 	 0.04853 	 0.05225 	 ~...
   77 	    65 	 0.05613 	 0.05662 	 ~...
   55 	    66 	 0.04440 	 0.05813 	 ~...
   96 	    67 	 0.06408 	 0.05962 	 ~...
  107 	    68 	 0.08701 	 0.06172 	 ~...
   92 	    69 	 0.06006 	 0.06223 	 ~...
   90 	    70 	 0.05997 	 0.06317 	 ~...
   50 	    71 	 0.04381 	 0.06318 	 ~...
   60 	    72 	 0.04580 	 0.06457 	 ~...
   57 	    73 	 0.04563 	 0.06486 	 ~...
   77 	    74 	 0.05613 	 0.06519 	 ~...
   91 	    75 	 0.06000 	 0.06558 	 ~...
   58 	    76 	 0.04567 	 0.06559 	 ~...
   59 	    77 	 0.04576 	 0.06590 	 ~...
   97 	    78 	 0.06599 	 0.06665 	 ~...
   73 	    79 	 0.05052 	 0.07131 	 ~...
   77 	    80 	 0.05613 	 0.07550 	 ~...
  102 	    81 	 0.07857 	 0.07721 	 ~...
  101 	    82 	 0.07775 	 0.07981 	 ~...
  105 	    83 	 0.08177 	 0.08613 	 ~...
   48 	    84 	 0.04274 	 0.08716 	 m..s
   72 	    85 	 0.05015 	 0.08855 	 m..s
   38 	    86 	 0.04146 	 0.09260 	 m..s
   37 	    87 	 0.04146 	 0.09363 	 m..s
   94 	    88 	 0.06037 	 0.09385 	 m..s
   44 	    89 	 0.04196 	 0.09492 	 m..s
   36 	    90 	 0.04129 	 0.09497 	 m..s
   39 	    91 	 0.04148 	 0.09588 	 m..s
   34 	    92 	 0.04104 	 0.09660 	 m..s
   31 	    93 	 0.04087 	 0.09671 	 m..s
   30 	    94 	 0.04087 	 0.09837 	 m..s
   89 	    95 	 0.05976 	 0.09922 	 m..s
  104 	    96 	 0.08038 	 0.09990 	 ~...
   74 	    97 	 0.05084 	 0.10220 	 m..s
  109 	    98 	 0.08863 	 0.10352 	 ~...
   77 	    99 	 0.05613 	 0.10387 	 m..s
   87 	   100 	 0.05912 	 0.10582 	 m..s
   88 	   101 	 0.05928 	 0.10601 	 m..s
   75 	   102 	 0.05105 	 0.10869 	 m..s
   85 	   103 	 0.05862 	 0.11240 	 m..s
  113 	   104 	 0.14372 	 0.12940 	 ~...
  108 	   105 	 0.08723 	 0.12970 	 m..s
  110 	   106 	 0.09054 	 0.13613 	 m..s
   99 	   107 	 0.07351 	 0.14016 	 m..s
  100 	   108 	 0.07382 	 0.14201 	 m..s
   98 	   109 	 0.07350 	 0.14341 	 m..s
  106 	   110 	 0.08678 	 0.14474 	 m..s
  103 	   111 	 0.07953 	 0.14633 	 m..s
  112 	   112 	 0.14259 	 0.16702 	 ~...
  114 	   113 	 0.17153 	 0.20588 	 m..s
  111 	   114 	 0.10127 	 0.20702 	 MISS
  115 	   115 	 0.18233 	 0.24897 	 m..s
  117 	   116 	 0.20404 	 0.27598 	 m..s
  118 	   117 	 0.22003 	 0.27746 	 m..s
  116 	   118 	 0.19596 	 0.27844 	 m..s
  119 	   119 	 0.28630 	 0.29075 	 ~...
  120 	   120 	 0.29409 	 0.29220 	 ~...
==========================================
r_mrr = 0.8524390459060669
r2_mrr = 0.6864331960678101
spearmanr_mrr@5 = 0.9962321519851685
spearmanr_mrr@10 = 0.8846924901008606
spearmanr_mrr@50 = 0.964886486530304
spearmanr_mrr@100 = 0.917232871055603
spearmanr_mrr@All = 0.9169026613235474
==========================================
test time: 0.44
Done Testing dataset OpenEA
total time taken: 195.94103980064392
training time taken: 180.2140190601349
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8524)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.6864)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9962)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8847)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9649)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9172)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9169)}}, 'test_loss': {'TransE': {'OpenEA': 0.9042894389858702}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 9809430085648068
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [249, 567, 568, 165, 236, 294, 940, 534, 43, 700, 200, 517, 539, 119, 379, 774, 266, 887, 521, 665, 53, 506, 873, 735, 1199, 1136, 880, 608, 911, 1200, 614, 76, 704, 894, 697, 746, 317, 729, 215, 275, 351, 59, 221, 947, 487, 394, 852, 392, 913, 158, 995, 636, 813, 960, 855, 1127, 677, 457, 789, 1018, 1207, 364, 599, 450, 761, 384, 844, 734, 1056, 966, 156, 357, 603, 826, 1082, 622, 57, 554, 580, 587, 346, 1196, 792, 747, 50, 99, 1094, 128, 621, 646, 917, 155, 1083, 935, 522, 615, 129, 1179, 850, 338, 739, 589, 388, 270, 444, 1081, 27, 254, 975, 963, 1057, 293, 879, 716, 1051, 724, 1000, 967, 14, 781, 39]
valid_ids (0): []
train_ids (1094): [1151, 430, 407, 989, 121, 252, 153, 73, 297, 369, 548, 387, 1111, 992, 328, 222, 157, 721, 843, 173, 490, 47, 1016, 949, 332, 380, 479, 1053, 1166, 519, 762, 1201, 1180, 478, 647, 1108, 118, 1078, 285, 202, 600, 1144, 659, 37, 930, 552, 780, 799, 1045, 239, 234, 1080, 657, 987, 886, 642, 45, 2, 835, 964, 494, 1097, 296, 415, 562, 709, 1021, 925, 997, 1107, 115, 612, 637, 923, 102, 448, 232, 1152, 149, 514, 341, 485, 311, 756, 620, 310, 5, 891, 134, 556, 907, 447, 48, 1154, 518, 875, 1150, 453, 764, 49, 899, 1091, 982, 1187, 1100, 1104, 861, 77, 948, 837, 1041, 1213, 177, 131, 515, 922, 19, 183, 101, 1121, 722, 1113, 810, 24, 736, 182, 359, 313, 206, 1023, 83, 1062, 595, 7, 312, 512, 993, 974, 968, 730, 1095, 634, 8, 315, 1146, 1070, 15, 306, 97, 573, 1020, 924, 1172, 190, 753, 605, 830, 921, 360, 566, 122, 141, 1074, 145, 1130, 583, 890, 290, 148, 495, 941, 508, 240, 154, 1208, 927, 784, 247, 928, 288, 1131, 976, 585, 1064, 832, 225, 1015, 21, 817, 794, 733, 216, 609, 897, 1055, 65, 439, 814, 62, 878, 866, 405, 1077, 549, 1, 708, 523, 606, 871, 1126, 1158, 1147, 227, 208, 300, 1120, 51, 815, 498, 641, 624, 117, 644, 318, 1156, 824, 591, 1195, 936, 1177, 691, 81, 827, 427, 1181, 1068, 888, 437, 586, 96, 1167, 682, 1106, 1205, 12, 147, 648, 785, 330, 749, 1197, 807, 31, 867, 320, 616, 668, 418, 468, 203, 868, 623, 434, 858, 180, 262, 795, 325, 1099, 314, 191, 55, 36, 1019, 272, 486, 802, 1139, 1191, 651, 488, 130, 212, 725, 1138, 425, 1079, 416, 771, 667, 841, 933, 454, 267, 500, 424, 419, 1149, 782, 493, 1001, 981, 653, 635, 957, 461, 1093, 1036, 932, 503, 678, 451, 712, 983, 466, 675, 470, 399, 582, 805, 342, 806, 86, 366, 1028, 449, 618, 446, 740, 885, 251, 669, 690, 139, 825, 819, 662, 1109, 125, 223, 271, 1098, 323, 302, 1054, 198, 391, 246, 598, 322, 1185, 398, 590, 304, 741, 1049, 1164, 538, 1034, 100, 643, 769, 18, 475, 95, 260, 187, 69, 853, 919, 463, 631, 576, 596, 776, 686, 671, 1115, 250, 905, 570, 146, 319, 816, 1169, 460, 396, 483, 1141, 151, 854, 607, 788, 869, 934, 41, 462, 1159, 1198, 877, 684, 710, 1037, 492, 417, 383, 38, 26, 803, 13, 990, 632, 979, 337, 564, 1044, 344, 847, 195, 783, 1137, 1162, 143, 1148, 501, 986, 32, 652, 527, 228, 1046, 939, 857, 279, 253, 985, 496, 105, 124, 674, 565, 1008, 348, 88, 625, 166, 135, 352, 908, 71, 779, 541, 104, 718, 680, 574, 1178, 1043, 1184, 358, 353, 220, 1029, 1006, 1135, 1076, 70, 482, 1087, 382, 1088, 244, 343, 201, 1048, 859, 1063, 876, 308, 441, 849, 355, 257, 906, 276, 1212, 980, 531, 560, 577, 409, 658, 80, 588, 563, 903, 679, 796, 142, 703, 443, 263, 9, 823, 1165, 289, 828, 970, 772, 414, 973, 640, 248, 1209, 836, 112, 1174, 773, 1168, 433, 58, 426, 1030, 110, 901, 687, 1124, 75, 10, 422, 602, 445, 918, 797, 209, 1203, 367, 601, 333, 186, 1142, 1125, 683, 61, 532, 743, 85, 305, 649, 472, 84, 1122, 196, 282, 1005, 261, 133, 831, 1007, 731, 386, 786, 277, 1129, 862, 713, 507, 1105, 412, 403, 226, 865, 1173, 726, 915, 714, 1112, 952, 243, 459, 159, 1022, 872, 1089, 1133, 988, 1110, 889, 1160, 268, 555, 25, 695, 1072, 860, 238, 1067, 745, 1175, 511, 959, 287, 693, 17, 804, 309, 611, 1014, 1052, 845, 1059, 720, 916, 404, 1123, 170, 469, 29, 1075, 224, 1002, 120, 1204, 787, 584, 702, 1040, 301, 1101, 1118, 630, 480, 281, 955, 1186, 937, 210, 676, 1193, 233, 594, 345, 23, 619, 1092, 856, 535, 497, 701, 1206, 558, 72, 436, 1011, 553, 758, 663, 870, 1116, 401, 321, 484, 881, 798, 231, 999, 64, 286, 950, 1214, 349, 205, 138, 1170, 91, 20, 628, 520, 280, 529, 1004, 207, 163, 274, 654, 370, 476, 1060, 1143, 800, 1153, 748, 473, 1024, 1009, 193, 229, 400, 898, 93, 471, 356, 406, 1066, 1031, 579, 108, 113, 895, 435, 965, 1017, 411, 971, 42, 372, 874, 811, 181, 1065, 499, 1190, 126, 1038, 11, 46, 1096, 481, 884, 793, 362, 197, 516, 160, 35, 543, 1183, 984, 581, 6, 455, 4, 199, 1119, 269, 218, 298, 944, 502, 705, 892, 1071, 820, 336, 350, 528, 597, 1032, 685, 375, 1188, 775, 996, 883, 1102, 510, 78, 1010, 1134, 335, 339, 303, 385, 1171, 540, 474, 738, 509, 211, 592, 242, 1176, 699, 137, 395, 1042, 63, 910, 1035, 397, 402, 381, 452, 732, 389, 467, 327, 464, 1058, 377, 777, 109, 938, 1085, 168, 882, 757, 28, 575, 245, 465, 1189, 978, 82, 833, 299, 421, 132, 1117, 89, 52, 778, 744, 442, 767, 656, 639, 283, 544, 106, 692, 750, 954, 264, 1114, 1061, 572, 54, 140, 256, 161, 696, 929, 22, 66, 571, 67, 525, 295, 284, 90, 423, 533, 962, 1025, 629, 664, 74, 16, 670, 926, 34, 961, 956, 127, 645, 920, 638, 904, 909, 144, 331, 545, 717, 505, 136, 742, 537, 791, 324, 185, 896, 536, 184, 162, 361, 1145, 1050, 863, 1155, 661, 846, 1047, 840, 707, 334, 900, 103, 998, 851, 189, 107, 491, 754, 408, 613, 44, 340, 194, 390, 1026, 839, 672, 204, 688, 719, 994, 550, 291, 1013, 546, 1012, 728, 801, 326, 255, 706, 977, 68, 347, 1073, 114, 376, 542, 969, 278, 374, 116, 650, 834, 912, 169, 593, 1140, 373, 92, 413, 838, 1033, 40, 259, 829, 991, 711, 513, 213, 428, 559, 943, 171, 953, 660, 557, 1090, 1161, 60, 87, 727, 1027, 241, 561, 617, 766, 902, 432, 456, 864, 307, 56, 378, 760, 604, 217, 763, 1132, 1128, 931, 94, 1210, 755, 178, 551, 808, 235, 365, 176, 214, 759, 167, 30, 192, 273, 1211, 458, 812, 33, 316, 914, 821, 723, 569, 329, 578, 477, 633, 0, 123, 530, 715, 438, 79, 809, 1194, 410, 946, 524, 152, 737, 393, 681, 150, 1182, 770, 1003, 666, 179, 431, 98, 694, 751, 1103, 429, 842, 626, 752, 627, 673, 174, 175, 848, 972, 354, 822, 768, 292, 1039, 1086, 489, 893, 1084, 689, 3, 790, 230, 172, 698, 188, 111, 237, 1163, 363, 1202, 1192, 164, 818, 440, 258, 504, 547, 371, 765, 526, 958, 420, 265, 655, 945, 1069, 951, 368, 610, 942, 1157, 219]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  738739121042679
the save name prefix for this run is:  chkpt-ID_738739121042679_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 262
rank avg (pred): 0.434 +- 0.018
mrr vals (pred, true): 0.000, 0.208
batch losses (mrrl, rdl): 0.0, 0.0028829889

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 992
rank avg (pred): 0.059 +- 0.047
mrr vals (pred, true): 0.281, 0.218
batch losses (mrrl, rdl): 0.0, 9.1042e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 482
rank avg (pred): 0.309 +- 0.278
mrr vals (pred, true): 0.342, 0.006
batch losses (mrrl, rdl): 0.0, 0.0004030667

Epoch over!
epoch time: 21.179

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 159
rank avg (pred): 0.331 +- 0.286
mrr vals (pred, true): 0.330, 0.016
batch losses (mrrl, rdl): 0.0, 0.0002485104

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 821
rank avg (pred): 0.047 +- 0.044
mrr vals (pred, true): 0.391, 0.084
batch losses (mrrl, rdl): 0.0, 6.8689e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 479
rank avg (pred): 0.309 +- 0.279
mrr vals (pred, true): 0.360, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003876455

Epoch over!
epoch time: 23.049

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 367
rank avg (pred): 0.337 +- 0.299
mrr vals (pred, true): 0.351, 0.036
batch losses (mrrl, rdl): 0.0, 0.0004584521

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 941
rank avg (pred): 0.355 +- 0.317
mrr vals (pred, true): 0.366, 0.097
batch losses (mrrl, rdl): 0.0, 0.0002688053

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 345
rank avg (pred): 0.354 +- 0.311
mrr vals (pred, true): 0.349, 0.023
batch losses (mrrl, rdl): 0.0, 0.0004582251

Epoch over!
epoch time: 18.237

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1091
rank avg (pred): 0.333 +- 0.297
mrr vals (pred, true): 0.353, 0.071
batch losses (mrrl, rdl): 0.0, 0.0005585092

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1177
rank avg (pred): 0.362 +- 0.312
mrr vals (pred, true): 0.343, 0.066
batch losses (mrrl, rdl): 0.0, 9.93855e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 361
rank avg (pred): 0.331 +- 0.295
mrr vals (pred, true): 0.357, 0.032
batch losses (mrrl, rdl): 0.0, 0.0003872236

Epoch over!
epoch time: 11.908

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 786
rank avg (pred): 0.314 +- 0.289
mrr vals (pred, true): 0.383, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003316874

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 195
rank avg (pred): 0.340 +- 0.307
mrr vals (pred, true): 0.358, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002152165

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 872
rank avg (pred): 0.343 +- 0.305
mrr vals (pred, true): 0.361, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002197169

Epoch over!
epoch time: 11.719

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 33
rank avg (pred): 0.090 +- 0.082
mrr vals (pred, true): 0.382, 0.066
batch losses (mrrl, rdl): 1.1047400236, 1.85023e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1202
rank avg (pred): 0.378 +- 0.159
mrr vals (pred, true): 0.069, 0.007
batch losses (mrrl, rdl): 0.0036783565, 0.0002621154

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1159
rank avg (pred): 0.168 +- 0.117
mrr vals (pred, true): 0.102, 0.108
batch losses (mrrl, rdl): 0.0003622383, 6.21167e-05

Epoch over!
epoch time: 12.271

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.122 +- 0.098
mrr vals (pred, true): 0.112, 0.109
batch losses (mrrl, rdl): 9.35794e-05, 7.6321e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.228 +- 0.150
mrr vals (pred, true): 0.094, 0.106
batch losses (mrrl, rdl): 0.0016109417, 0.0001757932

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 356
rank avg (pred): 0.406 +- 0.132
mrr vals (pred, true): 0.043, 0.056
batch losses (mrrl, rdl): 0.0004310384, 0.0009383545

Epoch over!
epoch time: 11.869

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 88
rank avg (pred): 0.367 +- 0.130
mrr vals (pred, true): 0.044, 0.016
batch losses (mrrl, rdl): 0.0003212457, 0.0003145285

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 908
rank avg (pred): 0.244 +- 0.170
mrr vals (pred, true): 0.101, 0.111
batch losses (mrrl, rdl): 0.0009939054, 0.0007529411

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 956
rank avg (pred): 0.347 +- 0.131
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 1.35639e-05, 0.0004638314

Epoch over!
epoch time: 11.886

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 262
rank avg (pred): 0.015 +- 0.013
mrr vals (pred, true): 0.157, 0.208
batch losses (mrrl, rdl): 0.0252735652, 6.9052e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 797
rank avg (pred): 0.348 +- 0.126
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 2.18035e-05, 0.0004655453

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1085
rank avg (pred): 0.386 +- 0.147
mrr vals (pred, true): 0.065, 0.102
batch losses (mrrl, rdl): 0.0137823625, 0.0010234183

Epoch over!
epoch time: 12.024

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 515
rank avg (pred): 0.326 +- 0.138
mrr vals (pred, true): 0.069, 0.095
batch losses (mrrl, rdl): 0.003740038, 0.0005722531

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 189
rank avg (pred): 0.365 +- 0.107
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 7.85536e-05, 0.0003715294

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 707
rank avg (pred): 0.406 +- 0.108
mrr vals (pred, true): 0.043, 0.006
batch losses (mrrl, rdl): 0.0004569918, 0.0002584617

Epoch over!
epoch time: 11.979

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 393
rank avg (pred): 0.350 +- 0.104
mrr vals (pred, true): 0.049, 0.023
batch losses (mrrl, rdl): 1.85252e-05, 0.0003301472

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 680
rank avg (pred): 0.411 +- 0.106
mrr vals (pred, true): 0.041, 0.006
batch losses (mrrl, rdl): 0.0007988277, 0.0002577745

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 945
rank avg (pred): 0.306 +- 0.102
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 6.95337e-05, 0.0007421069

Epoch over!
epoch time: 12.373

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1085
rank avg (pred): 0.372 +- 0.138
mrr vals (pred, true): 0.065, 0.102
batch losses (mrrl, rdl): 0.01413716, 0.0009003493

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1106
rank avg (pred): 0.360 +- 0.146
mrr vals (pred, true): 0.070, 0.131
batch losses (mrrl, rdl): 0.0372921303, 0.0008942013

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1117
rank avg (pred): 0.370 +- 0.127
mrr vals (pred, true): 0.055, 0.006
batch losses (mrrl, rdl): 0.0002134055, 0.0003447774

Epoch over!
epoch time: 11.956

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.360 +- 0.134
mrr vals (pred, true): 0.058, 0.007
batch losses (mrrl, rdl): 0.0006825863, 0.0004369036

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.367 +- 0.136
mrr vals (pred, true): 0.060, 0.104
batch losses (mrrl, rdl): 0.0192895718, 0.0008981618

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 290
rank avg (pred): 0.003 +- 0.004
mrr vals (pred, true): 0.280, 0.272
batch losses (mrrl, rdl): 0.0005801728, 6.03091e-05

Epoch over!
epoch time: 12.001

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 90
rank avg (pred): 0.374 +- 0.107
mrr vals (pred, true): 0.047, 0.013
batch losses (mrrl, rdl): 0.0001124006, 0.0002837849

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.239 +- 0.142
mrr vals (pred, true): 0.102, 0.106
batch losses (mrrl, rdl): 0.000141197, 0.0002032496

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 112
rank avg (pred): 0.384 +- 0.109
mrr vals (pred, true): 0.049, 0.017
batch losses (mrrl, rdl): 5.5113e-06, 0.0004458887

Epoch over!
epoch time: 12.535

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 777
rank avg (pred): 0.387 +- 0.109
mrr vals (pred, true): 0.046, 0.095
batch losses (mrrl, rdl): 0.0001353107, 0.0003975968

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 326
rank avg (pred): 0.389 +- 0.114
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 5.57995e-05, 0.0007967358

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 649
rank avg (pred): 0.375 +- 0.113
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 7.30496e-05, 0.0003309342

Epoch over!
epoch time: 12.113

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.057 +- 0.057
mrr vals (pred, true): 0.069, 0.079

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   40 	     0 	 0.05040 	 0.00581 	 m..s
   56 	     1 	 0.05409 	 0.00602 	 m..s
   66 	     2 	 0.05604 	 0.00625 	 m..s
   18 	     3 	 0.04816 	 0.00627 	 m..s
   17 	     4 	 0.04804 	 0.00633 	 m..s
    7 	     5 	 0.04605 	 0.00634 	 m..s
    9 	     6 	 0.04644 	 0.00634 	 m..s
   76 	     7 	 0.05871 	 0.00636 	 m..s
   54 	     8 	 0.05311 	 0.00638 	 m..s
    6 	     9 	 0.04603 	 0.00640 	 m..s
   31 	    10 	 0.04919 	 0.00642 	 m..s
   80 	    11 	 0.06123 	 0.00642 	 m..s
   21 	    12 	 0.04846 	 0.00644 	 m..s
   39 	    13 	 0.04983 	 0.00645 	 m..s
   55 	    14 	 0.05401 	 0.00646 	 m..s
   51 	    15 	 0.05275 	 0.00646 	 m..s
   16 	    16 	 0.04804 	 0.00648 	 m..s
   93 	    17 	 0.07314 	 0.00651 	 m..s
   33 	    18 	 0.04931 	 0.00651 	 m..s
   38 	    19 	 0.04982 	 0.00652 	 m..s
   29 	    20 	 0.04909 	 0.00652 	 m..s
   47 	    21 	 0.05179 	 0.00652 	 m..s
   20 	    22 	 0.04837 	 0.00653 	 m..s
   61 	    23 	 0.05521 	 0.00653 	 m..s
   60 	    24 	 0.05499 	 0.00658 	 m..s
   15 	    25 	 0.04745 	 0.00660 	 m..s
   23 	    26 	 0.04877 	 0.00661 	 m..s
   57 	    27 	 0.05422 	 0.00664 	 m..s
   27 	    28 	 0.04903 	 0.00668 	 m..s
   42 	    29 	 0.05110 	 0.00668 	 m..s
   26 	    30 	 0.04898 	 0.00669 	 m..s
   62 	    31 	 0.05551 	 0.00671 	 m..s
    4 	    32 	 0.04550 	 0.00867 	 m..s
    1 	    33 	 0.04399 	 0.00878 	 m..s
    0 	    34 	 0.04376 	 0.00959 	 m..s
    3 	    35 	 0.04506 	 0.00967 	 m..s
   36 	    36 	 0.04941 	 0.00990 	 m..s
    8 	    37 	 0.04634 	 0.01142 	 m..s
    2 	    38 	 0.04491 	 0.01171 	 m..s
   32 	    39 	 0.04929 	 0.01313 	 m..s
   13 	    40 	 0.04700 	 0.01413 	 m..s
   59 	    41 	 0.05482 	 0.01452 	 m..s
   49 	    42 	 0.05255 	 0.01485 	 m..s
   45 	    43 	 0.05151 	 0.01537 	 m..s
   50 	    44 	 0.05257 	 0.01539 	 m..s
   11 	    45 	 0.04693 	 0.01884 	 ~...
   14 	    46 	 0.04732 	 0.01960 	 ~...
   12 	    47 	 0.04694 	 0.01965 	 ~...
    5 	    48 	 0.04595 	 0.02420 	 ~...
   19 	    49 	 0.04824 	 0.02736 	 ~...
   10 	    50 	 0.04658 	 0.02997 	 ~...
   30 	    51 	 0.04914 	 0.03177 	 ~...
   28 	    52 	 0.04906 	 0.03243 	 ~...
   73 	    53 	 0.05755 	 0.03365 	 ~...
   44 	    54 	 0.05144 	 0.03445 	 ~...
   37 	    55 	 0.04978 	 0.03458 	 ~...
   24 	    56 	 0.04877 	 0.03489 	 ~...
   48 	    57 	 0.05217 	 0.03581 	 ~...
   41 	    58 	 0.05056 	 0.04121 	 ~...
   65 	    59 	 0.05578 	 0.04160 	 ~...
   67 	    60 	 0.05640 	 0.04195 	 ~...
   68 	    61 	 0.05644 	 0.04290 	 ~...
  100 	    62 	 0.09744 	 0.05382 	 m..s
   86 	    63 	 0.06491 	 0.05617 	 ~...
   79 	    64 	 0.06060 	 0.05645 	 ~...
   58 	    65 	 0.05425 	 0.05743 	 ~...
   63 	    66 	 0.05574 	 0.05959 	 ~...
   74 	    67 	 0.05760 	 0.05962 	 ~...
   82 	    68 	 0.06273 	 0.06335 	 ~...
   46 	    69 	 0.05158 	 0.06341 	 ~...
   89 	    70 	 0.06862 	 0.06387 	 ~...
   87 	    71 	 0.06767 	 0.06665 	 ~...
   53 	    72 	 0.05311 	 0.06665 	 ~...
   70 	    73 	 0.05680 	 0.06688 	 ~...
   69 	    74 	 0.05660 	 0.06890 	 ~...
  102 	    75 	 0.09899 	 0.06968 	 ~...
  101 	    76 	 0.09866 	 0.06971 	 ~...
   78 	    77 	 0.06042 	 0.07013 	 ~...
   77 	    78 	 0.05983 	 0.07447 	 ~...
   90 	    79 	 0.06891 	 0.07908 	 ~...
  103 	    80 	 0.12041 	 0.08233 	 m..s
   88 	    81 	 0.06828 	 0.08468 	 ~...
   75 	    82 	 0.05856 	 0.08716 	 ~...
   97 	    83 	 0.08807 	 0.08724 	 ~...
   98 	    84 	 0.08980 	 0.08816 	 ~...
   85 	    85 	 0.06489 	 0.08823 	 ~...
   43 	    86 	 0.05128 	 0.09181 	 m..s
   81 	    87 	 0.06188 	 0.09247 	 m..s
   91 	    88 	 0.06949 	 0.09453 	 ~...
   34 	    89 	 0.04935 	 0.09526 	 m..s
   52 	    90 	 0.05283 	 0.09585 	 m..s
  107 	    91 	 0.16086 	 0.09631 	 m..s
   22 	    92 	 0.04854 	 0.09708 	 m..s
   25 	    93 	 0.04889 	 0.09713 	 m..s
   92 	    94 	 0.06988 	 0.09780 	 ~...
   64 	    95 	 0.05576 	 0.09787 	 m..s
   35 	    96 	 0.04939 	 0.09801 	 m..s
   72 	    97 	 0.05744 	 0.09844 	 m..s
   84 	    98 	 0.06466 	 0.10072 	 m..s
   71 	    99 	 0.05710 	 0.10089 	 m..s
   96 	   100 	 0.08712 	 0.10489 	 ~...
   83 	   101 	 0.06303 	 0.10669 	 m..s
   94 	   102 	 0.07626 	 0.11092 	 m..s
  104 	   103 	 0.12756 	 0.11730 	 ~...
   95 	   104 	 0.08575 	 0.11815 	 m..s
  105 	   105 	 0.14421 	 0.12940 	 ~...
   99 	   106 	 0.09096 	 0.14815 	 m..s
  109 	   107 	 0.18466 	 0.16659 	 ~...
  111 	   108 	 0.20679 	 0.20183 	 ~...
  106 	   109 	 0.15935 	 0.20470 	 m..s
  108 	   110 	 0.16300 	 0.21706 	 m..s
  110 	   111 	 0.19384 	 0.24480 	 m..s
  112 	   112 	 0.25075 	 0.26002 	 ~...
  118 	   113 	 0.29842 	 0.26458 	 m..s
  116 	   114 	 0.27588 	 0.27042 	 ~...
  117 	   115 	 0.28228 	 0.27546 	 ~...
  113 	   116 	 0.25594 	 0.27844 	 ~...
  120 	   117 	 0.30379 	 0.28272 	 ~...
  114 	   118 	 0.26769 	 0.28350 	 ~...
  119 	   119 	 0.30092 	 0.28719 	 ~...
  115 	   120 	 0.26960 	 0.28765 	 ~...
==========================================
r_mrr = 0.9120579957962036
r2_mrr = 0.7995644807815552
spearmanr_mrr@5 = 0.9176144003868103
spearmanr_mrr@10 = 0.9814721941947937
spearmanr_mrr@50 = 0.9888253808021545
spearmanr_mrr@100 = 0.9540565609931946
spearmanr_mrr@All = 0.9500423669815063
==========================================
test time: 0.399
Done Testing dataset OpenEA
total time taken: 223.2394347190857
training time taken: 207.5776720046997
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.9121)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7996)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9176)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9815)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9888)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9541)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9500)}}, 'test_loss': {'TransE': {'OpenEA': 0.5812861093581887}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9873698302411392
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [762, 66, 432, 1202, 294, 39, 567, 1147, 777, 602, 769, 449, 649, 1152, 1185, 834, 594, 27, 148, 907, 308, 831, 171, 63, 127, 787, 647, 324, 993, 4, 268, 606, 1105, 1138, 561, 192, 274, 348, 252, 626, 720, 733, 965, 368, 452, 1171, 303, 45, 689, 1089, 112, 859, 1076, 134, 732, 318, 215, 694, 759, 283, 974, 908, 233, 1026, 577, 972, 273, 1044, 755, 1072, 657, 56, 634, 570, 548, 288, 607, 1181, 955, 679, 636, 195, 936, 446, 132, 305, 67, 453, 601, 1167, 827, 784, 349, 255, 523, 469, 86, 953, 416, 441, 901, 60, 403, 1002, 1104, 117, 249, 395, 476, 855, 155, 772, 1004, 630, 1, 1123, 1033, 1112, 59, 181, 1018]
valid_ids (0): []
train_ids (1094): [625, 269, 442, 880, 43, 921, 1193, 848, 204, 42, 819, 235, 793, 226, 1042, 579, 278, 969, 1135, 718, 1094, 1117, 325, 773, 1150, 311, 151, 598, 892, 468, 1028, 767, 517, 736, 533, 1078, 208, 717, 525, 1166, 605, 652, 711, 1145, 376, 668, 241, 345, 1091, 481, 394, 77, 987, 33, 618, 358, 933, 35, 482, 573, 26, 196, 1178, 31, 483, 1197, 826, 412, 669, 456, 444, 666, 1027, 1151, 611, 945, 957, 123, 1070, 995, 970, 1127, 914, 433, 179, 32, 266, 1148, 1130, 912, 222, 828, 341, 174, 1132, 583, 739, 357, 644, 223, 317, 340, 497, 1172, 778, 667, 65, 14, 615, 923, 99, 422, 562, 795, 991, 1075, 1155, 845, 404, 136, 124, 844, 806, 49, 699, 355, 296, 108, 761, 331, 629, 725, 664, 735, 2, 1122, 113, 724, 1020, 856, 1097, 290, 310, 319, 366, 837, 240, 242, 792, 841, 334, 838, 417, 628, 927, 488, 994, 106, 674, 541, 1116, 584, 935, 810, 182, 1096, 952, 703, 938, 378, 558, 57, 847, 138, 980, 374, 530, 111, 941, 705, 860, 750, 684, 54, 686, 776, 540, 162, 840, 1156, 764, 413, 803, 375, 971, 351, 74, 1034, 1164, 708, 802, 238, 698, 816, 490, 922, 1095, 64, 291, 1184, 332, 339, 542, 899, 861, 986, 508, 1006, 1124, 323, 489, 169, 219, 1161, 61, 516, 999, 157, 865, 830, 898, 1199, 135, 559, 532, 267, 808, 16, 352, 823, 115, 161, 259, 1209, 1015, 966, 425, 960, 930, 146, 299, 87, 613, 217, 1175, 239, 362, 889, 595, 1046, 79, 638, 563, 298, 985, 729, 279, 757, 919, 48, 383, 102, 367, 377, 621, 475, 165, 1037, 514, 254, 902, 380, 356, 1092, 313, 459, 905, 890, 1189, 121, 515, 300, 1131, 1086, 427, 659, 926, 758, 1031, 774, 1049, 1213, 493, 506, 1174, 676, 518, 670, 1068, 663, 440, 692, 963, 5, 280, 580, 578, 1036, 1170, 186, 96, 0, 1214, 557, 555, 1201, 682, 1050, 639, 47, 504, 545, 193, 436, 1139, 906, 867, 854, 843, 164, 248, 1059, 141, 526, 1208, 916, 565, 159, 829, 1067, 384, 321, 982, 363, 133, 1200, 75, 978, 507, 833, 943, 603, 910, 1108, 501, 200, 813, 693, 329, 1168, 1111, 715, 1203, 680, 1207, 650, 846, 896, 731, 372, 807, 988, 1023, 875, 706, 852, 874, 502, 1190, 599, 891, 172, 307, 402, 665, 770, 637, 72, 284, 789, 373, 261, 658, 864, 796, 1063, 500, 198, 839, 301, 1003, 920, 983, 328, 979, 385, 275, 1143, 260, 958, 431, 918, 90, 868, 588, 513, 354, 166, 998, 547, 528, 1016, 939, 480, 178, 1005, 462, 954, 69, 44, 83, 289, 185, 466, 435, 597, 1182, 1137, 783, 152, 1014, 272, 1035, 293, 105, 225, 23, 145, 643, 815, 94, 744, 1051, 142, 306, 917, 879, 10, 928, 1073, 258, 499, 473, 287, 552, 95, 1045, 721, 968, 173, 478, 343, 853, 81, 753, 1160, 660, 471, 590, 387, 641, 1109, 58, 140, 976, 393, 286, 477, 544, 961, 369, 420, 398, 582, 713, 104, 900, 696, 536, 257, 336, 798, 766, 1113, 396, 1030, 391, 871, 418, 655, 126, 405, 175, 486, 116, 1153, 1146, 690, 1093, 946, 1157, 1038, 445, 569, 1187, 1158, 1173, 534, 738, 688, 414, 131, 964, 194, 1142, 388, 704, 232, 791, 183, 749, 821, 103, 661, 568, 295, 948, 487, 246, 866, 474, 264, 1128, 1085, 82, 782, 672, 959, 197, 617, 415, 677, 1065, 1024, 631, 1007, 519, 726, 277, 40, 429, 389, 593, 924, 231, 479, 817, 915, 1040, 206, 276, 234, 538, 765, 1165, 212, 465, 785, 330, 484, 1194, 529, 539, 1163, 737, 114, 454, 213, 80, 1191, 400, 646, 651, 270, 297, 1062, 691, 156, 88, 591, 797, 977, 709, 485, 997, 962, 1126, 1019, 632, 247, 1079, 36, 304, 990, 931, 316, 543, 781, 392, 230, 1022, 122, 671, 93, 158, 1060, 101, 849, 408, 956, 681, 15, 913, 419, 804, 904, 522, 271, 1056, 22, 884, 1114, 46, 428, 604, 575, 627, 382, 742, 244, 862, 84, 70, 434, 13, 1082, 521, 592, 572, 447, 458, 775, 472, 1069, 424, 160, 62, 909, 614, 397, 722, 771, 747, 1180, 335, 1084, 421, 633, 1008, 818, 455, 29, 1144, 1159, 1088, 292, 741, 873, 869, 12, 184, 423, 89, 535, 37, 1102, 799, 24, 1000, 7, 1043, 149, 346, 399, 546, 1101, 850, 877, 746, 675, 3, 654, 409, 685, 600, 911, 364, 209, 20, 189, 809, 788, 1196, 386, 17, 925, 167, 401, 430, 190, 531, 551, 832, 886, 100, 730, 712, 78, 576, 1048, 125, 1115, 251, 110, 327, 130, 338, 587, 586, 322, 511, 616, 623, 800, 333, 1134, 752, 610, 1012, 188, 1154, 147, 950, 550, 1136, 932, 505, 492, 1183, 723, 951, 224, 1186, 894, 949, 858, 256, 463, 967, 1118, 1074, 822, 1047, 370, 885, 1011, 585, 1087, 1061, 872, 228, 851, 1133, 883, 38, 697, 406, 411, 437, 6, 221, 1032, 461, 53, 1107, 262, 220, 320, 187, 748, 439, 740, 495, 150, 745, 76, 878, 214, 168, 1103, 285, 897, 144, 347, 253, 1212, 407, 210, 1129, 1140, 1125, 379, 734, 199, 768, 836, 727, 236, 520, 973, 780, 707, 128, 1195, 25, 1029, 763, 619, 1058, 716, 553, 814, 450, 1204, 989, 1081, 229, 350, 390, 751, 887, 940, 937, 805, 1211, 443, 683, 1055, 801, 812, 91, 218, 1179, 381, 41, 835, 202, 154, 1188, 656, 498, 464, 635, 620, 825, 888, 1098, 1071, 560, 1077, 460, 581, 648, 622, 1110, 714, 981, 109, 984, 281, 794, 728, 85, 119, 245, 73, 139, 68, 207, 779, 50, 934, 28, 1013, 496, 566, 608, 1010, 700, 1177, 1099, 881, 9, 237, 8, 1169, 1162, 754, 1066, 687, 929, 360, 554, 903, 1192, 263, 662, 1053, 1149, 1119, 512, 882, 1100, 312, 143, 177, 1206, 589, 1198, 120, 1039, 1121, 564, 55, 491, 1041, 30, 710, 97, 760, 992, 1009, 18, 302, 205, 701, 211, 503, 640, 361, 107, 702, 1001, 365, 21, 153, 1064, 52, 337, 34, 1141, 426, 243, 1080, 265, 876, 216, 893, 201, 975, 1210, 92, 1205, 203, 359, 137, 510, 842, 645, 863, 537, 371, 11, 51, 457, 944, 191, 653, 410, 719, 857, 180, 571, 820, 19, 524, 1120, 695, 326, 1083, 451, 824, 470, 170, 942, 811, 227, 309, 448, 947, 353, 596, 1021, 494, 163, 624, 642, 1017, 612, 609, 118, 556, 574, 527, 1054, 438, 743, 314, 71, 250, 870, 895, 1090, 467, 786, 98, 315, 1025, 1106, 1176, 678, 1052, 509, 756, 344, 673, 1057, 176, 790, 282, 129, 342, 549, 996]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2703556478373032
the save name prefix for this run is:  chkpt-ID_2703556478373032_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.485 +- 0.006
mrr vals (pred, true): 0.000, 0.106
batch losses (mrrl, rdl): 0.0, 0.0022545215

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 813
rank avg (pred): 0.047 +- 0.027
mrr vals (pred, true): 0.133, 0.070
batch losses (mrrl, rdl): 0.0, 5.197e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 969
rank avg (pred): 0.347 +- 0.227
mrr vals (pred, true): 0.198, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003128583

Epoch over!
epoch time: 12.584

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 966
rank avg (pred): 0.353 +- 0.231
mrr vals (pred, true): 0.207, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002415799

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 880
rank avg (pred): 0.343 +- 0.256
mrr vals (pred, true): 0.275, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002648798

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1093
rank avg (pred): 0.282 +- 0.229
mrr vals (pred, true): 0.311, 0.110
batch losses (mrrl, rdl): 0.0, 0.0003465606

Epoch over!
epoch time: 12.344

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 361
rank avg (pred): 0.360 +- 0.274
mrr vals (pred, true): 0.276, 0.032
batch losses (mrrl, rdl): 0.0, 0.0005426638

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1196
rank avg (pred): 0.352 +- 0.284
mrr vals (pred, true): 0.315, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002428408

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 168
rank avg (pred): 0.331 +- 0.292
mrr vals (pred, true): 0.343, 0.006
batch losses (mrrl, rdl): 0.0, 0.000275372

Epoch over!
epoch time: 12.271

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 337
rank avg (pred): 0.292 +- 0.284
mrr vals (pred, true): 0.361, 0.033
batch losses (mrrl, rdl): 0.0, 0.000191805

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 440
rank avg (pred): 0.317 +- 0.296
mrr vals (pred, true): 0.341, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003101769

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 976
rank avg (pred): 0.052 +- 0.053
mrr vals (pred, true): 0.390, 0.277
batch losses (mrrl, rdl): 0.0, 8.0009e-06

Epoch over!
epoch time: 12.295

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 928
rank avg (pred): 0.359 +- 0.301
mrr vals (pred, true): 0.327, 0.095
batch losses (mrrl, rdl): 0.0, 0.0003064414

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1016
rank avg (pred): 0.252 +- 0.252
mrr vals (pred, true): 0.361, 0.099
batch losses (mrrl, rdl): 0.0, 0.0001661775

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 359
rank avg (pred): 0.328 +- 0.305
mrr vals (pred, true): 0.347, 0.054
batch losses (mrrl, rdl): 0.0, 0.0004723497

Epoch over!
epoch time: 12.394

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.369 +- 0.302
mrr vals (pred, true): 0.309, 0.037
batch losses (mrrl, rdl): 0.6727442145, 0.0001075837

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 715
rank avg (pred): 0.593 +- 0.179
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 0.0001005827, 0.0002809087

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1145
rank avg (pred): 0.055 +- 0.032
mrr vals (pred, true): 0.115, 0.103
batch losses (mrrl, rdl): 0.0014130836, 0.0003163753

Epoch over!
epoch time: 13.783

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1024
rank avg (pred): 0.513 +- 0.236
mrr vals (pred, true): 0.072, 0.106
batch losses (mrrl, rdl): 0.011477042, 0.0024024711

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 160
rank avg (pred): 0.545 +- 0.195
mrr vals (pred, true): 0.056, 0.022
batch losses (mrrl, rdl): 0.0003254663, 0.00209273

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.164, 0.213
batch losses (mrrl, rdl): 0.0240136031, 4.64659e-05

Epoch over!
epoch time: 13.622

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 129
rank avg (pred): 0.524 +- 0.191
mrr vals (pred, true): 0.057, 0.015
batch losses (mrrl, rdl): 0.0004400013, 0.0015763731

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 68
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.222, 0.193
batch losses (mrrl, rdl): 0.0087442622, 7.46176e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1086
rank avg (pred): 0.431 +- 0.218
mrr vals (pred, true): 0.063, 0.122
batch losses (mrrl, rdl): 0.0340168476, 0.001595647

Epoch over!
epoch time: 12.644

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 286
rank avg (pred): 0.023 +- 0.016
mrr vals (pred, true): 0.137, 0.137
batch losses (mrrl, rdl): 5.7924e-06, 7.83853e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 672
rank avg (pred): 0.492 +- 0.157
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 5.683e-07, 6.0223e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 662
rank avg (pred): 0.416 +- 0.122
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001648566, 0.0001739239

Epoch over!
epoch time: 13.272

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 444
rank avg (pred): 0.443 +- 0.145
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 5.11065e-05, 8.99131e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.242, 0.213
batch losses (mrrl, rdl): 0.0088598775, 6.24454e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1010
rank avg (pred): 0.407 +- 0.160
mrr vals (pred, true): 0.053, 0.057
batch losses (mrrl, rdl): 6.37355e-05, 0.0009714619

Epoch over!
epoch time: 12.994

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 745
rank avg (pred): 0.043 +- 0.031
mrr vals (pred, true): 0.126, 0.216
batch losses (mrrl, rdl): 0.0817482919, 8.9886e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.360 +- 0.149
mrr vals (pred, true): 0.056, 0.104
batch losses (mrrl, rdl): 0.0232558623, 0.0008439653

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 997
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.277, 0.288
batch losses (mrrl, rdl): 0.0012759164, 2.65844e-05

Epoch over!
epoch time: 13.885

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1119
rank avg (pred): 0.360 +- 0.164
mrr vals (pred, true): 0.062, 0.006
batch losses (mrrl, rdl): 0.0014931617, 0.0003206971

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1096
rank avg (pred): 0.303 +- 0.199
mrr vals (pred, true): 0.089, 0.121
batch losses (mrrl, rdl): 0.0106388628, 0.000504639

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1114
rank avg (pred): 0.284 +- 0.190
mrr vals (pred, true): 0.098, 0.006
batch losses (mrrl, rdl): 0.0232311301, 0.0007322545

Epoch over!
epoch time: 13.89

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1209
rank avg (pred): 0.401 +- 0.140
mrr vals (pred, true): 0.054, 0.007
batch losses (mrrl, rdl): 0.0001585265, 0.0001971411

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 191
rank avg (pred): 0.345 +- 0.098
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 4.6622e-06, 0.0004858516

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 223
rank avg (pred): 0.286 +- 0.075
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 7.21e-08, 0.0009858666

Epoch over!
epoch time: 13.285

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 203
rank avg (pred): 0.343 +- 0.095
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 2.6e-09, 0.0005502438

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 183
rank avg (pred): 0.356 +- 0.114
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 0.0001151823, 0.0004553937

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 797
rank avg (pred): 0.345 +- 0.091
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 1.792e-05, 0.0005292384

Epoch over!
epoch time: 13.364

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 371
rank avg (pred): 0.370 +- 0.139
mrr vals (pred, true): 0.056, 0.067
batch losses (mrrl, rdl): 0.0003696674, 0.0008175758

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 250
rank avg (pred): 0.290 +- 0.179
mrr vals (pred, true): 0.086, 0.111
batch losses (mrrl, rdl): 0.0061434889, 0.0009537706

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.379 +- 0.109
mrr vals (pred, true): 0.051, 0.036
batch losses (mrrl, rdl): 1.25582e-05, 0.0001610446

Epoch over!
epoch time: 13.173

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.373 +- 0.114
mrr vals (pred, true): 0.052, 0.084

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   54 	     0 	 0.05090 	 0.00572 	 m..s
   47 	     1 	 0.04964 	 0.00595 	 m..s
   57 	     2 	 0.05132 	 0.00602 	 m..s
   53 	     3 	 0.05088 	 0.00616 	 m..s
    5 	     4 	 0.04219 	 0.00618 	 m..s
   22 	     5 	 0.04452 	 0.00629 	 m..s
   32 	     6 	 0.04674 	 0.00637 	 m..s
   17 	     7 	 0.04387 	 0.00637 	 m..s
   12 	     8 	 0.04316 	 0.00637 	 m..s
   25 	     9 	 0.04537 	 0.00639 	 m..s
   35 	    10 	 0.04720 	 0.00640 	 m..s
   23 	    11 	 0.04466 	 0.00643 	 m..s
   28 	    12 	 0.04602 	 0.00644 	 m..s
   68 	    13 	 0.05386 	 0.00647 	 m..s
   70 	    14 	 0.05484 	 0.00647 	 m..s
   55 	    15 	 0.05100 	 0.00648 	 m..s
   10 	    16 	 0.04299 	 0.00648 	 m..s
   18 	    17 	 0.04388 	 0.00649 	 m..s
    0 	    18 	 0.04129 	 0.00650 	 m..s
    3 	    19 	 0.04199 	 0.00651 	 m..s
   71 	    20 	 0.05526 	 0.00652 	 m..s
   44 	    21 	 0.04878 	 0.00652 	 m..s
   30 	    22 	 0.04655 	 0.00654 	 m..s
   64 	    23 	 0.05255 	 0.00656 	 m..s
   83 	    24 	 0.06561 	 0.00660 	 m..s
   39 	    25 	 0.04823 	 0.00660 	 m..s
   84 	    26 	 0.06574 	 0.00667 	 m..s
   43 	    27 	 0.04874 	 0.00669 	 m..s
   56 	    28 	 0.05122 	 0.00670 	 m..s
   46 	    29 	 0.04938 	 0.00670 	 m..s
   26 	    30 	 0.04551 	 0.00670 	 m..s
   40 	    31 	 0.04824 	 0.00671 	 m..s
    2 	    32 	 0.04189 	 0.00819 	 m..s
    8 	    33 	 0.04276 	 0.00867 	 m..s
    6 	    34 	 0.04246 	 0.00882 	 m..s
   13 	    35 	 0.04321 	 0.00950 	 m..s
    4 	    36 	 0.04201 	 0.00959 	 m..s
    9 	    37 	 0.04292 	 0.01016 	 m..s
    1 	    38 	 0.04130 	 0.01106 	 m..s
   19 	    39 	 0.04402 	 0.01328 	 m..s
   15 	    40 	 0.04351 	 0.01376 	 ~...
   16 	    41 	 0.04352 	 0.01398 	 ~...
   24 	    42 	 0.04467 	 0.01416 	 m..s
   45 	    43 	 0.04883 	 0.01523 	 m..s
   31 	    44 	 0.04657 	 0.01684 	 ~...
   11 	    45 	 0.04301 	 0.01709 	 ~...
   20 	    46 	 0.04431 	 0.02024 	 ~...
   48 	    47 	 0.05021 	 0.02136 	 ~...
   58 	    48 	 0.05145 	 0.02198 	 ~...
    7 	    49 	 0.04252 	 0.02570 	 ~...
   14 	    50 	 0.04341 	 0.02637 	 ~...
   50 	    51 	 0.05079 	 0.03356 	 ~...
   38 	    52 	 0.04783 	 0.03397 	 ~...
   49 	    53 	 0.05047 	 0.03444 	 ~...
   62 	    54 	 0.05195 	 0.03526 	 ~...
   52 	    55 	 0.05086 	 0.03700 	 ~...
   36 	    56 	 0.04746 	 0.03738 	 ~...
   66 	    57 	 0.05289 	 0.04195 	 ~...
   59 	    58 	 0.05151 	 0.04417 	 ~...
  105 	    59 	 0.11506 	 0.05035 	 m..s
   73 	    60 	 0.05539 	 0.05557 	 ~...
   77 	    61 	 0.06208 	 0.05695 	 ~...
   63 	    62 	 0.05239 	 0.05743 	 ~...
   79 	    63 	 0.06321 	 0.06068 	 ~...
   82 	    64 	 0.06555 	 0.06387 	 ~...
   42 	    65 	 0.04846 	 0.06488 	 ~...
   80 	    66 	 0.06381 	 0.06497 	 ~...
   41 	    67 	 0.04845 	 0.06551 	 ~...
   81 	    68 	 0.06398 	 0.06558 	 ~...
   65 	    69 	 0.05260 	 0.06559 	 ~...
   60 	    70 	 0.05172 	 0.06589 	 ~...
   96 	    71 	 0.08545 	 0.06889 	 ~...
   92 	    72 	 0.07354 	 0.07219 	 ~...
   99 	    73 	 0.08667 	 0.07267 	 ~...
   69 	    74 	 0.05412 	 0.07477 	 ~...
   74 	    75 	 0.05707 	 0.07764 	 ~...
   86 	    76 	 0.06765 	 0.07908 	 ~...
  104 	    77 	 0.10994 	 0.08233 	 ~...
   67 	    78 	 0.05379 	 0.08277 	 ~...
   61 	    79 	 0.05191 	 0.08389 	 m..s
   72 	    80 	 0.05537 	 0.08615 	 m..s
   78 	    81 	 0.06252 	 0.09162 	 ~...
   34 	    82 	 0.04718 	 0.09191 	 m..s
   51 	    83 	 0.05083 	 0.09456 	 m..s
   29 	    84 	 0.04630 	 0.09500 	 m..s
   27 	    85 	 0.04596 	 0.09679 	 m..s
   37 	    86 	 0.04769 	 0.09695 	 m..s
   33 	    87 	 0.04694 	 0.09708 	 m..s
  106 	    88 	 0.11684 	 0.09751 	 ~...
   21 	    89 	 0.04438 	 0.09790 	 m..s
   91 	    90 	 0.07144 	 0.09837 	 ~...
   88 	    91 	 0.07077 	 0.10189 	 m..s
   97 	    92 	 0.08576 	 0.10479 	 ~...
  103 	    93 	 0.10368 	 0.10534 	 ~...
  101 	    94 	 0.09073 	 0.10670 	 ~...
  108 	    95 	 0.12075 	 0.10675 	 ~...
  102 	    96 	 0.09623 	 0.10775 	 ~...
   98 	    97 	 0.08635 	 0.10804 	 ~...
   94 	    98 	 0.07822 	 0.10948 	 m..s
   90 	    99 	 0.07085 	 0.11092 	 m..s
   75 	   100 	 0.06118 	 0.11133 	 m..s
   76 	   101 	 0.06179 	 0.11353 	 m..s
   85 	   102 	 0.06650 	 0.11916 	 m..s
   93 	   103 	 0.07730 	 0.11998 	 m..s
   87 	   104 	 0.06813 	 0.12089 	 m..s
  113 	   105 	 0.13420 	 0.12687 	 ~...
  112 	   106 	 0.13309 	 0.12940 	 ~...
  100 	   107 	 0.08872 	 0.12970 	 m..s
   95 	   108 	 0.08342 	 0.13066 	 m..s
   89 	   109 	 0.07080 	 0.14016 	 m..s
  115 	   110 	 0.15437 	 0.14759 	 ~...
  107 	   111 	 0.11934 	 0.16719 	 m..s
  114 	   112 	 0.14121 	 0.19077 	 m..s
  110 	   113 	 0.12945 	 0.20766 	 m..s
  109 	   114 	 0.12893 	 0.20882 	 m..s
  111 	   115 	 0.12988 	 0.21399 	 m..s
  116 	   116 	 0.20349 	 0.21641 	 ~...
  120 	   117 	 0.27294 	 0.24897 	 ~...
  118 	   118 	 0.25245 	 0.25509 	 ~...
  119 	   119 	 0.27137 	 0.26716 	 ~...
  117 	   120 	 0.23401 	 0.27825 	 m..s
==========================================
r_mrr = 0.8440805673599243
r2_mrr = 0.683883547782898
spearmanr_mrr@5 = 0.9777287244796753
spearmanr_mrr@10 = 0.9403822422027588
spearmanr_mrr@50 = 0.9649345874786377
spearmanr_mrr@100 = 0.9167864918708801
spearmanr_mrr@All = 0.9167397022247314
==========================================
test time: 0.415
Done Testing dataset OpenEA
total time taken: 211.30075573921204
training time taken: 196.3005678653717
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8441)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.6839)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9777)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9404)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9649)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9168)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9167)}}, 'test_loss': {'TransE': {'OpenEA': 0.7839977863841341}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 5172973932778372
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [280, 600, 907, 1062, 374, 523, 805, 1003, 215, 631, 1068, 475, 284, 558, 1085, 1191, 291, 560, 294, 203, 403, 62, 76, 603, 462, 486, 1148, 645, 234, 37, 1060, 923, 617, 163, 1099, 289, 38, 458, 251, 910, 788, 457, 288, 908, 1199, 1035, 859, 1192, 122, 650, 538, 705, 383, 134, 1049, 712, 120, 355, 90, 390, 927, 463, 361, 834, 742, 1040, 47, 53, 273, 671, 468, 1103, 111, 184, 948, 644, 585, 756, 953, 1050, 114, 776, 310, 1088, 1177, 881, 502, 321, 186, 206, 816, 672, 1135, 438, 888, 778, 734, 1193, 180, 1190, 919, 144, 620, 815, 869, 980, 865, 745, 945, 192, 65, 181, 137, 611, 784, 484, 398, 382, 845, 1044, 257]
valid_ids (0): []
train_ids (1094): [2, 417, 360, 349, 126, 938, 877, 450, 892, 681, 991, 1189, 1027, 951, 491, 591, 536, 1009, 26, 1057, 441, 36, 154, 115, 365, 1207, 193, 1111, 467, 613, 281, 952, 790, 1146, 689, 833, 148, 185, 1071, 297, 685, 641, 5, 88, 362, 937, 875, 1136, 950, 550, 470, 909, 31, 678, 225, 194, 1063, 990, 775, 731, 563, 728, 303, 500, 75, 473, 1, 421, 526, 573, 1138, 219, 87, 1007, 406, 849, 619, 1084, 1201, 802, 905, 898, 764, 1184, 338, 1140, 1129, 757, 525, 427, 168, 737, 1203, 1104, 456, 850, 460, 183, 616, 551, 187, 800, 597, 535, 814, 165, 241, 1178, 556, 70, 1149, 527, 188, 436, 1034, 694, 922, 335, 483, 1016, 174, 581, 981, 547, 231, 341, 565, 658, 205, 106, 683, 69, 768, 202, 508, 182, 548, 592, 896, 1131, 394, 1139, 837, 965, 711, 846, 723, 443, 367, 248, 430, 609, 626, 214, 41, 707, 1180, 395, 1123, 74, 855, 916, 399, 579, 259, 854, 763, 102, 578, 818, 260, 242, 724, 197, 544, 1118, 304, 844, 140, 46, 770, 1043, 608, 381, 930, 522, 83, 1015, 531, 18, 246, 546, 141, 782, 411, 868, 656, 1058, 640, 939, 435, 787, 60, 512, 623, 123, 847, 666, 434, 894, 1018, 873, 354, 1041, 906, 409, 166, 667, 632, 332, 1098, 1141, 655, 539, 179, 1032, 12, 794, 20, 1055, 925, 1090, 1107, 35, 679, 433, 1045, 84, 369, 429, 715, 700, 577, 221, 1144, 595, 903, 553, 1028, 1075, 1095, 736, 149, 228, 884, 1026, 1174, 29, 109, 52, 1185, 765, 1114, 561, 265, 358, 960, 690, 67, 413, 566, 505, 261, 676, 904, 199, 914, 841, 63, 575, 1073, 493, 157, 1061, 697, 1213, 777, 612, 159, 405, 956, 746, 375, 407, 286, 465, 112, 971, 774, 453, 449, 201, 751, 240, 913, 668, 268, 1117, 39, 59, 469, 510, 1022, 388, 1024, 810, 572, 296, 58, 836, 256, 1170, 900, 132, 207, 300, 1211, 1013, 196, 840, 696, 891, 1179, 835, 1054, 624, 598, 860, 738, 116, 32, 567, 1160, 743, 889, 718, 902, 801, 110, 385, 744, 516, 978, 879, 255, 218, 210, 147, 629, 773, 485, 10, 1030, 55, 410, 637, 1079, 236, 1167, 518, 1186, 599, 931, 97, 571, 628, 758, 125, 684, 1051, 941, 1183, 819, 830, 601, 85, 22, 540, 232, 425, 714, 1065, 857, 285, 1070, 498, 191, 542, 649, 162, 301, 95, 279, 799, 682, 1064, 771, 530, 478, 1134, 721, 1006, 515, 1048, 986, 1208, 1153, 893, 827, 298, 1151, 404, 424, 309, 252, 1000, 378, 1108, 1169, 719, 570, 1212, 447, 665, 804, 969, 178, 1082, 659, 959, 471, 245, 444, 25, 973, 829, 876, 1156, 1127, 208, 933, 489, 277, 583, 1092, 1168, 135, 755, 867, 161, 344, 293, 1200, 464, 45, 6, 532, 964, 331, 861, 1089, 313, 432, 1158, 1161, 11, 839, 588, 662, 769, 1157, 552, 543, 669, 920, 652, 209, 809, 48, 164, 9, 618, 722, 271, 440, 370, 574, 108, 828, 1078, 50, 51, 785, 308, 926, 885, 1069, 1175, 0, 1125, 371, 138, 704, 1142, 17, 795, 1019, 488, 13, 477, 807, 630, 852, 145, 340, 693, 1188, 160, 99, 1100, 673, 227, 129, 677, 509, 451, 576, 8, 1052, 121, 985, 146, 1025, 363, 1021, 94, 1074, 554, 647, 1101, 741, 942, 366, 979, 1038, 480, 235, 562, 1042, 482, 377, 408, 976, 119, 912, 921, 584, 393, 476, 1166, 798, 643, 1204, 1072, 1012, 1094, 494, 306, 564, 1087, 994, 1110, 992, 492, 44, 886, 322, 198, 1206, 459, 1143, 687, 1116, 1173, 93, 557, 549, 337, 727, 226, 173, 663, 496, 754, 786, 287, 177, 1010, 783, 901, 710, 230, 915, 761, 513, 247, 519, 79, 89, 262, 1029, 534, 80, 16, 72, 352, 838, 68, 706, 1122, 968, 998, 254, 302, 307, 28, 1155, 243, 1059, 77, 3, 153, 314, 709, 717, 797, 503, 319, 91, 1076, 1133, 1172, 212, 428, 1004, 1036, 386, 461, 590, 947, 733, 1046, 105, 858, 688, 823, 989, 529, 944, 448, 131, 1093, 781, 64, 336, 158, 330, 1198, 521, 455, 878, 270, 275, 848, 379, 325, 883, 292, 472, 528, 987, 1124, 670, 189, 347, 1120, 524, 1039, 753, 537, 767, 824, 957, 222, 1112, 169, 98, 278, 653, 664, 803, 946, 81, 373, 1163, 954, 283, 339, 351, 156, 200, 621, 789, 686, 1097, 416, 233, 596, 634, 353, 880, 862, 555, 133, 752, 870, 1162, 499, 1105, 1154, 474, 238, 1209, 295, 582, 654, 1113, 993, 1202, 982, 760, 949, 372, 446, 176, 396, 580, 559, 78, 1119, 541, 895, 675, 387, 863, 842, 350, 167, 40, 211, 749, 935, 272, 820, 1023, 7, 899, 1210, 934, 1031, 983, 258, 504, 56, 1182, 871, 817, 691, 725, 639, 812, 917, 1159, 1106, 821, 329, 402, 720, 481, 420, 155, 517, 412, 882, 1067, 791, 282, 928, 61, 1152, 1066, 1047, 1147, 324, 96, 674, 1008, 419, 1091, 1145, 346, 1002, 276, 713, 497, 651, 740, 729, 702, 320, 635, 19, 418, 328, 606, 150, 701, 312, 266, 204, 376, 107, 415, 290, 772, 364, 988, 34, 82, 750, 171, 15, 1017, 334, 636, 124, 1033, 831, 962, 811, 943, 454, 1121, 23, 1187, 972, 311, 414, 1083, 918, 1037, 431, 359, 1171, 43, 479, 152, 445, 229, 130, 391, 586, 924, 118, 1150, 269, 1056, 698, 426, 217, 250, 139, 14, 661, 514, 759, 607, 642, 30, 1011, 342, 822, 911, 762, 997, 615, 104, 42, 299, 963, 223, 897, 368, 316, 101, 955, 1165, 1176, 466, 54, 4, 73, 851, 128, 237, 1130, 507, 511, 806, 1005, 175, 216, 890, 439, 345, 961, 808, 589, 323, 646, 735, 136, 190, 977, 506, 348, 24, 1194, 66, 452, 545, 100, 648, 249, 703, 380, 151, 327, 1086, 1096, 936, 1102, 813, 796, 1137, 195, 716, 929, 357, 940, 932, 856, 501, 57, 27, 708, 975, 966, 220, 423, 779, 1195, 730, 103, 692, 326, 263, 224, 1128, 587, 333, 793, 1132, 1001, 33, 638, 490, 853, 984, 117, 887, 400, 397, 699, 315, 726, 1109, 1181, 1164, 1080, 610, 843, 318, 604, 732, 633, 660, 627, 71, 487, 780, 384, 832, 422, 974, 1053, 739, 970, 622, 995, 239, 593, 864, 21, 695, 569, 1197, 792, 680, 356, 49, 172, 533, 264, 274, 495, 343, 392, 866, 1077, 967, 568, 520, 996, 1214, 1081, 213, 958, 127, 170, 602, 142, 1014, 267, 401, 143, 748, 747, 253, 874, 625, 1115, 614, 92, 389, 766, 113, 86, 594, 825, 437, 305, 1205, 244, 872, 605, 826, 1196, 657, 1126, 999, 317, 442, 1020]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9657136412197398
the save name prefix for this run is:  chkpt-ID_9657136412197398_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1024
rank avg (pred): 0.526 +- 0.002
mrr vals (pred, true): 0.000, 0.106
batch losses (mrrl, rdl): 0.0, 0.0025433528

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 871
rank avg (pred): 0.349 +- 0.238
mrr vals (pred, true): 0.013, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002652054

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.372 +- 0.304
mrr vals (pred, true): 0.186, 0.036
batch losses (mrrl, rdl): 0.0, 0.0001034764

Epoch over!
epoch time: 12.128

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 655
rank avg (pred): 0.336 +- 0.299
mrr vals (pred, true): 0.256, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002657431

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 566
rank avg (pred): 0.148 +- 0.128
mrr vals (pred, true): 0.279, 0.095
batch losses (mrrl, rdl): 0.0, 6.8893e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 867
rank avg (pred): 0.336 +- 0.300
mrr vals (pred, true): 0.300, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002491958

Epoch over!
epoch time: 11.872

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 992
rank avg (pred): 0.063 +- 0.055
mrr vals (pred, true): 0.295, 0.218
batch losses (mrrl, rdl): 0.0, 1.26567e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 663
rank avg (pred): 0.352 +- 0.321
mrr vals (pred, true): 0.314, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001977809

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 848
rank avg (pred): 0.337 +- 0.300
mrr vals (pred, true): 0.305, 0.097
batch losses (mrrl, rdl): 0.0, 0.0001814753

Epoch over!
epoch time: 11.937

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1137
rank avg (pred): 0.130 +- 0.121
mrr vals (pred, true): 0.345, 0.108
batch losses (mrrl, rdl): 0.0, 5.52875e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.354 +- 0.321
mrr vals (pred, true): 0.324, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001825253

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 892
rank avg (pred): 0.096 +- 0.090
mrr vals (pred, true): 0.368, 0.058
batch losses (mrrl, rdl): 0.0, 9.4298e-06

Epoch over!
epoch time: 11.985

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 688
rank avg (pred): 0.378 +- 0.331
mrr vals (pred, true): 0.299, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001345781

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 211
rank avg (pred): 0.306 +- 0.281
mrr vals (pred, true): 0.352, 0.007
batch losses (mrrl, rdl): 0.0, 0.0004181149

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 703
rank avg (pred): 0.345 +- 0.326
mrr vals (pred, true): 0.357, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002307701

Epoch over!
epoch time: 11.973

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 394
rank avg (pred): 0.349 +- 0.302
mrr vals (pred, true): 0.315, 0.035
batch losses (mrrl, rdl): 0.7019819021, 0.0005363514

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 29
rank avg (pred): 0.066 +- 0.039
mrr vals (pred, true): 0.083, 0.122
batch losses (mrrl, rdl): 0.015157612, 7.9088e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 145
rank avg (pred): 0.450 +- 0.201
mrr vals (pred, true): 0.048, 0.020
batch losses (mrrl, rdl): 3.97789e-05, 0.000923352

Epoch over!
epoch time: 12.282

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 424
rank avg (pred): 0.272 +- 0.144
mrr vals (pred, true): 0.061, 0.006
batch losses (mrrl, rdl): 0.0012153815, 0.0009418612

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1059
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.278, 0.295
batch losses (mrrl, rdl): 0.0028774051, 1.92924e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 762
rank avg (pred): 0.287 +- 0.165
mrr vals (pred, true): 0.064, 0.084
batch losses (mrrl, rdl): 0.0019068024, 0.0001090671

Epoch over!
epoch time: 12.032

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 44
rank avg (pred): 0.009 +- 0.037
mrr vals (pred, true): 0.173, 0.191
batch losses (mrrl, rdl): 0.003226632, 4.58586e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 576
rank avg (pred): 0.462 +- 0.229
mrr vals (pred, true): 0.056, 0.010
batch losses (mrrl, rdl): 0.0003092565, 0.0002415297

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 580
rank avg (pred): 0.467 +- 0.225
mrr vals (pred, true): 0.050, 0.014
batch losses (mrrl, rdl): 8.928e-07, 0.0003201776

Epoch over!
epoch time: 12.244

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 969
rank avg (pred): 0.464 +- 0.246
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 1.82227e-05, 4.41348e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 300
rank avg (pred): 0.118 +- 0.127
mrr vals (pred, true): 0.074, 0.057
batch losses (mrrl, rdl): 0.0055812183, 1.31994e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 450
rank avg (pred): 0.425 +- 0.254
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 4.14344e-05, 0.0001134507

Epoch over!
epoch time: 12.092

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 974
rank avg (pred): 0.004 +- 0.038
mrr vals (pred, true): 0.241, 0.255
batch losses (mrrl, rdl): 0.0018932613, 2.76895e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 592
rank avg (pred): 0.443 +- 0.272
mrr vals (pred, true): 0.050, 0.016
batch losses (mrrl, rdl): 6.321e-07, 0.0001526108

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 164
rank avg (pred): 0.416 +- 0.274
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001287777, 0.0001524905

Epoch over!
epoch time: 12.116

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1179
rank avg (pred): 0.365 +- 0.273
mrr vals (pred, true): 0.060, 0.036
batch losses (mrrl, rdl): 0.0010783281, 2.69043e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 689
rank avg (pred): 0.486 +- 0.283
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.0002257059, 1.08751e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1076
rank avg (pred): 0.005 +- 0.023
mrr vals (pred, true): 0.233, 0.278
batch losses (mrrl, rdl): 0.0203204565, 2.67704e-05

Epoch over!
epoch time: 12.155

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 443
rank avg (pred): 0.348 +- 0.300
mrr vals (pred, true): 0.053, 0.007
batch losses (mrrl, rdl): 0.0001179526, 0.0005042707

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 4
rank avg (pred): 0.074 +- 0.171
mrr vals (pred, true): 0.090, 0.105
batch losses (mrrl, rdl): 0.0022780469, 3.8359e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1058
rank avg (pred): 0.002 +- 0.023
mrr vals (pred, true): 0.293, 0.278
batch losses (mrrl, rdl): 0.0020742109, 2.59902e-05

Epoch over!
epoch time: 11.998

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 388
rank avg (pred): 0.394 +- 0.310
mrr vals (pred, true): 0.052, 0.032
batch losses (mrrl, rdl): 5.84208e-05, 0.000436459

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 319
rank avg (pred): 0.016 +- 0.049
mrr vals (pred, true): 0.197, 0.091
batch losses (mrrl, rdl): 0.2149131, 8.87824e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 45
rank avg (pred): 0.054 +- 0.106
mrr vals (pred, true): 0.103, 0.167
batch losses (mrrl, rdl): 0.0407997444, 8.4529e-06

Epoch over!
epoch time: 11.967

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 559
rank avg (pred): 0.253 +- 0.311
mrr vals (pred, true): 0.055, 0.075
batch losses (mrrl, rdl): 0.0002225831, 3.52259e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 868
rank avg (pred): 0.331 +- 0.352
mrr vals (pred, true): 0.050, 0.007
batch losses (mrrl, rdl): 2.0527e-06, 0.0005770202

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1128
rank avg (pred): 0.296 +- 0.257
mrr vals (pred, true): 0.061, 0.006
batch losses (mrrl, rdl): 0.0012021115, 0.000783749

Epoch over!
epoch time: 12.213

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 779
rank avg (pred): 0.342 +- 0.274
mrr vals (pred, true): 0.058, 0.097
batch losses (mrrl, rdl): 0.0006755759, 8.3111e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 965
rank avg (pred): 0.445 +- 0.333
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 4.35057e-05, 9.43159e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 59
rank avg (pred): 0.030 +- 0.105
mrr vals (pred, true): 0.144, 0.129
batch losses (mrrl, rdl): 0.0022196334, 2.26629e-05

Epoch over!
epoch time: 12.161

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.085 +- 0.179
mrr vals (pred, true): 0.094, 0.120

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05078 	 0.00562 	 m..s
   80 	     1 	 0.05935 	 0.00575 	 m..s
   48 	     2 	 0.05232 	 0.00575 	 m..s
    0 	     3 	 0.05078 	 0.00600 	 m..s
   68 	     4 	 0.05499 	 0.00602 	 m..s
   46 	     5 	 0.05226 	 0.00613 	 m..s
    0 	     6 	 0.05078 	 0.00613 	 m..s
   43 	     7 	 0.05220 	 0.00614 	 m..s
   49 	     8 	 0.05239 	 0.00616 	 m..s
   29 	     9 	 0.05097 	 0.00618 	 m..s
    0 	    10 	 0.05078 	 0.00619 	 m..s
   92 	    11 	 0.09112 	 0.00619 	 m..s
    0 	    12 	 0.05078 	 0.00620 	 m..s
   33 	    13 	 0.05116 	 0.00621 	 m..s
   76 	    14 	 0.05640 	 0.00623 	 m..s
   60 	    15 	 0.05256 	 0.00632 	 m..s
   53 	    16 	 0.05244 	 0.00634 	 m..s
    0 	    17 	 0.05078 	 0.00635 	 m..s
    0 	    18 	 0.05078 	 0.00638 	 m..s
   65 	    19 	 0.05347 	 0.00639 	 m..s
   32 	    20 	 0.05113 	 0.00640 	 m..s
    0 	    21 	 0.05078 	 0.00642 	 m..s
    0 	    22 	 0.05078 	 0.00644 	 m..s
   50 	    23 	 0.05240 	 0.00646 	 m..s
   71 	    24 	 0.05570 	 0.00646 	 m..s
    0 	    25 	 0.05078 	 0.00647 	 m..s
   47 	    26 	 0.05228 	 0.00647 	 m..s
   84 	    27 	 0.05992 	 0.00648 	 m..s
    0 	    28 	 0.05078 	 0.00648 	 m..s
    0 	    29 	 0.05078 	 0.00651 	 m..s
   83 	    30 	 0.05983 	 0.00653 	 m..s
   72 	    31 	 0.05587 	 0.00653 	 m..s
   81 	    32 	 0.05943 	 0.00656 	 m..s
    0 	    33 	 0.05078 	 0.00657 	 m..s
   51 	    34 	 0.05240 	 0.00659 	 m..s
    0 	    35 	 0.05078 	 0.00661 	 m..s
   40 	    36 	 0.05195 	 0.00668 	 m..s
   41 	    37 	 0.05215 	 0.00670 	 m..s
   59 	    38 	 0.05254 	 0.00670 	 m..s
   61 	    39 	 0.05274 	 0.00670 	 m..s
   44 	    40 	 0.05220 	 0.00675 	 m..s
   56 	    41 	 0.05246 	 0.00678 	 m..s
    0 	    42 	 0.05078 	 0.00680 	 m..s
    0 	    43 	 0.05078 	 0.00898 	 m..s
    0 	    44 	 0.05078 	 0.00967 	 m..s
    0 	    45 	 0.05078 	 0.01013 	 m..s
    0 	    46 	 0.05078 	 0.01084 	 m..s
   31 	    47 	 0.05110 	 0.01152 	 m..s
   30 	    48 	 0.05097 	 0.01170 	 m..s
   37 	    49 	 0.05118 	 0.01320 	 m..s
    0 	    50 	 0.05078 	 0.01346 	 m..s
    0 	    51 	 0.05078 	 0.01355 	 m..s
   35 	    52 	 0.05117 	 0.01464 	 m..s
    0 	    53 	 0.05078 	 0.02118 	 ~...
   36 	    54 	 0.05118 	 0.02772 	 ~...
   34 	    55 	 0.05117 	 0.02799 	 ~...
   38 	    56 	 0.05131 	 0.03191 	 ~...
    0 	    57 	 0.05078 	 0.03202 	 ~...
    0 	    58 	 0.05078 	 0.03288 	 ~...
    0 	    59 	 0.05078 	 0.03449 	 ~...
   39 	    60 	 0.05194 	 0.03515 	 ~...
    0 	    61 	 0.05078 	 0.03682 	 ~...
   57 	    62 	 0.05247 	 0.03700 	 ~...
   70 	    63 	 0.05514 	 0.04417 	 ~...
   45 	    64 	 0.05222 	 0.04468 	 ~...
   69 	    65 	 0.05508 	 0.05164 	 ~...
   52 	    66 	 0.05242 	 0.05202 	 ~...
   55 	    67 	 0.05246 	 0.05450 	 ~...
   88 	    68 	 0.06561 	 0.06068 	 ~...
  100 	    69 	 0.12192 	 0.06172 	 m..s
   75 	    70 	 0.05633 	 0.06223 	 ~...
   64 	    71 	 0.05324 	 0.06588 	 ~...
   74 	    72 	 0.05600 	 0.06713 	 ~...
   87 	    73 	 0.06424 	 0.06932 	 ~...
   82 	    74 	 0.05966 	 0.06968 	 ~...
   85 	    75 	 0.06036 	 0.07223 	 ~...
   73 	    76 	 0.05594 	 0.07320 	 ~...
   78 	    77 	 0.05713 	 0.07603 	 ~...
   54 	    78 	 0.05244 	 0.07764 	 ~...
   67 	    79 	 0.05469 	 0.07875 	 ~...
  101 	    80 	 0.12204 	 0.08233 	 m..s
   62 	    81 	 0.05282 	 0.08482 	 m..s
   89 	    82 	 0.07157 	 0.08746 	 ~...
   91 	    83 	 0.08765 	 0.09061 	 ~...
   79 	    84 	 0.05760 	 0.09215 	 m..s
    0 	    85 	 0.05078 	 0.09260 	 m..s
  102 	    86 	 0.12536 	 0.09268 	 m..s
    0 	    87 	 0.05078 	 0.09630 	 m..s
   42 	    88 	 0.05216 	 0.09695 	 m..s
   66 	    89 	 0.05351 	 0.09718 	 m..s
   58 	    90 	 0.05254 	 0.09721 	 m..s
  103 	    91 	 0.12653 	 0.09751 	 ~...
   63 	    92 	 0.05307 	 0.09761 	 m..s
    0 	    93 	 0.05078 	 0.09766 	 m..s
   77 	    94 	 0.05651 	 0.10248 	 m..s
   86 	    95 	 0.06174 	 0.10386 	 m..s
   90 	    96 	 0.08529 	 0.10577 	 ~...
   94 	    97 	 0.09218 	 0.10631 	 ~...
   95 	    98 	 0.09247 	 0.11133 	 ~...
   93 	    99 	 0.09211 	 0.11353 	 ~...
   97 	   100 	 0.09364 	 0.11449 	 ~...
   96 	   101 	 0.09337 	 0.11969 	 ~...
   98 	   102 	 0.09402 	 0.12039 	 ~...
  105 	   103 	 0.13659 	 0.12649 	 ~...
   99 	   104 	 0.09555 	 0.14474 	 m..s
  104 	   105 	 0.13069 	 0.14935 	 ~...
  111 	   106 	 0.19169 	 0.15947 	 m..s
  110 	   107 	 0.18329 	 0.16659 	 ~...
  106 	   108 	 0.16109 	 0.16702 	 ~...
  109 	   109 	 0.18013 	 0.16863 	 ~...
  107 	   110 	 0.16789 	 0.19590 	 ~...
  108 	   111 	 0.16806 	 0.20024 	 m..s
  115 	   112 	 0.26067 	 0.20588 	 m..s
  114 	   113 	 0.23512 	 0.20766 	 ~...
  113 	   114 	 0.23278 	 0.21619 	 ~...
  112 	   115 	 0.22083 	 0.24186 	 ~...
  116 	   116 	 0.26267 	 0.27042 	 ~...
  117 	   117 	 0.26653 	 0.27292 	 ~...
  119 	   118 	 0.32740 	 0.29220 	 m..s
  118 	   119 	 0.32540 	 0.29431 	 m..s
  120 	   120 	 0.34430 	 0.30150 	 m..s
==========================================
r_mrr = 0.8975656032562256
r2_mrr = 0.7469315528869629
spearmanr_mrr@5 = 0.9975886940956116
spearmanr_mrr@10 = 0.9418588280677795
spearmanr_mrr@50 = 0.9900104999542236
spearmanr_mrr@100 = 0.9367754459381104
spearmanr_mrr@All = 0.9322717189788818
==========================================
test time: 0.389
Done Testing dataset OpenEA
total time taken: 197.09227967262268
training time taken: 181.63586139678955
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8976)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7469)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9976)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9419)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9900)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9368)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9323)}}, 'test_loss': {'TransE': {'OpenEA': 0.5205928171562846}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4163391549642081
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [99, 118, 141, 185, 545, 371, 343, 178, 584, 105, 576, 360, 1089, 672, 90, 875, 588, 816, 66, 294, 1020, 1137, 884, 1060, 851, 305, 366, 1086, 525, 1107, 170, 89, 70, 1110, 931, 983, 941, 156, 135, 65, 24, 372, 972, 204, 148, 1162, 184, 1075, 817, 74, 181, 597, 233, 1213, 270, 659, 990, 44, 892, 564, 1059, 19, 1080, 359, 571, 589, 802, 918, 812, 868, 325, 562, 695, 137, 977, 784, 1104, 384, 140, 120, 515, 201, 92, 1018, 624, 818, 910, 732, 231, 398, 1091, 67, 1131, 420, 462, 96, 1147, 361, 403, 388, 712, 905, 1156, 149, 858, 35, 529, 841, 1039, 1175, 590, 1056, 650, 1077, 516, 538, 1134, 530, 1132, 772, 308]
valid_ids (0): []
train_ids (1094): [408, 856, 575, 635, 313, 397, 847, 736, 1109, 651, 162, 1173, 58, 441, 880, 1144, 968, 1121, 332, 652, 499, 282, 660, 922, 368, 490, 1016, 47, 20, 988, 203, 131, 994, 456, 460, 862, 84, 240, 832, 748, 730, 451, 1182, 517, 326, 1030, 1076, 284, 257, 520, 215, 155, 123, 463, 111, 714, 81, 121, 951, 791, 1125, 956, 682, 800, 993, 852, 632, 933, 197, 39, 750, 303, 1054, 30, 965, 395, 518, 688, 767, 787, 212, 763, 605, 836, 1153, 324, 1029, 725, 1011, 2, 205, 318, 866, 647, 400, 1120, 206, 1158, 843, 729, 107, 487, 929, 986, 351, 1036, 1145, 604, 970, 154, 536, 1050, 586, 833, 225, 414, 164, 295, 1202, 1035, 432, 393, 1025, 136, 707, 1185, 1200, 312, 641, 160, 298, 0, 10, 283, 467, 996, 1068, 864, 26, 1118, 826, 168, 1196, 1032, 46, 244, 908, 405, 991, 183, 623, 989, 1017, 759, 182, 778, 631, 407, 268, 1177, 1043, 963, 654, 412, 1005, 734, 874, 331, 819, 116, 602, 514, 261, 582, 777, 234, 57, 553, 34, 1122, 637, 345, 378, 265, 599, 1087, 191, 198, 329, 504, 1053, 134, 825, 834, 279, 940, 448, 1006, 942, 895, 363, 1210, 709, 1031, 840, 161, 813, 949, 1151, 238, 354, 967, 731, 43, 72, 1205, 806, 511, 239, 367, 483, 958, 815, 1176, 783, 276, 1096, 78, 999, 565, 510, 879, 327, 698, 848, 452, 127, 1198, 292, 1212, 346, 782, 622, 726, 867, 307, 322, 680, 761, 200, 1214, 1037, 1069, 396, 32, 380, 608, 417, 1099, 104, 566, 861, 323, 98, 1195, 716, 296, 42, 413, 1190, 668, 556, 1171, 513, 176, 560, 373, 913, 364, 755, 321, 959, 1150, 925, 390, 213, 115, 628, 1027, 574, 891, 442, 1057, 63, 992, 100, 1046, 945, 1026, 163, 612, 649, 653, 1186, 438, 465, 581, 435, 541, 1055, 960, 281, 344, 756, 846, 110, 569, 315, 102, 287, 260, 677, 440, 842, 109, 1167, 1061, 543, 665, 1111, 619, 291, 1002, 975, 386, 873, 558, 754, 780, 55, 1040, 850, 655, 310, 697, 190, 717, 916, 216, 152, 86, 489, 610, 6, 1041, 814, 1038, 686, 245, 479, 662, 921, 1090, 890, 209, 1058, 1142, 1088, 609, 894, 876, 596, 337, 503, 319, 938, 129, 500, 97, 254, 739, 1157, 727, 831, 979, 51, 1094, 450, 824, 616, 807, 603, 795, 352, 917, 17, 1166, 468, 93, 693, 1100, 138, 948, 657, 689, 320, 355, 1204, 12, 835, 555, 580, 585, 563, 139, 781, 13, 611, 765, 551, 811, 444, 579, 676, 1045, 304, 334, 227, 255, 27, 1052, 477, 751, 69, 18, 410, 747, 1074, 406, 1085, 108, 669, 839, 786, 60, 906, 330, 379, 445, 1178, 436, 422, 1130, 926, 350, 1082, 274, 1012, 557, 969, 776, 194, 849, 1165, 286, 704, 76, 1115, 694, 1083, 1103, 187, 353, 1188, 188, 593, 117, 333, 1102, 976, 526, 886, 130, 700, 471, 627, 340, 1194, 167, 923, 230, 486, 443, 457, 752, 845, 301, 882, 1070, 722, 885, 498, 382, 1129, 392, 1064, 249, 822, 903, 1169, 935, 914, 171, 173, 22, 928, 626, 68, 218, 1, 280, 1023, 174, 837, 570, 803, 1199, 973, 741, 1143, 1007, 61, 144, 658, 1095, 540, 703, 1119, 470, 1049, 645, 595, 877, 674, 1138, 1097, 370, 246, 431, 646, 667, 259, 705, 642, 869, 1164, 606, 671, 794, 219, 860, 453, 1207, 953, 939, 1141, 1136, 122, 1009, 888, 357, 1133, 253, 25, 224, 328, 528, 636, 980, 1066, 912, 1154, 1191, 8, 512, 491, 1073, 724, 533, 64, 36, 237, 449, 40, 1028, 769, 447, 165, 946, 568, 29, 1187, 893, 132, 53, 547, 666, 808, 683, 347, 537, 293, 620, 1172, 911, 962, 1170, 1098, 23, 54, 484, 768, 770, 87, 419, 1019, 971, 242, 1146, 250, 681, 349, 1042, 241, 670, 944, 1139, 143, 421, 83, 28, 461, 853, 169, 252, 1093, 829, 592, 426, 1003, 454, 428, 101, 472, 544, 263, 844, 644, 774, 1101, 1078, 85, 930, 126, 455, 193, 901, 766, 598, 522, 1116, 362, 519, 936, 369, 402, 961, 809, 495, 915, 125, 1081, 91, 493, 713, 399, 1128, 56, 749, 38, 1001, 1179, 964, 799, 830, 737, 1192, 235, 199, 934, 673, 801, 195, 95, 740, 887, 207, 1208, 550, 920, 177, 974, 424, 572, 473, 31, 735, 299, 760, 661, 621, 7, 899, 663, 220, 706, 356, 497, 718, 1161, 810, 573, 387, 823, 600, 338, 1211, 300, 685, 710, 409, 643, 37, 854, 521, 721, 966, 805, 501, 587, 1108, 711, 317, 863, 466, 878, 383, 1168, 1201, 266, 900, 272, 523, 932, 764, 427, 696, 924, 629, 679, 285, 745, 430, 554, 699, 865, 11, 151, 508, 103, 481, 1063, 335, 625, 1160, 264, 309, 385, 546, 978, 401, 561, 229, 952, 210, 339, 1051, 532, 567, 821, 1044, 273, 243, 857, 228, 62, 88, 640, 1124, 534, 998, 475, 416, 771, 133, 75, 757, 859, 316, 889, 909, 506, 1072, 271, 474, 434, 648, 897, 719, 947, 708, 828, 159, 221, 377, 142, 548, 375, 982, 798, 1155, 728, 179, 404, 1034, 507, 79, 664, 502, 5, 336, 1065, 509, 492, 549, 290, 394, 256, 943, 145, 50, 494, 937, 773, 189, 114, 425, 52, 691, 793, 478, 1127, 476, 753, 871, 723, 1180, 262, 1206, 77, 1010, 630, 838, 775, 1000, 870, 1106, 656, 788, 1092, 790, 855, 1024, 746, 1135, 71, 1071, 236, 957, 358, 59, 701, 480, 577, 94, 439, 1184, 226, 927, 302, 1203, 211, 1048, 690, 147, 898, 1197, 469, 1014, 1193, 995, 289, 128, 157, 146, 1008, 247, 1126, 954, 985, 415, 45, 1163, 559, 119, 1149, 80, 376, 275, 269, 437, 881, 223, 459, 984, 1209, 1112, 180, 684, 175, 618, 202, 41, 1123, 423, 997, 381, 594, 1033, 365, 524, 3, 827, 488, 1183, 1140, 374, 702, 1062, 675, 633, 389, 542, 527, 1004, 1079, 715, 617, 539, 904, 196, 429, 797, 464, 1117, 446, 634, 591, 48, 907, 678, 614, 505, 742, 158, 692, 613, 106, 981, 872, 150, 1148, 222, 15, 433, 758, 1084, 1174, 902, 601, 391, 1114, 531, 820, 277, 1067, 792, 278, 987, 306, 16, 919, 789, 348, 779, 955, 314, 1105, 1181, 733, 267, 153, 112, 1159, 166, 796, 418, 639, 762, 192, 124, 341, 342, 1021, 552, 583, 21, 49, 33, 744, 248, 4, 578, 1152, 687, 411, 311, 1013, 535, 258, 785, 1113, 14, 297, 896, 186, 615, 251, 208, 485, 1015, 804, 607, 1189, 720, 9, 113, 1047, 496, 458, 217, 288, 214, 883, 950, 1022, 82, 172, 73, 738, 232, 482, 743, 638]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2659160883128310
the save name prefix for this run is:  chkpt-ID_2659160883128310_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 187
rank avg (pred): 0.534 +- 0.005
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001569343

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 502
rank avg (pred): 0.189 +- 0.114
mrr vals (pred, true): 0.165, 0.079
batch losses (mrrl, rdl): 0.0, 8.01392e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 585
rank avg (pred): 0.378 +- 0.253
mrr vals (pred, true): 0.243, 0.011
batch losses (mrrl, rdl): 0.0, 7.11814e-05

Epoch over!
epoch time: 12.179

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 598
rank avg (pred): 0.367 +- 0.257
mrr vals (pred, true): 0.260, 0.011
batch losses (mrrl, rdl): 0.0, 5.77755e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 376
rank avg (pred): 0.339 +- 0.272
mrr vals (pred, true): 0.316, 0.041
batch losses (mrrl, rdl): 0.0, 0.0005047009

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 482
rank avg (pred): 0.344 +- 0.287
mrr vals (pred, true): 0.337, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002356202

Epoch over!
epoch time: 11.988

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 856
rank avg (pred): 0.338 +- 0.288
mrr vals (pred, true): 0.346, 0.099
batch losses (mrrl, rdl): 0.0, 0.0001981107

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 513
rank avg (pred): 0.225 +- 0.193
mrr vals (pred, true): 0.346, 0.050
batch losses (mrrl, rdl): 0.0, 4.82028e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 152
rank avg (pred): 0.360 +- 0.303
mrr vals (pred, true): 0.336, 0.042
batch losses (mrrl, rdl): 0.0, 0.0006771513

Epoch over!
epoch time: 11.762

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 724
rank avg (pred): 0.339 +- 0.301
mrr vals (pred, true): 0.361, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002479884

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1130
rank avg (pred): 0.312 +- 0.294
mrr vals (pred, true): 0.364, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003725665

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 448
rank avg (pred): 0.327 +- 0.303
mrr vals (pred, true): 0.346, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002555798

Epoch over!
epoch time: 11.798

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1114
rank avg (pred): 0.345 +- 0.303
mrr vals (pred, true): 0.319, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002181028

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 394
rank avg (pred): 0.334 +- 0.302
mrr vals (pred, true): 0.286, 0.035
batch losses (mrrl, rdl): 0.0, 0.0004451146

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 702
rank avg (pred): 0.367 +- 0.306
mrr vals (pred, true): 0.318, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001436661

Epoch over!
epoch time: 11.816

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1047
rank avg (pred): 0.331 +- 0.296
mrr vals (pred, true): 0.325, 0.007
batch losses (mrrl, rdl): 0.7547784448, 0.0002910835

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 708
rank avg (pred): 0.556 +- 0.225
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 7.31419e-05, 0.0001370753

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 744
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.173, 0.216
batch losses (mrrl, rdl): 0.0185413826, 5.1563e-06

Epoch over!
epoch time: 12.078

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 374
rank avg (pred): 0.436 +- 0.265
mrr vals (pred, true): 0.061, 0.073
batch losses (mrrl, rdl): 0.0012836006, 0.0015177702

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1021
rank avg (pred): 0.489 +- 0.242
mrr vals (pred, true): 0.043, 0.089
batch losses (mrrl, rdl): 0.0004638161, 0.0019752602

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 783
rank avg (pred): 0.546 +- 0.168
mrr vals (pred, true): 0.028, 0.007
batch losses (mrrl, rdl): 0.0046491288, 0.0001487148

Epoch over!
epoch time: 11.949

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 843
rank avg (pred): 0.483 +- 0.217
mrr vals (pred, true): 0.043, 0.083
batch losses (mrrl, rdl): 0.0004609081, 0.0007211161

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1094
rank avg (pred): 0.385 +- 0.245
mrr vals (pred, true): 0.061, 0.101
batch losses (mrrl, rdl): 0.0160882194, 0.0010440623

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1185
rank avg (pred): 0.477 +- 0.199
mrr vals (pred, true): 0.045, 0.066
batch losses (mrrl, rdl): 0.0002274366, 0.0005943244

Epoch over!
epoch time: 11.93

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 291
rank avg (pred): 0.012 +- 0.009
mrr vals (pred, true): 0.165, 0.093
batch losses (mrrl, rdl): 0.1324687004, 0.0001043039

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 614
rank avg (pred): 0.456 +- 0.190
mrr vals (pred, true): 0.046, 0.035
batch losses (mrrl, rdl): 0.0001588381, 0.0004774439

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 893
rank avg (pred): 0.396 +- 0.214
mrr vals (pred, true): 0.057, 0.057
batch losses (mrrl, rdl): 0.0005139126, 0.0021124808

Epoch over!
epoch time: 11.994

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 469
rank avg (pred): 0.447 +- 0.186
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.000270289, 6.35761e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1051
rank avg (pred): 0.274 +- 0.182
mrr vals (pred, true): 0.082, 0.007
batch losses (mrrl, rdl): 0.0099271247, 0.0007547674

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 913
rank avg (pred): 0.311 +- 0.175
mrr vals (pred, true): 0.060, 0.095
batch losses (mrrl, rdl): 0.0010964702, 0.0014081286

Epoch over!
epoch time: 12.091

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1146
rank avg (pred): 0.088 +- 0.059
mrr vals (pred, true): 0.095, 0.107
batch losses (mrrl, rdl): 0.0014473841, 0.0001600015

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1177
rank avg (pred): 0.384 +- 0.200
mrr vals (pred, true): 0.062, 0.066
batch losses (mrrl, rdl): 0.001379119, 0.0001655782

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 750
rank avg (pred): 0.101 +- 0.065
mrr vals (pred, true): 0.097, 0.147
batch losses (mrrl, rdl): 0.0256530643, 0.0001094586

Epoch over!
epoch time: 12.174

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 114
rank avg (pred): 0.422 +- 0.168
mrr vals (pred, true): 0.042, 0.012
batch losses (mrrl, rdl): 0.0005672385, 0.0005416168

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 625
rank avg (pred): 0.399 +- 0.179
mrr vals (pred, true): 0.049, 0.012
batch losses (mrrl, rdl): 1.51836e-05, 0.0001317615

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1033
rank avg (pred): 0.347 +- 0.210
mrr vals (pred, true): 0.077, 0.007
batch losses (mrrl, rdl): 0.007341742, 0.000349777

Epoch over!
epoch time: 12.038

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 396
rank avg (pred): 0.381 +- 0.186
mrr vals (pred, true): 0.049, 0.028
batch losses (mrrl, rdl): 1.24607e-05, 0.000575812

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1105
rank avg (pred): 0.262 +- 0.169
mrr vals (pred, true): 0.081, 0.119
batch losses (mrrl, rdl): 0.014782073, 0.0002934232

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 91
rank avg (pred): 0.392 +- 0.174
mrr vals (pred, true): 0.051, 0.020
batch losses (mrrl, rdl): 9.0221e-06, 0.0005652701

Epoch over!
epoch time: 12.0

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1109
rank avg (pred): 0.350 +- 0.195
mrr vals (pred, true): 0.063, 0.007
batch losses (mrrl, rdl): 0.001741569, 0.0003265436

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 31
rank avg (pred): 0.135 +- 0.085
mrr vals (pred, true): 0.105, 0.083
batch losses (mrrl, rdl): 0.0302926786, 7.25058e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 186
rank avg (pred): 0.367 +- 0.186
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 9.7541e-06, 0.0002679775

Epoch over!
epoch time: 12.229

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 395
rank avg (pred): 0.374 +- 0.181
mrr vals (pred, true): 0.054, 0.066
batch losses (mrrl, rdl): 0.0001785262, 0.0008432411

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 551
rank avg (pred): 0.085 +- 0.054
mrr vals (pred, true): 0.087, 0.104
batch losses (mrrl, rdl): 0.0027342394, 0.0002253744

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 296
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.282, 0.281
batch losses (mrrl, rdl): 1.1454e-06, 6.04971e-05

Epoch over!
epoch time: 11.891

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.374 +- 0.178
mrr vals (pred, true): 0.048, 0.015

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   41 	     0 	 0.04631 	 0.00598 	 m..s
   32 	     1 	 0.04542 	 0.00611 	 m..s
   55 	     2 	 0.04954 	 0.00616 	 m..s
   19 	     3 	 0.04457 	 0.00619 	 m..s
    7 	     4 	 0.04334 	 0.00620 	 m..s
   77 	     5 	 0.05844 	 0.00628 	 m..s
   30 	     6 	 0.04534 	 0.00632 	 m..s
   64 	     7 	 0.05382 	 0.00634 	 m..s
   11 	     8 	 0.04431 	 0.00640 	 m..s
   53 	     9 	 0.04940 	 0.00640 	 m..s
   50 	    10 	 0.04830 	 0.00644 	 m..s
   29 	    11 	 0.04533 	 0.00646 	 m..s
   63 	    12 	 0.05261 	 0.00647 	 m..s
   91 	    13 	 0.07630 	 0.00648 	 m..s
   90 	    14 	 0.07622 	 0.00648 	 m..s
   13 	    15 	 0.04438 	 0.00651 	 m..s
   20 	    16 	 0.04463 	 0.00652 	 m..s
   28 	    17 	 0.04531 	 0.00657 	 m..s
   80 	    18 	 0.06185 	 0.00658 	 m..s
   54 	    19 	 0.04952 	 0.00659 	 m..s
   21 	    20 	 0.04465 	 0.00661 	 m..s
   23 	    21 	 0.04513 	 0.00663 	 m..s
   24 	    22 	 0.04519 	 0.00664 	 m..s
   81 	    23 	 0.06207 	 0.00667 	 m..s
   34 	    24 	 0.04559 	 0.00669 	 m..s
    1 	    25 	 0.04242 	 0.00670 	 m..s
   45 	    26 	 0.04722 	 0.00673 	 m..s
    9 	    27 	 0.04348 	 0.00801 	 m..s
    8 	    28 	 0.04346 	 0.00805 	 m..s
    2 	    29 	 0.04258 	 0.00961 	 m..s
   18 	    30 	 0.04455 	 0.01046 	 m..s
    3 	    31 	 0.04263 	 0.01138 	 m..s
   10 	    32 	 0.04425 	 0.01169 	 m..s
   12 	    33 	 0.04436 	 0.01244 	 m..s
   33 	    34 	 0.04548 	 0.01320 	 m..s
    4 	    35 	 0.04323 	 0.01330 	 ~...
    5 	    36 	 0.04328 	 0.01346 	 ~...
   48 	    37 	 0.04807 	 0.01442 	 m..s
   49 	    38 	 0.04813 	 0.01452 	 m..s
   60 	    39 	 0.05213 	 0.01485 	 m..s
   22 	    40 	 0.04487 	 0.01537 	 ~...
    6 	    41 	 0.04331 	 0.01884 	 ~...
   40 	    42 	 0.04627 	 0.01968 	 ~...
   39 	    43 	 0.04623 	 0.02024 	 ~...
   14 	    44 	 0.04441 	 0.02255 	 ~...
   17 	    45 	 0.04452 	 0.02320 	 ~...
   56 	    46 	 0.05105 	 0.02499 	 ~...
   31 	    47 	 0.04534 	 0.02782 	 ~...
   26 	    48 	 0.04523 	 0.03177 	 ~...
   27 	    49 	 0.04525 	 0.03191 	 ~...
   42 	    50 	 0.04648 	 0.03485 	 ~...
   25 	    51 	 0.04519 	 0.03492 	 ~...
   43 	    52 	 0.04713 	 0.03515 	 ~...
   67 	    53 	 0.05413 	 0.03581 	 ~...
   44 	    54 	 0.04715 	 0.03605 	 ~...
   57 	    55 	 0.05195 	 0.03700 	 ~...
   52 	    56 	 0.04853 	 0.03717 	 ~...
   36 	    57 	 0.04592 	 0.03734 	 ~...
   51 	    58 	 0.04839 	 0.03866 	 ~...
   47 	    59 	 0.04730 	 0.04039 	 ~...
   59 	    60 	 0.05203 	 0.04777 	 ~...
   62 	    61 	 0.05226 	 0.04971 	 ~...
   38 	    62 	 0.04613 	 0.05350 	 ~...
   83 	    63 	 0.06365 	 0.05617 	 ~...
   78 	    64 	 0.05873 	 0.05743 	 ~...
   65 	    65 	 0.05391 	 0.05813 	 ~...
   66 	    66 	 0.05405 	 0.05911 	 ~...
  100 	    67 	 0.10239 	 0.05950 	 m..s
   76 	    68 	 0.05809 	 0.06223 	 ~...
   61 	    69 	 0.05222 	 0.06395 	 ~...
   69 	    70 	 0.05522 	 0.06655 	 ~...
   68 	    71 	 0.05519 	 0.06713 	 ~...
   71 	    72 	 0.05631 	 0.07131 	 ~...
   94 	    73 	 0.08469 	 0.07267 	 ~...
   79 	    74 	 0.05892 	 0.07271 	 ~...
   70 	    75 	 0.05619 	 0.07477 	 ~...
   74 	    76 	 0.05719 	 0.07586 	 ~...
   73 	    77 	 0.05693 	 0.07603 	 ~...
   72 	    78 	 0.05691 	 0.07777 	 ~...
  101 	    79 	 0.11119 	 0.08233 	 ~...
   82 	    80 	 0.06237 	 0.08825 	 ~...
   46 	    81 	 0.04728 	 0.09004 	 m..s
  108 	    82 	 0.20018 	 0.09061 	 MISS
  109 	    83 	 0.20026 	 0.09087 	 MISS
   89 	    84 	 0.07282 	 0.09143 	 ~...
  110 	    85 	 0.20050 	 0.09178 	 MISS
   16 	    86 	 0.04445 	 0.09363 	 m..s
   75 	    87 	 0.05809 	 0.09385 	 m..s
   58 	    88 	 0.05202 	 0.09456 	 m..s
   86 	    89 	 0.07257 	 0.09486 	 ~...
   37 	    90 	 0.04596 	 0.09542 	 m..s
   35 	    91 	 0.04586 	 0.09660 	 m..s
   15 	    92 	 0.04443 	 0.09687 	 m..s
    0 	    93 	 0.04220 	 0.09739 	 m..s
   87 	    94 	 0.07257 	 0.10189 	 ~...
   85 	    95 	 0.06948 	 0.10453 	 m..s
   88 	    96 	 0.07257 	 0.10484 	 m..s
  103 	    97 	 0.11901 	 0.10675 	 ~...
   98 	    98 	 0.09146 	 0.10736 	 ~...
   84 	    99 	 0.06945 	 0.10766 	 m..s
   97 	   100 	 0.09110 	 0.10775 	 ~...
   99 	   101 	 0.09345 	 0.11841 	 ~...
   92 	   102 	 0.07692 	 0.12089 	 m..s
   93 	   103 	 0.07796 	 0.12157 	 m..s
  104 	   104 	 0.13152 	 0.12892 	 ~...
   96 	   105 	 0.08910 	 0.12970 	 m..s
   95 	   106 	 0.08857 	 0.14405 	 m..s
  105 	   107 	 0.13608 	 0.14759 	 ~...
  102 	   108 	 0.11795 	 0.16380 	 m..s
  106 	   109 	 0.14929 	 0.19103 	 m..s
  107 	   110 	 0.15056 	 0.19590 	 m..s
  112 	   111 	 0.25059 	 0.21936 	 m..s
  111 	   112 	 0.25010 	 0.22417 	 ~...
  115 	   113 	 0.25591 	 0.24897 	 ~...
  117 	   114 	 0.25960 	 0.27178 	 ~...
  114 	   115 	 0.25323 	 0.27280 	 ~...
  113 	   116 	 0.25275 	 0.27746 	 ~...
  116 	   117 	 0.25905 	 0.28765 	 ~...
  118 	   118 	 0.26965 	 0.29280 	 ~...
  119 	   119 	 0.27625 	 0.29501 	 ~...
  120 	   120 	 0.27638 	 0.30150 	 ~...
==========================================
r_mrr = 0.8830305933952332
r2_mrr = 0.760827362537384
spearmanr_mrr@5 = 0.8898932933807373
spearmanr_mrr@10 = 0.825305700302124
spearmanr_mrr@50 = 0.982356071472168
spearmanr_mrr@100 = 0.9606359601020813
spearmanr_mrr@All = 0.9558895826339722
==========================================
test time: 0.417
Done Testing dataset OpenEA
total time taken: 195.86443305015564
training time taken: 180.41234922409058
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8830)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7608)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.8899)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8253)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9824)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9606)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9559)}}, 'test_loss': {'TransE': {'OpenEA': 1.0994850866409251}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 4815533610657581
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [246, 952, 259, 564, 92, 1210, 994, 742, 110, 1171, 1143, 215, 580, 262, 623, 425, 21, 490, 229, 3, 178, 610, 1068, 1057, 25, 249, 281, 650, 790, 993, 1063, 431, 1033, 1008, 266, 112, 157, 1162, 1211, 627, 755, 765, 603, 424, 908, 822, 1021, 678, 1193, 740, 1098, 975, 657, 582, 950, 270, 343, 197, 34, 486, 322, 593, 273, 234, 1016, 460, 293, 642, 918, 662, 708, 988, 959, 156, 668, 739, 344, 379, 94, 934, 52, 992, 557, 601, 254, 520, 962, 541, 922, 658, 677, 581, 48, 910, 146, 64, 130, 1155, 764, 5, 168, 572, 165, 1093, 535, 1133, 15, 1074, 998, 127, 1131, 1061, 365, 944, 473, 1109, 892, 710, 268, 751, 1005]
valid_ids (0): []
train_ids (1094): [219, 760, 715, 1026, 149, 1213, 57, 702, 134, 615, 345, 480, 212, 1060, 835, 301, 394, 114, 855, 663, 653, 537, 1092, 1097, 88, 479, 831, 1132, 47, 628, 1121, 218, 575, 781, 488, 420, 984, 83, 9, 1066, 107, 498, 524, 1158, 1078, 275, 95, 300, 1001, 550, 738, 961, 442, 812, 70, 1043, 566, 629, 217, 186, 871, 693, 870, 290, 417, 1204, 1056, 438, 45, 435, 184, 966, 1198, 144, 841, 309, 400, 848, 508, 965, 1180, 607, 754, 291, 26, 354, 141, 940, 792, 1000, 546, 443, 357, 298, 1095, 898, 261, 151, 421, 671, 687, 86, 74, 507, 0, 265, 482, 771, 885, 469, 609, 888, 744, 447, 849, 103, 277, 904, 113, 779, 489, 314, 690, 163, 466, 982, 1176, 643, 495, 311, 1054, 1117, 717, 203, 890, 748, 308, 866, 857, 865, 821, 619, 87, 987, 830, 378, 875, 280, 207, 883, 1019, 478, 98, 891, 1208, 785, 1029, 85, 1126, 963, 237, 328, 78, 100, 1114, 423, 932, 384, 1062, 720, 803, 1045, 971, 660, 173, 1144, 12, 929, 373, 366, 2, 1166, 823, 576, 1183, 410, 382, 774, 879, 757, 1138, 787, 313, 39, 169, 376, 351, 882, 459, 145, 172, 91, 729, 712, 24, 29, 17, 1172, 1091, 463, 768, 968, 1173, 53, 894, 1145, 705, 521, 1018, 493, 1163, 125, 868, 540, 419, 121, 859, 986, 182, 450, 725, 170, 716, 1152, 224, 287, 63, 1202, 321, 383, 1165, 105, 1125, 1207, 35, 93, 18, 4, 867, 636, 1042, 513, 719, 1123, 617, 612, 1004, 342, 304, 198, 1148, 625, 844, 230, 776, 798, 1002, 483, 590, 302, 369, 363, 1147, 96, 555, 737, 356, 1041, 913, 468, 786, 797, 858, 56, 955, 377, 104, 251, 1159, 210, 359, 766, 728, 752, 534, 303, 679, 811, 1113, 554, 1201, 205, 721, 750, 462, 645, 532, 245, 808, 334, 109, 414, 1070, 1122, 856, 1178, 333, 452, 630, 142, 574, 1164, 842, 272, 772, 81, 887, 282, 364, 166, 746, 549, 7, 264, 1059, 602, 990, 1096, 135, 850, 767, 1187, 128, 506, 1194, 689, 701, 1003, 214, 139, 604, 23, 533, 241, 189, 16, 296, 516, 375, 519, 976, 122, 390, 225, 1076, 396, 22, 565, 531, 732, 547, 618, 267, 248, 1081, 640, 299, 1177, 133, 880, 499, 718, 353, 408, 500, 1013, 621, 77, 350, 89, 73, 67, 526, 683, 514, 682, 818, 1052, 881, 349, 46, 773, 1119, 152, 733, 55, 399, 324, 51, 132, 957, 54, 14, 11, 919, 1214, 1046, 1129, 634, 434, 905, 1101, 84, 544, 530, 568, 1192, 412, 1058, 199, 795, 131, 999, 209, 320, 921, 522, 115, 387, 405, 295, 876, 845, 348, 1038, 430, 1174, 644, 228, 372, 639, 253, 699, 27, 1009, 759, 911, 256, 886, 1146, 1151, 852, 902, 806, 1007, 1141, 76, 188, 896, 1032, 588, 1084, 30, 553, 154, 1099, 1047, 255, 780, 579, 325, 415, 641, 174, 980, 1195, 1127, 1137, 872, 727, 614, 318, 525, 567, 825, 223, 595, 1161, 936, 561, 160, 611, 698, 815, 793, 920, 801, 392, 1006, 1079, 1136, 613, 153, 406, 1206, 161, 43, 310, 99, 445, 654, 1088, 691, 1011, 1154, 448, 10, 429, 745, 380, 1010, 928, 1055, 492, 843, 1209, 711, 809, 232, 1139, 362, 32, 502, 1196, 196, 707, 1075, 545, 1106, 997, 72, 222, 402, 370, 457, 813, 200, 931, 1182, 374, 407, 206, 860, 620, 973, 204, 509, 1031, 454, 758, 220, 187, 583, 158, 191, 368, 464, 167, 829, 80, 472, 909, 433, 1130, 247, 1035, 926, 924, 50, 647, 775, 700, 242, 937, 42, 227, 724, 969, 58, 735, 893, 1087, 297, 527, 573, 1024, 1150, 346, 594, 985, 585, 504, 596, 1199, 233, 861, 907, 1184, 659, 202, 529, 586, 782, 260, 60, 386, 709, 1175, 1053, 147, 942, 467, 958, 578, 41, 978, 484, 1179, 238, 970, 951, 176, 824, 75, 385, 960, 1181, 1025, 355, 938, 827, 477, 305, 1080, 800, 213, 171, 116, 817, 1103, 180, 1083, 626, 862, 1190, 129, 61, 159, 1157, 675, 676, 646, 292, 672, 179, 1128, 286, 393, 927, 743, 539, 111, 226, 1102, 221, 68, 804, 1167, 140, 244, 79, 807, 941, 991, 637, 558, 1034, 749, 1115, 622, 945, 632, 1090, 306, 914, 401, 599, 736, 404, 470, 1086, 126, 597, 446, 570, 1212, 784, 331, 1064, 38, 69, 552, 956, 317, 1124, 193, 1108, 118, 326, 563, 416, 802, 847, 1140, 571, 917, 106, 703, 589, 694, 1170, 954, 923, 1, 605, 136, 820, 669, 826, 162, 741, 258, 428, 953, 901, 458, 474, 608, 819, 651, 1089, 491, 933, 455, 1065, 706, 930, 36, 1200, 235, 395, 269, 681, 8, 243, 461, 1189, 195, 475, 816, 481, 427, 1094, 1153, 398, 981, 97, 884, 667, 528, 471, 633, 897, 236, 624, 1205, 391, 1023, 889, 316, 551, 1082, 1085, 799, 1168, 686, 1120, 1160, 389, 330, 794, 211, 692, 684, 323, 912, 515, 285, 1051, 44, 1048, 426, 878, 638, 964, 763, 1028, 403, 65, 441, 307, 388, 895, 777, 935, 352, 216, 497, 695, 190, 577, 358, 503, 837, 789, 381, 979, 949, 332, 102, 494, 680, 274, 1203, 769, 730, 1135, 543, 90, 347, 1110, 397, 338, 164, 796, 279, 836, 451, 906, 119, 1037, 239, 834, 1191, 148, 231, 194, 512, 974, 1039, 143, 900, 432, 723, 439, 335, 1100, 569, 833, 201, 704, 722, 559, 150, 449, 713, 1112, 117, 1077, 538, 697, 252, 283, 371, 476, 840, 674, 40, 562, 1036, 66, 13, 289, 869, 120, 123, 548, 1022, 485, 838, 327, 1030, 1067, 465, 496, 319, 972, 696, 284, 591, 989, 183, 832, 1044, 600, 903, 814, 155, 437, 517, 805, 294, 943, 33, 436, 59, 846, 340, 864, 731, 422, 853, 560, 409, 863, 606, 839, 616, 747, 1116, 584, 288, 453, 1071, 670, 598, 649, 1069, 665, 874, 761, 208, 983, 339, 271, 948, 877, 1149, 501, 337, 1012, 315, 181, 518, 542, 413, 361, 810, 329, 1169, 177, 240, 341, 1107, 418, 1111, 635, 977, 1027, 185, 278, 263, 854, 31, 756, 946, 1104, 71, 440, 360, 1017, 124, 411, 20, 192, 666, 661, 685, 631, 1073, 1015, 536, 648, 947, 37, 788, 276, 510, 108, 1142, 367, 652, 783, 556, 456, 137, 1156, 1049, 312, 511, 28, 967, 664, 673, 1020, 336, 995, 1197, 851, 778, 791, 82, 1185, 762, 49, 175, 250, 916, 925, 1186, 828, 1072, 138, 656, 1188, 996, 726, 1050, 753, 62, 1040, 915, 505, 1118, 6, 873, 1105, 523, 444, 939, 257, 655, 714, 592, 688, 487, 734, 19, 1014, 770, 101, 587, 1134, 899]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9632047060569872
the save name prefix for this run is:  chkpt-ID_9632047060569872_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 733
rank avg (pred): 0.562 +- 0.005
mrr vals (pred, true): 0.000, 0.069
batch losses (mrrl, rdl): 0.0, 0.0055787503

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 722
rank avg (pred): 0.355 +- 0.260
mrr vals (pred, true): 0.093, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002235028

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 721
rank avg (pred): 0.392 +- 0.321
mrr vals (pred, true): 0.148, 0.006
batch losses (mrrl, rdl): 0.0, 8.85598e-05

Epoch over!
epoch time: 22.03

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 42
rank avg (pred): 0.099 +- 0.088
mrr vals (pred, true): 0.196, 0.104
batch losses (mrrl, rdl): 0.0, 2.00145e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 20
rank avg (pred): 0.080 +- 0.081
mrr vals (pred, true): 0.238, 0.254
batch losses (mrrl, rdl): 0.0, 2.93636e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1064
rank avg (pred): 0.047 +- 0.046
mrr vals (pred, true): 0.254, 0.213
batch losses (mrrl, rdl): 0.0, 6.1004e-06

Epoch over!
epoch time: 21.952

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 768
rank avg (pred): 0.364 +- 0.323
mrr vals (pred, true): 0.179, 0.097
batch losses (mrrl, rdl): 0.0, 0.0002759285

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 930
rank avg (pred): 0.379 +- 0.324
mrr vals (pred, true): 0.165, 0.096
batch losses (mrrl, rdl): 0.0, 0.0003452048

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1198
rank avg (pred): 0.392 +- 0.318
mrr vals (pred, true): 0.144, 0.007
batch losses (mrrl, rdl): 0.0, 8.68032e-05

Epoch over!
epoch time: 22.258

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 288
rank avg (pred): 0.075 +- 0.072
mrr vals (pred, true): 0.207, 0.098
batch losses (mrrl, rdl): 0.0, 1.18916e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 209
rank avg (pred): 0.349 +- 0.308
mrr vals (pred, true): 0.161, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002017021

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1166
rank avg (pred): 0.342 +- 0.305
mrr vals (pred, true): 0.175, 0.057
batch losses (mrrl, rdl): 0.0, 5.48961e-05

Epoch over!
epoch time: 21.514

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 77
rank avg (pred): 0.051 +- 0.056
mrr vals (pred, true): 0.278, 0.281
batch losses (mrrl, rdl): 0.0, 4.4689e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1082
rank avg (pred): 0.293 +- 0.288
mrr vals (pred, true): 0.181, 0.074
batch losses (mrrl, rdl): 0.0, 0.0002924897

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 357
rank avg (pred): 0.339 +- 0.306
mrr vals (pred, true): 0.150, 0.020
batch losses (mrrl, rdl): 0.0, 0.000221995

Epoch over!
epoch time: 21.174

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 946
rank avg (pred): 0.393 +- 0.329
mrr vals (pred, true): 0.139, 0.007
batch losses (mrrl, rdl): 0.0790588856, 7.88209e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.185, 0.301
batch losses (mrrl, rdl): 0.1346966475, 1.88727e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 54
rank avg (pred): 0.103 +- 0.072
mrr vals (pred, true): 0.109, 0.066
batch losses (mrrl, rdl): 0.0345671289, 2.07135e-05

Epoch over!
epoch time: 20.731

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 638
rank avg (pred): 0.400 +- 0.165
mrr vals (pred, true): 0.038, 0.032
batch losses (mrrl, rdl): 0.0014450635, 0.0002075459

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 338
rank avg (pred): 0.349 +- 0.169
mrr vals (pred, true): 0.055, 0.067
batch losses (mrrl, rdl): 0.0002335495, 0.000602828

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 971
rank avg (pred): 0.384 +- 0.162
mrr vals (pred, true): 0.040, 0.006
batch losses (mrrl, rdl): 0.000985648, 0.0002340898

Epoch over!
epoch time: 20.626

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 65
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.173, 0.196
batch losses (mrrl, rdl): 0.0053793746, 5.5023e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 913
rank avg (pred): 0.316 +- 0.183
mrr vals (pred, true): 0.061, 0.095
batch losses (mrrl, rdl): 0.0013118134, 0.0014755663

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 982
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.173, 0.213
batch losses (mrrl, rdl): 0.0163113549, 3.73627e-05

Epoch over!
epoch time: 21.201

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 328
rank avg (pred): 0.378 +- 0.142
mrr vals (pred, true): 0.038, 0.025
batch losses (mrrl, rdl): 0.0013297185, 0.0004732264

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 598
rank avg (pred): 0.327 +- 0.144
mrr vals (pred, true): 0.050, 0.011
batch losses (mrrl, rdl): 1.3976e-06, 8.48298e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 179
rank avg (pred): 0.331 +- 0.145
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002291556, 0.0005044091

Epoch over!
epoch time: 21.655

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 808
rank avg (pred): 0.374 +- 0.150
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002078043, 0.0002735635

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 353
rank avg (pred): 0.296 +- 0.136
mrr vals (pred, true): 0.039, 0.052
batch losses (mrrl, rdl): 0.0012699085, 0.0002472724

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 920
rank avg (pred): 0.367 +- 0.135
mrr vals (pred, true): 0.042, 0.096
batch losses (mrrl, rdl): 0.0007177578, 0.0002720331

Epoch over!
epoch time: 21.121

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 587
rank avg (pred): 0.361 +- 0.159
mrr vals (pred, true): 0.051, 0.034
batch losses (mrrl, rdl): 1.75597e-05, 0.0001321301

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 960
rank avg (pred): 0.367 +- 0.164
mrr vals (pred, true): 0.050, 0.007
batch losses (mrrl, rdl): 3.252e-07, 0.0002841957

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.350 +- 0.171
mrr vals (pred, true): 0.060, 0.006
batch losses (mrrl, rdl): 0.0009944047, 0.0003649481

Epoch over!
epoch time: 20.413

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 240
rank avg (pred): 0.393 +- 0.156
mrr vals (pred, true): 0.044, 0.007
batch losses (mrrl, rdl): 0.0003778351, 0.0001927277

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 555
rank avg (pred): 0.319 +- 0.262
mrr vals (pred, true): 0.044, 0.064
batch losses (mrrl, rdl): 0.0003223078, 0.0002624745

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 280
rank avg (pred): 0.077 +- 0.050
mrr vals (pred, true): 0.091, 0.120
batch losses (mrrl, rdl): 0.0087148035, 1.59246e-05

Epoch over!
epoch time: 21.314

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 602
rank avg (pred): 0.394 +- 0.344
mrr vals (pred, true): 0.047, 0.026
batch losses (mrrl, rdl): 0.0001184916, 0.0001077483

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 744
rank avg (pred): 0.057 +- 0.044
mrr vals (pred, true): 0.193, 0.216
batch losses (mrrl, rdl): 0.0056204107, 2.56142e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 24
rank avg (pred): 0.069 +- 0.054
mrr vals (pred, true): 0.110, 0.118
batch losses (mrrl, rdl): 0.0006987899, 2.68464e-05

Epoch over!
epoch time: 21.075

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1168
rank avg (pred): 0.336 +- 0.288
mrr vals (pred, true): 0.058, 0.068
batch losses (mrrl, rdl): 0.0005916214, 2.67121e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 192
rank avg (pred): 0.353 +- 0.307
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.0002987299, 0.0002331731

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 929
rank avg (pred): 0.381 +- 0.334
mrr vals (pred, true): 0.040, 0.094
batch losses (mrrl, rdl): 0.0009570125, 0.0002995481

Epoch over!
epoch time: 20.785

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 306
rank avg (pred): 0.135 +- 0.135
mrr vals (pred, true): 0.074, 0.087
batch losses (mrrl, rdl): 0.0058027441, 5.04958e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1075
rank avg (pred): 0.234 +- 0.205
mrr vals (pred, true): 0.229, 0.273
batch losses (mrrl, rdl): 0.0190059822, 0.0009728667

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 68
rank avg (pred): 0.031 +- 0.022
mrr vals (pred, true): 0.166, 0.193
batch losses (mrrl, rdl): 0.0068368861, 2.59658e-05

Epoch over!
epoch time: 20.485

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.177 +- 0.151
mrr vals (pred, true): 0.072, 0.079

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   75 	     0 	 0.05991 	 0.00602 	 m..s
   10 	     1 	 0.04482 	 0.00610 	 m..s
   34 	     2 	 0.04813 	 0.00623 	 m..s
   21 	     3 	 0.04692 	 0.00626 	 m..s
   64 	     4 	 0.05302 	 0.00630 	 m..s
   48 	     5 	 0.04968 	 0.00630 	 m..s
   32 	     6 	 0.04802 	 0.00632 	 m..s
   47 	     7 	 0.04957 	 0.00633 	 m..s
   17 	     8 	 0.04622 	 0.00634 	 m..s
    4 	     9 	 0.04406 	 0.00634 	 m..s
   49 	    10 	 0.04969 	 0.00635 	 m..s
    6 	    11 	 0.04451 	 0.00637 	 m..s
   37 	    12 	 0.04844 	 0.00637 	 m..s
   81 	    13 	 0.06680 	 0.00637 	 m..s
   29 	    14 	 0.04756 	 0.00640 	 m..s
   18 	    15 	 0.04646 	 0.00642 	 m..s
   23 	    16 	 0.04707 	 0.00643 	 m..s
   15 	    17 	 0.04566 	 0.00644 	 m..s
    0 	    18 	 0.03805 	 0.00645 	 m..s
    9 	    19 	 0.04477 	 0.00646 	 m..s
   50 	    20 	 0.04970 	 0.00647 	 m..s
   82 	    21 	 0.06718 	 0.00648 	 m..s
    3 	    22 	 0.04386 	 0.00648 	 m..s
   20 	    23 	 0.04676 	 0.00648 	 m..s
   61 	    24 	 0.05273 	 0.00650 	 m..s
   44 	    25 	 0.04915 	 0.00651 	 m..s
   63 	    26 	 0.05292 	 0.00651 	 m..s
   36 	    27 	 0.04829 	 0.00655 	 m..s
   38 	    28 	 0.04846 	 0.00656 	 m..s
   24 	    29 	 0.04710 	 0.00657 	 m..s
   39 	    30 	 0.04847 	 0.00659 	 m..s
   76 	    31 	 0.06073 	 0.00660 	 m..s
   42 	    32 	 0.04888 	 0.00660 	 m..s
    5 	    33 	 0.04407 	 0.00885 	 m..s
   12 	    34 	 0.04507 	 0.00967 	 m..s
   22 	    35 	 0.04697 	 0.00967 	 m..s
   35 	    36 	 0.04823 	 0.00971 	 m..s
    8 	    37 	 0.04470 	 0.01106 	 m..s
   13 	    38 	 0.04513 	 0.01263 	 m..s
   43 	    39 	 0.04896 	 0.01413 	 m..s
   52 	    40 	 0.04977 	 0.01485 	 m..s
   19 	    41 	 0.04646 	 0.01709 	 ~...
   31 	    42 	 0.04769 	 0.02013 	 ~...
   56 	    43 	 0.05023 	 0.02048 	 ~...
   57 	    44 	 0.05032 	 0.02136 	 ~...
   55 	    45 	 0.05022 	 0.02238 	 ~...
   45 	    46 	 0.04923 	 0.02377 	 ~...
   16 	    47 	 0.04614 	 0.02633 	 ~...
   11 	    48 	 0.04500 	 0.02736 	 ~...
   33 	    49 	 0.04807 	 0.03356 	 ~...
   53 	    50 	 0.04985 	 0.03429 	 ~...
   60 	    51 	 0.05270 	 0.03491 	 ~...
   40 	    52 	 0.04874 	 0.03571 	 ~...
   51 	    53 	 0.04974 	 0.03581 	 ~...
   46 	    54 	 0.04952 	 0.03717 	 ~...
   41 	    55 	 0.04883 	 0.04039 	 ~...
   54 	    56 	 0.04988 	 0.04107 	 ~...
   59 	    57 	 0.05135 	 0.04777 	 ~...
   67 	    58 	 0.05473 	 0.05188 	 ~...
   68 	    59 	 0.05476 	 0.05202 	 ~...
   74 	    60 	 0.05973 	 0.05617 	 ~...
   78 	    61 	 0.06485 	 0.05813 	 ~...
   73 	    62 	 0.05969 	 0.06068 	 ~...
   66 	    63 	 0.05421 	 0.06223 	 ~...
   28 	    64 	 0.04744 	 0.06440 	 ~...
   25 	    65 	 0.04727 	 0.06534 	 ~...
   27 	    66 	 0.04730 	 0.06586 	 ~...
   69 	    67 	 0.05523 	 0.06846 	 ~...
   65 	    68 	 0.05308 	 0.06928 	 ~...
   87 	    69 	 0.07618 	 0.07302 	 ~...
   58 	    70 	 0.05092 	 0.07339 	 ~...
   70 	    71 	 0.05541 	 0.07542 	 ~...
   84 	    72 	 0.07137 	 0.07908 	 ~...
   85 	    73 	 0.07167 	 0.07911 	 ~...
   88 	    74 	 0.07830 	 0.08414 	 ~...
    1 	    75 	 0.03828 	 0.08500 	 m..s
   93 	    76 	 0.08378 	 0.08724 	 ~...
   91 	    77 	 0.08331 	 0.08761 	 ~...
   72 	    78 	 0.05726 	 0.08855 	 m..s
   14 	    79 	 0.04525 	 0.09121 	 m..s
   90 	    80 	 0.08082 	 0.09279 	 ~...
   26 	    81 	 0.04727 	 0.09335 	 m..s
   98 	    82 	 0.09876 	 0.09435 	 ~...
   30 	    83 	 0.04757 	 0.09498 	 m..s
    7 	    84 	 0.04470 	 0.09739 	 m..s
    2 	    85 	 0.04277 	 0.09762 	 m..s
   86 	    86 	 0.07581 	 0.09922 	 ~...
   71 	    87 	 0.05580 	 0.10147 	 m..s
   79 	    88 	 0.06511 	 0.10549 	 m..s
   83 	    89 	 0.06846 	 0.10640 	 m..s
  100 	    90 	 0.11218 	 0.10833 	 ~...
   62 	    91 	 0.05276 	 0.10959 	 m..s
   80 	    92 	 0.06587 	 0.11133 	 m..s
   77 	    93 	 0.06123 	 0.11240 	 m..s
   97 	    94 	 0.09594 	 0.14112 	 m..s
   89 	    95 	 0.07913 	 0.14341 	 m..s
   92 	    96 	 0.08350 	 0.14486 	 m..s
   96 	    97 	 0.09106 	 0.14613 	 m..s
   99 	    98 	 0.11214 	 0.14871 	 m..s
   95 	    99 	 0.09094 	 0.14935 	 m..s
  104 	   100 	 0.15494 	 0.15731 	 ~...
  106 	   101 	 0.15761 	 0.16628 	 ~...
   94 	   102 	 0.08850 	 0.18164 	 m..s
  101 	   103 	 0.12946 	 0.18730 	 m..s
  103 	   104 	 0.13983 	 0.19077 	 m..s
  102 	   105 	 0.13674 	 0.20763 	 m..s
  108 	   106 	 0.17256 	 0.21262 	 m..s
  105 	   107 	 0.15742 	 0.21399 	 m..s
  111 	   108 	 0.22334 	 0.21775 	 ~...
  107 	   109 	 0.16216 	 0.24480 	 m..s
  116 	   110 	 0.24278 	 0.26458 	 ~...
  115 	   111 	 0.23854 	 0.26716 	 ~...
  114 	   112 	 0.23839 	 0.26855 	 m..s
  109 	   113 	 0.19354 	 0.27718 	 m..s
  113 	   114 	 0.23541 	 0.27844 	 m..s
  119 	   115 	 0.26664 	 0.28232 	 ~...
  112 	   116 	 0.22875 	 0.28350 	 m..s
  110 	   117 	 0.20002 	 0.28719 	 m..s
  118 	   118 	 0.26575 	 0.28985 	 ~...
  120 	   119 	 0.26772 	 0.29402 	 ~...
  117 	   120 	 0.25881 	 0.29431 	 m..s
==========================================
r_mrr = 0.916662335395813
r2_mrr = 0.7915408611297607
spearmanr_mrr@5 = 0.9119323492050171
spearmanr_mrr@10 = 0.9591056704521179
spearmanr_mrr@50 = 0.9897003769874573
spearmanr_mrr@100 = 0.9559905529022217
spearmanr_mrr@All = 0.9532753229141235
==========================================
test time: 0.869
Done Testing dataset OpenEA
total time taken: 335.01214122772217
training time taken: 319.28623366355896
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.9167)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7915)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9119)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9591)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9897)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9560)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9533)}}, 'test_loss': {'TransE': {'OpenEA': 1.001233057670106}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 3920567194370276
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [388, 775, 888, 579, 663, 357, 506, 214, 89, 372, 867, 307, 208, 1175, 172, 642, 993, 1005, 563, 558, 7, 1183, 43, 70, 1153, 546, 770, 246, 187, 381, 447, 489, 585, 378, 390, 715, 661, 193, 281, 85, 1152, 1060, 363, 112, 157, 1192, 32, 236, 29, 100, 1154, 396, 476, 120, 152, 404, 190, 932, 394, 653, 826, 416, 335, 505, 1049, 1204, 501, 305, 693, 473, 599, 493, 437, 813, 502, 571, 842, 491, 130, 697, 898, 687, 1047, 735, 223, 71, 329, 889, 1167, 438, 1000, 837, 584, 1053, 976, 670, 801, 1027, 44, 515, 864, 119, 131, 971, 276, 827, 751, 460, 425, 561, 519, 373, 610, 279, 86, 139, 948, 996, 729, 603, 251]
valid_ids (0): []
train_ids (1094): [213, 485, 533, 707, 1117, 349, 1004, 762, 31, 391, 723, 588, 1078, 630, 941, 957, 21, 862, 103, 30, 1190, 1038, 312, 446, 270, 740, 125, 601, 886, 819, 1011, 15, 475, 1057, 738, 52, 512, 299, 925, 646, 154, 828, 166, 1012, 896, 692, 250, 574, 1023, 69, 41, 589, 146, 121, 672, 592, 1186, 1066, 737, 641, 780, 622, 698, 821, 829, 868, 778, 934, 294, 61, 626, 793, 654, 354, 528, 1029, 1178, 235, 424, 161, 123, 91, 1169, 822, 68, 1164, 488, 612, 718, 40, 262, 189, 232, 796, 160, 392, 1101, 665, 1142, 1062, 598, 755, 303, 726, 355, 426, 352, 342, 700, 1203, 183, 716, 1080, 849, 1189, 529, 451, 648, 174, 368, 1010, 768, 1042, 550, 1126, 38, 51, 406, 118, 521, 836, 313, 947, 960, 455, 138, 365, 772, 159, 99, 840, 359, 710, 1025, 746, 129, 1075, 317, 221, 107, 903, 1081, 732, 800, 596, 149, 790, 967, 508, 841, 559, 1065, 635, 669, 291, 371, 703, 478, 75, 1063, 1006, 217, 551, 383, 962, 802, 1150, 468, 815, 771, 535, 321, 666, 482, 853, 855, 562, 965, 542, 135, 414, 311, 517, 701, 852, 901, 206, 835, 348, 469, 547, 415, 351, 197, 760, 1120, 496, 1100, 831, 549, 440, 839, 134, 1021, 399, 163, 894, 42, 922, 578, 938, 266, 1149, 370, 345, 195, 298, 736, 548, 807, 804, 434, 611, 20, 36, 48, 974, 725, 1119, 464, 795, 178, 369, 117, 682, 463, 1013, 538, 531, 744, 1077, 227, 683, 1094, 743, 791, 316, 634, 225, 946, 1043, 685, 27, 985, 680, 1185, 587, 1140, 169, 1137, 106, 1113, 741, 207, 456, 572, 385, 769, 1195, 878, 875, 429, 93, 238, 586, 895, 293, 577, 173, 268, 444, 500, 252, 880, 1079, 1184, 1114, 498, 63, 26, 997, 607, 609, 904, 817, 116, 629, 647, 1035, 869, 748, 1197, 799, 84, 1107, 192, 420, 1028, 185, 649, 633, 97, 1097, 758, 727, 1084, 691, 65, 600, 73, 619, 516, 428, 617, 871, 659, 1116, 64, 297, 1022, 866, 955, 750, 1198, 17, 4, 376, 490, 461, 708, 1045, 824, 1214, 540, 580, 47, 568, 315, 713, 637, 209, 702, 714, 690, 644, 541, 194, 353, 1020, 931, 304, 274, 927, 620, 308, 1118, 92, 1115, 556, 430, 24, 1099, 199, 514, 203, 215, 151, 337, 233, 1095, 1069, 495, 1034, 344, 411, 782, 882, 1098, 263, 1112, 761, 511, 1155, 499, 966, 928, 614, 109, 570, 747, 720, 115, 763, 306, 413, 991, 523, 144, 893, 838, 259, 834, 393, 1146, 675, 1207, 261, 230, 786, 58, 497, 581, 22, 981, 1187, 54, 854, 709, 458, 324, 806, 892, 1172, 441, 454, 1026, 459, 5, 846, 285, 220, 504, 436, 1211, 623, 216, 162, 127, 228, 11, 845, 101, 814, 899, 389, 62, 167, 242, 569, 356, 1181, 1200, 487, 977, 1170, 492, 678, 231, 972, 936, 328, 1179, 265, 857, 764, 1129, 239, 50, 1030, 124, 883, 12, 273, 384, 472, 906, 13, 905, 212, 699, 658, 890, 552, 452, 379, 1109, 939, 1014, 145, 168, 797, 865, 1159, 900, 1138, 986, 210, 543, 560, 933, 636, 474, 361, 945, 767, 908, 509, 310, 963, 861, 327, 914, 792, 926, 1058, 677, 382, 643, 1024, 628, 255, 1051, 616, 314, 887, 39, 76, 566, 776, 417, 387, 553, 794, 188, 1122, 0, 1128, 1143, 1196, 35, 87, 929, 565, 1160, 1212, 833, 133, 367, 959, 19, 798, 1134, 527, 386, 526, 789, 1072, 320, 300, 339, 2, 733, 1108, 1032, 465, 1007, 198, 921, 431, 289, 330, 990, 830, 583, 442, 1166, 1103, 343, 419, 241, 975, 410, 450, 979, 1105, 877, 754, 95, 8, 397, 1206, 923, 322, 992, 260, 229, 486, 935, 6, 181, 1135, 1208, 818, 1144, 590, 1133, 536, 940, 1180, 122, 57, 968, 1083, 812, 811, 844, 10, 667, 248, 752, 1067, 374, 432, 94, 1050, 72, 785, 573, 1074, 1056, 924, 1059, 1157, 1131, 287, 919, 427, 650, 809, 773, 90, 518, 1052, 377, 591, 1205, 1070, 114, 673, 987, 942, 78, 958, 175, 247, 111, 902, 597, 1018, 480, 332, 564, 350, 34, 618, 301, 756, 218, 621, 724, 978, 136, 158, 59, 734, 1139, 477, 16, 326, 916, 989, 284, 870, 1089, 645, 457, 1076, 483, 105, 681, 45, 863, 243, 88, 325, 671, 1033, 721, 765, 18, 627, 874, 631, 334, 625, 143, 471, 341, 1082, 9, 881, 104, 576, 1158, 850, 346, 267, 980, 1136, 848, 937, 582, 400, 1046, 719, 1209, 333, 1093, 366, 695, 155, 81, 532, 292, 689, 944, 1161, 234, 271, 401, 1041, 1037, 165, 398, 915, 479, 257, 728, 176, 323, 53, 843, 439, 1111, 1182, 575, 1092, 67, 608, 171, 885, 872, 674, 956, 766, 749, 200, 338, 1188, 555, 803, 331, 453, 950, 706, 1201, 730, 153, 113, 137, 781, 522, 503, 205, 49, 1176, 742, 264, 98, 1193, 557, 1087, 1003, 240, 1156, 994, 722, 449, 1102, 1199, 897, 982, 448, 539, 1125, 66, 1141, 731, 1, 196, 407, 1106, 999, 534, 911, 951, 375, 879, 825, 668, 1019, 1194, 759, 912, 606, 55, 340, 918, 191, 180, 286, 28, 1213, 148, 278, 1165, 909, 593, 953, 1174, 433, 783, 943, 1073, 275, 177, 686, 1001, 147, 421, 510, 96, 1104, 1096, 662, 638, 108, 1110, 816, 254, 704, 403, 1086, 83, 202, 405, 80, 358, 110, 545, 920, 624, 402, 364, 1040, 484, 408, 605, 126, 1090, 362, 3, 1151, 290, 983, 445, 74, 1055, 688, 705, 820, 132, 684, 211, 1191, 530, 1168, 1163, 282, 23, 1162, 470, 1036, 998, 604, 1171, 296, 513, 319, 652, 664, 224, 466, 226, 660, 283, 891, 219, 494, 711, 1124, 988, 970, 969, 954, 657, 524, 712, 336, 467, 1048, 787, 245, 615, 808, 1015, 961, 784, 1002, 102, 860, 184, 779, 788, 253, 1044, 973, 632, 640, 422, 1009, 60, 1123, 1039, 182, 984, 141, 858, 1088, 676, 302, 156, 525, 856, 696, 876, 917, 1031, 1064, 204, 280, 851, 694, 1147, 186, 520, 613, 380, 423, 481, 1173, 964, 309, 82, 873, 1145, 554, 79, 655, 1121, 753, 1130, 409, 272, 537, 639, 595, 810, 823, 256, 462, 1210, 295, 1017, 777, 1132, 907, 774, 237, 952, 25, 288, 884, 164, 1054, 1071, 201, 37, 757, 140, 258, 745, 1016, 1177, 847, 142, 77, 1202, 656, 859, 418, 1148, 739, 443, 717, 222, 594, 602, 544, 1061, 507, 46, 995, 1085, 1068, 949, 179, 567, 805, 910, 170, 269, 412, 1127, 435, 14, 347, 930, 277, 249, 1091, 679, 244, 318, 1008, 128, 56, 395, 913, 360, 651, 150, 832, 33]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  989689467638119
the save name prefix for this run is:  chkpt-ID_989689467638119_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 291
rank avg (pred): 0.447 +- 0.006
mrr vals (pred, true): 0.000, 0.093
batch losses (mrrl, rdl): 0.0, 0.0029135696

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 350
rank avg (pred): 0.386 +- 0.310
mrr vals (pred, true): 0.110, 0.071
batch losses (mrrl, rdl): 0.0, 0.0008945722

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 516
rank avg (pred): 0.185 +- 0.251
mrr vals (pred, true): 0.156, 0.050
batch losses (mrrl, rdl): 0.0, 1.55357e-05

Epoch over!
epoch time: 12.622

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 547
rank avg (pred): 0.187 +- 0.252
mrr vals (pred, true): 0.156, 0.065
batch losses (mrrl, rdl): 0.0, 7.108e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1065
rank avg (pred): 0.016 +- 0.023
mrr vals (pred, true): 0.250, 0.275
batch losses (mrrl, rdl): 0.0, 1.10872e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 97
rank avg (pred): 0.364 +- 0.317
mrr vals (pred, true): 0.155, 0.019
batch losses (mrrl, rdl): 0.0, 0.0003197464

Epoch over!
epoch time: 11.876

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1172
rank avg (pred): 0.373 +- 0.319
mrr vals (pred, true): 0.148, 0.037
batch losses (mrrl, rdl): 0.0, 4.06404e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 94
rank avg (pred): 0.349 +- 0.312
mrr vals (pred, true): 0.149, 0.020
batch losses (mrrl, rdl): 0.0, 0.0002497403

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1020
rank avg (pred): 0.336 +- 0.311
mrr vals (pred, true): 0.157, 0.088
batch losses (mrrl, rdl): 0.0, 0.0004342834

Epoch over!
epoch time: 11.932

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 665
rank avg (pred): 0.379 +- 0.322
mrr vals (pred, true): 0.169, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001765827

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 659
rank avg (pred): 0.384 +- 0.324
mrr vals (pred, true): 0.160, 0.007
batch losses (mrrl, rdl): 0.0, 0.0001791239

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 879
rank avg (pred): 0.362 +- 0.319
mrr vals (pred, true): 0.167, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002906121

Epoch over!
epoch time: 11.86

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 638
rank avg (pred): 0.371 +- 0.320
mrr vals (pred, true): 0.189, 0.032
batch losses (mrrl, rdl): 0.0, 4.81074e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 233
rank avg (pred): 0.324 +- 0.305
mrr vals (pred, true): 0.205, 0.006
batch losses (mrrl, rdl): 0.0, 0.0004343895

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 789
rank avg (pred): 0.360 +- 0.318
mrr vals (pred, true): 0.183, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002602786

Epoch over!
epoch time: 11.827

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1195
rank avg (pred): 0.410 +- 0.327
mrr vals (pred, true): 0.151, 0.006
batch losses (mrrl, rdl): 0.1027282402, 9.11896e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 91
rank avg (pred): 0.364 +- 0.186
mrr vals (pred, true): 0.048, 0.020
batch losses (mrrl, rdl): 4.09998e-05, 0.0003839369

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 224
rank avg (pred): 0.302 +- 0.185
mrr vals (pred, true): 0.064, 0.006
batch losses (mrrl, rdl): 0.0020933992, 0.0006302767

Epoch over!
epoch time: 12.27

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 677
rank avg (pred): 0.434 +- 0.252
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 8.33201e-05, 3.78537e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 431
rank avg (pred): 0.247 +- 0.179
mrr vals (pred, true): 0.063, 0.006
batch losses (mrrl, rdl): 0.0018170709, 0.0010950832

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 33
rank avg (pred): 0.153 +- 0.180
mrr vals (pred, true): 0.083, 0.066
batch losses (mrrl, rdl): 0.0107000247, 8.3611e-05

Epoch over!
epoch time: 11.988

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 873
rank avg (pred): 0.393 +- 0.231
mrr vals (pred, true): 0.055, 0.006
batch losses (mrrl, rdl): 0.0002740095, 0.0001536225

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 129
rank avg (pred): 0.388 +- 0.219
mrr vals (pred, true): 0.050, 0.015
batch losses (mrrl, rdl): 2.942e-07, 0.0004129109

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 420
rank avg (pred): 0.376 +- 0.194
mrr vals (pred, true): 0.050, 0.007
batch losses (mrrl, rdl): 1.6965e-06, 0.000215443

Epoch over!
epoch time: 11.931

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1055
rank avg (pred): 0.117 +- 0.178
mrr vals (pred, true): 0.214, 0.251
batch losses (mrrl, rdl): 0.013888781, 0.0001718806

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.218 +- 0.215
mrr vals (pred, true): 0.090, 0.007
batch losses (mrrl, rdl): 0.0160754025, 0.0011835526

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 879
rank avg (pred): 0.365 +- 0.201
mrr vals (pred, true): 0.051, 0.007
batch losses (mrrl, rdl): 1.95717e-05, 0.0003111909

Epoch over!
epoch time: 11.874

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 654
rank avg (pred): 0.400 +- 0.255
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.0001687267, 0.0001148356

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 578
rank avg (pred): 0.408 +- 0.305
mrr vals (pred, true): 0.048, 0.034
batch losses (mrrl, rdl): 3.74174e-05, 0.000107867

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 952
rank avg (pred): 0.364 +- 0.295
mrr vals (pred, true): 0.056, 0.007
batch losses (mrrl, rdl): 0.0004208678, 0.000336356

Epoch over!
epoch time: 12.001

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1080
rank avg (pred): 0.327 +- 0.318
mrr vals (pred, true): 0.061, 0.073
batch losses (mrrl, rdl): 0.0013058818, 0.0002427387

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 894
rank avg (pred): 0.316 +- 0.246
mrr vals (pred, true): 0.049, 0.085
batch losses (mrrl, rdl): 1.68784e-05, 0.0012324393

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 974
rank avg (pred): 0.017 +- 0.020
mrr vals (pred, true): 0.253, 0.255
batch losses (mrrl, rdl): 2.69104e-05, 1.30354e-05

Epoch over!
epoch time: 12.346

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 257
rank avg (pred): 0.071 +- 0.147
mrr vals (pred, true): 0.177, 0.242
batch losses (mrrl, rdl): 0.0425984897, 1.32434e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1092
rank avg (pred): 0.286 +- 0.269
mrr vals (pred, true): 0.053, 0.106
batch losses (mrrl, rdl): 0.0281502418, 0.0002210488

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 682
rank avg (pred): 0.406 +- 0.273
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 8.55456e-05, 0.0001641639

Epoch over!
epoch time: 12.035

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 455
rank avg (pred): 0.342 +- 0.361
mrr vals (pred, true): 0.063, 0.006
batch losses (mrrl, rdl): 0.0016283514, 0.0004766109

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 267
rank avg (pred): 0.139 +- 0.099
mrr vals (pred, true): 0.070, 0.157
batch losses (mrrl, rdl): 0.0760083199, 8.13873e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.290 +- 0.299
mrr vals (pred, true): 0.056, 0.007
batch losses (mrrl, rdl): 0.0003033763, 0.0007814599

Epoch over!
epoch time: 12.174

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 331
rank avg (pred): 0.291 +- 0.256
mrr vals (pred, true): 0.053, 0.028
batch losses (mrrl, rdl): 0.0001098741, 3.87919e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 452
rank avg (pred): 0.304 +- 0.339
mrr vals (pred, true): 0.076, 0.006
batch losses (mrrl, rdl): 0.0066212583, 0.0008014796

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 129
rank avg (pred): 0.422 +- 0.290
mrr vals (pred, true): 0.049, 0.015
batch losses (mrrl, rdl): 9.5623e-06, 0.0004189315

Epoch over!
epoch time: 12.032

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1179
rank avg (pred): 0.352 +- 0.276
mrr vals (pred, true): 0.054, 0.036
batch losses (mrrl, rdl): 0.0001721214, 1.6171e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 8
rank avg (pred): 0.088 +- 0.185
mrr vals (pred, true): 0.147, 0.153
batch losses (mrrl, rdl): 0.0003366534, 2.22139e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 598
rank avg (pred): 0.421 +- 0.283
mrr vals (pred, true): 0.057, 0.011
batch losses (mrrl, rdl): 0.000531908, 6.9347e-05

Epoch over!
epoch time: 12.038

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.349 +- 0.242
mrr vals (pred, true): 0.051, 0.032

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   59 	     0 	 0.05450 	 0.00572 	 m..s
   29 	     1 	 0.05115 	 0.00583 	 m..s
   13 	     2 	 0.04969 	 0.00610 	 m..s
   50 	     3 	 0.05301 	 0.00615 	 m..s
   34 	     4 	 0.05143 	 0.00616 	 m..s
   55 	     5 	 0.05352 	 0.00620 	 m..s
   86 	     6 	 0.06564 	 0.00625 	 m..s
   62 	     7 	 0.05500 	 0.00630 	 m..s
   24 	     8 	 0.05069 	 0.00631 	 m..s
    1 	     9 	 0.04832 	 0.00633 	 m..s
   70 	    10 	 0.05735 	 0.00634 	 m..s
   10 	    11 	 0.04924 	 0.00635 	 m..s
   61 	    12 	 0.05498 	 0.00635 	 m..s
   37 	    13 	 0.05181 	 0.00636 	 m..s
   47 	    14 	 0.05230 	 0.00641 	 m..s
   25 	    15 	 0.05082 	 0.00643 	 m..s
   57 	    16 	 0.05395 	 0.00644 	 m..s
    2 	    17 	 0.04835 	 0.00645 	 m..s
   45 	    18 	 0.05227 	 0.00645 	 m..s
    9 	    19 	 0.04892 	 0.00645 	 m..s
   18 	    20 	 0.05011 	 0.00647 	 m..s
   65 	    21 	 0.05641 	 0.00648 	 m..s
   17 	    22 	 0.04996 	 0.00648 	 m..s
   78 	    23 	 0.06000 	 0.00650 	 m..s
   68 	    24 	 0.05701 	 0.00653 	 m..s
   32 	    25 	 0.05121 	 0.00654 	 m..s
   15 	    26 	 0.04991 	 0.00655 	 m..s
   64 	    27 	 0.05599 	 0.00656 	 m..s
   71 	    28 	 0.05741 	 0.00656 	 m..s
    3 	    29 	 0.04848 	 0.00659 	 m..s
   69 	    30 	 0.05719 	 0.00660 	 m..s
    6 	    31 	 0.04857 	 0.00660 	 m..s
   30 	    32 	 0.05116 	 0.00660 	 m..s
   52 	    33 	 0.05330 	 0.00663 	 m..s
    7 	    34 	 0.04867 	 0.00673 	 m..s
   72 	    35 	 0.05744 	 0.00680 	 m..s
   58 	    36 	 0.05417 	 0.00961 	 m..s
   26 	    37 	 0.05106 	 0.00967 	 m..s
    8 	    38 	 0.04882 	 0.00967 	 m..s
   12 	    39 	 0.04952 	 0.01084 	 m..s
   51 	    40 	 0.05328 	 0.01138 	 m..s
   23 	    41 	 0.05061 	 0.01263 	 m..s
   22 	    42 	 0.05043 	 0.01320 	 m..s
    4 	    43 	 0.04852 	 0.01619 	 m..s
   42 	    44 	 0.05221 	 0.01653 	 m..s
    5 	    45 	 0.04856 	 0.01709 	 m..s
   20 	    46 	 0.05031 	 0.01857 	 m..s
   19 	    47 	 0.05015 	 0.01965 	 m..s
   21 	    48 	 0.05037 	 0.02023 	 m..s
   36 	    49 	 0.05171 	 0.02048 	 m..s
   40 	    50 	 0.05203 	 0.02118 	 m..s
   41 	    51 	 0.05206 	 0.02143 	 m..s
   46 	    52 	 0.05227 	 0.02179 	 m..s
   35 	    53 	 0.05170 	 0.02238 	 ~...
    0 	    54 	 0.04726 	 0.02420 	 ~...
   56 	    55 	 0.05382 	 0.02499 	 ~...
   54 	    56 	 0.05352 	 0.02805 	 ~...
   27 	    57 	 0.05107 	 0.03177 	 ~...
   43 	    58 	 0.05222 	 0.03485 	 ~...
   28 	    59 	 0.05113 	 0.03489 	 ~...
   44 	    60 	 0.05223 	 0.03492 	 ~...
   38 	    61 	 0.05186 	 0.03738 	 ~...
   87 	    62 	 0.06611 	 0.03893 	 ~...
   16 	    63 	 0.04991 	 0.04121 	 ~...
   53 	    64 	 0.05335 	 0.04185 	 ~...
   14 	    65 	 0.04973 	 0.04205 	 ~...
   79 	    66 	 0.06001 	 0.04907 	 ~...
   76 	    67 	 0.05898 	 0.05157 	 ~...
   74 	    68 	 0.05831 	 0.05164 	 ~...
   80 	    69 	 0.06002 	 0.05180 	 ~...
   48 	    70 	 0.05273 	 0.05180 	 ~...
  100 	    71 	 0.09779 	 0.05382 	 m..s
   75 	    72 	 0.05835 	 0.05557 	 ~...
   33 	    73 	 0.05127 	 0.05852 	 ~...
   39 	    74 	 0.05188 	 0.05911 	 ~...
   73 	    75 	 0.05763 	 0.05962 	 ~...
   84 	    76 	 0.06333 	 0.05962 	 ~...
   11 	    77 	 0.04940 	 0.06589 	 ~...
   77 	    78 	 0.05922 	 0.06624 	 ~...
   85 	    79 	 0.06379 	 0.06701 	 ~...
   60 	    80 	 0.05465 	 0.06935 	 ~...
   81 	    81 	 0.06054 	 0.06963 	 ~...
   95 	    82 	 0.09016 	 0.06971 	 ~...
   82 	    83 	 0.06121 	 0.07875 	 ~...
   89 	    84 	 0.07401 	 0.07911 	 ~...
   67 	    85 	 0.05689 	 0.09122 	 m..s
   97 	    86 	 0.09559 	 0.09161 	 ~...
   83 	    87 	 0.06142 	 0.09192 	 m..s
   92 	    88 	 0.08032 	 0.09434 	 ~...
   93 	    89 	 0.08064 	 0.09486 	 ~...
   91 	    90 	 0.07926 	 0.09544 	 ~...
   63 	    91 	 0.05503 	 0.09614 	 m..s
  109 	    92 	 0.13071 	 0.09631 	 m..s
   31 	    93 	 0.05119 	 0.09646 	 m..s
   49 	    94 	 0.05281 	 0.09673 	 m..s
   66 	    95 	 0.05673 	 0.09691 	 m..s
   94 	    96 	 0.08807 	 0.09780 	 ~...
   88 	    97 	 0.07230 	 0.10384 	 m..s
  102 	    98 	 0.10183 	 0.10441 	 ~...
  105 	    99 	 0.11034 	 0.10534 	 ~...
  103 	   100 	 0.10812 	 0.10611 	 ~...
  106 	   101 	 0.11463 	 0.10675 	 ~...
   96 	   102 	 0.09312 	 0.10791 	 ~...
   90 	   103 	 0.07555 	 0.11240 	 m..s
  111 	   104 	 0.14159 	 0.12209 	 ~...
  110 	   105 	 0.13722 	 0.12712 	 ~...
  101 	   106 	 0.10011 	 0.12784 	 ~...
   98 	   107 	 0.09725 	 0.14405 	 m..s
  104 	   108 	 0.10923 	 0.14613 	 m..s
   99 	   109 	 0.09745 	 0.14815 	 m..s
  115 	   110 	 0.17233 	 0.14871 	 ~...
  112 	   111 	 0.14335 	 0.15947 	 ~...
  114 	   112 	 0.16631 	 0.19103 	 ~...
  113 	   113 	 0.16591 	 0.19480 	 ~...
  108 	   114 	 0.13007 	 0.20470 	 m..s
  107 	   115 	 0.12182 	 0.20882 	 m..s
  116 	   116 	 0.24396 	 0.24199 	 ~...
  117 	   117 	 0.26163 	 0.26716 	 ~...
  118 	   118 	 0.26349 	 0.27652 	 ~...
  120 	   119 	 0.29361 	 0.28694 	 ~...
  119 	   120 	 0.27319 	 0.30150 	 ~...
==========================================
r_mrr = 0.901030421257019
r2_mrr = 0.7295335531234741
spearmanr_mrr@5 = 0.9714183211326599
spearmanr_mrr@10 = 0.9773082733154297
spearmanr_mrr@50 = 0.9865545630455017
spearmanr_mrr@100 = 0.9487983584403992
spearmanr_mrr@All = 0.9457086324691772
==========================================
test time: 0.392
Done Testing dataset OpenEA
total time taken: 197.4927260875702
training time taken: 181.28322315216064
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.9010)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7295)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9714)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9773)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9866)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9488)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9457)}}, 'test_loss': {'TransE': {'OpenEA': 0.48957588100165594}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 1019067770769621
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [160, 350, 301, 1082, 833, 1075, 773, 1163, 679, 71, 1099, 1001, 886, 355, 934, 827, 478, 457, 85, 980, 405, 1199, 1045, 1016, 745, 343, 634, 183, 1151, 241, 490, 139, 432, 881, 1116, 628, 492, 427, 800, 593, 1019, 522, 671, 565, 33, 320, 844, 1043, 166, 832, 345, 871, 186, 597, 6, 37, 15, 931, 283, 31, 36, 1214, 809, 613, 209, 333, 863, 5, 1169, 92, 596, 911, 625, 292, 686, 505, 1106, 1192, 1112, 471, 440, 504, 996, 841, 976, 280, 539, 586, 563, 247, 542, 375, 133, 1000, 1046, 2, 421, 117, 402, 211, 867, 100, 38, 795, 746, 188, 822, 319, 772, 232, 922, 304, 1061, 1160, 954, 511, 792, 296, 313, 973, 972]
valid_ids (0): []
train_ids (1094): [153, 129, 668, 59, 837, 239, 875, 155, 1170, 1053, 1107, 210, 857, 681, 299, 459, 644, 159, 851, 770, 606, 664, 226, 620, 967, 474, 62, 847, 1135, 873, 790, 1113, 1104, 363, 461, 879, 534, 760, 1042, 912, 13, 852, 122, 936, 1130, 932, 588, 1052, 380, 271, 739, 47, 791, 257, 683, 592, 1209, 14, 164, 723, 710, 57, 1057, 450, 484, 455, 660, 674, 755, 263, 937, 463, 659, 214, 16, 479, 887, 506, 696, 639, 849, 1054, 466, 584, 317, 332, 680, 602, 717, 741, 112, 882, 147, 344, 958, 477, 916, 158, 802, 530, 1109, 60, 284, 44, 884, 915, 751, 142, 486, 1050, 90, 1136, 354, 180, 148, 818, 276, 720, 1080, 1012, 558, 295, 688, 1133, 538, 338, 961, 356, 322, 665, 369, 1028, 771, 902, 1097, 305, 1068, 785, 1171, 1069, 248, 1131, 149, 892, 234, 677, 1184, 870, 562, 581, 782, 693, 26, 673, 655, 497, 1188, 987, 56, 279, 262, 874, 437, 22, 105, 803, 494, 716, 124, 174, 964, 499, 1147, 616, 136, 1100, 401, 1206, 204, 704, 945, 327, 220, 1128, 788, 985, 1180, 362, 1140, 315, 1166, 860, 167, 889, 191, 430, 896, 443, 955, 1093, 982, 981, 288, 780, 104, 1005, 168, 289, 811, 373, 1181, 152, 1033, 1048, 151, 53, 446, 491, 109, 742, 311, 781, 1123, 1146, 144, 1179, 907, 715, 743, 68, 260, 1105, 607, 970, 971, 8, 685, 1134, 901, 465, 551, 767, 118, 424, 274, 1201, 485, 829, 618, 605, 670, 258, 520, 1111, 583, 73, 146, 27, 225, 456, 98, 571, 794, 173, 946, 603, 267, 859, 687, 1035, 483, 331, 748, 691, 163, 959, 406, 30, 4, 357, 1013, 556, 784, 652, 589, 502, 787, 302, 177, 552, 1009, 1145, 645, 595, 546, 582, 796, 1132, 1120, 400, 736, 370, 196, 1154, 66, 776, 10, 452, 950, 1143, 49, 389, 858, 72, 481, 222, 1090, 540, 713, 1190, 1194, 268, 730, 75, 281, 391, 1176, 866, 1153, 992, 666, 434, 507, 270, 734, 995, 1167, 470, 761, 451, 162, 84, 642, 557, 87, 115, 65, 678, 205, 238, 880, 106, 726, 850, 498, 1186, 952, 1138, 891, 960, 190, 839, 918, 744, 690, 469, 138, 573, 116, 1081, 903, 19, 610, 909, 899, 58, 994, 7, 1011, 309, 777, 633, 468, 990, 1165, 462, 291, 1007, 612, 853, 207, 926, 622, 64, 108, 855, 448, 1018, 399, 1193, 414, 699, 436, 169, 667, 12, 1095, 1115, 500, 754, 388, 1110, 826, 41, 366, 88, 1213, 614, 553, 1020, 272, 949, 1041, 1197, 778, 1114, 298, 18, 365, 1049, 817, 352, 273, 890, 398, 371, 286, 29, 544, 1051, 250, 442, 569, 783, 404, 55, 384, 185, 988, 905, 348, 176, 962, 1155, 445, 1141, 351, 646, 547, 480, 413, 856, 753, 578, 335, 213, 626, 577, 925, 1148, 545, 339, 1059, 719, 711, 775, 113, 476, 1039, 227, 923, 762, 42, 135, 733, 1210, 1078, 805, 103, 181, 361, 554, 750, 658, 527, 1094, 82, 983, 623, 801, 300, 906, 812, 130, 526, 1108, 132, 641, 816, 842, 813, 1067, 287, 587, 728, 245, 640, 221, 194, 254, 897, 187, 1092, 215, 23, 835, 819, 525, 383, 1070, 1152, 150, 1064, 297, 314, 757, 920, 650, 458, 1119, 991, 797, 409, 269, 1162, 143, 316, 473, 1044, 516, 449, 460, 727, 1198, 255, 426, 156, 1077, 1062, 501, 865, 694, 868, 1, 428, 224, 714, 1177, 233, 904, 1004, 1126, 611, 524, 1172, 1030, 422, 823, 814, 97, 307, 1122, 845, 475, 360, 382, 464, 948, 1200, 165, 24, 749, 1182, 81, 561, 28, 854, 637, 528, 1003, 374, 769, 917, 549, 120, 1014, 878, 1204, 940, 698, 1002, 89, 649, 944, 86, 705, 953, 718, 764, 885, 123, 285, 708, 838, 95, 206, 1010, 512, 564, 1183, 377, 341, 80, 77, 386, 729, 393, 1084, 864, 1058, 536, 121, 198, 576, 199, 256, 1158, 1079, 417, 924, 806, 358, 786, 643, 579, 251, 326, 966, 703, 34, 560, 1060, 862, 707, 676, 69, 1159, 941, 763, 631, 1164, 487, 555, 178, 692, 193, 259, 913, 653, 189, 1187, 325, 243, 514, 48, 1023, 395, 342, 131, 101, 372, 725, 532, 495, 240, 977, 482, 519, 630, 935, 947, 700, 444, 963, 489, 110, 282, 175, 454, 94, 965, 294, 624, 1125, 535, 1137, 140, 550, 201, 598, 548, 779, 1096, 735, 789, 933, 1073, 1161, 1025, 46, 410, 793, 594, 1017, 721, 376, 340, 1142, 392, 747, 1174, 337, 419, 346, 919, 689, 647, 617, 651, 52, 575, 831, 1040, 321, 78, 230, 848, 537, 1006, 208, 1091, 111, 888, 367, 893, 407, 1211, 252, 910, 657, 425, 394, 894, 431, 91, 408, 615, 218, 1196, 834, 799, 323, 161, 43, 1089, 1034, 810, 11, 423, 1212, 228, 1118, 975, 35, 695, 861, 998, 574, 197, 1029, 1008, 1031, 590, 654, 277, 1063, 508, 334, 938, 202, 1207, 264, 1027, 70, 956, 608, 128, 1083, 808, 599, 137, 1117, 979, 876, 621, 3, 828, 1121, 521, 722, 93, 1032, 518, 39, 927, 541, 349, 566, 182, 1074, 840, 877, 737, 821, 636, 672, 1098, 438, 732, 435, 830, 968, 1127, 1157, 114, 1102, 1072, 766, 872, 107, 51, 1066, 330, 1024, 984, 195, 266, 324, 364, 9, 824, 869, 1071, 702, 381, 74, 219, 125, 669, 1056, 416, 242, 154, 1205, 756, 290, 663, 411, 591, 278, 758, 986, 768, 1055, 900, 433, 1202, 914, 820, 568, 804, 40, 930, 418, 1185, 368, 303, 533, 701, 509, 712, 898, 632, 157, 635, 928, 403, 1021, 1087, 619, 825, 385, 223, 629, 738, 570, 1036, 503, 308, 1065, 774, 353, 724, 1175, 310, 908, 706, 1037, 1088, 61, 172, 96, 1173, 974, 329, 453, 604, 192, 1101, 1208, 212, 648, 387, 656, 265, 1149, 543, 531, 993, 969, 63, 429, 336, 883, 249, 510, 179, 347, 1085, 127, 559, 1015, 1178, 517, 929, 759, 21, 32, 1195, 99, 25, 378, 217, 921, 79, 76, 203, 390, 50, 1129, 493, 661, 798, 45, 415, 1124, 740, 638, 957, 1026, 601, 752, 1189, 978, 1139, 1076, 229, 496, 807, 293, 585, 1144, 0, 126, 609, 765, 200, 600, 328, 441, 627, 662, 379, 1047, 216, 306, 261, 1203, 709, 989, 467, 846, 119, 244, 1086, 231, 529, 682, 939, 253, 237, 236, 312, 1168, 397, 141, 447, 580, 235, 675, 951, 1191, 246, 697, 943, 942, 184, 572, 83, 170, 420, 997, 134, 1156, 815, 513, 567, 515, 67, 275, 684, 1022, 20, 102, 396, 54, 412, 836, 1150, 359, 171, 999, 472, 318, 1038, 731, 17, 843, 439, 488, 895, 523, 1103, 145]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6496665747151938
the save name prefix for this run is:  chkpt-ID_6496665747151938_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 664
rank avg (pred): 0.412 +- 0.003
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002464143

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 696
rank avg (pred): 0.414 +- 0.208
mrr vals (pred, true): 0.001, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001124377

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.335 +- 0.215
mrr vals (pred, true): 0.207, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003462531

Epoch over!
epoch time: 11.975

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 779
rank avg (pred): 0.347 +- 0.233
mrr vals (pred, true): 0.233, 0.097
batch losses (mrrl, rdl): 0.0, 0.0002426739

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 110
rank avg (pred): 0.318 +- 0.257
mrr vals (pred, true): 0.294, 0.036
batch losses (mrrl, rdl): 0.0, 0.000349658

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 930
rank avg (pred): 0.380 +- 0.321
mrr vals (pred, true): 0.316, 0.096
batch losses (mrrl, rdl): 0.0, 0.0003944859

Epoch over!
epoch time: 11.891

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 997
rank avg (pred): 0.047 +- 0.041
mrr vals (pred, true): 0.353, 0.288
batch losses (mrrl, rdl): 0.0, 5.9692e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1137
rank avg (pred): 0.141 +- 0.124
mrr vals (pred, true): 0.342, 0.108
batch losses (mrrl, rdl): 0.0, 5.04695e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 685
rank avg (pred): 0.335 +- 0.303
mrr vals (pred, true): 0.356, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002832152

Epoch over!
epoch time: 11.74

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 721
rank avg (pred): 0.349 +- 0.314
mrr vals (pred, true): 0.353, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001982865

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 728
rank avg (pred): 0.347 +- 0.309
mrr vals (pred, true): 0.345, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002357118

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 627
rank avg (pred): 0.345 +- 0.316
mrr vals (pred, true): 0.356, 0.009
batch losses (mrrl, rdl): 0.0, 1.68731e-05

Epoch over!
epoch time: 11.732

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1097
rank avg (pred): 0.290 +- 0.273
mrr vals (pred, true): 0.365, 0.112
batch losses (mrrl, rdl): 0.0, 0.0004262731

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 262
rank avg (pred): 0.061 +- 0.056
mrr vals (pred, true): 0.371, 0.208
batch losses (mrrl, rdl): 0.0, 1.01931e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 726
rank avg (pred): 0.401 +- 0.344
mrr vals (pred, true): 0.321, 0.006
batch losses (mrrl, rdl): 0.0, 8.28144e-05

Epoch over!
epoch time: 11.777

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1207
rank avg (pred): 0.350 +- 0.311
mrr vals (pred, true): 0.333, 0.007
batch losses (mrrl, rdl): 0.8011473417, 0.0002039311

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 567
rank avg (pred): 0.407 +- 0.211
mrr vals (pred, true): 0.049, 0.009
batch losses (mrrl, rdl): 1.42153e-05, 9.59481e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 667
rank avg (pred): 0.470 +- 0.190
mrr vals (pred, true): 0.043, 0.006
batch losses (mrrl, rdl): 0.0004334542, 4.70609e-05

Epoch over!
epoch time: 12.24

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 442
rank avg (pred): 0.437 +- 0.212
mrr vals (pred, true): 0.061, 0.006
batch losses (mrrl, rdl): 0.0011737582, 5.67764e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 526
rank avg (pred): 0.340 +- 0.209
mrr vals (pred, true): 0.067, 0.078
batch losses (mrrl, rdl): 0.0027716949, 0.0005430088

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 846
rank avg (pred): 0.462 +- 0.180
mrr vals (pred, true): 0.045, 0.095
batch losses (mrrl, rdl): 0.0002461883, 0.0009092185

Epoch over!
epoch time: 11.907

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 469
rank avg (pred): 0.436 +- 0.196
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 8.046e-07, 7.06847e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1010
rank avg (pred): 0.367 +- 0.219
mrr vals (pred, true): 0.078, 0.057
batch losses (mrrl, rdl): 0.0077238423, 0.0006851867

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 265
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.172, 0.198
batch losses (mrrl, rdl): 0.0065147411, 7.93072e-05

Epoch over!
epoch time: 11.987

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 884
rank avg (pred): 0.452 +- 0.157
mrr vals (pred, true): 0.039, 0.007
batch losses (mrrl, rdl): 0.0011456787, 7.87589e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 282
rank avg (pred): 0.170 +- 0.116
mrr vals (pred, true): 0.084, 0.089
batch losses (mrrl, rdl): 0.0116046974, 0.0001497926

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1181
rank avg (pred): 0.381 +- 0.194
mrr vals (pred, true): 0.058, 0.034
batch losses (mrrl, rdl): 0.0006902145, 0.0001536037

Epoch over!
epoch time: 12.071

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1036
rank avg (pred): 0.364 +- 0.201
mrr vals (pred, true): 0.064, 0.006
batch losses (mrrl, rdl): 0.0018583944, 0.000257467

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 502
rank avg (pred): 0.357 +- 0.199
mrr vals (pred, true): 0.062, 0.079
batch losses (mrrl, rdl): 0.0014535415, 0.0006200982

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 167
rank avg (pred): 0.387 +- 0.174
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 0.0001048452, 0.0002128507

Epoch over!
epoch time: 12.016

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 212
rank avg (pred): 0.363 +- 0.189
mrr vals (pred, true): 0.062, 0.007
batch losses (mrrl, rdl): 0.0013795584, 0.0002719117

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 716
rank avg (pred): 0.393 +- 0.159
mrr vals (pred, true): 0.055, 0.006
batch losses (mrrl, rdl): 0.0002440694, 0.000225199

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.315 +- 0.195
mrr vals (pred, true): 0.079, 0.106
batch losses (mrrl, rdl): 0.0072394563, 0.0005035374

Epoch over!
epoch time: 12.03

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 938
rank avg (pred): 0.408 +- 0.137
mrr vals (pred, true): 0.045, 0.099
batch losses (mrrl, rdl): 0.0002781926, 0.0005126399

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 154
rank avg (pred): 0.371 +- 0.169
mrr vals (pred, true): 0.063, 0.020
batch losses (mrrl, rdl): 0.0017974613, 0.0005044279

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 382
rank avg (pred): 0.389 +- 0.138
mrr vals (pred, true): 0.046, 0.028
batch losses (mrrl, rdl): 0.000179151, 0.0005657877

Epoch over!
epoch time: 12.162

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1153
rank avg (pred): 0.233 +- 0.153
mrr vals (pred, true): 0.085, 0.106
batch losses (mrrl, rdl): 0.0043356731, 0.000191279

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 109
rank avg (pred): 0.370 +- 0.137
mrr vals (pred, true): 0.050, 0.017
batch losses (mrrl, rdl): 5.13e-07, 0.0003390126

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 975
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.278, 0.278
batch losses (mrrl, rdl): 9.8e-09, 2.77597e-05

Epoch over!
epoch time: 12.1

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 125
rank avg (pred): 0.354 +- 0.149
mrr vals (pred, true): 0.054, 0.040
batch losses (mrrl, rdl): 0.000136755, 0.0005617798

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 839
rank avg (pred): 0.382 +- 0.116
mrr vals (pred, true): 0.043, 0.096
batch losses (mrrl, rdl): 0.0004961739, 0.0003662065

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 715
rank avg (pred): 0.380 +- 0.107
mrr vals (pred, true): 0.041, 0.006
batch losses (mrrl, rdl): 0.000895584, 0.0003598668

Epoch over!
epoch time: 12.089

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1133
rank avg (pred): 0.305 +- 0.178
mrr vals (pred, true): 0.108, 0.006
batch losses (mrrl, rdl): 0.0335846879, 0.0006452283

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 919
rank avg (pred): 0.362 +- 0.121
mrr vals (pred, true): 0.050, 0.096
batch losses (mrrl, rdl): 1.1184e-06, 0.000282977

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 604
rank avg (pred): 0.379 +- 0.108
mrr vals (pred, true): 0.042, 0.014
batch losses (mrrl, rdl): 0.0006258735, 0.000128906

Epoch over!
epoch time: 12.139

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.343 +- 0.132
mrr vals (pred, true): 0.053, 0.022

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   49 	     0 	 0.05258 	 0.00575 	 m..s
   67 	     1 	 0.05856 	 0.00577 	 m..s
   71 	     2 	 0.06113 	 0.00597 	 m..s
   28 	     3 	 0.04813 	 0.00600 	 m..s
   20 	     4 	 0.04665 	 0.00600 	 m..s
   11 	     5 	 0.04559 	 0.00619 	 m..s
   36 	     6 	 0.05066 	 0.00623 	 m..s
   25 	     7 	 0.04742 	 0.00623 	 m..s
   48 	     8 	 0.05247 	 0.00630 	 m..s
   46 	     9 	 0.05224 	 0.00633 	 m..s
   12 	    10 	 0.04566 	 0.00635 	 m..s
   61 	    11 	 0.05720 	 0.00637 	 m..s
    8 	    12 	 0.04478 	 0.00639 	 m..s
   33 	    13 	 0.04987 	 0.00639 	 m..s
    6 	    14 	 0.04335 	 0.00643 	 m..s
   50 	    15 	 0.05259 	 0.00644 	 m..s
   35 	    16 	 0.05026 	 0.00646 	 m..s
   63 	    17 	 0.05782 	 0.00649 	 m..s
   29 	    18 	 0.04817 	 0.00651 	 m..s
    1 	    19 	 0.04183 	 0.00651 	 m..s
   44 	    20 	 0.05214 	 0.00651 	 m..s
   13 	    21 	 0.04570 	 0.00651 	 m..s
   72 	    22 	 0.06172 	 0.00652 	 m..s
   16 	    23 	 0.04620 	 0.00654 	 m..s
   98 	    24 	 0.09587 	 0.00655 	 m..s
   62 	    25 	 0.05771 	 0.00655 	 m..s
   45 	    26 	 0.05218 	 0.00655 	 m..s
    9 	    27 	 0.04527 	 0.00659 	 m..s
   15 	    28 	 0.04619 	 0.00660 	 m..s
   69 	    29 	 0.06027 	 0.00660 	 m..s
   27 	    30 	 0.04807 	 0.00662 	 m..s
   19 	    31 	 0.04659 	 0.00664 	 m..s
   40 	    32 	 0.05110 	 0.00665 	 m..s
   37 	    33 	 0.05074 	 0.00668 	 m..s
   54 	    34 	 0.05292 	 0.00669 	 m..s
   53 	    35 	 0.05291 	 0.00678 	 m..s
    0 	    36 	 0.04136 	 0.00805 	 m..s
    3 	    37 	 0.04208 	 0.01120 	 m..s
    2 	    38 	 0.04192 	 0.01184 	 m..s
   23 	    39 	 0.04706 	 0.01328 	 m..s
    4 	    40 	 0.04240 	 0.01376 	 ~...
    7 	    41 	 0.04471 	 0.01451 	 m..s
   10 	    42 	 0.04537 	 0.01575 	 ~...
   24 	    43 	 0.04738 	 0.01619 	 m..s
   30 	    44 	 0.04828 	 0.01653 	 m..s
   52 	    45 	 0.05269 	 0.01989 	 m..s
   18 	    46 	 0.04652 	 0.02175 	 ~...
   57 	    47 	 0.05355 	 0.02179 	 m..s
   51 	    48 	 0.05267 	 0.02182 	 m..s
   34 	    49 	 0.05024 	 0.02327 	 ~...
   32 	    50 	 0.04945 	 0.02347 	 ~...
    5 	    51 	 0.04298 	 0.02393 	 ~...
   31 	    52 	 0.04944 	 0.02713 	 ~...
   14 	    53 	 0.04594 	 0.02799 	 ~...
   21 	    54 	 0.04699 	 0.03491 	 ~...
   41 	    55 	 0.05119 	 0.03581 	 ~...
   43 	    56 	 0.05213 	 0.03630 	 ~...
   42 	    57 	 0.05200 	 0.04039 	 ~...
   60 	    58 	 0.05715 	 0.04886 	 ~...
   76 	    59 	 0.06319 	 0.05240 	 ~...
   70 	    60 	 0.06075 	 0.05844 	 ~...
   59 	    61 	 0.05625 	 0.05959 	 ~...
   78 	    62 	 0.06416 	 0.05962 	 ~...
   73 	    63 	 0.06259 	 0.06178 	 ~...
   79 	    64 	 0.06809 	 0.06335 	 ~...
   80 	    65 	 0.07059 	 0.06559 	 ~...
   66 	    66 	 0.05854 	 0.06577 	 ~...
   65 	    67 	 0.05826 	 0.06846 	 ~...
   90 	    68 	 0.08037 	 0.06980 	 ~...
   58 	    69 	 0.05566 	 0.07065 	 ~...
   64 	    70 	 0.05797 	 0.07223 	 ~...
   83 	    71 	 0.07467 	 0.07296 	 ~...
   68 	    72 	 0.05909 	 0.07447 	 ~...
   87 	    73 	 0.07882 	 0.07721 	 ~...
   77 	    74 	 0.06363 	 0.08095 	 ~...
   97 	    75 	 0.08942 	 0.08267 	 ~...
   47 	    76 	 0.05232 	 0.08716 	 m..s
   82 	    77 	 0.07422 	 0.08823 	 ~...
   17 	    78 	 0.04629 	 0.09004 	 m..s
   26 	    79 	 0.04785 	 0.09121 	 m..s
  105 	    80 	 0.14945 	 0.09148 	 m..s
   84 	    81 	 0.07515 	 0.09161 	 ~...
   74 	    82 	 0.06267 	 0.09192 	 ~...
   91 	    83 	 0.08112 	 0.09279 	 ~...
   75 	    84 	 0.06278 	 0.09315 	 m..s
   81 	    85 	 0.07090 	 0.09330 	 ~...
   22 	    86 	 0.04700 	 0.09363 	 m..s
   55 	    87 	 0.05300 	 0.09456 	 m..s
   56 	    88 	 0.05307 	 0.09470 	 m..s
   39 	    89 	 0.05082 	 0.09498 	 m..s
   38 	    90 	 0.05077 	 0.09851 	 m..s
   99 	    91 	 0.09730 	 0.09922 	 ~...
   92 	    92 	 0.08540 	 0.10352 	 ~...
   86 	    93 	 0.07807 	 0.10629 	 ~...
  101 	    94 	 0.10532 	 0.10641 	 ~...
  102 	    95 	 0.10536 	 0.10937 	 ~...
   96 	    96 	 0.08779 	 0.12039 	 m..s
   93 	    97 	 0.08542 	 0.12726 	 m..s
   95 	    98 	 0.08720 	 0.13066 	 m..s
   94 	    99 	 0.08577 	 0.13100 	 m..s
   85 	   100 	 0.07795 	 0.14026 	 m..s
   88 	   101 	 0.07948 	 0.14137 	 m..s
   89 	   102 	 0.08004 	 0.14341 	 m..s
  100 	   103 	 0.09894 	 0.14474 	 m..s
  106 	   104 	 0.14958 	 0.15132 	 ~...
  110 	   105 	 0.17405 	 0.15848 	 ~...
  108 	   106 	 0.17267 	 0.16628 	 ~...
  109 	   107 	 0.17275 	 0.19480 	 ~...
  111 	   108 	 0.17852 	 0.20024 	 ~...
  103 	   109 	 0.12609 	 0.20882 	 m..s
  107 	   110 	 0.15088 	 0.21619 	 m..s
  104 	   111 	 0.14715 	 0.21706 	 m..s
  114 	   112 	 0.27445 	 0.24106 	 m..s
  116 	   113 	 0.27790 	 0.24897 	 ~...
  112 	   114 	 0.26365 	 0.27280 	 ~...
  115 	   115 	 0.27787 	 0.27598 	 ~...
  117 	   116 	 0.28099 	 0.27652 	 ~...
  113 	   117 	 0.26775 	 0.28145 	 ~...
  120 	   118 	 0.33320 	 0.28694 	 m..s
  119 	   119 	 0.30600 	 0.29220 	 ~...
  118 	   120 	 0.30519 	 0.29402 	 ~...
==========================================
r_mrr = 0.8973246216773987
r2_mrr = 0.7851747870445251
spearmanr_mrr@5 = 0.9196944832801819
spearmanr_mrr@10 = 0.910206139087677
spearmanr_mrr@50 = 0.9830583930015564
spearmanr_mrr@100 = 0.9399835467338562
spearmanr_mrr@All = 0.9385775327682495
==========================================
test time: 0.398
Done Testing dataset OpenEA
total time taken: 195.84969854354858
training time taken: 180.3372220993042
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8973)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7852)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9197)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9102)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9831)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9400)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9386)}}, 'test_loss': {'TransE': {'OpenEA': 0.7316985756115173}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 9756890460457694
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [118, 727, 639, 169, 259, 247, 650, 657, 72, 562, 1195, 527, 606, 755, 310, 897, 277, 773, 518, 55, 1174, 204, 389, 203, 322, 824, 1160, 156, 136, 278, 315, 430, 820, 1101, 514, 29, 1097, 90, 930, 1142, 705, 818, 48, 1041, 423, 669, 565, 242, 616, 808, 1007, 126, 1130, 387, 1086, 580, 291, 954, 386, 519, 442, 356, 956, 28, 747, 999, 678, 624, 468, 813, 1020, 655, 859, 778, 597, 1052, 757, 328, 195, 1107, 849, 871, 450, 838, 276, 429, 879, 321, 659, 708, 338, 487, 101, 1154, 147, 1067, 103, 544, 1050, 654, 378, 970, 721, 109, 236, 789, 68, 913, 979, 684, 553, 438, 69, 269, 548, 782, 666, 963, 500, 695, 807]
valid_ids (0): []
train_ids (1094): [1005, 941, 350, 369, 191, 225, 449, 841, 905, 903, 302, 100, 508, 1186, 167, 318, 1116, 288, 306, 298, 874, 792, 981, 368, 688, 626, 816, 482, 420, 216, 185, 31, 556, 969, 1188, 454, 448, 1037, 32, 73, 345, 647, 393, 637, 696, 98, 102, 407, 633, 81, 57, 947, 1162, 1057, 1197, 590, 681, 592, 750, 854, 480, 946, 17, 887, 717, 1091, 806, 1151, 991, 1025, 989, 348, 215, 467, 1208, 643, 890, 484, 940, 220, 533, 283, 1093, 382, 823, 319, 282, 1148, 111, 46, 35, 343, 326, 543, 926, 844, 300, 992, 962, 598, 998, 155, 119, 117, 235, 1083, 65, 993, 1158, 501, 836, 271, 437, 1213, 43, 1143, 1015, 296, 359, 309, 144, 62, 402, 3, 50, 812, 1136, 1017, 1121, 1126, 329, 741, 233, 686, 744, 183, 713, 948, 754, 724, 6, 244, 826, 769, 796, 740, 625, 748, 88, 324, 596, 292, 984, 738, 405, 392, 1056, 953, 172, 1040, 404, 106, 1094, 358, 868, 59, 486, 371, 312, 390, 40, 465, 495, 1103, 1210, 492, 896, 1157, 672, 1112, 629, 509, 21, 950, 157, 847, 139, 559, 478, 133, 327, 16, 75, 619, 209, 1042, 765, 253, 563, 1194, 339, 1178, 80, 477, 1019, 113, 837, 1198, 1135, 1193, 523, 84, 1085, 251, 410, 772, 479, 1184, 538, 1129, 289, 604, 572, 770, 603, 673, 682, 435, 884, 461, 421, 140, 973, 964, 829, 581, 781, 898, 928, 266, 1055, 174, 894, 614, 1137, 560, 463, 313, 93, 357, 452, 267, 293, 944, 9, 680, 228, 1082, 71, 424, 376, 489, 561, 189, 797, 192, 1024, 907, 960, 336, 609, 1045, 631, 380, 122, 4, 190, 995, 394, 317, 337, 888, 99, 63, 349, 95, 408, 436, 971, 586, 308, 763, 985, 911, 728, 1051, 1014, 952, 1204, 273, 86, 11, 187, 1199, 447, 720, 471, 163, 1000, 123, 112, 51, 297, 904, 397, 749, 360, 77, 497, 431, 74, 845, 257, 652, 311, 1190, 703, 1095, 158, 146, 1008, 882, 900, 1175, 131, 1073, 23, 8, 166, 937, 746, 188, 777, 355, 370, 272, 587, 700, 1089, 367, 1203, 822, 779, 648, 843, 965, 211, 199, 1072, 10, 531, 712, 908, 400, 613, 1100, 164, 493, 466, 212, 511, 1119, 197, 615, 1038, 210, 444, 857, 1039, 1013, 1044, 383, 858, 706, 364, 224, 873, 1070, 529, 285, 915, 1187, 105, 475, 301, 130, 249, 114, 958, 446, 260, 656, 184, 331, 153, 7, 230, 891, 1110, 583, 351, 665, 1043, 34, 1201, 256, 229, 840, 726, 830, 760, 469, 1063, 756, 481, 1167, 545, 664, 1062, 795, 284, 280, 1205, 558, 588, 935, 886, 906, 551, 675, 499, 638, 827, 1122, 921, 1109, 316, 472, 1068, 555, 617, 132, 361, 374, 120, 799, 968, 129, 207, 1035, 365, 237, 1115, 142, 1152, 577, 1023, 776, 243, 264, 60, 549, 507, 175, 222, 918, 414, 1108, 1202, 354, 662, 938, 1032, 608, 434, 165, 645, 115, 290, 82, 951, 697, 372, 1081, 398, 742, 1077, 121, 42, 453, 221, 505, 22, 379, 766, 49, 1114, 725, 340, 774, 138, 671, 182, 1117, 1146, 1034, 108, 70, 893, 30, 988, 1002, 94, 522, 238, 160, 731, 737, 1047, 1102, 694, 37, 729, 263, 936, 618, 649, 710, 1159, 245, 885, 286, 530, 455, 517, 539, 801, 1026, 409, 1132, 929, 1182, 1149, 205, 912, 1124, 149, 683, 957, 512, 584, 254, 252, 804, 1133, 783, 150, 1211, 794, 470, 574, 793, 265, 537, 262, 942, 1145, 335, 832, 601, 241, 743, 788, 460, 1001, 219, 1036, 540, 1214, 864, 417, 589, 1131, 176, 767, 931, 735, 1076, 498, 64, 388, 761, 107, 1079, 186, 1031, 1206, 1169, 734, 303, 18, 1156, 506, 855, 934, 1171, 395, 1078, 320, 524, 1030, 628, 270, 861, 208, 787, 711, 1049, 1054, 1098, 52, 526, 701, 181, 1058, 687, 814, 67, 860, 217, 171, 240, 1092, 866, 294, 780, 223, 892, 239, 983, 699, 899, 798, 862, 47, 927, 564, 817, 585, 1029, 571, 620, 853, 702, 715, 917, 723, 594, 552, 1165, 125, 104, 803, 1183, 883, 753, 570, 491, 299, 218, 159, 152, 232, 976, 1087, 784, 600, 825, 58, 810, 819, 575, 314, 1180, 137, 110, 206, 363, 611, 1113, 45, 373, 719, 987, 674, 535, 762, 325, 36, 426, 1141, 975, 154, 226, 850, 704, 972, 1191, 670, 933, 602, 464, 96, 15, 143, 261, 366, 248, 54, 751, 881, 768, 775, 441, 341, 925, 1176, 916, 168, 640, 676, 352, 20, 1128, 307, 634, 97, 406, 610, 1099, 815, 432, 986, 800, 66, 595, 736, 396, 231, 644, 1147, 457, 811, 1144, 733, 630, 196, 661, 542, 1064, 250, 573, 990, 1118, 722, 173, 679, 1209, 433, 677, 790, 623, 213, 1090, 605, 914, 274, 385, 764, 867, 334, 1016, 834, 692, 901, 578, 831, 287, 716, 473, 1066, 939, 1074, 848, 25, 412, 591, 1125, 377, 1006, 690, 547, 346, 179, 966, 56, 1033, 342, 89, 872, 1046, 919, 332, 835, 520, 621, 1106, 967, 2, 490, 323, 974, 949, 961, 170, 116, 923, 1170, 1140, 91, 279, 1120, 162, 642, 128, 151, 1164, 1163, 305, 627, 791, 576, 24, 200, 27, 875, 691, 1069, 83, 636, 730, 1060, 641, 33, 646, 1153, 909, 1123, 503, 502, 268, 347, 483, 660, 439, 413, 698, 87, 458, 13, 76, 714, 488, 0, 344, 1096, 994, 504, 536, 234, 1155, 1177, 61, 943, 1179, 550, 1111, 476, 1075, 515, 579, 635, 295, 977, 541, 569, 910, 902, 759, 1012, 689, 945, 14, 607, 663, 425, 304, 1071, 281, 428, 440, 582, 1018, 38, 1021, 932, 255, 534, 1134, 653, 1084, 178, 1212, 375, 1189, 865, 399, 718, 496, 1053, 214, 878, 978, 474, 668, 148, 1, 246, 846, 401, 821, 456, 920, 193, 79, 828, 1161, 513, 445, 135, 557, 180, 833, 889, 877, 593, 459, 384, 1196, 528, 997, 124, 1059, 612, 959, 1028, 809, 568, 19, 41, 732, 924, 554, 275, 658, 693, 1105, 516, 1088, 599, 707, 1027, 418, 53, 1150, 1065, 1010, 198, 12, 771, 44, 416, 39, 922, 805, 996, 419, 739, 863, 566, 876, 1207, 1181, 1004, 852, 895, 856, 1104, 1185, 1080, 745, 870, 1011, 1172, 709, 1022, 415, 26, 1173, 1003, 202, 78, 510, 1048, 485, 141, 521, 494, 786, 955, 1127, 443, 632, 422, 451, 1192, 785, 145, 651, 177, 411, 227, 353, 758, 258, 980, 330, 1168, 1139, 391, 667, 880, 685, 567, 194, 546, 92, 532, 381, 851, 1061, 842, 85, 427, 1166, 127, 403, 1009, 5, 161, 982, 462, 752, 333, 839, 802, 1200, 525, 622, 869, 134, 362, 201, 1138]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1728454002594199
the save name prefix for this run is:  chkpt-ID_1728454002594199_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 394
rank avg (pred): 0.467 +- 0.001
mrr vals (pred, true): 0.000, 0.035
batch losses (mrrl, rdl): 0.0, 0.0013983856

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1168
rank avg (pred): 0.344 +- 0.271
mrr vals (pred, true): 0.044, 0.068
batch losses (mrrl, rdl): 0.0, 6.64458e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 595
rank avg (pred): 0.367 +- 0.312
mrr vals (pred, true): 0.089, 0.011
batch losses (mrrl, rdl): 0.0, 6.75482e-05

Epoch over!
epoch time: 12.104

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 411
rank avg (pred): 0.315 +- 0.284
mrr vals (pred, true): 0.114, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003773814

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 510
rank avg (pred): 0.173 +- 0.168
mrr vals (pred, true): 0.156, 0.074
batch losses (mrrl, rdl): 0.0, 5.80373e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 933
rank avg (pred): 0.355 +- 0.320
mrr vals (pred, true): 0.131, 0.094
batch losses (mrrl, rdl): 0.0, 0.0002317749

Epoch over!
epoch time: 11.967

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 317
rank avg (pred): 0.066 +- 0.065
mrr vals (pred, true): 0.209, 0.283
batch losses (mrrl, rdl): 0.0, 1.03327e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 295
rank avg (pred): 0.072 +- 0.072
mrr vals (pred, true): 0.224, 0.102
batch losses (mrrl, rdl): 0.0, 9.2826e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 89
rank avg (pred): 0.336 +- 0.296
mrr vals (pred, true): 0.123, 0.035
batch losses (mrrl, rdl): 0.0, 0.0004567723

Epoch over!
epoch time: 12.052

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 513
rank avg (pred): 0.239 +- 0.232
mrr vals (pred, true): 0.158, 0.050
batch losses (mrrl, rdl): 0.0, 5.48124e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1019
rank avg (pred): 0.303 +- 0.289
mrr vals (pred, true): 0.178, 0.058
batch losses (mrrl, rdl): 0.0, 0.0003248702

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1084
rank avg (pred): 0.327 +- 0.293
mrr vals (pred, true): 0.136, 0.103
batch losses (mrrl, rdl): 0.0, 0.0006238155

Epoch over!
epoch time: 12.13

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 373
rank avg (pred): 0.316 +- 0.292
mrr vals (pred, true): 0.155, 0.042
batch losses (mrrl, rdl): 0.0, 0.000354333

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 731
rank avg (pred): 0.037 +- 0.037
mrr vals (pred, true): 0.277, 0.056
batch losses (mrrl, rdl): 0.0, 3.42908e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 846
rank avg (pred): 0.317 +- 0.299
mrr vals (pred, true): 0.187, 0.095
batch losses (mrrl, rdl): 0.0, 0.0001294968

Epoch over!
epoch time: 12.017

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 529
rank avg (pred): 0.203 +- 0.190
mrr vals (pred, true): 0.150, 0.076
batch losses (mrrl, rdl): 0.1008272767, 4.0776e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 475
rank avg (pred): 0.356 +- 0.125
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.000231192, 0.0003914307

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 993
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.189, 0.267
batch losses (mrrl, rdl): 0.0613140762, 2.26433e-05

Epoch over!
epoch time: 12.218

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 51
rank avg (pred): 0.028 +- 0.023
mrr vals (pred, true): 0.186, 0.137
batch losses (mrrl, rdl): 0.0239325874, 5.34585e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 686
rank avg (pred): 0.416 +- 0.127
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001842766, 0.0001941636

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 134
rank avg (pred): 0.350 +- 0.147
mrr vals (pred, true): 0.058, 0.044
batch losses (mrrl, rdl): 0.0006336562, 0.0005336367

Epoch over!
epoch time: 12.042

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.362 +- 0.167
mrr vals (pred, true): 0.061, 0.006
batch losses (mrrl, rdl): 0.001222467, 0.0003121413

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 113
rank avg (pred): 0.363 +- 0.130
mrr vals (pred, true): 0.051, 0.038
batch losses (mrrl, rdl): 1.29279e-05, 0.0005965295

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1109
rank avg (pred): 0.356 +- 0.157
mrr vals (pred, true): 0.057, 0.007
batch losses (mrrl, rdl): 0.0004753848, 0.0003619388

Epoch over!
epoch time: 12.443

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 908
rank avg (pred): 0.182 +- 0.097
mrr vals (pred, true): 0.093, 0.111
batch losses (mrrl, rdl): 0.0034232831, 0.000306158

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 302
rank avg (pred): 0.051 +- 0.033
mrr vals (pred, true): 0.151, 0.116
batch losses (mrrl, rdl): 0.0122734569, 2.37472e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 629
rank avg (pred): 0.425 +- 0.115
mrr vals (pred, true): 0.043, 0.024
batch losses (mrrl, rdl): 0.0004464102, 0.0003118903

Epoch over!
epoch time: 12.376

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 770
rank avg (pred): 0.352 +- 0.125
mrr vals (pred, true): 0.053, 0.097
batch losses (mrrl, rdl): 8.99277e-05, 0.0002636964

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 441
rank avg (pred): 0.370 +- 0.108
mrr vals (pred, true): 0.043, 0.007
batch losses (mrrl, rdl): 0.0004366938, 0.0003564785

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 350
rank avg (pred): 0.359 +- 0.143
mrr vals (pred, true): 0.057, 0.071
batch losses (mrrl, rdl): 0.0004546351, 0.0007417287

Epoch over!
epoch time: 12.332

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 921
rank avg (pred): 0.407 +- 0.119
mrr vals (pred, true): 0.046, 0.092
batch losses (mrrl, rdl): 0.0001240943, 0.0004323768

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 230
rank avg (pred): 0.383 +- 0.135
mrr vals (pred, true): 0.054, 0.006
batch losses (mrrl, rdl): 0.00014871, 0.0002771571

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 738
rank avg (pred): 0.109 +- 0.070
mrr vals (pred, true): 0.084, 0.087
batch losses (mrrl, rdl): 0.0115139, 6.10075e-05

Epoch over!
epoch time: 12.457

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 685
rank avg (pred): 0.417 +- 0.116
mrr vals (pred, true): 0.048, 0.007
batch losses (mrrl, rdl): 5.49698e-05, 0.0001937593

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 130
rank avg (pred): 0.346 +- 0.132
mrr vals (pred, true): 0.056, 0.020
batch losses (mrrl, rdl): 0.0003815663, 0.0003540767

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 873
rank avg (pred): 0.378 +- 0.117
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 5.0645e-06, 0.0003170292

Epoch over!
epoch time: 12.099

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 875
rank avg (pred): 0.354 +- 0.106
mrr vals (pred, true): 0.048, 0.007
batch losses (mrrl, rdl): 3.84459e-05, 0.0004560831

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 100
rank avg (pred): 0.330 +- 0.129
mrr vals (pred, true): 0.060, 0.022
batch losses (mrrl, rdl): 0.0009428546, 0.0002399735

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 506
rank avg (pred): 0.193 +- 0.113
mrr vals (pred, true): 0.083, 0.098
batch losses (mrrl, rdl): 0.0106208604, 7.67075e-05

Epoch over!
epoch time: 12.105

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 18
rank avg (pred): 0.167 +- 0.100
mrr vals (pred, true): 0.131, 0.116
batch losses (mrrl, rdl): 0.002279554, 0.0001272595

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 108
rank avg (pred): 0.374 +- 0.110
mrr vals (pred, true): 0.049, 0.012
batch losses (mrrl, rdl): 4.9186e-06, 0.0002496111

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 208
rank avg (pred): 0.322 +- 0.127
mrr vals (pred, true): 0.059, 0.006
batch losses (mrrl, rdl): 0.0008204154, 0.0006098432

Epoch over!
epoch time: 12.198

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 603
rank avg (pred): 0.410 +- 0.116
mrr vals (pred, true): 0.049, 0.010
batch losses (mrrl, rdl): 6.3495e-06, 0.000150838

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.025 +- 0.018
mrr vals (pred, true): 0.228, 0.213
batch losses (mrrl, rdl): 0.002306611, 2.30749e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1069
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.280, 0.289
batch losses (mrrl, rdl): 0.0006802264, 2.20808e-05

Epoch over!
epoch time: 12.103

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.340 +- 0.110
mrr vals (pred, true): 0.051, 0.020

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   29 	     0 	 0.05064 	 0.00562 	 m..s
    3 	     1 	 0.04785 	 0.00613 	 m..s
   45 	     2 	 0.05326 	 0.00614 	 m..s
   88 	     3 	 0.07009 	 0.00619 	 m..s
   68 	     4 	 0.05717 	 0.00625 	 m..s
    1 	     5 	 0.04780 	 0.00625 	 m..s
   25 	     6 	 0.05043 	 0.00626 	 m..s
   35 	     7 	 0.05098 	 0.00627 	 m..s
   86 	     8 	 0.06881 	 0.00628 	 m..s
   17 	     9 	 0.04974 	 0.00632 	 m..s
    6 	    10 	 0.04816 	 0.00632 	 m..s
   32 	    11 	 0.05083 	 0.00633 	 m..s
   24 	    12 	 0.05037 	 0.00633 	 m..s
   60 	    13 	 0.05621 	 0.00634 	 m..s
    0 	    14 	 0.04768 	 0.00634 	 m..s
    8 	    15 	 0.04841 	 0.00634 	 m..s
    2 	    16 	 0.04784 	 0.00637 	 m..s
    7 	    17 	 0.04824 	 0.00637 	 m..s
   30 	    18 	 0.05070 	 0.00638 	 m..s
   65 	    19 	 0.05669 	 0.00638 	 m..s
   61 	    20 	 0.05625 	 0.00639 	 m..s
   14 	    21 	 0.04932 	 0.00640 	 m..s
   47 	    22 	 0.05485 	 0.00641 	 m..s
   83 	    23 	 0.06747 	 0.00641 	 m..s
   55 	    24 	 0.05565 	 0.00642 	 m..s
   87 	    25 	 0.07006 	 0.00642 	 m..s
   26 	    26 	 0.05043 	 0.00646 	 m..s
   41 	    27 	 0.05156 	 0.00646 	 m..s
   16 	    28 	 0.04970 	 0.00647 	 m..s
   69 	    29 	 0.05719 	 0.00649 	 m..s
   80 	    30 	 0.06604 	 0.00649 	 m..s
   12 	    31 	 0.04918 	 0.00650 	 m..s
   11 	    32 	 0.04885 	 0.00651 	 m..s
   34 	    33 	 0.05098 	 0.00654 	 m..s
   71 	    34 	 0.05830 	 0.00654 	 m..s
   66 	    35 	 0.05690 	 0.00655 	 m..s
   21 	    36 	 0.05018 	 0.00658 	 m..s
   13 	    37 	 0.04924 	 0.00664 	 m..s
   58 	    38 	 0.05588 	 0.00665 	 m..s
   72 	    39 	 0.05876 	 0.00671 	 m..s
   70 	    40 	 0.05775 	 0.00672 	 m..s
   67 	    41 	 0.05694 	 0.00673 	 m..s
    5 	    42 	 0.04795 	 0.00801 	 m..s
    4 	    43 	 0.04794 	 0.00805 	 m..s
   37 	    44 	 0.05121 	 0.01007 	 m..s
    9 	    45 	 0.04845 	 0.01016 	 m..s
   19 	    46 	 0.05009 	 0.01255 	 m..s
   20 	    47 	 0.05014 	 0.01346 	 m..s
   10 	    48 	 0.04867 	 0.01413 	 m..s
   50 	    49 	 0.05526 	 0.01485 	 m..s
   33 	    50 	 0.05085 	 0.01549 	 m..s
   53 	    51 	 0.05547 	 0.01664 	 m..s
   23 	    52 	 0.05027 	 0.01695 	 m..s
   22 	    53 	 0.05027 	 0.01698 	 m..s
   36 	    54 	 0.05098 	 0.01968 	 m..s
   18 	    55 	 0.05007 	 0.02023 	 ~...
   62 	    56 	 0.05634 	 0.02307 	 m..s
   31 	    57 	 0.05075 	 0.02506 	 ~...
   40 	    58 	 0.05147 	 0.02538 	 ~...
   54 	    59 	 0.05551 	 0.04430 	 ~...
   48 	    60 	 0.05488 	 0.04907 	 ~...
   75 	    61 	 0.06336 	 0.05596 	 ~...
   44 	    62 	 0.05319 	 0.05596 	 ~...
   43 	    63 	 0.05303 	 0.05837 	 ~...
   89 	    64 	 0.07177 	 0.05962 	 ~...
   52 	    65 	 0.05547 	 0.06087 	 ~...
  109 	    66 	 0.13255 	 0.06172 	 m..s
   51 	    67 	 0.05534 	 0.06519 	 ~...
   49 	    68 	 0.05522 	 0.06597 	 ~...
   46 	    69 	 0.05476 	 0.06602 	 ~...
   56 	    70 	 0.05567 	 0.06665 	 ~...
   57 	    71 	 0.05582 	 0.06890 	 ~...
   96 	    72 	 0.08890 	 0.06971 	 ~...
   97 	    73 	 0.09006 	 0.07343 	 ~...
   59 	    74 	 0.05612 	 0.07687 	 ~...
   79 	    75 	 0.06567 	 0.07777 	 ~...
   82 	    76 	 0.06680 	 0.08095 	 ~...
   99 	    77 	 0.09230 	 0.08425 	 ~...
   93 	    78 	 0.08320 	 0.08550 	 ~...
  100 	    79 	 0.09236 	 0.08613 	 ~...
   95 	    80 	 0.08797 	 0.08816 	 ~...
   78 	    81 	 0.06424 	 0.08825 	 ~...
  107 	    82 	 0.12923 	 0.08904 	 m..s
   73 	    83 	 0.06173 	 0.09088 	 ~...
   74 	    84 	 0.06255 	 0.09162 	 ~...
  112 	    85 	 0.15205 	 0.09178 	 m..s
  108 	    86 	 0.12997 	 0.09268 	 m..s
  115 	    87 	 0.17108 	 0.09435 	 m..s
   84 	    88 	 0.06828 	 0.09453 	 ~...
   63 	    89 	 0.05655 	 0.09470 	 m..s
   39 	    90 	 0.05134 	 0.09579 	 m..s
   42 	    91 	 0.05185 	 0.09588 	 m..s
   15 	    92 	 0.04969 	 0.09611 	 m..s
   64 	    93 	 0.05657 	 0.09648 	 m..s
   38 	    94 	 0.05123 	 0.09695 	 m..s
   27 	    95 	 0.05059 	 0.09718 	 m..s
   28 	    96 	 0.05060 	 0.09751 	 m..s
   77 	    97 	 0.06389 	 0.10193 	 m..s
   98 	    98 	 0.09112 	 0.10352 	 ~...
   92 	    99 	 0.08127 	 0.10361 	 ~...
   76 	   100 	 0.06368 	 0.10369 	 m..s
   94 	   101 	 0.08676 	 0.10387 	 ~...
  106 	   102 	 0.10506 	 0.10441 	 ~...
   85 	   103 	 0.06879 	 0.10869 	 m..s
  104 	   104 	 0.10380 	 0.10937 	 ~...
  113 	   105 	 0.15652 	 0.11116 	 m..s
  103 	   106 	 0.09796 	 0.11167 	 ~...
   90 	   107 	 0.07335 	 0.11182 	 m..s
   81 	   108 	 0.06639 	 0.11538 	 m..s
  102 	   109 	 0.09720 	 0.11969 	 ~...
   91 	   110 	 0.07565 	 0.12157 	 m..s
  114 	   111 	 0.15763 	 0.12209 	 m..s
  110 	   112 	 0.13757 	 0.13163 	 ~...
  111 	   113 	 0.13833 	 0.14112 	 ~...
  101 	   114 	 0.09414 	 0.14115 	 m..s
  105 	   115 	 0.10495 	 0.18164 	 m..s
  117 	   116 	 0.17761 	 0.19252 	 ~...
  116 	   117 	 0.17367 	 0.21399 	 m..s
  118 	   118 	 0.27555 	 0.27122 	 ~...
  120 	   119 	 0.28891 	 0.28924 	 ~...
  119 	   120 	 0.28518 	 0.29259 	 ~...
==========================================
r_mrr = 0.8136349320411682
r2_mrr = 0.5853498578071594
spearmanr_mrr@5 = 0.9849156141281128
spearmanr_mrr@10 = 0.9629555344581604
spearmanr_mrr@50 = 0.9722394347190857
spearmanr_mrr@100 = 0.9097980856895447
spearmanr_mrr@All = 0.9101828932762146
==========================================
test time: 0.453
Done Testing dataset OpenEA
total time taken: 198.2512559890747
training time taken: 183.1811602115631
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8136)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.5853)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9849)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9630)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9722)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9098)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9102)}}, 'test_loss': {'TransE': {'OpenEA': 0.877014378515014}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 9518718177570708
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1210, 194, 641, 313, 70, 619, 870, 399, 114, 747, 814, 1074, 299, 990, 408, 533, 1184, 788, 652, 445, 510, 298, 134, 936, 761, 373, 631, 950, 242, 757, 552, 559, 806, 215, 1140, 630, 364, 1066, 851, 113, 361, 317, 336, 1123, 664, 58, 1051, 265, 1141, 961, 784, 211, 468, 830, 549, 876, 824, 369, 296, 970, 1002, 1046, 725, 706, 1087, 946, 78, 654, 987, 891, 1107, 774, 1015, 1007, 1177, 727, 1095, 1164, 429, 161, 1014, 1139, 338, 66, 43, 907, 1100, 940, 135, 496, 268, 1120, 30, 1134, 126, 288, 769, 127, 287, 882, 512, 417, 65, 1006, 63, 332, 1076, 1093, 246, 48, 266, 231, 170, 99, 1191, 286, 820, 310, 1031, 1034, 1200]
valid_ids (0): []
train_ids (1094): [1001, 1106, 795, 182, 39, 580, 267, 89, 1017, 1154, 230, 574, 251, 188, 274, 312, 1128, 34, 219, 729, 1010, 668, 270, 646, 359, 992, 185, 1181, 318, 923, 410, 781, 1113, 307, 837, 577, 61, 1117, 557, 1108, 543, 50, 1194, 181, 759, 1047, 472, 1144, 400, 488, 110, 446, 335, 18, 1075, 667, 958, 213, 731, 603, 611, 62, 910, 174, 467, 195, 901, 1187, 59, 570, 939, 1195, 469, 1138, 191, 197, 10, 671, 464, 379, 662, 168, 573, 1092, 300, 1028, 849, 615, 499, 120, 698, 8, 1061, 3, 539, 985, 815, 407, 843, 693, 133, 236, 736, 276, 495, 812, 819, 550, 1077, 282, 956, 362, 913, 81, 854, 690, 492, 1085, 721, 579, 884, 278, 422, 542, 835, 793, 609, 535, 404, 79, 1190, 460, 1212, 218, 13, 315, 331, 732, 1068, 778, 1207, 953, 431, 800, 1173, 229, 622, 897, 389, 890, 345, 685, 618, 886, 975, 1202, 889, 105, 162, 214, 606, 75, 728, 344, 1097, 386, 121, 1158, 909, 302, 694, 660, 643, 128, 46, 520, 1201, 171, 762, 1122, 115, 439, 1180, 1142, 1011, 714, 771, 97, 275, 165, 665, 93, 466, 692, 928, 764, 945, 462, 629, 143, 677, 1118, 49, 358, 443, 932, 56, 963, 55, 334, 986, 6, 144, 568, 395, 484, 816, 476, 929, 661, 22, 280, 83, 21, 106, 9, 799, 968, 776, 486, 900, 1040, 962, 597, 919, 1130, 482, 333, 937, 471, 453, 31, 921, 637, 387, 420, 506, 1088, 766, 823, 1125, 645, 147, 254, 786, 263, 966, 595, 647, 767, 636, 983, 521, 454, 621, 628, 750, 765, 922, 982, 365, 1090, 138, 186, 1161, 610, 723, 357, 1114, 192, 250, 223, 354, 1168, 887, 73, 12, 497, 247, 942, 297, 1151, 598, 734, 749, 881, 872, 118, 426, 1099, 697, 423, 1105, 927, 259, 941, 902, 1170, 591, 57, 976, 457, 1179, 136, 911, 51, 1155, 674, 680, 841, 123, 850, 411, 1132, 663, 713, 374, 614, 451, 415, 959, 479, 166, 157, 979, 391, 80, 1145, 478, 208, 522, 366, 739, 632, 256, 253, 1044, 1196, 1033, 756, 742, 707, 514, 828, 33, 689, 281, 380, 515, 717, 633, 206, 883, 316, 183, 225, 69, 679, 1124, 895, 430, 1185, 1056, 403, 791, 150, 805, 1193, 1069, 783, 1104, 1023, 967, 1082, 624, 915, 808, 592, 593, 237, 401, 527, 832, 1127, 817, 903, 447, 224, 355, 437, 813, 561, 234, 878, 260, 108, 743, 390, 501, 1050, 669, 130, 103, 481, 563, 1183, 483, 1166, 311, 1018, 1121, 695, 172, 980, 1027, 1041, 1131, 602, 160, 279, 772, 938, 860, 227, 1133, 862, 523, 1004, 1080, 673, 72, 5, 914, 682, 261, 1143, 565, 394, 1174, 1029, 1149, 998, 32, 715, 988, 341, 537, 617, 807, 353, 232, 947, 1214, 779, 811, 865, 129, 705, 294, 1081, 290, 413, 888, 683, 16, 834, 656, 1101, 1111, 777, 1037, 612, 973, 175, 432, 474, 754, 578, 948, 567, 90, 271, 119, 1103, 733, 735, 691, 45, 60, 301, 789, 1160, 531, 240, 124, 1156, 53, 37, 24, 1186, 583, 326, 760, 676, 726, 327, 498, 875, 880, 504, 566, 584, 613, 763, 562, 560, 320, 758, 517, 485, 972, 587, 965, 1175, 26, 92, 822, 588, 351, 17, 861, 519, 226, 551, 319, 507, 600, 122, 1162, 155, 221, 569, 1009, 125, 590, 217, 235, 809, 651, 427, 1137, 249, 38, 370, 530, 285, 1119, 601, 203, 222, 383, 393, 245, 684, 330, 686, 867, 864, 424, 272, 1035, 797, 1102, 23, 1091, 594, 933, 871, 94, 85, 193, 154, 1203, 1163, 1135, 382, 201, 951, 513, 82, 198, 398, 428, 1208, 949, 825, 720, 199, 712, 1063, 1059, 737, 36, 169, 918, 196, 845, 414, 996, 894, 1152, 833, 352, 14, 586, 792, 220, 164, 112, 141, 1204, 156, 371, 752, 1182, 1003, 555, 456, 931, 19, 1096, 1112, 339, 649, 392, 323, 88, 27, 96, 1021, 1072, 991, 255, 152, 347, 869, 76, 1053, 91, 704, 406, 678, 836, 703, 233, 289, 178, 892, 377, 29, 435, 925, 745, 978, 239, 605, 11, 238, 116, 384, 623, 20, 1205, 490, 1169, 785, 216, 548, 879, 204, 1054, 190, 984, 441, 367, 269, 346, 145, 421, 450, 1079, 151, 572, 974, 644, 1153, 844, 205, 1062, 350, 360, 173, 412, 1115, 780, 1084, 1005, 314, 1116, 396, 989, 409, 896, 545, 1070, 740, 688, 639, 1165, 455, 264, 840, 1055, 252, 1110, 701, 1094, 452, 526, 885, 305, 955, 957, 340, 810, 184, 708, 971, 718, 343, 648, 1126, 262, 804, 1012, 102, 1020, 787, 1043, 342, 167, 1052, 589, 1206, 1086, 473, 475, 981, 101, 363, 505, 1032, 434, 782, 935, 855, 283, 368, 842, 658, 487, 348, 977, 829, 375, 388, 177, 874, 244, 709, 738, 1048, 711, 596, 744, 207, 827, 857, 626, 25, 681, 440, 444, 848, 803, 906, 416, 107, 2, 321, 1065, 328, 202, 877, 98, 200, 753, 372, 163, 148, 1057, 449, 821, 719, 67, 995, 1049, 1022, 746, 655, 529, 180, 659, 1030, 153, 916, 137, 558, 1199, 634, 322, 494, 770, 638, 1167, 42, 1171, 571, 385, 607, 491, 775, 710, 650, 1147, 1197, 68, 863, 378, 818, 502, 1038, 748, 158, 964, 1064, 768, 773, 436, 1036, 500, 64, 179, 926, 7, 858, 1178, 1071, 459, 576, 994, 856, 74, 554, 672, 303, 801, 15, 1078, 142, 4, 556, 640, 581, 904, 405, 349, 553, 920, 325, 866, 899, 912, 1060, 189, 509, 470, 461, 1039, 309, 924, 293, 159, 826, 397, 541, 52, 442, 1209, 503, 304, 243, 859, 716, 722, 846, 620, 1129, 402, 508, 585, 796, 873, 839, 176, 139, 997, 1188, 852, 1098, 356, 893, 131, 1024, 564, 699, 657, 209, 696, 730, 687, 306, 532, 241, 1042, 627, 146, 1016, 575, 117, 798, 625, 675, 1013, 480, 376, 969, 1189, 547, 1159, 1008, 905, 40, 257, 954, 111, 538, 943, 751, 1148, 794, 582, 228, 329, 425, 518, 86, 544, 790, 666, 1089, 831, 516, 292, 635, 1211, 277, 493, 642, 1058, 433, 1213, 44, 741, 132, 960, 1150, 700, 511, 84, 1073, 1198, 477, 87, 868, 525, 210, 616, 1067, 534, 1083, 109, 0, 755, 381, 419, 291, 1192, 934, 930, 465, 448, 847, 54, 999, 1025, 1176, 528, 524, 28, 438, 35, 77, 100, 308, 284, 149, 838, 944, 724, 546, 853, 47, 212, 608, 489, 908, 1045, 258, 295, 248, 1146, 95, 1, 898, 599, 1157, 917, 273, 1136, 187, 540, 463, 702, 1000, 604, 140, 993, 653, 324, 458, 952, 418, 104, 71, 1109, 1026, 1172, 802, 670, 536, 337, 1019, 41]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7897406682794256
the save name prefix for this run is:  chkpt-ID_7897406682794256_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 153
rank avg (pred): 0.482 +- 0.007
mrr vals (pred, true): 0.000, 0.016
batch losses (mrrl, rdl): 0.0, 0.0011624754

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 224
rank avg (pred): 0.380 +- 0.229
mrr vals (pred, true): 0.126, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001593443

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 802
rank avg (pred): 0.346 +- 0.263
mrr vals (pred, true): 0.251, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002456397

Epoch over!
epoch time: 12.579

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1009
rank avg (pred): 0.342 +- 0.272
mrr vals (pred, true): 0.267, 0.054
batch losses (mrrl, rdl): 0.0, 0.0004695187

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1132
rank avg (pred): 0.293 +- 0.265
mrr vals (pred, true): 0.330, 0.006
batch losses (mrrl, rdl): 0.0, 0.000487108

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 739
rank avg (pred): 0.054 +- 0.051
mrr vals (pred, true): 0.385, 0.087
batch losses (mrrl, rdl): 0.0, 1.7161e-06

Epoch over!
epoch time: 12.609

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 102
rank avg (pred): 0.358 +- 0.307
mrr vals (pred, true): 0.325, 0.014
batch losses (mrrl, rdl): 0.0, 0.0003060152

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 598
rank avg (pred): 0.403 +- 0.331
mrr vals (pred, true): 0.311, 0.011
batch losses (mrrl, rdl): 0.0, 0.0001629715

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 628
rank avg (pred): 0.403 +- 0.334
mrr vals (pred, true): 0.318, 0.011
batch losses (mrrl, rdl): 0.0, 0.0001333436

Epoch over!
epoch time: 12.512

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 626
rank avg (pred): 0.359 +- 0.313
mrr vals (pred, true): 0.342, 0.026
batch losses (mrrl, rdl): 0.0, 9.43922e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 792
rank avg (pred): 0.304 +- 0.285
mrr vals (pred, true): 0.379, 0.007
batch losses (mrrl, rdl): 0.0, 0.0004077368

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1055
rank avg (pred): 0.059 +- 0.057
mrr vals (pred, true): 0.404, 0.251
batch losses (mrrl, rdl): 0.0, 9.4937e-06

Epoch over!
epoch time: 12.364

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 920
rank avg (pred): 0.351 +- 0.307
mrr vals (pred, true): 0.350, 0.096
batch losses (mrrl, rdl): 0.0, 0.0002086332

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 46
rank avg (pred): 0.077 +- 0.071
mrr vals (pred, true): 0.371, 0.170
batch losses (mrrl, rdl): 0.0, 1.31444e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 229
rank avg (pred): 0.336 +- 0.304
mrr vals (pred, true): 0.368, 0.006
batch losses (mrrl, rdl): 0.0, 0.000262781

Epoch over!
epoch time: 12.427

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 986
rank avg (pred): 0.068 +- 0.066
mrr vals (pred, true): 0.410, 0.268
batch losses (mrrl, rdl): 0.2026572526, 2.44528e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1111
rank avg (pred): 0.336 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0248018056, 0.0006118079

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 776
rank avg (pred): 0.267 +- 0.000
mrr vals (pred, true): 0.000, 0.098
batch losses (mrrl, rdl): 0.0247513149, 0.0001491003

Epoch over!
epoch time: 14.012

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 511
rank avg (pred): 0.263 +- 0.000
mrr vals (pred, true): 0.000, 0.093
batch losses (mrrl, rdl): 0.0247469004, 0.0002045649

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.255 +- 0.000
mrr vals (pred, true): 0.000, 0.036
batch losses (mrrl, rdl): 0.0247397441, 0.0002786863

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 901
rank avg (pred): 0.247 +- 0.000
mrr vals (pred, true): 0.000, 0.066
batch losses (mrrl, rdl): 0.0247307606, 0.000533158

Epoch over!
epoch time: 13.949

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 702
rank avg (pred): 0.242 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.024725141, 0.0014362992

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.244 +- 0.000
mrr vals (pred, true): 0.000, 0.106
batch losses (mrrl, rdl): 0.1113915294, 0.0001674612

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 567
rank avg (pred): 0.245 +- 0.000
mrr vals (pred, true): 0.000, 0.009
batch losses (mrrl, rdl): 0.0247290079, 0.0004375883

Epoch over!
epoch time: 13.797

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 339
rank avg (pred): 0.239 +- 0.000
mrr vals (pred, true): 0.000, 0.022
batch losses (mrrl, rdl): 0.0247215685, 0.0001148979

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 651
rank avg (pred): 0.247 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247305166, 0.0013501431

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 948
rank avg (pred): 0.245 +- 0.000
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247282088, 0.0014145924

Epoch over!
epoch time: 12.632

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 818
rank avg (pred): 0.239 +- 0.000
mrr vals (pred, true): 0.000, 0.092
batch losses (mrrl, rdl): 0.0247220043, 0.0007163582

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 720
rank avg (pred): 0.233 +- 0.000
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247150548, 0.0015637503

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 224
rank avg (pred): 0.246 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247296318, 0.0013723715

Epoch over!
epoch time: 13.174

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 539
rank avg (pred): 0.242 +- 0.000
mrr vals (pred, true): 0.000, 0.088
batch losses (mrrl, rdl): 0.0247255731, 0.0001849954

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 571
rank avg (pred): 0.259 +- 0.000
mrr vals (pred, true): 0.000, 0.011
batch losses (mrrl, rdl): 0.0247429572, 0.0003219165

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 867
rank avg (pred): 0.237 +- 0.000
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247195996, 0.0014473789

Epoch over!
epoch time: 13.622

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 572
rank avg (pred): 0.228 +- 0.000
mrr vals (pred, true): 0.000, 0.024
batch losses (mrrl, rdl): 0.0247091055, 0.0003696658

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 279
rank avg (pred): 0.238 +- 0.000
mrr vals (pred, true): 0.000, 0.095
batch losses (mrrl, rdl): 0.0247205142, 0.0004672901

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 355
rank avg (pred): 0.230 +- 0.000
mrr vals (pred, true): 0.000, 0.028
batch losses (mrrl, rdl): 0.0247108974, 0.0001059887

Epoch over!
epoch time: 14.03

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 670
rank avg (pred): 0.230 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247107111, 0.0016384391

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 285
rank avg (pred): 0.243 +- 0.000
mrr vals (pred, true): 0.000, 0.103
batch losses (mrrl, rdl): 0.104906328, 0.0004870088

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 562
rank avg (pred): 0.240 +- 0.000
mrr vals (pred, true): 0.000, 0.078
batch losses (mrrl, rdl): 0.0247225892, 0.0001475776

Epoch over!
epoch time: 13.577

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 205
rank avg (pred): 0.254 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247386247, 0.0013198021

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 166
rank avg (pred): 0.247 +- 0.000
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0247306284, 0.0013468092

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 277
rank avg (pred): 0.251 +- 0.000
mrr vals (pred, true): 0.000, 0.073
batch losses (mrrl, rdl): 0.0247352589, 0.0004749704

Epoch over!
epoch time: 13.078

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1204
rank avg (pred): 0.249 +- 0.000
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0247331411, 0.0013784211

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 125
rank avg (pred): 0.250 +- 0.000
mrr vals (pred, true): 0.000, 0.040
batch losses (mrrl, rdl): 0.0247343257, 0.0001214343

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1106
rank avg (pred): 0.247 +- 0.000
mrr vals (pred, true): 0.000, 0.131
batch losses (mrrl, rdl): 0.1708914489, 0.0002063875

Epoch over!
epoch time: 13.573

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.246 +- 0.000
mrr vals (pred, true): 0.000, 0.007

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.00027 	 0.00562 	 ~...
    0 	     1 	 0.00027 	 0.00598 	 ~...
    0 	     2 	 0.00027 	 0.00602 	 ~...
    0 	     3 	 0.00027 	 0.00605 	 ~...
    0 	     4 	 0.00027 	 0.00609 	 ~...
    0 	     5 	 0.00027 	 0.00612 	 ~...
    0 	     6 	 0.00027 	 0.00615 	 ~...
    0 	     7 	 0.00027 	 0.00625 	 ~...
    0 	     8 	 0.00027 	 0.00626 	 ~...
    0 	     9 	 0.00027 	 0.00628 	 ~...
    0 	    10 	 0.00027 	 0.00634 	 ~...
    0 	    11 	 0.00027 	 0.00636 	 ~...
    0 	    12 	 0.00027 	 0.00639 	 ~...
    0 	    13 	 0.00027 	 0.00642 	 ~...
    0 	    14 	 0.00027 	 0.00642 	 ~...
    0 	    15 	 0.00027 	 0.00644 	 ~...
    0 	    16 	 0.00027 	 0.00646 	 ~...
    0 	    17 	 0.00027 	 0.00646 	 ~...
    0 	    18 	 0.00027 	 0.00646 	 ~...
    0 	    19 	 0.00027 	 0.00646 	 ~...
    0 	    20 	 0.00027 	 0.00650 	 ~...
    0 	    21 	 0.00027 	 0.00651 	 ~...
    0 	    22 	 0.00027 	 0.00651 	 ~...
    0 	    23 	 0.00027 	 0.00658 	 ~...
    0 	    24 	 0.00027 	 0.00659 	 ~...
    0 	    25 	 0.00027 	 0.00660 	 ~...
    0 	    26 	 0.00027 	 0.00663 	 ~...
    0 	    27 	 0.00027 	 0.00665 	 ~...
    0 	    28 	 0.00027 	 0.00666 	 ~...
    0 	    29 	 0.00027 	 0.00667 	 ~...
    0 	    30 	 0.00027 	 0.00670 	 ~...
    0 	    31 	 0.00027 	 0.00670 	 ~...
    0 	    32 	 0.00027 	 0.00670 	 ~...
    0 	    33 	 0.00027 	 0.00672 	 ~...
    0 	    34 	 0.00027 	 0.00673 	 ~...
    0 	    35 	 0.00027 	 0.00675 	 ~...
    0 	    36 	 0.00027 	 0.00690 	 ~...
    0 	    37 	 0.00027 	 0.00950 	 ~...
    0 	    38 	 0.00027 	 0.01152 	 ~...
    0 	    39 	 0.00027 	 0.01244 	 ~...
    0 	    40 	 0.00027 	 0.01355 	 ~...
    0 	    41 	 0.00027 	 0.01452 	 ~...
    0 	    42 	 0.00027 	 0.01613 	 ~...
    0 	    43 	 0.00027 	 0.01664 	 ~...
    0 	    44 	 0.00027 	 0.02136 	 ~...
    0 	    45 	 0.00027 	 0.02222 	 ~...
    0 	    46 	 0.00027 	 0.02557 	 ~...
    0 	    47 	 0.00027 	 0.02652 	 ~...
    0 	    48 	 0.00027 	 0.03191 	 m..s
    0 	    49 	 0.00027 	 0.03243 	 m..s
    0 	    50 	 0.00027 	 0.03290 	 m..s
    0 	    51 	 0.00027 	 0.03776 	 m..s
    0 	    52 	 0.00027 	 0.04185 	 m..s
    0 	    53 	 0.00027 	 0.04369 	 m..s
    0 	    54 	 0.00027 	 0.04417 	 m..s
    0 	    55 	 0.00027 	 0.05705 	 m..s
    0 	    56 	 0.00027 	 0.05761 	 m..s
    0 	    57 	 0.00027 	 0.05842 	 m..s
    0 	    58 	 0.00027 	 0.05956 	 m..s
    0 	    59 	 0.00027 	 0.06123 	 m..s
    0 	    60 	 0.00027 	 0.06311 	 m..s
    0 	    61 	 0.00027 	 0.06588 	 m..s
    0 	    62 	 0.00027 	 0.06649 	 m..s
    0 	    63 	 0.00027 	 0.06665 	 m..s
    0 	    64 	 0.00027 	 0.06966 	 m..s
    0 	    65 	 0.00027 	 0.07412 	 m..s
    0 	    66 	 0.00027 	 0.07453 	 m..s
    0 	    67 	 0.00027 	 0.07550 	 m..s
    0 	    68 	 0.00027 	 0.07911 	 m..s
    0 	    69 	 0.00027 	 0.07981 	 m..s
    0 	    70 	 0.00027 	 0.08075 	 m..s
    0 	    71 	 0.00027 	 0.08550 	 m..s
    0 	    72 	 0.00027 	 0.08615 	 m..s
    0 	    73 	 0.00027 	 0.08620 	 m..s
    0 	    74 	 0.00027 	 0.08694 	 m..s
    0 	    75 	 0.00027 	 0.08816 	 m..s
    0 	    76 	 0.00027 	 0.09171 	 m..s
    0 	    77 	 0.00027 	 0.09181 	 m..s
    0 	    78 	 0.00027 	 0.09555 	 m..s
    0 	    79 	 0.00027 	 0.09611 	 m..s
    0 	    80 	 0.00027 	 0.09660 	 m..s
    0 	    81 	 0.00027 	 0.09679 	 m..s
    0 	    82 	 0.00027 	 0.09713 	 m..s
    0 	    83 	 0.00027 	 0.09751 	 m..s
    0 	    84 	 0.00027 	 0.09790 	 m..s
    0 	    85 	 0.00027 	 0.09801 	 m..s
    0 	    86 	 0.00027 	 0.09837 	 m..s
    0 	    87 	 0.00027 	 0.10189 	 MISS
    0 	    88 	 0.00027 	 0.10453 	 MISS
    0 	    89 	 0.00027 	 0.10491 	 MISS
    0 	    90 	 0.00027 	 0.10709 	 MISS
    0 	    91 	 0.00027 	 0.10788 	 MISS
    0 	    92 	 0.00027 	 0.10959 	 MISS
    0 	    93 	 0.00027 	 0.10984 	 MISS
    0 	    94 	 0.00027 	 0.11051 	 MISS
    0 	    95 	 0.00027 	 0.11189 	 MISS
    0 	    96 	 0.00027 	 0.11353 	 MISS
    0 	    97 	 0.00027 	 0.11420 	 MISS
    0 	    98 	 0.00027 	 0.11538 	 MISS
    0 	    99 	 0.00027 	 0.11969 	 MISS
    0 	   100 	 0.00027 	 0.12399 	 MISS
    0 	   101 	 0.00027 	 0.12480 	 MISS
    0 	   102 	 0.00027 	 0.12635 	 MISS
    0 	   103 	 0.00027 	 0.12726 	 MISS
    0 	   104 	 0.00027 	 0.13667 	 MISS
    0 	   105 	 0.00027 	 0.14112 	 MISS
    0 	   106 	 0.00027 	 0.14115 	 MISS
    0 	   107 	 0.00027 	 0.14405 	 MISS
    0 	   108 	 0.00027 	 0.14815 	 MISS
    0 	   109 	 0.00027 	 0.15103 	 MISS
    0 	   110 	 0.00027 	 0.19077 	 MISS
    0 	   111 	 0.00027 	 0.19590 	 MISS
    0 	   112 	 0.00027 	 0.19802 	 MISS
    0 	   113 	 0.00027 	 0.22417 	 MISS
    0 	   114 	 0.00027 	 0.26458 	 MISS
    0 	   115 	 0.00027 	 0.27253 	 MISS
    0 	   116 	 0.00027 	 0.27718 	 MISS
    0 	   117 	 0.00027 	 0.27825 	 MISS
    0 	   118 	 0.00027 	 0.28145 	 MISS
    0 	   119 	 0.00027 	 0.28272 	 MISS
    0 	   120 	 0.00027 	 0.28727 	 MISS
==========================================
r_mrr = 9.17217377605084e-08
r2_mrr = -1.0035035610198975
spearmanr_mrr@5 = nan
spearmanr_mrr@10 = 6.55794394788245e-07
spearmanr_mrr@50 = nan
spearmanr_mrr@100 = nan
spearmanr_mrr@All = 1.1210435246766792e-07
==========================================
test time: 0.433
Done Testing dataset OpenEA
total time taken: 214.05606174468994
training time taken: 198.45510625839233
TWIG out ;))
Ablation done!
The best results were: None
The best settings found were:

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 3462737707435580
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [348, 587, 690, 422, 1087, 1113, 1102, 1070, 718, 1007, 818, 283, 543, 788, 354, 466, 450, 974, 11, 581, 1058, 1153, 1176, 731, 429, 1027, 252, 617, 1049, 10, 656, 2, 1096, 414, 1040, 472, 349, 203, 573, 574, 179, 223, 1045, 195, 343, 126, 138, 971, 1152, 534, 924, 779, 630, 28, 449, 793, 680, 186, 318, 471, 438, 668, 1095, 1012, 127, 250, 1154, 567, 726, 739, 143, 684, 1061, 1100, 536, 397, 334, 856, 1130, 29, 813, 180, 377, 838, 1026, 965, 358, 26, 106, 91, 57, 1125, 744, 707, 613, 82, 745, 5, 524, 647, 237, 828, 798, 385, 817, 1201, 485, 141, 895, 125, 1075, 976, 132, 1134, 792, 981, 352, 593, 622, 37, 1167]
valid_ids (0): []
train_ids (1094): [662, 113, 72, 735, 599, 746, 420, 41, 124, 1029, 807, 13, 987, 251, 521, 430, 275, 749, 269, 474, 929, 810, 761, 315, 796, 845, 277, 1000, 140, 952, 879, 620, 665, 253, 435, 486, 356, 776, 1107, 46, 948, 339, 116, 1207, 270, 1002, 852, 464, 220, 691, 1074, 1023, 1052, 395, 708, 706, 728, 689, 338, 766, 1137, 679, 403, 234, 335, 468, 479, 58, 565, 1047, 850, 1011, 607, 591, 263, 712, 325, 985, 546, 932, 207, 511, 115, 1043, 871, 644, 1164, 145, 173, 1031, 612, 866, 1132, 463, 922, 1206, 271, 60, 304, 507, 713, 477, 226, 980, 1156, 1051, 864, 205, 1163, 715, 440, 942, 289, 1073, 246, 475, 88, 762, 357, 15, 646, 111, 104, 437, 164, 258, 421, 905, 369, 160, 146, 936, 148, 944, 73, 518, 215, 1035, 632, 1092, 447, 50, 875, 200, 383, 688, 957, 753, 1099, 98, 1112, 213, 280, 1090, 506, 272, 137, 884, 1006, 488, 1148, 1146, 703, 777, 19, 681, 1055, 337, 306, 1143, 156, 915, 109, 541, 1003, 917, 392, 1105, 405, 309, 25, 659, 448, 526, 557, 729, 963, 227, 1205, 151, 281, 1120, 727, 451, 56, 1180, 881, 692, 638, 578, 1089, 470, 925, 624, 804, 1067, 797, 682, 87, 457, 547, 501, 598, 854, 674, 1122, 769, 240, 569, 803, 938, 1, 121, 428, 224, 1159, 673, 1157, 190, 800, 108, 1078, 34, 1028, 621, 552, 120, 157, 697, 278, 669, 637, 577, 633, 1128, 542, 161, 778, 1084, 1165, 836, 3, 874, 594, 103, 433, 855, 815, 738, 782, 1129, 678, 0, 896, 685, 123, 310, 1133, 68, 666, 892, 867, 409, 331, 602, 634, 1119, 719, 1021, 711, 802, 248, 1138, 4, 527, 386, 606, 1179, 1140, 554, 105, 490, 494, 610, 1004, 49, 130, 259, 355, 698, 912, 76, 496, 314, 1068, 805, 1053, 1033, 336, 24, 163, 1145, 222, 1185, 1196, 1010, 840, 27, 467, 441, 580, 84, 1189, 321, 588, 973, 517, 1014, 85, 366, 69, 919, 902, 904, 520, 1039, 273, 282, 305, 63, 891, 1093, 102, 516, 954, 562, 893, 575, 332, 460, 1079, 975, 945, 1136, 8, 242, 795, 823, 966, 481, 1178, 1135, 509, 185, 529, 316, 1103, 604, 1057, 424, 625, 843, 302, 298, 461, 772, 211, 771, 265, 899, 671, 616, 1104, 476, 780, 510, 301, 675, 1115, 709, 950, 653, 131, 523, 255, 327, 375, 1085, 900, 1169, 172, 279, 364, 216, 1204, 903, 379, 993, 655, 531, 672, 81, 1117, 576, 89, 806, 737, 1081, 1032, 841, 353, 20, 889, 225, 670, 1025, 221, 906, 754, 747, 142, 821, 152, 725, 897, 308, 695, 911, 947, 1036, 787, 834, 781, 1160, 489, 398, 401, 287, 652, 373, 1094, 916, 419, 44, 333, 544, 826, 601, 733, 360, 590, 631, 714, 851, 1155, 410, 1088, 734, 824, 515, 989, 540, 372, 293, 36, 898, 1202, 362, 848, 908, 859, 1059, 799, 537, 928, 495, 14, 59, 636, 999, 960, 1181, 230, 1060, 940, 110, 654, 721, 442, 595, 561, 1101, 784, 1038, 808, 344, 538, 35, 786, 1149, 701, 342, 951, 872, 31, 645, 52, 93, 499, 346, 558, 505, 564, 21, 70, 720, 370, 231, 23, 1212, 857, 432, 687, 865, 415, 789, 1191, 1083, 615, 64, 700, 380, 43, 175, 1192, 243, 1172, 459, 1211, 870, 1168, 994, 1147, 443, 758, 514, 849, 1210, 677, 371, 641, 862, 1080, 1194, 791, 750, 394, 74, 1126, 907, 444, 560, 427, 991, 133, 820, 704, 159, 266, 453, 436, 400, 365, 785, 241, 502, 969, 997, 291, 949, 1144, 664, 54, 100, 986, 260, 1158, 320, 307, 760, 1097, 295, 38, 229, 1187, 946, 608, 18, 303, 261, 535, 7, 53, 877, 292, 86, 964, 445, 847, 732, 1008, 118, 579, 387, 267, 559, 30, 12, 1173, 1009, 1062, 9, 40, 51, 742, 473, 519, 497, 801, 17, 770, 378, 978, 1141, 219, 883, 1013, 1124, 1108, 584, 193, 1109, 183, 1034, 1024, 551, 264, 326, 650, 553, 247, 830, 767, 995, 284, 723, 702, 484, 286, 117, 811, 33, 533, 341, 1042, 239, 45, 1213, 351, 210, 773, 941, 97, 933, 456, 218, 605, 206, 254, 194, 563, 1063, 837, 139, 136, 176, 667, 724, 1200, 550, 894, 431, 831, 888, 626, 736, 743, 972, 683, 640, 609, 1199, 1175, 775, 408, 890, 262, 794, 530, 763, 1183, 934, 513, 412, 833, 937, 465, 149, 959, 827, 571, 589, 319, 860, 1195, 618, 66, 730, 1098, 740, 382, 256, 1030, 492, 62, 885, 328, 1170, 1069, 1188, 909, 6, 212, 67, 487, 882, 78, 858, 299, 135, 174, 1182, 196, 1005, 1131, 312, 1116, 452, 710, 619, 480, 955, 918, 592, 1022, 1150, 99, 313, 979, 661, 1076, 1151, 92, 868, 233, 77, 404, 192, 1214, 235, 71, 764, 658, 829, 390, 114, 570, 413, 198, 819, 290, 388, 887, 330, 144, 417, 178, 853, 628, 55, 759, 1127, 503, 112, 783, 300, 95, 921, 170, 648, 583, 340, 80, 90, 238, 825, 705, 322, 1066, 717, 1114, 676, 1086, 1121, 627, 878, 166, 699, 285, 512, 439, 696, 426, 482, 158, 268, 914, 32, 276, 177, 167, 107, 1041, 317, 693, 182, 1190, 236, 1177, 1015, 968, 469, 1001, 943, 751, 491, 359, 1017, 1050, 1209, 1016, 367, 1174, 545, 329, 774, 150, 154, 741, 660, 1110, 1171, 752, 585, 901, 643, 931, 381, 582, 168, 835, 376, 1203, 572, 1019, 982, 839, 930, 249, 47, 165, 504, 294, 549, 923, 956, 1077, 716, 454, 809, 996, 181, 926, 399, 402, 478, 147, 539, 311, 48, 939, 844, 217, 962, 756, 548, 556, 927, 83, 406, 1071, 984, 425, 155, 209, 347, 1056, 913, 61, 184, 187, 297, 1106, 350, 389, 1048, 768, 323, 920, 863, 635, 816, 384, 345, 1193, 197, 169, 416, 1046, 748, 649, 992, 22, 876, 1082, 274, 79, 757, 411, 861, 961, 953, 983, 1020, 603, 458, 1118, 204, 597, 765, 1166, 16, 361, 967, 566, 935, 288, 1037, 418, 1111, 232, 988, 391, 122, 191, 958, 188, 880, 1184, 1091, 998, 910, 555, 722, 245, 94, 201, 39, 686, 639, 694, 600, 832, 842, 483, 508, 990, 101, 1054, 257, 651, 434, 202, 629, 886, 657, 423, 1123, 462, 873, 119, 790, 1072, 96, 244, 65, 1018, 393, 663, 128, 296, 977, 1161, 396, 596, 134, 363, 522, 525, 611, 1197, 1064, 446, 42, 208, 498, 532, 75, 846, 623, 324, 162, 214, 970, 528, 1186, 1139, 1044, 1142, 228, 129, 199, 869, 614, 755, 812, 171, 1208, 642, 493, 368, 455, 568, 500, 1065, 407, 586, 1198, 814, 189, 374, 153, 822, 1162]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7560007509420446
the save name prefix for this run is:  chkpt-ID_7560007509420446_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1118
rank avg (pred): 0.555 +- 0.003
mrr vals (pred, true): 0.000, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002088823

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 893
rank avg (pred): 0.149 +- 0.102
mrr vals (pred, true): 0.065, 0.057
batch losses (mrrl, rdl): 0.0, 9.27188e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 655
rank avg (pred): 0.350 +- 0.269
mrr vals (pred, true): 0.172, 0.006
batch losses (mrrl, rdl): 0.0, 0.0002263648

Epoch over!
epoch time: 12.174

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1073
rank avg (pred): 0.038 +- 0.031
mrr vals (pred, true): 0.285, 0.215
batch losses (mrrl, rdl): 0.0, 1.12019e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 67
rank avg (pred): 0.072 +- 0.062
mrr vals (pred, true): 0.303, 0.130
batch losses (mrrl, rdl): 0.0, 9.0865e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 145
rank avg (pred): 0.334 +- 0.294
mrr vals (pred, true): 0.274, 0.020
batch losses (mrrl, rdl): 0.0, 0.0003088017

Epoch over!
epoch time: 11.875

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 812
rank avg (pred): 0.040 +- 0.037
mrr vals (pred, true): 0.380, 0.059
batch losses (mrrl, rdl): 0.0, 4.0631e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 458
rank avg (pred): 0.316 +- 0.291
mrr vals (pred, true): 0.274, 0.006
batch losses (mrrl, rdl): 0.0, 0.0003351766

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 510
rank avg (pred): 0.198 +- 0.190
mrr vals (pred, true): 0.237, 0.074
batch losses (mrrl, rdl): 0.0, 3.71474e-05

Epoch over!
epoch time: 11.915

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 767
rank avg (pred): 0.350 +- 0.306
mrr vals (pred, true): 0.167, 0.097
batch losses (mrrl, rdl): 0.0, 0.0002432423

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 823
rank avg (pred): 0.041 +- 0.042
mrr vals (pred, true): 0.401, 0.142
batch losses (mrrl, rdl): 0.0, 2.6113e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 907
rank avg (pred): 0.062 +- 0.065
mrr vals (pred, true): 0.406, 0.114
batch losses (mrrl, rdl): 0.0, 9.9834e-06

Epoch over!
epoch time: 11.85

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 60
rank avg (pred): 0.090 +- 0.084
mrr vals (pred, true): 0.346, 0.065
batch losses (mrrl, rdl): 0.0, 2.73141e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 920
rank avg (pred): 0.374 +- 0.307
mrr vals (pred, true): 0.188, 0.096
batch losses (mrrl, rdl): 0.0, 0.0003009066

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 365
rank avg (pred): 0.288 +- 0.297
mrr vals (pred, true): 0.382, 0.066
batch losses (mrrl, rdl): 0.0, 0.0003176871

Epoch over!
epoch time: 11.928

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1029
rank avg (pred): 0.335 +- 0.303
mrr vals (pred, true): 0.184, 0.007
batch losses (mrrl, rdl): 0.1799462736, 0.0002691214

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 611
rank avg (pred): 0.583 +- 0.181
mrr vals (pred, true): 0.048, 0.032
batch losses (mrrl, rdl): 2.35596e-05, 0.0015180067

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 418
rank avg (pred): 0.529 +- 0.201
mrr vals (pred, true): 0.052, 0.007
batch losses (mrrl, rdl): 2.52499e-05, 0.0001234045

Epoch over!
epoch time: 12.211

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 55
rank avg (pred): 0.173 +- 0.112
mrr vals (pred, true): 0.136, 0.086
batch losses (mrrl, rdl): 0.0732464641, 0.0001923417

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 21
rank avg (pred): 0.161 +- 0.101
mrr vals (pred, true): 0.125, 0.108
batch losses (mrrl, rdl): 0.0028486717, 0.000117632

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 961
rank avg (pred): 0.470 +- 0.188
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 3.91217e-05, 4.52898e-05

Epoch over!
epoch time: 12.137

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 156
rank avg (pred): 0.479 +- 0.176
mrr vals (pred, true): 0.049, 0.015
batch losses (mrrl, rdl): 9.4908e-06, 0.0011067962

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 999
rank avg (pred): 0.412 +- 0.218
mrr vals (pred, true): 0.093, 0.056
batch losses (mrrl, rdl): 0.0181550179, 0.0009673375

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 894
rank avg (pred): 0.377 +- 0.200
mrr vals (pred, true): 0.062, 0.085
batch losses (mrrl, rdl): 0.0013644284, 0.0021439302

Epoch over!
epoch time: 12.116

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 207
rank avg (pred): 0.444 +- 0.156
mrr vals (pred, true): 0.041, 0.006
batch losses (mrrl, rdl): 0.0008503706, 9.26512e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 364
rank avg (pred): 0.426 +- 0.152
mrr vals (pred, true): 0.049, 0.032
batch losses (mrrl, rdl): 1.32099e-05, 0.0009399996

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 927
rank avg (pred): 0.398 +- 0.160
mrr vals (pred, true): 0.044, 0.098
batch losses (mrrl, rdl): 0.0003689434, 0.000444186

Epoch over!
epoch time: 12.292

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 528
rank avg (pred): 0.361 +- 0.178
mrr vals (pred, true): 0.050, 0.061
batch losses (mrrl, rdl): 4.989e-07, 0.0005578831

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 899
rank avg (pred): 0.266 +- 0.147
mrr vals (pred, true): 0.066, 0.105
batch losses (mrrl, rdl): 0.0148799103, 0.0008125843

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 996
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.307, 0.287
batch losses (mrrl, rdl): 0.0041335179, 2.45892e-05

Epoch over!
epoch time: 12.142

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1111
rank avg (pred): 0.353 +- 0.171
mrr vals (pred, true): 0.063, 0.006
batch losses (mrrl, rdl): 0.0016604436, 0.000340908

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 84
rank avg (pred): 0.381 +- 0.153
mrr vals (pred, true): 0.044, 0.012
batch losses (mrrl, rdl): 0.0003123826, 0.0002706252

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1018
rank avg (pred): 0.340 +- 0.175
mrr vals (pred, true): 0.057, 0.057
batch losses (mrrl, rdl): 0.0005034909, 0.0004527524

Epoch over!
epoch time: 12.356

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 980
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.253, 0.292
batch losses (mrrl, rdl): 0.0149873625, 2.07029e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1035
rank avg (pred): 0.350 +- 0.169
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 3.54882e-05, 0.0003732123

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 394
rank avg (pred): 0.369 +- 0.159
mrr vals (pred, true): 0.046, 0.035
batch losses (mrrl, rdl): 0.0001420154, 0.0005880749

Epoch over!
epoch time: 12.06

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 747
rank avg (pred): 0.160 +- 0.095
mrr vals (pred, true): 0.135, 0.088
batch losses (mrrl, rdl): 0.0720733032, 0.0002476858

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 899
rank avg (pred): 0.176 +- 0.103
mrr vals (pred, true): 0.071, 0.105
batch losses (mrrl, rdl): 0.0110691683, 0.0002298687

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 420
rank avg (pred): 0.353 +- 0.170
mrr vals (pred, true): 0.047, 0.007
batch losses (mrrl, rdl): 6.95879e-05, 0.0003344352

Epoch over!
epoch time: 12.067

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 152
rank avg (pred): 0.353 +- 0.178
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 9.6177e-06, 0.0005543026

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 217
rank avg (pred): 0.353 +- 0.179
mrr vals (pred, true): 0.052, 0.006
batch losses (mrrl, rdl): 3.78182e-05, 0.0003431735

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 274
rank avg (pred): 0.173 +- 0.096
mrr vals (pred, true): 0.084, 0.072
batch losses (mrrl, rdl): 0.0115261143, 0.0001335631

Epoch over!
epoch time: 11.991

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 957
rank avg (pred): 0.362 +- 0.181
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 7.40774e-05, 0.0003113704

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1117
rank avg (pred): 0.331 +- 0.174
mrr vals (pred, true): 0.056, 0.006
batch losses (mrrl, rdl): 0.0003652404, 0.0004757596

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 939
rank avg (pred): 0.358 +- 0.187
mrr vals (pred, true): 0.051, 0.097
batch losses (mrrl, rdl): 2.24918e-05, 0.0002318988

Epoch over!
epoch time: 12.25

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.359 +- 0.185
mrr vals (pred, true): 0.052, 0.022

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.04926 	 0.00575 	 m..s
   59 	     1 	 0.05017 	 0.00597 	 m..s
   19 	     2 	 0.04918 	 0.00606 	 m..s
   21 	     3 	 0.04922 	 0.00613 	 m..s
   17 	     4 	 0.04918 	 0.00613 	 m..s
   38 	     5 	 0.04947 	 0.00614 	 m..s
   50 	     6 	 0.04969 	 0.00619 	 m..s
   12 	     7 	 0.04916 	 0.00631 	 m..s
   20 	     8 	 0.04920 	 0.00631 	 m..s
   16 	     9 	 0.04917 	 0.00634 	 m..s
   35 	    10 	 0.04944 	 0.00638 	 m..s
   60 	    11 	 0.05021 	 0.00638 	 m..s
   71 	    12 	 0.05193 	 0.00638 	 m..s
  100 	    13 	 0.08613 	 0.00640 	 m..s
   57 	    14 	 0.04975 	 0.00640 	 m..s
   18 	    15 	 0.04918 	 0.00643 	 m..s
   79 	    16 	 0.05367 	 0.00645 	 m..s
   22 	    17 	 0.04924 	 0.00645 	 m..s
   28 	    18 	 0.04935 	 0.00645 	 m..s
   48 	    19 	 0.04967 	 0.00647 	 m..s
   42 	    20 	 0.04954 	 0.00647 	 m..s
   63 	    21 	 0.05053 	 0.00648 	 m..s
    5 	    22 	 0.04878 	 0.00648 	 m..s
   74 	    23 	 0.05245 	 0.00649 	 m..s
   30 	    24 	 0.04936 	 0.00650 	 m..s
    6 	    25 	 0.04893 	 0.00651 	 m..s
   51 	    26 	 0.04970 	 0.00651 	 m..s
   13 	    27 	 0.04917 	 0.00652 	 m..s
   52 	    28 	 0.04971 	 0.00653 	 m..s
   64 	    29 	 0.05054 	 0.00653 	 m..s
   72 	    30 	 0.05195 	 0.00653 	 m..s
   40 	    31 	 0.04950 	 0.00654 	 m..s
    8 	    32 	 0.04894 	 0.00655 	 m..s
   41 	    33 	 0.04950 	 0.00656 	 m..s
   44 	    34 	 0.04957 	 0.00663 	 m..s
   68 	    35 	 0.05157 	 0.00665 	 m..s
   27 	    36 	 0.04935 	 0.00668 	 m..s
   47 	    37 	 0.04964 	 0.00668 	 m..s
    1 	    38 	 0.04838 	 0.00670 	 m..s
   62 	    39 	 0.05037 	 0.00671 	 m..s
    4 	    40 	 0.04877 	 0.00825 	 m..s
    3 	    41 	 0.04874 	 0.00867 	 m..s
   15 	    42 	 0.04917 	 0.00950 	 m..s
   31 	    43 	 0.04936 	 0.01113 	 m..s
    7 	    44 	 0.04893 	 0.01134 	 m..s
   29 	    45 	 0.04936 	 0.01169 	 m..s
   14 	    46 	 0.04917 	 0.01171 	 m..s
   24 	    47 	 0.04930 	 0.01451 	 m..s
   33 	    48 	 0.04943 	 0.01523 	 m..s
   36 	    49 	 0.04945 	 0.01664 	 m..s
   10 	    50 	 0.04909 	 0.01758 	 m..s
   11 	    51 	 0.04913 	 0.01990 	 ~...
   49 	    52 	 0.04967 	 0.02062 	 ~...
   43 	    53 	 0.04954 	 0.02136 	 ~...
   45 	    54 	 0.04960 	 0.02153 	 ~...
   69 	    55 	 0.05163 	 0.02198 	 ~...
   54 	    56 	 0.04972 	 0.02595 	 ~...
   53 	    57 	 0.04971 	 0.02938 	 ~...
   55 	    58 	 0.04972 	 0.02954 	 ~...
   58 	    59 	 0.04978 	 0.03180 	 ~...
   34 	    60 	 0.04943 	 0.03397 	 ~...
   26 	    61 	 0.04932 	 0.03429 	 ~...
   56 	    62 	 0.04972 	 0.03445 	 ~...
   46 	    63 	 0.04960 	 0.03491 	 ~...
   78 	    64 	 0.05279 	 0.03526 	 ~...
   76 	    65 	 0.05266 	 0.03581 	 ~...
   32 	    66 	 0.04941 	 0.03625 	 ~...
   39 	    67 	 0.04949 	 0.03682 	 ~...
   66 	    68 	 0.05095 	 0.03837 	 ~...
   37 	    69 	 0.04946 	 0.03990 	 ~...
   73 	    70 	 0.05206 	 0.05011 	 ~...
  105 	    71 	 0.09874 	 0.05035 	 m..s
   81 	    72 	 0.05453 	 0.05635 	 ~...
   82 	    73 	 0.05464 	 0.05645 	 ~...
   67 	    74 	 0.05107 	 0.06486 	 ~...
   80 	    75 	 0.05386 	 0.06528 	 ~...
   61 	    76 	 0.05021 	 0.06589 	 ~...
   84 	    77 	 0.05717 	 0.06665 	 ~...
   83 	    78 	 0.05679 	 0.06971 	 ~...
   70 	    79 	 0.05187 	 0.07453 	 ~...
   91 	    80 	 0.07335 	 0.08407 	 ~...
   90 	    81 	 0.07286 	 0.08425 	 ~...
    0 	    82 	 0.04837 	 0.08434 	 m..s
   75 	    83 	 0.05253 	 0.08633 	 m..s
   96 	    84 	 0.07789 	 0.08724 	 ~...
   94 	    85 	 0.07484 	 0.08818 	 ~...
   65 	    86 	 0.05085 	 0.08995 	 m..s
  102 	    87 	 0.08844 	 0.09087 	 ~...
  103 	    88 	 0.09143 	 0.09178 	 ~...
    9 	    89 	 0.04901 	 0.09700 	 m..s
    2 	    90 	 0.04855 	 0.09751 	 m..s
   25 	    91 	 0.04931 	 0.09868 	 m..s
   77 	    92 	 0.05275 	 0.10170 	 m..s
   85 	    93 	 0.06230 	 0.10268 	 m..s
   92 	    94 	 0.07372 	 0.10441 	 m..s
   88 	    95 	 0.06814 	 0.10453 	 m..s
   97 	    96 	 0.07799 	 0.10534 	 ~...
   95 	    97 	 0.07692 	 0.10611 	 ~...
   89 	    98 	 0.07068 	 0.10948 	 m..s
  104 	    99 	 0.09284 	 0.11108 	 ~...
   98 	   100 	 0.07894 	 0.11538 	 m..s
   86 	   101 	 0.06516 	 0.12139 	 m..s
  107 	   102 	 0.14855 	 0.12209 	 ~...
  101 	   103 	 0.08755 	 0.12399 	 m..s
   87 	   104 	 0.06530 	 0.12635 	 m..s
   93 	   105 	 0.07461 	 0.13066 	 m..s
  106 	   106 	 0.10330 	 0.13362 	 m..s
   99 	   107 	 0.08303 	 0.14474 	 m..s
  109 	   108 	 0.16460 	 0.15848 	 ~...
  108 	   109 	 0.16110 	 0.16628 	 ~...
  110 	   110 	 0.17507 	 0.18723 	 ~...
  111 	   111 	 0.20884 	 0.21619 	 ~...
  112 	   112 	 0.20894 	 0.21638 	 ~...
  116 	   113 	 0.24351 	 0.21914 	 ~...
  113 	   114 	 0.21718 	 0.24189 	 ~...
  114 	   115 	 0.22182 	 0.25509 	 m..s
  115 	   116 	 0.23442 	 0.27280 	 m..s
  118 	   117 	 0.25349 	 0.27652 	 ~...
  117 	   118 	 0.25291 	 0.27828 	 ~...
  119 	   119 	 0.27917 	 0.29087 	 ~...
  120 	   120 	 0.28981 	 0.29402 	 ~...
==========================================
r_mrr = 0.904299795627594
r2_mrr = 0.7676472663879395
spearmanr_mrr@5 = 0.9957716464996338
spearmanr_mrr@10 = 0.950072705745697
spearmanr_mrr@50 = 0.988694965839386
spearmanr_mrr@100 = 0.9373763203620911
spearmanr_mrr@All = 0.9324875473976135
==========================================
test time: 0.387
Done Testing dataset OpenEA
total time taken: 197.13214683532715
training time taken: 181.83762574195862
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.9043)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7676)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9958)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9501)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9887)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9374)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9325)}}, 'test_loss': {'TransE': {'OpenEA': 0.4616242308256915}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 5801956104574769
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [653, 860, 612, 314, 240, 198, 819, 1207, 303, 538, 39, 886, 118, 844, 1138, 812, 65, 1020, 654, 823, 813, 7, 1146, 940, 918, 722, 1071, 1101, 1127, 943, 545, 24, 779, 488, 321, 710, 412, 241, 824, 977, 143, 1171, 432, 145, 738, 1130, 1191, 349, 727, 376, 973, 329, 517, 778, 1210, 360, 846, 553, 929, 1068, 705, 1095, 1148, 262, 319, 324, 556, 19, 286, 298, 641, 433, 856, 110, 1211, 224, 231, 78, 10, 476, 362, 68, 96, 58, 658, 739, 116, 648, 993, 661, 481, 15, 1116, 593, 1155, 273, 884, 948, 1136, 1160, 22, 1159, 1150, 1175, 135, 250, 112, 177, 358, 740, 1198, 875, 1126, 104, 798, 913, 81, 1031, 88, 1084, 868]
valid_ids (0): []
train_ids (1094): [902, 1063, 499, 1214, 1162, 14, 881, 622, 313, 745, 200, 185, 749, 1205, 683, 951, 939, 228, 797, 946, 893, 459, 652, 861, 270, 513, 1038, 1061, 930, 239, 344, 138, 410, 1104, 1118, 53, 698, 46, 578, 1196, 417, 777, 74, 463, 729, 1152, 614, 113, 20, 995, 1081, 1112, 328, 1115, 223, 703, 184, 796, 560, 63, 3, 910, 1135, 139, 934, 368, 528, 34, 597, 841, 1074, 310, 1145, 464, 1089, 157, 1111, 781, 204, 478, 715, 170, 1097, 446, 985, 1092, 523, 912, 221, 408, 697, 857, 1174, 941, 374, 889, 950, 832, 132, 592, 4, 984, 471, 1088, 381, 491, 390, 126, 516, 642, 305, 1064, 1085, 706, 936, 510, 456, 987, 965, 916, 404, 1093, 1076, 140, 816, 772, 770, 594, 531, 547, 67, 655, 477, 980, 549, 551, 1161, 76, 274, 758, 820, 422, 976, 308, 264, 101, 169, 1086, 828, 233, 611, 1042, 188, 807, 561, 982, 855, 1046, 931, 259, 426, 159, 769, 77, 685, 1208, 581, 66, 1029, 427, 1098, 890, 1033, 1035, 1187, 786, 1166, 75, 290, 175, 263, 60, 445, 507, 789, 768, 215, 567, 398, 450, 334, 955, 49, 315, 1034, 602, 373, 487, 41, 1040, 171, 862, 492, 983, 699, 386, 580, 880, 5, 585, 95, 800, 133, 539, 1186, 1066, 247, 219, 746, 1180, 48, 668, 1021, 162, 227, 1010, 1028, 13, 178, 93, 402, 356, 307, 391, 497, 586, 1128, 651, 428, 461, 482, 708, 0, 434, 106, 974, 158, 1080, 448, 914, 346, 971, 266, 355, 625, 701, 563, 990, 129, 922, 692, 92, 1, 335, 1057, 667, 662, 211, 474, 333, 256, 1007, 97, 557, 366, 532, 136, 1050, 351, 562, 850, 766, 265, 297, 283, 309, 94, 848, 811, 559, 453, 70, 921, 1099, 1056, 899, 1137, 436, 963, 127, 599, 1024, 757, 760, 629, 30, 666, 1058, 1179, 638, 217, 439, 89, 1140, 944, 1003, 485, 1121, 573, 679, 575, 782, 73, 623, 750, 595, 851, 111, 806, 898, 440, 268, 44, 42, 416, 484, 199, 361, 301, 479, 784, 245, 767, 1082, 588, 189, 71, 988, 103, 731, 790, 1001, 1009, 690, 1000, 659, 1107, 359, 455, 826, 393, 1078, 1192, 515, 252, 1144, 343, 700, 718, 1172, 647, 151, 258, 1185, 124, 1072, 325, 1168, 222, 1206, 957, 128, 107, 892, 192, 609, 424, 645, 236, 814, 1036, 480, 534, 163, 1132, 707, 1164, 568, 392, 338, 509, 882, 603, 379, 1125, 146, 799, 591, 649, 958, 587, 1193, 1197, 1123, 792, 1065, 260, 206, 1070, 780, 318, 415, 395, 1043, 815, 454, 646, 1190, 878, 529, 883, 458, 37, 730, 733, 656, 870, 421, 267, 933, 1077, 570, 414, 747, 1067, 947, 923, 762, 524, 743, 967, 134, 377, 1044, 583, 238, 845, 1079, 340, 430, 1204, 1049, 952, 1200, 681, 279, 311, 992, 871, 207, 1142, 300, 119, 613, 33, 339, 1014, 837, 213, 617, 407, 1060, 558, 776, 680, 122, 387, 153, 1165, 907, 804, 342, 1102, 840, 399, 716, 801, 1018, 775, 388, 214, 1139, 888, 571, 350, 576, 537, 341, 803, 550, 858, 664, 981, 326, 879, 1114, 773, 90, 425, 149, 520, 131, 1188, 1026, 903, 624, 29, 853, 353, 809, 1117, 953, 1131, 689, 755, 911, 438, 540, 925, 28, 367, 1189, 272, 1016, 1203, 917, 242, 320, 8, 754, 117, 842, 1090, 835, 927, 596, 928, 100, 304, 915, 831, 548, 744, 244, 942, 442, 62, 966, 1032, 369, 32, 839, 1048, 1041, 12, 142, 997, 634, 536, 821, 905, 35, 1108, 210, 1017, 682, 9, 1153, 843, 187, 678, 579, 87, 1199, 150, 633, 695, 429, 1170, 232, 23, 726, 765, 357, 1201, 409, 261, 774, 628, 237, 643, 1213, 80, 449, 873, 723, 891, 444, 759, 732, 1052, 383, 31, 1184, 64, 55, 630, 1122, 56, 156, 1030, 371, 486, 709, 589, 869, 504, 1141, 872, 51, 605, 1091, 637, 1012, 27, 1096, 544, 737, 500, 1022, 1051, 670, 618, 166, 527, 793, 574, 1015, 191, 1013, 394, 543, 401, 1173, 50, 179, 616, 287, 45, 1109, 834, 639, 864, 694, 431, 312, 938, 1045, 909, 203, 181, 1129, 631, 569, 11, 564, 229, 54, 1133, 601, 288, 1154, 867, 234, 1124, 714, 959, 209, 174, 230, 1027, 382, 818, 771, 610, 194, 475, 669, 202, 226, 535, 437, 721, 640, 734, 155, 1195, 251, 173, 460, 742, 6, 753, 822, 673, 384, 172, 193, 671, 626, 322, 720, 180, 121, 735, 787, 176, 908, 59, 494, 1176, 761, 291, 937, 1181, 473, 829, 403, 691, 863, 25, 866, 375, 420, 498, 675, 141, 489, 483, 712, 584, 152, 741, 830, 490, 43, 82, 1113, 400, 525, 1177, 496, 600, 1059, 462, 900, 541, 752, 493, 1073, 852, 364, 827, 472, 397, 802, 975, 518, 1134, 1039, 1156, 91, 216, 69, 165, 302, 277, 465, 469, 190, 197, 495, 748, 632, 865, 249, 330, 713, 269, 503, 336, 552, 650, 299, 725, 61, 347, 808, 1006, 317, 764, 389, 1143, 137, 756, 1110, 332, 998, 621, 457, 447, 316, 423, 1103, 904, 751, 130, 989, 606, 345, 275, 144, 1158, 590, 294, 289, 443, 1047, 411, 530, 969, 526, 235, 212, 859, 1025, 105, 26, 196, 788, 608, 1209, 704, 86, 546, 501, 183, 719, 84, 1183, 847, 791, 1194, 566, 354, 108, 254, 182, 348, 665, 413, 962, 895, 785, 1169, 278, 164, 452, 1157, 1106, 817, 1037, 406, 1202, 1100, 38, 419, 378, 810, 99, 577, 924, 280, 996, 40, 293, 468, 160, 964, 627, 565, 218, 887, 805, 1094, 257, 21, 147, 763, 370, 702, 385, 148, 1151, 876, 120, 285, 968, 16, 125, 663, 674, 514, 994, 208, 572, 380, 874, 636, 243, 961, 932, 1120, 795, 838, 396, 906, 1055, 337, 79, 687, 711, 1005, 919, 644, 72, 1147, 186, 693, 1182, 620, 123, 615, 978, 896, 522, 1119, 1053, 306, 195, 672, 435, 972, 1019, 619, 295, 225, 418, 83, 502, 466, 1087, 470, 877, 783, 1075, 1163, 255, 154, 677, 970, 323, 935, 467, 47, 949, 598, 686, 102, 885, 926, 441, 542, 954, 246, 728, 635, 511, 201, 161, 114, 115, 284, 1054, 1212, 901, 331, 849, 248, 999, 18, 897, 292, 555, 991, 1178, 220, 854, 945, 894, 98, 363, 521, 372, 688, 281, 508, 205, 282, 1083, 109, 451, 533, 1004, 405, 296, 1149, 607, 512, 684, 825, 2, 1011, 582, 271, 505, 657, 519, 736, 960, 979, 836, 676, 554, 1105, 85, 167, 352, 327, 794, 1062, 1069, 1002, 1167, 506, 696, 604, 920, 365, 986, 17, 168, 1008, 57, 833, 52, 717, 1023, 724, 956, 276, 660, 36, 253]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2172405729290142
the save name prefix for this run is:  chkpt-ID_2172405729290142_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 791
rank avg (pred): 0.587 +- 0.007
mrr vals (pred, true): 0.000, 0.007
batch losses (mrrl, rdl): 0.0, 0.0003279284

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 184
rank avg (pred): 0.361 +- 0.224
mrr vals (pred, true): 0.111, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002330462

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 608
rank avg (pred): 0.336 +- 0.274
mrr vals (pred, true): 0.239, 0.030
batch losses (mrrl, rdl): 0.0, 5.19849e-05

Epoch over!
epoch time: 12.02

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1028
rank avg (pred): 0.357 +- 0.276
mrr vals (pred, true): 0.225, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001761489

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 815
rank avg (pred): 0.034 +- 0.033
mrr vals (pred, true): 0.367, 0.069
batch losses (mrrl, rdl): 0.0, 1.19639e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 192
rank avg (pred): 0.357 +- 0.304
mrr vals (pred, true): 0.295, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001832036

Epoch over!
epoch time: 11.979

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 332
rank avg (pred): 0.296 +- 0.280
mrr vals (pred, true): 0.342, 0.058
batch losses (mrrl, rdl): 0.0, 0.0003025887

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 557
rank avg (pred): 0.164 +- 0.155
mrr vals (pred, true): 0.357, 0.101
batch losses (mrrl, rdl): 0.0, 4.50693e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 3
rank avg (pred): 0.087 +- 0.082
mrr vals (pred, true): 0.358, 0.073
batch losses (mrrl, rdl): 0.0, 2.21957e-05

Epoch over!
epoch time: 11.879

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 573
rank avg (pred): 0.363 +- 0.314
mrr vals (pred, true): 0.310, 0.008
batch losses (mrrl, rdl): 0.0, 3.81288e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 117
rank avg (pred): 0.303 +- 0.292
mrr vals (pred, true): 0.356, 0.013
batch losses (mrrl, rdl): 0.0, 9.47456e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1212
rank avg (pred): 0.407 +- 0.322
mrr vals (pred, true): 0.254, 0.006
batch losses (mrrl, rdl): 0.0, 5.036e-05

Epoch over!
epoch time: 11.801

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 383
rank avg (pred): 0.319 +- 0.295
mrr vals (pred, true): 0.342, 0.055
batch losses (mrrl, rdl): 0.0, 0.0004138882

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 49
rank avg (pred): 0.069 +- 0.068
mrr vals (pred, true): 0.397, 0.179
batch losses (mrrl, rdl): 0.0, 8.2242e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1099
rank avg (pred): 0.315 +- 0.297
mrr vals (pred, true): 0.353, 0.072
batch losses (mrrl, rdl): 0.0, 0.0004446676

Epoch over!
epoch time: 11.782

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 807
rank avg (pred): 0.327 +- 0.304
mrr vals (pred, true): 0.349, 0.006
batch losses (mrrl, rdl): 0.8951724768, 0.0002815977

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1214
rank avg (pred): 0.304 +- 0.149
mrr vals (pred, true): 0.067, 0.006
batch losses (mrrl, rdl): 0.0028345287, 0.000723138

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 430
rank avg (pred): 0.394 +- 0.151
mrr vals (pred, true): 0.045, 0.007
batch losses (mrrl, rdl): 0.0002386773, 0.0001990803

Epoch over!
epoch time: 12.298

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 508
rank avg (pred): 0.365 +- 0.181
mrr vals (pred, true): 0.063, 0.093
batch losses (mrrl, rdl): 0.0015860348, 0.0007199792

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 350
rank avg (pred): 0.431 +- 0.200
mrr vals (pred, true): 0.072, 0.071
batch losses (mrrl, rdl): 0.0048051183, 0.0014190183

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 186
rank avg (pred): 0.403 +- 0.150
mrr vals (pred, true): 0.044, 0.006
batch losses (mrrl, rdl): 0.0003412135, 0.0001886899

Epoch over!
epoch time: 12.074

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 455
rank avg (pred): 0.383 +- 0.168
mrr vals (pred, true): 0.055, 0.006
batch losses (mrrl, rdl): 0.0002103327, 0.0002292869

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 770
rank avg (pred): 0.375 +- 0.146
mrr vals (pred, true): 0.043, 0.097
batch losses (mrrl, rdl): 0.000532514, 0.0003357038

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 557
rank avg (pred): 0.278 +- 0.146
mrr vals (pred, true): 0.061, 0.101
batch losses (mrrl, rdl): 0.0167450774, 0.0003206474

Epoch over!
epoch time: 12.062

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 267
rank avg (pred): 0.067 +- 0.046
mrr vals (pred, true): 0.110, 0.157
batch losses (mrrl, rdl): 0.0222899579, 2.22423e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 220
rank avg (pred): 0.368 +- 0.139
mrr vals (pred, true): 0.045, 0.006
batch losses (mrrl, rdl): 0.0002753492, 0.0003567765

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 852
rank avg (pred): 0.360 +- 0.150
mrr vals (pred, true): 0.052, 0.098
batch losses (mrrl, rdl): 3.90825e-05, 0.0002665294

Epoch over!
epoch time: 12.011

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1076
rank avg (pred): 0.024 +- 0.017
mrr vals (pred, true): 0.191, 0.278
batch losses (mrrl, rdl): 0.0766611397, 8.2279e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 721
rank avg (pred): 0.349 +- 0.142
mrr vals (pred, true): 0.046, 0.006
batch losses (mrrl, rdl): 0.0001596498, 0.0004416307

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 207
rank avg (pred): 0.346 +- 0.145
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 7.4594e-06, 0.0004751759

Epoch over!
epoch time: 12.137

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 151
rank avg (pred): 0.348 +- 0.140
mrr vals (pred, true): 0.046, 0.023
batch losses (mrrl, rdl): 0.0001284271, 0.0003173364

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 591
rank avg (pred): 0.323 +- 0.142
mrr vals (pred, true): 0.052, 0.011
batch losses (mrrl, rdl): 6.00152e-05, 9.36158e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1152
rank avg (pred): 0.203 +- 0.125
mrr vals (pred, true): 0.094, 0.105
batch losses (mrrl, rdl): 0.0012259567, 0.0001015745

Epoch over!
epoch time: 12.026

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 524
rank avg (pred): 0.284 +- 0.142
mrr vals (pred, true): 0.062, 0.103
batch losses (mrrl, rdl): 0.0161463991, 0.0003556604

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 589
rank avg (pred): 0.307 +- 0.141
mrr vals (pred, true): 0.049, 0.015
batch losses (mrrl, rdl): 5.876e-06, 0.0001024077

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1158
rank avg (pred): 0.087 +- 0.064
mrr vals (pred, true): 0.133, 0.110
batch losses (mrrl, rdl): 0.0056194924, 0.0001611566

Epoch over!
epoch time: 12.272

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 86
rank avg (pred): 0.339 +- 0.139
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 2.193e-07, 0.0003771752

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 692
rank avg (pred): 0.321 +- 0.137
mrr vals (pred, true): 0.042, 0.006
batch losses (mrrl, rdl): 0.000607181, 0.0005878548

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 493
rank avg (pred): 0.363 +- 0.138
mrr vals (pred, true): 0.051, 0.070
batch losses (mrrl, rdl): 2.24121e-05, 0.000653975

Epoch over!
epoch time: 12.328

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 456
rank avg (pred): 0.361 +- 0.127
mrr vals (pred, true): 0.043, 0.006
batch losses (mrrl, rdl): 0.0005228937, 0.0003438744

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 222
rank avg (pred): 0.345 +- 0.132
mrr vals (pred, true): 0.044, 0.006
batch losses (mrrl, rdl): 0.0003961478, 0.0004680358

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 983
rank avg (pred): 0.013 +- 0.009
mrr vals (pred, true): 0.273, 0.219
batch losses (mrrl, rdl): 0.02909787, 2.58859e-05

Epoch over!
epoch time: 12.114

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 438
rank avg (pred): 0.360 +- 0.126
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 0.0001013293, 0.0003701563

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 664
rank avg (pred): 0.328 +- 0.130
mrr vals (pred, true): 0.048, 0.006
batch losses (mrrl, rdl): 5.1047e-05, 0.0005631656

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 232
rank avg (pred): 0.340 +- 0.126
mrr vals (pred, true): 0.043, 0.007
batch losses (mrrl, rdl): 0.0005282145, 0.0005196946

Epoch over!
epoch time: 12.09

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.315 +- 0.130
mrr vals (pred, true): 0.048, 0.006

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.04582 	 0.00597 	 m..s
   14 	     1 	 0.04514 	 0.00600 	 m..s
   65 	     2 	 0.05459 	 0.00609 	 m..s
    2 	     3 	 0.04114 	 0.00613 	 m..s
   35 	     4 	 0.04700 	 0.00620 	 m..s
   18 	     5 	 0.04562 	 0.00621 	 m..s
   24 	     6 	 0.04582 	 0.00625 	 m..s
    8 	     7 	 0.04255 	 0.00626 	 m..s
   32 	     8 	 0.04681 	 0.00626 	 m..s
   34 	     9 	 0.04687 	 0.00630 	 m..s
   45 	    10 	 0.04844 	 0.00631 	 m..s
   55 	    11 	 0.05008 	 0.00632 	 m..s
    5 	    12 	 0.04182 	 0.00639 	 m..s
   58 	    13 	 0.05040 	 0.00642 	 m..s
   13 	    14 	 0.04511 	 0.00644 	 m..s
    3 	    15 	 0.04149 	 0.00646 	 m..s
   63 	    16 	 0.05453 	 0.00649 	 m..s
   72 	    17 	 0.05714 	 0.00649 	 m..s
   28 	    18 	 0.04636 	 0.00652 	 m..s
   49 	    19 	 0.04896 	 0.00652 	 m..s
   51 	    20 	 0.04907 	 0.00654 	 m..s
   46 	    21 	 0.04846 	 0.00655 	 m..s
   39 	    22 	 0.04748 	 0.00657 	 m..s
    1 	    23 	 0.04076 	 0.00657 	 m..s
   66 	    24 	 0.05490 	 0.00658 	 m..s
   53 	    25 	 0.04969 	 0.00659 	 m..s
   36 	    26 	 0.04705 	 0.00659 	 m..s
   33 	    27 	 0.04684 	 0.00660 	 m..s
   50 	    28 	 0.04898 	 0.00660 	 m..s
   40 	    29 	 0.04762 	 0.00660 	 m..s
   29 	    30 	 0.04640 	 0.00661 	 m..s
   48 	    31 	 0.04885 	 0.00662 	 m..s
   12 	    32 	 0.04490 	 0.00664 	 m..s
   60 	    33 	 0.05280 	 0.00673 	 m..s
   43 	    34 	 0.04825 	 0.00680 	 m..s
   75 	    35 	 0.05912 	 0.00690 	 m..s
   16 	    36 	 0.04525 	 0.01052 	 m..s
   11 	    37 	 0.04442 	 0.01096 	 m..s
    0 	    38 	 0.04011 	 0.01244 	 ~...
   20 	    39 	 0.04576 	 0.01330 	 m..s
   27 	    40 	 0.04619 	 0.01591 	 m..s
   42 	    41 	 0.04816 	 0.01684 	 m..s
    4 	    42 	 0.04160 	 0.01709 	 ~...
    7 	    43 	 0.04213 	 0.01968 	 ~...
    6 	    44 	 0.04208 	 0.02024 	 ~...
    9 	    45 	 0.04263 	 0.02320 	 ~...
   10 	    46 	 0.04287 	 0.02954 	 ~...
   47 	    47 	 0.04860 	 0.03290 	 ~...
   52 	    48 	 0.04912 	 0.03356 	 ~...
   57 	    49 	 0.05040 	 0.03491 	 ~...
   61 	    50 	 0.05321 	 0.03526 	 ~...
   15 	    51 	 0.04516 	 0.03571 	 ~...
   25 	    52 	 0.04607 	 0.03625 	 ~...
   26 	    53 	 0.04610 	 0.03671 	 ~...
   41 	    54 	 0.04780 	 0.04096 	 ~...
   74 	    55 	 0.05772 	 0.04196 	 ~...
   59 	    56 	 0.05178 	 0.05180 	 ~...
   78 	    57 	 0.06454 	 0.05695 	 ~...
   56 	    58 	 0.05025 	 0.05911 	 ~...
   95 	    59 	 0.10177 	 0.05950 	 m..s
   77 	    60 	 0.06374 	 0.06068 	 ~...
  101 	    61 	 0.10808 	 0.06172 	 m..s
   38 	    62 	 0.04747 	 0.06318 	 ~...
   62 	    63 	 0.05440 	 0.06688 	 ~...
   98 	    64 	 0.10603 	 0.06971 	 m..s
   76 	    65 	 0.06177 	 0.07603 	 ~...
   64 	    66 	 0.05459 	 0.07687 	 ~...
   67 	    67 	 0.05497 	 0.07905 	 ~...
   84 	    68 	 0.07611 	 0.07981 	 ~...
   89 	    69 	 0.08657 	 0.08538 	 ~...
   85 	    70 	 0.07815 	 0.08620 	 ~...
   90 	    71 	 0.08828 	 0.08662 	 ~...
   73 	    72 	 0.05745 	 0.08716 	 ~...
   91 	    73 	 0.08975 	 0.08724 	 ~...
   92 	    74 	 0.09387 	 0.08761 	 ~...
   69 	    75 	 0.05633 	 0.08825 	 m..s
   80 	    76 	 0.06677 	 0.09143 	 ~...
  110 	    77 	 0.16067 	 0.09148 	 m..s
   88 	    78 	 0.08574 	 0.09279 	 ~...
   54 	    79 	 0.04970 	 0.09355 	 m..s
   19 	    80 	 0.04571 	 0.09396 	 m..s
   68 	    81 	 0.05560 	 0.09453 	 m..s
   70 	    82 	 0.05698 	 0.09471 	 m..s
   31 	    83 	 0.04649 	 0.09476 	 m..s
   21 	    84 	 0.04578 	 0.09700 	 m..s
   37 	    85 	 0.04721 	 0.09713 	 m..s
   22 	    86 	 0.04580 	 0.09713 	 m..s
   17 	    87 	 0.04537 	 0.09718 	 m..s
   44 	    88 	 0.04825 	 0.09739 	 m..s
   30 	    89 	 0.04646 	 0.09868 	 m..s
   79 	    90 	 0.06466 	 0.10348 	 m..s
   82 	    91 	 0.07298 	 0.10489 	 m..s
  102 	    92 	 0.11128 	 0.10631 	 ~...
   99 	    93 	 0.10628 	 0.10640 	 ~...
   83 	    94 	 0.07449 	 0.10670 	 m..s
   97 	    95 	 0.10585 	 0.10676 	 ~...
  100 	    96 	 0.10758 	 0.10791 	 ~...
  109 	    97 	 0.15772 	 0.10825 	 m..s
  107 	    98 	 0.15683 	 0.10837 	 m..s
   71 	    99 	 0.05704 	 0.10869 	 m..s
  112 	   100 	 0.16365 	 0.10937 	 m..s
   81 	   101 	 0.06883 	 0.11092 	 m..s
  105 	   102 	 0.12777 	 0.11108 	 ~...
  103 	   103 	 0.11754 	 0.11841 	 ~...
  104 	   104 	 0.11835 	 0.12480 	 ~...
   86 	   105 	 0.08030 	 0.12635 	 m..s
   96 	   106 	 0.10500 	 0.13362 	 ~...
   87 	   107 	 0.08568 	 0.13667 	 m..s
   94 	   108 	 0.10006 	 0.14115 	 m..s
   93 	   109 	 0.09914 	 0.14201 	 m..s
  111 	   110 	 0.16202 	 0.15133 	 ~...
  106 	   111 	 0.15308 	 0.16380 	 ~...
  108 	   112 	 0.15735 	 0.16654 	 ~...
  114 	   113 	 0.17968 	 0.19252 	 ~...
  113 	   114 	 0.17836 	 0.19590 	 ~...
  115 	   115 	 0.21584 	 0.20763 	 ~...
  116 	   116 	 0.28440 	 0.21616 	 m..s
  117 	   117 	 0.28932 	 0.24106 	 m..s
  118 	   118 	 0.29385 	 0.26716 	 ~...
  119 	   119 	 0.29567 	 0.27178 	 ~...
  120 	   120 	 0.33358 	 0.29431 	 m..s
==========================================
r_mrr = 0.8576846122741699
r2_mrr = 0.7034976482391357
spearmanr_mrr@5 = 0.8194838166236877
spearmanr_mrr@10 = 0.9391503930091858
spearmanr_mrr@50 = 0.9810678362846375
spearmanr_mrr@100 = 0.9221519231796265
spearmanr_mrr@All = 0.9177626371383667
==========================================
test time: 0.494
Done Testing dataset OpenEA
total time taken: 197.16799020767212
training time taken: 181.45713257789612
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8577)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7035)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.8195)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.9392)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9811)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9222)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9178)}}, 'test_loss': {'TransE': {'OpenEA': 0.7328067131093121}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 2867934705572319
Starting TWIG!
Loading datasets
Loading TransE...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [273, 1069, 167, 1062, 1148, 829, 845, 259, 648, 962, 198, 1091, 768, 465, 229, 71, 928, 125, 1133, 500, 1060, 258, 540, 481, 240, 402, 1097, 954, 1038, 596, 750, 268, 618, 433, 541, 1008, 159, 634, 670, 808, 621, 424, 788, 1178, 515, 435, 440, 960, 185, 989, 306, 580, 869, 1096, 1162, 278, 628, 676, 1093, 523, 727, 778, 573, 1183, 1186, 54, 480, 1005, 362, 723, 30, 1037, 0, 272, 737, 1023, 158, 665, 529, 572, 299, 138, 162, 333, 751, 155, 642, 130, 1111, 80, 524, 1003, 1120, 86, 1179, 686, 936, 193, 886, 752, 1079, 669, 581, 985, 445, 422, 320, 630, 196, 485, 836, 406, 50, 766, 1212, 121, 399, 318, 224, 862, 970]
valid_ids (0): []
train_ids (1094): [549, 986, 96, 969, 965, 8, 1174, 99, 827, 1116, 1207, 95, 823, 351, 565, 415, 213, 458, 476, 378, 918, 108, 764, 660, 493, 291, 254, 854, 508, 7, 620, 917, 41, 872, 847, 797, 644, 559, 852, 382, 721, 404, 56, 753, 395, 1129, 900, 667, 1044, 1134, 1034, 76, 1042, 1196, 1015, 757, 932, 899, 856, 168, 428, 60, 156, 576, 106, 256, 895, 470, 153, 958, 334, 939, 1172, 715, 189, 225, 1100, 1142, 521, 1105, 477, 762, 871, 979, 688, 612, 833, 1201, 771, 232, 1039, 649, 934, 709, 898, 52, 641, 503, 188, 817, 342, 1081, 956, 288, 756, 165, 1077, 1110, 301, 1199, 449, 777, 1108, 1, 821, 251, 443, 5, 365, 774, 711, 1094, 453, 53, 851, 81, 510, 1040, 608, 116, 358, 674, 1175, 681, 38, 384, 587, 197, 993, 905, 244, 740, 853, 1157, 180, 902, 890, 484, 151, 706, 386, 287, 1188, 200, 906, 730, 1022, 83, 974, 369, 437, 326, 680, 84, 646, 945, 135, 1019, 471, 911, 638, 846, 784, 325, 13, 266, 296, 606, 139, 1128, 271, 653, 277, 230, 340, 889, 1000, 617, 1187, 640, 514, 867, 380, 1112, 792, 594, 673, 302, 1018, 354, 913, 113, 1011, 1151, 915, 131, 261, 550, 805, 62, 564, 448, 136, 909, 262, 554, 769, 316, 314, 35, 461, 544, 699, 988, 603, 1125, 749, 298, 849, 822, 147, 1047, 242, 370, 964, 300, 607, 1055, 943, 473, 166, 1084, 772, 619, 357, 692, 190, 210, 187, 701, 528, 922, 234, 442, 178, 1029, 379, 941, 828, 700, 839, 388, 1090, 1159, 401, 546, 951, 1198, 629, 492, 475, 1088, 819, 359, 102, 947, 1043, 601, 1113, 1024, 579, 114, 560, 1017, 809, 746, 679, 313, 450, 352, 894, 537, 513, 177, 434, 1049, 1036, 781, 840, 48, 51, 431, 720, 85, 545, 661, 775, 1074, 91, 982, 657, 134, 604, 233, 491, 4, 1184, 1181, 1004, 593, 743, 356, 1013, 861, 1169, 149, 331, 536, 425, 441, 639, 360, 439, 810, 1130, 1126, 322, 1053, 1086, 1149, 1204, 971, 726, 487, 668, 78, 3, 403, 1076, 966, 1177, 870, 297, 804, 563, 901, 882, 803, 1046, 526, 1213, 409, 307, 908, 144, 164, 1085, 677, 488, 247, 710, 937, 1058, 1176, 760, 1194, 411, 570, 509, 345, 1132, 94, 289, 1164, 1109, 28, 1059, 636, 977, 651, 685, 24, 1098, 145, 92, 548, 115, 120, 184, 684, 530, 235, 430, 119, 160, 1123, 381, 744, 623, 332, 1061, 348, 782, 260, 65, 209, 578, 383, 157, 400, 252, 930, 542, 285, 31, 516, 324, 191, 474, 655, 1200, 935, 49, 864, 891, 408, 100, 44, 281, 192, 598, 1027, 372, 495, 691, 798, 447, 812, 972, 506, 174, 645, 19, 517, 218, 724, 245, 837, 204, 172, 1156, 843, 57, 678, 610, 779, 831, 1057, 1095, 309, 920, 1206, 417, 1078, 1001, 944, 703, 525, 926, 888, 72, 66, 1168, 1021, 532, 534, 647, 759, 697, 583, 21, 783, 616, 223, 995, 222, 444, 1064, 104, 967, 738, 531, 1124, 269, 1160, 1182, 276, 733, 787, 754, 339, 558, 844, 820, 780, 707, 363, 103, 719, 9, 1103, 838, 903, 1115, 652, 1192, 925, 734, 816, 1009, 146, 1016, 90, 722, 690, 877, 295, 466, 410, 884, 976, 1092, 343, 940, 555, 23, 875, 248, 148, 518, 605, 270, 58, 6, 1209, 519, 814, 539, 328, 214, 929, 127, 512, 10, 1006, 1185, 868, 459, 698, 330, 758, 42, 212, 98, 567, 385, 566, 61, 815, 141, 373, 413, 118, 991, 143, 597, 226, 535, 1150, 132, 946, 1155, 88, 834, 338, 129, 973, 483, 632, 1070, 446, 527, 712, 1173, 835, 396, 635, 75, 405, 883, 239, 1170, 1138, 274, 1203, 571, 137, 574, 859, 228, 1161, 765, 350, 1014, 631, 586, 317, 414, 975, 490, 702, 1214, 552, 662, 1211, 950, 507, 1137, 1141, 217, 142, 745, 264, 714, 310, 865, 874, 1054, 938, 39, 455, 556, 390, 561, 1121, 171, 377, 824, 551, 418, 1165, 241, 472, 795, 801, 216, 992, 1056, 613, 436, 1205, 429, 695, 452, 921, 663, 650, 767, 366, 68, 93, 286, 112, 105, 1143, 305, 857, 280, 785, 876, 624, 367, 451, 391, 879, 195, 694, 1102, 207, 1208, 394, 1050, 708, 625, 376, 1045, 1107, 17, 959, 265, 186, 654, 283, 599, 194, 791, 1135, 1114, 64, 832, 591, 1025, 220, 32, 375, 907, 253, 687, 637, 562, 237, 543, 881, 133, 150, 1041, 454, 152, 1158, 392, 1166, 553, 109, 59, 825, 892, 124, 412, 77, 1106, 615, 948, 501, 371, 1026, 1020, 931, 312, 996, 557, 353, 584, 689, 855, 89, 63, 199, 349, 742, 1190, 1118, 547, 463, 910, 1193, 347, 761, 999, 206, 981, 456, 1153, 215, 208, 643, 1117, 811, 73, 1197, 957, 850, 998, 980, 329, 614, 683, 468, 1051, 656, 1145, 464, 74, 387, 729, 997, 739, 2, 374, 671, 896, 842, 609, 904, 666, 675, 111, 1089, 290, 292, 978, 1007, 522, 589, 1202, 323, 494, 588, 716, 728, 70, 294, 497, 1033, 696, 161, 1104, 1012, 40, 961, 955, 421, 592, 478, 863, 154, 987, 55, 600, 236, 914, 1163, 924, 101, 97, 736, 860, 858, 1136, 953, 1147, 255, 368, 438, 622, 1146, 337, 43, 46, 467, 117, 173, 311, 990, 346, 489, 123, 246, 140, 182, 1140, 813, 848, 963, 718, 15, 419, 802, 18, 1191, 704, 968, 87, 994, 1075, 29, 201, 179, 426, 407, 1073, 279, 933, 12, 1083, 1065, 1063, 122, 505, 175, 1082, 82, 912, 585, 397, 682, 773, 47, 885, 763, 1032, 243, 664, 611, 169, 45, 748, 511, 126, 893, 282, 569, 79, 460, 1010, 69, 1071, 202, 984, 327, 176, 308, 504, 211, 227, 732, 1144, 658, 389, 1031, 927, 1066, 1171, 238, 1127, 293, 1099, 275, 267, 486, 878, 582, 713, 1072, 20, 577, 361, 16, 336, 1028, 806, 183, 747, 741, 533, 1048, 335, 923, 538, 344, 416, 1189, 36, 659, 11, 303, 107, 284, 799, 250, 818, 1139, 693, 315, 873, 1180, 887, 786, 34, 830, 1101, 163, 27, 457, 776, 26, 800, 203, 1122, 1087, 1002, 181, 897, 219, 432, 393, 983, 602, 790, 1035, 469, 257, 22, 916, 626, 355, 423, 502, 420, 633, 1052, 1154, 482, 919, 462, 731, 590, 826, 725, 1152, 841, 321, 952, 568, 949, 364, 942, 627, 880, 595, 37, 575, 398, 1167, 249, 1119, 304, 1067, 427, 796, 479, 231, 1030, 496, 25, 170, 807, 789, 1068, 1195, 794, 33, 14, 1131, 205, 498, 755, 499, 705, 341, 319, 128, 793, 672, 263, 735, 67, 866, 1210, 520, 110, 770, 1080, 221, 717]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1812625387827192
the save name prefix for this run is:  chkpt-ID_1812625387827192_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'TransE': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 564
rank avg (pred): 0.518 +- 0.010
mrr vals (pred, true): 0.000, 0.048
batch losses (mrrl, rdl): 0.0, 0.0019216613

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.056 +- 0.040
mrr vals (pred, true): 0.147, 0.241
batch losses (mrrl, rdl): 0.0, 7.0536e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1089
rank avg (pred): 0.360 +- 0.281
mrr vals (pred, true): 0.256, 0.075
batch losses (mrrl, rdl): 0.0, 0.0007028059

Epoch over!
epoch time: 12.174

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 96
rank avg (pred): 0.334 +- 0.274
mrr vals (pred, true): 0.274, 0.013
batch losses (mrrl, rdl): 0.0, 0.0001752831

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 513
rank avg (pred): 0.187 +- 0.173
mrr vals (pred, true): 0.330, 0.050
batch losses (mrrl, rdl): 0.0, 4.83441e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 792
rank avg (pred): 0.347 +- 0.306
mrr vals (pred, true): 0.314, 0.007
batch losses (mrrl, rdl): 0.0, 0.0002049089

Epoch over!
epoch time: 11.879

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1146
rank avg (pred): 0.150 +- 0.143
mrr vals (pred, true): 0.349, 0.107
batch losses (mrrl, rdl): 0.0, 4.13003e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 129
rank avg (pred): 0.338 +- 0.303
mrr vals (pred, true): 0.330, 0.015
batch losses (mrrl, rdl): 0.0, 0.0002505397

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 168
rank avg (pred): 0.333 +- 0.302
mrr vals (pred, true): 0.331, 0.006
batch losses (mrrl, rdl): 0.0, 0.000263613

Epoch over!
epoch time: 11.907

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 557
rank avg (pred): 0.246 +- 0.229
mrr vals (pred, true): 0.318, 0.101
batch losses (mrrl, rdl): 0.0, 0.0002122949

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1166
rank avg (pred): 0.349 +- 0.313
mrr vals (pred, true): 0.327, 0.057
batch losses (mrrl, rdl): 0.0, 8.44742e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 17
rank avg (pred): 0.061 +- 0.060
mrr vals (pred, true): 0.390, 0.187
batch losses (mrrl, rdl): 0.0, 6.5897e-06

Epoch over!
epoch time: 11.848

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 754
rank avg (pred): 0.024 +- 0.022
mrr vals (pred, true): 0.367, 0.217
batch losses (mrrl, rdl): 0.0, 5.094e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 898
rank avg (pred): 0.117 +- 0.111
mrr vals (pred, true): 0.357, 0.104
batch losses (mrrl, rdl): 0.0, 5.73386e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 290
rank avg (pred): 0.045 +- 0.043
mrr vals (pred, true): 0.382, 0.272
batch losses (mrrl, rdl): 0.0, 8.2125e-06

Epoch over!
epoch time: 12.002

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 951
rank avg (pred): 0.347 +- 0.312
mrr vals (pred, true): 0.353, 0.007
batch losses (mrrl, rdl): 0.9187146425, 0.0002017312

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1182
rank avg (pred): 0.414 +- 0.158
mrr vals (pred, true): 0.059, 0.059
batch losses (mrrl, rdl): 0.0007887121, 0.000294787

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.271 +- 0.131
mrr vals (pred, true): 0.075, 0.006
batch losses (mrrl, rdl): 0.0063409433, 0.0009811163

Epoch over!
epoch time: 12.259

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 661
rank avg (pred): 0.404 +- 0.138
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 2.19876e-05, 0.0002275052

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 852
rank avg (pred): 0.381 +- 0.147
mrr vals (pred, true): 0.049, 0.098
batch losses (mrrl, rdl): 1.15962e-05, 0.0003589941

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1087
rank avg (pred): 0.190 +- 0.089
mrr vals (pred, true): 0.058, 0.124
batch losses (mrrl, rdl): 0.0438171253, 7.30512e-05

Epoch over!
epoch time: 12.293

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 103
rank avg (pred): 0.378 +- 0.155
mrr vals (pred, true): 0.053, 0.023
batch losses (mrrl, rdl): 8.41936e-05, 0.0004337482

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 156
rank avg (pred): 0.367 +- 0.160
mrr vals (pred, true): 0.053, 0.015
batch losses (mrrl, rdl): 9.93711e-05, 0.0003020468

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 27
rank avg (pred): 0.021 +- 0.012
mrr vals (pred, true): 0.111, 0.064
batch losses (mrrl, rdl): 0.0369891599, 0.0001774172

Epoch over!
epoch time: 12.131

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 34
rank avg (pred): 0.013 +- 0.007
mrr vals (pred, true): 0.100, 0.084
batch losses (mrrl, rdl): 0.0247587916, 0.0001270365

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 139
rank avg (pred): 0.297 +- 0.156
mrr vals (pred, true): 0.041, 0.016
batch losses (mrrl, rdl): 0.0008792357, 7.87459e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1191
rank avg (pred): 0.368 +- 0.166
mrr vals (pred, true): 0.053, 0.006
batch losses (mrrl, rdl): 0.000105932, 0.0003113692

Epoch over!
epoch time: 12.051

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 68
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.155, 0.193
batch losses (mrrl, rdl): 0.0137671195, 7.45396e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1155
rank avg (pred): 0.034 +- 0.021
mrr vals (pred, true): 0.090, 0.106
batch losses (mrrl, rdl): 0.0025824804, 0.0003750595

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 935
rank avg (pred): 0.368 +- 0.188
mrr vals (pred, true): 0.052, 0.096
batch losses (mrrl, rdl): 2.40913e-05, 0.0002729538

Epoch over!
epoch time: 11.992

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 429
rank avg (pred): 0.332 +- 0.208
mrr vals (pred, true): 0.055, 0.007
batch losses (mrrl, rdl): 0.0002487513, 0.0004312195

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 123
rank avg (pred): 0.337 +- 0.209
mrr vals (pred, true): 0.050, 0.015
batch losses (mrrl, rdl): 1.393e-07, 0.0001634901

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 396
rank avg (pred): 0.343 +- 0.188
mrr vals (pred, true): 0.049, 0.028
batch losses (mrrl, rdl): 1.9117e-05, 0.000321876

Epoch over!
epoch time: 12.167

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 180
rank avg (pred): 0.383 +- 0.167
mrr vals (pred, true): 0.062, 0.006
batch losses (mrrl, rdl): 0.0015567404, 0.0002401183

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 104
rank avg (pred): 0.380 +- 0.169
mrr vals (pred, true): 0.051, 0.042
batch losses (mrrl, rdl): 8.6348e-06, 0.0006398877

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 732
rank avg (pred): 0.058 +- 0.040
mrr vals (pred, true): 0.116, 0.073
batch losses (mrrl, rdl): 0.043409463, 4.7158e-06

Epoch over!
epoch time: 12.015

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 340
rank avg (pred): 0.393 +- 0.155
mrr vals (pred, true): 0.056, 0.032
batch losses (mrrl, rdl): 0.000328577, 0.0006837229

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 331
rank avg (pred): 0.344 +- 0.195
mrr vals (pred, true): 0.051, 0.028
batch losses (mrrl, rdl): 2.00023e-05, 0.0002585071

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1113
rank avg (pred): 0.385 +- 0.217
mrr vals (pred, true): 0.066, 0.006
batch losses (mrrl, rdl): 0.0026233001, 0.0001609005

Epoch over!
epoch time: 11.93

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 744
rank avg (pred): 0.014 +- 0.010
mrr vals (pred, true): 0.153, 0.216
batch losses (mrrl, rdl): 0.0405935012, 2.467e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 377
rank avg (pred): 0.389 +- 0.168
mrr vals (pred, true): 0.048, 0.065
batch losses (mrrl, rdl): 3.18961e-05, 0.0008848655

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 987
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.313, 0.287
batch losses (mrrl, rdl): 0.0067568198, 2.53359e-05

Epoch over!
epoch time: 12.02

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 731
rank avg (pred): 0.182 +- 0.136
mrr vals (pred, true): 0.082, 0.056
batch losses (mrrl, rdl): 0.0102149434, 0.0002772084

running batch: 500 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 184
rank avg (pred): 0.385 +- 0.170
mrr vals (pred, true): 0.051, 0.007
batch losses (mrrl, rdl): 5.0918e-06, 0.0002322743

running batch: 1000 / 1094 and superbatch(1); data from TransE, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.375 +- 0.182
mrr vals (pred, true): 0.053, 0.037
batch losses (mrrl, rdl): 9.96362e-05, 0.0001121099

Epoch over!
epoch time: 12.038

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.095 +- 0.148
mrr vals (pred, true): 0.075, 0.061

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   26 	     0 	 0.05256 	 0.00597 	 m..s
   30 	     1 	 0.05280 	 0.00599 	 m..s
   10 	     2 	 0.05036 	 0.00600 	 m..s
    2 	     3 	 0.04889 	 0.00605 	 m..s
   41 	     4 	 0.05422 	 0.00610 	 m..s
   35 	     5 	 0.05297 	 0.00626 	 m..s
   20 	     6 	 0.05209 	 0.00626 	 m..s
   24 	     7 	 0.05246 	 0.00626 	 m..s
   13 	     8 	 0.05089 	 0.00627 	 m..s
   31 	     9 	 0.05281 	 0.00631 	 m..s
   44 	    10 	 0.05440 	 0.00632 	 m..s
   59 	    11 	 0.05685 	 0.00633 	 m..s
   28 	    12 	 0.05269 	 0.00633 	 m..s
   53 	    13 	 0.05554 	 0.00634 	 m..s
   19 	    14 	 0.05208 	 0.00635 	 m..s
   38 	    15 	 0.05344 	 0.00636 	 m..s
   95 	    16 	 0.08597 	 0.00637 	 m..s
   55 	    17 	 0.05569 	 0.00638 	 m..s
   25 	    18 	 0.05254 	 0.00639 	 m..s
   80 	    19 	 0.06741 	 0.00642 	 m..s
   68 	    20 	 0.05811 	 0.00642 	 m..s
    5 	    21 	 0.04991 	 0.00643 	 m..s
   85 	    22 	 0.07145 	 0.00644 	 m..s
   52 	    23 	 0.05549 	 0.00644 	 m..s
   81 	    24 	 0.06814 	 0.00645 	 m..s
   72 	    25 	 0.05872 	 0.00645 	 m..s
   78 	    26 	 0.06543 	 0.00653 	 m..s
   21 	    27 	 0.05218 	 0.00654 	 m..s
   33 	    28 	 0.05283 	 0.00656 	 m..s
   64 	    29 	 0.05767 	 0.00657 	 m..s
   60 	    30 	 0.05699 	 0.00657 	 m..s
   29 	    31 	 0.05276 	 0.00657 	 m..s
   14 	    32 	 0.05099 	 0.00659 	 m..s
   36 	    33 	 0.05303 	 0.00659 	 m..s
    6 	    34 	 0.04999 	 0.00660 	 m..s
   40 	    35 	 0.05420 	 0.00662 	 m..s
   45 	    36 	 0.05441 	 0.00663 	 m..s
   66 	    37 	 0.05795 	 0.00664 	 m..s
   49 	    38 	 0.05533 	 0.00664 	 m..s
   37 	    39 	 0.05304 	 0.00670 	 m..s
   65 	    40 	 0.05788 	 0.00672 	 m..s
   63 	    41 	 0.05750 	 0.00673 	 m..s
    9 	    42 	 0.05034 	 0.00825 	 m..s
    3 	    43 	 0.04921 	 0.00878 	 m..s
   12 	    44 	 0.05066 	 0.00950 	 m..s
   34 	    45 	 0.05292 	 0.00967 	 m..s
   32 	    46 	 0.05281 	 0.00988 	 m..s
   15 	    47 	 0.05135 	 0.01113 	 m..s
    1 	    48 	 0.04885 	 0.01120 	 m..s
   11 	    49 	 0.05040 	 0.01376 	 m..s
   16 	    50 	 0.05141 	 0.01413 	 m..s
   51 	    51 	 0.05548 	 0.01600 	 m..s
   27 	    52 	 0.05258 	 0.01894 	 m..s
   48 	    53 	 0.05488 	 0.02048 	 m..s
   57 	    54 	 0.05624 	 0.02175 	 m..s
    4 	    55 	 0.04977 	 0.02377 	 ~...
    0 	    56 	 0.04855 	 0.02393 	 ~...
   70 	    57 	 0.05835 	 0.02557 	 m..s
   69 	    58 	 0.05813 	 0.02713 	 m..s
    8 	    59 	 0.05021 	 0.03429 	 ~...
   46 	    60 	 0.05449 	 0.03581 	 ~...
   43 	    61 	 0.05433 	 0.03717 	 ~...
   39 	    62 	 0.05360 	 0.03738 	 ~...
   42 	    63 	 0.05426 	 0.03990 	 ~...
   61 	    64 	 0.05711 	 0.04160 	 ~...
   62 	    65 	 0.05726 	 0.04195 	 ~...
   56 	    66 	 0.05604 	 0.04880 	 ~...
  104 	    67 	 0.12235 	 0.05035 	 m..s
   79 	    68 	 0.06680 	 0.05188 	 ~...
   47 	    69 	 0.05462 	 0.05852 	 ~...
   87 	    70 	 0.07515 	 0.06068 	 ~...
   88 	    71 	 0.07543 	 0.06123 	 ~...
   58 	    72 	 0.05662 	 0.06318 	 ~...
   67 	    73 	 0.05802 	 0.06440 	 ~...
   89 	    74 	 0.07563 	 0.06584 	 ~...
   50 	    75 	 0.05540 	 0.06590 	 ~...
   54 	    76 	 0.05557 	 0.06673 	 ~...
   83 	    77 	 0.06925 	 0.07131 	 ~...
   71 	    78 	 0.05861 	 0.07586 	 ~...
   91 	    79 	 0.08048 	 0.07661 	 ~...
   73 	    80 	 0.05891 	 0.07764 	 ~...
   92 	    81 	 0.08146 	 0.08035 	 ~...
   74 	    82 	 0.05934 	 0.08482 	 ~...
   94 	    83 	 0.08534 	 0.08716 	 ~...
  109 	    84 	 0.14689 	 0.09171 	 m..s
   77 	    85 	 0.06513 	 0.09215 	 ~...
   76 	    86 	 0.06354 	 0.09486 	 m..s
   22 	    87 	 0.05225 	 0.09497 	 m..s
    7 	    88 	 0.05016 	 0.09662 	 m..s
   18 	    89 	 0.05176 	 0.09697 	 m..s
   17 	    90 	 0.05173 	 0.09718 	 m..s
   23 	    91 	 0.05227 	 0.09790 	 m..s
   75 	    92 	 0.05976 	 0.09825 	 m..s
   82 	    93 	 0.06831 	 0.10268 	 m..s
   84 	    94 	 0.07109 	 0.10369 	 m..s
   93 	    95 	 0.08314 	 0.10582 	 ~...
  103 	    96 	 0.12111 	 0.10631 	 ~...
   86 	    97 	 0.07233 	 0.10959 	 m..s
  107 	    98 	 0.14152 	 0.10984 	 m..s
  106 	    99 	 0.13886 	 0.11116 	 ~...
   96 	   100 	 0.08717 	 0.11182 	 ~...
   90 	   101 	 0.07706 	 0.11240 	 m..s
  108 	   102 	 0.14164 	 0.12010 	 ~...
   97 	   103 	 0.08822 	 0.12139 	 m..s
   98 	   104 	 0.09028 	 0.12436 	 m..s
   99 	   105 	 0.09311 	 0.14538 	 m..s
  100 	   106 	 0.09434 	 0.14613 	 m..s
  101 	   107 	 0.09475 	 0.14725 	 m..s
  102 	   108 	 0.11113 	 0.18164 	 m..s
  105 	   109 	 0.13501 	 0.19077 	 m..s
  111 	   110 	 0.16663 	 0.19480 	 ~...
  113 	   111 	 0.22946 	 0.20588 	 ~...
  110 	   112 	 0.15386 	 0.20702 	 m..s
  118 	   113 	 0.25719 	 0.26468 	 ~...
  115 	   114 	 0.23576 	 0.27546 	 m..s
  112 	   115 	 0.22797 	 0.27598 	 m..s
  114 	   116 	 0.23287 	 0.27685 	 m..s
  119 	   117 	 0.25848 	 0.28853 	 m..s
  120 	   118 	 0.26140 	 0.29009 	 ~...
  117 	   119 	 0.25660 	 0.29476 	 m..s
  116 	   120 	 0.24705 	 0.30150 	 m..s
==========================================
r_mrr = 0.8982235193252563
r2_mrr = 0.725165843963623
spearmanr_mrr@5 = 0.9703626036643982
spearmanr_mrr@10 = 0.8392016291618347
spearmanr_mrr@50 = 0.9877746105194092
spearmanr_mrr@100 = 0.955145537853241
spearmanr_mrr@All = 0.9544885158538818
==========================================
test time: 0.497
Done Testing dataset OpenEA
total time taken: 196.85993099212646
training time taken: 181.28987503051758
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'OpenEA': tensor(0.8982)}}, 'r2_mrr': {'TransE': {'OpenEA': tensor(0.7252)}}, 'spearmanr_mrr@5': {'TransE': {'OpenEA': tensor(0.9704)}}, 'spearmanr_mrr@10': {'TransE': {'OpenEA': tensor(0.8392)}}, 'spearmanr_mrr@50': {'TransE': {'OpenEA': tensor(0.9878)}}, 'spearmanr_mrr@100': {'TransE': {'OpenEA': tensor(0.9551)}}, 'spearmanr_mrr@All': {'TransE': {'OpenEA': tensor(0.9545)}}, 'test_loss': {'TransE': {'OpenEA': 0.7209168580448022}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 6356540888285267
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [769, 87, 592, 230, 665, 1117, 224, 1005, 924, 791, 1012, 25, 774, 741, 640, 1177, 676, 1175, 765, 469, 528, 39, 597, 1022, 115, 326, 440, 1089, 529, 1183, 4, 312, 1196, 576, 796, 546, 1108, 667, 1209, 617, 797, 746, 1140, 684, 995, 79, 188, 879, 27, 954, 1098, 1194, 623, 400, 424, 549, 764, 443, 779, 457, 490, 52, 955, 1066, 75, 985, 1006, 466, 1199, 289, 514, 706, 1195, 536, 92, 286, 887, 273, 480, 107, 788, 1128, 1214, 1122, 383, 1141, 385, 866, 541, 63, 532, 988, 239, 705, 1114, 896, 288, 1028, 1021, 975, 904, 609, 979, 453, 1146, 23, 1189, 1192, 262, 1008, 1020, 365, 714, 743, 504, 1084, 956, 118, 1061, 409, 448]
valid_ids (0): []
train_ids (1094): [276, 434, 1001, 538, 176, 957, 1166, 876, 53, 812, 168, 712, 161, 477, 451, 50, 733, 1212, 817, 936, 732, 611, 895, 1124, 1045, 635, 557, 1015, 859, 49, 583, 654, 467, 475, 923, 547, 297, 938, 252, 153, 622, 513, 946, 629, 739, 468, 15, 870, 185, 311, 16, 67, 343, 405, 548, 917, 878, 348, 74, 329, 174, 442, 666, 159, 108, 352, 331, 318, 726, 539, 1031, 959, 54, 47, 994, 1011, 540, 175, 1184, 1135, 89, 869, 757, 1046, 431, 200, 403, 197, 621, 523, 134, 64, 411, 822, 221, 700, 679, 905, 38, 939, 99, 35, 1118, 2, 776, 287, 1126, 958, 626, 551, 1, 1208, 398, 624, 800, 1093, 1151, 120, 1064, 196, 152, 685, 57, 178, 659, 429, 246, 897, 270, 966, 642, 997, 987, 1203, 267, 31, 494, 132, 638, 608, 1013, 484, 1123, 688, 497, 760, 110, 802, 669, 692, 1065, 643, 1047, 610, 521, 853, 444, 641, 96, 963, 1129, 360, 682, 1127, 750, 708, 1072, 186, 275, 821, 207, 281, 543, 1018, 804, 890, 1168, 898, 686, 689, 696, 672, 116, 1039, 1063, 1137, 1092, 157, 124, 103, 1100, 845, 1102, 1073, 423, 601, 272, 7, 978, 908, 603, 208, 104, 562, 412, 170, 1054, 1088, 839, 71, 1144, 868, 880, 893, 1167, 217, 264, 1062, 903, 965, 314, 1139, 40, 41, 293, 485, 282, 785, 742, 888, 234, 202, 1048, 449, 699, 1033, 1101, 581, 921, 430, 478, 225, 509, 974, 1105, 187, 674, 852, 336, 154, 740, 783, 930, 832, 1017, 810, 70, 1174, 268, 618, 1076, 518, 274, 1049, 369, 86, 941, 968, 932, 427, 826, 1121, 407, 775, 515, 1052, 1103, 929, 32, 292, 399, 1023, 579, 697, 66, 198, 359, 1142, 687, 269, 58, 373, 259, 1016, 702, 149, 323, 1145, 1120, 990, 195, 761, 462, 588, 19, 980, 1010, 219, 823, 1171, 942, 169, 203, 872, 1160, 432, 371, 856, 633, 349, 607, 565, 1155, 193, 1024, 1032, 848, 998, 829, 127, 752, 94, 1068, 945, 871, 1115, 310, 280, 709, 144, 332, 820, 652, 28, 495, 46, 439, 340, 486, 194, 177, 338, 33, 793, 719, 837, 366, 704, 222, 111, 693, 544, 350, 61, 698, 450, 465, 260, 136, 1173, 566, 671, 662, 1202, 782, 503, 807, 648, 1119, 631, 1161, 483, 656, 1110, 805, 59, 1143, 73, 780, 1069, 236, 1111, 1050, 999, 106, 1083, 561, 126, 925, 510, 842, 660, 794, 150, 381, 456, 716, 237, 316, 1136, 747, 458, 62, 482, 211, 394, 408, 657, 506, 137, 934, 192, 1152, 858, 818, 755, 645, 902, 651, 795, 964, 996, 410, 605, 1037, 786, 390, 1132, 1188, 567, 113, 637, 906, 296, 119, 675, 128, 320, 319, 644, 498, 860, 512, 231, 291, 758, 309, 838, 1026, 1204, 83, 44, 962, 68, 537, 8, 210, 604, 1014, 885, 1138, 1116, 1200, 1112, 78, 1190, 1198, 613, 285, 492, 1162, 971, 900, 522, 1034, 806, 1091, 1193, 294, 668, 139, 572, 1164, 241, 322, 460, 731, 146, 48, 1019, 1109, 815, 591, 505, 80, 745, 1056, 1071, 171, 535, 76, 101, 554, 673, 1150, 22, 375, 18, 650, 803, 166, 341, 681, 690, 91, 813, 232, 756, 727, 213, 553, 725, 940, 29, 147, 777, 1169, 722, 874, 694, 163, 1213, 26, 970, 1125, 701, 416, 358, 121, 1082, 125, 388, 370, 393, 1029, 1148, 661, 531, 599, 972, 335, 1009, 9, 798, 827, 949, 162, 730, 738, 206, 754, 784, 265, 14, 455, 873, 778, 1070, 952, 11, 209, 695, 406, 81, 301, 13, 257, 151, 441, 768, 459, 1149, 620, 658, 1179, 397, 1085, 347, 43, 402, 736, 333, 525, 315, 428, 167, 391, 636, 911, 916, 1003, 418, 619, 243, 436, 1186, 317, 6, 1051, 865, 248, 1187, 474, 36, 364, 1159, 17, 568, 10, 596, 723, 830, 993, 573, 950, 1131, 834, 395, 960, 363, 122, 1079, 586, 500, 533, 1035, 435, 901, 711, 182, 133, 487, 117, 1055, 244, 1201, 463, 1163, 508, 984, 1004, 922, 598, 284, 550, 563, 88, 307, 184, 1205, 417, 678, 261, 1007, 446, 367, 142, 479, 472, 24, 615, 892, 824, 251, 828, 308, 843, 361, 910, 1206, 772, 238, 799, 851, 437, 520, 664, 471, 863, 720, 977, 909, 1030, 634, 1067, 787, 218, 156, 299, 1095, 992, 967, 1025, 578, 1059, 158, 809, 321, 425, 100, 191, 933, 919, 683, 969, 511, 496, 889, 1096, 254, 339, 594, 875, 519, 862, 630, 374, 721, 1157, 45, 707, 42, 216, 1113, 491, 789, 499, 1147, 21, 526, 1087, 190, 951, 65, 1133, 600, 1000, 105, 90, 362, 585, 891, 713, 849, 833, 212, 372, 214, 98, 12, 593, 556, 324, 376, 734, 907, 438, 899, 691, 915, 173, 0, 220, 447, 476, 344, 247, 131, 180, 844, 249, 229, 918, 846, 790, 1053, 72, 302, 84, 155, 392, 728, 781, 886, 1042, 534, 189, 138, 290, 1038, 612, 1058, 857, 751, 748, 953, 337, 1107, 753, 976, 710, 85, 882, 847, 183, 1043, 1078, 555, 1158, 773, 34, 1207, 545, 914, 627, 564, 368, 422, 3, 488, 181, 628, 715, 836, 1002, 819, 384, 574, 461, 452, 677, 172, 334, 112, 811, 1074, 1156, 517, 655, 855, 766, 1027, 1106, 1060, 670, 935, 396, 379, 145, 353, 841, 552, 271, 912, 129, 524, 135, 1077, 413, 481, 215, 380, 1170, 542, 864, 123, 983, 30, 737, 1176, 831, 445, 95, 881, 1134, 1075, 279, 840, 1090, 767, 724, 258, 861, 278, 386, 305, 808, 1094, 926, 357, 303, 947, 404, 298, 1036, 614, 1081, 735, 931, 93, 328, 342, 382, 606, 351, 235, 419, 266, 60, 1181, 82, 346, 1130, 420, 470, 516, 894, 304, 632, 749, 825, 454, 927, 102, 587, 421, 164, 1172, 356, 5, 464, 595, 165, 1154, 616, 944, 507, 256, 109, 639, 414, 1191, 255, 729, 1080, 228, 703, 245, 816, 1211, 354, 501, 570, 1099, 663, 493, 199, 489, 130, 647, 577, 401, 883, 204, 961, 771, 1104, 530, 575, 560, 1057, 283, 625, 226, 233, 989, 884, 377, 1182, 680, 143, 559, 1153, 973, 242, 649, 502, 814, 646, 928, 1097, 51, 355, 913, 1178, 602, 295, 69, 55, 584, 97, 300, 205, 948, 867, 763, 569, 1165, 378, 433, 1210, 415, 991, 1040, 1197, 982, 943, 327, 240, 387, 1180, 56, 571, 877, 77, 770, 762, 141, 179, 37, 792, 201, 1185, 1041, 114, 835, 20, 653, 313, 580, 850, 250, 854, 426, 330, 717, 590, 473, 160, 263, 718, 759, 148, 253, 1044, 801, 277, 345, 527, 558, 389, 589, 920, 582, 1086, 981, 937, 306, 227, 140, 223, 744, 325, 986]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9065006184699382
the save name prefix for this run is:  chkpt-ID_9065006184699382_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 944
rank avg (pred): 0.552 +- 0.007
mrr vals (pred, true): 0.013, 0.086
batch losses (mrrl, rdl): 0.0, 0.0015297148

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1056
rank avg (pred): 0.064 +- 0.049
mrr vals (pred, true): 0.331, 0.610
batch losses (mrrl, rdl): 0.0, 4.01268e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 630
rank avg (pred): 0.406 +- 0.254
mrr vals (pred, true): 0.129, 0.032
batch losses (mrrl, rdl): 0.0, 1.58429e-05

Epoch over!
epoch time: 11.95

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 137
rank avg (pred): 0.354 +- 0.259
mrr vals (pred, true): 0.227, 0.093
batch losses (mrrl, rdl): 0.0, 0.0002127233

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 849
rank avg (pred): 0.337 +- 0.253
mrr vals (pred, true): 0.232, 0.091
batch losses (mrrl, rdl): 0.0, 0.0001118752

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 264
rank avg (pred): 0.031 +- 0.026
mrr vals (pred, true): 0.448, 0.507
batch losses (mrrl, rdl): 0.0, 1.555e-07

Epoch over!
epoch time: 11.941

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 151
rank avg (pred): 0.313 +- 0.255
mrr vals (pred, true): 0.286, 0.101
batch losses (mrrl, rdl): 0.0, 9.80315e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 277
rank avg (pred): 0.026 +- 0.023
mrr vals (pred, true): 0.483, 0.570
batch losses (mrrl, rdl): 0.0, 4.02e-08

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 374
rank avg (pred): 0.347 +- 0.253
mrr vals (pred, true): 0.209, 0.154
batch losses (mrrl, rdl): 0.0, 0.0004236937

Epoch over!
epoch time: 11.843

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 442
rank avg (pred): 0.343 +- 0.258
mrr vals (pred, true): 0.241, 0.043
batch losses (mrrl, rdl): 0.0, 8.49143e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1083
rank avg (pred): 0.308 +- 0.252
mrr vals (pred, true): 0.302, 0.134
batch losses (mrrl, rdl): 0.0, 0.0001522283

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 675
rank avg (pred): 0.398 +- 0.275
mrr vals (pred, true): 0.228, 0.042
batch losses (mrrl, rdl): 0.0, 9.083e-06

Epoch over!
epoch time: 11.754

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1127
rank avg (pred): 0.316 +- 0.260
mrr vals (pred, true): 0.315, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001983412

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 864
rank avg (pred): 0.309 +- 0.261
mrr vals (pred, true): 0.347, 0.041
batch losses (mrrl, rdl): 0.0, 0.0002311049

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 615
rank avg (pred): 0.388 +- 0.272
mrr vals (pred, true): 0.241, 0.034
batch losses (mrrl, rdl): 0.0, 1.10067e-05

Epoch over!
epoch time: 11.8

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 296
rank avg (pred): 0.027 +- 0.023
mrr vals (pred, true): 0.483, 0.621
batch losses (mrrl, rdl): 0.1908346415, 8.016e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 914
rank avg (pred): 0.125 +- 0.083
mrr vals (pred, true): 0.200, 0.331
batch losses (mrrl, rdl): 0.1713694632, 0.0001066135

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1109
rank avg (pred): 0.377 +- 0.187
mrr vals (pred, true): 0.081, 0.038
batch losses (mrrl, rdl): 0.009595897, 6.47398e-05

Epoch over!
epoch time: 12.021

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 189
rank avg (pred): 0.386 +- 0.159
mrr vals (pred, true): 0.057, 0.039
batch losses (mrrl, rdl): 0.0004244692, 9.48772e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1034
rank avg (pred): 0.371 +- 0.179
mrr vals (pred, true): 0.082, 0.041
batch losses (mrrl, rdl): 0.0101705249, 0.0001067708

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1171
rank avg (pred): 0.398 +- 0.126
mrr vals (pred, true): 0.040, 0.036
batch losses (mrrl, rdl): 0.0010288311, 4.35246e-05

Epoch over!
epoch time: 12.042

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 712
rank avg (pred): 0.366 +- 0.112
mrr vals (pred, true): 0.041, 0.037
batch losses (mrrl, rdl): 0.0008824999, 0.0001840199

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 305
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.630, 0.604
batch losses (mrrl, rdl): 0.0065949331, 5.0638e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 192
rank avg (pred): 0.371 +- 0.157
mrr vals (pred, true): 0.061, 0.041
batch losses (mrrl, rdl): 0.0012427628, 0.0001354588

Epoch over!
epoch time: 12.02

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 773
rank avg (pred): 0.383 +- 0.148
mrr vals (pred, true): 0.051, 0.089
batch losses (mrrl, rdl): 1.41408e-05, 0.0002180105

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 280
rank avg (pred): 0.013 +- 0.011
mrr vals (pred, true): 0.560, 0.576
batch losses (mrrl, rdl): 0.0025786983, 3.0242e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 474
rank avg (pred): 0.386 +- 0.156
mrr vals (pred, true): 0.056, 0.046
batch losses (mrrl, rdl): 0.0003092822, 5.6389e-05

Epoch over!
epoch time: 11.915

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1
rank avg (pred): 0.019 +- 0.018
mrr vals (pred, true): 0.529, 0.544
batch losses (mrrl, rdl): 0.0022450453, 1.1846e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 384
rank avg (pred): 0.362 +- 0.176
mrr vals (pred, true): 0.080, 0.083
batch losses (mrrl, rdl): 0.0087539349, 0.0001199109

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1206
rank avg (pred): 0.313 +- 0.093
mrr vals (pred, true): 0.045, 0.041
batch losses (mrrl, rdl): 0.0002540454, 0.0005183677

Epoch over!
epoch time: 12.142

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 728
rank avg (pred): 0.233 +- 0.060
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 1.86952e-05, 0.0010151424

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1207
rank avg (pred): 0.299 +- 0.111
mrr vals (pred, true): 0.063, 0.040
batch losses (mrrl, rdl): 0.0016699291, 0.0004197327

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 307
rank avg (pred): 0.016 +- 0.016
mrr vals (pred, true): 0.557, 0.555
batch losses (mrrl, rdl): 2.44077e-05, 1.6933e-06

Epoch over!
epoch time: 12.298

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 826
rank avg (pred): 0.015 +- 0.015
mrr vals (pred, true): 0.581, 0.561
batch losses (mrrl, rdl): 0.0041013584, 3.2932e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 339
rank avg (pred): 0.384 +- 0.160
mrr vals (pred, true): 0.053, 0.091
batch losses (mrrl, rdl): 7.07627e-05, 0.0002475406

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 237
rank avg (pred): 0.378 +- 0.159
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 4.86186e-05, 9.01856e-05

Epoch over!
epoch time: 12.15

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 250
rank avg (pred): 0.019 +- 0.020
mrr vals (pred, true): 0.573, 0.546
batch losses (mrrl, rdl): 0.0068541262, 8.643e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 133
rank avg (pred): 0.375 +- 0.161
mrr vals (pred, true): 0.059, 0.100
batch losses (mrrl, rdl): 0.0169431865, 0.0003938352

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1198
rank avg (pred): 0.361 +- 0.087
mrr vals (pred, true): 0.037, 0.041
batch losses (mrrl, rdl): 0.0017784868, 0.0001922816

Epoch over!
epoch time: 12.03

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 235
rank avg (pred): 0.351 +- 0.170
mrr vals (pred, true): 0.075, 0.039
batch losses (mrrl, rdl): 0.0062860358, 0.0001624942

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 844
rank avg (pred): 0.361 +- 0.167
mrr vals (pred, true): 0.072, 0.079
batch losses (mrrl, rdl): 0.0050111413, 9.76704e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 394
rank avg (pred): 0.342 +- 0.179
mrr vals (pred, true): 0.087, 0.115
batch losses (mrrl, rdl): 0.0075232959, 0.0002567582

Epoch over!
epoch time: 12.122

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 844
rank avg (pred): 0.387 +- 0.142
mrr vals (pred, true): 0.048, 0.079
batch losses (mrrl, rdl): 6.15412e-05, 0.0001752044

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 342
rank avg (pred): 0.349 +- 0.170
mrr vals (pred, true): 0.079, 0.092
batch losses (mrrl, rdl): 0.0082930606, 0.0001574489

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 909
rank avg (pred): 0.234 +- 0.149
mrr vals (pred, true): 0.177, 0.239
batch losses (mrrl, rdl): 0.038692452, 0.0004840224

Epoch over!
epoch time: 12.005

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.363 +- 0.145
mrr vals (pred, true): 0.050, 0.076

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.03725 	 0.03224 	 ~...
   57 	     1 	 0.05285 	 0.03488 	 ~...
    1 	     2 	 0.03565 	 0.03572 	 ~...
    4 	     3 	 0.03623 	 0.03582 	 ~...
    7 	     4 	 0.03769 	 0.03584 	 ~...
   10 	     5 	 0.04033 	 0.03672 	 ~...
   68 	     6 	 0.05461 	 0.03672 	 ~...
   16 	     7 	 0.04189 	 0.03692 	 ~...
   24 	     8 	 0.04559 	 0.03760 	 ~...
   43 	     9 	 0.05018 	 0.03779 	 ~...
   21 	    10 	 0.04360 	 0.03781 	 ~...
   20 	    11 	 0.04325 	 0.03781 	 ~...
    3 	    12 	 0.03616 	 0.03784 	 ~...
   53 	    13 	 0.05208 	 0.03808 	 ~...
   59 	    14 	 0.05321 	 0.03809 	 ~...
   71 	    15 	 0.05718 	 0.03827 	 ~...
   18 	    16 	 0.04307 	 0.03832 	 ~...
   15 	    17 	 0.04186 	 0.03833 	 ~...
    5 	    18 	 0.03629 	 0.03851 	 ~...
   11 	    19 	 0.04040 	 0.03862 	 ~...
   44 	    20 	 0.05040 	 0.03883 	 ~...
   54 	    21 	 0.05225 	 0.03904 	 ~...
    2 	    22 	 0.03582 	 0.03911 	 ~...
   34 	    23 	 0.04885 	 0.03915 	 ~...
   36 	    24 	 0.04948 	 0.03917 	 ~...
   14 	    25 	 0.04180 	 0.03933 	 ~...
   22 	    26 	 0.04369 	 0.03935 	 ~...
   61 	    27 	 0.05349 	 0.03951 	 ~...
   35 	    28 	 0.04890 	 0.03962 	 ~...
   60 	    29 	 0.05346 	 0.03964 	 ~...
   45 	    30 	 0.05091 	 0.03969 	 ~...
   29 	    31 	 0.04816 	 0.03986 	 ~...
   70 	    32 	 0.05656 	 0.03989 	 ~...
   55 	    33 	 0.05227 	 0.03994 	 ~...
   72 	    34 	 0.05741 	 0.04003 	 ~...
   77 	    35 	 0.06431 	 0.04034 	 ~...
   73 	    36 	 0.05754 	 0.04043 	 ~...
    8 	    37 	 0.03851 	 0.04043 	 ~...
   19 	    38 	 0.04313 	 0.04050 	 ~...
   51 	    39 	 0.05202 	 0.04063 	 ~...
   73 	    40 	 0.05754 	 0.04073 	 ~...
   56 	    41 	 0.05275 	 0.04080 	 ~...
    0 	    42 	 0.03560 	 0.04116 	 ~...
   48 	    43 	 0.05153 	 0.04116 	 ~...
   13 	    44 	 0.04137 	 0.04126 	 ~...
   25 	    45 	 0.04649 	 0.04191 	 ~...
   62 	    46 	 0.05365 	 0.04193 	 ~...
   28 	    47 	 0.04813 	 0.04197 	 ~...
   16 	    48 	 0.04189 	 0.04239 	 ~...
   12 	    49 	 0.04122 	 0.04246 	 ~...
   58 	    50 	 0.05297 	 0.04269 	 ~...
   47 	    51 	 0.05143 	 0.04281 	 ~...
   30 	    52 	 0.04817 	 0.04309 	 ~...
   76 	    53 	 0.06362 	 0.04353 	 ~...
    9 	    54 	 0.04004 	 0.04397 	 ~...
   88 	    55 	 0.08893 	 0.06120 	 ~...
   89 	    56 	 0.08957 	 0.06484 	 ~...
   87 	    57 	 0.08839 	 0.06822 	 ~...
   81 	    58 	 0.06540 	 0.06880 	 ~...
   78 	    59 	 0.06508 	 0.06896 	 ~...
   23 	    60 	 0.04433 	 0.06919 	 ~...
   67 	    61 	 0.05446 	 0.06999 	 ~...
   86 	    62 	 0.07380 	 0.07298 	 ~...
   80 	    63 	 0.06537 	 0.07306 	 ~...
   37 	    64 	 0.04964 	 0.07650 	 ~...
   31 	    65 	 0.04856 	 0.07924 	 m..s
   84 	    66 	 0.06673 	 0.08472 	 ~...
   46 	    67 	 0.05113 	 0.08483 	 m..s
   82 	    68 	 0.06542 	 0.08580 	 ~...
   27 	    69 	 0.04775 	 0.08933 	 m..s
   26 	    70 	 0.04728 	 0.09546 	 m..s
   33 	    71 	 0.04857 	 0.09651 	 m..s
   41 	    72 	 0.04972 	 0.09890 	 m..s
   49 	    73 	 0.05180 	 0.09934 	 m..s
   65 	    74 	 0.05430 	 0.10850 	 m..s
   90 	    75 	 0.11349 	 0.11299 	 ~...
   41 	    76 	 0.04972 	 0.11779 	 m..s
   69 	    77 	 0.05555 	 0.11853 	 m..s
   75 	    78 	 0.05931 	 0.11872 	 m..s
   50 	    79 	 0.05201 	 0.11896 	 m..s
   39 	    80 	 0.04965 	 0.11978 	 m..s
   85 	    81 	 0.06840 	 0.12127 	 m..s
   31 	    82 	 0.04856 	 0.12201 	 m..s
   40 	    83 	 0.04966 	 0.12305 	 m..s
   52 	    84 	 0.05202 	 0.12364 	 m..s
   79 	    85 	 0.06530 	 0.12527 	 m..s
   63 	    86 	 0.05368 	 0.13066 	 m..s
   83 	    87 	 0.06655 	 0.13357 	 m..s
   37 	    88 	 0.04964 	 0.13608 	 m..s
   64 	    89 	 0.05372 	 0.14478 	 m..s
   66 	    90 	 0.05445 	 0.14833 	 m..s
   95 	    91 	 0.18373 	 0.15001 	 m..s
   93 	    92 	 0.14210 	 0.15601 	 ~...
   92 	    93 	 0.14126 	 0.15811 	 ~...
   91 	    94 	 0.13696 	 0.17167 	 m..s
   94 	    95 	 0.18287 	 0.30550 	 MISS
  112 	    96 	 0.55310 	 0.49163 	 m..s
  109 	    97 	 0.55135 	 0.51020 	 m..s
   98 	    98 	 0.53273 	 0.51267 	 ~...
  114 	    99 	 0.55658 	 0.51365 	 m..s
   99 	   100 	 0.53298 	 0.51624 	 ~...
  100 	   101 	 0.53342 	 0.52270 	 ~...
  101 	   102 	 0.53467 	 0.53031 	 ~...
  103 	   103 	 0.53744 	 0.53107 	 ~...
  115 	   104 	 0.55677 	 0.53235 	 ~...
  102 	   105 	 0.53566 	 0.54273 	 ~...
  106 	   106 	 0.53994 	 0.54371 	 ~...
  108 	   107 	 0.54570 	 0.54395 	 ~...
  113 	   108 	 0.55635 	 0.55010 	 ~...
  116 	   109 	 0.55689 	 0.56051 	 ~...
  118 	   110 	 0.56850 	 0.56179 	 ~...
  117 	   111 	 0.56157 	 0.56633 	 ~...
  111 	   112 	 0.55266 	 0.57514 	 ~...
  107 	   113 	 0.54536 	 0.60871 	 m..s
  104 	   114 	 0.53779 	 0.61133 	 m..s
   96 	   115 	 0.52037 	 0.61521 	 m..s
   97 	   116 	 0.52169 	 0.61821 	 m..s
  110 	   117 	 0.55152 	 0.61930 	 m..s
  119 	   118 	 0.57889 	 0.62112 	 m..s
  120 	   119 	 0.64340 	 0.62158 	 ~...
  105 	   120 	 0.53847 	 0.62480 	 m..s
==========================================
r_mrr = 0.9843713641166687
r2_mrr = 0.9653266668319702
spearmanr_mrr@5 = 0.9372026920318604
spearmanr_mrr@10 = 0.44225481152534485
spearmanr_mrr@50 = 0.99272620677948
spearmanr_mrr@100 = 0.9911879301071167
spearmanr_mrr@All = 0.9914646148681641
==========================================
test time: 0.483
Done Testing dataset UMLS
total time taken: 186.73602962493896
training time taken: 180.6030294895172
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9844)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9653)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9372)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.4423)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9927)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9912)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9915)}}, 'test_loss': {'TransE': {'UMLS': 1.6350670222527697}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 5651539382329707
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [135, 1122, 1123, 336, 751, 733, 1099, 1015, 780, 15, 862, 909, 652, 296, 533, 396, 910, 86, 235, 1167, 831, 418, 468, 995, 547, 606, 69, 259, 35, 822, 1109, 654, 55, 101, 471, 271, 40, 596, 644, 840, 496, 9, 450, 1064, 994, 999, 949, 142, 324, 1039, 545, 122, 922, 874, 292, 210, 12, 317, 150, 81, 1210, 1211, 19, 456, 578, 811, 158, 1208, 532, 669, 841, 955, 729, 309, 356, 284, 881, 1028, 118, 600, 62, 427, 80, 60, 1044, 1108, 1200, 358, 1106, 382, 113, 244, 180, 375, 707, 104, 894, 583, 480, 693, 1130, 633, 1153, 944, 63, 683, 576, 624, 1095, 254, 160, 232, 936, 992, 925, 1023, 134, 307, 1119, 53, 98]
valid_ids (0): []
train_ids (1094): [732, 1129, 31, 898, 569, 1073, 331, 1075, 723, 68, 318, 447, 574, 7, 902, 1185, 764, 1021, 812, 207, 247, 77, 1030, 166, 655, 917, 686, 384, 139, 828, 455, 887, 861, 623, 304, 884, 1067, 1076, 835, 608, 719, 478, 827, 1152, 437, 1184, 137, 131, 406, 993, 90, 1175, 517, 904, 1118, 119, 85, 908, 1048, 1027, 1040, 738, 224, 800, 842, 241, 627, 511, 220, 954, 818, 38, 152, 1083, 873, 613, 523, 105, 335, 103, 757, 73, 796, 258, 1071, 813, 629, 591, 411, 233, 133, 273, 609, 168, 938, 385, 963, 626, 50, 327, 605, 374, 226, 1178, 529, 1002, 1029, 399, 33, 88, 978, 240, 1057, 865, 801, 515, 27, 1156, 986, 173, 449, 28, 880, 866, 1045, 689, 293, 93, 1018, 1173, 572, 1006, 863, 299, 489, 253, 1059, 465, 269, 748, 984, 741, 282, 48, 663, 414, 1159, 1125, 959, 246, 1144, 303, 473, 342, 37, 877, 516, 1035, 320, 1084, 567, 408, 407, 653, 1098, 99, 313, 188, 724, 937, 194, 16, 830, 262, 1046, 580, 337, 46, 321, 581, 488, 777, 616, 183, 193, 466, 551, 1212, 363, 648, 1198, 590, 250, 83, 675, 270, 537, 784, 893, 255, 369, 96, 394, 678, 283, 54, 1205, 1213, 798, 1072, 1056, 349, 839, 546, 965, 381, 360, 820, 1069, 117, 853, 837, 493, 467, 251, 620, 1082, 597, 25, 225, 1014, 632, 556, 367, 111, 575, 799, 1036, 343, 522, 485, 915, 430, 772, 577, 951, 859, 833, 649, 805, 17, 285, 639, 91, 461, 615, 933, 964, 768, 1116, 976, 1020, 962, 900, 746, 940, 990, 205, 1145, 229, 682, 409, 735, 291, 147, 997, 975, 75, 191, 804, 202, 870, 395, 787, 988, 792, 921, 212, 524, 1017, 743, 110, 501, 416, 958, 1147, 51, 67, 1160, 1024, 445, 508, 1157, 1139, 770, 345, 797, 680, 1050, 479, 647, 1037, 236, 1186, 643, 895, 330, 704, 595, 339, 1062, 4, 305, 346, 1007, 939, 361, 961, 112, 916, 94, 433, 593, 377, 472, 390, 1187, 29, 278, 889, 788, 114, 239, 1126, 1060, 1140, 1022, 1052, 1194, 991, 286, 222, 340, 720, 891, 469, 503, 539, 5, 3, 507, 477, 231, 1100, 121, 243, 209, 622, 351, 177, 747, 989, 957, 1000, 1199, 412, 857, 20, 1032, 423, 475, 1077, 594, 298, 116, 178, 540, 514, 758, 357, 1170, 228, 1009, 165, 359, 404, 932, 58, 876, 370, 727, 948, 341, 1074, 696, 132, 711, 66, 985, 364, 76, 1120, 846, 635, 1128, 1121, 1103, 734, 1143, 699, 599, 1171, 474, 332, 167, 334, 392, 753, 1, 120, 371, 448, 268, 181, 982, 1102, 1068, 301, 308, 72, 338, 603, 124, 943, 400, 878, 927, 500, 476, 1192, 26, 934, 1104, 294, 829, 314, 659, 497, 520, 728, 176, 1065, 171, 352, 1162, 143, 1207, 457, 190, 843, 1070, 435, 692, 519, 261, 879, 1081, 218, 1079, 710, 601, 149, 509, 434, 771, 151, 697, 996, 95, 534, 1190, 602, 754, 157, 1058, 163, 319, 774, 598, 950, 263, 806, 536, 24, 582, 946, 192, 326, 276, 204, 1114, 875, 868, 836, 6, 482, 755, 1203, 641, 779, 1165, 953, 1150, 36, 242, 460, 211, 184, 107, 1078, 92, 494, 323, 290, 1195, 353, 730, 1010, 667, 745, 1133, 295, 535, 1117, 1137, 199, 267, 589, 281, 570, 664, 454, 544, 931, 1209, 762, 79, 826, 808, 1026, 911, 981, 824, 956, 718, 1005, 1097, 417, 215, 297, 1166, 513, 817, 726, 967, 791, 694, 1158, 59, 484, 175, 402, 850, 636, 431, 1138, 775, 531, 558, 527, 1016, 39, 23, 89, 776, 851, 347, 860, 154, 49, 855, 1063, 803, 854, 737, 362, 703, 621, 453, 1182, 918, 715, 1176, 781, 760, 712, 156, 554, 844, 376, 1183, 773, 230, 125, 617, 492, 277, 1164, 530, 216, 538, 1149, 186, 750, 144, 568, 138, 611, 56, 1174, 969, 832, 896, 966, 420, 782, 637, 410, 196, 1001, 1169, 383, 483, 279, 213, 930, 252, 128, 1088, 783, 306, 161, 312, 84, 897, 1188, 552, 288, 713, 127, 555, 785, 354, 821, 487, 610, 1142, 333, 172, 634, 890, 740, 847, 1093, 426, 238, 973, 22, 765, 725, 1094, 260, 1003, 106, 1080, 695, 189, 825, 200, 1124, 1196, 565, 1177, 906, 656, 491, 650, 61, 266, 1148, 795, 329, 109, 245, 607, 506, 257, 1180, 662, 1089, 325, 563, 668, 618, 419, 155, 322, 365, 289, 71, 924, 838, 810, 794, 115, 405, 1112, 566, 219, 858, 1206, 1042, 767, 604, 510, 1115, 1155, 642, 78, 871, 214, 300, 0, 1091, 1054, 174, 438, 651, 1197, 1189, 793, 739, 612, 386, 550, 987, 1204, 677, 690, 901, 665, 972, 100, 1090, 945, 1151, 681, 145, 1013, 920, 548, 864, 311, 439, 398, 368, 1038, 481, 1055, 979, 108, 170, 1132, 350, 778, 882, 197, 1136, 1172, 907, 403, 32, 328, 815, 684, 21, 462, 18, 913, 802, 935, 1011, 1111, 470, 549, 823, 672, 561, 1201, 814, 912, 687, 153, 661, 1101, 717, 164, 942, 502, 899, 749, 499, 716, 44, 415, 658, 223, 756, 892, 670, 87, 185, 721, 968, 74, 960, 237, 441, 393, 130, 1066, 701, 452, 631, 187, 425, 42, 287, 126, 941, 1134, 1214, 769, 809, 742, 1043, 1191, 645, 424, 458, 97, 47, 444, 521, 8, 169, 34, 348, 10, 586, 217, 974, 736, 464, 705, 630, 1161, 43, 206, 234, 182, 459, 265, 807, 440, 1146, 355, 1163, 201, 162, 1047, 1096, 486, 790, 208, 413, 869, 141, 274, 1051, 302, 428, 14, 674, 422, 1110, 221, 679, 1033, 379, 1061, 366, 671, 971, 542, 528, 638, 573, 316, 280, 998, 387, 512, 1127, 136, 1019, 559, 272, 436, 660, 518, 429, 848, 947, 614, 1193, 905, 421, 700, 977, 983, 849, 1008, 446, 819, 1053, 625, 903, 786, 140, 845, 709, 706, 553, 65, 691, 564, 926, 432, 744, 179, 525, 557, 543, 1049, 761, 41, 45, 560, 463, 195, 1092, 1087, 685, 585, 579, 443, 588, 198, 584, 310, 722, 852, 13, 264, 1034, 248, 752, 708, 70, 391, 397, 1179, 203, 82, 698, 587, 541, 1141, 102, 619, 1113, 57, 759, 1168, 952, 628, 1012, 495, 249, 592, 872, 834, 1154, 646, 789, 378, 1086, 919, 816, 702, 315, 498, 1085, 30, 571, 1025, 344, 442, 1031, 867, 766, 372, 389, 123, 451, 562, 970, 64, 676, 888, 666, 52, 856, 980, 2, 688, 886, 256, 714, 148, 159, 380, 129, 401, 11, 640, 1131, 1105, 1135, 227, 388, 1181, 914, 929, 505, 885, 504, 1202, 146, 883, 731, 1107, 490, 373, 923, 673, 1004, 526, 1041, 928, 275, 657, 763]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2586741028841227
the save name prefix for this run is:  chkpt-ID_2586741028841227_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 816
rank avg (pred): 0.454 +- 0.003
mrr vals (pred, true): 0.016, 0.372
batch losses (mrrl, rdl): 0.0, 0.0035175201

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1030
rank avg (pred): 0.360 +- 0.153
mrr vals (pred, true): 0.083, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001608891

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 415
rank avg (pred): 0.331 +- 0.210
mrr vals (pred, true): 0.229, 0.039
batch losses (mrrl, rdl): 0.0, 0.0002153039

Epoch over!
epoch time: 12.071

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 448
rank avg (pred): 0.355 +- 0.225
mrr vals (pred, true): 0.236, 0.040
batch losses (mrrl, rdl): 0.0, 8.97442e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1103
rank avg (pred): 0.375 +- 0.252
mrr vals (pred, true): 0.261, 0.149
batch losses (mrrl, rdl): 0.0, 0.000582052

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 531
rank avg (pred): 0.123 +- 0.088
mrr vals (pred, true): 0.326, 0.067
batch losses (mrrl, rdl): 0.0, 0.001106391

Epoch over!
epoch time: 11.928

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 746
rank avg (pred): 0.062 +- 0.044
mrr vals (pred, true): 0.365, 0.566
batch losses (mrrl, rdl): 0.0, 2.71415e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 935
rank avg (pred): 0.318 +- 0.233
mrr vals (pred, true): 0.312, 0.085
batch losses (mrrl, rdl): 0.0, 7.08384e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 199
rank avg (pred): 0.356 +- 0.263
mrr vals (pred, true): 0.319, 0.034
batch losses (mrrl, rdl): 0.0, 0.0001098273

Epoch over!
epoch time: 11.933

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 253
rank avg (pred): 0.116 +- 0.086
mrr vals (pred, true): 0.353, 0.560
batch losses (mrrl, rdl): 0.0, 0.0001909742

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 318
rank avg (pred): 0.152 +- 0.111
mrr vals (pred, true): 0.339, 0.527
batch losses (mrrl, rdl): 0.0, 0.0003582715

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1082
rank avg (pred): 0.342 +- 0.255
mrr vals (pred, true): 0.328, 0.124
batch losses (mrrl, rdl): 0.0, 0.0003270753

Epoch over!
epoch time: 12.057

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 828
rank avg (pred): 0.093 +- 0.069
mrr vals (pred, true): 0.365, 0.449
batch losses (mrrl, rdl): 0.0, 4.04346e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 684
rank avg (pred): 0.326 +- 0.256
mrr vals (pred, true): 0.354, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001359726

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 11
rank avg (pred): 0.123 +- 0.093
mrr vals (pred, true): 0.362, 0.606
batch losses (mrrl, rdl): 0.0, 0.0002416307

Epoch over!
epoch time: 11.868

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 537
rank avg (pred): 0.098 +- 0.074
mrr vals (pred, true): 0.378, 0.062
batch losses (mrrl, rdl): 1.0754203796, 0.0013461824

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1136
rank avg (pred): 0.070 +- 0.045
mrr vals (pred, true): 0.313, 0.154
batch losses (mrrl, rdl): 0.2519209087, 0.0005167877

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1141
rank avg (pred): 0.032 +- 0.022
mrr vals (pred, true): 0.405, 0.158
batch losses (mrrl, rdl): 0.6093280911, 0.0007009879

Epoch over!
epoch time: 12.137

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 627
rank avg (pred): 0.891 +- 0.264
mrr vals (pred, true): 0.046, 0.036
batch losses (mrrl, rdl): 0.0001474179, 0.0038662148

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 117
rank avg (pred): 0.895 +- 0.257
mrr vals (pred, true): 0.046, 0.094
batch losses (mrrl, rdl): 0.0001977347, 0.006967843

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 30
rank avg (pred): 0.073 +- 0.053
mrr vals (pred, true): 0.364, 0.518
batch losses (mrrl, rdl): 0.2368515432, 3.91717e-05

Epoch over!
epoch time: 12.286

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 588
rank avg (pred): 0.854 +- 0.298
mrr vals (pred, true): 0.062, 0.037
batch losses (mrrl, rdl): 0.0015361862, 0.003103368

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 990
rank avg (pred): 0.030 +- 0.023
mrr vals (pred, true): 0.432, 0.601
batch losses (mrrl, rdl): 0.2843271196, 9.63e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 803
rank avg (pred): 0.889 +- 0.268
mrr vals (pred, true): 0.059, 0.036
batch losses (mrrl, rdl): 0.0007745501, 0.0032186706

Epoch over!
epoch time: 12.176

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 267
rank avg (pred): 0.125 +- 0.090
mrr vals (pred, true): 0.340, 0.517
batch losses (mrrl, rdl): 0.3130788803, 0.0002215654

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 623
rank avg (pred): 0.906 +- 0.223
mrr vals (pred, true): 0.035, 0.040
batch losses (mrrl, rdl): 0.0022640566, 0.0040180329

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 488
rank avg (pred): 0.041 +- 0.033
mrr vals (pred, true): 0.423, 0.112
batch losses (mrrl, rdl): 0.9670316577, 0.0012997035

Epoch over!
epoch time: 11.932

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 957
rank avg (pred): 0.910 +- 0.260
mrr vals (pred, true): 0.055, 0.039
batch losses (mrrl, rdl): 0.0002057434, 0.0040928358

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1056
rank avg (pred): 0.101 +- 0.074
mrr vals (pred, true): 0.341, 0.610
batch losses (mrrl, rdl): 0.7249400616, 0.0001453647

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1049
rank avg (pred): 0.797 +- 0.349
mrr vals (pred, true): 0.098, 0.039
batch losses (mrrl, rdl): 0.0230608359, 0.002430219

Epoch over!
epoch time: 12.201

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 294
rank avg (pred): 0.097 +- 0.077
mrr vals (pred, true): 0.383, 0.513
batch losses (mrrl, rdl): 0.1681217402, 0.0001068369

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 250
rank avg (pred): 0.146 +- 0.114
mrr vals (pred, true): 0.364, 0.546
batch losses (mrrl, rdl): 0.3324672282, 0.000345461

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1144
rank avg (pred): 0.028 +- 0.023
mrr vals (pred, true): 0.463, 0.152
batch losses (mrrl, rdl): 0.9703159332, 0.000980687

Epoch over!
epoch time: 12.07

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 49
rank avg (pred): 0.096 +- 0.075
mrr vals (pred, true): 0.383, 0.564
batch losses (mrrl, rdl): 0.3291372657, 0.0001126165

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 886
rank avg (pred): 0.928 +- 0.236
mrr vals (pred, true): 0.043, 0.040
batch losses (mrrl, rdl): 0.00044669, 0.004332433

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 819
rank avg (pred): 0.048 +- 0.039
mrr vals (pred, true): 0.435, 0.440
batch losses (mrrl, rdl): 0.0002496539, 7.874e-07

Epoch over!
epoch time: 12.054

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 25
rank avg (pred): 0.122 +- 0.096
mrr vals (pred, true): 0.380, 0.544
batch losses (mrrl, rdl): 0.26823771, 0.0002192758

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1189
rank avg (pred): 0.856 +- 0.300
mrr vals (pred, true): 0.070, 0.044
batch losses (mrrl, rdl): 0.0039547249, 0.0031381981

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 481
rank avg (pred): 0.888 +- 0.281
mrr vals (pred, true): 0.068, 0.039
batch losses (mrrl, rdl): 0.0032627699, 0.0037779016

Epoch over!
epoch time: 12.124

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1204
rank avg (pred): 0.888 +- 0.270
mrr vals (pred, true): 0.058, 0.043
batch losses (mrrl, rdl): 0.0005824564, 0.0037154534

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 331
rank avg (pred): 0.828 +- 0.343
mrr vals (pred, true): 0.093, 0.103
batch losses (mrrl, rdl): 0.0009854927, 0.0052206838

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 239
rank avg (pred): 0.840 +- 0.328
mrr vals (pred, true): 0.084, 0.038
batch losses (mrrl, rdl): 0.0115098199, 0.0027096893

Epoch over!
epoch time: 12.178

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 618
rank avg (pred): 0.928 +- 0.240
mrr vals (pred, true): 0.045, 0.038
batch losses (mrrl, rdl): 0.0002227368, 0.0046517048

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 217
rank avg (pred): 0.923 +- 0.247
mrr vals (pred, true): 0.049, 0.036
batch losses (mrrl, rdl): 4.7803e-06, 0.0038277141

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 108
rank avg (pred): 0.894 +- 0.277
mrr vals (pred, true): 0.069, 0.079
batch losses (mrrl, rdl): 0.0035327256, 0.0066209622

Epoch over!
epoch time: 12.177

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.909 +- 0.265
mrr vals (pred, true): 0.063, 0.084

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.06441 	 0.03006 	 m..s
   17 	     1 	 0.06364 	 0.03066 	 m..s
   23 	     2 	 0.06544 	 0.03249 	 m..s
   19 	     3 	 0.06440 	 0.03422 	 m..s
   52 	     4 	 0.08237 	 0.03488 	 m..s
   74 	     5 	 0.09489 	 0.03532 	 m..s
   46 	     6 	 0.07975 	 0.03538 	 m..s
   18 	     7 	 0.06425 	 0.03584 	 ~...
   24 	     8 	 0.06621 	 0.03601 	 m..s
   64 	     9 	 0.08669 	 0.03614 	 m..s
   56 	    10 	 0.08331 	 0.03667 	 m..s
   11 	    11 	 0.06298 	 0.03668 	 ~...
   30 	    12 	 0.06681 	 0.03757 	 ~...
   58 	    13 	 0.08387 	 0.03757 	 m..s
   66 	    14 	 0.08692 	 0.03795 	 m..s
   36 	    15 	 0.06863 	 0.03801 	 m..s
   65 	    16 	 0.08672 	 0.03817 	 m..s
    0 	    17 	 0.05742 	 0.03818 	 ~...
   42 	    18 	 0.07072 	 0.03824 	 m..s
   47 	    19 	 0.08010 	 0.03829 	 m..s
   41 	    20 	 0.07027 	 0.03939 	 m..s
   53 	    21 	 0.08304 	 0.03945 	 m..s
   27 	    22 	 0.06665 	 0.03959 	 ~...
   58 	    23 	 0.08387 	 0.03962 	 m..s
   29 	    24 	 0.06681 	 0.03964 	 ~...
   28 	    25 	 0.06670 	 0.03991 	 ~...
   32 	    26 	 0.06747 	 0.03991 	 ~...
   55 	    27 	 0.08309 	 0.04063 	 m..s
   75 	    28 	 0.09519 	 0.04073 	 m..s
   31 	    29 	 0.06726 	 0.04081 	 ~...
   73 	    30 	 0.09333 	 0.04107 	 m..s
   48 	    31 	 0.08037 	 0.04124 	 m..s
    0 	    32 	 0.05742 	 0.04126 	 ~...
   67 	    33 	 0.08808 	 0.04136 	 m..s
   21 	    34 	 0.06444 	 0.04186 	 ~...
    9 	    35 	 0.06220 	 0.04191 	 ~...
   43 	    36 	 0.07130 	 0.04267 	 ~...
    3 	    37 	 0.05864 	 0.04276 	 ~...
   24 	    38 	 0.06621 	 0.04326 	 ~...
   10 	    39 	 0.06239 	 0.04373 	 ~...
   53 	    40 	 0.08304 	 0.04390 	 m..s
   67 	    41 	 0.08808 	 0.04421 	 m..s
   37 	    42 	 0.06883 	 0.04465 	 ~...
   70 	    43 	 0.09035 	 0.04711 	 m..s
   96 	    44 	 0.40263 	 0.06896 	 MISS
   89 	    45 	 0.39680 	 0.07479 	 MISS
   16 	    46 	 0.06363 	 0.07561 	 ~...
    5 	    47 	 0.05995 	 0.08011 	 ~...
   87 	    48 	 0.39453 	 0.08036 	 MISS
   12 	    49 	 0.06305 	 0.08444 	 ~...
    5 	    50 	 0.05995 	 0.08476 	 ~...
    8 	    51 	 0.06193 	 0.08599 	 ~...
   15 	    52 	 0.06357 	 0.08633 	 ~...
    4 	    53 	 0.05940 	 0.08676 	 ~...
   26 	    54 	 0.06621 	 0.08687 	 ~...
    2 	    55 	 0.05816 	 0.08764 	 ~...
   13 	    56 	 0.06349 	 0.08889 	 ~...
   38 	    57 	 0.06951 	 0.08956 	 ~...
    7 	    58 	 0.06140 	 0.08968 	 ~...
   35 	    59 	 0.06788 	 0.09546 	 ~...
   39 	    60 	 0.06952 	 0.09556 	 ~...
   44 	    61 	 0.07401 	 0.09782 	 ~...
   22 	    62 	 0.06485 	 0.09999 	 m..s
   13 	    63 	 0.06349 	 0.10115 	 m..s
   33 	    64 	 0.06766 	 0.10182 	 m..s
   40 	    65 	 0.07007 	 0.10238 	 m..s
   34 	    66 	 0.06770 	 0.10340 	 m..s
   63 	    67 	 0.08668 	 0.10345 	 ~...
   60 	    68 	 0.08513 	 0.10675 	 ~...
   62 	    69 	 0.08636 	 0.10900 	 ~...
   45 	    70 	 0.07966 	 0.11256 	 m..s
   49 	    71 	 0.08138 	 0.11432 	 m..s
   70 	    72 	 0.09035 	 0.11702 	 ~...
   57 	    73 	 0.08339 	 0.11791 	 m..s
  114 	    74 	 0.43813 	 0.12112 	 MISS
   50 	    75 	 0.08222 	 0.12202 	 m..s
   69 	    76 	 0.08826 	 0.12917 	 m..s
   61 	    77 	 0.08616 	 0.12945 	 m..s
  112 	    78 	 0.43159 	 0.13486 	 MISS
   76 	    79 	 0.09710 	 0.14086 	 m..s
   78 	    80 	 0.09928 	 0.14282 	 m..s
   50 	    81 	 0.08222 	 0.14666 	 m..s
   77 	    82 	 0.09923 	 0.15032 	 m..s
   72 	    83 	 0.09157 	 0.15988 	 m..s
  119 	    84 	 0.44666 	 0.16056 	 MISS
  100 	    85 	 0.40787 	 0.16997 	 MISS
  103 	    86 	 0.41547 	 0.23945 	 MISS
  101 	    87 	 0.41358 	 0.27242 	 MISS
   99 	    88 	 0.40710 	 0.42189 	 ~...
   97 	    89 	 0.40546 	 0.43006 	 ~...
   79 	    90 	 0.37748 	 0.48792 	 MISS
   82 	    91 	 0.38366 	 0.48976 	 MISS
   83 	    92 	 0.38525 	 0.50860 	 MISS
   80 	    93 	 0.38270 	 0.51696 	 MISS
  102 	    94 	 0.41444 	 0.51997 	 MISS
   98 	    95 	 0.40621 	 0.52124 	 MISS
   81 	    96 	 0.38310 	 0.52169 	 MISS
   84 	    97 	 0.38764 	 0.52298 	 MISS
   85 	    98 	 0.38901 	 0.53031 	 MISS
  104 	    99 	 0.41627 	 0.53780 	 MISS
   88 	   100 	 0.39645 	 0.54085 	 MISS
   93 	   101 	 0.40036 	 0.54185 	 MISS
   86 	   102 	 0.39129 	 0.54621 	 MISS
   90 	   103 	 0.39860 	 0.54688 	 MISS
   94 	   104 	 0.40073 	 0.55497 	 MISS
  105 	   105 	 0.41636 	 0.55518 	 MISS
   91 	   106 	 0.39917 	 0.55618 	 MISS
   92 	   107 	 0.40032 	 0.57063 	 MISS
   95 	   108 	 0.40105 	 0.57114 	 MISS
  117 	   109 	 0.43835 	 0.59591 	 MISS
  116 	   110 	 0.43822 	 0.60247 	 MISS
  118 	   111 	 0.43958 	 0.61133 	 MISS
  107 	   112 	 0.42428 	 0.61243 	 MISS
  106 	   113 	 0.42422 	 0.61400 	 MISS
  110 	   114 	 0.42778 	 0.61485 	 MISS
  108 	   115 	 0.42772 	 0.62143 	 MISS
  120 	   116 	 0.44801 	 0.62322 	 MISS
  108 	   117 	 0.42772 	 0.62676 	 MISS
  111 	   118 	 0.42811 	 0.62809 	 MISS
  113 	   119 	 0.43215 	 0.63332 	 MISS
  115 	   120 	 0.43820 	 0.63497 	 MISS
==========================================
r_mrr = 0.8691410422325134
r2_mrr = 0.7311109900474548
spearmanr_mrr@5 = 0.9548907279968262
spearmanr_mrr@10 = 0.9265345931053162
spearmanr_mrr@50 = 0.7407172322273254
spearmanr_mrr@100 = 0.9063704609870911
spearmanr_mrr@All = 0.918694794178009
==========================================
test time: 0.423
Done Testing dataset UMLS
total time taken: 187.62332582473755
training time taken: 181.7041049003601
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.8691)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.7311)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9549)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9265)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.7407)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9064)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9187)}}, 'test_loss': {'TransE': {'UMLS': 16.26419938914478}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 4164796326378518
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [560, 1031, 995, 253, 759, 167, 782, 810, 1053, 331, 419, 1145, 887, 465, 20, 1062, 515, 1109, 406, 870, 394, 433, 837, 163, 755, 1127, 747, 613, 868, 961, 719, 423, 1138, 1097, 429, 495, 1155, 100, 997, 1195, 344, 835, 464, 590, 342, 641, 78, 674, 350, 592, 784, 930, 1077, 974, 373, 752, 449, 1197, 411, 488, 150, 191, 443, 1039, 343, 296, 1014, 379, 775, 620, 552, 608, 1105, 1079, 70, 991, 366, 935, 452, 2, 10, 307, 538, 249, 1169, 534, 603, 81, 1040, 990, 516, 300, 1111, 655, 566, 883, 110, 1206, 918, 606, 1007, 934, 518, 268, 1135, 374, 178, 37, 664, 677, 562, 754, 98, 1005, 557, 867, 710, 509, 13, 1084, 506]
valid_ids (0): []
train_ids (1094): [22, 60, 229, 232, 849, 134, 352, 902, 258, 1018, 476, 499, 335, 1027, 236, 625, 574, 186, 976, 55, 200, 1148, 1203, 800, 1081, 539, 1093, 401, 803, 547, 987, 309, 933, 1199, 1037, 391, 585, 50, 851, 571, 283, 950, 497, 123, 393, 458, 916, 369, 1041, 211, 758, 648, 251, 891, 1198, 430, 231, 273, 266, 83, 153, 1193, 226, 705, 1209, 267, 928, 1150, 372, 390, 767, 512, 896, 580, 706, 422, 1189, 1194, 593, 578, 334, 400, 543, 56, 1087, 901, 884, 546, 44, 118, 508, 228, 256, 227, 834, 619, 280, 937, 84, 607, 278, 3, 505, 1136, 47, 721, 527, 777, 424, 771, 487, 993, 722, 324, 358, 862, 744, 371, 193, 1012, 806, 561, 72, 627, 567, 327, 616, 704, 612, 760, 126, 564, 426, 1151, 772, 982, 1142, 120, 1057, 795, 913, 1046, 829, 878, 763, 609, 355, 716, 39, 953, 656, 1200, 329, 1187, 214, 520, 676, 692, 428, 846, 111, 182, 213, 1114, 1033, 942, 1035, 733, 869, 1143, 1066, 271, 463, 962, 146, 439, 412, 977, 454, 569, 336, 48, 1118, 776, 940, 1202, 90, 669, 302, 748, 573, 712, 955, 462, 645, 1179, 340, 646, 1165, 315, 801, 483, 220, 575, 364, 166, 368, 230, 49, 172, 42, 840, 377, 351, 717, 614, 91, 694, 1064, 672, 605, 1073, 637, 764, 1167, 970, 144, 769, 684, 973, 688, 1069, 409, 699, 639, 128, 685, 839, 570, 187, 818, 600, 504, 709, 734, 510, 147, 906, 670, 250, 531, 151, 965, 968, 1147, 565, 132, 1049, 633, 1044, 1130, 384, 602, 77, 943, 295, 1100, 856, 808, 700, 94, 853, 496, 316, 1140, 479, 260, 859, 347, 532, 349, 514, 957, 1117, 666, 421, 594, 778, 1170, 86, 756, 293, 208, 1, 338, 819, 702, 148, 204, 723, 773, 678, 568, 630, 217, 682, 598, 751, 116, 434, 1085, 353, 450, 407, 345, 323, 720, 1186, 790, 362, 203, 536, 498, 781, 107, 58, 34, 67, 701, 171, 32, 736, 1211, 233, 1107, 413, 774, 164, 958, 542, 860, 507, 291, 241, 1090, 981, 1160, 888, 1098, 629, 742, 1113, 885, 820, 375, 880, 442, 92, 1052, 794, 1086, 1099, 255, 821, 33, 844, 1180, 152, 524, 1110, 38, 1096, 816, 1009, 112, 817, 1208, 749, 681, 319, 671, 197, 73, 12, 944, 183, 828, 554, 396, 822, 445, 718, 673, 1002, 513, 188, 555, 1204, 88, 272, 221, 523, 456, 1092, 915, 640, 611, 397, 1201, 461, 892, 61, 201, 1154, 1068, 738, 1029, 952, 740, 960, 339, 866, 783, 130, 304, 1146, 89, 798, 165, 1063, 313, 850, 427, 235, 852, 979, 951, 95, 395, 1070, 383, 321, 796, 224, 1125, 572, 757, 954, 858, 1123, 921, 437, 971, 1175, 1188, 785, 949, 337, 1191, 779, 525, 836, 1008, 544, 15, 1101, 1132, 1015, 948, 1156, 727, 926, 730, 26, 41, 761, 938, 446, 924, 745, 199, 1185, 467, 889, 871, 244, 176, 596, 399, 802, 825, 865, 459, 621, 804, 1152, 984, 502, 588, 945, 234, 4, 281, 247, 1072, 841, 586, 357, 583, 890, 689, 1054, 257, 533, 886, 170, 481, 469, 989, 207, 1034, 998, 62, 482, 190, 63, 1112, 141, 1128, 117, 1119, 854, 581, 154, 1042, 1023, 1047, 653, 695, 1016, 1176, 985, 925, 654, 136, 491, 1164, 180, 963, 1174, 435, 624, 667, 305, 486, 563, 75, 1153, 108, 365, 158, 725, 815, 1076, 24, 558, 1059, 872, 301, 826, 405, 897, 662, 46, 1074, 320, 1051, 770, 996, 947, 1134, 277, 259, 743, 348, 876, 40, 380, 282, 941, 189, 276, 1032, 832, 441, 1022, 587, 101, 169, 184, 929, 1122, 739, 1080, 381, 292, 219, 1192, 807, 96, 80, 121, 1129, 1020, 553, 959, 238, 679, 792, 274, 1006, 848, 519, 517, 480, 932, 332, 905, 660, 863, 983, 715, 711, 1106, 104, 927, 1182, 54, 601, 138, 162, 298, 31, 873, 425, 216, 25, 908, 591, 1048, 980, 71, 668, 696, 500, 29, 746, 1050, 1214, 0, 972, 793, 766, 537, 159, 857, 978, 827, 661, 1157, 209, 417, 106, 814, 577, 1095, 521, 436, 875, 306, 787, 831, 1133, 1061, 899, 444, 1038, 1104, 245, 328, 912, 838, 579, 881, 448, 707, 907, 1011, 105, 447, 697, 113, 1166, 354, 535, 728, 370, 762, 303, 285, 248, 914, 1159, 489, 404, 644, 133, 286, 994, 657, 194, 526, 830, 389, 341, 1163, 222, 634, 917, 1141, 330, 93, 698, 988, 548, 855, 269, 103, 545, 904, 97, 797, 177, 652, 911, 1004, 842, 893, 102, 768, 1089, 789, 1071, 1030, 1003, 1115, 530, 432, 638, 1000, 408, 210, 894, 137, 946, 1013, 17, 1082, 845, 809, 824, 922, 173, 460, 683, 149, 551, 378, 478, 402, 909, 470, 474, 59, 582, 690, 416, 576, 431, 724, 731, 529, 23, 843, 82, 36, 753, 264, 99, 556, 385, 967, 311, 440, 765, 490, 610, 79, 185, 484, 312, 1144, 874, 920, 1168, 468, 1213, 45, 453, 541, 246, 658, 1124, 1196, 986, 472, 1149, 969, 279, 492, 387, 51, 703, 931, 114, 52, 750, 522, 847, 270, 729, 599, 359, 5, 813, 9, 265, 196, 1083, 903, 618, 218, 687, 1043, 1210, 1103, 617, 503, 1065, 414, 168, 1137, 691, 791, 1028, 181, 494, 1184, 595, 1067, 741, 43, 1177, 882, 923, 966, 382, 628, 636, 635, 290, 737, 7, 589, 1091, 1056, 360, 786, 726, 549, 642, 318, 939, 74, 174, 326, 115, 262, 615, 473, 287, 900, 1205, 289, 261, 477, 418, 1055, 294, 1158, 501, 192, 322, 1139, 919, 788, 131, 663, 28, 225, 299, 511, 1162, 212, 1078, 237, 1088, 363, 317, 650, 392, 263, 6, 205, 1120, 895, 124, 202, 780, 451, 713, 861, 1131, 455, 175, 438, 1025, 386, 156, 157, 87, 1126, 68, 66, 139, 1001, 85, 223, 999, 179, 65, 1207, 805, 356, 275, 127, 799, 649, 64, 475, 16, 1094, 936, 665, 140, 735, 680, 57, 877, 415, 1121, 69, 626, 242, 1173, 240, 125, 1190, 1021, 18, 686, 485, 457, 956, 584, 145, 1172, 143, 346, 622, 631, 812, 651, 647, 252, 333, 528, 898, 1045, 8, 1183, 604, 471, 1060, 308, 19, 35, 864, 811, 14, 1024, 53, 1075, 11, 879, 992, 1108, 493, 398, 325, 76, 1181, 975, 109, 1102, 1161, 643, 1058, 708, 714, 195, 1036, 254, 675, 239, 659, 540, 376, 559, 693, 129, 160, 1019, 964, 310, 388, 1026, 420, 823, 1116, 1171, 1212, 142, 632, 1178, 623, 297, 314, 466, 367, 243, 1017, 833, 135, 155, 597, 732, 910, 30, 1010, 403, 198, 215, 119, 161, 410, 206, 361, 27, 284, 122, 550, 21, 288]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8378122620358232
the save name prefix for this run is:  chkpt-ID_8378122620358232_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 237
rank avg (pred): 0.581 +- 0.005
mrr vals (pred, true): 0.013, 0.042
batch losses (mrrl, rdl): 0.0, 0.0005009652

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 816
rank avg (pred): 0.283 +- 0.209
mrr vals (pred, true): 0.206, 0.372
batch losses (mrrl, rdl): 0.0, 0.0013350643

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1059
rank avg (pred): 0.229 +- 0.208
mrr vals (pred, true): 0.342, 0.615
batch losses (mrrl, rdl): 0.0, 0.0010982035

Epoch over!
epoch time: 12.105

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 311
rank avg (pred): 0.232 +- 0.203
mrr vals (pred, true): 0.308, 0.619
batch losses (mrrl, rdl): 0.0, 0.0011207224

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1187
rank avg (pred): 0.297 +- 0.274
mrr vals (pred, true): 0.327, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001110765

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 862
rank avg (pred): 0.250 +- 0.232
mrr vals (pred, true): 0.332, 0.086
batch losses (mrrl, rdl): 0.0, 3.017e-06

Epoch over!
epoch time: 11.838

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 382
rank avg (pred): 0.220 +- 0.214
mrr vals (pred, true): 0.369, 0.103
batch losses (mrrl, rdl): 0.0, 1.90169e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 757
rank avg (pred): 0.228 +- 0.223
mrr vals (pred, true): 0.377, 0.088
batch losses (mrrl, rdl): 0.0, 2.36571e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 447
rank avg (pred): 0.221 +- 0.215
mrr vals (pred, true): 0.381, 0.044
batch losses (mrrl, rdl): 0.0, 0.0007137788

Epoch over!
epoch time: 11.813

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 857
rank avg (pred): 0.232 +- 0.227
mrr vals (pred, true): 0.387, 0.085
batch losses (mrrl, rdl): 0.0, 3.02367e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 489
rank avg (pred): 0.342 +- 0.306
mrr vals (pred, true): 0.343, 0.079
batch losses (mrrl, rdl): 0.0, 5.09275e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 980
rank avg (pred): 0.256 +- 0.225
mrr vals (pred, true): 0.280, 0.623
batch losses (mrrl, rdl): 0.0, 0.0013421749

Epoch over!
epoch time: 11.861

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 417
rank avg (pred): 0.220 +- 0.222
mrr vals (pred, true): 0.410, 0.039
batch losses (mrrl, rdl): 0.0, 0.0007472262

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 468
rank avg (pred): 0.204 +- 0.220
mrr vals (pred, true): 0.443, 0.044
batch losses (mrrl, rdl): 0.0, 0.0008338683

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1185
rank avg (pred): 0.289 +- 0.265
mrr vals (pred, true): 0.370, 0.035
batch losses (mrrl, rdl): 0.0, 0.0001865091

Epoch over!
epoch time: 11.809

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 679
rank avg (pred): 0.326 +- 0.290
mrr vals (pred, true): 0.367, 0.040
batch losses (mrrl, rdl): 1.0021423101, 0.000145078

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 166
rank avg (pred): 0.303 +- 0.218
mrr vals (pred, true): 0.249, 0.041
batch losses (mrrl, rdl): 0.3945089579, 0.0002808082

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 834
rank avg (pred): 0.366 +- 0.214
mrr vals (pred, true): 0.169, 0.583
batch losses (mrrl, rdl): 1.7096483707, 0.0026456253

Epoch over!
epoch time: 12.236

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 225
rank avg (pred): 0.314 +- 0.206
mrr vals (pred, true): 0.227, 0.042
batch losses (mrrl, rdl): 0.3126797676, 0.0002371306

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 450
rank avg (pred): 0.305 +- 0.214
mrr vals (pred, true): 0.256, 0.043
batch losses (mrrl, rdl): 0.4250500202, 0.0001881398

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 634
rank avg (pred): 0.357 +- 0.159
mrr vals (pred, true): 0.081, 0.041
batch losses (mrrl, rdl): 0.0093320142, 6.9992e-05

Epoch over!
epoch time: 11.984

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1052
rank avg (pred): 0.269 +- 0.178
mrr vals (pred, true): 0.245, 0.039
batch losses (mrrl, rdl): 0.3799610138, 0.000484705

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 262
rank avg (pred): 0.305 +- 0.207
mrr vals (pred, true): 0.253, 0.550
batch losses (mrrl, rdl): 0.8810964227, 0.0018588371

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 483
rank avg (pred): 0.321 +- 0.206
mrr vals (pred, true): 0.231, 0.039
batch losses (mrrl, rdl): 0.3293096423, 0.0002570128

Epoch over!
epoch time: 11.958

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 104
rank avg (pred): 0.272 +- 0.183
mrr vals (pred, true): 0.253, 0.107
batch losses (mrrl, rdl): 0.2150733471, 3.98462e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1130
rank avg (pred): 0.242 +- 0.155
mrr vals (pred, true): 0.242, 0.044
batch losses (mrrl, rdl): 0.3672769964, 0.0006567586

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 42
rank avg (pred): 0.334 +- 0.167
mrr vals (pred, true): 0.165, 0.517
batch losses (mrrl, rdl): 1.2409352064, 0.0021234471

Epoch over!
epoch time: 11.966

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1174
rank avg (pred): 0.438 +- 0.163
mrr vals (pred, true): 0.086, 0.036
batch losses (mrrl, rdl): 0.0126656936, 2.99106e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 5
rank avg (pred): 0.151 +- 0.103
mrr vals (pred, true): 0.285, 0.601
batch losses (mrrl, rdl): 1.0013073683, 0.0003967983

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 354
rank avg (pred): 0.326 +- 0.208
mrr vals (pred, true): 0.244, 0.084
batch losses (mrrl, rdl): 0.3772670031, 5.72704e-05

Epoch over!
epoch time: 12.07

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 187
rank avg (pred): 0.342 +- 0.191
mrr vals (pred, true): 0.213, 0.044
batch losses (mrrl, rdl): 0.2667588294, 0.0001428026

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 658
rank avg (pred): 0.526 +- 0.178
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 2.4015e-05, 0.0001921942

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 399
rank avg (pred): 0.368 +- 0.211
mrr vals (pred, true): 0.212, 0.112
batch losses (mrrl, rdl): 0.101677537, 0.0003803815

Epoch over!
epoch time: 12.036

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 353
rank avg (pred): 0.047 +- 0.033
mrr vals (pred, true): 0.365, 0.138
batch losses (mrrl, rdl): 0.5138278604, 0.0007844126

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 120
rank avg (pred): 0.349 +- 0.190
mrr vals (pred, true): 0.209, 0.085
batch losses (mrrl, rdl): 0.2530669272, 0.0001633515

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 355
rank avg (pred): 0.272 +- 0.176
mrr vals (pred, true): 0.256, 0.109
batch losses (mrrl, rdl): 0.2157960981, 2.97611e-05

Epoch over!
epoch time: 12.124

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1020
rank avg (pred): 0.271 +- 0.171
mrr vals (pred, true): 0.250, 0.120
batch losses (mrrl, rdl): 0.1705404222, 2.62633e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1075
rank avg (pred): 0.256 +- 0.152
mrr vals (pred, true): 0.236, 0.604
batch losses (mrrl, rdl): 1.3534770012, 0.0012825964

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 430
rank avg (pred): 0.318 +- 0.195
mrr vals (pred, true): 0.242, 0.039
batch losses (mrrl, rdl): 0.3686542511, 0.0002765792

Epoch over!
epoch time: 12.141

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 628
rank avg (pred): 0.474 +- 0.155
mrr vals (pred, true): 0.056, 0.038
batch losses (mrrl, rdl): 0.0003929243, 4.79487e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 138
rank avg (pred): 0.357 +- 0.192
mrr vals (pred, true): 0.206, 0.080
batch losses (mrrl, rdl): 0.2435806245, 0.0001898227

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 30
rank avg (pred): 0.345 +- 0.187
mrr vals (pred, true): 0.206, 0.518
batch losses (mrrl, rdl): 0.9734610319, 0.0022677025

Epoch over!
epoch time: 12.275

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 654
rank avg (pred): 0.387 +- 0.127
mrr vals (pred, true): 0.061, 0.042
batch losses (mrrl, rdl): 0.0011842655, 9.51529e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 729
rank avg (pred): 0.348 +- 0.189
mrr vals (pred, true): 0.205, 0.422
batch losses (mrrl, rdl): 0.47224617, 0.0020097005

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 858
rank avg (pred): 0.358 +- 0.210
mrr vals (pred, true): 0.230, 0.098
batch losses (mrrl, rdl): 0.3224895597, 0.0002659222

Epoch over!
epoch time: 12.02

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.425 +- 0.134
mrr vals (pred, true): 0.088, 0.106

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.06533 	 0.03006 	 m..s
    5 	     1 	 0.06477 	 0.03287 	 m..s
   31 	     2 	 0.08792 	 0.03550 	 m..s
   73 	     3 	 0.24828 	 0.03595 	 MISS
   52 	     4 	 0.23071 	 0.03597 	 MISS
   26 	     5 	 0.08715 	 0.03613 	 m..s
   87 	     6 	 0.25785 	 0.03614 	 MISS
  112 	     7 	 0.26915 	 0.03643 	 MISS
   70 	     8 	 0.24754 	 0.03668 	 MISS
   20 	     9 	 0.08327 	 0.03704 	 m..s
   59 	    10 	 0.24722 	 0.03735 	 MISS
    1 	    11 	 0.05763 	 0.03754 	 ~...
  114 	    12 	 0.27099 	 0.03757 	 MISS
   69 	    13 	 0.24751 	 0.03762 	 MISS
   19 	    14 	 0.08180 	 0.03781 	 m..s
   65 	    15 	 0.24729 	 0.03794 	 MISS
   74 	    16 	 0.24901 	 0.03803 	 MISS
   59 	    17 	 0.24722 	 0.03808 	 MISS
   21 	    18 	 0.08570 	 0.03811 	 m..s
    3 	    19 	 0.06126 	 0.03833 	 ~...
   24 	    20 	 0.08639 	 0.03845 	 m..s
   28 	    21 	 0.08750 	 0.03862 	 m..s
    9 	    22 	 0.07416 	 0.03866 	 m..s
   25 	    23 	 0.08709 	 0.03915 	 m..s
   77 	    24 	 0.25155 	 0.03920 	 MISS
  100 	    25 	 0.26390 	 0.03929 	 MISS
  110 	    26 	 0.26906 	 0.03942 	 MISS
  120 	    27 	 0.27350 	 0.03954 	 MISS
   59 	    28 	 0.24722 	 0.03954 	 MISS
   72 	    29 	 0.24754 	 0.03982 	 MISS
  102 	    30 	 0.26472 	 0.04013 	 MISS
    2 	    31 	 0.05816 	 0.04031 	 ~...
   90 	    32 	 0.25968 	 0.04050 	 MISS
   27 	    33 	 0.08716 	 0.04051 	 m..s
   41 	    34 	 0.21234 	 0.04070 	 MISS
   80 	    35 	 0.25340 	 0.04093 	 MISS
   55 	    36 	 0.23175 	 0.04136 	 MISS
   34 	    37 	 0.08927 	 0.04139 	 m..s
   16 	    38 	 0.07871 	 0.04153 	 m..s
  118 	    39 	 0.27314 	 0.04199 	 MISS
   97 	    40 	 0.26168 	 0.04227 	 MISS
   14 	    41 	 0.07752 	 0.04243 	 m..s
  115 	    42 	 0.27297 	 0.04262 	 MISS
  116 	    43 	 0.27297 	 0.04353 	 MISS
   38 	    44 	 0.12202 	 0.04536 	 m..s
   91 	    45 	 0.26065 	 0.04715 	 MISS
    8 	    46 	 0.06828 	 0.06341 	 ~...
    7 	    47 	 0.06550 	 0.06558 	 ~...
   11 	    48 	 0.07473 	 0.07169 	 ~...
    4 	    49 	 0.06470 	 0.07290 	 ~...
   12 	    50 	 0.07586 	 0.07324 	 ~...
   38 	    51 	 0.12202 	 0.08156 	 m..s
   36 	    52 	 0.09734 	 0.08159 	 ~...
   40 	    53 	 0.12448 	 0.08467 	 m..s
   44 	    54 	 0.21561 	 0.08482 	 MISS
   50 	    55 	 0.22265 	 0.08687 	 MISS
    0 	    56 	 0.05182 	 0.08736 	 m..s
   59 	    57 	 0.24722 	 0.08756 	 MISS
   48 	    58 	 0.22029 	 0.08861 	 MISS
   46 	    59 	 0.21838 	 0.08889 	 MISS
   37 	    60 	 0.11677 	 0.08899 	 ~...
   42 	    61 	 0.21306 	 0.08962 	 MISS
   65 	    62 	 0.24729 	 0.09213 	 MISS
   68 	    63 	 0.24749 	 0.09723 	 MISS
   56 	    64 	 0.23520 	 0.10033 	 MISS
   82 	    65 	 0.25341 	 0.10320 	 MISS
   78 	    66 	 0.25178 	 0.10331 	 MISS
   31 	    67 	 0.08792 	 0.10634 	 ~...
   83 	    68 	 0.25345 	 0.10811 	 MISS
  102 	    69 	 0.26472 	 0.11071 	 MISS
   10 	    70 	 0.07433 	 0.11248 	 m..s
   23 	    71 	 0.08604 	 0.11409 	 ~...
   98 	    72 	 0.26197 	 0.11432 	 MISS
   14 	    73 	 0.07752 	 0.11450 	 m..s
   84 	    74 	 0.25365 	 0.11464 	 MISS
   28 	    75 	 0.08750 	 0.11473 	 ~...
   21 	    76 	 0.08570 	 0.12090 	 m..s
   13 	    77 	 0.07711 	 0.12177 	 m..s
   35 	    78 	 0.08938 	 0.12522 	 m..s
   75 	    79 	 0.25155 	 0.12726 	 MISS
  104 	    80 	 0.26577 	 0.13066 	 MISS
  109 	    81 	 0.26902 	 0.13180 	 MISS
   92 	    82 	 0.26083 	 0.13799 	 MISS
  101 	    83 	 0.26410 	 0.14096 	 MISS
  113 	    84 	 0.26950 	 0.14453 	 MISS
  110 	    85 	 0.26906 	 0.14833 	 MISS
  108 	    86 	 0.26887 	 0.14973 	 MISS
  117 	    87 	 0.27310 	 0.15367 	 MISS
   33 	    88 	 0.08902 	 0.15500 	 m..s
  106 	    89 	 0.26624 	 0.15802 	 MISS
   18 	    90 	 0.07945 	 0.15805 	 m..s
   30 	    91 	 0.08761 	 0.16165 	 m..s
   17 	    92 	 0.07894 	 0.16485 	 m..s
   43 	    93 	 0.21559 	 0.43219 	 MISS
   59 	    94 	 0.24722 	 0.44254 	 MISS
   51 	    95 	 0.22659 	 0.49255 	 MISS
   70 	    96 	 0.24754 	 0.49823 	 MISS
   45 	    97 	 0.21643 	 0.51098 	 MISS
   67 	    98 	 0.24737 	 0.51297 	 MISS
   53 	    99 	 0.23148 	 0.54522 	 MISS
   54 	   100 	 0.23155 	 0.54936 	 MISS
   58 	   101 	 0.23646 	 0.55479 	 MISS
   81 	   102 	 0.25341 	 0.55497 	 MISS
   79 	   103 	 0.25277 	 0.55704 	 MISS
   76 	   104 	 0.25155 	 0.55974 	 MISS
   47 	   105 	 0.22023 	 0.57161 	 MISS
   57 	   106 	 0.23616 	 0.57341 	 MISS
   59 	   107 	 0.24722 	 0.57618 	 MISS
   48 	   108 	 0.22029 	 0.58131 	 MISS
   85 	   109 	 0.25591 	 0.60089 	 MISS
   96 	   110 	 0.26159 	 0.60150 	 MISS
  107 	   111 	 0.26789 	 0.60297 	 MISS
   94 	   112 	 0.26139 	 0.60425 	 MISS
  105 	   113 	 0.26618 	 0.60458 	 MISS
   88 	   114 	 0.25921 	 0.60723 	 MISS
   95 	   115 	 0.26159 	 0.60795 	 MISS
   89 	   116 	 0.25967 	 0.61133 	 MISS
   86 	   117 	 0.25648 	 0.61403 	 MISS
   99 	   118 	 0.26343 	 0.61509 	 MISS
  119 	   119 	 0.27349 	 0.62143 	 MISS
   93 	   120 	 0.26138 	 0.62217 	 MISS
==========================================
r_mrr = 0.3677821755409241
r2_mrr = 0.13438820838928223
spearmanr_mrr@5 = 0.9984942078590393
spearmanr_mrr@10 = 0.847434937953949
spearmanr_mrr@50 = 0.9110398888587952
spearmanr_mrr@100 = 0.5675277709960938
spearmanr_mrr@All = 0.6148558855056763
==========================================
test time: 0.395
Done Testing dataset UMLS
total time taken: 186.56742119789124
training time taken: 180.7185070514679
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.3678)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.1344)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9985)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8474)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9110)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.5675)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.6149)}}, 'test_loss': {'TransE': {'UMLS': 46.5635734378011}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 4158082564001064
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [851, 970, 941, 1181, 290, 1055, 223, 456, 1111, 1041, 168, 1016, 643, 513, 1191, 806, 286, 844, 723, 654, 455, 543, 812, 35, 790, 8, 1206, 1013, 763, 1066, 783, 32, 746, 101, 647, 0, 762, 589, 811, 885, 438, 557, 72, 1054, 1019, 903, 190, 569, 1214, 693, 779, 606, 300, 856, 733, 1141, 755, 909, 553, 306, 91, 817, 335, 356, 98, 244, 936, 1211, 720, 821, 1040, 886, 107, 722, 445, 770, 1075, 656, 246, 708, 1083, 302, 1014, 894, 69, 124, 996, 12, 1188, 685, 673, 1138, 1133, 460, 995, 1020, 753, 860, 854, 577, 1032, 110, 55, 1001, 1144, 534, 331, 614, 1050, 556, 324, 441, 814, 646, 748, 1137, 925, 914, 237, 17, 843]
valid_ids (0): []
train_ids (1094): [1183, 409, 993, 254, 129, 348, 354, 11, 1025, 778, 572, 102, 954, 910, 873, 1168, 578, 1061, 121, 248, 205, 1116, 615, 1189, 346, 325, 135, 786, 645, 29, 443, 946, 1154, 252, 588, 655, 1199, 282, 703, 43, 231, 394, 780, 220, 1148, 242, 974, 709, 1186, 846, 351, 801, 973, 85, 336, 175, 239, 681, 119, 966, 566, 530, 463, 565, 943, 210, 206, 263, 568, 1142, 1165, 50, 279, 905, 798, 19, 564, 256, 932, 309, 1200, 899, 418, 117, 944, 519, 277, 808, 729, 384, 667, 694, 24, 116, 953, 196, 1121, 809, 37, 1120, 1092, 355, 591, 251, 824, 400, 159, 581, 737, 209, 764, 634, 776, 850, 1126, 486, 311, 847, 1036, 1108, 296, 934, 266, 784, 982, 317, 392, 493, 405, 579, 1089, 829, 459, 1064, 616, 187, 662, 1063, 1067, 668, 980, 1115, 623, 92, 96, 1174, 484, 573, 482, 114, 388, 483, 613, 1021, 1150, 9, 489, 619, 350, 61, 153, 532, 485, 1155, 601, 716, 13, 174, 930, 358, 293, 415, 1039, 488, 1078, 963, 548, 14, 916, 1079, 434, 431, 374, 838, 1034, 857, 1159, 120, 490, 807, 751, 901, 452, 151, 1091, 1197, 215, 22, 956, 592, 81, 700, 1022, 1024, 314, 44, 841, 945, 620, 961, 1038, 988, 171, 872, 136, 449, 1146, 79, 867, 393, 782, 524, 679, 510, 1049, 386, 428, 525, 474, 211, 118, 272, 113, 672, 659, 717, 881, 462, 962, 426, 836, 514, 791, 981, 695, 712, 390, 688, 347, 1017, 420, 837, 39, 469, 1134, 774, 179, 464, 247, 416, 36, 476, 342, 567, 407, 499, 152, 896, 537, 128, 738, 660, 697, 203, 343, 454, 163, 305, 298, 1031, 106, 874, 931, 732, 502, 627, 882, 34, 1045, 1163, 1070, 200, 366, 1009, 923, 965, 926, 235, 76, 180, 719, 299, 487, 1167, 1190, 38, 978, 466, 1060, 1210, 198, 516, 768, 823, 41, 1011, 835, 517, 714, 461, 749, 657, 705, 212, 570, 991, 397, 253, 58, 536, 138, 871, 370, 147, 1000, 505, 391, 262, 772, 792, 741, 352, 225, 559, 707, 329, 423, 1099, 491, 78, 598, 410, 64, 1005, 1008, 216, 261, 169, 5, 590, 757, 547, 633, 275, 142, 60, 731, 1145, 866, 947, 1104, 382, 1112, 1028, 436, 228, 642, 940, 994, 86, 93, 1114, 88, 696, 1147, 383, 535, 1131, 740, 249, 270, 134, 1195, 189, 1012, 1002, 442, 398, 637, 429, 666, 84, 53, 349, 357, 967, 494, 735, 1042, 640, 972, 820, 202, 937, 361, 337, 194, 378, 952, 754, 711, 998, 1110, 268, 876, 255, 795, 521, 1100, 1158, 170, 1130, 1082, 1095, 868, 204, 140, 1101, 105, 968, 864, 853, 907, 976, 321, 173, 1088, 904, 130, 935, 31, 1156, 1173, 511, 1058, 260, 1015, 148, 1207, 523, 793, 481, 267, 1179, 222, 736, 859, 1003, 23, 161, 928, 341, 312, 345, 542, 766, 1162, 594, 810, 691, 167, 777, 307, 758, 471, 1212, 1084, 411, 715, 531, 677, 922, 964, 546, 137, 218, 702, 865, 665, 1194, 376, 593, 319, 89, 622, 1, 880, 586, 879, 726, 146, 1023, 412, 710, 457, 315, 805, 425, 51, 447, 333, 773, 1029, 408, 734, 177, 550, 214, 295, 401, 1073, 18, 115, 574, 1132, 477, 861, 765, 1157, 977, 969, 512, 427, 451, 54, 834, 1201, 75, 605, 1007, 771, 869, 1187, 472, 2, 796, 372, 815, 90, 626, 1103, 375, 689, 156, 385, 52, 653, 1026, 503, 554, 674, 127, 500, 756, 890, 323, 1097, 1059, 164, 924, 612, 403, 1068, 775, 387, 430, 960, 1004, 243, 301, 920, 957, 1202, 1193, 833, 424, 379, 234, 987, 219, 625, 241, 1065, 334, 744, 467, 1123, 951, 419, 1081, 603, 877, 1175, 638, 74, 549, 364, 721, 139, 558, 699, 100, 229, 652, 435, 406, 958, 617, 504, 1170, 663, 813, 208, 389, 56, 979, 166, 7, 1184, 328, 761, 373, 71, 433, 316, 340, 221, 274, 1052, 607, 111, 1209, 1171, 258, 585, 313, 680, 639, 395, 1178, 870, 671, 73, 1151, 618, 676, 551, 10, 1125, 1136, 1169, 624, 132, 911, 1135, 898, 197, 144, 948, 59, 125, 1037, 292, 875, 818, 600, 42, 294, 289, 718, 57, 949, 848, 172, 849, 584, 902, 4, 479, 1072, 402, 583, 1048, 1077, 30, 1117, 320, 828, 70, 917, 377, 830, 1047, 632, 83, 938, 360, 562, 658, 602, 192, 288, 413, 497, 25, 832, 1203, 528, 297, 199, 915, 1185, 109, 785, 478, 651, 629, 399, 155, 108, 21, 178, 892, 381, 77, 1149, 819, 992, 97, 912, 154, 803, 285, 112, 322, 839, 713, 575, 188, 670, 919, 444, 1198, 826, 541, 103, 636, 522, 1129, 975, 825, 40, 888, 701, 628, 468, 538, 362, 1152, 788, 104, 509, 822, 893, 184, 769, 587, 599, 730, 123, 887, 236, 747, 1090, 45, 1122, 939, 929, 6, 669, 889, 1053, 95, 986, 94, 781, 595, 1128, 230, 326, 631, 692, 1033, 900, 984, 563, 80, 304, 257, 1124, 496, 47, 518, 145, 66, 143, 728, 67, 165, 831, 515, 816, 1196, 1119, 470, 1205, 126, 999, 706, 264, 475, 310, 396, 227, 745, 276, 863, 99, 149, 157, 895, 291, 799, 1035, 927, 664, 158, 725, 122, 540, 1076, 989, 609, 498, 271, 308, 1109, 959, 608, 527, 245, 1213, 28, 724, 611, 884, 1118, 27, 1176, 480, 742, 1098, 1094, 68, 201, 597, 743, 1027, 1057, 529, 683, 1071, 641, 48, 1018, 465, 1127, 365, 1030, 648, 840, 678, 422, 339, 1204, 359, 787, 273, 186, 265, 767, 698, 238, 507, 727, 560, 49, 1074, 883, 439, 582, 942, 1085, 862, 533, 950, 1139, 3, 368, 908, 259, 858, 181, 287, 162, 684, 526, 690, 539, 552, 440, 417, 635, 1113, 269, 913, 508, 802, 353, 1044, 501, 176, 630, 15, 224, 369, 1096, 555, 571, 1161, 1172, 363, 404, 1140, 344, 897, 1164, 16, 794, 1056, 46, 327, 332, 1069, 131, 580, 675, 1192, 150, 450, 1051, 182, 852, 217, 281, 226, 997, 918, 278, 797, 1086, 878, 891, 1208, 827, 621, 193, 62, 446, 185, 371, 1043, 800, 432, 506, 983, 644, 492, 63, 421, 933, 380, 804, 160, 1006, 1080, 971, 1102, 473, 789, 195, 1180, 1182, 752, 661, 686, 561, 759, 545, 1087, 207, 284, 855, 250, 191, 280, 604, 330, 414, 303, 842, 183, 1010, 990, 750, 596, 649, 87, 495, 687, 650, 682, 82, 1093, 213, 704, 845, 739, 26, 1160, 448, 520, 232, 985, 576, 65, 1062, 1143, 1166, 1105, 1153, 233, 1046, 437, 610, 20, 1106, 283, 921, 133, 318, 367, 338, 955, 240, 544, 760, 906, 453, 458, 141, 33, 1107, 1177]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5244356936068407
the save name prefix for this run is:  chkpt-ID_5244356936068407_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 47
rank avg (pred): 0.530 +- 0.005
mrr vals (pred, true): 0.014, 0.621
batch losses (mrrl, rdl): 0.0, 0.0055908151

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 145
rank avg (pred): 0.380 +- 0.188
mrr vals (pred, true): 0.110, 0.078
batch losses (mrrl, rdl): 0.0, 0.000184156

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 487
rank avg (pred): 0.308 +- 0.201
mrr vals (pred, true): 0.233, 0.082
batch losses (mrrl, rdl): 0.0, 2.10146e-05

Epoch over!
epoch time: 12.133

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 764
rank avg (pred): 0.346 +- 0.218
mrr vals (pred, true): 0.218, 0.070
batch losses (mrrl, rdl): 0.0, 9.28283e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 366
rank avg (pred): 0.349 +- 0.240
mrr vals (pred, true): 0.270, 0.097
batch losses (mrrl, rdl): 0.0, 0.0002960642

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 275
rank avg (pred): 0.021 +- 0.017
mrr vals (pred, true): 0.469, 0.608
batch losses (mrrl, rdl): 0.0, 3.82e-08

Epoch over!
epoch time: 11.822

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 536
rank avg (pred): 0.306 +- 0.233
mrr vals (pred, true): 0.301, 0.113
batch losses (mrrl, rdl): 0.0, 3.64587e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 548
rank avg (pred): 0.305 +- 0.224
mrr vals (pred, true): 0.276, 0.127
batch losses (mrrl, rdl): 0.0, 3.91471e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 451
rank avg (pred): 0.318 +- 0.252
mrr vals (pred, true): 0.312, 0.040
batch losses (mrrl, rdl): 0.0, 0.000176961

Epoch over!
epoch time: 11.752

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 132
rank avg (pred): 0.344 +- 0.251
mrr vals (pred, true): 0.267, 0.088
batch losses (mrrl, rdl): 0.0, 0.0002011913

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1029
rank avg (pred): 0.328 +- 0.263
mrr vals (pred, true): 0.321, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001279282

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 177
rank avg (pred): 0.332 +- 0.266
mrr vals (pred, true): 0.323, 0.040
batch losses (mrrl, rdl): 0.0, 9.52959e-05

Epoch over!
epoch time: 11.889

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 419
rank avg (pred): 0.341 +- 0.263
mrr vals (pred, true): 0.300, 0.036
batch losses (mrrl, rdl): 0.0, 0.0001340248

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 478
rank avg (pred): 0.320 +- 0.256
mrr vals (pred, true): 0.308, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001705438

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 764
rank avg (pred): 0.317 +- 0.260
mrr vals (pred, true): 0.332, 0.070
batch losses (mrrl, rdl): 0.0, 4.90598e-05

Epoch over!
epoch time: 11.858

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 633
rank avg (pred): 0.421 +- 0.260
mrr vals (pred, true): 0.229, 0.034
batch losses (mrrl, rdl): 0.319919467, 7.8216e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 417
rank avg (pred): 0.490 +- 0.182
mrr vals (pred, true): 0.051, 0.039
batch losses (mrrl, rdl): 8.2184e-06, 9.1664e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 699
rank avg (pred): 0.542 +- 0.108
mrr vals (pred, true): 0.031, 0.041
batch losses (mrrl, rdl): 0.003665043, 0.0002777909

Epoch over!
epoch time: 12.199

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 134
rank avg (pred): 0.442 +- 0.210
mrr vals (pred, true): 0.077, 0.129
batch losses (mrrl, rdl): 0.0275039133, 0.0010424681

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 504
rank avg (pred): 0.449 +- 0.201
mrr vals (pred, true): 0.075, 0.073
batch losses (mrrl, rdl): 0.0060237227, 0.0003635044

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 690
rank avg (pred): 0.529 +- 0.108
mrr vals (pred, true): 0.033, 0.039
batch losses (mrrl, rdl): 0.0030604671, 0.0001441948

Epoch over!
epoch time: 11.856

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 454
rank avg (pred): 0.469 +- 0.173
mrr vals (pred, true): 0.052, 0.038
batch losses (mrrl, rdl): 5.11043e-05, 4.28113e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 535
rank avg (pred): 0.419 +- 0.205
mrr vals (pred, true): 0.088, 0.072
batch losses (mrrl, rdl): 0.0146929631, 0.0002046317

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 154
rank avg (pred): 0.444 +- 0.178
mrr vals (pred, true): 0.063, 0.094
batch losses (mrrl, rdl): 0.0016132451, 0.0007361085

Epoch over!
epoch time: 12.237

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 493
rank avg (pred): 0.417 +- 0.198
mrr vals (pred, true): 0.086, 0.083
batch losses (mrrl, rdl): 0.0131877689, 0.0002351085

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 233
rank avg (pred): 0.419 +- 0.190
mrr vals (pred, true): 0.076, 0.034
batch losses (mrrl, rdl): 0.0068092411, 2.3515e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 530
rank avg (pred): 0.370 +- 0.213
mrr vals (pred, true): 0.127, 0.123
batch losses (mrrl, rdl): 0.0001926511, 0.0001866862

Epoch over!
epoch time: 12.123

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1092
rank avg (pred): 0.408 +- 0.188
mrr vals (pred, true): 0.086, 0.131
batch losses (mrrl, rdl): 0.0201836582, 0.0005790298

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 522
rank avg (pred): 0.436 +- 0.161
mrr vals (pred, true): 0.066, 0.068
batch losses (mrrl, rdl): 0.0025579354, 0.0002224783

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 374
rank avg (pred): 0.375 +- 0.196
mrr vals (pred, true): 0.110, 0.154
batch losses (mrrl, rdl): 0.0187520068, 0.000552765

Epoch over!
epoch time: 12.08

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 132
rank avg (pred): 0.456 +- 0.130
mrr vals (pred, true): 0.046, 0.088
batch losses (mrrl, rdl): 0.0001777416, 0.0007725291

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 578
rank avg (pred): 0.461 +- 0.112
mrr vals (pred, true): 0.043, 0.041
batch losses (mrrl, rdl): 0.0005342977, 7.44817e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 800
rank avg (pred): 0.402 +- 0.115
mrr vals (pred, true): 0.052, 0.041
batch losses (mrrl, rdl): 5.94728e-05, 0.0001082922

Epoch over!
epoch time: 12.192

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 149
rank avg (pred): 0.364 +- 0.166
mrr vals (pred, true): 0.091, 0.120
batch losses (mrrl, rdl): 0.0083288429, 0.0004313554

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 701
rank avg (pred): 0.452 +- 0.109
mrr vals (pred, true): 0.047, 0.042
batch losses (mrrl, rdl): 6.6789e-05, 6.99711e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 507
rank avg (pred): 0.423 +- 0.137
mrr vals (pred, true): 0.058, 0.068
batch losses (mrrl, rdl): 0.0007051884, 0.000308352

Epoch over!
epoch time: 12.222

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 228
rank avg (pred): 0.412 +- 0.101
mrr vals (pred, true): 0.047, 0.038
batch losses (mrrl, rdl): 0.0001043171, 0.0001152143

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 940
rank avg (pred): 0.428 +- 0.124
mrr vals (pred, true): 0.054, 0.090
batch losses (mrrl, rdl): 0.0001754247, 0.0003790875

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 301
rank avg (pred): 0.020 +- 0.020
mrr vals (pred, true): 0.553, 0.571
batch losses (mrrl, rdl): 0.0030939083, 5.675e-07

Epoch over!
epoch time: 12.046

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 343
rank avg (pred): 0.370 +- 0.135
mrr vals (pred, true): 0.069, 0.127
batch losses (mrrl, rdl): 0.0334229432, 0.0003846392

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 287
rank avg (pred): 0.014 +- 0.014
mrr vals (pred, true): 0.588, 0.632
batch losses (mrrl, rdl): 0.0186408423, 9.239e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 558
rank avg (pred): 0.402 +- 0.131
mrr vals (pred, true): 0.063, 0.064
batch losses (mrrl, rdl): 0.0016269714, 9.39151e-05

Epoch over!
epoch time: 12.166

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 236
rank avg (pred): 0.365 +- 0.130
mrr vals (pred, true): 0.070, 0.036
batch losses (mrrl, rdl): 0.0039812969, 0.0001392014

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 955
rank avg (pred): 0.423 +- 0.105
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 0.0001118756, 5.26632e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1053
rank avg (pred): 0.009 +- 0.009
mrr vals (pred, true): 0.629, 0.603
batch losses (mrrl, rdl): 0.0067886808, 4.5272e-06

Epoch over!
epoch time: 12.123

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.509 +- 0.148
mrr vals (pred, true): 0.053, 0.088

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04548 	 0.03006 	 ~...
   59 	     1 	 0.07551 	 0.03261 	 m..s
    8 	     2 	 0.05033 	 0.03433 	 ~...
   11 	     3 	 0.05137 	 0.03511 	 ~...
   33 	     4 	 0.05877 	 0.03543 	 ~...
    3 	     5 	 0.04701 	 0.03601 	 ~...
   55 	     6 	 0.06990 	 0.03620 	 m..s
   76 	     7 	 0.13712 	 0.03686 	 MISS
   40 	     8 	 0.06001 	 0.03699 	 ~...
   23 	     9 	 0.05556 	 0.03737 	 ~...
   64 	    10 	 0.08206 	 0.03760 	 m..s
    8 	    11 	 0.05033 	 0.03769 	 ~...
   50 	    12 	 0.06600 	 0.03801 	 ~...
   74 	    13 	 0.12471 	 0.03870 	 m..s
   19 	    14 	 0.05440 	 0.03872 	 ~...
    6 	    15 	 0.04770 	 0.03900 	 ~...
    0 	    16 	 0.04508 	 0.03917 	 ~...
   71 	    17 	 0.11106 	 0.03942 	 m..s
   35 	    18 	 0.05943 	 0.03945 	 ~...
   58 	    19 	 0.07517 	 0.03945 	 m..s
   76 	    20 	 0.13712 	 0.03961 	 m..s
   16 	    21 	 0.05350 	 0.03964 	 ~...
   14 	    22 	 0.05325 	 0.04013 	 ~...
    7 	    23 	 0.04820 	 0.04041 	 ~...
   76 	    24 	 0.13712 	 0.04050 	 m..s
   69 	    25 	 0.10562 	 0.04050 	 m..s
   52 	    26 	 0.06813 	 0.04051 	 ~...
   42 	    27 	 0.06098 	 0.04058 	 ~...
   10 	    28 	 0.05079 	 0.04117 	 ~...
   46 	    29 	 0.06253 	 0.04126 	 ~...
   18 	    30 	 0.05421 	 0.04141 	 ~...
   26 	    31 	 0.05681 	 0.04177 	 ~...
   32 	    32 	 0.05841 	 0.04182 	 ~...
    0 	    33 	 0.04508 	 0.04186 	 ~...
   31 	    34 	 0.05840 	 0.04190 	 ~...
   66 	    35 	 0.08480 	 0.04235 	 m..s
   52 	    36 	 0.06813 	 0.04241 	 ~...
   48 	    37 	 0.06508 	 0.04270 	 ~...
   17 	    38 	 0.05395 	 0.04284 	 ~...
   29 	    39 	 0.05785 	 0.04302 	 ~...
   25 	    40 	 0.05665 	 0.04311 	 ~...
   22 	    41 	 0.05512 	 0.04358 	 ~...
   39 	    42 	 0.05995 	 0.04459 	 ~...
    3 	    43 	 0.04701 	 0.04474 	 ~...
    5 	    44 	 0.04720 	 0.04577 	 ~...
   47 	    45 	 0.06428 	 0.06341 	 ~...
   41 	    46 	 0.06065 	 0.06454 	 ~...
   43 	    47 	 0.06145 	 0.06644 	 ~...
   36 	    48 	 0.05955 	 0.06666 	 ~...
   63 	    49 	 0.07845 	 0.07315 	 ~...
   57 	    50 	 0.07310 	 0.07347 	 ~...
   51 	    51 	 0.06678 	 0.07561 	 ~...
   28 	    52 	 0.05774 	 0.07590 	 ~...
   27 	    53 	 0.05739 	 0.07870 	 ~...
   38 	    54 	 0.05995 	 0.08000 	 ~...
   44 	    55 	 0.06178 	 0.08625 	 ~...
   30 	    56 	 0.05819 	 0.08764 	 ~...
   12 	    57 	 0.05324 	 0.08768 	 m..s
   12 	    58 	 0.05324 	 0.08775 	 m..s
   24 	    59 	 0.05650 	 0.09135 	 m..s
   15 	    60 	 0.05342 	 0.09245 	 m..s
   45 	    61 	 0.06182 	 0.09301 	 m..s
   20 	    62 	 0.05473 	 0.09890 	 m..s
   34 	    63 	 0.05923 	 0.10115 	 m..s
   20 	    64 	 0.05473 	 0.10163 	 m..s
   37 	    65 	 0.05991 	 0.10210 	 m..s
   67 	    66 	 0.10554 	 0.10322 	 ~...
   49 	    67 	 0.06509 	 0.10331 	 m..s
   67 	    68 	 0.10554 	 0.10336 	 ~...
   62 	    69 	 0.07796 	 0.10900 	 m..s
   54 	    70 	 0.06848 	 0.11071 	 m..s
   56 	    71 	 0.07305 	 0.11432 	 m..s
   75 	    72 	 0.13201 	 0.11473 	 ~...
   65 	    73 	 0.08216 	 0.11872 	 m..s
   72 	    74 	 0.11453 	 0.11978 	 ~...
   61 	    75 	 0.07671 	 0.12398 	 m..s
   73 	    76 	 0.11728 	 0.13366 	 ~...
   80 	    77 	 0.14346 	 0.13799 	 ~...
   79 	    78 	 0.13764 	 0.13812 	 ~...
   70 	    79 	 0.11015 	 0.14510 	 m..s
   60 	    80 	 0.07596 	 0.14666 	 m..s
   81 	    81 	 0.17513 	 0.15153 	 ~...
   83 	    82 	 0.17798 	 0.15805 	 ~...
   84 	    83 	 0.19269 	 0.15811 	 m..s
   82 	    84 	 0.17753 	 0.16535 	 ~...
   86 	    85 	 0.28725 	 0.16997 	 MISS
   85 	    86 	 0.28383 	 0.23945 	 m..s
   86 	    87 	 0.28725 	 0.30753 	 ~...
   88 	    88 	 0.28964 	 0.33119 	 m..s
   98 	    89 	 0.51254 	 0.39892 	 MISS
   91 	    90 	 0.50547 	 0.42704 	 m..s
   93 	    91 	 0.50671 	 0.43006 	 m..s
   91 	    92 	 0.50547 	 0.43013 	 m..s
   89 	    93 	 0.50272 	 0.44128 	 m..s
  101 	    94 	 0.52532 	 0.48976 	 m..s
  100 	    95 	 0.52471 	 0.49071 	 m..s
  102 	    96 	 0.52900 	 0.51297 	 ~...
  102 	    97 	 0.52900 	 0.51304 	 ~...
  105 	    98 	 0.53140 	 0.52055 	 ~...
   90 	    99 	 0.50364 	 0.52124 	 ~...
   99 	   100 	 0.52389 	 0.52169 	 ~...
   96 	   101 	 0.50766 	 0.52523 	 ~...
  104 	   102 	 0.53063 	 0.53229 	 ~...
  108 	   103 	 0.55307 	 0.54085 	 ~...
  106 	   104 	 0.54738 	 0.54185 	 ~...
   94 	   105 	 0.50709 	 0.56633 	 m..s
  107 	   106 	 0.55205 	 0.57514 	 ~...
   97 	   107 	 0.50876 	 0.58044 	 m..s
  116 	   108 	 0.61895 	 0.58102 	 m..s
   94 	   109 	 0.50709 	 0.58131 	 m..s
  115 	   110 	 0.61647 	 0.59711 	 ~...
  117 	   111 	 0.62084 	 0.60413 	 ~...
  117 	   112 	 0.62084 	 0.60871 	 ~...
  113 	   113 	 0.61025 	 0.61133 	 ~...
  112 	   114 	 0.60511 	 0.61150 	 ~...
  109 	   115 	 0.60277 	 0.61211 	 ~...
  111 	   116 	 0.60453 	 0.61263 	 ~...
  109 	   117 	 0.60277 	 0.61400 	 ~...
  119 	   118 	 0.62286 	 0.62336 	 ~...
  114 	   119 	 0.61303 	 0.62509 	 ~...
  120 	   120 	 0.62515 	 0.63456 	 ~...
==========================================
r_mrr = 0.9851207733154297
r2_mrr = 0.9690924882888794
spearmanr_mrr@5 = 0.9271831512451172
spearmanr_mrr@10 = 0.8391169905662537
spearmanr_mrr@50 = 0.9859920740127563
spearmanr_mrr@100 = 0.9934164881706238
spearmanr_mrr@All = 0.9941260814666748
==========================================
test time: 0.409
Done Testing dataset UMLS
total time taken: 186.9651734828949
training time taken: 181.19401502609253
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9851)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9691)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9272)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8391)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9860)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9934)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9941)}}, 'test_loss': {'TransE': {'UMLS': 1.4506281123917688}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 2946777720149246
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1191, 1198, 844, 764, 967, 1108, 163, 997, 475, 406, 1032, 438, 10, 301, 598, 114, 766, 94, 1189, 1073, 239, 221, 1106, 459, 299, 1094, 825, 1059, 188, 603, 718, 285, 729, 1153, 461, 228, 982, 211, 328, 638, 747, 251, 914, 183, 119, 890, 78, 23, 678, 270, 1070, 112, 387, 149, 16, 518, 369, 871, 317, 477, 271, 423, 582, 1083, 1197, 207, 1145, 541, 605, 971, 195, 796, 452, 556, 1107, 485, 1125, 595, 1130, 504, 64, 93, 927, 643, 1181, 466, 142, 912, 201, 933, 490, 560, 763, 618, 1014, 783, 876, 327, 1148, 234, 401, 362, 963, 469, 591, 516, 744, 889, 36, 86, 934, 510, 500, 283, 891, 378, 960, 1211, 430, 937, 751]
valid_ids (0): []
train_ids (1094): [250, 448, 872, 657, 139, 802, 1116, 118, 403, 232, 690, 315, 263, 680, 677, 205, 588, 414, 198, 335, 1007, 1017, 1087, 361, 351, 1000, 51, 101, 1149, 993, 219, 364, 190, 642, 529, 308, 374, 561, 865, 800, 75, 812, 12, 174, 257, 822, 458, 27, 587, 1018, 537, 377, 359, 859, 658, 321, 1174, 1127, 870, 907, 853, 622, 137, 676, 866, 241, 615, 1185, 838, 383, 606, 509, 50, 204, 837, 750, 863, 433, 136, 84, 609, 372, 238, 206, 334, 1011, 147, 661, 972, 542, 793, 1203, 757, 132, 584, 696, 453, 717, 1028, 412, 88, 1023, 399, 245, 808, 512, 92, 897, 165, 65, 1192, 1112, 1204, 619, 557, 942, 956, 172, 887, 881, 987, 592, 602, 1043, 69, 919, 616, 784, 1140, 330, 1027, 77, 995, 670, 1114, 140, 666, 1133, 878, 181, 858, 1054, 624, 631, 760, 1210, 544, 291, 262, 267, 386, 1057, 148, 1119, 70, 1035, 604, 1172, 809, 737, 26, 146, 325, 868, 303, 265, 687, 1209, 785, 277, 1177, 476, 21, 54, 811, 445, 502, 305, 958, 788, 1193, 947, 425, 1040, 135, 553, 1051, 978, 884, 765, 166, 924, 363, 572, 778, 813, 786, 220, 1080, 869, 820, 679, 667, 520, 311, 79, 635, 662, 1199, 429, 877, 422, 906, 53, 713, 704, 324, 600, 979, 523, 768, 545, 155, 1179, 431, 154, 959, 489, 388, 437, 495, 1086, 629, 353, 1121, 90, 157, 72, 814, 875, 216, 105, 19, 748, 1006, 885, 227, 1205, 496, 945, 860, 835, 96, 306, 319, 625, 1030, 352, 948, 381, 1202, 1024, 462, 25, 1074, 307, 883, 152, 1136, 240, 761, 827, 652, 1100, 217, 513, 1056, 411, 464, 955, 707, 1003, 1176, 338, 597, 224, 432, 1009, 428, 138, 1019, 98, 733, 444, 856, 867, 612, 436, 567, 1013, 797, 607, 536, 4, 3, 1103, 734, 375, 177, 688, 357, 492, 1122, 486, 894, 392, 834, 1010, 903, 918, 845, 1058, 76, 274, 134, 235, 282, 817, 628, 904, 0, 532, 850, 49, 1081, 1002, 1161, 692, 725, 930, 196, 585, 81, 530, 921, 465, 647, 596, 39, 929, 479, 37, 184, 31, 511, 1085, 935, 278, 649, 575, 446, 824, 771, 656, 439, 720, 538, 117, 840, 1111, 297, 161, 113, 164, 525, 1012, 176, 440, 331, 427, 836, 191, 329, 1093, 1004, 830, 1170, 455, 1077, 210, 749, 1186, 1098, 549, 173, 233, 1123, 818, 1079, 34, 823, 1214, 347, 555, 491, 370, 129, 281, 1190, 807, 468, 715, 539, 832, 1066, 290, 1092, 418, 703, 527, 1160, 655, 449, 1132, 781, 879, 989, 266, 124, 222, 613, 312, 791, 805, 6, 974, 627, 1049, 546, 554, 954, 861, 373, 57, 395, 68, 899, 531, 304, 1041, 158, 1072, 130, 1104, 1061, 60, 360, 279, 497, 258, 478, 719, 48, 675, 908, 82, 964, 1162, 1105, 208, 41, 1053, 1147, 970, 202, 169, 379, 1099, 52, 1151, 1163, 400, 1206, 524, 116, 1015, 941, 775, 548, 1029, 13, 517, 1052, 237, 225, 571, 922, 1, 43, 849, 528, 326, 623, 759, 731, 63, 66, 313, 474, 882, 900, 193, 650, 754, 1137, 203, 159, 626, 724, 833, 981, 1101, 382, 333, 242, 673, 801, 709, 648, 253, 926, 566, 1064, 794, 1022, 18, 986, 99, 482, 695, 1152, 855, 199, 85, 776, 244, 633, 804, 236, 269, 292, 1129, 246, 699, 435, 151, 209, 186, 799, 975, 131, 1158, 746, 192, 711, 640, 366, 38, 102, 574, 484, 310, 1126, 772, 89, 33, 1165, 819, 1141, 1048, 1071, 637, 790, 255, 434, 153, 722, 35, 671, 773, 1078, 1055, 969, 1039, 864, 442, 632, 2, 29, 547, 231, 1175, 562, 590, 276, 910, 880, 782, 473, 215, 1180, 965, 940, 58, 1062, 726, 332, 994, 74, 946, 44, 294, 417, 59, 471, 145, 32, 573, 973, 415, 913, 636, 936, 91, 55, 569, 714, 180, 223, 443, 932, 498, 1021, 212, 314, 777, 447, 295, 1168, 515, 1008, 456, 911, 17, 1171, 712, 630, 621, 533, 200, 309, 80, 745, 886, 341, 350, 951, 405, 394, 391, 691, 1045, 742, 915, 1155, 20, 700, 1088, 1031, 732, 293, 487, 938, 249, 694, 551, 182, 284, 980, 342, 888, 874, 318, 344, 62, 179, 110, 730, 356, 769, 654, 185, 552, 701, 339, 843, 302, 952, 558, 559, 384, 968, 1201, 122, 286, 1090, 8, 170, 862, 1076, 1115, 9, 289, 961, 108, 998, 376, 962, 774, 390, 905, 581, 1159, 852, 1084, 1033, 1025, 1124, 1036, 1113, 839, 568, 420, 1005, 343, 398, 95, 463, 144, 736, 727, 803, 1046, 610, 893, 348, 999, 419, 787, 992, 1135, 322, 218, 426, 254, 107, 349, 816, 1139, 421, 653, 71, 296, 460, 735, 355, 24, 577, 336, 917, 841, 1089, 540, 522, 815, 753, 563, 273, 645, 741, 873, 128, 990, 1120, 1038, 1144, 681, 697, 1020, 583, 614, 189, 1167, 895, 898, 944, 1016, 288, 1195, 925, 248, 11, 789, 1134, 143, 1047, 684, 826, 550, 413, 586, 708, 916, 22, 404, 187, 261, 716, 127, 28, 1068, 663, 123, 739, 1143, 847, 1001, 646, 1082, 721, 949, 396, 214, 389, 901, 56, 1146, 519, 535, 617, 47, 494, 109, 167, 402, 1102, 1208, 508, 977, 685, 393, 320, 1050, 171, 795, 689, 408, 1091, 424, 892, 1128, 450, 454, 792, 639, 300, 1067, 665, 762, 457, 247, 30, 260, 280, 682, 705, 641, 472, 42, 1138, 7, 743, 779, 104, 470, 14, 40, 410, 1178, 565, 505, 1118, 1164, 1187, 570, 710, 175, 115, 256, 1063, 1075, 156, 416, 1183, 358, 848, 259, 996, 966, 593, 1034, 752, 1156, 168, 160, 578, 976, 1109, 943, 1110, 842, 230, 87, 272, 337, 564, 659, 162, 488, 738, 106, 243, 821, 931, 923, 493, 61, 1182, 126, 756, 483, 985, 1212, 521, 920, 580, 1207, 831, 5, 83, 668, 45, 514, 345, 594, 503, 767, 674, 758, 506, 298, 599, 501, 1060, 1131, 601, 854, 67, 371, 957, 397, 409, 73, 770, 611, 723, 953, 620, 316, 229, 991, 579, 103, 507, 1196, 634, 150, 1065, 576, 851, 1173, 197, 1117, 706, 1154, 683, 828, 672, 1194, 909, 1096, 950, 1097, 1142, 346, 983, 121, 664, 340, 120, 939, 467, 213, 141, 323, 100, 287, 275, 226, 1200, 589, 846, 698, 1213, 441, 385, 252, 928, 1157, 984, 693, 857, 1095, 499, 702, 480, 133, 97, 368, 669, 1169, 1042, 902, 810, 1150, 686, 651, 125, 1044, 526, 1188, 798, 268, 988, 481, 896, 780, 354, 660, 194, 1184, 46, 178, 806, 740, 534, 608, 380, 1166, 111, 644, 755, 543, 829, 1037, 264, 451, 407, 15, 1069, 365, 1026, 728, 367]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1663036519262332
the save name prefix for this run is:  chkpt-ID_1663036519262332_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 773
rank avg (pred): 0.569 +- 0.003
mrr vals (pred, true): 0.013, 0.089
batch losses (mrrl, rdl): 0.0, 0.0016564161

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 616
rank avg (pred): 0.383 +- 0.222
mrr vals (pred, true): 0.150, 0.038
batch losses (mrrl, rdl): 0.0, 3.6368e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 962
rank avg (pred): 0.352 +- 0.259
mrr vals (pred, true): 0.247, 0.040
batch losses (mrrl, rdl): 0.0, 8.59725e-05

Epoch over!
epoch time: 12.284

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1021
rank avg (pred): 0.354 +- 0.256
mrr vals (pred, true): 0.232, 0.123
batch losses (mrrl, rdl): 0.0, 0.0003368482

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 616
rank avg (pred): 0.375 +- 0.275
mrr vals (pred, true): 0.250, 0.038
batch losses (mrrl, rdl): 0.0, 3.24818e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 273
rank avg (pred): 0.044 +- 0.035
mrr vals (pred, true): 0.399, 0.543
batch losses (mrrl, rdl): 0.0, 4.7019e-06

Epoch over!
epoch time: 11.89

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 499
rank avg (pred): 0.296 +- 0.231
mrr vals (pred, true): 0.273, 0.076
batch losses (mrrl, rdl): 0.0, 1.07969e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 292
rank avg (pred): 0.036 +- 0.029
mrr vals (pred, true): 0.419, 0.571
batch losses (mrrl, rdl): 0.0, 2.6822e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 505
rank avg (pred): 0.303 +- 0.236
mrr vals (pred, true): 0.273, 0.074
batch losses (mrrl, rdl): 0.0, 4.338e-06

Epoch over!
epoch time: 11.807

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1143
rank avg (pred): 0.200 +- 0.165
mrr vals (pred, true): 0.331, 0.149
batch losses (mrrl, rdl): 0.0, 4.25095e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 809
rank avg (pred): 0.327 +- 0.252
mrr vals (pred, true): 0.256, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001377215

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 830
rank avg (pred): 0.022 +- 0.018
mrr vals (pred, true): 0.473, 0.448
batch losses (mrrl, rdl): 0.0, 2.03668e-05

Epoch over!
epoch time: 11.781

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 742
rank avg (pred): 0.026 +- 0.021
mrr vals (pred, true): 0.454, 0.512
batch losses (mrrl, rdl): 0.0, 2.7739e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1005
rank avg (pred): 0.320 +- 0.252
mrr vals (pred, true): 0.273, 0.131
batch losses (mrrl, rdl): 0.0, 0.0001833325

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1140
rank avg (pred): 0.230 +- 0.184
mrr vals (pred, true): 0.293, 0.156
batch losses (mrrl, rdl): 0.0, 4.88815e-05

Epoch over!
epoch time: 11.863

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 941
rank avg (pred): 0.341 +- 0.263
mrr vals (pred, true): 0.256, 0.086
batch losses (mrrl, rdl): 0.4238366783, 0.0001064015

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1100
rank avg (pred): 0.385 +- 0.170
mrr vals (pred, true): 0.062, 0.117
batch losses (mrrl, rdl): 0.0296437945, 0.0003719534

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 73
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.568, 0.570
batch losses (mrrl, rdl): 5.70853e-05, 4.5675e-06

Epoch over!
epoch time: 12.048

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1117
rank avg (pred): 0.308 +- 0.175
mrr vals (pred, true): 0.094, 0.039
batch losses (mrrl, rdl): 0.0196304265, 0.0003979271

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1045
rank avg (pred): 0.314 +- 0.157
mrr vals (pred, true): 0.073, 0.040
batch losses (mrrl, rdl): 0.0053228214, 0.0003050459

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 289
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.551, 0.562
batch losses (mrrl, rdl): 0.0012655621, 4.2268e-06

Epoch over!
epoch time: 12.096

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 780
rank avg (pred): 0.298 +- 0.181
mrr vals (pred, true): 0.122, 0.090
batch losses (mrrl, rdl): 0.0513901711, 2.77283e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 899
rank avg (pred): 0.108 +- 0.085
mrr vals (pred, true): 0.249, 0.109
batch losses (mrrl, rdl): 0.1975037903, 0.0001079828

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 976
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.668, 0.610
batch losses (mrrl, rdl): 0.0344160758, 6.2847e-06

Epoch over!
epoch time: 12.138

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 575
rank avg (pred): 0.394 +- 0.367
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001364837, 0.0001151383

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 514
rank avg (pred): 0.406 +- 0.117
mrr vals (pred, true): 0.045, 0.086
batch losses (mrrl, rdl): 0.0002658252, 0.000145793

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 948
rank avg (pred): 0.444 +- 0.238
mrr vals (pred, true): 0.034, 0.038
batch losses (mrrl, rdl): 0.0027026515, 7.843e-06

Epoch over!
epoch time: 12.148

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 277
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.575, 0.570
batch losses (mrrl, rdl): 0.0003012249, 5.6806e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 537
rank avg (pred): 0.417 +- 0.116
mrr vals (pred, true): 0.046, 0.062
batch losses (mrrl, rdl): 0.0001742642, 0.0001308764

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1120
rank avg (pred): 0.211 +- 0.140
mrr vals (pred, true): 0.125, 0.038
batch losses (mrrl, rdl): 0.0568166971, 0.0010723037

Epoch over!
epoch time: 11.956

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 257
rank avg (pred): 0.008 +- 0.007
mrr vals (pred, true): 0.624, 0.624
batch losses (mrrl, rdl): 3.608e-07, 3.5677e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 817
rank avg (pred): 0.021 +- 0.018
mrr vals (pred, true): 0.494, 0.399
batch losses (mrrl, rdl): 0.0909939334, 1.17185e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 952
rank avg (pred): 0.423 +- 0.223
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 1.67616e-05, 5.6836e-06

Epoch over!
epoch time: 12.229

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1049
rank avg (pred): 0.231 +- 0.141
mrr vals (pred, true): 0.112, 0.039
batch losses (mrrl, rdl): 0.0380107574, 0.0007561503

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 932
rank avg (pred): 0.384 +- 0.220
mrr vals (pred, true): 0.044, 0.090
batch losses (mrrl, rdl): 0.0003920971, 0.0001636898

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 57
rank avg (pred): 0.018 +- 0.015
mrr vals (pred, true): 0.487, 0.511
batch losses (mrrl, rdl): 0.0055574165, 3.3114e-06

Epoch over!
epoch time: 12.022

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 872
rank avg (pred): 0.415 +- 0.157
mrr vals (pred, true): 0.062, 0.043
batch losses (mrrl, rdl): 0.0015202713, 3.83849e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 609
rank avg (pred): 0.422 +- 0.397
mrr vals (pred, true): 0.049, 0.032
batch losses (mrrl, rdl): 1.45476e-05, 0.000181564

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 909
rank avg (pred): 0.136 +- 0.113
mrr vals (pred, true): 0.301, 0.239
batch losses (mrrl, rdl): 0.0374158993, 5.18661e-05

Epoch over!
epoch time: 12.023

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 807
rank avg (pred): 0.463 +- 0.176
mrr vals (pred, true): 0.047, 0.041
batch losses (mrrl, rdl): 7.75011e-05, 2.76307e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 859
rank avg (pred): 0.430 +- 0.193
mrr vals (pred, true): 0.058, 0.084
batch losses (mrrl, rdl): 0.0006427796, 0.0003337545

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 950
rank avg (pred): 0.412 +- 0.310
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 3.4012e-06, 4.16766e-05

Epoch over!
epoch time: 12.052

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 227
rank avg (pred): 0.373 +- 0.154
mrr vals (pred, true): 0.082, 0.042
batch losses (mrrl, rdl): 0.0101149911, 0.0001190439

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1033
rank avg (pred): 0.306 +- 0.124
mrr vals (pred, true): 0.078, 0.038
batch losses (mrrl, rdl): 0.008102065, 0.000414782

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 431
rank avg (pred): 0.299 +- 0.125
mrr vals (pred, true): 0.082, 0.041
batch losses (mrrl, rdl): 0.010262955, 0.0004358091

Epoch over!
epoch time: 11.947

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.420 +- 0.380
mrr vals (pred, true): 0.050, 0.036

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   11 	     0 	 0.05167 	 0.03261 	 ~...
   16 	     1 	 0.05294 	 0.03287 	 ~...
   13 	     2 	 0.05175 	 0.03433 	 ~...
   14 	     3 	 0.05216 	 0.03444 	 ~...
   56 	     4 	 0.07053 	 0.03476 	 m..s
   15 	     5 	 0.05253 	 0.03479 	 ~...
    2 	     6 	 0.04970 	 0.03580 	 ~...
   37 	     7 	 0.06325 	 0.03597 	 ~...
   83 	     8 	 0.10195 	 0.03612 	 m..s
    1 	     9 	 0.04953 	 0.03620 	 ~...
   18 	    10 	 0.05469 	 0.03699 	 ~...
   47 	    11 	 0.06867 	 0.03699 	 m..s
   59 	    12 	 0.07154 	 0.03743 	 m..s
   62 	    13 	 0.07619 	 0.03779 	 m..s
    6 	    14 	 0.05111 	 0.03793 	 ~...
   51 	    15 	 0.06948 	 0.03794 	 m..s
   68 	    16 	 0.08451 	 0.03809 	 m..s
   20 	    17 	 0.05538 	 0.03823 	 ~...
   72 	    18 	 0.08828 	 0.03827 	 m..s
   29 	    19 	 0.06181 	 0.03827 	 ~...
   23 	    20 	 0.05772 	 0.03836 	 ~...
   60 	    21 	 0.07159 	 0.03852 	 m..s
   32 	    22 	 0.06221 	 0.03854 	 ~...
   39 	    23 	 0.06428 	 0.03872 	 ~...
   28 	    24 	 0.06170 	 0.03880 	 ~...
   65 	    25 	 0.07733 	 0.03897 	 m..s
   64 	    26 	 0.07640 	 0.03915 	 m..s
    7 	    27 	 0.05133 	 0.03915 	 ~...
   66 	    28 	 0.07755 	 0.03920 	 m..s
   10 	    29 	 0.05165 	 0.03945 	 ~...
   74 	    30 	 0.09067 	 0.03961 	 m..s
    5 	    31 	 0.05067 	 0.03977 	 ~...
   68 	    32 	 0.08451 	 0.04006 	 m..s
    2 	    33 	 0.04970 	 0.04021 	 ~...
   45 	    34 	 0.06674 	 0.04029 	 ~...
   75 	    35 	 0.09574 	 0.04063 	 m..s
   12 	    36 	 0.05174 	 0.04075 	 ~...
   62 	    37 	 0.07619 	 0.04085 	 m..s
   21 	    38 	 0.05542 	 0.04109 	 ~...
    9 	    39 	 0.05135 	 0.04116 	 ~...
    4 	    40 	 0.05059 	 0.04154 	 ~...
   55 	    41 	 0.07036 	 0.04197 	 ~...
   86 	    42 	 0.10276 	 0.04207 	 m..s
   33 	    43 	 0.06288 	 0.04241 	 ~...
   85 	    44 	 0.10264 	 0.04262 	 m..s
   19 	    45 	 0.05472 	 0.04306 	 ~...
   48 	    46 	 0.06886 	 0.04361 	 ~...
    8 	    47 	 0.05135 	 0.04383 	 ~...
   78 	    48 	 0.09677 	 0.04390 	 m..s
   79 	    49 	 0.09751 	 0.04391 	 m..s
    0 	    50 	 0.04951 	 0.04397 	 ~...
   76 	    51 	 0.09582 	 0.04445 	 m..s
   48 	    52 	 0.06886 	 0.04491 	 ~...
   44 	    53 	 0.06637 	 0.04559 	 ~...
   22 	    54 	 0.05709 	 0.04609 	 ~...
   40 	    55 	 0.06443 	 0.06666 	 ~...
   50 	    56 	 0.06916 	 0.06880 	 ~...
   41 	    57 	 0.06481 	 0.06999 	 ~...
   25 	    58 	 0.05812 	 0.07276 	 ~...
   27 	    59 	 0.05922 	 0.07290 	 ~...
   17 	    60 	 0.05426 	 0.07298 	 ~...
   61 	    61 	 0.07600 	 0.07315 	 ~...
   26 	    62 	 0.05823 	 0.07328 	 ~...
   45 	    63 	 0.06674 	 0.07870 	 ~...
   52 	    64 	 0.07006 	 0.08032 	 ~...
   35 	    65 	 0.06303 	 0.08156 	 ~...
   38 	    66 	 0.06370 	 0.08472 	 ~...
   58 	    67 	 0.07085 	 0.08540 	 ~...
   30 	    68 	 0.06195 	 0.08739 	 ~...
   24 	    69 	 0.05812 	 0.08924 	 m..s
   43 	    70 	 0.06596 	 0.08956 	 ~...
   42 	    71 	 0.06526 	 0.08968 	 ~...
   30 	    72 	 0.06195 	 0.09161 	 ~...
   36 	    73 	 0.06315 	 0.09247 	 ~...
   57 	    74 	 0.07068 	 0.09496 	 ~...
   52 	    75 	 0.07006 	 0.09933 	 ~...
   34 	    76 	 0.06295 	 0.10136 	 m..s
   67 	    77 	 0.07880 	 0.10238 	 ~...
   84 	    78 	 0.10259 	 0.10383 	 ~...
   54 	    79 	 0.07008 	 0.10623 	 m..s
   89 	    80 	 0.11755 	 0.10634 	 ~...
   73 	    81 	 0.08920 	 0.11256 	 ~...
   70 	    82 	 0.08472 	 0.11400 	 ~...
   90 	    83 	 0.12121 	 0.11409 	 ~...
   71 	    84 	 0.08693 	 0.11998 	 m..s
   81 	    85 	 0.09912 	 0.13144 	 m..s
   80 	    86 	 0.09761 	 0.13366 	 m..s
   77 	    87 	 0.09632 	 0.13799 	 m..s
   88 	    88 	 0.10532 	 0.14083 	 m..s
   92 	    89 	 0.17056 	 0.14348 	 ~...
   87 	    90 	 0.10523 	 0.14825 	 m..s
   92 	    91 	 0.17056 	 0.15500 	 ~...
   82 	    92 	 0.09923 	 0.15988 	 m..s
   91 	    93 	 0.15488 	 0.16056 	 ~...
   94 	    94 	 0.18565 	 0.21459 	 ~...
   95 	    95 	 0.37630 	 0.30109 	 m..s
   96 	    96 	 0.37705 	 0.33119 	 m..s
   97 	    97 	 0.47583 	 0.42189 	 m..s
  105 	    98 	 0.53309 	 0.43219 	 MISS
   98 	    99 	 0.52502 	 0.49255 	 m..s
  100 	   100 	 0.53081 	 0.51229 	 ~...
   99 	   101 	 0.52594 	 0.51740 	 ~...
  103 	   102 	 0.53240 	 0.51997 	 ~...
  102 	   103 	 0.53231 	 0.52621 	 ~...
  101 	   104 	 0.53203 	 0.54522 	 ~...
  106 	   105 	 0.53612 	 0.55616 	 ~...
  108 	   106 	 0.55119 	 0.55759 	 ~...
  109 	   107 	 0.55715 	 0.57022 	 ~...
  111 	   108 	 0.55743 	 0.57063 	 ~...
  110 	   109 	 0.55729 	 0.57095 	 ~...
  107 	   110 	 0.53943 	 0.58088 	 m..s
  115 	   111 	 0.59906 	 0.59282 	 ~...
  104 	   112 	 0.53248 	 0.59640 	 m..s
  117 	   113 	 0.60531 	 0.60240 	 ~...
  116 	   114 	 0.59922 	 0.60425 	 ~...
  119 	   115 	 0.61451 	 0.61038 	 ~...
  114 	   116 	 0.59697 	 0.61212 	 ~...
  118 	   117 	 0.60547 	 0.61320 	 ~...
  112 	   118 	 0.59075 	 0.61471 	 ~...
  113 	   119 	 0.59083 	 0.62158 	 m..s
  119 	   120 	 0.61451 	 0.63497 	 ~...
==========================================
r_mrr = 0.9890487194061279
r2_mrr = 0.976786732673645
spearmanr_mrr@5 = 0.8269345164299011
spearmanr_mrr@10 = 0.840579628944397
spearmanr_mrr@50 = 0.9932080507278442
spearmanr_mrr@100 = 0.9946495294570923
spearmanr_mrr@All = 0.9950447678565979
==========================================
test time: 0.415
Done Testing dataset UMLS
total time taken: 186.34523248672485
training time taken: 180.77945733070374
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9890)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9768)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.8269)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8406)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9932)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9946)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9950)}}, 'test_loss': {'TransE': {'UMLS': 0.88336957750289}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 188013678276971
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [448, 1140, 706, 938, 920, 713, 512, 955, 343, 247, 251, 650, 624, 1167, 824, 73, 1072, 1052, 315, 31, 1143, 362, 291, 398, 881, 516, 941, 1064, 265, 503, 604, 1028, 993, 359, 583, 1191, 616, 862, 451, 437, 1026, 373, 552, 1208, 934, 301, 915, 555, 827, 102, 111, 490, 456, 261, 755, 727, 271, 211, 921, 811, 59, 1007, 136, 319, 159, 1061, 171, 1042, 815, 368, 700, 514, 417, 699, 903, 104, 355, 491, 878, 345, 679, 924, 865, 684, 848, 1090, 1154, 1111, 868, 329, 1119, 246, 668, 1151, 462, 565, 1011, 392, 1130, 507, 654, 595, 113, 310, 496, 334, 1144, 284, 859, 909, 985, 26, 596, 901, 972, 116, 834, 347, 639, 1006, 64]
valid_ids (0): []
train_ids (1094): [734, 923, 820, 61, 479, 888, 991, 1192, 1084, 775, 959, 344, 433, 60, 976, 761, 337, 214, 85, 235, 103, 78, 790, 300, 1093, 649, 593, 370, 831, 286, 193, 431, 253, 1060, 48, 954, 527, 133, 248, 1166, 323, 629, 348, 690, 902, 95, 515, 1160, 488, 80, 270, 866, 67, 203, 772, 774, 965, 1134, 240, 139, 1145, 622, 471, 958, 267, 852, 750, 625, 1066, 850, 269, 742, 469, 1127, 30, 293, 13, 145, 1124, 427, 1185, 784, 91, 208, 1059, 541, 27, 961, 661, 819, 243, 832, 298, 452, 146, 292, 502, 580, 435, 1152, 495, 169, 33, 705, 289, 328, 696, 330, 672, 776, 358, 646, 52, 242, 1147, 173, 928, 641, 945, 275, 758, 425, 288, 135, 125, 741, 238, 188, 245, 636, 314, 632, 1087, 140, 1039, 974, 953, 789, 485, 4, 1057, 230, 695, 989, 258, 688, 818, 539, 1016, 423, 562, 964, 383, 588, 1073, 1126, 760, 408, 781, 434, 1103, 224, 465, 950, 1034, 1139, 841, 42, 600, 735, 528, 1138, 204, 978, 25, 709, 382, 152, 18, 1010, 1069, 296, 1000, 87, 380, 88, 1070, 256, 883, 162, 384, 1027, 262, 454, 131, 914, 297, 336, 614, 376, 333, 342, 560, 1190, 1189, 49, 36, 659, 160, 788, 179, 510, 890, 1032, 470, 353, 405, 917, 35, 1, 257, 759, 43, 1180, 57, 17, 123, 803, 341, 1047, 872, 1079, 83, 157, 24, 6, 797, 704, 611, 712, 178, 1009, 1213, 281, 693, 236, 806, 810, 143, 0, 129, 1101, 692, 112, 108, 906, 586, 263, 79, 1102, 655, 436, 896, 1062, 278, 364, 187, 498, 1086, 874, 1125, 899, 780, 725, 466, 678, 927, 299, 1179, 174, 844, 324, 77, 1038, 482, 508, 190, 519, 846, 607, 545, 69, 394, 82, 717, 168, 1211, 99, 305, 295, 898, 971, 241, 1053, 492, 949, 664, 254, 418, 463, 520, 1163, 833, 402, 1077, 894, 880, 737, 363, 182, 1020, 1155, 445, 326, 547, 1022, 432, 575, 1033, 360, 925, 1193, 396, 533, 406, 266, 645, 592, 707, 480, 656, 130, 1178, 1108, 674, 826, 1168, 536, 598, 980, 773, 889, 745, 449, 115, 651, 316, 748, 724, 209, 1097, 1099, 986, 711, 200, 121, 181, 340, 829, 54, 155, 177, 1162, 642, 792, 893, 46, 733, 926, 671, 122, 335, 1063, 919, 802, 609, 682, 1199, 1159, 1201, 777, 1049, 1068, 578, 809, 1023, 442, 93, 1210, 500, 723, 569, 1091, 369, 677, 1196, 887, 814, 239, 106, 877, 752, 601, 97, 387, 791, 1209, 1116, 1114, 793, 997, 1165, 1177, 1109, 252, 105, 984, 150, 1129, 154, 134, 1078, 1107, 447, 746, 215, 443, 56, 937, 72, 1149, 1133, 557, 356, 892, 415, 44, 1045, 666, 863, 107, 414, 631, 786, 51, 977, 1182, 563, 210, 956, 232, 703, 313, 932, 566, 764, 633, 1141, 823, 697, 783, 626, 897, 474, 172, 517, 627, 1195, 378, 464, 982, 47, 948, 2, 184, 762, 216, 1002, 572, 352, 1054, 778, 548, 207, 571, 873, 610, 1089, 1104, 603, 581, 264, 75, 272, 867, 1025, 732, 426, 176, 553, 990, 65, 992, 701, 1004, 579, 768, 605, 1136, 1171, 5, 259, 165, 274, 838, 148, 416, 501, 412, 973, 331, 311, 694, 837, 618, 744, 141, 255, 233, 619, 1008, 879, 870, 404, 225, 635, 975, 8, 658, 568, 615, 506, 119, 804, 419, 166, 1001, 808, 205, 836, 497, 101, 570, 918, 1187, 484, 96, 62, 967, 1065, 726, 722, 461, 933, 41, 429, 1153, 637, 845, 856, 98, 529, 321, 673, 411, 339, 1029, 683, 486, 1169, 90, 186, 161, 1015, 936, 453, 1115, 317, 1112, 390, 1118, 280, 708, 1174, 822, 294, 439, 621, 1113, 1173, 460, 499, 74, 351, 638, 11, 1048, 306, 620, 1212, 189, 231, 170, 1003, 1202, 63, 195, 250, 202, 285, 374, 10, 222, 21, 652, 851, 58, 312, 244, 676, 930, 180, 81, 308, 84, 935, 276, 350, 1088, 37, 675, 559, 191, 957, 891, 441, 194, 736, 1204, 346, 494, 1146, 669, 908, 1206, 1161, 1095, 458, 1021, 381, 869, 361, 142, 389, 302, 132, 522, 393, 1150, 147, 525, 151, 770, 1110, 540, 640, 28, 662, 318, 840, 537, 385, 686, 564, 630, 531, 1158, 785, 756, 857, 943, 354, 218, 357, 794, 535, 660, 719, 32, 1198, 981, 1157, 422, 825, 446, 50, 283, 807, 511, 468, 237, 249, 749, 798, 963, 183, 1050, 307, 648, 1031, 766, 7, 667, 574, 573, 864, 1067, 279, 89, 1131, 407, 118, 994, 738, 1044, 1075, 542, 1186, 110, 634, 303, 1071, 613, 702, 413, 591, 226, 504, 623, 397, 403, 916, 39, 710, 117, 1024, 830, 680, 922, 799, 821, 1098, 728, 584, 729, 521, 1156, 260, 332, 475, 1184, 558, 175, 1203, 327, 1043, 290, 585, 375, 716, 409, 1017, 787, 951, 1056, 721, 444, 489, 472, 45, 577, 450, 907, 66, 731, 1018, 1005, 886, 1122, 718, 1040, 969, 882, 1121, 29, 1058, 944, 860, 372, 582, 968, 1092, 526, 843, 282, 338, 400, 1170, 939, 952, 1120, 40, 1105, 556, 1012, 228, 68, 534, 156, 192, 1080, 754, 608, 1200, 153, 587, 518, 430, 128, 467, 219, 395, 979, 23, 849, 998, 1106, 277, 962, 1037, 3, 1181, 1013, 532, 871, 483, 670, 858, 1148, 628, 320, 144, 388, 590, 199, 1081, 367, 795, 94, 842, 647, 1100, 1137, 1096, 322, 1172, 1051, 455, 365, 796, 1117, 853, 546, 38, 15, 86, 714, 904, 206, 905, 910, 687, 549, 19, 550, 1183, 987, 782, 771, 665, 1197, 597, 911, 401, 1135, 213, 1175, 988, 653, 900, 1142, 109, 481, 767, 715, 421, 551, 440, 828, 942, 966, 371, 124, 420, 201, 14, 554, 854, 685, 1164, 876, 234, 839, 273, 995, 1035, 304, 544, 1094, 92, 120, 386, 509, 126, 895, 543, 763, 524, 602, 999, 720, 220, 1207, 55, 366, 138, 801, 813, 1019, 114, 1214, 1014, 391, 309, 164, 473, 730, 1128, 855, 1194, 127, 612, 599, 197, 561, 1055, 698, 1082, 399, 487, 1188, 1041, 478, 817, 983, 76, 884, 223, 753, 1205, 816, 137, 765, 530, 589, 428, 457, 523, 960, 71, 513, 812, 185, 1083, 739, 377, 349, 221, 158, 912, 198, 663, 1176, 740, 149, 594, 1036, 1076, 644, 459, 847, 379, 16, 70, 913, 1123, 212, 217, 227, 196, 410, 163, 885, 996, 606, 493, 1132, 779, 576, 691, 9, 970, 20, 538, 757, 34, 931, 946, 229, 438, 1085, 167, 929, 805, 940, 617, 800, 53, 769, 643, 22, 751, 477, 875, 743, 1074, 747, 681, 424, 689, 268, 947, 1046, 505, 861, 657, 100, 12, 1030, 476, 287, 567, 325, 835]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7414060921254504
the save name prefix for this run is:  chkpt-ID_7414060921254504_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 401
rank avg (pred): 0.558 +- 0.004
mrr vals (pred, true): 0.013, 0.148
batch losses (mrrl, rdl): 0.0, 0.0025241752

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 200
rank avg (pred): 0.359 +- 0.178
mrr vals (pred, true): 0.058, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001166999

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1038
rank avg (pred): 0.337 +- 0.239
mrr vals (pred, true): 0.191, 0.038
batch losses (mrrl, rdl): 0.0, 0.000152313

Epoch over!
epoch time: 12.138

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1053
rank avg (pred): 0.034 +- 0.026
mrr vals (pred, true): 0.397, 0.603
batch losses (mrrl, rdl): 0.0, 2.166e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 463
rank avg (pred): 0.319 +- 0.241
mrr vals (pred, true): 0.229, 0.037
batch losses (mrrl, rdl): 0.0, 0.0002658906

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 137
rank avg (pred): 0.348 +- 0.262
mrr vals (pred, true): 0.232, 0.093
batch losses (mrrl, rdl): 0.0, 0.0001879583

Epoch over!
epoch time: 11.988

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 181
rank avg (pred): 0.364 +- 0.260
mrr vals (pred, true): 0.200, 0.042
batch losses (mrrl, rdl): 0.0, 4.91689e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 617
rank avg (pred): 0.382 +- 0.272
mrr vals (pred, true): 0.217, 0.040
batch losses (mrrl, rdl): 0.0, 1.28108e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 435
rank avg (pred): 0.346 +- 0.252
mrr vals (pred, true): 0.214, 0.040
batch losses (mrrl, rdl): 0.0, 7.67847e-05

Epoch over!
epoch time: 11.854

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 671
rank avg (pred): 0.387 +- 0.271
mrr vals (pred, true): 0.209, 0.040
batch losses (mrrl, rdl): 0.0, 3.53994e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 476
rank avg (pred): 0.314 +- 0.249
mrr vals (pred, true): 0.273, 0.039
batch losses (mrrl, rdl): 0.0, 0.0002077947

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1197
rank avg (pred): 0.393 +- 0.271
mrr vals (pred, true): 0.207, 0.039
batch losses (mrrl, rdl): 0.0, 1.5222e-05

Epoch over!
epoch time: 11.926

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 761
rank avg (pred): 0.307 +- 0.247
mrr vals (pred, true): 0.287, 0.089
batch losses (mrrl, rdl): 0.0, 6.94354e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 923
rank avg (pred): 0.354 +- 0.263
mrr vals (pred, true): 0.257, 0.090
batch losses (mrrl, rdl): 0.0, 0.0002132346

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 340
rank avg (pred): 0.328 +- 0.253
mrr vals (pred, true): 0.259, 0.116
batch losses (mrrl, rdl): 0.0, 0.0001984798

Epoch over!
epoch time: 12.083

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 210
rank avg (pred): 0.336 +- 0.261
mrr vals (pred, true): 0.274, 0.038
batch losses (mrrl, rdl): 0.5006177425, 0.0001429898

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1043
rank avg (pred): 0.385 +- 0.164
mrr vals (pred, true): 0.064, 0.038
batch losses (mrrl, rdl): 0.0020169793, 0.0001123195

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1156
rank avg (pred): 0.309 +- 0.222
mrr vals (pred, true): 0.207, 0.143
batch losses (mrrl, rdl): 0.0403145179, 0.0002442646

Epoch over!
epoch time: 12.454

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 553
rank avg (pred): 0.364 +- 0.156
mrr vals (pred, true): 0.063, 0.073
batch losses (mrrl, rdl): 0.0016155739, 4.56634e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 542
rank avg (pred): 0.326 +- 0.153
mrr vals (pred, true): 0.083, 0.118
batch losses (mrrl, rdl): 0.0123352353, 6.04247e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 991
rank avg (pred): 0.010 +- 0.009
mrr vals (pred, true): 0.596, 0.614
batch losses (mrrl, rdl): 0.0031762891, 3.463e-06

Epoch over!
epoch time: 12.115

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 82
rank avg (pred): 0.300 +- 0.147
mrr vals (pred, true): 0.070, 0.086
batch losses (mrrl, rdl): 0.003823394, 3.17281e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 870
rank avg (pred): 0.341 +- 0.137
mrr vals (pred, true): 0.051, 0.036
batch losses (mrrl, rdl): 1.5859e-05, 0.0003315474

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 524
rank avg (pred): 0.296 +- 0.153
mrr vals (pred, true): 0.102, 0.117
batch losses (mrrl, rdl): 0.0023519676, 3.74193e-05

Epoch over!
epoch time: 12.183

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1062
rank avg (pred): 0.010 +- 0.009
mrr vals (pred, true): 0.611, 0.622
batch losses (mrrl, rdl): 0.0011916675, 2.8831e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 773
rank avg (pred): 0.329 +- 0.138
mrr vals (pred, true): 0.050, 0.089
batch losses (mrrl, rdl): 6.684e-07, 5.61143e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 289
rank avg (pred): 0.011 +- 0.010
mrr vals (pred, true): 0.607, 0.562
batch losses (mrrl, rdl): 0.0205145888, 4.3135e-06

Epoch over!
epoch time: 12.173

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 847
rank avg (pred): 0.322 +- 0.143
mrr vals (pred, true): 0.053, 0.087
batch losses (mrrl, rdl): 0.000119812, 4.64884e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 550
rank avg (pred): 0.297 +- 0.156
mrr vals (pred, true): 0.112, 0.081
batch losses (mrrl, rdl): 0.0381527916, 6.76446e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 617
rank avg (pred): 0.307 +- 0.175
mrr vals (pred, true): 0.047, 0.040
batch losses (mrrl, rdl): 6.68248e-05, 0.0002793687

Epoch over!
epoch time: 12.147

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 854
rank avg (pred): 0.333 +- 0.139
mrr vals (pred, true): 0.052, 0.091
batch losses (mrrl, rdl): 6.23906e-05, 9.61391e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 809
rank avg (pred): 0.353 +- 0.134
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 2.18777e-05, 0.0001868057

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 91
rank avg (pred): 0.326 +- 0.142
mrr vals (pred, true): 0.054, 0.102
batch losses (mrrl, rdl): 0.0232366435, 0.0001567356

Epoch over!
epoch time: 12.236

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 505
rank avg (pred): 0.310 +- 0.149
mrr vals (pred, true): 0.060, 0.074
batch losses (mrrl, rdl): 0.0009527934, 3.17656e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 504
rank avg (pred): 0.311 +- 0.149
mrr vals (pred, true): 0.059, 0.073
batch losses (mrrl, rdl): 0.0008602453, 3.06698e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 69
rank avg (pred): 0.020 +- 0.020
mrr vals (pred, true): 0.570, 0.522
batch losses (mrrl, rdl): 0.0235863756, 3.569e-06

Epoch over!
epoch time: 11.928

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 996
rank avg (pred): 0.012 +- 0.012
mrr vals (pred, true): 0.620, 0.623
batch losses (mrrl, rdl): 9.9969e-05, 2.6342e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 694
rank avg (pred): 0.314 +- 0.178
mrr vals (pred, true): 0.048, 0.041
batch losses (mrrl, rdl): 4.95011e-05, 0.0003183713

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1067
rank avg (pred): 0.013 +- 0.014
mrr vals (pred, true): 0.615, 0.595
batch losses (mrrl, rdl): 0.0039162496, 1.3189e-06

Epoch over!
epoch time: 11.957

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 187
rank avg (pred): 0.336 +- 0.144
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.000244717, 0.00023244

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1050
rank avg (pred): 0.308 +- 0.155
mrr vals (pred, true): 0.105, 0.041
batch losses (mrrl, rdl): 0.0306704827, 0.0003527155

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 676
rank avg (pred): 0.305 +- 0.193
mrr vals (pred, true): 0.051, 0.038
batch losses (mrrl, rdl): 1.65155e-05, 0.0003799349

Epoch over!
epoch time: 12.044

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1195
rank avg (pred): 0.347 +- 0.155
mrr vals (pred, true): 0.041, 0.038
batch losses (mrrl, rdl): 0.0008262767, 0.0002154102

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 206
rank avg (pred): 0.319 +- 0.155
mrr vals (pred, true): 0.059, 0.041
batch losses (mrrl, rdl): 0.000742225, 0.0002733569

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 306
rank avg (pred): 0.026 +- 0.028
mrr vals (pred, true): 0.580, 0.532
batch losses (mrrl, rdl): 0.0228864849, 1.298e-07

Epoch over!
epoch time: 12.073

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.305 +- 0.155
mrr vals (pred, true): 0.096, 0.040

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.05536 	 0.03066 	 ~...
    4 	     1 	 0.05528 	 0.03444 	 ~...
   34 	     2 	 0.07599 	 0.03488 	 m..s
   24 	     3 	 0.06825 	 0.03532 	 m..s
   14 	     4 	 0.05628 	 0.03578 	 ~...
   22 	     5 	 0.06589 	 0.03620 	 ~...
    4 	     6 	 0.05528 	 0.03667 	 ~...
    9 	     7 	 0.05538 	 0.03693 	 ~...
   13 	     8 	 0.05607 	 0.03703 	 ~...
    3 	     9 	 0.05525 	 0.03724 	 ~...
   25 	    10 	 0.06873 	 0.03735 	 m..s
   10 	    11 	 0.05556 	 0.03747 	 ~...
   33 	    12 	 0.07574 	 0.03787 	 m..s
   68 	    13 	 0.10988 	 0.03801 	 m..s
   15 	    14 	 0.05667 	 0.03817 	 ~...
    7 	    15 	 0.05536 	 0.03824 	 ~...
    1 	    16 	 0.05523 	 0.03851 	 ~...
   41 	    17 	 0.08643 	 0.03854 	 m..s
   76 	    18 	 0.12415 	 0.03883 	 m..s
   82 	    19 	 0.14615 	 0.03922 	 MISS
   73 	    20 	 0.12014 	 0.03942 	 m..s
   55 	    21 	 0.09776 	 0.03943 	 m..s
   81 	    22 	 0.14353 	 0.03959 	 MISS
   77 	    23 	 0.12633 	 0.03962 	 m..s
   50 	    24 	 0.09624 	 0.03969 	 m..s
   20 	    25 	 0.05793 	 0.03982 	 ~...
    0 	    26 	 0.05523 	 0.04000 	 ~...
   44 	    27 	 0.09280 	 0.04026 	 m..s
   66 	    28 	 0.10690 	 0.04049 	 m..s
   16 	    29 	 0.05745 	 0.04070 	 ~...
   21 	    30 	 0.05797 	 0.04112 	 ~...
    2 	    31 	 0.05525 	 0.04116 	 ~...
   23 	    32 	 0.06821 	 0.04126 	 ~...
   72 	    33 	 0.11860 	 0.04136 	 m..s
   16 	    34 	 0.05745 	 0.04156 	 ~...
    6 	    35 	 0.05530 	 0.04186 	 ~...
   29 	    36 	 0.07154 	 0.04191 	 ~...
   18 	    37 	 0.05747 	 0.04263 	 ~...
   75 	    38 	 0.12111 	 0.04390 	 m..s
   43 	    39 	 0.09278 	 0.04598 	 m..s
   67 	    40 	 0.10763 	 0.06424 	 m..s
   63 	    41 	 0.10031 	 0.06558 	 m..s
   42 	    42 	 0.09110 	 0.06812 	 ~...
   12 	    43 	 0.05605 	 0.06919 	 ~...
   71 	    44 	 0.11185 	 0.07245 	 m..s
   62 	    45 	 0.09947 	 0.07290 	 ~...
   45 	    46 	 0.09356 	 0.07479 	 ~...
   38 	    47 	 0.08015 	 0.07572 	 ~...
   28 	    48 	 0.07052 	 0.08156 	 ~...
   27 	    49 	 0.06949 	 0.08401 	 ~...
   46 	    50 	 0.09505 	 0.08472 	 ~...
   11 	    51 	 0.05563 	 0.08496 	 ~...
   53 	    52 	 0.09728 	 0.08580 	 ~...
   30 	    53 	 0.07229 	 0.08599 	 ~...
   32 	    54 	 0.07317 	 0.08625 	 ~...
   19 	    55 	 0.05762 	 0.09013 	 m..s
   37 	    56 	 0.07910 	 0.09155 	 ~...
   49 	    57 	 0.09603 	 0.09173 	 ~...
   35 	    58 	 0.07801 	 0.09245 	 ~...
   31 	    59 	 0.07308 	 0.09672 	 ~...
   26 	    60 	 0.06914 	 0.10033 	 m..s
   60 	    61 	 0.09943 	 0.10210 	 ~...
   40 	    62 	 0.08365 	 0.10674 	 ~...
   35 	    63 	 0.07801 	 0.10675 	 ~...
   70 	    64 	 0.11132 	 0.10811 	 ~...
   48 	    65 	 0.09568 	 0.10863 	 ~...
   51 	    66 	 0.09662 	 0.11629 	 ~...
   59 	    67 	 0.09938 	 0.12120 	 ~...
   65 	    68 	 0.10073 	 0.12187 	 ~...
   38 	    69 	 0.08015 	 0.12202 	 m..s
   64 	    70 	 0.10065 	 0.12681 	 ~...
   57 	    71 	 0.09906 	 0.12726 	 ~...
   74 	    72 	 0.12044 	 0.12771 	 ~...
   52 	    73 	 0.09687 	 0.12885 	 m..s
   46 	    74 	 0.09505 	 0.13060 	 m..s
   79 	    75 	 0.13040 	 0.13367 	 ~...
   58 	    76 	 0.09932 	 0.13503 	 m..s
   69 	    77 	 0.11061 	 0.13923 	 ~...
   54 	    78 	 0.09745 	 0.14083 	 m..s
   56 	    79 	 0.09784 	 0.14091 	 m..s
   60 	    80 	 0.09943 	 0.14459 	 m..s
   78 	    81 	 0.12952 	 0.14478 	 ~...
   86 	    82 	 0.22060 	 0.14620 	 m..s
   84 	    83 	 0.19997 	 0.14944 	 m..s
   80 	    84 	 0.13198 	 0.14973 	 ~...
   85 	    85 	 0.20494 	 0.15153 	 m..s
   83 	    86 	 0.18130 	 0.15601 	 ~...
   87 	    87 	 0.22950 	 0.16156 	 m..s
   88 	    88 	 0.29439 	 0.23945 	 m..s
   90 	    89 	 0.30083 	 0.24515 	 m..s
   89 	    90 	 0.29635 	 0.30753 	 ~...
   91 	    91 	 0.30853 	 0.33468 	 ~...
   93 	    92 	 0.50901 	 0.43006 	 m..s
  113 	    93 	 0.57222 	 0.51292 	 m..s
  105 	    94 	 0.56582 	 0.51304 	 m..s
  112 	    95 	 0.57014 	 0.51602 	 m..s
  109 	    96 	 0.56952 	 0.51757 	 m..s
   92 	    97 	 0.50507 	 0.53204 	 ~...
   94 	    98 	 0.52937 	 0.54415 	 ~...
  105 	    99 	 0.56582 	 0.54533 	 ~...
  114 	   100 	 0.57283 	 0.54594 	 ~...
  100 	   101 	 0.56255 	 0.55759 	 ~...
  111 	   102 	 0.57012 	 0.56204 	 ~...
  108 	   103 	 0.56816 	 0.56998 	 ~...
   99 	   104 	 0.56203 	 0.57062 	 ~...
  101 	   105 	 0.56277 	 0.57063 	 ~...
  102 	   106 	 0.56330 	 0.57095 	 ~...
  103 	   107 	 0.56440 	 0.57564 	 ~...
   97 	   108 	 0.53345 	 0.57695 	 m..s
  117 	   109 	 0.62184 	 0.57735 	 m..s
   96 	   110 	 0.53242 	 0.58131 	 m..s
   95 	   111 	 0.53187 	 0.58296 	 m..s
   98 	   112 	 0.56201 	 0.60193 	 m..s
  119 	   113 	 0.62475 	 0.60247 	 ~...
  115 	   114 	 0.62066 	 0.60827 	 ~...
  107 	   115 	 0.56755 	 0.61212 	 m..s
  120 	   116 	 0.64084 	 0.61521 	 ~...
  118 	   117 	 0.62317 	 0.61839 	 ~...
  110 	   118 	 0.57004 	 0.61885 	 m..s
  116 	   119 	 0.62164 	 0.61930 	 ~...
  104 	   120 	 0.56443 	 0.63332 	 m..s
==========================================
r_mrr = 0.9847081899642944
r2_mrr = 0.9662284851074219
spearmanr_mrr@5 = 0.9872422814369202
spearmanr_mrr@10 = 0.8574017286300659
spearmanr_mrr@50 = 0.9921619892120361
spearmanr_mrr@100 = 0.9953216314315796
spearmanr_mrr@All = 0.99588942527771
==========================================
test time: 0.389
Done Testing dataset UMLS
total time taken: 187.33314967155457
training time taken: 181.76211643218994
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9847)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9662)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9872)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8574)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9922)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9953)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9959)}}, 'test_loss': {'TransE': {'UMLS': 1.759048226474988}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 8444590309522739
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [76, 165, 1189, 1037, 297, 477, 961, 298, 419, 1020, 662, 283, 252, 1198, 217, 107, 625, 53, 718, 309, 1026, 719, 1126, 695, 276, 336, 63, 775, 966, 513, 69, 9, 698, 450, 933, 1138, 476, 796, 1068, 1158, 833, 468, 246, 1109, 764, 1072, 1167, 720, 1173, 866, 249, 488, 219, 347, 663, 949, 1036, 12, 870, 948, 378, 1103, 478, 624, 729, 105, 1120, 587, 1062, 907, 205, 597, 694, 969, 575, 245, 728, 794, 186, 1063, 1153, 1134, 1028, 810, 855, 380, 51, 777, 655, 84, 376, 548, 388, 512, 1106, 418, 1172, 860, 287, 807, 1097, 492, 1016, 788, 1193, 199, 881, 830, 52, 772, 838, 504, 940, 1168, 67, 1092, 639, 410, 349, 306, 740]
valid_ids (0): []
train_ids (1094): [316, 883, 960, 590, 745, 806, 113, 333, 6, 634, 282, 224, 391, 100, 657, 366, 1075, 439, 344, 24, 676, 85, 467, 771, 364, 1034, 699, 536, 129, 227, 1123, 198, 483, 289, 799, 1061, 218, 882, 660, 352, 626, 659, 299, 592, 399, 516, 572, 1080, 386, 646, 820, 501, 171, 371, 260, 102, 1005, 327, 528, 704, 827, 871, 1132, 658, 184, 1119, 836, 974, 1145, 610, 200, 640, 884, 317, 363, 954, 464, 1130, 578, 278, 749, 593, 632, 161, 737, 191, 222, 1013, 466, 1127, 338, 277, 1212, 1083, 1104, 19, 786, 446, 232, 251, 300, 511, 579, 25, 918, 742, 1024, 325, 425, 550, 911, 226, 598, 411, 677, 782, 343, 586, 842, 875, 1115, 485, 1003, 326, 221, 817, 731, 0, 955, 549, 557, 608, 864, 945, 1078, 95, 18, 1162, 194, 906, 475, 137, 1043, 725, 576, 38, 1164, 987, 151, 43, 116, 1211, 1131, 708, 407, 714, 1204, 319, 872, 233, 329, 210, 917, 484, 717, 862, 398, 45, 1121, 547, 944, 808, 878, 994, 766, 1214, 438, 902, 41, 1160, 888, 111, 818, 259, 1091, 652, 1105, 874, 225, 1066, 527, 192, 971, 859, 992, 507, 611, 1152, 334, 594, 1067, 583, 112, 675, 1056, 1151, 175, 840, 891, 591, 561, 998, 465, 361, 519, 1004, 551, 254, 824, 821, 1009, 279, 440, 953, 1179, 815, 869, 1018, 978, 294, 152, 1015, 189, 795, 1012, 1213, 372, 641, 1038, 711, 423, 1156, 750, 401, 500, 243, 709, 479, 865, 999, 964, 1190, 1186, 1055, 975, 1117, 1114, 724, 1176, 895, 90, 257, 631, 781, 916, 451, 16, 293, 901, 1049, 56, 377, 1031, 1157, 877, 931, 497, 89, 558, 828, 995, 207, 1079, 912, 448, 495, 846, 546, 285, 145, 650, 403, 427, 216, 605, 211, 397, 885, 253, 1202, 898, 685, 1163, 702, 793, 235, 360, 845, 314, 305, 1140, 915, 973, 564, 230, 302, 1101, 890, 664, 447, 988, 80, 453, 751, 413, 730, 124, 1144, 1074, 496, 613, 762, 980, 1141, 288, 643, 726, 1082, 369, 942, 989, 754, 819, 62, 803, 900, 585, 792, 78, 1011, 311, 61, 160, 455, 1139, 541, 435, 963, 441, 48, 4, 710, 854, 537, 125, 602, 164, 588, 979, 991, 458, 573, 193, 798, 722, 134, 935, 7, 957, 1154, 1006, 17, 237, 426, 560, 983, 236, 837, 930, 581, 33, 228, 248, 802, 787, 733, 149, 168, 545, 339, 679, 1107, 402, 665, 540, 672, 746, 559, 263, 647, 926, 35, 101, 616, 150, 568, 91, 542, 385, 32, 959, 269, 522, 517, 370, 11, 582, 622, 144, 462, 1094, 1064, 563, 1051, 108, 82, 571, 206, 58, 908, 744, 445, 104, 812, 554, 437, 518, 400, 460, 456, 143, 1057, 753, 894, 1108, 505, 654, 690, 972, 123, 1021, 805, 684, 159, 47, 538, 977, 280, 472, 923, 158, 167, 574, 223, 118, 135, 39, 619, 10, 743, 521, 14, 270, 858, 117, 1116, 213, 1030, 1178, 984, 1122, 436, 509, 748, 444, 406, 1027, 22, 967, 1175, 706, 1085, 229, 741, 284, 1185, 290, 375, 1194, 274, 180, 739, 539, 351, 498, 968, 928, 331, 785, 404, 889, 629, 201, 530, 1102, 31, 23, 244, 183, 952, 146, 520, 122, 59, 1195, 927, 75, 1135, 357, 36, 1088, 181, 394, 153, 1183, 320, 767, 196, 667, 947, 166, 454, 13, 569, 555, 422, 275, 1201, 524, 127, 487, 301, 345, 212, 98, 723, 81, 374, 382, 162, 1184, 332, 147, 801, 489, 847, 886, 321, 261, 128, 656, 929, 997, 922, 609, 1040, 126, 876, 156, 532, 544, 44, 919, 896, 1071, 379, 1090, 409, 661, 1008, 1035, 1019, 1147, 308, 508, 474, 635, 323, 670, 490, 5, 88, 1205, 307, 687, 1001, 770, 328, 946, 420, 1033, 809, 531, 666, 66, 633, 1060, 615, 982, 1089, 442, 303, 179, 1118, 1196, 50, 494, 849, 29, 1069, 1133, 214, 202, 703, 603, 457, 178, 1045, 242, 873, 1039, 271, 797, 73, 716, 140, 950, 1150, 1041, 177, 848, 1046, 1148, 843, 1146, 256, 491, 757, 553, 1029, 1136, 627, 342, 250, 682, 239, 829, 231, 1014, 638, 700, 296, 503, 1042, 408, 674, 139, 486, 774, 20, 705, 238, 1054, 607, 1199, 783, 822, 72, 1182, 258, 1174, 1099, 241, 1207, 1169, 359, 904, 1050, 697, 292, 965, 350, 3, 30, 70, 390, 46, 892, 94, 1170, 312, 692, 99, 768, 234, 932, 459, 120, 264, 119, 1076, 42, 387, 49, 392, 396, 1208, 804, 693, 905, 247, 1161, 920, 480, 92, 188, 1191, 71, 620, 1053, 368, 880, 909, 141, 93, 636, 132, 1000, 1192, 861, 358, 623, 473, 1166, 169, 1113, 267, 913, 310, 956, 776, 595, 738, 867, 434, 1044, 707, 565, 1111, 903, 506, 841, 962, 514, 163, 823, 157, 384, 1022, 1149, 715, 381, 1181, 77, 780, 649, 1187, 1081, 197, 863, 567, 57, 1047, 470, 562, 1200, 1032, 1155, 130, 64, 552, 760, 173, 1010, 834, 1007, 1077, 8, 255, 1143, 15, 266, 596, 1098, 800, 172, 732, 1058, 668, 1084, 354, 535, 533, 170, 897, 893, 938, 761, 240, 621, 826, 735, 713, 1002, 773, 529, 60, 421, 27, 87, 993, 996, 618, 601, 315, 934, 758, 121, 452, 272, 681, 68, 1197, 790, 482, 2, 1129, 645, 789, 103, 1112, 493, 37, 778, 182, 752, 148, 510, 471, 925, 1203, 1159, 499, 1142, 853, 612, 268, 577, 281, 1070, 1171, 174, 154, 96, 405, 689, 1165, 97, 651, 1025, 40, 262, 831, 449, 110, 734, 109, 1125, 1209, 628, 79, 642, 1128, 1, 791, 1023, 970, 1073, 921, 373, 736, 653, 644, 857, 114, 958, 851, 155, 669, 337, 543, 424, 136, 1110, 106, 696, 365, 187, 678, 839, 131, 1124, 1188, 526, 763, 415, 769, 941, 176, 712, 525, 566, 813, 324, 1086, 1095, 1052, 630, 431, 584, 556, 220, 295, 747, 1100, 1096, 825, 142, 1210, 356, 291, 195, 852, 887, 463, 515, 721, 850, 606, 688, 937, 215, 910, 637, 614, 976, 835, 814, 856, 346, 1087, 1180, 534, 26, 648, 580, 1017, 924, 765, 727, 185, 432, 570, 686, 86, 265, 589, 74, 939, 318, 416, 65, 383, 832, 443, 55, 523, 417, 502, 1206, 412, 868, 208, 429, 691, 353, 341, 599, 83, 680, 811, 286, 304, 393, 335, 469, 330, 844, 115, 1093, 203, 617, 951, 362, 395, 701, 367, 755, 981, 1065, 914, 355, 816, 1059, 28, 34, 190, 600, 943, 759, 673, 671, 21, 985, 348, 879, 683, 1048, 756, 209, 204, 340, 461, 138, 1137, 433, 430, 936, 1177, 779, 784, 481, 54, 899, 322, 414, 604, 273, 313, 389, 986, 428, 133, 990]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4876231479988150
the save name prefix for this run is:  chkpt-ID_4876231479988150_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 975
rank avg (pred): 0.511 +- 0.008
mrr vals (pred, true): 0.014, 0.618
batch losses (mrrl, rdl): 0.0, 0.0051275194

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 571
rank avg (pred): 0.380 +- 0.240
mrr vals (pred, true): 0.210, 0.030
batch losses (mrrl, rdl): 0.0, 5.23299e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 32
rank avg (pred): 0.025 +- 0.019
mrr vals (pred, true): 0.440, 0.611
batch losses (mrrl, rdl): 0.0, 1.982e-07

Epoch over!
epoch time: 11.871

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 976
rank avg (pred): 0.031 +- 0.024
mrr vals (pred, true): 0.436, 0.610
batch losses (mrrl, rdl): 0.0, 1.5033e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 308
rank avg (pred): 0.017 +- 0.014
mrr vals (pred, true): 0.500, 0.619
batch losses (mrrl, rdl): 0.0, 1.828e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1175
rank avg (pred): 0.364 +- 0.267
mrr vals (pred, true): 0.290, 0.038
batch losses (mrrl, rdl): 0.0, 1.39994e-05

Epoch over!
epoch time: 11.922

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 257
rank avg (pred): 0.021 +- 0.017
mrr vals (pred, true): 0.470, 0.624
batch losses (mrrl, rdl): 0.0, 2.11e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 293
rank avg (pred): 0.023 +- 0.018
mrr vals (pred, true): 0.481, 0.628
batch losses (mrrl, rdl): 0.0, 1.7e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 896
rank avg (pred): 0.093 +- 0.075
mrr vals (pred, true): 0.379, 0.172
batch losses (mrrl, rdl): 0.0, 1.84844e-05

Epoch over!
epoch time: 11.813

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 912
rank avg (pred): 0.080 +- 0.064
mrr vals (pred, true): 0.392, 0.301
batch losses (mrrl, rdl): 0.0, 9.8205e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 136
rank avg (pred): 0.325 +- 0.254
mrr vals (pred, true): 0.316, 0.092
batch losses (mrrl, rdl): 0.0, 0.0001177905

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 398
rank avg (pred): 0.326 +- 0.251
mrr vals (pred, true): 0.305, 0.139
batch losses (mrrl, rdl): 0.0, 0.0002852679

Epoch over!
epoch time: 11.832

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 975
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.562, 0.618
batch losses (mrrl, rdl): 0.0, 2.9651e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 353
rank avg (pred): 0.333 +- 0.254
mrr vals (pred, true): 0.305, 0.138
batch losses (mrrl, rdl): 0.0, 0.0002997885

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 970
rank avg (pred): 0.348 +- 0.252
mrr vals (pred, true): 0.287, 0.043
batch losses (mrrl, rdl): 0.0, 8.51686e-05

Epoch over!
epoch time: 11.928

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 825
rank avg (pred): 0.020 +- 0.016
mrr vals (pred, true): 0.502, 0.581
batch losses (mrrl, rdl): 0.0614481494, 1.1366e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 985
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.597, 0.619
batch losses (mrrl, rdl): 0.0051680435, 5.519e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 661
rank avg (pred): 0.428 +- 0.102
mrr vals (pred, true): 0.036, 0.041
batch losses (mrrl, rdl): 0.0019735706, 6.54834e-05

Epoch over!
epoch time: 12.097

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 185
rank avg (pred): 0.413 +- 0.176
mrr vals (pred, true): 0.062, 0.044
batch losses (mrrl, rdl): 0.0014210952, 3.02312e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 786
rank avg (pred): 0.402 +- 0.195
mrr vals (pred, true): 0.066, 0.038
batch losses (mrrl, rdl): 0.0025979499, 5.13027e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 931
rank avg (pred): 0.381 +- 0.146
mrr vals (pred, true): 0.058, 0.093
batch losses (mrrl, rdl): 0.0006485677, 0.0002249368

Epoch over!
epoch time: 12.07

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1144
rank avg (pred): 0.104 +- 0.059
mrr vals (pred, true): 0.172, 0.152
batch losses (mrrl, rdl): 0.0041372939, 0.0004453326

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1171
rank avg (pred): 0.390 +- 0.124
mrr vals (pred, true): 0.051, 0.036
batch losses (mrrl, rdl): 1.06169e-05, 5.43922e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1002
rank avg (pred): 0.347 +- 0.183
mrr vals (pred, true): 0.082, 0.118
batch losses (mrrl, rdl): 0.0130569972, 0.0001881398

Epoch over!
epoch time: 12.033

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 140
rank avg (pred): 0.384 +- 0.205
mrr vals (pred, true): 0.078, 0.110
batch losses (mrrl, rdl): 0.0106771979, 0.0003811114

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 666
rank avg (pred): 0.416 +- 0.083
mrr vals (pred, true): 0.031, 0.038
batch losses (mrrl, rdl): 0.0037940755, 7.70291e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 148
rank avg (pred): 0.405 +- 0.203
mrr vals (pred, true): 0.071, 0.099
batch losses (mrrl, rdl): 0.0044653765, 0.0004575072

Epoch over!
epoch time: 12.123

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 319
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.603, 0.562
batch losses (mrrl, rdl): 0.0170173869, 6.8765e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 146
rank avg (pred): 0.403 +- 0.199
mrr vals (pred, true): 0.070, 0.110
batch losses (mrrl, rdl): 0.0157281086, 0.0005268775

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1152
rank avg (pred): 0.067 +- 0.037
mrr vals (pred, true): 0.195, 0.150
batch losses (mrrl, rdl): 0.0205559004, 0.0007803873

Epoch over!
epoch time: 12.179

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 531
rank avg (pred): 0.343 +- 0.135
mrr vals (pred, true): 0.060, 0.067
batch losses (mrrl, rdl): 0.0010518233, 3.36806e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 285
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.598, 0.512
batch losses (mrrl, rdl): 0.0726692528, 1.22961e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 541
rank avg (pred): 0.314 +- 0.151
mrr vals (pred, true): 0.086, 0.069
batch losses (mrrl, rdl): 0.0133175971, 5.41319e-05

Epoch over!
epoch time: 12.746

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 456
rank avg (pred): 0.365 +- 0.155
mrr vals (pred, true): 0.062, 0.038
batch losses (mrrl, rdl): 0.0014649791, 0.0001663499

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 955
rank avg (pred): 0.392 +- 0.152
mrr vals (pred, true): 0.059, 0.041
batch losses (mrrl, rdl): 0.0008215077, 5.35381e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 251
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.613, 0.612
batch losses (mrrl, rdl): 6.4758e-06, 5.1045e-06

Epoch over!
epoch time: 13.29

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 930
rank avg (pred): 0.340 +- 0.121
mrr vals (pred, true): 0.057, 0.089
batch losses (mrrl, rdl): 0.0004823322, 8.92642e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 415
rank avg (pred): 0.358 +- 0.142
mrr vals (pred, true): 0.058, 0.039
batch losses (mrrl, rdl): 0.0007209845, 0.0002044427

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 150
rank avg (pred): 0.388 +- 0.141
mrr vals (pred, true): 0.054, 0.087
batch losses (mrrl, rdl): 0.00014746, 0.0002740952

Epoch over!
epoch time: 13.39

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 773
rank avg (pred): 0.459 +- 0.208
mrr vals (pred, true): 0.059, 0.089
batch losses (mrrl, rdl): 0.000896316, 0.0006477853

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 471
rank avg (pred): 0.373 +- 0.162
mrr vals (pred, true): 0.068, 0.037
batch losses (mrrl, rdl): 0.0033366322, 0.0001235187

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 47
rank avg (pred): 0.005 +- 0.004
mrr vals (pred, true): 0.656, 0.621
batch losses (mrrl, rdl): 0.012643449, 5.0478e-06

Epoch over!
epoch time: 12.048

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 555
rank avg (pred): 0.353 +- 0.110
mrr vals (pred, true): 0.061, 0.064
batch losses (mrrl, rdl): 0.0011429187, 4.81799e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 688
rank avg (pred): 0.379 +- 0.078
mrr vals (pred, true): 0.046, 0.044
batch losses (mrrl, rdl): 0.00012578, 0.0001540631

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 206
rank avg (pred): 0.311 +- 0.132
mrr vals (pred, true): 0.062, 0.041
batch losses (mrrl, rdl): 0.0013945509, 0.0003258176

Epoch over!
epoch time: 11.949

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.583, 0.551

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04731 	 0.03066 	 ~...
   55 	     1 	 0.07262 	 0.03363 	 m..s
   21 	     2 	 0.05399 	 0.03374 	 ~...
   10 	     3 	 0.05181 	 0.03530 	 ~...
   18 	     4 	 0.05361 	 0.03532 	 ~...
   54 	     5 	 0.07246 	 0.03553 	 m..s
    4 	     6 	 0.04739 	 0.03578 	 ~...
    1 	     7 	 0.04731 	 0.03582 	 ~...
   73 	     8 	 0.10629 	 0.03589 	 m..s
   52 	     9 	 0.06781 	 0.03595 	 m..s
   79 	    10 	 0.11048 	 0.03643 	 m..s
   22 	    11 	 0.05406 	 0.03669 	 ~...
   13 	    12 	 0.05263 	 0.03702 	 ~...
    5 	    13 	 0.04835 	 0.03754 	 ~...
   68 	    14 	 0.10113 	 0.03757 	 m..s
   69 	    15 	 0.10139 	 0.03776 	 m..s
   23 	    16 	 0.05749 	 0.03794 	 ~...
   70 	    17 	 0.10154 	 0.03811 	 m..s
   12 	    18 	 0.05232 	 0.03862 	 ~...
   84 	    19 	 0.12296 	 0.03867 	 m..s
    0 	    20 	 0.04704 	 0.03874 	 ~...
   66 	    21 	 0.10064 	 0.03883 	 m..s
   13 	    22 	 0.05263 	 0.03908 	 ~...
   55 	    23 	 0.07262 	 0.03913 	 m..s
   38 	    24 	 0.06603 	 0.03913 	 ~...
   38 	    25 	 0.06603 	 0.03917 	 ~...
   17 	    26 	 0.05297 	 0.03933 	 ~...
    6 	    27 	 0.04866 	 0.03948 	 ~...
   66 	    28 	 0.10064 	 0.03962 	 m..s
    7 	    29 	 0.04866 	 0.03977 	 ~...
   15 	    30 	 0.05263 	 0.03977 	 ~...
   53 	    31 	 0.06931 	 0.03991 	 ~...
   75 	    32 	 0.10902 	 0.04036 	 m..s
   63 	    33 	 0.09162 	 0.04050 	 m..s
    9 	    34 	 0.05180 	 0.04075 	 ~...
    8 	    35 	 0.04877 	 0.04098 	 ~...
   20 	    36 	 0.05392 	 0.04116 	 ~...
   51 	    37 	 0.06717 	 0.04117 	 ~...
   38 	    38 	 0.06603 	 0.04122 	 ~...
   73 	    39 	 0.10629 	 0.04136 	 m..s
   35 	    40 	 0.06490 	 0.04149 	 ~...
   32 	    41 	 0.06271 	 0.04153 	 ~...
   49 	    42 	 0.06699 	 0.04191 	 ~...
   38 	    43 	 0.06603 	 0.04193 	 ~...
   38 	    44 	 0.06603 	 0.04197 	 ~...
   18 	    45 	 0.05361 	 0.04220 	 ~...
   23 	    46 	 0.05749 	 0.04276 	 ~...
   60 	    47 	 0.07945 	 0.04326 	 m..s
   61 	    48 	 0.07959 	 0.04361 	 m..s
   57 	    49 	 0.07669 	 0.04373 	 m..s
   16 	    50 	 0.05291 	 0.04397 	 ~...
   25 	    51 	 0.06026 	 0.04418 	 ~...
    3 	    52 	 0.04735 	 0.04474 	 ~...
   27 	    53 	 0.06091 	 0.04536 	 ~...
   28 	    54 	 0.06093 	 0.04611 	 ~...
   11 	    55 	 0.05216 	 0.04650 	 ~...
   34 	    56 	 0.06340 	 0.06454 	 ~...
   47 	    57 	 0.06649 	 0.06999 	 ~...
   33 	    58 	 0.06276 	 0.07276 	 ~...
   30 	    59 	 0.06190 	 0.07298 	 ~...
   58 	    60 	 0.07822 	 0.08032 	 ~...
   31 	    61 	 0.06222 	 0.08426 	 ~...
   38 	    62 	 0.06603 	 0.08482 	 ~...
   38 	    63 	 0.06603 	 0.08605 	 ~...
   29 	    64 	 0.06096 	 0.08739 	 ~...
   48 	    65 	 0.06682 	 0.08747 	 ~...
   50 	    66 	 0.06716 	 0.08775 	 ~...
   38 	    67 	 0.06603 	 0.08934 	 ~...
   26 	    68 	 0.06031 	 0.09010 	 ~...
   36 	    69 	 0.06495 	 0.09103 	 ~...
   38 	    70 	 0.06603 	 0.09661 	 m..s
   37 	    71 	 0.06533 	 0.09999 	 m..s
   65 	    72 	 0.09392 	 0.10902 	 ~...
   62 	    73 	 0.09096 	 0.11169 	 ~...
   83 	    74 	 0.11748 	 0.11248 	 ~...
   81 	    75 	 0.11347 	 0.11629 	 ~...
   64 	    76 	 0.09389 	 0.11872 	 ~...
   76 	    77 	 0.10963 	 0.11978 	 ~...
   59 	    78 	 0.07896 	 0.12672 	 m..s
   82 	    79 	 0.11680 	 0.12700 	 ~...
   71 	    80 	 0.10538 	 0.13054 	 ~...
   86 	    81 	 0.12617 	 0.13087 	 ~...
   80 	    82 	 0.11070 	 0.13812 	 ~...
   85 	    83 	 0.12436 	 0.14459 	 ~...
   72 	    84 	 0.10578 	 0.14897 	 m..s
   77 	    85 	 0.10966 	 0.15802 	 m..s
   89 	    86 	 0.15111 	 0.15805 	 ~...
   78 	    87 	 0.11008 	 0.15988 	 m..s
   88 	    88 	 0.14968 	 0.16007 	 ~...
   87 	    89 	 0.14822 	 0.16056 	 ~...
   90 	    90 	 0.15644 	 0.16461 	 ~...
   91 	    91 	 0.29554 	 0.33070 	 m..s
   92 	    92 	 0.51050 	 0.42189 	 m..s
   93 	    93 	 0.52631 	 0.43259 	 m..s
   96 	    94 	 0.52951 	 0.44254 	 m..s
  100 	    95 	 0.54593 	 0.44759 	 m..s
   94 	    96 	 0.52827 	 0.48976 	 m..s
   97 	    97 	 0.53718 	 0.49741 	 m..s
   98 	    98 	 0.54449 	 0.49823 	 m..s
  104 	    99 	 0.55289 	 0.50617 	 m..s
   94 	   100 	 0.52827 	 0.50860 	 ~...
   98 	   101 	 0.54449 	 0.51304 	 m..s
  101 	   102 	 0.54637 	 0.52169 	 ~...
  107 	   103 	 0.56632 	 0.52298 	 m..s
  101 	   104 	 0.54637 	 0.53031 	 ~...
  105 	   105 	 0.56479 	 0.53173 	 m..s
  107 	   106 	 0.56632 	 0.53229 	 m..s
  110 	   107 	 0.58247 	 0.53235 	 m..s
  106 	   108 	 0.56496 	 0.53417 	 m..s
  109 	   109 	 0.57576 	 0.54805 	 ~...
  111 	   110 	 0.58264 	 0.55065 	 m..s
  103 	   111 	 0.54726 	 0.55234 	 ~...
  112 	   112 	 0.59487 	 0.55857 	 m..s
  113 	   113 	 0.59609 	 0.57022 	 ~...
  118 	   114 	 0.65659 	 0.57735 	 m..s
  120 	   115 	 0.66488 	 0.60529 	 m..s
  116 	   116 	 0.65642 	 0.60541 	 m..s
  114 	   117 	 0.63843 	 0.61199 	 ~...
  116 	   118 	 0.65642 	 0.62217 	 m..s
  115 	   119 	 0.64343 	 0.62676 	 ~...
  119 	   120 	 0.65732 	 0.63164 	 ~...
==========================================
r_mrr = 0.9898566007614136
r2_mrr = 0.9683980941772461
spearmanr_mrr@5 = 0.6872130632400513
spearmanr_mrr@10 = 0.9384680390357971
spearmanr_mrr@50 = 0.9954898953437805
spearmanr_mrr@100 = 0.9956660270690918
spearmanr_mrr@All = 0.9960455298423767
==========================================
test time: 0.435
Done Testing dataset UMLS
total time taken: 189.47358179092407
training time taken: 183.79884576797485
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9899)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9684)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.6872)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9385)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9955)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9957)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9960)}}, 'test_loss': {'TransE': {'UMLS': 1.3131231546431081}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 5978971835762898
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [369, 120, 795, 94, 37, 1130, 45, 753, 684, 674, 696, 166, 659, 281, 115, 1064, 167, 584, 435, 230, 151, 103, 1155, 440, 305, 551, 735, 609, 228, 393, 700, 145, 149, 979, 1079, 504, 234, 314, 960, 56, 1180, 245, 448, 755, 642, 152, 127, 450, 1044, 349, 293, 79, 3, 1066, 630, 1060, 987, 612, 477, 489, 126, 456, 134, 377, 338, 1020, 1012, 886, 1095, 318, 853, 449, 434, 452, 692, 1055, 643, 699, 2, 889, 421, 874, 763, 367, 767, 806, 591, 399, 28, 430, 774, 657, 292, 352, 77, 733, 303, 278, 284, 271, 731, 32, 563, 727, 388, 695, 796, 1062, 29, 108, 70, 770, 593, 326, 507, 673, 137, 202, 896, 36, 60]
valid_ids (0): []
train_ids (1094): [836, 219, 906, 1108, 1028, 835, 778, 285, 1188, 701, 539, 88, 231, 82, 344, 1103, 244, 559, 408, 819, 373, 962, 1045, 1074, 636, 924, 365, 378, 572, 30, 663, 1073, 233, 375, 1153, 676, 57, 794, 777, 485, 342, 1000, 904, 486, 1, 150, 279, 73, 1134, 358, 282, 834, 158, 518, 46, 928, 1024, 764, 900, 501, 533, 42, 788, 930, 585, 850, 1147, 514, 320, 1082, 62, 8, 997, 33, 481, 811, 797, 535, 865, 851, 468, 394, 1131, 1009, 210, 100, 154, 1029, 945, 333, 597, 856, 43, 959, 1139, 536, 1206, 474, 807, 1112, 398, 1152, 467, 1038, 144, 54, 756, 649, 466, 654, 236, 901, 1101, 383, 1199, 269, 634, 1173, 520, 578, 723, 1201, 39, 1089, 718, 111, 340, 446, 689, 624, 1058, 482, 223, 698, 222, 498, 1156, 725, 800, 266, 976, 620, 571, 991, 368, 295, 346, 427, 569, 929, 783, 263, 799, 53, 948, 982, 445, 552, 191, 74, 1176, 1174, 980, 509, 419, 250, 64, 1068, 59, 632, 220, 923, 112, 709, 1136, 854, 537, 458, 1181, 490, 878, 55, 354, 156, 1026, 370, 1075, 740, 357, 561, 376, 1013, 613, 267, 10, 617, 493, 195, 883, 229, 1123, 603, 970, 1034, 265, 893, 1115, 206, 50, 1110, 704, 720, 784, 26, 499, 1076, 844, 791, 61, 863, 337, 892, 86, 1172, 290, 1063, 608, 650, 429, 1177, 1190, 93, 644, 1143, 255, 885, 629, 1141, 760, 451, 779, 817, 1184, 528, 175, 538, 633, 803, 25, 882, 18, 666, 589, 312, 992, 905, 833, 23, 1135, 240, 251, 204, 707, 356, 869, 1167, 647, 761, 1121, 1118, 862, 994, 1178, 1146, 384, 1168, 128, 990, 706, 768, 981, 1039, 575, 616, 185, 690, 464, 570, 940, 469, 502, 76, 1050, 762, 957, 1124, 343, 436, 1200, 136, 1195, 804, 1054, 545, 899, 306, 416, 147, 789, 826, 14, 621, 180, 574, 247, 871, 51, 280, 652, 34, 1086, 159, 553, 773, 606, 867, 1003, 1014, 69, 1059, 177, 380, 140, 668, 955, 168, 268, 703, 525, 1070, 772, 734, 876, 179, 732, 550, 442, 1017, 476, 232, 1088, 736, 861, 505, 825, 98, 497, 1032, 832, 323, 996, 1021, 249, 1071, 995, 967, 40, 386, 11, 463, 963, 728, 974, 1025, 347, 403, 895, 1209, 171, 390, 917, 1056, 371, 218, 461, 745, 364, 860, 645, 207, 192, 96, 1030, 106, 586, 546, 161, 888, 837, 679, 238, 1036, 565, 381, 661, 1037, 66, 38, 129, 299, 1212, 1149, 361, 838, 330, 1207, 1057, 1004, 527, 488, 260, 410, 363, 1041, 12, 694, 1120, 24, 397, 710, 1114, 109, 557, 224, 939, 261, 592, 677, 664, 444, 843, 27, 739, 573, 366, 857, 242, 1191, 387, 576, 500, 170, 712, 362, 877, 226, 359, 396, 5, 614, 301, 107, 812, 153, 172, 431, 447, 688, 872, 717, 1154, 201, 726, 311, 691, 71, 830, 534, 667, 0, 1011, 355, 300, 908, 125, 771, 286, 521, 680, 287, 164, 360, 414, 954, 903, 217, 933, 610, 855, 273, 927, 142, 309, 316, 22, 187, 818, 407, 462, 1091, 852, 291, 7, 600, 298, 480, 1019, 105, 952, 588, 155, 1043, 1104, 328, 189, 315, 1016, 622, 988, 926, 160, 915, 880, 969, 313, 749, 814, 200, 526, 524, 846, 656, 548, 1122, 1077, 675, 1093, 582, 289, 389, 67, 637, 457, 41, 351, 759, 1162, 382, 971, 277, 935, 625, 1150, 769, 133, 859, 215, 124, 141, 822, 670, 1144, 197, 47, 567, 516, 748, 583, 983, 1008, 121, 44, 1015, 1163, 925, 214, 1052, 919, 907, 15, 58, 441, 581, 638, 198, 1080, 1182, 671, 1048, 4, 619, 602, 181, 90, 385, 203, 813, 114, 1145, 765, 1100, 78, 868, 472, 437, 824, 6, 395, 35, 747, 785, 491, 635, 782, 989, 21, 640, 1208, 790, 345, 465, 938, 681, 248, 1194, 555, 601, 823, 708, 662, 512, 68, 746, 870, 1179, 506, 1159, 1196, 296, 798, 1203, 805, 669, 473, 958, 544, 423, 1072, 1046, 65, 225, 847, 246, 890, 1160, 227, 986, 1157, 235, 63, 757, 319, 627, 16, 169, 1061, 831, 401, 237, 912, 484, 122, 1113, 1127, 542, 1125, 460, 212, 751, 193, 19, 110, 148, 475, 272, 288, 307, 1211, 325, 993, 417, 426, 116, 776, 1069, 1165, 594, 787, 875, 1205, 420, 839, 335, 239, 372, 693, 322, 91, 99, 428, 302, 257, 579, 174, 540, 658, 1001, 199, 1098, 182, 631, 781, 913, 178, 615, 1002, 711, 20, 503, 1053, 117, 123, 660, 866, 80, 1099, 827, 921, 937, 750, 580, 842, 392, 492, 946, 968, 184, 1106, 391, 157, 879, 519, 941, 332, 510, 132, 495, 998, 1105, 1027, 453, 966, 1119, 262, 840, 716, 702, 118, 902, 1051, 415, 1092, 741, 1006, 729, 549, 808, 329, 183, 598, 81, 918, 1133, 1085, 1161, 264, 494, 83, 909, 422, 1187, 683, 1204, 241, 276, 1164, 1166, 715, 884, 459, 119, 374, 400, 596, 139, 530, 1067, 845, 705, 49, 1040, 1111, 984, 1007, 243, 1158, 1137, 438, 327, 1102, 858, 1065, 775, 1169, 639, 496, 651, 522, 252, 841, 413, 881, 911, 547, 934, 944, 953, 554, 810, 604, 104, 873, 529, 1151, 1186, 424, 999, 914, 1083, 478, 1116, 646, 1214, 221, 211, 849, 1078, 1128, 1109, 411, 1189, 531, 758, 665, 730, 283, 898, 714, 936, 682, 517, 947, 256, 87, 1084, 1018, 931, 949, 821, 253, 321, 942, 13, 828, 829, 17, 891, 802, 102, 961, 1097, 331, 766, 72, 532, 687, 1140, 194, 623, 176, 1132, 820, 412, 560, 590, 1138, 566, 1033, 595, 1183, 568, 894, 562, 965, 1193, 916, 173, 95, 208, 1087, 483, 956, 1192, 425, 190, 738, 628, 641, 697, 165, 722, 724, 186, 310, 487, 686, 848, 213, 815, 897, 1094, 932, 618, 1031, 743, 471, 339, 439, 1210, 443, 607, 1185, 1005, 887, 943, 801, 977, 75, 648, 1049, 341, 577, 1081, 455, 146, 143, 985, 1170, 508, 52, 754, 270, 1142, 744, 742, 1042, 1202, 1010, 543, 479, 135, 209, 1175, 304, 324, 1117, 163, 317, 922, 672, 786, 1035, 433, 752, 97, 541, 864, 1096, 1090, 274, 611, 964, 85, 780, 406, 587, 972, 48, 721, 336, 920, 511, 1023, 1213, 259, 1129, 162, 405, 816, 432, 353, 404, 713, 950, 719, 685, 308, 84, 605, 951, 258, 1171, 350, 564, 793, 1047, 975, 409, 294, 101, 9, 275, 1198, 626, 205, 454, 978, 653, 348, 402, 910, 196, 113, 1197, 1022, 334, 138, 1107, 513, 678, 1126, 130, 973, 556, 558, 418, 216, 379, 792, 131, 737, 655, 297, 188, 599, 89, 809, 31, 254, 1148, 470, 523, 92, 515]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1443753691825442
the save name prefix for this run is:  chkpt-ID_1443753691825442_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 829
rank avg (pred): 0.505 +- 0.007
mrr vals (pred, true): 0.015, 0.431
batch losses (mrrl, rdl): 0.0, 0.004411479

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 315
rank avg (pred): 0.061 +- 0.060
mrr vals (pred, true): 0.278, 0.518
batch losses (mrrl, rdl): 0.0, 2.77583e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 553
rank avg (pred): 0.296 +- 0.247
mrr vals (pred, true): 0.246, 0.073
batch losses (mrrl, rdl): 0.0, 1.87794e-05

Epoch over!
epoch time: 12.656

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 966
rank avg (pred): 0.347 +- 0.254
mrr vals (pred, true): 0.176, 0.044
batch losses (mrrl, rdl): 0.0, 9.15321e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1018
rank avg (pred): 0.325 +- 0.251
mrr vals (pred, true): 0.198, 0.111
batch losses (mrrl, rdl): 0.0, 0.0001270488

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 487
rank avg (pred): 0.309 +- 0.251
mrr vals (pred, true): 0.226, 0.082
batch losses (mrrl, rdl): 0.0, 4.2275e-06

Epoch over!
epoch time: 12.206

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 304
rank avg (pred): 0.028 +- 0.026
mrr vals (pred, true): 0.452, 0.580
batch losses (mrrl, rdl): 0.0, 2.363e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1191
rank avg (pred): 0.374 +- 0.258
mrr vals (pred, true): 0.160, 0.036
batch losses (mrrl, rdl): 0.0, 5.15026e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 648
rank avg (pred): 0.416 +- 0.271
mrr vals (pred, true): 0.160, 0.043
batch losses (mrrl, rdl): 0.0, 1.11068e-05

Epoch over!
epoch time: 12.091

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1034
rank avg (pred): 0.308 +- 0.258
mrr vals (pred, true): 0.268, 0.041
batch losses (mrrl, rdl): 0.0, 0.0002274707

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1069
rank avg (pred): 0.021 +- 0.019
mrr vals (pred, true): 0.493, 0.605
batch losses (mrrl, rdl): 0.0, 1.61e-08

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1084
rank avg (pred): 0.316 +- 0.262
mrr vals (pred, true): 0.254, 0.148
batch losses (mrrl, rdl): 0.0, 0.0002905344

Epoch over!
epoch time: 12.434

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 730
rank avg (pred): 0.016 +- 0.015
mrr vals (pred, true): 0.546, 0.444
batch losses (mrrl, rdl): 0.0, 3.09507e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1014
rank avg (pred): 0.320 +- 0.258
mrr vals (pred, true): 0.231, 0.138
batch losses (mrrl, rdl): 0.0, 0.0001901967

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1137
rank avg (pred): 0.204 +- 0.175
mrr vals (pred, true): 0.304, 0.165
batch losses (mrrl, rdl): 0.0, 1.35866e-05

Epoch over!
epoch time: 12.463

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 20
rank avg (pred): 0.028 +- 0.027
mrr vals (pred, true): 0.474, 0.608
batch losses (mrrl, rdl): 0.1801824868, 1.0593e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 368
rank avg (pred): 0.373 +- 0.198
mrr vals (pred, true): 0.087, 0.135
batch losses (mrrl, rdl): 0.0232438389, 0.0004471714

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 811
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.551, 0.430
batch losses (mrrl, rdl): 0.1464056522, 4.50566e-05

Epoch over!
epoch time: 12.779

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 192
rank avg (pred): 0.404 +- 0.192
mrr vals (pred, true): 0.060, 0.041
batch losses (mrrl, rdl): 0.000979566, 4.19301e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 324
rank avg (pred): 0.368 +- 0.196
mrr vals (pred, true): 0.077, 0.101
batch losses (mrrl, rdl): 0.0059899031, 0.0002143753

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1198
rank avg (pred): 0.418 +- 0.130
mrr vals (pred, true): 0.037, 0.041
batch losses (mrrl, rdl): 0.0017769272, 5.05976e-05

Epoch over!
epoch time: 12.375

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 199
rank avg (pred): 0.383 +- 0.192
mrr vals (pred, true): 0.064, 0.034
batch losses (mrrl, rdl): 0.0020190706, 0.000100943

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1174
rank avg (pred): 0.303 +- 0.118
mrr vals (pred, true): 0.052, 0.036
batch losses (mrrl, rdl): 3.48223e-05, 0.0002943903

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 622
rank avg (pred): 0.273 +- 0.205
mrr vals (pred, true): 0.047, 0.037
batch losses (mrrl, rdl): 0.0001113743, 0.0004927402

Epoch over!
epoch time: 12.788

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 219
rank avg (pred): 0.389 +- 0.172
mrr vals (pred, true): 0.043, 0.041
batch losses (mrrl, rdl): 0.0005305726, 9.13424e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 898
rank avg (pred): 0.101 +- 0.075
mrr vals (pred, true): 0.243, 0.102
batch losses (mrrl, rdl): 0.1999070942, 0.0001466783

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 193
rank avg (pred): 0.360 +- 0.182
mrr vals (pred, true): 0.047, 0.040
batch losses (mrrl, rdl): 0.0001024409, 0.0001140552

Epoch over!
epoch time: 12.993

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 557
rank avg (pred): 0.285 +- 0.195
mrr vals (pred, true): 0.118, 0.115
batch losses (mrrl, rdl): 0.0001266962, 2.80865e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 797
rank avg (pred): 0.357 +- 0.181
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.83221e-05, 0.0001314099

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 518
rank avg (pred): 0.279 +- 0.190
mrr vals (pred, true): 0.110, 0.114
batch losses (mrrl, rdl): 0.0001881737, 3.2332e-05

Epoch over!
epoch time: 12.306

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 832
rank avg (pred): 0.013 +- 0.010
mrr vals (pred, true): 0.516, 0.528
batch losses (mrrl, rdl): 0.00125495, 1.01763e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 784
rank avg (pred): 0.338 +- 0.178
mrr vals (pred, true): 0.055, 0.041
batch losses (mrrl, rdl): 0.0002492814, 0.0002107948

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 493
rank avg (pred): 0.304 +- 0.159
mrr vals (pred, true): 0.056, 0.083
batch losses (mrrl, rdl): 0.0004074138, 3.30403e-05

Epoch over!
epoch time: 12.279

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 877
rank avg (pred): 0.344 +- 0.179
mrr vals (pred, true): 0.054, 0.042
batch losses (mrrl, rdl): 0.0001822509, 0.0001402356

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1201
rank avg (pred): 0.338 +- 0.279
mrr vals (pred, true): 0.041, 0.036
batch losses (mrrl, rdl): 0.0007888941, 0.0002108937

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 532
rank avg (pred): 0.338 +- 0.218
mrr vals (pred, true): 0.079, 0.069
batch losses (mrrl, rdl): 0.0084888507, 1.47608e-05

Epoch over!
epoch time: 12.769

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 769
rank avg (pred): 0.333 +- 0.187
mrr vals (pred, true): 0.064, 0.076
batch losses (mrrl, rdl): 0.0020847176, 2.42946e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 952
rank avg (pred): 0.340 +- 0.137
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 0.0001085673, 0.0001767861

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 926
rank avg (pred): 0.339 +- 0.158
mrr vals (pred, true): 0.050, 0.076
batch losses (mrrl, rdl): 1.6286e-06, 4.26243e-05

Epoch over!
epoch time: 12.358

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 927
rank avg (pred): 0.348 +- 0.123
mrr vals (pred, true): 0.037, 0.092
batch losses (mrrl, rdl): 0.0017361026, 0.0001255257

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1067
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.636, 0.595
batch losses (mrrl, rdl): 0.0173516925, 4.736e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 550
rank avg (pred): 0.348 +- 0.204
mrr vals (pred, true): 0.055, 0.081
batch losses (mrrl, rdl): 0.000207661, 1.36582e-05

Epoch over!
epoch time: 12.87

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 178
rank avg (pred): 0.323 +- 0.168
mrr vals (pred, true): 0.054, 0.041
batch losses (mrrl, rdl): 0.0001940303, 0.0002344521

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 336
rank avg (pred): 0.313 +- 0.174
mrr vals (pred, true): 0.068, 0.100
batch losses (mrrl, rdl): 0.0032404866, 8.80274e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 698
rank avg (pred): 0.381 +- 0.348
mrr vals (pred, true): 0.046, 0.039
batch losses (mrrl, rdl): 0.0001729536, 0.0001314437

Epoch over!
epoch time: 12.108

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.296 +- 0.171
mrr vals (pred, true): 0.068, 0.106

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   32 	     0 	 0.05536 	 0.03224 	 ~...
   33 	     1 	 0.05564 	 0.03226 	 ~...
   37 	     2 	 0.05773 	 0.03433 	 ~...
   43 	     3 	 0.06168 	 0.03475 	 ~...
   13 	     4 	 0.05070 	 0.03569 	 ~...
   34 	     5 	 0.05570 	 0.03580 	 ~...
    4 	     6 	 0.04249 	 0.03699 	 ~...
   14 	     7 	 0.05091 	 0.03702 	 ~...
   20 	     8 	 0.05342 	 0.03702 	 ~...
   11 	     9 	 0.04858 	 0.03768 	 ~...
   60 	    10 	 0.07825 	 0.03795 	 m..s
   50 	    11 	 0.06429 	 0.03801 	 ~...
   20 	    12 	 0.05342 	 0.03818 	 ~...
    6 	    13 	 0.04528 	 0.03823 	 ~...
   49 	    14 	 0.06332 	 0.03852 	 ~...
   15 	    15 	 0.05138 	 0.03880 	 ~...
   44 	    16 	 0.06168 	 0.03882 	 ~...
   61 	    17 	 0.07877 	 0.03897 	 m..s
   75 	    18 	 0.09164 	 0.03914 	 m..s
    8 	    19 	 0.04609 	 0.03915 	 ~...
   74 	    20 	 0.08993 	 0.03954 	 m..s
   51 	    21 	 0.06632 	 0.03969 	 ~...
    3 	    22 	 0.04227 	 0.03971 	 ~...
   31 	    23 	 0.05427 	 0.03973 	 ~...
   20 	    24 	 0.05342 	 0.04013 	 ~...
   59 	    25 	 0.07084 	 0.04029 	 m..s
   72 	    26 	 0.08842 	 0.04034 	 m..s
   39 	    27 	 0.05826 	 0.04041 	 ~...
   45 	    28 	 0.06183 	 0.04055 	 ~...
   41 	    29 	 0.06056 	 0.04060 	 ~...
   42 	    30 	 0.06164 	 0.04070 	 ~...
   35 	    31 	 0.05578 	 0.04116 	 ~...
   17 	    32 	 0.05240 	 0.04117 	 ~...
    9 	    33 	 0.04624 	 0.04153 	 ~...
   36 	    34 	 0.05770 	 0.04156 	 ~...
    2 	    35 	 0.04199 	 0.04163 	 ~...
   20 	    36 	 0.05342 	 0.04197 	 ~...
   19 	    37 	 0.05291 	 0.04208 	 ~...
   73 	    38 	 0.08904 	 0.04227 	 m..s
   81 	    39 	 0.10160 	 0.04262 	 m..s
   38 	    40 	 0.05776 	 0.04263 	 ~...
   66 	    41 	 0.08244 	 0.04281 	 m..s
   20 	    42 	 0.05342 	 0.04284 	 ~...
   53 	    43 	 0.06662 	 0.04326 	 ~...
   52 	    44 	 0.06657 	 0.04361 	 ~...
   67 	    45 	 0.08255 	 0.04390 	 m..s
   20 	    46 	 0.05342 	 0.06666 	 ~...
    1 	    47 	 0.04037 	 0.06812 	 ~...
    0 	    48 	 0.04034 	 0.07298 	 m..s
   30 	    49 	 0.05427 	 0.07833 	 ~...
   12 	    50 	 0.04882 	 0.07855 	 ~...
   10 	    51 	 0.04656 	 0.07881 	 m..s
    7 	    52 	 0.04551 	 0.08490 	 m..s
    5 	    53 	 0.04331 	 0.08812 	 m..s
   29 	    54 	 0.05375 	 0.08933 	 m..s
   46 	    55 	 0.06286 	 0.08968 	 ~...
   20 	    56 	 0.05342 	 0.09154 	 m..s
   70 	    57 	 0.08586 	 0.09271 	 ~...
   48 	    58 	 0.06299 	 0.09637 	 m..s
   20 	    59 	 0.05342 	 0.09651 	 m..s
   40 	    60 	 0.05888 	 0.09653 	 m..s
   47 	    61 	 0.06292 	 0.09954 	 m..s
   18 	    62 	 0.05283 	 0.10102 	 m..s
   16 	    63 	 0.05204 	 0.10161 	 m..s
   20 	    64 	 0.05342 	 0.10163 	 m..s
   56 	    65 	 0.06780 	 0.10623 	 m..s
   55 	    66 	 0.06749 	 0.10885 	 m..s
   68 	    67 	 0.08492 	 0.10944 	 ~...
   54 	    68 	 0.06715 	 0.11161 	 m..s
   58 	    69 	 0.06989 	 0.11169 	 m..s
   79 	    70 	 0.10010 	 0.11360 	 ~...
   57 	    71 	 0.06847 	 0.11414 	 m..s
   80 	    72 	 0.10159 	 0.11699 	 ~...
   65 	    73 	 0.08103 	 0.11978 	 m..s
   69 	    74 	 0.08553 	 0.11998 	 m..s
   76 	    75 	 0.09846 	 0.12127 	 ~...
   62 	    76 	 0.08017 	 0.12672 	 m..s
   63 	    77 	 0.08041 	 0.12945 	 m..s
   77 	    78 	 0.09851 	 0.13577 	 m..s
   64 	    79 	 0.08096 	 0.13608 	 m..s
   78 	    80 	 0.09995 	 0.14938 	 m..s
   71 	    81 	 0.08760 	 0.15032 	 m..s
   82 	    82 	 0.14203 	 0.16165 	 ~...
   83 	    83 	 0.21612 	 0.17167 	 m..s
   86 	    84 	 0.47158 	 0.36050 	 MISS
   84 	    85 	 0.46713 	 0.41053 	 m..s
   91 	    86 	 0.53023 	 0.48233 	 m..s
   89 	    87 	 0.52161 	 0.49651 	 ~...
   87 	    88 	 0.51652 	 0.51696 	 ~...
   88 	    89 	 0.51915 	 0.51740 	 ~...
   85 	    90 	 0.46894 	 0.52124 	 m..s
   93 	    91 	 0.53403 	 0.52681 	 ~...
   90 	    92 	 0.52945 	 0.54980 	 ~...
   96 	    93 	 0.55108 	 0.55278 	 ~...
   95 	    94 	 0.54934 	 0.55479 	 ~...
   98 	    95 	 0.55194 	 0.56051 	 ~...
   99 	    96 	 0.56481 	 0.57063 	 ~...
  100 	    97 	 0.56746 	 0.57114 	 ~...
   97 	    98 	 0.55161 	 0.57341 	 ~...
   94 	    99 	 0.53912 	 0.58044 	 m..s
   92 	   100 	 0.53165 	 0.58131 	 m..s
  109 	   101 	 0.62094 	 0.59711 	 ~...
  107 	   102 	 0.61757 	 0.60053 	 ~...
  117 	   103 	 0.63468 	 0.60150 	 m..s
  101 	   104 	 0.60270 	 0.60247 	 ~...
  112 	   105 	 0.63024 	 0.60429 	 ~...
  111 	   106 	 0.62739 	 0.60458 	 ~...
  106 	   107 	 0.61739 	 0.60641 	 ~...
  104 	   108 	 0.60637 	 0.60871 	 ~...
  105 	   109 	 0.61690 	 0.61150 	 ~...
  120 	   110 	 0.64960 	 0.61199 	 m..s
  110 	   111 	 0.62355 	 0.61227 	 ~...
  113 	   112 	 0.63043 	 0.61739 	 ~...
  115 	   113 	 0.63315 	 0.62056 	 ~...
  103 	   114 	 0.60536 	 0.62217 	 ~...
  108 	   115 	 0.61758 	 0.62480 	 ~...
  119 	   116 	 0.63853 	 0.62835 	 ~...
  118 	   117 	 0.63485 	 0.63054 	 ~...
  102 	   118 	 0.60438 	 0.63174 	 ~...
  114 	   119 	 0.63194 	 0.63323 	 ~...
  116 	   120 	 0.63417 	 0.63332 	 ~...
==========================================
r_mrr = 0.9912985563278198
r2_mrr = 0.9823313355445862
spearmanr_mrr@5 = 0.6791432499885559
spearmanr_mrr@10 = 0.7765874266624451
spearmanr_mrr@50 = 0.9936417937278748
spearmanr_mrr@100 = 0.9958636164665222
spearmanr_mrr@All = 0.9960354566574097
==========================================
test time: 0.39
Done Testing dataset UMLS
total time taken: 193.53641653060913
training time taken: 187.9418659210205
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9913)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9823)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.6791)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.7766)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9936)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9959)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9960)}}, 'test_loss': {'TransE': {'UMLS': 0.9051130927109625}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 8999291551734174
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [675, 1190, 754, 547, 1124, 1044, 865, 872, 435, 22, 930, 1154, 538, 829, 187, 904, 31, 264, 137, 798, 57, 1199, 308, 14, 657, 239, 635, 1148, 464, 407, 3, 34, 608, 1193, 874, 286, 81, 452, 1084, 108, 1092, 684, 1030, 498, 380, 195, 790, 1058, 431, 58, 739, 492, 460, 230, 429, 32, 152, 356, 679, 724, 1150, 189, 994, 976, 662, 83, 554, 301, 839, 766, 1013, 225, 394, 869, 200, 854, 203, 1028, 1158, 1036, 475, 402, 622, 915, 215, 636, 700, 740, 1063, 691, 950, 964, 54, 275, 841, 836, 53, 176, 655, 282, 1061, 514, 1115, 197, 614, 974, 512, 738, 806, 773, 353, 1011, 405, 449, 1054, 870, 999, 126, 925, 1128, 887]
valid_ids (0): []
train_ids (1094): [287, 646, 261, 150, 1007, 96, 260, 975, 418, 1138, 690, 21, 558, 420, 314, 807, 556, 256, 1187, 24, 1067, 342, 666, 626, 39, 155, 576, 1031, 348, 1096, 470, 1163, 408, 191, 501, 320, 664, 891, 143, 1095, 699, 654, 693, 885, 535, 843, 159, 338, 30, 871, 568, 75, 811, 1198, 183, 1145, 471, 289, 127, 827, 447, 933, 998, 712, 210, 713, 673, 91, 945, 584, 45, 844, 572, 349, 719, 727, 764, 440, 528, 645, 1039, 139, 786, 436, 333, 559, 344, 583, 232, 414, 1023, 606, 172, 178, 952, 332, 1125, 551, 1099, 109, 746, 495, 1186, 1055, 1008, 480, 1175, 965, 1118, 1104, 561, 658, 1110, 876, 650, 787, 609, 883, 617, 224, 94, 20, 161, 1083, 817, 51, 723, 41, 847, 595, 43, 1172, 849, 92, 520, 866, 973, 598, 451, 273, 294, 784, 346, 979, 400, 422, 89, 620, 7, 732, 1079, 428, 624, 1053, 672, 879, 237, 124, 831, 388, 1000, 526, 749, 771, 1068, 1091, 252, 1041, 509, 1171, 506, 316, 565, 6, 816, 863, 992, 1214, 988, 95, 823, 206, 969, 483, 669, 819, 758, 902, 317, 369, 184, 539, 567, 981, 467, 511, 634, 1042, 959, 395, 1144, 809, 789, 780, 36, 623, 505, 726, 777, 86, 1184, 540, 1161, 355, 234, 1093, 413, 752, 1022, 110, 1126, 404, 966, 111, 423, 165, 741, 985, 1064, 419, 1006, 121, 28, 493, 962, 141, 382, 1166, 337, 949, 469, 778, 1, 456, 227, 97, 284, 1089, 1177, 698, 579, 199, 890, 995, 345, 951, 2, 160, 446, 209, 37, 74, 882, 11, 603, 785, 115, 715, 755, 298, 1017, 151, 534, 769, 707, 38, 250, 877, 211, 1035, 360, 430, 262, 607, 229, 441, 895, 1155, 990, 1162, 641, 631, 987, 834, 1076, 508, 359, 205, 958, 272, 399, 795, 295, 1066, 833, 848, 69, 398, 1108, 1117, 582, 709, 309, 1057, 873, 1038, 1159, 352, 15, 926, 1212, 1050, 590, 403, 26, 1120, 642, 67, 424, 437, 174, 135, 737, 112, 198, 372, 922, 481, 310, 941, 1073, 996, 462, 543, 1195, 611, 425, 899, 1121, 47, 1014, 213, 307, 146, 1209, 868, 957, 894, 814, 136, 326, 1116, 934, 82, 689, 168, 410, 65, 961, 524, 1183, 1101, 571, 772, 628, 937, 763, 46, 19, 1181, 397, 148, 648, 49, 181, 156, 238, 490, 193, 68, 1080, 782, 253, 731, 909, 363, 536, 72, 936, 293, 1143, 842, 970, 574, 861, 730, 542, 940, 171, 835, 248, 103, 1088, 244, 531, 850, 742, 1133, 494, 670, 1129, 663, 61, 1179, 668, 466, 602, 682, 312, 240, 828, 101, 963, 276, 12, 661, 625, 1109, 1137, 1078, 529, 247, 792, 305, 245, 725, 781, 1025, 718, 274, 1009, 1146, 1056, 120, 443, 545, 881, 503, 1202, 302, 1003, 1060, 893, 900, 604, 99, 1152, 593, 564, 13, 533, 1130, 640, 1071, 299, 802, 599, 521, 325, 824, 60, 947, 916, 800, 736, 406, 55, 913, 853, 695, 118, 569, 911, 1192, 889, 163, 759, 570, 219, 341, 370, 711, 799, 804, 627, 105, 175, 458, 285, 830, 903, 504, 1142, 686, 600, 455, 217, 257, 993, 812, 1047, 167, 519, 960, 288, 463, 884, 208, 1107, 1139, 376, 207, 591, 594, 216, 703, 179, 476, 619, 415, 927, 427, 246, 465, 1059, 350, 910, 222, 361, 201, 618, 653, 1100, 489, 1168, 632, 194, 867, 813, 1164, 1156, 914, 665, 1103, 1037, 845, 704, 837, 750, 767, 1111, 1019, 235, 1134, 401, 4, 17, 647, 532, 552, 177, 459, 548, 362, 808, 140, 880, 76, 263, 324, 319, 1106, 1094, 696, 1204, 633, 474, 496, 444, 296, 375, 649, 279, 897, 1102, 577, 5, 856, 613, 530, 450, 851, 1208, 129, 596, 541, 616, 892, 838, 581, 846, 630, 610, 1051, 687, 1026, 760, 1140, 585, 1176, 939, 328, 776, 468, 281, 986, 339, 18, 643, 484, 365, 327, 29, 720, 1211, 182, 1112, 1002, 85, 1153, 79, 1151, 421, 70, 580, 122, 852, 233, 434, 1081, 921, 439, 390, 1165, 605, 144, 637, 416, 783, 445, 557, 347, 271, 292, 1147, 87, 477, 1205, 917, 343, 212, 329, 685, 300, 131, 1034, 659, 832, 1070, 1075, 932, 566, 223, 396, 1122, 549, 1210, 793, 803, 676, 1123, 857, 1131, 1033, 442, 523, 1191, 923, 639, 157, 983, 16, 1136, 678, 461, 384, 331, 1090, 259, 721, 323, 123, 747, 714, 1197, 912, 929, 745, 956, 1105, 88, 93, 133, 204, 1024, 1200, 128, 389, 214, 66, 753, 202, 864, 818, 955, 1020, 688, 1004, 190, 946, 774, 334, 928, 537, 770, 472, 729, 280, 9, 107, 656, 48, 1213, 290, 1032, 638, 255, 1049, 322, 943, 791, 671, 562, 392, 515, 63, 516, 1169, 134, 935, 756, 44, 218, 378, 249, 706, 354, 762, 651, 1097, 417, 1207, 391, 454, 313, 100, 251, 908, 196, 297, 265, 1074, 267, 587, 1119, 1157, 801, 1132, 1043, 710, 1021, 954, 652, 73, 254, 1065, 166, 944, 231, 681, 1189, 8, 491, 705, 330, 905, 386, 387, 486, 448, 25, 589, 433, 924, 701, 728, 886, 318, 862, 517, 1082, 907, 488, 980, 106, 513, 743, 525, 735, 59, 164, 80, 42, 982, 487, 967, 660, 482, 518, 919, 186, 1077, 578, 192, 810, 270, 775, 716, 479, 1160, 500, 351, 826, 102, 1040, 40, 1052, 972, 236, 268, 381, 147, 1206, 154, 119, 371, 117, 383, 291, 888, 145, 779, 278, 366, 563, 1027, 615, 825, 522, 1029, 377, 340, 357, 629, 304, 717, 412, 898, 52, 989, 241, 373, 1141, 258, 303, 510, 185, 978, 677, 597, 125, 502, 393, 71, 822, 1086, 1098, 1185, 796, 113, 269, 1196, 226, 560, 918, 667, 306, 188, 336, 1001, 586, 840, 901, 149, 942, 553, 1046, 1174, 173, 84, 612, 457, 78, 860, 744, 1173, 1085, 507, 0, 478, 1180, 920, 138, 385, 761, 374, 815, 243, 748, 132, 158, 768, 485, 1182, 1016, 77, 575, 1005, 90, 697, 1113, 1018, 931, 315, 311, 473, 1170, 453, 797, 266, 938, 1203, 1135, 708, 991, 734, 426, 644, 98, 1048, 592, 1010, 875, 694, 27, 180, 1045, 1072, 497, 116, 321, 1167, 1194, 722, 805, 953, 153, 283, 114, 1087, 1188, 335, 896, 997, 499, 573, 358, 765, 242, 1114, 794, 1012, 555, 821, 601, 368, 438, 64, 977, 33, 62, 169, 674, 757, 788, 702, 1201, 550, 142, 948, 680, 878, 968, 859, 971, 432, 544, 10, 104, 56, 1127, 221, 546, 1178, 820, 277, 1062, 364, 855, 1069, 621, 1149, 733, 228, 588, 858, 35, 220, 50, 692, 984, 1015, 379, 367, 409, 751, 162, 683, 906, 130, 170, 23, 411, 527]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8248558604546647
the save name prefix for this run is:  chkpt-ID_8248558604546647_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 780
rank avg (pred): 0.462 +- 0.004
mrr vals (pred, true): 0.016, 0.090
batch losses (mrrl, rdl): 0.0, 0.0007145311

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 428
rank avg (pred): 0.349 +- 0.231
mrr vals (pred, true): 0.222, 0.037
batch losses (mrrl, rdl): 0.0, 0.0001157147

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 112
rank avg (pred): 0.311 +- 0.234
mrr vals (pred, true): 0.301, 0.101
batch losses (mrrl, rdl): 0.0, 0.0001158742

Epoch over!
epoch time: 12.025

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1033
rank avg (pred): 0.331 +- 0.248
mrr vals (pred, true): 0.300, 0.038
batch losses (mrrl, rdl): 0.0, 0.000140919

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 105
rank avg (pred): 0.317 +- 0.236
mrr vals (pred, true): 0.295, 0.091
batch losses (mrrl, rdl): 0.0, 0.0001300901

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 756
rank avg (pred): 0.327 +- 0.244
mrr vals (pred, true): 0.295, 0.091
batch losses (mrrl, rdl): 0.0, 0.000106174

Epoch over!
epoch time: 11.883

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 907
rank avg (pred): 0.110 +- 0.084
mrr vals (pred, true): 0.342, 0.331
batch losses (mrrl, rdl): 0.0, 6.77622e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 954
rank avg (pred): 0.382 +- 0.264
mrr vals (pred, true): 0.254, 0.042
batch losses (mrrl, rdl): 0.0, 2.39464e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 86
rank avg (pred): 0.340 +- 0.252
mrr vals (pred, true): 0.290, 0.113
batch losses (mrrl, rdl): 0.0, 0.0002099415

Epoch over!
epoch time: 11.782

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 134
rank avg (pred): 0.327 +- 0.246
mrr vals (pred, true): 0.298, 0.129
batch losses (mrrl, rdl): 0.0, 0.0003138806

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 226
rank avg (pred): 0.334 +- 0.246
mrr vals (pred, true): 0.288, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001126669

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 220
rank avg (pred): 0.327 +- 0.249
mrr vals (pred, true): 0.308, 0.037
batch losses (mrrl, rdl): 0.0, 0.0001761978

Epoch over!
epoch time: 11.983

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 886
rank avg (pred): 0.332 +- 0.256
mrr vals (pred, true): 0.314, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001230883

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 361
rank avg (pred): 0.299 +- 0.238
mrr vals (pred, true): 0.331, 0.114
batch losses (mrrl, rdl): 0.0, 7.60523e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 400
rank avg (pred): 0.328 +- 0.250
mrr vals (pred, true): 0.308, 0.119
batch losses (mrrl, rdl): 0.0, 0.0002785924

Epoch over!
epoch time: 11.902

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 979
rank avg (pred): 0.031 +- 0.026
mrr vals (pred, true): 0.465, 0.625
batch losses (mrrl, rdl): 0.2545154691, 1.6856e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1214
rank avg (pred): 0.395 +- 0.121
mrr vals (pred, true): 0.058, 0.038
batch losses (mrrl, rdl): 0.0006183538, 9.29696e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 325
rank avg (pred): 0.358 +- 0.149
mrr vals (pred, true): 0.085, 0.098
batch losses (mrrl, rdl): 0.0119526498, 0.0002112739

Epoch over!
epoch time: 12.103

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 932
rank avg (pred): 0.393 +- 0.099
mrr vals (pred, true): 0.042, 0.090
batch losses (mrrl, rdl): 0.0006292507, 0.0002627271

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 422
rank avg (pred): 0.340 +- 0.148
mrr vals (pred, true): 0.090, 0.038
batch losses (mrrl, rdl): 0.0156800561, 0.0002508655

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 722
rank avg (pred): 0.370 +- 0.095
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 8.5753e-06, 0.0001595671

Epoch over!
epoch time: 12.07

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 602
rank avg (pred): 0.374 +- 0.085
mrr vals (pred, true): 0.043, 0.040
batch losses (mrrl, rdl): 0.0005262583, 0.0001603539

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 784
rank avg (pred): 0.366 +- 0.106
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 7.77635e-05, 0.0001867697

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 80
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.630, 0.615
batch losses (mrrl, rdl): 0.0022951742, 4.0477e-06

Epoch over!
epoch time: 12.137

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 639
rank avg (pred): 0.377 +- 0.062
mrr vals (pred, true): 0.032, 0.036
batch losses (mrrl, rdl): 0.0032229004, 0.0001170454

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 345
rank avg (pred): 0.323 +- 0.137
mrr vals (pred, true): 0.092, 0.102
batch losses (mrrl, rdl): 0.000929891, 9.69097e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 515
rank avg (pred): 0.284 +- 0.154
mrr vals (pred, true): 0.139, 0.121
batch losses (mrrl, rdl): 0.0034273565, 4.36852e-05

Epoch over!
epoch time: 12.029

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 445
rank avg (pred): 0.354 +- 0.110
mrr vals (pred, true): 0.056, 0.041
batch losses (mrrl, rdl): 0.0003400829, 0.0001812384

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 257
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.557, 0.624
batch losses (mrrl, rdl): 0.0443692096, 2.0074e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 794
rank avg (pred): 0.348 +- 0.092
mrr vals (pred, true): 0.053, 0.039
batch losses (mrrl, rdl): 8.35435e-05, 0.0002449664

Epoch over!
epoch time: 12.275

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 540
rank avg (pred): 0.321 +- 0.118
mrr vals (pred, true): 0.076, 0.067
batch losses (mrrl, rdl): 0.0066900863, 6.97479e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1165
rank avg (pred): 0.353 +- 0.076
mrr vals (pred, true): 0.043, 0.038
batch losses (mrrl, rdl): 0.0005239642, 9.90433e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 847
rank avg (pred): 0.348 +- 0.085
mrr vals (pred, true): 0.050, 0.087
batch losses (mrrl, rdl): 2.243e-07, 0.0001119

Epoch over!
epoch time: 12.393

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 399
rank avg (pred): 0.333 +- 0.105
mrr vals (pred, true): 0.061, 0.112
batch losses (mrrl, rdl): 0.0255654957, 0.0001790993

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 443
rank avg (pred): 0.310 +- 0.122
mrr vals (pred, true): 0.079, 0.044
batch losses (mrrl, rdl): 0.0084981583, 0.0003891878

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 461
rank avg (pred): 0.299 +- 0.129
mrr vals (pred, true): 0.094, 0.042
batch losses (mrrl, rdl): 0.0194363035, 0.0004230527

Epoch over!
epoch time: 12.3

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 139
rank avg (pred): 0.325 +- 0.103
mrr vals (pred, true): 0.066, 0.090
batch losses (mrrl, rdl): 0.0026650613, 8.66781e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 835
rank avg (pred): 0.021 +- 0.018
mrr vals (pred, true): 0.504, 0.576
batch losses (mrrl, rdl): 0.0528195724, 7.858e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 630
rank avg (pred): 0.354 +- 0.058
mrr vals (pred, true): 0.036, 0.032
batch losses (mrrl, rdl): 0.0018995714, 0.0002647787

Epoch over!
epoch time: 12.193

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 103
rank avg (pred): 0.336 +- 0.081
mrr vals (pred, true): 0.054, 0.100
batch losses (mrrl, rdl): 0.0001803617, 0.0001896391

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 919
rank avg (pred): 0.348 +- 0.065
mrr vals (pred, true): 0.043, 0.086
batch losses (mrrl, rdl): 0.0004476666, 0.0001113174

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 487
rank avg (pred): 0.385 +- 0.141
mrr vals (pred, true): 0.066, 0.082
batch losses (mrrl, rdl): 0.0025881, 9.28337e-05

Epoch over!
epoch time: 12.071

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 617
rank avg (pred): 0.568 +- 0.174
mrr vals (pred, true): 0.042, 0.040
batch losses (mrrl, rdl): 0.0005769702, 0.0004183119

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 370
rank avg (pred): 0.318 +- 0.102
mrr vals (pred, true): 0.070, 0.115
batch losses (mrrl, rdl): 0.0202438328, 0.0001008056

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 623
rank avg (pred): 0.494 +- 0.161
mrr vals (pred, true): 0.041, 0.040
batch losses (mrrl, rdl): 0.0008240074, 9.03343e-05

Epoch over!
epoch time: 12.068

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.343 +- 0.059
mrr vals (pred, true): 0.041, 0.042

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04253 	 0.03491 	 ~...
   11 	     1 	 0.04760 	 0.03530 	 ~...
    0 	     2 	 0.03941 	 0.03569 	 ~...
   32 	     3 	 0.05195 	 0.03595 	 ~...
   84 	     4 	 0.09865 	 0.03611 	 m..s
    6 	     5 	 0.04428 	 0.03736 	 ~...
    3 	     6 	 0.04248 	 0.03754 	 ~...
    8 	     7 	 0.04496 	 0.03754 	 ~...
   56 	     8 	 0.07261 	 0.03795 	 m..s
   49 	     9 	 0.06759 	 0.03803 	 ~...
   69 	    10 	 0.07938 	 0.03806 	 m..s
   20 	    11 	 0.05038 	 0.03808 	 ~...
   64 	    12 	 0.07673 	 0.03809 	 m..s
   22 	    13 	 0.05040 	 0.03818 	 ~...
   23 	    14 	 0.05043 	 0.03836 	 ~...
   12 	    15 	 0.04762 	 0.03845 	 ~...
   55 	    16 	 0.07254 	 0.03845 	 m..s
   26 	    17 	 0.05118 	 0.03867 	 ~...
   58 	    18 	 0.07402 	 0.03888 	 m..s
   15 	    19 	 0.04961 	 0.03910 	 ~...
   57 	    20 	 0.07319 	 0.03924 	 m..s
   40 	    21 	 0.05823 	 0.03933 	 ~...
   42 	    22 	 0.05948 	 0.03950 	 ~...
   74 	    23 	 0.08443 	 0.03954 	 m..s
   67 	    24 	 0.07755 	 0.03962 	 m..s
    7 	    25 	 0.04474 	 0.04000 	 ~...
   43 	    26 	 0.06128 	 0.04041 	 ~...
   46 	    27 	 0.06673 	 0.04058 	 ~...
   45 	    28 	 0.06648 	 0.04085 	 ~...
   83 	    29 	 0.09837 	 0.04101 	 m..s
   25 	    30 	 0.05114 	 0.04112 	 ~...
   66 	    31 	 0.07689 	 0.04113 	 m..s
    1 	    32 	 0.04085 	 0.04116 	 ~...
   70 	    33 	 0.07955 	 0.04116 	 m..s
    9 	    34 	 0.04535 	 0.04128 	 ~...
   53 	    35 	 0.07232 	 0.04136 	 m..s
   38 	    36 	 0.05654 	 0.04142 	 ~...
    5 	    37 	 0.04344 	 0.04156 	 ~...
   16 	    38 	 0.04967 	 0.04172 	 ~...
   10 	    39 	 0.04588 	 0.04177 	 ~...
   24 	    40 	 0.05072 	 0.04182 	 ~...
   73 	    41 	 0.08363 	 0.04199 	 m..s
   13 	    42 	 0.04765 	 0.04210 	 ~...
    2 	    43 	 0.04101 	 0.04233 	 ~...
   44 	    44 	 0.06160 	 0.04239 	 ~...
   65 	    45 	 0.07684 	 0.04260 	 m..s
   82 	    46 	 0.09625 	 0.04262 	 m..s
   59 	    47 	 0.07419 	 0.04281 	 m..s
   14 	    48 	 0.04936 	 0.04284 	 ~...
   36 	    49 	 0.05503 	 0.04302 	 ~...
   81 	    50 	 0.08823 	 0.04329 	 m..s
   75 	    51 	 0.08539 	 0.04335 	 m..s
   31 	    52 	 0.05183 	 0.04349 	 ~...
   33 	    53 	 0.05270 	 0.04399 	 ~...
   27 	    54 	 0.05120 	 0.04554 	 ~...
   51 	    55 	 0.06870 	 0.04593 	 ~...
   52 	    56 	 0.07128 	 0.07324 	 ~...
   35 	    57 	 0.05380 	 0.07561 	 ~...
   37 	    58 	 0.05531 	 0.07655 	 ~...
   19 	    59 	 0.05010 	 0.07881 	 ~...
   30 	    60 	 0.05167 	 0.08011 	 ~...
   54 	    61 	 0.07248 	 0.08036 	 ~...
   29 	    62 	 0.05145 	 0.08234 	 m..s
   41 	    63 	 0.05883 	 0.08426 	 ~...
   48 	    64 	 0.06750 	 0.08580 	 ~...
   21 	    65 	 0.05038 	 0.08812 	 m..s
   28 	    66 	 0.05143 	 0.08873 	 m..s
   18 	    67 	 0.04997 	 0.08889 	 m..s
   39 	    68 	 0.05672 	 0.08899 	 m..s
   34 	    69 	 0.05280 	 0.09135 	 m..s
   62 	    70 	 0.07557 	 0.09271 	 ~...
   17 	    71 	 0.04977 	 0.09496 	 m..s
   47 	    72 	 0.06730 	 0.09552 	 ~...
   68 	    73 	 0.07842 	 0.10345 	 ~...
   71 	    74 	 0.07967 	 0.10944 	 ~...
   50 	    75 	 0.06796 	 0.11464 	 m..s
   76 	    76 	 0.08549 	 0.11629 	 m..s
   60 	    77 	 0.07476 	 0.11646 	 m..s
   85 	    78 	 0.10391 	 0.12421 	 ~...
   72 	    79 	 0.08177 	 0.13054 	 m..s
   79 	    80 	 0.08636 	 0.13087 	 m..s
   63 	    81 	 0.07601 	 0.13367 	 m..s
   80 	    82 	 0.08641 	 0.13797 	 m..s
   88 	    83 	 0.19096 	 0.14348 	 m..s
   61 	    84 	 0.07498 	 0.14510 	 m..s
   89 	    85 	 0.19422 	 0.14620 	 m..s
   78 	    86 	 0.08633 	 0.14666 	 m..s
   77 	    87 	 0.08605 	 0.14833 	 m..s
   87 	    88 	 0.17884 	 0.15235 	 ~...
   86 	    89 	 0.17634 	 0.16461 	 ~...
   91 	    90 	 0.29240 	 0.30550 	 ~...
   90 	    91 	 0.28789 	 0.33468 	 m..s
   94 	    92 	 0.51523 	 0.42415 	 m..s
   95 	    93 	 0.51544 	 0.43062 	 m..s
   92 	    94 	 0.51078 	 0.43259 	 m..s
   93 	    95 	 0.51411 	 0.43552 	 m..s
   96 	    96 	 0.52475 	 0.48233 	 m..s
  100 	    97 	 0.54313 	 0.50669 	 m..s
   98 	    98 	 0.52972 	 0.51077 	 ~...
   99 	    99 	 0.53666 	 0.52695 	 ~...
  108 	   100 	 0.57074 	 0.53535 	 m..s
   97 	   101 	 0.52929 	 0.53731 	 ~...
  102 	   102 	 0.56126 	 0.56007 	 ~...
  103 	   103 	 0.56266 	 0.56057 	 ~...
  105 	   104 	 0.56682 	 0.56964 	 ~...
  101 	   105 	 0.56121 	 0.57062 	 ~...
  104 	   106 	 0.56648 	 0.57095 	 ~...
  106 	   107 	 0.57014 	 0.57161 	 ~...
  107 	   108 	 0.57045 	 0.57514 	 ~...
  111 	   109 	 0.62669 	 0.58102 	 m..s
  115 	   110 	 0.63319 	 0.60541 	 ~...
  113 	   111 	 0.63010 	 0.60653 	 ~...
  109 	   112 	 0.62234 	 0.60723 	 ~...
  117 	   113 	 0.63986 	 0.60795 	 m..s
  110 	   114 	 0.62352 	 0.60969 	 ~...
  116 	   115 	 0.63367 	 0.61150 	 ~...
  119 	   116 	 0.64622 	 0.61521 	 m..s
  118 	   117 	 0.64231 	 0.61906 	 ~...
  112 	   118 	 0.63003 	 0.62322 	 ~...
  114 	   119 	 0.63136 	 0.62441 	 ~...
  120 	   120 	 0.65336 	 0.62676 	 ~...
==========================================
r_mrr = 0.9896402955055237
r2_mrr = 0.9762306213378906
spearmanr_mrr@5 = 0.957496166229248
spearmanr_mrr@10 = 0.9659688472747803
spearmanr_mrr@50 = 0.9916272759437561
spearmanr_mrr@100 = 0.993844747543335
spearmanr_mrr@All = 0.9943754076957703
==========================================
test time: 0.498
Done Testing dataset UMLS
total time taken: 187.35678362846375
training time taken: 181.78344702720642
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9896)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9762)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9575)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9660)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9916)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9938)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9944)}}, 'test_loss': {'TransE': {'UMLS': 1.0448293974204717}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7060350119780399
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [5, 805, 286, 1143, 435, 934, 1022, 133, 95, 58, 880, 1051, 504, 154, 553, 563, 717, 115, 110, 634, 65, 73, 198, 1196, 673, 712, 1004, 277, 584, 87, 278, 774, 854, 1016, 543, 411, 1060, 691, 929, 643, 1171, 246, 454, 758, 671, 452, 1081, 931, 9, 517, 849, 713, 838, 353, 1009, 937, 121, 153, 464, 692, 965, 417, 61, 847, 1191, 718, 275, 397, 1203, 247, 702, 366, 619, 788, 190, 684, 1121, 651, 344, 412, 979, 1189, 626, 163, 430, 70, 336, 525, 611, 865, 193, 169, 885, 1063, 1109, 54, 1002, 75, 436, 943, 891, 1206, 839, 3, 1182, 125, 372, 935, 475, 941, 697, 94, 162, 157, 349, 215, 164, 745, 382, 920, 882]
valid_ids (0): []
train_ids (1094): [704, 977, 1027, 195, 593, 1011, 723, 374, 1153, 1170, 476, 1043, 203, 676, 829, 1197, 602, 24, 342, 566, 730, 981, 22, 565, 354, 1093, 279, 334, 1086, 1126, 680, 338, 884, 40, 251, 777, 12, 1024, 42, 472, 996, 401, 227, 1095, 570, 390, 487, 310, 952, 524, 714, 963, 883, 1032, 118, 288, 742, 479, 808, 313, 600, 1018, 922, 1067, 518, 1089, 1144, 638, 314, 721, 596, 1124, 26, 1213, 796, 761, 62, 188, 519, 261, 1173, 262, 1155, 1106, 33, 670, 273, 533, 243, 940, 999, 72, 498, 375, 878, 710, 819, 1005, 863, 373, 564, 751, 968, 509, 844, 358, 705, 347, 630, 370, 269, 659, 298, 421, 511, 155, 1042, 1167, 597, 821, 1154, 441, 1122, 181, 287, 816, 31, 822, 914, 581, 424, 232, 1158, 495, 1147, 8, 689, 780, 257, 1015, 781, 749, 936, 677, 368, 44, 962, 924, 141, 926, 221, 96, 648, 1090, 457, 541, 100, 520, 143, 384, 842, 471, 0, 523, 453, 1113, 1099, 1142, 136, 1070, 901, 84, 127, 728, 848, 202, 802, 423, 501, 398, 477, 1161, 1199, 268, 657, 151, 776, 506, 944, 337, 798, 522, 173, 515, 365, 402, 210, 348, 381, 199, 99, 325, 285, 507, 655, 1119, 894, 703, 1098, 1052, 947, 988, 1148, 893, 661, 679, 1149, 399, 625, 39, 473, 272, 1177, 1118, 306, 624, 224, 492, 152, 116, 632, 732, 574, 289, 172, 910, 1207, 209, 254, 429, 332, 711, 793, 1053, 991, 956, 1120, 606, 330, 682, 818, 779, 434, 1047, 258, 367, 1136, 888, 311, 204, 1078, 836, 55, 736, 200, 415, 183, 890, 1160, 244, 201, 1056, 989, 1187, 1, 174, 845, 177, 823, 803, 315, 985, 864, 902, 1037, 752, 1025, 862, 1039, 953, 343, 361, 1192, 571, 57, 416, 1146, 391, 160, 456, 165, 63, 535, 403, 521, 800, 396, 117, 276, 431, 407, 496, 628, 1048, 1163, 499, 585, 556, 539, 250, 1017, 66, 978, 139, 967, 30, 304, 29, 771, 966, 120, 971, 785, 753, 147, 1133, 983, 1097, 1083, 420, 91, 846, 355, 872, 1080, 861, 239, 1190, 265, 877, 644, 948, 292, 783, 598, 159, 178, 345, 629, 1059, 1102, 681, 744, 414, 589, 573, 876, 1034, 986, 896, 184, 1050, 1058, 807, 175, 167, 132, 329, 1145, 674, 488, 824, 294, 898, 83, 10, 911, 995, 50, 378, 1201, 1175, 171, 545, 256, 303, 46, 129, 510, 913, 961, 757, 1210, 897, 90, 992, 908, 56, 623, 119, 892, 792, 128, 1033, 765, 59, 568, 791, 1020, 25, 49, 124, 1044, 969, 383, 960, 413, 1214, 1074, 470, 86, 512, 724, 380, 97, 137, 98, 357, 806, 104, 312, 1031, 1007, 690, 52, 35, 1092, 907, 32, 820, 85, 516, 706, 255, 814, 871, 906, 1066, 938, 1021, 609, 588, 219, 812, 1195, 1029, 1019, 895, 997, 536, 642, 1165, 494, 222, 558, 1030, 784, 1172, 1091, 1069, 662, 592, 738, 833, 333, 69, 653, 260, 899, 238, 449, 616, 231, 1138, 851, 850, 404, 701, 1073, 1100, 4, 928, 595, 346, 664, 466, 603, 1188, 933, 1115, 437, 1134, 675, 528, 392, 426, 686, 142, 249, 958, 750, 950, 841, 830, 582, 694, 319, 668, 107, 480, 252, 537, 317, 647, 503, 340, 130, 811, 15, 957, 557, 1023, 81, 1176, 916, 295, 734, 23, 170, 264, 284, 1151, 185, 725, 612, 410, 508, 881, 707, 970, 562, 405, 708, 267, 280, 546, 551, 731, 442, 263, 804, 1041, 205, 78, 932, 485, 283, 461, 554, 18, 608, 1127, 1162, 666, 775, 1064, 234, 719, 356, 422, 220, 229, 406, 605, 591, 866, 1204, 586, 236, 610, 1085, 909, 534, 235, 741, 318, 613, 60, 856, 870, 645, 633, 481, 601, 538, 951, 74, 326, 631, 1159, 835, 327, 700, 460, 1075, 514, 206, 650, 77, 458, 868, 179, 857, 927, 555, 1026, 832, 307, 561, 212, 1084, 2, 76, 770, 134, 1082, 196, 377, 1096, 1036, 646, 641, 813, 620, 1065, 530, 1186, 14, 359, 639, 1168, 576, 754, 635, 148, 1200, 855, 575, 604, 1179, 843, 1012, 735, 579, 443, 1152, 1107, 580, 695, 858, 917, 739, 131, 1157, 722, 385, 1088, 305, 772, 1208, 875, 1185, 51, 213, 683, 696, 955, 905, 799, 187, 240, 113, 439, 1003, 446, 297, 879, 89, 308, 678, 293, 976, 1150, 331, 994, 6, 138, 740, 726, 1045, 590, 889, 316, 320, 1116, 1049, 400, 621, 699, 497, 1117, 1001, 21, 194, 489, 778, 904, 688, 1132, 409, 502, 20, 161, 433, 617, 105, 270, 149, 1014, 1141, 242, 321, 787, 903, 782, 371, 408, 544, 1178, 853, 873, 1181, 919, 448, 290, 7, 972, 607, 687, 451, 109, 376, 114, 225, 11, 1040, 930, 53, 786, 918, 186, 826, 746, 82, 150, 166, 716, 693, 615, 500, 1166, 795, 360, 432, 1130, 767, 428, 867, 445, 1035, 363, 925, 1104, 37, 387, 1131, 1076, 28, 68, 351, 145, 36, 815, 984, 685, 1174, 302, 447, 228, 663, 658, 324, 300, 17, 218, 825, 1125, 1055, 483, 1209, 747, 323, 762, 474, 140, 1008, 38, 831, 395, 923, 103, 946, 810, 80, 743, 531, 362, 168, 259, 230, 146, 637, 484, 886, 817, 764, 773, 869, 945, 13, 982, 1183, 720, 733, 266, 660, 939, 462, 189, 282, 364, 1028, 766, 1110, 418, 809, 559, 350, 135, 1068, 1071, 223, 440, 438, 1103, 990, 527, 64, 301, 614, 993, 101, 1180, 245, 964, 915, 1205, 1062, 794, 123, 191, 1013, 912, 1094, 1038, 790, 532, 542, 1000, 122, 217, 698, 1202, 1105, 1072, 328, 837, 1193, 1198, 102, 636, 1077, 709, 1061, 973, 106, 197, 47, 335, 214, 797, 1079, 840, 526, 71, 468, 126, 216, 748, 455, 552, 1114, 763, 388, 1156, 560, 207, 954, 550, 789, 393, 467, 656, 1135, 860, 425, 1054, 112, 975, 769, 649, 1046, 192, 801, 41, 1164, 1101, 834, 34, 1128, 88, 980, 921, 296, 942, 998, 465, 291, 322, 341, 180, 1010, 427, 727, 529, 493, 852, 271, 1123, 665, 622, 450, 486, 19, 233, 578, 309, 547, 572, 1006, 1129, 389, 92, 1140, 1194, 79, 1108, 729, 874, 419, 182, 491, 241, 93, 226, 672, 379, 828, 43, 654, 1137, 339, 859, 211, 482, 1184, 594, 490, 469, 652, 111, 759, 768, 760, 987, 577, 827, 176, 459, 549, 478, 394, 16, 756, 386, 281, 583, 1169, 1211, 248, 45, 156, 444, 108, 1057, 67, 887, 715, 587, 755, 48, 567, 144, 640, 737, 1112, 208, 1087, 959, 1139, 352, 569, 618, 253, 299, 974, 548, 599, 1212, 669, 369, 900, 667, 274, 540, 27, 463, 1111, 627, 949, 237, 513, 158, 505]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2593247042893800
the save name prefix for this run is:  chkpt-ID_2593247042893800_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1091
rank avg (pred): 0.468 +- 0.003
mrr vals (pred, true): 0.016, 0.123
batch losses (mrrl, rdl): 0.0, 0.0010365283

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 570
rank avg (pred): 0.460 +- 0.044
mrr vals (pred, true): 0.016, 0.036
batch losses (mrrl, rdl): 0.0, 5.89877e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 537
rank avg (pred): 0.328 +- 0.230
mrr vals (pred, true): 0.225, 0.062
batch losses (mrrl, rdl): 0.0, 5.1868e-06

Epoch over!
epoch time: 12.135

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 228
rank avg (pred): 0.321 +- 0.224
mrr vals (pred, true): 0.230, 0.038
batch losses (mrrl, rdl): 0.0, 0.0002812994

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 910
rank avg (pred): 0.116 +- 0.089
mrr vals (pred, true): 0.327, 0.272
batch losses (mrrl, rdl): 0.0, 2.03104e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 212
rank avg (pred): 0.310 +- 0.237
mrr vals (pred, true): 0.283, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001755429

Epoch over!
epoch time: 11.844

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 446
rank avg (pred): 0.323 +- 0.242
mrr vals (pred, true): 0.264, 0.035
batch losses (mrrl, rdl): 0.0, 0.0002177631

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 823
rank avg (pred): 0.018 +- 0.014
mrr vals (pred, true): 0.497, 0.542
batch losses (mrrl, rdl): 0.0, 5.4374e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 925
rank avg (pred): 0.331 +- 0.253
mrr vals (pred, true): 0.287, 0.076
batch losses (mrrl, rdl): 0.0, 2.9965e-05

Epoch over!
epoch time: 11.93

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1102
rank avg (pred): 0.321 +- 0.241
mrr vals (pred, true): 0.264, 0.142
batch losses (mrrl, rdl): 0.0, 0.0003055111

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 200
rank avg (pred): 0.379 +- 0.252
mrr vals (pred, true): 0.200, 0.039
batch losses (mrrl, rdl): 0.0, 2.63309e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 629
rank avg (pred): 0.397 +- 0.269
mrr vals (pred, true): 0.225, 0.045
batch losses (mrrl, rdl): 0.0, 6.7271e-06

Epoch over!
epoch time: 11.768

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 470
rank avg (pred): 0.330 +- 0.250
mrr vals (pred, true): 0.276, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001126327

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 223
rank avg (pred): 0.329 +- 0.252
mrr vals (pred, true): 0.285, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001594268

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 740
rank avg (pred): 0.013 +- 0.011
mrr vals (pred, true): 0.546, 0.433
batch losses (mrrl, rdl): 0.0, 3.75486e-05

Epoch over!
epoch time: 11.886

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 548
rank avg (pred): 0.263 +- 0.213
mrr vals (pred, true): 0.317, 0.127
batch losses (mrrl, rdl): 0.3600781858, 2.41337e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1148
rank avg (pred): 0.263 +- 0.170
mrr vals (pred, true): 0.165, 0.143
batch losses (mrrl, rdl): 0.0045590186, 4.783e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 642
rank avg (pred): 0.425 +- 0.088
mrr vals (pred, true): 0.035, 0.035
batch losses (mrrl, rdl): 0.002127571, 3.77146e-05

Epoch over!
epoch time: 12.162

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 174
rank avg (pred): 0.392 +- 0.109
mrr vals (pred, true): 0.055, 0.042
batch losses (mrrl, rdl): 0.0002063319, 8.53707e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1069
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.651, 0.605
batch losses (mrrl, rdl): 0.0205902494, 4.8531e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 84
rank avg (pred): 0.393 +- 0.102
mrr vals (pred, true): 0.054, 0.073
batch losses (mrrl, rdl): 0.0001800679, 0.0001849314

Epoch over!
epoch time: 12.227

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 300
rank avg (pred): 0.014 +- 0.011
mrr vals (pred, true): 0.497, 0.513
batch losses (mrrl, rdl): 0.0026046061, 7.7742e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 913
rank avg (pred): 0.115 +- 0.087
mrr vals (pred, true): 0.311, 0.330
batch losses (mrrl, rdl): 0.0037876903, 8.69595e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 763
rank avg (pred): 0.358 +- 0.114
mrr vals (pred, true): 0.066, 0.067
batch losses (mrrl, rdl): 0.0026882263, 5.95811e-05

Epoch over!
epoch time: 12.319

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 359
rank avg (pred): 0.336 +- 0.164
mrr vals (pred, true): 0.095, 0.129
batch losses (mrrl, rdl): 0.0115076285, 0.0002370079

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 556
rank avg (pred): 0.347 +- 0.164
mrr vals (pred, true): 0.089, 0.073
batch losses (mrrl, rdl): 0.0154319415, 3.67445e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1123
rank avg (pred): 0.341 +- 0.159
mrr vals (pred, true): 0.086, 0.041
batch losses (mrrl, rdl): 0.0128470864, 0.0002065743

Epoch over!
epoch time: 12.926

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 533
rank avg (pred): 0.319 +- 0.179
mrr vals (pred, true): 0.120, 0.121
batch losses (mrrl, rdl): 9.6134e-06, 7.1319e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 431
rank avg (pred): 0.308 +- 0.158
mrr vals (pred, true): 0.108, 0.041
batch losses (mrrl, rdl): 0.033508651, 0.0003353842

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 928
rank avg (pred): 0.398 +- 0.097
mrr vals (pred, true): 0.055, 0.092
batch losses (mrrl, rdl): 0.0002030917, 0.0003790448

Epoch over!
epoch time: 12.67

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 310
rank avg (pred): 0.012 +- 0.010
mrr vals (pred, true): 0.557, 0.576
batch losses (mrrl, rdl): 0.0036311152, 3.4913e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 476
rank avg (pred): 0.326 +- 0.153
mrr vals (pred, true): 0.087, 0.039
batch losses (mrrl, rdl): 0.0134059954, 0.0002761471

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 51
rank avg (pred): 0.015 +- 0.012
mrr vals (pred, true): 0.527, 0.506
batch losses (mrrl, rdl): 0.0042014997, 4.9745e-06

Epoch over!
epoch time: 12.901

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 116
rank avg (pred): 0.370 +- 0.124
mrr vals (pred, true): 0.068, 0.107
batch losses (mrrl, rdl): 0.0151658338, 0.0002986563

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1042
rank avg (pred): 0.332 +- 0.146
mrr vals (pred, true): 0.089, 0.040
batch losses (mrrl, rdl): 0.0154151227, 0.0002821654

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 924
rank avg (pred): 0.400 +- 0.083
mrr vals (pred, true): 0.045, 0.069
batch losses (mrrl, rdl): 0.0002147593, 0.000196898

Epoch over!
epoch time: 13.376

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 82
rank avg (pred): 0.393 +- 0.090
mrr vals (pred, true): 0.052, 0.086
batch losses (mrrl, rdl): 4.88153e-05, 0.0003065467

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 138
rank avg (pred): 0.408 +- 0.099
mrr vals (pred, true): 0.054, 0.080
batch losses (mrrl, rdl): 0.0002013331, 0.0003868679

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 777
rank avg (pred): 0.437 +- 0.101
mrr vals (pred, true): 0.046, 0.086
batch losses (mrrl, rdl): 0.0001257026, 0.0005357055

Epoch over!
epoch time: 12.832

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 260
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.575, 0.627
batch losses (mrrl, rdl): 0.0272129625, 2.0619e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 370
rank avg (pred): 0.345 +- 0.119
mrr vals (pred, true): 0.073, 0.115
batch losses (mrrl, rdl): 0.0177241154, 0.0001773402

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 722
rank avg (pred): 0.379 +- 0.081
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 9.3416e-06, 0.0001457882

Epoch over!
epoch time: 13.339

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 816
rank avg (pred): 0.032 +- 0.027
mrr vals (pred, true): 0.461, 0.372
batch losses (mrrl, rdl): 0.0789992511, 5.7614e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 432
rank avg (pred): 0.385 +- 0.108
mrr vals (pred, true): 0.060, 0.038
batch losses (mrrl, rdl): 0.0010440231, 0.0001264217

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 752
rank avg (pred): 0.026 +- 0.021
mrr vals (pred, true): 0.485, 0.511
batch losses (mrrl, rdl): 0.0069550676, 2.192e-06

Epoch over!
epoch time: 12.638

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.624, 0.601

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04956 	 0.03433 	 ~...
   23 	     1 	 0.05541 	 0.03551 	 ~...
   52 	     2 	 0.06242 	 0.03597 	 ~...
   48 	     3 	 0.06165 	 0.03601 	 ~...
   27 	     4 	 0.05597 	 0.03620 	 ~...
   30 	     5 	 0.05613 	 0.03621 	 ~...
   35 	     6 	 0.05758 	 0.03665 	 ~...
   67 	     7 	 0.07493 	 0.03668 	 m..s
   70 	     8 	 0.07785 	 0.03678 	 m..s
   11 	     9 	 0.04998 	 0.03691 	 ~...
   36 	    10 	 0.05785 	 0.03724 	 ~...
   89 	    11 	 0.09570 	 0.03757 	 m..s
   20 	    12 	 0.05496 	 0.03768 	 ~...
    5 	    13 	 0.04841 	 0.03769 	 ~...
   88 	    14 	 0.09495 	 0.03806 	 m..s
   84 	    15 	 0.08860 	 0.03811 	 m..s
   53 	    16 	 0.06251 	 0.03828 	 ~...
    4 	    17 	 0.04745 	 0.03843 	 ~...
    0 	    18 	 0.04444 	 0.03872 	 ~...
   69 	    19 	 0.07736 	 0.03897 	 m..s
   15 	    20 	 0.05313 	 0.03917 	 ~...
   80 	    21 	 0.08448 	 0.03931 	 m..s
   14 	    22 	 0.05237 	 0.03935 	 ~...
    8 	    23 	 0.04916 	 0.03938 	 ~...
   65 	    24 	 0.07421 	 0.03943 	 m..s
   37 	    25 	 0.05796 	 0.03964 	 ~...
    7 	    26 	 0.04845 	 0.03977 	 ~...
   56 	    27 	 0.06632 	 0.03992 	 ~...
    2 	    28 	 0.04563 	 0.04004 	 ~...
   75 	    29 	 0.08123 	 0.04008 	 m..s
   19 	    30 	 0.05473 	 0.04017 	 ~...
   47 	    31 	 0.06163 	 0.04019 	 ~...
   63 	    32 	 0.07177 	 0.04041 	 m..s
   13 	    33 	 0.05236 	 0.04044 	 ~...
   50 	    34 	 0.06176 	 0.04051 	 ~...
   44 	    35 	 0.05918 	 0.04058 	 ~...
   72 	    36 	 0.07965 	 0.04085 	 m..s
   95 	    37 	 0.10504 	 0.04091 	 m..s
   29 	    38 	 0.05608 	 0.04112 	 ~...
    3 	    39 	 0.04734 	 0.04116 	 ~...
    1 	    40 	 0.04458 	 0.04117 	 ~...
    6 	    41 	 0.04842 	 0.04128 	 ~...
   10 	    42 	 0.04997 	 0.04141 	 ~...
   16 	    43 	 0.05339 	 0.04163 	 ~...
   91 	    44 	 0.09691 	 0.04199 	 m..s
   97 	    45 	 0.11085 	 0.04262 	 m..s
   58 	    46 	 0.06763 	 0.04302 	 ~...
   43 	    47 	 0.05900 	 0.04347 	 ~...
   25 	    48 	 0.05584 	 0.04397 	 ~...
   94 	    49 	 0.10241 	 0.04433 	 m..s
   46 	    50 	 0.06050 	 0.04554 	 ~...
   24 	    51 	 0.05560 	 0.04595 	 ~...
   38 	    52 	 0.05799 	 0.04685 	 ~...
   60 	    53 	 0.06886 	 0.06337 	 ~...
   61 	    54 	 0.06969 	 0.06644 	 ~...
   45 	    55 	 0.05992 	 0.07298 	 ~...
   74 	    56 	 0.08063 	 0.07347 	 ~...
   33 	    57 	 0.05731 	 0.08156 	 ~...
   31 	    58 	 0.05664 	 0.08234 	 ~...
   17 	    59 	 0.05348 	 0.08354 	 m..s
   76 	    60 	 0.08157 	 0.08396 	 ~...
   22 	    61 	 0.05516 	 0.08467 	 ~...
   18 	    62 	 0.05436 	 0.08483 	 m..s
   26 	    63 	 0.05594 	 0.08625 	 m..s
   40 	    64 	 0.05819 	 0.08734 	 ~...
   51 	    65 	 0.06225 	 0.08850 	 ~...
   55 	    66 	 0.06411 	 0.08933 	 ~...
   49 	    67 	 0.06174 	 0.08968 	 ~...
   12 	    68 	 0.05144 	 0.09013 	 m..s
   42 	    69 	 0.05866 	 0.09090 	 m..s
   54 	    70 	 0.06263 	 0.09135 	 ~...
   41 	    71 	 0.05837 	 0.09247 	 m..s
   39 	    72 	 0.05814 	 0.09253 	 m..s
   28 	    73 	 0.05603 	 0.09304 	 m..s
   64 	    74 	 0.07250 	 0.09360 	 ~...
   59 	    75 	 0.06835 	 0.09477 	 ~...
   21 	    76 	 0.05508 	 0.09651 	 m..s
   32 	    77 	 0.05669 	 0.09661 	 m..s
   77 	    78 	 0.08262 	 0.09676 	 ~...
   62 	    79 	 0.07089 	 0.09677 	 ~...
   66 	    80 	 0.07478 	 0.09723 	 ~...
   34 	    81 	 0.05737 	 0.09944 	 m..s
   68 	    82 	 0.07579 	 0.09999 	 ~...
   57 	    83 	 0.06650 	 0.10049 	 m..s
   73 	    84 	 0.08011 	 0.10340 	 ~...
   82 	    85 	 0.08651 	 0.11071 	 ~...
   92 	    86 	 0.09806 	 0.11157 	 ~...
   83 	    87 	 0.08758 	 0.11198 	 ~...
   98 	    88 	 0.11319 	 0.11360 	 ~...
   81 	    89 	 0.08597 	 0.11491 	 ~...
   85 	    90 	 0.09127 	 0.11734 	 ~...
   79 	    91 	 0.08413 	 0.11767 	 m..s
   87 	    92 	 0.09354 	 0.11779 	 ~...
   86 	    93 	 0.09148 	 0.12606 	 m..s
   71 	    94 	 0.07916 	 0.12672 	 m..s
   90 	    95 	 0.09650 	 0.13180 	 m..s
   78 	    96 	 0.08331 	 0.13662 	 m..s
   93 	    97 	 0.09995 	 0.13797 	 m..s
   96 	    98 	 0.10551 	 0.13812 	 m..s
   99 	    99 	 0.12650 	 0.14944 	 ~...
  100 	   100 	 0.16071 	 0.21459 	 m..s
  103 	   101 	 0.50791 	 0.48233 	 ~...
  106 	   102 	 0.52739 	 0.49163 	 m..s
  101 	   103 	 0.50630 	 0.50860 	 ~...
  102 	   104 	 0.50673 	 0.51304 	 ~...
  105 	   105 	 0.51701 	 0.53731 	 ~...
  107 	   106 	 0.54042 	 0.54533 	 ~...
  113 	   107 	 0.55770 	 0.55479 	 ~...
  109 	   108 	 0.55353 	 0.56007 	 ~...
  114 	   109 	 0.55813 	 0.56316 	 ~...
  111 	   110 	 0.55659 	 0.56975 	 ~...
  108 	   111 	 0.54948 	 0.56998 	 ~...
  110 	   112 	 0.55615 	 0.57514 	 ~...
  104 	   113 	 0.51567 	 0.58367 	 m..s
  117 	   114 	 0.62369 	 0.60102 	 ~...
  116 	   115 	 0.57169 	 0.60541 	 m..s
  119 	   116 	 0.63595 	 0.60795 	 ~...
  120 	   117 	 0.64153 	 0.61739 	 ~...
  118 	   118 	 0.63538 	 0.61949 	 ~...
  112 	   119 	 0.55673 	 0.62056 	 m..s
  115 	   120 	 0.55816 	 0.62480 	 m..s
==========================================
r_mrr = 0.9873542189598083
r2_mrr = 0.974584698677063
spearmanr_mrr@5 = 0.9665658473968506
spearmanr_mrr@10 = 0.8240877389907837
spearmanr_mrr@50 = 0.99799644947052
spearmanr_mrr@100 = 0.9959762692451477
spearmanr_mrr@All = 0.9958231449127197
==========================================
test time: 0.403
Done Testing dataset UMLS
total time taken: 193.6813564300537
training time taken: 187.42541313171387
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9874)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9746)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9666)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8241)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9980)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9960)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9958)}}, 'test_loss': {'TransE': {'UMLS': 0.7499355328909587}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4052553802049859
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [494, 819, 48, 119, 926, 21, 941, 853, 149, 918, 559, 235, 745, 1159, 160, 998, 481, 101, 981, 438, 87, 691, 311, 199, 154, 736, 404, 840, 731, 1074, 526, 362, 631, 1057, 1144, 1069, 181, 930, 642, 1111, 1059, 1191, 500, 905, 892, 988, 281, 598, 657, 250, 194, 761, 929, 846, 239, 4, 592, 407, 1193, 118, 528, 628, 345, 12, 995, 847, 370, 303, 405, 381, 331, 83, 660, 114, 447, 771, 174, 275, 430, 532, 137, 184, 151, 1179, 820, 955, 506, 1183, 227, 896, 999, 480, 389, 1134, 578, 670, 335, 204, 386, 312, 1004, 1046, 666, 1136, 925, 1151, 256, 553, 314, 875, 426, 1025, 457, 1042, 423, 196, 128, 726, 183, 837, 675]
valid_ids (0): []
train_ids (1094): [167, 801, 635, 1009, 938, 43, 513, 658, 724, 483, 984, 1044, 569, 88, 246, 367, 1122, 329, 462, 954, 649, 377, 313, 681, 877, 78, 1107, 208, 816, 602, 283, 551, 225, 398, 232, 202, 629, 1024, 186, 104, 1080, 942, 983, 835, 58, 556, 959, 169, 826, 659, 343, 916, 730, 529, 1195, 778, 318, 809, 932, 568, 1153, 350, 1188, 1211, 226, 1181, 346, 936, 347, 760, 26, 203, 969, 961, 717, 200, 779, 467, 460, 251, 238, 357, 1203, 1073, 1089, 365, 915, 1139, 600, 616, 135, 1021, 28, 706, 1124, 813, 650, 355, 328, 1053, 361, 651, 69, 782, 790, 1119, 945, 597, 1058, 1065, 108, 962, 1170, 366, 47, 150, 290, 525, 822, 815, 319, 887, 599, 1112, 116, 161, 738, 258, 842, 7, 299, 744, 767, 453, 109, 534, 451, 1196, 935, 53, 713, 757, 197, 1060, 968, 637, 212, 1110, 2, 41, 76, 475, 445, 1189, 673, 978, 469, 39, 780, 672, 223, 27, 839, 173, 818, 538, 1036, 478, 913, 185, 843, 84, 1047, 701, 255, 8, 163, 297, 80, 201, 752, 363, 982, 394, 55, 278, 1066, 110, 425, 966, 254, 291, 1062, 482, 580, 1209, 705, 606, 1143, 1091, 624, 523, 100, 269, 604, 567, 172, 718, 63, 920, 464, 348, 266, 1051, 805, 1104, 975, 287, 52, 68, 751, 1027, 1050, 1167, 123, 552, 1147, 931, 140, 284, 872, 686, 791, 924, 393, 618, 155, 699, 1101, 878, 817, 472, 1052, 899, 1205, 384, 158, 97, 832, 1037, 428, 808, 92, 737, 353, 585, 170, 764, 264, 126, 418, 667, 766, 54, 987, 861, 911, 1114, 294, 530, 298, 369, 385, 320, 862, 1076, 956, 632, 112, 122, 698, 844, 1141, 450, 215, 106, 17, 358, 406, 121, 684, 749, 507, 669, 973, 317, 136, 1063, 683, 933, 742, 1100, 1199, 399, 890, 166, 1173, 537, 171, 213, 415, 566, 865, 216, 646, 1210, 806, 620, 950, 410, 1149, 708, 895, 1000, 1155, 96, 309, 1045, 1001, 117, 879, 882, 514, 644, 536, 833, 509, 289, 391, 1093, 746, 241, 836, 416, 417, 187, 485, 1096, 231, 517, 612, 619, 591, 1108, 609, 753, 1115, 282, 1010, 980, 923, 831, 424, 162, 1088, 505, 851, 1182, 723, 678, 458, 263, 339, 1082, 1163, 1032, 664, 1017, 1034, 1165, 653, 133, 587, 687, 1169, 636, 383, 1022, 177, 876, 857, 220, 120, 1213, 1142, 73, 419, 456, 46, 946, 1038, 1056, 610, 855, 560, 125, 1092, 323, 704, 316, 883, 849, 70, 596, 633, 1113, 572, 222, 952, 38, 614, 265, 268, 807, 712, 34, 325, 31, 739, 957, 182, 648, 221, 189, 692, 61, 91, 468, 471, 1040, 322, 621, 75, 668, 561, 531, 965, 180, 991, 334, 812, 130, 240, 337, 850, 1098, 1099, 1105, 493, 914, 214, 1200, 288, 465, 548, 589, 466, 810, 582, 153, 868, 488, 588, 793, 401, 1028, 729, 565, 944, 1008, 178, 545, 829, 277, 1041, 728, 206, 190, 1118, 781, 754, 889, 1106, 1176, 308, 1148, 721, 584, 814, 971, 16, 727, 1020, 86, 558, 1048, 1164, 429, 62, 1067, 891, 1013, 295, 1130, 997, 512, 1078, 515, 127, 486, 501, 533, 919, 378, 433, 315, 72, 830, 510, 696, 823, 750, 30, 770, 586, 327, 245, 1121, 888, 341, 702, 662, 1083, 703, 402, 711, 260, 490, 205, 679, 976, 640, 421, 301, 741, 827, 338, 427, 784, 1177, 943, 852, 411, 564, 1156, 762, 863, 797, 800, 302, 958, 422, 716, 293, 138, 1054, 547, 60, 1150, 1186, 884, 102, 917, 522, 387, 789, 714, 113, 951, 804, 1180, 627, 593, 81, 19, 1003, 590, 652, 1202, 37, 732, 330, 157, 774, 95, 233, 502, 211, 700, 463, 625, 270, 605, 9, 147, 1135, 615, 6, 986, 848, 1187, 324, 379, 336, 249, 440, 1031, 626, 375, 351, 663, 449, 276, 98, 11, 307, 376, 40, 1095, 272, 803, 539, 168, 601, 496, 775, 993, 247, 927, 722, 477, 689, 934, 229, 1126, 111, 1162, 747, 107, 420, 979, 859, 874, 574, 1019, 45, 434, 693, 970, 413, 368, 1109, 1174, 1064, 132, 431, 783, 25, 1131, 1129, 885, 99, 179, 543, 563, 65, 305, 880, 1157, 546, 654, 1016, 74, 661, 967, 397, 455, 300, 725, 886, 437, 395, 51, 603, 611, 454, 29, 1125, 380, 907, 382, 734, 1166, 93, 459, 195, 148, 1084, 685, 193, 146, 124, 444, 1094, 484, 64, 1127, 720, 479, 364, 985, 321, 23, 165, 1071, 1152, 499, 436, 49, 912, 1175, 50, 535, 1197, 1123, 688, 949, 1006, 446, 655, 1208, 676, 207, 396, 939, 710, 948, 412, 131, 544, 1072, 1061, 217, 356, 198, 191, 243, 841, 115, 1012, 474, 10, 613, 252, 359, 432, 1079, 66, 921, 448, 267, 228, 1033, 352, 541, 1, 1023, 792, 285, 1161, 641, 373, 134, 715, 645, 763, 787, 825, 583, 210, 1140, 142, 443, 1132, 860, 24, 400, 188, 230, 571, 497, 897, 639, 409, 1086, 103, 594, 85, 1102, 1178, 82, 292, 595, 575, 273, 498, 94, 867, 296, 740, 516, 845, 79, 898, 1068, 244, 963, 607, 1207, 105, 773, 175, 1030, 665, 176, 218, 286, 695, 802, 1116, 452, 489, 1201, 145, 557, 1145, 893, 234, 156, 504, 77, 579, 1097, 542, 977, 152, 866, 236, 439, 519, 326, 71, 237, 304, 838, 1206, 1026, 647, 992, 570, 694, 758, 1185, 937, 13, 22, 748, 390, 796, 1039, 1137, 333, 392, 261, 690, 1018, 1146, 871, 881, 634, 511, 1049, 521, 1171, 129, 3, 491, 549, 765, 697, 864, 371, 994, 550, 209, 59, 503, 372, 909, 508, 257, 403, 870, 461, 253, 1055, 57, 1011, 682, 143, 1070, 974, 262, 928, 798, 36, 360, 719, 608, 1168, 902, 90, 576, 527, 540, 996, 990, 577, 858, 1002, 67, 414, 349, 44, 1090, 733, 1133, 1190, 811, 442, 671, 18, 755, 788, 89, 972, 0, 518, 344, 735, 159, 960, 1007, 139, 495, 1103, 340, 901, 1212, 408, 772, 35, 940, 487, 1154, 164, 854, 1085, 476, 1029, 922, 869, 821, 279, 1117, 989, 617, 470, 374, 900, 834, 904, 242, 15, 756, 5, 1160, 873, 1194, 776, 828, 20, 573, 56, 656, 824, 680, 581, 524, 1128, 562, 777, 1081, 769, 1077, 492, 630, 42, 759, 310, 622, 388, 274, 1005, 259, 1184, 1015, 271, 33, 1214, 1035, 280, 192, 964, 441, 555, 623, 342, 794, 677, 906, 638, 473, 709, 707, 908, 856, 14, 894, 1158, 799, 743, 520, 219, 306, 910, 1192, 1087, 795, 32, 332, 141, 1120, 786, 554, 785, 1198, 643, 1172, 1138, 1014, 248, 903, 947, 354, 224, 1043, 674, 1204, 144, 1075, 435, 768, 953]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7495428340862845
the save name prefix for this run is:  chkpt-ID_7495428340862845_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 18
rank avg (pred): 0.473 +- 0.005
mrr vals (pred, true): 0.016, 0.474
batch losses (mrrl, rdl): 0.0, 0.0042002779

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1200
rank avg (pred): 0.375 +- 0.224
mrr vals (pred, true): 0.198, 0.044
batch losses (mrrl, rdl): 0.0, 3.40442e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 298
rank avg (pred): 0.018 +- 0.013
mrr vals (pred, true): 0.485, 0.559
batch losses (mrrl, rdl): 0.0, 1.9745e-06

Epoch over!
epoch time: 12.283

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 212
rank avg (pred): 0.349 +- 0.244
mrr vals (pred, true): 0.283, 0.041
batch losses (mrrl, rdl): 0.0, 6.24187e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 911
rank avg (pred): 0.120 +- 0.097
mrr vals (pred, true): 0.378, 0.251
batch losses (mrrl, rdl): 0.0, 2.23448e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 346
rank avg (pred): 0.344 +- 0.245
mrr vals (pred, true): 0.302, 0.126
batch losses (mrrl, rdl): 0.0, 0.0003861673

Epoch over!
epoch time: 12.17

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1131
rank avg (pred): 0.330 +- 0.241
mrr vals (pred, true): 0.317, 0.037
batch losses (mrrl, rdl): 0.0, 0.0001729041

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1062
rank avg (pred): 0.026 +- 0.020
mrr vals (pred, true): 0.455, 0.622
batch losses (mrrl, rdl): 0.0, 5.876e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 822
rank avg (pred): 0.023 +- 0.018
mrr vals (pred, true): 0.470, 0.538
batch losses (mrrl, rdl): 0.0, 2.5182e-06

Epoch over!
epoch time: 12.019

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 491
rank avg (pred): 0.305 +- 0.224
mrr vals (pred, true): 0.319, 0.131
batch losses (mrrl, rdl): 0.0, 4.81298e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1084
rank avg (pred): 0.341 +- 0.251
mrr vals (pred, true): 0.327, 0.148
batch losses (mrrl, rdl): 0.0, 0.0004223588

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 997
rank avg (pred): 0.030 +- 0.023
mrr vals (pred, true): 0.448, 0.604
batch losses (mrrl, rdl): 0.0, 9.738e-07

Epoch over!
epoch time: 12.339

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 172
rank avg (pred): 0.324 +- 0.239
mrr vals (pred, true): 0.324, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001627266

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 817
rank avg (pred): 0.019 +- 0.015
mrr vals (pred, true): 0.493, 0.399
batch losses (mrrl, rdl): 0.0, 1.38274e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 279
rank avg (pred): 0.024 +- 0.019
mrr vals (pred, true): 0.475, 0.531
batch losses (mrrl, rdl): 0.0, 4.582e-07

Epoch over!
epoch time: 11.89

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 435
rank avg (pred): 0.316 +- 0.242
mrr vals (pred, true): 0.349, 0.040
batch losses (mrrl, rdl): 0.8939992785, 0.0001610021

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 262
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.560, 0.550
batch losses (mrrl, rdl): 0.0009609875, 5.5619e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 646
rank avg (pred): 0.206 +- 0.064
mrr vals (pred, true): 0.041, 0.035
batch losses (mrrl, rdl): 0.0008840915, 0.0011099125

Epoch over!
epoch time: 12.36

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 863
rank avg (pred): 0.301 +- 0.093
mrr vals (pred, true): 0.064, 0.094
batch losses (mrrl, rdl): 0.0020401338, 5.26048e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 859
rank avg (pred): 0.338 +- 0.103
mrr vals (pred, true): 0.062, 0.084
batch losses (mrrl, rdl): 0.0013585447, 6.50895e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1141
rank avg (pred): 0.256 +- 0.156
mrr vals (pred, true): 0.141, 0.158
batch losses (mrrl, rdl): 0.0029520295, 7.72466e-05

Epoch over!
epoch time: 12.089

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 935
rank avg (pred): 0.253 +- 0.105
mrr vals (pred, true): 0.054, 0.085
batch losses (mrrl, rdl): 0.0001792244, 5.29418e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 238
rank avg (pred): 0.343 +- 0.102
mrr vals (pred, true): 0.057, 0.039
batch losses (mrrl, rdl): 0.0004752143, 0.0003010228

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 476
rank avg (pred): 0.336 +- 0.153
mrr vals (pred, true): 0.088, 0.039
batch losses (mrrl, rdl): 0.0146969501, 0.000235304

Epoch over!
epoch time: 12.012

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 593
rank avg (pred): 0.240 +- 0.141
mrr vals (pred, true): 0.057, 0.039
batch losses (mrrl, rdl): 0.0004316779, 0.0007745793

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 580
rank avg (pred): 0.232 +- 0.156
mrr vals (pred, true): 0.052, 0.038
batch losses (mrrl, rdl): 3.67964e-05, 0.0007936881

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 450
rank avg (pred): 0.356 +- 0.104
mrr vals (pred, true): 0.056, 0.043
batch losses (mrrl, rdl): 0.000370023, 0.0001429605

Epoch over!
epoch time: 12.293

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1073
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.623, 0.613
batch losses (mrrl, rdl): 0.0008929835, 3.4999e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 248
rank avg (pred): 0.012 +- 0.011
mrr vals (pred, true): 0.582, 0.628
batch losses (mrrl, rdl): 0.0214941129, 1.5076e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1013
rank avg (pred): 0.269 +- 0.124
mrr vals (pred, true): 0.080, 0.145
batch losses (mrrl, rdl): 0.0420540124, 7.08876e-05

Epoch over!
epoch time: 12.116

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1072
rank avg (pred): 0.013 +- 0.012
mrr vals (pred, true): 0.578, 0.577
batch losses (mrrl, rdl): 1.1713e-06, 1.8307e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1125
rank avg (pred): 0.269 +- 0.124
mrr vals (pred, true): 0.084, 0.044
batch losses (mrrl, rdl): 0.0114355506, 0.0006015326

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 33
rank avg (pred): 0.028 +- 0.028
mrr vals (pred, true): 0.522, 0.540
batch losses (mrrl, rdl): 0.003369546, 1.413e-07

Epoch over!
epoch time: 12.323

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 616
rank avg (pred): 0.280 +- 0.154
mrr vals (pred, true): 0.047, 0.038
batch losses (mrrl, rdl): 0.0001111658, 0.0005108186

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 308
rank avg (pred): 0.008 +- 0.007
mrr vals (pred, true): 0.646, 0.619
batch losses (mrrl, rdl): 0.0073844548, 3.2894e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 479
rank avg (pred): 0.359 +- 0.150
mrr vals (pred, true): 0.086, 0.041
batch losses (mrrl, rdl): 0.0128130876, 0.0001359341

Epoch over!
epoch time: 11.978

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 579
rank avg (pred): 0.265 +- 0.170
mrr vals (pred, true): 0.051, 0.034
batch losses (mrrl, rdl): 1.333e-05, 0.0005613596

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 795
rank avg (pred): 0.409 +- 0.093
mrr vals (pred, true): 0.049, 0.037
batch losses (mrrl, rdl): 5.6835e-06, 9.4713e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1106
rank avg (pred): 0.275 +- 0.127
mrr vals (pred, true): 0.088, 0.160
batch losses (mrrl, rdl): 0.0515477173, 0.0001199511

Epoch over!
epoch time: 12.092

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 935
rank avg (pred): 0.321 +- 0.143
mrr vals (pred, true): 0.042, 0.085
batch losses (mrrl, rdl): 0.0005677319, 4.94716e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1100
rank avg (pred): 0.268 +- 0.130
mrr vals (pred, true): 0.085, 0.117
batch losses (mrrl, rdl): 0.0097622452, 2.74144e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 147
rank avg (pred): 0.393 +- 0.087
mrr vals (pred, true): 0.047, 0.077
batch losses (mrrl, rdl): 0.0001090606, 0.0003145054

Epoch over!
epoch time: 12.028

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 473
rank avg (pred): 0.388 +- 0.119
mrr vals (pred, true): 0.066, 0.039
batch losses (mrrl, rdl): 0.0026125014, 0.0001042493

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 502
rank avg (pred): 0.379 +- 0.108
mrr vals (pred, true): 0.063, 0.078
batch losses (mrrl, rdl): 0.0017998675, 8.97691e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 868
rank avg (pred): 0.361 +- 0.106
mrr vals (pred, true): 0.062, 0.040
batch losses (mrrl, rdl): 0.001375091, 0.0001818299

Epoch over!
epoch time: 12.208

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.333 +- 0.132
mrr vals (pred, true): 0.090, 0.124

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.04549 	 0.03303 	 ~...
   35 	     1 	 0.05179 	 0.03363 	 ~...
   59 	     2 	 0.07008 	 0.03430 	 m..s
    2 	     3 	 0.03984 	 0.03475 	 ~...
    6 	     4 	 0.04036 	 0.03523 	 ~...
    8 	     5 	 0.04047 	 0.03569 	 ~...
   80 	     6 	 0.08295 	 0.03611 	 m..s
   10 	     7 	 0.04148 	 0.03620 	 ~...
   56 	     8 	 0.06726 	 0.03672 	 m..s
   24 	     9 	 0.04765 	 0.03680 	 ~...
   11 	    10 	 0.04234 	 0.03694 	 ~...
   47 	    11 	 0.05865 	 0.03699 	 ~...
   75 	    12 	 0.07623 	 0.03730 	 m..s
    4 	    13 	 0.04005 	 0.03766 	 ~...
   42 	    14 	 0.05464 	 0.03780 	 ~...
   49 	    15 	 0.05888 	 0.03794 	 ~...
   69 	    16 	 0.07314 	 0.03809 	 m..s
    9 	    17 	 0.04127 	 0.03829 	 ~...
    1 	    18 	 0.03950 	 0.03829 	 ~...
   20 	    19 	 0.04600 	 0.03832 	 ~...
   12 	    20 	 0.04235 	 0.03833 	 ~...
   71 	    21 	 0.07433 	 0.03897 	 m..s
   13 	    22 	 0.04315 	 0.03933 	 ~...
   41 	    23 	 0.05421 	 0.03939 	 ~...
   58 	    24 	 0.06779 	 0.03940 	 ~...
   84 	    25 	 0.08648 	 0.03942 	 m..s
   78 	    26 	 0.07785 	 0.03959 	 m..s
   51 	    27 	 0.05955 	 0.03963 	 ~...
   54 	    28 	 0.06217 	 0.03964 	 ~...
   39 	    29 	 0.05351 	 0.03985 	 ~...
    7 	    30 	 0.04039 	 0.04024 	 ~...
    5 	    31 	 0.04018 	 0.04075 	 ~...
   14 	    32 	 0.04426 	 0.04124 	 ~...
   15 	    33 	 0.04503 	 0.04126 	 ~...
    3 	    34 	 0.04001 	 0.04128 	 ~...
   25 	    35 	 0.04779 	 0.04131 	 ~...
   63 	    36 	 0.07050 	 0.04163 	 ~...
   40 	    37 	 0.05411 	 0.04165 	 ~...
   26 	    38 	 0.04786 	 0.04171 	 ~...
    0 	    39 	 0.03877 	 0.04233 	 ~...
   50 	    40 	 0.05920 	 0.04408 	 ~...
   45 	    41 	 0.05683 	 0.04593 	 ~...
   28 	    42 	 0.04862 	 0.04609 	 ~...
   62 	    43 	 0.07045 	 0.06737 	 ~...
   52 	    44 	 0.06063 	 0.06822 	 ~...
   60 	    45 	 0.07030 	 0.06896 	 ~...
   55 	    46 	 0.06719 	 0.07279 	 ~...
   27 	    47 	 0.04831 	 0.07328 	 ~...
   57 	    48 	 0.06731 	 0.07347 	 ~...
   23 	    49 	 0.04726 	 0.07561 	 ~...
   22 	    50 	 0.04711 	 0.07624 	 ~...
   21 	    51 	 0.04647 	 0.08159 	 m..s
   31 	    52 	 0.04969 	 0.08483 	 m..s
   17 	    53 	 0.04526 	 0.08625 	 m..s
   37 	    54 	 0.05258 	 0.08676 	 m..s
   29 	    55 	 0.04905 	 0.08734 	 m..s
   36 	    56 	 0.05239 	 0.08756 	 m..s
   34 	    57 	 0.05143 	 0.08880 	 m..s
   18 	    58 	 0.04539 	 0.08899 	 m..s
   32 	    59 	 0.05048 	 0.09222 	 m..s
   66 	    60 	 0.07236 	 0.09271 	 ~...
   16 	    61 	 0.04513 	 0.09304 	 m..s
   44 	    62 	 0.05562 	 0.09360 	 m..s
   30 	    63 	 0.04922 	 0.09495 	 m..s
   38 	    64 	 0.05297 	 0.09546 	 m..s
   33 	    65 	 0.05070 	 0.09637 	 m..s
   46 	    66 	 0.05770 	 0.09782 	 m..s
   48 	    67 	 0.05867 	 0.09782 	 m..s
   43 	    68 	 0.05518 	 0.10102 	 m..s
   53 	    69 	 0.06121 	 0.10210 	 m..s
   65 	    70 	 0.07231 	 0.10331 	 m..s
   83 	    71 	 0.08618 	 0.10345 	 ~...
   81 	    72 	 0.08399 	 0.10383 	 ~...
   77 	    73 	 0.07771 	 0.10900 	 m..s
   68 	    74 	 0.07294 	 0.11400 	 m..s
   61 	    75 	 0.07037 	 0.11467 	 m..s
   64 	    76 	 0.07226 	 0.11646 	 m..s
   74 	    77 	 0.07597 	 0.11742 	 m..s
   67 	    78 	 0.07276 	 0.11998 	 m..s
   82 	    79 	 0.08600 	 0.12177 	 m..s
   87 	    80 	 0.09000 	 0.12359 	 m..s
   85 	    81 	 0.08659 	 0.12398 	 m..s
   72 	    82 	 0.07460 	 0.13402 	 m..s
   86 	    83 	 0.08810 	 0.13662 	 m..s
   79 	    84 	 0.08005 	 0.13689 	 m..s
   70 	    85 	 0.07397 	 0.13893 	 m..s
   73 	    86 	 0.07470 	 0.14083 	 m..s
   76 	    87 	 0.07726 	 0.14521 	 m..s
   90 	    88 	 0.17009 	 0.15153 	 ~...
   89 	    89 	 0.16953 	 0.15415 	 ~...
   91 	    90 	 0.17582 	 0.15486 	 ~...
   88 	    91 	 0.15444 	 0.16007 	 ~...
   92 	    92 	 0.18138 	 0.16156 	 ~...
   94 	    93 	 0.22826 	 0.17167 	 m..s
   93 	    94 	 0.19957 	 0.23331 	 m..s
   95 	    95 	 0.32222 	 0.31539 	 ~...
   98 	    96 	 0.52035 	 0.34052 	 MISS
   97 	    97 	 0.51669 	 0.41053 	 MISS
   99 	    98 	 0.52246 	 0.42680 	 m..s
  100 	    99 	 0.52339 	 0.43982 	 m..s
   96 	   100 	 0.50925 	 0.48976 	 ~...
  101 	   101 	 0.53150 	 0.49835 	 m..s
  105 	   102 	 0.54484 	 0.50510 	 m..s
  103 	   103 	 0.53674 	 0.52270 	 ~...
  108 	   104 	 0.55962 	 0.54371 	 ~...
  107 	   105 	 0.55745 	 0.54642 	 ~...
  102 	   106 	 0.53554 	 0.54980 	 ~...
  106 	   107 	 0.55528 	 0.55609 	 ~...
  104 	   108 	 0.54249 	 0.58367 	 m..s
  116 	   109 	 0.61023 	 0.60513 	 ~...
  111 	   110 	 0.60397 	 0.60795 	 ~...
  118 	   111 	 0.61350 	 0.61133 	 ~...
  109 	   112 	 0.59958 	 0.61247 	 ~...
  113 	   113 	 0.60525 	 0.61289 	 ~...
  115 	   114 	 0.60999 	 0.61471 	 ~...
  110 	   115 	 0.60244 	 0.61656 	 ~...
  114 	   116 	 0.60577 	 0.61942 	 ~...
  119 	   117 	 0.61812 	 0.62112 	 ~...
  120 	   118 	 0.62317 	 0.62441 	 ~...
  112 	   119 	 0.60421 	 0.63054 	 ~...
  117 	   120 	 0.61054 	 0.63323 	 ~...
==========================================
r_mrr = 0.9846278429031372
r2_mrr = 0.9664626121520996
spearmanr_mrr@5 = 0.9815835952758789
spearmanr_mrr@10 = 0.9844846725463867
spearmanr_mrr@50 = 0.9842677116394043
spearmanr_mrr@100 = 0.9893841743469238
spearmanr_mrr@All = 0.9898223876953125
==========================================
test time: 0.395
Done Testing dataset UMLS
total time taken: 188.9128975868225
training time taken: 182.67078161239624
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9846)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9665)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9816)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9845)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9843)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9894)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9898)}}, 'test_loss': {'TransE': {'UMLS': 1.2811934561923408}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4414916609848972
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [829, 74, 155, 1121, 1086, 191, 344, 1171, 896, 614, 153, 449, 857, 619, 1059, 434, 719, 594, 76, 261, 1036, 980, 731, 419, 134, 212, 190, 1123, 353, 211, 327, 560, 443, 730, 704, 75, 737, 404, 1106, 301, 563, 1136, 1102, 1093, 1074, 14, 591, 986, 467, 1170, 766, 627, 348, 971, 218, 420, 341, 297, 595, 753, 411, 1185, 93, 1176, 21, 120, 406, 983, 665, 396, 450, 934, 73, 51, 1031, 1048, 796, 909, 194, 706, 335, 1205, 1165, 587, 552, 247, 1113, 238, 649, 834, 132, 256, 260, 855, 1, 905, 1070, 1005, 528, 41, 334, 982, 492, 1000, 455, 604, 39, 990, 1053, 184, 641, 844, 89, 973, 500, 452, 209, 965, 498, 997, 920]
valid_ids (0): []
train_ids (1094): [1033, 551, 150, 9, 200, 948, 44, 15, 1180, 571, 930, 741, 299, 270, 784, 136, 374, 114, 732, 532, 929, 817, 622, 117, 554, 354, 795, 277, 129, 103, 1193, 557, 638, 1064, 1041, 520, 978, 987, 1016, 502, 293, 488, 540, 61, 1181, 280, 265, 616, 881, 830, 803, 565, 17, 1058, 818, 887, 513, 85, 154, 534, 388, 842, 287, 442, 879, 257, 911, 698, 972, 776, 1083, 1076, 1152, 224, 139, 462, 700, 10, 904, 1139, 1066, 788, 690, 50, 1198, 446, 979, 692, 1046, 273, 339, 1084, 309, 146, 439, 26, 564, 473, 138, 593, 580, 415, 456, 494, 168, 635, 727, 1056, 674, 860, 1047, 944, 1111, 1129, 64, 285, 54, 135, 651, 286, 820, 611, 445, 384, 232, 728, 505, 1196, 996, 765, 600, 414, 937, 1199, 116, 409, 1004, 679, 395, 538, 548, 831, 172, 1208, 1120, 440, 175, 791, 780, 561, 459, 963, 480, 48, 63, 1044, 824, 271, 1142, 382, 49, 599, 774, 142, 694, 639, 52, 244, 152, 330, 1168, 615, 304, 957, 512, 800, 642, 268, 609, 799, 787, 1039, 1037, 1001, 1130, 461, 838, 399, 1188, 72, 691, 969, 206, 36, 710, 606, 387, 797, 882, 137, 1020, 1071, 763, 547, 629, 94, 230, 722, 424, 597, 106, 441, 880, 681, 507, 32, 883, 417, 1122, 959, 715, 141, 598, 158, 1124, 740, 759, 1161, 504, 27, 99, 1211, 1002, 65, 68, 754, 274, 847, 901, 392, 756, 523, 8, 912, 977, 5, 368, 1184, 18, 1051, 655, 542, 203, 644, 251, 664, 954, 733, 482, 917, 861, 358, 1197, 1203, 1015, 1081, 111, 497, 670, 97, 290, 123, 777, 695, 886, 1105, 808, 966, 1169, 479, 485, 778, 592, 267, 789, 603, 573, 453, 975, 1023, 248, 839, 1024, 159, 284, 148, 1101, 677, 903, 663, 1183, 632, 363, 854, 914, 43, 687, 678, 661, 984, 913, 900, 867, 1144, 558, 28, 510, 174, 643, 899, 360, 1077, 131, 416, 12, 1092, 993, 1206, 515, 846, 188, 553, 768, 947, 840, 658, 669, 264, 617, 1209, 107, 1114, 1116, 1189, 127, 1146, 310, 870, 810, 724, 813, 47, 918, 331, 620, 31, 88, 156, 457, 356, 648, 1025, 288, 118, 180, 83, 657, 113, 725, 760, 877, 1008, 586, 3, 1212, 197, 974, 1140, 0, 832, 352, 570, 1164, 805, 888, 13, 916, 323, 885, 34, 1104, 367, 1028, 991, 222, 377, 544, 1191, 1095, 216, 567, 42, 329, 584, 794, 460, 472, 1009, 165, 219, 263, 1043, 1125, 1022, 852, 1158, 637, 748, 764, 961, 130, 898, 828, 183, 1112, 254, 864, 1182, 432, 253, 365, 549, 204, 1207, 522, 489, 448, 1190, 124, 454, 202, 444, 407, 1091, 233, 906, 541, 856, 226, 702, 652, 343, 699, 381, 466, 40, 246, 1119, 667, 291, 1145, 585, 259, 755, 1110, 734, 543, 953, 696, 516, 676, 24, 943, 964, 941, 385, 682, 827, 1089, 1160, 231, 56, 735, 179, 207, 486, 1153, 100, 581, 921, 187, 751, 804, 823, 793, 1061, 508, 1055, 305, 853, 144, 30, 785, 321, 346, 84, 869, 1078, 618, 902, 295, 509, 371, 79, 720, 347, 173, 758, 316, 925, 82, 1054, 147, 78, 370, 357, 315, 59, 60, 1162, 1010, 430, 866, 333, 895, 897, 272, 110, 87, 1137, 506, 413, 579, 401, 1202, 96, 556, 1163, 716, 198, 1067, 1131, 1103, 1073, 868, 390, 647, 19, 1032, 1049, 950, 939, 1052, 189, 6, 688, 1150, 940, 640, 653, 524, 81, 1143, 45, 711, 359, 283, 355, 1157, 275, 626, 956, 307, 386, 1147, 182, 90, 214, 311, 1118, 662, 607, 393, 306, 806, 529, 518, 527, 66, 421, 140, 1087, 671, 1149, 514, 892, 942, 757, 968, 628, 1148, 398, 583, 62, 340, 525, 252, 533, 790, 569, 709, 164, 20, 1214, 469, 625, 517, 1156, 496, 336, 229, 503, 1108, 729, 468, 77, 484, 1179, 383, 463, 405, 845, 816, 318, 1141, 962, 910, 672, 697, 239, 511, 574, 889, 490, 1079, 621, 163, 1035, 919, 437, 874, 366, 976, 361, 33, 255, 798, 848, 1201, 825, 499, 320, 1019, 822, 701, 372, 458, 807, 133, 1213, 612, 435, 631, 1097, 145, 91, 478, 1109, 927, 217, 960, 199, 495, 213, 705, 588, 526, 177, 1175, 71, 119, 412, 952, 37, 559, 782, 809, 746, 302, 750, 22, 1011, 470, 429, 1017, 313, 907, 531, 235, 521, 1115, 668, 769, 225, 128, 908, 545, 410, 4, 105, 596, 493, 933, 708, 1187, 1133, 1030, 935, 477, 575, 380, 364, 240, 319, 471, 258, 186, 1127, 221, 167, 772, 865, 634, 210, 1100, 1014, 427, 143, 195, 814, 636, 241, 428, 296, 1085, 476, 276, 1042, 1090, 322, 659, 815, 125, 481, 1134, 1080, 325, 605, 938, 601, 802, 836, 878, 786, 151, 1060, 101, 1021, 46, 703, 324, 1068, 876, 104, 1007, 1195, 161, 893, 1003, 922, 723, 841, 298, 958, 1050, 590, 872, 999, 176, 562, 1151, 602, 871, 126, 185, 781, 819, 86, 249, 337, 38, 689, 80, 530, 242, 623, 1154, 369, 474, 890, 29, 749, 693, 994, 1126, 536, 955, 752, 122, 650, 464, 1167, 767, 171, 537, 718, 539, 736, 215, 770, 1128, 717, 109, 884, 379, 779, 58, 436, 418, 350, 985, 576, 1012, 645, 1138, 835, 108, 673, 608, 1082, 11, 236, 431, 192, 572, 928, 308, 932, 1096, 568, 170, 833, 314, 851, 519, 1132, 926, 1038, 837, 761, 685, 747, 1062, 773, 294, 223, 112, 378, 53, 712, 397, 891, 610, 1204, 228, 578, 1045, 646, 282, 894, 25, 1178, 1099, 550, 1192, 70, 491, 1172, 328, 69, 394, 745, 1027, 949, 589, 1135, 812, 95, 738, 115, 266, 660, 98, 713, 169, 1029, 92, 1040, 1210, 220, 821, 451, 402, 7, 403, 555, 624, 317, 546, 1194, 1065, 995, 423, 196, 945, 426, 686, 1075, 389, 245, 281, 946, 289, 55, 160, 577, 465, 633, 475, 923, 57, 35, 1006, 862, 936, 1057, 102, 873, 762, 408, 501, 967, 859, 863, 707, 630, 375, 988, 205, 1034, 1013, 234, 227, 1098, 680, 262, 771, 1200, 743, 237, 23, 422, 181, 989, 67, 656, 1063, 721, 345, 269, 1072, 970, 300, 801, 850, 312, 487, 157, 775, 362, 915, 193, 1174, 998, 875, 201, 858, 684, 162, 121, 433, 391, 373, 981, 1159, 683, 1173, 666, 483, 726, 811, 376, 566, 2, 1155, 739, 1088, 1094, 438, 744, 425, 292, 1117, 178, 843, 278, 1107, 826, 992, 166, 742, 349, 16, 1026, 208, 849, 338, 447, 1186, 783, 332, 1069, 1018, 582, 250, 400, 326, 535, 303, 1166, 924, 951, 351, 654, 149, 279, 792, 613, 243, 342, 1177, 931, 675, 714]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9259870135941546
the save name prefix for this run is:  chkpt-ID_9259870135941546_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 959
rank avg (pred): 0.455 +- 0.006
mrr vals (pred, true): 0.016, 0.041
batch losses (mrrl, rdl): 0.0, 9.17725e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 691
rank avg (pred): 0.365 +- 0.203
mrr vals (pred, true): 0.152, 0.041
batch losses (mrrl, rdl): 0.0, 4.99619e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 310
rank avg (pred): 0.031 +- 0.024
mrr vals (pred, true): 0.433, 0.576
batch losses (mrrl, rdl): 0.0, 1.0339e-06

Epoch over!
epoch time: 12.181

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 793
rank avg (pred): 0.326 +- 0.238
mrr vals (pred, true): 0.278, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001839111

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 160
rank avg (pred): 0.325 +- 0.244
mrr vals (pred, true): 0.294, 0.098
batch losses (mrrl, rdl): 0.0, 0.0002165256

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 791
rank avg (pred): 0.317 +- 0.243
mrr vals (pred, true): 0.305, 0.041
batch losses (mrrl, rdl): 0.0, 0.0002017014

Epoch over!
epoch time: 11.999

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 535
rank avg (pred): 0.321 +- 0.239
mrr vals (pred, true): 0.292, 0.072
batch losses (mrrl, rdl): 0.0, 6.6683e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 392
rank avg (pred): 0.283 +- 0.221
mrr vals (pred, true): 0.314, 0.141
batch losses (mrrl, rdl): 0.0, 0.0001294153

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 771
rank avg (pred): 0.337 +- 0.254
mrr vals (pred, true): 0.297, 0.092
batch losses (mrrl, rdl): 0.0, 0.0001391833

Epoch over!
epoch time: 11.907

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 724
rank avg (pred): 0.409 +- 0.275
mrr vals (pred, true): 0.251, 0.038
batch losses (mrrl, rdl): 0.0, 8.2545e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 179
rank avg (pred): 0.325 +- 0.245
mrr vals (pred, true): 0.298, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001388523

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 221
rank avg (pred): 0.310 +- 0.241
mrr vals (pred, true): 0.310, 0.040
batch losses (mrrl, rdl): 0.0, 0.0002295825

Epoch over!
epoch time: 12.073

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1146
rank avg (pred): 0.223 +- 0.171
mrr vals (pred, true): 0.312, 0.150
batch losses (mrrl, rdl): 0.0, 1.71127e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 279
rank avg (pred): 0.041 +- 0.033
mrr vals (pred, true): 0.437, 0.531
batch losses (mrrl, rdl): 0.0, 3.488e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 651
rank avg (pred): 0.407 +- 0.263
mrr vals (pred, true): 0.236, 0.039
batch losses (mrrl, rdl): 0.0, 1.0667e-05

Epoch over!
epoch time: 12.033

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 127
rank avg (pred): 0.330 +- 0.253
mrr vals (pred, true): 0.296, 0.102
batch losses (mrrl, rdl): 0.3767134547, 0.0002105701

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 549
rank avg (pred): 0.264 +- 0.142
mrr vals (pred, true): 0.098, 0.061
batch losses (mrrl, rdl): 0.0235133059, 0.00018465

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 418
rank avg (pred): 0.412 +- 0.212
mrr vals (pred, true): 0.074, 0.040
batch losses (mrrl, rdl): 0.0055390126, 1.56261e-05

Epoch over!
epoch time: 12.433

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 484
rank avg (pred): 0.396 +- 0.246
mrr vals (pred, true): 0.100, 0.037
batch losses (mrrl, rdl): 0.0251081586, 2.1365e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 519
rank avg (pred): 0.291 +- 0.157
mrr vals (pred, true): 0.100, 0.067
batch losses (mrrl, rdl): 0.025488507, 9.0569e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 7
rank avg (pred): 0.018 +- 0.013
mrr vals (pred, true): 0.432, 0.544
batch losses (mrrl, rdl): 0.125120014, 2.735e-06

Epoch over!
epoch time: 12.361

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1137
rank avg (pred): 0.263 +- 0.161
mrr vals (pred, true): 0.128, 0.165
batch losses (mrrl, rdl): 0.0139956996, 0.0001074202

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 864
rank avg (pred): 0.472 +- 0.226
mrr vals (pred, true): 0.053, 0.041
batch losses (mrrl, rdl): 0.0001014183, 3.2232e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1196
rank avg (pred): 0.403 +- 0.123
mrr vals (pred, true): 0.044, 0.039
batch losses (mrrl, rdl): 0.0003420117, 8.02738e-05

Epoch over!
epoch time: 12.096

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1064
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.625, 0.602
batch losses (mrrl, rdl): 0.0049902229, 5.6998e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1194
rank avg (pred): 0.416 +- 0.103
mrr vals (pred, true): 0.038, 0.040
batch losses (mrrl, rdl): 0.0013786055, 6.07935e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1142
rank avg (pred): 0.320 +- 0.173
mrr vals (pred, true): 0.097, 0.149
batch losses (mrrl, rdl): 0.0266281702, 0.0002904082

Epoch over!
epoch time: 12.051

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 621
rank avg (pred): 0.416 +- 0.091
mrr vals (pred, true): 0.041, 0.032
batch losses (mrrl, rdl): 0.0008870222, 4.13391e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 922
rank avg (pred): 0.347 +- 0.142
mrr vals (pred, true): 0.053, 0.085
batch losses (mrrl, rdl): 6.81728e-05, 0.0001184548

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 100
rank avg (pred): 0.466 +- 0.204
mrr vals (pred, true): 0.045, 0.100
batch losses (mrrl, rdl): 0.030279845, 0.0009512606

Epoch over!
epoch time: 12.205

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 930
rank avg (pred): 0.424 +- 0.182
mrr vals (pred, true): 0.051, 0.089
batch losses (mrrl, rdl): 9.7227e-06, 0.0004214068

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1174
rank avg (pred): 0.365 +- 0.133
mrr vals (pred, true): 0.059, 0.036
batch losses (mrrl, rdl): 0.0008352713, 7.13639e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1003
rank avg (pred): 0.389 +- 0.205
mrr vals (pred, true): 0.061, 0.132
batch losses (mrrl, rdl): 0.0510080159, 0.0004320624

Epoch over!
epoch time: 12.377

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 54
rank avg (pred): 0.015 +- 0.013
mrr vals (pred, true): 0.510, 0.537
batch losses (mrrl, rdl): 0.0077215545, 5.6916e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 487
rank avg (pred): 0.349 +- 0.143
mrr vals (pred, true): 0.072, 0.082
batch losses (mrrl, rdl): 0.004916531, 4.75335e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 293
rank avg (pred): 0.010 +- 0.009
mrr vals (pred, true): 0.571, 0.628
batch losses (mrrl, rdl): 0.0326198228, 1.9152e-06

Epoch over!
epoch time: 12.357

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 790
rank avg (pred): 0.466 +- 0.222
mrr vals (pred, true): 0.049, 0.042
batch losses (mrrl, rdl): 1.56808e-05, 3.98266e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 895
rank avg (pred): 0.255 +- 0.190
mrr vals (pred, true): 0.186, 0.168
batch losses (mrrl, rdl): 0.0033970592, 0.000435702

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 540
rank avg (pred): 0.380 +- 0.118
mrr vals (pred, true): 0.060, 0.067
batch losses (mrrl, rdl): 0.0009658778, 6.67716e-05

Epoch over!
epoch time: 12.255

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 144
rank avg (pred): 0.398 +- 0.129
mrr vals (pred, true): 0.040, 0.092
batch losses (mrrl, rdl): 0.00090505, 0.0003852931

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 811
rank avg (pred): 0.027 +- 0.025
mrr vals (pred, true): 0.461, 0.430
batch losses (mrrl, rdl): 0.0093804309, 1.61818e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 410
rank avg (pred): 0.321 +- 0.194
mrr vals (pred, true): 0.075, 0.040
batch losses (mrrl, rdl): 0.0063222931, 0.0002069468

Epoch over!
epoch time: 12.283

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 198
rank avg (pred): 0.337 +- 0.167
mrr vals (pred, true): 0.065, 0.041
batch losses (mrrl, rdl): 0.0022667919, 0.0001970194

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 357
rank avg (pred): 0.391 +- 0.183
mrr vals (pred, true): 0.057, 0.090
batch losses (mrrl, rdl): 0.0005237404, 0.0002787098

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 215
rank avg (pred): 0.343 +- 0.188
mrr vals (pred, true): 0.066, 0.038
batch losses (mrrl, rdl): 0.0025834658, 0.0001984306

Epoch over!
epoch time: 12.135

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.019 +- 0.019
mrr vals (pred, true): 0.532, 0.431

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04557 	 0.03418 	 ~...
   59 	     1 	 0.08792 	 0.03430 	 m..s
    3 	     2 	 0.04654 	 0.03444 	 ~...
   16 	     3 	 0.05606 	 0.03496 	 ~...
   10 	     4 	 0.05142 	 0.03550 	 ~...
   19 	     5 	 0.05964 	 0.03551 	 ~...
   15 	     6 	 0.05508 	 0.03566 	 ~...
    6 	     7 	 0.04736 	 0.03580 	 ~...
   13 	     8 	 0.05482 	 0.03601 	 ~...
   64 	     9 	 0.09553 	 0.03604 	 m..s
   52 	    10 	 0.08117 	 0.03643 	 m..s
    0 	    11 	 0.04517 	 0.03645 	 ~...
   42 	    12 	 0.06655 	 0.03668 	 ~...
   17 	    13 	 0.05613 	 0.03680 	 ~...
   78 	    14 	 0.12096 	 0.03691 	 m..s
    5 	    15 	 0.04664 	 0.03693 	 ~...
    4 	    16 	 0.04659 	 0.03769 	 ~...
   31 	    17 	 0.06631 	 0.03780 	 ~...
   21 	    18 	 0.06236 	 0.03782 	 ~...
    7 	    19 	 0.04882 	 0.03784 	 ~...
   14 	    20 	 0.05493 	 0.03828 	 ~...
    2 	    21 	 0.04637 	 0.03851 	 ~...
   28 	    22 	 0.06466 	 0.03854 	 ~...
    8 	    23 	 0.04953 	 0.03862 	 ~...
   55 	    24 	 0.08499 	 0.03864 	 m..s
   27 	    25 	 0.06460 	 0.03869 	 ~...
   70 	    26 	 0.10359 	 0.03914 	 m..s
   31 	    27 	 0.06631 	 0.03920 	 ~...
    9 	    28 	 0.05031 	 0.03928 	 ~...
   79 	    29 	 0.12231 	 0.03954 	 m..s
   54 	    30 	 0.08490 	 0.04013 	 m..s
   41 	    31 	 0.06640 	 0.04024 	 ~...
   58 	    32 	 0.08719 	 0.04046 	 m..s
   56 	    33 	 0.08502 	 0.04068 	 m..s
   61 	    34 	 0.09082 	 0.04078 	 m..s
   75 	    35 	 0.10944 	 0.04091 	 m..s
   63 	    36 	 0.09475 	 0.04107 	 m..s
   66 	    37 	 0.09644 	 0.04136 	 m..s
   11 	    38 	 0.05147 	 0.04177 	 ~...
   31 	    39 	 0.06631 	 0.04197 	 ~...
   76 	    40 	 0.11036 	 0.04235 	 m..s
   72 	    41 	 0.10515 	 0.04262 	 m..s
   12 	    42 	 0.05373 	 0.04269 	 ~...
   24 	    43 	 0.06369 	 0.04302 	 ~...
   43 	    44 	 0.06658 	 0.04326 	 ~...
   71 	    45 	 0.10433 	 0.04353 	 m..s
   20 	    46 	 0.06224 	 0.04559 	 ~...
   18 	    47 	 0.05654 	 0.04650 	 ~...
   65 	    48 	 0.09572 	 0.04715 	 m..s
   29 	    49 	 0.06467 	 0.06558 	 ~...
   25 	    50 	 0.06398 	 0.06822 	 ~...
   47 	    51 	 0.06898 	 0.07655 	 ~...
   31 	    52 	 0.06631 	 0.07870 	 ~...
   23 	    53 	 0.06355 	 0.08156 	 ~...
   45 	    54 	 0.06767 	 0.08426 	 ~...
   31 	    55 	 0.06631 	 0.08469 	 ~...
   22 	    56 	 0.06345 	 0.08490 	 ~...
   31 	    57 	 0.06631 	 0.08540 	 ~...
   31 	    58 	 0.06631 	 0.08747 	 ~...
   30 	    59 	 0.06524 	 0.08771 	 ~...
   26 	    60 	 0.06439 	 0.08850 	 ~...
   31 	    61 	 0.06631 	 0.08924 	 ~...
   48 	    62 	 0.07034 	 0.09013 	 ~...
   31 	    63 	 0.06631 	 0.09496 	 ~...
   46 	    64 	 0.06788 	 0.10182 	 m..s
   83 	    65 	 0.15191 	 0.10383 	 m..s
   82 	    66 	 0.14385 	 0.10634 	 m..s
   44 	    67 	 0.06667 	 0.10702 	 m..s
   49 	    68 	 0.08004 	 0.11149 	 m..s
   81 	    69 	 0.14292 	 0.11360 	 ~...
   62 	    70 	 0.09151 	 0.11573 	 ~...
   60 	    71 	 0.09076 	 0.11936 	 ~...
   50 	    72 	 0.08056 	 0.12398 	 m..s
   31 	    73 	 0.06631 	 0.12681 	 m..s
   73 	    74 	 0.10760 	 0.12945 	 ~...
   53 	    75 	 0.08460 	 0.13066 	 m..s
   51 	    76 	 0.08114 	 0.13180 	 m..s
   67 	    77 	 0.09654 	 0.13270 	 m..s
   74 	    78 	 0.10794 	 0.13797 	 m..s
   68 	    79 	 0.09807 	 0.14043 	 m..s
   57 	    80 	 0.08650 	 0.14152 	 m..s
   69 	    81 	 0.09836 	 0.14185 	 m..s
   80 	    82 	 0.13330 	 0.14521 	 ~...
   84 	    83 	 0.21040 	 0.15415 	 m..s
   77 	    84 	 0.11414 	 0.15988 	 m..s
   85 	    85 	 0.23115 	 0.17167 	 m..s
   87 	    86 	 0.30878 	 0.23945 	 m..s
   86 	    87 	 0.30473 	 0.31539 	 ~...
   90 	    88 	 0.50121 	 0.36531 	 MISS
   91 	    89 	 0.50334 	 0.41053 	 m..s
   95 	    90 	 0.53221 	 0.43062 	 MISS
   88 	    91 	 0.49275 	 0.44419 	 m..s
   97 	    92 	 0.55619 	 0.49163 	 m..s
   89 	    93 	 0.49391 	 0.49835 	 ~...
  102 	    94 	 0.56849 	 0.50617 	 m..s
   94 	    95 	 0.53002 	 0.51292 	 ~...
   98 	    96 	 0.55792 	 0.51624 	 m..s
  101 	    97 	 0.56541 	 0.53417 	 m..s
   96 	    98 	 0.54562 	 0.54438 	 ~...
  100 	    99 	 0.56043 	 0.54533 	 ~...
  104 	   100 	 0.58169 	 0.55065 	 m..s
   99 	   101 	 0.56002 	 0.55609 	 ~...
  103 	   102 	 0.57730 	 0.56998 	 ~...
  107 	   103 	 0.59603 	 0.57095 	 ~...
   92 	   104 	 0.51626 	 0.58044 	 m..s
   93 	   105 	 0.52606 	 0.58296 	 m..s
  119 	   106 	 0.64120 	 0.58855 	 m..s
  113 	   107 	 0.62659 	 0.59282 	 m..s
  111 	   108 	 0.62217 	 0.60089 	 ~...
  120 	   109 	 0.65113 	 0.60240 	 m..s
  108 	   110 	 0.59631 	 0.60297 	 ~...
  112 	   111 	 0.62411 	 0.60425 	 ~...
  110 	   112 	 0.60096 	 0.60653 	 ~...
  116 	   113 	 0.63426 	 0.61247 	 ~...
  115 	   114 	 0.63239 	 0.61315 	 ~...
  106 	   115 	 0.59360 	 0.61471 	 ~...
  117 	   116 	 0.63931 	 0.61945 	 ~...
  105 	   117 	 0.58907 	 0.62127 	 m..s
  109 	   118 	 0.59998 	 0.62257 	 ~...
  114 	   119 	 0.62737 	 0.62686 	 ~...
  118 	   120 	 0.64071 	 0.62896 	 ~...
==========================================
r_mrr = 0.9860082864761353
r2_mrr = 0.9676032066345215
spearmanr_mrr@5 = 0.8925458788871765
spearmanr_mrr@10 = 0.9628974795341492
spearmanr_mrr@50 = 0.9904772043228149
spearmanr_mrr@100 = 0.9951475262641907
spearmanr_mrr@All = 0.9956803917884827
==========================================
test time: 0.388
Done Testing dataset UMLS
total time taken: 189.0513415336609
training time taken: 183.20390915870667
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9860)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9676)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.8925)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9629)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9905)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9951)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9957)}}, 'test_loss': {'TransE': {'UMLS': 1.6653753107157172}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9167631414731028
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [183, 368, 378, 29, 355, 608, 1044, 159, 937, 632, 452, 787, 1139, 1083, 1028, 232, 225, 857, 127, 1128, 642, 269, 718, 546, 208, 258, 1058, 645, 474, 224, 994, 156, 149, 1181, 412, 247, 451, 860, 963, 864, 1169, 26, 1189, 1023, 607, 590, 1084, 1203, 273, 335, 792, 1007, 326, 321, 46, 701, 919, 867, 688, 5, 628, 50, 985, 868, 391, 1052, 785, 1062, 1094, 119, 1002, 881, 264, 779, 1099, 463, 626, 419, 835, 908, 397, 732, 1177, 896, 746, 749, 255, 724, 1073, 577, 244, 565, 1048, 76, 400, 930, 409, 467, 630, 965, 674, 744, 114, 756, 14, 89, 1059, 4, 856, 767, 1145, 662, 466, 346, 19, 635, 912, 976, 726, 621, 131]
valid_ids (0): []
train_ids (1094): [791, 560, 20, 592, 43, 885, 151, 1102, 816, 352, 1187, 152, 1089, 629, 1130, 148, 680, 877, 393, 1026, 521, 643, 349, 300, 212, 354, 528, 44, 1012, 941, 10, 572, 563, 612, 299, 568, 973, 248, 814, 24, 890, 761, 784, 178, 458, 457, 671, 865, 365, 218, 486, 259, 694, 307, 364, 175, 793, 717, 48, 55, 1109, 327, 1172, 506, 1167, 1024, 317, 1027, 1198, 519, 822, 58, 145, 333, 1070, 1166, 338, 1066, 265, 465, 1199, 1036, 808, 1034, 293, 880, 613, 1141, 197, 888, 611, 1212, 53, 324, 682, 494, 382, 473, 54, 838, 777, 1118, 186, 525, 476, 206, 489, 492, 339, 1085, 990, 619, 286, 66, 435, 947, 1133, 698, 1180, 319, 13, 192, 130, 485, 562, 1110, 190, 1041, 128, 641, 686, 508, 848, 1086, 52, 418, 1079, 32, 692, 405, 254, 1096, 712, 989, 115, 341, 647, 834, 998, 56, 549, 770, 510, 797, 1069, 617, 844, 1098, 236, 579, 551, 772, 266, 667, 1132, 1082, 984, 1021, 573, 217, 1, 179, 1148, 786, 1164, 943, 1195, 314, 668, 491, 700, 207, 841, 81, 533, 322, 432, 453, 72, 1088, 1121, 445, 427, 1123, 847, 481, 921, 803, 819, 51, 297, 184, 650, 11, 271, 1191, 539, 1106, 1095, 395, 422, 95, 390, 850, 1149, 980, 1107, 821, 714, 1018, 876, 227, 1116, 366, 854, 403, 443, 972, 411, 15, 514, 578, 535, 64, 977, 1157, 558, 1140, 182, 813, 203, 362, 361, 374, 1093, 234, 1020, 869, 603, 878, 261, 901, 1035, 952, 518, 1202, 421, 721, 407, 669, 678, 728, 389, 738, 129, 79, 110, 472, 195, 820, 922, 1193, 360, 436, 530, 1075, 1192, 308, 773, 291, 589, 933, 120, 863, 1153, 342, 511, 1103, 725, 1174, 1184, 739, 135, 537, 371, 656, 804, 1186, 543, 1003, 30, 69, 139, 213, 294, 719, 133, 383, 1124, 239, 210, 637, 460, 1068, 547, 597, 1163, 150, 141, 999, 801, 798, 1105, 570, 523, 556, 991, 929, 468, 737, 1127, 758, 983, 97, 1178, 379, 1113, 915, 618, 196, 123, 639, 882, 113, 303, 709, 279, 1160, 1016, 544, 41, 843, 82, 211, 425, 100, 855, 978, 1134, 502, 62, 237, 99, 962, 1131, 161, 766, 522, 931, 274, 344, 1090, 504, 968, 742, 1108, 388, 776, 318, 788, 108, 644, 1074, 493, 909, 1072, 516, 1143, 907, 552, 1051, 245, 455, 126, 103, 1115, 858, 554, 795, 974, 903, 38, 685, 174, 424, 722, 104, 969, 125, 666, 730, 61, 600, 497, 1117, 323, 580, 1104, 187, 157, 1154, 337, 328, 906, 1014, 587, 954, 997, 950, 194, 311, 757, 495, 640, 40, 1129, 831, 594, 910, 567, 741, 70, 47, 924, 440, 825, 134, 859, 697, 242, 413, 1214, 193, 1097, 117, 137, 80, 331, 569, 509, 1037, 914, 689, 702, 658, 625, 1147, 45, 653, 301, 233, 221, 396, 616, 655, 837, 584, 849, 755, 272, 800, 462, 461, 817, 471, 270, 484, 1001, 57, 598, 789, 898, 296, 111, 745, 0, 1033, 1207, 928, 960, 691, 155, 751, 1179, 480, 280, 765, 892, 988, 143, 760, 27, 257, 623, 415, 571, 759, 512, 1137, 260, 85, 845, 332, 1013, 290, 967, 936, 434, 92, 73, 163, 981, 209, 874, 200, 169, 633, 1060, 381, 1005, 870, 1125, 112, 1170, 575, 353, 267, 1019, 966, 581, 1071, 601, 1208, 394, 91, 284, 235, 564, 566, 109, 615, 517, 59, 935, 945, 67, 553, 241, 144, 86, 399, 891, 951, 794, 631, 446, 665, 747, 1136, 1008, 214, 96, 879, 531, 1176, 65, 861, 343, 1063, 705, 651, 470, 199, 807, 1204, 648, 703, 735, 604, 1142, 832, 1182, 557, 1009, 281, 240, 423, 1040, 75, 586, 146, 806, 1171, 1077, 942, 1206, 138, 226, 526, 659, 369, 513, 74, 191, 829, 1087, 275, 310, 288, 33, 450, 923, 140, 764, 172, 68, 713, 905, 1100, 204, 1165, 345, 740, 1168, 1043, 283, 315, 654, 768, 1022, 610, 636, 98, 408, 356, 704, 1111, 263, 176, 731, 305, 1138, 1185, 118, 250, 180, 282, 993, 231, 414, 602, 1112, 363, 527, 1025, 417, 1173, 136, 707, 536, 7, 574, 77, 828, 763, 727, 664, 173, 524, 548, 1030, 588, 627, 359, 500, 420, 708, 812, 1076, 529, 672, 609, 351, 826, 313, 1101, 501, 3, 94, 1080, 622, 624, 370, 555, 833, 1054, 559, 1120, 743, 538, 325, 979, 503, 673, 734, 679, 720, 416, 818, 1197, 1049, 306, 278, 900, 1031, 105, 1196, 12, 723, 1045, 1067, 147, 1135, 456, 1144, 576, 154, 925, 1194, 449, 1029, 699, 444, 439, 836, 285, 88, 475, 895, 358, 312, 1047, 532, 448, 165, 695, 437, 28, 1081, 167, 1175, 926, 302, 614, 373, 309, 944, 663, 406, 1057, 769, 1159, 1119, 158, 1061, 596, 357, 606, 84, 101, 889, 189, 902, 330, 887, 550, 783, 469, 238, 866, 478, 591, 1053, 781, 715, 1188, 488, 824, 545, 367, 505, 124, 1011, 805, 431, 953, 957, 36, 334, 934, 401, 31, 918, 304, 22, 60, 911, 428, 706, 220, 752, 799, 652, 410, 675, 780, 477, 634, 684, 1210, 499, 277, 677, 386, 223, 116, 599, 1056, 71, 398, 595, 292, 681, 459, 729, 987, 1211, 438, 298, 16, 593, 649, 690, 774, 375, 295, 251, 683, 1183, 336, 790, 661, 541, 823, 932, 9, 168, 871, 660, 830, 657, 992, 166, 995, 894, 585, 1152, 202, 961, 736, 1078, 583, 1038, 496, 253, 1213, 778, 1114, 842, 958, 540, 185, 920, 733, 853, 716, 430, 377, 927, 498, 1158, 229, 638, 1039, 262, 205, 872, 25, 646, 605, 904, 122, 1017, 1015, 1126, 479, 87, 268, 996, 1190, 940, 753, 1065, 17, 181, 1150, 162, 827, 8, 956, 487, 955, 846, 975, 1055, 507, 775, 852, 873, 875, 380, 1091, 483, 748, 676, 316, 170, 107, 490, 320, 347, 49, 970, 106, 1201, 939, 693, 198, 201, 102, 153, 986, 1122, 35, 938, 946, 442, 132, 520, 447, 482, 160, 6, 1161, 1050, 1205, 256, 2, 620, 1146, 1032, 710, 392, 219, 899, 188, 1162, 1155, 372, 329, 289, 782, 1156, 230, 851, 948, 350, 810, 249, 982, 276, 78, 287, 1092, 711, 177, 1151, 802, 90, 949, 897, 534, 39, 815, 754, 222, 142, 964, 913, 542, 1010, 809, 404, 1046, 34, 464, 582, 959, 441, 696, 37, 384, 670, 840, 215, 1042, 83, 454, 1004, 884, 402, 93, 1064, 243, 1209, 164, 1006, 893, 771, 839, 1200, 687, 376, 18, 796, 228, 917, 561, 426, 216, 886, 21, 121, 246, 1000, 385, 387, 750, 429, 916, 762, 252, 883, 23, 63, 340, 515, 171, 433, 971, 862, 42, 348, 811]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5633655587527450
the save name prefix for this run is:  chkpt-ID_5633655587527450_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 854
rank avg (pred): 0.513 +- 0.005
mrr vals (pred, true): 0.014, 0.091
batch losses (mrrl, rdl): 0.0, 0.0012146061

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1095
rank avg (pred): 0.358 +- 0.240
mrr vals (pred, true): 0.183, 0.150
batch losses (mrrl, rdl): 0.0, 0.0005441192

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 958
rank avg (pred): 0.388 +- 0.268
mrr vals (pred, true): 0.234, 0.041
batch losses (mrrl, rdl): 0.0, 2.77415e-05

Epoch over!
epoch time: 12.139

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 431
rank avg (pred): 0.324 +- 0.256
mrr vals (pred, true): 0.290, 0.041
batch losses (mrrl, rdl): 0.0, 0.0001505179

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 789
rank avg (pred): 0.326 +- 0.253
mrr vals (pred, true): 0.277, 0.039
batch losses (mrrl, rdl): 0.0, 0.000192332

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 813
rank avg (pred): 0.045 +- 0.039
mrr vals (pred, true): 0.434, 0.528
batch losses (mrrl, rdl): 0.0, 2.05e-06

Epoch over!
epoch time: 12.098

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 521
rank avg (pred): 0.263 +- 0.227
mrr vals (pred, true): 0.336, 0.119
batch losses (mrrl, rdl): 0.0, 1.1727e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 207
rank avg (pred): 0.356 +- 0.257
mrr vals (pred, true): 0.246, 0.043
batch losses (mrrl, rdl): 0.0, 4.33023e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 826
rank avg (pred): 0.030 +- 0.027
mrr vals (pred, true): 0.482, 0.561
batch losses (mrrl, rdl): 0.0, 2.929e-07

Epoch over!
epoch time: 12.039

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1072
rank avg (pred): 0.019 +- 0.016
mrr vals (pred, true): 0.522, 0.577
batch losses (mrrl, rdl): 0.0, 2.484e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1066
rank avg (pred): 0.022 +- 0.020
mrr vals (pred, true): 0.513, 0.609
batch losses (mrrl, rdl): 0.0, 2.98e-08

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1045
rank avg (pred): 0.337 +- 0.255
mrr vals (pred, true): 0.256, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001095623

Epoch over!
epoch time: 11.917

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 599
rank avg (pred): 0.390 +- 0.269
mrr vals (pred, true): 0.220, 0.044
batch losses (mrrl, rdl): 0.0, 6.1316e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 725
rank avg (pred): 0.418 +- 0.272
mrr vals (pred, true): 0.193, 0.037
batch losses (mrrl, rdl): 0.0, 5.6552e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 803
rank avg (pred): 0.313 +- 0.262
mrr vals (pred, true): 0.328, 0.036
batch losses (mrrl, rdl): 0.0, 0.0002865044

Epoch over!
epoch time: 11.953

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 567
rank avg (pred): 0.408 +- 0.264
mrr vals (pred, true): 0.210, 0.030
batch losses (mrrl, rdl): 0.2575423419, 1.48155e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 152
rank avg (pred): 0.384 +- 0.149
mrr vals (pred, true): 0.070, 0.109
batch losses (mrrl, rdl): 0.0154529894, 0.0004006354

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 535
rank avg (pred): 0.341 +- 0.169
mrr vals (pred, true): 0.090, 0.072
batch losses (mrrl, rdl): 0.0156099834, 1.93047e-05

Epoch over!
epoch time: 12.297

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 587
rank avg (pred): 0.420 +- 0.114
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.001126986, 5.72744e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 623
rank avg (pred): 0.413 +- 0.117
mrr vals (pred, true): 0.040, 0.040
batch losses (mrrl, rdl): 0.0009801376, 6.3662e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 636
rank avg (pred): 0.418 +- 0.110
mrr vals (pred, true): 0.037, 0.035
batch losses (mrrl, rdl): 0.0016324157, 2.94906e-05

Epoch over!
epoch time: 12.176

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 523
rank avg (pred): 0.321 +- 0.175
mrr vals (pred, true): 0.093, 0.072
batch losses (mrrl, rdl): 0.0181420799, 3.62096e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 745
rank avg (pred): 0.026 +- 0.023
mrr vals (pred, true): 0.473, 0.584
batch losses (mrrl, rdl): 0.1218493357, 5.81e-08

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 417
rank avg (pred): 0.353 +- 0.149
mrr vals (pred, true): 0.059, 0.039
batch losses (mrrl, rdl): 0.0007674206, 0.000174619

Epoch over!
epoch time: 12.148

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 263
rank avg (pred): 0.010 +- 0.010
mrr vals (pred, true): 0.607, 0.622
batch losses (mrrl, rdl): 0.0024166959, 1.9222e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 503
rank avg (pred): 0.284 +- 0.185
mrr vals (pred, true): 0.117, 0.122
batch losses (mrrl, rdl): 0.0002339944, 2.90704e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 0
rank avg (pred): 0.316 +- 0.170
mrr vals (pred, true): 0.094, 0.491
batch losses (mrrl, rdl): 1.5704538822, 0.0017912486

Epoch over!
epoch time: 12.163

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 137
rank avg (pred): 0.310 +- 0.171
mrr vals (pred, true): 0.094, 0.093
batch losses (mrrl, rdl): 0.0190568585, 5.39998e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 576
rank avg (pred): 0.406 +- 0.106
mrr vals (pred, true): 0.037, 0.036
batch losses (mrrl, rdl): 0.0017443458, 3.76739e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 641
rank avg (pred): 0.380 +- 0.125
mrr vals (pred, true): 0.047, 0.036
batch losses (mrrl, rdl): 8.56365e-05, 0.0001575324

Epoch over!
epoch time: 12.068

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1009
rank avg (pred): 0.302 +- 0.170
mrr vals (pred, true): 0.104, 0.117
batch losses (mrrl, rdl): 0.0016744912, 7.19349e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 190
rank avg (pred): 0.342 +- 0.145
mrr vals (pred, true): 0.057, 0.043
batch losses (mrrl, rdl): 0.0005551032, 0.0001970214

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1104
rank avg (pred): 0.323 +- 0.155
mrr vals (pred, true): 0.079, 0.139
batch losses (mrrl, rdl): 0.0364371911, 0.0001958569

Epoch over!
epoch time: 12.363

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 312
rank avg (pred): 0.029 +- 0.029
mrr vals (pred, true): 0.525, 0.523
batch losses (mrrl, rdl): 6.73468e-05, 8.7e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 809
rank avg (pred): 0.417 +- 0.185
mrr vals (pred, true): 0.071, 0.046
batch losses (mrrl, rdl): 0.0045863218, 2.75619e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 483
rank avg (pred): 0.327 +- 0.149
mrr vals (pred, true): 0.073, 0.039
batch losses (mrrl, rdl): 0.0053013153, 0.0003127188

Epoch over!
epoch time: 12.193

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 31
rank avg (pred): 0.024 +- 0.025
mrr vals (pred, true): 0.549, 0.571
batch losses (mrrl, rdl): 0.0045705372, 2.327e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1057
rank avg (pred): 0.014 +- 0.015
mrr vals (pred, true): 0.595, 0.617
batch losses (mrrl, rdl): 0.0047290321, 7.723e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 134
rank avg (pred): 0.308 +- 0.153
mrr vals (pred, true): 0.073, 0.129
batch losses (mrrl, rdl): 0.0321056768, 0.0001723384

Epoch over!
epoch time: 12.19

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 561
rank avg (pred): 0.302 +- 0.156
mrr vals (pred, true): 0.076, 0.064
batch losses (mrrl, rdl): 0.0069189509, 0.0001014943

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 236
rank avg (pred): 0.301 +- 0.157
mrr vals (pred, true): 0.078, 0.036
batch losses (mrrl, rdl): 0.007613102, 0.0003628334

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1202
rank avg (pred): 0.356 +- 0.125
mrr vals (pred, true): 0.052, 0.037
batch losses (mrrl, rdl): 5.71175e-05, 0.0001957988

Epoch over!
epoch time: 12.291

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1030
rank avg (pred): 0.338 +- 0.137
mrr vals (pred, true): 0.065, 0.043
batch losses (mrrl, rdl): 0.0022704187, 0.00025826

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 168
rank avg (pred): 0.326 +- 0.142
mrr vals (pred, true): 0.066, 0.041
batch losses (mrrl, rdl): 0.0027175702, 0.0002872542

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 923
rank avg (pred): 0.380 +- 0.107
mrr vals (pred, true): 0.038, 0.090
batch losses (mrrl, rdl): 0.0015186541, 0.0002513347

Epoch over!
epoch time: 11.996

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.347 +- 0.125
mrr vals (pred, true): 0.048, 0.046

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.03748 	 0.03226 	 ~...
    4 	     1 	 0.03747 	 0.03230 	 ~...
   17 	     2 	 0.03882 	 0.03261 	 ~...
    8 	     3 	 0.03788 	 0.03449 	 ~...
    6 	     4 	 0.03749 	 0.03475 	 ~...
   61 	     5 	 0.06330 	 0.03488 	 ~...
   28 	     6 	 0.04124 	 0.03530 	 ~...
   33 	     7 	 0.04460 	 0.03551 	 ~...
   78 	     8 	 0.07129 	 0.03604 	 m..s
   75 	     9 	 0.06988 	 0.03643 	 m..s
   55 	    10 	 0.05783 	 0.03674 	 ~...
   83 	    11 	 0.07914 	 0.03691 	 m..s
   21 	    12 	 0.03963 	 0.03704 	 ~...
   11 	    13 	 0.03813 	 0.03754 	 ~...
   14 	    14 	 0.03824 	 0.03759 	 ~...
   12 	    15 	 0.03815 	 0.03766 	 ~...
   57 	    16 	 0.05852 	 0.03779 	 ~...
   76 	    17 	 0.06992 	 0.03795 	 m..s
   31 	    18 	 0.04410 	 0.03827 	 ~...
    2 	    19 	 0.03729 	 0.03829 	 ~...
   25 	    20 	 0.03993 	 0.03845 	 ~...
    3 	    21 	 0.03743 	 0.03848 	 ~...
   51 	    22 	 0.05392 	 0.03904 	 ~...
   82 	    23 	 0.07908 	 0.03922 	 m..s
   13 	    24 	 0.03819 	 0.03954 	 ~...
   60 	    25 	 0.06319 	 0.03962 	 ~...
   48 	    26 	 0.05277 	 0.03966 	 ~...
   26 	    27 	 0.03995 	 0.03972 	 ~...
    9 	    28 	 0.03802 	 0.03977 	 ~...
   15 	    29 	 0.03846 	 0.03982 	 ~...
   52 	    30 	 0.05412 	 0.04008 	 ~...
   41 	    31 	 0.04899 	 0.04027 	 ~...
   20 	    32 	 0.03959 	 0.04041 	 ~...
   62 	    33 	 0.06356 	 0.04049 	 ~...
    0 	    34 	 0.03693 	 0.04063 	 ~...
   72 	    35 	 0.06870 	 0.04116 	 ~...
   27 	    36 	 0.04120 	 0.04153 	 ~...
   36 	    37 	 0.04670 	 0.04172 	 ~...
   18 	    38 	 0.03887 	 0.04191 	 ~...
   53 	    39 	 0.05413 	 0.04191 	 ~...
   24 	    40 	 0.03992 	 0.04210 	 ~...
   22 	    41 	 0.03974 	 0.04230 	 ~...
   29 	    42 	 0.04147 	 0.04243 	 ~...
   86 	    43 	 0.09254 	 0.04262 	 m..s
   19 	    44 	 0.03957 	 0.04397 	 ~...
    7 	    45 	 0.03761 	 0.04431 	 ~...
   10 	    46 	 0.03810 	 0.04435 	 ~...
   44 	    47 	 0.05047 	 0.04465 	 ~...
   16 	    48 	 0.03874 	 0.04554 	 ~...
   40 	    49 	 0.04820 	 0.04609 	 ~...
   49 	    50 	 0.05283 	 0.04648 	 ~...
   23 	    51 	 0.03991 	 0.04685 	 ~...
   70 	    52 	 0.06784 	 0.06484 	 ~...
   81 	    53 	 0.07360 	 0.07245 	 ~...
   38 	    54 	 0.04724 	 0.07328 	 ~...
   46 	    55 	 0.05176 	 0.08032 	 ~...
   42 	    56 	 0.04909 	 0.08267 	 m..s
   37 	    57 	 0.04692 	 0.08469 	 m..s
   30 	    58 	 0.04306 	 0.08644 	 m..s
   39 	    59 	 0.04817 	 0.08775 	 m..s
   34 	    60 	 0.04471 	 0.08899 	 m..s
    1 	    61 	 0.03721 	 0.09072 	 m..s
   45 	    62 	 0.05059 	 0.09154 	 m..s
   43 	    63 	 0.04954 	 0.09173 	 m..s
   35 	    64 	 0.04644 	 0.09245 	 m..s
   32 	    65 	 0.04453 	 0.09247 	 m..s
   47 	    66 	 0.05221 	 0.09890 	 m..s
   50 	    67 	 0.05369 	 0.10161 	 m..s
   58 	    68 	 0.05871 	 0.10863 	 m..s
   56 	    69 	 0.05816 	 0.11149 	 m..s
   66 	    70 	 0.06582 	 0.11400 	 m..s
   59 	    71 	 0.05969 	 0.11483 	 m..s
   69 	    72 	 0.06646 	 0.11767 	 m..s
   64 	    73 	 0.06491 	 0.11853 	 m..s
   65 	    74 	 0.06578 	 0.11998 	 m..s
   77 	    75 	 0.07090 	 0.12127 	 m..s
   74 	    76 	 0.06950 	 0.12310 	 m..s
   79 	    77 	 0.07166 	 0.12398 	 m..s
   63 	    78 	 0.06464 	 0.12606 	 m..s
   54 	    79 	 0.05766 	 0.12642 	 m..s
   71 	    80 	 0.06868 	 0.12917 	 m..s
   73 	    81 	 0.06945 	 0.13144 	 m..s
   67 	    82 	 0.06587 	 0.13366 	 m..s
   85 	    83 	 0.08302 	 0.13503 	 m..s
   84 	    84 	 0.08234 	 0.14282 	 m..s
   68 	    85 	 0.06616 	 0.14833 	 m..s
   80 	    86 	 0.07294 	 0.14973 	 m..s
   87 	    87 	 0.13989 	 0.15500 	 ~...
   88 	    88 	 0.15175 	 0.16705 	 ~...
   89 	    89 	 0.18567 	 0.17167 	 ~...
   91 	    90 	 0.33947 	 0.30109 	 m..s
   90 	    91 	 0.32449 	 0.32864 	 ~...
   93 	    92 	 0.51059 	 0.42109 	 m..s
   96 	    93 	 0.53101 	 0.50303 	 ~...
   98 	    94 	 0.53589 	 0.50669 	 ~...
   94 	    95 	 0.52058 	 0.50875 	 ~...
   92 	    96 	 0.46657 	 0.51269 	 m..s
   95 	    97 	 0.52907 	 0.52124 	 ~...
  103 	    98 	 0.55695 	 0.54085 	 ~...
   97 	    99 	 0.53118 	 0.54273 	 ~...
  105 	   100 	 0.56156 	 0.54371 	 ~...
  107 	   101 	 0.57518 	 0.54532 	 ~...
  102 	   102 	 0.55420 	 0.54533 	 ~...
  104 	   103 	 0.55853 	 0.54688 	 ~...
  106 	   104 	 0.57246 	 0.55065 	 ~...
  101 	   105 	 0.54629 	 0.56633 	 ~...
   99 	   106 	 0.54031 	 0.57618 	 m..s
  100 	   107 	 0.54527 	 0.59640 	 m..s
  119 	   108 	 0.62414 	 0.60053 	 ~...
  116 	   109 	 0.61834 	 0.60102 	 ~...
  118 	   110 	 0.61999 	 0.60653 	 ~...
  111 	   111 	 0.60556 	 0.60969 	 ~...
  120 	   112 	 0.63096 	 0.61250 	 ~...
  109 	   113 	 0.58998 	 0.61320 	 ~...
  114 	   114 	 0.60864 	 0.61471 	 ~...
  115 	   115 	 0.61033 	 0.61698 	 ~...
  117 	   116 	 0.61845 	 0.61885 	 ~...
  112 	   117 	 0.60805 	 0.61930 	 ~...
  110 	   118 	 0.60182 	 0.62217 	 ~...
  113 	   119 	 0.60808 	 0.62322 	 ~...
  108 	   120 	 0.58887 	 0.62441 	 m..s
==========================================
r_mrr = 0.9899812340736389
r2_mrr = 0.9771274924278259
spearmanr_mrr@5 = 0.8790969252586365
spearmanr_mrr@10 = 0.9514257311820984
spearmanr_mrr@50 = 0.9967561364173889
spearmanr_mrr@100 = 0.9951246976852417
spearmanr_mrr@All = 0.9950872659683228
==========================================
test time: 0.398
Done Testing dataset UMLS
total time taken: 188.6397316455841
training time taken: 182.50405025482178
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9900)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9771)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.8791)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9514)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9968)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9951)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9951)}}, 'test_loss': {'TransE': {'UMLS': 1.0670447554336988}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 7647586144446678
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [319, 684, 458, 1028, 95, 1030, 1005, 22, 1040, 1147, 16, 308, 1107, 423, 126, 622, 554, 252, 232, 1045, 69, 711, 1070, 912, 142, 107, 973, 8, 607, 598, 330, 1162, 256, 1037, 457, 546, 400, 902, 1150, 1098, 564, 93, 415, 112, 395, 533, 119, 923, 933, 736, 505, 679, 692, 1009, 310, 41, 705, 236, 231, 167, 1092, 634, 191, 511, 47, 893, 435, 671, 766, 914, 71, 1058, 83, 1158, 78, 490, 515, 99, 619, 212, 396, 344, 296, 991, 335, 154, 321, 331, 111, 324, 816, 827, 203, 524, 224, 984, 73, 101, 237, 1197, 626, 757, 136, 1206, 25, 1117, 482, 275, 980, 307, 1011, 161, 820, 683, 102, 386, 559, 206, 1075, 710, 541]
valid_ids (0): []
train_ids (1094): [558, 333, 76, 149, 156, 105, 834, 693, 18, 433, 160, 447, 108, 585, 517, 936, 573, 1026, 556, 943, 650, 158, 449, 181, 843, 1074, 407, 441, 771, 70, 399, 787, 743, 15, 942, 814, 786, 1202, 502, 891, 632, 989, 1211, 828, 775, 680, 378, 38, 34, 141, 312, 687, 143, 548, 470, 450, 273, 1101, 1160, 1193, 175, 864, 261, 796, 493, 589, 593, 96, 553, 561, 724, 235, 1120, 139, 819, 313, 244, 993, 213, 185, 479, 188, 210, 1081, 487, 475, 536, 1064, 354, 516, 1049, 216, 389, 337, 653, 91, 674, 401, 217, 588, 369, 1130, 600, 468, 255, 1133, 483, 165, 956, 1114, 1212, 825, 714, 940, 60, 663, 254, 1099, 418, 532, 609, 972, 403, 350, 295, 186, 159, 978, 822, 1207, 1125, 57, 197, 253, 114, 329, 730, 380, 950, 865, 596, 1127, 886, 790, 769, 464, 897, 514, 473, 1199, 494, 570, 1189, 788, 123, 264, 540, 268, 484, 701, 1059, 961, 1149, 1001, 1029, 753, 385, 336, 677, 397, 987, 339, 1181, 463, 910, 768, 669, 260, 737, 641, 767, 1104, 826, 416, 909, 813, 522, 859, 187, 1177, 945, 409, 408, 1185, 249, 1140, 151, 537, 219, 200, 1085, 388, 660, 127, 162, 193, 86, 633, 98, 1078, 997, 215, 202, 1032, 963, 722, 1024, 690, 341, 81, 300, 667, 462, 839, 207, 610, 440, 670, 853, 452, 53, 841, 976, 420, 673, 563, 751, 1126, 240, 872, 284, 625, 120, 469, 225, 259, 1128, 686, 172, 949, 64, 643, 856, 205, 754, 325, 14, 1010, 930, 873, 177, 1000, 527, 958, 583, 580, 779, 1144, 921, 1068, 1113, 1034, 12, 960, 1191, 829, 17, 509, 990, 497, 1111, 1093, 662, 520, 474, 620, 798, 461, 1018, 1088, 824, 1151, 343, 994, 602, 1129, 655, 118, 306, 547, 274, 176, 1155, 729, 1036, 979, 866, 691, 831, 1091, 752, 265, 241, 145, 568, 749, 735, 276, 289, 40, 1015, 529, 1062, 196, 1124, 1006, 982, 731, 353, 180, 364, 964, 326, 62, 672, 640, 292, 1135, 271, 879, 748, 209, 135, 682, 318, 806, 1143, 944, 624, 234, 122, 981, 565, 460, 444, 1119, 830, 821, 1132, 305, 900, 1056, 179, 361, 1080, 850, 1198, 572, 780, 718, 642, 807, 290, 1072, 345, 1071, 1063, 1201, 616, 1131, 746, 1066, 72, 323, 1173, 639, 1067, 715, 957, 405, 346, 815, 838, 507, 519, 1213, 594, 974, 478, 192, 293, 489, 613, 500, 46, 608, 906, 774, 28, 849, 1031, 442, 518, 379, 723, 499, 611, 184, 538, 1115, 720, 257, 1214, 1174, 525, 229, 294, 419, 1209, 629, 1204, 471, 654, 157, 999, 392, 592, 628, 742, 542, 847, 621, 694, 531, 0, 685, 155, 125, 869, 835, 799, 501, 1152, 7, 727, 576, 656, 51, 472, 975, 131, 837, 85, 201, 148, 510, 63, 851, 584, 248, 368, 1178, 917, 696, 373, 436, 246, 54, 706, 812, 486, 1087, 476, 760, 1208, 699, 808, 223, 377, 934, 1044, 163, 817, 803, 492, 281, 645, 1042, 48, 84, 567, 56, 1021, 169, 466, 668, 21, 648, 44, 1002, 427, 777, 347, 429, 1008, 394, 94, 174, 852, 1137, 283, 506, 431, 681, 1182, 55, 311, 1105, 907, 384, 1176, 2, 858, 1016, 50, 1136, 1186, 659, 716, 285, 922, 1096, 560, 366, 762, 646, 320, 863, 27, 908, 352, 291, 1188, 6, 954, 376, 132, 586, 267, 1153, 398, 459, 413, 1073, 709, 758, 1027, 183, 481, 355, 279, 10, 59, 810, 818, 414, 846, 359, 243, 637, 387, 890, 801, 348, 1138, 417, 1053, 445, 892, 1203, 959, 92, 889, 230, 712, 666, 759, 1134, 534, 1014, 1159, 363, 434, 1055, 342, 913, 896, 977, 166, 147, 1090, 103, 390, 623, 566, 877, 1079, 1004, 925, 178, 371, 194, 82, 948, 675, 597, 985, 535, 1164, 581, 951, 1175, 222, 1025, 1118, 928, 1020, 996, 1154, 446, 1, 65, 823, 190, 133, 947, 938, 443, 233, 832, 875, 734, 728, 198, 406, 881, 916, 932, 1168, 258, 905, 657, 772, 1065, 303, 49, 739, 130, 732, 848, 713, 1035, 338, 704, 761, 884, 37, 995, 842, 860, 871, 513, 455, 612, 770, 1196, 432, 1041, 695, 883, 1165, 356, 404, 496, 1192, 1112, 1157, 1169, 569, 1180, 1033, 631, 488, 1046, 375, 526, 778, 150, 39, 508, 880, 530, 1103, 327, 349, 117, 854, 789, 170, 903, 270, 870, 523, 763, 658, 430, 491, 317, 575, 204, 664, 740, 1003, 700, 134, 1048, 747, 811, 1060, 374, 726, 451, 595, 89, 1051, 1210, 689, 391, 577, 765, 968, 795, 1183, 1187, 967, 110, 334, 247, 652, 782, 887, 926, 599, 1146, 571, 986, 485, 314, 635, 227, 426, 358, 717, 1116, 868, 935, 512, 545, 124, 802, 164, 195, 745, 833, 1122, 1095, 189, 1200, 80, 638, 800, 1023, 1022, 61, 617, 661, 467, 792, 214, 1145, 878, 58, 360, 1148, 939, 857, 402, 1156, 263, 1057, 885, 220, 773, 9, 970, 721, 272, 836, 702, 946, 480, 1076, 182, 603, 410, 888, 703, 750, 199, 1061, 895, 797, 1190, 100, 168, 297, 738, 42, 121, 931, 755, 1007, 1094, 579, 1121, 362, 67, 1054, 678, 1179, 302, 971, 4, 1194, 282, 146, 676, 30, 152, 19, 109, 615, 1038, 1017, 315, 1109, 744, 924, 1097, 601, 74, 874, 919, 250, 340, 647, 1106, 299, 733, 955, 791, 1139, 113, 920, 1163, 862, 381, 707, 756, 1195, 77, 636, 1013, 221, 899, 741, 228, 277, 90, 578, 266, 173, 614, 36, 644, 465, 75, 962, 861, 918, 1052, 1161, 421, 698, 88, 901, 504, 697, 137, 911, 555, 298, 651, 262, 1166, 218, 544, 528, 665, 370, 411, 590, 367, 280, 840, 992, 1110, 618, 129, 211, 915, 688, 781, 1123, 288, 33, 138, 498, 153, 562, 587, 453, 725, 550, 238, 785, 966, 793, 591, 425, 630, 383, 13, 68, 844, 927, 5, 23, 549, 97, 26, 539, 316, 965, 328, 32, 867, 1089, 357, 301, 438, 29, 454, 552, 304, 941, 1043, 87, 3, 24, 439, 437, 627, 422, 606, 1083, 983, 31, 845, 1184, 226, 1141, 1019, 882, 937, 809, 1172, 495, 128, 898, 708, 322, 1084, 557, 1039, 428, 245, 1167, 988, 1012, 242, 365, 805, 605, 1171, 719, 20, 351, 424, 43, 604, 521, 309, 794, 239, 649, 1050, 106, 764, 894, 783, 45, 116, 52, 477, 551, 412, 35, 11, 208, 144, 503, 1077, 574, 287, 1086, 1069, 140, 448, 998, 904, 855, 1047, 1108, 79, 776, 876, 1205, 969, 382, 1100, 66, 332, 1142, 582, 1082, 1170, 953, 804, 115, 543, 104, 952, 286, 171, 251, 929, 1102, 278, 456, 269, 372, 393, 784]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3661426538552693
the save name prefix for this run is:  chkpt-ID_3661426538552693_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 18
rank avg (pred): 0.549 +- 0.004
mrr vals (pred, true): 0.013, 0.474
batch losses (mrrl, rdl): 0.0, 0.0057523292

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 604
rank avg (pred): 0.433 +- 0.232
mrr vals (pred, true): 0.120, 0.037
batch losses (mrrl, rdl): 0.0, 1.02719e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 737
rank avg (pred): 0.022 +- 0.017
mrr vals (pred, true): 0.468, 0.365
batch losses (mrrl, rdl): 0.0, 1.61942e-05

Epoch over!
epoch time: 12.023

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1210
rank avg (pred): 0.380 +- 0.266
mrr vals (pred, true): 0.246, 0.038
batch losses (mrrl, rdl): 0.0, 4.12181e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 226
rank avg (pred): 0.337 +- 0.244
mrr vals (pred, true): 0.258, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001081189

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 847
rank avg (pred): 0.341 +- 0.255
mrr vals (pred, true): 0.273, 0.087
batch losses (mrrl, rdl): 0.0, 0.0001282508

Epoch over!
epoch time: 11.885

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 627
rank avg (pred): 0.394 +- 0.279
mrr vals (pred, true): 0.251, 0.036
batch losses (mrrl, rdl): 0.0, 1.21872e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 289
rank avg (pred): 0.024 +- 0.020
mrr vals (pred, true): 0.488, 0.562
batch losses (mrrl, rdl): 0.0, 6.36e-08

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 815
rank avg (pred): 0.015 +- 0.012
mrr vals (pred, true): 0.532, 0.532
batch losses (mrrl, rdl): 0.0, 9.9375e-06

Epoch over!
epoch time: 11.905

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 785
rank avg (pred): 0.337 +- 0.254
mrr vals (pred, true): 0.254, 0.044
batch losses (mrrl, rdl): 0.0, 4.47042e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 488
rank avg (pred): 0.329 +- 0.243
mrr vals (pred, true): 0.243, 0.112
batch losses (mrrl, rdl): 0.0, 8.99722e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 596
rank avg (pred): 0.442 +- 0.260
mrr vals (pred, true): 0.158, 0.037
batch losses (mrrl, rdl): 0.0, 5.2864e-06

Epoch over!
epoch time: 12.104

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 889
rank avg (pred): 0.334 +- 0.260
mrr vals (pred, true): 0.275, 0.039
batch losses (mrrl, rdl): 0.0, 0.000141936

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1080
rank avg (pred): 0.310 +- 0.258
mrr vals (pred, true): 0.321, 0.110
batch losses (mrrl, rdl): 0.0, 8.2017e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1213
rank avg (pred): 0.369 +- 0.266
mrr vals (pred, true): 0.247, 0.039
batch losses (mrrl, rdl): 0.0, 3.64075e-05

Epoch over!
epoch time: 11.821

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 531
rank avg (pred): 0.329 +- 0.248
mrr vals (pred, true): 0.252, 0.067
batch losses (mrrl, rdl): 0.4067030847, 4.5906e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 403
rank avg (pred): 0.399 +- 0.222
mrr vals (pred, true): 0.089, 0.116
batch losses (mrrl, rdl): 0.0075646285, 0.000664785

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 81
rank avg (pred): 0.430 +- 0.216
mrr vals (pred, true): 0.073, 0.089
batch losses (mrrl, rdl): 0.0051936014, 0.000595743

Epoch over!
epoch time: 12.18

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 756
rank avg (pred): 0.421 +- 0.221
mrr vals (pred, true): 0.078, 0.091
batch losses (mrrl, rdl): 0.0076549198, 0.0004793207

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 263
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.602, 0.622
batch losses (mrrl, rdl): 0.0041127373, 3.3177e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1209
rank avg (pred): 0.555 +- 0.140
mrr vals (pred, true): 0.023, 0.039
batch losses (mrrl, rdl): 0.0070269383, 0.0003214687

Epoch over!
epoch time: 12.106

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 811
rank avg (pred): 0.020 +- 0.015
mrr vals (pred, true): 0.459, 0.430
batch losses (mrrl, rdl): 0.0083873104, 2.66939e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1026
rank avg (pred): 0.360 +- 0.208
mrr vals (pred, true): 0.091, 0.039
batch losses (mrrl, rdl): 0.0169078615, 0.0001026884

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 91
rank avg (pred): 0.396 +- 0.195
mrr vals (pred, true): 0.066, 0.102
batch losses (mrrl, rdl): 0.01281262, 0.0005334678

Epoch over!
epoch time: 12.319

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 286
rank avg (pred): 0.012 +- 0.010
mrr vals (pred, true): 0.532, 0.575
batch losses (mrrl, rdl): 0.0185102895, 3.3353e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 169
rank avg (pred): 0.366 +- 0.190
mrr vals (pred, true): 0.072, 0.043
batch losses (mrrl, rdl): 0.0050419774, 6.51437e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 578
rank avg (pred): 0.542 +- 0.141
mrr vals (pred, true): 0.025, 0.041
batch losses (mrrl, rdl): 0.0060821855, 0.0002923141

Epoch over!
epoch time: 12.166

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1208
rank avg (pred): 0.436 +- 0.152
mrr vals (pred, true): 0.053, 0.035
batch losses (mrrl, rdl): 8.87577e-05, 3.705e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 82
rank avg (pred): 0.384 +- 0.179
mrr vals (pred, true): 0.055, 0.086
batch losses (mrrl, rdl): 0.00025661, 0.0002694828

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 390
rank avg (pred): 0.422 +- 0.210
mrr vals (pred, true): 0.064, 0.098
batch losses (mrrl, rdl): 0.0018931967, 0.0006627416

Epoch over!
epoch time: 12.323

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1194
rank avg (pred): 0.467 +- 0.132
mrr vals (pred, true): 0.040, 0.040
batch losses (mrrl, rdl): 0.0009884415, 7.17806e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 403
rank avg (pred): 0.428 +- 0.196
mrr vals (pred, true): 0.053, 0.116
batch losses (mrrl, rdl): 0.0398686156, 0.0008455215

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 176
rank avg (pred): 0.276 +- 0.158
mrr vals (pred, true): 0.088, 0.038
batch losses (mrrl, rdl): 0.0147783067, 0.0005329798

Epoch over!
epoch time: 12.419

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 72
rank avg (pred): 0.018 +- 0.015
mrr vals (pred, true): 0.486, 0.521
batch losses (mrrl, rdl): 0.0121795246, 3.3734e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 669
rank avg (pred): 0.511 +- 0.137
mrr vals (pred, true): 0.029, 0.041
batch losses (mrrl, rdl): 0.0045427084, 0.0001293751

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 248
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.647, 0.628
batch losses (mrrl, rdl): 0.0036428007, 4.3982e-06

Epoch over!
epoch time: 12.241

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 367
rank avg (pred): 0.423 +- 0.209
mrr vals (pred, true): 0.065, 0.109
batch losses (mrrl, rdl): 0.0188343115, 0.0006335761

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 377
rank avg (pred): 0.288 +- 0.164
mrr vals (pred, true): 0.080, 0.149
batch losses (mrrl, rdl): 0.0474578962, 0.0001207451

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 587
rank avg (pred): 0.461 +- 0.122
mrr vals (pred, true): 0.039, 0.047
batch losses (mrrl, rdl): 0.001107662, 6.49057e-05

Epoch over!
epoch time: 12.062

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 627
rank avg (pred): 0.435 +- 0.113
mrr vals (pred, true): 0.054, 0.036
batch losses (mrrl, rdl): 0.0001994713, 3.51918e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 664
rank avg (pred): 0.473 +- 0.116
mrr vals (pred, true): 0.035, 0.040
batch losses (mrrl, rdl): 0.0022347244, 8.14001e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 927
rank avg (pred): 0.461 +- 0.167
mrr vals (pred, true): 0.050, 0.092
batch losses (mrrl, rdl): 3.76e-08, 0.0007041874

Epoch over!
epoch time: 12.137

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1006
rank avg (pred): 0.458 +- 0.201
mrr vals (pred, true): 0.052, 0.145
batch losses (mrrl, rdl): 0.0853624269, 0.0010314977

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 745
rank avg (pred): 0.014 +- 0.012
mrr vals (pred, true): 0.553, 0.584
batch losses (mrrl, rdl): 0.0096008852, 3.0454e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 146
rank avg (pred): 0.304 +- 0.185
mrr vals (pred, true): 0.084, 0.110
batch losses (mrrl, rdl): 0.0065575191, 8.46068e-05

Epoch over!
epoch time: 12.369

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.014 +- 0.013
mrr vals (pred, true): 0.550, 0.562

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   49 	     0 	 0.08438 	 0.03488 	 m..s
   11 	     1 	 0.04961 	 0.03538 	 ~...
    2 	     2 	 0.04630 	 0.03572 	 ~...
   73 	     3 	 0.10121 	 0.03589 	 m..s
   10 	     4 	 0.04960 	 0.03613 	 ~...
   69 	     5 	 0.09847 	 0.03614 	 m..s
   14 	     6 	 0.05355 	 0.03617 	 ~...
   41 	     7 	 0.07487 	 0.03672 	 m..s
    8 	     8 	 0.04755 	 0.03736 	 ~...
    5 	     9 	 0.04742 	 0.03759 	 ~...
    9 	    10 	 0.04953 	 0.03768 	 ~...
    3 	    11 	 0.04686 	 0.03769 	 ~...
   26 	    12 	 0.06469 	 0.03794 	 ~...
   71 	    13 	 0.09912 	 0.03883 	 m..s
   52 	    14 	 0.08893 	 0.03888 	 m..s
   42 	    15 	 0.07528 	 0.03911 	 m..s
   15 	    16 	 0.05598 	 0.03915 	 ~...
   70 	    17 	 0.09861 	 0.03962 	 m..s
   77 	    18 	 0.10686 	 0.03963 	 m..s
    0 	    19 	 0.04627 	 0.03983 	 ~...
    6 	    20 	 0.04748 	 0.04000 	 ~...
   54 	    21 	 0.08962 	 0.04013 	 m..s
   26 	    22 	 0.06469 	 0.04041 	 ~...
   13 	    23 	 0.05032 	 0.04044 	 ~...
   68 	    24 	 0.09765 	 0.04050 	 m..s
   16 	    25 	 0.05600 	 0.04051 	 ~...
   76 	    26 	 0.10450 	 0.04052 	 m..s
    7 	    27 	 0.04750 	 0.04075 	 ~...
   65 	    28 	 0.09566 	 0.04078 	 m..s
    1 	    29 	 0.04628 	 0.04116 	 ~...
    4 	    30 	 0.04742 	 0.04141 	 ~...
   50 	    31 	 0.08480 	 0.04143 	 m..s
   19 	    32 	 0.06230 	 0.04190 	 ~...
   18 	    33 	 0.06223 	 0.04217 	 ~...
   64 	    34 	 0.09510 	 0.04227 	 m..s
   72 	    35 	 0.10037 	 0.04260 	 m..s
   67 	    36 	 0.09701 	 0.04445 	 m..s
   25 	    37 	 0.06418 	 0.04465 	 ~...
   66 	    38 	 0.09676 	 0.04576 	 m..s
   12 	    39 	 0.04980 	 0.04685 	 ~...
   35 	    40 	 0.07164 	 0.06286 	 ~...
   33 	    41 	 0.06655 	 0.06484 	 ~...
   59 	    42 	 0.09128 	 0.06737 	 ~...
   51 	    43 	 0.08509 	 0.06880 	 ~...
   34 	    44 	 0.06929 	 0.07102 	 ~...
   43 	    45 	 0.07739 	 0.07384 	 ~...
   20 	    46 	 0.06264 	 0.07572 	 ~...
   44 	    47 	 0.07814 	 0.08472 	 ~...
   17 	    48 	 0.06125 	 0.08739 	 ~...
   26 	    49 	 0.06469 	 0.08797 	 ~...
   24 	    50 	 0.06384 	 0.08812 	 ~...
   37 	    51 	 0.07227 	 0.08924 	 ~...
   32 	    52 	 0.06576 	 0.08956 	 ~...
   21 	    53 	 0.06274 	 0.09002 	 ~...
   30 	    54 	 0.06474 	 0.09155 	 ~...
   22 	    55 	 0.06350 	 0.09245 	 ~...
   36 	    56 	 0.07204 	 0.09360 	 ~...
   23 	    57 	 0.06360 	 0.09458 	 m..s
   39 	    58 	 0.07429 	 0.09493 	 ~...
   26 	    59 	 0.06469 	 0.09496 	 m..s
   38 	    60 	 0.07254 	 0.10115 	 ~...
   31 	    61 	 0.06519 	 0.10136 	 m..s
   40 	    62 	 0.07485 	 0.10182 	 ~...
   46 	    63 	 0.08000 	 0.10331 	 ~...
   48 	    64 	 0.08074 	 0.10900 	 ~...
   62 	    65 	 0.09246 	 0.11400 	 ~...
   60 	    66 	 0.09133 	 0.11433 	 ~...
   61 	    67 	 0.09210 	 0.11491 	 ~...
   63 	    68 	 0.09471 	 0.11646 	 ~...
   82 	    69 	 0.13774 	 0.11690 	 ~...
   79 	    70 	 0.10938 	 0.11734 	 ~...
   47 	    71 	 0.08064 	 0.11853 	 m..s
   45 	    72 	 0.07960 	 0.11872 	 m..s
   83 	    73 	 0.13931 	 0.12090 	 ~...
   80 	    74 	 0.13428 	 0.12112 	 ~...
   75 	    75 	 0.10395 	 0.12364 	 ~...
   53 	    76 	 0.08932 	 0.12398 	 m..s
   81 	    77 	 0.13435 	 0.12421 	 ~...
   74 	    78 	 0.10353 	 0.13054 	 ~...
   56 	    79 	 0.09037 	 0.13066 	 m..s
   55 	    80 	 0.08998 	 0.13180 	 m..s
   78 	    81 	 0.10926 	 0.13367 	 ~...
   58 	    82 	 0.09124 	 0.13545 	 m..s
   57 	    83 	 0.09047 	 0.13893 	 m..s
   85 	    84 	 0.16034 	 0.15235 	 ~...
   84 	    85 	 0.15177 	 0.15893 	 ~...
   87 	    86 	 0.16502 	 0.16461 	 ~...
   86 	    87 	 0.16301 	 0.20235 	 m..s
   88 	    88 	 0.25799 	 0.25523 	 ~...
   90 	    89 	 0.30713 	 0.30109 	 ~...
   89 	    90 	 0.26868 	 0.33119 	 m..s
   91 	    91 	 0.46827 	 0.34052 	 MISS
   92 	    92 	 0.47906 	 0.37214 	 MISS
   93 	    93 	 0.49507 	 0.42680 	 m..s
   95 	    94 	 0.50405 	 0.49255 	 ~...
   97 	    95 	 0.51223 	 0.49741 	 ~...
   94 	    96 	 0.50211 	 0.50303 	 ~...
   96 	    97 	 0.50502 	 0.52169 	 ~...
  102 	    98 	 0.54091 	 0.53535 	 ~...
   99 	    99 	 0.52505 	 0.54395 	 ~...
  105 	   100 	 0.55654 	 0.55497 	 ~...
  101 	   101 	 0.54048 	 0.55609 	 ~...
  100 	   102 	 0.52630 	 0.55616 	 ~...
  103 	   103 	 0.55008 	 0.56204 	 ~...
  106 	   104 	 0.55773 	 0.56998 	 ~...
  104 	   105 	 0.55100 	 0.57564 	 ~...
   98 	   106 	 0.51800 	 0.57695 	 m..s
  109 	   107 	 0.59715 	 0.60240 	 ~...
  114 	   108 	 0.61670 	 0.60413 	 ~...
  116 	   109 	 0.62468 	 0.60795 	 ~...
  110 	   110 	 0.59787 	 0.61211 	 ~...
  115 	   111 	 0.61928 	 0.61403 	 ~...
  119 	   112 	 0.63090 	 0.61906 	 ~...
  118 	   113 	 0.62687 	 0.61945 	 ~...
  120 	   114 	 0.63215 	 0.62092 	 ~...
  113 	   115 	 0.60856 	 0.62127 	 ~...
  117 	   116 	 0.62525 	 0.62134 	 ~...
  111 	   117 	 0.60479 	 0.62143 	 ~...
  108 	   118 	 0.58916 	 0.62257 	 m..s
  107 	   119 	 0.58790 	 0.62441 	 m..s
  112 	   120 	 0.60825 	 0.62963 	 ~...
==========================================
r_mrr = 0.9877955317497253
r2_mrr = 0.9754008054733276
spearmanr_mrr@5 = 0.9040163159370422
spearmanr_mrr@10 = 0.8636651039123535
spearmanr_mrr@50 = 0.990323543548584
spearmanr_mrr@100 = 0.9940376281738281
spearmanr_mrr@All = 0.9945679306983948
==========================================
test time: 0.485
Done Testing dataset UMLS
total time taken: 188.56171560287476
training time taken: 182.6173870563507
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9878)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9754)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9040)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8637)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9903)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9940)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9946)}}, 'test_loss': {'TransE': {'UMLS': 1.1785067618402536}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min deg neighbnour', 'o min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 7561796508551420
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1107, 97, 110, 296, 593, 1061, 884, 584, 723, 814, 275, 1158, 585, 236, 873, 794, 316, 120, 257, 524, 454, 644, 994, 251, 209, 439, 143, 552, 34, 786, 102, 195, 437, 1059, 659, 648, 535, 916, 743, 860, 1041, 456, 39, 344, 410, 136, 833, 772, 630, 656, 368, 1081, 742, 1170, 734, 286, 1183, 527, 592, 731, 699, 858, 2, 272, 243, 608, 427, 1130, 1091, 478, 46, 800, 510, 1123, 1079, 1117, 558, 639, 303, 930, 1040, 329, 1032, 354, 747, 531, 963, 632, 1180, 202, 388, 25, 101, 754, 453, 1151, 849, 670, 782, 911, 855, 685, 827, 781, 149, 152, 1199, 1136, 341, 784, 904, 1034, 161, 79, 571, 857, 253, 588, 1038, 809, 520]
valid_ids (0): []
train_ids (1094): [231, 1200, 1194, 627, 53, 1067, 996, 999, 466, 561, 1046, 540, 701, 666, 763, 1145, 788, 828, 966, 18, 324, 162, 591, 738, 505, 856, 297, 562, 1208, 1143, 450, 726, 1088, 951, 898, 1075, 408, 929, 706, 509, 1150, 720, 1212, 59, 93, 949, 1149, 314, 151, 711, 843, 635, 522, 62, 595, 1074, 971, 247, 1184, 918, 596, 917, 96, 266, 113, 302, 704, 586, 471, 768, 629, 312, 1166, 616, 125, 186, 981, 1021, 625, 852, 1213, 157, 845, 882, 155, 997, 219, 838, 894, 214, 75, 360, 896, 52, 1089, 1129, 146, 1182, 137, 823, 705, 812, 933, 108, 24, 836, 326, 1152, 555, 413, 1169, 693, 578, 69, 990, 100, 1196, 181, 669, 60, 696, 462, 441, 495, 196, 1192, 1163, 1049, 163, 173, 807, 61, 425, 765, 1175, 1050, 539, 551, 513, 1177, 311, 459, 573, 802, 475, 816, 435, 1179, 570, 1039, 279, 291, 1173, 641, 1132, 132, 660, 987, 872, 259, 67, 716, 910, 364, 198, 1037, 847, 725, 708, 806, 640, 939, 899, 366, 68, 1124, 1156, 1148, 977, 897, 690, 657, 901, 915, 867, 376, 416, 1206, 1154, 107, 988, 863, 776, 564, 150, 839, 301, 942, 862, 662, 804, 688, 727, 99, 1095, 835, 650, 773, 17, 1030, 321, 790, 523, 735, 905, 874, 759, 677, 1099, 164, 109, 883, 1178, 1109, 634, 741, 504, 131, 1197, 605, 117, 227, 822, 45, 534, 1168, 610, 574, 941, 499, 791, 945, 438, 1140, 292, 651, 166, 94, 1137, 671, 760, 82, 33, 517, 755, 955, 665, 547, 653, 41, 545, 140, 1120, 853, 43, 153, 1119, 1209, 861, 1204, 496, 598, 1090, 433, 159, 418, 697, 736, 1103, 992, 587, 372, 572, 374, 141, 1157, 749, 216, 753, 282, 1104, 834, 909, 514, 11, 642, 167, 124, 448, 906, 568, 430, 1098, 380, 255, 234, 1052, 832, 947, 950, 174, 232, 133, 698, 398, 1096, 1, 1214, 28, 948, 479, 85, 356, 484, 193, 178, 925, 1073, 976, 1051, 449, 550, 1024, 361, 819, 1131, 938, 780, 746, 879, 77, 944, 490, 404, 799, 128, 254, 1188, 820, 1016, 530, 500, 721, 207, 502, 581, 943, 261, 549, 240, 226, 924, 1181, 728, 464, 1133, 875, 419, 371, 895, 565, 707, 1004, 348, 222, 919, 74, 378, 968, 402, 560, 1057, 31, 414, 1118, 1029, 813, 777, 104, 215, 365, 307, 870, 970, 633, 91, 421, 1086, 702, 262, 145, 623, 1036, 638, 937, 691, 353, 446, 637, 779, 362, 258, 556, 491, 205, 335, 188, 423, 912, 27, 1055, 118, 399, 373, 333, 935, 846, 512, 160, 1189, 190, 271, 907, 972, 305, 946, 165, 339, 1071, 864, 48, 1082, 264, 563, 506, 1110, 516, 1033, 44, 934, 732, 63, 485, 3, 470, 293, 194, 1072, 220, 81, 959, 793, 184, 310, 488, 603, 826, 199, 375, 1043, 306, 64, 844, 168, 675, 260, 1122, 775, 521, 962, 319, 1153, 111, 4, 767, 544, 428, 1164, 796, 1015, 1159, 76, 526, 476, 903, 626, 49, 758, 112, 1017, 985, 619, 224, 83, 290, 1193, 406, 511, 142, 468, 575, 1187, 877, 805, 245, 367, 686, 400, 931, 238, 14, 761, 1108, 472, 869, 147, 922, 559, 242, 1093, 762, 515, 1048, 330, 729, 778, 1080, 553, 55, 350, 1142, 612, 1106, 72, 957, 722, 998, 129, 211, 463, 602, 267, 12, 315, 1026, 225, 116, 473, 983, 1077, 984, 927, 868, 617, 606, 288, 345, 713, 582, 287, 609, 771, 590, 751, 105, 825, 172, 1141, 733, 975, 280, 489, 993, 1031, 213, 655, 1065, 649, 1000, 498, 718, 923, 15, 98, 1085, 447, 1101, 37, 217, 1205, 210, 1054, 338, 1028, 458, 477, 170, 1147, 405, 1068, 1014, 230, 19, 90, 383, 1211, 385, 953, 246, 276, 1138, 229, 566, 886, 431, 770, 542, 304, 795, 32, 218, 821, 134, 1027, 600, 180, 1172, 355, 42, 1198, 115, 589, 235, 268, 393, 135, 1139, 370, 5, 407, 652, 681, 474, 601, 661, 541, 349, 359, 871, 465, 831, 611, 667, 532, 1105, 1210, 452, 241, 119, 792, 840, 557, 1176, 533, 538, 127, 810, 284, 1007, 442, 1003, 389, 979, 123, 1047, 548, 1005, 390, 1146, 392, 1025, 961, 57, 859, 1083, 13, 274, 646, 1011, 183, 1066, 336, 739, 817, 177, 579, 1135, 850, 1060, 269, 1171, 1207, 709, 342, 801, 295, 798, 467, 546, 1202, 973, 613, 967, 952, 543, 1069, 672, 1092, 185, 594, 501, 386, 913, 429, 694, 926, 851, 175, 347, 518, 684, 604, 830, 583, 841, 249, 1195, 692, 273, 325, 346, 285, 340, 171, 618, 1162, 233, 281, 1010, 432, 426, 461, 678, 1012, 964, 144, 622, 921, 434, 503, 189, 1191, 1165, 958, 674, 803, 854, 481, 680, 51, 480, 409, 451, 878, 318, 1203, 577, 599, 621, 757, 748, 536, 58, 201, 80, 0, 1062, 769, 1013, 332, 84, 250, 569, 337, 8, 223, 837, 645, 244, 936, 842, 537, 270, 334, 92, 1134, 1186, 299, 774, 787, 554, 424, 352, 865, 744, 208, 928, 308, 986, 1190, 156, 797, 766, 1115, 412, 715, 668, 712, 212, 1161, 908, 703, 197, 507, 436, 679, 440, 457, 1018, 65, 1076, 1053, 20, 35, 737, 176, 1008, 891, 395, 1009, 126, 417, 1125, 890, 687, 1112, 700, 519, 382, 182, 88, 1185, 1114, 256, 415, 169, 1121, 228, 313, 1144, 673, 978, 394, 265, 508, 263, 443, 866, 239, 10, 607, 636, 676, 1084, 730, 1002, 138, 719, 1078, 756, 1035, 940, 486, 1174, 358, 283, 114, 885, 497, 237, 1111, 932, 1126, 38, 663, 397, 29, 887, 529, 750, 492, 1045, 615, 1023, 824, 331, 139, 1113, 95, 469, 717, 73, 848, 36, 298, 483, 1155, 78, 888, 1160, 40, 70, 158, 1116, 960, 252, 829, 710, 148, 1063, 289, 322, 154, 203, 16, 695, 323, 21, 122, 444, 278, 482, 320, 221, 351, 317, 54, 764, 789, 989, 752, 422, 363, 22, 328, 624, 1100, 880, 893, 26, 1042, 980, 689, 294, 920, 66, 580, 494, 56, 576, 493, 965, 991, 309, 1019, 1056, 420, 785, 300, 377, 455, 191, 384, 956, 683, 1167, 525, 187, 1102, 597, 47, 1006, 87, 647, 277, 881, 1001, 89, 815, 343, 387, 808, 396, 1128, 1087, 1094, 892, 179, 740, 1097, 248, 628, 200, 889, 682, 103, 643, 206, 614, 379, 654, 1201, 954, 369, 130, 357, 811, 658, 664, 528, 30, 620, 995, 106, 391, 487, 23, 631, 1058, 969, 876, 445, 7, 121, 403, 71, 900, 9, 714, 1127, 567, 745, 783, 460, 982, 914, 902, 1020, 401, 86, 1044, 204, 50, 327, 192, 724, 1022, 1070, 818, 381, 411, 974, 1064, 6]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9297594232673226
the save name prefix for this run is:  chkpt-ID_9297594232673226_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 919
rank avg (pred): 0.442 +- 0.003
mrr vals (pred, true): 0.017, 0.086
batch losses (mrrl, rdl): 0.0, 0.0005123024

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 825
rank avg (pred): 0.046 +- 0.039
mrr vals (pred, true): 0.320, 0.581
batch losses (mrrl, rdl): 0.0, 7.9109e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 254
rank avg (pred): 0.029 +- 0.028
mrr vals (pred, true): 0.424, 0.628
batch losses (mrrl, rdl): 0.0, 1.7088e-06

Epoch over!
epoch time: 12.237

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 245
rank avg (pred): 0.033 +- 0.035
mrr vals (pred, true): 0.397, 0.612
batch losses (mrrl, rdl): 0.0, 2.8695e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1018
rank avg (pred): 0.303 +- 0.240
mrr vals (pred, true): 0.180, 0.111
batch losses (mrrl, rdl): 0.0, 5.42589e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 422
rank avg (pred): 0.349 +- 0.267
mrr vals (pred, true): 0.153, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001310813

Epoch over!
epoch time: 11.857

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 579
rank avg (pred): 0.402 +- 0.274
mrr vals (pred, true): 0.135, 0.034
batch losses (mrrl, rdl): 0.0, 1.52409e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1209
rank avg (pred): 0.413 +- 0.269
mrr vals (pred, true): 0.106, 0.039
batch losses (mrrl, rdl): 0.0, 2.9157e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 313
rank avg (pred): 0.042 +- 0.072
mrr vals (pred, true): 0.408, 0.565
batch losses (mrrl, rdl): 0.0, 8.4337e-06

Epoch over!
epoch time: 11.846

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 465
rank avg (pred): 0.335 +- 0.267
mrr vals (pred, true): 0.163, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001602742

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 886
rank avg (pred): 0.331 +- 0.266
mrr vals (pred, true): 0.179, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001689114

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 160
rank avg (pred): 0.336 +- 0.271
mrr vals (pred, true): 0.168, 0.098
batch losses (mrrl, rdl): 0.0, 0.0001958467

Epoch over!
epoch time: 11.825

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 103
rank avg (pred): 0.339 +- 0.268
mrr vals (pred, true): 0.164, 0.100
batch losses (mrrl, rdl): 0.0, 0.0002012295

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 910
rank avg (pred): 0.088 +- 0.132
mrr vals (pred, true): 0.319, 0.272
batch losses (mrrl, rdl): 0.0, 4.827e-07

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1097
rank avg (pred): 0.326 +- 0.268
mrr vals (pred, true): 0.186, 0.158
batch losses (mrrl, rdl): 0.0, 0.0002475945

Epoch over!
epoch time: 11.999

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 837
rank avg (pred): 0.319 +- 0.263
mrr vals (pred, true): 0.214, 0.088
batch losses (mrrl, rdl): 0.2703051567, 4.52767e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 132
rank avg (pred): 0.329 +- 0.101
mrr vals (pred, true): 0.041, 0.088
batch losses (mrrl, rdl): 0.0008798694, 0.0001066275

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 21
rank avg (pred): 0.015 +- 0.011
mrr vals (pred, true): 0.470, 0.498
batch losses (mrrl, rdl): 0.0080147134, 5.393e-06

Epoch over!
epoch time: 12.426

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 989
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.588, 0.620
batch losses (mrrl, rdl): 0.0099959606, 5.8271e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 599
rank avg (pred): 0.347 +- 0.079
mrr vals (pred, true): 0.030, 0.044
batch losses (mrrl, rdl): 0.0039244378, 0.0001990902

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 169
rank avg (pred): 0.313 +- 0.108
mrr vals (pred, true): 0.048, 0.043
batch losses (mrrl, rdl): 4.51804e-05, 0.0003140797

Epoch over!
epoch time: 12.27

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 24
rank avg (pred): 0.014 +- 0.010
mrr vals (pred, true): 0.488, 0.495
batch losses (mrrl, rdl): 0.0005807041, 5.1727e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 10
rank avg (pred): 0.011 +- 0.007
mrr vals (pred, true): 0.526, 0.545
batch losses (mrrl, rdl): 0.0038684364, 5.9539e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 377
rank avg (pred): 0.273 +- 0.142
mrr vals (pred, true): 0.103, 0.149
batch losses (mrrl, rdl): 0.021690011, 8.56901e-05

Epoch over!
epoch time: 12.043

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 228
rank avg (pred): 0.323 +- 0.101
mrr vals (pred, true): 0.043, 0.038
batch losses (mrrl, rdl): 0.000462207, 0.0004591306

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 131
rank avg (pred): 0.278 +- 0.134
mrr vals (pred, true): 0.092, 0.123
batch losses (mrrl, rdl): 0.0099732522, 8.56305e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 233
rank avg (pred): 0.276 +- 0.137
mrr vals (pred, true): 0.094, 0.034
batch losses (mrrl, rdl): 0.0192516781, 0.0006251971

Epoch over!
epoch time: 12.357

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 205
rank avg (pred): 0.314 +- 0.107
mrr vals (pred, true): 0.049, 0.039
batch losses (mrrl, rdl): 4.3246e-06, 0.0004190651

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1114
rank avg (pred): 0.268 +- 0.142
mrr vals (pred, true): 0.108, 0.040
batch losses (mrrl, rdl): 0.0334744416, 0.0006481623

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 877
rank avg (pred): 0.312 +- 0.108
mrr vals (pred, true): 0.056, 0.042
batch losses (mrrl, rdl): 0.0003503569, 0.0003434099

Epoch over!
epoch time: 12.07

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 19
rank avg (pred): 0.017 +- 0.013
mrr vals (pred, true): 0.475, 0.547
batch losses (mrrl, rdl): 0.0516181476, 1.5652e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 566
rank avg (pred): 0.259 +- 0.147
mrr vals (pred, true): 0.130, 0.125
batch losses (mrrl, rdl): 0.0001886854, 5.87322e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1021
rank avg (pred): 0.283 +- 0.125
mrr vals (pred, true): 0.085, 0.123
batch losses (mrrl, rdl): 0.0145315789, 5.53839e-05

Epoch over!
epoch time: 12.26

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1002
rank avg (pred): 0.282 +- 0.127
mrr vals (pred, true): 0.087, 0.118
batch losses (mrrl, rdl): 0.0091222571, 3.77295e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 78
rank avg (pred): 0.015 +- 0.011
mrr vals (pred, true): 0.507, 0.493
batch losses (mrrl, rdl): 0.0022242935, 5.884e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 70
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.568, 0.555
batch losses (mrrl, rdl): 0.0017726759, 4.1264e-06

Epoch over!
epoch time: 12.018

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 808
rank avg (pred): 0.321 +- 0.093
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 5.2142e-06, 0.0003751623

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 440
rank avg (pred): 0.288 +- 0.124
mrr vals (pred, true): 0.085, 0.040
batch losses (mrrl, rdl): 0.0124716423, 0.0005305256

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1169
rank avg (pred): 0.308 +- 0.106
mrr vals (pred, true): 0.060, 0.037
batch losses (mrrl, rdl): 0.0010950929, 0.0002412014

Epoch over!
epoch time: 12.141

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 26
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.655, 0.619
batch losses (mrrl, rdl): 0.012948039, 4.5424e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 91
rank avg (pred): 0.315 +- 0.093
mrr vals (pred, true): 0.055, 0.102
batch losses (mrrl, rdl): 0.0222773366, 0.0001303171

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1201
rank avg (pred): 0.314 +- 0.099
mrr vals (pred, true): 0.062, 0.036
batch losses (mrrl, rdl): 0.0014288649, 0.0004595479

Epoch over!
epoch time: 12.083

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 887
rank avg (pred): 0.318 +- 0.097
mrr vals (pred, true): 0.062, 0.038
batch losses (mrrl, rdl): 0.0014341011, 0.0003584683

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 927
rank avg (pred): 0.320 +- 0.093
mrr vals (pred, true): 0.056, 0.092
batch losses (mrrl, rdl): 0.000394489, 6.9535e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 603
rank avg (pred): 0.340 +- 0.062
mrr vals (pred, true): 0.032, 0.033
batch losses (mrrl, rdl): 0.0033044757, 0.0002581505

Epoch over!
epoch time: 12.325

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.300 +- 0.108
mrr vals (pred, true): 0.077, 0.044

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.03152 	 0.03025 	 ~...
    3 	     1 	 0.03071 	 0.03226 	 ~...
    1 	     2 	 0.02778 	 0.03423 	 ~...
   27 	     3 	 0.05119 	 0.03566 	 ~...
    4 	     4 	 0.03084 	 0.03578 	 ~...
   21 	     5 	 0.04930 	 0.03607 	 ~...
   65 	     6 	 0.08096 	 0.03614 	 m..s
   79 	     7 	 0.09105 	 0.03686 	 m..s
    6 	     8 	 0.03128 	 0.03694 	 ~...
    2 	     9 	 0.02824 	 0.03723 	 ~...
   12 	    10 	 0.04466 	 0.03737 	 ~...
   62 	    11 	 0.07875 	 0.03799 	 m..s
   56 	    12 	 0.07415 	 0.03801 	 m..s
   61 	    13 	 0.07865 	 0.03811 	 m..s
   15 	    14 	 0.04593 	 0.03827 	 ~...
   19 	    15 	 0.04899 	 0.03827 	 ~...
   35 	    16 	 0.05294 	 0.03832 	 ~...
    9 	    17 	 0.03378 	 0.03833 	 ~...
   30 	    18 	 0.05214 	 0.03836 	 ~...
   58 	    19 	 0.07501 	 0.03841 	 m..s
   18 	    20 	 0.04709 	 0.03845 	 ~...
   67 	    21 	 0.08165 	 0.03883 	 m..s
   10 	    22 	 0.03432 	 0.03900 	 ~...
   29 	    23 	 0.05206 	 0.03913 	 ~...
   14 	    24 	 0.04574 	 0.03915 	 ~...
   28 	    25 	 0.05133 	 0.03942 	 ~...
   53 	    26 	 0.07203 	 0.03951 	 m..s
   73 	    27 	 0.08669 	 0.03961 	 m..s
   11 	    28 	 0.04040 	 0.03971 	 ~...
   16 	    29 	 0.04608 	 0.03972 	 ~...
   32 	    30 	 0.05235 	 0.03973 	 ~...
   74 	    31 	 0.08739 	 0.04026 	 m..s
   69 	    32 	 0.08348 	 0.04036 	 m..s
   66 	    33 	 0.08116 	 0.04046 	 m..s
   60 	    34 	 0.07791 	 0.04050 	 m..s
   75 	    35 	 0.08797 	 0.04050 	 m..s
   39 	    36 	 0.05444 	 0.04061 	 ~...
    8 	    37 	 0.03300 	 0.04070 	 ~...
   17 	    38 	 0.04670 	 0.04070 	 ~...
   78 	    39 	 0.09070 	 0.04078 	 m..s
   77 	    40 	 0.08954 	 0.04107 	 m..s
   13 	    41 	 0.04546 	 0.04163 	 ~...
   41 	    42 	 0.05446 	 0.04208 	 ~...
   43 	    43 	 0.05508 	 0.04239 	 ~...
   55 	    44 	 0.07385 	 0.04267 	 m..s
    0 	    45 	 0.02738 	 0.04328 	 ~...
   71 	    46 	 0.08567 	 0.04390 	 m..s
   59 	    47 	 0.07723 	 0.04445 	 m..s
    5 	    48 	 0.03106 	 0.04577 	 ~...
   40 	    49 	 0.05445 	 0.04607 	 ~...
   22 	    50 	 0.04933 	 0.04711 	 ~...
   46 	    51 	 0.05801 	 0.06426 	 ~...
   47 	    52 	 0.05907 	 0.06558 	 ~...
   45 	    53 	 0.05791 	 0.06665 	 ~...
   50 	    54 	 0.07010 	 0.07228 	 ~...
   52 	    55 	 0.07158 	 0.07231 	 ~...
   36 	    56 	 0.05314 	 0.07276 	 ~...
   49 	    57 	 0.06914 	 0.08372 	 ~...
   31 	    58 	 0.05214 	 0.08469 	 m..s
   20 	    59 	 0.04926 	 0.08490 	 m..s
   24 	    60 	 0.05012 	 0.08747 	 m..s
   34 	    61 	 0.05254 	 0.08775 	 m..s
   44 	    62 	 0.05526 	 0.08861 	 m..s
   23 	    63 	 0.04993 	 0.08899 	 m..s
   37 	    64 	 0.05375 	 0.08934 	 m..s
   38 	    65 	 0.05376 	 0.09043 	 m..s
   25 	    66 	 0.05051 	 0.09090 	 m..s
   33 	    67 	 0.05241 	 0.09155 	 m..s
   42 	    68 	 0.05475 	 0.09245 	 m..s
   26 	    69 	 0.05052 	 0.09778 	 m..s
   63 	    70 	 0.07901 	 0.10142 	 ~...
   51 	    71 	 0.07098 	 0.10240 	 m..s
   81 	    72 	 0.09299 	 0.10900 	 ~...
   68 	    73 	 0.08167 	 0.10944 	 ~...
   48 	    74 	 0.06875 	 0.11071 	 m..s
   64 	    75 	 0.07973 	 0.11157 	 m..s
   57 	    76 	 0.07460 	 0.11169 	 m..s
   83 	    77 	 0.09760 	 0.11433 	 ~...
   84 	    78 	 0.09816 	 0.11690 	 ~...
   85 	    79 	 0.10083 	 0.11936 	 ~...
   54 	    80 	 0.07325 	 0.11998 	 m..s
   70 	    81 	 0.08531 	 0.12120 	 m..s
   72 	    82 	 0.08626 	 0.12290 	 m..s
   80 	    83 	 0.09266 	 0.13180 	 m..s
   76 	    84 	 0.08951 	 0.13270 	 m..s
   82 	    85 	 0.09409 	 0.13503 	 m..s
   86 	    86 	 0.12591 	 0.15415 	 ~...
   88 	    87 	 0.16706 	 0.16156 	 ~...
   87 	    88 	 0.14806 	 0.16461 	 ~...
   91 	    89 	 0.27547 	 0.25103 	 ~...
   89 	    90 	 0.25368 	 0.30550 	 m..s
   90 	    91 	 0.25733 	 0.32606 	 m..s
   93 	    92 	 0.45524 	 0.41053 	 m..s
   97 	    93 	 0.49093 	 0.43219 	 m..s
   99 	    94 	 0.49466 	 0.51193 	 ~...
   92 	    95 	 0.45373 	 0.51239 	 m..s
  102 	    96 	 0.51265 	 0.51624 	 ~...
   94 	    97 	 0.45963 	 0.52523 	 m..s
   95 	    98 	 0.48489 	 0.53015 	 m..s
   98 	    99 	 0.49334 	 0.53107 	 m..s
  105 	   100 	 0.53003 	 0.54395 	 ~...
  110 	   101 	 0.56866 	 0.54532 	 ~...
  100 	   102 	 0.49908 	 0.54980 	 m..s
   96 	   103 	 0.48616 	 0.55234 	 m..s
  108 	   104 	 0.55954 	 0.55886 	 ~...
  103 	   105 	 0.51564 	 0.55974 	 m..s
  109 	   106 	 0.56663 	 0.56051 	 ~...
  107 	   107 	 0.53492 	 0.56057 	 ~...
  104 	   108 	 0.51761 	 0.57161 	 m..s
  106 	   109 	 0.53137 	 0.57514 	 m..s
  101 	   110 	 0.50803 	 0.57695 	 m..s
  113 	   111 	 0.58066 	 0.60150 	 ~...
  119 	   112 	 0.62107 	 0.60458 	 ~...
  114 	   113 	 0.59118 	 0.60795 	 ~...
  111 	   114 	 0.56884 	 0.61212 	 m..s
  117 	   115 	 0.60143 	 0.61471 	 ~...
  116 	   116 	 0.59905 	 0.61521 	 ~...
  115 	   117 	 0.59177 	 0.62041 	 ~...
  120 	   118 	 0.62504 	 0.62143 	 ~...
  118 	   119 	 0.60320 	 0.62322 	 ~...
  112 	   120 	 0.57608 	 0.62400 	 m..s
==========================================
r_mrr = 0.9900044798851013
r2_mrr = 0.9775334596633911
spearmanr_mrr@5 = 0.8056136965751648
spearmanr_mrr@10 = 0.9563993811607361
spearmanr_mrr@50 = 0.9967563152313232
spearmanr_mrr@100 = 0.996760368347168
spearmanr_mrr@All = 0.9970194101333618
==========================================
test time: 0.449
Done Testing dataset UMLS
total time taken: 188.4064371585846
training time taken: 182.28289270401
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9900)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9775)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.8056)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9564)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9968)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9968)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9970)}}, 'test_loss': {'TransE': {'UMLS': 0.985568009184135}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max deg neighbnour', 'o max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 4595585880310682
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [388, 540, 865, 1067, 219, 452, 128, 520, 82, 1194, 812, 1145, 796, 451, 595, 1184, 501, 269, 820, 75, 725, 346, 317, 1133, 999, 937, 907, 677, 49, 1135, 1077, 1192, 376, 1018, 1183, 234, 697, 408, 781, 961, 596, 475, 723, 1102, 551, 766, 934, 915, 787, 261, 1016, 87, 561, 514, 951, 849, 375, 629, 558, 424, 41, 614, 762, 1083, 944, 120, 410, 1031, 1076, 1042, 136, 378, 608, 1011, 179, 827, 557, 0, 125, 1171, 928, 609, 1052, 946, 436, 981, 1108, 839, 444, 74, 1149, 1071, 230, 461, 326, 488, 980, 38, 1087, 768, 736, 496, 936, 1091, 1061, 441, 312, 301, 503, 751, 902, 840, 309, 960, 917, 689, 547, 354, 888, 583, 1169]
valid_ids (0): []
train_ids (1094): [214, 860, 743, 535, 474, 1072, 783, 339, 536, 1144, 458, 704, 1147, 1006, 977, 447, 712, 460, 1004, 328, 351, 299, 925, 331, 735, 133, 20, 1179, 683, 330, 678, 1103, 590, 493, 858, 306, 1025, 693, 806, 575, 355, 1134, 1180, 985, 127, 39, 563, 94, 793, 538, 1195, 1120, 797, 969, 726, 11, 40, 43, 421, 1211, 641, 566, 308, 177, 727, 236, 1054, 976, 525, 202, 788, 28, 967, 276, 1117, 546, 1146, 1062, 739, 701, 1079, 950, 430, 871, 549, 155, 186, 630, 957, 192, 758, 366, 764, 51, 635, 978, 18, 286, 502, 809, 1056, 1206, 1074, 504, 524, 35, 84, 416, 1188, 497, 1037, 882, 19, 457, 37, 938, 1170, 989, 495, 382, 91, 541, 1167, 322, 478, 500, 114, 1209, 34, 918, 259, 325, 464, 792, 876, 449, 879, 1047, 920, 134, 1043, 229, 73, 1093, 794, 836, 819, 627, 1201, 151, 422, 974, 318, 67, 613, 526, 107, 209, 838, 13, 141, 42, 32, 365, 336, 529, 89, 949, 878, 696, 657, 1051, 801, 210, 1007, 1106, 775, 687, 645, 1115, 958, 906, 383, 581, 706, 675, 837, 1138, 168, 356, 599, 682, 972, 1199, 1023, 1033, 767, 1212, 598, 1125, 624, 92, 1173, 756, 579, 1040, 1082, 110, 498, 224, 333, 130, 188, 1113, 637, 446, 785, 287, 971, 901, 982, 208, 909, 869, 477, 810, 1126, 303, 667, 390, 250, 1143, 691, 803, 1039, 1168, 752, 1129, 165, 257, 203, 513, 244, 572, 198, 361, 543, 634, 1196, 98, 965, 533, 206, 160, 968, 720, 845, 872, 491, 834, 8, 1160, 272, 892, 252, 518, 745, 509, 459, 176, 404, 988, 400, 190, 335, 567, 830, 44, 143, 811, 1048, 53, 1109, 63, 144, 1068, 352, 1104, 211, 419, 164, 698, 610, 121, 782, 732, 774, 294, 398, 861, 1015, 101, 653, 896, 226, 601, 759, 1154, 104, 199, 1022, 207, 146, 991, 997, 508, 126, 48, 649, 555, 170, 631, 277, 138, 377, 505, 432, 275, 485, 552, 194, 1034, 183, 738, 402, 147, 266, 467, 1110, 1012, 883, 1114, 577, 1009, 232, 406, 1041, 843, 476, 510, 342, 197, 620, 469, 973, 1142, 1096, 887, 770, 671, 562, 1176, 1127, 1159, 640, 201, 674, 799, 1075, 426, 233, 707, 757, 407, 1090, 626, 282, 486, 17, 670, 135, 59, 519, 494, 145, 189, 633, 445, 1029, 531, 271, 1122, 1044, 340, 619, 862, 72, 580, 578, 76, 1081, 874, 948, 718, 857, 688, 360, 399, 821, 995, 1128, 1151, 24, 945, 156, 223, 154, 332, 1116, 215, 795, 933, 60, 929, 908, 260, 492, 647, 471, 714, 484, 185, 337, 55, 293, 288, 983, 600, 329, 656, 1163, 1038, 530, 1130, 109, 1028, 427, 913, 487, 1137, 900, 798, 643, 172, 69, 975, 602, 713, 852, 853, 1165, 623, 1045, 456, 220, 36, 550, 831, 964, 894, 943, 870, 440, 1198, 1046, 175, 415, 264, 1050, 216, 813, 822, 26, 606, 30, 196, 658, 923, 746, 428, 1002, 393, 664, 826, 695, 709, 1024, 638, 187, 576, 1136, 521, 771, 1182, 313, 780, 80, 523, 490, 1013, 279, 846, 374, 280, 411, 132, 153, 243, 676, 717, 1161, 240, 62, 668, 397, 438, 1099, 47, 511, 1186, 574, 784, 573, 439, 711, 1197, 195, 654, 21, 1119, 644, 1140, 603, 632, 1191, 180, 1060, 911, 966, 113, 2, 570, 680, 81, 405, 754, 534, 516, 434, 265, 1124, 499, 642, 947, 1153, 86, 615, 777, 1185, 1, 730, 661, 23, 320, 765, 939, 1204, 802, 64, 646, 418, 897, 465, 1010, 300, 169, 481, 1080, 88, 564, 979, 297, 1019, 545, 112, 1088, 1214, 554, 1095, 686, 228, 50, 517, 1105, 263, 618, 1084, 790, 855, 984, 970, 448, 1139, 1059, 385, 921, 314, 565, 662, 345, 150, 305, 992, 122, 591, 854, 221, 560, 85, 817, 225, 1020, 1073, 379, 367, 1014, 14, 942, 1187, 129, 1049, 281, 710, 825, 222, 914, 859, 205, 990, 721, 159, 96, 54, 744, 111, 741, 249, 124, 715, 585, 68, 708, 1027, 6, 395, 241, 589, 1157, 506, 1026, 479, 255, 181, 903, 289, 731, 472, 4, 716, 1057, 1175, 804, 173, 284, 79, 359, 231, 895, 387, 941, 256, 384, 319, 1150, 414, 663, 423, 470, 703, 954, 1030, 334, 184, 639, 648, 152, 381, 12, 1202, 528, 728, 679, 344, 905, 962, 953, 848, 267, 694, 1152, 149, 315, 749, 1035, 994, 636, 927, 401, 235, 115, 1055, 864, 1086, 1207, 25, 559, 108, 242, 396, 83, 556, 295, 171, 116, 327, 453, 1164, 450, 1111, 323, 700, 245, 926, 246, 1101, 886, 212, 893, 239, 412, 455, 1181, 57, 166, 597, 290, 829, 254, 856, 357, 515, 1089, 372, 586, 137, 916, 321, 553, 99, 705, 773, 607, 621, 274, 311, 77, 786, 1200, 778, 22, 217, 996, 835, 587, 659, 1069, 262, 368, 489, 924, 1210, 425, 588, 1193, 193, 462, 1000, 507, 763, 1053, 748, 1036, 729, 413, 594, 625, 370, 660, 998, 10, 304, 877, 97, 512, 386, 273, 347, 1003, 779, 52, 1141, 316, 733, 433, 1158, 1190, 956, 58, 932, 815, 409, 358, 268, 364, 218, 343, 291, 1166, 772, 747, 940, 702, 1065, 685, 571, 31, 544, 750, 139, 1132, 1100, 881, 690, 665, 963, 655, 103, 1174, 7, 1005, 33, 106, 666, 442, 776, 466, 842, 278, 734, 986, 1097, 161, 1118, 1078, 1094, 1008, 302, 394, 29, 722, 338, 100, 650, 1178, 740, 71, 417, 542, 371, 922, 955, 463, 119, 348, 828, 296, 885, 866, 78, 292, 527, 93, 1208, 429, 537, 363, 611, 15, 253, 118, 868, 285, 1189, 959, 307, 480, 1032, 889, 369, 616, 163, 420, 760, 45, 832, 341, 1107, 1205, 875, 873, 952, 403, 27, 65, 258, 380, 3, 807, 66, 298, 805, 684, 851, 568, 1021, 1063, 157, 1156, 651, 823, 816, 719, 935, 353, 890, 761, 247, 373, 158, 737, 123, 612, 56, 1098, 389, 841, 131, 863, 1121, 473, 673, 884, 1058, 1155, 174, 227, 681, 930, 1085, 102, 1064, 652, 362, 1162, 251, 850, 824, 891, 548, 617, 569, 753, 808, 5, 1131, 919, 604, 248, 454, 1203, 117, 904, 213, 993, 283, 742, 238, 789, 468, 910, 1092, 880, 270, 1148, 1172, 90, 105, 1177, 1112, 431, 61, 582, 9, 435, 1017, 912, 437, 669, 482, 349, 724, 692, 593, 178, 847, 898, 755, 899, 443, 791, 522, 70, 672, 532, 191, 140, 844, 867, 699, 622, 628, 392, 1066, 584, 204, 605, 1070, 833, 95, 310, 167, 350, 800, 1123, 1213, 142, 769, 987, 483, 182, 391, 16, 46, 1001, 148, 200, 818, 931, 814, 162, 237, 539, 592, 324]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5611550623534398
the save name prefix for this run is:  chkpt-ID_5611550623534398_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 879
rank avg (pred): 0.477 +- 0.006
mrr vals (pred, true): 0.015, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001047976

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 336
rank avg (pred): 0.353 +- 0.001
mrr vals (pred, true): 0.021, 0.100
batch losses (mrrl, rdl): 0.0, 0.0002087742

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 633
rank avg (pred): 0.436 +- 0.000
mrr vals (pred, true): 0.017, 0.034
batch losses (mrrl, rdl): 0.0, 5.61252e-05

Epoch over!
epoch time: 12.113

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 221
rank avg (pred): 0.363 +- 0.000
mrr vals (pred, true): 0.020, 0.040
batch losses (mrrl, rdl): 0.0, 0.0002446515

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 578
rank avg (pred): 0.431 +- 0.000
mrr vals (pred, true): 0.017, 0.041
batch losses (mrrl, rdl): 0.0, 7.16661e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 643
rank avg (pred): 0.448 +- 0.000
mrr vals (pred, true): 0.016, 0.034
batch losses (mrrl, rdl): 0.0, 5.66217e-05

Epoch over!
epoch time: 11.831

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 642
rank avg (pred): 0.467 +- 0.000
mrr vals (pred, true): 0.016, 0.035
batch losses (mrrl, rdl): 0.0, 8.1378e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1201
rank avg (pred): 0.433 +- 0.000
mrr vals (pred, true): 0.017, 0.036
batch losses (mrrl, rdl): 0.0, 9.00323e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1033
rank avg (pred): 0.360 +- 0.000
mrr vals (pred, true): 0.020, 0.038
batch losses (mrrl, rdl): 0.0, 0.0002455843

Epoch over!
epoch time: 11.922

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1112
rank avg (pred): 0.347 +- 0.000
mrr vals (pred, true): 0.021, 0.036
batch losses (mrrl, rdl): 0.0, 0.0003251568

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 947
rank avg (pred): 0.358 +- 0.000
mrr vals (pred, true): 0.020, 0.040
batch losses (mrrl, rdl): 0.0, 0.0002621494

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 481
rank avg (pred): 0.360 +- 0.000
mrr vals (pred, true): 0.020, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001853349

Epoch over!
epoch time: 11.915

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 393
rank avg (pred): 0.366 +- 0.000
mrr vals (pred, true): 0.020, 0.097
batch losses (mrrl, rdl): 0.0, 0.000167086

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1203
rank avg (pred): 0.422 +- 0.000
mrr vals (pred, true): 0.017, 0.046
batch losses (mrrl, rdl): 0.0, 9.96982e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 174
rank avg (pred): 0.361 +- 0.000
mrr vals (pred, true): 0.020, 0.042
batch losses (mrrl, rdl): 0.0, 0.0002021688

Epoch over!
epoch time: 11.852

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 563
rank avg (pred): 0.335 +- 0.000
mrr vals (pred, true): 0.022, 0.114
batch losses (mrrl, rdl): 0.0842255056, 0.0001148986

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 27
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.528, 0.513
batch losses (mrrl, rdl): 0.0023562384, 1.45298e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 48
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.507, 0.505
batch losses (mrrl, rdl): 2.32067e-05, 1.20755e-05

Epoch over!
epoch time: 12.361

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 334
rank avg (pred): 0.092 +- 0.002
mrr vals (pred, true): 0.075, 0.127
batch losses (mrrl, rdl): 0.0264706556, 0.0003575184

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 972
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.612, 0.618
batch losses (mrrl, rdl): 0.0004654361, 6.536e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 162
rank avg (pred): 0.107 +- 0.001
mrr vals (pred, true): 0.065, 0.036
batch losses (mrrl, rdl): 0.0022504237, 0.0026340047

Epoch over!
epoch time: 11.983

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 113
rank avg (pred): 0.082 +- 0.001
mrr vals (pred, true): 0.084, 0.122
batch losses (mrrl, rdl): 0.0146231353, 0.000549012

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 59
rank avg (pred): 0.006 +- 0.000
mrr vals (pred, true): 0.563, 0.602
batch losses (mrrl, rdl): 0.0154326009, 5.4541e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 466
rank avg (pred): 0.100 +- 0.000
mrr vals (pred, true): 0.070, 0.038
batch losses (mrrl, rdl): 0.0038184365, 0.0022679626

Epoch over!
epoch time: 12.008

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 564
rank avg (pred): 0.168 +- 0.000
mrr vals (pred, true): 0.042, 0.063
batch losses (mrrl, rdl): 0.0005679609, 0.0007249696

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1044
rank avg (pred): 0.090 +- 0.000
mrr vals (pred, true): 0.077, 0.038
batch losses (mrrl, rdl): 0.0072043585, 0.0027099713

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 734
rank avg (pred): 0.011 +- 0.000
mrr vals (pred, true): 0.409, 0.512
batch losses (mrrl, rdl): 0.1069974452, 1.37355e-05

Epoch over!
epoch time: 12.214

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 446
rank avg (pred): 0.082 +- 0.000
mrr vals (pred, true): 0.083, 0.035
batch losses (mrrl, rdl): 0.0109090507, 0.0029164613

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 664
rank avg (pred): 0.161 +- 0.000
mrr vals (pred, true): 0.044, 0.040
batch losses (mrrl, rdl): 0.0003178796, 0.001626442

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 757
rank avg (pred): 0.088 +- 0.000
mrr vals (pred, true): 0.078, 0.088
batch losses (mrrl, rdl): 0.0078223655, 0.0008713499

Epoch over!
epoch time: 12.008

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 31
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.619, 0.571
batch losses (mrrl, rdl): 0.0236080885, 1.05505e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 442
rank avg (pred): 0.100 +- 0.000
mrr vals (pred, true): 0.069, 0.043
batch losses (mrrl, rdl): 0.0037630929, 0.0024019186

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1050
rank avg (pred): 0.079 +- 0.000
mrr vals (pred, true): 0.086, 0.041
batch losses (mrrl, rdl): 0.0131067019, 0.0027542105

Epoch over!
epoch time: 12.394

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 825
rank avg (pred): 0.006 +- 0.000
mrr vals (pred, true): 0.566, 0.581
batch losses (mrrl, rdl): 0.0021064319, 9.8186e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 97
rank avg (pred): 0.127 +- 0.000
mrr vals (pred, true): 0.055, 0.102
batch losses (mrrl, rdl): 0.0220631063, 0.000435447

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 950
rank avg (pred): 0.146 +- 0.000
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 1.59727e-05, 0.0018240495

Epoch over!
epoch time: 12.229

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 54
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.509, 0.537
batch losses (mrrl, rdl): 0.0080340458, 1.25401e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 65
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.599, 0.619
batch losses (mrrl, rdl): 0.0040993067, 5.2972e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1030
rank avg (pred): 0.072 +- 0.000
mrr vals (pred, true): 0.094, 0.043
batch losses (mrrl, rdl): 0.0191120282, 0.0029074021

Epoch over!
epoch time: 12.11

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1198
rank avg (pred): 0.145 +- 0.000
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 1.34831e-05, 0.0018944476

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 993
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.610, 0.608
batch losses (mrrl, rdl): 1.6551e-05, 7.1757e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 612
rank avg (pred): 0.132 +- 0.000
mrr vals (pred, true): 0.054, 0.039
batch losses (mrrl, rdl): 0.0001277626, 0.0017842567

Epoch over!
epoch time: 12.068

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 750
rank avg (pred): 0.008 +- 0.000
mrr vals (pred, true): 0.498, 0.537
batch losses (mrrl, rdl): 0.0158193782, 1.47839e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 755
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.522, 0.581
batch losses (mrrl, rdl): 0.0345900804, 8.4793e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 964
rank avg (pred): 0.154 +- 0.000
mrr vals (pred, true): 0.046, 0.041
batch losses (mrrl, rdl): 0.0001375621, 0.0016857284

Epoch over!
epoch time: 11.983

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.088 +- 0.000
mrr vals (pred, true): 0.078, 0.112

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.04797 	 0.03224 	 ~...
   13 	     1 	 0.04757 	 0.03444 	 ~...
   51 	     2 	 0.06893 	 0.03449 	 m..s
   41 	     3 	 0.05451 	 0.03464 	 ~...
   39 	     4 	 0.05425 	 0.03543 	 ~...
   26 	     5 	 0.04914 	 0.03601 	 ~...
   28 	     6 	 0.04923 	 0.03637 	 ~...
   11 	     7 	 0.04519 	 0.03662 	 ~...
   23 	     8 	 0.04905 	 0.03667 	 ~...
   29 	     9 	 0.04927 	 0.03672 	 ~...
   57 	    10 	 0.07733 	 0.03678 	 m..s
   12 	    11 	 0.04572 	 0.03699 	 ~...
   32 	    12 	 0.04972 	 0.03704 	 ~...
   59 	    13 	 0.07957 	 0.03718 	 m..s
   43 	    14 	 0.05455 	 0.03772 	 ~...
   22 	    15 	 0.04903 	 0.03811 	 ~...
   14 	    16 	 0.04772 	 0.03824 	 ~...
   27 	    17 	 0.04923 	 0.03832 	 ~...
   25 	    18 	 0.04912 	 0.03845 	 ~...
   88 	    19 	 0.11625 	 0.03870 	 m..s
   48 	    20 	 0.05743 	 0.03880 	 ~...
   86 	    21 	 0.11263 	 0.03922 	 m..s
   20 	    22 	 0.04808 	 0.03938 	 ~...
   81 	    23 	 0.10651 	 0.03959 	 m..s
   60 	    24 	 0.07959 	 0.04003 	 m..s
   68 	    25 	 0.08423 	 0.04012 	 m..s
   61 	    26 	 0.08151 	 0.04036 	 m..s
   24 	    27 	 0.04910 	 0.04046 	 ~...
   65 	    28 	 0.08322 	 0.04049 	 m..s
   31 	    29 	 0.04970 	 0.04050 	 ~...
   74 	    30 	 0.09903 	 0.04063 	 m..s
   64 	    31 	 0.08184 	 0.04085 	 m..s
   51 	    32 	 0.06893 	 0.04112 	 ~...
   15 	    33 	 0.04773 	 0.04117 	 ~...
   10 	    34 	 0.04519 	 0.04177 	 ~...
   35 	    35 	 0.05313 	 0.04197 	 ~...
   67 	    36 	 0.08417 	 0.04207 	 m..s
   71 	    37 	 0.08886 	 0.04262 	 m..s
   66 	    38 	 0.08382 	 0.04281 	 m..s
    6 	    39 	 0.04492 	 0.04330 	 ~...
   18 	    40 	 0.04796 	 0.04457 	 ~...
   21 	    41 	 0.04901 	 0.04459 	 ~...
   47 	    42 	 0.05736 	 0.04536 	 ~...
   30 	    43 	 0.04967 	 0.04577 	 ~...
   77 	    44 	 0.10233 	 0.04715 	 m..s
    4 	    45 	 0.04416 	 0.06389 	 ~...
    5 	    46 	 0.04426 	 0.06426 	 ~...
    2 	    47 	 0.04297 	 0.06667 	 ~...
   34 	    48 	 0.04982 	 0.07231 	 ~...
    0 	    49 	 0.04257 	 0.07327 	 m..s
    1 	    50 	 0.04279 	 0.07479 	 m..s
   51 	    51 	 0.06893 	 0.08000 	 ~...
   40 	    52 	 0.05427 	 0.08032 	 ~...
   33 	    53 	 0.04982 	 0.08036 	 m..s
   49 	    54 	 0.05818 	 0.08156 	 ~...
   51 	    55 	 0.06893 	 0.08234 	 ~...
   42 	    56 	 0.05454 	 0.08372 	 ~...
   17 	    57 	 0.04787 	 0.08483 	 m..s
   16 	    58 	 0.04781 	 0.08490 	 m..s
    3 	    59 	 0.04370 	 0.08580 	 m..s
   38 	    60 	 0.05376 	 0.08622 	 m..s
   50 	    61 	 0.05846 	 0.08633 	 ~...
   51 	    62 	 0.06893 	 0.08676 	 ~...
    9 	    63 	 0.04513 	 0.08764 	 m..s
   56 	    64 	 0.07623 	 0.09043 	 ~...
   44 	    65 	 0.05505 	 0.09090 	 m..s
   36 	    66 	 0.05324 	 0.09155 	 m..s
    7 	    67 	 0.04502 	 0.09231 	 m..s
    8 	    68 	 0.04506 	 0.09247 	 m..s
   37 	    69 	 0.05353 	 0.09451 	 m..s
   46 	    70 	 0.05735 	 0.09496 	 m..s
   45 	    71 	 0.05653 	 0.10238 	 m..s
   73 	    72 	 0.09604 	 0.10345 	 ~...
   70 	    73 	 0.08876 	 0.10902 	 ~...
   78 	    74 	 0.10292 	 0.11137 	 ~...
   58 	    75 	 0.07843 	 0.11169 	 m..s
   72 	    76 	 0.09034 	 0.11198 	 ~...
   79 	    77 	 0.10350 	 0.11248 	 ~...
   89 	    78 	 0.11754 	 0.11473 	 ~...
   80 	    79 	 0.10472 	 0.11699 	 ~...
   69 	    80 	 0.08715 	 0.11742 	 m..s
   63 	    81 	 0.08161 	 0.12127 	 m..s
   85 	    82 	 0.11226 	 0.12187 	 ~...
   84 	    83 	 0.11224 	 0.12290 	 ~...
   62 	    84 	 0.08155 	 0.12642 	 m..s
   75 	    85 	 0.10006 	 0.13366 	 m..s
   76 	    86 	 0.10191 	 0.13367 	 m..s
   82 	    87 	 0.10667 	 0.13723 	 m..s
   87 	    88 	 0.11481 	 0.13812 	 ~...
   90 	    89 	 0.19340 	 0.14077 	 m..s
   83 	    90 	 0.10722 	 0.14185 	 m..s
   92 	    91 	 0.21873 	 0.15500 	 m..s
   91 	    92 	 0.19361 	 0.16485 	 ~...
   93 	    93 	 0.27703 	 0.25523 	 ~...
   96 	    94 	 0.39292 	 0.31332 	 m..s
   94 	    95 	 0.38462 	 0.33070 	 m..s
   95 	    96 	 0.38534 	 0.33468 	 m..s
   98 	    97 	 0.47554 	 0.34052 	 MISS
   97 	    98 	 0.46591 	 0.42680 	 m..s
  100 	    99 	 0.49078 	 0.42704 	 m..s
  101 	   100 	 0.51708 	 0.49071 	 ~...
  103 	   101 	 0.53938 	 0.49163 	 m..s
  102 	   102 	 0.52942 	 0.51292 	 ~...
   99 	   103 	 0.48118 	 0.51997 	 m..s
  105 	   104 	 0.56418 	 0.52270 	 m..s
  104 	   105 	 0.54478 	 0.52298 	 ~...
  107 	   106 	 0.57646 	 0.56450 	 ~...
  106 	   107 	 0.57237 	 0.57095 	 ~...
  108 	   108 	 0.59857 	 0.57695 	 ~...
  115 	   109 	 0.67761 	 0.58207 	 m..s
  119 	   110 	 0.70115 	 0.59483 	 MISS
  114 	   111 	 0.67395 	 0.61289 	 m..s
  111 	   112 	 0.62666 	 0.61315 	 ~...
  116 	   113 	 0.68489 	 0.61509 	 m..s
  118 	   114 	 0.69373 	 0.61521 	 m..s
  113 	   115 	 0.63997 	 0.61698 	 ~...
  109 	   116 	 0.61995 	 0.61837 	 ~...
  120 	   117 	 0.70128 	 0.61917 	 m..s
  110 	   118 	 0.62611 	 0.61945 	 ~...
  117 	   119 	 0.69030 	 0.62257 	 m..s
  112 	   120 	 0.63041 	 0.63497 	 ~...
==========================================
r_mrr = 0.9866105914115906
r2_mrr = 0.9615221619606018
spearmanr_mrr@5 = 0.720908522605896
spearmanr_mrr@10 = 0.717829704284668
spearmanr_mrr@50 = 0.9928718209266663
spearmanr_mrr@100 = 0.9954196214675903
spearmanr_mrr@All = 0.9955868124961853
==========================================
test time: 0.386
Done Testing dataset UMLS
total time taken: 187.21835470199585
training time taken: 181.45290088653564
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9866)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9615)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.7209)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.7178)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9929)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9954)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9956)}}, 'test_loss': {'TransE': {'UMLS': 1.5682995281295007}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o mean deg neighbnour', 's mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 1612160193359283
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [938, 972, 594, 1062, 707, 268, 149, 492, 498, 164, 205, 169, 1059, 64, 410, 467, 857, 506, 168, 516, 141, 587, 982, 778, 995, 450, 153, 815, 52, 727, 462, 876, 1071, 1160, 7, 1065, 341, 533, 642, 614, 850, 156, 101, 195, 932, 828, 175, 38, 214, 1066, 69, 899, 956, 49, 239, 259, 50, 114, 1034, 210, 1052, 703, 1150, 352, 790, 829, 15, 436, 860, 574, 655, 877, 942, 912, 1077, 332, 641, 461, 1172, 821, 891, 402, 40, 1070, 741, 914, 864, 1035, 846, 830, 495, 204, 931, 93, 54, 1000, 883, 1105, 398, 274, 1085, 137, 194, 424, 490, 142, 80, 167, 980, 468, 759, 186, 1199, 249, 1146, 333, 228, 465, 109, 319, 913]
valid_ids (0): []
train_ids (1094): [643, 1054, 390, 459, 170, 1109, 1159, 423, 1076, 216, 1198, 796, 885, 666, 154, 904, 965, 890, 709, 702, 281, 1082, 543, 359, 95, 362, 305, 292, 1157, 378, 715, 211, 1009, 338, 173, 380, 57, 1184, 718, 1001, 408, 925, 1064, 859, 672, 652, 696, 845, 517, 162, 1058, 266, 792, 900, 496, 1080, 1135, 621, 855, 116, 512, 847, 620, 1132, 523, 1056, 923, 514, 470, 1144, 172, 474, 697, 43, 560, 147, 945, 1063, 767, 566, 1121, 48, 190, 1137, 227, 111, 488, 483, 356, 487, 978, 657, 96, 1049, 371, 671, 662, 577, 105, 250, 593, 629, 803, 501, 313, 497, 504, 957, 277, 330, 562, 276, 191, 695, 464, 705, 376, 152, 763, 988, 893, 412, 303, 690, 183, 133, 670, 664, 1033, 813, 881, 793, 768, 234, 393, 837, 589, 640, 532, 819, 404, 728, 1100, 983, 254, 608, 750, 2, 610, 1007, 576, 304, 388, 955, 355, 260, 689, 112, 311, 279, 820, 218, 324, 529, 1048, 946, 521, 622, 905, 789, 280, 325, 432, 746, 1020, 335, 1204, 618, 1142, 273, 761, 1191, 299, 23, 916, 421, 475, 350, 647, 201, 1189, 706, 947, 638, 903, 1188, 486, 1015, 688, 1086, 711, 700, 22, 235, 633, 535, 312, 18, 106, 811, 994, 122, 694, 775, 285, 1158, 26, 799, 525, 209, 977, 315, 624, 1061, 527, 76, 185, 45, 975, 723, 554, 1036, 328, 708, 872, 880, 744, 882, 480, 921, 970, 749, 1180, 635, 809, 1155, 399, 55, 91, 1029, 561, 1002, 1094, 351, 951, 25, 935, 752, 1067, 934, 329, 1167, 320, 964, 403, 895, 1053, 839, 555, 748, 918, 973, 21, 384, 413, 1092, 62, 129, 924, 851, 135, 814, 863, 676, 1143, 1127, 32, 1182, 817, 547, 862, 354, 1173, 306, 314, 827, 519, 997, 406, 578, 673, 791, 1210, 217, 753, 908, 505, 1113, 1031, 1003, 684, 1125, 582, 233, 826, 1038, 781, 558, 747, 202, 823, 739, 1045, 1117, 286, 919, 686, 220, 886, 255, 263, 454, 774, 646, 783, 472, 785, 1164, 1186, 258, 853, 625, 818, 262, 1108, 223, 960, 1040, 367, 1026, 71, 968, 836, 79, 317, 630, 110, 230, 119, 226, 471, 644, 445, 457, 1055, 418, 1177, 801, 447, 734, 756, 1043, 685, 1019, 118, 1141, 868, 572, 1018, 28, 639, 508, 1179, 383, 448, 681, 834, 270, 1128, 927, 735, 1193, 120, 929, 326, 309, 933, 143, 225, 677, 392, 717, 564, 1106, 909, 342, 1098, 27, 166, 47, 966, 298, 580, 1116, 874, 442, 1200, 1044, 430, 1081, 939, 1010, 1, 1073, 381, 856, 779, 650, 787, 100, 1089, 617, 39, 1114, 1075, 745, 337, 867, 293, 449, 146, 780, 1197, 999, 573, 452, 1097, 247, 769, 557, 431, 275, 444, 331, 81, 1041, 634, 757, 503, 556, 336, 1008, 8, 363, 1087, 1027, 282, 1178, 401, 522, 387, 518, 660, 1162, 981, 489, 1112, 13, 873, 160, 824, 509, 540, 692, 656, 427, 733, 887, 41, 295, 611, 658, 1124, 51, 161, 591, 720, 372, 682, 30, 1147, 651, 417, 788, 632, 419, 16, 502, 236, 294, 165, 848, 451, 871, 986, 797, 565, 145, 484, 396, 1131, 838, 550, 714, 499, 31, 1017, 11, 971, 36, 974, 65, 1057, 949, 928, 626, 1187, 963, 157, 539, 536, 1039, 316, 231, 825, 922, 439, 180, 123, 699, 721, 70, 1025, 14, 126, 301, 113, 24, 575, 579, 212, 1051, 667, 1004, 1016, 1023, 206, 967, 34, 272, 1154, 961, 806, 1183, 493, 1068, 425, 89, 606, 1096, 368, 108, 150, 1005, 1069, 17, 1014, 866, 807, 1030, 264, 366, 244, 736, 798, 713, 159, 422, 102, 99, 1169, 414, 1088, 346, 87, 510, 800, 669, 278, 976, 725, 340, 370, 598, 77, 842, 251, 858, 121, 0, 1176, 772, 852, 892, 615, 590, 1047, 1151, 198, 930, 546, 1111, 1161, 950, 958, 835, 906, 740, 151, 46, 265, 1032, 894, 491, 944, 178, 107, 537, 911, 548, 426, 737, 901, 473, 1060, 6, 365, 1122, 879, 203, 869, 1202, 758, 888, 1134, 810, 1208, 948, 229, 391, 453, 307, 1211, 870, 1195, 1022, 224, 680, 37, 520, 1168, 917, 1213, 177, 940, 989, 581, 171, 405, 455, 382, 72, 415, 1129, 463, 83, 310, 1207, 1093, 348, 1136, 513, 732, 954, 524, 637, 659, 252, 1138, 534, 66, 571, 369, 103, 645, 68, 795, 920, 10, 605, 138, 61, 738, 544, 477, 377, 90, 179, 124, 878, 551, 466, 131, 679, 321, 300, 1103, 693, 411, 1209, 649, 1149, 802, 751, 1050, 42, 567, 104, 1013, 654, 1175, 74, 400, 754, 29, 437, 1139, 188, 271, 619, 59, 538, 196, 33, 60, 1078, 623, 221, 588, 822, 1126, 985, 482, 428, 500, 5, 861, 322, 148, 898, 764, 1046, 1133, 443, 726, 139, 1115, 993, 1072, 596, 990, 559, 339, 1120, 35, 794, 242, 12, 1212, 782, 1192, 1024, 568, 193, 585, 353, 952, 1201, 78, 962, 144, 1166, 469, 553, 816, 584, 722, 832, 115, 192, 902, 831, 507, 88, 347, 597, 1101, 1079, 1084, 698, 616, 94, 586, 284, 786, 343, 843, 334, 607, 456, 1110, 476, 125, 875, 385, 215, 601, 261, 784, 528, 327, 82, 1148, 140, 840, 176, 84, 222, 805, 600, 675, 290, 361, 1104, 75, 1091, 53, 602, 238, 627, 663, 1006, 926, 479, 1196, 665, 1206, 397, 155, 189, 1021, 127, 460, 1095, 612, 1099, 770, 163, 1140, 1205, 854, 1012, 563, 760, 134, 743, 1203, 648, 3, 174, 996, 1118, 766, 1119, 130, 1130, 979, 628, 849, 440, 941, 357, 661, 73, 288, 1028, 604, 549, 776, 246, 943, 199, 1102, 485, 257, 4, 897, 613, 833, 729, 701, 386, 1214, 394, 674, 287, 429, 953, 937, 991, 511, 691, 592, 889, 1011, 478, 267, 58, 240, 291, 253, 712, 1037, 132, 213, 128, 599, 197, 207, 187, 297, 433, 884, 364, 1163, 716, 896, 344, 283, 1190, 910, 907, 136, 308, 1123, 1107, 86, 603, 1185, 812, 777, 1194, 583, 609, 765, 243, 19, 407, 959, 545, 44, 771, 998, 435, 724, 9, 992, 67, 1170, 97, 379, 289, 1074, 755, 318, 184, 1090, 804, 241, 182, 208, 389, 20, 984, 936, 730, 323, 416, 653, 569, 446, 541, 530, 481, 117, 595, 844, 678, 631, 92, 219, 1083, 1153, 1042, 841, 375, 762, 773, 256, 438, 269, 302, 969, 683, 742, 515, 1165, 808, 409, 374, 531, 1171, 494, 1174, 1145, 704, 865, 245, 248, 731, 1156, 1152, 200, 63, 542, 158, 719, 434, 441, 296, 1181, 360, 915, 85, 358, 987, 668, 56, 232, 373, 552, 710, 237, 570, 181, 420, 349, 458, 636, 345, 395, 98, 687, 526]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  751859937684772
the save name prefix for this run is:  chkpt-ID_751859937684772_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 933
rank avg (pred): 0.425 +- 0.007
mrr vals (pred, true): 0.017, 0.087
batch losses (mrrl, rdl): 0.0, 0.000375508

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 29
rank avg (pred): 0.043 +- 0.012
mrr vals (pred, true): 0.161, 0.601
batch losses (mrrl, rdl): 0.0, 9.5551e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 861
rank avg (pred): 0.324 +- 0.243
mrr vals (pred, true): 0.134, 0.081
batch losses (mrrl, rdl): 0.0, 8.8396e-06

Epoch over!
epoch time: 12.049

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 673
rank avg (pred): 0.411 +- 0.264
mrr vals (pred, true): 0.104, 0.041
batch losses (mrrl, rdl): 0.0, 4.9497e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1193
rank avg (pred): 0.405 +- 0.272
mrr vals (pred, true): 0.115, 0.039
batch losses (mrrl, rdl): 0.0, 1.75292e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 841
rank avg (pred): 0.357 +- 0.268
mrr vals (pred, true): 0.134, 0.080
batch losses (mrrl, rdl): 0.0, 0.000100566

Epoch over!
epoch time: 11.961

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 227
rank avg (pred): 0.317 +- 0.253
mrr vals (pred, true): 0.161, 0.042
batch losses (mrrl, rdl): 0.0, 0.000241822

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 732
rank avg (pred): 0.015 +- 0.042
mrr vals (pred, true): 0.612, 0.513
batch losses (mrrl, rdl): 0.0, 8.5728e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 691
rank avg (pred): 0.408 +- 0.263
mrr vals (pred, true): 0.110, 0.041
batch losses (mrrl, rdl): 0.0, 1.2514e-06

Epoch over!
epoch time: 12.06

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1159
rank avg (pred): 0.210 +- 0.229
mrr vals (pred, true): 0.208, 0.155
batch losses (mrrl, rdl): 0.0, 8.26e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 242
rank avg (pred): 0.325 +- 0.259
mrr vals (pred, true): 0.169, 0.044
batch losses (mrrl, rdl): 0.0, 0.0001384762

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 631
rank avg (pred): 0.420 +- 0.265
mrr vals (pred, true): 0.101, 0.035
batch losses (mrrl, rdl): 0.0, 5.7851e-06

Epoch over!
epoch time: 11.879

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 711
rank avg (pred): 0.407 +- 0.259
mrr vals (pred, true): 0.109, 0.040
batch losses (mrrl, rdl): 0.0, 7.2569e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 363
rank avg (pred): 0.390 +- 0.274
mrr vals (pred, true): 0.109, 0.091
batch losses (mrrl, rdl): 0.0, 0.0002100743

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 637
rank avg (pred): 0.433 +- 0.263
mrr vals (pred, true): 0.098, 0.036
batch losses (mrrl, rdl): 0.0, 5.4275e-06

Epoch over!
epoch time: 12.015

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 408
rank avg (pred): 0.325 +- 0.264
mrr vals (pred, true): 0.182, 0.038
batch losses (mrrl, rdl): 0.1752889305, 0.000211639

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 947
rank avg (pred): 0.510 +- 0.318
mrr vals (pred, true): 0.051, 0.040
batch losses (mrrl, rdl): 1.29025e-05, 3.94419e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 833
rank avg (pred): 0.281 +- 0.433
mrr vals (pred, true): 0.557, 0.552
batch losses (mrrl, rdl): 0.000179486, 0.0012845137

Epoch over!
epoch time: 12.474

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 582
rank avg (pred): 0.519 +- 0.300
mrr vals (pred, true): 0.034, 0.040
batch losses (mrrl, rdl): 0.0026174928, 0.0001230031

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 233
rank avg (pred): 0.445 +- 0.370
mrr vals (pred, true): 0.100, 0.034
batch losses (mrrl, rdl): 0.0248406492, 8.7897e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 334
rank avg (pred): 0.469 +- 0.329
mrr vals (pred, true): 0.056, 0.127
batch losses (mrrl, rdl): 0.0500813685, 0.0010026837

Epoch over!
epoch time: 12.194

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1047
rank avg (pred): 0.473 +- 0.369
mrr vals (pred, true): 0.086, 0.040
batch losses (mrrl, rdl): 0.0131409261, 6.19993e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 541
rank avg (pred): 0.459 +- 0.401
mrr vals (pred, true): 0.110, 0.069
batch losses (mrrl, rdl): 0.0357753746, 0.0002183239

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1208
rank avg (pred): 0.467 +- 0.361
mrr vals (pred, true): 0.073, 0.035
batch losses (mrrl, rdl): 0.0051705381, 6.90592e-05

Epoch over!
epoch time: 12.139

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 275
rank avg (pred): 0.289 +- 0.431
mrr vals (pred, true): 0.617, 0.608
batch losses (mrrl, rdl): 0.0008073313, 0.0014574233

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1102
rank avg (pred): 0.449 +- 0.362
mrr vals (pred, true): 0.100, 0.142
batch losses (mrrl, rdl): 0.0176894423, 0.0008245167

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 724
rank avg (pred): 0.380 +- 0.278
mrr vals (pred, true): 0.046, 0.038
batch losses (mrrl, rdl): 0.0001632108, 0.0001042694

Epoch over!
epoch time: 12.357

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 551
rank avg (pred): 0.412 +- 0.375
mrr vals (pred, true): 0.087, 0.117
batch losses (mrrl, rdl): 0.0089618694, 0.0002224838

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 569
rank avg (pred): 0.356 +- 0.252
mrr vals (pred, true): 0.050, 0.044
batch losses (mrrl, rdl): 8.501e-07, 0.0001240264

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 584
rank avg (pred): 0.312 +- 0.176
mrr vals (pred, true): 0.045, 0.042
batch losses (mrrl, rdl): 0.0002645798, 0.0003131969

Epoch over!
epoch time: 12.194

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 969
rank avg (pred): 0.316 +- 0.149
mrr vals (pred, true): 0.055, 0.046
batch losses (mrrl, rdl): 0.0002522397, 0.0003034247

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1193
rank avg (pred): 0.303 +- 0.142
mrr vals (pred, true): 0.049, 0.039
batch losses (mrrl, rdl): 1.45689e-05, 0.0004628713

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 961
rank avg (pred): 0.323 +- 0.147
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 5.42023e-05, 0.0002964407

Epoch over!
epoch time: 12.27

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 317
rank avg (pred): 0.114 +- 0.191
mrr vals (pred, true): 0.646, 0.635
batch losses (mrrl, rdl): 0.0011169696, 0.0002773383

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 600
rank avg (pred): 0.336 +- 0.203
mrr vals (pred, true): 0.040, 0.032
batch losses (mrrl, rdl): 0.0010093884, 0.0002398906

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 985
rank avg (pred): 0.154 +- 0.253
mrr vals (pred, true): 0.613, 0.619
batch losses (mrrl, rdl): 0.0003437839, 0.0004799966

Epoch over!
epoch time: 12.18

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 229
rank avg (pred): 0.333 +- 0.197
mrr vals (pred, true): 0.059, 0.044
batch losses (mrrl, rdl): 0.0008578725, 0.0001721519

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 433
rank avg (pred): 0.316 +- 0.186
mrr vals (pred, true): 0.070, 0.041
batch losses (mrrl, rdl): 0.0038492209, 0.0003568495

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 589
rank avg (pred): 0.336 +- 0.214
mrr vals (pred, true): 0.043, 0.038
batch losses (mrrl, rdl): 0.0004722405, 0.0002109448

Epoch over!
epoch time: 12.022

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 139
rank avg (pred): 0.364 +- 0.226
mrr vals (pred, true): 0.059, 0.090
batch losses (mrrl, rdl): 0.0008042234, 0.0001396236

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 244
rank avg (pred): 0.132 +- 0.167
mrr vals (pred, true): 0.535, 0.541
batch losses (mrrl, rdl): 0.000388188, 0.0003124625

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 911
rank avg (pred): 0.231 +- 0.284
mrr vals (pred, true): 0.309, 0.251
batch losses (mrrl, rdl): 0.0332139917, 0.000518028

Epoch over!
epoch time: 12.261

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 488
rank avg (pred): 0.324 +- 0.285
mrr vals (pred, true): 0.104, 0.112
batch losses (mrrl, rdl): 0.0006558133, 3.20884e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 55
rank avg (pred): 0.169 +- 0.247
mrr vals (pred, true): 0.555, 0.542
batch losses (mrrl, rdl): 0.0018298392, 0.0005160174

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 549
rank avg (pred): 0.416 +- 0.281
mrr vals (pred, true): 0.057, 0.061
batch losses (mrrl, rdl): 0.0004936512, 5.81284e-05

Epoch over!
epoch time: 12.294

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.327 +- 0.201
mrr vals (pred, true): 0.058, 0.097

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04826 	 0.03374 	 ~...
    7 	     1 	 0.05026 	 0.03418 	 ~...
   63 	     2 	 0.09071 	 0.03430 	 m..s
   14 	     3 	 0.05423 	 0.03475 	 ~...
   10 	     4 	 0.05284 	 0.03550 	 ~...
   13 	     5 	 0.05408 	 0.03555 	 ~...
   57 	     6 	 0.08027 	 0.03678 	 m..s
   30 	     7 	 0.06233 	 0.03680 	 ~...
   77 	     8 	 0.12032 	 0.03691 	 m..s
   23 	     9 	 0.05731 	 0.03692 	 ~...
   44 	    10 	 0.06634 	 0.03735 	 ~...
   39 	    11 	 0.06441 	 0.03743 	 ~...
   15 	    12 	 0.05423 	 0.03754 	 ~...
   21 	    13 	 0.05699 	 0.03757 	 ~...
   55 	    14 	 0.07074 	 0.03762 	 m..s
   61 	    15 	 0.08412 	 0.03809 	 m..s
   29 	    16 	 0.06170 	 0.03823 	 ~...
    0 	    17 	 0.04243 	 0.03829 	 ~...
   36 	    18 	 0.06332 	 0.03836 	 ~...
   42 	    19 	 0.06581 	 0.03913 	 ~...
   62 	    20 	 0.08759 	 0.03922 	 m..s
   66 	    21 	 0.09228 	 0.03931 	 m..s
    2 	    22 	 0.04768 	 0.03957 	 ~...
   56 	    23 	 0.07349 	 0.04003 	 m..s
   32 	    24 	 0.06277 	 0.04036 	 ~...
   74 	    25 	 0.11208 	 0.04036 	 m..s
   52 	    26 	 0.06986 	 0.04062 	 ~...
   27 	    27 	 0.05972 	 0.04063 	 ~...
   59 	    28 	 0.08217 	 0.04078 	 m..s
   31 	    29 	 0.06268 	 0.04141 	 ~...
   26 	    30 	 0.05890 	 0.04149 	 ~...
   11 	    31 	 0.05295 	 0.04177 	 ~...
   16 	    32 	 0.05467 	 0.04182 	 ~...
   76 	    33 	 0.11735 	 0.04207 	 m..s
   40 	    34 	 0.06485 	 0.04213 	 ~...
   65 	    35 	 0.09221 	 0.04227 	 m..s
    6 	    36 	 0.04846 	 0.04239 	 ~...
   18 	    37 	 0.05480 	 0.04263 	 ~...
   34 	    38 	 0.06308 	 0.04326 	 ~...
   53 	    39 	 0.07010 	 0.04347 	 ~...
   69 	    40 	 0.09664 	 0.04371 	 m..s
   47 	    41 	 0.06712 	 0.04373 	 ~...
   51 	    42 	 0.06910 	 0.04598 	 ~...
   12 	    43 	 0.05360 	 0.04650 	 ~...
    9 	    44 	 0.05263 	 0.07290 	 ~...
   37 	    45 	 0.06366 	 0.07328 	 ~...
    3 	    46 	 0.04801 	 0.07655 	 ~...
   22 	    47 	 0.05730 	 0.08267 	 ~...
    8 	    48 	 0.05212 	 0.08279 	 m..s
   38 	    49 	 0.06370 	 0.08284 	 ~...
   49 	    50 	 0.06825 	 0.08385 	 ~...
    1 	    51 	 0.04733 	 0.08426 	 m..s
   50 	    52 	 0.06851 	 0.08469 	 ~...
   17 	    53 	 0.05474 	 0.08472 	 ~...
    4 	    54 	 0.04820 	 0.08736 	 m..s
   45 	    55 	 0.06655 	 0.08752 	 ~...
   46 	    56 	 0.06663 	 0.08775 	 ~...
   24 	    57 	 0.05743 	 0.08850 	 m..s
   35 	    58 	 0.06312 	 0.08924 	 ~...
   48 	    59 	 0.06764 	 0.08956 	 ~...
   28 	    60 	 0.05987 	 0.08962 	 ~...
   20 	    61 	 0.05673 	 0.09009 	 m..s
   54 	    62 	 0.07066 	 0.09072 	 ~...
   19 	    63 	 0.05609 	 0.09253 	 m..s
   67 	    64 	 0.09240 	 0.09271 	 ~...
   43 	    65 	 0.06611 	 0.09495 	 ~...
   33 	    66 	 0.06305 	 0.09552 	 m..s
   25 	    67 	 0.05761 	 0.09672 	 m..s
   41 	    68 	 0.06515 	 0.09724 	 m..s
   81 	    69 	 0.15506 	 0.10855 	 m..s
   60 	    70 	 0.08359 	 0.10900 	 ~...
   58 	    71 	 0.08160 	 0.11414 	 m..s
   68 	    72 	 0.09495 	 0.11936 	 ~...
   64 	    73 	 0.09129 	 0.11998 	 ~...
   79 	    74 	 0.13313 	 0.12112 	 ~...
   78 	    75 	 0.12166 	 0.12177 	 ~...
   73 	    76 	 0.11077 	 0.13270 	 ~...
   75 	    77 	 0.11474 	 0.13341 	 ~...
   72 	    78 	 0.10514 	 0.13923 	 m..s
   71 	    79 	 0.10061 	 0.14096 	 m..s
   80 	    80 	 0.14711 	 0.15001 	 ~...
   82 	    81 	 0.16523 	 0.15149 	 ~...
   70 	    82 	 0.09856 	 0.15175 	 m..s
   84 	    83 	 0.17330 	 0.15235 	 ~...
   83 	    84 	 0.17223 	 0.21459 	 m..s
   85 	    85 	 0.26712 	 0.30109 	 m..s
   87 	    86 	 0.26818 	 0.33009 	 m..s
   86 	    87 	 0.26753 	 0.33119 	 m..s
   94 	    88 	 0.52398 	 0.43013 	 m..s
   96 	    89 	 0.52879 	 0.43062 	 m..s
   95 	    90 	 0.52437 	 0.44759 	 m..s
   97 	    91 	 0.53022 	 0.44884 	 m..s
   88 	    92 	 0.46657 	 0.48792 	 ~...
   90 	    93 	 0.48726 	 0.49823 	 ~...
   92 	    94 	 0.51432 	 0.51365 	 ~...
   91 	    95 	 0.51391 	 0.52169 	 ~...
   89 	    96 	 0.48500 	 0.53204 	 m..s
  104 	    97 	 0.58097 	 0.53235 	 m..s
   93 	    98 	 0.51641 	 0.53731 	 ~...
   98 	    99 	 0.53194 	 0.54431 	 ~...
   99 	   100 	 0.55326 	 0.54621 	 ~...
  100 	   101 	 0.56865 	 0.55618 	 ~...
  102 	   102 	 0.57072 	 0.55704 	 ~...
  101 	   103 	 0.56976 	 0.55759 	 ~...
  106 	   104 	 0.59210 	 0.56204 	 m..s
  105 	   105 	 0.58306 	 0.56450 	 ~...
  103 	   106 	 0.57704 	 0.57608 	 ~...
  114 	   107 	 0.61714 	 0.58207 	 m..s
  109 	   108 	 0.61040 	 0.59282 	 ~...
  119 	   109 	 0.62525 	 0.60240 	 ~...
  115 	   110 	 0.61813 	 0.60871 	 ~...
  116 	   111 	 0.61858 	 0.61123 	 ~...
  110 	   112 	 0.61099 	 0.61133 	 ~...
  118 	   113 	 0.62452 	 0.61250 	 ~...
  111 	   114 	 0.61369 	 0.61471 	 ~...
  117 	   115 	 0.62351 	 0.61485 	 ~...
  120 	   116 	 0.62695 	 0.61509 	 ~...
  112 	   117 	 0.61684 	 0.61837 	 ~...
  107 	   118 	 0.59679 	 0.61839 	 ~...
  113 	   119 	 0.61700 	 0.62217 	 ~...
  108 	   120 	 0.60600 	 0.62257 	 ~...
==========================================
r_mrr = 0.9889063835144043
r2_mrr = 0.9769214987754822
spearmanr_mrr@5 = 0.9217970371246338
spearmanr_mrr@10 = 0.9583402872085571
spearmanr_mrr@50 = 0.9945394992828369
spearmanr_mrr@100 = 0.9965113997459412
spearmanr_mrr@All = 0.9966714382171631
==========================================
test time: 0.394
Done Testing dataset UMLS
total time taken: 188.54273986816406
training time taken: 182.81723976135254
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9889)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9769)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9218)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9583)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9945)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9965)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9967)}}, 'test_loss': {'TransE': {'UMLS': 1.0384315947978848}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o num neighbnours', 's num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 1044488866231566
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1045, 996, 752, 477, 1061, 127, 145, 324, 868, 1004, 980, 282, 737, 1054, 670, 596, 1204, 555, 1034, 807, 566, 1105, 161, 574, 901, 428, 172, 8, 887, 260, 736, 358, 496, 768, 421, 657, 1002, 668, 157, 1026, 683, 564, 252, 875, 588, 397, 415, 95, 21, 794, 1152, 375, 822, 296, 18, 32, 389, 528, 4, 1166, 1062, 678, 224, 616, 539, 398, 171, 193, 823, 101, 480, 602, 898, 207, 427, 1114, 653, 303, 766, 1146, 1209, 640, 939, 56, 895, 677, 94, 99, 442, 1214, 1, 867, 618, 824, 1174, 356, 937, 318, 917, 9, 724, 628, 655, 1092, 904, 634, 612, 785, 133, 658, 239, 1135, 847, 940, 832, 580, 543, 636, 842, 723, 601]
valid_ids (0): []
train_ids (1094): [347, 363, 294, 877, 33, 355, 1097, 827, 242, 423, 1190, 290, 212, 110, 385, 926, 744, 31, 135, 142, 43, 1048, 283, 26, 447, 335, 492, 75, 179, 124, 441, 302, 816, 746, 1158, 188, 341, 617, 1159, 745, 406, 367, 988, 629, 1212, 525, 701, 764, 35, 295, 1084, 976, 568, 696, 1155, 586, 931, 989, 160, 413, 440, 198, 219, 1180, 865, 792, 644, 565, 394, 638, 719, 675, 818, 153, 422, 776, 372, 1094, 647, 469, 512, 377, 154, 278, 29, 460, 246, 905, 569, 292, 424, 689, 864, 797, 250, 273, 203, 1178, 165, 945, 47, 221, 495, 1169, 206, 825, 1070, 790, 16, 1038, 1142, 84, 177, 620, 15, 238, 520, 393, 609, 591, 134, 891, 614, 882, 479, 92, 382, 1203, 481, 370, 1027, 1138, 1102, 626, 829, 914, 30, 1015, 1132, 264, 1069, 557, 80, 1201, 848, 1085, 409, 130, 400, 274, 879, 83, 763, 269, 184, 881, 187, 468, 606, 354, 772, 1023, 12, 942, 115, 3, 656, 838, 1012, 299, 116, 201, 1091, 787, 774, 1161, 987, 364, 28, 232, 540, 930, 906, 654, 690, 458, 464, 851, 146, 831, 1068, 953, 225, 777, 288, 353, 78, 652, 418, 811, 575, 208, 907, 50, 77, 1148, 245, 861, 622, 54, 156, 51, 25, 671, 333, 1103, 513, 563, 809, 1000, 1183, 1195, 204, 1107, 36, 584, 1167, 799, 974, 401, 682, 371, 781, 497, 680, 106, 476, 1003, 1189, 840, 552, 708, 39, 532, 1181, 137, 1087, 465, 44, 1207, 470, 681, 843, 272, 1039, 268, 973, 812, 326, 784, 503, 1050, 1080, 852, 969, 359, 1041, 151, 241, 317, 615, 711, 649, 494, 585, 804, 265, 873, 510, 722, 605, 700, 6, 404, 1144, 1006, 795, 839, 337, 491, 801, 1131, 1020, 573, 925, 14, 780, 1013, 841, 1210, 360, 1205, 600, 408, 506, 863, 651, 966, 1150, 1188, 769, 985, 498, 410, 381, 1018, 211, 734, 862, 633, 1024, 941, 709, 79, 163, 920, 718, 185, 61, 886, 261, 1196, 248, 523, 1047, 558, 10, 1119, 1074, 548, 453, 791, 511, 924, 170, 1170, 577, 1078, 820, 998, 439, 277, 527, 489, 22, 287, 1162, 1049, 1184, 280, 817, 448, 553, 169, 88, 1096, 338, 733, 1056, 771, 305, 1077, 1194, 844, 182, 915, 125, 1008, 281, 541, 738, 392, 673, 872, 289, 55, 155, 762, 68, 627, 461, 478, 661, 484, 900, 902, 805, 1187, 229, 107, 666, 1206, 630, 183, 175, 200, 674, 226, 316, 1057, 405, 1083, 1089, 70, 1106, 1186, 603, 703, 1125, 1136, 632, 102, 452, 454, 304, 432, 459, 425, 921, 343, 908, 991, 625, 725, 685, 810, 395, 71, 1176, 971, 223, 387, 166, 149, 132, 645, 164, 910, 430, 128, 978, 414, 173, 1192, 1033, 800, 378, 159, 451, 582, 1028, 730, 209, 63, 195, 1075, 443, 1177, 147, 830, 803, 234, 814, 118, 947, 715, 748, 1011, 150, 1073, 399, 899, 779, 849, 581, 19, 275, 467, 213, 714, 913, 312, 589, 379, 554, 485, 117, 1182, 687, 1055, 426, 1072, 802, 643, 202, 300, 707, 216, 950, 948, 648, 97, 136, 667, 366, 598, 191, 883, 89, 753, 619, 858, 694, 73, 328, 49, 1154, 435, 693, 1172, 501, 309, 1160, 48, 108, 631, 610, 728, 1139, 58, 487, 60, 253, 1213, 1115, 438, 1140, 949, 1052, 521, 716, 1137, 897, 1151, 433, 702, 332, 981, 144, 650, 7, 933, 1199, 514, 505, 1021, 977, 1036, 217, 0, 1198, 1208, 967, 912, 621, 306, 123, 104, 1014, 559, 999, 1164, 1168, 502, 594, 982, 607, 1126, 662, 551, 320, 826, 599, 853, 37, 416, 313, 257, 168, 798, 271, 876, 919, 932, 972, 293, 383, 1043, 1143, 240, 178, 420, 1095, 517, 53, 534, 1082, 1079, 436, 1202, 1127, 1040, 330, 1029, 608, 138, 340, 105, 986, 1122, 466, 391, 1112, 970, 993, 922, 373, 754, 199, 263, 390, 402, 819, 344, 374, 361, 41, 778, 595, 1076, 870, 297, 1156, 994, 957, 279, 325, 562, 833, 960, 255, 936, 793, 522, 815, 903, 1053, 749, 888, 775, 943, 231, 916, 500, 1117, 1134, 958, 93, 944, 96, 315, 531, 515, 1120, 592, 992, 65, 834, 646, 866, 46, 604, 751, 59, 66, 1153, 2, 42, 230, 82, 270, 90, 721, 894, 664, 856, 859, 549, 729, 334, 176, 896, 91, 788, 742, 783, 327, 1065, 663, 561, 686, 13, 869, 141, 597, 975, 342, 1108, 113, 542, 1086, 533, 109, 419, 688, 57, 961, 11, 1060, 1130, 412, 1171, 720, 1116, 345, 483, 194, 556, 1200, 472, 590, 880, 307, 417, 997, 923, 1110, 139, 1059, 119, 362, 570, 482, 946, 86, 535, 301, 488, 965, 298, 938, 699, 210, 1175, 786, 45, 1191, 267, 773, 676, 52, 1064, 34, 835, 504, 72, 756, 310, 1007, 254, 747, 291, 111, 1104, 544, 1147, 437, 695, 1017, 5, 205, 403, 1037, 350, 81, 131, 152, 1001, 143, 368, 782, 929, 739, 457, 836, 259, 1128, 486, 1051, 761, 235, 1044, 576, 311, 1113, 1099, 321, 509, 243, 962, 1179, 471, 20, 1118, 572, 963, 64, 190, 351, 74, 526, 329, 1046, 679, 38, 1123, 704, 984, 705, 1025, 1185, 613, 623, 755, 684, 323, 434, 806, 1032, 537, 918, 956, 357, 909, 507, 1173, 1081, 174, 1009, 692, 979, 186, 384, 571, 431, 770, 928, 284, 1163, 697, 256, 1031, 854, 456, 710, 624, 251, 1035, 1121, 247, 98, 473, 122, 346, 1005, 837, 499, 121, 611, 796, 855, 935, 158, 757, 740, 1124, 1109, 369, 536, 884, 927, 964, 735, 579, 322, 518, 743, 713, 197, 845, 545, 24, 336, 103, 237, 1090, 878, 717, 114, 23, 990, 821, 1098, 893, 760, 758, 712, 167, 560, 181, 641, 1157, 860, 892, 983, 808, 429, 1016, 706, 1030, 493, 550, 215, 529, 1100, 1042, 727, 214, 228, 446, 635, 463, 857, 349, 189, 750, 593, 959, 233, 954, 1145, 1022, 1071, 17, 1129, 126, 192, 995, 871, 1101, 911, 27, 885, 444, 474, 388, 1093, 352, 726, 266, 731, 87, 530, 951, 741, 286, 1193, 218, 450, 1063, 516, 850, 789, 220, 376, 129, 660, 462, 319, 828, 546, 583, 698, 547, 538, 180, 767, 1067, 40, 952, 1088, 85, 227, 258, 249, 339, 196, 759, 455, 691, 813, 519, 162, 1058, 244, 732, 968, 148, 1133, 407, 411, 934, 380, 765, 672, 639, 314, 490, 348, 236, 120, 112, 140, 1149, 396, 1019, 331, 308, 874, 222, 1211, 642, 846, 100, 445, 1111, 955, 67, 69, 524, 76, 669, 665, 889, 365, 1197, 578, 637, 1141, 890, 62, 386, 1165, 587, 449, 475, 262, 1066, 285, 276, 1010, 508, 567, 659]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1104067080804520
the save name prefix for this run is:  chkpt-ID_1104067080804520_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 443
rank avg (pred): 0.631 +- 0.004
mrr vals (pred, true): 0.012, 0.044
batch losses (mrrl, rdl): 0.0, 0.0008160701

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 689
rank avg (pred): 0.369 +- 0.237
mrr vals (pred, true): 0.180, 0.040
batch losses (mrrl, rdl): 0.0, 5.06237e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 370
rank avg (pred): 0.338 +- 0.246
mrr vals (pred, true): 0.286, 0.115
batch losses (mrrl, rdl): 0.0, 0.0001963387

Epoch over!
epoch time: 12.118

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 108
rank avg (pred): 0.358 +- 0.254
mrr vals (pred, true): 0.265, 0.079
batch losses (mrrl, rdl): 0.0, 0.0002057903

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 504
rank avg (pred): 0.296 +- 0.219
mrr vals (pred, true): 0.290, 0.073
batch losses (mrrl, rdl): 0.0, 9.9637e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 763
rank avg (pred): 0.325 +- 0.236
mrr vals (pred, true): 0.264, 0.067
batch losses (mrrl, rdl): 0.0, 1.37125e-05

Epoch over!
epoch time: 11.845

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 199
rank avg (pred): 0.349 +- 0.248
mrr vals (pred, true): 0.255, 0.034
batch losses (mrrl, rdl): 0.0, 0.0001410137

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 873
rank avg (pred): 0.338 +- 0.247
mrr vals (pred, true): 0.256, 0.036
batch losses (mrrl, rdl): 0.0, 0.000152086

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 294
rank avg (pred): 0.026 +- 0.020
mrr vals (pred, true): 0.465, 0.513
batch losses (mrrl, rdl): 0.0, 3.044e-07

Epoch over!
epoch time: 11.895

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1181
rank avg (pred): 0.354 +- 0.274
mrr vals (pred, true): 0.318, 0.033
batch losses (mrrl, rdl): 0.0, 6.81698e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 327
rank avg (pred): 0.330 +- 0.258
mrr vals (pred, true): 0.328, 0.085
batch losses (mrrl, rdl): 0.0, 0.0001163229

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 967
rank avg (pred): 0.351 +- 0.276
mrr vals (pred, true): 0.311, 0.042
batch losses (mrrl, rdl): 0.0, 9.19066e-05

Epoch over!
epoch time: 12.022

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1058
rank avg (pred): 0.046 +- 0.038
mrr vals (pred, true): 0.437, 0.624
batch losses (mrrl, rdl): 0.0, 1.48267e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 108
rank avg (pred): 0.360 +- 0.257
mrr vals (pred, true): 0.267, 0.079
batch losses (mrrl, rdl): 0.0, 0.0002131703

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 374
rank avg (pred): 0.304 +- 0.240
mrr vals (pred, true): 0.309, 0.154
batch losses (mrrl, rdl): 0.0, 0.0002224853

Epoch over!
epoch time: 11.978

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 120
rank avg (pred): 0.334 +- 0.247
mrr vals (pred, true): 0.268, 0.085
batch losses (mrrl, rdl): 0.4761159718, 0.0001401802

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 339
rank avg (pred): 0.415 +- 0.206
mrr vals (pred, true): 0.081, 0.091
batch losses (mrrl, rdl): 0.0094852773, 0.0004194553

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 380
rank avg (pred): 0.373 +- 0.209
mrr vals (pred, true): 0.096, 0.131
batch losses (mrrl, rdl): 0.0123679182, 0.0004065037

Epoch over!
epoch time: 12.403

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 789
rank avg (pred): 0.442 +- 0.193
mrr vals (pred, true): 0.058, 0.039
batch losses (mrrl, rdl): 0.0006032628, 1.84362e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 256
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.526, 0.556
batch losses (mrrl, rdl): 0.0087988097, 6.713e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 208
rank avg (pred): 0.421 +- 0.201
mrr vals (pred, true): 0.076, 0.040
batch losses (mrrl, rdl): 0.0065200496, 1.59418e-05

Epoch over!
epoch time: 12.167

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 429
rank avg (pred): 0.389 +- 0.208
mrr vals (pred, true): 0.085, 0.038
batch losses (mrrl, rdl): 0.0119177476, 3.73855e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 514
rank avg (pred): 0.350 +- 0.195
mrr vals (pred, true): 0.101, 0.086
batch losses (mrrl, rdl): 0.0259233266, 2.8908e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1044
rank avg (pred): 0.390 +- 0.205
mrr vals (pred, true): 0.087, 0.038
batch losses (mrrl, rdl): 0.0135816708, 4.94147e-05

Epoch over!
epoch time: 12.141

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 19
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.545, 0.547
batch losses (mrrl, rdl): 4.75439e-05, 5.3547e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 510
rank avg (pred): 0.412 +- 0.180
mrr vals (pred, true): 0.068, 0.073
batch losses (mrrl, rdl): 0.0031546645, 0.0002615867

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 220
rank avg (pred): 0.435 +- 0.173
mrr vals (pred, true): 0.065, 0.037
batch losses (mrrl, rdl): 0.002113706, 2.53245e-05

Epoch over!
epoch time: 12.253

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 130
rank avg (pred): 0.425 +- 0.176
mrr vals (pred, true): 0.064, 0.094
batch losses (mrrl, rdl): 0.0019590578, 0.0006295989

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1124
rank avg (pred): 0.344 +- 0.221
mrr vals (pred, true): 0.133, 0.043
batch losses (mrrl, rdl): 0.069032833, 0.0001039664

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 466
rank avg (pred): 0.449 +- 0.145
mrr vals (pred, true): 0.051, 0.038
batch losses (mrrl, rdl): 6.4093e-06, 4.96991e-05

Epoch over!
epoch time: 12.128

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 262
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.544, 0.550
batch losses (mrrl, rdl): 0.0003842172, 5.0928e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1127
rank avg (pred): 0.347 +- 0.208
mrr vals (pred, true): 0.109, 0.039
batch losses (mrrl, rdl): 0.0347023793, 0.0001458054

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 899
rank avg (pred): 0.180 +- 0.133
mrr vals (pred, true): 0.185, 0.109
batch losses (mrrl, rdl): 0.0590096787, 1.7673e-06

Epoch over!
epoch time: 12.265

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 611
rank avg (pred): 0.468 +- 0.123
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 1.3753e-06, 7.50216e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 872
rank avg (pred): 0.427 +- 0.146
mrr vals (pred, true): 0.059, 0.043
batch losses (mrrl, rdl): 0.0008665015, 3.98688e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 48
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.558, 0.505
batch losses (mrrl, rdl): 0.0275126509, 8.3822e-06

Epoch over!
epoch time: 12.071

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 770
rank avg (pred): 0.457 +- 0.123
mrr vals (pred, true): 0.052, 0.102
batch losses (mrrl, rdl): 0.0246534795, 0.0008723483

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1097
rank avg (pred): 0.358 +- 0.185
mrr vals (pred, true): 0.086, 0.158
batch losses (mrrl, rdl): 0.051463455, 0.0004376729

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 659
rank avg (pred): 0.456 +- 0.115
mrr vals (pred, true): 0.053, 0.040
batch losses (mrrl, rdl): 7.38703e-05, 6.38448e-05

Epoch over!
epoch time: 12.317

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1182
rank avg (pred): 0.426 +- 0.137
mrr vals (pred, true): 0.058, 0.038
batch losses (mrrl, rdl): 0.0007022833, 3.13138e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 820
rank avg (pred): 0.022 +- 0.018
mrr vals (pred, true): 0.469, 0.427
batch losses (mrrl, rdl): 0.0174498539, 2.29533e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 667
rank avg (pred): 0.463 +- 0.094
mrr vals (pred, true): 0.045, 0.039
batch losses (mrrl, rdl): 0.0002450971, 6.85158e-05

Epoch over!
epoch time: 12.176

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1050
rank avg (pred): 0.361 +- 0.172
mrr vals (pred, true): 0.071, 0.041
batch losses (mrrl, rdl): 0.0045118127, 0.0001257397

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 672
rank avg (pred): 0.464 +- 0.088
mrr vals (pred, true): 0.038, 0.036
batch losses (mrrl, rdl): 0.0015004248, 6.33535e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 426
rank avg (pred): 0.413 +- 0.135
mrr vals (pred, true): 0.061, 0.040
batch losses (mrrl, rdl): 0.0011589504, 4.60717e-05

Epoch over!
epoch time: 12.1

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.352 +- 0.171
mrr vals (pred, true): 0.082, 0.040

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    3 	     0 	 0.04711 	 0.03479 	 ~...
   68 	     1 	 0.07695 	 0.03488 	 m..s
    4 	     2 	 0.04745 	 0.03491 	 ~...
   20 	     3 	 0.05249 	 0.03538 	 ~...
    7 	     4 	 0.04790 	 0.03555 	 ~...
    1 	     5 	 0.04555 	 0.03569 	 ~...
   39 	     6 	 0.05613 	 0.03596 	 ~...
   76 	     7 	 0.08912 	 0.03659 	 m..s
   23 	     8 	 0.05303 	 0.03667 	 ~...
   27 	     9 	 0.05355 	 0.03673 	 ~...
    2 	    10 	 0.04660 	 0.03694 	 ~...
   18 	    11 	 0.04974 	 0.03703 	 ~...
   19 	    12 	 0.05122 	 0.03713 	 ~...
    0 	    13 	 0.04496 	 0.03723 	 ~...
   14 	    14 	 0.04954 	 0.03754 	 ~...
    5 	    15 	 0.04783 	 0.03754 	 ~...
   31 	    16 	 0.05498 	 0.03760 	 ~...
   13 	    17 	 0.04932 	 0.03766 	 ~...
    6 	    18 	 0.04789 	 0.03774 	 ~...
   25 	    19 	 0.05328 	 0.03787 	 ~...
    9 	    20 	 0.04794 	 0.03793 	 ~...
   41 	    21 	 0.05629 	 0.03808 	 ~...
   81 	    22 	 0.09530 	 0.03809 	 m..s
   22 	    23 	 0.05297 	 0.03811 	 ~...
   15 	    24 	 0.04958 	 0.03817 	 ~...
   11 	    25 	 0.04834 	 0.03882 	 ~...
   75 	    26 	 0.08739 	 0.03883 	 m..s
   12 	    27 	 0.04932 	 0.03906 	 ~...
   60 	    28 	 0.06904 	 0.03911 	 ~...
   37 	    29 	 0.05570 	 0.03913 	 ~...
   43 	    30 	 0.05680 	 0.03923 	 ~...
   40 	    31 	 0.05623 	 0.03933 	 ~...
   26 	    32 	 0.05352 	 0.03954 	 ~...
   72 	    33 	 0.08196 	 0.03963 	 m..s
   65 	    34 	 0.07040 	 0.03964 	 m..s
   28 	    35 	 0.05360 	 0.03982 	 ~...
   79 	    36 	 0.09292 	 0.03989 	 m..s
   42 	    37 	 0.05676 	 0.03992 	 ~...
   21 	    38 	 0.05255 	 0.04023 	 ~...
   64 	    39 	 0.07015 	 0.04029 	 ~...
   17 	    40 	 0.04969 	 0.04043 	 ~...
   80 	    41 	 0.09527 	 0.04078 	 m..s
   46 	    42 	 0.05874 	 0.04122 	 ~...
   36 	    43 	 0.05564 	 0.04131 	 ~...
   16 	    44 	 0.04961 	 0.04141 	 ~...
    8 	    45 	 0.04793 	 0.04229 	 ~...
   32 	    46 	 0.05545 	 0.04262 	 ~...
   61 	    47 	 0.06942 	 0.04267 	 ~...
   33 	    48 	 0.05552 	 0.04306 	 ~...
   58 	    49 	 0.06625 	 0.04319 	 ~...
   62 	    50 	 0.07006 	 0.04361 	 ~...
   29 	    51 	 0.05390 	 0.04431 	 ~...
   10 	    52 	 0.04820 	 0.04577 	 ~...
   56 	    53 	 0.06533 	 0.06286 	 ~...
   53 	    54 	 0.06325 	 0.06424 	 ~...
   55 	    55 	 0.06403 	 0.06644 	 ~...
   51 	    56 	 0.06319 	 0.06822 	 ~...
   63 	    57 	 0.07012 	 0.07479 	 ~...
   45 	    58 	 0.05762 	 0.07833 	 ~...
   30 	    59 	 0.05410 	 0.08550 	 m..s
   34 	    60 	 0.05554 	 0.08734 	 m..s
   54 	    61 	 0.06337 	 0.08773 	 ~...
   44 	    62 	 0.05718 	 0.08968 	 m..s
   52 	    63 	 0.06324 	 0.09010 	 ~...
   50 	    64 	 0.06310 	 0.09247 	 ~...
   38 	    65 	 0.05608 	 0.09451 	 m..s
   24 	    66 	 0.05323 	 0.09458 	 m..s
   35 	    67 	 0.05562 	 0.09496 	 m..s
   59 	    68 	 0.06733 	 0.09556 	 ~...
   48 	    69 	 0.06248 	 0.09677 	 m..s
   47 	    70 	 0.06207 	 0.10049 	 m..s
   57 	    71 	 0.06589 	 0.10115 	 m..s
   49 	    72 	 0.06258 	 0.10161 	 m..s
   90 	    73 	 0.21723 	 0.10185 	 MISS
   66 	    74 	 0.07292 	 0.10238 	 ~...
   69 	    75 	 0.07958 	 0.10900 	 ~...
   82 	    76 	 0.09751 	 0.11433 	 ~...
   67 	    77 	 0.07666 	 0.11491 	 m..s
   77 	    78 	 0.08969 	 0.11767 	 ~...
   86 	    79 	 0.11344 	 0.12522 	 ~...
   85 	    80 	 0.11322 	 0.12528 	 ~...
   70 	    81 	 0.07989 	 0.12606 	 m..s
   74 	    82 	 0.08516 	 0.13054 	 m..s
   73 	    83 	 0.08319 	 0.13402 	 m..s
   78 	    84 	 0.09103 	 0.13662 	 m..s
   83 	    85 	 0.10575 	 0.13923 	 m..s
   84 	    86 	 0.10674 	 0.14096 	 m..s
   71 	    87 	 0.08102 	 0.14666 	 m..s
   89 	    88 	 0.15182 	 0.14963 	 ~...
   88 	    89 	 0.15090 	 0.15001 	 ~...
   87 	    90 	 0.13644 	 0.16485 	 ~...
   91 	    91 	 0.23240 	 0.16790 	 m..s
   92 	    92 	 0.31792 	 0.24515 	 m..s
   93 	    93 	 0.32023 	 0.30550 	 ~...
   94 	    94 	 0.32276 	 0.31332 	 ~...
   96 	    95 	 0.47348 	 0.34052 	 MISS
   95 	    96 	 0.47166 	 0.36531 	 MISS
  105 	    97 	 0.52866 	 0.47351 	 m..s
  106 	    98 	 0.53417 	 0.49741 	 m..s
  102 	    99 	 0.52382 	 0.49835 	 ~...
  107 	   100 	 0.53528 	 0.50860 	 ~...
   97 	   101 	 0.52116 	 0.51098 	 ~...
  108 	   102 	 0.53843 	 0.52681 	 ~...
  104 	   103 	 0.52777 	 0.52695 	 ~...
  101 	   104 	 0.52299 	 0.52761 	 ~...
  100 	   105 	 0.52283 	 0.53780 	 ~...
   99 	   106 	 0.52270 	 0.54222 	 ~...
  109 	   107 	 0.55872 	 0.54371 	 ~...
   98 	   108 	 0.52230 	 0.54415 	 ~...
  110 	   109 	 0.55935 	 0.54438 	 ~...
  103 	   110 	 0.52499 	 0.54980 	 ~...
  117 	   111 	 0.61738 	 0.58102 	 m..s
  112 	   112 	 0.60840 	 0.60641 	 ~...
  111 	   113 	 0.60775 	 0.61150 	 ~...
  114 	   114 	 0.61315 	 0.61211 	 ~...
  119 	   115 	 0.62388 	 0.61521 	 ~...
  116 	   116 	 0.61596 	 0.62143 	 ~...
  113 	   117 	 0.61044 	 0.62217 	 ~...
  120 	   118 	 0.62535 	 0.62257 	 ~...
  118 	   119 	 0.62047 	 0.62336 	 ~...
  115 	   120 	 0.61316 	 0.62686 	 ~...
==========================================
r_mrr = 0.9875151515007019
r2_mrr = 0.9738174676895142
spearmanr_mrr@5 = 0.8617020845413208
spearmanr_mrr@10 = 0.7957311272621155
spearmanr_mrr@50 = 0.9886900186538696
spearmanr_mrr@100 = 0.9925145506858826
spearmanr_mrr@All = 0.9930354356765747
==========================================
test time: 0.507
Done Testing dataset UMLS
total time taken: 188.89191436767578
training time taken: 182.46337890625
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9875)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9738)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.8617)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.7957)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9887)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9925)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9930)}}, 'test_loss': {'TransE': {'UMLS': 1.047087177481444}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min freq rel', 's min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 5515221797571992
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 1195, 820, 194, 683, 655, 831, 348, 356, 508, 1133, 582, 1169, 622, 120, 1101, 896, 1082, 430, 772, 454, 293, 521, 956, 231, 766, 647, 28, 44, 424, 845, 522, 730, 800, 516, 916, 892, 681, 964, 108, 218, 1172, 871, 737, 426, 4, 302, 375, 1043, 250, 135, 621, 403, 668, 109, 489, 84, 178, 885, 1120, 285, 1129, 275, 714, 188, 307, 803, 79, 753, 506, 274, 207, 855, 542, 1033, 336, 951, 485, 1025, 312, 994, 172, 1023, 1180, 43, 477, 280, 71, 331, 480, 571, 286, 321, 527, 840, 707, 317, 507, 875, 821, 182, 1022, 5, 1107, 134, 578, 924, 487, 1051, 532, 911, 860, 967, 1058, 1056, 351, 806, 7, 333, 96, 284]
valid_ids (0): []
train_ids (1094): [1171, 446, 973, 689, 166, 648, 999, 889, 882, 1071, 510, 1158, 1007, 619, 54, 858, 193, 808, 775, 539, 591, 301, 1048, 963, 722, 920, 809, 852, 756, 596, 229, 1065, 258, 85, 1103, 1034, 607, 369, 742, 1000, 904, 138, 149, 1087, 927, 1038, 1057, 401, 651, 568, 798, 835, 501, 1191, 928, 22, 63, 287, 1003, 221, 1140, 853, 1092, 626, 179, 884, 1168, 711, 376, 1132, 303, 136, 822, 765, 863, 550, 678, 814, 311, 437, 304, 575, 1064, 799, 23, 1165, 1040, 502, 1046, 244, 982, 364, 197, 189, 415, 656, 530, 747, 124, 614, 104, 1181, 584, 491, 83, 78, 617, 366, 91, 757, 33, 776, 484, 1096, 763, 606, 988, 206, 434, 248, 278, 208, 387, 212, 338, 987, 1111, 670, 1167, 225, 93, 75, 975, 1066, 211, 719, 349, 443, 129, 1045, 167, 14, 627, 624, 433, 40, 946, 630, 205, 910, 932, 583, 77, 29, 613, 923, 886, 322, 25, 837, 386, 515, 700, 786, 830, 754, 482, 903, 725, 297, 748, 1123, 488, 517, 448, 1104, 620, 17, 92, 930, 483, 833, 352, 906, 439, 611, 1061, 127, 601, 745, 665, 922, 368, 761, 807, 750, 374, 51, 1115, 631, 1077, 520, 984, 940, 815, 240, 98, 458, 277, 131, 457, 848, 378, 1203, 524, 384, 460, 383, 65, 868, 123, 751, 654, 1173, 267, 1069, 818, 636, 535, 47, 423, 1073, 396, 1024, 417, 393, 173, 61, 196, 1075, 995, 657, 644, 1002, 664, 661, 350, 381, 1189, 525, 156, 459, 685, 435, 970, 771, 245, 645, 1012, 849, 890, 1183, 1067, 699, 891, 1068, 1137, 597, 1124, 866, 59, 537, 721, 263, 48, 152, 346, 1141, 812, 492, 153, 341, 314, 228, 603, 1032, 879, 315, 473, 1078, 168, 122, 695, 767, 1105, 888, 313, 1106, 810, 1028, 704, 11, 316, 1197, 816, 279, 395, 1146, 824, 1150, 6, 696, 623, 504, 1006, 309, 1080, 150, 604, 1042, 998, 743, 706, 1153, 1193, 705, 87, 406, 388, 839, 672, 580, 158, 431, 690, 133, 909, 478, 468, 877, 1142, 405, 300, 217, 804, 119, 382, 1109, 1052, 1049, 101, 496, 900, 455, 13, 358, 541, 1015, 659, 559, 58, 677, 861, 81, 702, 394, 829, 200, 662, 99, 214, 371, 1102, 213, 834, 26, 555, 529, 760, 540, 817, 407, 740, 534, 788, 1113, 1093, 2, 334, 996, 1059, 404, 1116, 1044, 1214, 842, 880, 602, 37, 838, 409, 979, 513, 294, 981, 345, 836, 19, 320, 1089, 52, 986, 254, 936, 941, 989, 1019, 634, 715, 653, 872, 219, 512, 997, 347, 789, 180, 183, 117, 640, 794, 397, 1074, 1047, 398, 115, 1206, 377, 1147, 961, 447, 605, 1054, 857, 86, 62, 174, 847, 107, 3, 42, 1005, 974, 232, 795, 697, 471, 633, 587, 268, 944, 1016, 558, 768, 971, 1010, 1182, 41, 676, 226, 980, 292, 171, 116, 778, 1151, 867, 373, 256, 486, 780, 1017, 733, 805, 526, 444, 306, 185, 577, 324, 908, 342, 1186, 10, 1163, 1081, 1205, 1127, 1185, 408, 164, 1070, 841, 339, 203, 576, 402, 1152, 694, 1076, 445, 1085, 56, 901, 440, 1201, 146, 637, 247, 972, 533, 1014, 581, 728, 893, 1091, 1121, 545, 118, 1021, 94, 514, 15, 660, 658, 693, 1190, 450, 9, 939, 145, 242, 1198, 111, 220, 210, 566, 1118, 227, 246, 1041, 421, 686, 625, 692, 1031, 1122, 355, 441, 27, 764, 1013, 243, 1139, 241, 49, 475, 472, 21, 142, 370, 429, 671, 1160, 1149, 716, 959, 1039, 30, 414, 960, 72, 698, 432, 16, 1148, 412, 914, 574, 1098, 418, 106, 67, 774, 912, 53, 8, 565, 599, 155, 462, 1063, 717, 1095, 36, 1145, 723, 466, 1188, 953, 420, 259, 389, 328, 380, 519, 64, 865, 162, 1174, 76, 428, 175, 663, 105, 562, 1020, 873, 144, 523, 792, 554, 870, 992, 463, 112, 103, 801, 948, 1192, 323, 593, 609, 669, 755, 641, 271, 400, 399, 549, 257, 813, 934, 856, 216, 736, 703, 628, 954, 1204, 456, 390, 379, 308, 639, 915, 202, 673, 195, 1060, 12, 360, 283, 586, 365, 585, 361, 925, 147, 758, 55, 262, 413, 80, 24, 467, 1001, 139, 913, 497, 735, 330, 598, 594, 187, 1187, 340, 427, 438, 125, 790, 887, 864, 531, 160, 1035, 362, 326, 260, 712, 600, 335, 1143, 165, 553, 281, 746, 894, 121, 1164, 966, 720, 1210, 359, 773, 159, 500, 926, 186, 230, 222, 547, 643, 588, 1166, 569, 744, 255, 132, 354, 827, 895, 236, 325, 151, 770, 363, 319, 688, 176, 949, 68, 1196, 223, 666, 1170, 419, 32, 169, 235, 933, 968, 201, 291, 1179, 1084, 947, 1126, 1062, 781, 797, 851, 785, 897, 557, 727, 969, 181, 552, 1117, 950, 990, 782, 465, 1159, 1008, 1125, 787, 272, 385, 1131, 114, 215, 110, 1086, 876, 907, 113, 1004, 650, 983, 929, 252, 793, 918, 976, 1055, 615, 69, 157, 1176, 18, 629, 137, 1112, 1128, 209, 874, 57, 1100, 1097, 82, 461, 1154, 945, 823, 470, 449, 1029, 1110, 343, 238, 687, 1094, 411, 97, 573, 701, 1099, 1030, 570, 495, 731, 416, 955, 1155, 921, 616, 1083, 60, 859, 679, 161, 1079, 684, 919, 73, 234, 344, 551, 50, 762, 266, 590, 608, 1018, 498, 962, 952, 902, 958, 713, 957, 779, 204, 253, 1156, 811, 724, 708, 154, 937, 675, 938, 935, 31, 734, 632, 46, 1200, 265, 680, 474, 869, 464, 567, 141, 1088, 1175, 563, 518, 493, 784, 442, 452, 1199, 729, 469, 276, 1114, 579, 546, 1211, 1209, 1119, 392, 20, 572, 367, 353, 646, 783, 592, 548, 1161, 436, 140, 251, 1177, 494, 589, 1194, 34, 726, 1208, 479, 391, 1108, 100, 828, 357, 95, 638, 652, 505, 564, 1130, 148, 741, 825, 1212, 1213, 425, 282, 1135, 1184, 718, 337, 70, 38, 422, 769, 1009, 826, 878, 273, 819, 691, 844, 881, 1138, 1162, 270, 556, 1053, 862, 832, 511, 642, 796, 66, 1157, 310, 674, 991, 290, 682, 905, 264, 1134, 528, 289, 749, 846, 1090, 192, 237, 536, 45, 372, 130, 177, 1036, 239, 224, 883, 198, 977, 791, 184, 732, 1202, 595, 965, 978, 163, 752, 1144, 143, 709, 90, 74, 1207, 649, 476, 503, 190, 543, 499, 298, 261, 233, 1037, 759, 170, 917, 1, 993, 1027, 738, 329, 451, 318, 0, 269, 199, 410, 538, 943, 191, 88, 1072, 544, 509, 490, 89, 1050, 739, 850, 35, 288, 102, 560, 332, 1026, 612, 128, 854, 327, 985, 931, 295, 777, 942, 635, 710, 610, 126, 39, 249, 305, 618, 899, 1011, 802, 898, 843, 561, 667, 1136, 296, 453, 481, 299]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9483891844270800
the save name prefix for this run is:  chkpt-ID_9483891844270800_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1187
rank avg (pred): 0.484 +- 0.005
mrr vals (pred, true): 0.015, 0.043
batch losses (mrrl, rdl): 0.0, 0.000186005

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 776
rank avg (pred): 0.290 +- 0.206
mrr vals (pred, true): 0.266, 0.089
batch losses (mrrl, rdl): 0.0, 9.5884e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 379
rank avg (pred): 0.335 +- 0.236
mrr vals (pred, true): 0.279, 0.103
batch losses (mrrl, rdl): 0.0, 0.0001718613

Epoch over!
epoch time: 12.279

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1136
rank avg (pred): 0.192 +- 0.142
mrr vals (pred, true): 0.322, 0.154
batch losses (mrrl, rdl): 0.0, 2.74365e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1110
rank avg (pred): 0.335 +- 0.241
mrr vals (pred, true): 0.297, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001764159

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 763
rank avg (pred): 0.329 +- 0.240
mrr vals (pred, true): 0.308, 0.067
batch losses (mrrl, rdl): 0.0, 1.92208e-05

Epoch over!
epoch time: 11.98

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 195
rank avg (pred): 0.337 +- 0.242
mrr vals (pred, true): 0.298, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001616294

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 929
rank avg (pred): 0.331 +- 0.243
mrr vals (pred, true): 0.308, 0.093
batch losses (mrrl, rdl): 0.0, 0.0001118754

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 442
rank avg (pred): 0.327 +- 0.241
mrr vals (pred, true): 0.313, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001329631

Epoch over!
epoch time: 12.108

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1148
rank avg (pred): 0.279 +- 0.205
mrr vals (pred, true): 0.313, 0.143
batch losses (mrrl, rdl): 0.0, 7.33433e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 389
rank avg (pred): 0.326 +- 0.242
mrr vals (pred, true): 0.311, 0.134
batch losses (mrrl, rdl): 0.0, 0.0003166899

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 87
rank avg (pred): 0.320 +- 0.247
mrr vals (pred, true): 0.336, 0.085
batch losses (mrrl, rdl): 0.0, 0.0001016282

Epoch over!
epoch time: 11.933

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1068
rank avg (pred): 0.021 +- 0.016
mrr vals (pred, true): 0.476, 0.605
batch losses (mrrl, rdl): 0.0, 4.8e-09

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 580
rank avg (pred): 0.381 +- 0.267
mrr vals (pred, true): 0.291, 0.038
batch losses (mrrl, rdl): 0.0, 1.89337e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 340
rank avg (pred): 0.313 +- 0.247
mrr vals (pred, true): 0.352, 0.116
batch losses (mrrl, rdl): 0.0, 0.0001543063

Epoch over!
epoch time: 11.993

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 334
rank avg (pred): 0.317 +- 0.247
mrr vals (pred, true): 0.344, 0.127
batch losses (mrrl, rdl): 0.4696604609, 0.0003446231

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 329
rank avg (pred): 0.436 +- 0.216
mrr vals (pred, true): 0.113, 0.121
batch losses (mrrl, rdl): 0.0006767234, 0.0007557894

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 928
rank avg (pred): 0.520 +- 0.172
mrr vals (pred, true): 0.062, 0.092
batch losses (mrrl, rdl): 0.0013248961, 0.0013309822

Epoch over!
epoch time: 12.359

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 266
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.564, 0.611
batch losses (mrrl, rdl): 0.0220706109, 2.4183e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 166
rank avg (pred): 0.503 +- 0.170
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 2.0303e-05, 9.6406e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1063
rank avg (pred): 0.017 +- 0.014
mrr vals (pred, true): 0.502, 0.605
batch losses (mrrl, rdl): 0.1074916199, 6.815e-07

Epoch over!
epoch time: 12.173

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 673
rank avg (pred): 0.515 +- 0.163
mrr vals (pred, true): 0.049, 0.041
batch losses (mrrl, rdl): 1.0244e-05, 0.000146428

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 524
rank avg (pred): 0.333 +- 0.156
mrr vals (pred, true): 0.099, 0.117
batch losses (mrrl, rdl): 0.00311924, 8.74755e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 326
rank avg (pred): 0.387 +- 0.164
mrr vals (pred, true): 0.081, 0.121
batch losses (mrrl, rdl): 0.0165010598, 0.0004190053

Epoch over!
epoch time: 12.209

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1138
rank avg (pred): 0.317 +- 0.157
mrr vals (pred, true): 0.108, 0.158
batch losses (mrrl, rdl): 0.0250893869, 0.0002668977

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 186
rank avg (pred): 0.488 +- 0.175
mrr vals (pred, true): 0.052, 0.041
batch losses (mrrl, rdl): 6.17902e-05, 9.01878e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1198
rank avg (pred): 0.437 +- 0.154
mrr vals (pred, true): 0.064, 0.041
batch losses (mrrl, rdl): 0.002020784, 3.30361e-05

Epoch over!
epoch time: 12.155

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 374
rank avg (pred): 0.378 +- 0.143
mrr vals (pred, true): 0.075, 0.154
batch losses (mrrl, rdl): 0.061493922, 0.0005410018

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 761
rank avg (pred): 0.447 +- 0.137
mrr vals (pred, true): 0.053, 0.089
batch losses (mrrl, rdl): 6.25019e-05, 0.0006314633

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 111
rank avg (pred): 0.413 +- 0.121
mrr vals (pred, true): 0.057, 0.076
batch losses (mrrl, rdl): 0.0004691909, 0.0003204719

Epoch over!
epoch time: 12.104

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1057
rank avg (pred): 0.010 +- 0.009
mrr vals (pred, true): 0.609, 0.617
batch losses (mrrl, rdl): 0.0006020725, 2.2976e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1085
rank avg (pred): 0.369 +- 0.137
mrr vals (pred, true): 0.076, 0.152
batch losses (mrrl, rdl): 0.0579070747, 0.0004171103

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1188
rank avg (pred): 0.445 +- 0.139
mrr vals (pred, true): 0.048, 0.042
batch losses (mrrl, rdl): 3.83551e-05, 4.44099e-05

Epoch over!
epoch time: 12.499

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 354
rank avg (pred): 0.409 +- 0.128
mrr vals (pred, true): 0.056, 0.084
batch losses (mrrl, rdl): 0.0003637816, 0.0003052083

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 886
rank avg (pred): 0.403 +- 0.112
mrr vals (pred, true): 0.052, 0.040
batch losses (mrrl, rdl): 5.58268e-05, 8.55856e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 815
rank avg (pred): 0.041 +- 0.040
mrr vals (pred, true): 0.505, 0.532
batch losses (mrrl, rdl): 0.0073286481, 9.944e-07

Epoch over!
epoch time: 12.199

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 653
rank avg (pred): 0.410 +- 0.138
mrr vals (pred, true): 0.065, 0.037
batch losses (mrrl, rdl): 0.0021814094, 6.71871e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 397
rank avg (pred): 0.364 +- 0.099
mrr vals (pred, true): 0.059, 0.126
batch losses (mrrl, rdl): 0.0444405451, 0.0004462434

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 738
rank avg (pred): 0.042 +- 0.040
mrr vals (pred, true): 0.506, 0.424
batch losses (mrrl, rdl): 0.0663657635, 2.5416e-06

Epoch over!
epoch time: 12.141

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 991
rank avg (pred): 0.014 +- 0.014
mrr vals (pred, true): 0.589, 0.614
batch losses (mrrl, rdl): 0.0063290936, 1.4222e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 693
rank avg (pred): 0.445 +- 0.133
mrr vals (pred, true): 0.043, 0.036
batch losses (mrrl, rdl): 0.0004257458, 3.60926e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 233
rank avg (pred): 0.323 +- 0.100
mrr vals (pred, true): 0.079, 0.034
batch losses (mrrl, rdl): 0.0084627829, 0.0003965301

Epoch over!
epoch time: 12.133

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1061
rank avg (pred): 0.014 +- 0.014
mrr vals (pred, true): 0.593, 0.615
batch losses (mrrl, rdl): 0.0048646377, 8.679e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1202
rank avg (pred): 0.424 +- 0.151
mrr vals (pred, true): 0.052, 0.037
batch losses (mrrl, rdl): 2.90021e-05, 3.92407e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 726
rank avg (pred): 0.435 +- 0.144
mrr vals (pred, true): 0.043, 0.038
batch losses (mrrl, rdl): 0.0005343484, 6.10334e-05

Epoch over!
epoch time: 12.409

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.403 +- 0.146
mrr vals (pred, true): 0.060, 0.037

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04656 	 0.03025 	 ~...
    3 	     1 	 0.04614 	 0.03230 	 ~...
   35 	     2 	 0.05587 	 0.03374 	 ~...
   64 	     3 	 0.07762 	 0.03430 	 m..s
    9 	     4 	 0.05148 	 0.03538 	 ~...
   10 	     5 	 0.05195 	 0.03554 	 ~...
   82 	     6 	 0.09826 	 0.03612 	 m..s
   44 	     7 	 0.05996 	 0.03669 	 ~...
   23 	     8 	 0.05443 	 0.03692 	 ~...
   13 	     9 	 0.05258 	 0.03703 	 ~...
   41 	    10 	 0.05853 	 0.03704 	 ~...
    6 	    11 	 0.04719 	 0.03736 	 ~...
    4 	    12 	 0.04647 	 0.03754 	 ~...
   39 	    13 	 0.05745 	 0.03781 	 ~...
   68 	    14 	 0.08299 	 0.03788 	 m..s
   61 	    15 	 0.07474 	 0.03811 	 m..s
   71 	    16 	 0.08327 	 0.03811 	 m..s
   78 	    17 	 0.08877 	 0.03827 	 m..s
    8 	    18 	 0.05139 	 0.03829 	 ~...
   66 	    19 	 0.08111 	 0.03830 	 m..s
   81 	    20 	 0.09115 	 0.03870 	 m..s
   59 	    21 	 0.07130 	 0.03897 	 m..s
    2 	    22 	 0.04612 	 0.03911 	 ~...
   42 	    23 	 0.05966 	 0.03923 	 ~...
   72 	    24 	 0.08329 	 0.03945 	 m..s
    1 	    25 	 0.04605 	 0.03963 	 ~...
   54 	    26 	 0.06647 	 0.03963 	 ~...
   32 	    27 	 0.05538 	 0.03964 	 ~...
   49 	    28 	 0.06527 	 0.03964 	 ~...
   34 	    29 	 0.05574 	 0.03973 	 ~...
   58 	    30 	 0.07123 	 0.04003 	 m..s
   80 	    31 	 0.08912 	 0.04012 	 m..s
    0 	    32 	 0.04570 	 0.04021 	 ~...
   28 	    33 	 0.05479 	 0.04029 	 ~...
   24 	    34 	 0.05449 	 0.04061 	 ~...
   65 	    35 	 0.07786 	 0.04068 	 m..s
    7 	    36 	 0.05112 	 0.04124 	 ~...
   25 	    37 	 0.05458 	 0.04131 	 ~...
   43 	    38 	 0.05976 	 0.04136 	 ~...
   21 	    39 	 0.05432 	 0.04142 	 ~...
   16 	    40 	 0.05300 	 0.04217 	 ~...
   27 	    41 	 0.05479 	 0.04241 	 ~...
   11 	    42 	 0.05222 	 0.04284 	 ~...
   22 	    43 	 0.05437 	 0.04306 	 ~...
   36 	    44 	 0.05630 	 0.04330 	 ~...
   52 	    45 	 0.06547 	 0.04361 	 ~...
   70 	    46 	 0.08319 	 0.04433 	 m..s
   67 	    47 	 0.08121 	 0.04445 	 m..s
   26 	    48 	 0.05463 	 0.04459 	 ~...
   48 	    49 	 0.06418 	 0.06766 	 ~...
   30 	    50 	 0.05535 	 0.06794 	 ~...
   40 	    51 	 0.05772 	 0.06812 	 ~...
   73 	    52 	 0.08365 	 0.06896 	 ~...
   38 	    53 	 0.05707 	 0.06919 	 ~...
   57 	    54 	 0.06845 	 0.07071 	 ~...
   19 	    55 	 0.05407 	 0.07276 	 ~...
   47 	    56 	 0.06296 	 0.07290 	 ~...
   45 	    57 	 0.06035 	 0.07855 	 ~...
   15 	    58 	 0.05287 	 0.07881 	 ~...
   20 	    59 	 0.05409 	 0.08176 	 ~...
   55 	    60 	 0.06746 	 0.08212 	 ~...
   37 	    61 	 0.05699 	 0.08385 	 ~...
   14 	    62 	 0.05287 	 0.08444 	 m..s
   17 	    63 	 0.05324 	 0.08490 	 m..s
   46 	    64 	 0.06114 	 0.08658 	 ~...
   18 	    65 	 0.05383 	 0.08676 	 m..s
   29 	    66 	 0.05534 	 0.08747 	 m..s
   33 	    67 	 0.05557 	 0.08775 	 m..s
   31 	    68 	 0.05535 	 0.08934 	 m..s
   51 	    69 	 0.06539 	 0.09072 	 ~...
   12 	    70 	 0.05253 	 0.09496 	 m..s
   50 	    71 	 0.06538 	 0.09999 	 m..s
   53 	    72 	 0.06594 	 0.10238 	 m..s
   60 	    73 	 0.07275 	 0.10331 	 m..s
   56 	    74 	 0.06779 	 0.10702 	 m..s
   62 	    75 	 0.07599 	 0.11644 	 m..s
   85 	    76 	 0.12370 	 0.11766 	 ~...
   63 	    77 	 0.07690 	 0.11779 	 m..s
   84 	    78 	 0.12234 	 0.11882 	 ~...
   86 	    79 	 0.12442 	 0.11936 	 ~...
   83 	    80 	 0.10476 	 0.12177 	 ~...
   69 	    81 	 0.08318 	 0.12433 	 m..s
   77 	    82 	 0.08844 	 0.12945 	 m..s
   76 	    83 	 0.08517 	 0.13337 	 m..s
   74 	    84 	 0.08487 	 0.13689 	 m..s
   75 	    85 	 0.08509 	 0.14282 	 m..s
   79 	    86 	 0.08893 	 0.14666 	 m..s
   88 	    87 	 0.17201 	 0.17167 	 ~...
   87 	    88 	 0.16893 	 0.23331 	 m..s
   89 	    89 	 0.29832 	 0.25103 	 m..s
   90 	    90 	 0.33443 	 0.32606 	 ~...
   91 	    91 	 0.47294 	 0.36531 	 MISS
   94 	    92 	 0.51464 	 0.42680 	 m..s
   93 	    93 	 0.51187 	 0.43013 	 m..s
   92 	    94 	 0.47353 	 0.44419 	 ~...
  100 	    95 	 0.54867 	 0.50303 	 m..s
   97 	    96 	 0.54397 	 0.51229 	 m..s
   98 	    97 	 0.54426 	 0.52270 	 ~...
  101 	    98 	 0.55063 	 0.54371 	 ~...
   99 	    99 	 0.54815 	 0.54431 	 ~...
  102 	   100 	 0.55353 	 0.54642 	 ~...
  103 	   101 	 0.56972 	 0.54800 	 ~...
  105 	   102 	 0.57560 	 0.55278 	 ~...
  109 	   103 	 0.59632 	 0.55497 	 m..s
   95 	   104 	 0.52007 	 0.55518 	 m..s
  104 	   105 	 0.57516 	 0.56051 	 ~...
  106 	   106 	 0.58813 	 0.57514 	 ~...
  107 	   107 	 0.59278 	 0.57608 	 ~...
  108 	   108 	 0.59592 	 0.57648 	 ~...
   96 	   109 	 0.52188 	 0.58044 	 m..s
  112 	   110 	 0.65169 	 0.60102 	 m..s
  116 	   111 	 0.71513 	 0.60795 	 MISS
  114 	   112 	 0.68223 	 0.60997 	 m..s
  111 	   113 	 0.64980 	 0.61015 	 m..s
  113 	   114 	 0.66127 	 0.62322 	 m..s
  110 	   115 	 0.64451 	 0.62441 	 ~...
  117 	   116 	 0.71566 	 0.62509 	 m..s
  119 	   117 	 0.72449 	 0.62835 	 m..s
  115 	   118 	 0.68274 	 0.62963 	 m..s
  118 	   119 	 0.71668 	 0.63332 	 m..s
  120 	   120 	 0.72768 	 0.63497 	 m..s
==========================================
r_mrr = 0.9892085790634155
r2_mrr = 0.9686712622642517
spearmanr_mrr@5 = 0.9402315020561218
spearmanr_mrr@10 = 0.9581111669540405
spearmanr_mrr@50 = 0.9928515553474426
spearmanr_mrr@100 = 0.9944093823432922
spearmanr_mrr@All = 0.9945451021194458
==========================================
test time: 0.456
Done Testing dataset UMLS
total time taken: 189.09682393074036
training time taken: 183.1996591091156
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9892)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9687)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9402)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9581)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9929)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9944)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9945)}}, 'test_loss': {'TransE': {'UMLS': 1.494136209224962}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s max freq rel', 'o max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 8244399546254000
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1060, 801, 1208, 828, 877, 1212, 542, 701, 605, 169, 258, 1214, 364, 958, 342, 1132, 1151, 447, 388, 712, 1067, 987, 848, 718, 946, 514, 474, 1045, 574, 1186, 573, 68, 1119, 1001, 643, 179, 1005, 418, 476, 998, 765, 1191, 802, 819, 743, 73, 1093, 55, 311, 370, 850, 85, 500, 945, 270, 22, 1113, 1101, 281, 437, 881, 793, 110, 1107, 140, 385, 781, 563, 313, 259, 64, 888, 465, 1162, 623, 528, 580, 147, 163, 1163, 1129, 575, 80, 72, 473, 45, 874, 1199, 1180, 914, 74, 428, 666, 586, 102, 1206, 422, 18, 703, 224, 876, 1207, 1193, 1125, 862, 676, 243, 367, 842, 1116, 31, 717, 1131, 111, 649, 589, 84, 98, 546, 549, 578]
valid_ids (0): []
train_ids (1094): [343, 745, 937, 290, 933, 971, 512, 762, 190, 996, 257, 395, 795, 999, 867, 269, 1139, 6, 1104, 941, 543, 530, 411, 240, 13, 628, 777, 836, 77, 1144, 408, 53, 787, 796, 627, 1011, 602, 1188, 317, 323, 731, 617, 369, 857, 1148, 960, 495, 678, 423, 1053, 978, 750, 596, 986, 1146, 1213, 957, 276, 684, 620, 641, 1173, 780, 1020, 3, 44, 824, 568, 1178, 1039, 394, 969, 1044, 119, 1031, 37, 136, 1106, 1077, 1043, 964, 417, 362, 1081, 1130, 901, 135, 360, 839, 1003, 63, 640, 485, 608, 966, 203, 336, 65, 351, 95, 544, 738, 331, 501, 1012, 358, 157, 558, 403, 146, 1156, 296, 122, 306, 696, 779, 928, 220, 1085, 735, 400, 917, 809, 48, 484, 1052, 175, 421, 847, 920, 774, 739, 814, 798, 107, 896, 1059, 446, 886, 916, 1205, 366, 532, 831, 1021, 639, 727, 379, 803, 973, 794, 16, 834, 659, 27, 315, 1124, 116, 148, 382, 271, 1098, 349, 491, 143, 97, 521, 621, 87, 137, 195, 912, 807, 690, 478, 940, 266, 759, 1210, 10, 565, 1142, 883, 99, 1176, 167, 609, 209, 984, 249, 595, 1120, 723, 371, 1200, 754, 210, 94, 247, 644, 715, 786, 572, 763, 518, 237, 131, 672, 679, 769, 742, 1016, 1022, 1165, 354, 553, 406, 845, 926, 611, 193, 552, 631, 215, 126, 356, 1051, 165, 441, 1166, 846, 455, 898, 681, 142, 600, 128, 895, 646, 184, 348, 1126, 699, 1203, 91, 668, 365, 466, 730, 78, 930, 768, 982, 338, 316, 705, 218, 448, 171, 350, 504, 661, 934, 670, 947, 330, 944, 652, 935, 1075, 1155, 439, 52, 62, 583, 186, 704, 541, 413, 961, 288, 129, 101, 693, 721, 33, 398, 967, 866, 785, 687, 166, 412, 938, 855, 231, 92, 1080, 757, 792, 1088, 272, 638, 189, 261, 219, 393, 223, 334, 861, 954, 811, 321, 274, 650, 576, 70, 1, 75, 1114, 674, 487, 327, 716, 1108, 1152, 904, 138, 725, 496, 921, 997, 459, 654, 103, 235, 81, 604, 980, 925, 1159, 1128, 525, 335, 882, 783, 481, 929, 17, 616, 314, 775, 192, 337, 613, 188, 303, 469, 806, 594, 979, 851, 199, 1024, 1076, 410, 226, 244, 767, 1046, 637, 238, 284, 1175, 430, 658, 776, 943, 818, 634, 225, 59, 897, 172, 8, 50, 720, 279, 517, 840, 7, 368, 93, 632, 962, 753, 139, 664, 429, 629, 560, 708, 100, 1103, 234, 907, 507, 347, 1000, 1068, 1167, 1150, 216, 655, 452, 434, 79, 1030, 325, 663, 860, 823, 965, 1069, 1154, 38, 387, 622, 1035, 438, 653, 426, 1038, 510, 592, 158, 1185, 480, 714, 46, 183, 145, 1019, 339, 955, 733, 1177, 173, 729, 265, 11, 443, 582, 346, 1023, 185, 76, 206, 380, 196, 956, 180, 432, 458, 150, 267, 407, 1058, 1004, 378, 737, 373, 988, 669, 397, 642, 21, 490, 472, 1079, 112, 177, 756, 0, 1057, 509, 1095, 660, 564, 133, 740, 656, 635, 372, 409, 263, 1141, 260, 713, 1140, 1184, 830, 497, 837, 821, 939, 1195, 436, 1096, 985, 462, 1174, 1121, 1158, 1008, 1072, 555, 377, 598, 547, 537, 1050, 47, 275, 607, 707, 1157, 1149, 149, 645, 242, 1190, 88, 844, 86, 612, 719, 667, 648, 141, 894, 289, 698, 268, 320, 651, 771, 675, 599, 829, 503, 359, 755, 550, 1028, 153, 233, 307, 151, 591, 711, 1187, 121, 770, 948, 1168, 1183, 993, 435, 697, 300, 352, 14, 799, 1074, 477, 20, 557, 869, 43, 1169, 1123, 1048, 505, 1209, 854, 995, 590, 1172, 1034, 250, 1164, 174, 1100, 245, 301, 181, 1118, 647, 39, 927, 162, 1115, 1078, 890, 15, 752, 841, 470, 618, 766, 567, 545, 236, 130, 710, 902, 515, 187, 424, 892, 326, 483, 533, 1027, 1007, 310, 923, 453, 256, 593, 1026, 885, 386, 601, 264, 606, 294, 1002, 588, 908, 662, 416, 363, 118, 843, 832, 114, 58, 451, 764, 1117, 304, 1192, 502, 734, 1086, 357, 950, 125, 772, 345, 633, 527, 863, 891, 332, 1135, 390, 859, 871, 695, 893, 1083, 60, 1091, 56, 873, 461, 983, 19, 536, 51, 619, 878, 1202, 431, 880, 872, 922, 1041, 198, 952, 383, 1036, 569, 168, 1211, 35, 963, 161, 889, 826, 252, 784, 1062, 584, 324, 117, 1089, 746, 254, 722, 36, 182, 905, 1014, 511, 1197, 24, 106, 164, 865, 680, 402, 127, 556, 686, 626, 688, 749, 630, 1092, 990, 391, 1160, 40, 253, 1055, 683, 442, 513, 906, 468, 810, 977, 789, 489, 1013, 949, 516, 475, 603, 457, 534, 308, 875, 23, 1204, 706, 492, 747, 1127, 389, 227, 54, 773, 790, 344, 788, 176, 942, 726, 778, 109, 1009, 486, 178, 1110, 816, 34, 741, 152, 838, 559, 856, 144, 1161, 585, 229, 571, 539, 587, 579, 636, 376, 1070, 858, 392, 761, 1102, 1097, 493, 1063, 333, 1112, 425, 82, 526, 624, 531, 381, 911, 454, 420, 991, 241, 989, 915, 538, 1029, 736, 1094, 482, 760, 1137, 1182, 1181, 813, 124, 728, 1133, 282, 523, 519, 204, 554, 919, 909, 278, 1105, 4, 804, 551, 105, 852, 1006, 1090, 12, 820, 903, 671, 1138, 213, 69, 924, 160, 471, 968, 797, 899, 932, 207, 1109, 32, 522, 159, 211, 1015, 868, 700, 812, 953, 170, 1040, 1179, 577, 535, 202, 1073, 433, 67, 1065, 566, 318, 49, 581, 155, 450, 42, 808, 191, 57, 665, 156, 1017, 1025, 910, 197, 286, 239, 744, 355, 1189, 1087, 835, 709, 913, 154, 419, 570, 1054, 508, 682, 918, 123, 488, 329, 214, 405, 614, 1056, 83, 104, 115, 399, 959, 758, 900, 230, 822, 232, 689, 479, 1084, 71, 677, 322, 89, 994, 506, 1170, 751, 440, 981, 833, 685, 291, 248, 732, 1201, 384, 297, 610, 972, 277, 361, 800, 951, 970, 5, 305, 396, 374, 1033, 1099, 1018, 815, 1136, 61, 1134, 66, 625, 974, 134, 975, 201, 246, 1010, 931, 499, 1082, 319, 691, 936, 205, 887, 1145, 1049, 673, 1061, 228, 445, 222, 540, 415, 1198, 30, 217, 328, 1147, 524, 1071, 791, 29, 298, 25, 827, 2, 1042, 375, 26, 1032, 1066, 1037, 108, 194, 28, 340, 9, 597, 120, 817, 657, 251, 849, 561, 1047, 992, 692, 1143, 309, 444, 221, 302, 280, 805, 1122, 498, 293, 262, 884, 113, 312, 782, 529, 90, 1171, 460, 520, 132, 562, 467, 1194, 283, 1153, 96, 702, 414, 825, 464, 427, 879, 548, 456, 449, 615, 353, 1111, 976, 200, 864, 299, 404, 1064, 463, 694, 285, 273, 724, 287, 748, 292, 401, 295, 853, 212, 870, 494, 208, 1196, 255, 341, 41]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8667048587025227
the save name prefix for this run is:  chkpt-ID_8667048587025227_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 997
rank avg (pred): 0.424 +- 0.003
mrr vals (pred, true): 0.017, 0.604
batch losses (mrrl, rdl): 0.0, 0.0034356841

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 345
rank avg (pred): 0.319 +- 0.174
mrr vals (pred, true): 0.163, 0.102
batch losses (mrrl, rdl): 0.0, 9.16508e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 191
rank avg (pred): 0.320 +- 0.216
mrr vals (pred, true): 0.273, 0.040
batch losses (mrrl, rdl): 0.0, 0.0001851616

Epoch over!
epoch time: 11.993

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 256
rank avg (pred): 0.032 +- 0.023
mrr vals (pred, true): 0.413, 0.556
batch losses (mrrl, rdl): 0.0, 6.302e-07

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 35
rank avg (pred): 0.043 +- 0.031
mrr vals (pred, true): 0.398, 0.614
batch losses (mrrl, rdl): 0.0, 1.01363e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 456
rank avg (pred): 0.346 +- 0.243
mrr vals (pred, true): 0.303, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001304485

Epoch over!
epoch time: 11.727

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 779
rank avg (pred): 0.315 +- 0.228
mrr vals (pred, true): 0.319, 0.099
batch losses (mrrl, rdl): 0.0, 9.40124e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 709
rank avg (pred): 0.388 +- 0.270
mrr vals (pred, true): 0.297, 0.039
batch losses (mrrl, rdl): 0.0, 3.08104e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 461
rank avg (pred): 0.365 +- 0.254
mrr vals (pred, true): 0.298, 0.042
batch losses (mrrl, rdl): 0.0, 4.7898e-05

Epoch over!
epoch time: 11.866

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1192
rank avg (pred): 0.342 +- 0.250
mrr vals (pred, true): 0.322, 0.037
batch losses (mrrl, rdl): 0.0, 0.0001130045

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 840
rank avg (pred): 0.336 +- 0.251
mrr vals (pred, true): 0.331, 0.087
batch losses (mrrl, rdl): 0.0, 0.0001332993

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 680
rank avg (pred): 0.412 +- 0.283
mrr vals (pred, true): 0.292, 0.039
batch losses (mrrl, rdl): 0.0, 1.48125e-05

Epoch over!
epoch time: 11.831

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 355
rank avg (pred): 0.316 +- 0.236
mrr vals (pred, true): 0.334, 0.109
batch losses (mrrl, rdl): 0.0, 0.0001368093

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 780
rank avg (pred): 0.323 +- 0.245
mrr vals (pred, true): 0.333, 0.090
batch losses (mrrl, rdl): 0.0, 0.0001003819

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 406
rank avg (pred): 0.317 +- 0.245
mrr vals (pred, true): 0.344, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001850089

Epoch over!
epoch time: 11.957

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 558
rank avg (pred): 0.312 +- 0.241
mrr vals (pred, true): 0.341, 0.064
batch losses (mrrl, rdl): 0.8468080759, 1.09543e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 251
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.571, 0.612
batch losses (mrrl, rdl): 0.0169402193, 3.8754e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 159
rank avg (pred): 0.365 +- 0.112
mrr vals (pred, true): 0.057, 0.092
batch losses (mrrl, rdl): 0.0004747178, 0.0002366468

Epoch over!
epoch time: 12.221

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 330
rank avg (pred): 0.357 +- 0.119
mrr vals (pred, true): 0.062, 0.095
batch losses (mrrl, rdl): 0.0014656961, 0.0001508474

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 431
rank avg (pred): 0.317 +- 0.148
mrr vals (pred, true): 0.105, 0.041
batch losses (mrrl, rdl): 0.0304887444, 0.000306987

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 247
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.544, 0.545
batch losses (mrrl, rdl): 1.29607e-05, 7.224e-06

Epoch over!
epoch time: 12.069

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 496
rank avg (pred): 0.384 +- 0.178
mrr vals (pred, true): 0.080, 0.075
batch losses (mrrl, rdl): 0.0088361446, 7.93887e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 420
rank avg (pred): 0.334 +- 0.127
mrr vals (pred, true): 0.067, 0.040
batch losses (mrrl, rdl): 0.0029274169, 0.0002768733

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 999
rank avg (pred): 0.323 +- 0.129
mrr vals (pred, true): 0.070, 0.103
batch losses (mrrl, rdl): 0.0112967277, 8.88539e-05

Epoch over!
epoch time: 12.065

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 788
rank avg (pred): 0.352 +- 0.107
mrr vals (pred, true): 0.050, 0.039
batch losses (mrrl, rdl): 7.882e-07, 0.000230434

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1190
rank avg (pred): 0.434 +- 0.164
mrr vals (pred, true): 0.050, 0.039
batch losses (mrrl, rdl): 1.2773e-06, 3.69477e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 296
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.626, 0.621
batch losses (mrrl, rdl): 0.0002149019, 4.0116e-06

Epoch over!
epoch time: 12.01

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 356
rank avg (pred): 0.310 +- 0.139
mrr vals (pred, true): 0.089, 0.147
batch losses (mrrl, rdl): 0.0328515247, 0.0002313743

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1090
rank avg (pred): 0.295 +- 0.151
mrr vals (pred, true): 0.122, 0.128
batch losses (mrrl, rdl): 0.000323979, 9.09805e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 404
rank avg (pred): 0.299 +- 0.148
mrr vals (pred, true): 0.113, 0.145
batch losses (mrrl, rdl): 0.0103740916, 0.0001927197

Epoch over!
epoch time: 12.274

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 277
rank avg (pred): 0.012 +- 0.010
mrr vals (pred, true): 0.546, 0.570
batch losses (mrrl, rdl): 0.0057386425, 4.271e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 936
rank avg (pred): 0.351 +- 0.105
mrr vals (pred, true): 0.050, 0.088
batch losses (mrrl, rdl): 1.1208e-06, 0.000110967

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1081
rank avg (pred): 0.310 +- 0.138
mrr vals (pred, true): 0.088, 0.112
batch losses (mrrl, rdl): 0.0056005884, 6.67575e-05

Epoch over!
epoch time: 12.29

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 156
rank avg (pred): 0.347 +- 0.106
mrr vals (pred, true): 0.052, 0.083
batch losses (mrrl, rdl): 6.10126e-05, 0.0001193086

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 349
rank avg (pred): 0.321 +- 0.127
mrr vals (pred, true): 0.071, 0.127
batch losses (mrrl, rdl): 0.0309730023, 0.0002295892

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 544
rank avg (pred): 0.439 +- 0.240
mrr vals (pred, true): 0.082, 0.076
batch losses (mrrl, rdl): 0.0104181767, 0.0002605034

Epoch over!
epoch time: 12.272

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 503
rank avg (pred): 0.355 +- 0.186
mrr vals (pred, true): 0.089, 0.122
batch losses (mrrl, rdl): 0.0105723469, 0.0001962977

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 344
rank avg (pred): 0.300 +- 0.139
mrr vals (pred, true): 0.091, 0.132
batch losses (mrrl, rdl): 0.016525317, 9.54621e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1043
rank avg (pred): 0.319 +- 0.129
mrr vals (pred, true): 0.073, 0.038
batch losses (mrrl, rdl): 0.0052823937, 0.0004150167

Epoch over!
epoch time: 12.171

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1159
rank avg (pred): 0.247 +- 0.151
mrr vals (pred, true): 0.172, 0.155
batch losses (mrrl, rdl): 0.0030720143, 4.33492e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 556
rank avg (pred): 0.381 +- 0.155
mrr vals (pred, true): 0.064, 0.073
batch losses (mrrl, rdl): 0.0019759084, 6.63814e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 276
rank avg (pred): 0.022 +- 0.021
mrr vals (pred, true): 0.524, 0.532
batch losses (mrrl, rdl): 0.0005661885, 2.0068e-06

Epoch over!
epoch time: 12.208

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 78
rank avg (pred): 0.022 +- 0.020
mrr vals (pred, true): 0.521, 0.493
batch losses (mrrl, rdl): 0.0083346749, 1.7962e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 626
rank avg (pred): 0.342 +- 0.109
mrr vals (pred, true): 0.057, 0.047
batch losses (mrrl, rdl): 0.000489687, 0.0001821856

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 42
rank avg (pred): 0.026 +- 0.024
mrr vals (pred, true): 0.511, 0.517
batch losses (mrrl, rdl): 0.0003347782, 2.169e-07

Epoch over!
epoch time: 12.293

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.007 +- 0.007
mrr vals (pred, true): 0.652, 0.621

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   12 	     0 	 0.05300 	 0.03433 	 ~...
   42 	     1 	 0.07468 	 0.03488 	 m..s
   69 	     2 	 0.12003 	 0.03488 	 m..s
    2 	     3 	 0.04627 	 0.03490 	 ~...
   11 	     4 	 0.05126 	 0.03555 	 ~...
   55 	     5 	 0.08397 	 0.03597 	 m..s
   37 	     6 	 0.07182 	 0.03617 	 m..s
   32 	     7 	 0.06954 	 0.03620 	 m..s
    6 	     8 	 0.04891 	 0.03643 	 ~...
   93 	     9 	 0.16635 	 0.03659 	 MISS
    7 	    10 	 0.04910 	 0.03691 	 ~...
   90 	    11 	 0.15175 	 0.03702 	 MISS
   48 	    12 	 0.07846 	 0.03718 	 m..s
   50 	    13 	 0.08069 	 0.03719 	 m..s
   30 	    14 	 0.06848 	 0.03743 	 m..s
   56 	    15 	 0.08446 	 0.03760 	 m..s
   54 	    16 	 0.08391 	 0.03762 	 m..s
    9 	    17 	 0.04957 	 0.03769 	 ~...
    8 	    18 	 0.04947 	 0.03774 	 ~...
   84 	    19 	 0.14220 	 0.03783 	 MISS
    3 	    20 	 0.04852 	 0.03784 	 ~...
    4 	    21 	 0.04888 	 0.03784 	 ~...
   45 	    22 	 0.07619 	 0.03806 	 m..s
   25 	    23 	 0.06645 	 0.03818 	 ~...
    0 	    24 	 0.04441 	 0.03829 	 ~...
   91 	    25 	 0.15769 	 0.03864 	 MISS
   79 	    26 	 0.13685 	 0.03867 	 m..s
   89 	    27 	 0.14958 	 0.03880 	 MISS
   13 	    28 	 0.05787 	 0.03912 	 ~...
   43 	    29 	 0.07516 	 0.03933 	 m..s
   77 	    30 	 0.13352 	 0.03945 	 m..s
   86 	    31 	 0.14288 	 0.03945 	 MISS
   35 	    32 	 0.07093 	 0.03955 	 m..s
    5 	    33 	 0.04888 	 0.03957 	 ~...
   67 	    34 	 0.11715 	 0.03963 	 m..s
   38 	    35 	 0.07245 	 0.03973 	 m..s
   10 	    36 	 0.05119 	 0.03977 	 ~...
   16 	    37 	 0.06200 	 0.03986 	 ~...
   65 	    38 	 0.10527 	 0.03991 	 m..s
    1 	    39 	 0.04621 	 0.04004 	 ~...
   71 	    40 	 0.12749 	 0.04012 	 m..s
   82 	    41 	 0.13971 	 0.04026 	 m..s
   19 	    42 	 0.06303 	 0.04037 	 ~...
   33 	    43 	 0.06974 	 0.04051 	 ~...
   24 	    44 	 0.06614 	 0.04075 	 ~...
   76 	    45 	 0.13246 	 0.04085 	 m..s
   18 	    46 	 0.06294 	 0.04114 	 ~...
   22 	    47 	 0.06549 	 0.04119 	 ~...
   15 	    48 	 0.06166 	 0.04124 	 ~...
   78 	    49 	 0.13549 	 0.04136 	 m..s
   46 	    50 	 0.07630 	 0.04191 	 m..s
   47 	    51 	 0.07712 	 0.04207 	 m..s
   29 	    52 	 0.06809 	 0.04213 	 ~...
   39 	    53 	 0.07404 	 0.04230 	 m..s
   41 	    54 	 0.07468 	 0.04239 	 m..s
   60 	    55 	 0.09046 	 0.04347 	 m..s
   17 	    56 	 0.06223 	 0.04383 	 ~...
   75 	    57 	 0.13241 	 0.04391 	 m..s
   58 	    58 	 0.08525 	 0.04408 	 m..s
   80 	    59 	 0.13757 	 0.04445 	 m..s
   14 	    60 	 0.05861 	 0.04457 	 ~...
   57 	    61 	 0.08523 	 0.04648 	 m..s
   20 	    62 	 0.06337 	 0.04661 	 ~...
   36 	    63 	 0.07171 	 0.06120 	 ~...
   52 	    64 	 0.08132 	 0.06484 	 ~...
   51 	    65 	 0.08082 	 0.06822 	 ~...
   34 	    66 	 0.07025 	 0.07276 	 ~...
   27 	    67 	 0.06694 	 0.07572 	 ~...
   28 	    68 	 0.06764 	 0.07720 	 ~...
   21 	    69 	 0.06435 	 0.07924 	 ~...
   23 	    70 	 0.06556 	 0.08550 	 ~...
   53 	    71 	 0.08335 	 0.08580 	 ~...
   49 	    72 	 0.07971 	 0.08599 	 ~...
   59 	    73 	 0.08846 	 0.08955 	 ~...
   40 	    74 	 0.07448 	 0.09043 	 ~...
   63 	    75 	 0.10436 	 0.09213 	 ~...
   44 	    76 	 0.07539 	 0.09245 	 ~...
   31 	    77 	 0.06937 	 0.09724 	 ~...
   61 	    78 	 0.09905 	 0.09934 	 ~...
   26 	    79 	 0.06655 	 0.10033 	 m..s
   70 	    80 	 0.12098 	 0.10322 	 ~...
   87 	    81 	 0.14569 	 0.10383 	 m..s
   62 	    82 	 0.10113 	 0.10885 	 ~...
   72 	    83 	 0.13001 	 0.11037 	 ~...
   73 	    84 	 0.13194 	 0.11071 	 ~...
   66 	    85 	 0.10661 	 0.11169 	 ~...
   92 	    86 	 0.15854 	 0.11360 	 m..s
   64 	    87 	 0.10523 	 0.11368 	 ~...
   74 	    88 	 0.13236 	 0.11432 	 ~...
   68 	    89 	 0.11899 	 0.11467 	 ~...
   85 	    90 	 0.14274 	 0.11766 	 ~...
   88 	    91 	 0.14674 	 0.13066 	 ~...
   83 	    92 	 0.14060 	 0.13337 	 ~...
   81 	    93 	 0.13865 	 0.14043 	 ~...
   94 	    94 	 0.24222 	 0.16156 	 m..s
   95 	    95 	 0.34826 	 0.33119 	 ~...
   96 	    96 	 0.50785 	 0.43982 	 m..s
   97 	    97 	 0.50954 	 0.44884 	 m..s
  101 	    98 	 0.54295 	 0.47351 	 m..s
  100 	    99 	 0.53589 	 0.49651 	 m..s
  103 	   100 	 0.54653 	 0.50875 	 m..s
   99 	   101 	 0.53562 	 0.52055 	 ~...
  102 	   102 	 0.54473 	 0.52621 	 ~...
  104 	   103 	 0.54769 	 0.53015 	 ~...
   98 	   104 	 0.51846 	 0.53107 	 ~...
  109 	   105 	 0.57040 	 0.53535 	 m..s
  108 	   106 	 0.56740 	 0.54185 	 ~...
  111 	   107 	 0.57486 	 0.54621 	 ~...
  107 	   108 	 0.56739 	 0.55759 	 ~...
  110 	   109 	 0.57143 	 0.56505 	 ~...
  106 	   110 	 0.56737 	 0.56998 	 ~...
  105 	   111 	 0.56711 	 0.57062 	 ~...
  119 	   112 	 0.64635 	 0.59483 	 m..s
  114 	   113 	 0.63403 	 0.61315 	 ~...
  112 	   114 	 0.63241 	 0.61485 	 ~...
  113 	   115 	 0.63366 	 0.61801 	 ~...
  116 	   116 	 0.64039 	 0.61942 	 ~...
  120 	   117 	 0.65190 	 0.62056 	 m..s
  115 	   118 	 0.63945 	 0.62441 	 ~...
  117 	   119 	 0.64084 	 0.63054 	 ~...
  118 	   120 	 0.64113 	 0.63174 	 ~...
==========================================
r_mrr = 0.9865190386772156
r2_mrr = 0.9516578912734985
spearmanr_mrr@5 = 0.9095106720924377
spearmanr_mrr@10 = 0.9289613366127014
spearmanr_mrr@50 = 0.998294472694397
spearmanr_mrr@100 = 0.9988314509391785
spearmanr_mrr@All = 0.9982571601867676
==========================================
test time: 0.384
Done Testing dataset UMLS
total time taken: 187.4283845424652
training time taken: 181.7047140598297
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9865)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9517)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.9095)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.9290)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9983)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9988)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9983)}}, 'test_loss': {'TransE': {'UMLS': 2.074260872832383}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 7035051392482141
Starting TWIG!
Loading datasets
Loading TransE...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1020, 771, 661, 1038, 432, 544, 94, 402, 344, 1213, 703, 964, 1141, 1161, 179, 756, 1034, 842, 271, 380, 189, 439, 896, 176, 1184, 63, 644, 469, 423, 500, 301, 908, 636, 815, 801, 556, 51, 751, 631, 679, 673, 280, 972, 881, 437, 913, 989, 361, 884, 466, 140, 763, 678, 543, 474, 1187, 906, 443, 755, 351, 349, 522, 1202, 617, 42, 398, 783, 700, 975, 795, 91, 255, 625, 102, 64, 701, 804, 648, 101, 774, 711, 582, 650, 944, 163, 1013, 853, 546, 482, 653, 519, 607, 251, 1178, 710, 405, 1052, 1, 341, 905, 234, 600, 1182, 515, 86, 39, 30, 666, 8, 169, 1169, 514, 747, 969, 410, 890, 198, 332, 986, 342, 148]
valid_ids (0): []
train_ids (1094): [37, 529, 698, 817, 588, 605, 214, 173, 545, 172, 1201, 203, 420, 974, 143, 118, 586, 295, 524, 288, 922, 359, 1078, 707, 990, 481, 619, 206, 246, 321, 278, 732, 465, 312, 1066, 209, 1006, 537, 260, 871, 257, 592, 379, 300, 865, 868, 134, 553, 159, 493, 982, 940, 264, 632, 733, 71, 621, 608, 195, 1157, 525, 317, 126, 557, 1071, 657, 594, 44, 850, 40, 1059, 576, 542, 365, 429, 794, 115, 128, 117, 244, 646, 1051, 866, 776, 23, 389, 89, 767, 717, 1193, 430, 1212, 252, 424, 746, 139, 171, 394, 523, 916, 156, 170, 1144, 699, 1117, 1042, 814, 1080, 491, 348, 1063, 1167, 1145, 616, 475, 655, 535, 454, 910, 955, 885, 360, 721, 427, 259, 505, 254, 1073, 434, 689, 520, 1067, 683, 452, 263, 324, 26, 144, 66, 723, 180, 483, 207, 199, 162, 387, 480, 346, 399, 215, 762, 65, 343, 937, 970, 953, 1105, 108, 1174, 1021, 496, 994, 1019, 851, 859, 193, 502, 338, 618, 538, 995, 530, 1092, 55, 1011, 408, 183, 382, 153, 981, 114, 383, 936, 928, 1082, 213, 1069, 790, 392, 897, 1154, 297, 510, 1166, 310, 941, 925, 821, 414, 381, 1135, 722, 460, 663, 779, 1110, 370, 791, 580, 561, 99, 421, 1137, 555, 58, 1035, 1168, 1127, 495, 436, 1057, 838, 568, 28, 220, 934, 415, 745, 235, 891, 456, 285, 240, 942, 1043, 385, 654, 355, 323, 141, 1143, 258, 799, 660, 548, 373, 959, 83, 759, 521, 879, 957, 468, 107, 658, 643, 630, 1008, 0, 231, 2, 1016, 286, 591, 966, 787, 656, 847, 681, 290, 73, 917, 499, 82, 1090, 1097, 367, 1060, 1208, 854, 219, 31, 1017, 81, 478, 824, 888, 760, 21, 1004, 1048, 1185, 448, 637, 882, 41, 503, 1196, 517, 662, 714, 1120, 813, 1087, 1054, 708, 739, 413, 145, 1115, 536, 315, 575, 682, 59, 190, 729, 1147, 291, 407, 674, 695, 444, 340, 404, 1203, 740, 60, 489, 563, 100, 914, 416, 949, 135, 980, 36, 731, 769, 803, 585, 22, 287, 438, 362, 999, 61, 222, 840, 1152, 276, 1014, 668, 1083, 810, 877, 789, 541, 1068, 898, 1150, 13, 299, 425, 154, 777, 702, 48, 742, 820, 54, 844, 292, 599, 236, 119, 131, 138, 827, 441, 649, 933, 1195, 802, 1176, 1119, 1146, 160, 479, 1050, 684, 996, 793, 841, 418, 1031, 178, 67, 694, 24, 924, 142, 87, 487, 1018, 640, 572, 719, 157, 567, 511, 307, 1181, 528, 1151, 211, 302, 902, 224, 217, 741, 16, 1088, 303, 70, 624, 62, 68, 1200, 1102, 758, 845, 304, 744, 1037, 1121, 333, 554, 1026, 319, 455, 593, 1138, 88, 186, 1130, 692, 946, 216, 98, 184, 363, 129, 889, 377, 880, 947, 272, 768, 440, 136, 50, 196, 457, 1077, 628, 550, 1192, 17, 904, 106, 1056, 52, 1036, 704, 870, 1126, 15, 326, 1053, 1197, 828, 1189, 861, 35, 839, 531, 943, 57, 686, 831, 150, 856, 132, 1015, 920, 1070, 893, 921, 356, 256, 248, 664, 1091, 225, 597, 38, 788, 1045, 1075, 635, 212, 562, 1160, 998, 785, 1081, 19, 1003, 34, 459, 133, 886, 1095, 716, 1153, 895, 32, 1109, 453, 33, 1159, 1047, 289, 780, 1118, 1139, 816, 876, 188, 146, 266, 976, 147, 926, 110, 1122, 95, 464, 1030, 1162, 396, 1142, 602, 867, 761, 167, 347, 612, 911, 93, 449, 743, 12, 269, 105, 488, 473, 670, 596, 948, 993, 516, 1194, 932, 1116, 490, 725, 353, 6, 352, 450, 645, 372, 875, 1061, 798, 375, 935, 293, 862, 610, 1103, 241, 371, 279, 579, 14, 598, 296, 570, 477, 952, 1210, 1177, 1106, 152, 497, 208, 900, 90, 1046, 9, 29, 846, 691, 281, 426, 805, 237, 298, 915, 1096, 313, 1134, 238, 1112, 200, 676, 166, 175, 325, 1100, 565, 931, 792, 967, 174, 938, 311, 122, 80, 1032, 485, 706, 1124, 182, 1072, 647, 498, 903, 819, 963, 873, 614, 1132, 335, 829, 374, 1128, 1188, 1179, 1089, 539, 693, 274, 390, 1111, 641, 331, 634, 626, 328, 1064, 1114, 202, 1131, 391, 601, 595, 267, 675, 187, 233, 354, 27, 161, 53, 494, 1007, 560, 10, 1094, 262, 97, 532, 318, 111, 345, 56, 164, 368, 507, 445, 401, 518, 907, 47, 754, 718, 513, 228, 1033, 467, 566, 210, 18, 104, 927, 807, 1002, 834, 569, 181, 417, 736, 1086, 403, 388, 1163, 1204, 734, 221, 872, 284, 249, 526, 357, 687, 484, 458, 1190, 75, 765, 860, 1183, 74, 918, 406, 1074, 901, 177, 393, 43, 735, 337, 615, 712, 973, 168, 149, 322, 1108, 1155, 1027, 84, 316, 137, 463, 571, 165, 314, 151, 232, 728, 894, 77, 606, 709, 930, 306, 121, 1098, 672, 589, 965, 46, 85, 1180, 835, 247, 431, 832, 961, 864, 800, 113, 985, 1136, 49, 185, 909, 978, 447, 945, 69, 727, 613, 826, 581, 533, 369, 358, 808, 609, 887, 275, 977, 336, 627, 504, 1164, 45, 577, 962, 629, 283, 578, 96, 811, 836, 1198, 486, 1171, 76, 1041, 1158, 1076, 749, 848, 1207, 855, 992, 782, 651, 451, 786, 837, 757, 752, 770, 277, 573, 1023, 833, 812, 1170, 818, 378, 282, 112, 669, 883, 1165, 892, 3, 671, 192, 1009, 1214, 1065, 1000, 1025, 386, 1085, 508, 971, 857, 843, 103, 419, 991, 11, 400, 652, 1205, 509, 724, 574, 622, 784, 191, 205, 988, 334, 1058, 327, 534, 830, 899, 158, 1123, 584, 633, 7, 397, 878, 250, 750, 984, 223, 125, 968, 737, 696, 923, 201, 1084, 1125, 680, 753, 435, 825, 229, 726, 1191, 1173, 130, 713, 796, 1024, 665, 869, 690, 273, 620, 1028, 772, 547, 919, 912, 677, 950, 1099, 852, 72, 748, 320, 1039, 204, 261, 1029, 242, 4, 705, 1209, 411, 587, 116, 461, 720, 78, 590, 364, 1001, 611, 623, 715, 552, 1093, 512, 778, 1022, 849, 639, 446, 1156, 956, 997, 527, 194, 1040, 939, 858, 1148, 1129, 476, 329, 309, 5, 688, 270, 124, 979, 1206, 1079, 603, 1140, 604, 79, 781, 1186, 1104, 551, 412, 558, 730, 350, 1055, 471, 472, 384, 127, 25, 123, 109, 642, 697, 1133, 376, 239, 766, 958, 366, 409, 806, 1113, 197, 305, 422, 227, 667, 1062, 1044, 1012, 983, 1175, 428, 1101, 308, 1010, 1172, 339, 1211, 549, 265, 775, 559, 797, 243, 395, 442, 462, 155, 863, 268, 1049, 659, 1107, 987, 218, 230, 929, 294, 822, 330, 564, 492, 506, 1005, 685, 764, 823, 809, 253, 638, 20, 1199, 874, 954, 433, 226, 738, 501, 583, 951, 773, 245, 1149, 120, 540, 470, 92, 960]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1845299487710782
the save name prefix for this run is:  chkpt-ID_1845299487710782_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'TransE': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 827
rank avg (pred): 0.543 +- 0.005
mrr vals (pred, true): 0.014, 0.577
batch losses (mrrl, rdl): 0.0, 0.0057357955

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 10
rank avg (pred): 0.042 +- 0.026
mrr vals (pred, true): 0.313, 0.545
batch losses (mrrl, rdl): 0.0, 4.9222e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1126
rank avg (pred): 0.340 +- 0.228
mrr vals (pred, true): 0.252, 0.038
batch losses (mrrl, rdl): 0.0, 0.0001438446

Epoch over!
epoch time: 12.092

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 573
rank avg (pred): 0.392 +- 0.252
mrr vals (pred, true): 0.238, 0.035
batch losses (mrrl, rdl): 0.0, 6.9958e-06

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 142
rank avg (pred): 0.320 +- 0.240
mrr vals (pred, true): 0.302, 0.090
batch losses (mrrl, rdl): 0.0, 0.0001047326

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 782
rank avg (pred): 0.330 +- 0.243
mrr vals (pred, true): 0.286, 0.089
batch losses (mrrl, rdl): 0.0, 8.32429e-05

Epoch over!
epoch time: 11.8

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 642
rank avg (pred): 0.420 +- 0.277
mrr vals (pred, true): 0.248, 0.035
batch losses (mrrl, rdl): 0.0, 2.56017e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1147
rank avg (pred): 0.159 +- 0.123
mrr vals (pred, true): 0.339, 0.159
batch losses (mrrl, rdl): 0.0, 0.0001218828

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1154
rank avg (pred): 0.213 +- 0.164
mrr vals (pred, true): 0.317, 0.146
batch losses (mrrl, rdl): 0.0, 3.00693e-05

Epoch over!
epoch time: 11.876

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 562
rank avg (pred): 0.305 +- 0.233
mrr vals (pred, true): 0.306, 0.072
batch losses (mrrl, rdl): 0.0, 1.85748e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 96
rank avg (pred): 0.318 +- 0.243
mrr vals (pred, true): 0.303, 0.082
batch losses (mrrl, rdl): 0.0, 7.27025e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 173
rank avg (pred): 0.321 +- 0.249
mrr vals (pred, true): 0.308, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001490022

Epoch over!
epoch time: 11.873

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 981
rank avg (pred): 0.023 +- 0.018
mrr vals (pred, true): 0.473, 0.613
batch losses (mrrl, rdl): 0.0, 7.33e-08

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 347
rank avg (pred): 0.356 +- 0.245
mrr vals (pred, true): 0.250, 0.145
batch losses (mrrl, rdl): 0.0, 0.000446376

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 128
rank avg (pred): 0.323 +- 0.253
mrr vals (pred, true): 0.309, 0.117
batch losses (mrrl, rdl): 0.0, 0.0002508834

Epoch over!
epoch time: 11.958

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 727
rank avg (pred): 0.390 +- 0.279
mrr vals (pred, true): 0.279, 0.043
batch losses (mrrl, rdl): 0.5245658159, 1.26364e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 295
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.593, 0.557
batch losses (mrrl, rdl): 0.0126703791, 5.406e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1167
rank avg (pred): 0.488 +- 0.155
mrr vals (pred, true): 0.040, 0.035
batch losses (mrrl, rdl): 0.0009830408, 0.0001473442

Epoch over!
epoch time: 12.432

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 854
rank avg (pred): 0.497 +- 0.190
mrr vals (pred, true): 0.052, 0.091
batch losses (mrrl, rdl): 2.61401e-05, 0.0010728176

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 823
rank avg (pred): 0.014 +- 0.012
mrr vals (pred, true): 0.547, 0.542
batch losses (mrrl, rdl): 0.00026397, 8.1896e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1080
rank avg (pred): 0.388 +- 0.185
mrr vals (pred, true): 0.069, 0.110
batch losses (mrrl, rdl): 0.0167269055, 0.0003107756

Epoch over!
epoch time: 12.202

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 739
rank avg (pred): 0.014 +- 0.012
mrr vals (pred, true): 0.539, 0.436
batch losses (mrrl, rdl): 0.1066972166, 3.18266e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 452
rank avg (pred): 0.337 +- 0.199
mrr vals (pred, true): 0.091, 0.043
batch losses (mrrl, rdl): 0.0165625345, 0.0001311235

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 869
rank avg (pred): 0.643 +- 0.172
mrr vals (pred, true): 0.043, 0.039
batch losses (mrrl, rdl): 0.0004960223, 0.0008406024

Epoch over!
epoch time: 12.186

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 429
rank avg (pred): 0.448 +- 0.188
mrr vals (pred, true): 0.063, 0.038
batch losses (mrrl, rdl): 0.0015907072, 2.36293e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1117
rank avg (pred): 0.333 +- 0.192
mrr vals (pred, true): 0.088, 0.039
batch losses (mrrl, rdl): 0.0144824581, 0.0002575745

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 630
rank avg (pred): 0.587 +- 0.194
mrr vals (pred, true): 0.043, 0.032
batch losses (mrrl, rdl): 0.0004243217, 0.0003345654

Epoch over!
epoch time: 12.249

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 447
rank avg (pred): 0.455 +- 0.146
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002830166, 5.20346e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 672
rank avg (pred): 0.583 +- 0.194
mrr vals (pred, true): 0.044, 0.036
batch losses (mrrl, rdl): 0.0004026352, 0.0002696402

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 12
rank avg (pred): 0.034 +- 0.030
mrr vals (pred, true): 0.457, 0.490
batch losses (mrrl, rdl): 0.0104954261, 5.156e-07

Epoch over!
epoch time: 12.195

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 656
rank avg (pred): 0.548 +- 0.175
mrr vals (pred, true): 0.050, 0.037
batch losses (mrrl, rdl): 7.6e-09, 0.0002013053

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 766
rank avg (pred): 0.500 +- 0.146
mrr vals (pred, true): 0.054, 0.095
batch losses (mrrl, rdl): 0.0001471563, 0.0010875226

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 594
rank avg (pred): 0.593 +- 0.228
mrr vals (pred, true): 0.051, 0.034
batch losses (mrrl, rdl): 1.98212e-05, 0.0004935988

Epoch over!
epoch time: 12.255

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 662
rank avg (pred): 0.552 +- 0.187
mrr vals (pred, true): 0.052, 0.035
batch losses (mrrl, rdl): 3.96466e-05, 0.0002181883

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 105
rank avg (pred): 0.491 +- 0.148
mrr vals (pred, true): 0.056, 0.091
batch losses (mrrl, rdl): 0.0003093414, 0.0011062737

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 980
rank avg (pred): 0.011 +- 0.010
mrr vals (pred, true): 0.578, 0.623
batch losses (mrrl, rdl): 0.0199328344, 3.619e-06

Epoch over!
epoch time: 12.186

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 749
rank avg (pred): 0.026 +- 0.023
mrr vals (pred, true): 0.485, 0.421
batch losses (mrrl, rdl): 0.0406477265, 1.63453e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 550
rank avg (pred): 0.302 +- 0.154
mrr vals (pred, true): 0.079, 0.081
batch losses (mrrl, rdl): 0.0083274944, 5.95997e-05

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 1017
rank avg (pred): 0.283 +- 0.165
mrr vals (pred, true): 0.093, 0.111
batch losses (mrrl, rdl): 0.0031944748, 3.90008e-05

Epoch over!
epoch time: 12.218

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 937
rank avg (pred): 0.448 +- 0.146
mrr vals (pred, true): 0.064, 0.092
batch losses (mrrl, rdl): 0.0019619016, 0.000664758

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 103
rank avg (pred): 0.438 +- 0.129
mrr vals (pred, true): 0.057, 0.100
batch losses (mrrl, rdl): 0.0005056809, 0.0007799094

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 588
rank avg (pred): 0.584 +- 0.248
mrr vals (pred, true): 0.043, 0.037
batch losses (mrrl, rdl): 0.0005308228, 0.0003355384

Epoch over!
epoch time: 12.053

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 222
rank avg (pred): 0.513 +- 0.186
mrr vals (pred, true): 0.053, 0.039
batch losses (mrrl, rdl): 9.53766e-05, 9.20154e-05

running batch: 500 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 15
rank avg (pred): 0.019 +- 0.018
mrr vals (pred, true): 0.533, 0.488
batch losses (mrrl, rdl): 0.0204056669, 2.7816e-06

running batch: 1000 / 1094 and superbatch(1); data from TransE, UMLS, run 2.1, exp 18
rank avg (pred): 0.018 +- 0.017
mrr vals (pred, true): 0.541, 0.474
batch losses (mrrl, rdl): 0.0452808589, 3.7511e-06

Epoch over!
epoch time: 12.199

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM TransE and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.313 +- 0.130
mrr vals (pred, true): 0.076, 0.120

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.05005 	 0.03249 	 ~...
   37 	     1 	 0.05527 	 0.03476 	 ~...
    0 	     2 	 0.04996 	 0.03479 	 ~...
    3 	     3 	 0.05007 	 0.03491 	 ~...
    7 	     4 	 0.05026 	 0.03523 	 ~...
   27 	     5 	 0.05445 	 0.03550 	 ~...
   52 	     6 	 0.05742 	 0.03597 	 ~...
   11 	     7 	 0.05065 	 0.03613 	 ~...
   41 	     8 	 0.05539 	 0.03637 	 ~...
   42 	     9 	 0.05540 	 0.03669 	 ~...
   28 	    10 	 0.05457 	 0.03699 	 ~...
   33 	    11 	 0.05519 	 0.03702 	 ~...
   45 	    12 	 0.05592 	 0.03704 	 ~...
   20 	    13 	 0.05216 	 0.03713 	 ~...
   39 	    14 	 0.05535 	 0.03741 	 ~...
   19 	    15 	 0.05212 	 0.03747 	 ~...
    8 	    16 	 0.05028 	 0.03759 	 ~...
   63 	    17 	 0.06101 	 0.03779 	 ~...
   56 	    18 	 0.05873 	 0.03786 	 ~...
   58 	    19 	 0.05980 	 0.03794 	 ~...
   81 	    20 	 0.07458 	 0.03799 	 m..s
   29 	    21 	 0.05491 	 0.03828 	 ~...
   16 	    22 	 0.05116 	 0.03829 	 ~...
   62 	    23 	 0.06090 	 0.03841 	 ~...
   67 	    24 	 0.06585 	 0.03845 	 ~...
   30 	    25 	 0.05497 	 0.03856 	 ~...
   23 	    26 	 0.05361 	 0.03872 	 ~...
   22 	    27 	 0.05356 	 0.03910 	 ~...
   64 	    28 	 0.06232 	 0.03915 	 ~...
   90 	    29 	 0.08181 	 0.03922 	 m..s
   26 	    30 	 0.05438 	 0.03942 	 ~...
    6 	    31 	 0.05024 	 0.03948 	 ~...
    4 	    32 	 0.05018 	 0.03957 	 ~...
    1 	    33 	 0.04998 	 0.03983 	 ~...
   15 	    34 	 0.05090 	 0.03994 	 ~...
    5 	    35 	 0.05019 	 0.04000 	 ~...
   69 	    36 	 0.06651 	 0.04012 	 ~...
   12 	    37 	 0.05073 	 0.04021 	 ~...
   80 	    38 	 0.07417 	 0.04026 	 m..s
   74 	    39 	 0.06777 	 0.04036 	 ~...
   86 	    40 	 0.07719 	 0.04052 	 m..s
   24 	    41 	 0.05366 	 0.04058 	 ~...
   76 	    42 	 0.06791 	 0.04078 	 ~...
   18 	    43 	 0.05146 	 0.04117 	 ~...
   31 	    44 	 0.05500 	 0.04142 	 ~...
   17 	    45 	 0.05126 	 0.04143 	 ~...
    9 	    46 	 0.05041 	 0.04156 	 ~...
   34 	    47 	 0.05521 	 0.04191 	 ~...
   13 	    48 	 0.05086 	 0.04230 	 ~...
   40 	    49 	 0.05539 	 0.04265 	 ~...
   10 	    50 	 0.05050 	 0.04328 	 ~...
   53 	    51 	 0.05784 	 0.04347 	 ~...
   82 	    52 	 0.07526 	 0.04353 	 m..s
   55 	    53 	 0.05858 	 0.04593 	 ~...
   48 	    54 	 0.05661 	 0.04611 	 ~...
   54 	    55 	 0.05840 	 0.04648 	 ~...
   32 	    56 	 0.05508 	 0.04661 	 ~...
   14 	    57 	 0.05089 	 0.04711 	 ~...
   36 	    58 	 0.05524 	 0.04950 	 ~...
   79 	    59 	 0.06962 	 0.06484 	 ~...
   77 	    60 	 0.06839 	 0.06644 	 ~...
   35 	    61 	 0.05523 	 0.06666 	 ~...
   78 	    62 	 0.06944 	 0.06673 	 ~...
   75 	    63 	 0.06784 	 0.06766 	 ~...
   91 	    64 	 0.08183 	 0.07315 	 ~...
   88 	    65 	 0.07963 	 0.07645 	 ~...
   21 	    66 	 0.05269 	 0.08550 	 m..s
   87 	    67 	 0.07868 	 0.08580 	 ~...
   49 	    68 	 0.05695 	 0.08633 	 ~...
   57 	    69 	 0.05909 	 0.08658 	 ~...
   51 	    70 	 0.05741 	 0.08968 	 m..s
   25 	    71 	 0.05375 	 0.09072 	 m..s
   59 	    72 	 0.06009 	 0.09213 	 m..s
   47 	    73 	 0.05635 	 0.09222 	 m..s
   43 	    74 	 0.05564 	 0.09245 	 m..s
   60 	    75 	 0.06028 	 0.09552 	 m..s
   44 	    76 	 0.05577 	 0.09637 	 m..s
   38 	    77 	 0.05530 	 0.09651 	 m..s
   46 	    78 	 0.05625 	 0.09921 	 m..s
   50 	    79 	 0.05731 	 0.10210 	 m..s
   92 	    80 	 0.11268 	 0.10383 	 ~...
   66 	    81 	 0.06396 	 0.10900 	 m..s
   71 	    82 	 0.06729 	 0.11037 	 m..s
   72 	    83 	 0.06752 	 0.11256 	 m..s
   65 	    84 	 0.06271 	 0.11449 	 m..s
   84 	    85 	 0.07568 	 0.11978 	 m..s
   94 	    86 	 0.13302 	 0.12090 	 ~...
   61 	    87 	 0.06048 	 0.12672 	 m..s
   85 	    88 	 0.07653 	 0.13087 	 m..s
   68 	    89 	 0.06638 	 0.13180 	 m..s
   70 	    90 	 0.06671 	 0.13270 	 m..s
   73 	    91 	 0.06763 	 0.13341 	 m..s
   89 	    92 	 0.07963 	 0.13923 	 m..s
   83 	    93 	 0.07533 	 0.14510 	 m..s
   93 	    94 	 0.11926 	 0.15811 	 m..s
   95 	    95 	 0.13616 	 0.17167 	 m..s
   98 	    96 	 0.26862 	 0.29373 	 ~...
   96 	    97 	 0.26683 	 0.31539 	 m..s
   97 	    98 	 0.26811 	 0.32864 	 m..s
   99 	    99 	 0.26909 	 0.33009 	 m..s
  102 	   100 	 0.50991 	 0.43219 	 m..s
  109 	   101 	 0.51917 	 0.50617 	 ~...
  103 	   102 	 0.51514 	 0.51624 	 ~...
  106 	   103 	 0.51585 	 0.51705 	 ~...
  108 	   104 	 0.51847 	 0.51839 	 ~...
  105 	   105 	 0.51577 	 0.51997 	 ~...
  101 	   106 	 0.48965 	 0.52124 	 m..s
  104 	   107 	 0.51560 	 0.53031 	 ~...
  100 	   108 	 0.44178 	 0.53204 	 m..s
  107 	   109 	 0.51693 	 0.54438 	 ~...
  111 	   110 	 0.54559 	 0.55759 	 ~...
  114 	   111 	 0.55552 	 0.57063 	 ~...
  116 	   112 	 0.55606 	 0.57095 	 ~...
  112 	   113 	 0.55229 	 0.57648 	 ~...
  110 	   114 	 0.54307 	 0.58131 	 m..s
  117 	   115 	 0.58012 	 0.61211 	 m..s
  119 	   116 	 0.58783 	 0.61212 	 ~...
  113 	   117 	 0.55242 	 0.61821 	 m..s
  115 	   118 	 0.55597 	 0.61839 	 m..s
  118 	   119 	 0.58709 	 0.61970 	 m..s
  120 	   120 	 0.58895 	 0.62896 	 m..s
==========================================
r_mrr = 0.9877776503562927
r2_mrr = 0.9725276231765747
spearmanr_mrr@5 = 0.7602267265319824
spearmanr_mrr@10 = 0.8296961188316345
spearmanr_mrr@50 = 0.9975774884223938
spearmanr_mrr@100 = 0.9938801527023315
spearmanr_mrr@All = 0.9936400651931763
==========================================
test time: 0.39
Done Testing dataset UMLS
total time taken: 187.8507583141327
training time taken: 182.2368507385254
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'TransE': {'UMLS': tensor(0.9878)}}, 'r2_mrr': {'TransE': {'UMLS': tensor(0.9725)}}, 'spearmanr_mrr@5': {'TransE': {'UMLS': tensor(0.7602)}}, 'spearmanr_mrr@10': {'TransE': {'UMLS': tensor(0.8297)}}, 'spearmanr_mrr@50': {'TransE': {'UMLS': tensor(0.9976)}}, 'spearmanr_mrr@100': {'TransE': {'UMLS': tensor(0.9939)}}, 'spearmanr_mrr@All': {'TransE': {'UMLS': tensor(0.9936)}}, 'test_loss': {'TransE': {'UMLS': 0.998993486452946}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

