using splits:
test_ids (121): [767, 834, 903, 315, 297, 951, 260, 22, 606, 1098, 143, 589, 865, 134, 509, 161, 1178, 482, 171, 45, 1023, 1141, 646, 311, 397, 850, 1047, 600, 477, 227, 191, 961, 399, 61, 732, 641, 189, 1148, 158, 966, 660, 99, 854, 759, 1020, 599, 667, 1126, 742, 1122, 534, 85, 942, 425, 1033, 9, 130, 1087, 835, 1058, 181, 841, 513, 810, 138, 418, 712, 915, 620, 740, 1182, 524, 271, 721, 139, 25, 786, 981, 211, 654, 388, 292, 447, 320, 778, 86, 173, 1163, 693, 363, 338, 403, 819, 808, 1092, 866, 470, 510, 612, 1176, 881, 587, 619, 719, 726, 496, 536, 424, 441, 497, 607, 975, 55, 839, 815, 647, 919, 501, 1053, 117, 67]
valid_ids (0): []
train_ids (1082): [616, 598, 296, 420, 75, 773, 1160, 891, 792, 567, 97, 368, 1107, 249, 433, 691, 367, 932, 82, 836, 360, 1089, 1096, 131, 988, 664, 877, 456, 1110, 994, 701, 1121, 242, 1050, 542, 68, 112, 560, 912, 235, 230, 1120, 1067, 1029, 36, 307, 507, 174, 692, 852, 1157, 1210, 674, 362, 868, 132, 329, 384, 730, 165, 576, 1129, 302, 1056, 770, 258, 147, 344, 1077, 1045, 328, 1094, 878, 226, 485, 251, 28, 417, 203, 1197, 1001, 395, 309, 1040, 774, 154, 1167, 144, 209, 747, 858, 79, 1019, 1155, 371, 1175, 717, 114, 374, 108, 971, 768, 1079, 716, 351, 239, 629, 1201, 578, 704, 1009, 90, 176, 924, 229, 332, 298, 0, 18, 849, 106, 32, 763, 751, 908, 997, 261, 977, 200, 499, 630, 956, 404, 723, 202, 874, 402, 223, 519, 383, 405, 923, 414, 475, 800, 1123, 1185, 59, 184, 700, 561, 1144, 1109, 562, 913, 102, 618, 708, 71, 335, 100, 1119, 925, 466, 451, 358, 993, 803, 529, 490, 1194, 628, 743, 699, 543, 361, 169, 341, 35, 705, 893, 685, 219, 843, 1003, 210, 583, 1052, 299, 906, 789, 354, 886, 776, 428, 910, 381, 16, 1086, 557, 506, 101, 7, 817, 538, 394, 331, 824, 1011, 1093, 88, 1170, 120, 797, 887, 1184, 460, 948, 571, 38, 225, 94, 150, 310, 1049, 474, 916, 480, 109, 602, 1068, 648, 390, 1007, 791, 1018, 481, 1060, 198, 1008, 559, 1113, 695, 471, 182, 483, 741, 426, 991, 669, 488, 347, 406, 867, 563, 313, 353, 282, 777, 155, 448, 1071, 256, 337, 1046, 343, 686, 429, 287, 449, 221, 889, 412, 822, 720, 1172, 439, 979, 709, 707, 762, 728, 581, 823, 163, 746, 1183, 437, 1164, 166, 1138, 1128, 1065, 511, 207, 1125, 515, 783, 880, 1108, 987, 69, 220, 386, 233, 625, 62, 842, 177, 1000, 462, 118, 1006, 1156, 537, 936, 98, 624, 766, 281, 175, 1072, 222, 495, 3, 805, 170, 678, 754, 517, 52, 1146, 1012, 267, 820, 115, 141, 784, 336, 582, 703, 864, 894, 306, 1214, 1057, 706, 20, 137, 492, 80, 682, 577, 847, 1044, 827, 377, 1084, 769, 197, 688, 160, 992, 962, 531, 234, 1188, 1037, 2, 597, 608, 532, 610, 148, 765, 12, 922, 1168, 1153, 545, 731, 304, 92, 1055, 1036, 632, 793, 1097, 190, 914, 547, 635, 1117, 47, 1162, 1085, 1161, 1139, 504, 946, 494, 1193, 940, 897, 615, 1202, 935, 795, 1073, 1196, 369, 427, 516, 479, 539, 15, 911, 24, 846, 573, 758, 340, 870, 826, 450, 651, 266, 469, 8, 694, 1191, 1152, 572, 216, 665, 11, 326, 535, 796, 183, 280, 954, 941, 892, 421, 252, 213, 963, 568, 416, 400, 753, 254, 217, 93, 454, 201, 799, 1041, 623, 382, 1015, 146, 947, 305, 125, 232, 398, 833, 34, 1099, 123, 164, 735, 530, 883, 1017, 48, 650, 950, 653, 603, 359, 514, 128, 980, 828, 113, 357, 957, 1076, 968, 643, 614, 976, 204, 283, 904, 711, 1024, 103, 1, 352, 87, 642, 594, 569, 592, 396, 111, 1091, 1190, 228, 275, 574, 188, 1074, 1134, 959, 46, 1147, 431, 565, 407, 939, 970, 566, 983, 638, 550, 857, 436, 807, 325, 224, 270, 830, 917, 273, 290, 486, 392, 253, 385, 859, 710, 65, 17, 928, 645, 564, 493, 288, 926, 408, 199, 644, 952, 840, 26, 187, 821, 172, 661, 611, 487, 554, 1151, 129, 121, 996, 579, 640, 167, 214, 929, 453, 1137, 933, 180, 801, 1154, 604, 1114, 825, 269, 697, 1211, 1030, 1207, 64, 526, 78, 53, 415, 284, 690, 1111, 984, 419, 237, 444, 575, 586, 670, 733, 478, 37, 323, 556, 943, 63, 1027, 193, 681, 289, 901, 902, 262, 468, 673, 461, 81, 327, 1166, 1061, 1179, 1115, 1043, 1209, 33, 508, 387, 764, 788, 246, 1059, 464, 238, 634, 544, 317, 484, 958, 463, 816, 443, 855, 675, 1005, 77, 342, 5, 802, 553, 445, 1051, 365, 440, 215, 679, 990, 330, 30, 1031, 192, 1035, 1213, 527, 518, 885, 1102, 279, 255, 845, 179, 1127, 149, 105, 745, 127, 1165, 1198, 110, 6, 522, 303, 74, 871, 899, 1130, 366, 107, 157, 1010, 875, 813, 965, 649, 333, 609, 156, 276, 458, 1145, 322, 1064, 869, 617, 860, 1195, 301, 982, 319, 737, 873, 775, 945, 814, 844, 1189, 729, 621, 838, 876, 355, 346, 411, 595, 1158, 585, 862, 1136, 83, 879, 51, 781, 423, 438, 1206, 153, 756, 755, 168, 937, 851, 663, 882, 605, 1208, 432, 1004, 861, 194, 1013, 19, 995, 633, 498, 265, 119, 1078, 918, 1088, 500, 300, 380, 312, 1066, 391, 512, 782, 931, 446, 145, 410, 452, 72, 804, 1038, 308, 13, 122, 744, 1118, 502, 856, 135, 212, 1132, 725, 293, 655, 639, 96, 798, 324, 677, 1150, 657, 809, 243, 818, 231, 1032, 734, 41, 434, 1002, 73, 652, 245, 978, 622, 1104, 56, 339, 401, 413, 473, 70, 713, 1083, 736, 760, 1106, 551, 208, 683, 376, 631, 593, 76, 521, 900, 84, 626, 334, 687, 601, 636, 286, 295, 853, 930, 356, 178, 1095, 555, 676, 104, 580, 1149, 1026, 1070, 698, 285, 1021, 696, 1200, 525, 40, 1080, 1181, 316, 627, 1192, 999, 29, 373, 714, 552, 689, 749, 205, 549, 848, 277, 588, 905, 953, 722, 1135, 1022, 264, 989, 1101, 967, 1116, 921, 274, 50, 752, 806, 23, 142, 934, 1199, 89, 960, 780, 422, 116, 787, 457, 672, 1081, 268, 658, 1063, 278, 1054, 1171, 985, 811, 196, 684, 21, 748, 558, 1186, 1204, 541, 570, 613, 10, 533, 1143, 54, 898, 378, 1062, 151, 974, 244, 348, 718, 321, 467, 969, 248, 435, 126, 241, 790, 964, 218, 491, 1016, 812, 1034, 375, 1025, 133, 140, 884, 455, 837, 1133, 1205, 42, 986, 505, 370, 1124, 206, 972, 1174, 520, 1075, 186, 637, 503, 349, 57, 162, 909, 185, 896, 159, 938, 1028, 291, 832, 739, 872, 890, 49, 590, 523, 489, 364, 152, 4, 409, 379, 779, 761, 702, 772, 472, 259, 738, 1103, 465, 1042, 671, 345, 195, 95, 591, 250, 1142, 907, 247, 314, 998, 58, 1177, 136, 476, 430, 1173, 955, 1180, 1169, 442, 1100, 895, 1039, 1090, 1069, 294, 757, 39, 389, 656, 318, 727, 831, 27, 44, 750, 785, 715, 66, 393, 920, 540, 724, 372, 31, 1140, 794, 584, 927, 124, 659, 91, 1187, 662, 459, 528, 1105, 973, 240, 548, 680, 1014, 257, 236, 1159, 263, 949, 272, 1212, 863, 596, 668, 771, 350, 888]
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0; data from UMLS, run 2.1, exp 616
rank avg (pred): 0.531 +- 0.002
mrr vals (pred, true): 0.014, 0.042
pnt losses (mrrl, rdl): 0.0, 3.8279e-06

running batch: 500; data from UMLS, run 2.1, exp 103
rank avg (pred): 0.450 +- 0.067
mrr vals (pred, true): 0.017, 0.047
pnt losses (mrrl, rdl): 0.0, 9.057e-07

running batch: 1000; data from UMLS, run 2.1, exp 472
rank avg (pred): 0.436 +- 0.088
mrr vals (pred, true): 0.018, 0.046
pnt losses (mrrl, rdl): 0.0, 4.796e-07

running batch: 1500; data from UMLS, run 2.2, exp 846
rank avg (pred): 0.436 +- 0.151
mrr vals (pred, true): 0.025, 0.048
pnt losses (mrrl, rdl): 0.0, 3.06e-07

running batch: 2000; data from UMLS, run 2.2, exp 748
rank avg (pred): 0.132 +- 0.082
mrr vals (pred, true): 0.166, 0.319
pnt losses (mrrl, rdl): 0.0, 4.587e-07

Epoch over!
epoch time: 33.908

Epoch 2 -- 
running batch: 0; data from UMLS, run 2.1, exp 616
rank avg (pred): 0.451 +- 0.209
mrr vals (pred, true): 0.060, 0.042
pnt losses (mrrl, rdl): 0.0, 2.834e-07

running batch: 500; data from UMLS, run 2.1, exp 103
rank avg (pred): 0.439 +- 0.243
mrr vals (pred, true): 0.102, 0.047
pnt losses (mrrl, rdl): 0.0, 1.046e-07

running batch: 1000; data from UMLS, run 2.1, exp 472
rank avg (pred): 0.432 +- 0.262
mrr vals (pred, true): 0.134, 0.046
pnt losses (mrrl, rdl): 0.0, 7.7e-09

running batch: 1500; data from UMLS, run 2.2, exp 846
rank avg (pred): 0.436 +- 0.268
mrr vals (pred, true): 0.137, 0.048
pnt losses (mrrl, rdl): 0.0, 2.16e-08

running batch: 2000; data from UMLS, run 2.2, exp 748
rank avg (pred): 0.129 +- 0.090
mrr vals (pred, true): 0.267, 0.319
pnt losses (mrrl, rdl): 0.0, 3.263e-07

Epoch over!
epoch time: 34.346

Done training phase:  0
Epoch 1 -- 
running batch: 0; data from UMLS, run 2.1, exp 616
rank avg (pred): 0.445 +- 0.267
mrr vals (pred, true): 0.140, 0.042
pnt losses (mrrl, rdl): 0.0960707366, 4.165e-07

running batch: 500; data from UMLS, run 2.1, exp 103
rank avg (pred): 0.399 +- 0.138
mrr vals (pred, true): 0.051, 0.047
pnt losses (mrrl, rdl): 0.0001199301, 1.2336e-06

running batch: 1000; data from UMLS, run 2.1, exp 472
rank avg (pred): 0.403 +- 0.118
mrr vals (pred, true): 0.039, 0.046
pnt losses (mrrl, rdl): 0.0004253318, 1.1298e-06

running batch: 1500; data from UMLS, run 2.2, exp 846
rank avg (pred): 0.424 +- 0.125
mrr vals (pred, true): 0.046, 0.048
pnt losses (mrrl, rdl): 6.96741e-05, 4.833e-07

running batch: 2000; data from UMLS, run 2.2, exp 748
rank avg (pred): 0.042 +- 0.025
mrr vals (pred, true): 0.319, 0.319
pnt losses (mrrl, rdl): 5.68e-08, 4.639e-06

Epoch over!
epoch time: 34.921

Epoch 2 -- 
running batch: 0; data from UMLS, run 2.1, exp 616
rank avg (pred): 0.558 +- 0.141
mrr vals (pred, true): 0.042, 0.042
pnt losses (mrrl, rdl): 1.3573e-06, 7.196e-06

running batch: 500; data from UMLS, run 2.1, exp 103
rank avg (pred): 0.382 +- 0.117
mrr vals (pred, true): 0.051, 0.047
pnt losses (mrrl, rdl): 0.0001459509, 2.5578e-06

running batch: 1000; data from UMLS, run 2.1, exp 472
rank avg (pred): 0.382 +- 0.130
mrr vals (pred, true): 0.040, 0.046
pnt losses (mrrl, rdl): 0.0003706061, 2.4984e-06

running batch: 1500; data from UMLS, run 2.2, exp 846
rank avg (pred): 0.387 +- 0.121
mrr vals (pred, true): 0.046, 0.048
pnt losses (mrrl, rdl): 5.07937e-05, 2.3251e-06

running batch: 2000; data from UMLS, run 2.2, exp 748
rank avg (pred): 0.050 +- 0.031
mrr vals (pred, true): 0.327, 0.319
pnt losses (mrrl, rdl): 0.0005471952, 3.6746e-06

Epoch over!
epoch time: 36.023

Epoch 3 -- 
running batch: 0; data from UMLS, run 2.1, exp 616
rank avg (pred): 0.560 +- 0.113
mrr vals (pred, true): 0.040, 0.042
pnt losses (mrrl, rdl): 2.79117e-05, 7.6848e-06

running batch: 500; data from UMLS, run 2.1, exp 103
rank avg (pred): 0.355 +- 0.125
mrr vals (pred, true): 0.051, 0.047
pnt losses (mrrl, rdl): 0.0001761498, 5.4064e-06

running batch: 1000; data from UMLS, run 2.1, exp 472
rank avg (pred): 0.364 +- 0.149
mrr vals (pred, true): 0.041, 0.046
pnt losses (mrrl, rdl): 0.0002150045, 4.3549e-06

running batch: 1500; data from UMLS, run 2.2, exp 846
rank avg (pred): 0.393 +- 0.136
mrr vals (pred, true): 0.046, 0.048
pnt losses (mrrl, rdl): 5.86966e-05, 1.8299e-06

running batch: 2000; data from UMLS, run 2.2, exp 748
rank avg (pred): 0.077 +- 0.050
mrr vals (pred, true): 0.318, 0.319
pnt losses (mrrl, rdl): 7.8831e-06, 1.2167e-06

Epoch over!
epoch time: 37.521

Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.396 +- 0.134
mrr vals (pred, true): 0.047, 0.043

Evaluation for UMLS on the test set
==========================================

(Sorted by True MRR values)
Predicted MRRs 	 True MRRs
------------------------------------------
0.026046380400657654 	 0.01901227794587612
0.026688873767852783 	 0.02014322765171528
0.026688873767852783 	 0.020248999819159508
0.026046380400657654 	 0.020502964034676552
0.025843262672424316 	 0.021110069006681442
0.025843262672424316 	 0.021767141297459602
0.02946510724723339 	 0.022630179300904274
0.024269454181194305 	 0.02327798679471016
0.03304100036621094 	 0.023699799552559853
0.02611766941845417 	 0.024140600115060806
0.027438011020421982 	 0.024325493723154068
0.028363294899463654 	 0.02466714195907116
0.027438011020421982 	 0.024746794253587723
0.026496699079871178 	 0.02484562061727047
0.024269454181194305 	 0.02485971711575985
0.03013494424521923 	 0.024972569197416306
0.03013494424521923 	 0.02522987127304077
0.03304100036621094 	 0.025235667824745178
0.026496699079871178 	 0.025793183594942093
0.025111958384513855 	 0.025868749246001244
0.03257562965154648 	 0.026167435571551323
0.028363294899463654 	 0.02636955864727497
0.02946510724723339 	 0.0266532301902771
0.03846447914838791 	 0.02695481665432453
0.025111958384513855 	 0.027670670300722122
0.03257562965154648 	 0.027944505214691162
0.02611766941845417 	 0.029529646039009094
0.03846447914838791 	 0.03137117624282837
0.03670431300997734 	 0.032100025564432144
0.04051114246249199 	 0.033584434539079666
0.03670431300997734 	 0.03388344496488571
0.03834262117743492 	 0.03405838459730148
0.03834262117743492 	 0.034171730279922485
0.04051114246249199 	 0.0344768762588501
0.03985625505447388 	 0.034479349851608276
0.038716789335012436 	 0.035177480429410934
0.0466078519821167 	 0.035437460988759995
0.04025784134864807 	 0.035483457148075104
0.038716789335012436 	 0.036305204033851624
0.04162623733282089 	 0.036459844559431076
0.041201453655958176 	 0.03687036409974098
0.04062077775597572 	 0.036992862820625305
0.03987598791718483 	 0.037044428288936615
0.03885522857308388 	 0.03747216984629631
0.04095087945461273 	 0.03840930387377739
0.04025784134864807 	 0.03884386643767357
0.03796453773975372 	 0.038847438991069794
0.0467064194381237 	 0.03909123316407204
0.039544861763715744 	 0.03926670178771019
0.039544861763715744 	 0.039312705397605896
0.041201453655958176 	 0.03937135636806488
0.03952505439519882 	 0.03948157653212547
0.04703687131404877 	 0.03955230861902237
0.03952505439519882 	 0.039851896464824677
0.047185562551021576 	 0.04008961841464043
0.04095087945461273 	 0.04018724337220192
0.04065936803817749 	 0.04023614898324013
0.03879569470882416 	 0.04035545140504837
0.04162623733282089 	 0.04038001224398613
0.04801489785313606 	 0.040408555418252945
0.038265448063611984 	 0.04049674794077873
0.04550330340862274 	 0.040533121675252914
0.04738383740186691 	 0.04087226465344429
0.04734588414430618 	 0.04094018414616585
0.038265448063611984 	 0.041287511587142944
0.03822030872106552 	 0.04136058688163757
0.04714363068342209 	 0.04142501577734947
0.04744070768356323 	 0.041469477117061615
0.03985625505447388 	 0.04157405346632004
0.03987598791718483 	 0.04163471236824989
0.03822030872106552 	 0.04189727082848549
0.04041533172130585 	 0.04206017032265663
0.0467064194381237 	 0.04207504913210869
0.046881403774023056 	 0.042406097054481506
0.04065936803817749 	 0.04260354861617088
0.047736674547195435 	 0.04261063039302826
0.04687611013650894 	 0.04271402955055237
0.04723323881626129 	 0.04275700822472572
0.04702478647232056 	 0.04305602237582207
0.04676336050033569 	 0.04326927289366722
0.046561338007450104 	 0.04353133589029312
0.04774906486272812 	 0.0436079278588295
0.046831805258989334 	 0.043645359575748444
0.03879569470882416 	 0.04366683214902878
0.048358771950006485 	 0.0436854250729084
0.047391101717948914 	 0.04369112104177475
0.03796453773975372 	 0.04373832419514656
0.0468791238963604 	 0.043791092932224274
0.04630346596240997 	 0.043995171785354614
0.047513000667095184 	 0.044126082211732864
0.04714363068342209 	 0.04420103505253792
0.047977838665246964 	 0.04423436522483826
0.04550330340862274 	 0.044293273240327835
0.04697783291339874 	 0.04440216347575188
0.046126049011945724 	 0.044545359909534454
0.047391101717948914 	 0.04454592615365982
0.047011688351631165 	 0.044563427567481995
0.047035250812768936 	 0.04479734227061272
0.047324731945991516 	 0.04482435807585716
0.046898506581783295 	 0.045094192028045654
0.04628368839621544 	 0.04515465348958969
0.03879290819168091 	 0.045167602598667145
0.038881316781044006 	 0.04518784582614899
0.047181472182273865 	 0.04520825669169426
0.046126049011945724 	 0.045370880514383316
0.04693498834967613 	 0.04549645259976387
0.047181472182273865 	 0.04566936194896698
0.038881316781044006 	 0.045777104794979095
0.04603695496916771 	 0.04592272266745567
0.04801489785313606 	 0.04593709856271744
0.046561338007450104 	 0.04625868424773216
0.04738383740186691 	 0.04625905677676201
0.04697234556078911 	 0.04627582058310509
0.04828856140375137 	 0.04655187949538231
0.04844396561384201 	 0.04665511101484299
0.04752945899963379 	 0.04665983468294144
0.04704580456018448 	 0.04668403044342995
0.04700964689254761 	 0.046916596591472626
0.04540868103504181 	 0.046994879841804504
0.047615598887205124 	 0.04701061546802521
0.04628368839621544 	 0.04701905697584152
0.04700964689254761 	 0.04702542722225189
0.04713670536875725 	 0.047086842358112335
0.038285255432128906 	 0.04713992774486542
0.04630346596240997 	 0.04722273349761963
0.04697234556078911 	 0.0474671833217144
0.04827677458524704 	 0.047600939869880676
0.047086603939533234 	 0.04764162003993988
0.047615598887205124 	 0.047732848674058914
0.0483454093337059 	 0.04817711561918259
0.0483454093337059 	 0.04818395897746086
0.046881403774023056 	 0.048190899193286896
0.0406356118619442 	 0.04822370037436485
0.04011200740933418 	 0.04827207326889038
0.04752945899963379 	 0.04833891615271568
0.04828856140375137 	 0.04842860996723175
0.042363740503787994 	 0.048438429832458496
0.047185562551021576 	 0.04855053126811981
0.04704580456018448 	 0.04855949804186821
0.04827677458524704 	 0.04860858991742134
0.047011688351631165 	 0.048655517399311066
0.04697783291339874 	 0.04870794340968132
0.047220468521118164 	 0.04878751188516617
0.04844396561384201 	 0.048864658921957016
0.04693498834967613 	 0.048930972814559937
0.0406356118619442 	 0.04896869137883186
0.047035250812768936 	 0.04898199066519737
0.03885522857308388 	 0.049135319888591766
0.0466078519821167 	 0.04927906021475792
0.046964727342128754 	 0.049358513206243515
0.0476919487118721 	 0.049380313605070114
0.04777362570166588 	 0.04957657679915428
0.04603695496916771 	 0.0495842769742012
0.038610994815826416 	 0.049590449780225754
0.047324731945991516 	 0.0497579388320446
0.04821374639868736 	 0.049896448850631714
0.047513000667095184 	 0.05014357343316078
0.04674143344163895 	 0.050270821899175644
0.046342846006155014 	 0.0505097396671772
0.04774906486272812 	 0.05056457221508026
0.04674143344163895 	 0.05068950727581978
0.04734588414430618 	 0.05086950212717056
0.03879290819168091 	 0.05093161389231682
0.046964727342128754 	 0.05094859376549721
0.04062077775597572 	 0.05095751956105232
0.038285255432128906 	 0.05099565535783768
0.04540868103504181 	 0.05116703733801842
0.047086603939533234 	 0.051199864596128464
0.04744070768356323 	 0.05133404582738876
0.04702478647232056 	 0.05137281492352486
0.038610994815826416 	 0.051667265594005585
0.04821374639868736 	 0.051804911345243454
0.046898506581783295 	 0.0518244169652462
0.04011200740933418 	 0.05203264579176903
0.042363740503787994 	 0.052148524671792984
0.0468791238963604 	 0.05243628844618797
0.046342846006155014 	 0.0528007447719574
0.046831805258989334 	 0.05293651670217514
0.04723323881626129 	 0.05332837253808975
0.04777362570166588 	 0.05340009555220604
0.0476919487118721 	 0.05365379899740219
0.048358771950006485 	 0.053963806480169296
0.04676336050033569 	 0.05403280258178711
0.04687611013650894 	 0.054094746708869934
0.047736674547195435 	 0.05421387776732445
0.04703687131404877 	 0.05438901484012604
0.047977838665246964 	 0.05524349585175514
0.047220468521118164 	 0.056530874222517014
0.04041533172130585 	 0.05977191776037216
0.04713670536875725 	 0.06076590716838837
0.20204176008701324 	 0.1810765117406845
0.20125384628772736 	 0.18117085099220276
0.20688240230083466 	 0.18888519704341888
0.19899451732635498 	 0.18969745934009552
0.20688240230083466 	 0.19276924431324005
0.21701470017433167 	 0.19464214146137238
0.19899451732635498 	 0.1960158795118332
0.19723564386367798 	 0.19603534042835236
0.2087557464838028 	 0.19711504876613617
0.20204176008701324 	 0.19871516525745392
0.20125384628772736 	 0.2019096314907074
0.2087557464838028 	 0.2040712684392929
0.25601860880851746 	 0.2053784281015396
0.2286376953125 	 0.21149252355098724
0.21701470017433167 	 0.21227645874023438
0.23760926723480225 	 0.21635518968105316
0.2286376953125 	 0.21805109083652496
0.19723564386367798 	 0.21915510296821594
0.23760926723480225 	 0.22773414850234985
0.22791020572185516 	 0.23087501525878906
0.22231249511241913 	 0.23217003047466278
0.25601860880851746 	 0.23289592564105988
0.22231249511241913 	 0.24074165523052216
0.2209644317626953 	 0.2439204901456833
0.2193506509065628 	 0.24656075239181519
0.24461734294891357 	 0.24876080453395844
0.2209644317626953 	 0.2516258656978607
0.22791020572185516 	 0.25288742780685425
0.2193506509065628 	 0.26047763228416443
0.25233861804008484 	 0.2687990069389343
0.2872127890586853 	 0.2714383602142334
0.24461734294891357 	 0.27556994557380676
0.27818164229393005 	 0.2773684561252594
0.24189701676368713 	 0.2807803750038147
0.24189701676368713 	 0.28487512469291687
0.25233861804008484 	 0.2887305021286011
0.30660998821258545 	 0.2910597622394562
0.27149832248687744 	 0.2963320016860962
0.2872127890586853 	 0.2978752851486206
0.30660998821258545 	 0.29966503381729126
0.27818164229393005 	 0.30019211769104004
0.29817143082618713 	 0.30703768134117126
0.27149832248687744 	 0.3079633116722107
0.29817143082618713 	 0.3112635314464569
0.3192073404788971 	 0.3235786557197571
0.3192073404788971 	 0.3395030200481415
0.3610892593860626 	 0.3881693184375763
0.3610892593860626 	 0.4064561128616333
0.34078285098075867 	 0.5045468211174011
0.34078285098075867 	 0.5242881178855896
0.35564684867858887 	 0.539236307144165
0.35564684867858887 	 0.5725268721580505

r_mrr = tensor([[1.0000, 0.9742],
        [0.9742, 1.0000]])
r2_mrr = 0.9352352023124695
average test loss: 0.007048227249913077
	test time: 1.152
Done Testing dataset UMLS
total time taken: 192.62408351898193
training time taken: 177.8782172203064
