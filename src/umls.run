using splits:
test_ids (121): [597, 746, 43, 910, 288, 145, 1176, 967, 1023, 309, 391, 727, 930, 507, 1033, 600, 267, 934, 1078, 175, 565, 139, 113, 954, 789, 446, 816, 11, 674, 633, 742, 304, 389, 365, 987, 854, 702, 108, 1024, 771, 157, 592, 607, 1032, 14, 1010, 678, 2, 666, 1001, 1037, 737, 582, 464, 8, 344, 531, 282, 160, 1165, 21, 704, 971, 543, 1153, 687, 656, 156, 372, 183, 715, 478, 106, 1026, 211, 481, 1084, 1206, 578, 552, 599, 524, 525, 747, 245, 1209, 326, 749, 1199, 1109, 225, 763, 1074, 856, 796, 23, 1157, 166, 651, 313, 500, 1152, 1142, 129, 403, 661, 1027, 1050, 122, 800, 1131, 1138, 793, 380, 440, 831, 1048, 744, 642, 576, 685]
valid_ids (0): []
train_ids (1094): [173, 1046, 945, 615, 439, 1102, 278, 623, 1077, 98, 220, 522, 226, 1114, 105, 1005, 659, 228, 17, 376, 435, 115, 126, 1002, 1047, 813, 1197, 598, 634, 180, 399, 617, 804, 719, 367, 885, 1179, 388, 923, 202, 717, 562, 542, 769, 235, 192, 387, 1066, 1053, 67, 946, 1144, 612, 91, 916, 859, 40, 255, 510, 641, 219, 724, 1192, 489, 426, 772, 825, 53, 1012, 780, 937, 755, 294, 257, 177, 677, 123, 394, 631, 490, 436, 259, 110, 437, 1185, 823, 912, 443, 281, 832, 56, 680, 734, 27, 907, 1175, 1168, 924, 1123, 778, 377, 1210, 999, 1057, 483, 204, 608, 564, 451, 187, 686, 838, 619, 221, 997, 862, 7, 1100, 723, 218, 51, 275, 776, 745, 339, 638, 665, 109, 781, 312, 189, 1164, 283, 390, 364, 782, 857, 1212, 821, 36, 395, 889, 414, 209, 398, 541, 779, 784, 341, 1049, 198, 132, 753, 584, 141, 914, 1148, 1205, 917, 579, 176, 864, 382, 247, 649, 250, 1139, 181, 1040, 811, 970, 1143, 1004, 465, 950, 353, 561, 1124, 567, 47, 80, 819, 493, 880, 237, 84, 829, 697, 144, 978, 266, 149, 347, 1106, 664, 371, 318, 711, 1058, 100, 1089, 75, 861, 1130, 30, 956, 551, 892, 899, 585, 0, 352, 136, 1198, 696, 138, 611, 69, 230, 874, 690, 878, 830, 991, 134, 939, 688, 296, 348, 279, 701, 501, 654, 342, 1186, 485, 15, 1146, 790, 496, 140, 761, 249, 244, 1007, 50, 93, 432, 117, 671, 732, 1122, 869, 625, 1019, 104, 299, 571, 120, 807, 663, 1214, 591, 24, 383, 941, 906, 197, 1107, 328, 716, 357, 1082, 393, 363, 1069, 1055, 994, 196, 103, 271, 48, 1073, 1208, 794, 5, 1011, 826, 614, 1045, 851, 884, 580, 815, 68, 528, 935, 95, 893, 1116, 871, 993, 1162, 415, 233, 774, 1016, 972, 289, 979, 386, 1072, 143, 968, 1028, 843, 1121, 1039, 689, 553, 989, 72, 18, 827, 476, 1203, 1202, 4, 158, 272, 445, 404, 1067, 419, 853, 227, 142, 374, 1051, 714, 658, 146, 837, 513, 814, 38, 401, 359, 1030, 9, 845, 243, 795, 402, 76, 396, 71, 768, 1184, 757, 1154, 546, 410, 873, 499, 1018, 707, 695, 273, 265, 969, 759, 90, 699, 1115, 817, 647, 147, 479, 833, 1201, 442, 1035, 1097, 1129, 503, 92, 1120, 627, 740, 560, 185, 448, 210, 453, 89, 1092, 648, 1200, 1101, 223, 236, 986, 1195, 725, 1095, 425, 1087, 514, 470, 52, 675, 306, 898, 581, 397, 850, 660, 1183, 1025, 416, 65, 812, 616, 1111, 325, 544, 519, 897, 901, 280, 343, 292, 262, 1117, 462, 362, 879, 291, 637, 1188, 765, 360, 913, 900, 420, 354, 842, 975, 1085, 407, 1180, 327, 179, 786, 471, 751, 261, 1171, 277, 58, 1141, 764, 797, 667, 480, 502, 712, 201, 682, 468, 530, 532, 512, 270, 605, 477, 883, 1105, 212, 992, 990, 182, 905, 741, 411, 421, 646, 205, 248, 55, 1149, 533, 1021, 1034, 258, 406, 977, 995, 1108, 1181, 207, 253, 317, 25, 232, 808, 6, 314, 700, 632, 721, 107, 1140, 806, 980, 1161, 35, 234, 891, 863, 366, 621, 1006, 706, 330, 876, 1094, 942, 604, 472, 96, 1191, 1204, 1038, 887, 911, 487, 1182, 251, 787, 133, 457, 1065, 290, 773, 920, 311, 1003, 413, 756, 548, 618, 1134, 1104, 322, 547, 1125, 284, 908, 805, 1118, 523, 748, 662, 974, 982, 246, 1017, 427, 511, 952, 332, 128, 57, 19, 1086, 676, 163, 83, 568, 559, 222, 835, 242, 118, 1090, 191, 151, 1170, 921, 458, 929, 1015, 368, 463, 936, 860, 256, 809, 736, 981, 82, 918, 570, 557, 573, 872, 484, 798, 295, 672, 1158, 783, 1079, 657, 460, 321, 350, 285, 894, 693, 1008, 491, 590, 549, 855, 1173, 731, 653, 433, 1059, 566, 1193, 392, 224, 949, 708, 1169, 263, 130, 606, 931, 996, 102, 1145, 504, 137, 655, 97, 517, 1194, 758, 1151, 461, 252, 1098, 1088, 718, 333, 206, 1056, 127, 928, 964, 370, 1, 31, 238, 1110, 10, 430, 976, 922, 164, 983, 963, 125, 86, 1167, 629, 188, 788, 165, 509, 349, 698, 1132, 574, 1099, 159, 60, 620, 959, 498, 444, 558, 74, 927, 609, 572, 722, 203, 508, 875, 172, 785, 28, 505, 650, 124, 131, 720, 834, 681, 973, 938, 459, 260, 488, 1207, 431, 728, 302, 733, 1096, 601, 469, 178, 305, 955, 556, 957, 767, 54, 594, 791, 161, 909, 750, 948, 828, 154, 903, 447, 640, 1133, 1136, 168, 555, 966, 276, 836, 943, 610, 1031, 626, 867, 520, 984, 703, 323, 563, 1009, 1091, 116, 1080, 454, 422, 762, 486, 538, 1036, 20, 1172, 849, 473, 385, 915, 81, 890, 155, 135, 754, 792, 153, 953, 586, 405, 730, 536, 577, 61, 1103, 400, 301, 193, 705, 1013, 170, 423, 1137, 925, 803, 70, 1178, 240, 298, 356, 208, 456, 587, 683, 760, 844, 355, 766, 358, 841, 29, 199, 492, 1174, 799, 59, 375, 33, 1113, 310, 274, 1075, 1054, 639, 88, 668, 882, 919, 94, 1000, 593, 752, 644, 684, 539, 450, 46, 603, 1119, 268, 167, 998, 526, 287, 101, 269, 497, 369, 595, 630, 16, 1177, 424, 1014, 1196, 49, 951, 886, 434, 466, 710, 417, 888, 452, 214, 379, 213, 119, 652, 550, 215, 1041, 669, 628, 902, 588, 441, 777, 482, 408, 506, 44, 428, 1150, 384, 335, 361, 944, 329, 1156, 12, 839, 186, 316, 77, 535, 738, 26, 985, 1020, 895, 449, 569, 438, 961, 340, 331, 229, 896, 1063, 866, 709, 297, 636, 475, 1068, 545, 87, 1044, 190, 378, 216, 1128, 1070, 381, 847, 1052, 735, 1190, 150, 1062, 495, 351, 537, 319, 540, 334, 474, 670, 602, 39, 726, 904, 801, 409, 1127, 66, 307, 293, 940, 162, 622, 852, 1076, 1160, 308, 1083, 1022, 958, 1060, 114, 1135, 962, 112, 200, 412, 1071, 583, 42, 324, 1061, 45, 554, 345, 613, 300, 743, 13, 775, 346, 933, 32, 373, 1163, 521, 467, 858, 694, 1043, 965, 455, 320, 254, 1093, 518, 846, 79, 1081, 286, 515, 315, 820, 1166, 34, 739, 802, 1112, 1042, 624, 770, 877, 1159, 824, 62, 810, 338, 1189, 1211, 713, 1187, 64, 947, 932, 1155, 1126, 41, 589, 529, 264, 865, 111, 99, 960, 1147, 988, 195, 241, 645, 121, 171, 429, 169, 679, 494, 239, 1064, 516, 534, 868, 231, 673, 73, 418, 527, 63, 596, 22, 1029, 174, 184, 729, 194, 881, 337, 692, 870, 336, 85, 691, 303, 643, 926, 37, 818, 575, 78, 3, 1213, 848, 152, 822, 148, 635, 840, 217]
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 2 and 2: 3
Epoch 1 -- 
running batch: 0; data from UMLS, run 2.1, exp 173
rank avg (pred): 0.469 +- 0.006
mrr vals (pred, true): 0.016, 0.045
pnt losses (mrrl, rdl): 0.0, 1.438e-06

running batch: 500; data from UMLS, run 2.1, exp 1181
rank avg (pred): 0.495 +- 0.004
mrr vals (pred, true): 0.015, 0.038
pnt losses (mrrl, rdl): 0.0, 3.3664e-06

running batch: 1000; data from UMLS, run 2.1, exp 320
rank avg (pred): 0.170 +- 0.006
mrr vals (pred, true): 0.042, 0.197
pnt losses (mrrl, rdl): 0.0, 9.507e-07

running batch: 1500; data from UMLS, run 2.2, exp 425
rank avg (pred): 0.438 +- 0.001
mrr vals (pred, true): 0.017, 0.048
pnt losses (mrrl, rdl): 0.0, 8.968e-07

running batch: 2000; data from UMLS, run 2.2, exp 26
rank avg (pred): 0.185 +- 0.003
mrr vals (pred, true): 0.039, 0.221
pnt losses (mrrl, rdl): 0.0, 4.381e-07

Epoch over!
epoch time: 34.445

Epoch 2 -- 
running batch: 0; data from UMLS, run 2.1, exp 173
rank avg (pred): 0.446 +- 0.001
mrr vals (pred, true): 0.016, 0.045
pnt losses (mrrl, rdl): 0.0, 6.867e-07

running batch: 500; data from UMLS, run 2.1, exp 1181
rank avg (pred): 0.463 +- 0.001
mrr vals (pred, true): 0.016, 0.038
pnt losses (mrrl, rdl): 0.0, 1.092e-06

running batch: 1000; data from UMLS, run 2.1, exp 320
rank avg (pred): 0.180 +- 0.001
mrr vals (pred, true): 0.040, 0.197
pnt losses (mrrl, rdl): 0.0, 5.459e-07

running batch: 1500; data from UMLS, run 2.2, exp 425
rank avg (pred): 0.438 +- 0.000
mrr vals (pred, true): 0.017, 0.048
pnt losses (mrrl, rdl): 0.0, 8.938e-07

running batch: 2000; data from UMLS, run 2.2, exp 26
rank avg (pred): 0.187 +- 0.000
mrr vals (pred, true): 0.038, 0.221
pnt losses (mrrl, rdl): 0.0, 4.749e-07

Epoch over!
epoch time: 35.147

Done training phase:  0
Epoch 1 -- 
running batch: 0; data from UMLS, run 2.1, exp 173
rank avg (pred): 0.447 +- 0.000
mrr vals (pred, true): 0.016, 0.045
pnt losses (mrrl, rdl): 0.0082212985, 7.013e-07

running batch: 500; data from UMLS, run 2.1, exp 1181
rank avg (pred): 0.227 +- 0.070
mrr vals (pred, true): 0.039, 0.038
pnt losses (mrrl, rdl): 2.07765e-05, 3.96165e-05

running batch: 1000; data from UMLS, run 2.1, exp 320
rank avg (pred): 0.040 +- 0.014
mrr vals (pred, true): 0.191, 0.197
pnt losses (mrrl, rdl): 0.0004259539, 2.30086e-05

running batch: 1500; data from UMLS, run 2.2, exp 425
rank avg (pred): 0.259 +- 0.049
mrr vals (pred, true): 0.029, 0.048
pnt losses (mrrl, rdl): 0.0033428275, 3.36342e-05

running batch: 2000; data from UMLS, run 2.2, exp 26
rank avg (pred): 0.038 +- 0.016
mrr vals (pred, true): 0.220, 0.221
pnt losses (mrrl, rdl): 1.45108e-05, 1.63386e-05

Epoch over!
epoch time: 37.627

Epoch 2 -- 
running batch: 0; data from UMLS, run 2.1, exp 173
rank avg (pred): 0.168 +- 0.054
mrr vals (pred, true): 0.051, 0.045
pnt losses (mrrl, rdl): 0.0003505761, 6.52688e-05

running batch: 500; data from UMLS, run 2.1, exp 1181
rank avg (pred): 0.180 +- 0.049
mrr vals (pred, true): 0.046, 0.038
pnt losses (mrrl, rdl): 0.0006685555, 5.92105e-05

running batch: 1000; data from UMLS, run 2.1, exp 320
rank avg (pred): 0.045 +- 0.021
mrr vals (pred, true): 0.216, 0.197
pnt losses (mrrl, rdl): 0.0035137299, 2.14897e-05

running batch: 1500; data from UMLS, run 2.2, exp 425
rank avg (pred): 0.223 +- 0.064
mrr vals (pred, true): 0.037, 0.048
pnt losses (mrrl, rdl): 0.0010623338, 4.73924e-05

running batch: 2000; data from UMLS, run 2.2, exp 26
rank avg (pred): 0.056 +- 0.028
mrr vals (pred, true): 0.212, 0.221
pnt losses (mrrl, rdl): 0.000830883, 1.21978e-05

Epoch over!
epoch time: 36.936

Epoch 3 -- 
running batch: 0; data from UMLS, run 2.1, exp 173
rank avg (pred): 0.183 +- 0.061
mrr vals (pred, true): 0.047, 0.045
pnt losses (mrrl, rdl): 2.79424e-05, 5.8272e-05

running batch: 500; data from UMLS, run 2.1, exp 1181
rank avg (pred): 0.191 +- 0.050
mrr vals (pred, true): 0.043, 0.038
pnt losses (mrrl, rdl): 0.0002130371, 5.4241e-05

running batch: 1000; data from UMLS, run 2.1, exp 320
rank avg (pred): 0.063 +- 0.034
mrr vals (pred, true): 0.225, 0.197
pnt losses (mrrl, rdl): 0.0076407501, 1.68289e-05

running batch: 1500; data from UMLS, run 2.2, exp 425
rank avg (pred): 0.198 +- 0.076
mrr vals (pred, true): 0.050, 0.048
pnt losses (mrrl, rdl): 3.69059e-05, 5.78702e-05

running batch: 2000; data from UMLS, run 2.2, exp 26
rank avg (pred): 0.079 +- 0.043
mrr vals (pred, true): 0.208, 0.221
pnt losses (mrrl, rdl): 0.001577254, 7.7675e-06

Epoch over!
epoch time: 36.786

Done training phase:  1
Testing model with dataset UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.157 +- 0.022
mrr vals (pred, true): 0.046, 0.037

Evaluation for UMLS on the test set
==========================================

(Sorted by True MRR values)
Predicted MRRs 	 True MRRs
------------------------------------------
0.03092758171260357 	 0.018752800300717354 	 miss
0.03092758171260357 	 0.020450986921787262 	 miss
0.029335008934140205 	 0.022987565025687218 	 ~
0.02417292818427086 	 0.023041613399982452 	 ~
0.020717425271868706 	 0.023144498467445374 	 ~
0.02495245821774006 	 0.02327798679471016 	 ~
0.023778168484568596 	 0.023424558341503143 	 ~
0.023148898035287857 	 0.023740366101264954 	 ~
0.02417292818427086 	 0.023883739486336708 	 ~
0.031153101474046707 	 0.02403140440583229 	 ~
0.029335008934140205 	 0.024056600406765938 	 ~
0.0249207504093647 	 0.024144770577549934 	 ~
0.02140669710934162 	 0.024239057675004005 	 ~
0.02140669710934162 	 0.02442600205540657 	 ~
0.02474740706384182 	 0.024669872596859932 	 ~
0.02495245821774006 	 0.02485971711575985 	 ~
0.02277810126543045 	 0.02527785301208496 	 ~
0.020717425271868706 	 0.02535575069487095 	 ~
0.03362211957573891 	 0.026066146790981293 	 ~
0.0249207504093647 	 0.026216158643364906 	 ~
0.02277810126543045 	 0.026334170252084732 	 ~
0.02474740706384182 	 0.02637500874698162 	 ~
0.024697884917259216 	 0.026400573551654816 	 ~
0.023778168484568596 	 0.026421628892421722 	 ~
0.03362211957573891 	 0.026508400216698647 	 ~
0.023148898035287857 	 0.026602357625961304 	 ~
0.024697884917259216 	 0.027814600616693497 	 ~
0.031153101474046707 	 0.02848825231194496 	 ~
0.037379708141088486 	 0.029786767438054085 	 ~
0.043343327939510345 	 0.03073578141629696 	 miss
0.037379708141088486 	 0.03238515928387642 	 ~
0.03629093989729881 	 0.03405838459730148 	 ~
0.03629093989729881 	 0.034171730279922485 	 ~
0.033823683857917786 	 0.03444935008883476 	 ~
0.047020819038152695 	 0.03461018577218056 	 miss
0.033823683857917786 	 0.03512485325336456 	 ~
0.047020819038152695 	 0.035487864166498184 	 miss
0.043343327939510345 	 0.035824961960315704 	 ~
0.04206385836005211 	 0.03636784851551056 	 ~
0.041613392531871796 	 0.036644503474235535 	 ~
0.046226393431425095 	 0.036817263811826706 	 ~
0.04206385836005211 	 0.03688788786530495 	 ~
0.045653726905584335 	 0.036992862820625305 	 ~
0.04357437416911125 	 0.03740879148244858 	 ~
0.041178494691848755 	 0.037529394030570984 	 ~
0.042215745896101 	 0.038847438991069794 	 ~
0.052661240100860596 	 0.03919484466314316 	 miss
0.04054858162999153 	 0.03926744684576988 	 ~
0.03709349408745766 	 0.039896219968795776 	 ~
0.043736789375543594 	 0.04019593447446823 	 ~
0.04624054208397865 	 0.04021887108683586 	 ~
0.03805207088589668 	 0.04022476077079773 	 ~
0.04341140016913414 	 0.04035545140504837 	 ~
0.04464685544371605 	 0.040452051907777786 	 ~
0.04708878695964813 	 0.04046334698796272 	 ~
0.04783874377608299 	 0.040517717599868774 	 ~
0.0368429496884346 	 0.040812112390995026 	 ~
0.03955076262354851 	 0.04087226465344429 	 ~
0.04323854669928551 	 0.040905434638261795 	 ~
0.050766076892614365 	 0.04143751412630081 	 ~
0.03682028129696846 	 0.041469477117061615 	 ~
0.043736789375543594 	 0.04150528460741043 	 ~
0.041613392531871796 	 0.04165087267756462 	 ~
0.041613392531871796 	 0.041737720370292664 	 ~
0.041287198662757874 	 0.04176430404186249 	 ~
0.041613392531871796 	 0.04237916320562363 	 ~
0.041178494691848755 	 0.04258408024907112 	 ~
0.03822844475507736 	 0.04278584197163582 	 ~
0.036300621926784515 	 0.043131161481142044 	 ~
0.039853036403656006 	 0.04318705201148987 	 ~
0.050766076892614365 	 0.04346238449215889 	 ~
0.04071503505110741 	 0.043537989258766174 	 ~
0.04341140016913414 	 0.04366683214902878 	 ~
0.042215745896101 	 0.04373832419514656 	 ~
0.037378281354904175 	 0.0439322330057621 	 ~
0.041613392531871796 	 0.04396215081214905 	 ~
0.040652863681316376 	 0.043995171785354614 	 ~
0.04323854669928551 	 0.044149480760097504 	 ~
0.037404224276542664 	 0.04420000687241554 	 ~
0.039629608392715454 	 0.04423855245113373 	 ~
0.03830701857805252 	 0.044310007244348526 	 ~
0.045078616589307785 	 0.04435811936855316 	 ~
0.03897442668676376 	 0.04440216347575188 	 ~
0.03843345865607262 	 0.04444177448749542 	 ~
0.041613392531871796 	 0.0444621779024601 	 ~
0.04099088907241821 	 0.04446781426668167 	 ~
0.042302533984184265 	 0.04460151493549347 	 ~
0.039858512580394745 	 0.044625118374824524 	 ~
0.04021863639354706 	 0.04465361684560776 	 ~
0.04307379201054573 	 0.04467156529426575 	 ~
0.04624054208397865 	 0.044705599546432495 	 ~
0.04752969369292259 	 0.04472651705145836 	 ~
0.04021863639354706 	 0.044731564819812775 	 ~
0.04464685544371605 	 0.044809628278017044 	 ~
0.03843345865607262 	 0.04480984807014465 	 ~
0.03777399659156799 	 0.04484467953443527 	 ~
0.04584402218461037 	 0.044878922402858734 	 ~
0.03892704099416733 	 0.04493262246251106 	 ~
0.04783874377608299 	 0.045331694185733795 	 ~
0.03442550078034401 	 0.04534126818180084 	 miss
0.041613392531871796 	 0.045433323830366135 	 ~
0.04634629935026169 	 0.045500747859478 	 ~
0.04018240049481392 	 0.04558679461479187 	 ~
0.03987066075205803 	 0.04559871926903725 	 ~
0.04594597592949867 	 0.04563778638839722 	 ~
0.037378281354904175 	 0.04563980922102928 	 ~
0.041613392531871796 	 0.045711640268564224 	 ~
0.03722849488258362 	 0.04584431275725365 	 ~
0.03840210661292076 	 0.04585835337638855 	 ~
0.04357437416911125 	 0.0459432527422905 	 ~
0.041613392531871796 	 0.04595748707652092 	 ~
0.037253763526678085 	 0.04599134251475334 	 ~
0.041613392531871796 	 0.04613073915243149 	 ~
0.03955076262354851 	 0.04625905677676201 	 ~
0.0433533601462841 	 0.046307772397994995 	 ~
0.04071503505110741 	 0.0463334359228611 	 ~
0.03777399659156799 	 0.046399835497140884 	 ~
0.046226393431425095 	 0.04653491824865341 	 ~
0.041613392531871796 	 0.04657765105366707 	 ~
0.041287198662757874 	 0.04661218449473381 	 ~
0.03442550078034401 	 0.04669781029224396 	 miss
0.04634629935026169 	 0.04688733071088791 	 ~
0.036581914871931076 	 0.047041717916727066 	 miss
0.036851365119218826 	 0.047046590596437454 	 miss
0.04905792325735092 	 0.04706022888422012 	 ~
0.039858512580394745 	 0.04712012782692909 	 ~
0.040652863681316376 	 0.04722273349761963 	 ~
0.03840210661292076 	 0.04751383513212204 	 ~
0.04905792325735092 	 0.047548796981573105 	 ~
0.039853036403656006 	 0.04763629660010338 	 ~
0.03892704099416733 	 0.04787735268473625 	 ~
0.03328105807304382 	 0.04801437631249428 	 miss
0.042302533984184265 	 0.048148009926080704 	 ~
0.04584402218461037 	 0.048194728791713715 	 ~
0.041613392531871796 	 0.04852288216352463 	 ~
0.03709349408745766 	 0.0485723577439785 	 miss
0.039629608392715454 	 0.04862109571695328 	 ~
0.03830701857805252 	 0.04866746813058853 	 miss
0.04752969369292259 	 0.04868145287036896 	 ~
0.041613392531871796 	 0.04870225861668587 	 ~
0.03897442668676376 	 0.04870794340968132 	 ~
0.036588337272405624 	 0.04890349507331848 	 miss
0.04936527833342552 	 0.04906530678272247 	 ~
0.041613392531871796 	 0.04918099567294121 	 ~
0.03776520490646362 	 0.04929475858807564 	 miss
0.04065237194299698 	 0.049358513206243515 	 ~
0.04594597592949867 	 0.04935925081372261 	 ~
0.05147622153162956 	 0.04949241876602173 	 ~
0.03805207088589668 	 0.049534499645233154 	 miss
0.04054858162999153 	 0.04970356076955795 	 ~
0.05106606334447861 	 0.04975679889321327 	 ~
0.04708878695964813 	 0.04981977120041847 	 ~
0.03796575590968132 	 0.049896448850631714 	 miss
0.03823324665427208 	 0.05022237077355385 	 miss
0.036588337272405624 	 0.050232551991939545 	 miss
0.037404224276542664 	 0.0502905435860157 	 miss
0.03987066075205803 	 0.05035090073943138 	 miss
0.04018240049481392 	 0.05038877949118614 	 miss
0.041613392531871796 	 0.05044877901673317 	 ~
0.041613392531871796 	 0.050519462674856186 	 ~
0.041613392531871796 	 0.050527218729257584 	 ~
0.041613392531871796 	 0.050727423280477524 	 ~
0.04065237194299698 	 0.05094859376549721 	 miss
0.045653726905584335 	 0.05095751956105232 	 ~
0.04936527833342552 	 0.050999101251363754 	 ~
0.03822844475507736 	 0.05113581568002701 	 miss
0.03776520490646362 	 0.05132576823234558 	 miss
0.03682028129696846 	 0.05133404582738876 	 miss
0.05106606334447861 	 0.05172743275761604 	 ~
0.03796575590968132 	 0.051804911345243454 	 miss
0.045078616589307785 	 0.05188344046473503 	 ~
0.04307379201054573 	 0.051991309970617294 	 ~
0.0433533601462841 	 0.05199422314763069 	 ~
0.037253763526678085 	 0.05207915976643562 	 miss
0.04473793879151344 	 0.052688054740428925 	 ~
0.03722849488258362 	 0.05299878865480423 	 miss
0.036300621926784515 	 0.053078703582286835 	 miss
0.052661240100860596 	 0.05355646461248398 	 ~
0.036851365119218826 	 0.053712714463472366 	 miss
0.03823324665427208 	 0.05438936874270439 	 miss
0.0395592525601387 	 0.05445541813969612 	 miss
0.05147622153162956 	 0.054819364100694656 	 ~
0.040963198989629745 	 0.05482087284326553 	 miss
0.041613392531871796 	 0.054982949048280716 	 miss
0.040963198989629745 	 0.05530436709523201 	 miss
0.041613392531871796 	 0.05647377669811249 	 miss
0.036581914871931076 	 0.05653548985719681 	 miss
0.04473793879151344 	 0.05728677660226822 	 miss
0.03328105807304382 	 0.0581449493765831 	 miss
0.0395592525601387 	 0.058267075568437576 	 miss
0.0368429496884346 	 0.05837392434477806 	 miss
0.04099088907241821 	 0.058505408465862274 	 miss
0.17085544764995575 	 0.1751450151205063 	 ~
0.17224547266960144 	 0.1754651814699173 	 ~
0.1930244415998459 	 0.1807640939950943 	 miss
0.17224547266960144 	 0.1875997930765152 	 miss
0.16594767570495605 	 0.19106347858905792 	 miss
0.19295810163021088 	 0.19454899430274963 	 ~
0.20183244347572327 	 0.19882306456565857 	 ~
0.2000911831855774 	 0.20550106465816498 	 ~
0.17085544764995575 	 0.20614206790924072 	 miss
0.16594767570495605 	 0.20687584578990936 	 miss
0.20183244347572327 	 0.2089625597000122 	 ~
0.19295810163021088 	 0.20910866558551788 	 miss
0.17922478914260864 	 0.2108914852142334 	 miss
0.2000911831855774 	 0.21102270483970642 	 miss
0.1930244415998459 	 0.2124388962984085 	 miss
0.22914385795593262 	 0.21776919066905975 	 miss
0.19885951280593872 	 0.21802707016468048 	 miss
0.19745777547359467 	 0.2181139886379242 	 miss
0.19745777547359467 	 0.21982504427433014 	 miss
0.22914385795593262 	 0.22231587767601013 	 ~
0.17922478914260864 	 0.22730658948421478 	 miss
0.23190279304981232 	 0.22811566293239594 	 ~
0.2221917361021042 	 0.22991785407066345 	 ~
0.2221917361021042 	 0.23090121150016785 	 ~
0.20352855324745178 	 0.23212459683418274 	 miss
0.19885951280593872 	 0.24050909280776978 	 miss
0.23190279304981232 	 0.24217700958251953 	 miss
0.20352855324745178 	 0.24333946406841278 	 miss
0.20575807988643646 	 0.24515008926391602 	 miss
0.19977504014968872 	 0.25267818570137024 	 miss
0.20132221281528473 	 0.25434398651123047 	 miss
0.20132221281528473 	 0.2615586817264557 	 miss
0.26122868061065674 	 0.262342631816864 	 ~
0.24428099393844604 	 0.26320335268974304 	 miss
0.25832173228263855 	 0.2714383602142334 	 miss
0.24428099393844604 	 0.27378302812576294 	 miss
0.19977504014968872 	 0.27424511313438416 	 miss
0.26122868061065674 	 0.2757151424884796 	 miss
0.20575807988643646 	 0.2810293436050415 	 miss
0.23712775111198425 	 0.29170307517051697 	 miss
0.23712775111198425 	 0.2933429181575775 	 miss
0.25832173228263855 	 0.2978752851486206 	 miss
0.2806227505207062 	 0.30309024453163147 	 miss
0.255839079618454 	 0.3056948184967041 	 miss
0.2468520998954773 	 0.3072573244571686 	 miss
0.255839079618454 	 0.3142557740211487 	 miss
0.2755112946033478 	 0.31747525930404663 	 miss
0.2755112946033478 	 0.3191111087799072 	 miss
0.2806227505207062 	 0.3191469609737396 	 miss
0.2468520998954773 	 0.32158538699150085 	 miss

r_mrr = tensor([[1.0000, 0.9893],
        [0.9893, 1.0000]])
r2_mrr = 0.9545525908470154
~@All = 0.6611570247933884
~@-5 = 31.4
~@-10 = 15.2
~@-25 = 5.48
~@-50 = 2.32
~@-100 = 0.67
average test loss: 0.0032914633187874267
	test time: 1.067
Done Testing dataset UMLS
total time taken: 195.45897006988525
training time taken: 182.01597952842712
