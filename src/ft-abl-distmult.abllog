===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 5219839097124932
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [996, 718, 636, 982, 882, 867, 688, 287, 488, 729, 625, 106, 93, 90, 59, 314, 851, 1067, 880, 544, 1042, 977, 918, 583, 687, 721, 413, 319, 985, 472, 550, 403, 166, 948, 380, 1050, 445, 1059, 375, 848, 15, 150, 777, 364, 1055, 338, 483, 513, 551, 565, 991, 1151, 1025, 9, 960, 846, 611, 1179, 707, 1154, 117, 661, 1148, 760, 507, 68, 241, 874, 98, 35, 919, 705, 836, 758, 670, 466, 1004, 1103, 1170, 1160, 752, 856, 669, 101, 350, 81, 441, 87, 622, 439, 731, 136, 92, 1144, 558, 766, 529, 339, 737, 966, 579, 863, 456, 111, 809, 223, 467, 5, 621, 933, 930, 574, 73, 491, 992, 248, 790, 941, 494, 177, 336]
valid_ids (0): []
train_ids (1094): [684, 686, 1199, 335, 746, 431, 34, 373, 1188, 566, 1077, 50, 1073, 1196, 843, 182, 143, 1048, 635, 564, 653, 440, 516, 1205, 1128, 459, 586, 668, 1121, 104, 842, 22, 601, 202, 963, 590, 797, 807, 976, 1190, 172, 449, 218, 678, 1017, 97, 185, 831, 105, 819, 554, 840, 1031, 58, 176, 402, 51, 1141, 1119, 386, 608, 452, 11, 776, 1189, 257, 340, 1000, 916, 348, 135, 527, 894, 1020, 480, 799, 514, 39, 597, 304, 161, 675, 951, 262, 272, 1052, 1043, 691, 1209, 525, 193, 289, 852, 538, 306, 76, 1172, 895, 627, 572, 32, 454, 288, 147, 1127, 1117, 242, 1081, 376, 773, 878, 291, 194, 141, 595, 944, 757, 1138, 968, 473, 643, 806, 1191, 1165, 363, 553, 292, 365, 213, 765, 645, 710, 748, 99, 871, 1069, 978, 256, 165, 351, 74, 835, 56, 868, 461, 716, 424, 630, 311, 372, 156, 522, 349, 250, 1, 952, 1161, 609, 235, 649, 832, 762, 486, 258, 315, 94, 639, 821, 134, 224, 128, 374, 186, 1192, 692, 210, 66, 95, 619, 666, 602, 1082, 548, 824, 132, 753, 453, 405, 70, 1065, 534, 1036, 169, 866, 942, 1201, 1005, 232, 317, 352, 725, 109, 16, 1168, 754, 151, 1078, 893, 1146, 395, 273, 632, 873, 1093, 460, 530, 723, 612, 929, 493, 158, 1098, 384, 646, 316, 401, 359, 533, 444, 389, 361, 302, 642, 764, 88, 1058, 945, 75, 667, 681, 1212, 1203, 103, 436, 301, 980, 730, 532, 1028, 787, 344, 1032, 568, 665, 537, 1132, 211, 29, 276, 618, 927, 497, 260, 1034, 594, 183, 1094, 249, 531, 973, 327, 499, 604, 734, 33, 333, 330, 167, 610, 903, 475, 305, 704, 854, 261, 415, 450, 334, 383, 1011, 465, 429, 1111, 118, 253, 801, 769, 139, 286, 406, 780, 1198, 898, 1051, 1171, 180, 750, 936, 1030, 323, 872, 294, 956, 1007, 21, 1193, 495, 421, 598, 393, 1169, 100, 1114, 149, 822, 796, 458, 658, 366, 307, 1136, 240, 295, 805, 508, 12, 837, 899, 370, 85, 703, 159, 228, 849, 244, 1027, 209, 673, 251, 217, 237, 616, 733, 1100, 4, 157, 556, 410, 1149, 972, 802, 744, 512, 770, 962, 86, 771, 813, 1023, 119, 798, 122, 222, 382, 360, 607, 971, 783, 540, 362, 647, 577, 1210, 524, 198, 975, 570, 652, 1163, 404, 255, 197, 42, 1064, 275, 1095, 883, 1175, 1142, 447, 502, 408, 476, 407, 64, 909, 923, 1075, 786, 1118, 277, 280, 23, 65, 1038, 430, 252, 814, 55, 953, 656, 25, 782, 367, 221, 448, 215, 912, 1021, 1153, 504, 1070, 726, 371, 346, 369, 815, 308, 743, 506, 907, 641, 888, 714, 416, 20, 397, 188, 693, 113, 500, 477, 542, 1057, 496, 859, 19, 788, 212, 672, 1174, 1143, 219, 736, 71, 318, 283, 584, 1029, 1214, 1099, 997, 478, 14, 205, 1158, 592, 322, 269, 546, 747, 120, 426, 266, 394, 633, 216, 624, 300, 175, 425, 1104, 126, 1182, 1054, 152, 560, 1084, 60, 140, 745, 662, 358, 877, 381, 18, 664, 1040, 605, 541, 234, 1072, 701, 435, 881, 489, 751, 901, 715, 785, 274, 246, 391, 1024, 588, 549, 178, 463, 593, 679, 983, 690, 233, 192, 932, 131, 184, 1003, 321, 67, 481, 950, 284, 1112, 28, 887, 471, 1130, 1106, 123, 839, 412, 293, 1035, 680, 54, 312, 57, 634, 63, 626, 812, 759, 144, 1180, 791, 886, 904, 133, 268, 1197, 41, 428, 1150, 778, 870, 271, 127, 46, 148, 26, 468, 345, 921, 1088, 523, 171, 238, 420, 713, 332, 578, 231, 949, 955, 720, 7, 1008, 79, 45, 379, 855, 1194, 694, 1044, 884, 582, 519, 925, 1120, 700, 937, 173, 897, 580, 1107, 1178, 423, 671, 857, 278, 229, 830, 959, 1026, 44, 1018, 208, 521, 1002, 230, 1076, 697, 187, 1156, 1089, 297, 6, 1206, 536, 575, 826, 201, 1200, 890, 265, 969, 433, 1173, 1183, 1110, 1091, 816, 1097, 329, 1010, 1126, 355, 168, 650, 8, 834, 1009, 543, 1012, 89, 712, 1113, 162, 518, 706, 1157, 27, 663, 1092, 946, 1211, 591, 451, 484, 61, 470, 676, 638, 585, 685, 264, 154, 981, 1083, 236, 803, 1131, 492, 555, 1101, 954, 889, 1213, 357, 644, 629, 49, 995, 1105, 928, 1162, 337, 908, 1116, 853, 943, 935, 779, 1145, 91, 1066, 617, 245, 823, 164, 1176, 227, 207, 1019, 775, 377, 767, 443, 974, 865, 40, 53, 1186, 587, 220, 1063, 838, 225, 37, 72, 1152, 279, 309, 987, 829, 112, 130, 993, 989, 596, 719, 206, 957, 891, 1087, 1096, 1053, 696, 1166, 434, 793, 196, 199, 285, 174, 387, 48, 505, 761, 1139, 38, 742, 702, 267, 398, 1047, 810, 396, 474, 170, 539, 1185, 1060, 727, 979, 299, 17, 469, 399, 1014, 437, 864, 847, 818, 325, 557, 892, 1202, 1123, 1129, 683, 270, 442, 990, 190, 189, 243, 559, 110, 1167, 1071, 845, 695, 958, 254, 709, 390, 820, 576, 931, 947, 571, 1039, 482, 988, 47, 828, 446, 732, 414, 1045, 347, 939, 724, 107, 545, 655, 552, 749, 0, 774, 487, 43, 160, 153, 869, 657, 77, 1125, 800, 682, 637, 226, 1006, 772, 1049, 1208, 1033, 290, 654, 498, 756, 78, 817, 994, 965, 567, 1037, 998, 1085, 281, 784, 457, 698, 3, 515, 858, 940, 80, 1090, 385, 741, 677, 1080, 501, 967, 142, 400, 922, 62, 310, 1124, 137, 792, 353, 204, 1015, 628, 82, 1133, 1016, 422, 526, 69, 569, 1184, 910, 781, 1086, 1147, 689, 1181, 419, 115, 1074, 503, 1109, 620, 31, 509, 1041, 485, 200, 844, 879, 511, 1135, 961, 615, 906, 1056, 924, 561, 614, 535, 1155, 520, 331, 342, 862, 1079, 298, 547, 934, 999, 660, 885, 392, 984, 313, 1022, 181, 841, 432, 794, 827, 455, 84, 1068, 146, 263, 648, 717, 722, 356, 1177, 708, 354, 739, 102, 623, 905, 2, 1102, 36, 599, 603, 613, 259, 417, 247, 83, 24, 850, 914, 214, 464, 239, 490, 970, 341, 1195, 438, 600, 808, 964, 191, 938, 328, 1159, 155, 631, 96, 427, 30, 145, 711, 1134, 573, 114, 728, 1122, 1164, 1187, 303, 1108, 1001, 1207, 825, 876, 108, 163, 699, 674, 179, 562, 203, 13, 735, 917, 10, 795, 915, 462, 1204, 388, 116, 986, 125, 1062, 755, 902, 738, 1046, 138, 740, 811, 900, 195, 563, 804, 913, 1115, 52, 659, 1013, 418, 860, 920, 411, 326, 510, 581, 640, 479, 833, 606, 589, 926, 517, 911, 282, 651, 896, 409, 343, 124, 1140, 875, 378, 861, 763, 789, 1061, 768, 320, 324, 528, 296, 1137, 121, 129, 368]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2531937323629027
the save name prefix for this run is:  chkpt-ID_2531937323629027_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 148
rank avg (pred): 0.467 +- 0.004
mrr vals (pred, true): 0.001, 0.220
batch losses (mrrl, rdl): 0.0, 0.0036387255

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 404
rank avg (pred): 0.262 +- 0.159
mrr vals (pred, true): 0.076, 0.194
batch losses (mrrl, rdl): 0.0, 0.0009789676

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1198
rank avg (pred): 0.316 +- 0.275
mrr vals (pred, true): 0.281, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002972851

Epoch over!
epoch time: 12.386

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.306 +- 0.272
mrr vals (pred, true): 0.292, 0.004
batch losses (mrrl, rdl): 0.0, 0.000324519

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 58
rank avg (pred): 0.043 +- 0.043
mrr vals (pred, true): 0.414, 0.185
batch losses (mrrl, rdl): 0.0, 5.0635e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 898
rank avg (pred): 0.303 +- 0.319
mrr vals (pred, true): 0.389, 0.001
batch losses (mrrl, rdl): 0.0, 0.0022828102

Epoch over!
epoch time: 11.656

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.304 +- 0.309
mrr vals (pred, true): 0.366, 0.218
batch losses (mrrl, rdl): 0.0, 0.0016396773

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1206
rank avg (pred): 0.398 +- 0.357
mrr vals (pred, true): 0.302, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001048731

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 970
rank avg (pred): 0.451 +- 0.374
mrr vals (pred, true): 0.259, 0.004
batch losses (mrrl, rdl): 0.0, 6.85806e-05

Epoch over!
epoch time: 11.85

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 417
rank avg (pred): 0.311 +- 0.295
mrr vals (pred, true): 0.298, 0.004
batch losses (mrrl, rdl): 0.0, 0.0003022132

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 16
rank avg (pred): 0.080 +- 0.091
mrr vals (pred, true): 0.480, 0.252
batch losses (mrrl, rdl): 0.0, 5.97236e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 170
rank avg (pred): 0.250 +- 0.264
mrr vals (pred, true): 0.391, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006693995

Epoch over!
epoch time: 11.703

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 450
rank avg (pred): 0.250 +- 0.268
mrr vals (pred, true): 0.412, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007026399

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.248 +- 0.263
mrr vals (pred, true): 0.397, 0.201
batch losses (mrrl, rdl): 0.0, 0.001060624

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 797
rank avg (pred): 0.329 +- 0.329
mrr vals (pred, true): 0.357, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002745949

Epoch over!
epoch time: 11.799

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1157
rank avg (pred): 0.290 +- 0.301
mrr vals (pred, true): 0.355, 0.049
batch losses (mrrl, rdl): 0.9319678545, 0.0004174599

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1130
rank avg (pred): 0.369 +- 0.256
mrr vals (pred, true): 0.108, 0.004
batch losses (mrrl, rdl): 0.0335742831, 0.0001129329

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 404
rank avg (pred): 0.163 +- 0.126
mrr vals (pred, true): 0.135, 0.194
batch losses (mrrl, rdl): 0.0343473479, 0.000269397

Epoch over!
epoch time: 12.174

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 484
rank avg (pred): 0.195 +- 0.148
mrr vals (pred, true): 0.141, 0.004
batch losses (mrrl, rdl): 0.0826183558, 0.0015267999

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.195 +- 0.150
mrr vals (pred, true): 0.148, 0.005
batch losses (mrrl, rdl): 0.0964905769, 0.0014528811

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 875
rank avg (pred): 0.401 +- 0.235
mrr vals (pred, true): 0.125, 0.004
batch losses (mrrl, rdl): 0.0559294038, 7.60265e-05

Epoch over!
epoch time: 12.139

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 205
rank avg (pred): 0.308 +- 0.216
mrr vals (pred, true): 0.113, 0.003
batch losses (mrrl, rdl): 0.0402444005, 0.0004873644

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1073
rank avg (pred): 0.027 +- 0.021
mrr vals (pred, true): 0.192, 0.222
batch losses (mrrl, rdl): 0.0090238433, 2.36202e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 811
rank avg (pred): 0.025 +- 0.019
mrr vals (pred, true): 0.209, 0.149
batch losses (mrrl, rdl): 0.0362517275, 0.0001426825

Epoch over!
epoch time: 12.131

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 144
rank avg (pred): 0.371 +- 0.232
mrr vals (pred, true): 0.100, 0.232
batch losses (mrrl, rdl): 0.1735980958, 0.0023365386

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 320
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.277, 0.285
batch losses (mrrl, rdl): 0.0007264639, 2.80033e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 866
rank avg (pred): 0.403 +- 0.201
mrr vals (pred, true): 0.083, 0.004
batch losses (mrrl, rdl): 0.0108165527, 9.26929e-05

Epoch over!
epoch time: 12.082

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 865
rank avg (pred): 0.393 +- 0.204
mrr vals (pred, true): 0.098, 0.004
batch losses (mrrl, rdl): 0.0232168753, 0.0001281704

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 284
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.242, 0.277
batch losses (mrrl, rdl): 0.0117976852, 2.08742e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1032
rank avg (pred): 0.386 +- 0.206
mrr vals (pred, true): 0.102, 0.005
batch losses (mrrl, rdl): 0.0271576867, 0.0001182822

Epoch over!
epoch time: 12.114

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 806
rank avg (pred): 0.393 +- 0.197
mrr vals (pred, true): 0.094, 0.004
batch losses (mrrl, rdl): 0.01919448, 0.0001159196

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.442 +- 0.181
mrr vals (pred, true): 0.069, 0.075
batch losses (mrrl, rdl): 0.0036430752, 0.000471852

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 268
rank avg (pred): 0.013 +- 0.015
mrr vals (pred, true): 0.281, 0.210
batch losses (mrrl, rdl): 0.0503406525, 2.74977e-05

Epoch over!
epoch time: 12.063

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 178
rank avg (pred): 0.258 +- 0.199
mrr vals (pred, true): 0.153, 0.005
batch losses (mrrl, rdl): 0.1056306064, 0.0007624458

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 843
rank avg (pred): 0.363 +- 0.187
mrr vals (pred, true): 0.101, 0.033
batch losses (mrrl, rdl): 0.0256760772, 0.0001046655

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 660
rank avg (pred): 0.392 +- 0.166
mrr vals (pred, true): 0.069, 0.003
batch losses (mrrl, rdl): 0.003640112, 0.0001689608

Epoch over!
epoch time: 11.849

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.127 +- 0.145
mrr vals (pred, true): 0.216, 0.058
batch losses (mrrl, rdl): 0.2747860253, 4.08582e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.294 +- 0.195
mrr vals (pred, true): 0.132, 0.005
batch losses (mrrl, rdl): 0.0673050508, 0.0006332232

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.300 +- 0.184
mrr vals (pred, true): 0.122, 0.228
batch losses (mrrl, rdl): 0.1122976169, 0.0014066246

Epoch over!
epoch time: 11.864

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 485
rank avg (pred): 0.171 +- 0.120
mrr vals (pred, true): 0.162, 0.005
batch losses (mrrl, rdl): 0.1251995713, 0.0018895528

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.061 +- 0.060
mrr vals (pred, true): 0.251, 0.259
batch losses (mrrl, rdl): 0.000712173, 7.59e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1187
rank avg (pred): 0.400 +- 0.167
mrr vals (pred, true): 0.067, 0.151
batch losses (mrrl, rdl): 0.0712232217, 0.0011059856

Epoch over!
epoch time: 11.959

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 186
rank avg (pred): 0.300 +- 0.179
mrr vals (pred, true): 0.107, 0.003
batch losses (mrrl, rdl): 0.0320927911, 0.0006235274

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 329
rank avg (pred): 0.304 +- 0.185
mrr vals (pred, true): 0.120, 0.179
batch losses (mrrl, rdl): 0.0350554287, 0.0013540235

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 769
rank avg (pred): 0.296 +- 0.193
mrr vals (pred, true): 0.105, 0.043
batch losses (mrrl, rdl): 0.030733794, 0.0005203056

Epoch over!
epoch time: 11.86

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.082 +- 0.062
mrr vals (pred, true): 0.216, 0.291

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.07202 	 0.00073 	 m..s
    6 	     1 	 0.07119 	 0.00074 	 m..s
    9 	     2 	 0.07276 	 0.00076 	 m..s
  104 	     3 	 0.18679 	 0.00104 	 MISS
    0 	     4 	 0.06508 	 0.00249 	 m..s
   27 	     5 	 0.09026 	 0.00277 	 m..s
    1 	     6 	 0.06528 	 0.00284 	 m..s
   76 	     7 	 0.12274 	 0.00310 	 MISS
   91 	     8 	 0.13832 	 0.00342 	 MISS
   69 	     9 	 0.11674 	 0.00343 	 MISS
   85 	    10 	 0.13534 	 0.00344 	 MISS
   26 	    11 	 0.08969 	 0.00345 	 m..s
   86 	    12 	 0.13568 	 0.00347 	 MISS
    5 	    13 	 0.07115 	 0.00348 	 m..s
   68 	    14 	 0.11589 	 0.00350 	 MISS
   21 	    15 	 0.08505 	 0.00353 	 m..s
   92 	    16 	 0.14800 	 0.00354 	 MISS
   82 	    17 	 0.13198 	 0.00359 	 MISS
   43 	    18 	 0.09821 	 0.00371 	 m..s
   38 	    19 	 0.09719 	 0.00385 	 m..s
   18 	    20 	 0.08299 	 0.00390 	 m..s
   46 	    21 	 0.09888 	 0.00394 	 m..s
   37 	    22 	 0.09701 	 0.00400 	 m..s
    2 	    23 	 0.06532 	 0.00403 	 m..s
   46 	    24 	 0.09888 	 0.00407 	 m..s
   89 	    25 	 0.13765 	 0.00411 	 MISS
   79 	    26 	 0.12798 	 0.00417 	 MISS
   34 	    27 	 0.09653 	 0.00421 	 m..s
   19 	    28 	 0.08317 	 0.00437 	 m..s
   35 	    29 	 0.09671 	 0.00438 	 m..s
   32 	    30 	 0.09218 	 0.00451 	 m..s
   87 	    31 	 0.13600 	 0.00457 	 MISS
   94 	    32 	 0.14870 	 0.00463 	 MISS
   62 	    33 	 0.10834 	 0.00467 	 MISS
   14 	    34 	 0.08031 	 0.00483 	 m..s
    8 	    35 	 0.07267 	 0.00489 	 m..s
   44 	    36 	 0.09832 	 0.00498 	 m..s
   22 	    37 	 0.08656 	 0.00528 	 m..s
   60 	    38 	 0.10736 	 0.00532 	 MISS
   46 	    39 	 0.09888 	 0.00559 	 m..s
   24 	    40 	 0.08725 	 0.02123 	 m..s
   33 	    41 	 0.09449 	 0.02871 	 m..s
   54 	    42 	 0.09929 	 0.03628 	 m..s
   65 	    43 	 0.11084 	 0.03688 	 m..s
   53 	    44 	 0.09924 	 0.03829 	 m..s
   55 	    45 	 0.09960 	 0.04165 	 m..s
   50 	    46 	 0.09895 	 0.04300 	 m..s
   51 	    47 	 0.09907 	 0.04484 	 m..s
   29 	    48 	 0.09061 	 0.04576 	 m..s
   52 	    49 	 0.09920 	 0.04740 	 m..s
   28 	    50 	 0.09059 	 0.04999 	 m..s
   15 	    51 	 0.08149 	 0.05249 	 ~...
   16 	    52 	 0.08153 	 0.05279 	 ~...
   39 	    53 	 0.09746 	 0.05678 	 m..s
   41 	    54 	 0.09783 	 0.05706 	 m..s
   36 	    55 	 0.09673 	 0.05797 	 m..s
   11 	    56 	 0.07944 	 0.05810 	 ~...
   12 	    57 	 0.07980 	 0.05918 	 ~...
   13 	    58 	 0.08001 	 0.06077 	 ~...
   25 	    59 	 0.08939 	 0.07497 	 ~...
   30 	    60 	 0.09066 	 0.10585 	 ~...
   98 	    61 	 0.17526 	 0.10812 	 m..s
   17 	    62 	 0.08292 	 0.12552 	 m..s
   97 	    63 	 0.17426 	 0.12614 	 m..s
   31 	    64 	 0.09104 	 0.12878 	 m..s
   20 	    65 	 0.08368 	 0.13780 	 m..s
    4 	    66 	 0.07092 	 0.14181 	 m..s
    3 	    67 	 0.07086 	 0.14931 	 m..s
   40 	    68 	 0.09757 	 0.14977 	 m..s
   10 	    69 	 0.07912 	 0.15121 	 m..s
   66 	    70 	 0.11247 	 0.15737 	 m..s
  106 	    71 	 0.19010 	 0.15748 	 m..s
   42 	    72 	 0.09790 	 0.15954 	 m..s
   81 	    73 	 0.13038 	 0.16374 	 m..s
   84 	    74 	 0.13352 	 0.16551 	 m..s
   83 	    75 	 0.13273 	 0.16622 	 m..s
   46 	    76 	 0.09888 	 0.16654 	 m..s
   80 	    77 	 0.12806 	 0.17776 	 m..s
   77 	    78 	 0.12418 	 0.17849 	 m..s
   73 	    79 	 0.11947 	 0.17922 	 m..s
   74 	    80 	 0.12099 	 0.18342 	 m..s
   88 	    81 	 0.13664 	 0.18495 	 m..s
  107 	    82 	 0.20313 	 0.19153 	 ~...
  108 	    83 	 0.20649 	 0.19248 	 ~...
   23 	    84 	 0.08704 	 0.19394 	 MISS
   75 	    85 	 0.12122 	 0.19576 	 m..s
  101 	    86 	 0.18531 	 0.19589 	 ~...
   56 	    87 	 0.10011 	 0.19766 	 m..s
  102 	    88 	 0.18535 	 0.20587 	 ~...
   61 	    89 	 0.10751 	 0.20811 	 MISS
  103 	    90 	 0.18591 	 0.20837 	 ~...
  109 	    91 	 0.21041 	 0.21020 	 ~...
   59 	    92 	 0.10685 	 0.21117 	 MISS
   71 	    93 	 0.11759 	 0.21238 	 m..s
   78 	    94 	 0.12586 	 0.21270 	 m..s
   67 	    95 	 0.11477 	 0.21748 	 MISS
   95 	    96 	 0.14916 	 0.21920 	 m..s
   45 	    97 	 0.09883 	 0.21969 	 MISS
  116 	    98 	 0.23311 	 0.21991 	 ~...
  110 	    99 	 0.21078 	 0.22001 	 ~...
   90 	   100 	 0.13826 	 0.22485 	 m..s
   70 	   101 	 0.11693 	 0.22550 	 MISS
   72 	   102 	 0.11922 	 0.22649 	 MISS
   64 	   103 	 0.10887 	 0.22727 	 MISS
   58 	   104 	 0.10305 	 0.22931 	 MISS
   63 	   105 	 0.10871 	 0.22971 	 MISS
   57 	   106 	 0.10260 	 0.23224 	 MISS
  119 	   107 	 0.24608 	 0.23662 	 ~...
  113 	   108 	 0.21670 	 0.23733 	 ~...
   93 	   109 	 0.14863 	 0.24461 	 m..s
  115 	   110 	 0.22077 	 0.24796 	 ~...
   96 	   111 	 0.16551 	 0.25028 	 m..s
  105 	   112 	 0.19006 	 0.25578 	 m..s
  112 	   113 	 0.21638 	 0.25648 	 m..s
  114 	   114 	 0.21774 	 0.25867 	 m..s
  118 	   115 	 0.24255 	 0.26103 	 ~...
  117 	   116 	 0.24206 	 0.27652 	 m..s
  120 	   117 	 0.26811 	 0.28464 	 ~...
  111 	   118 	 0.21629 	 0.29138 	 m..s
   99 	   119 	 0.17630 	 0.30466 	 MISS
  100 	   120 	 0.17938 	 0.31564 	 MISS
==========================================
r_mrr = 0.6139053106307983
r2_mrr = 0.34249866008758545
spearmanr_mrr@5 = 0.9168029427528381
spearmanr_mrr@10 = 0.9774066209793091
spearmanr_mrr@50 = 0.9668077230453491
spearmanr_mrr@100 = 0.8873095512390137
spearmanr_mrr@All = 0.9118936657905579
==========================================
test time: 0.404
Done Testing dataset CoDExSmall
total time taken: 190.79771733283997
training time taken: 180.14379596710205
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6139)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.3425)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9168)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9774)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9668)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8873)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9119)}}, 'test_loss': {'DistMult': {'CoDExSmall': 5.0706274441854475}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 508425322870164
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 1122, 973, 460, 838, 863, 909, 21, 504, 464, 89, 865, 842, 266, 1184, 609, 899, 1110, 285, 455, 921, 748, 52, 550, 604, 328, 224, 633, 193, 763, 299, 990, 969, 1019, 1138, 272, 1171, 110, 497, 1146, 42, 696, 1046, 46, 645, 56, 300, 941, 77, 76, 954, 124, 6, 232, 32, 848, 326, 1153, 823, 364, 815, 225, 600, 576, 182, 914, 1173, 797, 1035, 854, 1113, 729, 454, 11, 427, 394, 896, 1126, 939, 140, 857, 416, 866, 214, 817, 279, 1069, 593, 543, 937, 793, 762, 1038, 432, 1060, 888, 711, 362, 818, 956, 1, 469, 142, 625, 377, 578, 506, 329, 839, 895, 929, 556, 101, 1005, 1196, 352, 260, 970, 1116, 322, 55]
valid_ids (0): []
train_ids (1094): [388, 230, 437, 675, 201, 383, 579, 862, 400, 280, 1054, 35, 10, 978, 1157, 652, 1158, 655, 268, 1088, 286, 1037, 367, 639, 318, 674, 515, 241, 1057, 591, 778, 940, 636, 1130, 713, 136, 715, 475, 619, 67, 430, 531, 8, 209, 632, 130, 786, 15, 755, 692, 319, 252, 664, 611, 424, 47, 943, 583, 827, 462, 1174, 294, 491, 1109, 363, 1197, 841, 1103, 790, 338, 1034, 78, 907, 1125, 386, 648, 912, 988, 852, 289, 29, 812, 45, 40, 856, 379, 723, 757, 777, 119, 301, 754, 568, 679, 700, 274, 1190, 993, 1089, 672, 980, 1042, 606, 544, 629, 349, 710, 314, 706, 760, 867, 221, 1136, 66, 61, 396, 1043, 667, 530, 484, 994, 1133, 596, 638, 135, 282, 313, 802, 642, 792, 307, 139, 1049, 137, 1008, 803, 335, 305, 1094, 433, 850, 1092, 602, 936, 378, 178, 492, 255, 1102, 534, 389, 87, 80, 60, 1191, 1093, 1023, 324, 779, 933, 100, 890, 112, 1058, 86, 871, 858, 958, 787, 191, 785, 949, 1017, 740, 668, 566, 85, 1121, 177, 357, 218, 238, 269, 479, 93, 687, 630, 561, 676, 744, 143, 846, 1118, 1135, 722, 292, 864, 188, 97, 892, 1072, 885, 25, 567, 102, 799, 820, 613, 637, 935, 1032, 254, 64, 964, 179, 747, 104, 195, 938, 242, 308, 1131, 944, 624, 985, 31, 81, 62, 804, 49, 735, 989, 107, 1096, 623, 580, 123, 371, 98, 798, 53, 183, 155, 312, 925, 1020, 945, 159, 283, 204, 75, 582, 1140, 766, 16, 950, 444, 1183, 196, 947, 855, 502, 1181, 805, 1214, 540, 281, 1167, 880, 331, 1067, 213, 1175, 273, 733, 824, 96, 3, 800, 791, 458, 1178, 343, 987, 374, 553, 983, 592, 247, 789, 971, 658, 1134, 441, 836, 1112, 1064, 886, 277, 918, 1154, 756, 222, 457, 346, 927, 720, 51, 170, 708, 320, 494, 468, 446, 495, 256, 486, 205, 752, 295, 1099, 878, 659, 702, 961, 761, 897, 948, 923, 1015, 117, 459, 765, 995, 354, 511, 1114, 194, 879, 290, 536, 407, 519, 1198, 192, 0, 125, 1106, 82, 37, 1039, 784, 471, 1193, 1009, 1036, 197, 1212, 529, 133, 1048, 439, 569, 651, 169, 877, 443, 122, 1194, 448, 239, 1172, 926, 38, 922, 1045, 816, 180, 1151, 1074, 1016, 649, 1024, 563, 910, 185, 425, 1101, 50, 677, 783, 128, 772, 418, 919, 1162, 693, 889, 1195, 808, 234, 873, 617, 120, 605, 369, 1176, 208, 1021, 429, 111, 271, 1168, 54, 361, 391, 891, 574, 333, 2, 828, 210, 1003, 344, 1029, 717, 549, 105, 245, 175, 533, 1161, 1170, 421, 240, 1090, 203, 265, 1111, 28, 481, 216, 1031, 138, 882, 874, 43, 1203, 1200, 17, 951, 588, 287, 339, 1041, 202, 235, 554, 1079, 164, 1145, 1210, 671, 466, 653, 745, 814, 1075, 310, 643, 1115, 1071, 911, 1185, 36, 199, 276, 641, 884, 870, 1001, 1107, 275, 351, 48, 616, 689, 33, 631, 325, 734, 622, 463, 115, 152, 306, 1091, 607, 837, 1123, 678, 782, 695, 423, 894, 682, 72, 90, 413, 1025, 825, 581, 560, 957, 1026, 844, 603, 1018, 399, 144, 974, 417, 1211, 171, 380, 258, 392, 725, 296, 703, 132, 24, 20, 627, 924, 1143, 573, 709, 381, 541, 806, 612, 408, 522, 771, 411, 490, 598, 634, 79, 893, 472, 1076, 236, 930, 1055, 1155, 160, 39, 226, 750, 847, 705, 1084, 434, 215, 341, 151, 595, 207, 1204, 116, 71, 304, 385, 233, 996, 704, 154, 906, 517, 438, 1177, 661, 356, 44, 728, 801, 770, 336, 1053, 575, 251, 382, 1105, 330, 1108, 982, 565, 913, 499, 449, 965, 521, 410, 1040, 902, 683, 1068, 545, 719, 572, 716, 810, 347, 1059, 1147, 172, 1010, 998, 262, 74, 966, 30, 1186, 309, 348, 167, 1202, 58, 376, 1061, 881, 165, 1165, 1002, 190, 920, 610, 718, 426, 398, 350, 153, 714, 302, 253, 900, 738, 1169, 422, 366, 131, 397, 781, 1033, 483, 477, 916, 688, 932, 586, 451, 764, 1201, 181, 860, 654, 480, 570, 503, 470, 849, 908, 1156, 833, 872, 528, 141, 1166, 963, 1137, 650, 539, 150, 547, 1150, 405, 1132, 999, 1187, 915, 1189, 514, 493, 1100, 984, 876, 246, 1117, 327, 70, 795, 577, 1004, 200, 7, 1087, 168, 261, 1022, 509, 1163, 1073, 690, 507, 134, 946, 12, 428, 524, 1097, 84, 206, 1207, 826, 903, 635, 473, 186, 518, 928, 173, 489, 1065, 13, 1160, 597, 628, 1027, 843, 157, 217, 1119, 419, 680, 291, 359, 584, 146, 587, 420, 952, 219, 9, 440, 527, 263, 1047, 1179, 1028, 303, 409, 1006, 450, 546, 829, 402, 721, 739, 775, 1014, 780, 1013, 231, 108, 1164, 293, 774, 482, 751, 1124, 220, 997, 284, 1213, 726, 942, 736, 384, 851, 737, 288, 360, 496, 753, 727, 807, 663, 788, 614, 227, 1192, 129, 883, 1141, 859, 387, 340, 749, 564, 662, 979, 834, 986, 353, 732, 1208, 478, 505, 187, 542, 311, 510, 68, 250, 59, 229, 730, 620, 212, 594, 315, 666, 644, 435, 237, 759, 372, 665, 548, 373, 794, 538, 65, 887, 41, 685, 647, 1030, 796, 1083, 121, 724, 488, 1188, 223, 114, 768, 370, 968, 1159, 508, 1152, 467, 975, 345, 163, 532, 853, 1062, 249, 758, 525, 162, 189, 684, 537, 321, 278, 904, 447, 332, 1086, 453, 355, 1056, 57, 557, 149, 337, 63, 585, 699, 516, 198, 211, 821, 742, 1104, 822, 590, 94, 1149, 166, 599, 819, 656, 618, 415, 741, 694, 589, 615, 1205, 646, 452, 868, 145, 1120, 512, 243, 1066, 669, 840, 19, 1199, 1052, 126, 1011, 316, 811, 869, 861, 358, 158, 555, 4, 334, 1078, 601, 395, 967, 776, 27, 686, 393, 640, 767, 461, 88, 436, 526, 501, 270, 500, 1128, 69, 513, 1098, 977, 1209, 562, 298, 1129, 1095, 401, 1070, 498, 1182, 342, 1080, 962, 73, 773, 670, 1012, 1139, 832, 485, 26, 976, 571, 259, 1082, 14, 551, 520, 113, 559, 106, 960, 184, 552, 845, 118, 442, 34, 691, 487, 905, 412, 174, 109, 1144, 813, 608, 707, 712, 22, 898, 465, 390, 673, 558, 746, 1081, 375, 323, 92, 267, 953, 23, 414, 228, 95, 5, 406, 698, 981, 476, 147, 403, 626, 1180, 835, 1148, 992, 18, 264, 1077, 445, 1127, 1007, 148, 523, 959, 769, 955, 1044, 809, 535, 176, 1206, 934, 456, 697, 127, 660, 156, 1000, 431, 297, 1050, 991, 875, 99, 731, 621, 83, 161, 244, 404, 317, 103, 681, 701, 368, 474, 931, 91, 365, 917, 830, 1063, 972, 901, 743, 1051, 1085, 257, 1142, 657, 248]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4810767613836557
the save name prefix for this run is:  chkpt-ID_4810767613836557_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.543 +- 0.002
mrr vals (pred, true): 0.001, 0.040
batch losses (mrrl, rdl): 0.0, 0.0021516604

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1106
rank avg (pred): 0.341 +- 0.224
mrr vals (pred, true): 0.055, 0.211
batch losses (mrrl, rdl): 0.0, 0.0018804919

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 720
rank avg (pred): 0.353 +- 0.291
mrr vals (pred, true): 0.120, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001551367

Epoch over!
epoch time: 12.041

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 837
rank avg (pred): 0.429 +- 0.338
mrr vals (pred, true): 0.137, 0.004
batch losses (mrrl, rdl): 0.0, 3.98666e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.247 +- 0.263
mrr vals (pred, true): 0.209, 0.005
batch losses (mrrl, rdl): 0.0, 0.0007617074

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.268 +- 0.297
mrr vals (pred, true): 0.278, 0.202
batch losses (mrrl, rdl): 0.0, 0.0011716555

Epoch over!
epoch time: 11.801

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.235 +- 0.263
mrr vals (pred, true): 0.277, 0.241
batch losses (mrrl, rdl): 0.0, 0.0009775078

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 305
rank avg (pred): 0.081 +- 0.104
mrr vals (pred, true): 0.391, 0.221
batch losses (mrrl, rdl): 0.0, 2.24028e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 631
rank avg (pred): 0.230 +- 0.283
mrr vals (pred, true): 0.359, 0.164
batch losses (mrrl, rdl): 0.0, 0.0001853101

Epoch over!
epoch time: 11.928

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 125
rank avg (pred): 0.298 +- 0.303
mrr vals (pred, true): 0.222, 0.194
batch losses (mrrl, rdl): 0.0, 0.0014999642

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.365 +- 0.338
mrr vals (pred, true): 0.252, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001168355

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 524
rank avg (pred): 0.064 +- 0.084
mrr vals (pred, true): 0.482, 0.048
batch losses (mrrl, rdl): 0.0, 0.0005944041

Epoch over!
epoch time: 11.898

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 891
rank avg (pred): 0.252 +- 0.294
mrr vals (pred, true): 0.352, 0.001
batch losses (mrrl, rdl): 0.0, 0.0011555002

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.243 +- 0.290
mrr vals (pred, true): 0.366, 0.226
batch losses (mrrl, rdl): 0.0, 0.0010214121

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.284 +- 0.297
mrr vals (pred, true): 0.279, 0.115
batch losses (mrrl, rdl): 0.0, 9.61945e-05

Epoch over!
epoch time: 11.929

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.091 +- 0.117
mrr vals (pred, true): 0.421, 0.275
batch losses (mrrl, rdl): 0.2109719664, 6.73773e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 889
rank avg (pred): 0.475 +- 0.188
mrr vals (pred, true): 0.070, 0.005
batch losses (mrrl, rdl): 0.0041535143, 3.64311e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 928
rank avg (pred): 0.477 +- 0.161
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0011969298, 0.0016125367

Epoch over!
epoch time: 12.277

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.255 +- 0.174
mrr vals (pred, true): 0.168, 0.186
batch losses (mrrl, rdl): 0.0030526242, 0.0009719858

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.417 +- 0.180
mrr vals (pred, true): 0.097, 0.174
batch losses (mrrl, rdl): 0.0595999472, 0.0024814217

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.261 +- 0.177
mrr vals (pred, true): 0.180, 0.047
batch losses (mrrl, rdl): 0.1693373024, 5.69557e-05

Epoch over!
epoch time: 12.036

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.375 +- 0.189
mrr vals (pred, true): 0.136, 0.004
batch losses (mrrl, rdl): 0.0733073875, 0.0001828822

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 856
rank avg (pred): 0.497 +- 0.187
mrr vals (pred, true): 0.068, 0.057
batch losses (mrrl, rdl): 0.0030989887, 0.0018090234

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.513 +- 0.162
mrr vals (pred, true): 0.033, 0.004
batch losses (mrrl, rdl): 0.0030102571, 6.39595e-05

Epoch over!
epoch time: 12.061

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.253 +- 0.164
mrr vals (pred, true): 0.162, 0.045
batch losses (mrrl, rdl): 0.1253955513, 5.07789e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.353 +- 0.160
mrr vals (pred, true): 0.120, 0.001
batch losses (mrrl, rdl): 0.048407279, 0.0013791085

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.352 +- 0.147
mrr vals (pred, true): 0.100, 0.004
batch losses (mrrl, rdl): 0.0254432596, 0.0002875455

Epoch over!
epoch time: 12.079

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 647
rank avg (pred): 0.334 +- 0.157
mrr vals (pred, true): 0.131, 0.170
batch losses (mrrl, rdl): 0.0147726499, 0.0006797582

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 97
rank avg (pred): 0.332 +- 0.149
mrr vals (pred, true): 0.120, 0.201
batch losses (mrrl, rdl): 0.0657262802, 0.0017312876

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.336 +- 0.135
mrr vals (pred, true): 0.094, 0.005
batch losses (mrrl, rdl): 0.0194339119, 0.0004705531

Epoch over!
epoch time: 12.04

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.307 +- 0.149
mrr vals (pred, true): 0.142, 0.004
batch losses (mrrl, rdl): 0.0844370723, 0.0005643269

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 401
rank avg (pred): 0.321 +- 0.137
mrr vals (pred, true): 0.105, 0.207
batch losses (mrrl, rdl): 0.1047328934, 0.0015804197

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.445 +- 0.198
mrr vals (pred, true): 0.070, 0.004
batch losses (mrrl, rdl): 0.0039527956, 4.93554e-05

Epoch over!
epoch time: 12.229

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.298 +- 0.141
mrr vals (pred, true): 0.127, 0.128
batch losses (mrrl, rdl): 7.3375e-06, 0.0013828333

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1097
rank avg (pred): 0.301 +- 0.137
mrr vals (pred, true): 0.126, 0.216
batch losses (mrrl, rdl): 0.0804825798, 0.0013967985

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 475
rank avg (pred): 0.299 +- 0.133
mrr vals (pred, true): 0.116, 0.004
batch losses (mrrl, rdl): 0.0436426103, 0.000635238

Epoch over!
epoch time: 12.206

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 60
rank avg (pred): 0.174 +- 0.122
mrr vals (pred, true): 0.174, 0.186
batch losses (mrrl, rdl): 0.0014500117, 0.0003435384

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.314 +- 0.125
mrr vals (pred, true): 0.094, 0.160
batch losses (mrrl, rdl): 0.0424389243, 0.0004877123

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.294 +- 0.123
mrr vals (pred, true): 0.113, 0.006
batch losses (mrrl, rdl): 0.0403182432, 0.0006645081

Epoch over!
epoch time: 11.921

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.290 +- 0.133
mrr vals (pred, true): 0.131, 0.004
batch losses (mrrl, rdl): 0.0662014037, 0.0007439266

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.140 +- 0.094
mrr vals (pred, true): 0.177, 0.057
batch losses (mrrl, rdl): 0.160370186, 9.3059e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1177
rank avg (pred): 0.289 +- 0.141
mrr vals (pred, true): 0.151, 0.152
batch losses (mrrl, rdl): 9.0017e-06, 0.0003803169

Epoch over!
epoch time: 12.212

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1001
rank avg (pred): 0.309 +- 0.123
mrr vals (pred, true): 0.099, 0.189
batch losses (mrrl, rdl): 0.0816674083, 0.0013634858

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 639
rank avg (pred): 0.296 +- 0.131
mrr vals (pred, true): 0.116, 0.135
batch losses (mrrl, rdl): 0.0034895488, 0.0003434137

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 100
rank avg (pred): 0.292 +- 0.126
mrr vals (pred, true): 0.116, 0.201
batch losses (mrrl, rdl): 0.0725693926, 0.0013008942

Epoch over!
epoch time: 11.978

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.279 +- 0.130
mrr vals (pred, true): 0.140, 0.238

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   20 	     0 	 0.06236 	 0.00073 	 m..s
    3 	     1 	 0.04230 	 0.00073 	 m..s
   11 	     2 	 0.06173 	 0.00074 	 m..s
   12 	     3 	 0.06174 	 0.00075 	 m..s
   16 	     4 	 0.06207 	 0.00075 	 m..s
   85 	     5 	 0.14183 	 0.00113 	 MISS
   29 	     6 	 0.11157 	 0.00130 	 MISS
   80 	     7 	 0.13941 	 0.00214 	 MISS
   33 	     8 	 0.11274 	 0.00219 	 MISS
    9 	     9 	 0.06118 	 0.00324 	 m..s
    4 	    10 	 0.04275 	 0.00344 	 m..s
    8 	    11 	 0.06103 	 0.00351 	 m..s
   28 	    12 	 0.11122 	 0.00354 	 MISS
   61 	    13 	 0.12862 	 0.00356 	 MISS
   62 	    14 	 0.13256 	 0.00357 	 MISS
   39 	    15 	 0.11830 	 0.00364 	 MISS
   78 	    16 	 0.13924 	 0.00369 	 MISS
    0 	    17 	 0.04150 	 0.00380 	 m..s
   54 	    18 	 0.12409 	 0.00388 	 MISS
   13 	    19 	 0.06177 	 0.00391 	 m..s
   41 	    20 	 0.11854 	 0.00395 	 MISS
   44 	    21 	 0.11894 	 0.00395 	 MISS
   50 	    22 	 0.12002 	 0.00395 	 MISS
   14 	    23 	 0.06204 	 0.00399 	 m..s
   10 	    24 	 0.06163 	 0.00399 	 m..s
   42 	    25 	 0.11862 	 0.00399 	 MISS
   70 	    26 	 0.13447 	 0.00402 	 MISS
   66 	    27 	 0.13359 	 0.00409 	 MISS
   77 	    28 	 0.13811 	 0.00412 	 MISS
   59 	    29 	 0.12609 	 0.00412 	 MISS
   87 	    30 	 0.14283 	 0.00420 	 MISS
   55 	    31 	 0.12413 	 0.00421 	 MISS
   18 	    32 	 0.06219 	 0.00422 	 m..s
   78 	    33 	 0.13924 	 0.00422 	 MISS
   53 	    34 	 0.12293 	 0.00423 	 MISS
   39 	    35 	 0.11830 	 0.00426 	 MISS
    1 	    36 	 0.04198 	 0.00427 	 m..s
   64 	    37 	 0.13283 	 0.00429 	 MISS
   30 	    38 	 0.11162 	 0.00432 	 MISS
   14 	    39 	 0.06204 	 0.00442 	 m..s
   73 	    40 	 0.13668 	 0.00458 	 MISS
   23 	    41 	 0.10596 	 0.00464 	 MISS
   75 	    42 	 0.13808 	 0.00483 	 MISS
   35 	    43 	 0.11318 	 0.00493 	 MISS
    2 	    44 	 0.04223 	 0.00495 	 m..s
    5 	    45 	 0.04295 	 0.00547 	 m..s
   45 	    46 	 0.11922 	 0.00576 	 MISS
    7 	    47 	 0.04378 	 0.01792 	 ~...
    6 	    48 	 0.04356 	 0.01819 	 ~...
   45 	    49 	 0.11922 	 0.03246 	 m..s
   38 	    50 	 0.11777 	 0.03256 	 m..s
   22 	    51 	 0.06315 	 0.03688 	 ~...
   21 	    52 	 0.06301 	 0.03789 	 ~...
   88 	    53 	 0.18344 	 0.03829 	 MISS
   88 	    54 	 0.18344 	 0.04495 	 MISS
   88 	    55 	 0.18344 	 0.04945 	 MISS
  101 	    56 	 0.19046 	 0.05178 	 MISS
  104 	    57 	 0.19280 	 0.05290 	 MISS
   16 	    58 	 0.06207 	 0.05706 	 ~...
   33 	    59 	 0.11274 	 0.05801 	 m..s
   19 	    60 	 0.06220 	 0.07491 	 ~...
   36 	    61 	 0.11528 	 0.10502 	 ~...
   25 	    62 	 0.11025 	 0.10812 	 ~...
   74 	    63 	 0.13721 	 0.12426 	 ~...
   81 	    64 	 0.13980 	 0.12907 	 ~...
   76 	    65 	 0.13809 	 0.12924 	 ~...
   86 	    66 	 0.14272 	 0.13371 	 ~...
   49 	    67 	 0.11968 	 0.13473 	 ~...
   31 	    68 	 0.11186 	 0.13633 	 ~...
   52 	    69 	 0.12146 	 0.13839 	 ~...
  114 	    70 	 0.23888 	 0.14257 	 m..s
  119 	    71 	 0.25431 	 0.14898 	 MISS
   43 	    72 	 0.11871 	 0.15013 	 m..s
   48 	    73 	 0.11965 	 0.15039 	 m..s
   68 	    74 	 0.13415 	 0.15954 	 ~...
  116 	    75 	 0.24087 	 0.16097 	 m..s
  110 	    76 	 0.21754 	 0.16649 	 m..s
   83 	    77 	 0.13990 	 0.16862 	 ~...
   72 	    78 	 0.13522 	 0.16895 	 m..s
   27 	    79 	 0.11053 	 0.17763 	 m..s
   47 	    80 	 0.11937 	 0.17776 	 m..s
   88 	    81 	 0.18344 	 0.17869 	 ~...
   26 	    82 	 0.11027 	 0.17926 	 m..s
   58 	    83 	 0.12599 	 0.18155 	 m..s
   24 	    84 	 0.10982 	 0.18752 	 m..s
   56 	    85 	 0.12503 	 0.18776 	 m..s
   37 	    86 	 0.11597 	 0.18798 	 m..s
   88 	    87 	 0.18344 	 0.19366 	 ~...
   88 	    88 	 0.18344 	 0.19452 	 ~...
   51 	    89 	 0.12111 	 0.19771 	 m..s
   57 	    90 	 0.12520 	 0.19780 	 m..s
   60 	    91 	 0.12824 	 0.19952 	 m..s
   63 	    92 	 0.13272 	 0.20303 	 m..s
   65 	    93 	 0.13331 	 0.20571 	 m..s
  100 	    94 	 0.19015 	 0.20645 	 ~...
   88 	    95 	 0.18344 	 0.20793 	 ~...
   69 	    96 	 0.13434 	 0.21189 	 m..s
   88 	    97 	 0.18344 	 0.21259 	 ~...
   32 	    98 	 0.11249 	 0.21766 	 MISS
  103 	    99 	 0.19237 	 0.21803 	 ~...
   88 	   100 	 0.18344 	 0.22067 	 m..s
  115 	   101 	 0.23966 	 0.22196 	 ~...
  107 	   102 	 0.19520 	 0.22270 	 ~...
  102 	   103 	 0.19229 	 0.22442 	 m..s
  106 	   104 	 0.19360 	 0.22482 	 m..s
   71 	   105 	 0.13474 	 0.22485 	 m..s
   67 	   106 	 0.13360 	 0.22855 	 m..s
  109 	   107 	 0.21486 	 0.23105 	 ~...
  112 	   108 	 0.21758 	 0.23266 	 ~...
  113 	   109 	 0.21986 	 0.23504 	 ~...
   84 	   110 	 0.13997 	 0.23717 	 m..s
   88 	   111 	 0.18344 	 0.23828 	 m..s
   82 	   112 	 0.13989 	 0.23847 	 m..s
  110 	   113 	 0.21754 	 0.24099 	 ~...
  117 	   114 	 0.24168 	 0.24792 	 ~...
  118 	   115 	 0.24851 	 0.26639 	 ~...
   88 	   116 	 0.18344 	 0.27117 	 m..s
   88 	   117 	 0.18344 	 0.27856 	 m..s
  105 	   118 	 0.19335 	 0.28112 	 m..s
  108 	   119 	 0.20749 	 0.28556 	 m..s
  120 	   120 	 0.29093 	 0.32036 	 ~...
==========================================
r_mrr = 0.6502235531806946
r2_mrr = 0.32126951217651367
spearmanr_mrr@5 = 0.9920704364776611
spearmanr_mrr@10 = 0.960848867893219
spearmanr_mrr@50 = 0.9674414396286011
spearmanr_mrr@100 = 0.8849689960479736
spearmanr_mrr@All = 0.8911663889884949
==========================================
test time: 0.402
Done Testing dataset CoDExSmall
total time taken: 190.68375706672668
training time taken: 181.15093636512756
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6502)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.3213)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9921)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9608)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9674)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8850)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8912)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.817199671117123}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 2733616111716865
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [903, 328, 1167, 1212, 540, 1022, 856, 1148, 911, 593, 904, 377, 984, 228, 305, 281, 423, 436, 545, 162, 409, 268, 1024, 958, 349, 836, 577, 65, 772, 1197, 400, 1161, 70, 846, 831, 1187, 462, 1067, 266, 499, 914, 1119, 805, 1004, 670, 1068, 589, 101, 672, 467, 827, 79, 700, 699, 418, 659, 693, 171, 1056, 717, 394, 466, 1175, 117, 718, 665, 148, 1147, 351, 439, 1177, 966, 433, 724, 707, 196, 876, 848, 775, 762, 896, 592, 1214, 123, 729, 661, 306, 238, 89, 734, 746, 668, 711, 226, 515, 310, 100, 785, 1211, 821, 907, 562, 802, 921, 617, 404, 1111, 141, 820, 71, 233, 736, 833, 694, 360, 789, 814, 615, 382, 142, 760]
valid_ids (0): []
train_ids (1094): [894, 739, 526, 732, 764, 556, 154, 1210, 580, 634, 538, 504, 383, 235, 29, 946, 458, 867, 631, 1162, 863, 222, 1114, 888, 362, 1174, 189, 850, 774, 223, 509, 293, 1133, 2, 949, 627, 242, 740, 1059, 520, 583, 187, 103, 939, 152, 883, 697, 1087, 131, 837, 1052, 1120, 1095, 389, 77, 422, 1163, 183, 744, 195, 243, 737, 380, 301, 962, 988, 1014, 150, 960, 190, 209, 352, 551, 490, 109, 120, 172, 408, 289, 261, 182, 929, 318, 658, 572, 1008, 258, 374, 844, 241, 447, 630, 1016, 376, 1077, 229, 806, 657, 719, 927, 208, 449, 86, 517, 157, 344, 868, 399, 345, 1091, 1100, 249, 1176, 60, 675, 1043, 444, 877, 265, 855, 508, 749, 484, 1060, 88, 95, 923, 287, 59, 563, 935, 616, 1019, 225, 678, 1080, 521, 933, 144, 406, 568, 1153, 192, 854, 692, 781, 481, 1013, 165, 275, 290, 129, 721, 731, 518, 198, 1191, 1116, 1122, 548, 555, 413, 31, 512, 75, 626, 849, 606, 424, 82, 671, 611, 158, 941, 928, 695, 69, 917, 1118, 149, 940, 603, 1128, 432, 1126, 346, 943, 535, 981, 197, 461, 838, 994, 255, 800, 498, 1127, 354, 456, 916, 956, 1033, 708, 893, 1018, 1020, 1149, 401, 500, 507, 1173, 777, 277, 194, 751, 472, 353, 78, 703, 825, 14, 459, 1090, 973, 15, 899, 1089, 359, 111, 106, 664, 1182, 476, 696, 793, 451, 487, 858, 188, 704, 370, 163, 272, 743, 332, 1107, 81, 145, 473, 912, 479, 337, 440, 236, 677, 442, 944, 385, 340, 167, 19, 263, 922, 1085, 1048, 529, 674, 248, 130, 652, 17, 810, 1146, 395, 1104, 201, 125, 232, 537, 35, 990, 987, 1097, 710, 480, 327, 90, 1134, 219, 974, 733, 202, 807, 861, 181, 553, 477, 783, 947, 860, 112, 56, 1179, 654, 215, 685, 558, 1155, 586, 522, 726, 1201, 778, 748, 1037, 333, 516, 794, 124, 533, 722, 115, 690, 1154, 214, 283, 673, 906, 823, 393, 87, 411, 788, 497, 121, 930, 813, 792, 1203, 1072, 396, 51, 1031, 667, 909, 895, 811, 330, 107, 637, 884, 683, 1124, 342, 495, 104, 632, 527, 879, 567, 1098, 1140, 1054, 1075, 299, 590, 768, 308, 887, 1010, 218, 1049, 460, 367, 581, 1164, 605, 742, 761, 598, 464, 254, 375, 262, 766, 1145, 386, 1115, 513, 528, 231, 1135, 392, 865, 828, 1186, 660, 96, 274, 534, 983, 1021, 321, 547, 278, 1199, 804, 1026, 93, 797, 1006, 217, 482, 1017, 246, 1045, 1032, 784, 997, 882, 795, 334, 485, 641, 253, 610, 763, 63, 1074, 878, 76, 478, 1058, 584, 607, 859, 84, 869, 1168, 791, 118, 434, 519, 44, 381, 184, 213, 453, 622, 443, 55, 206, 651, 138, 292, 315, 959, 576, 320, 506, 132, 1151, 998, 1169, 986, 28, 525, 952, 669, 613, 957, 801, 852, 969, 752, 323, 829, 1065, 716, 220, 560, 276, 224, 474, 1166, 866, 91, 738, 595, 798, 297, 625, 180, 816, 938, 350, 999, 1159, 176, 684, 635, 174, 643, 1180, 1012, 1213, 1188, 809, 566, 133, 578, 1025, 343, 357, 1200, 597, 1108, 951, 99, 1204, 273, 483, 647, 1183, 623, 339, 361, 531, 412, 552, 68, 715, 62, 1202, 614, 758, 620, 1207, 680, 417, 10, 491, 698, 1195, 1123, 1030, 720, 913, 264, 295, 296, 1005, 179, 471, 54, 1046, 812, 369, 757, 32, 853, 1142, 542, 842, 146, 11, 286, 1079, 5, 961, 559, 1205, 965, 322, 1157, 1036, 972, 116, 1064, 985, 329, 702, 16, 221, 20, 1047, 771, 937, 830, 1138, 953, 1081, 178, 366, 1156, 435, 74, 18, 80, 39, 1063, 786, 257, 834, 430, 618, 250, 1039, 514, 288, 1112, 968, 505, 587, 872, 588, 915, 964, 431, 502, 714, 730, 799, 845, 390, 139, 204, 324, 1040, 511, 1196, 437, 642, 1035, 1160, 1184, 582, 363, 410, 532, 624, 503, 463, 325, 1076, 1129, 832, 835, 1034, 12, 446, 136, 256, 92, 151, 608, 676, 628, 317, 1002, 991, 1071, 919, 1105, 1069, 979, 585, 926, 457, 862, 523, 319, 338, 892, 594, 307, 57, 1093, 689, 1178, 279, 127, 1094, 621, 452, 114, 554, 425, 205, 160, 967, 83, 1062, 314, 619, 175, 638, 1132, 741, 280, 686, 387, 488, 169, 874, 420, 186, 230, 216, 924, 1101, 97, 438, 298, 122, 1209, 415, 723, 779, 609, 826, 153, 602, 954, 7, 364, 493, 687, 22, 161, 1165, 108, 889, 137, 1103, 536, 428, 948, 128, 931, 355, 384, 403, 237, 767, 1096, 85, 126, 379, 311, 787, 1029, 113, 886, 701, 942, 147, 441, 640, 600, 1110, 1003, 421, 549, 977, 770, 429, 46, 1023, 414, 819, 891, 378, 285, 870, 890, 1001, 776, 207, 469, 170, 840, 570, 747, 24, 98, 655, 579, 544, 902, 416, 918, 191, 901, 227, 936, 43, 1027, 454, 135, 291, 1137, 755, 1057, 1131, 662, 681, 982, 27, 591, 1051, 1009, 765, 302, 539, 405, 688, 1102, 489, 571, 569, 1136, 240, 1092, 910, 419, 709, 336, 300, 1088, 140, 851, 159, 1144, 313, 155, 649, 309, 1143, 134, 4, 934, 388, 975, 843, 30, 564, 575, 372, 646, 864, 546, 365, 871, 976, 1189, 282, 36, 727, 759, 105, 166, 45, 49, 1208, 873, 326, 510, 269, 725, 808, 34, 629, 1050, 37, 199, 1171, 407, 25, 494, 244, 21, 267, 348, 782, 356, 648, 1139, 251, 398, 465, 335, 650, 177, 978, 596, 885, 880, 501, 604, 847, 745, 612, 212, 818, 682, 304, 64, 245, 1078, 73, 200, 1172, 1194, 1028, 1185, 541, 817, 550, 316, 1117, 1130, 397, 955, 1121, 119, 1158, 271, 496, 1125, 557, 193, 6, 492, 42, 284, 639, 203, 897, 402, 920, 996, 824, 644, 445, 1, 66, 1055, 992, 713, 1099, 530, 450, 756, 347, 803, 1082, 260, 211, 391, 1073, 524, 1000, 875, 185, 995, 455, 259, 633, 656, 574, 50, 40, 1193, 247, 239, 773, 769, 67, 963, 1198, 1041, 728, 303, 565, 470, 796, 753, 1113, 23, 1141, 1152, 705, 371, 58, 1044, 48, 341, 1061, 426, 102, 358, 754, 38, 294, 599, 679, 706, 573, 252, 898, 841, 1192, 168, 1038, 9, 94, 691, 61, 1011, 905, 636, 857, 822, 1042, 486, 475, 663, 3, 210, 110, 52, 815, 1181, 945, 925, 8, 1066, 1150, 989, 173, 1109, 331, 645, 950, 653, 543, 735, 26, 270, 72, 0, 234, 1206, 750, 427, 881, 143, 993, 1015, 1053, 312, 1170, 1086, 790, 971, 900, 468, 47, 712, 164, 780, 1084, 839, 53, 1007, 448, 561, 970, 373, 666, 156, 368, 33, 1083, 601, 908, 1070, 1106, 41, 932, 980, 13, 1190]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3307245663304244
the save name prefix for this run is:  chkpt-ID_3307245663304244_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 461
rank avg (pred): 0.492 +- 0.004
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001097101

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 625
rank avg (pred): 0.263 +- 0.115
mrr vals (pred, true): 0.009, 0.160
batch losses (mrrl, rdl): 0.0, 0.0002465626

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.144 +- 0.109
mrr vals (pred, true): 0.191, 0.228
batch losses (mrrl, rdl): 0.0, 0.0001996086

Epoch over!
epoch time: 12.128

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 33
rank avg (pred): 0.161 +- 0.125
mrr vals (pred, true): 0.209, 0.198
batch losses (mrrl, rdl): 0.0, 0.0002916862

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 784
rank avg (pred): 0.344 +- 0.284
mrr vals (pred, true): 0.232, 0.005
batch losses (mrrl, rdl): 0.0, 0.0002170886

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 658
rank avg (pred): 0.206 +- 0.182
mrr vals (pred, true): 0.294, 0.004
batch losses (mrrl, rdl): 0.0, 0.0012296046

Epoch over!
epoch time: 12.031

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 36
rank avg (pred): 0.177 +- 0.151
mrr vals (pred, true): 0.262, 0.221
batch losses (mrrl, rdl): 0.0, 0.0004129333

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.197 +- 0.173
mrr vals (pred, true): 0.332, 0.059
batch losses (mrrl, rdl): 0.0, 8.93e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 172
rank avg (pred): 0.163 +- 0.150
mrr vals (pred, true): 0.337, 0.004
batch losses (mrrl, rdl): 0.0, 0.0018127549

Epoch over!
epoch time: 11.871

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 749
rank avg (pred): 0.313 +- 0.299
mrr vals (pred, true): 0.325, 0.167
batch losses (mrrl, rdl): 0.0, 0.0015957667

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 219
rank avg (pred): 0.146 +- 0.148
mrr vals (pred, true): 0.365, 0.004
batch losses (mrrl, rdl): 0.0, 0.0021256607

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 218
rank avg (pred): 0.199 +- 0.272
mrr vals (pred, true): 0.321, 0.004
batch losses (mrrl, rdl): 0.0, 0.0014522673

Epoch over!
epoch time: 11.869

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 761
rank avg (pred): 0.534 +- 0.318
mrr vals (pred, true): 0.027, 0.025
batch losses (mrrl, rdl): 0.0, 0.0001148897

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 340
rank avg (pred): 0.230 +- 0.299
mrr vals (pred, true): 0.283, 0.218
batch losses (mrrl, rdl): 0.0, 0.0006410858

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 753
rank avg (pred): 0.299 +- 0.325
mrr vals (pred, true): 0.182, 0.148
batch losses (mrrl, rdl): 0.0, 0.0011698608

Epoch over!
epoch time: 11.869

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 750
rank avg (pred): 0.308 +- 0.332
mrr vals (pred, true): 0.217, 0.154
batch losses (mrrl, rdl): 0.0401578024, 0.0012716874

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 683
rank avg (pred): 0.432 +- 0.320
mrr vals (pred, true): 0.101, 0.003
batch losses (mrrl, rdl): 0.0262584072, 8.10591e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 252
rank avg (pred): 0.138 +- 0.184
mrr vals (pred, true): 0.172, 0.234
batch losses (mrrl, rdl): 0.0383965969, 0.0002391925

Epoch over!
epoch time: 12.096

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 325
rank avg (pred): 0.122 +- 0.175
mrr vals (pred, true): 0.193, 0.188
batch losses (mrrl, rdl): 0.0003432459, 0.0001187413

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 429
rank avg (pred): 0.144 +- 0.184
mrr vals (pred, true): 0.177, 0.004
batch losses (mrrl, rdl): 0.1606435925, 0.0019984839

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 571
rank avg (pred): 0.315 +- 0.176
mrr vals (pred, true): 0.099, 0.097
batch losses (mrrl, rdl): 0.0242524426, 0.0001246092

Epoch over!
epoch time: 12.21

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 212
rank avg (pred): 0.162 +- 0.186
mrr vals (pred, true): 0.166, 0.004
batch losses (mrrl, rdl): 0.134904325, 0.0018010625

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 732
rank avg (pred): 0.504 +- 0.229
mrr vals (pred, true): 0.064, 0.067
batch losses (mrrl, rdl): 0.0019647954, 0.0018382835

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 771
rank avg (pred): 0.342 +- 0.181
mrr vals (pred, true): 0.102, 0.030
batch losses (mrrl, rdl): 0.0275195912, 0.0004290313

Epoch over!
epoch time: 12.167

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.172 +- 0.191
mrr vals (pred, true): 0.167, 0.005
batch losses (mrrl, rdl): 0.1366481185, 0.001567935

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 317
rank avg (pred): 0.163 +- 0.195
mrr vals (pred, true): 0.171, 0.283
batch losses (mrrl, rdl): 0.1244384199, 0.0004019151

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1089
rank avg (pred): 0.175 +- 0.186
mrr vals (pred, true): 0.168, 0.201
batch losses (mrrl, rdl): 0.0106632374, 0.000346158

Epoch over!
epoch time: 12.137

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1115
rank avg (pred): 0.180 +- 0.189
mrr vals (pred, true): 0.163, 0.004
batch losses (mrrl, rdl): 0.1273040026, 0.0016501519

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 776
rank avg (pred): 0.340 +- 0.180
mrr vals (pred, true): 0.116, 0.075
batch losses (mrrl, rdl): 0.0439779907, 9.39973e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 245
rank avg (pred): 0.191 +- 0.186
mrr vals (pred, true): 0.166, 0.227
batch losses (mrrl, rdl): 0.0369000584, 0.0005361004

Epoch over!
epoch time: 12.108

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1076
rank avg (pred): 0.186 +- 0.185
mrr vals (pred, true): 0.165, 0.257
batch losses (mrrl, rdl): 0.0851818323, 0.0005135658

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 569
rank avg (pred): 0.259 +- 0.169
mrr vals (pred, true): 0.129, 0.118
batch losses (mrrl, rdl): 0.0013756163, 7.03119e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.222 +- 0.179
mrr vals (pred, true): 0.153, 0.005
batch losses (mrrl, rdl): 0.1061050668, 0.0011726613

Epoch over!
epoch time: 12.472

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 632
rank avg (pred): 0.353 +- 0.170
mrr vals (pred, true): 0.090, 0.175
batch losses (mrrl, rdl): 0.0727010816, 0.0008084101

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1082
rank avg (pred): 0.188 +- 0.181
mrr vals (pred, true): 0.167, 0.175
batch losses (mrrl, rdl): 0.0006898736, 0.0002755056

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 115
rank avg (pred): 0.208 +- 0.177
mrr vals (pred, true): 0.165, 0.212
batch losses (mrrl, rdl): 0.0219564121, 0.0005564793

Epoch over!
epoch time: 12.336

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 374
rank avg (pred): 0.185 +- 0.189
mrr vals (pred, true): 0.148, 0.190
batch losses (mrrl, rdl): 0.0176864266, 0.0004273071

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 541
rank avg (pred): 0.321 +- 0.165
mrr vals (pred, true): 0.099, 0.045
batch losses (mrrl, rdl): 0.0239204206, 0.000252117

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 137
rank avg (pred): 0.195 +- 0.178
mrr vals (pred, true): 0.167, 0.191
batch losses (mrrl, rdl): 0.0059701819, 0.0004437525

Epoch over!
epoch time: 12.12

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.415 +- 0.180
mrr vals (pred, true): 0.066, 0.005
batch losses (mrrl, rdl): 0.0025691274, 5.51681e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.173 +- 0.189
mrr vals (pred, true): 0.172, 0.216
batch losses (mrrl, rdl): 0.0189749692, 0.0003252522

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 792
rank avg (pred): 0.361 +- 0.169
mrr vals (pred, true): 0.074, 0.005
batch losses (mrrl, rdl): 0.0059579308, 0.0002926906

Epoch over!
epoch time: 12.277

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.150 +- 0.194
mrr vals (pred, true): 0.185, 0.299
batch losses (mrrl, rdl): 0.1287244558, 0.0003553438

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 338
rank avg (pred): 0.201 +- 0.182
mrr vals (pred, true): 0.158, 0.179
batch losses (mrrl, rdl): 0.0046103941, 0.0004198597

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 830
rank avg (pred): 0.350 +- 0.164
mrr vals (pred, true): 0.085, 0.218
batch losses (mrrl, rdl): 0.1762230992, 0.0020085941

Epoch over!
epoch time: 12.155

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.572 +- 0.254
mrr vals (pred, true): 0.062, 0.001

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.06150 	 0.00073 	 m..s
    0 	     1 	 0.06150 	 0.00091 	 m..s
    0 	     2 	 0.06150 	 0.00104 	 m..s
    0 	     3 	 0.06150 	 0.00107 	 m..s
    0 	     4 	 0.06150 	 0.00113 	 m..s
    0 	     5 	 0.06150 	 0.00121 	 m..s
    0 	     6 	 0.06150 	 0.00219 	 m..s
    0 	     7 	 0.06150 	 0.00222 	 m..s
   60 	     8 	 0.09162 	 0.00277 	 m..s
  108 	     9 	 0.16776 	 0.00332 	 MISS
   59 	    10 	 0.09106 	 0.00333 	 m..s
   85 	    11 	 0.16269 	 0.00343 	 MISS
  103 	    12 	 0.16709 	 0.00344 	 MISS
    0 	    13 	 0.06150 	 0.00346 	 m..s
   89 	    14 	 0.16434 	 0.00347 	 MISS
    0 	    15 	 0.06150 	 0.00348 	 m..s
   51 	    16 	 0.08472 	 0.00349 	 m..s
  100 	    17 	 0.16665 	 0.00356 	 MISS
   13 	    18 	 0.06220 	 0.00362 	 m..s
   96 	    19 	 0.16608 	 0.00364 	 MISS
   62 	    20 	 0.09820 	 0.00369 	 m..s
   32 	    21 	 0.07760 	 0.00369 	 m..s
   73 	    22 	 0.16065 	 0.00373 	 MISS
   29 	    23 	 0.07673 	 0.00375 	 m..s
   39 	    24 	 0.08029 	 0.00378 	 m..s
   17 	    25 	 0.07297 	 0.00393 	 m..s
   24 	    26 	 0.07622 	 0.00394 	 m..s
   75 	    27 	 0.16075 	 0.00394 	 MISS
   20 	    28 	 0.07380 	 0.00396 	 m..s
   26 	    29 	 0.07669 	 0.00401 	 m..s
   34 	    30 	 0.07874 	 0.00412 	 m..s
   71 	    31 	 0.15918 	 0.00416 	 MISS
  109 	    32 	 0.16782 	 0.00424 	 MISS
   47 	    33 	 0.08413 	 0.00425 	 m..s
  120 	    34 	 0.17562 	 0.00432 	 MISS
   43 	    35 	 0.08309 	 0.00434 	 m..s
   63 	    36 	 0.10428 	 0.00435 	 m..s
   46 	    37 	 0.08405 	 0.00443 	 m..s
   36 	    38 	 0.07983 	 0.00448 	 m..s
   37 	    39 	 0.08009 	 0.00449 	 m..s
   64 	    40 	 0.10606 	 0.00451 	 MISS
   97 	    41 	 0.16629 	 0.00452 	 MISS
  104 	    42 	 0.16709 	 0.00457 	 MISS
   88 	    43 	 0.16309 	 0.00459 	 MISS
   55 	    44 	 0.08916 	 0.00464 	 m..s
   83 	    45 	 0.16227 	 0.00471 	 MISS
   79 	    46 	 0.16176 	 0.00484 	 MISS
    0 	    47 	 0.06150 	 0.00502 	 m..s
   98 	    48 	 0.16633 	 0.00521 	 MISS
   66 	    49 	 0.11343 	 0.00533 	 MISS
   33 	    50 	 0.07831 	 0.00559 	 m..s
   93 	    51 	 0.16566 	 0.00586 	 MISS
   22 	    52 	 0.07586 	 0.01197 	 m..s
    0 	    53 	 0.06150 	 0.01819 	 m..s
   41 	    54 	 0.08077 	 0.02089 	 m..s
   14 	    55 	 0.06222 	 0.02123 	 m..s
   37 	    56 	 0.08009 	 0.03260 	 m..s
   21 	    57 	 0.07383 	 0.04846 	 ~...
   31 	    58 	 0.07712 	 0.05100 	 ~...
   15 	    59 	 0.06271 	 0.05123 	 ~...
   24 	    60 	 0.07622 	 0.05555 	 ~...
   54 	    61 	 0.08882 	 0.05678 	 m..s
   49 	    62 	 0.08461 	 0.05706 	 ~...
   58 	    63 	 0.08996 	 0.05797 	 m..s
   23 	    64 	 0.07601 	 0.05856 	 ~...
   18 	    65 	 0.07379 	 0.06077 	 ~...
   12 	    66 	 0.06179 	 0.07000 	 ~...
   51 	    67 	 0.08472 	 0.07381 	 ~...
   16 	    68 	 0.06278 	 0.10812 	 m..s
   57 	    69 	 0.08981 	 0.10912 	 ~...
   42 	    70 	 0.08158 	 0.11465 	 m..s
   40 	    71 	 0.08062 	 0.13439 	 m..s
   64 	    72 	 0.10606 	 0.13929 	 m..s
   18 	    73 	 0.07379 	 0.14437 	 m..s
   30 	    74 	 0.07707 	 0.14583 	 m..s
   68 	    75 	 0.11449 	 0.15013 	 m..s
   26 	    76 	 0.07669 	 0.15118 	 m..s
   28 	    77 	 0.07671 	 0.15183 	 m..s
   60 	    78 	 0.09162 	 0.15719 	 m..s
   35 	    79 	 0.07923 	 0.16450 	 m..s
   67 	    80 	 0.11378 	 0.17539 	 m..s
   92 	    81 	 0.16493 	 0.17776 	 ~...
   77 	    82 	 0.16139 	 0.18752 	 ~...
  100 	    83 	 0.16665 	 0.18798 	 ~...
  115 	    84 	 0.17144 	 0.19390 	 ~...
   70 	    85 	 0.15869 	 0.19394 	 m..s
  114 	    86 	 0.17143 	 0.19952 	 ~...
   69 	    87 	 0.15826 	 0.20056 	 m..s
  118 	    88 	 0.17538 	 0.20127 	 ~...
  105 	    89 	 0.16709 	 0.20139 	 m..s
   74 	    90 	 0.16065 	 0.20303 	 m..s
   99 	    91 	 0.16634 	 0.20847 	 m..s
   78 	    92 	 0.16168 	 0.20874 	 m..s
   71 	    93 	 0.15918 	 0.20936 	 m..s
  118 	    94 	 0.17538 	 0.21035 	 m..s
   81 	    95 	 0.16194 	 0.21315 	 m..s
   49 	    96 	 0.08461 	 0.21735 	 MISS
   80 	    97 	 0.16186 	 0.21827 	 m..s
  102 	    98 	 0.16705 	 0.21971 	 m..s
  117 	    99 	 0.17390 	 0.21972 	 m..s
   76 	   100 	 0.16098 	 0.21990 	 m..s
   89 	   101 	 0.16434 	 0.22122 	 m..s
  116 	   102 	 0.17169 	 0.22196 	 m..s
   94 	   103 	 0.16584 	 0.22511 	 m..s
   82 	   104 	 0.16203 	 0.22550 	 m..s
   48 	   105 	 0.08420 	 0.22602 	 MISS
  110 	   106 	 0.16825 	 0.22855 	 m..s
  107 	   107 	 0.16750 	 0.22910 	 m..s
   86 	   108 	 0.16298 	 0.23084 	 m..s
   53 	   109 	 0.08853 	 0.23590 	 MISS
   45 	   110 	 0.08375 	 0.23662 	 MISS
   56 	   111 	 0.08946 	 0.23847 	 MISS
  111 	   112 	 0.16867 	 0.24346 	 m..s
  112 	   113 	 0.16868 	 0.25300 	 m..s
  106 	   114 	 0.16747 	 0.25534 	 m..s
   44 	   115 	 0.08349 	 0.25581 	 MISS
   95 	   116 	 0.16606 	 0.25867 	 m..s
   84 	   117 	 0.16241 	 0.26750 	 MISS
   91 	   118 	 0.16444 	 0.27141 	 MISS
  113 	   119 	 0.17023 	 0.27607 	 MISS
   87 	   120 	 0.16305 	 0.30443 	 MISS
==========================================
r_mrr = 0.4660506844520569
r2_mrr = 0.18061864376068115
spearmanr_mrr@5 = 0.7073398232460022
spearmanr_mrr@10 = 0.8931030035018921
spearmanr_mrr@50 = 0.9624319076538086
spearmanr_mrr@100 = 0.9692460298538208
spearmanr_mrr@All = 0.967703104019165
==========================================
test time: 0.47
Done Testing dataset CoDExSmall
total time taken: 192.2868525981903
training time taken: 182.3914806842804
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.4661)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.1806)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.7073)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.8931)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9624)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.9692)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9677)}}, 'test_loss': {'DistMult': {'CoDExSmall': 6.098985173506662}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 2660138701403651
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [881, 274, 983, 1157, 1150, 555, 1015, 791, 419, 719, 757, 921, 36, 364, 1188, 941, 558, 901, 198, 832, 696, 40, 61, 281, 1194, 59, 739, 378, 53, 607, 1139, 814, 675, 984, 353, 928, 1039, 255, 460, 846, 43, 362, 903, 46, 210, 375, 957, 1169, 544, 823, 310, 371, 651, 193, 27, 864, 501, 120, 751, 1115, 89, 792, 550, 882, 621, 899, 926, 620, 736, 880, 775, 594, 730, 166, 110, 496, 291, 139, 1027, 1108, 1103, 160, 799, 724, 701, 737, 654, 249, 1081, 233, 269, 156, 12, 435, 1098, 686, 441, 179, 1154, 1023, 1173, 996, 294, 908, 973, 282, 968, 1145, 333, 532, 706, 712, 655, 127, 841, 1160, 162, 184, 1, 749, 222]
valid_ids (0): []
train_ids (1094): [617, 334, 562, 1105, 1043, 844, 1159, 138, 1198, 509, 429, 24, 994, 224, 497, 627, 998, 1199, 257, 833, 802, 1085, 1165, 785, 566, 180, 860, 952, 1028, 22, 384, 403, 443, 695, 35, 810, 761, 200, 936, 77, 723, 411, 287, 347, 631, 482, 766, 100, 277, 672, 1211, 163, 355, 689, 504, 146, 1013, 48, 381, 315, 207, 949, 1017, 18, 1210, 1084, 23, 1118, 454, 769, 214, 1053, 679, 1111, 746, 389, 143, 392, 405, 913, 907, 358, 1186, 455, 918, 56, 342, 728, 506, 1022, 537, 1079, 895, 643, 1185, 244, 794, 1060, 388, 929, 866, 1051, 336, 352, 426, 1163, 106, 545, 820, 540, 524, 1208, 438, 1152, 767, 440, 58, 990, 759, 923, 155, 965, 800, 979, 126, 173, 1175, 569, 889, 1137, 112, 1004, 1126, 638, 174, 124, 644, 21, 565, 1038, 781, 424, 851, 930, 363, 39, 859, 343, 900, 571, 1046, 745, 650, 945, 665, 1029, 452, 253, 1148, 1113, 189, 959, 597, 60, 987, 1156, 290, 1026, 408, 1136, 326, 483, 891, 836, 232, 57, 539, 639, 31, 771, 1166, 503, 584, 1068, 177, 83, 659, 365, 312, 956, 523, 190, 788, 87, 19, 946, 511, 1037, 1178, 436, 932, 1020, 669, 181, 54, 783, 1000, 938, 579, 1155, 768, 196, 811, 288, 718, 185, 71, 286, 1171, 1146, 492, 714, 611, 962, 250, 808, 25, 741, 300, 1124, 670, 176, 1003, 1087, 916, 893, 47, 314, 321, 301, 734, 81, 777, 444, 434, 246, 868, 495, 848, 619, 13, 374, 450, 839, 733, 103, 20, 773, 822, 380, 711, 824, 1168, 512, 266, 637, 464, 1086, 15, 1007, 1102, 829, 385, 320, 349, 552, 653, 95, 323, 298, 105, 753, 69, 813, 704, 580, 1054, 1089, 499, 311, 1128, 152, 633, 459, 489, 169, 886, 217, 357, 1011, 747, 468, 226, 593, 279, 414, 939, 821, 570, 449, 165, 1164, 313, 517, 150, 977, 806, 26, 239, 604, 616, 234, 2, 416, 534, 585, 850, 219, 225, 588, 835, 1096, 167, 758, 583, 168, 568, 80, 855, 1106, 694, 284, 1042, 73, 919, 33, 367, 231, 656, 115, 1073, 340, 394, 1205, 433, 681, 406, 682, 360, 243, 680, 674, 911, 431, 1138, 985, 954, 151, 238, 904, 386, 1062, 710, 1200, 1101, 1070, 809, 914, 327, 272, 790, 1192, 1177, 116, 400, 215, 251, 328, 37, 415, 119, 576, 74, 410, 157, 4, 636, 687, 975, 465, 445, 1063, 1083, 1076, 1010, 149, 1024, 1197, 1190, 472, 346, 123, 974, 992, 595, 413, 471, 1080, 581, 887, 425, 778, 700, 667, 154, 872, 108, 283, 756, 892, 807, 1123, 615, 254, 1184, 264, 563, 873, 890, 303, 601, 144, 209, 285, 463, 256, 493, 995, 292, 178, 211, 396, 1117, 986, 182, 359, 270, 484, 79, 553, 705, 731, 1120, 1040, 1045, 518, 796, 235, 366, 933, 609, 755, 856, 409, 1014, 516, 369, 68, 260, 688, 332, 519, 572, 707, 319, 1181, 629, 480, 826, 828, 513, 1100, 600, 951, 420, 692, 554, 671, 625, 1092, 129, 1134, 547, 612, 344, 685, 750, 922, 10, 634, 1041, 339, 1202, 804, 118, 107, 30, 159, 702, 1201, 527, 953, 191, 469, 1167, 812, 218, 462, 838, 780, 722, 556, 86, 764, 302, 915, 248, 491, 421, 842, 652, 1064, 628, 442, 1074, 748, 430, 765, 646, 1077, 978, 970, 699, 1109, 1203, 803, 1075, 658, 894, 883, 293, 567, 502, 220, 797, 991, 7, 927, 1091, 446, 75, 795, 242, 691, 587, 976, 1093, 879, 849, 909, 11, 1110, 478, 265, 397, 370, 1135, 510, 236, 898, 542, 88, 267, 308, 716, 192, 1183, 213, 1095, 330, 819, 17, 521, 221, 131, 676, 94, 316, 278, 1125, 439, 1132, 498, 1180, 863, 546, 295, 522, 1056, 1147, 865, 66, 1078, 1055, 988, 140, 752, 657, 935, 65, 418, 677, 275, 391, 708, 531, 526, 660, 1191, 132, 473, 377, 456, 717, 51, 354, 114, 910, 982, 825, 1033, 206, 1212, 461, 479, 348, 559, 6, 1019, 268, 404, 477, 1172, 997, 96, 869, 774, 372, 194, 437, 1001, 432, 673, 98, 818, 14, 989, 111, 229, 309, 787, 241, 172, 350, 770, 128, 1129, 878, 325, 1072, 1008, 395, 1044, 683, 16, 470, 448, 548, 104, 399, 713, 1195, 912, 137, 383, 661, 62, 720, 276, 917, 195, 740, 582, 613, 3, 1193, 259, 1196, 457, 801, 1158, 1187, 1144, 549, 475, 458, 102, 34, 101, 870, 398, 109, 1067, 738, 1097, 1130, 317, 390, 122, 877, 345, 575, 201, 228, 148, 742, 133, 1149, 170, 204, 950, 1142, 862, 599, 837, 827, 525, 401, 97, 1050, 605, 205, 1127, 1006, 161, 1209, 1104, 164, 368, 42, 338, 63, 1009, 197, 341, 183, 666, 993, 817, 64, 72, 488, 533, 942, 273, 560, 754, 626, 972, 38, 32, 188, 1021, 41, 28, 980, 387, 306, 1088, 529, 793, 1030, 447, 263, 888, 958, 1107, 335, 84, 379, 136, 861, 622, 487, 937, 322, 361, 1025, 948, 896, 782, 125, 564, 1094, 481, 373, 1018, 1161, 141, 91, 29, 67, 635, 964, 1002, 649, 299, 760, 647, 1065, 476, 494, 258, 230, 684, 55, 1189, 514, 624, 763, 905, 925, 171, 1143, 591, 186, 175, 331, 981, 725, 135, 1119, 1213, 1012, 721, 417, 735, 645, 577, 535, 1153, 590, 606, 871, 407, 117, 280, 324, 93, 902, 840, 1214, 697, 1071, 955, 76, 853, 920, 474, 632, 831, 508, 969, 153, 662, 1151, 262, 45, 208, 453, 779, 1049, 1179, 515, 1058, 541, 589, 147, 451, 1057, 610, 1170, 0, 538, 726, 1052, 82, 199, 356, 145, 1112, 428, 351, 247, 121, 943, 237, 1059, 602, 1204, 252, 947, 561, 727, 158, 924, 897, 203, 1061, 1114, 52, 1162, 1016, 240, 1176, 543, 875, 640, 49, 641, 618, 729, 423, 858, 971, 467, 776, 536, 668, 815, 130, 427, 1082, 505, 642, 1034, 92, 961, 297, 1116, 608, 485, 854, 1099, 99, 9, 402, 78, 1174, 960, 5, 586, 663, 678, 134, 289, 592, 216, 847, 70, 304, 834, 296, 393, 1122, 271, 693, 551, 805, 772, 490, 212, 1047, 934, 422, 843, 1207, 857, 967, 318, 113, 466, 1005, 867, 1031, 744, 798, 614, 223, 1066, 1090, 885, 1032, 329, 376, 709, 963, 732, 245, 623, 1206, 578, 698, 966, 1131, 520, 664, 500, 816, 789, 90, 261, 142, 648, 884, 573, 762, 940, 412, 1036, 1069, 382, 598, 715, 931, 85, 703, 830, 845, 337, 307, 507, 1048, 852, 1182, 906, 530, 876, 305, 786, 1140, 8, 630, 1035, 44, 187, 743, 486, 574, 596, 690, 202, 557, 528, 1133, 603, 784, 1121, 874, 999, 50, 227, 1141, 944]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6195565893011902
the save name prefix for this run is:  chkpt-ID_6195565893011902_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 912
rank avg (pred): 0.529 +- 0.004
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001100369

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 473
rank avg (pred): 0.329 +- 0.187
mrr vals (pred, true): 0.069, 0.005
batch losses (mrrl, rdl): 0.0, 0.000359002

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 81
rank avg (pred): 0.221 +- 0.155
mrr vals (pred, true): 0.208, 0.208
batch losses (mrrl, rdl): 0.0, 0.0005952147

Epoch over!
epoch time: 12.182

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 687
rank avg (pred): 0.386 +- 0.271
mrr vals (pred, true): 0.206, 0.004
batch losses (mrrl, rdl): 0.0, 8.40955e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 432
rank avg (pred): 0.288 +- 0.221
mrr vals (pred, true): 0.252, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005591701

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.051 +- 0.044
mrr vals (pred, true): 0.353, 0.218
batch losses (mrrl, rdl): 0.0, 2.5452e-06

Epoch over!
epoch time: 11.803

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 704
rank avg (pred): 0.286 +- 0.255
mrr vals (pred, true): 0.311, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004688765

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 230
rank avg (pred): 0.247 +- 0.221
mrr vals (pred, true): 0.332, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007786565

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 865
rank avg (pred): 0.329 +- 0.300
mrr vals (pred, true): 0.326, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002698938

Epoch over!
epoch time: 11.885

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1119
rank avg (pred): 0.287 +- 0.239
mrr vals (pred, true): 0.294, 0.006
batch losses (mrrl, rdl): 0.0, 0.0004801733

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.072 +- 0.073
mrr vals (pred, true): 0.380, 0.174
batch losses (mrrl, rdl): 0.0, 1.92901e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 694
rank avg (pred): 0.299 +- 0.262
mrr vals (pred, true): 0.243, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004185369

Epoch over!
epoch time: 12.083

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 471
rank avg (pred): 0.275 +- 0.243
mrr vals (pred, true): 0.244, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005775917

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 499
rank avg (pred): 0.191 +- 0.172
mrr vals (pred, true): 0.198, 0.157
batch losses (mrrl, rdl): 0.0, 8.19902e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1174
rank avg (pred): 0.312 +- 0.266
mrr vals (pred, true): 0.209, 0.150
batch losses (mrrl, rdl): 0.0, 0.0005391747

Epoch over!
epoch time: 11.841

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 324
rank avg (pred): 0.240 +- 0.232
mrr vals (pred, true): 0.282, 0.218
batch losses (mrrl, rdl): 0.041864533, 0.0008478034

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 154
rank avg (pred): 0.230 +- 0.135
mrr vals (pred, true): 0.119, 0.219
batch losses (mrrl, rdl): 0.0993330181, 0.0007091873

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 563
rank avg (pred): 0.344 +- 0.192
mrr vals (pred, true): 0.121, 0.040
batch losses (mrrl, rdl): 0.0501029491, 0.0003311706

Epoch over!
epoch time: 12.112

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 355
rank avg (pred): 0.233 +- 0.131
mrr vals (pred, true): 0.111, 0.214
batch losses (mrrl, rdl): 0.1062518358, 0.0006732715

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 405
rank avg (pred): 0.256 +- 0.145
mrr vals (pred, true): 0.121, 0.004
batch losses (mrrl, rdl): 0.0501303561, 0.0009203422

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 192
rank avg (pred): 0.214 +- 0.125
mrr vals (pred, true): 0.144, 0.004
batch losses (mrrl, rdl): 0.0886253715, 0.0012559504

Epoch over!
epoch time: 12.085

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.251 +- 0.138
mrr vals (pred, true): 0.118, 0.003
batch losses (mrrl, rdl): 0.0461447053, 0.0010353019

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 807
rank avg (pred): 0.413 +- 0.180
mrr vals (pred, true): 0.089, 0.004
batch losses (mrrl, rdl): 0.01539726, 0.0001006583

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 693
rank avg (pred): 0.415 +- 0.170
mrr vals (pred, true): 0.072, 0.004
batch losses (mrrl, rdl): 0.0048215073, 0.0001085656

Epoch over!
epoch time: 12.078

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.227 +- 0.126
mrr vals (pred, true): 0.120, 0.197
batch losses (mrrl, rdl): 0.0595408678, 0.0006129974

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.008 +- 0.005
mrr vals (pred, true): 0.248, 0.245
batch losses (mrrl, rdl): 0.0001121774, 1.22054e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 977
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.270, 0.305
batch losses (mrrl, rdl): 0.0117901005, 1.52033e-05

Epoch over!
epoch time: 12.3

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 42
rank avg (pred): 0.020 +- 0.011
mrr vals (pred, true): 0.179, 0.213
batch losses (mrrl, rdl): 0.0113448137, 2.3777e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.262 +- 0.142
mrr vals (pred, true): 0.110, 0.005
batch losses (mrrl, rdl): 0.0363408811, 0.0008890011

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 114
rank avg (pred): 0.255 +- 0.143
mrr vals (pred, true): 0.126, 0.216
batch losses (mrrl, rdl): 0.0814372599, 0.0008676323

Epoch over!
epoch time: 12.043

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 238
rank avg (pred): 0.244 +- 0.139
mrr vals (pred, true): 0.137, 0.005
batch losses (mrrl, rdl): 0.075839676, 0.0010771675

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 425
rank avg (pred): 0.244 +- 0.135
mrr vals (pred, true): 0.115, 0.004
batch losses (mrrl, rdl): 0.0428734273, 0.0011487458

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 798
rank avg (pred): 0.439 +- 0.202
mrr vals (pred, true): 0.073, 0.004
batch losses (mrrl, rdl): 0.0055145817, 5.36817e-05

Epoch over!
epoch time: 12.278

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 178
rank avg (pred): 0.298 +- 0.161
mrr vals (pred, true): 0.104, 0.005
batch losses (mrrl, rdl): 0.0289591923, 0.0005609408

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 149
rank avg (pred): 0.287 +- 0.161
mrr vals (pred, true): 0.126, 0.201
batch losses (mrrl, rdl): 0.0573458336, 0.001236249

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 483
rank avg (pred): 0.265 +- 0.147
mrr vals (pred, true): 0.124, 0.005
batch losses (mrrl, rdl): 0.0550023764, 0.0009075709

Epoch over!
epoch time: 12.079

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 579
rank avg (pred): 0.359 +- 0.173
mrr vals (pred, true): 0.105, 0.151
batch losses (mrrl, rdl): 0.0209893957, 0.0007943579

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.501 +- 0.241
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 9.267e-07, 1.43672e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1036
rank avg (pred): 0.298 +- 0.167
mrr vals (pred, true): 0.132, 0.004
batch losses (mrrl, rdl): 0.067917116, 0.0005467939

Epoch over!
epoch time: 12.075

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 237
rank avg (pred): 0.231 +- 0.126
mrr vals (pred, true): 0.114, 0.004
batch losses (mrrl, rdl): 0.0415028594, 0.0012012167

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1071
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.267, 0.215
batch losses (mrrl, rdl): 0.0275128242, 4.73877e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 947
rank avg (pred): 0.546 +- 0.260
mrr vals (pred, true): 0.035, 0.005
batch losses (mrrl, rdl): 0.0023038916, 5.64406e-05

Epoch over!
epoch time: 12.012

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 212
rank avg (pred): 0.323 +- 0.174
mrr vals (pred, true): 0.107, 0.004
batch losses (mrrl, rdl): 0.0326191485, 0.0004615339

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1050
rank avg (pred): 0.163 +- 0.096
mrr vals (pred, true): 0.159, 0.004
batch losses (mrrl, rdl): 0.1185940653, 0.0020700649

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 24
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.235, 0.246
batch losses (mrrl, rdl): 0.0012313479, 1.31119e-05

Epoch over!
epoch time: 12.04

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.435 +- 0.227
mrr vals (pred, true): 0.078, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03673 	 0.00073 	 m..s
    5 	     1 	 0.04046 	 0.00073 	 m..s
    2 	     2 	 0.03739 	 0.00074 	 m..s
    3 	     3 	 0.03933 	 0.00076 	 m..s
    6 	     4 	 0.04443 	 0.00094 	 m..s
   92 	     5 	 0.14304 	 0.00104 	 MISS
   69 	     6 	 0.13168 	 0.00104 	 MISS
    9 	     7 	 0.04651 	 0.00121 	 m..s
    8 	     8 	 0.04531 	 0.00159 	 m..s
   81 	     9 	 0.13719 	 0.00310 	 MISS
   35 	    10 	 0.08967 	 0.00339 	 m..s
   85 	    11 	 0.13975 	 0.00341 	 MISS
   15 	    12 	 0.07800 	 0.00341 	 m..s
   86 	    13 	 0.13976 	 0.00351 	 MISS
   28 	    14 	 0.08819 	 0.00352 	 m..s
   74 	    15 	 0.13252 	 0.00359 	 MISS
   20 	    16 	 0.08338 	 0.00378 	 m..s
   56 	    17 	 0.12757 	 0.00390 	 MISS
   66 	    18 	 0.13114 	 0.00392 	 MISS
    4 	    19 	 0.04044 	 0.00393 	 m..s
   72 	    20 	 0.13228 	 0.00394 	 MISS
    0 	    21 	 0.03671 	 0.00395 	 m..s
   93 	    22 	 0.14321 	 0.00398 	 MISS
   57 	    23 	 0.12773 	 0.00404 	 MISS
   16 	    24 	 0.07883 	 0.00408 	 m..s
   63 	    25 	 0.13041 	 0.00409 	 MISS
   57 	    26 	 0.12773 	 0.00416 	 MISS
   33 	    27 	 0.08953 	 0.00420 	 m..s
   91 	    28 	 0.14235 	 0.00424 	 MISS
   17 	    29 	 0.07935 	 0.00429 	 m..s
   46 	    30 	 0.09348 	 0.00429 	 m..s
   81 	    31 	 0.13719 	 0.00429 	 MISS
   17 	    32 	 0.07935 	 0.00438 	 m..s
   38 	    33 	 0.09070 	 0.00449 	 m..s
   30 	    34 	 0.08922 	 0.00451 	 m..s
   27 	    35 	 0.08807 	 0.00466 	 m..s
   21 	    36 	 0.08366 	 0.00467 	 m..s
   39 	    37 	 0.09079 	 0.00480 	 m..s
   90 	    38 	 0.14191 	 0.00484 	 MISS
   67 	    39 	 0.13128 	 0.00498 	 MISS
   11 	    40 	 0.06320 	 0.00505 	 m..s
   13 	    41 	 0.06444 	 0.00507 	 m..s
   31 	    42 	 0.08932 	 0.00509 	 m..s
   71 	    43 	 0.13228 	 0.00531 	 MISS
   42 	    44 	 0.09109 	 0.00544 	 m..s
   10 	    45 	 0.06184 	 0.00552 	 m..s
   26 	    46 	 0.08699 	 0.00552 	 m..s
    6 	    47 	 0.04443 	 0.00576 	 m..s
   14 	    48 	 0.06683 	 0.00616 	 m..s
   54 	    49 	 0.12715 	 0.00639 	 MISS
   23 	    50 	 0.08450 	 0.00682 	 m..s
   50 	    51 	 0.09791 	 0.02871 	 m..s
   24 	    52 	 0.08694 	 0.03651 	 m..s
   47 	    53 	 0.09576 	 0.03829 	 m..s
   51 	    54 	 0.09899 	 0.04242 	 m..s
   45 	    55 	 0.09341 	 0.04740 	 m..s
   43 	    56 	 0.09188 	 0.04889 	 m..s
   95 	    57 	 0.17807 	 0.05123 	 MISS
   49 	    58 	 0.09753 	 0.05249 	 m..s
   40 	    59 	 0.09092 	 0.05691 	 m..s
   19 	    60 	 0.07975 	 0.05797 	 ~...
   40 	    61 	 0.09092 	 0.05918 	 m..s
   52 	    62 	 0.10423 	 0.06515 	 m..s
   11 	    63 	 0.06320 	 0.07381 	 ~...
   22 	    64 	 0.08448 	 0.11825 	 m..s
   36 	    65 	 0.08970 	 0.12053 	 m..s
   36 	    66 	 0.08970 	 0.12552 	 m..s
   60 	    67 	 0.12827 	 0.13481 	 ~...
   24 	    68 	 0.08694 	 0.14547 	 m..s
   34 	    69 	 0.08959 	 0.15039 	 m..s
   32 	    70 	 0.08950 	 0.15393 	 m..s
   47 	    71 	 0.09576 	 0.15530 	 m..s
   43 	    72 	 0.09188 	 0.15616 	 m..s
   29 	    73 	 0.08822 	 0.16363 	 m..s
   65 	    74 	 0.13056 	 0.16691 	 m..s
   53 	    75 	 0.11875 	 0.16739 	 m..s
   76 	    76 	 0.13308 	 0.17408 	 m..s
   60 	    77 	 0.12827 	 0.17482 	 m..s
   62 	    78 	 0.12851 	 0.18155 	 m..s
   98 	    79 	 0.23194 	 0.18169 	 m..s
   59 	    80 	 0.12779 	 0.18668 	 m..s
   55 	    81 	 0.12756 	 0.18752 	 m..s
   78 	    82 	 0.13417 	 0.19021 	 m..s
   99 	    83 	 0.23566 	 0.19248 	 m..s
   68 	    84 	 0.13160 	 0.19766 	 m..s
   70 	    85 	 0.13225 	 0.19780 	 m..s
   77 	    86 	 0.13403 	 0.19823 	 m..s
  103 	    87 	 0.24856 	 0.19977 	 m..s
   83 	    88 	 0.13742 	 0.20235 	 m..s
  108 	    89 	 0.26241 	 0.20414 	 m..s
   87 	    90 	 0.13976 	 0.20622 	 m..s
  100 	    91 	 0.23710 	 0.21615 	 ~...
  106 	    92 	 0.25602 	 0.21764 	 m..s
  116 	    93 	 0.28717 	 0.21833 	 m..s
  113 	    94 	 0.27580 	 0.21930 	 m..s
   84 	    95 	 0.13822 	 0.21935 	 m..s
   89 	    96 	 0.14132 	 0.22038 	 m..s
  105 	    97 	 0.25466 	 0.22109 	 m..s
   94 	    98 	 0.14345 	 0.22436 	 m..s
  115 	    99 	 0.28647 	 0.22442 	 m..s
   64 	   100 	 0.13052 	 0.22485 	 m..s
   80 	   101 	 0.13609 	 0.22570 	 m..s
  102 	   102 	 0.24747 	 0.22618 	 ~...
  109 	   103 	 0.26456 	 0.22640 	 m..s
   88 	   104 	 0.14123 	 0.22807 	 m..s
   75 	   105 	 0.13296 	 0.22908 	 m..s
   73 	   106 	 0.13242 	 0.23007 	 m..s
  101 	   107 	 0.24526 	 0.23105 	 ~...
  107 	   108 	 0.25998 	 0.23373 	 ~...
  104 	   109 	 0.25084 	 0.23706 	 ~...
   95 	   110 	 0.17807 	 0.23717 	 m..s
   95 	   111 	 0.17807 	 0.24182 	 m..s
   79 	   112 	 0.13418 	 0.24461 	 MISS
  111 	   113 	 0.27162 	 0.25534 	 ~...
  112 	   114 	 0.27562 	 0.26639 	 ~...
  117 	   115 	 0.28840 	 0.26750 	 ~...
  110 	   116 	 0.26795 	 0.27141 	 ~...
  113 	   117 	 0.27580 	 0.27384 	 ~...
  120 	   118 	 0.33674 	 0.29138 	 m..s
  118 	   119 	 0.30183 	 0.30373 	 ~...
  119 	   120 	 0.31971 	 0.30418 	 ~...
==========================================
r_mrr = 0.7371904850006104
r2_mrr = 0.4518410563468933
spearmanr_mrr@5 = 0.9375606179237366
spearmanr_mrr@10 = 0.9252525568008423
spearmanr_mrr@50 = 0.8943980932235718
spearmanr_mrr@100 = 0.8479489684104919
spearmanr_mrr@All = 0.878105103969574
==========================================
test time: 0.396
Done Testing dataset CoDExSmall
total time taken: 191.2580111026764
training time taken: 181.36529088020325
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7372)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4518)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9376)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9253)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8944)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8479)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8781)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.9020875033165794}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 5759413499077296
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [889, 243, 476, 458, 249, 977, 351, 1060, 1164, 17, 1128, 558, 772, 321, 1105, 596, 763, 127, 732, 289, 1029, 11, 136, 99, 780, 1062, 610, 710, 1175, 384, 232, 154, 517, 456, 980, 67, 427, 626, 203, 948, 568, 997, 223, 676, 753, 87, 313, 540, 276, 174, 387, 500, 843, 1159, 163, 861, 656, 85, 139, 350, 200, 1048, 931, 643, 1106, 20, 305, 408, 273, 646, 109, 511, 1025, 1005, 740, 425, 903, 1138, 78, 96, 1211, 797, 1061, 378, 83, 590, 1134, 149, 1142, 267, 449, 1201, 651, 1031, 112, 492, 825, 1047, 1037, 312, 1178, 735, 451, 247, 585, 399, 8, 158, 370, 573, 445, 1091, 34, 468, 1024, 159, 33, 617, 201, 840, 799]
valid_ids (0): []
train_ids (1094): [148, 81, 57, 721, 877, 452, 13, 1087, 719, 496, 360, 199, 1191, 786, 100, 157, 386, 960, 569, 895, 762, 802, 282, 1127, 416, 288, 850, 238, 853, 874, 992, 406, 114, 471, 541, 742, 463, 1143, 743, 91, 553, 811, 869, 973, 804, 867, 383, 993, 356, 1054, 302, 1093, 837, 325, 286, 3, 1194, 146, 778, 432, 911, 892, 213, 1171, 1103, 974, 1075, 1006, 228, 347, 1000, 420, 579, 366, 951, 1129, 301, 74, 947, 441, 1189, 418, 934, 1044, 1161, 619, 1072, 300, 284, 152, 145, 258, 9, 698, 989, 448, 1140, 882, 1003, 217, 536, 412, 110, 222, 757, 578, 1108, 1065, 657, 730, 981, 1056, 926, 293, 134, 696, 95, 31, 439, 51, 784, 856, 202, 263, 140, 1130, 684, 443, 264, 566, 75, 1212, 25, 188, 311, 944, 1050, 830, 922, 713, 489, 1034, 28, 41, 173, 580, 277, 212, 129, 298, 822, 1079, 252, 1176, 116, 563, 625, 779, 891, 679, 642, 333, 354, 829, 470, 1173, 589, 587, 894, 235, 1022, 402, 688, 885, 1098, 820, 1213, 475, 1137, 21, 358, 582, 921, 570, 380, 1084, 411, 707, 169, 245, 197, 985, 905, 1193, 604, 1036, 618, 120, 521, 485, 22, 90, 852, 562, 331, 118, 430, 318, 1162, 237, 1059, 652, 674, 689, 828, 549, 1168, 435, 798, 600, 268, 682, 552, 988, 530, 875, 53, 224, 107, 594, 806, 858, 673, 501, 43, 183, 1184, 663, 916, 991, 942, 628, 1070, 497, 841, 122, 680, 389, 827, 1073, 274, 352, 1170, 623, 936, 789, 369, 690, 156, 132, 1158, 906, 986, 904, 966, 662, 0, 510, 297, 1206, 72, 764, 253, 338, 121, 182, 290, 720, 564, 812, 636, 700, 736, 1192, 1163, 193, 632, 315, 65, 454, 415, 442, 979, 575, 678, 395, 631, 896, 1145, 381, 505, 444, 1039, 543, 433, 927, 294, 859, 595, 1207, 768, 349, 467, 893, 461, 39, 266, 1019, 603, 1203, 512, 401, 241, 307, 1141, 211, 106, 667, 453, 538, 1012, 561, 1081, 838, 1013, 609, 635, 715, 557, 1139, 520, 1051, 447, 382, 773, 147, 781, 949, 1132, 473, 488, 373, 932, 332, 965, 704, 125, 82, 255, 162, 1126, 259, 460, 40, 180, 920, 756, 972, 377, 654, 883, 56, 160, 1076, 925, 353, 144, 117, 170, 1041, 1069, 375, 161, 546, 345, 308, 322, 250, 613, 469, 555, 30, 355, 554, 529, 1110, 910, 758, 124, 390, 964, 372, 1104, 576, 55, 751, 1026, 835, 479, 647, 598, 952, 45, 583, 18, 419, 687, 890, 1007, 1092, 697, 128, 240, 36, 24, 1052, 655, 648, 176, 192, 509, 309, 115, 634, 365, 887, 428, 348, 1205, 519, 913, 943, 769, 607, 774, 292, 1099, 990, 659, 847, 363, 760, 206, 1004, 1096, 782, 1017, 694, 560, 574, 586, 164, 14, 438, 337, 846, 186, 898, 1038, 219, 1014, 1027, 1016, 665, 103, 50, 172, 204, 477, 998, 296, 1117, 54, 744, 391, 44, 622, 481, 866, 670, 257, 1174, 915, 248, 108, 508, 591, 620, 878, 1209, 1020, 545, 1124, 946, 319, 346, 98, 930, 593, 767, 978, 84, 788, 111, 155, 503, 1177, 606, 602, 516, 12, 417, 660, 48, 1121, 15, 1066, 870, 94, 711, 261, 410, 480, 959, 1118, 1214, 32, 171, 938, 699, 340, 971, 533, 1107, 1144, 967, 1102, 1045, 937, 907, 929, 283, 761, 462, 681, 1119, 873, 539, 166, 945, 7, 275, 1077, 398, 336, 271, 181, 69, 436, 1010, 914, 734, 547, 137, 765, 1160, 216, 1035, 709, 849, 123, 615, 190, 649, 1190, 368, 816, 731, 234, 361, 499, 330, 982, 727, 198, 969, 483, 1182, 422, 209, 324, 683, 59, 1187, 1148, 800, 577, 824, 537, 150, 737, 526, 650, 785, 722, 388, 995, 611, 374, 624, 1101, 664, 712, 227, 1064, 490, 1172, 999, 983, 823, 47, 738, 379, 272, 572, 52, 633, 280, 178, 601, 1030, 175, 1149, 919, 376, 630, 437, 334, 242, 902, 42, 269, 295, 855, 138, 246, 49, 1135, 984, 826, 194, 299, 627, 933, 876, 1115, 1009, 975, 316, 868, 306, 794, 729, 1146, 1196, 431, 1071, 105, 528, 230, 565, 961, 532, 225, 950, 86, 524, 498, 394, 571, 714, 879, 637, 534, 113, 329, 702, 518, 783, 567, 434, 621, 327, 484, 871, 776, 392, 923, 502, 770, 1136, 1085, 807, 486, 1074, 1185, 66, 371, 940, 404, 35, 705, 405, 478, 515, 897, 1113, 385, 909, 1008, 359, 792, 1042, 793, 218, 1090, 1111, 527, 834, 426, 1199, 1181, 808, 513, 1, 1088, 831, 1154, 1195, 92, 917, 1001, 1100, 1002, 455, 270, 281, 341, 2, 440, 1021, 706, 599, 326, 76, 265, 658, 278, 29, 185, 865, 364, 1198, 304, 1089, 1080, 262, 725, 1125, 1028, 941, 1015, 523, 775, 80, 1040, 968, 1208, 189, 796, 605, 215, 1067, 1116, 733, 135, 862, 424, 987, 101, 953, 935, 403, 328, 1152, 863, 771, 46, 177, 343, 491, 195, 814, 133, 1197, 1167, 357, 126, 260, 413, 857, 795, 525, 588, 396, 168, 68, 1150, 550, 612, 1023, 414, 752, 1165, 1063, 19, 556, 256, 196, 141, 726, 93, 1049, 955, 494, 832, 323, 597, 535, 747, 464, 142, 482, 717, 507, 1097, 749, 970, 746, 872, 16, 908, 836, 344, 239, 754, 695, 104, 686, 208, 1078, 912, 638, 220, 291, 880, 421, 818, 1114, 860, 787, 423, 1057, 581, 817, 207, 692, 187, 231, 884, 1083, 317, 693, 342, 1094, 833, 244, 335, 644, 976, 1153, 221, 38, 559, 639, 1131, 367, 143, 407, 888, 1055, 1043, 339, 487, 668, 1179, 544, 1169, 285, 531, 254, 958, 73, 718, 89, 810, 102, 88, 167, 493, 584, 1032, 755, 362, 645, 504, 962, 27, 750, 1053, 64, 954, 70, 899, 614, 4, 10, 629, 815, 1068, 1082, 675, 691, 821, 1018, 71, 472, 446, 803, 514, 474, 506, 640, 400, 608, 708, 466, 1109, 801, 766, 723, 1202, 233, 97, 844, 842, 994, 314, 131, 918, 63, 724, 62, 939, 6, 205, 854, 809, 845, 320, 303, 79, 393, 1204, 1046, 23, 459, 457, 1011, 839, 409, 661, 522, 957, 901, 741, 790, 677, 465, 1112, 1086, 791, 1186, 881, 813, 251, 130, 928, 151, 819, 748, 701, 191, 1133, 666, 1122, 1210, 672, 61, 184, 1166, 1120, 1058, 214, 210, 1188, 777, 685, 848, 616, 1095, 963, 956, 716, 37, 551, 805, 1183, 287, 226, 1147, 77, 279, 669, 864, 641, 739, 310, 26, 900, 153, 58, 542, 119, 1151, 759, 886, 1180, 1156, 996, 924, 592, 703, 60, 236, 653, 1157, 728, 429, 397, 1155, 5, 165, 745, 450, 548, 851, 495, 1033, 179, 229, 671, 1200, 1123]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3496520609485006
the save name prefix for this run is:  chkpt-ID_3496520609485006_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 352
rank avg (pred): 0.445 +- 0.005
mrr vals (pred, true): 0.001, 0.212
batch losses (mrrl, rdl): 0.0, 0.0031305773

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.052 +- 0.039
mrr vals (pred, true): 0.195, 0.277
batch losses (mrrl, rdl): 0.0, 4.8183e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.303 +- 0.231
mrr vals (pred, true): 0.139, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004539001

Epoch over!
epoch time: 12.089

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 917
rank avg (pred): 0.516 +- 0.391
mrr vals (pred, true): 0.178, 0.001
batch losses (mrrl, rdl): 0.0, 0.000185369

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 209
rank avg (pred): 0.264 +- 0.217
mrr vals (pred, true): 0.245, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007359674

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1160
rank avg (pred): 0.173 +- 0.149
mrr vals (pred, true): 0.307, 0.052
batch losses (mrrl, rdl): 0.0, 1.02808e-05

Epoch over!
epoch time: 11.909

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 478
rank avg (pred): 0.253 +- 0.206
mrr vals (pred, true): 0.205, 0.005
batch losses (mrrl, rdl): 0.0, 0.0008172063

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 481
rank avg (pred): 0.244 +- 0.209
mrr vals (pred, true): 0.238, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009146139

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 234
rank avg (pred): 0.253 +- 0.220
mrr vals (pred, true): 0.291, 0.004
batch losses (mrrl, rdl): 0.0, 0.0008789428

Epoch over!
epoch time: 11.694

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 544
rank avg (pred): 0.230 +- 0.193
mrr vals (pred, true): 0.241, 0.047
batch losses (mrrl, rdl): 0.0, 1.59884e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 202
rank avg (pred): 0.245 +- 0.220
mrr vals (pred, true): 0.292, 0.005
batch losses (mrrl, rdl): 0.0, 0.0008297233

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 600
rank avg (pred): 0.341 +- 0.265
mrr vals (pred, true): 0.142, 0.124
batch losses (mrrl, rdl): 0.0, 0.0006283383

Epoch over!
epoch time: 12.028

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 634
rank avg (pred): 0.271 +- 0.251
mrr vals (pred, true): 0.308, 0.161
batch losses (mrrl, rdl): 0.0, 0.0003311838

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.171 +- 0.148
mrr vals (pred, true): 0.199, 0.174
batch losses (mrrl, rdl): 0.0, 0.0003992456

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 531
rank avg (pred): 0.221 +- 0.201
mrr vals (pred, true): 0.260, 0.035
batch losses (mrrl, rdl): 0.0, 7.1992e-06

Epoch over!
epoch time: 11.887

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 723
rank avg (pred): 0.326 +- 0.276
mrr vals (pred, true): 0.235, 0.005
batch losses (mrrl, rdl): 0.3414422274, 0.0002374513

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 464
rank avg (pred): 0.312 +- 0.205
mrr vals (pred, true): 0.145, 0.004
batch losses (mrrl, rdl): 0.0897749811, 0.0004820568

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 976
rank avg (pred): 0.020 +- 0.013
mrr vals (pred, true): 0.225, 0.302
batch losses (mrrl, rdl): 0.059307497, 3.8408e-06

Epoch over!
epoch time: 12.23

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 252
rank avg (pred): 0.019 +- 0.013
mrr vals (pred, true): 0.237, 0.234
batch losses (mrrl, rdl): 0.000132499, 1.86667e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 104
rank avg (pred): 0.317 +- 0.199
mrr vals (pred, true): 0.150, 0.193
batch losses (mrrl, rdl): 0.0183350332, 0.0016605405

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.316 +- 0.194
mrr vals (pred, true): 0.124, 0.004
batch losses (mrrl, rdl): 0.0549267232, 0.0004460114

Epoch over!
epoch time: 11.958

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 21
rank avg (pred): 0.016 +- 0.011
mrr vals (pred, true): 0.256, 0.248
batch losses (mrrl, rdl): 0.000640656, 7.7608e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 957
rank avg (pred): 0.646 +- 0.214
mrr vals (pred, true): 0.036, 0.004
batch losses (mrrl, rdl): 0.0018336186, 0.0005181921

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.314 +- 0.121
mrr vals (pred, true): 0.063, 0.004
batch losses (mrrl, rdl): 0.0016050641, 0.0005295736

Epoch over!
epoch time: 12.013

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.012 +- 0.008
mrr vals (pred, true): 0.242, 0.218
batch losses (mrrl, rdl): 0.0061660139, 4.29798e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 653
rank avg (pred): 0.304 +- 0.152
mrr vals (pred, true): 0.113, 0.006
batch losses (mrrl, rdl): 0.0390826017, 0.0005503237

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 841
rank avg (pred): 0.409 +- 0.219
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 4.25613e-05, 0.000166254

Epoch over!
epoch time: 12.076

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1
rank avg (pred): 0.024 +- 0.031
mrr vals (pred, true): 0.258, 0.231
batch losses (mrrl, rdl): 0.00711407, 3.8446e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 921
rank avg (pred): 0.556 +- 0.280
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.0037779401, 0.0010874372

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1147
rank avg (pred): 0.300 +- 0.216
mrr vals (pred, true): 0.078, 0.048
batch losses (mrrl, rdl): 0.0077295126, 0.0003484809

Epoch over!
epoch time: 12.204

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1197
rank avg (pred): 0.274 +- 0.143
mrr vals (pred, true): 0.106, 0.004
batch losses (mrrl, rdl): 0.0311513618, 0.0008455674

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 317
rank avg (pred): 0.109 +- 0.126
mrr vals (pred, true): 0.229, 0.283
batch losses (mrrl, rdl): 0.0285437368, 0.0001229002

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 113
rank avg (pred): 0.236 +- 0.142
mrr vals (pred, true): 0.126, 0.192
batch losses (mrrl, rdl): 0.0446165428, 0.0007063667

Epoch over!
epoch time: 12.23

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 745
rank avg (pred): 0.138 +- 0.100
mrr vals (pred, true): 0.145, 0.142
batch losses (mrrl, rdl): 0.0001084645, 0.0001829135

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 982
rank avg (pred): 0.130 +- 0.113
mrr vals (pred, true): 0.238, 0.196
batch losses (mrrl, rdl): 0.0176022276, 0.0001712874

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 939
rank avg (pred): 0.506 +- 0.386
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0010038306, 0.0014572542

Epoch over!
epoch time: 12.122

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 124
rank avg (pred): 0.232 +- 0.147
mrr vals (pred, true): 0.146, 0.206
batch losses (mrrl, rdl): 0.0359361395, 0.0007461984

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.291 +- 0.306
mrr vals (pred, true): 0.093, 0.043
batch losses (mrrl, rdl): 0.0187161807, 8.5779e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1131
rank avg (pred): 0.224 +- 0.141
mrr vals (pred, true): 0.143, 0.004
batch losses (mrrl, rdl): 0.087413311, 0.0012771602

Epoch over!
epoch time: 12.192

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.032 +- 0.032
mrr vals (pred, true): 0.296, 0.259
batch losses (mrrl, rdl): 0.0142927598, 5.9893e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 823
rank avg (pred): 0.115 +- 0.080
mrr vals (pred, true): 0.237, 0.237
batch losses (mrrl, rdl): 8.193e-07, 0.0001153062

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1203
rank avg (pred): 0.305 +- 0.162
mrr vals (pred, true): 0.110, 0.005
batch losses (mrrl, rdl): 0.036283996, 0.0005662434

Epoch over!
epoch time: 12.154

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.390 +- 0.219
mrr vals (pred, true): 0.062, 0.004
batch losses (mrrl, rdl): 0.0015470391, 0.0001694826

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.073 +- 0.052
mrr vals (pred, true): 0.264, 0.275
batch losses (mrrl, rdl): 0.0011801979, 2.18524e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 263
rank avg (pred): 0.040 +- 0.030
mrr vals (pred, true): 0.264, 0.224
batch losses (mrrl, rdl): 0.0165940505, 9.716e-07

Epoch over!
epoch time: 12.145

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.390 +- 0.283
mrr vals (pred, true): 0.074, 0.005

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04914 	 0.00076 	 m..s
   23 	     1 	 0.10595 	 0.00104 	 MISS
    2 	     2 	 0.05473 	 0.00121 	 m..s
   74 	     3 	 0.15795 	 0.00323 	 MISS
   78 	     4 	 0.15824 	 0.00326 	 MISS
   62 	     5 	 0.14885 	 0.00343 	 MISS
   85 	     6 	 0.16172 	 0.00354 	 MISS
   62 	     7 	 0.14885 	 0.00357 	 MISS
   37 	     8 	 0.12267 	 0.00357 	 MISS
   26 	     9 	 0.11249 	 0.00363 	 MISS
   71 	    10 	 0.15466 	 0.00371 	 MISS
   74 	    11 	 0.15795 	 0.00372 	 MISS
   38 	    12 	 0.12450 	 0.00382 	 MISS
   22 	    13 	 0.10271 	 0.00387 	 m..s
    9 	    14 	 0.06158 	 0.00391 	 m..s
   34 	    15 	 0.11965 	 0.00396 	 MISS
    0 	    16 	 0.04653 	 0.00403 	 m..s
   76 	    17 	 0.15810 	 0.00411 	 MISS
   44 	    18 	 0.13414 	 0.00412 	 MISS
   69 	    19 	 0.15265 	 0.00413 	 MISS
   49 	    20 	 0.13800 	 0.00413 	 MISS
   30 	    21 	 0.11404 	 0.00428 	 MISS
   57 	    22 	 0.14008 	 0.00442 	 MISS
   73 	    23 	 0.15732 	 0.00442 	 MISS
   84 	    24 	 0.16135 	 0.00443 	 MISS
   46 	    25 	 0.13571 	 0.00446 	 MISS
   61 	    26 	 0.14266 	 0.00455 	 MISS
   42 	    27 	 0.13182 	 0.00456 	 MISS
   10 	    28 	 0.07353 	 0.00460 	 m..s
   18 	    29 	 0.09467 	 0.00466 	 m..s
   43 	    30 	 0.13268 	 0.00471 	 MISS
   47 	    31 	 0.13572 	 0.00485 	 MISS
   33 	    32 	 0.11792 	 0.00485 	 MISS
   68 	    33 	 0.15190 	 0.00511 	 MISS
   79 	    34 	 0.15887 	 0.00587 	 MISS
    7 	    35 	 0.06038 	 0.00616 	 m..s
    5 	    36 	 0.05774 	 0.01792 	 m..s
    8 	    37 	 0.06056 	 0.02089 	 m..s
    6 	    38 	 0.06034 	 0.02583 	 m..s
   13 	    39 	 0.07473 	 0.02871 	 m..s
    3 	    40 	 0.05628 	 0.03331 	 ~...
    3 	    41 	 0.05628 	 0.03453 	 ~...
   12 	    42 	 0.07404 	 0.03906 	 m..s
   13 	    43 	 0.07473 	 0.05100 	 ~...
   11 	    44 	 0.07358 	 0.05414 	 ~...
   15 	    45 	 0.08799 	 0.06592 	 ~...
   23 	    46 	 0.10595 	 0.06720 	 m..s
   19 	    47 	 0.09644 	 0.09914 	 ~...
   20 	    48 	 0.09675 	 0.10487 	 ~...
   16 	    49 	 0.09020 	 0.10602 	 ~...
   21 	    50 	 0.10121 	 0.10910 	 ~...
   92 	    51 	 0.16691 	 0.14258 	 ~...
   35 	    52 	 0.11983 	 0.14437 	 ~...
   93 	    53 	 0.18559 	 0.14760 	 m..s
   91 	    54 	 0.16656 	 0.14898 	 ~...
   27 	    55 	 0.11252 	 0.14941 	 m..s
   17 	    56 	 0.09440 	 0.15042 	 m..s
   64 	    57 	 0.15005 	 0.15292 	 ~...
   25 	    58 	 0.11216 	 0.15346 	 m..s
   32 	    59 	 0.11443 	 0.15568 	 m..s
   29 	    60 	 0.11282 	 0.15738 	 m..s
   31 	    61 	 0.11429 	 0.15958 	 m..s
   35 	    62 	 0.11983 	 0.16331 	 m..s
   28 	    63 	 0.11266 	 0.16450 	 m..s
   94 	    64 	 0.19436 	 0.16556 	 ~...
   66 	    65 	 0.15024 	 0.16565 	 ~...
   65 	    66 	 0.15015 	 0.16605 	 ~...
   90 	    67 	 0.16645 	 0.16862 	 ~...
   54 	    68 	 0.13889 	 0.18342 	 m..s
   60 	    69 	 0.14198 	 0.18639 	 m..s
   50 	    70 	 0.13821 	 0.18699 	 m..s
   96 	    71 	 0.24392 	 0.18733 	 m..s
   48 	    72 	 0.13591 	 0.19254 	 m..s
   98 	    73 	 0.24494 	 0.19762 	 m..s
   87 	    74 	 0.16233 	 0.20045 	 m..s
   50 	    75 	 0.13821 	 0.20140 	 m..s
   58 	    76 	 0.14128 	 0.20235 	 m..s
   59 	    77 	 0.14160 	 0.20437 	 m..s
   95 	    78 	 0.24243 	 0.20499 	 m..s
   88 	    79 	 0.16262 	 0.21092 	 m..s
   40 	    80 	 0.13158 	 0.21117 	 m..s
   52 	    81 	 0.13881 	 0.21238 	 m..s
   55 	    82 	 0.13912 	 0.21386 	 m..s
   86 	    83 	 0.16198 	 0.21545 	 m..s
   77 	    84 	 0.15822 	 0.21594 	 m..s
  111 	    85 	 0.26557 	 0.21730 	 m..s
  105 	    86 	 0.26344 	 0.21764 	 m..s
   39 	    87 	 0.12511 	 0.21766 	 m..s
  120 	    88 	 0.28776 	 0.21787 	 m..s
   52 	    89 	 0.13881 	 0.21877 	 m..s
   72 	    90 	 0.15583 	 0.21969 	 m..s
  114 	    91 	 0.26717 	 0.21997 	 m..s
   55 	    92 	 0.13912 	 0.22038 	 m..s
  104 	    93 	 0.26226 	 0.22122 	 m..s
   89 	    94 	 0.16290 	 0.22209 	 m..s
   97 	    95 	 0.24458 	 0.22232 	 ~...
  105 	    96 	 0.26344 	 0.22334 	 m..s
   40 	    97 	 0.13158 	 0.22501 	 m..s
   70 	    98 	 0.15352 	 0.22511 	 m..s
  112 	    99 	 0.26669 	 0.22688 	 m..s
  108 	   100 	 0.26408 	 0.22806 	 m..s
   80 	   101 	 0.15947 	 0.22910 	 m..s
   81 	   102 	 0.15976 	 0.23007 	 m..s
   45 	   103 	 0.13416 	 0.23101 	 m..s
   83 	   104 	 0.16055 	 0.23667 	 m..s
   67 	   105 	 0.15135 	 0.23779 	 m..s
  101 	   106 	 0.25892 	 0.23940 	 ~...
  101 	   107 	 0.25892 	 0.24099 	 ~...
   99 	   108 	 0.25523 	 0.24123 	 ~...
   81 	   109 	 0.15976 	 0.24220 	 m..s
   99 	   110 	 0.25523 	 0.24343 	 ~...
  103 	   111 	 0.25962 	 0.24858 	 ~...
  109 	   112 	 0.26409 	 0.25786 	 ~...
  107 	   113 	 0.26347 	 0.25996 	 ~...
  109 	   114 	 0.26409 	 0.27612 	 ~...
  113 	   115 	 0.26695 	 0.29784 	 m..s
  115 	   116 	 0.27361 	 0.29964 	 ~...
  116 	   117 	 0.27688 	 0.30466 	 ~...
  118 	   118 	 0.28388 	 0.31364 	 ~...
  116 	   119 	 0.27688 	 0.31510 	 m..s
  119 	   120 	 0.28758 	 0.32036 	 m..s
==========================================
r_mrr = 0.6796684861183167
r2_mrr = 0.40437114238739014
spearmanr_mrr@5 = 0.9514037370681763
spearmanr_mrr@10 = 0.9361338019371033
spearmanr_mrr@50 = 0.7795985341072083
spearmanr_mrr@100 = 0.8179503679275513
spearmanr_mrr@All = 0.8752306699752808
==========================================
test time: 0.391
Done Testing dataset CoDExSmall
total time taken: 191.27840113639832
training time taken: 181.39727592468262
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6797)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4044)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9514)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9361)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.7796)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8180)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8752)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.428452170614037}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 2420341342983926
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [679, 990, 369, 661, 553, 1146, 84, 284, 904, 747, 780, 460, 738, 26, 809, 230, 576, 354, 70, 366, 228, 1148, 482, 919, 439, 17, 473, 501, 942, 424, 529, 1018, 465, 160, 364, 1073, 438, 595, 155, 36, 678, 640, 630, 244, 798, 719, 98, 1103, 820, 539, 892, 340, 1039, 64, 229, 750, 390, 1205, 289, 1017, 1186, 884, 819, 388, 619, 607, 523, 475, 412, 433, 227, 664, 181, 535, 158, 1206, 561, 857, 73, 916, 178, 177, 66, 1072, 562, 866, 319, 1139, 832, 927, 1036, 614, 579, 941, 736, 908, 347, 620, 20, 715, 874, 1150, 127, 945, 1050, 195, 437, 694, 1159, 1091, 700, 1200, 257, 786, 1001, 954, 647, 291, 1154, 662, 898]
valid_ids (0): []
train_ids (1094): [2, 889, 274, 1211, 546, 888, 45, 303, 756, 410, 506, 573, 342, 753, 850, 211, 235, 329, 324, 1048, 762, 8, 140, 758, 191, 1096, 174, 748, 1006, 415, 852, 43, 102, 687, 1095, 720, 1214, 1174, 639, 253, 1077, 1011, 914, 1057, 759, 844, 1162, 1031, 1142, 681, 30, 906, 938, 723, 24, 785, 1136, 494, 571, 1016, 853, 701, 418, 318, 478, 778, 1168, 7, 769, 655, 180, 1010, 430, 245, 1172, 481, 685, 445, 559, 240, 721, 332, 400, 466, 952, 810, 830, 1093, 575, 602, 1165, 773, 962, 497, 770, 987, 746, 622, 48, 802, 527, 1191, 569, 761, 991, 827, 627, 254, 1177, 733, 507, 674, 1185, 368, 1104, 22, 122, 725, 1198, 760, 335, 545, 842, 416, 57, 1175, 406, 200, 1082, 262, 1084, 428, 273, 1098, 534, 966, 757, 735, 979, 896, 429, 551, 833, 40, 683, 548, 493, 584, 514, 741, 372, 714, 610, 643, 359, 212, 525, 1028, 978, 1060, 395, 787, 293, 1021, 246, 305, 742, 768, 170, 310, 1130, 440, 772, 444, 54, 1117, 864, 880, 811, 1065, 1047, 463, 642, 729, 184, 104, 456, 164, 280, 1079, 782, 248, 986, 1080, 1081, 617, 210, 929, 153, 91, 1045, 1128, 116, 1101, 763, 500, 988, 146, 74, 901, 88, 1030, 172, 931, 960, 333, 1090, 670, 969, 509, 25, 206, 781, 1199, 1135, 1023, 666, 32, 1094, 358, 1149, 265, 251, 441, 696, 790, 499, 949, 590, 600, 895, 570, 1169, 963, 824, 147, 636, 189, 691, 1078, 225, 276, 847, 815, 800, 615, 414, 812, 886, 572, 76, 495, 933, 68, 1127, 789, 498, 910, 754, 1153, 130, 269, 951, 1112, 1055, 258, 699, 1062, 612, 1190, 867, 281, 185, 11, 726, 169, 1075, 709, 261, 677, 50, 1015, 637, 110, 621, 817, 651, 601, 337, 652, 260, 672, 1120, 134, 1115, 932, 14, 542, 379, 151, 512, 123, 879, 85, 1170, 698, 1070, 788, 654, 835, 86, 304, 256, 975, 377, 1167, 518, 1107, 111, 634, 183, 771, 394, 710, 680, 313, 1033, 435, 79, 314, 344, 957, 426, 224, 1106, 1187, 31, 152, 108, 290, 90, 1194, 922, 327, 878, 505, 541, 1020, 234, 799, 450, 718, 480, 249, 1183, 80, 479, 974, 154, 1197, 1076, 383, 939, 851, 336, 722, 21, 131, 427, 792, 1019, 618, 166, 490, 1192, 483, 97, 774, 531, 401, 425, 357, 51, 209, 893, 259, 1181, 971, 688, 900, 202, 348, 1066, 1176, 46, 156, 215, 955, 667, 606, 1126, 1184, 221, 326, 382, 536, 204, 716, 1042, 1195, 83, 538, 398, 1013, 352, 838, 345, 924, 232, 380, 496, 1118, 1067, 845, 145, 135, 568, 766, 1124, 320, 1151, 608, 707, 386, 555, 1038, 985, 243, 1007, 277, 485, 133, 403, 476, 1009, 669, 373, 176, 1171, 409, 1051, 854, 549, 188, 1212, 1125, 744, 378, 141, 157, 190, 411, 1123, 375, 219, 961, 948, 998, 532, 72, 58, 457, 795, 580, 885, 784, 1049, 1041, 983, 624, 99, 384, 605, 75, 791, 558, 508, 165, 1032, 641, 448, 503, 374, 724, 905, 77, 980, 93, 663, 537, 404, 585, 238, 587, 162, 935, 1110, 717, 711, 182, 764, 12, 826, 692, 462, 848, 192, 873, 899, 363, 511, 139, 890, 689, 3, 422, 861, 1012, 806, 338, 739, 740, 37, 656, 446, 413, 323, 168, 351, 282, 1178, 855, 1054, 252, 1140, 526, 61, 1005, 126, 632, 604, 519, 350, 515, 1014, 603, 339, 517, 442, 1069, 81, 330, 150, 793, 1022, 920, 592, 921, 676, 103, 1152, 33, 829, 510, 504, 649, 78, 263, 207, 272, 217, 704, 436, 860, 1074, 299, 897, 596, 28, 887, 241, 682, 846, 658, 408, 1046, 447, 836, 956, 566, 593, 599, 469, 684, 730, 994, 316, 312, 965, 950, 63, 356, 5, 1003, 1133, 461, 1026, 959, 560, 1143, 449, 114, 19, 106, 367, 278, 918, 745, 397, 849, 1111, 673, 911, 236, 547, 1088, 361, 1155, 301, 112, 453, 1008, 266, 831, 113, 1083, 226, 977, 638, 218, 891, 626, 10, 591, 633, 1043, 1132, 311, 371, 894, 296, 1207, 431, 976, 1040, 13, 1037, 743, 841, 92, 69, 1109, 513, 34, 734, 247, 1059, 993, 392, 953, 283, 279, 486, 1163, 47, 816, 389, 87, 923, 35, 59, 370, 958, 1138, 402, 109, 628, 242, 376, 1134, 455, 1092, 161, 1102, 1158, 477, 1114, 65, 623, 233, 1085, 946, 871, 173, 343, 583, 671, 997, 749, 727, 15, 999, 365, 285, 1161, 9, 1052, 1208, 464, 972, 752, 470, 645, 1156, 533, 44, 930, 95, 737, 903, 391, 292, 875, 1193, 1, 1204, 52, 399, 909, 947, 657, 62, 1210, 474, 865, 297, 82, 144, 563, 487, 616, 41, 859, 1116, 697, 804, 288, 1027, 1113, 915, 71, 137, 564, 100, 237, 712, 471, 163, 231, 1119, 668, 353, 936, 708, 271, 216, 315, 765, 1145, 635, 322, 355, 360, 222, 925, 205, 818, 713, 796, 143, 825, 167, 629, 834, 94, 298, 1100, 89, 1188, 443, 201, 877, 522, 1201, 186, 872, 268, 926, 928, 1108, 912, 937, 4, 843, 705, 943, 1180, 970, 828, 732, 863, 693, 862, 597, 837, 489, 107, 49, 1000, 1196, 124, 574, 451, 528, 286, 115, 60, 581, 554, 309, 213, 128, 1122, 1173, 989, 16, 840, 1129, 516, 1087, 609, 302, 194, 492, 653, 944, 706, 675, 992, 940, 644, 776, 856, 1157, 419, 432, 1137, 660, 421, 270, 405, 813, 565, 755, 808, 917, 396, 148, 582, 648, 393, 530, 119, 779, 349, 613, 488, 196, 1179, 540, 132, 556, 594, 794, 346, 598, 381, 870, 702, 120, 807, 821, 417, 6, 686, 220, 27, 387, 214, 868, 287, 611, 179, 967, 934, 858, 331, 902, 588, 1189, 423, 193, 458, 117, 1035, 39, 767, 1029, 1097, 1024, 552, 121, 472, 0, 665, 198, 1121, 484, 996, 783, 175, 334, 1002, 23, 881, 321, 29, 1147, 275, 454, 203, 308, 1166, 521, 984, 1203, 1064, 328, 728, 325, 1141, 1089, 317, 208, 307, 982, 159, 1044, 839, 913, 199, 968, 407, 907, 101, 125, 1144, 1131, 187, 543, 520, 797, 129, 149, 544, 524, 577, 1058, 300, 171, 142, 589, 197, 650, 659, 690, 468, 306, 239, 491, 1099, 459, 995, 695, 805, 105, 1071, 646, 751, 56, 1004, 42, 586, 1025, 731, 67, 775, 38, 703, 1034, 420, 250, 964, 876, 1182, 294, 801, 822, 1068, 1209, 578, 467, 557, 18, 452, 1160, 823, 803, 118, 981, 1086, 362, 567, 96, 814, 53, 1105, 264, 136, 1056, 55, 502, 1053, 973, 295, 434, 267, 625, 1202, 255, 550, 1213, 1061, 1063, 777, 882, 385, 223, 341, 1164, 138, 631, 883, 869]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5994404721095290
the save name prefix for this run is:  chkpt-ID_5994404721095290_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 44
rank avg (pred): 0.520 +- 0.012
mrr vals (pred, true): 0.001, 0.212
batch losses (mrrl, rdl): 0.0, 0.0047679208

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.359 +- 0.275
mrr vals (pred, true): 0.091, 0.039
batch losses (mrrl, rdl): 0.0, 1.98163e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 316
rank avg (pred): 0.046 +- 0.040
mrr vals (pred, true): 0.331, 0.274
batch losses (mrrl, rdl): 0.0, 1.771e-06

Epoch over!
epoch time: 12.061

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.313 +- 0.256
mrr vals (pred, true): 0.236, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003586002

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 601
rank avg (pred): 0.328 +- 0.264
mrr vals (pred, true): 0.182, 0.138
batch losses (mrrl, rdl): 0.0, 0.0005723221

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 716
rank avg (pred): 0.297 +- 0.262
mrr vals (pred, true): 0.298, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003472544

Epoch over!
epoch time: 12.041

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.032 +- 0.032
mrr vals (pred, true): 0.405, 0.259
batch losses (mrrl, rdl): 0.0, 4.8637e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 862
rank avg (pred): 0.419 +- 0.306
mrr vals (pred, true): 0.083, 0.037
batch losses (mrrl, rdl): 0.0, 2.63458e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1089
rank avg (pred): 0.243 +- 0.244
mrr vals (pred, true): 0.367, 0.201
batch losses (mrrl, rdl): 0.0, 0.0009068543

Epoch over!
epoch time: 11.873

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1111
rank avg (pred): 0.260 +- 0.237
mrr vals (pred, true): 0.251, 0.005
batch losses (mrrl, rdl): 0.0, 0.0007554633

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.221 +- 0.231
mrr vals (pred, true): 0.320, 0.209
batch losses (mrrl, rdl): 0.0, 0.0007253651

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 961
rank avg (pred): 0.635 +- 0.406
mrr vals (pred, true): 0.083, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006693421

Epoch over!
epoch time: 11.976

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 269
rank avg (pred): 0.020 +- 0.023
mrr vals (pred, true): 0.472, 0.218
batch losses (mrrl, rdl): 0.0, 1.59613e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1081
rank avg (pred): 0.239 +- 0.240
mrr vals (pred, true): 0.350, 0.174
batch losses (mrrl, rdl): 0.0, 0.0006696286

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 706
rank avg (pred): 0.270 +- 0.266
mrr vals (pred, true): 0.353, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005648978

Epoch over!
epoch time: 11.86

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 48
rank avg (pred): 0.064 +- 0.068
mrr vals (pred, true): 0.398, 0.229
batch losses (mrrl, rdl): 0.2874284685, 6.4249e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.140 +- 0.096
mrr vals (pred, true): 0.160, 0.141
batch losses (mrrl, rdl): 0.0036491058, 0.0001679381

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 41
rank avg (pred): 0.068 +- 0.045
mrr vals (pred, true): 0.183, 0.207
batch losses (mrrl, rdl): 0.005612338, 9.7162e-06

Epoch over!
epoch time: 12.482

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 410
rank avg (pred): 0.359 +- 0.188
mrr vals (pred, true): 0.120, 0.003
batch losses (mrrl, rdl): 0.048619695, 0.0003038588

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 341
rank avg (pred): 0.357 +- 0.178
mrr vals (pred, true): 0.107, 0.185
batch losses (mrrl, rdl): 0.061166998, 0.0020482349

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.303 +- 0.170
mrr vals (pred, true): 0.127, 0.178
batch losses (mrrl, rdl): 0.0250534154, 0.0006654887

Epoch over!
epoch time: 12.055

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1080
rank avg (pred): 0.350 +- 0.174
mrr vals (pred, true): 0.105, 0.186
batch losses (mrrl, rdl): 0.0648230463, 0.0017827866

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 546
rank avg (pred): 0.367 +- 0.149
mrr vals (pred, true): 0.083, 0.040
batch losses (mrrl, rdl): 0.0111451829, 0.0004858646

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.318 +- 0.179
mrr vals (pred, true): 0.119, 0.209
batch losses (mrrl, rdl): 0.0818617195, 0.0015563446

Epoch over!
epoch time: 12.12

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 193
rank avg (pred): 0.323 +- 0.175
mrr vals (pred, true): 0.111, 0.004
batch losses (mrrl, rdl): 0.0370720997, 0.0004363237

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.364 +- 0.129
mrr vals (pred, true): 0.069, 0.075
batch losses (mrrl, rdl): 0.0037594168, 0.0001683918

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1094
rank avg (pred): 0.287 +- 0.173
mrr vals (pred, true): 0.156, 0.203
batch losses (mrrl, rdl): 0.0217012279, 0.0012537766

Epoch over!
epoch time: 12.195

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.011 +- 0.007
mrr vals (pred, true): 0.290, 0.299
batch losses (mrrl, rdl): 0.0007816658, 1.9472e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1172
rank avg (pred): 0.341 +- 0.123
mrr vals (pred, true): 0.071, 0.143
batch losses (mrrl, rdl): 0.0517576411, 0.0005706587

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 654
rank avg (pred): 0.327 +- 0.136
mrr vals (pred, true): 0.078, 0.004
batch losses (mrrl, rdl): 0.00776419, 0.0005145771

Epoch over!
epoch time: 12.086

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 69
rank avg (pred): 0.092 +- 0.063
mrr vals (pred, true): 0.207, 0.211
batch losses (mrrl, rdl): 0.0001963213, 3.67307e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 769
rank avg (pred): 0.529 +- 0.192
mrr vals (pred, true): 0.060, 0.043
batch losses (mrrl, rdl): 0.0010330917, 0.0003187634

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 705
rank avg (pred): 0.329 +- 0.123
mrr vals (pred, true): 0.070, 0.004
batch losses (mrrl, rdl): 0.0038532065, 0.0004573422

Epoch over!
epoch time: 12.135

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 542
rank avg (pred): 0.327 +- 0.125
mrr vals (pred, true): 0.071, 0.052
batch losses (mrrl, rdl): 0.004618302, 0.0002650164

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 657
rank avg (pred): 0.304 +- 0.132
mrr vals (pred, true): 0.094, 0.004
batch losses (mrrl, rdl): 0.0196775459, 0.0005701495

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 492
rank avg (pred): 0.211 +- 0.119
mrr vals (pred, true): 0.147, 0.166
batch losses (mrrl, rdl): 0.0035468512, 0.0001383009

Epoch over!
epoch time: 12.05

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 294
rank avg (pred): 0.036 +- 0.025
mrr vals (pred, true): 0.266, 0.304
batch losses (mrrl, rdl): 0.0143338228, 1.3754e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 466
rank avg (pred): 0.277 +- 0.151
mrr vals (pred, true): 0.127, 0.005
batch losses (mrrl, rdl): 0.059952978, 0.0007948277

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1211
rank avg (pred): 0.308 +- 0.121
mrr vals (pred, true): 0.083, 0.004
batch losses (mrrl, rdl): 0.0107986927, 0.0007102169

Epoch over!
epoch time: 12.171

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.231 +- 0.161
mrr vals (pred, true): 0.243, 0.223
batch losses (mrrl, rdl): 0.0040104361, 0.0008388127

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 250
rank avg (pred): 0.194 +- 0.131
mrr vals (pred, true): 0.226, 0.230
batch losses (mrrl, rdl): 9.87768e-05, 0.0004811475

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 485
rank avg (pred): 0.282 +- 0.139
mrr vals (pred, true): 0.113, 0.005
batch losses (mrrl, rdl): 0.0392357111, 0.00077646

Epoch over!
epoch time: 12.013

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 295
rank avg (pred): 0.054 +- 0.037
mrr vals (pred, true): 0.256, 0.299
batch losses (mrrl, rdl): 0.0178241991, 5.5578e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 142
rank avg (pred): 0.258 +- 0.143
mrr vals (pred, true): 0.150, 0.203
batch losses (mrrl, rdl): 0.0284689851, 0.0008878342

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 122
rank avg (pred): 0.266 +- 0.140
mrr vals (pred, true): 0.127, 0.199
batch losses (mrrl, rdl): 0.0513473004, 0.0010276249

Epoch over!
epoch time: 12.226

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.280 +- 0.127
mrr vals (pred, true): 0.105, 0.003

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04183 	 0.00073 	 m..s
    2 	     1 	 0.04356 	 0.00076 	 m..s
    3 	     2 	 0.04380 	 0.00076 	 m..s
   12 	     3 	 0.06552 	 0.00094 	 m..s
   27 	     4 	 0.09094 	 0.00104 	 m..s
   13 	     5 	 0.06640 	 0.00105 	 m..s
   11 	     6 	 0.06223 	 0.00107 	 m..s
   18 	     7 	 0.07450 	 0.00138 	 m..s
   17 	     8 	 0.07385 	 0.00173 	 m..s
   31 	     9 	 0.09767 	 0.00277 	 m..s
   40 	    10 	 0.10463 	 0.00280 	 MISS
    4 	    11 	 0.04822 	 0.00284 	 m..s
   74 	    12 	 0.12900 	 0.00311 	 MISS
   65 	    13 	 0.12769 	 0.00344 	 MISS
    1 	    14 	 0.04339 	 0.00351 	 m..s
   81 	    15 	 0.13061 	 0.00355 	 MISS
   65 	    16 	 0.12769 	 0.00355 	 MISS
   73 	    17 	 0.12893 	 0.00364 	 MISS
   71 	    18 	 0.12854 	 0.00375 	 MISS
   69 	    19 	 0.12805 	 0.00375 	 MISS
   55 	    20 	 0.11740 	 0.00385 	 MISS
   95 	    21 	 0.14443 	 0.00385 	 MISS
   31 	    22 	 0.09767 	 0.00399 	 m..s
   86 	    23 	 0.13187 	 0.00401 	 MISS
    9 	    24 	 0.06170 	 0.00404 	 m..s
   75 	    25 	 0.12930 	 0.00409 	 MISS
   29 	    26 	 0.09418 	 0.00411 	 m..s
   53 	    27 	 0.11258 	 0.00416 	 MISS
   91 	    28 	 0.13739 	 0.00419 	 MISS
   15 	    29 	 0.06763 	 0.00421 	 m..s
   62 	    30 	 0.12620 	 0.00422 	 MISS
   81 	    31 	 0.13061 	 0.00424 	 MISS
   48 	    32 	 0.10679 	 0.00425 	 MISS
    7 	    33 	 0.05485 	 0.00427 	 m..s
   39 	    34 	 0.10337 	 0.00434 	 m..s
   72 	    35 	 0.12861 	 0.00438 	 MISS
   14 	    36 	 0.06721 	 0.00441 	 m..s
    5 	    37 	 0.04844 	 0.00443 	 m..s
   40 	    38 	 0.10463 	 0.00445 	 MISS
   51 	    39 	 0.10817 	 0.00448 	 MISS
   63 	    40 	 0.12671 	 0.00459 	 MISS
   81 	    41 	 0.13061 	 0.00471 	 MISS
   89 	    42 	 0.13382 	 0.00474 	 MISS
   76 	    43 	 0.12934 	 0.00478 	 MISS
    8 	    44 	 0.06066 	 0.00498 	 m..s
   70 	    45 	 0.12839 	 0.00531 	 MISS
   63 	    46 	 0.12671 	 0.00532 	 MISS
   54 	    47 	 0.11275 	 0.00538 	 MISS
   43 	    48 	 0.10470 	 0.00543 	 m..s
   35 	    49 	 0.10120 	 0.00552 	 m..s
   60 	    50 	 0.12349 	 0.00637 	 MISS
    6 	    51 	 0.05400 	 0.00638 	 m..s
   10 	    52 	 0.06218 	 0.02583 	 m..s
   22 	    53 	 0.08257 	 0.03229 	 m..s
   23 	    54 	 0.08365 	 0.03260 	 m..s
   23 	    55 	 0.08365 	 0.03267 	 m..s
   20 	    56 	 0.08118 	 0.03727 	 m..s
   21 	    57 	 0.08139 	 0.03792 	 m..s
   25 	    58 	 0.08404 	 0.04130 	 m..s
   19 	    59 	 0.08002 	 0.04300 	 m..s
   30 	    60 	 0.09737 	 0.05290 	 m..s
   26 	    61 	 0.08924 	 0.05918 	 m..s
   28 	    62 	 0.09106 	 0.06077 	 m..s
   45 	    63 	 0.10478 	 0.06515 	 m..s
   42 	    64 	 0.10465 	 0.06592 	 m..s
   16 	    65 	 0.06781 	 0.07491 	 ~...
   34 	    66 	 0.09953 	 0.10502 	 ~...
   47 	    67 	 0.10564 	 0.13062 	 ~...
   96 	    68 	 0.15395 	 0.14547 	 ~...
   46 	    69 	 0.10557 	 0.14601 	 m..s
   49 	    70 	 0.10759 	 0.14938 	 m..s
   52 	    71 	 0.11192 	 0.15001 	 m..s
   33 	    72 	 0.09832 	 0.15121 	 m..s
  100 	    73 	 0.17085 	 0.15356 	 ~...
   44 	    74 	 0.10476 	 0.15393 	 m..s
   97 	    75 	 0.15973 	 0.15616 	 ~...
   50 	    76 	 0.10791 	 0.15696 	 m..s
   37 	    77 	 0.10312 	 0.16363 	 m..s
   98 	    78 	 0.16498 	 0.16583 	 ~...
   99 	    79 	 0.16577 	 0.16695 	 ~...
   36 	    80 	 0.10281 	 0.16989 	 m..s
   37 	    81 	 0.10312 	 0.17162 	 m..s
   59 	    82 	 0.12310 	 0.18588 	 m..s
   90 	    83 	 0.13557 	 0.18699 	 m..s
   61 	    84 	 0.12609 	 0.18944 	 m..s
   67 	    85 	 0.12771 	 0.19576 	 m..s
   68 	    86 	 0.12795 	 0.19709 	 m..s
   93 	    87 	 0.13838 	 0.19726 	 m..s
   58 	    88 	 0.12207 	 0.19766 	 m..s
   57 	    89 	 0.11950 	 0.20045 	 m..s
   87 	    90 	 0.13196 	 0.20310 	 m..s
  105 	    91 	 0.22917 	 0.20645 	 ~...
  101 	    92 	 0.20808 	 0.20874 	 ~...
   79 	    93 	 0.13041 	 0.20890 	 m..s
  102 	    94 	 0.21241 	 0.20901 	 ~...
  103 	    95 	 0.21386 	 0.21607 	 ~...
   56 	    96 	 0.11842 	 0.21770 	 m..s
  113 	    97 	 0.24667 	 0.21991 	 ~...
   92 	    98 	 0.13788 	 0.22038 	 m..s
  109 	    99 	 0.23588 	 0.22078 	 ~...
  104 	   100 	 0.21421 	 0.22109 	 ~...
  112 	   101 	 0.23972 	 0.22110 	 ~...
  116 	   102 	 0.24762 	 0.22199 	 ~...
  111 	   103 	 0.23865 	 0.22278 	 ~...
   85 	   104 	 0.13101 	 0.22466 	 m..s
   80 	   105 	 0.13045 	 0.22485 	 m..s
   88 	   106 	 0.13229 	 0.22570 	 m..s
   84 	   107 	 0.13073 	 0.23132 	 MISS
  117 	   108 	 0.25198 	 0.23316 	 ~...
  107 	   109 	 0.23413 	 0.23590 	 ~...
   77 	   110 	 0.12963 	 0.23677 	 MISS
   78 	   111 	 0.12983 	 0.23697 	 MISS
  114 	   112 	 0.24714 	 0.23940 	 ~...
   94 	   113 	 0.13998 	 0.24180 	 MISS
  110 	   114 	 0.23781 	 0.24182 	 ~...
  106 	   115 	 0.23227 	 0.24343 	 ~...
  108 	   116 	 0.23505 	 0.25263 	 ~...
  115 	   117 	 0.24720 	 0.27668 	 ~...
  118 	   118 	 0.27882 	 0.28464 	 ~...
  120 	   119 	 0.28136 	 0.29784 	 ~...
  119 	   120 	 0.27992 	 0.30418 	 ~...
==========================================
r_mrr = 0.7206791639328003
r2_mrr = 0.4138837456703186
spearmanr_mrr@5 = 0.8896629214286804
spearmanr_mrr@10 = 0.9415587186813354
spearmanr_mrr@50 = 0.8784684538841248
spearmanr_mrr@100 = 0.8285483717918396
spearmanr_mrr@All = 0.8617717623710632
==========================================
test time: 0.584
Done Testing dataset CoDExSmall
total time taken: 191.93980741500854
training time taken: 182.00375747680664
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7207)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4139)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.8897)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9416)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8785)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8285)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8618)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.7001226311876962}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 4674981592856998
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1094, 455, 634, 32, 805, 730, 1050, 521, 278, 548, 268, 582, 594, 1174, 282, 985, 835, 649, 437, 457, 247, 188, 16, 952, 321, 855, 839, 1056, 734, 223, 930, 957, 1128, 625, 754, 1171, 945, 253, 1061, 80, 1175, 490, 655, 688, 811, 436, 320, 769, 452, 748, 774, 23, 1083, 1199, 825, 962, 991, 1054, 210, 1036, 19, 803, 653, 54, 943, 219, 259, 164, 102, 1135, 326, 145, 135, 1091, 143, 648, 37, 737, 224, 402, 958, 609, 872, 875, 533, 194, 349, 412, 818, 876, 144, 126, 1210, 715, 520, 561, 48, 979, 44, 1071, 617, 392, 1129, 106, 1102, 687, 168, 63, 492, 611, 502, 200, 903, 67, 1108, 1185, 824, 963, 986, 852, 701]
valid_ids (0): []
train_ids (1094): [242, 505, 564, 707, 435, 186, 311, 937, 865, 211, 421, 927, 178, 571, 418, 801, 753, 451, 526, 131, 196, 886, 447, 104, 1213, 971, 1113, 1006, 516, 692, 420, 816, 1131, 263, 355, 397, 14, 81, 1153, 831, 430, 140, 553, 411, 303, 925, 458, 351, 363, 93, 929, 1138, 1205, 432, 993, 907, 842, 42, 125, 1028, 1200, 1162, 751, 236, 433, 550, 21, 1154, 951, 348, 319, 1012, 1003, 441, 731, 239, 959, 75, 1074, 1045, 1209, 1164, 938, 503, 598, 1189, 782, 644, 174, 454, 255, 1151, 820, 563, 86, 627, 25, 271, 884, 95, 1157, 328, 39, 325, 1019, 1063, 1041, 79, 777, 861, 274, 2, 926, 470, 792, 132, 877, 916, 269, 826, 409, 403, 758, 723, 1110, 1156, 551, 356, 542, 401, 335, 700, 58, 92, 755, 440, 148, 66, 983, 973, 1053, 127, 1097, 515, 17, 666, 339, 620, 994, 336, 950, 804, 786, 394, 874, 49, 797, 785, 672, 531, 491, 212, 36, 696, 918, 873, 361, 374, 385, 89, 362, 749, 84, 391, 1166, 756, 152, 1121, 765, 301, 1069, 387, 159, 915, 346, 76, 1072, 989, 449, 988, 133, 156, 752, 3, 358, 583, 848, 545, 1000, 1095, 966, 379, 60, 371, 307, 844, 225, 608, 718, 243, 373, 1105, 7, 558, 287, 750, 539, 911, 31, 171, 395, 712, 262, 713, 1120, 234, 899, 427, 389, 469, 162, 535, 249, 673, 509, 1186, 961, 96, 431, 297, 182, 1158, 261, 726, 497, 97, 784, 871, 709, 15, 475, 536, 1117, 512, 357, 1150, 773, 1109, 540, 187, 960, 575, 496, 322, 481, 460, 87, 300, 304, 645, 1192, 290, 313, 256, 474, 632, 1112, 921, 172, 283, 1184, 552, 935, 1015, 947, 114, 725, 849, 612, 997, 870, 13, 312, 549, 1084, 1143, 1134, 103, 1139, 710, 636, 1132, 1089, 791, 264, 588, 1022, 488, 352, 1170, 9, 590, 891, 1197, 1020, 589, 577, 316, 101, 484, 689, 837, 417, 879, 847, 888, 864, 614, 1191, 585, 123, 284, 1034, 828, 708, 621, 459, 1079, 508, 288, 91, 1060, 822, 821, 670, 980, 604, 694, 560, 354, 344, 579, 112, 939, 154, 191, 28, 906, 534, 965, 856, 982, 1073, 578, 1206, 808, 350, 453, 257, 1093, 142, 615, 177, 169, 942, 173, 1203, 1127, 158, 829, 424, 82, 368, 817, 833, 461, 838, 720, 221, 794, 668, 139, 525, 775, 439, 895, 1011, 1141, 55, 853, 832, 678, 912, 57, 719, 1031, 203, 867, 622, 468, 727, 429, 841, 428, 1090, 562, 1165, 665, 650, 546, 176, 630, 381, 664, 109, 823, 228, 592, 423, 796, 1013, 56, 1133, 862, 232, 308, 364, 717, 799, 1049, 43, 1194, 506, 721, 631, 359, 1195, 334, 778, 149, 170, 651, 759, 776, 64, 414, 969, 462, 372, 111, 1122, 834, 685, 98, 11, 105, 1123, 541, 595, 586, 479, 1046, 342, 519, 619, 568, 810, 52, 1145, 556, 1002, 527, 341, 944, 110, 1008, 207, 466, 1027, 624, 998, 587, 1208, 760, 296, 663, 972, 260, 956, 473, 347, 254, 258, 222, 893, 779, 332, 883, 857, 1116, 377, 854, 602, 978, 216, 107, 1004, 443, 868, 416, 0, 742, 405, 662, 370, 33, 375, 714, 680, 1037, 26, 1106, 528, 738, 465, 1100, 908, 291, 547, 1058, 1142, 641, 941, 442, 252, 46, 1052, 18, 380, 1182, 658, 62, 798, 900, 121, 1086, 669, 1196, 1099, 345, 317, 1017, 529, 314, 1029, 30, 1068, 119, 554, 74, 635, 330, 887, 286, 147, 559, 65, 1025, 472, 984, 1009, 596, 1059, 281, 613, 1026, 677, 691, 338, 1092, 600, 892, 740, 360, 426, 241, 607, 530, 728, 1124, 859, 767, 977, 309, 122, 165, 1207, 1001, 61, 1169, 995, 836, 100, 647, 1148, 659, 230, 1188, 815, 1152, 214, 298, 1146, 279, 967, 1064, 1, 894, 217, 250, 643, 711, 12, 914, 642, 686, 498, 1103, 456, 922, 936, 202, 757, 1193, 408, 819, 679, 201, 406, 783, 369, 494, 40, 59, 398, 523, 4, 761, 576, 266, 566, 276, 766, 1160, 486, 538, 1168, 806, 476, 1038, 990, 763, 681, 499, 167, 569, 273, 1030, 981, 137, 231, 999, 795, 809, 199, 1167, 1107, 869, 606, 1098, 206, 716, 628, 1062, 388, 415, 733, 1190, 160, 118, 20, 337, 1178, 108, 640, 293, 878, 964, 99, 1136, 860, 190, 919, 882, 928, 1075, 1078, 1126, 544, 968, 1048, 299, 573, 386, 504, 400, 524, 846, 146, 1085, 580, 830, 248, 514, 904, 1005, 446, 1163, 850, 574, 1070, 845, 932, 764, 204, 365, 73, 41, 699, 690, 1111, 889, 781, 34, 1080, 975, 53, 866, 557, 485, 584, 272, 205, 471, 1082, 616, 376, 445, 917, 450, 136, 5, 1101, 772, 565, 195, 1076, 949, 1179, 150, 789, 605, 1137, 88, 1014, 923, 1202, 1214, 94, 741, 410, 909, 657, 814, 323, 863, 682, 209, 555, 438, 378, 27, 702, 115, 185, 157, 275, 183, 277, 623, 141, 1155, 310, 1043, 601, 218, 396, 1119, 116, 517, 913, 1147, 654, 652, 638, 50, 639, 735, 1066, 593, 1176, 629, 464, 413, 384, 113, 744, 1055, 366, 898, 992, 47, 955, 610, 1149, 1067, 189, 1183, 480, 151, 1039, 745, 353, 77, 184, 934, 463, 896, 1010, 280, 245, 885, 793, 770, 390, 138, 38, 180, 404, 974, 467, 768, 1024, 1042, 603, 237, 10, 448, 661, 851, 697, 787, 129, 289, 425, 724, 1035, 626, 591, 1130, 567, 1087, 161, 1161, 128, 570, 705, 843, 513, 1114, 1177, 996, 208, 483, 572, 660, 324, 240, 532, 407, 1051, 1104, 698, 71, 905, 45, 294, 920, 130, 790, 1172, 646, 1187, 124, 1023, 1140, 743, 1032, 444, 477, 285, 70, 671, 193, 1096, 976, 722, 746, 970, 175, 306, 72, 198, 1040, 265, 840, 802, 500, 1088, 1057, 134, 8, 902, 910, 736, 305, 315, 890, 238, 597, 478, 85, 693, 543, 181, 1159, 729, 827, 215, 226, 762, 1144, 90, 434, 333, 518, 1212, 51, 192, 706, 1081, 329, 327, 501, 674, 940, 1016, 270, 618, 511, 489, 1118, 780, 788, 897, 179, 1033, 880, 1181, 487, 704, 771, 522, 24, 399, 419, 343, 302, 68, 1211, 220, 382, 924, 684, 1047, 695, 656, 267, 166, 1018, 675, 1007, 197, 954, 739, 948, 931, 495, 510, 295, 858, 69, 1044, 812, 229, 732, 340, 1065, 1021, 227, 244, 683, 537, 987, 953, 637, 1180, 251, 881, 29, 235, 83, 813, 393, 581, 901, 383, 1077, 482, 507, 747, 946, 676, 667, 292, 633, 120, 800, 1201, 6, 331, 22, 1125, 153, 318, 422, 933, 703, 233, 213, 493, 78, 163, 1115, 367, 1204, 246, 35, 155, 807, 599, 1173, 117, 1198]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4119288082181835
the save name prefix for this run is:  chkpt-ID_4119288082181835_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.522 +- 0.007
mrr vals (pred, true): 0.001, 0.259
batch losses (mrrl, rdl): 0.0, 0.0048691756

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1141
rank avg (pred): 0.165 +- 0.098
mrr vals (pred, true): 0.020, 0.140
batch losses (mrrl, rdl): 0.0, 5.00382e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.309 +- 0.224
mrr vals (pred, true): 0.182, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004218683

Epoch over!
epoch time: 12.037

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 206
rank avg (pred): 0.253 +- 0.193
mrr vals (pred, true): 0.220, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009255442

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.197 +- 0.168
mrr vals (pred, true): 0.328, 0.242
batch losses (mrrl, rdl): 0.0, 0.0005182284

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 272
rank avg (pred): 0.068 +- 0.055
mrr vals (pred, true): 0.314, 0.238
batch losses (mrrl, rdl): 0.0, 9.2421e-06

Epoch over!
epoch time: 11.877

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 580
rank avg (pred): 0.319 +- 0.263
mrr vals (pred, true): 0.315, 0.109
batch losses (mrrl, rdl): 0.0, 0.0002108182

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 451
rank avg (pred): 0.271 +- 0.229
mrr vals (pred, true): 0.349, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006239089

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 960
rank avg (pred): 0.446 +- 0.362
mrr vals (pred, true): 0.165, 0.005
batch losses (mrrl, rdl): 0.0, 5.1592e-05

Epoch over!
epoch time: 11.753

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.227 +- 0.194
mrr vals (pred, true): 0.373, 0.201
batch losses (mrrl, rdl): 0.0, 0.0007925602

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1022
rank avg (pred): 0.246 +- 0.210
mrr vals (pred, true): 0.364, 0.218
batch losses (mrrl, rdl): 0.0, 0.0009646352

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.426 +- 0.325
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 0.0, 2.14567e-05

Epoch over!
epoch time: 11.809

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.485 +- 0.393
mrr vals (pred, true): 0.213, 0.002
batch losses (mrrl, rdl): 0.0, 0.0001401713

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 332
rank avg (pred): 0.215 +- 0.189
mrr vals (pred, true): 0.155, 0.192
batch losses (mrrl, rdl): 0.0, 0.0006113779

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.250 +- 0.232
mrr vals (pred, true): 0.286, 0.230
batch losses (mrrl, rdl): 0.0, 0.0009269633

Epoch over!
epoch time: 11.833

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.116 +- 0.111
mrr vals (pred, true): 0.339, 0.141
batch losses (mrrl, rdl): 0.3929500282, 9.58449e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.019 +- 0.011
mrr vals (pred, true): 0.205, 0.256
batch losses (mrrl, rdl): 0.0259309635, 1.44873e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.448 +- 0.231
mrr vals (pred, true): 0.088, 0.156
batch losses (mrrl, rdl): 0.045788765, 0.0017191996

Epoch over!
epoch time: 12.1

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 827
rank avg (pred): 0.045 +- 0.027
mrr vals (pred, true): 0.177, 0.256
batch losses (mrrl, rdl): 0.0616724379, 2.5359e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.179 +- 0.096
mrr vals (pred, true): 0.125, 0.004
batch losses (mrrl, rdl): 0.0563468263, 0.0020008406

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.443 +- 0.226
mrr vals (pred, true): 0.102, 0.153
batch losses (mrrl, rdl): 0.026509393, 0.0017388294

Epoch over!
epoch time: 12.118

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.258 +- 0.136
mrr vals (pred, true): 0.114, 0.213
batch losses (mrrl, rdl): 0.0992367044, 0.0008728521

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.011 +- 0.006
mrr vals (pred, true): 0.215, 0.253
batch losses (mrrl, rdl): 0.0142736007, 1.13688e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 467
rank avg (pred): 0.238 +- 0.124
mrr vals (pred, true): 0.128, 0.003
batch losses (mrrl, rdl): 0.060576044, 0.0011866289

Epoch over!
epoch time: 11.969

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 940
rank avg (pred): 0.698 +- 0.294
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0005989178, 9.12594e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 659
rank avg (pred): 0.424 +- 0.216
mrr vals (pred, true): 0.119, 0.003
batch losses (mrrl, rdl): 0.0481305495, 4.54811e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 810
rank avg (pred): 0.118 +- 0.059
mrr vals (pred, true): 0.091, 0.151
batch losses (mrrl, rdl): 0.0360144451, 1.3057e-05

Epoch over!
epoch time: 12.069

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 244
rank avg (pred): 0.016 +- 0.009
mrr vals (pred, true): 0.204, 0.221
batch losses (mrrl, rdl): 0.0030089647, 2.66867e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.542 +- 0.241
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002139066, 0.0020675282

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.266 +- 0.142
mrr vals (pred, true): 0.091, 0.043
batch losses (mrrl, rdl): 0.0167723913, 6.19521e-05

Epoch over!
epoch time: 12.289

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 35
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.260, 0.192
batch losses (mrrl, rdl): 0.046999611, 4.64577e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 635
rank avg (pred): 0.395 +- 0.199
mrr vals (pred, true): 0.096, 0.160
batch losses (mrrl, rdl): 0.0409125835, 0.0011333617

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.409 +- 0.211
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.0240946263, 7.08439e-05

Epoch over!
epoch time: 12.365

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1152
rank avg (pred): 0.257 +- 0.131
mrr vals (pred, true): 0.071, 0.055
batch losses (mrrl, rdl): 0.0042573577, 9.29371e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.393 +- 0.204
mrr vals (pred, true): 0.104, 0.004
batch losses (mrrl, rdl): 0.0286831912, 9.63134e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.239 +- 0.137
mrr vals (pred, true): 0.130, 0.004
batch losses (mrrl, rdl): 0.0632769242, 0.0012076463

Epoch over!
epoch time: 11.982

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 78
rank avg (pred): 0.012 +- 0.007
mrr vals (pred, true): 0.205, 0.222
batch losses (mrrl, rdl): 0.0029598128, 3.49555e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.320 +- 0.175
mrr vals (pred, true): 0.093, 0.156
batch losses (mrrl, rdl): 0.0394563563, 0.0005108317

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 334
rank avg (pred): 0.238 +- 0.134
mrr vals (pred, true): 0.136, 0.203
batch losses (mrrl, rdl): 0.0441908874, 0.0007626243

Epoch over!
epoch time: 12.133

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.322 +- 0.175
mrr vals (pred, true): 0.115, 0.201
batch losses (mrrl, rdl): 0.0730625018, 0.001687947

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1021
rank avg (pred): 0.261 +- 0.158
mrr vals (pred, true): 0.145, 0.207
batch losses (mrrl, rdl): 0.0385965407, 0.0010125347

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.401 +- 0.201
mrr vals (pred, true): 0.073, 0.001
batch losses (mrrl, rdl): 0.005232939, 0.002435419

Epoch over!
epoch time: 12.211

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 543
rank avg (pred): 0.510 +- 0.228
mrr vals (pred, true): 0.057, 0.049
batch losses (mrrl, rdl): 0.0005267778, 0.0016921285

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.398 +- 0.187
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0008983106, 9.27798e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 619
rank avg (pred): 0.353 +- 0.178
mrr vals (pred, true): 0.092, 0.172
batch losses (mrrl, rdl): 0.064075008, 0.0008682107

Epoch over!
epoch time: 12.008

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.207 +- 0.122
mrr vals (pred, true): 0.133, 0.203

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04644 	 0.00074 	 m..s
    1 	     1 	 0.04644 	 0.00076 	 m..s
   24 	     2 	 0.06889 	 0.00104 	 m..s
    0 	     3 	 0.04098 	 0.00121 	 m..s
   30 	     4 	 0.07934 	 0.00340 	 m..s
   67 	     5 	 0.12530 	 0.00343 	 MISS
   75 	     6 	 0.13075 	 0.00344 	 MISS
   13 	     7 	 0.05229 	 0.00344 	 m..s
   62 	     8 	 0.12410 	 0.00348 	 MISS
   47 	     9 	 0.10579 	 0.00352 	 MISS
   73 	    10 	 0.12944 	 0.00356 	 MISS
    1 	    11 	 0.04644 	 0.00363 	 m..s
   39 	    12 	 0.10359 	 0.00366 	 m..s
   46 	    13 	 0.10548 	 0.00371 	 MISS
   27 	    14 	 0.07784 	 0.00376 	 m..s
   70 	    15 	 0.12573 	 0.00380 	 MISS
   63 	    16 	 0.12417 	 0.00385 	 MISS
   65 	    17 	 0.12424 	 0.00385 	 MISS
    1 	    18 	 0.04644 	 0.00393 	 m..s
   53 	    19 	 0.11822 	 0.00394 	 MISS
   55 	    20 	 0.12182 	 0.00394 	 MISS
    8 	    21 	 0.05049 	 0.00395 	 m..s
   58 	    22 	 0.12311 	 0.00398 	 MISS
   73 	    23 	 0.12944 	 0.00403 	 MISS
    1 	    24 	 0.04644 	 0.00412 	 m..s
   50 	    25 	 0.11450 	 0.00419 	 MISS
   69 	    26 	 0.12547 	 0.00422 	 MISS
   19 	    27 	 0.05840 	 0.00423 	 m..s
   64 	    28 	 0.12423 	 0.00423 	 MISS
   38 	    29 	 0.10346 	 0.00437 	 m..s
   80 	    30 	 0.13253 	 0.00438 	 MISS
   78 	    31 	 0.13227 	 0.00443 	 MISS
   10 	    32 	 0.05135 	 0.00443 	 m..s
    7 	    33 	 0.04751 	 0.00443 	 m..s
   11 	    34 	 0.05179 	 0.00445 	 m..s
   61 	    35 	 0.12364 	 0.00446 	 MISS
   18 	    36 	 0.05791 	 0.00464 	 m..s
   54 	    37 	 0.11894 	 0.00469 	 MISS
   42 	    38 	 0.10394 	 0.00478 	 m..s
   78 	    39 	 0.13227 	 0.00496 	 MISS
    1 	    40 	 0.04644 	 0.00502 	 m..s
   15 	    41 	 0.05308 	 0.00504 	 m..s
   85 	    42 	 0.13387 	 0.00521 	 MISS
   45 	    43 	 0.10534 	 0.00543 	 m..s
   36 	    44 	 0.10288 	 0.00578 	 m..s
   39 	    45 	 0.10359 	 0.00682 	 m..s
   25 	    46 	 0.07410 	 0.03246 	 m..s
   14 	    47 	 0.05251 	 0.03267 	 ~...
   16 	    48 	 0.05484 	 0.03909 	 ~...
    9 	    49 	 0.05118 	 0.04276 	 ~...
   21 	    50 	 0.06594 	 0.04382 	 ~...
   17 	    51 	 0.05732 	 0.05590 	 ~...
   22 	    52 	 0.06786 	 0.05615 	 ~...
   23 	    53 	 0.06798 	 0.05981 	 ~...
   20 	    54 	 0.05943 	 0.06300 	 ~...
   31 	    55 	 0.07990 	 0.07000 	 ~...
   12 	    56 	 0.05216 	 0.07395 	 ~...
   41 	    57 	 0.10390 	 0.10742 	 ~...
   35 	    58 	 0.10239 	 0.12053 	 ~...
   37 	    59 	 0.10314 	 0.12924 	 ~...
   26 	    60 	 0.07767 	 0.13473 	 m..s
   32 	    61 	 0.08414 	 0.13481 	 m..s
   28 	    62 	 0.07920 	 0.14437 	 m..s
   33 	    63 	 0.08696 	 0.14888 	 m..s
   28 	    64 	 0.07920 	 0.14994 	 m..s
   89 	    65 	 0.15787 	 0.15698 	 ~...
   76 	    66 	 0.13139 	 0.15803 	 ~...
   43 	    67 	 0.10474 	 0.15954 	 m..s
   44 	    68 	 0.10516 	 0.16140 	 m..s
   88 	    69 	 0.14553 	 0.16163 	 ~...
   48 	    70 	 0.10583 	 0.16450 	 m..s
   86 	    71 	 0.14426 	 0.16605 	 ~...
   34 	    72 	 0.08724 	 0.16628 	 m..s
   49 	    73 	 0.10617 	 0.16654 	 m..s
   90 	    74 	 0.16337 	 0.16862 	 ~...
   87 	    75 	 0.14496 	 0.17512 	 m..s
   56 	    76 	 0.12264 	 0.17763 	 m..s
   95 	    77 	 0.21174 	 0.18234 	 ~...
   94 	    78 	 0.21118 	 0.19366 	 ~...
   59 	    79 	 0.12313 	 0.20044 	 m..s
   84 	    80 	 0.13289 	 0.20045 	 m..s
   82 	    81 	 0.13266 	 0.20099 	 m..s
   57 	    82 	 0.12302 	 0.20127 	 m..s
   66 	    83 	 0.12469 	 0.20179 	 m..s
   81 	    84 	 0.13265 	 0.20257 	 m..s
   98 	    85 	 0.21256 	 0.20499 	 ~...
   99 	    86 	 0.21438 	 0.20718 	 ~...
  100 	    87 	 0.22191 	 0.20837 	 ~...
   72 	    88 	 0.12621 	 0.20947 	 m..s
   83 	    89 	 0.13275 	 0.21005 	 m..s
  111 	    90 	 0.24021 	 0.21035 	 ~...
   68 	    91 	 0.12531 	 0.21152 	 m..s
   96 	    92 	 0.21233 	 0.21152 	 ~...
   97 	    93 	 0.21246 	 0.21177 	 ~...
   51 	    94 	 0.11665 	 0.21270 	 m..s
  112 	    95 	 0.24197 	 0.21487 	 ~...
  104 	    96 	 0.22603 	 0.21730 	 ~...
  101 	    97 	 0.22256 	 0.21798 	 ~...
  106 	    98 	 0.22751 	 0.21933 	 ~...
  106 	    99 	 0.22751 	 0.22495 	 ~...
  113 	   100 	 0.24514 	 0.22627 	 ~...
  108 	   101 	 0.22853 	 0.22879 	 ~...
   60 	   102 	 0.12357 	 0.23172 	 MISS
   52 	   103 	 0.11762 	 0.23205 	 MISS
   71 	   104 	 0.12575 	 0.23218 	 MISS
   93 	   105 	 0.19650 	 0.23589 	 m..s
  109 	   106 	 0.23107 	 0.23636 	 ~...
   77 	   107 	 0.13199 	 0.23821 	 MISS
   91 	   108 	 0.19072 	 0.24317 	 m..s
  110 	   109 	 0.23406 	 0.24716 	 ~...
   92 	   110 	 0.19570 	 0.24858 	 m..s
  102 	   111 	 0.22436 	 0.25083 	 ~...
  105 	   112 	 0.22713 	 0.25179 	 ~...
  115 	   113 	 0.26061 	 0.25344 	 ~...
  102 	   114 	 0.22436 	 0.25578 	 m..s
  114 	   115 	 0.25354 	 0.27384 	 ~...
  119 	   116 	 0.27916 	 0.27612 	 ~...
  117 	   117 	 0.26977 	 0.28511 	 ~...
  116 	   118 	 0.26343 	 0.30443 	 m..s
  118 	   119 	 0.27769 	 0.31364 	 m..s
  120 	   120 	 0.28070 	 0.31754 	 m..s
==========================================
r_mrr = 0.7577105164527893
r2_mrr = 0.5329698324203491
spearmanr_mrr@5 = 0.9833477735519409
spearmanr_mrr@10 = 0.970052182674408
spearmanr_mrr@50 = 0.9070538878440857
spearmanr_mrr@100 = 0.8747016191482544
spearmanr_mrr@All = 0.9070436954498291
==========================================
test time: 0.396
Done Testing dataset CoDExSmall
total time taken: 191.06837630271912
training time taken: 181.0266134738922
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7577)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.5330)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9833)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9701)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9071)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8747)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9070)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.2178972701840394}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 782163540610260
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [961, 1011, 423, 1214, 1134, 383, 731, 1088, 1033, 394, 405, 269, 288, 898, 284, 795, 57, 586, 1017, 715, 505, 957, 519, 188, 707, 363, 848, 1132, 655, 403, 1019, 964, 670, 354, 913, 679, 14, 1199, 246, 613, 75, 178, 609, 139, 53, 52, 420, 976, 607, 908, 63, 339, 28, 1168, 989, 148, 979, 1201, 1111, 1167, 1122, 765, 1094, 949, 545, 1062, 1014, 435, 897, 772, 820, 793, 769, 82, 464, 1182, 1200, 250, 201, 697, 636, 278, 554, 238, 174, 1181, 470, 382, 1048, 991, 198, 583, 921, 945, 969, 786, 350, 1086, 774, 604, 199, 813, 1056, 140, 404, 630, 1003, 313, 1096, 451, 736, 422, 168, 682, 1149, 625, 664, 1142, 584, 181, 928]
valid_ids (0): []
train_ids (1094): [778, 955, 428, 34, 706, 13, 341, 1150, 296, 657, 230, 860, 98, 388, 1117, 766, 1080, 113, 1020, 623, 321, 80, 754, 1040, 292, 656, 216, 557, 639, 177, 1126, 789, 650, 1022, 1084, 40, 1043, 783, 755, 833, 749, 1119, 771, 830, 709, 193, 906, 693, 540, 995, 610, 454, 853, 516, 775, 56, 746, 959, 315, 15, 640, 626, 575, 307, 819, 1058, 366, 926, 294, 910, 1068, 668, 1093, 1118, 1206, 184, 498, 131, 938, 161, 412, 525, 1202, 924, 834, 179, 305, 561, 94, 1187, 227, 1103, 143, 993, 396, 816, 149, 1038, 169, 548, 432, 476, 934, 1016, 565, 990, 433, 1175, 351, 543, 718, 716, 306, 888, 665, 605, 24, 727, 735, 681, 1082, 417, 68, 799, 20, 91, 779, 598, 880, 37, 580, 197, 239, 153, 1045, 488, 266, 323, 806, 458, 868, 684, 62, 588, 751, 532, 45, 962, 343, 745, 1140, 145, 67, 603, 915, 480, 978, 1095, 495, 539, 358, 517, 499, 787, 1049, 105, 47, 332, 1107, 276, 566, 494, 1197, 36, 1083, 801, 644, 634, 1037, 275, 429, 7, 667, 688, 1015, 64, 1211, 933, 975, 815, 373, 1108, 1114, 151, 570, 1030, 289, 8, 130, 245, 175, 431, 837, 265, 96, 791, 273, 418, 1063, 796, 846, 861, 182, 32, 556, 214, 948, 981, 881, 914, 1021, 4, 911, 828, 1079, 522, 643, 2, 160, 974, 364, 812, 48, 792, 564, 460, 635, 869, 1113, 1121, 591, 42, 274, 1059, 1002, 734, 61, 982, 695, 112, 425, 389, 615, 1141, 807, 1099, 106, 618, 845, 43, 1191, 484, 549, 259, 11, 980, 157, 407, 300, 89, 1067, 713, 426, 362, 1208, 9, 16, 1024, 1209, 672, 243, 1018, 21, 659, 1129, 507, 577, 205, 691, 673, 491, 46, 864, 10, 996, 967, 386, 1055, 1213, 58, 529, 568, 109, 817, 527, 559, 666, 1092, 99, 100, 633, 882, 759, 78, 1130, 704, 855, 165, 747, 919, 1210, 832, 41, 1013, 600, 946, 1073, 760, 825, 541, 1075, 122, 395, 23, 553, 249, 999, 1098, 1177, 863, 497, 866, 329, 1158, 838, 937, 973, 1179, 271, 279, 1065, 1131, 803, 741, 459, 73, 349, 1186, 352, 381, 1036, 430, 805, 263, 689, 638, 87, 1171, 652, 1029, 518, 208, 1138, 1166, 572, 137, 827, 537, 456, 224, 101, 524, 753, 714, 449, 84, 374, 965, 1164, 761, 858, 242, 879, 69, 687, 282, 737, 573, 1178, 852, 79, 406, 324, 642, 723, 1051, 653, 763, 378, 947, 475, 55, 984, 22, 490, 253, 277, 876, 1154, 597, 357, 302, 1101, 71, 171, 38, 121, 1185, 331, 361, 896, 1105, 1143, 654, 1136, 402, 411, 611, 337, 146, 486, 399, 647, 255, 267, 283, 936, 297, 1120, 878, 442, 445, 930, 211, 994, 322, 1147, 700, 508, 138, 225, 685, 206, 166, 1115, 5, 602, 1035, 671, 83, 338, 1112, 397, 25, 1031, 463, 546, 385, 1026, 851, 943, 85, 295, 1077, 788, 972, 1100, 50, 489, 621, 971, 773, 680, 1046, 849, 1133, 103, 708, 986, 455, 675, 348, 102, 187, 1041, 528, 599, 628, 1137, 86, 1123, 590, 400, 1050, 207, 1076, 234, 334, 54, 530, 578, 1102, 195, 30, 1006, 220, 501, 1146, 236, 790, 892, 493, 416, 823, 927, 387, 506, 172, 200, 309, 514, 1189, 123, 762, 562, 438, 932, 756, 164, 465, 900, 658, 439, 1023, 750, 93, 134, 632, 854, 421, 126, 478, 369, 1170, 764, 916, 931, 585, 108, 370, 677, 631, 894, 612, 446, 574, 1087, 344, 468, 739, 88, 1061, 136, 159, 152, 19, 509, 39, 1160, 441, 885, 325, 60, 147, 1151, 533, 1124, 992, 721, 162, 1116, 717, 390, 440, 641, 857, 1004, 257, 340, 701, 163, 90, 368, 535, 232, 595, 998, 794, 606, 479, 264, 76, 678, 258, 954, 436, 1057, 719, 191, 204, 808, 326, 744, 142, 1139, 784, 150, 115, 1027, 167, 536, 218, 301, 840, 346, 119, 821, 608, 738, 569, 434, 987, 550, 1091, 733, 170, 596, 49, 720, 1, 1128, 120, 1001, 155, 593, 558, 376, 538, 291, 1097, 31, 699, 237, 829, 614, 192, 674, 124, 582, 877, 1145, 551, 342, 314, 758, 29, 345, 408, 127, 547, 141, 1039, 213, 690, 781, 710, 809, 893, 144, 229, 1012, 33, 116, 917, 703, 627, 272, 859, 1205, 299, 59, 251, 355, 335, 1053, 477, 1127, 645, 1165, 1009, 377, 929, 114, 902, 310, 826, 26, 409, 871, 901, 183, 462, 443, 353, 581, 712, 474, 552, 622, 380, 414, 316, 576, 180, 542, 132, 104, 483, 1204, 240, 1194, 466, 843, 190, 447, 1081, 1078, 1109, 392, 1176, 970, 173, 359, 696, 44, 886, 905, 317, 1106, 874, 1089, 887, 95, 1044, 722, 333, 1025, 1148, 209, 1192, 17, 935, 1000, 960, 336, 944, 918, 70, 1007, 320, 770, 850, 616, 752, 776, 1005, 940, 836, 285, 814, 555, 117, 873, 268, 683, 521, 624, 379, 473, 822, 244, 217, 247, 1196, 308, 966, 0, 711, 968, 281, 1161, 619, 223, 798, 1157, 202, 452, 1174, 444, 215, 904, 424, 110, 780, 222, 286, 523, 365, 154, 1054, 496, 977, 469, 1028, 895, 694, 676, 492, 398, 811, 637, 129, 221, 865, 298, 920, 958, 629, 92, 419, 330, 1090, 1184, 18, 125, 740, 729, 824, 1085, 233, 728, 939, 504, 1042, 1069, 328, 768, 360, 1070, 1010, 617, 601, 802, 724, 1104, 472, 1032, 1212, 810, 1066, 72, 620, 692, 81, 1159, 111, 1125, 1110, 903, 841, 1173, 950, 27, 482, 312, 196, 587, 800, 923, 1153, 280, 189, 467, 579, 1047, 203, 226, 212, 1072, 77, 371, 186, 648, 883, 804, 726, 231, 662, 1064, 3, 870, 1155, 646, 1188, 907, 74, 66, 391, 453, 797, 875, 862, 515, 97, 256, 592, 777, 1183, 303, 649, 457, 210, 651, 367, 318, 393, 767, 988, 1071, 1163, 372, 481, 867, 589, 663, 531, 1207, 1172, 831, 942, 261, 241, 65, 35, 1074, 884, 563, 705, 951, 1060, 953, 660, 725, 952, 133, 912, 156, 889, 135, 6, 842, 571, 560, 375, 327, 856, 1008, 1203, 304, 311, 260, 839, 118, 743, 963, 1169, 997, 427, 520, 899, 262, 384, 526, 248, 270, 835, 235, 544, 983, 922, 485, 128, 1052, 12, 410, 941, 1162, 185, 500, 785, 502, 293, 252, 319, 219, 461, 290, 1144, 513, 1198, 487, 471, 511, 534, 698, 702, 107, 450, 872, 512, 158, 1180, 415, 1190, 818, 661, 730, 1152, 228, 356, 401, 1156, 347, 891, 985, 890, 757, 567, 254, 956, 686, 194, 503, 413, 448, 287, 844, 742, 925, 782, 1135, 51, 748, 1195, 1034, 1193, 669, 510, 847, 594, 732, 437, 176, 909]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5560990120470893
the save name prefix for this run is:  chkpt-ID_5560990120470893_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 573
rank avg (pred): 0.529 +- 0.009
mrr vals (pred, true): 0.001, 0.105
batch losses (mrrl, rdl): 0.0, 0.0018658718

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 826
rank avg (pred): 0.186 +- 0.031
mrr vals (pred, true): 0.003, 0.260
batch losses (mrrl, rdl): 0.0, 0.0004347441

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1043
rank avg (pred): 0.227 +- 0.101
mrr vals (pred, true): 0.022, 0.004
batch losses (mrrl, rdl): 0.0, 0.0012206719

Epoch over!
epoch time: 11.96

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 424
rank avg (pred): 0.302 +- 0.139
mrr vals (pred, true): 0.029, 0.006
batch losses (mrrl, rdl): 0.0, 0.0006017929

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.061 +- 0.043
mrr vals (pred, true): 0.246, 0.245
batch losses (mrrl, rdl): 0.0, 1.88741e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.253 +- 0.202
mrr vals (pred, true): 0.281, 0.212
batch losses (mrrl, rdl): 0.0, 0.0009795061

Epoch over!
epoch time: 11.879

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1125
rank avg (pred): 0.251 +- 0.199
mrr vals (pred, true): 0.278, 0.004
batch losses (mrrl, rdl): 0.0, 0.0008503833

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.314 +- 0.253
mrr vals (pred, true): 0.280, 0.005
batch losses (mrrl, rdl): 0.0, 0.0003609732

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 965
rank avg (pred): 0.463 +- 0.369
mrr vals (pred, true): 0.274, 0.003
batch losses (mrrl, rdl): 0.0, 7.8363e-05

Epoch over!
epoch time: 11.91

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 917
rank avg (pred): 0.470 +- 0.361
mrr vals (pred, true): 0.246, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003035863

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.245 +- 0.203
mrr vals (pred, true): 0.254, 0.005
batch losses (mrrl, rdl): 0.0, 0.0008578884

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1150
rank avg (pred): 0.169 +- 0.149
mrr vals (pred, true): 0.327, 0.065
batch losses (mrrl, rdl): 0.0, 9.9714e-06

Epoch over!
epoch time: 11.866

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.234 +- 0.205
mrr vals (pred, true): 0.302, 0.005
batch losses (mrrl, rdl): 0.0, 0.000932503

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 331
rank avg (pred): 0.238 +- 0.217
mrr vals (pred, true): 0.305, 0.203
batch losses (mrrl, rdl): 0.0, 0.0008133585

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 321
rank avg (pred): 0.028 +- 0.026
mrr vals (pred, true): 0.390, 0.276
batch losses (mrrl, rdl): 0.0, 7.8499e-06

Epoch over!
epoch time: 11.787

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 846
rank avg (pred): 0.406 +- 0.328
mrr vals (pred, true): 0.239, 0.058
batch losses (mrrl, rdl): 0.3555159569, 0.0008996082

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 552
rank avg (pred): 0.393 +- 0.247
mrr vals (pred, true): 0.084, 0.035
batch losses (mrrl, rdl): 0.0116106365, 0.0006464454

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1002
rank avg (pred): 0.185 +- 0.122
mrr vals (pred, true): 0.115, 0.199
batch losses (mrrl, rdl): 0.0698576346, 0.0003917841

Epoch over!
epoch time: 12.185

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1008
rank avg (pred): 0.144 +- 0.101
mrr vals (pred, true): 0.124, 0.198
batch losses (mrrl, rdl): 0.0548764504, 0.0001716138

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.116 +- 0.081
mrr vals (pred, true): 0.117, 0.207
batch losses (mrrl, rdl): 0.0808648765, 7.7734e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1018
rank avg (pred): 0.156 +- 0.107
mrr vals (pred, true): 0.130, 0.197
batch losses (mrrl, rdl): 0.0448162518, 0.0002350117

Epoch over!
epoch time: 12.097

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 869
rank avg (pred): 0.368 +- 0.193
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 5.54568e-05, 0.0002102718

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 180
rank avg (pred): 0.145 +- 0.097
mrr vals (pred, true): 0.124, 0.004
batch losses (mrrl, rdl): 0.0542529747, 0.0022113391

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 620
rank avg (pred): 0.338 +- 0.205
mrr vals (pred, true): 0.097, 0.164
batch losses (mrrl, rdl): 0.0437829569, 0.0006705841

Epoch over!
epoch time: 12.07

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1190
rank avg (pred): 0.359 +- 0.207
mrr vals (pred, true): 0.081, 0.006
batch losses (mrrl, rdl): 0.0094654346, 0.0001776089

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1059
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.307, 0.316
batch losses (mrrl, rdl): 0.0007853485, 1.68748e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 686
rank avg (pred): 0.364 +- 0.205
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0208289009, 0.0001805971

Epoch over!
epoch time: 12.025

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1031
rank avg (pred): 0.179 +- 0.109
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0577865541, 0.0016819681

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.167 +- 0.108
mrr vals (pred, true): 0.144, 0.228
batch losses (mrrl, rdl): 0.0701286867, 0.000300128

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 776
rank avg (pred): 0.556 +- 0.233
mrr vals (pred, true): 0.023, 0.075
batch losses (mrrl, rdl): 0.0075319684, 0.0012689591

Epoch over!
epoch time: 12.149

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 101
rank avg (pred): 0.193 +- 0.125
mrr vals (pred, true): 0.142, 0.178
batch losses (mrrl, rdl): 0.0130411284, 0.0004620013

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.011 +- 0.008
mrr vals (pred, true): 0.242, 0.223
batch losses (mrrl, rdl): 0.0035301116, 2.52652e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 656
rank avg (pred): 0.392 +- 0.210
mrr vals (pred, true): 0.103, 0.004
batch losses (mrrl, rdl): 0.0278599095, 0.0001138315

Epoch over!
epoch time: 12.271

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 570
rank avg (pred): 0.388 +- 0.207
mrr vals (pred, true): 0.087, 0.097
batch losses (mrrl, rdl): 0.013440581, 0.0002526189

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 559
rank avg (pred): 0.363 +- 0.182
mrr vals (pred, true): 0.062, 0.035
batch losses (mrrl, rdl): 0.0014958396, 0.0003391706

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 968
rank avg (pred): 0.543 +- 0.251
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 6.0769e-05, 0.0001458898

Epoch over!
epoch time: 12.153

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 557
rank avg (pred): 0.456 +- 0.219
mrr vals (pred, true): 0.062, 0.038
batch losses (mrrl, rdl): 0.0014198165, 0.0011008057

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 129
rank avg (pred): 0.226 +- 0.123
mrr vals (pred, true): 0.113, 0.234
batch losses (mrrl, rdl): 0.146410346, 0.0007202361

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1169
rank avg (pred): 0.392 +- 0.203
mrr vals (pred, true): 0.074, 0.118
batch losses (mrrl, rdl): 0.0195649303, 0.0005443831

Epoch over!
epoch time: 12.227

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 608
rank avg (pred): 0.387 +- 0.208
mrr vals (pred, true): 0.101, 0.155
batch losses (mrrl, rdl): 0.029758025, 0.0011755329

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 900
rank avg (pred): 0.571 +- 0.273
mrr vals (pred, true): 0.069, 0.002
batch losses (mrrl, rdl): 0.003700146, 0.0001776785

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.398 +- 0.200
mrr vals (pred, true): 0.083, 0.145
batch losses (mrrl, rdl): 0.0390420333, 0.0011068519

Epoch over!
epoch time: 12.096

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 103
rank avg (pred): 0.305 +- 0.185
mrr vals (pred, true): 0.132, 0.200
batch losses (mrrl, rdl): 0.0465032421, 0.0015155191

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.273 +- 0.158
mrr vals (pred, true): 0.120, 0.197
batch losses (mrrl, rdl): 0.0590763055, 0.0010150024

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.380 +- 0.199
mrr vals (pred, true): 0.117, 0.149
batch losses (mrrl, rdl): 0.010657521, 0.0009990803

Epoch over!
epoch time: 12.099

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.568 +- 0.260
mrr vals (pred, true): 0.056, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05563 	 0.00073 	 m..s
    0 	     1 	 0.05563 	 0.00076 	 m..s
    0 	     2 	 0.05563 	 0.00094 	 m..s
   16 	     3 	 0.06027 	 0.00097 	 m..s
  100 	     4 	 0.18056 	 0.00104 	 MISS
    0 	     5 	 0.05563 	 0.00124 	 m..s
   15 	     6 	 0.05957 	 0.00138 	 m..s
   38 	     7 	 0.09474 	 0.00280 	 m..s
    0 	     8 	 0.05563 	 0.00324 	 m..s
   93 	     9 	 0.15130 	 0.00354 	 MISS
    0 	    10 	 0.05563 	 0.00363 	 m..s
   75 	    11 	 0.14415 	 0.00368 	 MISS
   61 	    12 	 0.13916 	 0.00374 	 MISS
   44 	    13 	 0.10065 	 0.00376 	 m..s
   73 	    14 	 0.14379 	 0.00388 	 MISS
   78 	    15 	 0.14522 	 0.00392 	 MISS
    0 	    16 	 0.05563 	 0.00393 	 m..s
   49 	    17 	 0.11219 	 0.00394 	 MISS
   35 	    18 	 0.09018 	 0.00398 	 m..s
    0 	    19 	 0.05563 	 0.00399 	 m..s
   45 	    20 	 0.10073 	 0.00401 	 m..s
    0 	    21 	 0.05563 	 0.00403 	 m..s
   66 	    22 	 0.14321 	 0.00408 	 MISS
   25 	    23 	 0.07543 	 0.00411 	 m..s
   53 	    24 	 0.13620 	 0.00413 	 MISS
   48 	    25 	 0.10163 	 0.00416 	 m..s
   81 	    26 	 0.14579 	 0.00419 	 MISS
   57 	    27 	 0.13766 	 0.00421 	 MISS
   80 	    28 	 0.14570 	 0.00424 	 MISS
   62 	    29 	 0.14092 	 0.00430 	 MISS
    0 	    30 	 0.05563 	 0.00431 	 m..s
   96 	    31 	 0.15598 	 0.00432 	 MISS
   87 	    32 	 0.15051 	 0.00433 	 MISS
   88 	    33 	 0.15060 	 0.00442 	 MISS
    0 	    34 	 0.05563 	 0.00443 	 m..s
   41 	    35 	 0.09896 	 0.00443 	 m..s
   27 	    36 	 0.08194 	 0.00451 	 m..s
   84 	    37 	 0.14920 	 0.00452 	 MISS
   79 	    38 	 0.14535 	 0.00455 	 MISS
   85 	    39 	 0.14928 	 0.00459 	 MISS
   60 	    40 	 0.13853 	 0.00459 	 MISS
   14 	    41 	 0.05669 	 0.00467 	 m..s
   92 	    42 	 0.15110 	 0.00469 	 MISS
   94 	    43 	 0.15316 	 0.00471 	 MISS
   95 	    44 	 0.15466 	 0.00474 	 MISS
   71 	    45 	 0.14346 	 0.00483 	 MISS
   46 	    46 	 0.10138 	 0.00485 	 m..s
   39 	    47 	 0.09569 	 0.00543 	 m..s
   18 	    48 	 0.06087 	 0.00638 	 m..s
   23 	    49 	 0.07483 	 0.00682 	 m..s
   17 	    50 	 0.06081 	 0.02089 	 m..s
   13 	    51 	 0.05656 	 0.04276 	 ~...
   99 	    52 	 0.18024 	 0.04277 	 MISS
   22 	    53 	 0.06925 	 0.04682 	 ~...
   20 	    54 	 0.06711 	 0.04683 	 ~...
   21 	    55 	 0.06784 	 0.05555 	 ~...
   26 	    56 	 0.07770 	 0.05672 	 ~...
   19 	    57 	 0.06228 	 0.05706 	 ~...
   12 	    58 	 0.05608 	 0.07395 	 ~...
   11 	    59 	 0.05601 	 0.07487 	 ~...
   32 	    60 	 0.08676 	 0.10123 	 ~...
   28 	    61 	 0.08230 	 0.10430 	 ~...
   33 	    62 	 0.08701 	 0.11465 	 ~...
   97 	    63 	 0.17176 	 0.12614 	 m..s
   24 	    64 	 0.07495 	 0.12878 	 m..s
   29 	    65 	 0.08253 	 0.12924 	 m..s
   34 	    66 	 0.08776 	 0.13062 	 m..s
   31 	    67 	 0.08263 	 0.13780 	 m..s
   58 	    68 	 0.13774 	 0.14258 	 ~...
   47 	    69 	 0.10149 	 0.14455 	 m..s
   30 	    70 	 0.08260 	 0.14468 	 m..s
   43 	    71 	 0.10051 	 0.14559 	 m..s
   37 	    72 	 0.09458 	 0.15393 	 m..s
   90 	    73 	 0.15107 	 0.15763 	 ~...
   36 	    74 	 0.09414 	 0.15954 	 m..s
   42 	    75 	 0.09965 	 0.16391 	 m..s
   64 	    76 	 0.14218 	 0.16862 	 ~...
   40 	    77 	 0.09613 	 0.16895 	 m..s
   77 	    78 	 0.14493 	 0.18342 	 m..s
   63 	    79 	 0.14146 	 0.18388 	 m..s
   56 	    80 	 0.13728 	 0.18776 	 m..s
  103 	    81 	 0.22904 	 0.18886 	 m..s
   76 	    82 	 0.14445 	 0.19390 	 m..s
  102 	    83 	 0.22849 	 0.19708 	 m..s
   50 	    84 	 0.13426 	 0.19771 	 m..s
   89 	    85 	 0.15061 	 0.19998 	 m..s
   59 	    86 	 0.13822 	 0.20189 	 m..s
   65 	    87 	 0.14224 	 0.20235 	 m..s
   54 	    88 	 0.13699 	 0.20257 	 m..s
   69 	    89 	 0.14336 	 0.20338 	 m..s
   52 	    90 	 0.13616 	 0.20390 	 m..s
   55 	    91 	 0.13716 	 0.20546 	 m..s
  106 	    92 	 0.23246 	 0.20718 	 ~...
  105 	    93 	 0.22993 	 0.20837 	 ~...
   67 	    94 	 0.14323 	 0.20847 	 m..s
   51 	    95 	 0.13596 	 0.20890 	 m..s
   74 	    96 	 0.14403 	 0.21372 	 m..s
   68 	    97 	 0.14329 	 0.21721 	 m..s
  108 	    98 	 0.23746 	 0.21787 	 ~...
  111 	    99 	 0.24170 	 0.21833 	 ~...
   86 	   100 	 0.14979 	 0.21920 	 m..s
   72 	   101 	 0.14347 	 0.21990 	 m..s
  113 	   102 	 0.24465 	 0.22183 	 ~...
  110 	   103 	 0.24028 	 0.22482 	 ~...
  107 	   104 	 0.23331 	 0.22512 	 ~...
  114 	   105 	 0.24469 	 0.22627 	 ~...
   70 	   106 	 0.14344 	 0.22855 	 m..s
  104 	   107 	 0.22944 	 0.22955 	 ~...
   83 	   108 	 0.14709 	 0.23041 	 m..s
   91 	   109 	 0.15108 	 0.23224 	 m..s
  109 	   110 	 0.23780 	 0.23373 	 ~...
   98 	   111 	 0.17945 	 0.23590 	 m..s
   82 	   112 	 0.14582 	 0.23697 	 m..s
  101 	   113 	 0.21981 	 0.24744 	 ~...
  112 	   114 	 0.24330 	 0.25996 	 ~...
  118 	   115 	 0.25507 	 0.27668 	 ~...
  119 	   116 	 0.26528 	 0.29861 	 m..s
  116 	   117 	 0.24711 	 0.30182 	 m..s
  115 	   118 	 0.24582 	 0.30216 	 m..s
  117 	   119 	 0.25471 	 0.30443 	 m..s
  120 	   120 	 0.26883 	 0.31754 	 m..s
==========================================
r_mrr = 0.6572158336639404
r2_mrr = 0.36087048053741455
spearmanr_mrr@5 = 0.8523547053337097
spearmanr_mrr@10 = 0.834837019443512
spearmanr_mrr@50 = 0.854183554649353
spearmanr_mrr@100 = 0.8920440673828125
spearmanr_mrr@All = 0.9136959910392761
==========================================
test time: 0.39
Done Testing dataset CoDExSmall
total time taken: 191.30950546264648
training time taken: 181.241952419281
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6572)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.3609)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.8524)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.8348)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8542)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8920)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9137)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.342219228850809}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 4865937335602121
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1055, 1161, 0, 268, 55, 615, 1147, 285, 127, 1090, 418, 22, 313, 650, 480, 701, 484, 441, 751, 174, 910, 329, 711, 217, 662, 82, 1195, 1023, 1006, 457, 599, 558, 597, 353, 1018, 808, 271, 159, 476, 1100, 979, 557, 863, 538, 1191, 459, 641, 181, 422, 88, 300, 167, 496, 410, 115, 594, 371, 477, 1002, 565, 664, 344, 519, 391, 355, 109, 897, 70, 393, 1099, 499, 833, 603, 1039, 66, 1182, 535, 868, 908, 1091, 569, 302, 735, 988, 673, 246, 261, 307, 1128, 705, 258, 166, 502, 1185, 706, 514, 913, 992, 560, 1122, 610, 479, 63, 928, 243, 583, 1125, 952, 1050, 368, 450, 1213, 57, 256, 481, 561, 663, 257, 354, 1146, 428]
valid_ids (0): []
train_ids (1094): [80, 1035, 96, 634, 85, 782, 624, 43, 1202, 58, 991, 417, 239, 1065, 99, 28, 842, 339, 1133, 629, 3, 759, 554, 687, 406, 674, 32, 856, 654, 986, 971, 377, 1109, 154, 728, 815, 215, 1017, 400, 171, 108, 501, 273, 244, 320, 145, 446, 780, 505, 537, 464, 289, 297, 439, 581, 675, 164, 125, 282, 420, 52, 325, 1079, 158, 409, 324, 331, 838, 390, 413, 970, 186, 528, 612, 685, 704, 852, 295, 846, 608, 926, 357, 267, 894, 1108, 813, 298, 596, 632, 427, 882, 646, 720, 322, 493, 890, 1, 767, 648, 434, 198, 670, 948, 475, 415, 817, 911, 684, 1036, 254, 1180, 200, 68, 693, 540, 497, 916, 488, 382, 136, 1139, 772, 671, 487, 380, 350, 1003, 627, 474, 59, 930, 1208, 630, 750, 823, 379, 990, 375, 722, 702, 1145, 791, 1151, 444, 1193, 749, 605, 492, 53, 721, 1165, 549, 114, 276, 442, 881, 809, 618, 311, 511, 64, 983, 1038, 1061, 414, 907, 33, 194, 843, 1166, 253, 423, 788, 521, 977, 151, 31, 958, 678, 438, 577, 29, 1105, 279, 1027, 572, 367, 349, 814, 1007, 452, 697, 770, 607, 252, 707, 359, 1093, 968, 310, 1075, 250, 864, 216, 448, 5, 466, 1067, 1062, 976, 133, 898, 242, 424, 939, 544, 529, 604, 730, 1110, 163, 914, 639, 614, 854, 384, 912, 161, 1088, 628, 700, 798, 471, 736, 828, 709, 978, 202, 71, 472, 765, 555, 328, 1121, 389, 361, 517, 113, 110, 531, 48, 1092, 433, 606, 436, 146, 1207, 1011, 224, 197, 314, 695, 1179, 431, 399, 1203, 1005, 786, 762, 613, 92, 682, 394, 89, 713, 338, 90, 1137, 525, 965, 666, 345, 625, 232, 533, 172, 228, 46, 1021, 262, 1031, 130, 981, 7, 333, 909, 102, 223, 959, 1184, 771, 588, 1154, 879, 160, 369, 378, 611, 238, 951, 327, 1004, 1096, 12, 787, 547, 1160, 1149, 392, 1214, 723, 1025, 539, 1186, 315, 779, 822, 1008, 1115, 1032, 845, 857, 542, 974, 373, 653, 1033, 935, 587, 1010, 901, 1177, 1117, 938, 699, 348, 1116, 647, 893, 155, 1168, 1014, 601, 347, 1152, 411, 849, 11, 342, 169, 1150, 1054, 218, 995, 245, 803, 530, 138, 876, 177, 75, 884, 1140, 1045, 123, 463, 508, 1173, 1157, 1012, 826, 582, 103, 1049, 1174, 426, 987, 874, 1169, 638, 340, 287, 211, 567, 676, 408, 906, 1101, 1175, 84, 1148, 954, 1042, 686, 147, 1124, 343, 1172, 545, 150, 24, 17, 1070, 1041, 326, 967, 729, 21, 1136, 960, 78, 1197, 226, 1053, 972, 124, 1131, 510, 39, 1094, 998, 792, 235, 1156, 407, 1126, 563, 1026, 396, 225, 626, 805, 67, 458, 689, 1028, 144, 858, 934, 1142, 73, 131, 398, 830, 72, 1013, 896, 1051, 550, 937, 278, 506, 139, 187, 461, 299, 86, 421, 260, 1196, 366, 334, 173, 552, 395, 887, 34, 251, 30, 973, 1058, 1081, 571, 850, 284, 559, 761, 292, 275, 281, 1107, 575, 230, 1123, 790, 162, 1127, 157, 305, 668, 41, 240, 220, 236, 241, 500, 209, 925, 248, 207, 490, 467, 799, 963, 853, 383, 140, 49, 920, 1144, 1106, 744, 265, 755, 45, 1016, 919, 62, 902, 860, 997, 796, 401, 129, 742, 593, 91, 435, 941, 1155, 1095, 516, 358, 824, 376, 756, 494, 93, 316, 453, 1052, 523, 1143, 365, 25, 219, 1046, 969, 1057, 306, 449, 797, 694, 922, 107, 522, 135, 1076, 97, 768, 203, 753, 231, 121, 915, 622, 763, 748, 949, 984, 374, 69, 1135, 760, 841, 955, 179, 861, 489, 994, 943, 795, 385, 222, 623, 870, 586, 579, 512, 769, 35, 156, 341, 2, 364, 54, 717, 644, 176, 964, 255, 1212, 335, 180, 1060, 1112, 65, 775, 1114, 1043, 1086, 1190, 178, 580, 867, 405, 498, 134, 50, 1059, 773, 524, 945, 989, 195, 620, 76, 381, 658, 81, 142, 503, 1069, 286, 532, 570, 272, 309, 42, 738, 1118, 437, 456, 362, 18, 921, 486, 642, 1178, 621, 892, 757, 740, 865, 903, 714, 175, 266, 589, 568, 553, 1200, 1083, 304, 683, 840, 1015, 551, 1097, 478, 877, 672, 754, 468, 889, 708, 848, 534, 19, 527, 221, 346, 541, 895, 318, 206, 677, 758, 564, 932, 982, 386, 504, 105, 1111, 847, 283, 962, 412, 462, 372, 834, 189, 880, 578, 183, 126, 337, 652, 835, 1119, 227, 878, 56, 637, 746, 141, 1211, 270, 118, 387, 691, 120, 485, 844, 811, 592, 669, 851, 430, 839, 77, 956, 293, 1198, 831, 660, 402, 111, 429, 388, 1064, 659, 696, 731, 469, 900, 233, 716, 1044, 509, 192, 301, 872, 330, 360, 741, 518, 112, 184, 100, 36, 122, 931, 1089, 715, 781, 148, 303, 432, 288, 832, 1192, 739, 15, 566, 598, 319, 927, 836, 264, 980, 404, 536, 947, 1134, 13, 263, 869, 454, 196, 645, 655, 871, 837, 886, 212, 665, 4, 810, 1205, 933, 595, 576, 923, 515, 859, 445, 204, 710, 143, 1080, 363, 40, 294, 27, 680, 513, 416, 205, 208, 1162, 918, 117, 210, 229, 866, 1153, 74, 996, 60, 460, 829, 600, 1138, 734, 789, 774, 875, 153, 26, 1085, 336, 1048, 946, 280, 1072, 419, 1063, 816, 801, 95, 317, 1171, 584, 785, 985, 312, 1030, 793, 507, 104, 820, 899, 616, 905, 827, 885, 1037, 891, 1199, 649, 1129, 784, 1001, 259, 688, 602, 732, 247, 690, 548, 352, 1077, 465, 573, 1034, 745, 888, 291, 1098, 1141, 116, 679, 719, 1047, 1159, 802, 188, 783, 1084, 633, 296, 546, 79, 1024, 590, 447, 966, 617, 37, 483, 764, 942, 777, 993, 961, 778, 370, 149, 8, 323, 1040, 1000, 1074, 440, 718, 562, 574, 1188, 1164, 403, 940, 269, 185, 98, 168, 692, 1029, 950, 609, 1181, 543, 1102, 128, 1056, 152, 165, 234, 51, 862, 491, 87, 308, 635, 1163, 455, 743, 667, 94, 1130, 924, 20, 356, 277, 1078, 1113, 1132, 1183, 636, 1167, 591, 193, 1009, 1073, 651, 332, 213, 199, 556, 495, 137, 482, 643, 725, 526, 191, 657, 703, 61, 1066, 656, 201, 776, 470, 1120, 1189, 681, 6, 443, 47, 812, 727, 1206, 904, 807, 917, 1201, 747, 16, 726, 1104, 724, 1176, 1158, 975, 936, 1071, 1022, 818, 619, 1170, 14, 9, 929, 873, 821, 794, 883, 473, 1187, 451, 38, 855, 190, 766, 640, 1020, 804, 274, 712, 1019, 132, 397, 1209, 999, 10, 119, 520, 1210, 800, 752, 290, 170, 83, 351, 698, 631, 737, 214, 1194, 237, 953, 425, 23, 106, 944, 182, 1103, 585, 1087, 733, 321, 806, 1204, 825, 957, 44, 819, 1082, 661, 249, 1068, 101]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3234694198400357
the save name prefix for this run is:  chkpt-ID_3234694198400357_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 864
rank avg (pred): 0.562 +- 0.003
mrr vals (pred, true): 0.001, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002189644

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1070
rank avg (pred): 0.051 +- 0.028
mrr vals (pred, true): 0.113, 0.268
batch losses (mrrl, rdl): 0.0, 3.9338e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 443
rank avg (pred): 0.271 +- 0.171
mrr vals (pred, true): 0.147, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007355914

Epoch over!
epoch time: 12.111

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 340
rank avg (pred): 0.219 +- 0.158
mrr vals (pred, true): 0.232, 0.218
batch losses (mrrl, rdl): 0.0, 0.0006727609

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 140
rank avg (pred): 0.253 +- 0.188
mrr vals (pred, true): 0.251, 0.188
batch losses (mrrl, rdl): 0.0, 0.0008969205

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 789
rank avg (pred): 0.428 +- 0.359
mrr vals (pred, true): 0.306, 0.003
batch losses (mrrl, rdl): 0.0, 6.0074e-05

Epoch over!
epoch time: 11.829

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 894
rank avg (pred): 0.531 +- 0.386
mrr vals (pred, true): 0.224, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001073485

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 107
rank avg (pred): 0.246 +- 0.209
mrr vals (pred, true): 0.322, 0.189
batch losses (mrrl, rdl): 0.0, 0.0009706356

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.292 +- 0.238
mrr vals (pred, true): 0.240, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004552384

Epoch over!
epoch time: 11.912

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.286 +- 0.250
mrr vals (pred, true): 0.306, 0.005
batch losses (mrrl, rdl): 0.0, 0.0005186509

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 170
rank avg (pred): 0.204 +- 0.180
mrr vals (pred, true): 0.280, 0.004
batch losses (mrrl, rdl): 0.0, 0.0012529023

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 644
rank avg (pred): 0.310 +- 0.257
mrr vals (pred, true): 0.220, 0.165
batch losses (mrrl, rdl): 0.0, 0.0005648536

Epoch over!
epoch time: 11.765

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.045 +- 0.044
mrr vals (pred, true): 0.393, 0.256
batch losses (mrrl, rdl): 0.0, 1.3789e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 468
rank avg (pred): 0.295 +- 0.254
mrr vals (pred, true): 0.247, 0.006
batch losses (mrrl, rdl): 0.0, 0.0005098197

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 48
rank avg (pred): 0.045 +- 0.043
mrr vals (pred, true): 0.336, 0.229
batch losses (mrrl, rdl): 0.0, 3.2127e-06

Epoch over!
epoch time: 11.733

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 530
rank avg (pred): 0.184 +- 0.184
mrr vals (pred, true): 0.330, 0.038
batch losses (mrrl, rdl): 0.7825902104, 2.70563e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.012 +- 0.007
mrr vals (pred, true): 0.205, 0.295
batch losses (mrrl, rdl): 0.0796279609, 1.60996e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.226 +- 0.141
mrr vals (pred, true): 0.152, 0.212
batch losses (mrrl, rdl): 0.0359524526, 0.0006765791

Epoch over!
epoch time: 12.192

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 394
rank avg (pred): 0.330 +- 0.202
mrr vals (pred, true): 0.140, 0.229
batch losses (mrrl, rdl): 0.0788005814, 0.001824912

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.369 +- 0.217
mrr vals (pred, true): 0.111, 0.212
batch losses (mrrl, rdl): 0.102983579, 0.0023105349

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 676
rank avg (pred): 0.408 +- 0.217
mrr vals (pred, true): 0.094, 0.004
batch losses (mrrl, rdl): 0.0197386388, 5.27998e-05

Epoch over!
epoch time: 12.021

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 803
rank avg (pred): 0.607 +- 0.240
mrr vals (pred, true): 0.034, 0.004
batch losses (mrrl, rdl): 0.0025503172, 0.0002676033

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 333
rank avg (pred): 0.213 +- 0.129
mrr vals (pred, true): 0.150, 0.229
batch losses (mrrl, rdl): 0.0622558072, 0.0005685609

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 926
rank avg (pred): 0.621 +- 0.242
mrr vals (pred, true): 0.026, 0.001
batch losses (mrrl, rdl): 0.0056380457, 0.0005472941

Epoch over!
epoch time: 11.958

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 855
rank avg (pred): 0.473 +- 0.207
mrr vals (pred, true): 0.071, 0.063
batch losses (mrrl, rdl): 0.0044998182, 0.0014528112

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 510
rank avg (pred): 0.214 +- 0.123
mrr vals (pred, true): 0.106, 0.165
batch losses (mrrl, rdl): 0.0347318873, 0.0001573245

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 996
rank avg (pred): 0.034 +- 0.025
mrr vals (pred, true): 0.252, 0.291
batch losses (mrrl, rdl): 0.0151982903, 1.331e-06

Epoch over!
epoch time: 12.083

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 288
rank avg (pred): 0.018 +- 0.014
mrr vals (pred, true): 0.289, 0.299
batch losses (mrrl, rdl): 0.0009497377, 1.11926e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 469
rank avg (pred): 0.222 +- 0.127
mrr vals (pred, true): 0.128, 0.004
batch losses (mrrl, rdl): 0.0610886291, 0.0013927508

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 681
rank avg (pred): 0.423 +- 0.197
mrr vals (pred, true): 0.089, 0.004
batch losses (mrrl, rdl): 0.0148659199, 5.17473e-05

Epoch over!
epoch time: 12.174

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 463
rank avg (pred): 0.283 +- 0.159
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0574402735, 0.0007346002

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1052
rank avg (pred): 0.256 +- 0.147
mrr vals (pred, true): 0.122, 0.004
batch losses (mrrl, rdl): 0.0520765558, 0.0009769045

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 896
rank avg (pred): 0.499 +- 0.248
mrr vals (pred, true): 0.053, 0.002
batch losses (mrrl, rdl): 0.0001141202, 0.0002066431

Epoch over!
epoch time: 12.34

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1011
rank avg (pred): 0.262 +- 0.156
mrr vals (pred, true): 0.131, 0.205
batch losses (mrrl, rdl): 0.0554191731, 0.0010107822

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 922
rank avg (pred): 0.557 +- 0.233
mrr vals (pred, true): 0.041, 0.001
batch losses (mrrl, rdl): 0.0008460791, 0.0010370809

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 11
rank avg (pred): 0.058 +- 0.044
mrr vals (pred, true): 0.242, 0.241
batch losses (mrrl, rdl): 2.12117e-05, 1.3867e-05

Epoch over!
epoch time: 12.076

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.368 +- 0.176
mrr vals (pred, true): 0.092, 0.005
batch losses (mrrl, rdl): 0.0180025827, 0.0002510001

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 874
rank avg (pred): 0.451 +- 0.198
mrr vals (pred, true): 0.058, 0.004
batch losses (mrrl, rdl): 0.0005765187, 3.95507e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1150
rank avg (pred): 0.318 +- 0.183
mrr vals (pred, true): 0.117, 0.065
batch losses (mrrl, rdl): 0.0455459803, 0.0005344179

Epoch over!
epoch time: 12.017

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 316
rank avg (pred): 0.029 +- 0.024
mrr vals (pred, true): 0.263, 0.274
batch losses (mrrl, rdl): 0.0012609665, 4.4768e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 84
rank avg (pred): 0.275 +- 0.173
mrr vals (pred, true): 0.135, 0.203
batch losses (mrrl, rdl): 0.0464983471, 0.0009915089

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 517
rank avg (pred): 0.355 +- 0.173
mrr vals (pred, true): 0.094, 0.054
batch losses (mrrl, rdl): 0.0191451684, 0.0004617008

Epoch over!
epoch time: 11.904

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 429
rank avg (pred): 0.232 +- 0.155
mrr vals (pred, true): 0.133, 0.004
batch losses (mrrl, rdl): 0.0688059554, 0.0011711431

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.368 +- 0.167
mrr vals (pred, true): 0.098, 0.005
batch losses (mrrl, rdl): 0.023489641, 0.0002552276

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 374
rank avg (pred): 0.231 +- 0.161
mrr vals (pred, true): 0.148, 0.190
batch losses (mrrl, rdl): 0.0179959293, 0.000712767

Epoch over!
epoch time: 12.209

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.023 +- 0.021
mrr vals (pred, true): 0.270, 0.250

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.05213 	 0.00076 	 m..s
    5 	     1 	 0.06014 	 0.00094 	 m..s
    3 	     2 	 0.05264 	 0.00097 	 m..s
   44 	     3 	 0.10677 	 0.00104 	 MISS
    6 	     4 	 0.06478 	 0.00124 	 m..s
    7 	     5 	 0.06646 	 0.00251 	 m..s
   47 	     6 	 0.13342 	 0.00310 	 MISS
   59 	     7 	 0.13841 	 0.00326 	 MISS
    4 	     8 	 0.05714 	 0.00331 	 m..s
   46 	     9 	 0.13313 	 0.00342 	 MISS
   90 	    10 	 0.16745 	 0.00344 	 MISS
   91 	    11 	 0.16837 	 0.00350 	 MISS
    9 	    12 	 0.09197 	 0.00352 	 m..s
   85 	    13 	 0.15426 	 0.00353 	 MISS
   82 	    14 	 0.14887 	 0.00359 	 MISS
    9 	    15 	 0.09197 	 0.00363 	 m..s
    9 	    16 	 0.09197 	 0.00363 	 m..s
   55 	    17 	 0.13568 	 0.00368 	 MISS
    9 	    18 	 0.09197 	 0.00369 	 m..s
   94 	    19 	 0.17254 	 0.00380 	 MISS
    1 	    20 	 0.04803 	 0.00384 	 m..s
   79 	    21 	 0.14778 	 0.00385 	 MISS
   89 	    22 	 0.16696 	 0.00388 	 MISS
    9 	    23 	 0.09197 	 0.00390 	 m..s
   88 	    24 	 0.16634 	 0.00392 	 MISS
   93 	    25 	 0.17185 	 0.00393 	 MISS
   92 	    26 	 0.17092 	 0.00399 	 MISS
    9 	    27 	 0.09197 	 0.00399 	 m..s
   41 	    28 	 0.09606 	 0.00401 	 m..s
    9 	    29 	 0.09197 	 0.00411 	 m..s
   75 	    30 	 0.14588 	 0.00420 	 MISS
   60 	    31 	 0.13876 	 0.00424 	 MISS
   62 	    32 	 0.13919 	 0.00428 	 MISS
   53 	    33 	 0.13450 	 0.00432 	 MISS
    9 	    34 	 0.09197 	 0.00438 	 m..s
   40 	    35 	 0.09596 	 0.00441 	 m..s
   71 	    36 	 0.14229 	 0.00443 	 MISS
    9 	    37 	 0.09197 	 0.00451 	 m..s
   56 	    38 	 0.13642 	 0.00471 	 MISS
   61 	    39 	 0.13877 	 0.00474 	 MISS
   45 	    40 	 0.12839 	 0.00486 	 MISS
   65 	    41 	 0.14020 	 0.00493 	 MISS
    0 	    42 	 0.03744 	 0.00504 	 m..s
   72 	    43 	 0.14244 	 0.00531 	 MISS
   43 	    44 	 0.09950 	 0.00532 	 m..s
   42 	    45 	 0.09877 	 0.01197 	 m..s
    9 	    46 	 0.09197 	 0.02871 	 m..s
    9 	    47 	 0.09197 	 0.03267 	 m..s
    9 	    48 	 0.09197 	 0.03628 	 m..s
    8 	    49 	 0.08784 	 0.03688 	 m..s
    9 	    50 	 0.09197 	 0.03850 	 m..s
    9 	    51 	 0.09197 	 0.03903 	 m..s
    9 	    52 	 0.09197 	 0.03927 	 m..s
    9 	    53 	 0.09197 	 0.04130 	 m..s
    9 	    54 	 0.09197 	 0.04682 	 m..s
    9 	    55 	 0.09197 	 0.04818 	 m..s
   39 	    56 	 0.09212 	 0.04846 	 m..s
    9 	    57 	 0.09197 	 0.05290 	 m..s
    9 	    58 	 0.09197 	 0.11759 	 ~...
    9 	    59 	 0.09197 	 0.12053 	 ~...
    9 	    60 	 0.09197 	 0.12639 	 m..s
    9 	    61 	 0.09197 	 0.12878 	 m..s
    9 	    62 	 0.09197 	 0.13310 	 m..s
    9 	    63 	 0.09197 	 0.13439 	 m..s
    9 	    64 	 0.09197 	 0.14455 	 m..s
    9 	    65 	 0.09197 	 0.15517 	 m..s
   74 	    66 	 0.14585 	 0.15530 	 ~...
    9 	    67 	 0.09197 	 0.15568 	 m..s
   78 	    68 	 0.14650 	 0.15719 	 ~...
   81 	    69 	 0.14851 	 0.16163 	 ~...
    9 	    70 	 0.09197 	 0.16203 	 m..s
    9 	    71 	 0.09197 	 0.16628 	 m..s
   87 	    72 	 0.16426 	 0.16691 	 ~...
   96 	    73 	 0.19436 	 0.17869 	 ~...
   49 	    74 	 0.13378 	 0.17926 	 m..s
   66 	    75 	 0.14035 	 0.18132 	 m..s
   51 	    76 	 0.13398 	 0.18668 	 m..s
   97 	    77 	 0.20184 	 0.18886 	 ~...
   58 	    78 	 0.13829 	 0.18961 	 m..s
   48 	    79 	 0.13352 	 0.19682 	 m..s
   95 	    80 	 0.17633 	 0.19697 	 ~...
   69 	    81 	 0.14177 	 0.19709 	 m..s
   63 	    82 	 0.13949 	 0.19782 	 m..s
   84 	    83 	 0.15409 	 0.19823 	 m..s
   86 	    84 	 0.16231 	 0.19891 	 m..s
   64 	    85 	 0.14007 	 0.19965 	 m..s
   50 	    86 	 0.13381 	 0.19998 	 m..s
   68 	    87 	 0.14138 	 0.20033 	 m..s
   67 	    88 	 0.14078 	 0.20045 	 m..s
   76 	    89 	 0.14592 	 0.20313 	 m..s
  102 	    90 	 0.23213 	 0.20587 	 ~...
   80 	    91 	 0.14840 	 0.20622 	 m..s
   98 	    92 	 0.20739 	 0.20718 	 ~...
  112 	    93 	 0.24873 	 0.20793 	 m..s
   99 	    94 	 0.20780 	 0.20874 	 ~...
  100 	    95 	 0.20889 	 0.20901 	 ~...
  115 	    96 	 0.25274 	 0.21035 	 m..s
   52 	    97 	 0.13447 	 0.21151 	 m..s
   54 	    98 	 0.13458 	 0.21386 	 m..s
   70 	    99 	 0.14187 	 0.21389 	 m..s
  105 	   100 	 0.23787 	 0.21997 	 ~...
   57 	   101 	 0.13807 	 0.22038 	 m..s
  114 	   102 	 0.25256 	 0.22254 	 m..s
  103 	   103 	 0.23365 	 0.22278 	 ~...
  108 	   104 	 0.23862 	 0.22339 	 ~...
  106 	   105 	 0.23818 	 0.22374 	 ~...
  107 	   106 	 0.23848 	 0.22512 	 ~...
  101 	   107 	 0.23010 	 0.22602 	 ~...
  109 	   108 	 0.24216 	 0.22713 	 ~...
  110 	   109 	 0.24394 	 0.22974 	 ~...
   77 	   110 	 0.14625 	 0.23697 	 m..s
  113 	   111 	 0.25150 	 0.23772 	 ~...
   73 	   112 	 0.14453 	 0.23779 	 m..s
  104 	   113 	 0.23748 	 0.24029 	 ~...
   83 	   114 	 0.15243 	 0.24284 	 m..s
  119 	   115 	 0.27047 	 0.25028 	 ~...
  116 	   116 	 0.25287 	 0.25996 	 ~...
  111 	   117 	 0.24853 	 0.26308 	 ~...
  117 	   118 	 0.25585 	 0.27117 	 ~...
  118 	   119 	 0.26693 	 0.28857 	 ~...
  120 	   120 	 0.30244 	 0.31754 	 ~...
==========================================
r_mrr = 0.6668228507041931
r2_mrr = 0.3275182843208313
spearmanr_mrr@5 = 0.9795042872428894
spearmanr_mrr@10 = 0.9540428519248962
spearmanr_mrr@50 = 0.9132813811302185
spearmanr_mrr@100 = 0.8754452466964722
spearmanr_mrr@All = 0.8994544744491577
==========================================
test time: 0.412
Done Testing dataset CoDExSmall
total time taken: 191.08914256095886
training time taken: 180.8080575466156
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6668)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.3275)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9795)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9540)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9133)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8754)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8995)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.267381477504387}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7725030105057986
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [301, 116, 545, 655, 507, 808, 279, 620, 934, 531, 306, 774, 679, 510, 275, 336, 891, 85, 1150, 501, 63, 742, 1038, 413, 985, 441, 1044, 21, 383, 331, 417, 361, 1065, 257, 1119, 897, 863, 644, 15, 781, 1204, 427, 1132, 928, 231, 22, 740, 1072, 238, 1166, 31, 323, 834, 726, 298, 590, 734, 838, 699, 70, 552, 573, 93, 661, 146, 340, 927, 233, 641, 457, 779, 804, 20, 80, 468, 1196, 990, 518, 259, 855, 443, 579, 872, 302, 307, 540, 470, 90, 1016, 71, 596, 769, 1013, 1086, 992, 577, 1155, 1170, 627, 585, 938, 1, 646, 47, 462, 162, 278, 519, 756, 780, 473, 1001, 1141, 542, 304, 2, 913, 593, 1080, 961, 1022]
valid_ids (0): []
train_ids (1094): [55, 1151, 1164, 114, 725, 1110, 23, 430, 980, 280, 270, 757, 1039, 5, 475, 1193, 294, 1210, 1199, 467, 956, 801, 944, 782, 669, 532, 74, 1056, 724, 404, 1098, 563, 662, 766, 929, 344, 589, 460, 486, 712, 647, 185, 155, 1051, 17, 795, 166, 535, 628, 262, 654, 168, 131, 741, 14, 823, 1172, 702, 154, 525, 978, 659, 521, 458, 35, 260, 1178, 365, 342, 435, 497, 949, 343, 377, 536, 952, 1078, 369, 1027, 140, 332, 1055, 480, 715, 1106, 748, 642, 1198, 713, 558, 771, 72, 201, 737, 1123, 300, 657, 1111, 937, 1108, 833, 49, 348, 445, 656, 173, 447, 223, 451, 328, 407, 1003, 33, 264, 122, 719, 1160, 1149, 842, 547, 236, 248, 1100, 1168, 682, 494, 1214, 723, 574, 706, 971, 514, 106, 743, 676, 1006, 172, 1125, 539, 50, 947, 1101, 61, 925, 123, 1159, 568, 580, 455, 720, 45, 1138, 548, 556, 819, 469, 733, 716, 1209, 511, 115, 412, 605, 625, 767, 885, 1020, 1174, 1034, 516, 993, 564, 1052, 835, 75, 89, 1126, 1112, 1103, 479, 354, 253, 826, 637, 895, 129, 700, 554, 148, 82, 705, 1181, 493, 128, 1068, 1029, 915, 936, 1036, 1095, 143, 919, 698, 452, 324, 425, 630, 889, 973, 920, 738, 488, 345, 350, 164, 822, 42, 884, 764, 603, 388, 793, 492, 672, 239, 1122, 753, 1114, 214, 965, 312, 871, 401, 687, 308, 87, 193, 418, 776, 629, 138, 689, 101, 176, 600, 466, 640, 831, 946, 349, 330, 530, 960, 1010, 792, 69, 954, 1124, 703, 1066, 261, 760, 235, 569, 91, 775, 572, 132, 1088, 81, 180, 648, 1075, 597, 945, 761, 496, 645, 426, 1041, 843, 839, 228, 18, 245, 395, 456, 1045, 896, 989, 1043, 942, 582, 650, 1211, 249, 1037, 1182, 744, 181, 205, 732, 446, 624, 751, 272, 777, 914, 890, 635, 282, 633, 1208, 9, 99, 52, 88, 608, 851, 459, 288, 192, 387, 133, 1069, 537, 364, 359, 865, 51, 1186, 297, 1202, 873, 828, 182, 1053, 7, 799, 951, 158, 394, 184, 610, 1049, 905, 113, 218, 1116, 136, 888, 832, 190, 338, 968, 156, 159, 841, 255, 611, 879, 752, 199, 322, 675, 534, 24, 296, 111, 487, 878, 500, 363, 440, 506, 1063, 187, 119, 745, 276, 40, 754, 824, 68, 416, 287, 329, 232, 202, 333, 442, 549, 759, 571, 1131, 104, 43, 478, 722, 1105, 1017, 830, 1189, 439, 1008, 805, 415, 314, 1213, 988, 763, 846, 341, 327, 912, 1176, 875, 1084, 319, 709, 1067, 1060, 1071, 1096, 73, 996, 810, 584, 224, 557, 243, 286, 449, 1000, 592, 357, 171, 433, 562, 247, 1152, 409, 677, 267, 30, 38, 615, 870, 292, 790, 718, 538, 124, 178, 815, 921, 299, 567, 303, 482, 1021, 188, 693, 665, 1093, 750, 371, 317, 546, 150, 170, 561, 1135, 550, 398, 269, 1033, 707, 880, 935, 27, 126, 410, 1062, 768, 576, 903, 34, 599, 714, 421, 898, 522, 32, 673, 681, 972, 337, 382, 1073, 786, 432, 555, 708, 477, 678, 392, 553, 614, 1154, 200, 339, 1127, 785, 213, 840, 66, 623, 334, 1167, 103, 1087, 56, 802, 1134, 4, 1032, 220, 854, 244, 1142, 861, 6, 773, 1083, 995, 836, 606, 1023, 520, 64, 789, 523, 619, 1057, 1156, 121, 450, 651, 79, 994, 1185, 1194, 77, 1205, 977, 591, 1070, 618, 882, 403, 818, 1201, 527, 739, 108, 356, 899, 551, 174, 194, 736, 886, 1207, 290, 483, 429, 1148, 1143, 526, 755, 622, 1146, 346, 969, 1091, 1175, 1158, 222, 953, 1012, 820, 437, 316, 1042, 436, 320, 62, 251, 428, 959, 1104, 175, 1019, 313, 894, 691, 643, 83, 283, 1030, 604, 144, 422, 153, 680, 503, 1117, 731, 911, 1007, 25, 293, 97, 1128, 1092, 36, 1177, 533, 237, 474, 86, 570, 701, 987, 142, 632, 67, 495, 234, 844, 671, 966, 12, 216, 370, 970, 240, 660, 710, 916, 1015, 794, 639, 504, 940, 29, 607, 226, 609, 408, 102, 274, 950, 1058, 1097, 1048, 207, 285, 225, 1031, 717, 860, 509, 814, 221, 845, 380, 310, 908, 747, 1165, 362, 1139, 943, 37, 1035, 529, 1183, 137, 683, 490, 800, 857, 812, 867, 909, 163, 694, 204, 258, 697, 784, 246, 157, 385, 1157, 1147, 1018, 1107, 1144, 979, 621, 612, 256, 1094, 758, 997, 378, 864, 360, 1187, 1133, 183, 877, 721, 165, 930, 636, 829, 368, 263, 729, 1140, 1191, 100, 809, 684, 791, 229, 352, 1180, 438, 668, 1171, 955, 664, 1005, 57, 983, 400, 465, 197, 321, 666, 981, 453, 986, 1009, 1089, 134, 727, 964, 711, 230, 289, 852, 481, 1195, 749, 8, 1184, 318, 464, 1120, 783, 1059, 375, 167, 746, 1203, 941, 271, 856, 900, 817, 3, 1173, 76, 849, 141, 1077, 663, 414, 347, 59, 517, 353, 991, 1099, 16, 1047, 396, 811, 351, 859, 39, 196, 858, 893, 638, 728, 98, 94, 0, 125, 384, 420, 367, 444, 179, 847, 389, 219, 528, 393, 513, 565, 411, 907, 118, 982, 1024, 19, 373, 594, 1061, 797, 762, 1206, 559, 881, 1074, 78, 212, 962, 1102, 117, 160, 848, 177, 787, 967, 11, 53, 151, 386, 44, 311, 887, 406, 215, 381, 120, 397, 512, 827, 1130, 670, 326, 1064, 471, 1082, 203, 999, 1054, 379, 975, 883, 335, 1136, 273, 152, 112, 423, 926, 601, 26, 617, 575, 498, 765, 1081, 1161, 46, 515, 974, 667, 1090, 1169, 472, 1200, 135, 948, 505, 92, 1026, 252, 431, 217, 524, 130, 268, 1014, 1145, 209, 544, 807, 1115, 798, 54, 543, 295, 1076, 206, 391, 692, 602, 837, 1050, 109, 1011, 189, 541, 901, 254, 695, 476, 616, 566, 191, 922, 374, 910, 1153, 735, 850, 902, 658, 1197, 147, 690, 1046, 918, 1025, 1113, 1190, 1109, 778, 399, 502, 281, 816, 1163, 366, 241, 208, 957, 613, 402, 485, 499, 1085, 923, 508, 862, 95, 931, 1137, 376, 932, 419, 315, 250, 491, 372, 355, 998, 41, 578, 1188, 704, 560, 463, 1129, 906, 586, 796, 358, 461, 581, 853, 211, 96, 60, 917, 48, 107, 813, 868, 291, 127, 730, 325, 821, 933, 424, 65, 28, 110, 186, 806, 688, 169, 1004, 984, 454, 772, 788, 242, 1121, 198, 145, 595, 284, 869, 1162, 652, 305, 434, 924, 265, 1028, 583, 976, 277, 649, 390, 825, 1192, 803, 13, 195, 405, 770, 588, 161, 139, 266, 105, 587, 1002, 876, 674, 598, 1040, 1179, 1118, 149, 686, 634, 1212, 58, 84, 484, 210, 963, 874, 653, 10, 448, 892, 489, 631, 685, 866, 904, 1079, 696, 958, 227, 626, 309, 939]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8226097263966443
the save name prefix for this run is:  chkpt-ID_8226097263966443_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1127
rank avg (pred): 0.473 +- 0.004
mrr vals (pred, true): 0.001, 0.003
batch losses (mrrl, rdl): 0.0, 9.44649e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 310
rank avg (pred): 0.091 +- 0.062
mrr vals (pred, true): 0.065, 0.255
batch losses (mrrl, rdl): 0.0, 4.3872e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.459 +- 0.319
mrr vals (pred, true): 0.095, 0.039
batch losses (mrrl, rdl): 0.0, 0.0001829706

Epoch over!
epoch time: 11.99

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 980
rank avg (pred): 0.042 +- 0.033
mrr vals (pred, true): 0.255, 0.315
batch losses (mrrl, rdl): 0.0, 3.255e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 268
rank avg (pred): 0.026 +- 0.022
mrr vals (pred, true): 0.335, 0.210
batch losses (mrrl, rdl): 0.0, 1.1819e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 384
rank avg (pred): 0.245 +- 0.217
mrr vals (pred, true): 0.290, 0.222
batch losses (mrrl, rdl): 0.0, 0.0008986868

Epoch over!
epoch time: 11.916

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1169
rank avg (pred): 0.388 +- 0.312
mrr vals (pred, true): 0.263, 0.118
batch losses (mrrl, rdl): 0.0, 0.0005377169

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1195
rank avg (pred): 0.360 +- 0.306
mrr vals (pred, true): 0.283, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001369201

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 800
rank avg (pred): 0.446 +- 0.339
mrr vals (pred, true): 0.246, 0.004
batch losses (mrrl, rdl): 0.0, 3.01947e-05

Epoch over!
epoch time: 11.978

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 313
rank avg (pred): 0.055 +- 0.052
mrr vals (pred, true): 0.325, 0.260
batch losses (mrrl, rdl): 0.0, 3.1682e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1156
rank avg (pred): 0.162 +- 0.160
mrr vals (pred, true): 0.307, 0.048
batch losses (mrrl, rdl): 0.0, 7.8109e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.023 +- 0.023
mrr vals (pred, true): 0.355, 0.259
batch losses (mrrl, rdl): 0.0, 1.40234e-05

Epoch over!
epoch time: 12.042

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 993
rank avg (pred): 0.019 +- 0.020
mrr vals (pred, true): 0.444, 0.254
batch losses (mrrl, rdl): 0.0, 1.28695e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 4
rank avg (pred): 0.046 +- 0.047
mrr vals (pred, true): 0.371, 0.238
batch losses (mrrl, rdl): 0.0, 3.259e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 16
rank avg (pred): 0.032 +- 0.033
mrr vals (pred, true): 0.391, 0.252
batch losses (mrrl, rdl): 0.0, 4.092e-07

Epoch over!
epoch time: 12.112

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 145
rank avg (pred): 0.238 +- 0.239
mrr vals (pred, true): 0.303, 0.212
batch losses (mrrl, rdl): 0.0830364302, 0.0009205527

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 747
rank avg (pred): 0.195 +- 0.151
mrr vals (pred, true): 0.169, 0.166
batch losses (mrrl, rdl): 8.07854e-05, 0.0005084606

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 648
rank avg (pred): 0.370 +- 0.218
mrr vals (pred, true): 0.093, 0.005
batch losses (mrrl, rdl): 0.0182255227, 0.0001600636

Epoch over!
epoch time: 12.034

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.325 +- 0.191
mrr vals (pred, true): 0.099, 0.004
batch losses (mrrl, rdl): 0.0240959935, 0.0003880685

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 772
rank avg (pred): 0.460 +- 0.218
mrr vals (pred, true): 0.053, 0.021
batch losses (mrrl, rdl): 0.0001106046, 0.0001431445

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 494
rank avg (pred): 0.279 +- 0.189
mrr vals (pred, true): 0.127, 0.164
batch losses (mrrl, rdl): 0.0135568455, 0.0004504365

Epoch over!
epoch time: 11.949

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 68
rank avg (pred): 0.072 +- 0.059
mrr vals (pred, true): 0.204, 0.210
batch losses (mrrl, rdl): 0.0004090419, 1.23722e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 242
rank avg (pred): 0.254 +- 0.181
mrr vals (pred, true): 0.143, 0.003
batch losses (mrrl, rdl): 0.0864306241, 0.0009244482

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1115
rank avg (pred): 0.328 +- 0.184
mrr vals (pred, true): 0.093, 0.004
batch losses (mrrl, rdl): 0.0183782633, 0.0004373407

Epoch over!
epoch time: 12.141

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1110
rank avg (pred): 0.312 +- 0.180
mrr vals (pred, true): 0.110, 0.005
batch losses (mrrl, rdl): 0.0357527062, 0.0004667297

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1069
rank avg (pred): 0.034 +- 0.036
mrr vals (pred, true): 0.305, 0.286
batch losses (mrrl, rdl): 0.0039008837, 1.9085e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 879
rank avg (pred): 0.409 +- 0.175
mrr vals (pred, true): 0.066, 0.003
batch losses (mrrl, rdl): 0.0026495419, 9.97742e-05

Epoch over!
epoch time: 12.016

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 811
rank avg (pred): 0.278 +- 0.194
mrr vals (pred, true): 0.140, 0.149
batch losses (mrrl, rdl): 0.0008561516, 0.000749516

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.309 +- 0.164
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0074953837, 0.0005458875

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 685
rank avg (pred): 0.291 +- 0.163
mrr vals (pred, true): 0.093, 0.004
batch losses (mrrl, rdl): 0.0186379515, 0.000617837

Epoch over!
epoch time: 12.241

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1191
rank avg (pred): 0.372 +- 0.202
mrr vals (pred, true): 0.093, 0.005
batch losses (mrrl, rdl): 0.0180819798, 0.0001822407

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 893
rank avg (pred): 0.479 +- 0.217
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.65847e-05, 8.8387e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 955
rank avg (pred): 0.618 +- 0.270
mrr vals (pred, true): 0.019, 0.004
batch losses (mrrl, rdl): 0.0098090675, 0.0002690931

Epoch over!
epoch time: 12.451

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.032 +- 0.033
mrr vals (pred, true): 0.296, 0.298
batch losses (mrrl, rdl): 4.71277e-05, 2.1939e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 352
rank avg (pred): 0.240 +- 0.157
mrr vals (pred, true): 0.135, 0.212
batch losses (mrrl, rdl): 0.0588720441, 0.0007402268

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 755
rank avg (pred): 0.158 +- 0.115
mrr vals (pred, true): 0.167, 0.141
batch losses (mrrl, rdl): 0.0069027818, 0.000251943

Epoch over!
epoch time: 12.12

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 829
rank avg (pred): 0.113 +- 0.087
mrr vals (pred, true): 0.243, 0.204
batch losses (mrrl, rdl): 0.0159646906, 9.32748e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 482
rank avg (pred): 0.246 +- 0.157
mrr vals (pred, true): 0.136, 0.004
batch losses (mrrl, rdl): 0.0732669458, 0.0010927819

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 839
rank avg (pred): 0.507 +- 0.259
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0004078938, 8.7619e-06

Epoch over!
epoch time: 12.113

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 701
rank avg (pred): 0.301 +- 0.173
mrr vals (pred, true): 0.100, 0.004
batch losses (mrrl, rdl): 0.024913745, 0.0006112462

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 123
rank avg (pred): 0.240 +- 0.158
mrr vals (pred, true): 0.139, 0.231
batch losses (mrrl, rdl): 0.0844603851, 0.0008185465

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.160 +- 0.113
mrr vals (pred, true): 0.141, 0.162
batch losses (mrrl, rdl): 0.0048014983, 3.69762e-05

Epoch over!
epoch time: 12.151

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.524 +- 0.310
mrr vals (pred, true): 0.051, 0.006
batch losses (mrrl, rdl): 5.3121e-06, 3.38502e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 850
rank avg (pred): 0.393 +- 0.241
mrr vals (pred, true): 0.074, 0.046
batch losses (mrrl, rdl): 0.0058785458, 0.0001165114

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 587
rank avg (pred): 0.441 +- 0.306
mrr vals (pred, true): 0.086, 0.104
batch losses (mrrl, rdl): 0.0031909575, 0.0004756938

Epoch over!
epoch time: 12.021

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.065 +- 0.053
mrr vals (pred, true): 0.240, 0.218

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.03424 	 0.00075 	 m..s
    6 	     1 	 0.03467 	 0.00076 	 m..s
    3 	     2 	 0.03388 	 0.00076 	 m..s
    1 	     3 	 0.03137 	 0.00076 	 m..s
   16 	     4 	 0.05151 	 0.00097 	 m..s
    8 	     5 	 0.03664 	 0.00124 	 m..s
   18 	     6 	 0.05310 	 0.00149 	 m..s
   35 	     7 	 0.07564 	 0.00277 	 m..s
   40 	     8 	 0.08177 	 0.00280 	 m..s
   82 	     9 	 0.11828 	 0.00299 	 MISS
   55 	    10 	 0.10211 	 0.00313 	 m..s
    9 	    11 	 0.04768 	 0.00331 	 m..s
   78 	    12 	 0.11635 	 0.00332 	 MISS
   76 	    13 	 0.11460 	 0.00344 	 MISS
   53 	    14 	 0.10092 	 0.00350 	 m..s
   79 	    15 	 0.11667 	 0.00359 	 MISS
    4 	    16 	 0.03413 	 0.00363 	 m..s
   63 	    17 	 0.10525 	 0.00374 	 MISS
   77 	    18 	 0.11615 	 0.00387 	 MISS
    2 	    19 	 0.03345 	 0.00395 	 ~...
   56 	    20 	 0.10224 	 0.00399 	 m..s
   48 	    21 	 0.08769 	 0.00400 	 m..s
   14 	    22 	 0.04983 	 0.00407 	 m..s
   83 	    23 	 0.12005 	 0.00412 	 MISS
   65 	    24 	 0.10607 	 0.00416 	 MISS
   42 	    25 	 0.08267 	 0.00425 	 m..s
    0 	    26 	 0.03018 	 0.00441 	 ~...
   61 	    27 	 0.10492 	 0.00442 	 MISS
   84 	    28 	 0.12014 	 0.00452 	 MISS
   62 	    29 	 0.10513 	 0.00478 	 MISS
   67 	    30 	 0.10676 	 0.00483 	 MISS
   69 	    31 	 0.10821 	 0.00484 	 MISS
   41 	    32 	 0.08234 	 0.00493 	 m..s
   30 	    33 	 0.07401 	 0.00493 	 m..s
    7 	    34 	 0.03468 	 0.00495 	 ~...
   59 	    35 	 0.10422 	 0.00586 	 m..s
   80 	    36 	 0.11707 	 0.00587 	 MISS
   31 	    37 	 0.07443 	 0.00682 	 m..s
   10 	    38 	 0.04800 	 0.02208 	 ~...
   11 	    39 	 0.04822 	 0.02583 	 ~...
   20 	    40 	 0.05385 	 0.03452 	 ~...
   25 	    41 	 0.05518 	 0.03459 	 ~...
   24 	    42 	 0.05442 	 0.03688 	 ~...
   12 	    43 	 0.04876 	 0.04078 	 ~...
   13 	    44 	 0.04963 	 0.04276 	 ~...
   17 	    45 	 0.05301 	 0.04682 	 ~...
   19 	    46 	 0.05354 	 0.05100 	 ~...
   22 	    47 	 0.05428 	 0.05240 	 ~...
   23 	    48 	 0.05433 	 0.05331 	 ~...
   21 	    49 	 0.05398 	 0.05555 	 ~...
   27 	    50 	 0.05784 	 0.06145 	 ~...
   26 	    51 	 0.05613 	 0.06300 	 ~...
   28 	    52 	 0.06205 	 0.06515 	 ~...
   51 	    53 	 0.09306 	 0.07000 	 ~...
   15 	    54 	 0.05005 	 0.07395 	 ~...
   29 	    55 	 0.07367 	 0.09895 	 ~...
   32 	    56 	 0.07470 	 0.10487 	 m..s
   36 	    57 	 0.07586 	 0.10910 	 m..s
   38 	    58 	 0.07638 	 0.10912 	 m..s
   39 	    59 	 0.07816 	 0.12642 	 m..s
   87 	    60 	 0.14363 	 0.14024 	 ~...
   88 	    61 	 0.14564 	 0.14547 	 ~...
   45 	    62 	 0.08428 	 0.14931 	 m..s
   34 	    63 	 0.07547 	 0.15013 	 m..s
   33 	    64 	 0.07534 	 0.15042 	 m..s
   37 	    65 	 0.07591 	 0.15121 	 m..s
   44 	    66 	 0.08386 	 0.15346 	 m..s
   85 	    67 	 0.13563 	 0.15737 	 ~...
   43 	    68 	 0.08292 	 0.15958 	 m..s
   49 	    69 	 0.08800 	 0.16203 	 m..s
   90 	    70 	 0.15899 	 0.16268 	 ~...
   46 	    71 	 0.08440 	 0.16363 	 m..s
   47 	    72 	 0.08699 	 0.16518 	 m..s
   86 	    73 	 0.13935 	 0.16538 	 ~...
   89 	    74 	 0.15535 	 0.16556 	 ~...
   52 	    75 	 0.09413 	 0.18579 	 m..s
   50 	    76 	 0.09191 	 0.18944 	 m..s
   60 	    77 	 0.10490 	 0.19254 	 m..s
   66 	    78 	 0.10625 	 0.19479 	 m..s
   91 	    79 	 0.20754 	 0.20034 	 ~...
   68 	    80 	 0.10772 	 0.20079 	 m..s
   64 	    81 	 0.10546 	 0.20189 	 m..s
   70 	    82 	 0.10969 	 0.20256 	 m..s
   54 	    83 	 0.10104 	 0.20338 	 MISS
  102 	    84 	 0.24005 	 0.20587 	 m..s
  104 	    85 	 0.24042 	 0.20645 	 m..s
   93 	    86 	 0.21167 	 0.20718 	 ~...
   92 	    87 	 0.21096 	 0.20874 	 ~...
   94 	    88 	 0.21308 	 0.20936 	 ~...
   99 	    89 	 0.23952 	 0.21324 	 ~...
   97 	    90 	 0.23746 	 0.21434 	 ~...
   57 	    91 	 0.10270 	 0.21539 	 MISS
  100 	    92 	 0.23968 	 0.21758 	 ~...
   74 	    93 	 0.11127 	 0.21770 	 MISS
   96 	    94 	 0.23687 	 0.21798 	 ~...
   58 	    95 	 0.10287 	 0.21827 	 MISS
   73 	    96 	 0.11119 	 0.21875 	 MISS
  109 	    97 	 0.24385 	 0.21933 	 ~...
  103 	    98 	 0.24012 	 0.22059 	 ~...
   95 	    99 	 0.23655 	 0.22278 	 ~...
  101 	   100 	 0.23985 	 0.22339 	 ~...
   75 	   101 	 0.11337 	 0.22411 	 MISS
  106 	   102 	 0.24140 	 0.22627 	 ~...
   72 	   103 	 0.11092 	 0.22727 	 MISS
   81 	   104 	 0.11749 	 0.22931 	 MISS
   71 	   105 	 0.11039 	 0.22971 	 MISS
  113 	   106 	 0.24725 	 0.23105 	 ~...
  117 	   107 	 0.25150 	 0.23316 	 ~...
   98 	   108 	 0.23753 	 0.23319 	 ~...
  105 	   109 	 0.24084 	 0.23702 	 ~...
  115 	   110 	 0.24910 	 0.23772 	 ~...
  111 	   111 	 0.24505 	 0.23940 	 ~...
  112 	   112 	 0.24541 	 0.23944 	 ~...
  116 	   113 	 0.24934 	 0.24792 	 ~...
  107 	   114 	 0.24327 	 0.25300 	 ~...
  114 	   115 	 0.24743 	 0.25578 	 ~...
  119 	   116 	 0.26323 	 0.25648 	 ~...
  118 	   117 	 0.25497 	 0.25657 	 ~...
  108 	   118 	 0.24331 	 0.26308 	 ~...
  110 	   119 	 0.24427 	 0.27856 	 m..s
  120 	   120 	 0.28725 	 0.28516 	 ~...
==========================================
r_mrr = 0.7520107626914978
r2_mrr = 0.5614614486694336
spearmanr_mrr@5 = 0.9167070388793945
spearmanr_mrr@10 = 0.8945829272270203
spearmanr_mrr@50 = 0.8621744513511658
spearmanr_mrr@100 = 0.8579511642456055
spearmanr_mrr@All = 0.8946243524551392
==========================================
test time: 0.478
Done Testing dataset CoDExSmall
total time taken: 191.66630506515503
training time taken: 181.82357454299927
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7520)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.5615)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9167)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.8946)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8622)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8580)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8946)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.2047562429543177}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4486418957022485
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [781, 336, 1017, 486, 670, 353, 512, 880, 26, 296, 237, 813, 615, 1012, 567, 6, 1152, 41, 613, 653, 1010, 1026, 110, 927, 471, 689, 423, 298, 162, 439, 33, 765, 735, 150, 87, 273, 1071, 342, 306, 1110, 349, 231, 258, 105, 165, 27, 483, 704, 28, 370, 576, 648, 164, 404, 772, 827, 234, 1135, 601, 973, 1167, 149, 843, 1041, 818, 35, 681, 588, 1128, 718, 773, 147, 1196, 1168, 130, 756, 794, 538, 351, 767, 463, 55, 989, 1137, 388, 760, 786, 1008, 1183, 882, 184, 350, 699, 762, 895, 1164, 253, 341, 856, 1073, 684, 1134, 53, 1015, 698, 94, 708, 933, 1214, 870, 865, 69, 955, 346, 1057, 326, 1087, 811, 1132, 330, 261]
valid_ids (0): []
train_ids (1094): [858, 700, 1031, 707, 995, 706, 731, 355, 380, 469, 1066, 948, 751, 280, 91, 36, 182, 50, 76, 254, 673, 745, 730, 826, 966, 978, 289, 920, 195, 339, 1170, 92, 444, 1011, 119, 474, 1004, 901, 816, 1206, 787, 20, 1051, 835, 657, 557, 450, 1160, 558, 8, 726, 935, 961, 1146, 153, 801, 930, 623, 1081, 80, 1093, 982, 410, 1121, 859, 1068, 1104, 470, 593, 1097, 1101, 2, 9, 1193, 127, 467, 722, 112, 603, 912, 664, 286, 74, 671, 1161, 759, 1033, 386, 154, 403, 609, 190, 903, 1151, 125, 238, 1067, 727, 16, 1117, 608, 445, 392, 947, 1118, 1094, 161, 625, 1116, 244, 1016, 540, 270, 179, 146, 646, 391, 1169, 305, 301, 831, 38, 495, 304, 285, 923, 54, 579, 577, 299, 916, 95, 658, 318, 728, 227, 878, 365, 72, 778, 398, 1034, 1083, 1032, 1047, 115, 93, 451, 476, 57, 313, 390, 884, 1019, 14, 929, 1158, 1058, 846, 531, 986, 460, 1143, 458, 205, 833, 1150, 457, 120, 368, 260, 1029, 847, 823, 627, 564, 712, 491, 784, 523, 957, 186, 377, 893, 854, 62, 268, 48, 743, 331, 1042, 675, 1106, 275, 357, 199, 369, 128, 871, 586, 639, 534, 307, 433, 178, 183, 171, 1014, 716, 951, 672, 240, 714, 894, 347, 628, 266, 739, 156, 630, 425, 886, 66, 96, 409, 375, 1197, 1124, 315, 605, 747, 251, 552, 524, 60, 659, 908, 405, 416, 785, 1075, 1200, 780, 1095, 553, 1102, 49, 389, 802, 513, 544, 562, 983, 909, 230, 505, 1122, 725, 1157, 761, 245, 382, 817, 1105, 1192, 427, 1063, 809, 497, 1202, 311, 876, 533, 116, 99, 297, 1030, 434, 1089, 796, 1074, 293, 309, 791, 898, 148, 566, 1144, 1207, 848, 507, 600, 447, 272, 89, 1209, 666, 1055, 111, 1006, 257, 776, 192, 461, 1018, 489, 738, 994, 1025, 656, 713, 680, 132, 749, 873, 508, 494, 742, 555, 1085, 674, 842, 526, 988, 1082, 103, 345, 175, 967, 1153, 78, 7, 492, 201, 650, 549, 1145, 734, 34, 705, 755, 1024, 965, 741, 757, 4, 678, 883, 70, 866, 372, 560, 71, 233, 314, 117, 750, 599, 852, 1129, 188, 431, 0, 753, 770, 550, 922, 950, 721, 44, 595, 11, 321, 928, 194, 424, 931, 596, 411, 343, 1187, 1127, 122, 1096, 335, 1072, 906, 374, 527, 913, 956, 276, 793, 782, 732, 442, 294, 1115, 1022, 572, 998, 107, 621, 774, 582, 277, 319, 963, 585, 75, 815, 529, 992, 643, 212, 701, 807, 64, 200, 252, 652, 869, 626, 1023, 758, 530, 979, 1046, 185, 402, 547, 729, 918, 970, 812, 10, 262, 581, 971, 209, 1159, 422, 810, 1125, 239, 669, 337, 56, 449, 1060, 975, 287, 717, 429, 647, 622, 768, 448, 216, 136, 518, 393, 406, 151, 720, 1009, 946, 159, 620, 528, 24, 584, 651, 248, 949, 267, 302, 515, 84, 500, 464, 934, 366, 889, 269, 378, 493, 1007, 954, 490, 17, 855, 207, 271, 316, 358, 303, 635, 911, 1103, 1002, 475, 30, 215, 376, 1162, 644, 247, 1027, 872, 710, 224, 925, 1163, 535, 1155, 541, 1133, 479, 545, 839, 536, 228, 1079, 81, 754, 636, 687, 1107, 897, 959, 452, 612, 1140, 1204, 373, 462, 829, 59, 259, 875, 282, 969, 520, 210, 624, 568, 677, 556, 1005, 891, 1194, 1184, 783, 487, 1199, 438, 1111, 703, 1176, 841, 384, 142, 940, 400, 1208, 638, 395, 25, 551, 1044, 362, 225, 29, 432, 332, 481, 418, 12, 591, 1165, 173, 58, 454, 915, 1172, 1003, 22, 746, 1203, 1213, 1136, 124, 804, 850, 694, 607, 459, 1077, 740, 942, 715, 455, 1195, 408, 498, 300, 667, 999, 905, 921, 900, 141, 68, 163, 435, 1039, 691, 800, 598, 21, 1108, 39, 98, 805, 480, 436, 143, 288, 137, 851, 256, 86, 232, 795, 764, 665, 616, 1040, 594, 709, 1201, 291, 1109, 574, 61, 797, 501, 168, 695, 32, 1182, 340, 844, 597, 1064, 140, 690, 1084, 939, 77, 104, 121, 421, 1054, 611, 539, 1001, 155, 981, 1123, 13, 655, 1174, 509, 697, 619, 100, 937, 821, 219, 649, 176, 1099, 1154, 1038, 338, 1112, 868, 102, 144, 82, 881, 312, 885, 354, 532, 634, 85, 985, 663, 478, 443, 348, 1139, 437, 1088, 170, 18, 236, 792, 902, 1037, 836, 968, 246, 158, 419, 67, 990, 504, 208, 676, 1185, 468, 1119, 367, 472, 242, 1070, 222, 97, 838, 662, 1212, 1211, 641, 352, 633, 223, 977, 5, 590, 417, 1062, 1086, 363, 1198, 693, 1180, 1156, 629, 583, 737, 592, 135, 604, 860, 323, 456, 134, 679, 385, 642, 1148, 941, 1126, 537, 1113, 1028, 887, 840, 514, 265, 571, 1191, 295, 322, 221, 867, 484, 1050, 334, 640, 580, 845, 453, 1065, 1013, 814, 888, 379, 991, 1061, 565, 465, 283, 861, 711, 987, 788, 919, 775, 241, 51, 229, 145, 1171, 356, 863, 290, 1020, 779, 129, 837, 1166, 1090, 522, 736, 19, 381, 953, 668, 413, 1053, 849, 569, 958, 325, 777, 575, 570, 281, 654, 561, 879, 926, 101, 1076, 1138, 972, 213, 1059, 1120, 139, 503, 440, 399, 692, 198, 521, 15, 1181, 359, 106, 1098, 853, 748, 789, 415, 420, 324, 752, 798, 892, 1078, 719, 1147, 177, 1175, 563, 974, 573, 1210, 542, 361, 525, 226, 733, 278, 546, 292, 167, 364, 1186, 255, 686, 519, 360, 1036, 264, 45, 138, 396, 412, 1049, 83, 114, 1205, 1021, 407, 46, 499, 799, 90, 133, 661, 830, 113, 40, 63, 511, 877, 993, 214, 1100, 485, 1069, 907, 344, 310, 1091, 702, 820, 169, 160, 824, 263, 328, 73, 47, 1092, 414, 174, 506, 517, 862, 1048, 126, 790, 944, 825, 31, 473, 1177, 217, 203, 181, 88, 193, 383, 936, 187, 397, 166, 211, 960, 688, 932, 744, 803, 52, 1190, 543, 724, 284, 482, 890, 249, 202, 945, 152, 387, 682, 606, 1179, 488, 1000, 819, 118, 874, 502, 279, 317, 578, 832, 371, 206, 610, 614, 723, 1056, 632, 996, 763, 769, 980, 131, 822, 1130, 976, 172, 917, 1178, 696, 308, 1052, 1149, 997, 554, 1142, 466, 196, 65, 864, 510, 3, 618, 899, 548, 394, 37, 204, 1189, 1114, 857, 329, 645, 430, 43, 587, 631, 1188, 617, 962, 180, 683, 426, 896, 952, 984, 766, 964, 250, 943, 191, 559, 660, 1131, 637, 428, 218, 1045, 516, 157, 23, 108, 477, 924, 320, 243, 42, 1035, 1, 1080, 327, 806, 197, 401, 274, 589, 685, 1173, 1141, 446, 441, 834, 904, 828, 109, 914, 496, 808, 189, 1043, 333, 79, 938, 235, 771, 123, 602, 220, 910]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7675666536263713
the save name prefix for this run is:  chkpt-ID_7675666536263713_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.568 +- 0.006
mrr vals (pred, true): 0.001, 0.200
batch losses (mrrl, rdl): 0.0, 0.0056648464

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.229 +- 0.151
mrr vals (pred, true): 0.069, 0.004
batch losses (mrrl, rdl): 0.0, 0.0011543263

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.397 +- 0.299
mrr vals (pred, true): 0.193, 0.005
batch losses (mrrl, rdl): 0.0, 3.43221e-05

Epoch over!
epoch time: 12.095

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 584
rank avg (pred): 0.305 +- 0.234
mrr vals (pred, true): 0.222, 0.104
batch losses (mrrl, rdl): 0.0, 8.54159e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 740
rank avg (pred): 0.082 +- 0.070
mrr vals (pred, true): 0.340, 0.166
batch losses (mrrl, rdl): 0.0, 2.72783e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 4
rank avg (pred): 0.032 +- 0.028
mrr vals (pred, true): 0.376, 0.238
batch losses (mrrl, rdl): 0.0, 9.933e-07

Epoch over!
epoch time: 11.83

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 376
rank avg (pred): 0.253 +- 0.215
mrr vals (pred, true): 0.317, 0.215
batch losses (mrrl, rdl): 0.0, 0.0009705959

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1121
rank avg (pred): 0.259 +- 0.229
mrr vals (pred, true): 0.351, 0.005
batch losses (mrrl, rdl): 0.0, 0.0007341169

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 717
rank avg (pred): 0.335 +- 0.271
mrr vals (pred, true): 0.239, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002413708

Epoch over!
epoch time: 11.898

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 143
rank avg (pred): 0.275 +- 0.236
mrr vals (pred, true): 0.243, 0.202
batch losses (mrrl, rdl): 0.0, 0.0012163016

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.265 +- 0.251
mrr vals (pred, true): 0.271, 0.149
batch losses (mrrl, rdl): 0.0, 0.0002731763

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1038
rank avg (pred): 0.234 +- 0.223
mrr vals (pred, true): 0.284, 0.004
batch losses (mrrl, rdl): 0.0, 0.0008572646

Epoch over!
epoch time: 11.92

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 301
rank avg (pred): 0.073 +- 0.069
mrr vals (pred, true): 0.314, 0.218
batch losses (mrrl, rdl): 0.0, 9.9845e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1103
rank avg (pred): 0.226 +- 0.217
mrr vals (pred, true): 0.282, 0.198
batch losses (mrrl, rdl): 0.0, 0.000762235

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 278
rank avg (pred): 0.058 +- 0.056
mrr vals (pred, true): 0.342, 0.226
batch losses (mrrl, rdl): 0.0, 3.5814e-06

Epoch over!
epoch time: 11.886

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 122
rank avg (pred): 0.264 +- 0.241
mrr vals (pred, true): 0.262, 0.199
batch losses (mrrl, rdl): 0.0403951481, 0.0011440946

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 883
rank avg (pred): 0.672 +- 0.318
mrr vals (pred, true): 0.090, 0.005
batch losses (mrrl, rdl): 0.0162644256, 0.0008388003

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.404 +- 0.201
mrr vals (pred, true): 0.093, 0.166
batch losses (mrrl, rdl): 0.0535500646, 0.0015343559

Epoch over!
epoch time: 12.303

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 420
rank avg (pred): 0.332 +- 0.201
mrr vals (pred, true): 0.112, 0.004
batch losses (mrrl, rdl): 0.038483806, 0.0003825715

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 173
rank avg (pred): 0.358 +- 0.217
mrr vals (pred, true): 0.131, 0.003
batch losses (mrrl, rdl): 0.0649542436, 0.0002479181

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 453
rank avg (pred): 0.235 +- 0.146
mrr vals (pred, true): 0.128, 0.004
batch losses (mrrl, rdl): 0.0609974898, 0.0012023494

Epoch over!
epoch time: 12.138

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.396 +- 0.188
mrr vals (pred, true): 0.092, 0.178
batch losses (mrrl, rdl): 0.0733339787, 0.0015248572

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 142
rank avg (pred): 0.298 +- 0.179
mrr vals (pred, true): 0.118, 0.203
batch losses (mrrl, rdl): 0.0715732425, 0.0012994581

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.025 +- 0.016
mrr vals (pred, true): 0.198, 0.243
batch losses (mrrl, rdl): 0.0198399369, 2.3931e-06

Epoch over!
epoch time: 12.152

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 596
rank avg (pred): 0.366 +- 0.189
mrr vals (pred, true): 0.110, 0.153
batch losses (mrrl, rdl): 0.0190903451, 0.0007797822

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 403
rank avg (pred): 0.276 +- 0.163
mrr vals (pred, true): 0.125, 0.219
batch losses (mrrl, rdl): 0.0894517899, 0.0011586749

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.329, 0.295
batch losses (mrrl, rdl): 0.0115586082, 2.68595e-05

Epoch over!
epoch time: 12.273

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 284
rank avg (pred): 0.017 +- 0.011
mrr vals (pred, true): 0.216, 0.277
batch losses (mrrl, rdl): 0.0371643715, 1.30796e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 223
rank avg (pred): 0.294 +- 0.177
mrr vals (pred, true): 0.146, 0.003
batch losses (mrrl, rdl): 0.0926420838, 0.0006508577

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 313
rank avg (pred): 0.011 +- 0.007
mrr vals (pred, true): 0.240, 0.260
batch losses (mrrl, rdl): 0.0041831508, 3.15368e-05

Epoch over!
epoch time: 12.151

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 378
rank avg (pred): 0.237 +- 0.146
mrr vals (pred, true): 0.155, 0.230
batch losses (mrrl, rdl): 0.0569707938, 0.000715377

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.275, 0.298
batch losses (mrrl, rdl): 0.0051146587, 2.292e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1162
rank avg (pred): 0.349 +- 0.161
mrr vals (pred, true): 0.102, 0.075
batch losses (mrrl, rdl): 0.0274647493, 0.0001250759

Epoch over!
epoch time: 12.184

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 503
rank avg (pred): 0.322 +- 0.178
mrr vals (pred, true): 0.120, 0.152
batch losses (mrrl, rdl): 0.0104233548, 0.0006991514

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 967
rank avg (pred): 0.602 +- 0.192
mrr vals (pred, true): 0.040, 0.004
batch losses (mrrl, rdl): 0.0010427567, 0.0003082906

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 447
rank avg (pred): 0.242 +- 0.144
mrr vals (pred, true): 0.156, 0.005
batch losses (mrrl, rdl): 0.1123257726, 0.0010481402

Epoch over!
epoch time: 12.054

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 182
rank avg (pred): 0.328 +- 0.175
mrr vals (pred, true): 0.113, 0.004
batch losses (mrrl, rdl): 0.0396473669, 0.0003492902

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 67
rank avg (pred): 0.019 +- 0.011
mrr vals (pred, true): 0.209, 0.205
batch losses (mrrl, rdl): 0.000178169, 2.37644e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.335, 0.298
batch losses (mrrl, rdl): 0.0139489323, 2.91504e-05

Epoch over!
epoch time: 12.164

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 442
rank avg (pred): 0.279 +- 0.163
mrr vals (pred, true): 0.147, 0.004
batch losses (mrrl, rdl): 0.0940429494, 0.0007401388

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.366 +- 0.126
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 5.227e-05, 0.0002976234

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 942
rank avg (pred): 0.477 +- 0.201
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.0003488582, 0.001742978

Epoch over!
epoch time: 12.141

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.349 +- 0.133
mrr vals (pred, true): 0.051, 0.040
batch losses (mrrl, rdl): 3.3108e-06, 0.0003818644

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1188
rank avg (pred): 0.339 +- 0.153
mrr vals (pred, true): 0.100, 0.005
batch losses (mrrl, rdl): 0.0248219818, 0.000429184

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 971
rank avg (pred): 0.521 +- 0.224
mrr vals (pred, true): 0.042, 0.004
batch losses (mrrl, rdl): 0.0006505144, 4.06245e-05

Epoch over!
epoch time: 12.379

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.802 +- 0.206
mrr vals (pred, true): 0.042, 0.022

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04183 	 0.00076 	 m..s
    1 	     1 	 0.03907 	 0.00076 	 m..s
   21 	     2 	 0.05961 	 0.00104 	 m..s
    0 	     3 	 0.03724 	 0.00130 	 m..s
   79 	     4 	 0.12136 	 0.00299 	 MISS
   50 	     5 	 0.10360 	 0.00341 	 MISS
   73 	     6 	 0.11838 	 0.00344 	 MISS
   70 	     7 	 0.11795 	 0.00361 	 MISS
   87 	     8 	 0.12284 	 0.00371 	 MISS
   77 	     9 	 0.12068 	 0.00371 	 MISS
   49 	    10 	 0.10320 	 0.00372 	 m..s
   19 	    11 	 0.05916 	 0.00380 	 m..s
   18 	    12 	 0.05809 	 0.00388 	 m..s
   57 	    13 	 0.10614 	 0.00392 	 MISS
    3 	    14 	 0.04186 	 0.00393 	 m..s
   60 	    15 	 0.10660 	 0.00394 	 MISS
   37 	    16 	 0.09496 	 0.00401 	 m..s
   43 	    17 	 0.09944 	 0.00411 	 m..s
   86 	    18 	 0.12248 	 0.00416 	 MISS
   68 	    19 	 0.11445 	 0.00416 	 MISS
   89 	    20 	 0.12414 	 0.00424 	 MISS
   36 	    21 	 0.09417 	 0.00425 	 m..s
   42 	    22 	 0.09926 	 0.00426 	 m..s
   56 	    23 	 0.10602 	 0.00432 	 MISS
   15 	    24 	 0.05178 	 0.00438 	 m..s
    7 	    25 	 0.04610 	 0.00438 	 m..s
   40 	    26 	 0.09737 	 0.00439 	 m..s
   14 	    27 	 0.05171 	 0.00441 	 m..s
   45 	    28 	 0.10030 	 0.00442 	 m..s
   88 	    29 	 0.12328 	 0.00443 	 MISS
   27 	    30 	 0.07557 	 0.00451 	 m..s
   48 	    31 	 0.10162 	 0.00451 	 m..s
   90 	    32 	 0.12474 	 0.00463 	 MISS
   66 	    33 	 0.11416 	 0.00464 	 MISS
   10 	    34 	 0.05045 	 0.00467 	 m..s
   32 	    35 	 0.08158 	 0.00478 	 m..s
   93 	    36 	 0.12715 	 0.00483 	 MISS
   61 	    37 	 0.10676 	 0.00484 	 MISS
   24 	    38 	 0.07513 	 0.00493 	 m..s
   41 	    39 	 0.09919 	 0.00559 	 m..s
   35 	    40 	 0.08320 	 0.00578 	 m..s
   13 	    41 	 0.05163 	 0.00638 	 m..s
   11 	    42 	 0.05077 	 0.01819 	 m..s
    5 	    43 	 0.04192 	 0.02051 	 ~...
    4 	    44 	 0.04190 	 0.02089 	 ~...
   12 	    45 	 0.05129 	 0.02123 	 m..s
    6 	    46 	 0.04226 	 0.02208 	 ~...
   22 	    47 	 0.06037 	 0.03246 	 ~...
   17 	    48 	 0.05798 	 0.03331 	 ~...
   23 	    49 	 0.06062 	 0.03903 	 ~...
   26 	    50 	 0.07537 	 0.04277 	 m..s
   20 	    51 	 0.05922 	 0.05472 	 ~...
   16 	    52 	 0.05181 	 0.05678 	 ~...
    9 	    53 	 0.04813 	 0.07487 	 ~...
    8 	    54 	 0.04634 	 0.07522 	 ~...
   29 	    55 	 0.07706 	 0.10123 	 ~...
   33 	    56 	 0.08190 	 0.10334 	 ~...
   31 	    57 	 0.08083 	 0.10502 	 ~...
   34 	    58 	 0.08292 	 0.10602 	 ~...
   30 	    59 	 0.07763 	 0.11465 	 m..s
   38 	    60 	 0.09586 	 0.13439 	 m..s
   44 	    61 	 0.10011 	 0.13805 	 m..s
   96 	    62 	 0.14429 	 0.14456 	 ~...
   25 	    63 	 0.07519 	 0.14724 	 m..s
   28 	    64 	 0.07682 	 0.14888 	 m..s
   47 	    65 	 0.10141 	 0.15735 	 m..s
   95 	    66 	 0.14364 	 0.15803 	 ~...
   94 	    67 	 0.13139 	 0.16249 	 m..s
   92 	    68 	 0.12710 	 0.16262 	 m..s
   39 	    69 	 0.09692 	 0.16391 	 m..s
   97 	    70 	 0.14611 	 0.16862 	 ~...
   58 	    71 	 0.10621 	 0.17763 	 m..s
   99 	    72 	 0.19686 	 0.17869 	 ~...
   64 	    73 	 0.10711 	 0.18342 	 m..s
   54 	    74 	 0.10542 	 0.18527 	 m..s
   67 	    75 	 0.11439 	 0.18668 	 m..s
   98 	    76 	 0.19446 	 0.19153 	 ~...
   69 	    77 	 0.11773 	 0.19390 	 m..s
  100 	    78 	 0.19694 	 0.19708 	 ~...
  104 	    79 	 0.20107 	 0.19762 	 ~...
   84 	    80 	 0.12239 	 0.19778 	 m..s
   72 	    81 	 0.11834 	 0.19780 	 m..s
  102 	    82 	 0.19911 	 0.19977 	 ~...
   53 	    83 	 0.10490 	 0.20127 	 m..s
   71 	    84 	 0.11805 	 0.20140 	 m..s
   62 	    85 	 0.10692 	 0.20155 	 m..s
   83 	    86 	 0.12233 	 0.20206 	 m..s
   65 	    87 	 0.11408 	 0.20532 	 m..s
  101 	    88 	 0.19700 	 0.20665 	 ~...
   81 	    89 	 0.12193 	 0.20842 	 m..s
   85 	    90 	 0.12248 	 0.20890 	 m..s
   63 	    91 	 0.10698 	 0.21083 	 MISS
  105 	    92 	 0.20518 	 0.21103 	 ~...
   51 	    93 	 0.10445 	 0.21117 	 MISS
  106 	    94 	 0.21509 	 0.21324 	 ~...
   76 	    95 	 0.12040 	 0.21447 	 m..s
  115 	    96 	 0.24323 	 0.21487 	 ~...
   78 	    97 	 0.12080 	 0.21594 	 m..s
   52 	    98 	 0.10463 	 0.21763 	 MISS
   91 	    99 	 0.12522 	 0.21935 	 m..s
   46 	   100 	 0.10110 	 0.21956 	 MISS
  117 	   101 	 0.24609 	 0.22199 	 ~...
  114 	   102 	 0.23761 	 0.22254 	 ~...
   74 	   103 	 0.11844 	 0.22466 	 MISS
  112 	   104 	 0.22249 	 0.22495 	 ~...
   80 	   105 	 0.12179 	 0.22649 	 MISS
  107 	   106 	 0.21824 	 0.22688 	 ~...
  108 	   107 	 0.21949 	 0.22713 	 ~...
   75 	   108 	 0.11988 	 0.22910 	 MISS
   59 	   109 	 0.10657 	 0.22917 	 MISS
   55 	   110 	 0.10573 	 0.22931 	 MISS
  110 	   111 	 0.22226 	 0.23266 	 ~...
  109 	   112 	 0.22066 	 0.23373 	 ~...
   82 	   113 	 0.12205 	 0.24087 	 MISS
  113 	   114 	 0.22719 	 0.25263 	 ~...
  111 	   115 	 0.22239 	 0.25300 	 m..s
  103 	   116 	 0.20062 	 0.25581 	 m..s
  118 	   117 	 0.25667 	 0.26639 	 ~...
  116 	   118 	 0.24336 	 0.29500 	 m..s
  119 	   119 	 0.25750 	 0.30182 	 m..s
  120 	   120 	 0.26293 	 0.30295 	 m..s
==========================================
r_mrr = 0.691801130771637
r2_mrr = 0.4643715023994446
spearmanr_mrr@5 = 0.9745853543281555
spearmanr_mrr@10 = 0.96210116147995
spearmanr_mrr@50 = 0.8787675499916077
spearmanr_mrr@100 = 0.8048208951950073
spearmanr_mrr@All = 0.8584805130958557
==========================================
test time: 0.62
Done Testing dataset CoDExSmall
total time taken: 191.2544469833374
training time taken: 182.27088332176208
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6918)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4644)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9746)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9621)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8788)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8048)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8585)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.015869275366185}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4303420726650775
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 421, 392, 1014, 319, 547, 417, 654, 855, 925, 1168, 371, 224, 714, 355, 213, 484, 955, 1196, 804, 409, 807, 673, 1089, 828, 62, 154, 341, 125, 27, 576, 1151, 121, 84, 1160, 435, 759, 304, 874, 259, 1157, 134, 635, 607, 272, 1099, 385, 32, 580, 598, 796, 459, 1106, 527, 321, 267, 1011, 143, 1052, 867, 591, 716, 1188, 914, 870, 227, 1204, 275, 300, 447, 68, 751, 130, 210, 410, 615, 443, 157, 348, 2, 972, 60, 736, 913, 104, 625, 480, 884, 1060, 781, 922, 787, 1091, 329, 299, 370, 303, 155, 382, 798, 425, 78, 1042, 1173, 406, 959, 659, 1067, 1203, 715, 1122, 767, 1087, 556, 400, 1057, 241, 9, 706, 856, 937]
valid_ids (0): []
train_ids (1094): [95, 353, 948, 509, 273, 1199, 644, 992, 1137, 1094, 703, 82, 456, 269, 776, 1023, 915, 649, 129, 719, 1181, 575, 243, 1053, 826, 908, 761, 971, 74, 483, 772, 5, 1019, 879, 753, 1070, 90, 747, 667, 562, 1013, 426, 102, 601, 144, 819, 839, 458, 434, 1071, 295, 287, 708, 848, 454, 1075, 850, 639, 73, 825, 558, 260, 1100, 599, 829, 296, 248, 424, 712, 1142, 919, 658, 460, 1171, 857, 211, 803, 450, 1051, 18, 1128, 1004, 895, 905, 617, 12, 939, 1193, 966, 133, 1135, 492, 1195, 1167, 301, 120, 1103, 1098, 1049, 502, 1180, 208, 713, 111, 627, 324, 583, 1007, 618, 445, 1048, 1205, 632, 318, 735, 372, 277, 265, 1147, 845, 545, 270, 284, 640, 528, 985, 1101, 264, 452, 574, 100, 860, 89, 597, 595, 784, 239, 399, 1086, 186, 451, 950, 779, 970, 503, 1012, 340, 317, 87, 175, 488, 739, 205, 431, 550, 361, 1090, 26, 1170, 397, 518, 793, 231, 590, 989, 298, 114, 201, 69, 778, 813, 721, 696, 105, 1113, 37, 81, 1159, 469, 866, 854, 165, 1043, 262, 810, 203, 57, 475, 977, 740, 873, 572, 365, 827, 1024, 1092, 332, 229, 338, 695, 146, 1214, 947, 209, 629, 853, 552, 729, 1114, 903, 1134, 1208, 790, 918, 336, 279, 46, 233, 702, 670, 80, 872, 1115, 660, 136, 1149, 64, 215, 268, 156, 592, 40, 762, 543, 768, 52, 325, 880, 920, 21, 886, 717, 1213, 608, 328, 620, 1136, 1130, 403, 1123, 780, 110, 996, 374, 823, 30, 1107, 147, 398, 221, 153, 407, 891, 25, 305, 569, 145, 1045, 91, 816, 108, 47, 333, 344, 515, 228, 22, 184, 169, 1001, 815, 968, 331, 805, 643, 648, 230, 573, 88, 560, 489, 487, 529, 737, 34, 167, 979, 782, 15, 1097, 1041, 1076, 77, 1082, 85, 462, 878, 974, 605, 500, 883, 168, 662, 347, 1059, 1210, 570, 491, 422, 1073, 466, 252, 137, 987, 170, 63, 923, 54, 844, 628, 159, 1035, 468, 537, 70, 687, 728, 775, 148, 432, 724, 1186, 764, 461, 470, 257, 151, 393, 376, 285, 638, 1132, 245, 464, 603, 1172, 471, 748, 909, 172, 668, 894, 732, 115, 786, 166, 244, 140, 783, 292, 549, 359, 619, 741, 306, 669, 742, 478, 904, 838, 534, 991, 1069, 678, 930, 297, 103, 954, 967, 119, 652, 362, 181, 189, 725, 927, 463, 282, 1085, 395, 630, 745, 1175, 794, 578, 593, 997, 865, 1009, 36, 342, 437, 536, 692, 1145, 770, 830, 1058, 897, 164, 283, 750, 1003, 664, 1184, 316, 349, 817, 932, 1006, 413, 636, 1072, 986, 360, 238, 936, 928, 423, 150, 681, 935, 7, 131, 963, 694, 1032, 246, 579, 49, 1064, 634, 477, 1118, 219, 777, 582, 98, 1080, 862, 193, 101, 863, 266, 28, 523, 522, 621, 797, 236, 709, 554, 13, 419, 943, 93, 861, 834, 1183, 482, 1, 812, 1112, 526, 727, 1095, 387, 841, 840, 56, 1044, 3, 1040, 187, 611, 92, 6, 94, 1126, 436, 960, 14, 1198, 564, 357, 586, 1150, 594, 66, 48, 614, 314, 1127, 1133, 637, 646, 1108, 308, 258, 738, 944, 377, 1038, 1111, 1010, 521, 1125, 44, 495, 501, 898, 23, 188, 726, 773, 1081, 984, 1000, 1102, 366, 749, 1143, 311, 831, 29, 1055, 286, 563, 892, 496, 588, 519, 179, 290, 253, 567, 524, 746, 525, 921, 38, 19, 1015, 888, 722, 900, 135, 814, 20, 1025, 97, 206, 415, 472, 138, 975, 799, 404, 1185, 765, 4, 194, 256, 851, 806, 774, 112, 0, 758, 565, 139, 1165, 697, 881, 345, 1212, 788, 1169, 952, 1020, 242, 679, 811, 938, 1031, 142, 988, 384, 320, 995, 1202, 1131, 680, 339, 671, 1152, 312, 71, 688, 197, 1034, 438, 976, 1008, 192, 467, 1189, 53, 416, 542, 657, 17, 514, 1206, 280, 731, 852, 655, 11, 907, 118, 96, 1046, 113, 538, 1084, 212, 801, 755, 1109, 202, 1088, 733, 641, 160, 367, 39, 754, 42, 1117, 1017, 465, 394, 504, 389, 789, 929, 1029, 10, 1047, 497, 1036, 689, 401, 67, 1096, 973, 1192, 288, 182, 122, 771, 541, 158, 785, 375, 994, 388, 58, 833, 962, 396, 1187, 978, 566, 255, 561, 875, 1139, 520, 220, 693, 141, 899, 51, 335, 822, 1021, 479, 1116, 546, 585, 701, 949, 998, 1074, 234, 1141, 846, 613, 276, 176, 43, 232, 677, 132, 1144, 1176, 808, 414, 356, 75, 183, 358, 1056, 281, 707, 1062, 683, 942, 906, 1154, 261, 204, 61, 983, 832, 704, 16, 405, 408, 1028, 45, 982, 964, 718, 1140, 474, 499, 559, 555, 1018, 606, 124, 1153, 596, 612, 802, 951, 383, 455, 730, 792, 278, 1093, 294, 418, 1191, 247, 1120, 577, 173, 289, 123, 1066, 651, 800, 548, 847, 1039, 161, 217, 1065, 882, 616, 493, 476, 207, 584, 511, 969, 1002, 581, 354, 274, 896, 626, 291, 330, 149, 1124, 763, 836, 35, 877, 390, 924, 931, 791, 1121, 1026, 126, 917, 642, 322, 869, 876, 216, 1110, 198, 1200, 177, 843, 363, 871, 720, 199, 532, 531, 1146, 1119, 1027, 901, 911, 185, 271, 1050, 956, 196, 128, 1201, 310, 859, 557, 958, 59, 430, 442, 744, 1174, 412, 498, 65, 544, 946, 218, 535, 1162, 505, 868, 481, 1077, 622, 86, 999, 449, 539, 824, 571, 698, 842, 600, 76, 350, 705, 448, 1030, 820, 178, 510, 485, 690, 490, 663, 1194, 551, 609, 1016, 666, 1022, 33, 195, 427, 457, 902, 858, 700, 769, 351, 240, 1079, 428, 486, 610, 337, 473, 1163, 734, 650, 981, 379, 107, 225, 116, 24, 1129, 334, 106, 8, 1177, 887, 645, 849, 661, 302, 1158, 453, 433, 1033, 254, 1104, 1164, 589, 684, 711, 961, 1182, 893, 1138, 55, 604, 1054, 364, 665, 533, 152, 127, 190, 309, 647, 293, 313, 352, 263, 623, 685, 682, 315, 378, 980, 200, 916, 674, 327, 933, 223, 953, 323, 391, 250, 1063, 699, 835, 889, 926, 766, 890, 381, 631, 1083, 587, 1037, 99, 540, 760, 993, 117, 516, 691, 513, 818, 1105, 343, 506, 174, 1190, 1179, 1068, 1197, 251, 1166, 72, 934, 743, 508, 180, 214, 568, 885, 1209, 386, 965, 402, 757, 672, 440, 171, 109, 517, 553, 653, 941, 368, 945, 420, 710, 1156, 1148, 723, 235, 864, 226, 249, 444, 624, 990, 346, 633, 530, 957, 676, 41, 1078, 446, 795, 373, 163, 1161, 940, 307, 1211, 222, 752, 686, 1207, 50, 162, 1061, 31, 494, 79, 512, 1005, 83, 191, 675, 821, 237, 910, 326, 369, 837, 411, 439, 1155, 912, 809, 756, 441, 380, 602, 429, 656, 507]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2464276625823700
the save name prefix for this run is:  chkpt-ID_2464276625823700_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 290
rank avg (pred): 0.439 +- 0.006
mrr vals (pred, true): 0.001, 0.298
batch losses (mrrl, rdl): 0.0, 0.0034419976

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.264 +- 0.103
mrr vals (pred, true): 0.021, 0.212
batch losses (mrrl, rdl): 0.0, 0.000966103

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 199
rank avg (pred): 0.286 +- 0.145
mrr vals (pred, true): 0.096, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007236397

Epoch over!
epoch time: 12.131

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.384 +- 0.185
mrr vals (pred, true): 0.097, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001704544

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 689
rank avg (pred): 0.327 +- 0.187
mrr vals (pred, true): 0.144, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004140188

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 457
rank avg (pred): 0.256 +- 0.176
mrr vals (pred, true): 0.196, 0.003
batch losses (mrrl, rdl): 0.0, 0.0008517581

Epoch over!
epoch time: 11.789

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1048
rank avg (pred): 0.174 +- 0.128
mrr vals (pred, true): 0.221, 0.004
batch losses (mrrl, rdl): 0.0, 0.0017991412

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1101
rank avg (pred): 0.257 +- 0.205
mrr vals (pred, true): 0.246, 0.207
batch losses (mrrl, rdl): 0.0, 0.0009916533

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 538
rank avg (pred): 0.235 +- 0.183
mrr vals (pred, true): 0.207, 0.039
batch losses (mrrl, rdl): 0.0, 1.55911e-05

Epoch over!
epoch time: 11.898

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 287
rank avg (pred): 0.058 +- 0.050
mrr vals (pred, true): 0.289, 0.277
batch losses (mrrl, rdl): 0.0, 9.494e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 872
rank avg (pred): 0.423 +- 0.279
mrr vals (pred, true): 0.149, 0.004
batch losses (mrrl, rdl): 0.0, 1.74408e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 379
rank avg (pred): 0.237 +- 0.240
mrr vals (pred, true): 0.249, 0.219
batch losses (mrrl, rdl): 0.0, 0.0008709608

Epoch over!
epoch time: 11.908

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1066
rank avg (pred): 0.039 +- 0.040
mrr vals (pred, true): 0.279, 0.246
batch losses (mrrl, rdl): 0.0, 2.4894e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 6
rank avg (pred): 0.020 +- 0.022
mrr vals (pred, true): 0.337, 0.233
batch losses (mrrl, rdl): 0.0, 5.1135e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1123
rank avg (pred): 0.241 +- 0.276
mrr vals (pred, true): 0.238, 0.005
batch losses (mrrl, rdl): 0.0, 0.0007478061

Epoch over!
epoch time: 12.091

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 564
rank avg (pred): 0.213 +- 0.254
mrr vals (pred, true): 0.286, 0.033
batch losses (mrrl, rdl): 0.5557765961, 8.6906e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 638
rank avg (pred): 0.461 +- 0.273
mrr vals (pred, true): 0.106, 0.153
batch losses (mrrl, rdl): 0.0223096684, 0.0018387797

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 766
rank avg (pred): 0.616 +- 0.208
mrr vals (pred, true): 0.055, 0.075
batch losses (mrrl, rdl): 0.0002045583, 0.0018849865

Epoch over!
epoch time: 12.442

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 567
rank avg (pred): 0.598 +- 0.220
mrr vals (pred, true): 0.069, 0.103
batch losses (mrrl, rdl): 0.0116751418, 0.0025208504

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 642
rank avg (pred): 0.573 +- 0.225
mrr vals (pred, true): 0.076, 0.128
batch losses (mrrl, rdl): 0.0277463533, 0.0032380959

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1003
rank avg (pred): 0.312 +- 0.223
mrr vals (pred, true): 0.139, 0.204
batch losses (mrrl, rdl): 0.0419510454, 0.0015633472

Epoch over!
epoch time: 12.215

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 166
rank avg (pred): 0.333 +- 0.231
mrr vals (pred, true): 0.137, 0.003
batch losses (mrrl, rdl): 0.0765571371, 0.0003186146

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1019
rank avg (pred): 0.307 +- 0.211
mrr vals (pred, true): 0.133, 0.198
batch losses (mrrl, rdl): 0.0417152643, 0.0014695035

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1092
rank avg (pred): 0.386 +- 0.244
mrr vals (pred, true): 0.129, 0.212
batch losses (mrrl, rdl): 0.0682396814, 0.0024835661

Epoch over!
epoch time: 12.305

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1102
rank avg (pred): 0.387 +- 0.239
mrr vals (pred, true): 0.121, 0.210
batch losses (mrrl, rdl): 0.079051286, 0.0024701077

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 168
rank avg (pred): 0.381 +- 0.228
mrr vals (pred, true): 0.123, 0.005
batch losses (mrrl, rdl): 0.052716136, 0.0001539578

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1154
rank avg (pred): 0.489 +- 0.205
mrr vals (pred, true): 0.080, 0.059
batch losses (mrrl, rdl): 0.0089272968, 0.0018231008

Epoch over!
epoch time: 12.28

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 958
rank avg (pred): 0.549 +- 0.144
mrr vals (pred, true): 0.029, 0.005
batch losses (mrrl, rdl): 0.004537777, 0.0001747735

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.479 +- 0.196
mrr vals (pred, true): 0.075, 0.036
batch losses (mrrl, rdl): 0.0060550589, 0.0013730694

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 76
rank avg (pred): 0.017 +- 0.012
mrr vals (pred, true): 0.221, 0.218
batch losses (mrrl, rdl): 6.65203e-05, 3.04398e-05

Epoch over!
epoch time: 12.349

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1061
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.260, 0.314
batch losses (mrrl, rdl): 0.0283956155, 1.15796e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1120
rank avg (pred): 0.342 +- 0.203
mrr vals (pred, true): 0.129, 0.004
batch losses (mrrl, rdl): 0.0625143647, 0.0003656897

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.465 +- 0.176
mrr vals (pred, true): 0.072, 0.003
batch losses (mrrl, rdl): 0.0048437491, 4.03668e-05

Epoch over!
epoch time: 12.506

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1194
rank avg (pred): 0.431 +- 0.194
mrr vals (pred, true): 0.090, 0.005
batch losses (mrrl, rdl): 0.0162511114, 5.77514e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1209
rank avg (pred): 0.423 +- 0.195
mrr vals (pred, true): 0.095, 0.005
batch losses (mrrl, rdl): 0.0205259435, 6.89134e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 416
rank avg (pred): 0.291 +- 0.167
mrr vals (pred, true): 0.132, 0.004
batch losses (mrrl, rdl): 0.0668456703, 0.0006270037

Epoch over!
epoch time: 12.136

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 10
rank avg (pred): 0.016 +- 0.010
mrr vals (pred, true): 0.240, 0.243
batch losses (mrrl, rdl): 5.48148e-05, 7.7245e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 18
rank avg (pred): 0.021 +- 0.014
mrr vals (pred, true): 0.234, 0.245
batch losses (mrrl, rdl): 0.0010949743, 3.094e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 974
rank avg (pred): 0.017 +- 0.013
mrr vals (pred, true): 0.266, 0.260
batch losses (mrrl, rdl): 0.0004536019, 7.4868e-06

Epoch over!
epoch time: 12.071

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 815
rank avg (pred): 0.409 +- 0.182
mrr vals (pred, true): 0.074, 0.058
batch losses (mrrl, rdl): 0.0057499977, 0.0012358024

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 491
rank avg (pred): 0.165 +- 0.106
mrr vals (pred, true): 0.165, 0.166
batch losses (mrrl, rdl): 1.11756e-05, 5.34219e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.356 +- 0.174
mrr vals (pred, true): 0.104, 0.004
batch losses (mrrl, rdl): 0.0288584176, 0.0002758789

Epoch over!
epoch time: 12.159

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 91
rank avg (pred): 0.238 +- 0.131
mrr vals (pred, true): 0.119, 0.199
batch losses (mrrl, rdl): 0.0639273971, 0.0007922595

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 486
rank avg (pred): 0.174 +- 0.099
mrr vals (pred, true): 0.145, 0.162
batch losses (mrrl, rdl): 0.003030851, 5.64158e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 783
rank avg (pred): 0.434 +- 0.136
mrr vals (pred, true): 0.052, 0.005
batch losses (mrrl, rdl): 2.97472e-05, 9.00115e-05

Epoch over!
epoch time: 12.155

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.408 +- 0.169
mrr vals (pred, true): 0.088, 0.163

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03263 	 0.00074 	 m..s
    3 	     1 	 0.03485 	 0.00075 	 m..s
    0 	     2 	 0.03223 	 0.00075 	 m..s
   43 	     3 	 0.08809 	 0.00104 	 m..s
    8 	     4 	 0.04139 	 0.00113 	 m..s
    7 	     5 	 0.04105 	 0.00124 	 m..s
   35 	     6 	 0.07464 	 0.00333 	 m..s
   55 	     7 	 0.11323 	 0.00342 	 MISS
   85 	     8 	 0.12176 	 0.00350 	 MISS
    5 	     9 	 0.03970 	 0.00353 	 m..s
   60 	    10 	 0.11596 	 0.00356 	 MISS
   16 	    11 	 0.05738 	 0.00358 	 m..s
   96 	    12 	 0.13332 	 0.00374 	 MISS
   31 	    13 	 0.06868 	 0.00378 	 m..s
   93 	    14 	 0.12673 	 0.00380 	 MISS
   10 	    15 	 0.04228 	 0.00385 	 m..s
   68 	    16 	 0.11803 	 0.00387 	 MISS
    6 	    17 	 0.03982 	 0.00388 	 m..s
   76 	    18 	 0.11976 	 0.00388 	 MISS
   56 	    19 	 0.11333 	 0.00391 	 MISS
   87 	    20 	 0.12268 	 0.00392 	 MISS
    2 	    21 	 0.03480 	 0.00393 	 m..s
   36 	    22 	 0.08502 	 0.00395 	 m..s
   86 	    23 	 0.12259 	 0.00398 	 MISS
   95 	    24 	 0.13032 	 0.00400 	 MISS
   48 	    25 	 0.09052 	 0.00400 	 m..s
   61 	    26 	 0.11669 	 0.00401 	 MISS
   92 	    27 	 0.12515 	 0.00402 	 MISS
   15 	    28 	 0.05716 	 0.00404 	 m..s
   65 	    29 	 0.11708 	 0.00404 	 MISS
   14 	    30 	 0.05715 	 0.00407 	 m..s
   88 	    31 	 0.12319 	 0.00417 	 MISS
   89 	    32 	 0.12320 	 0.00420 	 MISS
   11 	    33 	 0.05600 	 0.00421 	 m..s
   61 	    34 	 0.11669 	 0.00423 	 MISS
   17 	    35 	 0.05741 	 0.00441 	 m..s
   32 	    36 	 0.07011 	 0.00441 	 m..s
   71 	    37 	 0.11828 	 0.00442 	 MISS
   59 	    38 	 0.11461 	 0.00442 	 MISS
    4 	    39 	 0.03528 	 0.00445 	 m..s
   38 	    40 	 0.08589 	 0.00451 	 m..s
   84 	    41 	 0.12169 	 0.00452 	 MISS
   49 	    42 	 0.09116 	 0.00462 	 m..s
   18 	    43 	 0.05750 	 0.00470 	 m..s
   26 	    44 	 0.06607 	 0.00493 	 m..s
   25 	    45 	 0.06468 	 0.00509 	 m..s
    9 	    46 	 0.04207 	 0.00516 	 m..s
   46 	    47 	 0.08862 	 0.00524 	 m..s
   40 	    48 	 0.08619 	 0.00543 	 m..s
   19 	    49 	 0.05839 	 0.02208 	 m..s
   22 	    50 	 0.06267 	 0.04495 	 ~...
   21 	    51 	 0.06226 	 0.04845 	 ~...
   28 	    52 	 0.06666 	 0.04889 	 ~...
   27 	    53 	 0.06633 	 0.04962 	 ~...
   33 	    54 	 0.07377 	 0.05249 	 ~...
   34 	    55 	 0.07424 	 0.05279 	 ~...
   13 	    56 	 0.05645 	 0.05678 	 ~...
   12 	    57 	 0.05610 	 0.06300 	 ~...
   20 	    58 	 0.05974 	 0.07522 	 ~...
   23 	    59 	 0.06429 	 0.10123 	 m..s
   24 	    60 	 0.06462 	 0.10502 	 m..s
   30 	    61 	 0.06860 	 0.10894 	 m..s
   42 	    62 	 0.08674 	 0.13439 	 m..s
   29 	    63 	 0.06712 	 0.13772 	 m..s
   47 	    64 	 0.08923 	 0.15039 	 m..s
   41 	    65 	 0.08645 	 0.15393 	 m..s
   39 	    66 	 0.08615 	 0.15822 	 m..s
   37 	    67 	 0.08562 	 0.15954 	 m..s
   44 	    68 	 0.08833 	 0.15955 	 m..s
   45 	    69 	 0.08835 	 0.16331 	 m..s
   97 	    70 	 0.13938 	 0.16691 	 ~...
   57 	    71 	 0.11369 	 0.17926 	 m..s
   53 	    72 	 0.11114 	 0.18527 	 m..s
   98 	    73 	 0.17512 	 0.18577 	 ~...
  100 	    74 	 0.18585 	 0.19178 	 ~...
   74 	    75 	 0.11946 	 0.19253 	 m..s
  101 	    76 	 0.19296 	 0.19366 	 ~...
   61 	    77 	 0.11669 	 0.19375 	 m..s
   80 	    78 	 0.12090 	 0.19691 	 m..s
   69 	    79 	 0.11806 	 0.19726 	 m..s
   67 	    80 	 0.11781 	 0.19823 	 m..s
   51 	    81 	 0.11015 	 0.19965 	 m..s
   99 	    82 	 0.18385 	 0.19977 	 ~...
   50 	    83 	 0.10874 	 0.20045 	 m..s
   52 	    84 	 0.11055 	 0.20053 	 m..s
   58 	    85 	 0.11441 	 0.20099 	 m..s
   61 	    86 	 0.11669 	 0.20179 	 m..s
   79 	    87 	 0.12069 	 0.20310 	 m..s
   54 	    88 	 0.11317 	 0.20532 	 m..s
   73 	    89 	 0.11933 	 0.20546 	 m..s
  112 	    90 	 0.22589 	 0.20793 	 ~...
   70 	    91 	 0.11826 	 0.20816 	 m..s
   77 	    92 	 0.11982 	 0.20847 	 m..s
  102 	    93 	 0.19690 	 0.21020 	 ~...
   83 	    94 	 0.12148 	 0.21092 	 m..s
   75 	    95 	 0.11975 	 0.21389 	 m..s
   81 	    96 	 0.12118 	 0.21447 	 m..s
  114 	    97 	 0.22657 	 0.21580 	 ~...
  106 	    98 	 0.21273 	 0.21584 	 ~...
   91 	    99 	 0.12413 	 0.21594 	 m..s
   94 	   100 	 0.13007 	 0.21721 	 m..s
   78 	   101 	 0.12048 	 0.21877 	 m..s
  104 	   102 	 0.21033 	 0.21933 	 ~...
   90 	   103 	 0.12364 	 0.21972 	 m..s
   66 	   104 	 0.11713 	 0.21993 	 MISS
  113 	   105 	 0.22606 	 0.22059 	 ~...
  110 	   106 	 0.22409 	 0.22067 	 ~...
   82 	   107 	 0.12130 	 0.22174 	 MISS
  103 	   108 	 0.20941 	 0.22232 	 ~...
  116 	   109 	 0.23194 	 0.22334 	 ~...
   72 	   110 	 0.11848 	 0.23334 	 MISS
  111 	   111 	 0.22427 	 0.23702 	 ~...
  109 	   112 	 0.22403 	 0.23828 	 ~...
  105 	   113 	 0.21140 	 0.23944 	 ~...
  107 	   114 	 0.21737 	 0.24796 	 m..s
  117 	   115 	 0.23393 	 0.25867 	 ~...
  115 	   116 	 0.23109 	 0.26289 	 m..s
  120 	   117 	 0.24629 	 0.27612 	 ~...
  119 	   118 	 0.24529 	 0.28464 	 m..s
  108 	   119 	 0.21822 	 0.30295 	 m..s
  118 	   120 	 0.23638 	 0.32036 	 m..s
==========================================
r_mrr = 0.7060666084289551
r2_mrr = 0.4753287434577942
spearmanr_mrr@5 = 0.9667114019393921
spearmanr_mrr@10 = 0.9868279099464417
spearmanr_mrr@50 = 0.8191769123077393
spearmanr_mrr@100 = 0.8283284306526184
spearmanr_mrr@All = 0.8697354197502136
==========================================
test time: 0.396
Done Testing dataset CoDExSmall
total time taken: 192.8549473285675
training time taken: 182.90634298324585
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7061)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4753)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9667)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9868)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8192)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8283)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8697)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.012908568154671}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7939117544682113
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [217, 855, 233, 256, 765, 447, 1037, 275, 276, 620, 640, 675, 1096, 401, 691, 722, 754, 1141, 1051, 880, 772, 145, 93, 1136, 290, 658, 917, 630, 1110, 146, 1039, 934, 689, 1126, 367, 926, 969, 1069, 85, 415, 462, 1142, 122, 719, 512, 711, 965, 324, 104, 112, 237, 195, 663, 509, 731, 767, 1212, 380, 713, 966, 467, 779, 537, 431, 971, 545, 204, 437, 720, 438, 956, 466, 983, 316, 1165, 29, 590, 1073, 894, 960, 959, 309, 662, 297, 653, 1124, 73, 441, 541, 449, 247, 1055, 1095, 191, 807, 674, 1034, 633, 834, 34, 420, 1076, 234, 206, 1214, 1199, 756, 829, 845, 1100, 232, 16, 686, 1001, 858, 886, 814, 626, 933, 357, 425]
valid_ids (0): []
train_ids (1094): [870, 957, 332, 1213, 666, 1115, 141, 750, 546, 391, 874, 615, 113, 348, 681, 169, 847, 744, 660, 723, 190, 1133, 248, 273, 1102, 1166, 948, 253, 1081, 1181, 444, 579, 820, 548, 595, 1083, 883, 343, 592, 538, 598, 669, 216, 1056, 943, 564, 827, 972, 696, 843, 386, 114, 840, 241, 508, 264, 850, 761, 240, 892, 821, 193, 946, 360, 314, 902, 993, 359, 143, 27, 24, 982, 1109, 514, 6, 504, 296, 735, 510, 1200, 1030, 1091, 947, 135, 147, 1107, 261, 398, 171, 1029, 23, 1047, 1175, 453, 1108, 745, 1120, 333, 460, 501, 1103, 35, 841, 542, 924, 1122, 547, 805, 131, 138, 250, 12, 102, 995, 334, 823, 494, 417, 846, 258, 1007, 1009, 1008, 422, 281, 1044, 1066, 793, 970, 656, 382, 931, 288, 629, 1, 279, 992, 238, 635, 646, 739, 1159, 305, 1211, 543, 100, 887, 601, 1035, 428, 589, 743, 519, 1012, 223, 1157, 1087, 411, 885, 303, 539, 586, 1036, 600, 1094, 351, 710, 154, 574, 647, 746, 553, 347, 918, 106, 219, 70, 738, 904, 407, 158, 403, 523, 285, 1179, 624, 955, 607, 1038, 860, 618, 1135, 1052, 912, 962, 440, 783, 818, 188, 413, 1152, 869, 1114, 1154, 94, 628, 19, 163, 676, 1129, 617, 211, 134, 162, 1049, 474, 976, 636, 725, 139, 0, 242, 377, 341, 1143, 721, 176, 389, 803, 121, 550, 436, 1150, 571, 133, 412, 639, 62, 175, 654, 697, 1188, 1067, 84, 220, 1192, 529, 762, 96, 310, 130, 1137, 554, 490, 1176, 1074, 796, 520, 1170, 435, 246, 637, 954, 363, 419, 185, 205, 737, 632, 372, 1182, 717, 1060, 930, 361, 985, 832, 263, 809, 1146, 1050, 464, 940, 362, 1209, 1167, 459, 115, 1169, 945, 173, 74, 319, 67, 648, 395, 790, 1015, 344, 1162, 891, 485, 254, 1097, 489, 1042, 667, 393, 371, 914, 14, 493, 906, 758, 222, 312, 784, 25, 1032, 741, 838, 328, 424, 968, 798, 1197, 578, 998, 495, 157, 612, 879, 320, 819, 480, 1006, 218, 552, 919, 699, 517, 476, 161, 1040, 944, 751, 602, 534, 788, 975, 1145, 349, 680, 565, 8, 203, 32, 1180, 1138, 340, 597, 842, 142, 17, 208, 409, 764, 549, 535, 69, 257, 272, 352, 268, 473, 786, 716, 5, 198, 625, 777, 921, 148, 1084, 861, 339, 346, 862, 483, 81, 949, 265, 186, 399, 575, 318, 189, 117, 868, 621, 937, 698, 321, 1045, 43, 997, 544, 336, 1104, 749, 503, 1207, 307, 655, 661, 325, 563, 867, 83, 68, 785, 373, 151, 323, 350, 540, 82, 243, 1206, 1003, 518, 48, 77, 354, 292, 181, 406, 1088, 651, 472, 236, 950, 1183, 752, 524, 172, 329, 127, 729, 753, 266, 837, 463, 470, 1160, 613, 1031, 38, 769, 967, 802, 72, 889, 442, 980, 614, 53, 446, 1054, 913, 604, 155, 649, 1119, 1057, 212, 1064, 863, 1189, 616, 136, 402, 728, 747, 706, 566, 327, 262, 877, 202, 168, 384, 1061, 366, 1093, 907, 991, 1195, 668, 1128, 488, 815, 1011, 734, 1023, 817, 408, 455, 694, 313, 915, 928, 1098, 252, 922, 562, 194, 300, 810, 789, 561, 294, 47, 1024, 787, 140, 811, 491, 1111, 559, 671, 376, 828, 245, 797, 650, 499, 461, 677, 866, 782, 199, 851, 123, 457, 1078, 610, 1046, 576, 989, 98, 707, 1193, 603, 228, 469, 652, 804, 505, 197, 718, 1149, 1071, 445, 683, 63, 1210, 153, 1041, 701, 1184, 1168, 996, 942, 1065, 911, 414, 558, 430, 80, 315, 700, 1112, 129, 831, 726, 1090, 201, 1043, 326, 283, 452, 355, 702, 584, 99, 835, 964, 583, 778, 308, 1163, 51, 688, 196, 1132, 854, 1144, 118, 670, 330, 800, 1021, 78, 56, 594, 267, 511, 378, 383, 174, 665, 591, 977, 1186, 277, 59, 1058, 105, 156, 311, 875, 1020, 42, 760, 410, 54, 251, 231, 712, 1164, 306, 1191, 1068, 335, 580, 1082, 528, 76, 732, 21, 293, 882, 773, 836, 641, 183, 214, 434, 97, 484, 95, 150, 200, 896, 923, 1022, 65, 57, 577, 905, 45, 433, 878, 496, 890, 953, 759, 31, 60, 556, 768, 79, 342, 92, 1201, 659, 605, 90, 1148, 481, 259, 1118, 269, 400, 299, 37, 727, 780, 421, 672, 572, 1053, 755, 1086, 46, 30, 1158, 492, 551, 274, 184, 774, 187, 555, 1077, 864, 15, 26, 36, 249, 479, 515, 375, 111, 695, 390, 908, 179, 581, 984, 766, 587, 1178, 465, 392, 872, 536, 1204, 533, 833, 66, 678, 244, 405, 448, 317, 3, 478, 994, 730, 939, 852, 456, 1025, 1113, 839, 423, 1147, 159, 132, 903, 282, 1048, 925, 13, 1177, 693, 963, 39, 137, 987, 52, 812, 374, 568, 429, 1187, 387, 1208, 298, 644, 951, 859, 596, 608, 876, 881, 144, 213, 426, 830, 981, 952, 1026, 11, 623, 813, 160, 1134, 9, 271, 638, 1194, 560, 385, 125, 475, 1017, 128, 1027, 192, 941, 973, 439, 1174, 215, 149, 531, 1010, 657, 1079, 126, 525, 826, 1196, 55, 497, 388, 301, 1072, 418, 1205, 679, 381, 1028, 416, 103, 61, 532, 748, 255, 848, 853, 687, 709, 642, 1203, 808, 20, 1014, 107, 432, 41, 740, 775, 229, 1019, 1127, 365, 856, 582, 781, 1105, 44, 498, 1059, 235, 631, 89, 379, 715, 221, 888, 1002, 569, 284, 487, 353, 799, 209, 794, 801, 978, 295, 606, 593, 230, 108, 337, 116, 207, 356, 120, 506, 1153, 101, 645, 958, 736, 1130, 500, 1013, 705, 88, 1116, 109, 527, 18, 1106, 124, 521, 322, 338, 304, 1016, 451, 87, 792, 450, 1198, 643, 49, 684, 516, 7, 901, 166, 673, 152, 1092, 824, 1202, 110, 929, 28, 369, 961, 742, 1151, 884, 302, 526, 1117, 227, 522, 585, 893, 471, 622, 331, 898, 1173, 427, 692, 895, 1121, 260, 397, 619, 557, 865, 1101, 170, 86, 776, 58, 177, 627, 1123, 224, 708, 4, 979, 770, 816, 1070, 900, 71, 443, 10, 871, 988, 482, 910, 849, 1155, 920, 690, 1063, 507, 573, 370, 567, 795, 822, 724, 1080, 477, 986, 599, 2, 935, 873, 1161, 1139, 1075, 33, 178, 345, 486, 1171, 167, 1085, 791, 1033, 180, 404, 1140, 91, 1125, 1190, 936, 75, 806, 825, 226, 771, 502, 733, 844, 165, 897, 899, 703, 1089, 368, 364, 999, 685, 990, 280, 182, 291, 932, 1018, 396, 1004, 164, 974, 916, 1131, 664, 704, 1172, 609, 1005, 530, 289, 278, 714, 458, 927, 468, 287, 394, 938, 22, 611, 1156, 1062, 358, 857, 513, 634, 1000, 239, 909, 588, 454, 570, 64, 270, 50, 286, 1185, 763, 757, 210, 119, 225, 1099, 682, 40]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1561269977600664
the save name prefix for this run is:  chkpt-ID_1561269977600664_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 124
rank avg (pred): 0.426 +- 0.010
mrr vals (pred, true): 0.001, 0.206
batch losses (mrrl, rdl): 0.0, 0.0029734438

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.243 +- 0.150
mrr vals (pred, true): 0.130, 0.004
batch losses (mrrl, rdl): 0.0, 0.0011784991

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 824
rank avg (pred): 0.140 +- 0.106
mrr vals (pred, true): 0.233, 0.243
batch losses (mrrl, rdl): 0.0, 0.0002264588

Epoch over!
epoch time: 12.789

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.246 +- 0.199
mrr vals (pred, true): 0.270, 0.005
batch losses (mrrl, rdl): 0.0, 0.0008803599

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 677
rank avg (pred): 0.288 +- 0.233
mrr vals (pred, true): 0.262, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005163804

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 890
rank avg (pred): 0.362 +- 0.284
mrr vals (pred, true): 0.255, 0.004
batch losses (mrrl, rdl): 0.0, 0.000111681

Epoch over!
epoch time: 12.516

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.264 +- 0.213
mrr vals (pred, true): 0.247, 0.200
batch losses (mrrl, rdl): 0.0, 0.0011052544

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 7
rank avg (pred): 0.059 +- 0.056
mrr vals (pred, true): 0.393, 0.232
batch losses (mrrl, rdl): 0.0, 1.20105e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 68
rank avg (pred): 0.054 +- 0.050
mrr vals (pred, true): 0.370, 0.210
batch losses (mrrl, rdl): 0.0, 2.3533e-06

Epoch over!
epoch time: 12.35

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 349
rank avg (pred): 0.230 +- 0.210
mrr vals (pred, true): 0.326, 0.201
batch losses (mrrl, rdl): 0.0, 0.0008195328

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 246
rank avg (pred): 0.033 +- 0.032
mrr vals (pred, true): 0.441, 0.225
batch losses (mrrl, rdl): 0.0, 5.0984e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 490
rank avg (pred): 0.152 +- 0.140
mrr vals (pred, true): 0.363, 0.175
batch losses (mrrl, rdl): 0.0, 3.11675e-05

Epoch over!
epoch time: 12.068

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1197
rank avg (pred): 0.294 +- 0.252
mrr vals (pred, true): 0.281, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004706505

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 907
rank avg (pred): 0.469 +- 0.377
mrr vals (pred, true): 0.279, 0.001
batch losses (mrrl, rdl): 0.0, 0.0006535014

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 838
rank avg (pred): 0.459 +- 0.358
mrr vals (pred, true): 0.249, 0.005
batch losses (mrrl, rdl): 0.0, 5.99675e-05

Epoch over!
epoch time: 12.112

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.085 +- 0.077
mrr vals (pred, true): 0.342, 0.032
batch losses (mrrl, rdl): 0.8514357805, 0.0012120163

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.682 +- 0.306
mrr vals (pred, true): 0.070, 0.006
batch losses (mrrl, rdl): 0.0039644623, 0.0009642696

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 472
rank avg (pred): 0.146 +- 0.121
mrr vals (pred, true): 0.126, 0.003
batch losses (mrrl, rdl): 0.0582951866, 0.0022223291

Epoch over!
epoch time: 12.53

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1032
rank avg (pred): 0.131 +- 0.113
mrr vals (pred, true): 0.140, 0.005
batch losses (mrrl, rdl): 0.0815292895, 0.0023131215

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 487
rank avg (pred): 0.188 +- 0.135
mrr vals (pred, true): 0.105, 0.160
batch losses (mrrl, rdl): 0.0303056259, 7.22617e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 522
rank avg (pred): 0.316 +- 0.201
mrr vals (pred, true): 0.098, 0.037
batch losses (mrrl, rdl): 0.0229220092, 0.0002341112

Epoch over!
epoch time: 12.929

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 854
rank avg (pred): 0.326 +- 0.177
mrr vals (pred, true): 0.089, 0.038
batch losses (mrrl, rdl): 0.0151079316, 0.0005379826

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 203
rank avg (pred): 0.275 +- 0.189
mrr vals (pred, true): 0.141, 0.004
batch losses (mrrl, rdl): 0.0825543627, 0.0007428502

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 833
rank avg (pred): 0.031 +- 0.023
mrr vals (pred, true): 0.200, 0.226
batch losses (mrrl, rdl): 0.0070157656, 9.4871e-06

Epoch over!
epoch time: 12.546

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 239
rank avg (pred): 0.296 +- 0.199
mrr vals (pred, true): 0.137, 0.004
batch losses (mrrl, rdl): 0.0749954805, 0.0006022799

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 13
rank avg (pred): 0.018 +- 0.013
mrr vals (pred, true): 0.235, 0.241
batch losses (mrrl, rdl): 0.0003504843, 7.1798e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 236
rank avg (pred): 0.306 +- 0.194
mrr vals (pred, true): 0.133, 0.004
batch losses (mrrl, rdl): 0.068624422, 0.000488441

Epoch over!
epoch time: 12.466

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 188
rank avg (pred): 0.332 +- 0.204
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0574748591, 0.0003326858

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 612
rank avg (pred): 0.333 +- 0.200
mrr vals (pred, true): 0.108, 0.135
batch losses (mrrl, rdl): 0.0074874121, 0.0006306405

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 529
rank avg (pred): 0.358 +- 0.181
mrr vals (pred, true): 0.076, 0.043
batch losses (mrrl, rdl): 0.0067663654, 0.0004141636

Epoch over!
epoch time: 13.225

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 353
rank avg (pred): 0.301 +- 0.189
mrr vals (pred, true): 0.145, 0.187
batch losses (mrrl, rdl): 0.0177321918, 0.001395066

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1173
rank avg (pred): 0.341 +- 0.196
mrr vals (pred, true): 0.102, 0.150
batch losses (mrrl, rdl): 0.0233480167, 0.0006632885

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 785
rank avg (pred): 0.588 +- 0.268
mrr vals (pred, true): 0.030, 0.004
batch losses (mrrl, rdl): 0.0038704118, 0.0001162315

Epoch over!
epoch time: 13.402

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 577
rank avg (pred): 0.370 +- 0.168
mrr vals (pred, true): 0.069, 0.109
batch losses (mrrl, rdl): 0.0158168431, 0.0003928398

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1205
rank avg (pred): 0.329 +- 0.194
mrr vals (pred, true): 0.112, 0.004
batch losses (mrrl, rdl): 0.0378678031, 0.0003971199

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 728
rank avg (pred): 0.326 +- 0.189
mrr vals (pred, true): 0.092, 0.004
batch losses (mrrl, rdl): 0.0174703039, 0.0003688184

Epoch over!
epoch time: 12.964

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 365
rank avg (pred): 0.313 +- 0.192
mrr vals (pred, true): 0.143, 0.199
batch losses (mrrl, rdl): 0.0313319303, 0.0015248305

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 875
rank avg (pred): 0.411 +- 0.185
mrr vals (pred, true): 0.066, 0.004
batch losses (mrrl, rdl): 0.0025199482, 0.0001113584

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 220
rank avg (pred): 0.316 +- 0.189
mrr vals (pred, true): 0.139, 0.005
batch losses (mrrl, rdl): 0.0793362781, 0.0004042184

Epoch over!
epoch time: 13.46

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 427
rank avg (pred): 0.291 +- 0.178
mrr vals (pred, true): 0.148, 0.004
batch losses (mrrl, rdl): 0.0965335593, 0.0006773351

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1173
rank avg (pred): 0.335 +- 0.172
mrr vals (pred, true): 0.080, 0.150
batch losses (mrrl, rdl): 0.0494974703, 0.0006054752

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.332 +- 0.170
mrr vals (pred, true): 0.074, 0.004
batch losses (mrrl, rdl): 0.0059106066, 0.0003848948

Epoch over!
epoch time: 13.52

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 739
rank avg (pred): 0.119 +- 0.078
mrr vals (pred, true): 0.189, 0.175
batch losses (mrrl, rdl): 0.0020171963, 0.0001129614

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 511
rank avg (pred): 0.167 +- 0.106
mrr vals (pred, true): 0.175, 0.153
batch losses (mrrl, rdl): 0.0049586305, 4.9984e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 536
rank avg (pred): 0.352 +- 0.149
mrr vals (pred, true): 0.067, 0.042
batch losses (mrrl, rdl): 0.0029817526, 0.0003457594

Epoch over!
epoch time: 12.734

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.311 +- 0.178
mrr vals (pred, true): 0.132, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03420 	 0.00074 	 m..s
    5 	     1 	 0.03514 	 0.00075 	 m..s
    4 	     2 	 0.03463 	 0.00076 	 m..s
   16 	     3 	 0.05027 	 0.00104 	 m..s
   12 	     4 	 0.04363 	 0.00116 	 m..s
   85 	     5 	 0.13687 	 0.00311 	 MISS
    7 	     6 	 0.03620 	 0.00312 	 m..s
   70 	     7 	 0.13117 	 0.00332 	 MISS
   30 	     8 	 0.08086 	 0.00339 	 m..s
   57 	     9 	 0.12690 	 0.00347 	 MISS
    1 	    10 	 0.03421 	 0.00348 	 m..s
   77 	    11 	 0.13310 	 0.00355 	 MISS
   82 	    12 	 0.13554 	 0.00357 	 MISS
   14 	    13 	 0.04801 	 0.00358 	 m..s
   67 	    14 	 0.13051 	 0.00359 	 MISS
   44 	    15 	 0.08619 	 0.00363 	 m..s
   32 	    16 	 0.08139 	 0.00369 	 m..s
   89 	    17 	 0.13838 	 0.00371 	 MISS
   91 	    18 	 0.13958 	 0.00371 	 MISS
   58 	    19 	 0.12697 	 0.00372 	 MISS
   28 	    20 	 0.07520 	 0.00375 	 m..s
   35 	    21 	 0.08205 	 0.00376 	 m..s
   95 	    22 	 0.14189 	 0.00379 	 MISS
   61 	    23 	 0.12744 	 0.00382 	 MISS
   29 	    24 	 0.07688 	 0.00383 	 m..s
   48 	    25 	 0.08929 	 0.00385 	 m..s
   93 	    26 	 0.14115 	 0.00393 	 MISS
   80 	    27 	 0.13349 	 0.00395 	 MISS
   62 	    28 	 0.12760 	 0.00396 	 MISS
   90 	    29 	 0.13881 	 0.00397 	 MISS
    3 	    30 	 0.03459 	 0.00399 	 m..s
    6 	    31 	 0.03618 	 0.00399 	 m..s
   37 	    32 	 0.08331 	 0.00399 	 m..s
   46 	    33 	 0.08702 	 0.00400 	 m..s
   71 	    34 	 0.13131 	 0.00401 	 MISS
   39 	    35 	 0.08452 	 0.00401 	 m..s
   49 	    36 	 0.08976 	 0.00403 	 m..s
   86 	    37 	 0.13751 	 0.00416 	 MISS
   72 	    38 	 0.13145 	 0.00416 	 MISS
   87 	    39 	 0.13753 	 0.00419 	 MISS
   10 	    40 	 0.03676 	 0.00422 	 m..s
   45 	    41 	 0.08647 	 0.00429 	 m..s
   83 	    42 	 0.13568 	 0.00431 	 MISS
   74 	    43 	 0.13163 	 0.00432 	 MISS
   38 	    44 	 0.08388 	 0.00432 	 m..s
   64 	    45 	 0.12900 	 0.00433 	 MISS
   55 	    46 	 0.12459 	 0.00438 	 MISS
   11 	    47 	 0.04307 	 0.00441 	 m..s
   81 	    48 	 0.13468 	 0.00442 	 MISS
   20 	    49 	 0.06100 	 0.00444 	 m..s
    8 	    50 	 0.03627 	 0.00445 	 m..s
   40 	    51 	 0.08468 	 0.00451 	 m..s
   78 	    52 	 0.13345 	 0.00452 	 MISS
   69 	    53 	 0.13116 	 0.00457 	 MISS
   68 	    54 	 0.13073 	 0.00464 	 MISS
   19 	    55 	 0.05609 	 0.00467 	 m..s
    2 	    56 	 0.03457 	 0.00489 	 ~...
   88 	    57 	 0.13760 	 0.00531 	 MISS
   41 	    58 	 0.08502 	 0.00552 	 m..s
   43 	    59 	 0.08605 	 0.00578 	 m..s
   42 	    60 	 0.08581 	 0.00582 	 m..s
   13 	    61 	 0.04725 	 0.02089 	 ~...
    9 	    62 	 0.03640 	 0.02364 	 ~...
   26 	    63 	 0.06631 	 0.04019 	 ~...
   15 	    64 	 0.04972 	 0.04078 	 ~...
   24 	    65 	 0.06626 	 0.04520 	 ~...
   21 	    66 	 0.06232 	 0.04665 	 ~...
   51 	    67 	 0.09611 	 0.05123 	 m..s
   24 	    68 	 0.06626 	 0.05555 	 ~...
   22 	    69 	 0.06360 	 0.06300 	 ~...
   18 	    70 	 0.05276 	 0.07487 	 ~...
   17 	    71 	 0.05028 	 0.07522 	 ~...
   34 	    72 	 0.08175 	 0.10910 	 ~...
   27 	    73 	 0.06748 	 0.11018 	 m..s
   23 	    74 	 0.06559 	 0.12614 	 m..s
   33 	    75 	 0.08139 	 0.12907 	 m..s
   31 	    76 	 0.08112 	 0.13062 	 m..s
   98 	    77 	 0.15644 	 0.14024 	 ~...
  101 	    78 	 0.16672 	 0.14258 	 ~...
   47 	    79 	 0.08760 	 0.14938 	 m..s
   36 	    80 	 0.08272 	 0.14941 	 m..s
   97 	    81 	 0.14781 	 0.15348 	 ~...
   96 	    82 	 0.14343 	 0.15698 	 ~...
  100 	    83 	 0.16137 	 0.16262 	 ~...
   50 	    84 	 0.09139 	 0.16363 	 m..s
   99 	    85 	 0.16129 	 0.17755 	 ~...
   54 	    86 	 0.12449 	 0.18495 	 m..s
  103 	    87 	 0.20059 	 0.18733 	 ~...
   59 	    88 	 0.12699 	 0.18944 	 m..s
   66 	    89 	 0.13028 	 0.19253 	 m..s
   52 	    90 	 0.12116 	 0.19254 	 m..s
  102 	    91 	 0.19065 	 0.19268 	 ~...
   65 	    92 	 0.12923 	 0.19892 	 m..s
   84 	    93 	 0.13615 	 0.20033 	 m..s
   63 	    94 	 0.12859 	 0.20079 	 m..s
  105 	    95 	 0.21162 	 0.20351 	 ~...
  115 	    96 	 0.23197 	 0.20414 	 ~...
   76 	    97 	 0.13258 	 0.20437 	 m..s
   60 	    98 	 0.12710 	 0.20734 	 m..s
  108 	    99 	 0.21974 	 0.20881 	 ~...
   92 	   100 	 0.14064 	 0.20947 	 m..s
   75 	   101 	 0.13253 	 0.21152 	 m..s
   94 	   102 	 0.14115 	 0.21372 	 m..s
   73 	   103 	 0.13159 	 0.21600 	 m..s
  107 	   104 	 0.21897 	 0.21730 	 ~...
   56 	   105 	 0.12676 	 0.21750 	 m..s
  106 	   106 	 0.21844 	 0.21991 	 ~...
  117 	   107 	 0.25390 	 0.22199 	 m..s
  109 	   108 	 0.22255 	 0.22374 	 ~...
   79 	   109 	 0.13346 	 0.22577 	 m..s
  110 	   110 	 0.22358 	 0.22806 	 ~...
   53 	   111 	 0.12235 	 0.22971 	 MISS
  113 	   112 	 0.22677 	 0.23319 	 ~...
  104 	   113 	 0.21046 	 0.23702 	 ~...
  118 	   114 	 0.25535 	 0.25028 	 ~...
  111 	   115 	 0.22431 	 0.25179 	 ~...
  119 	   116 	 0.25834 	 0.25739 	 ~...
  112 	   117 	 0.22436 	 0.25930 	 m..s
  116 	   118 	 0.23929 	 0.27400 	 m..s
  120 	   119 	 0.27888 	 0.28556 	 ~...
  114 	   120 	 0.23164 	 0.29787 	 m..s
==========================================
r_mrr = 0.6908584237098694
r2_mrr = 0.35804224014282227
spearmanr_mrr@5 = 0.8944076299667358
spearmanr_mrr@10 = 0.9581198692321777
spearmanr_mrr@50 = 0.8474913239479065
spearmanr_mrr@100 = 0.8719141483306885
spearmanr_mrr@All = 0.8772691488265991
==========================================
test time: 0.394
Done Testing dataset CoDExSmall
total time taken: 202.27003049850464
training time taken: 192.07804155349731
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6909)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.3580)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.8944)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9581)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8475)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8719)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8773)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.5752928631354735}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 9648260179318608
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [256, 342, 294, 785, 881, 37, 637, 616, 430, 644, 182, 1120, 1108, 309, 1078, 336, 764, 676, 333, 349, 878, 316, 1043, 613, 949, 494, 400, 908, 150, 1056, 86, 912, 445, 966, 1168, 1018, 391, 310, 573, 847, 850, 468, 1027, 280, 1156, 653, 264, 816, 36, 326, 241, 433, 1014, 94, 950, 1214, 49, 985, 904, 725, 906, 515, 188, 1034, 499, 782, 504, 215, 772, 1064, 399, 22, 1097, 105, 810, 74, 665, 187, 190, 2, 582, 893, 814, 419, 939, 781, 550, 386, 868, 12, 962, 943, 214, 638, 829, 1066, 734, 864, 640, 561, 347, 1204, 965, 1000, 558, 983, 989, 129, 45, 208, 1158, 461, 1181, 451, 397, 262, 340, 223, 954, 120, 917]
valid_ids (0): []
train_ids (1094): [857, 650, 719, 923, 1058, 625, 783, 945, 351, 125, 1136, 25, 136, 516, 395, 691, 1118, 102, 1001, 594, 1209, 549, 1103, 147, 258, 581, 1199, 5, 1153, 35, 1009, 1207, 1096, 563, 952, 1161, 465, 463, 1134, 460, 514, 675, 371, 958, 293, 658, 385, 1157, 849, 790, 259, 799, 177, 79, 875, 517, 405, 976, 897, 696, 1200, 843, 126, 513, 181, 348, 570, 163, 388, 90, 235, 431, 830, 502, 1184, 584, 471, 910, 10, 553, 443, 1203, 862, 278, 702, 576, 731, 46, 1174, 84, 577, 376, 357, 931, 598, 145, 140, 1149, 413, 325, 1054, 915, 886, 1030, 447, 331, 678, 263, 444, 104, 52, 751, 792, 905, 663, 851, 381, 979, 526, 436, 1042, 439, 353, 291, 987, 1127, 17, 760, 980, 870, 1115, 483, 184, 1109, 416, 191, 272, 1023, 143, 818, 590, 968, 685, 379, 592, 523, 64, 845, 1143, 464, 510, 162, 547, 605, 901, 610, 42, 667, 1082, 168, 527, 362, 1186, 883, 528, 882, 710, 1048, 865, 884, 1090, 559, 1057, 377, 47, 647, 787, 78, 423, 1104, 179, 932, 578, 437, 543, 1016, 1035, 642, 1210, 257, 282, 890, 560, 902, 833, 396, 960, 273, 1017, 213, 947, 113, 18, 1179, 40, 364, 251, 662, 601, 101, 686, 648, 1020, 791, 564, 1197, 995, 707, 606, 836, 290, 1132, 859, 59, 1019, 1125, 977, 1061, 72, 828, 621, 587, 343, 1065, 300, 1085, 169, 874, 307, 769, 1113, 612, 71, 363, 654, 729, 61, 1122, 321, 212, 796, 283, 478, 82, 1005, 914, 271, 1183, 426, 111, 160, 715, 540, 754, 242, 1053, 860, 572, 450, 319, 721, 335, 304, 723, 387, 34, 519, 909, 733, 53, 265, 172, 422, 903, 132, 110, 414, 420, 951, 732, 1089, 824, 207, 1013, 529, 334, 750, 858, 997, 763, 596, 1160, 604, 938, 926, 580, 1032, 359, 531, 407, 56, 1111, 755, 503, 619, 77, 557, 848, 689, 512, 1102, 372, 724, 925, 1133, 13, 934, 1198, 907, 63, 861, 520, 548, 1169, 1110, 497, 737, 27, 509, 1081, 1114, 623, 652, 155, 631, 19, 330, 122, 314, 770, 1041, 81, 591, 834, 268, 953, 891, 1182, 133, 154, 449, 482, 1140, 922, 1116, 714, 575, 244, 198, 1010, 350, 673, 773, 608, 226, 195, 96, 706, 567, 446, 490, 511, 927, 885, 602, 1155, 275, 609, 894, 67, 73, 1193, 31, 197, 793, 963, 1202, 305, 853, 969, 481, 562, 76, 116, 837, 91, 1059, 742, 127, 417, 759, 368, 1038, 972, 554, 1173, 831, 852, 970, 356, 641, 473, 588, 425, 1093, 301, 1080, 38, 1025, 981, 1172, 541, 660, 1029, 877, 1105, 856, 1098, 217, 332, 778, 1100, 599, 495, 286, 297, 645, 1063, 7, 83, 618, 863, 937, 924, 1159, 919, 33, 827, 178, 205, 338, 216, 752, 615, 85, 539, 80, 532, 930, 477, 717, 21, 1185, 185, 462, 1092, 60, 803, 720, 672, 303, 626, 1175, 284, 109, 99, 780, 323, 1086, 398, 392, 786, 175, 346, 159, 961, 703, 586, 1037, 889, 209, 700, 839, 1189, 1021, 239, 669, 1124, 199, 248, 1040, 260, 747, 1167, 967, 1206, 739, 921, 899, 681, 176, 88, 620, 518, 245, 229, 118, 448, 164, 276, 643, 123, 51, 804, 730, 1144, 629, 1045, 138, 311, 1046, 639, 234, 811, 459, 896, 928, 825, 93, 249, 538, 753, 946, 43, 1131, 821, 1074, 95, 378, 551, 100, 141, 1015, 1117, 892, 498, 777, 535, 170, 880, 156, 112, 670, 20, 406, 920, 1135, 354, 1026, 992, 666, 222, 250, 26, 867, 728, 767, 708, 32, 148, 210, 956, 1073, 1212, 1055, 589, 131, 299, 408, 466, 795, 1107, 75, 651, 668, 194, 1084, 888, 775, 193, 579, 410, 913, 281, 69, 1163, 534, 617, 383, 274, 1091, 738, 54, 990, 1130, 1094, 634, 1024, 798, 30, 722, 48, 1075, 955, 911, 237, 55, 988, 1060, 530, 761, 552, 1004, 611, 964, 743, 657, 716, 664, 1147, 1171, 114, 491, 487, 756, 569, 1049, 871, 165, 1142, 533, 595, 522, 823, 900, 467, 687, 202, 1069, 228, 1011, 66, 603, 1076, 14, 784, 1067, 434, 693, 712, 704, 339, 556, 480, 699, 566, 1083, 1195, 246, 774, 986, 412, 374, 11, 916, 475, 600, 324, 835, 65, 684, 1201, 971, 402, 959, 500, 139, 382, 656, 1148, 384, 158, 812, 735, 415, 1154, 635, 1192, 501, 671, 855, 1176, 525, 1178, 994, 367, 121, 933, 749, 16, 805, 1051, 456, 380, 876, 255, 24, 661, 6, 745, 701, 1187, 130, 794, 269, 973, 329, 211, 746, 627, 1146, 457, 1180, 758, 674, 289, 369, 762, 574, 679, 630, 1208, 768, 128, 683, 1039, 1028, 9, 1126, 157, 295, 1141, 1121, 840, 524, 1031, 203, 935, 108, 243, 1044, 233, 292, 1079, 124, 427, 469, 236, 225, 998, 1165, 313, 1101, 887, 390, 315, 173, 8, 1, 705, 555, 328, 872, 1128, 841, 993, 942, 152, 428, 948, 394, 607, 44, 624, 3, 403, 401, 1003, 779, 842, 438, 545, 161, 221, 1205, 58, 489, 1008, 736, 655, 373, 219, 895, 404, 1006, 748, 1022, 151, 320, 247, 832, 393, 149, 117, 485, 429, 646, 200, 50, 298, 727, 39, 941, 166, 1139, 238, 869, 991, 296, 713, 789, 484, 936, 375, 288, 788, 218, 424, 680, 695, 593, 492, 317, 999, 632, 1213, 496, 366, 365, 41, 153, 822, 361, 1072, 726, 370, 231, 940, 508, 254, 62, 411, 106, 201, 192, 435, 694, 1070, 622, 698, 806, 421, 227, 279, 224, 1152, 776, 659, 565, 455, 807, 1190, 677, 232, 1099, 29, 1150, 302, 134, 312, 189, 28, 813, 771, 1106, 493, 978, 809, 546, 802, 355, 929, 252, 975, 797, 135, 360, 285, 1164, 476, 697, 144, 119, 89, 984, 1062, 472, 453, 741, 454, 568, 996, 103, 571, 918, 196, 167, 1211, 597, 479, 341, 506, 441, 765, 1052, 261, 137, 800, 146, 536, 866, 23, 186, 352, 389, 452, 633, 718, 1047, 766, 614, 521, 1138, 230, 1137, 544, 344, 220, 1123, 507, 1166, 879, 1071, 474, 740, 682, 838, 1033, 4, 345, 171, 1077, 1162, 174, 418, 583, 690, 898, 97, 488, 505, 1007, 70, 327, 92, 253, 358, 0, 711, 1170, 409, 826, 308, 266, 1129, 1068, 318, 944, 1088, 974, 57, 1012, 815, 628, 1177, 306, 982, 432, 844, 636, 87, 277, 442, 846, 458, 270, 287, 1087, 1145, 692, 470, 142, 107, 98, 649, 240, 542, 180, 801, 1036, 1151, 585, 1188, 1119, 808, 537, 957, 204, 1191, 183, 337, 1194, 1050, 1112, 322, 1002, 15, 709, 1196, 1095, 819, 440, 688, 115, 757, 486, 267, 820, 206, 873, 68, 854, 744, 817]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2110502777337082
the save name prefix for this run is:  chkpt-ID_2110502777337082_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 296
rank avg (pred): 0.466 +- 0.014
mrr vals (pred, true): 0.001, 0.295
batch losses (mrrl, rdl): 0.0, 0.0039189137

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 905
rank avg (pred): 0.468 +- 0.286
mrr vals (pred, true): 0.014, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002347032

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 117
rank avg (pred): 0.262 +- 0.190
mrr vals (pred, true): 0.183, 0.225
batch losses (mrrl, rdl): 0.0, 0.0010863594

Epoch over!
epoch time: 12.332

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.035 +- 0.027
mrr vals (pred, true): 0.283, 0.223
batch losses (mrrl, rdl): 0.0, 3.2051e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 663
rank avg (pred): 0.409 +- 0.302
mrr vals (pred, true): 0.222, 0.004
batch losses (mrrl, rdl): 0.0, 3.38003e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 402
rank avg (pred): 0.268 +- 0.228
mrr vals (pred, true): 0.286, 0.238
batch losses (mrrl, rdl): 0.0, 0.0011558022

Epoch over!
epoch time: 12.087

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 181
rank avg (pred): 0.263 +- 0.229
mrr vals (pred, true): 0.291, 0.005
batch losses (mrrl, rdl): 0.0, 0.0006255207

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 15
rank avg (pred): 0.033 +- 0.030
mrr vals (pred, true): 0.344, 0.256
batch losses (mrrl, rdl): 0.0, 5.569e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 431
rank avg (pred): 0.238 +- 0.214
mrr vals (pred, true): 0.295, 0.004
batch losses (mrrl, rdl): 0.0, 0.00087285

Epoch over!
epoch time: 12.226

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1049
rank avg (pred): 0.231 +- 0.212
mrr vals (pred, true): 0.309, 0.005
batch losses (mrrl, rdl): 0.0, 0.0010563161

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1059
rank avg (pred): 0.032 +- 0.030
mrr vals (pred, true): 0.355, 0.316
batch losses (mrrl, rdl): 0.0, 5.463e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1031
rank avg (pred): 0.245 +- 0.219
mrr vals (pred, true): 0.278, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007648406

Epoch over!
epoch time: 12.037

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 795
rank avg (pred): 0.405 +- 0.341
mrr vals (pred, true): 0.267, 0.005
batch losses (mrrl, rdl): 0.0, 4.54945e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1178
rank avg (pred): 0.283 +- 0.242
mrr vals (pred, true): 0.284, 0.163
batch losses (mrrl, rdl): 0.0, 0.0003988793

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 678
rank avg (pred): 0.339 +- 0.285
mrr vals (pred, true): 0.262, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002129021

Epoch over!
epoch time: 12.611

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 751
rank avg (pred): 0.069 +- 0.070
mrr vals (pred, true): 0.383, 0.167
batch losses (mrrl, rdl): 0.4689722955, 1.10568e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 594
rank avg (pred): 0.432 +- 0.275
mrr vals (pred, true): 0.112, 0.121
batch losses (mrrl, rdl): 0.0006665217, 0.0013437495

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 531
rank avg (pred): 0.329 +- 0.229
mrr vals (pred, true): 0.116, 0.035
batch losses (mrrl, rdl): 0.0433819182, 0.0002692404

Epoch over!
epoch time: 12.447

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1198
rank avg (pred): 0.391 +- 0.210
mrr vals (pred, true): 0.079, 0.004
batch losses (mrrl, rdl): 0.0086800717, 0.0001087777

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1167
rank avg (pred): 0.400 +- 0.236
mrr vals (pred, true): 0.087, 0.115
batch losses (mrrl, rdl): 0.0075204922, 0.0006194545

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 583
rank avg (pred): 0.326 +- 0.220
mrr vals (pred, true): 0.111, 0.129
batch losses (mrrl, rdl): 0.0033205203, 0.0003408991

Epoch over!
epoch time: 12.343

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 255
rank avg (pred): 0.033 +- 0.025
mrr vals (pred, true): 0.264, 0.219
batch losses (mrrl, rdl): 0.0203052256, 5.0771e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 777
rank avg (pred): 0.435 +- 0.235
mrr vals (pred, true): 0.070, 0.046
batch losses (mrrl, rdl): 0.0039236625, 0.0001263554

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 770
rank avg (pred): 0.396 +- 0.228
mrr vals (pred, true): 0.079, 0.026
batch losses (mrrl, rdl): 0.0083958721, 0.000275107

Epoch over!
epoch time: 12.987

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 5
rank avg (pred): 0.057 +- 0.049
mrr vals (pred, true): 0.243, 0.237
batch losses (mrrl, rdl): 0.0002877426, 1.08668e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 177
rank avg (pred): 0.278 +- 0.183
mrr vals (pred, true): 0.106, 0.005
batch losses (mrrl, rdl): 0.0314726345, 0.0006901548

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 775
rank avg (pred): 0.504 +- 0.247
mrr vals (pred, true): 0.036, 0.074
batch losses (mrrl, rdl): 0.0018814647, 0.0007870863

Epoch over!
epoch time: 12.349

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 576
rank avg (pred): 0.288 +- 0.190
mrr vals (pred, true): 0.096, 0.105
batch losses (mrrl, rdl): 0.0008279572, 6.01247e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1091
rank avg (pred): 0.246 +- 0.167
mrr vals (pred, true): 0.134, 0.200
batch losses (mrrl, rdl): 0.0437514782, 0.0008047178

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 186
rank avg (pred): 0.299 +- 0.193
mrr vals (pred, true): 0.098, 0.003
batch losses (mrrl, rdl): 0.0229168795, 0.0006233365

Epoch over!
epoch time: 12.853

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 968
rank avg (pred): 0.541 +- 0.281
mrr vals (pred, true): 0.031, 0.004
batch losses (mrrl, rdl): 0.0035267556, 8.76526e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 15
rank avg (pred): 0.079 +- 0.064
mrr vals (pred, true): 0.228, 0.256
batch losses (mrrl, rdl): 0.0079598231, 4.89935e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 229
rank avg (pred): 0.259 +- 0.168
mrr vals (pred, true): 0.132, 0.004
batch losses (mrrl, rdl): 0.0676937252, 0.0009070357

Epoch over!
epoch time: 13.049

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1029
rank avg (pred): 0.297 +- 0.184
mrr vals (pred, true): 0.108, 0.004
batch losses (mrrl, rdl): 0.0341807157, 0.0005978945

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 446
rank avg (pred): 0.269 +- 0.167
mrr vals (pred, true): 0.120, 0.004
batch losses (mrrl, rdl): 0.0483090244, 0.0008688621

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 752
rank avg (pred): 0.209 +- 0.140
mrr vals (pred, true): 0.181, 0.157
batch losses (mrrl, rdl): 0.0054117441, 0.000585632

Epoch over!
epoch time: 12.371

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 412
rank avg (pred): 0.266 +- 0.170
mrr vals (pred, true): 0.132, 0.004
batch losses (mrrl, rdl): 0.067953445, 0.0008346074

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 260
rank avg (pred): 0.102 +- 0.080
mrr vals (pred, true): 0.221, 0.235
batch losses (mrrl, rdl): 0.0019356799, 8.21847e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 172
rank avg (pred): 0.288 +- 0.176
mrr vals (pred, true): 0.112, 0.004
batch losses (mrrl, rdl): 0.038748648, 0.0006438988

Epoch over!
epoch time: 12.897

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 283
rank avg (pred): 0.067 +- 0.059
mrr vals (pred, true): 0.239, 0.275
batch losses (mrrl, rdl): 0.0133746285, 1.46798e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 96
rank avg (pred): 0.292 +- 0.171
mrr vals (pred, true): 0.111, 0.225
batch losses (mrrl, rdl): 0.1293696761, 0.0012889454

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 110
rank avg (pred): 0.252 +- 0.159
mrr vals (pred, true): 0.129, 0.198
batch losses (mrrl, rdl): 0.0475879461, 0.0008608719

Epoch over!
epoch time: 12.484

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 882
rank avg (pred): 0.443 +- 0.236
mrr vals (pred, true): 0.043, 0.004
batch losses (mrrl, rdl): 0.0004993215, 1.81898e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 453
rank avg (pred): 0.297 +- 0.164
mrr vals (pred, true): 0.109, 0.004
batch losses (mrrl, rdl): 0.0346022323, 0.0006586068

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 981
rank avg (pred): 0.017 +- 0.013
mrr vals (pred, true): 0.278, 0.207
batch losses (mrrl, rdl): 0.0502244458, 2.01916e-05

Epoch over!
epoch time: 12.366

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.122 +- 0.095
mrr vals (pred, true): 0.228, 0.224

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.04257 	 0.00074 	 m..s
   15 	     1 	 0.04922 	 0.00076 	 m..s
   10 	     2 	 0.04423 	 0.00089 	 m..s
   14 	     3 	 0.04520 	 0.00094 	 m..s
   13 	     4 	 0.04494 	 0.00104 	 m..s
    3 	     5 	 0.03910 	 0.00107 	 m..s
    2 	     6 	 0.03871 	 0.00141 	 m..s
   22 	     7 	 0.05940 	 0.00148 	 m..s
    6 	     8 	 0.04238 	 0.00312 	 m..s
   30 	     9 	 0.07717 	 0.00341 	 m..s
   68 	    10 	 0.13850 	 0.00343 	 MISS
    8 	    11 	 0.04261 	 0.00348 	 m..s
    5 	    12 	 0.04173 	 0.00351 	 m..s
   88 	    13 	 0.14616 	 0.00359 	 MISS
    4 	    14 	 0.04022 	 0.00362 	 m..s
   34 	    15 	 0.09636 	 0.00363 	 m..s
   73 	    16 	 0.14200 	 0.00364 	 MISS
   40 	    17 	 0.10131 	 0.00369 	 m..s
   75 	    18 	 0.14226 	 0.00372 	 MISS
   77 	    19 	 0.14277 	 0.00376 	 MISS
    1 	    20 	 0.03247 	 0.00383 	 ~...
   11 	    21 	 0.04446 	 0.00384 	 m..s
   60 	    22 	 0.13310 	 0.00389 	 MISS
   63 	    23 	 0.13578 	 0.00394 	 MISS
   46 	    24 	 0.11326 	 0.00395 	 MISS
   59 	    25 	 0.13298 	 0.00395 	 MISS
   36 	    26 	 0.09950 	 0.00400 	 m..s
   39 	    27 	 0.10056 	 0.00401 	 m..s
   74 	    28 	 0.14202 	 0.00411 	 MISS
   17 	    29 	 0.05044 	 0.00412 	 m..s
   92 	    30 	 0.14745 	 0.00417 	 MISS
   67 	    31 	 0.13742 	 0.00419 	 MISS
    9 	    32 	 0.04396 	 0.00429 	 m..s
   70 	    33 	 0.13899 	 0.00431 	 MISS
    0 	    34 	 0.03225 	 0.00431 	 ~...
   80 	    35 	 0.14340 	 0.00433 	 MISS
   79 	    36 	 0.14325 	 0.00442 	 MISS
   87 	    37 	 0.14486 	 0.00443 	 MISS
   91 	    38 	 0.14714 	 0.00458 	 MISS
   66 	    39 	 0.13735 	 0.00469 	 MISS
   54 	    40 	 0.12706 	 0.00498 	 MISS
   90 	    41 	 0.14712 	 0.00499 	 MISS
   37 	    42 	 0.09964 	 0.00578 	 m..s
   76 	    43 	 0.14257 	 0.00587 	 MISS
   29 	    44 	 0.07357 	 0.00607 	 m..s
   64 	    45 	 0.13579 	 0.00639 	 MISS
   24 	    46 	 0.06767 	 0.02089 	 m..s
   12 	    47 	 0.04456 	 0.02133 	 ~...
   25 	    48 	 0.06801 	 0.02208 	 m..s
   26 	    49 	 0.06863 	 0.02510 	 m..s
   20 	    50 	 0.05735 	 0.02871 	 ~...
   51 	    51 	 0.12499 	 0.03250 	 m..s
   21 	    52 	 0.05813 	 0.03267 	 ~...
   18 	    53 	 0.05168 	 0.03829 	 ~...
   28 	    54 	 0.07332 	 0.04607 	 ~...
   16 	    55 	 0.05028 	 0.04783 	 ~...
   23 	    56 	 0.06106 	 0.05123 	 ~...
   49 	    57 	 0.11635 	 0.05123 	 m..s
   19 	    58 	 0.05180 	 0.05856 	 ~...
   27 	    59 	 0.07261 	 0.05984 	 ~...
   48 	    60 	 0.11430 	 0.07000 	 m..s
   35 	    61 	 0.09664 	 0.10123 	 ~...
   32 	    62 	 0.09319 	 0.10487 	 ~...
   33 	    63 	 0.09404 	 0.10742 	 ~...
   31 	    64 	 0.08441 	 0.14559 	 m..s
   42 	    65 	 0.11132 	 0.14938 	 m..s
   47 	    66 	 0.11398 	 0.15104 	 m..s
   41 	    67 	 0.10505 	 0.15348 	 m..s
   94 	    68 	 0.15612 	 0.15719 	 ~...
   96 	    69 	 0.16392 	 0.16097 	 ~...
   95 	    70 	 0.15804 	 0.16374 	 ~...
   43 	    71 	 0.11180 	 0.16391 	 m..s
   45 	    72 	 0.11273 	 0.16518 	 m..s
   44 	    73 	 0.11228 	 0.16875 	 m..s
   38 	    74 	 0.10056 	 0.17057 	 m..s
   61 	    75 	 0.13396 	 0.17763 	 m..s
   52 	    76 	 0.12641 	 0.18239 	 m..s
   55 	    77 	 0.12775 	 0.18539 	 m..s
   82 	    78 	 0.14367 	 0.18540 	 m..s
   86 	    79 	 0.14480 	 0.18588 	 m..s
   53 	    80 	 0.12665 	 0.19709 	 m..s
   81 	    81 	 0.14365 	 0.20127 	 m..s
   50 	    82 	 0.12392 	 0.20155 	 m..s
   78 	    83 	 0.14301 	 0.20313 	 m..s
  101 	    84 	 0.20446 	 0.20351 	 ~...
   99 	    85 	 0.19140 	 0.20414 	 ~...
  114 	    86 	 0.23833 	 0.20951 	 ~...
   98 	    87 	 0.18500 	 0.21152 	 ~...
  103 	    88 	 0.21034 	 0.21168 	 ~...
   85 	    89 	 0.14443 	 0.21345 	 m..s
   93 	    90 	 0.15202 	 0.21576 	 m..s
   72 	    91 	 0.14029 	 0.21721 	 m..s
   62 	    92 	 0.13557 	 0.21770 	 m..s
   58 	    93 	 0.13176 	 0.21956 	 m..s
   83 	    94 	 0.14409 	 0.21972 	 m..s
  104 	    95 	 0.21090 	 0.21997 	 ~...
  113 	    96 	 0.23747 	 0.22067 	 ~...
   97 	    97 	 0.18429 	 0.22109 	 m..s
  106 	    98 	 0.22753 	 0.22374 	 ~...
   71 	    99 	 0.13945 	 0.22649 	 m..s
   69 	   100 	 0.13875 	 0.22807 	 m..s
   56 	   101 	 0.12925 	 0.22908 	 m..s
   65 	   102 	 0.13647 	 0.22917 	 m..s
   57 	   103 	 0.13028 	 0.22931 	 m..s
  112 	   104 	 0.23739 	 0.23018 	 ~...
  102 	   105 	 0.20905 	 0.23088 	 ~...
   89 	   106 	 0.14693 	 0.23382 	 m..s
   84 	   107 	 0.14422 	 0.23667 	 m..s
  111 	   108 	 0.23530 	 0.23706 	 ~...
  116 	   109 	 0.24358 	 0.23772 	 ~...
  109 	   110 	 0.23133 	 0.23944 	 ~...
  115 	   111 	 0.23883 	 0.24592 	 ~...
  110 	   112 	 0.23149 	 0.25534 	 ~...
  100 	   113 	 0.19293 	 0.25578 	 m..s
  107 	   114 	 0.23082 	 0.25930 	 ~...
  108 	   115 	 0.23118 	 0.26518 	 m..s
  118 	   116 	 0.25448 	 0.27400 	 ~...
  120 	   117 	 0.26343 	 0.27924 	 ~...
  105 	   118 	 0.22110 	 0.30182 	 m..s
  119 	   119 	 0.25965 	 0.30373 	 m..s
  117 	   120 	 0.24475 	 0.30443 	 m..s
==========================================
r_mrr = 0.7291773557662964
r2_mrr = 0.4786618947982788
spearmanr_mrr@5 = 0.9513016939163208
spearmanr_mrr@10 = 0.9693549275398254
spearmanr_mrr@50 = 0.885844886302948
spearmanr_mrr@100 = 0.8740764856338501
spearmanr_mrr@All = 0.8946552276611328
==========================================
test time: 0.508
Done Testing dataset CoDExSmall
total time taken: 197.6669442653656
training time taken: 188.0240330696106
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7292)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4787)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9513)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9694)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8858)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8741)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8947)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.8501357395325613}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 4586819324266094
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [284, 831, 246, 71, 19, 361, 1166, 1134, 1201, 722, 816, 3, 731, 364, 81, 1138, 112, 931, 782, 296, 691, 935, 608, 69, 336, 923, 894, 992, 297, 38, 1068, 1186, 370, 51, 735, 42, 910, 405, 305, 1167, 554, 880, 646, 221, 308, 456, 355, 248, 399, 867, 198, 555, 602, 957, 786, 34, 23, 1038, 885, 961, 377, 228, 14, 624, 876, 1070, 408, 373, 897, 779, 1094, 217, 651, 604, 467, 1159, 161, 1132, 33, 1026, 850, 466, 956, 113, 421, 1183, 1149, 703, 349, 785, 881, 643, 639, 411, 896, 201, 844, 803, 443, 520, 548, 1205, 274, 930, 967, 92, 236, 1071, 149, 693, 671, 1187, 243, 969, 622, 630, 748, 613, 777, 383, 676]
valid_ids (0): []
train_ids (1094): [1088, 1192, 629, 1098, 823, 1120, 921, 534, 186, 718, 62, 524, 340, 57, 801, 889, 258, 828, 345, 18, 1154, 1007, 26, 440, 1180, 591, 1066, 389, 479, 1050, 714, 331, 721, 496, 679, 1073, 756, 1041, 561, 668, 323, 354, 487, 346, 556, 793, 747, 653, 356, 552, 610, 615, 887, 271, 214, 877, 379, 189, 917, 893, 859, 235, 905, 1001, 860, 819, 725, 128, 631, 821, 787, 1080, 759, 483, 315, 589, 448, 523, 1165, 337, 1122, 978, 713, 412, 1004, 338, 1097, 133, 1058, 266, 974, 299, 1000, 868, 277, 1016, 963, 663, 288, 376, 550, 1101, 352, 832, 1121, 773, 400, 115, 1191, 1012, 754, 446, 962, 543, 68, 451, 817, 66, 162, 290, 1190, 692, 1059, 110, 429, 1052, 792, 928, 80, 570, 966, 845, 410, 234, 511, 1175, 392, 253, 566, 30, 260, 362, 229, 951, 997, 197, 517, 879, 884, 557, 477, 150, 560, 233, 933, 1048, 1025, 1130, 372, 494, 763, 551, 1072, 1053, 344, 1092, 934, 999, 584, 125, 837, 899, 24, 929, 525, 460, 797, 160, 285, 690, 680, 869, 1086, 843, 132, 380, 1140, 141, 302, 397, 681, 450, 1013, 516, 1069, 914, 1185, 170, 1079, 158, 537, 533, 387, 851, 509, 12, 1179, 423, 901, 968, 965, 518, 1199, 28, 343, 497, 226, 848, 758, 637, 508, 324, 209, 946, 134, 1145, 638, 925, 611, 438, 120, 307, 49, 7, 1116, 757, 665, 1177, 402, 163, 1198, 15, 872, 1125, 598, 122, 1207, 375, 108, 1168, 334, 52, 585, 1163, 804, 4, 368, 526, 401, 640, 842, 127, 318, 1024, 1114, 712, 1084, 871, 328, 977, 1076, 737, 949, 674, 64, 454, 199, 168, 195, 1011, 357, 242, 781, 565, 1184, 240, 633, 687, 612, 41, 959, 1210, 601, 67, 683, 393, 43, 970, 507, 1206, 776, 701, 188, 1081, 1054, 1003, 280, 1214, 1027, 1146, 841, 16, 21, 652, 82, 22, 491, 634, 689, 648, 856, 529, 60, 157, 97, 603, 211, 891, 79, 427, 874, 780, 164, 461, 70, 101, 586, 490, 498, 220, 1049, 655, 771, 495, 519, 251, 169, 707, 1115, 1082, 826, 558, 109, 244, 219, 590, 532, 1144, 1010, 635, 366, 938, 8, 10, 1119, 784, 1062, 121, 861, 892, 1161, 1074, 697, 649, 762, 728, 350, 453, 87, 203, 594, 696, 1113, 814, 417, 979, 107, 1006, 1089, 265, 666, 911, 796, 1176, 314, 751, 138, 862, 213, 709, 184, 878, 1021, 670, 1141, 29, 732, 47, 353, 789, 93, 1103, 937, 906, 800, 152, 866, 650, 1112, 890, 514, 208, 1015, 886, 123, 472, 455, 846, 1102, 40, 326, 499, 176, 702, 644, 799, 404, 470, 27, 658, 73, 738, 282, 1008, 645, 147, 617, 912, 990, 175, 1031, 1211, 1139, 659, 1203, 319, 824, 247, 972, 291, 1106, 656, 1197, 950, 895, 434, 593, 310, 1036, 794, 898, 1075, 1160, 11, 863, 1034, 167, 119, 976, 452, 1174, 528, 694, 322, 873, 941, 486, 984, 54, 853, 301, 1104, 1057, 717, 420, 268, 818, 135, 306, 165, 1035, 657, 954, 58, 682, 245, 185, 753, 53, 791, 191, 447, 330, 231, 730, 916, 1150, 1032, 178, 812, 1117, 143, 1039, 329, 388, 945, 286, 813, 918, 685, 391, 1093, 182, 936, 283, 1123, 85, 578, 744, 750, 1044, 287, 430, 348, 426, 275, 17, 218, 742, 926, 259, 261, 1213, 1204, 215, 146, 562, 339, 173, 545, 567, 541, 815, 705, 729, 332, 1195, 662, 437, 413, 571, 765, 1, 398, 367, 710, 588, 641, 192, 210, 1143, 661, 991, 964, 445, 1051, 194, 327, 838, 269, 196, 1037, 900, 95, 960, 320, 289, 1046, 238, 549, 431, 1107, 313, 1009, 985, 432, 493, 1083, 227, 971, 428, 190, 772, 806, 124, 711, 471, 761, 769, 920, 475, 1078, 395, 875, 913, 607, 241, 358, 580, 363, 1042, 544, 1126, 409, 442, 1200, 542, 317, 1018, 174, 333, 86, 422, 205, 749, 1047, 888, 369, 577, 778, 1181, 232, 582, 940, 1189, 1171, 724, 870, 677, 419, 530, 106, 942, 72, 131, 56, 74, 745, 802, 156, 278, 1096, 783, 407, 767, 276, 988, 335, 1055, 994, 204, 118, 609, 351, 180, 1040, 298, 256, 396, 177, 148, 114, 84, 715, 136, 295, 484, 249, 482, 1148, 675, 788, 425, 1194, 883, 1085, 102, 104, 600, 1129, 130, 1155, 347, 907, 987, 144, 932, 250, 535, 489, 212, 193, 669, 621, 740, 1100, 839, 1182, 605, 441, 513, 1014, 78, 581, 309, 625, 538, 480, 596, 223, 465, 599, 378, 628, 614, 1109, 667, 45, 371, 374, 183, 1063, 569, 111, 1162, 385, 706, 834, 65, 833, 1188, 512, 61, 1212, 312, 515, 684, 126, 840, 947, 381, 501, 727, 695, 755, 25, 403, 958, 230, 574, 903, 943, 1169, 980, 116, 986, 636, 808, 1208, 996, 1135, 536, 416, 88, 1105, 647, 311, 207, 36, 733, 1118, 811, 583, 390, 83, 764, 632, 272, 592, 418, 865, 1172, 1017, 1170, 564, 944, 807, 500, 1147, 224, 1045, 449, 1124, 618, 187, 5, 13, 1164, 506, 222, 94, 382, 1196, 365, 1110, 664, 855, 206, 281, 678, 504, 904, 1023, 252, 836, 485, 1173, 492, 1091, 273, 1202, 98, 181, 716, 1002, 809, 955, 626, 522, 540, 909, 129, 768, 760, 1028, 468, 546, 46, 37, 736, 752, 76, 316, 539, 464, 708, 798, 620, 686, 444, 90, 386, 1033, 830, 1152, 59, 1178, 254, 654, 852, 154, 159, 563, 166, 827, 1151, 939, 321, 32, 975, 433, 1137, 835, 1099, 1136, 1193, 559, 1153, 502, 137, 510, 521, 1020, 153, 304, 6, 790, 117, 435, 673, 952, 202, 478, 1064, 257, 1061, 1056, 48, 527, 847, 948, 766, 1029, 1087, 384, 103, 595, 726, 998, 225, 1127, 457, 462, 0, 20, 1043, 770, 424, 704, 849, 100, 39, 96, 924, 267, 75, 415, 91, 294, 805, 660, 989, 476, 774, 915, 89, 142, 463, 829, 35, 179, 77, 406, 575, 1133, 1157, 739, 746, 1108, 1111, 488, 723, 9, 474, 151, 719, 439, 459, 579, 572, 50, 698, 503, 775, 239, 44, 973, 858, 810, 360, 2, 325, 993, 394, 642, 1128, 469, 155, 882, 436, 1158, 795, 1060, 619, 820, 741, 1019, 140, 927, 237, 1077, 700, 31, 293, 481, 1090, 1156, 587, 292, 341, 264, 983, 99, 473, 1209, 981, 262, 105, 919, 606, 505, 139, 743, 145, 922, 864, 255, 359, 300, 1095, 200, 672, 1022, 263, 627, 573, 279, 902, 568, 688, 63, 531, 982, 576, 995, 270, 547, 171, 822, 908, 953, 1065, 616, 458, 699, 1142, 342, 597, 1067, 1131, 553, 414, 734, 1005, 825, 720, 857, 172, 623, 854, 303, 1030, 216, 55]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9851026077322748
the save name prefix for this run is:  chkpt-ID_9851026077322748_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 918
rank avg (pred): 0.414 +- 0.006
mrr vals (pred, true): 0.001, 0.002
batch losses (mrrl, rdl): 0.0, 0.0007948012

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 987
rank avg (pred): 0.107 +- 0.068
mrr vals (pred, true): 0.058, 0.292
batch losses (mrrl, rdl): 0.0, 0.0001062745

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1064
rank avg (pred): 0.044 +- 0.037
mrr vals (pred, true): 0.289, 0.221
batch losses (mrrl, rdl): 0.0, 2.9897e-06

Epoch over!
epoch time: 13.022

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 103
rank avg (pred): 0.242 +- 0.202
mrr vals (pred, true): 0.263, 0.200
batch losses (mrrl, rdl): 0.0, 0.0009266385

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 53
rank avg (pred): 0.064 +- 0.058
mrr vals (pred, true): 0.340, 0.234
batch losses (mrrl, rdl): 0.0, 8.2978e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 709
rank avg (pred): 0.289 +- 0.245
mrr vals (pred, true): 0.282, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005480268

Epoch over!
epoch time: 12.536

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 533
rank avg (pred): 0.195 +- 0.184
mrr vals (pred, true): 0.334, 0.044
batch losses (mrrl, rdl): 0.0, 1.93442e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 851
rank avg (pred): 0.412 +- 0.308
mrr vals (pred, true): 0.214, 0.042
batch losses (mrrl, rdl): 0.0, 0.0003087111

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 83
rank avg (pred): 0.273 +- 0.238
mrr vals (pred, true): 0.266, 0.186
batch losses (mrrl, rdl): 0.0, 0.0011231302

Epoch over!
epoch time: 12.319

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1189
rank avg (pred): 0.352 +- 0.297
mrr vals (pred, true): 0.271, 0.005
batch losses (mrrl, rdl): 0.0, 0.0001649792

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 813
rank avg (pred): 0.154 +- 0.151
mrr vals (pred, true): 0.383, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001008896

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 747
rank avg (pred): 0.118 +- 0.114
mrr vals (pred, true): 0.377, 0.166
batch losses (mrrl, rdl): 0.0, 0.0001147192

Epoch over!
epoch time: 12.044

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1100
rank avg (pred): 0.240 +- 0.227
mrr vals (pred, true): 0.334, 0.200
batch losses (mrrl, rdl): 0.0, 0.0008723258

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1008
rank avg (pred): 0.241 +- 0.230
mrr vals (pred, true): 0.337, 0.198
batch losses (mrrl, rdl): 0.0, 0.000895738

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 169
rank avg (pred): 0.244 +- 0.229
mrr vals (pred, true): 0.332, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009550654

Epoch over!
epoch time: 12.432

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 406
rank avg (pred): 0.266 +- 0.241
mrr vals (pred, true): 0.314, 0.004
batch losses (mrrl, rdl): 0.6945845485, 0.0006152118

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 119
rank avg (pred): 0.374 +- 0.194
mrr vals (pred, true): 0.071, 0.200
batch losses (mrrl, rdl): 0.1673056036, 0.0023380057

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 744
rank avg (pred): 0.284 +- 0.191
mrr vals (pred, true): 0.182, 0.128
batch losses (mrrl, rdl): 0.0289365929, 0.0012980272

Epoch over!
epoch time: 12.515

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 939
rank avg (pred): 0.560 +- 0.167
mrr vals (pred, true): 0.028, 0.001
batch losses (mrrl, rdl): 0.0049340748, 0.0008865757

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 303
rank avg (pred): 0.041 +- 0.029
mrr vals (pred, true): 0.249, 0.216
batch losses (mrrl, rdl): 0.0109558003, 5.3977e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 333
rank avg (pred): 0.337 +- 0.186
mrr vals (pred, true): 0.125, 0.229
batch losses (mrrl, rdl): 0.1092803925, 0.0018358085

Epoch over!
epoch time: 12.188

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 877
rank avg (pred): 0.393 +- 0.153
mrr vals (pred, true): 0.068, 0.004
batch losses (mrrl, rdl): 0.0032529153, 0.0001736562

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 253
rank avg (pred): 0.029 +- 0.020
mrr vals (pred, true): 0.259, 0.225
batch losses (mrrl, rdl): 0.0116587477, 1.08757e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.373 +- 0.151
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0070363702, 0.0002298604

Epoch over!
epoch time: 12.472

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 698
rank avg (pred): 0.341 +- 0.151
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.016528355, 0.0003725829

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 686
rank avg (pred): 0.325 +- 0.154
mrr vals (pred, true): 0.110, 0.004
batch losses (mrrl, rdl): 0.0355343446, 0.0004176068

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.364 +- 0.136
mrr vals (pred, true): 0.064, 0.145
batch losses (mrrl, rdl): 0.065273501, 0.0007854895

Epoch over!
epoch time: 12.863

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 583
rank avg (pred): 0.353 +- 0.158
mrr vals (pred, true): 0.093, 0.129
batch losses (mrrl, rdl): 0.0125983302, 0.0004857531

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 713
rank avg (pred): 0.340 +- 0.124
mrr vals (pred, true): 0.058, 0.006
batch losses (mrrl, rdl): 0.0006907129, 0.0004533313

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1084
rank avg (pred): 0.310 +- 0.140
mrr vals (pred, true): 0.101, 0.199
batch losses (mrrl, rdl): 0.0966799408, 0.0014593789

Epoch over!
epoch time: 12.429

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.356 +- 0.136
mrr vals (pred, true): 0.076, 0.004
batch losses (mrrl, rdl): 0.0065393131, 0.000317437

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 390
rank avg (pred): 0.279 +- 0.158
mrr vals (pred, true): 0.138, 0.231
batch losses (mrrl, rdl): 0.0873197466, 0.0011157474

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1153
rank avg (pred): 0.329 +- 0.123
mrr vals (pred, true): 0.078, 0.052
batch losses (mrrl, rdl): 0.0079756612, 0.0003910621

Epoch over!
epoch time: 12.888

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1023
rank avg (pred): 0.280 +- 0.149
mrr vals (pred, true): 0.117, 0.206
batch losses (mrrl, rdl): 0.0790417194, 0.0011820414

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 849
rank avg (pred): 0.378 +- 0.143
mrr vals (pred, true): 0.070, 0.044
batch losses (mrrl, rdl): 0.003923981, 0.000262557

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1040
rank avg (pred): 0.273 +- 0.151
mrr vals (pred, true): 0.138, 0.004
batch losses (mrrl, rdl): 0.0767120048, 0.0007877569

Epoch over!
epoch time: 12.314

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 52
rank avg (pred): 0.085 +- 0.056
mrr vals (pred, true): 0.211, 0.225
batch losses (mrrl, rdl): 0.0019453738, 3.06881e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 384
rank avg (pred): 0.285 +- 0.145
mrr vals (pred, true): 0.110, 0.222
batch losses (mrrl, rdl): 0.1267027706, 0.0011744956

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 430
rank avg (pred): 0.282 +- 0.137
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0453944132, 0.0008101235

Epoch over!
epoch time: 12.819

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 913
rank avg (pred): 0.518 +- 0.245
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0005283288, 3.24329e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 307
rank avg (pred): 0.023 +- 0.015
mrr vals (pred, true): 0.271, 0.263
batch losses (mrrl, rdl): 0.0005588437, 1.39027e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1164
rank avg (pred): 0.438 +- 0.157
mrr vals (pred, true): 0.054, 0.106
batch losses (mrrl, rdl): 0.0265411064, 0.0008860502

Epoch over!
epoch time: 12.666

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 277
rank avg (pred): 0.035 +- 0.023
mrr vals (pred, true): 0.244, 0.226
batch losses (mrrl, rdl): 0.0033130057, 9.1231e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 563
rank avg (pred): 0.308 +- 0.111
mrr vals (pred, true): 0.074, 0.040
batch losses (mrrl, rdl): 0.0058777654, 0.000162453

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1129
rank avg (pred): 0.262 +- 0.136
mrr vals (pred, true): 0.132, 0.005
batch losses (mrrl, rdl): 0.067143023, 0.0009254371

Epoch over!
epoch time: 12.459

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.160 +- 0.103
mrr vals (pred, true): 0.238, 0.277

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.03741 	 0.00074 	 m..s
    0 	     1 	 0.03256 	 0.00075 	 m..s
    5 	     2 	 0.03724 	 0.00075 	 m..s
    8 	     3 	 0.03747 	 0.00076 	 m..s
   26 	     4 	 0.07341 	 0.00097 	 m..s
   57 	     5 	 0.11124 	 0.00104 	 MISS
   20 	     6 	 0.07093 	 0.00116 	 m..s
   23 	     7 	 0.07290 	 0.00219 	 m..s
    9 	     8 	 0.04800 	 0.00251 	 m..s
   29 	     9 	 0.07441 	 0.00288 	 m..s
   56 	    10 	 0.10886 	 0.00335 	 MISS
   71 	    11 	 0.13339 	 0.00340 	 MISS
   24 	    12 	 0.07301 	 0.00341 	 m..s
   70 	    13 	 0.13199 	 0.00347 	 MISS
   12 	    14 	 0.05169 	 0.00353 	 m..s
   61 	    15 	 0.12509 	 0.00354 	 MISS
   94 	    16 	 0.14627 	 0.00354 	 MISS
   10 	    17 	 0.04817 	 0.00362 	 m..s
    1 	    18 	 0.03665 	 0.00363 	 m..s
   39 	    19 	 0.09174 	 0.00363 	 m..s
   83 	    20 	 0.13873 	 0.00370 	 MISS
   41 	    21 	 0.09248 	 0.00378 	 m..s
    4 	    22 	 0.03714 	 0.00390 	 m..s
   62 	    23 	 0.12530 	 0.00391 	 MISS
    3 	    24 	 0.03697 	 0.00393 	 m..s
    2 	    25 	 0.03670 	 0.00399 	 m..s
    6 	    26 	 0.03730 	 0.00399 	 m..s
   76 	    27 	 0.13547 	 0.00399 	 MISS
   49 	    28 	 0.09811 	 0.00403 	 m..s
   59 	    29 	 0.11819 	 0.00416 	 MISS
   90 	    30 	 0.14328 	 0.00424 	 MISS
   55 	    31 	 0.10753 	 0.00425 	 MISS
   84 	    32 	 0.13934 	 0.00432 	 MISS
   42 	    33 	 0.09293 	 0.00432 	 m..s
   64 	    34 	 0.12642 	 0.00441 	 MISS
   68 	    35 	 0.13072 	 0.00442 	 MISS
   16 	    36 	 0.06826 	 0.00445 	 m..s
   91 	    37 	 0.14390 	 0.00455 	 MISS
   63 	    38 	 0.12542 	 0.00456 	 MISS
   80 	    39 	 0.13750 	 0.00457 	 MISS
   27 	    40 	 0.07407 	 0.00464 	 m..s
   48 	    41 	 0.09660 	 0.00466 	 m..s
   25 	    42 	 0.07327 	 0.00467 	 m..s
   92 	    43 	 0.14406 	 0.00471 	 MISS
   73 	    44 	 0.13422 	 0.00483 	 MISS
   51 	    45 	 0.10205 	 0.00485 	 m..s
   36 	    46 	 0.09115 	 0.00530 	 m..s
   11 	    47 	 0.04869 	 0.00638 	 m..s
   19 	    48 	 0.07002 	 0.02510 	 m..s
   13 	    49 	 0.05454 	 0.02914 	 ~...
   58 	    50 	 0.11612 	 0.03250 	 m..s
   14 	    51 	 0.06484 	 0.03651 	 ~...
   17 	    52 	 0.06855 	 0.04078 	 ~...
   18 	    53 	 0.06979 	 0.04576 	 ~...
   28 	    54 	 0.07416 	 0.04607 	 ~...
   21 	    55 	 0.07097 	 0.04683 	 ~...
   15 	    56 	 0.06587 	 0.05590 	 ~...
   31 	    57 	 0.08290 	 0.05672 	 ~...
   22 	    58 	 0.07100 	 0.05981 	 ~...
   30 	    59 	 0.08251 	 0.06592 	 ~...
   32 	    60 	 0.08807 	 0.09895 	 ~...
   38 	    61 	 0.09171 	 0.11465 	 ~...
   33 	    62 	 0.08903 	 0.11972 	 m..s
   52 	    63 	 0.10617 	 0.12614 	 ~...
   34 	    64 	 0.08930 	 0.13062 	 m..s
   37 	    65 	 0.09156 	 0.13488 	 m..s
   89 	    66 	 0.14224 	 0.14898 	 ~...
   35 	    67 	 0.09083 	 0.14977 	 m..s
   53 	    68 	 0.10668 	 0.15001 	 m..s
   54 	    69 	 0.10690 	 0.15118 	 m..s
   46 	    70 	 0.09620 	 0.15166 	 m..s
   47 	    71 	 0.09629 	 0.15508 	 m..s
   50 	    72 	 0.10182 	 0.15735 	 m..s
   43 	    73 	 0.09376 	 0.15738 	 m..s
   45 	    74 	 0.09432 	 0.15958 	 m..s
   44 	    75 	 0.09418 	 0.16391 	 m..s
   95 	    76 	 0.14851 	 0.16862 	 ~...
   82 	    77 	 0.13827 	 0.16862 	 m..s
   40 	    78 	 0.09188 	 0.16895 	 m..s
   60 	    79 	 0.12302 	 0.17849 	 m..s
   98 	    80 	 0.19825 	 0.18733 	 ~...
   74 	    81 	 0.13428 	 0.19245 	 m..s
  100 	    82 	 0.20152 	 0.19762 	 ~...
   87 	    83 	 0.14181 	 0.19856 	 m..s
   81 	    84 	 0.13752 	 0.19952 	 m..s
   67 	    85 	 0.12990 	 0.20127 	 m..s
   75 	    86 	 0.13524 	 0.20140 	 m..s
   69 	    87 	 0.13122 	 0.20189 	 m..s
   72 	    88 	 0.13384 	 0.20257 	 m..s
   85 	    89 	 0.14055 	 0.20437 	 m..s
  109 	    90 	 0.22910 	 0.20587 	 ~...
   66 	    91 	 0.12944 	 0.20811 	 m..s
  116 	    92 	 0.23851 	 0.20881 	 ~...
   97 	    93 	 0.19440 	 0.20936 	 ~...
  101 	    94 	 0.20415 	 0.21103 	 ~...
  102 	    95 	 0.20600 	 0.21259 	 ~...
   77 	    96 	 0.13650 	 0.21389 	 m..s
  107 	    97 	 0.22583 	 0.21487 	 ~...
   96 	    98 	 0.18705 	 0.21492 	 ~...
   86 	    99 	 0.14133 	 0.21594 	 m..s
  110 	   100 	 0.23215 	 0.21997 	 ~...
  105 	   101 	 0.22424 	 0.22001 	 ~...
  115 	   102 	 0.23844 	 0.22122 	 ~...
   88 	   103 	 0.14192 	 0.22171 	 m..s
   78 	   104 	 0.13683 	 0.22411 	 m..s
   79 	   105 	 0.13736 	 0.22485 	 m..s
  111 	   106 	 0.23252 	 0.22512 	 ~...
  117 	   107 	 0.23918 	 0.22640 	 ~...
  108 	   108 	 0.22721 	 0.22791 	 ~...
   65 	   109 	 0.12730 	 0.22931 	 MISS
  106 	   110 	 0.22442 	 0.23636 	 ~...
   93 	   111 	 0.14540 	 0.23667 	 m..s
   99 	   112 	 0.20038 	 0.23847 	 m..s
  112 	   113 	 0.23292 	 0.24052 	 ~...
  103 	   114 	 0.21911 	 0.24716 	 ~...
  104 	   115 	 0.22388 	 0.24744 	 ~...
  113 	   116 	 0.23521 	 0.25636 	 ~...
  119 	   117 	 0.24245 	 0.26771 	 ~...
  118 	   118 	 0.23934 	 0.27607 	 m..s
  114 	   119 	 0.23815 	 0.27668 	 m..s
  120 	   120 	 0.25208 	 0.29500 	 m..s
==========================================
r_mrr = 0.739770233631134
r2_mrr = 0.4912324547767639
spearmanr_mrr@5 = 0.8906809687614441
spearmanr_mrr@10 = 0.907021701335907
spearmanr_mrr@50 = 0.862387478351593
spearmanr_mrr@100 = 0.895245373249054
spearmanr_mrr@All = 0.9143602848052979
==========================================
test time: 0.476
Done Testing dataset CoDExSmall
total time taken: 198.53703355789185
training time taken: 188.52224826812744
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7398)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4912)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.8907)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9070)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8624)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8952)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9144)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.2193144040793413}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 5557917464467117
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [536, 776, 661, 450, 1058, 515, 1159, 9, 545, 944, 152, 91, 681, 403, 281, 639, 694, 237, 885, 192, 758, 1199, 997, 980, 747, 43, 1163, 331, 894, 358, 292, 527, 990, 61, 993, 433, 383, 273, 507, 1138, 957, 1100, 1157, 1214, 122, 905, 73, 808, 249, 924, 219, 1107, 898, 766, 832, 820, 398, 550, 1172, 24, 922, 880, 189, 340, 1090, 367, 795, 893, 951, 900, 113, 979, 706, 910, 623, 1034, 402, 1054, 723, 860, 998, 368, 1030, 174, 471, 1061, 597, 326, 197, 62, 244, 33, 265, 927, 289, 742, 1140, 1213, 755, 803, 875, 656, 225, 140, 921, 1057, 72, 884, 391, 194, 112, 557, 274, 794, 253, 59, 1044, 212, 878, 58, 60]
valid_ids (0): []
train_ids (1094): [185, 401, 912, 609, 526, 941, 102, 588, 355, 1023, 1152, 704, 790, 210, 310, 566, 770, 655, 240, 36, 147, 809, 46, 1046, 899, 1015, 207, 458, 1069, 1207, 211, 916, 182, 939, 302, 975, 428, 1062, 290, 188, 642, 1070, 1174, 1202, 276, 1007, 709, 449, 77, 695, 378, 343, 381, 646, 901, 562, 1173, 834, 89, 1137, 1043, 1001, 598, 178, 807, 640, 11, 915, 349, 455, 481, 88, 464, 114, 1098, 115, 432, 670, 828, 931, 32, 144, 238, 1119, 316, 602, 518, 519, 744, 421, 865, 802, 1200, 845, 54, 215, 148, 506, 227, 153, 1197, 680, 388, 959, 844, 788, 821, 826, 982, 1005, 1047, 1193, 141, 914, 1056, 732, 396, 136, 259, 283, 389, 1110, 1080, 171, 517, 1032, 604, 40, 1167, 676, 621, 1180, 745, 569, 82, 218, 260, 606, 440, 360, 940, 962, 395, 35, 964, 778, 772, 1150, 119, 567, 268, 1022, 529, 128, 241, 970, 654, 105, 1203, 243, 356, 1106, 41, 352, 801, 1081, 612, 50, 181, 484, 1004, 1008, 262, 1040, 669, 1039, 1155, 137, 172, 15, 601, 864, 416, 693, 994, 477, 282, 871, 354, 846, 781, 607, 819, 516, 1149, 671, 659, 441, 415, 904, 638, 591, 474, 1093, 165, 126, 85, 1078, 973, 1088, 1195, 1136, 1189, 318, 1184, 969, 261, 1094, 525, 836, 103, 304, 1059, 540, 359, 193, 184, 689, 1082, 495, 869, 311, 816, 1158, 1120, 56, 721, 399, 462, 1160, 374, 1052, 867, 1182, 690, 1037, 67, 677, 216, 626, 332, 135, 955, 662, 339, 53, 929, 938, 874, 424, 1031, 280, 3, 502, 1161, 762, 1102, 1141, 538, 317, 173, 1049, 574, 838, 1143, 371, 635, 1133, 503, 786, 937, 157, 926, 622, 556, 907, 409, 7, 568, 175, 658, 101, 156, 314, 966, 97, 812, 925, 267, 217, 443, 668, 445, 30, 734, 1194, 201, 579, 902, 1072, 978, 51, 501, 679, 1105, 263, 228, 315, 410, 999, 37, 120, 111, 553, 949, 943, 1071, 633, 1186, 542, 1051, 544, 168, 223, 796, 489, 329, 511, 44, 585, 96, 775, 202, 13, 861, 420, 1073, 858, 664, 1027, 710, 840, 996, 851, 1122, 774, 342, 463, 151, 444, 1118, 269, 950, 752, 10, 341, 663, 648, 1166, 84, 879, 883, 909, 124, 737, 876, 531, 643, 68, 20, 595, 429, 627, 967, 337, 473, 974, 308, 888, 1142, 896, 815, 652, 408, 320, 992, 230, 465, 348, 75, 221, 892, 561, 232, 1135, 573, 347, 1038, 453, 14, 678, 270, 667, 239, 327, 490, 98, 437, 716, 376, 1124, 948, 499, 1154, 653, 920, 749, 1020, 229, 350, 482, 986, 1201, 512, 684, 582, 1003, 850, 264, 246, 1025, 1128, 149, 130, 1171, 1036, 176, 1178, 1168, 1147, 351, 988, 251, 647, 400, 767, 711, 377, 1177, 972, 822, 768, 254, 129, 200, 434, 1029, 613, 1095, 305, 422, 452, 746, 641, 522, 146, 323, 1017, 952, 863, 196, 719, 603, 1132, 509, 1156, 31, 100, 1145, 514, 1188, 995, 143, 1033, 636, 150, 1055, 703, 5, 468, 220, 319, 806, 780, 438, 810, 583, 480, 303, 106, 682, 700, 696, 1169, 1097, 565, 117, 1101, 38, 321, 632, 797, 532, 406, 523, 1196, 439, 71, 571, 279, 45, 191, 123, 1086, 764, 372, 908, 1125, 322, 166, 580, 508, 1117, 785, 163, 811, 451, 418, 387, 1079, 483, 1096, 170, 817, 284, 498, 42, 79, 599, 835, 674, 492, 397, 0, 1181, 563, 1065, 467, 127, 724, 25, 338, 447, 584, 634, 868, 208, 824, 513, 618, 738, 890, 843, 702, 945, 87, 534, 345, 686, 1076, 575, 707, 392, 739, 86, 205, 167, 855, 257, 1010, 731, 384, 57, 611, 177, 277, 1103, 1068, 1146, 328, 1139, 295, 954, 1091, 946, 160, 300, 18, 1075, 688, 1115, 620, 159, 161, 1035, 90, 625, 34, 287, 779, 1041, 162, 369, 1179, 630, 541, 761, 558, 4, 976, 906, 547, 554, 385, 947, 521, 829, 769, 1151, 1165, 154, 248, 935, 334, 1191, 722, 985, 226, 981, 80, 255, 754, 740, 759, 818, 577, 27, 930, 672, 109, 203, 1176, 793, 644, 827, 1011, 1063, 309, 751, 81, 714, 831, 1108, 407, 520, 206, 551, 275, 330, 637, 903, 792, 617, 825, 1074, 991, 692, 842, 76, 1016, 297, 862, 1208, 891, 699, 49, 23, 375, 121, 837, 728, 530, 475, 727, 675, 799, 581, 1013, 139, 460, 1121, 555, 1092, 570, 390, 546, 266, 1112, 1024, 17, 22, 705, 614, 748, 199, 272, 1014, 771, 763, 1009, 1048, 685, 1162, 872, 715, 1019, 19, 989, 628, 804, 800, 134, 735, 419, 236, 1116, 446, 650, 537, 870, 222, 918, 660, 651, 645, 426, 494, 491, 971, 294, 605, 805, 179, 857, 708, 1066, 487, 183, 589, 234, 125, 1083, 64, 1002, 743, 886, 26, 431, 718, 958, 65, 132, 1211, 1175, 336, 687, 936, 1, 344, 984, 209, 466, 1085, 213, 1111, 285, 1131, 756, 848, 859, 928, 1198, 497, 1134, 1153, 164, 454, 629, 960, 853, 169, 965, 1064, 942, 363, 92, 977, 765, 1164, 247, 730, 1185, 469, 791, 48, 413, 291, 624, 52, 543, 1123, 312, 63, 757, 839, 616, 733, 934, 913, 968, 1050, 78, 698, 1099, 911, 505, 1109, 430, 987, 94, 95, 535, 457, 560, 190, 887, 983, 590, 296, 1087, 823, 919, 787, 528, 673, 138, 414, 683, 373, 1067, 596, 155, 1127, 1130, 881, 877, 564, 353, 1053, 70, 252, 404, 773, 932, 852, 313, 539, 488, 195, 1190, 783, 572, 107, 66, 760, 435, 1006, 1104, 849, 666, 28, 953, 286, 245, 665, 578, 1114, 293, 559, 1206, 720, 364, 346, 83, 470, 963, 224, 600, 1028, 256, 923, 729, 961, 8, 833, 895, 99, 366, 423, 459, 69, 1187, 649, 592, 370, 301, 242, 1060, 631, 496, 1126, 712, 250, 158, 713, 576, 889, 549, 231, 610, 448, 436, 417, 21, 1113, 472, 586, 55, 411, 214, 116, 74, 271, 753, 1192, 782, 93, 278, 306, 1084, 405, 789, 1000, 933, 258, 1077, 813, 118, 362, 479, 16, 608, 1209, 1183, 594, 717, 382, 701, 361, 476, 427, 180, 814, 741, 504, 288, 1212, 548, 394, 1089, 847, 298, 204, 917, 2, 1045, 593, 524, 510, 299, 307, 412, 726, 866, 533, 854, 1026, 187, 873, 1144, 325, 365, 442, 750, 145, 697, 882, 29, 6, 47, 841, 380, 357, 335, 1148, 478, 956, 133, 198, 233, 615, 386, 830, 856, 1210, 784, 485, 552, 1018, 1204, 12, 1205, 104, 108, 1129, 777, 379, 461, 333, 657, 1042, 131, 691, 1021, 493, 110, 393, 619, 456, 425, 486, 186, 725, 1170, 500, 39, 324, 235, 897, 142, 798, 587, 736, 1012]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3165434729588150
the save name prefix for this run is:  chkpt-ID_3165434729588150_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1181
rank avg (pred): 0.601 +- 0.002
mrr vals (pred, true): 0.001, 0.146
batch losses (mrrl, rdl): 0.0, 0.003775045

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1047
rank avg (pred): 0.216 +- 0.172
mrr vals (pred, true): 0.204, 0.005
batch losses (mrrl, rdl): 0.0, 0.0013197355

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 476
rank avg (pred): 0.218 +- 0.178
mrr vals (pred, true): 0.323, 0.003
batch losses (mrrl, rdl): 0.0, 0.0012838054

Epoch over!
epoch time: 12.312

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 522
rank avg (pred): 0.218 +- 0.169
mrr vals (pred, true): 0.181, 0.037
batch losses (mrrl, rdl): 0.0, 1.34178e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 614
rank avg (pred): 0.308 +- 0.249
mrr vals (pred, true): 0.169, 0.157
batch losses (mrrl, rdl): 0.0, 0.0005131193

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 57
rank avg (pred): 0.047 +- 0.040
mrr vals (pred, true): 0.348, 0.189
batch losses (mrrl, rdl): 0.0, 4.3852e-06

Epoch over!
epoch time: 11.867

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.194 +- 0.169
mrr vals (pred, true): 0.308, 0.040
batch losses (mrrl, rdl): 0.0, 1.73594e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1176
rank avg (pred): 0.312 +- 0.271
mrr vals (pred, true): 0.264, 0.156
batch losses (mrrl, rdl): 0.0, 0.000503845

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 702
rank avg (pred): 0.289 +- 0.268
mrr vals (pred, true): 0.296, 0.004
batch losses (mrrl, rdl): 0.0, 0.0004842304

Epoch over!
epoch time: 12.441

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 428
rank avg (pred): 0.279 +- 0.256
mrr vals (pred, true): 0.273, 0.005
batch losses (mrrl, rdl): 0.0, 0.0004968321

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.102 +- 0.099
mrr vals (pred, true): 0.331, 0.174
batch losses (mrrl, rdl): 0.0, 8.17699e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 96
rank avg (pred): 0.259 +- 0.238
mrr vals (pred, true): 0.284, 0.225
batch losses (mrrl, rdl): 0.0, 0.0010948334

Epoch over!
epoch time: 12.394

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 874
rank avg (pred): 0.323 +- 0.272
mrr vals (pred, true): 0.235, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002982356

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 970
rank avg (pred): 0.558 +- 0.390
mrr vals (pred, true): 0.197, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002425847

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 106
rank avg (pred): 0.209 +- 0.206
mrr vals (pred, true): 0.310, 0.213
batch losses (mrrl, rdl): 0.0, 0.0006724214

Epoch over!
epoch time: 11.941

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1103
rank avg (pred): 0.259 +- 0.250
mrr vals (pred, true): 0.276, 0.198
batch losses (mrrl, rdl): 0.0613050237, 0.0010785289

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1143
rank avg (pred): 0.426 +- 0.190
mrr vals (pred, true): 0.089, 0.057
batch losses (mrrl, rdl): 0.0149563625, 0.0011949284

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 306
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.233, 0.253
batch losses (mrrl, rdl): 0.0041100238, 2.66635e-05

Epoch over!
epoch time: 12.477

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.324 +- 0.190
mrr vals (pred, true): 0.123, 0.212
batch losses (mrrl, rdl): 0.0796793401, 0.0017003751

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 11
rank avg (pred): 0.021 +- 0.013
mrr vals (pred, true): 0.203, 0.241
batch losses (mrrl, rdl): 0.0142142689, 4.2441e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 195
rank avg (pred): 0.264 +- 0.162
mrr vals (pred, true): 0.149, 0.003
batch losses (mrrl, rdl): 0.0983133465, 0.0008546775

Epoch over!
epoch time: 12.44

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 617
rank avg (pred): 0.410 +- 0.176
mrr vals (pred, true): 0.076, 0.165
batch losses (mrrl, rdl): 0.0784140751, 0.0013369266

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 724
rank avg (pred): 0.408 +- 0.170
mrr vals (pred, true): 0.075, 0.004
batch losses (mrrl, rdl): 0.0064551323, 0.0001566678

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 528
rank avg (pred): 0.376 +- 0.189
mrr vals (pred, true): 0.105, 0.035
batch losses (mrrl, rdl): 0.0301512778, 0.0005612076

Epoch over!
epoch time: 12.435

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 51
rank avg (pred): 0.006 +- 0.004
mrr vals (pred, true): 0.267, 0.228
batch losses (mrrl, rdl): 0.0150519963, 5.27399e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 179
rank avg (pred): 0.268 +- 0.159
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0451263078, 0.0008608242

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1183
rank avg (pred): 0.394 +- 0.169
mrr vals (pred, true): 0.075, 0.157
batch losses (mrrl, rdl): 0.0674501732, 0.0011058267

Epoch over!
epoch time: 12.157

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1035
rank avg (pred): 0.323 +- 0.187
mrr vals (pred, true): 0.123, 0.004
batch losses (mrrl, rdl): 0.0538500808, 0.000412944

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 52
rank avg (pred): 0.019 +- 0.011
mrr vals (pred, true): 0.214, 0.225
batch losses (mrrl, rdl): 0.0011747262, 2.16667e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 246
rank avg (pred): 0.017 +- 0.010
mrr vals (pred, true): 0.219, 0.225
batch losses (mrrl, rdl): 0.0003323289, 2.10072e-05

Epoch over!
epoch time: 12.591

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 857
rank avg (pred): 0.381 +- 0.172
mrr vals (pred, true): 0.082, 0.075
batch losses (mrrl, rdl): 0.0104057863, 0.000705039

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 948
rank avg (pred): 0.590 +- 0.198
mrr vals (pred, true): 0.054, 0.004
batch losses (mrrl, rdl): 0.0001773426, 0.0002928098

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 46
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.255, 0.224
batch losses (mrrl, rdl): 0.0095235193, 4.19568e-05

Epoch over!
epoch time: 12.813

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 495
rank avg (pred): 0.265 +- 0.137
mrr vals (pred, true): 0.093, 0.147
batch losses (mrrl, rdl): 0.0292386655, 0.0003342035

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.069 +- 0.039
mrr vals (pred, true): 0.141, 0.032
batch losses (mrrl, rdl): 0.0833830684, 0.0014142748

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1117
rank avg (pred): 0.277 +- 0.162
mrr vals (pred, true): 0.139, 0.004
batch losses (mrrl, rdl): 0.0792745799, 0.0007035683

Epoch over!
epoch time: 12.971

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.463 +- 0.206
mrr vals (pred, true): 0.075, 0.005
batch losses (mrrl, rdl): 0.0062575247, 3.06048e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 319
rank avg (pred): 0.004 +- 0.002
mrr vals (pred, true): 0.278, 0.285
batch losses (mrrl, rdl): 0.0003984113, 3.50341e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 118
rank avg (pred): 0.210 +- 0.119
mrr vals (pred, true): 0.125, 0.212
batch losses (mrrl, rdl): 0.0766780525, 0.000560732

Epoch over!
epoch time: 13.061

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 585
rank avg (pred): 0.373 +- 0.169
mrr vals (pred, true): 0.073, 0.150
batch losses (mrrl, rdl): 0.0599685237, 0.0008792061

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 722
rank avg (pred): 0.366 +- 0.168
mrr vals (pred, true): 0.080, 0.004
batch losses (mrrl, rdl): 0.0087771183, 0.0002215065

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 343
rank avg (pred): 0.292 +- 0.167
mrr vals (pred, true): 0.120, 0.201
batch losses (mrrl, rdl): 0.0669862628, 0.0013431673

Epoch over!
epoch time: 13.092

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 991
rank avg (pred): 0.024 +- 0.014
mrr vals (pred, true): 0.197, 0.208
batch losses (mrrl, rdl): 0.0011862569, 1.37328e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.238 +- 0.143
mrr vals (pred, true): 0.159, 0.242
batch losses (mrrl, rdl): 0.0691685379, 0.0007816925

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 295
rank avg (pred): 0.004 +- 0.002
mrr vals (pred, true): 0.291, 0.299
batch losses (mrrl, rdl): 0.000506895, 3.03697e-05

Epoch over!
epoch time: 13.137

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.331 +- 0.185
mrr vals (pred, true): 0.108, 0.042

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.05960 	 0.00073 	 m..s
    3 	     1 	 0.05969 	 0.00074 	 m..s
    5 	     2 	 0.05995 	 0.00075 	 m..s
    0 	     3 	 0.05944 	 0.00076 	 m..s
    6 	     4 	 0.06005 	 0.00076 	 m..s
   11 	     5 	 0.06523 	 0.00116 	 m..s
    9 	     6 	 0.06144 	 0.00117 	 m..s
   12 	     7 	 0.06618 	 0.00138 	 m..s
   10 	     8 	 0.06480 	 0.00148 	 m..s
    7 	     9 	 0.06103 	 0.00196 	 m..s
    8 	    10 	 0.06114 	 0.00251 	 m..s
   37 	    11 	 0.08011 	 0.00277 	 m..s
   29 	    12 	 0.07690 	 0.00288 	 m..s
   76 	    13 	 0.12864 	 0.00313 	 MISS
   51 	    14 	 0.11132 	 0.00326 	 MISS
   19 	    15 	 0.07161 	 0.00331 	 m..s
   53 	    16 	 0.11298 	 0.00348 	 MISS
   20 	    17 	 0.07244 	 0.00363 	 m..s
   65 	    18 	 0.12220 	 0.00364 	 MISS
   73 	    19 	 0.12730 	 0.00366 	 MISS
   85 	    20 	 0.13989 	 0.00371 	 MISS
   68 	    21 	 0.12497 	 0.00371 	 MISS
   21 	    22 	 0.07253 	 0.00376 	 m..s
   80 	    23 	 0.13344 	 0.00380 	 MISS
   42 	    24 	 0.08630 	 0.00387 	 m..s
    1 	    25 	 0.05947 	 0.00393 	 m..s
   77 	    26 	 0.13030 	 0.00399 	 MISS
   23 	    27 	 0.07295 	 0.00401 	 m..s
   78 	    28 	 0.13303 	 0.00403 	 MISS
    4 	    29 	 0.05991 	 0.00405 	 m..s
   83 	    30 	 0.13519 	 0.00422 	 MISS
   31 	    31 	 0.07760 	 0.00423 	 m..s
   27 	    32 	 0.07583 	 0.00426 	 m..s
   58 	    33 	 0.11718 	 0.00427 	 MISS
   88 	    34 	 0.15241 	 0.00431 	 MISS
   16 	    35 	 0.07094 	 0.00438 	 m..s
   32 	    36 	 0.07776 	 0.00441 	 m..s
   17 	    37 	 0.07102 	 0.00445 	 m..s
   30 	    38 	 0.07693 	 0.00448 	 m..s
   28 	    39 	 0.07661 	 0.00451 	 m..s
   86 	    40 	 0.14299 	 0.00462 	 MISS
   34 	    41 	 0.07810 	 0.00467 	 m..s
   14 	    42 	 0.07061 	 0.00467 	 m..s
   79 	    43 	 0.13306 	 0.00471 	 MISS
   26 	    44 	 0.07570 	 0.00481 	 m..s
   81 	    45 	 0.13372 	 0.00486 	 MISS
   13 	    46 	 0.06873 	 0.00528 	 m..s
   33 	    47 	 0.07777 	 0.00607 	 m..s
   36 	    48 	 0.07981 	 0.01668 	 m..s
   41 	    49 	 0.08514 	 0.03829 	 m..s
   43 	    50 	 0.10189 	 0.03850 	 m..s
   48 	    51 	 0.10846 	 0.04210 	 m..s
   35 	    52 	 0.07818 	 0.04509 	 m..s
   38 	    53 	 0.08046 	 0.04889 	 m..s
   45 	    54 	 0.10311 	 0.04962 	 m..s
   44 	    55 	 0.10207 	 0.05555 	 m..s
   46 	    56 	 0.10323 	 0.05856 	 m..s
   40 	    57 	 0.08310 	 0.06592 	 ~...
   15 	    58 	 0.07062 	 0.07497 	 ~...
   18 	    59 	 0.07110 	 0.07508 	 ~...
   24 	    60 	 0.07522 	 0.12639 	 m..s
   25 	    61 	 0.07551 	 0.13488 	 m..s
   87 	    62 	 0.14871 	 0.14076 	 ~...
   22 	    63 	 0.07255 	 0.14256 	 m..s
   74 	    64 	 0.12744 	 0.14733 	 ~...
   62 	    65 	 0.11858 	 0.14898 	 m..s
   39 	    66 	 0.08080 	 0.15625 	 m..s
   69 	    67 	 0.12558 	 0.15737 	 m..s
   84 	    68 	 0.13618 	 0.16268 	 ~...
   82 	    69 	 0.13444 	 0.16583 	 m..s
   50 	    70 	 0.11076 	 0.17763 	 m..s
   94 	    71 	 0.21694 	 0.18169 	 m..s
   98 	    72 	 0.22339 	 0.18529 	 m..s
   71 	    73 	 0.12648 	 0.18564 	 m..s
   97 	    74 	 0.22032 	 0.18577 	 m..s
   55 	    75 	 0.11394 	 0.18776 	 m..s
   57 	    76 	 0.11697 	 0.18961 	 m..s
   91 	    77 	 0.21046 	 0.19178 	 ~...
   54 	    78 	 0.11349 	 0.19245 	 m..s
   93 	    79 	 0.21654 	 0.19248 	 ~...
   52 	    80 	 0.11294 	 0.19534 	 m..s
   96 	    81 	 0.21953 	 0.19762 	 ~...
   60 	    82 	 0.11801 	 0.19782 	 m..s
   72 	    83 	 0.12718 	 0.19884 	 m..s
   56 	    84 	 0.11419 	 0.19892 	 m..s
   59 	    85 	 0.11777 	 0.20033 	 m..s
   63 	    86 	 0.12031 	 0.20143 	 m..s
   61 	    87 	 0.11847 	 0.20189 	 m..s
   47 	    88 	 0.10761 	 0.20256 	 m..s
   67 	    89 	 0.12339 	 0.20313 	 m..s
   66 	    90 	 0.12324 	 0.20437 	 m..s
  102 	    91 	 0.23950 	 0.20645 	 m..s
   64 	    92 	 0.12105 	 0.21600 	 m..s
  108 	    93 	 0.25365 	 0.21608 	 m..s
   95 	    94 	 0.21945 	 0.21615 	 ~...
   92 	    95 	 0.21351 	 0.21764 	 ~...
   49 	    96 	 0.10856 	 0.21770 	 MISS
   70 	    97 	 0.12561 	 0.21920 	 m..s
  106 	    98 	 0.24910 	 0.21991 	 ~...
   99 	    99 	 0.22742 	 0.22110 	 ~...
  100 	   100 	 0.23161 	 0.22495 	 ~...
  110 	   101 	 0.25809 	 0.22640 	 m..s
  112 	   102 	 0.26301 	 0.22688 	 m..s
  107 	   103 	 0.25015 	 0.22780 	 ~...
   89 	   104 	 0.20330 	 0.23590 	 m..s
   75 	   105 	 0.12756 	 0.23821 	 MISS
   90 	   106 	 0.20766 	 0.24182 	 m..s
  103 	   107 	 0.24185 	 0.24623 	 ~...
  101 	   108 	 0.23737 	 0.24796 	 ~...
  114 	   109 	 0.27296 	 0.25344 	 ~...
  104 	   110 	 0.24418 	 0.25434 	 ~...
  109 	   111 	 0.25619 	 0.27141 	 ~...
  116 	   112 	 0.29017 	 0.28985 	 ~...
  111 	   113 	 0.26119 	 0.29434 	 m..s
  105 	   114 	 0.24895 	 0.29461 	 m..s
  118 	   115 	 0.29412 	 0.29784 	 ~...
  113 	   116 	 0.26511 	 0.29964 	 m..s
  115 	   117 	 0.27923 	 0.30295 	 ~...
  117 	   118 	 0.29258 	 0.31364 	 ~...
  119 	   119 	 0.29925 	 0.31510 	 ~...
  120 	   120 	 0.32344 	 0.31754 	 ~...
==========================================
r_mrr = 0.7960482239723206
r2_mrr = 0.5727207660675049
spearmanr_mrr@5 = 0.7078499794006348
spearmanr_mrr@10 = 0.8716776371002197
spearmanr_mrr@50 = 0.9011667966842651
spearmanr_mrr@100 = 0.8888619542121887
spearmanr_mrr@All = 0.9131490588188171
==========================================
test time: 0.391
Done Testing dataset CoDExSmall
total time taken: 198.599524974823
training time taken: 188.59774112701416
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7960)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.5727)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.7078)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.8717)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9012)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8889)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9131)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.2085127742648183}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 3827817496515613
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [226, 23, 1069, 184, 279, 627, 408, 409, 767, 595, 272, 106, 421, 938, 362, 207, 629, 598, 888, 415, 55, 12, 1080, 430, 585, 677, 248, 703, 200, 165, 856, 394, 1133, 539, 616, 914, 471, 127, 423, 407, 607, 203, 138, 543, 624, 974, 35, 779, 315, 561, 164, 747, 643, 1102, 71, 834, 395, 630, 224, 366, 281, 962, 67, 403, 546, 956, 817, 797, 680, 1023, 209, 28, 457, 427, 850, 617, 660, 141, 249, 709, 835, 126, 13, 620, 1091, 965, 515, 945, 379, 845, 812, 61, 582, 1022, 656, 462, 855, 72, 241, 433, 724, 406, 913, 580, 1114, 22, 45, 1166, 800, 1040, 339, 1213, 311, 1188, 280, 1161, 712, 973, 182, 1032, 548]
valid_ids (0): []
train_ids (1094): [128, 720, 640, 42, 1155, 260, 1158, 46, 443, 648, 437, 678, 794, 981, 1143, 826, 1063, 1202, 1013, 1151, 189, 438, 1025, 301, 450, 979, 823, 1167, 635, 190, 150, 988, 1203, 738, 731, 51, 94, 244, 198, 560, 771, 840, 626, 987, 1092, 773, 518, 488, 901, 912, 802, 1019, 1137, 98, 199, 684, 966, 692, 204, 59, 179, 833, 116, 392, 376, 1131, 1126, 1189, 468, 781, 950, 711, 188, 69, 15, 1109, 612, 1101, 628, 1116, 65, 266, 1209, 536, 553, 594, 155, 556, 1008, 633, 479, 131, 1174, 934, 611, 578, 830, 389, 192, 780, 355, 228, 1185, 107, 665, 646, 565, 424, 551, 1169, 976, 986, 191, 1181, 60, 236, 879, 1160, 1094, 1165, 210, 841, 849, 1117, 1083, 717, 208, 1049, 157, 809, 1175, 89, 744, 168, 805, 1195, 385, 922, 118, 1200, 1146, 202, 66, 520, 801, 730, 186, 796, 1029, 253, 920, 1003, 225, 445, 25, 785, 926, 872, 26, 1010, 770, 1118, 664, 758, 234, 881, 977, 32, 975, 1128, 402, 152, 31, 848, 832, 348, 869, 610, 492, 143, 896, 533, 196, 337, 153, 1211, 768, 718, 64, 156, 482, 854, 217, 992, 563, 1072, 372, 916, 282, 729, 119, 255, 755, 857, 306, 144, 527, 1140, 838, 490, 81, 398, 417, 1021, 792, 439, 762, 302, 336, 714, 657, 695, 806, 935, 275, 122, 899, 478, 382, 844, 432, 137, 257, 748, 774, 605, 606, 903, 171, 230, 690, 1179, 1110, 444, 795, 276, 428, 353, 685, 504, 526, 906, 475, 175, 27, 231, 290, 1144, 333, 882, 622, 1024, 516, 220, 951, 619, 1205, 426, 608, 1051, 791, 908, 895, 694, 4, 52, 365, 623, 486, 215, 24, 592, 753, 1170, 999, 318, 1104, 851, 383, 1044, 63, 1000, 335, 1120, 875, 1136, 1097, 452, 419, 638, 1016, 9, 980, 810, 710, 998, 615, 1050, 238, 573, 500, 82, 943, 289, 173, 741, 263, 864, 265, 575, 1046, 530, 581, 227, 245, 989, 347, 380, 877, 522, 1129, 1027, 216, 708, 576, 632, 567, 890, 268, 158, 867, 1065, 1015, 698, 1036, 101, 1115, 892, 787, 219, 944, 736, 964, 324, 146, 113, 212, 545, 273, 596, 982, 30, 889, 507, 843, 497, 505, 572, 878, 859, 474, 679, 1178, 43, 672, 370, 1020, 374, 411, 993, 343, 528, 655, 557, 295, 47, 930, 1154, 498, 40, 270, 682, 509, 1043, 304, 1100, 501, 559, 529, 1005, 50, 776, 961, 955, 1098, 931, 317, 1111, 136, 900, 727, 940, 274, 1041, 466, 880, 681, 29, 142, 719, 166, 658, 554, 434, 360, 740, 1075, 871, 1055, 994, 341, 897, 1150, 702, 465, 405, 669, 7, 705, 183, 893, 163, 332, 907, 316, 176, 853, 455, 958, 707, 991, 499, 1196, 733, 358, 1107, 160, 614, 1142, 48, 821, 641, 804, 418, 1141, 870, 330, 1124, 997, 667, 1152, 858, 798, 932, 1033, 891, 1085, 1047, 177, 484, 344, 722, 86, 11, 1009, 267, 53, 1077, 873, 162, 1156, 754, 827, 359, 493, 93, 378, 923, 625, 1157, 194, 464, 299, 750, 847, 777, 1093, 898, 503, 735, 259, 1186, 250, 495, 233, 1004, 591, 33, 613, 132, 1037, 454, 320, 531, 334, 278, 577, 808, 721, 1076, 73, 970, 953, 621, 1028, 745, 579, 1187, 960, 58, 18, 1180, 167, 201, 1149, 675, 397, 583, 76, 697, 769, 1011, 996, 502, 728, 1132, 972, 886, 1201, 524, 558, 569, 704, 737, 534, 0, 477, 414, 393, 37, 910, 793, 125, 822, 1038, 1057, 815, 1064, 453, 1067, 839, 661, 751, 1066, 663, 757, 968, 590, 650, 924, 340, 480, 723, 642, 1190, 1173, 725, 352, 1212, 799, 766, 170, 458, 990, 361, 151, 784, 172, 456, 1054, 699, 921, 825, 284, 1007, 446, 121, 863, 1134, 514, 19, 749, 952, 555, 243, 1127, 1030, 743, 1035, 283, 928, 390, 1122, 83, 496, 1073, 399, 510, 541, 1139, 49, 1006, 1042, 396, 84, 90, 254, 102, 1171, 92, 1135, 44, 971, 235, 742, 463, 1058, 140, 451, 20, 1084, 447, 364, 287, 788, 782, 659, 297, 885, 1199, 852, 133, 1145, 865, 485, 519, 1061, 1113, 95, 936, 846, 862, 367, 139, 746, 91, 647, 87, 726, 688, 21, 75, 917, 1026, 652, 1088, 1108, 760, 149, 947, 193, 772, 237, 691, 288, 1159, 902, 487, 820, 1089, 384, 1214, 449, 1204, 512, 161, 756, 828, 1130, 294, 459, 74, 425, 300, 586, 247, 790, 108, 568, 310, 670, 978, 36, 1034, 1182, 134, 588, 811, 1078, 124, 338, 494, 242, 508, 148, 765, 874, 346, 412, 1060, 214, 431, 544, 313, 861, 481, 1002, 589, 1177, 593, 473, 807, 195, 542, 789, 483, 400, 537, 5, 696, 1172, 54, 1017, 205, 375, 70, 314, 959, 1138, 1192, 229, 1095, 1070, 130, 783, 734, 239, 371, 683, 666, 538, 387, 946, 829, 887, 328, 420, 68, 422, 511, 1031, 868, 949, 566, 716, 99, 256, 57, 1062, 178, 842, 1087, 662, 114, 356, 814, 476, 246, 540, 824, 645, 429, 600, 1184, 221, 251, 836, 327, 331, 644, 404, 700, 1059, 564, 547, 584, 187, 112, 1208, 469, 277, 1148, 948, 876, 676, 1106, 1045, 915, 321, 639, 884, 1099, 883, 1123, 919, 816, 574, 96, 837, 587, 213, 441, 345, 1053, 103, 38, 1163, 1194, 115, 232, 1183, 467, 303, 312, 671, 100, 1048, 894, 14, 174, 941, 41, 686, 440, 381, 752, 206, 8, 298, 147, 985, 286, 363, 1210, 562, 532, 604, 957, 135, 775, 223, 88, 34, 939, 759, 552, 602, 351, 933, 995, 293, 1071, 307, 62, 984, 85, 687, 523, 104, 571, 342, 309, 368, 601, 109, 927, 218, 1018, 1103, 6, 386, 435, 1090, 1207, 350, 1112, 525, 271, 1039, 597, 145, 818, 325, 1164, 786, 1074, 354, 929, 261, 159, 111, 963, 105, 517, 636, 258, 262, 550, 79, 911, 909, 472, 78, 706, 1056, 388, 654, 56, 715, 269, 17, 461, 983, 442, 819, 673, 10, 954, 240, 1125, 308, 319, 506, 180, 570, 739, 129, 860, 296, 436, 1081, 413, 918, 535, 323, 1193, 1147, 448, 460, 491, 1206, 373, 377, 470, 693, 329, 1121, 1012, 80, 181, 1176, 764, 1153, 120, 322, 1068, 401, 264, 649, 1191, 357, 222, 937, 369, 252, 904, 77, 185, 110, 1197, 813, 599, 778, 1014, 1001, 609, 631, 154, 618, 549, 1096, 291, 1052, 803, 211, 1, 1198, 701, 653, 761, 117, 489, 1119, 831, 39, 416, 967, 169, 637, 2, 521, 942, 763, 603, 925, 285, 349, 905, 97, 326, 634, 732, 16, 1105, 410, 391, 668, 713, 1086, 651, 1168, 123, 689, 674, 969, 1162, 866, 1082, 197, 1079, 305, 292, 3, 513]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1767226810537736
the save name prefix for this run is:  chkpt-ID_1767226810537736_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 998
rank avg (pred): 0.498 +- 0.005
mrr vals (pred, true): 0.001, 0.295
batch losses (mrrl, rdl): 0.0, 0.0045443596

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 266
rank avg (pred): 0.054 +- 0.042
mrr vals (pred, true): 0.248, 0.222
batch losses (mrrl, rdl): 0.0, 2.1238e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 505
rank avg (pred): 0.183 +- 0.148
mrr vals (pred, true): 0.234, 0.158
batch losses (mrrl, rdl): 0.0, 7.54165e-05

Epoch over!
epoch time: 12.141

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 525
rank avg (pred): 0.159 +- 0.142
mrr vals (pred, true): 0.303, 0.040
batch losses (mrrl, rdl): 0.0, 7.98402e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 185
rank avg (pred): 0.243 +- 0.206
mrr vals (pred, true): 0.267, 0.005
batch losses (mrrl, rdl): 0.0, 0.0008384096

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1191
rank avg (pred): 0.384 +- 0.300
mrr vals (pred, true): 0.165, 0.005
batch losses (mrrl, rdl): 0.0, 6.83899e-05

Epoch over!
epoch time: 12.456

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 792
rank avg (pred): 0.440 +- 0.330
mrr vals (pred, true): 0.154, 0.005
batch losses (mrrl, rdl): 0.0, 2.46264e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 545
rank avg (pred): 0.173 +- 0.162
mrr vals (pred, true): 0.319, 0.056
batch losses (mrrl, rdl): 0.0, 2.77028e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 110
rank avg (pred): 0.222 +- 0.218
mrr vals (pred, true): 0.296, 0.198
batch losses (mrrl, rdl): 0.0, 0.000709296

Epoch over!
epoch time: 12.232

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 799
rank avg (pred): 0.445 +- 0.337
mrr vals (pred, true): 0.230, 0.006
batch losses (mrrl, rdl): 0.0, 2.93349e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1017
rank avg (pred): 0.238 +- 0.231
mrr vals (pred, true): 0.294, 0.209
batch losses (mrrl, rdl): 0.0, 0.0008722445

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 262
rank avg (pred): 0.076 +- 0.079
mrr vals (pred, true): 0.367, 0.230
batch losses (mrrl, rdl): 0.0, 2.54794e-05

Epoch over!
epoch time: 11.844

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 926
rank avg (pred): 0.609 +- 0.386
mrr vals (pred, true): 0.203, 0.001
batch losses (mrrl, rdl): 0.0, 0.000407593

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 859
rank avg (pred): 0.342 +- 0.294
mrr vals (pred, true): 0.259, 0.044
batch losses (mrrl, rdl): 0.0, 4.73298e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 490
rank avg (pred): 0.152 +- 0.161
mrr vals (pred, true): 0.333, 0.175
batch losses (mrrl, rdl): 0.0, 2.83495e-05

Epoch over!
epoch time: 11.99

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 120
rank avg (pred): 0.236 +- 0.244
mrr vals (pred, true): 0.325, 0.228
batch losses (mrrl, rdl): 0.0937781408, 0.0009124569

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 132
rank avg (pred): 0.330 +- 0.224
mrr vals (pred, true): 0.154, 0.229
batch losses (mrrl, rdl): 0.0560542867, 0.0018494048

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 636
rank avg (pred): 0.588 +- 0.241
mrr vals (pred, true): 0.060, 0.138
batch losses (mrrl, rdl): 0.0608214885, 0.0034437266

Epoch over!
epoch time: 12.406

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 134
rank avg (pred): 0.341 +- 0.203
mrr vals (pred, true): 0.106, 0.197
batch losses (mrrl, rdl): 0.0824652463, 0.001970113

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1060
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.285, 0.320
batch losses (mrrl, rdl): 0.0128282746, 1.16431e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1071
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.265, 0.215
batch losses (mrrl, rdl): 0.0249226019, 3.89741e-05

Epoch over!
epoch time: 12.336

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 869
rank avg (pred): 0.511 +- 0.192
mrr vals (pred, true): 0.063, 0.005
batch losses (mrrl, rdl): 0.0016813793, 7.6512e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 289
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.310, 0.298
batch losses (mrrl, rdl): 0.001441358, 2.7011e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 130
rank avg (pred): 0.295 +- 0.179
mrr vals (pred, true): 0.128, 0.214
batch losses (mrrl, rdl): 0.0742015988, 0.001380979

Epoch over!
epoch time: 12.528

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 155
rank avg (pred): 0.298 +- 0.173
mrr vals (pred, true): 0.112, 0.197
batch losses (mrrl, rdl): 0.0733731687, 0.0013989317

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 25
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.232, 0.239
batch losses (mrrl, rdl): 0.0004764143, 1.36005e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 511
rank avg (pred): 0.208 +- 0.128
mrr vals (pred, true): 0.137, 0.153
batch losses (mrrl, rdl): 0.0026084937, 0.0001220487

Epoch over!
epoch time: 12.503

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 157
rank avg (pred): 0.294 +- 0.163
mrr vals (pred, true): 0.112, 0.222
batch losses (mrrl, rdl): 0.1203820705, 0.0013625004

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 398
rank avg (pred): 0.275 +- 0.165
mrr vals (pred, true): 0.139, 0.186
batch losses (mrrl, rdl): 0.022185538, 0.0011052045

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 400
rank avg (pred): 0.281 +- 0.160
mrr vals (pred, true): 0.114, 0.220
batch losses (mrrl, rdl): 0.1123679727, 0.0011284343

Epoch over!
epoch time: 12.19

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.304 +- 0.146
mrr vals (pred, true): 0.096, 0.032
batch losses (mrrl, rdl): 0.0209299065, 7.22535e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 21
rank avg (pred): 0.021 +- 0.013
mrr vals (pred, true): 0.206, 0.248
batch losses (mrrl, rdl): 0.017927723, 4.6118e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 369
rank avg (pred): 0.271 +- 0.154
mrr vals (pred, true): 0.115, 0.242
batch losses (mrrl, rdl): 0.161099121, 0.0010730975

Epoch over!
epoch time: 13.051

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 983
rank avg (pred): 0.014 +- 0.009
mrr vals (pred, true): 0.236, 0.204
batch losses (mrrl, rdl): 0.0103106964, 2.23614e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 387
rank avg (pred): 0.272 +- 0.150
mrr vals (pred, true): 0.112, 0.242
batch losses (mrrl, rdl): 0.1703104675, 0.0010836492

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 402
rank avg (pred): 0.251 +- 0.153
mrr vals (pred, true): 0.138, 0.238
batch losses (mrrl, rdl): 0.0997812673, 0.0009061376

Epoch over!
epoch time: 12.864

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 507
rank avg (pred): 0.216 +- 0.134
mrr vals (pred, true): 0.157, 0.157
batch losses (mrrl, rdl): 2.9446e-06, 0.0001649186

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 832
rank avg (pred): 0.014 +- 0.009
mrr vals (pred, true): 0.237, 0.242
batch losses (mrrl, rdl): 0.0002336565, 2.69651e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 941
rank avg (pred): 0.558 +- 0.165
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004542077, 0.0010224996

Epoch over!
epoch time: 12.285

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 623
rank avg (pred): 0.335 +- 0.153
mrr vals (pred, true): 0.091, 0.156
batch losses (mrrl, rdl): 0.0421153307, 0.0006152161

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 733
rank avg (pred): 0.290 +- 0.130
mrr vals (pred, true): 0.098, 0.066
batch losses (mrrl, rdl): 0.0230959635, 0.0001606291

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1131
rank avg (pred): 0.257 +- 0.146
mrr vals (pred, true): 0.128, 0.004
batch losses (mrrl, rdl): 0.0600742586, 0.0009707086

Epoch over!
epoch time: 13.106

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 745
rank avg (pred): 0.074 +- 0.045
mrr vals (pred, true): 0.181, 0.142
batch losses (mrrl, rdl): 0.0157494023, 1.41278e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 211
rank avg (pred): 0.252 +- 0.148
mrr vals (pred, true): 0.146, 0.005
batch losses (mrrl, rdl): 0.0928840861, 0.0009128226

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 592
rank avg (pred): 0.370 +- 0.151
mrr vals (pred, true): 0.076, 0.175
batch losses (mrrl, rdl): 0.0983566195, 0.0008660421

Epoch over!
epoch time: 12.33

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.258 +- 0.143
mrr vals (pred, true): 0.129, 0.004

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    3 	     0 	 0.03435 	 0.00076 	 m..s
    6 	     1 	 0.04905 	 0.00113 	 m..s
    7 	     2 	 0.04909 	 0.00124 	 m..s
    2 	     3 	 0.03431 	 0.00312 	 m..s
   40 	     4 	 0.09207 	 0.00321 	 m..s
   28 	     5 	 0.07683 	 0.00325 	 m..s
   60 	     6 	 0.12930 	 0.00332 	 MISS
   60 	     7 	 0.12930 	 0.00336 	 MISS
   56 	     8 	 0.12716 	 0.00341 	 MISS
   60 	     9 	 0.12930 	 0.00344 	 MISS
   38 	    10 	 0.08986 	 0.00356 	 m..s
   88 	    11 	 0.13150 	 0.00356 	 MISS
   48 	    12 	 0.10533 	 0.00363 	 MISS
   60 	    13 	 0.12930 	 0.00364 	 MISS
   82 	    14 	 0.12963 	 0.00365 	 MISS
   11 	    15 	 0.06169 	 0.00367 	 m..s
   60 	    16 	 0.12930 	 0.00371 	 MISS
   59 	    17 	 0.12837 	 0.00376 	 MISS
   90 	    18 	 0.13246 	 0.00382 	 MISS
   23 	    19 	 0.07197 	 0.00387 	 m..s
   58 	    20 	 0.12825 	 0.00391 	 MISS
    8 	    21 	 0.05901 	 0.00391 	 m..s
   81 	    22 	 0.12955 	 0.00392 	 MISS
   53 	    23 	 0.12425 	 0.00394 	 MISS
   60 	    24 	 0.12930 	 0.00394 	 MISS
   52 	    25 	 0.12397 	 0.00395 	 MISS
    1 	    26 	 0.03428 	 0.00399 	 m..s
   91 	    27 	 0.13250 	 0.00404 	 MISS
    4 	    28 	 0.03437 	 0.00412 	 m..s
   87 	    29 	 0.13123 	 0.00412 	 MISS
   60 	    30 	 0.12930 	 0.00413 	 MISS
   85 	    31 	 0.13003 	 0.00417 	 MISS
   84 	    32 	 0.12985 	 0.00418 	 MISS
   60 	    33 	 0.12930 	 0.00423 	 MISS
   93 	    34 	 0.13441 	 0.00432 	 MISS
   51 	    35 	 0.12372 	 0.00438 	 MISS
   16 	    36 	 0.06646 	 0.00442 	 m..s
   57 	    37 	 0.12804 	 0.00443 	 MISS
    0 	    38 	 0.03154 	 0.00443 	 ~...
   60 	    39 	 0.12930 	 0.00446 	 MISS
   39 	    40 	 0.09170 	 0.00446 	 m..s
   86 	    41 	 0.13048 	 0.00448 	 MISS
   43 	    42 	 0.09357 	 0.00449 	 m..s
   50 	    43 	 0.12068 	 0.00451 	 MISS
   92 	    44 	 0.13378 	 0.00456 	 MISS
   17 	    45 	 0.06726 	 0.00509 	 m..s
   33 	    46 	 0.08844 	 0.00530 	 m..s
   36 	    47 	 0.08921 	 0.00544 	 m..s
   18 	    48 	 0.06769 	 0.01197 	 m..s
    5 	    49 	 0.04515 	 0.02364 	 ~...
   49 	    50 	 0.10727 	 0.03256 	 m..s
   24 	    51 	 0.07412 	 0.03267 	 m..s
   29 	    52 	 0.07956 	 0.03792 	 m..s
   20 	    53 	 0.06921 	 0.03977 	 ~...
   10 	    54 	 0.05929 	 0.04078 	 ~...
   12 	    55 	 0.06550 	 0.04607 	 ~...
   19 	    56 	 0.06866 	 0.04945 	 ~...
   14 	    57 	 0.06603 	 0.05678 	 ~...
   21 	    58 	 0.07104 	 0.05856 	 ~...
   22 	    59 	 0.07168 	 0.05981 	 ~...
   15 	    60 	 0.06623 	 0.06300 	 ~...
    9 	    61 	 0.05911 	 0.07522 	 ~...
   13 	    62 	 0.06564 	 0.09895 	 m..s
   27 	    63 	 0.07614 	 0.10742 	 m..s
   26 	    64 	 0.07577 	 0.10894 	 m..s
   30 	    65 	 0.08715 	 0.11972 	 m..s
   47 	    66 	 0.10527 	 0.12580 	 ~...
   32 	    67 	 0.08820 	 0.12642 	 m..s
   31 	    68 	 0.08757 	 0.13062 	 m..s
   34 	    69 	 0.08859 	 0.14601 	 m..s
   25 	    70 	 0.07514 	 0.15042 	 m..s
   37 	    71 	 0.08972 	 0.15393 	 m..s
   41 	    72 	 0.09252 	 0.15548 	 m..s
   42 	    73 	 0.09332 	 0.15738 	 m..s
   35 	    74 	 0.08894 	 0.15822 	 m..s
   46 	    75 	 0.09789 	 0.16363 	 m..s
   45 	    76 	 0.09674 	 0.16450 	 m..s
   95 	    77 	 0.15361 	 0.16583 	 ~...
   44 	    78 	 0.09373 	 0.16875 	 m..s
   97 	    79 	 0.20751 	 0.17869 	 ~...
   60 	    80 	 0.12930 	 0.18155 	 m..s
   96 	    81 	 0.20727 	 0.18169 	 ~...
   55 	    82 	 0.12574 	 0.18579 	 m..s
  100 	    83 	 0.21700 	 0.19153 	 ~...
   60 	    84 	 0.12930 	 0.19450 	 m..s
   98 	    85 	 0.20809 	 0.19708 	 ~...
   60 	    86 	 0.12930 	 0.20045 	 m..s
   99 	    87 	 0.21546 	 0.20499 	 ~...
   94 	    88 	 0.13845 	 0.20622 	 m..s
  101 	    89 	 0.22507 	 0.20936 	 ~...
   60 	    90 	 0.12930 	 0.20944 	 m..s
   60 	    91 	 0.12930 	 0.21005 	 m..s
   54 	    92 	 0.12532 	 0.21270 	 m..s
   60 	    93 	 0.12930 	 0.21315 	 m..s
  107 	    94 	 0.26162 	 0.21608 	 m..s
  102 	    95 	 0.22785 	 0.21764 	 ~...
   83 	    96 	 0.12971 	 0.21827 	 m..s
   60 	    97 	 0.12930 	 0.21878 	 m..s
   60 	    98 	 0.12930 	 0.21920 	 m..s
  103 	    99 	 0.23024 	 0.22001 	 ~...
   80 	   100 	 0.12934 	 0.22038 	 m..s
   60 	   101 	 0.12930 	 0.22855 	 m..s
  109 	   102 	 0.26266 	 0.23088 	 m..s
   60 	   103 	 0.12930 	 0.23172 	 MISS
   89 	   104 	 0.13159 	 0.23224 	 MISS
  116 	   105 	 0.28960 	 0.23319 	 m..s
  115 	   106 	 0.28427 	 0.23589 	 m..s
   60 	   107 	 0.12930 	 0.23677 	 MISS
  105 	   108 	 0.25002 	 0.23706 	 ~...
  114 	   109 	 0.26452 	 0.23772 	 ~...
  106 	   110 	 0.25560 	 0.23828 	 ~...
  104 	   111 	 0.24881 	 0.24069 	 ~...
  108 	   112 	 0.26174 	 0.24716 	 ~...
  117 	   113 	 0.29952 	 0.25954 	 m..s
  111 	   114 	 0.26340 	 0.25966 	 ~...
  112 	   115 	 0.26367 	 0.26518 	 ~...
  118 	   116 	 0.31055 	 0.26639 	 m..s
  113 	   117 	 0.26448 	 0.27141 	 ~...
  110 	   118 	 0.26326 	 0.27856 	 ~...
  120 	   119 	 0.32564 	 0.28556 	 m..s
  119 	   120 	 0.32200 	 0.28609 	 m..s
==========================================
r_mrr = 0.6929594278335571
r2_mrr = 0.4037303328514099
spearmanr_mrr@5 = 0.9973418116569519
spearmanr_mrr@10 = 0.9361890554428101
spearmanr_mrr@50 = 0.9006087183952332
spearmanr_mrr@100 = 0.830420970916748
spearmanr_mrr@All = 0.866722047328949
==========================================
test time: 0.468
Done Testing dataset CoDExSmall
total time taken: 196.12273836135864
training time taken: 186.8056902885437
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6930)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4037)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9973)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9362)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9006)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8304)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8667)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.9530380099167814}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 4507206163146030
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [154, 1135, 347, 603, 1085, 1070, 295, 322, 944, 70, 913, 1035, 261, 956, 905, 1061, 58, 1186, 147, 827, 834, 179, 235, 458, 724, 1138, 768, 5, 38, 41, 1031, 208, 921, 90, 614, 506, 1117, 927, 938, 126, 205, 805, 312, 568, 107, 611, 939, 1154, 895, 1043, 1174, 1143, 527, 277, 909, 888, 36, 176, 986, 908, 541, 863, 646, 222, 64, 296, 139, 902, 947, 280, 532, 875, 173, 548, 362, 401, 729, 994, 1180, 448, 82, 612, 537, 122, 831, 764, 1083, 457, 288, 1013, 371, 644, 454, 728, 639, 213, 406, 412, 186, 622, 198, 211, 273, 57, 1003, 376, 855, 730, 326, 110, 967, 1077, 364, 459, 15, 502, 742, 890, 44, 1093, 543]
valid_ids (0): []
train_ids (1094): [142, 521, 1209, 499, 311, 279, 1184, 387, 814, 571, 1152, 703, 830, 467, 1165, 1214, 677, 252, 1158, 1131, 940, 105, 1028, 716, 1212, 982, 175, 518, 904, 577, 554, 14, 931, 225, 1058, 970, 9, 508, 236, 101, 451, 823, 610, 534, 748, 444, 717, 722, 32, 625, 52, 941, 873, 293, 820, 950, 1029, 1102, 455, 690, 1032, 1192, 1056, 328, 196, 664, 308, 1100, 493, 148, 576, 617, 75, 708, 230, 1161, 1006, 1210, 172, 869, 1170, 860, 245, 1207, 170, 1183, 813, 1189, 272, 128, 601, 849, 751, 232, 977, 866, 427, 1022, 268, 410, 247, 264, 450, 976, 885, 765, 251, 200, 501, 899, 123, 1160, 609, 659, 250, 95, 758, 795, 192, 1046, 290, 197, 828, 204, 332, 1109, 1016, 178, 484, 528, 442, 590, 1082, 903, 1203, 190, 431, 341, 731, 707, 565, 1208, 689, 642, 74, 524, 536, 960, 1, 18, 556, 12, 157, 253, 242, 1057, 384, 784, 98, 763, 535, 1126, 740, 397, 954, 645, 803, 483, 519, 476, 637, 1125, 1071, 934, 386, 833, 604, 846, 928, 632, 462, 901, 1063, 530, 94, 523, 88, 415, 775, 700, 973, 145, 948, 854, 265, 1167, 259, 127, 394, 143, 929, 21, 1145, 370, 393, 714, 550, 1147, 692, 355, 24, 298, 369, 923, 1047, 980, 1037, 491, 656, 1111, 1087, 61, 563, 267, 301, 806, 788, 162, 132, 774, 494, 755, 158, 999, 136, 206, 497, 1086, 1188, 801, 270, 479, 434, 782, 1053, 626, 783, 516, 238, 353, 1033, 171, 607, 338, 1042, 413, 500, 26, 68, 691, 77, 574, 544, 1140, 972, 195, 343, 822, 379, 920, 702, 630, 678, 1079, 942, 667, 358, 414, 1023, 636, 137, 80, 35, 354, 1201, 1144, 465, 850, 1090, 76, 837, 19, 1012, 449, 683, 351, 817, 649, 1052, 786, 693, 861, 1055, 865, 615, 1009, 69, 121, 481, 660, 134, 891, 1097, 769, 108, 505, 349, 894, 666, 557, 29, 1114, 221, 741, 963, 681, 496, 997, 825, 50, 53, 686, 529, 749, 1198, 174, 435, 234, 229, 7, 447, 581, 1081, 373, 320, 1091, 1162, 463, 658, 1094, 314, 1044, 1089, 971, 968, 1115, 503, 260, 654, 486, 840, 405, 1092, 1171, 974, 853, 628, 721, 1020, 780, 572, 309, 949, 859, 487, 1132, 672, 969, 843, 1200, 372, 150, 1168, 633, 1026, 348, 794, 73, 851, 857, 93, 1015, 662, 48, 597, 228, 911, 470, 687, 793, 163, 804, 83, 791, 1064, 709, 1206, 657, 648, 11, 1112, 1182, 433, 640, 1080, 701, 958, 1129, 278, 325, 870, 591, 684, 821, 383, 37, 1088, 34, 634, 316, 89, 498, 233, 871, 1039, 785, 1098, 424, 318, 220, 471, 81, 452, 824, 66, 907, 138, 995, 1050, 771, 111, 1001, 618, 647, 305, 428, 743, 1191, 365, 43, 395, 513, 438, 984, 161, 224, 345, 336, 898, 797, 990, 20, 104, 867, 943, 97, 883, 416, 564, 1038, 274, 167, 669, 916, 461, 623, 886, 436, 62, 256, 992, 605, 1155, 218, 239, 738, 51, 808, 202, 1149, 340, 1169, 85, 443, 131, 1025, 1068, 408, 652, 212, 1177, 1196, 331, 868, 919, 720, 248, 893, 72, 514, 177, 1076, 464, 802, 578, 661, 1008, 439, 790, 249, 217, 324, 596, 141, 575, 877, 1119, 246, 352, 400, 533, 297, 469, 812, 600, 113, 112, 522, 342, 650, 796, 411, 133, 219, 747, 130, 836, 776, 896, 333, 1051, 159, 166, 1130, 165, 711, 25, 1096, 598, 619, 1122, 746, 673, 965, 359, 446, 432, 892, 712, 935, 92, 1095, 350, 900, 629, 975, 194, 725, 392, 933, 989, 588, 109, 207, 185, 106, 385, 1069, 339, 398, 153, 862, 787, 382, 237, 835, 86, 540, 55, 1193, 1156, 573, 924, 203, 887, 697, 932, 732, 983, 872, 118, 284, 509, 847, 183, 13, 715, 1141, 695, 191, 389, 189, 560, 63, 811, 1116, 705, 42, 1190, 71, 149, 344, 979, 1150, 396, 30, 582, 889, 651, 511, 897, 1127, 1118, 429, 704, 445, 276, 1073, 674, 1005, 54, 262, 1011, 579, 809, 275, 475, 215, 156, 0, 1204, 880, 1041, 832, 841, 1197, 180, 1139, 1178, 488, 1120, 569, 146, 1175, 103, 752, 282, 754, 378, 45, 546, 271, 733, 1136, 1078, 594, 285, 1065, 987, 226, 879, 363, 419, 1040, 1105, 961, 243, 937, 299, 584, 231, 266, 884, 621, 981, 829, 1142, 766, 675, 99, 140, 417, 335, 144, 482, 466, 922, 957, 1000, 781, 792, 1213, 329, 1036, 1187, 561, 799, 478, 33, 539, 214, 407, 485, 551, 1004, 441, 47, 549, 10, 966, 426, 915, 882, 1179, 100, 366, 679, 1014, 437, 28, 374, 303, 1007, 515, 504, 761, 670, 507, 120, 744, 1159, 404, 468, 583, 84, 269, 114, 91, 193, 490, 79, 294, 472, 188, 606, 1067, 635, 49, 735, 914, 777, 753, 129, 17, 845, 826, 368, 698, 78, 1019, 418, 1017, 1099, 718, 608, 918, 1134, 525, 27, 223, 480, 263, 912, 489, 283, 1173, 473, 858, 337, 425, 542, 545, 517, 1137, 570, 964, 770, 737, 388, 291, 538, 696, 1172, 377, 688, 955, 685, 710, 682, 1164, 599, 1034, 254, 135, 653, 800, 310, 750, 559, 59, 878, 1048, 124, 1066, 1059, 510, 1124, 655, 164, 119, 380, 403, 399, 643, 65, 255, 391, 216, 1195, 1010, 1133, 474, 638, 988, 1166, 313, 456, 56, 745, 567, 1146, 586, 665, 819, 381, 874, 713, 760, 258, 1027, 789, 210, 402, 60, 1148, 421, 580, 360, 613, 547, 512, 936, 181, 996, 1060, 998, 1074, 945, 520, 1202, 187, 668, 592, 917, 624, 22, 553, 962, 818, 566, 453, 1045, 993, 757, 589, 807, 767, 4, 1084, 327, 201, 1104, 1002, 46, 8, 209, 727, 739, 1103, 117, 759, 810, 838, 492, 422, 773, 779, 881, 593, 816, 1018, 306, 680, 953, 951, 723, 676, 959, 925, 125, 169, 281, 1072, 23, 842, 361, 1176, 257, 699, 160, 1121, 420, 319, 526, 330, 844, 155, 357, 115, 641, 1101, 315, 555, 726, 423, 627, 16, 985, 1185, 244, 856, 1199, 1107, 663, 1049, 1153, 798, 102, 1108, 1021, 839, 1123, 1163, 671, 227, 587, 1151, 562, 734, 991, 116, 286, 168, 39, 460, 40, 151, 495, 1211, 430, 1054, 719, 558, 815, 375, 864, 910, 926, 952, 302, 184, 87, 1062, 323, 1113, 346, 1194, 1075, 304, 287, 96, 321, 300, 1205, 356, 762, 585, 1157, 620, 240, 317, 1181, 876, 706, 199, 31, 631, 334, 552, 756, 307, 367, 440, 848, 1128, 2, 182, 3, 1106, 930, 602, 1030, 736, 1024, 152, 772, 595, 616, 390, 852, 6, 978, 531, 409, 292, 241, 946, 1110, 906, 289, 477, 67, 694, 778]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2416535086208089
the save name prefix for this run is:  chkpt-ID_2416535086208089_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 397
rank avg (pred): 0.489 +- 0.003
mrr vals (pred, true): 0.001, 0.213
batch losses (mrrl, rdl): 0.0, 0.0040112152

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 138
rank avg (pred): 0.291 +- 0.156
mrr vals (pred, true): 0.011, 0.209
batch losses (mrrl, rdl): 0.0, 0.0012199746

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 243
rank avg (pred): 0.067 +- 0.049
mrr vals (pred, true): 0.163, 0.220
batch losses (mrrl, rdl): 0.0, 9.801e-06

Epoch over!
epoch time: 12.785

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 943
rank avg (pred): 0.363 +- 0.280
mrr vals (pred, true): 0.180, 0.001
batch losses (mrrl, rdl): 0.0, 0.0025666079

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 396
rank avg (pred): 0.236 +- 0.189
mrr vals (pred, true): 0.289, 0.234
batch losses (mrrl, rdl): 0.0, 0.0008347594

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1084
rank avg (pred): 0.266 +- 0.212
mrr vals (pred, true): 0.300, 0.199
batch losses (mrrl, rdl): 0.0, 0.0011179984

Epoch over!
epoch time: 11.911

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 883
rank avg (pred): 0.401 +- 0.323
mrr vals (pred, true): 0.304, 0.005
batch losses (mrrl, rdl): 0.0, 4.26869e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 626
rank avg (pred): 0.296 +- 0.243
mrr vals (pred, true): 0.353, 0.149
batch losses (mrrl, rdl): 0.0, 0.0004335511

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 427
rank avg (pred): 0.290 +- 0.236
mrr vals (pred, true): 0.333, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005462669

Epoch over!
epoch time: 12.18

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 331
rank avg (pred): 0.245 +- 0.202
mrr vals (pred, true): 0.358, 0.203
batch losses (mrrl, rdl): 0.0, 0.000850914

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 924
rank avg (pred): 0.480 +- 0.382
mrr vals (pred, true): 0.265, 0.001
batch losses (mrrl, rdl): 0.0, 0.0011386154

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 135
rank avg (pred): 0.257 +- 0.210
mrr vals (pred, true): 0.268, 0.209
batch losses (mrrl, rdl): 0.0, 0.0009894791

Epoch over!
epoch time: 12.328

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 997
rank avg (pred): 0.054 +- 0.044
mrr vals (pred, true): 0.327, 0.300
batch losses (mrrl, rdl): 0.0, 6.4322e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 415
rank avg (pred): 0.267 +- 0.221
mrr vals (pred, true): 0.314, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006646435

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 137
rank avg (pred): 0.241 +- 0.202
mrr vals (pred, true): 0.334, 0.191
batch losses (mrrl, rdl): 0.0, 0.0008282671

Epoch over!
epoch time: 12.118

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 948
rank avg (pred): 0.539 +- 0.392
mrr vals (pred, true): 0.199, 0.004
batch losses (mrrl, rdl): 0.2213885486, 0.0002063155

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 606
rank avg (pred): 0.420 +- 0.213
mrr vals (pred, true): 0.056, 0.132
batch losses (mrrl, rdl): 0.0571418852, 0.0013215165

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 275
rank avg (pred): 0.005 +- 0.003
mrr vals (pred, true): 0.279, 0.237
batch losses (mrrl, rdl): 0.0177141, 4.62664e-05

Epoch over!
epoch time: 13.206

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.420 +- 0.190
mrr vals (pred, true): 0.041, 0.004
batch losses (mrrl, rdl): 0.000735296, 6.69472e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 45
rank avg (pred): 0.016 +- 0.009
mrr vals (pred, true): 0.194, 0.231
batch losses (mrrl, rdl): 0.0139687192, 3.0511e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 741
rank avg (pred): 0.066 +- 0.040
mrr vals (pred, true): 0.145, 0.174
batch losses (mrrl, rdl): 0.0084219305, 1.02701e-05

Epoch over!
epoch time: 12.898

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 711
rank avg (pred): 0.391 +- 0.204
mrr vals (pred, true): 0.069, 0.004
batch losses (mrrl, rdl): 0.0034712665, 0.0001186006

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 536
rank avg (pred): 0.341 +- 0.170
mrr vals (pred, true): 0.074, 0.042
batch losses (mrrl, rdl): 0.0059020966, 0.0002940029

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 736
rank avg (pred): 0.100 +- 0.053
mrr vals (pred, true): 0.074, 0.001
batch losses (mrrl, rdl): 0.0056104613, 0.0029586388

Epoch over!
epoch time: 12.776

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 50
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.225, 0.222
batch losses (mrrl, rdl): 0.000100454, 3.63463e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 762
rank avg (pred): 0.474 +- 0.214
mrr vals (pred, true): 0.069, 0.018
batch losses (mrrl, rdl): 0.0036180273, 0.0003313075

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 712
rank avg (pred): 0.366 +- 0.208
mrr vals (pred, true): 0.117, 0.005
batch losses (mrrl, rdl): 0.044443924, 0.0001654754

Epoch over!
epoch time: 12.41

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 707
rank avg (pred): 0.364 +- 0.208
mrr vals (pred, true): 0.126, 0.004
batch losses (mrrl, rdl): 0.0583859682, 0.0001937427

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 24
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.313, 0.246
batch losses (mrrl, rdl): 0.044521682, 1.97532e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 725
rank avg (pred): 0.373 +- 0.208
mrr vals (pred, true): 0.121, 0.004
batch losses (mrrl, rdl): 0.0497239381, 0.000152308

Epoch over!
epoch time: 13.697

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1058
rank avg (pred): 0.013 +- 0.008
mrr vals (pred, true): 0.252, 0.294
batch losses (mrrl, rdl): 0.0178195182, 9.0851e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1200
rank avg (pred): 0.361 +- 0.192
mrr vals (pred, true): 0.082, 0.004
batch losses (mrrl, rdl): 0.010411703, 0.0002216873

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 857
rank avg (pred): 0.299 +- 0.152
mrr vals (pred, true): 0.088, 0.075
batch losses (mrrl, rdl): 0.0145575013, 0.0002408599

Epoch over!
epoch time: 12.425

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 174
rank avg (pred): 0.370 +- 0.196
mrr vals (pred, true): 0.100, 0.005
batch losses (mrrl, rdl): 0.0248628594, 0.0002146773

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 30
rank avg (pred): 0.022 +- 0.013
mrr vals (pred, true): 0.203, 0.196
batch losses (mrrl, rdl): 0.0005295295, 1.84923e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 748
rank avg (pred): 0.052 +- 0.032
mrr vals (pred, true): 0.184, 0.169
batch losses (mrrl, rdl): 0.0023488242, 2.2423e-06

Epoch over!
epoch time: 13.035

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 816
rank avg (pred): 0.198 +- 0.102
mrr vals (pred, true): 0.069, 0.033
batch losses (mrrl, rdl): 0.0035597091, 0.0004221278

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 819
rank avg (pred): 0.038 +- 0.024
mrr vals (pred, true): 0.192, 0.221
batch losses (mrrl, rdl): 0.0083461311, 6.2904e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 656
rank avg (pred): 0.353 +- 0.179
mrr vals (pred, true): 0.070, 0.004
batch losses (mrrl, rdl): 0.00381226, 0.0002894998

Epoch over!
epoch time: 12.68

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 423
rank avg (pred): 0.352 +- 0.194
mrr vals (pred, true): 0.122, 0.004
batch losses (mrrl, rdl): 0.051750727, 0.0002441471

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 410
rank avg (pred): 0.363 +- 0.191
mrr vals (pred, true): 0.108, 0.003
batch losses (mrrl, rdl): 0.0341051929, 0.0002809954

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 389
rank avg (pred): 0.325 +- 0.190
mrr vals (pred, true): 0.136, 0.201
batch losses (mrrl, rdl): 0.0422948934, 0.0017160763

Epoch over!
epoch time: 12.874

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 319
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.255, 0.285
batch losses (mrrl, rdl): 0.0090542091, 2.59427e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 249
rank avg (pred): 0.017 +- 0.010
mrr vals (pred, true): 0.218, 0.218
batch losses (mrrl, rdl): 1.0245e-06, 1.85062e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 884
rank avg (pred): 0.480 +- 0.177
mrr vals (pred, true): 0.049, 0.004
batch losses (mrrl, rdl): 1.41769e-05, 4.44832e-05

Epoch over!
epoch time: 12.191

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.318 +- 0.179
mrr vals (pred, true): 0.120, 0.219

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04934 	 0.00073 	 m..s
    1 	     1 	 0.04934 	 0.00074 	 m..s
    1 	     2 	 0.04934 	 0.00076 	 m..s
    1 	     3 	 0.04934 	 0.00076 	 m..s
    1 	     4 	 0.04934 	 0.00076 	 m..s
    0 	     5 	 0.04718 	 0.00094 	 m..s
    1 	     6 	 0.04934 	 0.00117 	 m..s
   19 	     7 	 0.05165 	 0.00124 	 m..s
    1 	     8 	 0.04934 	 0.00130 	 m..s
    1 	     9 	 0.04934 	 0.00210 	 m..s
   21 	    10 	 0.05286 	 0.00214 	 m..s
   56 	    11 	 0.09492 	 0.00320 	 m..s
   73 	    12 	 0.11893 	 0.00323 	 MISS
   47 	    13 	 0.08711 	 0.00327 	 m..s
   82 	    14 	 0.12088 	 0.00344 	 MISS
   64 	    15 	 0.11625 	 0.00346 	 MISS
   46 	    16 	 0.08270 	 0.00357 	 m..s
   74 	    17 	 0.11909 	 0.00359 	 MISS
   55 	    18 	 0.09352 	 0.00362 	 m..s
   54 	    19 	 0.09192 	 0.00385 	 m..s
   86 	    20 	 0.12193 	 0.00387 	 MISS
    1 	    21 	 0.04934 	 0.00390 	 m..s
   67 	    22 	 0.11773 	 0.00390 	 MISS
    1 	    23 	 0.04934 	 0.00399 	 m..s
   76 	    24 	 0.11924 	 0.00402 	 MISS
   49 	    25 	 0.08930 	 0.00404 	 m..s
   57 	    26 	 0.09855 	 0.00404 	 m..s
   84 	    27 	 0.12151 	 0.00412 	 MISS
   18 	    28 	 0.05106 	 0.00413 	 m..s
   77 	    29 	 0.11963 	 0.00420 	 MISS
   43 	    30 	 0.07654 	 0.00422 	 m..s
   16 	    31 	 0.05016 	 0.00423 	 m..s
   66 	    32 	 0.11736 	 0.00424 	 MISS
   79 	    33 	 0.11999 	 0.00426 	 MISS
   89 	    34 	 0.13109 	 0.00433 	 MISS
   29 	    35 	 0.05751 	 0.00442 	 m..s
    1 	    36 	 0.04934 	 0.00443 	 m..s
   42 	    37 	 0.07499 	 0.00449 	 m..s
    1 	    38 	 0.04934 	 0.00483 	 m..s
   69 	    39 	 0.11797 	 0.00489 	 MISS
   72 	    40 	 0.11889 	 0.00497 	 MISS
   75 	    41 	 0.11920 	 0.00502 	 MISS
    1 	    42 	 0.04934 	 0.02133 	 ~...
   17 	    43 	 0.05101 	 0.03688 	 ~...
   15 	    44 	 0.04965 	 0.04019 	 ~...
   23 	    45 	 0.05295 	 0.04242 	 ~...
   25 	    46 	 0.05401 	 0.04520 	 ~...
    1 	    47 	 0.04934 	 0.04586 	 ~...
   24 	    48 	 0.05364 	 0.04945 	 ~...
   27 	    49 	 0.05547 	 0.04962 	 ~...
   28 	    50 	 0.05702 	 0.05677 	 ~...
   22 	    51 	 0.05291 	 0.05918 	 ~...
   20 	    52 	 0.05273 	 0.05981 	 ~...
   26 	    53 	 0.05468 	 0.06300 	 ~...
   30 	    54 	 0.06247 	 0.09914 	 m..s
   32 	    55 	 0.06526 	 0.10812 	 m..s
   35 	    56 	 0.07065 	 0.13310 	 m..s
   31 	    57 	 0.06382 	 0.13481 	 m..s
   41 	    58 	 0.07366 	 0.13488 	 m..s
   40 	    59 	 0.07356 	 0.13520 	 m..s
   91 	    60 	 0.14982 	 0.14257 	 ~...
   33 	    61 	 0.06856 	 0.14524 	 m..s
   93 	    62 	 0.15264 	 0.14898 	 ~...
   36 	    63 	 0.07181 	 0.14977 	 m..s
   34 	    64 	 0.06902 	 0.14994 	 m..s
   37 	    65 	 0.07188 	 0.15001 	 m..s
   45 	    66 	 0.07866 	 0.15696 	 m..s
   92 	    67 	 0.15039 	 0.15803 	 ~...
   38 	    68 	 0.07289 	 0.15958 	 m..s
   90 	    69 	 0.13419 	 0.16163 	 ~...
   61 	    70 	 0.11040 	 0.16268 	 m..s
   44 	    71 	 0.07812 	 0.16518 	 m..s
   39 	    72 	 0.07324 	 0.16654 	 m..s
   59 	    73 	 0.10019 	 0.17763 	 m..s
   71 	    74 	 0.11864 	 0.18155 	 m..s
   95 	    75 	 0.17430 	 0.18529 	 ~...
   60 	    76 	 0.10091 	 0.18588 	 m..s
   96 	    77 	 0.17649 	 0.18886 	 ~...
   50 	    78 	 0.09049 	 0.18916 	 m..s
   52 	    79 	 0.09165 	 0.19170 	 MISS
   63 	    80 	 0.11386 	 0.19780 	 m..s
   87 	    81 	 0.12274 	 0.19823 	 m..s
   62 	    82 	 0.11382 	 0.19892 	 m..s
   53 	    83 	 0.09176 	 0.19998 	 MISS
   58 	    84 	 0.09880 	 0.20044 	 MISS
   65 	    85 	 0.11654 	 0.20235 	 m..s
   48 	    86 	 0.08724 	 0.20390 	 MISS
   88 	    87 	 0.12398 	 0.20433 	 m..s
   98 	    88 	 0.17826 	 0.20665 	 ~...
   85 	    89 	 0.12183 	 0.20734 	 m..s
   97 	    90 	 0.17743 	 0.20874 	 m..s
   94 	    91 	 0.17278 	 0.21177 	 m..s
  101 	    92 	 0.18217 	 0.21492 	 m..s
   83 	    93 	 0.12137 	 0.21500 	 m..s
   81 	    94 	 0.12063 	 0.21539 	 m..s
   99 	    95 	 0.18035 	 0.21607 	 m..s
   78 	    96 	 0.11968 	 0.21877 	 m..s
  100 	    97 	 0.18146 	 0.22109 	 m..s
  112 	    98 	 0.22873 	 0.22254 	 ~...
   70 	    99 	 0.11838 	 0.22485 	 MISS
  106 	   100 	 0.20689 	 0.22583 	 ~...
  110 	   101 	 0.21771 	 0.22688 	 ~...
   51 	   102 	 0.09058 	 0.22727 	 MISS
   80 	   103 	 0.12054 	 0.23172 	 MISS
  111 	   104 	 0.22697 	 0.23319 	 ~...
  105 	   105 	 0.20663 	 0.23733 	 m..s
  103 	   106 	 0.20046 	 0.23847 	 m..s
   68 	   107 	 0.11775 	 0.24087 	 MISS
  107 	   108 	 0.20867 	 0.25083 	 m..s
  102 	   109 	 0.19250 	 0.25581 	 m..s
  109 	   110 	 0.21340 	 0.25638 	 m..s
  104 	   111 	 0.20493 	 0.25648 	 m..s
  108 	   112 	 0.21306 	 0.25786 	 m..s
  113 	   113 	 0.22884 	 0.26518 	 m..s
  117 	   114 	 0.25798 	 0.26771 	 ~...
  118 	   115 	 0.26263 	 0.27841 	 ~...
  115 	   116 	 0.24824 	 0.28112 	 m..s
  116 	   117 	 0.24825 	 0.29500 	 m..s
  119 	   118 	 0.26894 	 0.29861 	 ~...
  114 	   119 	 0.24707 	 0.29862 	 m..s
  120 	   120 	 0.28161 	 0.31364 	 m..s
==========================================
r_mrr = 0.7280722856521606
r2_mrr = 0.5127334594726562
spearmanr_mrr@5 = 0.9716888666152954
spearmanr_mrr@10 = 0.9699143767356873
spearmanr_mrr@50 = 0.9632448554039001
spearmanr_mrr@100 = 0.9042399525642395
spearmanr_mrr@All = 0.9276101589202881
==========================================
test time: 0.503
Done Testing dataset CoDExSmall
total time taken: 199.49441599845886
training time taken: 190.09241557121277
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7281)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.5127)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9717)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9699)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9632)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.9042)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9276)}}, 'test_loss': {'DistMult': {'CoDExSmall': 4.021057321057015}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 677895851252848
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [43, 1152, 474, 359, 658, 307, 587, 225, 178, 562, 1057, 774, 396, 715, 962, 502, 667, 1112, 1026, 1140, 879, 598, 959, 1018, 1144, 723, 166, 700, 883, 790, 676, 516, 85, 650, 939, 654, 175, 1039, 152, 901, 342, 1184, 12, 985, 770, 425, 57, 660, 842, 246, 923, 24, 54, 1071, 311, 276, 483, 729, 711, 366, 1133, 362, 335, 818, 347, 389, 809, 1110, 45, 837, 436, 1185, 941, 46, 343, 1101, 791, 482, 557, 549, 242, 255, 1085, 627, 219, 196, 87, 238, 61, 890, 74, 1012, 107, 885, 563, 360, 111, 397, 164, 142, 968, 892, 18, 174, 361, 162, 1132, 303, 1073, 780, 489, 220, 50, 843, 689, 236, 1115, 1202, 100, 656, 77]
valid_ids (0): []
train_ids (1094): [1056, 115, 405, 1176, 1168, 331, 772, 1000, 559, 432, 600, 556, 896, 752, 351, 1034, 792, 836, 535, 773, 767, 679, 775, 928, 7, 468, 1207, 1007, 499, 873, 903, 838, 847, 364, 233, 925, 285, 165, 584, 1019, 376, 534, 149, 292, 1035, 1206, 201, 487, 1004, 112, 727, 339, 41, 599, 607, 1155, 810, 0, 995, 446, 1067, 553, 608, 926, 145, 706, 66, 915, 1055, 1008, 63, 764, 65, 900, 691, 504, 289, 566, 506, 1211, 777, 481, 308, 105, 5, 987, 554, 8, 241, 651, 1064, 633, 296, 1143, 1043, 863, 163, 11, 78, 212, 301, 853, 393, 156, 756, 765, 944, 547, 256, 619, 907, 789, 615, 617, 804, 932, 294, 438, 4, 263, 1014, 284, 946, 442, 522, 337, 315, 686, 641, 420, 1183, 702, 128, 746, 418, 1051, 526, 492, 189, 856, 931, 922, 1096, 251, 322, 431, 623, 496, 97, 704, 755, 1038, 1082, 1029, 829, 203, 858, 788, 319, 58, 456, 701, 1186, 986, 864, 213, 579, 929, 832, 462, 473, 271, 973, 494, 695, 918, 758, 391, 747, 34, 824, 927, 28, 528, 1213, 517, 320, 787, 1161, 914, 889, 867, 611, 993, 227, 971, 527, 124, 1188, 582, 371, 1127, 458, 694, 576, 813, 674, 1125, 975, 493, 1083, 464, 561, 440, 413, 693, 514, 216, 610, 763, 120, 352, 349, 757, 109, 146, 782, 326, 317, 1098, 861, 476, 1006, 750, 421, 350, 976, 524, 110, 505, 834, 1003, 934, 471, 234, 899, 1002, 816, 161, 1158, 855, 232, 510, 139, 388, 382, 1146, 327, 583, 961, 806, 395, 625, 930, 872, 515, 414, 123, 1024, 970, 194, 1015, 906, 898, 448, 739, 1058, 148, 935, 1205, 338, 798, 1059, 274, 606, 947, 205, 1154, 681, 445, 978, 325, 759, 741, 353, 247, 191, 55, 845, 1052, 79, 1103, 605, 1089, 678, 297, 620, 616, 707, 1061, 609, 891, 383, 449, 844, 684, 210, 595, 193, 1027, 1032, 984, 558, 137, 722, 749, 118, 687, 675, 400, 1040, 507, 1028, 1163, 416, 511, 1126, 917, 1107, 158, 539, 1134, 119, 720, 379, 828, 603, 1138, 575, 893, 800, 974, 281, 187, 136, 1001, 1075, 272, 1078, 640, 306, 848, 1137, 1159, 182, 444, 384, 952, 439, 910, 530, 169, 321, 1214, 89, 718, 589, 126, 1076, 571, 200, 198, 1106, 724, 1050, 1066, 820, 332, 268, 447, 1088, 730, 104, 646, 812, 370, 1118, 868, 762, 134, 634, 390, 938, 1091, 245, 748, 799, 884, 1025, 257, 121, 1189, 463, 785, 288, 578, 531, 70, 101, 1121, 1196, 1179, 570, 839, 356, 497, 433, 406, 786, 1187, 644, 1141, 869, 409, 614, 86, 555, 967, 99, 500, 30, 1175, 44, 15, 424, 460, 354, 664, 387, 830, 717, 275, 503, 217, 1113, 1095, 969, 39, 1102, 631, 10, 1195, 696, 513, 983, 592, 685, 269, 344, 754, 870, 865, 1072, 223, 1149, 280, 1097, 62, 265, 159, 206, 170, 1120, 1047, 738, 626, 94, 22, 290, 690, 113, 851, 1191, 430, 1100, 532, 14, 710, 160, 154, 51, 1104, 441, 734, 1109, 286, 846, 260, 543, 179, 807, 1169, 733, 1124, 237, 398, 683, 283, 888, 254, 886, 1190, 977, 48, 1164, 1165, 1208, 68, 373, 668, 692, 417, 784, 211, 380, 989, 793, 150, 334, 745, 955, 1045, 1192, 33, 208, 1197, 950, 1081, 732, 466, 184, 850, 336, 453, 138, 147, 712, 966, 565, 407, 803, 355, 1212, 630, 912, 38, 725, 643, 357, 744, 1150, 92, 825, 218, 73, 1209, 188, 699, 484, 731, 1172, 860, 753, 346, 647, 310, 1063, 1181, 1011, 302, 612, 20, 551, 776, 751, 341, 1060, 680, 708, 996, 192, 958, 1044, 88, 659, 1177, 300, 1068, 963, 548, 207, 84, 540, 1135, 21, 141, 480, 450, 766, 714, 655, 64, 1079, 1178, 849, 314, 854, 1031, 429, 811, 895, 378, 728, 957, 330, 852, 239, 377, 226, 47, 1037, 1204, 999, 305, 954, 478, 1142, 1166, 185, 71, 177, 90, 1092, 1129, 519, 721, 385, 454, 833, 117, 560, 1022, 533, 264, 1156, 358, 956, 1116, 40, 601, 1194, 410, 180, 318, 726, 636, 670, 742, 943, 911, 964, 495, 1200, 657, 652, 381, 1053, 716, 965, 1123, 278, 13, 573, 541, 72, 538, 509, 1020, 1148, 399, 878, 369, 697, 781, 988, 243, 577, 1062, 221, 287, 486, 1203, 365, 114, 122, 457, 282, 1048, 933, 508, 1198, 768, 1077, 669, 876, 597, 329, 230, 737, 709, 949, 760, 31, 887, 894, 897, 719, 98, 498, 1160, 167, 1199, 427, 794, 401, 796, 544, 459, 299, 374, 713, 29, 224, 997, 545, 518, 23, 743, 593, 525, 761, 1173, 661, 95, 479, 437, 80, 435, 1167, 1005, 1023, 624, 1139, 248, 1130, 979, 677, 877, 273, 35, 443, 25, 229, 404, 937, 470, 1090, 942, 992, 1108, 1013, 3, 249, 1128, 82, 59, 982, 291, 862, 904, 1171, 157, 621, 130, 1170, 9, 635, 155, 808, 882, 452, 412, 1094, 2, 304, 1099, 270, 536, 408, 1210, 663, 313, 916, 638, 1122, 1010, 67, 96, 596, 841, 821, 568, 673, 920, 469, 204, 1182, 258, 402, 1153, 981, 666, 980, 83, 26, 222, 228, 831, 235, 994, 585, 16, 1136, 244, 665, 151, 945, 171, 426, 736, 316, 1086, 703, 771, 1174, 740, 783, 143, 1147, 586, 279, 653, 214, 1030, 682, 990, 475, 153, 490, 106, 801, 415, 467, 76, 19, 428, 613, 173, 779, 642, 594, 1084, 632, 827, 671, 688, 1021, 1131, 116, 521, 546, 924, 176, 637, 1201, 564, 199, 622, 1070, 880, 345, 902, 960, 1087, 698, 197, 202, 434, 795, 881, 940, 1151, 323, 181, 231, 909, 183, 1065, 132, 375, 1069, 125, 645, 567, 523, 835, 672, 1111, 953, 913, 875, 569, 529, 948, 580, 6, 936, 75, 419, 814, 951, 309, 572, 628, 423, 368, 705, 552, 602, 1162, 908, 991, 1, 394, 1180, 998, 649, 91, 403, 919, 485, 267, 215, 127, 253, 465, 581, 1119, 542, 312, 1033, 386, 455, 1093, 168, 53, 27, 822, 840, 819, 240, 1080, 550, 133, 293, 1016, 392, 1157, 859, 1017, 348, 735, 1049, 871, 1145, 32, 537, 491, 340, 1074, 37, 422, 472, 866, 298, 69, 817, 259, 372, 477, 857, 648, 1041, 769, 56, 1114, 144, 618, 778, 1054, 102, 277, 295, 108, 140, 823, 588, 42, 1193, 1046, 874, 261, 1105, 921, 93, 172, 190, 328, 1009, 972, 52, 250, 131, 905, 129, 802, 17, 135, 629, 662, 461, 451, 60, 324, 209, 266, 639, 103, 195, 252, 1036, 805, 590, 591, 826, 333, 363, 512, 488, 411, 574, 604, 186, 1042, 367, 49, 36, 81, 1117, 520, 501, 815, 262, 797]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9137863676932640
the save name prefix for this run is:  chkpt-ID_9137863676932640_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 990
rank avg (pred): 0.506 +- 0.008
mrr vals (pred, true): 0.001, 0.206
batch losses (mrrl, rdl): 0.0, 0.0044936636

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 237
rank avg (pred): 0.277 +- 0.213
mrr vals (pred, true): 0.057, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006184859

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 464
rank avg (pred): 0.271 +- 0.226
mrr vals (pred, true): 0.138, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006972656

Epoch over!
epoch time: 12.217

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 26
rank avg (pred): 0.043 +- 0.039
mrr vals (pred, true): 0.319, 0.253
batch losses (mrrl, rdl): 0.0, 3.0197e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 794
rank avg (pred): 0.460 +- 0.353
mrr vals (pred, true): 0.066, 0.004
batch losses (mrrl, rdl): 0.0, 4.97761e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 254
rank avg (pred): 0.056 +- 0.051
mrr vals (pred, true): 0.328, 0.237
batch losses (mrrl, rdl): 0.0, 3.8266e-06

Epoch over!
epoch time: 12.902

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 663
rank avg (pred): 0.400 +- 0.301
mrr vals (pred, true): 0.059, 0.004
batch losses (mrrl, rdl): 0.0, 5.17753e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 526
rank avg (pred): 0.189 +- 0.182
mrr vals (pred, true): 0.334, 0.042
batch losses (mrrl, rdl): 0.0, 1.2518e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 871
rank avg (pred): 0.411 +- 0.346
mrr vals (pred, true): 0.271, 0.005
batch losses (mrrl, rdl): 0.0, 4.29732e-05

Epoch over!
epoch time: 12.483

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1130
rank avg (pred): 0.245 +- 0.219
mrr vals (pred, true): 0.256, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007872823

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 187
rank avg (pred): 0.255 +- 0.244
mrr vals (pred, true): 0.349, 0.004
batch losses (mrrl, rdl): 0.0, 0.0006666479

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 772
rank avg (pred): 0.421 +- 0.329
mrr vals (pred, true): 0.252, 0.021
batch losses (mrrl, rdl): 0.0, 5.84948e-05

Epoch over!
epoch time: 12.477

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 264
rank avg (pred): 0.060 +- 0.060
mrr vals (pred, true): 0.394, 0.210
batch losses (mrrl, rdl): 0.0, 4.7268e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 561
rank avg (pred): 0.173 +- 0.160
mrr vals (pred, true): 0.280, 0.033
batch losses (mrrl, rdl): 0.0, 7.84149e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 101
rank avg (pred): 0.257 +- 0.245
mrr vals (pred, true): 0.316, 0.178
batch losses (mrrl, rdl): 0.0, 0.0011111774

Epoch over!
epoch time: 12.213

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 995
rank avg (pred): 0.020 +- 0.020
mrr vals (pred, true): 0.402, 0.256
batch losses (mrrl, rdl): 0.2115238011, 1.24847e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 277
rank avg (pred): 0.129 +- 0.095
mrr vals (pred, true): 0.179, 0.226
batch losses (mrrl, rdl): 0.021565523, 0.0001378171

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 95
rank avg (pred): 0.338 +- 0.209
mrr vals (pred, true): 0.143, 0.185
batch losses (mrrl, rdl): 0.018226264, 0.0018703454

Epoch over!
epoch time: 12.857

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 182
rank avg (pred): 0.338 +- 0.206
mrr vals (pred, true): 0.138, 0.004
batch losses (mrrl, rdl): 0.0781871378, 0.0002560328

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1125
rank avg (pred): 0.291 +- 0.183
mrr vals (pred, true): 0.138, 0.004
batch losses (mrrl, rdl): 0.077596806, 0.0006170889

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 501
rank avg (pred): 0.213 +- 0.125
mrr vals (pred, true): 0.111, 0.145
batch losses (mrrl, rdl): 0.0116200754, 0.0001400285

Epoch over!
epoch time: 12.776

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 595
rank avg (pred): 0.305 +- 0.181
mrr vals (pred, true): 0.125, 0.146
batch losses (mrrl, rdl): 0.0045779948, 0.0003967174

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 293
rank avg (pred): 0.034 +- 0.033
mrr vals (pred, true): 0.278, 0.292
batch losses (mrrl, rdl): 0.0021442831, 1.2959e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 371
rank avg (pred): 0.234 +- 0.156
mrr vals (pred, true): 0.119, 0.198
batch losses (mrrl, rdl): 0.0620201565, 0.000727108

Epoch over!
epoch time: 12.649

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1037
rank avg (pred): 0.231 +- 0.157
mrr vals (pred, true): 0.140, 0.004
batch losses (mrrl, rdl): 0.0813340992, 0.0011825791

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 109
rank avg (pred): 0.227 +- 0.151
mrr vals (pred, true): 0.127, 0.214
batch losses (mrrl, rdl): 0.0758883432, 0.0006580799

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 472
rank avg (pred): 0.215 +- 0.150
mrr vals (pred, true): 0.132, 0.003
batch losses (mrrl, rdl): 0.0676079541, 0.001370898

Epoch over!
epoch time: 12.736

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 248
rank avg (pred): 0.087 +- 0.067
mrr vals (pred, true): 0.229, 0.220
batch losses (mrrl, rdl): 0.0007500547, 3.97893e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 129
rank avg (pred): 0.231 +- 0.150
mrr vals (pred, true): 0.120, 0.234
batch losses (mrrl, rdl): 0.1288363039, 0.0007706466

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 814
rank avg (pred): 0.175 +- 0.118
mrr vals (pred, true): 0.141, 0.051
batch losses (mrrl, rdl): 0.0833984464, 0.0002218632

Epoch over!
epoch time: 12.439

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 141
rank avg (pred): 0.258 +- 0.148
mrr vals (pred, true): 0.115, 0.213
batch losses (mrrl, rdl): 0.0961947888, 0.000871282

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 822
rank avg (pred): 0.125 +- 0.084
mrr vals (pred, true): 0.215, 0.241
batch losses (mrrl, rdl): 0.0068039401, 0.0001483079

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 167
rank avg (pred): 0.211 +- 0.138
mrr vals (pred, true): 0.128, 0.005
batch losses (mrrl, rdl): 0.0614237599, 0.0013762756

Epoch over!
epoch time: 12.525

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 248
rank avg (pred): 0.077 +- 0.061
mrr vals (pred, true): 0.233, 0.220
batch losses (mrrl, rdl): 0.0016643024, 2.39316e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 207
rank avg (pred): 0.212 +- 0.145
mrr vals (pred, true): 0.133, 0.003
batch losses (mrrl, rdl): 0.0687474832, 0.0013575624

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 350
rank avg (pred): 0.262 +- 0.151
mrr vals (pred, true): 0.111, 0.183
batch losses (mrrl, rdl): 0.0528972633, 0.0009171649

Epoch over!
epoch time: 12.563

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 51
rank avg (pred): 0.101 +- 0.072
mrr vals (pred, true): 0.234, 0.228
batch losses (mrrl, rdl): 0.0004257692, 5.3054e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 692
rank avg (pred): 0.288 +- 0.196
mrr vals (pred, true): 0.118, 0.004
batch losses (mrrl, rdl): 0.0464335233, 0.0007172212

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 190
rank avg (pred): 0.219 +- 0.137
mrr vals (pred, true): 0.128, 0.005
batch losses (mrrl, rdl): 0.0604554191, 0.0013092348

Epoch over!
epoch time: 12.225

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 630
rank avg (pred): 0.389 +- 0.233
mrr vals (pred, true): 0.082, 0.131
batch losses (mrrl, rdl): 0.0240621194, 0.0009159702

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 75
rank avg (pred): 0.056 +- 0.045
mrr vals (pred, true): 0.266, 0.222
batch losses (mrrl, rdl): 0.0195370875, 2.772e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 310
rank avg (pred): 0.066 +- 0.053
mrr vals (pred, true): 0.259, 0.255
batch losses (mrrl, rdl): 0.0001222237, 9.8834e-06

Epoch over!
epoch time: 12.43

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1207
rank avg (pred): 0.430 +- 0.301
mrr vals (pred, true): 0.085, 0.004
batch losses (mrrl, rdl): 0.0120205898, 7.68992e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 789
rank avg (pred): 0.541 +- 0.239
mrr vals (pred, true): 0.022, 0.003
batch losses (mrrl, rdl): 0.0080965012, 4.25783e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1043
rank avg (pred): 0.209 +- 0.124
mrr vals (pred, true): 0.127, 0.004
batch losses (mrrl, rdl): 0.0591757149, 0.0013418138

Epoch over!
epoch time: 12.373

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.090 +- 0.066
mrr vals (pred, true): 0.203, 0.216

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.05796 	 0.00073 	 m..s
   26 	     1 	 0.06025 	 0.00074 	 m..s
   10 	     2 	 0.03542 	 0.00075 	 m..s
   13 	     3 	 0.03667 	 0.00159 	 m..s
   29 	     4 	 0.08005 	 0.00173 	 m..s
   24 	     5 	 0.05915 	 0.00288 	 m..s
   70 	     6 	 0.12448 	 0.00302 	 MISS
   83 	     7 	 0.12967 	 0.00310 	 MISS
   34 	     8 	 0.08775 	 0.00325 	 m..s
   18 	     9 	 0.04869 	 0.00343 	 m..s
    1 	    10 	 0.01988 	 0.00345 	 ~...
   59 	    11 	 0.12136 	 0.00351 	 MISS
   42 	    12 	 0.09603 	 0.00363 	 m..s
   77 	    13 	 0.12835 	 0.00367 	 MISS
   39 	    14 	 0.09410 	 0.00369 	 m..s
   66 	    15 	 0.12416 	 0.00370 	 MISS
   72 	    16 	 0.12631 	 0.00373 	 MISS
    6 	    17 	 0.03189 	 0.00377 	 ~...
   33 	    18 	 0.08666 	 0.00378 	 m..s
   92 	    19 	 0.13436 	 0.00380 	 MISS
   35 	    20 	 0.09033 	 0.00383 	 m..s
   38 	    21 	 0.09368 	 0.00387 	 m..s
   48 	    22 	 0.11720 	 0.00392 	 MISS
   79 	    23 	 0.12866 	 0.00394 	 MISS
   22 	    24 	 0.05774 	 0.00395 	 m..s
   19 	    25 	 0.05532 	 0.00412 	 m..s
   17 	    26 	 0.04375 	 0.00413 	 m..s
   81 	    27 	 0.12897 	 0.00415 	 MISS
   55 	    28 	 0.11861 	 0.00416 	 MISS
   36 	    29 	 0.09241 	 0.00417 	 m..s
   73 	    30 	 0.12664 	 0.00419 	 MISS
   30 	    31 	 0.08255 	 0.00422 	 m..s
   94 	    32 	 0.13517 	 0.00422 	 MISS
   46 	    33 	 0.09789 	 0.00434 	 m..s
   40 	    34 	 0.09453 	 0.00438 	 m..s
   48 	    35 	 0.11720 	 0.00442 	 MISS
   20 	    36 	 0.05756 	 0.00445 	 m..s
   95 	    37 	 0.13809 	 0.00448 	 MISS
   47 	    38 	 0.10149 	 0.00451 	 m..s
   75 	    39 	 0.12706 	 0.00452 	 MISS
   25 	    40 	 0.06002 	 0.00457 	 m..s
   62 	    41 	 0.12280 	 0.00459 	 MISS
   87 	    42 	 0.13112 	 0.00463 	 MISS
   63 	    43 	 0.12311 	 0.00464 	 MISS
   80 	    44 	 0.12881 	 0.00471 	 MISS
   43 	    45 	 0.09608 	 0.00481 	 m..s
   91 	    46 	 0.13354 	 0.00483 	 MISS
   90 	    47 	 0.13164 	 0.00484 	 MISS
    9 	    48 	 0.03299 	 0.00498 	 ~...
   89 	    49 	 0.13154 	 0.00505 	 MISS
   76 	    50 	 0.12710 	 0.00521 	 MISS
   56 	    51 	 0.12020 	 0.00531 	 MISS
   44 	    52 	 0.09632 	 0.00543 	 m..s
    3 	    53 	 0.02534 	 0.00547 	 ~...
    0 	    54 	 0.01883 	 0.00552 	 ~...
   15 	    55 	 0.03688 	 0.02583 	 ~...
   16 	    56 	 0.04222 	 0.02595 	 ~...
   27 	    57 	 0.06736 	 0.03246 	 m..s
   14 	    58 	 0.03668 	 0.03260 	 ~...
    2 	    59 	 0.02297 	 0.03331 	 ~...
    7 	    60 	 0.03210 	 0.03415 	 ~...
    4 	    61 	 0.02589 	 0.03850 	 ~...
    8 	    62 	 0.03224 	 0.03993 	 ~...
   12 	    63 	 0.03617 	 0.05472 	 ~...
    5 	    64 	 0.03009 	 0.05762 	 ~...
   11 	    65 	 0.03592 	 0.05810 	 ~...
   21 	    66 	 0.05764 	 0.07395 	 ~...
   45 	    67 	 0.09741 	 0.10429 	 ~...
   28 	    68 	 0.07920 	 0.10812 	 ~...
   37 	    69 	 0.09299 	 0.12642 	 m..s
   31 	    70 	 0.08276 	 0.13839 	 m..s
   98 	    71 	 0.16343 	 0.14733 	 ~...
   96 	    72 	 0.14588 	 0.15307 	 ~...
   41 	    73 	 0.09596 	 0.15822 	 m..s
   97 	    74 	 0.14799 	 0.16163 	 ~...
   32 	    75 	 0.08578 	 0.16628 	 m..s
   69 	    76 	 0.12444 	 0.18155 	 m..s
   99 	    77 	 0.19471 	 0.18169 	 ~...
  100 	    78 	 0.19753 	 0.18234 	 ~...
   68 	    79 	 0.12433 	 0.18252 	 m..s
   57 	    80 	 0.12029 	 0.18579 	 m..s
   48 	    81 	 0.11720 	 0.18588 	 m..s
  101 	    82 	 0.19755 	 0.18886 	 ~...
   48 	    83 	 0.11720 	 0.18916 	 m..s
   48 	    84 	 0.11720 	 0.19170 	 m..s
   85 	    85 	 0.12990 	 0.19254 	 m..s
   61 	    86 	 0.12262 	 0.19534 	 m..s
   60 	    87 	 0.12155 	 0.19709 	 m..s
   71 	    88 	 0.12470 	 0.20122 	 m..s
   64 	    89 	 0.12362 	 0.20139 	 m..s
   48 	    90 	 0.11720 	 0.20140 	 m..s
   74 	    91 	 0.12704 	 0.20303 	 m..s
   67 	    92 	 0.12419 	 0.20709 	 m..s
   58 	    93 	 0.12087 	 0.20842 	 m..s
   65 	    94 	 0.12409 	 0.21117 	 m..s
  103 	    95 	 0.23202 	 0.21168 	 ~...
   84 	    96 	 0.12967 	 0.21345 	 m..s
  120 	    97 	 0.26903 	 0.21487 	 m..s
  113 	    98 	 0.24570 	 0.21580 	 ~...
  102 	    99 	 0.20269 	 0.21615 	 ~...
   93 	   100 	 0.13466 	 0.21748 	 m..s
  108 	   101 	 0.23735 	 0.21930 	 ~...
  106 	   102 	 0.23320 	 0.22157 	 ~...
  119 	   103 	 0.26902 	 0.22199 	 m..s
  104 	   104 	 0.23210 	 0.22270 	 ~...
   78 	   105 	 0.12842 	 0.22411 	 m..s
  109 	   106 	 0.23886 	 0.22442 	 ~...
  107 	   107 	 0.23394 	 0.22512 	 ~...
  114 	   108 	 0.24670 	 0.22806 	 ~...
   54 	   109 	 0.11720 	 0.22917 	 MISS
  110 	   110 	 0.24170 	 0.23088 	 ~...
   86 	   111 	 0.13108 	 0.23410 	 MISS
   82 	   112 	 0.12900 	 0.23677 	 MISS
  111 	   113 	 0.24350 	 0.23706 	 ~...
   88 	   114 	 0.13122 	 0.24346 	 MISS
  118 	   115 	 0.25946 	 0.24480 	 ~...
  117 	   116 	 0.25730 	 0.24623 	 ~...
  105 	   117 	 0.23265 	 0.25578 	 ~...
  112 	   118 	 0.24445 	 0.25966 	 ~...
  115 	   119 	 0.24961 	 0.26308 	 ~...
  116 	   120 	 0.25187 	 0.30295 	 m..s
==========================================
r_mrr = 0.6896710991859436
r2_mrr = 0.4147653579711914
spearmanr_mrr@5 = 0.7617266178131104
spearmanr_mrr@10 = 0.8779511451721191
spearmanr_mrr@50 = 0.8615055680274963
spearmanr_mrr@100 = 0.8123024106025696
spearmanr_mrr@All = 0.8424728512763977
==========================================
test time: 0.386
Done Testing dataset CoDExSmall
total time taken: 197.97124648094177
training time taken: 188.32374334335327
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.6897)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4148)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.7617)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.8780)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8615)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8123)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8425)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.752933129449957}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 6418307216144532
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [46, 326, 723, 811, 290, 1178, 109, 1100, 1060, 303, 50, 543, 725, 920, 1139, 137, 903, 1165, 1194, 992, 571, 1051, 48, 864, 891, 11, 269, 1007, 884, 1212, 504, 136, 837, 821, 923, 173, 408, 847, 61, 745, 558, 387, 602, 419, 889, 135, 369, 848, 793, 226, 159, 235, 366, 1062, 738, 749, 676, 353, 89, 521, 555, 939, 951, 812, 873, 1031, 1048, 542, 20, 122, 119, 431, 934, 1124, 653, 704, 927, 43, 896, 1199, 118, 563, 700, 1008, 948, 570, 150, 899, 1173, 625, 806, 639, 836, 207, 672, 286, 794, 133, 1113, 442, 1039, 196, 550, 75, 1163, 1032, 935, 1092, 892, 260, 329, 245, 1184, 1013, 351, 199, 6, 1059, 142, 265, 70]
valid_ids (0): []
train_ids (1094): [604, 1014, 804, 667, 987, 330, 529, 217, 396, 844, 409, 979, 1130, 340, 106, 1111, 575, 190, 128, 695, 721, 252, 669, 546, 318, 36, 1118, 824, 673, 562, 335, 921, 461, 1135, 78, 635, 473, 490, 394, 12, 397, 430, 141, 251, 1054, 463, 576, 455, 1112, 172, 516, 194, 1037, 1017, 210, 153, 904, 460, 828, 417, 779, 736, 377, 1190, 495, 789, 716, 1192, 805, 285, 35, 401, 241, 1144, 719, 249, 748, 827, 483, 1157, 262, 19, 801, 950, 1063, 1169, 263, 1105, 395, 858, 389, 544, 915, 445, 759, 780, 317, 724, 434, 16, 1061, 888, 204, 893, 169, 654, 877, 712, 1171, 64, 886, 1129, 810, 341, 358, 642, 706, 1127, 1027, 626, 32, 770, 1053, 280, 671, 545, 548, 485, 55, 375, 115, 90, 803, 972, 1128, 524, 675, 412, 658, 853, 502, 404, 300, 863, 945, 261, 1042, 819, 1045, 1211, 613, 197, 29, 938, 478, 616, 1133, 424, 641, 1016, 800, 458, 222, 1078, 259, 1162, 52, 418, 305, 383, 767, 778, 1121, 1015, 677, 955, 632, 782, 792, 906, 1168, 465, 1172, 435, 567, 438, 525, 559, 292, 699, 80, 357, 208, 1088, 323, 1142, 22, 650, 250, 708, 423, 228, 386, 857, 345, 468, 769, 611, 762, 1012, 557, 580, 1196, 662, 2, 963, 38, 1122, 1099, 855, 339, 157, 711, 72, 714, 739, 909, 175, 707, 615, 509, 1176, 931, 640, 1208, 471, 97, 1115, 574, 826, 1026, 645, 124, 1207, 312, 753, 722, 533, 678, 894, 1025, 825, 110, 551, 954, 385, 538, 220, 1181, 818, 701, 474, 620, 974, 58, 937, 247, 1066, 517, 216, 506, 333, 783, 332, 534, 1152, 777, 462, 60, 907, 776, 941, 831, 564, 693, 988, 997, 617, 809, 475, 713, 221, 510, 202, 1041, 796, 165, 496, 67, 230, 1154, 1095, 3, 952, 644, 99, 930, 1134, 741, 940, 597, 936, 663, 599, 96, 160, 1087, 561, 420, 198, 1109, 470, 117, 751, 4, 244, 407, 1147, 1065, 897, 63, 1188, 347, 156, 93, 39, 528, 768, 308, 1110, 609, 443, 114, 336, 761, 1179, 648, 505, 1164, 270, 912, 182, 598, 1097, 132, 715, 448, 682, 984, 28, 464, 859, 472, 594, 188, 231, 772, 1046, 88, 910, 1123, 568, 815, 275, 742, 7, 1050, 94, 279, 512, 62, 983, 54, 629, 382, 27, 282, 926, 469, 1206, 276, 755, 612, 554, 999, 246, 40, 343, 766, 69, 1033, 588, 1000, 991, 187, 25, 1005, 816, 685, 21, 327, 1209, 1098, 624, 271, 201, 497, 92, 1019, 1150, 606, 319, 998, 577, 289, 466, 807, 905, 838, 1058, 86, 1107, 870, 879, 350, 968, 1156, 586, 1070, 515, 850, 477, 833, 342, 146, 1205, 321, 985, 994, 253, 1117, 619, 1202, 348, 487, 664, 900, 492, 140, 530, 582, 17, 1187, 593, 84, 255, 949, 24, 539, 33, 774, 1177, 1167, 503, 151, 784, 652, 9, 584, 797, 508, 163, 964, 493, 414, 1114, 1201, 234, 371, 1116, 933, 149, 822, 278, 924, 861, 638, 718, 911, 908, 355, 41, 334, 1, 489, 283, 552, 608, 730, 1145, 1086, 381, 922, 928, 610, 876, 914, 10, 301, 898, 986, 447, 764, 102, 237, 970, 656, 744, 883, 291, 364, 467, 95, 829, 398, 882, 1140, 436, 630, 657, 233, 481, 281, 363, 171, 978, 823, 126, 1028, 164, 942, 752, 304, 919, 499, 307, 30, 618, 852, 129, 595, 969, 689, 1074, 378, 456, 1198, 439, 660, 953, 680, 359, 960, 154, 560, 476, 392, 81, 1020, 569, 116, 540, 874, 781, 679, 413, 1094, 865, 162, 756, 184, 798, 621, 147, 665, 243, 268, 433, 236, 393, 692, 399, 388, 296, 479, 651, 814, 684, 1214, 549, 754, 287, 183, 158, 785, 881, 1166, 846, 491, 703, 34, 367, 842, 771, 179, 82, 349, 446, 494, 537, 1149, 1071, 1126, 990, 71, 337, 91, 1079, 362, 138, 1101, 519, 743, 961, 808, 23, 871, 121, 123, 215, 709, 181, 788, 205, 1072, 536, 839, 1119, 1189, 1055, 627, 143, 1160, 573, 298, 728, 916, 178, 227, 646, 1075, 813, 1077, 170, 565, 313, 108, 180, 670, 740, 637, 1106, 134, 45, 450, 603, 391, 1186, 1175, 688, 622, 1040, 191, 498, 895, 66, 1146, 995, 384, 729, 426, 1151, 1009, 541, 229, 177, 757, 860, 176, 583, 320, 405, 1023, 1203, 103, 655, 1081, 238, 929, 242, 687, 572, 832, 589, 294, 1161, 74, 746, 0, 79, 578, 760, 1200, 862, 85, 125, 167, 1180, 284, 726, 1102, 293, 710, 288, 309, 451, 1057, 1035, 918, 1030, 522, 267, 297, 634, 1103, 449, 239, 790, 277, 354, 1043, 107, 139, 731, 315, 1155, 148, 820, 956, 403, 352, 212, 592, 659, 225, 691, 845, 325, 971, 717, 758, 390, 511, 374, 453, 834, 1143, 13, 1004, 1089, 787, 866, 1159, 735, 324, 376, 1029, 902, 1080, 100, 579, 189, 87, 444, 224, 101, 486, 869, 254, 480, 195, 1210, 406, 943, 967, 37, 733, 437, 843, 702, 1213, 981, 429, 817, 957, 131, 786, 977, 1069, 631, 311, 1001, 185, 200, 737, 240, 1131, 1064, 636, 696, 365, 1108, 65, 1148, 1052, 402, 59, 913, 454, 1174, 989, 161, 440, 1195, 856, 1022, 174, 1104, 614, 1010, 15, 885, 416, 218, 368, 791, 53, 605, 314, 683, 1068, 973, 1082, 750, 596, 1096, 57, 232, 841, 880, 373, 152, 500, 1158, 1132, 727, 425, 890, 166, 356, 47, 1021, 49, 77, 211, 993, 944, 104, 338, 1018, 306, 1036, 219, 547, 206, 1083, 531, 698, 299, 1170, 996, 372, 76, 532, 257, 1138, 773, 872, 56, 328, 581, 273, 379, 601, 26, 720, 213, 331, 527, 591, 507, 681, 428, 5, 1034, 18, 31, 623, 484, 192, 441, 1011, 1125, 1049, 674, 647, 264, 73, 459, 1093, 965, 8, 120, 1185, 1067, 258, 44, 966, 666, 1044, 501, 272, 628, 830, 849, 410, 1137, 607, 452, 370, 421, 223, 346, 302, 1193, 868, 42, 130, 975, 145, 932, 1076, 432, 1141, 566, 214, 1136, 1090, 1047, 400, 633, 947, 1120, 203, 193, 360, 887, 83, 982, 457, 690, 51, 686, 556, 316, 1153, 668, 14, 835, 361, 946, 705, 1073, 248, 105, 851, 144, 274, 1002, 734, 344, 514, 962, 427, 1091, 186, 310, 553, 765, 518, 1191, 775, 643, 959, 867, 1197, 1183, 113, 209, 322, 523, 747, 266, 98, 513, 763, 980, 917, 1024, 411, 415, 112, 68, 697, 482, 1084, 649, 1003, 1056, 422, 526, 520, 111, 295, 976, 958, 802, 840, 1038, 732, 901, 878, 925, 256, 168, 694, 1182, 854, 799, 127, 875, 587, 1204, 380, 795, 1006, 1085, 155, 600, 585, 488, 661, 535, 590]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6254364806297936
the save name prefix for this run is:  chkpt-ID_6254364806297936_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1180
rank avg (pred): 0.442 +- 0.006
mrr vals (pred, true): 0.001, 0.145
batch losses (mrrl, rdl): 0.0, 0.0014732681

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 146
rank avg (pred): 0.222 +- 0.115
mrr vals (pred, true): 0.010, 0.201
batch losses (mrrl, rdl): 0.0, 0.0006500863

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 818
rank avg (pred): 0.158 +- 0.105
mrr vals (pred, true): 0.099, 0.032
batch losses (mrrl, rdl): 0.0, 0.0006099301

Epoch over!
epoch time: 12.798

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1181
rank avg (pred): 0.349 +- 0.225
mrr vals (pred, true): 0.087, 0.146
batch losses (mrrl, rdl): 0.0, 0.0007325327

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 300
rank avg (pred): 0.029 +- 0.021
mrr vals (pred, true): 0.300, 0.208
batch losses (mrrl, rdl): 0.0, 2.1442e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 267
rank avg (pred): 0.060 +- 0.045
mrr vals (pred, true): 0.278, 0.223
batch losses (mrrl, rdl): 0.0, 6.4479e-06

Epoch over!
epoch time: 11.843

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 608
rank avg (pred): 0.318 +- 0.238
mrr vals (pred, true): 0.250, 0.155
batch losses (mrrl, rdl): 0.0, 0.000640649

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 188
rank avg (pred): 0.242 +- 0.178
mrr vals (pred, true): 0.232, 0.004
batch losses (mrrl, rdl): 0.0, 0.0009665735

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1030
rank avg (pred): 0.231 +- 0.187
mrr vals (pred, true): 0.306, 0.005
batch losses (mrrl, rdl): 0.0, 0.0011122248

Epoch over!
epoch time: 12.063

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 870
rank avg (pred): 0.395 +- 0.302
mrr vals (pred, true): 0.270, 0.004
batch losses (mrrl, rdl): 0.0, 4.9755e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1149
rank avg (pred): 0.165 +- 0.136
mrr vals (pred, true): 0.309, 0.057
batch losses (mrrl, rdl): 0.0, 1.23714e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 584
rank avg (pred): 0.313 +- 0.253
mrr vals (pred, true): 0.305, 0.104
batch losses (mrrl, rdl): 0.0, 0.0001014662

Epoch over!
epoch time: 12.47

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 748
rank avg (pred): 0.081 +- 0.070
mrr vals (pred, true): 0.354, 0.169
batch losses (mrrl, rdl): 0.0, 2.56424e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 554
rank avg (pred): 0.193 +- 0.160
mrr vals (pred, true): 0.314, 0.047
batch losses (mrrl, rdl): 0.0, 2.32831e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 802
rank avg (pred): 0.398 +- 0.326
mrr vals (pred, true): 0.302, 0.003
batch losses (mrrl, rdl): 0.0, 6.50023e-05

Epoch over!
epoch time: 11.978

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1175
rank avg (pred): 0.296 +- 0.260
mrr vals (pred, true): 0.322, 0.144
batch losses (mrrl, rdl): 0.3147686124, 0.0003483422

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 161
rank avg (pred): 0.374 +- 0.217
mrr vals (pred, true): 0.119, 0.199
batch losses (mrrl, rdl): 0.0632503405, 0.0024268881

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 184
rank avg (pred): 0.435 +- 0.252
mrr vals (pred, true): 0.123, 0.003
batch losses (mrrl, rdl): 0.0526946262, 2.04568e-05

Epoch over!
epoch time: 12.419

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 585
rank avg (pred): 0.542 +- 0.240
mrr vals (pred, true): 0.097, 0.150
batch losses (mrrl, rdl): 0.0283343885, 0.0028210415

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 944
rank avg (pred): 0.588 +- 0.174
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.85434e-05, 0.0005956434

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 117
rank avg (pred): 0.250 +- 0.154
mrr vals (pred, true): 0.133, 0.225
batch losses (mrrl, rdl): 0.0847595781, 0.000926086

Epoch over!
epoch time: 12.608

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 807
rank avg (pred): 0.540 +- 0.202
mrr vals (pred, true): 0.082, 0.004
batch losses (mrrl, rdl): 0.010084915, 0.0001345071

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 5
rank avg (pred): 0.021 +- 0.013
mrr vals (pred, true): 0.198, 0.237
batch losses (mrrl, rdl): 0.0158342943, 6.3231e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 189
rank avg (pred): 0.301 +- 0.177
mrr vals (pred, true): 0.112, 0.005
batch losses (mrrl, rdl): 0.0390550084, 0.0005351844

Epoch over!
epoch time: 11.99

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1067
rank avg (pred): 0.024 +- 0.015
mrr vals (pred, true): 0.198, 0.259
batch losses (mrrl, rdl): 0.0362144336, 1.35392e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 200
rank avg (pred): 0.351 +- 0.209
mrr vals (pred, true): 0.123, 0.004
batch losses (mrrl, rdl): 0.0531017594, 0.0002602021

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 880
rank avg (pred): 0.499 +- 0.177
mrr vals (pred, true): 0.077, 0.005
batch losses (mrrl, rdl): 0.0074419915, 7.24313e-05

Epoch over!
epoch time: 12.774

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 867
rank avg (pred): 0.521 +- 0.148
mrr vals (pred, true): 0.053, 0.004
batch losses (mrrl, rdl): 7.76844e-05, 9.09716e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 729
rank avg (pred): 0.473 +- 0.189
mrr vals (pred, true): 0.083, 0.108
batch losses (mrrl, rdl): 0.0065276637, 0.0034453438

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 610
rank avg (pred): 0.426 +- 0.198
mrr vals (pred, true): 0.089, 0.156
batch losses (mrrl, rdl): 0.0447409265, 0.0014640718

Epoch over!
epoch time: 11.951

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 919
rank avg (pred): 0.514 +- 0.124
mrr vals (pred, true): 0.042, 0.003
batch losses (mrrl, rdl): 0.0005816646, 0.0001194986

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 88
rank avg (pred): 0.309 +- 0.184
mrr vals (pred, true): 0.136, 0.197
batch losses (mrrl, rdl): 0.0372564867, 0.0014158869

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 565
rank avg (pred): 0.319 +- 0.168
mrr vals (pred, true): 0.107, 0.036
batch losses (mrrl, rdl): 0.0328114107, 0.0002123197

Epoch over!
epoch time: 13.033

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.299 +- 0.172
mrr vals (pred, true): 0.121, 0.216
batch losses (mrrl, rdl): 0.0901743695, 0.0012813798

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 668
rank avg (pred): 0.423 +- 0.190
mrr vals (pred, true): 0.091, 0.004
batch losses (mrrl, rdl): 0.0166949891, 5.54748e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1006
rank avg (pred): 0.200 +- 0.124
mrr vals (pred, true): 0.144, 0.197
batch losses (mrrl, rdl): 0.0280176923, 0.0005075645

Epoch over!
epoch time: 12.18

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 844
rank avg (pred): 0.472 +- 0.129
mrr vals (pred, true): 0.056, 0.029
batch losses (mrrl, rdl): 0.0004174975, 0.0002693551

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 551
rank avg (pred): 0.374 +- 0.184
mrr vals (pred, true): 0.086, 0.045
batch losses (mrrl, rdl): 0.013157336, 0.0004868249

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 978
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.261, 0.319
batch losses (mrrl, rdl): 0.0339695364, 9.3367e-06

Epoch over!
epoch time: 12.484

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 568
rank avg (pred): 0.425 +- 0.185
mrr vals (pred, true): 0.095, 0.099
batch losses (mrrl, rdl): 0.0201119315, 0.0005268132

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 852
rank avg (pred): 0.444 +- 0.163
mrr vals (pred, true): 0.075, 0.039
batch losses (mrrl, rdl): 0.0064211003, 0.0001766232

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 730
rank avg (pred): 0.423 +- 0.182
mrr vals (pred, true): 0.096, 0.135
batch losses (mrrl, rdl): 0.0149110984, 0.0027606371

Epoch over!
epoch time: 12.379

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 605
rank avg (pred): 0.309 +- 0.166
mrr vals (pred, true): 0.095, 0.165
batch losses (mrrl, rdl): 0.0487773679, 0.0004521902

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1044
rank avg (pred): 0.207 +- 0.131
mrr vals (pred, true): 0.122, 0.003
batch losses (mrrl, rdl): 0.0523955561, 0.0014417563

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 866
rank avg (pred): 0.480 +- 0.118
mrr vals (pred, true): 0.046, 0.004
batch losses (mrrl, rdl): 0.0001731637, 7.1803e-05

Epoch over!
epoch time: 12.555

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.015 +- 0.013
mrr vals (pred, true): 0.269, 0.224

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.03812 	 0.00074 	 m..s
    0 	     1 	 0.02973 	 0.00075 	 ~...
    7 	     2 	 0.03850 	 0.00075 	 m..s
    4 	     3 	 0.03797 	 0.00075 	 m..s
    5 	     4 	 0.03798 	 0.00076 	 m..s
   14 	     5 	 0.05432 	 0.00121 	 m..s
   13 	     6 	 0.04843 	 0.00149 	 m..s
   12 	     7 	 0.04819 	 0.00173 	 m..s
   11 	     8 	 0.04736 	 0.00219 	 m..s
    2 	     9 	 0.02993 	 0.00256 	 ~...
   67 	    10 	 0.11985 	 0.00320 	 MISS
   17 	    11 	 0.05708 	 0.00324 	 m..s
   89 	    12 	 0.13205 	 0.00336 	 MISS
   53 	    13 	 0.10423 	 0.00357 	 MISS
   41 	    14 	 0.07908 	 0.00363 	 m..s
   43 	    15 	 0.07993 	 0.00372 	 m..s
   83 	    16 	 0.12816 	 0.00373 	 MISS
   22 	    17 	 0.06132 	 0.00373 	 m..s
   45 	    18 	 0.08467 	 0.00375 	 m..s
   33 	    19 	 0.07399 	 0.00376 	 m..s
    9 	    20 	 0.04204 	 0.00377 	 m..s
   72 	    21 	 0.12260 	 0.00379 	 MISS
   87 	    22 	 0.13130 	 0.00393 	 MISS
   78 	    23 	 0.12678 	 0.00394 	 MISS
   49 	    24 	 0.08799 	 0.00395 	 m..s
   68 	    25 	 0.12104 	 0.00397 	 MISS
    3 	    26 	 0.03078 	 0.00403 	 ~...
    1 	    27 	 0.02978 	 0.00405 	 ~...
   79 	    28 	 0.12685 	 0.00408 	 MISS
   16 	    29 	 0.05664 	 0.00409 	 m..s
   65 	    30 	 0.11927 	 0.00413 	 MISS
   69 	    31 	 0.12127 	 0.00417 	 MISS
    8 	    32 	 0.04179 	 0.00429 	 m..s
   60 	    33 	 0.11502 	 0.00432 	 MISS
   48 	    34 	 0.08589 	 0.00434 	 m..s
   15 	    35 	 0.05659 	 0.00438 	 m..s
   19 	    36 	 0.06060 	 0.00441 	 m..s
   54 	    37 	 0.10528 	 0.00451 	 MISS
   61 	    38 	 0.11589 	 0.00456 	 MISS
   18 	    39 	 0.06047 	 0.00460 	 m..s
   23 	    40 	 0.06442 	 0.00480 	 m..s
   46 	    41 	 0.08555 	 0.00481 	 m..s
   84 	    42 	 0.12886 	 0.00502 	 MISS
   64 	    43 	 0.11911 	 0.00531 	 MISS
   26 	    44 	 0.06944 	 0.00533 	 m..s
   10 	    45 	 0.04642 	 0.00576 	 m..s
   40 	    46 	 0.07903 	 0.00578 	 m..s
   58 	    47 	 0.11228 	 0.00639 	 MISS
   24 	    48 	 0.06456 	 0.01668 	 m..s
   39 	    49 	 0.07837 	 0.02871 	 m..s
   31 	    50 	 0.07286 	 0.03651 	 m..s
   30 	    51 	 0.07221 	 0.03829 	 m..s
   37 	    52 	 0.07751 	 0.03993 	 m..s
   29 	    53 	 0.07191 	 0.04945 	 ~...
   27 	    54 	 0.07051 	 0.05240 	 ~...
   28 	    55 	 0.07116 	 0.05615 	 ~...
   20 	    56 	 0.06064 	 0.05706 	 ~...
   21 	    57 	 0.06129 	 0.05984 	 ~...
   35 	    58 	 0.07556 	 0.09683 	 ~...
   32 	    59 	 0.07378 	 0.09715 	 ~...
   25 	    60 	 0.06538 	 0.11018 	 m..s
   50 	    61 	 0.08935 	 0.12580 	 m..s
   47 	    62 	 0.08573 	 0.13488 	 m..s
   34 	    63 	 0.07531 	 0.13839 	 m..s
   97 	    64 	 0.18628 	 0.14155 	 m..s
   51 	    65 	 0.09063 	 0.14888 	 m..s
   36 	    66 	 0.07738 	 0.15039 	 m..s
   42 	    67 	 0.07915 	 0.15166 	 m..s
   93 	    68 	 0.13634 	 0.15616 	 ~...
   38 	    69 	 0.07826 	 0.15954 	 m..s
   94 	    70 	 0.14611 	 0.16097 	 ~...
   44 	    71 	 0.08375 	 0.16331 	 m..s
   96 	    72 	 0.18288 	 0.16695 	 ~...
   95 	    73 	 0.17779 	 0.16739 	 ~...
   57 	    74 	 0.11153 	 0.17763 	 m..s
   56 	    75 	 0.11134 	 0.17926 	 m..s
   98 	    76 	 0.22187 	 0.18169 	 m..s
   66 	    77 	 0.11932 	 0.18668 	 m..s
   63 	    78 	 0.11838 	 0.18752 	 m..s
   75 	    79 	 0.12476 	 0.19107 	 m..s
   62 	    80 	 0.11799 	 0.19778 	 m..s
   75 	    81 	 0.12476 	 0.19892 	 m..s
   75 	    82 	 0.12476 	 0.20002 	 m..s
   55 	    83 	 0.11090 	 0.20033 	 m..s
   85 	    84 	 0.12917 	 0.20303 	 m..s
   52 	    85 	 0.10356 	 0.20384 	 MISS
   99 	    86 	 0.22541 	 0.20587 	 ~...
  100 	    87 	 0.22924 	 0.20874 	 ~...
   86 	    88 	 0.13001 	 0.20947 	 m..s
   59 	    89 	 0.11245 	 0.21153 	 m..s
   80 	    90 	 0.12699 	 0.21238 	 m..s
   82 	    91 	 0.12747 	 0.21240 	 m..s
   81 	    92 	 0.12706 	 0.21386 	 m..s
   70 	    93 	 0.12155 	 0.21539 	 m..s
  103 	    94 	 0.24969 	 0.21580 	 m..s
  101 	    95 	 0.23116 	 0.21615 	 ~...
  102 	    96 	 0.23338 	 0.21735 	 ~...
  105 	    97 	 0.25200 	 0.21787 	 m..s
  116 	    98 	 0.27714 	 0.21833 	 m..s
   88 	    99 	 0.13185 	 0.21860 	 m..s
  114 	   100 	 0.27349 	 0.22157 	 m..s
  109 	   101 	 0.26597 	 0.22183 	 m..s
  111 	   102 	 0.26859 	 0.22442 	 m..s
   91 	   103 	 0.13276 	 0.22649 	 m..s
  104 	   104 	 0.25035 	 0.22675 	 ~...
  113 	   105 	 0.27199 	 0.22780 	 m..s
  110 	   106 	 0.26840 	 0.22879 	 m..s
   71 	   107 	 0.12231 	 0.22910 	 MISS
  115 	   108 	 0.27405 	 0.23266 	 m..s
  108 	   109 	 0.26419 	 0.23504 	 ~...
  117 	   110 	 0.28073 	 0.23662 	 m..s
   74 	   111 	 0.12458 	 0.23677 	 MISS
   92 	   112 	 0.13505 	 0.23779 	 MISS
  112 	   113 	 0.27061 	 0.23940 	 m..s
  106 	   114 	 0.25815 	 0.24099 	 ~...
   90 	   115 	 0.13235 	 0.24180 	 MISS
   73 	   116 	 0.12441 	 0.24220 	 MISS
  107 	   117 	 0.26242 	 0.26971 	 ~...
  118 	   118 	 0.28298 	 0.29787 	 ~...
  119 	   119 	 0.28357 	 0.31564 	 m..s
  120 	   120 	 0.28697 	 0.32036 	 m..s
==========================================
r_mrr = 0.7394739985466003
r2_mrr = 0.5253316164016724
spearmanr_mrr@5 = 0.9558998346328735
spearmanr_mrr@10 = 0.9303591251373291
spearmanr_mrr@50 = 0.8001734018325806
spearmanr_mrr@100 = 0.8382343649864197
spearmanr_mrr@All = 0.875945508480072
==========================================
test time: 0.423
Done Testing dataset CoDExSmall
total time taken: 196.10037112236023
training time taken: 186.00933384895325
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7395)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.5253)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9559)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9304)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.8002)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8382)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.8759)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.644217964814743}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 602161709425367
Starting TWIG!
Loading datasets
Loading DistMult...
Loading CoDExSmall...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [583, 551, 866, 657, 259, 465, 659, 58, 320, 1064, 78, 598, 424, 143, 812, 427, 715, 743, 716, 119, 476, 1045, 1095, 1201, 250, 818, 964, 568, 1123, 532, 742, 218, 815, 1147, 305, 601, 655, 1207, 42, 709, 264, 967, 989, 245, 711, 787, 806, 908, 53, 690, 70, 483, 50, 983, 1039, 92, 595, 828, 564, 1113, 339, 641, 438, 98, 539, 674, 757, 304, 1055, 534, 834, 960, 96, 650, 1140, 21, 1132, 826, 451, 361, 1187, 199, 47, 45, 766, 25, 379, 443, 224, 1189, 410, 139, 938, 1071, 557, 87, 1003, 767, 689, 425, 499, 852, 280, 94, 240, 306, 229, 404, 769, 615, 809, 848, 315, 553, 393, 880, 286, 473, 55, 700, 981]
valid_ids (0): []
train_ids (1094): [118, 1198, 1067, 755, 801, 11, 990, 687, 204, 108, 731, 1088, 608, 719, 1162, 833, 955, 811, 399, 1195, 431, 249, 1041, 508, 1112, 61, 524, 63, 822, 105, 185, 799, 442, 581, 756, 279, 82, 153, 702, 906, 19, 737, 708, 1120, 631, 1202, 879, 114, 323, 459, 786, 537, 299, 441, 497, 391, 490, 471, 390, 696, 46, 800, 322, 588, 673, 248, 904, 40, 289, 1103, 751, 1186, 444, 661, 559, 946, 721, 781, 179, 995, 999, 850, 837, 470, 593, 883, 717, 857, 1065, 232, 398, 209, 831, 969, 1079, 182, 876, 1094, 243, 353, 916, 724, 985, 206, 1153, 710, 88, 293, 996, 734, 336, 356, 90, 918, 591, 177, 1155, 3, 522, 1044, 1058, 176, 513, 663, 518, 874, 448, 408, 738, 367, 622, 6, 872, 360, 932, 447, 630, 437, 1209, 359, 713, 378, 1139, 1200, 155, 491, 1030, 362, 963, 382, 258, 901, 197, 34, 1072, 211, 265, 685, 17, 1046, 327, 516, 162, 853, 397, 798, 373, 902, 1026, 191, 352, 1061, 1125, 1069, 794, 541, 183, 174, 1119, 457, 1012, 4, 529, 693, 222, 219, 54, 52, 129, 103, 914, 36, 939, 157, 1197, 954, 774, 619, 558, 1036, 648, 554, 260, 384, 125, 607, 1062, 924, 1097, 933, 749, 288, 194, 582, 1037, 504, 664, 763, 1118, 329, 682, 894, 579, 484, 808, 1019, 1089, 328, 500, 1051, 1056, 184, 779, 201, 796, 950, 928, 1060, 15, 546, 851, 10, 406, 86, 152, 235, 686, 489, 1034, 1165, 727, 220, 670, 349, 364, 1005, 308, 1134, 758, 32, 523, 1059, 1110, 1031, 1160, 333, 978, 71, 186, 895, 462, 810, 135, 1068, 44, 163, 775, 1048, 110, 403, 703, 1047, 1106, 945, 1188, 324, 345, 458, 449, 885, 549, 262, 1133, 621, 411, 600, 547, 846, 231, 120, 417, 1091, 440, 246, 91, 1029, 464, 318, 1050, 535, 344, 971, 1137, 509, 861, 421, 791, 991, 538, 841, 838, 1028, 637, 1111, 503, 639, 651, 966, 266, 886, 590, 980, 530, 1013, 839, 498, 836, 854, 375, 132, 1211, 432, 667, 1178, 772, 512, 175, 270, 370, 354, 587, 22, 131, 330, 691, 705, 455, 226, 863, 43, 620, 893, 1168, 698, 1146, 1022, 975, 221, 979, 962, 366, 272, 446, 627, 778, 435, 241, 493, 931, 433, 677, 1135, 783, 1169, 909, 23, 877, 217, 422, 884, 141, 927, 343, 1194, 819, 407, 1175, 169, 212, 575, 35, 80, 935, 1156, 301, 342, 987, 488, 254, 242, 227, 198, 905, 1126, 79, 126, 122, 26, 671, 142, 210, 511, 515, 656, 74, 452, 357, 507, 603, 552, 389, 436, 665, 792, 797, 445, 1163, 602, 430, 1070, 1117, 392, 236, 1042, 643, 1124, 773, 121, 947, 616, 612, 1021, 128, 744, 1171, 707, 666, 481, 675, 1170, 1205, 255, 423, 64, 247, 816, 316, 1084, 681, 84, 735, 982, 326, 171, 596, 1213, 1087, 574, 725, 31, 1075, 127, 1074, 771, 1199, 130, 144, 37, 67, 823, 560, 1172, 635, 1129, 624, 526, 626, 911, 412, 291, 814, 913, 992, 533, 146, 625, 542, 334, 202, 337, 269, 563, 18, 567, 521, 1001, 1007, 2, 594, 62, 986, 213, 369, 623, 1157, 561, 290, 654, 660, 109, 943, 907, 573, 386, 517, 672, 576, 496, 368, 81, 860, 495, 1014, 405, 428, 817, 388, 695, 889, 592, 73, 477, 387, 613, 303, 56, 479, 60, 844, 1142, 658, 494, 365, 68, 51, 180, 85, 520, 281, 33, 788, 1138, 562, 486, 414, 543, 694, 253, 99, 413, 1077, 793, 1174, 314, 920, 346, 688, 145, 380, 205, 1158, 1073, 977, 636, 865, 1090, 502, 418, 1109, 903, 728, 482, 284, 570, 394, 454, 113, 1141, 974, 456, 1104, 1011, 569, 917, 1009, 475, 385, 97, 1183, 891, 402, 873, 528, 878, 89, 138, 1144, 506, 765, 605, 460, 115, 803, 27, 732, 642, 300, 965, 921, 958, 1114, 325, 1150, 611, 919, 813, 168, 669, 1105, 736, 57, 1159, 701, 915, 1057, 1086, 739, 1085, 1035, 401, 332, 776, 167, 1016, 1099, 1180, 505, 759, 697, 790, 1098, 1025, 28, 453, 1167, 652, 887, 133, 572, 780, 159, 1015, 429, 720, 548, 383, 825, 203, 501, 287, 712, 173, 1116, 519, 948, 647, 148, 514, 937, 868, 59, 376, 550, 770, 24, 1093, 1152, 1040, 396, 644, 633, 589, 942, 347, 117, 225, 922, 750, 1204, 678, 555, 485, 72, 899, 843, 39, 892, 76, 820, 1032, 338, 492, 350, 1149, 156, 953, 525, 718, 1179, 976, 381, 1143, 1006, 251, 733, 1131, 363, 1184, 341, 1190, 706, 134, 295, 1148, 30, 944, 746, 881, 545, 1206, 832, 102, 261, 1101, 752, 745, 1128, 1010, 923, 890, 420, 930, 1121, 150, 480, 164, 450, 586, 859, 1063, 302, 351, 1177, 298, 1161, 1027, 1182, 1164, 1130, 956, 910, 882, 319, 434, 870, 994, 1052, 277, 740, 649, 556, 107, 310, 1033, 730, 584, 181, 867, 187, 668, 1078, 604, 292, 172, 154, 215, 1049, 1108, 14, 845, 638, 577, 871, 760, 234, 925, 566, 158, 69, 609, 565, 223, 439, 1081, 1193, 311, 415, 1127, 645, 161, 170, 1122, 748, 864, 137, 101, 926, 862, 340, 1203, 267, 123, 487, 1176, 20, 952, 1004, 377, 274, 789, 296, 200, 355, 228, 140, 936, 683, 540, 847, 1192, 371, 961, 1000, 722, 723, 805, 75, 188, 151, 1020, 729, 1023, 988, 400, 632, 165, 273, 951, 1173, 472, 940, 634, 41, 1100, 777, 824, 466, 1080, 941, 1038, 1, 684, 900, 1181, 680, 48, 196, 1136, 348, 1066, 16, 1151, 1107, 256, 840, 510, 761, 571, 29, 998, 929, 136, 317, 1008, 233, 959, 312, 9, 409, 536, 762, 704, 276, 544, 372, 830, 193, 469, 106, 416, 527, 160, 331, 244, 842, 973, 897, 1102, 802, 753, 395, 124, 83, 214, 309, 934, 849, 238, 285, 461, 804, 997, 888, 100, 699, 679, 912, 653, 147, 896, 640, 1196, 112, 77, 149, 467, 252, 856, 785, 104, 617, 13, 297, 1092, 321, 95, 768, 578, 764, 307, 374, 629, 463, 784, 949, 257, 875, 858, 1096, 178, 898, 66, 335, 271, 65, 782, 662, 599, 192, 8, 754, 474, 237, 468, 827, 1017, 957, 294, 618, 313, 1191, 1212, 426, 968, 1214, 195, 597, 49, 606, 676, 610, 1210, 116, 1208, 614, 646, 263, 239, 970, 726, 747, 478, 1145, 283, 984, 628, 12, 580, 216, 190, 38, 692, 275, 714, 585, 1002, 7, 1185, 207, 972, 278, 93, 268, 111, 855, 807, 531, 1115, 835, 1018, 1166, 0, 1054, 1082, 5, 230, 189, 829, 419, 869, 166, 741, 1024, 1154, 795, 282, 1043, 208, 1076, 358, 993, 1083, 1053, 821]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  599702261446070
the save name prefix for this run is:  chkpt-ID_599702261446070_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'DistMult': {'CoDExSmall': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1065
rank avg (pred): 0.514 +- 0.005
mrr vals (pred, true): 0.001, 0.257
batch losses (mrrl, rdl): 0.0, 0.0047771386

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 560
rank avg (pred): 0.139 +- 0.059
mrr vals (pred, true): 0.023, 0.039
batch losses (mrrl, rdl): 0.0, 0.0002508144

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 587
rank avg (pred): 0.341 +- 0.213
mrr vals (pred, true): 0.147, 0.104
batch losses (mrrl, rdl): 0.0, 0.0002145169

Epoch over!
epoch time: 12.525

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1016
rank avg (pred): 0.301 +- 0.196
mrr vals (pred, true): 0.169, 0.219
batch losses (mrrl, rdl): 0.0, 0.0014630986

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 153
rank avg (pred): 0.245 +- 0.186
mrr vals (pred, true): 0.251, 0.232
batch losses (mrrl, rdl): 0.0, 0.0009459488

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 956
rank avg (pred): 0.379 +- 0.294
mrr vals (pred, true): 0.275, 0.004
batch losses (mrrl, rdl): 0.0, 8.10273e-05

Epoch over!
epoch time: 12.407

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 530
rank avg (pred): 0.200 +- 0.156
mrr vals (pred, true): 0.278, 0.038
batch losses (mrrl, rdl): 0.0, 2.7809e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 415
rank avg (pred): 0.256 +- 0.209
mrr vals (pred, true): 0.327, 0.004
batch losses (mrrl, rdl): 0.0, 0.0007759507

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 242
rank avg (pred): 0.224 +- 0.195
mrr vals (pred, true): 0.353, 0.003
batch losses (mrrl, rdl): 0.0, 0.0011334055

Epoch over!
epoch time: 12.539

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 371
rank avg (pred): 0.215 +- 0.187
mrr vals (pred, true): 0.353, 0.198
batch losses (mrrl, rdl): 0.0, 0.0006397408

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 210
rank avg (pred): 0.252 +- 0.208
mrr vals (pred, true): 0.338, 0.004
batch losses (mrrl, rdl): 0.0, 0.0008352678

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 336
rank avg (pred): 0.312 +- 0.258
mrr vals (pred, true): 0.343, 0.229
batch losses (mrrl, rdl): 0.0, 0.0016592584

Epoch over!
epoch time: 12.133

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 209
rank avg (pred): 0.227 +- 0.197
mrr vals (pred, true): 0.377, 0.004
batch losses (mrrl, rdl): 0.0, 0.0010819737

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1202
rank avg (pred): 0.280 +- 0.239
mrr vals (pred, true): 0.371, 0.004
batch losses (mrrl, rdl): 0.0, 0.0005410383

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 611
rank avg (pred): 0.322 +- 0.271
mrr vals (pred, true): 0.364, 0.167
batch losses (mrrl, rdl): 0.0, 0.0006718239

Epoch over!
epoch time: 12.19

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 477
rank avg (pred): 0.260 +- 0.215
mrr vals (pred, true): 0.337, 0.004
batch losses (mrrl, rdl): 0.8251118064, 0.0008434373

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 750
rank avg (pred): 0.448 +- 0.434
mrr vals (pred, true): 0.119, 0.154
batch losses (mrrl, rdl): 0.0116178961, 0.0025243852

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 630
rank avg (pred): 0.552 +- 0.383
mrr vals (pred, true): 0.094, 0.131
batch losses (mrrl, rdl): 0.0136662042, 0.0021808611

Epoch over!
epoch time: 12.671

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1170
rank avg (pred): 0.661 +- 0.347
mrr vals (pred, true): 0.080, 0.149
batch losses (mrrl, rdl): 0.0484361798, 0.0040683374

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 693
rank avg (pred): 0.533 +- 0.385
mrr vals (pred, true): 0.077, 0.004
batch losses (mrrl, rdl): 0.0075131282, 7.46553e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 882
rank avg (pred): 0.645 +- 0.340
mrr vals (pred, true): 0.060, 0.004
batch losses (mrrl, rdl): 0.0009631609, 0.0004862907

Epoch over!
epoch time: 12.12

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 914
rank avg (pred): 0.704 +- 0.326
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.35941e-05, 0.0001698131

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 309
rank avg (pred): 0.323 +- 0.462
mrr vals (pred, true): 0.231, 0.259
batch losses (mrrl, rdl): 0.007826386, 0.0015501037

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1041
rank avg (pred): 0.359 +- 0.449
mrr vals (pred, true): 0.141, 0.004
batch losses (mrrl, rdl): 0.0831953138, 0.0005406529

Epoch over!
epoch time: 12.654

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 160
rank avg (pred): 0.402 +- 0.421
mrr vals (pred, true): 0.129, 0.226
batch losses (mrrl, rdl): 0.0935039669, 0.0020124707

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 705
rank avg (pred): 0.477 +- 0.369
mrr vals (pred, true): 0.063, 0.004
batch losses (mrrl, rdl): 0.0017625275, 6.12803e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 895
rank avg (pred): 0.519 +- 0.350
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.97129e-05, 0.0002496578

Epoch over!
epoch time: 11.925

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 273
rank avg (pred): 0.270 +- 0.435
mrr vals (pred, true): 0.192, 0.227
batch losses (mrrl, rdl): 0.0119692003, 0.0010324667

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 796
rank avg (pred): 0.612 +- 0.315
mrr vals (pred, true): 0.037, 0.005
batch losses (mrrl, rdl): 0.0018216365, 0.0003298571

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 897
rank avg (pred): 0.475 +- 0.350
mrr vals (pred, true): 0.072, 0.001
batch losses (mrrl, rdl): 0.0046329997, 0.0009920079

Epoch over!
epoch time: 11.989

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1028
rank avg (pred): 0.326 +- 0.402
mrr vals (pred, true): 0.128, 0.004
batch losses (mrrl, rdl): 0.0614200234, 0.0006731555

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 718
rank avg (pred): 0.398 +- 0.339
mrr vals (pred, true): 0.081, 0.006
batch losses (mrrl, rdl): 0.0098601468, 0.000155806

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 354
rank avg (pred): 0.229 +- 0.303
mrr vals (pred, true): 0.149, 0.237
batch losses (mrrl, rdl): 0.0777327195, 0.0005383991

Epoch over!
epoch time: 12.114

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 604
rank avg (pred): 0.376 +- 0.313
mrr vals (pred, true): 0.111, 0.169
batch losses (mrrl, rdl): 0.0334073603, 0.0007483517

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 327
rank avg (pred): 0.263 +- 0.261
mrr vals (pred, true): 0.126, 0.216
batch losses (mrrl, rdl): 0.0797003433, 0.0008166184

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 1094
rank avg (pred): 0.230 +- 0.282
mrr vals (pred, true): 0.137, 0.203
batch losses (mrrl, rdl): 0.042416174, 0.0006456929

Epoch over!
epoch time: 11.891

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 915
rank avg (pred): 0.720 +- 0.305
mrr vals (pred, true): 0.033, 0.001
batch losses (mrrl, rdl): 0.0030234633, 0.0002747811

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 204
rank avg (pred): 0.286 +- 0.294
mrr vals (pred, true): 0.127, 0.004
batch losses (mrrl, rdl): 0.0599878505, 0.0006823083

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 509
rank avg (pred): 0.123 +- 0.168
mrr vals (pred, true): 0.140, 0.178
batch losses (mrrl, rdl): 0.013902979, 1.07614e-05

Epoch over!
epoch time: 12.009

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 121
rank avg (pred): 0.268 +- 0.300
mrr vals (pred, true): 0.133, 0.220
batch losses (mrrl, rdl): 0.0760072172, 0.0009554542

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 833
rank avg (pred): 0.124 +- 0.182
mrr vals (pred, true): 0.151, 0.226
batch losses (mrrl, rdl): 0.0568962507, 0.0001452706

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 344
rank avg (pred): 0.279 +- 0.309
mrr vals (pred, true): 0.140, 0.181
batch losses (mrrl, rdl): 0.0168703366, 0.0009268713

Epoch over!
epoch time: 12.137

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 480
rank avg (pred): 0.318 +- 0.321
mrr vals (pred, true): 0.122, 0.004
batch losses (mrrl, rdl): 0.0519932732, 0.0005376118

running batch: 500 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 30
rank avg (pred): 0.068 +- 0.117
mrr vals (pred, true): 0.227, 0.196
batch losses (mrrl, rdl): 0.0096711945, 1.47889e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, CoDExSmall, run 2.1, exp 962
rank avg (pred): 0.542 +- 0.298
mrr vals (pred, true): 0.062, 0.004
batch losses (mrrl, rdl): 0.0014498871, 4.61246e-05

Epoch over!
epoch time: 12.079

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG CoDExSmall
Running eval on the test set
running batch: 0
rank avg (pred): 0.388 +- 0.286
mrr vals (pred, true): 0.094, 0.129

Evaluation for CoDExSmall on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.05962 	 0.00076 	 m..s
    0 	     1 	 0.03923 	 0.00094 	 m..s
   62 	     2 	 0.12304 	 0.00326 	 MISS
   36 	     3 	 0.09549 	 0.00330 	 m..s
   42 	     4 	 0.09628 	 0.00333 	 m..s
   60 	     5 	 0.11896 	 0.00342 	 MISS
   70 	     6 	 0.13023 	 0.00355 	 MISS
   39 	     7 	 0.09561 	 0.00356 	 m..s
   27 	     8 	 0.09338 	 0.00369 	 m..s
   71 	     9 	 0.13032 	 0.00375 	 MISS
    1 	    10 	 0.05774 	 0.00385 	 m..s
   45 	    11 	 0.09962 	 0.00385 	 m..s
    1 	    12 	 0.05774 	 0.00390 	 m..s
    1 	    13 	 0.05774 	 0.00403 	 m..s
   58 	    14 	 0.11767 	 0.00408 	 MISS
   12 	    15 	 0.06255 	 0.00409 	 m..s
   69 	    16 	 0.12914 	 0.00412 	 MISS
   59 	    17 	 0.11844 	 0.00421 	 MISS
   51 	    18 	 0.10954 	 0.00423 	 MISS
   56 	    19 	 0.11685 	 0.00424 	 MISS
    1 	    20 	 0.05774 	 0.00427 	 m..s
   44 	    21 	 0.09694 	 0.00427 	 m..s
   66 	    22 	 0.12713 	 0.00430 	 MISS
   85 	    23 	 0.14023 	 0.00432 	 MISS
   40 	    24 	 0.09562 	 0.00434 	 m..s
   33 	    25 	 0.09510 	 0.00438 	 m..s
   68 	    26 	 0.12818 	 0.00442 	 MISS
   64 	    27 	 0.12670 	 0.00442 	 MISS
   78 	    28 	 0.13512 	 0.00442 	 MISS
   24 	    29 	 0.08345 	 0.00445 	 m..s
   54 	    30 	 0.11238 	 0.00448 	 MISS
   32 	    31 	 0.09508 	 0.00451 	 m..s
   77 	    32 	 0.13322 	 0.00463 	 MISS
   17 	    33 	 0.06647 	 0.00467 	 m..s
   80 	    34 	 0.13802 	 0.00471 	 MISS
   63 	    35 	 0.12591 	 0.00478 	 MISS
   81 	    36 	 0.13827 	 0.00483 	 MISS
   43 	    37 	 0.09672 	 0.00485 	 m..s
    1 	    38 	 0.05774 	 0.00489 	 m..s
   14 	    39 	 0.06276 	 0.00498 	 m..s
    1 	    40 	 0.05774 	 0.00505 	 m..s
   25 	    41 	 0.08614 	 0.00520 	 m..s
   31 	    42 	 0.09507 	 0.00524 	 m..s
   65 	    43 	 0.12705 	 0.00531 	 MISS
   38 	    44 	 0.09553 	 0.00543 	 m..s
   72 	    45 	 0.13057 	 0.00637 	 MISS
   29 	    46 	 0.09367 	 0.00682 	 m..s
   20 	    47 	 0.06920 	 0.03082 	 m..s
   86 	    48 	 0.14636 	 0.03246 	 MISS
   18 	    49 	 0.06761 	 0.03343 	 m..s
   22 	    50 	 0.07063 	 0.03727 	 m..s
    8 	    51 	 0.06070 	 0.03792 	 ~...
   10 	    52 	 0.06209 	 0.03850 	 ~...
   21 	    53 	 0.07000 	 0.03909 	 m..s
   19 	    54 	 0.06845 	 0.04242 	 ~...
   11 	    55 	 0.06210 	 0.04276 	 ~...
   16 	    56 	 0.06402 	 0.04484 	 ~...
   23 	    57 	 0.08088 	 0.04846 	 m..s
   15 	    58 	 0.06306 	 0.05706 	 ~...
   84 	    59 	 0.14020 	 0.05801 	 m..s
    9 	    60 	 0.06206 	 0.07497 	 ~...
   13 	    61 	 0.06273 	 0.07522 	 ~...
   26 	    62 	 0.08843 	 0.09914 	 ~...
   83 	    63 	 0.13979 	 0.12580 	 ~...
   28 	    64 	 0.09355 	 0.12878 	 m..s
   30 	    65 	 0.09404 	 0.13439 	 m..s
   41 	    66 	 0.09563 	 0.13805 	 m..s
   34 	    67 	 0.09523 	 0.14601 	 m..s
   90 	    68 	 0.17397 	 0.14733 	 ~...
   46 	    69 	 0.10066 	 0.15118 	 m..s
   79 	    70 	 0.13771 	 0.15719 	 ~...
   37 	    71 	 0.09549 	 0.15822 	 m..s
   35 	    72 	 0.09529 	 0.16203 	 m..s
   88 	    73 	 0.15716 	 0.16268 	 ~...
   87 	    74 	 0.15528 	 0.16704 	 ~...
   49 	    75 	 0.10898 	 0.17849 	 m..s
   96 	    76 	 0.22235 	 0.17869 	 m..s
   95 	    77 	 0.21960 	 0.18529 	 m..s
   67 	    78 	 0.12809 	 0.19390 	 m..s
   47 	    79 	 0.10424 	 0.19576 	 m..s
   55 	    80 	 0.11337 	 0.20002 	 m..s
   53 	    81 	 0.11096 	 0.20155 	 m..s
   52 	    82 	 0.10987 	 0.20179 	 m..s
   57 	    83 	 0.11701 	 0.20235 	 m..s
   75 	    84 	 0.13214 	 0.20390 	 m..s
  101 	    85 	 0.23811 	 0.20414 	 m..s
  108 	    86 	 0.25098 	 0.20702 	 m..s
   93 	    87 	 0.21435 	 0.20874 	 ~...
   82 	    88 	 0.13924 	 0.20947 	 m..s
  107 	    89 	 0.25062 	 0.20951 	 m..s
   48 	    90 	 0.10806 	 0.21117 	 MISS
   94 	    91 	 0.21839 	 0.21259 	 ~...
  105 	    92 	 0.24676 	 0.21434 	 m..s
  116 	    93 	 0.27852 	 0.21487 	 m..s
   89 	    94 	 0.16962 	 0.21584 	 m..s
   74 	    95 	 0.13208 	 0.21878 	 m..s
   98 	    96 	 0.22881 	 0.21933 	 ~...
  111 	    97 	 0.26012 	 0.22059 	 m..s
  113 	    98 	 0.26445 	 0.22067 	 m..s
  106 	    99 	 0.24766 	 0.22122 	 ~...
  103 	   100 	 0.24373 	 0.22157 	 ~...
  102 	   101 	 0.23960 	 0.22232 	 ~...
   76 	   102 	 0.13235 	 0.22411 	 m..s
   50 	   103 	 0.10905 	 0.22501 	 MISS
   99 	   104 	 0.22915 	 0.22675 	 ~...
   97 	   105 	 0.22524 	 0.22955 	 ~...
  110 	   106 	 0.25913 	 0.23088 	 ~...
   61 	   107 	 0.12188 	 0.23224 	 MISS
   92 	   108 	 0.17874 	 0.23319 	 m..s
  100 	   109 	 0.23577 	 0.23373 	 ~...
  109 	   110 	 0.25402 	 0.23893 	 ~...
   73 	   111 	 0.13125 	 0.24284 	 MISS
  104 	   112 	 0.24552 	 0.24792 	 ~...
  119 	   113 	 0.29823 	 0.25028 	 m..s
  115 	   114 	 0.27216 	 0.25300 	 ~...
   91 	   115 	 0.17752 	 0.26008 	 m..s
  114 	   116 	 0.27056 	 0.26518 	 ~...
  112 	   117 	 0.26294 	 0.26971 	 ~...
  118 	   118 	 0.29740 	 0.28511 	 ~...
  120 	   119 	 0.30064 	 0.28609 	 ~...
  117 	   120 	 0.28040 	 0.30182 	 ~...
==========================================
r_mrr = 0.7421225905418396
r2_mrr = 0.4583629369735718
spearmanr_mrr@5 = 0.9334588050842285
spearmanr_mrr@10 = 0.9808396697044373
spearmanr_mrr@50 = 0.9054985642433167
spearmanr_mrr@100 = 0.8736151456832886
spearmanr_mrr@All = 0.9028862118721008
==========================================
test time: 0.444
Done Testing dataset CoDExSmall
total time taken: 192.8672080039978
training time taken: 183.88710951805115
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'CoDExSmall': tensor(0.7421)}}, 'r2_mrr': {'DistMult': {'CoDExSmall': tensor(0.4584)}}, 'spearmanr_mrr@5': {'DistMult': {'CoDExSmall': tensor(0.9335)}}, 'spearmanr_mrr@10': {'DistMult': {'CoDExSmall': tensor(0.9808)}}, 'spearmanr_mrr@50': {'DistMult': {'CoDExSmall': tensor(0.9055)}}, 'spearmanr_mrr@100': {'DistMult': {'CoDExSmall': tensor(0.8736)}}, 'spearmanr_mrr@All': {'DistMult': {'CoDExSmall': tensor(0.9029)}}, 'test_loss': {'DistMult': {'CoDExSmall': 3.6091025935311336}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 8004585941334529
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [269, 920, 1178, 370, 129, 596, 853, 957, 908, 422, 861, 156, 754, 702, 891, 55, 765, 731, 209, 670, 514, 332, 1128, 1018, 1043, 941, 955, 397, 1202, 16, 516, 163, 432, 1076, 1097, 838, 198, 474, 986, 544, 638, 415, 231, 818, 686, 36, 712, 895, 915, 41, 546, 182, 1013, 61, 216, 305, 453, 962, 880, 535, 19, 537, 420, 43, 777, 902, 242, 1160, 1056, 600, 555, 884, 82, 945, 229, 1195, 877, 1136, 1015, 191, 612, 1085, 899, 931, 22, 873, 463, 746, 718, 570, 1193, 1102, 926, 1000, 1208, 339, 306, 490, 890, 134, 609, 121, 855, 476, 414, 888, 970, 426, 495, 572, 462, 882, 693, 1164, 952, 905, 684, 1007, 510, 1016, 930]
valid_ids (0): []
train_ids (1094): [1155, 131, 714, 312, 831, 192, 646, 508, 97, 1059, 918, 656, 440, 213, 876, 1023, 579, 985, 742, 423, 90, 30, 141, 640, 214, 975, 906, 469, 536, 1072, 1051, 849, 342, 1074, 1024, 889, 762, 1166, 732, 886, 629, 1048, 867, 10, 841, 392, 524, 266, 408, 78, 1009, 525, 825, 735, 438, 3, 999, 369, 815, 429, 784, 1040, 206, 433, 1036, 1100, 786, 11, 268, 1163, 1006, 154, 1148, 947, 284, 446, 245, 195, 382, 690, 388, 1168, 790, 792, 959, 736, 310, 1189, 549, 107, 625, 256, 393, 218, 193, 1027, 1196, 152, 262, 692, 604, 79, 713, 897, 738, 817, 733, 44, 40, 145, 324, 1144, 487, 606, 630, 522, 329, 892, 1068, 402, 724, 1146, 361, 1050, 334, 641, 1135, 364, 404, 1127, 751, 1037, 345, 101, 1021, 359, 576, 375, 607, 940, 354, 343, 139, 326, 367, 628, 937, 740, 111, 563, 1083, 114, 605, 1025, 688, 834, 568, 1080, 37, 898, 548, 1062, 1124, 616, 936, 878, 1120, 559, 452, 865, 956, 1, 710, 466, 1147, 648, 62, 618, 248, 827, 530, 990, 587, 1042, 309, 781, 1153, 128, 125, 564, 323, 390, 864, 919, 223, 34, 622, 1125, 814, 527, 807, 396, 140, 308, 866, 743, 602, 1031, 627, 1150, 1105, 672, 1093, 31, 1030, 820, 974, 418, 1142, 319, 796, 639, 1075, 830, 448, 1098, 939, 357, 562, 874, 1038, 389, 798, 922, 431, 540, 1212, 595, 86, 964, 542, 836, 811, 144, 589, 489, 557, 178, 511, 1179, 179, 727, 1129, 270, 954, 381, 938, 1122, 1114, 21, 1033, 806, 913, 1101, 185, 1073, 38, 430, 943, 519, 997, 721, 681, 631, 149, 437, 25, 227, 1133, 633, 824, 552, 881, 745, 483, 278, 6, 518, 108, 917, 1069, 482, 935, 445, 887, 528, 1061, 328, 60, 1077, 346, 497, 53, 135, 585, 934, 24, 479, 93, 1206, 984, 809, 883, 805, 680, 23, 5, 74, 427, 653, 228, 117, 51, 142, 478, 1175, 603, 837, 744, 9, 520, 289, 584, 54, 950, 571, 1032, 502, 973, 812, 789, 987, 443, 250, 33, 333, 835, 484, 927, 683, 772, 671, 200, 813, 492, 455, 1191, 706, 281, 729, 1003, 1165, 1065, 588, 283, 451, 1194, 1019, 1063, 803, 1113, 301, 726, 1204, 1055, 391, 59, 398, 267, 66, 533, 1103, 1197, 980, 608, 184, 907, 504, 948, 8, 1167, 1177, 791, 863, 362, 1078, 105, 903, 532, 788, 459, 292, 719, 251, 254, 293, 313, 1188, 802, 447, 635, 48, 481, 197, 655, 493, 660, 138, 341, 435, 320, 1034, 816, 57, 1108, 946, 350, 444, 285, 264, 158, 4, 159, 127, 541, 750, 967, 795, 371, 1205, 661, 1054, 110, 1190, 663, 132, 794, 632, 299, 1119, 253, 1200, 598, 868, 870, 993, 103, 473, 260, 160, 871, 842, 348, 1207, 989, 1181, 860, 416, 1180, 146, 783, 207, 829, 92, 747, 1084, 1210, 316, 166, 238, 1141, 574, 573, 409, 439, 210, 172, 924, 850, 591, 360, 475, 983, 569, 679, 1159, 928, 691, 302, 244, 896, 39, 538, 387, 566, 933, 1086, 912, 1071, 50, 304, 471, 161, 225, 311, 1096, 28, 190, 774, 116, 1092, 958, 164, 804, 95, 1070, 384, 336, 349, 383, 196, 626, 1186, 1154, 965, 461, 668, 64, 425, 77, 441, 823, 1058, 1104, 1214, 1213, 273, 534, 249, 279, 467, 45, 623, 219, 1192, 212, 720, 509, 1156, 748, 711, 406, 529, 979, 599, 852, 186, 872, 65, 1052, 428, 303, 122, 233, 1047, 1049, 230, 1008, 2, 1045, 317, 69, 457, 846, 488, 581, 1044, 1118, 298, 554, 124, 1134, 780, 717, 403, 331, 526, 921, 998, 202, 366, 1064, 769, 689, 1028, 910, 417, 685, 1137, 314, 464, 63, 297, 373, 75, 468, 1017, 173, 1089, 201, 372, 84, 1183, 87, 793, 1111, 81, 88, 678, 203, 971, 358, 755, 1138, 844, 330, 862, 1149, 171, 12, 1046, 505, 470, 47, 649, 737, 322, 465, 344, 904, 699, 376, 1117, 664, 168, 1131, 300, 176, 676, 611, 261, 411, 634, 153, 620, 76, 338, 85, 335, 966, 756, 222, 923, 286, 687, 725, 91, 352, 123, 673, 1012, 723, 550, 162, 1173, 799, 294, 593, 766, 205, 32, 1095, 1123, 840, 1203, 137, 610, 96, 650, 424, 378, 759, 401, 458, 234, 1187, 617, 102, 258, 377, 199, 115, 106, 925, 1109, 421, 1087, 1132, 1022, 208, 543, 592, 407, 480, 578, 991, 703, 645, 845, 859, 951, 94, 953, 911, 771, 356, 259, 399, 708, 477, 1116, 18, 636, 287, 1011, 1157, 739, 49, 1209, 553, 436, 130, 705, 819, 120, 1088, 558, 147, 1005, 169, 450, 151, 575, 113, 290, 1039, 778, 68, 288, 52, 263, 136, 275, 988, 1161, 770, 642, 1170, 696, 669, 856, 992, 561, 916, 583, 843, 1020, 385, 296, 380, 355, 647, 280, 1029, 351, 909, 282, 615, 722, 826, 315, 704, 507, 1099, 100, 460, 157, 978, 1106, 551, 839, 942, 1162, 80, 1041, 494, 307, 758, 1139, 71, 851, 15, 1145, 99, 1198, 601, 665, 1185, 728, 614, 810, 491, 741, 104, 694, 832, 1094, 594, 961, 565, 547, 662, 1090, 701, 26, 506, 1110, 700, 539, 972, 58, 217, 215, 67, 963, 885, 1014, 73, 1010, 340, 854, 624, 72, 667, 1152, 1171, 255, 347, 20, 1001, 666, 109, 70, 652, 764, 545, 513, 501, 413, 272, 1201, 211, 220, 232, 112, 932, 1057, 325, 643, 734, 808, 675, 879, 126, 98, 1199, 847, 405, 17, 695, 1053, 613, 83, 189, 637, 243, 386, 857, 150, 257, 621, 175, 716, 247, 496, 801, 177, 252, 1174, 1067, 1081, 761, 419, 658, 833, 27, 1107, 619, 914, 353, 828, 265, 194, 119, 749, 400, 800, 763, 240, 760, 869, 118, 1112, 976, 374, 982, 1182, 1115, 379, 531, 1176, 35, 821, 485, 944, 368, 13, 773, 89, 994, 960, 929, 295, 521, 556, 410, 500, 170, 567, 224, 752, 597, 442, 768, 682, 221, 365, 456, 776, 454, 472, 29, 582, 486, 499, 143, 204, 0, 894, 580, 654, 1151, 1184, 241, 321, 644, 785, 165, 981, 188, 657, 291, 767, 1121, 274, 226, 996, 7, 1026, 515, 995, 1169, 715, 858, 174, 394, 1140, 46, 586, 498, 56, 1035, 1158, 1082, 1079, 523, 14, 782, 787, 412, 677, 434, 797, 674, 512, 1130, 757, 42, 730, 155, 181, 183, 698, 327, 277, 1143, 276, 246, 1004, 1002, 337, 318, 822, 167, 949, 239, 659, 1126, 893, 503, 1172, 449, 1211, 1066, 709, 697, 779, 590, 363, 235, 1060, 395, 707, 577, 517, 753, 133, 901, 187, 271, 969, 900, 848, 1091, 236, 968, 775, 875, 560, 180, 237, 148, 977, 651]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9006216755560088
the save name prefix for this run is:  chkpt-ID_9006216755560088_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 85
rank avg (pred): 0.588 +- 0.002
mrr vals (pred, true): 0.000, 0.143
batch losses (mrrl, rdl): 0.0, 0.0018317228

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.297 +- 0.230
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0, 0.000724193

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 779
rank avg (pred): 0.403 +- 0.316
mrr vals (pred, true): 0.086, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005068019

Epoch over!
epoch time: 11.889

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.384 +- 0.300
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001642379

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.375 +- 0.291
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0, 4.36313e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 533
rank avg (pred): 0.261 +- 0.206
mrr vals (pred, true): 0.107, 0.208
batch losses (mrrl, rdl): 0.0, 6.94598e-05

Epoch over!
epoch time: 11.768

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 146
rank avg (pred): 0.373 +- 0.288
mrr vals (pred, true): 0.077, 0.166
batch losses (mrrl, rdl): 0.0, 0.0001378978

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 447
rank avg (pred): 0.407 +- 0.294
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0, 9.79741e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 857
rank avg (pred): 0.375 +- 0.296
mrr vals (pred, true): 0.106, 0.031
batch losses (mrrl, rdl): 0.0, 8.7015e-06

Epoch over!
epoch time: 11.83

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.374 +- 0.296
mrr vals (pred, true): 0.108, 0.103
batch losses (mrrl, rdl): 0.0, 2.95584e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.368 +- 0.293
mrr vals (pred, true): 0.139, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002074331

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.386 +- 0.303
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0, 4.75027e-05

Epoch over!
epoch time: 11.729

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 433
rank avg (pred): 0.372 +- 0.295
mrr vals (pred, true): 0.115, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001468577

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.424 +- 0.333
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0, 5.38973e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 934
rank avg (pred): 0.439 +- 0.350
mrr vals (pred, true): 0.153, 0.000
batch losses (mrrl, rdl): 0.0, 0.0032017685

Epoch over!
epoch time: 12.342

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.402 +- 0.317
mrr vals (pred, true): 0.106, 0.001
batch losses (mrrl, rdl): 0.0315930769, 9.47468e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 553
rank avg (pred): 0.124 +- 0.188
mrr vals (pred, true): 0.182, 0.198
batch losses (mrrl, rdl): 0.0024994756, 0.000265118

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.409 +- 0.268
mrr vals (pred, true): 0.091, 0.003
batch losses (mrrl, rdl): 0.0171100106, 3.94849e-05

Epoch over!
epoch time: 12.149

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.553 +- 0.301
mrr vals (pred, true): 0.078, 0.001
batch losses (mrrl, rdl): 0.0079567097, 0.0001466997

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 738
rank avg (pred): 0.049 +- 0.056
mrr vals (pred, true): 0.223, 0.107
batch losses (mrrl, rdl): 0.1338853538, 0.000432352

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.500 +- 0.276
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.014197126, 8.2185e-06

Epoch over!
epoch time: 12.129

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 841
rank avg (pred): 0.571 +- 0.367
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0368793197, 0.00012989

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.479 +- 0.271
mrr vals (pred, true): 0.091, 0.146
batch losses (mrrl, rdl): 0.0299426131, 0.000679872

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 590
rank avg (pred): 0.499 +- 0.259
mrr vals (pred, true): 0.085, 0.159
batch losses (mrrl, rdl): 0.0551303253, 0.0006432069

Epoch over!
epoch time: 12.126

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 263
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.409, 0.372
batch losses (mrrl, rdl): 0.0135803493, 0.0003075627

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.453 +- 0.264
mrr vals (pred, true): 0.102, 0.138
batch losses (mrrl, rdl): 0.0132385138, 0.0002825941

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1113
rank avg (pred): 0.564 +- 0.328
mrr vals (pred, true): 0.091, 0.000
batch losses (mrrl, rdl): 0.0169253275, 0.000171158

Epoch over!
epoch time: 12.128

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1107
rank avg (pred): 0.563 +- 0.315
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0140696624, 0.0001213841

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.462 +- 0.246
mrr vals (pred, true): 0.089, 0.144
batch losses (mrrl, rdl): 0.0302998815, 0.0006614084

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 87
rank avg (pred): 0.525 +- 0.277
mrr vals (pred, true): 0.087, 0.156
batch losses (mrrl, rdl): 0.0467817709, 0.001071775

Epoch over!
epoch time: 13.078

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 792
rank avg (pred): 0.498 +- 0.251
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.0139290914, 1.38136e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.386, 0.349
batch losses (mrrl, rdl): 0.0141659947, 0.0004308711

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 542
rank avg (pred): 0.259 +- 0.194
mrr vals (pred, true): 0.195, 0.187
batch losses (mrrl, rdl): 0.0007694695, 7.23414e-05

Epoch over!
epoch time: 12.664

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.495 +- 0.243
mrr vals (pred, true): 0.087, 0.146
batch losses (mrrl, rdl): 0.0351488627, 0.0002982741

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.065 +- 0.078
mrr vals (pred, true): 0.277, 0.322
batch losses (mrrl, rdl): 0.0201252438, 0.0001668736

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.523 +- 0.249
mrr vals (pred, true): 0.085, 0.000
batch losses (mrrl, rdl): 0.011994116, 9.1294e-06

Epoch over!
epoch time: 12.729

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 759
rank avg (pred): 0.508 +- 0.284
mrr vals (pred, true): 0.096, 0.001
batch losses (mrrl, rdl): 0.0215131119, 4.33462e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.492 +- 0.266
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0192136839, 2.32177e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 52
rank avg (pred): 0.180 +- 0.150
mrr vals (pred, true): 0.248, 0.265
batch losses (mrrl, rdl): 0.0030455883, 7.60414e-05

Epoch over!
epoch time: 12.177

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 556
rank avg (pred): 0.354 +- 0.256
mrr vals (pred, true): 0.183, 0.165
batch losses (mrrl, rdl): 0.003409154, 0.0001701144

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 223
rank avg (pred): 0.454 +- 0.210
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0158753507, 5.43257e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.506 +- 0.292
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0179850347, 3.88213e-05

Epoch over!
epoch time: 12.177

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 657
rank avg (pred): 0.538 +- 0.256
mrr vals (pred, true): 0.073, 0.001
batch losses (mrrl, rdl): 0.005067029, 2.04556e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 811
rank avg (pred): 0.162 +- 0.136
mrr vals (pred, true): 0.268, 0.059
batch losses (mrrl, rdl): 0.4746847153, 6.59034e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 984
rank avg (pred): 0.318 +- 0.235
mrr vals (pred, true): 0.184, 0.269
batch losses (mrrl, rdl): 0.0717617348, 0.0004899226

Epoch over!
epoch time: 12.099

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.024 +- 0.027
mrr vals (pred, true): 0.398, 0.366

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   13 	     0 	 0.09843 	 5e-0500 	 m..s
  101 	     1 	 0.21714 	 6e-0500 	 MISS
   95 	     2 	 0.19942 	 6e-0500 	 MISS
   15 	     3 	 0.09944 	 6e-0500 	 m..s
   62 	     4 	 0.11122 	 7e-0500 	 MISS
   18 	     5 	 0.09976 	 7e-0500 	 m..s
   42 	     6 	 0.10525 	 0.00013 	 MISS
   16 	     7 	 0.09958 	 0.00013 	 m..s
   70 	     8 	 0.11306 	 0.00014 	 MISS
   26 	     9 	 0.10144 	 0.00016 	 MISS
   36 	    10 	 0.10378 	 0.00017 	 MISS
   48 	    11 	 0.10685 	 0.00018 	 MISS
   25 	    12 	 0.10111 	 0.00018 	 MISS
   53 	    13 	 0.10858 	 0.00018 	 MISS
   41 	    14 	 0.10472 	 0.00018 	 MISS
   72 	    15 	 0.11456 	 0.00020 	 MISS
   22 	    16 	 0.10047 	 0.00021 	 MISS
   12 	    17 	 0.09838 	 0.00022 	 m..s
   64 	    18 	 0.11133 	 0.00022 	 MISS
   55 	    19 	 0.10942 	 0.00023 	 MISS
   21 	    20 	 0.10019 	 0.00023 	 m..s
   80 	    21 	 0.14177 	 0.00023 	 MISS
    6 	    22 	 0.09589 	 0.00024 	 m..s
   44 	    23 	 0.10584 	 0.00025 	 MISS
   52 	    24 	 0.10855 	 0.00025 	 MISS
   39 	    25 	 0.10462 	 0.00025 	 MISS
   60 	    26 	 0.11036 	 0.00025 	 MISS
   29 	    27 	 0.10215 	 0.00025 	 MISS
   66 	    28 	 0.11231 	 0.00025 	 MISS
   68 	    29 	 0.11299 	 0.00026 	 MISS
    1 	    30 	 0.09358 	 0.00027 	 m..s
    4 	    31 	 0.09513 	 0.00027 	 m..s
   67 	    32 	 0.11273 	 0.00027 	 MISS
   74 	    33 	 0.11627 	 0.00028 	 MISS
   49 	    34 	 0.10701 	 0.00028 	 MISS
   79 	    35 	 0.13688 	 0.00029 	 MISS
   11 	    36 	 0.09795 	 0.00032 	 m..s
   78 	    37 	 0.13403 	 0.00032 	 MISS
   72 	    38 	 0.11456 	 0.00033 	 MISS
   47 	    39 	 0.10666 	 0.00036 	 MISS
   38 	    40 	 0.10459 	 0.00038 	 MISS
   46 	    41 	 0.10626 	 0.00040 	 MISS
    9 	    42 	 0.09709 	 0.00040 	 m..s
   30 	    43 	 0.10215 	 0.00040 	 MISS
   94 	    44 	 0.19415 	 0.00042 	 MISS
   23 	    45 	 0.10061 	 0.00046 	 MISS
   34 	    46 	 0.10368 	 0.00048 	 MISS
   50 	    47 	 0.10727 	 0.00050 	 MISS
   77 	    48 	 0.13163 	 0.00051 	 MISS
   33 	    49 	 0.10325 	 0.00056 	 MISS
   61 	    50 	 0.11039 	 0.00060 	 MISS
   10 	    51 	 0.09757 	 0.00062 	 m..s
   35 	    52 	 0.10369 	 0.00069 	 MISS
    7 	    53 	 0.09640 	 0.00078 	 m..s
   83 	    54 	 0.14398 	 0.00112 	 MISS
   28 	    55 	 0.10157 	 0.00642 	 m..s
   71 	    56 	 0.11335 	 0.00823 	 MISS
   69 	    57 	 0.11303 	 0.00829 	 MISS
   93 	    58 	 0.18805 	 0.00861 	 MISS
   24 	    59 	 0.10074 	 0.00907 	 m..s
   87 	    60 	 0.17515 	 0.00969 	 MISS
   17 	    61 	 0.09968 	 0.01934 	 m..s
  116 	    62 	 0.29778 	 0.02535 	 MISS
   84 	    63 	 0.16304 	 0.02598 	 MISS
   85 	    64 	 0.16662 	 0.02689 	 MISS
  115 	    65 	 0.27625 	 0.05263 	 MISS
   27 	    66 	 0.10148 	 0.11596 	 ~...
  109 	    67 	 0.24012 	 0.11626 	 MISS
    8 	    68 	 0.09674 	 0.11647 	 ~...
    0 	    69 	 0.09300 	 0.12381 	 m..s
    5 	    70 	 0.09577 	 0.13210 	 m..s
   54 	    71 	 0.10871 	 0.13254 	 ~...
    3 	    72 	 0.09465 	 0.13456 	 m..s
    2 	    73 	 0.09415 	 0.13507 	 m..s
   51 	    74 	 0.10753 	 0.13665 	 ~...
  112 	    75 	 0.24921 	 0.13911 	 MISS
   56 	    76 	 0.10968 	 0.14050 	 m..s
   31 	    77 	 0.10265 	 0.14108 	 m..s
   40 	    78 	 0.10471 	 0.14193 	 m..s
   19 	    79 	 0.09980 	 0.14660 	 m..s
   45 	    80 	 0.10609 	 0.14679 	 m..s
   86 	    81 	 0.16825 	 0.14756 	 ~...
   82 	    82 	 0.14362 	 0.14995 	 ~...
   65 	    83 	 0.11144 	 0.15311 	 m..s
   75 	    84 	 0.12051 	 0.15324 	 m..s
   81 	    85 	 0.14351 	 0.15768 	 ~...
   76 	    86 	 0.12051 	 0.15800 	 m..s
   63 	    87 	 0.11125 	 0.15903 	 m..s
   58 	    88 	 0.10973 	 0.15909 	 m..s
   32 	    89 	 0.10269 	 0.15939 	 m..s
   14 	    90 	 0.09924 	 0.16031 	 m..s
   37 	    91 	 0.10429 	 0.16184 	 m..s
  106 	    92 	 0.22649 	 0.16260 	 m..s
   99 	    93 	 0.21544 	 0.16644 	 m..s
   59 	    94 	 0.10994 	 0.16900 	 m..s
   89 	    95 	 0.17684 	 0.17108 	 ~...
   43 	    96 	 0.10571 	 0.17491 	 m..s
   91 	    97 	 0.18028 	 0.17576 	 ~...
   20 	    98 	 0.09987 	 0.17681 	 m..s
   97 	    99 	 0.20606 	 0.17790 	 ~...
   88 	   100 	 0.17651 	 0.18182 	 ~...
   90 	   101 	 0.17881 	 0.18736 	 ~...
   57 	   102 	 0.10970 	 0.19208 	 m..s
  104 	   103 	 0.22050 	 0.19952 	 ~...
  105 	   104 	 0.22604 	 0.20496 	 ~...
   96 	   105 	 0.20159 	 0.20883 	 ~...
   92 	   106 	 0.18760 	 0.22450 	 m..s
  108 	   107 	 0.23501 	 0.24150 	 ~...
  111 	   108 	 0.24627 	 0.25479 	 ~...
   98 	   109 	 0.21502 	 0.25578 	 m..s
  113 	   110 	 0.25002 	 0.26378 	 ~...
  102 	   111 	 0.21718 	 0.26759 	 m..s
  100 	   112 	 0.21683 	 0.27477 	 m..s
  110 	   113 	 0.24437 	 0.27863 	 m..s
  107 	   114 	 0.22649 	 0.29470 	 m..s
  103 	   115 	 0.21995 	 0.29610 	 m..s
  118 	   116 	 0.32875 	 0.33536 	 ~...
  119 	   117 	 0.34444 	 0.34760 	 ~...
  117 	   118 	 0.32159 	 0.35024 	 ~...
  120 	   119 	 0.39772 	 0.36583 	 m..s
  114 	   120 	 0.26316 	 0.37521 	 MISS
==========================================
r_mrr = 0.6640280485153198
r2_mrr = 0.17378705739974976
spearmanr_mrr@5 = 0.9502888917922974
spearmanr_mrr@10 = 0.9577975273132324
spearmanr_mrr@50 = 0.9488202929496765
spearmanr_mrr@100 = 0.9264305233955383
spearmanr_mrr@All = 0.9348998665809631
==========================================
test time: 0.521
Done Testing dataset DBpedia50
total time taken: 188.47108554840088
training time taken: 183.59440565109253
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.6640)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.1738)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9503)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9578)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9488)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.9264)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9349)}}, 'test_loss': {'DistMult': {'DBpedia50': 5.725423796306131}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 8817225249415981
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [161, 652, 173, 1056, 1201, 314, 832, 996, 138, 784, 79, 845, 523, 443, 991, 1029, 1195, 429, 574, 400, 1093, 497, 422, 992, 631, 880, 467, 1014, 872, 979, 968, 642, 1059, 1178, 888, 109, 344, 938, 1209, 1140, 943, 415, 302, 685, 1210, 1211, 77, 883, 1185, 794, 167, 636, 150, 827, 260, 767, 183, 772, 263, 1196, 780, 17, 902, 1146, 1062, 462, 729, 629, 1025, 549, 106, 112, 180, 802, 848, 757, 325, 997, 446, 281, 1081, 1067, 825, 507, 160, 187, 399, 635, 1179, 1212, 364, 812, 663, 411, 605, 884, 60, 124, 897, 1006, 360, 756, 1145, 492, 486, 732, 1085, 701, 604, 191, 395, 807, 569, 1105, 499, 433, 834, 117, 268, 864, 847]
valid_ids (0): []
train_ids (1094): [894, 148, 632, 954, 1053, 105, 1205, 46, 1020, 749, 207, 623, 867, 939, 1176, 722, 73, 913, 937, 509, 273, 692, 127, 1090, 179, 21, 978, 837, 555, 495, 898, 1134, 971, 45, 822, 252, 511, 1169, 1151, 831, 266, 1009, 149, 1018, 447, 1136, 93, 798, 655, 739, 724, 985, 649, 505, 582, 346, 1167, 1017, 1075, 571, 377, 34, 809, 485, 836, 74, 1064, 1011, 542, 348, 965, 489, 1164, 192, 496, 609, 349, 977, 792, 1034, 391, 607, 1104, 50, 493, 861, 1102, 689, 806, 826, 398, 987, 471, 241, 1197, 638, 1092, 1086, 1010, 853, 472, 515, 1168, 345, 1155, 423, 242, 257, 615, 855, 564, 915, 775, 830, 730, 1036, 139, 146, 1070, 580, 76, 1199, 553, 1202, 125, 208, 1039, 4, 95, 860, 375, 768, 551, 821, 70, 567, 682, 1181, 63, 1173, 116, 779, 370, 1040, 115, 586, 336, 514, 357, 30, 49, 935, 251, 300, 189, 25, 218, 870, 829, 53, 1148, 1157, 1082, 926, 673, 328, 601, 487, 243, 481, 1061, 843, 3, 587, 64, 714, 740, 1016, 1200, 442, 734, 1187, 548, 1072, 174, 1193, 1182, 1007, 285, 537, 312, 478, 665, 760, 96, 184, 130, 136, 380, 534, 698, 1089, 246, 32, 288, 828, 428, 256, 340, 190, 810, 52, 742, 1186, 1172, 330, 650, 910, 612, 1131, 1125, 129, 236, 134, 62, 568, 132, 1084, 535, 522, 808, 240, 316, 451, 85, 172, 37, 185, 875, 570, 944, 1055, 128, 824, 221, 122, 547, 407, 223, 1088, 157, 466, 710, 378, 1139, 1019, 437, 811, 877, 168, 691, 613, 849, 833, 332, 813, 386, 584, 277, 1129, 627, 475, 700, 92, 750, 169, 1076, 929, 480, 1153, 239, 624, 1184, 284, 1191, 986, 1027, 512, 800, 600, 65, 166, 674, 859, 653, 726, 1124, 751, 140, 483, 137, 804, 508, 75, 66, 1094, 389, 118, 554, 656, 404, 31, 372, 1174, 980, 660, 541, 200, 215, 470, 626, 602, 871, 502, 900, 787, 575, 367, 408, 941, 394, 320, 983, 677, 1203, 126, 886, 783, 1012, 182, 1095, 297, 453, 1120, 560, 538, 101, 181, 588, 625, 435, 863, 773, 752, 29, 697, 317, 896, 339, 86, 1043, 718, 572, 1047, 865, 606, 211, 721, 371, 1050, 1154, 479, 41, 102, 222, 620, 272, 696, 556, 1180, 313, 1096, 755, 1114, 550, 1115, 577, 999, 158, 1065, 20, 425, 392, 895, 97, 924, 513, 163, 1194, 662, 347, 283, 338, 225, 956, 282, 488, 646, 1099, 230, 846, 100, 664, 790, 321, 889, 322, 503, 258, 290, 579, 921, 374, 786, 264, 562, 1177, 852, 279, 679, 934, 213, 634, 110, 1198, 177, 269, 705, 7, 1005, 528, 795, 817, 1189, 153, 706, 342, 1058, 789, 869, 412, 1004, 899, 68, 477, 753, 839, 1204, 707, 9, 71, 456, 526, 231, 165, 510, 1206, 1038, 51, 48, 844, 401, 928, 254, 585, 13, 310, 33, 379, 227, 1071, 1112, 737, 1190, 265, 91, 741, 362, 1, 1126, 617, 1080, 57, 527, 891, 1144, 44, 459, 976, 1117, 142, 666, 936, 5, 524, 686, 1165, 879, 595, 10, 1111, 94, 969, 1214, 881, 123, 276, 205, 640, 301, 1107, 1057, 69, 908, 176, 1028, 350, 1158, 280, 1054, 120, 1013, 680, 621, 1091, 893, 611, 820, 641, 530, 521, 6, 1128, 858, 838, 1101, 1188, 684, 464, 616, 14, 1152, 558, 43, 566, 363, 1192, 973, 444, 657, 368, 121, 520, 933, 426, 953, 1078, 501, 335, 195, 396, 274, 1031, 922, 206, 341, 962, 430, 669, 704, 287, 993, 1023, 785, 1132, 295, 862, 402, 355, 418, 197, 1097, 723, 591, 1063, 1073, 99, 961, 1123, 463, 630, 659, 393, 536, 690, 498, 383, 907, 289, 532, 544, 175, 735, 914, 133, 298, 857, 376, 543, 354, 919, 1116, 1147, 226, 781, 959, 436, 967, 648, 525, 958, 531, 713, 1024, 319, 856, 381, 271, 416, 628, 709, 193, 352, 1069, 59, 717, 80, 963, 186, 388, 1100, 1208, 876, 54, 334, 927, 1142, 818, 942, 1160, 770, 874, 597, 61, 1106, 590, 1098, 947, 683, 397, 619, 1026, 776, 940, 712, 1213, 108, 318, 27, 250, 930, 596, 454, 716, 974, 949, 103, 356, 589, 687, 610, 82, 155, 309, 439, 676, 1046, 410, 851, 1161, 233, 711, 299, 162, 545, 307, 22, 291, 819, 141, 736, 835, 728, 988, 373, 461, 18, 998, 733, 214, 267, 984, 743, 643, 529, 1022, 406, 671, 23, 762, 111, 970, 678, 1051, 196, 28, 131, 1207, 905, 667, 758, 946, 668, 995, 468, 194, 306, 892, 911, 912, 906, 925, 1127, 592, 622, 720, 235, 769, 778, 1037, 1162, 761, 1138, 972, 405, 114, 490, 255, 1002, 552, 931, 15, 78, 143, 159, 637, 916, 245, 576, 26, 1159, 1156, 188, 981, 540, 771, 315, 982, 202, 854, 670, 727, 932, 1183, 945, 199, 294, 278, 702, 286, 441, 403, 901, 209, 1049, 1166, 815, 438, 661, 504, 305, 1103, 694, 1032, 87, 1030, 343, 11, 1044, 329, 293, 39, 164, 573, 296, 765, 469, 1052, 887, 1143, 764, 217, 675, 337, 1000, 868, 873, 427, 238, 744, 420, 639, 850, 1066, 658, 842, 955, 67, 424, 12, 866, 960, 715, 1118, 840, 58, 878, 517, 465, 473, 19, 951, 719, 681, 903, 748, 904, 262, 688, 1083, 647, 382, 210, 458, 559, 449, 1035, 608, 358, 593, 390, 234, 457, 38, 793, 1045, 693, 56, 308, 1068, 506, 645, 326, 156, 353, 249, 738, 259, 1133, 434, 331, 1041, 365, 203, 1021, 224, 212, 90, 89, 651, 2, 699, 654, 40, 598, 384, 614, 703, 731, 445, 603, 474, 782, 759, 918, 557, 324, 460, 801, 323, 500, 516, 599, 746, 747, 816, 1110, 275, 948, 237, 909, 1163, 539, 989, 644, 494, 232, 450, 154, 1015, 882, 1171, 533, 303, 885, 1130, 333, 1121, 431, 229, 578, 952, 359, 24, 482, 35, 563, 151, 990, 796, 1150, 432, 994, 565, 1137, 519, 920, 1001, 1077, 1033, 204, 766, 917, 448, 745, 1074, 228, 763, 244, 366, 248, 1060, 419, 369, 413, 198, 618, 561, 220, 81, 594, 440, 219, 178, 107, 361, 421, 774, 135, 546, 814, 1109, 201, 113, 387, 261, 788, 964, 119, 581, 1122, 409, 152, 144, 311, 216, 754, 16, 84, 1079, 0, 247, 1108, 170, 452, 476, 890, 1149, 484, 292, 8, 491, 47, 1170, 823, 83, 327, 777, 1042, 104, 88, 708, 1003, 1113, 803, 1135, 1141, 791, 841, 672, 633, 72, 414, 923, 417, 583, 797, 36, 799, 1087, 966, 1175, 253, 1048, 957, 42, 351, 950, 385, 975, 1119, 98, 518, 805, 145, 147, 171, 455, 55, 270, 1008, 725, 304, 695]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3863470700813678
the save name prefix for this run is:  chkpt-ID_3863470700813678_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.554 +- 0.007
mrr vals (pred, true): 0.000, 0.153
batch losses (mrrl, rdl): 0.0, 0.001702034

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.308 +- 0.206
mrr vals (pred, true): 0.033, 0.144
batch losses (mrrl, rdl): 0.0, 0.0007032339

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.409 +- 0.310
mrr vals (pred, true): 0.135, 0.000
batch losses (mrrl, rdl): 0.0, 3.1178e-05

Epoch over!
epoch time: 12.132

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 736
rank avg (pred): 0.409 +- 0.323
mrr vals (pred, true): 0.184, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001665421

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.167 +- 0.149
mrr vals (pred, true): 0.266, 0.211
batch losses (mrrl, rdl): 0.0, 4.11967e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.575 +- 0.375
mrr vals (pred, true): 0.093, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002453419

Epoch over!
epoch time: 12.09

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 476
rank avg (pred): 0.349 +- 0.315
mrr vals (pred, true): 0.258, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002255427

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 113
rank avg (pred): 0.334 +- 0.307
mrr vals (pred, true): 0.279, 0.141
batch losses (mrrl, rdl): 0.0, 7.78489e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 331
rank avg (pred): 0.372 +- 0.320
mrr vals (pred, true): 0.235, 0.175
batch losses (mrrl, rdl): 0.0, 0.0001560214

Epoch over!
epoch time: 11.772

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 94
rank avg (pred): 0.361 +- 0.324
mrr vals (pred, true): 0.249, 0.193
batch losses (mrrl, rdl): 0.0, 7.75263e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 935
rank avg (pred): 0.589 +- 0.364
mrr vals (pred, true): 0.085, 0.000
batch losses (mrrl, rdl): 0.0, 0.0012850323

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 201
rank avg (pred): 0.368 +- 0.319
mrr vals (pred, true): 0.216, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002554961

Epoch over!
epoch time: 11.677

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.370 +- 0.319
mrr vals (pred, true): 0.213, 0.139
batch losses (mrrl, rdl): 0.0, 3.25123e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.390 +- 0.328
mrr vals (pred, true): 0.225, 0.000
batch losses (mrrl, rdl): 0.0, 2.76906e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 479
rank avg (pred): 0.397 +- 0.314
mrr vals (pred, true): 0.193, 0.000
batch losses (mrrl, rdl): 0.0, 4.26764e-05

Epoch over!
epoch time: 11.766

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.387 +- 0.315
mrr vals (pred, true): 0.198, 0.147
batch losses (mrrl, rdl): 0.0253688097, 0.0001127824

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 688
rank avg (pred): 0.458 +- 0.233
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0312984027, 3.89208e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.470 +- 0.218
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0085373325, 1.96718e-05

Epoch over!
epoch time: 12.197

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.453 +- 0.231
mrr vals (pred, true): 0.095, 0.165
batch losses (mrrl, rdl): 0.0494666956, 0.0007749635

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 818
rank avg (pred): 0.466 +- 0.219
mrr vals (pred, true): 0.089, 0.025
batch losses (mrrl, rdl): 0.015118272, 0.0001971533

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 19
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.345, 0.350
batch losses (mrrl, rdl): 0.0003193849, 0.0002879811

Epoch over!
epoch time: 11.891

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 464
rank avg (pred): 0.443 +- 0.230
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0269766357, 8.73084e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 98
rank avg (pred): 0.443 +- 0.242
mrr vals (pred, true): 0.093, 0.175
batch losses (mrrl, rdl): 0.0663194358, 0.0003152016

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 683
rank avg (pred): 0.454 +- 0.228
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0225348063, 2.54139e-05

Epoch over!
epoch time: 12.291

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 455
rank avg (pred): 0.432 +- 0.237
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0281895809, 6.85856e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1156
rank avg (pred): 0.076 +- 0.059
mrr vals (pred, true): 0.245, 0.217
batch losses (mrrl, rdl): 0.0080926334, 0.001123363

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 947
rank avg (pred): 0.528 +- 0.146
mrr vals (pred, true): 0.041, 0.000
batch losses (mrrl, rdl): 0.0008196611, 6.62747e-05

Epoch over!
epoch time: 12.779

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.350 +- 0.183
mrr vals (pred, true): 0.108, 0.139
batch losses (mrrl, rdl): 0.0097663468, 0.0004873947

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.391 +- 0.172
mrr vals (pred, true): 0.082, 0.118
batch losses (mrrl, rdl): 0.013555008, 0.0006928563

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.288 +- 0.160
mrr vals (pred, true): 0.110, 0.021
batch losses (mrrl, rdl): 0.0358339734, 0.0022105228

Epoch over!
epoch time: 12.449

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.451 +- 0.216
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0190110784, 2.35952e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.291 +- 0.215
mrr vals (pred, true): 0.210, 0.221
batch losses (mrrl, rdl): 0.0012785804, 0.0003655553

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.240 +- 0.146
mrr vals (pred, true): 0.101, 0.021
batch losses (mrrl, rdl): 0.0256354921, 0.0028977757

Epoch over!
epoch time: 12.51

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.005 +- 0.004
mrr vals (pred, true): 0.307, 0.362
batch losses (mrrl, rdl): 0.0306200683, 0.0003617369

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.505 +- 0.155
mrr vals (pred, true): 0.048, 0.019
batch losses (mrrl, rdl): 4.51955e-05, 0.0003442668

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.447 +- 0.204
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.0133332843, 6.63462e-05

Epoch over!
epoch time: 12.112

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 643
rank avg (pred): 0.439 +- 0.213
mrr vals (pred, true): 0.102, 0.134
batch losses (mrrl, rdl): 0.0103922179, 0.0001559531

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 627
rank avg (pred): 0.435 +- 0.210
mrr vals (pred, true): 0.093, 0.114
batch losses (mrrl, rdl): 0.0045090956, 0.0002688224

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 287
rank avg (pred): 0.252 +- 0.179
mrr vals (pred, true): 0.196, 0.260
batch losses (mrrl, rdl): 0.0410539471, 0.0003614726

Epoch over!
epoch time: 13.074

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.494 +- 0.172
mrr vals (pred, true): 0.067, 0.026
batch losses (mrrl, rdl): 0.0027568175, 0.0003475142

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 543
rank avg (pred): 0.244 +- 0.169
mrr vals (pred, true): 0.181, 0.138
batch losses (mrrl, rdl): 0.0182793215, 0.0001039252

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.331, 0.383
batch losses (mrrl, rdl): 0.0271490403, 0.0002414597

Epoch over!
epoch time: 12.512

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 249
rank avg (pred): 0.014 +- 0.010
mrr vals (pred, true): 0.301, 0.354
batch losses (mrrl, rdl): 0.0285869204, 0.0003063945

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.260 +- 0.158
mrr vals (pred, true): 0.114, 0.130
batch losses (mrrl, rdl): 0.0028639182, 0.0001789279

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 129
rank avg (pred): 0.432 +- 0.212
mrr vals (pred, true): 0.102, 0.169
batch losses (mrrl, rdl): 0.0446463972, 0.0003545026

Epoch over!
epoch time: 12.598

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.454 +- 0.204
mrr vals (pred, true): 0.091, 0.155

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   22 	     0 	 0.05902 	 5e-0500 	 m..s
   17 	     1 	 0.04926 	 7e-0500 	 m..s
   20 	     2 	 0.05030 	 0.00010 	 m..s
   16 	     3 	 0.04909 	 0.00011 	 m..s
   18 	     4 	 0.04995 	 0.00014 	 m..s
   71 	     5 	 0.09768 	 0.00015 	 m..s
    2 	     6 	 0.03665 	 0.00016 	 m..s
   70 	     7 	 0.09714 	 0.00016 	 m..s
   76 	     8 	 0.09848 	 0.00017 	 m..s
   73 	     9 	 0.09810 	 0.00018 	 m..s
   34 	    10 	 0.09003 	 0.00018 	 m..s
    4 	    11 	 0.03684 	 0.00018 	 m..s
   82 	    12 	 0.09911 	 0.00018 	 m..s
   32 	    13 	 0.08991 	 0.00019 	 m..s
   69 	    14 	 0.09694 	 0.00019 	 m..s
   80 	    15 	 0.09893 	 0.00020 	 m..s
   65 	    16 	 0.09674 	 0.00020 	 m..s
    1 	    17 	 0.03656 	 0.00022 	 m..s
   29 	    18 	 0.08919 	 0.00022 	 m..s
   61 	    19 	 0.09530 	 0.00022 	 m..s
   62 	    20 	 0.09539 	 0.00023 	 m..s
   36 	    21 	 0.09063 	 0.00023 	 m..s
    0 	    22 	 0.03649 	 0.00027 	 m..s
   18 	    23 	 0.04995 	 0.00027 	 m..s
   68 	    24 	 0.09687 	 0.00027 	 m..s
   15 	    25 	 0.04871 	 0.00027 	 m..s
   30 	    26 	 0.08939 	 0.00028 	 m..s
   46 	    27 	 0.09341 	 0.00028 	 m..s
   12 	    28 	 0.04844 	 0.00034 	 m..s
   58 	    29 	 0.09486 	 0.00036 	 m..s
   57 	    30 	 0.09470 	 0.00036 	 m..s
   72 	    31 	 0.09790 	 0.00041 	 m..s
   31 	    32 	 0.08973 	 0.00044 	 m..s
   45 	    33 	 0.09324 	 0.00045 	 m..s
   25 	    34 	 0.08599 	 0.00048 	 m..s
   41 	    35 	 0.09259 	 0.00053 	 m..s
    7 	    36 	 0.04545 	 0.00056 	 m..s
   49 	    37 	 0.09352 	 0.00075 	 m..s
    5 	    38 	 0.03692 	 0.00086 	 m..s
   50 	    39 	 0.09354 	 0.00112 	 m..s
   12 	    40 	 0.04844 	 0.00143 	 m..s
    8 	    41 	 0.04551 	 0.00149 	 m..s
    6 	    42 	 0.04531 	 0.00179 	 m..s
   33 	    43 	 0.08996 	 0.00261 	 m..s
   11 	    44 	 0.04576 	 0.00267 	 m..s
   23 	    45 	 0.05926 	 0.00504 	 m..s
    3 	    46 	 0.03672 	 0.00980 	 ~...
   14 	    47 	 0.04865 	 0.01742 	 m..s
    9 	    48 	 0.04562 	 0.01885 	 ~...
   60 	    49 	 0.09528 	 0.02598 	 m..s
    9 	    50 	 0.04562 	 0.02709 	 ~...
   24 	    51 	 0.05936 	 0.02767 	 m..s
   21 	    52 	 0.05543 	 0.05033 	 ~...
   27 	    53 	 0.08714 	 0.10737 	 ~...
   87 	    54 	 0.10017 	 0.11547 	 ~...
   53 	    55 	 0.09440 	 0.12393 	 ~...
   40 	    56 	 0.09114 	 0.12500 	 m..s
   51 	    57 	 0.09374 	 0.12750 	 m..s
   56 	    58 	 0.09460 	 0.13157 	 m..s
   81 	    59 	 0.09902 	 0.13345 	 m..s
   42 	    60 	 0.09266 	 0.13457 	 m..s
   35 	    61 	 0.09036 	 0.13558 	 m..s
   83 	    62 	 0.09945 	 0.13563 	 m..s
   74 	    63 	 0.09837 	 0.13636 	 m..s
   77 	    64 	 0.09852 	 0.13665 	 m..s
   54 	    65 	 0.09447 	 0.13748 	 m..s
   66 	    66 	 0.09677 	 0.13755 	 m..s
   74 	    67 	 0.09837 	 0.14195 	 m..s
   55 	    68 	 0.09455 	 0.14204 	 m..s
   44 	    69 	 0.09287 	 0.14525 	 m..s
   94 	    70 	 0.21183 	 0.14923 	 m..s
   28 	    71 	 0.08781 	 0.14927 	 m..s
   67 	    72 	 0.09686 	 0.15180 	 m..s
   42 	    73 	 0.09266 	 0.15457 	 m..s
   39 	    74 	 0.09108 	 0.15486 	 m..s
   48 	    75 	 0.09352 	 0.15773 	 m..s
   47 	    76 	 0.09350 	 0.15865 	 m..s
   63 	    77 	 0.09602 	 0.15939 	 m..s
   26 	    78 	 0.08626 	 0.15939 	 m..s
   78 	    79 	 0.09870 	 0.15983 	 m..s
   52 	    80 	 0.09428 	 0.16327 	 m..s
   86 	    81 	 0.10002 	 0.16753 	 m..s
   85 	    82 	 0.09998 	 0.16757 	 m..s
   84 	    83 	 0.09981 	 0.16809 	 m..s
   92 	    84 	 0.20467 	 0.16826 	 m..s
   59 	    85 	 0.09522 	 0.16857 	 m..s
   91 	    86 	 0.10404 	 0.16903 	 m..s
   63 	    87 	 0.09602 	 0.17409 	 m..s
   93 	    88 	 0.20803 	 0.17535 	 m..s
   37 	    89 	 0.09100 	 0.17614 	 m..s
   90 	    90 	 0.10132 	 0.18258 	 m..s
   79 	    91 	 0.09881 	 0.18669 	 m..s
  103 	    92 	 0.23404 	 0.20377 	 m..s
   38 	    93 	 0.09104 	 0.20470 	 MISS
   88 	    94 	 0.10048 	 0.20620 	 MISS
   89 	    95 	 0.10088 	 0.21465 	 MISS
  101 	    96 	 0.23161 	 0.22329 	 ~...
   95 	    97 	 0.21542 	 0.22741 	 ~...
  100 	    98 	 0.23104 	 0.24670 	 ~...
  105 	    99 	 0.23671 	 0.24970 	 ~...
   97 	   100 	 0.21900 	 0.25351 	 m..s
  112 	   101 	 0.30735 	 0.26209 	 m..s
  109 	   102 	 0.26781 	 0.26345 	 ~...
  113 	   103 	 0.30782 	 0.26472 	 m..s
   99 	   104 	 0.23070 	 0.27050 	 m..s
   96 	   105 	 0.21754 	 0.27292 	 m..s
  107 	   106 	 0.24444 	 0.27466 	 m..s
  115 	   107 	 0.31459 	 0.27508 	 m..s
  118 	   108 	 0.32203 	 0.28241 	 m..s
   98 	   109 	 0.22854 	 0.28672 	 m..s
  114 	   110 	 0.31237 	 0.29003 	 ~...
  106 	   111 	 0.24227 	 0.29422 	 m..s
  102 	   112 	 0.23208 	 0.30876 	 m..s
  104 	   113 	 0.23569 	 0.31189 	 m..s
  116 	   114 	 0.32152 	 0.33349 	 ~...
  116 	   115 	 0.32152 	 0.35127 	 ~...
  111 	   116 	 0.26805 	 0.36372 	 m..s
  120 	   117 	 0.33377 	 0.37167 	 m..s
  108 	   118 	 0.26440 	 0.37521 	 MISS
  119 	   119 	 0.32558 	 0.37539 	 m..s
  109 	   120 	 0.26781 	 0.38555 	 MISS
==========================================
r_mrr = 0.8398552536964417
r2_mrr = 0.682655930519104
spearmanr_mrr@5 = 0.8632593154907227
spearmanr_mrr@10 = 0.860313355922699
spearmanr_mrr@50 = 0.964077353477478
spearmanr_mrr@100 = 0.86374431848526
spearmanr_mrr@All = 0.8919457793235779
==========================================
test time: 0.413
Done Testing dataset DBpedia50
total time taken: 189.0732581615448
training time taken: 184.32253217697144
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8399)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6827)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8633)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.8603)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9641)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8637)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8919)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.9485453328379663}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 1928397725351642
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [605, 611, 395, 444, 198, 1066, 927, 620, 182, 130, 570, 493, 954, 777, 1070, 699, 62, 664, 291, 1139, 246, 345, 505, 456, 1009, 808, 735, 740, 795, 635, 1015, 308, 2, 815, 972, 970, 663, 916, 579, 950, 303, 170, 276, 797, 1098, 945, 754, 806, 1050, 44, 534, 765, 1182, 37, 233, 968, 1210, 903, 452, 118, 172, 39, 1202, 573, 1021, 1102, 1023, 84, 714, 459, 547, 898, 1200, 421, 786, 1141, 289, 1147, 528, 1165, 1175, 935, 235, 1006, 501, 618, 438, 256, 1159, 624, 396, 461, 778, 789, 128, 1053, 5, 532, 382, 918, 403, 1164, 4, 897, 828, 856, 522, 716, 650, 1160, 14, 725, 728, 937, 915, 210, 830, 794, 173, 1113, 906]
valid_ids (0): []
train_ids (1094): [388, 774, 283, 77, 65, 689, 428, 484, 30, 1004, 615, 437, 34, 15, 324, 802, 567, 639, 378, 1156, 1157, 871, 991, 106, 647, 1135, 224, 900, 366, 936, 634, 1114, 1074, 318, 1137, 599, 361, 446, 263, 1196, 642, 422, 0, 1012, 841, 1034, 691, 178, 372, 367, 304, 538, 632, 28, 578, 562, 253, 1091, 231, 593, 307, 869, 189, 25, 383, 1024, 1128, 81, 631, 103, 751, 454, 807, 341, 1010, 498, 472, 317, 107, 633, 78, 843, 328, 64, 677, 196, 957, 833, 524, 286, 840, 545, 842, 537, 565, 944, 949, 45, 1038, 379, 810, 158, 169, 893, 1062, 924, 120, 697, 575, 1203, 1105, 503, 790, 1127, 696, 596, 1185, 1179, 448, 1011, 70, 679, 550, 1000, 419, 997, 526, 203, 1151, 912, 803, 29, 159, 832, 250, 992, 595, 115, 658, 239, 274, 877, 582, 380, 1187, 137, 717, 184, 836, 544, 1126, 127, 306, 354, 260, 757, 373, 813, 1209, 652, 878, 743, 363, 226, 564, 694, 1018, 1168, 192, 775, 365, 59, 755, 666, 53, 1108, 207, 1145, 55, 408, 1041, 301, 800, 951, 335, 353, 1104, 48, 1194, 637, 710, 1, 988, 171, 102, 441, 1110, 1022, 932, 216, 707, 58, 749, 1090, 721, 905, 453, 124, 1030, 586, 236, 901, 101, 996, 278, 331, 433, 385, 202, 649, 215, 38, 180, 1013, 393, 571, 109, 657, 1111, 1086, 72, 1020, 265, 623, 139, 1028, 870, 1082, 121, 431, 469, 722, 931, 926, 849, 352, 763, 894, 166, 476, 1097, 561, 69, 746, 907, 730, 928, 629, 1014, 850, 958, 960, 425, 54, 1036, 733, 726, 232, 976, 117, 684, 1027, 1155, 1103, 1214, 785, 105, 160, 351, 113, 26, 218, 520, 356, 164, 1007, 298, 1071, 183, 13, 1150, 872, 370, 739, 93, 793, 132, 126, 280, 964, 560, 16, 1084, 685, 781, 851, 1115, 1047, 585, 320, 604, 32, 859, 555, 475, 779, 332, 762, 252, 829, 219, 1096, 626, 470, 259, 947, 138, 262, 85, 756, 973, 603, 440, 266, 863, 540, 704, 668, 1199, 917, 497, 49, 899, 411, 277, 720, 742, 12, 1109, 548, 831, 701, 1055, 412, 142, 816, 398, 1146, 330, 375, 400, 1019, 199, 427, 804, 457, 546, 750, 188, 434, 921, 168, 97, 1046, 934, 153, 1032, 536, 284, 981, 110, 129, 1143, 1033, 297, 323, 1189, 513, 712, 402, 614, 598, 641, 930, 695, 211, 9, 237, 868, 875, 819, 350, 1134, 977, 1044, 682, 788, 442, 381, 698, 702, 1180, 409, 768, 1026, 212, 247, 860, 688, 541, 100, 884, 507, 709, 342, 1121, 622, 116, 499, 1017, 264, 11, 1130, 334, 1054, 66, 348, 52, 206, 74, 591, 47, 881, 1186, 150, 435, 602, 50, 776, 673, 193, 176, 1077, 390, 998, 1058, 509, 825, 1089, 1129, 812, 413, 465, 485, 389, 764, 1008, 423, 339, 123, 769, 619, 386, 606, 636, 455, 848, 952, 693, 601, 1193, 282, 1174, 3, 194, 191, 919, 61, 1191, 759, 99, 261, 943, 374, 993, 805, 494, 886, 576, 512, 240, 122, 466, 295, 1136, 731, 271, 18, 681, 293, 204, 149, 683, 834, 533, 1076, 521, 478, 686, 157, 787, 299, 473, 1072, 1079, 177, 706, 296, 847, 854, 1125, 975, 167, 890, 858, 165, 19, 60, 867, 1120, 185, 529, 94, 21, 445, 515, 782, 760, 201, 140, 96, 510, 135, 145, 978, 111, 747, 391, 417, 6, 86, 221, 338, 316, 865, 922, 824, 144, 588, 79, 443, 486, 880, 1178, 1201, 346, 674, 488, 439, 814, 753, 963, 1016, 1162, 518, 1207, 700, 772, 312, 883, 22, 1132, 305, 217, 965, 429, 40, 1075, 715, 979, 451, 962, 559, 961, 337, 783, 967, 392, 527, 1052, 502, 359, 1064, 162, 1059, 36, 827, 220, 490, 516, 430, 197, 941, 826, 584, 517, 761, 57, 630, 82, 1063, 251, 290, 670, 1197, 131, 895, 112, 556, 321, 450, 287, 1152, 344, 566, 1176, 1173, 491, 597, 1094, 986, 1085, 617, 577, 468, 24, 1212, 73, 410, 608, 729, 1095, 1065, 909, 672, 1166, 891, 773, 969, 487, 1068, 496, 267, 908, 1040, 1087, 879, 852, 643, 1213, 724, 424, 329, 671, 294, 362, 799, 432, 553, 325, 416, 270, 770, 938, 771, 889, 460, 1069, 1093, 874, 8, 855, 920, 156, 405, 594, 42, 758, 718, 543, 17, 1119, 35, 1045, 1100, 948, 1154, 662, 1123, 242, 228, 1122, 1167, 589, 63, 200, 745, 551, 939, 268, 360, 143, 656, 243, 492, 911, 857, 713, 358, 680, 414, 811, 736, 539, 92, 554, 690, 195, 798, 474, 214, 1083, 241, 613, 349, 319, 397, 853, 581, 477, 896, 481, 1158, 1170, 51, 1140, 1092, 272, 355, 792, 1148, 91, 519, 309, 667, 796, 181, 552, 213, 1043, 904, 186, 1124, 862, 1177, 467, 480, 506, 1190, 244, 1039, 801, 269, 114, 141, 719, 966, 33, 959, 839, 1029, 147, 987, 845, 844, 942, 946, 1035, 511, 655, 347, 404, 654, 27, 542, 646, 1060, 67, 653, 401, 310, 1107, 530, 563, 675, 384, 371, 767, 187, 1106, 300, 87, 7, 95, 1208, 621, 227, 583, 10, 676, 665, 504, 648, 245, 910, 837, 659, 368, 549, 31, 1211, 766, 154, 580, 523, 1116, 953, 41, 489, 846, 175, 956, 415, 1037, 835, 343, 1169, 1131, 406, 1056, 1138, 327, 645, 933, 133, 1206, 155, 887, 151, 902, 279, 1003, 376, 1183, 273, 995, 458, 651, 669, 838, 1101, 885, 23, 88, 678, 134, 861, 607, 703, 610, 1118, 738, 394, 1031, 1005, 90, 557, 531, 612, 660, 464, 1195, 1112, 1198, 238, 482, 864, 587, 230, 322, 146, 125, 982, 1048, 479, 734, 436, 708, 377, 98, 1081, 1001, 462, 336, 821, 744, 302, 687, 1142, 1149, 1002, 1192, 1153, 311, 223, 999, 985, 254, 179, 940, 46, 357, 640, 984, 20, 1163, 292, 471, 208, 340, 644, 866, 387, 705, 1067, 463, 1188, 369, 1133, 791, 1051, 447, 990, 108, 873, 508, 818, 809, 315, 723, 89, 711, 1184, 1057, 913, 229, 222, 1061, 234, 525, 600, 625, 737, 980, 971, 732, 314, 255, 616, 882, 119, 820, 68, 418, 592, 1049, 495, 275, 281, 257, 1088, 152, 1204, 407, 249, 258, 75, 1080, 627, 288, 364, 449, 994, 1205, 569, 568, 780, 558, 161, 190, 326, 974, 748, 483, 399, 1171, 1144, 56, 609, 955, 163, 285, 876, 174, 817, 1073, 225, 1117, 43, 923, 1042, 1099, 1161, 929, 822, 572, 426, 248, 83, 205, 661, 692, 741, 80, 104, 1025, 784, 333, 209, 313, 76, 914, 574, 752, 500, 823, 628, 1078, 1172, 925, 148, 590, 1181, 71, 888, 638, 136, 727, 514, 420, 535, 989, 983, 892]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9078661167181808
the save name prefix for this run is:  chkpt-ID_9078661167181808_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.484 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001072648

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.384 +- 0.230
mrr vals (pred, true): 0.029, 0.140
batch losses (mrrl, rdl): 0.0, 0.0007593046

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 904
rank avg (pred): 0.496 +- 0.388
mrr vals (pred, true): 0.189, 0.021
batch losses (mrrl, rdl): 0.0, 0.0001572773

Epoch over!
epoch time: 12.128

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 919
rank avg (pred): 0.492 +- 0.387
mrr vals (pred, true): 0.192, 0.000
batch losses (mrrl, rdl): 0.0, 5.07034e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.326 +- 0.262
mrr vals (pred, true): 0.197, 0.001
batch losses (mrrl, rdl): 0.0, 0.0004157802

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1127
rank avg (pred): 0.291 +- 0.255
mrr vals (pred, true): 0.275, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005708024

Epoch over!
epoch time: 12.065

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.359 +- 0.321
mrr vals (pred, true): 0.249, 0.026
batch losses (mrrl, rdl): 0.0, 5.5877e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.329 +- 0.276
mrr vals (pred, true): 0.123, 0.349
batch losses (mrrl, rdl): 0.0, 0.0008795988

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 422
rank avg (pred): 0.293 +- 0.282
mrr vals (pred, true): 0.186, 0.001
batch losses (mrrl, rdl): 0.0, 0.0006252865

Epoch over!
epoch time: 11.715

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 105
rank avg (pred): 0.301 +- 0.279
mrr vals (pred, true): 0.177, 0.165
batch losses (mrrl, rdl): 0.0, 3.51879e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 775
rank avg (pred): 0.358 +- 0.322
mrr vals (pred, true): 0.158, 0.010
batch losses (mrrl, rdl): 0.0, 1.23483e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.288 +- 0.287
mrr vals (pred, true): 0.239, 0.168
batch losses (mrrl, rdl): 0.0, 2.3589e-05

Epoch over!
epoch time: 11.699

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 161
rank avg (pred): 0.296 +- 0.282
mrr vals (pred, true): 0.135, 0.155
batch losses (mrrl, rdl): 0.0, 1.33666e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 268
rank avg (pred): 0.279 +- 0.267
mrr vals (pred, true): 0.187, 0.375
batch losses (mrrl, rdl): 0.0, 0.0008390537

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 721
rank avg (pred): 0.344 +- 0.320
mrr vals (pred, true): 0.187, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001723287

Epoch over!
epoch time: 11.786

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1185
rank avg (pred): 0.361 +- 0.319
mrr vals (pred, true): 0.154, 0.115
batch losses (mrrl, rdl): 0.0147938896, 2.82116e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 680
rank avg (pred): 0.364 +- 0.248
mrr vals (pred, true): 0.151, 0.000
batch losses (mrrl, rdl): 0.1012093946, 0.000121372

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 520
rank avg (pred): 0.428 +- 0.211
mrr vals (pred, true): 0.123, 0.153
batch losses (mrrl, rdl): 0.0089791315, 0.0005667669

Epoch over!
epoch time: 12.211

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.836 +- 0.313
mrr vals (pred, true): 0.070, 0.130
batch losses (mrrl, rdl): 0.0362380147, 0.0064874976

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 672
rank avg (pred): 0.437 +- 0.277
mrr vals (pred, true): 0.149, 0.000
batch losses (mrrl, rdl): 0.0974411964, 1.53869e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 319
rank avg (pred): 0.320 +- 0.243
mrr vals (pred, true): 0.155, 0.245
batch losses (mrrl, rdl): 0.0812694728, 0.0006412204

Epoch over!
epoch time: 12.165

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.781 +- 0.306
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0081705358, 0.0015456831

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 103
rank avg (pred): 0.346 +- 0.244
mrr vals (pred, true): 0.172, 0.178
batch losses (mrrl, rdl): 0.0004122981, 7.41328e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1086
rank avg (pred): 0.260 +- 0.186
mrr vals (pred, true): 0.169, 0.161
batch losses (mrrl, rdl): 0.0005684467, 5.67287e-05

Epoch over!
epoch time: 11.939

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.320 +- 0.230
mrr vals (pred, true): 0.172, 0.142
batch losses (mrrl, rdl): 0.0088315755, 5.35369e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 755
rank avg (pred): 0.704 +- 0.225
mrr vals (pred, true): 0.065, 0.156
batch losses (mrrl, rdl): 0.0831267089, 0.0043548043

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.379 +- 0.180
mrr vals (pred, true): 0.141, 0.003
batch losses (mrrl, rdl): 0.0833637714, 0.0001584734

Epoch over!
epoch time: 11.966

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1103
rank avg (pred): 0.351 +- 0.237
mrr vals (pred, true): 0.157, 0.159
batch losses (mrrl, rdl): 3.13714e-05, 0.000107313

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 555
rank avg (pred): 0.388 +- 0.172
mrr vals (pred, true): 0.108, 0.176
batch losses (mrrl, rdl): 0.0452888198, 0.0002675471

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.256 +- 0.173
mrr vals (pred, true): 0.181, 0.364
batch losses (mrrl, rdl): 0.3347918689, 0.0003713499

Epoch over!
epoch time: 11.93

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 497
rank avg (pred): 0.419 +- 0.272
mrr vals (pred, true): 0.165, 0.282
batch losses (mrrl, rdl): 0.1385239065, 0.0005955197

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 132
rank avg (pred): 0.304 +- 0.191
mrr vals (pred, true): 0.156, 0.153
batch losses (mrrl, rdl): 0.000113269, 7.36542e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1099
rank avg (pred): 0.275 +- 0.180
mrr vals (pred, true): 0.171, 0.139
batch losses (mrrl, rdl): 0.0101791359, 0.0001168254

Epoch over!
epoch time: 12.164

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.357 +- 0.207
mrr vals (pred, true): 0.144, 0.322
batch losses (mrrl, rdl): 0.3152317107, 0.0010307218

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.403 +- 0.244
mrr vals (pred, true): 0.167, 0.153
batch losses (mrrl, rdl): 0.001803913, 0.0004872938

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.344 +- 0.188
mrr vals (pred, true): 0.139, 0.000
batch losses (mrrl, rdl): 0.0789007694, 0.0004554545

Epoch over!
epoch time: 12.136

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1170
rank avg (pred): 0.365 +- 0.175
mrr vals (pred, true): 0.139, 0.127
batch losses (mrrl, rdl): 0.0014064152, 5.41089e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 467
rank avg (pred): 0.362 +- 0.198
mrr vals (pred, true): 0.142, 0.000
batch losses (mrrl, rdl): 0.0837431923, 0.0003379551

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 483
rank avg (pred): 0.265 +- 0.198
mrr vals (pred, true): 0.167, 0.000
batch losses (mrrl, rdl): 0.13587147, 0.0010490947

Epoch over!
epoch time: 12.022

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.392 +- 0.274
mrr vals (pred, true): 0.156, 0.000
batch losses (mrrl, rdl): 0.1124611199, 0.0001865317

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.427 +- 0.325
mrr vals (pred, true): 0.181, 0.000
batch losses (mrrl, rdl): 0.1717711687, 0.0001085393

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 426
rank avg (pred): 0.220 +- 0.269
mrr vals (pred, true): 0.194, 0.000
batch losses (mrrl, rdl): 0.2073766291, 0.0014612054

Epoch over!
epoch time: 11.938

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 790
rank avg (pred): 0.422 +- 0.135
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0040348908, 0.0002256905

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1051
rank avg (pred): 0.361 +- 0.283
mrr vals (pred, true): 0.162, 0.000
batch losses (mrrl, rdl): 0.1250674725, 0.000468159

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 63
rank avg (pred): 0.393 +- 0.238
mrr vals (pred, true): 0.148, 0.217
batch losses (mrrl, rdl): 0.0486065596, 0.0009454825

Epoch over!
epoch time: 12.1

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.373 +- 0.202
mrr vals (pred, true): 0.141, 0.125

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.07157 	 5e-0500 	 m..s
   14 	     1 	 0.07287 	 5e-0500 	 m..s
   25 	     2 	 0.08486 	 0.00013 	 m..s
   13 	     3 	 0.07236 	 0.00014 	 m..s
  118 	     4 	 0.19285 	 0.00016 	 MISS
   65 	     5 	 0.15066 	 0.00016 	 MISS
   82 	     6 	 0.15372 	 0.00017 	 MISS
   40 	     7 	 0.13916 	 0.00017 	 MISS
    0 	     8 	 0.06488 	 0.00018 	 m..s
   24 	     9 	 0.08484 	 0.00018 	 m..s
   48 	    10 	 0.14140 	 0.00018 	 MISS
   84 	    11 	 0.15972 	 0.00019 	 MISS
  111 	    12 	 0.18301 	 0.00020 	 MISS
  101 	    13 	 0.16997 	 0.00020 	 MISS
   22 	    14 	 0.08482 	 0.00020 	 m..s
   16 	    15 	 0.08309 	 0.00021 	 m..s
   51 	    16 	 0.14213 	 0.00022 	 MISS
    3 	    17 	 0.07155 	 0.00022 	 m..s
  117 	    18 	 0.18826 	 0.00022 	 MISS
   63 	    19 	 0.14906 	 0.00023 	 MISS
   57 	    20 	 0.14594 	 0.00023 	 MISS
   86 	    21 	 0.16528 	 0.00025 	 MISS
    9 	    22 	 0.07205 	 0.00025 	 m..s
   47 	    23 	 0.14116 	 0.00025 	 MISS
   65 	    24 	 0.15066 	 0.00026 	 MISS
   15 	    25 	 0.07755 	 0.00026 	 m..s
  113 	    26 	 0.18498 	 0.00027 	 MISS
   16 	    27 	 0.08309 	 0.00028 	 m..s
  119 	    28 	 0.19424 	 0.00029 	 MISS
    9 	    29 	 0.07205 	 0.00029 	 m..s
   28 	    30 	 0.08489 	 0.00030 	 m..s
   87 	    31 	 0.16564 	 0.00031 	 MISS
   65 	    32 	 0.15066 	 0.00034 	 MISS
   43 	    33 	 0.14088 	 0.00036 	 MISS
   80 	    34 	 0.15334 	 0.00038 	 MISS
   65 	    35 	 0.15066 	 0.00038 	 MISS
    0 	    36 	 0.06488 	 0.00039 	 m..s
   32 	    37 	 0.13263 	 0.00040 	 MISS
    2 	    38 	 0.06730 	 0.00056 	 m..s
   21 	    39 	 0.08477 	 0.00068 	 m..s
   19 	    40 	 0.08456 	 0.00090 	 m..s
   97 	    41 	 0.16798 	 0.00091 	 MISS
   56 	    42 	 0.14482 	 0.00096 	 MISS
    3 	    43 	 0.07155 	 0.00119 	 m..s
    8 	    44 	 0.07167 	 0.00149 	 m..s
   65 	    45 	 0.15066 	 0.00171 	 MISS
   26 	    46 	 0.08488 	 0.00179 	 m..s
  103 	    47 	 0.17200 	 0.00236 	 MISS
  112 	    48 	 0.18404 	 0.00833 	 MISS
    7 	    49 	 0.07162 	 0.00845 	 m..s
   12 	    50 	 0.07221 	 0.00969 	 m..s
   30 	    51 	 0.09070 	 0.01606 	 m..s
   11 	    52 	 0.07213 	 0.01662 	 m..s
    6 	    53 	 0.07159 	 0.01728 	 m..s
   23 	    54 	 0.08483 	 0.01934 	 m..s
   18 	    55 	 0.08390 	 0.03198 	 m..s
   26 	    56 	 0.08488 	 0.10268 	 ~...
   40 	    57 	 0.13916 	 0.10738 	 m..s
   59 	    58 	 0.14613 	 0.11596 	 m..s
   19 	    59 	 0.08456 	 0.11626 	 m..s
   58 	    60 	 0.14608 	 0.11677 	 ~...
   33 	    61 	 0.13295 	 0.12369 	 ~...
   45 	    62 	 0.14089 	 0.12500 	 ~...
   65 	    63 	 0.15066 	 0.12961 	 ~...
   90 	    64 	 0.16612 	 0.13227 	 m..s
   53 	    65 	 0.14250 	 0.13254 	 ~...
   29 	    66 	 0.09067 	 0.13282 	 m..s
   43 	    67 	 0.14088 	 0.13558 	 ~...
   61 	    68 	 0.14785 	 0.13794 	 ~...
   51 	    69 	 0.14213 	 0.13873 	 ~...
   93 	    70 	 0.16731 	 0.14108 	 ~...
   89 	    71 	 0.16610 	 0.14228 	 ~...
   31 	    72 	 0.09076 	 0.14285 	 m..s
   60 	    73 	 0.14714 	 0.14497 	 ~...
   50 	    74 	 0.14168 	 0.14549 	 ~...
   54 	    75 	 0.14293 	 0.14590 	 ~...
  106 	    76 	 0.17284 	 0.14632 	 ~...
   46 	    77 	 0.14116 	 0.14665 	 ~...
   77 	    78 	 0.15155 	 0.14927 	 ~...
   65 	    79 	 0.15066 	 0.15436 	 ~...
   92 	    80 	 0.16682 	 0.15711 	 ~...
  100 	    81 	 0.16886 	 0.15903 	 ~...
   36 	    82 	 0.13607 	 0.15951 	 ~...
   97 	    83 	 0.16798 	 0.16004 	 ~...
  120 	    84 	 0.19473 	 0.16301 	 m..s
   88 	    85 	 0.16596 	 0.16317 	 ~...
  105 	    86 	 0.17208 	 0.16398 	 ~...
  116 	    87 	 0.18764 	 0.16753 	 ~...
   96 	    88 	 0.16794 	 0.16907 	 ~...
   79 	    89 	 0.15327 	 0.16947 	 ~...
   91 	    90 	 0.16655 	 0.17675 	 ~...
   37 	    91 	 0.13797 	 0.18800 	 m..s
   34 	    92 	 0.13307 	 0.19074 	 m..s
   65 	    93 	 0.15066 	 0.19298 	 m..s
   49 	    94 	 0.14141 	 0.20586 	 m..s
   80 	    95 	 0.15334 	 0.21094 	 m..s
   35 	    96 	 0.13530 	 0.21254 	 m..s
   42 	    97 	 0.13988 	 0.21901 	 m..s
   65 	    98 	 0.15066 	 0.22441 	 m..s
   39 	    99 	 0.13907 	 0.22450 	 m..s
   38 	   100 	 0.13820 	 0.23254 	 m..s
   65 	   101 	 0.15066 	 0.23829 	 m..s
   65 	   102 	 0.15066 	 0.24048 	 m..s
   65 	   103 	 0.15066 	 0.24229 	 m..s
   78 	   104 	 0.15289 	 0.26209 	 MISS
   62 	   105 	 0.14822 	 0.26466 	 MISS
   55 	   106 	 0.14428 	 0.27412 	 MISS
   83 	   107 	 0.15428 	 0.28735 	 MISS
   63 	   108 	 0.14906 	 0.28805 	 MISS
   99 	   109 	 0.16839 	 0.29260 	 MISS
   95 	   110 	 0.16752 	 0.30319 	 MISS
   85 	   111 	 0.16163 	 0.30561 	 MISS
  102 	   112 	 0.17049 	 0.31099 	 MISS
   94 	   113 	 0.16733 	 0.31202 	 MISS
  108 	   114 	 0.18181 	 0.32253 	 MISS
  104 	   115 	 0.17202 	 0.32533 	 MISS
  107 	   116 	 0.18073 	 0.33015 	 MISS
  109 	   117 	 0.18242 	 0.33393 	 MISS
  110 	   118 	 0.18276 	 0.34516 	 MISS
  115 	   119 	 0.18530 	 0.34745 	 MISS
  114 	   120 	 0.18513 	 0.36720 	 MISS
==========================================
r_mrr = 0.48932960629463196
r2_mrr = 0.15863966941833496
spearmanr_mrr@5 = 0.8812748193740845
spearmanr_mrr@10 = 0.9525024890899658
spearmanr_mrr@50 = 0.9665394425392151
spearmanr_mrr@100 = 0.8440011739730835
spearmanr_mrr@All = 0.828107476234436
==========================================
test time: 0.413
Done Testing dataset DBpedia50
total time taken: 184.99970817565918
training time taken: 180.4412875175476
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.4893)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.1586)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8813)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9525)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9665)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8440)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8281)}}, 'test_loss': {'DistMult': {'DBpedia50': 8.102349183354818}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 2519578377623106
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [946, 1115, 365, 374, 1125, 559, 777, 1046, 5, 460, 669, 927, 1156, 847, 453, 533, 565, 487, 479, 1160, 283, 502, 948, 394, 599, 953, 231, 1071, 773, 807, 687, 858, 1053, 952, 797, 1092, 302, 315, 1173, 177, 409, 14, 708, 232, 132, 430, 1129, 1209, 892, 134, 793, 1081, 623, 918, 1158, 543, 399, 75, 690, 261, 766, 1002, 654, 1, 703, 494, 506, 850, 433, 735, 792, 72, 184, 814, 505, 9, 836, 8, 1033, 758, 267, 92, 595, 928, 384, 611, 775, 45, 320, 33, 1098, 545, 345, 491, 661, 1001, 206, 883, 391, 373, 1161, 864, 974, 975, 239, 606, 523, 155, 1166, 389, 1049, 678, 934, 70, 589, 925, 939, 1057, 1176, 715, 1143]
valid_ids (0): []
train_ids (1094): [252, 720, 323, 564, 825, 423, 1184, 450, 326, 10, 577, 891, 106, 557, 629, 995, 916, 21, 1153, 637, 973, 36, 878, 930, 1084, 718, 726, 461, 738, 1132, 1083, 113, 124, 1059, 949, 586, 18, 635, 1089, 674, 1127, 1126, 982, 47, 406, 67, 520, 485, 750, 489, 879, 377, 30, 376, 762, 516, 943, 1031, 555, 986, 861, 828, 295, 388, 756, 319, 172, 412, 515, 658, 429, 572, 272, 795, 769, 696, 684, 811, 1061, 761, 273, 622, 193, 714, 960, 812, 787, 870, 278, 392, 662, 324, 655, 154, 783, 385, 857, 809, 26, 481, 932, 349, 1114, 573, 356, 846, 958, 378, 1137, 439, 138, 675, 888, 774, 298, 346, 43, 996, 819, 933, 1020, 474, 162, 544, 615, 260, 670, 1155, 170, 894, 331, 563, 192, 994, 82, 619, 826, 620, 277, 367, 1171, 390, 576, 69, 281, 1019, 317, 947, 660, 935, 709, 929, 532, 1067, 587, 386, 1186, 236, 626, 628, 1111, 173, 202, 621, 194, 1204, 122, 652, 227, 890, 411, 148, 276, 362, 1009, 457, 404, 369, 395, 601, 695, 881, 127, 845, 63, 956, 446, 496, 957, 1110, 136, 581, 452, 294, 1212, 503, 116, 51, 1203, 471, 665, 803, 765, 663, 683, 197, 62, 689, 1139, 309, 393, 1193, 451, 344, 251, 142, 189, 259, 596, 174, 329, 217, 796, 548, 989, 722, 977, 966, 1178, 438, 799, 1121, 478, 978, 1174, 490, 1076, 435, 1105, 729, 422, 732, 1078, 872, 467, 17, 185, 1116, 743, 1042, 1024, 530, 906, 649, 582, 1068, 226, 816, 269, 328, 552, 509, 354, 1179, 1128, 744, 131, 483, 1006, 1198, 125, 920, 224, 262, 425, 728, 874, 211, 27, 876, 180, 1181, 691, 806, 643, 440, 352, 782, 198, 396, 558, 991, 99, 144, 476, 482, 274, 46, 215, 962, 228, 257, 1085, 1087, 889, 468, 417, 1054, 473, 838, 1072, 550, 877, 1051, 264, 980, 831, 848, 519, 335, 1063, 1135, 221, 379, 118, 119, 1044, 1007, 1148, 157, 98, 321, 907, 1034, 723, 68, 554, 727, 48, 188, 94, 898, 842, 244, 421, 455, 786, 223, 77, 712, 763, 1177, 107, 896, 165, 528, 922, 1010, 618, 1022, 357, 80, 39, 105, 1101, 993, 470, 1090, 103, 616, 149, 753, 679, 1069, 527, 495, 608, 199, 97, 242, 746, 330, 686, 488, 900, 710, 1107, 290, 271, 1207, 1104, 535, 921, 359, 327, 759, 380, 751, 1138, 1168, 268, 146, 713, 778, 940, 549, 1014, 614, 969, 711, 1048, 147, 871, 1106, 1093, 1065, 432, 585, 694, 0, 1201, 431, 1151, 1088, 1102, 574, 229, 992, 768, 987, 682, 540, 292, 1062, 464, 1025, 334, 204, 1169, 1142, 1191, 1208, 50, 513, 602, 1013, 1058, 914, 1146, 37, 307, 954, 1008, 126, 1205, 1074, 944, 187, 212, 560, 286, 1196, 284, 141, 1035, 854, 499, 917, 588, 398, 737, 1199, 332, 716, 59, 76, 990, 901, 238, 810, 899, 931, 104, 444, 312, 873, 772, 1124, 347, 688, 979, 905, 667, 1036, 339, 869, 463, 818, 692, 536, 3, 32, 893, 225, 780, 627, 58, 579, 859, 158, 1012, 1206, 531, 524, 400, 297, 537, 760, 1170, 604, 145, 521, 650, 266, 484, 243, 325, 466, 465, 591, 827, 764, 1144, 644, 458, 1150, 970, 366, 755, 437, 419, 1162, 401, 701, 741, 1103, 843, 1094, 371, 776, 1005, 407, 168, 190, 1082, 784, 833, 52, 634, 91, 74, 250, 1004, 805, 11, 603, 348, 672, 717, 340, 265, 213, 361, 160, 849, 724, 1192, 997, 1149, 299, 248, 733, 964, 856, 959, 676, 1100, 253, 245, 1141, 343, 510, 90, 834, 112, 133, 196, 641, 525, 441, 707, 166, 924, 1163, 742, 815, 538, 280, 191, 287, 651, 445, 44, 84, 875, 129, 1038, 779, 61, 913, 1167, 121, 613, 341, 1189, 291, 195, 1064, 951, 370, 1079, 424, 1214, 115, 255, 685, 1211, 1195, 167, 1152, 1190, 454, 1086, 1210, 645, 653, 547, 919, 153, 950, 405, 29, 642, 342, 853, 6, 1077, 447, 83, 42, 813, 34, 546, 85, 902, 353, 504, 820, 486, 397, 1052, 1200, 590, 114, 702, 1165, 556, 804, 183, 443, 462, 798, 78, 1164, 408, 976, 222, 275, 1140, 955, 566, 680, 1113, 985, 88, 182, 971, 886, 968, 247, 288, 333, 1029, 794, 562, 740, 1017, 909, 988, 673, 410, 830, 817, 270, 128, 1117, 1123, 575, 64, 1073, 387, 498, 1157, 338, 363, 171, 705, 150, 730, 71, 840, 865, 426, 305, 942, 617, 293, 402, 625, 829, 1131, 719, 910, 638, 569, 984, 1041, 57, 789, 35, 375, 164, 159, 1091, 304, 12, 844, 624, 256, 96, 936, 220, 66, 1112, 1011, 648, 1095, 529, 81, 175, 137, 882, 55, 120, 434, 38, 1183, 203, 923, 631, 73, 1136, 911, 868, 1023, 210, 551, 216, 981, 143, 668, 594, 657, 785, 736, 1075, 414, 561, 235, 757, 837, 1108, 54, 28, 15, 108, 233, 40, 355, 1147, 938, 181, 771, 522, 605, 130, 704, 511, 926, 1045, 553, 1120, 752, 915, 135, 823, 459, 310, 318, 767, 500, 632, 904, 863, 246, 22, 1070, 2, 20, 381, 65, 788, 903, 580, 95, 436, 416, 748, 791, 1056, 636, 1000, 102, 633, 1040, 963, 855, 607, 364, 477, 731, 699, 802, 301, 230, 497, 442, 1021, 420, 1039, 1026, 646, 163, 140, 912, 79, 1175, 311, 1109, 967, 539, 214, 571, 427, 972, 862, 880, 475, 1188, 139, 492, 101, 822, 998, 1030, 1197, 93, 178, 205, 13, 1122, 1015, 570, 49, 337, 698, 241, 1187, 584, 1060, 860, 508, 583, 1003, 983, 999, 598, 739, 1159, 1066, 200, 1213, 382, 89, 351, 24, 87, 111, 1194, 677, 413, 821, 218, 117, 156, 1018, 313, 965, 169, 700, 1097, 824, 1037, 542, 885, 808, 518, 835, 1202, 693, 1185, 428, 1016, 1047, 541, 152, 472, 945, 176, 306, 801, 110, 469, 53, 1080, 161, 285, 360, 1096, 866, 639, 725, 1145, 754, 884, 526, 908, 578, 841, 1099, 832, 418, 659, 350, 449, 186, 41, 507, 237, 514, 597, 1130, 60, 1154, 656, 19, 852, 316, 151, 372, 358, 1050, 512, 610, 480, 109, 1043, 314, 448, 207, 895, 1119, 282, 612, 1172, 263, 897, 336, 567, 415, 16, 600, 630, 697, 201, 517, 56, 1028, 7, 368, 289, 867, 1027, 403, 279, 208, 593, 25, 300, 800, 706, 209, 456, 249, 745, 1055, 219, 887, 1134, 681, 308, 1032, 1118, 23, 1182, 671, 734, 179, 322, 839, 640, 609, 4, 501, 86, 749, 961, 383, 721, 647, 234, 790, 254, 664, 747, 1180, 123, 666, 100, 1133, 240, 770, 851, 568, 296, 534, 941, 592, 493, 303, 31, 781, 937, 258]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1252934476557140
the save name prefix for this run is:  chkpt-ID_1252934476557140_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 567
rank avg (pred): 0.588 +- 0.002
mrr vals (pred, true): 0.000, 0.140
batch losses (mrrl, rdl): 0.0, 0.0009577647

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 997
rank avg (pred): 0.130 +- 0.100
mrr vals (pred, true): 0.148, 0.275
batch losses (mrrl, rdl): 0.0, 9.06636e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 293
rank avg (pred): 0.189 +- 0.143
mrr vals (pred, true): 0.065, 0.293
batch losses (mrrl, rdl): 0.0, 7.17337e-05

Epoch over!
epoch time: 11.786

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.120 +- 0.096
mrr vals (pred, true): 0.260, 0.363
batch losses (mrrl, rdl): 0.0, 4.18914e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 225
rank avg (pred): 0.395 +- 0.285
mrr vals (pred, true): 0.167, 0.000
batch losses (mrrl, rdl): 0.0, 6.25427e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.155 +- 0.127
mrr vals (pred, true): 0.239, 0.144
batch losses (mrrl, rdl): 0.0, 3.94506e-05

Epoch over!
epoch time: 11.8

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 165
rank avg (pred): 0.376 +- 0.298
mrr vals (pred, true): 0.226, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002793258

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 471
rank avg (pred): 0.374 +- 0.302
mrr vals (pred, true): 0.222, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002527366

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.404 +- 0.299
mrr vals (pred, true): 0.158, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001495813

Epoch over!
epoch time: 11.897

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 510
rank avg (pred): 0.309 +- 0.261
mrr vals (pred, true): 0.219, 0.279
batch losses (mrrl, rdl): 0.0, 0.0001716048

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.228 +- 0.194
mrr vals (pred, true): 0.235, 0.080
batch losses (mrrl, rdl): 0.0, 8.01799e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1168
rank avg (pred): 0.400 +- 0.302
mrr vals (pred, true): 0.198, 0.135
batch losses (mrrl, rdl): 0.0, 7.08108e-05

Epoch over!
epoch time: 11.785

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 131
rank avg (pred): 0.378 +- 0.308
mrr vals (pred, true): 0.188, 0.156
batch losses (mrrl, rdl): 0.0, 0.0002452379

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.417 +- 0.299
mrr vals (pred, true): 0.194, 0.019
batch losses (mrrl, rdl): 0.0, 5.24479e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 6
rank avg (pred): 0.156 +- 0.136
mrr vals (pred, true): 0.197, 0.316
batch losses (mrrl, rdl): 0.0, 5.28765e-05

Epoch over!
epoch time: 11.774

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 782
rank avg (pred): 0.501 +- 0.304
mrr vals (pred, true): 0.123, 0.000
batch losses (mrrl, rdl): 0.0532255135, 0.0003229442

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1134
rank avg (pred): 0.030 +- 0.024
mrr vals (pred, true): 0.230, 0.282
batch losses (mrrl, rdl): 0.0267969966, 0.0009164935

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.540 +- 0.244
mrr vals (pred, true): 0.070, 0.146
batch losses (mrrl, rdl): 0.0574082173, 0.0005355969

Epoch over!
epoch time: 12.009

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 161
rank avg (pred): 0.478 +- 0.263
mrr vals (pred, true): 0.081, 0.155
batch losses (mrrl, rdl): 0.0544284992, 0.0007049405

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.458 +- 0.259
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0417208262, 2.57909e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 602
rank avg (pred): 0.464 +- 0.243
mrr vals (pred, true): 0.093, 0.124
batch losses (mrrl, rdl): 0.0094810994, 0.0002025287

Epoch over!
epoch time: 12.196

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.458 +- 0.245
mrr vals (pred, true): 0.097, 0.164
batch losses (mrrl, rdl): 0.0453030355, 0.000503325

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 415
rank avg (pred): 0.461 +- 0.231
mrr vals (pred, true): 0.091, 0.000
batch losses (mrrl, rdl): 0.0170302223, 2.2922e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 400
rank avg (pred): 0.423 +- 0.249
mrr vals (pred, true): 0.110, 0.152
batch losses (mrrl, rdl): 0.0170808714, 0.000413209

Epoch over!
epoch time: 12.134

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 805
rank avg (pred): 0.616 +- 0.254
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.8937e-05, 0.0001270023

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.420 +- 0.240
mrr vals (pred, true): 0.109, 0.166
batch losses (mrrl, rdl): 0.0325117595, 0.0003893944

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.582 +- 0.238
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0004008874, 6.21585e-05

Epoch over!
epoch time: 12.066

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1022
rank avg (pred): 0.431 +- 0.221
mrr vals (pred, true): 0.092, 0.134
batch losses (mrrl, rdl): 0.0176288392, 0.0003476444

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 22
rank avg (pred): 0.101 +- 0.082
mrr vals (pred, true): 0.296, 0.335
batch losses (mrrl, rdl): 0.0156034632, 7.93512e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 829
rank avg (pred): 0.477 +- 0.238
mrr vals (pred, true): 0.097, 0.133
batch losses (mrrl, rdl): 0.0134570152, 0.0024239318

Epoch over!
epoch time: 11.967

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 429
rank avg (pred): 0.418 +- 0.211
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0279726367, 0.0001041795

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 36
rank avg (pred): 0.205 +- 0.160
mrr vals (pred, true): 0.250, 0.268
batch losses (mrrl, rdl): 0.0031244401, 9.50043e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 445
rank avg (pred): 0.395 +- 0.211
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0276399069, 0.0002088063

Epoch over!
epoch time: 12.488

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 78
rank avg (pred): 0.161 +- 0.129
mrr vals (pred, true): 0.271, 0.262
batch losses (mrrl, rdl): 0.000797273, 7.46643e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 296
rank avg (pred): 0.122 +- 0.098
mrr vals (pred, true): 0.335, 0.322
batch losses (mrrl, rdl): 0.0016138896, 5.90419e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1144
rank avg (pred): 0.247 +- 0.176
mrr vals (pred, true): 0.220, 0.231
batch losses (mrrl, rdl): 0.0014345894, 0.0001106637

Epoch over!
epoch time: 11.994

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 377
rank avg (pred): 0.385 +- 0.212
mrr vals (pred, true): 0.107, 0.165
batch losses (mrrl, rdl): 0.032954704, 0.0001873009

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 361
rank avg (pred): 0.389 +- 0.211
mrr vals (pred, true): 0.107, 0.133
batch losses (mrrl, rdl): 0.0069282493, 0.0002182129

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1155
rank avg (pred): 0.259 +- 0.187
mrr vals (pred, true): 0.211, 0.217
batch losses (mrrl, rdl): 0.000419351, 8.58734e-05

Epoch over!
epoch time: 11.885

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.388 +- 0.206
mrr vals (pred, true): 0.107, 0.000
batch losses (mrrl, rdl): 0.0328990929, 9.11671e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 457
rank avg (pred): 0.384 +- 0.202
mrr vals (pred, true): 0.107, 0.000
batch losses (mrrl, rdl): 0.0329656675, 0.0001802355

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.402 +- 0.184
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0267876349, 0.0001274106

Epoch over!
epoch time: 11.856

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.397 +- 0.185
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0273832195, 0.0001388066

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 855
rank avg (pred): 0.556 +- 0.227
mrr vals (pred, true): 0.049, 0.009
batch losses (mrrl, rdl): 1.31258e-05, 0.000552549

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.590 +- 0.279
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001382269, 0.0019245273

Epoch over!
epoch time: 11.9

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.552 +- 0.246
mrr vals (pred, true): 0.056, 0.001

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.05018 	 5e-0500 	 m..s
    2 	     1 	 0.05018 	 6e-0500 	 m..s
    7 	     2 	 0.05568 	 6e-0500 	 m..s
    4 	     3 	 0.05271 	 0.00010 	 m..s
   28 	     4 	 0.10474 	 0.00016 	 MISS
   52 	     5 	 0.10639 	 0.00016 	 MISS
   36 	     6 	 0.10506 	 0.00017 	 MISS
   18 	     7 	 0.05838 	 0.00018 	 m..s
   16 	     8 	 0.05800 	 0.00018 	 m..s
    6 	     9 	 0.05535 	 0.00019 	 m..s
   60 	    10 	 0.10722 	 0.00020 	 MISS
   14 	    11 	 0.05762 	 0.00020 	 m..s
   64 	    12 	 0.10792 	 0.00021 	 MISS
   62 	    13 	 0.10757 	 0.00021 	 MISS
   31 	    14 	 0.10485 	 0.00021 	 MISS
    0 	    15 	 0.04943 	 0.00021 	 m..s
   32 	    16 	 0.10488 	 0.00021 	 MISS
   54 	    17 	 0.10644 	 0.00021 	 MISS
   21 	    18 	 0.05886 	 0.00023 	 m..s
   55 	    19 	 0.10647 	 0.00024 	 MISS
   69 	    20 	 0.10860 	 0.00025 	 MISS
   76 	    21 	 0.10962 	 0.00025 	 MISS
   71 	    22 	 0.10862 	 0.00026 	 MISS
   49 	    23 	 0.10602 	 0.00026 	 MISS
   61 	    24 	 0.10736 	 0.00026 	 MISS
    5 	    25 	 0.05372 	 0.00027 	 m..s
   69 	    26 	 0.10860 	 0.00028 	 MISS
   79 	    27 	 0.11253 	 0.00028 	 MISS
   82 	    28 	 0.11724 	 0.00030 	 MISS
   39 	    29 	 0.10528 	 0.00030 	 MISS
   50 	    30 	 0.10604 	 0.00031 	 MISS
   52 	    31 	 0.10639 	 0.00031 	 MISS
   78 	    32 	 0.11090 	 0.00033 	 MISS
    1 	    33 	 0.04991 	 0.00033 	 m..s
   15 	    34 	 0.05787 	 0.00034 	 m..s
   66 	    35 	 0.10815 	 0.00034 	 MISS
   41 	    36 	 0.10561 	 0.00036 	 MISS
   10 	    37 	 0.05663 	 0.00039 	 m..s
   20 	    38 	 0.05875 	 0.00042 	 m..s
   77 	    39 	 0.11064 	 0.00043 	 MISS
   12 	    40 	 0.05733 	 0.00049 	 m..s
   28 	    41 	 0.10474 	 0.00053 	 MISS
   56 	    42 	 0.10662 	 0.00064 	 MISS
    8 	    43 	 0.05617 	 0.00091 	 m..s
   45 	    44 	 0.10583 	 0.00106 	 MISS
   19 	    45 	 0.05867 	 0.00118 	 m..s
   10 	    46 	 0.05663 	 0.00119 	 m..s
   24 	    47 	 0.06376 	 0.00190 	 m..s
   41 	    48 	 0.10561 	 0.00206 	 MISS
    9 	    49 	 0.05648 	 0.00375 	 m..s
   13 	    50 	 0.05745 	 0.00476 	 m..s
   22 	    51 	 0.05899 	 0.00963 	 m..s
   22 	    52 	 0.05899 	 0.01114 	 m..s
   17 	    53 	 0.05815 	 0.01742 	 m..s
   80 	    54 	 0.11359 	 0.02922 	 m..s
   26 	    55 	 0.10309 	 0.10212 	 ~...
   37 	    56 	 0.10524 	 0.11403 	 ~...
   25 	    57 	 0.10249 	 0.12227 	 ~...
   34 	    58 	 0.10499 	 0.12252 	 ~...
   51 	    59 	 0.10614 	 0.12838 	 ~...
   59 	    60 	 0.10688 	 0.13157 	 ~...
   47 	    61 	 0.10590 	 0.13654 	 m..s
   84 	    62 	 0.22355 	 0.13811 	 m..s
   38 	    63 	 0.10528 	 0.13893 	 m..s
   27 	    64 	 0.10444 	 0.14021 	 m..s
   40 	    65 	 0.10557 	 0.14152 	 m..s
   74 	    66 	 0.10940 	 0.14195 	 m..s
   58 	    67 	 0.10677 	 0.14228 	 m..s
   30 	    68 	 0.10483 	 0.14549 	 m..s
   86 	    69 	 0.23225 	 0.14923 	 m..s
   83 	    70 	 0.12680 	 0.15238 	 ~...
   81 	    71 	 0.11529 	 0.15279 	 m..s
   46 	    72 	 0.10587 	 0.15294 	 m..s
   72 	    73 	 0.10882 	 0.15343 	 m..s
   33 	    74 	 0.10493 	 0.15518 	 m..s
   43 	    75 	 0.10574 	 0.15565 	 m..s
   35 	    76 	 0.10506 	 0.15697 	 m..s
   67 	    77 	 0.10845 	 0.15768 	 m..s
   44 	    78 	 0.10581 	 0.15856 	 m..s
   74 	    79 	 0.10940 	 0.16301 	 m..s
   91 	    80 	 0.24661 	 0.16436 	 m..s
   87 	    81 	 0.23334 	 0.16472 	 m..s
   73 	    82 	 0.10931 	 0.16490 	 m..s
   48 	    83 	 0.10601 	 0.16703 	 m..s
   63 	    84 	 0.10760 	 0.16744 	 m..s
   65 	    85 	 0.10812 	 0.16902 	 m..s
   68 	    86 	 0.10851 	 0.17173 	 m..s
   94 	    87 	 0.24833 	 0.17630 	 m..s
   57 	    88 	 0.10673 	 0.17913 	 m..s
   93 	    89 	 0.24773 	 0.19309 	 m..s
   99 	    90 	 0.25241 	 0.20836 	 m..s
  101 	    91 	 0.25559 	 0.21272 	 m..s
  107 	    92 	 0.26912 	 0.21534 	 m..s
   97 	    93 	 0.25075 	 0.21705 	 m..s
  120 	    94 	 0.31496 	 0.21862 	 m..s
   92 	    95 	 0.24757 	 0.22405 	 ~...
  105 	    96 	 0.26559 	 0.22450 	 m..s
  108 	    97 	 0.27105 	 0.22741 	 m..s
  115 	    98 	 0.29855 	 0.23038 	 m..s
  110 	    99 	 0.28342 	 0.23125 	 m..s
  106 	   100 	 0.26828 	 0.23511 	 m..s
  111 	   101 	 0.28441 	 0.26251 	 ~...
  117 	   102 	 0.29923 	 0.27258 	 ~...
   99 	   103 	 0.25241 	 0.27314 	 ~...
   90 	   104 	 0.23464 	 0.27333 	 m..s
   85 	   105 	 0.22624 	 0.27768 	 m..s
   87 	   106 	 0.23334 	 0.28674 	 m..s
   94 	   107 	 0.24833 	 0.28735 	 m..s
  111 	   108 	 0.28441 	 0.28899 	 ~...
   89 	   109 	 0.23456 	 0.29109 	 m..s
  109 	   110 	 0.28238 	 0.30141 	 ~...
  117 	   111 	 0.29923 	 0.31099 	 ~...
   96 	   112 	 0.24957 	 0.33111 	 m..s
  103 	   113 	 0.25742 	 0.34211 	 m..s
  102 	   114 	 0.25641 	 0.34516 	 m..s
  104 	   115 	 0.26320 	 0.34745 	 m..s
   98 	   116 	 0.25111 	 0.35217 	 MISS
  114 	   117 	 0.29430 	 0.35294 	 m..s
  119 	   118 	 0.31107 	 0.36564 	 m..s
  115 	   119 	 0.29855 	 0.36606 	 m..s
  113 	   120 	 0.28991 	 0.36729 	 m..s
==========================================
r_mrr = 0.8545414209365845
r2_mrr = 0.6487069129943848
spearmanr_mrr@5 = 0.717979371547699
spearmanr_mrr@10 = 0.8760977983474731
spearmanr_mrr@50 = 0.8501259684562683
spearmanr_mrr@100 = 0.8755295872688293
spearmanr_mrr@All = 0.9017927646636963
==========================================
test time: 0.514
Done Testing dataset DBpedia50
total time taken: 184.90885615348816
training time taken: 180.11282420158386
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8545)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6487)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.7180)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.8761)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.8501)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8755)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9018)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.931510794456699}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 7032145443614279
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [866, 283, 496, 993, 941, 331, 1098, 131, 4, 740, 483, 908, 935, 774, 222, 221, 1114, 1155, 600, 112, 860, 629, 1067, 955, 217, 676, 85, 146, 1206, 270, 800, 594, 1182, 1167, 70, 767, 1143, 1009, 28, 339, 721, 44, 799, 356, 1141, 1078, 525, 9, 446, 148, 858, 406, 267, 651, 1148, 1005, 96, 1127, 383, 63, 364, 1178, 640, 705, 790, 957, 154, 23, 40, 390, 313, 1196, 759, 744, 350, 542, 1164, 598, 487, 426, 990, 887, 209, 127, 527, 752, 116, 1139, 902, 48, 820, 579, 1061, 129, 560, 715, 341, 913, 258, 241, 601, 1187, 208, 596, 829, 1157, 753, 1181, 295, 176, 193, 535, 617, 353, 65, 377, 561, 371, 999, 747, 891]
valid_ids (0): []
train_ids (1094): [852, 95, 595, 287, 338, 1109, 1211, 639, 569, 1100, 796, 500, 842, 1126, 683, 1119, 157, 87, 66, 697, 1016, 247, 1194, 907, 567, 1068, 144, 585, 630, 896, 155, 672, 1184, 196, 714, 756, 704, 628, 572, 304, 1004, 14, 1083, 815, 284, 865, 300, 387, 5, 205, 423, 449, 43, 194, 47, 97, 12, 849, 899, 180, 795, 807, 895, 279, 666, 855, 430, 1207, 1199, 361, 202, 987, 84, 848, 1104, 975, 973, 413, 1147, 691, 395, 792, 653, 1146, 174, 71, 164, 69, 564, 171, 328, 644, 877, 299, 39, 459, 810, 804, 605, 75, 956, 805, 86, 745, 1041, 937, 916, 537, 352, 1069, 1160, 929, 1195, 159, 893, 342, 710, 280, 102, 1032, 1081, 1163, 850, 519, 1076, 1172, 490, 551, 522, 187, 61, 699, 185, 769, 844, 200, 871, 107, 961, 124, 610, 351, 290, 378, 126, 737, 435, 1180, 302, 638, 686, 264, 190, 494, 1120, 1192, 186, 754, 637, 461, 477, 379, 559, 599, 1028, 183, 195, 1095, 854, 189, 346, 121, 1070, 897, 818, 1073, 108, 656, 322, 199, 411, 1110, 237, 486, 802, 372, 1091, 294, 1125, 192, 405, 997, 1133, 979, 1088, 969, 276, 659, 316, 1080, 1118, 652, 967, 1161, 349, 1062, 74, 924, 198, 832, 881, 265, 32, 388, 864, 689, 281, 252, 523, 964, 396, 1193, 197, 616, 803, 172, 938, 417, 528, 433, 974, 568, 1189, 354, 434, 1138, 562, 89, 251, 458, 288, 20, 1013, 268, 685, 647, 983, 761, 1102, 718, 272, 755, 340, 507, 847, 416, 584, 624, 45, 1071, 843, 403, 463, 91, 654, 243, 103, 622, 687, 158, 306, 55, 13, 64, 462, 3, 690, 359, 798, 440, 702, 720, 382, 986, 1027, 33, 249, 822, 1018, 481, 760, 1175, 976, 474, 1074, 374, 473, 1035, 757, 1144, 228, 188, 811, 1090, 750, 468, 256, 18, 266, 883, 489, 540, 0, 471, 46, 869, 840, 1152, 257, 904, 733, 150, 213, 1024, 1129, 175, 401, 472, 278, 381, 611, 1176, 402, 120, 1166, 936, 99, 557, 1072, 424, 688, 779, 271, 425, 436, 138, 137, 118, 214, 545, 1047, 1151, 1149, 1185, 565, 318, 447, 900, 602, 1077, 867, 1030, 49, 526, 26, 515, 1103, 890, 248, 1121, 548, 245, 778, 856, 708, 531, 1135, 1200, 1075, 587, 1170, 54, 771, 948, 207, 36, 173, 942, 370, 764, 879, 109, 15, 603, 658, 1053, 1064, 923, 1212, 94, 319, 184, 1036, 476, 1011, 736, 427, 100, 1168, 636, 231, 422, 835, 591, 1162, 563, 274, 674, 34, 825, 996, 119, 978, 376, 994, 170, 211, 1169, 1079, 592, 1115, 1165, 139, 791, 570, 857, 963, 934, 730, 586, 7, 442, 589, 648, 546, 24, 952, 226, 614, 597, 437, 577, 831, 541, 1210, 335, 38, 549, 385, 244, 125, 814, 837, 786, 503, 580, 882, 412, 410, 414, 1153, 552, 236, 375, 627, 136, 1038, 79, 166, 400, 1204, 457, 419, 1108, 880, 460, 10, 50, 1156, 52, 669, 218, 841, 408, 729, 1046, 1014, 233, 536, 1044, 497, 1128, 254, 1087, 431, 716, 661, 836, 315, 409, 875, 1101, 909, 991, 1105, 140, 1049, 163, 429, 1019, 332, 78, 393, 646, 132, 821, 817, 1150, 806, 1209, 445, 293, 113, 1132, 933, 928, 178, 360, 479, 111, 269, 1065, 451, 670, 945, 1171, 21, 1043, 235, 566, 1026, 41, 1066, 81, 210, 544, 513, 297, 492, 1015, 1099, 989, 1054, 839, 362, 859, 944, 1086, 777, 114, 134, 98, 732, 775, 225, 677, 524, 135, 246, 305, 530, 482, 301, 260, 147, 706, 1085, 1050, 925, 478, 1202, 62, 1021, 450, 262, 1116, 491, 863, 532, 1060, 1097, 418, 1145, 212, 1006, 1029, 696, 443, 72, 998, 204, 995, 366, 1158, 149, 152, 1039, 1188, 303, 992, 469, 224, 1117, 141, 607, 498, 980, 680, 724, 845, 735, 58, 327, 741, 663, 220, 191, 618, 499, 1023, 444, 684, 671, 939, 898, 53, 613, 1213, 931, 389, 165, 323, 307, 915, 312, 878, 1186, 593, 25, 123, 1, 60, 830, 355, 533, 398, 508, 739, 668, 1123, 1048, 678, 940, 160, 679, 550, 285, 731, 80, 358, 828, 1003, 884, 781, 665, 962, 1130, 438, 348, 960, 770, 547, 723, 971, 455, 1177, 619, 1051, 693, 620, 179, 151, 324, 277, 773, 784, 1063, 675, 454, 145, 694, 30, 888, 529, 329, 625, 512, 667, 325, 673, 853, 510, 922, 448, 35, 106, 206, 988, 682, 337, 475, 168, 711, 432, 851, 834, 167, 238, 788, 467, 465, 643, 582, 240, 1198, 912, 1179, 1010, 862, 576, 719, 16, 161, 368, 914, 615, 521, 110, 951, 336, 981, 727, 1058, 657, 292, 631, 1055, 1159, 1084, 1008, 1052, 762, 330, 230, 517, 894, 1089, 801, 509, 954, 259, 919, 573, 782, 789, 29, 984, 397, 484, 273, 751, 1096, 1190, 367, 365, 308, 612, 441, 1034, 1112, 493, 608, 333, 255, 950, 920, 6, 712, 1214, 793, 905, 311, 1092, 953, 321, 642, 581, 946, 326, 588, 664, 275, 1131, 972, 660, 17, 766, 809, 578, 813, 728, 345, 117, 816, 947, 886, 37, 1136, 1124, 966, 700, 373, 239, 558, 968, 142, 1033, 604, 115, 749, 504, 51, 932, 725, 452, 384, 1037, 310, 59, 68, 88, 1107, 153, 819, 133, 713, 1173, 420, 334, 1174, 317, 56, 758, 1042, 910, 1040, 2, 82, 538, 506, 609, 763, 543, 783, 1056, 11, 485, 965, 470, 776, 1022, 215, 949, 701, 906, 590, 872, 958, 917, 1106, 633, 539, 130, 520, 456, 812, 1017, 556, 380, 1183, 101, 911, 182, 76, 641, 219, 901, 386, 726, 128, 709, 19, 1059, 838, 394, 787, 514, 1201, 982, 282, 870, 343, 286, 428, 1093, 1007, 253, 407, 391, 1082, 742, 229, 927, 826, 505, 943, 42, 734, 681, 748, 583, 1140, 698, 518, 242, 27, 415, 892, 645, 309, 404, 1045, 466, 298, 808, 768, 861, 649, 743, 921, 31, 765, 703, 223, 846, 369, 553, 722, 344, 772, 291, 1025, 421, 480, 555, 626, 143, 1094, 738, 632, 1057, 511, 92, 650, 621, 985, 1197, 232, 216, 780, 73, 635, 623, 794, 250, 873, 692, 90, 464, 320, 717, 889, 868, 1001, 662, 105, 347, 314, 707, 575, 1002, 177, 169, 516, 823, 296, 1012, 1134, 234, 439, 57, 67, 574, 903, 162, 357, 824, 918, 930, 201, 1203, 1122, 1020, 488, 122, 1191, 926, 959, 501, 453, 104, 261, 797, 785, 363, 876, 289, 874, 606, 203, 1154, 970, 634, 1000, 263, 495, 1208, 156, 1205, 1113, 833, 227, 655, 695, 8, 1142, 885, 977, 1137, 83, 502, 571, 1111, 22, 392, 181, 827, 77, 93, 746, 399, 534, 1031, 554]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9375447042018908
the save name prefix for this run is:  chkpt-ID_9375447042018908_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 854
rank avg (pred): 0.513 +- 0.003
mrr vals (pred, true): 0.000, 0.008
batch losses (mrrl, rdl): 0.0, 0.0003758883

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 700
rank avg (pred): 0.397 +- 0.320
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 6.91292e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1214
rank avg (pred): 0.417 +- 0.332
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001236995

Epoch over!
epoch time: 12.069

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 133
rank avg (pred): 0.403 +- 0.327
mrr vals (pred, true): 0.001, 0.162
batch losses (mrrl, rdl): 0.0, 0.0001367733

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.386 +- 0.321
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001156295

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 314
rank avg (pred): 0.167 +- 0.287
mrr vals (pred, true): 0.006, 0.273
batch losses (mrrl, rdl): 0.0, 6.8699e-06

Epoch over!
epoch time: 12.067

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1013
rank avg (pred): 0.405 +- 0.321
mrr vals (pred, true): 0.001, 0.177
batch losses (mrrl, rdl): 0.0, 0.0001372772

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 86
rank avg (pred): 0.416 +- 0.326
mrr vals (pred, true): 0.001, 0.137
batch losses (mrrl, rdl): 0.0, 0.0002141784

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.358 +- 0.319
mrr vals (pred, true): 0.003, 0.154
batch losses (mrrl, rdl): 0.0, 1.09422e-05

Epoch over!
epoch time: 11.866

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 534
rank avg (pred): 0.285 +- 0.305
mrr vals (pred, true): 0.004, 0.188
batch losses (mrrl, rdl): 0.0, 1.13261e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 538
rank avg (pred): 0.264 +- 0.314
mrr vals (pred, true): 0.022, 0.191
batch losses (mrrl, rdl): 0.0, 1.64906e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1045
rank avg (pred): 0.380 +- 0.318
mrr vals (pred, true): 0.012, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002154241

Epoch over!
epoch time: 11.838

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.105 +- 0.213
mrr vals (pred, true): 0.119, 0.363
batch losses (mrrl, rdl): 0.0, 9.519e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.197 +- 0.289
mrr vals (pred, true): 0.056, 0.194
batch losses (mrrl, rdl): 0.0, 4.41713e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 320
rank avg (pred): 0.165 +- 0.294
mrr vals (pred, true): 0.073, 0.219
batch losses (mrrl, rdl): 0.0, 1.13999e-05

Epoch over!
epoch time: 11.831

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 382
rank avg (pred): 0.416 +- 0.319
mrr vals (pred, true): 0.005, 0.130
batch losses (mrrl, rdl): 0.156504482, 0.0001452622

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.361 +- 0.245
mrr vals (pred, true): 0.122, 0.000
batch losses (mrrl, rdl): 0.0523207895, 0.0001255828

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 232
rank avg (pred): 0.416 +- 0.299
mrr vals (pred, true): 0.113, 0.000
batch losses (mrrl, rdl): 0.0390826017, 8.86987e-05

Epoch over!
epoch time: 11.955

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 306
rank avg (pred): 0.198 +- 0.268
mrr vals (pred, true): 0.231, 0.163
batch losses (mrrl, rdl): 0.04642611, 8.1991e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 325
rank avg (pred): 0.391 +- 0.284
mrr vals (pred, true): 0.095, 0.169
batch losses (mrrl, rdl): 0.0547974966, 0.0001166873

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 20
rank avg (pred): 0.144 +- 0.258
mrr vals (pred, true): 0.336, 0.335
batch losses (mrrl, rdl): 1.58388e-05, 7.45433e-05

Epoch over!
epoch time: 12.091

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 223
rank avg (pred): 0.429 +- 0.310
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.024679957, 6.89124e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.548 +- 0.344
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0091672763, 0.0011710839

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 639
rank avg (pred): 0.456 +- 0.314
mrr vals (pred, true): 0.083, 0.155
batch losses (mrrl, rdl): 0.052621942, 5.74557e-05

Epoch over!
epoch time: 12.05

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.113 +- 0.177
mrr vals (pred, true): 0.302, 0.364
batch losses (mrrl, rdl): 0.0384820364, 1.85269e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 776
rank avg (pred): 0.486 +- 0.299
mrr vals (pred, true): 0.079, 0.019
batch losses (mrrl, rdl): 0.0083995387, 0.0001688321

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.398 +- 0.277
mrr vals (pred, true): 0.098, 0.000
batch losses (mrrl, rdl): 0.0233547185, 0.0001473161

Epoch over!
epoch time: 12.247

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1000
rank avg (pred): 0.399 +- 0.259
mrr vals (pred, true): 0.089, 0.147
batch losses (mrrl, rdl): 0.0328185596, 0.00025143

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 952
rank avg (pred): 0.641 +- 0.269
mrr vals (pred, true): 0.028, 0.000
batch losses (mrrl, rdl): 0.0047594733, 0.0003542878

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 877
rank avg (pred): 0.479 +- 0.272
mrr vals (pred, true): 0.068, 0.000
batch losses (mrrl, rdl): 0.0032303389, 2.8757e-06

Epoch over!
epoch time: 11.907

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 739
rank avg (pred): 0.391 +- 0.285
mrr vals (pred, true): 0.106, 0.123
batch losses (mrrl, rdl): 0.0028466978, 0.0006492203

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 485
rank avg (pred): 0.420 +- 0.275
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0142316595, 5.68087e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 492
rank avg (pred): 0.210 +- 0.348
mrr vals (pred, true): 0.268, 0.262
batch losses (mrrl, rdl): 0.0003646225, 5.54308e-05

Epoch over!
epoch time: 12.101

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 619
rank avg (pred): 0.458 +- 0.307
mrr vals (pred, true): 0.079, 0.147
batch losses (mrrl, rdl): 0.0469024815, 0.0002221217

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 731
rank avg (pred): 0.388 +- 0.243
mrr vals (pred, true): 0.098, 0.053
batch losses (mrrl, rdl): 0.022870956, 0.0007118846

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 827
rank avg (pred): 0.283 +- 0.230
mrr vals (pred, true): 0.124, 0.205
batch losses (mrrl, rdl): 0.0647616014, 0.0001734274

Epoch over!
epoch time: 12.064

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 775
rank avg (pred): 0.492 +- 0.266
mrr vals (pred, true): 0.068, 0.010
batch losses (mrrl, rdl): 0.0031567826, 0.0002479389

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1088
rank avg (pred): 0.380 +- 0.253
mrr vals (pred, true): 0.099, 0.181
batch losses (mrrl, rdl): 0.0662339106, 0.0001474028

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.366 +- 0.238
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0313975736, 0.0004261842

Epoch over!
epoch time: 12.227

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.490 +- 0.259
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0028238562, 1.65435e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.398 +- 0.270
mrr vals (pred, true): 0.103, 0.177
batch losses (mrrl, rdl): 0.0541533083, 0.0001511472

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1027
rank avg (pred): 0.375 +- 0.246
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0263145678, 0.0002076487

Epoch over!
epoch time: 11.798

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.135 +- 0.186
mrr vals (pred, true): 0.288, 0.273
batch losses (mrrl, rdl): 0.0023972304, 1.09268e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1066
rank avg (pred): 0.145 +- 0.199
mrr vals (pred, true): 0.312, 0.312
batch losses (mrrl, rdl): 2.148e-07, 1.4328e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 5
rank avg (pred): 0.132 +- 0.246
mrr vals (pred, true): 0.354, 0.345
batch losses (mrrl, rdl): 0.0007483393, 6.374e-06

Epoch over!
epoch time: 12.026

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.513 +- 0.238
mrr vals (pred, true): 0.050, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.06261 	 5e-0500 	 m..s
    6 	     1 	 0.06242 	 5e-0500 	 m..s
    4 	     2 	 0.05784 	 0.00013 	 m..s
   57 	     3 	 0.10146 	 0.00017 	 MISS
   58 	     4 	 0.10155 	 0.00018 	 MISS
   10 	     5 	 0.06635 	 0.00019 	 m..s
   62 	     6 	 0.10196 	 0.00021 	 MISS
   39 	     7 	 0.09734 	 0.00022 	 m..s
   54 	     8 	 0.10119 	 0.00022 	 MISS
   74 	     9 	 0.10497 	 0.00023 	 MISS
   61 	    10 	 0.10188 	 0.00023 	 MISS
   16 	    11 	 0.09011 	 0.00023 	 m..s
   41 	    12 	 0.09764 	 0.00024 	 m..s
    3 	    13 	 0.05008 	 0.00025 	 m..s
   53 	    14 	 0.10102 	 0.00026 	 MISS
   60 	    15 	 0.10184 	 0.00027 	 MISS
   34 	    16 	 0.09564 	 0.00028 	 m..s
   75 	    17 	 0.10525 	 0.00031 	 MISS
    5 	    18 	 0.05881 	 0.00032 	 m..s
   40 	    19 	 0.09744 	 0.00032 	 m..s
    1 	    20 	 0.04050 	 0.00037 	 m..s
    2 	    21 	 0.04740 	 0.00042 	 m..s
   29 	    22 	 0.09393 	 0.00043 	 m..s
   69 	    23 	 0.10264 	 0.00043 	 MISS
   36 	    24 	 0.09623 	 0.00043 	 m..s
    0 	    25 	 0.04045 	 0.00063 	 m..s
   55 	    26 	 0.10119 	 0.00066 	 MISS
    8 	    27 	 0.06386 	 0.00072 	 m..s
   13 	    28 	 0.07033 	 0.00089 	 m..s
   14 	    29 	 0.07110 	 0.00201 	 m..s
   31 	    30 	 0.09421 	 0.00206 	 m..s
   30 	    31 	 0.09401 	 0.00257 	 m..s
   20 	    32 	 0.09127 	 0.00261 	 m..s
   15 	    33 	 0.08487 	 0.00336 	 m..s
   12 	    34 	 0.06787 	 0.00476 	 m..s
   27 	    35 	 0.09331 	 0.00861 	 m..s
    9 	    36 	 0.06401 	 0.01370 	 m..s
   27 	    37 	 0.09331 	 0.02598 	 m..s
   11 	    38 	 0.06713 	 0.02709 	 m..s
   80 	    39 	 0.13027 	 0.07969 	 m..s
   35 	    40 	 0.09570 	 0.10262 	 ~...
   82 	    41 	 0.15001 	 0.10268 	 m..s
   21 	    42 	 0.09143 	 0.10737 	 ~...
   38 	    43 	 0.09661 	 0.11961 	 ~...
   18 	    44 	 0.09061 	 0.12135 	 m..s
   22 	    45 	 0.09153 	 0.12273 	 m..s
   17 	    46 	 0.09030 	 0.12381 	 m..s
   46 	    47 	 0.09797 	 0.12420 	 ~...
   83 	    48 	 0.15089 	 0.12785 	 ~...
   42 	    49 	 0.09767 	 0.12880 	 m..s
   47 	    50 	 0.09808 	 0.12922 	 m..s
   44 	    51 	 0.09769 	 0.13210 	 m..s
   78 	    52 	 0.10548 	 0.13227 	 ~...
   33 	    53 	 0.09488 	 0.13238 	 m..s
   25 	    54 	 0.09279 	 0.13254 	 m..s
   85 	    55 	 0.17822 	 0.13345 	 m..s
   22 	    56 	 0.09153 	 0.13434 	 m..s
   24 	    57 	 0.09165 	 0.13665 	 m..s
   32 	    58 	 0.09452 	 0.13749 	 m..s
   25 	    59 	 0.09279 	 0.13800 	 m..s
   79 	    60 	 0.12951 	 0.13961 	 ~...
   84 	    61 	 0.17741 	 0.14150 	 m..s
   71 	    62 	 0.10383 	 0.14228 	 m..s
   64 	    63 	 0.10216 	 0.14309 	 m..s
   37 	    64 	 0.09656 	 0.14497 	 m..s
   67 	    65 	 0.10249 	 0.14525 	 m..s
   51 	    66 	 0.09994 	 0.14656 	 m..s
   19 	    67 	 0.09068 	 0.14665 	 m..s
   65 	    68 	 0.10218 	 0.14680 	 m..s
   80 	    69 	 0.13027 	 0.14932 	 ~...
   48 	    70 	 0.09843 	 0.15606 	 m..s
   50 	    71 	 0.09987 	 0.15773 	 m..s
   76 	    72 	 0.10526 	 0.15831 	 m..s
   76 	    73 	 0.10526 	 0.15883 	 m..s
   68 	    74 	 0.10250 	 0.16376 	 m..s
   72 	    75 	 0.10478 	 0.16396 	 m..s
   49 	    76 	 0.09934 	 0.16476 	 m..s
   45 	    77 	 0.09794 	 0.16562 	 m..s
   56 	    78 	 0.10135 	 0.16629 	 m..s
   66 	    79 	 0.10240 	 0.16741 	 m..s
   70 	    80 	 0.10306 	 0.16900 	 m..s
   72 	    81 	 0.10478 	 0.17246 	 m..s
   87 	    82 	 0.18042 	 0.17470 	 ~...
   52 	    83 	 0.10100 	 0.17491 	 m..s
   59 	    84 	 0.10167 	 0.17511 	 m..s
   63 	    85 	 0.10210 	 0.17704 	 m..s
   88 	    86 	 0.18603 	 0.17790 	 ~...
   42 	    87 	 0.09767 	 0.17807 	 m..s
   91 	    88 	 0.20038 	 0.18550 	 ~...
   89 	    89 	 0.19946 	 0.18655 	 ~...
   97 	    90 	 0.23992 	 0.18756 	 m..s
   86 	    91 	 0.17963 	 0.19185 	 ~...
   99 	    92 	 0.24005 	 0.21250 	 ~...
  100 	    93 	 0.24083 	 0.21272 	 ~...
   94 	    94 	 0.23635 	 0.21714 	 ~...
   92 	    95 	 0.23563 	 0.21750 	 ~...
   95 	    96 	 0.23638 	 0.22405 	 ~...
   98 	    97 	 0.23995 	 0.22461 	 ~...
   96 	    98 	 0.23987 	 0.22818 	 ~...
   89 	    99 	 0.19946 	 0.23497 	 m..s
  102 	   100 	 0.25080 	 0.23511 	 ~...
  101 	   101 	 0.24469 	 0.24128 	 ~...
  104 	   102 	 0.25344 	 0.24172 	 ~...
  106 	   103 	 0.25559 	 0.24229 	 ~...
  111 	   104 	 0.28973 	 0.24977 	 m..s
  107 	   105 	 0.26523 	 0.25390 	 ~...
  103 	   106 	 0.25199 	 0.25673 	 ~...
  110 	   107 	 0.28000 	 0.26466 	 ~...
  111 	   108 	 0.28973 	 0.27311 	 ~...
  109 	   109 	 0.27942 	 0.27412 	 ~...
  107 	   110 	 0.26523 	 0.27768 	 ~...
  114 	   111 	 0.30830 	 0.31189 	 ~...
   93 	   112 	 0.23631 	 0.31337 	 m..s
  105 	   113 	 0.25347 	 0.31455 	 m..s
  113 	   114 	 0.30508 	 0.31820 	 ~...
  116 	   115 	 0.33667 	 0.33015 	 ~...
  115 	   116 	 0.33153 	 0.35217 	 ~...
  117 	   117 	 0.34336 	 0.35294 	 ~...
  117 	   118 	 0.34336 	 0.35651 	 ~...
  119 	   119 	 0.34862 	 0.36435 	 ~...
  120 	   120 	 0.36990 	 0.39385 	 ~...
==========================================
r_mrr = 0.852393627166748
r2_mrr = 0.7070749402046204
spearmanr_mrr@5 = 0.9853039979934692
spearmanr_mrr@10 = 0.9584036469459534
spearmanr_mrr@50 = 0.9481251239776611
spearmanr_mrr@100 = 0.877406895160675
spearmanr_mrr@All = 0.8920564651489258
==========================================
test time: 0.406
Done Testing dataset DBpedia50
total time taken: 185.2624475955963
training time taken: 180.60560655593872
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8524)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.7071)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9853)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9584)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9481)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8774)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8921)}}, 'test_loss': {'DistMult': {'DBpedia50': 1.973029766113541}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 5928191506465951
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1135, 1199, 168, 429, 1120, 827, 387, 1206, 760, 331, 852, 1108, 686, 344, 118, 819, 573, 198, 963, 935, 650, 1067, 718, 40, 841, 223, 643, 422, 156, 237, 67, 329, 1211, 473, 1064, 84, 1082, 224, 350, 663, 17, 1087, 561, 1142, 96, 332, 315, 811, 971, 375, 1198, 1008, 29, 820, 221, 220, 5, 148, 1066, 238, 404, 201, 778, 232, 688, 144, 1146, 165, 1012, 1157, 879, 873, 709, 536, 277, 882, 1197, 498, 524, 1192, 63, 788, 931, 653, 1193, 771, 388, 151, 670, 863, 1007, 636, 314, 776, 728, 496, 349, 185, 124, 494, 732, 154, 541, 1075, 1098, 533, 504, 1138, 587, 393, 607, 1046, 854, 608, 1116, 825, 1140, 469, 559, 463, 929]
valid_ids (0): []
train_ids (1094): [1154, 1158, 697, 1004, 91, 359, 453, 632, 545, 209, 822, 327, 1043, 896, 952, 522, 680, 932, 729, 638, 207, 1041, 720, 48, 1048, 659, 321, 1028, 250, 475, 691, 791, 217, 481, 1065, 526, 601, 540, 745, 304, 438, 295, 600, 1056, 972, 391, 1024, 953, 99, 702, 976, 574, 1015, 73, 960, 141, 779, 907, 1068, 758, 560, 282, 310, 877, 1175, 131, 136, 507, 108, 994, 384, 80, 754, 377, 285, 137, 228, 83, 525, 354, 342, 252, 229, 780, 1152, 334, 748, 895, 642, 1011, 950, 455, 208, 848, 657, 180, 172, 1190, 757, 707, 1052, 139, 58, 916, 996, 117, 894, 1039, 385, 1110, 1131, 843, 1059, 170, 286, 1061, 736, 737, 428, 549, 1009, 472, 1047, 322, 248, 263, 784, 613, 122, 1148, 1074, 528, 818, 869, 126, 551, 897, 200, 20, 598, 694, 586, 785, 1072, 700, 685, 970, 977, 660, 1101, 1117, 408, 159, 288, 502, 793, 1111, 75, 775, 566, 509, 905, 888, 458, 519, 406, 413, 734, 465, 437, 770, 163, 986, 973, 520, 471, 581, 743, 340, 205, 495, 503, 347, 805, 910, 1121, 814, 476, 182, 324, 213, 405, 801, 564, 530, 28, 606, 1194, 578, 382, 262, 731, 13, 176, 677, 287, 576, 462, 77, 113, 152, 56, 162, 1070, 202, 194, 865, 920, 762, 817, 1210, 1164, 468, 21, 480, 1174, 1038, 962, 1079, 904, 641, 1153, 542, 240, 585, 426, 487, 478, 662, 49, 204, 22, 257, 753, 150, 1163, 491, 749, 1034, 701, 403, 1005, 992, 348, 328, 1107, 1205, 1088, 974, 192, 810, 596, 695, 1027, 42, 925, 1159, 772, 269, 889, 1080, 605, 312, 673, 804, 206, 203, 357, 837, 103, 956, 1089, 866, 450, 553, 712, 816, 308, 787, 957, 272, 386, 571, 371, 740, 742, 1172, 1014, 179, 330, 409, 23, 547, 16, 730, 158, 868, 421, 27, 146, 281, 72, 693, 361, 190, 1026, 548, 479, 1203, 1020, 713, 0, 358, 1130, 510, 1053, 1186, 696, 254, 887, 432, 947, 672, 452, 618, 768, 592, 32, 993, 639, 698, 111, 107, 355, 412, 800, 690, 1050, 1109, 591, 912, 433, 1042, 774, 764, 270, 726, 903, 1195, 50, 214, 362, 682, 197, 565, 95, 1171, 915, 186, 86, 24, 485, 537, 517, 934, 1168, 937, 69, 187, 1181, 265, 338, 374, 913, 647, 215, 1045, 946, 750, 497, 477, 1165, 18, 630, 756, 216, 1208, 851, 1114, 85, 292, 160, 789, 516, 134, 1123, 1118, 3, 1188, 30, 311, 886, 420, 1051, 918, 961, 241, 500, 181, 1083, 944, 235, 460, 449, 558, 634, 142, 431, 101, 246, 759, 346, 948, 501, 105, 883, 773, 893, 747, 554, 114, 400, 424, 876, 430, 583, 1139, 684, 703, 880, 562, 674, 279, 615, 809, 802, 299, 555, 881, 710, 267, 398, 1169, 864, 383, 389, 1161, 1078, 826, 378, 445, 43, 301, 1115, 1113, 425, 951, 518, 620, 579, 751, 723, 699, 922, 7, 37, 1173, 942, 652, 411, 644, 47, 336, 130, 1177, 651, 39, 1071, 1006, 1129, 623, 563, 1150, 390, 899, 1000, 399, 352, 489, 923, 293, 927, 26, 439, 323, 326, 178, 123, 1094, 557, 648, 884, 1214, 668, 964, 175, 1097, 546, 514, 829, 1081, 132, 366, 1049, 177, 872, 844, 830, 459, 975, 924, 183, 997, 980, 78, 61, 813, 1187, 998, 376, 656, 834, 954, 654, 1044, 264, 637, 544, 102, 958, 898, 319, 523, 681, 309, 965, 725, 616, 1084, 812, 933, 236, 1145, 906, 320, 368, 276, 531, 1162, 949, 10, 715, 1143, 741, 1191, 402, 512, 1076, 506, 333, 850, 414, 143, 629, 885, 38, 360, 856, 291, 1010, 1054, 1144, 225, 917, 149, 717, 396, 33, 456, 1096, 278, 302, 714, 840, 110, 767, 233, 724, 138, 577, 945, 534, 938, 766, 679, 369, 646, 1037, 247, 870, 335, 53, 966, 230, 318, 987, 835, 461, 129, 1207, 515, 658, 727, 1141, 892, 832, 1085, 1212, 849, 188, 256, 82, 51, 307, 604, 484, 664, 511, 853, 66, 54, 1147, 397, 1002, 116, 249, 457, 539, 88, 1178, 704, 735, 1092, 1030, 1200, 259, 943, 443, 1077, 1156, 394, 763, 1196, 245, 955, 807, 1183, 164, 752, 1023, 226, 171, 687, 1179, 588, 255, 1204, 610, 365, 45, 1073, 1100, 446, 31, 1069, 1060, 621, 100, 273, 676, 55, 1003, 87, 499, 9, 434, 1032, 612, 928, 984, 353, 550, 1018, 1149, 1151, 1189, 675, 625, 4, 1036, 106, 379, 859, 372, 417, 678, 441, 985, 351, 380, 769, 593, 59, 570, 218, 999, 968, 527, 8, 1033, 1134, 661, 930, 711, 1025, 871, 283, 891, 597, 1106, 153, 1063, 846, 665, 683, 855, 862, 76, 614, 967, 483, 602, 874, 305, 275, 1013, 1155, 983, 25, 234, 645, 1095, 568, 341, 211, 532, 622, 538, 1160, 911, 133, 173, 444, 1091, 231, 300, 1119, 782, 339, 721, 556, 251, 1176, 959, 15, 505, 990, 823, 890, 161, 940, 1167, 435, 1132, 492, 68, 781, 189, 1090, 392, 89, 222, 41, 803, 655, 1099, 258, 914, 635, 35, 795, 1055, 482, 464, 169, 640, 765, 529, 589, 1209, 120, 119, 988, 594, 611, 590, 11, 1062, 135, 689, 628, 1105, 12, 280, 466, 582, 706, 839, 261, 979, 196, 1125, 1136, 343, 1133, 57, 70, 857, 908, 325, 777, 6, 109, 289, 1126, 395, 410, 284, 867, 239, 794, 166, 381, 62, 290, 79, 244, 761, 294, 666, 833, 1127, 790, 858, 808, 436, 847, 861, 242, 1180, 755, 978, 824, 36, 508, 671, 1137, 991, 609, 364, 253, 1166, 448, 1016, 274, 367, 692, 792, 93, 191, 902, 783, 838, 1201, 667, 2, 617, 467, 34, 941, 981, 580, 939, 266, 901, 603, 212, 1040, 535, 451, 271, 454, 1035, 155, 97, 969, 370, 174, 52, 313, 297, 1029, 14, 722, 926, 401, 631, 81, 786, 633, 1001, 71, 860, 875, 316, 112, 486, 1019, 1093, 1, 921, 799, 64, 806, 317, 1104, 490, 104, 1086, 708, 982, 1124, 1031, 418, 828, 243, 145, 716, 419, 298, 909, 919, 1185, 738, 74, 744, 157, 575, 94, 797, 626, 552, 669, 845, 989, 1170, 1057, 219, 831, 337, 878, 569, 427, 1128, 345, 268, 733, 415, 798, 140, 46, 193, 65, 649, 842, 184, 1184, 115, 199, 296, 373, 900, 227, 1103, 260, 1182, 167, 1017, 1022, 44, 440, 1112, 1021, 493, 407, 599, 739, 521, 543, 1122, 125, 121, 1213, 719, 619, 513, 936, 836, 1202, 705, 1102, 92, 470, 447, 60, 442, 595, 363, 356, 416, 584, 474, 567, 147, 796, 1058, 19, 572, 90, 815, 488, 627, 127, 624, 210, 306, 746, 98, 128, 195, 995, 303, 821, 423]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8921355145104759
the save name prefix for this run is:  chkpt-ID_8921355145104759_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 467
rank avg (pred): 0.527 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001132163

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1011
rank avg (pred): 0.399 +- 0.231
mrr vals (pred, true): 0.152, 0.158
batch losses (mrrl, rdl): 0.0, 0.0002277958

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.222 +- 0.144
mrr vals (pred, true): 0.217, 0.080
batch losses (mrrl, rdl): 0.0, 9.46817e-05

Epoch over!
epoch time: 12.119

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 321
rank avg (pred): 0.182 +- 0.120
mrr vals (pred, true): 0.214, 0.231
batch losses (mrrl, rdl): 0.0, 7.88021e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.271 +- 0.216
mrr vals (pred, true): 0.243, 0.203
batch losses (mrrl, rdl): 0.0, 0.0001076505

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 74
rank avg (pred): 0.206 +- 0.167
mrr vals (pred, true): 0.220, 0.245
batch losses (mrrl, rdl): 0.0, 0.0001003146

Epoch over!
epoch time: 11.84

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 42
rank avg (pred): 0.165 +- 0.133
mrr vals (pred, true): 0.219, 0.250
batch losses (mrrl, rdl): 0.0, 4.09799e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 700
rank avg (pred): 0.435 +- 0.301
mrr vals (pred, true): 0.204, 0.000
batch losses (mrrl, rdl): 0.0, 1.26007e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 747
rank avg (pred): 0.229 +- 0.206
mrr vals (pred, true): 0.227, 0.080
batch losses (mrrl, rdl): 0.0, 7.65868e-05

Epoch over!
epoch time: 11.968

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1190
rank avg (pred): 0.375 +- 0.302
mrr vals (pred, true): 0.229, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002326946

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 22
rank avg (pred): 0.109 +- 0.109
mrr vals (pred, true): 0.247, 0.335
batch losses (mrrl, rdl): 0.0, 5.94298e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 691
rank avg (pred): 0.411 +- 0.324
mrr vals (pred, true): 0.224, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001141814

Epoch over!
epoch time: 11.846

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 600
rank avg (pred): 0.440 +- 0.309
mrr vals (pred, true): 0.185, 0.132
batch losses (mrrl, rdl): 0.0, 0.0002064516

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.456 +- 0.311
mrr vals (pred, true): 0.175, 0.153
batch losses (mrrl, rdl): 0.0, 0.0008332186

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 619
rank avg (pred): 0.407 +- 0.316
mrr vals (pred, true): 0.219, 0.147
batch losses (mrrl, rdl): 0.0, 0.0001400796

Epoch over!
epoch time: 11.776

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1038
rank avg (pred): 0.371 +- 0.332
mrr vals (pred, true): 0.229, 0.000
batch losses (mrrl, rdl): 0.3214040101, 0.0001317355

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 699
rank avg (pred): 0.549 +- 0.336
mrr vals (pred, true): 0.080, 0.000
batch losses (mrrl, rdl): 0.0090286359, 8.84622e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 865
rank avg (pred): 0.654 +- 0.245
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0003006755, 0.0005304431

Epoch over!
epoch time: 12.339

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 578
rank avg (pred): 0.541 +- 0.310
mrr vals (pred, true): 0.105, 0.127
batch losses (mrrl, rdl): 0.0048513296, 0.0010393704

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 900
rank avg (pred): 0.559 +- 0.300
mrr vals (pred, true): 0.082, 0.028
batch losses (mrrl, rdl): 0.0100560309, 0.0001440056

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 649
rank avg (pred): 0.554 +- 0.284
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0143755311, 5.51887e-05

Epoch over!
epoch time: 12.136

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.533 +- 0.294
mrr vals (pred, true): 0.088, 0.001
batch losses (mrrl, rdl): 0.0147327669, 5.61692e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 68
rank avg (pred): 0.157 +- 0.152
mrr vals (pred, true): 0.251, 0.233
batch losses (mrrl, rdl): 0.0034170221, 3.51603e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 212
rank avg (pred): 0.504 +- 0.291
mrr vals (pred, true): 0.078, 0.000
batch losses (mrrl, rdl): 0.0075747902, 2.48887e-05

Epoch over!
epoch time: 12.045

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 881
rank avg (pred): 0.606 +- 0.215
mrr vals (pred, true): 0.058, 0.000
batch losses (mrrl, rdl): 0.0005819362, 0.0005170361

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.479 +- 0.288
mrr vals (pred, true): 0.093, 0.000
batch losses (mrrl, rdl): 0.0182135664, 3.08755e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.474 +- 0.280
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0223309472, 8.4082e-06

Epoch over!
epoch time: 11.984

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 402
rank avg (pred): 0.464 +- 0.285
mrr vals (pred, true): 0.095, 0.178
batch losses (mrrl, rdl): 0.0692344382, 0.0004467719

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.446 +- 0.285
mrr vals (pred, true): 0.098, 0.000
batch losses (mrrl, rdl): 0.0228524748, 2.4198e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 206
rank avg (pred): 0.426 +- 0.282
mrr vals (pred, true): 0.114, 0.000
batch losses (mrrl, rdl): 0.0410122946, 1.17136e-05

Epoch over!
epoch time: 12.168

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.437 +- 0.272
mrr vals (pred, true): 0.115, 0.177
batch losses (mrrl, rdl): 0.0385956243, 0.000366239

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 576
rank avg (pred): 0.473 +- 0.241
mrr vals (pred, true): 0.086, 0.135
batch losses (mrrl, rdl): 0.0234265029, 0.0002719134

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 163
rank avg (pred): 0.419 +- 0.267
mrr vals (pred, true): 0.105, 0.000
batch losses (mrrl, rdl): 0.0303192064, 5.67156e-05

Epoch over!
epoch time: 12.125

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 705
rank avg (pred): 0.459 +- 0.237
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0106649287, 3.37034e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.402 +- 0.264
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0288450178, 0.0001088868

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 704
rank avg (pred): 0.448 +- 0.227
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0272006616, 6.07786e-05

Epoch over!
epoch time: 12.138

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1102
rank avg (pred): 0.384 +- 0.259
mrr vals (pred, true): 0.120, 0.141
batch losses (mrrl, rdl): 0.0043239249, 0.0001831795

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 347
rank avg (pred): 0.403 +- 0.246
mrr vals (pred, true): 0.089, 0.171
batch losses (mrrl, rdl): 0.0676970258, 0.0002912502

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 240
rank avg (pred): 0.379 +- 0.251
mrr vals (pred, true): 0.127, 0.001
batch losses (mrrl, rdl): 0.058835987, 0.0001010321

Epoch over!
epoch time: 12.157

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 14
rank avg (pred): 0.016 +- 0.015
mrr vals (pred, true): 0.358, 0.347
batch losses (mrrl, rdl): 0.0010792451, 0.0003528195

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.388 +- 0.242
mrr vals (pred, true): 0.107, 0.192
batch losses (mrrl, rdl): 0.0730347782, 0.0002106406

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1092
rank avg (pred): 0.405 +- 0.233
mrr vals (pred, true): 0.083, 0.167
batch losses (mrrl, rdl): 0.0713679343, 0.0004173194

Epoch over!
epoch time: 12.279

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 376
rank avg (pred): 0.409 +- 0.226
mrr vals (pred, true): 0.104, 0.171
batch losses (mrrl, rdl): 0.0444949195, 0.0002327383

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 548
rank avg (pred): 0.341 +- 0.247
mrr vals (pred, true): 0.168, 0.185
batch losses (mrrl, rdl): 0.0027511537, 0.0002110278

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 6
rank avg (pred): 0.029 +- 0.027
mrr vals (pred, true): 0.334, 0.316
batch losses (mrrl, rdl): 0.0032138876, 0.0002716854

Epoch over!
epoch time: 11.952

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.474 +- 0.388
mrr vals (pred, true): 0.208, 0.280

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    7 	     0 	 0.04889 	 5e-0500 	 m..s
    4 	     1 	 0.04886 	 6e-0500 	 m..s
   10 	     2 	 0.04934 	 0.00013 	 m..s
   23 	     3 	 0.08686 	 0.00013 	 m..s
   21 	     4 	 0.08617 	 0.00014 	 m..s
   12 	     5 	 0.04950 	 0.00015 	 m..s
   67 	     6 	 0.10097 	 0.00015 	 MISS
   69 	     7 	 0.10157 	 0.00016 	 MISS
   25 	     8 	 0.08703 	 0.00017 	 m..s
   42 	     9 	 0.09512 	 0.00018 	 m..s
   32 	    10 	 0.09226 	 0.00019 	 m..s
   43 	    11 	 0.09532 	 0.00019 	 m..s
    2 	    12 	 0.04885 	 0.00019 	 m..s
   72 	    13 	 0.10185 	 0.00020 	 MISS
   65 	    14 	 0.10085 	 0.00021 	 MISS
    9 	    15 	 0.04926 	 0.00021 	 m..s
   72 	    16 	 0.10185 	 0.00022 	 MISS
   74 	    17 	 0.10190 	 0.00022 	 MISS
   60 	    18 	 0.09948 	 0.00023 	 m..s
   22 	    19 	 0.08662 	 0.00023 	 m..s
   41 	    20 	 0.09511 	 0.00023 	 m..s
   39 	    21 	 0.09488 	 0.00023 	 m..s
   30 	    22 	 0.09119 	 0.00023 	 m..s
   65 	    23 	 0.10085 	 0.00024 	 MISS
    1 	    24 	 0.04885 	 0.00024 	 m..s
   51 	    25 	 0.09721 	 0.00025 	 m..s
   81 	    26 	 0.10292 	 0.00025 	 MISS
   49 	    27 	 0.09671 	 0.00025 	 m..s
   46 	    28 	 0.09553 	 0.00025 	 m..s
   19 	    29 	 0.08277 	 0.00026 	 m..s
   44 	    30 	 0.09533 	 0.00026 	 m..s
   78 	    31 	 0.10257 	 0.00026 	 MISS
   24 	    32 	 0.08696 	 0.00027 	 m..s
   26 	    33 	 0.08739 	 0.00027 	 m..s
   50 	    34 	 0.09713 	 0.00027 	 m..s
   45 	    35 	 0.09552 	 0.00028 	 m..s
    3 	    36 	 0.04886 	 0.00028 	 m..s
   47 	    37 	 0.09558 	 0.00029 	 m..s
   81 	    38 	 0.10292 	 0.00029 	 MISS
   34 	    39 	 0.09278 	 0.00032 	 m..s
   85 	    40 	 0.10511 	 0.00034 	 MISS
    6 	    41 	 0.04889 	 0.00035 	 m..s
   68 	    42 	 0.10136 	 0.00042 	 MISS
    8 	    43 	 0.04924 	 0.00046 	 m..s
   77 	    44 	 0.10219 	 0.00050 	 MISS
   13 	    45 	 0.04959 	 0.00051 	 m..s
   61 	    46 	 0.09975 	 0.00069 	 m..s
   38 	    47 	 0.09416 	 0.00112 	 m..s
    0 	    48 	 0.04885 	 0.00457 	 m..s
   17 	    49 	 0.05966 	 0.00504 	 m..s
    5 	    50 	 0.04887 	 0.00554 	 m..s
   14 	    51 	 0.04968 	 0.00826 	 m..s
   16 	    52 	 0.05031 	 0.00828 	 m..s
   15 	    53 	 0.05025 	 0.00828 	 m..s
   11 	    54 	 0.04949 	 0.01885 	 m..s
   18 	    55 	 0.06110 	 0.05864 	 ~...
   27 	    56 	 0.08782 	 0.13256 	 m..s
   31 	    57 	 0.09196 	 0.13393 	 m..s
   29 	    58 	 0.08825 	 0.13457 	 m..s
   40 	    59 	 0.09510 	 0.13794 	 m..s
   84 	    60 	 0.10317 	 0.14050 	 m..s
   91 	    61 	 0.15469 	 0.14103 	 ~...
   89 	    62 	 0.10620 	 0.14150 	 m..s
   70 	    63 	 0.10163 	 0.14228 	 m..s
   86 	    64 	 0.10529 	 0.14416 	 m..s
   71 	    65 	 0.10182 	 0.14423 	 m..s
   48 	    66 	 0.09599 	 0.14632 	 m..s
   64 	    67 	 0.10039 	 0.14639 	 m..s
   36 	    68 	 0.09392 	 0.14995 	 m..s
   87 	    69 	 0.10531 	 0.15436 	 m..s
   75 	    70 	 0.10208 	 0.15436 	 m..s
   27 	    71 	 0.08782 	 0.15541 	 m..s
   33 	    72 	 0.09245 	 0.15705 	 m..s
   62 	    73 	 0.10021 	 0.15883 	 m..s
   79 	    74 	 0.10283 	 0.15920 	 m..s
   20 	    75 	 0.08405 	 0.15999 	 m..s
   52 	    76 	 0.09747 	 0.16049 	 m..s
   58 	    77 	 0.09868 	 0.16184 	 m..s
   35 	    78 	 0.09353 	 0.16345 	 m..s
   56 	    79 	 0.09811 	 0.16444 	 m..s
   88 	    80 	 0.10542 	 0.16513 	 m..s
   57 	    81 	 0.09812 	 0.16582 	 m..s
   54 	    82 	 0.09749 	 0.16629 	 m..s
   76 	    83 	 0.10214 	 0.16741 	 m..s
   52 	    84 	 0.09747 	 0.16800 	 m..s
   59 	    85 	 0.09885 	 0.16903 	 m..s
   62 	    86 	 0.10021 	 0.17199 	 m..s
   36 	    87 	 0.09392 	 0.17511 	 m..s
   80 	    88 	 0.10287 	 0.17614 	 m..s
  104 	    89 	 0.20878 	 0.17630 	 m..s
   83 	    90 	 0.10316 	 0.17704 	 m..s
   55 	    91 	 0.09778 	 0.17728 	 m..s
   99 	    92 	 0.20080 	 0.18756 	 ~...
  101 	    93 	 0.20712 	 0.19185 	 ~...
   93 	    94 	 0.16782 	 0.20208 	 m..s
  100 	    95 	 0.20179 	 0.20377 	 ~...
   92 	    96 	 0.16522 	 0.20470 	 m..s
   90 	    97 	 0.15349 	 0.20620 	 m..s
  105 	    98 	 0.20946 	 0.20836 	 ~...
   94 	    99 	 0.18791 	 0.20837 	 ~...
   95 	   100 	 0.19840 	 0.21037 	 ~...
   97 	   101 	 0.20001 	 0.21750 	 ~...
   98 	   102 	 0.20006 	 0.22818 	 ~...
  118 	   103 	 0.27814 	 0.23038 	 m..s
  102 	   104 	 0.20779 	 0.24231 	 m..s
  111 	   105 	 0.25441 	 0.25390 	 ~...
   96 	   106 	 0.19899 	 0.25950 	 m..s
  107 	   107 	 0.21400 	 0.26212 	 m..s
  108 	   108 	 0.24569 	 0.26345 	 ~...
  112 	   109 	 0.25888 	 0.26422 	 ~...
  117 	   110 	 0.27310 	 0.26952 	 ~...
  106 	   111 	 0.21005 	 0.27292 	 m..s
  110 	   112 	 0.25372 	 0.27685 	 ~...
  103 	   113 	 0.20816 	 0.27975 	 m..s
  109 	   114 	 0.24790 	 0.29109 	 m..s
  113 	   115 	 0.25947 	 0.30005 	 m..s
  114 	   116 	 0.26449 	 0.31189 	 m..s
  116 	   117 	 0.27180 	 0.31202 	 m..s
  119 	   118 	 0.32522 	 0.33349 	 ~...
  115 	   119 	 0.27080 	 0.33444 	 m..s
  120 	   120 	 0.32690 	 0.34516 	 ~...
==========================================
r_mrr = 0.8174304366111755
r2_mrr = 0.6127789616584778
spearmanr_mrr@5 = 0.814490795135498
spearmanr_mrr@10 = 0.8602175712585449
spearmanr_mrr@50 = 0.9599713683128357
spearmanr_mrr@100 = 0.8230004906654358
spearmanr_mrr@All = 0.8599869608879089
==========================================
test time: 0.391
Done Testing dataset DBpedia50
total time taken: 186.0945394039154
training time taken: 181.32314085960388
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8174)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6128)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8145)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.8602)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9600)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8230)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8600)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.4633802361713606}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 3656779935486475
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [710, 466, 94, 638, 630, 732, 1135, 12, 485, 917, 27, 567, 415, 798, 830, 430, 958, 805, 660, 3, 521, 883, 327, 1130, 160, 729, 375, 1039, 557, 525, 494, 1067, 149, 661, 127, 357, 67, 680, 559, 666, 909, 442, 98, 1208, 433, 482, 65, 329, 460, 389, 89, 1085, 891, 642, 37, 1156, 617, 134, 48, 1174, 1004, 514, 93, 236, 1012, 137, 202, 694, 565, 508, 1209, 586, 759, 571, 69, 1066, 368, 1010, 331, 185, 701, 427, 859, 676, 394, 1098, 1026, 214, 738, 980, 689, 218, 831, 74, 1038, 566, 1069, 481, 601, 937, 151, 1163, 843, 1139, 355, 795, 869, 714, 32, 991, 363, 188, 632, 440, 416, 510, 952, 600, 498, 285, 83]
valid_ids (0): []
train_ids (1094): [1161, 682, 432, 106, 1164, 652, 441, 1013, 1118, 204, 112, 421, 1158, 988, 615, 1045, 283, 408, 211, 298, 978, 428, 92, 708, 944, 20, 742, 890, 913, 237, 1192, 324, 999, 108, 1047, 84, 1078, 1180, 463, 696, 450, 813, 266, 1068, 634, 70, 195, 295, 502, 374, 380, 531, 376, 971, 217, 471, 6, 29, 72, 781, 1083, 183, 1091, 912, 116, 81, 523, 1058, 301, 1185, 91, 670, 597, 168, 45, 964, 386, 848, 246, 648, 306, 656, 575, 817, 960, 1189, 393, 297, 1015, 947, 626, 14, 262, 101, 359, 837, 613, 325, 612, 923, 123, 943, 1082, 270, 103, 28, 1030, 500, 51, 370, 852, 1213, 871, 728, 649, 849, 725, 529, 975, 570, 910, 286, 665, 470, 903, 130, 469, 900, 867, 250, 113, 125, 782, 518, 820, 167, 233, 931, 1061, 724, 1207, 562, 1119, 181, 981, 955, 114, 86, 175, 267, 1194, 274, 886, 780, 135, 68, 1152, 300, 1172, 802, 213, 414, 207, 377, 655, 110, 778, 599, 1166, 367, 1142, 396, 606, 1199, 221, 703, 1170, 815, 954, 10, 446, 503, 877, 438, 1132, 337, 951, 646, 1196, 1147, 695, 138, 902, 105, 866, 834, 678, 497, 1086, 429, 119, 49, 953, 273, 618, 147, 206, 8, 1020, 1198, 1022, 894, 171, 602, 1044, 82, 225, 657, 673, 1017, 773, 833, 255, 174, 946, 145, 771, 814, 131, 46, 540, 179, 459, 102, 115, 53, 1033, 777, 737, 1094, 397, 1205, 1137, 687, 893, 936, 664, 604, 263, 1141, 581, 691, 487, 475, 1001, 406, 969, 751, 766, 230, 328, 1165, 749, 1120, 767, 579, 854, 1202, 240, 228, 349, 58, 924, 361, 139, 700, 530, 304, 1167, 38, 161, 2, 884, 824, 1053, 208, 986, 685, 410, 919, 22, 823, 532, 323, 401, 1123, 451, 847, 260, 807, 774, 492, 1128, 249, 922, 876, 744, 80, 305, 758, 23, 461, 107, 133, 775, 1150, 472, 353, 1063, 940, 42, 752, 1071, 637, 519, 235, 855, 1028, 520, 914, 573, 743, 57, 607, 403, 874, 704, 1102, 157, 1089, 1134, 1140, 662, 76, 166, 572, 790, 13, 395, 611, 568, 111, 636, 173, 1182, 289, 1151, 897, 987, 265, 199, 243, 278, 330, 495, 30, 287, 765, 473, 968, 339, 1073, 276, 580, 832, 784, 535, 934, 484, 281, 735, 383, 1116, 1023, 945, 681, 534, 811, 995, 268, 1007, 818, 574, 1181, 994, 851, 242, 616, 949, 513, 1168, 335, 210, 1065, 610, 245, 50, 804, 512, 722, 1059, 939, 545, 162, 789, 479, 1169, 41, 576, 1201, 1077, 344, 658, 1084, 1060, 384, 85, 1197, 311, 966, 378, 194, 750, 192, 1186, 358, 881, 672, 24, 476, 1131, 1173, 762, 862, 159, 983, 925, 589, 164, 794, 850, 785, 120, 290, 153, 1040, 15, 957, 1112, 241, 1021, 400, 1187, 1179, 1111, 1100, 1117, 382, 619, 60, 596, 1121, 223, 148, 1088, 836, 1153, 587, 1109, 1008, 78, 727, 561, 187, 603, 44, 803, 317, 522, 464, 756, 62, 822, 956, 845, 180, 716, 352, 643, 748, 984, 1171, 746, 1200, 783, 515, 592, 1075, 1003, 719, 1043, 840, 552, 184, 439, 1029, 40, 763, 392, 918, 261, 1037, 190, 163, 1146, 216, 1096, 404, 941, 282, 764, 313, 1154, 950, 52, 549, 399, 583, 713, 810, 829, 588, 935, 741, 129, 537, 55, 435, 1035, 828, 827, 799, 277, 996, 788, 684, 77, 248, 809, 244, 547, 1203, 598, 419, 585, 1122, 875, 1024, 226, 591, 257, 819, 594, 1042, 1124, 1108, 930, 455, 373, 64, 25, 620, 683, 1177, 1025, 916, 388, 315, 1101, 516, 635, 538, 808, 1188, 99, 787, 739, 895, 1211, 1110, 972, 674, 911, 640, 232, 797, 870, 677, 962, 431, 791, 1148, 842, 318, 959, 271, 170, 1133, 772, 224, 563, 34, 178, 320, 141, 452, 1081, 543, 668, 75, 182, 730, 448, 348, 4, 292, 1032, 1155, 443, 1127, 307, 189, 745, 1178, 191, 288, 905, 556, 321, 627, 709, 1079, 360, 66, 21, 504, 1074, 711, 1175, 857, 546, 569, 761, 932, 928, 1070, 254, 5, 688, 888, 693, 346, 861, 736, 126, 39, 1019, 593, 18, 553, 345, 391, 542, 731, 172, 467, 885, 1097, 524, 528, 308, 926, 155, 539, 247, 229, 825, 338, 302, 663, 31, 1064, 907, 47, 990, 631, 507, 898, 222, 333, 754, 623, 1195, 251, 614, 326, 203, 734, 465, 1095, 284, 63, 872, 793, 985, 712, 417, 906, 558, 239, 921, 517, 118, 371, 993, 365, 56, 1105, 733, 920, 154, 1184, 456, 444, 858, 555, 1190, 1036, 16, 671, 422, 364, 1176, 136, 122, 35, 454, 316, 697, 977, 908, 659, 1093, 294, 839, 757, 800, 54, 720, 96, 1062, 550, 901, 806, 641, 197, 915, 132, 760, 369, 879, 590, 629, 965, 1055, 645, 90, 156, 755, 970, 457, 622, 87, 434, 835, 411, 212, 379, 1113, 165, 420, 948, 124, 489, 726, 499, 1049, 989, 342, 973, 979, 385, 1092, 1183, 405, 816, 334, 942, 253, 200, 398, 1072, 275, 1002, 490, 846, 486, 477, 356, 1099, 1005, 1051, 279, 699, 1149, 625, 1126, 340, 209, 312, 605, 462, 1076, 653, 1048, 483, 844, 1046, 740, 792, 582, 838, 826, 201, 496, 1212, 480, 1103, 595, 474, 647, 882, 1125, 453, 309, 97, 1057, 1162, 1204, 564, 33, 584, 1, 707, 747, 9, 150, 272, 675, 1157, 215, 753, 887, 578, 1018, 1006, 445, 227, 544, 468, 1052, 413, 390, 1210, 1136, 896, 639, 258, 491, 144, 967, 280, 177, 776, 424, 865, 982, 880, 821, 624, 169, 650, 501, 142, 853, 95, 702, 205, 193, 362, 1000, 577, 1114, 1016, 1206, 259, 158, 873, 61, 963, 0, 878, 718, 447, 1193, 768, 1145, 992, 293, 933, 196, 1009, 509, 402, 997, 899, 79, 644, 234, 140, 73, 238, 252, 88, 1214, 43, 651, 864, 366, 146, 1143, 36, 104, 109, 1191, 1027, 927, 426, 381, 17, 770, 291, 1087, 220, 667, 387, 351, 723, 998, 621, 449, 863, 929, 121, 974, 299, 341, 198, 548, 11, 319, 526, 264, 1144, 536, 860, 554, 633, 628, 1138, 889, 1014, 527, 303, 609, 705, 493, 186, 1011, 7, 1104, 1080, 1159, 436, 706, 219, 343, 541, 892, 269, 347, 698, 296, 100, 856, 1107, 769, 1106, 1031, 721, 176, 412, 314, 407, 715, 505, 812, 1054, 152, 1115, 425, 117, 1050, 322, 654, 551, 26, 801, 437, 717, 256, 786, 59, 332, 1034, 533, 868, 904, 143, 669, 1090, 478, 372, 679, 418, 690, 511, 1056, 841, 938, 336, 458, 19, 350, 796, 71, 560, 310, 354, 961, 779, 231, 1160, 423, 409, 128, 1129, 608, 488, 506, 692, 1041, 686, 976]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3262413142098884
the save name prefix for this run is:  chkpt-ID_3262413142098884_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 976
rank avg (pred): 0.535 +- 0.006
mrr vals (pred, true): 0.000, 0.316
batch losses (mrrl, rdl): 0.0, 0.0032199309

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1195
rank avg (pred): 0.475 +- 0.003
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001328761

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.469 +- 0.002
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 9.74553e-05

Epoch over!
epoch time: 12.036

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 196
rank avg (pred): 0.387 +- 0.001
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004043902

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1132
rank avg (pred): 0.392 +- 0.002
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005164122

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.194 +- 0.001
mrr vals (pred, true): 0.000, 0.267
batch losses (mrrl, rdl): 0.0, 0.000105232

Epoch over!
epoch time: 11.69

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 677
rank avg (pred): 0.457 +- 0.001
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001686317

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 232
rank avg (pred): 0.446 +- 0.001
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001975668

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 750
rank avg (pred): 0.271 +- 0.000
mrr vals (pred, true): 0.000, 0.137
batch losses (mrrl, rdl): 0.0, 0.0002258157

Epoch over!
epoch time: 11.573

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.436 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002337463

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.421 +- 0.000
mrr vals (pred, true): 0.000, 0.158
batch losses (mrrl, rdl): 0.0, 0.0003736188

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1122
rank avg (pred): 0.407 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002346384

Epoch over!
epoch time: 11.773

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1162
rank avg (pred): 0.433 +- 0.000
mrr vals (pred, true): 0.000, 0.133
batch losses (mrrl, rdl): 0.0, 0.000205987

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 840
rank avg (pred): 0.491 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.00010862

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 109
rank avg (pred): 0.402 +- 0.000
mrr vals (pred, true): 0.000, 0.127
batch losses (mrrl, rdl): 0.0, 0.000312

Epoch over!
epoch time: 11.767

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 605
rank avg (pred): 0.453 +- 0.000
mrr vals (pred, true): 0.000, 0.125
batch losses (mrrl, rdl): 0.1560312659, 0.000310837

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 352
rank avg (pred): 0.425 +- 0.334
mrr vals (pred, true): 0.089, 0.137
batch losses (mrrl, rdl): 0.0230869744, 0.0001476427

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1165
rank avg (pred): 0.367 +- 0.351
mrr vals (pred, true): 0.084, 0.146
batch losses (mrrl, rdl): 0.0383700766, 3.77777e-05

Epoch over!
epoch time: 11.91

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 462
rank avg (pred): 0.483 +- 0.397
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0153896362, 7.38236e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 705
rank avg (pred): 0.446 +- 0.404
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0222393032, 0.0001479569

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 923
rank avg (pred): 0.769 +- 0.361
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0004015514, 8.08609e-05

Epoch over!
epoch time: 11.944

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 766
rank avg (pred): 0.630 +- 0.413
mrr vals (pred, true): 0.073, 0.011
batch losses (mrrl, rdl): 0.0053303638, 0.0010196423

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 16
rank avg (pred): 0.136 +- 0.340
mrr vals (pred, true): 0.370, 0.348
batch losses (mrrl, rdl): 0.0048702136, 4.05937e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.643 +- 0.403
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0027306171, 0.0005705119

Epoch over!
epoch time: 11.891

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1055
rank avg (pred): 0.138 +- 0.340
mrr vals (pred, true): 0.338, 0.330
batch losses (mrrl, rdl): 0.0007613216, 4.85701e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1096
rank avg (pred): 0.378 +- 0.402
mrr vals (pred, true): 0.104, 0.176
batch losses (mrrl, rdl): 0.0526943728, 3.10955e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.506 +- 0.419
mrr vals (pred, true): 0.097, 0.124
batch losses (mrrl, rdl): 0.0072511355, 0.000389383

Epoch over!
epoch time: 11.786

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 233
rank avg (pred): 0.489 +- 0.416
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0143522574, 0.0001017466

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 789
rank avg (pred): 0.801 +- 0.343
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001985181, 0.0013378817

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 719
rank avg (pred): 0.463 +- 0.375
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0211486109, 0.0001186989

Epoch over!
epoch time: 12.109

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1055
rank avg (pred): 0.125 +- 0.308
mrr vals (pred, true): 0.381, 0.330
batch losses (mrrl, rdl): 0.0265675131, 2.25214e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 248
rank avg (pred): 0.101 +- 0.207
mrr vals (pred, true): 0.349, 0.359
batch losses (mrrl, rdl): 0.001088684, 9.6207e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.398 +- 0.286
mrr vals (pred, true): 0.094, 0.003
batch losses (mrrl, rdl): 0.019136237, 4.24234e-05

Epoch over!
epoch time: 12.184

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 730
rank avg (pred): 0.537 +- 0.265
mrr vals (pred, true): 0.040, 0.067
batch losses (mrrl, rdl): 0.0009671318, 0.0026668671

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.123 +- 0.189
mrr vals (pred, true): 0.352, 0.363
batch losses (mrrl, rdl): 0.0012710532, 1.42602e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.295 +- 0.239
mrr vals (pred, true): 0.132, 0.000
batch losses (mrrl, rdl): 0.0675327182, 0.0004427927

Epoch over!
epoch time: 11.947

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 153
rank avg (pred): 0.416 +- 0.284
mrr vals (pred, true): 0.090, 0.155
batch losses (mrrl, rdl): 0.0430545025, 0.0005159021

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 123
rank avg (pred): 0.362 +- 0.250
mrr vals (pred, true): 0.101, 0.157
batch losses (mrrl, rdl): 0.0312801301, 0.0001224071

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 392
rank avg (pred): 0.354 +- 0.239
mrr vals (pred, true): 0.093, 0.169
batch losses (mrrl, rdl): 0.0572126657, 0.0001279968

Epoch over!
epoch time: 12.101

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 335
rank avg (pred): 0.339 +- 0.236
mrr vals (pred, true): 0.104, 0.174
batch losses (mrrl, rdl): 0.049195461, 5.50963e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.369 +- 0.260
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0272444189, 0.0001480885

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.367 +- 0.246
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0154438391, 3.75462e-05

Epoch over!
epoch time: 12.058

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1146
rank avg (pred): 0.261 +- 0.236
mrr vals (pred, true): 0.190, 0.204
batch losses (mrrl, rdl): 0.0019292086, 3.92194e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1202
rank avg (pred): 0.400 +- 0.260
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0161063522, 0.0001880184

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 125
rank avg (pred): 0.398 +- 0.279
mrr vals (pred, true): 0.109, 0.181
batch losses (mrrl, rdl): 0.0522648692, 0.0002133824

Epoch over!
epoch time: 11.793

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.383 +- 0.263
mrr vals (pred, true): 0.109, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   48 	     0 	 0.11810 	 0.00016 	 MISS
   85 	     1 	 0.14199 	 0.00016 	 MISS
   31 	     2 	 0.11198 	 0.00017 	 MISS
   36 	     3 	 0.11406 	 0.00017 	 MISS
   21 	     4 	 0.10938 	 0.00018 	 MISS
   22 	     5 	 0.10944 	 0.00018 	 MISS
   57 	     6 	 0.12111 	 0.00019 	 MISS
   29 	     7 	 0.11186 	 0.00019 	 MISS
   75 	     8 	 0.12913 	 0.00020 	 MISS
   84 	     9 	 0.14118 	 0.00020 	 MISS
   33 	    10 	 0.11247 	 0.00020 	 MISS
    8 	    11 	 0.06147 	 0.00021 	 m..s
   87 	    12 	 0.14721 	 0.00021 	 MISS
   52 	    13 	 0.11915 	 0.00021 	 MISS
   56 	    14 	 0.12050 	 0.00023 	 MISS
   87 	    15 	 0.14721 	 0.00027 	 MISS
   10 	    16 	 0.06211 	 0.00029 	 m..s
   63 	    17 	 0.12318 	 0.00029 	 MISS
    4 	    18 	 0.05487 	 0.00029 	 m..s
   75 	    19 	 0.12913 	 0.00030 	 MISS
   48 	    20 	 0.11810 	 0.00031 	 MISS
    1 	    21 	 0.04905 	 0.00033 	 m..s
   68 	    22 	 0.12472 	 0.00033 	 MISS
   14 	    23 	 0.06463 	 0.00034 	 m..s
   66 	    24 	 0.12391 	 0.00036 	 MISS
   82 	    25 	 0.13623 	 0.00036 	 MISS
   44 	    26 	 0.11552 	 0.00036 	 MISS
   68 	    27 	 0.12472 	 0.00038 	 MISS
   64 	    28 	 0.12331 	 0.00040 	 MISS
   17 	    29 	 0.10863 	 0.00040 	 MISS
   26 	    30 	 0.11038 	 0.00040 	 MISS
    0 	    31 	 0.04231 	 0.00042 	 m..s
   23 	    32 	 0.10983 	 0.00044 	 MISS
   53 	    33 	 0.11956 	 0.00047 	 MISS
   85 	    34 	 0.14199 	 0.00050 	 MISS
   50 	    35 	 0.11821 	 0.00053 	 MISS
    2 	    36 	 0.04926 	 0.00058 	 m..s
   42 	    37 	 0.11532 	 0.00061 	 MISS
    3 	    38 	 0.05275 	 0.00063 	 m..s
   28 	    39 	 0.11107 	 0.00065 	 MISS
    9 	    40 	 0.06205 	 0.00068 	 m..s
   13 	    41 	 0.06432 	 0.00118 	 m..s
   12 	    42 	 0.06369 	 0.00130 	 m..s
   18 	    43 	 0.10867 	 0.00257 	 MISS
   39 	    44 	 0.11472 	 0.00360 	 MISS
    6 	    45 	 0.05809 	 0.00504 	 m..s
   11 	    46 	 0.06231 	 0.00845 	 m..s
   15 	    47 	 0.07843 	 0.01645 	 m..s
    7 	    48 	 0.05892 	 0.02684 	 m..s
    5 	    49 	 0.05753 	 0.02767 	 ~...
   20 	    50 	 0.10875 	 0.10262 	 ~...
   73 	    51 	 0.12569 	 0.10683 	 ~...
   79 	    52 	 0.13287 	 0.11165 	 ~...
   38 	    53 	 0.11469 	 0.11583 	 ~...
   30 	    54 	 0.11194 	 0.12135 	 ~...
   32 	    55 	 0.11207 	 0.12289 	 ~...
   51 	    56 	 0.11887 	 0.12402 	 ~...
   24 	    57 	 0.10986 	 0.12681 	 ~...
   65 	    58 	 0.12348 	 0.12839 	 ~...
   16 	    59 	 0.10837 	 0.13210 	 ~...
   40 	    60 	 0.11476 	 0.13322 	 ~...
   24 	    61 	 0.10986 	 0.13507 	 ~...
   46 	    62 	 0.11646 	 0.13533 	 ~...
   27 	    63 	 0.11070 	 0.13636 	 ~...
   60 	    64 	 0.12243 	 0.13683 	 ~...
   37 	    65 	 0.11456 	 0.13990 	 ~...
   35 	    66 	 0.11301 	 0.14006 	 ~...
   58 	    67 	 0.12208 	 0.14228 	 ~...
   89 	    68 	 0.16546 	 0.14285 	 ~...
   77 	    69 	 0.13102 	 0.14657 	 ~...
   43 	    70 	 0.11540 	 0.14680 	 m..s
   45 	    71 	 0.11558 	 0.15177 	 m..s
   19 	    72 	 0.10871 	 0.15226 	 m..s
   70 	    73 	 0.12476 	 0.15518 	 m..s
   61 	    74 	 0.12271 	 0.15555 	 m..s
   83 	    75 	 0.14030 	 0.15705 	 ~...
   55 	    76 	 0.12035 	 0.15768 	 m..s
   34 	    77 	 0.11249 	 0.15920 	 m..s
   90 	    78 	 0.16885 	 0.15939 	 ~...
   62 	    79 	 0.12292 	 0.15939 	 m..s
   99 	    80 	 0.20957 	 0.16258 	 m..s
   41 	    81 	 0.11530 	 0.16327 	 m..s
   95 	    82 	 0.18825 	 0.16436 	 ~...
   72 	    83 	 0.12493 	 0.16513 	 m..s
   59 	    84 	 0.12218 	 0.16582 	 m..s
   54 	    85 	 0.11971 	 0.16703 	 m..s
   71 	    86 	 0.12488 	 0.16735 	 m..s
   47 	    87 	 0.11739 	 0.16927 	 m..s
   67 	    88 	 0.12471 	 0.16937 	 m..s
   77 	    89 	 0.13102 	 0.16980 	 m..s
   92 	    90 	 0.17135 	 0.17108 	 ~...
   91 	    91 	 0.17073 	 0.17470 	 ~...
   80 	    92 	 0.13310 	 0.17481 	 m..s
   81 	    93 	 0.13534 	 0.17511 	 m..s
   95 	    94 	 0.18825 	 0.17630 	 ~...
   93 	    95 	 0.17748 	 0.17963 	 ~...
  103 	    96 	 0.22409 	 0.18177 	 m..s
  100 	    97 	 0.21233 	 0.18865 	 ~...
   74 	    98 	 0.12749 	 0.19279 	 m..s
   94 	    99 	 0.17958 	 0.19391 	 ~...
   97 	   100 	 0.19073 	 0.21705 	 ~...
  105 	   101 	 0.23131 	 0.22152 	 ~...
   98 	   102 	 0.19613 	 0.22546 	 ~...
  102 	   103 	 0.21861 	 0.23829 	 ~...
  110 	   104 	 0.26437 	 0.24458 	 ~...
  109 	   105 	 0.26134 	 0.24670 	 ~...
  104 	   106 	 0.22785 	 0.25673 	 ~...
  101 	   107 	 0.21741 	 0.25950 	 m..s
  108 	   108 	 0.25263 	 0.27412 	 ~...
  111 	   109 	 0.27220 	 0.27685 	 ~...
  113 	   110 	 0.28643 	 0.27863 	 ~...
  107 	   111 	 0.24682 	 0.27975 	 m..s
  116 	   112 	 0.29394 	 0.28396 	 ~...
  112 	   113 	 0.28521 	 0.29109 	 ~...
  114 	   114 	 0.28757 	 0.31189 	 ~...
  114 	   115 	 0.28757 	 0.31202 	 ~...
  106 	   116 	 0.24478 	 0.31337 	 m..s
  118 	   117 	 0.31096 	 0.31887 	 ~...
  117 	   118 	 0.31049 	 0.34918 	 m..s
  120 	   119 	 0.33081 	 0.35648 	 ~...
  119 	   120 	 0.31281 	 0.36809 	 m..s
==========================================
r_mrr = 0.8186524510383606
r2_mrr = 0.5465173721313477
spearmanr_mrr@5 = 0.8354611396789551
spearmanr_mrr@10 = 0.9267098903656006
spearmanr_mrr@50 = 0.9771332740783691
spearmanr_mrr@100 = 0.8698809742927551
spearmanr_mrr@All = 0.8928778767585754
==========================================
test time: 0.385
Done Testing dataset DBpedia50
total time taken: 183.51192140579224
training time taken: 179.00688576698303
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8187)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.5465)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8355)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9267)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9771)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8699)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8929)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.482197543962684}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 7189001384334985
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [286, 499, 702, 198, 786, 861, 789, 472, 1205, 800, 442, 379, 217, 338, 311, 754, 840, 365, 844, 941, 1098, 817, 386, 17, 366, 1115, 842, 832, 1033, 279, 234, 874, 78, 791, 619, 1050, 1010, 2, 109, 190, 1153, 872, 502, 742, 596, 244, 966, 169, 278, 435, 600, 873, 245, 507, 820, 346, 35, 1077, 777, 798, 1060, 439, 945, 342, 810, 1066, 494, 511, 755, 657, 233, 188, 426, 257, 1003, 979, 246, 912, 802, 28, 371, 95, 37, 1099, 1161, 505, 156, 36, 911, 117, 637, 749, 866, 849, 239, 202, 18, 667, 421, 20, 703, 121, 385, 1145, 748, 305, 1150, 273, 538, 503, 16, 335, 498, 219, 449, 42, 83, 693, 1008, 675, 143]
valid_ids (0): []
train_ids (1094): [22, 753, 1122, 545, 106, 750, 793, 1124, 122, 528, 814, 68, 668, 450, 1016, 537, 192, 1015, 404, 915, 304, 65, 1088, 145, 1168, 124, 323, 319, 1183, 553, 1146, 515, 986, 130, 1030, 1097, 578, 235, 1005, 645, 510, 374, 76, 1013, 1159, 97, 142, 819, 669, 158, 1007, 63, 953, 367, 815, 411, 210, 249, 926, 184, 100, 1172, 473, 344, 957, 173, 1086, 361, 375, 427, 956, 665, 62, 828, 659, 529, 1190, 1114, 774, 90, 1069, 821, 1207, 620, 80, 712, 1194, 289, 110, 882, 640, 564, 913, 87, 895, 66, 1211, 53, 1052, 119, 805, 86, 64, 70, 928, 977, 635, 1213, 557, 720, 1012, 247, 783, 952, 818, 642, 707, 512, 934, 322, 277, 958, 519, 463, 662, 1078, 334, 560, 1163, 614, 162, 136, 1079, 325, 443, 103, 829, 1027, 987, 296, 393, 146, 933, 845, 524, 594, 530, 718, 376, 909, 392, 13, 1164, 1141, 1109, 458, 1018, 281, 1076, 582, 1193, 220, 369, 1151, 788, 328, 1001, 1180, 556, 853, 276, 1212, 1186, 629, 149, 682, 1158, 1203, 1189, 332, 937, 506, 569, 714, 163, 388, 67, 568, 884, 464, 608, 255, 213, 1110, 520, 1177, 617, 237, 51, 406, 923, 745, 49, 701, 743, 150, 521, 484, 756, 248, 359, 253, 650, 1031, 540, 787, 459, 431, 809, 983, 1200, 587, 420, 558, 516, 187, 1123, 713, 903, 836, 161, 867, 155, 308, 761, 216, 673, 1119, 525, 422, 985, 726, 1139, 148, 626, 847, 839, 661, 355, 887, 964, 965, 630, 348, 287, 127, 32, 317, 921, 653, 275, 779, 813, 576, 40, 1026, 271, 852, 1160, 674, 708, 272, 846, 77, 310, 280, 889, 491, 1108, 180, 988, 445, 1167, 925, 114, 166, 981, 333, 434, 606, 9, 605, 960, 522, 205, 816, 1091, 211, 590, 1191, 961, 141, 92, 11, 835, 414, 1131, 838, 888, 973, 1116, 303, 942, 906, 684, 1043, 232, 687, 46, 260, 685, 274, 1063, 5, 639, 946, 984, 209, 899, 1000, 625, 559, 757, 615, 824, 944, 1198, 480, 948, 997, 695, 706, 1017, 603, 1154, 378, 900, 175, 765, 947, 574, 288, 331, 803, 597, 467, 518, 3, 1201, 879, 896, 633, 696, 1023, 416, 740, 241, 446, 1125, 768, 638, 976, 691, 466, 717, 1083, 865, 716, 972, 770, 394, 643, 655, 711, 428, 377, 1051, 381, 52, 55, 1175, 1082, 980, 739, 157, 171, 283, 834, 226, 531, 709, 1181, 1021, 1196, 1032, 284, 330, 858, 797, 101, 671, 1127, 681, 1130, 1081, 172, 399, 949, 609, 993, 179, 82, 592, 168, 723, 932, 351, 1040, 1038, 841, 475, 56, 1187, 462, 881, 290, 562, 360, 804, 527, 830, 694, 1002, 1162, 362, 892, 808, 517, 6, 267, 251, 1140, 300, 402, 876, 737, 1155, 893, 730, 151, 352, 825, 236, 566, 8, 307, 1134, 589, 1132, 1045, 490, 26, 29, 1195, 316, 93, 391, 258, 991, 123, 862, 405, 508, 759, 664, 1152, 54, 700, 120, 387, 919, 214, 1, 61, 622, 128, 731, 570, 1144, 1173, 869, 766, 543, 898, 343, 1009, 567, 357, 951, 575, 513, 955, 349, 196, 752, 623, 794, 697, 1148, 14, 546, 10, 0, 1058, 690, 591, 409, 535, 160, 1101, 48, 710, 69, 154, 137, 1209, 699, 185, 1137, 1102, 831, 1184, 91, 12, 922, 914, 182, 767, 195, 1165, 1072, 1113, 1014, 641, 950, 719, 721, 126, 165, 390, 403, 773, 34, 1179, 850, 238, 968, 380, 539, 324, 680, 261, 1118, 479, 221, 396, 910, 610, 104, 920, 1105, 400, 468, 492, 410, 1156, 859, 894, 1182, 1202, 1073, 544, 938, 250, 792, 660, 417, 183, 301, 1090, 57, 489, 908, 313, 452, 457, 384, 222, 998, 85, 782, 975, 1065, 654, 135, 1157, 45, 321, 848, 880, 7, 1025, 1138, 612, 50, 868, 59, 153, 1047, 1074, 526, 326, 465, 666, 481, 407, 886, 573, 648, 263, 176, 456, 1129, 438, 649, 1111, 131, 837, 547, 230, 1029, 495, 1034, 644, 129, 268, 159, 616, 1096, 164, 577, 771, 1093, 107, 971, 125, 140, 561, 1126, 598, 1075, 415, 593, 363, 353, 736, 939, 1054, 1024, 509, 1062, 533, 851, 181, 73, 856, 432, 84, 223, 907, 995, 580, 297, 1143, 1094, 1192, 397, 373, 532, 1176, 105, 563, 1112, 646, 240, 801, 299, 552, 74, 38, 963, 663, 19, 652, 320, 843, 218, 347, 382, 254, 71, 1039, 550, 927, 1055, 1042, 193, 1185, 747, 1019, 1061, 208, 764, 327, 1197, 424, 108, 584, 586, 312, 500, 1067, 536, 1006, 448, 823, 744, 461, 604, 493, 732, 1174, 565, 132, 588, 1178, 627, 1199, 227, 781, 822, 293, 113, 954, 1210, 336, 1107, 1092, 785, 962, 476, 918, 585, 790, 1041, 102, 796, 722, 167, 24, 437, 370, 72, 878, 1169, 116, 81, 225, 891, 425, 430, 285, 613, 47, 1049, 996, 401, 689, 534, 454, 496, 302, 408, 776, 354, 1171, 677, 1087, 812, 1080, 294, 262, 469, 778, 583, 940, 270, 902, 1103, 1044, 890, 1120, 256, 204, 970, 144, 295, 672, 571, 741, 990, 191, 88, 870, 883, 651, 133, 1133, 345, 96, 658, 337, 885, 31, 857, 318, 692, 1206, 989, 854, 98, 1037, 860, 41, 30, 523, 618, 924, 1064, 189, 775, 398, 43, 372, 292, 487, 44, 772, 197, 994, 599, 905, 1135, 967, 266, 769, 935, 215, 1106, 39, 855, 1028, 1100, 551, 784, 746, 1004, 207, 548, 554, 75, 1057, 89, 470, 264, 341, 607, 504, 314, 383, 1170, 725, 203, 486, 799, 904, 871, 897, 212, 231, 542, 572, 943, 315, 760, 864, 94, 875, 356, 99, 581, 1022, 916, 174, 1035, 632, 177, 265, 1056, 488, 4, 833, 25, 762, 482, 440, 999, 460, 350, 978, 795, 1204, 601, 200, 595, 1149, 433, 474, 229, 602, 478, 1117, 901, 471, 1095, 982, 194, 15, 826, 549, 959, 178, 364, 624, 611, 329, 930, 1048, 698, 340, 621, 1166, 735, 206, 807, 441, 634, 224, 444, 58, 483, 170, 678, 992, 1104, 79, 429, 21, 60, 112, 33, 269, 419, 827, 728, 514, 453, 368, 1020, 738, 389, 936, 1011, 477, 1214, 27, 729, 917, 1059, 111, 541, 298, 656, 199, 497, 1208, 679, 1142, 676, 152, 242, 228, 1089, 1147, 118, 631, 1071, 734, 1136, 758, 688, 751, 715, 806, 863, 1070, 1121, 186, 138, 243, 1068, 1053, 705, 727, 412, 451, 485, 115, 724, 670, 686, 733, 501, 418, 931, 139, 252, 647, 811, 1188, 395, 201, 763, 555, 306, 447, 134, 413, 780, 358, 1036, 339, 628, 969, 1046, 1084, 436, 683, 877, 282, 291, 309, 1128, 1085, 259, 636, 974, 579, 455, 929, 23, 423, 147, 704]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6331642107274654
the save name prefix for this run is:  chkpt-ID_6331642107274654_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 647
rank avg (pred): 0.519 +- 0.002
mrr vals (pred, true): 0.000, 0.147
batch losses (mrrl, rdl): 0.0, 0.0007279732

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 940
rank avg (pred): 0.530 +- 0.322
mrr vals (pred, true): 0.113, 0.000
batch losses (mrrl, rdl): 0.0, 0.0016267626

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 627
rank avg (pred): 0.431 +- 0.340
mrr vals (pred, true): 0.160, 0.114
batch losses (mrrl, rdl): 0.0, 0.0001973867

Epoch over!
epoch time: 11.85

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.411 +- 0.313
mrr vals (pred, true): 0.150, 0.160
batch losses (mrrl, rdl): 0.0, 0.0004254525

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1118
rank avg (pred): 0.432 +- 0.314
mrr vals (pred, true): 0.032, 0.000
batch losses (mrrl, rdl): 0.0, 5.43937e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1032
rank avg (pred): 0.386 +- 0.309
mrr vals (pred, true): 0.122, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001065436

Epoch over!
epoch time: 11.769

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 606
rank avg (pred): 0.427 +- 0.326
mrr vals (pred, true): 0.118, 0.128
batch losses (mrrl, rdl): 0.0, 0.0001984534

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.192 +- 0.295
mrr vals (pred, true): 0.124, 0.267
batch losses (mrrl, rdl): 0.0, 2.59941e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 732
rank avg (pred): 0.261 +- 0.316
mrr vals (pred, true): 0.157, 0.005
batch losses (mrrl, rdl): 0.0, 3.15614e-05

Epoch over!
epoch time: 11.695

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 845
rank avg (pred): 0.461 +- 0.338
mrr vals (pred, true): 0.156, 0.000
batch losses (mrrl, rdl): 0.0, 1.36025e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1069
rank avg (pred): 0.128 +- 0.278
mrr vals (pred, true): 0.213, 0.319
batch losses (mrrl, rdl): 0.0, 2.19417e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 466
rank avg (pred): 0.403 +- 0.334
mrr vals (pred, true): 0.171, 0.000
batch losses (mrrl, rdl): 0.0, 5.12307e-05

Epoch over!
epoch time: 11.712

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.414 +- 0.322
mrr vals (pred, true): 0.145, 0.001
batch losses (mrrl, rdl): 0.0, 0.000137653

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.141 +- 0.285
mrr vals (pred, true): 0.189, 0.383
batch losses (mrrl, rdl): 0.0, 2.81774e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.422 +- 0.322
mrr vals (pred, true): 0.176, 0.139
batch losses (mrrl, rdl): 0.0, 7.4898e-05

Epoch over!
epoch time: 11.853

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1201
rank avg (pred): 0.406 +- 0.325
mrr vals (pred, true): 0.184, 0.000
batch losses (mrrl, rdl): 0.1795640439, 0.0001530126

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 148
rank avg (pred): 0.399 +- 0.265
mrr vals (pred, true): 0.079, 0.167
batch losses (mrrl, rdl): 0.0779249966, 0.0006468684

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.398 +- 0.279
mrr vals (pred, true): 0.111, 0.175
batch losses (mrrl, rdl): 0.0404503606, 0.0001553476

Epoch over!
epoch time: 12.121

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.518 +- 0.315
mrr vals (pred, true): 0.089, 0.000
batch losses (mrrl, rdl): 0.0149299465, 0.0014022033

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 193
rank avg (pred): 0.442 +- 0.286
mrr vals (pred, true): 0.067, 0.000
batch losses (mrrl, rdl): 0.0029454215, 6.68672e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 989
rank avg (pred): 0.189 +- 0.246
mrr vals (pred, true): 0.273, 0.289
batch losses (mrrl, rdl): 0.0023723505, 2.77961e-05

Epoch over!
epoch time: 12.352

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 488
rank avg (pred): 0.193 +- 0.245
mrr vals (pred, true): 0.260, 0.262
batch losses (mrrl, rdl): 3.36215e-05, 8.4284e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 227
rank avg (pred): 0.385 +- 0.254
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0272424184, 7.72911e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 752
rank avg (pred): 0.474 +- 0.279
mrr vals (pred, true): 0.073, 0.128
batch losses (mrrl, rdl): 0.0304124337, 0.0012480607

Epoch over!
epoch time: 12.274

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 729
rank avg (pred): 0.491 +- 0.289
mrr vals (pred, true): 0.054, 0.028
batch losses (mrrl, rdl): 0.0001505326, 0.0019178839

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 34
rank avg (pred): 0.239 +- 0.237
mrr vals (pred, true): 0.207, 0.181
batch losses (mrrl, rdl): 0.0068446733, 0.0001003129

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 317
rank avg (pred): 0.199 +- 0.229
mrr vals (pred, true): 0.274, 0.225
batch losses (mrrl, rdl): 0.0248866826, 7.08794e-05

Epoch over!
epoch time: 12.074

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 405
rank avg (pred): 0.360 +- 0.245
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0367260575, 0.0005103345

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.325 +- 0.264
mrr vals (pred, true): 0.157, 0.203
batch losses (mrrl, rdl): 0.0220521186, 0.0001148177

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 828
rank avg (pred): 0.316 +- 0.231
mrr vals (pred, true): 0.151, 0.133
batch losses (mrrl, rdl): 0.0032670794, 0.0006002682

Epoch over!
epoch time: 12.006

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 0
rank avg (pred): 0.398 +- 0.284
mrr vals (pred, true): 0.128, 0.324
batch losses (mrrl, rdl): 0.383647114, 0.0014485745

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.422 +- 0.277
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0259643644, 1.89307e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 56
rank avg (pred): 0.263 +- 0.233
mrr vals (pred, true): 0.256, 0.198
batch losses (mrrl, rdl): 0.0341541134, 0.000238094

Epoch over!
epoch time: 12.096

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 916
rank avg (pred): 0.580 +- 0.292
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0026472213, 0.000190522

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 315
rank avg (pred): 0.267 +- 0.234
mrr vals (pred, true): 0.266, 0.230
batch losses (mrrl, rdl): 0.0124146435, 0.0002977426

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.382 +- 0.241
mrr vals (pred, true): 0.092, 0.147
batch losses (mrrl, rdl): 0.031016374, 0.000113792

Epoch over!
epoch time: 11.935

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 345
rank avg (pred): 0.337 +- 0.230
mrr vals (pred, true): 0.110, 0.163
batch losses (mrrl, rdl): 0.0277026109, 6.88754e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.243 +- 0.226
mrr vals (pred, true): 0.244, 0.242
batch losses (mrrl, rdl): 4.32253e-05, 0.0001902084

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 688
rank avg (pred): 0.411 +- 0.266
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.0245616771, 0.0001532403

Epoch over!
epoch time: 11.955

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 593
rank avg (pred): 0.422 +- 0.302
mrr vals (pred, true): 0.112, 0.121
batch losses (mrrl, rdl): 0.0008074585, 0.0001399341

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 106
rank avg (pred): 0.414 +- 0.308
mrr vals (pred, true): 0.111, 0.187
batch losses (mrrl, rdl): 0.0569730103, 0.0002945438

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.470 +- 0.344
mrr vals (pred, true): 0.105, 0.003
batch losses (mrrl, rdl): 0.0299100019, 1.9939e-05

Epoch over!
epoch time: 12.107

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 570
rank avg (pred): 0.425 +- 0.282
mrr vals (pred, true): 0.095, 0.116
batch losses (mrrl, rdl): 0.0042699222, 0.0001909768

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.515 +- 0.278
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.16065e-05, 3.71446e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 801
rank avg (pred): 0.471 +- 0.293
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.0004597641, 1.7934e-06

Epoch over!
epoch time: 12.072

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.203 +- 0.213
mrr vals (pred, true): 0.203, 0.258

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.03174 	 5e-0500 	 m..s
   11 	     1 	 0.03411 	 0.00016 	 m..s
   72 	     2 	 0.09556 	 0.00016 	 m..s
   79 	     3 	 0.09810 	 0.00016 	 m..s
   62 	     4 	 0.09382 	 0.00017 	 m..s
    8 	     5 	 0.03246 	 0.00018 	 m..s
    4 	     6 	 0.02976 	 0.00018 	 ~...
   15 	     7 	 0.04059 	 0.00018 	 m..s
   10 	     8 	 0.03311 	 0.00018 	 m..s
   13 	     9 	 0.03889 	 0.00019 	 m..s
   67 	    10 	 0.09423 	 0.00020 	 m..s
   60 	    11 	 0.09308 	 0.00021 	 m..s
   37 	    12 	 0.08849 	 0.00021 	 m..s
   29 	    13 	 0.08657 	 0.00021 	 m..s
   12 	    14 	 0.03567 	 0.00021 	 m..s
   42 	    15 	 0.08939 	 0.00021 	 m..s
   16 	    16 	 0.04080 	 0.00022 	 m..s
   38 	    17 	 0.08904 	 0.00022 	 m..s
   63 	    18 	 0.09398 	 0.00022 	 m..s
   73 	    19 	 0.09567 	 0.00023 	 m..s
   44 	    20 	 0.08960 	 0.00023 	 m..s
    7 	    21 	 0.03224 	 0.00023 	 m..s
   56 	    22 	 0.09145 	 0.00025 	 m..s
    1 	    23 	 0.02876 	 0.00025 	 ~...
   69 	    24 	 0.09475 	 0.00025 	 m..s
    9 	    25 	 0.03266 	 0.00026 	 m..s
    5 	    26 	 0.03159 	 0.00027 	 m..s
   81 	    27 	 0.10465 	 0.00030 	 MISS
   76 	    28 	 0.09688 	 0.00033 	 m..s
   57 	    29 	 0.09191 	 0.00038 	 m..s
   61 	    30 	 0.09372 	 0.00039 	 m..s
   80 	    31 	 0.10149 	 0.00043 	 MISS
    2 	    32 	 0.02942 	 0.00046 	 ~...
   18 	    33 	 0.04161 	 0.00046 	 m..s
   19 	    34 	 0.04219 	 0.00056 	 m..s
   55 	    35 	 0.09141 	 0.00071 	 m..s
   48 	    36 	 0.08998 	 0.00078 	 m..s
   26 	    37 	 0.08412 	 0.00082 	 m..s
   35 	    38 	 0.08762 	 0.00087 	 m..s
   41 	    39 	 0.08927 	 0.00091 	 m..s
   47 	    40 	 0.08997 	 0.00093 	 m..s
   45 	    41 	 0.08988 	 0.00116 	 m..s
   17 	    42 	 0.04124 	 0.00118 	 m..s
   14 	    43 	 0.03952 	 0.00143 	 m..s
   27 	    44 	 0.08593 	 0.00188 	 m..s
   20 	    45 	 0.04384 	 0.00823 	 m..s
    0 	    46 	 0.02436 	 0.01777 	 ~...
   21 	    47 	 0.05204 	 0.02558 	 ~...
    3 	    48 	 0.02959 	 0.03414 	 ~...
   22 	    49 	 0.05252 	 0.03790 	 ~...
   66 	    50 	 0.09410 	 0.10313 	 ~...
   52 	    51 	 0.09119 	 0.11403 	 ~...
   75 	    52 	 0.09665 	 0.11626 	 ~...
   64 	    53 	 0.09399 	 0.11903 	 ~...
   28 	    54 	 0.08628 	 0.12381 	 m..s
   49 	    55 	 0.09005 	 0.12649 	 m..s
   65 	    56 	 0.09409 	 0.12750 	 m..s
   34 	    57 	 0.08719 	 0.12839 	 m..s
   50 	    58 	 0.09050 	 0.12856 	 m..s
   54 	    59 	 0.09133 	 0.13210 	 m..s
   33 	    60 	 0.08709 	 0.13359 	 m..s
   25 	    61 	 0.08350 	 0.13874 	 m..s
   39 	    62 	 0.08922 	 0.13893 	 m..s
   76 	    63 	 0.09688 	 0.14050 	 m..s
   82 	    64 	 0.12475 	 0.14150 	 ~...
   24 	    65 	 0.08348 	 0.14228 	 m..s
   46 	    66 	 0.08995 	 0.14432 	 m..s
   53 	    67 	 0.09126 	 0.14716 	 m..s
   58 	    68 	 0.09203 	 0.14841 	 m..s
   31 	    69 	 0.08674 	 0.15436 	 m..s
   78 	    70 	 0.09711 	 0.15551 	 m..s
   51 	    71 	 0.09105 	 0.15574 	 m..s
   32 	    72 	 0.08701 	 0.15591 	 m..s
   30 	    73 	 0.08669 	 0.15697 	 m..s
   23 	    74 	 0.08344 	 0.15898 	 m..s
   68 	    75 	 0.09460 	 0.15909 	 m..s
   43 	    76 	 0.08945 	 0.16980 	 m..s
   71 	    77 	 0.09504 	 0.17096 	 m..s
   74 	    78 	 0.09593 	 0.17409 	 m..s
   40 	    79 	 0.08924 	 0.17434 	 m..s
   70 	    80 	 0.09487 	 0.17565 	 m..s
   36 	    81 	 0.08846 	 0.17807 	 m..s
   93 	    82 	 0.20034 	 0.18205 	 ~...
   83 	    83 	 0.12838 	 0.18258 	 m..s
   59 	    84 	 0.09241 	 0.18452 	 m..s
   84 	    85 	 0.18250 	 0.19113 	 ~...
   87 	    86 	 0.19051 	 0.20910 	 ~...
   88 	    87 	 0.19106 	 0.21250 	 ~...
   94 	    88 	 0.20070 	 0.21437 	 ~...
   90 	    89 	 0.19435 	 0.21655 	 ~...
   92 	    90 	 0.19710 	 0.22010 	 ~...
   89 	    91 	 0.19274 	 0.22329 	 m..s
   98 	    92 	 0.21105 	 0.22477 	 ~...
   91 	    93 	 0.19644 	 0.23829 	 m..s
   85 	    94 	 0.18576 	 0.25014 	 m..s
  108 	    95 	 0.26472 	 0.25706 	 ~...
   95 	    96 	 0.20331 	 0.25823 	 m..s
   99 	    97 	 0.21771 	 0.25969 	 m..s
   97 	    98 	 0.21096 	 0.26160 	 m..s
   96 	    99 	 0.20963 	 0.26378 	 m..s
   86 	   100 	 0.18978 	 0.26759 	 m..s
  102 	   101 	 0.25721 	 0.27333 	 ~...
  101 	   102 	 0.25677 	 0.27508 	 ~...
  100 	   103 	 0.25348 	 0.27685 	 ~...
  107 	   104 	 0.26375 	 0.28499 	 ~...
  106 	   105 	 0.26248 	 0.28735 	 ~...
  103 	   106 	 0.26089 	 0.29003 	 ~...
  105 	   107 	 0.26181 	 0.29109 	 ~...
  104 	   108 	 0.26112 	 0.31202 	 m..s
  113 	   109 	 0.34478 	 0.32253 	 ~...
  112 	   110 	 0.34384 	 0.33220 	 ~...
  114 	   111 	 0.34551 	 0.33349 	 ~...
  111 	   112 	 0.34330 	 0.33393 	 ~...
  117 	   113 	 0.35115 	 0.33456 	 ~...
  115 	   114 	 0.34977 	 0.34184 	 ~...
  110 	   115 	 0.34013 	 0.34760 	 ~...
  109 	   116 	 0.29129 	 0.35787 	 m..s
  120 	   117 	 0.37843 	 0.36280 	 ~...
  116 	   118 	 0.35034 	 0.36372 	 ~...
  118 	   119 	 0.35680 	 0.37125 	 ~...
  119 	   120 	 0.35873 	 0.38092 	 ~...
==========================================
r_mrr = 0.8872554302215576
r2_mrr = 0.7733268141746521
spearmanr_mrr@5 = 0.9485352039337158
spearmanr_mrr@10 = 0.8902509212493896
spearmanr_mrr@50 = 0.9806104302406311
spearmanr_mrr@100 = 0.8843057751655579
spearmanr_mrr@All = 0.910689115524292
==========================================
test time: 0.402
Done Testing dataset DBpedia50
total time taken: 184.96559619903564
training time taken: 180.3363709449768
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8873)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.7733)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9485)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.8903)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9806)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8843)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9107)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.112702183359943}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 7858386549971944
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1076, 613, 1131, 1066, 947, 526, 42, 161, 313, 914, 167, 137, 116, 668, 976, 4, 949, 1012, 1188, 735, 466, 30, 1144, 44, 225, 688, 1083, 724, 1135, 299, 1210, 749, 1178, 1057, 460, 765, 1096, 692, 933, 1119, 276, 954, 939, 1200, 3, 863, 521, 1133, 322, 705, 113, 1141, 1079, 811, 788, 575, 1168, 817, 1120, 861, 1146, 836, 58, 506, 1147, 301, 772, 881, 507, 306, 827, 1091, 345, 1016, 887, 775, 154, 391, 367, 1193, 434, 809, 532, 789, 1151, 942, 132, 839, 390, 92, 734, 852, 814, 385, 16, 717, 866, 898, 961, 439, 1173, 872, 502, 186, 401, 581, 1153, 1011, 339, 270, 807, 43, 860, 1166, 547, 1054, 41, 52, 925, 266, 1084]
valid_ids (0): []
train_ids (1094): [54, 179, 35, 321, 127, 689, 777, 833, 531, 255, 412, 729, 1212, 955, 202, 642, 570, 600, 68, 163, 617, 300, 672, 361, 386, 1183, 1094, 727, 124, 259, 125, 372, 597, 304, 612, 810, 338, 84, 184, 665, 1180, 143, 793, 366, 985, 271, 588, 1031, 134, 986, 1070, 260, 249, 712, 1127, 61, 393, 806, 957, 148, 611, 967, 1179, 980, 834, 1034, 24, 273, 441, 471, 221, 926, 737, 655, 150, 217, 624, 1080, 1020, 326, 522, 63, 1078, 65, 1152, 508, 750, 327, 91, 493, 50, 406, 956, 1014, 349, 17, 759, 1206, 1181, 1081, 101, 529, 907, 844, 258, 801, 855, 356, 1051, 1205, 910, 212, 485, 346, 920, 1007, 427, 1087, 886, 697, 649, 469, 1209, 573, 571, 880, 408, 364, 250, 122, 395, 824, 199, 274, 1041, 864, 1194, 165, 1105, 1021, 974, 781, 703, 607, 286, 940, 1088, 982, 1053, 465, 481, 213, 1177, 1037, 608, 19, 676, 494, 117, 319, 643, 281, 194, 902, 865, 563, 491, 236, 858, 470, 656, 201, 943, 103, 399, 1207, 1211, 411, 1050, 1075, 670, 287, 435, 214, 1059, 592, 572, 661, 984, 680, 82, 468, 1157, 558, 561, 244, 690, 776, 472, 604, 229, 1042, 1138, 83, 990, 368, 1114, 1160, 423, 1095, 1000, 894, 669, 882, 200, 38, 1113, 622, 12, 952, 602, 456, 365, 378, 1004, 135, 267, 1018, 93, 348, 537, 1140, 681, 293, 486, 362, 407, 1009, 1162, 1174, 331, 979, 631, 544, 657, 845, 305, 787, 13, 564, 355, 700, 492, 487, 178, 1150, 972, 1032, 422, 543, 619, 428, 1203, 342, 527, 53, 838, 144, 1202, 847, 80, 282, 536, 796, 1159, 1027, 256, 1143, 1071, 1122, 187, 679, 1099, 822, 708, 1052, 704, 999, 755, 517, 351, 591, 220, 849, 523, 535, 1136, 398, 792, 195, 662, 36, 285, 1033, 1077, 1139, 463, 231, 474, 962, 153, 303, 66, 90, 691, 936, 1171, 918, 400, 869, 946, 462, 757, 639, 81, 525, 311, 443, 577, 1074, 1030, 1008, 879, 482, 278, 911, 197, 1109, 97, 823, 515, 234, 1068, 114, 1126, 20, 627, 871, 39, 634, 761, 6, 175, 419, 878, 731, 519, 442, 170, 610, 971, 238, 929, 237, 461, 915, 1165, 753, 959, 1, 257, 752, 1115, 138, 222, 747, 994, 953, 965, 207, 1169, 504, 1060, 510, 841, 644, 152, 966, 315, 1124, 916, 94, 223, 454, 945, 298, 32, 516, 897, 1149, 867, 173, 1145, 585, 678, 742, 336, 296, 130, 785, 900, 436, 478, 713, 848, 495, 464, 176, 601, 599, 477, 387, 447, 1130, 433, 151, 1154, 183, 48, 859, 744, 1046, 695, 1061, 264, 964, 26, 873, 343, 1056, 648, 177, 459, 722, 1204, 297, 190, 254, 206, 560, 1161, 128, 181, 973, 228, 392, 913, 1103, 620, 751, 720, 371, 795, 72, 551, 760, 635, 1199, 131, 798, 1155, 14, 246, 1025, 565, 567, 158, 1110, 562, 596, 302, 198, 1006, 709, 698, 45, 628, 1187, 969, 715, 557, 1112, 1090, 808, 832, 295, 766, 556, 748, 344, 542, 226, 951, 329, 509, 889, 1055, 584, 586, 893, 388, 1176, 292, 1156, 938, 651, 437, 721, 374, 1047, 802, 22, 714, 870, 330, 379, 819, 505, 921, 800, 147, 904, 224, 1085, 931, 677, 180, 699, 598, 937, 738, 112, 272, 252, 353, 970, 603, 812, 284, 1064, 778, 280, 856, 7, 328, 1003, 786, 583, 647, 56, 782, 927, 595, 559, 307, 333, 701, 1118, 896, 1163, 968, 934, 1035, 790, 501, 323, 430, 1197, 1101, 813, 768, 1010, 998, 1015, 290, 182, 576, 324, 1023, 265, 139, 730, 696, 1190, 769, 404, 582, 318, 710, 34, 95, 1134, 410, 587, 370, 574, 1132, 928, 770, 409, 1049, 736, 1058, 1116, 449, 189, 837, 230, 804, 1129, 108, 652, 1073, 1017, 247, 1201, 1189, 241, 623, 105, 901, 233, 51, 484, 106, 205, 376, 605, 988, 917, 743, 741, 85, 636, 763, 394, 415, 767, 983, 1063, 740, 263, 851, 1102, 658, 15, 829, 268, 18, 209, 1100, 1106, 363, 73, 541, 686, 733, 1038, 118, 377, 357, 496, 438, 923, 640, 123, 453, 883, 473, 550, 253, 853, 846, 444, 615, 164, 566, 79, 102, 549, 666, 421, 1024, 702, 291, 1195, 126, 402, 745, 107, 86, 1022, 684, 219, 978, 193, 1044, 554, 876, 136, 764, 310, 70, 590, 2, 431, 350, 381, 895, 641, 129, 794, 835, 5, 892, 726, 960, 815, 868, 448, 490, 555, 172, 25, 831, 1086, 1048, 1013, 1028, 633, 28, 1184, 62, 76, 728, 716, 314, 997, 771, 87, 1137, 1158, 524, 625, 162, 308, 98, 1072, 29, 354, 141, 885, 663, 160, 779, 445, 816, 963, 110, 513, 146, 1191, 373, 739, 384, 630, 891, 498, 185, 240, 1182, 483, 414, 426, 606, 944, 653, 579, 987, 1067, 279, 1128, 580, 78, 168, 1172, 991, 1107, 958, 100, 1089, 1148, 996, 218, 204, 60, 906, 432, 1123, 251, 159, 948, 166, 930, 440, 111, 479, 1111, 216, 403, 548, 552, 1026, 553, 539, 499, 512, 667, 1192, 488, 341, 242, 694, 188, 21, 171, 719, 75, 992, 416, 683, 1029, 375, 843, 756, 1043, 840, 905, 1198, 55, 88, 289, 208, 774, 674, 1092, 783, 121, 874, 1121, 578, 10, 358, 1186, 451, 33, 57, 71, 145, 780, 1097, 142, 37, 1170, 618, 120, 500, 211, 616, 545, 903, 830, 425, 540, 11, 784, 334, 732, 1104, 59, 803, 725, 169, 196, 671, 1213, 104, 288, 380, 1082, 818, 382, 977, 935, 418, 489, 283, 932, 174, 1040, 594, 1069, 232, 269, 389, 758, 480, 693, 317, 646, 47, 654, 660, 950, 1098, 450, 664, 149, 316, 530, 850, 325, 69, 629, 40, 922, 239, 762, 995, 396, 458, 191, 320, 1108, 569, 589, 1164, 294, 993, 675, 919, 975, 452, 405, 133, 799, 685, 413, 821, 842, 888, 546, 511, 335, 707, 650, 1142, 1125, 912, 74, 797, 1036, 746, 31, 262, 687, 156, 1005, 826, 862, 534, 875, 337, 261, 533, 609, 369, 245, 27, 64, 791, 514, 397, 192, 820, 989, 115, 420, 275, 773, 429, 825, 119, 638, 340, 659, 243, 924, 909, 109, 890, 140, 215, 0, 497, 614, 1062, 857, 1039, 1185, 626, 67, 8, 89, 1214, 711, 77, 854, 1208, 347, 1175, 49, 1093, 1167, 417, 96, 475, 235, 1117, 99, 706, 424, 621, 248, 1002, 157, 1065, 210, 682, 277, 332, 568, 941, 673, 877, 312, 227, 360, 455, 593, 1001, 203, 446, 23, 309, 467, 457, 503, 723, 476, 908, 520, 637, 528, 155, 383, 981, 754, 899, 632, 1196, 718, 884, 359, 645, 352, 518, 46, 538, 1045, 805, 9, 1019, 828]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4545665955189231
the save name prefix for this run is:  chkpt-ID_4545665955189231_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 527
rank avg (pred): 0.390 +- 0.005
mrr vals (pred, true): 0.000, 0.186
batch losses (mrrl, rdl): 0.0, 0.0003270211

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 436
rank avg (pred): 0.424 +- 0.038
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001654079

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1198
rank avg (pred): 0.428 +- 0.211
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0, 4.57135e-05

Epoch over!
epoch time: 12.12

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 642
rank avg (pred): 0.434 +- 0.257
mrr vals (pred, true): 0.095, 0.136
batch losses (mrrl, rdl): 0.0, 0.0001862171

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.328 +- 0.238
mrr vals (pred, true): 0.216, 0.118
batch losses (mrrl, rdl): 0.0, 0.0003547271

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1021
rank avg (pred): 0.399 +- 0.284
mrr vals (pred, true): 0.177, 0.177
batch losses (mrrl, rdl): 0.0, 0.0002218721

Epoch over!
epoch time: 11.74

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.166 +- 0.122
mrr vals (pred, true): 0.229, 0.218
batch losses (mrrl, rdl): 0.0, 6.60767e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.400 +- 0.278
mrr vals (pred, true): 0.118, 0.149
batch losses (mrrl, rdl): 0.0, 0.0002532686

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.400 +- 0.300
mrr vals (pred, true): 0.145, 0.179
batch losses (mrrl, rdl): 0.0, 0.0001994083

Epoch over!
epoch time: 11.662

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 801
rank avg (pred): 0.423 +- 0.341
mrr vals (pred, true): 0.222, 0.000
batch losses (mrrl, rdl): 0.0, 4.24787e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 638
rank avg (pred): 0.387 +- 0.305
mrr vals (pred, true): 0.150, 0.135
batch losses (mrrl, rdl): 0.0, 8.31378e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1082
rank avg (pred): 0.400 +- 0.302
mrr vals (pred, true): 0.135, 0.160
batch losses (mrrl, rdl): 0.0, 0.0001703568

Epoch over!
epoch time: 11.669

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 768
rank avg (pred): 0.460 +- 0.312
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003756057

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 564
rank avg (pred): 0.284 +- 0.235
mrr vals (pred, true): 0.187, 0.153
batch losses (mrrl, rdl): 0.0, 4.61382e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.236 +- 0.193
mrr vals (pred, true): 0.170, 0.228
batch losses (mrrl, rdl): 0.0, 0.0001222561

Epoch over!
epoch time: 11.721

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 67
rank avg (pred): 0.175 +- 0.149
mrr vals (pred, true): 0.222, 0.259
batch losses (mrrl, rdl): 0.0139852762, 7.20168e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 272
rank avg (pred): 0.184 +- 0.172
mrr vals (pred, true): 0.227, 0.221
batch losses (mrrl, rdl): 0.0002995719, 5.2881e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.160 +- 0.226
mrr vals (pred, true): 0.262, 0.162
batch losses (mrrl, rdl): 0.1000431478, 4.3372e-06

Epoch over!
epoch time: 12.462

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 525
rank avg (pred): 0.253 +- 0.247
mrr vals (pred, true): 0.156, 0.175
batch losses (mrrl, rdl): 0.0036719306, 2.54517e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 726
rank avg (pred): 0.396 +- 0.250
mrr vals (pred, true): 0.098, 0.001
batch losses (mrrl, rdl): 0.0227876566, 9.87771e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 487
rank avg (pred): 0.200 +- 0.257
mrr vals (pred, true): 0.270, 0.278
batch losses (mrrl, rdl): 0.0006648925, 1.79867e-05

Epoch over!
epoch time: 12.311

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 846
rank avg (pred): 0.534 +- 0.259
mrr vals (pred, true): 0.066, 0.020
batch losses (mrrl, rdl): 0.0024236608, 0.0004993546

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 996
rank avg (pred): 0.189 +- 0.250
mrr vals (pred, true): 0.315, 0.294
batch losses (mrrl, rdl): 0.0042871241, 0.000145501

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 309
rank avg (pred): 0.238 +- 0.249
mrr vals (pred, true): 0.220, 0.164
batch losses (mrrl, rdl): 0.0314681008, 0.0001405157

Epoch over!
epoch time: 12.223

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 199
rank avg (pred): 0.367 +- 0.243
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0377414897, 0.0003267487

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 849
rank avg (pred): 0.544 +- 0.247
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0021420722, 3.77109e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 557
rank avg (pred): 0.323 +- 0.242
mrr vals (pred, true): 0.179, 0.194
batch losses (mrrl, rdl): 0.0021306542, 0.0001373808

Epoch over!
epoch time: 11.923

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 449
rank avg (pred): 0.389 +- 0.224
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0218963735, 0.0002251978

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1050
rank avg (pred): 0.384 +- 0.277
mrr vals (pred, true): 0.110, 0.001
batch losses (mrrl, rdl): 0.0362800546, 5.37468e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 699
rank avg (pred): 0.385 +- 0.231
mrr vals (pred, true): 0.099, 0.000
batch losses (mrrl, rdl): 0.0243307557, 0.0002489207

Epoch over!
epoch time: 11.839

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.303 +- 0.237
mrr vals (pred, true): 0.228, 0.214
batch losses (mrrl, rdl): 0.0018124129, 0.0003837019

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1050
rank avg (pred): 0.405 +- 0.256
mrr vals (pred, true): 0.102, 0.001
batch losses (mrrl, rdl): 0.0274203885, 3.50177e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.361 +- 0.222
mrr vals (pred, true): 0.092, 0.178
batch losses (mrrl, rdl): 0.073772952, 0.0001551804

Epoch over!
epoch time: 12.021

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 778
rank avg (pred): 0.580 +- 0.269
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002446937, 5.81441e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 550
rank avg (pred): 0.311 +- 0.229
mrr vals (pred, true): 0.171, 0.185
batch losses (mrrl, rdl): 0.0021029471, 7.17377e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.326 +- 0.238
mrr vals (pred, true): 0.167, 0.171
batch losses (mrrl, rdl): 0.0002001494, 0.0007744936

Epoch over!
epoch time: 11.846

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 278
rank avg (pred): 0.257 +- 0.219
mrr vals (pred, true): 0.232, 0.225
batch losses (mrrl, rdl): 0.0005848721, 0.0001405289

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1204
rank avg (pred): 0.453 +- 0.255
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.0134515688, 2.07121e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1139
rank avg (pred): 0.275 +- 0.223
mrr vals (pred, true): 0.246, 0.274
batch losses (mrrl, rdl): 0.0080819624, 6.70468e-05

Epoch over!
epoch time: 11.927

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 902
rank avg (pred): 0.522 +- 0.245
mrr vals (pred, true): 0.073, 0.026
batch losses (mrrl, rdl): 0.0053536044, 0.0001174708

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 201
rank avg (pred): 0.397 +- 0.212
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0160596091, 0.0002511549

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 617
rank avg (pred): 0.383 +- 0.214
mrr vals (pred, true): 0.095, 0.121
batch losses (mrrl, rdl): 0.0071248454, 0.000121368

Epoch over!
epoch time: 11.938

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 897
rank avg (pred): 0.583 +- 0.269
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003289038, 0.0016664754

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 924
rank avg (pred): 0.638 +- 0.266
mrr vals (pred, true): 0.032, 0.000
batch losses (mrrl, rdl): 0.0031873186, 0.0012807122

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 563
rank avg (pred): 0.274 +- 0.216
mrr vals (pred, true): 0.197, 0.216
batch losses (mrrl, rdl): 0.0036345122, 3.71464e-05

Epoch over!
epoch time: 11.814

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.174 +- 0.219
mrr vals (pred, true): 0.309, 0.295

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04898 	 5e-0500 	 m..s
    1 	     1 	 0.03777 	 6e-0500 	 m..s
   11 	     2 	 0.05257 	 6e-0500 	 m..s
    8 	     3 	 0.04870 	 6e-0500 	 m..s
   14 	     4 	 0.05502 	 0.00011 	 m..s
   34 	     5 	 0.09032 	 0.00013 	 m..s
   22 	     6 	 0.05919 	 0.00014 	 m..s
   63 	     7 	 0.10040 	 0.00015 	 MISS
   51 	     8 	 0.09694 	 0.00016 	 m..s
   42 	     9 	 0.09526 	 0.00017 	 m..s
   43 	    10 	 0.09557 	 0.00018 	 m..s
   31 	    11 	 0.08921 	 0.00018 	 m..s
   36 	    12 	 0.09129 	 0.00018 	 m..s
   10 	    13 	 0.04987 	 0.00019 	 m..s
    2 	    14 	 0.03951 	 0.00020 	 m..s
    3 	    15 	 0.04020 	 0.00021 	 m..s
   12 	    16 	 0.05295 	 0.00022 	 m..s
    6 	    17 	 0.04481 	 0.00025 	 m..s
   20 	    18 	 0.05768 	 0.00025 	 m..s
   32 	    19 	 0.08943 	 0.00025 	 m..s
   60 	    20 	 0.09899 	 0.00026 	 m..s
   64 	    21 	 0.10067 	 0.00026 	 MISS
   29 	    22 	 0.08741 	 0.00026 	 m..s
    5 	    23 	 0.04405 	 0.00027 	 m..s
   13 	    24 	 0.05449 	 0.00027 	 m..s
   27 	    25 	 0.08413 	 0.00028 	 m..s
   47 	    26 	 0.09587 	 0.00031 	 m..s
    7 	    27 	 0.04496 	 0.00035 	 m..s
   39 	    28 	 0.09408 	 0.00035 	 m..s
   67 	    29 	 0.10350 	 0.00036 	 MISS
   73 	    30 	 0.10534 	 0.00040 	 MISS
   43 	    31 	 0.09557 	 0.00043 	 m..s
   17 	    32 	 0.05668 	 0.00050 	 m..s
   75 	    33 	 0.10570 	 0.00078 	 MISS
   23 	    34 	 0.06065 	 0.00089 	 m..s
   45 	    35 	 0.09579 	 0.00093 	 m..s
   72 	    36 	 0.10531 	 0.00094 	 MISS
   41 	    37 	 0.09449 	 0.00111 	 m..s
   79 	    38 	 0.11082 	 0.00117 	 MISS
    0 	    39 	 0.03776 	 0.00150 	 m..s
   24 	    40 	 0.06071 	 0.00201 	 m..s
    4 	    41 	 0.04236 	 0.00457 	 m..s
   16 	    42 	 0.05606 	 0.00823 	 m..s
   15 	    43 	 0.05593 	 0.00826 	 m..s
   21 	    44 	 0.05783 	 0.00828 	 m..s
   26 	    45 	 0.08241 	 0.00954 	 m..s
   19 	    46 	 0.05755 	 0.00963 	 m..s
   18 	    47 	 0.05701 	 0.01934 	 m..s
   78 	    48 	 0.10774 	 0.02558 	 m..s
   40 	    49 	 0.09422 	 0.02922 	 m..s
   25 	    50 	 0.07777 	 0.03967 	 m..s
   38 	    51 	 0.09172 	 0.05864 	 m..s
   55 	    52 	 0.09745 	 0.11583 	 ~...
   57 	    53 	 0.09791 	 0.12074 	 ~...
   48 	    54 	 0.09618 	 0.12856 	 m..s
   30 	    55 	 0.08851 	 0.13509 	 m..s
   62 	    56 	 0.09990 	 0.13617 	 m..s
   28 	    57 	 0.08589 	 0.13665 	 m..s
   33 	    58 	 0.08944 	 0.13893 	 m..s
   53 	    59 	 0.09711 	 0.14093 	 m..s
   35 	    60 	 0.09065 	 0.14152 	 m..s
   37 	    61 	 0.09154 	 0.14604 	 m..s
   49 	    62 	 0.09630 	 0.14656 	 m..s
   70 	    63 	 0.10481 	 0.14730 	 m..s
   85 	    64 	 0.19635 	 0.15238 	 m..s
   59 	    65 	 0.09870 	 0.15279 	 m..s
   66 	    66 	 0.10173 	 0.15486 	 m..s
   58 	    67 	 0.09792 	 0.15565 	 m..s
   56 	    68 	 0.09784 	 0.15758 	 m..s
   87 	    69 	 0.20137 	 0.15777 	 m..s
   52 	    70 	 0.09703 	 0.15815 	 m..s
   71 	    71 	 0.10495 	 0.15856 	 m..s
   68 	    72 	 0.10393 	 0.15898 	 m..s
   81 	    73 	 0.15794 	 0.15951 	 ~...
   97 	    74 	 0.22667 	 0.16260 	 m..s
   80 	    75 	 0.11221 	 0.16301 	 m..s
   61 	    76 	 0.09927 	 0.16303 	 m..s
   50 	    77 	 0.09686 	 0.16376 	 m..s
   54 	    78 	 0.09725 	 0.16513 	 m..s
   69 	    79 	 0.10403 	 0.16809 	 m..s
   74 	    80 	 0.10558 	 0.17491 	 m..s
   77 	    81 	 0.10596 	 0.17646 	 m..s
   65 	    82 	 0.10094 	 0.17704 	 m..s
   83 	    83 	 0.18144 	 0.17963 	 ~...
   46 	    84 	 0.09583 	 0.19208 	 m..s
   76 	    85 	 0.10593 	 0.19358 	 m..s
   82 	    86 	 0.16765 	 0.19489 	 ~...
   98 	    87 	 0.22676 	 0.19955 	 ~...
   92 	    88 	 0.20959 	 0.20377 	 ~...
   86 	    89 	 0.19701 	 0.20470 	 ~...
   94 	    90 	 0.21638 	 0.20496 	 ~...
   91 	    91 	 0.20915 	 0.20586 	 ~...
   88 	    92 	 0.20192 	 0.20910 	 ~...
  103 	    93 	 0.24386 	 0.20955 	 m..s
   96 	    94 	 0.22235 	 0.21094 	 ~...
   84 	    95 	 0.19276 	 0.21745 	 ~...
   93 	    96 	 0.21037 	 0.21901 	 ~...
   99 	    97 	 0.22925 	 0.22873 	 ~...
   89 	    98 	 0.20299 	 0.23149 	 ~...
   95 	    99 	 0.21808 	 0.24128 	 ~...
  100 	   100 	 0.23100 	 0.24150 	 ~...
  102 	   101 	 0.24001 	 0.24172 	 ~...
  101 	   102 	 0.23424 	 0.24229 	 ~...
   90 	   103 	 0.20720 	 0.25014 	 m..s
  106 	   104 	 0.27941 	 0.26466 	 ~...
  104 	   105 	 0.27021 	 0.26510 	 ~...
  112 	   106 	 0.31650 	 0.27314 	 m..s
  107 	   107 	 0.28161 	 0.27333 	 ~...
  105 	   108 	 0.27663 	 0.27975 	 ~...
  108 	   109 	 0.29510 	 0.29003 	 ~...
  109 	   110 	 0.29775 	 0.29297 	 ~...
  111 	   111 	 0.30884 	 0.29470 	 ~...
  110 	   112 	 0.30700 	 0.31202 	 ~...
  117 	   113 	 0.35702 	 0.31608 	 m..s
  114 	   114 	 0.34048 	 0.33015 	 ~...
  115 	   115 	 0.34248 	 0.33164 	 ~...
  116 	   116 	 0.35010 	 0.34760 	 ~...
  113 	   117 	 0.33344 	 0.34918 	 ~...
  118 	   118 	 0.38714 	 0.36330 	 ~...
  119 	   119 	 0.38956 	 0.36564 	 ~...
  120 	   120 	 0.39707 	 0.37335 	 ~...
==========================================
r_mrr = 0.8834872245788574
r2_mrr = 0.7479745149612427
spearmanr_mrr@5 = 0.9840233325958252
spearmanr_mrr@10 = 0.9728929996490479
spearmanr_mrr@50 = 0.9783901572227478
spearmanr_mrr@100 = 0.8875871300697327
spearmanr_mrr@All = 0.9114342927932739
==========================================
test time: 0.54
Done Testing dataset DBpedia50
total time taken: 184.54772210121155
training time taken: 179.81549310684204
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8835)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.7480)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9840)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9729)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9784)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8876)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9114)}}, 'test_loss': {'DistMult': {'DBpedia50': 1.8024012570058403}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7325289107363570
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [920, 502, 184, 1133, 500, 592, 668, 1148, 258, 1174, 767, 1046, 887, 70, 295, 487, 1175, 652, 120, 16, 516, 321, 1099, 508, 1113, 1053, 52, 970, 530, 133, 1045, 325, 243, 797, 438, 1152, 1176, 271, 1102, 608, 1170, 625, 1210, 597, 1181, 369, 851, 994, 273, 383, 89, 496, 968, 181, 1038, 969, 84, 939, 698, 929, 99, 653, 1011, 460, 177, 973, 620, 82, 1031, 977, 840, 503, 1199, 832, 279, 1060, 548, 68, 991, 439, 1016, 450, 408, 388, 87, 341, 606, 1091, 1186, 126, 563, 69, 256, 728, 1139, 251, 556, 922, 3, 14, 434, 1026, 714, 482, 864, 941, 1125, 918, 658, 716, 230, 945, 749, 574, 687, 1171, 1049, 1012, 18, 37, 580]
valid_ids (0): []
train_ids (1094): [1094, 418, 927, 400, 1054, 201, 878, 111, 550, 289, 246, 39, 40, 986, 389, 75, 506, 925, 440, 380, 891, 898, 744, 857, 119, 513, 776, 821, 672, 953, 1119, 818, 946, 691, 303, 476, 0, 909, 268, 8, 285, 1015, 199, 865, 419, 727, 1190, 223, 729, 1043, 182, 480, 924, 1201, 1197, 297, 537, 45, 1112, 573, 517, 560, 957, 1137, 551, 93, 240, 631, 1041, 338, 789, 451, 1184, 628, 1179, 769, 1040, 614, 307, 1081, 562, 1114, 1150, 889, 838, 700, 121, 585, 740, 913, 812, 187, 1159, 591, 1183, 527, 932, 993, 558, 493, 456, 237, 1117, 160, 1172, 895, 967, 42, 534, 190, 661, 130, 1059, 290, 1071, 762, 155, 208, 207, 481, 141, 1196, 826, 484, 168, 478, 1212, 218, 495, 755, 626, 124, 754, 235, 162, 224, 173, 464, 1187, 324, 463, 978, 422, 647, 1193, 1124, 809, 611, 618, 100, 28, 853, 371, 71, 35, 686, 676, 645, 702, 954, 1028, 656, 813, 346, 919, 535, 515, 526, 955, 671, 747, 486, 869, 989, 455, 533, 900, 917, 304, 145, 417, 90, 518, 1143, 1009, 961, 241, 83, 431, 1032, 1200, 884, 221, 1214, 314, 217, 1021, 1083, 349, 1204, 1065, 1019, 51, 106, 31, 879, 1131, 1156, 854, 972, 1169, 971, 326, 801, 1127, 651, 416, 567, 347, 858, 806, 191, 1108, 600, 62, 699, 421, 681, 483, 196, 603, 437, 873, 405, 77, 219, 741, 281, 781, 866, 1055, 657, 856, 97, 169, 947, 188, 981, 916, 1023, 1084, 316, 1090, 992, 150, 339, 568, 570, 1182, 212, 452, 860, 862, 791, 29, 844, 161, 1085, 475, 876, 209, 733, 469, 498, 1030, 1157, 1118, 708, 514, 612, 435, 679, 1014, 137, 158, 807, 61, 306, 1158, 131, 828, 760, 379, 151, 78, 286, 1105, 587, 1213, 931, 710, 646, 1198, 522, 731, 673, 139, 1077, 1062, 852, 694, 870, 332, 311, 110, 787, 649, 38, 565, 943, 63, 892, 144, 512, 538, 665, 842, 904, 394, 914, 291, 270, 950, 32, 178, 340, 393, 391, 1075, 489, 377, 525, 1069, 1086, 982, 1001, 1202, 56, 157, 354, 693, 1138, 1206, 829, 704, 72, 319, 677, 350, 59, 1130, 888, 598, 1164, 125, 765, 963, 983, 468, 453, 1111, 24, 644, 761, 1154, 547, 477, 293, 1189, 976, 539, 1002, 320, 616, 604, 578, 793, 890, 774, 262, 1078, 461, 726, 85, 413, 1136, 175, 1050, 770, 819, 472, 1110, 855, 752, 707, 328, 984, 1155, 544, 1101, 905, 875, 798, 272, 1057, 1018, 406, 1058, 176, 689, 105, 443, 1024, 259, 613, 846, 910, 1096, 488, 206, 269, 352, 923, 822, 25, 666, 283, 334, 609, 524, 893, 959, 412, 494, 763, 232, 88, 22, 764, 81, 958, 997, 127, 784, 936, 66, 129, 385, 12, 933, 835, 794, 238, 885, 778, 519, 331, 715, 874, 1140, 1017, 1104, 777, 629, 1191, 1100, 799, 47, 724, 1185, 542, 872, 205, 610, 594, 590, 101, 280, 662, 115, 210, 718, 951, 1020, 965, 935, 785, 630, 429, 847, 395, 287, 186, 588, 103, 617, 359, 366, 743, 102, 20, 449, 198, 593, 1177, 678, 1160, 323, 58, 147, 192, 664, 696, 1116, 109, 409, 607, 757, 897, 1064, 674, 226, 949, 639, 682, 344, 841, 446, 814, 154, 557, 583, 333, 1163, 1173, 112, 605, 786, 420, 690, 1072, 466, 479, 1033, 701, 995, 685, 54, 403, 255, 312, 149, 589, 436, 41, 734, 355, 447, 263, 723, 859, 353, 579, 153, 92, 1, 529, 128, 30, 899, 1192, 203, 523, 1147, 411, 1103, 926, 309, 908, 123, 156, 824, 660, 684, 1098, 335, 980, 596, 930, 117, 257, 601, 1207, 302, 402, 720, 17, 1109, 880, 5, 632, 1178, 10, 247, 337, 571, 1141, 64, 1037, 1006, 136, 228, 470, 737, 705, 680, 1082, 575, 713, 1208, 1079, 261, 284, 572, 670, 780, 148, 467, 135, 638, 397, 848, 342, 759, 559, 396, 751, 43, 390, 771, 552, 1162, 634, 398, 27, 800, 877, 937, 944, 962, 564, 9, 107, 1151, 881, 1180, 654, 387, 619, 810, 627, 1047, 275, 1056, 602, 26, 50, 1004, 820, 717, 911, 278, 633, 239, 849, 21, 811, 663, 1027, 1129, 473, 975, 803, 425, 1035, 659, 1097, 211, 351, 566, 985, 301, 772, 831, 492, 1168, 637, 1115, 688, 471, 202, 432, 942, 1165, 1088, 1003, 692, 640, 655, 882, 553, 1123, 775, 368, 44, 541, 528, 845, 505, 1153, 850, 1106, 624, 706, 252, 166, 372, 225, 367, 172, 966, 348, 53, 1121, 894, 424, 376, 448, 1005, 1188, 444, 902, 1194, 1205, 214, 915, 536, 370, 248, 292, 648, 134, 315, 282, 1070, 996, 180, 531, 1211, 990, 96, 392, 883, 1022, 906, 796, 98, 987, 1080, 57, 364, 94, 756, 712, 622, 839, 1034, 782, 1013, 511, 213, 1052, 374, 277, 790, 220, 222, 329, 65, 116, 545, 1042, 825, 91, 561, 896, 549, 465, 288, 1134, 48, 318, 871, 104, 650, 159, 675, 861, 520, 1149, 276, 1128, 1107, 697, 490, 76, 683, 948, 501, 298, 1063, 802, 378, 15, 363, 742, 532, 423, 1008, 95, 132, 138, 294, 499, 709, 474, 1036, 867, 365, 382, 783, 868, 952, 46, 739, 974, 113, 928, 1120, 204, 249, 736, 510, 122, 491, 921, 236, 361, 1048, 635, 384, 1051, 183, 497, 843, 555, 576, 414, 903, 362, 938, 1068, 641, 231, 1209, 360, 459, 722, 912, 80, 410, 4, 1122, 34, 695, 457, 1195, 1166, 901, 266, 1073, 586, 23, 823, 74, 67, 1000, 1087, 299, 441, 234, 956, 1132, 768, 415, 725, 152, 711, 358, 540, 750, 313, 194, 442, 357, 250, 274, 336, 1029, 356, 322, 907, 433, 13, 373, 426, 485, 507, 1135, 817, 1144, 171, 521, 254, 317, 79, 310, 330, 758, 140, 343, 1142, 1095, 253, 185, 73, 49, 33, 804, 264, 833, 595, 827, 546, 245, 195, 399, 1066, 830, 1039, 643, 19, 345, 216, 816, 454, 577, 375, 667, 584, 999, 703, 428, 233, 621, 773, 581, 163, 863, 1010, 458, 197, 308, 242, 11, 1146, 6, 427, 2, 504, 227, 215, 960, 193, 748, 179, 988, 746, 623, 836, 1161, 260, 1061, 1126, 719, 170, 1093, 615, 738, 404, 636, 554, 805, 114, 167, 244, 886, 1067, 1145, 164, 934, 60, 174, 732, 795, 582, 1074, 669, 265, 300, 642, 964, 108, 1092, 753, 1044, 1007, 1089, 305, 808, 792, 543, 407, 430, 998, 1203, 142, 599, 36, 779, 788, 55, 569, 745, 86, 267, 815, 146, 766, 979, 735, 229, 445, 327, 1167, 509, 200, 401, 143, 1025, 462, 837, 940, 834, 381, 7, 1076, 721, 730, 189, 296, 386, 165, 118]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3975236193949154
the save name prefix for this run is:  chkpt-ID_3975236193949154_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 713
rank avg (pred): 0.592 +- 0.008
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002888414

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.445 +- 0.260
mrr vals (pred, true): 0.174, 0.000
batch losses (mrrl, rdl): 0.0, 1.9823e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 475
rank avg (pred): 0.388 +- 0.240
mrr vals (pred, true): 0.244, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001837636

Epoch over!
epoch time: 11.908

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 422
rank avg (pred): 0.375 +- 0.231
mrr vals (pred, true): 0.250, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002709915

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 39
rank avg (pred): 0.153 +- 0.095
mrr vals (pred, true): 0.261, 0.224
batch losses (mrrl, rdl): 0.0, 9.68677e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.364 +- 0.225
mrr vals (pred, true): 0.260, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001278001

Epoch over!
epoch time: 12.057

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 568
rank avg (pred): 0.412 +- 0.255
mrr vals (pred, true): 0.260, 0.134
batch losses (mrrl, rdl): 0.0, 0.0003281944

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1207
rank avg (pred): 0.411 +- 0.254
mrr vals (pred, true): 0.260, 0.000
batch losses (mrrl, rdl): 0.0, 2.91603e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1111
rank avg (pred): 0.407 +- 0.252
mrr vals (pred, true): 0.261, 0.000
batch losses (mrrl, rdl): 0.0, 6.42277e-05

Epoch over!
epoch time: 11.713

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.364 +- 0.225
mrr vals (pred, true): 0.262, 0.153
batch losses (mrrl, rdl): 0.0, 0.0002934503

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 575
rank avg (pred): 0.410 +- 0.254
mrr vals (pred, true): 0.259, 0.121
batch losses (mrrl, rdl): 0.0, 0.0002833014

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 351
rank avg (pred): 0.394 +- 0.259
mrr vals (pred, true): 0.260, 0.146
batch losses (mrrl, rdl): 0.0, 0.0002590324

Epoch over!
epoch time: 11.649

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1150
rank avg (pred): 0.271 +- 0.179
mrr vals (pred, true): 0.264, 0.220
batch losses (mrrl, rdl): 0.0, 9.5017e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.398 +- 0.259
mrr vals (pred, true): 0.258, 0.000
batch losses (mrrl, rdl): 0.0, 9.86926e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1081
rank avg (pred): 0.371 +- 0.243
mrr vals (pred, true): 0.259, 0.132
batch losses (mrrl, rdl): 0.0, 0.0001855855

Epoch over!
epoch time: 11.922

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1183
rank avg (pred): 0.398 +- 0.262
mrr vals (pred, true): 0.260, 0.145
batch losses (mrrl, rdl): 0.1328210384, 5.22236e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.259 +- 0.178
mrr vals (pred, true): 0.113, 0.145
batch losses (mrrl, rdl): 0.0102394829, 0.000234139

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 679
rank avg (pred): 0.253 +- 0.185
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0281420313, 0.000949617

Epoch over!
epoch time: 12.324

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1073
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.279, 0.257
batch losses (mrrl, rdl): 0.0048881192, 0.0005385514

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 839
rank avg (pred): 0.556 +- 0.267
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001951764, 0.0001455051

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1163
rank avg (pred): 0.292 +- 0.182
mrr vals (pred, true): 0.094, 0.140
batch losses (mrrl, rdl): 0.0211206432, 8.529e-05

Epoch over!
epoch time: 11.987

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 417
rank avg (pred): 0.286 +- 0.200
mrr vals (pred, true): 0.093, 0.000
batch losses (mrrl, rdl): 0.0181128196, 0.0007812831

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 978
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.390, 0.348
batch losses (mrrl, rdl): 0.0175071172, 0.0003684973

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 874
rank avg (pred): 0.421 +- 0.194
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0144973593, 0.0001031784

Epoch over!
epoch time: 11.912

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.287 +- 0.197
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0263155475, 0.0012901352

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 855
rank avg (pred): 0.456 +- 0.189
mrr vals (pred, true): 0.051, 0.009
batch losses (mrrl, rdl): 2.08406e-05, 0.0002148782

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 100
rank avg (pred): 0.268 +- 0.205
mrr vals (pred, true): 0.106, 0.200
batch losses (mrrl, rdl): 0.0869477838, 9.92142e-05

Epoch over!
epoch time: 12.107

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 182
rank avg (pred): 0.258 +- 0.200
mrr vals (pred, true): 0.132, 0.000
batch losses (mrrl, rdl): 0.0668174773, 0.0010406239

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.356 +- 0.214
mrr vals (pred, true): 0.092, 0.139
batch losses (mrrl, rdl): 0.022179082, 5.9589e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 582
rank avg (pred): 0.461 +- 0.222
mrr vals (pred, true): 0.074, 0.131
batch losses (mrrl, rdl): 0.0327316038, 0.0002929951

Epoch over!
epoch time: 12.077

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.477 +- 0.246
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0109221414, 1.55124e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 613
rank avg (pred): 0.377 +- 0.253
mrr vals (pred, true): 0.106, 0.146
batch losses (mrrl, rdl): 0.016242506, 3.04277e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 102
rank avg (pred): 0.445 +- 0.326
mrr vals (pred, true): 0.103, 0.159
batch losses (mrrl, rdl): 0.0314231627, 0.000388212

Epoch over!
epoch time: 12.4

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.449 +- 0.324
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0333235636, 2.12011e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 876
rank avg (pred): 0.440 +- 0.157
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0011145826, 0.0001194564

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1195
rank avg (pred): 0.494 +- 0.238
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0159777123, 2.29038e-05

Epoch over!
epoch time: 12.081

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1066
rank avg (pred): 0.149 +- 0.125
mrr vals (pred, true): 0.286, 0.312
batch losses (mrrl, rdl): 0.006852753, 5.55374e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 144
rank avg (pred): 0.408 +- 0.244
mrr vals (pred, true): 0.098, 0.144
batch losses (mrrl, rdl): 0.021531526, 0.0002769734

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1072
rank avg (pred): 0.208 +- 0.171
mrr vals (pred, true): 0.244, 0.276
batch losses (mrrl, rdl): 0.0101174917, 8.01372e-05

Epoch over!
epoch time: 11.967

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 514
rank avg (pred): 0.297 +- 0.244
mrr vals (pred, true): 0.158, 0.171
batch losses (mrrl, rdl): 0.0018323597, 6.75008e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 75
rank avg (pred): 0.051 +- 0.043
mrr vals (pred, true): 0.256, 0.231
batch losses (mrrl, rdl): 0.0060425634, 0.0003038071

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 697
rank avg (pred): 0.393 +- 0.221
mrr vals (pred, true): 0.103, 0.001
batch losses (mrrl, rdl): 0.0276611187, 6.21843e-05

Epoch over!
epoch time: 11.951

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 878
rank avg (pred): 0.451 +- 0.151
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003070261, 5.69217e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 229
rank avg (pred): 0.385 +- 0.203
mrr vals (pred, true): 0.099, 0.000
batch losses (mrrl, rdl): 0.023709543, 0.0003142268

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 494
rank avg (pred): 0.180 +- 0.151
mrr vals (pred, true): 0.298, 0.291
batch losses (mrrl, rdl): 0.0004585489, 0.0002165713

Epoch over!
epoch time: 11.96

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.659 +- 0.283
mrr vals (pred, true): 0.054, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.05853 	 5e-0500 	 m..s
    9 	     1 	 0.05978 	 6e-0500 	 m..s
    1 	     2 	 0.05437 	 0.00012 	 m..s
   19 	     3 	 0.10217 	 0.00014 	 MISS
   67 	     4 	 0.11741 	 0.00015 	 MISS
   45 	     5 	 0.10952 	 0.00016 	 MISS
   82 	     6 	 0.15979 	 0.00016 	 MISS
    2 	     7 	 0.05485 	 0.00018 	 m..s
    0 	     8 	 0.05366 	 0.00018 	 m..s
   27 	     9 	 0.10395 	 0.00018 	 MISS
   11 	    10 	 0.06109 	 0.00018 	 m..s
   12 	    11 	 0.06647 	 0.00020 	 m..s
   52 	    12 	 0.11129 	 0.00021 	 MISS
   59 	    13 	 0.11307 	 0.00021 	 MISS
   60 	    14 	 0.11451 	 0.00023 	 MISS
   10 	    15 	 0.06088 	 0.00023 	 m..s
   46 	    16 	 0.10964 	 0.00023 	 MISS
   53 	    17 	 0.11163 	 0.00024 	 MISS
   54 	    18 	 0.11230 	 0.00024 	 MISS
    4 	    19 	 0.05826 	 0.00025 	 m..s
   62 	    20 	 0.11493 	 0.00025 	 MISS
   41 	    21 	 0.10917 	 0.00026 	 MISS
   30 	    22 	 0.10533 	 0.00026 	 MISS
   39 	    23 	 0.10896 	 0.00026 	 MISS
   42 	    24 	 0.10936 	 0.00028 	 MISS
   20 	    25 	 0.10225 	 0.00029 	 MISS
   43 	    26 	 0.10947 	 0.00030 	 MISS
   51 	    27 	 0.11104 	 0.00030 	 MISS
   37 	    28 	 0.10784 	 0.00032 	 MISS
   80 	    29 	 0.15582 	 0.00033 	 MISS
   58 	    30 	 0.11289 	 0.00034 	 MISS
   32 	    31 	 0.10601 	 0.00036 	 MISS
   63 	    32 	 0.11501 	 0.00038 	 MISS
   56 	    33 	 0.11265 	 0.00038 	 MISS
    3 	    34 	 0.05493 	 0.00039 	 m..s
   76 	    35 	 0.12432 	 0.00040 	 MISS
   40 	    36 	 0.10910 	 0.00040 	 MISS
   15 	    37 	 0.07756 	 0.00046 	 m..s
    8 	    38 	 0.05855 	 0.00054 	 m..s
   16 	    39 	 0.10110 	 0.00075 	 MISS
   14 	    40 	 0.07710 	 0.00089 	 m..s
   49 	    41 	 0.11022 	 0.00093 	 MISS
   78 	    42 	 0.13614 	 0.00094 	 MISS
   81 	    43 	 0.15673 	 0.00122 	 MISS
    5 	    44 	 0.05838 	 0.00149 	 m..s
    7 	    45 	 0.05855 	 0.00554 	 m..s
   13 	    46 	 0.06763 	 0.02709 	 m..s
   48 	    47 	 0.10997 	 0.11249 	 ~...
   47 	    48 	 0.10996 	 0.11677 	 ~...
   24 	    49 	 0.10296 	 0.11810 	 ~...
   28 	    50 	 0.10425 	 0.12289 	 ~...
   31 	    51 	 0.10569 	 0.12420 	 ~...
   25 	    52 	 0.10300 	 0.12726 	 ~...
   44 	    53 	 0.10951 	 0.12838 	 ~...
   23 	    54 	 0.10276 	 0.13434 	 m..s
   21 	    55 	 0.10244 	 0.13873 	 m..s
   57 	    56 	 0.11284 	 0.13874 	 ~...
   22 	    57 	 0.10248 	 0.13896 	 m..s
   29 	    58 	 0.10458 	 0.14021 	 m..s
   61 	    59 	 0.11487 	 0.14108 	 ~...
   66 	    60 	 0.11714 	 0.14193 	 ~...
   26 	    61 	 0.10385 	 0.14349 	 m..s
   38 	    62 	 0.10851 	 0.14632 	 m..s
   72 	    63 	 0.11971 	 0.14657 	 ~...
   83 	    64 	 0.18284 	 0.14756 	 m..s
   36 	    65 	 0.10777 	 0.15047 	 m..s
   35 	    66 	 0.10711 	 0.15489 	 m..s
   33 	    67 	 0.10628 	 0.15541 	 m..s
   34 	    68 	 0.10669 	 0.15579 	 m..s
   65 	    69 	 0.11566 	 0.15758 	 m..s
   77 	    70 	 0.12508 	 0.15831 	 m..s
   17 	    71 	 0.10131 	 0.15865 	 m..s
   18 	    72 	 0.10200 	 0.15898 	 m..s
   69 	    73 	 0.11794 	 0.16009 	 m..s
   50 	    74 	 0.11050 	 0.16049 	 m..s
   71 	    75 	 0.11954 	 0.16170 	 m..s
   68 	    76 	 0.11752 	 0.16302 	 m..s
   55 	    77 	 0.11251 	 0.16303 	 m..s
   84 	    78 	 0.18622 	 0.16495 	 ~...
   64 	    79 	 0.11560 	 0.16513 	 m..s
   74 	    80 	 0.12226 	 0.16857 	 m..s
   75 	    81 	 0.12422 	 0.18258 	 m..s
   85 	    82 	 0.18803 	 0.18464 	 ~...
   79 	    83 	 0.13650 	 0.18493 	 m..s
   73 	    84 	 0.12177 	 0.18553 	 m..s
   89 	    85 	 0.22074 	 0.18865 	 m..s
   70 	    86 	 0.11887 	 0.19208 	 m..s
   88 	    87 	 0.22039 	 0.19494 	 ~...
   86 	    88 	 0.19377 	 0.20349 	 ~...
   90 	    89 	 0.22599 	 0.21272 	 ~...
   97 	    90 	 0.24856 	 0.21437 	 m..s
   91 	    91 	 0.23247 	 0.21581 	 ~...
   94 	    92 	 0.23765 	 0.21655 	 ~...
   96 	    93 	 0.24586 	 0.22174 	 ~...
   87 	    94 	 0.22005 	 0.22461 	 ~...
  103 	    95 	 0.28313 	 0.23077 	 m..s
   95 	    96 	 0.24413 	 0.23281 	 ~...
   92 	    97 	 0.23595 	 0.23829 	 ~...
   99 	    98 	 0.27224 	 0.24670 	 ~...
  104 	    99 	 0.28823 	 0.25390 	 m..s
   98 	   100 	 0.27008 	 0.26510 	 ~...
  100 	   101 	 0.27774 	 0.27333 	 ~...
   93 	   102 	 0.23735 	 0.27412 	 m..s
  101 	   103 	 0.27886 	 0.27768 	 ~...
  108 	   104 	 0.31362 	 0.28396 	 ~...
  105 	   105 	 0.29067 	 0.28499 	 ~...
  107 	   106 	 0.29822 	 0.28797 	 ~...
  102 	   107 	 0.28161 	 0.29360 	 ~...
  111 	   108 	 0.33836 	 0.30207 	 m..s
  119 	   109 	 0.35001 	 0.31099 	 m..s
  106 	   110 	 0.29085 	 0.31455 	 ~...
  112 	   111 	 0.33939 	 0.32678 	 ~...
  118 	   112 	 0.34855 	 0.33220 	 ~...
  114 	   113 	 0.34114 	 0.33731 	 ~...
  115 	   114 	 0.34232 	 0.34745 	 ~...
  110 	   115 	 0.33164 	 0.34760 	 ~...
  109 	   116 	 0.32627 	 0.34918 	 ~...
  113 	   117 	 0.34065 	 0.35651 	 ~...
  120 	   118 	 0.35591 	 0.36280 	 ~...
  117 	   119 	 0.34651 	 0.36720 	 ~...
  116 	   120 	 0.34526 	 0.37411 	 ~...
==========================================
r_mrr = 0.8623972535133362
r2_mrr = 0.6726154088973999
spearmanr_mrr@5 = 0.9428256154060364
spearmanr_mrr@10 = 0.9571846127510071
spearmanr_mrr@50 = 0.9727789163589478
spearmanr_mrr@100 = 0.8897634744644165
spearmanr_mrr@All = 0.9101647138595581
==========================================
test time: 0.478
Done Testing dataset DBpedia50
total time taken: 185.13917636871338
training time taken: 180.5600757598877
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8624)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6726)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9428)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9572)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9728)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8898)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9102)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.2716343843057984}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 3732072874912622
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [83, 176, 591, 121, 1054, 745, 993, 164, 461, 1061, 847, 194, 599, 436, 430, 362, 893, 1075, 848, 914, 1109, 119, 33, 1202, 846, 337, 1148, 629, 115, 509, 1071, 539, 311, 871, 1116, 153, 534, 936, 426, 257, 756, 37, 243, 887, 1151, 963, 958, 903, 374, 608, 308, 1070, 476, 175, 763, 899, 333, 760, 1145, 54, 603, 246, 794, 519, 796, 313, 635, 880, 56, 474, 6, 823, 829, 195, 1101, 1063, 878, 144, 790, 689, 475, 331, 815, 356, 435, 777, 491, 585, 497, 697, 465, 909, 1194, 464, 1083, 456, 989, 981, 412, 406, 930, 261, 172, 390, 378, 405, 90, 167, 384, 844, 233, 1181, 399, 531, 336, 758, 85, 163, 761, 286, 984]
valid_ids (0): []
train_ids (1094): [875, 62, 1066, 877, 741, 990, 417, 343, 1125, 929, 1014, 1123, 955, 403, 671, 1146, 801, 285, 791, 256, 792, 23, 684, 1019, 650, 35, 1126, 806, 386, 649, 938, 583, 563, 705, 87, 713, 92, 1105, 391, 607, 548, 573, 1168, 483, 707, 822, 12, 470, 965, 107, 1113, 303, 595, 1093, 126, 422, 641, 982, 493, 632, 22, 368, 67, 602, 1049, 340, 811, 578, 716, 288, 553, 111, 864, 881, 423, 214, 621, 1004, 654, 244, 189, 506, 554, 728, 751, 890, 110, 1047, 49, 407, 65, 701, 1144, 1154, 197, 98, 10, 1090, 351, 879, 326, 917, 1082, 415, 746, 648, 853, 510, 275, 320, 316, 322, 503, 365, 287, 652, 21, 865, 1053, 535, 136, 895, 1176, 278, 432, 201, 1128, 307, 885, 540, 1097, 622, 862, 1201, 112, 724, 170, 225, 185, 514, 805, 1196, 749, 886, 1078, 849, 729, 868, 342, 610, 279, 1058, 492, 84, 1163, 158, 1141, 488, 66, 908, 284, 447, 251, 772, 1080, 884, 904, 200, 557, 769, 755, 827, 81, 1212, 830, 147, 773, 1206, 670, 485, 413, 951, 562, 1189, 20, 105, 36, 738, 788, 859, 148, 262, 516, 224, 103, 732, 655, 481, 851, 935, 1012, 269, 1150, 690, 766, 370, 517, 924, 797, 236, 582, 4, 241, 734, 712, 381, 633, 80, 611, 1197, 344, 379, 874, 783, 1118, 99, 888, 957, 1167, 1110, 778, 559, 742, 793, 765, 397, 997, 387, 495, 994, 679, 389, 1041, 234, 180, 1074, 60, 845, 438, 710, 1121, 318, 151, 782, 525, 513, 623, 706, 544, 396, 134, 102, 979, 218, 42, 658, 94, 572, 926, 215, 202, 1002, 454, 346, 804, 714, 567, 424, 940, 267, 1022, 1065, 941, 187, 91, 717, 733, 678, 643, 928, 921, 1057, 1149, 32, 463, 767, 663, 31, 1133, 907, 479, 392, 905, 208, 293, 825, 1021, 77, 46, 837, 718, 58, 9, 486, 613, 520, 776, 645, 1205, 703, 1119, 532, 157, 677, 1036, 942, 1028, 1204, 211, 852, 1087, 821, 118, 814, 1214, 664, 524, 526, 143, 781, 694, 468, 1129, 931, 818, 274, 784, 489, 681, 1001, 372, 138, 108, 79, 128, 1084, 350, 1077, 660, 918, 740, 1011, 433, 832, 437, 297, 787, 428, 715, 268, 1031, 809, 150, 238, 15, 45, 161, 948, 836, 1102, 249, 76, 310, 956, 969, 1188, 100, 972, 722, 580, 527, 443, 1015, 579, 1136, 444, 1135, 89, 259, 181, 124, 7, 891, 523, 1076, 1023, 394, 575, 977, 358, 184, 473, 484, 600, 135, 114, 345, 371, 472, 744, 188, 642, 63, 634, 699, 619, 604, 26, 306, 348, 160, 858, 75, 1195, 1138, 298, 779, 537, 704, 248, 409, 449, 367, 518, 636, 292, 154, 1182, 727, 305, 589, 141, 2, 304, 70, 721, 925, 171, 1, 983, 383, 725, 496, 1186, 43, 330, 339, 651, 960, 1132, 68, 51, 477, 1112, 235, 975, 55, 731, 590, 228, 283, 120, 1172, 850, 726, 838, 876, 231, 263, 441, 624, 667, 301, 113, 500, 317, 177, 451, 946, 1127, 894, 762, 1098, 869, 460, 361, 452, 702, 295, 425, 1060, 616, 617, 291, 1020, 349, 687, 1104, 1055, 359, 676, 873, 536, 980, 166, 939, 369, 759, 352, 558, 270, 223, 471, 482, 584, 770, 834, 97, 934, 408, 14, 1180, 27, 1027, 1164, 419, 910, 1152, 854, 1140, 1016, 123, 1170, 179, 754, 954, 892, 596, 57, 1086, 82, 870, 1193, 668, 820, 1103, 469, 669, 817, 803, 637, 1203, 1029, 597, 711, 1190, 156, 1209, 686, 1174, 675, 656, 183, 101, 357, 467, 302, 199, 1183, 1072, 192, 819, 696, 962, 1124, 429, 1122, 618, 480, 866, 1159, 1198, 512, 137, 565, 324, 1211, 971, 1155, 511, 457, 528, 657, 533, 221, 780, 768, 842, 680, 896, 673, 321, 612, 59, 427, 142, 264, 872, 0, 568, 53, 901, 96, 1096, 626, 1179, 665, 190, 289, 117, 315, 764, 576, 882, 1046, 606, 1115, 1130, 1137, 753, 556, 366, 1120, 93, 266, 132, 1161, 219, 50, 808, 1067, 952, 216, 169, 442, 265, 462, 186, 1165, 355, 494, 395, 393, 335, 1039, 708, 639, 688, 205, 332, 230, 906, 341, 418, 564, 646, 478, 661, 440, 73, 1007, 691, 719, 198, 683, 282, 314, 593, 327, 798, 1013, 1114, 453, 191, 466, 529, 546, 674, 682, 382, 1079, 786, 1185, 541, 130, 571, 659, 843, 95, 976, 1160, 1037, 915, 71, 48, 598, 1009, 1038, 203, 182, 220, 226, 609, 168, 978, 44, 17, 968, 1095, 577, 61, 152, 252, 404, 1099, 334, 1200, 258, 961, 373, 106, 24, 434, 125, 1068, 775, 401, 855, 730, 1017, 1142, 146, 638, 363, 294, 19, 856, 949, 299, 222, 431, 698, 398, 995, 1005, 30, 757, 771, 992, 631, 319, 547, 1171, 1094, 5, 140, 947, 1035, 826, 416, 18, 1192, 933, 1073, 1143, 841, 1207, 986, 247, 347, 810, 227, 867, 700, 1089, 750, 239, 1106, 375, 421, 122, 1158, 1032, 490, 860, 210, 254, 34, 959, 1064, 807, 1059, 644, 902, 504, 1052, 897, 974, 1173, 1111, 196, 325, 709, 40, 816, 276, 88, 74, 64, 863, 28, 1162, 640, 991, 662, 155, 328, 988, 919, 666, 149, 920, 1033, 1048, 193, 912, 11, 605, 802, 385, 945, 414, 1157, 1026, 209, 104, 923, 109, 828, 795, 1006, 627, 999, 927, 739, 501, 1169, 1208, 857, 145, 970, 620, 883, 922, 237, 628, 439, 458, 785, 207, 515, 323, 647, 499, 950, 38, 1003, 566, 281, 581, 229, 833, 260, 543, 774, 1025, 1184, 1040, 507, 789, 300, 1091, 1024, 813, 1210, 212, 133, 521, 1050, 1177, 173, 1056, 1107, 653, 743, 13, 360, 551, 752, 296, 273, 388, 550, 290, 410, 1131, 129, 1000, 271, 39, 502, 380, 41, 800, 165, 735, 569, 420, 561, 450, 953, 1018, 1199, 987, 1117, 943, 522, 174, 1081, 1134, 25, 505, 402, 748, 3, 601, 720, 587, 549, 560, 861, 538, 250, 1139, 400, 376, 911, 630, 498, 985, 455, 240, 542, 998, 459, 162, 217, 116, 799, 206, 448, 232, 840, 329, 1043, 69, 615, 570, 1042, 139, 309, 354, 900, 1147, 487, 964, 52, 736, 1156, 545, 552, 1153, 1030, 625, 1051, 1008, 280, 508, 1213, 723, 824, 245, 16, 586, 835, 695, 672, 1085, 1088, 693, 1100, 78, 353, 692, 47, 932, 555, 204, 1034, 253, 812, 242, 1045, 1187, 446, 967, 29, 574, 839, 996, 1044, 445, 747, 685, 966, 588, 898, 159, 530, 213, 131, 277, 127, 614, 973, 594, 272, 592, 255, 8, 937, 889, 1108, 1010, 178, 916, 377, 737, 831, 913, 411, 944, 86, 338, 364, 1062, 1166, 1175, 1178, 1191, 1069, 1092, 312, 72]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4575526622048045
the save name prefix for this run is:  chkpt-ID_4575526622048045_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 692
rank avg (pred): 0.545 +- 0.006
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001899372

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1177
rank avg (pred): 0.478 +- 0.341
mrr vals (pred, true): 0.052, 0.138
batch losses (mrrl, rdl): 0.0, 0.0003996131

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.370 +- 0.264
mrr vals (pred, true): 0.033, 0.000
batch losses (mrrl, rdl): 0.0, 0.00015239

Epoch over!
epoch time: 12.115

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.203 +- 0.149
mrr vals (pred, true): 0.042, 0.192
batch losses (mrrl, rdl): 0.0, 0.00010931

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.358 +- 0.273
mrr vals (pred, true): 0.103, 0.140
batch losses (mrrl, rdl): 0.0, 0.0005888134

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1166
rank avg (pred): 0.392 +- 0.306
mrr vals (pred, true): 0.188, 0.142
batch losses (mrrl, rdl): 0.0, 1.49061e-05

Epoch over!
epoch time: 11.84

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.131 +- 0.100
mrr vals (pred, true): 0.131, 0.194
batch losses (mrrl, rdl): 0.0, 6.5013e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 950
rank avg (pred): 0.583 +- 0.402
mrr vals (pred, true): 0.134, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003831794

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 831
rank avg (pred): 0.190 +- 0.149
mrr vals (pred, true): 0.188, 0.159
batch losses (mrrl, rdl): 0.0, 8.89284e-05

Epoch over!
epoch time: 11.974

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1152
rank avg (pred): 0.259 +- 0.201
mrr vals (pred, true): 0.153, 0.195
batch losses (mrrl, rdl): 0.0, 0.000110171

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 979
rank avg (pred): 0.134 +- 0.104
mrr vals (pred, true): 0.210, 0.364
batch losses (mrrl, rdl): 0.0, 5.63808e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 793
rank avg (pred): 0.524 +- 0.282
mrr vals (pred, true): 0.012, 0.000
batch losses (mrrl, rdl): 0.0, 6.97533e-05

Epoch over!
epoch time: 11.891

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1160
rank avg (pred): 0.281 +- 0.217
mrr vals (pred, true): 0.177, 0.225
batch losses (mrrl, rdl): 0.0, 9.51315e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 769
rank avg (pred): 0.466 +- 0.322
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001353559

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 344
rank avg (pred): 0.383 +- 0.294
mrr vals (pred, true): 0.211, 0.169
batch losses (mrrl, rdl): 0.0, 4.27408e-05

Epoch over!
epoch time: 11.958

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.372 +- 0.270
mrr vals (pred, true): 0.055, 0.026
batch losses (mrrl, rdl): 0.0002260221, 1.89486e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 885
rank avg (pred): 0.834 +- 0.313
mrr vals (pred, true): 0.074, 0.000
batch losses (mrrl, rdl): 0.0059639523, 0.00193393

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 67
rank avg (pred): 0.022 +- 0.017
mrr vals (pred, true): 0.269, 0.259
batch losses (mrrl, rdl): 0.0009062008, 0.0003508742

Epoch over!
epoch time: 12.109

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 611
rank avg (pred): 0.494 +- 0.290
mrr vals (pred, true): 0.103, 0.145
batch losses (mrrl, rdl): 0.0180889666, 0.0005285956

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 371
rank avg (pred): 0.413 +- 0.258
mrr vals (pred, true): 0.111, 0.178
batch losses (mrrl, rdl): 0.0444500707, 0.0003513632

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 31
rank avg (pred): 0.026 +- 0.020
mrr vals (pred, true): 0.210, 0.189
batch losses (mrrl, rdl): 0.0044269823, 0.0005106459

Epoch over!
epoch time: 12.185

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 952
rank avg (pred): 0.922 +- 0.213
mrr vals (pred, true): 0.031, 0.000
batch losses (mrrl, rdl): 0.0035546825, 0.003204284

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 882
rank avg (pred): 0.709 +- 0.244
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0007658557, 0.0011826078

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 258
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.323, 0.357
batch losses (mrrl, rdl): 0.0114016291, 0.0002019374

Epoch over!
epoch time: 12.19

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1042
rank avg (pred): 0.362 +- 0.213
mrr vals (pred, true): 0.098, 0.000
batch losses (mrrl, rdl): 0.0231344439, 0.0002689935

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 25
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.357, 0.363
batch losses (mrrl, rdl): 0.0004026286, 0.0003124173

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.381 +- 0.208
mrr vals (pred, true): 0.097, 0.157
batch losses (mrrl, rdl): 0.036703147, 0.0001228613

Epoch over!
epoch time: 12.162

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 235
rank avg (pred): 0.367 +- 0.217
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0260982178, 0.0003706284

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 927
rank avg (pred): 0.608 +- 0.197
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.3359e-05, 0.0001450586

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 648
rank avg (pred): 0.480 +- 0.199
mrr vals (pred, true): 0.070, 0.000
batch losses (mrrl, rdl): 0.0038429582, 3.50756e-05

Epoch over!
epoch time: 12.004

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 193
rank avg (pred): 0.361 +- 0.216
mrr vals (pred, true): 0.105, 0.000
batch losses (mrrl, rdl): 0.0303705502, 0.0003864115

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 729
rank avg (pred): 0.400 +- 0.204
mrr vals (pred, true): 0.098, 0.028
batch losses (mrrl, rdl): 0.0233351141, 0.0012245097

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 285
rank avg (pred): 0.084 +- 0.064
mrr vals (pred, true): 0.254, 0.222
batch losses (mrrl, rdl): 0.0106856516, 0.0001975921

Epoch over!
epoch time: 12.312

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.460 +- 0.191
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0067145289, 0.0034409829

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 280
rank avg (pred): 0.128 +- 0.097
mrr vals (pred, true): 0.261, 0.211
batch losses (mrrl, rdl): 0.0249126181, 5.18163e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 273
rank avg (pred): 0.090 +- 0.069
mrr vals (pred, true): 0.253, 0.217
batch losses (mrrl, rdl): 0.0131249512, 0.0002132967

Epoch over!
epoch time: 11.937

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 684
rank avg (pred): 0.413 +- 0.195
mrr vals (pred, true): 0.089, 0.001
batch losses (mrrl, rdl): 0.0151902828, 0.0001668282

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 255
rank avg (pred): 0.029 +- 0.022
mrr vals (pred, true): 0.308, 0.361
batch losses (mrrl, rdl): 0.0283584651, 0.000183401

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 499
rank avg (pred): 0.071 +- 0.053
mrr vals (pred, true): 0.264, 0.275
batch losses (mrrl, rdl): 0.0012176388, 0.0009428335

Epoch over!
epoch time: 12.082

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 470
rank avg (pred): 0.409 +- 0.192
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0176913813, 7.16838e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 216
rank avg (pred): 0.402 +- 0.197
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.020505745, 0.0001029526

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 342
rank avg (pred): 0.367 +- 0.208
mrr vals (pred, true): 0.114, 0.176
batch losses (mrrl, rdl): 0.0380663164, 0.0001396587

Epoch over!
epoch time: 11.989

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 443
rank avg (pred): 0.393 +- 0.197
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0274164695, 0.0001426754

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.074 +- 0.056
mrr vals (pred, true): 0.274, 0.296
batch losses (mrrl, rdl): 0.0046762181, 0.0008354761

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.400 +- 0.194
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0197906084, 0.0001516136

Epoch over!
epoch time: 11.852

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.396 +- 0.199
mrr vals (pred, true): 0.104, 0.170

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   16 	     0 	 0.05359 	 6e-0500 	 m..s
    9 	     1 	 0.04906 	 7e-0500 	 m..s
    7 	     2 	 0.04892 	 0.00016 	 m..s
   63 	     3 	 0.10551 	 0.00017 	 MISS
   59 	     4 	 0.10526 	 0.00017 	 MISS
   13 	     5 	 0.05088 	 0.00018 	 m..s
    5 	     6 	 0.04862 	 0.00019 	 m..s
   10 	     7 	 0.04909 	 0.00019 	 m..s
   71 	     8 	 0.10643 	 0.00020 	 MISS
   12 	     9 	 0.05073 	 0.00020 	 m..s
   74 	    10 	 0.10702 	 0.00020 	 MISS
    8 	    11 	 0.04906 	 0.00021 	 m..s
   48 	    12 	 0.10351 	 0.00021 	 MISS
   85 	    13 	 0.11753 	 0.00021 	 MISS
   35 	    14 	 0.09808 	 0.00022 	 m..s
   40 	    15 	 0.10136 	 0.00022 	 MISS
   84 	    16 	 0.11286 	 0.00022 	 MISS
    3 	    17 	 0.04856 	 0.00022 	 m..s
    1 	    18 	 0.04849 	 0.00024 	 m..s
   67 	    19 	 0.10625 	 0.00025 	 MISS
   39 	    20 	 0.10078 	 0.00026 	 MISS
   61 	    21 	 0.10537 	 0.00026 	 MISS
   50 	    22 	 0.10365 	 0.00026 	 MISS
   17 	    23 	 0.05505 	 0.00027 	 m..s
   60 	    24 	 0.10529 	 0.00030 	 MISS
   83 	    25 	 0.11090 	 0.00030 	 MISS
   18 	    26 	 0.06001 	 0.00031 	 m..s
   29 	    27 	 0.09005 	 0.00036 	 m..s
   52 	    28 	 0.10416 	 0.00036 	 MISS
    6 	    29 	 0.04891 	 0.00037 	 m..s
    4 	    30 	 0.04860 	 0.00037 	 m..s
   53 	    31 	 0.10420 	 0.00038 	 MISS
   77 	    32 	 0.10741 	 0.00040 	 MISS
   65 	    33 	 0.10606 	 0.00041 	 MISS
   45 	    34 	 0.10300 	 0.00042 	 MISS
   30 	    35 	 0.09574 	 0.00044 	 m..s
   43 	    36 	 0.10243 	 0.00045 	 MISS
   47 	    37 	 0.10320 	 0.00051 	 MISS
   37 	    38 	 0.09980 	 0.00053 	 m..s
   73 	    39 	 0.10691 	 0.00065 	 MISS
   56 	    40 	 0.10506 	 0.00066 	 MISS
   55 	    41 	 0.10470 	 0.00071 	 MISS
   14 	    42 	 0.05119 	 0.00076 	 m..s
    0 	    43 	 0.04842 	 0.00086 	 m..s
   21 	    44 	 0.06051 	 0.00089 	 m..s
   11 	    45 	 0.04910 	 0.00091 	 m..s
    2 	    46 	 0.04853 	 0.00118 	 m..s
   15 	    47 	 0.05120 	 0.00179 	 m..s
   75 	    48 	 0.10723 	 0.00236 	 MISS
   23 	    49 	 0.07299 	 0.01728 	 m..s
   20 	    50 	 0.06039 	 0.01742 	 m..s
   22 	    51 	 0.06134 	 0.01885 	 m..s
   19 	    52 	 0.06007 	 0.01979 	 m..s
   24 	    53 	 0.07413 	 0.02684 	 m..s
   27 	    54 	 0.08768 	 0.03198 	 m..s
   25 	    55 	 0.07416 	 0.03967 	 m..s
   31 	    56 	 0.09591 	 0.10212 	 ~...
   34 	    57 	 0.09776 	 0.10737 	 ~...
   62 	    58 	 0.10538 	 0.11849 	 ~...
   36 	    59 	 0.09966 	 0.12653 	 ~...
   28 	    60 	 0.08825 	 0.12919 	 m..s
   42 	    61 	 0.10172 	 0.12922 	 ~...
   54 	    62 	 0.10460 	 0.13144 	 ~...
   86 	    63 	 0.13191 	 0.13345 	 ~...
   38 	    64 	 0.10004 	 0.13434 	 m..s
   49 	    65 	 0.10362 	 0.13508 	 m..s
   33 	    66 	 0.09619 	 0.13558 	 m..s
   91 	    67 	 0.20295 	 0.13691 	 m..s
   66 	    68 	 0.10619 	 0.14195 	 m..s
   69 	    69 	 0.10632 	 0.14309 	 m..s
   64 	    70 	 0.10570 	 0.14423 	 m..s
   88 	    71 	 0.18036 	 0.14433 	 m..s
   76 	    72 	 0.10732 	 0.14736 	 m..s
   80 	    73 	 0.10790 	 0.14843 	 m..s
   26 	    74 	 0.08566 	 0.15196 	 m..s
   70 	    75 	 0.10638 	 0.15294 	 m..s
   68 	    76 	 0.10627 	 0.15457 	 m..s
   72 	    77 	 0.10663 	 0.15532 	 m..s
   32 	    78 	 0.09615 	 0.15541 	 m..s
   79 	    79 	 0.10770 	 0.15819 	 m..s
   57 	    80 	 0.10514 	 0.15909 	 m..s
   58 	    81 	 0.10525 	 0.16376 	 m..s
   41 	    82 	 0.10140 	 0.16504 	 m..s
   82 	    83 	 0.11012 	 0.16809 	 m..s
   51 	    84 	 0.10402 	 0.16980 	 m..s
   81 	    85 	 0.10792 	 0.17511 	 m..s
   44 	    86 	 0.10251 	 0.17663 	 m..s
   78 	    87 	 0.10749 	 0.17849 	 m..s
   46 	    88 	 0.10308 	 0.17913 	 m..s
   90 	    89 	 0.19802 	 0.18800 	 ~...
   94 	    90 	 0.20610 	 0.19309 	 ~...
   87 	    91 	 0.13234 	 0.19309 	 m..s
   93 	    92 	 0.20568 	 0.19791 	 ~...
   89 	    93 	 0.19732 	 0.19917 	 ~...
   92 	    94 	 0.20449 	 0.20990 	 ~...
   96 	    95 	 0.21195 	 0.22329 	 ~...
   99 	    96 	 0.21684 	 0.22461 	 ~...
  101 	    97 	 0.21761 	 0.22873 	 ~...
   95 	    98 	 0.20822 	 0.23829 	 m..s
  100 	    99 	 0.21718 	 0.24048 	 ~...
   97 	   100 	 0.21529 	 0.24172 	 ~...
   98 	   101 	 0.21611 	 0.25823 	 m..s
  102 	   102 	 0.21787 	 0.25969 	 m..s
  103 	   103 	 0.25539 	 0.26428 	 ~...
  105 	   104 	 0.26516 	 0.26880 	 ~...
  107 	   105 	 0.27088 	 0.27032 	 ~...
  106 	   106 	 0.26779 	 0.27258 	 ~...
  104 	   107 	 0.26351 	 0.27311 	 ~...
  114 	   108 	 0.33508 	 0.27797 	 m..s
  111 	   109 	 0.28515 	 0.28241 	 ~...
  108 	   110 	 0.27563 	 0.28674 	 ~...
  109 	   111 	 0.27816 	 0.28879 	 ~...
  113 	   112 	 0.33495 	 0.31560 	 ~...
  116 	   113 	 0.34121 	 0.32253 	 ~...
  112 	   114 	 0.32130 	 0.32533 	 ~...
  110 	   115 	 0.27986 	 0.33444 	 m..s
  115 	   116 	 0.34051 	 0.33731 	 ~...
  119 	   117 	 0.35296 	 0.36330 	 ~...
  120 	   118 	 0.35501 	 0.36606 	 ~...
  117 	   119 	 0.34948 	 0.38092 	 m..s
  118 	   120 	 0.35196 	 0.39385 	 m..s
==========================================
r_mrr = 0.8776112794876099
r2_mrr = 0.7091642618179321
spearmanr_mrr@5 = 0.956917941570282
spearmanr_mrr@10 = 0.880953311920166
spearmanr_mrr@50 = 0.9731971621513367
spearmanr_mrr@100 = 0.8860282301902771
spearmanr_mrr@All = 0.9091944694519043
==========================================
test time: 0.417
Done Testing dataset DBpedia50
total time taken: 186.3003635406494
training time taken: 181.0789041519165
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8776)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.7092)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9569)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.8810)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9732)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8860)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9092)}}, 'test_loss': {'DistMult': {'DBpedia50': 1.912030366838735}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9307693417040594
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [117, 144, 1031, 1175, 455, 620, 352, 656, 737, 1129, 621, 540, 187, 790, 207, 809, 200, 675, 583, 1141, 186, 59, 211, 764, 416, 580, 1016, 14, 309, 506, 575, 626, 348, 9, 308, 770, 692, 42, 100, 678, 1065, 58, 490, 978, 461, 861, 337, 1027, 389, 820, 182, 629, 1144, 172, 424, 746, 945, 450, 953, 1102, 409, 791, 276, 62, 771, 273, 1049, 748, 1214, 1012, 862, 230, 520, 80, 1083, 990, 741, 1195, 1110, 195, 599, 381, 1077, 690, 439, 509, 388, 43, 333, 1101, 427, 848, 468, 1106, 932, 283, 923, 1021, 518, 435, 1191, 717, 394, 776, 54, 218, 78, 357, 395, 638, 1038, 217, 361, 478, 342, 341, 899, 174, 634, 683, 879]
valid_ids (0): []
train_ids (1094): [365, 827, 536, 71, 1114, 568, 609, 646, 11, 208, 481, 527, 456, 364, 666, 48, 1075, 153, 1043, 467, 814, 966, 938, 1179, 766, 561, 893, 147, 397, 137, 851, 566, 1131, 784, 1143, 617, 476, 782, 630, 661, 242, 929, 193, 1168, 65, 702, 511, 266, 1078, 607, 167, 586, 134, 808, 574, 832, 980, 697, 53, 1189, 1163, 1176, 1004, 963, 1182, 1067, 767, 448, 559, 724, 719, 838, 1007, 331, 943, 1030, 460, 500, 95, 407, 285, 290, 1089, 613, 300, 1044, 996, 973, 432, 886, 46, 839, 898, 852, 89, 1, 37, 408, 265, 974, 659, 769, 649, 1162, 327, 0, 1178, 805, 61, 225, 299, 1093, 155, 731, 1158, 521, 119, 437, 73, 594, 40, 1164, 142, 184, 1068, 856, 160, 720, 426, 721, 870, 271, 84, 760, 420, 335, 112, 1040, 732, 1050, 105, 837, 1074, 1213, 569, 336, 474, 751, 595, 457, 83, 314, 136, 952, 543, 651, 170, 1145, 874, 362, 123, 270, 818, 698, 565, 171, 1169, 502, 191, 246, 608, 1111, 637, 747, 522, 235, 558, 591, 849, 961, 1015, 56, 356, 722, 765, 18, 533, 835, 1193, 693, 802, 1133, 260, 900, 1206, 404, 1198, 446, 431, 992, 101, 801, 664, 910, 685, 1186, 998, 34, 32, 330, 366, 1072, 981, 325, 1104, 542, 178, 1196, 91, 1156, 45, 21, 673, 503, 470, 148, 994, 600, 190, 282, 858, 882, 449, 736, 645, 383, 27, 743, 627, 359, 438, 1070, 949, 551, 884, 1091, 510, 584, 247, 635, 24, 1076, 169, 578, 530, 227, 881, 799, 77, 162, 892, 368, 680, 322, 1146, 173, 164, 585, 433, 31, 1194, 794, 12, 1167, 991, 968, 87, 201, 491, 909, 412, 1166, 103, 97, 988, 582, 124, 922, 85, 632, 234, 405, 387, 777, 1032, 81, 399, 129, 1095, 1002, 597, 26, 122, 108, 1154, 1029, 1203, 339, 369, 867, 1035, 815, 278, 1197, 550, 819, 1121, 419, 50, 755, 749, 1116, 168, 1036, 555, 291, 706, 256, 1138, 1105, 414, 705, 219, 588, 343, 1118, 871, 710, 323, 13, 400, 1066, 338, 951, 311, 773, 854, 25, 66, 1071, 1019, 654, 1052, 798, 789, 263, 250, 917, 98, 465, 347, 1041, 507, 220, 708, 524, 442, 657, 891, 829, 203, 557, 403, 462, 681, 593, 386, 640, 1153, 964, 216, 684, 236, 935, 44, 340, 847, 213, 320, 307, 623, 869, 535, 28, 484, 872, 1113, 401, 729, 942, 233, 483, 677, 545, 243, 957, 810, 920, 804, 590, 324, 619, 1053, 1039, 228, 79, 596, 775, 662, 614, 1047, 946, 289, 113, 284, 612, 1125, 486, 49, 750, 1045, 668, 865, 644, 713, 926, 1211, 262, 1207, 57, 1201, 259, 948, 378, 570, 3, 598, 1134, 372, 127, 477, 1069, 116, 552, 223, 679, 908, 1185, 180, 384, 280, 1064, 444, 1155, 778, 275, 728, 60, 1092, 374, 529, 106, 703, 237, 96, 1199, 984, 822, 498, 989, 204, 35, 496, 759, 1088, 785, 650, 255, 346, 1120, 979, 377, 700, 655, 226, 985, 264, 1202, 699, 221, 671, 183, 1003, 198, 757, 315, 94, 241, 526, 367, 986, 1024, 1180, 826, 257, 140, 1136, 466, 911, 1037, 133, 1119, 1160, 660, 146, 316, 501, 279, 1152, 398, 936, 454, 135, 1073, 74, 306, 744, 1161, 855, 143, 993, 505, 1126, 312, 209, 453, 512, 956, 252, 413, 850, 761, 1135, 418, 913, 1117, 930, 92, 762, 915, 954, 360, 33, 197, 912, 834, 69, 1082, 725, 1063, 1097, 523, 674, 326, 166, 796, 430, 1107, 179, 298, 712, 756, 1025, 240, 504, 1017, 110, 396, 232, 1151, 379, 1057, 1165, 828, 669, 642, 977, 30, 303, 473, 1132, 845, 292, 695, 788, 628, 130, 317, 286, 962, 1139, 689, 139, 495, 423, 441, 254, 534, 159, 763, 1183, 803, 625, 181, 321, 853, 382, 1046, 17, 592, 924, 452, 406, 866, 999, 1042, 329, 488, 349, 1013, 1142, 258, 554, 294, 469, 1190, 745, 417, 863, 971, 873, 742, 206, 6, 676, 145, 643, 1188, 987, 480, 843, 824, 658, 781, 792, 916, 1200, 903, 967, 562, 287, 72, 519, 905, 1140, 813, 563, 840, 716, 188, 354, 1100, 1096, 29, 281, 102, 633, 571, 603, 1115, 1184, 780, 1130, 177, 268, 616, 1205, 128, 231, 548, 222, 370, 1137, 959, 1010, 768, 261, 459, 927, 772, 376, 375, 70, 515, 5, 1212, 895, 734, 1056, 463, 23, 269, 549, 831, 1210, 422, 7, 738, 154, 860, 618, 880, 138, 560, 20, 121, 955, 194, 39, 471, 931, 199, 189, 688, 429, 573, 727, 149, 421, 447, 411, 272, 983, 475, 544, 670, 876, 889, 440, 754, 914, 902, 758, 1085, 267, 812, 686, 508, 975, 1098, 328, 525, 950, 156, 1127, 779, 36, 251, 38, 783, 1173, 157, 940, 513, 126, 109, 682, 528, 553, 494, 825, 1094, 752, 1014, 539, 1108, 941, 1005, 648, 132, 64, 878, 921, 410, 19, 577, 380, 636, 93, 10, 332, 907, 1001, 479, 868, 1081, 472, 274, 1122, 833, 897, 1026, 937, 482, 497, 52, 1062, 492, 925, 538, 487, 687, 972, 353, 841, 224, 514, 906, 1000, 464, 718, 701, 704, 589, 141, 857, 176, 894, 1061, 391, 517, 939, 402, 205, 1124, 295, 358, 1192, 1059, 786, 76, 249, 229, 68, 672, 22, 816, 615, 443, 390, 118, 733, 919, 214, 1008, 665, 175, 890, 150, 373, 795, 541, 537, 753, 711, 896, 114, 653, 392, 104, 1034, 774, 2, 960, 901, 499, 944, 238, 807, 210, 1147, 1009, 904, 709, 844, 1051, 1187, 302, 192, 730, 1159, 288, 934, 1055, 877, 875, 1208, 248, 385, 107, 1099, 99, 239, 152, 564, 1209, 605, 445, 1090, 161, 696, 1174, 86, 41, 434, 610, 304, 1033, 918, 277, 1123, 888, 115, 305, 715, 516, 1181, 67, 1028, 567, 547, 997, 726, 624, 811, 691, 556, 344, 319, 371, 579, 485, 800, 318, 714, 165, 821, 393, 120, 842, 8, 297, 546, 125, 196, 75, 928, 245, 885, 576, 723, 707, 458, 16, 735, 363, 622, 965, 531, 253, 1006, 334, 1080, 995, 958, 1149, 587, 163, 976, 51, 667, 694, 641, 1204, 887, 131, 1157, 296, 451, 350, 493, 355, 55, 293, 606, 1171, 47, 1172, 740, 947, 572, 830, 90, 969, 1112, 4, 602, 82, 601, 817, 611, 1022, 806, 647, 215, 1058, 1020, 158, 836, 864, 1170, 982, 1048, 604, 1103, 652, 1054, 1148, 1150, 883, 425, 351, 15, 489, 797, 185, 202, 1018, 111, 1084, 631, 1109, 859, 532, 787, 415, 1177, 212, 1128, 846, 1023, 244, 1011, 933, 1086, 1060, 428, 1087, 345, 63, 88, 663, 1079, 310, 581, 739, 151, 436, 970, 301, 793, 823, 313, 639]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7228702395880010
the save name prefix for this run is:  chkpt-ID_7228702395880010_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 141
rank avg (pred): 0.508 +- 0.006
mrr vals (pred, true): 0.000, 0.139
batch losses (mrrl, rdl): 0.0, 0.0007224692

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 428
rank avg (pred): 0.340 +- 0.018
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005660065

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 957
rank avg (pred): 0.460 +- 0.190
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 4.83026e-05

Epoch over!
epoch time: 12.161

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1145
rank avg (pred): 0.323 +- 0.149
mrr vals (pred, true): 0.001, 0.223
batch losses (mrrl, rdl): 0.0, 0.0001865469

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1189
rank avg (pred): 0.420 +- 0.248
mrr vals (pred, true): 0.182, 0.000
batch losses (mrrl, rdl): 0.0, 5.88718e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 943
rank avg (pred): 0.478 +- 0.270
mrr vals (pred, true): 0.195, 0.000
batch losses (mrrl, rdl): 0.0, 0.0025810471

Epoch over!
epoch time: 11.797

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 4
rank avg (pred): 0.144 +- 0.104
mrr vals (pred, true): 0.205, 0.330
batch losses (mrrl, rdl): 0.0, 5.28678e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.390 +- 0.307
mrr vals (pred, true): 0.209, 0.142
batch losses (mrrl, rdl): 0.0, 0.0002052424

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 516
rank avg (pred): 0.276 +- 0.228
mrr vals (pred, true): 0.211, 0.148
batch losses (mrrl, rdl): 0.0, 5.68204e-05

Epoch over!
epoch time: 11.824

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 913
rank avg (pred): 0.523 +- 0.295
mrr vals (pred, true): 0.204, 0.003
batch losses (mrrl, rdl): 0.0, 0.0001365629

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.366 +- 0.308
mrr vals (pred, true): 0.208, 0.008
batch losses (mrrl, rdl): 0.0, 0.0001302496

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 460
rank avg (pred): 0.344 +- 0.297
mrr vals (pred, true): 0.212, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003311656

Epoch over!
epoch time: 11.839

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 166
rank avg (pred): 0.372 +- 0.321
mrr vals (pred, true): 0.211, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001358678

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 233
rank avg (pred): 0.372 +- 0.317
mrr vals (pred, true): 0.207, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001216392

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.376 +- 0.324
mrr vals (pred, true): 0.205, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001746997

Epoch over!
epoch time: 11.905

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 957
rank avg (pred): 0.595 +- 0.302
mrr vals (pred, true): 0.186, 0.000
batch losses (mrrl, rdl): 0.1845459044, 0.0002444118

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.615 +- 0.249
mrr vals (pred, true): 0.030, 0.149
batch losses (mrrl, rdl): 0.1423899233, 0.0032086615

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.077 +- 0.081
mrr vals (pred, true): 0.269, 0.263
batch losses (mrrl, rdl): 0.0003362455, 0.0006686072

Epoch over!
epoch time: 12.378

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 180
rank avg (pred): 0.469 +- 0.299
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0292433035, 2.1862e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.473 +- 0.326
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0375753902, 2.74055e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 850
rank avg (pred): 0.636 +- 0.255
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002679171, 0.0009455964

Epoch over!
epoch time: 12.166

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 493
rank avg (pred): 0.059 +- 0.062
mrr vals (pred, true): 0.279, 0.262
batch losses (mrrl, rdl): 0.0028455742, 0.0008500343

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 586
rank avg (pred): 0.525 +- 0.285
mrr vals (pred, true): 0.065, 0.156
batch losses (mrrl, rdl): 0.081682235, 0.000771641

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1131
rank avg (pred): 0.449 +- 0.287
mrr vals (pred, true): 0.110, 0.001
batch losses (mrrl, rdl): 0.035819117, 8.9717e-06

Epoch over!
epoch time: 12.026

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 444
rank avg (pred): 0.422 +- 0.262
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0288196802, 0.0001162921

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 972
rank avg (pred): 0.084 +- 0.099
mrr vals (pred, true): 0.345, 0.306
batch losses (mrrl, rdl): 0.0154172443, 0.0001672179

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 956
rank avg (pred): 0.646 +- 0.228
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 3.5337e-06, 0.0007956619

Epoch over!
epoch time: 11.971

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.409 +- 0.247
mrr vals (pred, true): 0.113, 0.145
batch losses (mrrl, rdl): 0.0103676375, 0.0001260067

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 253
rank avg (pred): 0.009 +- 0.010
mrr vals (pred, true): 0.375, 0.378
batch losses (mrrl, rdl): 5.22288e-05, 0.000220135

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.267 +- 0.180
mrr vals (pred, true): 0.112, 0.149
batch losses (mrrl, rdl): 0.0142183304, 0.0001746476

Epoch over!
epoch time: 12.053

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1151
rank avg (pred): 0.542 +- 0.415
mrr vals (pred, true): 0.227, 0.229
batch losses (mrrl, rdl): 3.66611e-05, 0.001238062

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.375 +- 0.234
mrr vals (pred, true): 0.118, 0.000
batch losses (mrrl, rdl): 0.0458232723, 0.0004001665

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.662 +- 0.185
mrr vals (pred, true): 0.020, 0.001
batch losses (mrrl, rdl): 0.0092726583, 0.000910265

Epoch over!
epoch time: 11.998

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 434
rank avg (pred): 0.385 +- 0.232
mrr vals (pred, true): 0.104, 0.000
batch losses (mrrl, rdl): 0.0294737257, 0.0001928267

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 51
rank avg (pred): 0.205 +- 0.153
mrr vals (pred, true): 0.170, 0.267
batch losses (mrrl, rdl): 0.0945135579, 0.0001149041

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 826
rank avg (pred): 0.127 +- 0.095
mrr vals (pred, true): 0.181, 0.198
batch losses (mrrl, rdl): 0.0028604241, 0.0001810452

Epoch over!
epoch time: 11.866

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 318
rank avg (pred): 0.037 +- 0.033
mrr vals (pred, true): 0.270, 0.162
batch losses (mrrl, rdl): 0.117978096, 0.0004180047

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.411 +- 0.233
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0373426899, 6.27992e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 933
rank avg (pred): 0.639 +- 0.205
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0007275929, 0.0009763882

Epoch over!
epoch time: 11.893

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.410 +- 0.237
mrr vals (pred, true): 0.093, 0.000
batch losses (mrrl, rdl): 0.0189160854, 0.0001184039

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1157
rank avg (pred): 0.368 +- 0.302
mrr vals (pred, true): 0.209, 0.188
batch losses (mrrl, rdl): 0.0047818101, 9.77924e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 752
rank avg (pred): 0.312 +- 0.212
mrr vals (pred, true): 0.135, 0.128
batch losses (mrrl, rdl): 0.0004609597, 0.0002788849

Epoch over!
epoch time: 12.129

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 111
rank avg (pred): 0.363 +- 0.230
mrr vals (pred, true): 0.106, 0.125
batch losses (mrrl, rdl): 0.0037360177, 0.0001570523

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 410
rank avg (pred): 0.346 +- 0.216
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.0248610992, 0.0004271075

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 189
rank avg (pred): 0.373 +- 0.230
mrr vals (pred, true): 0.113, 0.000
batch losses (mrrl, rdl): 0.040014755, 0.0002282178

Epoch over!
epoch time: 11.951

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.406 +- 0.231
mrr vals (pred, true): 0.082, 0.174

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   15 	     0 	 0.05638 	 6e-0500 	 m..s
    6 	     1 	 0.03514 	 0.00013 	 m..s
    2 	     2 	 0.01737 	 0.00013 	 ~...
    7 	     3 	 0.05071 	 0.00015 	 m..s
   53 	     4 	 0.08267 	 0.00015 	 m..s
   82 	     5 	 0.08876 	 0.00015 	 m..s
    5 	     6 	 0.02684 	 0.00016 	 ~...
   44 	     7 	 0.08225 	 0.00017 	 m..s
   44 	     8 	 0.08225 	 0.00017 	 m..s
    3 	     9 	 0.02667 	 0.00018 	 ~...
    1 	    10 	 0.01686 	 0.00018 	 ~...
   81 	    11 	 0.08858 	 0.00018 	 m..s
    9 	    12 	 0.05176 	 0.00018 	 m..s
   59 	    13 	 0.08375 	 0.00019 	 m..s
   39 	    14 	 0.07861 	 0.00019 	 m..s
   74 	    15 	 0.08608 	 0.00020 	 m..s
   41 	    16 	 0.07969 	 0.00020 	 m..s
   33 	    17 	 0.07628 	 0.00020 	 m..s
   52 	    18 	 0.08258 	 0.00020 	 m..s
   28 	    19 	 0.07389 	 0.00020 	 m..s
   57 	    20 	 0.08311 	 0.00021 	 m..s
    0 	    21 	 0.01657 	 0.00021 	 ~...
   31 	    22 	 0.07613 	 0.00021 	 m..s
   23 	    23 	 0.07224 	 0.00021 	 m..s
   83 	    24 	 0.08902 	 0.00024 	 m..s
   75 	    25 	 0.08639 	 0.00025 	 m..s
   63 	    26 	 0.08388 	 0.00026 	 m..s
   61 	    27 	 0.08385 	 0.00027 	 m..s
   62 	    28 	 0.08388 	 0.00027 	 m..s
   42 	    29 	 0.08074 	 0.00027 	 m..s
   43 	    30 	 0.08086 	 0.00029 	 m..s
   51 	    31 	 0.08226 	 0.00030 	 m..s
   86 	    32 	 0.09059 	 0.00032 	 m..s
   71 	    33 	 0.08527 	 0.00033 	 m..s
   70 	    34 	 0.08516 	 0.00033 	 m..s
   27 	    35 	 0.07387 	 0.00034 	 m..s
   21 	    36 	 0.07208 	 0.00035 	 m..s
    4 	    37 	 0.02668 	 0.00037 	 ~...
   78 	    38 	 0.08679 	 0.00038 	 m..s
   88 	    39 	 0.09183 	 0.00043 	 m..s
   85 	    40 	 0.08980 	 0.00046 	 m..s
   16 	    41 	 0.06506 	 0.00047 	 m..s
    8 	    42 	 0.05113 	 0.00050 	 m..s
   11 	    43 	 0.05397 	 0.00051 	 m..s
   34 	    44 	 0.07651 	 0.00064 	 m..s
   44 	    45 	 0.08225 	 0.00065 	 m..s
   54 	    46 	 0.08268 	 0.00071 	 m..s
   44 	    47 	 0.08225 	 0.00093 	 m..s
   32 	    48 	 0.07619 	 0.00106 	 m..s
   36 	    49 	 0.07686 	 0.00111 	 m..s
   30 	    50 	 0.07601 	 0.00116 	 m..s
   64 	    51 	 0.08390 	 0.00117 	 m..s
   12 	    52 	 0.05440 	 0.00823 	 m..s
   13 	    53 	 0.05445 	 0.00823 	 m..s
   10 	    54 	 0.05209 	 0.01885 	 m..s
   14 	    55 	 0.05467 	 0.01885 	 m..s
   22 	    56 	 0.07213 	 0.10212 	 ~...
   90 	    57 	 0.10296 	 0.10313 	 ~...
   26 	    58 	 0.07285 	 0.10737 	 m..s
   20 	    59 	 0.07180 	 0.11677 	 m..s
   17 	    60 	 0.06538 	 0.12074 	 m..s
   19 	    61 	 0.06697 	 0.12699 	 m..s
   35 	    62 	 0.07674 	 0.12862 	 m..s
   68 	    63 	 0.08426 	 0.13287 	 m..s
   25 	    64 	 0.07268 	 0.13507 	 m..s
   79 	    65 	 0.08681 	 0.13508 	 m..s
   44 	    66 	 0.08225 	 0.13533 	 m..s
   58 	    67 	 0.08355 	 0.13656 	 m..s
   97 	    68 	 0.18295 	 0.13691 	 m..s
   60 	    69 	 0.08384 	 0.13873 	 m..s
   18 	    70 	 0.06675 	 0.13896 	 m..s
   92 	    71 	 0.11738 	 0.13911 	 ~...
   76 	    72 	 0.08662 	 0.14108 	 m..s
   24 	    73 	 0.07241 	 0.14134 	 m..s
   96 	    74 	 0.15775 	 0.14150 	 ~...
   91 	    75 	 0.10425 	 0.14294 	 m..s
   89 	    76 	 0.09267 	 0.14408 	 m..s
   44 	    77 	 0.08225 	 0.14423 	 m..s
   29 	    78 	 0.07512 	 0.14613 	 m..s
   40 	    79 	 0.07880 	 0.14736 	 m..s
   65 	    80 	 0.08409 	 0.14927 	 m..s
   56 	    81 	 0.08287 	 0.14935 	 m..s
   94 	    82 	 0.12367 	 0.15324 	 ~...
   69 	    83 	 0.08495 	 0.15518 	 m..s
   99 	    84 	 0.18792 	 0.15777 	 m..s
   37 	    85 	 0.07716 	 0.15831 	 m..s
   67 	    86 	 0.08425 	 0.16049 	 m..s
   93 	    87 	 0.12130 	 0.16385 	 m..s
  105 	    88 	 0.20014 	 0.16408 	 m..s
   80 	    89 	 0.08703 	 0.16513 	 m..s
   55 	    90 	 0.08269 	 0.16703 	 m..s
   87 	    91 	 0.09179 	 0.16809 	 m..s
   72 	    92 	 0.08538 	 0.16827 	 m..s
   44 	    93 	 0.08225 	 0.17409 	 m..s
   73 	    94 	 0.08549 	 0.17565 	 m..s
   77 	    95 	 0.08674 	 0.17675 	 m..s
   38 	    96 	 0.07825 	 0.17849 	 MISS
   95 	    97 	 0.13426 	 0.19017 	 m..s
   84 	    98 	 0.08917 	 0.19208 	 MISS
  104 	    99 	 0.20003 	 0.19212 	 ~...
  103 	   100 	 0.19693 	 0.19298 	 ~...
   66 	   101 	 0.08412 	 0.19965 	 MISS
  101 	   102 	 0.19395 	 0.20496 	 ~...
  100 	   103 	 0.19241 	 0.21094 	 ~...
  102 	   104 	 0.19479 	 0.21655 	 ~...
  106 	   105 	 0.20233 	 0.23149 	 ~...
  107 	   106 	 0.20805 	 0.23511 	 ~...
  108 	   107 	 0.22044 	 0.24048 	 ~...
  115 	   108 	 0.26746 	 0.24977 	 ~...
  111 	   109 	 0.24499 	 0.24997 	 ~...
   98 	   110 	 0.18791 	 0.25014 	 m..s
  110 	   111 	 0.23202 	 0.25479 	 ~...
  109 	   112 	 0.22611 	 0.26160 	 m..s
  114 	   113 	 0.26483 	 0.26466 	 ~...
  113 	   114 	 0.25657 	 0.27314 	 ~...
  112 	   115 	 0.25619 	 0.27797 	 ~...
  116 	   116 	 0.30173 	 0.31242 	 ~...
  119 	   117 	 0.32428 	 0.34745 	 ~...
  120 	   118 	 0.34580 	 0.34832 	 ~...
  117 	   119 	 0.31364 	 0.35217 	 m..s
  118 	   120 	 0.31924 	 0.35787 	 m..s
==========================================
r_mrr = 0.8022674322128296
r2_mrr = 0.6222876906394958
spearmanr_mrr@5 = 0.8125610947608948
spearmanr_mrr@10 = 0.9827698469161987
spearmanr_mrr@50 = 0.9726914167404175
spearmanr_mrr@100 = 0.8330875635147095
spearmanr_mrr@All = 0.8580908179283142
==========================================
test time: 0.414
Done Testing dataset DBpedia50
total time taken: 185.5823621749878
training time taken: 180.43131232261658
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8023)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6223)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8126)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9828)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9727)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8331)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8581)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.4528773793645087}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 3921256419589720
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [399, 574, 506, 1103, 115, 858, 104, 1193, 167, 1017, 1001, 829, 631, 1005, 759, 1119, 211, 65, 670, 408, 1052, 43, 668, 12, 559, 720, 933, 589, 1135, 620, 490, 347, 8, 555, 313, 921, 1025, 711, 1123, 233, 214, 661, 40, 163, 1162, 449, 1107, 249, 545, 782, 866, 511, 1164, 689, 873, 862, 430, 524, 786, 883, 173, 746, 798, 1099, 1077, 371, 121, 142, 277, 113, 429, 865, 391, 1140, 1079, 475, 1171, 788, 634, 1058, 510, 47, 64, 1045, 22, 331, 324, 1097, 685, 617, 1157, 520, 361, 898, 1134, 908, 303, 360, 536, 771, 1152, 29, 963, 1118, 55, 1030, 400, 238, 389, 1089, 825, 340, 976, 310, 783, 83, 881, 292, 1197, 37, 281]
valid_ids (0): []
train_ids (1094): [997, 961, 1083, 778, 726, 33, 1056, 548, 345, 132, 151, 1055, 965, 824, 533, 1146, 137, 845, 958, 1150, 498, 1028, 892, 308, 236, 46, 651, 74, 494, 274, 515, 492, 1125, 1029, 593, 330, 57, 1080, 801, 1061, 861, 1194, 654, 931, 509, 990, 1143, 774, 305, 658, 476, 719, 587, 627, 1120, 1211, 306, 953, 1163, 678, 923, 280, 1068, 728, 342, 750, 1202, 193, 831, 1012, 171, 546, 714, 860, 848, 842, 1138, 378, 992, 346, 54, 69, 422, 95, 1003, 124, 791, 772, 272, 1203, 797, 1004, 607, 254, 158, 610, 255, 24, 443, 608, 298, 939, 1122, 832, 265, 872, 904, 1113, 96, 472, 401, 894, 48, 962, 1169, 98, 942, 522, 1188, 101, 650, 600, 384, 715, 1145, 1149, 936, 1092, 185, 501, 710, 767, 705, 275, 1112, 833, 276, 839, 296, 199, 882, 470, 179, 950, 804, 1106, 823, 416, 840, 790, 73, 332, 740, 516, 456, 438, 508, 322, 141, 1067, 135, 512, 1046, 270, 919, 623, 643, 16, 317, 737, 530, 1110, 63, 325, 223, 71, 279, 215, 949, 1090, 849, 291, 1011, 519, 835, 590, 450, 1167, 11, 252, 150, 977, 444, 673, 915, 375, 1021, 256, 155, 1050, 1210, 15, 484, 300, 42, 392, 133, 260, 647, 625, 597, 85, 760, 358, 735, 918, 326, 1085, 421, 229, 694, 59, 602, 1127, 556, 154, 49, 834, 1027, 1104, 567, 1002, 17, 1126, 709, 803, 126, 288, 66, 646, 1074, 1013, 906, 565, 102, 686, 907, 149, 208, 967, 766, 1142, 123, 145, 1192, 1036, 459, 1174, 161, 221, 217, 1032, 235, 286, 605, 722, 930, 1209, 471, 500, 432, 1070, 856, 1181, 122, 396, 912, 811, 5, 1180, 1088, 1084, 558, 578, 415, 899, 329, 493, 1153, 1116, 920, 189, 640, 411, 419, 232, 169, 946, 390, 674, 1024, 630, 177, 87, 26, 355, 1173, 114, 77, 38, 947, 250, 316, 1151, 79, 757, 847, 564, 730, 1075, 1176, 483, 1200, 662, 261, 1183, 478, 414, 940, 815, 995, 370, 1057, 1158, 991, 407, 120, 335, 97, 1214, 127, 642, 736, 339, 367, 480, 301, 34, 386, 1124, 911, 1064, 295, 615, 1066, 850, 352, 806, 21, 159, 863, 468, 176, 944, 240, 241, 219, 165, 1148, 143, 935, 821, 945, 999, 697, 903, 328, 594, 1054, 365, 1208, 451, 307, 209, 36, 534, 902, 304, 681, 107, 495, 1000, 542, 385, 916, 60, 1044, 30, 762, 636, 70, 676, 192, 588, 659, 554, 551, 639, 251, 527, 381, 813, 707, 532, 1100, 183, 799, 552, 1206, 802, 1129, 111, 175, 537, 1059, 56, 591, 571, 688, 604, 318, 800, 1130, 78, 644, 664, 934, 569, 789, 4, 1147, 50, 178, 382, 230, 112, 729, 403, 404, 1204, 751, 1189, 693, 553, 393, 436, 1137, 764, 248, 197, 455, 1187, 723, 987, 486, 988, 110, 129, 237, 846, 1010, 649, 100, 1114, 201, 628, 338, 666, 739, 932, 362, 570, 637, 822, 614, 462, 68, 733, 893, 271, 84, 1199, 805, 473, 1022, 748, 663, 586, 2, 613, 168, 187, 960, 1033, 246, 538, 1091, 576, 388, 529, 853, 889, 680, 423, 216, 891, 464, 162, 273, 1096, 181, 491, 1060, 830, 1105, 447, 203, 952, 982, 624, 417, 146, 80, 1047, 989, 787, 349, 285, 25, 878, 160, 1136, 45, 1082, 373, 540, 58, 466, 1019, 289, 776, 1161, 854, 195, 784, 732, 702, 562, 577, 336, 67, 202, 973, 844, 377, 460, 927, 671, 1049, 1179, 134, 1071, 76, 734, 290, 268, 53, 521, 809, 633, 453, 1048, 166, 721, 138, 877, 875, 619, 870, 156, 603, 10, 794, 925, 994, 264, 814, 836, 1207, 816, 327, 528, 425, 1023, 888, 356, 13, 851, 993, 855, 14, 539, 1212, 1111, 585, 247, 81, 712, 874, 632, 452, 704, 638, 957, 409, 828, 518, 348, 363, 259, 616, 596, 312, 1168, 364, 980, 1159, 153, 1039, 859, 765, 1098, 61, 966, 242, 1132, 320, 196, 488, 157, 1191, 575, 1, 1053, 890, 535, 550, 1062, 323, 601, 344, 941, 1177, 517, 213, 905, 924, 544, 692, 580, 956, 489, 1043, 951, 928, 612, 656, 948, 970, 598, 1195, 428, 1093, 418, 368, 1108, 549, 561, 467, 257, 376, 756, 448, 1063, 106, 763, 94, 481, 374, 1175, 968, 792, 985, 445, 118, 981, 996, 3, 62, 573, 435, 350, 366, 18, 1156, 1128, 913, 869, 568, 92, 23, 188, 282, 1131, 41, 769, 103, 758, 88, 818, 745, 566, 867, 139, 210, 682, 584, 563, 1065, 198, 387, 648, 926, 871, 477, 857, 212, 547, 1155, 82, 879, 205, 117, 9, 1031, 724, 180, 224, 1086, 463, 793, 172, 838, 278, 302, 1172, 984, 174, 144, 269, 140, 505, 796, 700, 1042, 1009, 807, 1201, 752, 379, 457, 207, 194, 541, 1109, 234, 959, 657, 1115, 44, 690, 204, 513, 897, 514, 299, 669, 1073, 442, 51, 1040, 1081, 572, 1186, 969, 218, 499, 503, 753, 731, 972, 439, 297, 1117, 446, 222, 487, 0, 937, 293, 655, 557, 372, 1095, 1185, 465, 283, 755, 699, 410, 454, 785, 19, 420, 75, 130, 186, 284, 964, 90, 1178, 200, 1160, 938, 253, 667, 32, 665, 35, 433, 321, 717, 979, 837, 405, 206, 653, 485, 812, 31, 152, 427, 727, 660, 618, 641, 698, 909, 334, 191, 245, 725, 136, 914, 1144, 742, 482, 1026, 971, 599, 900, 190, 105, 747, 72, 1008, 424, 1007, 1037, 496, 955, 779, 652, 1102, 887, 182, 267, 243, 703, 777, 1205, 86, 412, 978, 744, 1133, 239, 819, 319, 27, 1139, 402, 1087, 413, 91, 896, 109, 266, 497, 582, 220, 1213, 592, 6, 351, 380, 226, 507, 917, 258, 1038, 901, 458, 773, 125, 895, 164, 394, 749, 672, 474, 1041, 52, 1018, 695, 314, 579, 469, 1072, 706, 876, 1015, 687, 116, 606, 147, 99, 262, 311, 780, 383, 583, 713, 1076, 683, 431, 170, 611, 1190, 369, 1165, 479, 93, 609, 718, 635, 395, 621, 1014, 244, 1094, 679, 131, 543, 696, 353, 1166, 852, 406, 531, 1069, 398, 28, 1034, 359, 1035, 119, 7, 626, 677, 357, 743, 983, 986, 1006, 817, 341, 761, 1182, 354, 309, 622, 184, 701, 716, 263, 910, 39, 675, 1020, 868, 708, 287, 523, 434, 343, 645, 337, 1121, 89, 595, 108, 294, 826, 810, 225, 525, 1170, 333, 886, 775, 20, 461, 954, 922, 885, 1196, 943, 741, 526, 770, 998, 1101, 768, 1141, 884, 864, 1016, 975, 560, 148, 1198, 504, 820, 1051, 502, 440, 437, 827, 227, 397, 441, 754, 880, 1078, 808, 691, 974, 841, 843, 128, 629, 1154, 228, 684, 426, 315, 738, 581, 795, 929, 781, 1184, 231]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9629633605974734
the save name prefix for this run is:  chkpt-ID_9629633605974734_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 551
rank avg (pred): 0.545 +- 0.004
mrr vals (pred, true): 0.000, 0.189
batch losses (mrrl, rdl): 0.0, 0.0014527264

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 406
rank avg (pred): 0.375 +- 0.222
mrr vals (pred, true): 0.081, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003504992

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 997
rank avg (pred): 0.158 +- 0.127
mrr vals (pred, true): 0.138, 0.275
batch losses (mrrl, rdl): 0.0, 5.13042e-05

Epoch over!
epoch time: 11.833

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 367
rank avg (pred): 0.361 +- 0.281
mrr vals (pred, true): 0.115, 0.158
batch losses (mrrl, rdl): 0.0, 0.0003475105

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 489
rank avg (pred): 0.270 +- 0.215
mrr vals (pred, true): 0.121, 0.263
batch losses (mrrl, rdl): 0.0, 8.42402e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 508
rank avg (pred): 0.256 +- 0.202
mrr vals (pred, true): 0.129, 0.284
batch losses (mrrl, rdl): 0.0, 9.58535e-05

Epoch over!
epoch time: 11.737

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1181
rank avg (pred): 0.390 +- 0.305
mrr vals (pred, true): 0.119, 0.134
batch losses (mrrl, rdl): 0.0, 0.0001071595

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 448
rank avg (pred): 0.385 +- 0.281
mrr vals (pred, true): 0.075, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001303134

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 128
rank avg (pred): 0.361 +- 0.287
mrr vals (pred, true): 0.122, 0.169
batch losses (mrrl, rdl): 0.0, 0.0001684509

Epoch over!
epoch time: 11.711

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 315
rank avg (pred): 0.135 +- 0.108
mrr vals (pred, true): 0.128, 0.230
batch losses (mrrl, rdl): 0.0, 6.93157e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1031
rank avg (pred): 0.370 +- 0.295
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0, 7.36219e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 833
rank avg (pred): 0.197 +- 0.155
mrr vals (pred, true): 0.123, 0.194
batch losses (mrrl, rdl): 0.0, 0.0001052

Epoch over!
epoch time: 11.65

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1186
rank avg (pred): 0.401 +- 0.294
mrr vals (pred, true): 0.090, 0.143
batch losses (mrrl, rdl): 0.0, 0.0002357918

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.449 +- 0.348
mrr vals (pred, true): 0.128, 0.001
batch losses (mrrl, rdl): 0.0, 3.7772e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1175
rank avg (pred): 0.401 +- 0.316
mrr vals (pred, true): 0.127, 0.139
batch losses (mrrl, rdl): 0.0, 0.0002160919

Epoch over!
epoch time: 11.758

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.380 +- 0.298
mrr vals (pred, true): 0.117, 0.149
batch losses (mrrl, rdl): 0.0105933119, 0.0001774582

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.487 +- 0.300
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0209048539, 5.34611e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 302
rank avg (pred): 0.014 +- 0.012
mrr vals (pred, true): 0.241, 0.227
batch losses (mrrl, rdl): 0.0017625603, 0.0004558057

Epoch over!
epoch time: 11.902

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 314
rank avg (pred): 0.020 +- 0.016
mrr vals (pred, true): 0.213, 0.273
batch losses (mrrl, rdl): 0.0359922983, 0.0004669865

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 636
rank avg (pred): 0.375 +- 0.238
mrr vals (pred, true): 0.097, 0.135
batch losses (mrrl, rdl): 0.0141519774, 7.58956e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.433 +- 0.240
mrr vals (pred, true): 0.085, 0.003
batch losses (mrrl, rdl): 0.0125273429, 1.73224e-05

Epoch over!
epoch time: 12.059

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 24
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.303, 0.329
batch losses (mrrl, rdl): 0.0071864454, 0.0004928686

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 219
rank avg (pred): 0.397 +- 0.252
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.02666598, 2.36499e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 806
rank avg (pred): 0.628 +- 0.260
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 4.50439e-05, 0.0004521529

Epoch over!
epoch time: 12.069

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1206
rank avg (pred): 0.413 +- 0.239
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0211532824, 0.0001891822

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 396
rank avg (pred): 0.415 +- 0.238
mrr vals (pred, true): 0.105, 0.157
batch losses (mrrl, rdl): 0.0272610858, 0.0002180976

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 275
rank avg (pred): 0.084 +- 0.081
mrr vals (pred, true): 0.267, 0.214
batch losses (mrrl, rdl): 0.0278174412, 0.0003000327

Epoch over!
epoch time: 12.17

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 486
rank avg (pred): 0.099 +- 0.092
mrr vals (pred, true): 0.241, 0.265
batch losses (mrrl, rdl): 0.0058518108, 0.0004463017

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 379
rank avg (pred): 0.419 +- 0.228
mrr vals (pred, true): 0.098, 0.144
batch losses (mrrl, rdl): 0.0215087552, 0.0004003637

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 452
rank avg (pred): 0.410 +- 0.224
mrr vals (pred, true): 0.098, 0.000
batch losses (mrrl, rdl): 0.0231943689, 0.0001410487

Epoch over!
epoch time: 12.215

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1039
rank avg (pred): 0.428 +- 0.223
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0193186328, 0.0001865832

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.398 +- 0.221
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0267412961, 0.0001291938

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 192
rank avg (pred): 0.391 +- 0.220
mrr vals (pred, true): 0.103, 0.001
batch losses (mrrl, rdl): 0.0279938579, 0.0001911119

Epoch over!
epoch time: 12.102

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.403 +- 0.218
mrr vals (pred, true): 0.099, 0.179
batch losses (mrrl, rdl): 0.063935414, 0.0002479027

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 498
rank avg (pred): 0.191 +- 0.208
mrr vals (pred, true): 0.246, 0.277
batch losses (mrrl, rdl): 0.0097329067, 0.0001540147

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 330
rank avg (pred): 0.428 +- 0.221
mrr vals (pred, true): 0.098, 0.170
batch losses (mrrl, rdl): 0.0524857044, 0.0003078334

Epoch over!
epoch time: 12.046

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1026
rank avg (pred): 0.432 +- 0.221
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.0214262046, 0.0002302023

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 297
rank avg (pred): 0.291 +- 0.222
mrr vals (pred, true): 0.210, 0.176
batch losses (mrrl, rdl): 0.0112843961, 0.0003663406

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1006
rank avg (pred): 0.398 +- 0.212
mrr vals (pred, true): 0.099, 0.168
batch losses (mrrl, rdl): 0.0472990498, 0.0003624879

Epoch over!
epoch time: 12.092

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.261 +- 0.218
mrr vals (pred, true): 0.231, 0.263
batch losses (mrrl, rdl): 0.0096616885, 0.0002415247

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 951
rank avg (pred): 0.615 +- 0.272
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.43932e-05, 7.48602e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1196
rank avg (pred): 0.400 +- 0.205
mrr vals (pred, true): 0.091, 0.003
batch losses (mrrl, rdl): 0.0164876468, 8.83545e-05

Epoch over!
epoch time: 12.054

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 478
rank avg (pred): 0.434 +- 0.233
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0177636575, 7.06993e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.263 +- 0.211
mrr vals (pred, true): 0.253, 0.278
batch losses (mrrl, rdl): 0.0061025862, 0.0002684052

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 91
rank avg (pred): 0.392 +- 0.207
mrr vals (pred, true): 0.099, 0.155
batch losses (mrrl, rdl): 0.0315772295, 0.0002949874

Epoch over!
epoch time: 12.023

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.425 +- 0.233
mrr vals (pred, true): 0.096, 0.142

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04886 	 6e-0500 	 m..s
   11 	     1 	 0.04895 	 0.00011 	 m..s
    0 	     2 	 0.04884 	 0.00011 	 m..s
   20 	     3 	 0.09375 	 0.00013 	 m..s
   17 	     4 	 0.05156 	 0.00014 	 m..s
   10 	     5 	 0.04891 	 0.00015 	 m..s
    5 	     6 	 0.04886 	 0.00016 	 m..s
   68 	     7 	 0.09926 	 0.00017 	 m..s
   35 	     8 	 0.09609 	 0.00018 	 m..s
   30 	     9 	 0.09541 	 0.00019 	 m..s
    8 	    10 	 0.04886 	 0.00019 	 m..s
   73 	    11 	 0.10064 	 0.00020 	 MISS
   43 	    12 	 0.09703 	 0.00021 	 m..s
   62 	    13 	 0.09845 	 0.00021 	 m..s
   75 	    14 	 0.10076 	 0.00021 	 MISS
   32 	    15 	 0.09551 	 0.00021 	 m..s
   76 	    16 	 0.10088 	 0.00022 	 MISS
   72 	    17 	 0.10013 	 0.00023 	 m..s
   71 	    18 	 0.09974 	 0.00024 	 m..s
    7 	    19 	 0.04886 	 0.00025 	 m..s
   51 	    20 	 0.09743 	 0.00025 	 m..s
   13 	    21 	 0.04950 	 0.00025 	 m..s
    1 	    22 	 0.04885 	 0.00026 	 m..s
   48 	    23 	 0.09734 	 0.00026 	 m..s
   70 	    24 	 0.09934 	 0.00027 	 m..s
   58 	    25 	 0.09830 	 0.00028 	 m..s
   40 	    26 	 0.09655 	 0.00030 	 m..s
   50 	    27 	 0.09740 	 0.00033 	 m..s
   16 	    28 	 0.05002 	 0.00034 	 m..s
    3 	    29 	 0.04885 	 0.00034 	 m..s
   59 	    30 	 0.09831 	 0.00036 	 m..s
   53 	    31 	 0.09756 	 0.00036 	 m..s
   49 	    32 	 0.09737 	 0.00038 	 m..s
   74 	    33 	 0.10072 	 0.00040 	 MISS
   55 	    34 	 0.09804 	 0.00042 	 m..s
   19 	    35 	 0.09297 	 0.00044 	 m..s
   27 	    36 	 0.09481 	 0.00044 	 m..s
   21 	    37 	 0.09411 	 0.00045 	 m..s
   14 	    38 	 0.04986 	 0.00046 	 m..s
    2 	    39 	 0.04885 	 0.00063 	 m..s
   57 	    40 	 0.09820 	 0.00069 	 m..s
    9 	    41 	 0.04891 	 0.00118 	 m..s
   56 	    42 	 0.09807 	 0.00360 	 m..s
    4 	    43 	 0.04885 	 0.00457 	 m..s
   15 	    44 	 0.04986 	 0.00476 	 m..s
   12 	    45 	 0.04941 	 0.00823 	 m..s
   18 	    46 	 0.07809 	 0.00861 	 m..s
   22 	    47 	 0.09427 	 0.11677 	 ~...
   65 	    48 	 0.09857 	 0.11810 	 ~...
   25 	    49 	 0.09468 	 0.12135 	 ~...
   45 	    50 	 0.09706 	 0.13053 	 m..s
   60 	    51 	 0.09840 	 0.13086 	 m..s
   29 	    52 	 0.09528 	 0.13254 	 m..s
   26 	    53 	 0.09481 	 0.13286 	 m..s
   33 	    54 	 0.09577 	 0.13287 	 m..s
   83 	    55 	 0.17111 	 0.13345 	 m..s
   61 	    56 	 0.09841 	 0.13654 	 m..s
   23 	    57 	 0.09450 	 0.13748 	 m..s
   47 	    58 	 0.09733 	 0.13874 	 m..s
   81 	    59 	 0.12272 	 0.13911 	 ~...
   52 	    60 	 0.09754 	 0.14031 	 m..s
   42 	    61 	 0.09693 	 0.14093 	 m..s
   38 	    62 	 0.09640 	 0.14195 	 m..s
   64 	    63 	 0.09852 	 0.14208 	 m..s
   28 	    64 	 0.09483 	 0.14613 	 m..s
   37 	    65 	 0.09638 	 0.15180 	 m..s
   79 	    66 	 0.10372 	 0.15311 	 m..s
   82 	    67 	 0.16965 	 0.15324 	 ~...
   31 	    68 	 0.09547 	 0.15343 	 m..s
   63 	    69 	 0.09849 	 0.15457 	 m..s
   24 	    70 	 0.09463 	 0.15518 	 m..s
   34 	    71 	 0.09606 	 0.15565 	 m..s
   66 	    72 	 0.09876 	 0.15861 	 m..s
   44 	    73 	 0.09705 	 0.15865 	 m..s
   54 	    74 	 0.09776 	 0.15909 	 m..s
   36 	    75 	 0.09627 	 0.15939 	 m..s
   77 	    76 	 0.10121 	 0.15983 	 m..s
   85 	    77 	 0.18062 	 0.16472 	 ~...
   88 	    78 	 0.19695 	 0.16644 	 m..s
   93 	    79 	 0.20296 	 0.16947 	 m..s
   67 	    80 	 0.09895 	 0.16980 	 m..s
   78 	    81 	 0.10366 	 0.17100 	 m..s
   80 	    82 	 0.10699 	 0.17212 	 m..s
   69 	    83 	 0.09930 	 0.17246 	 m..s
   41 	    84 	 0.09677 	 0.17511 	 m..s
   46 	    85 	 0.09726 	 0.17552 	 m..s
   84 	    86 	 0.17125 	 0.17576 	 ~...
   95 	    87 	 0.20377 	 0.17630 	 ~...
   39 	    88 	 0.09641 	 0.17807 	 m..s
   91 	    89 	 0.19919 	 0.18756 	 ~...
   89 	    90 	 0.19832 	 0.19494 	 ~...
   94 	    91 	 0.20357 	 0.20103 	 ~...
   87 	    92 	 0.18720 	 0.20208 	 ~...
   99 	    93 	 0.21261 	 0.20354 	 ~...
   90 	    94 	 0.19879 	 0.20496 	 ~...
   86 	    95 	 0.18356 	 0.20620 	 ~...
   97 	    96 	 0.20884 	 0.20837 	 ~...
   98 	    97 	 0.20924 	 0.21037 	 ~...
   92 	    98 	 0.20294 	 0.22818 	 ~...
   96 	    99 	 0.20444 	 0.23829 	 m..s
  100 	   100 	 0.21451 	 0.24172 	 ~...
  101 	   101 	 0.21616 	 0.24231 	 ~...
  105 	   102 	 0.22908 	 0.25351 	 ~...
  106 	   103 	 0.24731 	 0.25479 	 ~...
  102 	   104 	 0.21637 	 0.25673 	 m..s
  111 	   105 	 0.29516 	 0.25706 	 m..s
  108 	   106 	 0.26949 	 0.26345 	 ~...
  107 	   107 	 0.26926 	 0.26482 	 ~...
  110 	   108 	 0.27795 	 0.26495 	 ~...
  114 	   109 	 0.32719 	 0.27314 	 m..s
  109 	   110 	 0.27704 	 0.27863 	 ~...
  103 	   111 	 0.22398 	 0.27975 	 m..s
  104 	   112 	 0.22585 	 0.28166 	 m..s
  112 	   113 	 0.32015 	 0.31608 	 ~...
  113 	   114 	 0.32138 	 0.33164 	 ~...
  120 	   115 	 0.36478 	 0.33536 	 ~...
  118 	   116 	 0.34341 	 0.34211 	 ~...
  117 	   117 	 0.33477 	 0.35404 	 ~...
  115 	   118 	 0.33045 	 0.35787 	 ~...
  116 	   119 	 0.33397 	 0.36809 	 m..s
  119 	   120 	 0.35658 	 0.37250 	 ~...
==========================================
r_mrr = 0.8589635491371155
r2_mrr = 0.7066256999969482
spearmanr_mrr@5 = 0.9373036623001099
spearmanr_mrr@10 = 0.910102128982544
spearmanr_mrr@50 = 0.9500938057899475
spearmanr_mrr@100 = 0.8479738235473633
spearmanr_mrr@All = 0.880271315574646
==========================================
test time: 0.495
Done Testing dataset DBpedia50
total time taken: 184.69311809539795
training time taken: 179.97278952598572
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8590)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.7066)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9373)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9101)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9501)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8480)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8803)}}, 'test_loss': {'DistMult': {'DBpedia50': 1.9222775135549455}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 5053379673425983
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [800, 1052, 1043, 1054, 768, 651, 691, 1084, 683, 34, 620, 211, 492, 859, 1196, 1126, 630, 924, 21, 675, 1212, 1011, 597, 1195, 355, 32, 122, 280, 743, 215, 646, 1047, 290, 810, 640, 38, 1072, 848, 1159, 448, 971, 317, 1015, 842, 169, 1139, 637, 610, 218, 1009, 1211, 607, 545, 68, 142, 312, 285, 81, 793, 165, 92, 780, 99, 658, 553, 1204, 739, 727, 742, 30, 402, 192, 35, 459, 710, 901, 827, 1180, 1022, 925, 349, 1010, 990, 988, 778, 220, 219, 426, 457, 645, 240, 1182, 514, 340, 689, 540, 585, 1137, 1121, 580, 533, 467, 1016, 360, 469, 1166, 37, 1001, 807, 509, 1118, 511, 520, 363, 346, 470, 1109, 90, 906, 109, 797]
valid_ids (0): []
train_ids (1094): [1036, 89, 128, 113, 826, 102, 1002, 724, 328, 870, 16, 348, 1103, 832, 453, 939, 143, 368, 239, 868, 140, 843, 958, 1082, 301, 394, 1039, 475, 764, 1071, 833, 518, 766, 233, 666, 1153, 622, 435, 237, 389, 1151, 1097, 548, 762, 26, 369, 927, 119, 1148, 655, 98, 1164, 82, 305, 295, 555, 977, 272, 1006, 217, 704, 1154, 178, 54, 111, 126, 412, 461, 849, 480, 874, 77, 430, 907, 1147, 300, 468, 1163, 1141, 790, 275, 883, 471, 1037, 1077, 678, 390, 858, 523, 718, 1061, 1021, 973, 584, 604, 836, 158, 314, 1089, 188, 950, 999, 451, 947, 256, 1062, 884, 129, 558, 546, 401, 1023, 760, 519, 547, 798, 15, 1091, 767, 145, 302, 352, 96, 1060, 334, 1070, 106, 638, 43, 692, 56, 878, 134, 152, 0, 266, 1132, 788, 980, 1114, 605, 544, 498, 789, 375, 654, 717, 1183, 602, 433, 136, 484, 1158, 29, 700, 866, 254, 279, 516, 1134, 831, 792, 478, 315, 60, 248, 452, 773, 753, 146, 161, 1055, 327, 955, 802, 617, 304, 75, 157, 209, 1105, 853, 247, 329, 1129, 1020, 761, 672, 822, 486, 1069, 685, 567, 1075, 1116, 964, 936, 1179, 623, 52, 750, 706, 932, 417, 441, 1003, 482, 18, 811, 578, 715, 425, 1133, 214, 679, 744, 125, 823, 730, 1181, 131, 909, 855, 13, 1143, 3, 663, 1173, 1024, 912, 444, 1034, 660, 483, 938, 515, 163, 265, 156, 711, 370, 364, 1198, 371, 564, 28, 991, 825, 1115, 9, 404, 695, 752, 454, 432, 207, 48, 919, 147, 582, 552, 942, 103, 828, 418, 398, 108, 1112, 243, 899, 261, 2, 69, 589, 307, 725, 91, 565, 1149, 694, 488, 926, 719, 194, 373, 777, 893, 698, 1078, 1144, 759, 1057, 155, 366, 481, 861, 1168, 148, 687, 84, 269, 671, 335, 282, 223, 677, 245, 466, 1028, 525, 442, 324, 1202, 337, 189, 794, 130, 643, 1136, 179, 437, 244, 784, 495, 941, 359, 587, 407, 202, 900, 493, 70, 795, 913, 350, 374, 309, 477, 673, 510, 1162, 856, 965, 403, 676, 1111, 959, 204, 963, 127, 501, 592, 499, 343, 286, 436, 319, 318, 1110, 639, 1152, 1178, 979, 267, 763, 388, 379, 1209, 74, 1012, 517, 455, 386, 414, 1160, 581, 857, 494, 353, 174, 252, 550, 543, 253, 813, 8, 65, 839, 170, 97, 19, 1029, 1100, 181, 1117, 182, 88, 1190, 903, 1170, 358, 987, 1200, 365, 886, 313, 657, 124, 1176, 1146, 805, 908, 1203, 46, 838, 62, 532, 779, 186, 1085, 61, 708, 228, 729, 809, 341, 456, 342, 1130, 326, 434, 153, 86, 869, 1169, 931, 1045, 212, 902, 575, 166, 621, 748, 918, 1184, 820, 320, 123, 120, 115, 536, 577, 594, 624, 667, 198, 627, 167, 213, 180, 193, 234, 524, 139, 847, 920, 1096, 226, 885, 559, 246, 1007, 387, 816, 176, 141, 892, 659, 682, 749, 787, 238, 303, 774, 421, 11, 865, 940, 1076, 281, 1102, 1210, 45, 331, 662, 332, 489, 852, 14, 680, 615, 786, 626, 781, 996, 76, 230, 242, 1156, 427, 380, 367, 201, 573, 873, 1066, 1208, 824, 570, 521, 867, 150, 420, 845, 1026, 216, 197, 636, 1175, 556, 527, 507, 1005, 12, 1128, 905, 232, 411, 93, 205, 1088, 1090, 603, 1155, 250, 354, 296, 87, 737, 221, 249, 653, 720, 634, 311, 1092, 1142, 1068, 362, 406, 41, 967, 1051, 968, 723, 1185, 894, 735, 688, 731, 998, 535, 864, 702, 55, 396, 782, 686, 57, 339, 24, 51, 357, 628, 100, 1177, 1150, 67, 382, 665, 500, 395, 713, 400, 283, 44, 1013, 994, 879, 1098, 571, 601, 681, 1056, 850, 443, 460, 851, 512, 1119, 502, 1035, 871, 78, 701, 834, 22, 568, 803, 946, 1046, 172, 151, 289, 72, 133, 1213, 707, 1030, 1165, 928, 785, 566, 291, 721, 473, 330, 440, 64, 933, 953, 596, 422, 149, 1058, 837, 419, 47, 6, 1048, 895, 880, 1187, 554, 208, 229, 1083, 569, 185, 1042, 1101, 929, 1140, 1108, 496, 258, 95, 195, 712, 770, 40, 751, 661, 508, 983, 551, 522, 288, 345, 531, 668, 1113, 614, 1073, 992, 5, 664, 588, 670, 560, 796, 732, 1161, 1086, 1138, 644, 1120, 916, 196, 476, 593, 117, 1205, 974, 1079, 611, 981, 1080, 583, 392, 316, 598, 1008, 812, 53, 94, 1041, 458, 464, 714, 618, 1123, 922, 1053, 557, 572, 804, 1135, 1197, 372, 83, 815, 930, 251, 917, 306, 344, 632, 1017, 429, 791, 854, 4, 995, 829, 271, 1188, 758, 308, 222, 1131, 80, 1106, 1004, 79, 923, 321, 173, 224, 472, 25, 1050, 819, 817, 875, 107, 293, 408, 538, 112, 257, 969, 338, 287, 397, 42, 876, 203, 504, 674, 292, 503, 888, 1087, 746, 877, 262, 934, 647, 462, 1207, 137, 333, 911, 1040, 487, 641, 1192, 776, 277, 599, 600, 1081, 1201, 449, 951, 684, 491, 612, 187, 887, 890, 101, 956, 446, 294, 1019, 241, 73, 574, 297, 970, 726, 184, 264, 989, 649, 1027, 323, 479, 775, 1145, 898, 881, 897, 1099, 356, 738, 1063, 497, 1025, 561, 1033, 255, 818, 997, 168, 616, 528, 361, 1214, 872, 669, 159, 138, 910, 656, 549, 1104, 225, 736, 650, 1067, 579, 431, 808, 733, 745, 409, 1171, 539, 274, 754, 860, 438, 863, 413, 972, 190, 23, 526, 191, 696, 385, 896, 474, 423, 889, 276, 709, 1065, 914, 465, 943, 841, 105, 110, 1000, 948, 741, 591, 606, 259, 1122, 757, 952, 424, 27, 284, 39, 351, 1093, 962, 298, 821, 445, 299, 450, 199, 882, 613, 116, 1059, 1074, 576, 537, 586, 957, 1194, 530, 633, 63, 1193, 747, 393, 263, 391, 984, 415, 132, 236, 1049, 625, 541, 268, 162, 830, 278, 862, 921, 177, 447, 160, 1124, 1127, 1031, 1157, 635, 505, 1189, 801, 1, 485, 336, 978, 310, 33, 183, 175, 114, 949, 1172, 405, 383, 619, 976, 975, 58, 384, 399, 506, 135, 118, 20, 210, 835, 844, 121, 347, 322, 1174, 954, 891, 609, 652, 1018, 381, 697, 716, 71, 154, 325, 1014, 705, 960, 227, 631, 915, 171, 765, 1199, 642, 513, 36, 806, 799, 814, 542, 231, 982, 985, 376, 772, 728, 756, 563, 966, 273, 10, 840, 270, 1186, 416, 200, 1044, 769, 378, 629, 937, 608, 693, 699, 206, 7, 144, 846, 377, 703, 771, 463, 590, 783, 17, 104, 986, 1032, 722, 595, 534, 59, 755, 1167, 85, 66, 428, 1107, 529, 945, 1064, 690, 648, 1191, 1094, 1206, 1125, 260, 562, 490, 235, 410, 164, 944, 740, 993, 935, 439, 1038, 734, 31, 904, 50, 1095, 49, 961]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2265162395238766
the save name prefix for this run is:  chkpt-ID_2265162395238766_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 194
rank avg (pred): 0.546 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001148507

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 54
rank avg (pred): 0.166 +- 0.118
mrr vals (pred, true): 0.152, 0.137
batch losses (mrrl, rdl): 0.0, 6.87028e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 430
rank avg (pred): 0.379 +- 0.270
mrr vals (pred, true): 0.168, 0.000
batch losses (mrrl, rdl): 0.0, 8.51544e-05

Epoch over!
epoch time: 12.078

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 308
rank avg (pred): 0.135 +- 0.096
mrr vals (pred, true): 0.178, 0.240
batch losses (mrrl, rdl): 0.0, 5.06644e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 723
rank avg (pred): 0.406 +- 0.291
mrr vals (pred, true): 0.191, 0.000
batch losses (mrrl, rdl): 0.0, 1.24293e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 170
rank avg (pred): 0.373 +- 0.272
mrr vals (pred, true): 0.226, 0.008
batch losses (mrrl, rdl): 0.0, 0.0001204069

Epoch over!
epoch time: 11.859

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 438
rank avg (pred): 0.371 +- 0.269
mrr vals (pred, true): 0.221, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001210382

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1135
rank avg (pred): 0.260 +- 0.190
mrr vals (pred, true): 0.234, 0.280
batch losses (mrrl, rdl): 0.0, 0.0001194509

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 59
rank avg (pred): 0.151 +- 0.114
mrr vals (pred, true): 0.261, 0.192
batch losses (mrrl, rdl): 0.0, 4.01862e-05

Epoch over!
epoch time: 11.927

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 279
rank avg (pred): 0.145 +- 0.109
mrr vals (pred, true): 0.245, 0.214
batch losses (mrrl, rdl): 0.0, 3.0264e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1152
rank avg (pred): 0.286 +- 0.220
mrr vals (pred, true): 0.250, 0.195
batch losses (mrrl, rdl): 0.0, 7.36823e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 175
rank avg (pred): 0.376 +- 0.294
mrr vals (pred, true): 0.251, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002173479

Epoch over!
epoch time: 11.976

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.391 +- 0.288
mrr vals (pred, true): 0.203, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001412988

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 372
rank avg (pred): 0.375 +- 0.293
mrr vals (pred, true): 0.229, 0.155
batch losses (mrrl, rdl): 0.0, 0.0001395452

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 495
rank avg (pred): 0.244 +- 0.196
mrr vals (pred, true): 0.174, 0.296
batch losses (mrrl, rdl): 0.0, 8.64886e-05

Epoch over!
epoch time: 11.765

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 174
rank avg (pred): 0.362 +- 0.296
mrr vals (pred, true): 0.211, 0.000
batch losses (mrrl, rdl): 0.2579425275, 0.0001008789

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 339
rank avg (pred): 0.452 +- 0.311
mrr vals (pred, true): 0.092, 0.175
batch losses (mrrl, rdl): 0.0688152015, 0.0003952695

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 863
rank avg (pred): 0.698 +- 0.327
mrr vals (pred, true): 0.060, 0.008
batch losses (mrrl, rdl): 0.0009907302, 3.47547e-05

Epoch over!
epoch time: 11.973

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.359 +- 0.253
mrr vals (pred, true): 0.108, 0.124
batch losses (mrrl, rdl): 0.0026101642, 5.11696e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 75
rank avg (pred): 0.010 +- 0.009
mrr vals (pred, true): 0.248, 0.231
batch losses (mrrl, rdl): 0.0027569747, 0.0005116205

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 372
rank avg (pred): 0.340 +- 0.240
mrr vals (pred, true): 0.107, 0.155
batch losses (mrrl, rdl): 0.0229347982, 6.16464e-05

Epoch over!
epoch time: 12.048

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 313
rank avg (pred): 0.019 +- 0.016
mrr vals (pred, true): 0.216, 0.242
batch losses (mrrl, rdl): 0.0067678401, 0.0003567807

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1184
rank avg (pred): 0.412 +- 0.294
mrr vals (pred, true): 0.102, 0.131
batch losses (mrrl, rdl): 0.0086622769, 5.81699e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.089 +- 0.072
mrr vals (pred, true): 0.208, 0.176
batch losses (mrrl, rdl): 0.0098917298, 0.0008755372

Epoch over!
epoch time: 12.169

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.287, 0.247
batch losses (mrrl, rdl): 0.0163225736, 0.000892896

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 262
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.379, 0.363
batch losses (mrrl, rdl): 0.0025664994, 0.0002588903

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.580 +- 0.356
mrr vals (pred, true): 0.092, 0.032
batch losses (mrrl, rdl): 0.0175933111, 0.0019526202

Epoch over!
epoch time: 12.132

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.411 +- 0.247
mrr vals (pred, true): 0.078, 0.103
batch losses (mrrl, rdl): 0.0060234908, 9.69643e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1141
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.274, 0.265
batch losses (mrrl, rdl): 0.0008965016, 0.0018923716

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.261, 0.278
batch losses (mrrl, rdl): 0.0028103718, 0.0005910298

Epoch over!
epoch time: 12.057

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1071
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.314, 0.273
batch losses (mrrl, rdl): 0.0168275815, 0.0004071819

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 65
rank avg (pred): 0.042 +- 0.033
mrr vals (pred, true): 0.227, 0.257
batch losses (mrrl, rdl): 0.0087770149, 0.0004865897

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 160
rank avg (pred): 0.329 +- 0.232
mrr vals (pred, true): 0.119, 0.163
batch losses (mrrl, rdl): 0.0199591406, 5.67948e-05

Epoch over!
epoch time: 12.379

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 414
rank avg (pred): 0.349 +- 0.233
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0330691747, 0.0002119444

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 342
rank avg (pred): 0.417 +- 0.301
mrr vals (pred, true): 0.118, 0.176
batch losses (mrrl, rdl): 0.0331274755, 0.0002815753

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 358
rank avg (pred): 0.411 +- 0.261
mrr vals (pred, true): 0.091, 0.154
batch losses (mrrl, rdl): 0.0395405479, 0.000201566

Epoch over!
epoch time: 11.959

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 720
rank avg (pred): 0.424 +- 0.251
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0161288884, 8.5324e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1093
rank avg (pred): 0.440 +- 0.282
mrr vals (pred, true): 0.081, 0.133
batch losses (mrrl, rdl): 0.0276980326, 0.0004671184

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.327, 0.371
batch losses (mrrl, rdl): 0.0196928624, 0.0003535862

Epoch over!
epoch time: 11.768

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 896
rank avg (pred): 0.693 +- 0.254
mrr vals (pred, true): 0.059, 0.000
batch losses (mrrl, rdl): 0.0007317762, 0.0004743216

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 497
rank avg (pred): 0.016 +- 0.012
mrr vals (pred, true): 0.246, 0.282
batch losses (mrrl, rdl): 0.0133266496, 0.001379104

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1192
rank avg (pred): 0.436 +- 0.252
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0147057828, 4.3383e-05

Epoch over!
epoch time: 12.055

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 817
rank avg (pred): 0.519 +- 0.240
mrr vals (pred, true): 0.061, 0.026
batch losses (mrrl, rdl): 0.001297198, 0.0004313606

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 548
rank avg (pred): 0.279 +- 0.200
mrr vals (pred, true): 0.165, 0.185
batch losses (mrrl, rdl): 0.0039443262, 6.58644e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 734
rank avg (pred): 0.291 +- 0.141
mrr vals (pred, true): 0.060, 0.010
batch losses (mrrl, rdl): 0.0010272106, 5.54574e-05

Epoch over!
epoch time: 11.838

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.660 +- 0.239
mrr vals (pred, true): 0.044, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03556 	 6e-0500 	 m..s
    1 	     1 	 0.03593 	 0.00010 	 m..s
    8 	     2 	 0.04563 	 0.00010 	 m..s
   56 	     3 	 0.09708 	 0.00013 	 m..s
    9 	     4 	 0.04673 	 0.00013 	 m..s
   56 	     5 	 0.09708 	 0.00014 	 m..s
   35 	     6 	 0.09424 	 0.00015 	 m..s
   10 	     7 	 0.04704 	 0.00015 	 m..s
   56 	     8 	 0.09708 	 0.00015 	 m..s
   38 	     9 	 0.09517 	 0.00017 	 m..s
   25 	    10 	 0.09050 	 0.00018 	 m..s
   20 	    11 	 0.08979 	 0.00018 	 m..s
   56 	    12 	 0.09708 	 0.00018 	 m..s
   23 	    13 	 0.09011 	 0.00019 	 m..s
    4 	    14 	 0.04388 	 0.00019 	 m..s
   47 	    15 	 0.09600 	 0.00019 	 m..s
   56 	    16 	 0.09708 	 0.00020 	 m..s
   56 	    17 	 0.09708 	 0.00020 	 m..s
    7 	    18 	 0.04548 	 0.00020 	 m..s
   42 	    19 	 0.09531 	 0.00021 	 m..s
   26 	    20 	 0.09060 	 0.00021 	 m..s
   80 	    21 	 0.10016 	 0.00022 	 m..s
   56 	    22 	 0.09708 	 0.00023 	 m..s
   82 	    23 	 0.10076 	 0.00023 	 MISS
   41 	    24 	 0.09530 	 0.00023 	 m..s
   71 	    25 	 0.09710 	 0.00027 	 m..s
    5 	    26 	 0.04535 	 0.00027 	 m..s
   30 	    27 	 0.09244 	 0.00028 	 m..s
   45 	    28 	 0.09563 	 0.00030 	 m..s
   86 	    29 	 0.10234 	 0.00031 	 MISS
   56 	    30 	 0.09708 	 0.00032 	 m..s
   75 	    31 	 0.09759 	 0.00033 	 m..s
   81 	    32 	 0.10024 	 0.00033 	 m..s
   72 	    33 	 0.09711 	 0.00035 	 m..s
    2 	    34 	 0.04187 	 0.00035 	 m..s
   55 	    35 	 0.09707 	 0.00042 	 m..s
   11 	    36 	 0.04713 	 0.00042 	 m..s
   27 	    37 	 0.09110 	 0.00044 	 m..s
    3 	    38 	 0.04247 	 0.00046 	 m..s
   53 	    39 	 0.09686 	 0.00048 	 m..s
   84 	    40 	 0.10085 	 0.00058 	 MISS
   40 	    41 	 0.09523 	 0.00063 	 m..s
   31 	    42 	 0.09300 	 0.00087 	 m..s
   16 	    43 	 0.08780 	 0.00116 	 m..s
   14 	    44 	 0.05843 	 0.00130 	 m..s
   49 	    45 	 0.09630 	 0.00171 	 m..s
   56 	    46 	 0.09708 	 0.00261 	 m..s
   52 	    47 	 0.09667 	 0.00284 	 m..s
   12 	    48 	 0.04814 	 0.01662 	 m..s
   13 	    49 	 0.05649 	 0.01885 	 m..s
    6 	    50 	 0.04542 	 0.03386 	 ~...
   15 	    51 	 0.08289 	 0.03790 	 m..s
   43 	    52 	 0.09545 	 0.11677 	 ~...
   90 	    53 	 0.14376 	 0.11903 	 ~...
   88 	    54 	 0.13547 	 0.12263 	 ~...
   50 	    55 	 0.09650 	 0.12402 	 ~...
   36 	    56 	 0.09493 	 0.12532 	 m..s
   21 	    57 	 0.08998 	 0.12649 	 m..s
   39 	    58 	 0.09522 	 0.12750 	 m..s
   54 	    59 	 0.09695 	 0.12839 	 m..s
   91 	    60 	 0.14544 	 0.13043 	 ~...
   51 	    61 	 0.09663 	 0.13227 	 m..s
   22 	    62 	 0.09008 	 0.13244 	 m..s
   19 	    63 	 0.08966 	 0.13256 	 m..s
   56 	    64 	 0.09708 	 0.13428 	 m..s
   33 	    65 	 0.09376 	 0.13749 	 m..s
   56 	    66 	 0.09708 	 0.13896 	 m..s
   32 	    67 	 0.09362 	 0.13909 	 m..s
   24 	    68 	 0.09030 	 0.14053 	 m..s
   37 	    69 	 0.09503 	 0.14152 	 m..s
   46 	    70 	 0.09584 	 0.14208 	 m..s
   44 	    71 	 0.09555 	 0.14665 	 m..s
   17 	    72 	 0.08793 	 0.15047 	 m..s
   56 	    73 	 0.09708 	 0.15196 	 m..s
   34 	    74 	 0.09422 	 0.15197 	 m..s
   18 	    75 	 0.08867 	 0.15226 	 m..s
   93 	    76 	 0.15249 	 0.15324 	 ~...
   29 	    77 	 0.09172 	 0.15343 	 m..s
   56 	    78 	 0.09708 	 0.15758 	 m..s
  104 	    79 	 0.22591 	 0.15812 	 m..s
   28 	    80 	 0.09139 	 0.15819 	 m..s
   76 	    81 	 0.09768 	 0.15856 	 m..s
   56 	    82 	 0.09708 	 0.15903 	 m..s
   73 	    83 	 0.09714 	 0.15939 	 m..s
   48 	    84 	 0.09616 	 0.16041 	 m..s
   89 	    85 	 0.14106 	 0.16385 	 ~...
   95 	    86 	 0.16097 	 0.16472 	 ~...
   74 	    87 	 0.09728 	 0.16927 	 m..s
   83 	    88 	 0.10076 	 0.17096 	 m..s
   92 	    89 	 0.14797 	 0.17108 	 ~...
   85 	    90 	 0.10103 	 0.17199 	 m..s
   79 	    91 	 0.09952 	 0.17552 	 m..s
   87 	    92 	 0.10263 	 0.17815 	 m..s
   99 	    93 	 0.20890 	 0.18090 	 ~...
  102 	    94 	 0.21963 	 0.18177 	 m..s
  103 	    95 	 0.22355 	 0.18205 	 m..s
   77 	    96 	 0.09772 	 0.18493 	 m..s
   56 	    97 	 0.09708 	 0.19208 	 m..s
   78 	    98 	 0.09798 	 0.19358 	 m..s
   94 	    99 	 0.15615 	 0.19825 	 m..s
   98 	   100 	 0.20009 	 0.20470 	 ~...
  101 	   101 	 0.21351 	 0.20836 	 ~...
  108 	   102 	 0.22925 	 0.21099 	 ~...
   97 	   103 	 0.19788 	 0.21745 	 ~...
  105 	   104 	 0.22708 	 0.22152 	 ~...
  117 	   105 	 0.28915 	 0.22455 	 m..s
   96 	   106 	 0.18374 	 0.23254 	 m..s
  107 	   107 	 0.22769 	 0.23281 	 ~...
  100 	   108 	 0.21283 	 0.23829 	 ~...
  106 	   109 	 0.22755 	 0.23948 	 ~...
  111 	   110 	 0.25955 	 0.24977 	 ~...
  113 	   111 	 0.26421 	 0.25706 	 ~...
  109 	   112 	 0.25283 	 0.26209 	 ~...
  110 	   113 	 0.25935 	 0.27412 	 ~...
  114 	   114 	 0.27723 	 0.27575 	 ~...
  115 	   115 	 0.27934 	 0.27797 	 ~...
  112 	   116 	 0.26051 	 0.27990 	 ~...
  116 	   117 	 0.28915 	 0.29682 	 ~...
  119 	   118 	 0.33121 	 0.32274 	 ~...
  118 	   119 	 0.29040 	 0.34285 	 m..s
  120 	   120 	 0.34120 	 0.36330 	 ~...
==========================================
r_mrr = 0.7928600907325745
r2_mrr = 0.5875605344772339
spearmanr_mrr@5 = 0.8948017358779907
spearmanr_mrr@10 = 0.9522233009338379
spearmanr_mrr@50 = 0.9577683806419373
spearmanr_mrr@100 = 0.8075411915779114
spearmanr_mrr@All = 0.8420817255973816
==========================================
test time: 0.402
Done Testing dataset DBpedia50
total time taken: 185.26564717292786
training time taken: 180.4429988861084
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.7929)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.5876)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8948)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9522)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9578)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8075)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8421)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.2787184818007518}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 8254701711281827
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [154, 386, 877, 1189, 328, 761, 515, 496, 6, 417, 1018, 12, 1033, 671, 206, 249, 471, 999, 909, 112, 89, 638, 75, 1184, 880, 996, 774, 293, 850, 627, 308, 835, 180, 968, 236, 416, 211, 305, 1135, 639, 406, 862, 474, 665, 1047, 334, 756, 53, 590, 1170, 487, 262, 901, 817, 194, 146, 400, 752, 534, 144, 392, 810, 551, 189, 9, 191, 1174, 605, 65, 963, 162, 115, 1002, 138, 546, 848, 787, 269, 816, 579, 1140, 500, 934, 364, 466, 1071, 223, 209, 464, 547, 882, 1012, 998, 991, 222, 647, 291, 340, 67, 271, 818, 495, 472, 491, 673, 642, 512, 425, 1150, 911, 462, 321, 373, 616, 1034, 1139, 212, 780, 1053, 168, 1046]
valid_ids (0): []
train_ids (1094): [860, 728, 635, 387, 931, 325, 208, 185, 1205, 718, 976, 623, 600, 139, 316, 809, 971, 375, 362, 480, 771, 614, 233, 260, 733, 948, 588, 48, 871, 697, 651, 26, 423, 114, 634, 856, 957, 202, 747, 490, 894, 859, 332, 1011, 1029, 201, 785, 435, 949, 1025, 70, 475, 874, 580, 456, 514, 219, 975, 352, 503, 586, 1156, 1107, 297, 674, 175, 896, 1049, 961, 42, 479, 812, 273, 612, 331, 36, 1090, 955, 1072, 152, 108, 529, 64, 734, 722, 388, 68, 649, 928, 399, 796, 513, 284, 13, 380, 956, 765, 815, 511, 606, 1207, 696, 505, 1130, 251, 50, 253, 57, 577, 715, 327, 173, 229, 1144, 281, 492, 499, 239, 889, 313, 21, 710, 192, 439, 432, 47, 596, 763, 736, 188, 164, 536, 390, 252, 247, 721, 73, 445, 426, 141, 157, 844, 618, 368, 581, 1105, 1057, 1054, 32, 675, 15, 187, 942, 1093, 829, 419, 764, 1097, 78, 19, 620, 881, 83, 203, 37, 1142, 637, 1213, 170, 397, 1121, 870, 125, 1206, 1137, 692, 724, 919, 1035, 494, 555, 1041, 729, 1208, 183, 585, 689, 1045, 24, 940, 921, 558, 270, 77, 415, 145, 200, 1125, 684, 825, 453, 318, 640, 259, 1059, 179, 307, 824, 954, 895, 570, 709, 857, 1020, 1064, 843, 572, 0, 779, 1088, 1165, 604, 1056, 395, 486, 469, 652, 461, 836, 630, 452, 1154, 1087, 484, 1101, 661, 1147, 986, 535, 323, 1060, 498, 287, 1003, 356, 962, 195, 897, 906, 1082, 995, 1199, 993, 599, 607, 1179, 806, 278, 1043, 97, 1081, 1080, 660, 625, 396, 544, 129, 133, 754, 225, 904, 147, 242, 346, 506, 374, 681, 1079, 594, 703, 1068, 369, 990, 1136, 217, 258, 410, 370, 1172, 1098, 52, 95, 1133, 485, 899, 23, 584, 978, 842, 947, 459, 1197, 153, 977, 959, 778, 427, 436, 454, 643, 578, 730, 16, 176, 447, 371, 213, 104, 44, 3, 277, 353, 751, 122, 1118, 181, 644, 772, 907, 507, 174, 548, 120, 613, 320, 1168, 650, 553, 172, 658, 750, 98, 137, 974, 1074, 762, 1076, 267, 516, 646, 159, 304, 298, 552, 890, 1119, 411, 550, 935, 830, 302, 910, 1173, 20, 711, 448, 1116, 389, 178, 363, 407, 39, 227, 929, 63, 46, 94, 401, 539, 230, 922, 420, 915, 465, 177, 985, 303, 72, 256, 800, 1051, 1160, 1091, 339, 917, 254, 805, 1134, 372, 984, 636, 451, 130, 814, 964, 442, 127, 744, 1006, 1023, 405, 621, 408, 1026, 430, 926, 615, 1063, 134, 833, 377, 967, 82, 394, 979, 1127, 608, 686, 88, 568, 808, 840, 483, 276, 566, 35, 424, 945, 5, 330, 493, 807, 398, 312, 107, 379, 1048, 705, 1092, 927, 382, 790, 617, 508, 348, 165, 549, 1181, 821, 943, 1001, 306, 1145, 749, 759, 668, 804, 932, 786, 1159, 966, 972, 1203, 1095, 158, 559, 79, 1086, 654, 1042, 69, 788, 582, 845, 329, 90, 822, 603, 463, 873, 1039, 324, 336, 151, 864, 1157, 933, 414, 91, 866, 672, 467, 7, 433, 849, 1161, 556, 521, 10, 851, 587, 893, 1106, 4, 1084, 732, 629, 51, 1099, 231, 1198, 1028, 1062, 583, 279, 884, 601, 1152, 266, 27, 656, 1055, 746, 55, 244, 66, 981, 204, 1185, 669, 450, 589, 142, 1149, 611, 335, 1169, 357, 776, 470, 983, 941, 119, 131, 1073, 1194, 939, 272, 865, 913, 803, 1036, 1192, 662, 169, 898, 628, 567, 1005, 1113, 811, 846, 99, 1148, 103, 592, 1164, 56, 720, 124, 525, 33, 1167, 443, 690, 719, 265, 648, 240, 641, 920, 126, 685, 792, 1096, 852, 1178, 404, 338, 317, 659, 524, 92, 367, 431, 885, 886, 299, 1187, 753, 109, 770, 1022, 243, 337, 393, 241, 791, 1070, 1021, 758, 878, 887, 381, 228, 121, 969, 708, 858, 973, 872, 632, 1143, 171, 161, 1102, 602, 14, 923, 478, 701, 737, 366, 87, 994, 936, 837, 1141, 1, 892, 755, 688, 383, 326, 542, 437, 828, 801, 86, 1201, 528, 111, 93, 538, 702, 235, 855, 742, 1104, 214, 532, 930, 264, 1155, 802, 717, 793, 706, 609, 449, 422, 838, 221, 952, 598, 1153, 143, 767, 199, 234, 322, 123, 1114, 1019, 554, 128, 832, 924, 704, 925, 84, 1017, 545, 193, 560, 1013, 434, 248, 1032, 509, 1196, 726, 8, 591, 667, 1209, 768, 1108, 655, 795, 38, 745, 526, 1214, 136, 1117, 781, 100, 365, 679, 45, 766, 1038, 1067, 440, 879, 149, 1158, 997, 347, 682, 989, 1075, 354, 1129, 215, 160, 216, 557, 309, 274, 488, 155, 224, 841, 226, 1191, 847, 861, 1162, 96, 593, 85, 1190, 473, 595, 446, 245, 1030, 429, 914, 315, 1146, 413, 1200, 197, 62, 28, 992, 799, 41, 533, 1037, 716, 735, 541, 156, 1186, 1110, 1050, 378, 76, 1016, 1044, 489, 903, 286, 275, 1124, 982, 1007, 1183, 739, 1008, 645, 773, 1188, 1163, 1027, 723, 163, 54, 531, 797, 1138, 820, 1031, 412, 1195, 876, 740, 481, 71, 694, 1089, 946, 1100, 707, 633, 314, 569, 29, 918, 311, 457, 831, 198, 343, 563, 875, 207, 333, 190, 700, 900, 561, 677, 34, 1182, 246, 1193, 571, 458, 182, 619, 502, 912, 958, 349, 441, 823, 296, 166, 687, 1066, 537, 220, 691, 61, 819, 40, 1009, 1112, 775, 1065, 409, 186, 813, 1166, 520, 1132, 1078, 798, 1014, 105, 350, 714, 1058, 916, 664, 1212, 853, 250, 282, 1085, 1004, 1069, 150, 295, 218, 826, 30, 527, 342, 951, 1177, 418, 43, 908, 562, 1111, 827, 476, 713, 421, 1210, 135, 1120, 725, 784, 1151, 310, 1123, 110, 980, 268, 210, 1010, 610, 345, 760, 1052, 288, 80, 518, 49, 839, 769, 22, 902, 624, 292, 285, 888, 670, 676, 965, 1024, 257, 712, 2, 540, 953, 631, 438, 794, 460, 782, 237, 699, 238, 937, 1175, 678, 359, 738, 184, 106, 510, 403, 1000, 748, 1211, 905, 1109, 575, 74, 17, 868, 25, 1131, 60, 1083, 101, 1103, 960, 653, 294, 148, 1015, 869, 666, 289, 523, 522, 564, 1204, 834, 477, 757, 657, 574, 950, 384, 205, 622, 698, 58, 113, 597, 361, 1094, 683, 1180, 1061, 1202, 504, 731, 117, 261, 468, 501, 283, 301, 196, 543, 263, 376, 519, 355, 428, 573, 102, 987, 988, 140, 626, 444, 1115, 31, 743, 530, 680, 344, 517, 1128, 280, 358, 59, 944, 116, 1122, 455, 777, 1077, 854, 482, 741, 319, 11, 132, 695, 385, 1171, 891, 360, 867, 118, 341, 167, 290, 863, 351, 402, 232, 497, 789, 663, 18, 1040, 693, 81, 255, 1126, 970, 883, 727, 783, 1176, 300, 938, 565, 576, 391]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4413811063692282
the save name prefix for this run is:  chkpt-ID_4413811063692282_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 202
rank avg (pred): 0.455 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001120247

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 857
rank avg (pred): 0.424 +- 0.201
mrr vals (pred, true): 0.019, 0.031
batch losses (mrrl, rdl): 0.0, 6.11154e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.370 +- 0.311
mrr vals (pred, true): 0.098, 0.124
batch losses (mrrl, rdl): 0.0, 8.90521e-05

Epoch over!
epoch time: 11.938

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 820
rank avg (pred): 0.175 +- 0.207
mrr vals (pred, true): 0.114, 0.142
batch losses (mrrl, rdl): 0.0, 1.80329e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 88
rank avg (pred): 0.370 +- 0.317
mrr vals (pred, true): 0.082, 0.142
batch losses (mrrl, rdl): 0.0, 5.03279e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.440 +- 0.310
mrr vals (pred, true): 0.067, 0.153
batch losses (mrrl, rdl): 0.0, 0.00050789

Epoch over!
epoch time: 11.727

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 884
rank avg (pred): 0.483 +- 0.322
mrr vals (pred, true): 0.072, 0.001
batch losses (mrrl, rdl): 0.0, 1.24313e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.543 +- 0.338
mrr vals (pred, true): 0.074, 0.001
batch losses (mrrl, rdl): 0.0, 9.4112e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 239
rank avg (pred): 0.419 +- 0.320
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001915683

Epoch over!
epoch time: 11.824

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 753
rank avg (pred): 0.284 +- 0.336
mrr vals (pred, true): 0.109, 0.149
batch losses (mrrl, rdl): 0.0, 5.41953e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 250
rank avg (pred): 0.139 +- 0.266
mrr vals (pred, true): 0.147, 0.361
batch losses (mrrl, rdl): 0.0, 1.31704e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.404 +- 0.311
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0, 0.000114728

Epoch over!
epoch time: 11.736

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 788
rank avg (pred): 0.507 +- 0.326
mrr vals (pred, true): 0.084, 0.005
batch losses (mrrl, rdl): 0.0, 6.03641e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 528
rank avg (pred): 0.267 +- 0.313
mrr vals (pred, true): 0.162, 0.213
batch losses (mrrl, rdl): 0.0, 3.73192e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 530
rank avg (pred): 0.305 +- 0.319
mrr vals (pred, true): 0.149, 0.203
batch losses (mrrl, rdl): 0.0, 1.73993e-05

Epoch over!
epoch time: 11.787

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 139
rank avg (pred): 0.413 +- 0.320
mrr vals (pred, true): 0.119, 0.142
batch losses (mrrl, rdl): 0.0053695566, 3.62421e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1027
rank avg (pred): 0.428 +- 0.260
mrr vals (pred, true): 0.088, 0.000
batch losses (mrrl, rdl): 0.0142532364, 5.68536e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 237
rank avg (pred): 0.447 +- 0.272
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0067236791, 6.18316e-05

Epoch over!
epoch time: 12.205

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 710
rank avg (pred): 0.447 +- 0.306
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0193056539, 0.0001366509

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 542
rank avg (pred): 0.295 +- 0.370
mrr vals (pred, true): 0.187, 0.187
batch losses (mrrl, rdl): 4.8737e-06, 2.12272e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 526
rank avg (pred): 0.287 +- 0.360
mrr vals (pred, true): 0.178, 0.195
batch losses (mrrl, rdl): 0.0028863803, 3.73319e-05

Epoch over!
epoch time: 11.896

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.435 +- 0.289
mrr vals (pred, true): 0.087, 0.149
batch losses (mrrl, rdl): 0.0384573042, 0.0002372285

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.386 +- 0.300
mrr vals (pred, true): 0.116, 0.000
batch losses (mrrl, rdl): 0.0433934405, 0.0003181833

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 767
rank avg (pred): 0.540 +- 0.317
mrr vals (pred, true): 0.043, 0.027
batch losses (mrrl, rdl): 0.0004427212, 0.0002817849

Epoch over!
epoch time: 11.861

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 743
rank avg (pred): 0.351 +- 0.314
mrr vals (pred, true): 0.114, 0.130
batch losses (mrrl, rdl): 0.002678717, 0.0002827282

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1173
rank avg (pred): 0.444 +- 0.316
mrr vals (pred, true): 0.094, 0.139
batch losses (mrrl, rdl): 0.0202070698, 7.01358e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.439 +- 0.279
mrr vals (pred, true): 0.072, 0.000
batch losses (mrrl, rdl): 0.0050523216, 5.51149e-05

Epoch over!
epoch time: 11.938

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 559
rank avg (pred): 0.278 +- 0.380
mrr vals (pred, true): 0.209, 0.176
batch losses (mrrl, rdl): 0.0106867915, 3.60958e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 188
rank avg (pred): 0.396 +- 0.307
mrr vals (pred, true): 0.107, 0.000
batch losses (mrrl, rdl): 0.0326376446, 0.0002254216

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.278 +- 0.304
mrr vals (pred, true): 0.122, 0.139
batch losses (mrrl, rdl): 0.0028731222, 4.55514e-05

Epoch over!
epoch time: 11.898

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 216
rank avg (pred): 0.415 +- 0.289
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0194704533, 8.88248e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1123
rank avg (pred): 0.391 +- 0.312
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0275678895, 0.0002352692

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 797
rank avg (pred): 0.547 +- 0.340
mrr vals (pred, true): 0.057, 0.000
batch losses (mrrl, rdl): 0.000505325, 1.82723e-05

Epoch over!
epoch time: 11.919

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 634
rank avg (pred): 0.416 +- 0.282
mrr vals (pred, true): 0.096, 0.146
batch losses (mrrl, rdl): 0.0247661211, 0.0002012089

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.412 +- 0.288
mrr vals (pred, true): 0.105, 0.000
batch losses (mrrl, rdl): 0.0304327942, 0.0001110392

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 473
rank avg (pred): 0.389 +- 0.279
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0265669413, 0.000105836

Epoch over!
epoch time: 12.06

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 509
rank avg (pred): 0.181 +- 0.369
mrr vals (pred, true): 0.311, 0.278
batch losses (mrrl, rdl): 0.0107234642, 0.0002674304

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 675
rank avg (pred): 0.445 +- 0.275
mrr vals (pred, true): 0.077, 0.001
batch losses (mrrl, rdl): 0.0074693258, 9.31849e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 676
rank avg (pred): 0.431 +- 0.269
mrr vals (pred, true): 0.087, 0.003
batch losses (mrrl, rdl): 0.0135228205, 9.74312e-05

Epoch over!
epoch time: 11.924

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1094
rank avg (pred): 0.439 +- 0.303
mrr vals (pred, true): 0.107, 0.178
batch losses (mrrl, rdl): 0.0505472124, 0.0003353829

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 870
rank avg (pred): 0.543 +- 0.368
mrr vals (pred, true): 0.042, 0.000
batch losses (mrrl, rdl): 0.00058996, 4.77684e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 531
rank avg (pred): 0.329 +- 0.333
mrr vals (pred, true): 0.206, 0.199
batch losses (mrrl, rdl): 0.0004608282, 1.5573e-05

Epoch over!
epoch time: 11.983

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 457
rank avg (pred): 0.388 +- 0.296
mrr vals (pred, true): 0.118, 0.000
batch losses (mrrl, rdl): 0.0459189489, 0.0002219829

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1116
rank avg (pred): 0.420 +- 0.321
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0266516097, 0.0001945409

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 28
rank avg (pred): 0.303 +- 0.328
mrr vals (pred, true): 0.200, 0.212
batch losses (mrrl, rdl): 0.001584486, 0.0001259925

Epoch over!
epoch time: 11.982

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.390 +- 0.299
mrr vals (pred, true): 0.118, 0.177

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04946 	 5e-0500 	 m..s
    7 	     1 	 0.06208 	 0.00010 	 m..s
   31 	     2 	 0.10770 	 0.00013 	 MISS
   74 	     3 	 0.11286 	 0.00014 	 MISS
   83 	     4 	 0.11820 	 0.00016 	 MISS
   60 	     5 	 0.11133 	 0.00019 	 MISS
    5 	     6 	 0.05928 	 0.00019 	 m..s
   51 	     7 	 0.11034 	 0.00020 	 MISS
   57 	     8 	 0.11108 	 0.00021 	 MISS
   52 	     9 	 0.11046 	 0.00021 	 MISS
   10 	    10 	 0.06488 	 0.00021 	 m..s
   77 	    11 	 0.11557 	 0.00021 	 MISS
   26 	    12 	 0.10665 	 0.00022 	 MISS
   55 	    13 	 0.11098 	 0.00022 	 MISS
    2 	    14 	 0.04976 	 0.00022 	 m..s
   38 	    15 	 0.10840 	 0.00023 	 MISS
   76 	    16 	 0.11426 	 0.00023 	 MISS
   66 	    17 	 0.11166 	 0.00025 	 MISS
   47 	    18 	 0.10899 	 0.00025 	 MISS
    9 	    19 	 0.06325 	 0.00025 	 m..s
   68 	    20 	 0.11211 	 0.00026 	 MISS
   21 	    21 	 0.10236 	 0.00027 	 MISS
   12 	    22 	 0.06796 	 0.00027 	 m..s
   58 	    23 	 0.11114 	 0.00028 	 MISS
   46 	    24 	 0.10883 	 0.00028 	 MISS
   63 	    25 	 0.11156 	 0.00028 	 MISS
   23 	    26 	 0.10432 	 0.00030 	 MISS
   32 	    27 	 0.10783 	 0.00031 	 MISS
   61 	    28 	 0.11144 	 0.00031 	 MISS
    3 	    29 	 0.04989 	 0.00031 	 m..s
   78 	    30 	 0.11571 	 0.00032 	 MISS
   48 	    31 	 0.10917 	 0.00032 	 MISS
   80 	    32 	 0.11599 	 0.00033 	 MISS
   29 	    33 	 0.10761 	 0.00034 	 MISS
   17 	    34 	 0.09135 	 0.00034 	 m..s
   64 	    35 	 0.11161 	 0.00041 	 MISS
   56 	    36 	 0.11106 	 0.00045 	 MISS
   79 	    37 	 0.11580 	 0.00047 	 MISS
    8 	    38 	 0.06317 	 0.00049 	 m..s
   53 	    39 	 0.11064 	 0.00066 	 MISS
    0 	    40 	 0.04939 	 0.00086 	 m..s
    4 	    41 	 0.05924 	 0.00149 	 m..s
   15 	    42 	 0.06953 	 0.00823 	 m..s
    6 	    43 	 0.06062 	 0.01370 	 m..s
   16 	    44 	 0.06957 	 0.01777 	 m..s
   11 	    45 	 0.06522 	 0.01885 	 m..s
   39 	    46 	 0.10842 	 0.02535 	 m..s
   41 	    47 	 0.10858 	 0.02556 	 m..s
   40 	    48 	 0.10854 	 0.02558 	 m..s
   14 	    49 	 0.06885 	 0.02684 	 m..s
   13 	    50 	 0.06851 	 0.03386 	 m..s
   18 	    51 	 0.09140 	 0.03790 	 m..s
   24 	    52 	 0.10613 	 0.11420 	 ~...
   20 	    53 	 0.10183 	 0.12289 	 ~...
   42 	    54 	 0.10865 	 0.12500 	 ~...
   44 	    55 	 0.10875 	 0.12516 	 ~...
   22 	    56 	 0.10280 	 0.12726 	 ~...
   81 	    57 	 0.11724 	 0.12785 	 ~...
   19 	    58 	 0.10121 	 0.13118 	 ~...
   25 	    59 	 0.10639 	 0.13359 	 ~...
   34 	    60 	 0.10803 	 0.13507 	 ~...
   54 	    61 	 0.11088 	 0.13618 	 ~...
   35 	    62 	 0.10807 	 0.13636 	 ~...
   62 	    63 	 0.11145 	 0.14204 	 m..s
   70 	    64 	 0.11220 	 0.14423 	 m..s
   28 	    65 	 0.10737 	 0.14497 	 m..s
   69 	    66 	 0.11215 	 0.14525 	 m..s
   59 	    67 	 0.11126 	 0.14657 	 m..s
   49 	    68 	 0.10944 	 0.14749 	 m..s
   72 	    69 	 0.11237 	 0.15180 	 m..s
   65 	    70 	 0.11164 	 0.15457 	 m..s
   37 	    71 	 0.10824 	 0.15523 	 m..s
   45 	    72 	 0.10876 	 0.15773 	 m..s
   67 	    73 	 0.11196 	 0.15786 	 m..s
   75 	    74 	 0.11376 	 0.15879 	 m..s
   85 	    75 	 0.17927 	 0.15951 	 ~...
   43 	    76 	 0.10868 	 0.16031 	 m..s
   30 	    77 	 0.10762 	 0.16396 	 m..s
   27 	    78 	 0.10736 	 0.16490 	 m..s
   36 	    79 	 0.10815 	 0.16513 	 m..s
   71 	    80 	 0.11227 	 0.16562 	 m..s
   33 	    81 	 0.10792 	 0.16876 	 m..s
   73 	    82 	 0.11243 	 0.16902 	 m..s
   86 	    83 	 0.18833 	 0.17499 	 ~...
   50 	    84 	 0.11033 	 0.17552 	 m..s
   82 	    85 	 0.11760 	 0.17704 	 m..s
   84 	    86 	 0.17620 	 0.18182 	 ~...
   89 	    87 	 0.20302 	 0.18800 	 ~...
   87 	    88 	 0.19146 	 0.18855 	 ~...
   88 	    89 	 0.19543 	 0.19491 	 ~...
   91 	    90 	 0.21620 	 0.22010 	 ~...
   92 	    91 	 0.21642 	 0.22174 	 ~...
   97 	    92 	 0.25142 	 0.23077 	 ~...
   96 	    93 	 0.24672 	 0.23125 	 ~...
   95 	    94 	 0.23237 	 0.24048 	 ~...
  103 	    95 	 0.28134 	 0.24670 	 m..s
  107 	    96 	 0.29068 	 0.25390 	 m..s
   94 	    97 	 0.22365 	 0.25673 	 m..s
   99 	    98 	 0.26558 	 0.25835 	 ~...
   90 	    99 	 0.21338 	 0.25950 	 m..s
  109 	   100 	 0.29531 	 0.26345 	 m..s
   93 	   101 	 0.21859 	 0.26378 	 m..s
  113 	   102 	 0.30723 	 0.27258 	 m..s
  100 	   103 	 0.27539 	 0.27412 	 ~...
  112 	   104 	 0.29966 	 0.27673 	 ~...
  101 	   105 	 0.27844 	 0.27768 	 ~...
  102 	   106 	 0.28084 	 0.27975 	 ~...
  108 	   107 	 0.29327 	 0.28674 	 ~...
  111 	   108 	 0.29712 	 0.28797 	 ~...
  105 	   109 	 0.28601 	 0.28877 	 ~...
   98 	   110 	 0.26110 	 0.29260 	 m..s
  106 	   111 	 0.28783 	 0.29343 	 ~...
  110 	   112 	 0.29580 	 0.29422 	 ~...
  104 	   113 	 0.28187 	 0.29610 	 ~...
  118 	   114 	 0.35696 	 0.31099 	 m..s
  114 	   115 	 0.32839 	 0.31560 	 ~...
  116 	   116 	 0.33777 	 0.35217 	 ~...
  117 	   117 	 0.34339 	 0.35404 	 ~...
  120 	   118 	 0.38076 	 0.36320 	 ~...
  119 	   119 	 0.37923 	 0.36583 	 ~...
  115 	   120 	 0.33581 	 0.36809 	 m..s
==========================================
r_mrr = 0.8723083734512329
r2_mrr = 0.6807790994644165
spearmanr_mrr@5 = 0.9588682055473328
spearmanr_mrr@10 = 0.9268713593482971
spearmanr_mrr@50 = 0.978814959526062
spearmanr_mrr@100 = 0.8726403117179871
spearmanr_mrr@All = 0.8971291184425354
==========================================
test time: 0.454
Done Testing dataset DBpedia50
total time taken: 183.96768975257874
training time taken: 179.19143557548523
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8723)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6808)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9589)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9269)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9788)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8726)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8971)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.0267555840910063}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 9832682499863930
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [945, 705, 91, 353, 193, 70, 587, 172, 630, 552, 500, 1093, 675, 581, 634, 351, 1116, 1031, 742, 504, 1194, 1029, 629, 1052, 97, 13, 888, 325, 19, 916, 697, 1023, 726, 1190, 1103, 408, 813, 287, 334, 731, 188, 883, 282, 703, 1135, 436, 386, 1126, 223, 230, 419, 789, 520, 340, 971, 851, 624, 1098, 61, 656, 521, 935, 260, 750, 55, 1017, 574, 128, 565, 297, 593, 542, 298, 1200, 421, 202, 842, 969, 145, 235, 761, 155, 66, 1072, 740, 1136, 606, 1111, 544, 101, 1034, 1204, 40, 492, 1077, 528, 217, 153, 657, 641, 1125, 390, 288, 1189, 371, 860, 724, 157, 51, 470, 445, 557, 1076, 163, 98, 499, 835, 10, 941, 269, 931]
valid_ids (0): []
train_ids (1094): [1048, 130, 572, 181, 1130, 125, 48, 225, 812, 820, 1011, 613, 514, 320, 144, 948, 625, 843, 1144, 413, 618, 264, 678, 921, 1008, 978, 579, 147, 1012, 914, 263, 392, 929, 361, 1037, 1114, 1210, 957, 1172, 330, 135, 982, 592, 444, 471, 468, 988, 702, 122, 881, 26, 1181, 212, 938, 307, 209, 1146, 75, 876, 178, 598, 126, 901, 1080, 597, 161, 1120, 631, 1083, 224, 694, 1207, 633, 214, 18, 563, 554, 467, 1075, 693, 380, 576, 752, 1100, 808, 666, 176, 794, 335, 1184, 547, 1068, 517, 567, 676, 900, 966, 787, 821, 186, 459, 295, 1140, 551, 1159, 268, 838, 973, 1133, 1117, 302, 893, 252, 691, 715, 160, 979, 897, 614, 642, 932, 138, 1199, 706, 133, 928, 956, 99, 684, 543, 885, 823, 985, 937, 1063, 1058, 1145, 206, 824, 774, 939, 469, 1137, 560, 725, 1059, 387, 1016, 45, 241, 1128, 910, 874, 1175, 1055, 68, 204, 911, 365, 14, 964, 24, 665, 132, 924, 233, 171, 717, 853, 785, 936, 350, 513, 140, 426, 58, 783, 1148, 285, 1187, 1009, 695, 748, 33, 254, 1060, 1087, 478, 1170, 7, 942, 456, 990, 686, 718, 1209, 1105, 627, 1074, 819, 35, 137, 1078, 420, 457, 947, 1040, 292, 1169, 965, 773, 397, 71, 418, 1092, 707, 34, 1167, 652, 727, 227, 778, 191, 446, 603, 534, 411, 1026, 951, 1155, 170, 972, 617, 1025, 1102, 404, 741, 258, 1044, 738, 647, 103, 815, 505, 757, 1001, 485, 1165, 604, 1101, 210, 541, 185, 612, 635, 739, 42, 331, 1139, 770, 879, 219, 991, 753, 296, 247, 60, 531, 1020, 195, 816, 1010, 615, 17, 305, 602, 643, 799, 136, 466, 360, 359, 927, 904, 182, 248, 115, 1113, 747, 626, 189, 479, 1211, 197, 311, 493, 1065, 246, 120, 584, 700, 890, 638, 840, 655, 756, 352, 620, 699, 646, 388, 69, 555, 134, 149, 348, 1041, 414, 434, 121, 271, 46, 889, 274, 293, 681, 95, 396, 74, 958, 280, 150, 790, 600, 332, 3, 146, 165, 1050, 1153, 711, 765, 1066, 428, 215, 1115, 915, 1123, 423, 925, 1180, 976, 222, 784, 743, 515, 501, 522, 780, 39, 211, 461, 1182, 569, 312, 997, 830, 100, 272, 429, 668, 451, 205, 1166, 848, 416, 1193, 796, 409, 90, 253, 917, 758, 975, 124, 465, 1196, 127, 2, 628, 1108, 968, 301, 775, 1150, 594, 119, 381, 131, 619, 198, 1160, 868, 512, 431, 545, 59, 909, 277, 930, 558, 229, 37, 403, 410, 1195, 907, 173, 525, 810, 218, 207, 535, 1149, 1192, 113, 950, 1019, 800, 807, 1067, 480, 63, 719, 1110, 992, 452, 76, 107, 234, 52, 795, 1039, 483, 129, 1158, 304, 412, 494, 827, 22, 237, 993, 685, 832, 886, 637, 1062, 855, 463, 899, 1178, 856, 339, 393, 88, 401, 1038, 77, 803, 142, 1035, 549, 167, 1081, 802, 1082, 766, 1032, 326, 508, 376, 908, 310, 1164, 317, 1061, 148, 329, 363, 829, 786, 980, 266, 1006, 1201, 1179, 432, 828, 902, 961, 1099, 257, 817, 1047, 995, 861, 919, 220, 546, 1173, 1107, 994, 1198, 357, 105, 611, 1147, 1154, 203, 487, 384, 448, 873, 709, 474, 481, 806, 255, 1097, 1, 869, 940, 506, 284, 123, 996, 651, 85, 720, 162, 658, 291, 1057, 519, 156, 112, 1171, 400, 44, 1091, 183, 54, 364, 782, 383, 422, 184, 788, 654, 477, 850, 511, 745, 918, 151, 427, 455, 841, 349, 43, 308, 309, 509, 847, 1015, 722, 109, 454, 953, 779, 952, 366, 949, 962, 299, 346, 208, 49, 28, 649, 143, 623, 690, 755, 674, 496, 548, 1090, 116, 912, 1127, 894, 355, 36, 159, 267, 259, 1177, 289, 83, 67, 672, 529, 871, 0, 721, 273, 1138, 696, 1152, 84, 56, 645, 523, 677, 797, 347, 1106, 243, 1096, 878, 337, 687, 863, 596, 72, 472, 437, 844, 80, 379, 50, 590, 342, 903, 236, 497, 683, 564, 875, 1186, 805, 29, 462, 998, 370, 859, 663, 1124, 793, 729, 781, 977, 834, 967, 488, 1021, 846, 884, 1049, 1208, 226, 510, 194, 15, 516, 290, 591, 1094, 1079, 11, 92, 905, 1022, 23, 1112, 327, 21, 435, 764, 728, 944, 1161, 923, 368, 440, 1205, 760, 692, 250, 792, 450, 303, 751, 262, 744, 524, 216, 716, 417, 877, 689, 960, 502, 1122, 256, 460, 891, 880, 1007, 405, 1071, 710, 640, 1141, 837, 275, 385, 1191, 660, 659, 870, 5, 6, 179, 1004, 213, 804, 653, 582, 1003, 599, 391, 1176, 1163, 857, 491, 333, 527, 374, 96, 532, 141, 586, 698, 41, 959, 57, 1131, 322, 166, 441, 389, 324, 749, 1119, 244, 164, 589, 858, 550, 458, 825, 1156, 196, 946, 588, 1064, 369, 169, 152, 490, 769, 736, 553, 270, 1168, 906, 1212, 1045, 238, 540, 679, 1013, 1183, 1024, 737, 556, 814, 375, 809, 1143, 763, 754, 896, 605, 168, 664, 154, 27, 854, 12, 87, 73, 192, 1197, 1121, 1056, 1151, 622, 319, 714, 47, 704, 670, 449, 771, 776, 378, 447, 338, 580, 9, 433, 1042, 836, 86, 245, 583, 94, 595, 661, 970, 1089, 200, 314, 983, 306, 791, 538, 1018, 475, 733, 425, 251, 1028, 16, 1086, 31, 887, 818, 1188, 577, 453, 723, 673, 1084, 680, 1053, 1033, 495, 221, 872, 377, 682, 530, 328, 438, 845, 1206, 323, 486, 424, 954, 852, 568, 610, 621, 313, 316, 1174, 526, 484, 65, 104, 518, 507, 895, 669, 81, 402, 864, 53, 667, 406, 826, 866, 578, 231, 228, 345, 25, 32, 180, 442, 701, 1070, 943, 867, 1109, 430, 239, 616, 1142, 82, 240, 913, 158, 367, 934, 734, 399, 38, 114, 232, 798, 1185, 372, 648, 498, 111, 394, 585, 89, 632, 831, 1043, 1162, 1129, 1213, 920, 898, 759, 767, 639, 315, 892, 688, 671, 362, 1046, 281, 341, 566, 536, 283, 1069, 1000, 1014, 708, 443, 8, 30, 382, 1073, 561, 199, 318, 1134, 730, 539, 822, 772, 862, 321, 356, 1095, 833, 768, 571, 1036, 746, 981, 93, 415, 999, 102, 1088, 1002, 106, 358, 644, 732, 201, 955, 265, 1005, 713, 117, 575, 559, 190, 926, 1085, 1027, 489, 503, 1132, 62, 242, 300, 118, 482, 279, 294, 398, 762, 636, 989, 849, 984, 1118, 261, 1203, 662, 1202, 4, 476, 175, 79, 249, 974, 1051, 735, 64, 601, 354, 963, 407, 608, 286, 987, 336, 276, 865, 607, 609, 177, 1104, 712, 78, 801, 573, 933, 344, 562, 395, 986, 1030, 108, 570, 110, 464, 278, 882, 187, 777, 1054, 473, 1214, 20, 922, 533, 439, 373, 811, 174, 343, 839, 139, 1157, 537, 650]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2862054390058037
the save name prefix for this run is:  chkpt-ID_2862054390058037_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 563
rank avg (pred): 0.518 +- 0.002
mrr vals (pred, true): 0.000, 0.216
batch losses (mrrl, rdl): 0.0, 0.0011860833

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 763
rank avg (pred): 0.464 +- 0.270
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0, 4.03258e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 173
rank avg (pred): 0.400 +- 0.275
mrr vals (pred, true): 0.223, 0.000
batch losses (mrrl, rdl): 0.0, 6.12902e-05

Epoch over!
epoch time: 11.781

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 633
rank avg (pred): 0.403 +- 0.289
mrr vals (pred, true): 0.239, 0.124
batch losses (mrrl, rdl): 0.0, 0.000129808

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 261
rank avg (pred): 0.135 +- 0.099
mrr vals (pred, true): 0.251, 0.366
batch losses (mrrl, rdl): 0.0, 8.87998e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 906
rank avg (pred): 0.576 +- 0.392
mrr vals (pred, true): 0.201, 0.017
batch losses (mrrl, rdl): 0.0, 0.0002465037

Epoch over!
epoch time: 11.666

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1154
rank avg (pred): 0.277 +- 0.201
mrr vals (pred, true): 0.241, 0.197
batch losses (mrrl, rdl): 0.0, 8.30427e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 601
rank avg (pred): 0.392 +- 0.304
mrr vals (pred, true): 0.239, 0.103
batch losses (mrrl, rdl): 0.0, 4.7869e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.365 +- 0.272
mrr vals (pred, true): 0.179, 0.159
batch losses (mrrl, rdl): 0.0, 0.0001325635

Epoch over!
epoch time: 11.921

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 2
rank avg (pred): 0.147 +- 0.114
mrr vals (pred, true): 0.235, 0.334
batch losses (mrrl, rdl): 0.0, 4.22341e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1120
rank avg (pred): 0.335 +- 0.257
mrr vals (pred, true): 0.203, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003843417

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 160
rank avg (pred): 0.378 +- 0.294
mrr vals (pred, true): 0.190, 0.163
batch losses (mrrl, rdl): 0.0, 0.0001030761

Epoch over!
epoch time: 11.737

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 366
rank avg (pred): 0.380 +- 0.292
mrr vals (pred, true): 0.117, 0.185
batch losses (mrrl, rdl): 0.0, 0.0001898515

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 20
rank avg (pred): 0.130 +- 0.104
mrr vals (pred, true): 0.277, 0.335
batch losses (mrrl, rdl): 0.0, 5.3957e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 756
rank avg (pred): 0.452 +- 0.321
mrr vals (pred, true): 0.150, 0.001
batch losses (mrrl, rdl): 0.0, 1.86721e-05

Epoch over!
epoch time: 11.743

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 549
rank avg (pred): 0.282 +- 0.219
mrr vals (pred, true): 0.197, 0.175
batch losses (mrrl, rdl): 0.0046649664, 6.28464e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 458
rank avg (pred): 0.394 +- 0.288
mrr vals (pred, true): 0.121, 0.000
batch losses (mrrl, rdl): 0.0499270186, 0.00010848

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 205
rank avg (pred): 0.436 +- 0.285
mrr vals (pred, true): 0.085, 0.001
batch losses (mrrl, rdl): 0.0121003659, 6.41761e-05

Epoch over!
epoch time: 12.283

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 882
rank avg (pred): 0.661 +- 0.385
mrr vals (pred, true): 0.092, 0.000
batch losses (mrrl, rdl): 0.0173450727, 0.0007677993

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 722
rank avg (pred): 0.430 +- 0.284
mrr vals (pred, true): 0.084, 0.000
batch losses (mrrl, rdl): 0.0118717095, 4.81362e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1079
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.293, 0.332
batch losses (mrrl, rdl): 0.0147210471, 0.0005309497

Epoch over!
epoch time: 12.007

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.770 +- 0.329
mrr vals (pred, true): 0.064, 0.008
batch losses (mrrl, rdl): 0.0018422982, 0.0007350202

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 302
rank avg (pred): 0.019 +- 0.014
mrr vals (pred, true): 0.217, 0.227
batch losses (mrrl, rdl): 0.001125528, 0.0004297031

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 251
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.356, 0.374
batch losses (mrrl, rdl): 0.0031568683, 0.0002905198

Epoch over!
epoch time: 11.985

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 506
rank avg (pred): 0.003 +- 0.002
mrr vals (pred, true): 0.295, 0.273
batch losses (mrrl, rdl): 0.0047495468, 0.0013043449

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.434 +- 0.280
mrr vals (pred, true): 0.099, 0.139
batch losses (mrrl, rdl): 0.0161816515, 0.0002515813

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 250
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.324, 0.361
batch losses (mrrl, rdl): 0.0133332983, 0.0003006543

Epoch over!
epoch time: 12.127

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 772
rank avg (pred): 0.770 +- 0.318
mrr vals (pred, true): 0.061, 0.000
batch losses (mrrl, rdl): 0.0012227893, 0.0001987206

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 463
rank avg (pred): 0.410 +- 0.285
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.036649853, 1.40689e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 114
rank avg (pred): 0.418 +- 0.275
mrr vals (pred, true): 0.108, 0.145
batch losses (mrrl, rdl): 0.0134977372, 0.0001504044

Epoch over!
epoch time: 11.951

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 786
rank avg (pred): 0.882 +- 0.285
mrr vals (pred, true): 0.030, 0.000
batch losses (mrrl, rdl): 0.0040116976, 0.0025965034

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1188
rank avg (pred): 0.376 +- 0.259
mrr vals (pred, true): 0.114, 0.000
batch losses (mrrl, rdl): 0.0413104519, 0.000106701

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.386 +- 0.246
mrr vals (pred, true): 0.103, 0.168
batch losses (mrrl, rdl): 0.0425134823, 0.0002916592

Epoch over!
epoch time: 12.44

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.016 +- 0.012
mrr vals (pred, true): 0.264, 0.255
batch losses (mrrl, rdl): 0.0008853677, 0.0004608601

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 84
rank avg (pred): 0.397 +- 0.250
mrr vals (pred, true): 0.103, 0.146
batch losses (mrrl, rdl): 0.0190294087, 0.000222447

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.460 +- 0.259
mrr vals (pred, true): 0.094, 0.157
batch losses (mrrl, rdl): 0.0398583636, 0.0005197446

Epoch over!
epoch time: 12.114

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 986
rank avg (pred): 0.012 +- 0.009
mrr vals (pred, true): 0.283, 0.275
batch losses (mrrl, rdl): 0.0006283403, 0.0006200309

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1149
rank avg (pred): 0.031 +- 0.023
mrr vals (pred, true): 0.261, 0.228
batch losses (mrrl, rdl): 0.0109822918, 0.0016554716

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1016
rank avg (pred): 0.367 +- 0.239
mrr vals (pred, true): 0.108, 0.192
batch losses (mrrl, rdl): 0.0712830275, 0.0001467831

Epoch over!
epoch time: 12.028

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 64
rank avg (pred): 0.029 +- 0.022
mrr vals (pred, true): 0.229, 0.201
batch losses (mrrl, rdl): 0.0079624215, 0.0005112626

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 122
rank avg (pred): 0.440 +- 0.249
mrr vals (pred, true): 0.093, 0.160
batch losses (mrrl, rdl): 0.0449669696, 0.0007190151

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.415 +- 0.242
mrr vals (pred, true): 0.126, 0.154
batch losses (mrrl, rdl): 0.0079852194, 0.0003024531

Epoch over!
epoch time: 11.994

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 247
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.301, 0.348
batch losses (mrrl, rdl): 0.0225742366, 0.0002703729

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 665
rank avg (pred): 0.422 +- 0.223
mrr vals (pred, true): 0.087, 0.000
batch losses (mrrl, rdl): 0.0136461956, 0.0001100063

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1059
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.461, 0.386
batch losses (mrrl, rdl): 0.0576350056, 0.0002107561

Epoch over!
epoch time: 11.834

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.612 +- 0.244
mrr vals (pred, true): 0.012, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.03543 	 5e-0500 	 m..s
    6 	     1 	 0.03315 	 5e-0500 	 m..s
    4 	     2 	 0.03234 	 6e-0500 	 m..s
    9 	     3 	 0.05676 	 0.00014 	 m..s
   17 	     4 	 0.08149 	 0.00015 	 m..s
   78 	     5 	 0.09913 	 0.00016 	 m..s
   67 	     6 	 0.09358 	 0.00017 	 m..s
   37 	     7 	 0.09066 	 0.00017 	 m..s
    0 	     8 	 0.01239 	 0.00018 	 ~...
   19 	     9 	 0.08307 	 0.00018 	 m..s
   57 	    10 	 0.09233 	 0.00020 	 m..s
   37 	    11 	 0.09066 	 0.00020 	 m..s
   75 	    12 	 0.09805 	 0.00020 	 m..s
   74 	    13 	 0.09788 	 0.00021 	 m..s
   24 	    14 	 0.08629 	 0.00021 	 m..s
    1 	    15 	 0.01936 	 0.00021 	 ~...
   68 	    16 	 0.09432 	 0.00021 	 m..s
   37 	    17 	 0.09066 	 0.00022 	 m..s
   77 	    18 	 0.09911 	 0.00022 	 m..s
   58 	    19 	 0.09260 	 0.00022 	 m..s
    2 	    20 	 0.02358 	 0.00022 	 ~...
   72 	    21 	 0.09709 	 0.00024 	 m..s
   56 	    22 	 0.09221 	 0.00024 	 m..s
   37 	    23 	 0.09066 	 0.00025 	 m..s
   16 	    24 	 0.08058 	 0.00025 	 m..s
   37 	    25 	 0.09066 	 0.00026 	 m..s
   37 	    26 	 0.09066 	 0.00027 	 m..s
   81 	    27 	 0.09947 	 0.00027 	 m..s
   13 	    28 	 0.07177 	 0.00029 	 m..s
   37 	    29 	 0.09066 	 0.00030 	 m..s
   33 	    30 	 0.08952 	 0.00031 	 m..s
   10 	    31 	 0.05936 	 0.00034 	 m..s
   22 	    32 	 0.08548 	 0.00034 	 m..s
   65 	    33 	 0.09308 	 0.00035 	 m..s
    7 	    34 	 0.03321 	 0.00035 	 m..s
   30 	    35 	 0.08810 	 0.00036 	 m..s
   32 	    36 	 0.08912 	 0.00040 	 m..s
   55 	    37 	 0.09219 	 0.00042 	 m..s
   28 	    38 	 0.08757 	 0.00043 	 m..s
   11 	    39 	 0.06107 	 0.00046 	 m..s
    5 	    40 	 0.03256 	 0.00046 	 m..s
   37 	    41 	 0.09066 	 0.00046 	 m..s
   80 	    42 	 0.09933 	 0.00047 	 m..s
   18 	    43 	 0.08293 	 0.00053 	 m..s
    3 	    44 	 0.02959 	 0.00054 	 ~...
   34 	    45 	 0.08957 	 0.00082 	 m..s
   21 	    46 	 0.08474 	 0.00106 	 m..s
   27 	    47 	 0.08732 	 0.00116 	 m..s
   12 	    48 	 0.06128 	 0.00201 	 m..s
   54 	    49 	 0.09219 	 0.00284 	 m..s
   31 	    50 	 0.08859 	 0.03525 	 m..s
   14 	    51 	 0.07564 	 0.05263 	 ~...
   86 	    52 	 0.11043 	 0.10268 	 ~...
   20 	    53 	 0.08458 	 0.10737 	 ~...
   85 	    54 	 0.10378 	 0.11903 	 ~...
   73 	    55 	 0.09732 	 0.12137 	 ~...
   29 	    56 	 0.08774 	 0.12369 	 m..s
   26 	    57 	 0.08710 	 0.12838 	 m..s
   62 	    58 	 0.09297 	 0.12880 	 m..s
   15 	    59 	 0.08052 	 0.12985 	 m..s
   37 	    60 	 0.09066 	 0.13086 	 m..s
   64 	    61 	 0.09303 	 0.13345 	 m..s
   61 	    62 	 0.09289 	 0.13359 	 m..s
   66 	    63 	 0.09348 	 0.13617 	 m..s
   84 	    64 	 0.10171 	 0.13671 	 m..s
   59 	    65 	 0.09260 	 0.14228 	 m..s
   37 	    66 	 0.09066 	 0.14592 	 m..s
   23 	    67 	 0.08606 	 0.14613 	 m..s
   35 	    68 	 0.09041 	 0.14639 	 m..s
   25 	    69 	 0.08710 	 0.15226 	 m..s
   92 	    70 	 0.18123 	 0.15324 	 ~...
   60 	    71 	 0.09265 	 0.15495 	 m..s
   37 	    72 	 0.09066 	 0.15532 	 m..s
   37 	    73 	 0.09066 	 0.15762 	 m..s
   69 	    74 	 0.09501 	 0.15786 	 m..s
   63 	    75 	 0.09297 	 0.15861 	 m..s
   76 	    76 	 0.09833 	 0.15865 	 m..s
   36 	    77 	 0.09064 	 0.16004 	 m..s
   37 	    78 	 0.09066 	 0.16376 	 m..s
   96 	    79 	 0.19955 	 0.16436 	 m..s
   93 	    80 	 0.19240 	 0.16644 	 ~...
   71 	    81 	 0.09636 	 0.16857 	 m..s
   50 	    82 	 0.09070 	 0.16907 	 m..s
   52 	    83 	 0.09102 	 0.16945 	 m..s
   51 	    84 	 0.09076 	 0.17173 	 m..s
   79 	    85 	 0.09927 	 0.17276 	 m..s
   83 	    86 	 0.10157 	 0.17481 	 m..s
   82 	    87 	 0.09954 	 0.17552 	 m..s
   99 	    88 	 0.20302 	 0.17592 	 ~...
   53 	    89 	 0.09137 	 0.17611 	 m..s
   87 	    90 	 0.15923 	 0.17805 	 ~...
   70 	    91 	 0.09610 	 0.17807 	 m..s
   95 	    92 	 0.19789 	 0.17963 	 ~...
   90 	    93 	 0.17461 	 0.18655 	 ~...
   89 	    94 	 0.16460 	 0.18736 	 ~...
   97 	    95 	 0.20063 	 0.19391 	 ~...
   88 	    96 	 0.16257 	 0.19491 	 m..s
  102 	    97 	 0.21926 	 0.19952 	 ~...
   91 	    98 	 0.17659 	 0.21254 	 m..s
  103 	    99 	 0.22457 	 0.21272 	 ~...
   94 	   100 	 0.19355 	 0.21371 	 ~...
  101 	   101 	 0.21217 	 0.21755 	 ~...
   98 	   102 	 0.20199 	 0.22818 	 ~...
  100 	   103 	 0.21000 	 0.22848 	 ~...
  107 	   104 	 0.26123 	 0.25578 	 ~...
  110 	   105 	 0.28230 	 0.25980 	 ~...
  115 	   106 	 0.29720 	 0.26209 	 m..s
  106 	   107 	 0.25064 	 0.26748 	 ~...
  109 	   108 	 0.27794 	 0.26952 	 ~...
  112 	   109 	 0.28819 	 0.27508 	 ~...
  108 	   110 	 0.27297 	 0.27575 	 ~...
  104 	   111 	 0.24133 	 0.27975 	 m..s
  113 	   112 	 0.29246 	 0.28797 	 ~...
  105 	   113 	 0.24612 	 0.28984 	 m..s
  111 	   114 	 0.28732 	 0.29470 	 ~...
  118 	   115 	 0.32415 	 0.35024 	 ~...
  119 	   116 	 0.39697 	 0.35127 	 m..s
  116 	   117 	 0.31969 	 0.35340 	 m..s
  114 	   118 	 0.29579 	 0.35787 	 m..s
  117 	   119 	 0.32396 	 0.36229 	 m..s
  120 	   120 	 0.40725 	 0.36583 	 m..s
==========================================
r_mrr = 0.8398878574371338
r2_mrr = 0.6770589351654053
spearmanr_mrr@5 = 0.9212285876274109
spearmanr_mrr@10 = 0.724014937877655
spearmanr_mrr@50 = 0.9500246644020081
spearmanr_mrr@100 = 0.8437973856925964
spearmanr_mrr@All = 0.8670982718467712
==========================================
test time: 0.392
Done Testing dataset DBpedia50
total time taken: 185.14835834503174
training time taken: 180.06118154525757
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8399)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6771)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9212)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.7240)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9500)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8438)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8671)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.2123887028428726}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 3734228699327612
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1124, 1039, 1162, 789, 659, 331, 984, 73, 991, 424, 313, 699, 399, 693, 988, 854, 1180, 583, 810, 241, 1131, 392, 879, 429, 362, 436, 1120, 1170, 1002, 498, 1202, 556, 599, 622, 236, 132, 630, 144, 1058, 323, 1074, 637, 408, 343, 896, 1038, 1052, 1005, 49, 844, 989, 378, 41, 434, 402, 94, 2, 356, 178, 613, 96, 1111, 517, 612, 518, 750, 1025, 671, 963, 45, 825, 771, 228, 882, 1021, 481, 28, 1193, 851, 916, 186, 880, 17, 698, 177, 85, 908, 315, 1018, 227, 371, 603, 775, 925, 194, 629, 365, 314, 296, 510, 48, 224, 105, 652, 935, 1023, 1033, 254, 232, 162, 419, 496, 1158, 1119, 61, 868, 134, 508, 700, 970, 618]
valid_ids (0): []
train_ids (1094): [1159, 417, 588, 468, 1118, 864, 731, 898, 772, 62, 67, 195, 871, 153, 798, 115, 119, 631, 244, 758, 274, 109, 621, 1097, 752, 1187, 780, 1214, 322, 739, 861, 1020, 423, 180, 34, 449, 648, 95, 470, 1044, 422, 1081, 139, 40, 509, 1157, 368, 151, 867, 168, 803, 413, 1104, 1205, 900, 366, 51, 68, 142, 229, 1008, 996, 530, 372, 179, 465, 310, 866, 678, 7, 824, 978, 1129, 494, 596, 522, 924, 972, 1156, 158, 110, 1026, 1101, 968, 176, 1206, 552, 12, 1079, 403, 802, 192, 607, 560, 728, 708, 947, 376, 815, 488, 27, 1169, 982, 389, 13, 764, 1068, 735, 471, 906, 873, 350, 456, 369, 1146, 975, 383, 619, 87, 609, 933, 171, 749, 328, 623, 409, 899, 240, 977, 1004, 674, 432, 938, 462, 535, 561, 1078, 155, 1096, 672, 837, 1168, 388, 922, 1128, 527, 1147, 285, 483, 99, 710, 594, 687, 580, 395, 295, 574, 292, 416, 1212, 251, 831, 234, 445, 796, 1133, 643, 71, 357, 847, 966, 932, 415, 452, 971, 732, 628, 1108, 602, 55, 1088, 394, 892, 10, 104, 913, 777, 1053, 526, 493, 321, 474, 489, 33, 1123, 579, 799, 1196, 1048, 459, 1009, 396, 967, 635, 894, 601, 44, 1042, 636, 420, 743, 499, 39, 785, 781, 288, 919, 543, 238, 980, 454, 725, 30, 114, 691, 277, 1019, 88, 248, 734, 647, 1164, 307, 828, 849, 450, 1056, 282, 1063, 1012, 696, 231, 675, 391, 1114, 808, 839, 990, 656, 11, 216, 272, 746, 354, 537, 91, 1036, 720, 727, 976, 787, 220, 495, 877, 206, 221, 412, 129, 327, 742, 472, 941, 89, 853, 353, 212, 547, 414, 278, 170, 548, 878, 1029, 198, 1076, 249, 640, 1102, 421, 1184, 401, 337, 1177, 956, 156, 318, 146, 512, 615, 1186, 715, 358, 386, 726, 721, 102, 223, 918, 529, 289, 143, 137, 536, 1207, 776, 895, 565, 239, 1121, 901, 1110, 3, 70, 111, 161, 576, 539, 1060, 345, 103, 1087, 1103, 572, 410, 586, 1150, 848, 347, 373, 437, 654, 397, 860, 805, 65, 50, 52, 1043, 163, 501, 148, 18, 165, 598, 273, 738, 1024, 657, 890, 411, 36, 820, 202, 1185, 4, 931, 205, 426, 768, 407, 1151, 923, 477, 1051, 390, 1034, 644, 6, 773, 9, 814, 874, 641, 507, 1073, 486, 279, 24, 910, 1006, 605, 361, 237, 521, 519, 766, 538, 1011, 965, 500, 1197, 127, 697, 181, 393, 545, 306, 993, 214, 107, 428, 505, 182, 729, 639, 430, 222, 1194, 1139, 642, 166, 688, 929, 242, 571, 569, 747, 582, 1213, 1199, 845, 774, 490, 1178, 942, 979, 1155, 324, 84, 595, 435, 66, 404, 934, 418, 191, 669, 463, 464, 998, 460, 377, 485, 1149, 722, 902, 308, 1046, 75, 573, 816, 524, 287, 597, 1064, 567, 949, 233, 438, 440, 250, 606, 193, 141, 1109, 427, 384, 855, 779, 1138, 341, 575, 1145, 1045, 937, 14, 112, 21, 487, 125, 1191, 994, 200, 138, 620, 349, 246, 670, 1144, 842, 1166, 568, 885, 1135, 1099, 564, 346, 562, 259, 1122, 56, 443, 338, 632, 317, 1037, 329, 484, 64, 650, 891, 786, 267, 225, 664, 829, 447, 754, 126, 905, 159, 823, 255, 1093, 625, 253, 16, 196, 1134, 912, 907, 930, 352, 888, 340, 86, 1090, 304, 767, 515, 325, 442, 265, 719, 686, 549, 1091, 190, 266, 275, 441, 1100, 917, 903, 1015, 865, 1094, 709, 600, 887, 1075, 841, 291, 1211, 832, 293, 332, 585, 541, 1116, 380, 32, 1016, 199, 1204, 525, 461, 281, 210, 467, 319, 724, 1141, 995, 1201, 544, 843, 433, 184, 744, 260, 948, 1203, 15, 681, 634, 116, 969, 733, 869, 791, 1027, 26, 852, 1165, 666, 90, 268, 245, 627, 473, 359, 577, 1028, 286, 876, 578, 794, 817, 59, 677, 1179, 339, 197, 624, 532, 957, 793, 150, 348, 638, 1054, 174, 381, 961, 145, 203, 718, 626, 661, 952, 312, 492, 1065, 1017, 633, 122, 884, 1085, 911, 542, 1126, 958, 1160, 782, 1195, 1210, 333, 737, 1080, 1092, 658, 169, 448, 1112, 795, 1106, 1163, 665, 757, 128, 962, 858, 207, 875, 835, 673, 425, 270, 1071, 469, 262, 247, 189, 1095, 106, 1188, 534, 97, 491, 503, 8, 904, 264, 797, 320, 859, 1001, 706, 130, 992, 936, 133, 1132, 1148, 589, 283, 19, 256, 723, 550, 1062, 187, 1125, 342, 608, 431, 311, 502, 58, 1003, 497, 1030, 451, 563, 294, 444, 959, 926, 98, 551, 712, 1041, 651, 778, 1113, 570, 707, 1183, 1209, 360, 72, 1175, 881, 684, 334, 1098, 974, 857, 1067, 759, 77, 705, 883, 63, 985, 685, 164, 1172, 1161, 692, 140, 755, 363, 1031, 690, 80, 172, 748, 617, 668, 57, 1049, 783, 31, 973, 475, 101, 188, 889, 351, 1072, 846, 587, 204, 297, 983, 1105, 1032, 316, 740, 124, 20, 921, 1153, 1069, 955, 379, 950, 834, 915, 893, 559, 1040, 219, 711, 42, 154, 25, 761, 717, 986, 714, 1171, 897, 610, 528, 1189, 482, 546, 400, 53, 850, 683, 1115, 1140, 792, 804, 997, 370, 581, 1136, 964, 1089, 590, 1154, 213, 82, 680, 1057, 953, 649, 784, 215, 29, 476, 653, 655, 1000, 290, 299, 682, 614, 466, 330, 593, 252, 157, 1050, 263, 821, 160, 78, 769, 611, 702, 217, 523, 108, 1200, 1082, 1083, 382, 121, 763, 201, 756, 591, 1137, 558, 838, 694, 47, 123, 1198, 800, 480, 939, 822, 208, 807, 504, 173, 584, 1066, 704, 1077, 147, 960, 136, 663, 269, 271, 981, 943, 37, 790, 553, 1035, 439, 284, 554, 385, 336, 801, 1176, 1, 741, 243, 1181, 1014, 730, 1143, 557, 185, 344, 1022, 940, 211, 1055, 826, 1007, 944, 149, 120, 819, 387, 300, 1142, 375, 516, 230, 1084, 280, 679, 809, 646, 592, 836, 1047, 533, 736, 46, 954, 676, 43, 226, 856, 1174, 478, 667, 261, 81, 60, 92, 302, 833, 513, 22, 751, 69, 364, 367, 1086, 209, 753, 540, 1117, 406, 1173, 616, 999, 830, 788, 945, 862, 74, 765, 1182, 301, 695, 812, 303, 1127, 131, 1107, 83, 1013, 914, 1010, 1167, 1208, 920, 827, 117, 305, 453, 660, 76, 183, 1190, 946, 806, 520, 928, 840, 716, 872, 506, 818, 175, 298, 927, 35, 762, 604, 811, 886, 326, 1059, 555, 0, 93, 100, 335, 1152, 258, 566, 398, 79, 405, 38, 458, 257, 446, 167, 703, 235, 662, 1070, 374, 455, 870, 135, 951, 1192, 152, 457, 23, 745, 863, 514, 909, 701, 276, 5, 813, 1061, 987, 113, 760, 355, 218, 531, 479, 1130, 713, 770, 118, 511, 309, 645, 54, 689]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5620701021019421
the save name prefix for this run is:  chkpt-ID_5620701021019421_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 616
rank avg (pred): 0.552 +- 0.005
mrr vals (pred, true): 0.000, 0.125
batch losses (mrrl, rdl): 0.0, 0.000904922

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 360
rank avg (pred): 0.439 +- 0.274
mrr vals (pred, true): 0.052, 0.159
batch losses (mrrl, rdl): 0.0, 0.00042158

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 863
rank avg (pred): 0.441 +- 0.340
mrr vals (pred, true): 0.177, 0.008
batch losses (mrrl, rdl): 0.0, 0.0006992832

Epoch over!
epoch time: 11.995

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1088
rank avg (pred): 0.382 +- 0.290
mrr vals (pred, true): 0.143, 0.181
batch losses (mrrl, rdl): 0.0, 0.0001994866

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 147
rank avg (pred): 0.408 +- 0.282
mrr vals (pred, true): 0.044, 0.159
batch losses (mrrl, rdl): 0.0, 0.0001755747

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.369 +- 0.306
mrr vals (pred, true): 0.180, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001228951

Epoch over!
epoch time: 11.842

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 66
rank avg (pred): 0.176 +- 0.259
mrr vals (pred, true): 0.201, 0.214
batch losses (mrrl, rdl): 0.0, 1.3839e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 690
rank avg (pred): 0.419 +- 0.333
mrr vals (pred, true): 0.095, 0.001
batch losses (mrrl, rdl): 0.0, 0.000103381

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.428 +- 0.328
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001569651

Epoch over!
epoch time: 11.788

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 479
rank avg (pred): 0.355 +- 0.318
mrr vals (pred, true): 0.189, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001946194

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 545
rank avg (pred): 0.305 +- 0.301
mrr vals (pred, true): 0.107, 0.165
batch losses (mrrl, rdl): 0.0, 2.69398e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 127
rank avg (pred): 0.402 +- 0.327
mrr vals (pred, true): 0.200, 0.147
batch losses (mrrl, rdl): 0.0, 0.0001213705

Epoch over!
epoch time: 11.874

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 949
rank avg (pred): 0.536 +- 0.352
mrr vals (pred, true): 0.127, 0.001
batch losses (mrrl, rdl): 0.0, 8.63584e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 417
rank avg (pred): 0.427 +- 0.308
mrr vals (pred, true): 0.029, 0.000
batch losses (mrrl, rdl): 0.0, 8.03163e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 44
rank avg (pred): 0.185 +- 0.286
mrr vals (pred, true): 0.219, 0.242
batch losses (mrrl, rdl): 0.0, 3.0811e-06

Epoch over!
epoch time: 11.677

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 453
rank avg (pred): 0.392 +- 0.325
mrr vals (pred, true): 0.149, 0.000
batch losses (mrrl, rdl): 0.0989036933, 0.0001380411

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 614
rank avg (pred): 0.509 +- 0.399
mrr vals (pred, true): 0.092, 0.145
batch losses (mrrl, rdl): 0.0281642657, 0.0005127889

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.442 +- 0.356
mrr vals (pred, true): 0.099, 0.000
batch losses (mrrl, rdl): 0.0239073187, 7.17774e-05

Epoch over!
epoch time: 12.216

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 39
rank avg (pred): 0.264 +- 0.379
mrr vals (pred, true): 0.263, 0.224
batch losses (mrrl, rdl): 0.015261909, 0.0001544508

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1070
rank avg (pred): 0.205 +- 0.347
mrr vals (pred, true): 0.336, 0.325
batch losses (mrrl, rdl): 0.0010514075, 0.0001513362

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 13
rank avg (pred): 0.153 +- 0.294
mrr vals (pred, true): 0.337, 0.362
batch losses (mrrl, rdl): 0.0065185903, 2.95463e-05

Epoch over!
epoch time: 11.997

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 282
rank avg (pred): 0.220 +- 0.309
mrr vals (pred, true): 0.246, 0.218
batch losses (mrrl, rdl): 0.0079738796, 6.17745e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 848
rank avg (pred): 0.460 +- 0.245
mrr vals (pred, true): 0.065, 0.019
batch losses (mrrl, rdl): 0.0022957441, 8.22502e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1043
rank avg (pred): 0.404 +- 0.298
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0267999768, 4.39189e-05

Epoch over!
epoch time: 12.121

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 716
rank avg (pred): 0.406 +- 0.296
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0333836749, 0.0001094374

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 718
rank avg (pred): 0.391 +- 0.268
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0310076959, 9.694e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 735
rank avg (pred): 0.577 +- 0.329
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.0157692824, 4.86481e-05

Epoch over!
epoch time: 12.031

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.215 +- 0.253
mrr vals (pred, true): 0.219, 0.189
batch losses (mrrl, rdl): 0.0089921793, 1.5305e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.132 +- 0.250
mrr vals (pred, true): 0.354, 0.366
batch losses (mrrl, rdl): 0.0013794446, 3.0642e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 983
rank avg (pred): 0.169 +- 0.225
mrr vals (pred, true): 0.281, 0.262
batch losses (mrrl, rdl): 0.0037848139, 2.80266e-05

Epoch over!
epoch time: 12.0

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1110
rank avg (pred): 0.398 +- 0.220
mrr vals (pred, true): 0.079, 0.000
batch losses (mrrl, rdl): 0.008684963, 0.0001885278

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.388 +- 0.227
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.0251248498, 9.05727e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 29
rank avg (pred): 0.201 +- 0.245
mrr vals (pred, true): 0.235, 0.208
batch losses (mrrl, rdl): 0.0072549325, 3.33875e-05

Epoch over!
epoch time: 12.052

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 911
rank avg (pred): 0.550 +- 0.269
mrr vals (pred, true): 0.067, 0.018
batch losses (mrrl, rdl): 0.0030520631, 6.94916e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 710
rank avg (pred): 0.410 +- 0.237
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0274785124, 0.0001964413

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 301
rank avg (pred): 0.184 +- 0.221
mrr vals (pred, true): 0.242, 0.200
batch losses (mrrl, rdl): 0.0182422604, 2.79628e-05

Epoch over!
epoch time: 11.862

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 69
rank avg (pred): 0.174 +- 0.217
mrr vals (pred, true): 0.214, 0.189
batch losses (mrrl, rdl): 0.0066006593, 4.02485e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 744
rank avg (pred): 0.280 +- 0.234
mrr vals (pred, true): 0.141, 0.140
batch losses (mrrl, rdl): 2.4657e-05, 0.00019203

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 145
rank avg (pred): 0.409 +- 0.217
mrr vals (pred, true): 0.097, 0.158
batch losses (mrrl, rdl): 0.0372034349, 0.0002953358

Epoch over!
epoch time: 12.024

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 702
rank avg (pred): 0.405 +- 0.218
mrr vals (pred, true): 0.101, 0.001
batch losses (mrrl, rdl): 0.0259366427, 0.0001680069

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 898
rank avg (pred): 0.648 +- 0.293
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 2.48566e-05, 0.001220779

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 967
rank avg (pred): 0.567 +- 0.258
mrr vals (pred, true): 0.056, 0.000
batch losses (mrrl, rdl): 0.0003381318, 0.0001533749

Epoch over!
epoch time: 12.023

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.386 +- 0.220
mrr vals (pred, true): 0.109, 0.000
batch losses (mrrl, rdl): 0.0349582322, 0.0002152557

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 176
rank avg (pred): 0.389 +- 0.224
mrr vals (pred, true): 0.112, 0.000
batch losses (mrrl, rdl): 0.0383382775, 0.0003371003

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 90
rank avg (pred): 0.393 +- 0.220
mrr vals (pred, true): 0.105, 0.158
batch losses (mrrl, rdl): 0.027812019, 0.0001571442

Epoch over!
epoch time: 12.059

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.364 +- 0.226
mrr vals (pred, true): 0.108, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03941 	 5e-0500 	 m..s
    3 	     1 	 0.04412 	 5e-0500 	 m..s
    2 	     2 	 0.04187 	 6e-0500 	 m..s
   19 	     3 	 0.08945 	 0.00013 	 m..s
   42 	     4 	 0.09433 	 0.00014 	 m..s
   11 	     5 	 0.05080 	 0.00015 	 m..s
   82 	     6 	 0.10441 	 0.00015 	 MISS
    5 	     7 	 0.04480 	 0.00016 	 m..s
   23 	     8 	 0.09111 	 0.00017 	 m..s
    8 	     9 	 0.04729 	 0.00019 	 m..s
   56 	    10 	 0.09675 	 0.00020 	 m..s
   52 	    11 	 0.09585 	 0.00020 	 m..s
   78 	    12 	 0.10005 	 0.00021 	 m..s
    1 	    13 	 0.04095 	 0.00021 	 m..s
   12 	    14 	 0.05096 	 0.00021 	 m..s
   85 	    15 	 0.10674 	 0.00021 	 MISS
    9 	    16 	 0.04751 	 0.00022 	 m..s
   48 	    17 	 0.09499 	 0.00022 	 m..s
   18 	    18 	 0.08888 	 0.00022 	 m..s
   24 	    19 	 0.09115 	 0.00022 	 m..s
   83 	    20 	 0.10484 	 0.00023 	 MISS
   39 	    21 	 0.09396 	 0.00023 	 m..s
   40 	    22 	 0.09404 	 0.00024 	 m..s
   28 	    23 	 0.09223 	 0.00025 	 m..s
    6 	    24 	 0.04492 	 0.00025 	 m..s
   27 	    25 	 0.09209 	 0.00026 	 m..s
   73 	    26 	 0.09917 	 0.00026 	 m..s
   84 	    27 	 0.10513 	 0.00026 	 MISS
   45 	    28 	 0.09480 	 0.00026 	 m..s
   58 	    29 	 0.09701 	 0.00026 	 m..s
   68 	    30 	 0.09783 	 0.00027 	 m..s
   86 	    31 	 0.10723 	 0.00027 	 MISS
   15 	    32 	 0.05632 	 0.00027 	 m..s
   41 	    33 	 0.09423 	 0.00028 	 m..s
   14 	    34 	 0.05247 	 0.00029 	 m..s
   65 	    35 	 0.09771 	 0.00030 	 m..s
   81 	    36 	 0.10208 	 0.00031 	 MISS
   31 	    37 	 0.09241 	 0.00033 	 m..s
   57 	    38 	 0.09686 	 0.00033 	 m..s
   87 	    39 	 0.10808 	 0.00035 	 MISS
   70 	    40 	 0.09860 	 0.00036 	 m..s
   64 	    41 	 0.09759 	 0.00038 	 m..s
   51 	    42 	 0.09559 	 0.00043 	 m..s
   61 	    43 	 0.09732 	 0.00045 	 m..s
   10 	    44 	 0.04977 	 0.00046 	 m..s
   74 	    45 	 0.09923 	 0.00047 	 m..s
   16 	    46 	 0.05778 	 0.00051 	 m..s
   38 	    47 	 0.09278 	 0.00075 	 m..s
   89 	    48 	 0.11379 	 0.00078 	 MISS
   55 	    49 	 0.09668 	 0.00117 	 m..s
   13 	    50 	 0.05226 	 0.00828 	 m..s
    4 	    51 	 0.04436 	 0.00861 	 m..s
    7 	    52 	 0.04655 	 0.00963 	 m..s
   17 	    53 	 0.05951 	 0.03790 	 ~...
   32 	    54 	 0.09267 	 0.10212 	 ~...
   32 	    55 	 0.09267 	 0.10737 	 ~...
   25 	    56 	 0.09118 	 0.10738 	 ~...
   32 	    57 	 0.09267 	 0.11757 	 ~...
   21 	    58 	 0.09035 	 0.12532 	 m..s
   37 	    59 	 0.09277 	 0.12649 	 m..s
   32 	    60 	 0.09267 	 0.12653 	 m..s
   26 	    61 	 0.09167 	 0.12699 	 m..s
   22 	    62 	 0.09048 	 0.12726 	 m..s
   72 	    63 	 0.09908 	 0.12922 	 m..s
   54 	    64 	 0.09653 	 0.13144 	 m..s
   20 	    65 	 0.08948 	 0.13286 	 m..s
   90 	    66 	 0.11554 	 0.13671 	 ~...
   66 	    67 	 0.09776 	 0.14195 	 m..s
   49 	    68 	 0.09502 	 0.14309 	 m..s
   47 	    69 	 0.09499 	 0.14423 	 m..s
   30 	    70 	 0.09235 	 0.14604 	 m..s
   29 	    71 	 0.09230 	 0.14679 	 m..s
   32 	    72 	 0.09267 	 0.15226 	 m..s
   46 	    73 	 0.09486 	 0.15279 	 m..s
   91 	    74 	 0.15717 	 0.15473 	 ~...
   75 	    75 	 0.09938 	 0.15697 	 m..s
   63 	    76 	 0.09737 	 0.15768 	 m..s
   53 	    77 	 0.09634 	 0.15983 	 m..s
   69 	    78 	 0.09808 	 0.16004 	 m..s
   67 	    79 	 0.09778 	 0.16031 	 m..s
   44 	    80 	 0.09476 	 0.16490 	 m..s
   92 	    81 	 0.16260 	 0.16495 	 ~...
   77 	    82 	 0.09965 	 0.16504 	 m..s
   59 	    83 	 0.09710 	 0.16546 	 m..s
   43 	    84 	 0.09457 	 0.16629 	 m..s
   76 	    85 	 0.09943 	 0.16876 	 m..s
   71 	    86 	 0.09862 	 0.17246 	 m..s
   79 	    87 	 0.10167 	 0.17511 	 m..s
   62 	    88 	 0.09735 	 0.17675 	 m..s
   80 	    89 	 0.10183 	 0.17807 	 m..s
   60 	    90 	 0.09712 	 0.17815 	 m..s
   93 	    91 	 0.17282 	 0.19017 	 ~...
   50 	    92 	 0.09550 	 0.19279 	 m..s
   88 	    93 	 0.10825 	 0.19805 	 m..s
   95 	    94 	 0.19732 	 0.19952 	 ~...
   94 	    95 	 0.17715 	 0.20620 	 ~...
   96 	    96 	 0.19851 	 0.21250 	 ~...
   99 	    97 	 0.23801 	 0.21534 	 ~...
  109 	    98 	 0.25913 	 0.23038 	 ~...
   97 	    99 	 0.22098 	 0.24150 	 ~...
   98 	   100 	 0.22674 	 0.24172 	 ~...
  106 	   101 	 0.25349 	 0.24670 	 ~...
  108 	   102 	 0.25574 	 0.25390 	 ~...
  112 	   103 	 0.27738 	 0.25474 	 ~...
  104 	   104 	 0.24534 	 0.26486 	 ~...
  110 	   105 	 0.26114 	 0.26880 	 ~...
  102 	   106 	 0.24055 	 0.27292 	 m..s
  103 	   107 	 0.24529 	 0.27685 	 m..s
  105 	   108 	 0.24542 	 0.27808 	 m..s
  107 	   109 	 0.25486 	 0.27863 	 ~...
  111 	   110 	 0.26925 	 0.28396 	 ~...
  115 	   111 	 0.30148 	 0.28879 	 ~...
  101 	   112 	 0.23945 	 0.28899 	 m..s
  116 	   113 	 0.30302 	 0.29682 	 ~...
  100 	   114 	 0.23893 	 0.31337 	 m..s
  114 	   115 	 0.28217 	 0.31627 	 m..s
  113 	   116 	 0.27788 	 0.32199 	 m..s
  118 	   117 	 0.33563 	 0.33349 	 ~...
  117 	   118 	 0.33436 	 0.33393 	 ~...
  120 	   119 	 0.35297 	 0.37250 	 ~...
  119 	   120 	 0.35206 	 0.39525 	 m..s
==========================================
r_mrr = 0.8393254280090332
r2_mrr = 0.6617954969406128
spearmanr_mrr@5 = 0.8314780592918396
spearmanr_mrr@10 = 0.9257462024688721
spearmanr_mrr@50 = 0.9790605902671814
spearmanr_mrr@100 = 0.8489775657653809
spearmanr_mrr@All = 0.8760269284248352
==========================================
test time: 0.388
Done Testing dataset DBpedia50
total time taken: 185.1469497680664
training time taken: 180.01093745231628
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8393)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6618)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.8315)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9257)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9791)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8490)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8760)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.2871643025973754}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 3189340799894055
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [476, 725, 279, 1212, 672, 468, 136, 87, 1139, 798, 1012, 718, 125, 346, 963, 274, 998, 618, 642, 801, 1123, 460, 708, 695, 740, 448, 86, 855, 1172, 39, 363, 861, 748, 1192, 171, 179, 189, 555, 329, 1027, 903, 217, 784, 742, 882, 233, 670, 1167, 1190, 473, 957, 680, 819, 284, 868, 23, 428, 612, 706, 240, 653, 914, 886, 175, 1135, 1104, 387, 1060, 771, 25, 324, 447, 785, 373, 185, 561, 434, 581, 431, 402, 27, 1151, 57, 536, 354, 501, 248, 783, 715, 107, 262, 1176, 1005, 1021, 101, 797, 49, 24, 730, 55, 1069, 1092, 283, 120, 551, 439, 1018, 356, 304, 121, 1020, 609, 961, 1195, 417, 401, 864, 863, 946, 529, 243]
valid_ids (0): []
train_ids (1094): [826, 344, 176, 955, 908, 590, 1137, 498, 711, 6, 635, 809, 143, 347, 647, 1130, 1207, 518, 859, 1025, 199, 168, 699, 106, 113, 471, 1083, 358, 104, 996, 412, 749, 1096, 720, 629, 66, 948, 641, 1174, 632, 1093, 381, 605, 133, 1183, 654, 818, 896, 28, 1067, 904, 312, 254, 218, 362, 227, 763, 893, 571, 221, 508, 958, 61, 1076, 466, 1082, 506, 1197, 816, 857, 765, 319, 562, 339, 1110, 997, 1024, 63, 511, 799, 585, 366, 731, 315, 210, 391, 70, 42, 625, 902, 991, 1211, 187, 167, 999, 737, 17, 340, 1208, 1053, 602, 645, 844, 65, 219, 122, 482, 1097, 1149, 169, 601, 463, 103, 913, 787, 456, 1187, 1014, 418, 1054, 1180, 172, 823, 528, 1048, 744, 349, 916, 207, 942, 323, 769, 833, 548, 1133, 1152, 1122, 390, 941, 901, 357, 490, 791, 1203, 845, 1040, 286, 541, 228, 622, 159, 610, 132, 1072, 435, 154, 52, 747, 659, 1049, 752, 1147, 751, 735, 922, 258, 364, 375, 962, 422, 293, 1154, 15, 608, 736, 297, 194, 1168, 396, 318, 420, 502, 865, 303, 881, 974, 778, 944, 986, 514, 37, 202, 67, 796, 455, 260, 626, 1036, 91, 496, 152, 880, 1199, 290, 762, 807, 971, 1171, 539, 794, 686, 29, 252, 690, 604, 178, 987, 732, 1071, 1042, 600, 268, 734, 1163, 838, 525, 559, 643, 126, 45, 560, 296, 805, 757, 385, 156, 879, 97, 712, 3, 369, 1001, 117, 166, 513, 36, 99, 376, 619, 848, 546, 73, 1050, 636, 872, 777, 361, 1189, 814, 1131, 575, 205, 570, 867, 452, 945, 332, 827, 287, 370, 979, 11, 704, 328, 223, 151, 313, 789, 956, 874, 241, 321, 83, 884, 69, 728, 1188, 384, 994, 627, 325, 800, 289, 840, 534, 266, 353, 96, 831, 374, 410, 1185, 1162, 445, 912, 141, 267, 270, 90, 1124, 13, 939, 419, 877, 593, 147, 760, 300, 19, 389, 1008, 889, 1209, 582, 953, 427, 1143, 1114, 1205, 758, 1175, 682, 1148, 249, 1103, 140, 825, 1144, 658, 395, 229, 444, 553, 239, 157, 54, 1078, 921, 1173, 927, 907, 691, 583, 843, 1140, 115, 367, 441, 992, 59, 12, 661, 183, 576, 500, 77, 1105, 433, 1142, 246, 713, 954, 111, 1030, 892, 257, 371, 208, 808, 359, 477, 1000, 196, 407, 421, 1108, 469, 1125, 1099, 72, 173, 674, 568, 523, 929, 1145, 1007, 31, 355, 786, 662, 432, 510, 965, 1102, 1010, 898, 360, 38, 972, 937, 835, 984, 891, 550, 545, 648, 552, 723, 191, 184, 85, 453, 1158, 1153, 348, 307, 563, 968, 917, 507, 1029, 212, 928, 493, 1126, 709, 793, 84, 1136, 1196, 253, 1128, 557, 309, 821, 727, 973, 540, 135, 119, 1134, 276, 398, 621, 1041, 533, 594, 337, 926, 788, 430, 35, 377, 530, 679, 237, 842, 930, 705, 683, 660, 1178, 1051, 822, 129, 1061, 726, 782, 403, 484, 1085, 767, 1002, 675, 1202, 301, 1118, 1022, 209, 465, 888, 657, 1016, 853, 564, 897, 651, 124, 885, 467, 1094, 1166, 1015, 98, 1095, 155, 62, 776, 1198, 516, 288, 588, 216, 676, 1181, 775, 646, 255, 900, 1013, 397, 764, 1193, 829, 768, 932, 1056, 515, 556, 160, 1032, 386, 399, 162, 614, 920, 995, 697, 1116, 481, 830, 480, 538, 224, 1037, 198, 193, 285, 81, 1100, 149, 137, 1011, 1146, 624, 543, 21, 940, 895, 710, 755, 203, 74, 770, 620, 446, 580, 128, 1023, 351, 589, 305, 565, 860, 392, 273, 1191, 1017, 1073, 694, 925, 0, 1112, 792, 41, 978, 404, 474, 628, 14, 673, 164, 192, 779, 1129, 1075, 1087, 804, 414, 330, 969, 733, 153, 1028, 109, 806, 180, 756, 1206, 869, 745, 245, 519, 425, 475, 630, 436, 242, 1160, 220, 382, 849, 871, 924, 302, 1089, 894, 668, 333, 990, 1106, 1090, 1081, 472, 43, 190, 1156, 754, 450, 933, 264, 483, 158, 1052, 46, 684, 815, 985, 437, 542, 975, 89, 80, 451, 1165, 750, 1127, 442, 1111, 322, 247, 611, 1066, 34, 130, 852, 949, 1077, 211, 1141, 459, 1026, 48, 547, 139, 909, 637, 1194, 161, 7, 108, 899, 1063, 342, 890, 774, 1138, 341, 280, 841, 458, 314, 102, 1120, 526, 1214, 846, 584, 685, 873, 824, 905, 1003, 298, 537, 1004, 934, 558, 380, 177, 817, 951, 163, 26, 438, 803, 486, 1098, 1046, 429, 423, 531, 544, 138, 617, 811, 967, 1117, 406, 906, 729, 910, 597, 938, 828, 1070, 664, 485, 204, 331, 520, 719, 365, 652, 79, 512, 400, 146, 470, 766, 100, 592, 1047, 292, 616, 408, 82, 966, 724, 114, 281, 1062, 578, 505, 1091, 32, 30, 1065, 277, 649, 116, 492, 613, 1169, 231, 9, 988, 688, 509, 1031, 982, 352, 1159, 1121, 5, 236, 234, 950, 338, 700, 586, 862, 813, 943, 275, 866, 1009, 20, 671, 213, 308, 105, 44, 295, 409, 1034, 772, 75, 1084, 1045, 131, 244, 717, 856, 1044, 294, 935, 1101, 1057, 591, 1170, 47, 810, 457, 850, 174, 640, 878, 1086, 76, 278, 667, 499, 1079, 887, 875, 687, 832, 549, 644, 144, 665, 573, 802, 781, 1119, 854, 993, 915, 56, 677, 424, 638, 393, 93, 454, 596, 195, 759, 698, 977, 372, 1213, 335, 918, 1074, 1055, 837, 88, 123, 820, 413, 443, 186, 150, 22, 1, 656, 1164, 773, 142, 1064, 587, 923, 1038, 595, 368, 68, 282, 440, 663, 693, 110, 976, 148, 1201, 336, 1035, 16, 1150, 981, 188, 464, 53, 847, 1043, 603, 678, 572, 345, 689, 461, 812, 60, 983, 1039, 623, 753, 554, 478, 33, 259, 320, 989, 222, 650, 479, 681, 606, 716, 8, 1107, 487, 1080, 58, 1068, 633, 599, 870, 655, 960, 145, 836, 1200, 834, 911, 95, 462, 201, 1161, 851, 112, 721, 1006, 494, 316, 271, 405, 1177, 64, 181, 504, 134, 1179, 291, 569, 1113, 524, 261, 1088, 722, 92, 118, 883, 235, 1182, 959, 18, 858, 1033, 269, 317, 334, 701, 182, 250, 50, 1059, 78, 327, 1157, 170, 383, 306, 2, 1058, 567, 94, 952, 197, 394, 790, 1019, 527, 350, 489, 615, 256, 707, 272, 936, 532, 503, 714, 1155, 839, 4, 214, 426, 415, 1204, 535, 761, 522, 326, 1115, 311, 488, 449, 343, 165, 692, 40, 127, 225, 743, 666, 795, 411, 741, 964, 703, 491, 1184, 634, 970, 598, 702, 931, 738, 299, 1109, 232, 379, 238, 10, 200, 263, 206, 739, 980, 1132, 497, 746, 378, 388, 574, 310, 607, 566, 517, 251, 1186, 631, 265, 495, 919, 71, 579, 230, 696, 1210, 876, 639, 416, 51, 947, 577, 669, 215, 780, 226, 521]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2341358330888383
the save name prefix for this run is:  chkpt-ID_2341358330888383_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 77
rank avg (pred): 0.478 +- 0.011
mrr vals (pred, true): 0.000, 0.250
batch losses (mrrl, rdl): 0.0, 0.0018298555

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1129
rank avg (pred): 0.360 +- 0.277
mrr vals (pred, true): 0.142, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002155736

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 404
rank avg (pred): 0.360 +- 0.278
mrr vals (pred, true): 0.104, 0.164
batch losses (mrrl, rdl): 0.0, 0.0001091058

Epoch over!
epoch time: 11.879

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 222
rank avg (pred): 0.376 +- 0.271
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 0.0, 8.28452e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.175 +- 0.134
mrr vals (pred, true): 0.108, 0.255
batch losses (mrrl, rdl): 0.0, 6.16054e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 500
rank avg (pred): 0.233 +- 0.180
mrr vals (pred, true): 0.241, 0.288
batch losses (mrrl, rdl): 0.0, 0.0001624145

Epoch over!
epoch time: 11.84

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 938
rank avg (pred): 0.589 +- 0.382
mrr vals (pred, true): 0.140, 0.003
batch losses (mrrl, rdl): 0.0, 2.5254e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1126
rank avg (pred): 0.370 +- 0.285
mrr vals (pred, true): 0.210, 0.003
batch losses (mrrl, rdl): 0.0, 0.0002029958

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1153
rank avg (pred): 0.270 +- 0.209
mrr vals (pred, true): 0.258, 0.209
batch losses (mrrl, rdl): 0.0, 8.04404e-05

Epoch over!
epoch time: 11.928

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1048
rank avg (pred): 0.382 +- 0.296
mrr vals (pred, true): 0.252, 0.000
batch losses (mrrl, rdl): 0.0, 0.0003930899

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 242
rank avg (pred): 0.359 +- 0.280
mrr vals (pred, true): 0.249, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001507782

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1015
rank avg (pred): 0.372 +- 0.288
mrr vals (pred, true): 0.213, 0.159
batch losses (mrrl, rdl): 0.0, 0.0001595008

Epoch over!
epoch time: 11.774

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 316
rank avg (pred): 0.140 +- 0.111
mrr vals (pred, true): 0.276, 0.302
batch losses (mrrl, rdl): 0.0, 4.26772e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 694
rank avg (pred): 0.389 +- 0.289
mrr vals (pred, true): 0.229, 0.001
batch losses (mrrl, rdl): 0.0, 3.01125e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 63
rank avg (pred): 0.157 +- 0.124
mrr vals (pred, true): 0.279, 0.217
batch losses (mrrl, rdl): 0.0, 7.65594e-05

Epoch over!
epoch time: 11.76

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1001
rank avg (pred): 0.356 +- 0.282
mrr vals (pred, true): 0.260, 0.153
batch losses (mrrl, rdl): 0.1136692539, 0.0001115282

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 919
rank avg (pred): 0.581 +- 0.189
mrr vals (pred, true): 0.032, 0.000
batch losses (mrrl, rdl): 0.0031891968, 0.0001888129

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.577 +- 0.195
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 7.2313e-06, 0.0001035586

Epoch over!
epoch time: 11.99

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 942
rank avg (pred): 0.566 +- 0.182
mrr vals (pred, true): 0.034, 0.000
batch losses (mrrl, rdl): 0.0024305847, 0.0019038196

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1057
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.370, 0.366
batch losses (mrrl, rdl): 0.00018781, 0.000398959

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.308, 0.302
batch losses (mrrl, rdl): 0.0004098695, 0.0004300955

Epoch over!
epoch time: 11.866

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 745
rank avg (pred): 0.270 +- 0.190
mrr vals (pred, true): 0.154, 0.118
batch losses (mrrl, rdl): 0.0122684818, 0.000176022

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1053
rank avg (pred): 0.001 +- 0.000
mrr vals (pred, true): 0.362, 0.311
batch losses (mrrl, rdl): 0.0261267368, 0.0004172241

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 503
rank avg (pred): 0.030 +- 0.023
mrr vals (pred, true): 0.278, 0.285
batch losses (mrrl, rdl): 0.0004433515, 0.001372966

Epoch over!
epoch time: 12.015

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 173
rank avg (pred): 0.408 +- 0.227
mrr vals (pred, true): 0.097, 0.000
batch losses (mrrl, rdl): 0.0219287183, 7.92936e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1087
rank avg (pred): 0.413 +- 0.218
mrr vals (pred, true): 0.093, 0.163
batch losses (mrrl, rdl): 0.0493068434, 0.0003892621

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 191
rank avg (pred): 0.419 +- 0.213
mrr vals (pred, true): 0.083, 0.000
batch losses (mrrl, rdl): 0.0109503139, 7.01661e-05

Epoch over!
epoch time: 11.85

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 668
rank avg (pred): 0.403 +- 0.224
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0265863985, 6.36044e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 580
rank avg (pred): 0.401 +- 0.222
mrr vals (pred, true): 0.112, 0.139
batch losses (mrrl, rdl): 0.0074875797, 0.0001241175

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 347
rank avg (pred): 0.394 +- 0.226
mrr vals (pred, true): 0.109, 0.171
batch losses (mrrl, rdl): 0.038180165, 0.00025981

Epoch over!
epoch time: 11.885

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 746
rank avg (pred): 0.342 +- 0.236
mrr vals (pred, true): 0.138, 0.139
batch losses (mrrl, rdl): 1.45745e-05, 0.0004283471

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1148
rank avg (pred): 0.234 +- 0.174
mrr vals (pred, true): 0.253, 0.225
batch losses (mrrl, rdl): 0.0078540659, 0.0001239422

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1078
rank avg (pred): 0.012 +- 0.009
mrr vals (pred, true): 0.286, 0.318
batch losses (mrrl, rdl): 0.0104237162, 0.000388183

Epoch over!
epoch time: 12.03

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1131
rank avg (pred): 0.395 +- 0.211
mrr vals (pred, true): 0.102, 0.001
batch losses (mrrl, rdl): 0.0270928908, 0.0001296418

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1200
rank avg (pred): 0.408 +- 0.198
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0193058718, 0.000268502

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 8
rank avg (pred): 0.003 +- 0.003
mrr vals (pred, true): 0.319, 0.342
batch losses (mrrl, rdl): 0.0052618114, 0.0005915115

Epoch over!
epoch time: 11.783

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.343 +- 0.230
mrr vals (pred, true): 0.183, 0.164
batch losses (mrrl, rdl): 0.0037720983, 0.0001747112

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 936
rank avg (pred): 0.655 +- 0.194
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.88522e-05, 0.0001677742

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 138
rank avg (pred): 0.405 +- 0.207
mrr vals (pred, true): 0.098, 0.142
batch losses (mrrl, rdl): 0.0191025287, 0.0003739601

Epoch over!
epoch time: 11.882

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 838
rank avg (pred): 0.480 +- 0.121
mrr vals (pred, true): 0.049, 0.006
batch losses (mrrl, rdl): 1.74485e-05, 8.90079e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 814
rank avg (pred): 0.453 +- 0.150
mrr vals (pred, true): 0.063, 0.029
batch losses (mrrl, rdl): 0.0016330478, 0.0009656787

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1034
rank avg (pred): 0.391 +- 0.196
mrr vals (pred, true): 0.098, 0.000
batch losses (mrrl, rdl): 0.0234370418, 0.0001381102

Epoch over!
epoch time: 11.84

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.381 +- 0.201
mrr vals (pred, true): 0.101, 0.166
batch losses (mrrl, rdl): 0.0416845009, 0.0002263881

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 560
rank avg (pred): 0.329 +- 0.231
mrr vals (pred, true): 0.202, 0.235
batch losses (mrrl, rdl): 0.0106696319, 8.50496e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 41
rank avg (pred): 0.327 +- 0.234
mrr vals (pred, true): 0.208, 0.242
batch losses (mrrl, rdl): 0.0114408797, 0.0006734168

Epoch over!
epoch time: 11.708

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.391 +- 0.195
mrr vals (pred, true): 0.098, 0.001

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04884 	 0.00013 	 m..s
   11 	     1 	 0.04892 	 0.00015 	 m..s
    5 	     2 	 0.04884 	 0.00016 	 m..s
   41 	     3 	 0.09472 	 0.00016 	 m..s
   39 	     4 	 0.09439 	 0.00016 	 m..s
   57 	     5 	 0.09715 	 0.00017 	 m..s
   69 	     6 	 0.09822 	 0.00018 	 m..s
   70 	     7 	 0.09828 	 0.00018 	 m..s
    7 	     8 	 0.04885 	 0.00018 	 m..s
   53 	     9 	 0.09695 	 0.00018 	 m..s
   29 	    10 	 0.09290 	 0.00018 	 m..s
   81 	    11 	 0.09953 	 0.00018 	 m..s
   35 	    12 	 0.09369 	 0.00019 	 m..s
    2 	    13 	 0.04884 	 0.00019 	 m..s
    6 	    14 	 0.04885 	 0.00019 	 m..s
   59 	    15 	 0.09740 	 0.00019 	 m..s
    3 	    16 	 0.04884 	 0.00019 	 m..s
    9 	    17 	 0.04887 	 0.00020 	 m..s
   82 	    18 	 0.10057 	 0.00020 	 MISS
   22 	    19 	 0.08440 	 0.00020 	 m..s
   13 	    20 	 0.04895 	 0.00020 	 m..s
   71 	    21 	 0.09829 	 0.00020 	 m..s
   54 	    22 	 0.09698 	 0.00021 	 m..s
   12 	    23 	 0.04895 	 0.00021 	 m..s
    8 	    24 	 0.04885 	 0.00022 	 m..s
   32 	    25 	 0.09341 	 0.00022 	 m..s
   87 	    26 	 0.10178 	 0.00022 	 MISS
   37 	    27 	 0.09399 	 0.00023 	 m..s
   24 	    28 	 0.08984 	 0.00023 	 m..s
   50 	    29 	 0.09672 	 0.00025 	 m..s
   45 	    30 	 0.09584 	 0.00026 	 m..s
   15 	    31 	 0.04904 	 0.00026 	 m..s
   44 	    32 	 0.09527 	 0.00027 	 m..s
   63 	    33 	 0.09788 	 0.00028 	 m..s
   25 	    34 	 0.09036 	 0.00031 	 m..s
   74 	    35 	 0.09837 	 0.00032 	 m..s
   55 	    36 	 0.09699 	 0.00033 	 m..s
    1 	    37 	 0.04884 	 0.00034 	 m..s
   79 	    38 	 0.09904 	 0.00038 	 m..s
   42 	    39 	 0.09489 	 0.00040 	 m..s
   88 	    40 	 0.10328 	 0.00048 	 MISS
   89 	    41 	 0.10399 	 0.00050 	 MISS
   61 	    42 	 0.09761 	 0.00051 	 m..s
   83 	    43 	 0.10061 	 0.00058 	 MISS
   72 	    44 	 0.09832 	 0.00069 	 m..s
    0 	    45 	 0.04596 	 0.00091 	 m..s
   46 	    46 	 0.09654 	 0.00093 	 m..s
   85 	    47 	 0.10111 	 0.00096 	 MISS
   10 	    48 	 0.04892 	 0.00118 	 m..s
   43 	    49 	 0.09496 	 0.00206 	 m..s
   17 	    50 	 0.04936 	 0.00823 	 m..s
   18 	    51 	 0.05130 	 0.00828 	 m..s
   14 	    52 	 0.04896 	 0.00907 	 m..s
   19 	    53 	 0.05748 	 0.01728 	 m..s
   20 	    54 	 0.06111 	 0.03967 	 ~...
   16 	    55 	 0.04935 	 0.06656 	 ~...
   86 	    56 	 0.10112 	 0.10268 	 ~...
   51 	    57 	 0.09689 	 0.10313 	 ~...
   52 	    58 	 0.09694 	 0.10738 	 ~...
   28 	    59 	 0.09275 	 0.11647 	 ~...
   36 	    60 	 0.09379 	 0.11876 	 ~...
   64 	    61 	 0.09791 	 0.11903 	 ~...
   49 	    62 	 0.09669 	 0.12922 	 m..s
   78 	    63 	 0.09896 	 0.13617 	 m..s
   47 	    64 	 0.09666 	 0.13636 	 m..s
   76 	    65 	 0.09886 	 0.13668 	 m..s
   21 	    66 	 0.08100 	 0.13800 	 m..s
   34 	    67 	 0.09357 	 0.13898 	 m..s
   58 	    68 	 0.09729 	 0.14021 	 m..s
   56 	    69 	 0.09713 	 0.14031 	 m..s
   33 	    70 	 0.09353 	 0.14170 	 m..s
   60 	    71 	 0.09749 	 0.14414 	 m..s
   92 	    72 	 0.11049 	 0.14416 	 m..s
   48 	    73 	 0.09666 	 0.14679 	 m..s
   77 	    74 	 0.09893 	 0.14730 	 m..s
   27 	    75 	 0.09262 	 0.15086 	 m..s
   75 	    76 	 0.09855 	 0.15579 	 m..s
   68 	    77 	 0.09821 	 0.15705 	 m..s
   97 	    78 	 0.20635 	 0.15838 	 m..s
   66 	    79 	 0.09810 	 0.15909 	 m..s
   26 	    80 	 0.09232 	 0.16031 	 m..s
   98 	    81 	 0.20851 	 0.16258 	 m..s
   65 	    82 	 0.09796 	 0.16302 	 m..s
   91 	    83 	 0.10806 	 0.16508 	 m..s
   31 	    84 	 0.09335 	 0.16513 	 m..s
  103 	    85 	 0.21817 	 0.16644 	 m..s
   23 	    86 	 0.08781 	 0.16744 	 m..s
   38 	    87 	 0.09438 	 0.16800 	 m..s
   73 	    88 	 0.09833 	 0.16902 	 m..s
   40 	    89 	 0.09464 	 0.16927 	 m..s
   84 	    90 	 0.10094 	 0.17096 	 m..s
   62 	    91 	 0.09771 	 0.17246 	 m..s
   94 	    92 	 0.18422 	 0.17275 	 ~...
   93 	    93 	 0.17093 	 0.17576 	 ~...
   90 	    94 	 0.10490 	 0.17611 	 m..s
   30 	    95 	 0.09334 	 0.17675 	 m..s
   67 	    96 	 0.09821 	 0.17815 	 m..s
   80 	    97 	 0.09944 	 0.18120 	 m..s
   96 	    98 	 0.20354 	 0.18855 	 ~...
   95 	    99 	 0.18541 	 0.19185 	 ~...
   99 	   100 	 0.21256 	 0.19825 	 ~...
  101 	   101 	 0.21577 	 0.20604 	 ~...
  100 	   102 	 0.21451 	 0.21437 	 ~...
  102 	   103 	 0.21616 	 0.22441 	 ~...
  104 	   104 	 0.22203 	 0.22873 	 ~...
  105 	   105 	 0.22505 	 0.23511 	 ~...
  107 	   106 	 0.24097 	 0.24231 	 ~...
  106 	   107 	 0.24043 	 0.26095 	 ~...
  111 	   108 	 0.28891 	 0.26486 	 ~...
  109 	   109 	 0.26468 	 0.27412 	 ~...
  110 	   110 	 0.27123 	 0.27975 	 ~...
  108 	   111 	 0.25298 	 0.28805 	 m..s
  112 	   112 	 0.30124 	 0.28877 	 ~...
  113 	   113 	 0.32794 	 0.31887 	 ~...
  114 	   114 	 0.33430 	 0.32932 	 ~...
  117 	   115 	 0.35039 	 0.33731 	 ~...
  119 	   116 	 0.35713 	 0.35940 	 ~...
  118 	   117 	 0.35064 	 0.36280 	 ~...
  120 	   118 	 0.36256 	 0.36320 	 ~...
  115 	   119 	 0.33709 	 0.36337 	 ~...
  116 	   120 	 0.34707 	 0.36435 	 ~...
==========================================
r_mrr = 0.839961588382721
r2_mrr = 0.6615205407142639
spearmanr_mrr@5 = 0.7852212190628052
spearmanr_mrr@10 = 0.9734356999397278
spearmanr_mrr@50 = 0.9558194875717163
spearmanr_mrr@100 = 0.8374167084693909
spearmanr_mrr@All = 0.8684079051017761
==========================================
test time: 0.445
Done Testing dataset DBpedia50
total time taken: 183.34302282333374
training time taken: 178.5371401309967
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8400)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6615)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.7852)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9734)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9558)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8374)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.8684)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.079836991186312}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 8400672867204421
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [374, 1058, 1145, 990, 193, 568, 396, 378, 426, 1154, 264, 818, 729, 88, 840, 130, 657, 695, 105, 371, 851, 859, 514, 157, 1116, 718, 33, 960, 306, 683, 131, 1025, 251, 619, 521, 79, 361, 752, 620, 462, 1171, 154, 236, 68, 273, 643, 202, 248, 419, 920, 969, 171, 135, 569, 697, 530, 473, 909, 638, 805, 1100, 149, 563, 40, 104, 1119, 849, 678, 420, 28, 1207, 1174, 993, 363, 1035, 89, 364, 1080, 8, 1061, 921, 584, 471, 1059, 414, 14, 1094, 976, 961, 791, 1147, 1208, 565, 498, 1107, 444, 702, 50, 933, 132, 962, 449, 467, 1111, 1148, 170, 545, 490, 764, 108, 926, 298, 188, 432, 773, 877, 1045, 65, 576, 1104, 581]
valid_ids (0): []
train_ids (1094): [74, 215, 628, 393, 373, 907, 813, 435, 777, 558, 26, 241, 31, 983, 939, 1105, 185, 716, 137, 503, 84, 536, 272, 277, 213, 945, 36, 586, 801, 63, 77, 892, 222, 1146, 886, 540, 328, 1041, 85, 144, 670, 973, 1087, 3, 51, 1067, 355, 101, 201, 445, 646, 693, 747, 800, 713, 45, 383, 1160, 881, 928, 914, 474, 1167, 1179, 763, 332, 9, 267, 1026, 1126, 602, 996, 817, 138, 1037, 1185, 541, 863, 308, 1036, 946, 366, 664, 382, 1005, 1199, 862, 1144, 134, 952, 145, 727, 225, 239, 571, 81, 487, 506, 564, 303, 1124, 483, 1106, 398, 196, 1130, 370, 807, 512, 493, 684, 410, 949, 866, 237, 532, 345, 738, 865, 288, 400, 221, 1153, 428, 189, 850, 433, 1193, 612, 1162, 523, 744, 526, 1180, 392, 825, 991, 812, 124, 685, 1161, 404, 495, 746, 481, 351, 421, 788, 413, 390, 263, 475, 964, 1093, 275, 1027, 879, 457, 30, 717, 326, 607, 810, 119, 466, 216, 13, 322, 641, 680, 146, 48, 174, 864, 1189, 992, 984, 627, 142, 38, 1170, 1023, 448, 1014, 955, 407, 446, 822, 874, 291, 86, 304, 782, 292, 334, 338, 1169, 671, 311, 197, 625, 293, 1006, 1099, 2, 22, 313, 721, 107, 310, 163, 460, 637, 968, 254, 977, 1200, 944, 917, 787, 1051, 1157, 468, 786, 942, 703, 1008, 184, 577, 461, 574, 299, 422, 258, 233, 724, 387, 1102, 96, 59, 261, 954, 1016, 868, 1182, 172, 114, 870, 1081, 205, 694, 87, 916, 122, 828, 324, 83, 547, 971, 855, 882, 312, 672, 452, 191, 103, 1084, 815, 1214, 270, 7, 479, 965, 836, 901, 280, 780, 700, 1017, 329, 484, 200, 1079, 922, 736, 876, 1003, 318, 340, 488, 601, 330, 230, 268, 406, 1156, 391, 554, 896, 190, 814, 517, 218, 408, 1120, 110, 1086, 1209, 556, 1134, 665, 651, 49, 835, 1138, 12, 994, 1152, 923, 42, 301, 918, 749, 54, 653, 504, 343, 710, 603, 938, 951, 1021, 203, 327, 1108, 911, 379, 244, 931, 177, 112, 284, 679, 295, 1060, 739, 953, 243, 529, 358, 416, 476, 1206, 1151, 783, 871, 41, 346, 1071, 600, 599, 535, 943, 897, 593, 15, 891, 159, 842, 656, 1150, 224, 596, 522, 531, 799, 210, 910, 639, 1089, 803, 765, 302, 1075, 560, 46, 1078, 186, 499, 1128, 957, 320, 769, 24, 1131, 285, 742, 1018, 947, 1048, 442, 903, 102, 394, 1069, 823, 676, 615, 555, 621, 539, 180, 307, 567, 271, 337, 644, 369, 940, 425, 1066, 55, 257, 988, 265, 469, 755, 1049, 353, 411, 809, 819, 199, 1028, 294, 92, 316, 632, 590, 533, 711, 794, 978, 1139, 274, 553, 1177, 21, 315, 1112, 436, 464, 972, 347, 608, 611, 552, 737, 360, 1164, 242, 491, 767, 17, 510, 357, 1072, 551, 956, 127, 349, 1022, 440, 1034, 509, 706, 797, 1204, 659, 654, 1110, 260, 645, 906, 572, 887, 431, 214, 595, 238, 838, 456, 450, 853, 206, 528, 78, 1065, 784, 587, 1095, 37, 1039, 384, 579, 802, 682, 158, 53, 719, 1118, 1201, 662, 772, 402, 898, 262, 278, 489, 147, 970, 751, 959, 152, 336, 1109, 527, 32, 1040, 666, 123, 833, 397, 1178, 543, 1012, 57, 27, 1195, 872, 16, 1155, 919, 908, 168, 515, 635, 562, 290, 894, 289, 1011, 376, 692, 708, 614, 542, 867, 844, 1033, 1085, 93, 1142, 1015, 377, 591, 924, 756, 497, 668, 1068, 129, 350, 578, 561, 1062, 235, 1175, 438, 252, 1097, 259, 121, 1186, 472, 486, 375, 624, 709, 795, 1031, 1053, 516, 255, 401, 1183, 1202, 176, 760, 443, 941, 372, 580, 834, 1159, 1212, 843, 240, 75, 227, 598, 966, 832, 893, 1090, 1042, 689, 231, 141, 1013, 661, 161, 548, 1001, 613, 90, 981, 1057, 649, 1173, 217, 1038, 470, 570, 91, 743, 1127, 155, 854, 730, 1082, 72, 537, 181, 70, 1073, 852, 250, 636, 1181, 309, 701, 839, 754, 913, 1019, 325, 691, 477, 386, 798, 1030, 806, 405, 211, 1136, 796, 133, 811, 113, 35, 998, 1083, 781, 451, 841, 575, 362, 677, 559, 588, 546, 732, 1125, 699, 359, 1121, 557, 365, 333, 1194, 331, 245, 761, 136, 856, 793, 592, 232, 344, 888, 589, 164, 912, 634, 1055, 830, 418, 633, 1122, 974, 424, 549, 858, 502, 118, 1096, 513, 857, 228, 1198, 1172, 905, 195, 895, 1046, 669, 594, 735, 1044, 1000, 1076, 789, 999, 380, 156, 148, 204, 447, 182, 335, 463, 688, 778, 437, 321, 658, 282, 696, 726, 1054, 352, 100, 757, 500, 690, 403, 597, 929, 861, 496, 935, 779, 734, 820, 453, 720, 655, 544, 501, 167, 837, 482, 770, 889, 899, 505, 925, 323, 1140, 846, 173, 44, 1113, 675, 731, 109, 52, 585, 276, 485, 209, 279, 1137, 963, 1088, 1056, 1188, 492, 1043, 616, 162, 674, 1070, 605, 247, 458, 305, 178, 39, 741, 98, 417, 175, 283, 847, 389, 785, 494, 723, 1092, 19, 550, 0, 967, 934, 1010, 704, 524, 663, 423, 1024, 80, 915, 1190, 111, 58, 223, 715, 1184, 745, 606, 23, 975, 890, 354, 816, 1103, 314, 774, 1064, 1047, 395, 1052, 341, 712, 194, 660, 519, 1077, 1114, 705, 883, 821, 986, 234, 1063, 725, 208, 880, 25, 429, 4, 356, 1149, 10, 869, 43, 626, 1135, 1168, 116, 507, 76, 686, 936, 212, 115, 511, 652, 958, 1205, 824, 220, 73, 1196, 792, 140, 179, 286, 18, 827, 459, 1197, 1115, 604, 478, 67, 622, 650, 1117, 995, 687, 1009, 1091, 1032, 388, 885, 1007, 399, 1020, 617, 151, 1141, 296, 904, 348, 1213, 826, 534, 95, 771, 927, 246, 768, 610, 1176, 66, 368, 776, 987, 1187, 1074, 165, 117, 1166, 1002, 520, 566, 1050, 1203, 518, 790, 62, 714, 804, 47, 207, 848, 583, 948, 187, 629, 430, 1004, 845, 748, 56, 989, 1143, 441, 253, 126, 427, 342, 69, 11, 150, 982, 29, 153, 1098, 1029, 219, 873, 667, 623, 950, 82, 61, 120, 609, 125, 269, 642, 573, 166, 900, 762, 385, 454, 775, 1132, 139, 1210, 681, 226, 733, 753, 618, 229, 297, 1192, 409, 1211, 758, 631, 465, 198, 128, 630, 884, 766, 902, 722, 143, 71, 582, 508, 256, 434, 937, 6, 980, 808, 648, 1133, 979, 480, 860, 647, 455, 1191, 249, 94, 183, 1165, 1, 831, 367, 740, 317, 319, 106, 750, 878, 728, 381, 439, 64, 985, 759, 1158, 169, 829, 698, 266, 932, 60, 707, 930, 1101, 673, 640, 415, 1129, 1163, 412, 875, 1123, 339, 281, 97, 5, 300, 538, 20, 287, 160, 192, 997, 34, 99, 525]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8129535089091037
the save name prefix for this run is:  chkpt-ID_8129535089091037_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 383
rank avg (pred): 0.420 +- 0.003
mrr vals (pred, true): 0.000, 0.124
batch losses (mrrl, rdl): 0.0, 0.0003032026

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.189 +- 0.282
mrr vals (pred, true): 0.000, 0.263
batch losses (mrrl, rdl): 0.0, 5.2912e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 888
rank avg (pred): 0.544 +- 0.304
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 5.5152e-06

Epoch over!
epoch time: 11.952

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 425
rank avg (pred): 0.426 +- 0.337
mrr vals (pred, true): 0.002, 0.000
batch losses (mrrl, rdl): 0.0, 9.5025e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 230
rank avg (pred): 0.431 +- 0.326
mrr vals (pred, true): 0.034, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001088026

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 645
rank avg (pred): 0.444 +- 0.336
mrr vals (pred, true): 0.075, 0.139
batch losses (mrrl, rdl): 0.0, 0.0001331133

Epoch over!
epoch time: 11.847

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 418
rank avg (pred): 0.431 +- 0.329
mrr vals (pred, true): 0.060, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001229512

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 930
rank avg (pred): 0.548 +- 0.345
mrr vals (pred, true): 0.105, 0.000
batch losses (mrrl, rdl): 0.0, 0.0018136707

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 183
rank avg (pred): 0.409 +- 0.325
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0, 8.65762e-05

Epoch over!
epoch time: 11.725

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 698
rank avg (pred): 0.415 +- 0.322
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001173243

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1141
rank avg (pred): 0.323 +- 0.316
mrr vals (pred, true): 0.129, 0.265
batch losses (mrrl, rdl): 0.0, 1.91361e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1005
rank avg (pred): 0.420 +- 0.314
mrr vals (pred, true): 0.125, 0.172
batch losses (mrrl, rdl): 0.0, 0.0001961236

Epoch over!
epoch time: 11.614

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 966
rank avg (pred): 0.594 +- 0.347
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001435993

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1008
rank avg (pred): 0.422 +- 0.317
mrr vals (pred, true): 0.098, 0.154
batch losses (mrrl, rdl): 0.0, 0.0001588032

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 830
rank avg (pred): 0.209 +- 0.308
mrr vals (pred, true): 0.192, 0.143
batch losses (mrrl, rdl): 0.0, 5.38086e-05

Epoch over!
epoch time: 11.756

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 73
rank avg (pred): 0.185 +- 0.298
mrr vals (pred, true): 0.190, 0.278
batch losses (mrrl, rdl): 0.0780321062, 6.6303e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 875
rank avg (pred): 0.645 +- 0.328
mrr vals (pred, true): 0.071, 0.000
batch losses (mrrl, rdl): 0.0042127767, 0.0003625845

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1167
rank avg (pred): 0.508 +- 0.349
mrr vals (pred, true): 0.076, 0.138
batch losses (mrrl, rdl): 0.0380437337, 0.000354484

Epoch over!
epoch time: 12.085

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 142
rank avg (pred): 0.455 +- 0.354
mrr vals (pred, true): 0.084, 0.142
batch losses (mrrl, rdl): 0.0336601548, 0.0001585712

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 245
rank avg (pred): 0.125 +- 0.295
mrr vals (pred, true): 0.357, 0.371
batch losses (mrrl, rdl): 0.0020058008, 1.07206e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 896
rank avg (pred): 0.681 +- 0.317
mrr vals (pred, true): 0.076, 0.000
batch losses (mrrl, rdl): 0.0065424568, 0.0006242843

Epoch over!
epoch time: 12.076

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 446
rank avg (pred): 0.463 +- 0.373
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0203271788, 4.18e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1052
rank avg (pred): 0.413 +- 0.352
mrr vals (pred, true): 0.105, 0.000
batch losses (mrrl, rdl): 0.0307938065, 0.0001370435

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.161 +- 0.318
mrr vals (pred, true): 0.326, 0.328
batch losses (mrrl, rdl): 5.46732e-05, 9.8298e-06

Epoch over!
epoch time: 12.01

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 544
rank avg (pred): 0.282 +- 0.330
mrr vals (pred, true): 0.170, 0.187
batch losses (mrrl, rdl): 0.0029125044, 8.722e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 82
rank avg (pred): 0.400 +- 0.326
mrr vals (pred, true): 0.110, 0.142
batch losses (mrrl, rdl): 0.0104789902, 0.0001163232

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 460
rank avg (pred): 0.513 +- 0.392
mrr vals (pred, true): 0.111, 0.000
batch losses (mrrl, rdl): 0.0369557962, 6.44036e-05

Epoch over!
epoch time: 12.043

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 167
rank avg (pred): 0.444 +- 0.348
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.0247203391, 6.79227e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1084
rank avg (pred): 0.319 +- 0.269
mrr vals (pred, true): 0.105, 0.194
batch losses (mrrl, rdl): 0.0783610269, 1.40126e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1117
rank avg (pred): 0.386 +- 0.304
mrr vals (pred, true): 0.099, 0.000
batch losses (mrrl, rdl): 0.0244207457, 0.0001973547

Epoch over!
epoch time: 12.071

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 200
rank avg (pred): 0.299 +- 0.217
mrr vals (pred, true): 0.096, 0.000
batch losses (mrrl, rdl): 0.021063216, 0.0007624857

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 819
rank avg (pred): 0.253 +- 0.352
mrr vals (pred, true): 0.179, 0.144
batch losses (mrrl, rdl): 0.0120408321, 0.0002875541

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 38
rank avg (pred): 0.370 +- 0.431
mrr vals (pred, true): 0.238, 0.239
batch losses (mrrl, rdl): 1.05088e-05, 0.000560951

Epoch over!
epoch time: 12.08

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 259
rank avg (pred): 0.214 +- 0.378
mrr vals (pred, true): 0.341, 0.392
batch losses (mrrl, rdl): 0.0264913142, 0.0002167961

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1193
rank avg (pred): 0.405 +- 0.290
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0190077703, 0.0002590979

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 362
rank avg (pred): 0.471 +- 0.322
mrr vals (pred, true): 0.099, 0.165
batch losses (mrrl, rdl): 0.0430659242, 0.0006490196

Epoch over!
epoch time: 12.05

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1115
rank avg (pred): 0.315 +- 0.286
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0419523939, 0.0002987862

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1062
rank avg (pred): 0.167 +- 0.332
mrr vals (pred, true): 0.290, 0.309
batch losses (mrrl, rdl): 0.0035255381, 7.60137e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 221
rank avg (pred): 0.371 +- 0.321
mrr vals (pred, true): 0.095, 0.000
batch losses (mrrl, rdl): 0.0198090971, 0.0002533371

Epoch over!
epoch time: 11.925

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 611
rank avg (pred): 0.483 +- 0.326
mrr vals (pred, true): 0.083, 0.145
batch losses (mrrl, rdl): 0.038557604, 0.0002658444

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 413
rank avg (pred): 0.346 +- 0.298
mrr vals (pred, true): 0.094, 0.000
batch losses (mrrl, rdl): 0.0193451159, 0.0005959523

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 585
rank avg (pred): 0.467 +- 0.322
mrr vals (pred, true): 0.090, 0.152
batch losses (mrrl, rdl): 0.0381993838, 9.83112e-05

Epoch over!
epoch time: 12.161

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 373
rank avg (pred): 0.401 +- 0.327
mrr vals (pred, true): 0.091, 0.169
batch losses (mrrl, rdl): 0.0612447448, 4.39718e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 190
rank avg (pred): 0.365 +- 0.321
mrr vals (pred, true): 0.110, 0.000
batch losses (mrrl, rdl): 0.0360514, 0.0001587778

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 128
rank avg (pred): 0.370 +- 0.324
mrr vals (pred, true): 0.089, 0.169
batch losses (mrrl, rdl): 0.0633885488, 8.38034e-05

Epoch over!
epoch time: 11.722

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.357 +- 0.315
mrr vals (pred, true): 0.103, 0.179

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04912 	 6e-0500 	 m..s
    2 	     1 	 0.04880 	 7e-0500 	 m..s
   12 	     2 	 0.05618 	 0.00010 	 m..s
    0 	     3 	 0.04874 	 0.00011 	 m..s
    4 	     4 	 0.04889 	 0.00016 	 m..s
   85 	     5 	 0.12264 	 0.00016 	 MISS
    3 	     6 	 0.04888 	 0.00018 	 m..s
    1 	     7 	 0.04874 	 0.00018 	 m..s
    7 	     8 	 0.04910 	 0.00019 	 m..s
   28 	     9 	 0.09293 	 0.00020 	 m..s
   58 	    10 	 0.10484 	 0.00020 	 MISS
   38 	    11 	 0.09713 	 0.00021 	 m..s
   42 	    12 	 0.09948 	 0.00021 	 m..s
   23 	    13 	 0.09166 	 0.00021 	 m..s
   83 	    14 	 0.11727 	 0.00021 	 MISS
   86 	    15 	 0.12339 	 0.00022 	 MISS
    5 	    16 	 0.04894 	 0.00023 	 m..s
   72 	    17 	 0.10856 	 0.00024 	 MISS
   24 	    18 	 0.09193 	 0.00025 	 m..s
   43 	    19 	 0.09953 	 0.00025 	 m..s
   13 	    20 	 0.05959 	 0.00025 	 m..s
   49 	    21 	 0.10329 	 0.00026 	 MISS
   33 	    22 	 0.09467 	 0.00027 	 m..s
   63 	    23 	 0.10525 	 0.00027 	 MISS
   69 	    24 	 0.10696 	 0.00028 	 MISS
   62 	    25 	 0.10519 	 0.00028 	 MISS
   11 	    26 	 0.05393 	 0.00029 	 m..s
   37 	    27 	 0.09699 	 0.00030 	 m..s
   76 	    28 	 0.10966 	 0.00031 	 MISS
   65 	    29 	 0.10545 	 0.00032 	 MISS
   64 	    30 	 0.10529 	 0.00034 	 MISS
   77 	    31 	 0.11168 	 0.00038 	 MISS
   59 	    32 	 0.10492 	 0.00040 	 MISS
   22 	    33 	 0.09121 	 0.00040 	 m..s
   47 	    34 	 0.10270 	 0.00042 	 MISS
   16 	    35 	 0.06020 	 0.00046 	 m..s
   41 	    36 	 0.09926 	 0.00047 	 m..s
   45 	    37 	 0.10018 	 0.00048 	 m..s
   80 	    38 	 0.11247 	 0.00048 	 MISS
   10 	    39 	 0.04913 	 0.00050 	 m..s
   27 	    40 	 0.09273 	 0.00053 	 m..s
    8 	    41 	 0.04911 	 0.00054 	 m..s
   14 	    42 	 0.05977 	 0.00056 	 m..s
    6 	    43 	 0.04909 	 0.00071 	 m..s
   35 	    44 	 0.09657 	 0.00078 	 m..s
   19 	    45 	 0.08918 	 0.00082 	 m..s
   34 	    46 	 0.09641 	 0.00106 	 m..s
   15 	    47 	 0.06008 	 0.00130 	 m..s
   81 	    48 	 0.11379 	 0.00833 	 MISS
   60 	    49 	 0.10497 	 0.02535 	 m..s
   17 	    50 	 0.07433 	 0.02684 	 m..s
   18 	    51 	 0.07614 	 0.02767 	 m..s
   32 	    52 	 0.09348 	 0.11677 	 ~...
   26 	    53 	 0.09261 	 0.11810 	 ~...
   30 	    54 	 0.09327 	 0.12289 	 ~...
   79 	    55 	 0.11247 	 0.12785 	 ~...
   66 	    56 	 0.10579 	 0.13144 	 ~...
   57 	    57 	 0.10431 	 0.13287 	 ~...
   71 	    58 	 0.10754 	 0.13321 	 ~...
   31 	    59 	 0.09344 	 0.13393 	 m..s
   21 	    60 	 0.09084 	 0.13414 	 m..s
   20 	    61 	 0.08992 	 0.13469 	 m..s
   25 	    62 	 0.09202 	 0.13507 	 m..s
   70 	    63 	 0.10735 	 0.13579 	 ~...
   39 	    64 	 0.09883 	 0.13617 	 m..s
   40 	    65 	 0.09915 	 0.13690 	 m..s
   36 	    66 	 0.09667 	 0.13755 	 m..s
   78 	    67 	 0.11171 	 0.13898 	 ~...
   74 	    68 	 0.10921 	 0.14194 	 m..s
   75 	    69 	 0.10950 	 0.14441 	 m..s
   82 	    70 	 0.11464 	 0.14657 	 m..s
   50 	    71 	 0.10330 	 0.14701 	 m..s
   29 	    72 	 0.09295 	 0.14716 	 m..s
   46 	    73 	 0.10226 	 0.15177 	 m..s
   61 	    74 	 0.10514 	 0.15279 	 m..s
   44 	    75 	 0.09974 	 0.15606 	 m..s
   73 	    76 	 0.10913 	 0.15711 	 m..s
   56 	    77 	 0.10424 	 0.15773 	 m..s
   68 	    78 	 0.10634 	 0.15983 	 m..s
  102 	    79 	 0.23864 	 0.16260 	 m..s
   48 	    80 	 0.10329 	 0.16317 	 m..s
   96 	    81 	 0.20936 	 0.16436 	 m..s
   90 	    82 	 0.19570 	 0.16472 	 m..s
   84 	    83 	 0.11782 	 0.16546 	 m..s
   67 	    84 	 0.10613 	 0.16927 	 m..s
   54 	    85 	 0.10351 	 0.16945 	 m..s
   88 	    86 	 0.18110 	 0.17108 	 ~...
   87 	    87 	 0.12538 	 0.17212 	 m..s
   53 	    88 	 0.10345 	 0.17704 	 m..s
   55 	    89 	 0.10372 	 0.17787 	 m..s
   52 	    90 	 0.10343 	 0.17807 	 m..s
   51 	    91 	 0.10336 	 0.17913 	 m..s
   89 	    92 	 0.19482 	 0.17963 	 ~...
   92 	    93 	 0.20286 	 0.19309 	 ~...
   93 	    94 	 0.20416 	 0.19747 	 ~...
   91 	    95 	 0.19935 	 0.20349 	 ~...
   97 	    96 	 0.21114 	 0.20586 	 ~...
   98 	    97 	 0.21680 	 0.21250 	 ~...
   99 	    98 	 0.22020 	 0.21581 	 ~...
  101 	    99 	 0.23074 	 0.21655 	 ~...
   94 	   100 	 0.20454 	 0.22329 	 ~...
   95 	   101 	 0.20793 	 0.22461 	 ~...
  100 	   102 	 0.22380 	 0.22818 	 ~...
  103 	   103 	 0.24224 	 0.22848 	 ~...
  104 	   104 	 0.24442 	 0.23281 	 ~...
  107 	   105 	 0.26579 	 0.24977 	 ~...
  109 	   106 	 0.26669 	 0.25479 	 ~...
  105 	   107 	 0.24535 	 0.25673 	 ~...
  110 	   108 	 0.27050 	 0.27311 	 ~...
  108 	   109 	 0.26651 	 0.27685 	 ~...
  106 	   110 	 0.25519 	 0.28672 	 m..s
  111 	   111 	 0.27372 	 0.30216 	 ~...
  112 	   112 	 0.32097 	 0.31608 	 ~...
  113 	   113 	 0.32875 	 0.34211 	 ~...
  114 	   114 	 0.33228 	 0.34745 	 ~...
  117 	   115 	 0.35006 	 0.35940 	 ~...
  120 	   116 	 0.36010 	 0.36862 	 ~...
  115 	   117 	 0.33931 	 0.37250 	 m..s
  116 	   118 	 0.34858 	 0.37411 	 ~...
  119 	   119 	 0.35664 	 0.38555 	 ~...
  118 	   120 	 0.35458 	 0.39385 	 m..s
==========================================
r_mrr = 0.8648439049720764
r2_mrr = 0.6882607340812683
spearmanr_mrr@5 = 0.9452968239784241
spearmanr_mrr@10 = 0.9342474341392517
spearmanr_mrr@50 = 0.9574957489967346
spearmanr_mrr@100 = 0.8791961073875427
spearmanr_mrr@All = 0.9022259712219238
==========================================
test time: 0.496
Done Testing dataset DBpedia50
total time taken: 184.375883102417
training time taken: 179.6713981628418
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8648)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6883)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9453)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9342)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9575)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.8792)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9022)}}, 'test_loss': {'DistMult': {'DBpedia50': 1.9706716141154175}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 8808177105511363
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [964, 1087, 416, 1099, 657, 90, 684, 663, 817, 1169, 190, 1070, 319, 1111, 1203, 209, 239, 421, 33, 883, 414, 776, 980, 262, 961, 797, 1113, 874, 813, 483, 325, 343, 629, 408, 250, 760, 256, 213, 444, 745, 890, 535, 2, 1018, 237, 541, 151, 1125, 995, 973, 164, 1027, 25, 1119, 88, 895, 638, 635, 38, 192, 245, 484, 290, 649, 1038, 1137, 221, 210, 1127, 215, 795, 167, 1114, 244, 958, 199, 1154, 226, 246, 19, 716, 819, 990, 548, 576, 431, 391, 1120, 1044, 1147, 451, 1045, 118, 89, 1129, 562, 860, 114, 630, 282, 63, 608, 924, 591, 855, 488, 220, 264, 1207, 620, 551, 436, 181, 862, 524, 146, 438, 777, 217, 260, 233]
valid_ids (0): []
train_ids (1094): [191, 570, 276, 85, 969, 1157, 759, 1035, 678, 285, 266, 422, 564, 446, 120, 493, 922, 746, 149, 1002, 1021, 173, 143, 669, 23, 1104, 410, 946, 24, 65, 294, 1164, 1179, 1043, 230, 232, 747, 1067, 454, 404, 393, 1059, 389, 34, 778, 531, 370, 732, 470, 258, 131, 571, 876, 911, 1053, 1153, 102, 1130, 666, 328, 133, 615, 1123, 208, 227, 871, 763, 171, 744, 891, 672, 364, 752, 656, 533, 1210, 450, 506, 566, 914, 1115, 418, 1201, 494, 688, 1, 967, 539, 611, 921, 278, 989, 626, 1167, 1069, 786, 132, 117, 1178, 82, 1071, 257, 160, 129, 715, 322, 793, 312, 751, 485, 448, 851, 694, 1074, 501, 590, 505, 315, 1185, 675, 555, 1131, 648, 559, 104, 987, 428, 296, 400, 437, 378, 1031, 1181, 22, 612, 372, 558, 800, 712, 187, 527, 174, 1191, 497, 920, 298, 942, 81, 710, 1006, 417, 887, 103, 880, 898, 465, 303, 772, 976, 882, 96, 439, 91, 55, 1092, 650, 122, 481, 931, 424, 944, 1116, 354, 701, 981, 405, 196, 134, 1083, 836, 80, 1098, 1054, 1208, 1064, 664, 766, 469, 31, 427, 473, 1174, 7, 324, 479, 538, 1200, 830, 1042, 166, 161, 9, 368, 704, 938, 1065, 326, 721, 953, 1138, 818, 308, 696, 991, 731, 279, 791, 5, 476, 283, 682, 897, 486, 42, 457, 1202, 625, 1148, 634, 782, 547, 299, 660, 1088, 121, 288, 573, 603, 348, 186, 729, 185, 347, 846, 534, 273, 521, 1198, 908, 423, 894, 651, 532, 286, 147, 542, 1205, 610, 775, 201, 277, 821, 11, 472, 951, 1030, 184, 730, 402, 520, 750, 907, 932, 528, 578, 291, 115, 335, 107, 743, 593, 838, 1072, 642, 1101, 384, 126, 842, 927, 163, 893, 1095, 222, 119, 641, 823, 503, 840, 1066, 583, 175, 219, 339, 955, 130, 1041, 1050, 912, 1060, 74, 1106, 100, 810, 30, 464, 796, 736, 837, 902, 1008, 76, 43, 1142, 783, 605, 511, 12, 714, 866, 784, 255, 905, 628, 105, 556, 235, 455, 21, 724, 1193, 875, 655, 892, 833, 67, 646, 1015, 584, 674, 617, 639, 917, 26, 1197, 1012, 755, 433, 318, 193, 904, 975, 691, 329, 690, 557, 144, 827, 240, 203, 1028, 491, 913, 383, 1168, 519, 29, 574, 1204, 361, 498, 373, 73, 1158, 176, 859, 409, 1182, 419, 61, 1062, 1188, 306, 478, 153, 490, 300, 594, 848, 442, 792, 297, 108, 327, 399, 896, 916, 965, 530, 1194, 654, 477, 48, 711, 1105, 720, 474, 394, 362, 514, 801, 901, 139, 977, 6, 374, 28, 1150, 447, 706, 1199, 62, 77, 84, 495, 680, 613, 988, 756, 367, 839, 780, 607, 197, 434, 685, 900, 683, 344, 1033, 764, 158, 986, 540, 1117, 725, 459, 658, 183, 109, 864, 36, 877, 609, 334, 1022, 1102, 1046, 456, 336, 261, 1085, 66, 737, 323, 8, 377, 979, 466, 661, 413, 577, 263, 717, 211, 722, 145, 929, 188, 142, 124, 723, 1047, 218, 739, 934, 407, 482, 906, 753, 86, 71, 886, 333, 928, 568, 57, 668, 254, 1132, 489, 627, 94, 14, 53, 748, 598, 870, 1103, 305, 204, 371, 17, 398, 452, 350, 1016, 395, 676, 1001, 18, 1189, 804, 925, 695, 853, 386, 757, 622, 252, 194, 75, 978, 930, 1165, 808, 467, 1029, 125, 919, 1007, 259, 523, 289, 1146, 487, 1049, 95, 205, 1000, 513, 342, 165, 346, 1160, 652, 923, 420, 910, 758, 1133, 236, 59, 769, 970, 1048, 600, 579, 58, 170, 1186, 356, 858, 552, 353, 845, 637, 589, 596, 1162, 1139, 4, 512, 1052, 966, 618, 238, 453, 604, 352, 1023, 47, 915, 1143, 888, 1079, 1091, 1155, 1020, 251, 549, 950, 304, 265, 35, 1134, 677, 1126, 112, 432, 281, 141, 1024, 401, 844, 247, 69, 623, 726, 993, 854, 168, 234, 621, 380, 412, 1017, 98, 162, 645, 41, 475, 592, 231, 994, 274, 667, 1211, 37, 40, 492, 948, 903, 396, 225, 956, 317, 1176, 1080, 647, 773, 500, 809, 771, 403, 575, 15, 1108, 580, 770, 508, 560, 788, 313, 554, 673, 982, 1081, 572, 1145, 873, 83, 1213, 768, 546, 563, 735, 689, 1141, 155, 0, 799, 206, 180, 207, 458, 1056, 996, 70, 101, 10, 461, 949, 1019, 1151, 767, 522, 381, 172, 831, 754, 806, 807, 1183, 719, 345, 971, 1051, 1036, 865, 449, 765, 229, 516, 275, 952, 429, 614, 27, 707, 60, 309, 1073, 128, 443, 387, 947, 1177, 116, 20, 974, 781, 606, 195, 959, 496, 1003, 960, 659, 1058, 87, 1063, 709, 526, 692, 382, 529, 847, 441, 879, 1195, 228, 662, 1009, 341, 152, 507, 1034, 899, 1109, 242, 624, 565, 631, 1082, 742, 708, 1166, 79, 178, 738, 968, 1097, 1076, 543, 567, 582, 415, 869, 159, 390, 828, 307, 941, 1184, 933, 727, 820, 992, 861, 200, 214, 1144, 284, 700, 1171, 1128, 212, 138, 111, 595, 653, 177, 826, 1214, 1039, 587, 794, 957, 272, 1180, 1055, 665, 311, 599, 97, 863, 440, 72, 445, 602, 687, 1061, 216, 136, 49, 553, 832, 1057, 45, 636, 954, 1196, 198, 733, 517, 52, 1075, 985, 320, 1011, 388, 825, 1156, 113, 1090, 137, 878, 1124, 292, 425, 270, 295, 681, 363, 338, 705, 360, 223, 545, 741, 935, 632, 515, 110, 1192, 44, 156, 616, 1089, 150, 581, 337, 426, 824, 749, 1100, 937, 1206, 812, 1212, 375, 1170, 963, 510, 1037, 586, 644, 271, 406, 1112, 834, 885, 51, 302, 358, 321, 926, 881, 798, 1122, 561, 909, 179, 918, 1078, 1149, 939, 518, 430, 1010, 544, 1121, 502, 287, 940, 157, 1086, 702, 1004, 884, 537, 154, 843, 310, 779, 1163, 135, 850, 829, 349, 202, 728, 703, 679, 268, 1068, 93, 983, 46, 355, 106, 1118, 435, 248, 785, 999, 972, 1135, 822, 867, 1175, 340, 460, 619, 1005, 718, 787, 984, 330, 480, 525, 301, 868, 169, 789, 713, 945, 293, 499, 99, 856, 463, 68, 1161, 997, 774, 585, 39, 601, 790, 734, 1094, 1084, 852, 699, 1209, 314, 811, 369, 462, 504, 1013, 550, 1152, 280, 835, 92, 56, 253, 1136, 332, 802, 698, 267, 1110, 359, 243, 316, 468, 697, 670, 123, 411, 351, 815, 379, 643, 224, 936, 597, 397, 148, 331, 841, 943, 962, 536, 762, 509, 269, 54, 1025, 1096, 249, 998, 693, 385, 1173, 872, 392, 569, 1032, 1014, 140, 1107, 1093, 64, 376, 1026, 1140, 761, 803, 1077, 32, 16, 241, 366, 671, 805, 471, 50, 357, 640, 633, 1187, 3, 127, 1190, 849, 1172, 1159, 189, 182, 78, 13, 889, 365, 857, 686, 1040, 814, 816, 740, 588]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1330777416949727
the save name prefix for this run is:  chkpt-ID_1330777416949727_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1025
rank avg (pred): 0.532 +- 0.001
mrr vals (pred, true): 0.000, 0.160
batch losses (mrrl, rdl): 0.0, 0.0010314493

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1212
rank avg (pred): 0.457 +- 0.243
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 0.0, 2.43935e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 11
rank avg (pred): 0.160 +- 0.104
mrr vals (pred, true): 0.197, 0.349
batch losses (mrrl, rdl): 0.0, 8.87714e-05

Epoch over!
epoch time: 11.987

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 501
rank avg (pred): 0.331 +- 0.215
mrr vals (pred, true): 0.195, 0.288
batch losses (mrrl, rdl): 0.0, 0.0002094862

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 686
rank avg (pred): 0.378 +- 0.256
mrr vals (pred, true): 0.217, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001600658

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 145
rank avg (pred): 0.393 +- 0.263
mrr vals (pred, true): 0.209, 0.158
batch losses (mrrl, rdl): 0.0, 0.0002346993

Epoch over!
epoch time: 11.862

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 708
rank avg (pred): 0.409 +- 0.275
mrr vals (pred, true): 0.212, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001022302

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 267
rank avg (pred): 0.119 +- 0.081
mrr vals (pred, true): 0.242, 0.353
batch losses (mrrl, rdl): 0.0, 4.9451e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 597
rank avg (pred): 0.386 +- 0.261
mrr vals (pred, true): 0.224, 0.150
batch losses (mrrl, rdl): 0.0, 8.64548e-05

Epoch over!
epoch time: 11.636

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 721
rank avg (pred): 0.385 +- 0.261
mrr vals (pred, true): 0.225, 0.000
batch losses (mrrl, rdl): 0.0, 7.68927e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 365
rank avg (pred): 0.391 +- 0.265
mrr vals (pred, true): 0.229, 0.157
batch losses (mrrl, rdl): 0.0, 0.0002164327

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.416 +- 0.250
mrr vals (pred, true): 0.162, 0.147
batch losses (mrrl, rdl): 0.0, 0.0002341787

Epoch over!
epoch time: 11.561

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 395
rank avg (pred): 0.379 +- 0.252
mrr vals (pred, true): 0.220, 0.149
batch losses (mrrl, rdl): 0.0, 0.0001935681

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 112
rank avg (pred): 0.374 +- 0.257
mrr vals (pred, true): 0.223, 0.145
batch losses (mrrl, rdl): 0.0, 0.0001884552

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 375
rank avg (pred): 0.386 +- 0.262
mrr vals (pred, true): 0.229, 0.166
batch losses (mrrl, rdl): 0.0, 0.0002429699

Epoch over!
epoch time: 11.692

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 810
rank avg (pred): 0.163 +- 0.110
mrr vals (pred, true): 0.220, 0.038
batch losses (mrrl, rdl): 0.290234834, 5.80003e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 706
rank avg (pred): 0.410 +- 0.276
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0257217102, 0.0001262851

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 62
rank avg (pred): 0.037 +- 0.029
mrr vals (pred, true): 0.221, 0.193
batch losses (mrrl, rdl): 0.0080639431, 0.0005127036

Epoch over!
epoch time: 12.158

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 333
rank avg (pred): 0.313 +- 0.230
mrr vals (pred, true): 0.116, 0.147
batch losses (mrrl, rdl): 0.010083952, 5.59784e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 323
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.307, 0.255
batch losses (mrrl, rdl): 0.0276528001, 0.0005263875

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 659
rank avg (pred): 0.396 +- 0.273
mrr vals (pred, true): 0.090, 0.000
batch losses (mrrl, rdl): 0.016217405, 7.02836e-05

Epoch over!
epoch time: 11.974

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1049
rank avg (pred): 0.364 +- 0.262
mrr vals (pred, true): 0.109, 0.000
batch losses (mrrl, rdl): 0.034915559, 0.0002303206

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 825
rank avg (pred): 0.123 +- 0.091
mrr vals (pred, true): 0.154, 0.206
batch losses (mrrl, rdl): 0.0277259406, 0.0001258264

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1001
rank avg (pred): 0.450 +- 0.306
mrr vals (pred, true): 0.086, 0.153
batch losses (mrrl, rdl): 0.0447939187, 0.0004723994

Epoch over!
epoch time: 12.088

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1175
rank avg (pred): 0.412 +- 0.279
mrr vals (pred, true): 0.099, 0.139
batch losses (mrrl, rdl): 0.0160234571, 0.00024246

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 374
rank avg (pred): 0.403 +- 0.278
mrr vals (pred, true): 0.105, 0.179
batch losses (mrrl, rdl): 0.0545190871, 0.0002100674

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 157
rank avg (pred): 0.406 +- 0.279
mrr vals (pred, true): 0.104, 0.169
batch losses (mrrl, rdl): 0.0431913808, 0.0002156039

Epoch over!
epoch time: 11.934

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 558
rank avg (pred): 0.056 +- 0.044
mrr vals (pred, true): 0.191, 0.181
batch losses (mrrl, rdl): 0.0009215311, 0.0015287475

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 450
rank avg (pred): 0.416 +- 0.273
mrr vals (pred, true): 0.102, 0.000
batch losses (mrrl, rdl): 0.0272973478, 6.45479e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1006
rank avg (pred): 0.618 +- 0.394
mrr vals (pred, true): 0.092, 0.168
batch losses (mrrl, rdl): 0.0567174293, 0.0019909989

Epoch over!
epoch time: 11.941

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 991
rank avg (pred): 0.007 +- 0.006
mrr vals (pred, true): 0.251, 0.247
batch losses (mrrl, rdl): 0.0002097509, 0.0008717641

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1097
rank avg (pred): 0.367 +- 0.276
mrr vals (pred, true): 0.110, 0.153
batch losses (mrrl, rdl): 0.0186794046, 0.0003102067

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 939
rank avg (pred): 0.866 +- 0.307
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 3e-10, 3.8614e-05

Epoch over!
epoch time: 12.189

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 843
rank avg (pred): 0.900 +- 0.289
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.14796e-05, 0.0032948761

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 925
rank avg (pred): 0.945 +- 0.218
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0011525905, 0.0002582294

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 594
rank avg (pred): 0.406 +- 0.290
mrr vals (pred, true): 0.108, 0.120
batch losses (mrrl, rdl): 0.0012681434, 0.0001232388

Epoch over!
epoch time: 12.044

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 815
rank avg (pred): 0.652 +- 0.395
mrr vals (pred, true): 0.089, 0.032
batch losses (mrrl, rdl): 0.0149858762, 0.0025352798

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 265
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.371, 0.383
batch losses (mrrl, rdl): 0.0015234057, 0.000246464

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 937
rank avg (pred): 0.865 +- 0.307
mrr vals (pred, true): 0.050, 0.008
batch losses (mrrl, rdl): 1.427e-07, 0.0015534409

Epoch over!
epoch time: 11.892

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1083
rank avg (pred): 0.116 +- 0.090
mrr vals (pred, true): 0.143, 0.168
batch losses (mrrl, rdl): 0.0064557856, 0.0006658932

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1053
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.353, 0.311
batch losses (mrrl, rdl): 0.0174046159, 0.0004165538

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 482
rank avg (pred): 0.416 +- 0.285
mrr vals (pred, true): 0.103, 0.000
batch losses (mrrl, rdl): 0.0276326165, 5.81255e-05

Epoch over!
epoch time: 11.965

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 435
rank avg (pred): 0.410 +- 0.285
mrr vals (pred, true): 0.109, 0.001
batch losses (mrrl, rdl): 0.0344439819, 8.9072e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 840
rank avg (pred): 0.924 +- 0.247
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 2.0292e-05, 0.003192402

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 85
rank avg (pred): 0.381 +- 0.271
mrr vals (pred, true): 0.106, 0.143
batch losses (mrrl, rdl): 0.0140123107, 0.0002562277

Epoch over!
epoch time: 11.91

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.880 +- 0.304
mrr vals (pred, true): 0.050, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.05049 	 6e-0500 	 m..s
    0 	     1 	 0.04812 	 0.00010 	 m..s
   24 	     2 	 0.10448 	 0.00015 	 MISS
   21 	     3 	 0.10435 	 0.00015 	 MISS
   80 	     4 	 0.10950 	 0.00016 	 MISS
   24 	     5 	 0.10448 	 0.00017 	 MISS
   24 	     6 	 0.10448 	 0.00017 	 MISS
    4 	     7 	 0.04951 	 0.00018 	 m..s
   19 	     8 	 0.10200 	 0.00018 	 MISS
    2 	     9 	 0.04920 	 0.00019 	 m..s
   24 	    10 	 0.10448 	 0.00020 	 MISS
   24 	    11 	 0.10448 	 0.00020 	 MISS
   78 	    12 	 0.10871 	 0.00020 	 MISS
    6 	    13 	 0.04962 	 0.00020 	 m..s
    5 	    14 	 0.04958 	 0.00021 	 m..s
   73 	    15 	 0.10727 	 0.00021 	 MISS
   24 	    16 	 0.10448 	 0.00021 	 MISS
   76 	    17 	 0.10806 	 0.00021 	 MISS
   24 	    18 	 0.10448 	 0.00021 	 MISS
   24 	    19 	 0.10448 	 0.00021 	 MISS
   24 	    20 	 0.10448 	 0.00021 	 MISS
   10 	    21 	 0.05120 	 0.00022 	 m..s
   72 	    22 	 0.10702 	 0.00022 	 MISS
   24 	    23 	 0.10448 	 0.00023 	 MISS
   24 	    24 	 0.10448 	 0.00023 	 MISS
   24 	    25 	 0.10448 	 0.00023 	 MISS
   20 	    26 	 0.10396 	 0.00023 	 MISS
   24 	    27 	 0.10448 	 0.00024 	 MISS
    1 	    28 	 0.04854 	 0.00024 	 m..s
   24 	    29 	 0.10448 	 0.00024 	 MISS
   24 	    30 	 0.10448 	 0.00025 	 MISS
   24 	    31 	 0.10448 	 0.00026 	 MISS
   24 	    32 	 0.10448 	 0.00026 	 MISS
   24 	    33 	 0.10448 	 0.00029 	 MISS
   24 	    34 	 0.10448 	 0.00030 	 MISS
   24 	    35 	 0.10448 	 0.00031 	 MISS
   77 	    36 	 0.10870 	 0.00031 	 MISS
   82 	    37 	 0.10955 	 0.00031 	 MISS
   24 	    38 	 0.10448 	 0.00031 	 MISS
   24 	    39 	 0.10448 	 0.00032 	 MISS
   24 	    40 	 0.10448 	 0.00034 	 MISS
   12 	    41 	 0.05148 	 0.00034 	 m..s
   24 	    42 	 0.10448 	 0.00036 	 MISS
   74 	    43 	 0.10762 	 0.00036 	 MISS
   24 	    44 	 0.10448 	 0.00038 	 MISS
   24 	    45 	 0.10448 	 0.00038 	 MISS
   24 	    46 	 0.10448 	 0.00038 	 MISS
   24 	    47 	 0.10448 	 0.00039 	 MISS
    7 	    48 	 0.04974 	 0.00044 	 m..s
   24 	    49 	 0.10448 	 0.00046 	 MISS
   79 	    50 	 0.10885 	 0.00048 	 MISS
   84 	    51 	 0.11322 	 0.00048 	 MISS
   15 	    52 	 0.05970 	 0.00060 	 m..s
   24 	    53 	 0.10448 	 0.00062 	 MISS
   24 	    54 	 0.10448 	 0.00063 	 MISS
    3 	    55 	 0.04947 	 0.00068 	 m..s
   23 	    56 	 0.10447 	 0.00082 	 MISS
   24 	    57 	 0.10448 	 0.00109 	 MISS
   86 	    58 	 0.11648 	 0.00122 	 MISS
   13 	    59 	 0.05397 	 0.00201 	 m..s
   14 	    60 	 0.05682 	 0.00823 	 m..s
   11 	    61 	 0.05122 	 0.00907 	 m..s
    8 	    62 	 0.04979 	 0.01885 	 m..s
   17 	    63 	 0.08601 	 0.02558 	 m..s
   16 	    64 	 0.07799 	 0.03525 	 m..s
   24 	    65 	 0.10448 	 0.10737 	 ~...
   24 	    66 	 0.10448 	 0.11677 	 ~...
   87 	    67 	 0.15452 	 0.11849 	 m..s
   69 	    68 	 0.10577 	 0.12919 	 ~...
   22 	    69 	 0.10446 	 0.13469 	 m..s
   24 	    70 	 0.10448 	 0.13507 	 m..s
   24 	    71 	 0.10448 	 0.13558 	 m..s
   24 	    72 	 0.10448 	 0.13874 	 m..s
   18 	    73 	 0.09615 	 0.14037 	 m..s
   90 	    74 	 0.19182 	 0.14103 	 m..s
   71 	    75 	 0.10666 	 0.14194 	 m..s
   88 	    76 	 0.17340 	 0.14416 	 ~...
   24 	    77 	 0.10448 	 0.14495 	 m..s
   70 	    78 	 0.10658 	 0.14657 	 m..s
   24 	    79 	 0.10448 	 0.15226 	 m..s
   24 	    80 	 0.10448 	 0.15436 	 m..s
   24 	    81 	 0.10448 	 0.15541 	 m..s
   24 	    82 	 0.10448 	 0.15565 	 m..s
   81 	    83 	 0.10954 	 0.15819 	 m..s
   24 	    84 	 0.10448 	 0.15920 	 m..s
   24 	    85 	 0.10448 	 0.16031 	 m..s
   83 	    86 	 0.10983 	 0.16345 	 m..s
   24 	    87 	 0.10448 	 0.16562 	 m..s
   75 	    88 	 0.10792 	 0.16857 	 m..s
   94 	    89 	 0.21725 	 0.17790 	 m..s
   89 	    90 	 0.18939 	 0.18464 	 ~...
   91 	    91 	 0.19927 	 0.18855 	 ~...
   97 	    92 	 0.23301 	 0.19309 	 m..s
   95 	    93 	 0.22085 	 0.19747 	 ~...
   85 	    94 	 0.11638 	 0.19805 	 m..s
   92 	    95 	 0.20016 	 0.20208 	 ~...
   93 	    96 	 0.21632 	 0.20569 	 ~...
   96 	    97 	 0.23154 	 0.20586 	 ~...
   98 	    98 	 0.24337 	 0.21750 	 ~...
  100 	    99 	 0.25638 	 0.21755 	 m..s
   99 	   100 	 0.25566 	 0.23948 	 ~...
  103 	   101 	 0.29869 	 0.24533 	 m..s
  101 	   102 	 0.29437 	 0.24977 	 m..s
  102 	   103 	 0.29809 	 0.26194 	 m..s
  104 	   104 	 0.29871 	 0.26570 	 m..s
  105 	   105 	 0.30446 	 0.27990 	 ~...
  111 	   106 	 0.34266 	 0.30207 	 m..s
  108 	   107 	 0.33820 	 0.32253 	 ~...
  115 	   108 	 0.34569 	 0.32533 	 ~...
  109 	   109 	 0.33998 	 0.33393 	 ~...
  110 	   110 	 0.34095 	 0.34184 	 ~...
  106 	   111 	 0.30756 	 0.34285 	 m..s
  118 	   112 	 0.36162 	 0.35024 	 ~...
  114 	   113 	 0.34464 	 0.35127 	 ~...
  117 	   114 	 0.35789 	 0.35648 	 ~...
  107 	   115 	 0.33710 	 0.36096 	 ~...
  120 	   116 	 0.36747 	 0.36320 	 ~...
  116 	   117 	 0.35700 	 0.36337 	 ~...
  112 	   118 	 0.34394 	 0.36720 	 ~...
  119 	   119 	 0.36387 	 0.36862 	 ~...
  113 	   120 	 0.34459 	 0.37125 	 ~...
==========================================
r_mrr = 0.8997100591659546
r2_mrr = 0.6591379642486572
spearmanr_mrr@5 = 0.9956303834915161
spearmanr_mrr@10 = 0.9010329246520996
spearmanr_mrr@50 = 0.9651992917060852
spearmanr_mrr@100 = 0.906384289264679
spearmanr_mrr@All = 0.9173080325126648
==========================================
test time: 0.389
Done Testing dataset DBpedia50
total time taken: 184.52086091041565
training time taken: 179.27559113502502
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8997)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6591)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.9956)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.9010)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9652)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.9064)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9173)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.2616035042883595}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 5089547278025486
Starting TWIG!
Loading datasets
Loading DistMult...
Loading DBpedia50...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [787, 937, 29, 301, 1070, 635, 927, 196, 1115, 525, 300, 312, 736, 928, 889, 864, 298, 43, 52, 350, 1173, 535, 975, 554, 35, 817, 254, 463, 464, 1072, 417, 957, 800, 979, 452, 720, 209, 946, 863, 802, 1027, 398, 728, 1004, 219, 1006, 44, 12, 696, 263, 580, 100, 998, 530, 200, 467, 461, 865, 1016, 793, 965, 485, 1021, 1048, 950, 1212, 241, 949, 477, 476, 259, 275, 658, 792, 724, 1182, 282, 67, 997, 407, 506, 90, 95, 687, 992, 1064, 829, 905, 146, 977, 909, 166, 502, 544, 897, 171, 1189, 1158, 370, 754, 212, 152, 405, 118, 1118, 652, 1053, 679, 164, 1124, 468, 732, 240, 874, 859, 1194, 27, 1161, 1208, 547, 710]
valid_ids (0): []
train_ids (1094): [1008, 30, 264, 1040, 429, 1199, 83, 938, 511, 684, 879, 908, 1096, 1082, 235, 780, 339, 840, 1170, 430, 46, 790, 717, 126, 40, 640, 133, 990, 424, 1176, 986, 344, 584, 685, 92, 379, 285, 872, 267, 329, 594, 1002, 423, 1147, 167, 42, 862, 366, 17, 248, 1139, 626, 674, 1045, 903, 1063, 216, 120, 70, 1140, 1135, 910, 215, 712, 1203, 891, 58, 701, 314, 636, 286, 678, 1132, 1191, 898, 289, 776, 336, 822, 617, 706, 139, 884, 1019, 615, 586, 596, 1069, 165, 994, 272, 278, 1011, 236, 1037, 565, 311, 604, 351, 80, 831, 832, 338, 772, 1138, 522, 875, 1054, 136, 1177, 395, 188, 337, 915, 825, 1155, 1046, 322, 589, 1012, 28, 142, 764, 406, 471, 265, 1166, 99, 688, 116, 1156, 914, 475, 119, 361, 873, 644, 624, 537, 1101, 75, 797, 266, 576, 82, 552, 608, 693, 974, 297, 667, 437, 291, 761, 945, 107, 442, 33, 926, 602, 976, 184, 4, 447, 129, 571, 474, 1157, 354, 15, 1078, 1003, 357, 1030, 819, 1092, 854, 1085, 996, 32, 671, 560, 1091, 1029, 849, 161, 1175, 936, 655, 971, 381, 670, 798, 252, 697, 750, 1071, 203, 543, 597, 871, 402, 848, 568, 93, 513, 1039, 175, 1129, 587, 923, 751, 548, 741, 707, 450, 1133, 1007, 434, 162, 661, 1026, 440, 220, 823, 489, 108, 669, 1162, 987, 230, 1080, 851, 1214, 686, 494, 526, 607, 614, 238, 838, 705, 319, 247, 900, 359, 834, 155, 112, 746, 722, 193, 1009, 585, 813, 618, 555, 642, 677, 1094, 1160, 1035, 833, 714, 572, 1113, 492, 20, 190, 493, 951, 625, 1109, 699, 84, 270, 377, 481, 305, 341, 536, 31, 76, 364, 1130, 1184, 963, 540, 432, 887, 723, 153, 7, 791, 683, 13, 556, 721, 262, 308, 806, 1144, 426, 953, 1081, 61, 244, 783, 211, 1110, 404, 331, 105, 482, 490, 803, 656, 326, 205, 748, 156, 151, 726, 828, 258, 392, 878, 1057, 245, 564, 1017, 747, 738, 961, 389, 170, 1056, 1152, 601, 606, 553, 91, 892, 154, 634, 514, 1127, 869, 98, 342, 982, 866, 826, 113, 520, 901, 456, 1098, 470, 78, 1028, 56, 436, 518, 9, 1159, 824, 969, 427, 1151, 836, 808, 466, 478, 559, 956, 39, 1204, 373, 340, 49, 315, 692, 599, 8, 104, 302, 666, 74, 1117, 399, 804, 268, 630, 645, 1186, 72, 277, 89, 138, 899, 1126, 149, 16, 85, 296, 504, 144, 1076, 242, 307, 411, 130, 169, 1005, 433, 207, 659, 919, 657, 993, 197, 579, 206, 561, 1095, 739, 391, 959, 810, 71, 185, 1051, 1116, 922, 1104, 542, 1202, 11, 182, 766, 400, 1020, 249, 742, 283, 106, 1042, 501, 284, 718, 1099, 1043, 509, 88, 978, 1197, 725, 622, 1145, 348, 360, 558, 852, 676, 1185, 292, 94, 1136, 1024, 79, 893, 1150, 911, 274, 860, 327, 243, 228, 36, 480, 745, 1196, 256, 195, 1108, 439, 566, 1014, 1084, 970, 1163, 346, 163, 512, 650, 942, 664, 310, 168, 773, 68, 631, 369, 794, 421, 737, 1167, 66, 435, 647, 1119, 371, 653, 610, 223, 839, 309, 409, 985, 181, 1210, 469, 173, 455, 251, 306, 958, 21, 947, 1060, 287, 719, 498, 820, 855, 1062, 204, 416, 782, 499, 1088, 59, 397, 811, 96, 174, 713, 1077, 816, 763, 87, 330, 114, 1065, 1206, 966, 313, 731, 1190, 48, 867, 145, 62, 462, 18, 261, 1052, 1105, 562, 288, 532, 964, 807, 821, 883, 488, 1066, 902, 396, 637, 147, 1122, 765, 1154, 529, 385, 744, 131, 178, 229, 2, 378, 122, 609, 269, 38, 941, 708, 47, 743, 510, 595, 449, 621, 60, 524, 1025, 408, 213, 1022, 214, 1038, 293, 110, 1142, 1106, 1174, 629, 913, 519, 1180, 53, 809, 255, 387, 827, 487, 441, 375, 574, 704, 63, 128, 183, 186, 775, 380, 702, 843, 1153, 124, 210, 1183, 343, 534, 6, 1067, 567, 50, 727, 317, 1033, 231, 358, 237, 176, 217, 598, 944, 443, 1079, 202, 638, 208, 904, 955, 557, 25, 5, 1121, 845, 363, 1165, 374, 578, 785, 491, 691, 453, 115, 1141, 352, 1083, 222, 694, 273, 581, 1049, 14, 137, 22, 1172, 844, 316, 0, 225, 103, 189, 425, 10, 460, 523, 299, 495, 246, 479, 393, 592, 451, 973, 730, 484, 668, 778, 279, 716, 888, 508, 324, 1146, 454, 500, 709, 698, 117, 861, 1131, 199, 1086, 734, 984, 1050, 896, 980, 1198, 528, 431, 989, 916, 842, 349, 952, 332, 445, 1125, 224, 295, 880, 675, 158, 1111, 24, 894, 962, 760, 774, 386, 538, 740, 1213, 649, 260, 886, 157, 541, 1181, 895, 132, 715, 603, 383, 179, 907, 935, 77, 496, 280, 777, 700, 912, 682, 1100, 57, 367, 192, 931, 818, 34, 786, 125, 753, 943, 355, 1195, 1089, 1059, 680, 23, 323, 41, 665, 570, 353, 65, 384, 403, 801, 483, 198, 1075, 815, 805, 177, 102, 639, 218, 590, 835, 194, 917, 1207, 1, 444, 1044, 372, 1032, 140, 752, 1087, 465, 613, 1097, 1034, 1102, 660, 633, 616, 846, 939, 1192, 473, 45, 1010, 575, 521, 812, 516, 253, 159, 233, 967, 611, 779, 1178, 1058, 250, 303, 870, 876, 1187, 376, 632, 507, 853, 413, 881, 1164, 232, 1023, 81, 733, 160, 382, 134, 459, 769, 26, 191, 318, 438, 51, 837, 281, 290, 527, 1073, 918, 1090, 755, 428, 988, 294, 1000, 549, 54, 623, 172, 954, 583, 1001, 593, 410, 304, 968, 1068, 847, 588, 73, 1031, 711, 472, 930, 135, 868, 550, 109, 148, 651, 1103, 420, 619, 1201, 226, 781, 858, 789, 345, 187, 127, 995, 681, 690, 69, 768, 1209, 320, 1061, 759, 257, 394, 531, 486, 356, 920, 505, 627, 1211, 1123, 612, 1200, 271, 1137, 933, 448, 762, 150, 960, 757, 857, 663, 458, 551, 422, 758, 885, 546, 628, 227, 703, 457, 569, 515, 830, 1179, 221, 1193, 591, 418, 1047, 662, 934, 111, 850, 412, 1149, 121, 771, 648, 1055, 276, 695, 1143, 890, 201, 333, 1188, 925, 924, 999, 563, 335, 334, 365, 654, 981, 646, 643, 321, 1120, 795, 503, 767, 1205, 770, 388, 37, 1107, 991, 1036, 756, 882, 1171, 419, 673, 641, 972, 1013, 784, 414, 390, 180, 1015, 1128, 735, 796, 689, 1074, 906, 921, 573, 856, 19, 1093, 1041, 347, 814, 1168, 55, 841, 545, 940, 517, 788, 948, 729, 577, 368, 1148, 3, 86, 446, 605, 582, 97, 929, 141, 362, 415, 1018, 1134, 401, 239, 600, 328, 799, 1169, 234, 325, 497, 1114, 101, 983, 749, 1112, 932, 533, 539, 143, 620, 672, 123, 64, 877]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5143361267777201
the save name prefix for this run is:  chkpt-ID_5143361267777201_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'DistMult': {'DBpedia50': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 761
rank avg (pred): 0.516 +- 0.004
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001305451

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 454
rank avg (pred): 0.421 +- 0.275
mrr vals (pred, true): 0.009, 0.000
batch losses (mrrl, rdl): 0.0, 2.95793e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1171
rank avg (pred): 0.452 +- 0.327
mrr vals (pred, true): 0.050, 0.118
batch losses (mrrl, rdl): 0.0, 0.0002279157

Epoch over!
epoch time: 11.91

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 72
rank avg (pred): 0.160 +- 0.124
mrr vals (pred, true): 0.140, 0.263
batch losses (mrrl, rdl): 0.0, 6.40606e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 549
rank avg (pred): 0.294 +- 0.232
mrr vals (pred, true): 0.115, 0.175
batch losses (mrrl, rdl): 0.0, 8.22015e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 674
rank avg (pred): 0.494 +- 0.335
mrr vals (pred, true): 0.016, 0.000
batch losses (mrrl, rdl): 0.0, 6.64825e-05

Epoch over!
epoch time: 11.856

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 225
rank avg (pred): 0.345 +- 0.273
mrr vals (pred, true): 0.120, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002002222

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 852
rank avg (pred): 0.457 +- 0.346
mrr vals (pred, true): 0.074, 0.008
batch losses (mrrl, rdl): 0.0, 0.0003677666

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 441
rank avg (pred): 0.362 +- 0.295
mrr vals (pred, true): 0.290, 0.000
batch losses (mrrl, rdl): 0.0, 0.0002491869

Epoch over!
epoch time: 12.018

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 560
rank avg (pred): 0.300 +- 0.242
mrr vals (pred, true): 0.301, 0.235
batch losses (mrrl, rdl): 0.0, 5.23885e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 513
rank avg (pred): 0.285 +- 0.228
mrr vals (pred, true): 0.180, 0.144
batch losses (mrrl, rdl): 0.0, 6.09353e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 397
rank avg (pred): 0.360 +- 0.285
mrr vals (pred, true): 0.222, 0.158
batch losses (mrrl, rdl): 0.0, 0.0001239034

Epoch over!
epoch time: 11.857

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1180
rank avg (pred): 0.363 +- 0.292
mrr vals (pred, true): 0.148, 0.125
batch losses (mrrl, rdl): 0.0, 2.94261e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 980
rank avg (pred): 0.164 +- 0.133
mrr vals (pred, true): 0.157, 0.356
batch losses (mrrl, rdl): 0.0, 9.45132e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1125
rank avg (pred): 0.350 +- 0.283
mrr vals (pred, true): 0.238, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001265978

Epoch over!
epoch time: 11.93

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 973
rank avg (pred): 0.165 +- 0.135
mrr vals (pred, true): 0.246, 0.302
batch losses (mrrl, rdl): 0.0311098211, 7.52839e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 353
rank avg (pred): 0.430 +- 0.298
mrr vals (pred, true): 0.109, 0.129
batch losses (mrrl, rdl): 0.0039380607, 0.000263117

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 7
rank avg (pred): 0.002 +- 0.001
mrr vals (pred, true): 0.306, 0.328
batch losses (mrrl, rdl): 0.0046404549, 0.0005568912

Epoch over!
epoch time: 12.173

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 799
rank avg (pred): 0.572 +- 0.374
mrr vals (pred, true): 0.074, 0.001
batch losses (mrrl, rdl): 0.0060017044, 0.0002604723

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 256
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.372, 0.367
batch losses (mrrl, rdl): 0.000259275, 0.0002693472

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 183
rank avg (pred): 0.424 +- 0.298
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0258369446, 1.41608e-05

Epoch over!
epoch time: 11.898

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 551
rank avg (pred): 0.129 +- 0.097
mrr vals (pred, true): 0.153, 0.189
batch losses (mrrl, rdl): 0.0124350237, 0.0006049079

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 824
rank avg (pred): 0.216 +- 0.158
mrr vals (pred, true): 0.129, 0.171
batch losses (mrrl, rdl): 0.0178036839, 0.0001607334

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 639
rank avg (pred): 0.465 +- 0.267
mrr vals (pred, true): 0.079, 0.155
batch losses (mrrl, rdl): 0.0578892417, 0.000146046

Epoch over!
epoch time: 12.028

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 622
rank avg (pred): 0.469 +- 0.266
mrr vals (pred, true): 0.098, 0.118
batch losses (mrrl, rdl): 0.0038813544, 0.0002108174

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 902
rank avg (pred): 0.453 +- 0.266
mrr vals (pred, true): 0.054, 0.026
batch losses (mrrl, rdl): 0.0001691557, 0.0001248352

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 328
rank avg (pred): 0.420 +- 0.288
mrr vals (pred, true): 0.114, 0.136
batch losses (mrrl, rdl): 0.0048187673, 0.0003265612

Epoch over!
epoch time: 12.264

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 540
rank avg (pred): 0.214 +- 0.158
mrr vals (pred, true): 0.167, 0.164
batch losses (mrrl, rdl): 0.0001314704, 0.0001600226

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 620
rank avg (pred): 0.429 +- 0.234
mrr vals (pred, true): 0.090, 0.117
batch losses (mrrl, rdl): 0.0071099889, 0.0001449287

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 220
rank avg (pred): 0.420 +- 0.227
mrr vals (pred, true): 0.100, 0.000
batch losses (mrrl, rdl): 0.0251035597, 3.34963e-05

Epoch over!
epoch time: 11.933

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 674
rank avg (pred): 0.410 +- 0.251
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0340131149, 3.55264e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 390
rank avg (pred): 0.414 +- 0.230
mrr vals (pred, true): 0.102, 0.164
batch losses (mrrl, rdl): 0.0383839607, 0.0002751013

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 234
rank avg (pred): 0.400 +- 0.225
mrr vals (pred, true): 0.101, 0.000
batch losses (mrrl, rdl): 0.0261204056, 7.38891e-05

Epoch over!
epoch time: 12.063

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 894
rank avg (pred): 0.470 +- 0.248
mrr vals (pred, true): 0.064, 0.000
batch losses (mrrl, rdl): 0.0019825562, 0.0021526169

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 556
rank avg (pred): 0.361 +- 0.270
mrr vals (pred, true): 0.182, 0.165
batch losses (mrrl, rdl): 0.0029692079, 0.0001822978

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1108
rank avg (pred): 0.397 +- 0.294
mrr vals (pred, true): 0.115, 0.000
batch losses (mrrl, rdl): 0.0423673354, 5.42752e-05

Epoch over!
epoch time: 12.044

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 587
rank avg (pred): 0.420 +- 0.232
mrr vals (pred, true): 0.099, 0.146
batch losses (mrrl, rdl): 0.0219998099, 0.000117213

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 367
rank avg (pred): 0.358 +- 0.213
mrr vals (pred, true): 0.110, 0.158
batch losses (mrrl, rdl): 0.0235787183, 0.0003039072

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 18
rank avg (pred): 0.157 +- 0.160
mrr vals (pred, true): 0.342, 0.332
batch losses (mrrl, rdl): 0.0010596747, 6.31002e-05

Epoch over!
epoch time: 11.769

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 1040
rank avg (pred): 0.351 +- 0.232
mrr vals (pred, true): 0.082, 0.000
batch losses (mrrl, rdl): 0.0102949832, 0.0003491995

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 21
rank avg (pred): 0.108 +- 0.111
mrr vals (pred, true): 0.331, 0.323
batch losses (mrrl, rdl): 0.0006030949, 5.03721e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 960
rank avg (pred): 0.574 +- 0.273
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002383789, 0.0001243604

Epoch over!
epoch time: 11.856

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 791
rank avg (pred): 0.484 +- 0.289
mrr vals (pred, true): 0.071, 0.000
batch losses (mrrl, rdl): 0.0045672678, 3.50858e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 871
rank avg (pred): 0.496 +- 0.292
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004663627, 1.0192e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, DBpedia50, run 2.1, exp 572
rank avg (pred): 0.380 +- 0.224
mrr vals (pred, true): 0.108, 0.135
batch losses (mrrl, rdl): 0.0070642144, 0.000121103

Epoch over!
epoch time: 11.675

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG DBpedia50
Running eval on the test set
running batch: 0
rank avg (pred): 0.498 +- 0.293
mrr vals (pred, true): 0.063, 0.000

Evaluation for DBpedia50 on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.06640 	 5e-0500 	 m..s
    0 	     1 	 0.06296 	 0.00013 	 m..s
    0 	     2 	 0.06296 	 0.00013 	 m..s
   49 	     3 	 0.09985 	 0.00014 	 m..s
   62 	     4 	 0.10416 	 0.00014 	 MISS
   19 	     5 	 0.07100 	 0.00015 	 m..s
    0 	     6 	 0.06296 	 0.00016 	 m..s
   75 	     7 	 0.10856 	 0.00017 	 MISS
   55 	     8 	 0.10164 	 0.00017 	 MISS
   30 	     9 	 0.09694 	 0.00018 	 m..s
    0 	    10 	 0.06296 	 0.00018 	 m..s
   24 	    11 	 0.09395 	 0.00018 	 m..s
   68 	    12 	 0.10571 	 0.00018 	 MISS
   43 	    13 	 0.09874 	 0.00018 	 m..s
   40 	    14 	 0.09832 	 0.00018 	 m..s
   81 	    15 	 0.11686 	 0.00018 	 MISS
   35 	    16 	 0.09770 	 0.00019 	 m..s
    0 	    17 	 0.06296 	 0.00019 	 m..s
   76 	    18 	 0.10878 	 0.00019 	 MISS
   70 	    19 	 0.10585 	 0.00020 	 MISS
   65 	    20 	 0.10492 	 0.00020 	 MISS
   41 	    21 	 0.09848 	 0.00020 	 m..s
   77 	    22 	 0.11133 	 0.00021 	 MISS
   29 	    23 	 0.09677 	 0.00021 	 m..s
   56 	    24 	 0.10165 	 0.00021 	 MISS
   22 	    25 	 0.08473 	 0.00022 	 m..s
   46 	    26 	 0.09925 	 0.00022 	 m..s
   20 	    27 	 0.07122 	 0.00022 	 m..s
   16 	    28 	 0.06438 	 0.00023 	 m..s
   27 	    29 	 0.09624 	 0.00023 	 m..s
   34 	    30 	 0.09760 	 0.00023 	 m..s
   33 	    31 	 0.09743 	 0.00025 	 m..s
   45 	    32 	 0.09895 	 0.00026 	 m..s
   39 	    33 	 0.09814 	 0.00026 	 m..s
   66 	    34 	 0.10523 	 0.00028 	 MISS
   28 	    35 	 0.09668 	 0.00030 	 m..s
   71 	    36 	 0.10725 	 0.00030 	 MISS
    0 	    37 	 0.06296 	 0.00031 	 m..s
   54 	    38 	 0.10148 	 0.00032 	 MISS
   57 	    39 	 0.10195 	 0.00034 	 MISS
   79 	    40 	 0.11388 	 0.00035 	 MISS
   36 	    41 	 0.09785 	 0.00036 	 m..s
   37 	    42 	 0.09790 	 0.00040 	 m..s
   74 	    43 	 0.10849 	 0.00042 	 MISS
   14 	    44 	 0.06414 	 0.00042 	 m..s
   52 	    45 	 0.10064 	 0.00043 	 MISS
   80 	    46 	 0.11572 	 0.00043 	 MISS
   51 	    47 	 0.10022 	 0.00048 	 m..s
   59 	    48 	 0.10239 	 0.00051 	 MISS
    0 	    49 	 0.06296 	 0.00056 	 m..s
   53 	    50 	 0.10131 	 0.00058 	 MISS
   64 	    51 	 0.10435 	 0.00075 	 MISS
    0 	    52 	 0.06296 	 0.00091 	 m..s
    0 	    53 	 0.06296 	 0.00119 	 m..s
   18 	    54 	 0.06993 	 0.00130 	 m..s
   15 	    55 	 0.06421 	 0.00143 	 m..s
    0 	    56 	 0.06296 	 0.00150 	 m..s
    0 	    57 	 0.06296 	 0.00375 	 m..s
   44 	    58 	 0.09882 	 0.00441 	 m..s
   23 	    59 	 0.08670 	 0.00504 	 m..s
   21 	    60 	 0.07360 	 0.00828 	 m..s
    0 	    61 	 0.06296 	 0.00845 	 m..s
   38 	    62 	 0.09814 	 0.02558 	 m..s
   13 	    63 	 0.06373 	 0.02684 	 m..s
    0 	    64 	 0.06296 	 0.02689 	 m..s
   58 	    65 	 0.10232 	 0.11403 	 ~...
   83 	    66 	 0.12098 	 0.11626 	 ~...
   84 	    67 	 0.16570 	 0.13345 	 m..s
   31 	    68 	 0.09728 	 0.13558 	 m..s
   73 	    69 	 0.10746 	 0.13683 	 ~...
   26 	    70 	 0.09622 	 0.13893 	 m..s
   63 	    71 	 0.10432 	 0.13896 	 m..s
   48 	    72 	 0.09978 	 0.14137 	 m..s
   89 	    73 	 0.21364 	 0.14177 	 m..s
   25 	    74 	 0.09618 	 0.14665 	 m..s
   67 	    75 	 0.10528 	 0.14765 	 m..s
   61 	    76 	 0.10300 	 0.15324 	 m..s
   32 	    77 	 0.09731 	 0.15436 	 m..s
   78 	    78 	 0.11251 	 0.15574 	 m..s
   99 	    79 	 0.23034 	 0.15812 	 m..s
   69 	    80 	 0.10577 	 0.15819 	 m..s
   82 	    81 	 0.11949 	 0.15883 	 m..s
   87 	    82 	 0.19314 	 0.15951 	 m..s
   88 	    83 	 0.19944 	 0.16258 	 m..s
   42 	    84 	 0.09858 	 0.16562 	 m..s
   60 	    85 	 0.10243 	 0.16753 	 m..s
   86 	    86 	 0.18969 	 0.17470 	 ~...
   50 	    87 	 0.10016 	 0.17675 	 m..s
   90 	    88 	 0.21598 	 0.17790 	 m..s
  100 	    89 	 0.23275 	 0.18205 	 m..s
   85 	    90 	 0.18499 	 0.18736 	 ~...
   47 	    91 	 0.09938 	 0.19208 	 m..s
   91 	    92 	 0.21626 	 0.19467 	 ~...
   94 	    93 	 0.22166 	 0.19955 	 ~...
   72 	    94 	 0.10725 	 0.19965 	 m..s
   95 	    95 	 0.22323 	 0.20349 	 ~...
   98 	    96 	 0.22770 	 0.20496 	 ~...
   96 	    97 	 0.22476 	 0.20837 	 ~...
  101 	    98 	 0.23785 	 0.21432 	 ~...
  103 	    99 	 0.25131 	 0.21534 	 m..s
   97 	   100 	 0.22511 	 0.21755 	 ~...
   92 	   101 	 0.21962 	 0.22848 	 ~...
  102 	   102 	 0.24305 	 0.24229 	 ~...
   93 	   103 	 0.22086 	 0.25950 	 m..s
  104 	   104 	 0.25659 	 0.26510 	 ~...
  105 	   105 	 0.28322 	 0.27050 	 ~...
  109 	   106 	 0.30875 	 0.27314 	 m..s
  106 	   107 	 0.30377 	 0.27333 	 m..s
  107 	   108 	 0.30378 	 0.27466 	 ~...
  110 	   109 	 0.31412 	 0.27575 	 m..s
  108 	   110 	 0.30673 	 0.28877 	 ~...
  111 	   111 	 0.32292 	 0.30005 	 ~...
  120 	   112 	 0.36691 	 0.31099 	 m..s
  115 	   113 	 0.34145 	 0.32533 	 ~...
  114 	   114 	 0.34100 	 0.32678 	 ~...
  112 	   115 	 0.33644 	 0.36372 	 ~...
  113 	   116 	 0.33975 	 0.36729 	 ~...
  116 	   117 	 0.34351 	 0.36809 	 ~...
  118 	   118 	 0.36314 	 0.37167 	 ~...
  117 	   119 	 0.35915 	 0.39249 	 m..s
  119 	   120 	 0.36539 	 0.39525 	 ~...
==========================================
r_mrr = 0.8996158838272095
r2_mrr = 0.6485785245895386
spearmanr_mrr@5 = 0.69953453540802
spearmanr_mrr@10 = 0.8991878628730774
spearmanr_mrr@50 = 0.951080858707428
spearmanr_mrr@100 = 0.9190323948860168
spearmanr_mrr@All = 0.9316400289535522
==========================================
test time: 0.392
Done Testing dataset DBpedia50
total time taken: 184.4272119998932
training time taken: 179.727445602417
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'DBpedia50': tensor(0.8996)}}, 'r2_mrr': {'DistMult': {'DBpedia50': tensor(0.6486)}}, 'spearmanr_mrr@5': {'DistMult': {'DBpedia50': tensor(0.6995)}}, 'spearmanr_mrr@10': {'DistMult': {'DBpedia50': tensor(0.8992)}}, 'spearmanr_mrr@50': {'DistMult': {'DBpedia50': tensor(0.9511)}}, 'spearmanr_mrr@100': {'DistMult': {'DBpedia50': tensor(0.9190)}}, 'spearmanr_mrr@All': {'DistMult': {'DBpedia50': tensor(0.9316)}}, 'test_loss': {'DistMult': {'DBpedia50': 2.1510254262102535}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 6439349733452592
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1115, 72, 489, 123, 780, 1133, 210, 1157, 831, 343, 192, 488, 872, 898, 274, 233, 736, 185, 29, 1177, 27, 885, 527, 33, 1087, 77, 868, 773, 995, 615, 850, 795, 818, 589, 522, 292, 925, 289, 573, 679, 973, 716, 713, 953, 971, 86, 164, 804, 314, 769, 1207, 17, 845, 1070, 1111, 191, 892, 665, 869, 179, 173, 1125, 705, 586, 680, 59, 268, 658, 1052, 864, 266, 1180, 767, 372, 799, 766, 1167, 67, 440, 649, 512, 463, 98, 295, 690, 190, 419, 281, 371, 442, 1056, 778, 65, 1124, 836, 1026, 265, 968, 507, 1140, 770, 249, 1050, 927, 46, 443, 1109, 1119, 808, 193, 553, 745, 1162, 975, 961, 917, 146, 1104, 1079, 722, 206]
valid_ids (0): []
train_ids (1094): [879, 866, 979, 653, 822, 636, 223, 1176, 1078, 834, 893, 151, 1200, 843, 92, 1066, 1130, 163, 1048, 558, 307, 1151, 39, 308, 108, 1016, 613, 83, 207, 538, 226, 221, 525, 1040, 88, 511, 1170, 135, 635, 1196, 122, 637, 389, 8, 776, 994, 639, 1146, 838, 115, 858, 403, 102, 552, 1181, 227, 729, 31, 1193, 378, 675, 867, 760, 1108, 177, 883, 1059, 454, 940, 165, 172, 283, 280, 313, 741, 820, 812, 789, 580, 242, 742, 677, 293, 980, 697, 394, 1159, 462, 490, 473, 505, 168, 340, 1047, 60, 1010, 365, 807, 588, 514, 286, 457, 1043, 315, 335, 258, 1017, 771, 988, 810, 777, 93, 417, 833, 536, 646, 1148, 398, 918, 1019, 100, 156, 548, 1014, 330, 161, 4, 1120, 861, 218, 182, 708, 853, 881, 1074, 285, 90, 1145, 1093, 414, 13, 328, 563, 184, 12, 565, 1055, 683, 784, 544, 621, 1039, 513, 517, 1057, 821, 309, 425, 167, 628, 663, 461, 215, 384, 523, 617, 64, 119, 147, 499, 668, 129, 400, 768, 32, 520, 718, 269, 578, 865, 254, 609, 974, 938, 397, 1175, 178, 405, 891, 1063, 797, 592, 754, 300, 1173, 312, 1185, 727, 350, 737, 992, 468, 390, 802, 319, 700, 627, 890, 556, 759, 691, 847, 45, 500, 744, 188, 684, 855, 431, 696, 753, 469, 775, 707, 1204, 44, 346, 51, 68, 368, 35, 271, 751, 96, 337, 862, 570, 367, 75, 608, 48, 395, 1008, 1073, 1123, 555, 1187, 385, 595, 248, 125, 1003, 447, 446, 287, 201, 998, 1006, 1144, 396, 124, 23, 967, 870, 426, 430, 428, 1156, 1032, 964, 983, 260, 1113, 393, 509, 943, 1077, 476, 839, 1172, 878, 453, 689, 720, 515, 25, 519, 765, 1142, 912, 1129, 761, 1199, 342, 949, 894, 101, 84, 450, 857, 937, 120, 416, 990, 671, 19, 590, 99, 966, 341, 415, 603, 977, 97, 171, 1081, 582, 9, 798, 692, 758, 533, 535, 436, 612, 852, 1054, 740, 1049, 1161, 1117, 638, 817, 209, 919, 1021, 541, 321, 434, 301, 537, 1209, 47, 1097, 1110, 1206, 939, 1174, 481, 131, 657, 724, 1096, 130, 1099, 1062, 1205, 1023, 1041, 726, 10, 1138, 53, 251, 1082, 1031, 574, 711, 986, 728, 731, 1083, 94, 222, 911, 1154, 375, 437, 352, 934, 1136, 28, 38, 152, 1195, 267, 903, 126, 503, 554, 928, 116, 264, 1036, 682, 965, 924, 856, 1095, 316, 475, 877, 985, 485, 549, 1131, 219, 651, 144, 634, 1210, 380, 166, 464, 1089, 725, 85, 569, 545, 596, 204, 568, 246, 1211, 876, 946, 757, 670, 1197, 1007, 357, 229, 234, 236, 501, 1164, 829, 104, 160, 1212, 214, 55, 333, 497, 551, 685, 791, 42, 957, 550, 14, 150, 63, 626, 752, 873, 1072, 654, 200, 618, 250, 591, 305, 1208, 747, 909, 1076, 87, 408, 1135, 564, 175, 622, 26, 978, 370, 95, 299, 231, 1139, 460, 1002, 56, 467, 411, 155, 240, 216, 243, 521, 133, 401, 73, 910, 1137, 921, 194, 253, 1046, 619, 926, 1033, 471, 423, 1000, 908, 805, 606, 455, 277, 382, 715, 801, 955, 960, 1068, 297, 859, 273, 294, 279, 374, 976, 623, 958, 792, 991, 734, 762, 34, 1100, 851, 772, 424, 811, 730, 76, 224, 458, 733, 688, 41, 298, 860, 334, 763, 470, 593, 914, 1203, 930, 114, 449, 399, 54, 79, 602, 571, 954, 902, 1, 291, 387, 945, 208, 647, 486, 1012, 504, 972, 1042, 1128, 1061, 52, 969, 656, 183, 493, 1116, 275, 498, 91, 388, 435, 413, 931, 539, 756, 629, 176, 1134, 153, 899, 364, 502, 577, 1213, 1192, 16, 529, 1155, 71, 296, 228, 37, 452, 981, 840, 562, 534, 532, 783, 186, 997, 290, 239, 1127, 559, 377, 721, 211, 941, 630, 1152, 659, 212, 257, 78, 989, 579, 664, 673, 1035, 687, 597, 996, 655, 474, 906, 247, 1011, 373, 1044, 1022, 1067, 1085, 681, 276, 49, 433, 703, 1029, 785, 794, 494, 441, 484, 323, 238, 1122, 1037, 339, 36, 530, 1092, 148, 451, 895, 360, 338, 897, 137, 1098, 510, 920, 439, 58, 318, 1064, 1182, 326, 616, 796, 848, 196, 356, 157, 459, 782, 409, 947, 444, 331, 379, 1020, 336, 1171, 984, 7, 3, 572, 524, 391, 50, 1163, 427, 706, 1198, 1141, 508, 607, 324, 107, 40, 650, 111, 429, 110, 203, 118, 672, 1025, 561, 678, 1015, 518, 472, 1169, 674, 963, 932, 560, 667, 587, 1086, 738, 935, 813, 788, 710, 698, 406, 149, 982, 793, 158, 660, 900, 1143, 66, 841, 863, 1105, 648, 835, 959, 492, 1045, 383, 542, 465, 610, 907, 232, 1132, 2, 213, 302, 882, 5, 594, 1202, 392, 112, 1034, 1114, 642, 901, 478, 174, 824, 1038, 1091, 252, 528, 1080, 1118, 815, 404, 482, 1009, 875, 169, 113, 109, 830, 82, 358, 922, 220, 506, 89, 288, 779, 106, 1201, 230, 1051, 1101, 1189, 632, 6, 633, 842, 162, 584, 774, 699, 1001, 1065, 583, 432, 410, 329, 11, 1106, 557, 128, 351, 546, 198, 412, 320, 601, 1153, 543, 694, 256, 823, 1053, 225, 1013, 828, 666, 1027, 614, 884, 418, 781, 235, 270, 1088, 480, 620, 790, 244, 456, 819, 755, 1178, 888, 844, 145, 359, 970, 871, 154, 1160, 719, 1103, 739, 948, 189, 746, 701, 1149, 723, 605, 714, 915, 325, 1112, 787, 929, 896, 887, 880, 272, 1158, 956, 1121, 886, 121, 662, 1194, 345, 1071, 304, 750, 278, 846, 139, 103, 62, 1166, 942, 962, 516, 263, 717, 702, 904, 611, 322, 575, 923, 826, 205, 202, 138, 1190, 748, 1102, 479, 849, 134, 1094, 21, 669, 1126, 241, 245, 354, 827, 1018, 1075, 282, 526, 366, 61, 709, 803, 951, 420, 363, 355, 1150, 933, 24, 117, 136, 70, 624, 143, 353, 448, 1165, 604, 255, 105, 676, 376, 284, 1084, 381, 1004, 438, 361, 576, 347, 644, 764, 732, 491, 43, 362, 567, 402, 181, 306, 643, 640, 141, 950, 531, 466, 1024, 695, 1030, 303, 600, 421, 261, 913, 652, 1214, 693, 814, 69, 585, 905, 22, 20, 547, 317, 311, 809, 348, 1179, 1147, 132, 993, 187, 445, 661, 496, 987, 310, 686, 631, 344, 735, 1028, 1191, 332, 1107, 566, 18, 936, 74, 180, 159, 1060, 645, 407, 477, 712, 217, 1058, 199, 1069, 80, 916, 349, 800, 57, 140, 327, 1186, 262, 816, 15, 1188, 142, 837, 0, 743, 999, 386, 749, 422, 641, 195, 599, 81, 581, 540, 944, 889, 369, 487, 786, 625, 483, 704, 197, 825, 30, 952, 259, 1005, 1184, 127, 874, 1168, 170, 1090, 854, 237, 1183, 598, 832, 806, 495]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4583656875995092
the save name prefix for this run is:  chkpt-ID_4583656875995092_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1023
rank avg (pred): 0.475 +- 0.005
mrr vals (pred, true): 0.020, 0.253
batch losses (mrrl, rdl): 0.0, 0.0028478769

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1064
rank avg (pred): 0.091 +- 0.066
mrr vals (pred, true): 0.270, 0.441
batch losses (mrrl, rdl): 0.0, 3.09651e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 410
rank avg (pred): 0.266 +- 0.232
mrr vals (pred, true): 0.331, 0.058
batch losses (mrrl, rdl): 0.0, 0.0005424781

Epoch over!
epoch time: 11.88

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 564
rank avg (pred): 0.071 +- 0.066
mrr vals (pred, true): 0.451, 0.355
batch losses (mrrl, rdl): 0.0, 3.508e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 121
rank avg (pred): 0.254 +- 0.241
mrr vals (pred, true): 0.402, 0.248
batch losses (mrrl, rdl): 0.0, 0.0007807234

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 119
rank avg (pred): 0.242 +- 0.234
mrr vals (pred, true): 0.417, 0.237
batch losses (mrrl, rdl): 0.0, 0.0006075616

Epoch over!
epoch time: 11.985

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 232
rank avg (pred): 0.270 +- 0.245
mrr vals (pred, true): 0.363, 0.055
batch losses (mrrl, rdl): 0.0, 0.0007045023

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 270
rank avg (pred): 0.043 +- 0.042
mrr vals (pred, true): 0.525, 0.439
batch losses (mrrl, rdl): 0.0, 4.8164e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1058
rank avg (pred): 0.051 +- 0.050
mrr vals (pred, true): 0.508, 0.467
batch losses (mrrl, rdl): 0.0, 4.786e-07

Epoch over!
epoch time: 11.713

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 794
rank avg (pred): 0.279 +- 0.272
mrr vals (pred, true): 0.431, 0.055
batch losses (mrrl, rdl): 0.0, 0.0004514712

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 391
rank avg (pred): 0.240 +- 0.235
mrr vals (pred, true): 0.439, 0.254
batch losses (mrrl, rdl): 0.0, 0.0005321156

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 731
rank avg (pred): 0.107 +- 0.106
mrr vals (pred, true): 0.476, 0.307
batch losses (mrrl, rdl): 0.0, 2.83056e-05

Epoch over!
epoch time: 11.73

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 884
rank avg (pred): 0.309 +- 0.297
mrr vals (pred, true): 0.421, 0.054
batch losses (mrrl, rdl): 0.0, 0.0003027474

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 535
rank avg (pred): 0.055 +- 0.055
mrr vals (pred, true): 0.516, 0.366
batch losses (mrrl, rdl): 0.0, 1.8342e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 932
rank avg (pred): 0.313 +- 0.298
mrr vals (pred, true): 0.427, 0.054
batch losses (mrrl, rdl): 0.0, 0.0003203472

Epoch over!
epoch time: 11.93

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 807
rank avg (pred): 0.313 +- 0.298
mrr vals (pred, true): 0.430, 0.046
batch losses (mrrl, rdl): 1.4436459541, 0.0002949223

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 362
rank avg (pred): 0.352 +- 0.231
mrr vals (pred, true): 0.159, 0.263
batch losses (mrrl, rdl): 0.1074594259, 0.0015559836

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 685
rank avg (pred): 0.365 +- 0.222
mrr vals (pred, true): 0.142, 0.052
batch losses (mrrl, rdl): 0.0850520879, 0.000132032

Epoch over!
epoch time: 12.217

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 536
rank avg (pred): 0.050 +- 0.038
mrr vals (pred, true): 0.375, 0.399
batch losses (mrrl, rdl): 0.0055899508, 2.6252e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 839
rank avg (pred): 0.428 +- 0.175
mrr vals (pred, true): 0.077, 0.054
batch losses (mrrl, rdl): 0.0075223008, 4.0328e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1082
rank avg (pred): 0.376 +- 0.160
mrr vals (pred, true): 0.091, 0.233
batch losses (mrrl, rdl): 0.2008600533, 0.0017559483

Epoch over!
epoch time: 12.008

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 569
rank avg (pred): 0.338 +- 0.232
mrr vals (pred, true): 0.198, 0.232
batch losses (mrrl, rdl): 0.0110730156, 0.0013058387

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 918
rank avg (pred): 0.333 +- 0.134
mrr vals (pred, true): 0.093, 0.047
batch losses (mrrl, rdl): 0.0188579187, 0.0004883247

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1152
rank avg (pred): 0.093 +- 0.067
mrr vals (pred, true): 0.298, 0.372
batch losses (mrrl, rdl): 0.0541697219, 2.27789e-05

Epoch over!
epoch time: 12.059

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 466
rank avg (pred): 0.345 +- 0.231
mrr vals (pred, true): 0.189, 0.058
batch losses (mrrl, rdl): 0.19267717, 0.0001878356

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 933
rank avg (pred): 0.341 +- 0.151
mrr vals (pred, true): 0.111, 0.046
batch losses (mrrl, rdl): 0.0370577946, 0.0006286385

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 359
rank avg (pred): 0.365 +- 0.222
mrr vals (pred, true): 0.145, 0.225
batch losses (mrrl, rdl): 0.0644489154, 0.0012307827

Epoch over!
epoch time: 12.227

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 267
rank avg (pred): 0.032 +- 0.023
mrr vals (pred, true): 0.407, 0.446
batch losses (mrrl, rdl): 0.0152086085, 1.05875e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 299
rank avg (pred): 0.029 +- 0.021
mrr vals (pred, true): 0.426, 0.431
batch losses (mrrl, rdl): 0.0002079517, 1.39905e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 829
rank avg (pred): 0.031 +- 0.023
mrr vals (pred, true): 0.429, 0.412
batch losses (mrrl, rdl): 0.0027562869, 1.24596e-05

Epoch over!
epoch time: 11.905

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 505
rank avg (pred): 0.043 +- 0.031
mrr vals (pred, true): 0.366, 0.405
batch losses (mrrl, rdl): 0.0148069076, 5.7934e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 117
rank avg (pred): 0.366 +- 0.207
mrr vals (pred, true): 0.139, 0.226
batch losses (mrrl, rdl): 0.0763011575, 0.0016038003

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 487
rank avg (pred): 0.050 +- 0.037
mrr vals (pred, true): 0.372, 0.396
batch losses (mrrl, rdl): 0.0059856223, 2.6609e-06

Epoch over!
epoch time: 12.217

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 755
rank avg (pred): 0.051 +- 0.037
mrr vals (pred, true): 0.370, 0.201
batch losses (mrrl, rdl): 0.2856696844, 5.6467e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 396
rank avg (pred): 0.341 +- 0.181
mrr vals (pred, true): 0.143, 0.264
batch losses (mrrl, rdl): 0.1474153847, 0.0013733347

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 858
rank avg (pred): 0.373 +- 0.183
mrr vals (pred, true): 0.124, 0.048
batch losses (mrrl, rdl): 0.054328382, 0.0002152917

Epoch over!
epoch time: 12.224

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 874
rank avg (pred): 0.386 +- 0.165
mrr vals (pred, true): 0.097, 0.056
batch losses (mrrl, rdl): 0.022171963, 0.0001639923

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 34
rank avg (pred): 0.021 +- 0.015
mrr vals (pred, true): 0.469, 0.440
batch losses (mrrl, rdl): 0.0083811497, 2.33023e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 744
rank avg (pred): 0.056 +- 0.042
mrr vals (pred, true): 0.362, 0.260
batch losses (mrrl, rdl): 0.1027938873, 2.0259e-05

Epoch over!
epoch time: 12.132

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 254
rank avg (pred): 0.025 +- 0.018
mrr vals (pred, true): 0.451, 0.441
batch losses (mrrl, rdl): 0.0010298591, 1.82504e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 432
rank avg (pred): 0.371 +- 0.203
mrr vals (pred, true): 0.136, 0.055
batch losses (mrrl, rdl): 0.0739988759, 9.58848e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 709
rank avg (pred): 0.362 +- 0.224
mrr vals (pred, true): 0.165, 0.055
batch losses (mrrl, rdl): 0.1329386383, 0.0001121484

Epoch over!
epoch time: 12.047

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 781
rank avg (pred): 0.368 +- 0.154
mrr vals (pred, true): 0.103, 0.047
batch losses (mrrl, rdl): 0.0284167919, 0.0003909101

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 856
rank avg (pred): 0.380 +- 0.164
mrr vals (pred, true): 0.103, 0.056
batch losses (mrrl, rdl): 0.0281995907, 0.0001379619

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 99
rank avg (pred): 0.392 +- 0.172
mrr vals (pred, true): 0.108, 0.242
batch losses (mrrl, rdl): 0.1796188802, 0.0019089655

Epoch over!
epoch time: 12.038

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.391 +- 0.152
mrr vals (pred, true): 0.096, 0.054

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   78 	     0 	 0.22863 	 0.02078 	 MISS
   82 	     1 	 0.26922 	 0.02078 	 MISS
   85 	     2 	 0.28629 	 0.02078 	 MISS
    1 	     3 	 0.08978 	 0.03867 	 m..s
   31 	     4 	 0.11271 	 0.04646 	 m..s
   33 	     5 	 0.11365 	 0.04720 	 m..s
   24 	     6 	 0.10819 	 0.04757 	 m..s
   35 	     7 	 0.11451 	 0.04771 	 m..s
   36 	     8 	 0.11511 	 0.04803 	 m..s
   16 	     9 	 0.10452 	 0.04803 	 m..s
   14 	    10 	 0.10256 	 0.04811 	 m..s
    2 	    11 	 0.09067 	 0.04836 	 m..s
   81 	    12 	 0.26080 	 0.04854 	 MISS
   55 	    13 	 0.16845 	 0.04878 	 MISS
   43 	    14 	 0.13457 	 0.04899 	 m..s
   19 	    15 	 0.10512 	 0.04927 	 m..s
   49 	    16 	 0.14908 	 0.04950 	 m..s
   38 	    17 	 0.11532 	 0.04951 	 m..s
    8 	    18 	 0.10075 	 0.04978 	 m..s
   55 	    19 	 0.16845 	 0.04980 	 MISS
   10 	    20 	 0.10112 	 0.04992 	 m..s
   30 	    21 	 0.11257 	 0.05059 	 m..s
   55 	    22 	 0.16845 	 0.05085 	 MISS
   55 	    23 	 0.16845 	 0.05113 	 MISS
   18 	    24 	 0.10510 	 0.05126 	 m..s
   39 	    25 	 0.11625 	 0.05142 	 m..s
   32 	    26 	 0.11350 	 0.05187 	 m..s
   25 	    27 	 0.10832 	 0.05193 	 m..s
    6 	    28 	 0.09627 	 0.05197 	 m..s
   52 	    29 	 0.15765 	 0.05216 	 MISS
   53 	    30 	 0.15859 	 0.05234 	 MISS
   55 	    31 	 0.16845 	 0.05286 	 MISS
   55 	    32 	 0.16845 	 0.05297 	 MISS
   17 	    33 	 0.10475 	 0.05325 	 m..s
   55 	    34 	 0.16845 	 0.05349 	 MISS
   40 	    35 	 0.11687 	 0.05358 	 m..s
   23 	    36 	 0.10691 	 0.05375 	 m..s
    4 	    37 	 0.09594 	 0.05412 	 m..s
   76 	    38 	 0.16926 	 0.05418 	 MISS
   55 	    39 	 0.16845 	 0.05451 	 MISS
   55 	    40 	 0.16845 	 0.05452 	 MISS
   55 	    41 	 0.16845 	 0.05458 	 MISS
   15 	    42 	 0.10303 	 0.05460 	 m..s
   55 	    43 	 0.16845 	 0.05471 	 MISS
   46 	    44 	 0.14182 	 0.05474 	 m..s
   21 	    45 	 0.10667 	 0.05482 	 m..s
   55 	    46 	 0.16845 	 0.05488 	 MISS
   13 	    47 	 0.10224 	 0.05526 	 m..s
   28 	    48 	 0.11000 	 0.05543 	 m..s
   55 	    49 	 0.16845 	 0.05596 	 MISS
   11 	    50 	 0.10165 	 0.05603 	 m..s
   54 	    51 	 0.16816 	 0.05718 	 MISS
   26 	    52 	 0.10974 	 0.05719 	 m..s
   11 	    53 	 0.10165 	 0.05722 	 m..s
   55 	    54 	 0.16845 	 0.05742 	 MISS
   55 	    55 	 0.16845 	 0.05757 	 MISS
   55 	    56 	 0.16845 	 0.05813 	 MISS
   34 	    57 	 0.11436 	 0.05843 	 m..s
   37 	    58 	 0.11522 	 0.05878 	 m..s
    4 	    59 	 0.09594 	 0.05879 	 m..s
   45 	    60 	 0.14147 	 0.05960 	 m..s
   55 	    61 	 0.16845 	 0.05971 	 MISS
   20 	    62 	 0.10523 	 0.06028 	 m..s
   55 	    63 	 0.16845 	 0.06190 	 MISS
   27 	    64 	 0.10990 	 0.06476 	 m..s
   29 	    65 	 0.11039 	 0.18635 	 m..s
   91 	    66 	 0.33504 	 0.19588 	 MISS
    9 	    67 	 0.10110 	 0.21900 	 MISS
   48 	    68 	 0.14651 	 0.22051 	 m..s
    3 	    69 	 0.09500 	 0.22773 	 MISS
   55 	    70 	 0.16845 	 0.23729 	 m..s
    7 	    71 	 0.09742 	 0.23744 	 MISS
   77 	    72 	 0.17116 	 0.23787 	 m..s
   51 	    73 	 0.15647 	 0.24172 	 m..s
   50 	    74 	 0.14954 	 0.24272 	 m..s
   55 	    75 	 0.16845 	 0.24298 	 m..s
   47 	    76 	 0.14572 	 0.24581 	 MISS
   22 	    77 	 0.10674 	 0.25213 	 MISS
    0 	    78 	 0.08939 	 0.25379 	 MISS
   41 	    79 	 0.12585 	 0.25655 	 MISS
   55 	    80 	 0.16845 	 0.25718 	 m..s
   42 	    81 	 0.12809 	 0.26188 	 MISS
   44 	    82 	 0.13750 	 0.26472 	 MISS
   80 	    83 	 0.26073 	 0.26671 	 ~...
   94 	    84 	 0.34516 	 0.32527 	 ~...
   97 	    85 	 0.35044 	 0.36349 	 ~...
   89 	    86 	 0.32541 	 0.36576 	 m..s
   98 	    87 	 0.35601 	 0.37166 	 ~...
   87 	    88 	 0.30401 	 0.37401 	 m..s
   96 	    89 	 0.35025 	 0.37807 	 ~...
   92 	    90 	 0.33629 	 0.38118 	 m..s
   88 	    91 	 0.30865 	 0.38881 	 m..s
   90 	    92 	 0.32694 	 0.39224 	 m..s
   79 	    93 	 0.22909 	 0.40923 	 MISS
  115 	    94 	 0.40548 	 0.41713 	 ~...
  117 	    95 	 0.41219 	 0.41934 	 ~...
  107 	    96 	 0.39331 	 0.42502 	 m..s
   99 	    97 	 0.36183 	 0.42791 	 m..s
  110 	    98 	 0.39921 	 0.42818 	 ~...
  105 	    99 	 0.39238 	 0.43162 	 m..s
  120 	   100 	 0.42454 	 0.43261 	 ~...
  114 	   101 	 0.40367 	 0.43359 	 ~...
  104 	   102 	 0.38641 	 0.43385 	 m..s
  106 	   103 	 0.39251 	 0.43446 	 m..s
  108 	   104 	 0.39475 	 0.43616 	 m..s
  109 	   105 	 0.39800 	 0.43877 	 m..s
  116 	   106 	 0.40757 	 0.43946 	 m..s
  113 	   107 	 0.40303 	 0.44052 	 m..s
  111 	   108 	 0.40135 	 0.44075 	 m..s
  101 	   109 	 0.37423 	 0.44102 	 m..s
  102 	   110 	 0.37498 	 0.44401 	 m..s
  103 	   111 	 0.37692 	 0.44425 	 m..s
  100 	   112 	 0.36564 	 0.44455 	 m..s
  112 	   113 	 0.40182 	 0.44555 	 m..s
   94 	   114 	 0.34516 	 0.44708 	 MISS
  119 	   115 	 0.41876 	 0.45054 	 m..s
   83 	   116 	 0.28062 	 0.45083 	 MISS
  118 	   117 	 0.41443 	 0.45678 	 m..s
   93 	   118 	 0.34505 	 0.45760 	 MISS
   86 	   119 	 0.30062 	 0.46572 	 MISS
   84 	   120 	 0.28380 	 0.47171 	 MISS
==========================================
r_mrr = 0.8445820808410645
r2_mrr = 0.6773566007614136
spearmanr_mrr@5 = 0.9925653338432312
spearmanr_mrr@10 = 0.9948405623435974
spearmanr_mrr@50 = 0.9638566374778748
spearmanr_mrr@100 = 0.95573890209198
spearmanr_mrr@All = 0.9632543921470642
==========================================
test time: 0.52
Done Testing dataset Kinships
total time taken: 187.938214302063
training time taken: 180.89381527900696
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8446)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.6774)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9926)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9948)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9639)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.9557)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9633)}}, 'test_loss': {'DistMult': {'Kinships': 10.785974664788228}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 6557380255459631
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1104, 175, 1011, 369, 1085, 137, 858, 802, 372, 915, 283, 653, 411, 214, 423, 1019, 231, 179, 966, 377, 625, 353, 338, 91, 1171, 192, 824, 74, 1072, 438, 367, 773, 946, 253, 1116, 1023, 701, 177, 68, 581, 104, 1109, 515, 394, 378, 380, 448, 19, 1183, 1103, 444, 401, 757, 522, 1043, 851, 328, 307, 1088, 379, 1140, 597, 229, 977, 775, 182, 1132, 1209, 166, 736, 784, 334, 938, 618, 1137, 1189, 586, 519, 698, 920, 95, 537, 774, 891, 1017, 545, 1196, 251, 965, 558, 3, 992, 210, 899, 1045, 959, 1068, 574, 462, 450, 1063, 791, 1033, 715, 278, 512, 943, 1035, 589, 937, 1139, 616, 156, 663, 551, 591, 92, 1151, 1012, 273, 439]
valid_ids (0): []
train_ids (1094): [163, 124, 1004, 576, 912, 559, 230, 391, 680, 244, 162, 381, 262, 44, 113, 553, 14, 948, 276, 1032, 612, 1027, 949, 489, 37, 1203, 960, 212, 514, 384, 473, 1200, 345, 234, 1054, 335, 905, 650, 108, 1041, 1212, 141, 189, 386, 483, 115, 390, 690, 629, 1076, 861, 485, 981, 282, 722, 1157, 704, 733, 1093, 1129, 20, 631, 643, 414, 604, 717, 542, 985, 955, 465, 794, 1053, 500, 1124, 803, 1042, 594, 787, 830, 582, 986, 25, 84, 795, 1156, 458, 1089, 297, 289, 410, 624, 630, 1111, 505, 461, 563, 118, 259, 355, 145, 647, 809, 490, 1090, 445, 1115, 164, 33, 874, 584, 106, 211, 1036, 777, 148, 1211, 729, 302, 1122, 357, 80, 467, 1049, 588, 223, 344, 838, 366, 460, 536, 286, 1168, 1021, 610, 573, 1006, 785, 205, 889, 840, 31, 711, 351, 1097, 1055, 1062, 2, 552, 879, 644, 257, 1001, 245, 1210, 796, 157, 1166, 863, 518, 430, 556, 83, 1110, 776, 914, 304, 429, 455, 929, 388, 408, 1119, 605, 991, 347, 779, 724, 590, 18, 281, 600, 1141, 934, 662, 857, 1113, 480, 602, 979, 1020, 1118, 0, 492, 562, 855, 731, 892, 620, 284, 924, 111, 1098, 557, 53, 484, 42, 199, 720, 51, 208, 767, 668, 8, 1087, 502, 261, 961, 425, 727, 1190, 907, 1079, 316, 1073, 633, 1213, 1134, 1080, 55, 1038, 815, 89, 361, 546, 491, 665, 1108, 744, 939, 782, 45, 255, 725, 543, 964, 702, 517, 716, 474, 1128, 318, 242, 5, 1074, 611, 159, 1167, 1069, 682, 693, 373, 607, 989, 511, 761, 1078, 56, 671, 585, 1114, 88, 703, 922, 726, 139, 1127, 947, 890, 99, 919, 403, 109, 61, 881, 407, 739, 471, 634, 613, 435, 326, 227, 859, 812, 468, 265, 856, 823, 399, 641, 238, 1188, 1135, 689, 925, 9, 415, 1092, 329, 478, 110, 1100, 495, 1164, 370, 486, 1174, 48, 1106, 1138, 126, 93, 930, 962, 13, 374, 990, 828, 412, 745, 1061, 799, 1152, 1182, 127, 549, 1126, 735, 7, 233, 292, 39, 246, 808, 184, 497, 475, 880, 849, 658, 1050, 382, 76, 201, 1075, 1037, 674, 237, 1084, 342, 138, 105, 526, 314, 888, 332, 1091, 453, 1178, 728, 548, 96, 902, 748, 747, 524, 967, 923, 875, 186, 942, 999, 493, 195, 375, 853, 151, 235, 645, 350, 870, 327, 1070, 165, 457, 279, 1191, 666, 1150, 30, 822, 617, 420, 877, 988, 299, 181, 107, 868, 975, 769, 886, 120, 596, 323, 94, 398, 885, 356, 308, 994, 160, 752, 1161, 734, 422, 71, 432, 1086, 538, 968, 476, 579, 1195, 1214, 406, 147, 116, 359, 1159, 798, 1013, 131, 606, 1169, 694, 664, 1179, 447, 1022, 1040, 1187, 405, 927, 393, 73, 1048, 368, 187, 354, 686, 206, 587, 781, 34, 216, 12, 1197, 341, 993, 431, 498, 27, 1123, 931, 362, 319, 599, 578, 1144, 434, 122, 443, 1056, 207, 240, 560, 213, 66, 534, 135, 1173, 452, 550, 1143, 687, 778, 275, 52, 81, 609, 1102, 309, 541, 760, 154, 149, 400, 528, 150, 1047, 692, 638, 532, 1206, 180, 958, 385, 753, 125, 63, 659, 842, 451, 882, 409, 417, 397, 887, 928, 893, 78, 732, 813, 950, 193, 232, 248, 811, 829, 315, 272, 869, 535, 1064, 392, 648, 952, 32, 1148, 903, 269, 908, 196, 984, 860, 1044, 1125, 864, 876, 667, 1099, 699, 320, 144, 190, 530, 1184, 661, 636, 695, 331, 168, 459, 140, 496, 305, 254, 170, 158, 69, 564, 848, 615, 250, 1082, 1025, 1175, 917, 1158, 568, 654, 831, 1162, 577, 1077, 264, 688, 383, 635, 996, 987, 487, 933, 936, 758, 871, 1, 161, 507, 1170, 681, 622, 194, 969, 271, 129, 567, 222, 1101, 755, 215, 70, 974, 50, 973, 134, 926, 221, 21, 569, 86, 696, 679, 814, 249, 873, 270, 516, 293, 247, 971, 554, 153, 913, 572, 178, 800, 169, 1147, 835, 43, 646, 477, 837, 626, 595, 311, 1002, 691, 296, 102, 176, 146, 520, 174, 570, 510, 709, 15, 705, 531, 1029, 963, 1000, 456, 1007, 191, 239, 119, 418, 352, 866, 1051, 426, 396, 501, 1008, 112, 509, 839, 441, 846, 527, 810, 555, 436, 404, 364, 1066, 64, 660, 185, 1131, 421, 442, 143, 883, 277, 363, 825, 343, 580, 117, 220, 130, 749, 395, 940, 513, 67, 1052, 1146, 103, 852, 675, 1058, 97, 1094, 260, 437, 1018, 252, 1142, 719, 575, 1154, 766, 770, 40, 298, 1003, 896, 449, 789, 805, 291, 1181, 983, 656, 427, 317, 878, 826, 816, 114, 503, 1185, 945, 598, 708, 1105, 1057, 676, 621, 547, 533, 583, 58, 142, 1016, 1096, 706, 730, 371, 980, 539, 1199, 780, 1028, 172, 499, 1120, 818, 28, 1059, 916, 197, 746, 628, 463, 183, 121, 79, 783, 561, 867, 592, 523, 865, 209, 918, 741, 1065, 204, 1177, 330, 951, 224, 217, 623, 754, 642, 290, 1155, 16, 333, 100, 285, 801, 566, 845, 832, 41, 77, 413, 358, 226, 797, 684, 982, 921, 714, 1149, 957, 995, 529, 339, 944, 128, 901, 1193, 488, 472, 57, 4, 521, 132, 266, 768, 713, 267, 525, 672, 935, 1095, 1071, 60, 683, 806, 26, 72, 772, 348, 895, 737, 756, 685, 155, 482, 508, 36, 469, 322, 1034, 203, 652, 750, 24, 152, 454, 325, 218, 1009, 900, 365, 854, 788, 11, 571, 1202, 1205, 1160, 1201, 87, 911, 198, 506, 65, 268, 1026, 673, 771, 274, 759, 640, 906, 1031, 910, 336, 228, 740, 295, 1107, 1067, 324, 904, 303, 1112, 35, 544, 402, 608, 862, 678, 909, 820, 428, 136, 243, 817, 346, 833, 54, 90, 970, 998, 494, 280, 932, 1136, 738, 313, 1046, 721, 416, 765, 263, 804, 424, 1005, 1204, 236, 337, 1024, 651, 38, 202, 1208, 6, 49, 956, 23, 1039, 619, 98, 301, 1194, 123, 897, 997, 1121, 884, 565, 792, 360, 85, 173, 649, 1186, 1172, 310, 637, 763, 954, 742, 200, 287, 376, 827, 639, 1081, 101, 718, 807, 601, 850, 1133, 59, 241, 171, 340, 836, 790, 657, 321, 446, 743, 62, 712, 841, 1153, 847, 479, 751, 188, 312, 75, 978, 29, 46, 22, 258, 941, 133, 953, 1130, 1014, 1083, 1010, 710, 898, 697, 723, 300, 389, 1176, 793, 540, 976, 306, 1030, 972, 762, 669, 1207, 466, 764, 593, 1015, 225, 844, 677, 627, 821, 819, 294, 288, 167, 670, 834, 1165, 1180, 1163, 1198, 614, 481, 707, 464, 433, 256, 82, 387, 1145, 1192, 47, 419, 603, 894, 504, 219, 470, 10, 349, 843, 17, 1060, 440, 655, 786, 632, 700, 1117, 872]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9211968346014076
the save name prefix for this run is:  chkpt-ID_9211968346014076_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 995
rank avg (pred): 0.460 +- 0.003
mrr vals (pred, true): 0.021, 0.441
batch losses (mrrl, rdl): 0.0, 0.0035171569

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 675
rank avg (pred): 0.340 +- 0.040
mrr vals (pred, true): 0.028, 0.057
batch losses (mrrl, rdl): 0.0, 0.0004365298

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 145
rank avg (pred): 0.306 +- 0.059
mrr vals (pred, true): 0.033, 0.256
batch losses (mrrl, rdl): 0.0, 0.0009839465

Epoch over!
epoch time: 12.169

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 716
rank avg (pred): 0.282 +- 0.060
mrr vals (pred, true): 0.036, 0.055
batch losses (mrrl, rdl): 0.0, 0.0008175073

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 220
rank avg (pred): 0.251 +- 0.171
mrr vals (pred, true): 0.218, 0.056
batch losses (mrrl, rdl): 0.0, 0.0007272227

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 656
rank avg (pred): 0.250 +- 0.193
mrr vals (pred, true): 0.299, 0.045
batch losses (mrrl, rdl): 0.0, 0.0008804766

Epoch over!
epoch time: 11.933

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 904
rank avg (pred): 0.095 +- 0.078
mrr vals (pred, true): 0.382, 0.287
batch losses (mrrl, rdl): 0.0, 8.0913e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1059
rank avg (pred): 0.133 +- 0.113
mrr vals (pred, true): 0.386, 0.463
batch losses (mrrl, rdl): 0.0, 0.000160863

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 906
rank avg (pred): 0.244 +- 0.211
mrr vals (pred, true): 0.367, 0.036
batch losses (mrrl, rdl): 0.0, 0.0014157835

Epoch over!
epoch time: 11.912

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 244
rank avg (pred): 0.030 +- 0.026
mrr vals (pred, true): 0.511, 0.434
batch losses (mrrl, rdl): 0.0, 1.44053e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1025
rank avg (pred): 0.269 +- 0.223
mrr vals (pred, true): 0.318, 0.238
batch losses (mrrl, rdl): 0.0, 0.0007577237

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 947
rank avg (pred): 0.406 +- 0.306
mrr vals (pred, true): 0.257, 0.059
batch losses (mrrl, rdl): 0.0, 2.97628e-05

Epoch over!
epoch time: 11.966

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 814
rank avg (pred): 0.094 +- 0.081
mrr vals (pred, true): 0.402, 0.213
batch losses (mrrl, rdl): 0.0, 4.5219e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 483
rank avg (pred): 0.253 +- 0.231
mrr vals (pred, true): 0.405, 0.047
batch losses (mrrl, rdl): 0.0, 0.0007044204

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 563
rank avg (pred): 0.069 +- 0.061
mrr vals (pred, true): 0.439, 0.368
batch losses (mrrl, rdl): 0.0, 6.687e-07

Epoch over!
epoch time: 11.956

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 125
rank avg (pred): 0.280 +- 0.227
mrr vals (pred, true): 0.288, 0.254
batch losses (mrrl, rdl): 0.0116402749, 0.0009356797

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 493
rank avg (pred): 0.028 +- 0.020
mrr vals (pred, true): 0.439, 0.383
batch losses (mrrl, rdl): 0.0309981853, 2.64405e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 819
rank avg (pred): 0.078 +- 0.053
mrr vals (pred, true): 0.306, 0.413
batch losses (mrrl, rdl): 0.1146696508, 1.12052e-05

Epoch over!
epoch time: 12.199

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 745
rank avg (pred): 0.120 +- 0.081
mrr vals (pred, true): 0.255, 0.196
batch losses (mrrl, rdl): 0.0352210775, 4.7357e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 73
rank avg (pred): 0.031 +- 0.022
mrr vals (pred, true): 0.425, 0.447
batch losses (mrrl, rdl): 0.0051706862, 1.55342e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1115
rank avg (pred): 0.438 +- 0.254
mrr vals (pred, true): 0.147, 0.054
batch losses (mrrl, rdl): 0.0936071649, 7.6073e-06

Epoch over!
epoch time: 12.019

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1047
rank avg (pred): 0.369 +- 0.205
mrr vals (pred, true): 0.133, 0.050
batch losses (mrrl, rdl): 0.0689856187, 0.0001771162

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 662
rank avg (pred): 0.364 +- 0.206
mrr vals (pred, true): 0.136, 0.054
batch losses (mrrl, rdl): 0.0734958947, 9.14839e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 711
rank avg (pred): 0.344 +- 0.202
mrr vals (pred, true): 0.153, 0.050
batch losses (mrrl, rdl): 0.1062982827, 0.0002762807

Epoch over!
epoch time: 12.242

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 897
rank avg (pred): 0.555 +- 0.314
mrr vals (pred, true): 0.133, 0.021
batch losses (mrrl, rdl): 0.0681182668, 0.0004138085

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 125
rank avg (pred): 0.362 +- 0.200
mrr vals (pred, true): 0.132, 0.254
batch losses (mrrl, rdl): 0.1507862657, 0.0017006753

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 810
rank avg (pred): 0.168 +- 0.104
mrr vals (pred, true): 0.199, 0.305
batch losses (mrrl, rdl): 0.1113958284, 0.0001802036

Epoch over!
epoch time: 12.144

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 373
rank avg (pred): 0.350 +- 0.204
mrr vals (pred, true): 0.150, 0.274
batch losses (mrrl, rdl): 0.1524011493, 0.0014289856

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 31
rank avg (pred): 0.031 +- 0.021
mrr vals (pred, true): 0.417, 0.434
batch losses (mrrl, rdl): 0.0029033062, 1.47351e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 705
rank avg (pred): 0.343 +- 0.195
mrr vals (pred, true): 0.143, 0.055
batch losses (mrrl, rdl): 0.0863830075, 0.0002776909

Epoch over!
epoch time: 12.116

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1128
rank avg (pred): 0.345 +- 0.195
mrr vals (pred, true): 0.142, 0.053
batch losses (mrrl, rdl): 0.0853903741, 0.0002642845

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1206
rank avg (pred): 0.350 +- 0.191
mrr vals (pred, true): 0.133, 0.054
batch losses (mrrl, rdl): 0.0692774206, 0.0002070005

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 994
rank avg (pred): 0.027 +- 0.019
mrr vals (pred, true): 0.434, 0.442
batch losses (mrrl, rdl): 0.0006730534, 1.55158e-05

Epoch over!
epoch time: 12.128

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 902
rank avg (pred): 0.040 +- 0.027
mrr vals (pred, true): 0.383, 0.364
batch losses (mrrl, rdl): 0.0039114621, 1.345e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 546
rank avg (pred): 0.030 +- 0.020
mrr vals (pred, true): 0.401, 0.384
batch losses (mrrl, rdl): 0.0029888214, 2.85037e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 509
rank avg (pred): 0.044 +- 0.030
mrr vals (pred, true): 0.367, 0.367
batch losses (mrrl, rdl): 2.1e-09, 1.05069e-05

Epoch over!
epoch time: 12.275

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 877
rank avg (pred): 0.755 +- 0.248
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 4.209e-07, 0.0017659012

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1120
rank avg (pred): 0.338 +- 0.191
mrr vals (pred, true): 0.148, 0.058
batch losses (mrrl, rdl): 0.0958077908, 0.0002785139

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 86
rank avg (pred): 0.340 +- 0.200
mrr vals (pred, true): 0.159, 0.237
batch losses (mrrl, rdl): 0.0615145601, 0.0011315685

Epoch over!
epoch time: 11.888

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1091
rank avg (pred): 0.337 +- 0.199
mrr vals (pred, true): 0.162, 0.223
batch losses (mrrl, rdl): 0.0378652513, 0.0010174345

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 299
rank avg (pred): 0.025 +- 0.017
mrr vals (pred, true): 0.442, 0.431
batch losses (mrrl, rdl): 0.0011630238, 1.94258e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 40
rank avg (pred): 0.029 +- 0.020
mrr vals (pred, true): 0.422, 0.448
batch losses (mrrl, rdl): 0.0064703934, 1.44317e-05

Epoch over!
epoch time: 12.138

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 276
rank avg (pred): 0.045 +- 0.029
mrr vals (pred, true): 0.351, 0.439
batch losses (mrrl, rdl): 0.0766916499, 3.698e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 34
rank avg (pred): 0.029 +- 0.019
mrr vals (pred, true): 0.418, 0.440
batch losses (mrrl, rdl): 0.0047861575, 1.42289e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 215
rank avg (pred): 0.339 +- 0.192
mrr vals (pred, true): 0.150, 0.055
batch losses (mrrl, rdl): 0.1004448086, 0.0002200837

Epoch over!
epoch time: 12.081

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.332 +- 0.197
mrr vals (pred, true): 0.166, 0.252

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.11955 	 0.02078 	 m..s
   17 	     1 	 0.11843 	 0.02078 	 m..s
    2 	     2 	 0.05514 	 0.04019 	 ~...
   10 	     3 	 0.09211 	 0.04757 	 m..s
   74 	     4 	 0.16771 	 0.04803 	 MISS
   12 	     5 	 0.09666 	 0.04874 	 m..s
   26 	     6 	 0.16334 	 0.04883 	 MISS
   45 	     7 	 0.16463 	 0.04899 	 MISS
   86 	     8 	 0.17114 	 0.04907 	 MISS
   80 	     9 	 0.16923 	 0.04950 	 MISS
   83 	    10 	 0.16958 	 0.04958 	 MISS
   66 	    11 	 0.16631 	 0.04980 	 MISS
   44 	    12 	 0.16459 	 0.04982 	 MISS
    9 	    13 	 0.09166 	 0.05000 	 m..s
   14 	    14 	 0.09990 	 0.05075 	 m..s
   20 	    15 	 0.13500 	 0.05085 	 m..s
   64 	    16 	 0.16611 	 0.05130 	 MISS
    4 	    17 	 0.06670 	 0.05178 	 ~...
   11 	    18 	 0.09620 	 0.05183 	 m..s
   26 	    19 	 0.16334 	 0.05186 	 MISS
   69 	    20 	 0.16651 	 0.05206 	 MISS
    7 	    21 	 0.08719 	 0.05262 	 m..s
   14 	    22 	 0.09990 	 0.05271 	 m..s
   49 	    23 	 0.16495 	 0.05287 	 MISS
   56 	    24 	 0.16571 	 0.05324 	 MISS
   84 	    25 	 0.17071 	 0.05338 	 MISS
    0 	    26 	 0.03285 	 0.05343 	 ~...
    4 	    27 	 0.06670 	 0.05347 	 ~...
   88 	    28 	 0.17434 	 0.05353 	 MISS
   26 	    29 	 0.16334 	 0.05356 	 MISS
   21 	    30 	 0.13772 	 0.05360 	 m..s
   48 	    31 	 0.16489 	 0.05364 	 MISS
   58 	    32 	 0.16583 	 0.05371 	 MISS
    1 	    33 	 0.05289 	 0.05375 	 ~...
   26 	    34 	 0.16334 	 0.05420 	 MISS
    8 	    35 	 0.08762 	 0.05428 	 m..s
   13 	    36 	 0.09944 	 0.05462 	 m..s
   42 	    37 	 0.16436 	 0.05483 	 MISS
   39 	    38 	 0.16394 	 0.05501 	 MISS
   26 	    39 	 0.16334 	 0.05507 	 MISS
    6 	    40 	 0.06717 	 0.05573 	 ~...
   73 	    41 	 0.16761 	 0.05574 	 MISS
   23 	    42 	 0.14220 	 0.05585 	 m..s
   16 	    43 	 0.10100 	 0.05668 	 m..s
   89 	    44 	 0.17513 	 0.05727 	 MISS
    3 	    45 	 0.06490 	 0.05736 	 ~...
   51 	    46 	 0.16515 	 0.05780 	 MISS
   75 	    47 	 0.16788 	 0.05794 	 MISS
   70 	    48 	 0.16666 	 0.05810 	 MISS
   75 	    49 	 0.16788 	 0.06029 	 MISS
   26 	    50 	 0.16334 	 0.06172 	 MISS
   71 	    51 	 0.16726 	 0.06470 	 MISS
   91 	    52 	 0.22440 	 0.08177 	 MISS
   59 	    53 	 0.16583 	 0.17817 	 ~...
   26 	    54 	 0.16334 	 0.19199 	 ~...
   26 	    55 	 0.16334 	 0.20161 	 m..s
   50 	    56 	 0.16499 	 0.21389 	 m..s
   52 	    57 	 0.16531 	 0.21670 	 m..s
   24 	    58 	 0.15429 	 0.22051 	 m..s
   87 	    59 	 0.17159 	 0.22322 	 m..s
   85 	    60 	 0.17073 	 0.22808 	 m..s
   90 	    61 	 0.17581 	 0.22931 	 m..s
   65 	    62 	 0.16625 	 0.23128 	 m..s
   81 	    63 	 0.16936 	 0.23187 	 m..s
   26 	    64 	 0.16334 	 0.23238 	 m..s
   67 	    65 	 0.16647 	 0.23328 	 m..s
   82 	    66 	 0.16942 	 0.23496 	 m..s
   78 	    67 	 0.16874 	 0.23510 	 m..s
   63 	    68 	 0.16595 	 0.23790 	 m..s
   67 	    69 	 0.16647 	 0.23823 	 m..s
   57 	    70 	 0.16583 	 0.23903 	 m..s
   60 	    71 	 0.16585 	 0.24044 	 m..s
   26 	    72 	 0.16334 	 0.24073 	 m..s
   26 	    73 	 0.16334 	 0.24547 	 m..s
   25 	    74 	 0.15611 	 0.24581 	 m..s
   72 	    75 	 0.16757 	 0.24636 	 m..s
   60 	    76 	 0.16585 	 0.24763 	 m..s
   60 	    77 	 0.16585 	 0.24772 	 m..s
   46 	    78 	 0.16475 	 0.24830 	 m..s
   40 	    79 	 0.16417 	 0.25018 	 m..s
   53 	    80 	 0.16566 	 0.25213 	 m..s
   53 	    81 	 0.16566 	 0.25319 	 m..s
   79 	    82 	 0.16910 	 0.25499 	 m..s
   19 	    83 	 0.13040 	 0.25680 	 MISS
   43 	    84 	 0.16459 	 0.25896 	 m..s
   26 	    85 	 0.16334 	 0.26210 	 m..s
   55 	    86 	 0.16569 	 0.26320 	 m..s
   47 	    87 	 0.16486 	 0.26472 	 m..s
   26 	    88 	 0.16334 	 0.26501 	 MISS
   77 	    89 	 0.16811 	 0.26746 	 m..s
   41 	    90 	 0.16436 	 0.27135 	 MISS
   22 	    91 	 0.13860 	 0.28193 	 MISS
   92 	    92 	 0.26932 	 0.28743 	 ~...
  116 	    93 	 0.41168 	 0.36109 	 m..s
  102 	    94 	 0.39168 	 0.36349 	 ~...
  110 	    95 	 0.40210 	 0.36576 	 m..s
  108 	    96 	 0.39907 	 0.36692 	 m..s
  111 	    97 	 0.40229 	 0.37263 	 ~...
   94 	    98 	 0.37214 	 0.37761 	 ~...
   96 	    99 	 0.37480 	 0.38821 	 ~...
   95 	   100 	 0.37341 	 0.39586 	 ~...
  118 	   101 	 0.41430 	 0.39788 	 ~...
  114 	   102 	 0.40438 	 0.40465 	 ~...
  117 	   103 	 0.41209 	 0.40923 	 ~...
  106 	   104 	 0.39551 	 0.40942 	 ~...
   93 	   105 	 0.35669 	 0.42314 	 m..s
  119 	   106 	 0.41715 	 0.42435 	 ~...
  104 	   107 	 0.39318 	 0.42696 	 m..s
  115 	   108 	 0.40471 	 0.43176 	 ~...
  113 	   109 	 0.40342 	 0.43244 	 ~...
   99 	   110 	 0.38335 	 0.43247 	 m..s
  112 	   111 	 0.40285 	 0.43430 	 m..s
   97 	   112 	 0.37618 	 0.43679 	 m..s
  101 	   113 	 0.38838 	 0.44039 	 m..s
  107 	   114 	 0.39562 	 0.44335 	 m..s
  100 	   115 	 0.38537 	 0.44342 	 m..s
  109 	   116 	 0.40167 	 0.44604 	 m..s
  105 	   117 	 0.39539 	 0.44691 	 m..s
  103 	   118 	 0.39318 	 0.44837 	 m..s
   98 	   119 	 0.38036 	 0.44970 	 m..s
  119 	   120 	 0.41715 	 0.47042 	 m..s
==========================================
r_mrr = 0.8469091057777405
r2_mrr = 0.7045586109161377
spearmanr_mrr@5 = 0.6719844341278076
spearmanr_mrr@10 = 0.8028401732444763
spearmanr_mrr@50 = 0.9786182045936584
spearmanr_mrr@100 = 0.8478860259056091
spearmanr_mrr@All = 0.881161630153656
==========================================
test time: 0.393
Done Testing dataset Kinships
total time taken: 188.38953256607056
training time taken: 181.62789154052734
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8469)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7046)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.6720)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.8028)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9786)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8479)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.8812)}}, 'test_loss': {'DistMult': {'Kinships': 8.000178206657438}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 3888587108062485
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [753, 559, 878, 733, 355, 87, 345, 1027, 1128, 954, 919, 101, 1082, 107, 1057, 1109, 118, 569, 1040, 24, 205, 1149, 481, 626, 614, 1174, 329, 535, 365, 84, 805, 175, 834, 249, 900, 1004, 74, 890, 622, 658, 18, 867, 1014, 1134, 957, 599, 699, 125, 1133, 587, 346, 272, 662, 992, 741, 238, 56, 766, 65, 58, 140, 1018, 225, 142, 122, 486, 818, 700, 800, 1013, 304, 941, 709, 208, 273, 618, 690, 100, 1155, 885, 1093, 1168, 330, 512, 1052, 1201, 251, 10, 247, 912, 1098, 872, 385, 57, 593, 635, 918, 688, 36, 1106, 759, 513, 164, 422, 1167, 196, 1148, 201, 189, 197, 19, 491, 846, 322, 681, 421, 813, 103, 417, 143, 837]
valid_ids (0): []
train_ids (1094): [1047, 233, 182, 951, 96, 581, 851, 584, 206, 278, 1060, 174, 523, 54, 425, 881, 1058, 966, 342, 26, 1161, 651, 844, 351, 558, 446, 668, 1045, 905, 91, 1065, 848, 1062, 1159, 1108, 498, 1083, 1138, 1118, 420, 369, 432, 1136, 672, 1002, 702, 1, 1207, 312, 154, 703, 466, 437, 338, 590, 83, 531, 283, 217, 679, 831, 997, 1195, 293, 876, 689, 1172, 546, 743, 961, 817, 255, 773, 874, 270, 295, 179, 470, 479, 382, 629, 1008, 275, 379, 104, 9, 370, 548, 412, 218, 775, 552, 3, 471, 90, 282, 95, 1088, 23, 1102, 1030, 763, 782, 783, 27, 646, 522, 840, 1053, 343, 740, 788, 530, 1069, 257, 677, 404, 583, 33, 862, 325, 669, 792, 532, 52, 378, 187, 82, 698, 1039, 644, 441, 1011, 64, 996, 307, 211, 631, 311, 1132, 1094, 462, 728, 110, 1044, 1150, 215, 139, 945, 1055, 748, 588, 638, 440, 749, 585, 17, 1205, 704, 480, 572, 1092, 790, 1121, 1127, 413, 43, 429, 1012, 711, 1151, 579, 756, 279, 1203, 1214, 1208, 1105, 993, 931, 770, 673, 1183, 427, 924, 828, 750, 1163, 575, 258, 61, 297, 363, 73, 633, 285, 586, 37, 518, 316, 693, 300, 28, 871, 392, 942, 916, 712, 135, 1147, 294, 235, 1176, 248, 397, 974, 1194, 29, 76, 111, 78, 72, 445, 1070, 146, 1009, 331, 398, 1073, 129, 1107, 863, 1206, 987, 554, 746, 825, 1170, 994, 92, 320, 1007, 442, 0, 502, 598, 340, 469, 1061, 281, 415, 636, 390, 1096, 506, 1141, 1157, 31, 567, 776, 899, 594, 1140, 1111, 769, 845, 543, 906, 784, 648, 882, 461, 649, 647, 203, 1064, 214, 510, 77, 274, 39, 550, 290, 393, 683, 893, 639, 317, 11, 263, 789, 180, 1114, 624, 63, 541, 326, 276, 30, 376, 476, 949, 1129, 984, 595, 130, 403, 38, 611, 223, 986, 525, 477, 861, 86, 989, 226, 239, 229, 371, 1123, 822, 608, 1179, 50, 988, 25, 321, 373, 913, 1162, 847, 768, 798, 1085, 456, 952, 377, 287, 81, 927, 306, 944, 487, 888, 93, 640, 947, 898, 20, 496, 760, 1063, 684, 1144, 171, 973, 909, 920, 1035, 744, 910, 1017, 209, 484, 500, 141, 148, 617, 97, 1143, 864, 710, 547, 576, 515, 184, 1117, 727, 880, 88, 144, 938, 830, 368, 983, 692, 752, 765, 747, 1177, 85, 212, 34, 1178, 405, 613, 359, 123, 781, 578, 1022, 1019, 13, 682, 150, 105, 524, 1112, 811, 729, 384, 591, 1212, 678, 833, 127, 115, 691, 886, 237, 855, 106, 191, 1021, 620, 895, 45, 426, 1152, 787, 972, 1187, 625, 977, 922, 660, 250, 859, 298, 656, 832, 341, 925, 687, 244, 564, 835, 939, 870, 460, 616, 971, 894, 960, 301, 44, 120, 810, 438, 434, 779, 745, 902, 408, 483, 35, 537, 1180, 1025, 514, 328, 344, 1054, 303, 464, 334, 452, 1086, 200, 697, 252, 418, 216, 41, 519, 468, 566, 352, 738, 465, 654, 1029, 261, 520, 1185, 210, 1036, 1100, 607, 1046, 1120, 1079, 495, 1031, 169, 877, 126, 936, 731, 48, 713, 601, 1188, 675, 433, 632, 149, 1125, 315, 489, 131, 1010, 764, 75, 815, 1153, 400, 21, 387, 670, 231, 158, 161, 1192, 1066, 799, 402, 347, 1033, 956, 323, 780, 259, 475, 236, 264, 396, 665, 488, 948, 1142, 1181, 2, 641, 655, 198, 167, 178, 192, 243, 933, 1198, 501, 424, 791, 1146, 4, 1003, 609, 695, 60, 414, 1084, 70, 152, 269, 89, 319, 1184, 168, 793, 410, 542, 915, 926, 221, 853, 932, 109, 1038, 265, 778, 928, 652, 527, 254, 842, 145, 305, 367, 435, 472, 176, 1204, 1034, 119, 714, 245, 580, 562, 754, 637, 716, 8, 159, 207, 112, 540, 1091, 1197, 1126, 809, 896, 976, 589, 155, 1042, 416, 1131, 1130, 950, 940, 1196, 539, 336, 188, 386, 857, 364, 1015, 999, 722, 674, 545, 812, 721, 66, 797, 1145, 383, 645, 1154, 350, 1048, 67, 685, 1199, 302, 1202, 606, 409, 240, 114, 284, 602, 571, 138, 447, 734, 419, 664, 995, 353, 873, 998, 968, 958, 1101, 929, 1191, 934, 737, 124, 503, 1156, 478, 538, 814, 819, 544, 804, 563, 557, 356, 246, 785, 1056, 508, 241, 204, 574, 448, 1097, 310, 854, 1006, 1186, 357, 634, 268, 1165, 808, 561, 42, 715, 219, 883, 494, 102, 431, 935, 875, 962, 901, 493, 1164, 1041, 374, 1087, 267, 453, 959, 708, 505, 199, 801, 980, 761, 1075, 1166, 195, 375, 596, 868, 856, 277, 701, 40, 1016, 473, 911, 516, 865, 553, 430, 549, 965, 757, 361, 726, 193, 68, 1071, 360, 767, 917, 253, 904, 389, 395, 1175, 511, 406, 666, 348, 657, 157, 381, 884, 485, 132, 889, 492, 454, 14, 162, 923, 47, 153, 517, 723, 354, 339, 720, 930, 719, 1020, 1099, 597, 1051, 705, 556, 163, 482, 1135, 12, 394, 451, 185, 592, 843, 964, 852, 62, 286, 1119, 222, 450, 529, 953, 332, 568, 1037, 1001, 220, 170, 771, 612, 165, 509, 979, 1089, 173, 806, 291, 55, 628, 1032, 1078, 99, 1090, 457, 772, 603, 892, 1209, 16, 136, 181, 820, 725, 117, 650, 362, 1043, 1050, 113, 762, 985, 458, 299, 324, 774, 869, 850, 266, 327, 891, 1213, 718, 1049, 680, 280, 401, 582, 1139, 1113, 1171, 560, 823, 982, 391, 600, 841, 807, 735, 407, 1028, 667, 186, 838, 309, 573, 619, 160, 459, 969, 866, 1122, 615, 642, 751, 724, 630, 1110, 190, 1193, 455, 242, 333, 1210, 1104, 777, 786, 1023, 388, 829, 1158, 7, 313, 296, 46, 467, 32, 1115, 507, 227, 1103, 1124, 1005, 663, 202, 943, 289, 858, 755, 358, 49, 98, 604, 937, 366, 1137, 94, 1074, 166, 1081, 534, 839, 694, 803, 262, 423, 739, 794, 156, 288, 228, 372, 908, 661, 183, 439, 795, 80, 151, 1173, 526, 232, 1095, 730, 308, 1182, 1068, 849, 887, 975, 260, 623, 732, 497, 380, 736, 411, 69, 577, 51, 463, 1024, 551, 903, 314, 1190, 521, 318, 1076, 194, 836, 1116, 981, 133, 707, 349, 53, 536, 671, 860, 116, 1160, 271, 22, 827, 610, 490, 802, 256, 335, 970, 570, 108, 963, 555, 706, 474, 528, 676, 1059, 128, 121, 1211, 533, 172, 399, 758, 627, 1072, 1067, 914, 234, 879, 907, 821, 696, 1080, 621, 1200, 921, 71, 337, 213, 967, 444, 605, 449, 134, 1189, 991, 1026, 990, 147, 1169, 643, 717, 659, 946, 436, 1000, 230, 826, 978, 897, 796, 565, 504, 824, 224, 742, 499, 443, 1077, 955, 6, 292, 5, 428, 686, 59, 653, 177, 816, 79, 15, 137]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3516266246836159
the save name prefix for this run is:  chkpt-ID_3516266246836159_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 706
rank avg (pred): 0.567 +- 0.007
mrr vals (pred, true): 0.017, 0.060
batch losses (mrrl, rdl): 0.0, 0.0003684814

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 936
rank avg (pred): 0.268 +- 0.247
mrr vals (pred, true): 0.296, 0.055
batch losses (mrrl, rdl): 0.0, 0.0006077432

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1067
rank avg (pred): 0.177 +- 0.206
mrr vals (pred, true): 0.443, 0.452
batch losses (mrrl, rdl): 0.0, 0.0004431386

Epoch over!
epoch time: 11.953

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 819
rank avg (pred): 0.295 +- 0.291
mrr vals (pred, true): 0.345, 0.413
batch losses (mrrl, rdl): 0.0, 0.0014607005

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 545
rank avg (pred): 0.255 +- 0.245
mrr vals (pred, true): 0.327, 0.388
batch losses (mrrl, rdl): 0.0, 0.0009836579

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 951
rank avg (pred): 0.333 +- 0.347
mrr vals (pred, true): 0.407, 0.057
batch losses (mrrl, rdl): 0.0, 0.000277597

Epoch over!
epoch time: 11.965

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 190
rank avg (pred): 0.179 +- 0.218
mrr vals (pred, true): 0.486, 0.058
batch losses (mrrl, rdl): 0.0, 0.0013498573

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 689
rank avg (pred): 0.187 +- 0.214
mrr vals (pred, true): 0.433, 0.056
batch losses (mrrl, rdl): 0.0, 0.0013195486

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 8
rank avg (pred): 0.195 +- 0.228
mrr vals (pred, true): 0.464, 0.430
batch losses (mrrl, rdl): 0.0, 0.000576988

Epoch over!
epoch time: 11.902

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 127
rank avg (pred): 0.186 +- 0.228
mrr vals (pred, true): 0.520, 0.285
batch losses (mrrl, rdl): 0.0, 0.0003628157

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 979
rank avg (pred): 0.210 +- 0.237
mrr vals (pred, true): 0.452, 0.477
batch losses (mrrl, rdl): 0.0, 0.0007145298

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 869
rank avg (pred): 0.303 +- 0.318
mrr vals (pred, true): 0.411, 0.055
batch losses (mrrl, rdl): 0.0, 0.0003574471

Epoch over!
epoch time: 11.807

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 642
rank avg (pred): 0.248 +- 0.227
mrr vals (pred, true): 0.293, 0.247
batch losses (mrrl, rdl): 0.0, 0.0006551533

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 781
rank avg (pred): 0.303 +- 0.321
mrr vals (pred, true): 0.448, 0.047
batch losses (mrrl, rdl): 0.0, 0.0005426931

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 740
rank avg (pred): 0.243 +- 0.286
mrr vals (pred, true): 0.496, 0.397
batch losses (mrrl, rdl): 0.0, 0.0009685707

Epoch over!
epoch time: 11.847

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 95
rank avg (pred): 0.221 +- 0.235
mrr vals (pred, true): 0.417, 0.240
batch losses (mrrl, rdl): 0.3105790615, 0.0005037898

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 293
rank avg (pred): 0.359 +- 0.232
mrr vals (pred, true): 0.234, 0.428
batch losses (mrrl, rdl): 0.3748116195, 0.0022202889

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 980
rank avg (pred): 0.323 +- 0.210
mrr vals (pred, true): 0.239, 0.471
batch losses (mrrl, rdl): 0.5378537774, 0.0017483669

Epoch over!
epoch time: 12.19

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1178
rank avg (pred): 0.350 +- 0.218
mrr vals (pred, true): 0.230, 0.249
batch losses (mrrl, rdl): 0.0037527904, 0.0016242362

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 631
rank avg (pred): 0.341 +- 0.218
mrr vals (pred, true): 0.249, 0.220
batch losses (mrrl, rdl): 0.0084911855, 0.0010583589

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 295
rank avg (pred): 0.343 +- 0.215
mrr vals (pred, true): 0.245, 0.446
batch losses (mrrl, rdl): 0.4029923677, 0.0019651984

Epoch over!
epoch time: 11.93

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 959
rank avg (pred): 0.761 +- 0.333
mrr vals (pred, true): 0.129, 0.053
batch losses (mrrl, rdl): 0.0620340519, 0.0017480203

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 609
rank avg (pred): 0.330 +- 0.206
mrr vals (pred, true): 0.244, 0.257
batch losses (mrrl, rdl): 0.0017399304, 0.001299692

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 827
rank avg (pred): 0.734 +- 0.356
mrr vals (pred, true): 0.161, 0.333
batch losses (mrrl, rdl): 0.2953817248, 0.0081822053

Epoch over!
epoch time: 12.07

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 732
rank avg (pred): 0.826 +- 0.327
mrr vals (pred, true): 0.112, 0.206
batch losses (mrrl, rdl): 0.0877861828, 0.0094875935

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1061
rank avg (pred): 0.334 +- 0.191
mrr vals (pred, true): 0.235, 0.476
batch losses (mrrl, rdl): 0.5811619759, 0.0018661539

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 356
rank avg (pred): 0.311 +- 0.188
mrr vals (pred, true): 0.246, 0.229
batch losses (mrrl, rdl): 0.0028789574, 0.0010302101

Epoch over!
epoch time: 12.339

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 529
rank avg (pred): 0.333 +- 0.183
mrr vals (pred, true): 0.222, 0.372
batch losses (mrrl, rdl): 0.2262662649, 0.0016808461

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 506
rank avg (pred): 0.315 +- 0.177
mrr vals (pred, true): 0.234, 0.396
batch losses (mrrl, rdl): 0.2625089586, 0.0015094123

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 63
rank avg (pred): 0.311 +- 0.175
mrr vals (pred, true): 0.235, 0.448
batch losses (mrrl, rdl): 0.4536141455, 0.0015051983

Epoch over!
epoch time: 12.023

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1102
rank avg (pred): 0.301 +- 0.178
mrr vals (pred, true): 0.244, 0.265
batch losses (mrrl, rdl): 0.004475195, 0.0009796206

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 465
rank avg (pred): 0.301 +- 0.170
mrr vals (pred, true): 0.239, 0.047
batch losses (mrrl, rdl): 0.3564251661, 0.0005274947

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 374
rank avg (pred): 0.303 +- 0.166
mrr vals (pred, true): 0.234, 0.262
batch losses (mrrl, rdl): 0.0074576521, 0.0010024711

Epoch over!
epoch time: 12.381

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 838
rank avg (pred): 0.789 +- 0.334
mrr vals (pred, true): 0.133, 0.053
batch losses (mrrl, rdl): 0.0685442388, 0.0019507948

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 624
rank avg (pred): 0.291 +- 0.159
mrr vals (pred, true): 0.236, 0.225
batch losses (mrrl, rdl): 0.0011503631, 0.0008690614

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 172
rank avg (pred): 0.300 +- 0.160
mrr vals (pred, true): 0.225, 0.047
batch losses (mrrl, rdl): 0.3058178425, 0.000530872

Epoch over!
epoch time: 12.05

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1180
rank avg (pred): 0.291 +- 0.160
mrr vals (pred, true): 0.237, 0.186
batch losses (mrrl, rdl): 0.0253542215, 0.0003263027

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1193
rank avg (pred): 0.268 +- 0.157
mrr vals (pred, true): 0.251, 0.053
batch losses (mrrl, rdl): 0.4048788846, 0.0006540493

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 436
rank avg (pred): 0.282 +- 0.155
mrr vals (pred, true): 0.238, 0.053
batch losses (mrrl, rdl): 0.3533941805, 0.0006255234

Epoch over!
epoch time: 12.179

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1046
rank avg (pred): 0.289 +- 0.155
mrr vals (pred, true): 0.228, 0.051
batch losses (mrrl, rdl): 0.3154917359, 0.0005619921

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 388
rank avg (pred): 0.281 +- 0.153
mrr vals (pred, true): 0.229, 0.257
batch losses (mrrl, rdl): 0.0079816235, 0.0008259158

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 542
rank avg (pred): 0.266 +- 0.146
mrr vals (pred, true): 0.238, 0.376
batch losses (mrrl, rdl): 0.1906208992, 0.0009497147

Epoch over!
epoch time: 12.068

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 377
rank avg (pred): 0.260 +- 0.145
mrr vals (pred, true): 0.242, 0.250
batch losses (mrrl, rdl): 0.0007016491, 0.0006272382

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1191
rank avg (pred): 0.264 +- 0.144
mrr vals (pred, true): 0.238, 0.049
batch losses (mrrl, rdl): 0.3534178138, 0.0007795545

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 610
rank avg (pred): 0.258 +- 0.139
mrr vals (pred, true): 0.236, 0.242
batch losses (mrrl, rdl): 0.0004279227, 0.0006045461

Epoch over!
epoch time: 11.907

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.900 +- 0.278
mrr vals (pred, true): 0.080, 0.332

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.07596 	 0.02078 	 m..s
   81 	     1 	 0.22592 	 0.04634 	 MISS
    0 	     2 	 0.07537 	 0.04720 	 ~...
  111 	     3 	 0.23084 	 0.04733 	 MISS
   21 	     4 	 0.13855 	 0.04771 	 m..s
  115 	     5 	 0.23169 	 0.04803 	 MISS
   14 	     6 	 0.09796 	 0.04858 	 m..s
   84 	     7 	 0.22596 	 0.04935 	 MISS
   88 	     8 	 0.22604 	 0.04935 	 MISS
    1 	     9 	 0.07590 	 0.04970 	 ~...
    6 	    10 	 0.08570 	 0.04997 	 m..s
   63 	    11 	 0.22507 	 0.05085 	 MISS
   55 	    12 	 0.22397 	 0.05088 	 MISS
   15 	    13 	 0.09952 	 0.05174 	 m..s
  119 	    14 	 0.23439 	 0.05193 	 MISS
   75 	    15 	 0.22555 	 0.05206 	 MISS
    5 	    16 	 0.08457 	 0.05212 	 m..s
   17 	    17 	 0.10076 	 0.05213 	 m..s
   41 	    18 	 0.21733 	 0.05234 	 MISS
   22 	    19 	 0.14875 	 0.05244 	 m..s
    9 	    20 	 0.09258 	 0.05249 	 m..s
  105 	    21 	 0.22979 	 0.05276 	 MISS
  100 	    22 	 0.22882 	 0.05292 	 MISS
   43 	    23 	 0.22290 	 0.05365 	 MISS
   13 	    24 	 0.09773 	 0.05373 	 m..s
   89 	    25 	 0.22604 	 0.05376 	 MISS
   29 	    26 	 0.20834 	 0.05392 	 MISS
   94 	    27 	 0.22698 	 0.05395 	 MISS
   19 	    28 	 0.12141 	 0.05405 	 m..s
  109 	    29 	 0.23042 	 0.05409 	 MISS
   82 	    30 	 0.22595 	 0.05423 	 MISS
   99 	    31 	 0.22861 	 0.05463 	 MISS
  118 	    32 	 0.23335 	 0.05482 	 MISS
   11 	    33 	 0.09368 	 0.05498 	 m..s
   53 	    34 	 0.22374 	 0.05531 	 MISS
   96 	    35 	 0.22732 	 0.05543 	 MISS
   51 	    36 	 0.22357 	 0.05574 	 MISS
   71 	    37 	 0.22548 	 0.05589 	 MISS
  107 	    38 	 0.22985 	 0.05616 	 MISS
   87 	    39 	 0.22600 	 0.05688 	 MISS
   42 	    40 	 0.21848 	 0.05789 	 MISS
  104 	    41 	 0.22966 	 0.05795 	 MISS
   20 	    42 	 0.13512 	 0.05878 	 m..s
    2 	    43 	 0.07596 	 0.05879 	 ~...
   23 	    44 	 0.15243 	 0.05906 	 m..s
   56 	    45 	 0.22404 	 0.05960 	 MISS
    7 	    46 	 0.08576 	 0.14805 	 m..s
   69 	    47 	 0.22536 	 0.17431 	 m..s
   11 	    48 	 0.09368 	 0.19851 	 MISS
   72 	    49 	 0.22548 	 0.21795 	 ~...
   78 	    50 	 0.22584 	 0.21948 	 ~...
   85 	    51 	 0.22597 	 0.22191 	 ~...
   57 	    52 	 0.22445 	 0.22982 	 ~...
  102 	    53 	 0.22938 	 0.23019 	 ~...
   49 	    54 	 0.22316 	 0.23149 	 ~...
   44 	    55 	 0.22290 	 0.23156 	 ~...
   28 	    56 	 0.20801 	 0.23238 	 ~...
  115 	    57 	 0.23169 	 0.23295 	 ~...
   48 	    58 	 0.22316 	 0.23322 	 ~...
   97 	    59 	 0.22794 	 0.23549 	 ~...
   57 	    60 	 0.22445 	 0.23732 	 ~...
   35 	    61 	 0.21305 	 0.23816 	 ~...
   93 	    62 	 0.22663 	 0.24002 	 ~...
   92 	    63 	 0.22654 	 0.24026 	 ~...
   98 	    64 	 0.22810 	 0.24224 	 ~...
   82 	    65 	 0.22595 	 0.24248 	 ~...
   37 	    66 	 0.21375 	 0.24281 	 ~...
   46 	    67 	 0.22300 	 0.24499 	 ~...
   33 	    68 	 0.21101 	 0.24569 	 m..s
  113 	    69 	 0.23144 	 0.24683 	 ~...
   52 	    70 	 0.22358 	 0.25189 	 ~...
  108 	    71 	 0.23018 	 0.25361 	 ~...
   24 	    72 	 0.18692 	 0.25379 	 m..s
   67 	    73 	 0.22533 	 0.25383 	 ~...
   25 	    74 	 0.18950 	 0.25397 	 m..s
   70 	    75 	 0.22543 	 0.25432 	 ~...
   47 	    76 	 0.22312 	 0.25594 	 m..s
  119 	    77 	 0.23439 	 0.25893 	 ~...
   57 	    78 	 0.22445 	 0.26251 	 m..s
   50 	    79 	 0.22352 	 0.26251 	 m..s
   29 	    80 	 0.20834 	 0.26320 	 m..s
   65 	    81 	 0.22531 	 0.26716 	 m..s
   77 	    82 	 0.22576 	 0.26727 	 m..s
   10 	    83 	 0.09286 	 0.26773 	 MISS
  109 	    84 	 0.23042 	 0.26914 	 m..s
  114 	    85 	 0.23157 	 0.27073 	 m..s
   31 	    86 	 0.20942 	 0.27274 	 m..s
  101 	    87 	 0.22902 	 0.27353 	 m..s
    4 	    88 	 0.08012 	 0.33233 	 MISS
   18 	    89 	 0.11850 	 0.33846 	 MISS
    8 	    90 	 0.08857 	 0.33874 	 MISS
   54 	    91 	 0.22381 	 0.35476 	 MISS
   37 	    92 	 0.21375 	 0.36576 	 MISS
   39 	    93 	 0.21452 	 0.36606 	 MISS
   45 	    94 	 0.22294 	 0.38105 	 MISS
   36 	    95 	 0.21357 	 0.38177 	 MISS
   15 	    96 	 0.09952 	 0.38496 	 MISS
   40 	    97 	 0.21592 	 0.38590 	 MISS
   34 	    98 	 0.21283 	 0.38764 	 MISS
   91 	    99 	 0.22634 	 0.38768 	 MISS
   62 	   100 	 0.22462 	 0.39438 	 MISS
   80 	   101 	 0.22591 	 0.39451 	 MISS
   66 	   102 	 0.22532 	 0.41901 	 MISS
   79 	   103 	 0.22584 	 0.42587 	 MISS
   95 	   104 	 0.22705 	 0.42696 	 MISS
   90 	   105 	 0.22609 	 0.42847 	 MISS
   86 	   106 	 0.22600 	 0.43100 	 MISS
  106 	   107 	 0.22984 	 0.43176 	 MISS
   26 	   108 	 0.19821 	 0.43284 	 MISS
   64 	   109 	 0.22527 	 0.43359 	 MISS
   57 	   110 	 0.22445 	 0.43393 	 MISS
   31 	   111 	 0.20942 	 0.43430 	 MISS
   67 	   112 	 0.22533 	 0.43638 	 MISS
   74 	   113 	 0.22551 	 0.43849 	 MISS
   73 	   114 	 0.22549 	 0.43978 	 MISS
  102 	   115 	 0.22938 	 0.44455 	 MISS
   27 	   116 	 0.20493 	 0.44607 	 MISS
  117 	   117 	 0.23187 	 0.44642 	 MISS
   57 	   118 	 0.22445 	 0.44837 	 MISS
   76 	   119 	 0.22570 	 0.44970 	 MISS
  112 	   120 	 0.23122 	 0.47952 	 MISS
==========================================
r_mrr = 0.28566816449165344
r2_mrr = 0.0645037293434143
spearmanr_mrr@5 = 0.6179569959640503
spearmanr_mrr@10 = 0.7980788350105286
spearmanr_mrr@50 = 0.898325502872467
spearmanr_mrr@100 = 0.6377201080322266
spearmanr_mrr@All = 0.6524956822395325
==========================================
test time: 0.433
Done Testing dataset Kinships
total time taken: 187.94654488563538
training time taken: 181.12157773971558
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.2857)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.0645)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.6180)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.7981)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.8983)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.6377)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.6525)}}, 'test_loss': {'DistMult': {'Kinships': 24.611126351170242}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 6596451614249046
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [40, 856, 354, 243, 1047, 930, 71, 444, 670, 545, 432, 717, 495, 1096, 151, 1064, 131, 17, 925, 316, 303, 464, 408, 699, 598, 787, 589, 659, 415, 861, 218, 199, 149, 29, 519, 380, 417, 1199, 429, 637, 472, 1013, 1062, 1011, 348, 312, 279, 987, 1092, 1145, 487, 225, 301, 998, 773, 179, 802, 986, 1114, 569, 1131, 591, 808, 1007, 819, 99, 448, 1132, 1044, 98, 1192, 30, 9, 664, 996, 751, 85, 441, 1112, 782, 1197, 70, 876, 244, 197, 19, 509, 570, 237, 35, 1141, 720, 726, 318, 1063, 542, 127, 496, 831, 1035, 872, 1214, 502, 11, 425, 280, 378, 812, 778, 1060, 638, 174, 484, 798, 1032, 1061, 1105, 739, 159, 248, 411]
valid_ids (0): []
train_ids (1094): [953, 363, 676, 1193, 1117, 758, 368, 1150, 173, 1140, 682, 546, 356, 1116, 592, 297, 619, 281, 193, 929, 427, 1118, 265, 379, 48, 603, 794, 599, 612, 57, 78, 374, 391, 941, 5, 737, 525, 473, 23, 962, 41, 718, 112, 167, 177, 918, 610, 1049, 658, 890, 486, 990, 416, 471, 55, 584, 1159, 491, 649, 1170, 952, 1058, 1201, 1026, 605, 656, 1079, 877, 435, 183, 1055, 246, 724, 561, 475, 403, 330, 668, 571, 433, 1110, 481, 103, 404, 1176, 974, 747, 498, 694, 826, 337, 774, 250, 544, 50, 991, 860, 1134, 67, 678, 940, 1078, 516, 950, 221, 172, 1172, 227, 674, 68, 869, 1037, 684, 314, 180, 109, 1187, 393, 1074, 326, 437, 198, 182, 617, 206, 597, 345, 700, 306, 479, 60, 508, 1208, 154, 257, 799, 1083, 290, 927, 882, 240, 847, 985, 184, 932, 145, 640, 108, 970, 390, 65, 25, 1143, 431, 106, 192, 1070, 1046, 646, 514, 234, 1059, 999, 335, 1207, 135, 976, 152, 83, 652, 552, 252, 606, 191, 84, 797, 870, 588, 818, 322, 258, 352, 625, 833, 719, 370, 650, 321, 816, 115, 777, 136, 144, 789, 942, 873, 853, 91, 107, 230, 1183, 129, 181, 697, 195, 755, 527, 1, 1169, 738, 984, 644, 517, 1137, 405, 412, 533, 308, 820, 710, 229, 434, 1088, 194, 90, 1165, 474, 1135, 439, 1202, 702, 513, 746, 1174, 743, 342, 236, 245, 754, 386, 249, 241, 1151, 982, 351, 66, 273, 259, 483, 854, 365, 1077, 862, 955, 537, 261, 510, 285, 949, 934, 1136, 223, 716, 965, 759, 1010, 232, 620, 500, 274, 213, 975, 834, 87, 141, 501, 642, 383, 1128, 707, 385, 613, 292, 1076, 943, 1147, 733, 900, 74, 752, 157, 62, 105, 395, 1095, 628, 1189, 456, 376, 667, 966, 634, 482, 842, 1041, 547, 884, 828, 864, 16, 536, 26, 800, 1023, 1001, 1084, 825, 226, 196, 735, 215, 1153, 1033, 58, 708, 443, 741, 165, 372, 212, 538, 608, 624, 27, 457, 647, 220, 898, 377, 729, 521, 373, 904, 219, 711, 305, 242, 643, 899, 12, 558, 394, 418, 761, 905, 211, 791, 1212, 333, 1210, 548, 45, 917, 32, 160, 309, 1109, 260, 1004, 1127, 187, 878, 426, 51, 288, 621, 458, 130, 75, 304, 1205, 1188, 399, 779, 703, 256, 604, 298, 132, 1206, 896, 117, 1133, 557, 323, 1085, 1175, 948, 360, 551, 630, 302, 1031, 398, 1091, 400, 979, 367, 1103, 564, 566, 410, 836, 559, 922, 217, 1022, 1106, 413, 786, 840, 291, 43, 671, 284, 1099, 122, 549, 848, 1009, 963, 375, 912, 936, 512, 645, 31, 283, 396, 865, 1016, 822, 506, 222, 307, 1098, 1005, 1052, 111, 964, 1177, 575, 419, 73, 666, 829, 504, 92, 734, 683, 147, 470, 207, 293, 857, 140, 728, 736, 845, 420, 315, 430, 709, 680, 1082, 185, 80, 654, 771, 919, 269, 926, 732, 933, 59, 202, 781, 233, 1158, 846, 1161, 4, 387, 171, 275, 1014, 715, 88, 121, 550, 524, 727, 823, 515, 300, 691, 169, 1086, 937, 636, 436, 344, 805, 289, 748, 881, 272, 995, 1071, 959, 837, 113, 447, 1051, 824, 915, 821, 332, 54, 21, 480, 69, 1045, 477, 595, 883, 1173, 287, 1018, 1093, 795, 572, 102, 453, 1190, 1015, 346, 499, 669, 95, 170, 1050, 1067, 200, 1113, 266, 247, 895, 282, 555, 578, 796, 347, 1209, 161, 1115, 449, 86, 189, 1182, 320, 1036, 908, 665, 1163, 1156, 815, 651, 693, 633, 493, 690, 407, 886, 1040, 138, 362, 263, 463, 1024, 879, 44, 166, 343, 583, 713, 104, 1102, 270, 685, 921, 830, 485, 123, 553, 692, 489, 327, 476, 579, 535, 1097, 208, 622, 1029, 851, 1198, 1017, 235, 698, 764, 63, 543, 858, 568, 397, 695, 409, 310, 349, 677, 540, 350, 772, 6, 231, 1101, 749, 0, 1185, 461, 20, 627, 1065, 971, 1171, 887, 462, 1119, 980, 522, 868, 497, 814, 655, 523, 1138, 1104, 641, 762, 13, 361, 389, 722, 175, 600, 745, 8, 1149, 954, 353, 909, 817, 278, 587, 973, 77, 1130, 148, 1000, 567, 961, 488, 601, 760, 89, 339, 712, 3, 1121, 1157, 364, 594, 39, 968, 186, 268, 892, 271, 33, 766, 139, 163, 1090, 1122, 96, 623, 616, 660, 530, 1139, 661, 1089, 945, 406, 1186, 15, 581, 730, 916, 388, 295, 1073, 1075, 534, 188, 381, 585, 835, 577, 672, 866, 903, 423, 146, 871, 255, 446, 1126, 1108, 783, 118, 1125, 1094, 355, 455, 1144, 276, 137, 478, 1006, 494, 371, 639, 635, 469, 1168, 317, 626, 528, 1012, 1019, 209, 596, 36, 859, 1148, 1008, 22, 704, 319, 1081, 422, 972, 827, 1068, 631, 262, 1087, 1053, 993, 775, 1162, 756, 459, 924, 392, 1021, 983, 253, 648, 366, 931, 565, 277, 532, 1043, 338, 1184, 744, 804, 205, 168, 341, 988, 757, 768, 384, 14, 451, 511, 238, 153, 1057, 855, 753, 47, 688, 843, 944, 541, 340, 880, 1200, 1027, 18, 1181, 28, 947, 267, 580, 442, 125, 1072, 1191, 239, 369, 1107, 440, 460, 867, 216, 554, 539, 1054, 811, 294, 844, 891, 382, 210, 679, 24, 1034, 1020, 37, 299, 629, 740, 1003, 618, 780, 958, 64, 529, 663, 1030, 526, 79, 1042, 1069, 852, 401, 214, 124, 706, 507, 1179, 531, 424, 923, 832, 42, 939, 7, 116, 997, 150, 989, 928, 334, 1100, 889, 1080, 849, 776, 264, 725, 902, 938, 742, 156, 34, 468, 901, 897, 76, 56, 602, 421, 142, 920, 143, 503, 492, 328, 910, 10, 311, 763, 93, 1160, 329, 82, 981, 1056, 49, 632, 935, 52, 977, 960, 801, 967, 445, 574, 1194, 957, 673, 1204, 1154, 53, 359, 38, 176, 101, 792, 978, 765, 615, 785, 1180, 784, 204, 1038, 696, 46, 114, 1039, 614, 788, 705, 653, 1025, 158, 358, 793, 863, 254, 1066, 992, 807, 1152, 573, 336, 1155, 1028, 164, 110, 563, 689, 190, 61, 951, 286, 582, 770, 134, 907, 590, 128, 809, 914, 296, 1146, 119, 1196, 803, 325, 875, 850, 100, 969, 838, 750, 414, 520, 609, 1124, 1164, 1178, 228, 888, 224, 810, 1111, 806, 402, 769, 723, 438, 576, 721, 675, 560, 155, 556, 994, 1048, 714, 518, 1123, 1166, 657, 133, 906, 81, 593, 1120, 586, 203, 790, 1142, 466, 251, 450, 162, 1167, 913, 894, 331, 731, 607, 452, 686, 94, 490, 662, 2, 687, 505, 841, 767, 956, 681, 893, 313, 178, 1129, 1203, 357, 946, 839, 97, 201, 611, 467, 465, 72, 911, 1211, 428, 1195, 885, 813, 562, 1002, 1213, 120, 701, 454, 324, 126, 874]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  578551382359514
the save name prefix for this run is:  chkpt-ID_578551382359514_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 177
rank avg (pred): 0.514 +- 0.002
mrr vals (pred, true): 0.019, 0.058
batch losses (mrrl, rdl): 0.0, 0.0001556978

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 214
rank avg (pred): 0.326 +- 0.128
mrr vals (pred, true): 0.041, 0.053
batch losses (mrrl, rdl): 0.0, 0.0004012545

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 386
rank avg (pred): 0.198 +- 0.146
mrr vals (pred, true): 0.221, 0.167
batch losses (mrrl, rdl): 0.0, 4.4155e-06

Epoch over!
epoch time: 11.978

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 447
rank avg (pred): 0.267 +- 0.198
mrr vals (pred, true): 0.208, 0.055
batch losses (mrrl, rdl): 0.0, 0.0006495034

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 850
rank avg (pred): 0.379 +- 0.298
mrr vals (pred, true): 0.276, 0.050
batch losses (mrrl, rdl): 0.0, 6.62729e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 491
rank avg (pred): 0.058 +- 0.053
mrr vals (pred, true): 0.435, 0.381
batch losses (mrrl, rdl): 0.0, 6.186e-07

Epoch over!
epoch time: 11.815

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 926
rank avg (pred): 0.476 +- 0.336
mrr vals (pred, true): 0.211, 0.050
batch losses (mrrl, rdl): 0.0, 3.49594e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 946
rank avg (pred): 0.443 +- 0.317
mrr vals (pred, true): 0.212, 0.052
batch losses (mrrl, rdl): 0.0, 2.09925e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1140
rank avg (pred): 0.110 +- 0.099
mrr vals (pred, true): 0.390, 0.409
batch losses (mrrl, rdl): 0.0, 6.48137e-05

Epoch over!
epoch time: 11.817

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 547
rank avg (pred): 0.050 +- 0.045
mrr vals (pred, true): 0.450, 0.380
batch losses (mrrl, rdl): 0.0, 4.1253e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 428
rank avg (pred): 0.263 +- 0.240
mrr vals (pred, true): 0.369, 0.051
batch losses (mrrl, rdl): 0.0, 0.0005768788

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 848
rank avg (pred): 0.415 +- 0.313
mrr vals (pred, true): 0.248, 0.051
batch losses (mrrl, rdl): 0.0, 2.71897e-05

Epoch over!
epoch time: 11.918

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1184
rank avg (pred): 0.251 +- 0.232
mrr vals (pred, true): 0.341, 0.238
batch losses (mrrl, rdl): 0.0, 0.0006822154

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 296
rank avg (pred): 0.068 +- 0.066
mrr vals (pred, true): 0.462, 0.453
batch losses (mrrl, rdl): 0.0, 7.0366e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1164
rank avg (pred): 0.237 +- 0.229
mrr vals (pred, true): 0.381, 0.274
batch losses (mrrl, rdl): 0.0, 0.0006419626

Epoch over!
epoch time: 11.939

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 850
rank avg (pred): 0.408 +- 0.300
mrr vals (pred, true): 0.236, 0.050
batch losses (mrrl, rdl): 0.3460636735, 2.59176e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 761
rank avg (pred): 0.740 +- 0.245
mrr vals (pred, true): 0.027, 0.048
batch losses (mrrl, rdl): 0.0051116636, 0.0012060042

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 307
rank avg (pred): 0.023 +- 0.017
mrr vals (pred, true): 0.454, 0.443
batch losses (mrrl, rdl): 0.0010492391, 2.35908e-05

Epoch over!
epoch time: 12.146

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 888
rank avg (pred): 0.690 +- 0.243
mrr vals (pred, true): 0.036, 0.052
batch losses (mrrl, rdl): 0.002091134, 0.0009701009

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 65
rank avg (pred): 0.026 +- 0.017
mrr vals (pred, true): 0.415, 0.434
batch losses (mrrl, rdl): 0.0036022442, 1.87811e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 511
rank avg (pred): 0.030 +- 0.020
mrr vals (pred, true): 0.389, 0.397
batch losses (mrrl, rdl): 0.0006295741, 2.0308e-05

Epoch over!
epoch time: 12.144

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 995
rank avg (pred): 0.018 +- 0.012
mrr vals (pred, true): 0.471, 0.441
batch losses (mrrl, rdl): 0.0089498246, 3.06512e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1207
rank avg (pred): 0.299 +- 0.191
mrr vals (pred, true): 0.165, 0.055
batch losses (mrrl, rdl): 0.1323728114, 0.0004556296

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 153
rank avg (pred): 0.309 +- 0.186
mrr vals (pred, true): 0.146, 0.241
batch losses (mrrl, rdl): 0.0911305994, 0.0011285284

Epoch over!
epoch time: 11.884

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 286
rank avg (pred): 0.024 +- 0.017
mrr vals (pred, true): 0.443, 0.445
batch losses (mrrl, rdl): 3.48612e-05, 2.1283e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 844
rank avg (pred): 0.650 +- 0.249
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 0.0001028481, 0.0005667143

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 352
rank avg (pred): 0.296 +- 0.187
mrr vals (pred, true): 0.162, 0.217
batch losses (mrrl, rdl): 0.0301703829, 0.0003686657

Epoch over!
epoch time: 12.228

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 245
rank avg (pred): 0.027 +- 0.018
mrr vals (pred, true): 0.417, 0.414
batch losses (mrrl, rdl): 6.87805e-05, 1.88177e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 120
rank avg (pred): 0.303 +- 0.183
mrr vals (pred, true): 0.148, 0.252
batch losses (mrrl, rdl): 0.10783571, 0.0010061107

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1079
rank avg (pred): 0.017 +- 0.011
mrr vals (pred, true): 0.487, 0.447
batch losses (mrrl, rdl): 0.0160686597, 3.11411e-05

Epoch over!
epoch time: 12.156

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 169
rank avg (pred): 0.289 +- 0.188
mrr vals (pred, true): 0.177, 0.054
batch losses (mrrl, rdl): 0.1620555073, 0.0004918042

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 956
rank avg (pred): 0.593 +- 0.210
mrr vals (pred, true): 0.040, 0.058
batch losses (mrrl, rdl): 0.0010904563, 0.0003885759

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1014
rank avg (pred): 0.289 +- 0.186
mrr vals (pred, true): 0.176, 0.271
batch losses (mrrl, rdl): 0.0889277458, 0.0009774412

Epoch over!
epoch time: 12.232

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1126
rank avg (pred): 0.287 +- 0.186
mrr vals (pred, true): 0.173, 0.055
batch losses (mrrl, rdl): 0.1524440795, 0.000573516

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 577
rank avg (pred): 0.314 +- 0.174
mrr vals (pred, true): 0.129, 0.228
batch losses (mrrl, rdl): 0.0993575081, 0.0010538457

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 180
rank avg (pred): 0.308 +- 0.176
mrr vals (pred, true): 0.141, 0.059
batch losses (mrrl, rdl): 0.0834911168, 0.0003471822

Epoch over!
epoch time: 12.085

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 721
rank avg (pred): 0.314 +- 0.171
mrr vals (pred, true): 0.131, 0.058
batch losses (mrrl, rdl): 0.0655591264, 0.0004004846

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 62
rank avg (pred): 0.024 +- 0.017
mrr vals (pred, true): 0.440, 0.423
batch losses (mrrl, rdl): 0.0029432443, 2.32821e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 201
rank avg (pred): 0.290 +- 0.182
mrr vals (pred, true): 0.170, 0.049
batch losses (mrrl, rdl): 0.1441755593, 0.000618534

Epoch over!
epoch time: 12.233

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 489
rank avg (pred): 0.045 +- 0.030
mrr vals (pred, true): 0.339, 0.392
batch losses (mrrl, rdl): 0.0287162606, 4.9781e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1008
rank avg (pred): 0.295 +- 0.176
mrr vals (pred, true): 0.153, 0.243
batch losses (mrrl, rdl): 0.0806916356, 0.0009866098

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 755
rank avg (pred): 0.346 +- 0.193
mrr vals (pred, true): 0.108, 0.201
batch losses (mrrl, rdl): 0.0851082504, 0.0013887421

Epoch over!
epoch time: 11.975

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 24
rank avg (pred): 0.024 +- 0.017
mrr vals (pred, true): 0.435, 0.446
batch losses (mrrl, rdl): 0.0012352301, 1.9994e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 457
rank avg (pred): 0.314 +- 0.166
mrr vals (pred, true): 0.124, 0.053
batch losses (mrrl, rdl): 0.0547474548, 0.000403106

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 138
rank avg (pred): 0.296 +- 0.174
mrr vals (pred, true): 0.153, 0.234
batch losses (mrrl, rdl): 0.0659682676, 0.0008867687

Epoch over!
epoch time: 11.99

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.021 +- 0.014
mrr vals (pred, true): 0.447, 0.448

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03862 	 0.03867 	 ~...
    4 	     1 	 0.04187 	 0.04509 	 ~...
    0 	     2 	 0.03813 	 0.04701 	 ~...
    2 	     3 	 0.04108 	 0.04720 	 ~...
   21 	     4 	 0.12184 	 0.04853 	 m..s
   44 	     5 	 0.13311 	 0.04889 	 m..s
   13 	     6 	 0.11837 	 0.04962 	 m..s
   57 	     7 	 0.13818 	 0.04979 	 m..s
   22 	     8 	 0.12206 	 0.04980 	 m..s
   41 	     9 	 0.13249 	 0.04997 	 m..s
   55 	    10 	 0.13712 	 0.05001 	 m..s
   17 	    11 	 0.12012 	 0.05029 	 m..s
   58 	    12 	 0.13851 	 0.05099 	 m..s
   18 	    13 	 0.12084 	 0.05101 	 m..s
   48 	    14 	 0.13388 	 0.05119 	 m..s
   64 	    15 	 0.14300 	 0.05130 	 m..s
   62 	    16 	 0.14042 	 0.05140 	 m..s
   27 	    17 	 0.12382 	 0.05168 	 m..s
   60 	    18 	 0.13939 	 0.05234 	 m..s
   10 	    19 	 0.05055 	 0.05252 	 ~...
   65 	    20 	 0.14355 	 0.05292 	 m..s
   31 	    21 	 0.12604 	 0.05298 	 m..s
   68 	    22 	 0.14444 	 0.05324 	 m..s
   38 	    23 	 0.13078 	 0.05324 	 m..s
   52 	    24 	 0.13444 	 0.05340 	 m..s
   70 	    25 	 0.14485 	 0.05355 	 m..s
   68 	    26 	 0.14444 	 0.05361 	 m..s
    6 	    27 	 0.04461 	 0.05363 	 ~...
   65 	    28 	 0.14355 	 0.05371 	 m..s
   10 	    29 	 0.05055 	 0.05375 	 ~...
   13 	    30 	 0.11837 	 0.05392 	 m..s
   52 	    31 	 0.13444 	 0.05420 	 m..s
   61 	    32 	 0.13982 	 0.05439 	 m..s
    5 	    33 	 0.04356 	 0.05446 	 ~...
    9 	    34 	 0.04954 	 0.05462 	 ~...
   45 	    35 	 0.13319 	 0.05465 	 m..s
    3 	    36 	 0.04144 	 0.05498 	 ~...
   46 	    37 	 0.13367 	 0.05519 	 m..s
   72 	    38 	 0.14547 	 0.05533 	 m..s
   59 	    39 	 0.13882 	 0.05560 	 m..s
   67 	    40 	 0.14416 	 0.05570 	 m..s
   12 	    41 	 0.05272 	 0.05582 	 ~...
   28 	    42 	 0.12410 	 0.05589 	 m..s
   32 	    43 	 0.12774 	 0.05625 	 m..s
   51 	    44 	 0.13424 	 0.05688 	 m..s
   36 	    45 	 0.13016 	 0.05728 	 m..s
   43 	    46 	 0.13282 	 0.05828 	 m..s
   37 	    47 	 0.13035 	 0.05854 	 m..s
    7 	    48 	 0.04563 	 0.05879 	 ~...
   71 	    49 	 0.14520 	 0.05888 	 m..s
    8 	    50 	 0.04663 	 0.06028 	 ~...
   56 	    51 	 0.13804 	 0.06470 	 m..s
   54 	    52 	 0.13567 	 0.19199 	 m..s
   42 	    53 	 0.13259 	 0.21715 	 m..s
   73 	    54 	 0.14549 	 0.21942 	 m..s
   19 	    55 	 0.12098 	 0.22051 	 m..s
   47 	    56 	 0.13378 	 0.22549 	 m..s
   26 	    57 	 0.12366 	 0.22734 	 MISS
   34 	    58 	 0.12867 	 0.23134 	 MISS
   40 	    59 	 0.13139 	 0.23134 	 m..s
   29 	    60 	 0.12414 	 0.23156 	 MISS
   76 	    61 	 0.14682 	 0.23496 	 m..s
   30 	    62 	 0.12473 	 0.23674 	 MISS
   39 	    63 	 0.13110 	 0.23828 	 MISS
   73 	    64 	 0.14549 	 0.24098 	 m..s
   25 	    65 	 0.12356 	 0.24238 	 MISS
   33 	    66 	 0.12837 	 0.24548 	 MISS
   63 	    67 	 0.14193 	 0.24601 	 MISS
   75 	    68 	 0.14619 	 0.24617 	 m..s
   20 	    69 	 0.12165 	 0.24997 	 MISS
   16 	    70 	 0.11931 	 0.25163 	 MISS
   15 	    71 	 0.11922 	 0.25680 	 MISS
   23 	    72 	 0.12316 	 0.25718 	 MISS
   49 	    73 	 0.13396 	 0.26210 	 MISS
   50 	    74 	 0.13397 	 0.26914 	 MISS
   35 	    75 	 0.12975 	 0.27815 	 MISS
   24 	    76 	 0.12329 	 0.28497 	 MISS
   82 	    77 	 0.37947 	 0.31072 	 m..s
   80 	    78 	 0.36673 	 0.36654 	 ~...
   90 	    79 	 0.40574 	 0.37263 	 m..s
   87 	    80 	 0.39199 	 0.37606 	 ~...
   77 	    81 	 0.32190 	 0.38369 	 m..s
   89 	    82 	 0.39494 	 0.38821 	 ~...
   84 	    83 	 0.38049 	 0.39038 	 ~...
   91 	    84 	 0.42449 	 0.39310 	 m..s
   88 	    85 	 0.39316 	 0.39610 	 ~...
   86 	    86 	 0.39152 	 0.39875 	 ~...
   83 	    87 	 0.37992 	 0.40130 	 ~...
   79 	    88 	 0.35742 	 0.41130 	 m..s
   81 	    89 	 0.37005 	 0.41336 	 m..s
   85 	    90 	 0.38574 	 0.41636 	 m..s
   78 	    91 	 0.33855 	 0.41713 	 m..s
   96 	    92 	 0.44584 	 0.42698 	 ~...
  104 	    93 	 0.45091 	 0.42791 	 ~...
   92 	    94 	 0.43859 	 0.42880 	 ~...
  111 	    95 	 0.45663 	 0.42881 	 ~...
  103 	    96 	 0.45073 	 0.43023 	 ~...
  113 	    97 	 0.46090 	 0.43162 	 ~...
  115 	    98 	 0.46382 	 0.43250 	 m..s
  100 	    99 	 0.44839 	 0.43412 	 ~...
   94 	   100 	 0.44117 	 0.43430 	 ~...
  106 	   101 	 0.45123 	 0.43783 	 ~...
   97 	   102 	 0.44700 	 0.43964 	 ~...
   93 	   103 	 0.43921 	 0.44052 	 ~...
  114 	   104 	 0.46319 	 0.44123 	 ~...
  120 	   105 	 0.47370 	 0.44133 	 m..s
  108 	   106 	 0.45154 	 0.44251 	 ~...
  116 	   107 	 0.46408 	 0.44335 	 ~...
  102 	   108 	 0.45067 	 0.44344 	 ~...
  107 	   109 	 0.45150 	 0.44408 	 ~...
  118 	   110 	 0.46932 	 0.44432 	 ~...
  101 	   111 	 0.45061 	 0.44573 	 ~...
   95 	   112 	 0.44474 	 0.44622 	 ~...
  104 	   113 	 0.45091 	 0.44712 	 ~...
   98 	   114 	 0.44727 	 0.44759 	 ~...
  109 	   115 	 0.45553 	 0.45098 	 ~...
   99 	   116 	 0.44777 	 0.45465 	 ~...
  109 	   117 	 0.45553 	 0.45836 	 ~...
  117 	   118 	 0.46922 	 0.45926 	 ~...
  112 	   119 	 0.46034 	 0.46245 	 ~...
  119 	   120 	 0.46991 	 0.47648 	 ~...
==========================================
r_mrr = 0.9052336812019348
r2_mrr = 0.8185944557189941
spearmanr_mrr@5 = 0.8880153894424438
spearmanr_mrr@10 = 0.9265264868736267
spearmanr_mrr@50 = 0.9829395413398743
spearmanr_mrr@100 = 0.9118069410324097
spearmanr_mrr@All = 0.9278472661972046
==========================================
test time: 0.394
Done Testing dataset Kinships
total time taken: 188.0818419456482
training time taken: 180.99750542640686
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.9052)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.8186)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.8880)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9265)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9829)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.9118)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9278)}}, 'test_loss': {'DistMult': {'Kinships': 6.3404745482639555}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 361029881349393
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [707, 613, 346, 664, 275, 657, 494, 813, 854, 1002, 552, 768, 885, 880, 434, 1062, 241, 1208, 121, 326, 772, 670, 962, 1182, 610, 624, 246, 1128, 353, 143, 738, 394, 558, 294, 176, 928, 1173, 888, 1109, 695, 411, 287, 537, 534, 719, 774, 1194, 1181, 256, 1000, 324, 487, 490, 953, 614, 510, 1185, 741, 12, 315, 43, 999, 119, 977, 312, 1151, 731, 1123, 302, 421, 548, 565, 580, 97, 730, 1057, 1207, 356, 110, 297, 536, 1069, 128, 775, 933, 420, 960, 301, 1096, 67, 833, 42, 277, 770, 157, 997, 828, 0, 892, 966, 656, 209, 156, 117, 369, 41, 239, 799, 1214, 1120, 9, 336, 561, 396, 1188, 1192, 599, 713, 520, 115, 460]
valid_ids (0): []
train_ids (1094): [89, 964, 437, 823, 107, 740, 556, 637, 788, 673, 846, 1099, 581, 518, 563, 442, 243, 1138, 877, 782, 1122, 1102, 20, 79, 1176, 427, 455, 908, 433, 108, 529, 562, 1124, 945, 866, 191, 881, 967, 88, 549, 444, 391, 845, 930, 8, 389, 824, 469, 559, 943, 347, 28, 650, 340, 293, 378, 635, 1061, 501, 1133, 475, 141, 778, 30, 1141, 408, 78, 517, 667, 284, 451, 450, 797, 1139, 644, 808, 162, 919, 690, 53, 173, 764, 926, 441, 271, 37, 607, 900, 158, 1156, 446, 697, 393, 986, 418, 39, 958, 1183, 1058, 848, 48, 811, 619, 55, 682, 899, 1082, 723, 1144, 186, 10, 230, 329, 934, 991, 995, 989, 202, 1121, 497, 478, 757, 283, 579, 684, 604, 820, 70, 605, 921, 413, 99, 814, 27, 1149, 780, 1046, 2, 118, 163, 319, 998, 608, 338, 449, 929, 876, 164, 412, 365, 803, 832, 270, 791, 793, 334, 868, 840, 879, 603, 102, 931, 678, 318, 1167, 722, 519, 133, 189, 134, 498, 373, 542, 474, 183, 153, 901, 776, 233, 816, 728, 858, 1084, 950, 992, 197, 33, 1041, 452, 533, 669, 90, 939, 1178, 1190, 1035, 14, 550, 258, 633, 508, 180, 454, 404, 1213, 909, 547, 165, 990, 1093, 1142, 827, 1119, 235, 526, 756, 961, 748, 458, 317, 303, 622, 1037, 61, 703, 335, 746, 292, 266, 225, 821, 431, 345, 954, 860, 267, 101, 686, 376, 1077, 993, 320, 676, 836, 1114, 211, 1177, 212, 568, 727, 1026, 658, 1199, 742, 1168, 996, 1078, 13, 250, 291, 123, 159, 1012, 11, 523, 1022, 514, 589, 1211, 92, 804, 190, 462, 1131, 1028, 111, 1174, 865, 511, 380, 609, 62, 630, 1201, 626, 916, 1075, 841, 896, 600, 257, 1005, 124, 308, 582, 951, 830, 717, 895, 787, 1202, 515, 300, 152, 709, 479, 1148, 863, 354, 109, 538, 1191, 273, 21, 231, 1184, 203, 72, 794, 743, 112, 305, 912, 675, 1029, 735, 440, 1008, 19, 617, 1147, 578, 228, 947, 1076, 710, 1157, 483, 274, 359, 665, 259, 443, 344, 834, 1083, 390, 1193, 486, 321, 1094, 503, 1170, 22, 588, 1016, 927, 1150, 779, 155, 182, 281, 445, 374, 980, 1080, 122, 316, 187, 56, 1063, 295, 894, 627, 146, 666, 923, 789, 463, 724, 1197, 532, 903, 1034, 785, 473, 1111, 1204, 435, 453, 629, 1169, 528, 504, 482, 96, 601, 60, 1198, 1098, 652, 516, 646, 557, 649, 1052, 306, 530, 83, 861, 1068, 426, 1060, 763, 288, 711, 137, 367, 1140, 1160, 1053, 554, 645, 965, 1115, 636, 1049, 181, 265, 974, 149, 310, 975, 366, 795, 248, 1166, 969, 535, 1087, 363, 736, 459, 642, 7, 105, 883, 843, 236, 1200, 429, 681, 737, 148, 428, 130, 219, 1206, 24, 17, 1010, 289, 871, 6, 715, 247, 1006, 151, 1023, 704, 1045, 113, 783, 331, 290, 1054, 539, 80, 1048, 705, 145, 1195, 655, 918, 298, 1205, 174, 1118, 936, 777, 963, 844, 734, 1047, 718, 185, 385, 1073, 269, 956, 643, 84, 870, 1004, 521, 647, 531, 74, 971, 223, 296, 261, 95, 1125, 4, 733, 819, 714, 1051, 77, 612, 253, 34, 278, 886, 69, 216, 911, 1001, 448, 688, 314, 712, 379, 387, 477, 807, 154, 973, 496, 595, 282, 752, 382, 867, 417, 1043, 598, 304, 749, 551, 628, 352, 169, 606, 217, 25, 683, 687, 52, 208, 1009, 75, 1130, 35, 251, 244, 196, 602, 1159, 957, 761, 560, 1107, 696, 940, 691, 555, 1163, 430, 576, 1055, 527, 689, 659, 1189, 1101, 583, 720, 1196, 1038, 160, 342, 567, 1032, 812, 855, 1209, 472, 392, 1153, 424, 869, 227, 835, 898, 221, 415, 822, 1097, 955, 199, 416, 674, 893, 500, 175, 859, 671, 1135, 574, 806, 200, 611, 406, 692, 76, 1027, 663, 1064, 397, 850, 461, 466, 839, 584, 23, 252, 1154, 120, 509, 935, 800, 773, 260, 596, 623, 809, 507, 632, 790, 862, 309, 910, 1020, 47, 932, 218, 238, 395, 481, 1108, 220, 214, 874, 132, 360, 201, 699, 470, 51, 802, 31, 349, 1039, 1155, 750, 1030, 661, 760, 480, 204, 1050, 229, 762, 126, 144, 313, 100, 409, 82, 668, 784, 988, 402, 1165, 586, 591, 268, 44, 371, 592, 543, 985, 73, 525, 907, 16, 195, 1013, 616, 81, 1106, 179, 457, 884, 328, 249, 36, 1100, 725, 272, 26, 1031, 577, 357, 590, 765, 685, 29, 491, 15, 341, 398, 362, 285, 45, 91, 593, 815, 887, 938, 255, 242, 1042, 597, 585, 370, 546, 873, 286, 837, 753, 38, 401, 438, 693, 553, 142, 838, 1074, 98, 66, 135, 825, 767, 1014, 1044, 541, 226, 698, 167, 63, 1180, 18, 1024, 330, 245, 1158, 136, 372, 410, 856, 116, 193, 1015, 93, 805, 213, 58, 1210, 68, 348, 172, 403, 240, 323, 651, 1017, 745, 299, 114, 170, 1127, 864, 388, 587, 327, 1136, 489, 983, 754, 1091, 1011, 944, 755, 361, 618, 184, 522, 234, 64, 801, 215, 276, 857, 981, 810, 355, 54, 85, 1164, 224, 50, 575, 471, 672, 680, 512, 467, 513, 891, 1070, 339, 842, 702, 524, 631, 59, 1066, 653, 1088, 968, 332, 206, 572, 648, 492, 726, 638, 786, 732, 634, 979, 5, 131, 982, 906, 280, 263, 1126, 882, 1137, 1212, 1081, 493, 177, 381, 758, 660, 1105, 976, 506, 1089, 615, 818, 1161, 166, 701, 1085, 890, 1145, 484, 364, 694, 739, 468, 350, 205, 545, 677, 566, 729, 1112, 621, 1175, 759, 407, 937, 502, 540, 207, 987, 654, 942, 386, 358, 897, 625, 140, 914, 423, 1056, 351, 679, 1092, 798, 399, 569, 949, 1067, 700, 913, 46, 639, 829, 708, 383, 1007, 1186, 125, 924, 237, 311, 464, 1203, 564, 769, 1079, 1110, 831, 781, 904, 662, 432, 878, 792, 279, 422, 570, 232, 849, 439, 465, 1019, 1146, 1, 1072, 852, 1021, 161, 377, 87, 127, 922, 1090, 817, 178, 325, 984, 796, 766, 71, 436, 1187, 495, 889, 400, 1025, 1103, 1040, 447, 139, 905, 1018, 505, 49, 959, 194, 1171, 333, 826, 1132, 948, 188, 192, 307, 456, 262, 641, 171, 952, 476, 1065, 594, 1129, 499, 57, 264, 368, 222, 322, 198, 1162, 104, 721, 1036, 337, 1003, 419, 853, 872, 384, 970, 343, 751, 978, 129, 425, 375, 847, 1104, 485, 972, 210, 488, 1179, 620, 925, 716, 902, 1059, 1143, 744, 405, 414, 1116, 103, 706, 40, 1152, 86, 544, 1134, 915, 1117, 994, 573, 1071, 941, 640, 747, 1113, 1086, 946, 168, 920, 571, 65, 917, 254, 94, 851, 1172, 147, 32, 771, 106, 1095, 138, 875, 1033, 150, 3]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8561331982638825
the save name prefix for this run is:  chkpt-ID_8561331982638825_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 802
rank avg (pred): 0.561 +- 0.006
mrr vals (pred, true): 0.017, 0.055
batch losses (mrrl, rdl): 0.0, 0.0002674309

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 307
rank avg (pred): 0.063 +- 0.039
mrr vals (pred, true): 0.250, 0.443
batch losses (mrrl, rdl): 0.0, 1.4549e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 14
rank avg (pred): 0.055 +- 0.048
mrr vals (pred, true): 0.418, 0.445
batch losses (mrrl, rdl): 0.0, 3.396e-07

Epoch over!
epoch time: 11.977

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 706
rank avg (pred): 0.275 +- 0.235
mrr vals (pred, true): 0.291, 0.060
batch losses (mrrl, rdl): 0.0, 0.0004204149

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 357
rank avg (pred): 0.267 +- 0.230
mrr vals (pred, true): 0.303, 0.225
batch losses (mrrl, rdl): 0.0, 0.000734293

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 145
rank avg (pred): 0.259 +- 0.220
mrr vals (pred, true): 0.292, 0.256
batch losses (mrrl, rdl): 0.0, 0.0007638203

Epoch over!
epoch time: 11.92

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 611
rank avg (pred): 0.252 +- 0.217
mrr vals (pred, true): 0.303, 0.215
batch losses (mrrl, rdl): 0.0, 0.000477747

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 572
rank avg (pred): 0.279 +- 0.229
mrr vals (pred, true): 0.260, 0.226
batch losses (mrrl, rdl): 0.0, 0.0006977093

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 454
rank avg (pred): 0.239 +- 0.222
mrr vals (pred, true): 0.359, 0.054
batch losses (mrrl, rdl): 0.0, 0.0007609678

Epoch over!
epoch time: 11.835

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 389
rank avg (pred): 0.246 +- 0.222
mrr vals (pred, true): 0.334, 0.241
batch losses (mrrl, rdl): 0.0, 0.0006140207

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 765
rank avg (pred): 0.418 +- 0.322
mrr vals (pred, true): 0.242, 0.054
batch losses (mrrl, rdl): 0.0, 2.44334e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 470
rank avg (pred): 0.292 +- 0.241
mrr vals (pred, true): 0.270, 0.053
batch losses (mrrl, rdl): 0.0, 0.0004185284

Epoch over!
epoch time: 11.618

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 14
rank avg (pred): 0.079 +- 0.074
mrr vals (pred, true): 0.430, 0.445
batch losses (mrrl, rdl): 0.0, 1.62352e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 496
rank avg (pred): 0.066 +- 0.064
mrr vals (pred, true): 0.463, 0.401
batch losses (mrrl, rdl): 0.0, 1.275e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 567
rank avg (pred): 0.263 +- 0.233
mrr vals (pred, true): 0.307, 0.229
batch losses (mrrl, rdl): 0.0, 0.0007197667

Epoch over!
epoch time: 11.985

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1020
rank avg (pred): 0.255 +- 0.235
mrr vals (pred, true): 0.345, 0.267
batch losses (mrrl, rdl): 0.0607361048, 0.0007243925

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 681
rank avg (pred): 0.332 +- 0.196
mrr vals (pred, true): 0.123, 0.051
batch losses (mrrl, rdl): 0.0535355322, 0.0002831047

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 989
rank avg (pred): 0.028 +- 0.020
mrr vals (pred, true): 0.401, 0.468
batch losses (mrrl, rdl): 0.0443556719, 1.5075e-05

Epoch over!
epoch time: 12.244

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 850
rank avg (pred): 0.606 +- 0.288
mrr vals (pred, true): 0.090, 0.050
batch losses (mrrl, rdl): 0.0156615283, 0.0004583804

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 765
rank avg (pred): 0.710 +- 0.255
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 1.9463e-05, 0.001348905

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 863
rank avg (pred): 0.722 +- 0.243
mrr vals (pred, true): 0.044, 0.051
batch losses (mrrl, rdl): 0.0004129241, 0.0012459013

Epoch over!
epoch time: 12.197

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 341
rank avg (pred): 0.330 +- 0.207
mrr vals (pred, true): 0.143, 0.237
batch losses (mrrl, rdl): 0.087059468, 0.0013376788

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 512
rank avg (pred): 0.049 +- 0.035
mrr vals (pred, true): 0.352, 0.366
batch losses (mrrl, rdl): 0.001874437, 6.6526e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1018
rank avg (pred): 0.330 +- 0.209
mrr vals (pred, true): 0.141, 0.242
batch losses (mrrl, rdl): 0.102097176, 0.0011304301

Epoch over!
epoch time: 12.08

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1197
rank avg (pred): 0.333 +- 0.210
mrr vals (pred, true): 0.148, 0.056
batch losses (mrrl, rdl): 0.0957744718, 0.0002496672

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 596
rank avg (pred): 0.354 +- 0.195
mrr vals (pred, true): 0.105, 0.207
batch losses (mrrl, rdl): 0.104710944, 0.001062936

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 445
rank avg (pred): 0.329 +- 0.207
mrr vals (pred, true): 0.148, 0.055
batch losses (mrrl, rdl): 0.0959350169, 0.0003045162

Epoch over!
epoch time: 12.018

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 75
rank avg (pred): 0.023 +- 0.016
mrr vals (pred, true): 0.441, 0.428
batch losses (mrrl, rdl): 0.0017479078, 2.87265e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 687
rank avg (pred): 0.334 +- 0.205
mrr vals (pred, true): 0.140, 0.058
batch losses (mrrl, rdl): 0.0818134397, 0.0002410007

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 554
rank avg (pred): 0.031 +- 0.023
mrr vals (pred, true): 0.411, 0.395
batch losses (mrrl, rdl): 0.0026048659, 2.05047e-05

Epoch over!
epoch time: 12.12

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 281
rank avg (pred): 0.028 +- 0.019
mrr vals (pred, true): 0.412, 0.433
batch losses (mrrl, rdl): 0.0041479636, 1.75814e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 957
rank avg (pred): 0.557 +- 0.223
mrr vals (pred, true): 0.079, 0.052
batch losses (mrrl, rdl): 0.0083528589, 0.0002243054

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 652
rank avg (pred): 0.326 +- 0.201
mrr vals (pred, true): 0.160, 0.059
batch losses (mrrl, rdl): 0.1216994077, 0.0002540526

Epoch over!
epoch time: 12.219

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 18
rank avg (pred): 0.023 +- 0.017
mrr vals (pred, true): 0.458, 0.433
batch losses (mrrl, rdl): 0.0063373735, 2.38659e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 864
rank avg (pred): 0.682 +- 0.172
mrr vals (pred, true): 0.030, 0.048
batch losses (mrrl, rdl): 0.00406363, 0.0009277058

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 116
rank avg (pred): 0.318 +- 0.200
mrr vals (pred, true): 0.151, 0.180
batch losses (mrrl, rdl): 0.0084046163, 0.0004052138

Epoch over!
epoch time: 12.102

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 505
rank avg (pred): 0.048 +- 0.033
mrr vals (pred, true): 0.336, 0.405
batch losses (mrrl, rdl): 0.0476555489, 3.0948e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 100
rank avg (pred): 0.326 +- 0.197
mrr vals (pred, true): 0.149, 0.273
batch losses (mrrl, rdl): 0.1528428793, 0.0011432122

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 513
rank avg (pred): 0.047 +- 0.034
mrr vals (pred, true): 0.352, 0.355
batch losses (mrrl, rdl): 5.68932e-05, 1.07836e-05

Epoch over!
epoch time: 12.348

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 357
rank avg (pred): 0.315 +- 0.200
mrr vals (pred, true): 0.156, 0.225
batch losses (mrrl, rdl): 0.0479444191, 0.0010858745

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 819
rank avg (pred): 0.035 +- 0.025
mrr vals (pred, true): 0.386, 0.413
batch losses (mrrl, rdl): 0.007473873, 9.8869e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1160
rank avg (pred): 0.030 +- 0.021
mrr vals (pred, true): 0.414, 0.371
batch losses (mrrl, rdl): 0.0181389209, 2.61707e-05

Epoch over!
epoch time: 11.936

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 827
rank avg (pred): 0.072 +- 0.051
mrr vals (pred, true): 0.301, 0.333
batch losses (mrrl, rdl): 0.0104021356, 4.208e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 56
rank avg (pred): 0.031 +- 0.022
mrr vals (pred, true): 0.402, 0.419
batch losses (mrrl, rdl): 0.0030433524, 1.34426e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 141
rank avg (pred): 0.309 +- 0.198
mrr vals (pred, true): 0.159, 0.237
batch losses (mrrl, rdl): 0.0607016385, 0.000989939

Epoch over!
epoch time: 11.98

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.328 +- 0.187
mrr vals (pred, true): 0.125, 0.053

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.10837 	 0.04542 	 m..s
    4 	     1 	 0.04453 	 0.04608 	 ~...
   14 	     2 	 0.06251 	 0.04771 	 ~...
    3 	     3 	 0.04419 	 0.04779 	 ~...
    6 	     4 	 0.04818 	 0.04803 	 ~...
   54 	     5 	 0.12709 	 0.04803 	 m..s
    0 	     6 	 0.03832 	 0.04836 	 ~...
   21 	     7 	 0.10729 	 0.04963 	 m..s
    5 	     8 	 0.04458 	 0.05000 	 ~...
   18 	     9 	 0.10417 	 0.05029 	 m..s
   10 	    10 	 0.04977 	 0.05075 	 ~...
    8 	    11 	 0.04928 	 0.05087 	 ~...
   16 	    12 	 0.10212 	 0.05108 	 m..s
   36 	    13 	 0.12525 	 0.05113 	 m..s
    7 	    14 	 0.04884 	 0.05126 	 ~...
   30 	    15 	 0.12039 	 0.05130 	 m..s
    9 	    16 	 0.04964 	 0.05131 	 ~...
   75 	    17 	 0.13723 	 0.05149 	 m..s
   14 	    18 	 0.06251 	 0.05162 	 ~...
   36 	    19 	 0.12525 	 0.05182 	 m..s
   25 	    20 	 0.11131 	 0.05196 	 m..s
   20 	    21 	 0.10723 	 0.05234 	 m..s
   36 	    22 	 0.12525 	 0.05268 	 m..s
    2 	    23 	 0.04413 	 0.05270 	 ~...
   68 	    24 	 0.13215 	 0.05276 	 m..s
   12 	    25 	 0.06070 	 0.05284 	 ~...
    1 	    26 	 0.04399 	 0.05325 	 ~...
   36 	    27 	 0.12525 	 0.05342 	 m..s
   36 	    28 	 0.12525 	 0.05379 	 m..s
   57 	    29 	 0.12853 	 0.05429 	 m..s
   66 	    30 	 0.13176 	 0.05456 	 m..s
   36 	    31 	 0.12525 	 0.05502 	 m..s
   21 	    32 	 0.10729 	 0.05541 	 m..s
   73 	    33 	 0.13478 	 0.05543 	 m..s
   13 	    34 	 0.06149 	 0.05552 	 ~...
   63 	    35 	 0.12977 	 0.05595 	 m..s
   17 	    36 	 0.10400 	 0.05625 	 m..s
   11 	    37 	 0.05014 	 0.05668 	 ~...
   59 	    38 	 0.12865 	 0.05745 	 m..s
   66 	    39 	 0.13176 	 0.05760 	 m..s
   34 	    40 	 0.12298 	 0.05795 	 m..s
   75 	    41 	 0.13723 	 0.05854 	 m..s
   30 	    42 	 0.12039 	 0.05880 	 m..s
   79 	    43 	 0.16582 	 0.19851 	 m..s
   26 	    44 	 0.11171 	 0.20349 	 m..s
   70 	    45 	 0.13364 	 0.20531 	 m..s
   56 	    46 	 0.12817 	 0.21795 	 m..s
   27 	    47 	 0.11177 	 0.21829 	 MISS
   50 	    48 	 0.12637 	 0.21871 	 m..s
   36 	    49 	 0.12525 	 0.22482 	 m..s
   51 	    50 	 0.12671 	 0.22593 	 m..s
   36 	    51 	 0.12525 	 0.22897 	 MISS
   64 	    52 	 0.13057 	 0.22899 	 m..s
   27 	    53 	 0.11177 	 0.23128 	 MISS
   70 	    54 	 0.13364 	 0.23134 	 m..s
   65 	    55 	 0.13085 	 0.23238 	 MISS
   36 	    56 	 0.12525 	 0.23314 	 MISS
   36 	    57 	 0.12525 	 0.23322 	 MISS
   32 	    58 	 0.12121 	 0.23527 	 MISS
   36 	    59 	 0.12525 	 0.23724 	 MISS
   36 	    60 	 0.12525 	 0.23736 	 MISS
   74 	    61 	 0.13722 	 0.23749 	 MISS
   69 	    62 	 0.13284 	 0.24098 	 MISS
   36 	    63 	 0.12525 	 0.24241 	 MISS
   35 	    64 	 0.12381 	 0.24499 	 MISS
   53 	    65 	 0.12693 	 0.24764 	 MISS
   33 	    66 	 0.12126 	 0.24799 	 MISS
   55 	    67 	 0.12753 	 0.24830 	 MISS
   19 	    68 	 0.10471 	 0.24903 	 MISS
   52 	    69 	 0.12693 	 0.25083 	 MISS
   58 	    70 	 0.12862 	 0.25499 	 MISS
   62 	    71 	 0.12935 	 0.25634 	 MISS
   60 	    72 	 0.12901 	 0.25896 	 MISS
   36 	    73 	 0.12525 	 0.26251 	 MISS
   72 	    74 	 0.13395 	 0.26274 	 MISS
   61 	    75 	 0.12913 	 0.26410 	 MISS
   29 	    76 	 0.11511 	 0.26671 	 MISS
   24 	    77 	 0.11040 	 0.27501 	 MISS
   78 	    78 	 0.14140 	 0.30657 	 MISS
   77 	    79 	 0.13928 	 0.33058 	 MISS
   80 	    80 	 0.31639 	 0.33846 	 ~...
   84 	    81 	 0.35919 	 0.34551 	 ~...
   85 	    82 	 0.35946 	 0.35294 	 ~...
   92 	    83 	 0.36595 	 0.36109 	 ~...
   93 	    84 	 0.36622 	 0.36206 	 ~...
   83 	    85 	 0.35728 	 0.36692 	 ~...
   90 	    86 	 0.36513 	 0.36721 	 ~...
   91 	    87 	 0.36540 	 0.37045 	 ~...
   85 	    88 	 0.35946 	 0.37213 	 ~...
   89 	    89 	 0.36381 	 0.37516 	 ~...
   95 	    90 	 0.36666 	 0.38170 	 ~...
   97 	    91 	 0.38681 	 0.39489 	 ~...
   87 	    92 	 0.36241 	 0.39610 	 m..s
   94 	    93 	 0.36628 	 0.39788 	 m..s
   82 	    94 	 0.35700 	 0.39852 	 m..s
   80 	    95 	 0.31639 	 0.41259 	 m..s
  114 	    96 	 0.44578 	 0.41396 	 m..s
  105 	    97 	 0.43502 	 0.43162 	 ~...
  110 	    98 	 0.44018 	 0.43180 	 ~...
   96 	    99 	 0.37180 	 0.43199 	 m..s
  104 	   100 	 0.43132 	 0.43446 	 ~...
  113 	   101 	 0.44545 	 0.43581 	 ~...
   99 	   102 	 0.42477 	 0.43650 	 ~...
   99 	   103 	 0.42477 	 0.43964 	 ~...
  103 	   104 	 0.42976 	 0.43967 	 ~...
   88 	   105 	 0.36282 	 0.44006 	 m..s
  111 	   106 	 0.44540 	 0.44087 	 ~...
  120 	   107 	 0.45652 	 0.44123 	 ~...
  102 	   108 	 0.42938 	 0.44306 	 ~...
  118 	   109 	 0.45513 	 0.44370 	 ~...
  115 	   110 	 0.44715 	 0.44408 	 ~...
  117 	   111 	 0.45308 	 0.44580 	 ~...
  101 	   112 	 0.42775 	 0.44612 	 ~...
  111 	   113 	 0.44540 	 0.44625 	 ~...
  119 	   114 	 0.45620 	 0.44777 	 ~...
  106 	   115 	 0.43548 	 0.44803 	 ~...
  108 	   116 	 0.43914 	 0.45134 	 ~...
  107 	   117 	 0.43768 	 0.45790 	 ~...
   98 	   118 	 0.42245 	 0.46171 	 m..s
  116 	   119 	 0.44965 	 0.47042 	 ~...
  109 	   120 	 0.43989 	 0.47952 	 m..s
==========================================
r_mrr = 0.88810133934021
r2_mrr = 0.7610737085342407
spearmanr_mrr@5 = 0.8908988237380981
spearmanr_mrr@10 = 0.9358575940132141
spearmanr_mrr@50 = 0.9637129902839661
spearmanr_mrr@100 = 0.8811726570129395
spearmanr_mrr@All = 0.9058139324188232
==========================================
test time: 0.532
Done Testing dataset Kinships
total time taken: 187.86169242858887
training time taken: 181.17496299743652
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8881)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7611)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.8909)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9359)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9637)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8812)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9058)}}, 'test_loss': {'DistMult': {'Kinships': 7.060499010192871}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 4375677851112455
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [766, 693, 118, 763, 911, 256, 964, 471, 1144, 803, 167, 137, 1209, 470, 609, 392, 613, 337, 1192, 238, 1107, 413, 204, 14, 1012, 788, 926, 1203, 274, 243, 192, 543, 946, 93, 621, 320, 864, 229, 833, 450, 524, 90, 948, 1199, 25, 1128, 91, 82, 252, 399, 1156, 357, 684, 67, 520, 319, 504, 844, 205, 460, 43, 1173, 949, 15, 677, 1093, 87, 745, 1179, 944, 170, 221, 950, 690, 233, 922, 441, 1070, 861, 1024, 440, 588, 374, 388, 415, 899, 641, 919, 521, 531, 110, 626, 900, 152, 642, 1044, 1150, 1050, 532, 558, 29, 1076, 659, 390, 97, 294, 56, 369, 828, 222, 8, 863, 1029, 234, 1153, 1064, 420, 115, 161, 476, 187]
valid_ids (0): []
train_ids (1094): [489, 116, 757, 462, 630, 910, 160, 789, 815, 13, 829, 315, 723, 683, 493, 191, 977, 1102, 992, 33, 104, 638, 831, 625, 1043, 172, 548, 434, 920, 1084, 656, 624, 77, 184, 74, 954, 961, 1213, 96, 571, 179, 506, 726, 879, 345, 32, 376, 658, 366, 296, 819, 542, 249, 1014, 875, 494, 153, 704, 851, 469, 27, 248, 1101, 417, 1211, 368, 26, 426, 1187, 837, 63, 1138, 956, 564, 724, 279, 147, 986, 303, 173, 485, 232, 46, 528, 208, 589, 181, 34, 874, 502, 927, 406, 79, 696, 50, 397, 869, 212, 935, 411, 144, 557, 1158, 0, 583, 446, 496, 1113, 495, 422, 44, 353, 1178, 908, 1034, 577, 231, 714, 99, 546, 673, 442, 1124, 391, 481, 563, 759, 663, 322, 982, 674, 1066, 362, 482, 53, 1004, 611, 628, 1016, 281, 377, 686, 227, 605, 891, 796, 1106, 744, 890, 808, 746, 848, 1055, 1205, 1017, 769, 710, 240, 403, 459, 916, 1176, 511, 360, 801, 1167, 1127, 750, 645, 1114, 551, 523, 675, 235, 777, 62, 270, 966, 479, 258, 639, 340, 824, 1149, 1202, 794, 351, 226, 718, 717, 1069, 720, 338, 687, 1189, 994, 145, 318, 702, 1090, 293, 995, 291, 58, 335, 1036, 45, 664, 66, 509, 568, 594, 703, 164, 384, 804, 467, 188, 355, 484, 739, 151, 847, 1117, 530, 514, 540, 1032, 218, 1181, 572, 533, 267, 885, 845, 122, 522, 197, 271, 1091, 253, 40, 342, 990, 195, 70, 387, 933, 76, 826, 289, 1033, 807, 1077, 1078, 903, 92, 461, 963, 284, 883, 1135, 297, 955, 1056, 945, 786, 823, 725, 858, 358, 535, 402, 134, 456, 250, 644, 1162, 449, 872, 1061, 326, 298, 17, 778, 57, 756, 21, 1018, 758, 580, 1147, 119, 660, 359, 854, 1040, 740, 55, 816, 817, 574, 1104, 860, 443, 925, 380, 1155, 216, 1054, 830, 220, 373, 561, 1109, 131, 1201, 1180, 203, 246, 810, 813, 1120, 892, 190, 427, 913, 1129, 12, 356, 841, 873, 593, 38, 219, 1152, 775, 1208, 202, 343, 884, 898, 47, 616, 587, 214, 404, 738, 1049, 959, 431, 19, 239, 755, 1000, 194, 1099, 301, 870, 310, 51, 676, 478, 283, 378, 230, 20, 1086, 1140, 1125, 410, 261, 632, 562, 1068, 1072, 748, 727, 1087, 1142, 712, 16, 513, 897, 1134, 1067, 608, 61, 312, 454, 1133, 553, 189, 398, 780, 527, 407, 773, 277, 555, 1071, 163, 1214, 401, 695, 706, 975, 694, 464, 1042, 365, 1110, 701, 762, 272, 567, 128, 818, 886, 1059, 856, 602, 811, 41, 610, 855, 107, 790, 490, 412, 1053, 405, 1019, 425, 1011, 947, 1141, 1168, 306, 6, 73, 688, 1123, 1, 336, 1210, 843, 1145, 953, 893, 499, 1166, 505, 581, 154, 123, 765, 809, 988, 680, 483, 1185, 622, 103, 1163, 466, 767, 734, 924, 868, 985, 732, 211, 1108, 733, 722, 1175, 165, 1160, 1206, 619, 106, 223, 741, 938, 849, 965, 242, 797, 941, 292, 199, 772, 132, 1157, 10, 127, 1001, 3, 595, 64, 1008, 1174, 550, 286, 636, 997, 1028, 1082, 547, 236, 314, 751, 842, 503, 175, 308, 182, 168, 633, 501, 1080, 263, 1195, 447, 853, 171, 709, 876, 95, 678, 1085, 381, 882, 1021, 386, 1146, 350, 515, 1052, 278, 918, 24, 259, 1047, 54, 200, 812, 105, 715, 670, 692, 668, 598, 582, 288, 993, 614, 244, 371, 299, 795, 254, 917, 112, 429, 266, 325, 183, 480, 48, 661, 928, 984, 685, 970, 423, 1184, 643, 549, 921, 649, 554, 782, 363, 1046, 1062, 737, 241, 474, 1188, 419, 237, 518, 1172, 1030, 108, 80, 42, 539, 75, 156, 1015, 1005, 68, 367, 541, 498, 689, 681, 721, 951, 418, 881, 409, 859, 455, 1137, 573, 9, 749, 623, 979, 383, 1190, 565, 344, 618, 408, 37, 719, 120, 827, 331, 1165, 300, 705, 143, 81, 617, 937, 389, 545, 177, 1177, 525, 421, 822, 586, 436, 728, 468, 743, 1026, 1207, 1079, 771, 711, 334, 1009, 761, 650, 973, 430, 193, 850, 129, 433, 1139, 840, 109, 754, 1048, 287, 372, 862, 89, 747, 393, 671, 1027, 396, 457, 1023, 285, 1025, 987, 814, 328, 902, 983, 329, 30, 176, 1170, 1031, 836, 1081, 534, 1045, 1111, 735, 126, 615, 834, 1075, 996, 731, 787, 276, 140, 437, 1089, 121, 569, 206, 1197, 472, 1094, 634, 185, 85, 273, 475, 354, 764, 1041, 867, 395, 969, 888, 364, 575, 217, 280, 665, 84, 71, 1095, 11, 631, 566, 215, 497, 1010, 784, 257, 774, 930, 7, 321, 957, 444, 800, 166, 915, 894, 22, 646, 652, 559, 976, 117, 225, 904, 1060, 648, 114, 83, 486, 439, 640, 552, 1191, 835, 31, 150, 463, 180, 1171, 607, 453, 962, 1154, 667, 700, 228, 1151, 72, 793, 591, 932, 972, 906, 912, 544, 346, 1115, 538, 247, 414, 1051, 1148, 1013, 201, 999, 260, 556, 139, 779, 776, 465, 596, 1161, 302, 579, 989, 560, 60, 488, 852, 791, 599, 698, 901, 1169, 317, 923, 783, 1037, 458, 1200, 473, 432, 36, 451, 162, 305, 785, 135, 939, 974, 752, 500, 174, 931, 313, 304, 604, 59, 1002, 895, 347, 39, 4, 207, 736, 213, 662, 603, 1121, 1204, 1182, 1196, 512, 600, 936, 635, 657, 1022, 86, 1136, 1007, 295, 590, 576, 877, 1100, 5, 282, 1198, 730, 379, 536, 978, 169, 487, 210, 352, 1088, 49, 341, 492, 517, 1035, 424, 1112, 529, 111, 307, 316, 245, 196, 713, 88, 510, 805, 375, 654, 349, 339, 35, 52, 1131, 914, 729, 311, 265, 1186, 753, 78, 142, 760, 209, 871, 971, 124, 940, 1116, 255, 846, 130, 102, 672, 98, 627, 133, 157, 100, 1126, 125, 839, 998, 620, 155, 251, 880, 23, 323, 327, 943, 991, 825, 348, 1130, 1098, 798, 178, 1183, 275, 262, 647, 802, 1003, 428, 1083, 1057, 448, 697, 1063, 101, 332, 186, 578, 768, 361, 968, 149, 682, 1159, 400, 1038, 585, 929, 597, 94, 669, 1122, 1006, 1074, 435, 1058, 570, 1096, 896, 1118, 1193, 980, 1039, 18, 1073, 1020, 1105, 832, 507, 148, 1065, 141, 519, 28, 651, 438, 820, 821, 606, 909, 477, 333, 416, 445, 792, 136, 981, 65, 866, 1164, 907, 770, 1194, 592, 264, 865, 290, 691, 526, 370, 806, 146, 699, 781, 666, 269, 878, 508, 934, 708, 838, 516, 1103, 612, 491, 679, 452, 742, 158, 537, 159, 138, 113, 889, 653, 637, 707, 1092, 584, 655, 601, 960, 324, 2, 952, 716, 198, 958, 905, 1097, 394, 385, 1119, 309, 1132, 857, 967, 1212, 799, 224, 629, 1143, 69, 382, 268, 887, 330, 942]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3655748739255678
the save name prefix for this run is:  chkpt-ID_3655748739255678_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 13
rank avg (pred): 0.405 +- 0.008
mrr vals (pred, true): 0.023, 0.435
batch losses (mrrl, rdl): 0.0, 0.002628407

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 416
rank avg (pred): 0.249 +- 0.031
mrr vals (pred, true): 0.038, 0.054
batch losses (mrrl, rdl): 0.0, 0.0011064642

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1151
rank avg (pred): 0.108 +- 0.010
mrr vals (pred, true): 0.083, 0.398
batch losses (mrrl, rdl): 0.0, 4.61433e-05

Epoch over!
epoch time: 12.01

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 596
rank avg (pred): 0.252 +- 0.017
mrr vals (pred, true): 0.037, 0.207
batch losses (mrrl, rdl): 0.0, 0.0002579967

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 750
rank avg (pred): 0.118 +- 0.002
mrr vals (pred, true): 0.076, 0.387
batch losses (mrrl, rdl): 0.0, 6.21567e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 182
rank avg (pred): 0.306 +- 0.004
mrr vals (pred, true): 0.031, 0.057
batch losses (mrrl, rdl): 0.0, 0.0006436428

Epoch over!
epoch time: 11.745

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 372
rank avg (pred): 0.297 +- 0.007
mrr vals (pred, true): 0.032, 0.265
batch losses (mrrl, rdl): 0.0, 0.0008046297

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 449
rank avg (pred): 0.290 +- 0.005
mrr vals (pred, true): 0.032, 0.049
batch losses (mrrl, rdl): 0.0, 0.0008323322

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 331
rank avg (pred): 0.315 +- 0.002
mrr vals (pred, true): 0.030, 0.209
batch losses (mrrl, rdl): 0.0, 0.0008044868

Epoch over!
epoch time: 11.703

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1136
rank avg (pred): 0.073 +- 0.000
mrr vals (pred, true): 0.118, 0.409
batch losses (mrrl, rdl): 0.0, 3.7457e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 410
rank avg (pred): 0.294 +- 0.002
mrr vals (pred, true): 0.032, 0.058
batch losses (mrrl, rdl): 0.0, 0.0006903249

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 272
rank avg (pred): 0.036 +- 0.000
mrr vals (pred, true): 0.213, 0.434
batch losses (mrrl, rdl): 0.0, 8.52e-06

Epoch over!
epoch time: 11.849

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 826
rank avg (pred): 0.142 +- 0.000
mrr vals (pred, true): 0.064, 0.203
batch losses (mrrl, rdl): 0.0, 2.51277e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 561
rank avg (pred): 0.066 +- 0.000
mrr vals (pred, true): 0.128, 0.353
batch losses (mrrl, rdl): 0.0, 2.3338e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 302
rank avg (pred): 0.021 +- 0.000
mrr vals (pred, true): 0.321, 0.414
batch losses (mrrl, rdl): 0.0, 2.82835e-05

Epoch over!
epoch time: 11.836

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 395
rank avg (pred): 0.260 +- 0.000
mrr vals (pred, true): 0.036, 0.254
batch losses (mrrl, rdl): 0.4774492681, 0.0005333307

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 547
rank avg (pred): 0.010 +- 0.006
mrr vals (pred, true): 0.530, 0.380
batch losses (mrrl, rdl): 0.223852694, 6.24418e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 708
rank avg (pred): 0.108 +- 0.058
mrr vals (pred, true): 0.165, 0.058
batch losses (mrrl, rdl): 0.1311422586, 0.0026595362

Epoch over!
epoch time: 12.18

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1091
rank avg (pred): 0.124 +- 0.050
mrr vals (pred, true): 0.107, 0.223
batch losses (mrrl, rdl): 0.1338288188, 5.9581e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 406
rank avg (pred): 0.130 +- 0.060
mrr vals (pred, true): 0.120, 0.054
batch losses (mrrl, rdl): 0.048655007, 0.0022424657

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 345
rank avg (pred): 0.117 +- 0.060
mrr vals (pred, true): 0.155, 0.238
batch losses (mrrl, rdl): 0.0686708242, 2.2343e-06

Epoch over!
epoch time: 11.969

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1148
rank avg (pred): 0.031 +- 0.018
mrr vals (pred, true): 0.342, 0.388
batch losses (mrrl, rdl): 0.0211208127, 2.31509e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 643
rank avg (pred): 0.125 +- 0.067
mrr vals (pred, true): 0.161, 0.239
batch losses (mrrl, rdl): 0.0613218732, 6.3978e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 203
rank avg (pred): 0.130 +- 0.068
mrr vals (pred, true): 0.144, 0.053
batch losses (mrrl, rdl): 0.0886301398, 0.0023135454

Epoch over!
epoch time: 12.088

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 257
rank avg (pred): 0.018 +- 0.010
mrr vals (pred, true): 0.444, 0.434
batch losses (mrrl, rdl): 0.0008828009, 3.29848e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 795
rank avg (pred): 0.342 +- 0.139
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0005036802, 0.0003554086

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1175
rank avg (pred): 0.159 +- 0.070
mrr vals (pred, true): 0.112, 0.264
batch losses (mrrl, rdl): 0.2313542962, 0.0001017967

Epoch over!
epoch time: 11.898

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 380
rank avg (pred): 0.117 +- 0.069
mrr vals (pred, true): 0.194, 0.192
batch losses (mrrl, rdl): 5.18385e-05, 2.59103e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 603
rank avg (pred): 0.155 +- 0.077
mrr vals (pred, true): 0.137, 0.224
batch losses (mrrl, rdl): 0.0754738599, 2.77318e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 473
rank avg (pred): 0.148 +- 0.074
mrr vals (pred, true): 0.134, 0.050
batch losses (mrrl, rdl): 0.0704783723, 0.0022576831

Epoch over!
epoch time: 12.115

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 725
rank avg (pred): 0.155 +- 0.071
mrr vals (pred, true): 0.123, 0.052
batch losses (mrrl, rdl): 0.0534380414, 0.0020593975

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 428
rank avg (pred): 0.125 +- 0.074
mrr vals (pred, true): 0.194, 0.051
batch losses (mrrl, rdl): 0.2060837746, 0.0023680681

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 92
rank avg (pred): 0.165 +- 0.076
mrr vals (pred, true): 0.117, 0.239
batch losses (mrrl, rdl): 0.1479250342, 9.59283e-05

Epoch over!
epoch time: 12.426

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 660
rank avg (pred): 0.150 +- 0.080
mrr vals (pred, true): 0.151, 0.057
batch losses (mrrl, rdl): 0.102860041, 0.001867021

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 796
rank avg (pred): 0.435 +- 0.164
mrr vals (pred, true): 0.047, 0.050
batch losses (mrrl, rdl): 7.63408e-05, 4.22835e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 103
rank avg (pred): 0.152 +- 0.081
mrr vals (pred, true): 0.155, 0.246
batch losses (mrrl, rdl): 0.0819973797, 8.16659e-05

Epoch over!
epoch time: 12.095

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 567
rank avg (pred): 0.159 +- 0.083
mrr vals (pred, true): 0.144, 0.229
batch losses (mrrl, rdl): 0.0732074976, 7.32188e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 193
rank avg (pred): 0.156 +- 0.083
mrr vals (pred, true): 0.150, 0.062
batch losses (mrrl, rdl): 0.1001037806, 0.0018971305

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 313
rank avg (pred): 0.012 +- 0.007
mrr vals (pred, true): 0.509, 0.438
batch losses (mrrl, rdl): 0.0510915704, 3.99111e-05

Epoch over!
epoch time: 12.108

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 438
rank avg (pred): 0.145 +- 0.082
mrr vals (pred, true): 0.171, 0.054
batch losses (mrrl, rdl): 0.1469493657, 0.0020376774

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1123
rank avg (pred): 0.161 +- 0.081
mrr vals (pred, true): 0.137, 0.055
batch losses (mrrl, rdl): 0.0753557384, 0.0019241158

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1078
rank avg (pred): 0.015 +- 0.008
mrr vals (pred, true): 0.468, 0.452
batch losses (mrrl, rdl): 0.0025328835, 3.50753e-05

Epoch over!
epoch time: 12.018

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 63
rank avg (pred): 0.024 +- 0.013
mrr vals (pred, true): 0.382, 0.448
batch losses (mrrl, rdl): 0.0433496162, 2.58986e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 50
rank avg (pred): 0.030 +- 0.016
mrr vals (pred, true): 0.333, 0.446
batch losses (mrrl, rdl): 0.1278585792, 1.27827e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 83
rank avg (pred): 0.182 +- 0.081
mrr vals (pred, true): 0.115, 0.203
batch losses (mrrl, rdl): 0.0784615576, 9.39455e-05

Epoch over!
epoch time: 11.964

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.469 +- 0.178
mrr vals (pred, true): 0.052, 0.059

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.12371 	 0.02078 	 MISS
   16 	     1 	 0.05223 	 0.04321 	 ~...
   58 	     2 	 0.13482 	 0.04634 	 m..s
    0 	     3 	 0.05189 	 0.04694 	 ~...
    0 	     4 	 0.05189 	 0.04701 	 ~...
   58 	     5 	 0.13482 	 0.04768 	 m..s
    0 	     6 	 0.05189 	 0.04811 	 ~...
    0 	     7 	 0.05189 	 0.04832 	 ~...
    0 	     8 	 0.05189 	 0.04833 	 ~...
    0 	     9 	 0.05189 	 0.04945 	 ~...
   37 	    10 	 0.12961 	 0.04950 	 m..s
   61 	    11 	 0.13486 	 0.04950 	 m..s
    0 	    12 	 0.05189 	 0.04957 	 ~...
   55 	    13 	 0.13430 	 0.04962 	 m..s
    0 	    14 	 0.05189 	 0.04970 	 ~...
    0 	    15 	 0.05189 	 0.04975 	 ~...
   47 	    16 	 0.13108 	 0.04979 	 m..s
    0 	    17 	 0.05189 	 0.05022 	 ~...
   62 	    18 	 0.13487 	 0.05079 	 m..s
   21 	    19 	 0.12592 	 0.05101 	 m..s
   15 	    20 	 0.05193 	 0.05112 	 ~...
    0 	    21 	 0.05189 	 0.05114 	 ~...
   51 	    22 	 0.13236 	 0.05136 	 m..s
   27 	    23 	 0.12897 	 0.05140 	 m..s
   66 	    24 	 0.13507 	 0.05150 	 m..s
   64 	    25 	 0.13497 	 0.05168 	 m..s
    0 	    26 	 0.05189 	 0.05178 	 ~...
   23 	    27 	 0.12868 	 0.05186 	 m..s
   44 	    28 	 0.13081 	 0.05233 	 m..s
   82 	    29 	 0.13761 	 0.05234 	 m..s
    0 	    30 	 0.05189 	 0.05252 	 ~...
   56 	    31 	 0.13456 	 0.05276 	 m..s
   63 	    32 	 0.13493 	 0.05284 	 m..s
   65 	    33 	 0.13503 	 0.05286 	 m..s
   81 	    34 	 0.13744 	 0.05297 	 m..s
   79 	    35 	 0.13685 	 0.05325 	 m..s
   75 	    36 	 0.13587 	 0.05337 	 m..s
   73 	    37 	 0.13551 	 0.05340 	 m..s
    0 	    38 	 0.05189 	 0.05365 	 ~...
   57 	    39 	 0.13469 	 0.05398 	 m..s
   18 	    40 	 0.12533 	 0.05464 	 m..s
   35 	    41 	 0.12952 	 0.05483 	 m..s
   78 	    42 	 0.13679 	 0.05515 	 m..s
   52 	    43 	 0.13285 	 0.05543 	 m..s
   60 	    44 	 0.13484 	 0.05570 	 m..s
   43 	    45 	 0.13074 	 0.05612 	 m..s
   80 	    46 	 0.13730 	 0.05745 	 m..s
   41 	    47 	 0.13030 	 0.05817 	 m..s
    0 	    48 	 0.05189 	 0.05878 	 ~...
   34 	    49 	 0.12943 	 0.05880 	 m..s
   39 	    50 	 0.13016 	 0.05917 	 m..s
   24 	    51 	 0.12882 	 0.05960 	 m..s
   31 	    52 	 0.12927 	 0.06074 	 m..s
   53 	    53 	 0.13333 	 0.06172 	 m..s
   30 	    54 	 0.12917 	 0.19116 	 m..s
   86 	    55 	 0.29012 	 0.19588 	 m..s
   85 	    56 	 0.13860 	 0.22524 	 m..s
   84 	    57 	 0.13814 	 0.22610 	 m..s
   67 	    58 	 0.13510 	 0.22613 	 m..s
   20 	    59 	 0.12576 	 0.22648 	 MISS
   19 	    60 	 0.12571 	 0.23149 	 MISS
   72 	    61 	 0.13542 	 0.23314 	 m..s
   45 	    62 	 0.13083 	 0.23426 	 MISS
   46 	    63 	 0.13101 	 0.23724 	 MISS
   28 	    64 	 0.12909 	 0.23823 	 MISS
   38 	    65 	 0.12969 	 0.24002 	 MISS
   68 	    66 	 0.13523 	 0.24073 	 MISS
   48 	    67 	 0.13113 	 0.24588 	 MISS
   26 	    68 	 0.12897 	 0.24699 	 MISS
   50 	    69 	 0.13162 	 0.24708 	 MISS
   22 	    70 	 0.12800 	 0.24784 	 MISS
   69 	    71 	 0.13529 	 0.24863 	 MISS
   49 	    72 	 0.13160 	 0.24880 	 MISS
   76 	    73 	 0.13605 	 0.25361 	 MISS
   83 	    74 	 0.13773 	 0.25634 	 MISS
   36 	    75 	 0.12956 	 0.25732 	 MISS
   71 	    76 	 0.13532 	 0.25732 	 MISS
   40 	    77 	 0.13022 	 0.25896 	 MISS
   42 	    78 	 0.13071 	 0.26165 	 MISS
   77 	    79 	 0.13616 	 0.26242 	 MISS
   32 	    80 	 0.12935 	 0.26274 	 MISS
   33 	    81 	 0.12936 	 0.26367 	 MISS
   74 	    82 	 0.13563 	 0.26501 	 MISS
   54 	    83 	 0.13367 	 0.26727 	 MISS
   28 	    84 	 0.12909 	 0.27026 	 MISS
   25 	    85 	 0.12897 	 0.27501 	 MISS
   69 	    86 	 0.13529 	 0.29102 	 MISS
   88 	    87 	 0.35264 	 0.34172 	 ~...
   94 	    88 	 0.39702 	 0.35878 	 m..s
   99 	    89 	 0.41945 	 0.36106 	 m..s
   97 	    90 	 0.41058 	 0.36285 	 m..s
   91 	    91 	 0.36135 	 0.36692 	 ~...
   99 	    92 	 0.41945 	 0.36721 	 m..s
   89 	    93 	 0.36108 	 0.37294 	 ~...
  102 	    94 	 0.42247 	 0.37342 	 m..s
  101 	    95 	 0.42219 	 0.38422 	 m..s
   89 	    96 	 0.36108 	 0.38493 	 ~...
   87 	    97 	 0.34880 	 0.38496 	 m..s
   98 	    98 	 0.41289 	 0.38747 	 ~...
   96 	    99 	 0.40875 	 0.39223 	 ~...
   92 	   100 	 0.38679 	 0.40353 	 ~...
  114 	   101 	 0.44687 	 0.41901 	 ~...
  117 	   102 	 0.45298 	 0.41934 	 m..s
  116 	   103 	 0.44935 	 0.42959 	 ~...
   93 	   104 	 0.39563 	 0.43199 	 m..s
  111 	   105 	 0.44085 	 0.43446 	 ~...
  110 	   106 	 0.44073 	 0.43783 	 ~...
   95 	   107 	 0.40322 	 0.44006 	 m..s
  113 	   108 	 0.44592 	 0.44052 	 ~...
  118 	   109 	 0.47283 	 0.44133 	 m..s
  115 	   110 	 0.44777 	 0.44226 	 ~...
  103 	   111 	 0.43341 	 0.44314 	 ~...
  107 	   112 	 0.43916 	 0.44414 	 ~...
  103 	   113 	 0.43341 	 0.44446 	 ~...
  105 	   114 	 0.43835 	 0.44527 	 ~...
  112 	   115 	 0.44230 	 0.44612 	 ~...
  119 	   116 	 0.47880 	 0.44770 	 m..s
  106 	   117 	 0.43883 	 0.44777 	 ~...
  108 	   118 	 0.43988 	 0.45100 	 ~...
  109 	   119 	 0.44036 	 0.45134 	 ~...
  120 	   120 	 0.48645 	 0.45760 	 ~...
==========================================
r_mrr = 0.86781907081604
r2_mrr = 0.7501546740531921
spearmanr_mrr@5 = 0.9001061916351318
spearmanr_mrr@10 = 0.9559636116027832
spearmanr_mrr@50 = 0.9751644134521484
spearmanr_mrr@100 = 0.8610839247703552
spearmanr_mrr@All = 0.8873698711395264
==========================================
test time: 0.394
Done Testing dataset Kinships
total time taken: 187.5408775806427
training time taken: 180.4562520980835
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8678)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7502)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9001)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9560)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9752)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8611)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.8874)}}, 'test_loss': {'DistMult': {'Kinships': 7.3612772561536985}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 7103102160345296
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [308, 400, 258, 1038, 728, 343, 1077, 815, 741, 870, 789, 49, 771, 172, 256, 92, 628, 98, 924, 743, 819, 222, 395, 833, 640, 567, 54, 1131, 662, 1031, 1035, 955, 500, 706, 476, 701, 788, 446, 173, 572, 1068, 451, 963, 947, 839, 1103, 139, 287, 317, 970, 578, 568, 398, 425, 87, 635, 782, 1189, 672, 683, 78, 834, 862, 613, 1181, 11, 390, 758, 916, 772, 365, 59, 43, 746, 176, 558, 803, 611, 295, 554, 522, 842, 826, 1139, 1098, 1022, 755, 1182, 273, 678, 379, 805, 145, 61, 666, 621, 345, 889, 1011, 203, 100, 363, 1057, 975, 501, 1119, 1095, 297, 417, 738, 253, 518, 1028, 1039, 385, 954, 602, 213, 670, 655, 749]
valid_ids (0): []
train_ids (1094): [1019, 813, 404, 1062, 966, 690, 882, 904, 936, 684, 171, 112, 52, 1006, 533, 650, 211, 366, 1199, 867, 46, 828, 458, 466, 778, 648, 1002, 221, 796, 111, 331, 940, 997, 424, 397, 534, 995, 802, 1097, 521, 653, 957, 888, 480, 1036, 871, 912, 1026, 270, 1202, 727, 474, 582, 456, 433, 1066, 877, 1116, 1105, 227, 422, 442, 1210, 384, 983, 162, 421, 1128, 119, 527, 207, 255, 248, 774, 1188, 483, 1088, 247, 6, 1111, 362, 461, 761, 1178, 1175, 1187, 619, 656, 902, 1118, 573, 1025, 581, 263, 1143, 339, 1185, 914, 1135, 1067, 376, 1212, 5, 937, 536, 751, 154, 449, 890, 1015, 801, 586, 160, 1048, 971, 905, 281, 498, 1213, 1186, 35, 195, 83, 360, 891, 488, 105, 852, 1046, 338, 1043, 1083, 876, 1075, 931, 1196, 561, 827, 816, 288, 753, 319, 570, 121, 37, 709, 926, 1134, 356, 323, 896, 283, 641, 1179, 149, 129, 490, 457, 57, 552, 750, 836, 1027, 1155, 134, 140, 123, 1209, 559, 235, 1152, 1141, 524, 1164, 158, 696, 908, 523, 642, 911, 267, 775, 622, 979, 516, 392, 695, 760, 864, 298, 620, 420, 445, 776, 617, 186, 90, 199, 76, 378, 925, 1140, 933, 18, 9, 486, 130, 494, 29, 783, 615, 375, 289, 39, 636, 732, 19, 754, 1051, 1171, 250, 1099, 718, 932, 825, 324, 597, 95, 103, 1032, 40, 1059, 785, 113, 106, 20, 10, 661, 699, 241, 848, 651, 885, 677, 440, 438, 1149, 909, 237, 810, 675, 631, 935, 806, 278, 1069, 557, 201, 795, 386, 1113, 1060, 503, 580, 21, 790, 497, 791, 41, 688, 634, 600, 934, 74, 192, 373, 807, 1120, 992, 135, 835, 799, 1165, 412, 1166, 668, 1151, 1021, 686, 450, 846, 623, 664, 282, 1170, 444, 47, 544, 820, 1056, 1124, 274, 707, 840, 152, 739, 873, 159, 469, 1096, 1130, 713, 426, 236, 1132, 232, 563, 564, 589, 1086, 372, 687, 332, 157, 437, 4, 251, 604, 850, 51, 949, 509, 944, 505, 859, 855, 126, 24, 1072, 962, 1144, 872, 549, 526, 577, 381, 710, 265, 735, 50, 1138, 34, 1109, 1195, 843, 562, 75, 942, 69, 880, 565, 973, 708, 1087, 318, 787, 1000, 837, 627, 187, 455, 124, 315, 487, 532, 797, 644, 296, 1040, 808, 689, 812, 311, 110, 674, 233, 210, 264, 368, 1091, 583, 541, 605, 733, 1052, 512, 148, 109, 822, 1012, 1044, 555, 1159, 704, 1017, 151, 430, 340, 599, 736, 71, 431, 120, 407, 1172, 155, 1089, 907, 1137, 294, 184, 1005, 79, 1016, 355, 1003, 609, 73, 535, 1049, 225, 181, 153, 238, 346, 491, 1125, 1024, 769, 1054, 624, 198, 272, 183, 127, 847, 520, 1200, 204, 218, 1147, 548, 143, 998, 857, 475, 322, 419, 245, 406, 328, 226, 798, 244, 734, 939, 17, 1142, 380, 762, 142, 652, 612, 530, 167, 132, 23, 1126, 1190, 402, 102, 547, 193, 506, 730, 344, 1127, 1009, 489, 646, 45, 128, 391, 354, 579, 694, 929, 504, 575, 767, 326, 206, 477, 1085, 770, 928, 818, 70, 436, 756, 671, 1122, 64, 1156, 405, 447, 747, 974, 780, 951, 7, 881, 175, 793, 551, 1154, 209, 163, 849, 84, 528, 657, 320, 1020, 492, 481, 587, 350, 310, 482, 886, 938, 744, 88, 133, 1110, 1146, 485, 731, 291, 721, 817, 208, 841, 189, 131, 742, 32, 725, 86, 510, 829, 1194, 341, 903, 484, 1102, 48, 58, 964, 603, 519, 453, 920, 1037, 408, 1045, 349, 465, 1047, 94, 1080, 680, 883, 923, 658, 147, 614, 632, 969, 85, 919, 369, 722, 538, 359, 856, 740, 507, 649, 1197, 921, 700, 667, 459, 993, 716, 388, 660, 276, 539, 830, 702, 773, 1081, 986, 108, 330, 1076, 313, 1, 540, 1004, 1090, 1214, 1001, 1121, 1148, 321, 953, 514, 705, 266, 1115, 1117, 495, 401, 220, 590, 115, 82, 1029, 249, 394, 66, 1100, 93, 1160, 303, 854, 268, 591, 996, 0, 36, 234, 224, 114, 1184, 389, 499, 104, 1153, 571, 616, 1041, 279, 1198, 608, 190, 682, 337, 529, 309, 712, 439, 174, 976, 335, 1050, 197, 1079, 887, 347, 462, 1074, 260, 779, 434, 768, 831, 306, 1204, 560, 821, 336, 863, 874, 286, 30, 358, 329, 606, 26, 1177, 960, 592, 205, 1106, 999, 333, 215, 794, 895, 101, 28, 804, 262, 383, 3, 415, 194, 418, 853, 765, 654, 511, 1192, 918, 697, 814, 228, 626, 387, 823, 824, 866, 1206, 427, 429, 989, 594, 637, 25, 685, 946, 737, 117, 1104, 1169, 361, 729, 645, 899, 868, 230, 243, 858, 1092, 703, 1173, 596, 435, 574, 991, 894, 89, 166, 342, 513, 432, 980, 811, 553, 146, 1163, 212, 1180, 1161, 164, 182, 792, 67, 965, 984, 1042, 96, 1071, 525, 312, 1065, 179, 219, 679, 663, 144, 277, 325, 200, 566, 851, 348, 307, 150, 576, 242, 1030, 454, 467, 630, 16, 169, 136, 414, 647, 598, 724, 1158, 994, 845, 1207, 748, 463, 659, 413, 27, 1183, 271, 80, 941, 165, 1201, 314, 15, 537, 443, 1133, 1058, 714, 959, 726, 869, 301, 676, 1053, 161, 542, 958, 441, 1203, 968, 53, 97, 493, 681, 1064, 202, 1193, 915, 191, 68, 906, 377, 838, 601, 766, 452, 217, 800, 141, 948, 900, 698, 374, 327, 809, 1174, 99, 122, 304, 786, 214, 1167, 8, 299, 1023, 62, 588, 137, 269, 14, 229, 1055, 1078, 673, 496, 22, 987, 893, 116, 107, 334, 55, 403, 952, 715, 517, 764, 865, 275, 546, 1033, 884, 1157, 917, 1205, 239, 464, 1123, 1093, 473, 879, 409, 943, 178, 1162, 364, 945, 56, 393, 44, 1094, 353, 633, 897, 502, 638, 285, 416, 292, 302, 1084, 910, 711, 988, 367, 1114, 898, 138, 1150, 399, 745, 460, 1136, 585, 717, 784, 618, 1061, 254, 1073, 930, 861, 471, 757, 231, 168, 470, 216, 396, 91, 280, 77, 13, 1018, 72, 478, 860, 752, 985, 531, 33, 972, 156, 42, 290, 81, 950, 595, 719, 196, 257, 1107, 1108, 723, 118, 1191, 982, 878, 692, 1176, 428, 1168, 927, 515, 720, 693, 351, 1007, 410, 643, 545, 901, 967, 293, 60, 259, 38, 550, 508, 1013, 832, 1101, 691, 316, 261, 305, 763, 370, 12, 125, 892, 556, 543, 669, 777, 629, 423, 875, 1014, 990, 1070, 584, 759, 31, 593, 610, 246, 1129, 180, 1112, 65, 352, 177, 977, 1008, 781, 411, 1145, 607, 1082, 1211, 1034, 665, 371, 625, 569, 170, 284, 2, 300, 1010, 185, 922, 1063, 382, 223, 252, 913, 844, 1208, 468, 961, 978, 472, 63, 188, 956, 357, 448, 240, 639, 479, 981]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7660804830617600
the save name prefix for this run is:  chkpt-ID_7660804830617600_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 985
rank avg (pred): 0.557 +- 0.003
mrr vals (pred, true): 0.017, 0.455
batch losses (mrrl, rdl): 0.0, 0.0054325261

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 383
rank avg (pred): 0.247 +- 0.164
mrr vals (pred, true): 0.135, 0.249
batch losses (mrrl, rdl): 0.0, 0.0004196733

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 223
rank avg (pred): 0.276 +- 0.215
mrr vals (pred, true): 0.222, 0.052
batch losses (mrrl, rdl): 0.0, 0.0005664021

Epoch over!
epoch time: 11.991

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 66
rank avg (pred): 0.060 +- 0.052
mrr vals (pred, true): 0.387, 0.440
batch losses (mrrl, rdl): 0.0, 3.972e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 423
rank avg (pred): 0.249 +- 0.229
mrr vals (pred, true): 0.338, 0.051
batch losses (mrrl, rdl): 0.0, 0.000747564

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1041
rank avg (pred): 0.299 +- 0.244
mrr vals (pred, true): 0.263, 0.058
batch losses (mrrl, rdl): 0.0, 0.000304609

Epoch over!
epoch time: 11.858

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 906
rank avg (pred): 0.180 +- 0.170
mrr vals (pred, true): 0.358, 0.036
batch losses (mrrl, rdl): 0.0, 0.0023027805

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 715
rank avg (pred): 0.295 +- 0.256
mrr vals (pred, true): 0.298, 0.054
batch losses (mrrl, rdl): 0.0, 0.0003849323

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1120
rank avg (pred): 0.245 +- 0.231
mrr vals (pred, true): 0.363, 0.058
batch losses (mrrl, rdl): 0.0, 0.000744838

Epoch over!
epoch time: 11.845

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 857
rank avg (pred): 0.414 +- 0.325
mrr vals (pred, true): 0.256, 0.054
batch losses (mrrl, rdl): 0.0, 2.60877e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 886
rank avg (pred): 0.414 +- 0.299
mrr vals (pred, true): 0.218, 0.057
batch losses (mrrl, rdl): 0.0, 1.49089e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1128
rank avg (pred): 0.287 +- 0.243
mrr vals (pred, true): 0.267, 0.053
batch losses (mrrl, rdl): 0.0, 0.0004872762

Epoch over!
epoch time: 11.887

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 675
rank avg (pred): 0.259 +- 0.247
mrr vals (pred, true): 0.362, 0.057
batch losses (mrrl, rdl): 0.0, 0.0006104755

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1183
rank avg (pred): 0.226 +- 0.225
mrr vals (pred, true): 0.411, 0.245
batch losses (mrrl, rdl): 0.0, 0.0004530411

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 84
rank avg (pred): 0.263 +- 0.243
mrr vals (pred, true): 0.322, 0.219
batch losses (mrrl, rdl): 0.0, 0.0005325828

Epoch over!
epoch time: 11.847

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 563
rank avg (pred): 0.094 +- 0.090
mrr vals (pred, true): 0.413, 0.368
batch losses (mrrl, rdl): 0.0203804467, 2.27471e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 242
rank avg (pred): 0.355 +- 0.216
mrr vals (pred, true): 0.147, 0.060
batch losses (mrrl, rdl): 0.0933030397, 0.0001793

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 41
rank avg (pred): 0.029 +- 0.019
mrr vals (pred, true): 0.406, 0.443
batch losses (mrrl, rdl): 0.0134042362, 1.60087e-05

Epoch over!
epoch time: 12.338

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 270
rank avg (pred): 0.026 +- 0.019
mrr vals (pred, true): 0.435, 0.439
batch losses (mrrl, rdl): 0.0002386165, 2.17921e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1178
rank avg (pred): 0.365 +- 0.209
mrr vals (pred, true): 0.148, 0.249
batch losses (mrrl, rdl): 0.1024568081, 0.0017622195

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 272
rank avg (pred): 0.022 +- 0.016
mrr vals (pred, true): 0.469, 0.434
batch losses (mrrl, rdl): 0.0121536395, 2.36093e-05

Epoch over!
epoch time: 12.235

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 25
rank avg (pred): 0.020 +- 0.014
mrr vals (pred, true): 0.456, 0.451
batch losses (mrrl, rdl): 0.0002736641, 2.26953e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 779
rank avg (pred): 0.688 +- 0.273
mrr vals (pred, true): 0.072, 0.050
batch losses (mrrl, rdl): 0.0046888017, 0.0009118004

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 202
rank avg (pred): 0.348 +- 0.199
mrr vals (pred, true): 0.131, 0.058
batch losses (mrrl, rdl): 0.0653174818, 0.0001762897

Epoch over!
epoch time: 12.081

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 936
rank avg (pred): 0.686 +- 0.289
mrr vals (pred, true): 0.081, 0.055
batch losses (mrrl, rdl): 0.009914523, 0.0009416621

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 293
rank avg (pred): 0.029 +- 0.019
mrr vals (pred, true): 0.403, 0.428
batch losses (mrrl, rdl): 0.0061092144, 1.26004e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 86
rank avg (pred): 0.339 +- 0.191
mrr vals (pred, true): 0.137, 0.237
batch losses (mrrl, rdl): 0.101281181, 0.001097313

Epoch over!
epoch time: 11.886

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 447
rank avg (pred): 0.351 +- 0.197
mrr vals (pred, true): 0.136, 0.055
batch losses (mrrl, rdl): 0.0734461099, 0.0002082909

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 843
rank avg (pred): 0.870 +- 0.212
mrr vals (pred, true): 0.027, 0.049
batch losses (mrrl, rdl): 0.0054726545, 0.0027498922

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 229
rank avg (pred): 0.348 +- 0.192
mrr vals (pred, true): 0.132, 0.062
batch losses (mrrl, rdl): 0.0673699081, 0.0001594121

Epoch over!
epoch time: 12.034

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 998
rank avg (pred): 0.017 +- 0.011
mrr vals (pred, true): 0.484, 0.459
batch losses (mrrl, rdl): 0.0059445989, 3.01154e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 574
rank avg (pred): 0.320 +- 0.186
mrr vals (pred, true): 0.148, 0.178
batch losses (mrrl, rdl): 0.008834755, 0.0006243593

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 660
rank avg (pred): 0.326 +- 0.175
mrr vals (pred, true): 0.126, 0.057
batch losses (mrrl, rdl): 0.0582320839, 0.0002706697

Epoch over!
epoch time: 12.142

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 677
rank avg (pred): 0.320 +- 0.197
mrr vals (pred, true): 0.170, 0.055
batch losses (mrrl, rdl): 0.1431081891, 0.0003491638

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 864
rank avg (pred): 0.866 +- 0.244
mrr vals (pred, true): 0.034, 0.048
batch losses (mrrl, rdl): 0.0025032889, 0.0027693941

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 357
rank avg (pred): 0.330 +- 0.187
mrr vals (pred, true): 0.133, 0.225
batch losses (mrrl, rdl): 0.0851730332, 0.0012134524

Epoch over!
epoch time: 12.044

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 156
rank avg (pred): 0.291 +- 0.179
mrr vals (pred, true): 0.174, 0.248
batch losses (mrrl, rdl): 0.0557885654, 0.0010102551

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 194
rank avg (pred): 0.333 +- 0.185
mrr vals (pred, true): 0.139, 0.057
batch losses (mrrl, rdl): 0.078401342, 0.000235379

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 863
rank avg (pred): 0.735 +- 0.230
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002497489, 0.001394898

Epoch over!
epoch time: 12.074

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1206
rank avg (pred): 0.309 +- 0.183
mrr vals (pred, true): 0.160, 0.054
batch losses (mrrl, rdl): 0.1211228967, 0.0004036002

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 282
rank avg (pred): 0.023 +- 0.014
mrr vals (pred, true): 0.419, 0.442
batch losses (mrrl, rdl): 0.0053452412, 2.48613e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 42
rank avg (pred): 0.021 +- 0.012
mrr vals (pred, true): 0.425, 0.432
batch losses (mrrl, rdl): 0.0004916243, 3.26471e-05

Epoch over!
epoch time: 11.837

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 939
rank avg (pred): 0.870 +- 0.242
mrr vals (pred, true): 0.037, 0.043
batch losses (mrrl, rdl): 0.001664239, 0.0023108397

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 515
rank avg (pred): 0.031 +- 0.018
mrr vals (pred, true): 0.357, 0.378
batch losses (mrrl, rdl): 0.0040874765, 2.39257e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 972
rank avg (pred): 0.023 +- 0.014
mrr vals (pred, true): 0.419, 0.441
batch losses (mrrl, rdl): 0.0046921242, 2.29051e-05

Epoch over!
epoch time: 12.01

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.021 +- 0.013
mrr vals (pred, true): 0.430, 0.457

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.05827 	 0.04618 	 ~...
   60 	     1 	 0.14121 	 0.04664 	 m..s
   74 	     2 	 0.14247 	 0.04754 	 m..s
   45 	     3 	 0.14029 	 0.04802 	 m..s
   36 	     4 	 0.13988 	 0.04831 	 m..s
    7 	     5 	 0.04659 	 0.04833 	 ~...
    9 	     6 	 0.04816 	 0.04846 	 ~...
   22 	     7 	 0.13014 	 0.04948 	 m..s
   14 	     8 	 0.05471 	 0.04955 	 ~...
   13 	     9 	 0.05247 	 0.04957 	 ~...
    2 	    10 	 0.03736 	 0.04972 	 ~...
    1 	    11 	 0.03407 	 0.04985 	 ~...
   29 	    12 	 0.13781 	 0.05001 	 m..s
   61 	    13 	 0.14122 	 0.05011 	 m..s
   24 	    14 	 0.13423 	 0.05029 	 m..s
   77 	    15 	 0.14267 	 0.05037 	 m..s
    9 	    16 	 0.04816 	 0.05087 	 ~...
   15 	    17 	 0.05479 	 0.05174 	 ~...
    8 	    18 	 0.04676 	 0.05179 	 ~...
   54 	    19 	 0.14079 	 0.05196 	 m..s
   15 	    20 	 0.05479 	 0.05233 	 ~...
   11 	    21 	 0.04832 	 0.05252 	 ~...
    3 	    22 	 0.03743 	 0.05287 	 ~...
   61 	    23 	 0.14122 	 0.05288 	 m..s
   42 	    24 	 0.14014 	 0.05292 	 m..s
   34 	    25 	 0.13979 	 0.05297 	 m..s
   37 	    26 	 0.13998 	 0.05299 	 m..s
   69 	    27 	 0.14182 	 0.05337 	 m..s
   12 	    28 	 0.05163 	 0.05340 	 ~...
   79 	    29 	 0.14299 	 0.05365 	 m..s
    6 	    30 	 0.04542 	 0.05405 	 ~...
    4 	    31 	 0.03776 	 0.05411 	 ~...
   48 	    32 	 0.14057 	 0.05420 	 m..s
    5 	    33 	 0.04444 	 0.05447 	 ~...
   76 	    34 	 0.14256 	 0.05501 	 m..s
   35 	    35 	 0.13984 	 0.05515 	 m..s
   82 	    36 	 0.14609 	 0.05574 	 m..s
   32 	    37 	 0.13968 	 0.05600 	 m..s
   71 	    38 	 0.14224 	 0.05602 	 m..s
   68 	    39 	 0.14178 	 0.05664 	 m..s
   22 	    40 	 0.13014 	 0.05797 	 m..s
   54 	    41 	 0.14079 	 0.05813 	 m..s
   49 	    42 	 0.14069 	 0.05843 	 m..s
   26 	    43 	 0.13741 	 0.05888 	 m..s
   18 	    44 	 0.06484 	 0.05938 	 ~...
   41 	    45 	 0.14007 	 0.05955 	 m..s
    0 	    46 	 0.03256 	 0.06025 	 ~...
   81 	    47 	 0.14404 	 0.06102 	 m..s
   19 	    48 	 0.09817 	 0.08458 	 ~...
   87 	    49 	 0.29511 	 0.10016 	 MISS
   84 	    50 	 0.26724 	 0.20065 	 m..s
   85 	    51 	 0.29208 	 0.20295 	 m..s
   65 	    52 	 0.14136 	 0.20531 	 m..s
   40 	    53 	 0.14006 	 0.21450 	 m..s
   72 	    54 	 0.14225 	 0.21466 	 m..s
   46 	    55 	 0.14053 	 0.21836 	 m..s
   66 	    56 	 0.14140 	 0.22555 	 m..s
   44 	    57 	 0.14022 	 0.22648 	 m..s
   78 	    58 	 0.14269 	 0.22911 	 m..s
   38 	    59 	 0.14002 	 0.23340 	 m..s
   33 	    60 	 0.13973 	 0.23480 	 m..s
   27 	    61 	 0.13742 	 0.23724 	 m..s
   31 	    62 	 0.13963 	 0.23730 	 m..s
   56 	    63 	 0.14092 	 0.23749 	 m..s
   63 	    64 	 0.14131 	 0.23790 	 m..s
   20 	    65 	 0.12677 	 0.23816 	 MISS
   52 	    66 	 0.14075 	 0.23903 	 m..s
   57 	    67 	 0.14093 	 0.24002 	 m..s
   21 	    68 	 0.12935 	 0.24172 	 MISS
   70 	    69 	 0.14214 	 0.24722 	 MISS
   50 	    70 	 0.14071 	 0.24826 	 MISS
   47 	    71 	 0.14055 	 0.24863 	 MISS
   30 	    72 	 0.13793 	 0.25118 	 MISS
   59 	    73 	 0.14119 	 0.25366 	 MISS
   63 	    74 	 0.14131 	 0.25383 	 MISS
   75 	    75 	 0.14247 	 0.25445 	 MISS
   43 	    76 	 0.14015 	 0.25572 	 MISS
   39 	    77 	 0.14005 	 0.25594 	 MISS
   52 	    78 	 0.14075 	 0.25718 	 MISS
   80 	    79 	 0.14300 	 0.26072 	 MISS
   28 	    80 	 0.13746 	 0.26151 	 MISS
   58 	    81 	 0.14118 	 0.26210 	 MISS
   73 	    82 	 0.14243 	 0.26251 	 MISS
   51 	    83 	 0.14073 	 0.26746 	 MISS
   25 	    84 	 0.13512 	 0.27274 	 MISS
   67 	    85 	 0.14164 	 0.27353 	 MISS
   91 	    86 	 0.37202 	 0.33846 	 m..s
   86 	    87 	 0.29277 	 0.33874 	 m..s
   83 	    88 	 0.26253 	 0.33885 	 m..s
   90 	    89 	 0.36364 	 0.35933 	 ~...
   89 	    90 	 0.36302 	 0.36349 	 ~...
   93 	    91 	 0.37335 	 0.36692 	 ~...
   94 	    92 	 0.37596 	 0.37320 	 ~...
   88 	    93 	 0.36165 	 0.39461 	 m..s
   97 	    94 	 0.38464 	 0.41083 	 ~...
   96 	    95 	 0.38433 	 0.41259 	 ~...
   99 	    96 	 0.39721 	 0.41336 	 ~...
   95 	    97 	 0.37781 	 0.41423 	 m..s
   91 	    98 	 0.37202 	 0.41472 	 m..s
  100 	    99 	 0.39822 	 0.42435 	 ~...
  106 	   100 	 0.41864 	 0.42818 	 ~...
  101 	   101 	 0.41242 	 0.42880 	 ~...
  107 	   102 	 0.41874 	 0.42947 	 ~...
   98 	   103 	 0.38885 	 0.43199 	 m..s
  108 	   104 	 0.41880 	 0.43376 	 ~...
  109 	   105 	 0.42820 	 0.43751 	 ~...
  112 	   106 	 0.43004 	 0.44444 	 ~...
  114 	   107 	 0.43088 	 0.44555 	 ~...
  111 	   108 	 0.43000 	 0.44580 	 ~...
  110 	   109 	 0.42868 	 0.44598 	 ~...
  119 	   110 	 0.45707 	 0.44604 	 ~...
  102 	   111 	 0.41350 	 0.44612 	 m..s
  116 	   112 	 0.43284 	 0.44625 	 ~...
  102 	   113 	 0.41350 	 0.44691 	 m..s
  115 	   114 	 0.43248 	 0.44837 	 ~...
  104 	   115 	 0.41397 	 0.45085 	 m..s
  105 	   116 	 0.41617 	 0.45134 	 m..s
  120 	   117 	 0.45708 	 0.45483 	 ~...
  113 	   118 	 0.43035 	 0.45747 	 ~...
  118 	   119 	 0.43632 	 0.47171 	 m..s
  117 	   120 	 0.43630 	 0.47952 	 m..s
==========================================
r_mrr = 0.8772244453430176
r2_mrr = 0.7632191777229309
spearmanr_mrr@5 = 0.9690840244293213
spearmanr_mrr@10 = 0.979454517364502
spearmanr_mrr@50 = 0.9636147618293762
spearmanr_mrr@100 = 0.8756372928619385
spearmanr_mrr@All = 0.9055238962173462
==========================================
test time: 0.397
Done Testing dataset Kinships
total time taken: 187.52046990394592
training time taken: 180.568785905838
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8772)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7632)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9691)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9795)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9636)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8756)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9055)}}, 'test_loss': {'DistMult': {'Kinships': 7.070997644110321}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 1295534603164979
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [678, 1020, 592, 978, 386, 902, 585, 1161, 214, 990, 191, 370, 1166, 380, 583, 395, 1158, 96, 175, 412, 966, 205, 686, 426, 1114, 235, 816, 494, 268, 250, 152, 885, 1188, 556, 449, 824, 243, 776, 266, 1050, 455, 690, 1047, 137, 279, 771, 25, 622, 643, 1124, 1203, 1076, 843, 558, 723, 1026, 800, 194, 999, 798, 1136, 621, 100, 737, 540, 446, 66, 294, 321, 1196, 109, 86, 1160, 721, 952, 57, 1024, 20, 606, 389, 437, 105, 436, 1147, 653, 390, 892, 1112, 789, 64, 163, 871, 356, 942, 802, 150, 281, 433, 683, 672, 387, 1198, 363, 1098, 596, 792, 300, 787, 948, 668, 67, 1205, 856, 507, 283, 82, 808, 173, 1063, 719, 78]
valid_ids (0): []
train_ids (1094): [210, 724, 1058, 1057, 102, 170, 313, 471, 1127, 1039, 743, 1106, 398, 997, 377, 146, 320, 778, 341, 579, 936, 264, 311, 658, 400, 781, 200, 872, 1053, 1025, 584, 624, 142, 614, 806, 85, 330, 972, 60, 206, 807, 899, 947, 561, 325, 42, 1016, 312, 845, 211, 569, 930, 1212, 275, 399, 762, 691, 883, 1123, 533, 1162, 692, 479, 510, 361, 288, 472, 434, 756, 237, 1194, 117, 1169, 650, 637, 224, 1079, 1074, 1157, 983, 366, 37, 90, 1, 955, 661, 1038, 587, 880, 921, 416, 1113, 349, 480, 187, 877, 552, 758, 748, 249, 429, 616, 230, 984, 938, 543, 745, 524, 1060, 875, 933, 526, 810, 461, 707, 1061, 674, 575, 409, 1031, 5, 581, 1182, 302, 530, 148, 1207, 973, 53, 1164, 450, 218, 864, 701, 932, 574, 30, 945, 1011, 852, 413, 8, 1091, 931, 960, 1015, 729, 337, 1192, 115, 24, 1081, 741, 192, 744, 43, 270, 13, 430, 1036, 431, 508, 422, 958, 559, 594, 443, 1141, 1183, 139, 865, 512, 1144, 233, 154, 299, 1129, 445, 577, 404, 591, 750, 419, 623, 149, 48, 694, 156, 566, 444, 657, 582, 553, 849, 306, 628, 962, 95, 442, 38, 1101, 1134, 1199, 1149, 223, 988, 1208, 649, 597, 285, 1052, 937, 505, 768, 174, 916, 714, 1108, 71, 602, 760, 213, 1142, 318, 1121, 326, 121, 664, 659, 568, 976, 1146, 609, 166, 375, 919, 358, 1152, 1213, 588, 554, 4, 1201, 1209, 927, 333, 1180, 350, 542, 204, 483, 207, 231, 133, 92, 706, 309, 269, 995, 497, 626, 342, 642, 735, 457, 1200, 1033, 402, 420, 120, 681, 548, 992, 549, 365, 651, 459, 481, 1041, 406, 1156, 21, 844, 193, 1023, 343, 484, 335, 1004, 893, 1010, 1095, 1049, 850, 401, 454, 253, 267, 1174, 599, 696, 31, 814, 39, 730, 276, 226, 368, 232, 34, 1088, 767, 56, 963, 829, 91, 98, 272, 598, 169, 199, 357, 221, 384, 1068, 847, 72, 178, 1055, 439, 1165, 830, 669, 177, 1075, 879, 185, 1072, 151, 925, 315, 634, 1056, 532, 107, 469, 996, 751, 462, 728, 307, 636, 385, 407, 360, 1154, 926, 539, 1104, 161, 1100, 1048, 108, 424, 970, 673, 870, 125, 713, 815, 841, 1137, 1027, 527, 145, 134, 761, 564, 954, 241, 523, 456, 1155, 19, 184, 834, 410, 720, 7, 890, 906, 881, 1135, 1030, 1179, 159, 645, 9, 1019, 770, 1128, 935, 323, 212, 851, 833, 397, 991, 1195, 519, 55, 688, 47, 466, 801, 590, 652, 1046, 372, 557, 832, 703, 1006, 943, 1168, 939, 975, 1062, 1085, 853, 1028, 470, 274, 54, 334, 965, 817, 203, 418, 215, 709, 240, 520, 295, 260, 726, 739, 448, 842, 662, 998, 12, 352, 589, 114, 50, 489, 104, 1082, 563, 14, 929, 1120, 393, 1001, 23, 795, 619, 534, 89, 908, 482, 684, 18, 77, 994, 113, 980, 640, 826, 171, 1080, 347, 373, 1139, 780, 537, 1184, 52, 128, 670, 94, 256, 777, 1070, 766, 247, 515, 982, 202, 465, 560, 301, 1119, 181, 1151, 172, 676, 118, 578, 317, 51, 290, 1012, 1189, 747, 1064, 44, 509, 956, 345, 46, 251, 828, 130, 1066, 894, 961, 772, 1175, 1084, 1117, 793, 629, 705, 28, 974, 222, 873, 812, 123, 103, 284, 280, 485, 438, 40, 217, 93, 665, 804, 491, 242, 427, 435, 677, 562, 521, 234, 403, 600, 180, 49, 1008, 805, 1193, 898, 376, 884, 225, 654, 698, 124, 478, 374, 914, 1206, 1191, 87, 157, 1099, 666, 303, 825, 355, 62, 138, 944, 689, 371, 477, 625, 682, 164, 535, 378, 1122, 809, 69, 1132, 1177, 297, 704, 1211, 1014, 1021, 291, 1034, 106, 475, 501, 417, 601, 712, 112, 525, 917, 1044, 167, 239, 411, 354, 848, 388, 576, 1148, 58, 838, 603, 946, 785, 783, 1111, 153, 1037, 97, 971, 860, 143, 405, 261, 36, 660, 15, 336, 1002, 255, 278, 248, 529, 1202, 866, 887, 1045, 338, 263, 474, 511, 447, 1007, 620, 467, 219, 506, 332, 861, 132, 786, 903, 950, 904, 344, 254, 791, 820, 75, 1005, 362, 1109, 835, 551, 888, 891, 840, 220, 784, 855, 1172, 1040, 949, 201, 1204, 473, 76, 702, 453, 570, 141, 827, 1087, 981, 769, 65, 858, 45, 127, 1131, 101, 59, 1145, 126, 88, 293, 1029, 685, 84, 1126, 1077, 244, 905, 746, 32, 188, 1167, 314, 421, 147, 759, 740, 74, 464, 136, 863, 580, 440, 618, 1089, 1138, 339, 774, 382, 1115, 396, 79, 0, 878, 364, 16, 155, 895, 868, 964, 331, 639, 216, 733, 496, 1181, 238, 517, 763, 968, 168, 1018, 262, 1143, 876, 818, 708, 183, 498, 2, 874, 353, 198, 869, 451, 160, 502, 22, 189, 846, 381, 607, 1171, 492, 779, 452, 734, 1059, 1013, 907, 33, 176, 573, 277, 165, 1105, 1176, 901, 329, 1071, 236, 182, 836, 586, 1150, 595, 941, 1159, 918, 1022, 680, 367, 794, 228, 486, 928, 655, 923, 41, 17, 897, 940, 732, 531, 1009, 351, 641, 287, 711, 544, 319, 957, 752, 122, 35, 415, 1133, 782, 131, 764, 695, 328, 831, 83, 718, 715, 610, 271, 229, 819, 889, 259, 1093, 571, 555, 129, 327, 648, 742, 647, 111, 822, 1210, 208, 414, 162, 1054, 700, 1125, 631, 796, 140, 503, 909, 1116, 882, 1118, 839, 725, 550, 432, 773, 144, 273, 993, 1107, 1153, 1110, 26, 920, 493, 951, 316, 854, 186, 969, 310, 500, 1197, 209, 632, 286, 1187, 245, 1065, 633, 346, 1078, 886, 736, 246, 110, 967, 1102, 488, 545, 541, 565, 116, 468, 81, 190, 567, 857, 716, 348, 227, 383, 68, 513, 663, 627, 699, 304, 675, 755, 753, 679, 615, 986, 953, 296, 977, 989, 593, 135, 754, 915, 1163, 27, 1000, 959, 896, 1032, 460, 322, 924, 697, 803, 1103, 476, 499, 934, 656, 910, 617, 394, 922, 495, 757, 258, 11, 1073, 428, 308, 538, 536, 487, 305, 1173, 547, 504, 1185, 99, 738, 644, 1130, 289, 604, 749, 252, 80, 797, 1069, 837, 423, 369, 913, 1043, 458, 1097, 987, 257, 1035, 862, 1086, 6, 613, 195, 811, 514, 731, 788, 1140, 859, 635, 612, 867, 813, 1003, 611, 340, 1094, 693, 518, 823, 1096, 1042, 765, 197, 441, 646, 298, 687, 1178, 522, 546, 265, 179, 799, 821, 900, 630, 722, 463, 282, 1170, 667, 359, 985, 292, 73, 790, 158, 1051, 63, 727, 717, 1214, 1186, 608, 572, 912, 379, 490, 979, 911, 10, 1083, 516, 408, 671, 638, 1017, 1090, 775, 70, 119, 392, 391, 3, 29, 528, 61, 324, 1092, 710, 1190, 1067, 605, 425, 196]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4352851468419124
the save name prefix for this run is:  chkpt-ID_4352851468419124_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 981
rank avg (pred): 0.455 +- 0.006
mrr vals (pred, true): 0.021, 0.433
batch losses (mrrl, rdl): 0.0, 0.0034606971

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 336
rank avg (pred): 0.266 +- 0.193
mrr vals (pred, true): 0.195, 0.235
batch losses (mrrl, rdl): 0.0, 0.0006833266

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 7
rank avg (pred): 0.035 +- 0.033
mrr vals (pred, true): 0.499, 0.436
batch losses (mrrl, rdl): 0.0, 8.3526e-06

Epoch over!
epoch time: 12.164

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 811
rank avg (pred): 0.128 +- 0.109
mrr vals (pred, true): 0.350, 0.283
batch losses (mrrl, rdl): 0.0, 5.75009e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1145
rank avg (pred): 0.066 +- 0.064
mrr vals (pred, true): 0.468, 0.393
batch losses (mrrl, rdl): 0.0, 1.6041e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 192
rank avg (pred): 0.210 +- 0.193
mrr vals (pred, true): 0.370, 0.050
batch losses (mrrl, rdl): 0.0, 0.0011030207

Epoch over!
epoch time: 12.158

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 456
rank avg (pred): 0.266 +- 0.223
mrr vals (pred, true): 0.304, 0.064
batch losses (mrrl, rdl): 0.0, 0.0006195662

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1080
rank avg (pred): 0.260 +- 0.219
mrr vals (pred, true): 0.295, 0.234
batch losses (mrrl, rdl): 0.0, 0.0006088785

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 512
rank avg (pred): 0.105 +- 0.100
mrr vals (pred, true): 0.446, 0.366
batch losses (mrrl, rdl): 0.0, 4.41115e-05

Epoch over!
epoch time: 11.969

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1039
rank avg (pred): 0.280 +- 0.234
mrr vals (pred, true): 0.313, 0.053
batch losses (mrrl, rdl): 0.0, 0.0005009227

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 60
rank avg (pred): 0.042 +- 0.042
mrr vals (pred, true): 0.515, 0.427
batch losses (mrrl, rdl): 0.0, 5.376e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 754
rank avg (pred): 0.066 +- 0.064
mrr vals (pred, true): 0.481, 0.109
batch losses (mrrl, rdl): 0.0, 0.0004318826

Epoch over!
epoch time: 11.999

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 255
rank avg (pred): 0.066 +- 0.063
mrr vals (pred, true): 0.465, 0.440
batch losses (mrrl, rdl): 0.0, 4.3839e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 525
rank avg (pred): 0.037 +- 0.035
mrr vals (pred, true): 0.502, 0.368
batch losses (mrrl, rdl): 0.0, 2.11513e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1212
rank avg (pred): 0.270 +- 0.223
mrr vals (pred, true): 0.254, 0.050
batch losses (mrrl, rdl): 0.0, 0.0006020645

Epoch over!
epoch time: 11.902

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 628
rank avg (pred): 0.251 +- 0.240
mrr vals (pred, true): 0.402, 0.233
batch losses (mrrl, rdl): 0.2850507498, 0.0005530566

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 685
rank avg (pred): 0.408 +- 0.209
mrr vals (pred, true): 0.115, 0.052
batch losses (mrrl, rdl): 0.0424448065, 4.60846e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 543
rank avg (pred): 0.055 +- 0.039
mrr vals (pred, true): 0.345, 0.363
batch losses (mrrl, rdl): 0.0033474267, 3.9011e-06

Epoch over!
epoch time: 12.279

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1168
rank avg (pred): 0.376 +- 0.191
mrr vals (pred, true): 0.127, 0.254
batch losses (mrrl, rdl): 0.1614163518, 0.0017567356

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 841
rank avg (pred): 0.631 +- 0.191
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002090033, 0.0005436453

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 909
rank avg (pred): 0.052 +- 0.034
mrr vals (pred, true): 0.323, 0.358
batch losses (mrrl, rdl): 0.0124196224, 4.2722e-06

Epoch over!
epoch time: 11.986

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 253
rank avg (pred): 0.022 +- 0.014
mrr vals (pred, true): 0.442, 0.447
batch losses (mrrl, rdl): 0.0002764086, 1.99969e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 812
rank avg (pred): 0.196 +- 0.134
mrr vals (pred, true): 0.232, 0.311
batch losses (mrrl, rdl): 0.0614018068, 0.0003629727

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 901
rank avg (pred): 0.060 +- 0.040
mrr vals (pred, true): 0.325, 0.332
batch losses (mrrl, rdl): 0.0004149758, 1.9364e-06

Epoch over!
epoch time: 12.224

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 403
rank avg (pred): 0.350 +- 0.190
mrr vals (pred, true): 0.140, 0.228
batch losses (mrrl, rdl): 0.0768713877, 0.0014509395

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 210
rank avg (pred): 0.330 +- 0.195
mrr vals (pred, true): 0.172, 0.049
batch losses (mrrl, rdl): 0.1478316188, 0.0003217466

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 440
rank avg (pred): 0.356 +- 0.175
mrr vals (pred, true): 0.129, 0.053
batch losses (mrrl, rdl): 0.063158378, 0.0002021514

Epoch over!
epoch time: 11.975

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1118
rank avg (pred): 0.362 +- 0.173
mrr vals (pred, true): 0.127, 0.060
batch losses (mrrl, rdl): 0.058660008, 0.0001729883

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 829
rank avg (pred): 0.033 +- 0.022
mrr vals (pred, true): 0.395, 0.412
batch losses (mrrl, rdl): 0.0031067859, 1.06305e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 661
rank avg (pred): 0.339 +- 0.178
mrr vals (pred, true): 0.145, 0.051
batch losses (mrrl, rdl): 0.0904070586, 0.000266097

Epoch over!
epoch time: 12.001

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 486
rank avg (pred): 0.035 +- 0.023
mrr vals (pred, true): 0.380, 0.388
batch losses (mrrl, rdl): 0.0005463754, 1.80473e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1184
rank avg (pred): 0.343 +- 0.172
mrr vals (pred, true): 0.139, 0.238
batch losses (mrrl, rdl): 0.0982412249, 0.0014079476

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1117
rank avg (pred): 0.356 +- 0.168
mrr vals (pred, true): 0.130, 0.056
batch losses (mrrl, rdl): 0.0635968, 0.0002231124

Epoch over!
epoch time: 12.108

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 104
rank avg (pred): 0.318 +- 0.185
mrr vals (pred, true): 0.175, 0.229
batch losses (mrrl, rdl): 0.0299060363, 0.0009281027

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 664
rank avg (pred): 0.318 +- 0.183
mrr vals (pred, true): 0.172, 0.056
batch losses (mrrl, rdl): 0.149061963, 0.0003145561

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1204
rank avg (pred): 0.315 +- 0.183
mrr vals (pred, true): 0.177, 0.049
batch losses (mrrl, rdl): 0.1612403989, 0.0004372696

Epoch over!
epoch time: 11.879

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 420
rank avg (pred): 0.341 +- 0.167
mrr vals (pred, true): 0.139, 0.059
batch losses (mrrl, rdl): 0.0785927474, 0.0003014095

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 256
rank avg (pred): 0.024 +- 0.016
mrr vals (pred, true): 0.433, 0.446
batch losses (mrrl, rdl): 0.0017864873, 1.82848e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1133
rank avg (pred): 0.335 +- 0.162
mrr vals (pred, true): 0.143, 0.052
batch losses (mrrl, rdl): 0.085866645, 0.0002985997

Epoch over!
epoch time: 11.942

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 682
rank avg (pred): 0.317 +- 0.173
mrr vals (pred, true): 0.170, 0.054
batch losses (mrrl, rdl): 0.1439223886, 0.0003468218

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1211
rank avg (pred): 0.326 +- 0.164
mrr vals (pred, true): 0.152, 0.054
batch losses (mrrl, rdl): 0.103969723, 0.0003531727

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 528
rank avg (pred): 0.046 +- 0.031
mrr vals (pred, true): 0.364, 0.352
batch losses (mrrl, rdl): 0.0014618749, 1.37233e-05

Epoch over!
epoch time: 12.038

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 593
rank avg (pred): 0.353 +- 0.145
mrr vals (pred, true): 0.128, 0.243
batch losses (mrrl, rdl): 0.1319905967, 0.001548458

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 254
rank avg (pred): 0.025 +- 0.017
mrr vals (pred, true): 0.445, 0.441
batch losses (mrrl, rdl): 0.0001685765, 1.87273e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 530
rank avg (pred): 0.043 +- 0.030
mrr vals (pred, true): 0.384, 0.386
batch losses (mrrl, rdl): 6.82859e-05, 7.7515e-06

Epoch over!
epoch time: 11.878

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.339 +- 0.161
mrr vals (pred, true): 0.145, 0.048

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   86 	     0 	 0.16205 	 0.02078 	 MISS
   44 	     1 	 0.14085 	 0.02078 	 MISS
   10 	     2 	 0.04964 	 0.04299 	 ~...
   49 	     3 	 0.14346 	 0.04634 	 m..s
   65 	     4 	 0.14714 	 0.04757 	 m..s
   15 	     5 	 0.06076 	 0.04771 	 ~...
   58 	     6 	 0.14546 	 0.04802 	 m..s
    9 	     7 	 0.04916 	 0.04846 	 ~...
    4 	     8 	 0.04333 	 0.04864 	 ~...
   37 	     9 	 0.13734 	 0.04876 	 m..s
   32 	    10 	 0.13562 	 0.04878 	 m..s
   64 	    11 	 0.14698 	 0.04925 	 m..s
    3 	    12 	 0.04315 	 0.04945 	 ~...
   74 	    13 	 0.15058 	 0.04982 	 MISS
   60 	    14 	 0.14645 	 0.04992 	 m..s
    7 	    15 	 0.04879 	 0.04997 	 ~...
   43 	    16 	 0.14026 	 0.04997 	 m..s
   11 	    17 	 0.05594 	 0.05000 	 ~...
    2 	    18 	 0.04285 	 0.05070 	 ~...
   70 	    19 	 0.14830 	 0.05077 	 m..s
   79 	    20 	 0.15404 	 0.05099 	 MISS
   66 	    21 	 0.14740 	 0.05206 	 m..s
   68 	    22 	 0.14800 	 0.05247 	 m..s
   29 	    23 	 0.13484 	 0.05263 	 m..s
   78 	    24 	 0.15212 	 0.05287 	 m..s
   23 	    25 	 0.13268 	 0.05298 	 m..s
   71 	    26 	 0.14832 	 0.05325 	 m..s
   77 	    27 	 0.15193 	 0.05325 	 m..s
    1 	    28 	 0.04077 	 0.05331 	 ~...
   28 	    29 	 0.13471 	 0.05360 	 m..s
   63 	    30 	 0.14650 	 0.05361 	 m..s
    6 	    31 	 0.04869 	 0.05363 	 ~...
   25 	    32 	 0.13402 	 0.05379 	 m..s
   12 	    33 	 0.05807 	 0.05382 	 ~...
   82 	    34 	 0.15625 	 0.05398 	 MISS
   22 	    35 	 0.12904 	 0.05412 	 m..s
   47 	    36 	 0.14261 	 0.05430 	 m..s
    0 	    37 	 0.04056 	 0.05447 	 ~...
   13 	    38 	 0.05844 	 0.05462 	 ~...
   85 	    39 	 0.16167 	 0.05486 	 MISS
    5 	    40 	 0.04355 	 0.05498 	 ~...
   80 	    41 	 0.15437 	 0.05502 	 m..s
   51 	    42 	 0.14378 	 0.05520 	 m..s
   67 	    43 	 0.14796 	 0.05541 	 m..s
   46 	    44 	 0.14248 	 0.05574 	 m..s
   16 	    45 	 0.06539 	 0.05582 	 ~...
   73 	    46 	 0.14985 	 0.05600 	 m..s
   34 	    47 	 0.13666 	 0.05657 	 m..s
   27 	    48 	 0.13468 	 0.05664 	 m..s
   18 	    49 	 0.12014 	 0.05797 	 m..s
   31 	    50 	 0.13546 	 0.05804 	 m..s
   83 	    51 	 0.16013 	 0.05807 	 MISS
   62 	    52 	 0.14649 	 0.05813 	 m..s
   81 	    53 	 0.15463 	 0.05960 	 m..s
   14 	    54 	 0.06014 	 0.06022 	 ~...
    8 	    55 	 0.04880 	 0.06028 	 ~...
   35 	    56 	 0.13671 	 0.16674 	 m..s
   24 	    57 	 0.13326 	 0.19199 	 m..s
   52 	    58 	 0.14505 	 0.20737 	 m..s
   69 	    59 	 0.14821 	 0.21829 	 m..s
   42 	    60 	 0.13965 	 0.21836 	 m..s
   61 	    61 	 0.14646 	 0.22588 	 m..s
   75 	    62 	 0.15109 	 0.22610 	 m..s
   54 	    63 	 0.14518 	 0.22648 	 m..s
   26 	    64 	 0.13421 	 0.22899 	 m..s
   20 	    65 	 0.12721 	 0.23183 	 MISS
   84 	    66 	 0.16020 	 0.23698 	 m..s
   56 	    67 	 0.14529 	 0.23729 	 m..s
   87 	    68 	 0.16312 	 0.23928 	 m..s
   72 	    69 	 0.14977 	 0.24060 	 m..s
   30 	    70 	 0.13526 	 0.24098 	 MISS
   50 	    71 	 0.14371 	 0.24121 	 m..s
   76 	    72 	 0.15112 	 0.24269 	 m..s
   17 	    73 	 0.12012 	 0.24855 	 MISS
   41 	    74 	 0.13951 	 0.24863 	 MISS
   55 	    75 	 0.14521 	 0.24952 	 MISS
   39 	    76 	 0.13858 	 0.24969 	 MISS
   53 	    77 	 0.14512 	 0.25189 	 MISS
   38 	    78 	 0.13834 	 0.25445 	 MISS
   19 	    79 	 0.12368 	 0.26233 	 MISS
   48 	    80 	 0.14301 	 0.26242 	 MISS
   40 	    81 	 0.13864 	 0.26361 	 MISS
   57 	    82 	 0.14545 	 0.26367 	 MISS
   36 	    83 	 0.13689 	 0.26501 	 MISS
   88 	    84 	 0.19202 	 0.26671 	 m..s
   45 	    85 	 0.14182 	 0.26715 	 MISS
   59 	    86 	 0.14560 	 0.26720 	 MISS
   21 	    87 	 0.12852 	 0.27274 	 MISS
   33 	    88 	 0.13616 	 0.27353 	 MISS
   96 	    89 	 0.40142 	 0.35679 	 m..s
   93 	    90 	 0.37323 	 0.36122 	 ~...
   91 	    91 	 0.37015 	 0.36206 	 ~...
   89 	    92 	 0.35384 	 0.36367 	 ~...
   92 	    93 	 0.37295 	 0.36692 	 ~...
   95 	    94 	 0.39701 	 0.37131 	 ~...
   90 	    95 	 0.36475 	 0.37669 	 ~...
   94 	    96 	 0.38417 	 0.38881 	 ~...
   97 	    97 	 0.40262 	 0.39325 	 ~...
   99 	    98 	 0.41115 	 0.40889 	 ~...
   98 	    99 	 0.41101 	 0.42314 	 ~...
  118 	   100 	 0.45172 	 0.42492 	 ~...
  106 	   101 	 0.41928 	 0.42502 	 ~...
  103 	   102 	 0.41425 	 0.42847 	 ~...
  112 	   103 	 0.42506 	 0.43046 	 ~...
  114 	   104 	 0.42759 	 0.43133 	 ~...
  107 	   105 	 0.41948 	 0.43261 	 ~...
  100 	   106 	 0.41116 	 0.43446 	 ~...
  116 	   107 	 0.43452 	 0.43783 	 ~...
  105 	   108 	 0.41743 	 0.43934 	 ~...
  111 	   109 	 0.42363 	 0.43997 	 ~...
  102 	   110 	 0.41423 	 0.44003 	 ~...
  109 	   111 	 0.42342 	 0.44039 	 ~...
  119 	   112 	 0.45399 	 0.44335 	 ~...
  110 	   113 	 0.42353 	 0.44401 	 ~...
  104 	   114 	 0.41503 	 0.44444 	 ~...
  115 	   115 	 0.42774 	 0.44573 	 ~...
  117 	   116 	 0.44866 	 0.44770 	 ~...
  113 	   117 	 0.42752 	 0.44777 	 ~...
  108 	   118 	 0.42011 	 0.45100 	 m..s
  101 	   119 	 0.41263 	 0.46297 	 m..s
  120 	   120 	 0.46895 	 0.46434 	 ~...
==========================================
r_mrr = 0.8694595694541931
r2_mrr = 0.7555838823318481
spearmanr_mrr@5 = 0.8034808039665222
spearmanr_mrr@10 = 0.9068478941917419
spearmanr_mrr@50 = 0.9833829998970032
spearmanr_mrr@100 = 0.8780596256256104
spearmanr_mrr@All = 0.9014527201652527
==========================================
test time: 0.404
Done Testing dataset Kinships
total time taken: 187.82921195030212
training time taken: 180.9760389328003
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8695)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7556)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.8035)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9068)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9834)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8781)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9015)}}, 'test_loss': {'DistMult': {'Kinships': 7.194715311470645}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 948062385052008
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [52, 563, 1115, 45, 493, 121, 392, 1211, 732, 252, 113, 938, 804, 825, 316, 961, 372, 1046, 669, 998, 1142, 515, 75, 152, 109, 803, 66, 1044, 212, 1084, 56, 1026, 817, 810, 558, 20, 214, 882, 614, 1133, 446, 506, 885, 1078, 131, 1201, 51, 719, 657, 703, 267, 147, 780, 595, 649, 280, 712, 496, 687, 343, 42, 728, 61, 202, 776, 379, 1061, 1150, 1144, 327, 958, 888, 1207, 63, 1132, 996, 277, 3, 569, 620, 1176, 319, 122, 799, 1072, 824, 1036, 1168, 919, 291, 1148, 701, 352, 706, 502, 704, 869, 295, 254, 845, 1135, 371, 546, 838, 737, 135, 808, 668, 698, 555, 1100, 1165, 1068, 647, 491, 158, 30, 631, 178, 878, 950]
valid_ids (0): []
train_ids (1094): [500, 238, 453, 495, 169, 336, 643, 1017, 1178, 389, 835, 41, 822, 17, 222, 1103, 397, 46, 304, 809, 963, 854, 747, 76, 613, 1208, 307, 617, 261, 257, 1114, 263, 265, 849, 1092, 575, 659, 209, 628, 784, 11, 276, 1185, 433, 664, 33, 755, 576, 1000, 862, 1059, 997, 432, 60, 408, 978, 124, 1099, 931, 132, 739, 119, 157, 1164, 840, 912, 133, 847, 298, 440, 443, 801, 416, 449, 108, 35, 384, 800, 218, 635, 182, 421, 1153, 211, 904, 1121, 95, 74, 1094, 905, 383, 830, 1066, 522, 1199, 898, 557, 44, 989, 606, 237, 322, 105, 480, 125, 684, 516, 691, 476, 83, 346, 84, 577, 272, 895, 789, 1049, 964, 1110, 193, 565, 782, 40, 1005, 990, 299, 674, 87, 303, 475, 884, 920, 427, 771, 438, 472, 441, 1063, 781, 922, 1210, 976, 770, 354, 334, 466, 89, 1156, 163, 987, 350, 901, 320, 488, 189, 715, 1091, 879, 615, 127, 952, 451, 1180, 458, 72, 796, 155, 1206, 1183, 1116, 305, 481, 274, 399, 422, 842, 529, 1169, 477, 380, 536, 330, 654, 850, 177, 678, 638, 233, 827, 1031, 794, 924, 154, 752, 589, 370, 868, 607, 501, 112, 525, 1012, 543, 486, 936, 183, 791, 123, 85, 955, 857, 192, 130, 805, 1147, 911, 510, 297, 129, 767, 205, 559, 150, 411, 527, 623, 1124, 381, 311, 1122, 1024, 892, 420, 1073, 685, 361, 1071, 1111, 344, 957, 73, 413, 180, 772, 984, 253, 1161, 251, 754, 512, 511, 1038, 586, 347, 459, 484, 54, 1209, 1138, 726, 841, 889, 966, 926, 430, 220, 626, 1003, 677, 206, 795, 906, 262, 562, 896, 398, 1043, 1214, 1140, 593, 1011, 144, 1143, 673, 1196, 634, 695, 326, 191, 661, 1181, 848, 723, 1002, 391, 683, 540, 448, 549, 603, 315, 245, 627, 883, 203, 890, 282, 934, 337, 1014, 1001, 582, 579, 244, 533, 969, 139, 232, 0, 520, 531, 537, 294, 983, 743, 679, 705, 1193, 221, 975, 1204, 454, 1137, 948, 9, 710, 513, 368, 915, 14, 748, 839, 479, 1163, 769, 1159, 104, 436, 1055, 713, 418, 1057, 1170, 207, 289, 1083, 1076, 640, 844, 867, 736, 417, 1197, 229, 393, 259, 1009, 145, 1025, 786, 201, 619, 175, 483, 447, 338, 4, 1125, 914, 235, 973, 834, 1104, 339, 725, 28, 530, 716, 956, 226, 50, 1152, 1155, 463, 876, 55, 1128, 410, 128, 941, 474, 688, 714, 733, 67, 341, 740, 27, 700, 1139, 317, 57, 863, 377, 199, 1106, 407, 329, 310, 1096, 734, 991, 342, 1089, 821, 930, 746, 281, 699, 621, 762, 564, 1051, 351, 376, 1171, 137, 120, 903, 223, 916, 390, 273, 78, 553, 439, 107, 190, 751, 648, 65, 609, 18, 608, 897, 618, 818, 306, 1184, 1172, 856, 325, 694, 1020, 947, 25, 548, 697, 968, 764, 1047, 1157, 1162, 761, 13, 852, 759, 717, 159, 1175, 711, 395, 1149, 12, 424, 1117, 880, 219, 1119, 521, 881, 148, 596, 909, 833, 1056, 247, 798, 1041, 47, 605, 414, 806, 239, 32, 165, 1120, 940, 1213, 58, 836, 629, 318, 204, 581, 656, 749, 756, 541, 231, 1203, 887, 450, 434, 1205, 864, 375, 571, 331, 721, 652, 369, 362, 602, 665, 170, 918, 1015, 729, 181, 278, 1053, 210, 1032, 250, 526, 625, 1064, 524, 290, 19, 161, 270, 401, 302, 788, 610, 22, 374, 1037, 913, 323, 1190, 1018, 999, 15, 572, 16, 444, 508, 300, 758, 946, 1167, 1, 279, 1022, 213, 140, 1028, 927, 312, 256, 908, 875, 283, 689, 1045, 954, 452, 561, 168, 340, 872, 1195, 1200, 101, 1075, 172, 651, 470, 599, 1006, 1189, 1202, 959, 388, 773, 967, 765, 1013, 404, 349, 675, 23, 923, 1112, 591, 1188, 308, 935, 5, 1198, 523, 859, 972, 142, 921, 69, 632, 249, 598, 36, 425, 1040, 116, 81, 409, 179, 284, 38, 1035, 542, 151, 509, 636, 62, 663, 951, 473, 1007, 1130, 1192, 637, 702, 489, 1182, 1126, 566, 1146, 91, 241, 583, 240, 584, 456, 980, 387, 1050, 1131, 230, 217, 43, 1151, 853, 503, 24, 960, 680, 568, 939, 497, 1158, 587, 768, 787, 823, 21, 29, 738, 492, 1166, 1101, 365, 103, 811, 90, 266, 328, 394, 136, 953, 482, 932, 1033, 925, 39, 720, 594, 1085, 1090, 1023, 550, 670, 622, 80, 666, 471, 630, 271, 866, 832, 464, 658, 532, 692, 797, 578, 910, 945, 560, 1065, 505, 611, 356, 707, 146, 624, 828, 763, 1102, 248, 268, 469, 861, 1098, 774, 644, 186, 332, 727, 70, 184, 360, 173, 745, 1145, 645, 1093, 1008, 551, 402, 224, 1067, 153, 400, 461, 646, 79, 1019, 415, 367, 197, 164, 275, 816, 141, 1034, 815, 200, 92, 994, 777, 301, 544, 208, 855, 460, 48, 775, 364, 499, 264, 378, 676, 545, 26, 287, 366, 457, 236, 877, 288, 187, 313, 321, 837, 539, 353, 465, 196, 437, 1134, 1129, 333, 783, 7, 98, 445, 385, 1212, 977, 6, 345, 82, 494, 215, 985, 900, 633, 462, 406, 1021, 874, 528, 348, 292, 807, 94, 485, 731, 742, 382, 655, 53, 672, 616, 1187, 363, 138, 1095, 870, 242, 386, 110, 943, 753, 760, 735, 1179, 1113, 1127, 355, 100, 357, 696, 1029, 162, 1077, 194, 426, 309, 419, 650, 1173, 59, 722, 965, 405, 792, 143, 64, 937, 682, 156, 902, 744, 335, 1186, 1105, 612, 690, 1069, 126, 871, 8, 819, 592, 49, 31, 1088, 974, 1016, 653, 174, 1048, 258, 829, 894, 730, 97, 504, 1039, 1141, 814, 1107, 358, 917, 490, 435, 986, 1154, 99, 293, 865, 428, 724, 962, 891, 114, 68, 234, 933, 71, 641, 662, 296, 1010, 944, 547, 1062, 570, 979, 285, 802, 552, 228, 2, 766, 554, 1027, 149, 468, 535, 660, 77, 779, 899, 403, 1097, 324, 102, 314, 860, 1109, 246, 255, 517, 667, 1160, 34, 970, 111, 718, 1074, 167, 518, 873, 1030, 198, 995, 580, 507, 604, 227, 590, 442, 1082, 1080, 534, 498, 639, 686, 992, 971, 812, 118, 574, 1004, 750, 269, 431, 681, 597, 1194, 981, 826, 556, 86, 478, 519, 928, 1177, 982, 1087, 1136, 886, 243, 429, 1123, 1052, 37, 1054, 793, 708, 538, 93, 117, 588, 600, 396, 567, 893, 671, 1060, 693, 1081, 487, 757, 942, 843, 846, 929, 166, 216, 195, 188, 88, 790, 160, 949, 709, 907, 1058, 467, 831, 642, 601, 1042, 820, 373, 96, 10, 115, 785, 412, 514, 1118, 813, 359, 286, 455, 741, 993, 1079, 176, 171, 851, 1174, 423, 778, 1070, 988, 858, 134, 185, 1086, 1191, 106, 1108, 260, 225, 585, 573]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4941401439333306
the save name prefix for this run is:  chkpt-ID_4941401439333306_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1093
rank avg (pred): 0.500 +- 0.002
mrr vals (pred, true): 0.019, 0.254
batch losses (mrrl, rdl): 0.0, 0.0032311818

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 928
rank avg (pred): 0.289 +- 0.194
mrr vals (pred, true): 0.152, 0.048
batch losses (mrrl, rdl): 0.0, 0.0006263788

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1067
rank avg (pred): 0.061 +- 0.047
mrr vals (pred, true): 0.358, 0.452
batch losses (mrrl, rdl): 0.0, 1.4664e-06

Epoch over!
epoch time: 12.078

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 145
rank avg (pred): 0.281 +- 0.223
mrr vals (pred, true): 0.277, 0.256
batch losses (mrrl, rdl): 0.0, 0.0009453627

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 360
rank avg (pred): 0.274 +- 0.225
mrr vals (pred, true): 0.317, 0.259
batch losses (mrrl, rdl): 0.0, 0.0009366696

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 986
rank avg (pred): 0.018 +- 0.016
mrr vals (pred, true): 0.548, 0.444
batch losses (mrrl, rdl): 0.0, 2.76974e-05

Epoch over!
epoch time: 11.91

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 992
rank avg (pred): 0.026 +- 0.023
mrr vals (pred, true): 0.509, 0.432
batch losses (mrrl, rdl): 0.0, 2.07617e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 250
rank avg (pred): 0.060 +- 0.052
mrr vals (pred, true): 0.432, 0.440
batch losses (mrrl, rdl): 0.0, 1.9756e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 322
rank avg (pred): 0.064 +- 0.056
mrr vals (pred, true): 0.441, 0.446
batch losses (mrrl, rdl): 0.0, 2.2393e-06

Epoch over!
epoch time: 11.756

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 242
rank avg (pred): 0.255 +- 0.214
mrr vals (pred, true): 0.314, 0.060
batch losses (mrrl, rdl): 0.0, 0.0006960763

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 741
rank avg (pred): 0.127 +- 0.107
mrr vals (pred, true): 0.367, 0.338
batch losses (mrrl, rdl): 0.0, 8.37687e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 285
rank avg (pred): 0.048 +- 0.043
mrr vals (pred, true): 0.472, 0.455
batch losses (mrrl, rdl): 0.0, 1.6826e-06

Epoch over!
epoch time: 11.848

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 175
rank avg (pred): 0.242 +- 0.215
mrr vals (pred, true): 0.395, 0.052
batch losses (mrrl, rdl): 0.0, 0.0007498806

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1003
rank avg (pred): 0.258 +- 0.222
mrr vals (pred, true): 0.358, 0.248
batch losses (mrrl, rdl): 0.0, 0.000742292

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 206
rank avg (pred): 0.282 +- 0.241
mrr vals (pred, true): 0.333, 0.053
batch losses (mrrl, rdl): 0.0, 0.0005025417

Epoch over!
epoch time: 11.951

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 229
rank avg (pred): 0.258 +- 0.227
mrr vals (pred, true): 0.357, 0.062
batch losses (mrrl, rdl): 0.945376277, 0.0005131056

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 33
rank avg (pred): 0.020 +- 0.012
mrr vals (pred, true): 0.427, 0.444
batch losses (mrrl, rdl): 0.0031140076, 3.0876e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 210
rank avg (pred): 0.332 +- 0.212
mrr vals (pred, true): 0.151, 0.049
batch losses (mrrl, rdl): 0.1016630828, 0.0002923826

Epoch over!
epoch time: 12.125

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 596
rank avg (pred): 0.330 +- 0.205
mrr vals (pred, true): 0.139, 0.207
batch losses (mrrl, rdl): 0.0472352132, 0.00087965

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1081
rank avg (pred): 0.259 +- 0.169
mrr vals (pred, true): 0.150, 0.228
batch losses (mrrl, rdl): 0.0619988181, 0.0006067162

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 482
rank avg (pred): 0.282 +- 0.193
mrr vals (pred, true): 0.182, 0.053
batch losses (mrrl, rdl): 0.1748750806, 0.0006330393

Epoch over!
epoch time: 12.084

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 516
rank avg (pred): 0.035 +- 0.024
mrr vals (pred, true): 0.383, 0.395
batch losses (mrrl, rdl): 0.0014339968, 2.11964e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 538
rank avg (pred): 0.031 +- 0.022
mrr vals (pred, true): 0.402, 0.374
batch losses (mrrl, rdl): 0.0082496116, 2.89367e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 765
rank avg (pred): 0.548 +- 0.246
mrr vals (pred, true): 0.065, 0.054
batch losses (mrrl, rdl): 0.002325298, 0.0002328416

Epoch over!
epoch time: 12.277

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1184
rank avg (pred): 0.252 +- 0.158
mrr vals (pred, true): 0.145, 0.238
batch losses (mrrl, rdl): 0.0864608139, 0.0005883694

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 87
rank avg (pred): 0.337 +- 0.206
mrr vals (pred, true): 0.117, 0.240
batch losses (mrrl, rdl): 0.1515559703, 0.0013852179

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 889
rank avg (pred): 0.455 +- 0.224
mrr vals (pred, true): 0.085, 0.053
batch losses (mrrl, rdl): 0.0124040758, 1.00156e-05

Epoch over!
epoch time: 12.231

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 175
rank avg (pred): 0.348 +- 0.200
mrr vals (pred, true): 0.121, 0.052
batch losses (mrrl, rdl): 0.0501145944, 0.0002057185

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 857
rank avg (pred): 0.552 +- 0.264
mrr vals (pred, true): 0.079, 0.054
batch losses (mrrl, rdl): 0.0085572638, 0.0002374793

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 707
rank avg (pred): 0.330 +- 0.210
mrr vals (pred, true): 0.150, 0.053
batch losses (mrrl, rdl): 0.0992447063, 0.000274267

Epoch over!
epoch time: 11.86

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 665
rank avg (pred): 0.329 +- 0.209
mrr vals (pred, true): 0.144, 0.060
batch losses (mrrl, rdl): 0.0878897011, 0.0002733581

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1096
rank avg (pred): 0.302 +- 0.193
mrr vals (pred, true): 0.148, 0.241
batch losses (mrrl, rdl): 0.0856955051, 0.0005771287

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1188
rank avg (pred): 0.316 +- 0.191
mrr vals (pred, true): 0.118, 0.055
batch losses (mrrl, rdl): 0.0462981462, 0.0003825178

Epoch over!
epoch time: 12.121

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 656
rank avg (pred): 0.331 +- 0.206
mrr vals (pred, true): 0.142, 0.045
batch losses (mrrl, rdl): 0.0838511512, 0.0003450925

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 247
rank avg (pred): 0.034 +- 0.024
mrr vals (pred, true): 0.385, 0.440
batch losses (mrrl, rdl): 0.0295558665, 7.5379e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 688
rank avg (pred): 0.334 +- 0.203
mrr vals (pred, true): 0.136, 0.056
batch losses (mrrl, rdl): 0.0738943443, 0.0002380272

Epoch over!
epoch time: 11.95

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 312
rank avg (pred): 0.027 +- 0.019
mrr vals (pred, true): 0.419, 0.441
batch losses (mrrl, rdl): 0.0049144709, 2.07913e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 759
rank avg (pred): 0.579 +- 0.257
mrr vals (pred, true): 0.070, 0.052
batch losses (mrrl, rdl): 0.0041284524, 0.0002338789

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 820
rank avg (pred): 0.024 +- 0.016
mrr vals (pred, true): 0.430, 0.421
batch losses (mrrl, rdl): 0.0008904113, 2.22473e-05

Epoch over!
epoch time: 11.935

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 418
rank avg (pred): 0.337 +- 0.198
mrr vals (pred, true): 0.132, 0.056
batch losses (mrrl, rdl): 0.0678220019, 0.0002826609

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 916
rank avg (pred): 0.172 +- 0.095
mrr vals (pred, true): 0.137, 0.085
batch losses (mrrl, rdl): 0.0761729702, 0.0001255336

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 81
rank avg (pred): 0.344 +- 0.192
mrr vals (pred, true): 0.126, 0.243
batch losses (mrrl, rdl): 0.1370751113, 0.0014550457

Epoch over!
epoch time: 11.95

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 347
rank avg (pred): 0.327 +- 0.201
mrr vals (pred, true): 0.140, 0.274
batch losses (mrrl, rdl): 0.1784258187, 0.0013017871

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 785
rank avg (pred): 0.618 +- 0.252
mrr vals (pred, true): 0.060, 0.054
batch losses (mrrl, rdl): 0.0010852932, 0.000534301

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 201
rank avg (pred): 0.336 +- 0.196
mrr vals (pred, true): 0.134, 0.049
batch losses (mrrl, rdl): 0.0702823326, 0.0003333167

Epoch over!
epoch time: 12.062

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.026 +- 0.019
mrr vals (pred, true): 0.433, 0.450

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.11151 	 0.02078 	 m..s
   18 	     1 	 0.09332 	 0.02078 	 m..s
   17 	     2 	 0.07093 	 0.04497 	 ~...
    0 	     3 	 0.05821 	 0.04646 	 ~...
   22 	     4 	 0.13516 	 0.04754 	 m..s
   15 	     5 	 0.06406 	 0.04771 	 ~...
   14 	     6 	 0.05837 	 0.04874 	 ~...
   28 	     7 	 0.13632 	 0.04883 	 m..s
    0 	     8 	 0.05821 	 0.04927 	 ~...
    0 	     9 	 0.05821 	 0.04957 	 ~...
    0 	    10 	 0.05821 	 0.04970 	 ~...
    0 	    11 	 0.05821 	 0.04978 	 ~...
   63 	    12 	 0.14960 	 0.04992 	 m..s
   70 	    13 	 0.15592 	 0.05077 	 MISS
   33 	    14 	 0.13810 	 0.05108 	 m..s
   31 	    15 	 0.13755 	 0.05116 	 m..s
    0 	    16 	 0.05821 	 0.05126 	 ~...
    0 	    17 	 0.05821 	 0.05162 	 ~...
   56 	    18 	 0.14453 	 0.05193 	 m..s
   16 	    19 	 0.06529 	 0.05197 	 ~...
   25 	    20 	 0.13535 	 0.05214 	 m..s
   35 	    21 	 0.13860 	 0.05216 	 m..s
    0 	    22 	 0.05821 	 0.05241 	 ~...
    0 	    23 	 0.05821 	 0.05244 	 ~...
   72 	    24 	 0.15842 	 0.05250 	 MISS
    0 	    25 	 0.05821 	 0.05254 	 ~...
   20 	    26 	 0.13472 	 0.05287 	 m..s
   48 	    27 	 0.14107 	 0.05309 	 m..s
   44 	    28 	 0.14084 	 0.05314 	 m..s
   57 	    29 	 0.14567 	 0.05324 	 m..s
   73 	    30 	 0.15971 	 0.05340 	 MISS
    0 	    31 	 0.05821 	 0.05365 	 ~...
   62 	    32 	 0.14939 	 0.05381 	 m..s
   65 	    33 	 0.15082 	 0.05412 	 m..s
   64 	    34 	 0.15072 	 0.05423 	 m..s
    0 	    35 	 0.05821 	 0.05460 	 ~...
   21 	    36 	 0.13514 	 0.05501 	 m..s
   37 	    37 	 0.13982 	 0.05502 	 m..s
   66 	    38 	 0.15148 	 0.05543 	 m..s
   52 	    39 	 0.14284 	 0.05664 	 m..s
   42 	    40 	 0.14043 	 0.05772 	 m..s
   71 	    41 	 0.15801 	 0.05804 	 m..s
   39 	    42 	 0.14024 	 0.05840 	 m..s
   67 	    43 	 0.15157 	 0.05848 	 m..s
   45 	    44 	 0.14097 	 0.05955 	 m..s
    0 	    45 	 0.05821 	 0.06022 	 ~...
    0 	    46 	 0.05821 	 0.06028 	 ~...
   41 	    47 	 0.14043 	 0.06230 	 m..s
   46 	    48 	 0.14100 	 0.18207 	 m..s
   49 	    49 	 0.14109 	 0.18802 	 m..s
   75 	    50 	 0.20264 	 0.20613 	 ~...
   68 	    51 	 0.15241 	 0.21711 	 m..s
   58 	    52 	 0.14639 	 0.21752 	 m..s
   43 	    53 	 0.14047 	 0.21967 	 m..s
   24 	    54 	 0.13519 	 0.22230 	 m..s
   34 	    55 	 0.13842 	 0.23156 	 m..s
   61 	    56 	 0.14924 	 0.23687 	 m..s
   30 	    57 	 0.13740 	 0.23787 	 MISS
   69 	    58 	 0.15273 	 0.23790 	 m..s
   50 	    59 	 0.14133 	 0.23799 	 m..s
   51 	    60 	 0.14191 	 0.23872 	 m..s
   55 	    61 	 0.14421 	 0.24172 	 m..s
   74 	    62 	 0.16275 	 0.24434 	 m..s
   27 	    63 	 0.13540 	 0.24474 	 MISS
   40 	    64 	 0.14030 	 0.24764 	 MISS
   60 	    65 	 0.14740 	 0.24818 	 MISS
   54 	    66 	 0.14387 	 0.24969 	 MISS
   26 	    67 	 0.13539 	 0.24997 	 MISS
   59 	    68 	 0.14710 	 0.25397 	 MISS
   47 	    69 	 0.14102 	 0.26135 	 MISS
   36 	    70 	 0.13947 	 0.26242 	 MISS
   29 	    71 	 0.13675 	 0.26251 	 MISS
   32 	    72 	 0.13782 	 0.26472 	 MISS
   38 	    73 	 0.14017 	 0.26716 	 MISS
   23 	    74 	 0.13518 	 0.26719 	 MISS
   53 	    75 	 0.14292 	 0.29102 	 MISS
   76 	    76 	 0.35880 	 0.30493 	 m..s
   86 	    77 	 0.39577 	 0.36521 	 m..s
   84 	    78 	 0.39396 	 0.36692 	 ~...
   82 	    79 	 0.38613 	 0.36813 	 ~...
   77 	    80 	 0.36713 	 0.36815 	 ~...
   85 	    81 	 0.39525 	 0.37761 	 ~...
   92 	    82 	 0.42705 	 0.37994 	 m..s
   80 	    83 	 0.38351 	 0.38105 	 ~...
   81 	    84 	 0.38358 	 0.38337 	 ~...
   87 	    85 	 0.39585 	 0.38415 	 ~...
   90 	    86 	 0.42550 	 0.38422 	 m..s
   88 	    87 	 0.41840 	 0.38768 	 m..s
   91 	    88 	 0.42639 	 0.39223 	 m..s
   78 	    89 	 0.38323 	 0.39602 	 ~...
   79 	    90 	 0.38350 	 0.39875 	 ~...
   83 	    91 	 0.38642 	 0.40130 	 ~...
  120 	    92 	 0.50214 	 0.40942 	 m..s
   89 	    93 	 0.42422 	 0.41283 	 ~...
  108 	    94 	 0.45107 	 0.41901 	 m..s
   94 	    95 	 0.43229 	 0.42314 	 ~...
  100 	    96 	 0.44239 	 0.42791 	 ~...
  103 	    97 	 0.44732 	 0.42947 	 ~...
  112 	    98 	 0.45527 	 0.43023 	 ~...
  102 	    99 	 0.44697 	 0.43180 	 ~...
   96 	   100 	 0.43511 	 0.43229 	 ~...
   99 	   101 	 0.44155 	 0.43244 	 ~...
  101 	   102 	 0.44473 	 0.43257 	 ~...
   93 	   103 	 0.43112 	 0.43934 	 ~...
  109 	   104 	 0.45405 	 0.44003 	 ~...
  105 	   105 	 0.45009 	 0.44083 	 ~...
  104 	   106 	 0.44807 	 0.44314 	 ~...
  114 	   107 	 0.46847 	 0.44370 	 ~...
  111 	   108 	 0.45512 	 0.44414 	 ~...
   98 	   109 	 0.44071 	 0.44555 	 ~...
  118 	   110 	 0.47752 	 0.44604 	 m..s
   97 	   111 	 0.43637 	 0.44616 	 ~...
  107 	   112 	 0.45052 	 0.44622 	 ~...
  106 	   113 	 0.45013 	 0.44704 	 ~...
  110 	   114 	 0.45490 	 0.44804 	 ~...
   95 	   115 	 0.43303 	 0.45046 	 ~...
  119 	   116 	 0.48334 	 0.45098 	 m..s
  117 	   117 	 0.47538 	 0.45194 	 ~...
  113 	   118 	 0.46267 	 0.45465 	 ~...
  116 	   119 	 0.47492 	 0.45926 	 ~...
  115 	   120 	 0.47319 	 0.47648 	 ~...
==========================================
r_mrr = 0.9102528095245361
r2_mrr = 0.8254917860031128
spearmanr_mrr@5 = 0.9990416765213013
spearmanr_mrr@10 = 0.9356383681297302
spearmanr_mrr@50 = 0.9601439833641052
spearmanr_mrr@100 = 0.9040317535400391
spearmanr_mrr@All = 0.928221583366394
==========================================
test time: 0.458
Done Testing dataset Kinships
total time taken: 187.994873046875
training time taken: 180.675616979599
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.9103)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.8255)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9990)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9356)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9601)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.9040)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9282)}}, 'test_loss': {'DistMult': {'Kinships': 5.734441379499913}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 9858336502325712
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [669, 902, 1097, 203, 713, 906, 518, 192, 608, 315, 1199, 391, 71, 875, 561, 501, 860, 486, 724, 1063, 722, 149, 1212, 502, 579, 631, 1075, 411, 202, 1057, 92, 265, 793, 781, 924, 765, 842, 858, 1117, 488, 225, 56, 616, 605, 434, 178, 673, 69, 309, 487, 297, 215, 206, 901, 218, 1038, 310, 1109, 721, 507, 459, 29, 878, 95, 1125, 515, 566, 460, 755, 195, 967, 868, 278, 792, 437, 657, 907, 954, 370, 670, 600, 876, 861, 83, 830, 469, 201, 412, 426, 895, 466, 1163, 963, 700, 144, 745, 809, 395, 814, 267, 828, 464, 243, 893, 462, 934, 946, 942, 645, 98, 142, 485, 1011, 1072, 472, 153, 748, 1150, 372, 30, 768]
valid_ids (0): []
train_ids (1094): [870, 899, 743, 692, 877, 448, 183, 815, 603, 774, 408, 479, 266, 14, 270, 163, 1160, 343, 523, 800, 1197, 898, 834, 879, 1034, 498, 617, 483, 984, 357, 1012, 1053, 24, 1171, 602, 286, 1052, 970, 232, 1189, 23, 275, 332, 799, 969, 950, 953, 449, 402, 806, 689, 513, 1149, 49, 885, 965, 42, 574, 628, 1143, 156, 709, 606, 188, 443, 1062, 543, 569, 549, 827, 551, 918, 496, 87, 222, 558, 582, 126, 846, 396, 429, 346, 1156, 362, 941, 504, 1136, 711, 1030, 5, 1067, 398, 481, 1024, 554, 474, 478, 880, 480, 161, 750, 1152, 935, 919, 2, 634, 982, 347, 445, 575, 340, 921, 1095, 511, 734, 703, 1081, 1180, 625, 1146, 604, 9, 1015, 1201, 1008, 116, 550, 720, 679, 832, 913, 1157, 714, 674, 307, 399, 761, 312, 678, 980, 304, 939, 118, 185, 824, 651, 512, 471, 923, 74, 263, 299, 430, 510, 912, 291, 88, 848, 620, 699, 650, 268, 432, 205, 955, 1035, 306, 4, 1124, 1004, 1089, 756, 50, 84, 695, 60, 936, 305, 335, 938, 859, 422, 123, 1202, 991, 1209, 593, 509, 311, 811, 331, 122, 820, 114, 843, 764, 138, 117, 476, 247, 903, 995, 482, 668, 500, 237, 612, 302, 53, 21, 294, 663, 998, 420, 562, 255, 889, 945, 572, 624, 327, 567, 1166, 1019, 1059, 719, 588, 986, 1168, 130, 872, 18, 613, 647, 896, 1006, 406, 216, 129, 882, 1190, 1003, 660, 1086, 388, 102, 694, 390, 590, 12, 580, 182, 837, 548, 1040, 696, 121, 254, 491, 922, 1206, 51, 367, 1056, 214, 1014, 345, 1179, 160, 497, 184, 1016, 319, 470, 702, 1042, 32, 869, 637, 57, 687, 1069, 55, 850, 988, 855, 1064, 823, 981, 1126, 654, 221, 968, 622, 990, 517, 1093, 38, 900, 704, 556, 63, 1214, 355, 667, 423, 808, 17, 379, 1147, 979, 169, 829, 72, 888, 971, 147, 145, 47, 1107, 812, 492, 839, 1151, 1060, 1017, 1049, 973, 65, 409, 905, 1174, 150, 235, 977, 863, 818, 1087, 997, 494, 931, 552, 1065, 1001, 256, 405, 1158, 1043, 547, 112, 13, 317, 174, 397, 104, 1205, 328, 26, 752, 228, 619, 585, 929, 797, 382, 61, 1007, 531, 926, 966, 508, 717, 181, 68, 887, 167, 538, 961, 801, 1113, 200, 521, 1144, 15, 441, 454, 289, 1025, 753, 760, 911, 1023, 959, 1132, 1196, 648, 48, 248, 1133, 707, 716, 97, 690, 366, 854, 323, 630, 639, 693, 665, 822, 394, 447, 640, 833, 1031, 34, 686, 28, 380, 1140, 140, 641, 1137, 164, 524, 136, 360, 656, 723, 344, 1118, 300, 1200, 1068, 27, 659, 1208, 350, 37, 1029, 363, 199, 1164, 1111, 826, 555, 632, 1120, 177, 279, 762, 77, 175, 996, 601, 1002, 1047, 573, 993, 1013, 975, 1092, 1198, 168, 36, 537, 303, 64, 577, 473, 635, 733, 281, 546, 985, 318, 728, 1080, 739, 751, 179, 1203, 428, 207, 1128, 949, 416, 952, 1050, 1055, 769, 100, 134, 359, 223, 599, 847, 614, 457, 607, 726, 333, 621, 227, 994, 269, 308, 1195, 514, 587, 103, 992, 789, 1175, 831, 1058, 414, 819, 892, 76, 239, 1121, 280, 1073, 782, 962, 701, 1061, 285, 948, 727, 598, 111, 718, 576, 381, 204, 1142, 940, 784, 146, 570, 532, 909, 356, 298, 11, 463, 45, 529, 377, 339, 1108, 1114, 475, 770, 410, 386, 749, 987, 85, 1018, 320, 871, 712, 817, 1022, 1182, 240, 874, 856, 807, 93, 638, 1032, 932, 224, 143, 341, 1103, 530, 626, 533, 964, 754, 920, 1210, 1041, 535, 106, 794, 1186, 385, 477, 219, 1167, 708, 86, 883, 226, 503, 453, 383, 506, 208, 737, 115, 771, 257, 245, 705, 70, 1194, 788, 795, 776, 132, 732, 729, 786, 220, 465, 780, 1123, 862, 908, 43, 425, 545, 39, 1110, 66, 615, 1161, 455, 1054, 1066, 611, 539, 337, 1187, 736, 779, 450, 1102, 1071, 231, 44, 119, 1094, 623, 804, 99, 542, 418, 387, 241, 1122, 461, 544, 260, 1207, 435, 958, 1099, 401, 1005, 564, 677, 1009, 857, 35, 59, 917, 933, 522, 1115, 91, 519, 54, 568, 1155, 353, 1173, 400, 1076, 1172, 1105, 681, 775, 864, 1048, 790, 125, 629, 277, 253, 609, 292, 419, 1010, 904, 1193, 415, 189, 1165, 886, 928, 944, 375, 351, 671, 661, 490, 565, 698, 664, 951, 1169, 421, 1159, 796, 747, 960, 259, 1176, 284, 595, 643, 1083, 155, 213, 338, 683, 676, 915, 162, 433, 436, 1039, 943, 326, 493, 772, 198, 738, 983, 282, 322, 1036, 691, 389, 1085, 1138, 252, 79, 1192, 652, 133, 778, 374, 499, 209, 293, 413, 1037, 427, 364, 1082, 1027, 1116, 314, 1026, 851, 31, 378, 742, 384, 1112, 440, 276, 894, 107, 816, 589, 321, 844, 658, 1045, 1070, 246, 316, 528, 1145, 777, 873, 516, 1020, 444, 1000, 417, 348, 1088, 1129, 75, 526, 685, 290, 947, 194, 1046, 810, 261, 890, 6, 8, 821, 586, 706, 120, 1183, 190, 358, 937, 594, 866, 697, 1101, 16, 1106, 110, 158, 1170, 791, 525, 557, 766, 249, 210, 881, 352, 62, 578, 897, 193, 196, 141, 757, 151, 197, 1211, 217, 1021, 655, 242, 1074, 78, 313, 258, 1091, 1077, 759, 113, 1188, 1104, 135, 1028, 489, 730, 541, 1134, 90, 58, 744, 914, 458, 369, 295, 803, 20, 646, 283, 105, 19, 715, 581, 725, 767, 81, 186, 1084, 956, 930, 1191, 735, 740, 927, 439, 584, 627, 244, 288, 468, 731, 3, 392, 999, 336, 1178, 1100, 233, 972, 989, 271, 1153, 52, 325, 324, 746, 108, 139, 636, 7, 211, 368, 234, 424, 925, 82, 22, 838, 349, 301, 451, 853, 171, 741, 484, 666, 505, 373, 137, 840, 10, 1162, 456, 152, 527, 33, 841, 559, 978, 553, 802, 371, 236, 1033, 642, 1130, 644, 1096, 852, 672, 773, 849, 296, 976, 212, 835, 1135, 1177, 495, 25, 407, 262, 73, 540, 404, 675, 124, 89, 273, 1204, 1098, 571, 758, 365, 618, 798, 610, 452, 287, 159, 393, 597, 264, 1119, 910, 446, 230, 891, 128, 633, 682, 127, 1127, 591, 1, 361, 191, 0, 520, 865, 1148, 131, 67, 354, 957, 251, 680, 867, 154, 274, 534, 172, 46, 109, 41, 166, 688, 1044, 825, 229, 176, 442, 836, 165, 653, 1139, 763, 157, 330, 974, 438, 560, 467, 101, 1131, 1079, 805, 329, 148, 334, 80, 649, 1154, 813, 170, 180, 563, 596, 884, 238, 250, 376, 536, 710, 173, 1181, 1051, 1185, 96, 592, 1184, 684, 845, 662, 787, 783, 403, 785, 916, 94, 1090, 583, 1078, 272, 431, 1213, 187, 40, 342, 1141]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2738235999805460
the save name prefix for this run is:  chkpt-ID_2738235999805460_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1110
rank avg (pred): 0.534 +- 0.003
mrr vals (pred, true): 0.018, 0.049
batch losses (mrrl, rdl): 0.0, 0.000169756

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 778
rank avg (pred): 0.310 +- 0.223
mrr vals (pred, true): 0.194, 0.047
batch losses (mrrl, rdl): 0.0, 0.0004438301

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 345
rank avg (pred): 0.296 +- 0.243
mrr vals (pred, true): 0.282, 0.238
batch losses (mrrl, rdl): 0.0, 0.0008791239

Epoch over!
epoch time: 12.144

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 461
rank avg (pred): 0.279 +- 0.245
mrr vals (pred, true): 0.296, 0.056
batch losses (mrrl, rdl): 0.0, 0.0004762163

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1174
rank avg (pred): 0.286 +- 0.248
mrr vals (pred, true): 0.269, 0.242
batch losses (mrrl, rdl): 0.0, 0.0007561748

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 0
rank avg (pred): 0.246 +- 0.255
mrr vals (pred, true): 0.360, 0.437
batch losses (mrrl, rdl): 0.0, 0.0010047881

Epoch over!
epoch time: 11.867

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1200
rank avg (pred): 0.209 +- 0.248
mrr vals (pred, true): 0.448, 0.053
batch losses (mrrl, rdl): 0.0, 0.0009976721

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 735
rank avg (pred): 0.263 +- 0.254
mrr vals (pred, true): 0.326, 0.021
batch losses (mrrl, rdl): 0.0, 0.0006328648

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 174
rank avg (pred): 0.264 +- 0.268
mrr vals (pred, true): 0.362, 0.055
batch losses (mrrl, rdl): 0.0, 0.000573499

Epoch over!
epoch time: 11.972

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 840
rank avg (pred): 0.471 +- 0.212
mrr vals (pred, true): 0.118, 0.050
batch losses (mrrl, rdl): 0.0, 1.65023e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 801
rank avg (pred): 0.384 +- 0.248
mrr vals (pred, true): 0.217, 0.056
batch losses (mrrl, rdl): 0.0, 4.62053e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1091
rank avg (pred): 0.231 +- 0.267
mrr vals (pred, true): 0.461, 0.223
batch losses (mrrl, rdl): 0.0, 0.0003783396

Epoch over!
epoch time: 11.828

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 911
rank avg (pred): 0.084 +- 0.095
mrr vals (pred, true): 0.486, 0.342
batch losses (mrrl, rdl): 0.0, 9.966e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 784
rank avg (pred): 0.439 +- 0.231
mrr vals (pred, true): 0.170, 0.053
batch losses (mrrl, rdl): 0.0, 1.20973e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 816
rank avg (pred): 0.150 +- 0.170
mrr vals (pred, true): 0.487, 0.021
batch losses (mrrl, rdl): 0.0, 0.0017638158

Epoch over!
epoch time: 11.951

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 569
rank avg (pred): 0.218 +- 0.261
mrr vals (pred, true): 0.485, 0.232
batch losses (mrrl, rdl): 0.643407166, 0.0004333422

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 393
rank avg (pred): 0.458 +- 0.218
mrr vals (pred, true): 0.126, 0.256
batch losses (mrrl, rdl): 0.1690230668, 0.0029056189

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 91
rank avg (pred): 0.441 +- 0.219
mrr vals (pred, true): 0.138, 0.238
batch losses (mrrl, rdl): 0.0997226313, 0.0021983206

Epoch over!
epoch time: 12.371

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 564
rank avg (pred): 0.125 +- 0.110
mrr vals (pred, true): 0.383, 0.355
batch losses (mrrl, rdl): 0.0082546109, 8.36923e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 490
rank avg (pred): 0.119 +- 0.104
mrr vals (pred, true): 0.385, 0.382
batch losses (mrrl, rdl): 9.50704e-05, 8.18238e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1166
rank avg (pred): 0.423 +- 0.211
mrr vals (pred, true): 0.142, 0.237
batch losses (mrrl, rdl): 0.0906888843, 0.0025021033

Epoch over!
epoch time: 12.037

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 543
rank avg (pred): 0.114 +- 0.098
mrr vals (pred, true): 0.374, 0.363
batch losses (mrrl, rdl): 0.0013440903, 5.57652e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 970
rank avg (pred): 0.500 +- 0.139
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 1.7488e-05, 9.09329e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1142
rank avg (pred): 0.067 +- 0.058
mrr vals (pred, true): 0.413, 0.413
batch losses (mrrl, rdl): 8.105e-07, 2.0094e-06

Epoch over!
epoch time: 12.021

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 615
rank avg (pred): 0.397 +- 0.212
mrr vals (pred, true): 0.152, 0.262
batch losses (mrrl, rdl): 0.1197307259, 0.0020927279

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1180
rank avg (pred): 0.424 +- 0.196
mrr vals (pred, true): 0.128, 0.186
batch losses (mrrl, rdl): 0.0336172879, 0.0013865239

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 181
rank avg (pred): 0.382 +- 0.213
mrr vals (pred, true): 0.161, 0.052
batch losses (mrrl, rdl): 0.1237213984, 9.41746e-05

Epoch over!
epoch time: 12.102

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 147
rank avg (pred): 0.405 +- 0.202
mrr vals (pred, true): 0.147, 0.239
batch losses (mrrl, rdl): 0.0833778754, 0.0021016083

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 212
rank avg (pred): 0.400 +- 0.198
mrr vals (pred, true): 0.145, 0.052
batch losses (mrrl, rdl): 0.0899993777, 5.60092e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 306
rank avg (pred): 0.039 +- 0.035
mrr vals (pred, true): 0.474, 0.444
batch losses (mrrl, rdl): 0.0091171628, 7.2687e-06

Epoch over!
epoch time: 11.911

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1168
rank avg (pred): 0.404 +- 0.190
mrr vals (pred, true): 0.132, 0.254
batch losses (mrrl, rdl): 0.1494338065, 0.002098605

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1179
rank avg (pred): 0.392 +- 0.192
mrr vals (pred, true): 0.146, 0.248
batch losses (mrrl, rdl): 0.1027852446, 0.0019794244

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 418
rank avg (pred): 0.391 +- 0.187
mrr vals (pred, true): 0.138, 0.056
batch losses (mrrl, rdl): 0.0776462704, 0.0001061752

Epoch over!
epoch time: 12.051

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 161
rank avg (pred): 0.385 +- 0.189
mrr vals (pred, true): 0.146, 0.226
batch losses (mrrl, rdl): 0.0635545254, 0.0019308616

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 710
rank avg (pred): 0.377 +- 0.187
mrr vals (pred, true): 0.159, 0.049
batch losses (mrrl, rdl): 0.1183295399, 0.00013286

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 756
rank avg (pred): 0.465 +- 0.093
mrr vals (pred, true): 0.039, 0.050
batch losses (mrrl, rdl): 0.0012108771, 6.02499e-05

Epoch over!
epoch time: 11.841

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1062
rank avg (pred): 0.083 +- 0.070
mrr vals (pred, true): 0.411, 0.432
batch losses (mrrl, rdl): 0.0043387297, 1.76864e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 343
rank avg (pred): 0.385 +- 0.173
mrr vals (pred, true): 0.124, 0.242
batch losses (mrrl, rdl): 0.1389191449, 0.0019325017

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 369
rank avg (pred): 0.351 +- 0.182
mrr vals (pred, true): 0.172, 0.259
batch losses (mrrl, rdl): 0.0761105344, 0.0013036625

Epoch over!
epoch time: 11.882

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 598
rank avg (pred): 0.347 +- 0.179
mrr vals (pred, true): 0.172, 0.231
batch losses (mrrl, rdl): 0.035236571, 0.0013753811

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 28
rank avg (pred): 0.088 +- 0.073
mrr vals (pred, true): 0.407, 0.442
batch losses (mrrl, rdl): 0.0123212188, 2.65078e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 207
rank avg (pred): 0.367 +- 0.160
mrr vals (pred, true): 0.141, 0.050
batch losses (mrrl, rdl): 0.0833880529, 0.0002017995

Epoch over!
epoch time: 11.763

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 614
rank avg (pred): 0.371 +- 0.156
mrr vals (pred, true): 0.140, 0.263
batch losses (mrrl, rdl): 0.1506407708, 0.0017721948

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1014
rank avg (pred): 0.342 +- 0.164
mrr vals (pred, true): 0.156, 0.271
batch losses (mrrl, rdl): 0.1314921379, 0.001467483

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 969
rank avg (pred): 0.416 +- 0.070
mrr vals (pred, true): 0.037, 0.054
batch losses (mrrl, rdl): 0.0016034523, 0.0001421484

Epoch over!
epoch time: 11.754

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.330 +- 0.160
mrr vals (pred, true): 0.157, 0.058

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   22 	     0 	 0.14467 	 0.03579 	 MISS
    1 	     1 	 0.04113 	 0.04210 	 ~...
    2 	     2 	 0.04184 	 0.04299 	 ~...
    0 	     3 	 0.02817 	 0.04618 	 ~...
    5 	     4 	 0.04760 	 0.04684 	 ~...
    7 	     5 	 0.05540 	 0.04701 	 ~...
   19 	     6 	 0.08110 	 0.04757 	 m..s
   29 	     7 	 0.15629 	 0.04803 	 MISS
   60 	     8 	 0.16612 	 0.04925 	 MISS
   49 	     9 	 0.16280 	 0.04935 	 MISS
   53 	    10 	 0.16375 	 0.04950 	 MISS
   11 	    11 	 0.06289 	 0.04955 	 ~...
   76 	    12 	 0.17344 	 0.04958 	 MISS
   25 	    13 	 0.15112 	 0.04967 	 MISS
   23 	    14 	 0.14882 	 0.04979 	 m..s
    6 	    15 	 0.05407 	 0.04985 	 ~...
   26 	    16 	 0.15383 	 0.05011 	 MISS
   48 	    17 	 0.16133 	 0.05029 	 MISS
   30 	    18 	 0.15720 	 0.05108 	 MISS
    9 	    19 	 0.05887 	 0.05110 	 ~...
   34 	    20 	 0.15895 	 0.05113 	 MISS
   57 	    21 	 0.16490 	 0.05116 	 MISS
   83 	    22 	 0.17598 	 0.05119 	 MISS
   61 	    23 	 0.16618 	 0.05130 	 MISS
   12 	    24 	 0.06828 	 0.05131 	 ~...
   52 	    25 	 0.16313 	 0.05139 	 MISS
   44 	    26 	 0.16088 	 0.05142 	 MISS
   10 	    27 	 0.06270 	 0.05174 	 ~...
    4 	    28 	 0.04659 	 0.05178 	 ~...
   13 	    29 	 0.07112 	 0.05210 	 ~...
   38 	    30 	 0.15917 	 0.05229 	 MISS
   16 	    31 	 0.07467 	 0.05244 	 ~...
   55 	    32 	 0.16413 	 0.05297 	 MISS
   64 	    33 	 0.16630 	 0.05298 	 MISS
   81 	    34 	 0.17573 	 0.05298 	 MISS
    3 	    35 	 0.04531 	 0.05341 	 ~...
   54 	    36 	 0.16379 	 0.05349 	 MISS
   15 	    37 	 0.07353 	 0.05376 	 ~...
   14 	    38 	 0.07232 	 0.05382 	 ~...
   75 	    39 	 0.17242 	 0.05383 	 MISS
   73 	    40 	 0.17202 	 0.05412 	 MISS
   74 	    41 	 0.17212 	 0.05439 	 MISS
   18 	    42 	 0.07921 	 0.05446 	 ~...
   35 	    43 	 0.15897 	 0.05452 	 MISS
   20 	    44 	 0.08137 	 0.05469 	 ~...
   67 	    45 	 0.16732 	 0.05475 	 MISS
   42 	    46 	 0.16029 	 0.05586 	 MISS
   17 	    47 	 0.07640 	 0.05593 	 ~...
   84 	    48 	 0.17605 	 0.05595 	 MISS
   51 	    49 	 0.16286 	 0.05688 	 MISS
   78 	    50 	 0.17387 	 0.05706 	 MISS
    8 	    51 	 0.05570 	 0.05722 	 ~...
   31 	    52 	 0.15720 	 0.05743 	 m..s
   79 	    53 	 0.17464 	 0.05745 	 MISS
   39 	    54 	 0.15954 	 0.05789 	 MISS
   70 	    55 	 0.17047 	 0.05791 	 MISS
   77 	    56 	 0.17353 	 0.05797 	 MISS
   37 	    57 	 0.15905 	 0.05807 	 MISS
   50 	    58 	 0.16281 	 0.05840 	 MISS
   32 	    59 	 0.15735 	 0.05848 	 m..s
   21 	    60 	 0.14261 	 0.09433 	 m..s
   86 	    61 	 0.22610 	 0.19588 	 m..s
   68 	    62 	 0.16787 	 0.19706 	 ~...
   85 	    63 	 0.21143 	 0.20065 	 ~...
   58 	    64 	 0.16516 	 0.20332 	 m..s
   87 	    65 	 0.30593 	 0.21277 	 m..s
   40 	    66 	 0.15995 	 0.21967 	 m..s
   62 	    67 	 0.16619 	 0.22191 	 m..s
   46 	    68 	 0.16093 	 0.22486 	 m..s
   47 	    69 	 0.16132 	 0.23455 	 m..s
   24 	    70 	 0.15103 	 0.23649 	 m..s
   65 	    71 	 0.16650 	 0.23674 	 m..s
   59 	    72 	 0.16609 	 0.23903 	 m..s
   63 	    73 	 0.16624 	 0.24044 	 m..s
   41 	    74 	 0.16022 	 0.24084 	 m..s
   69 	    75 	 0.16897 	 0.24100 	 m..s
   36 	    76 	 0.15898 	 0.24505 	 m..s
   33 	    77 	 0.15878 	 0.24511 	 m..s
   56 	    78 	 0.16480 	 0.24587 	 m..s
   80 	    79 	 0.17465 	 0.25355 	 m..s
   82 	    80 	 0.17585 	 0.25445 	 m..s
   66 	    81 	 0.16669 	 0.25718 	 m..s
   43 	    82 	 0.16058 	 0.25781 	 m..s
   27 	    83 	 0.15537 	 0.26210 	 MISS
   72 	    84 	 0.17172 	 0.26472 	 m..s
   71 	    85 	 0.17155 	 0.26720 	 m..s
   45 	    86 	 0.16092 	 0.27135 	 MISS
   28 	    87 	 0.15544 	 0.29182 	 MISS
   89 	    88 	 0.33288 	 0.33189 	 ~...
   90 	    89 	 0.37609 	 0.35294 	 ~...
   94 	    90 	 0.39090 	 0.35933 	 m..s
   88 	    91 	 0.31621 	 0.36367 	 m..s
   95 	    92 	 0.39125 	 0.37761 	 ~...
   96 	    93 	 0.39510 	 0.38118 	 ~...
   91 	    94 	 0.37708 	 0.38371 	 ~...
  100 	    95 	 0.39999 	 0.38764 	 ~...
   97 	    96 	 0.39549 	 0.38881 	 ~...
   98 	    97 	 0.39610 	 0.39223 	 ~...
   99 	    98 	 0.39829 	 0.39610 	 ~...
   92 	    99 	 0.38351 	 0.39875 	 ~...
  117 	   100 	 0.44625 	 0.40942 	 m..s
   93 	   101 	 0.38427 	 0.41423 	 ~...
  102 	   102 	 0.42687 	 0.41783 	 ~...
  106 	   103 	 0.43086 	 0.41901 	 ~...
  101 	   104 	 0.40677 	 0.42063 	 ~...
  103 	   105 	 0.42869 	 0.43023 	 ~...
  115 	   106 	 0.44560 	 0.43247 	 ~...
  118 	   107 	 0.45108 	 0.43385 	 ~...
  110 	   108 	 0.44048 	 0.43538 	 ~...
  108 	   109 	 0.43251 	 0.43550 	 ~...
  111 	   110 	 0.44292 	 0.43783 	 ~...
  107 	   111 	 0.43156 	 0.44006 	 ~...
  109 	   112 	 0.43993 	 0.44009 	 ~...
  105 	   113 	 0.43053 	 0.44052 	 ~...
  114 	   114 	 0.44531 	 0.44087 	 ~...
  116 	   115 	 0.44586 	 0.44335 	 ~...
  120 	   116 	 0.45694 	 0.44616 	 ~...
  113 	   117 	 0.44484 	 0.44625 	 ~...
  104 	   118 	 0.42992 	 0.44712 	 ~...
  112 	   119 	 0.44418 	 0.46006 	 ~...
  119 	   120 	 0.45584 	 0.47952 	 ~...
==========================================
r_mrr = 0.892385721206665
r2_mrr = 0.7705968618392944
spearmanr_mrr@5 = 0.850908100605011
spearmanr_mrr@10 = 0.9079487323760986
spearmanr_mrr@50 = 0.9877604246139526
spearmanr_mrr@100 = 0.9048556089401245
spearmanr_mrr@All = 0.9177614450454712
==========================================
test time: 0.414
Done Testing dataset Kinships
total time taken: 187.38457942008972
training time taken: 179.98216319084167
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8924)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7706)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.8509)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9079)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9878)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.9049)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9178)}}, 'test_loss': {'DistMult': {'Kinships': 7.134177284497127}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 6359678256679415
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [156, 391, 36, 1155, 121, 1187, 999, 1034, 1181, 324, 188, 671, 462, 944, 611, 297, 1180, 982, 496, 447, 419, 1031, 838, 595, 120, 954, 701, 325, 186, 1154, 1020, 823, 454, 368, 437, 897, 131, 817, 689, 957, 431, 989, 1096, 376, 597, 607, 445, 500, 942, 15, 974, 0, 1116, 1205, 894, 330, 1092, 1179, 364, 946, 947, 92, 1213, 916, 220, 173, 669, 930, 414, 800, 489, 977, 223, 962, 66, 226, 1055, 876, 920, 562, 711, 196, 369, 1168, 713, 275, 696, 970, 1015, 477, 1138, 641, 510, 211, 442, 1069, 1152, 1114, 745, 1172, 287, 521, 776, 443, 598, 314, 83, 773, 584, 1073, 1178, 896, 106, 923, 51, 43, 491, 268, 108, 482, 812]
valid_ids (0): []
train_ids (1094): [241, 70, 996, 1170, 787, 664, 316, 245, 82, 1087, 453, 1021, 1029, 400, 506, 875, 224, 871, 841, 1214, 222, 501, 702, 603, 933, 55, 700, 516, 546, 918, 1177, 900, 811, 628, 261, 52, 682, 380, 1001, 1129, 310, 663, 1059, 929, 69, 980, 899, 184, 303, 1058, 960, 183, 1210, 1127, 113, 828, 752, 786, 76, 483, 834, 1191, 1119, 1186, 569, 735, 623, 435, 968, 1048, 104, 1078, 657, 71, 22, 1130, 789, 809, 588, 362, 547, 613, 356, 215, 97, 652, 1123, 550, 433, 1, 146, 136, 739, 886, 1153, 851, 227, 1192, 631, 599, 311, 925, 98, 724, 1079, 1085, 344, 940, 1109, 45, 803, 457, 539, 878, 1202, 848, 943, 733, 1124, 499, 480, 290, 1035, 978, 101, 881, 615, 1016, 677, 116, 185, 528, 128, 137, 570, 783, 446, 403, 819, 887, 888, 1063, 1131, 844, 243, 276, 401, 246, 214, 648, 775, 704, 873, 493, 201, 208, 794, 1195, 165, 206, 1097, 1062, 267, 958, 190, 632, 707, 20, 481, 537, 1004, 771, 279, 38, 198, 7, 1141, 1041, 80, 1006, 144, 62, 41, 193, 1198, 505, 110, 509, 1167, 544, 565, 530, 764, 636, 488, 59, 976, 1142, 814, 6, 908, 859, 93, 174, 740, 766, 922, 522, 541, 1188, 710, 622, 604, 4, 822, 567, 804, 605, 111, 549, 606, 469, 553, 868, 421, 114, 430, 529, 601, 757, 465, 1082, 308, 690, 1045, 955, 336, 647, 1189, 885, 312, 813, 1136, 913, 177, 396, 898, 1089, 969, 591, 381, 1158, 142, 837, 640, 1139, 354, 777, 127, 576, 479, 399, 200, 154, 1011, 272, 1060, 265, 755, 239, 164, 406, 869, 65, 721, 972, 355, 644, 466, 238, 596, 586, 1106, 321, 260, 1212, 133, 1022, 744, 797, 1080, 81, 346, 117, 827, 398, 722, 616, 438, 262, 367, 747, 1203, 357, 1057, 358, 750, 1171, 450, 1132, 404, 731, 984, 1113, 649, 788, 459, 1200, 1100, 29, 484, 155, 737, 329, 497, 793, 534, 274, 973, 449, 902, 618, 458, 248, 74, 691, 831, 33, 633, 934, 545, 37, 441, 1128, 542, 444, 273, 552, 202, 75, 759, 416, 327, 1013, 912, 17, 1145, 455, 170, 257, 407, 624, 180, 763, 12, 1046, 536, 351, 298, 1182, 194, 383, 19, 798, 289, 317, 687, 883, 256, 667, 609, 96, 994, 945, 335, 820, 983, 694, 374, 328, 627, 157, 953, 285, 219, 315, 293, 408, 1122, 1149, 385, 697, 1135, 593, 845, 100, 986, 134, 10, 1094, 746, 1039, 1077, 924, 866, 387, 1126, 612, 642, 11, 532, 94, 26, 575, 1017, 77, 284, 1206, 1076, 686, 28, 580, 270, 386, 212, 566, 191, 784, 1027, 1208, 1099, 761, 865, 574, 266, 130, 125, 698, 46, 1143, 1000, 343, 168, 361, 626, 388, 533, 91, 805, 1144, 394, 807, 931, 60, 742, 675, 296, 653, 993, 397, 334, 867, 1061, 205, 513, 429, 830, 1081, 34, 617, 720, 825, 518, 850, 332, 84, 228, 145, 204, 1169, 507, 1010, 579, 1091, 614, 1088, 756, 1014, 451, 171, 1108, 242, 112, 995, 1147, 234, 880, 600, 651, 122, 233, 1201, 309, 393, 427, 439, 941, 1024, 872, 678, 30, 555, 857, 1086, 322, 48, 86, 1042, 231, 709, 568, 487, 411, 105, 808, 864, 765, 326, 835, 950, 304, 910, 192, 281, 554, 475, 412, 723, 662, 585, 410, 935, 1185, 621, 774, 858, 870, 129, 1121, 217, 1117, 187, 163, 79, 514, 464, 1084, 693, 939, 1005, 695, 1023, 502, 107, 685, 743, 178, 492, 1211, 1043, 578, 990, 420, 258, 561, 1162, 89, 147, 1008, 556, 378, 716, 668, 1065, 402, 1018, 754, 1009, 625, 1047, 413, 829, 1196, 305, 405, 861, 213, 207, 688, 816, 959, 1066, 460, 810, 31, 1003, 132, 253, 997, 975, 49, 679, 660, 948, 684, 904, 998, 225, 1054, 821, 307, 1049, 99, 353, 581, 699, 151, 926, 758, 478, 467, 727, 166, 849, 463, 269, 892, 1007, 508, 801, 149, 21, 348, 791, 313, 1174, 473, 152, 118, 160, 889, 650, 1183, 981, 379, 956, 681, 1103, 349, 712, 719, 153, 543, 61, 254, 729, 879, 474, 1125, 1072, 515, 594, 1093, 717, 538, 85, 123, 815, 498, 619, 210, 189, 306, 674, 5, 1075, 854, 961, 434, 571, 655, 425, 703, 490, 862, 1151, 54, 748, 1019, 264, 162, 503, 159, 175, 842, 68, 418, 288, 179, 1071, 906, 963, 852, 964, 782, 736, 301, 58, 1115, 1107, 24, 139, 796, 1052, 422, 135, 181, 115, 47, 1118, 428, 161, 331, 371, 337, 790, 832, 979, 472, 1150, 610, 1012, 860, 339, 456, 384, 526, 250, 563, 903, 531, 1038, 582, 88, 72, 3, 914, 486, 732, 1209, 847, 646, 1051, 148, 1173, 436, 333, 893, 656, 389, 32, 523, 323, 1161, 666, 1184, 319, 665, 1175, 373, 318, 40, 286, 560, 35, 551, 63, 971, 221, 67, 767, 252, 768, 1050, 635, 240, 1159, 991, 350, 18, 1105, 937, 370, 527, 638, 440, 714, 1194, 919, 320, 158, 251, 395, 965, 87, 525, 377, 1193, 1090, 1204, 780, 199, 583, 952, 927, 520, 27, 1033, 1163, 928, 726, 366, 172, 1040, 392, 890, 590, 9, 1104, 209, 360, 363, 230, 683, 938, 1137, 124, 840, 706, 282, 278, 195, 654, 1032, 1056, 375, 915, 461, 751, 417, 1037, 573, 572, 494, 218, 372, 102, 470, 203, 728, 432, 874, 589, 42, 818, 352, 1101, 283, 577, 967, 517, 138, 907, 56, 448, 424, 833, 8, 1074, 291, 1025, 1166, 884, 645, 824, 895, 672, 150, 409, 16, 738, 1026, 1134, 1146, 1111, 216, 909, 390, 64, 1028, 778, 519, 843, 1067, 197, 587, 237, 236, 863, 1070, 345, 634, 753, 1064, 452, 620, 249, 1190, 1165, 13, 271, 504, 1036, 932, 770, 592, 167, 708, 103, 769, 839, 73, 921, 300, 176, 799, 426, 846, 548, 949, 95, 244, 1197, 856, 294, 643, 109, 44, 535, 725, 779, 53, 715, 1140, 608, 1030, 50, 1164, 781, 365, 302, 877, 232, 673, 966, 1083, 119, 1120, 1044, 90, 229, 559, 540, 676, 512, 1157, 141, 826, 718, 182, 806, 1199, 2, 39, 1068, 987, 476, 126, 468, 78, 341, 936, 734, 25, 415, 1160, 340, 485, 760, 658, 295, 901, 670, 1133, 347, 169, 853, 1148, 951, 382, 630, 855, 629, 692, 564, 342, 905, 785, 795, 988, 1053, 637, 1207, 495, 802, 1110, 259, 792, 1095, 1176, 1156, 772, 992, 1112, 680, 235, 639, 1098, 57, 661, 836, 292, 882, 762, 247, 705, 524, 557, 659, 140, 359, 255, 749, 280, 730, 299, 14, 143, 891, 911, 423, 558, 917, 741, 985, 263, 277, 511, 1102, 1002, 23, 471, 338, 602]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5698259655636195
the save name prefix for this run is:  chkpt-ID_5698259655636195_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 520
rank avg (pred): 0.497 +- 0.013
mrr vals (pred, true): 0.019, 0.367
batch losses (mrrl, rdl): 0.0, 0.0039795907

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 950
rank avg (pred): 0.489 +- 0.224
mrr vals (pred, true): 0.042, 0.054
batch losses (mrrl, rdl): 0.0, 2.3219e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 345
rank avg (pred): 0.294 +- 0.224
mrr vals (pred, true): 0.226, 0.238
batch losses (mrrl, rdl): 0.0, 0.0008489301

Epoch over!
epoch time: 11.896

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 272
rank avg (pred): 0.068 +- 0.053
mrr vals (pred, true): 0.352, 0.434
batch losses (mrrl, rdl): 0.0, 4.6401e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 703
rank avg (pred): 0.257 +- 0.215
mrr vals (pred, true): 0.324, 0.053
batch losses (mrrl, rdl): 0.0, 0.0007469858

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 201
rank avg (pred): 0.262 +- 0.222
mrr vals (pred, true): 0.325, 0.049
batch losses (mrrl, rdl): 0.0, 0.0007162519

Epoch over!
epoch time: 11.775

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1010
rank avg (pred): 0.259 +- 0.218
mrr vals (pred, true): 0.321, 0.247
batch losses (mrrl, rdl): 0.0, 0.0006767249

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 259
rank avg (pred): 0.023 +- 0.021
mrr vals (pred, true): 0.538, 0.449
batch losses (mrrl, rdl): 0.0, 2.00109e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 935
rank avg (pred): 0.449 +- 0.312
mrr vals (pred, true): 0.200, 0.042
batch losses (mrrl, rdl): 0.0, 5.02403e-05

Epoch over!
epoch time: 11.838

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 159
rank avg (pred): 0.334 +- 0.213
mrr vals (pred, true): 0.128, 0.252
batch losses (mrrl, rdl): 0.0, 0.0013959638

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 565
rank avg (pred): 0.092 +- 0.080
mrr vals (pred, true): 0.398, 0.370
batch losses (mrrl, rdl): 0.0, 2.0018e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 79
rank avg (pred): 0.036 +- 0.034
mrr vals (pred, true): 0.511, 0.444
batch losses (mrrl, rdl): 0.0, 9.1214e-06

Epoch over!
epoch time: 11.914

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 924
rank avg (pred): 0.409 +- 0.324
mrr vals (pred, true): 0.295, 0.046
batch losses (mrrl, rdl): 0.0, 7.1915e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1027
rank avg (pred): 0.227 +- 0.209
mrr vals (pred, true): 0.372, 0.056
batch losses (mrrl, rdl): 0.0, 0.0008782112

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 841
rank avg (pred): 0.446 +- 0.299
mrr vals (pred, true): 0.185, 0.051
batch losses (mrrl, rdl): 0.0, 1.07475e-05

Epoch over!
epoch time: 11.859

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 289
rank avg (pred): 0.062 +- 0.055
mrr vals (pred, true): 0.436, 0.451
batch losses (mrrl, rdl): 0.0019778477, 1.4895e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1174
rank avg (pred): 0.349 +- 0.222
mrr vals (pred, true): 0.156, 0.242
batch losses (mrrl, rdl): 0.0745200366, 0.0012704105

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 244
rank avg (pred): 0.029 +- 0.021
mrr vals (pred, true): 0.426, 0.434
batch losses (mrrl, rdl): 0.000671514, 1.57258e-05

Epoch over!
epoch time: 12.588

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 4
rank avg (pred): 0.024 +- 0.018
mrr vals (pred, true): 0.456, 0.436
batch losses (mrrl, rdl): 0.0039606434, 2.28889e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 84
rank avg (pred): 0.350 +- 0.211
mrr vals (pred, true): 0.126, 0.219
batch losses (mrrl, rdl): 0.0872837901, 0.0011509921

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 887
rank avg (pred): 0.531 +- 0.196
mrr vals (pred, true): 0.047, 0.057
batch losses (mrrl, rdl): 0.0001090468, 0.0001926228

Epoch over!
epoch time: 12.49

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 808
rank avg (pred): 0.544 +- 0.197
mrr vals (pred, true): 0.053, 0.060
batch losses (mrrl, rdl): 8.12573e-05, 0.0002289713

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 204
rank avg (pred): 0.350 +- 0.206
mrr vals (pred, true): 0.117, 0.048
batch losses (mrrl, rdl): 0.0451403856, 0.0002010904

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1113
rank avg (pred): 0.305 +- 0.208
mrr vals (pred, true): 0.173, 0.055
batch losses (mrrl, rdl): 0.1502892673, 0.0004062263

Epoch over!
epoch time: 11.896

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 174
rank avg (pred): 0.327 +- 0.217
mrr vals (pred, true): 0.166, 0.055
batch losses (mrrl, rdl): 0.1348708868, 0.0003008187

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 847
rank avg (pred): 0.496 +- 0.192
mrr vals (pred, true): 0.056, 0.059
batch losses (mrrl, rdl): 0.0003305693, 0.0001053194

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1140
rank avg (pred): 0.031 +- 0.022
mrr vals (pred, true): 0.410, 0.409
batch losses (mrrl, rdl): 8.0518e-06, 1.92824e-05

Epoch over!
epoch time: 12.082

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 301
rank avg (pred): 0.027 +- 0.019
mrr vals (pred, true): 0.430, 0.444
batch losses (mrrl, rdl): 0.001886665, 1.61467e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 553
rank avg (pred): 0.036 +- 0.027
mrr vals (pred, true): 0.397, 0.378
batch losses (mrrl, rdl): 0.0036294118, 1.76582e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1197
rank avg (pred): 0.341 +- 0.201
mrr vals (pred, true): 0.123, 0.056
batch losses (mrrl, rdl): 0.0528728925, 0.000230598

Epoch over!
epoch time: 12.243

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 835
rank avg (pred): 0.118 +- 0.084
mrr vals (pred, true): 0.261, 0.411
batch losses (mrrl, rdl): 0.2235205323, 6.4218e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 647
rank avg (pred): 0.323 +- 0.209
mrr vals (pred, true): 0.148, 0.222
batch losses (mrrl, rdl): 0.0555150583, 0.0012050391

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1066
rank avg (pred): 0.031 +- 0.021
mrr vals (pred, true): 0.387, 0.453
batch losses (mrrl, rdl): 0.0439394414, 1.01664e-05

Epoch over!
epoch time: 12.209

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 405
rank avg (pred): 0.334 +- 0.203
mrr vals (pred, true): 0.135, 0.051
batch losses (mrrl, rdl): 0.0721579865, 0.0002948893

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 249
rank avg (pred): 0.024 +- 0.016
mrr vals (pred, true): 0.431, 0.445
batch losses (mrrl, rdl): 0.0017548257, 2.21287e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1021
rank avg (pred): 0.329 +- 0.202
mrr vals (pred, true): 0.137, 0.229
batch losses (mrrl, rdl): 0.0845790729, 0.0011612753

Epoch over!
epoch time: 12.185

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 933
rank avg (pred): 0.559 +- 0.159
mrr vals (pred, true): 0.037, 0.046
batch losses (mrrl, rdl): 0.0016790187, 8.63269e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 792
rank avg (pred): 0.506 +- 0.198
mrr vals (pred, true): 0.065, 0.054
batch losses (mrrl, rdl): 0.0021048677, 7.47256e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 547
rank avg (pred): 0.050 +- 0.036
mrr vals (pred, true): 0.358, 0.380
batch losses (mrrl, rdl): 0.0049683708, 4.8343e-06

Epoch over!
epoch time: 12.297

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 560
rank avg (pred): 0.044 +- 0.031
mrr vals (pred, true): 0.358, 0.383
batch losses (mrrl, rdl): 0.0063673444, 7.9616e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 782
rank avg (pred): 0.483 +- 0.182
mrr vals (pred, true): 0.074, 0.053
batch losses (mrrl, rdl): 0.0057136947, 3.80911e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 343
rank avg (pred): 0.321 +- 0.201
mrr vals (pred, true): 0.150, 0.242
batch losses (mrrl, rdl): 0.0845645741, 0.0012587289

Epoch over!
epoch time: 12.104

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 609
rank avg (pred): 0.324 +- 0.199
mrr vals (pred, true): 0.139, 0.257
batch losses (mrrl, rdl): 0.1411010176, 0.0012113084

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 747
rank avg (pred): 0.030 +- 0.021
mrr vals (pred, true): 0.413, 0.413
batch losses (mrrl, rdl): 7.626e-07, 1.73833e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 905
rank avg (pred): 0.068 +- 0.046
mrr vals (pred, true): 0.296, 0.309
batch losses (mrrl, rdl): 0.0016597572, 2.112e-06

Epoch over!
epoch time: 11.901

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.326 +- 0.195
mrr vals (pred, true): 0.128, 0.248

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   16 	     0 	 0.10481 	 0.02078 	 m..s
   82 	     1 	 0.13820 	 0.02078 	 MISS
   75 	     2 	 0.13566 	 0.02078 	 MISS
    7 	     3 	 0.03436 	 0.04299 	 ~...
    5 	     4 	 0.03224 	 0.04321 	 ~...
   12 	     5 	 0.04965 	 0.04509 	 ~...
    3 	     6 	 0.03194 	 0.04713 	 ~...
   69 	     7 	 0.13289 	 0.04904 	 m..s
   47 	     8 	 0.12752 	 0.04935 	 m..s
   39 	     9 	 0.12498 	 0.04952 	 m..s
   78 	    10 	 0.13693 	 0.04958 	 m..s
   66 	    11 	 0.13223 	 0.04963 	 m..s
    8 	    12 	 0.04024 	 0.04997 	 ~...
   18 	    13 	 0.10914 	 0.04999 	 m..s
   32 	    14 	 0.12221 	 0.05113 	 m..s
   13 	    15 	 0.05000 	 0.05174 	 ~...
    0 	    16 	 0.03067 	 0.05178 	 ~...
   50 	    17 	 0.12770 	 0.05182 	 m..s
   34 	    18 	 0.12264 	 0.05240 	 m..s
   11 	    19 	 0.04937 	 0.05249 	 ~...
   10 	    20 	 0.04373 	 0.05254 	 ~...
   73 	    21 	 0.13497 	 0.05262 	 m..s
   86 	    22 	 0.14337 	 0.05298 	 m..s
    2 	    23 	 0.03178 	 0.05325 	 ~...
    6 	    24 	 0.03350 	 0.05340 	 ~...
   33 	    25 	 0.12251 	 0.05361 	 m..s
    9 	    26 	 0.04106 	 0.05375 	 ~...
   60 	    27 	 0.13094 	 0.05418 	 m..s
   85 	    28 	 0.14314 	 0.05418 	 m..s
   15 	    29 	 0.06172 	 0.05446 	 ~...
   83 	    30 	 0.13842 	 0.05451 	 m..s
   25 	    31 	 0.12022 	 0.05486 	 m..s
   61 	    32 	 0.13113 	 0.05488 	 m..s
   76 	    33 	 0.13577 	 0.05501 	 m..s
   59 	    34 	 0.13060 	 0.05507 	 m..s
   53 	    35 	 0.12852 	 0.05513 	 m..s
   77 	    36 	 0.13622 	 0.05536 	 m..s
   29 	    37 	 0.12164 	 0.05567 	 m..s
   63 	    38 	 0.13152 	 0.05628 	 m..s
   42 	    39 	 0.12614 	 0.05707 	 m..s
   35 	    40 	 0.12395 	 0.05707 	 m..s
   20 	    41 	 0.11296 	 0.05728 	 m..s
    4 	    42 	 0.03207 	 0.05736 	 ~...
   58 	    43 	 0.13001 	 0.05746 	 m..s
   71 	    44 	 0.13458 	 0.05813 	 m..s
   17 	    45 	 0.10615 	 0.05848 	 m..s
    1 	    46 	 0.03157 	 0.05938 	 ~...
   14 	    47 	 0.05372 	 0.06022 	 ~...
   37 	    48 	 0.12445 	 0.06090 	 m..s
   22 	    49 	 0.11979 	 0.06102 	 m..s
   62 	    50 	 0.13128 	 0.06144 	 m..s
   44 	    51 	 0.12677 	 0.06164 	 m..s
   88 	    52 	 0.16648 	 0.08458 	 m..s
   23 	    53 	 0.12002 	 0.18635 	 m..s
   41 	    54 	 0.12594 	 0.18802 	 m..s
   89 	    55 	 0.21903 	 0.19588 	 ~...
   55 	    56 	 0.12927 	 0.20332 	 m..s
   54 	    57 	 0.12899 	 0.20396 	 m..s
   87 	    58 	 0.15180 	 0.20925 	 m..s
   26 	    59 	 0.12115 	 0.21450 	 m..s
   46 	    60 	 0.12743 	 0.21829 	 m..s
   43 	    61 	 0.12649 	 0.22808 	 MISS
   80 	    62 	 0.13709 	 0.22898 	 m..s
   38 	    63 	 0.12468 	 0.23019 	 MISS
   40 	    64 	 0.12567 	 0.23134 	 MISS
   65 	    65 	 0.13211 	 0.23171 	 m..s
   49 	    66 	 0.12769 	 0.23337 	 MISS
   31 	    67 	 0.12182 	 0.23373 	 MISS
   72 	    68 	 0.13470 	 0.23467 	 m..s
   28 	    69 	 0.12157 	 0.23749 	 MISS
   74 	    70 	 0.13512 	 0.23903 	 MISS
   84 	    71 	 0.13981 	 0.24098 	 MISS
   79 	    72 	 0.13702 	 0.24588 	 MISS
   64 	    73 	 0.13178 	 0.24601 	 MISS
   36 	    74 	 0.12406 	 0.24669 	 MISS
   56 	    75 	 0.12967 	 0.24764 	 MISS
   23 	    76 	 0.12002 	 0.24784 	 MISS
   57 	    77 	 0.12967 	 0.24799 	 MISS
   52 	    78 	 0.12820 	 0.24830 	 MISS
   27 	    79 	 0.12152 	 0.24907 	 MISS
   68 	    80 	 0.13257 	 0.24987 	 MISS
   67 	    81 	 0.13232 	 0.24997 	 MISS
   45 	    82 	 0.12732 	 0.25030 	 MISS
   30 	    83 	 0.12167 	 0.25074 	 MISS
   51 	    84 	 0.12796 	 0.25228 	 MISS
   81 	    85 	 0.13745 	 0.25355 	 MISS
   19 	    86 	 0.11103 	 0.25397 	 MISS
   70 	    87 	 0.13318 	 0.25896 	 MISS
   48 	    88 	 0.12765 	 0.26715 	 MISS
   21 	    89 	 0.11575 	 0.27363 	 MISS
   90 	    90 	 0.23552 	 0.31072 	 m..s
   91 	    91 	 0.34602 	 0.36106 	 ~...
   96 	    92 	 0.36255 	 0.37175 	 ~...
  101 	    93 	 0.37862 	 0.37192 	 ~...
   94 	    94 	 0.36018 	 0.37320 	 ~...
   97 	    95 	 0.36396 	 0.38105 	 ~...
   92 	    96 	 0.35148 	 0.38655 	 m..s
   95 	    97 	 0.36196 	 0.39224 	 m..s
  100 	    98 	 0.37470 	 0.39451 	 ~...
   93 	    99 	 0.35242 	 0.39489 	 m..s
   98 	   100 	 0.36560 	 0.40130 	 m..s
   99 	   101 	 0.36582 	 0.41782 	 m..s
  106 	   102 	 0.39097 	 0.43014 	 m..s
  117 	   103 	 0.41856 	 0.43065 	 ~...
  113 	   104 	 0.41328 	 0.43100 	 ~...
  105 	   105 	 0.39045 	 0.43229 	 m..s
  118 	   106 	 0.43349 	 0.43581 	 ~...
  115 	   107 	 0.41535 	 0.43650 	 ~...
  107 	   108 	 0.39398 	 0.43885 	 m..s
  116 	   109 	 0.41669 	 0.43931 	 ~...
  110 	   110 	 0.40613 	 0.43946 	 m..s
  108 	   111 	 0.40577 	 0.44003 	 m..s
  114 	   112 	 0.41447 	 0.44110 	 ~...
  102 	   113 	 0.38184 	 0.44226 	 m..s
  112 	   114 	 0.41233 	 0.44401 	 m..s
  109 	   115 	 0.40611 	 0.44580 	 m..s
  119 	   116 	 0.43911 	 0.44625 	 ~...
  103 	   117 	 0.38459 	 0.45134 	 m..s
  120 	   118 	 0.44063 	 0.45790 	 ~...
  111 	   119 	 0.40795 	 0.46786 	 m..s
  104 	   120 	 0.38992 	 0.47042 	 m..s
==========================================
r_mrr = 0.8702186346054077
r2_mrr = 0.7401238679885864
spearmanr_mrr@5 = 0.9654807448387146
spearmanr_mrr@10 = 0.9833399057388306
spearmanr_mrr@50 = 0.9910420179367065
spearmanr_mrr@100 = 0.879253089427948
spearmanr_mrr@All = 0.9027836322784424
==========================================
test time: 0.396
Done Testing dataset Kinships
total time taken: 188.55494666099548
training time taken: 181.74525785446167
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8702)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7401)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9655)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9833)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9910)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8793)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9028)}}, 'test_loss': {'DistMult': {'Kinships': 7.314314395382098}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7267501249025243
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [409, 989, 542, 1089, 29, 837, 1000, 475, 138, 510, 477, 1202, 1137, 1169, 1177, 842, 887, 1032, 61, 483, 909, 299, 1064, 32, 63, 437, 321, 208, 1161, 880, 1027, 1149, 69, 953, 1024, 531, 224, 638, 733, 1141, 1057, 450, 937, 121, 428, 978, 524, 71, 706, 936, 374, 160, 799, 869, 849, 882, 690, 166, 715, 794, 742, 554, 752, 237, 170, 357, 1185, 573, 1113, 425, 1056, 1038, 591, 585, 895, 784, 349, 761, 866, 952, 1168, 1028, 352, 637, 495, 80, 416, 21, 1104, 611, 502, 984, 558, 151, 547, 1069, 712, 839, 165, 728, 1191, 363, 302, 228, 371, 1001, 342, 249, 1016, 649, 360, 447, 1020, 835, 135, 39, 873, 243, 227, 615, 244]
valid_ids (0): []
train_ids (1094): [236, 1010, 1187, 605, 949, 461, 106, 161, 64, 904, 432, 1171, 197, 152, 626, 184, 944, 211, 529, 993, 894, 58, 812, 72, 256, 990, 1115, 140, 355, 384, 1204, 376, 169, 484, 382, 639, 49, 998, 478, 127, 261, 408, 481, 377, 291, 193, 846, 499, 657, 130, 44, 684, 254, 1125, 137, 651, 847, 277, 636, 857, 129, 1054, 1061, 164, 1142, 65, 233, 42, 830, 927, 640, 746, 633, 513, 517, 415, 1033, 298, 923, 1071, 1152, 192, 710, 441, 762, 36, 235, 87, 340, 431, 1110, 672, 661, 570, 27, 407, 465, 1196, 1040, 1014, 919, 167, 683, 146, 482, 420, 940, 454, 1074, 536, 621, 1037, 362, 430, 831, 442, 668, 346, 419, 618, 101, 1025, 539, 1164, 183, 94, 538, 1183, 186, 608, 195, 889, 1063, 412, 1085, 1138, 457, 860, 188, 963, 1013, 304, 527, 1068, 366, 319, 89, 1005, 1214, 563, 522, 17, 823, 229, 568, 663, 738, 653, 694, 231, 888, 134, 496, 589, 95, 815, 1081, 601, 399, 720, 1167, 398, 722, 1199, 541, 18, 604, 204, 607, 404, 345, 486, 520, 1151, 859, 213, 747, 516, 187, 631, 296, 60, 190, 281, 814, 1075, 418, 403, 181, 1174, 734, 1083, 1198, 617, 6, 834, 767, 995, 353, 246, 702, 93, 773, 323, 704, 198, 1072, 11, 22, 896, 1051, 74, 309, 964, 86, 874, 813, 981, 596, 361, 796, 406, 1150, 1143, 700, 469, 545, 932, 588, 1203, 664, 459, 307, 372, 1213, 1206, 1176, 400, 848, 79, 806, 120, 779, 907, 708, 965, 717, 276, 479, 263, 180, 1043, 753, 1107, 119, 928, 508, 88, 555, 1178, 199, 31, 878, 898, 1049, 348, 492, 646, 606, 776, 7, 111, 456, 1192, 367, 645, 840, 178, 232, 511, 280, 997, 850, 23, 444, 1184, 535, 1207, 600, 385, 1039, 1165, 438, 763, 1166, 157, 335, 1046, 343, 929, 396, 43, 284, 532, 206, 553, 1131, 45, 1078, 820, 1162, 811, 1026, 123, 504, 1084, 914, 743, 82, 916, 1029, 662, 863, 117, 819, 556, 359, 821, 1126, 689, 901, 967, 1134, 526, 133, 1004, 1153, 518, 401, 951, 537, 364, 507, 854, 283, 54, 1058, 194, 270, 789, 561, 546, 77, 1117, 226, 971, 1021, 463, 678, 8, 620, 792, 336, 1136, 287, 603, 326, 827, 458, 665, 316, 320, 402, 1062, 150, 567, 790, 1070, 3, 711, 464, 103, 1127, 525, 391, 991, 676, 448, 393, 841, 185, 241, 1065, 931, 55, 1017, 758, 1140, 451, 769, 800, 209, 1159, 817, 669, 629, 303, 200, 10, 1175, 798, 274, 992, 581, 337, 973, 565, 1066, 305, 153, 756, 1189, 487, 1181, 1077, 118, 1172, 159, 1109, 324, 1112, 78, 380, 574, 132, 619, 498, 1156, 551, 724, 1211, 476, 216, 282, 311, 667, 172, 333, 977, 713, 577, 644, 658, 675, 1086, 269, 81, 330, 144, 390, 543, 632, 1145, 899, 785, 612, 462, 957, 107, 552, 698, 754, 300, 9, 616, 13, 1098, 810, 48, 5, 341, 803, 654, 257, 740, 294, 687, 918, 92, 723, 915, 489, 755, 959, 347, 614, 271, 449, 350, 35, 397, 331, 179, 258, 125, 30, 622, 911, 84, 795, 259, 41, 719, 297, 46, 83, 783, 368, 576, 1108, 770, 736, 729, 884, 845, 480, 760, 435, 314, 627, 51, 423, 1023, 485, 774, 292, 560, 578, 1048, 956, 434, 413, 650, 703, 491, 688, 679, 515, 503, 745, 727, 968, 505, 905, 175, 136, 239, 50, 1095, 386, 1148, 217, 1092, 872, 85, 116, 59, 1007, 369, 858, 994, 1179, 922, 804, 540, 950, 925, 351, 987, 697, 171, 778, 643, 630, 176, 155, 920, 506, 960, 955, 721, 912, 334, 308, 590, 1186, 1106, 521, 264, 443, 338, 634, 1011, 856, 780, 674, 113, 749, 958, 1031, 290, 1130, 864, 56, 1052, 218, 935, 440, 822, 583, 234, 312, 381, 1067, 455, 983, 1123, 329, 327, 62, 908, 288, 1055, 66, 852, 879, 225, 921, 34, 429, 471, 202, 844, 1097, 876, 1111, 344, 699, 128, 726, 253, 223, 201, 248, 387, 1135, 500, 757, 979, 76, 544, 1041, 818, 751, 610, 1096, 933, 750, 974, 1154, 154, 735, 16, 718, 189, 392, 1205, 210, 286, 808, 569, 945, 976, 917, 897, 370, 777, 356, 575, 40, 594, 247, 737, 145, 468, 519, 53, 673, 881, 240, 938, 1047, 1212, 272, 528, 0, 628, 969, 114, 1120, 828, 1059, 293, 122, 685, 268, 37, 549, 802, 1146, 405, 453, 439, 705, 205, 262, 943, 289, 493, 177, 1100, 214, 473, 1139, 771, 962, 26, 75, 1195, 1105, 851, 252, 255, 70, 732, 1163, 625, 124, 947, 642, 358, 833, 1157, 867, 670, 548, 829, 445, 1088, 207, 982, 1030, 865, 427, 394, 238, 365, 975, 692, 861, 709, 701, 325, 14, 656, 267, 514, 1093, 474, 824, 212, 666, 395, 587, 1003, 1, 714, 1018, 433, 954, 1147, 1182, 488, 765, 660, 417, 768, 1133, 104, 424, 1036, 686, 1045, 1132, 422, 942, 215, 946, 446, 597, 1173, 173, 131, 913, 1160, 12, 677, 379, 1194, 807, 843, 295, 671, 375, 1079, 986, 571, 20, 652, 1129, 1015, 306, 572, 100, 797, 1035, 885, 102, 891, 550, 680, 793, 182, 265, 67, 655, 109, 490, 47, 787, 96, 731, 832, 1193, 1118, 91, 996, 436, 388, 584, 332, 278, 279, 222, 825, 245, 494, 1002, 641, 472, 1019, 602, 383, 57, 1201, 805, 648, 322, 251, 466, 1006, 149, 15, 707, 759, 559, 930, 497, 28, 999, 313, 826, 924, 781, 242, 966, 970, 624, 1121, 509, 838, 961, 739, 168, 1180, 1094, 853, 691, 1087, 1012, 647, 191, 142, 613, 196, 373, 534, 317, 452, 906, 1114, 1188, 741, 599, 250, 90, 174, 52, 426, 862, 609, 421, 275, 1076, 143, 870, 318, 467, 566, 115, 1200, 1034, 163, 695, 220, 635, 112, 883, 460, 301, 315, 1155, 203, 411, 110, 900, 744, 378, 99, 1053, 105, 716, 221, 1102, 557, 38, 926, 2, 1060, 592, 1101, 1082, 598, 579, 786, 24, 156, 748, 470, 108, 1158, 816, 875, 354, 523, 1122, 148, 725, 772, 266, 681, 230, 530, 730, 941, 682, 1128, 219, 988, 980, 310, 939, 595, 1042, 562, 1124, 273, 1044, 389, 1080, 791, 902, 141, 855, 533, 775, 414, 1022, 19, 25, 1073, 782, 285, 972, 801, 696, 1009, 659, 580, 33, 985, 98, 1144, 68, 73, 582, 147, 903, 836, 1090, 871, 877, 501, 328, 158, 788, 890, 162, 139, 126, 766, 4, 1091, 1099, 512, 1209, 1208, 764, 623, 910, 97, 593, 934, 893, 886, 1116, 1210, 868, 339, 586, 1119, 410, 260, 1103, 1190, 1170, 809, 564, 1050, 693, 948, 1008, 1197, 892]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2511527694940704
the save name prefix for this run is:  chkpt-ID_2511527694940704_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 78
rank avg (pred): 0.491 +- 0.008
mrr vals (pred, true): 0.019, 0.444
batch losses (mrrl, rdl): 0.0, 0.0040329644

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 144
rank avg (pred): 0.254 +- 0.046
mrr vals (pred, true): 0.038, 0.246
batch losses (mrrl, rdl): 0.0, 0.0005193111

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 619
rank avg (pred): 0.268 +- 0.116
mrr vals (pred, true): 0.062, 0.227
batch losses (mrrl, rdl): 0.0, 0.0005543705

Epoch over!
epoch time: 11.949

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 310
rank avg (pred): 0.056 +- 0.032
mrr vals (pred, true): 0.253, 0.435
batch losses (mrrl, rdl): 0.0, 4.278e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 175
rank avg (pred): 0.289 +- 0.202
mrr vals (pred, true): 0.198, 0.052
batch losses (mrrl, rdl): 0.0, 0.0004772264

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 334
rank avg (pred): 0.251 +- 0.201
mrr vals (pred, true): 0.298, 0.233
batch losses (mrrl, rdl): 0.0, 0.0006308301

Epoch over!
epoch time: 11.868

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 92
rank avg (pred): 0.245 +- 0.197
mrr vals (pred, true): 0.303, 0.239
batch losses (mrrl, rdl): 0.0, 0.0005695861

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 403
rank avg (pred): 0.255 +- 0.210
mrr vals (pred, true): 0.311, 0.228
batch losses (mrrl, rdl): 0.0, 0.0006465321

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 805
rank avg (pred): 0.398 +- 0.303
mrr vals (pred, true): 0.269, 0.054
batch losses (mrrl, rdl): 0.0, 3.75206e-05

Epoch over!
epoch time: 11.992

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1063
rank avg (pred): 0.029 +- 0.026
mrr vals (pred, true): 0.506, 0.443
batch losses (mrrl, rdl): 0.0, 1.34433e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 147
rank avg (pred): 0.233 +- 0.211
mrr vals (pred, true): 0.374, 0.239
batch losses (mrrl, rdl): 0.0, 0.0005021494

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 879
rank avg (pred): 0.453 +- 0.314
mrr vals (pred, true): 0.219, 0.054
batch losses (mrrl, rdl): 0.0, 2.35386e-05

Epoch over!
epoch time: 11.912

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1153
rank avg (pred): 0.016 +- 0.015
mrr vals (pred, true): 0.579, 0.373
batch losses (mrrl, rdl): 0.0, 5.26729e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 26
rank avg (pred): 0.093 +- 0.078
mrr vals (pred, true): 0.367, 0.424
batch losses (mrrl, rdl): 0.0, 3.35197e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 253
rank avg (pred): 0.053 +- 0.050
mrr vals (pred, true): 0.477, 0.447
batch losses (mrrl, rdl): 0.0, 3.53e-07

Epoch over!
epoch time: 11.839

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1176
rank avg (pred): 0.241 +- 0.221
mrr vals (pred, true): 0.377, 0.248
batch losses (mrrl, rdl): 0.1657467782, 0.0005832132

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 67
rank avg (pred): 0.058 +- 0.049
mrr vals (pred, true): 0.415, 0.434
batch losses (mrrl, rdl): 0.0037578784, 4.787e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 678
rank avg (pred): 0.349 +- 0.236
mrr vals (pred, true): 0.179, 0.048
batch losses (mrrl, rdl): 0.1652725041, 0.0001870965

Epoch over!
epoch time: 12.305

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 748
rank avg (pred): 0.062 +- 0.048
mrr vals (pred, true): 0.358, 0.421
batch losses (mrrl, rdl): 0.0396257304, 1.2054e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 183
rank avg (pred): 0.423 +- 0.241
mrr vals (pred, true): 0.117, 0.060
batch losses (mrrl, rdl): 0.0449793823, 1.20699e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 28
rank avg (pred): 0.102 +- 0.088
mrr vals (pred, true): 0.419, 0.442
batch losses (mrrl, rdl): 0.0056687491, 5.52694e-05

Epoch over!
epoch time: 11.77

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 325
rank avg (pred): 0.354 +- 0.230
mrr vals (pred, true): 0.148, 0.204
batch losses (mrrl, rdl): 0.0316586047, 0.001064396

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 686
rank avg (pred): 0.363 +- 0.211
mrr vals (pred, true): 0.120, 0.052
batch losses (mrrl, rdl): 0.0489446186, 0.0001358432

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 658
rank avg (pred): 0.347 +- 0.225
mrr vals (pred, true): 0.151, 0.052
batch losses (mrrl, rdl): 0.1017070189, 0.0001853573

Epoch over!
epoch time: 12.214

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 418
rank avg (pred): 0.330 +- 0.224
mrr vals (pred, true): 0.171, 0.056
batch losses (mrrl, rdl): 0.1471589208, 0.0002779782

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 421
rank avg (pred): 0.349 +- 0.216
mrr vals (pred, true): 0.133, 0.058
batch losses (mrrl, rdl): 0.0682375431, 0.000162518

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 457
rank avg (pred): 0.328 +- 0.223
mrr vals (pred, true): 0.176, 0.053
batch losses (mrrl, rdl): 0.1595856249, 0.0002517072

Epoch over!
epoch time: 11.808

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 44
rank avg (pred): 0.108 +- 0.094
mrr vals (pred, true): 0.427, 0.431
batch losses (mrrl, rdl): 0.0001582957, 6.82479e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 548
rank avg (pred): 0.231 +- 0.197
mrr vals (pred, true): 0.385, 0.375
batch losses (mrrl, rdl): 0.0010089707, 0.0007299838

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 284
rank avg (pred): 0.098 +- 0.085
mrr vals (pred, true): 0.427, 0.448
batch losses (mrrl, rdl): 0.0043019471, 4.99532e-05

Epoch over!
epoch time: 12.026

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 907
rank avg (pred): 0.390 +- 0.217
mrr vals (pred, true): 0.112, 0.094
batch losses (mrrl, rdl): 0.0381061062, 0.0005775936

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 529
rank avg (pred): 0.212 +- 0.183
mrr vals (pred, true): 0.406, 0.372
batch losses (mrrl, rdl): 0.0113724452, 0.0005682501

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 124
rank avg (pred): 0.312 +- 0.194
mrr vals (pred, true): 0.156, 0.258
batch losses (mrrl, rdl): 0.1048147678, 0.0010996456

Epoch over!
epoch time: 12.206

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 901
rank avg (pred): 0.067 +- 0.052
mrr vals (pred, true): 0.343, 0.332
batch losses (mrrl, rdl): 0.0013213968, 2.49e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 134
rank avg (pred): 0.338 +- 0.201
mrr vals (pred, true): 0.134, 0.241
batch losses (mrrl, rdl): 0.114462629, 0.0010489449

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 807
rank avg (pred): 0.524 +- 0.181
mrr vals (pred, true): 0.056, 0.046
batch losses (mrrl, rdl): 0.0003983031, 0.0001011334

Epoch over!
epoch time: 12.223

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 549
rank avg (pred): 0.301 +- 0.259
mrr vals (pred, true): 0.402, 0.356
batch losses (mrrl, rdl): 0.020965701, 0.00139908

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 108
rank avg (pred): 0.301 +- 0.172
mrr vals (pred, true): 0.137, 0.232
batch losses (mrrl, rdl): 0.0900436342, 0.000884239

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 25
rank avg (pred): 0.034 +- 0.028
mrr vals (pred, true): 0.471, 0.451
batch losses (mrrl, rdl): 0.0040521002, 7.6607e-06

Epoch over!
epoch time: 12.029

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 990
rank avg (pred): 0.077 +- 0.066
mrr vals (pred, true): 0.445, 0.425
batch losses (mrrl, rdl): 0.0041009355, 1.14008e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 13
rank avg (pred): 0.080 +- 0.068
mrr vals (pred, true): 0.438, 0.435
batch losses (mrrl, rdl): 6.86196e-05, 1.62199e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 217
rank avg (pred): 0.305 +- 0.164
mrr vals (pred, true): 0.123, 0.056
batch losses (mrrl, rdl): 0.0525643229, 0.0004787351

Epoch over!
epoch time: 12.257

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 292
rank avg (pred): 0.056 +- 0.047
mrr vals (pred, true): 0.451, 0.457
batch losses (mrrl, rdl): 0.0003568825, 4.254e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 720
rank avg (pred): 0.321 +- 0.195
mrr vals (pred, true): 0.142, 0.051
batch losses (mrrl, rdl): 0.08514487, 0.0004169144

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1051
rank avg (pred): 0.338 +- 0.194
mrr vals (pred, true): 0.125, 0.050
batch losses (mrrl, rdl): 0.0568030849, 0.0003430441

Epoch over!
epoch time: 12.252

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.332 +- 0.210
mrr vals (pred, true): 0.142, 0.060

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.06477 	 0.04497 	 ~...
   38 	     1 	 0.13316 	 0.04673 	 m..s
   26 	     2 	 0.13017 	 0.04754 	 m..s
    0 	     3 	 0.02363 	 0.04836 	 ~...
    2 	     4 	 0.03616 	 0.04841 	 ~...
   58 	     5 	 0.13705 	 0.04853 	 m..s
   15 	     6 	 0.06158 	 0.04855 	 ~...
   60 	     7 	 0.13793 	 0.04888 	 m..s
   73 	     8 	 0.14268 	 0.04950 	 m..s
   47 	     9 	 0.13519 	 0.04963 	 m..s
    4 	    10 	 0.03829 	 0.04985 	 ~...
   72 	    11 	 0.14244 	 0.05001 	 m..s
   25 	    12 	 0.12923 	 0.05011 	 m..s
   41 	    13 	 0.13396 	 0.05037 	 m..s
   46 	    14 	 0.13493 	 0.05071 	 m..s
   16 	    15 	 0.06451 	 0.05088 	 ~...
    9 	    16 	 0.04348 	 0.05126 	 ~...
   69 	    17 	 0.14190 	 0.05138 	 m..s
   78 	    18 	 0.14528 	 0.05181 	 m..s
    8 	    19 	 0.04303 	 0.05213 	 ~...
   81 	    20 	 0.14714 	 0.05216 	 m..s
   10 	    21 	 0.04599 	 0.05271 	 ~...
   79 	    22 	 0.14530 	 0.05298 	 m..s
    1 	    23 	 0.02461 	 0.05331 	 ~...
    5 	    24 	 0.03969 	 0.05347 	 ~...
   39 	    25 	 0.13330 	 0.05356 	 m..s
   77 	    26 	 0.14501 	 0.05364 	 m..s
   71 	    27 	 0.14237 	 0.05383 	 m..s
   45 	    28 	 0.13490 	 0.05386 	 m..s
   59 	    29 	 0.13742 	 0.05395 	 m..s
    7 	    30 	 0.04017 	 0.05411 	 ~...
    6 	    31 	 0.04002 	 0.05432 	 ~...
   40 	    32 	 0.13374 	 0.05453 	 m..s
   12 	    33 	 0.04682 	 0.05455 	 ~...
    3 	    34 	 0.03815 	 0.05460 	 ~...
   54 	    35 	 0.13609 	 0.05465 	 m..s
   13 	    36 	 0.05613 	 0.05470 	 ~...
   49 	    37 	 0.13526 	 0.05483 	 m..s
   61 	    38 	 0.13800 	 0.05513 	 m..s
   11 	    39 	 0.04625 	 0.05552 	 ~...
   50 	    40 	 0.13549 	 0.05616 	 m..s
   14 	    41 	 0.05712 	 0.05658 	 ~...
   26 	    42 	 0.13017 	 0.05748 	 m..s
   67 	    43 	 0.14036 	 0.05825 	 m..s
   70 	    44 	 0.14228 	 0.05952 	 m..s
   64 	    45 	 0.13877 	 0.05955 	 m..s
   26 	    46 	 0.13017 	 0.05960 	 m..s
   62 	    47 	 0.13821 	 0.05998 	 m..s
   44 	    48 	 0.13486 	 0.06230 	 m..s
   82 	    49 	 0.18927 	 0.14805 	 m..s
   75 	    50 	 0.14298 	 0.18591 	 m..s
   52 	    51 	 0.13558 	 0.19706 	 m..s
   42 	    52 	 0.13460 	 0.20089 	 m..s
   55 	    53 	 0.13614 	 0.20349 	 m..s
   26 	    54 	 0.13017 	 0.21450 	 m..s
   76 	    55 	 0.14422 	 0.21711 	 m..s
   66 	    56 	 0.14034 	 0.21752 	 m..s
   65 	    57 	 0.13887 	 0.21836 	 m..s
   26 	    58 	 0.13017 	 0.21900 	 m..s
   80 	    59 	 0.14566 	 0.22524 	 m..s
   26 	    60 	 0.13017 	 0.22610 	 m..s
   26 	    61 	 0.13017 	 0.22734 	 m..s
   26 	    62 	 0.13017 	 0.23134 	 MISS
   68 	    63 	 0.14083 	 0.23413 	 m..s
   53 	    64 	 0.13592 	 0.23551 	 m..s
   48 	    65 	 0.13524 	 0.23787 	 MISS
   74 	    66 	 0.14287 	 0.24060 	 m..s
   26 	    67 	 0.13017 	 0.24548 	 MISS
   57 	    68 	 0.13672 	 0.24639 	 MISS
   51 	    69 	 0.13553 	 0.24764 	 MISS
   19 	    70 	 0.08254 	 0.24855 	 MISS
   26 	    71 	 0.13017 	 0.25213 	 MISS
   21 	    72 	 0.10833 	 0.25397 	 MISS
   22 	    73 	 0.11879 	 0.25655 	 MISS
   18 	    74 	 0.07625 	 0.25680 	 MISS
   20 	    75 	 0.10259 	 0.25748 	 MISS
   63 	    76 	 0.13848 	 0.25853 	 MISS
   43 	    77 	 0.13473 	 0.26165 	 MISS
   23 	    78 	 0.12257 	 0.26188 	 MISS
   24 	    79 	 0.12588 	 0.26309 	 MISS
   37 	    80 	 0.13038 	 0.26715 	 MISS
   26 	    81 	 0.13017 	 0.27504 	 MISS
   56 	    82 	 0.13633 	 0.27815 	 MISS
   87 	    83 	 0.37036 	 0.35525 	 ~...
   84 	    84 	 0.32323 	 0.35817 	 m..s
   93 	    85 	 0.39955 	 0.35878 	 m..s
   90 	    86 	 0.39443 	 0.36692 	 ~...
   89 	    87 	 0.39426 	 0.37294 	 ~...
   92 	    88 	 0.39938 	 0.37606 	 ~...
   94 	    89 	 0.39960 	 0.38009 	 ~...
   91 	    90 	 0.39470 	 0.38590 	 ~...
   96 	    91 	 0.40150 	 0.39038 	 ~...
   95 	    92 	 0.39982 	 0.39461 	 ~...
   85 	    93 	 0.33012 	 0.39489 	 m..s
   97 	    94 	 0.40297 	 0.39875 	 ~...
   88 	    95 	 0.37710 	 0.40034 	 ~...
   98 	    96 	 0.40447 	 0.40465 	 ~...
   83 	    97 	 0.23800 	 0.41063 	 MISS
  117 	    98 	 0.45187 	 0.41396 	 m..s
   86 	    99 	 0.35034 	 0.41636 	 m..s
  109 	   100 	 0.44287 	 0.42770 	 ~...
  111 	   101 	 0.44299 	 0.42947 	 ~...
  116 	   102 	 0.45127 	 0.43085 	 ~...
  101 	   103 	 0.43694 	 0.43133 	 ~...
  113 	   104 	 0.44393 	 0.43412 	 ~...
  106 	   105 	 0.44121 	 0.43550 	 ~...
  110 	   106 	 0.44288 	 0.43783 	 ~...
  107 	   107 	 0.44235 	 0.44052 	 ~...
  120 	   108 	 0.46036 	 0.44133 	 ~...
  104 	   109 	 0.44020 	 0.44168 	 ~...
  103 	   110 	 0.43967 	 0.44327 	 ~...
   99 	   111 	 0.42601 	 0.44352 	 ~...
  114 	   112 	 0.44543 	 0.44455 	 ~...
  115 	   113 	 0.44761 	 0.44618 	 ~...
  112 	   114 	 0.44350 	 0.44712 	 ~...
  102 	   115 	 0.43925 	 0.44804 	 ~...
  108 	   116 	 0.44284 	 0.45790 	 ~...
  100 	   117 	 0.42913 	 0.46434 	 m..s
  118 	   118 	 0.45347 	 0.46572 	 ~...
  105 	   119 	 0.44035 	 0.46786 	 ~...
  119 	   120 	 0.45422 	 0.47952 	 ~...
==========================================
r_mrr = 0.87712162733078
r2_mrr = 0.7622140645980835
spearmanr_mrr@5 = 0.9702891707420349
spearmanr_mrr@10 = 0.9855734705924988
spearmanr_mrr@50 = 0.9827719330787659
spearmanr_mrr@100 = 0.8869998455047607
spearmanr_mrr@All = 0.914008378982544
==========================================
test time: 0.393
Done Testing dataset Kinships
total time taken: 188.1031620502472
training time taken: 181.11529564857483
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8771)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7622)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9703)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9856)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9828)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8870)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9140)}}, 'test_loss': {'DistMult': {'Kinships': 7.258479605965476}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 7729071668358768
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [940, 203, 865, 35, 586, 305, 1052, 419, 548, 80, 240, 1164, 398, 383, 702, 1187, 565, 280, 867, 416, 819, 928, 562, 194, 883, 846, 892, 651, 431, 687, 601, 860, 902, 391, 344, 648, 767, 255, 872, 525, 87, 544, 335, 1048, 1139, 530, 440, 371, 117, 366, 40, 558, 54, 731, 1085, 365, 813, 510, 1068, 467, 557, 88, 165, 1046, 368, 1161, 1038, 560, 1182, 1090, 1149, 321, 653, 64, 823, 997, 1200, 497, 1167, 498, 696, 382, 1, 930, 1184, 128, 439, 723, 286, 862, 1146, 839, 121, 785, 529, 931, 76, 956, 1002, 802, 506, 658, 1169, 966, 420, 200, 152, 859, 338, 869, 552, 969, 6, 612, 413, 108, 777, 1136, 942, 1011, 204]
valid_ids (0): []
train_ids (1094): [751, 1214, 825, 184, 810, 598, 1155, 597, 1008, 900, 1041, 492, 798, 306, 864, 932, 345, 980, 805, 991, 953, 316, 1088, 977, 201, 1080, 160, 409, 272, 263, 360, 71, 703, 830, 1121, 271, 357, 1020, 155, 1083, 1213, 58, 1058, 622, 124, 1174, 303, 748, 906, 285, 198, 1034, 786, 434, 691, 449, 727, 205, 313, 1079, 415, 1181, 1141, 7, 403, 279, 626, 397, 1007, 1197, 26, 837, 994, 555, 781, 477, 776, 968, 242, 17, 712, 1054, 761, 317, 610, 499, 500, 185, 589, 57, 1190, 646, 127, 1153, 988, 649, 833, 721, 367, 699, 215, 627, 1025, 514, 296, 47, 348, 676, 1006, 1159, 520, 34, 1138, 1180, 504, 941, 1107, 485, 701, 814, 879, 1113, 1099, 1028, 827, 179, 984, 591, 590, 315, 922, 898, 618, 484, 1069, 531, 332, 175, 1032, 461, 995, 588, 1017, 1075, 395, 104, 1142, 78, 599, 1122, 667, 1091, 818, 572, 45, 1133, 519, 443, 469, 351, 292, 1104, 644, 1108, 170, 86, 125, 1015, 172, 281, 301, 522, 384, 331, 302, 473, 871, 993, 592, 518, 278, 550, 491, 325, 1024, 526, 1019, 604, 1065, 532, 845, 105, 100, 650, 577, 523, 107, 425, 311, 1204, 706, 249, 689, 692, 442, 1119, 1071, 949, 634, 451, 771, 513, 16, 923, 1029, 568, 704, 438, 686, 231, 435, 934, 261, 606, 437, 433, 1070, 613, 947, 422, 1196, 732, 584, 624, 299, 161, 166, 1188, 101, 256, 1171, 1074, 1100, 943, 364, 83, 964, 878, 333, 809, 1208, 674, 148, 844, 11, 275, 326, 329, 996, 693, 388, 246, 475, 1117, 669, 489, 816, 801, 115, 393, 159, 234, 1072, 193, 1060, 411, 549, 390, 370, 1152, 149, 962, 911, 5, 169, 319, 600, 840, 441, 739, 874, 528, 65, 540, 73, 685, 652, 665, 736, 593, 547, 831, 153, 248, 406, 886, 855, 1157, 821, 251, 112, 517, 836, 710, 679, 917, 605, 69, 378, 225, 881, 486, 524, 18, 1059, 954, 1150, 647, 223, 921, 683, 468, 615, 9, 129, 14, 151, 150, 749, 747, 570, 664, 247, 1018, 488, 904, 354, 52, 873, 471, 1192, 973, 1145, 578, 1111, 4, 211, 210, 1147, 875, 111, 958, 268, 950, 807, 386, 1185, 655, 412, 1210, 483, 447, 190, 428, 545, 445, 182, 815, 330, 1044, 671, 938, 289, 267, 353, 675, 312, 1000, 998, 714, 120, 794, 1031, 960, 457, 707, 192, 1166, 567, 220, 754, 778, 294, 737, 374, 1125, 429, 630, 539, 654, 569, 717, 718, 156, 677, 269, 72, 899, 542, 979, 177, 1151, 551, 870, 766, 30, 908, 848, 640, 581, 27, 334, 829, 218, 94, 358, 561, 768, 136, 755, 607, 1120, 288, 629, 625, 336, 511, 895, 342, 162, 1179, 349, 868, 999, 487, 219, 913, 93, 834, 284, 402, 910, 207, 959, 70, 527, 765, 1093, 660, 682, 444, 705, 180, 42, 118, 183, 452, 1096, 614, 608, 1170, 213, 926, 1110, 905, 933, 1137, 828, 937, 1055, 282, 1033, 779, 735, 852, 594, 79, 657, 1005, 851, 253, 67, 887, 1203, 232, 1115, 427, 619, 505, 373, 355, 110, 708, 681, 1082, 307, 41, 265, 1012, 212, 1131, 1086, 1168, 1004, 257, 621, 208, 135, 450, 975, 951, 713, 939, 1212, 186, 96, 609, 448, 270, 154, 756, 308, 340, 863, 639, 23, 314, 1073, 603, 227, 479, 1039, 293, 19, 733, 1123, 623, 103, 1076, 343, 241, 888, 481, 273, 961, 812, 436, 290, 59, 1050, 66, 258, 1023, 760, 740, 1081, 1126, 1135, 659, 379, 1105, 199, 971, 698, 244, 596, 914, 493, 967, 1103, 1087, 396, 324, 983, 1143, 912, 889, 298, 700, 143, 206, 1097, 470, 401, 84, 1202, 730, 924, 709, 217, 237, 38, 502, 893, 318, 347, 482, 1027, 1209, 1040, 74, 1183, 1112, 896, 22, 15, 1030, 51, 806, 356, 501, 50, 919, 595, 952, 176, 456, 222, 935, 363, 670, 230, 1094, 1148, 885, 464, 804, 564, 1078, 62, 446, 235, 495, 800, 684, 453, 841, 12, 750, 1092, 187, 1207, 259, 459, 264, 116, 1114, 13, 337, 91, 793, 797, 916, 233, 915, 769, 454, 1211, 1009, 1140, 799, 33, 533, 1102, 1062, 1003, 0, 350, 109, 31, 95, 1061, 795, 417, 1134, 847, 628, 772, 89, 46, 328, 274, 792, 925, 1042, 1084, 1177, 465, 1035, 668, 410, 987, 133, 1056, 141, 796, 729, 817, 1014, 480, 327, 811, 8, 430, 784, 146, 835, 75, 780, 189, 37, 909, 400, 250, 385, 986, 516, 25, 891, 515, 787, 191, 310, 617, 641, 521, 380, 126, 556, 387, 277, 139, 1109, 85, 494, 1098, 339, 842, 876, 77, 982, 632, 746, 119, 509, 826, 1130, 68, 715, 214, 276, 60, 929, 972, 236, 662, 1205, 462, 466, 209, 672, 948, 254, 39, 738, 123, 61, 1124, 260, 164, 167, 1176, 195, 1066, 432, 36, 1156, 376, 304, 890, 178, 965, 56, 575, 508, 145, 507, 228, 957, 690, 587, 1144, 1132, 1198, 49, 266, 537, 541, 359, 1051, 2, 571, 579, 678, 585, 789, 322, 1010, 688, 245, 985, 99, 424, 936, 1045, 90, 770, 142, 476, 1047, 389, 55, 131, 680, 734, 122, 490, 10, 974, 392, 81, 1106, 1154, 144, 407, 976, 1206, 1013, 408, 773, 460, 21, 262, 631, 790, 130, 1101, 381, 1022, 775, 1089, 822, 138, 163, 858, 849, 861, 361, 728, 224, 1158, 758, 743, 323, 1162, 907, 1057, 866, 352, 1037, 132, 229, 216, 636, 291, 602, 643, 346, 820, 741, 496, 1178, 97, 573, 472, 1063, 1128, 989, 535, 638, 978, 238, 377, 1064, 1163, 574, 877, 300, 576, 764, 853, 757, 171, 1172, 1116, 620, 1195, 405, 503, 239, 752, 719, 421, 48, 404, 918, 137, 720, 1199, 763, 546, 854, 543, 1026, 903, 897, 1173, 534, 1043, 458, 20, 43, 226, 394, 536, 656, 1165, 637, 832, 1127, 106, 197, 478, 134, 1016, 243, 1021, 633, 295, 372, 399, 616, 894, 24, 694, 970, 181, 63, 955, 158, 1194, 663, 29, 563, 673, 666, 920, 744, 341, 742, 1201, 418, 695, 320, 173, 808, 375, 850, 252, 369, 287, 945, 856, 774, 1186, 1193, 157, 1189, 283, 963, 553, 583, 753, 1036, 726, 1118, 3, 28, 857, 642, 838, 645, 788, 1049, 414, 221, 990, 782, 697, 114, 455, 725, 147, 582, 1095, 1191, 362, 722, 538, 309, 98, 803, 92, 174, 901, 102, 1175, 566, 202, 759, 635, 1160, 711, 791, 944, 426, 44, 188, 661, 843, 53, 168, 512, 992, 611, 927, 32, 1129, 196, 1001, 882, 297, 423, 580, 474, 824, 554, 880, 884, 463, 1067, 981, 113, 745, 82, 140, 716, 559, 724, 1053, 762, 783, 946, 1077]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5963786955734398
the save name prefix for this run is:  chkpt-ID_5963786955734398_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 912
rank avg (pred): 0.496 +- 0.003
mrr vals (pred, true): 0.019, 0.268
batch losses (mrrl, rdl): 0.0, 0.0036411977

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1079
rank avg (pred): 0.099 +- 0.049
mrr vals (pred, true): 0.153, 0.447
batch losses (mrrl, rdl): 0.0, 4.36897e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 384
rank avg (pred): 0.291 +- 0.208
mrr vals (pred, true): 0.212, 0.230
batch losses (mrrl, rdl): 0.0, 0.0009438912

Epoch over!
epoch time: 12.007

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 660
rank avg (pred): 0.276 +- 0.222
mrr vals (pred, true): 0.274, 0.057
batch losses (mrrl, rdl): 0.0, 0.0004424689

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 101
rank avg (pred): 0.282 +- 0.229
mrr vals (pred, true): 0.278, 0.240
batch losses (mrrl, rdl): 0.0, 0.000959174

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 43
rank avg (pred): 0.030 +- 0.027
mrr vals (pred, true): 0.491, 0.451
batch losses (mrrl, rdl): 0.0, 1.4025e-05

Epoch over!
epoch time: 11.667

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 492
rank avg (pred): 0.092 +- 0.084
mrr vals (pred, true): 0.408, 0.393
batch losses (mrrl, rdl): 0.0, 2.77113e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1026
rank avg (pred): 0.278 +- 0.232
mrr vals (pred, true): 0.303, 0.050
batch losses (mrrl, rdl): 0.0, 0.0005509335

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 112
rank avg (pred): 0.237 +- 0.226
mrr vals (pred, true): 0.392, 0.236
batch losses (mrrl, rdl): 0.0, 0.0004732911

Epoch over!
epoch time: 11.732

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 682
rank avg (pred): 0.251 +- 0.227
mrr vals (pred, true): 0.345, 0.054
batch losses (mrrl, rdl): 0.0, 0.0006295912

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 830
rank avg (pred): 0.129 +- 0.119
mrr vals (pred, true): 0.394, 0.418
batch losses (mrrl, rdl): 0.0, 0.0001418797

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 470
rank avg (pred): 0.223 +- 0.210
mrr vals (pred, true): 0.382, 0.053
batch losses (mrrl, rdl): 0.0, 0.0009376206

Epoch over!
epoch time: 11.633

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 231
rank avg (pred): 0.257 +- 0.234
mrr vals (pred, true): 0.354, 0.049
batch losses (mrrl, rdl): 0.0, 0.0006904704

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 126
rank avg (pred): 0.260 +- 0.241
mrr vals (pred, true): 0.367, 0.234
batch losses (mrrl, rdl): 0.0, 0.0006099715

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 650
rank avg (pred): 0.247 +- 0.238
mrr vals (pred, true): 0.397, 0.054
batch losses (mrrl, rdl): 0.0, 0.0006458032

Epoch over!
epoch time: 11.647

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 145
rank avg (pred): 0.258 +- 0.235
mrr vals (pred, true): 0.348, 0.256
batch losses (mrrl, rdl): 0.0849704519, 0.000785845

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1172
rank avg (pred): 0.345 +- 0.210
mrr vals (pred, true): 0.150, 0.234
batch losses (mrrl, rdl): 0.0698498636, 0.0013467269

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 251
rank avg (pred): 0.028 +- 0.021
mrr vals (pred, true): 0.437, 0.427
batch losses (mrrl, rdl): 0.0009677599, 1.77011e-05

Epoch over!
epoch time: 12.019

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1113
rank avg (pred): 0.367 +- 0.215
mrr vals (pred, true): 0.151, 0.055
batch losses (mrrl, rdl): 0.1023087502, 0.0001356555

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 167
rank avg (pred): 0.373 +- 0.209
mrr vals (pred, true): 0.120, 0.061
batch losses (mrrl, rdl): 0.0493131876, 9.14775e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 410
rank avg (pred): 0.358 +- 0.214
mrr vals (pred, true): 0.145, 0.058
batch losses (mrrl, rdl): 0.0895772055, 0.0001439927

Epoch over!
epoch time: 12.282

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 518
rank avg (pred): 0.039 +- 0.028
mrr vals (pred, true): 0.388, 0.359
batch losses (mrrl, rdl): 0.0082739051, 1.767e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 654
rank avg (pred): 0.352 +- 0.210
mrr vals (pred, true): 0.149, 0.060
batch losses (mrrl, rdl): 0.0982074365, 0.0001632253

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 403
rank avg (pred): 0.342 +- 0.211
mrr vals (pred, true): 0.164, 0.228
batch losses (mrrl, rdl): 0.0407980904, 0.0013988969

Epoch over!
epoch time: 12.324

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 499
rank avg (pred): 0.047 +- 0.034
mrr vals (pred, true): 0.364, 0.412
batch losses (mrrl, rdl): 0.0230578352, 1.9699e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 619
rank avg (pred): 0.355 +- 0.202
mrr vals (pred, true): 0.135, 0.227
batch losses (mrrl, rdl): 0.0850350261, 0.0014098424

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 331
rank avg (pred): 0.318 +- 0.208
mrr vals (pred, true): 0.191, 0.209
batch losses (mrrl, rdl): 0.0032236241, 0.0009844877

Epoch over!
epoch time: 12.064

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 287
rank avg (pred): 0.033 +- 0.022
mrr vals (pred, true): 0.384, 0.446
batch losses (mrrl, rdl): 0.0383076034, 9.8001e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 48
rank avg (pred): 0.025 +- 0.017
mrr vals (pred, true): 0.429, 0.429
batch losses (mrrl, rdl): 4.4307e-06, 2.42649e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 378
rank avg (pred): 0.345 +- 0.196
mrr vals (pred, true): 0.136, 0.235
batch losses (mrrl, rdl): 0.0971305296, 0.0012781098

Epoch over!
epoch time: 12.082

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 960
rank avg (pred): 0.750 +- 0.185
mrr vals (pred, true): 0.030, 0.053
batch losses (mrrl, rdl): 0.0041841529, 0.0017321685

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 430
rank avg (pred): 0.308 +- 0.191
mrr vals (pred, true): 0.175, 0.058
batch losses (mrrl, rdl): 0.1568629146, 0.0003783656

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 561
rank avg (pred): 0.039 +- 0.027
mrr vals (pred, true): 0.381, 0.353
batch losses (mrrl, rdl): 0.0077434825, 2.44788e-05

Epoch over!
epoch time: 12.225

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 86
rank avg (pred): 0.337 +- 0.189
mrr vals (pred, true): 0.136, 0.237
batch losses (mrrl, rdl): 0.1021303385, 0.0010783427

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 242
rank avg (pred): 0.325 +- 0.193
mrr vals (pred, true): 0.157, 0.060
batch losses (mrrl, rdl): 0.1142542213, 0.000332312

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 995
rank avg (pred): 0.028 +- 0.019
mrr vals (pred, true): 0.415, 0.441
batch losses (mrrl, rdl): 0.0068933656, 1.70474e-05

Epoch over!
epoch time: 12.104

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 910
rank avg (pred): 0.053 +- 0.034
mrr vals (pred, true): 0.324, 0.353
batch losses (mrrl, rdl): 0.008411875, 4.5206e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 736
rank avg (pred): 0.430 +- 0.219
mrr vals (pred, true): 0.109, 0.021
batch losses (mrrl, rdl): 0.0354001373, 3.85856e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 576
rank avg (pred): 0.320 +- 0.175
mrr vals (pred, true): 0.134, 0.239
batch losses (mrrl, rdl): 0.1106669679, 0.0012123289

Epoch over!
epoch time: 12.084

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 229
rank avg (pred): 0.337 +- 0.177
mrr vals (pred, true): 0.119, 0.062
batch losses (mrrl, rdl): 0.0471006185, 0.0002146763

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 898
rank avg (pred): 0.445 +- 0.214
mrr vals (pred, true): 0.112, 0.021
batch losses (mrrl, rdl): 0.0381234623, 3.23651e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 745
rank avg (pred): 0.155 +- 0.097
mrr vals (pred, true): 0.214, 0.196
batch losses (mrrl, rdl): 0.0032952589, 5.04845e-05

Epoch over!
epoch time: 11.997

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 780
rank avg (pred): 0.652 +- 0.158
mrr vals (pred, true): 0.032, 0.049
batch losses (mrrl, rdl): 0.0032269252, 0.0005404664

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 441
rank avg (pred): 0.287 +- 0.184
mrr vals (pred, true): 0.193, 0.056
batch losses (mrrl, rdl): 0.204532519, 0.0005477364

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 269
rank avg (pred): 0.021 +- 0.013
mrr vals (pred, true): 0.435, 0.444
batch losses (mrrl, rdl): 0.0008526311, 2.45731e-05

Epoch over!
epoch time: 12.167

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.496 +- 0.176
mrr vals (pred, true): 0.077, 0.045

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.07632 	 0.04297 	 m..s
    2 	     1 	 0.05218 	 0.04299 	 ~...
   10 	     2 	 0.07664 	 0.04468 	 m..s
   13 	     3 	 0.07789 	 0.04509 	 m..s
   54 	     4 	 0.17965 	 0.04768 	 MISS
   14 	     5 	 0.08257 	 0.04779 	 m..s
   81 	     6 	 0.19435 	 0.04904 	 MISS
    6 	     7 	 0.07134 	 0.04972 	 ~...
   19 	     8 	 0.10407 	 0.04975 	 m..s
   50 	     9 	 0.17784 	 0.04982 	 MISS
   12 	    10 	 0.07763 	 0.05000 	 ~...
   55 	    11 	 0.17977 	 0.05011 	 MISS
   45 	    12 	 0.17687 	 0.05032 	 MISS
   41 	    13 	 0.17514 	 0.05042 	 MISS
   79 	    14 	 0.18872 	 0.05077 	 MISS
   20 	    15 	 0.11069 	 0.05131 	 m..s
   62 	    16 	 0.18165 	 0.05135 	 MISS
   46 	    17 	 0.17708 	 0.05181 	 MISS
   35 	    18 	 0.17512 	 0.05230 	 MISS
   43 	    19 	 0.17603 	 0.05234 	 MISS
   61 	    20 	 0.18159 	 0.05290 	 MISS
   16 	    21 	 0.08759 	 0.05290 	 m..s
   76 	    22 	 0.18632 	 0.05297 	 MISS
   51 	    23 	 0.17807 	 0.05297 	 MISS
   73 	    24 	 0.18435 	 0.05325 	 MISS
   29 	    25 	 0.16268 	 0.05337 	 MISS
   74 	    26 	 0.18453 	 0.05338 	 MISS
   15 	    27 	 0.08754 	 0.05358 	 m..s
   60 	    28 	 0.18120 	 0.05383 	 MISS
    3 	    29 	 0.05691 	 0.05394 	 ~...
    8 	    30 	 0.07572 	 0.05400 	 ~...
    7 	    31 	 0.07195 	 0.05411 	 ~...
    1 	    32 	 0.05181 	 0.05429 	 ~...
    4 	    33 	 0.06511 	 0.05460 	 ~...
   17 	    34 	 0.09193 	 0.05462 	 m..s
   77 	    35 	 0.18640 	 0.05482 	 MISS
   59 	    36 	 0.18119 	 0.05488 	 MISS
    5 	    37 	 0.07079 	 0.05498 	 ~...
   18 	    38 	 0.09863 	 0.05593 	 m..s
   52 	    39 	 0.17907 	 0.05657 	 MISS
   78 	    40 	 0.18654 	 0.05698 	 MISS
   34 	    41 	 0.17480 	 0.05746 	 MISS
   35 	    42 	 0.17512 	 0.05772 	 MISS
   11 	    43 	 0.07717 	 0.05838 	 ~...
    0 	    44 	 0.04418 	 0.05879 	 ~...
   57 	    45 	 0.18089 	 0.05880 	 MISS
   21 	    46 	 0.11292 	 0.05906 	 m..s
   69 	    47 	 0.18335 	 0.05917 	 MISS
   75 	    48 	 0.18456 	 0.06090 	 MISS
   83 	    49 	 0.27569 	 0.19851 	 m..s
   30 	    50 	 0.16310 	 0.20531 	 m..s
   42 	    51 	 0.17564 	 0.22593 	 m..s
   49 	    52 	 0.17770 	 0.22636 	 m..s
   66 	    53 	 0.18240 	 0.22646 	 m..s
   35 	    54 	 0.17512 	 0.22897 	 m..s
   70 	    55 	 0.18393 	 0.22971 	 m..s
   26 	    56 	 0.15973 	 0.23128 	 m..s
   68 	    57 	 0.18319 	 0.23155 	 m..s
   44 	    58 	 0.17662 	 0.23171 	 m..s
   27 	    59 	 0.15986 	 0.23187 	 m..s
   72 	    60 	 0.18435 	 0.23467 	 m..s
   80 	    61 	 0.19025 	 0.23503 	 m..s
   35 	    62 	 0.17512 	 0.23787 	 m..s
   31 	    63 	 0.16444 	 0.23847 	 m..s
   48 	    64 	 0.17761 	 0.24002 	 m..s
   64 	    65 	 0.18179 	 0.24013 	 m..s
   24 	    66 	 0.15687 	 0.24060 	 m..s
   28 	    67 	 0.16042 	 0.24581 	 m..s
   35 	    68 	 0.17512 	 0.24722 	 m..s
   63 	    69 	 0.18178 	 0.24763 	 m..s
   47 	    70 	 0.17752 	 0.24764 	 m..s
   35 	    71 	 0.17512 	 0.24779 	 m..s
   71 	    72 	 0.18402 	 0.24937 	 m..s
   32 	    73 	 0.17426 	 0.25074 	 m..s
   65 	    74 	 0.18193 	 0.25333 	 m..s
   53 	    75 	 0.17923 	 0.25355 	 m..s
   23 	    76 	 0.14573 	 0.25379 	 MISS
   22 	    77 	 0.13961 	 0.25748 	 MISS
   58 	    78 	 0.18090 	 0.26210 	 m..s
   67 	    79 	 0.18284 	 0.26242 	 m..s
   56 	    80 	 0.18062 	 0.26251 	 m..s
   33 	    81 	 0.17452 	 0.26671 	 m..s
   25 	    82 	 0.15955 	 0.27397 	 MISS
   82 	    83 	 0.22440 	 0.30657 	 m..s
   89 	    84 	 0.39677 	 0.36325 	 m..s
   84 	    85 	 0.30334 	 0.36367 	 m..s
   99 	    86 	 0.41193 	 0.36692 	 m..s
   88 	    87 	 0.39196 	 0.36751 	 ~...
  104 	    88 	 0.42304 	 0.37045 	 m..s
  101 	    89 	 0.41378 	 0.37175 	 m..s
   94 	    90 	 0.40574 	 0.37210 	 m..s
   87 	    91 	 0.39189 	 0.37213 	 ~...
   90 	    92 	 0.39934 	 0.37516 	 ~...
   86 	    93 	 0.38748 	 0.37543 	 ~...
   97 	    94 	 0.40839 	 0.38333 	 ~...
   96 	    95 	 0.40824 	 0.38590 	 ~...
   85 	    96 	 0.38559 	 0.38613 	 ~...
   92 	    97 	 0.40225 	 0.38639 	 ~...
   91 	    98 	 0.40218 	 0.39308 	 ~...
  107 	    99 	 0.43669 	 0.39489 	 m..s
  102 	   100 	 0.42026 	 0.39602 	 ~...
   93 	   101 	 0.40313 	 0.40219 	 ~...
   95 	   102 	 0.40765 	 0.40889 	 ~...
  103 	   103 	 0.42037 	 0.41336 	 ~...
   98 	   104 	 0.41056 	 0.42435 	 ~...
  100 	   105 	 0.41258 	 0.43014 	 ~...
  119 	   106 	 0.48188 	 0.43133 	 m..s
  109 	   107 	 0.44002 	 0.43376 	 ~...
  112 	   108 	 0.45753 	 0.43991 	 ~...
  118 	   109 	 0.46645 	 0.44168 	 ~...
  114 	   110 	 0.46052 	 0.44251 	 ~...
  113 	   111 	 0.45879 	 0.44365 	 ~...
  115 	   112 	 0.46098 	 0.44410 	 ~...
  108 	   113 	 0.43706 	 0.44451 	 ~...
  117 	   114 	 0.46537 	 0.44534 	 ~...
  116 	   115 	 0.46345 	 0.44604 	 ~...
  120 	   116 	 0.48289 	 0.44750 	 m..s
  106 	   117 	 0.43558 	 0.44759 	 ~...
  110 	   118 	 0.45342 	 0.45465 	 ~...
  111 	   119 	 0.45582 	 0.46171 	 ~...
  105 	   120 	 0.43130 	 0.46297 	 m..s
==========================================
r_mrr = 0.8798270225524902
r2_mrr = 0.7596104145050049
spearmanr_mrr@5 = 0.9484200477600098
spearmanr_mrr@10 = 0.9744764566421509
spearmanr_mrr@50 = 0.9809221029281616
spearmanr_mrr@100 = 0.8829020857810974
spearmanr_mrr@All = 0.9136597514152527
==========================================
test time: 0.549
Done Testing dataset Kinships
total time taken: 187.6784701347351
training time taken: 180.6451072692871
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8798)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7596)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9484)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9745)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9809)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8829)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9137)}}, 'test_loss': {'DistMult': {'Kinships': 7.008217297978263}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 2087341477706789
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1131, 719, 180, 534, 362, 582, 406, 49, 30, 260, 654, 533, 454, 668, 435, 136, 1038, 1115, 428, 573, 1059, 526, 1135, 1161, 1136, 446, 282, 1064, 786, 1175, 297, 448, 767, 660, 880, 672, 1182, 442, 445, 319, 1090, 188, 1104, 352, 1148, 381, 314, 749, 935, 639, 1058, 1033, 66, 870, 1114, 345, 1036, 1091, 1128, 731, 214, 389, 433, 1177, 588, 970, 845, 696, 353, 713, 962, 1032, 122, 1031, 889, 368, 738, 158, 1194, 185, 784, 558, 431, 360, 865, 43, 648, 16, 825, 739, 929, 727, 1169, 456, 1139, 882, 896, 1041, 186, 290, 421, 357, 472, 1189, 398, 513, 22, 1009, 135, 1086, 516, 1053, 225, 793, 597, 229, 1068, 56, 817, 333, 804]
valid_ids (0): []
train_ids (1094): [1127, 625, 438, 1003, 443, 688, 584, 647, 855, 768, 462, 670, 1039, 336, 169, 753, 805, 565, 58, 9, 591, 497, 24, 594, 969, 1142, 973, 227, 892, 808, 71, 423, 531, 1179, 750, 385, 177, 194, 971, 1082, 850, 467, 1055, 557, 34, 1207, 875, 261, 243, 702, 15, 479, 482, 1022, 154, 1019, 240, 107, 546, 709, 583, 1170, 26, 1203, 405, 394, 890, 1134, 128, 887, 1147, 239, 644, 88, 296, 1000, 867, 280, 488, 155, 139, 72, 40, 1085, 152, 20, 555, 697, 525, 972, 537, 1210, 790, 129, 824, 795, 1071, 359, 329, 550, 404, 545, 1073, 759, 674, 328, 919, 383, 208, 800, 690, 121, 1010, 1101, 838, 599, 407, 632, 311, 1046, 821, 595, 755, 1057, 102, 653, 903, 473, 118, 138, 737, 1008, 344, 918, 293, 905, 695, 572, 980, 444, 846, 1199, 891, 411, 106, 848, 560, 1049, 274, 922, 210, 209, 952, 150, 1099, 728, 54, 313, 600, 1097, 221, 1163, 763, 337, 272, 942, 267, 940, 590, 1196, 104, 1119, 37, 832, 553, 592, 593, 12, 163, 351, 307, 125, 736, 377, 187, 224, 1125, 914, 975, 89, 233, 1001, 748, 778, 758, 1020, 413, 801, 965, 735, 770, 162, 782, 91, 315, 103, 449, 602, 764, 202, 999, 205, 618, 977, 577, 363, 430, 387, 396, 953, 603, 938, 292, 252, 90, 245, 1109, 643, 773, 447, 478, 429, 923, 666, 141, 756, 571, 1074, 1080, 628, 716, 327, 989, 204, 528, 1, 646, 190, 957, 686, 101, 112, 524, 857, 461, 212, 655, 966, 1112, 258, 909, 217, 536, 1171, 98, 171, 564, 1056, 301, 403, 1120, 1117, 140, 495, 802, 1193, 1111, 123, 1197, 420, 500, 341, 343, 829, 853, 1105, 1072, 601, 28, 757, 788, 833, 1106, 60, 415, 623, 985, 798, 1116, 393, 651, 835, 97, 324, 694, 774, 146, 700, 634, 743, 1202, 1140, 105, 1152, 160, 562, 269, 111, 55, 687, 254, 250, 2, 1211, 32, 94, 742, 611, 199, 303, 943, 609, 399, 132, 51, 1156, 538, 80, 904, 263, 883, 656, 561, 1100, 249, 235, 412, 391, 510, 663, 598, 1123, 1002, 807, 35, 285, 1145, 417, 785, 416, 349, 616, 920, 876, 326, 69, 715, 636, 59, 554, 149, 885, 916, 578, 453, 509, 316, 499, 476, 242, 858, 1084, 166, 771, 745, 681, 703, 1153, 1167, 287, 680, 1006, 752, 300, 976, 939, 859, 1176, 77, 685, 7, 523, 791, 304, 535, 1209, 926, 1118, 1201, 419, 1204, 1208, 161, 854, 330, 514, 792, 1122, 126, 1113, 606, 1035, 946, 987, 959, 678, 4, 979, 127, 621, 286, 675, 871, 724, 789, 997, 271, 1077, 207, 388, 459, 995, 677, 33, 541, 455, 318, 803, 219, 153, 366, 458, 563, 1093, 8, 721, 321, 1029, 507, 1184, 119, 772, 908, 676, 947, 839, 506, 740, 305, 504, 799, 197, 57, 827, 130, 944, 134, 256, 951, 480, 559, 775, 813, 949, 860, 266, 568, 1137, 92, 819, 116, 452, 1070, 990, 376, 331, 502, 222, 1047, 378, 450, 1164, 355, 1124, 61, 3, 1166, 732, 1165, 607, 174, 75, 512, 604, 532, 822, 1052, 991, 338, 569, 640, 203, 556, 41, 373, 769, 983, 361, 165, 487, 1107, 131, 1158, 734, 856, 667, 426, 1027, 895, 884, 956, 936, 408, 705, 610, 1014, 191, 868, 626, 317, 661, 1198, 810, 996, 1045, 629, 701, 751, 86, 682, 501, 347, 761, 1069, 234, 852, 156, 649, 1007, 469, 549, 1185, 552, 907, 963, 196, 226, 124, 893, 994, 760, 707, 1013, 142, 729, 1205, 465, 284, 615, 605, 527, 818, 613, 236, 776, 79, 400, 877, 659, 1021, 230, 794, 878, 27, 539, 1102, 114, 746, 783, 1030, 25, 924, 109, 21, 517, 457, 974, 439, 570, 998, 812, 237, 1040, 145, 993, 567, 1061, 159, 503, 862, 650, 664, 1044, 830, 170, 1004, 281, 251, 372, 619, 950, 722, 11, 879, 726, 14, 1126, 382, 99, 652, 367, 836, 1149, 87, 780, 566, 576, 18, 706, 712, 1213, 179, 1011, 714, 520, 295, 365, 1138, 1129, 108, 247, 547, 933, 1181, 172, 1005, 580, 1043, 238, 23, 492, 183, 1130, 548, 978, 1172, 268, 522, 490, 1195, 496, 1180, 110, 491, 255, 216, 955, 948, 1078, 828, 181, 228, 1162, 837, 1186, 692, 189, 288, 1075, 100, 143, 115, 1095, 967, 861, 1168, 45, 277, 485, 954, 117, 386, 777, 1024, 323, 44, 1087, 515, 241, 635, 873, 657, 375, 0, 1025, 814, 466, 67, 894, 1191, 133, 63, 299, 289, 354, 587, 586, 913, 340, 74, 866, 992, 816, 711, 48, 1103, 310, 410, 708, 575, 1092, 941, 1212, 471, 463, 574, 96, 881, 42, 342, 542, 278, 1200, 718, 521, 662, 1094, 910, 70, 671, 1121, 725, 73, 358, 834, 38, 620, 425, 427, 397, 13, 52, 809, 275, 864, 374, 232, 62, 1155, 151, 231, 691, 630, 811, 698, 765, 246, 717, 787, 470, 669, 831, 1067, 346, 356, 432, 309, 164, 218, 322, 1060, 308, 1098, 257, 863, 380, 401, 53, 474, 1048, 930, 481, 744, 988, 508, 422, 928, 291, 283, 849, 120, 544, 518, 1026, 826, 184, 900, 684, 1141, 148, 897, 766, 76, 19, 982, 223, 505, 781, 451, 84, 902, 665, 434, 596, 402, 642, 981, 638, 1062, 1066, 1018, 530, 608, 741, 440, 392, 540, 325, 206, 872, 201, 658, 464, 641, 306, 175, 1174, 1081, 1012, 797, 475, 1160, 1042, 424, 1159, 1178, 178, 898, 81, 312, 1110, 5, 921, 710, 36, 371, 47, 248, 617, 441, 265, 543, 390, 1150, 198, 631, 915, 1015, 689, 511, 6, 418, 95, 262, 335, 484, 1076, 1079, 1146, 409, 730, 673, 82, 622, 612, 1051, 264, 843, 931, 984, 182, 961, 200, 320, 1023, 579, 78, 215, 193, 841, 779, 294, 699, 899, 173, 645, 364, 350, 85, 1037, 1206, 276, 958, 192, 494, 796, 901, 83, 437, 869, 820, 348, 436, 298, 934, 851, 1132, 806, 720, 723, 679, 815, 1054, 468, 302, 874, 906, 65, 1183, 1190, 1154, 31, 195, 384, 414, 633, 519, 379, 986, 489, 147, 113, 273, 220, 259, 1133, 581, 1143, 529, 964, 144, 176, 886, 683, 844, 339, 927, 1017, 1088, 733, 925, 213, 334, 370, 395, 483, 960, 211, 253, 1192, 627, 823, 244, 1151, 840, 460, 637, 68, 551, 754, 911, 168, 137, 64, 270, 493, 1034, 46, 704, 937, 167, 932, 29, 912, 968, 1187, 486, 157, 1083, 1096, 1065, 1016, 1144, 693, 847, 1050, 279, 50, 1063, 477, 498, 332, 1108, 93, 945, 842, 1173, 589, 624, 17, 1214, 917, 1028, 585, 10, 888, 614, 1157, 369, 39, 747, 762, 1188, 1089]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3736844726192935
the save name prefix for this run is:  chkpt-ID_3736844726192935_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 921
rank avg (pred): 0.578 +- 0.002
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0, 0.0002167066

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1191
rank avg (pred): 0.371 +- 0.224
mrr vals (pred, true): 0.118, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001175386

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1075
rank avg (pred): 0.037 +- 0.038
mrr vals (pred, true): 0.514, 0.460
batch losses (mrrl, rdl): 0.0, 6.3958e-06

Epoch over!
epoch time: 11.975

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 123
rank avg (pred): 0.257 +- 0.233
mrr vals (pred, true): 0.322, 0.243
batch losses (mrrl, rdl): 0.0, 0.0006974125

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 195
rank avg (pred): 0.241 +- 0.233
mrr vals (pred, true): 0.356, 0.051
batch losses (mrrl, rdl): 0.0, 0.0007038998

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 924
rank avg (pred): 0.419 +- 0.334
mrr vals (pred, true): 0.284, 0.046
batch losses (mrrl, rdl): 0.0, 6.57064e-05

Epoch over!
epoch time: 11.892

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1171
rank avg (pred): 0.278 +- 0.246
mrr vals (pred, true): 0.305, 0.231
batch losses (mrrl, rdl): 0.0, 0.0007232437

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 257
rank avg (pred): 0.044 +- 0.042
mrr vals (pred, true): 0.477, 0.434
batch losses (mrrl, rdl): 0.0, 3.4586e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1138
rank avg (pred): 0.093 +- 0.097
mrr vals (pred, true): 0.462, 0.418
batch losses (mrrl, rdl): 0.0, 3.46225e-05

Epoch over!
epoch time: 11.895

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1062
rank avg (pred): 0.038 +- 0.040
mrr vals (pred, true): 0.524, 0.432
batch losses (mrrl, rdl): 0.0, 7.0182e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 925
rank avg (pred): 0.493 +- 0.321
mrr vals (pred, true): 0.185, 0.039
batch losses (mrrl, rdl): 0.0, 2.73898e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 952
rank avg (pred): 0.519 +- 0.330
mrr vals (pred, true): 0.199, 0.053
batch losses (mrrl, rdl): 0.0, 0.0001196839

Epoch over!
epoch time: 11.92

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 346
rank avg (pred): 0.257 +- 0.245
mrr vals (pred, true): 0.357, 0.245
batch losses (mrrl, rdl): 0.0, 0.0007510391

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1165
rank avg (pred): 0.248 +- 0.251
mrr vals (pred, true): 0.411, 0.261
batch losses (mrrl, rdl): 0.0, 0.0007371349

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 926
rank avg (pred): 0.569 +- 0.304
mrr vals (pred, true): 0.129, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001876252

Epoch over!
epoch time: 11.924

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1198
rank avg (pred): 0.253 +- 0.245
mrr vals (pred, true): 0.361, 0.056
batch losses (mrrl, rdl): 0.9670960903, 0.0005742975

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 6
rank avg (pred): 0.027 +- 0.019
mrr vals (pred, true): 0.426, 0.444
batch losses (mrrl, rdl): 0.0031330958, 1.63253e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 291
rank avg (pred): 0.023 +- 0.017
mrr vals (pred, true): 0.458, 0.447
batch losses (mrrl, rdl): 0.0011528897, 2.56923e-05

Epoch over!
epoch time: 12.302

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 760
rank avg (pred): 0.742 +- 0.192
mrr vals (pred, true): 0.029, 0.048
batch losses (mrrl, rdl): 0.0044456474, 0.0013924775

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 802
rank avg (pred): 0.509 +- 0.207
mrr vals (pred, true): 0.093, 0.055
batch losses (mrrl, rdl): 0.0184789952, 8.69277e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1106
rank avg (pred): 0.340 +- 0.213
mrr vals (pred, true): 0.168, 0.259
batch losses (mrrl, rdl): 0.0819545612, 0.0015246259

Epoch over!
epoch time: 11.945

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 31
rank avg (pred): 0.025 +- 0.017
mrr vals (pred, true): 0.421, 0.434
batch losses (mrrl, rdl): 0.0016835005, 2.15073e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 743
rank avg (pred): 0.047 +- 0.030
mrr vals (pred, true): 0.320, 0.415
batch losses (mrrl, rdl): 0.090200685, 3.6438e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1188
rank avg (pred): 0.350 +- 0.205
mrr vals (pred, true): 0.157, 0.055
batch losses (mrrl, rdl): 0.1140414998, 0.0002061136

Epoch over!
epoch time: 11.859

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 641
rank avg (pred): 0.348 +- 0.205
mrr vals (pred, true): 0.156, 0.246
batch losses (mrrl, rdl): 0.0804314688, 0.0015276377

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 794
rank avg (pred): 0.525 +- 0.189
mrr vals (pred, true): 0.083, 0.055
batch losses (mrrl, rdl): 0.0106350128, 0.0001294769

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1187
rank avg (pred): 0.339 +- 0.202
mrr vals (pred, true): 0.166, 0.251
batch losses (mrrl, rdl): 0.0723606274, 0.0012778891

Epoch over!
epoch time: 12.036

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 349
rank avg (pred): 0.315 +- 0.183
mrr vals (pred, true): 0.156, 0.246
batch losses (mrrl, rdl): 0.0823873356, 0.0011500627

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 900
rank avg (pred): 0.056 +- 0.035
mrr vals (pred, true): 0.300, 0.385
batch losses (mrrl, rdl): 0.0714592487, 9.283e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1133
rank avg (pred): 0.322 +- 0.181
mrr vals (pred, true): 0.147, 0.052
batch losses (mrrl, rdl): 0.0942895189, 0.0003339246

Epoch over!
epoch time: 12.008

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 13
rank avg (pred): 0.013 +- 0.008
mrr vals (pred, true): 0.508, 0.435
batch losses (mrrl, rdl): 0.0527335741, 3.8047e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 183
rank avg (pred): 0.378 +- 0.168
mrr vals (pred, true): 0.118, 0.060
batch losses (mrrl, rdl): 0.0459454507, 0.0001374016

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 317
rank avg (pred): 0.025 +- 0.017
mrr vals (pred, true): 0.417, 0.438
batch losses (mrrl, rdl): 0.0043354407, 1.89406e-05

Epoch over!
epoch time: 12.011

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 806
rank avg (pred): 0.647 +- 0.193
mrr vals (pred, true): 0.057, 0.054
batch losses (mrrl, rdl): 0.0004351969, 0.0007755051

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 776
rank avg (pred): 0.600 +- 0.189
mrr vals (pred, true): 0.065, 0.060
batch losses (mrrl, rdl): 0.0023376052, 0.0006699123

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1049
rank avg (pred): 0.334 +- 0.179
mrr vals (pred, true): 0.131, 0.054
batch losses (mrrl, rdl): 0.0661386102, 0.0003332041

Epoch over!
epoch time: 11.918

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 329
rank avg (pred): 0.341 +- 0.177
mrr vals (pred, true): 0.129, 0.230
batch losses (mrrl, rdl): 0.1012467593, 0.0011238309

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1172
rank avg (pred): 0.338 +- 0.174
mrr vals (pred, true): 0.125, 0.234
batch losses (mrrl, rdl): 0.1192760393, 0.0012330837

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 960
rank avg (pred): 0.773 +- 0.172
mrr vals (pred, true): 0.034, 0.053
batch losses (mrrl, rdl): 0.0026260992, 0.0020535686

Epoch over!
epoch time: 11.805

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 890
rank avg (pred): 0.627 +- 0.202
mrr vals (pred, true): 0.066, 0.054
batch losses (mrrl, rdl): 0.0026244465, 0.0006052122

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 862
rank avg (pred): 0.636 +- 0.173
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 7.38012e-05, 0.0005335811

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1132
rank avg (pred): 0.304 +- 0.173
mrr vals (pred, true): 0.154, 0.053
batch losses (mrrl, rdl): 0.108589828, 0.0005034432

Epoch over!
epoch time: 12.015

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 335
rank avg (pred): 0.314 +- 0.178
mrr vals (pred, true): 0.152, 0.240
batch losses (mrrl, rdl): 0.0769786388, 0.0008491414

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 263
rank avg (pred): 0.020 +- 0.012
mrr vals (pred, true): 0.435, 0.436
batch losses (mrrl, rdl): 1.05305e-05, 2.79059e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 453
rank avg (pred): 0.288 +- 0.164
mrr vals (pred, true): 0.156, 0.057
batch losses (mrrl, rdl): 0.1123243868, 0.0005729959

Epoch over!
epoch time: 12.251

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.310 +- 0.175
mrr vals (pred, true): 0.151, 0.059

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   77 	     0 	 0.15905 	 0.02078 	 MISS
   20 	     1 	 0.13778 	 0.02078 	 MISS
    1 	     2 	 0.03947 	 0.04238 	 ~...
   14 	     3 	 0.07899 	 0.04497 	 m..s
   11 	     4 	 0.06784 	 0.04646 	 ~...
   85 	     5 	 0.17041 	 0.04904 	 MISS
   10 	     6 	 0.06609 	 0.04927 	 ~...
   25 	     7 	 0.14611 	 0.04963 	 m..s
   17 	     8 	 0.12623 	 0.04999 	 m..s
   53 	     9 	 0.14775 	 0.05011 	 m..s
   56 	    10 	 0.14792 	 0.05113 	 m..s
   82 	    11 	 0.16782 	 0.05138 	 MISS
   27 	    12 	 0.14699 	 0.05145 	 m..s
    4 	    13 	 0.04580 	 0.05197 	 ~...
   13 	    14 	 0.07260 	 0.05210 	 ~...
   52 	    15 	 0.14773 	 0.05250 	 m..s
   27 	    16 	 0.14699 	 0.05276 	 m..s
   76 	    17 	 0.15887 	 0.05287 	 MISS
    6 	    18 	 0.04699 	 0.05287 	 ~...
    0 	    19 	 0.03912 	 0.05325 	 ~...
    2 	    20 	 0.04167 	 0.05340 	 ~...
    9 	    21 	 0.05459 	 0.05347 	 ~...
   12 	    22 	 0.06989 	 0.05358 	 ~...
   84 	    23 	 0.16823 	 0.05361 	 MISS
   50 	    24 	 0.14768 	 0.05361 	 m..s
   27 	    25 	 0.14699 	 0.05379 	 m..s
    8 	    26 	 0.05381 	 0.05400 	 ~...
   81 	    27 	 0.16780 	 0.05412 	 MISS
   71 	    28 	 0.15431 	 0.05414 	 MISS
   70 	    29 	 0.15358 	 0.05418 	 m..s
   27 	    30 	 0.14699 	 0.05439 	 m..s
   27 	    31 	 0.14699 	 0.05451 	 m..s
   80 	    32 	 0.16474 	 0.05465 	 MISS
   54 	    33 	 0.14787 	 0.05502 	 m..s
   27 	    34 	 0.14699 	 0.05536 	 m..s
    5 	    35 	 0.04677 	 0.05552 	 ~...
   62 	    36 	 0.14950 	 0.05574 	 m..s
   79 	    37 	 0.16472 	 0.05585 	 MISS
   27 	    38 	 0.14699 	 0.05664 	 m..s
   24 	    39 	 0.14427 	 0.05681 	 m..s
   27 	    40 	 0.14699 	 0.05688 	 m..s
   86 	    41 	 0.17206 	 0.05707 	 MISS
   87 	    42 	 0.17214 	 0.05742 	 MISS
   60 	    43 	 0.14882 	 0.05746 	 m..s
   72 	    44 	 0.15453 	 0.05770 	 m..s
   69 	    45 	 0.15259 	 0.05795 	 m..s
   15 	    46 	 0.11343 	 0.05797 	 m..s
   78 	    47 	 0.15939 	 0.05804 	 MISS
   18 	    48 	 0.12972 	 0.05853 	 m..s
   65 	    49 	 0.15100 	 0.05888 	 m..s
   57 	    50 	 0.14814 	 0.06018 	 m..s
    3 	    51 	 0.04451 	 0.06025 	 ~...
   23 	    52 	 0.14407 	 0.06090 	 m..s
    7 	    53 	 0.05094 	 0.06101 	 ~...
   74 	    54 	 0.15538 	 0.06102 	 m..s
   27 	    55 	 0.14699 	 0.06172 	 m..s
   75 	    56 	 0.15664 	 0.06399 	 m..s
   27 	    57 	 0.14699 	 0.06470 	 m..s
   26 	    58 	 0.14674 	 0.20275 	 m..s
   59 	    59 	 0.14843 	 0.20531 	 m..s
   27 	    60 	 0.14699 	 0.21711 	 m..s
   27 	    61 	 0.14699 	 0.21752 	 m..s
   68 	    62 	 0.15165 	 0.21900 	 m..s
   27 	    63 	 0.14699 	 0.22190 	 m..s
   27 	    64 	 0.14699 	 0.22306 	 m..s
   27 	    65 	 0.14699 	 0.22524 	 m..s
   55 	    66 	 0.14788 	 0.22808 	 m..s
   83 	    67 	 0.16812 	 0.23064 	 m..s
   27 	    68 	 0.14699 	 0.23155 	 m..s
   27 	    69 	 0.14699 	 0.23238 	 m..s
   27 	    70 	 0.14699 	 0.23467 	 m..s
   51 	    71 	 0.14773 	 0.23700 	 m..s
   19 	    72 	 0.13003 	 0.23816 	 MISS
   63 	    73 	 0.14952 	 0.24060 	 m..s
   27 	    74 	 0.14699 	 0.24098 	 m..s
   73 	    75 	 0.15484 	 0.24474 	 m..s
   67 	    76 	 0.15126 	 0.24722 	 m..s
   21 	    77 	 0.14154 	 0.24826 	 MISS
   16 	    78 	 0.11675 	 0.24880 	 MISS
   66 	    79 	 0.15114 	 0.25213 	 MISS
   61 	    80 	 0.14896 	 0.25655 	 MISS
   22 	    81 	 0.14216 	 0.25748 	 MISS
   27 	    82 	 0.14699 	 0.25853 	 MISS
   27 	    83 	 0.14699 	 0.26206 	 MISS
   27 	    84 	 0.14699 	 0.26303 	 MISS
   58 	    85 	 0.14838 	 0.26399 	 MISS
   64 	    86 	 0.15098 	 0.26534 	 MISS
   27 	    87 	 0.14699 	 0.26716 	 MISS
   89 	    88 	 0.23574 	 0.30657 	 m..s
  103 	    89 	 0.41980 	 0.34551 	 m..s
   95 	    90 	 0.40165 	 0.35476 	 m..s
  102 	    91 	 0.41912 	 0.36692 	 m..s
   88 	    92 	 0.22944 	 0.36815 	 MISS
   97 	    93 	 0.40487 	 0.37832 	 ~...
  101 	    94 	 0.41863 	 0.37923 	 m..s
   91 	    95 	 0.39557 	 0.37994 	 ~...
   90 	    96 	 0.38802 	 0.38768 	 ~...
   96 	    97 	 0.40219 	 0.39474 	 ~...
   92 	    98 	 0.39715 	 0.40889 	 ~...
   94 	    99 	 0.40024 	 0.41083 	 ~...
   98 	   100 	 0.40525 	 0.41130 	 ~...
   99 	   101 	 0.40780 	 0.41259 	 ~...
  111 	   102 	 0.44750 	 0.41901 	 ~...
   93 	   103 	 0.40000 	 0.42435 	 ~...
  112 	   104 	 0.44907 	 0.43023 	 ~...
  113 	   105 	 0.45054 	 0.43351 	 ~...
  116 	   106 	 0.45600 	 0.43393 	 ~...
  120 	   107 	 0.47856 	 0.43603 	 m..s
  118 	   108 	 0.46392 	 0.43923 	 ~...
  109 	   109 	 0.44315 	 0.43946 	 ~...
  114 	   110 	 0.45210 	 0.44003 	 ~...
  100 	   111 	 0.41753 	 0.44133 	 ~...
  108 	   112 	 0.44202 	 0.44234 	 ~...
  117 	   113 	 0.45718 	 0.44314 	 ~...
  119 	   114 	 0.46768 	 0.44598 	 ~...
  106 	   115 	 0.43269 	 0.44604 	 ~...
  107 	   116 	 0.43823 	 0.44625 	 ~...
  104 	   117 	 0.42508 	 0.44951 	 ~...
  115 	   118 	 0.45395 	 0.45134 	 ~...
  110 	   119 	 0.44357 	 0.46264 	 ~...
  105 	   120 	 0.43037 	 0.46726 	 m..s
==========================================
r_mrr = 0.8669744729995728
r2_mrr = 0.7450608611106873
spearmanr_mrr@5 = 0.9461647272109985
spearmanr_mrr@10 = 0.9715569615364075
spearmanr_mrr@50 = 0.9841685891151428
spearmanr_mrr@100 = 0.8807864785194397
spearmanr_mrr@All = 0.9016018509864807
==========================================
test time: 0.405
Done Testing dataset Kinships
total time taken: 187.151997089386
training time taken: 180.2167797088623
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8670)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7451)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9462)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9716)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9842)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8808)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9016)}}, 'test_loss': {'DistMult': {'Kinships': 7.704606708359279}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 6157148552337508
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [461, 1151, 745, 655, 689, 447, 754, 1140, 653, 691, 631, 510, 527, 123, 704, 196, 1051, 377, 137, 1027, 672, 981, 198, 928, 1076, 81, 1085, 499, 1071, 925, 215, 860, 199, 651, 238, 944, 1160, 1026, 122, 954, 426, 1074, 213, 643, 821, 617, 1007, 281, 52, 747, 540, 544, 398, 249, 729, 416, 248, 216, 712, 474, 1134, 403, 911, 1182, 378, 739, 563, 306, 9, 484, 25, 124, 721, 857, 325, 1045, 702, 1165, 262, 1000, 898, 1192, 331, 280, 1036, 197, 1002, 918, 775, 1022, 705, 824, 355, 440, 6, 1070, 568, 60, 1052, 17, 768, 688, 1014, 1193, 489, 927, 781, 21, 869, 1078, 1171, 1167, 84, 567, 1159, 10, 33, 517, 538, 562, 906]
valid_ids (0): []
train_ids (1094): [1212, 289, 465, 236, 539, 592, 476, 243, 1062, 376, 276, 985, 962, 131, 1046, 1099, 140, 120, 365, 518, 706, 256, 723, 916, 995, 233, 532, 485, 318, 173, 0, 1179, 316, 397, 301, 1149, 175, 935, 576, 336, 91, 1166, 1094, 208, 480, 228, 763, 362, 624, 176, 473, 941, 969, 1198, 727, 838, 434, 1132, 1173, 542, 939, 1141, 1073, 1180, 564, 346, 1133, 1177, 913, 503, 107, 324, 839, 335, 1121, 493, 658, 580, 410, 246, 622, 895, 896, 265, 585, 467, 218, 508, 468, 843, 291, 1169, 350, 810, 61, 194, 639, 1191, 610, 322, 558, 652, 1195, 1164, 1131, 1033, 253, 628, 1185, 613, 247, 951, 159, 149, 433, 386, 823, 687, 412, 383, 504, 345, 1048, 222, 675, 487, 28, 908, 597, 1112, 746, 51, 909, 574, 674, 988, 699, 885, 1013, 118, 830, 369, 603, 219, 251, 1058, 531, 141, 1100, 1049, 758, 615, 195, 514, 250, 96, 894, 1186, 638, 300, 186, 1196, 840, 43, 817, 997, 36, 1130, 591, 812, 1043, 390, 919, 1200, 636, 446, 227, 498, 549, 171, 1162, 1135, 1189, 978, 1020, 99, 1136, 179, 464, 224, 308, 214, 442, 646, 1095, 1075, 828, 922, 799, 311, 633, 816, 825, 14, 22, 333, 849, 1090, 695, 833, 303, 697, 996, 707, 694, 522, 725, 12, 555, 525, 798, 806, 845, 73, 682, 654, 413, 1029, 1003, 621, 1152, 1109, 74, 770, 483, 35, 192, 618, 943, 1201, 548, 1187, 590, 854, 328, 145, 418, 802, 663, 1143, 1154, 183, 871, 642, 1142, 815, 1064, 327, 762, 1213, 423, 989, 551, 842, 665, 559, 41, 818, 611, 268, 891, 856, 181, 92, 637, 1103, 931, 286, 600, 341, 874, 1088, 62, 992, 809, 1093, 711, 866, 670, 1108, 797, 430, 1042, 69, 190, 737, 2, 143, 819, 1044, 582, 156, 160, 556, 698, 88, 728, 1032, 589, 1053, 226, 482, 38, 3, 696, 44, 1050, 547, 1174, 760, 307, 204, 112, 947, 506, 488, 1125, 632, 1190, 1101, 769, 20, 220, 945, 732, 822, 800, 391, 202, 952, 528, 279, 462, 870, 910, 1056, 258, 625, 701, 379, 740, 835, 964, 1117, 623, 382, 210, 949, 163, 406, 172, 1008, 553, 374, 557, 859, 298, 387, 608, 97, 594, 892, 667, 310, 1001, 715, 1170, 805, 960, 101, 209, 212, 776, 385, 130, 836, 515, 782, 1104, 450, 546, 27, 720, 753, 332, 139, 774, 417, 620, 785, 719, 779, 647, 358, 76, 933, 57, 1161, 887, 95, 765, 363, 158, 321, 834, 142, 1176, 545, 89, 968, 505, 135, 569, 889, 678, 463, 972, 873, 1006, 449, 1089, 102, 755, 428, 109, 133, 448, 1158, 366, 221, 552, 154, 673, 861, 491, 437, 263, 957, 241, 645, 1205, 852, 1054, 169, 207, 152, 104, 855, 1081, 686, 534, 7, 37, 578, 629, 472, 924, 607, 255, 877, 930, 876, 1031, 1037, 370, 1097, 1175, 605, 305, 445, 151, 312, 1040, 1098, 296, 994, 1012, 264, 693, 513, 477, 736, 486, 541, 40, 188, 973, 150, 231, 656, 260, 319, 1123, 79, 273, 676, 90, 648, 1069, 535, 804, 1015, 864, 167, 343, 177, 184, 759, 337, 659, 767, 259, 1066, 93, 731, 1178, 826, 794, 381, 404, 875, 113, 294, 752, 103, 814, 1111, 347, 537, 901, 1146, 1148, 1019, 614, 115, 352, 903, 1059, 166, 1197, 980, 235, 1030, 459, 217, 201, 1113, 297, 1153, 436, 117, 583, 66, 974, 902, 1087, 1110, 764, 269, 827, 971, 900, 1206, 134, 271, 274, 356, 703, 1, 1055, 612, 1168, 126, 929, 965, 1194, 1139, 444, 677, 384, 844, 791, 441, 396, 496, 1183, 389, 47, 23, 714, 1156, 257, 86, 1004, 455, 106, 1009, 1082, 1207, 394, 185, 923, 533, 886, 1079, 1010, 1086, 148, 1202, 494, 664, 329, 986, 627, 427, 439, 334, 948, 967, 888, 560, 75, 979, 914, 921, 717, 868, 174, 1118, 1096, 1188, 530, 741, 203, 30, 657, 58, 744, 1063, 966, 1210, 554, 272, 1077, 650, 982, 1028, 666, 983, 1204, 526, 1091, 111, 419, 1083, 932, 671, 561, 283, 573, 1181, 1129, 65, 180, 915, 851, 1126, 127, 116, 883, 309, 48, 1214, 619, 904, 681, 644, 1038, 285, 708, 593, 63, 626, 566, 351, 1145, 880, 315, 435, 338, 795, 722, 1211, 119, 270, 1025, 178, 684, 453, 354, 261, 314, 277, 225, 950, 425, 460, 862, 187, 1155, 1061, 409, 451, 232, 878, 771, 414, 1068, 1065, 371, 516, 32, 344, 4, 572, 847, 718, 683, 466, 304, 793, 478, 523, 479, 738, 1120, 407, 372, 586, 811, 481, 780, 831, 596, 15, 543, 938, 223, 846, 588, 302, 602, 278, 39, 205, 565, 164, 71, 500, 108, 920, 599, 807, 934, 54, 1209, 5, 1172, 367, 1163, 606, 733, 716, 373, 411, 709, 942, 520, 146, 778, 239, 400, 813, 1128, 136, 266, 211, 431, 955, 293, 13, 8, 317, 45, 457, 1060, 832, 288, 59, 340, 193, 157, 640, 368, 863, 495, 469, 679, 867, 42, 83, 240, 275, 342, 899, 155, 422, 31, 16, 926, 787, 395, 19, 990, 550, 501, 245, 1017, 710, 897, 443, 206, 796, 1016, 524, 380, 1057, 353, 730, 85, 529, 168, 144, 668, 128, 595, 244, 571, 584, 700, 907, 1021, 1092, 78, 641, 890, 1184, 1124, 841, 749, 375, 242, 536, 64, 53, 320, 1041, 46, 323, 977, 1023, 287, 72, 77, 121, 837, 349, 601, 748, 165, 1018, 853, 751, 1137, 507, 685, 937, 454, 200, 230, 415, 497, 575, 429, 829, 991, 326, 1127, 786, 777, 742, 401, 49, 267, 56, 884, 129, 34, 879, 132, 1157, 912, 850, 1034, 295, 872, 734, 392, 191, 458, 432, 959, 348, 773, 808, 313, 290, 987, 70, 865, 940, 1084, 452, 976, 635, 1105, 772, 361, 284, 254, 1147, 55, 399, 471, 357, 512, 456, 519, 669, 11, 609, 364, 750, 1047, 1005, 604, 713, 690, 125, 50, 661, 587, 660, 237, 956, 110, 388, 788, 946, 502, 958, 420, 511, 87, 438, 475, 735, 1138, 803, 80, 162, 1122, 1199, 790, 993, 402, 470, 67, 299, 234, 1102, 726, 252, 147, 998, 1144, 1119, 1072, 509, 858, 792, 936, 521, 492, 359, 917, 801, 24, 882, 68, 649, 598, 421, 393, 905, 292, 161, 581, 784, 229, 1115, 724, 1039, 360, 692, 761, 579, 26, 1116, 182, 616, 94, 138, 405, 100, 662, 408, 105, 1080, 783, 961, 1106, 282, 170, 1107, 634, 680, 570, 29, 330, 577, 881, 424, 953, 339, 1203, 970, 153, 1067, 848, 1035, 114, 757, 984, 1150, 82, 1114, 999, 18, 98, 766, 1024, 820, 789, 743, 1208, 756, 1011, 893, 630, 189, 963, 975, 490]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9636499047713794
the save name prefix for this run is:  chkpt-ID_9636499047713794_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 738
rank avg (pred): 0.501 +- 0.001
mrr vals (pred, true): 0.019, 0.413
batch losses (mrrl, rdl): 0.0, 0.0042708963

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 414
rank avg (pred): 0.357 +- 0.170
mrr vals (pred, true): 0.063, 0.062
batch losses (mrrl, rdl): 0.0, 0.0002439467

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 202
rank avg (pred): 0.261 +- 0.201
mrr vals (pred, true): 0.243, 0.058
batch losses (mrrl, rdl): 0.0, 0.0005991101

Epoch over!
epoch time: 11.855

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 779
rank avg (pred): 0.407 +- 0.288
mrr vals (pred, true): 0.217, 0.050
batch losses (mrrl, rdl): 0.0, 4.15191e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 73
rank avg (pred): 0.044 +- 0.038
mrr vals (pred, true): 0.442, 0.447
batch losses (mrrl, rdl): 0.0, 3.9107e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 974
rank avg (pred): 0.052 +- 0.047
mrr vals (pred, true): 0.453, 0.439
batch losses (mrrl, rdl): 0.0, 5.924e-07

Epoch over!
epoch time: 11.9

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 842
rank avg (pred): 0.371 +- 0.290
mrr vals (pred, true): 0.290, 0.050
batch losses (mrrl, rdl): 0.0, 9.44935e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 670
rank avg (pred): 0.245 +- 0.221
mrr vals (pred, true): 0.351, 0.050
batch losses (mrrl, rdl): 0.0, 0.0007403364

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 808
rank avg (pred): 0.435 +- 0.320
mrr vals (pred, true): 0.262, 0.060
batch losses (mrrl, rdl): 0.0, 2.67128e-05

Epoch over!
epoch time: 11.742

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 924
rank avg (pred): 0.518 +- 0.348
mrr vals (pred, true): 0.217, 0.046
batch losses (mrrl, rdl): 0.0, 9.30612e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 433
rank avg (pred): 0.221 +- 0.212
mrr vals (pred, true): 0.383, 0.054
batch losses (mrrl, rdl): 0.0, 0.0008319033

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1059
rank avg (pred): 0.088 +- 0.080
mrr vals (pred, true): 0.424, 0.463
batch losses (mrrl, rdl): 0.0, 3.15104e-05

Epoch over!
epoch time: 11.745

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 641
rank avg (pred): 0.267 +- 0.230
mrr vals (pred, true): 0.326, 0.246
batch losses (mrrl, rdl): 0.0, 0.0008188299

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 87
rank avg (pred): 0.278 +- 0.227
mrr vals (pred, true): 0.285, 0.240
batch losses (mrrl, rdl): 0.0, 0.0008935655

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 782
rank avg (pred): 0.414 +- 0.309
mrr vals (pred, true): 0.281, 0.053
batch losses (mrrl, rdl): 0.0, 2.75577e-05

Epoch over!
epoch time: 12.045

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 791
rank avg (pred): 0.419 +- 0.302
mrr vals (pred, true): 0.257, 0.053
batch losses (mrrl, rdl): 0.4269049168, 1.4923e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 64
rank avg (pred): 0.030 +- 0.024
mrr vals (pred, true): 0.454, 0.463
batch losses (mrrl, rdl): 0.000828364, 1.11103e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 219
rank avg (pred): 0.330 +- 0.239
mrr vals (pred, true): 0.212, 0.052
batch losses (mrrl, rdl): 0.2616500258, 0.0002086361

Epoch over!
epoch time: 12.398

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 776
rank avg (pred): 0.614 +- 0.235
mrr vals (pred, true): 0.074, 0.060
batch losses (mrrl, rdl): 0.0059005329, 0.0007426608

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1112
rank avg (pred): 0.380 +- 0.195
mrr vals (pred, true): 0.122, 0.051
batch losses (mrrl, rdl): 0.0511962362, 9.57482e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 577
rank avg (pred): 0.339 +- 0.213
mrr vals (pred, true): 0.168, 0.228
batch losses (mrrl, rdl): 0.0360574424, 0.0013410666

Epoch over!
epoch time: 12.011

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 733
rank avg (pred): 0.188 +- 0.140
mrr vals (pred, true): 0.243, 0.148
batch losses (mrrl, rdl): 0.0910822451, 2.4281e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 490
rank avg (pred): 0.043 +- 0.036
mrr vals (pred, true): 0.420, 0.382
batch losses (mrrl, rdl): 0.0148100276, 8.4514e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 48
rank avg (pred): 0.042 +- 0.035
mrr vals (pred, true): 0.439, 0.429
batch losses (mrrl, rdl): 0.0010538354, 6.2325e-06

Epoch over!
epoch time: 12.383

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1117
rank avg (pred): 0.341 +- 0.192
mrr vals (pred, true): 0.142, 0.056
batch losses (mrrl, rdl): 0.0846710503, 0.0002559131

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1077
rank avg (pred): 0.024 +- 0.021
mrr vals (pred, true): 0.512, 0.455
batch losses (mrrl, rdl): 0.0331714451, 1.93708e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1004
rank avg (pred): 0.312 +- 0.216
mrr vals (pred, true): 0.178, 0.247
batch losses (mrrl, rdl): 0.0475778095, 0.0010216496

Epoch over!
epoch time: 12.175

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 822
rank avg (pred): 0.076 +- 0.066
mrr vals (pred, true): 0.403, 0.397
batch losses (mrrl, rdl): 0.0003742882, 6.7071e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 598
rank avg (pred): 0.348 +- 0.184
mrr vals (pred, true): 0.127, 0.231
batch losses (mrrl, rdl): 0.1092344448, 0.0013769122

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 212
rank avg (pred): 0.341 +- 0.178
mrr vals (pred, true): 0.128, 0.052
batch losses (mrrl, rdl): 0.0608498678, 0.0002487434

Epoch over!
epoch time: 12.33

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 854
rank avg (pred): 0.490 +- 0.244
mrr vals (pred, true): 0.059, 0.053
batch losses (mrrl, rdl): 0.0007758457, 4.6077e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 494
rank avg (pred): 0.133 +- 0.120
mrr vals (pred, true): 0.386, 0.362
batch losses (mrrl, rdl): 0.0059386855, 0.0001223836

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 960
rank avg (pred): 0.478 +- 0.250
mrr vals (pred, true): 0.069, 0.053
batch losses (mrrl, rdl): 0.0035950402, 3.8691e-06

Epoch over!
epoch time: 12.384

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 309
rank avg (pred): 0.303 +- 0.261
mrr vals (pred, true): 0.371, 0.440
batch losses (mrrl, rdl): 0.0475952029, 0.0015725706

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 169
rank avg (pred): 0.319 +- 0.183
mrr vals (pred, true): 0.149, 0.054
batch losses (mrrl, rdl): 0.0976663828, 0.00034185

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 573
rank avg (pred): 0.310 +- 0.155
mrr vals (pred, true): 0.139, 0.257
batch losses (mrrl, rdl): 0.1370598972, 0.0010200123

Epoch over!
epoch time: 11.924

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 603
rank avg (pred): 0.325 +- 0.166
mrr vals (pred, true): 0.137, 0.224
batch losses (mrrl, rdl): 0.0750419199, 0.0009584891

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 427
rank avg (pred): 0.316 +- 0.184
mrr vals (pred, true): 0.156, 0.051
batch losses (mrrl, rdl): 0.1115034372, 0.0003971912

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 3
rank avg (pred): 0.150 +- 0.136
mrr vals (pred, true): 0.440, 0.432
batch losses (mrrl, rdl): 0.0005285086, 0.0002443437

Epoch over!
epoch time: 12.022

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 915
rank avg (pred): 0.265 +- 0.209
mrr vals (pred, true): 0.230, 0.082
batch losses (mrrl, rdl): 0.3229138851, 4.71321e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 418
rank avg (pred): 0.275 +- 0.171
mrr vals (pred, true): 0.187, 0.056
batch losses (mrrl, rdl): 0.1875716448, 0.0006769876

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 471
rank avg (pred): 0.277 +- 0.143
mrr vals (pred, true): 0.152, 0.051
batch losses (mrrl, rdl): 0.1050432399, 0.0006999975

Epoch over!
epoch time: 12.022

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 76
rank avg (pred): 0.069 +- 0.059
mrr vals (pred, true): 0.439, 0.444
batch losses (mrrl, rdl): 0.0002472563, 4.7877e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 263
rank avg (pred): 0.080 +- 0.071
mrr vals (pred, true): 0.446, 0.436
batch losses (mrrl, rdl): 0.0008903437, 1.53573e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1068
rank avg (pred): 0.097 +- 0.086
mrr vals (pred, true): 0.449, 0.446
batch losses (mrrl, rdl): 7.25577e-05, 4.50261e-05

Epoch over!
epoch time: 12.029

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.270 +- 0.143
mrr vals (pred, true): 0.158, 0.056

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   12 	     0 	 0.08667 	 0.02078 	 m..s
   16 	     1 	 0.14994 	 0.03579 	 MISS
    0 	     2 	 0.02928 	 0.03867 	 ~...
    4 	     3 	 0.05340 	 0.04321 	 ~...
    3 	     4 	 0.05319 	 0.04684 	 ~...
    1 	     5 	 0.04261 	 0.04720 	 ~...
    8 	     6 	 0.06992 	 0.04779 	 ~...
   19 	     7 	 0.15778 	 0.04935 	 MISS
   19 	     8 	 0.15778 	 0.04982 	 MISS
   19 	     9 	 0.15778 	 0.04992 	 MISS
   66 	    10 	 0.15889 	 0.05013 	 MISS
   19 	    11 	 0.15778 	 0.05042 	 MISS
    9 	    12 	 0.07281 	 0.05075 	 ~...
    5 	    13 	 0.06760 	 0.05131 	 ~...
    6 	    14 	 0.06858 	 0.05174 	 ~...
   19 	    15 	 0.15778 	 0.05204 	 MISS
   19 	    16 	 0.15778 	 0.05230 	 MISS
   17 	    17 	 0.15593 	 0.05234 	 MISS
   69 	    18 	 0.16187 	 0.05250 	 MISS
   19 	    19 	 0.15778 	 0.05279 	 MISS
   19 	    20 	 0.15778 	 0.05297 	 MISS
   68 	    21 	 0.16106 	 0.05299 	 MISS
   19 	    22 	 0.15778 	 0.05314 	 MISS
   19 	    23 	 0.15778 	 0.05316 	 MISS
   19 	    24 	 0.15778 	 0.05324 	 MISS
   11 	    25 	 0.08626 	 0.05374 	 m..s
   19 	    26 	 0.15778 	 0.05383 	 MISS
   19 	    27 	 0.15778 	 0.05401 	 MISS
   76 	    28 	 0.16745 	 0.05412 	 MISS
    2 	    29 	 0.04843 	 0.05460 	 ~...
   19 	    30 	 0.15778 	 0.05474 	 MISS
   19 	    31 	 0.15778 	 0.05475 	 MISS
   70 	    32 	 0.16190 	 0.05482 	 MISS
   61 	    33 	 0.15798 	 0.05502 	 MISS
   62 	    34 	 0.15802 	 0.05513 	 MISS
   19 	    35 	 0.15778 	 0.05543 	 MISS
   19 	    36 	 0.15778 	 0.05567 	 MISS
   19 	    37 	 0.15778 	 0.05574 	 MISS
   19 	    38 	 0.15778 	 0.05589 	 MISS
   10 	    39 	 0.08170 	 0.05593 	 ~...
   19 	    40 	 0.15778 	 0.05602 	 MISS
   19 	    41 	 0.15778 	 0.05605 	 MISS
   19 	    42 	 0.15778 	 0.05616 	 MISS
    7 	    43 	 0.06901 	 0.05719 	 ~...
   71 	    44 	 0.16203 	 0.05780 	 MISS
   15 	    45 	 0.14152 	 0.05797 	 m..s
   19 	    46 	 0.15778 	 0.05807 	 m..s
   73 	    47 	 0.16382 	 0.05828 	 MISS
   19 	    48 	 0.15778 	 0.06230 	 m..s
   79 	    49 	 0.29314 	 0.10886 	 MISS
   78 	    50 	 0.29034 	 0.19588 	 m..s
   19 	    51 	 0.15778 	 0.20349 	 m..s
   19 	    52 	 0.15778 	 0.20396 	 m..s
   19 	    53 	 0.15778 	 0.20531 	 m..s
   67 	    54 	 0.16104 	 0.20919 	 m..s
   19 	    55 	 0.15778 	 0.21466 	 m..s
   19 	    56 	 0.15778 	 0.21948 	 m..s
   19 	    57 	 0.15778 	 0.21967 	 m..s
   19 	    58 	 0.15778 	 0.22528 	 m..s
   75 	    59 	 0.16491 	 0.22771 	 m..s
   19 	    60 	 0.15778 	 0.22911 	 m..s
   19 	    61 	 0.15778 	 0.23128 	 m..s
   19 	    62 	 0.15778 	 0.23128 	 m..s
   14 	    63 	 0.13443 	 0.23134 	 m..s
   74 	    64 	 0.16483 	 0.23187 	 m..s
   63 	    65 	 0.15804 	 0.23496 	 m..s
   19 	    66 	 0.15778 	 0.23732 	 m..s
   19 	    67 	 0.15778 	 0.23928 	 m..s
   19 	    68 	 0.15778 	 0.24262 	 m..s
   19 	    69 	 0.15778 	 0.24272 	 m..s
   64 	    70 	 0.15822 	 0.24722 	 m..s
   72 	    71 	 0.16224 	 0.25018 	 m..s
   60 	    72 	 0.15781 	 0.25366 	 m..s
   13 	    73 	 0.12112 	 0.25379 	 MISS
   19 	    74 	 0.15778 	 0.25815 	 MISS
   18 	    75 	 0.15677 	 0.26135 	 MISS
   19 	    76 	 0.15778 	 0.26501 	 MISS
   19 	    77 	 0.15778 	 0.26716 	 MISS
   65 	    78 	 0.15888 	 0.27073 	 MISS
   77 	    79 	 0.24512 	 0.30334 	 m..s
   80 	    80 	 0.33652 	 0.34172 	 ~...
   92 	    81 	 0.40778 	 0.36122 	 m..s
   81 	    82 	 0.39219 	 0.36813 	 ~...
   96 	    83 	 0.41132 	 0.37131 	 m..s
   87 	    84 	 0.40280 	 0.37166 	 m..s
   83 	    85 	 0.39673 	 0.37175 	 ~...
   86 	    86 	 0.40061 	 0.37356 	 ~...
   89 	    87 	 0.40739 	 0.37473 	 m..s
   90 	    88 	 0.40741 	 0.37543 	 m..s
   88 	    89 	 0.40513 	 0.39224 	 ~...
   91 	    90 	 0.40768 	 0.39255 	 ~...
   93 	    91 	 0.40845 	 0.39438 	 ~...
   82 	    92 	 0.39555 	 0.39489 	 ~...
   95 	    93 	 0.41128 	 0.39788 	 ~...
   85 	    94 	 0.40056 	 0.40923 	 ~...
   99 	    95 	 0.43225 	 0.41107 	 ~...
   97 	    96 	 0.41252 	 0.41130 	 ~...
   84 	    97 	 0.40035 	 0.41169 	 ~...
   94 	    98 	 0.41105 	 0.41303 	 ~...
   98 	    99 	 0.43090 	 0.42314 	 ~...
  110 	   100 	 0.44970 	 0.42698 	 ~...
  103 	   101 	 0.44065 	 0.42728 	 ~...
  114 	   102 	 0.45702 	 0.42791 	 ~...
  105 	   103 	 0.44092 	 0.43261 	 ~...
  100 	   104 	 0.43902 	 0.43275 	 ~...
  107 	   105 	 0.44608 	 0.43741 	 ~...
  109 	   106 	 0.44681 	 0.43849 	 ~...
  108 	   107 	 0.44653 	 0.43964 	 ~...
  116 	   108 	 0.46918 	 0.44352 	 ~...
  111 	   109 	 0.45169 	 0.44410 	 ~...
  101 	   110 	 0.43958 	 0.44414 	 ~...
  104 	   111 	 0.44075 	 0.44425 	 ~...
  113 	   112 	 0.45600 	 0.44455 	 ~...
  118 	   113 	 0.47242 	 0.44706 	 ~...
  112 	   114 	 0.45590 	 0.44770 	 ~...
  106 	   115 	 0.44200 	 0.44981 	 ~...
  115 	   116 	 0.46487 	 0.45046 	 ~...
  119 	   117 	 0.47245 	 0.45100 	 ~...
  117 	   118 	 0.46941 	 0.45194 	 ~...
  102 	   119 	 0.44010 	 0.45465 	 ~...
  120 	   120 	 0.47463 	 0.45760 	 ~...
==========================================
r_mrr = 0.8957081437110901
r2_mrr = 0.7851170897483826
spearmanr_mrr@5 = 0.9121307134628296
spearmanr_mrr@10 = 0.9307416677474976
spearmanr_mrr@50 = 0.9657239317893982
spearmanr_mrr@100 = 0.8995801210403442
spearmanr_mrr@All = 0.9179210066795349
==========================================
test time: 0.738
Done Testing dataset Kinships
total time taken: 189.14412474632263
training time taken: 181.7629315853119
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8957)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7851)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9121)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9307)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9657)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8996)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9179)}}, 'test_loss': {'DistMult': {'Kinships': 6.84260300265305}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 4621304194246565
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [785, 1090, 60, 1127, 181, 612, 681, 70, 95, 1035, 720, 365, 643, 832, 935, 445, 391, 192, 983, 211, 1044, 604, 1171, 455, 875, 963, 637, 930, 1214, 901, 26, 1110, 504, 49, 1052, 484, 187, 372, 442, 179, 1049, 704, 1109, 629, 713, 255, 305, 1096, 805, 94, 837, 924, 244, 352, 56, 91, 534, 490, 960, 562, 953, 256, 625, 863, 1179, 480, 336, 765, 198, 556, 111, 771, 979, 974, 1108, 666, 1141, 467, 874, 748, 921, 1006, 133, 197, 632, 1012, 499, 932, 1071, 344, 191, 900, 552, 216, 972, 812, 1203, 119, 368, 335, 259, 544, 155, 396, 1060, 1069, 170, 193, 33, 1080, 9, 491, 428, 16, 184, 497, 645, 110, 672, 280, 1038]
valid_ids (0): []
train_ids (1094): [1002, 520, 728, 4, 112, 439, 833, 814, 618, 860, 636, 233, 297, 570, 324, 61, 517, 1200, 862, 320, 726, 53, 267, 686, 651, 703, 321, 715, 118, 742, 1021, 939, 716, 987, 648, 955, 652, 316, 1166, 376, 28, 298, 970, 1170, 12, 671, 760, 357, 262, 127, 587, 877, 63, 677, 403, 242, 898, 460, 196, 628, 1160, 471, 945, 413, 423, 1125, 740, 1158, 754, 880, 51, 1093, 503, 125, 1104, 857, 350, 64, 616, 1033, 1210, 478, 931, 1128, 241, 1190, 1173, 1199, 456, 190, 132, 876, 23, 816, 100, 710, 1068, 1204, 212, 400, 1087, 143, 554, 980, 157, 793, 234, 688, 917, 349, 370, 735, 1119, 992, 177, 355, 188, 565, 425, 801, 772, 724, 905, 270, 661, 576, 910, 1167, 543, 333, 1026, 58, 692, 340, 164, 301, 1151, 718, 328, 392, 257, 693, 856, 500, 285, 142, 165, 849, 1115, 828, 228, 701, 402, 887, 678, 360, 2, 1070, 904, 803, 751, 1001, 1092, 730, 1032, 437, 896, 123, 514, 535, 14, 1189, 548, 1123, 598, 1091, 238, 521, 865, 827, 1154, 382, 607, 727, 691, 844, 667, 773, 443, 1024, 419, 229, 343, 1000, 861, 501, 408, 135, 452, 1046, 753, 1034, 1113, 555, 258, 1121, 946, 202, 1043, 810, 302, 787, 1134, 537, 659, 83, 690, 226, 729, 733, 5, 362, 743, 454, 778, 545, 845, 1082, 398, 653, 568, 1106, 278, 951, 811, 22, 466, 487, 139, 796, 513, 140, 790, 920, 325, 182, 627, 81, 928, 32, 749, 231, 93, 783, 1019, 218, 1054, 39, 220, 882, 249, 1135, 683, 642, 42, 240, 43, 304, 567, 1163, 1099, 265, 154, 6, 1156, 999, 223, 731, 489, 289, 758, 410, 1076, 638, 55, 1029, 1027, 750, 989, 1097, 675, 558, 19, 994, 649, 1098, 136, 381, 1103, 1025, 871, 709, 872, 266, 462, 591, 1061, 44, 622, 434, 902, 213, 770, 746, 639, 640, 676, 650, 98, 75, 279, 1020, 1072, 86, 364, 848, 563, 912, 717, 571, 741, 103, 1102, 779, 829, 664, 797, 1150, 647, 294, 1051, 971, 283, 705, 464, 1036, 804, 1201, 474, 791, 475, 41, 243, 538, 1195, 878, 966, 152, 149, 371, 1062, 245, 909, 263, 996, 532, 852, 404, 137, 1015, 831, 699, 327, 359, 663, 855, 1172, 206, 407, 73, 1122, 172, 79, 1111, 1152, 588, 107, 163, 679, 1039, 1146, 405, 124, 582, 1011, 891, 919, 850, 236, 1066, 883, 511, 609, 1126, 732, 397, 502, 1013, 488, 892, 739, 67, 146, 834, 985, 208, 153, 766, 841, 899, 117, 313, 239, 941, 846, 401, 272, 318, 777, 62, 531, 134, 47, 174, 1153, 433, 395, 260, 10, 97, 505, 820, 200, 1009, 603, 988, 826, 696, 1048, 468, 818, 326, 385, 194, 299, 156, 424, 338, 224, 601, 312, 546, 203, 654, 431, 936, 367, 248, 291, 635, 600, 768, 626, 1074, 1053, 522, 387, 50, 1184, 269, 533, 261, 700, 903, 859, 209, 342, 390, 482, 1157, 662, 914, 998, 685, 300, 440, 755, 1145, 807, 674, 981, 416, 1144, 1007, 1088, 1116, 906, 160, 784, 351, 575, 354, 101, 281, 965, 519, 169, 725, 1081, 830, 331, 621, 293, 427, 762, 329, 623, 409, 374, 1185, 246, 120, 389, 615, 114, 252, 493, 1022, 54, 108, 1129, 1073, 895, 1124, 721, 1197, 232, 589, 290, 159, 961, 485, 737, 109, 967, 429, 633, 843, 1047, 1175, 84, 457, 126, 885, 795, 1107, 168, 578, 30, 130, 88, 559, 774, 864, 166, 1030, 1079, 553, 1089, 1014, 1064, 547, 470, 557, 449, 494, 524, 465, 509, 90, 995, 606, 1120, 913, 486, 1177, 610, 894, 453, 161, 45, 1114, 247, 761, 1174, 207, 1193, 1050, 377, 78, 923, 583, 836, 800, 426, 435, 492, 657, 958, 144, 947, 1085, 341, 162, 235, 46, 66, 148, 719, 682, 1202, 253, 1205, 210, 337, 145, 230, 916, 847, 776, 0, 1078, 96, 1100, 808, 375, 48, 1212, 890, 332, 711, 366, 420, 1065, 764, 254, 1147, 756, 436, 822, 68, 630, 527, 508, 788, 35, 937, 373, 65, 948, 602, 418, 798, 506, 838, 363, 1137, 1196, 323, 660, 1132, 498, 507, 215, 217, 964, 438, 978, 1213, 599, 167, 214, 706, 1056, 412, 512, 353, 879, 1182, 1136, 31, 881, 21, 824, 417, 221, 689, 89, 673, 594, 933, 668, 572, 461, 189, 1003, 379, 529, 282, 736, 817, 34, 430, 997, 934, 287, 813, 669, 646, 150, 141, 1040, 57, 451, 8, 80, 586, 292, 356, 1194, 712, 1028, 1055, 1140, 183, 1143, 178, 561, 631, 697, 1010, 617, 525, 542, 1207, 411, 929, 794, 339, 869, 528, 204, 694, 195, 85, 432, 311, 510, 707, 873, 515, 870, 1186, 317, 422, 954, 388, 968, 866, 446, 1117, 886, 665, 275, 1183, 227, 835, 199, 620, 472, 76, 539, 264, 71, 151, 481, 1178, 769, 180, 1005, 1211, 7, 477, 550, 1017, 819, 644, 399, 296, 1037, 1180, 113, 853, 977, 346, 1045, 1191, 131, 925, 969, 670, 634, 516, 450, 775, 483, 745, 786, 767, 823, 584, 944, 579, 908, 394, 911, 723, 1016, 1075, 976, 574, 752, 384, 893, 13, 59, 1181, 744, 369, 782, 448, 20, 698, 1004, 1008, 802, 868, 840, 611, 38, 815, 1176, 121, 361, 334, 176, 858, 943, 173, 271, 1165, 959, 286, 479, 92, 889, 1083, 496, 82, 102, 1162, 915, 1101, 115, 956, 447, 825, 577, 821, 406, 72, 122, 29, 1155, 77, 1059, 310, 476, 307, 458, 358, 596, 747, 597, 347, 421, 414, 780, 580, 128, 940, 473, 147, 684, 25, 36, 658, 990, 288, 982, 1094, 695, 763, 1133, 605, 984, 738, 15, 222, 295, 18, 1118, 308, 1149, 781, 87, 69, 927, 655, 581, 702, 806, 595, 759, 839, 27, 566, 185, 315, 348, 104, 734, 942, 592, 986, 809, 564, 792, 415, 129, 641, 938, 3, 1023, 303, 158, 1198, 17, 268, 888, 918, 624, 523, 495, 569, 1206, 273, 1105, 314, 251, 526, 319, 441, 1095, 799, 1084, 619, 378, 1168, 867, 277, 560, 991, 74, 897, 950, 1161, 907, 393, 383, 1057, 138, 593, 1208, 530, 722, 1164, 1042, 1131, 459, 40, 250, 205, 957, 613, 52, 276, 789, 1130, 1187, 1086, 1209, 309, 1138, 551, 1112, 1067, 949, 469, 1169, 585, 1018, 540, 757, 237, 175, 714, 973, 37, 284, 106, 518, 656, 219, 201, 975, 884, 330, 1188, 608, 274, 590, 962, 24, 463, 536, 922, 225, 1058, 851, 11, 116, 952, 842, 614, 1041, 1139, 708, 1142, 993, 687, 345, 1031, 444, 549, 541, 171, 380, 680, 306, 186, 1192, 1148, 573, 105, 1, 322, 386, 854, 99, 926, 1159, 1063, 1077]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9275023596512702
the save name prefix for this run is:  chkpt-ID_9275023596512702_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 105
rank avg (pred): 0.552 +- 0.004
mrr vals (pred, true): 0.017, 0.262
batch losses (mrrl, rdl): 0.0, 0.0046583056

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 313
rank avg (pred): 0.023 +- 0.019
mrr vals (pred, true): 0.464, 0.438
batch losses (mrrl, rdl): 0.0, 2.30356e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 436
rank avg (pred): 0.261 +- 0.213
mrr vals (pred, true): 0.214, 0.053
batch losses (mrrl, rdl): 0.0, 0.0006571633

Epoch over!
epoch time: 11.976

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 175
rank avg (pred): 0.297 +- 0.238
mrr vals (pred, true): 0.204, 0.052
batch losses (mrrl, rdl): 0.0, 0.0003852611

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 565
rank avg (pred): 0.086 +- 0.087
mrr vals (pred, true): 0.425, 0.370
batch losses (mrrl, rdl): 0.0, 1.38803e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 340
rank avg (pred): 0.244 +- 0.232
mrr vals (pred, true): 0.327, 0.245
batch losses (mrrl, rdl): 0.0, 0.0006235827

Epoch over!
epoch time: 11.797

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 392
rank avg (pred): 0.261 +- 0.235
mrr vals (pred, true): 0.288, 0.291
batch losses (mrrl, rdl): 0.0, 0.0008076603

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 911
rank avg (pred): 0.092 +- 0.089
mrr vals (pred, true): 0.399, 0.342
batch losses (mrrl, rdl): 0.0, 1.72359e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 968
rank avg (pred): 0.378 +- 0.314
mrr vals (pred, true): 0.284, 0.065
batch losses (mrrl, rdl): 0.0, 4.33341e-05

Epoch over!
epoch time: 11.842

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 236
rank avg (pred): 0.240 +- 0.234
mrr vals (pred, true): 0.341, 0.053
batch losses (mrrl, rdl): 0.0, 0.0007035884

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 237
rank avg (pred): 0.257 +- 0.238
mrr vals (pred, true): 0.326, 0.049
batch losses (mrrl, rdl): 0.0, 0.0006343347

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 512
rank avg (pred): 0.075 +- 0.079
mrr vals (pred, true): 0.470, 0.366
batch losses (mrrl, rdl): 0.0, 3.6764e-06

Epoch over!
epoch time: 11.63

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1193
rank avg (pred): 0.242 +- 0.236
mrr vals (pred, true): 0.357, 0.053
batch losses (mrrl, rdl): 0.0, 0.0006645627

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 933
rank avg (pred): 0.431 +- 0.301
mrr vals (pred, true): 0.206, 0.046
batch losses (mrrl, rdl): 0.0, 7.54309e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 45
rank avg (pred): 0.067 +- 0.076
mrr vals (pred, true): 0.545, 0.433
batch losses (mrrl, rdl): 0.0, 2.9974e-06

Epoch over!
epoch time: 11.691

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1188
rank avg (pred): 0.278 +- 0.247
mrr vals (pred, true): 0.304, 0.055
batch losses (mrrl, rdl): 0.643528223, 0.0004961094

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1195
rank avg (pred): 0.432 +- 0.192
mrr vals (pred, true): 0.080, 0.057
batch losses (mrrl, rdl): 0.0092967236, 3.28153e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1022
rank avg (pred): 0.319 +- 0.216
mrr vals (pred, true): 0.173, 0.254
batch losses (mrrl, rdl): 0.0644662604, 0.0012522469

Epoch over!
epoch time: 12.413

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 573
rank avg (pred): 0.339 +- 0.219
mrr vals (pred, true): 0.149, 0.257
batch losses (mrrl, rdl): 0.1151035726, 0.0013695402

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 750
rank avg (pred): 0.022 +- 0.019
mrr vals (pred, true): 0.510, 0.387
batch losses (mrrl, rdl): 0.1512430757, 3.70344e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 108
rank avg (pred): 0.341 +- 0.208
mrr vals (pred, true): 0.137, 0.232
batch losses (mrrl, rdl): 0.0890274271, 0.0012903963

Epoch over!
epoch time: 12.358

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 509
rank avg (pred): 0.114 +- 0.101
mrr vals (pred, true): 0.373, 0.367
batch losses (mrrl, rdl): 0.0003851858, 6.17058e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 998
rank avg (pred): 0.050 +- 0.045
mrr vals (pred, true): 0.471, 0.459
batch losses (mrrl, rdl): 0.0012780313, 7.34e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 926
rank avg (pred): 0.570 +- 0.165
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 6.47247e-05, 0.0001792856

Epoch over!
epoch time: 12.121

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 385
rank avg (pred): 0.348 +- 0.215
mrr vals (pred, true): 0.138, 0.254
batch losses (mrrl, rdl): 0.1347204894, 0.0014648488

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1165
rank avg (pred): 0.335 +- 0.202
mrr vals (pred, true): 0.144, 0.261
batch losses (mrrl, rdl): 0.1371913254, 0.0014195218

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 653
rank avg (pred): 0.325 +- 0.184
mrr vals (pred, true): 0.131, 0.050
batch losses (mrrl, rdl): 0.0658887997, 0.0003383897

Epoch over!
epoch time: 12.378

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1094
rank avg (pred): 0.311 +- 0.218
mrr vals (pred, true): 0.185, 0.260
batch losses (mrrl, rdl): 0.0571030937, 0.0011839776

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 449
rank avg (pred): 0.355 +- 0.191
mrr vals (pred, true): 0.116, 0.049
batch losses (mrrl, rdl): 0.0438912548, 0.0002398389

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 239
rank avg (pred): 0.323 +- 0.194
mrr vals (pred, true): 0.151, 0.053
batch losses (mrrl, rdl): 0.1029241681, 0.0002988346

Epoch over!
epoch time: 12.249

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 257
rank avg (pred): 0.264 +- 0.230
mrr vals (pred, true): 0.401, 0.434
batch losses (mrrl, rdl): 0.0110665699, 0.0011359648

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1195
rank avg (pred): 0.356 +- 0.186
mrr vals (pred, true): 0.120, 0.057
batch losses (mrrl, rdl): 0.0483290777, 0.000220647

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 419
rank avg (pred): 0.319 +- 0.201
mrr vals (pred, true): 0.164, 0.055
batch losses (mrrl, rdl): 0.1288791895, 0.0003432694

Epoch over!
epoch time: 11.97

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 895
rank avg (pred): 0.523 +- 0.251
mrr vals (pred, true): 0.110, 0.197
batch losses (mrrl, rdl): 0.0757046714, 0.0036596202

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1195
rank avg (pred): 0.338 +- 0.168
mrr vals (pred, true): 0.128, 0.057
batch losses (mrrl, rdl): 0.0606692284, 0.0003220663

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 262
rank avg (pred): 0.086 +- 0.076
mrr vals (pred, true): 0.441, 0.447
batch losses (mrrl, rdl): 0.0003610286, 2.74492e-05

Epoch over!
epoch time: 12.204

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1081
rank avg (pred): 0.328 +- 0.184
mrr vals (pred, true): 0.142, 0.228
batch losses (mrrl, rdl): 0.0748830736, 0.0011961219

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 269
rank avg (pred): 0.084 +- 0.074
mrr vals (pred, true): 0.435, 0.444
batch losses (mrrl, rdl): 0.0009705705, 2.27485e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 459
rank avg (pred): 0.336 +- 0.219
mrr vals (pred, true): 0.157, 0.057
batch losses (mrrl, rdl): 0.1141976938, 0.0002011697

Epoch over!
epoch time: 12.25

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1014
rank avg (pred): 0.299 +- 0.189
mrr vals (pred, true): 0.165, 0.271
batch losses (mrrl, rdl): 0.1108471602, 0.0010625536

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1102
rank avg (pred): 0.332 +- 0.197
mrr vals (pred, true): 0.144, 0.265
batch losses (mrrl, rdl): 0.1451384723, 0.0012676497

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1011
rank avg (pred): 0.280 +- 0.181
mrr vals (pred, true): 0.184, 0.262
batch losses (mrrl, rdl): 0.0609917901, 0.0008086965

Epoch over!
epoch time: 12.104

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 687
rank avg (pred): 0.303 +- 0.170
mrr vals (pred, true): 0.151, 0.058
batch losses (mrrl, rdl): 0.1028944477, 0.0004449415

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 563
rank avg (pred): 0.303 +- 0.263
mrr vals (pred, true): 0.408, 0.368
batch losses (mrrl, rdl): 0.0156304818, 0.001479247

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 71
rank avg (pred): 0.144 +- 0.125
mrr vals (pred, true): 0.425, 0.447
batch losses (mrrl, rdl): 0.0047650952, 0.0002097433

Epoch over!
epoch time: 12.335

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.677 +- 0.192
mrr vals (pred, true): 0.061, 0.054

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.05686 	 0.04238 	 ~...
    5 	     1 	 0.06022 	 0.04507 	 ~...
   11 	     2 	 0.06953 	 0.04509 	 ~...
    1 	     3 	 0.04988 	 0.04618 	 ~...
   71 	     4 	 0.17797 	 0.04803 	 MISS
    0 	     5 	 0.04840 	 0.04836 	 ~...
    4 	     6 	 0.05859 	 0.04846 	 ~...
   56 	     7 	 0.17046 	 0.04878 	 MISS
   63 	     8 	 0.17168 	 0.04884 	 MISS
   51 	     9 	 0.16913 	 0.04948 	 MISS
   39 	    10 	 0.16755 	 0.04950 	 MISS
   72 	    11 	 0.18030 	 0.04950 	 MISS
   12 	    12 	 0.07225 	 0.04955 	 ~...
   60 	    13 	 0.17082 	 0.04962 	 MISS
   47 	    14 	 0.16833 	 0.04980 	 MISS
   75 	    15 	 0.18094 	 0.05011 	 MISS
   43 	    16 	 0.16814 	 0.05088 	 MISS
   20 	    17 	 0.16254 	 0.05101 	 MISS
   66 	    18 	 0.17331 	 0.05112 	 MISS
    7 	    19 	 0.06324 	 0.05112 	 ~...
   17 	    20 	 0.16125 	 0.05113 	 MISS
   52 	    21 	 0.16949 	 0.05138 	 MISS
   80 	    22 	 0.19183 	 0.05175 	 MISS
   58 	    23 	 0.17049 	 0.05186 	 MISS
   74 	    24 	 0.18082 	 0.05204 	 MISS
    9 	    25 	 0.06890 	 0.05213 	 ~...
    3 	    26 	 0.05800 	 0.05270 	 ~...
   60 	    27 	 0.17082 	 0.05286 	 MISS
   16 	    28 	 0.16059 	 0.05314 	 MISS
   82 	    29 	 0.19518 	 0.05340 	 MISS
   13 	    30 	 0.07263 	 0.05376 	 ~...
    8 	    31 	 0.06799 	 0.05388 	 ~...
   79 	    32 	 0.18495 	 0.05391 	 MISS
    6 	    33 	 0.06079 	 0.05394 	 ~...
   48 	    34 	 0.16834 	 0.05398 	 MISS
   50 	    35 	 0.16910 	 0.05401 	 MISS
   10 	    36 	 0.06942 	 0.05405 	 ~...
   81 	    37 	 0.19467 	 0.05420 	 MISS
   44 	    38 	 0.16817 	 0.05430 	 MISS
   37 	    39 	 0.16695 	 0.05451 	 MISS
   14 	    40 	 0.07909 	 0.05469 	 ~...
   76 	    41 	 0.18298 	 0.05482 	 MISS
   31 	    42 	 0.16598 	 0.05536 	 MISS
   15 	    43 	 0.08081 	 0.05568 	 ~...
   38 	    44 	 0.16740 	 0.05589 	 MISS
   42 	    45 	 0.16788 	 0.05698 	 MISS
   55 	    46 	 0.17016 	 0.05746 	 MISS
   24 	    47 	 0.16334 	 0.05797 	 MISS
   60 	    48 	 0.17082 	 0.05828 	 MISS
   53 	    49 	 0.17001 	 0.05854 	 MISS
   57 	    50 	 0.17049 	 0.05963 	 MISS
   68 	    51 	 0.17558 	 0.06190 	 MISS
   77 	    52 	 0.18386 	 0.16042 	 ~...
   67 	    53 	 0.17341 	 0.18850 	 ~...
   26 	    54 	 0.16443 	 0.21218 	 m..s
   73 	    55 	 0.18035 	 0.21711 	 m..s
   69 	    56 	 0.17601 	 0.21979 	 m..s
   41 	    57 	 0.16782 	 0.22764 	 m..s
   21 	    58 	 0.16263 	 0.22971 	 m..s
   34 	    59 	 0.16688 	 0.23128 	 m..s
   83 	    60 	 0.19520 	 0.23155 	 m..s
   65 	    61 	 0.17256 	 0.23314 	 m..s
   64 	    62 	 0.17213 	 0.23442 	 m..s
   30 	    63 	 0.16558 	 0.23455 	 m..s
   35 	    64 	 0.16688 	 0.23467 	 m..s
   49 	    65 	 0.16884 	 0.23503 	 m..s
   46 	    66 	 0.16825 	 0.23527 	 m..s
   25 	    67 	 0.16407 	 0.23736 	 m..s
   36 	    68 	 0.16694 	 0.23823 	 m..s
   23 	    69 	 0.16295 	 0.23928 	 m..s
   29 	    70 	 0.16503 	 0.24013 	 m..s
   33 	    71 	 0.16610 	 0.24044 	 m..s
   78 	    72 	 0.18412 	 0.24073 	 m..s
   40 	    73 	 0.16773 	 0.24076 	 m..s
   18 	    74 	 0.16139 	 0.24092 	 m..s
   70 	    75 	 0.17625 	 0.24098 	 m..s
   28 	    76 	 0.16497 	 0.24548 	 m..s
   22 	    77 	 0.16293 	 0.24636 	 m..s
   32 	    78 	 0.16608 	 0.24784 	 m..s
   45 	    79 	 0.16824 	 0.25355 	 m..s
   19 	    80 	 0.16237 	 0.25661 	 m..s
   27 	    81 	 0.16454 	 0.26251 	 m..s
   54 	    82 	 0.17002 	 0.26410 	 m..s
   59 	    83 	 0.17060 	 0.26472 	 m..s
   84 	    84 	 0.37433 	 0.31072 	 m..s
   85 	    85 	 0.38086 	 0.33189 	 m..s
   89 	    86 	 0.38746 	 0.34551 	 m..s
   94 	    87 	 0.40836 	 0.36325 	 m..s
   87 	    88 	 0.38734 	 0.37175 	 ~...
   90 	    89 	 0.38818 	 0.37213 	 ~...
   91 	    90 	 0.38839 	 0.37543 	 ~...
   92 	    91 	 0.39067 	 0.37669 	 ~...
   97 	    92 	 0.40950 	 0.38105 	 ~...
   96 	    93 	 0.40892 	 0.38170 	 ~...
   86 	    94 	 0.38119 	 0.38496 	 ~...
  100 	    95 	 0.41987 	 0.39627 	 ~...
   88 	    96 	 0.38737 	 0.40353 	 ~...
   95 	    97 	 0.40862 	 0.41169 	 ~...
   93 	    98 	 0.39762 	 0.41636 	 ~...
   99 	    99 	 0.41906 	 0.41901 	 ~...
   98 	   100 	 0.41784 	 0.42063 	 ~...
  115 	   101 	 0.43569 	 0.42358 	 ~...
  102 	   102 	 0.42050 	 0.42728 	 ~...
  104 	   103 	 0.42144 	 0.42881 	 ~...
  107 	   104 	 0.42983 	 0.43412 	 ~...
  112 	   105 	 0.43423 	 0.43474 	 ~...
  117 	   106 	 0.44234 	 0.43885 	 ~...
  113 	   107 	 0.43495 	 0.43923 	 ~...
  114 	   108 	 0.43503 	 0.43964 	 ~...
  111 	   109 	 0.43161 	 0.43991 	 ~...
  116 	   110 	 0.44232 	 0.44064 	 ~...
  103 	   111 	 0.42077 	 0.44425 	 ~...
  101 	   112 	 0.42036 	 0.44598 	 ~...
  109 	   113 	 0.43151 	 0.44612 	 ~...
  105 	   114 	 0.42417 	 0.44750 	 ~...
  110 	   115 	 0.43153 	 0.44905 	 ~...
  108 	   116 	 0.43080 	 0.44981 	 ~...
  106 	   117 	 0.42575 	 0.45465 	 ~...
  118 	   118 	 0.44346 	 0.45790 	 ~...
  119 	   119 	 0.44732 	 0.46245 	 ~...
  120 	   120 	 0.45232 	 0.47721 	 ~...
==========================================
r_mrr = 0.885617733001709
r2_mrr = 0.7625911235809326
spearmanr_mrr@5 = 0.9730810523033142
spearmanr_mrr@10 = 0.9413227438926697
spearmanr_mrr@50 = 0.965816080570221
spearmanr_mrr@100 = 0.8917346000671387
spearmanr_mrr@All = 0.9096482396125793
==========================================
test time: 0.394
Done Testing dataset Kinships
total time taken: 188.70327234268188
training time taken: 181.78569722175598
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8856)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7626)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9731)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9413)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9658)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8917)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9096)}}, 'test_loss': {'DistMult': {'Kinships': 7.368036114261486}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 6014342729436296
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [933, 451, 139, 565, 32, 681, 301, 768, 997, 275, 1043, 682, 1076, 122, 647, 318, 711, 162, 518, 806, 26, 365, 467, 1188, 950, 1034, 1047, 276, 1041, 816, 138, 56, 1071, 696, 893, 305, 661, 667, 354, 461, 500, 851, 508, 892, 241, 248, 874, 459, 680, 803, 598, 499, 1028, 780, 141, 1159, 358, 773, 417, 387, 59, 1068, 329, 889, 308, 93, 729, 287, 497, 293, 217, 822, 1183, 491, 595, 644, 231, 144, 456, 463, 443, 990, 257, 199, 811, 774, 92, 850, 900, 957, 703, 1194, 1101, 1114, 866, 920, 337, 984, 1210, 342, 447, 943, 264, 1207, 1038, 747, 812, 315, 1124, 614, 385, 743, 130, 1150, 759, 418, 529, 845, 549, 190, 12]
valid_ids (0): []
train_ids (1094): [243, 359, 637, 233, 131, 254, 906, 394, 140, 678, 937, 1088, 1001, 112, 170, 973, 1091, 1030, 291, 453, 925, 70, 121, 273, 945, 979, 1070, 183, 157, 1087, 401, 297, 16, 6, 319, 801, 80, 875, 498, 403, 0, 1048, 640, 1190, 786, 73, 707, 1072, 1147, 5, 128, 953, 18, 826, 955, 125, 502, 568, 570, 754, 655, 299, 507, 1064, 48, 536, 864, 970, 67, 999, 651, 259, 582, 163, 590, 578, 750, 1119, 348, 147, 912, 1138, 534, 23, 1002, 956, 402, 148, 914, 627, 201, 1152, 82, 1196, 361, 630, 441, 47, 340, 390, 731, 708, 3, 668, 562, 211, 616, 378, 589, 44, 135, 849, 1121, 242, 221, 799, 187, 234, 435, 2, 686, 936, 642, 971, 338, 533, 1016, 83, 844, 9, 830, 755, 993, 886, 30, 374, 1051, 477, 346, 884, 693, 1129, 302, 1090, 1184, 1049, 292, 913, 208, 751, 567, 336, 1069, 831, 876, 371, 865, 787, 895, 462, 127, 664, 1092, 908, 171, 68, 1052, 653, 734, 416, 530, 1199, 195, 193, 382, 891, 495, 232, 709, 757, 1073, 213, 594, 1165, 882, 662, 1213, 1094, 975, 656, 372, 724, 422, 603, 623, 256, 861, 215, 926, 289, 314, 120, 996, 457, 785, 89, 349, 99, 114, 690, 804, 972, 853, 353, 620, 17, 102, 540, 260, 1055, 397, 689, 1059, 209, 873, 174, 373, 172, 272, 1115, 294, 1050, 1127, 1025, 493, 312, 409, 295, 646, 878, 448, 322, 1180, 395, 426, 756, 558, 615, 216, 1093, 355, 388, 622, 519, 1200, 645, 576, 181, 1128, 1105, 184, 607, 1044, 452, 1098, 357, 261, 407, 597, 868, 444, 612, 596, 1172, 400, 1134, 657, 897, 194, 569, 995, 1163, 1102, 255, 420, 100, 1085, 325, 154, 634, 24, 1166, 165, 64, 415, 1078, 290, 688, 784, 320, 438, 362, 1181, 1133, 1136, 7, 899, 1029, 62, 846, 1193, 285, 1, 196, 699, 79, 252, 356, 909, 555, 847, 538, 676, 240, 1089, 1067, 161, 683, 1009, 776, 670, 915, 1131, 1169, 506, 949, 929, 631, 366, 944, 572, 1013, 760, 725, 283, 694, 212, 722, 470, 476, 697, 687, 798, 334, 133, 492, 251, 728, 81, 559, 692, 189, 503, 20, 1117, 1126, 496, 109, 857, 341, 303, 599, 939, 1061, 863, 191, 721, 951, 742, 718, 767, 982, 1053, 263, 1185, 421, 986, 542, 1208, 19, 898, 108, 547, 65, 455, 643, 1026, 1198, 442, 249, 11, 1007, 84, 408, 1111, 494, 737, 168, 250, 541, 137, 855, 186, 204, 560, 1173, 219, 654, 429, 910, 160, 980, 460, 1045, 546, 704, 126, 1122, 1060, 1178, 1176, 1182, 870, 509, 770, 228, 35, 1024, 4, 740, 489, 879, 669, 968, 685, 98, 1008, 524, 129, 245, 1022, 333, 40, 962, 766, 1201, 958, 152, 517, 931, 626, 428, 1135, 505, 1123, 544, 328, 71, 87, 611, 1132, 436, 466, 1209, 522, 1142, 1082, 270, 577, 632, 396, 1074, 1171, 1214, 155, 304, 749, 205, 76, 53, 856, 548, 324, 574, 928, 1189, 1112, 91, 807, 1162, 368, 46, 85, 1191, 1141, 575, 1042, 280, 862, 410, 226, 1021, 473, 1006, 298, 633, 1179, 829, 1158, 658, 179, 316, 1167, 758, 907, 814, 188, 1149, 300, 563, 977, 379, 1204, 472, 34, 535, 150, 921, 363, 941, 672, 665, 554, 641, 1003, 1096, 389, 119, 795, 278, 267, 413, 286, 331, 1153, 978, 386, 841, 539, 922, 296, 445, 824, 712, 384, 592, 1058, 309, 419, 75, 738, 713, 458, 335, 652, 182, 613, 714, 809, 802, 375, 449, 779, 1000, 684, 629, 989, 918, 424, 1108, 1205, 177, 1081, 720, 1192, 1027, 815, 1175, 919, 347, 1066, 867, 116, 63, 551, 229, 425, 156, 777, 1174, 51, 134, 207, 1168, 545, 214, 430, 671, 480, 404, 45, 1120, 1143, 1032, 800, 772, 946, 105, 789, 601, 588, 97, 464, 218, 871, 446, 377, 1097, 1106, 967, 606, 513, 1186, 872, 727, 468, 271, 571, 963, 57, 210, 192, 36, 166, 1139, 14, 532, 153, 790, 103, 167, 123, 159, 381, 310, 1160, 791, 778, 1077, 1063, 835, 832, 1140, 1014, 746, 501, 1037, 948, 96, 220, 974, 650, 332, 1020, 27, 432, 37, 515, 1005, 543, 317, 29, 719, 164, 723, 604, 307, 1054, 197, 732, 836, 427, 1056, 392, 769, 1109, 383, 823, 78, 149, 411, 618, 1211, 61, 1011, 819, 848, 247, 268, 843, 200, 556, 1107, 820, 206, 1206, 883, 700, 610, 808, 587, 602, 981, 930, 94, 414, 237, 203, 115, 833, 675, 142, 483, 330, 1099, 821, 639, 660, 579, 834, 940, 666, 752, 924, 469, 952, 564, 440, 1151, 399, 965, 903, 474, 258, 486, 960, 364, 1104, 434, 691, 817, 854, 225, 282, 516, 398, 465, 431, 736, 227, 514, 1125, 880, 605, 1018, 136, 994, 288, 175, 887, 894, 369, 1039, 584, 33, 230, 905, 178, 284, 72, 992, 762, 566, 698, 169, 730, 1031, 934, 744, 10, 1100, 1195, 1083, 1004, 679, 158, 748, 405, 145, 901, 837, 1144, 327, 521, 763, 69, 1035, 1116, 1046, 306, 964, 88, 1017, 281, 1177, 323, 797, 311, 705, 512, 151, 583, 608, 966, 617, 15, 13, 673, 1155, 380, 481, 479, 842, 916, 49, 343, 942, 827, 146, 31, 969, 710, 765, 412, 106, 593, 1040, 976, 561, 813, 695, 352, 360, 701, 902, 1075, 223, 244, 277, 1156, 52, 1023, 1161, 573, 1010, 246, 591, 321, 393, 706, 21, 781, 663, 520, 107, 628, 185, 1157, 1170, 235, 828, 43, 553, 433, 55, 796, 376, 74, 648, 132, 1062, 860, 54, 531, 537, 42, 659, 50, 624, 1154, 735, 726, 1203, 339, 1118, 581, 782, 58, 881, 1057, 266, 1145, 90, 124, 39, 585, 77, 274, 877, 677, 326, 988, 173, 557, 586, 528, 202, 482, 1015, 840, 715, 775, 888, 110, 1033, 771, 625, 238, 423, 262, 983, 527, 454, 985, 600, 702, 478, 609, 764, 488, 180, 961, 852, 253, 487, 101, 954, 923, 825, 635, 1113, 236, 370, 485, 41, 1080, 437, 510, 733, 959, 1148, 1187, 117, 525, 739, 987, 38, 367, 1084, 1146, 279, 741, 838, 344, 935, 1164, 858, 313, 717, 917, 938, 111, 471, 224, 619, 636, 947, 22, 1079, 1197, 550, 351, 761, 113, 406, 1065, 504, 998, 896, 674, 523, 345, 927, 788, 552, 176, 1110, 526, 66, 25, 753, 1103, 859, 1202, 269, 716, 511, 638, 450, 1095, 439, 885, 1130, 222, 911, 104, 28, 1137, 198, 904, 810, 621, 890, 794, 1019, 580, 869, 792, 839, 239, 475, 1036, 818, 95, 783, 391, 60, 991, 649, 793, 1012, 350, 805, 265, 1086, 490, 745, 118, 1212, 86, 8, 932, 484, 143]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5787634573844018
the save name prefix for this run is:  chkpt-ID_5787634573844018_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 46
rank avg (pred): 0.443 +- 0.004
mrr vals (pred, true): 0.021, 0.439
batch losses (mrrl, rdl): 0.0, 0.0032109751

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1051
rank avg (pred): 0.341 +- 0.025
mrr vals (pred, true): 0.028, 0.050
batch losses (mrrl, rdl): 0.0, 0.0005138487

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 225
rank avg (pred): 0.279 +- 0.075
mrr vals (pred, true): 0.063, 0.057
batch losses (mrrl, rdl): 0.0, 0.0007771343

Epoch over!
epoch time: 12.006

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 733
rank avg (pred): 0.099 +- 0.027
mrr vals (pred, true): 0.131, 0.148
batch losses (mrrl, rdl): 0.0, 0.0001824113

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 3
rank avg (pred): 0.053 +- 0.015
mrr vals (pred, true): 0.202, 0.432
batch losses (mrrl, rdl): 0.0, 7.878e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 754
rank avg (pred): 0.123 +- 0.036
mrr vals (pred, true): 0.135, 0.109
batch losses (mrrl, rdl): 0.0, 0.0001764869

Epoch over!
epoch time: 12.003

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 361
rank avg (pred): 0.298 +- 0.082
mrr vals (pred, true): 0.083, 0.245
batch losses (mrrl, rdl): 0.0, 0.000872461

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 555
rank avg (pred): 0.042 +- 0.011
mrr vals (pred, true): 0.236, 0.365
batch losses (mrrl, rdl): 0.0, 1.76715e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 377
rank avg (pred): 0.273 +- 0.076
mrr vals (pred, true): 0.098, 0.250
batch losses (mrrl, rdl): 0.0, 0.0006608419

Epoch over!
epoch time: 11.976

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1067
rank avg (pred): 0.024 +- 0.007
mrr vals (pred, true): 0.326, 0.452
batch losses (mrrl, rdl): 0.0, 2.07783e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 996
rank avg (pred): 0.038 +- 0.011
mrr vals (pred, true): 0.247, 0.451
batch losses (mrrl, rdl): 0.0, 5.3878e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1156
rank avg (pred): 0.066 +- 0.018
mrr vals (pred, true): 0.183, 0.387
batch losses (mrrl, rdl): 0.0, 8.341e-07

Epoch over!
epoch time: 11.841

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1192
rank avg (pred): 0.263 +- 0.073
mrr vals (pred, true): 0.101, 0.052
batch losses (mrrl, rdl): 0.0, 0.0008394796

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 123
rank avg (pred): 0.271 +- 0.075
mrr vals (pred, true): 0.101, 0.243
batch losses (mrrl, rdl): 0.0, 0.0006399204

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 786
rank avg (pred): 0.442 +- 0.122
mrr vals (pred, true): 0.089, 0.061
batch losses (mrrl, rdl): 0.0, 6.50341e-05

Epoch over!
epoch time: 11.946

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1110
rank avg (pred): 0.304 +- 0.084
mrr vals (pred, true): 0.097, 0.049
batch losses (mrrl, rdl): 0.021785384, 0.0006096617

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 646
rank avg (pred): 0.205 +- 0.091
mrr vals (pred, true): 0.137, 0.237
batch losses (mrrl, rdl): 0.0996080488, 0.0002360529

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 922
rank avg (pred): 0.806 +- 0.294
mrr vals (pred, true): 0.088, 0.050
batch losses (mrrl, rdl): 0.0143787842, 0.0017431439

Epoch over!
epoch time: 12.448

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 492
rank avg (pred): 0.019 +- 0.009
mrr vals (pred, true): 0.412, 0.393
batch losses (mrrl, rdl): 0.0034558934, 3.54979e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 687
rank avg (pred): 0.190 +- 0.080
mrr vals (pred, true): 0.131, 0.058
batch losses (mrrl, rdl): 0.0662083104, 0.0015359356

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 405
rank avg (pred): 0.196 +- 0.078
mrr vals (pred, true): 0.122, 0.051
batch losses (mrrl, rdl): 0.0516240336, 0.0016200687

Epoch over!
epoch time: 12.15

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 109
rank avg (pred): 0.184 +- 0.080
mrr vals (pred, true): 0.137, 0.250
batch losses (mrrl, rdl): 0.1274900138, 0.0001440417

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 488
rank avg (pred): 0.023 +- 0.011
mrr vals (pred, true): 0.380, 0.381
batch losses (mrrl, rdl): 2.75913e-05, 3.60395e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 581
rank avg (pred): 0.164 +- 0.072
mrr vals (pred, true): 0.141, 0.248
batch losses (mrrl, rdl): 0.113912262, 0.0001176624

Epoch over!
epoch time: 12.203

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1072
rank avg (pred): 0.016 +- 0.007
mrr vals (pred, true): 0.436, 0.409
batch losses (mrrl, rdl): 0.0069674915, 3.85525e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 625
rank avg (pred): 0.162 +- 0.073
mrr vals (pred, true): 0.147, 0.246
batch losses (mrrl, rdl): 0.0992846787, 8.21341e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 922
rank avg (pred): 0.878 +- 0.256
mrr vals (pred, true): 0.063, 0.050
batch losses (mrrl, rdl): 0.0015808783, 0.0027000515

Epoch over!
epoch time: 12.299

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1075
rank avg (pred): 0.015 +- 0.007
mrr vals (pred, true): 0.456, 0.460
batch losses (mrrl, rdl): 0.0001873788, 3.37335e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 724
rank avg (pred): 0.154 +- 0.077
mrr vals (pred, true): 0.169, 0.052
batch losses (mrrl, rdl): 0.1423828006, 0.0021855105

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 555
rank avg (pred): 0.029 +- 0.014
mrr vals (pred, true): 0.340, 0.365
batch losses (mrrl, rdl): 0.0062618847, 3.51654e-05

Epoch over!
epoch time: 12.083

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 94
rank avg (pred): 0.168 +- 0.072
mrr vals (pred, true): 0.137, 0.241
batch losses (mrrl, rdl): 0.1078626961, 0.0001211558

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 196
rank avg (pred): 0.168 +- 0.073
mrr vals (pred, true): 0.141, 0.049
batch losses (mrrl, rdl): 0.0829802752, 0.0019532777

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1214
rank avg (pred): 0.131 +- 0.062
mrr vals (pred, true): 0.168, 0.059
batch losses (mrrl, rdl): 0.139626503, 0.0022350517

Epoch over!
epoch time: 12.258

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 707
rank avg (pred): 0.163 +- 0.070
mrr vals (pred, true): 0.140, 0.053
batch losses (mrrl, rdl): 0.0810604692, 0.0019190707

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 862
rank avg (pred): 0.850 +- 0.244
mrr vals (pred, true): 0.043, 0.050
batch losses (mrrl, rdl): 0.000480071, 0.0025136524

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 140
rank avg (pred): 0.143 +- 0.064
mrr vals (pred, true): 0.157, 0.174
batch losses (mrrl, rdl): 0.0030096003, 6.5808e-05

Epoch over!
epoch time: 12.174

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1176
rank avg (pred): 0.154 +- 0.061
mrr vals (pred, true): 0.135, 0.248
batch losses (mrrl, rdl): 0.1273796111, 6.54614e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 369
rank avg (pred): 0.140 +- 0.060
mrr vals (pred, true): 0.150, 0.259
batch losses (mrrl, rdl): 0.1177335083, 1.91624e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1112
rank avg (pred): 0.135 +- 0.058
mrr vals (pred, true): 0.153, 0.051
batch losses (mrrl, rdl): 0.1053459346, 0.0021984312

Epoch over!
epoch time: 12.468

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 85
rank avg (pred): 0.138 +- 0.064
mrr vals (pred, true): 0.164, 0.217
batch losses (mrrl, rdl): 0.0283214673, 4.67268e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 739
rank avg (pred): 0.016 +- 0.007
mrr vals (pred, true): 0.435, 0.411
batch losses (mrrl, rdl): 0.0054327995, 3.76008e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 527
rank avg (pred): 0.022 +- 0.010
mrr vals (pred, true): 0.367, 0.372
batch losses (mrrl, rdl): 0.0002136768, 3.87948e-05

Epoch over!
epoch time: 11.971

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 942
rank avg (pred): 0.818 +- 0.235
mrr vals (pred, true): 0.042, 0.043
batch losses (mrrl, rdl): 0.0005908806, 0.0017275279

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 835
rank avg (pred): 0.037 +- 0.014
mrr vals (pred, true): 0.273, 0.411
batch losses (mrrl, rdl): 0.1899381876, 2.26463e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 854
rank avg (pred): 0.854 +- 0.233
mrr vals (pred, true): 0.037, 0.053
batch losses (mrrl, rdl): 0.0016413226, 0.0026902065

Epoch over!
epoch time: 11.992

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.884 +- 0.238
mrr vals (pred, true): 0.035, 0.046

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   18 	     0 	 0.13437 	 0.02078 	 MISS
    1 	     1 	 0.03387 	 0.04019 	 ~...
    2 	     2 	 0.03526 	 0.04608 	 ~...
   76 	     3 	 0.17715 	 0.04757 	 MISS
   69 	     4 	 0.16109 	 0.04831 	 MISS
   39 	     5 	 0.14965 	 0.04907 	 MISS
    7 	     6 	 0.04828 	 0.04927 	 ~...
   14 	     7 	 0.07106 	 0.04951 	 ~...
   48 	     8 	 0.15118 	 0.04952 	 MISS
   13 	     9 	 0.06857 	 0.04957 	 ~...
   72 	    10 	 0.16509 	 0.04963 	 MISS
   43 	    11 	 0.15070 	 0.04997 	 MISS
   44 	    12 	 0.15076 	 0.05011 	 MISS
   19 	    13 	 0.13963 	 0.05014 	 m..s
   67 	    14 	 0.15889 	 0.05037 	 MISS
   50 	    15 	 0.15204 	 0.05088 	 MISS
   20 	    16 	 0.14468 	 0.05093 	 m..s
   29 	    17 	 0.14792 	 0.05119 	 m..s
   15 	    18 	 0.07198 	 0.05131 	 ~...
    0 	    19 	 0.03273 	 0.05197 	 ~...
   10 	    20 	 0.06229 	 0.05212 	 ~...
    8 	    21 	 0.05633 	 0.05249 	 ~...
    6 	    22 	 0.04710 	 0.05287 	 ~...
   62 	    23 	 0.15560 	 0.05292 	 MISS
   37 	    24 	 0.14922 	 0.05309 	 m..s
   23 	    25 	 0.14591 	 0.05324 	 m..s
   35 	    26 	 0.14893 	 0.05340 	 m..s
   78 	    27 	 0.17983 	 0.05361 	 MISS
   11 	    28 	 0.06354 	 0.05363 	 ~...
    3 	    29 	 0.03902 	 0.05365 	 ~...
    4 	    30 	 0.04123 	 0.05375 	 ~...
   36 	    31 	 0.14893 	 0.05394 	 m..s
   59 	    32 	 0.15385 	 0.05418 	 m..s
   12 	    33 	 0.06486 	 0.05428 	 ~...
   71 	    34 	 0.16335 	 0.05429 	 MISS
    9 	    35 	 0.05908 	 0.05432 	 ~...
   30 	    36 	 0.14793 	 0.05471 	 m..s
   53 	    37 	 0.15267 	 0.05513 	 m..s
   24 	    38 	 0.14619 	 0.05541 	 m..s
   41 	    39 	 0.15038 	 0.05543 	 m..s
   32 	    40 	 0.14806 	 0.05557 	 m..s
   16 	    41 	 0.07618 	 0.05568 	 ~...
   63 	    42 	 0.15574 	 0.05569 	 MISS
   45 	    43 	 0.15080 	 0.05596 	 m..s
   64 	    44 	 0.15687 	 0.05605 	 MISS
   17 	    45 	 0.07765 	 0.05668 	 ~...
   58 	    46 	 0.15372 	 0.05698 	 m..s
   46 	    47 	 0.15099 	 0.05706 	 m..s
   77 	    48 	 0.17838 	 0.05728 	 MISS
    5 	    49 	 0.04338 	 0.05736 	 ~...
   31 	    50 	 0.14801 	 0.05757 	 m..s
   73 	    51 	 0.16714 	 0.05770 	 MISS
   75 	    52 	 0.17331 	 0.05810 	 MISS
   56 	    53 	 0.15346 	 0.06090 	 m..s
   74 	    54 	 0.16944 	 0.06399 	 MISS
   33 	    55 	 0.14885 	 0.18802 	 m..s
   65 	    56 	 0.15703 	 0.19116 	 m..s
   49 	    57 	 0.15155 	 0.22230 	 m..s
   61 	    58 	 0.15552 	 0.22982 	 m..s
   34 	    59 	 0.14886 	 0.23134 	 m..s
   21 	    60 	 0.14545 	 0.23413 	 m..s
   28 	    61 	 0.14791 	 0.23480 	 m..s
   60 	    62 	 0.15450 	 0.23554 	 m..s
   26 	    63 	 0.14749 	 0.23710 	 m..s
   47 	    64 	 0.15111 	 0.23903 	 m..s
   38 	    65 	 0.14962 	 0.24547 	 m..s
   22 	    66 	 0.14559 	 0.24587 	 MISS
   51 	    67 	 0.15243 	 0.24617 	 m..s
   68 	    68 	 0.16003 	 0.24654 	 m..s
   52 	    69 	 0.15248 	 0.24699 	 m..s
   54 	    70 	 0.15301 	 0.25383 	 MISS
   57 	    71 	 0.15370 	 0.26251 	 MISS
   25 	    72 	 0.14673 	 0.26251 	 MISS
   66 	    73 	 0.15880 	 0.26309 	 MISS
   42 	    74 	 0.15069 	 0.26361 	 MISS
   79 	    75 	 0.18783 	 0.26671 	 m..s
   40 	    76 	 0.15025 	 0.26716 	 MISS
   27 	    77 	 0.14787 	 0.27616 	 MISS
   55 	    78 	 0.15302 	 0.27928 	 MISS
   82 	    79 	 0.24159 	 0.28251 	 m..s
   70 	    80 	 0.16324 	 0.29182 	 MISS
   80 	    81 	 0.22087 	 0.30334 	 m..s
   81 	    82 	 0.22899 	 0.31072 	 m..s
   84 	    83 	 0.37780 	 0.35580 	 ~...
   85 	    84 	 0.38341 	 0.35933 	 ~...
   89 	    85 	 0.38490 	 0.36325 	 ~...
   93 	    86 	 0.38829 	 0.37045 	 ~...
   90 	    87 	 0.38573 	 0.37210 	 ~...
   92 	    88 	 0.38651 	 0.37320 	 ~...
   91 	    89 	 0.38599 	 0.38105 	 ~...
   94 	    90 	 0.39226 	 0.38496 	 ~...
   88 	    91 	 0.38394 	 0.39223 	 ~...
   87 	    92 	 0.38391 	 0.39255 	 ~...
  104 	    93 	 0.44084 	 0.39661 	 m..s
   95 	    94 	 0.39392 	 0.39919 	 ~...
   86 	    95 	 0.38349 	 0.41169 	 ~...
   96 	    96 	 0.40692 	 0.41303 	 ~...
   83 	    97 	 0.37095 	 0.41472 	 m..s
   99 	    98 	 0.43831 	 0.41901 	 ~...
  105 	    99 	 0.45156 	 0.42358 	 ~...
   97 	   100 	 0.43551 	 0.42492 	 ~...
  115 	   101 	 0.46619 	 0.42698 	 m..s
  117 	   102 	 0.46672 	 0.42760 	 m..s
  102 	   103 	 0.44009 	 0.42770 	 ~...
  101 	   104 	 0.44006 	 0.42818 	 ~...
  118 	   105 	 0.46676 	 0.43431 	 m..s
  114 	   106 	 0.46455 	 0.43568 	 ~...
  112 	   107 	 0.46331 	 0.43581 	 ~...
  113 	   108 	 0.46363 	 0.43896 	 ~...
  100 	   109 	 0.43851 	 0.43967 	 ~...
  107 	   110 	 0.45985 	 0.44087 	 ~...
  109 	   111 	 0.46164 	 0.44344 	 ~...
  108 	   112 	 0.45991 	 0.44408 	 ~...
  120 	   113 	 0.46927 	 0.44580 	 ~...
  110 	   114 	 0.46205 	 0.44604 	 ~...
   98 	   115 	 0.43609 	 0.44618 	 ~...
  119 	   116 	 0.46867 	 0.44750 	 ~...
  116 	   117 	 0.46635 	 0.44770 	 ~...
  106 	   118 	 0.45861 	 0.44981 	 ~...
  111 	   119 	 0.46205 	 0.45747 	 ~...
  103 	   120 	 0.44059 	 0.46171 	 ~...
==========================================
r_mrr = 0.8929332494735718
r2_mrr = 0.7891621589660645
spearmanr_mrr@5 = 0.9904645681381226
spearmanr_mrr@10 = 0.8772444128990173
spearmanr_mrr@50 = 0.9876721501350403
spearmanr_mrr@100 = 0.893872082233429
spearmanr_mrr@All = 0.9151432514190674
==========================================
test time: 0.393
Done Testing dataset Kinships
total time taken: 189.29663968086243
training time taken: 182.28711485862732
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8929)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7892)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9905)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.8772)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9877)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8939)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9151)}}, 'test_loss': {'DistMult': {'Kinships': 6.910570623014792}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 8738157128374912
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [182, 208, 162, 276, 796, 1173, 926, 1183, 460, 514, 302, 93, 950, 581, 678, 630, 1016, 116, 757, 456, 880, 511, 213, 167, 835, 526, 212, 370, 468, 1209, 934, 873, 477, 1124, 1094, 656, 1053, 445, 444, 153, 466, 1205, 257, 764, 216, 991, 551, 1195, 336, 913, 859, 588, 156, 102, 576, 1002, 846, 516, 541, 774, 734, 317, 327, 498, 347, 1158, 717, 919, 505, 1000, 160, 847, 899, 256, 698, 751, 715, 562, 1161, 1089, 136, 537, 1153, 932, 333, 839, 821, 726, 411, 398, 719, 197, 958, 801, 422, 963, 268, 1047, 906, 418, 1159, 542, 828, 353, 1065, 777, 176, 38, 969, 490, 134, 352, 471, 528, 454, 1034, 791, 401, 391, 945, 785]
valid_ids (0): []
train_ids (1094): [818, 691, 739, 856, 14, 243, 1208, 95, 977, 356, 232, 824, 1181, 465, 1003, 907, 935, 980, 177, 651, 1101, 831, 1033, 1037, 683, 78, 961, 191, 1149, 558, 819, 278, 659, 224, 1118, 749, 1202, 339, 384, 1045, 1168, 185, 201, 499, 536, 1060, 974, 22, 138, 36, 1018, 643, 1160, 381, 475, 396, 290, 54, 627, 822, 1207, 931, 483, 82, 172, 282, 917, 793, 393, 882, 28, 410, 1138, 669, 649, 535, 273, 790, 1120, 1152, 794, 461, 611, 787, 673, 369, 463, 646, 120, 916, 539, 253, 187, 1070, 805, 608, 255, 569, 52, 557, 879, 12, 313, 744, 32, 436, 925, 591, 41, 840, 429, 311, 237, 448, 375, 103, 1106, 682, 488, 1199, 128, 293, 567, 1014, 766, 1054, 944, 56, 101, 83, 1164, 686, 681, 628, 540, 378, 1027, 264, 2, 148, 431, 632, 741, 110, 813, 631, 439, 481, 606, 929, 836, 583, 357, 784, 523, 877, 140, 768, 263, 13, 395, 1043, 890, 100, 587, 598, 416, 137, 8, 620, 275, 892, 662, 1114, 721, 655, 484, 420, 966, 850, 27, 44, 756, 1042, 1147, 638, 462, 61, 474, 188, 952, 1082, 214, 954, 616, 210, 555, 520, 159, 872, 15, 968, 79, 489, 1052, 529, 1174, 178, 692, 809, 563, 512, 392, 1194, 1049, 1004, 453, 179, 604, 509, 151, 970, 267, 192, 437, 500, 1007, 402, 1046, 600, 96, 697, 441, 106, 761, 299, 1040, 1105, 226, 808, 891, 578, 527, 885, 838, 782, 48, 610, 1171, 765, 139, 605, 1073, 1196, 113, 222, 355, 298, 860, 513, 1197, 458, 566, 957, 487, 451, 26, 123, 889, 60, 613, 920, 740, 586, 705, 1031, 675, 534, 430, 990, 670, 29, 1112, 1175, 181, 190, 132, 617, 320, 995, 17, 53, 90, 72, 478, 37, 579, 911, 1097, 1136, 144, 799, 387, 109, 309, 1129, 360, 364, 1186, 80, 773, 1140, 700, 225, 830, 564, 665, 1122, 130, 747, 811, 1125, 58, 108, 343, 1064, 1155, 946, 377, 825, 399, 236, 390, 359, 271, 1066, 903, 769, 806, 1063, 1096, 637, 1130, 728, 762, 574, 967, 888, 1109, 924, 1163, 729, 5, 57, 975, 440, 491, 152, 900, 400, 857, 205, 64, 1036, 1166, 997, 435, 476, 671, 374, 230, 914, 404, 1, 99, 571, 450, 875, 1087, 565, 994, 397, 20, 142, 170, 742, 778, 1024, 827, 350, 68, 304, 1115, 202, 1144, 125, 1151, 358, 10, 168, 973, 19, 81, 862, 989, 1139, 789, 259, 164, 547, 365, 624, 269, 549, 962, 603, 295, 1099, 300, 895, 442, 447, 217, 443, 707, 948, 292, 992, 65, 1090, 696, 972, 1178, 635, 652, 413, 690, 909, 55, 956, 797, 45, 1142, 1146, 876, 301, 609, 193, 1137, 1148, 708, 843, 59, 1029, 1069, 131, 905, 896, 570, 486, 965, 626, 296, 1201, 1211, 783, 897, 220, 702, 976, 1111, 874, 280, 941, 942, 1204, 864, 519, 1102, 1035, 883, 438, 325, 284, 1038, 363, 73, 869, 775, 89, 998, 676, 1019, 124, 265, 260, 940, 1015, 196, 452, 209, 1188, 1008, 324, 251, 544, 332, 129, 711, 1116, 126, 1131, 495, 650, 663, 999, 982, 479, 693, 723, 607, 661, 1020, 894, 158, 522, 1121, 211, 814, 1192, 955, 1126, 227, 127, 229, 653, 105, 657, 680, 303, 1072, 1132, 1079, 781, 845, 1128, 335, 703, 326, 908, 233, 200, 507, 85, 625, 281, 709, 1032, 904, 927, 618, 508, 199, 533, 1086, 798, 959, 532, 928, 91, 341, 660, 104, 936, 1156, 861, 473, 351, 832, 385, 780, 33, 244, 349, 94, 922, 97, 1172, 713, 346, 530, 1048, 433, 985, 867, 538, 379, 1076, 6, 901, 679, 594, 1010, 800, 504, 515, 169, 143, 1084, 858, 674, 270, 687, 561, 362, 286, 98, 572, 577, 1212, 344, 145, 1104, 7, 49, 1117, 701, 585, 853, 330, 758, 1154, 792, 596, 615, 389, 77, 1162, 415, 664, 634, 122, 219, 1210, 1057, 943, 1088, 645, 366, 834, 1206, 146, 1006, 316, 470, 964, 239, 1170, 868, 1044, 174, 854, 340, 111, 1193, 754, 246, 310, 795, 114, 1062, 844, 1214, 1067, 510, 672, 75, 367, 0, 175, 750, 878, 641, 1103, 469, 612, 107, 779, 371, 803, 403, 40, 710, 1189, 86, 1059, 724, 485, 614, 328, 658, 531, 88, 550, 988, 545, 1176, 893, 506, 767, 1184, 307, 863, 851, 221, 407, 306, 1141, 601, 321, 815, 714, 9, 112, 548, 971, 619, 1058, 589, 1203, 786, 329, 580, 373, 414, 186, 412, 590, 978, 1182, 1150, 1092, 69, 50, 247, 748, 331, 745, 1167, 71, 746, 382, 319, 983, 380, 46, 1026, 11, 1135, 426, 1023, 39, 1185, 250, 736, 277, 770, 812, 939, 147, 42, 599, 1200, 30, 92, 1012, 849, 31, 1078, 887, 755, 848, 287, 376, 16, 923, 737, 575, 194, 870, 622, 647, 63, 712, 171, 322, 1028, 241, 449, 1190, 223, 1110, 837, 621, 930, 492, 279, 1127, 1091, 644, 51, 3, 688, 231, 816, 315, 884, 249, 386, 871, 35, 235, 446, 423, 135, 738, 117, 464, 735, 1077, 866, 1061, 1083, 74, 1085, 141, 833, 1123, 157, 1022, 1039, 518, 368, 718, 274, 684, 1017, 953, 568, 546, 602, 685, 457, 1051, 266, 240, 501, 66, 689, 165, 1119, 759, 722, 155, 560, 184, 1165, 195, 732, 425, 636, 1005, 203, 354, 204, 502, 348, 639, 521, 118, 163, 459, 405, 1100, 543, 949, 4, 87, 248, 283, 996, 573, 342, 951, 121, 763, 305, 772, 1179, 323, 1071, 388, 297, 294, 261, 826, 553, 455, 150, 1133, 406, 1198, 133, 752, 372, 18, 215, 1108, 334, 1177, 595, 979, 1050, 47, 623, 947, 807, 198, 912, 1169, 654, 417, 480, 1098, 915, 23, 76, 960, 1113, 242, 582, 1011, 1055, 424, 394, 115, 427, 262, 694, 308, 720, 67, 337, 497, 1074, 21, 776, 1001, 993, 695, 706, 285, 312, 841, 743, 1095, 288, 593, 493, 938, 1134, 525, 318, 881, 207, 1107, 1068, 238, 408, 987, 161, 421, 716, 788, 731, 119, 43, 886, 206, 1056, 432, 668, 1030, 667, 218, 820, 467, 725, 252, 228, 1093, 291, 1041, 183, 189, 554, 842, 733, 428, 642, 503, 804, 254, 234, 817, 345, 154, 34, 802, 482, 1143, 1081, 597, 984, 727, 981, 918, 810, 434, 245, 855, 494, 1021, 552, 730, 1145, 419, 937, 314, 1075, 559, 62, 1009, 898, 556, 1187, 1080, 704, 771, 986, 1180, 338, 173, 760, 1157, 910, 823, 1013, 852, 648, 409, 84, 149, 166, 699, 666, 24, 383, 921, 180, 584, 592, 258, 361, 640, 496, 472, 524, 517, 629, 25, 933, 865, 289, 829, 70, 1191, 1025, 902, 677, 753, 633, 272, 1213]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1711305876456518
the save name prefix for this run is:  chkpt-ID_1711305876456518_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1107
rank avg (pred): 0.553 +- 0.007
mrr vals (pred, true): 0.017, 0.052
batch losses (mrrl, rdl): 0.0, 0.0001887361

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 569
rank avg (pred): 0.218 +- 0.169
mrr vals (pred, true): 0.229, 0.232
batch losses (mrrl, rdl): 0.0, 0.0003192491

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1134
rank avg (pred): 0.081 +- 0.064
mrr vals (pred, true): 0.357, 0.394
batch losses (mrrl, rdl): 0.0, 9.199e-06

Epoch over!
epoch time: 12.053

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 848
rank avg (pred): 0.353 +- 0.271
mrr vals (pred, true): 0.259, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001422862

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 683
rank avg (pred): 0.200 +- 0.175
mrr vals (pred, true): 0.386, 0.056
batch losses (mrrl, rdl): 0.0, 0.0012772015

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 507
rank avg (pred): 0.114 +- 0.097
mrr vals (pred, true): 0.407, 0.389
batch losses (mrrl, rdl): 0.0, 6.96732e-05

Epoch over!
epoch time: 12.004

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 260
rank avg (pred): 0.032 +- 0.027
mrr vals (pred, true): 0.486, 0.434
batch losses (mrrl, rdl): 0.0, 1.19187e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1090
rank avg (pred): 0.355 +- 0.257
mrr vals (pred, true): 0.220, 0.232
batch losses (mrrl, rdl): 0.0, 0.0013904581

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 713
rank avg (pred): 0.209 +- 0.192
mrr vals (pred, true): 0.408, 0.051
batch losses (mrrl, rdl): 0.0, 0.0011063976

Epoch over!
epoch time: 11.78

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 803
rank avg (pred): 0.433 +- 0.326
mrr vals (pred, true): 0.248, 0.050
batch losses (mrrl, rdl): 0.0, 3.54864e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 766
rank avg (pred): 0.424 +- 0.342
mrr vals (pred, true): 0.299, 0.059
batch losses (mrrl, rdl): 0.0, 4.36549e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 96
rank avg (pred): 0.267 +- 0.238
mrr vals (pred, true): 0.386, 0.241
batch losses (mrrl, rdl): 0.0, 0.0008588795

Epoch over!
epoch time: 11.94

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 55
rank avg (pred): 0.014 +- 0.013
mrr vals (pred, true): 0.603, 0.439
batch losses (mrrl, rdl): 0.0, 3.71099e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1067
rank avg (pred): 0.027 +- 0.026
mrr vals (pred, true): 0.545, 0.452
batch losses (mrrl, rdl): 0.0, 1.57935e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1131
rank avg (pred): 0.253 +- 0.236
mrr vals (pred, true): 0.383, 0.059
batch losses (mrrl, rdl): 0.0, 0.0005922511

Epoch over!
epoch time: 11.889

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 794
rank avg (pred): 0.426 +- 0.332
mrr vals (pred, true): 0.266, 0.055
batch losses (mrrl, rdl): 0.4686231613, 2.79692e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1060
rank avg (pred): 0.581 +- 0.488
mrr vals (pred, true): 0.387, 0.462
batch losses (mrrl, rdl): 0.0569555461, 0.0048633013

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 69
rank avg (pred): 0.569 +- 0.490
mrr vals (pred, true): 0.412, 0.435
batch losses (mrrl, rdl): 0.0054683117, 0.0046261619

Epoch over!
epoch time: 12.279

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 754
rank avg (pred): 0.580 +- 0.488
mrr vals (pred, true): 0.306, 0.109
batch losses (mrrl, rdl): 0.3880274594, 0.0027332907

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 224
rank avg (pred): 0.592 +- 0.483
mrr vals (pred, true): 0.164, 0.058
batch losses (mrrl, rdl): 0.1298750937, 0.0006753286

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 342
rank avg (pred): 0.593 +- 0.482
mrr vals (pred, true): 0.154, 0.263
batch losses (mrrl, rdl): 0.1187467203, 0.0044065365

Epoch over!
epoch time: 11.988

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1130
rank avg (pred): 0.593 +- 0.481
mrr vals (pred, true): 0.144, 0.053
batch losses (mrrl, rdl): 0.0890975147, 0.0005681669

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 905
rank avg (pred): 0.588 +- 0.486
mrr vals (pred, true): 0.266, 0.309
batch losses (mrrl, rdl): 0.0180504341, 0.004616613

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 565
rank avg (pred): 0.566 +- 0.489
mrr vals (pred, true): 0.395, 0.370
batch losses (mrrl, rdl): 0.0058722449, 0.0044628629

Epoch over!
epoch time: 12.318

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 297
rank avg (pred): 0.510 +- 0.493
mrr vals (pred, true): 0.462, 0.446
batch losses (mrrl, rdl): 0.0025885855, 0.0037061395

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1179
rank avg (pred): 0.592 +- 0.481
mrr vals (pred, true): 0.154, 0.248
batch losses (mrrl, rdl): 0.0880179107, 0.004276637

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 769
rank avg (pred): 0.626 +- 0.445
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003188167, 0.0005410117

Epoch over!
epoch time: 11.823

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 555
rank avg (pred): 0.541 +- 0.491
mrr vals (pred, true): 0.428, 0.365
batch losses (mrrl, rdl): 0.0393232554, 0.0039979587

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 252
rank avg (pred): 0.538 +- 0.492
mrr vals (pred, true): 0.433, 0.444
batch losses (mrrl, rdl): 0.0011829386, 0.0041634403

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1200
rank avg (pred): 0.604 +- 0.477
mrr vals (pred, true): 0.112, 0.053
batch losses (mrrl, rdl): 0.0389295891, 0.0006446774

Epoch over!
epoch time: 11.984

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 319
rank avg (pred): 0.552 +- 0.493
mrr vals (pred, true): 0.422, 0.443
batch losses (mrrl, rdl): 0.0045920853, 0.0043405755

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 481
rank avg (pred): 0.601 +- 0.473
mrr vals (pred, true): 0.102, 0.047
batch losses (mrrl, rdl): 0.0270097777, 0.000549223

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 232
rank avg (pred): 0.603 +- 0.473
mrr vals (pred, true): 0.093, 0.055
batch losses (mrrl, rdl): 0.0188020635, 0.000530424

Epoch over!
epoch time: 12.354

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1024
rank avg (pred): 0.602 +- 0.477
mrr vals (pred, true): 0.117, 0.226
batch losses (mrrl, rdl): 0.1186185181, 0.004453375

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 596
rank avg (pred): 0.610 +- 0.471
mrr vals (pred, true): 0.091, 0.207
batch losses (mrrl, rdl): 0.1355139017, 0.0037889055

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 798
rank avg (pred): 0.624 +- 0.453
mrr vals (pred, true): 0.052, 0.054
batch losses (mrrl, rdl): 2.8187e-05, 0.0006000599

Epoch over!
epoch time: 12.101

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 964
rank avg (pred): 0.630 +- 0.448
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 8.70343e-05, 0.0005958662

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 646
rank avg (pred): 0.595 +- 0.480
mrr vals (pred, true): 0.146, 0.237
batch losses (mrrl, rdl): 0.0838136822, 0.0042281365

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 168
rank avg (pred): 0.579 +- 0.482
mrr vals (pred, true): 0.140, 0.053
batch losses (mrrl, rdl): 0.0805614442, 0.0005317693

Epoch over!
epoch time: 12.387

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 450
rank avg (pred): 0.604 +- 0.474
mrr vals (pred, true): 0.111, 0.055
batch losses (mrrl, rdl): 0.0370094962, 0.0007001833

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 238
rank avg (pred): 0.613 +- 0.475
mrr vals (pred, true): 0.123, 0.055
batch losses (mrrl, rdl): 0.0534386933, 0.0006700879

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 641
rank avg (pred): 0.618 +- 0.472
mrr vals (pred, true): 0.113, 0.246
batch losses (mrrl, rdl): 0.1775862873, 0.0047195656

Epoch over!
epoch time: 12.192

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 479
rank avg (pred): 0.602 +- 0.474
mrr vals (pred, true): 0.121, 0.057
batch losses (mrrl, rdl): 0.0501868799, 0.0006586599

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 559
rank avg (pred): 0.617 +- 0.484
mrr vals (pred, true): 0.373, 0.382
batch losses (mrrl, rdl): 0.0007569438, 0.0053381366

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 892
rank avg (pred): 0.616 +- 0.482
mrr vals (pred, true): 0.261, 0.267
batch losses (mrrl, rdl): 0.0003769952, 0.0049870322

Epoch over!
epoch time: 12.222

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.584 +- 0.485
mrr vals (pred, true): 0.192, 0.057

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05461 	 0.02078 	 m..s
    1 	     1 	 0.05464 	 0.03579 	 ~...
   10 	     2 	 0.06199 	 0.04210 	 ~...
   47 	     3 	 0.17148 	 0.04542 	 MISS
   77 	     4 	 0.20410 	 0.04757 	 MISS
   61 	     5 	 0.18667 	 0.04802 	 MISS
   70 	     6 	 0.19594 	 0.04883 	 MISS
    7 	     7 	 0.06129 	 0.04955 	 ~...
   88 	     8 	 0.24928 	 0.04962 	 MISS
   82 	     9 	 0.21326 	 0.04963 	 MISS
   15 	    10 	 0.06289 	 0.04968 	 ~...
   18 	    11 	 0.06744 	 0.04970 	 ~...
    5 	    12 	 0.06048 	 0.04975 	 ~...
   22 	    13 	 0.07327 	 0.04975 	 ~...
   23 	    14 	 0.07933 	 0.04997 	 ~...
   27 	    15 	 0.14071 	 0.04997 	 m..s
   75 	    16 	 0.20067 	 0.05079 	 MISS
    3 	    17 	 0.06036 	 0.05088 	 ~...
   71 	    18 	 0.19609 	 0.05095 	 MISS
   40 	    19 	 0.16204 	 0.05119 	 MISS
   34 	    20 	 0.15474 	 0.05130 	 MISS
   42 	    21 	 0.16369 	 0.05186 	 MISS
   38 	    22 	 0.15962 	 0.05196 	 MISS
   62 	    23 	 0.18759 	 0.05204 	 MISS
   45 	    24 	 0.17018 	 0.05214 	 MISS
    8 	    25 	 0.06168 	 0.05241 	 ~...
   13 	    26 	 0.06245 	 0.05290 	 ~...
   86 	    27 	 0.22755 	 0.05299 	 MISS
   24 	    28 	 0.07947 	 0.05343 	 ~...
   58 	    29 	 0.18425 	 0.05356 	 MISS
   19 	    30 	 0.06834 	 0.05365 	 ~...
   73 	    31 	 0.19925 	 0.05371 	 MISS
   11 	    32 	 0.06224 	 0.05388 	 ~...
   21 	    33 	 0.06942 	 0.05394 	 ~...
   54 	    34 	 0.18197 	 0.05395 	 MISS
   16 	    35 	 0.06715 	 0.05411 	 ~...
   78 	    36 	 0.20710 	 0.05418 	 MISS
    9 	    37 	 0.06183 	 0.05429 	 ~...
   35 	    38 	 0.15778 	 0.05463 	 MISS
   85 	    39 	 0.22732 	 0.05486 	 MISS
   68 	    40 	 0.19478 	 0.05502 	 MISS
   53 	    41 	 0.18178 	 0.05536 	 MISS
    6 	    42 	 0.06112 	 0.05552 	 ~...
   28 	    43 	 0.14114 	 0.05569 	 m..s
   20 	    44 	 0.06861 	 0.05573 	 ~...
   33 	    45 	 0.15447 	 0.05589 	 m..s
   14 	    46 	 0.06250 	 0.05603 	 ~...
   17 	    47 	 0.06736 	 0.05620 	 ~...
   12 	    48 	 0.06236 	 0.05668 	 ~...
   56 	    49 	 0.18313 	 0.05715 	 MISS
   67 	    50 	 0.19213 	 0.05727 	 MISS
   84 	    51 	 0.21969 	 0.05728 	 MISS
   69 	    52 	 0.19492 	 0.05728 	 MISS
   66 	    53 	 0.19195 	 0.05745 	 MISS
   81 	    54 	 0.21032 	 0.05797 	 MISS
    2 	    55 	 0.06022 	 0.05906 	 ~...
    4 	    56 	 0.06045 	 0.05920 	 ~...
   39 	    57 	 0.16142 	 0.06074 	 MISS
   87 	    58 	 0.24454 	 0.06399 	 MISS
   89 	    59 	 0.26710 	 0.09479 	 MISS
   37 	    60 	 0.15917 	 0.17996 	 ~...
   50 	    61 	 0.17882 	 0.18591 	 ~...
   59 	    62 	 0.18507 	 0.20161 	 ~...
   46 	    63 	 0.17136 	 0.20349 	 m..s
   72 	    64 	 0.19680 	 0.21711 	 ~...
   49 	    65 	 0.17649 	 0.22190 	 m..s
   60 	    66 	 0.18561 	 0.22462 	 m..s
   31 	    67 	 0.15253 	 0.23128 	 m..s
   41 	    68 	 0.16271 	 0.23238 	 m..s
   25 	    69 	 0.13984 	 0.23527 	 m..s
   83 	    70 	 0.21885 	 0.23551 	 ~...
   36 	    71 	 0.15834 	 0.23799 	 m..s
   44 	    72 	 0.16831 	 0.23940 	 m..s
   29 	    73 	 0.14304 	 0.24060 	 m..s
   65 	    74 	 0.19124 	 0.24070 	 m..s
   74 	    75 	 0.19964 	 0.24100 	 m..s
   48 	    76 	 0.17449 	 0.24547 	 m..s
   30 	    77 	 0.14684 	 0.24699 	 MISS
   51 	    78 	 0.17993 	 0.24722 	 m..s
   32 	    79 	 0.15260 	 0.24772 	 m..s
   80 	    80 	 0.20778 	 0.24830 	 m..s
   79 	    81 	 0.20720 	 0.24880 	 m..s
   63 	    82 	 0.18941 	 0.25355 	 m..s
   52 	    83 	 0.18171 	 0.25366 	 m..s
   57 	    84 	 0.18362 	 0.26030 	 m..s
   43 	    85 	 0.16557 	 0.26274 	 m..s
   26 	    86 	 0.14020 	 0.26534 	 MISS
   76 	    87 	 0.20245 	 0.26720 	 m..s
   55 	    88 	 0.18281 	 0.27382 	 m..s
   64 	    89 	 0.19056 	 0.27504 	 m..s
   90 	    90 	 0.33453 	 0.33622 	 ~...
  107 	    91 	 0.40218 	 0.35152 	 m..s
   97 	    92 	 0.39581 	 0.36109 	 m..s
   93 	    93 	 0.39224 	 0.37175 	 ~...
  106 	    94 	 0.40186 	 0.37342 	 ~...
  102 	    95 	 0.40028 	 0.37528 	 ~...
  105 	    96 	 0.40114 	 0.37606 	 ~...
  103 	    97 	 0.40038 	 0.37780 	 ~...
   99 	    98 	 0.39948 	 0.37832 	 ~...
  110 	    99 	 0.40876 	 0.38170 	 ~...
   92 	   100 	 0.38158 	 0.38369 	 ~...
   96 	   101 	 0.39356 	 0.39255 	 ~...
   95 	   102 	 0.39338 	 0.39325 	 ~...
  108 	   103 	 0.40236 	 0.39474 	 ~...
  100 	   104 	 0.39978 	 0.39586 	 ~...
   98 	   105 	 0.39623 	 0.39676 	 ~...
  109 	   106 	 0.40858 	 0.40219 	 ~...
   94 	   107 	 0.39305 	 0.40495 	 ~...
   91 	   108 	 0.34697 	 0.41063 	 m..s
  104 	   109 	 0.40100 	 0.41107 	 ~...
  113 	   110 	 0.44066 	 0.41396 	 ~...
  111 	   111 	 0.43201 	 0.43335 	 ~...
  117 	   112 	 0.44776 	 0.43431 	 ~...
  112 	   113 	 0.43880 	 0.43440 	 ~...
  115 	   114 	 0.44517 	 0.43751 	 ~...
  114 	   115 	 0.44352 	 0.43896 	 ~...
  101 	   116 	 0.39993 	 0.44006 	 m..s
  120 	   117 	 0.45373 	 0.44401 	 ~...
  116 	   118 	 0.44716 	 0.44499 	 ~...
  118 	   119 	 0.45071 	 0.44612 	 ~...
  119 	   120 	 0.45200 	 0.44951 	 ~...
==========================================
r_mrr = 0.8488600850105286
r2_mrr = 0.6834275722503662
spearmanr_mrr@5 = 0.9252692461013794
spearmanr_mrr@10 = 0.9367982745170593
spearmanr_mrr@50 = 0.993011474609375
spearmanr_mrr@100 = 0.9254506230354309
spearmanr_mrr@All = 0.9333354234695435
==========================================
test time: 0.391
Done Testing dataset Kinships
total time taken: 188.63124752044678
training time taken: 181.76863551139832
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8489)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.6834)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9253)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9368)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9930)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.9255)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9333)}}, 'test_loss': {'DistMult': {'Kinships': 9.199754473695066}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 9746375646397996
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1167, 329, 79, 739, 593, 723, 231, 545, 309, 845, 698, 418, 65, 606, 84, 46, 1128, 1048, 915, 1134, 943, 840, 1017, 669, 821, 381, 810, 987, 842, 477, 12, 668, 147, 205, 125, 294, 286, 1001, 434, 687, 129, 707, 4, 240, 60, 899, 1179, 53, 480, 8, 2, 499, 392, 85, 1089, 813, 190, 106, 1209, 939, 248, 311, 1140, 478, 218, 645, 543, 524, 647, 308, 704, 1163, 226, 442, 1051, 1143, 1211, 729, 276, 41, 1176, 1055, 555, 201, 1117, 1170, 140, 626, 1212, 378, 176, 568, 1027, 1103, 193, 516, 155, 721, 981, 1174, 886, 651, 1070, 763, 627, 650, 1138, 942, 1186, 825, 426, 73, 1189, 1120, 802, 55, 920, 28, 918, 123, 498]
valid_ids (0): []
train_ids (1094): [1214, 379, 624, 951, 561, 330, 642, 881, 115, 66, 401, 991, 214, 1037, 181, 1173, 590, 307, 796, 888, 1210, 277, 528, 353, 831, 163, 768, 388, 697, 210, 851, 678, 900, 209, 760, 709, 296, 1191, 1041, 877, 969, 549, 998, 23, 130, 14, 132, 879, 490, 634, 441, 1095, 520, 930, 482, 793, 649, 278, 718, 858, 811, 445, 394, 166, 578, 790, 346, 944, 1203, 1145, 622, 798, 803, 1151, 596, 563, 1091, 13, 458, 518, 570, 773, 658, 15, 1015, 198, 1164, 875, 786, 456, 252, 1102, 457, 454, 670, 86, 1193, 199, 931, 435, 154, 38, 247, 334, 841, 836, 238, 419, 871, 685, 323, 996, 592, 859, 466, 955, 267, 856, 400, 769, 1129, 1177, 903, 986, 883, 491, 523, 715, 157, 743, 961, 664, 756, 680, 179, 666, 64, 259, 728, 688, 1094, 101, 701, 373, 783, 994, 58, 504, 675, 356, 317, 800, 475, 682, 677, 325, 312, 1078, 94, 548, 372, 791, 459, 290, 933, 767, 200, 74, 78, 119, 156, 118, 162, 358, 742, 1202, 967, 422, 429, 185, 342, 1026, 249, 175, 661, 139, 173, 817, 1034, 148, 1108, 17, 1036, 88, 1, 6, 1004, 246, 905, 387, 510, 243, 1118, 174, 1074, 1086, 572, 988, 919, 801, 486, 260, 646, 1201, 1008, 272, 902, 618, 740, 684, 500, 546, 1049, 494, 241, 603, 3, 979, 1058, 288, 135, 391, 415, 211, 508, 759, 517, 982, 1052, 83, 1156, 1032, 1110, 313, 735, 892, 167, 228, 1038, 62, 713, 1077, 519, 109, 237, 522, 749, 965, 348, 389, 253, 124, 737, 122, 215, 245, 295, 702, 576, 908, 950, 142, 357, 133, 623, 619, 1047, 47, 1035, 533, 1013, 285, 302, 131, 421, 439, 127, 946, 361, 301, 1127, 750, 316, 1107, 872, 45, 54, 861, 385, 492, 582, 695, 693, 452, 44, 992, 474, 1132, 909, 481, 1093, 273, 239, 1040, 7, 540, 662, 1012, 380, 34, 714, 159, 1050, 894, 414, 635, 959, 51, 1072, 298, 692, 581, 970, 416, 826, 954, 878, 244, 852, 183, 844, 922, 332, 483, 1061, 613, 656, 374, 77, 300, 324, 962, 1161, 39, 1162, 972, 192, 912, 364, 819, 876, 397, 1084, 489, 1081, 1105, 462, 29, 496, 799, 997, 1206, 449, 539, 754, 553, 406, 639, 604, 1087, 1147, 460, 1188, 1064, 365, 591, 676, 863, 839, 691, 417, 487, 515, 37, 91, 168, 1204, 367, 161, 751, 87, 507, 432, 509, 1022, 617, 377, 68, 476, 1130, 112, 99, 1131, 1045, 121, 32, 734, 537, 279, 679, 512, 1075, 340, 868, 726, 804, 110, 443, 484, 690, 337, 1199, 319, 448, 806, 587, 1207, 59, 1180, 1005, 536, 874, 48, 1028, 755, 468, 1009, 706, 938, 990, 404, 424, 72, 832, 194, 1139, 322, 1079, 779, 1115, 621, 223, 1113, 144, 265, 502, 262, 164, 1141, 384, 667, 673, 315, 873, 1080, 327, 846, 923, 689, 889, 141, 440, 36, 1144, 648, 61, 203, 830, 978, 1200, 178, 146, 150, 565, 1205, 1178, 220, 1148, 453, 1184, 562, 501, 652, 571, 1057, 25, 1106, 824, 532, 18, 172, 386, 822, 1063, 197, 829, 229, 398, 616, 1160, 665, 632, 927, 344, 1194, 1016, 789, 747, 674, 864, 525, 848, 631, 281, 828, 347, 1066, 816, 446, 1175, 196, 409, 188, 977, 1185, 663, 93, 1071, 89, 940, 33, 369, 1100, 1169, 1062, 69, 823, 1114, 1116, 67, 960, 455, 352, 236, 827, 834, 1073, 1195, 165, 1125, 866, 580, 999, 584, 1025, 436, 808, 1136, 376, 463, 363, 552, 867, 686, 270, 535, 80, 1135, 608, 995, 897, 630, 683, 534, 22, 629, 941, 812, 907, 657, 271, 287, 493, 473, 1159, 741, 1183, 395, 503, 611, 855, 405, 904, 331, 306, 521, 513, 469, 428, 948, 1197, 202, 612, 92, 182, 338, 382, 1190, 564, 318, 814, 1146, 339, 153, 1150, 283, 1068, 105, 1152, 917, 76, 1192, 297, 497, 488, 470, 1056, 292, 5, 350, 1213, 1098, 722, 774, 1082, 560, 926, 1097, 833, 609, 314, 396, 929, 731, 880, 427, 780, 765, 542, 579, 42, 1096, 1003, 35, 56, 643, 974, 1119, 355, 711, 558, 547, 720, 901, 890, 180, 752, 100, 304, 408, 354, 43, 1208, 26, 934, 820, 70, 569, 11, 1023, 1067, 818, 640, 256, 310, 186, 605, 1033, 895, 264, 653, 506, 250, 1133, 976, 1149, 993, 230, 321, 225, 853, 138, 465, 328, 30, 705, 407, 869, 1030, 169, 1007, 431, 104, 968, 438, 75, 1109, 757, 937, 849, 1024, 600, 40, 654, 712, 402, 914, 594, 187, 343, 730, 255, 764, 207, 805, 589, 191, 1099, 1153, 116, 857, 781, 50, 766, 732, 1142, 550, 275, 413, 1172, 97, 703, 336, 1006, 785, 370, 637, 544, 947, 10, 699, 1122, 189, 217, 303, 251, 1171, 511, 126, 1069, 1124, 744, 838, 807, 433, 1053, 794, 966, 1154, 1076, 222, 538, 242, 410, 206, 924, 708, 160, 911, 753, 375, 27, 636, 117, 610, 633, 925, 280, 450, 120, 49, 1168, 1029, 566, 1059, 1181, 1000, 1002, 913, 102, 638, 655, 216, 556, 1137, 795, 696, 599, 447, 885, 158, 472, 351, 577, 149, 615, 531, 583, 284, 758, 57, 671, 644, 541, 326, 9, 854, 588, 1155, 882, 145, 906, 299, 1187, 896, 20, 865, 261, 921, 1090, 614, 461, 952, 1011, 305, 985, 527, 788, 430, 1092, 108, 1157, 776, 557, 607, 575, 898, 554, 221, 850, 574, 291, 1010, 710, 598, 916, 467, 98, 213, 371, 815, 771, 16, 893, 177, 748, 862, 514, 366, 204, 257, 551, 293, 641, 341, 320, 96, 843, 1198, 111, 597, 82, 601, 1101, 567, 208, 945, 887, 1043, 1018, 891, 234, 778, 24, 1021, 274, 953, 736, 681, 983, 333, 787, 949, 1060, 1166, 263, 1019, 1085, 1112, 725, 95, 266, 797, 761, 254, 775, 936, 727, 19, 719, 485, 971, 1031, 345, 349, 63, 784, 745, 103, 659, 471, 235, 1165, 772, 1044, 128, 700, 1039, 809, 151, 530, 1123, 1046, 137, 717, 479, 620, 383, 625, 733, 31, 770, 1083, 152, 269, 777, 90, 425, 1196, 444, 884, 660, 233, 762, 411, 1020, 989, 628, 975, 390, 170, 1126, 1121, 716, 694, 136, 224, 738, 107, 113, 282, 935, 505, 792, 1054, 268, 184, 52, 870, 362, 956, 360, 602, 420, 227, 335, 928, 359, 212, 289, 232, 910, 403, 81, 219, 782, 437, 0, 984, 393, 258, 526, 21, 964, 860, 1104, 195, 957, 134, 963, 585, 495, 835, 1158, 464, 114, 595, 399, 573, 171, 1065, 973, 1042, 451, 559, 746, 423, 412, 932, 837, 71, 1014, 672, 368, 586, 143, 529, 1111, 1088, 724, 847, 980, 1182, 958]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3625581143562470
the save name prefix for this run is:  chkpt-ID_3625581143562470_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 806
rank avg (pred): 0.542 +- 0.003
mrr vals (pred, true): 0.018, 0.054
batch losses (mrrl, rdl): 0.0, 0.0002057853

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 604
rank avg (pred): 0.326 +- 0.226
mrr vals (pred, true): 0.169, 0.257
batch losses (mrrl, rdl): 0.0, 0.0012538942

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 109
rank avg (pred): 0.223 +- 0.210
mrr vals (pred, true): 0.349, 0.250
batch losses (mrrl, rdl): 0.0, 0.0004216117

Epoch over!
epoch time: 12.038

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1062
rank avg (pred): 0.042 +- 0.039
mrr vals (pred, true): 0.459, 0.432
batch losses (mrrl, rdl): 0.0, 4.4638e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 405
rank avg (pred): 0.246 +- 0.231
mrr vals (pred, true): 0.359, 0.051
batch losses (mrrl, rdl): 0.0, 0.0007669017

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 206
rank avg (pred): 0.228 +- 0.227
mrr vals (pred, true): 0.395, 0.053
batch losses (mrrl, rdl): 0.0, 0.0009133705

Epoch over!
epoch time: 12.011

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 137
rank avg (pred): 0.272 +- 0.227
mrr vals (pred, true): 0.259, 0.265
batch losses (mrrl, rdl): 0.0, 0.0008726833

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 264
rank avg (pred): 0.098 +- 0.090
mrr vals (pred, true): 0.391, 0.436
batch losses (mrrl, rdl): 0.0, 5.03354e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 515
rank avg (pred): 0.033 +- 0.033
mrr vals (pred, true): 0.535, 0.378
batch losses (mrrl, rdl): 0.0, 2.03966e-05

Epoch over!
epoch time: 11.691

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 453
rank avg (pred): 0.249 +- 0.238
mrr vals (pred, true): 0.358, 0.057
batch losses (mrrl, rdl): 0.0, 0.0006761517

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 683
rank avg (pred): 0.228 +- 0.232
mrr vals (pred, true): 0.405, 0.056
batch losses (mrrl, rdl): 0.0, 0.0008776998

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1042
rank avg (pred): 0.260 +- 0.243
mrr vals (pred, true): 0.349, 0.061
batch losses (mrrl, rdl): 0.0, 0.0005982241

Epoch over!
epoch time: 11.804

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 748
rank avg (pred): 0.072 +- 0.072
mrr vals (pred, true): 0.473, 0.421
batch losses (mrrl, rdl): 0.0, 6.5378e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 672
rank avg (pred): 0.286 +- 0.231
mrr vals (pred, true): 0.257, 0.058
batch losses (mrrl, rdl): 0.0, 0.0004090888

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1126
rank avg (pred): 0.213 +- 0.205
mrr vals (pred, true): 0.394, 0.055
batch losses (mrrl, rdl): 0.0, 0.0010675117

Epoch over!
epoch time: 11.987

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 409
rank avg (pred): 0.238 +- 0.242
mrr vals (pred, true): 0.426, 0.060
batch losses (mrrl, rdl): 1.4104270935, 0.0006447516

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 36
rank avg (pred): 0.074 +- 0.066
mrr vals (pred, true): 0.413, 0.431
batch losses (mrrl, rdl): 0.0031156382, 7.0371e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 847
rank avg (pred): 0.559 +- 0.199
mrr vals (pred, true): 0.052, 0.059
batch losses (mrrl, rdl): 4.67108e-05, 0.0003469189

Epoch over!
epoch time: 12.221

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 635
rank avg (pred): 0.405 +- 0.244
mrr vals (pred, true): 0.114, 0.256
batch losses (mrrl, rdl): 0.2015475184, 0.0021335469

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 113
rank avg (pred): 0.357 +- 0.215
mrr vals (pred, true): 0.139, 0.182
batch losses (mrrl, rdl): 0.0181204714, 0.0007657157

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 214
rank avg (pred): 0.330 +- 0.226
mrr vals (pred, true): 0.188, 0.053
batch losses (mrrl, rdl): 0.1912783831, 0.0002371806

Epoch over!
epoch time: 11.882

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1157
rank avg (pred): 0.256 +- 0.232
mrr vals (pred, true): 0.396, 0.374
batch losses (mrrl, rdl): 0.0049496158, 0.0010123129

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 822
rank avg (pred): 0.191 +- 0.171
mrr vals (pred, true): 0.368, 0.397
batch losses (mrrl, rdl): 0.0082331849, 0.000460981

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 285
rank avg (pred): 0.080 +- 0.072
mrr vals (pred, true): 0.447, 0.455
batch losses (mrrl, rdl): 0.0005232482, 1.50974e-05

Epoch over!
epoch time: 12.112

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 890
rank avg (pred): 0.461 +- 0.142
mrr vals (pred, true): 0.060, 0.054
batch losses (mrrl, rdl): 0.0010776133, 4.48659e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1047
rank avg (pred): 0.348 +- 0.207
mrr vals (pred, true): 0.137, 0.050
batch losses (mrrl, rdl): 0.0764340311, 0.0002570711

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 618
rank avg (pred): 0.359 +- 0.202
mrr vals (pred, true): 0.130, 0.263
batch losses (mrrl, rdl): 0.1785739362, 0.001454583

Epoch over!
epoch time: 12.208

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 871
rank avg (pred): 0.483 +- 0.099
mrr vals (pred, true): 0.030, 0.051
batch losses (mrrl, rdl): 0.0039563607, 6.9834e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 794
rank avg (pred): 0.484 +- 0.156
mrr vals (pred, true): 0.059, 0.055
batch losses (mrrl, rdl): 0.0008786622, 5.74822e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 22
rank avg (pred): 0.082 +- 0.073
mrr vals (pred, true): 0.420, 0.436
batch losses (mrrl, rdl): 0.0026008077, 2.16275e-05

Epoch over!
epoch time: 12.111

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 23
rank avg (pred): 0.086 +- 0.077
mrr vals (pred, true): 0.419, 0.431
batch losses (mrrl, rdl): 0.0014187203, 2.56043e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 567
rank avg (pred): 0.363 +- 0.190
mrr vals (pred, true): 0.124, 0.229
batch losses (mrrl, rdl): 0.1099108979, 0.0015669933

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 202
rank avg (pred): 0.354 +- 0.191
mrr vals (pred, true): 0.131, 0.058
batch losses (mrrl, rdl): 0.0655345395, 0.0001661122

Epoch over!
epoch time: 12.335

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 656
rank avg (pred): 0.362 +- 0.188
mrr vals (pred, true): 0.128, 0.045
batch losses (mrrl, rdl): 0.0610349253, 0.0002321998

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 500
rank avg (pred): 0.361 +- 0.309
mrr vals (pred, true): 0.385, 0.373
batch losses (mrrl, rdl): 0.0014375249, 0.0022219708

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 631
rank avg (pred): 0.332 +- 0.196
mrr vals (pred, true): 0.165, 0.220
batch losses (mrrl, rdl): 0.0294304825, 0.0009383412

Epoch over!
epoch time: 12.087

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 262
rank avg (pred): 0.053 +- 0.046
mrr vals (pred, true): 0.454, 0.447
batch losses (mrrl, rdl): 0.0004804092, 3.499e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 160
rank avg (pred): 0.328 +- 0.193
mrr vals (pred, true): 0.155, 0.236
batch losses (mrrl, rdl): 0.0649346486, 0.0011246492

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 872
rank avg (pred): 0.468 +- 0.063
mrr vals (pred, true): 0.024, 0.059
batch losses (mrrl, rdl): 0.0067540626, 7.75141e-05

Epoch over!
epoch time: 12.198

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 592
rank avg (pred): 0.317 +- 0.196
mrr vals (pred, true): 0.182, 0.232
batch losses (mrrl, rdl): 0.0248510428, 0.0010444367

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 48
rank avg (pred): 0.092 +- 0.078
mrr vals (pred, true): 0.427, 0.429
batch losses (mrrl, rdl): 2.85419e-05, 2.86076e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 159
rank avg (pred): 0.333 +- 0.202
mrr vals (pred, true): 0.165, 0.252
batch losses (mrrl, rdl): 0.0751524568, 0.0013783335

Epoch over!
epoch time: 12.0

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 823
rank avg (pred): 0.175 +- 0.150
mrr vals (pred, true): 0.413, 0.430
batch losses (mrrl, rdl): 0.0030005388, 0.0003720692

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 465
rank avg (pred): 0.322 +- 0.182
mrr vals (pred, true): 0.146, 0.047
batch losses (mrrl, rdl): 0.0924241468, 0.0003907671

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1124
rank avg (pred): 0.322 +- 0.186
mrr vals (pred, true): 0.161, 0.048
batch losses (mrrl, rdl): 0.1231437176, 0.0004094372

Epoch over!
epoch time: 11.947

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.377 +- 0.161
mrr vals (pred, true): 0.123, 0.254

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   11 	     0 	 0.08250 	 0.02078 	 m..s
    4 	     1 	 0.04702 	 0.04019 	 ~...
    8 	     2 	 0.05385 	 0.04271 	 ~...
    5 	     3 	 0.04722 	 0.04299 	 ~...
   30 	     4 	 0.15003 	 0.04634 	 MISS
    1 	     5 	 0.04349 	 0.04720 	 ~...
    0 	     6 	 0.04007 	 0.04832 	 ~...
   69 	     7 	 0.16301 	 0.04883 	 MISS
   32 	     8 	 0.15064 	 0.04907 	 MISS
   54 	     9 	 0.15519 	 0.04935 	 MISS
    6 	    10 	 0.05121 	 0.04958 	 ~...
   79 	    11 	 0.17027 	 0.04963 	 MISS
   49 	    12 	 0.15474 	 0.04967 	 MISS
    7 	    13 	 0.05199 	 0.04985 	 ~...
   69 	    14 	 0.16301 	 0.05013 	 MISS
   41 	    15 	 0.15287 	 0.05042 	 MISS
   18 	    16 	 0.14480 	 0.05135 	 m..s
   22 	    17 	 0.14643 	 0.05186 	 m..s
   62 	    18 	 0.15878 	 0.05196 	 MISS
    3 	    19 	 0.04585 	 0.05197 	 ~...
   26 	    20 	 0.14705 	 0.05276 	 m..s
   69 	    21 	 0.16301 	 0.05290 	 MISS
   44 	    22 	 0.15309 	 0.05298 	 MISS
   55 	    23 	 0.15586 	 0.05314 	 MISS
   59 	    24 	 0.15681 	 0.05325 	 MISS
   46 	    25 	 0.15403 	 0.05342 	 MISS
   15 	    26 	 0.14326 	 0.05381 	 m..s
   57 	    27 	 0.15630 	 0.05394 	 MISS
   67 	    28 	 0.16194 	 0.05412 	 MISS
   65 	    29 	 0.16119 	 0.05451 	 MISS
    9 	    30 	 0.06157 	 0.05462 	 ~...
   48 	    31 	 0.15432 	 0.05569 	 m..s
   29 	    32 	 0.14979 	 0.05574 	 m..s
   25 	    33 	 0.14688 	 0.05586 	 m..s
   60 	    34 	 0.15787 	 0.05595 	 MISS
   28 	    35 	 0.14882 	 0.05616 	 m..s
   10 	    36 	 0.06467 	 0.05683 	 ~...
    2 	    37 	 0.04474 	 0.05736 	 ~...
   47 	    38 	 0.15420 	 0.05757 	 m..s
   19 	    39 	 0.14514 	 0.05760 	 m..s
   34 	    40 	 0.15093 	 0.05772 	 m..s
   69 	    41 	 0.16301 	 0.05804 	 MISS
   69 	    42 	 0.16301 	 0.05807 	 MISS
   13 	    43 	 0.13362 	 0.05848 	 m..s
   78 	    44 	 0.16949 	 0.05963 	 MISS
   77 	    45 	 0.16865 	 0.06057 	 MISS
   56 	    46 	 0.15627 	 0.06144 	 m..s
   39 	    47 	 0.15267 	 0.06190 	 m..s
   61 	    48 	 0.15831 	 0.08177 	 m..s
   16 	    49 	 0.14332 	 0.16188 	 ~...
   43 	    50 	 0.15295 	 0.17431 	 ~...
   33 	    51 	 0.15081 	 0.18591 	 m..s
   82 	    52 	 0.33788 	 0.19851 	 MISS
   23 	    53 	 0.14656 	 0.20089 	 m..s
   24 	    54 	 0.14673 	 0.21389 	 m..s
   53 	    55 	 0.15496 	 0.21466 	 m..s
   42 	    56 	 0.15293 	 0.21715 	 m..s
   37 	    57 	 0.15215 	 0.21948 	 m..s
   69 	    58 	 0.16301 	 0.22230 	 m..s
   69 	    59 	 0.16301 	 0.22764 	 m..s
   38 	    60 	 0.15252 	 0.22982 	 m..s
   50 	    61 	 0.15476 	 0.23149 	 m..s
   45 	    62 	 0.15360 	 0.23266 	 m..s
   40 	    63 	 0.15276 	 0.23455 	 m..s
   66 	    64 	 0.16149 	 0.23496 	 m..s
   27 	    65 	 0.14747 	 0.23649 	 m..s
   58 	    66 	 0.15670 	 0.23872 	 m..s
   20 	    67 	 0.14559 	 0.23977 	 m..s
   21 	    68 	 0.14596 	 0.24248 	 m..s
   35 	    69 	 0.15194 	 0.24269 	 m..s
   36 	    70 	 0.15195 	 0.24272 	 m..s
   68 	    71 	 0.16225 	 0.24281 	 m..s
   17 	    72 	 0.14343 	 0.24784 	 MISS
   52 	    73 	 0.15481 	 0.24818 	 m..s
   12 	    74 	 0.12314 	 0.25379 	 MISS
   31 	    75 	 0.15027 	 0.25432 	 MISS
   63 	    76 	 0.15978 	 0.26206 	 MISS
   14 	    77 	 0.14151 	 0.26746 	 MISS
   69 	    78 	 0.16301 	 0.26922 	 MISS
   51 	    79 	 0.15479 	 0.27363 	 MISS
   64 	    80 	 0.15982 	 0.29102 	 MISS
   81 	    81 	 0.30943 	 0.30334 	 ~...
   83 	    82 	 0.37484 	 0.30493 	 m..s
   86 	    83 	 0.38974 	 0.35878 	 m..s
   87 	    84 	 0.39109 	 0.36285 	 ~...
   90 	    85 	 0.39490 	 0.36521 	 ~...
   80 	    86 	 0.29750 	 0.36815 	 m..s
   89 	    87 	 0.39313 	 0.37824 	 ~...
   84 	    88 	 0.38879 	 0.38821 	 ~...
   93 	    89 	 0.40036 	 0.39438 	 ~...
   88 	    90 	 0.39117 	 0.39474 	 ~...
   92 	    91 	 0.39983 	 0.40219 	 ~...
   85 	    92 	 0.38917 	 0.40923 	 ~...
   96 	    93 	 0.42298 	 0.41107 	 ~...
   95 	    94 	 0.41500 	 0.41130 	 ~...
   91 	    95 	 0.39913 	 0.41169 	 ~...
  108 	    96 	 0.43742 	 0.41563 	 ~...
   94 	    97 	 0.40178 	 0.41782 	 ~...
  110 	    98 	 0.44213 	 0.42698 	 ~...
  104 	    99 	 0.43284 	 0.42728 	 ~...
  111 	   100 	 0.44296 	 0.42959 	 ~...
  103 	   101 	 0.43252 	 0.43275 	 ~...
   99 	   102 	 0.42725 	 0.43359 	 ~...
  109 	   103 	 0.44165 	 0.43597 	 ~...
  113 	   104 	 0.44345 	 0.43877 	 ~...
  106 	   105 	 0.43584 	 0.43896 	 ~...
   97 	   106 	 0.42710 	 0.43902 	 ~...
  115 	   107 	 0.44639 	 0.43967 	 ~...
  105 	   108 	 0.43466 	 0.44009 	 ~...
  119 	   109 	 0.45866 	 0.44110 	 ~...
  114 	   110 	 0.44617 	 0.44173 	 ~...
   98 	   111 	 0.42722 	 0.44232 	 ~...
  100 	   112 	 0.42845 	 0.44306 	 ~...
  116 	   113 	 0.44933 	 0.44382 	 ~...
  102 	   114 	 0.43092 	 0.44522 	 ~...
  107 	   115 	 0.43737 	 0.44534 	 ~...
  112 	   116 	 0.44330 	 0.44730 	 ~...
  118 	   117 	 0.45608 	 0.44777 	 ~...
  101 	   118 	 0.42977 	 0.45747 	 ~...
  120 	   119 	 0.46309 	 0.45760 	 ~...
  117 	   120 	 0.45067 	 0.45836 	 ~...
==========================================
r_mrr = 0.8891711831092834
r2_mrr = 0.7844066619873047
spearmanr_mrr@5 = 0.9197723865509033
spearmanr_mrr@10 = 0.971939742565155
spearmanr_mrr@50 = 0.9684930443763733
spearmanr_mrr@100 = 0.9022185206413269
spearmanr_mrr@All = 0.9183885455131531
==========================================
test time: 0.405
Done Testing dataset Kinships
total time taken: 188.2253577709198
training time taken: 181.0968623161316
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8892)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7844)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9198)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9719)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9685)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.9022)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9184)}}, 'test_loss': {'DistMult': {'Kinships': 6.878864426005748}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 7268825403311360
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [114, 464, 951, 167, 838, 264, 605, 883, 385, 362, 480, 257, 671, 399, 949, 635, 495, 267, 327, 434, 469, 232, 406, 796, 127, 903, 290, 386, 839, 205, 510, 1130, 522, 889, 943, 524, 247, 1104, 665, 870, 484, 725, 1165, 1131, 855, 530, 1142, 479, 997, 259, 142, 731, 1013, 965, 1173, 905, 1149, 502, 754, 1029, 825, 514, 305, 314, 994, 494, 1084, 897, 1139, 189, 75, 345, 584, 597, 970, 578, 273, 186, 1175, 105, 64, 1113, 860, 794, 518, 251, 645, 1001, 1186, 37, 433, 297, 979, 526, 1023, 1026, 1191, 66, 938, 262, 203, 892, 225, 1159, 335, 431, 1109, 587, 696, 644, 676, 161, 873, 1058, 109, 963, 588, 972, 1018, 298, 150]
valid_ids (0): []
train_ids (1094): [737, 1086, 712, 791, 470, 988, 374, 246, 1033, 413, 230, 284, 1208, 924, 913, 1079, 261, 1027, 556, 1020, 710, 1, 72, 48, 172, 577, 1138, 319, 581, 663, 133, 819, 307, 593, 455, 65, 682, 772, 210, 702, 801, 175, 25, 738, 888, 183, 170, 740, 332, 199, 858, 223, 638, 121, 705, 637, 35, 793, 811, 953, 786, 300, 289, 238, 893, 500, 1148, 920, 1016, 196, 700, 1071, 1164, 561, 436, 869, 643, 85, 1178, 709, 1017, 201, 437, 438, 106, 371, 1060, 912, 499, 70, 1065, 271, 124, 782, 1147, 533, 751, 56, 392, 887, 1050, 824, 166, 23, 727, 329, 389, 277, 6, 143, 572, 458, 564, 901, 1196, 1195, 285, 160, 188, 103, 67, 847, 917, 973, 342, 693, 1106, 129, 102, 1068, 1036, 1161, 357, 922, 0, 1085, 1019, 942, 844, 616, 685, 1102, 1205, 1028, 258, 803, 907, 789, 218, 746, 641, 989, 932, 1077, 153, 14, 854, 51, 785, 68, 842, 1140, 711, 465, 366, 435, 704, 720, 107, 417, 862, 809, 1108, 229, 573, 235, 1123, 1194, 1204, 352, 1074, 178, 214, 759, 835, 843, 180, 475, 274, 872, 453, 304, 978, 675, 1133, 110, 960, 269, 139, 539, 615, 781, 574, 805, 954, 804, 1037, 5, 36, 303, 381, 10, 909, 850, 351, 969, 29, 977, 1180, 940, 910, 1115, 89, 242, 836, 1078, 764, 719, 516, 292, 163, 1124, 1193, 123, 625, 830, 47, 513, 456, 1152, 380, 477, 856, 63, 322, 713, 1129, 1031, 220, 1064, 84, 370, 78, 55, 492, 760, 1080, 817, 598, 1177, 871, 1083, 1141, 12, 604, 411, 197, 840, 239, 278, 559, 545, 906, 1155, 523, 363, 1057, 250, 895, 648, 620, 831, 689, 1111, 1046, 46, 981, 387, 686, 761, 294, 155, 253, 118, 491, 1095, 1054, 899, 1039, 591, 945, 213, 86, 621, 485, 1048, 337, 962, 111, 557, 1214, 624, 313, 349, 445, 373, 651, 444, 4, 652, 777, 1171, 224, 308, 779, 99, 173, 410, 324, 569, 885, 424, 664, 276, 1120, 310, 1009, 355, 339, 657, 776, 54, 33, 995, 1000, 179, 254, 404, 890, 642, 1200, 863, 1066, 1192, 87, 402, 22, 1047, 1117, 369, 1024, 159, 553, 1088, 565, 286, 265, 359, 320, 612, 1097, 1090, 1010, 136, 575, 1127, 993, 724, 618, 818, 900, 291, 815, 134, 1059, 157, 698, 875, 971, 1153, 611, 3, 209, 991, 266, 535, 1005, 946, 808, 623, 1135, 681, 821, 916, 763, 1211, 1014, 877, 252, 45, 190, 1157, 34, 976, 586, 959, 1126, 145, 420, 62, 957, 1012, 443, 282, 472, 18, 1212, 745, 138, 741, 233, 521, 1032, 119, 736, 39, 659, 662, 739, 442, 1021, 93, 268, 1082, 212, 354, 542, 200, 439, 204, 228, 1052, 547, 688, 774, 857, 1184, 914, 462, 589, 16, 76, 1063, 832, 829, 1183, 636, 617, 83, 1101, 493, 390, 944, 592, 409, 629, 116, 128, 400, 783, 104, 987, 837, 21, 936, 653, 275, 600, 240, 628, 1201, 998, 607, 904, 185, 8, 926, 98, 807, 800, 488, 806, 376, 50, 596, 350, 1107, 911, 501, 1150, 930, 112, 551, 896, 100, 487, 429, 918, 227, 706, 130, 790, 19, 137, 146, 958, 115, 394, 94, 1174, 343, 852, 861, 208, 947, 1210, 234, 1007, 517, 1045, 53, 921, 428, 184, 672, 463, 418, 1125, 716, 9, 1143, 1176, 554, 568, 154, 723, 602, 1003, 1197, 865, 966, 646, 309, 288, 538, 627, 483, 610, 679, 216, 44, 1006, 982, 79, 407, 853, 1053, 243, 822, 415, 849, 563, 703, 721, 38, 732, 931, 1116, 140, 459, 941, 511, 1185, 1051, 631, 955, 583, 828, 735, 177, 622, 1092, 90, 207, 687, 1022, 722, 669, 714, 202, 255, 498, 1002, 915, 684, 579, 1011, 718, 908, 1154, 697, 670, 331, 325, 729, 169, 403, 353, 975, 1114, 473, 347, 509, 919, 476, 287, 874, 1035, 1076, 1030, 466, 1073, 762, 328, 384, 747, 851, 1158, 396, 1069, 1167, 999, 540, 1110, 823, 742, 626, 950, 367, 211, 414, 395, 1203, 421, 750, 765, 408, 531, 1156, 678, 787, 194, 30, 1137, 28, 97, 80, 534, 474, 377, 323, 548, 401, 74, 1041, 985, 486, 176, 650, 1100, 58, 92, 302, 933, 77, 1202, 40, 990, 489, 576, 496, 528, 24, 881, 749, 1199, 13, 582, 222, 773, 20, 15, 59, 263, 1144, 471, 879, 26, 144, 792, 1172, 1038, 1213, 364, 769, 683, 81, 968, 894, 859, 171, 427, 599, 507, 694, 231, 505, 1081, 346, 449, 802, 733, 440, 1207, 541, 752, 690, 344, 1187, 147, 101, 864, 668, 1061, 379, 468, 447, 326, 43, 1163, 91, 1189, 1128, 580, 237, 236, 775, 1098, 187, 1004, 378, 546, 1166, 317, 726, 937, 281, 544, 249, 245, 543, 397, 1087, 788, 673, 333, 241, 165, 193, 226, 368, 658, 120, 519, 1103, 318, 336, 192, 69, 221, 886, 656, 1008, 148, 164, 82, 748, 570, 640, 1099, 1043, 1062, 358, 1160, 206, 529, 95, 983, 734, 717, 608, 467, 934, 964, 191, 550, 778, 301, 609, 71, 884, 996, 168, 730, 757, 1067, 1198, 244, 256, 590, 61, 633, 1188, 867, 753, 1181, 283, 1040, 131, 135, 1056, 634, 866, 149, 992, 1162, 215, 299, 898, 7, 356, 1070, 695, 383, 96, 1132, 1146, 360, 306, 986, 929, 549, 701, 948, 571, 1136, 174, 503, 667, 552, 743, 1145, 1091, 31, 260, 446, 482, 388, 461, 1034, 41, 432, 27, 768, 60, 11, 606, 594, 295, 639, 460, 771, 152, 567, 699, 820, 1134, 655, 984, 848, 393, 311, 537, 452, 1096, 692, 923, 334, 691, 654, 448, 430, 677, 755, 756, 158, 845, 361, 382, 122, 661, 279, 614, 766, 770, 419, 902, 1209, 666, 195, 555, 296, 649, 497, 315, 1015, 57, 562, 88, 1072, 1093, 797, 1025, 481, 1118, 647, 833, 660, 372, 812, 113, 451, 536, 391, 151, 312, 141, 270, 1170, 891, 426, 454, 532, 952, 450, 1055, 365, 595, 2, 1119, 619, 156, 961, 841, 1094, 1049, 826, 585, 398, 272, 416, 882, 1112, 880, 967, 744, 527, 182, 707, 330, 217, 1105, 348, 425, 928, 125, 767, 506, 1151, 1075, 939, 248, 504, 799, 1121, 520, 32, 632, 52, 827, 1179, 1089, 876, 117, 49, 758, 1182, 674, 316, 108, 708, 956, 925, 375, 798, 478, 17, 1044, 566, 601, 980, 423, 321, 1168, 508, 613, 1042, 219, 715, 834, 868, 422, 603, 878, 515, 457, 814, 1206, 680, 795, 405, 132, 340, 1190, 441, 560, 280, 1169, 558, 73, 490, 412, 338, 816, 846, 341, 1122, 126, 512, 198, 813, 42, 630, 525, 728, 181, 162, 784, 780, 935, 293, 974, 810, 927]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1869797868787988
the save name prefix for this run is:  chkpt-ID_1869797868787988_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 312
rank avg (pred): 0.511 +- 0.006
mrr vals (pred, true): 0.019, 0.441
batch losses (mrrl, rdl): 0.0, 0.0044166036

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 465
rank avg (pred): 0.273 +- 0.180
mrr vals (pred, true): 0.143, 0.047
batch losses (mrrl, rdl): 0.0, 0.000693807

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 44
rank avg (pred): 0.048 +- 0.042
mrr vals (pred, true): 0.423, 0.431
batch losses (mrrl, rdl): 0.0, 1.6246e-06

Epoch over!
epoch time: 12.113

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 768
rank avg (pred): 0.374 +- 0.279
mrr vals (pred, true): 0.256, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001092218

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 178
rank avg (pred): 0.256 +- 0.219
mrr vals (pred, true): 0.316, 0.051
batch losses (mrrl, rdl): 0.0, 0.0006648111

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 10
rank avg (pred): 0.057 +- 0.056
mrr vals (pred, true): 0.462, 0.438
batch losses (mrrl, rdl): 0.0, 6.137e-07

Epoch over!
epoch time: 11.925

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 748
rank avg (pred): 0.074 +- 0.066
mrr vals (pred, true): 0.402, 0.421
batch losses (mrrl, rdl): 0.0, 7.8352e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 48
rank avg (pred): 0.067 +- 0.068
mrr vals (pred, true): 0.481, 0.429
batch losses (mrrl, rdl): 0.0, 2.6423e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 564
rank avg (pred): 0.051 +- 0.046
mrr vals (pred, true): 0.457, 0.355
batch losses (mrrl, rdl): 0.0, 6.9418e-06

Epoch over!
epoch time: 11.789

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1012
rank avg (pred): 0.275 +- 0.237
mrr vals (pred, true): 0.327, 0.241
batch losses (mrrl, rdl): 0.0, 0.0007334792

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1212
rank avg (pred): 0.267 +- 0.238
mrr vals (pred, true): 0.344, 0.050
batch losses (mrrl, rdl): 0.0, 0.0005864698

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 859
rank avg (pred): 0.386 +- 0.310
mrr vals (pred, true): 0.287, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001052404

Epoch over!
epoch time: 11.733

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 208
rank avg (pred): 0.264 +- 0.235
mrr vals (pred, true): 0.333, 0.054
batch losses (mrrl, rdl): 0.0, 0.0006042548

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 737
rank avg (pred): 0.319 +- 0.274
mrr vals (pred, true): 0.286, 0.021
batch losses (mrrl, rdl): 0.0, 0.0003302289

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 937
rank avg (pred): 0.453 +- 0.324
mrr vals (pred, true): 0.244, 0.053
batch losses (mrrl, rdl): 0.0, 3.50552e-05

Epoch over!
epoch time: 11.603

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 62
rank avg (pred): 0.031 +- 0.031
mrr vals (pred, true): 0.540, 0.423
batch losses (mrrl, rdl): 0.136364013, 1.33734e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 482
rank avg (pred): 0.335 +- 0.222
mrr vals (pred, true): 0.163, 0.053
batch losses (mrrl, rdl): 0.1284678429, 0.0002896071

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 611
rank avg (pred): 0.337 +- 0.217
mrr vals (pred, true): 0.156, 0.215
batch losses (mrrl, rdl): 0.0336748026, 0.0011133171

Epoch over!
epoch time: 12.099

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 299
rank avg (pred): 0.063 +- 0.054
mrr vals (pred, true): 0.430, 0.431
batch losses (mrrl, rdl): 3.4134e-06, 2.1271e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 496
rank avg (pred): 0.122 +- 0.106
mrr vals (pred, true): 0.365, 0.401
batch losses (mrrl, rdl): 0.0131734628, 9.61007e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 499
rank avg (pred): 0.099 +- 0.087
mrr vals (pred, true): 0.381, 0.412
batch losses (mrrl, rdl): 0.0094238194, 4.74393e-05

Epoch over!
epoch time: 12.001

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 672
rank avg (pred): 0.391 +- 0.184
mrr vals (pred, true): 0.081, 0.058
batch losses (mrrl, rdl): 0.0093053849, 7.4173e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 909
rank avg (pred): 0.134 +- 0.109
mrr vals (pred, true): 0.280, 0.358
batch losses (mrrl, rdl): 0.0613794848, 0.0001207175

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 925
rank avg (pred): 0.495 +- 0.113
mrr vals (pred, true): 0.032, 0.039
batch losses (mrrl, rdl): 0.0032150405, 7.89064e-05

Epoch over!
epoch time: 11.917

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1067
rank avg (pred): 0.067 +- 0.059
mrr vals (pred, true): 0.450, 0.452
batch losses (mrrl, rdl): 3.0885e-05, 4.2686e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 498
rank avg (pred): 0.118 +- 0.103
mrr vals (pred, true): 0.384, 0.402
batch losses (mrrl, rdl): 0.0033086077, 8.79095e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 907
rank avg (pred): 0.407 +- 0.256
mrr vals (pred, true): 0.123, 0.094
batch losses (mrrl, rdl): 0.0531088188, 0.0007284141

Epoch over!
epoch time: 12.309

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 477
rank avg (pred): 0.332 +- 0.202
mrr vals (pred, true): 0.135, 0.050
batch losses (mrrl, rdl): 0.0729239658, 0.0003175249

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 746
rank avg (pred): 0.105 +- 0.089
mrr vals (pred, true): 0.362, 0.339
batch losses (mrrl, rdl): 0.0053048581, 3.22305e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 528
rank avg (pred): 0.329 +- 0.282
mrr vals (pred, true): 0.375, 0.352
batch losses (mrrl, rdl): 0.00539091, 0.0017342576

Epoch over!
epoch time: 12.179

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1072
rank avg (pred): 0.101 +- 0.088
mrr vals (pred, true): 0.434, 0.409
batch losses (mrrl, rdl): 0.0058464911, 4.6792e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 187
rank avg (pred): 0.286 +- 0.199
mrr vals (pred, true): 0.196, 0.050
batch losses (mrrl, rdl): 0.2138122618, 0.000484587

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1114
rank avg (pred): 0.315 +- 0.200
mrr vals (pred, true): 0.158, 0.054
batch losses (mrrl, rdl): 0.1164659858, 0.0003667168

Epoch over!
epoch time: 12.205

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 818
rank avg (pred): 0.473 +- 0.244
mrr vals (pred, true): 0.098, 0.021
batch losses (mrrl, rdl): 0.0230489466, 7.45585e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1093
rank avg (pred): 0.315 +- 0.196
mrr vals (pred, true): 0.151, 0.254
batch losses (mrrl, rdl): 0.1044425517, 0.0010022919

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1008
rank avg (pred): 0.320 +- 0.193
mrr vals (pred, true): 0.145, 0.243
batch losses (mrrl, rdl): 0.0960549042, 0.001248771

Epoch over!
epoch time: 12.21

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 501
rank avg (pred): 0.100 +- 0.084
mrr vals (pred, true): 0.393, 0.414
batch losses (mrrl, rdl): 0.0043418217, 4.29103e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 341
rank avg (pred): 0.313 +- 0.195
mrr vals (pred, true): 0.169, 0.237
batch losses (mrrl, rdl): 0.045880992, 0.0011692758

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 821
rank avg (pred): 0.096 +- 0.081
mrr vals (pred, true): 0.430, 0.411
batch losses (mrrl, rdl): 0.0037155815, 3.83783e-05

Epoch over!
epoch time: 12.089

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1181
rank avg (pred): 0.356 +- 0.198
mrr vals (pred, true): 0.126, 0.237
batch losses (mrrl, rdl): 0.1241652295, 0.0015821912

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 511
rank avg (pred): 0.121 +- 0.101
mrr vals (pred, true): 0.372, 0.397
batch losses (mrrl, rdl): 0.0061834091, 9.19605e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 789
rank avg (pred): 0.435 +- 0.083
mrr vals (pred, true): 0.034, 0.054
batch losses (mrrl, rdl): 0.0026305164, 9.16955e-05

Epoch over!
epoch time: 12.177

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 879
rank avg (pred): 0.475 +- 0.125
mrr vals (pred, true): 0.047, 0.054
batch losses (mrrl, rdl): 9.73364e-05, 6.34708e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 703
rank avg (pred): 0.359 +- 0.231
mrr vals (pred, true): 0.164, 0.053
batch losses (mrrl, rdl): 0.130634129, 0.000178059

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 471
rank avg (pred): 0.324 +- 0.178
mrr vals (pred, true): 0.128, 0.051
batch losses (mrrl, rdl): 0.0608877242, 0.0003561706

Epoch over!
epoch time: 12.03

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.326 +- 0.177
mrr vals (pred, true): 0.132, 0.221

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   17 	     0 	 0.06574 	 0.02078 	 m..s
    3 	     1 	 0.03710 	 0.04019 	 ~...
   31 	     2 	 0.13181 	 0.04634 	 m..s
   71 	     3 	 0.13566 	 0.04803 	 m..s
   15 	     4 	 0.05005 	 0.04874 	 ~...
   77 	     5 	 0.13949 	 0.04888 	 m..s
   64 	     6 	 0.13300 	 0.04904 	 m..s
   16 	     7 	 0.05371 	 0.04955 	 ~...
   12 	     8 	 0.04849 	 0.04968 	 ~...
   74 	     9 	 0.13864 	 0.04992 	 m..s
   19 	    10 	 0.11571 	 0.04999 	 m..s
   10 	    11 	 0.04783 	 0.05088 	 ~...
   22 	    12 	 0.12862 	 0.05090 	 m..s
    7 	    13 	 0.04070 	 0.05114 	 ~...
   31 	    14 	 0.13181 	 0.05119 	 m..s
    9 	    15 	 0.04706 	 0.05131 	 ~...
   14 	    16 	 0.04990 	 0.05183 	 ~...
   29 	    17 	 0.13133 	 0.05193 	 m..s
   65 	    18 	 0.13346 	 0.05240 	 m..s
    8 	    19 	 0.04230 	 0.05254 	 ~...
   31 	    20 	 0.13181 	 0.05259 	 m..s
    2 	    21 	 0.03697 	 0.05287 	 ~...
   31 	    22 	 0.13181 	 0.05297 	 m..s
    4 	    23 	 0.03718 	 0.05340 	 ~...
   62 	    24 	 0.13279 	 0.05361 	 m..s
   31 	    25 	 0.13181 	 0.05376 	 m..s
   58 	    26 	 0.13259 	 0.05379 	 m..s
   31 	    27 	 0.13181 	 0.05383 	 m..s
    6 	    28 	 0.04044 	 0.05411 	 ~...
   78 	    29 	 0.14272 	 0.05453 	 m..s
   13 	    30 	 0.04955 	 0.05470 	 ~...
   31 	    31 	 0.13181 	 0.05482 	 m..s
    5 	    32 	 0.04010 	 0.05593 	 ~...
   31 	    33 	 0.13181 	 0.05595 	 m..s
   76 	    34 	 0.13932 	 0.05612 	 m..s
   52 	    35 	 0.13186 	 0.05688 	 m..s
   54 	    36 	 0.13212 	 0.05692 	 m..s
    1 	    37 	 0.03038 	 0.05708 	 ~...
   55 	    38 	 0.13212 	 0.05828 	 m..s
   11 	    39 	 0.04819 	 0.05858 	 ~...
   68 	    40 	 0.13405 	 0.05888 	 m..s
   61 	    41 	 0.13276 	 0.05963 	 m..s
   31 	    42 	 0.13181 	 0.05971 	 m..s
    0 	    43 	 0.02949 	 0.06025 	 m..s
   56 	    44 	 0.13214 	 0.06074 	 m..s
   51 	    45 	 0.13183 	 0.06090 	 m..s
   80 	    46 	 0.18712 	 0.10886 	 m..s
   31 	    47 	 0.13181 	 0.16674 	 m..s
   72 	    48 	 0.13672 	 0.20089 	 m..s
   31 	    49 	 0.13181 	 0.22070 	 m..s
   31 	    50 	 0.13181 	 0.22191 	 m..s
   31 	    51 	 0.13181 	 0.22588 	 m..s
   31 	    52 	 0.13181 	 0.22613 	 m..s
   23 	    53 	 0.12872 	 0.22808 	 m..s
   66 	    54 	 0.13403 	 0.23238 	 m..s
   69 	    55 	 0.13469 	 0.23266 	 m..s
   31 	    56 	 0.13181 	 0.23337 	 MISS
   60 	    57 	 0.13274 	 0.23426 	 MISS
   30 	    58 	 0.13146 	 0.23455 	 MISS
   73 	    59 	 0.13774 	 0.23687 	 m..s
   63 	    60 	 0.13291 	 0.23799 	 MISS
   18 	    61 	 0.11527 	 0.23816 	 MISS
   57 	    62 	 0.13246 	 0.24013 	 MISS
   31 	    63 	 0.13181 	 0.24224 	 MISS
   21 	    64 	 0.12174 	 0.24880 	 MISS
   31 	    65 	 0.13181 	 0.24969 	 MISS
   67 	    66 	 0.13404 	 0.25213 	 MISS
   70 	    67 	 0.13510 	 0.25319 	 MISS
   31 	    68 	 0.13181 	 0.25383 	 MISS
   25 	    69 	 0.12919 	 0.25594 	 MISS
   24 	    70 	 0.12916 	 0.25781 	 MISS
   53 	    71 	 0.13197 	 0.26072 	 MISS
   75 	    72 	 0.13900 	 0.26135 	 MISS
   20 	    73 	 0.11610 	 0.26233 	 MISS
   27 	    74 	 0.13123 	 0.26274 	 MISS
   31 	    75 	 0.13181 	 0.26303 	 MISS
   26 	    76 	 0.13103 	 0.26399 	 MISS
   79 	    77 	 0.14938 	 0.26671 	 MISS
   31 	    78 	 0.13181 	 0.26914 	 MISS
   28 	    79 	 0.13131 	 0.27616 	 MISS
   59 	    80 	 0.13261 	 0.28497 	 MISS
   81 	    81 	 0.18878 	 0.30657 	 MISS
   83 	    82 	 0.28462 	 0.30868 	 ~...
   88 	    83 	 0.38466 	 0.35878 	 ~...
   87 	    84 	 0.38410 	 0.35933 	 ~...
   84 	    85 	 0.31504 	 0.36137 	 m..s
   89 	    86 	 0.38580 	 0.36206 	 ~...
   93 	    87 	 0.38835 	 0.36349 	 ~...
   82 	    88 	 0.20740 	 0.36815 	 MISS
   90 	    89 	 0.38671 	 0.37780 	 ~...
   91 	    90 	 0.38692 	 0.37832 	 ~...
   96 	    91 	 0.39825 	 0.38590 	 ~...
   86 	    92 	 0.38360 	 0.38639 	 ~...
   97 	    93 	 0.39832 	 0.39038 	 ~...
   95 	    94 	 0.39593 	 0.39255 	 ~...
   85 	    95 	 0.38020 	 0.39489 	 ~...
   92 	    96 	 0.38808 	 0.39875 	 ~...
   94 	    97 	 0.39435 	 0.41283 	 ~...
   98 	    98 	 0.40625 	 0.42435 	 ~...
  110 	    99 	 0.44020 	 0.42696 	 ~...
  102 	   100 	 0.43631 	 0.42739 	 ~...
  116 	   101 	 0.44502 	 0.42791 	 ~...
  115 	   102 	 0.44492 	 0.43393 	 ~...
  106 	   103 	 0.43938 	 0.43431 	 ~...
  112 	   104 	 0.44083 	 0.43568 	 ~...
  105 	   105 	 0.43885 	 0.43841 	 ~...
  111 	   106 	 0.44039 	 0.43946 	 ~...
  114 	   107 	 0.44444 	 0.43978 	 ~...
  103 	   108 	 0.43721 	 0.44003 	 ~...
  120 	   109 	 0.46862 	 0.44064 	 ~...
  113 	   110 	 0.44313 	 0.44196 	 ~...
   99 	   111 	 0.42918 	 0.44616 	 ~...
  108 	   112 	 0.43968 	 0.44625 	 ~...
  100 	   113 	 0.43504 	 0.44706 	 ~...
  104 	   114 	 0.43776 	 0.44750 	 ~...
  109 	   115 	 0.44013 	 0.44837 	 ~...
  107 	   116 	 0.43950 	 0.44905 	 ~...
  117 	   117 	 0.45052 	 0.46171 	 ~...
  101 	   118 	 0.43609 	 0.46297 	 ~...
  118 	   119 	 0.45310 	 0.46726 	 ~...
  119 	   120 	 0.45833 	 0.47721 	 ~...
==========================================
r_mrr = 0.8820232152938843
r2_mrr = 0.7638147473335266
spearmanr_mrr@5 = 0.9681723713874817
spearmanr_mrr@10 = 0.9786340594291687
spearmanr_mrr@50 = 0.9737354516983032
spearmanr_mrr@100 = 0.8665658831596375
spearmanr_mrr@All = 0.8983858227729797
==========================================
test time: 0.389
Done Testing dataset Kinships
total time taken: 187.60197401046753
training time taken: 180.82548999786377
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8820)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7638)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9682)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.9786)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9737)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8666)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.8984)}}, 'test_loss': {'DistMult': {'Kinships': 7.034627850835022}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 9679830691813768
Starting TWIG!
Loading datasets
Loading DistMult...
Loading Kinships...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1177, 1124, 1156, 596, 908, 968, 151, 288, 222, 320, 879, 71, 1097, 420, 1049, 904, 376, 670, 976, 59, 971, 146, 422, 822, 686, 805, 1179, 1046, 203, 902, 560, 574, 1213, 872, 115, 571, 375, 354, 147, 223, 857, 1188, 128, 67, 1070, 1130, 983, 60, 511, 544, 132, 250, 416, 276, 395, 27, 285, 568, 552, 591, 733, 549, 674, 703, 397, 638, 1161, 442, 78, 839, 349, 600, 569, 482, 374, 538, 326, 1146, 1037, 602, 874, 491, 1076, 835, 304, 832, 383, 1170, 1044, 300, 119, 371, 437, 96, 825, 720, 291, 993, 790, 318, 948, 414, 601, 542, 262, 550, 943, 745, 346, 32, 58, 637, 33, 193, 972, 764, 1073, 387, 396, 1022, 1157]
valid_ids (0): []
train_ids (1094): [951, 780, 900, 1103, 470, 333, 775, 294, 364, 880, 583, 767, 995, 366, 485, 1133, 803, 697, 402, 946, 595, 867, 1206, 689, 682, 1144, 1209, 1064, 233, 1152, 124, 1104, 380, 1061, 915, 415, 748, 743, 363, 1095, 357, 315, 413, 779, 118, 740, 1, 208, 650, 236, 24, 807, 737, 516, 1159, 1114, 473, 1172, 194, 990, 759, 950, 1155, 1138, 772, 960, 228, 1154, 590, 350, 1047, 1185, 688, 136, 17, 8, 980, 94, 786, 35, 478, 214, 79, 69, 890, 246, 488, 1027, 113, 384, 480, 706, 14, 244, 529, 710, 1084, 625, 114, 356, 88, 612, 1212, 741, 580, 582, 599, 752, 570, 417, 575, 0, 158, 160, 64, 1096, 75, 152, 234, 837, 373, 669, 907, 263, 271, 289, 514, 2, 418, 82, 506, 979, 455, 210, 99, 1067, 668, 1062, 445, 273, 827, 1151, 309, 257, 308, 46, 252, 566, 1071, 339, 921, 444, 731, 1136, 1189, 1182, 217, 524, 917, 186, 957, 1078, 57, 409, 66, 185, 1207, 451, 859, 896, 428, 45, 817, 765, 1134, 997, 665, 799, 588, 713, 351, 86, 781, 241, 213, 42, 1112, 852, 1186, 751, 1023, 385, 695, 794, 277, 715, 711, 44, 535, 942, 801, 1183, 704, 476, 1034, 259, 965, 808, 533, 792, 29, 988, 548, 267, 148, 1056, 172, 358, 28, 1092, 1148, 256, 1029, 1201, 831, 1098, 607, 547, 1202, 657, 238, 197, 165, 541, 581, 526, 848, 1033, 307, 1140, 265, 1042, 130, 104, 818, 83, 1164, 796, 986, 812, 592, 314, 646, 398, 632, 347, 1025, 609, 279, 709, 199, 435, 875, 945, 520, 92, 937, 360, 1091, 742, 378, 1171, 1055, 487, 40, 935, 73, 1142, 139, 1077, 1063, 405, 1032, 174, 303, 523, 787, 249, 474, 343, 429, 850, 809, 159, 873, 275, 778, 1020, 654, 206, 725, 851, 753, 446, 153, 441, 919, 348, 838, 135, 1173, 994, 826, 131, 1019, 763, 19, 954, 127, 532, 620, 386, 776, 1162, 565, 1191, 192, 112, 498, 530, 1153, 934, 1135, 618, 175, 209, 628, 286, 701, 20, 426, 1060, 608, 394, 678, 811, 564, 149, 297, 785, 157, 95, 166, 593, 388, 1126, 129, 885, 317, 1002, 758, 705, 109, 1143, 189, 1122, 352, 963, 52, 525, 50, 1106, 649, 31, 1163, 369, 522, 1200, 43, 920, 509, 856, 145, 598, 847, 683, 164, 707, 195, 992, 1053, 1093, 782, 1102, 510, 540, 829, 627, 125, 939, 179, 558, 681, 231, 264, 998, 450, 211, 278, 853, 693, 141, 658, 1030, 1058, 16, 866, 224, 1190, 777, 865, 431, 201, 749, 475, 138, 673, 340, 505, 621, 955, 298, 666, 361, 200, 739, 1045, 63, 15, 421, 68, 245, 163, 845, 773, 1088, 101, 1080, 708, 161, 48, 331, 1016, 1205, 98, 292, 918, 723, 393, 133, 824, 268, 1111, 1101, 302, 841, 727, 225, 329, 1011, 977, 334, 864, 221, 495, 282, 586, 814, 311, 270, 1079, 472, 1015, 528, 962, 483, 248, 6, 589, 551, 1039, 432, 54, 181, 1072, 806, 1013, 162, 176, 984, 1194, 77, 577, 1083, 928, 1038, 672, 458, 901, 755, 25, 36, 1204, 519, 468, 287, 447, 37, 750, 168, 377, 1131, 89, 1149, 513, 648, 359, 239, 1057, 337, 906, 1147, 486, 728, 736, 881, 336, 187, 182, 154, 1113, 970, 820, 605, 362, 675, 255, 55, 215, 382, 220, 655, 1166, 137, 714, 501, 1203, 408, 721, 912, 1005, 1006, 389, 462, 1109, 1160, 894, 93, 103, 878, 843, 345, 85, 372, 958, 461, 407, 1074, 11, 1196, 768, 830, 576, 1099, 585, 467, 332, 536, 891, 321, 512, 640, 893, 999, 554, 338, 633, 1043, 521, 121, 770, 367, 481, 527, 626, 110, 242, 207, 889, 793, 1007, 237, 761, 611, 296, 281, 330, 771, 218, 229, 471, 499, 272, 1187, 518, 579, 205, 464, 440, 927, 406, 30, 760, 1169, 293, 854, 1054, 253, 117, 892, 327, 717, 846, 313, 925, 634, 973, 235, 619, 660, 687, 274, 784, 517, 1116, 563, 41, 1125, 84, 684, 1107, 1021, 876, 493, 1208, 546, 269, 539, 614, 1059, 860, 622, 774, 623, 617, 243, 284, 1087, 804, 1090, 1100, 494, 1094, 310, 38, 430, 479, 819, 457, 21, 567, 810, 123, 1210, 1035, 4, 1069, 1178, 987, 72, 232, 1127, 813, 1004, 328, 23, 1017, 863, 1121, 802, 1012, 316, 9, 910, 1028, 1041, 12, 578, 718, 929, 691, 465, 365, 1105, 1132, 1108, 562, 322, 700, 769, 1214, 531, 676, 1003, 463, 490, 65, 1066, 449, 766, 884, 722, 1158, 306, 956, 883, 1031, 216, 643, 1120, 62, 433, 368, 100, 436, 381, 738, 923, 651, 855, 401, 754, 871, 434, 261, 212, 886, 1052, 1082, 677, 299, 438, 120, 897, 1137, 106, 858, 844, 1081, 325, 469, 941, 940, 319, 604, 1010, 91, 423, 989, 1180, 425, 692, 254, 1184, 1141, 730, 47, 1181, 90, 1050, 895, 107, 557, 641, 1008, 219, 823, 543, 51, 815, 150, 724, 190, 969, 134, 559, 170, 140, 1009, 1123, 797, 204, 370, 696, 1195, 290, 1117, 198, 644, 22, 694, 909, 453, 122, 905, 266, 639, 959, 922, 828, 645, 74, 1119, 18, 926, 656, 10, 964, 419, 967, 662, 553, 653, 353, 953, 126, 783, 1168, 484, 76, 791, 1075, 459, 762, 545, 400, 573, 903, 911, 610, 180, 877, 702, 312, 789, 1115, 301, 561, 944, 931, 240, 3, 636, 1192, 226, 492, 869, 230, 53, 335, 156, 833, 39, 975, 642, 251, 111, 454, 556, 1048, 982, 635, 404, 887, 26, 667, 1198, 795, 712, 1089, 424, 659, 816, 898, 1068, 344, 34, 504, 1040, 489, 7, 756, 439, 1175, 978, 836, 191, 477, 981, 116, 616, 924, 403, 143, 729, 1128, 1211, 991, 196, 914, 169, 1085, 584, 664, 679, 932, 448, 70, 515, 800, 930, 324, 183, 280, 821, 295, 81, 671, 690, 699, 1197, 1150, 173, 502, 882, 142, 500, 56, 508, 341, 1145, 757, 1051, 497, 355, 283, 849, 108, 735, 537, 615, 399, 507, 390, 933, 456, 177, 936, 961, 171, 572, 888, 716, 49, 13, 594, 105, 985, 952, 870, 555, 661, 260, 966, 1000, 938, 606, 410, 167, 247, 411, 613, 747, 460, 342, 1167, 746, 323, 842, 974, 1129, 102, 913, 534, 1165, 685, 680, 947, 496, 652, 834, 996, 188, 1001, 1193, 412, 178, 427, 1086, 597, 663, 1014, 734, 624, 452, 392, 788, 798, 1026, 726, 391, 5, 1176, 443, 1110, 629, 97, 899, 647, 80, 227, 202, 840, 305, 862, 630, 1018, 87, 1024, 1065, 184, 144, 258, 155, 868, 732, 1036, 1174, 587, 744, 861, 1199, 1139, 949, 503, 61, 379, 631, 698, 466, 603, 719, 1118, 916]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4791485652718769
the save name prefix for this run is:  chkpt-ID_4791485652718769_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'DistMult': {'Kinships': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 890
rank avg (pred): 0.533 +- 0.005
mrr vals (pred, true): 0.018, 0.054
batch losses (mrrl, rdl): 0.0, 0.0001627281

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 229
rank avg (pred): 0.273 +- 0.170
mrr vals (pred, true): 0.144, 0.062
batch losses (mrrl, rdl): 0.0, 0.0005477789

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 164
rank avg (pred): 0.204 +- 0.178
mrr vals (pred, true): 0.317, 0.051
batch losses (mrrl, rdl): 0.0, 0.0012231636

Epoch over!
epoch time: 11.961

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 263
rank avg (pred): 0.074 +- 0.068
mrr vals (pred, true): 0.407, 0.436
batch losses (mrrl, rdl): 0.0, 9.4298e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 20
rank avg (pred): 0.053 +- 0.047
mrr vals (pred, true): 0.431, 0.439
batch losses (mrrl, rdl): 0.0, 3.263e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 924
rank avg (pred): 0.486 +- 0.348
mrr vals (pred, true): 0.226, 0.046
batch losses (mrrl, rdl): 0.0, 5.24645e-05

Epoch over!
epoch time: 11.945

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 950
rank avg (pred): 0.432 +- 0.333
mrr vals (pred, true): 0.266, 0.054
batch losses (mrrl, rdl): 0.0, 3.01167e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 837
rank avg (pred): 0.488 +- 0.297
mrr vals (pred, true): 0.136, 0.052
batch losses (mrrl, rdl): 0.0, 6.58902e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 840
rank avg (pred): 0.372 +- 0.321
mrr vals (pred, true): 0.327, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001290125

Epoch over!
epoch time: 11.925

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 436
rank avg (pred): 0.258 +- 0.230
mrr vals (pred, true): 0.336, 0.053
batch losses (mrrl, rdl): 0.0, 0.0006346934

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 186
rank avg (pred): 0.300 +- 0.234
mrr vals (pred, true): 0.248, 0.050
batch losses (mrrl, rdl): 0.0, 0.0003784903

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 160
rank avg (pred): 0.259 +- 0.232
mrr vals (pred, true): 0.350, 0.236
batch losses (mrrl, rdl): 0.0, 0.0006284029

Epoch over!
epoch time: 11.904

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 401
rank avg (pred): 0.284 +- 0.244
mrr vals (pred, true): 0.327, 0.202
batch losses (mrrl, rdl): 0.0, 0.0005260223

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 231
rank avg (pred): 0.278 +- 0.238
mrr vals (pred, true): 0.308, 0.049
batch losses (mrrl, rdl): 0.0, 0.000549344

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 118
rank avg (pred): 0.255 +- 0.234
mrr vals (pred, true): 0.367, 0.267
batch losses (mrrl, rdl): 0.0, 0.0007102598

Epoch over!
epoch time: 11.665

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 251
rank avg (pred): 0.053 +- 0.051
mrr vals (pred, true): 0.495, 0.427
batch losses (mrrl, rdl): 0.0459481776, 4.162e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 888
rank avg (pred): 0.776 +- 0.252
mrr vals (pred, true): 0.046, 0.052
batch losses (mrrl, rdl): 0.0001473544, 0.0018198807

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 648
rank avg (pred): 0.383 +- 0.225
mrr vals (pred, true): 0.136, 0.057
batch losses (mrrl, rdl): 0.0738040134, 7.64512e-05

Epoch over!
epoch time: 12.105

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 373
rank avg (pred): 0.361 +- 0.237
mrr vals (pred, true): 0.169, 0.274
batch losses (mrrl, rdl): 0.1096419916, 0.0015819741

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 212
rank avg (pred): 0.363 +- 0.229
mrr vals (pred, true): 0.154, 0.052
batch losses (mrrl, rdl): 0.1074631065, 0.0001171773

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 840
rank avg (pred): 0.814 +- 0.244
mrr vals (pred, true): 0.048, 0.050
batch losses (mrrl, rdl): 2.3665e-05, 0.0021414382

Epoch over!
epoch time: 11.942

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 980
rank avg (pred): 0.035 +- 0.028
mrr vals (pred, true): 0.453, 0.471
batch losses (mrrl, rdl): 0.0031565826, 7.7811e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 784
rank avg (pred): 0.782 +- 0.238
mrr vals (pred, true): 0.054, 0.053
batch losses (mrrl, rdl): 0.0001782086, 0.0021273855

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 483
rank avg (pred): 0.370 +- 0.212
mrr vals (pred, true): 0.132, 0.047
batch losses (mrrl, rdl): 0.0674351156, 0.0001461578

Epoch over!
epoch time: 12.023

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 36
rank avg (pred): 0.101 +- 0.087
mrr vals (pred, true): 0.429, 0.431
batch losses (mrrl, rdl): 5.91674e-05, 4.61638e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1080
rank avg (pred): 0.352 +- 0.221
mrr vals (pred, true): 0.155, 0.234
batch losses (mrrl, rdl): 0.0624634027, 0.0013812669

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1178
rank avg (pred): 0.363 +- 0.210
mrr vals (pred, true): 0.135, 0.249
batch losses (mrrl, rdl): 0.1298011243, 0.0017192893

Epoch over!
epoch time: 12.182

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1113
rank avg (pred): 0.343 +- 0.216
mrr vals (pred, true): 0.156, 0.055
batch losses (mrrl, rdl): 0.1117112041, 0.0002191069

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 541
rank avg (pred): 0.220 +- 0.184
mrr vals (pred, true): 0.382, 0.375
batch losses (mrrl, rdl): 0.0005171951, 0.0006355506

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 394
rank avg (pred): 0.327 +- 0.216
mrr vals (pred, true): 0.172, 0.255
batch losses (mrrl, rdl): 0.069226332, 0.001328753

Epoch over!
epoch time: 12.185

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 257
rank avg (pred): 0.061 +- 0.051
mrr vals (pred, true): 0.454, 0.434
batch losses (mrrl, rdl): 0.0038453527, 9.269e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 35
rank avg (pred): 0.111 +- 0.093
mrr vals (pred, true): 0.421, 0.443
batch losses (mrrl, rdl): 0.004548646, 7.98725e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 391
rank avg (pred): 0.364 +- 0.193
mrr vals (pred, true): 0.127, 0.254
batch losses (mrrl, rdl): 0.1606267542, 0.0015066278

Epoch over!
epoch time: 12.474

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 235
rank avg (pred): 0.340 +- 0.204
mrr vals (pred, true): 0.153, 0.051
batch losses (mrrl, rdl): 0.1056325361, 0.0002592829

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 527
rank avg (pred): 0.178 +- 0.148
mrr vals (pred, true): 0.394, 0.372
batch losses (mrrl, rdl): 0.0048073973, 0.0003335878

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 467
rank avg (pred): 0.350 +- 0.192
mrr vals (pred, true): 0.132, 0.057
batch losses (mrrl, rdl): 0.0668153837, 0.0002225762

Epoch over!
epoch time: 12.314

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 461
rank avg (pred): 0.351 +- 0.190
mrr vals (pred, true): 0.131, 0.056
batch losses (mrrl, rdl): 0.0651380718, 0.0002155105

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 74
rank avg (pred): 0.066 +- 0.055
mrr vals (pred, true): 0.427, 0.450
batch losses (mrrl, rdl): 0.004970483, 4.9668e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 419
rank avg (pred): 0.327 +- 0.172
mrr vals (pred, true): 0.131, 0.055
batch losses (mrrl, rdl): 0.0651571229, 0.0003474265

Epoch over!
epoch time: 11.913

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 467
rank avg (pred): 0.334 +- 0.205
mrr vals (pred, true): 0.153, 0.057
batch losses (mrrl, rdl): 0.1062974483, 0.0002720354

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 133
rank avg (pred): 0.329 +- 0.191
mrr vals (pred, true): 0.147, 0.189
batch losses (mrrl, rdl): 0.0168349184, 0.0005598281

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 1212
rank avg (pred): 0.340 +- 0.182
mrr vals (pred, true): 0.132, 0.050
batch losses (mrrl, rdl): 0.0678331703, 0.0002899662

Epoch over!
epoch time: 12.142

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 705
rank avg (pred): 0.312 +- 0.197
mrr vals (pred, true): 0.167, 0.055
batch losses (mrrl, rdl): 0.1379870027, 0.0004251957

running batch: 500 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 821
rank avg (pred): 0.059 +- 0.049
mrr vals (pred, true): 0.430, 0.411
batch losses (mrrl, rdl): 0.0037240656, 5.025e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, Kinships, run 2.1, exp 102
rank avg (pred): 0.288 +- 0.135
mrr vals (pred, true): 0.134, 0.254
batch losses (mrrl, rdl): 0.1425095052, 0.0008968147

Epoch over!
epoch time: 11.809

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG Kinships
Running eval on the test set
running batch: 0
rank avg (pred): 0.343 +- 0.173
mrr vals (pred, true): 0.124, 0.219

Evaluation for Kinships on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   13 	     0 	 0.11204 	 0.03915 	 m..s
    5 	     1 	 0.03701 	 0.04019 	 ~...
   72 	     2 	 0.14559 	 0.04747 	 m..s
   54 	     3 	 0.12644 	 0.04757 	 m..s
    4 	     4 	 0.03520 	 0.04945 	 ~...
    0 	     5 	 0.03000 	 0.04997 	 ~...
   21 	     6 	 0.12189 	 0.05029 	 m..s
   73 	     7 	 0.14588 	 0.05077 	 m..s
   31 	     8 	 0.12409 	 0.05101 	 m..s
   62 	     9 	 0.13100 	 0.05182 	 m..s
   66 	    10 	 0.13521 	 0.05247 	 m..s
   61 	    11 	 0.12923 	 0.05259 	 m..s
   31 	    12 	 0.12409 	 0.05262 	 m..s
   31 	    13 	 0.12409 	 0.05297 	 m..s
   64 	    14 	 0.13321 	 0.05298 	 m..s
   71 	    15 	 0.14156 	 0.05309 	 m..s
   68 	    16 	 0.13681 	 0.05340 	 m..s
   10 	    17 	 0.06077 	 0.05374 	 ~...
   27 	    18 	 0.12317 	 0.05383 	 m..s
   69 	    19 	 0.13788 	 0.05391 	 m..s
    9 	    20 	 0.05649 	 0.05405 	 ~...
    7 	    21 	 0.04835 	 0.05411 	 ~...
    8 	    22 	 0.05087 	 0.05415 	 ~...
   31 	    23 	 0.12409 	 0.05451 	 m..s
   18 	    24 	 0.12163 	 0.05463 	 m..s
   65 	    25 	 0.13361 	 0.05515 	 m..s
    2 	    26 	 0.03286 	 0.05526 	 ~...
   31 	    27 	 0.12409 	 0.05541 	 m..s
   22 	    28 	 0.12193 	 0.05548 	 m..s
   11 	    29 	 0.06476 	 0.05568 	 ~...
   31 	    30 	 0.12409 	 0.05707 	 m..s
    3 	    31 	 0.03409 	 0.05879 	 ~...
   16 	    32 	 0.12070 	 0.05880 	 m..s
    1 	    33 	 0.03235 	 0.06083 	 ~...
   17 	    34 	 0.12161 	 0.06164 	 m..s
   59 	    35 	 0.12893 	 0.06190 	 m..s
    6 	    36 	 0.04278 	 0.06476 	 ~...
   76 	    37 	 0.23401 	 0.14805 	 m..s
   31 	    38 	 0.12409 	 0.16188 	 m..s
   15 	    39 	 0.12070 	 0.17817 	 m..s
   75 	    40 	 0.21831 	 0.19588 	 ~...
   19 	    41 	 0.12175 	 0.20326 	 m..s
   74 	    42 	 0.14919 	 0.20737 	 m..s
   23 	    43 	 0.12212 	 0.21466 	 m..s
   26 	    44 	 0.12274 	 0.21871 	 m..s
   31 	    45 	 0.12409 	 0.21900 	 m..s
   55 	    46 	 0.12665 	 0.22486 	 m..s
   31 	    47 	 0.12409 	 0.22734 	 MISS
   31 	    48 	 0.12409 	 0.22897 	 MISS
   31 	    49 	 0.12409 	 0.23156 	 MISS
   57 	    50 	 0.12767 	 0.23730 	 MISS
   56 	    51 	 0.12756 	 0.23736 	 MISS
   31 	    52 	 0.12409 	 0.23787 	 MISS
   51 	    53 	 0.12477 	 0.23872 	 MISS
   31 	    54 	 0.12409 	 0.24060 	 MISS
   14 	    55 	 0.11977 	 0.24121 	 MISS
   58 	    56 	 0.12782 	 0.24298 	 MISS
   52 	    57 	 0.12517 	 0.24499 	 MISS
   28 	    58 	 0.12334 	 0.24505 	 MISS
   31 	    59 	 0.12409 	 0.24548 	 MISS
   60 	    60 	 0.12905 	 0.24617 	 MISS
   29 	    61 	 0.12365 	 0.24639 	 MISS
   49 	    62 	 0.12424 	 0.24738 	 MISS
   31 	    63 	 0.12409 	 0.24779 	 MISS
   31 	    64 	 0.12409 	 0.24784 	 MISS
   67 	    65 	 0.13543 	 0.24937 	 MISS
   25 	    66 	 0.12272 	 0.25030 	 MISS
   24 	    67 	 0.12219 	 0.25198 	 MISS
   70 	    68 	 0.13947 	 0.25366 	 MISS
   31 	    69 	 0.12409 	 0.25445 	 MISS
   20 	    70 	 0.12177 	 0.25595 	 MISS
   63 	    71 	 0.13216 	 0.25634 	 MISS
   12 	    72 	 0.10395 	 0.25680 	 MISS
   31 	    73 	 0.12409 	 0.26165 	 MISS
   30 	    74 	 0.12409 	 0.26361 	 MISS
   50 	    75 	 0.12435 	 0.26410 	 MISS
   53 	    76 	 0.12521 	 0.27815 	 MISS
   79 	    77 	 0.29084 	 0.28688 	 ~...
   86 	    78 	 0.38464 	 0.35580 	 ~...
   85 	    79 	 0.38367 	 0.36151 	 ~...
   80 	    80 	 0.29498 	 0.36367 	 m..s
   78 	    81 	 0.26034 	 0.36815 	 MISS
   88 	    82 	 0.38520 	 0.37213 	 ~...
   83 	    83 	 0.37910 	 0.37356 	 ~...
   92 	    84 	 0.39290 	 0.37401 	 ~...
   87 	    85 	 0.38516 	 0.37543 	 ~...
   84 	    86 	 0.38266 	 0.37606 	 ~...
   89 	    87 	 0.38741 	 0.38105 	 ~...
   82 	    88 	 0.37807 	 0.38333 	 ~...
   90 	    89 	 0.39078 	 0.38613 	 ~...
   91 	    90 	 0.39127 	 0.38747 	 ~...
   95 	    91 	 0.42942 	 0.39627 	 m..s
   96 	    92 	 0.43178 	 0.39661 	 m..s
   81 	    93 	 0.35422 	 0.39676 	 m..s
   77 	    94 	 0.24567 	 0.41063 	 MISS
   97 	    95 	 0.43331 	 0.42587 	 ~...
  101 	    96 	 0.43875 	 0.42728 	 ~...
   94 	    97 	 0.42938 	 0.42770 	 ~...
   93 	    98 	 0.42924 	 0.42818 	 ~...
  103 	    99 	 0.44324 	 0.43046 	 ~...
   99 	   100 	 0.43450 	 0.43162 	 ~...
  100 	   101 	 0.43494 	 0.43446 	 ~...
  113 	   102 	 0.45329 	 0.43474 	 ~...
  104 	   103 	 0.44446 	 0.43638 	 ~...
  107 	   104 	 0.44711 	 0.43896 	 ~...
  115 	   105 	 0.46297 	 0.43931 	 ~...
  116 	   106 	 0.46334 	 0.43997 	 ~...
  110 	   107 	 0.45208 	 0.44010 	 ~...
  118 	   108 	 0.46770 	 0.44064 	 ~...
  111 	   109 	 0.45299 	 0.44344 	 ~...
  102 	   110 	 0.43890 	 0.44425 	 ~...
  106 	   111 	 0.44662 	 0.44444 	 ~...
  105 	   112 	 0.44463 	 0.44446 	 ~...
  109 	   113 	 0.44982 	 0.44688 	 ~...
  112 	   114 	 0.45314 	 0.44704 	 ~...
  114 	   115 	 0.46086 	 0.44706 	 ~...
   98 	   116 	 0.43383 	 0.44712 	 ~...
  117 	   117 	 0.46525 	 0.44770 	 ~...
  119 	   118 	 0.47131 	 0.44790 	 ~...
  108 	   119 	 0.44904 	 0.45472 	 ~...
  120 	   120 	 0.47270 	 0.45760 	 ~...
==========================================
r_mrr = 0.8762164115905762
r2_mrr = 0.7338718175888062
spearmanr_mrr@5 = 0.9377990365028381
spearmanr_mrr@10 = 0.8323703408241272
spearmanr_mrr@50 = 0.9737857580184937
spearmanr_mrr@100 = 0.8910751938819885
spearmanr_mrr@All = 0.9048658609390259
==========================================
test time: 0.394
Done Testing dataset Kinships
total time taken: 187.8405065536499
training time taken: 180.9420714378357
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'Kinships': tensor(0.8762)}}, 'r2_mrr': {'DistMult': {'Kinships': tensor(0.7339)}}, 'spearmanr_mrr@5': {'DistMult': {'Kinships': tensor(0.9378)}}, 'spearmanr_mrr@10': {'DistMult': {'Kinships': tensor(0.8324)}}, 'spearmanr_mrr@50': {'DistMult': {'Kinships': tensor(0.9738)}}, 'spearmanr_mrr@100': {'DistMult': {'Kinships': tensor(0.8911)}}, 'spearmanr_mrr@All': {'DistMult': {'Kinships': tensor(0.9049)}}, 'test_loss': {'DistMult': {'Kinships': 7.360897347540913}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 5627025037156091
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [680, 963, 757, 956, 260, 369, 615, 707, 155, 1042, 1158, 444, 35, 10, 483, 1195, 456, 278, 380, 676, 404, 361, 558, 595, 1096, 753, 824, 184, 302, 445, 821, 806, 1006, 491, 78, 919, 472, 339, 501, 393, 308, 1125, 955, 63, 170, 1035, 1082, 606, 416, 773, 1151, 1005, 494, 780, 1025, 81, 712, 44, 1003, 381, 697, 534, 981, 476, 493, 91, 1185, 1087, 812, 432, 610, 458, 427, 1144, 437, 1001, 276, 1032, 590, 1170, 167, 489, 301, 45, 435, 1047, 253, 677, 309, 896, 394, 180, 759, 957, 284, 6, 231, 843, 862, 341, 622, 270, 886, 724, 482, 609, 1190, 582, 249, 411, 1037, 300, 1046, 499, 777, 656, 541, 587, 150, 169, 888]
valid_ids (0): []
train_ids (1094): [876, 714, 1054, 836, 626, 735, 469, 705, 214, 200, 585, 219, 426, 275, 1147, 791, 1112, 321, 915, 316, 382, 880, 576, 783, 910, 1064, 351, 140, 916, 505, 235, 992, 313, 421, 303, 422, 671, 27, 689, 1074, 924, 850, 322, 979, 1045, 723, 368, 224, 594, 365, 277, 258, 1161, 165, 754, 34, 203, 75, 778, 845, 701, 1002, 447, 584, 646, 933, 33, 455, 428, 431, 917, 131, 147, 352, 363, 570, 878, 722, 550, 178, 904, 1167, 935, 841, 630, 747, 204, 12, 987, 1036, 751, 989, 185, 117, 190, 336, 130, 868, 79, 462, 503, 264, 729, 892, 991, 653, 825, 621, 182, 272, 359, 1092, 279, 1056, 1068, 1198, 920, 826, 375, 202, 1146, 866, 13, 1072, 1152, 134, 581, 823, 323, 406, 1097, 767, 732, 567, 781, 51, 1209, 758, 312, 1148, 733, 739, 121, 738, 177, 464, 583, 267, 1093, 632, 521, 254, 858, 737, 861, 822, 116, 288, 527, 1040, 771, 996, 223, 1019, 1207, 209, 251, 111, 776, 634, 601, 479, 954, 752, 402, 995, 710, 761, 335, 1041, 523, 1039, 439, 396, 210, 1077, 304, 135, 334, 233, 967, 171, 373, 236, 305, 354, 156, 95, 348, 693, 648, 160, 330, 695, 152, 1110, 1140, 545, 1066, 389, 488, 914, 813, 698, 1100, 946, 941, 1179, 1114, 28, 16, 810, 485, 403, 814, 317, 417, 337, 832, 803, 788, 438, 26, 531, 230, 1169, 237, 819, 326, 779, 283, 512, 148, 0, 942, 1023, 468, 103, 800, 597, 263, 1070, 1141, 529, 694, 86, 669, 683, 811, 287, 1192, 1214, 226, 285, 655, 645, 704, 1142, 159, 372, 378, 311, 64, 446, 978, 629, 608, 854, 681, 36, 708, 687, 1033, 1136, 959, 887, 212, 936, 672, 137, 30, 1199, 1095, 572, 419, 715, 938, 804, 1048, 997, 1120, 189, 833, 504, 232, 478, 205, 1171, 213, 520, 740, 480, 867, 763, 742, 589, 3, 430, 1126, 179, 186, 1085, 29, 643, 709, 153, 986, 901, 85, 486, 22, 969, 1067, 562, 123, 424, 731, 703, 201, 1, 115, 119, 515, 261, 197, 314, 384, 1017, 217, 631, 328, 21, 1015, 994, 926, 766, 647, 1084, 908, 506, 741, 295, 132, 1128, 87, 4, 498, 1108, 1210, 183, 1000, 1124, 559, 667, 619, 39, 1213, 97, 965, 195, 420, 674, 652, 43, 664, 72, 1094, 891, 168, 875, 1132, 101, 1078, 852, 1180, 98, 816, 1186, 641, 1183, 885, 1156, 442, 307, 657, 551, 857, 1104, 227, 640, 17, 221, 817, 452, 573, 93, 474, 487, 1020, 1043, 829, 906, 644, 569, 556, 557, 661, 149, 1168, 358, 222, 9, 53, 555, 749, 921, 1079, 42, 877, 496, 691, 495, 440, 787, 770, 789, 984, 273, 1212, 1118, 871, 141, 292, 471, 526, 400, 666, 423, 349, 1201, 614, 70, 1090, 47, 616, 259, 319, 864, 225, 1091, 847, 248, 174, 897, 870, 1065, 164, 364, 720, 173, 357, 542, 346, 196, 1160, 890, 490, 240, 124, 966, 846, 790, 792, 1007, 1098, 106, 41, 18, 947, 1172, 976, 638, 678, 772, 931, 88, 840, 660, 1049, 162, 815, 74, 988, 863, 566, 122, 983, 246, 1135, 1089, 528, 11, 768, 105, 869, 940, 786, 463, 552, 580, 1127, 762, 802, 510, 84, 252, 338, 320, 280, 112, 1069, 807, 443, 1044, 553, 533, 68, 860, 1071, 838, 848, 579, 554, 964, 166, 623, 310, 1202, 387, 407, 399, 1101, 827, 90, 470, 1165, 306, 668, 620, 588, 719, 107, 736, 208, 434, 408, 1163, 1184, 565, 903, 48, 1193, 289, 350, 66, 612, 1197, 899, 1189, 398, 1107, 952, 795, 1155, 1159, 71, 1164, 1031, 1004, 905, 1157, 1010, 1143, 568, 637, 120, 410, 799, 229, 663, 578, 519, 198, 2, 395, 716, 466, 522, 571, 675, 717, 59, 745, 118, 49, 73, 89, 1162, 379, 1175, 1024, 500, 414, 23, 713, 8, 61, 670, 513, 1061, 1038, 982, 839, 543, 256, 561, 937, 376, 879, 1153, 271, 1204, 5, 344, 797, 525, 893, 702, 243, 785, 535, 1028, 726, 925, 1026, 62, 922, 808, 126, 161, 467, 1057, 711, 889, 818, 748, 930, 586, 627, 325, 900, 356, 158, 939, 998, 146, 297, 592, 244, 782, 1055, 774, 730, 696, 682, 530, 650, 750, 1203, 1188, 798, 392, 281, 775, 546, 50, 928, 949, 1182, 52, 635, 67, 1021, 980, 1200, 176, 388, 38, 290, 865, 1133, 756, 274, 658, 1012, 977, 46, 1191, 327, 835, 1076, 602, 884, 207, 429, 517, 109, 607, 397, 331, 1060, 902, 157, 142, 507, 706, 460, 332, 138, 367, 538, 129, 206, 796, 25, 401, 599, 1109, 1119, 370, 728, 299, 968, 1149, 564, 990, 639, 1138, 918, 617, 37, 54, 518, 65, 481, 333, 1206, 642, 909, 451, 929, 970, 242, 673, 927, 92, 347, 769, 895, 366, 69, 958, 1130, 220, 537, 296, 1154, 593, 1115, 794, 113, 60, 1177, 600, 355, 743, 125, 257, 974, 139, 1117, 744, 1030, 524, 881, 801, 96, 127, 649, 151, 145, 191, 943, 324, 1173, 475, 40, 102, 1022, 15, 1121, 1122, 872, 549, 108, 1062, 172, 1137, 502, 286, 809, 842, 725, 293, 577, 1134, 82, 574, 234, 511, 1083, 1063, 686, 374, 187, 746, 611, 837, 547, 448, 973, 383, 291, 1008, 898, 405, 1102, 1086, 1211, 188, 1073, 1178, 651, 449, 32, 718, 932, 975, 266, 948, 856, 907, 1014, 993, 193, 509, 563, 7, 1080, 1081, 874, 175, 412, 684, 575, 596, 163, 318, 951, 853, 239, 265, 1181, 665, 19, 128, 282, 241, 883, 540, 459, 727, 1013, 1058, 913, 211, 247, 154, 1113, 960, 497, 536, 516, 418, 269, 1018, 961, 1099, 215, 685, 618, 972, 24, 385, 1088, 94, 945, 457, 484, 985, 734, 199, 1105, 688, 1050, 100, 342, 953, 544, 433, 76, 477, 934, 605, 1027, 923, 262, 450, 1145, 390, 514, 1174, 1029, 1011, 539, 58, 1111, 1051, 20, 454, 83, 834, 345, 636, 912, 465, 628, 755, 1053, 1116, 1150, 820, 844, 245, 218, 255, 461, 1131, 415, 492, 473, 268, 114, 360, 654, 603, 238, 353, 851, 699, 315, 894, 194, 453, 143, 441, 436, 1075, 250, 679, 1009, 136, 849, 692, 613, 1208, 1106, 944, 181, 662, 1123, 690, 329, 1205, 700, 31, 192, 598, 391, 216, 532, 764, 55, 377, 425, 873, 298, 1176, 591, 1139, 828, 1194, 1187, 228, 659, 144, 760, 294, 1034, 99, 971, 371, 859, 409, 1016, 77, 110, 560, 999, 1196, 56, 1052, 548, 721, 413, 805, 343, 604, 340, 1059, 133, 1166, 793, 911, 386, 625, 784, 104, 14, 508, 1129, 882, 765, 624, 950, 57, 80, 831, 855, 633, 830, 362, 1103, 962]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6318669839843700
the save name prefix for this run is:  chkpt-ID_6318669839843700_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1093
rank avg (pred): 0.531 +- 0.004
mrr vals (pred, true): 0.000, 0.065
batch losses (mrrl, rdl): 0.0, 0.0008845372

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 617
rank avg (pred): 0.413 +- 0.218
mrr vals (pred, true): 0.001, 0.071
batch losses (mrrl, rdl): 0.0, 0.0001572028

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 906
rank avg (pred): 0.343 +- 0.238
mrr vals (pred, true): 0.162, 0.001
batch losses (mrrl, rdl): 0.0, 0.0013295197

Epoch over!
epoch time: 12.12

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 351
rank avg (pred): 0.377 +- 0.269
mrr vals (pred, true): 0.179, 0.052
batch losses (mrrl, rdl): 0.0, 5.69306e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 893
rank avg (pred): 0.371 +- 0.294
mrr vals (pred, true): 0.210, 0.003
batch losses (mrrl, rdl): 0.0, 8.5853e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 814
rank avg (pred): 0.095 +- 0.075
mrr vals (pred, true): 0.225, 0.010
batch losses (mrrl, rdl): 0.0, 0.0005061927

Epoch over!
epoch time: 12.059

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 892
rank avg (pred): 0.329 +- 0.264
mrr vals (pred, true): 0.210, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003509015

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1065
rank avg (pred): 0.091 +- 0.078
mrr vals (pred, true): 0.237, 0.240
batch losses (mrrl, rdl): 0.0, 3.85088e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 983
rank avg (pred): 0.156 +- 0.122
mrr vals (pred, true): 0.206, 0.192
batch losses (mrrl, rdl): 0.0, 5.97886e-05

Epoch over!
epoch time: 11.864

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 179
rank avg (pred): 0.399 +- 0.294
mrr vals (pred, true): 0.186, 0.001
batch losses (mrrl, rdl): 0.0, 7.69918e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 504
rank avg (pred): 0.286 +- 0.228
mrr vals (pred, true): 0.201, 0.277
batch losses (mrrl, rdl): 0.0, 0.0002646767

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.401 +- 0.310
mrr vals (pred, true): 0.207, 0.001
batch losses (mrrl, rdl): 0.0, 8.12925e-05

Epoch over!
epoch time: 12.222

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 474
rank avg (pred): 0.365 +- 0.293
mrr vals (pred, true): 0.213, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001615986

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 87
rank avg (pred): 0.376 +- 0.283
mrr vals (pred, true): 0.171, 0.063
batch losses (mrrl, rdl): 0.0, 5.92438e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 105
rank avg (pred): 0.427 +- 0.312
mrr vals (pred, true): 0.107, 0.067
batch losses (mrrl, rdl): 0.0, 0.0002766222

Epoch over!
epoch time: 12.116

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 688
rank avg (pred): 0.386 +- 0.309
mrr vals (pred, true): 0.180, 0.001
batch losses (mrrl, rdl): 0.1684342325, 0.0001372829

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 511
rank avg (pred): 0.096 +- 0.243
mrr vals (pred, true): 0.244, 0.275
batch losses (mrrl, rdl): 0.0096854456, 0.0002151576

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 692
rank avg (pred): 0.447 +- 0.222
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004457574, 6.01406e-05

Epoch over!
epoch time: 12.426

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 0
rank avg (pred): 0.419 +- 0.287
mrr vals (pred, true): 0.076, 0.279
batch losses (mrrl, rdl): 0.4115083814, 0.002409749

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 755
rank avg (pred): 0.207 +- 0.285
mrr vals (pred, true): 0.203, 0.147
batch losses (mrrl, rdl): 0.0319360755, 6.0285e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 252
rank avg (pred): 0.139 +- 0.263
mrr vals (pred, true): 0.201, 0.294
batch losses (mrrl, rdl): 0.087014623, 0.0001597458

Epoch over!
epoch time: 12.319

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 158
rank avg (pred): 0.436 +- 0.209
mrr vals (pred, true): 0.052, 0.062
batch losses (mrrl, rdl): 3.97175e-05, 0.0001877022

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 425
rank avg (pred): 0.425 +- 0.219
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.30338e-05, 0.0001378688

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 850
rank avg (pred): 0.455 +- 0.246
mrr vals (pred, true): 0.052, 0.003
batch losses (mrrl, rdl): 4.23559e-05, 9.80151e-05

Epoch over!
epoch time: 12.165

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 894
rank avg (pred): 0.556 +- 0.390
mrr vals (pred, true): 0.178, 0.000
batch losses (mrrl, rdl): 0.1637160182, 0.000950052

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 508
rank avg (pred): 0.100 +- 0.247
mrr vals (pred, true): 0.259, 0.274
batch losses (mrrl, rdl): 0.0022831529, 0.0002230441

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 621
rank avg (pred): 0.430 +- 0.333
mrr vals (pred, true): 0.062, 0.050
batch losses (mrrl, rdl): 0.001355906, 2.80999e-05

Epoch over!
epoch time: 12.362

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 364
rank avg (pred): 0.423 +- 0.310
mrr vals (pred, true): 0.048, 0.052
batch losses (mrrl, rdl): 2.39629e-05, 4.54625e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 525
rank avg (pred): 0.331 +- 0.333
mrr vals (pred, true): 0.160, 0.176
batch losses (mrrl, rdl): 0.0025535817, 0.0001565839

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 256
rank avg (pred): 0.052 +- 0.192
mrr vals (pred, true): 0.283, 0.303
batch losses (mrrl, rdl): 0.0038626352, 5.9983e-06

Epoch over!
epoch time: 12.441

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 354
rank avg (pred): 0.416 +- 0.317
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 6.1374e-06, 5.51077e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 649
rank avg (pred): 0.401 +- 0.325
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0009060759, 0.0002288888

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 746
rank avg (pred): 0.112 +- 0.199
mrr vals (pred, true): 0.210, 0.138
batch losses (mrrl, rdl): 0.0518072769, 0.0002079462

Epoch over!
epoch time: 12.339

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 446
rank avg (pred): 0.390 +- 0.330
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001597165, 0.0002054326

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 140
rank avg (pred): 0.390 +- 0.316
mrr vals (pred, true): 0.055, 0.047
batch losses (mrrl, rdl): 0.0002599424, 1.69041e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 346
rank avg (pred): 0.390 +- 0.233
mrr vals (pred, true): 0.047, 0.071
batch losses (mrrl, rdl): 6.8771e-05, 7.86776e-05

Epoch over!
epoch time: 12.135

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 786
rank avg (pred): 0.553 +- 0.292
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004632703, 2.17487e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 199
rank avg (pred): 0.412 +- 0.340
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.002288552, 0.0001576978

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 255
rank avg (pred): 0.055 +- 0.183
mrr vals (pred, true): 0.275, 0.299
batch losses (mrrl, rdl): 0.0058977222, 3.4707e-06

Epoch over!
epoch time: 12.127

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 297
rank avg (pred): 0.127 +- 0.202
mrr vals (pred, true): 0.187, 0.135
batch losses (mrrl, rdl): 0.0273599066, 3.0787e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 343
rank avg (pred): 0.385 +- 0.246
mrr vals (pred, true): 0.058, 0.085
batch losses (mrrl, rdl): 0.0005806325, 6.69189e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1162
rank avg (pred): 0.529 +- 0.295
mrr vals (pred, true): 0.047, 0.057
batch losses (mrrl, rdl): 9.19821e-05, 0.0003252633

Epoch over!
epoch time: 12.416

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 208
rank avg (pred): 0.423 +- 0.309
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001633508, 0.000139828

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 566
rank avg (pred): 0.187 +- 0.230
mrr vals (pred, true): 0.194, 0.182
batch losses (mrrl, rdl): 0.0014429364, 5.56511e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 428
rank avg (pred): 0.407 +- 0.226
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.63533e-05, 0.0001740026

Epoch over!
epoch time: 12.446

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.431 +- 0.356
mrr vals (pred, true): 0.050, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   89 	     0 	 0.16568 	 0.00011 	 MISS
   33 	     1 	 0.05220 	 0.00026 	 m..s
   34 	     2 	 0.05228 	 0.00028 	 m..s
   19 	     3 	 0.05011 	 0.00049 	 m..s
   50 	     4 	 0.05301 	 0.00050 	 m..s
   65 	     5 	 0.05471 	 0.00051 	 m..s
    6 	     6 	 0.04761 	 0.00051 	 m..s
   16 	     7 	 0.04989 	 0.00053 	 m..s
   15 	     8 	 0.04981 	 0.00054 	 m..s
    2 	     9 	 0.04299 	 0.00055 	 m..s
   56 	    10 	 0.05378 	 0.00055 	 m..s
   21 	    11 	 0.05066 	 0.00055 	 m..s
   27 	    12 	 0.05156 	 0.00056 	 m..s
   62 	    13 	 0.05431 	 0.00058 	 m..s
   28 	    14 	 0.05159 	 0.00058 	 m..s
   30 	    15 	 0.05183 	 0.00059 	 m..s
   59 	    16 	 0.05383 	 0.00059 	 m..s
   68 	    17 	 0.05497 	 0.00061 	 m..s
   41 	    18 	 0.05271 	 0.00061 	 m..s
   67 	    19 	 0.05495 	 0.00061 	 m..s
    7 	    20 	 0.04780 	 0.00062 	 m..s
   54 	    21 	 0.05351 	 0.00063 	 m..s
   55 	    22 	 0.05363 	 0.00063 	 m..s
   45 	    23 	 0.05285 	 0.00063 	 m..s
   29 	    24 	 0.05182 	 0.00064 	 m..s
   71 	    25 	 0.05518 	 0.00064 	 m..s
   63 	    26 	 0.05436 	 0.00068 	 m..s
   47 	    27 	 0.05287 	 0.00068 	 m..s
   49 	    28 	 0.05295 	 0.00069 	 m..s
   58 	    29 	 0.05382 	 0.00070 	 m..s
   72 	    30 	 0.05520 	 0.00071 	 m..s
   18 	    31 	 0.05006 	 0.00073 	 m..s
   31 	    32 	 0.05191 	 0.00076 	 m..s
   80 	    33 	 0.05640 	 0.00076 	 m..s
   66 	    34 	 0.05473 	 0.00077 	 m..s
   48 	    35 	 0.05294 	 0.00084 	 m..s
   38 	    36 	 0.05264 	 0.00084 	 m..s
   42 	    37 	 0.05273 	 0.00085 	 m..s
   57 	    38 	 0.05380 	 0.00085 	 m..s
    5 	    39 	 0.04685 	 0.00087 	 m..s
   40 	    40 	 0.05269 	 0.00087 	 m..s
   17 	    41 	 0.05001 	 0.00088 	 m..s
   79 	    42 	 0.05633 	 0.00089 	 m..s
   52 	    43 	 0.05331 	 0.00090 	 m..s
   51 	    44 	 0.05330 	 0.00092 	 m..s
   23 	    45 	 0.05116 	 0.00099 	 m..s
   73 	    46 	 0.05521 	 0.00104 	 m..s
   32 	    47 	 0.05219 	 0.00112 	 m..s
    0 	    48 	 0.04267 	 0.00115 	 m..s
   10 	    49 	 0.04809 	 0.00116 	 m..s
    9 	    50 	 0.04794 	 0.00129 	 m..s
   14 	    51 	 0.04874 	 0.00150 	 m..s
   25 	    52 	 0.05125 	 0.00292 	 m..s
   77 	    53 	 0.05598 	 0.04093 	 ~...
   60 	    54 	 0.05394 	 0.04796 	 ~...
   20 	    55 	 0.05055 	 0.04825 	 ~...
   37 	    56 	 0.05246 	 0.04843 	 ~...
   78 	    57 	 0.05598 	 0.04881 	 ~...
   75 	    58 	 0.05578 	 0.05117 	 ~...
   74 	    59 	 0.05523 	 0.05373 	 ~...
   61 	    60 	 0.05401 	 0.05576 	 ~...
   98 	    61 	 0.18610 	 0.05583 	 MISS
   46 	    62 	 0.05285 	 0.05587 	 ~...
   11 	    63 	 0.04820 	 0.05685 	 ~...
   43 	    64 	 0.05281 	 0.05708 	 ~...
   36 	    65 	 0.05233 	 0.05721 	 ~...
   12 	    66 	 0.04850 	 0.05831 	 ~...
   76 	    67 	 0.05597 	 0.05905 	 ~...
   69 	    68 	 0.05508 	 0.06083 	 ~...
   24 	    69 	 0.05118 	 0.06111 	 ~...
   26 	    70 	 0.05126 	 0.06246 	 ~...
   13 	    71 	 0.04858 	 0.06329 	 ~...
   64 	    72 	 0.05442 	 0.06412 	 ~...
    8 	    73 	 0.04789 	 0.06447 	 ~...
   53 	    74 	 0.05348 	 0.06524 	 ~...
   39 	    75 	 0.05267 	 0.06589 	 ~...
   82 	    76 	 0.06039 	 0.06605 	 ~...
   70 	    77 	 0.05509 	 0.06669 	 ~...
   22 	    78 	 0.05115 	 0.06786 	 ~...
    3 	    79 	 0.04335 	 0.06842 	 ~...
   44 	    80 	 0.05284 	 0.06947 	 ~...
   35 	    81 	 0.05229 	 0.07383 	 ~...
   83 	    82 	 0.06040 	 0.07508 	 ~...
   81 	    83 	 0.05645 	 0.07680 	 ~...
    4 	    84 	 0.04341 	 0.07720 	 m..s
    1 	    85 	 0.04294 	 0.08425 	 m..s
   87 	    86 	 0.15687 	 0.13221 	 ~...
   94 	    87 	 0.18359 	 0.14065 	 m..s
   97 	    88 	 0.18415 	 0.14178 	 m..s
   96 	    89 	 0.18398 	 0.14441 	 m..s
  100 	    90 	 0.18805 	 0.14548 	 m..s
   95 	    91 	 0.18384 	 0.14839 	 m..s
   93 	    92 	 0.18303 	 0.15264 	 m..s
  102 	    93 	 0.19485 	 0.15826 	 m..s
  103 	    94 	 0.19538 	 0.15863 	 m..s
   90 	    95 	 0.17177 	 0.16162 	 ~...
  105 	    96 	 0.20004 	 0.17081 	 ~...
   91 	    97 	 0.17394 	 0.17280 	 ~...
   92 	    98 	 0.18220 	 0.18394 	 ~...
   84 	    99 	 0.11959 	 0.18582 	 m..s
   99 	   100 	 0.18777 	 0.19277 	 ~...
   88 	   101 	 0.16564 	 0.19856 	 m..s
  101 	   102 	 0.18967 	 0.20633 	 ~...
   85 	   103 	 0.14202 	 0.22645 	 m..s
  104 	   104 	 0.19955 	 0.22740 	 ~...
  111 	   105 	 0.22750 	 0.22805 	 ~...
   86 	   106 	 0.15076 	 0.23016 	 m..s
  106 	   107 	 0.20166 	 0.23716 	 m..s
  107 	   108 	 0.20198 	 0.23907 	 m..s
  113 	   109 	 0.25271 	 0.24736 	 ~...
  115 	   110 	 0.25655 	 0.24895 	 ~...
  108 	   111 	 0.20306 	 0.25400 	 m..s
  116 	   112 	 0.26044 	 0.25682 	 ~...
  110 	   113 	 0.21974 	 0.25900 	 m..s
  117 	   114 	 0.26160 	 0.26847 	 ~...
  109 	   115 	 0.21292 	 0.27434 	 m..s
  112 	   116 	 0.24775 	 0.27927 	 m..s
  114 	   117 	 0.25617 	 0.28330 	 ~...
  120 	   118 	 0.30301 	 0.30087 	 ~...
  119 	   119 	 0.28585 	 0.30201 	 ~...
  118 	   120 	 0.28474 	 0.30501 	 ~...
==========================================
r_mrr = 0.917648196220398
r2_mrr = 0.7800756096839905
spearmanr_mrr@5 = 0.9542220234870911
spearmanr_mrr@10 = 0.9431554675102234
spearmanr_mrr@50 = 0.9635443687438965
spearmanr_mrr@100 = 0.9629903435707092
spearmanr_mrr@All = 0.9651287198066711
==========================================
test time: 0.457
Done Testing dataset OpenEA
total time taken: 199.7896921634674
training time taken: 184.07471632957458
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9176)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.7801)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9542)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9432)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9635)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9630)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9651)}}, 'test_loss': {'DistMult': {'OpenEA': 0.8192383947553026}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 9578538038609070
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [620, 1020, 329, 1171, 858, 249, 1115, 176, 966, 923, 215, 389, 623, 562, 302, 324, 229, 263, 419, 1203, 847, 880, 739, 998, 440, 888, 248, 710, 1106, 288, 396, 1127, 887, 808, 1206, 782, 631, 748, 1119, 769, 31, 678, 242, 1099, 184, 757, 260, 875, 694, 790, 610, 200, 1068, 776, 46, 1180, 702, 365, 1151, 788, 132, 959, 500, 659, 204, 523, 687, 817, 1175, 896, 539, 864, 940, 158, 379, 391, 559, 425, 439, 924, 581, 1103, 587, 648, 162, 371, 1212, 239, 763, 690, 749, 984, 1192, 466, 818, 795, 363, 475, 778, 452, 462, 540, 746, 23, 489, 871, 962, 698, 272, 988, 412, 291, 120, 1147, 1077, 553, 815, 618, 98, 609, 221]
valid_ids (0): []
train_ids (1094): [787, 181, 297, 147, 29, 281, 898, 450, 773, 846, 1056, 1148, 433, 568, 897, 807, 293, 919, 717, 1105, 464, 392, 93, 18, 490, 201, 920, 1154, 517, 1052, 624, 600, 792, 1143, 1118, 573, 820, 825, 238, 415, 634, 435, 222, 264, 980, 65, 469, 1022, 593, 252, 731, 234, 542, 22, 987, 431, 308, 340, 637, 766, 115, 1110, 527, 479, 52, 917, 35, 616, 1098, 432, 546, 455, 76, 67, 960, 567, 674, 833, 992, 45, 767, 764, 352, 1079, 999, 1201, 621, 355, 1094, 693, 1125, 974, 438, 643, 633, 884, 1161, 967, 822, 358, 1018, 564, 413, 592, 226, 1182, 666, 699, 575, 989, 971, 668, 774, 194, 997, 28, 232, 1091, 783, 1062, 118, 346, 47, 803, 740, 1166, 834, 382, 712, 544, 642, 685, 713, 359, 853, 571, 467, 253, 1021, 483, 1071, 1023, 797, 626, 629, 1093, 119, 1037, 1028, 647, 1202, 628, 481, 276, 1156, 401, 709, 1191, 294, 217, 1104, 1000, 859, 1015, 726, 470, 134, 986, 321, 497, 4, 225, 1160, 649, 196, 163, 169, 1186, 357, 160, 370, 289, 195, 1102, 918, 100, 381, 102, 138, 1038, 107, 1096, 451, 1090, 56, 211, 180, 977, 613, 762, 874, 170, 771, 420, 186, 1041, 233, 636, 561, 531, 916, 759, 137, 1111, 473, 1047, 255, 727, 758, 1117, 1025, 597, 730, 1165, 760, 150, 965, 903, 548, 877, 1139, 349, 237, 19, 70, 832, 1185, 198, 85, 1149, 216, 798, 388, 913, 1051, 1162, 973, 948, 251, 696, 104, 503, 1024, 259, 535, 656, 1060, 101, 1057, 686, 77, 208, 277, 84, 1109, 886, 1152, 632, 582, 1135, 1164, 437, 453, 484, 520, 1211, 824, 522, 5, 376, 703, 518, 968, 755, 579, 243, 446, 931, 635, 1014, 405, 284, 1138, 1133, 675, 625, 7, 1064, 143, 1004, 951, 835, 32, 779, 975, 614, 86, 323, 283, 384, 122, 105, 909, 1072, 958, 839, 867, 828, 729, 353, 1005, 1080, 250, 534, 41, 1045, 350, 1101, 129, 369, 1140, 905, 1214, 156, 397, 171, 873, 700, 203, 135, 943, 378, 953, 202, 494, 849, 280, 61, 870, 1061, 1126, 399, 715, 88, 59, 73, 861, 53, 262, 644, 44, 1034, 444, 268, 258, 1026, 768, 558, 509, 220, 338, 735, 1196, 1074, 994, 417, 716, 906, 947, 205, 848, 1008, 317, 780, 166, 1007, 505, 409, 14, 179, 476, 1054, 673, 21, 930, 765, 83, 336, 922, 69, 1176, 80, 360, 454, 136, 1011, 572, 1173, 142, 153, 1058, 366, 511, 615, 598, 602, 1153, 182, 372, 1210, 254, 826, 552, 275, 299, 63, 599, 1112, 172, 1084, 578, 944, 830, 996, 1078, 159, 55, 993, 810, 802, 537, 328, 528, 653, 1194, 236, 214, 510, 25, 227, 301, 1050, 364, 1204, 1035, 530, 1174, 210, 801, 175, 661, 719, 374, 213, 584, 942, 486, 811, 0, 459, 337, 292, 448, 168, 662, 287, 1006, 991, 402, 50, 133, 112, 646, 1159, 796, 403, 62, 167, 532, 1097, 271, 688, 718, 1137, 241, 821, 261, 844, 315, 1184, 1036, 79, 458, 152, 131, 639, 836, 1048, 747, 929, 508, 689, 926, 191, 894, 752, 295, 816, 447, 8, 212, 316, 725, 1046, 941, 630, 161, 309, 94, 68, 491, 851, 183, 1172, 722, 342, 895, 892, 445, 720, 770, 640, 1017, 524, 970, 326, 197, 1200, 13, 1145, 304, 144, 1086, 273, 576, 738, 536, 979, 334, 589, 1065, 485, 914, 1197, 854, 869, 850, 925, 612, 177, 488, 27, 937, 1095, 428, 300, 743, 165, 800, 728, 723, 857, 332, 981, 928, 495, 36, 130, 480, 734, 1042, 964, 1027, 704, 619, 1059, 956, 286, 591, 43, 375, 310, 889, 1029, 921, 1199, 1190, 827, 680, 560, 1142, 344, 513, 406, 9, 1040, 333, 706, 1167, 541, 487, 840, 306, 1085, 207, 141, 282, 805, 603, 125, 1, 985, 708, 188, 651, 679, 550, 1114, 1066, 411, 842, 976, 496, 878, 1136, 1113, 580, 588, 1129, 416, 1044, 1134, 1157, 51, 377, 116, 1187, 1002, 111, 912, 343, 2, 1116, 794, 590, 502, 347, 1012, 789, 348, 876, 872, 585, 775, 240, 1055, 1010, 1131, 934, 1189, 89, 705, 113, 1177, 1069, 506, 421, 732, 1087, 891, 461, 902, 911, 607, 809, 477, 314, 1130, 12, 1063, 331, 20, 148, 54, 193, 617, 1144, 66, 1195, 804, 395, 935, 269, 654, 6, 482, 952, 414, 145, 963, 1123, 777, 128, 430, 692, 110, 865, 187, 26, 38, 799, 504, 681, 278, 650, 424, 1132, 361, 711, 1049, 784, 1108, 669, 695, 1073, 881, 1188, 91, 772, 622, 37, 174, 545, 1208, 990, 1158, 521, 322, 551, 74, 831, 10, 390, 17, 744, 751, 761, 1053, 154, 982, 745, 1181, 157, 901, 185, 164, 933, 1183, 501, 684, 404, 515, 983, 676, 368, 367, 422, 586, 950, 427, 936, 596, 879, 472, 189, 474, 893, 721, 1088, 99, 638, 387, 742, 1169, 672, 190, 664, 554, 78, 463, 30, 298, 793, 1193, 307, 383, 219, 841, 611, 426, 883, 408, 516, 829, 493, 677, 34, 1009, 819, 311, 1120, 781, 97, 199, 750, 449, 663, 682, 945, 274, 478, 303, 58, 103, 1178, 39, 574, 305, 290, 319, 556, 512, 33, 1003, 436, 645, 256, 813, 423, 863, 538, 279, 791, 209, 407, 57, 658, 140, 24, 16, 223, 837, 1121, 498, 410, 946, 320, 670, 285, 978, 87, 492, 665, 296, 786, 957, 149, 547, 60, 855, 441, 1075, 660, 151, 72, 1107, 499, 1092, 40, 714, 1043, 124, 90, 915, 868, 139, 265, 1076, 106, 230, 885, 1198, 362, 1082, 457, 1122, 595, 386, 701, 555, 608, 244, 270, 1163, 96, 82, 533, 393, 121, 932, 456, 549, 318, 908, 11, 641, 525, 900, 529, 570, 860, 351, 224, 228, 961, 890, 114, 1030, 856, 1039, 206, 1019, 606, 838, 1141, 1124, 519, 117, 939, 1033, 460, 1170, 235, 927, 1205, 1150, 1100, 48, 81, 852, 1128, 812, 418, 335, 380, 92, 443, 652, 594, 691, 1067, 434, 247, 604, 109, 938, 907, 862, 465, 972, 514, 1179, 995, 339, 507, 569, 313, 42, 1213, 356, 823, 246, 655, 1146, 753, 904, 429, 1070, 442, 949, 1032, 123, 707, 1089, 1016, 266, 1081, 385, 15, 400, 736, 178, 95, 468, 267, 173, 325, 330, 1209, 601, 1001, 354, 192, 126, 394, 49, 557, 565, 583, 1031, 683, 231, 1168, 741, 667, 910, 845, 955, 1207, 1013, 373, 471, 754, 341, 806, 785, 345, 127, 526, 724, 64, 756, 954, 577, 327, 899, 1083, 657, 737, 882, 566, 969, 71, 3, 543, 697, 108, 155, 398, 257, 245, 605, 563, 866, 218, 75, 1155, 146, 814, 627, 733, 671, 312, 843]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  677701165176765
the save name prefix for this run is:  chkpt-ID_677701165176765_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 820
rank avg (pred): 0.491 +- 0.003
mrr vals (pred, true): 0.000, 0.139
batch losses (mrrl, rdl): 0.0, 0.0029757118

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 157
rank avg (pred): 0.434 +- 0.267
mrr vals (pred, true): 0.123, 0.074
batch losses (mrrl, rdl): 0.0, 0.0002658112

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 597
rank avg (pred): 0.407 +- 0.295
mrr vals (pred, true): 0.168, 0.051
batch losses (mrrl, rdl): 0.0, 9.67977e-05

Epoch over!
epoch time: 12.121

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 834
rank avg (pred): 0.340 +- 0.263
mrr vals (pred, true): 0.175, 0.163
batch losses (mrrl, rdl): 0.0, 0.0007779812

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 948
rank avg (pred): 0.482 +- 0.336
mrr vals (pred, true): 0.147, 0.001
batch losses (mrrl, rdl): 0.0, 1.81938e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 662
rank avg (pred): 0.410 +- 0.318
mrr vals (pred, true): 0.120, 0.001
batch losses (mrrl, rdl): 0.0, 8.57063e-05

Epoch over!
epoch time: 11.883

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 44
rank avg (pred): 0.148 +- 0.213
mrr vals (pred, true): 0.176, 0.237
batch losses (mrrl, rdl): 0.0, 4.52081e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 254
rank avg (pred): 0.124 +- 0.221
mrr vals (pred, true): 0.195, 0.304
batch losses (mrrl, rdl): 0.0, 0.0001621474

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 130
rank avg (pred): 0.436 +- 0.322
mrr vals (pred, true): 0.125, 0.065
batch losses (mrrl, rdl): 0.0, 0.0001743185

Epoch over!
epoch time: 12.027

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 39
rank avg (pred): 0.147 +- 0.253
mrr vals (pred, true): 0.176, 0.183
batch losses (mrrl, rdl): 0.0, 2.86595e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 992
rank avg (pred): 0.128 +- 0.246
mrr vals (pred, true): 0.232, 0.203
batch losses (mrrl, rdl): 0.0, 1.66242e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1029
rank avg (pred): 0.368 +- 0.309
mrr vals (pred, true): 0.166, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002338814

Epoch over!
epoch time: 11.881

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 62
rank avg (pred): 0.114 +- 0.216
mrr vals (pred, true): 0.205, 0.151
batch losses (mrrl, rdl): 0.0, 7.639e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 42
rank avg (pred): 0.115 +- 0.217
mrr vals (pred, true): 0.203, 0.203
batch losses (mrrl, rdl): 0.0, 3.2236e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1107
rank avg (pred): 0.423 +- 0.317
mrr vals (pred, true): 0.148, 0.001
batch losses (mrrl, rdl): 0.0, 8.17041e-05

Epoch over!
epoch time: 12.005

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.122 +- 0.224
mrr vals (pred, true): 0.190, 0.269
batch losses (mrrl, rdl): 0.0629252046, 0.0001215926

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 270
rank avg (pred): 0.154 +- 0.198
mrr vals (pred, true): 0.185, 0.148
batch losses (mrrl, rdl): 0.0131312776, 4.2867e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1161
rank avg (pred): 0.446 +- 0.176
mrr vals (pred, true): 0.050, 0.059
batch losses (mrrl, rdl): 1.5155e-06, 0.0002374689

Epoch over!
epoch time: 12.318

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1083
rank avg (pred): 0.434 +- 0.163
mrr vals (pred, true): 0.046, 0.080
batch losses (mrrl, rdl): 0.0001816265, 0.0002746393

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1196
rank avg (pred): 0.441 +- 0.170
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001691444, 8.99908e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 474
rank avg (pred): 0.422 +- 0.172
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001647649, 0.0001202298

Epoch over!
epoch time: 12.207

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 555
rank avg (pred): 0.192 +- 0.146
mrr vals (pred, true): 0.154, 0.154
batch losses (mrrl, rdl): 3.2144e-06, 0.0001018907

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 995
rank avg (pred): 0.134 +- 0.127
mrr vals (pred, true): 0.224, 0.217
batch losses (mrrl, rdl): 0.0004722784, 2.84338e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 2
rank avg (pred): 0.077 +- 0.091
mrr vals (pred, true): 0.272, 0.279
batch losses (mrrl, rdl): 0.0005085046, 1.24462e-05

Epoch over!
epoch time: 12.223

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 580
rank avg (pred): 0.450 +- 0.267
mrr vals (pred, true): 0.060, 0.058
batch losses (mrrl, rdl): 0.0010615876, 0.0001097843

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 791
rank avg (pred): 0.636 +- 0.264
mrr vals (pred, true): 0.037, 0.001
batch losses (mrrl, rdl): 0.0018011061, 0.0003262878

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 17
rank avg (pred): 0.100 +- 0.114
mrr vals (pred, true): 0.283, 0.305
batch losses (mrrl, rdl): 0.0046554636, 3.56543e-05

Epoch over!
epoch time: 12.168

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 441
rank avg (pred): 0.449 +- 0.259
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.81512e-05, 6.82103e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 787
rank avg (pred): 0.603 +- 0.294
mrr vals (pred, true): 0.044, 0.001
batch losses (mrrl, rdl): 0.000372019, 0.0002104595

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 352
rank avg (pred): 0.446 +- 0.277
mrr vals (pred, true): 0.055, 0.048
batch losses (mrrl, rdl): 0.0002306138, 0.0001234728

Epoch over!
epoch time: 11.902

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 357
rank avg (pred): 0.440 +- 0.268
mrr vals (pred, true): 0.056, 0.049
batch losses (mrrl, rdl): 0.0003510324, 0.0001258172

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1034
rank avg (pred): 0.443 +- 0.279
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.7484e-06, 9.19829e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 131
rank avg (pred): 0.473 +- 0.281
mrr vals (pred, true): 0.048, 0.059
batch losses (mrrl, rdl): 5.84642e-05, 0.0002163701

Epoch over!
epoch time: 12.446

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 920
rank avg (pred): 0.574 +- 0.288
mrr vals (pred, true): 0.052, 0.004
batch losses (mrrl, rdl): 5.32877e-05, 6.39044e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 472
rank avg (pred): 0.432 +- 0.263
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.34533e-05, 0.0001425157

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1036
rank avg (pred): 0.446 +- 0.281
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.000254791, 0.000109497

Epoch over!
epoch time: 12.266

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 768
rank avg (pred): 0.521 +- 0.285
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001125205, 7.27172e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 14
rank avg (pred): 0.101 +- 0.117
mrr vals (pred, true): 0.285, 0.299
batch losses (mrrl, rdl): 0.0018167606, 3.39247e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 626
rank avg (pred): 0.449 +- 0.282
mrr vals (pred, true): 0.043, 0.050
batch losses (mrrl, rdl): 0.0004804867, 9.99823e-05

Epoch over!
epoch time: 11.975

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 504
rank avg (pred): 0.086 +- 0.104
mrr vals (pred, true): 0.294, 0.277
batch losses (mrrl, rdl): 0.0028616539, 0.0003105494

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 536
rank avg (pred): 0.151 +- 0.146
mrr vals (pred, true): 0.226, 0.204
batch losses (mrrl, rdl): 0.0048911967, 0.0001653445

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 507
rank avg (pred): 0.112 +- 0.125
mrr vals (pred, true): 0.287, 0.277
batch losses (mrrl, rdl): 0.0010465849, 0.0001976627

Epoch over!
epoch time: 12.17

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 555
rank avg (pred): 0.179 +- 0.145
mrr vals (pred, true): 0.196, 0.154
batch losses (mrrl, rdl): 0.0179257933, 0.0001221458

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 392
rank avg (pred): 0.439 +- 0.288
mrr vals (pred, true): 0.051, 0.059
batch losses (mrrl, rdl): 1.85592e-05, 8.69771e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1008
rank avg (pred): 0.436 +- 0.285
mrr vals (pred, true): 0.049, 0.050
batch losses (mrrl, rdl): 2.14199e-05, 6.87589e-05

Epoch over!
epoch time: 11.942

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.463 +- 0.284
mrr vals (pred, true): 0.048, 0.055

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   45 	     0 	 0.04935 	 9e-0500 	 m..s
   87 	     1 	 0.05353 	 0.00011 	 m..s
   70 	     2 	 0.05106 	 0.00013 	 m..s
   66 	     3 	 0.05070 	 0.00030 	 m..s
   29 	     4 	 0.04852 	 0.00048 	 m..s
   84 	     5 	 0.05303 	 0.00049 	 m..s
   27 	     6 	 0.04833 	 0.00049 	 m..s
   41 	     7 	 0.04910 	 0.00050 	 m..s
   52 	     8 	 0.04984 	 0.00051 	 m..s
   83 	     9 	 0.05254 	 0.00051 	 m..s
    1 	    10 	 0.04594 	 0.00052 	 m..s
   44 	    11 	 0.04912 	 0.00055 	 m..s
    0 	    12 	 0.04543 	 0.00055 	 m..s
   50 	    13 	 0.04982 	 0.00056 	 m..s
   10 	    14 	 0.04727 	 0.00057 	 m..s
   26 	    15 	 0.04821 	 0.00057 	 m..s
   86 	    16 	 0.05352 	 0.00058 	 m..s
   81 	    17 	 0.05237 	 0.00058 	 m..s
   21 	    18 	 0.04795 	 0.00058 	 m..s
   25 	    19 	 0.04808 	 0.00059 	 m..s
   16 	    20 	 0.04783 	 0.00060 	 m..s
    3 	    21 	 0.04610 	 0.00060 	 m..s
   41 	    22 	 0.04910 	 0.00061 	 m..s
   16 	    23 	 0.04783 	 0.00061 	 m..s
   24 	    24 	 0.04807 	 0.00061 	 m..s
    4 	    25 	 0.04618 	 0.00063 	 m..s
    7 	    26 	 0.04713 	 0.00063 	 m..s
   37 	    27 	 0.04896 	 0.00066 	 m..s
   12 	    28 	 0.04729 	 0.00066 	 m..s
   61 	    29 	 0.05032 	 0.00068 	 m..s
   46 	    30 	 0.04938 	 0.00069 	 m..s
   39 	    31 	 0.04909 	 0.00069 	 m..s
   10 	    32 	 0.04727 	 0.00070 	 m..s
   50 	    33 	 0.04982 	 0.00071 	 m..s
   48 	    34 	 0.04956 	 0.00071 	 m..s
   49 	    35 	 0.04980 	 0.00073 	 m..s
    6 	    36 	 0.04709 	 0.00074 	 m..s
   67 	    37 	 0.05094 	 0.00075 	 m..s
   74 	    38 	 0.05122 	 0.00076 	 m..s
   14 	    39 	 0.04749 	 0.00077 	 m..s
   31 	    40 	 0.04875 	 0.00078 	 m..s
    9 	    41 	 0.04720 	 0.00078 	 m..s
   79 	    42 	 0.05205 	 0.00082 	 m..s
    8 	    43 	 0.04717 	 0.00082 	 m..s
   54 	    44 	 0.05009 	 0.00083 	 m..s
   72 	    45 	 0.05111 	 0.00086 	 m..s
   13 	    46 	 0.04737 	 0.00087 	 m..s
   68 	    47 	 0.05099 	 0.00093 	 m..s
   75 	    48 	 0.05127 	 0.00098 	 m..s
    5 	    49 	 0.04651 	 0.00098 	 m..s
   62 	    50 	 0.05045 	 0.00107 	 m..s
   15 	    51 	 0.04771 	 0.00109 	 m..s
   53 	    52 	 0.04991 	 0.00112 	 m..s
   55 	    53 	 0.05009 	 0.00112 	 m..s
   65 	    54 	 0.05066 	 0.00134 	 m..s
   80 	    55 	 0.05208 	 0.00150 	 m..s
   70 	    56 	 0.05106 	 0.00255 	 m..s
   90 	    57 	 0.05726 	 0.00438 	 m..s
   89 	    58 	 0.05367 	 0.00439 	 m..s
   73 	    59 	 0.05113 	 0.00458 	 m..s
   30 	    60 	 0.04864 	 0.00628 	 m..s
   34 	    61 	 0.04890 	 0.00717 	 m..s
   87 	    62 	 0.05353 	 0.01048 	 m..s
   36 	    63 	 0.04894 	 0.04173 	 ~...
   58 	    64 	 0.05024 	 0.05208 	 ~...
   18 	    65 	 0.04792 	 0.05231 	 ~...
   38 	    66 	 0.04907 	 0.05373 	 ~...
   19 	    67 	 0.04793 	 0.05544 	 ~...
   22 	    68 	 0.04801 	 0.05559 	 ~...
   85 	    69 	 0.05337 	 0.05564 	 ~...
   28 	    70 	 0.04845 	 0.05576 	 ~...
   32 	    71 	 0.04880 	 0.05605 	 ~...
   22 	    72 	 0.04801 	 0.05658 	 ~...
   56 	    73 	 0.05019 	 0.05701 	 ~...
   35 	    74 	 0.04892 	 0.05707 	 ~...
   82 	    75 	 0.05245 	 0.05720 	 ~...
   59 	    76 	 0.05024 	 0.05816 	 ~...
   20 	    77 	 0.04793 	 0.05889 	 ~...
   78 	    78 	 0.05174 	 0.05927 	 ~...
   59 	    79 	 0.05024 	 0.06111 	 ~...
   69 	    80 	 0.05105 	 0.06146 	 ~...
   33 	    81 	 0.04880 	 0.06177 	 ~...
   47 	    82 	 0.04945 	 0.06192 	 ~...
   77 	    83 	 0.05150 	 0.06258 	 ~...
   56 	    84 	 0.05019 	 0.06554 	 ~...
   64 	    85 	 0.05059 	 0.06582 	 ~...
    2 	    86 	 0.04601 	 0.06605 	 ~...
   40 	    87 	 0.04910 	 0.06828 	 ~...
   43 	    88 	 0.04910 	 0.07174 	 ~...
   63 	    89 	 0.05055 	 0.07673 	 ~...
   76 	    90 	 0.05130 	 0.07943 	 ~...
   92 	    91 	 0.08947 	 0.12495 	 m..s
   93 	    92 	 0.08948 	 0.12784 	 m..s
   95 	    93 	 0.12997 	 0.13046 	 ~...
   91 	    94 	 0.08622 	 0.13299 	 m..s
   94 	    95 	 0.10448 	 0.13828 	 m..s
   96 	    96 	 0.13595 	 0.15219 	 ~...
   97 	    97 	 0.15416 	 0.15826 	 ~...
   98 	    98 	 0.15656 	 0.16133 	 ~...
   99 	    99 	 0.17375 	 0.16226 	 ~...
  100 	   100 	 0.17597 	 0.17126 	 ~...
  103 	   101 	 0.18847 	 0.17309 	 ~...
  101 	   102 	 0.18649 	 0.18566 	 ~...
  104 	   103 	 0.18937 	 0.20099 	 ~...
  107 	   104 	 0.20696 	 0.21487 	 ~...
  113 	   105 	 0.22157 	 0.23016 	 ~...
  108 	   106 	 0.20757 	 0.23311 	 ~...
  102 	   107 	 0.18732 	 0.23443 	 m..s
  110 	   108 	 0.21211 	 0.25365 	 m..s
  115 	   109 	 0.26225 	 0.25400 	 ~...
  105 	   110 	 0.19015 	 0.25762 	 m..s
  111 	   111 	 0.21425 	 0.25811 	 m..s
  109 	   112 	 0.21126 	 0.26056 	 m..s
  112 	   113 	 0.22047 	 0.26359 	 m..s
  117 	   114 	 0.27206 	 0.26721 	 ~...
  106 	   115 	 0.19199 	 0.26783 	 m..s
  116 	   116 	 0.27166 	 0.27823 	 ~...
  114 	   117 	 0.26005 	 0.28330 	 ~...
  120 	   118 	 0.27558 	 0.30087 	 ~...
  118 	   119 	 0.27275 	 0.30184 	 ~...
  119 	   120 	 0.27395 	 0.30663 	 m..s
==========================================
r_mrr = 0.9514942169189453
r2_mrr = 0.8245859146118164
spearmanr_mrr@5 = 0.8683622479438782
spearmanr_mrr@10 = 0.7921599745750427
spearmanr_mrr@50 = 0.985292375087738
spearmanr_mrr@100 = 0.9606937170028687
spearmanr_mrr@All = 0.9609020352363586
==========================================
test time: 0.441
Done Testing dataset OpenEA
total time taken: 197.0904200077057
training time taken: 182.03771114349365
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9515)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8246)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.8684)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.7922)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9853)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9607)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9609)}}, 'test_loss': {'DistMult': {'OpenEA': 0.34048276064913807}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 8943215232102266
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1048, 1068, 1012, 496, 142, 80, 1104, 306, 226, 4, 799, 873, 84, 613, 1108, 55, 237, 83, 937, 738, 382, 1013, 296, 1161, 1061, 827, 1020, 479, 1030, 38, 90, 953, 797, 1203, 1034, 86, 133, 275, 268, 18, 1112, 831, 978, 821, 701, 312, 396, 12, 244, 230, 228, 642, 639, 885, 210, 984, 598, 49, 924, 121, 472, 119, 166, 505, 888, 1186, 176, 510, 1005, 656, 747, 155, 657, 162, 285, 617, 968, 484, 658, 489, 1166, 167, 1199, 838, 1143, 906, 216, 565, 494, 264, 1006, 1167, 135, 940, 7, 693, 486, 516, 807, 82, 542, 1040, 1037, 1052, 463, 1025, 746, 87, 735, 644, 1043, 626, 820, 730, 962, 1152, 409, 485, 1036, 917, 520]
valid_ids (0): []
train_ids (1094): [1007, 459, 1131, 305, 803, 804, 1063, 708, 1154, 1059, 842, 321, 1057, 234, 384, 905, 932, 106, 192, 1195, 32, 352, 614, 28, 127, 1046, 1078, 1111, 1180, 722, 488, 893, 497, 548, 713, 531, 1162, 871, 220, 1130, 91, 326, 99, 37, 1003, 443, 622, 624, 1023, 1065, 1120, 538, 118, 575, 1049, 883, 270, 715, 332, 233, 360, 442, 290, 76, 1002, 94, 753, 774, 79, 736, 213, 303, 519, 635, 0, 467, 178, 1188, 594, 462, 665, 1200, 775, 853, 951, 277, 828, 407, 1055, 802, 822, 1145, 426, 338, 660, 209, 530, 766, 1075, 1213, 574, 661, 963, 196, 337, 999, 131, 517, 336, 1193, 198, 362, 819, 1032, 100, 421, 719, 381, 535, 707, 501, 864, 895, 779, 177, 1054, 1207, 731, 525, 374, 544, 11, 325, 588, 65, 1092, 441, 430, 255, 1122, 30, 51, 1132, 604, 982, 621, 111, 95, 359, 239, 279, 1102, 561, 206, 385, 278, 1087, 638, 868, 700, 221, 683, 493, 651, 712, 414, 1156, 633, 972, 444, 402, 1026, 322, 636, 343, 720, 699, 431, 449, 840, 907, 58, 959, 768, 1045, 1204, 834, 474, 481, 53, 438, 1093, 1202, 796, 559, 1079, 364, 299, 434, 181, 23, 33, 781, 291, 283, 1191, 369, 584, 50, 218, 896, 1080, 227, 714, 350, 579, 400, 539, 97, 455, 85, 767, 954, 592, 541, 132, 750, 1, 199, 1085, 928, 1010, 377, 760, 180, 846, 787, 1201, 345, 1101, 1123, 667, 1174, 881, 17, 418, 757, 550, 504, 1091, 41, 892, 625, 25, 814, 1171, 419, 568, 911, 174, 1008, 785, 253, 1135, 694, 734, 356, 808, 1113, 789, 380, 1069, 589, 26, 406, 717, 778, 543, 829, 946, 355, 487, 113, 772, 643, 901, 903, 507, 263, 784, 690, 294, 971, 1173, 465, 231, 791, 521, 666, 1185, 36, 70, 692, 859, 123, 764, 894, 1172, 733, 185, 511, 897, 1077, 931, 391, 183, 751, 632, 117, 967, 170, 351, 1121, 383, 242, 1160, 677, 1175, 96, 1000, 674, 71, 1165, 347, 495, 593, 47, 1138, 62, 865, 482, 891, 756, 282, 152, 556, 211, 801, 102, 393, 115, 1021, 236, 532, 925, 59, 267, 841, 995, 104, 606, 970, 786, 798, 920, 620, 318, 52, 655, 728, 109, 609, 31, 1070, 882, 762, 450, 718, 676, 42, 288, 116, 250, 172, 836, 794, 949, 673, 314, 957, 223, 1129, 600, 912, 1164, 653, 1190, 522, 468, 1117, 114, 1194, 397, 145, 195, 862, 705, 867, 1009, 1187, 998, 46, 861, 508, 456, 358, 562, 184, 1004, 502, 1126, 551, 696, 1110, 554, 1184, 723, 628, 850, 825, 150, 961, 182, 1056, 466, 378, 256, 761, 1144, 165, 514, 874, 14, 63, 448, 533, 57, 153, 191, 260, 317, 737, 988, 523, 222, 390, 366, 682, 1064, 35, 73, 103, 681, 680, 577, 637, 134, 981, 387, 404, 721, 1103, 179, 1197, 685, 729, 289, 440, 432, 711, 411, 599, 149, 405, 884, 899, 190, 156, 433, 24, 607, 413, 204, 6, 212, 476, 765, 460, 706, 710, 194, 478, 395, 500, 560, 975, 889, 420, 1146, 458, 688, 107, 1050, 410, 408, 368, 1125, 547, 93, 439, 950, 399, 566, 980, 668, 740, 641, 146, 371, 812, 994, 852, 754, 866, 445, 926, 569, 748, 663, 634, 265, 157, 608, 858, 938, 1119, 1017, 1105, 331, 1139, 545, 1137, 66, 704, 847, 996, 349, 886, 365, 246, 716, 851, 1019, 136, 993, 943, 973, 702, 1183, 72, 528, 1076, 346, 1149, 983, 175, 88, 375, 1029, 755, 571, 247, 379, 776, 1095, 403, 258, 54, 936, 672, 709, 126, 143, 354, 570, 168, 208, 526, 363, 125, 1086, 915, 1016, 670, 171, 913, 469, 743, 164, 424, 647, 910, 373, 394, 724, 537, 1018, 138, 534, 45, 1177, 428, 811, 1060, 215, 447, 876, 1134, 266, 1053, 1163, 1044, 1028, 140, 939, 415, 572, 631, 902, 583, 969, 339, 1205, 1155, 646, 1074, 16, 344, 159, 74, 947, 112, 29, 1192, 879, 64, 612, 650, 348, 471, 13, 1157, 147, 129, 898, 238, 887, 1153, 202, 353, 506, 890, 578, 1118, 48, 1039, 576, 923, 948, 823, 1210, 284, 752, 652, 933, 1031, 934, 997, 503, 837, 451, 669, 271, 68, 161, 989, 619, 726, 813, 662, 582, 334, 308, 311, 1170, 108, 742, 872, 992, 990, 141, 67, 1116, 262, 40, 857, 122, 130, 557, 1182, 780, 187, 137, 1098, 1176, 1088, 596, 540, 173, 1022, 75, 914, 214, 78, 437, 1107, 464, 985, 77, 788, 844, 679, 329, 909, 916, 935, 1011, 1066, 965, 1089, 225, 512, 461, 5, 800, 56, 269, 741, 217, 310, 1099, 816, 515, 361, 292, 691, 684, 602, 388, 848, 229, 330, 1128, 877, 158, 645, 333, 3, 128, 429, 203, 648, 616, 454, 281, 945, 1096, 453, 900, 1179, 880, 553, 795, 34, 1109, 1001, 782, 1090, 860, 315, 929, 309, 1124, 524, 695, 1051, 219, 328, 659, 110, 991, 154, 790, 372, 398, 297, 603, 1140, 490, 1209, 151, 19, 295, 739, 1208, 197, 81, 640, 629, 649, 623, 9, 323, 878, 327, 697, 964, 758, 1206, 930, 1168, 416, 826, 769, 1115, 956, 1159, 1178, 849, 835, 986, 499, 324, 952, 839, 927, 8, 549, 342, 810, 870, 1094, 615, 245, 987, 200, 189, 427, 921, 105, 703, 139, 955, 186, 316, 376, 581, 918, 148, 1133, 818, 773, 357, 809, 480, 43, 483, 1035, 976, 725, 341, 386, 120, 1147, 1214, 22, 763, 272, 792, 830, 759, 770, 207, 1072, 611, 1047, 163, 425, 509, 518, 477, 919, 856, 473, 601, 273, 417, 1127, 298, 563, 1027, 261, 201, 678, 412, 44, 1151, 783, 144, 276, 513, 590, 340, 960, 302, 287, 254, 1073, 10, 1084, 942, 875, 301, 567, 1097, 98, 205, 843, 470, 1083, 664, 39, 1081, 529, 855, 689, 232, 854, 675, 304, 1038, 1014, 370, 1181, 1189, 313, 727, 597, 806, 1196, 1158, 1033, 280, 744, 536, 686, 160, 452, 241, 1141, 498, 558, 591, 817, 815, 491, 89, 698, 1100, 1150, 587, 527, 60, 286, 1106, 69, 805, 966, 446, 20, 457, 1148, 492, 193, 319, 15, 618, 586, 274, 555, 1136, 435, 908, 293, 941, 124, 21, 552, 61, 423, 845, 654, 101, 2, 1114, 1041, 904, 422, 595, 564, 1211, 367, 958, 610, 188, 732, 252, 824, 580, 248, 257, 1142, 863, 401, 251, 240, 1015, 1062, 793, 475, 389, 687, 1169, 1042, 259, 605, 944, 224, 436, 546, 27, 335, 243, 300, 92, 979, 671, 392, 627, 585, 777, 832, 573, 1198, 1212, 320, 1067, 833, 1071, 235, 1058, 1024, 1082, 749, 974, 745, 249, 977, 922, 169, 869, 630, 771, 307]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6536493115483800
the save name prefix for this run is:  chkpt-ID_6536493115483800_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 564
rank avg (pred): 0.481 +- 0.008
mrr vals (pred, true): 0.000, 0.173
batch losses (mrrl, rdl): 0.0, 0.0013671411

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 407
rank avg (pred): 0.268 +- 0.190
mrr vals (pred, true): 0.134, 0.001
batch losses (mrrl, rdl): 0.0, 0.0009002234

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 864
rank avg (pred): 0.387 +- 0.318
mrr vals (pred, true): 0.272, 0.001
batch losses (mrrl, rdl): 0.0, 9.4415e-05

Epoch over!
epoch time: 12.157

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 977
rank avg (pred): 0.287 +- 0.255
mrr vals (pred, true): 0.324, 0.305
batch losses (mrrl, rdl): 0.0, 0.001146442

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 596
rank avg (pred): 0.297 +- 0.265
mrr vals (pred, true): 0.284, 0.050
batch losses (mrrl, rdl): 0.0, 3.82299e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 30
rank avg (pred): 0.279 +- 0.266
mrr vals (pred, true): 0.368, 0.142
batch losses (mrrl, rdl): 0.0, 0.0005634817

Epoch over!
epoch time: 11.995

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 772
rank avg (pred): 0.415 +- 0.350
mrr vals (pred, true): 0.177, 0.000
batch losses (mrrl, rdl): 0.0, 0.0004708131

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 461
rank avg (pred): 0.288 +- 0.269
mrr vals (pred, true): 0.348, 0.000
batch losses (mrrl, rdl): 0.0, 0.0005817881

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 840
rank avg (pred): 0.343 +- 0.305
mrr vals (pred, true): 0.344, 0.001
batch losses (mrrl, rdl): 0.0, 0.000272508

Epoch over!
epoch time: 12.0

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 10
rank avg (pred): 0.300 +- 0.286
mrr vals (pred, true): 0.381, 0.302
batch losses (mrrl, rdl): 0.0, 0.0013495819

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 0
rank avg (pred): 0.291 +- 0.285
mrr vals (pred, true): 0.349, 0.279
batch losses (mrrl, rdl): 0.0, 0.0012550837

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.345 +- 0.285
mrr vals (pred, true): 0.243, 0.001
batch losses (mrrl, rdl): 0.0, 0.00028015

Epoch over!
epoch time: 11.999

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 864
rank avg (pred): 0.355 +- 0.320
mrr vals (pred, true): 0.353, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001841376

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1125
rank avg (pred): 0.278 +- 0.287
mrr vals (pred, true): 0.381, 0.001
batch losses (mrrl, rdl): 0.0, 0.0006034906

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 839
rank avg (pred): 0.349 +- 0.314
mrr vals (pred, true): 0.355, 0.002
batch losses (mrrl, rdl): 0.0, 0.0002402052

Epoch over!
epoch time: 12.443

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 913
rank avg (pred): 0.483 +- 0.380
mrr vals (pred, true): 0.313, 0.013
batch losses (mrrl, rdl): 0.693513751, 1.78774e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 570
rank avg (pred): 0.432 +- 0.225
mrr vals (pred, true): 0.116, 0.053
batch losses (mrrl, rdl): 0.0436659865, 0.0001460421

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 160
rank avg (pred): 0.308 +- 0.229
mrr vals (pred, true): 0.109, 0.056
batch losses (mrrl, rdl): 0.0350096226, 3.37986e-05

Epoch over!
epoch time: 12.447

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 620
rank avg (pred): 0.299 +- 0.231
mrr vals (pred, true): 0.114, 0.055
batch losses (mrrl, rdl): 0.0408303589, 5.99859e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 788
rank avg (pred): 0.515 +- 0.181
mrr vals (pred, true): 0.066, 0.000
batch losses (mrrl, rdl): 0.0025816243, 5.86008e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 281
rank avg (pred): 0.395 +- 0.194
mrr vals (pred, true): 0.092, 0.235
batch losses (mrrl, rdl): 0.2056512833, 0.0019512144

Epoch over!
epoch time: 12.215

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.401 +- 0.191
mrr vals (pred, true): 0.097, 0.001
batch losses (mrrl, rdl): 0.0218971893, 0.0001844106

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 568
rank avg (pred): 0.386 +- 0.192
mrr vals (pred, true): 0.086, 0.071
batch losses (mrrl, rdl): 0.0130295856, 9.25998e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 221
rank avg (pred): 0.355 +- 0.186
mrr vals (pred, true): 0.106, 0.001
batch losses (mrrl, rdl): 0.0311141554, 0.0003735463

Epoch over!
epoch time: 12.235

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 683
rank avg (pred): 0.422 +- 0.153
mrr vals (pred, true): 0.070, 0.001
batch losses (mrrl, rdl): 0.0041289944, 0.000152835

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 14
rank avg (pred): 0.286 +- 0.213
mrr vals (pred, true): 0.125, 0.299
batch losses (mrrl, rdl): 0.3024516106, 0.0011265159

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 207
rank avg (pred): 0.296 +- 0.206
mrr vals (pred, true): 0.115, 0.001
batch losses (mrrl, rdl): 0.0420263596, 0.0006786339

Epoch over!
epoch time: 12.457

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 566
rank avg (pred): 0.339 +- 0.181
mrr vals (pred, true): 0.097, 0.182
batch losses (mrrl, rdl): 0.0725397244, 0.0003324832

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 725
rank avg (pred): 0.351 +- 0.164
mrr vals (pred, true): 0.099, 0.001
batch losses (mrrl, rdl): 0.0235972926, 0.0004354383

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 332
rank avg (pred): 0.281 +- 0.196
mrr vals (pred, true): 0.120, 0.051
batch losses (mrrl, rdl): 0.0491496399, 0.0001628275

Epoch over!
epoch time: 12.316

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 871
rank avg (pred): 0.472 +- 0.207
mrr vals (pred, true): 0.069, 0.000
batch losses (mrrl, rdl): 0.0037663195, 5.22263e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1091
rank avg (pred): 0.321 +- 0.169
mrr vals (pred, true): 0.100, 0.056
batch losses (mrrl, rdl): 0.0252756625, 8.10844e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 616
rank avg (pred): 0.319 +- 0.158
mrr vals (pred, true): 0.113, 0.055
batch losses (mrrl, rdl): 0.0396346077, 0.0001237643

Epoch over!
epoch time: 12.392

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 664
rank avg (pred): 0.382 +- 0.138
mrr vals (pred, true): 0.093, 0.001
batch losses (mrrl, rdl): 0.0187008679, 0.0002986287

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 707
rank avg (pred): 0.365 +- 0.136
mrr vals (pred, true): 0.105, 0.001
batch losses (mrrl, rdl): 0.0300262459, 0.000361569

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 825
rank avg (pred): 0.440 +- 0.196
mrr vals (pred, true): 0.069, 0.177
batch losses (mrrl, rdl): 0.1164925769, 0.001703569

Epoch over!
epoch time: 12.082

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 587
rank avg (pred): 0.239 +- 0.191
mrr vals (pred, true): 0.146, 0.066
batch losses (mrrl, rdl): 0.0916946381, 0.0002290795

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.247 +- 0.190
mrr vals (pred, true): 0.134, 0.324
batch losses (mrrl, rdl): 0.3618170321, 0.0007495346

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1126
rank avg (pred): 0.310 +- 0.146
mrr vals (pred, true): 0.117, 0.001
batch losses (mrrl, rdl): 0.0443641469, 0.0007144369

Epoch over!
epoch time: 12.157

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 201
rank avg (pred): 0.356 +- 0.132
mrr vals (pred, true): 0.104, 0.001
batch losses (mrrl, rdl): 0.0296966638, 0.0004284452

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 293
rank avg (pred): 0.302 +- 0.148
mrr vals (pred, true): 0.117, 0.230
batch losses (mrrl, rdl): 0.1277278364, 0.0009683855

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 483
rank avg (pred): 0.291 +- 0.151
mrr vals (pred, true): 0.121, 0.001
batch losses (mrrl, rdl): 0.0498108529, 0.0008473049

Epoch over!
epoch time: 12.118

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1176
rank avg (pred): 0.317 +- 0.142
mrr vals (pred, true): 0.117, 0.059
batch losses (mrrl, rdl): 0.0450123288, 0.0001268029

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 604
rank avg (pred): 0.349 +- 0.124
mrr vals (pred, true): 0.092, 0.053
batch losses (mrrl, rdl): 0.0173276868, 0.0001114066

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 397
rank avg (pred): 0.283 +- 0.148
mrr vals (pred, true): 0.123, 0.060
batch losses (mrrl, rdl): 0.0527714342, 0.0001255093

Epoch over!
epoch time: 11.989

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.307 +- 0.142
mrr vals (pred, true): 0.118, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.06426 	 9e-0500 	 m..s
    2 	     1 	 0.05902 	 0.00013 	 m..s
   19 	     2 	 0.07029 	 0.00024 	 m..s
   61 	     3 	 0.11017 	 0.00048 	 MISS
   95 	     4 	 0.12102 	 0.00049 	 MISS
  106 	     5 	 0.12193 	 0.00051 	 MISS
   53 	     6 	 0.10645 	 0.00051 	 MISS
   79 	     7 	 0.11831 	 0.00053 	 MISS
   31 	     8 	 0.09283 	 0.00054 	 m..s
   57 	     9 	 0.10798 	 0.00055 	 MISS
   66 	    10 	 0.11391 	 0.00055 	 MISS
   69 	    11 	 0.11548 	 0.00055 	 MISS
  111 	    12 	 0.12280 	 0.00056 	 MISS
   76 	    13 	 0.11793 	 0.00056 	 MISS
  102 	    14 	 0.12149 	 0.00056 	 MISS
   67 	    15 	 0.11544 	 0.00056 	 MISS
   85 	    16 	 0.12015 	 0.00057 	 MISS
   40 	    17 	 0.09715 	 0.00058 	 m..s
   22 	    18 	 0.07301 	 0.00058 	 m..s
    7 	    19 	 0.06434 	 0.00058 	 m..s
  120 	    20 	 0.12494 	 0.00058 	 MISS
  113 	    21 	 0.12282 	 0.00060 	 MISS
   93 	    22 	 0.12088 	 0.00066 	 MISS
   11 	    23 	 0.06780 	 0.00067 	 m..s
  117 	    24 	 0.12356 	 0.00068 	 MISS
   42 	    25 	 0.09768 	 0.00069 	 m..s
   48 	    26 	 0.10013 	 0.00069 	 m..s
   86 	    27 	 0.12015 	 0.00069 	 MISS
  119 	    28 	 0.12385 	 0.00070 	 MISS
   78 	    29 	 0.11813 	 0.00070 	 MISS
  108 	    30 	 0.12195 	 0.00071 	 MISS
   33 	    31 	 0.09316 	 0.00074 	 m..s
   37 	    32 	 0.09562 	 0.00076 	 m..s
   83 	    33 	 0.11902 	 0.00076 	 MISS
   16 	    34 	 0.06922 	 0.00077 	 m..s
   23 	    35 	 0.07308 	 0.00078 	 m..s
  110 	    36 	 0.12262 	 0.00079 	 MISS
   82 	    37 	 0.11868 	 0.00084 	 MISS
   60 	    38 	 0.10964 	 0.00085 	 MISS
   43 	    39 	 0.09822 	 0.00087 	 m..s
    1 	    40 	 0.05899 	 0.00089 	 m..s
   10 	    41 	 0.06686 	 0.00098 	 m..s
   64 	    42 	 0.11243 	 0.00107 	 MISS
   21 	    43 	 0.07149 	 0.00112 	 m..s
    3 	    44 	 0.06318 	 0.00112 	 m..s
    4 	    45 	 0.06336 	 0.00134 	 m..s
    0 	    46 	 0.05816 	 0.00440 	 m..s
   15 	    47 	 0.06903 	 0.00510 	 m..s
    5 	    48 	 0.06336 	 0.00541 	 m..s
   33 	    49 	 0.09316 	 0.04001 	 m..s
   38 	    50 	 0.09570 	 0.04487 	 m..s
   26 	    51 	 0.08183 	 0.04587 	 m..s
  106 	    52 	 0.12193 	 0.04831 	 m..s
   31 	    53 	 0.09283 	 0.04869 	 m..s
   97 	    54 	 0.12110 	 0.04882 	 m..s
   27 	    55 	 0.08277 	 0.05014 	 m..s
   39 	    56 	 0.09609 	 0.05072 	 m..s
   89 	    57 	 0.12065 	 0.05417 	 m..s
   80 	    58 	 0.11837 	 0.05905 	 m..s
   90 	    59 	 0.12065 	 0.05965 	 m..s
   35 	    60 	 0.09475 	 0.05969 	 m..s
   50 	    61 	 0.10457 	 0.06050 	 m..s
   65 	    62 	 0.11383 	 0.06246 	 m..s
   73 	    63 	 0.11704 	 0.06258 	 m..s
  104 	    64 	 0.12157 	 0.06272 	 m..s
   41 	    65 	 0.09728 	 0.06373 	 m..s
   81 	    66 	 0.11855 	 0.06378 	 m..s
   92 	    67 	 0.12070 	 0.06585 	 m..s
   93 	    68 	 0.12088 	 0.06609 	 m..s
   68 	    69 	 0.11547 	 0.06650 	 m..s
  101 	    70 	 0.12146 	 0.06748 	 m..s
   55 	    71 	 0.10711 	 0.06783 	 m..s
  118 	    72 	 0.12378 	 0.06842 	 m..s
  105 	    73 	 0.12168 	 0.06912 	 m..s
   46 	    74 	 0.09900 	 0.07046 	 ~...
   56 	    75 	 0.10745 	 0.07128 	 m..s
  111 	    76 	 0.12280 	 0.07383 	 m..s
   87 	    77 	 0.12025 	 0.07431 	 m..s
   98 	    78 	 0.12112 	 0.07485 	 m..s
  115 	    79 	 0.12344 	 0.07720 	 m..s
   74 	    80 	 0.11726 	 0.07943 	 m..s
   18 	    81 	 0.07027 	 0.08904 	 ~...
   13 	    82 	 0.06800 	 0.11561 	 m..s
   14 	    83 	 0.06809 	 0.12182 	 m..s
   24 	    84 	 0.07708 	 0.13113 	 m..s
   20 	    85 	 0.07145 	 0.13828 	 m..s
   28 	    86 	 0.08364 	 0.13864 	 m..s
    9 	    87 	 0.06684 	 0.13870 	 m..s
   25 	    88 	 0.08179 	 0.14674 	 m..s
   30 	    89 	 0.09202 	 0.15026 	 m..s
    8 	    90 	 0.06679 	 0.15264 	 m..s
   11 	    91 	 0.06780 	 0.15880 	 m..s
   29 	    92 	 0.09099 	 0.16345 	 m..s
   51 	    93 	 0.10476 	 0.17639 	 m..s
   17 	    94 	 0.06951 	 0.17806 	 MISS
   45 	    95 	 0.09869 	 0.18290 	 m..s
   52 	    96 	 0.10516 	 0.18671 	 m..s
   49 	    97 	 0.10040 	 0.18680 	 m..s
   47 	    98 	 0.10002 	 0.19080 	 m..s
   62 	    99 	 0.11046 	 0.20438 	 m..s
   44 	   100 	 0.09835 	 0.21400 	 MISS
   72 	   101 	 0.11669 	 0.21965 	 MISS
   35 	   102 	 0.09475 	 0.22229 	 MISS
   63 	   103 	 0.11177 	 0.22936 	 MISS
   75 	   104 	 0.11751 	 0.23311 	 MISS
   54 	   105 	 0.10673 	 0.24977 	 MISS
   91 	   106 	 0.12067 	 0.25365 	 MISS
   58 	   107 	 0.10821 	 0.25400 	 MISS
   83 	   108 	 0.11902 	 0.25682 	 MISS
   69 	   109 	 0.11548 	 0.27022 	 MISS
  102 	   110 	 0.12149 	 0.27544 	 MISS
   59 	   111 	 0.10845 	 0.27569 	 MISS
  109 	   112 	 0.12224 	 0.27640 	 MISS
   71 	   113 	 0.11569 	 0.27939 	 MISS
   96 	   114 	 0.12104 	 0.28719 	 MISS
  100 	   115 	 0.12134 	 0.29794 	 MISS
   88 	   116 	 0.12062 	 0.30040 	 MISS
   77 	   117 	 0.11808 	 0.30458 	 MISS
   99 	   118 	 0.12122 	 0.30614 	 MISS
  115 	   119 	 0.12344 	 0.30764 	 MISS
  114 	   120 	 0.12341 	 0.32201 	 MISS
==========================================
r_mrr = 0.19639180600643158
r2_mrr = 0.016983211040496826
spearmanr_mrr@5 = 0.9933937191963196
spearmanr_mrr@10 = 0.9412828683853149
spearmanr_mrr@50 = 0.9666475653648376
spearmanr_mrr@100 = 0.7895659804344177
spearmanr_mrr@All = 0.7524538636207581
==========================================
test time: 0.423
Done Testing dataset OpenEA
total time taken: 198.2404386997223
training time taken: 183.48494267463684
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.1964)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.0170)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9934)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9413)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9666)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.7896)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.7525)}}, 'test_loss': {'DistMult': {'OpenEA': 8.835964175872505}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 8649224008780068
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [944, 1089, 595, 390, 42, 649, 1207, 196, 515, 280, 768, 566, 484, 377, 696, 587, 1112, 1182, 1162, 1206, 347, 834, 364, 109, 983, 1023, 873, 557, 205, 240, 769, 1148, 640, 313, 780, 69, 1046, 284, 739, 679, 968, 841, 363, 234, 705, 775, 389, 576, 1075, 92, 233, 692, 228, 934, 898, 1141, 79, 182, 247, 1170, 597, 1122, 187, 1103, 999, 637, 848, 1050, 671, 41, 810, 1131, 645, 1150, 1078, 74, 37, 307, 3, 132, 817, 517, 842, 475, 1077, 913, 266, 838, 615, 106, 844, 967, 55, 312, 948, 904, 612, 231, 851, 641, 631, 764, 700, 716, 1049, 93, 894, 489, 925, 202, 1011, 658, 404, 620, 90, 607, 1191, 1124, 388, 476, 682]
valid_ids (0): []
train_ids (1094): [1203, 467, 334, 384, 580, 1095, 431, 353, 103, 283, 878, 57, 808, 602, 984, 865, 112, 1054, 168, 559, 793, 354, 500, 1094, 448, 456, 748, 308, 26, 507, 603, 840, 1096, 606, 117, 110, 237, 752, 830, 754, 585, 1193, 221, 563, 327, 734, 323, 293, 1066, 571, 1187, 742, 573, 674, 446, 230, 211, 1137, 1022, 1045, 782, 1163, 397, 115, 497, 877, 1083, 723, 922, 625, 963, 341, 1130, 525, 63, 686, 1149, 708, 709, 857, 905, 181, 490, 596, 920, 67, 383, 1060, 744, 798, 733, 1038, 111, 164, 1195, 14, 568, 550, 369, 335, 47, 521, 959, 660, 1012, 1169, 1108, 1192, 36, 470, 78, 1210, 81, 766, 207, 940, 486, 1194, 462, 243, 303, 1025, 324, 522, 320, 281, 439, 755, 222, 249, 860, 460, 425, 670, 888, 778, 264, 31, 511, 1073, 896, 1009, 415, 1070, 893, 1074, 491, 368, 1000, 217, 609, 836, 342, 652, 590, 238, 608, 478, 1185, 1111, 1105, 1015, 185, 884, 1091, 929, 84, 482, 450, 188, 720, 169, 150, 676, 982, 560, 1040, 627, 474, 819, 909, 882, 891, 794, 972, 965, 455, 405, 229, 866, 886, 427, 96, 361, 738, 225, 371, 424, 911, 776, 139, 166, 728, 367, 1199, 711, 788, 813, 214, 235, 91, 971, 492, 178, 773, 97, 791, 897, 287, 680, 1209, 402, 195, 310, 916, 72, 862, 823, 206, 879, 105, 444, 758, 1005, 158, 663, 726, 138, 691, 378, 694, 732, 128, 296, 21, 39, 787, 1160, 902, 989, 693, 80, 129, 531, 756, 582, 814, 820, 907, 262, 95, 970, 518, 969, 535, 272, 688, 433, 359, 1063, 703, 487, 642, 116, 58, 459, 120, 43, 113, 339, 1202, 761, 466, 1175, 244, 942, 1064, 19, 956, 662, 173, 1153, 108, 547, 251, 1028, 1177, 1126, 365, 509, 689, 978, 70, 148, 976, 118, 863, 122, 928, 1084, 99, 328, 953, 825, 549, 245, 598, 858, 849, 604, 1183, 1110, 165, 410, 100, 301, 561, 23, 512, 88, 1115, 765, 643, 73, 85, 736, 1180, 1161, 1188, 1184, 721, 695, 375, 917, 151, 623, 393, 1020, 534, 687, 685, 48, 322, 161, 937, 1013, 938, 520, 458, 387, 83, 56, 827, 713, 815, 29, 380, 204, 346, 718, 599, 437, 505, 629, 803, 302, 852, 1156, 741, 993, 605, 762, 174, 297, 579, 870, 943, 781, 628, 1007, 149, 701, 480, 140, 386, 853, 1057, 1154, 826, 831, 892, 715, 1164, 316, 868, 610, 331, 519, 213, 17, 880, 408, 1113, 1138, 481, 918, 20, 786, 259, 574, 276, 406, 15, 528, 1098, 179, 900, 1143, 832, 632, 44, 381, 1068, 966, 921, 1076, 50, 1213, 554, 919, 89, 270, 294, 667, 600, 472, 1071, 382, 309, 385, 1142, 1065, 1168, 923, 634, 1003, 184, 876, 1147, 747, 1128, 792, 170, 783, 398, 273, 952, 453, 946, 6, 1, 871, 532, 7, 552, 22, 134, 413, 958, 4, 1114, 1006, 1106, 1031, 49, 40, 278, 1211, 936, 931, 10, 712, 160, 1198, 617, 477, 318, 68, 469, 275, 30, 583, 290, 445, 355, 556, 332, 947, 435, 86, 1109, 45, 414, 60, 565, 1146, 291, 451, 465, 949, 881, 153, 906, 197, 760, 508, 1008, 1041, 344, 699, 1086, 855, 1033, 219, 770, 910, 636, 990, 471, 570, 669, 422, 429, 960, 601, 421, 653, 510, 730, 821, 927, 802, 659, 325, 975, 1201, 1061, 423, 1181, 933, 553, 194, 1107, 1117, 254, 357, 434, 32, 856, 449, 621, 416, 321, 227, 763, 183, 1173, 704, 403, 533, 717, 572, 145, 767, 1082, 1048, 499, 805, 498, 279, 784, 1099, 666, 1059, 0, 1152, 156, 209, 1055, 816, 930, 1030, 447, 282, 76, 980, 883, 162, 504, 119, 804, 33, 678, 167, 1212, 1190, 986, 376, 874, 11, 530, 1035, 647, 981, 955, 1039, 286, 1140, 267, 53, 991, 198, 845, 27, 724, 176, 1090, 1085, 330, 13, 630, 65, 1016, 352, 241, 391, 277, 903, 172, 319, 208, 51, 1021, 562, 463, 690, 295, 239, 646, 257, 861, 996, 1166, 464, 396, 624, 524, 224, 189, 885, 616, 345, 326, 289, 785, 260, 746, 300, 232, 1136, 985, 1157, 141, 1024, 593, 314, 664, 443, 180, 25, 697, 974, 255, 87, 142, 672, 650, 226, 835, 157, 250, 454, 1018, 348, 154, 581, 992, 797, 638, 75, 253, 268, 311, 867, 722, 392, 1139, 542, 926, 588, 263, 789, 483, 1134, 1127, 1056, 473, 774, 411, 35, 751, 107, 432, 753, 159, 513, 932, 875, 673, 529, 358, 9, 1125, 537, 1176, 743, 125, 1159, 1120, 997, 199, 1155, 1004, 133, 790, 190, 292, 1051, 24, 1135, 98, 5, 1123, 592, 683, 812, 379, 1072, 635, 135, 1104, 274, 757, 502, 644, 901, 555, 1034, 102, 681, 366, 759, 372, 962, 1208, 495, 941, 523, 727, 548, 258, 1119, 305, 1179, 2, 1037, 977, 661, 1133, 828, 771, 1174, 1052, 436, 419, 203, 514, 216, 218, 104, 137, 220, 236, 1043, 210, 1097, 38, 614, 987, 468, 362, 964, 847, 1116, 725, 114, 833, 503, 1027, 577, 698, 77, 401, 155, 737, 795, 1014, 527, 146, 915, 315, 144, 409, 1019, 546, 538, 558, 1214, 126, 806, 412, 945, 998, 749, 1204, 261, 796, 285, 957, 123, 200, 1165, 370, 655, 1069, 850, 54, 269, 924, 64, 488, 130, 740, 1145, 356, 1151, 418, 1017, 1081, 163, 540, 1087, 818, 1053, 622, 799, 706, 246, 594, 317, 651, 395, 1062, 611, 506, 995, 61, 545, 417, 714, 306, 543, 252, 399, 1158, 843, 526, 961, 908, 567, 299, 899, 101, 340, 811, 801, 750, 1029, 1196, 461, 338, 201, 1172, 62, 824, 152, 648, 349, 360, 452, 46, 973, 939, 177, 1100, 1079, 800, 854, 485, 677, 440, 1205, 829, 591, 864, 1186, 131, 1132, 1044, 654, 639, 979, 336, 1121, 1088, 428, 374, 869, 442, 1092, 337, 1001, 890, 1042, 59, 872, 496, 895, 304, 288, 516, 951, 719, 501, 1002, 626, 578, 575, 66, 1101, 430, 1189, 136, 988, 1058, 541, 400, 950, 256, 564, 121, 16, 589, 407, 1178, 657, 772, 1171, 889, 551, 777, 186, 1129, 668, 1026, 192, 729, 809, 242, 351, 1010, 675, 215, 420, 438, 143, 1144, 1093, 684, 1167, 613, 1197, 394, 994, 171, 618, 779, 707, 193, 954, 493, 457, 223, 586, 248, 147, 702, 731, 536, 18, 8, 656, 1032, 191, 569, 265, 887, 1067, 494, 373, 912, 822, 329, 271, 1200, 544, 12, 479, 665, 837, 1080, 1036, 584, 633, 82, 333, 94, 212, 846, 619, 350, 71, 34, 745, 441, 914, 807, 735, 127, 124, 1047, 52, 28, 426, 175, 298, 1102, 710, 839, 935, 343, 1118, 859, 539]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3889179129344006
the save name prefix for this run is:  chkpt-ID_3889179129344006_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 783
rank avg (pred): 0.547 +- 0.004
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001581026

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 903
rank avg (pred): 0.397 +- 0.216
mrr vals (pred, true): 0.060, 0.011
batch losses (mrrl, rdl): 0.0, 0.00029783

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 33
rank avg (pred): 0.101 +- 0.075
mrr vals (pred, true): 0.177, 0.141
batch losses (mrrl, rdl): 0.0, 5.59832e-05

Epoch over!
epoch time: 12.148

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1156
rank avg (pred): 0.211 +- 0.157
mrr vals (pred, true): 0.169, 0.219
batch losses (mrrl, rdl): 0.0, 6.03542e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 918
rank avg (pred): 0.525 +- 0.351
mrr vals (pred, true): 0.142, 0.001
batch losses (mrrl, rdl): 0.0, 8.07588e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1081
rank avg (pred): 0.367 +- 0.277
mrr vals (pred, true): 0.168, 0.048
batch losses (mrrl, rdl): 0.0, 2.90414e-05

Epoch over!
epoch time: 11.717

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 449
rank avg (pred): 0.397 +- 0.300
mrr vals (pred, true): 0.165, 0.001
batch losses (mrrl, rdl): 0.0, 9.24893e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 632
rank avg (pred): 0.385 +- 0.296
mrr vals (pred, true): 0.156, 0.062
batch losses (mrrl, rdl): 0.0, 6.2348e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 453
rank avg (pred): 0.378 +- 0.297
mrr vals (pred, true): 0.161, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001305814

Epoch over!
epoch time: 11.998

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 487
rank avg (pred): 0.184 +- 0.149
mrr vals (pred, true): 0.175, 0.249
batch losses (mrrl, rdl): 0.0, 6.44473e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 800
rank avg (pred): 0.514 +- 0.333
mrr vals (pred, true): 0.118, 0.001
batch losses (mrrl, rdl): 0.0, 5.00254e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 96
rank avg (pred): 0.386 +- 0.305
mrr vals (pred, true): 0.160, 0.076
batch losses (mrrl, rdl): 0.0, 0.0001008714

Epoch over!
epoch time: 11.985

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 653
rank avg (pred): 0.410 +- 0.305
mrr vals (pred, true): 0.129, 0.001
batch losses (mrrl, rdl): 0.0, 6.99977e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 224
rank avg (pred): 0.377 +- 0.298
mrr vals (pred, true): 0.140, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001427472

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.399 +- 0.314
mrr vals (pred, true): 0.147, 0.056
batch losses (mrrl, rdl): 0.0, 8.69778e-05

Epoch over!
epoch time: 12.085

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.083 +- 0.069
mrr vals (pred, true): 0.189, 0.324
batch losses (mrrl, rdl): 0.1821283251, 1.72341e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 311
rank avg (pred): 0.085 +- 0.110
mrr vals (pred, true): 0.241, 0.238
batch losses (mrrl, rdl): 0.000106044, 1.55099e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 966
rank avg (pred): 0.612 +- 0.240
mrr vals (pred, true): 0.025, 0.001
batch losses (mrrl, rdl): 0.0060716062, 0.0002712697

Epoch over!
epoch time: 12.271

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 573
rank avg (pred): 0.422 +- 0.220
mrr vals (pred, true): 0.056, 0.052
batch losses (mrrl, rdl): 0.0003368943, 0.0001264709

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 874
rank avg (pred): 0.512 +- 0.217
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001544744, 4.34992e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 837
rank avg (pred): 0.507 +- 0.202
mrr vals (pred, true): 0.039, 0.004
batch losses (mrrl, rdl): 0.0012126307, 5.52036e-05

Epoch over!
epoch time: 12.037

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 219
rank avg (pred): 0.419 +- 0.218
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0010838904, 9.5095e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1099
rank avg (pred): 0.430 +- 0.211
mrr vals (pred, true): 0.053, 0.057
batch losses (mrrl, rdl): 9.34092e-05, 0.0002124352

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 807
rank avg (pred): 0.558 +- 0.216
mrr vals (pred, true): 0.031, 0.001
batch losses (mrrl, rdl): 0.003575007, 0.0001033351

Epoch over!
epoch time: 12.222

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.431 +- 0.209
mrr vals (pred, true): 0.058, 0.056
batch losses (mrrl, rdl): 0.0006043831, 0.0001951928

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 355
rank avg (pred): 0.431 +- 0.212
mrr vals (pred, true): 0.060, 0.063
batch losses (mrrl, rdl): 0.0009374768, 0.0002194909

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 880
rank avg (pred): 0.525 +- 0.202
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0012108386, 5.61431e-05

Epoch over!
epoch time: 12.082

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 50
rank avg (pred): 0.050 +- 0.098
mrr vals (pred, true): 0.274, 0.207
batch losses (mrrl, rdl): 0.045109652, 0.0001250025

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 104
rank avg (pred): 0.427 +- 0.208
mrr vals (pred, true): 0.055, 0.082
batch losses (mrrl, rdl): 0.0002073521, 0.0002762936

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 263
rank avg (pred): 0.104 +- 0.164
mrr vals (pred, true): 0.249, 0.302
batch losses (mrrl, rdl): 0.0283127129, 8.10285e-05

Epoch over!
epoch time: 12.103

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.101 +- 0.162
mrr vals (pred, true): 0.247, 0.230
batch losses (mrrl, rdl): 0.0027779015, 0.0002092722

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 352
rank avg (pred): 0.414 +- 0.205
mrr vals (pred, true): 0.064, 0.048
batch losses (mrrl, rdl): 0.0018704089, 0.0001694158

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 304
rank avg (pred): 0.161 +- 0.195
mrr vals (pred, true): 0.214, 0.151
batch losses (mrrl, rdl): 0.0399158821, 7.76069e-05

Epoch over!
epoch time: 12.119

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 299
rank avg (pred): 0.074 +- 0.123
mrr vals (pred, true): 0.230, 0.160
batch losses (mrrl, rdl): 0.0484825596, 3.39056e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 845
rank avg (pred): 0.507 +- 0.204
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001210863, 3.64165e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 128
rank avg (pred): 0.430 +- 0.194
mrr vals (pred, true): 0.055, 0.058
batch losses (mrrl, rdl): 0.0002210819, 0.0002585123

Epoch over!
epoch time: 12.133

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1213
rank avg (pred): 0.512 +- 0.214
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001550453, 3.97836e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 871
rank avg (pred): 0.545 +- 0.207
mrr vals (pred, true): 0.035, 0.000
batch losses (mrrl, rdl): 0.0022428266, 6.94282e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 805
rank avg (pred): 0.463 +- 0.184
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001648333, 5.17381e-05

Epoch over!
epoch time: 12.37

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 100
rank avg (pred): 0.419 +- 0.189
mrr vals (pred, true): 0.059, 0.083
batch losses (mrrl, rdl): 0.0007729392, 0.0002124672

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 111
rank avg (pred): 0.424 +- 0.189
mrr vals (pred, true): 0.053, 0.038
batch losses (mrrl, rdl): 7.6011e-05, 0.000164222

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 491
rank avg (pred): 0.306 +- 0.224
mrr vals (pred, true): 0.180, 0.247
batch losses (mrrl, rdl): 0.0457707755, 0.0003332242

Epoch over!
epoch time: 12.185

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 899
rank avg (pred): 0.422 +- 0.230
mrr vals (pred, true): 0.077, 0.000
batch losses (mrrl, rdl): 0.0073344605, 0.0005136485

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1071
rank avg (pred): 0.165 +- 0.196
mrr vals (pred, true): 0.225, 0.214
batch losses (mrrl, rdl): 0.0012478156, 0.0001108218

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 519
rank avg (pred): 0.251 +- 0.208
mrr vals (pred, true): 0.171, 0.136
batch losses (mrrl, rdl): 0.0122161843, 5.8454e-05

Epoch over!
epoch time: 12.099

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.707 +- 0.243
mrr vals (pred, true): 0.031, 0.000

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03096 	 8e-0500 	 m..s
    1 	     1 	 0.03096 	 9e-0500 	 m..s
   83 	     2 	 0.05349 	 0.00010 	 m..s
    0 	     3 	 0.03074 	 0.00013 	 m..s
   84 	     4 	 0.05359 	 0.00022 	 m..s
    4 	     5 	 0.03606 	 0.00026 	 m..s
   72 	     6 	 0.04618 	 0.00048 	 m..s
   85 	     7 	 0.05431 	 0.00048 	 m..s
   36 	     8 	 0.04593 	 0.00048 	 m..s
   30 	     9 	 0.04512 	 0.00050 	 m..s
    7 	    10 	 0.03849 	 0.00051 	 m..s
   12 	    11 	 0.03970 	 0.00051 	 m..s
   31 	    12 	 0.04524 	 0.00055 	 m..s
   36 	    13 	 0.04593 	 0.00055 	 m..s
   36 	    14 	 0.04593 	 0.00055 	 m..s
   36 	    15 	 0.04593 	 0.00056 	 m..s
    8 	    16 	 0.03855 	 0.00057 	 m..s
   73 	    17 	 0.04623 	 0.00058 	 m..s
   36 	    18 	 0.04593 	 0.00058 	 m..s
   10 	    19 	 0.03953 	 0.00059 	 m..s
   16 	    20 	 0.04104 	 0.00059 	 m..s
   28 	    21 	 0.04413 	 0.00061 	 m..s
   36 	    22 	 0.04593 	 0.00063 	 m..s
   88 	    23 	 0.06052 	 0.00064 	 m..s
   36 	    24 	 0.04593 	 0.00064 	 m..s
   36 	    25 	 0.04593 	 0.00065 	 m..s
    5 	    26 	 0.03737 	 0.00065 	 m..s
   36 	    27 	 0.04593 	 0.00066 	 m..s
    3 	    28 	 0.03582 	 0.00067 	 m..s
   79 	    29 	 0.04865 	 0.00068 	 m..s
   36 	    30 	 0.04593 	 0.00068 	 m..s
   36 	    31 	 0.04593 	 0.00069 	 m..s
   36 	    32 	 0.04593 	 0.00069 	 m..s
   20 	    33 	 0.04291 	 0.00073 	 m..s
   88 	    34 	 0.06052 	 0.00073 	 m..s
   36 	    35 	 0.04593 	 0.00075 	 m..s
   26 	    36 	 0.04397 	 0.00081 	 m..s
   36 	    37 	 0.04593 	 0.00084 	 m..s
   19 	    38 	 0.04286 	 0.00086 	 m..s
    6 	    39 	 0.03832 	 0.00089 	 m..s
   21 	    40 	 0.04350 	 0.00090 	 m..s
   24 	    41 	 0.04396 	 0.00091 	 m..s
   76 	    42 	 0.04672 	 0.00092 	 m..s
   87 	    43 	 0.05987 	 0.00094 	 m..s
   34 	    44 	 0.04549 	 0.00098 	 m..s
   35 	    45 	 0.04590 	 0.00099 	 m..s
   36 	    46 	 0.04593 	 0.00108 	 m..s
   36 	    47 	 0.04593 	 0.00116 	 m..s
   23 	    48 	 0.04372 	 0.00171 	 m..s
   24 	    49 	 0.04396 	 0.00294 	 m..s
   92 	    50 	 0.11399 	 0.00439 	 MISS
   33 	    51 	 0.04546 	 0.00510 	 m..s
   32 	    52 	 0.04529 	 0.00589 	 m..s
   29 	    53 	 0.04418 	 0.00611 	 m..s
   81 	    54 	 0.05346 	 0.00925 	 m..s
   81 	    55 	 0.05346 	 0.01348 	 m..s
   91 	    56 	 0.09856 	 0.03512 	 m..s
   36 	    57 	 0.04593 	 0.04147 	 ~...
   36 	    58 	 0.04593 	 0.04825 	 ~...
   78 	    59 	 0.04732 	 0.04831 	 ~...
   75 	    60 	 0.04632 	 0.04843 	 ~...
   36 	    61 	 0.04593 	 0.05064 	 ~...
   36 	    62 	 0.04593 	 0.05187 	 ~...
   36 	    63 	 0.04593 	 0.05403 	 ~...
   36 	    64 	 0.04593 	 0.05445 	 ~...
   70 	    65 	 0.04609 	 0.05537 	 ~...
   15 	    66 	 0.04082 	 0.05544 	 ~...
   36 	    67 	 0.04593 	 0.05564 	 ~...
   74 	    68 	 0.04627 	 0.05652 	 ~...
   13 	    69 	 0.03975 	 0.05693 	 ~...
   22 	    70 	 0.04353 	 0.05701 	 ~...
   11 	    71 	 0.03964 	 0.05708 	 ~...
   36 	    72 	 0.04593 	 0.05760 	 ~...
   71 	    73 	 0.04615 	 0.05806 	 ~...
   36 	    74 	 0.04593 	 0.05905 	 ~...
   36 	    75 	 0.04593 	 0.05927 	 ~...
   36 	    76 	 0.04593 	 0.05963 	 ~...
   36 	    77 	 0.04593 	 0.05987 	 ~...
   36 	    78 	 0.04593 	 0.06111 	 ~...
   36 	    79 	 0.04593 	 0.06187 	 ~...
   77 	    80 	 0.04716 	 0.06258 	 ~...
   36 	    81 	 0.04593 	 0.06297 	 ~...
   17 	    82 	 0.04126 	 0.06605 	 ~...
    9 	    83 	 0.03931 	 0.06657 	 ~...
   80 	    84 	 0.04930 	 0.06828 	 ~...
   14 	    85 	 0.04026 	 0.06992 	 ~...
   17 	    86 	 0.04126 	 0.07304 	 m..s
   36 	    87 	 0.04593 	 0.07453 	 ~...
   36 	    88 	 0.04593 	 0.07485 	 ~...
   86 	    89 	 0.05525 	 0.07507 	 ~...
   36 	    90 	 0.04593 	 0.07821 	 m..s
   27 	    91 	 0.04399 	 0.07978 	 m..s
   90 	    92 	 0.08846 	 0.12784 	 m..s
   96 	    93 	 0.15197 	 0.13955 	 ~...
   95 	    94 	 0.14608 	 0.14021 	 ~...
   99 	    95 	 0.18388 	 0.15026 	 m..s
   93 	    96 	 0.11453 	 0.16306 	 m..s
   97 	    97 	 0.16051 	 0.17490 	 ~...
  100 	    98 	 0.18645 	 0.18212 	 ~...
  106 	    99 	 0.19784 	 0.19080 	 ~...
  116 	   100 	 0.23676 	 0.19249 	 m..s
  101 	   101 	 0.18941 	 0.20232 	 ~...
  101 	   102 	 0.18941 	 0.20300 	 ~...
  111 	   103 	 0.21920 	 0.21193 	 ~...
  107 	   104 	 0.20017 	 0.21352 	 ~...
  108 	   105 	 0.20094 	 0.21861 	 ~...
  115 	   106 	 0.23161 	 0.22435 	 ~...
  104 	   107 	 0.19272 	 0.22454 	 m..s
  112 	   108 	 0.21998 	 0.22966 	 ~...
  105 	   109 	 0.19732 	 0.22972 	 m..s
  114 	   110 	 0.22784 	 0.23408 	 ~...
  108 	   111 	 0.20094 	 0.23436 	 m..s
  118 	   112 	 0.24801 	 0.23545 	 ~...
  110 	   113 	 0.20523 	 0.23907 	 m..s
   94 	   114 	 0.14310 	 0.25400 	 MISS
  120 	   115 	 0.28105 	 0.26056 	 ~...
  119 	   116 	 0.28065 	 0.26852 	 ~...
  112 	   117 	 0.21998 	 0.26915 	 m..s
  103 	   118 	 0.19238 	 0.28408 	 m..s
   98 	   119 	 0.18109 	 0.28471 	 MISS
  117 	   120 	 0.23693 	 0.30429 	 m..s
==========================================
r_mrr = 0.9200525879859924
r2_mrr = 0.8048808574676514
spearmanr_mrr@5 = 0.8441667556762695
spearmanr_mrr@10 = 0.9112868309020996
spearmanr_mrr@50 = 0.9935539960861206
spearmanr_mrr@100 = 0.9682860374450684
spearmanr_mrr@All = 0.9692327976226807
==========================================
test time: 0.391
Done Testing dataset OpenEA
total time taken: 196.3136579990387
training time taken: 182.002690076828
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9201)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8049)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.8442)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9113)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9936)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9683)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9692)}}, 'test_loss': {'DistMult': {'OpenEA': 0.6616643932065926}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 4406815375814997
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [761, 505, 562, 176, 275, 946, 1057, 65, 677, 435, 1003, 316, 629, 6, 493, 469, 1126, 973, 972, 662, 170, 1052, 547, 245, 63, 947, 260, 305, 796, 412, 873, 471, 130, 307, 1025, 88, 293, 1213, 67, 780, 958, 341, 579, 1087, 1013, 957, 552, 922, 621, 474, 857, 681, 371, 1118, 168, 504, 1082, 393, 320, 81, 620, 941, 935, 416, 915, 660, 657, 489, 464, 5, 1069, 968, 659, 672, 954, 224, 930, 1122, 34, 1092, 799, 899, 771, 337, 793, 348, 907, 249, 548, 124, 826, 1059, 154, 346, 928, 936, 1074, 196, 248, 906, 634, 481, 719, 51, 667, 312, 1047, 1184, 352, 292, 1155, 580, 1066, 46, 19, 955, 155, 298, 965, 537, 628]
valid_ids (0): []
train_ids (1094): [363, 429, 439, 769, 649, 66, 1120, 1166, 756, 752, 492, 15, 299, 171, 160, 706, 856, 134, 49, 209, 576, 1015, 453, 267, 318, 701, 656, 90, 210, 240, 705, 655, 982, 236, 456, 1199, 151, 21, 286, 809, 120, 324, 904, 676, 811, 712, 84, 38, 447, 863, 483, 0, 678, 404, 24, 496, 508, 1127, 482, 690, 526, 165, 1007, 396, 525, 45, 633, 715, 909, 1108, 1186, 114, 1170, 1147, 564, 903, 913, 1188, 700, 779, 397, 218, 709, 432, 823, 524, 1048, 441, 78, 478, 458, 843, 617, 116, 687, 1102, 1002, 1114, 201, 142, 688, 626, 446, 204, 50, 350, 250, 514, 355, 83, 306, 668, 268, 69, 1131, 152, 1138, 383, 836, 574, 1113, 1130, 1169, 644, 121, 82, 1104, 539, 786, 242, 625, 992, 635, 586, 553, 43, 994, 42, 716, 175, 686, 596, 1094, 459, 259, 444, 1124, 327, 911, 332, 815, 178, 421, 588, 842, 300, 787, 1111, 281, 797, 729, 875, 1078, 1075, 1037, 1010, 926, 816, 520, 311, 472, 1183, 859, 892, 463, 1070, 180, 379, 543, 764, 261, 732, 531, 1165, 94, 989, 422, 1099, 658, 365, 1021, 1091, 282, 96, 169, 1089, 238, 1054, 97, 174, 288, 987, 122, 889, 64, 228, 833, 960, 146, 618, 361, 362, 1152, 244, 399, 415, 713, 411, 727, 212, 1088, 473, 25, 452, 289, 74, 339, 794, 931, 841, 380, 791, 647, 751, 883, 358, 1171, 943, 87, 343, 623, 517, 369, 652, 241, 106, 101, 52, 1020, 527, 637, 516, 325, 825, 1022, 1158, 1036, 849, 3, 1204, 717, 1190, 986, 375, 1051, 840, 592, 254, 1045, 347, 129, 434, 227, 1, 1129, 486, 126, 322, 491, 17, 118, 860, 1016, 753, 185, 133, 247, 231, 874, 297, 726, 56, 378, 1205, 609, 253, 1042, 1153, 988, 1049, 419, 974, 949, 942, 30, 1145, 1001, 387, 739, 742, 758, 226, 923, 1109, 342, 448, 1163, 905, 285, 886, 290, 23, 590, 575, 683, 1062, 150, 394, 119, 179, 95, 561, 790, 140, 895, 1202, 141, 1136, 1083, 1008, 927, 788, 502, 803, 912, 865, 978, 1177, 748, 266, 445, 449, 582, 653, 530, 100, 230, 145, 1157, 1160, 512, 166, 398, 1115, 1116, 924, 798, 484, 1211, 319, 541, 725, 1096, 424, 898, 685, 692, 650, 206, 506, 772, 736, 466, 53, 565, 921, 390, 420, 59, 1105, 345, 832, 1189, 137, 1024, 814, 1072, 808, 645, 433, 349, 619, 465, 213, 567, 1060, 135, 85, 1123, 340, 409, 896, 359, 515, 536, 740, 861, 571, 568, 223, 44, 545, 741, 805, 270, 951, 746, 77, 707, 1173, 953, 829, 890, 884, 99, 559, 669, 1076, 12, 607, 894, 880, 916, 1065, 919, 1162, 551, 1207, 1140, 499, 812, 781, 197, 881, 403, 981, 1117, 377, 188, 354, 721, 125, 891, 211, 164, 893, 643, 1030, 161, 795, 1141, 792, 730, 876, 144, 1084, 388, 428, 785, 186, 670, 356, 554, 997, 112, 32, 1035, 985, 13, 71, 877, 544, 93, 522, 1133, 73, 127, 256, 374, 60, 589, 1101, 1208, 1164, 443, 996, 485, 975, 835, 674, 1156, 747, 11, 392, 866, 616, 538, 479, 631, 405, 494, 334, 869, 959, 804, 367, 1097, 593, 385, 990, 1100, 1142, 262, 837, 172, 711, 39, 333, 167, 57, 183, 901, 470, 878, 55, 979, 132, 980, 680, 689, 819, 1029, 279, 945, 1073, 540, 838, 423, 766, 182, 1139, 1014, 693, 759, 14, 651, 703, 817, 1005, 578, 386, 533, 1023, 9, 1172, 323, 274, 1154, 920, 495, 622, 854, 54, 222, 908, 939, 722, 148, 313, 535, 302, 630, 86, 743, 770, 971, 778, 871, 1004, 303, 1034, 219, 549, 31, 646, 202, 918, 1119, 400, 550, 455, 283, 738, 783, 364, 113, 1018, 1041, 220, 762, 846, 983, 1009, 1144, 440, 1110, 511, 1196, 128, 608, 1081, 239, 189, 357, 407, 143, 902, 984, 602, 47, 1159, 442, 1182, 1178, 696, 29, 910, 308, 20, 581, 4, 208, 207, 317, 115, 666, 867, 532, 572, 105, 1038, 1148, 139, 1193, 431, 569, 914, 7, 451, 627, 510, 757, 710, 1135, 271, 776, 675, 89, 566, 462, 699, 950, 818, 153, 373, 967, 138, 314, 595, 782, 521, 879, 111, 933, 425, 131, 603, 1214, 731, 1187, 682, 252, 760, 70, 612, 853, 698, 10, 92, 872, 828, 16, 488, 413, 868, 615, 583, 1080, 587, 1112, 557, 1149, 110, 370, 376, 654, 468, 372, 177, 735, 401, 845, 745, 149, 1128, 1071, 767, 601, 273, 221, 68, 336, 714, 107, 858, 763, 136, 265, 1200, 956, 1086, 27, 263, 534, 624, 414, 497, 855, 750, 187, 190, 1064, 234, 822, 704, 410, 1046, 1017, 232, 287, 599, 217, 503, 437, 246, 1012, 1137, 278, 614, 36, 807, 1077, 1168, 591, 1006, 847, 528, 870, 938, 391, 802, 203, 184, 368, 821, 1150, 594, 940, 1058, 665, 1151, 673, 691, 103, 108, 810, 1044, 467, 1212, 1180, 830, 834, 734, 944, 1174, 1028, 613, 560, 237, 330, 1167, 1103, 888, 755, 679, 610, 1146, 328, 934, 200, 461, 1203, 329, 296, 806, 156, 848, 585, 76, 784, 684, 215, 487, 917, 104, 948, 584, 258, 952, 475, 315, 885, 1192, 280, 18, 335, 887, 1206, 708, 642, 1053, 22, 661, 862, 418, 62, 162, 507, 850, 26, 1011, 326, 961, 999, 765, 40, 309, 563, 963, 600, 1085, 611, 301, 454, 205, 966, 577, 1161, 864, 546, 1055, 509, 598, 897, 460, 604, 395, 519, 80, 518, 264, 1068, 295, 33, 824, 523, 255, 638, 882, 1132, 243, 158, 664, 1195, 851, 194, 831, 269, 852, 1067, 79, 969, 75, 925, 501, 338, 1033, 389, 1056, 1098, 932, 417, 558, 820, 500, 1179, 789, 937, 498, 480, 1181, 632, 697, 195, 58, 353, 102, 35, 801, 157, 1185, 720, 477, 723, 304, 663, 733, 1191, 702, 277, 294, 37, 402, 1121, 384, 193, 991, 91, 737, 344, 61, 1176, 695, 276, 360, 1043, 1143, 774, 198, 970, 1000, 597, 1079, 408, 284, 671, 436, 28, 331, 718, 199, 694, 117, 381, 1090, 728, 1175, 844, 964, 192, 1027, 773, 1197, 406, 800, 1040, 768, 427, 272, 1063, 542, 827, 724, 555, 48, 775, 1125, 490, 438, 976, 382, 929, 977, 8, 123, 1095, 1210, 1050, 163, 1061, 1031, 556, 998, 321, 98, 1201, 310, 639, 457, 839, 173, 1093, 1026, 529, 214, 1032, 366, 233, 636, 426, 159, 181, 1039, 962, 900, 1194, 570, 640, 291, 41, 191, 2, 351, 1209, 573, 476, 749, 606, 235, 813, 216, 648, 450, 1198, 995, 993, 513, 430, 147, 1134, 72, 777, 1019, 1106, 229, 754, 641, 1107, 744, 109, 225, 257, 605, 251]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7580743448180963
the save name prefix for this run is:  chkpt-ID_7580743448180963_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 497
rank avg (pred): 0.414 +- 0.003
mrr vals (pred, true): 0.000, 0.264
batch losses (mrrl, rdl): 0.0, 0.0010480657

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 545
rank avg (pred): 0.211 +- 0.002
mrr vals (pred, true): 0.000, 0.144
batch losses (mrrl, rdl): 0.0, 0.0001537944

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 792
rank avg (pred): 0.517 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.000115104

Epoch over!
epoch time: 11.938

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 932
rank avg (pred): 0.577 +- 0.005
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0021260099

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 999
rank avg (pred): 0.397 +- 0.092
mrr vals (pred, true): 0.002, 0.055
batch losses (mrrl, rdl): 0.0, 0.0001490238

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 175
rank avg (pred): 0.419 +- 0.263
mrr vals (pred, true): 0.177, 0.001
batch losses (mrrl, rdl): 0.0, 4.8884e-05

Epoch over!
epoch time: 11.915

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 26
rank avg (pred): 0.092 +- 0.065
mrr vals (pred, true): 0.242, 0.306
batch losses (mrrl, rdl): 0.0, 2.92466e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1002
rank avg (pred): 0.391 +- 0.282
mrr vals (pred, true): 0.220, 0.070
batch losses (mrrl, rdl): 0.0, 0.0001095012

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 357
rank avg (pred): 0.374 +- 0.267
mrr vals (pred, true): 0.218, 0.049
batch losses (mrrl, rdl): 0.0, 7.16555e-05

Epoch over!
epoch time: 11.733

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 283
rank avg (pred): 0.092 +- 0.070
mrr vals (pred, true): 0.254, 0.236
batch losses (mrrl, rdl): 0.0, 3.42668e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 908
rank avg (pred): 0.485 +- 0.354
mrr vals (pred, true): 0.225, 0.003
batch losses (mrrl, rdl): 0.0, 0.0003057793

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 575
rank avg (pred): 0.386 +- 0.284
mrr vals (pred, true): 0.213, 0.063
batch losses (mrrl, rdl): 0.0, 9.00711e-05

Epoch over!
epoch time: 11.874

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 964
rank avg (pred): 0.578 +- 0.378
mrr vals (pred, true): 0.174, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002625329

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1194
rank avg (pred): 0.385 +- 0.297
mrr vals (pred, true): 0.231, 0.001
batch losses (mrrl, rdl): 0.0, 9.70627e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 639
rank avg (pred): 0.419 +- 0.298
mrr vals (pred, true): 0.170, 0.064
batch losses (mrrl, rdl): 0.0, 0.0001871494

Epoch over!
epoch time: 11.864

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 570
rank avg (pred): 0.416 +- 0.296
mrr vals (pred, true): 0.169, 0.053
batch losses (mrrl, rdl): 0.142406106, 9.95164e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 543
rank avg (pred): 0.140 +- 0.112
mrr vals (pred, true): 0.156, 0.129
batch losses (mrrl, rdl): 0.0068641948, 0.0002340098

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1129
rank avg (pred): 0.451 +- 0.269
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.94352e-05, 6.55962e-05

Epoch over!
epoch time: 12.211

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 985
rank avg (pred): 0.131 +- 0.136
mrr vals (pred, true): 0.241, 0.220
batch losses (mrrl, rdl): 0.0043603349, 2.43306e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 467
rank avg (pred): 0.470 +- 0.289
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001874375, 4.91433e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 383
rank avg (pred): 0.449 +- 0.297
mrr vals (pred, true): 0.048, 0.055
batch losses (mrrl, rdl): 3.01846e-05, 0.0001028289

Epoch over!
epoch time: 12.106

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 287
rank avg (pred): 0.162 +- 0.143
mrr vals (pred, true): 0.236, 0.240
batch losses (mrrl, rdl): 0.0001621032, 0.0001261063

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 256
rank avg (pred): 0.122 +- 0.123
mrr vals (pred, true): 0.300, 0.303
batch losses (mrrl, rdl): 9.23282e-05, 0.000142309

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1162
rank avg (pred): 0.480 +- 0.313
mrr vals (pred, true): 0.045, 0.057
batch losses (mrrl, rdl): 0.0002612885, 0.0001245022

Epoch over!
epoch time: 12.062

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 565
rank avg (pred): 0.215 +- 0.162
mrr vals (pred, true): 0.145, 0.183
batch losses (mrrl, rdl): 0.0140583497, 6.75445e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 58
rank avg (pred): 0.191 +- 0.153
mrr vals (pred, true): 0.174, 0.134
batch losses (mrrl, rdl): 0.0156102804, 9.92224e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 220
rank avg (pred): 0.455 +- 0.319
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001748778, 7.29466e-05

Epoch over!
epoch time: 12.123

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 334
rank avg (pred): 0.419 +- 0.302
mrr vals (pred, true): 0.053, 0.065
batch losses (mrrl, rdl): 6.51831e-05, 8.1263e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 734
rank avg (pred): 0.396 +- 0.314
mrr vals (pred, true): 0.070, 0.009
batch losses (mrrl, rdl): 0.0041393684, 0.0002585789

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 998
rank avg (pred): 0.164 +- 0.138
mrr vals (pred, true): 0.189, 0.264
batch losses (mrrl, rdl): 0.0563813746, 8.13997e-05

Epoch over!
epoch time: 11.984

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 597
rank avg (pred): 0.426 +- 0.310
mrr vals (pred, true): 0.052, 0.051
batch losses (mrrl, rdl): 6.03054e-05, 3.50885e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 625
rank avg (pred): 0.408 +- 0.332
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 2.41373e-05, 9.5241e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 253
rank avg (pred): 0.091 +- 0.091
mrr vals (pred, true): 0.302, 0.305
batch losses (mrrl, rdl): 0.0001088712, 5.1764e-05

Epoch over!
epoch time: 12.469

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 574
rank avg (pred): 0.474 +- 0.349
mrr vals (pred, true): 0.047, 0.054
batch losses (mrrl, rdl): 6.76558e-05, 0.0001164598

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 569
rank avg (pred): 0.404 +- 0.310
mrr vals (pred, true): 0.059, 0.048
batch losses (mrrl, rdl): 0.000736738, 1.12496e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1007
rank avg (pred): 0.497 +- 0.330
mrr vals (pred, true): 0.047, 0.072
batch losses (mrrl, rdl): 0.0001189501, 0.0003236046

Epoch over!
epoch time: 12.093

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 94
rank avg (pred): 0.397 +- 0.306
mrr vals (pred, true): 0.066, 0.067
batch losses (mrrl, rdl): 0.0026413938, 4.54423e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 203
rank avg (pred): 0.436 +- 0.351
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 3.04e-07, 0.0001275897

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.109 +- 0.107
mrr vals (pred, true): 0.308, 0.269
batch losses (mrrl, rdl): 0.0149879158, 6.52965e-05

Epoch over!
epoch time: 12.163

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 139
rank avg (pred): 0.448 +- 0.343
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 5.493e-07, 9.82814e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 254
rank avg (pred): 0.092 +- 0.093
mrr vals (pred, true): 0.296, 0.304
batch losses (mrrl, rdl): 0.0006276791, 4.59672e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1001
rank avg (pred): 0.522 +- 0.304
mrr vals (pred, true): 0.049, 0.063
batch losses (mrrl, rdl): 1.20209e-05, 0.0003706289

Epoch over!
epoch time: 11.817

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 567
rank avg (pred): 0.507 +- 0.318
mrr vals (pred, true): 0.050, 0.059
batch losses (mrrl, rdl): 1.3627e-06, 0.0002333157

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 444
rank avg (pred): 0.456 +- 0.347
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.23099e-05, 8.19601e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 541
rank avg (pred): 0.209 +- 0.164
mrr vals (pred, true): 0.151, 0.132
batch losses (mrrl, rdl): 0.0035075147, 6.59213e-05

Epoch over!
epoch time: 12.315

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.595 +- 0.306
mrr vals (pred, true): 0.048, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   10 	     0 	 0.05258 	 8e-0500 	 m..s
   26 	     1 	 0.05371 	 8e-0500 	 m..s
   15 	     2 	 0.05274 	 0.00012 	 m..s
   60 	     3 	 0.06009 	 0.00020 	 m..s
   77 	     4 	 0.06388 	 0.00026 	 m..s
   76 	     5 	 0.06356 	 0.00027 	 m..s
    2 	     6 	 0.04593 	 0.00038 	 m..s
   31 	     7 	 0.05407 	 0.00049 	 m..s
   49 	     8 	 0.05698 	 0.00051 	 m..s
   20 	     9 	 0.05320 	 0.00052 	 m..s
   40 	    10 	 0.05534 	 0.00055 	 m..s
   39 	    11 	 0.05524 	 0.00056 	 m..s
   29 	    12 	 0.05389 	 0.00057 	 m..s
    4 	    13 	 0.05168 	 0.00057 	 m..s
    0 	    14 	 0.04525 	 0.00058 	 m..s
   67 	    15 	 0.06182 	 0.00058 	 m..s
   62 	    16 	 0.06042 	 0.00058 	 m..s
   17 	    17 	 0.05298 	 0.00058 	 m..s
   53 	    18 	 0.05811 	 0.00058 	 m..s
   12 	    19 	 0.05270 	 0.00059 	 m..s
   36 	    20 	 0.05486 	 0.00059 	 m..s
   67 	    21 	 0.06182 	 0.00059 	 m..s
   38 	    22 	 0.05513 	 0.00061 	 m..s
   35 	    23 	 0.05470 	 0.00064 	 m..s
   16 	    24 	 0.05294 	 0.00066 	 m..s
    3 	    25 	 0.04757 	 0.00068 	 m..s
   58 	    26 	 0.05980 	 0.00070 	 m..s
   64 	    27 	 0.06137 	 0.00071 	 m..s
   21 	    28 	 0.05325 	 0.00071 	 m..s
   19 	    29 	 0.05311 	 0.00071 	 m..s
    9 	    30 	 0.05257 	 0.00072 	 m..s
   17 	    31 	 0.05298 	 0.00073 	 m..s
   59 	    32 	 0.06007 	 0.00073 	 m..s
   56 	    33 	 0.05876 	 0.00073 	 m..s
   37 	    34 	 0.05504 	 0.00073 	 m..s
   32 	    35 	 0.05452 	 0.00075 	 m..s
   65 	    36 	 0.06138 	 0.00077 	 m..s
    5 	    37 	 0.05180 	 0.00079 	 m..s
   12 	    38 	 0.05270 	 0.00083 	 m..s
   41 	    39 	 0.05558 	 0.00084 	 m..s
   67 	    40 	 0.06182 	 0.00087 	 m..s
    1 	    41 	 0.04552 	 0.00088 	 m..s
    6 	    42 	 0.05208 	 0.00089 	 m..s
   74 	    43 	 0.06295 	 0.00098 	 m..s
   61 	    44 	 0.06013 	 0.00098 	 m..s
   48 	    45 	 0.05676 	 0.00103 	 m..s
   11 	    46 	 0.05265 	 0.00110 	 m..s
   79 	    47 	 0.08053 	 0.00112 	 m..s
    6 	    48 	 0.05208 	 0.00125 	 m..s
   78 	    49 	 0.07957 	 0.00359 	 m..s
   80 	    50 	 0.08144 	 0.00422 	 m..s
   23 	    51 	 0.05342 	 0.00475 	 m..s
   30 	    52 	 0.05389 	 0.00500 	 m..s
   71 	    53 	 0.06253 	 0.00628 	 m..s
   33 	    54 	 0.05455 	 0.04300 	 ~...
   22 	    55 	 0.05334 	 0.04752 	 ~...
   52 	    56 	 0.05784 	 0.05021 	 ~...
   42 	    57 	 0.05583 	 0.05057 	 ~...
   28 	    58 	 0.05374 	 0.05117 	 ~...
   44 	    59 	 0.05604 	 0.05260 	 ~...
   70 	    60 	 0.06183 	 0.05438 	 ~...
   34 	    61 	 0.05461 	 0.05544 	 ~...
   47 	    62 	 0.05668 	 0.05653 	 ~...
   54 	    63 	 0.05817 	 0.05685 	 ~...
    8 	    64 	 0.05236 	 0.05707 	 ~...
   63 	    65 	 0.06092 	 0.05721 	 ~...
   24 	    66 	 0.05355 	 0.05764 	 ~...
   46 	    67 	 0.05658 	 0.05781 	 ~...
   50 	    68 	 0.05752 	 0.05828 	 ~...
   51 	    69 	 0.05783 	 0.05885 	 ~...
   27 	    70 	 0.05372 	 0.06130 	 ~...
   55 	    71 	 0.05849 	 0.06171 	 ~...
   14 	    72 	 0.05271 	 0.06246 	 ~...
   75 	    73 	 0.06345 	 0.06447 	 ~...
   25 	    74 	 0.05367 	 0.06494 	 ~...
   45 	    75 	 0.05614 	 0.06609 	 ~...
   66 	    76 	 0.06141 	 0.06669 	 ~...
   72 	    77 	 0.06255 	 0.07139 	 ~...
   43 	    78 	 0.05603 	 0.07383 	 ~...
   72 	    79 	 0.06255 	 0.07389 	 ~...
   57 	    80 	 0.05911 	 0.08425 	 ~...
   83 	    81 	 0.17986 	 0.12554 	 m..s
   92 	    82 	 0.20754 	 0.14811 	 m..s
   84 	    83 	 0.18069 	 0.15094 	 ~...
   85 	    84 	 0.18141 	 0.15221 	 ~...
  101 	    85 	 0.21172 	 0.15883 	 m..s
   96 	    86 	 0.20859 	 0.16078 	 m..s
   99 	    87 	 0.21156 	 0.16345 	 m..s
   81 	    88 	 0.16881 	 0.17141 	 ~...
   82 	    89 	 0.17919 	 0.17309 	 ~...
   86 	    90 	 0.18204 	 0.18015 	 ~...
   98 	    91 	 0.21088 	 0.18082 	 m..s
   87 	    92 	 0.20537 	 0.18394 	 ~...
   89 	    93 	 0.20710 	 0.19080 	 ~...
   90 	    94 	 0.20713 	 0.20696 	 ~...
  102 	    95 	 0.23984 	 0.20773 	 m..s
   95 	    96 	 0.20846 	 0.21247 	 ~...
   92 	    97 	 0.20754 	 0.21861 	 ~...
   97 	    98 	 0.20991 	 0.22069 	 ~...
   99 	    99 	 0.21156 	 0.22977 	 ~...
   91 	   100 	 0.20751 	 0.23443 	 ~...
  103 	   101 	 0.24922 	 0.23944 	 ~...
  104 	   102 	 0.25044 	 0.23996 	 ~...
   92 	   103 	 0.20754 	 0.24007 	 m..s
  109 	   104 	 0.28507 	 0.24895 	 m..s
  107 	   105 	 0.28110 	 0.25400 	 ~...
   88 	   106 	 0.20707 	 0.25434 	 m..s
  104 	   107 	 0.25044 	 0.26253 	 ~...
  120 	   108 	 0.31923 	 0.26974 	 m..s
  106 	   109 	 0.28108 	 0.27749 	 ~...
  111 	   110 	 0.29397 	 0.27823 	 ~...
  116 	   111 	 0.30243 	 0.27927 	 ~...
  108 	   112 	 0.28500 	 0.27939 	 ~...
  110 	   113 	 0.29371 	 0.28088 	 ~...
  119 	   114 	 0.31919 	 0.28142 	 m..s
  113 	   115 	 0.29740 	 0.28170 	 ~...
  114 	   116 	 0.29780 	 0.28330 	 ~...
  112 	   117 	 0.29476 	 0.30087 	 ~...
  115 	   118 	 0.29946 	 0.30463 	 ~...
  117 	   119 	 0.31695 	 0.30982 	 ~...
  118 	   120 	 0.31699 	 0.32969 	 ~...
==========================================
r_mrr = 0.964712381362915
r2_mrr = 0.8481544256210327
spearmanr_mrr@5 = 0.8192204236984253
spearmanr_mrr@10 = 0.9153136014938354
spearmanr_mrr@50 = 0.980647087097168
spearmanr_mrr@100 = 0.9785889387130737
spearmanr_mrr@All = 0.9796941876411438
==========================================
test time: 0.459
Done Testing dataset OpenEA
total time taken: 196.21947145462036
training time taken: 181.20018601417542
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9647)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8482)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.8192)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9153)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9806)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9786)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9797)}}, 'test_loss': {'DistMult': {'OpenEA': 0.40122300958501}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 6861649783297940
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1050, 1113, 462, 279, 217, 75, 1074, 588, 986, 0, 624, 587, 204, 955, 861, 1110, 304, 923, 1056, 337, 158, 113, 522, 880, 692, 144, 80, 648, 724, 761, 371, 1132, 837, 66, 251, 447, 833, 355, 245, 260, 809, 179, 103, 511, 1037, 1196, 151, 816, 73, 97, 980, 82, 896, 405, 1012, 44, 237, 1082, 987, 764, 618, 660, 943, 438, 492, 708, 524, 1060, 222, 711, 731, 451, 652, 1199, 963, 948, 1160, 606, 209, 1011, 733, 244, 975, 77, 621, 49, 785, 556, 54, 302, 206, 116, 464, 993, 680, 569, 771, 376, 581, 367, 401, 1122, 124, 308, 67, 889, 122, 1191, 104, 1032, 659, 466, 544, 663, 180, 625, 1182, 669, 1112, 735, 315]
valid_ids (0): []
train_ids (1094): [358, 257, 594, 665, 386, 823, 1135, 770, 41, 309, 472, 318, 571, 129, 1129, 170, 820, 592, 699, 620, 391, 817, 615, 480, 292, 187, 893, 956, 351, 608, 1169, 233, 327, 970, 787, 593, 945, 1093, 1006, 561, 661, 64, 517, 1154, 742, 61, 1044, 372, 38, 235, 795, 579, 612, 1023, 636, 352, 545, 477, 395, 1007, 181, 573, 37, 788, 605, 1024, 381, 917, 457, 236, 433, 696, 397, 599, 991, 46, 275, 285, 446, 902, 469, 202, 713, 159, 421, 542, 633, 916, 726, 1005, 1202, 737, 555, 243, 877, 562, 239, 796, 547, 350, 149, 1141, 491, 1173, 228, 580, 1043, 1094, 954, 19, 1167, 941, 85, 740, 965, 707, 929, 627, 1038, 404, 939, 71, 838, 130, 961, 282, 90, 178, 39, 425, 1047, 390, 900, 1186, 388, 488, 199, 473, 51, 1021, 436, 422, 366, 1069, 1068, 1013, 600, 754, 514, 1017, 671, 250, 853, 74, 242, 1104, 1214, 842, 512, 115, 157, 389, 804, 1101, 437, 3, 818, 972, 62, 377, 769, 790, 306, 96, 319, 380, 345, 173, 453, 111, 1130, 373, 1073, 667, 474, 485, 368, 730, 1193, 746, 1097, 949, 1076, 523, 208, 591, 307, 238, 1162, 1146, 647, 224, 276, 280, 163, 506, 471, 392, 1161, 835, 28, 1095, 461, 489, 810, 914, 240, 765, 936, 828, 751, 1091, 253, 1036, 481, 942, 784, 1001, 136, 962, 622, 60, 69, 273, 768, 487, 529, 321, 637, 65, 91, 675, 172, 1055, 317, 48, 1121, 241, 1140, 1151, 767, 133, 921, 26, 774, 1170, 32, 753, 867, 234, 1210, 226, 1209, 881, 1086, 537, 1025, 1212, 1138, 155, 1148, 777, 799, 775, 803, 577, 297, 84, 118, 1134, 278, 286, 213, 52, 1123, 411, 50, 969, 63, 320, 901, 574, 834, 402, 1019, 468, 844, 840, 1072, 174, 14, 550, 518, 681, 20, 106, 776, 190, 448, 128, 1053, 9, 197, 1051, 1010, 576, 791, 503, 688, 254, 1163, 727, 349, 99, 734, 656, 16, 854, 335, 482, 836, 841, 990, 538, 1157, 1126, 974, 22, 897, 839, 56, 750, 15, 449, 132, 1045, 895, 1015, 246, 870, 634, 288, 638, 831, 979, 1195, 604, 499, 322, 676, 829, 903, 192, 100, 756, 684, 326, 1178, 695, 223, 105, 879, 1099, 348, 513, 258, 697, 1144, 568, 643, 365, 379, 406, 188, 6, 875, 31, 1035, 1083, 310, 305, 617, 1070, 398, 479, 685, 121, 454, 966, 189, 932, 443, 271, 1111, 1030, 229, 109, 868, 1064, 323, 314, 30, 778, 1149, 797, 748, 1071, 1198, 1018, 93, 23, 919, 525, 891, 533, 1197, 5, 1084, 832, 1100, 387, 1090, 1125, 885, 346, 1061, 344, 316, 720, 263, 215, 456, 140, 483, 535, 444, 1004, 1042, 642, 757, 40, 595, 1139, 802, 1033, 845, 938, 994, 114, 169, 1119, 982, 1203, 664, 603, 560, 721, 1128, 168, 430, 565, 300, 435, 898, 221, 1200, 17, 420, 510, 865, 1, 575, 646, 918, 888, 951, 700, 851, 1080, 295, 644, 364, 890, 112, 1040, 957, 682, 528, 551, 231, 850, 270, 291, 882, 156, 567, 922, 35, 557, 13, 908, 801, 983, 1192, 450, 752, 476, 267, 1171, 214, 42, 1114, 564, 811, 1046, 718, 290, 164, 1127, 1208, 826, 710, 704, 709, 1048, 1079, 911, 1176, 412, 907, 1002, 539, 905, 123, 334, 1153, 651, 1054, 184, 410, 1059, 928, 780, 670, 944, 356, 950, 301, 1087, 819, 862, 264, 1118, 583, 195, 86, 498, 857, 182, 497, 806, 312, 1031, 613, 920, 10, 460, 978, 18, 674, 741, 691, 493, 732, 937, 1088, 927, 629, 328, 33, 1065, 959, 72, 931, 393, 698, 200, 296, 101, 486, 1137, 988, 1034, 666, 378, 1143, 176, 1000, 747, 331, 201, 792, 382, 459, 1177, 4, 1014, 532, 715, 299, 230, 415, 1009, 601, 892, 1081, 507, 255, 654, 423, 1039, 755, 261, 343, 690, 141, 1052, 484, 269, 543, 559, 536, 871, 496, 909, 672, 521, 693, 728, 616, 495, 196, 655, 194, 678, 668, 1057, 878, 1179, 641, 409, 166, 925, 611, 793, 375, 662, 992, 960, 95, 626, 120, 717, 519, 177, 1159, 821, 1181, 773, 640, 714, 126, 183, 1098, 333, 632, 1205, 884, 455, 293, 1120, 763, 869, 143, 658, 984, 582, 849, 34, 1166, 650, 70, 1041, 1190, 400, 515, 154, 403, 798, 1109, 702, 428, 940, 653, 419, 274, 1106, 1003, 298, 998, 935, 995, 578, 502, 374, 976, 848, 930, 598, 813, 1066, 470, 794, 500, 369, 546, 29, 1085, 822, 883, 716, 501, 805, 967, 540, 520, 824, 899, 162, 738, 370, 701, 504, 89, 683, 1201, 610, 873, 441, 1105, 452, 687, 1078, 27, 527, 330, 153, 946, 973, 146, 1152, 160, 342, 915, 619, 1206, 703, 341, 766, 508, 530, 1062, 1150, 249, 859, 887, 87, 1027, 139, 552, 572, 294, 1168, 394, 458, 736, 142, 78, 516, 1096, 256, 906, 1063, 847, 262, 947, 723, 43, 1092, 989, 1175, 55, 566, 924, 399, 762, 781, 1108, 814, 505, 1089, 1174, 359, 589, 259, 825, 1029, 1115, 1147, 705, 1189, 1131, 706, 102, 36, 266, 968, 760, 864, 509, 597, 933, 219, 431, 161, 843, 440, 465, 800, 442, 759, 11, 874, 347, 332, 686, 852, 383, 24, 812, 541, 913, 904, 1020, 289, 1049, 628, 427, 807, 147, 88, 478, 602, 1008, 772, 872, 414, 212, 417, 165, 1124, 584, 463, 786, 729, 531, 152, 558, 248, 712, 98, 125, 635, 630, 150, 1156, 657, 1026, 117, 186, 277, 1067, 1016, 1107, 216, 338, 110, 860, 631, 272, 268, 1204, 999, 2, 745, 45, 743, 198, 434, 1077, 127, 283, 1022, 1158, 815, 1165, 1184, 808, 1117, 21, 47, 1207, 83, 353, 108, 886, 855, 287, 490, 1194, 689, 25, 357, 119, 220, 934, 876, 252, 76, 167, 549, 534, 467, 997, 1103, 426, 131, 175, 137, 385, 225, 719, 284, 863, 609, 185, 694, 590, 894, 445, 92, 1142, 1136, 1188, 526, 679, 408, 926, 1075, 649, 138, 134, 677, 971, 362, 191, 1145, 846, 247, 354, 645, 1180, 324, 858, 952, 1028, 1172, 311, 313, 1187, 339, 856, 673, 193, 1058, 59, 912, 303, 416, 1102, 8, 439, 432, 171, 1211, 548, 107, 782, 1133, 553, 12, 1164, 586, 148, 265, 1185, 953, 964, 361, 407, 739, 135, 360, 722, 1116, 207, 585, 639, 985, 384, 554, 218, 1155, 996, 145, 614, 827, 977, 475, 758, 623, 1183, 830, 607, 424, 79, 779, 227, 396, 232, 1213, 363, 725, 329, 749, 570, 783, 281, 210, 981, 789, 58, 413, 57, 68, 325, 958, 429, 336, 596, 7, 866, 81, 418, 211, 340, 203, 205, 563, 94, 744, 910, 494, 53]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7239210844666301
the save name prefix for this run is:  chkpt-ID_7239210844666301_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 514
rank avg (pred): 0.486 +- 0.004
mrr vals (pred, true): 0.000, 0.133
batch losses (mrrl, rdl): 0.0, 0.0015099852

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1016
rank avg (pred): 0.419 +- 0.323
mrr vals (pred, true): 0.004, 0.076
batch losses (mrrl, rdl): 0.0, 0.0001339284

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 255
rank avg (pred): 0.103 +- 0.234
mrr vals (pred, true): 0.035, 0.299
batch losses (mrrl, rdl): 0.0, 6.61181e-05

Epoch over!
epoch time: 12.02

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.263 +- 0.297
mrr vals (pred, true): 0.004, 0.212
batch losses (mrrl, rdl): 0.0, 2.3559e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 693
rank avg (pred): 0.424 +- 0.314
mrr vals (pred, true): 0.001, 0.001
batch losses (mrrl, rdl): 0.0, 8.74251e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 811
rank avg (pred): 0.139 +- 0.261
mrr vals (pred, true): 0.030, 0.055
batch losses (mrrl, rdl): 0.0, 1.88167e-05

Epoch over!
epoch time: 11.816

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 211
rank avg (pred): 0.355 +- 0.312
mrr vals (pred, true): 0.020, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003251724

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 762
rank avg (pred): 0.484 +- 0.324
mrr vals (pred, true): 0.002, 0.001
batch losses (mrrl, rdl): 0.0, 7.8581e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.391 +- 0.322
mrr vals (pred, true): 0.008, 0.000
batch losses (mrrl, rdl): 0.0, 0.000178992

Epoch over!
epoch time: 11.754

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 884
rank avg (pred): 0.453 +- 0.301
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 3.41932e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 427
rank avg (pred): 0.447 +- 0.306
mrr vals (pred, true): 0.001, 0.000
batch losses (mrrl, rdl): 0.0, 5.74291e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.434 +- 0.312
mrr vals (pred, true): 0.001, 0.068
batch losses (mrrl, rdl): 0.0, 0.000138745

Epoch over!
epoch time: 11.997

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1161
rank avg (pred): 0.447 +- 0.310
mrr vals (pred, true): 0.001, 0.059
batch losses (mrrl, rdl): 0.0, 0.0001336773

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 997
rank avg (pred): 0.077 +- 0.202
mrr vals (pred, true): 0.122, 0.258
batch losses (mrrl, rdl): 0.0, 3.82169e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 322
rank avg (pred): 0.074 +- 0.200
mrr vals (pred, true): 0.103, 0.256
batch losses (mrrl, rdl): 0.0, 1.69863e-05

Epoch over!
epoch time: 12.623

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 368
rank avg (pred): 0.427 +- 0.330
mrr vals (pred, true): 0.005, 0.056
batch losses (mrrl, rdl): 0.0201730411, 0.0001308399

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 513
rank avg (pred): 0.163 +- 0.320
mrr vals (pred, true): 0.180, 0.131
batch losses (mrrl, rdl): 0.0242885947, 0.0001126446

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 515
rank avg (pred): 0.167 +- 0.321
mrr vals (pred, true): 0.183, 0.140
batch losses (mrrl, rdl): 0.0187687278, 0.0001040921

Epoch over!
epoch time: 12.322

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 332
rank avg (pred): 0.455 +- 0.297
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002720621, 0.000138517

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 783
rank avg (pred): 0.495 +- 0.249
mrr vals (pred, true): 0.032, 0.001
batch losses (mrrl, rdl): 0.0032630633, 1.00834e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1001
rank avg (pred): 0.478 +- 0.249
mrr vals (pred, true): 0.036, 0.063
batch losses (mrrl, rdl): 0.0019437582, 0.000297567

Epoch over!
epoch time: 12.013

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 559
rank avg (pred): 0.177 +- 0.315
mrr vals (pred, true): 0.168, 0.186
batch losses (mrrl, rdl): 0.0031329112, 6.47878e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 390
rank avg (pred): 0.422 +- 0.291
mrr vals (pred, true): 0.072, 0.041
batch losses (mrrl, rdl): 0.0049292897, 9.47826e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 448
rank avg (pred): 0.436 +- 0.273
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0010392333, 7.70882e-05

Epoch over!
epoch time: 11.926

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1173
rank avg (pred): 0.441 +- 0.276
mrr vals (pred, true): 0.060, 0.063
batch losses (mrrl, rdl): 0.00106422, 0.0001913083

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 589
rank avg (pred): 0.437 +- 0.274
mrr vals (pred, true): 0.060, 0.073
batch losses (mrrl, rdl): 0.0009503325, 0.0001569773

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 184
rank avg (pred): 0.430 +- 0.254
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.78327e-05, 8.22571e-05

Epoch over!
epoch time: 11.996

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 857
rank avg (pred): 0.477 +- 0.252
mrr vals (pred, true): 0.037, 0.006
batch losses (mrrl, rdl): 0.0016590499, 3.1927e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 34
rank avg (pred): 0.164 +- 0.247
mrr vals (pred, true): 0.201, 0.161
batch losses (mrrl, rdl): 0.0164866149, 2.53278e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 526
rank avg (pred): 0.268 +- 0.278
mrr vals (pred, true): 0.169, 0.173
batch losses (mrrl, rdl): 0.0001732794, 3.46757e-05

Epoch over!
epoch time: 12.121

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 17
rank avg (pred): 0.106 +- 0.194
mrr vals (pred, true): 0.282, 0.305
batch losses (mrrl, rdl): 0.0053453236, 3.25574e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 230
rank avg (pred): 0.461 +- 0.275
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0006771777, 2.90155e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 391
rank avg (pred): 0.463 +- 0.264
mrr vals (pred, true): 0.052, 0.061
batch losses (mrrl, rdl): 5.16765e-05, 0.0002205747

Epoch over!
epoch time: 12.204

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 718
rank avg (pred): 0.487 +- 0.272
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001112838, 1.01094e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 870
rank avg (pred): 0.491 +- 0.189
mrr vals (pred, true): 0.020, 0.001
batch losses (mrrl, rdl): 0.009039754, 4.61057e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 806
rank avg (pred): 0.466 +- 0.242
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.000283176, 4.43111e-05

Epoch over!
epoch time: 12.0

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 86
rank avg (pred): 0.475 +- 0.265
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.000192589, 0.0001843947

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 487
rank avg (pred): 0.156 +- 0.287
mrr vals (pred, true): 0.274, 0.249
batch losses (mrrl, rdl): 0.0062322682, 3.66862e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 622
rank avg (pred): 0.483 +- 0.282
mrr vals (pred, true): 0.050, 0.049
batch losses (mrrl, rdl): 1.919e-07, 0.0001537614

Epoch over!
epoch time: 12.2

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 13
rank avg (pred): 0.071 +- 0.187
mrr vals (pred, true): 0.277, 0.300
batch losses (mrrl, rdl): 0.0052018249, 1.4206e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 385
rank avg (pred): 0.475 +- 0.294
mrr vals (pred, true): 0.053, 0.055
batch losses (mrrl, rdl): 7.42815e-05, 0.0001625829

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 108
rank avg (pred): 0.470 +- 0.282
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 1.51213e-05, 0.0001602804

Epoch over!
epoch time: 12.018

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 628
rank avg (pred): 0.462 +- 0.279
mrr vals (pred, true): 0.051, 0.058
batch losses (mrrl, rdl): 6.9035e-06, 0.000159327

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1150
rank avg (pred): 0.158 +- 0.293
mrr vals (pred, true): 0.243, 0.230
batch losses (mrrl, rdl): 0.001819051, 7.24432e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 143
rank avg (pred): 0.471 +- 0.285
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 4.7081e-06, 0.0001862588

Epoch over!
epoch time: 11.883

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.484 +- 0.285
mrr vals (pred, true): 0.048, 0.000

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.03762 	 8e-0500 	 m..s
   14 	     1 	 0.04686 	 0.00011 	 m..s
   82 	     2 	 0.05962 	 0.00024 	 m..s
   13 	     3 	 0.04604 	 0.00027 	 m..s
   12 	     4 	 0.04581 	 0.00035 	 m..s
   22 	     5 	 0.04794 	 0.00048 	 m..s
   26 	     6 	 0.04816 	 0.00049 	 m..s
   23 	     7 	 0.04797 	 0.00051 	 m..s
   70 	     8 	 0.05004 	 0.00051 	 m..s
   80 	     9 	 0.05213 	 0.00053 	 m..s
   58 	    10 	 0.04913 	 0.00055 	 m..s
   24 	    11 	 0.04805 	 0.00056 	 m..s
   45 	    12 	 0.04888 	 0.00056 	 m..s
    1 	    13 	 0.02728 	 0.00057 	 ~...
   34 	    14 	 0.04855 	 0.00057 	 m..s
    0 	    15 	 0.02676 	 0.00058 	 ~...
    7 	    16 	 0.04044 	 0.00059 	 m..s
    9 	    17 	 0.04503 	 0.00060 	 m..s
   54 	    18 	 0.04907 	 0.00060 	 m..s
   45 	    19 	 0.04888 	 0.00060 	 m..s
   49 	    20 	 0.04892 	 0.00061 	 m..s
   64 	    21 	 0.04950 	 0.00061 	 m..s
   32 	    22 	 0.04846 	 0.00061 	 m..s
   36 	    23 	 0.04856 	 0.00062 	 m..s
   39 	    24 	 0.04866 	 0.00063 	 m..s
    8 	    25 	 0.04069 	 0.00064 	 m..s
    4 	    26 	 0.03055 	 0.00064 	 ~...
   47 	    27 	 0.04890 	 0.00064 	 m..s
   50 	    28 	 0.04893 	 0.00065 	 m..s
    2 	    29 	 0.02929 	 0.00067 	 ~...
   63 	    30 	 0.04945 	 0.00068 	 m..s
    3 	    31 	 0.03017 	 0.00068 	 ~...
   33 	    32 	 0.04850 	 0.00070 	 m..s
   75 	    33 	 0.05080 	 0.00070 	 m..s
   11 	    34 	 0.04557 	 0.00073 	 m..s
   44 	    35 	 0.04887 	 0.00073 	 m..s
   27 	    36 	 0.04819 	 0.00073 	 m..s
   66 	    37 	 0.04957 	 0.00073 	 m..s
   15 	    38 	 0.04750 	 0.00075 	 m..s
   81 	    39 	 0.05254 	 0.00080 	 m..s
   34 	    40 	 0.04855 	 0.00082 	 m..s
   10 	    41 	 0.04553 	 0.00083 	 m..s
   48 	    42 	 0.04891 	 0.00084 	 m..s
   76 	    43 	 0.05083 	 0.00085 	 m..s
   28 	    44 	 0.04820 	 0.00085 	 m..s
   57 	    45 	 0.04910 	 0.00087 	 m..s
   25 	    46 	 0.04812 	 0.00087 	 m..s
   37 	    47 	 0.04861 	 0.00089 	 m..s
   62 	    48 	 0.04939 	 0.00091 	 m..s
   71 	    49 	 0.05008 	 0.00099 	 m..s
   16 	    50 	 0.04754 	 0.00109 	 m..s
   79 	    51 	 0.05187 	 0.00115 	 m..s
   84 	    52 	 0.06080 	 0.00381 	 m..s
    5 	    53 	 0.03270 	 0.00447 	 ~...
   85 	    54 	 0.06675 	 0.00464 	 m..s
   60 	    55 	 0.04920 	 0.03871 	 ~...
   65 	    56 	 0.04951 	 0.04796 	 ~...
   41 	    57 	 0.04877 	 0.04825 	 ~...
   69 	    58 	 0.04990 	 0.05021 	 ~...
   67 	    59 	 0.04975 	 0.05240 	 ~...
   52 	    60 	 0.04895 	 0.05291 	 ~...
   67 	    61 	 0.04975 	 0.05326 	 ~...
   55 	    62 	 0.04909 	 0.05373 	 ~...
   61 	    63 	 0.04925 	 0.05685 	 ~...
   17 	    64 	 0.04772 	 0.05707 	 ~...
   31 	    65 	 0.04839 	 0.05763 	 ~...
   53 	    66 	 0.04905 	 0.05764 	 ~...
   74 	    67 	 0.05074 	 0.05806 	 ~...
   19 	    68 	 0.04780 	 0.05969 	 ~...
   51 	    69 	 0.04895 	 0.06011 	 ~...
   30 	    70 	 0.04822 	 0.06177 	 ~...
   56 	    71 	 0.04910 	 0.06203 	 ~...
   42 	    72 	 0.04880 	 0.06258 	 ~...
   38 	    73 	 0.04863 	 0.06265 	 ~...
   40 	    74 	 0.04869 	 0.06321 	 ~...
   43 	    75 	 0.04883 	 0.06378 	 ~...
   21 	    76 	 0.04791 	 0.06554 	 ~...
   18 	    77 	 0.04778 	 0.06585 	 ~...
   77 	    78 	 0.05104 	 0.06605 	 ~...
   72 	    79 	 0.05036 	 0.06625 	 ~...
   20 	    80 	 0.04786 	 0.06840 	 ~...
   29 	    81 	 0.04822 	 0.07389 	 ~...
   59 	    82 	 0.04919 	 0.07673 	 ~...
   83 	    83 	 0.06019 	 0.08079 	 ~...
   72 	    84 	 0.05036 	 0.08195 	 m..s
   78 	    85 	 0.05111 	 0.08886 	 m..s
   87 	    86 	 0.12322 	 0.12986 	 ~...
   91 	    87 	 0.15980 	 0.13583 	 ~...
   92 	    88 	 0.16748 	 0.15093 	 ~...
   93 	    89 	 0.16808 	 0.15826 	 ~...
   86 	    90 	 0.12278 	 0.15855 	 m..s
   88 	    91 	 0.13110 	 0.16009 	 ~...
   90 	    92 	 0.13534 	 0.17635 	 m..s
   88 	    93 	 0.13110 	 0.17666 	 m..s
   94 	    94 	 0.16856 	 0.17924 	 ~...
   98 	    95 	 0.17877 	 0.20050 	 ~...
   94 	    96 	 0.16856 	 0.20696 	 m..s
  107 	    97 	 0.21971 	 0.21965 	 ~...
  104 	    98 	 0.21845 	 0.22369 	 ~...
   99 	    99 	 0.19458 	 0.22448 	 ~...
   97 	   100 	 0.17762 	 0.22740 	 m..s
  103 	   101 	 0.21776 	 0.22936 	 ~...
  101 	   102 	 0.19643 	 0.23004 	 m..s
  100 	   103 	 0.19491 	 0.23115 	 m..s
  104 	   104 	 0.21845 	 0.23206 	 ~...
  106 	   105 	 0.21868 	 0.23545 	 ~...
   96 	   106 	 0.16906 	 0.23716 	 m..s
  108 	   107 	 0.22842 	 0.23827 	 ~...
  102 	   108 	 0.20431 	 0.23944 	 m..s
  110 	   109 	 0.24929 	 0.25176 	 ~...
  109 	   110 	 0.24617 	 0.26014 	 ~...
  111 	   111 	 0.25535 	 0.27465 	 ~...
  112 	   112 	 0.27145 	 0.27913 	 ~...
  114 	   113 	 0.27638 	 0.28088 	 ~...
  113 	   114 	 0.27593 	 0.28573 	 ~...
  114 	   115 	 0.27638 	 0.28719 	 ~...
  116 	   116 	 0.28034 	 0.30087 	 ~...
  117 	   117 	 0.28670 	 0.30616 	 ~...
  118 	   118 	 0.28946 	 0.31362 	 ~...
  119 	   119 	 0.29942 	 0.31837 	 ~...
  120 	   120 	 0.30144 	 0.32380 	 ~...
==========================================
r_mrr = 0.9613431692123413
r2_mrr = 0.8793824315071106
spearmanr_mrr@5 = 0.9722428321838379
spearmanr_mrr@10 = 0.9391413927078247
spearmanr_mrr@50 = 0.994162917137146
spearmanr_mrr@100 = 0.969576358795166
spearmanr_mrr@All = 0.9703892469406128
==========================================
test time: 0.457
Done Testing dataset OpenEA
total time taken: 196.86175680160522
training time taken: 181.42272567749023
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9613)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8794)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9722)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9391)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9942)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9696)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9704)}}, 'test_loss': {'DistMult': {'OpenEA': 0.2954568122868295}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 6482657741887021
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [831, 398, 756, 12, 874, 22, 364, 889, 1062, 726, 1200, 1190, 984, 1058, 329, 1061, 356, 1059, 109, 1201, 451, 169, 852, 151, 457, 103, 791, 1117, 462, 1086, 570, 622, 588, 904, 880, 70, 550, 671, 982, 466, 944, 277, 345, 1166, 524, 1076, 1060, 763, 698, 104, 97, 555, 408, 1033, 371, 782, 219, 840, 663, 812, 192, 1021, 851, 882, 148, 83, 404, 1210, 801, 688, 1131, 813, 1195, 773, 884, 420, 266, 611, 1146, 303, 646, 717, 665, 470, 708, 308, 856, 295, 247, 713, 75, 336, 686, 312, 896, 1179, 651, 358, 253, 768, 695, 87, 641, 1051, 1112, 675, 1066, 9, 43, 534, 347, 977, 218, 414, 48, 1073, 960, 117, 57, 324, 258]
valid_ids (0): []
train_ids (1094): [631, 888, 229, 521, 829, 1134, 283, 206, 758, 484, 116, 1014, 200, 4, 659, 839, 980, 734, 598, 415, 24, 58, 992, 183, 918, 681, 1083, 209, 93, 584, 576, 316, 13, 461, 184, 73, 737, 684, 822, 687, 966, 362, 112, 927, 624, 1142, 664, 105, 82, 1100, 507, 1071, 1042, 479, 139, 615, 548, 797, 897, 761, 513, 1137, 958, 542, 91, 581, 232, 394, 1095, 114, 627, 596, 667, 311, 418, 964, 163, 496, 1120, 429, 994, 54, 1046, 1212, 764, 1043, 1136, 1054, 1090, 231, 402, 1056, 842, 578, 477, 733, 640, 78, 893, 214, 634, 1070, 363, 1018, 288, 1079, 1211, 1129, 1160, 1025, 629, 1214, 201, 527, 31, 1057, 268, 707, 69, 493, 950, 728, 1085, 118, 1194, 385, 1186, 557, 208, 536, 1022, 185, 1209, 847, 305, 620, 350, 252, 915, 544, 282, 338, 241, 905, 817, 153, 1102, 121, 240, 62, 403, 1093, 929, 337, 518, 644, 571, 635, 1088, 907, 514, 1159, 648, 1135, 873, 849, 946, 926, 357, 98, 908, 271, 260, 895, 178, 628, 776, 986, 1154, 608, 603, 452, 1109, 310, 975, 77, 724, 807, 878, 181, 894, 1163, 868, 930, 826, 906, 770, 872, 1004, 538, 475, 870, 126, 509, 767, 999, 1035, 1064, 891, 696, 101, 1105, 552, 658, 775, 670, 561, 1080, 1157, 317, 1007, 769, 269, 690, 900, 787, 354, 1165, 832, 574, 600, 610, 784, 1067, 328, 39, 1144, 445, 860, 111, 861, 1203, 621, 431, 92, 215, 540, 592, 579, 931, 395, 469, 72, 1132, 94, 1001, 468, 341, 467, 259, 692, 494, 924, 210, 1050, 175, 106, 71, 1149, 941, 981, 1097, 254, 939, 963, 1124, 649, 18, 1138, 819, 545, 1111, 14, 704, 248, 437, 612, 327, 102, 745, 1016, 1072, 645, 459, 188, 837, 1030, 1032, 1099, 196, 361, 916, 711, 546, 373, 195, 23, 1172, 68, 700, 140, 221, 809, 323, 625, 935, 932, 705, 47, 865, 84, 938, 501, 1092, 454, 164, 146, 313, 572, 344, 771, 256, 6, 460, 383, 875, 198, 1063, 1037, 44, 549, 742, 1084, 1041, 177, 922, 693, 359, 1202, 883, 294, 482, 405, 1104, 388, 480, 987, 816, 859, 376, 1015, 430, 1204, 458, 249, 558, 1068, 1167, 564, 187, 224, 747, 993, 824, 848, 374, 709, 774, 1143, 1171, 142, 152, 413, 732, 792, 29, 340, 730, 890, 330, 261, 647, 1052, 1000, 162, 393, 1045, 235, 844, 1127, 63, 632, 971, 1110, 729, 715, 138, 560, 1173, 5, 506, 1213, 779, 159, 64, 320, 522, 504, 86, 191, 391, 786, 1011, 16, 650, 951, 34, 519, 27, 720, 1119, 973, 1147, 727, 161, 435, 302, 367, 407, 805, 562, 1118, 869, 1196, 854, 275, 1181, 307, 28, 585, 677, 2, 1180, 1019, 954, 296, 250, 702, 703, 274, 100, 793, 427, 197, 499, 474, 515, 10, 7, 265, 272, 952, 202, 446, 559, 51, 335, 236, 1148, 920, 390, 909, 943, 483, 1023, 488, 264, 988, 974, 193, 589, 478, 913, 953, 602, 934, 133, 1115, 1207, 113, 382, 147, 537, 877, 820, 565, 790, 910, 88, 61, 143, 19, 529, 136, 531, 36, 160, 1178, 1098, 948, 750, 473, 66, 788, 392, 230, 997, 237, 850, 59, 378, 638, 991, 76, 297, 587, 743, 901, 123, 95, 321, 1158, 290, 251, 270, 497, 516, 680, 438, 682, 656, 660, 1096, 52, 502, 301, 11, 1003, 1048, 962, 976, 676, 421, 735, 306, 1128, 879, 706, 278, 520, 485, 1155, 1103, 442, 755, 567, 845, 979, 389, 867, 315, 661, 45, 1024, 547, 360, 613, 1123, 158, 1133, 885, 168, 881, 553, 637, 242, 65, 1182, 956, 67, 503, 937, 1031, 970, 1141, 137, 871, 368, 490, 1038, 1193, 1187, 772, 481, 370, 190, 1161, 352, 666, 583, 996, 556, 808, 1125, 35, 551, 42, 287, 911, 381, 289, 498, 157, 463, 56, 510, 921, 1013, 377, 212, 149, 972, 380, 1005, 318, 796, 15, 967, 1208, 1049, 226, 834, 273, 1199, 194, 762, 49, 1027, 448, 293, 211, 606, 85, 436, 823, 836, 714, 495, 652, 601, 339, 419, 783, 286, 721, 145, 108, 748, 736, 366, 1055, 125, 351, 127, 505, 719, 657, 1106, 914, 955, 563, 186, 811, 853, 1188, 1184, 741, 617, 789, 607, 925, 653, 759, 20, 577, 55, 440, 525, 575, 623, 90, 299, 864, 375, 353, 279, 1008, 1034, 633, 46, 444, 990, 1140, 511, 122, 1108, 233, 60, 566, 616, 678, 1176, 766, 1183, 155, 841, 898, 120, 471, 1162, 862, 802, 1139, 795, 995, 372, 346, 441, 298, 285, 425, 618, 1151, 1192, 781, 985, 267, 857, 33, 412, 220, 863, 343, 928, 131, 411, 80, 1009, 892, 983, 489, 334, 207, 642, 167, 757, 1012, 417, 17, 590, 110, 227, 738, 32, 284, 81, 532, 1026, 238, 154, 1010, 225, 777, 754, 722, 234, 426, 1205, 639, 753, 683, 716, 300, 539, 173, 798, 619, 827, 1177, 500, 902, 456, 517, 838, 406, 569, 858, 593, 523, 333, 216, 434, 486, 189, 936, 785, 1126, 8, 855, 3, 673, 450, 203, 1197, 945, 476, 989, 1156, 443, 947, 1185, 1121, 1206, 40, 1130, 806, 1069, 379, 396, 1006, 141, 843, 26, 1002, 746, 53, 541, 156, 424, 150, 449, 751, 917, 1017, 401, 508, 968, 512, 694, 262, 739, 662, 778, 107, 205, 1020, 1116, 99, 731, 170, 314, 487, 416, 899, 554, 400, 1174, 528, 326, 174, 535, 228, 204, 1044, 50, 998, 369, 332, 573, 655, 876, 325, 213, 580, 712, 794, 543, 626, 526, 568, 594, 171, 799, 432, 166, 723, 978, 0, 800, 835, 1036, 397, 886, 699, 1153, 129, 725, 959, 830, 447, 679, 309, 1145, 933, 291, 1089, 472, 179, 1164, 1082, 1053, 292, 263, 1150, 255, 940, 132, 1101, 1191, 804, 409, 1175, 828, 691, 1077, 172, 217, 453, 582, 280, 833, 319, 199, 180, 919, 810, 144, 923, 239, 387, 365, 597, 942, 903, 386, 643, 1075, 1087, 79, 689, 384, 25, 244, 1122, 455, 718, 669, 355, 1198, 74, 410, 654, 1169, 614, 96, 399, 1078, 1107, 464, 222, 530, 331, 423, 38, 128, 428, 124, 740, 348, 182, 30, 349, 887, 697, 1170, 674, 165, 322, 949, 37, 599, 961, 591, 710, 765, 744, 492, 245, 1094, 1189, 130, 1065, 803, 89, 815, 821, 21, 912, 630, 1168, 965, 969, 609, 257, 223, 1047, 1091, 276, 701, 135, 604, 1039, 533, 41, 749, 246, 866, 586, 1028, 134, 439, 1074, 752, 176, 281, 846, 119, 685, 957, 818, 1113, 115, 814, 1040, 1114, 422, 491, 595, 1152, 1, 636, 243, 780, 465, 605, 825, 342, 668, 1081, 304, 672, 433, 760, 1029]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  48310380606809
the save name prefix for this run is:  chkpt-ID_48310380606809_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 536
rank avg (pred): 0.475 +- 0.006
mrr vals (pred, true): 0.000, 0.204
batch losses (mrrl, rdl): 0.0, 0.0013662589

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 377
rank avg (pred): 0.424 +- 0.261
mrr vals (pred, true): 0.118, 0.060
batch losses (mrrl, rdl): 0.0, 0.0002137373

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1096
rank avg (pred): 0.391 +- 0.311
mrr vals (pred, true): 0.184, 0.056
batch losses (mrrl, rdl): 0.0, 7.14476e-05

Epoch over!
epoch time: 12.058

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 119
rank avg (pred): 0.402 +- 0.316
mrr vals (pred, true): 0.177, 0.060
batch losses (mrrl, rdl): 0.0, 7.56075e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 607
rank avg (pred): 0.425 +- 0.319
mrr vals (pred, true): 0.147, 0.058
batch losses (mrrl, rdl): 0.0, 0.0001029842

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 556
rank avg (pred): 0.209 +- 0.302
mrr vals (pred, true): 0.171, 0.159
batch losses (mrrl, rdl): 0.0, 1.64541e-05

Epoch over!
epoch time: 11.991

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 427
rank avg (pred): 0.423 +- 0.307
mrr vals (pred, true): 0.106, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001138049

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 728
rank avg (pred): 0.415 +- 0.316
mrr vals (pred, true): 0.134, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001073152

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 36
rank avg (pred): 0.107 +- 0.230
mrr vals (pred, true): 0.203, 0.200
batch losses (mrrl, rdl): 0.0, 2.8094e-06

Epoch over!
epoch time: 12.001

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 59
rank avg (pred): 0.096 +- 0.217
mrr vals (pred, true): 0.193, 0.152
batch losses (mrrl, rdl): 0.0, 1.37093e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 539
rank avg (pred): 0.234 +- 0.318
mrr vals (pred, true): 0.147, 0.201
batch losses (mrrl, rdl): 0.0, 6.4711e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 795
rank avg (pred): 0.471 +- 0.325
mrr vals (pred, true): 0.159, 0.001
batch losses (mrrl, rdl): 0.0, 7.7657e-06

Epoch over!
epoch time: 12.08

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 282
rank avg (pred): 0.083 +- 0.202
mrr vals (pred, true): 0.233, 0.200
batch losses (mrrl, rdl): 0.0, 6.6613e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 443
rank avg (pred): 0.390 +- 0.314
mrr vals (pred, true): 0.166, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002148733

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 275
rank avg (pred): 0.096 +- 0.218
mrr vals (pred, true): 0.194, 0.163
batch losses (mrrl, rdl): 0.0, 4.0344e-06

Epoch over!
epoch time: 11.764

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 67
rank avg (pred): 0.095 +- 0.215
mrr vals (pred, true): 0.218, 0.207
batch losses (mrrl, rdl): 0.0012761924, 5.0411e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1173
rank avg (pred): 0.375 +- 0.195
mrr vals (pred, true): 0.049, 0.063
batch losses (mrrl, rdl): 1.60439e-05, 9.33662e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 591
rank avg (pred): 0.437 +- 0.165
mrr vals (pred, true): 0.044, 0.067
batch losses (mrrl, rdl): 0.0003117408, 0.0002681966

Epoch over!
epoch time: 12.297

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 498
rank avg (pred): 0.007 +- 0.007
mrr vals (pred, true): 0.251, 0.270
batch losses (mrrl, rdl): 0.0037562381, 0.0008319994

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 8
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.323, 0.283
batch losses (mrrl, rdl): 0.0159506314, 0.0001186223

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 150
rank avg (pred): 0.403 +- 0.186
mrr vals (pred, true): 0.049, 0.061
batch losses (mrrl, rdl): 3.8653e-06, 0.0001687671

Epoch over!
epoch time: 12.151

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 199
rank avg (pred): 0.398 +- 0.189
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.299e-05, 0.0001676281

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 454
rank avg (pred): 0.398 +- 0.191
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001797266, 0.0001670063

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1043
rank avg (pred): 0.405 +- 0.191
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002278279, 0.0001371396

Epoch over!
epoch time: 12.359

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 638
rank avg (pred): 0.386 +- 0.209
mrr vals (pred, true): 0.053, 0.060
batch losses (mrrl, rdl): 9.00196e-05, 9.54258e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 241
rank avg (pred): 0.404 +- 0.190
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.18933e-05, 0.0001552634

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 576
rank avg (pred): 0.463 +- 0.143
mrr vals (pred, true): 0.039, 0.078
batch losses (mrrl, rdl): 0.0011198645, 0.0003747029

Epoch over!
epoch time: 12.355

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 875
rank avg (pred): 0.478 +- 0.123
mrr vals (pred, true): 0.037, 0.001
batch losses (mrrl, rdl): 0.0017688806, 8.20304e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 293
rank avg (pred): 0.099 +- 0.106
mrr vals (pred, true): 0.237, 0.230
batch losses (mrrl, rdl): 0.000529278, 1.61987e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 417
rank avg (pred): 0.432 +- 0.171
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 6.33602e-05, 0.0001025729

Epoch over!
epoch time: 12.0

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 89
rank avg (pred): 0.434 +- 0.162
mrr vals (pred, true): 0.045, 0.054
batch losses (mrrl, rdl): 0.0002329328, 0.0002414016

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1204
rank avg (pred): 0.421 +- 0.176
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.08699e-05, 0.0001240132

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1197
rank avg (pred): 0.421 +- 0.173
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 6.59852e-05, 0.0001444725

Epoch over!
epoch time: 12.511

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 846
rank avg (pred): 0.462 +- 0.132
mrr vals (pred, true): 0.039, 0.006
batch losses (mrrl, rdl): 0.0012302212, 0.0001078089

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 412
rank avg (pred): 0.433 +- 0.159
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001946074, 0.0001126697

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 934
rank avg (pred): 0.554 +- 0.170
mrr vals (pred, true): 0.038, 0.000
batch losses (mrrl, rdl): 0.0014442406, 0.0022801603

Epoch over!
epoch time: 12.129

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 499
rank avg (pred): 0.097 +- 0.143
mrr vals (pred, true): 0.268, 0.268
batch losses (mrrl, rdl): 4.7525e-06, 0.000229017

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 488
rank avg (pred): 0.102 +- 0.153
mrr vals (pred, true): 0.264, 0.252
batch losses (mrrl, rdl): 0.0014909894, 0.0001838163

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 479
rank avg (pred): 0.404 +- 0.182
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004890464, 0.0001646275

Epoch over!
epoch time: 12.107

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 922
rank avg (pred): 0.560 +- 0.154
mrr vals (pred, true): 0.035, 0.000
batch losses (mrrl, rdl): 0.0021098449, 9.17581e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 385
rank avg (pred): 0.405 +- 0.184
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001778823, 0.0001233687

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 232
rank avg (pred): 0.411 +- 0.183
mrr vals (pred, true): 0.052, 0.000
batch losses (mrrl, rdl): 3.64163e-05, 0.000156941

Epoch over!
epoch time: 12.267

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 859
rank avg (pred): 0.456 +- 0.140
mrr vals (pred, true): 0.039, 0.002
batch losses (mrrl, rdl): 0.0011343824, 0.0001423588

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 416
rank avg (pred): 0.421 +- 0.175
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 5.4224e-06, 0.0001246018

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 766
rank avg (pred): 0.452 +- 0.148
mrr vals (pred, true): 0.041, 0.006
batch losses (mrrl, rdl): 0.0007944959, 8.34289e-05

Epoch over!
epoch time: 12.406

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.217 +- 0.167
mrr vals (pred, true): 0.152, 0.159

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   55 	     0 	 0.04581 	 0.00011 	 m..s
    5 	     1 	 0.03780 	 0.00013 	 m..s
    6 	     2 	 0.03820 	 0.00028 	 m..s
    7 	     3 	 0.03823 	 0.00030 	 m..s
   12 	     4 	 0.03853 	 0.00037 	 m..s
   46 	     5 	 0.04460 	 0.00049 	 m..s
   79 	     6 	 0.04734 	 0.00049 	 m..s
   15 	     7 	 0.03880 	 0.00049 	 m..s
   75 	     8 	 0.04699 	 0.00050 	 m..s
   64 	     9 	 0.04632 	 0.00052 	 m..s
   81 	    10 	 0.04743 	 0.00053 	 m..s
   44 	    11 	 0.04423 	 0.00053 	 m..s
   33 	    12 	 0.04263 	 0.00055 	 m..s
   52 	    13 	 0.04555 	 0.00055 	 m..s
   83 	    14 	 0.04765 	 0.00055 	 m..s
   22 	    15 	 0.04101 	 0.00055 	 m..s
   69 	    16 	 0.04652 	 0.00058 	 m..s
   52 	    17 	 0.04555 	 0.00059 	 m..s
   23 	    18 	 0.04125 	 0.00059 	 m..s
   18 	    19 	 0.04040 	 0.00061 	 m..s
   61 	    20 	 0.04617 	 0.00063 	 m..s
   15 	    21 	 0.03880 	 0.00063 	 m..s
   58 	    22 	 0.04598 	 0.00064 	 m..s
   72 	    23 	 0.04674 	 0.00067 	 m..s
   33 	    24 	 0.04263 	 0.00068 	 m..s
   39 	    25 	 0.04372 	 0.00068 	 m..s
    0 	    26 	 0.03534 	 0.00068 	 m..s
    1 	    27 	 0.03543 	 0.00069 	 m..s
   76 	    28 	 0.04707 	 0.00071 	 m..s
   11 	    29 	 0.03849 	 0.00073 	 m..s
    8 	    30 	 0.03845 	 0.00073 	 m..s
   21 	    31 	 0.04058 	 0.00074 	 m..s
   10 	    32 	 0.03846 	 0.00077 	 m..s
   74 	    33 	 0.04685 	 0.00077 	 m..s
   56 	    34 	 0.04584 	 0.00078 	 m..s
   50 	    35 	 0.04542 	 0.00080 	 m..s
   57 	    36 	 0.04589 	 0.00080 	 m..s
   48 	    37 	 0.04499 	 0.00080 	 m..s
   71 	    38 	 0.04665 	 0.00082 	 m..s
    9 	    39 	 0.03846 	 0.00083 	 m..s
   64 	    40 	 0.04632 	 0.00086 	 m..s
   78 	    41 	 0.04721 	 0.00086 	 m..s
   30 	    42 	 0.04213 	 0.00087 	 m..s
   20 	    43 	 0.04052 	 0.00087 	 m..s
   26 	    44 	 0.04168 	 0.00089 	 m..s
   54 	    45 	 0.04557 	 0.00092 	 m..s
    4 	    46 	 0.03770 	 0.00097 	 m..s
   13 	    47 	 0.03876 	 0.00104 	 m..s
   46 	    48 	 0.04460 	 0.00109 	 m..s
    3 	    49 	 0.03584 	 0.00123 	 m..s
   31 	    50 	 0.04239 	 0.00131 	 m..s
    2 	    51 	 0.03546 	 0.00211 	 m..s
   14 	    52 	 0.03879 	 0.00294 	 m..s
   17 	    53 	 0.03888 	 0.00635 	 m..s
   86 	    54 	 0.05294 	 0.00866 	 m..s
   80 	    55 	 0.04736 	 0.00925 	 m..s
   66 	    56 	 0.04636 	 0.04381 	 ~...
   49 	    57 	 0.04541 	 0.04732 	 ~...
   77 	    58 	 0.04710 	 0.04751 	 ~...
   42 	    59 	 0.04422 	 0.04825 	 ~...
   84 	    60 	 0.04808 	 0.04881 	 ~...
   59 	    61 	 0.04611 	 0.05187 	 ~...
   19 	    62 	 0.04052 	 0.05272 	 ~...
   67 	    63 	 0.04642 	 0.05373 	 ~...
   70 	    64 	 0.04657 	 0.05403 	 ~...
   67 	    65 	 0.04642 	 0.05445 	 ~...
   85 	    66 	 0.05235 	 0.05583 	 ~...
   35 	    67 	 0.04268 	 0.05605 	 ~...
   40 	    68 	 0.04417 	 0.05707 	 ~...
   29 	    69 	 0.04207 	 0.05763 	 ~...
   32 	    70 	 0.04257 	 0.05889 	 ~...
   82 	    71 	 0.04745 	 0.05968 	 ~...
   73 	    72 	 0.04678 	 0.06036 	 ~...
   45 	    73 	 0.04432 	 0.06187 	 ~...
   51 	    74 	 0.04549 	 0.06187 	 ~...
   27 	    75 	 0.04181 	 0.06272 	 ~...
   63 	    76 	 0.04622 	 0.06291 	 ~...
   37 	    77 	 0.04331 	 0.06625 	 ~...
   42 	    78 	 0.04422 	 0.06653 	 ~...
   24 	    79 	 0.04133 	 0.06748 	 ~...
   60 	    80 	 0.04611 	 0.06752 	 ~...
   28 	    81 	 0.04191 	 0.06912 	 ~...
   62 	    82 	 0.04618 	 0.07304 	 ~...
   41 	    83 	 0.04418 	 0.07606 	 m..s
   36 	    84 	 0.04282 	 0.07694 	 m..s
   38 	    85 	 0.04340 	 0.08195 	 m..s
   25 	    86 	 0.04152 	 0.08886 	 m..s
   91 	    87 	 0.16268 	 0.13420 	 ~...
   92 	    88 	 0.16656 	 0.14364 	 ~...
   93 	    89 	 0.16935 	 0.15339 	 ~...
   87 	    90 	 0.14542 	 0.15385 	 ~...
   89 	    91 	 0.15218 	 0.15880 	 ~...
   88 	    92 	 0.14706 	 0.15902 	 ~...
   99 	    93 	 0.18058 	 0.17280 	 ~...
   90 	    94 	 0.15439 	 0.17666 	 ~...
   97 	    95 	 0.17438 	 0.19080 	 ~...
  101 	    96 	 0.19870 	 0.19476 	 ~...
  100 	    97 	 0.19552 	 0.20460 	 ~...
   94 	    98 	 0.17199 	 0.21003 	 m..s
   95 	    99 	 0.17229 	 0.21217 	 m..s
  102 	   100 	 0.19993 	 0.21315 	 ~...
  108 	   101 	 0.22002 	 0.22332 	 ~...
  106 	   102 	 0.21210 	 0.22369 	 ~...
   96 	   103 	 0.17312 	 0.22389 	 m..s
   98 	   104 	 0.17952 	 0.22740 	 m..s
  103 	   105 	 0.20408 	 0.23311 	 ~...
  105 	   106 	 0.20861 	 0.23996 	 m..s
  104 	   107 	 0.20735 	 0.24123 	 m..s
  107 	   108 	 0.21338 	 0.26619 	 m..s
  111 	   109 	 0.28451 	 0.28408 	 ~...
  109 	   110 	 0.28396 	 0.29437 	 ~...
  109 	   111 	 0.28396 	 0.29794 	 ~...
  113 	   112 	 0.28720 	 0.30026 	 ~...
  115 	   113 	 0.28969 	 0.30429 	 ~...
  117 	   114 	 0.29146 	 0.30439 	 ~...
  114 	   115 	 0.28726 	 0.30501 	 ~...
  112 	   116 	 0.28709 	 0.30519 	 ~...
  116 	   117 	 0.28979 	 0.31170 	 ~...
  118 	   118 	 0.29280 	 0.32201 	 ~...
  118 	   119 	 0.29280 	 0.32380 	 m..s
  118 	   120 	 0.29280 	 0.32969 	 m..s
==========================================
r_mrr = 0.9667753577232361
r2_mrr = 0.8996427059173584
spearmanr_mrr@5 = 0.9357903003692627
spearmanr_mrr@10 = 0.9009526968002319
spearmanr_mrr@50 = 0.9917886257171631
spearmanr_mrr@100 = 0.9739193916320801
spearmanr_mrr@All = 0.9745388031005859
==========================================
test time: 0.423
Done Testing dataset OpenEA
total time taken: 198.06512928009033
training time taken: 182.97885942459106
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9668)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8996)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9358)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9010)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9918)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9739)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9745)}}, 'test_loss': {'DistMult': {'OpenEA': 0.2882812844209184}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 6947424689162423
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [680, 187, 1195, 1209, 706, 634, 65, 1104, 1144, 354, 186, 20, 889, 956, 34, 437, 393, 13, 239, 163, 1121, 129, 1153, 100, 873, 1125, 640, 135, 519, 692, 1143, 281, 905, 702, 531, 795, 472, 767, 215, 805, 227, 765, 97, 310, 548, 755, 684, 182, 976, 323, 149, 555, 991, 1028, 433, 502, 119, 544, 931, 708, 448, 1064, 619, 188, 256, 56, 52, 642, 987, 730, 605, 726, 514, 184, 543, 985, 1006, 710, 462, 107, 960, 255, 403, 496, 970, 830, 290, 971, 452, 379, 880, 817, 820, 655, 404, 269, 1050, 371, 274, 953, 370, 1166, 131, 1089, 1155, 580, 174, 1113, 358, 537, 807, 938, 349, 1086, 77, 941, 159, 1182, 681, 739, 45]
valid_ids (0): []
train_ids (1094): [202, 920, 685, 483, 123, 503, 417, 1011, 471, 1184, 3, 615, 67, 900, 212, 746, 198, 291, 355, 745, 1021, 1061, 979, 485, 524, 36, 891, 977, 328, 944, 1134, 588, 788, 427, 989, 1053, 487, 796, 439, 850, 939, 731, 787, 528, 568, 579, 718, 943, 1042, 424, 23, 1157, 398, 1024, 272, 651, 654, 1025, 1140, 848, 99, 425, 832, 968, 1126, 165, 553, 141, 887, 1213, 972, 384, 352, 311, 564, 451, 459, 1117, 583, 1022, 614, 336, 88, 108, 645, 912, 895, 11, 828, 219, 922, 41, 539, 280, 855, 80, 675, 205, 673, 1000, 993, 647, 676, 683, 735, 742, 810, 857, 842, 203, 698, 901, 1099, 264, 266, 1069, 94, 789, 999, 570, 1062, 286, 1167, 587, 443, 183, 1014, 378, 1047, 1201, 672, 824, 136, 1199, 919, 854, 663, 407, 516, 2, 162, 1194, 969, 558, 390, 933, 566, 415, 1039, 102, 150, 966, 696, 846, 46, 319, 813, 497, 192, 156, 259, 1204, 493, 1015, 1004, 662, 701, 76, 986, 14, 885, 177, 721, 190, 74, 73, 5, 957, 500, 575, 597, 997, 888, 126, 1055, 581, 152, 1188, 1038, 760, 603, 703, 9, 118, 1116, 963, 779, 978, 722, 24, 83, 777, 438, 302, 171, 27, 340, 1138, 728, 1177, 1081, 1163, 1109, 418, 7, 373, 1192, 374, 321, 1072, 1070, 248, 940, 284, 312, 87, 643, 1026, 1149, 4, 535, 406, 526, 506, 909, 426, 271, 623, 567, 911, 392, 786, 1020, 545, 275, 1178, 447, 515, 372, 865, 902, 361, 952, 723, 649, 608, 444, 862, 1058, 430, 1123, 229, 1137, 1044, 875, 630, 877, 69, 611, 1068, 195, 737, 1093, 1196, 1171, 273, 324, 556, 921, 679, 343, 251, 552, 429, 756, 839, 12, 1135, 903, 385, 932, 666, 220, 317, 216, 750, 762, 523, 1175, 1100, 768, 278, 359, 1131, 1148, 169, 298, 831, 613, 856, 122, 660, 584, 946, 53, 859, 296, 646, 1008, 160, 185, 1054, 709, 241, 173, 480, 214, 1193, 419, 199, 441, 1190, 181, 127, 874, 422, 207, 360, 369, 1161, 780, 363, 775, 344, 1096, 51, 98, 814, 1165, 1059, 837, 314, 917, 664, 436, 794, 628, 232, 844, 306, 261, 833, 113, 61, 297, 235, 1033, 860, 711, 1208, 180, 863, 157, 39, 1010, 697, 717, 914, 293, 465, 973, 221, 1115, 670, 234, 784, 1052, 213, 937, 481, 428, 890, 823, 1187, 450, 130, 687, 620, 479, 599, 81, 208, 578, 827, 951, 1048, 573, 617, 460, 638, 759, 405, 276, 690, 521, 469, 641, 1073, 397, 766, 197, 283, 179, 456, 58, 476, 498, 959, 95, 154, 994, 294, 1211, 211, 16, 1120, 125, 1156, 246, 288, 849, 6, 408, 980, 674, 299, 534, 389, 82, 851, 870, 300, 204, 1160, 876, 111, 287, 494, 59, 618, 801, 734, 104, 1063, 560, 432, 1046, 380, 1210, 116, 1152, 106, 930, 467, 260, 342, 138, 604, 412, 729, 838, 505, 818, 791, 1019, 1037, 455, 414, 509, 861, 667, 1159, 749, 1075, 103, 333, 490, 461, 132, 1197, 339, 258, 413, 1207, 489, 650, 1023, 399, 947, 91, 542, 883, 1085, 879, 50, 804, 364, 1118, 326, 350, 338, 929, 279, 318, 893, 1092, 84, 96, 172, 1066, 665, 48, 852, 1007, 38, 764, 700, 598, 43, 247, 453, 693, 508, 231, 1133, 836, 945, 1082, 1173, 572, 1198, 44, 341, 1127, 602, 1017, 704, 401, 466, 1110, 1108, 377, 1150, 409, 715, 238, 1147, 420, 621, 562, 771, 716, 178, 847, 688, 411, 304, 1056, 529, 906, 520, 170, 327, 240, 538, 1076, 499, 536, 1087, 591, 858, 49, 1111, 25, 253, 1016, 151, 918, 153, 896, 206, 974, 86, 29, 934, 1067, 793, 0, 1060, 935, 797, 1083, 586, 478, 301, 1141, 164, 714, 292, 309, 1128, 686, 1130, 353, 201, 1049, 217, 228, 778, 774, 965, 637, 809, 593, 362, 601, 511, 1088, 594, 744, 431, 334, 1091, 267, 325, 936, 892, 223, 513, 30, 996, 741, 926, 470, 950, 31, 329, 886, 434, 772, 47, 899, 995, 904, 268, 908, 482, 845, 368, 386, 866, 316, 143, 798, 622, 610, 1124, 1158, 840, 659, 32, 134, 33, 961, 639, 725, 872, 561, 191, 829, 17, 803, 168, 585, 491, 501, 1095, 1103, 923, 492, 609, 592, 282, 1106, 1029, 1174, 694, 320, 28, 554, 115, 388, 55, 758, 525, 1119, 569, 1094, 120, 773, 982, 137, 488, 245, 616, 236, 913, 458, 1105, 332, 607, 563, 648, 92, 210, 1186, 967, 1214, 357, 313, 189, 224, 475, 815, 477, 954, 927, 295, 15, 747, 486, 1018, 990, 1036, 1102, 175, 60, 468, 233, 1202, 89, 1162, 330, 606, 770, 565, 1079, 93, 507, 1002, 816, 423, 416, 910, 834, 1146, 669, 148, 1098, 806, 75, 63, 1142, 376, 66, 656, 10, 1203, 166, 85, 551, 736, 547, 22, 769, 668, 121, 707, 652, 395, 1189, 345, 72, 557, 1090, 1077, 1041, 577, 8, 624, 699, 1057, 464, 1078, 161, 532, 691, 445, 924, 695, 209, 671, 315, 1145, 819, 644, 263, 1185, 517, 252, 79, 265, 719, 864, 595, 442, 661, 600, 1009, 1206, 367, 732, 351, 800, 992, 146, 1043, 915, 1114, 826, 303, 1170, 869, 811, 571, 783, 1132, 139, 394, 381, 867, 225, 196, 112, 155, 942, 712, 790, 193, 754, 1101, 218, 881, 907, 1176, 504, 981, 550, 365, 705, 449, 26, 230, 308, 955, 446, 21, 495, 1034, 249, 1191, 18, 743, 549, 1045, 257, 1084, 474, 1097, 1112, 402, 894, 871, 1169, 792, 176, 305, 761, 677, 738, 142, 101, 878, 117, 1001, 1, 105, 337, 625, 853, 90, 753, 988, 147, 78, 626, 62, 1151, 1212, 574, 612, 1003, 821, 812, 627, 1168, 133, 799, 140, 897, 391, 1179, 653, 1027, 421, 1030, 158, 410, 382, 1065, 958, 733, 802, 1032, 484, 882, 720, 916, 396, 678, 540, 1074, 243, 37, 454, 194, 242, 763, 400, 387, 109, 782, 1013, 1180, 64, 383, 1181, 322, 1035, 636, 658, 254, 40, 335, 463, 781, 262, 998, 713, 962, 657, 776, 682, 751, 727, 740, 114, 785, 757, 596, 331, 822, 35, 128, 57, 541, 632, 530, 843, 250, 1154, 589, 167, 510, 522, 226, 1164, 110, 346, 440, 576, 356, 222, 1012, 1200, 144, 590, 925, 949, 473, 457, 635, 983, 68, 54, 1139, 975, 1183, 200, 582, 347, 289, 868, 546, 42, 71, 270, 375, 285, 1205, 1071, 808, 984, 1107, 1040, 1136, 724, 884, 928, 559, 841, 825, 145, 689, 518, 948, 1051, 512, 1005, 19, 964, 1031, 527, 631, 629, 307, 237, 633, 533, 748, 277, 1129, 898, 752, 244, 70, 1122, 1080, 1172, 435, 348, 366, 835, 124]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8924187220196391
the save name prefix for this run is:  chkpt-ID_8924187220196391_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 579
rank avg (pred): 0.462 +- 0.005
mrr vals (pred, true): 0.000, 0.053
batch losses (mrrl, rdl): 0.0, 0.000299645

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 901
rank avg (pred): 0.413 +- 0.103
mrr vals (pred, true): 0.001, 0.015
batch losses (mrrl, rdl): 0.0, 0.0001960014

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 446
rank avg (pred): 0.388 +- 0.244
mrr vals (pred, true): 0.159, 0.001
batch losses (mrrl, rdl): 0.0, 0.000132968

Epoch over!
epoch time: 12.16

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 737
rank avg (pred): 0.227 +- 0.150
mrr vals (pred, true): 0.181, 0.001
batch losses (mrrl, rdl): 0.0, 0.000796978

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 855
rank avg (pred): 0.433 +- 0.311
mrr vals (pred, true): 0.203, 0.007
batch losses (mrrl, rdl): 0.0, 1.50204e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 285
rank avg (pred): 0.103 +- 0.076
mrr vals (pred, true): 0.227, 0.187
batch losses (mrrl, rdl): 0.0, 2.40583e-05

Epoch over!
epoch time: 12.037

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.389 +- 0.281
mrr vals (pred, true): 0.209, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001036679

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 896
rank avg (pred): 0.480 +- 0.337
mrr vals (pred, true): 0.188, 0.000
batch losses (mrrl, rdl): 0.0, 0.0014119002

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1185
rank avg (pred): 0.440 +- 0.305
mrr vals (pred, true): 0.188, 0.061
batch losses (mrrl, rdl): 0.0, 0.0002225297

Epoch over!
epoch time: 11.86

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 315
rank avg (pred): 0.085 +- 0.066
mrr vals (pred, true): 0.254, 0.238
batch losses (mrrl, rdl): 0.0, 3.55871e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 23
rank avg (pred): 0.098 +- 0.077
mrr vals (pred, true): 0.247, 0.307
batch losses (mrrl, rdl): 0.0, 3.28304e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 625
rank avg (pred): 0.406 +- 0.298
mrr vals (pred, true): 0.185, 0.053
batch losses (mrrl, rdl): 0.0, 8.2241e-05

Epoch over!
epoch time: 11.899

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 542
rank avg (pred): 0.236 +- 0.181
mrr vals (pred, true): 0.207, 0.147
batch losses (mrrl, rdl): 0.0, 6.22598e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 234
rank avg (pred): 0.399 +- 0.299
mrr vals (pred, true): 0.205, 0.002
batch losses (mrrl, rdl): 0.0, 8.04591e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 695
rank avg (pred): 0.385 +- 0.293
mrr vals (pred, true): 0.176, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001250362

Epoch over!
epoch time: 11.965

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.356 +- 0.281
mrr vals (pred, true): 0.189, 0.001
batch losses (mrrl, rdl): 0.1944973171, 0.0002479007

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 251
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.224, 0.286
batch losses (mrrl, rdl): 0.0380383469, 5.3866e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 264
rank avg (pred): 0.002 +- 0.002
mrr vals (pred, true): 0.271, 0.305
batch losses (mrrl, rdl): 0.0110529326, 5.84027e-05

Epoch over!
epoch time: 12.459

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1119
rank avg (pred): 0.416 +- 0.245
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0007170996, 8.5629e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 624
rank avg (pred): 0.423 +- 0.261
mrr vals (pred, true): 0.052, 0.052
batch losses (mrrl, rdl): 4.92554e-05, 0.000118692

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 625
rank avg (pred): 0.424 +- 0.278
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 4.82312e-05, 8.83776e-05

Epoch over!
epoch time: 12.277

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 704
rank avg (pred): 0.390 +- 0.281
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004939622, 0.0001142661

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.366 +- 0.263
mrr vals (pred, true): 0.057, 0.001
batch losses (mrrl, rdl): 0.0004940743, 0.0002686881

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 810
rank avg (pred): 0.389 +- 0.253
mrr vals (pred, true): 0.087, 0.035
batch losses (mrrl, rdl): 0.0137415631, 0.0015058185

Epoch over!
epoch time: 12.075

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1167
rank avg (pred): 0.399 +- 0.312
mrr vals (pred, true): 0.049, 0.074
batch losses (mrrl, rdl): 1.35201e-05, 4.5189e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 200
rank avg (pred): 0.415 +- 0.295
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.34587e-05, 5.90493e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 993
rank avg (pred): 0.172 +- 0.187
mrr vals (pred, true): 0.206, 0.224
batch losses (mrrl, rdl): 0.0034651628, 7.71508e-05

Epoch over!
epoch time: 12.059

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 76
rank avg (pred): 0.122 +- 0.158
mrr vals (pred, true): 0.215, 0.237
batch losses (mrrl, rdl): 0.0045381645, 1.24161e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 304
rank avg (pred): 0.217 +- 0.208
mrr vals (pred, true): 0.176, 0.151
batch losses (mrrl, rdl): 0.0064977282, 0.0002831687

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 836
rank avg (pred): 0.284 +- 0.241
mrr vals (pred, true): 0.184, 0.170
batch losses (mrrl, rdl): 0.0017836125, 0.0004594435

Epoch over!
epoch time: 12.165

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 973
rank avg (pred): 0.045 +- 0.110
mrr vals (pred, true): 0.293, 0.281
batch losses (mrrl, rdl): 0.0014106173, 2.05011e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 85
rank avg (pred): 0.385 +- 0.327
mrr vals (pred, true): 0.052, 0.067
batch losses (mrrl, rdl): 3.41169e-05, 2.65196e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 381
rank avg (pred): 0.437 +- 0.324
mrr vals (pred, true): 0.048, 0.041
batch losses (mrrl, rdl): 5.16553e-05, 0.0001301914

Epoch over!
epoch time: 12.233

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.179 +- 0.191
mrr vals (pred, true): 0.212, 0.212
batch losses (mrrl, rdl): 2.464e-07, 5.54562e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 728
rank avg (pred): 0.429 +- 0.317
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.01125e-05, 6.12147e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 362
rank avg (pred): 0.475 +- 0.351
mrr vals (pred, true): 0.046, 0.056
batch losses (mrrl, rdl): 0.0001749922, 0.000298744

Epoch over!
epoch time: 12.029

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 520
rank avg (pred): 0.264 +- 0.216
mrr vals (pred, true): 0.171, 0.139
batch losses (mrrl, rdl): 0.0106708594, 8.06448e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 625
rank avg (pred): 0.514 +- 0.301
mrr vals (pred, true): 0.039, 0.053
batch losses (mrrl, rdl): 0.0013046687, 0.0003696059

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 715
rank avg (pred): 0.403 +- 0.328
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.41701e-05, 0.0001224173

Epoch over!
epoch time: 11.985

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 559
rank avg (pred): 0.223 +- 0.194
mrr vals (pred, true): 0.201, 0.186
batch losses (mrrl, rdl): 0.0022023951, 4.25609e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 995
rank avg (pred): 0.179 +- 0.180
mrr vals (pred, true): 0.225, 0.217
batch losses (mrrl, rdl): 0.0005746359, 9.91306e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 235
rank avg (pred): 0.406 +- 0.350
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 8.6806e-06, 0.0001629564

Epoch over!
epoch time: 12.093

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 574
rank avg (pred): 0.408 +- 0.347
mrr vals (pred, true): 0.052, 0.054
batch losses (mrrl, rdl): 5.61279e-05, 4.21924e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 432
rank avg (pred): 0.430 +- 0.355
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001324773, 0.0001014963

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 516
rank avg (pred): 0.236 +- 0.197
mrr vals (pred, true): 0.132, 0.131
batch losses (mrrl, rdl): 7.5333e-06, 5.1255e-05

Epoch over!
epoch time: 12.197

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.372 +- 0.352
mrr vals (pred, true): 0.057, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   23 	     0 	 0.05351 	 0.00011 	 m..s
   25 	     1 	 0.05376 	 0.00012 	 m..s
   18 	     2 	 0.05313 	 0.00048 	 m..s
    2 	     3 	 0.05103 	 0.00049 	 m..s
   66 	     4 	 0.05601 	 0.00050 	 m..s
   53 	     5 	 0.05511 	 0.00051 	 m..s
   69 	     6 	 0.05702 	 0.00051 	 m..s
   55 	     7 	 0.05523 	 0.00052 	 m..s
   32 	     8 	 0.05426 	 0.00053 	 m..s
   69 	     9 	 0.05702 	 0.00055 	 m..s
   13 	    10 	 0.05281 	 0.00055 	 m..s
    5 	    11 	 0.05140 	 0.00057 	 m..s
    0 	    12 	 0.04784 	 0.00058 	 m..s
   62 	    13 	 0.05556 	 0.00059 	 m..s
   35 	    14 	 0.05434 	 0.00061 	 m..s
    9 	    15 	 0.05202 	 0.00062 	 m..s
   34 	    16 	 0.05431 	 0.00063 	 m..s
   46 	    17 	 0.05480 	 0.00063 	 m..s
   48 	    18 	 0.05494 	 0.00064 	 m..s
   69 	    19 	 0.05702 	 0.00064 	 m..s
   68 	    20 	 0.05677 	 0.00064 	 m..s
   69 	    21 	 0.05702 	 0.00066 	 m..s
   57 	    22 	 0.05527 	 0.00067 	 m..s
   60 	    23 	 0.05546 	 0.00073 	 m..s
   69 	    24 	 0.05702 	 0.00073 	 m..s
   69 	    25 	 0.05702 	 0.00075 	 m..s
   19 	    26 	 0.05316 	 0.00075 	 m..s
   17 	    27 	 0.05305 	 0.00075 	 m..s
   31 	    28 	 0.05423 	 0.00076 	 m..s
   56 	    29 	 0.05525 	 0.00077 	 m..s
   43 	    30 	 0.05458 	 0.00078 	 m..s
   27 	    31 	 0.05409 	 0.00078 	 m..s
   41 	    32 	 0.05450 	 0.00081 	 m..s
   65 	    33 	 0.05592 	 0.00081 	 m..s
   36 	    34 	 0.05435 	 0.00082 	 m..s
   69 	    35 	 0.05702 	 0.00083 	 m..s
   63 	    36 	 0.05558 	 0.00083 	 m..s
   40 	    37 	 0.05444 	 0.00087 	 m..s
    8 	    38 	 0.05152 	 0.00087 	 m..s
   12 	    39 	 0.05276 	 0.00087 	 m..s
   47 	    40 	 0.05492 	 0.00097 	 m..s
   51 	    41 	 0.05510 	 0.00098 	 m..s
   52 	    42 	 0.05511 	 0.00098 	 m..s
   58 	    43 	 0.05531 	 0.00101 	 m..s
   29 	    44 	 0.05414 	 0.00105 	 m..s
   69 	    45 	 0.05702 	 0.00109 	 m..s
   80 	    46 	 0.06086 	 0.00439 	 m..s
   28 	    47 	 0.05411 	 0.00545 	 m..s
   54 	    48 	 0.05522 	 0.00637 	 m..s
   49 	    49 	 0.05498 	 0.00752 	 m..s
   10 	    50 	 0.05234 	 0.00925 	 m..s
   69 	    51 	 0.05702 	 0.04001 	 ~...
   20 	    52 	 0.05320 	 0.04173 	 ~...
   24 	    53 	 0.05370 	 0.04381 	 ~...
   39 	    54 	 0.05442 	 0.04383 	 ~...
    1 	    55 	 0.05094 	 0.04825 	 ~...
   37 	    56 	 0.05438 	 0.04831 	 ~...
   15 	    57 	 0.05295 	 0.04955 	 ~...
   22 	    58 	 0.05350 	 0.05072 	 ~...
   64 	    59 	 0.05574 	 0.05103 	 ~...
   26 	    60 	 0.05408 	 0.05117 	 ~...
    7 	    61 	 0.05147 	 0.05194 	 ~...
   44 	    62 	 0.05460 	 0.05560 	 ~...
   69 	    63 	 0.05702 	 0.05653 	 ~...
    3 	    64 	 0.05117 	 0.05707 	 ~...
   59 	    65 	 0.05543 	 0.05763 	 ~...
   69 	    66 	 0.05702 	 0.05806 	 ~...
   67 	    67 	 0.05619 	 0.05828 	 ~...
    6 	    68 	 0.05141 	 0.05906 	 ~...
   38 	    69 	 0.05440 	 0.05965 	 ~...
   14 	    70 	 0.05285 	 0.05969 	 ~...
   33 	    71 	 0.05428 	 0.06187 	 ~...
   11 	    72 	 0.05274 	 0.06577 	 ~...
   16 	    73 	 0.05303 	 0.06622 	 ~...
   42 	    74 	 0.05457 	 0.06745 	 ~...
   45 	    75 	 0.05470 	 0.06748 	 ~...
   30 	    76 	 0.05416 	 0.06842 	 ~...
   21 	    77 	 0.05345 	 0.06992 	 ~...
    4 	    78 	 0.05132 	 0.07507 	 ~...
   50 	    79 	 0.05504 	 0.08255 	 ~...
   61 	    80 	 0.05551 	 0.08644 	 m..s
   81 	    81 	 0.06142 	 0.08904 	 ~...
   83 	    82 	 0.15974 	 0.12784 	 m..s
   85 	    83 	 0.16627 	 0.12934 	 m..s
   87 	    84 	 0.17002 	 0.12986 	 m..s
   90 	    85 	 0.17286 	 0.13270 	 m..s
   84 	    86 	 0.16289 	 0.13631 	 ~...
   88 	    87 	 0.17066 	 0.13668 	 m..s
   86 	    88 	 0.16999 	 0.13870 	 m..s
   82 	    89 	 0.15149 	 0.14670 	 ~...
   96 	    90 	 0.20020 	 0.15071 	 m..s
   91 	    91 	 0.17292 	 0.15221 	 ~...
   89 	    92 	 0.17251 	 0.15385 	 ~...
   97 	    93 	 0.20142 	 0.16042 	 m..s
   92 	    94 	 0.18921 	 0.16078 	 ~...
   98 	    95 	 0.20817 	 0.17630 	 m..s
  100 	    96 	 0.21068 	 0.18015 	 m..s
   94 	    97 	 0.19347 	 0.18582 	 ~...
   95 	    98 	 0.19370 	 0.18671 	 ~...
   93 	    99 	 0.19187 	 0.18721 	 ~...
  103 	   100 	 0.21964 	 0.19829 	 ~...
   99 	   101 	 0.20844 	 0.20773 	 ~...
  108 	   102 	 0.24659 	 0.21567 	 m..s
  107 	   103 	 0.24588 	 0.21801 	 ~...
  106 	   104 	 0.23171 	 0.22049 	 ~...
  102 	   105 	 0.21698 	 0.22069 	 ~...
  101 	   106 	 0.21437 	 0.22141 	 ~...
  105 	   107 	 0.22690 	 0.22712 	 ~...
  110 	   108 	 0.25437 	 0.23206 	 ~...
  111 	   109 	 0.25487 	 0.23475 	 ~...
  104 	   110 	 0.22638 	 0.23521 	 ~...
  109 	   111 	 0.25411 	 0.25900 	 ~...
  112 	   112 	 0.26297 	 0.26014 	 ~...
  113 	   113 	 0.27563 	 0.26576 	 ~...
  114 	   114 	 0.27574 	 0.27022 	 ~...
  116 	   115 	 0.28977 	 0.29885 	 ~...
  115 	   116 	 0.28869 	 0.30014 	 ~...
  117 	   117 	 0.29131 	 0.30308 	 ~...
  119 	   118 	 0.30014 	 0.30605 	 ~...
  118 	   119 	 0.29538 	 0.30750 	 ~...
  120 	   120 	 0.30155 	 0.30759 	 ~...
==========================================
r_mrr = 0.9627620577812195
r2_mrr = 0.843565046787262
spearmanr_mrr@5 = 0.9433903098106384
spearmanr_mrr@10 = 0.9548988938331604
spearmanr_mrr@50 = 0.9736641049385071
spearmanr_mrr@100 = 0.969314694404602
spearmanr_mrr@All = 0.969488799571991
==========================================
test time: 0.392
Done Testing dataset OpenEA
total time taken: 197.14197206497192
training time taken: 181.95503497123718
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9628)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8436)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9434)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9549)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9737)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9693)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9695)}}, 'test_loss': {'DistMult': {'OpenEA': 0.2524330507585546}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 248854857392949
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1211, 1039, 1024, 1045, 346, 868, 1178, 486, 972, 559, 1005, 586, 345, 910, 240, 697, 1016, 473, 276, 544, 785, 923, 712, 422, 1091, 851, 191, 1193, 284, 156, 135, 1046, 788, 74, 857, 1190, 214, 650, 930, 85, 1022, 321, 366, 328, 37, 509, 558, 954, 1185, 208, 1047, 980, 1172, 225, 455, 695, 897, 320, 822, 1173, 853, 836, 304, 1195, 978, 783, 905, 453, 1034, 1114, 1101, 431, 901, 16, 1062, 1199, 1187, 298, 67, 680, 1205, 247, 339, 909, 57, 1023, 464, 59, 663, 975, 160, 662, 230, 55, 125, 743, 89, 1127, 1036, 831, 1021, 1043, 928, 686, 382, 1017, 1037, 361, 893, 171, 234, 206, 931, 823, 102, 362, 844, 1176, 1029, 1133, 848]
valid_ids (0): []
train_ids (1094): [229, 974, 802, 1140, 572, 766, 643, 891, 161, 46, 621, 35, 651, 342, 1074, 392, 1106, 1111, 458, 405, 219, 237, 1166, 879, 541, 854, 424, 23, 878, 472, 68, 791, 270, 924, 1148, 301, 294, 657, 18, 114, 1104, 146, 614, 71, 988, 251, 306, 113, 981, 62, 120, 631, 952, 263, 871, 655, 1159, 497, 772, 505, 460, 122, 1000, 86, 757, 996, 810, 1164, 186, 211, 202, 52, 1038, 1009, 606, 282, 167, 212, 319, 790, 737, 491, 373, 428, 725, 636, 1081, 136, 646, 872, 805, 729, 259, 960, 986, 1060, 1150, 384, 956, 970, 1126, 1213, 1189, 1082, 898, 756, 637, 1174, 285, 203, 222, 1144, 951, 730, 439, 530, 959, 581, 529, 315, 65, 1085, 358, 163, 781, 717, 661, 1069, 798, 40, 91, 469, 900, 421, 476, 704, 1160, 927, 890, 1063, 579, 850, 554, 795, 387, 971, 832, 0, 538, 42, 170, 95, 312, 349, 552, 946, 88, 393, 17, 296, 451, 741, 1117, 13, 847, 773, 876, 999, 896, 243, 417, 326, 1033, 944, 1067, 112, 934, 261, 1068, 1151, 852, 1170, 198, 1026, 985, 779, 565, 82, 601, 624, 875, 1027, 882, 334, 410, 752, 919, 61, 484, 111, 258, 808, 322, 341, 1025, 533, 204, 815, 244, 97, 1186, 997, 443, 188, 1102, 1110, 696, 286, 1049, 197, 589, 327, 635, 706, 902, 1059, 765, 1012, 277, 300, 1157, 682, 653, 1184, 746, 892, 1162, 31, 49, 307, 673, 96, 169, 827, 11, 659, 448, 691, 152, 364, 351, 649, 209, 271, 668, 739, 1212, 465, 1080, 1115, 179, 196, 137, 175, 652, 1031, 224, 47, 1096, 543, 1073, 189, 913, 354, 218, 969, 545, 596, 837, 709, 758, 498, 964, 488, 1154, 724, 966, 426, 54, 516, 1077, 15, 512, 19, 908, 1145, 178, 1122, 1105, 866, 671, 679, 1057, 575, 518, 616, 314, 423, 53, 444, 246, 76, 926, 963, 627, 1015, 289, 618, 1138, 1194, 904, 714, 887, 602, 45, 1191, 733, 845, 69, 195, 703, 1055, 140, 941, 654, 1072, 744, 490, 12, 800, 429, 28, 479, 617, 493, 953, 674, 480, 993, 22, 861, 990, 563, 194, 690, 399, 94, 1208, 1196, 587, 440, 406, 705, 580, 401, 1130, 1135, 478, 1088, 1116, 1070, 979, 281, 310, 80, 408, 1093, 846, 1206, 711, 376, 355, 98, 1120, 600, 238, 75, 605, 820, 461, 58, 819, 998, 26, 797, 93, 275, 403, 590, 814, 502, 976, 867, 522, 534, 164, 1078, 736, 60, 269, 1134, 253, 553, 430, 692, 681, 159, 548, 666, 1201, 391, 266, 531, 1066, 367, 172, 803, 526, 597, 1006, 335, 279, 557, 475, 368, 254, 940, 442, 914, 1020, 4, 574, 571, 105, 449, 148, 760, 241, 1041, 825, 1058, 280, 436, 166, 267, 1198, 829, 777, 684, 492, 297, 347, 402, 268, 1108, 793, 119, 48, 495, 496, 916, 374, 799, 1139, 27, 100, 955, 948, 239, 157, 344, 221, 231, 824, 363, 457, 155, 912, 886, 834, 1032, 1147, 371, 216, 332, 1118, 701, 550, 720, 1042, 745, 644, 419, 883, 806, 626, 794, 818, 884, 1103, 1065, 713, 353, 380, 710, 459, 856, 379, 608, 1119, 860, 501, 201, 456, 412, 523, 400, 610, 787, 607, 283, 370, 950, 865, 642, 639, 915, 356, 217, 468, 911, 200, 1136, 1094, 228, 1050, 1207, 124, 386, 775, 427, 292, 467, 1052, 994, 6, 1188, 8, 336, 716, 107, 390, 132, 835, 1149, 513, 236, 984, 641, 205, 761, 540, 176, 1123, 907, 723, 769, 658, 1087, 450, 973, 369, 29, 1161, 255, 1203, 735, 570, 32, 123, 629, 595, 982, 311, 957, 942, 562, 1053, 149, 1141, 708, 72, 1125, 154, 598, 308, 1056, 564, 372, 508, 784, 903, 360, 535, 287, 707, 1167, 494, 1156, 129, 647, 774, 759, 992, 260, 862, 226, 256, 1181, 223, 732, 1097, 395, 265, 1177, 1202, 833, 471, 274, 25, 1044, 740, 133, 210, 435, 329, 546, 452, 323, 539, 619, 889, 511, 1210, 117, 551, 66, 165, 375, 573, 536, 937, 388, 858, 676, 1165, 1197, 193, 965, 1183, 764, 272, 77, 173, 1086, 989, 295, 183, 1048, 299, 935, 1040, 63, 507, 863, 325, 670, 1168, 1018, 357, 625, 525, 843, 1004, 1002, 762, 138, 313, 103, 87, 1095, 153, 925, 591, 252, 192, 667, 394, 750, 151, 660, 5, 343, 599, 1214, 447, 1146, 318, 288, 104, 524, 411, 1008, 877, 177, 592, 36, 232, 885, 477, 1152, 213, 583, 144, 184, 734, 398, 264, 542, 101, 273, 338, 78, 895, 630, 73, 638, 1099, 977, 83, 51, 420, 1142, 888, 180, 500, 330, 1011, 147, 532, 1175, 121, 807, 90, 727, 849, 1155, 109, 547, 499, 503, 1171, 162, 881, 770, 158, 812, 1131, 967, 612, 816, 1019, 10, 396, 632, 126, 434, 813, 728, 1137, 1132, 39, 142, 1054, 463, 672, 894, 958, 1010, 873, 350, 482, 418, 483, 932, 506, 567, 220, 1128, 613, 722, 38, 603, 921, 995, 859, 731, 207, 1013, 1180, 754, 316, 584, 576, 1100, 780, 317, 1169, 1076, 604, 947, 675, 792, 474, 487, 1179, 1153, 755, 445, 811, 520, 748, 1192, 1084, 839, 446, 1089, 470, 1003, 917, 593, 648, 749, 181, 702, 880, 9, 961, 233, 519, 56, 462, 389, 348, 21, 383, 687, 128, 929, 409, 830, 416, 782, 767, 385, 127, 768, 987, 840, 1075, 585, 141, 397, 415, 333, 801, 577, 182, 809, 106, 1007, 771, 485, 249, 309, 116, 582, 262, 293, 664, 555, 185, 517, 804, 1035, 1083, 786, 70, 174, 1121, 1071, 404, 425, 763, 215, 568, 337, 1143, 150, 1051, 821, 623, 1124, 1079, 870, 515, 3, 700, 527, 227, 1028, 139, 1113, 906, 826, 1200, 698, 685, 939, 751, 689, 110, 556, 145, 566, 1129, 694, 44, 441, 1092, 79, 968, 413, 528, 433, 874, 1163, 796, 699, 1014, 949, 290, 628, 715, 721, 719, 108, 1001, 432, 678, 1109, 352, 594, 41, 250, 521, 438, 377, 381, 1064, 115, 1158, 669, 359, 130, 936, 869, 168, 726, 34, 609, 828, 945, 842, 1112, 278, 753, 560, 131, 365, 134, 1090, 578, 645, 7, 688, 81, 64, 437, 143, 569, 841, 615, 738, 1107, 303, 991, 43, 331, 864, 30, 481, 778, 305, 84, 665, 199, 33, 257, 414, 24, 245, 99, 187, 611, 817, 933, 838, 489, 789, 633, 1061, 855, 983, 190, 378, 514, 504, 454, 537, 291, 918, 588, 747, 302, 943, 340, 634, 920, 693, 50, 922, 324, 1204, 640, 656, 1182, 510, 407, 677, 118, 92, 20, 248, 14, 938, 242, 620, 1098, 1, 1030, 235, 899, 2, 742, 466, 962, 1209, 622, 549, 561, 683, 718, 776]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6506170420749743
the save name prefix for this run is:  chkpt-ID_6506170420749743_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 350
rank avg (pred): 0.554 +- 0.003
mrr vals (pred, true): 0.000, 0.063
batch losses (mrrl, rdl): 0.0, 0.0010804718

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 684
rank avg (pred): 0.426 +- 0.256
mrr vals (pred, true): 0.135, 0.001
batch losses (mrrl, rdl): 0.0, 4.41206e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 732
rank avg (pred): 0.246 +- 0.169
mrr vals (pred, true): 0.218, 0.007
batch losses (mrrl, rdl): 0.0, 4.29397e-05

Epoch over!
epoch time: 12.267

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1074
rank avg (pred): 0.061 +- 0.045
mrr vals (pred, true): 0.268, 0.239
batch losses (mrrl, rdl): 0.0, 7.82132e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 267
rank avg (pred): 0.084 +- 0.062
mrr vals (pred, true): 0.268, 0.303
batch losses (mrrl, rdl): 0.0, 3.15218e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1078
rank avg (pred): 0.073 +- 0.052
mrr vals (pred, true): 0.250, 0.269
batch losses (mrrl, rdl): 0.0, 5.39319e-05

Epoch over!
epoch time: 11.76

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 248
rank avg (pred): 0.053 +- 0.039
mrr vals (pred, true): 0.262, 0.278
batch losses (mrrl, rdl): 0.0, 1.01298e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 517
rank avg (pred): 0.226 +- 0.173
mrr vals (pred, true): 0.230, 0.140
batch losses (mrrl, rdl): 0.0, 6.43688e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 688
rank avg (pred): 0.395 +- 0.291
mrr vals (pred, true): 0.208, 0.001
batch losses (mrrl, rdl): 0.0, 0.000109906

Epoch over!
epoch time: 11.755

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 375
rank avg (pred): 0.365 +- 0.295
mrr vals (pred, true): 0.244, 0.064
batch losses (mrrl, rdl): 0.0, 6.04495e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 780
rank avg (pred): 0.506 +- 0.323
mrr vals (pred, true): 0.188, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001798627

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 409
rank avg (pred): 0.369 +- 0.302
mrr vals (pred, true): 0.230, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001853496

Epoch over!
epoch time: 11.819

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 920
rank avg (pred): 0.508 +- 0.314
mrr vals (pred, true): 0.168, 0.004
batch losses (mrrl, rdl): 0.0, 2.92619e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 185
rank avg (pred): 0.363 +- 0.296
mrr vals (pred, true): 0.205, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001879677

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 311
rank avg (pred): 0.086 +- 0.075
mrr vals (pred, true): 0.228, 0.238
batch losses (mrrl, rdl): 0.0, 2.58615e-05

Epoch over!
epoch time: 12.067

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 374
rank avg (pred): 0.430 +- 0.311
mrr vals (pred, true): 0.135, 0.056
batch losses (mrrl, rdl): 0.0714615881, 0.0002304616

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 671
rank avg (pred): 0.513 +- 0.244
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 6.63743e-05, 3.74263e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 33
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.194, 0.141
batch losses (mrrl, rdl): 0.0273094531, 0.0003662266

Epoch over!
epoch time: 12.323

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.408 +- 0.243
mrr vals (pred, true): 0.060, 0.068
batch losses (mrrl, rdl): 0.0010935948, 0.0001610575

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 803
rank avg (pred): 0.477 +- 0.249
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002641002, 1.387e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 398
rank avg (pred): 0.410 +- 0.251
mrr vals (pred, true): 0.055, 0.067
batch losses (mrrl, rdl): 0.0002602524, 0.0001723583

Epoch over!
epoch time: 12.146

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 814
rank avg (pred): 0.275 +- 0.198
mrr vals (pred, true): 0.062, 0.010
batch losses (mrrl, rdl): 0.0014535945, 6.58775e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 565
rank avg (pred): 0.035 +- 0.036
mrr vals (pred, true): 0.200, 0.183
batch losses (mrrl, rdl): 0.0028834948, 0.0009109997

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 756
rank avg (pred): 0.551 +- 0.292
mrr vals (pred, true): 0.041, 0.002
batch losses (mrrl, rdl): 0.0008453364, 0.0001120304

Epoch over!
epoch time: 12.173

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1192
rank avg (pred): 0.467 +- 0.294
mrr vals (pred, true): 0.061, 0.001
batch losses (mrrl, rdl): 0.0012950038, 6.7234e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1140
rank avg (pred): 0.066 +- 0.096
mrr vals (pred, true): 0.277, 0.262
batch losses (mrrl, rdl): 0.0022493969, 0.0003991557

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 142
rank avg (pred): 0.426 +- 0.324
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 0.0001146561, 0.0001001462

Epoch over!
epoch time: 12.263

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 64
rank avg (pred): 0.197 +- 0.236
mrr vals (pred, true): 0.164, 0.203
batch losses (mrrl, rdl): 0.0151605643, 0.0001713805

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 702
rank avg (pred): 0.426 +- 0.324
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.19873e-05, 6.06241e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 704
rank avg (pred): 0.421 +- 0.323
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.000142748, 4.9712e-05

Epoch over!
epoch time: 12.022

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 850
rank avg (pred): 0.512 +- 0.279
mrr vals (pred, true): 0.044, 0.003
batch losses (mrrl, rdl): 0.0003407249, 1.05811e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 150
rank avg (pred): 0.421 +- 0.316
mrr vals (pred, true): 0.058, 0.061
batch losses (mrrl, rdl): 0.0006026032, 0.0001687735

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 310
rank avg (pred): 0.167 +- 0.226
mrr vals (pred, true): 0.183, 0.221
batch losses (mrrl, rdl): 0.01490328, 0.0001275384

Epoch over!
epoch time: 11.969

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 285
rank avg (pred): 0.151 +- 0.211
mrr vals (pred, true): 0.206, 0.187
batch losses (mrrl, rdl): 0.0038571083, 7.14408e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 221
rank avg (pred): 0.447 +- 0.293
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001820032, 2.61883e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 281
rank avg (pred): 0.174 +- 0.238
mrr vals (pred, true): 0.208, 0.235
batch losses (mrrl, rdl): 0.0072222739, 0.0001654133

Epoch over!
epoch time: 12.093

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 865
rank avg (pred): 0.515 +- 0.295
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001841487, 5.18374e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 915
rank avg (pred): 0.482 +- 0.287
mrr vals (pred, true): 0.060, 0.004
batch losses (mrrl, rdl): 0.0009965627, 0.0001604145

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 499
rank avg (pred): 0.100 +- 0.170
mrr vals (pred, true): 0.255, 0.268
batch losses (mrrl, rdl): 0.0017095937, 0.0001893975

Epoch over!
epoch time: 12.025

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1059
rank avg (pred): 0.019 +- 0.042
mrr vals (pred, true): 0.325, 0.330
batch losses (mrrl, rdl): 0.0001943022, 6.51163e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 410
rank avg (pred): 0.428 +- 0.289
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.000156936, 4.83505e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 219
rank avg (pred): 0.447 +- 0.275
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.81675e-05, 2.6118e-05

Epoch over!
epoch time: 12.02

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.122 +- 0.184
mrr vals (pred, true): 0.204, 0.186
batch losses (mrrl, rdl): 0.0032236991, 0.0002115565

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 86
rank avg (pred): 0.443 +- 0.272
mrr vals (pred, true): 0.044, 0.048
batch losses (mrrl, rdl): 0.0003203414, 0.000152501

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.414 +- 0.276
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 9.71e-08, 7.67092e-05

Epoch over!
epoch time: 11.996

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.447 +- 0.271
mrr vals (pred, true): 0.047, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.04631 	 8e-0500 	 m..s
   34 	     1 	 0.04856 	 0.00010 	 m..s
   17 	     2 	 0.04618 	 0.00011 	 m..s
   18 	     3 	 0.04626 	 0.00034 	 m..s
    1 	     4 	 0.04180 	 0.00049 	 m..s
   36 	     5 	 0.04905 	 0.00050 	 m..s
   36 	     6 	 0.04905 	 0.00051 	 m..s
   36 	     7 	 0.04905 	 0.00052 	 m..s
   31 	     8 	 0.04830 	 0.00055 	 m..s
   23 	     9 	 0.04707 	 0.00055 	 m..s
   82 	    10 	 0.05215 	 0.00055 	 m..s
   36 	    11 	 0.04905 	 0.00056 	 m..s
   36 	    12 	 0.04905 	 0.00056 	 m..s
   36 	    13 	 0.04905 	 0.00056 	 m..s
   36 	    14 	 0.04905 	 0.00057 	 m..s
    0 	    15 	 0.04072 	 0.00058 	 m..s
   84 	    16 	 0.05377 	 0.00058 	 m..s
   36 	    17 	 0.04905 	 0.00059 	 m..s
   36 	    18 	 0.04905 	 0.00060 	 m..s
   67 	    19 	 0.04917 	 0.00060 	 m..s
   36 	    20 	 0.04905 	 0.00061 	 m..s
   14 	    21 	 0.04597 	 0.00061 	 m..s
    3 	    22 	 0.04229 	 0.00064 	 m..s
   36 	    23 	 0.04905 	 0.00064 	 m..s
   35 	    24 	 0.04876 	 0.00065 	 m..s
   76 	    25 	 0.05051 	 0.00065 	 m..s
    2 	    26 	 0.04209 	 0.00065 	 m..s
   89 	    27 	 0.05429 	 0.00066 	 m..s
   22 	    28 	 0.04702 	 0.00067 	 m..s
   81 	    29 	 0.05205 	 0.00068 	 m..s
   30 	    30 	 0.04824 	 0.00068 	 m..s
   71 	    31 	 0.04970 	 0.00068 	 m..s
   36 	    32 	 0.04905 	 0.00069 	 m..s
   69 	    33 	 0.04960 	 0.00070 	 m..s
   72 	    34 	 0.05009 	 0.00070 	 m..s
   87 	    35 	 0.05398 	 0.00070 	 m..s
   36 	    36 	 0.04905 	 0.00073 	 m..s
   16 	    37 	 0.04617 	 0.00073 	 m..s
   36 	    38 	 0.04905 	 0.00073 	 m..s
   36 	    39 	 0.04905 	 0.00074 	 m..s
   36 	    40 	 0.04905 	 0.00076 	 m..s
    6 	    41 	 0.04514 	 0.00077 	 m..s
   80 	    42 	 0.05177 	 0.00082 	 m..s
   36 	    43 	 0.04905 	 0.00084 	 m..s
   36 	    44 	 0.04905 	 0.00084 	 m..s
   27 	    45 	 0.04749 	 0.00085 	 m..s
    4 	    46 	 0.04252 	 0.00086 	 m..s
   90 	    47 	 0.05442 	 0.00087 	 m..s
   75 	    48 	 0.05040 	 0.00087 	 m..s
   29 	    49 	 0.04785 	 0.00090 	 m..s
    5 	    50 	 0.04343 	 0.00108 	 m..s
   36 	    51 	 0.04905 	 0.00171 	 m..s
   24 	    52 	 0.04713 	 0.00255 	 m..s
   11 	    53 	 0.04568 	 0.00294 	 m..s
   15 	    54 	 0.04608 	 0.00475 	 m..s
   10 	    55 	 0.04551 	 0.00589 	 m..s
    9 	    56 	 0.04549 	 0.00628 	 m..s
   83 	    57 	 0.05243 	 0.00925 	 m..s
   85 	    58 	 0.05387 	 0.01507 	 m..s
   88 	    59 	 0.05416 	 0.01708 	 m..s
   91 	    60 	 0.05579 	 0.01958 	 m..s
   36 	    61 	 0.04905 	 0.04001 	 ~...
   28 	    62 	 0.04760 	 0.04436 	 ~...
   36 	    63 	 0.04905 	 0.04869 	 ~...
   36 	    64 	 0.04905 	 0.05157 	 ~...
   36 	    65 	 0.04905 	 0.05199 	 ~...
   25 	    66 	 0.04734 	 0.05221 	 ~...
   26 	    67 	 0.04748 	 0.05378 	 ~...
   36 	    68 	 0.04905 	 0.05563 	 ~...
   33 	    69 	 0.04847 	 0.05564 	 ~...
   36 	    70 	 0.04905 	 0.05574 	 ~...
   68 	    71 	 0.04956 	 0.05643 	 ~...
   36 	    72 	 0.04905 	 0.05778 	 ~...
    8 	    73 	 0.04520 	 0.05884 	 ~...
   12 	    74 	 0.04580 	 0.05893 	 ~...
   13 	    75 	 0.04586 	 0.06111 	 ~...
   36 	    76 	 0.04905 	 0.06298 	 ~...
   32 	    77 	 0.04846 	 0.06305 	 ~...
   36 	    78 	 0.04905 	 0.06358 	 ~...
   36 	    79 	 0.04905 	 0.06524 	 ~...
   36 	    80 	 0.04905 	 0.06559 	 ~...
   21 	    81 	 0.04695 	 0.06711 	 ~...
   36 	    82 	 0.04905 	 0.06752 	 ~...
   20 	    83 	 0.04648 	 0.06786 	 ~...
   74 	    84 	 0.05024 	 0.07004 	 ~...
   77 	    85 	 0.05084 	 0.07139 	 ~...
   79 	    86 	 0.05135 	 0.07507 	 ~...
   70 	    87 	 0.04965 	 0.07587 	 ~...
   73 	    88 	 0.05018 	 0.07606 	 ~...
   86 	    89 	 0.05394 	 0.07720 	 ~...
    7 	    90 	 0.04517 	 0.07722 	 m..s
   78 	    91 	 0.05085 	 0.07800 	 ~...
   92 	    92 	 0.13158 	 0.12986 	 ~...
   97 	    93 	 0.16092 	 0.13420 	 ~...
  101 	    94 	 0.17543 	 0.14548 	 ~...
   99 	    95 	 0.17123 	 0.14811 	 ~...
   98 	    96 	 0.16329 	 0.15026 	 ~...
  104 	    97 	 0.17895 	 0.15093 	 ~...
  100 	    98 	 0.17384 	 0.15246 	 ~...
   93 	    99 	 0.14783 	 0.15777 	 ~...
   94 	   100 	 0.14862 	 0.15880 	 ~...
  105 	   101 	 0.18412 	 0.16162 	 ~...
  107 	   102 	 0.19155 	 0.17047 	 ~...
   96 	   103 	 0.15131 	 0.17952 	 ~...
  112 	   104 	 0.24604 	 0.18082 	 m..s
  106 	   105 	 0.18978 	 0.18566 	 ~...
   95 	   106 	 0.14937 	 0.19016 	 m..s
  103 	   107 	 0.17834 	 0.20696 	 ~...
  109 	   108 	 0.20917 	 0.21315 	 ~...
  110 	   109 	 0.23976 	 0.22435 	 ~...
  102 	   110 	 0.17716 	 0.22454 	 m..s
  108 	   111 	 0.19507 	 0.23907 	 m..s
  113 	   112 	 0.25235 	 0.24977 	 ~...
  111 	   113 	 0.24183 	 0.25221 	 ~...
  116 	   114 	 0.28396 	 0.26974 	 ~...
  117 	   115 	 0.28535 	 0.27623 	 ~...
  114 	   116 	 0.27151 	 0.28408 	 ~...
  115 	   117 	 0.27759 	 0.29748 	 ~...
  118 	   118 	 0.28785 	 0.30616 	 ~...
  119 	   119 	 0.31408 	 0.30764 	 ~...
  120 	   120 	 0.32050 	 0.31837 	 ~...
==========================================
r_mrr = 0.9464740753173828
r2_mrr = 0.8311481475830078
spearmanr_mrr@5 = 0.8081927299499512
spearmanr_mrr@10 = 0.9608906507492065
spearmanr_mrr@50 = 0.9933626055717468
spearmanr_mrr@100 = 0.9626641273498535
spearmanr_mrr@All = 0.9625130891799927
==========================================
test time: 0.405
Done Testing dataset OpenEA
total time taken: 196.63783526420593
training time taken: 181.1785912513733
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9465)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8311)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.8082)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9609)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9934)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9627)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9625)}}, 'test_loss': {'DistMult': {'OpenEA': 0.20749161362527957}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 9809430085648068
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [249, 567, 568, 165, 236, 294, 940, 534, 43, 700, 200, 517, 539, 119, 379, 774, 266, 887, 521, 665, 53, 506, 873, 735, 1199, 1136, 880, 608, 911, 1200, 614, 76, 704, 894, 697, 746, 317, 729, 215, 275, 351, 59, 221, 947, 487, 394, 852, 392, 913, 158, 995, 636, 813, 960, 855, 1127, 677, 457, 789, 1018, 1207, 364, 599, 450, 761, 384, 844, 734, 1056, 966, 156, 357, 603, 826, 1082, 622, 57, 554, 580, 587, 346, 1196, 792, 747, 50, 99, 1094, 128, 621, 646, 917, 155, 1083, 935, 522, 615, 129, 1179, 850, 338, 739, 589, 388, 270, 444, 1081, 27, 254, 975, 963, 1057, 293, 879, 716, 1051, 724, 1000, 967, 14, 781, 39]
valid_ids (0): []
train_ids (1094): [1151, 430, 407, 989, 121, 252, 153, 73, 297, 369, 548, 387, 1111, 992, 328, 222, 157, 721, 843, 173, 490, 47, 1016, 949, 332, 380, 479, 1053, 1166, 519, 762, 1201, 1180, 478, 647, 1108, 118, 1078, 285, 202, 600, 1144, 659, 37, 930, 552, 780, 799, 1045, 239, 234, 1080, 657, 987, 886, 642, 45, 2, 835, 964, 494, 1097, 296, 415, 562, 709, 1021, 925, 997, 1107, 115, 612, 637, 923, 102, 448, 232, 1152, 149, 514, 341, 485, 311, 756, 620, 310, 5, 891, 134, 556, 907, 447, 48, 1154, 518, 875, 1150, 453, 764, 49, 899, 1091, 982, 1187, 1100, 1104, 861, 77, 948, 837, 1041, 1213, 177, 131, 515, 922, 19, 183, 101, 1121, 722, 1113, 810, 24, 736, 182, 359, 313, 206, 1023, 83, 1062, 595, 7, 312, 512, 993, 974, 968, 730, 1095, 634, 8, 315, 1146, 1070, 15, 306, 97, 573, 1020, 924, 1172, 190, 753, 605, 830, 921, 360, 566, 122, 141, 1074, 145, 1130, 583, 890, 290, 148, 495, 941, 508, 240, 154, 1208, 927, 784, 247, 928, 288, 1131, 976, 585, 1064, 832, 225, 1015, 21, 817, 794, 733, 216, 609, 897, 1055, 65, 439, 814, 62, 878, 866, 405, 1077, 549, 1, 708, 523, 606, 871, 1126, 1158, 1147, 227, 208, 300, 1120, 51, 815, 498, 641, 624, 117, 644, 318, 1156, 824, 591, 1195, 936, 1177, 691, 81, 827, 427, 1181, 1068, 888, 437, 586, 96, 1167, 682, 1106, 1205, 12, 147, 648, 785, 330, 749, 1197, 807, 31, 867, 320, 616, 668, 418, 468, 203, 868, 623, 434, 858, 180, 262, 795, 325, 1099, 314, 191, 55, 36, 1019, 272, 486, 802, 1139, 1191, 651, 488, 130, 212, 725, 1138, 425, 1079, 416, 771, 667, 841, 933, 454, 267, 500, 424, 419, 1149, 782, 493, 1001, 981, 653, 635, 957, 461, 1093, 1036, 932, 503, 678, 451, 712, 983, 466, 675, 470, 399, 582, 805, 342, 806, 86, 366, 1028, 449, 618, 446, 740, 885, 251, 669, 690, 139, 825, 819, 662, 1109, 125, 223, 271, 1098, 323, 302, 1054, 198, 391, 246, 598, 322, 1185, 398, 590, 304, 741, 1049, 1164, 538, 1034, 100, 643, 769, 18, 475, 95, 260, 187, 69, 853, 919, 463, 631, 576, 596, 776, 686, 671, 1115, 250, 905, 570, 146, 319, 816, 1169, 460, 396, 483, 1141, 151, 854, 607, 788, 869, 934, 41, 462, 1159, 1198, 877, 684, 710, 1037, 492, 417, 383, 38, 26, 803, 13, 990, 632, 979, 337, 564, 1044, 344, 847, 195, 783, 1137, 1162, 143, 1148, 501, 986, 32, 652, 527, 228, 1046, 939, 857, 279, 253, 985, 496, 105, 124, 674, 565, 1008, 348, 88, 625, 166, 135, 352, 908, 71, 779, 541, 104, 718, 680, 574, 1178, 1043, 1184, 358, 353, 220, 1029, 1006, 1135, 1076, 70, 482, 1087, 382, 1088, 244, 343, 201, 1048, 859, 1063, 876, 308, 441, 849, 355, 257, 906, 276, 1212, 980, 531, 560, 577, 409, 658, 80, 588, 563, 903, 679, 796, 142, 703, 443, 263, 9, 823, 1165, 289, 828, 970, 772, 414, 973, 640, 248, 1209, 836, 112, 1174, 773, 1168, 433, 58, 426, 1030, 110, 901, 687, 1124, 75, 10, 422, 602, 445, 918, 797, 209, 1203, 367, 601, 333, 186, 1142, 1125, 683, 61, 532, 743, 85, 305, 649, 472, 84, 1122, 196, 282, 1005, 261, 133, 831, 1007, 731, 386, 786, 277, 1129, 862, 713, 507, 1105, 412, 403, 226, 865, 1173, 726, 915, 714, 1112, 952, 243, 459, 159, 1022, 872, 1089, 1133, 988, 1110, 889, 1160, 268, 555, 25, 695, 1072, 860, 238, 1067, 745, 1175, 511, 959, 287, 693, 17, 804, 309, 611, 1014, 1052, 845, 1059, 720, 916, 404, 1123, 170, 469, 29, 1075, 224, 1002, 120, 1204, 787, 584, 702, 1040, 301, 1101, 1118, 630, 480, 281, 955, 1186, 937, 210, 676, 1193, 233, 594, 345, 23, 619, 1092, 856, 535, 497, 701, 1206, 558, 72, 436, 1011, 553, 758, 663, 870, 1116, 401, 321, 484, 881, 798, 231, 999, 64, 286, 950, 1214, 349, 205, 138, 1170, 91, 20, 628, 520, 280, 529, 1004, 207, 163, 274, 654, 370, 476, 1060, 1143, 800, 1153, 748, 473, 1024, 1009, 193, 229, 400, 898, 93, 471, 356, 406, 1066, 1031, 579, 108, 113, 895, 435, 965, 1017, 411, 971, 42, 372, 874, 811, 181, 1065, 499, 1190, 126, 1038, 11, 46, 1096, 481, 884, 793, 362, 197, 516, 160, 35, 543, 1183, 984, 581, 6, 455, 4, 199, 1119, 269, 218, 298, 944, 502, 705, 892, 1071, 820, 336, 350, 528, 597, 1032, 685, 375, 1188, 775, 996, 883, 1102, 510, 78, 1010, 1134, 335, 339, 303, 385, 1171, 540, 474, 738, 509, 211, 592, 242, 1176, 699, 137, 395, 1042, 63, 910, 1035, 397, 402, 381, 452, 732, 389, 467, 327, 464, 1058, 377, 777, 109, 938, 1085, 168, 882, 757, 28, 575, 245, 465, 1189, 978, 82, 833, 299, 421, 132, 1117, 89, 52, 778, 744, 442, 767, 656, 639, 283, 544, 106, 692, 750, 954, 264, 1114, 1061, 572, 54, 140, 256, 161, 696, 929, 22, 66, 571, 67, 525, 295, 284, 90, 423, 533, 962, 1025, 629, 664, 74, 16, 670, 926, 34, 961, 956, 127, 645, 920, 638, 904, 909, 144, 331, 545, 717, 505, 136, 742, 537, 791, 324, 185, 896, 536, 184, 162, 361, 1145, 1050, 863, 1155, 661, 846, 1047, 840, 707, 334, 900, 103, 998, 851, 189, 107, 491, 754, 408, 613, 44, 340, 194, 390, 1026, 839, 672, 204, 688, 719, 994, 550, 291, 1013, 546, 1012, 728, 801, 326, 255, 706, 977, 68, 347, 1073, 114, 376, 542, 969, 278, 374, 116, 650, 834, 912, 169, 593, 1140, 373, 92, 413, 838, 1033, 40, 259, 829, 991, 711, 513, 213, 428, 559, 943, 171, 953, 660, 557, 1090, 1161, 60, 87, 727, 1027, 241, 561, 617, 766, 902, 432, 456, 864, 307, 56, 378, 760, 604, 217, 763, 1132, 1128, 931, 94, 1210, 755, 178, 551, 808, 235, 365, 176, 214, 759, 167, 30, 192, 273, 1211, 458, 812, 33, 316, 914, 821, 723, 569, 329, 578, 477, 633, 0, 123, 530, 715, 438, 79, 809, 1194, 410, 946, 524, 152, 737, 393, 681, 150, 1182, 770, 1003, 666, 179, 431, 98, 694, 751, 1103, 429, 842, 626, 752, 627, 673, 174, 175, 848, 972, 354, 822, 768, 292, 1039, 1086, 489, 893, 1084, 689, 3, 790, 230, 172, 698, 188, 111, 237, 1163, 363, 1202, 1192, 164, 818, 440, 258, 504, 547, 371, 765, 526, 958, 420, 265, 655, 945, 1069, 951, 368, 610, 942, 1157, 219]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  738739121042679
the save name prefix for this run is:  chkpt-ID_738739121042679_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 262
rank avg (pred): 0.434 +- 0.018
mrr vals (pred, true): 0.000, 0.302
batch losses (mrrl, rdl): 0.0, 0.0031092416

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 992
rank avg (pred): 0.086 +- 0.062
mrr vals (pred, true): 0.207, 0.203
batch losses (mrrl, rdl): 0.0, 5.93676e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 482
rank avg (pred): 0.387 +- 0.288
mrr vals (pred, true): 0.152, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001105227

Epoch over!
epoch time: 11.941

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 159
rank avg (pred): 0.394 +- 0.291
mrr vals (pred, true): 0.176, 0.066
batch losses (mrrl, rdl): 0.0, 0.0001183185

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 821
rank avg (pred): 0.110 +- 0.084
mrr vals (pred, true): 0.211, 0.153
batch losses (mrrl, rdl): 0.0, 2.62732e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 479
rank avg (pred): 0.391 +- 0.294
mrr vals (pred, true): 0.178, 0.001
batch losses (mrrl, rdl): 0.0, 9.51616e-05

Epoch over!
epoch time: 11.853

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 367
rank avg (pred): 0.391 +- 0.292
mrr vals (pred, true): 0.175, 0.063
batch losses (mrrl, rdl): 0.0, 0.0001418566

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 941
rank avg (pred): 0.515 +- 0.354
mrr vals (pred, true): 0.108, 0.000
batch losses (mrrl, rdl): 0.0, 0.0022026282

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 345
rank avg (pred): 0.435 +- 0.308
mrr vals (pred, true): 0.048, 0.076
batch losses (mrrl, rdl): 0.0, 0.0002863415

Epoch over!
epoch time: 11.779

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1091
rank avg (pred): 0.397 +- 0.289
mrr vals (pred, true): 0.045, 0.056
batch losses (mrrl, rdl): 0.0, 0.0001227694

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1177
rank avg (pred): 0.385 +- 0.283
mrr vals (pred, true): 0.077, 0.067
batch losses (mrrl, rdl): 0.0, 7.00439e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 361
rank avg (pred): 0.365 +- 0.282
mrr vals (pred, true): 0.082, 0.065
batch losses (mrrl, rdl): 0.0, 4.81432e-05

Epoch over!
epoch time: 11.958

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 786
rank avg (pred): 0.507 +- 0.348
mrr vals (pred, true): 0.028, 0.001
batch losses (mrrl, rdl): 0.0, 5.88549e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 195
rank avg (pred): 0.372 +- 0.293
mrr vals (pred, true): 0.086, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001550719

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 872
rank avg (pred): 0.498 +- 0.346
mrr vals (pred, true): 0.034, 0.001
batch losses (mrrl, rdl): 0.0, 5.22758e-05

Epoch over!
epoch time: 11.828

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 33
rank avg (pred): 0.120 +- 0.094
mrr vals (pred, true): 0.086, 0.141
batch losses (mrrl, rdl): 0.0309497453, 3.43879e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1202
rank avg (pred): 0.479 +- 0.143
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001765384, 7.05697e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1159
rank avg (pred): 0.028 +- 0.021
mrr vals (pred, true): 0.203, 0.225
batch losses (mrrl, rdl): 0.0048682801, 0.0008429558

Epoch over!
epoch time: 12.566

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1160
rank avg (pred): 0.015 +- 0.011
mrr vals (pred, true): 0.234, 0.230
batch losses (mrrl, rdl): 0.0001594432, 0.0008352173

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1148
rank avg (pred): 0.068 +- 0.052
mrr vals (pred, true): 0.217, 0.212
batch losses (mrrl, rdl): 0.0002342893, 0.0006075922

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 356
rank avg (pred): 0.433 +- 0.161
mrr vals (pred, true): 0.056, 0.047
batch losses (mrrl, rdl): 0.0003177297, 0.0002181398

Epoch over!
epoch time: 12.273

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 88
rank avg (pred): 0.434 +- 0.133
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 4.70218e-05, 0.0002399338

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 908
rank avg (pred): 0.552 +- 0.348
mrr vals (pred, true): 0.107, 0.003
batch losses (mrrl, rdl): 0.0324937962, 0.0001005426

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 956
rank avg (pred): 0.537 +- 0.138
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0014069076, 0.0001241836

Epoch over!
epoch time: 12.088

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 262
rank avg (pred): 0.006 +- 0.005
mrr vals (pred, true): 0.295, 0.302
batch losses (mrrl, rdl): 0.0005750584, 5.5431e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 797
rank avg (pred): 0.546 +- 0.144
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0010740894, 0.0001297625

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1085
rank avg (pred): 0.426 +- 0.130
mrr vals (pred, true): 0.048, 0.060
batch losses (mrrl, rdl): 3.38765e-05, 0.0002506755

Epoch over!
epoch time: 12.215

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 515
rank avg (pred): 0.190 +- 0.131
mrr vals (pred, true): 0.144, 0.140
batch losses (mrrl, rdl): 0.0002131303, 0.0001211993

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 189
rank avg (pred): 0.420 +- 0.144
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 9.9246e-06, 0.000163874

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 707
rank avg (pred): 0.438 +- 0.140
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.7951e-06, 0.0001154477

Epoch over!
epoch time: 12.12

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 393
rank avg (pred): 0.414 +- 0.155
mrr vals (pred, true): 0.057, 0.051
batch losses (mrrl, rdl): 0.0004841569, 0.0001789014

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 680
rank avg (pred): 0.428 +- 0.138
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.06821e-05, 0.0001321472

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 945
rank avg (pred): 0.549 +- 0.135
mrr vals (pred, true): 0.035, 0.001
batch losses (mrrl, rdl): 0.0021749756, 0.0001235041

Epoch over!
epoch time: 12.294

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1085
rank avg (pred): 0.422 +- 0.130
mrr vals (pred, true): 0.050, 0.060
batch losses (mrrl, rdl): 1.983e-07, 0.0002378114

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1106
rank avg (pred): 0.406 +- 0.152
mrr vals (pred, true): 0.056, 0.062
batch losses (mrrl, rdl): 0.0003230518, 0.0001940069

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1117
rank avg (pred): 0.427 +- 0.128
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.46323e-05, 0.0001711649

Epoch over!
epoch time: 12.166

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1210
rank avg (pred): 0.445 +- 0.125
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.33625e-05, 0.0001377436

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.411 +- 0.145
mrr vals (pred, true): 0.053, 0.068
batch losses (mrrl, rdl): 0.0001139491, 0.0002053619

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 290
rank avg (pred): 0.086 +- 0.059
mrr vals (pred, true): 0.258, 0.218
batch losses (mrrl, rdl): 0.01615352, 4.29976e-05

Epoch over!
epoch time: 11.825

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 90
rank avg (pred): 0.424 +- 0.127
mrr vals (pred, true): 0.049, 0.075
batch losses (mrrl, rdl): 9.2615e-06, 0.0002473278

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.366 +- 0.249
mrr vals (pred, true): 0.186, 0.186
batch losses (mrrl, rdl): 1.0992e-06, 0.0005484857

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 112
rank avg (pred): 0.421 +- 0.169
mrr vals (pred, true): 0.061, 0.057
batch losses (mrrl, rdl): 0.0013108681, 0.000227809

Epoch over!
epoch time: 12.061

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 777
rank avg (pred): 0.497 +- 0.121
mrr vals (pred, true): 0.043, 0.001
batch losses (mrrl, rdl): 0.0004556781, 0.0001516454

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 326
rank avg (pred): 0.420 +- 0.137
mrr vals (pred, true): 0.051, 0.066
batch losses (mrrl, rdl): 3.9779e-06, 0.0002156347

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 649
rank avg (pred): 0.410 +- 0.139
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.65581e-05, 0.0002152793

Epoch over!
epoch time: 12.06

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.031 +- 0.021
mrr vals (pred, true): 0.289, 0.283

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.04272 	 8e-0500 	 m..s
    9 	     1 	 0.04295 	 0.00013 	 m..s
   62 	     2 	 0.05368 	 0.00022 	 m..s
   10 	     3 	 0.04450 	 0.00023 	 m..s
   83 	     4 	 0.08146 	 0.00024 	 m..s
   13 	     5 	 0.04463 	 0.00037 	 m..s
   29 	     6 	 0.04899 	 0.00048 	 m..s
   75 	     7 	 0.05663 	 0.00049 	 m..s
   53 	     8 	 0.05173 	 0.00049 	 m..s
   39 	     9 	 0.04976 	 0.00049 	 m..s
   14 	    10 	 0.04499 	 0.00050 	 m..s
    7 	    11 	 0.04287 	 0.00051 	 m..s
   30 	    12 	 0.04901 	 0.00051 	 m..s
   71 	    13 	 0.05603 	 0.00052 	 m..s
   67 	    14 	 0.05514 	 0.00053 	 m..s
   24 	    15 	 0.04862 	 0.00055 	 m..s
    0 	    16 	 0.03970 	 0.00058 	 m..s
   58 	    17 	 0.05295 	 0.00058 	 m..s
   72 	    18 	 0.05609 	 0.00061 	 m..s
    8 	    19 	 0.04288 	 0.00064 	 m..s
   38 	    20 	 0.04970 	 0.00065 	 m..s
    2 	    21 	 0.04146 	 0.00065 	 m..s
   63 	    22 	 0.05373 	 0.00066 	 m..s
    3 	    23 	 0.04205 	 0.00068 	 m..s
    1 	    24 	 0.04091 	 0.00068 	 m..s
   11 	    25 	 0.04452 	 0.00071 	 m..s
   40 	    26 	 0.04977 	 0.00071 	 m..s
   51 	    27 	 0.05107 	 0.00072 	 m..s
   43 	    28 	 0.04991 	 0.00074 	 m..s
   19 	    29 	 0.04563 	 0.00075 	 m..s
    6 	    30 	 0.04280 	 0.00076 	 m..s
   65 	    31 	 0.05480 	 0.00080 	 m..s
   21 	    32 	 0.04821 	 0.00080 	 m..s
   12 	    33 	 0.04463 	 0.00083 	 m..s
   35 	    34 	 0.04951 	 0.00085 	 m..s
   26 	    35 	 0.04867 	 0.00085 	 m..s
   25 	    36 	 0.04866 	 0.00086 	 m..s
   59 	    37 	 0.05316 	 0.00087 	 m..s
    4 	    38 	 0.04235 	 0.00097 	 m..s
   16 	    39 	 0.04528 	 0.00098 	 m..s
   17 	    40 	 0.04538 	 0.00309 	 m..s
   86 	    41 	 0.08852 	 0.00541 	 m..s
   18 	    42 	 0.04544 	 0.00655 	 m..s
   15 	    43 	 0.04515 	 0.00684 	 m..s
   82 	    44 	 0.07655 	 0.00866 	 m..s
   85 	    45 	 0.08492 	 0.00878 	 m..s
   81 	    46 	 0.07597 	 0.01348 	 m..s
   84 	    47 	 0.08402 	 0.01722 	 m..s
   73 	    48 	 0.05609 	 0.04173 	 ~...
   79 	    49 	 0.05922 	 0.04348 	 ~...
   28 	    50 	 0.04899 	 0.04751 	 ~...
   50 	    51 	 0.05090 	 0.04758 	 ~...
   37 	    52 	 0.04965 	 0.04881 	 ~...
   78 	    53 	 0.05898 	 0.04933 	 ~...
   41 	    54 	 0.04985 	 0.05021 	 ~...
   76 	    55 	 0.05669 	 0.05180 	 ~...
   70 	    56 	 0.05584 	 0.05187 	 ~...
   46 	    57 	 0.05030 	 0.05398 	 ~...
   69 	    58 	 0.05556 	 0.05445 	 ~...
   42 	    59 	 0.04987 	 0.05553 	 ~...
   56 	    60 	 0.05211 	 0.05685 	 ~...
   61 	    61 	 0.05354 	 0.05689 	 ~...
   27 	    62 	 0.04896 	 0.05708 	 ~...
   54 	    63 	 0.05195 	 0.05775 	 ~...
   23 	    64 	 0.04846 	 0.05817 	 ~...
   32 	    65 	 0.04913 	 0.05828 	 ~...
   45 	    66 	 0.05015 	 0.05836 	 ~...
   31 	    67 	 0.04905 	 0.05860 	 ~...
   64 	    68 	 0.05465 	 0.05881 	 ~...
   60 	    69 	 0.05326 	 0.05969 	 ~...
   57 	    70 	 0.05240 	 0.06177 	 ~...
   49 	    71 	 0.05073 	 0.06181 	 ~...
   55 	    72 	 0.05204 	 0.06246 	 ~...
   36 	    73 	 0.04963 	 0.06257 	 ~...
   34 	    74 	 0.04921 	 0.06291 	 ~...
   68 	    75 	 0.05515 	 0.06298 	 ~...
   74 	    76 	 0.05651 	 0.06319 	 ~...
   66 	    77 	 0.05500 	 0.06577 	 ~...
   22 	    78 	 0.04824 	 0.06605 	 ~...
   52 	    79 	 0.05167 	 0.06895 	 ~...
   77 	    80 	 0.05757 	 0.06947 	 ~...
   33 	    81 	 0.04918 	 0.07067 	 ~...
   48 	    82 	 0.05067 	 0.07139 	 ~...
   44 	    83 	 0.05009 	 0.07170 	 ~...
   20 	    84 	 0.04813 	 0.07310 	 ~...
   47 	    85 	 0.05047 	 0.08002 	 ~...
   80 	    86 	 0.07164 	 0.08161 	 ~...
   87 	    87 	 0.16151 	 0.11561 	 m..s
   88 	    88 	 0.16302 	 0.12784 	 m..s
   94 	    89 	 0.19019 	 0.13420 	 m..s
  101 	    90 	 0.20412 	 0.13828 	 m..s
   92 	    91 	 0.18378 	 0.13965 	 m..s
   90 	    92 	 0.16656 	 0.14021 	 ~...
   93 	    93 	 0.18855 	 0.14058 	 m..s
   95 	    94 	 0.19123 	 0.14839 	 m..s
   97 	    95 	 0.19635 	 0.15246 	 m..s
   89 	    96 	 0.16593 	 0.16009 	 ~...
   96 	    97 	 0.19476 	 0.16345 	 m..s
   91 	    98 	 0.18039 	 0.16576 	 ~...
   99 	    99 	 0.19971 	 0.17141 	 ~...
  100 	   100 	 0.20245 	 0.17280 	 ~...
   98 	   101 	 0.19908 	 0.18313 	 ~...
  109 	   102 	 0.25331 	 0.19936 	 m..s
  103 	   103 	 0.22828 	 0.20099 	 ~...
  107 	   104 	 0.24319 	 0.20655 	 m..s
  106 	   105 	 0.24152 	 0.21741 	 ~...
  104 	   106 	 0.23813 	 0.21798 	 ~...
  102 	   107 	 0.20498 	 0.22389 	 ~...
  105 	   108 	 0.24041 	 0.22977 	 ~...
  108 	   109 	 0.24455 	 0.23673 	 ~...
  111 	   110 	 0.27869 	 0.24930 	 ~...
  113 	   111 	 0.28449 	 0.27124 	 ~...
  110 	   112 	 0.25778 	 0.27373 	 ~...
  112 	   113 	 0.27987 	 0.27439 	 ~...
  114 	   114 	 0.28891 	 0.28330 	 ~...
  120 	   115 	 0.29071 	 0.29860 	 ~...
  116 	   116 	 0.28918 	 0.30388 	 ~...
  118 	   117 	 0.28946 	 0.30429 	 ~...
  119 	   118 	 0.29020 	 0.30616 	 ~...
  117 	   119 	 0.28935 	 0.30982 	 ~...
  115 	   120 	 0.28913 	 0.31362 	 ~...
==========================================
r_mrr = 0.9509611129760742
r2_mrr = 0.8412918448448181
spearmanr_mrr@5 = 0.9937841296195984
spearmanr_mrr@10 = 0.8758341670036316
spearmanr_mrr@50 = 0.973794162273407
spearmanr_mrr@100 = 0.9713588953018188
spearmanr_mrr@All = 0.9720525145530701
==========================================
test time: 0.429
Done Testing dataset OpenEA
total time taken: 196.6342875957489
training time taken: 181.53357315063477
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9510)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8413)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9938)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.8758)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9738)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9714)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9721)}}, 'test_loss': {'DistMult': {'OpenEA': 0.43855965590046253}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9873698302411392
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [762, 66, 432, 1202, 294, 39, 567, 1147, 777, 602, 769, 449, 649, 1152, 1185, 834, 594, 27, 148, 907, 308, 831, 171, 63, 127, 787, 647, 324, 993, 4, 268, 606, 1105, 1138, 561, 192, 274, 348, 252, 626, 720, 733, 965, 368, 452, 1171, 303, 45, 689, 1089, 112, 859, 1076, 134, 732, 318, 215, 694, 759, 283, 974, 908, 233, 1026, 577, 972, 273, 1044, 755, 1072, 657, 56, 634, 570, 548, 288, 607, 1181, 955, 679, 636, 195, 936, 446, 132, 305, 67, 453, 601, 1167, 827, 784, 349, 255, 523, 469, 86, 953, 416, 441, 901, 60, 403, 1002, 1104, 117, 249, 395, 476, 855, 155, 772, 1004, 630, 1, 1123, 1033, 1112, 59, 181, 1018]
valid_ids (0): []
train_ids (1094): [625, 269, 442, 880, 43, 921, 1193, 848, 204, 42, 819, 235, 793, 226, 1042, 579, 278, 969, 1135, 718, 1094, 1117, 325, 773, 1150, 311, 151, 598, 892, 468, 1028, 767, 517, 736, 533, 1078, 208, 717, 525, 1166, 605, 652, 711, 1145, 376, 668, 241, 345, 1091, 481, 394, 77, 987, 33, 618, 358, 933, 35, 482, 573, 26, 196, 1178, 31, 483, 1197, 826, 412, 669, 456, 444, 666, 1027, 1151, 611, 945, 957, 123, 1070, 995, 970, 1127, 914, 433, 179, 32, 266, 1148, 1130, 912, 222, 828, 341, 174, 1132, 583, 739, 357, 644, 223, 317, 340, 497, 1172, 778, 667, 65, 14, 615, 923, 99, 422, 562, 795, 991, 1075, 1155, 845, 404, 136, 124, 844, 806, 49, 699, 355, 296, 108, 761, 331, 629, 725, 664, 735, 2, 1122, 113, 724, 1020, 856, 1097, 290, 310, 319, 366, 837, 240, 242, 792, 841, 334, 838, 417, 628, 927, 488, 994, 106, 674, 541, 1116, 584, 935, 810, 182, 1096, 952, 703, 938, 378, 558, 57, 847, 138, 980, 374, 530, 111, 941, 705, 860, 750, 684, 54, 686, 776, 540, 162, 840, 1156, 764, 413, 803, 375, 971, 351, 74, 1034, 1164, 708, 802, 238, 698, 816, 490, 922, 1095, 64, 291, 1184, 332, 339, 542, 899, 861, 986, 508, 1006, 1124, 323, 489, 169, 219, 1161, 61, 516, 999, 157, 865, 830, 898, 1199, 135, 559, 532, 267, 808, 16, 352, 823, 115, 161, 259, 1209, 1015, 966, 425, 960, 930, 146, 299, 87, 613, 217, 1175, 239, 362, 889, 595, 1046, 79, 638, 563, 298, 985, 729, 279, 757, 919, 48, 383, 102, 367, 377, 621, 475, 165, 1037, 514, 254, 902, 380, 356, 1092, 313, 459, 905, 890, 1189, 121, 515, 300, 1131, 1086, 427, 659, 926, 758, 1031, 774, 1049, 1213, 493, 506, 1174, 676, 518, 670, 1068, 663, 440, 692, 963, 5, 280, 580, 578, 1036, 1170, 186, 96, 0, 1214, 557, 555, 1201, 682, 1050, 639, 47, 504, 545, 193, 436, 1139, 906, 867, 854, 843, 164, 248, 1059, 141, 526, 1208, 916, 565, 159, 829, 1067, 384, 321, 982, 363, 133, 1200, 75, 978, 507, 833, 943, 603, 910, 1108, 501, 200, 813, 693, 329, 1168, 1111, 715, 1203, 680, 1207, 650, 846, 896, 731, 372, 807, 988, 1023, 875, 706, 852, 874, 502, 1190, 599, 891, 172, 307, 402, 665, 770, 637, 72, 284, 789, 373, 261, 658, 864, 796, 1063, 500, 198, 839, 301, 1003, 920, 983, 328, 979, 385, 275, 1143, 260, 958, 431, 918, 90, 868, 588, 513, 354, 166, 998, 547, 528, 1016, 939, 480, 178, 1005, 462, 954, 69, 44, 83, 289, 185, 466, 435, 597, 1182, 1137, 783, 152, 1014, 272, 1035, 293, 105, 225, 23, 145, 643, 815, 94, 744, 1051, 142, 306, 917, 879, 10, 928, 1073, 258, 499, 473, 287, 552, 95, 1045, 721, 968, 173, 478, 343, 853, 81, 753, 1160, 660, 471, 590, 387, 641, 1109, 58, 140, 976, 393, 286, 477, 544, 961, 369, 420, 398, 582, 713, 104, 900, 696, 536, 257, 336, 798, 766, 1113, 396, 1030, 391, 871, 418, 655, 126, 405, 175, 486, 116, 1153, 1146, 690, 1093, 946, 1157, 1038, 445, 569, 1187, 1158, 1173, 534, 738, 688, 414, 131, 964, 194, 1142, 388, 704, 232, 791, 183, 749, 821, 103, 661, 568, 295, 948, 487, 246, 866, 474, 264, 1128, 1085, 82, 782, 672, 959, 197, 617, 415, 677, 1065, 1024, 631, 1007, 519, 726, 277, 40, 429, 389, 593, 924, 231, 479, 817, 915, 1040, 206, 276, 234, 538, 765, 1165, 212, 465, 785, 330, 484, 1194, 529, 539, 1163, 737, 114, 454, 213, 80, 1191, 400, 646, 651, 270, 297, 1062, 691, 156, 88, 591, 797, 977, 709, 485, 997, 962, 1126, 1019, 632, 247, 1079, 36, 304, 990, 931, 316, 543, 781, 392, 230, 1022, 122, 671, 93, 158, 1060, 101, 849, 408, 956, 681, 15, 913, 419, 804, 904, 522, 271, 1056, 22, 884, 1114, 46, 428, 604, 575, 627, 382, 742, 244, 862, 84, 70, 434, 13, 1082, 521, 592, 572, 447, 458, 775, 472, 1069, 424, 160, 62, 909, 614, 397, 722, 771, 747, 1180, 335, 1084, 421, 633, 1008, 818, 455, 29, 1144, 1159, 1088, 292, 741, 873, 869, 12, 184, 423, 89, 535, 37, 1102, 799, 24, 1000, 7, 1043, 149, 346, 399, 546, 1101, 850, 877, 746, 675, 3, 654, 409, 685, 600, 911, 364, 209, 20, 189, 809, 788, 1196, 386, 17, 925, 167, 401, 430, 190, 531, 551, 832, 886, 100, 730, 712, 78, 576, 1048, 125, 1115, 251, 110, 327, 130, 338, 587, 586, 322, 511, 616, 623, 800, 333, 1134, 752, 610, 1012, 188, 1154, 147, 950, 550, 1136, 932, 505, 492, 1183, 723, 951, 224, 1186, 894, 949, 858, 256, 463, 967, 1118, 1074, 822, 1047, 370, 885, 1011, 585, 1087, 1061, 872, 228, 851, 1133, 883, 38, 697, 406, 411, 437, 6, 221, 1032, 461, 53, 1107, 262, 220, 320, 187, 748, 439, 740, 495, 150, 745, 76, 878, 214, 168, 1103, 285, 897, 144, 347, 253, 1212, 407, 210, 1129, 1140, 1125, 379, 734, 199, 768, 836, 727, 236, 520, 973, 780, 707, 128, 1195, 25, 1029, 763, 619, 1058, 716, 553, 814, 450, 1204, 989, 1081, 229, 350, 390, 751, 887, 940, 937, 805, 1211, 443, 683, 1055, 801, 812, 91, 218, 1179, 381, 41, 835, 202, 154, 1188, 656, 498, 464, 635, 620, 825, 888, 1098, 1071, 560, 1077, 460, 581, 648, 622, 1110, 714, 981, 109, 984, 281, 794, 728, 85, 119, 245, 73, 139, 68, 207, 779, 50, 934, 28, 1013, 496, 566, 608, 1010, 700, 1177, 1099, 881, 9, 237, 8, 1169, 1162, 754, 1066, 687, 929, 360, 554, 903, 1192, 263, 662, 1053, 1149, 1119, 512, 882, 1100, 312, 143, 177, 1206, 589, 1198, 120, 1039, 1121, 564, 55, 491, 1041, 30, 710, 97, 760, 992, 1009, 18, 302, 205, 701, 211, 503, 640, 361, 107, 702, 1001, 365, 21, 153, 1064, 52, 337, 34, 1141, 426, 243, 1080, 265, 876, 216, 893, 201, 975, 1210, 92, 1205, 203, 359, 137, 510, 842, 645, 863, 537, 371, 11, 51, 457, 944, 191, 653, 410, 719, 857, 180, 571, 820, 19, 524, 1120, 695, 326, 1083, 451, 824, 470, 170, 942, 811, 227, 309, 448, 947, 353, 596, 1021, 494, 163, 624, 642, 1017, 612, 609, 118, 556, 574, 527, 1054, 438, 743, 314, 71, 250, 870, 895, 1090, 467, 786, 98, 315, 1025, 1106, 1176, 678, 1052, 509, 756, 344, 673, 1057, 176, 790, 282, 129, 342, 549, 996]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2703556478373032
the save name prefix for this run is:  chkpt-ID_2703556478373032_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1144
rank avg (pred): 0.485 +- 0.006
mrr vals (pred, true): 0.000, 0.186
batch losses (mrrl, rdl): 0.0, 0.0014816084

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 813
rank avg (pred): 0.188 +- 0.128
mrr vals (pred, true): 0.172, 0.009
batch losses (mrrl, rdl): 0.0, 6.99374e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 969
rank avg (pred): 0.596 +- 0.373
mrr vals (pred, true): 0.178, 0.001
batch losses (mrrl, rdl): 0.0, 0.0003272516

Epoch over!
epoch time: 12.067

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 966
rank avg (pred): 0.530 +- 0.360
mrr vals (pred, true): 0.215, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001378804

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 880
rank avg (pred): 0.544 +- 0.353
mrr vals (pred, true): 0.194, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001424755

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1093
rank avg (pred): 0.360 +- 0.275
mrr vals (pred, true): 0.247, 0.065
batch losses (mrrl, rdl): 0.0, 6.23025e-05

Epoch over!
epoch time: 11.97

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 361
rank avg (pred): 0.415 +- 0.294
mrr vals (pred, true): 0.221, 0.065
batch losses (mrrl, rdl): 0.0, 0.0001767126

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1196
rank avg (pred): 0.388 +- 0.287
mrr vals (pred, true): 0.227, 0.001
batch losses (mrrl, rdl): 0.0, 9.69172e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 168
rank avg (pred): 0.379 +- 0.285
mrr vals (pred, true): 0.240, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001488169

Epoch over!
epoch time: 11.974

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 337
rank avg (pred): 0.377 +- 0.285
mrr vals (pred, true): 0.241, 0.074
batch losses (mrrl, rdl): 0.0, 0.0001173777

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 440
rank avg (pred): 0.371 +- 0.279
mrr vals (pred, true): 0.226, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001579285

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 976
rank avg (pred): 0.075 +- 0.058
mrr vals (pred, true): 0.239, 0.307
batch losses (mrrl, rdl): 0.0, 2.05295e-05

Epoch over!
epoch time: 11.853

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 928
rank avg (pred): 0.553 +- 0.367
mrr vals (pred, true): 0.206, 0.005
batch losses (mrrl, rdl): 0.0, 4.95069e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1016
rank avg (pred): 0.383 +- 0.285
mrr vals (pred, true): 0.208, 0.076
batch losses (mrrl, rdl): 0.0, 9.62501e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 359
rank avg (pred): 0.386 +- 0.284
mrr vals (pred, true): 0.202, 0.043
batch losses (mrrl, rdl): 0.0, 7.64807e-05

Epoch over!
epoch time: 11.818

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.416 +- 0.302
mrr vals (pred, true): 0.200, 0.056
batch losses (mrrl, rdl): 0.2247748077, 0.0001429796

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 715
rank avg (pred): 0.449 +- 0.153
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 8.73456e-05, 8.93424e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1145
rank avg (pred): 0.092 +- 0.066
mrr vals (pred, true): 0.184, 0.175
batch losses (mrrl, rdl): 0.000819599, 0.0004951604

Epoch over!
epoch time: 12.218

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1024
rank avg (pred): 0.434 +- 0.151
mrr vals (pred, true): 0.054, 0.078
batch losses (mrrl, rdl): 0.0001467085, 0.0002819345

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 160
rank avg (pred): 0.445 +- 0.137
mrr vals (pred, true): 0.047, 0.056
batch losses (mrrl, rdl): 9.75786e-05, 0.0003369992

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.014 +- 0.011
mrr vals (pred, true): 0.230, 0.215
batch losses (mrrl, rdl): 0.0020042816, 0.0002012089

Epoch over!
epoch time: 12.045

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 129
rank avg (pred): 0.413 +- 0.148
mrr vals (pred, true): 0.058, 0.066
batch losses (mrrl, rdl): 0.0005798959, 0.0002070325

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 68
rank avg (pred): 0.242 +- 0.165
mrr vals (pred, true): 0.166, 0.227
batch losses (mrrl, rdl): 0.0371710546, 0.000453115

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1086
rank avg (pred): 0.421 +- 0.127
mrr vals (pred, true): 0.048, 0.062
batch losses (mrrl, rdl): 3.60729e-05, 0.0002209363

Epoch over!
epoch time: 12.943

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 286
rank avg (pred): 0.162 +- 0.116
mrr vals (pred, true): 0.195, 0.236
batch losses (mrrl, rdl): 0.0171656441, 0.0001167545

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 672
rank avg (pred): 0.411 +- 0.137
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.75375e-05, 0.0001924788

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 662
rank avg (pred): 0.416 +- 0.126
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.34108e-05, 0.0001800145

Epoch over!
epoch time: 12.523

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 444
rank avg (pred): 0.412 +- 0.128
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.77302e-05, 0.0001916024

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.044 +- 0.032
mrr vals (pred, true): 0.228, 0.215
batch losses (mrrl, rdl): 0.0015743285, 0.0001089752

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1010
rank avg (pred): 0.404 +- 0.136
mrr vals (pred, true): 0.051, 0.045
batch losses (mrrl, rdl): 1.63602e-05, 0.0001397938

Epoch over!
epoch time: 12.229

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 745
rank avg (pred): 0.388 +- 0.215
mrr vals (pred, true): 0.128, 0.142
batch losses (mrrl, rdl): 0.0018628507, 0.000975322

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1103
rank avg (pred): 0.409 +- 0.128
mrr vals (pred, true): 0.049, 0.068
batch losses (mrrl, rdl): 9.4635e-06, 0.0002056008

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 997
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.269, 0.258
batch losses (mrrl, rdl): 0.0011853124, 0.000275119

Epoch over!
epoch time: 12.536

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1119
rank avg (pred): 0.410 +- 0.122
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 4.51462e-05, 0.000222911

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1096
rank avg (pred): 0.428 +- 0.146
mrr vals (pred, true): 0.051, 0.056
batch losses (mrrl, rdl): 2.15494e-05, 0.0002620687

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1114
rank avg (pred): 0.409 +- 0.128
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.1477e-06, 0.0001885854

Epoch over!
epoch time: 11.847

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1209
rank avg (pred): 0.405 +- 0.129
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.23736e-05, 0.0002627691

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 191
rank avg (pred): 0.426 +- 0.122
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001074281, 0.0001869155

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 223
rank avg (pred): 0.419 +- 0.129
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.56621e-05, 0.000169263

Epoch over!
epoch time: 12.105

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 203
rank avg (pred): 0.422 +- 0.121
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001371203, 0.0001689124

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 183
rank avg (pred): 0.392 +- 0.154
mrr vals (pred, true): 0.060, 0.001
batch losses (mrrl, rdl): 0.0009728289, 0.0002706838

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 797
rank avg (pred): 0.509 +- 0.212
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.35399e-05, 3.72147e-05

Epoch over!
epoch time: 12.313

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 371
rank avg (pred): 0.418 +- 0.137
mrr vals (pred, true): 0.051, 0.057
batch losses (mrrl, rdl): 9.5097e-06, 0.0001926122

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 250
rank avg (pred): 0.010 +- 0.010
mrr vals (pred, true): 0.289, 0.283
batch losses (mrrl, rdl): 0.0003675725, 4.36431e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.419 +- 0.120
mrr vals (pred, true): 0.048, 0.048
batch losses (mrrl, rdl): 5.35871e-05, 0.0001646377

Epoch over!
epoch time: 12.801

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.576 +- 0.249
mrr vals (pred, true): 0.042, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    8 	     0 	 0.04634 	 0.00027 	 m..s
   36 	     1 	 0.04867 	 0.00049 	 m..s
   65 	     2 	 0.04932 	 0.00050 	 m..s
   66 	     3 	 0.04942 	 0.00051 	 m..s
   37 	     4 	 0.04868 	 0.00053 	 m..s
   32 	     5 	 0.04836 	 0.00053 	 m..s
   34 	     6 	 0.04854 	 0.00055 	 m..s
   33 	     7 	 0.04853 	 0.00056 	 m..s
    0 	     8 	 0.03752 	 0.00058 	 m..s
   75 	     9 	 0.04997 	 0.00058 	 m..s
    2 	    10 	 0.04295 	 0.00059 	 m..s
   38 	    11 	 0.04870 	 0.00059 	 m..s
   23 	    12 	 0.04806 	 0.00060 	 m..s
   69 	    13 	 0.04949 	 0.00061 	 m..s
   11 	    14 	 0.04756 	 0.00064 	 m..s
    1 	    15 	 0.04241 	 0.00065 	 m..s
   20 	    16 	 0.04795 	 0.00067 	 m..s
   49 	    17 	 0.04897 	 0.00068 	 m..s
   24 	    18 	 0.04816 	 0.00071 	 m..s
   10 	    19 	 0.04752 	 0.00072 	 m..s
   45 	    20 	 0.04894 	 0.00074 	 m..s
   19 	    21 	 0.04790 	 0.00074 	 m..s
   28 	    22 	 0.04827 	 0.00084 	 m..s
   18 	    23 	 0.04787 	 0.00086 	 m..s
   62 	    24 	 0.04923 	 0.00087 	 m..s
    7 	    25 	 0.04421 	 0.00088 	 m..s
   54 	    26 	 0.04907 	 0.00091 	 m..s
   14 	    27 	 0.04774 	 0.00092 	 m..s
   13 	    28 	 0.04774 	 0.00099 	 m..s
   52 	    29 	 0.04901 	 0.00108 	 m..s
   12 	    30 	 0.04765 	 0.00110 	 m..s
   21 	    31 	 0.04797 	 0.00116 	 m..s
    5 	    32 	 0.04383 	 0.00117 	 m..s
   55 	    33 	 0.04910 	 0.00118 	 m..s
    4 	    34 	 0.04337 	 0.00125 	 m..s
    6 	    35 	 0.04413 	 0.00129 	 m..s
   17 	    36 	 0.04784 	 0.00147 	 m..s
   41 	    37 	 0.04879 	 0.00209 	 m..s
   81 	    38 	 0.05777 	 0.00320 	 m..s
   80 	    39 	 0.05625 	 0.00359 	 m..s
   78 	    40 	 0.05258 	 0.00381 	 m..s
    3 	    41 	 0.04325 	 0.00500 	 m..s
   35 	    42 	 0.04864 	 0.00655 	 m..s
   77 	    43 	 0.05247 	 0.00718 	 m..s
   79 	    44 	 0.05270 	 0.01507 	 m..s
   46 	    45 	 0.04894 	 0.04796 	 ~...
   22 	    46 	 0.04798 	 0.04831 	 ~...
   76 	    47 	 0.05142 	 0.04831 	 ~...
   39 	    48 	 0.04872 	 0.05014 	 ~...
   48 	    49 	 0.04895 	 0.05038 	 ~...
   40 	    50 	 0.04874 	 0.05052 	 ~...
   56 	    51 	 0.04910 	 0.05194 	 ~...
   51 	    52 	 0.04900 	 0.05223 	 ~...
   42 	    53 	 0.04880 	 0.05230 	 ~...
   43 	    54 	 0.04884 	 0.05231 	 ~...
   26 	    55 	 0.04825 	 0.05262 	 ~...
   61 	    56 	 0.04920 	 0.05272 	 ~...
   74 	    57 	 0.04995 	 0.05328 	 ~...
   53 	    58 	 0.04906 	 0.05398 	 ~...
   29 	    59 	 0.04831 	 0.05445 	 ~...
   50 	    60 	 0.04899 	 0.05531 	 ~...
   68 	    61 	 0.04948 	 0.05560 	 ~...
   31 	    62 	 0.04834 	 0.05612 	 ~...
   44 	    63 	 0.04886 	 0.05653 	 ~...
   25 	    64 	 0.04824 	 0.05653 	 ~...
   27 	    65 	 0.04826 	 0.05689 	 ~...
   67 	    66 	 0.04947 	 0.05701 	 ~...
   47 	    67 	 0.04895 	 0.05760 	 ~...
   57 	    68 	 0.04911 	 0.05860 	 ~...
   63 	    69 	 0.04924 	 0.05885 	 ~...
   71 	    70 	 0.04964 	 0.05889 	 ~...
   15 	    71 	 0.04778 	 0.05965 	 ~...
   30 	    72 	 0.04832 	 0.06036 	 ~...
   73 	    73 	 0.04975 	 0.06111 	 ~...
   58 	    74 	 0.04913 	 0.06246 	 ~...
   60 	    75 	 0.04919 	 0.06340 	 ~...
   72 	    76 	 0.04972 	 0.06397 	 ~...
   70 	    77 	 0.04950 	 0.06446 	 ~...
   64 	    78 	 0.04931 	 0.07014 	 ~...
   59 	    79 	 0.04916 	 0.07270 	 ~...
    9 	    80 	 0.04735 	 0.07431 	 ~...
   16 	    81 	 0.04780 	 0.07964 	 m..s
   89 	    82 	 0.17301 	 0.13287 	 m..s
   88 	    83 	 0.17001 	 0.14058 	 ~...
   97 	    84 	 0.17770 	 0.14364 	 m..s
   92 	    85 	 0.17557 	 0.14511 	 m..s
   82 	    86 	 0.13561 	 0.14670 	 ~...
   96 	    87 	 0.17767 	 0.15071 	 ~...
   85 	    88 	 0.14852 	 0.15221 	 ~...
   94 	    89 	 0.17614 	 0.15246 	 ~...
   83 	    90 	 0.13893 	 0.15880 	 ~...
  102 	    91 	 0.18420 	 0.15883 	 ~...
   91 	    92 	 0.17511 	 0.16042 	 ~...
   98 	    93 	 0.17799 	 0.16256 	 ~...
   86 	    94 	 0.14872 	 0.16306 	 ~...
   84 	    95 	 0.14843 	 0.17126 	 ~...
   90 	    96 	 0.17353 	 0.17639 	 ~...
   87 	    97 	 0.15166 	 0.17806 	 ~...
   95 	    98 	 0.17709 	 0.17924 	 ~...
   99 	    99 	 0.17809 	 0.18313 	 ~...
   93 	   100 	 0.17606 	 0.18394 	 ~...
  100 	   101 	 0.17922 	 0.20696 	 ~...
  101 	   102 	 0.18291 	 0.21487 	 m..s
  106 	   103 	 0.21812 	 0.22064 	 ~...
  107 	   104 	 0.21993 	 0.22448 	 ~...
  104 	   105 	 0.18748 	 0.22740 	 m..s
  103 	   106 	 0.18518 	 0.23553 	 m..s
  111 	   107 	 0.23139 	 0.24123 	 ~...
  109 	   108 	 0.22245 	 0.25281 	 m..s
  108 	   109 	 0.22239 	 0.25762 	 m..s
  105 	   110 	 0.21655 	 0.25900 	 m..s
  118 	   111 	 0.28909 	 0.26974 	 ~...
  112 	   112 	 0.26767 	 0.27010 	 ~...
  113 	   113 	 0.28306 	 0.27365 	 ~...
  110 	   114 	 0.22772 	 0.27373 	 m..s
  119 	   115 	 0.28955 	 0.27439 	 ~...
  114 	   116 	 0.28317 	 0.27544 	 ~...
  115 	   117 	 0.28655 	 0.28330 	 ~...
  116 	   118 	 0.28823 	 0.29425 	 ~...
  117 	   119 	 0.28835 	 0.29885 	 ~...
  120 	   120 	 0.29949 	 0.30614 	 ~...
==========================================
r_mrr = 0.9578075408935547
r2_mrr = 0.8835759162902832
spearmanr_mrr@5 = 0.7406318783760071
spearmanr_mrr@10 = 0.5963456034660339
spearmanr_mrr@50 = 0.9787932634353638
spearmanr_mrr@100 = 0.9685662984848022
spearmanr_mrr@All = 0.9662869572639465
==========================================
test time: 0.391
Done Testing dataset OpenEA
total time taken: 199.01328945159912
training time taken: 183.701007604599
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9578)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8836)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.7406)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.5963)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9788)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9686)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9663)}}, 'test_loss': {'DistMult': {'OpenEA': 0.25100691963598365}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 5172973932778372
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [280, 600, 907, 1062, 374, 523, 805, 1003, 215, 631, 1068, 475, 284, 558, 1085, 1191, 291, 560, 294, 203, 403, 62, 76, 603, 462, 486, 1148, 645, 234, 37, 1060, 923, 617, 163, 1099, 289, 38, 458, 251, 910, 788, 457, 288, 908, 1199, 1035, 859, 1192, 122, 650, 538, 705, 383, 134, 1049, 712, 120, 355, 90, 390, 927, 463, 361, 834, 742, 1040, 47, 53, 273, 671, 468, 1103, 111, 184, 948, 644, 585, 756, 953, 1050, 114, 776, 310, 1088, 1177, 881, 502, 321, 186, 206, 816, 672, 1135, 438, 888, 778, 734, 1193, 180, 1190, 919, 144, 620, 815, 869, 980, 865, 745, 945, 192, 65, 181, 137, 611, 784, 484, 398, 382, 845, 1044, 257]
valid_ids (0): []
train_ids (1094): [2, 417, 360, 349, 126, 938, 877, 450, 892, 681, 991, 1189, 1027, 951, 491, 591, 536, 1009, 26, 1057, 441, 36, 154, 115, 365, 1207, 193, 1111, 467, 613, 281, 952, 790, 1146, 689, 833, 148, 185, 1071, 297, 685, 641, 5, 88, 362, 937, 875, 1136, 950, 550, 470, 909, 31, 678, 225, 194, 1063, 990, 775, 731, 563, 728, 303, 500, 75, 473, 1, 421, 526, 573, 1138, 219, 87, 1007, 406, 849, 619, 1084, 1201, 802, 905, 898, 764, 1184, 338, 1140, 1129, 757, 525, 427, 168, 737, 1203, 1104, 456, 850, 460, 183, 616, 551, 187, 800, 597, 535, 814, 165, 241, 1178, 556, 70, 1149, 527, 188, 436, 1034, 694, 922, 335, 483, 1016, 174, 581, 981, 547, 231, 341, 565, 658, 205, 106, 683, 69, 768, 202, 508, 182, 548, 592, 896, 1131, 394, 1139, 837, 965, 711, 846, 723, 443, 367, 248, 430, 609, 626, 214, 41, 707, 1180, 395, 1123, 74, 855, 916, 399, 579, 259, 854, 763, 102, 578, 818, 260, 242, 724, 197, 544, 1118, 304, 844, 140, 46, 770, 1043, 608, 381, 930, 522, 83, 1015, 531, 18, 246, 546, 141, 782, 411, 868, 656, 1058, 640, 939, 435, 787, 60, 512, 623, 123, 847, 666, 434, 894, 1018, 873, 354, 1041, 906, 409, 166, 667, 632, 332, 1098, 1141, 655, 539, 179, 1032, 12, 794, 20, 1055, 925, 1090, 1107, 35, 679, 433, 1045, 84, 369, 429, 715, 700, 577, 221, 1144, 595, 903, 553, 1028, 1075, 1095, 736, 149, 228, 884, 1026, 1174, 29, 109, 52, 1185, 765, 1114, 561, 265, 358, 960, 690, 67, 413, 566, 505, 261, 676, 904, 199, 914, 841, 63, 575, 1073, 493, 157, 1061, 697, 1213, 777, 612, 159, 405, 956, 746, 375, 407, 286, 465, 112, 971, 774, 453, 449, 201, 751, 240, 913, 668, 268, 1117, 39, 59, 469, 510, 1022, 388, 1024, 810, 572, 296, 58, 836, 256, 1170, 900, 132, 207, 300, 1211, 1013, 196, 840, 696, 891, 1179, 835, 1054, 624, 598, 860, 738, 116, 32, 567, 1160, 743, 889, 718, 902, 801, 110, 385, 744, 516, 978, 879, 255, 218, 210, 147, 629, 773, 485, 10, 1030, 55, 410, 637, 1079, 236, 1167, 518, 1186, 599, 931, 97, 571, 628, 758, 125, 684, 1051, 941, 1183, 819, 830, 601, 85, 22, 540, 232, 425, 714, 1065, 857, 285, 1070, 498, 191, 542, 649, 162, 301, 95, 279, 799, 682, 1064, 771, 530, 478, 1134, 721, 1006, 515, 1048, 986, 1208, 1153, 893, 827, 298, 1151, 404, 424, 309, 252, 1000, 378, 1108, 1169, 719, 570, 1212, 447, 665, 804, 969, 178, 1082, 659, 959, 471, 245, 444, 25, 973, 829, 876, 1156, 1127, 208, 933, 489, 277, 583, 1092, 1168, 135, 755, 867, 161, 344, 293, 1200, 464, 45, 6, 532, 964, 331, 861, 1089, 313, 432, 1158, 1161, 11, 839, 588, 662, 769, 1157, 552, 543, 669, 920, 652, 209, 809, 48, 164, 9, 618, 722, 271, 440, 370, 574, 108, 828, 1078, 50, 51, 785, 308, 926, 885, 1069, 1175, 0, 1125, 371, 138, 704, 1142, 17, 795, 1019, 488, 13, 477, 807, 630, 852, 145, 340, 693, 1188, 160, 99, 1100, 673, 227, 129, 677, 509, 451, 576, 8, 1052, 121, 985, 146, 1025, 363, 1021, 94, 1074, 554, 647, 1101, 741, 942, 366, 979, 1038, 480, 235, 562, 1042, 482, 377, 408, 976, 119, 912, 921, 584, 393, 476, 1166, 798, 643, 1204, 1072, 1012, 1094, 494, 306, 564, 1087, 994, 1110, 992, 492, 44, 886, 322, 198, 1206, 459, 1143, 687, 1116, 1173, 93, 557, 549, 337, 727, 226, 173, 663, 496, 754, 786, 287, 177, 1010, 783, 901, 710, 230, 915, 761, 513, 247, 519, 79, 89, 262, 1029, 534, 80, 16, 72, 352, 838, 68, 706, 1122, 968, 998, 254, 302, 307, 28, 1155, 243, 1059, 77, 3, 153, 314, 709, 717, 797, 503, 319, 91, 1076, 1133, 1172, 212, 428, 1004, 1036, 386, 461, 590, 947, 733, 1046, 105, 858, 688, 823, 989, 529, 944, 448, 131, 1093, 781, 64, 336, 158, 330, 1198, 521, 455, 878, 270, 275, 848, 379, 325, 883, 292, 472, 528, 987, 1124, 670, 189, 347, 1120, 524, 1039, 753, 537, 767, 824, 957, 222, 1112, 169, 98, 278, 653, 664, 803, 946, 81, 373, 1163, 954, 283, 339, 351, 156, 200, 621, 789, 686, 1097, 416, 233, 596, 634, 353, 880, 862, 555, 133, 752, 870, 1162, 499, 1105, 1154, 474, 238, 1209, 295, 582, 654, 1113, 993, 1202, 982, 760, 949, 372, 446, 176, 396, 580, 559, 78, 1119, 541, 895, 675, 387, 863, 842, 350, 167, 40, 211, 749, 935, 272, 820, 1023, 7, 899, 1210, 934, 1031, 983, 258, 504, 56, 1182, 871, 817, 691, 725, 639, 812, 917, 1159, 1106, 821, 329, 402, 720, 481, 420, 155, 517, 412, 882, 1067, 791, 282, 928, 61, 1152, 1066, 1047, 1147, 324, 96, 674, 1008, 419, 1091, 1145, 346, 1002, 276, 713, 497, 651, 740, 729, 702, 320, 635, 19, 418, 328, 606, 150, 701, 312, 266, 204, 376, 107, 415, 290, 772, 364, 988, 34, 82, 750, 171, 15, 1017, 334, 636, 124, 1033, 831, 962, 811, 943, 454, 1121, 23, 1187, 972, 311, 414, 1083, 918, 1037, 431, 359, 1171, 43, 479, 152, 445, 229, 130, 391, 586, 924, 118, 1150, 269, 1056, 698, 426, 217, 250, 139, 14, 661, 514, 759, 607, 642, 30, 1011, 342, 822, 911, 762, 997, 615, 104, 42, 299, 963, 223, 897, 368, 316, 101, 955, 1165, 1176, 466, 54, 4, 73, 851, 128, 237, 1130, 507, 511, 806, 1005, 175, 216, 890, 439, 345, 961, 808, 589, 323, 646, 735, 136, 190, 977, 506, 348, 24, 1194, 66, 452, 545, 100, 648, 249, 703, 380, 151, 327, 1086, 1096, 936, 1102, 813, 796, 1137, 195, 716, 929, 357, 940, 932, 856, 501, 57, 27, 708, 975, 966, 220, 423, 779, 1195, 730, 103, 692, 326, 263, 224, 1128, 587, 333, 793, 1132, 1001, 33, 638, 490, 853, 984, 117, 887, 400, 397, 699, 315, 726, 1109, 1181, 1164, 1080, 610, 843, 318, 604, 732, 633, 660, 627, 71, 487, 780, 384, 832, 422, 974, 1053, 739, 970, 622, 995, 239, 593, 864, 21, 695, 569, 1197, 792, 680, 356, 49, 172, 533, 264, 274, 495, 343, 392, 866, 1077, 967, 568, 520, 996, 1214, 1081, 213, 958, 127, 170, 602, 142, 1014, 267, 401, 143, 748, 747, 253, 874, 625, 1115, 614, 92, 389, 766, 113, 86, 594, 825, 437, 305, 1205, 244, 872, 605, 826, 1196, 657, 1126, 999, 317, 442, 1020]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9657136412197398
the save name prefix for this run is:  chkpt-ID_9657136412197398_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1024
rank avg (pred): 0.526 +- 0.002
mrr vals (pred, true): 0.000, 0.078
batch losses (mrrl, rdl): 0.0, 0.0008025806

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 871
rank avg (pred): 0.503 +- 0.219
mrr vals (pred, true): 0.039, 0.000
batch losses (mrrl, rdl): 0.0, 2.53459e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.441 +- 0.303
mrr vals (pred, true): 0.070, 0.048
batch losses (mrrl, rdl): 0.0, 0.0001333373

Epoch over!
epoch time: 12.253

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 655
rank avg (pred): 0.389 +- 0.295
mrr vals (pred, true): 0.077, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001299498

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 566
rank avg (pred): 0.241 +- 0.193
mrr vals (pred, true): 0.082, 0.182
batch losses (mrrl, rdl): 0.0, 5.93004e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 867
rank avg (pred): 0.434 +- 0.331
mrr vals (pred, true): 0.091, 0.001
batch losses (mrrl, rdl): 0.0, 4.22694e-05

Epoch over!
epoch time: 12.231

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 992
rank avg (pred): 0.084 +- 0.071
mrr vals (pred, true): 0.109, 0.203
batch losses (mrrl, rdl): 0.0, 5.81286e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 663
rank avg (pred): 0.419 +- 0.291
mrr vals (pred, true): 0.086, 0.001
batch losses (mrrl, rdl): 0.0, 5.29582e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 848
rank avg (pred): 0.505 +- 0.280
mrr vals (pred, true): 0.073, 0.006
batch losses (mrrl, rdl): 0.0, 0.0001142119

Epoch over!
epoch time: 12.165

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1137
rank avg (pred): 0.208 +- 0.168
mrr vals (pred, true): 0.105, 0.270
batch losses (mrrl, rdl): 0.0, 6.0155e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1208
rank avg (pred): 0.390 +- 0.298
mrr vals (pred, true): 0.110, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001348831

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 892
rank avg (pred): 0.573 +- 0.356
mrr vals (pred, true): 0.101, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001422644

Epoch over!
epoch time: 12.409

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 688
rank avg (pred): 0.410 +- 0.303
mrr vals (pred, true): 0.113, 0.001
batch losses (mrrl, rdl): 0.0, 8.76397e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 211
rank avg (pred): 0.385 +- 0.299
mrr vals (pred, true): 0.095, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001499407

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 703
rank avg (pred): 0.385 +- 0.302
mrr vals (pred, true): 0.103, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001304542

Epoch over!
epoch time: 11.913

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 394
rank avg (pred): 0.426 +- 0.302
mrr vals (pred, true): 0.111, 0.069
batch losses (mrrl, rdl): 0.0368990302, 0.0002239133

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 29
rank avg (pred): 0.018 +- 0.015
mrr vals (pred, true): 0.229, 0.166
batch losses (mrrl, rdl): 0.0402129292, 0.0002984226

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 145
rank avg (pred): 0.423 +- 0.202
mrr vals (pred, true): 0.048, 0.060
batch losses (mrrl, rdl): 3.87566e-05, 0.0002299066

Epoch over!
epoch time: 12.257

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 424
rank avg (pred): 0.469 +- 0.294
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0004145118, 6.5868e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1059
rank avg (pred): 0.029 +- 0.024
mrr vals (pred, true): 0.304, 0.330
batch losses (mrrl, rdl): 0.0063955002, 4.91899e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 762
rank avg (pred): 0.533 +- 0.131
mrr vals (pred, true): 0.038, 0.001
batch losses (mrrl, rdl): 0.0014402473, 0.0001027251

Epoch over!
epoch time: 12.08

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 44
rank avg (pred): 0.123 +- 0.099
mrr vals (pred, true): 0.180, 0.237
batch losses (mrrl, rdl): 0.0321771502, 3.01532e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 576
rank avg (pred): 0.442 +- 0.160
mrr vals (pred, true): 0.048, 0.078
batch losses (mrrl, rdl): 5.72153e-05, 0.0002785462

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 580
rank avg (pred): 0.427 +- 0.172
mrr vals (pred, true): 0.051, 0.058
batch losses (mrrl, rdl): 9.7985e-06, 0.0001654953

Epoch over!
epoch time: 12.091

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 969
rank avg (pred): 0.509 +- 0.143
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 0.0001158602, 7.03062e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 300
rank avg (pred): 0.162 +- 0.128
mrr vals (pred, true): 0.188, 0.142
batch losses (mrrl, rdl): 0.0215229988, 6.08227e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 450
rank avg (pred): 0.420 +- 0.168
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 2.9443e-06, 0.00014612

Epoch over!
epoch time: 11.982

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 974
rank avg (pred): 0.017 +- 0.013
mrr vals (pred, true): 0.292, 0.274
batch losses (mrrl, rdl): 0.0031647526, 9.40574e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 592
rank avg (pred): 0.463 +- 0.117
mrr vals (pred, true): 0.047, 0.072
batch losses (mrrl, rdl): 9.96583e-05, 0.0003373986

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 164
rank avg (pred): 0.419 +- 0.161
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.798e-07, 0.0001170248

Epoch over!
epoch time: 12.054

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1179
rank avg (pred): 0.425 +- 0.195
mrr vals (pred, true): 0.055, 0.048
batch losses (mrrl, rdl): 0.0002121686, 0.0001616987

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 689
rank avg (pred): 0.422 +- 0.166
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 4.836e-07, 0.0001379266

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1076
rank avg (pred): 0.033 +- 0.024
mrr vals (pred, true): 0.236, 0.241
batch losses (mrrl, rdl): 0.0002746143, 0.0001369139

Epoch over!
epoch time: 12.076

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 443
rank avg (pred): 0.410 +- 0.179
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 9.4696e-06, 0.0001620876

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 4
rank avg (pred): 0.004 +- 0.003
mrr vals (pred, true): 0.296, 0.275
batch losses (mrrl, rdl): 0.0043517635, 0.0001065627

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1058
rank avg (pred): 0.171 +- 0.140
mrr vals (pred, true): 0.270, 0.312
batch losses (mrrl, rdl): 0.0171025936, 0.0003012166

Epoch over!
epoch time: 12.025

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 388
rank avg (pred): 0.427 +- 0.180
mrr vals (pred, true): 0.051, 0.054
batch losses (mrrl, rdl): 3.1615e-06, 0.0002237698

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 319
rank avg (pred): 0.063 +- 0.060
mrr vals (pred, true): 0.232, 0.180
batch losses (mrrl, rdl): 0.0278039873, 6.64389e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 45
rank avg (pred): 0.077 +- 0.060
mrr vals (pred, true): 0.241, 0.259
batch losses (mrrl, rdl): 0.0031286343, 7.21052e-05

Epoch over!
epoch time: 12.062

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 559
rank avg (pred): 0.185 +- 0.137
mrr vals (pred, true): 0.195, 0.186
batch losses (mrrl, rdl): 0.0008149461, 0.0001111602

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 868
rank avg (pred): 0.586 +- 0.158
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002321492, 0.0002090971

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1128
rank avg (pred): 0.445 +- 0.179
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.79141e-05, 6.64776e-05

Epoch over!
epoch time: 12.727

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 779
rank avg (pred): 0.551 +- 0.199
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 5.23502e-05, 4.71594e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 965
rank avg (pred): 0.554 +- 0.151
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.38414e-05, 0.0001563239

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 59
rank avg (pred): 0.122 +- 0.097
mrr vals (pred, true): 0.193, 0.152
batch losses (mrrl, rdl): 0.0162662733, 3.32462e-05

Epoch over!
epoch time: 12.552

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.096 +- 0.077
mrr vals (pred, true): 0.191, 0.234

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   75 	     0 	 0.05842 	 0.00048 	 m..s
   43 	     1 	 0.05187 	 0.00048 	 m..s
   42 	     2 	 0.05176 	 0.00049 	 m..s
    9 	     3 	 0.04683 	 0.00049 	 m..s
    1 	     4 	 0.04135 	 0.00051 	 m..s
   24 	     5 	 0.05038 	 0.00051 	 m..s
   15 	     6 	 0.04885 	 0.00051 	 m..s
   32 	     7 	 0.05123 	 0.00051 	 m..s
   36 	     8 	 0.05156 	 0.00051 	 m..s
   21 	     9 	 0.05023 	 0.00052 	 m..s
   25 	    10 	 0.05064 	 0.00053 	 m..s
   44 	    11 	 0.05188 	 0.00054 	 m..s
   41 	    12 	 0.05176 	 0.00055 	 m..s
   77 	    13 	 0.06044 	 0.00055 	 m..s
   17 	    14 	 0.04918 	 0.00055 	 m..s
   10 	    15 	 0.04702 	 0.00056 	 m..s
   37 	    16 	 0.05157 	 0.00056 	 m..s
    8 	    17 	 0.04658 	 0.00056 	 m..s
    0 	    18 	 0.04089 	 0.00057 	 m..s
    4 	    19 	 0.04261 	 0.00058 	 m..s
    3 	    20 	 0.04210 	 0.00058 	 m..s
   16 	    21 	 0.04885 	 0.00059 	 m..s
   73 	    22 	 0.05554 	 0.00061 	 m..s
   33 	    23 	 0.05141 	 0.00061 	 m..s
   30 	    24 	 0.05101 	 0.00063 	 m..s
   66 	    25 	 0.05442 	 0.00063 	 m..s
   50 	    26 	 0.05250 	 0.00063 	 m..s
   12 	    27 	 0.04861 	 0.00065 	 m..s
   67 	    28 	 0.05445 	 0.00066 	 m..s
   68 	    29 	 0.05456 	 0.00067 	 m..s
   18 	    30 	 0.04925 	 0.00068 	 m..s
    5 	    31 	 0.04615 	 0.00071 	 m..s
   65 	    32 	 0.05440 	 0.00074 	 m..s
   14 	    33 	 0.04884 	 0.00075 	 m..s
   23 	    34 	 0.05038 	 0.00081 	 m..s
   51 	    35 	 0.05253 	 0.00082 	 m..s
   76 	    36 	 0.05994 	 0.00085 	 m..s
   34 	    37 	 0.05147 	 0.00087 	 m..s
    7 	    38 	 0.04636 	 0.00088 	 m..s
   52 	    39 	 0.05256 	 0.00089 	 m..s
   64 	    40 	 0.05438 	 0.00092 	 m..s
   13 	    41 	 0.04872 	 0.00099 	 m..s
   20 	    42 	 0.04984 	 0.00112 	 m..s
   26 	    43 	 0.05074 	 0.00118 	 m..s
   63 	    44 	 0.05419 	 0.00171 	 m..s
   31 	    45 	 0.05114 	 0.00209 	 m..s
    6 	    46 	 0.04627 	 0.00211 	 m..s
   35 	    47 	 0.05150 	 0.00255 	 m..s
    2 	    48 	 0.04180 	 0.00292 	 m..s
   87 	    49 	 0.12892 	 0.00320 	 MISS
   85 	    50 	 0.11495 	 0.00359 	 MISS
   19 	    51 	 0.04948 	 0.00462 	 m..s
   82 	    52 	 0.06542 	 0.00464 	 m..s
   53 	    53 	 0.05261 	 0.00628 	 m..s
   83 	    54 	 0.06655 	 0.00878 	 m..s
   84 	    55 	 0.07730 	 0.01048 	 m..s
   86 	    56 	 0.12127 	 0.01708 	 MISS
   71 	    57 	 0.05503 	 0.03790 	 ~...
   60 	    58 	 0.05278 	 0.04147 	 ~...
   48 	    59 	 0.05204 	 0.04869 	 ~...
   39 	    60 	 0.05171 	 0.05053 	 ~...
   45 	    61 	 0.05192 	 0.05194 	 ~...
   72 	    62 	 0.05506 	 0.05226 	 ~...
   69 	    63 	 0.05495 	 0.05291 	 ~...
   56 	    64 	 0.05269 	 0.05544 	 ~...
   29 	    65 	 0.05088 	 0.05545 	 ~...
   81 	    66 	 0.06320 	 0.05554 	 ~...
   28 	    67 	 0.05079 	 0.05587 	 ~...
   59 	    68 	 0.05273 	 0.05658 	 ~...
   70 	    69 	 0.05497 	 0.05816 	 ~...
   74 	    70 	 0.05631 	 0.05927 	 ~...
   61 	    71 	 0.05290 	 0.05968 	 ~...
   58 	    72 	 0.05272 	 0.05993 	 ~...
   40 	    73 	 0.05172 	 0.06011 	 ~...
   11 	    74 	 0.04848 	 0.06248 	 ~...
   80 	    75 	 0.06263 	 0.06257 	 ~...
   49 	    76 	 0.05204 	 0.06321 	 ~...
   38 	    77 	 0.05163 	 0.06446 	 ~...
   46 	    78 	 0.05197 	 0.06447 	 ~...
   47 	    79 	 0.05202 	 0.06524 	 ~...
   27 	    80 	 0.05077 	 0.06653 	 ~...
   78 	    81 	 0.06106 	 0.06657 	 ~...
   79 	    82 	 0.06201 	 0.06717 	 ~...
   54 	    83 	 0.05267 	 0.06783 	 ~...
   57 	    84 	 0.05270 	 0.06828 	 ~...
   55 	    85 	 0.05268 	 0.07128 	 ~...
   22 	    86 	 0.05027 	 0.07485 	 ~...
   62 	    87 	 0.05361 	 0.08523 	 m..s
   89 	    88 	 0.17037 	 0.14153 	 ~...
   92 	    89 	 0.18737 	 0.14511 	 m..s
   96 	    90 	 0.19134 	 0.15060 	 m..s
   88 	    91 	 0.16701 	 0.15453 	 ~...
  101 	    92 	 0.20185 	 0.16162 	 m..s
   90 	    93 	 0.17429 	 0.16306 	 ~...
   91 	    94 	 0.17532 	 0.17126 	 ~...
  100 	    95 	 0.19973 	 0.19054 	 ~...
  102 	    96 	 0.20875 	 0.19335 	 ~...
  112 	    97 	 0.25557 	 0.19936 	 m..s
  105 	    98 	 0.24175 	 0.20101 	 m..s
  108 	    99 	 0.24717 	 0.20387 	 m..s
  103 	   100 	 0.21212 	 0.21193 	 ~...
  104 	   101 	 0.23304 	 0.21315 	 ~...
   97 	   102 	 0.19316 	 0.22069 	 ~...
   95 	   103 	 0.19125 	 0.22141 	 m..s
   98 	   104 	 0.19394 	 0.22229 	 ~...
   99 	   105 	 0.19505 	 0.22454 	 ~...
   93 	   106 	 0.19101 	 0.23436 	 m..s
  110 	   107 	 0.25047 	 0.23673 	 ~...
   94 	   108 	 0.19105 	 0.23907 	 m..s
  113 	   109 	 0.26951 	 0.24977 	 ~...
  109 	   110 	 0.25032 	 0.25221 	 ~...
  115 	   111 	 0.28257 	 0.25365 	 ~...
  106 	   112 	 0.24303 	 0.25762 	 ~...
  114 	   113 	 0.27835 	 0.26576 	 ~...
  107 	   114 	 0.24528 	 0.26783 	 ~...
  111 	   115 	 0.25176 	 0.27373 	 ~...
  116 	   116 	 0.28556 	 0.27518 	 ~...
  117 	   117 	 0.28835 	 0.28573 	 ~...
  118 	   118 	 0.28970 	 0.29863 	 ~...
  120 	   119 	 0.30220 	 0.31837 	 ~...
  119 	   120 	 0.30219 	 0.32380 	 ~...
==========================================
r_mrr = 0.9461801052093506
r2_mrr = 0.8192975521087646
spearmanr_mrr@5 = 0.9681106209754944
spearmanr_mrr@10 = 0.9125217795372009
spearmanr_mrr@50 = 0.9836330413818359
spearmanr_mrr@100 = 0.9764770865440369
spearmanr_mrr@All = 0.9775957465171814
==========================================
test time: 0.396
Done Testing dataset OpenEA
total time taken: 198.85708260536194
training time taken: 183.35133242607117
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9462)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8193)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9681)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9125)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9836)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9765)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9776)}}, 'test_loss': {'DistMult': {'OpenEA': 0.45476483584207017}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4163391549642081
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [99, 118, 141, 185, 545, 371, 343, 178, 584, 105, 576, 360, 1089, 672, 90, 875, 588, 816, 66, 294, 1020, 1137, 884, 1060, 851, 305, 366, 1086, 525, 1107, 170, 89, 70, 1110, 931, 983, 941, 156, 135, 65, 24, 372, 972, 204, 148, 1162, 184, 1075, 817, 74, 181, 597, 233, 1213, 270, 659, 990, 44, 892, 564, 1059, 19, 1080, 359, 571, 589, 802, 918, 812, 868, 325, 562, 695, 137, 977, 784, 1104, 384, 140, 120, 515, 201, 92, 1018, 624, 818, 910, 732, 231, 398, 1091, 67, 1131, 420, 462, 96, 1147, 361, 403, 388, 712, 905, 1156, 149, 858, 35, 529, 841, 1039, 1175, 590, 1056, 650, 1077, 516, 538, 1134, 530, 1132, 772, 308]
valid_ids (0): []
train_ids (1094): [408, 856, 575, 635, 313, 397, 847, 736, 1109, 651, 162, 1173, 58, 441, 880, 1144, 968, 1121, 332, 652, 499, 282, 660, 922, 368, 490, 1016, 47, 20, 988, 203, 131, 994, 456, 460, 862, 84, 240, 832, 748, 730, 451, 1182, 517, 326, 1030, 1076, 284, 257, 520, 215, 155, 123, 463, 111, 714, 81, 121, 951, 791, 1125, 956, 682, 800, 993, 852, 632, 933, 197, 39, 750, 303, 1054, 30, 965, 395, 518, 688, 767, 787, 212, 763, 605, 836, 1153, 324, 1029, 725, 1011, 2, 205, 318, 866, 647, 400, 1120, 206, 1158, 843, 729, 107, 487, 929, 986, 351, 1036, 1145, 604, 970, 154, 536, 1050, 586, 833, 225, 414, 164, 295, 1202, 1035, 432, 393, 1025, 136, 707, 1185, 1200, 312, 641, 160, 298, 0, 10, 283, 467, 996, 1068, 864, 26, 1118, 826, 168, 1196, 1032, 46, 244, 908, 405, 991, 183, 623, 989, 1017, 759, 182, 778, 631, 407, 268, 1177, 1043, 963, 654, 412, 1005, 734, 874, 331, 819, 116, 602, 514, 261, 582, 777, 234, 57, 553, 34, 1122, 637, 345, 378, 265, 599, 1087, 191, 198, 329, 504, 1053, 134, 825, 834, 279, 940, 448, 1006, 942, 895, 363, 1210, 709, 1031, 840, 161, 813, 949, 1151, 238, 354, 967, 731, 43, 72, 1205, 806, 511, 239, 367, 483, 958, 815, 1176, 783, 276, 1096, 78, 999, 565, 510, 879, 327, 698, 848, 452, 127, 1198, 292, 1212, 346, 782, 622, 726, 867, 307, 322, 680, 761, 200, 1214, 1037, 1069, 396, 32, 380, 608, 417, 1099, 104, 566, 861, 323, 98, 1195, 716, 296, 42, 413, 1190, 668, 556, 1171, 513, 176, 560, 373, 913, 364, 755, 321, 959, 1150, 925, 390, 213, 115, 628, 1027, 574, 891, 442, 1057, 63, 992, 100, 1046, 945, 1026, 163, 612, 649, 653, 1186, 438, 465, 581, 435, 541, 1055, 960, 281, 344, 756, 846, 110, 569, 315, 102, 287, 260, 677, 440, 842, 109, 1167, 1061, 543, 665, 1111, 619, 291, 1002, 975, 386, 873, 558, 754, 780, 55, 1040, 850, 655, 310, 697, 190, 717, 916, 216, 152, 86, 489, 610, 6, 1041, 814, 1038, 686, 245, 479, 662, 921, 1090, 890, 209, 1058, 1142, 1088, 609, 894, 876, 596, 337, 503, 319, 938, 129, 500, 97, 254, 739, 1157, 727, 831, 979, 51, 1094, 450, 824, 616, 807, 603, 795, 352, 917, 17, 1166, 468, 93, 693, 1100, 138, 948, 657, 689, 320, 355, 1204, 12, 835, 555, 580, 585, 563, 139, 781, 13, 611, 765, 551, 811, 444, 579, 676, 1045, 304, 334, 227, 255, 27, 1052, 477, 751, 69, 18, 410, 747, 1074, 406, 1085, 108, 669, 839, 786, 60, 906, 330, 379, 445, 1178, 436, 422, 1130, 926, 350, 1082, 274, 1012, 557, 969, 776, 194, 849, 1165, 286, 704, 76, 1115, 694, 1083, 1103, 187, 353, 1188, 188, 593, 117, 333, 1102, 976, 526, 886, 130, 700, 471, 627, 340, 1194, 167, 923, 230, 486, 443, 457, 752, 845, 301, 882, 1070, 722, 885, 498, 382, 1129, 392, 1064, 249, 822, 903, 1169, 935, 914, 171, 173, 22, 928, 626, 68, 218, 1, 280, 1023, 174, 837, 570, 803, 1199, 973, 741, 1143, 1007, 61, 144, 658, 1095, 540, 703, 1119, 470, 1049, 645, 595, 877, 674, 1138, 1097, 370, 246, 431, 646, 667, 259, 705, 642, 869, 1164, 606, 671, 794, 219, 860, 453, 1207, 953, 939, 1141, 1136, 122, 1009, 888, 357, 1133, 253, 25, 224, 328, 528, 636, 980, 1066, 912, 1154, 1191, 8, 512, 491, 1073, 724, 533, 64, 36, 237, 449, 40, 1028, 769, 447, 165, 946, 568, 29, 1187, 893, 132, 53, 547, 666, 808, 683, 347, 537, 293, 620, 1172, 911, 962, 1170, 1098, 23, 54, 484, 768, 770, 87, 419, 1019, 971, 242, 1146, 250, 681, 349, 1042, 241, 670, 944, 1139, 143, 421, 83, 28, 461, 853, 169, 252, 1093, 829, 592, 426, 1003, 454, 428, 101, 472, 544, 263, 844, 644, 774, 1101, 1078, 85, 930, 126, 455, 193, 901, 766, 598, 522, 1116, 362, 519, 936, 369, 402, 961, 809, 495, 915, 125, 1081, 91, 493, 713, 399, 1128, 56, 749, 38, 1001, 1179, 964, 799, 830, 737, 1192, 235, 199, 934, 673, 801, 195, 95, 740, 887, 207, 1208, 550, 920, 177, 974, 424, 572, 473, 31, 735, 299, 760, 661, 621, 7, 899, 663, 220, 706, 356, 497, 718, 1161, 810, 573, 387, 823, 600, 338, 1211, 300, 685, 710, 409, 643, 37, 854, 521, 721, 966, 805, 501, 587, 1108, 711, 317, 863, 466, 878, 383, 1168, 1201, 266, 900, 272, 523, 932, 764, 427, 696, 924, 629, 679, 285, 745, 430, 554, 699, 865, 11, 151, 508, 103, 481, 1063, 335, 625, 1160, 264, 309, 385, 546, 978, 401, 561, 229, 952, 210, 339, 1051, 532, 567, 821, 1044, 273, 243, 857, 228, 62, 88, 640, 1124, 534, 998, 475, 416, 771, 133, 75, 757, 859, 316, 889, 909, 506, 1072, 271, 474, 434, 648, 897, 719, 947, 708, 828, 159, 221, 377, 142, 548, 375, 982, 798, 1155, 728, 179, 404, 1034, 507, 79, 664, 502, 5, 336, 1065, 509, 492, 549, 290, 394, 256, 943, 145, 50, 494, 937, 773, 189, 114, 425, 52, 691, 793, 478, 1127, 476, 753, 871, 723, 1180, 262, 1206, 77, 1010, 630, 838, 775, 1000, 870, 1106, 656, 788, 1092, 790, 855, 1024, 746, 1135, 71, 1071, 236, 957, 358, 59, 701, 480, 577, 94, 439, 1184, 226, 927, 302, 1203, 211, 1048, 690, 147, 898, 1197, 469, 1014, 1193, 995, 289, 128, 157, 146, 1008, 247, 1126, 954, 985, 415, 45, 1163, 559, 119, 1149, 80, 376, 275, 269, 437, 881, 223, 459, 984, 1209, 1112, 180, 684, 175, 618, 202, 41, 1123, 423, 997, 381, 594, 1033, 365, 524, 3, 827, 488, 1183, 1140, 374, 702, 1062, 675, 633, 389, 542, 527, 1004, 1079, 715, 617, 539, 904, 196, 429, 797, 464, 1117, 446, 634, 591, 48, 907, 678, 614, 505, 742, 158, 692, 613, 106, 981, 872, 150, 1148, 222, 15, 433, 758, 1084, 1174, 902, 601, 391, 1114, 531, 820, 277, 1067, 792, 278, 987, 306, 16, 919, 789, 348, 779, 955, 314, 1105, 1181, 733, 267, 153, 112, 1159, 166, 796, 418, 639, 762, 192, 124, 341, 342, 1021, 552, 583, 21, 49, 33, 744, 248, 4, 578, 1152, 687, 411, 311, 1013, 535, 258, 785, 1113, 14, 297, 896, 186, 615, 251, 208, 485, 1015, 804, 607, 1189, 720, 9, 113, 1047, 496, 458, 217, 288, 214, 883, 950, 1022, 82, 172, 73, 738, 232, 482, 743, 638]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2659160883128310
the save name prefix for this run is:  chkpt-ID_2659160883128310_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 187
rank avg (pred): 0.534 +- 0.005
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.000154389

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 502
rank avg (pred): 0.240 +- 0.021
mrr vals (pred, true): 0.000, 0.266
batch losses (mrrl, rdl): 0.0, 0.0001535831

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 585
rank avg (pred): 0.459 +- 0.308
mrr vals (pred, true): 0.023, 0.062
batch losses (mrrl, rdl): 0.0, 0.0001808686

Epoch over!
epoch time: 12.288

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 598
rank avg (pred): 0.422 +- 0.311
mrr vals (pred, true): 0.027, 0.046
batch losses (mrrl, rdl): 0.0, 2.6605e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 376
rank avg (pred): 0.421 +- 0.312
mrr vals (pred, true): 0.030, 0.068
batch losses (mrrl, rdl): 0.0, 9.68911e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 482
rank avg (pred): 0.435 +- 0.319
mrr vals (pred, true): 0.032, 0.001
batch losses (mrrl, rdl): 0.0, 6.52209e-05

Epoch over!
epoch time: 11.933

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 856
rank avg (pred): 0.465 +- 0.318
mrr vals (pred, true): 0.033, 0.006
batch losses (mrrl, rdl): 0.0, 1.86756e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 513
rank avg (pred): 0.201 +- 0.270
mrr vals (pred, true): 0.048, 0.131
batch losses (mrrl, rdl): 0.0, 2.04255e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 152
rank avg (pred): 0.438 +- 0.308
mrr vals (pred, true): 0.034, 0.055
batch losses (mrrl, rdl): 0.0, 0.0001044035

Epoch over!
epoch time: 11.997

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 724
rank avg (pred): 0.449 +- 0.312
mrr vals (pred, true): 0.034, 0.001
batch losses (mrrl, rdl): 0.0, 3.27344e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1130
rank avg (pred): 0.415 +- 0.323
mrr vals (pred, true): 0.032, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001066638

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 448
rank avg (pred): 0.430 +- 0.314
mrr vals (pred, true): 0.025, 0.001
batch losses (mrrl, rdl): 0.0, 8.85911e-05

Epoch over!
epoch time: 12.014

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1114
rank avg (pred): 0.410 +- 0.296
mrr vals (pred, true): 0.022, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001346501

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 394
rank avg (pred): 0.427 +- 0.310
mrr vals (pred, true): 0.019, 0.069
batch losses (mrrl, rdl): 0.0, 0.0001586475

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 702
rank avg (pred): 0.415 +- 0.315
mrr vals (pred, true): 0.022, 0.001
batch losses (mrrl, rdl): 0.0, 9.82843e-05

Epoch over!
epoch time: 11.872

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1047
rank avg (pred): 0.429 +- 0.339
mrr vals (pred, true): 0.022, 0.001
batch losses (mrrl, rdl): 0.0076698582, 6.46894e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 708
rank avg (pred): 0.415 +- 0.252
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 6.4856e-06, 6.6366e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 744
rank avg (pred): 0.296 +- 0.281
mrr vals (pred, true): 0.120, 0.138
batch losses (mrrl, rdl): 0.0033011395, 0.0002997389

Epoch over!
epoch time: 12.239

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 374
rank avg (pred): 0.411 +- 0.238
mrr vals (pred, true): 0.049, 0.056
batch losses (mrrl, rdl): 1.97498e-05, 0.0001753519

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1021
rank avg (pred): 0.414 +- 0.243
mrr vals (pred, true): 0.051, 0.068
batch losses (mrrl, rdl): 2.5392e-06, 0.0001857219

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 783
rank avg (pred): 0.671 +- 0.232
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0011896009, 0.000595108

Epoch over!
epoch time: 12.199

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 843
rank avg (pred): 0.699 +- 0.229
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0020870504, 0.0007812727

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1094
rank avg (pred): 0.415 +- 0.239
mrr vals (pred, true): 0.053, 0.063
batch losses (mrrl, rdl): 7.24854e-05, 0.0001770939

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1185
rank avg (pred): 0.443 +- 0.251
mrr vals (pred, true): 0.051, 0.061
batch losses (mrrl, rdl): 1.94551e-05, 0.0002184185

Epoch over!
epoch time: 12.086

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 291
rank avg (pred): 0.183 +- 0.214
mrr vals (pred, true): 0.257, 0.268
batch losses (mrrl, rdl): 0.0012334884, 0.0001724186

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 614
rank avg (pred): 0.412 +- 0.206
mrr vals (pred, true): 0.055, 0.058
batch losses (mrrl, rdl): 0.0002443994, 0.0001505214

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 893
rank avg (pred): 0.542 +- 0.226
mrr vals (pred, true): 0.050, 0.003
batch losses (mrrl, rdl): 1.9171e-06, 0.0001355511

Epoch over!
epoch time: 12.048

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 469
rank avg (pred): 0.409 +- 0.197
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.69516e-05, 0.0001253111

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1051
rank avg (pred): 0.403 +- 0.209
mrr vals (pred, true): 0.056, 0.001
batch losses (mrrl, rdl): 0.0003800683, 0.000145411

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 913
rank avg (pred): 0.429 +- 0.182
mrr vals (pred, true): 0.051, 0.013
batch losses (mrrl, rdl): 2.17492e-05, 0.0002361871

Epoch over!
epoch time: 12.193

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1146
rank avg (pred): 0.167 +- 0.197
mrr vals (pred, true): 0.218, 0.212
batch losses (mrrl, rdl): 0.0003092267, 7.50085e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1177
rank avg (pred): 0.442 +- 0.200
mrr vals (pred, true): 0.051, 0.067
batch losses (mrrl, rdl): 4.6643e-06, 0.0002298375

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 750
rank avg (pred): 0.277 +- 0.216
mrr vals (pred, true): 0.142, 0.144
batch losses (mrrl, rdl): 3.55361e-05, 0.0003819583

Epoch over!
epoch time: 12.206

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 114
rank avg (pred): 0.440 +- 0.215
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 8.41346e-05, 0.0001859772

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 625
rank avg (pred): 0.436 +- 0.211
mrr vals (pred, true): 0.051, 0.053
batch losses (mrrl, rdl): 1.07063e-05, 0.000158042

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1033
rank avg (pred): 0.473 +- 0.223
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.84609e-05, 3.73532e-05

Epoch over!
epoch time: 12.28

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 396
rank avg (pred): 0.433 +- 0.214
mrr vals (pred, true): 0.051, 0.063
batch losses (mrrl, rdl): 1.78229e-05, 0.0002676425

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1105
rank avg (pred): 0.396 +- 0.214
mrr vals (pred, true): 0.059, 0.080
batch losses (mrrl, rdl): 0.0007991141, 0.0001478953

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 91
rank avg (pred): 0.414 +- 0.210
mrr vals (pred, true): 0.052, 0.077
batch losses (mrrl, rdl): 3.73229e-05, 0.0001758951

Epoch over!
epoch time: 12.213

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1109
rank avg (pred): 0.442 +- 0.213
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 6.6567e-06, 0.0001006382

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 31
rank avg (pred): 0.190 +- 0.235
mrr vals (pred, true): 0.183, 0.152
batch losses (mrrl, rdl): 0.0092604952, 8.04901e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 186
rank avg (pred): 0.420 +- 0.227
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 2.6885e-06, 0.0001205683

Epoch over!
epoch time: 12.284

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 395
rank avg (pred): 0.458 +- 0.262
mrr vals (pred, true): 0.049, 0.053
batch losses (mrrl, rdl): 1.43251e-05, 0.0002015837

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 551
rank avg (pred): 0.260 +- 0.208
mrr vals (pred, true): 0.170, 0.174
batch losses (mrrl, rdl): 0.0001262404, 6.33699e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 296
rank avg (pred): 0.112 +- 0.203
mrr vals (pred, true): 0.239, 0.204
batch losses (mrrl, rdl): 0.012335198, 5.5195e-06

Epoch over!
epoch time: 12.315

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.420 +- 0.247
mrr vals (pred, true): 0.049, 0.072

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04761 	 0.00011 	 m..s
    5 	     1 	 0.04766 	 0.00012 	 m..s
    6 	     2 	 0.04773 	 0.00027 	 m..s
   83 	     3 	 0.05397 	 0.00049 	 m..s
   34 	     4 	 0.04967 	 0.00049 	 m..s
    0 	     5 	 0.04625 	 0.00054 	 m..s
   37 	     6 	 0.05038 	 0.00055 	 m..s
   33 	     7 	 0.04967 	 0.00055 	 m..s
    7 	     8 	 0.04797 	 0.00058 	 m..s
   38 	     9 	 0.05039 	 0.00060 	 m..s
   38 	    10 	 0.05039 	 0.00061 	 m..s
    3 	    11 	 0.04727 	 0.00061 	 m..s
    9 	    12 	 0.04825 	 0.00061 	 m..s
   27 	    13 	 0.04935 	 0.00063 	 m..s
   81 	    14 	 0.05297 	 0.00064 	 m..s
   72 	    15 	 0.05137 	 0.00064 	 m..s
   18 	    16 	 0.04872 	 0.00065 	 m..s
   38 	    17 	 0.05039 	 0.00068 	 m..s
   38 	    18 	 0.05039 	 0.00069 	 m..s
   16 	    19 	 0.04870 	 0.00073 	 m..s
   65 	    20 	 0.05077 	 0.00073 	 m..s
   21 	    21 	 0.04905 	 0.00073 	 m..s
   13 	    22 	 0.04865 	 0.00075 	 m..s
   80 	    23 	 0.05297 	 0.00077 	 m..s
   29 	    24 	 0.04938 	 0.00078 	 m..s
   38 	    25 	 0.05039 	 0.00082 	 m..s
   38 	    26 	 0.05039 	 0.00084 	 m..s
    2 	    27 	 0.04724 	 0.00086 	 m..s
    1 	    28 	 0.04710 	 0.00088 	 m..s
   22 	    29 	 0.04906 	 0.00091 	 m..s
   26 	    30 	 0.04933 	 0.00118 	 m..s
   32 	    31 	 0.04956 	 0.00126 	 m..s
   25 	    32 	 0.04917 	 0.00145 	 m..s
   15 	    33 	 0.04869 	 0.00294 	 m..s
   79 	    34 	 0.05262 	 0.00438 	 m..s
   70 	    35 	 0.05122 	 0.00439 	 m..s
    8 	    36 	 0.04810 	 0.00458 	 m..s
   68 	    37 	 0.05091 	 0.00464 	 m..s
   76 	    38 	 0.05175 	 0.00718 	 m..s
   78 	    39 	 0.05213 	 0.00925 	 m..s
   35 	    40 	 0.05002 	 0.01708 	 m..s
   38 	    41 	 0.05039 	 0.04001 	 ~...
   38 	    42 	 0.05039 	 0.04044 	 ~...
   38 	    43 	 0.05039 	 0.04317 	 ~...
   38 	    44 	 0.05039 	 0.04348 	 ~...
   38 	    45 	 0.05039 	 0.04670 	 ~...
   73 	    46 	 0.05140 	 0.04831 	 ~...
   38 	    47 	 0.05039 	 0.04955 	 ~...
   38 	    48 	 0.05039 	 0.05053 	 ~...
   38 	    49 	 0.05039 	 0.05064 	 ~...
   38 	    50 	 0.05039 	 0.05080 	 ~...
   66 	    51 	 0.05080 	 0.05194 	 ~...
   38 	    52 	 0.05039 	 0.05240 	 ~...
   84 	    53 	 0.05403 	 0.05378 	 ~...
   38 	    54 	 0.05039 	 0.05445 	 ~...
   38 	    55 	 0.05039 	 0.05445 	 ~...
   77 	    56 	 0.05197 	 0.05563 	 ~...
   85 	    57 	 0.05769 	 0.05583 	 ~...
   14 	    58 	 0.04868 	 0.05652 	 ~...
   69 	    59 	 0.05113 	 0.05689 	 ~...
   38 	    60 	 0.05039 	 0.05707 	 ~...
   38 	    61 	 0.05039 	 0.05778 	 ~...
   38 	    62 	 0.05039 	 0.05816 	 ~...
   38 	    63 	 0.05039 	 0.05833 	 ~...
   82 	    64 	 0.05366 	 0.05965 	 ~...
   23 	    65 	 0.04911 	 0.06143 	 ~...
   24 	    66 	 0.04916 	 0.06187 	 ~...
   75 	    67 	 0.05163 	 0.06297 	 ~...
   38 	    68 	 0.05039 	 0.06298 	 ~...
   11 	    69 	 0.04855 	 0.06457 	 ~...
   74 	    70 	 0.05148 	 0.06492 	 ~...
   38 	    71 	 0.05039 	 0.06524 	 ~...
   64 	    72 	 0.05047 	 0.06615 	 ~...
   38 	    73 	 0.05039 	 0.06653 	 ~...
   30 	    74 	 0.04946 	 0.06697 	 ~...
   28 	    75 	 0.04936 	 0.07170 	 ~...
   38 	    76 	 0.05039 	 0.07174 	 ~...
   19 	    77 	 0.04872 	 0.07310 	 ~...
   36 	    78 	 0.05029 	 0.07485 	 ~...
   20 	    79 	 0.04878 	 0.07508 	 ~...
   67 	    80 	 0.05087 	 0.07577 	 ~...
   12 	    81 	 0.04865 	 0.07637 	 ~...
   10 	    82 	 0.04852 	 0.07821 	 ~...
   71 	    83 	 0.05131 	 0.07943 	 ~...
   31 	    84 	 0.04952 	 0.08454 	 m..s
   17 	    85 	 0.04870 	 0.08886 	 m..s
   87 	    86 	 0.14735 	 0.13113 	 ~...
   88 	    87 	 0.14801 	 0.13955 	 ~...
   86 	    88 	 0.14592 	 0.14379 	 ~...
   93 	    89 	 0.18180 	 0.14839 	 m..s
   92 	    90 	 0.17913 	 0.15863 	 ~...
   94 	    91 	 0.18183 	 0.15883 	 ~...
   90 	    92 	 0.15711 	 0.16887 	 ~...
  100 	    93 	 0.18429 	 0.17301 	 ~...
   98 	    94 	 0.18369 	 0.17309 	 ~...
   89 	    95 	 0.15699 	 0.17635 	 ~...
   91 	    96 	 0.15727 	 0.17794 	 ~...
   97 	    97 	 0.18304 	 0.17924 	 ~...
  107 	    98 	 0.22569 	 0.19249 	 m..s
  103 	    99 	 0.18500 	 0.19335 	 ~...
  106 	   100 	 0.21998 	 0.19528 	 ~...
   96 	   101 	 0.18299 	 0.20696 	 ~...
   99 	   102 	 0.18387 	 0.21003 	 ~...
  104 	   103 	 0.18527 	 0.21487 	 ~...
  102 	   104 	 0.18452 	 0.21898 	 m..s
   95 	   105 	 0.18266 	 0.22069 	 m..s
  108 	   106 	 0.23010 	 0.22435 	 ~...
  105 	   107 	 0.18666 	 0.22740 	 m..s
  109 	   108 	 0.23933 	 0.23545 	 ~...
  101 	   109 	 0.18445 	 0.23716 	 m..s
  113 	   110 	 0.26927 	 0.26056 	 ~...
  112 	   111 	 0.26624 	 0.26952 	 ~...
  114 	   112 	 0.29243 	 0.26974 	 ~...
  111 	   113 	 0.26517 	 0.27105 	 ~...
  110 	   114 	 0.24082 	 0.27373 	 m..s
  118 	   115 	 0.30058 	 0.30244 	 ~...
  116 	   116 	 0.29824 	 0.30463 	 ~...
  115 	   117 	 0.29823 	 0.30519 	 ~...
  117 	   118 	 0.29929 	 0.31362 	 ~...
  120 	   119 	 0.31593 	 0.32380 	 ~...
  119 	   120 	 0.31472 	 0.32969 	 ~...
==========================================
r_mrr = 0.9533174633979797
r2_mrr = 0.8858360648155212
spearmanr_mrr@5 = 0.9605446457862854
spearmanr_mrr@10 = 0.9313347339630127
spearmanr_mrr@50 = 0.9889946579933167
spearmanr_mrr@100 = 0.9687634110450745
spearmanr_mrr@All = 0.9620674848556519
==========================================
test time: 0.669
Done Testing dataset OpenEA
total time taken: 197.63729643821716
training time taken: 182.89822673797607
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9533)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8858)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9605)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9313)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9890)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9688)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9621)}}, 'test_loss': {'DistMult': {'OpenEA': 0.19214262267269078}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 4815533610657581
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [246, 952, 259, 564, 92, 1210, 994, 742, 110, 1171, 1143, 215, 580, 262, 623, 425, 21, 490, 229, 3, 178, 610, 1068, 1057, 25, 249, 281, 650, 790, 993, 1063, 431, 1033, 1008, 266, 112, 157, 1162, 1211, 627, 755, 765, 603, 424, 908, 822, 1021, 678, 1193, 740, 1098, 975, 657, 582, 950, 270, 343, 197, 34, 486, 322, 593, 273, 234, 1016, 460, 293, 642, 918, 662, 708, 988, 959, 156, 668, 739, 344, 379, 94, 934, 52, 992, 557, 601, 254, 520, 962, 541, 922, 658, 677, 581, 48, 910, 146, 64, 130, 1155, 764, 5, 168, 572, 165, 1093, 535, 1133, 15, 1074, 998, 127, 1131, 1061, 365, 944, 473, 1109, 892, 710, 268, 751, 1005]
valid_ids (0): []
train_ids (1094): [219, 760, 715, 1026, 149, 1213, 57, 702, 134, 615, 345, 480, 212, 1060, 835, 301, 394, 114, 855, 663, 653, 537, 1092, 1097, 88, 479, 831, 1132, 47, 628, 1121, 218, 575, 781, 488, 420, 984, 83, 9, 1066, 107, 498, 524, 1158, 1078, 275, 95, 300, 1001, 550, 738, 961, 442, 812, 70, 1043, 566, 629, 217, 186, 871, 693, 870, 290, 417, 1204, 1056, 438, 45, 435, 184, 966, 1198, 144, 841, 309, 400, 848, 508, 965, 1180, 607, 754, 291, 26, 354, 141, 940, 792, 1000, 546, 443, 357, 298, 1095, 898, 261, 151, 421, 671, 687, 86, 74, 507, 0, 265, 482, 771, 885, 469, 609, 888, 744, 447, 849, 103, 277, 904, 113, 779, 489, 314, 690, 163, 466, 982, 1176, 643, 495, 311, 1054, 1117, 717, 203, 890, 748, 308, 866, 857, 865, 821, 619, 87, 987, 830, 378, 875, 280, 207, 883, 1019, 478, 98, 891, 1208, 785, 1029, 85, 1126, 963, 237, 328, 78, 100, 1114, 423, 932, 384, 1062, 720, 803, 1045, 971, 660, 173, 1144, 12, 929, 373, 366, 2, 1166, 823, 576, 1183, 410, 382, 774, 879, 757, 1138, 787, 313, 39, 169, 376, 351, 882, 459, 145, 172, 91, 729, 712, 24, 29, 17, 1172, 1091, 463, 768, 968, 1173, 53, 894, 1145, 705, 521, 1018, 493, 1163, 125, 868, 540, 419, 121, 859, 986, 182, 450, 725, 170, 716, 1152, 224, 287, 63, 1202, 321, 383, 1165, 105, 1125, 1207, 35, 93, 18, 4, 867, 636, 1042, 513, 719, 1123, 617, 612, 1004, 342, 304, 198, 1148, 625, 844, 230, 776, 798, 1002, 483, 590, 302, 369, 363, 1147, 96, 555, 737, 356, 1041, 913, 468, 786, 797, 858, 56, 955, 377, 104, 251, 1159, 210, 359, 766, 728, 752, 534, 303, 679, 811, 1113, 554, 1201, 205, 721, 750, 462, 645, 532, 245, 808, 334, 109, 414, 1070, 1122, 856, 1178, 333, 452, 630, 142, 574, 1164, 842, 272, 772, 81, 887, 282, 364, 166, 746, 549, 7, 264, 1059, 602, 990, 1096, 135, 850, 767, 1187, 128, 506, 1194, 689, 701, 1003, 214, 139, 604, 23, 533, 241, 189, 16, 296, 516, 375, 519, 976, 122, 390, 225, 1076, 396, 22, 565, 531, 732, 547, 618, 267, 248, 1081, 640, 299, 1177, 133, 880, 499, 718, 353, 408, 500, 1013, 621, 77, 350, 89, 73, 67, 526, 683, 514, 682, 818, 1052, 881, 349, 46, 773, 1119, 152, 733, 55, 399, 324, 51, 132, 957, 54, 14, 11, 919, 1214, 1046, 1129, 634, 434, 905, 1101, 84, 544, 530, 568, 1192, 412, 1058, 199, 795, 131, 999, 209, 320, 921, 522, 115, 387, 405, 295, 876, 845, 348, 1038, 430, 1174, 644, 228, 372, 639, 253, 699, 27, 1009, 759, 911, 256, 886, 1146, 1151, 852, 902, 806, 1007, 1141, 76, 188, 896, 1032, 588, 1084, 30, 553, 154, 1099, 1047, 255, 780, 579, 325, 415, 641, 174, 980, 1195, 1127, 1137, 872, 727, 614, 318, 525, 567, 825, 223, 595, 1161, 936, 561, 160, 611, 698, 815, 793, 920, 801, 392, 1006, 1079, 1136, 613, 153, 406, 1206, 161, 43, 310, 99, 445, 654, 1088, 691, 1011, 1154, 448, 10, 429, 745, 380, 1010, 928, 1055, 492, 843, 1209, 711, 809, 232, 1139, 362, 32, 502, 1196, 196, 707, 1075, 545, 1106, 997, 72, 222, 402, 370, 457, 813, 200, 931, 1182, 374, 407, 206, 860, 620, 973, 204, 509, 1031, 454, 758, 220, 187, 583, 158, 191, 368, 464, 167, 829, 80, 472, 909, 433, 1130, 247, 1035, 926, 924, 50, 647, 775, 700, 242, 937, 42, 227, 724, 969, 58, 735, 893, 1087, 297, 527, 573, 1024, 1150, 346, 594, 985, 585, 504, 596, 1199, 233, 861, 907, 1184, 659, 202, 529, 586, 782, 260, 60, 386, 709, 1175, 1053, 147, 942, 467, 958, 578, 41, 978, 484, 1179, 238, 970, 951, 176, 824, 75, 385, 960, 1181, 1025, 355, 938, 827, 477, 305, 1080, 800, 213, 171, 116, 817, 1103, 180, 1083, 626, 862, 1190, 129, 61, 159, 1157, 675, 676, 646, 292, 672, 179, 1128, 286, 393, 927, 743, 539, 111, 226, 1102, 221, 68, 804, 1167, 140, 244, 79, 807, 941, 991, 637, 558, 1034, 749, 1115, 622, 945, 632, 1090, 306, 914, 401, 599, 736, 404, 470, 1086, 126, 597, 446, 570, 1212, 784, 331, 1064, 38, 69, 552, 956, 317, 1124, 193, 1108, 118, 326, 563, 416, 802, 847, 1140, 571, 917, 106, 703, 589, 694, 1170, 954, 923, 1, 605, 136, 820, 669, 826, 162, 741, 258, 428, 953, 901, 458, 474, 608, 819, 651, 1089, 491, 933, 455, 1065, 706, 930, 36, 1200, 235, 395, 269, 681, 8, 243, 461, 1189, 195, 475, 816, 481, 427, 1094, 1153, 398, 981, 97, 884, 667, 528, 471, 633, 897, 236, 624, 1205, 391, 1023, 889, 316, 551, 1082, 1085, 799, 1168, 686, 1120, 1160, 389, 330, 794, 211, 692, 684, 323, 912, 515, 285, 1051, 44, 1048, 426, 878, 638, 964, 763, 1028, 403, 65, 441, 307, 388, 895, 777, 935, 352, 216, 497, 695, 190, 577, 358, 503, 837, 789, 381, 979, 949, 332, 102, 494, 680, 274, 1203, 769, 730, 1135, 543, 90, 347, 1110, 397, 338, 164, 796, 279, 836, 451, 906, 119, 1037, 239, 834, 1191, 148, 231, 194, 512, 974, 1039, 143, 900, 432, 723, 439, 335, 1100, 569, 833, 201, 704, 722, 559, 150, 449, 713, 1112, 117, 1077, 538, 697, 252, 283, 371, 476, 840, 674, 40, 562, 1036, 66, 13, 289, 869, 120, 123, 548, 1022, 485, 838, 327, 1030, 1067, 465, 496, 319, 972, 696, 284, 591, 989, 183, 832, 1044, 600, 903, 814, 155, 437, 517, 805, 294, 943, 33, 436, 59, 846, 340, 864, 731, 422, 853, 560, 409, 863, 606, 839, 616, 747, 1116, 584, 288, 453, 1071, 670, 598, 649, 1069, 665, 874, 761, 208, 983, 339, 271, 948, 877, 1149, 501, 337, 1012, 315, 181, 518, 542, 413, 361, 810, 329, 1169, 177, 240, 341, 1107, 418, 1111, 635, 977, 1027, 185, 278, 263, 854, 31, 756, 946, 1104, 71, 440, 360, 1017, 124, 411, 20, 192, 666, 661, 685, 631, 1073, 1015, 536, 648, 947, 37, 788, 276, 510, 108, 1142, 367, 652, 783, 556, 456, 137, 1156, 1049, 312, 511, 28, 967, 664, 673, 1020, 336, 995, 1197, 851, 778, 791, 82, 1185, 762, 49, 175, 250, 916, 925, 1186, 828, 1072, 138, 656, 1188, 996, 726, 1050, 753, 62, 1040, 915, 505, 1118, 6, 873, 1105, 523, 444, 939, 257, 655, 714, 592, 688, 487, 734, 19, 1014, 770, 101, 587, 1134, 899]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9632047060569872
the save name prefix for this run is:  chkpt-ID_9632047060569872_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 733
rank avg (pred): 0.562 +- 0.005
mrr vals (pred, true): 0.000, 0.004
batch losses (mrrl, rdl): 0.0, 0.0012498896

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 722
rank avg (pred): 0.401 +- 0.282
mrr vals (pred, true): 0.110, 0.001
batch losses (mrrl, rdl): 0.0, 8.22653e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 721
rank avg (pred): 0.404 +- 0.296
mrr vals (pred, true): 0.142, 0.001
batch losses (mrrl, rdl): 0.0, 8.15233e-05

Epoch over!
epoch time: 12.109

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 42
rank avg (pred): 0.128 +- 0.099
mrr vals (pred, true): 0.173, 0.203
batch losses (mrrl, rdl): 0.0, 3.54706e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 20
rank avg (pred): 0.086 +- 0.068
mrr vals (pred, true): 0.172, 0.308
batch losses (mrrl, rdl): 0.0, 2.26742e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1064
rank avg (pred): 0.074 +- 0.058
mrr vals (pred, true): 0.181, 0.227
batch losses (mrrl, rdl): 0.0, 4.63581e-05

Epoch over!
epoch time: 12.022

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 768
rank avg (pred): 0.484 +- 0.331
mrr vals (pred, true): 0.132, 0.001
batch losses (mrrl, rdl): 0.0, 6.3699e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 930
rank avg (pred): 0.534 +- 0.354
mrr vals (pred, true): 0.124, 0.000
batch losses (mrrl, rdl): 0.0, 0.0020226121

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1198
rank avg (pred): 0.384 +- 0.292
mrr vals (pred, true): 0.143, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001290148

Epoch over!
epoch time: 11.974

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 288
rank avg (pred): 0.085 +- 0.064
mrr vals (pred, true): 0.160, 0.258
batch losses (mrrl, rdl): 0.0, 4.49947e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 209
rank avg (pred): 0.434 +- 0.315
mrr vals (pred, true): 0.127, 0.001
batch losses (mrrl, rdl): 0.0, 2.80251e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1166
rank avg (pred): 0.364 +- 0.291
mrr vals (pred, true): 0.144, 0.067
batch losses (mrrl, rdl): 0.0, 3.63735e-05

Epoch over!
epoch time: 11.977

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 77
rank avg (pred): 0.091 +- 0.073
mrr vals (pred, true): 0.162, 0.232
batch losses (mrrl, rdl): 0.0, 4.92742e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1082
rank avg (pred): 0.377 +- 0.296
mrr vals (pred, true): 0.132, 0.057
batch losses (mrrl, rdl): 0.0, 4.60049e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 357
rank avg (pred): 0.406 +- 0.302
mrr vals (pred, true): 0.116, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001364186

Epoch over!
epoch time: 12.013

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 946
rank avg (pred): 0.569 +- 0.349
mrr vals (pred, true): 0.092, 0.001
batch losses (mrrl, rdl): 0.0176456459, 0.0002138754

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1060
rank avg (pred): 0.000 +- 0.000
mrr vals (pred, true): 0.390, 0.324
batch losses (mrrl, rdl): 0.0432242416, 0.0001294022

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 54
rank avg (pred): 0.025 +- 0.018
mrr vals (pred, true): 0.181, 0.136
batch losses (mrrl, rdl): 0.0208053812, 0.0003400122

Epoch over!
epoch time: 12.303

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 638
rank avg (pred): 0.440 +- 0.152
mrr vals (pred, true): 0.049, 0.060
batch losses (mrrl, rdl): 1.37946e-05, 0.0002672608

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 338
rank avg (pred): 0.435 +- 0.147
mrr vals (pred, true): 0.048, 0.069
batch losses (mrrl, rdl): 2.46098e-05, 0.0002823269

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 971
rank avg (pred): 0.519 +- 0.171
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.78121e-05, 6.48891e-05

Epoch over!
epoch time: 11.986

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 65
rank avg (pred): 0.027 +- 0.020
mrr vals (pred, true): 0.182, 0.221
batch losses (mrrl, rdl): 0.0146551998, 0.0002117819

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 913
rank avg (pred): 0.407 +- 0.226
mrr vals (pred, true): 0.083, 0.013
batch losses (mrrl, rdl): 0.0108887292, 0.0002216265

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 982
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.216, 0.205
batch losses (mrrl, rdl): 0.0012162481, 0.000300855

Epoch over!
epoch time: 12.975

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 328
rank avg (pred): 0.419 +- 0.153
mrr vals (pred, true): 0.053, 0.052
batch losses (mrrl, rdl): 6.38164e-05, 0.0002088862

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 598
rank avg (pred): 0.425 +- 0.148
mrr vals (pred, true): 0.049, 0.046
batch losses (mrrl, rdl): 6.1457e-06, 0.0001495677

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 179
rank avg (pred): 0.422 +- 0.149
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.01135e-05, 0.0001416673

Epoch over!
epoch time: 12.369

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 808
rank avg (pred): 0.503 +- 0.188
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.80567e-05, 4.45748e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 353
rank avg (pred): 0.435 +- 0.124
mrr vals (pred, true): 0.043, 0.054
batch losses (mrrl, rdl): 0.0004340391, 0.0002285856

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 920
rank avg (pred): 0.670 +- 0.210
mrr vals (pred, true): 0.034, 0.004
batch losses (mrrl, rdl): 0.0026910268, 0.0005361242

Epoch over!
epoch time: 12.495

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 587
rank avg (pred): 0.407 +- 0.154
mrr vals (pred, true): 0.055, 0.066
batch losses (mrrl, rdl): 0.0002745015, 0.0001768179

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 960
rank avg (pred): 0.631 +- 0.224
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002797612, 0.0004453309

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.418 +- 0.139
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.62383e-05, 0.0001788063

Epoch over!
epoch time: 12.553

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 240
rank avg (pred): 0.421 +- 0.137
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.69049e-05, 0.0001810762

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 555
rank avg (pred): 0.288 +- 0.201
mrr vals (pred, true): 0.139, 0.154
batch losses (mrrl, rdl): 0.002352987, 0.0001451713

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 280
rank avg (pred): 0.092 +- 0.066
mrr vals (pred, true): 0.211, 0.234
batch losses (mrrl, rdl): 0.005239774, 2.63199e-05

Epoch over!
epoch time: 12.167

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 602
rank avg (pred): 0.418 +- 0.142
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 9e-10, 0.0001639125

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 744
rank avg (pred): 0.149 +- 0.102
mrr vals (pred, true): 0.165, 0.138
batch losses (mrrl, rdl): 0.0073692696, 0.0001619548

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 24
rank avg (pred): 0.033 +- 0.024
mrr vals (pred, true): 0.301, 0.302
batch losses (mrrl, rdl): 1.39442e-05, 3.73777e-05

Epoch over!
epoch time: 12.099

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1168
rank avg (pred): 0.471 +- 0.115
mrr vals (pred, true): 0.042, 0.080
batch losses (mrrl, rdl): 0.0006601284, 0.0004432654

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 192
rank avg (pred): 0.428 +- 0.137
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.5329e-06, 0.0001428932

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 929
rank avg (pred): 0.676 +- 0.223
mrr vals (pred, true): 0.044, 0.005
batch losses (mrrl, rdl): 0.0003450703, 0.000415617

Epoch over!
epoch time: 12.14

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 306
rank avg (pred): 0.080 +- 0.055
mrr vals (pred, true): 0.184, 0.214
batch losses (mrrl, rdl): 0.0090973498, 4.74138e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1075
rank avg (pred): 0.112 +- 0.079
mrr vals (pred, true): 0.237, 0.235
batch losses (mrrl, rdl): 1.54952e-05, 2.96111e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 68
rank avg (pred): 0.121 +- 0.082
mrr vals (pred, true): 0.170, 0.227
batch losses (mrrl, rdl): 0.0324496292, 3.22579e-05

Epoch over!
epoch time: 12.823

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.283, 0.279

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   12 	     0 	 0.04710 	 8e-0500 	 m..s
   14 	     1 	 0.04761 	 0.00013 	 m..s
    1 	     2 	 0.03386 	 0.00038 	 m..s
    3 	     3 	 0.03451 	 0.00048 	 m..s
   49 	     4 	 0.05314 	 0.00049 	 m..s
   19 	     5 	 0.05001 	 0.00053 	 m..s
   50 	     6 	 0.05360 	 0.00053 	 m..s
    0 	     7 	 0.03369 	 0.00054 	 m..s
    8 	     8 	 0.04561 	 0.00055 	 m..s
   53 	     9 	 0.05373 	 0.00055 	 m..s
   13 	    10 	 0.04737 	 0.00055 	 m..s
   69 	    11 	 0.05770 	 0.00055 	 m..s
   68 	    12 	 0.05694 	 0.00056 	 m..s
   20 	    13 	 0.05034 	 0.00056 	 m..s
   17 	    14 	 0.04863 	 0.00056 	 m..s
   55 	    15 	 0.05378 	 0.00058 	 m..s
    4 	    16 	 0.03910 	 0.00061 	 m..s
   48 	    17 	 0.05309 	 0.00064 	 m..s
   31 	    18 	 0.05139 	 0.00065 	 m..s
   41 	    19 	 0.05222 	 0.00066 	 m..s
   47 	    20 	 0.05305 	 0.00067 	 m..s
    5 	    21 	 0.03961 	 0.00067 	 m..s
    7 	    22 	 0.04549 	 0.00068 	 m..s
   27 	    23 	 0.05120 	 0.00068 	 m..s
   40 	    24 	 0.05200 	 0.00070 	 m..s
   44 	    25 	 0.05231 	 0.00070 	 m..s
   28 	    26 	 0.05122 	 0.00072 	 m..s
   26 	    27 	 0.05116 	 0.00073 	 m..s
   43 	    28 	 0.05230 	 0.00073 	 m..s
   62 	    29 	 0.05455 	 0.00074 	 m..s
   60 	    30 	 0.05441 	 0.00081 	 m..s
   66 	    31 	 0.05601 	 0.00082 	 m..s
    2 	    32 	 0.03424 	 0.00083 	 m..s
   18 	    33 	 0.04945 	 0.00087 	 m..s
   16 	    34 	 0.04862 	 0.00093 	 m..s
   32 	    35 	 0.05142 	 0.00129 	 m..s
   11 	    36 	 0.04699 	 0.00134 	 m..s
    9 	    37 	 0.04605 	 0.00145 	 m..s
   30 	    38 	 0.05138 	 0.00171 	 m..s
   73 	    39 	 0.08330 	 0.00320 	 m..s
   56 	    40 	 0.05386 	 0.00752 	 m..s
   72 	    41 	 0.07127 	 0.01708 	 m..s
   38 	    42 	 0.05187 	 0.04173 	 ~...
   24 	    43 	 0.05112 	 0.04191 	 ~...
   59 	    44 	 0.05432 	 0.04285 	 ~...
   37 	    45 	 0.05186 	 0.04670 	 ~...
   39 	    46 	 0.05196 	 0.04960 	 ~...
   58 	    47 	 0.05420 	 0.05072 	 ~...
   52 	    48 	 0.05367 	 0.05208 	 ~...
   63 	    49 	 0.05456 	 0.05231 	 ~...
   51 	    50 	 0.05363 	 0.05373 	 ~...
   42 	    51 	 0.05225 	 0.05384 	 ~...
   54 	    52 	 0.05378 	 0.05531 	 ~...
    6 	    53 	 0.04543 	 0.05652 	 ~...
   36 	    54 	 0.05177 	 0.05653 	 ~...
   21 	    55 	 0.05076 	 0.05720 	 ~...
   15 	    56 	 0.04796 	 0.05828 	 ~...
   65 	    57 	 0.05532 	 0.06257 	 ~...
   64 	    58 	 0.05485 	 0.06297 	 ~...
   34 	    59 	 0.05161 	 0.06298 	 ~...
   22 	    60 	 0.05100 	 0.06304 	 ~...
   29 	    61 	 0.05134 	 0.06340 	 ~...
   67 	    62 	 0.05645 	 0.06472 	 ~...
   25 	    63 	 0.05113 	 0.06494 	 ~...
   33 	    64 	 0.05157 	 0.06524 	 ~...
   10 	    65 	 0.04634 	 0.06589 	 ~...
   57 	    66 	 0.05392 	 0.06743 	 ~...
   35 	    67 	 0.05163 	 0.06752 	 ~...
   23 	    68 	 0.05110 	 0.07431 	 ~...
   71 	    69 	 0.05889 	 0.07459 	 ~...
   46 	    70 	 0.05303 	 0.07587 	 ~...
   45 	    71 	 0.05274 	 0.07673 	 ~...
   61 	    72 	 0.05445 	 0.07720 	 ~...
   70 	    73 	 0.05806 	 0.08454 	 ~...
   77 	    74 	 0.14883 	 0.12640 	 ~...
   74 	    75 	 0.14178 	 0.12784 	 ~...
   79 	    76 	 0.16058 	 0.13221 	 ~...
   80 	    77 	 0.16215 	 0.13864 	 ~...
   76 	    78 	 0.14593 	 0.14372 	 ~...
   84 	    79 	 0.18550 	 0.14511 	 m..s
   82 	    80 	 0.17511 	 0.14670 	 ~...
   83 	    81 	 0.18549 	 0.14839 	 m..s
   75 	    82 	 0.14576 	 0.15453 	 ~...
   85 	    83 	 0.18569 	 0.16078 	 ~...
   90 	    84 	 0.20754 	 0.17301 	 m..s
   81 	    85 	 0.17073 	 0.17490 	 ~...
   88 	    86 	 0.19700 	 0.18671 	 ~...
   91 	    87 	 0.20925 	 0.18864 	 ~...
   78 	    88 	 0.15132 	 0.19016 	 m..s
   86 	    89 	 0.19318 	 0.20253 	 ~...
   92 	    90 	 0.22337 	 0.20293 	 ~...
   89 	    91 	 0.20462 	 0.20773 	 ~...
   93 	    92 	 0.22485 	 0.21545 	 ~...
   95 	    93 	 0.23312 	 0.22448 	 ~...
   94 	    94 	 0.23310 	 0.22534 	 ~...
  100 	    95 	 0.24286 	 0.22977 	 ~...
   99 	    96 	 0.24166 	 0.23475 	 ~...
   87 	    97 	 0.19546 	 0.23521 	 m..s
   96 	    98 	 0.23338 	 0.23944 	 ~...
  101 	    99 	 0.26368 	 0.24977 	 ~...
  104 	   100 	 0.28039 	 0.25365 	 ~...
   97 	   101 	 0.24074 	 0.25600 	 ~...
  102 	   102 	 0.26618 	 0.25727 	 ~...
  105 	   103 	 0.28059 	 0.25811 	 ~...
  103 	   104 	 0.27865 	 0.26359 	 ~...
   98 	   105 	 0.24130 	 0.26619 	 ~...
  107 	   106 	 0.28278 	 0.27903 	 ~...
  111 	   107 	 0.29906 	 0.28170 	 ~...
  106 	   108 	 0.28122 	 0.28330 	 ~...
  110 	   109 	 0.29831 	 0.28471 	 ~...
  112 	   110 	 0.30372 	 0.29490 	 ~...
  109 	   111 	 0.29565 	 0.29696 	 ~...
  118 	   112 	 0.32067 	 0.29939 	 ~...
  120 	   113 	 0.32120 	 0.30219 	 ~...
  113 	   114 	 0.30403 	 0.30388 	 ~...
  115 	   115 	 0.31883 	 0.30429 	 ~...
  117 	   116 	 0.31913 	 0.30614 	 ~...
  114 	   117 	 0.30596 	 0.30616 	 ~...
  116 	   118 	 0.31888 	 0.30631 	 ~...
  108 	   119 	 0.29118 	 0.30982 	 ~...
  119 	   120 	 0.32075 	 0.32201 	 ~...
==========================================
r_mrr = 0.9714640974998474
r2_mrr = 0.9117175340652466
spearmanr_mrr@5 = 0.6770175695419312
spearmanr_mrr@10 = 0.7241918444633484
spearmanr_mrr@50 = 0.992493212223053
spearmanr_mrr@100 = 0.981669008731842
spearmanr_mrr@All = 0.9813413023948669
==========================================
test time: 0.421
Done Testing dataset OpenEA
total time taken: 199.14173340797424
training time taken: 184.50171637535095
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9715)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.9117)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.6770)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.7242)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9925)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9817)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9813)}}, 'test_loss': {'DistMult': {'OpenEA': 0.23620614214087254}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 3920567194370276
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [388, 775, 888, 579, 663, 357, 506, 214, 89, 372, 867, 307, 208, 1175, 172, 642, 993, 1005, 563, 558, 7, 1183, 43, 70, 1153, 546, 770, 246, 187, 381, 447, 489, 585, 378, 390, 715, 661, 193, 281, 85, 1152, 1060, 363, 112, 157, 1192, 32, 236, 29, 100, 1154, 396, 476, 120, 152, 404, 190, 932, 394, 653, 826, 416, 335, 505, 1049, 1204, 501, 305, 693, 473, 599, 493, 437, 813, 502, 571, 842, 491, 130, 697, 898, 687, 1047, 735, 223, 71, 329, 889, 1167, 438, 1000, 837, 584, 1053, 976, 670, 801, 1027, 44, 515, 864, 119, 131, 971, 276, 827, 751, 460, 425, 561, 519, 373, 610, 279, 86, 139, 948, 996, 729, 603, 251]
valid_ids (0): []
train_ids (1094): [213, 485, 533, 707, 1117, 349, 1004, 762, 31, 391, 723, 588, 1078, 630, 941, 957, 21, 862, 103, 30, 1190, 1038, 312, 446, 270, 740, 125, 601, 886, 819, 1011, 15, 475, 1057, 738, 52, 512, 299, 925, 646, 154, 828, 166, 1012, 896, 692, 250, 574, 1023, 69, 41, 589, 146, 121, 672, 592, 1186, 1066, 737, 641, 780, 622, 698, 821, 829, 868, 778, 934, 294, 61, 626, 793, 654, 354, 528, 1029, 1178, 235, 424, 161, 123, 91, 1169, 822, 68, 1164, 488, 612, 718, 40, 262, 189, 232, 796, 160, 392, 1101, 665, 1142, 1062, 598, 755, 303, 726, 355, 426, 352, 342, 700, 1203, 183, 716, 1080, 849, 1189, 529, 451, 648, 174, 368, 1010, 768, 1042, 550, 1126, 38, 51, 406, 118, 521, 836, 313, 947, 960, 455, 138, 365, 772, 159, 99, 840, 359, 710, 1025, 746, 129, 1075, 317, 221, 107, 903, 1081, 732, 800, 596, 149, 790, 967, 508, 841, 559, 1065, 635, 669, 291, 371, 703, 478, 75, 1063, 1006, 217, 551, 383, 962, 802, 1150, 468, 815, 771, 535, 321, 666, 482, 853, 855, 562, 965, 542, 135, 414, 311, 517, 701, 852, 901, 206, 835, 348, 469, 547, 415, 351, 197, 760, 1120, 496, 1100, 831, 549, 440, 839, 134, 1021, 399, 163, 894, 42, 922, 578, 938, 266, 1149, 370, 345, 195, 298, 736, 548, 807, 804, 434, 611, 20, 36, 48, 974, 725, 1119, 464, 795, 178, 369, 117, 682, 463, 1013, 538, 531, 744, 1077, 227, 683, 1094, 743, 791, 316, 634, 225, 946, 1043, 685, 27, 985, 680, 1185, 587, 1140, 169, 1137, 106, 1113, 741, 207, 456, 572, 385, 769, 1195, 878, 875, 429, 93, 238, 586, 895, 293, 577, 173, 268, 444, 500, 252, 880, 1079, 1184, 1114, 498, 63, 26, 997, 607, 609, 904, 817, 116, 629, 647, 1035, 869, 748, 1197, 799, 84, 1107, 192, 420, 1028, 185, 649, 633, 97, 1097, 758, 727, 1084, 691, 65, 600, 73, 619, 516, 428, 617, 871, 659, 1116, 64, 297, 1022, 866, 955, 750, 1198, 17, 4, 376, 490, 461, 708, 1045, 824, 1214, 540, 580, 47, 568, 315, 713, 637, 209, 702, 714, 690, 644, 541, 194, 353, 1020, 931, 304, 274, 927, 620, 308, 1118, 92, 1115, 556, 430, 24, 1099, 199, 514, 203, 215, 151, 337, 233, 1095, 1069, 495, 1034, 344, 411, 782, 882, 1098, 263, 1112, 761, 511, 1155, 499, 966, 928, 614, 109, 570, 747, 720, 115, 763, 306, 413, 991, 523, 144, 893, 838, 259, 834, 393, 1146, 675, 1207, 261, 230, 786, 58, 497, 581, 22, 981, 1187, 54, 854, 709, 458, 324, 806, 892, 1172, 441, 454, 1026, 459, 5, 846, 285, 220, 504, 436, 1211, 623, 216, 162, 127, 228, 11, 845, 101, 814, 899, 389, 62, 167, 242, 569, 356, 1181, 1200, 487, 977, 1170, 492, 678, 231, 972, 936, 328, 1179, 265, 857, 764, 1129, 239, 50, 1030, 124, 883, 12, 273, 384, 472, 906, 13, 905, 212, 699, 658, 890, 552, 452, 379, 1109, 939, 1014, 145, 168, 797, 865, 1159, 900, 1138, 986, 210, 543, 560, 933, 636, 474, 361, 945, 767, 908, 509, 310, 963, 861, 327, 914, 792, 926, 1058, 677, 382, 643, 1024, 628, 255, 1051, 616, 314, 887, 39, 76, 566, 776, 417, 387, 553, 794, 188, 1122, 0, 1128, 1143, 1196, 35, 87, 929, 565, 1160, 1212, 833, 133, 367, 959, 19, 798, 1134, 527, 386, 526, 789, 1072, 320, 300, 339, 2, 733, 1108, 1032, 465, 1007, 198, 921, 431, 289, 330, 990, 830, 583, 442, 1166, 1103, 343, 419, 241, 975, 410, 450, 979, 1105, 877, 754, 95, 8, 397, 1206, 923, 322, 992, 260, 229, 486, 935, 6, 181, 1135, 1208, 818, 1144, 590, 1133, 536, 940, 1180, 122, 57, 968, 1083, 812, 811, 844, 10, 667, 248, 752, 1067, 374, 432, 94, 1050, 72, 785, 573, 1074, 1056, 924, 1059, 1157, 1131, 287, 919, 427, 650, 809, 773, 90, 518, 1052, 377, 591, 1205, 1070, 114, 673, 987, 942, 78, 958, 175, 247, 111, 902, 597, 1018, 480, 332, 564, 350, 34, 618, 301, 756, 218, 621, 724, 978, 136, 158, 59, 734, 1139, 477, 16, 326, 916, 989, 284, 870, 1089, 645, 457, 1076, 483, 105, 681, 45, 863, 243, 88, 325, 671, 1033, 721, 765, 18, 627, 874, 631, 334, 625, 143, 471, 341, 1082, 9, 881, 104, 576, 1158, 850, 346, 267, 980, 1136, 848, 937, 582, 400, 1046, 719, 1209, 333, 1093, 366, 695, 155, 81, 532, 292, 689, 944, 1161, 234, 271, 401, 1041, 1037, 165, 398, 915, 479, 257, 728, 176, 323, 53, 843, 439, 1111, 1182, 575, 1092, 67, 608, 171, 885, 872, 674, 956, 766, 749, 200, 338, 1188, 555, 803, 331, 453, 950, 706, 1201, 730, 153, 113, 137, 781, 522, 503, 205, 49, 1176, 742, 264, 98, 1193, 557, 1087, 1003, 240, 1156, 994, 722, 449, 1102, 1199, 897, 982, 448, 539, 1125, 66, 1141, 731, 1, 196, 407, 1106, 999, 534, 911, 951, 375, 879, 825, 668, 1019, 1194, 759, 912, 606, 55, 340, 918, 191, 180, 286, 28, 1213, 148, 278, 1165, 909, 593, 953, 1174, 433, 783, 943, 1073, 275, 177, 686, 1001, 147, 421, 510, 96, 1104, 1096, 662, 638, 108, 1110, 816, 254, 704, 403, 1086, 83, 202, 405, 80, 358, 110, 545, 920, 624, 402, 364, 1040, 484, 408, 605, 126, 1090, 362, 3, 1151, 290, 983, 445, 74, 1055, 688, 705, 820, 132, 684, 211, 1191, 530, 1168, 1163, 282, 23, 1162, 470, 1036, 998, 604, 1171, 296, 513, 319, 652, 664, 224, 466, 226, 660, 283, 891, 219, 494, 711, 1124, 988, 970, 969, 954, 657, 524, 712, 336, 467, 1048, 787, 245, 615, 808, 1015, 961, 784, 1002, 102, 860, 184, 779, 788, 253, 1044, 973, 632, 640, 422, 1009, 60, 1123, 1039, 182, 984, 141, 858, 1088, 676, 302, 156, 525, 856, 696, 876, 917, 1031, 1064, 204, 280, 851, 694, 1147, 186, 520, 613, 380, 423, 481, 1173, 964, 309, 82, 873, 1145, 554, 79, 655, 1121, 753, 1130, 409, 272, 537, 639, 595, 810, 823, 256, 462, 1210, 295, 1017, 777, 1132, 907, 774, 237, 952, 25, 288, 884, 164, 1054, 1071, 201, 37, 757, 140, 258, 745, 1016, 1177, 847, 142, 77, 1202, 656, 859, 418, 1148, 739, 443, 717, 222, 594, 602, 544, 1061, 507, 46, 995, 1085, 1068, 949, 179, 567, 805, 910, 170, 269, 412, 1127, 435, 14, 347, 930, 277, 249, 1091, 679, 244, 318, 1008, 128, 56, 395, 913, 360, 651, 150, 832, 33]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  989689467638119
the save name prefix for this run is:  chkpt-ID_989689467638119_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 291
rank avg (pred): 0.447 +- 0.006
mrr vals (pred, true): 0.000, 0.268
batch losses (mrrl, rdl): 0.0, 0.0024727699

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 350
rank avg (pred): 0.394 +- 0.254
mrr vals (pred, true): 0.078, 0.063
batch losses (mrrl, rdl): 0.0, 0.0001327892

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 516
rank avg (pred): 0.246 +- 0.280
mrr vals (pred, true): 0.120, 0.131
batch losses (mrrl, rdl): 0.0, 8.5175e-06

Epoch over!
epoch time: 12.235

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 547
rank avg (pred): 0.206 +- 0.241
mrr vals (pred, true): 0.118, 0.126
batch losses (mrrl, rdl): 0.0, 1.52739e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1065
rank avg (pred): 0.053 +- 0.092
mrr vals (pred, true): 0.181, 0.240
batch losses (mrrl, rdl): 0.0, 8.67824e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 97
rank avg (pred): 0.427 +- 0.304
mrr vals (pred, true): 0.078, 0.058
batch losses (mrrl, rdl): 0.0, 8.17709e-05

Epoch over!
epoch time: 11.674

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1172
rank avg (pred): 0.433 +- 0.311
mrr vals (pred, true): 0.077, 0.044
batch losses (mrrl, rdl): 0.0, 5.89525e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 94
rank avg (pred): 0.420 +- 0.314
mrr vals (pred, true): 0.099, 0.067
batch losses (mrrl, rdl): 0.0, 0.0001322294

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1020
rank avg (pred): 0.409 +- 0.307
mrr vals (pred, true): 0.107, 0.079
batch losses (mrrl, rdl): 0.0, 9.81145e-05

Epoch over!
epoch time: 11.849

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 665
rank avg (pred): 0.425 +- 0.324
mrr vals (pred, true): 0.122, 0.001
batch losses (mrrl, rdl): 0.0, 9.20005e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 659
rank avg (pred): 0.441 +- 0.311
mrr vals (pred, true): 0.087, 0.001
batch losses (mrrl, rdl): 0.0, 7.56058e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 879
rank avg (pred): 0.544 +- 0.331
mrr vals (pred, true): 0.124, 0.001
batch losses (mrrl, rdl): 0.0, 7.97304e-05

Epoch over!
epoch time: 11.947

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 638
rank avg (pred): 0.413 +- 0.320
mrr vals (pred, true): 0.168, 0.060
batch losses (mrrl, rdl): 0.0, 6.34996e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 233
rank avg (pred): 0.417 +- 0.315
mrr vals (pred, true): 0.146, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001074952

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 789
rank avg (pred): 0.549 +- 0.323
mrr vals (pred, true): 0.120, 0.001
batch losses (mrrl, rdl): 0.0, 7.74326e-05

Epoch over!
epoch time: 11.917

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1195
rank avg (pred): 0.440 +- 0.319
mrr vals (pred, true): 0.157, 0.001
batch losses (mrrl, rdl): 0.113453418, 4.63421e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 91
rank avg (pred): 0.457 +- 0.255
mrr vals (pred, true): 0.054, 0.077
batch losses (mrrl, rdl): 0.0001412415, 0.0002610708

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 224
rank avg (pred): 0.435 +- 0.241
mrr vals (pred, true): 0.039, 0.001
batch losses (mrrl, rdl): 0.0012962436, 0.0001109508

Epoch over!
epoch time: 12.274

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 677
rank avg (pred): 0.436 +- 0.259
mrr vals (pred, true): 0.053, 0.000
batch losses (mrrl, rdl): 9.34128e-05, 0.0001000946

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 431
rank avg (pred): 0.418 +- 0.259
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.03588e-05, 0.0001550525

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 33
rank avg (pred): 0.153 +- 0.176
mrr vals (pred, true): 0.182, 0.141
batch losses (mrrl, rdl): 0.0164979678, 2.31686e-05

Epoch over!
epoch time: 12.152

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 873
rank avg (pred): 0.494 +- 0.275
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 8.34051e-05, 3.8962e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 129
rank avg (pred): 0.443 +- 0.287
mrr vals (pred, true): 0.052, 0.066
batch losses (mrrl, rdl): 2.50544e-05, 0.000142323

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 420
rank avg (pred): 0.485 +- 0.296
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 6.17089e-05, 6.3936e-06

Epoch over!
epoch time: 12.168

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1055
rank avg (pred): 0.060 +- 0.111
mrr vals (pred, true): 0.297, 0.275
batch losses (mrrl, rdl): 0.0051255566, 4.3211e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.409 +- 0.312
mrr vals (pred, true): 0.065, 0.001
batch losses (mrrl, rdl): 0.0023365163, 0.0001752456

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 879
rank avg (pred): 0.542 +- 0.321
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 4.20947e-05, 2.59784e-05

Epoch over!
epoch time: 12.296

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 654
rank avg (pred): 0.456 +- 0.317
mrr vals (pred, true): 0.049, 0.000
batch losses (mrrl, rdl): 1.12431e-05, 6.27453e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 578
rank avg (pred): 0.493 +- 0.315
mrr vals (pred, true): 0.045, 0.056
batch losses (mrrl, rdl): 0.0002368292, 0.0002131167

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 952
rank avg (pred): 0.578 +- 0.309
mrr vals (pred, true): 0.034, 0.000
batch losses (mrrl, rdl): 0.0025392827, 5.9398e-05

Epoch over!
epoch time: 12.344

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1080
rank avg (pred): 0.439 +- 0.317
mrr vals (pred, true): 0.049, 0.061
batch losses (mrrl, rdl): 9.3422e-06, 0.0001060036

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 894
rank avg (pred): 0.501 +- 0.310
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0003797542, 0.0017644376

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 974
rank avg (pred): 0.067 +- 0.121
mrr vals (pred, true): 0.296, 0.274
batch losses (mrrl, rdl): 0.0046470952, 5.7382e-06

Epoch over!
epoch time: 12.295

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 257
rank avg (pred): 0.069 +- 0.140
mrr vals (pred, true): 0.296, 0.299
batch losses (mrrl, rdl): 7.18801e-05, 1.33121e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1092
rank avg (pred): 0.436 +- 0.319
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 2.6311e-06, 8.47337e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 682
rank avg (pred): 0.433 +- 0.321
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 3.785e-06, 8.49207e-05

Epoch over!
epoch time: 12.171

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 455
rank avg (pred): 0.454 +- 0.333
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 6.14146e-05, 5.40892e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 267
rank avg (pred): 0.033 +- 0.053
mrr vals (pred, true): 0.302, 0.303
batch losses (mrrl, rdl): 2.12339e-05, 1.54164e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1040
rank avg (pred): 0.448 +- 0.311
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.000150397, 5.80296e-05

Epoch over!
epoch time: 12.182

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 331
rank avg (pred): 0.380 +- 0.307
mrr vals (pred, true): 0.051, 0.059
batch losses (mrrl, rdl): 6.0318e-06, 8.6352e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 452
rank avg (pred): 0.423 +- 0.313
mrr vals (pred, true): 0.047, 0.000
batch losses (mrrl, rdl): 0.0001148121, 0.0001395427

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 129
rank avg (pred): 0.437 +- 0.327
mrr vals (pred, true): 0.051, 0.066
batch losses (mrrl, rdl): 1.3379e-05, 0.0001201525

Epoch over!
epoch time: 13.032

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1179
rank avg (pred): 0.447 +- 0.330
mrr vals (pred, true): 0.054, 0.048
batch losses (mrrl, rdl): 0.000148908, 8.52749e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 8
rank avg (pred): 0.090 +- 0.111
mrr vals (pred, true): 0.295, 0.283
batch losses (mrrl, rdl): 0.0013626389, 1.78055e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 598
rank avg (pred): 0.412 +- 0.336
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 8.41956e-05, 1.98089e-05

Epoch over!
epoch time: 12.894

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.418 +- 0.335
mrr vals (pred, true): 0.047, 0.054

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   42 	     0 	 0.04943 	 9e-0500 	 m..s
    5 	     1 	 0.04573 	 0.00010 	 m..s
   81 	     2 	 0.05730 	 0.00024 	 m..s
   73 	     3 	 0.05199 	 0.00043 	 m..s
   28 	     4 	 0.04836 	 0.00047 	 m..s
   58 	     5 	 0.05034 	 0.00048 	 m..s
   34 	     6 	 0.04888 	 0.00049 	 m..s
   30 	     7 	 0.04853 	 0.00051 	 m..s
   27 	     8 	 0.04830 	 0.00052 	 m..s
   21 	     9 	 0.04768 	 0.00054 	 m..s
   66 	    10 	 0.05096 	 0.00055 	 m..s
   51 	    11 	 0.05000 	 0.00055 	 m..s
    8 	    12 	 0.04625 	 0.00056 	 m..s
    0 	    13 	 0.03518 	 0.00057 	 m..s
   70 	    14 	 0.05115 	 0.00058 	 m..s
   12 	    15 	 0.04718 	 0.00061 	 m..s
    6 	    16 	 0.04595 	 0.00062 	 m..s
   39 	    17 	 0.04925 	 0.00066 	 m..s
   14 	    18 	 0.04735 	 0.00068 	 m..s
   41 	    19 	 0.04931 	 0.00068 	 m..s
   19 	    20 	 0.04761 	 0.00069 	 m..s
   62 	    21 	 0.05068 	 0.00069 	 m..s
   52 	    22 	 0.05004 	 0.00071 	 m..s
   67 	    23 	 0.05103 	 0.00073 	 m..s
   45 	    24 	 0.04964 	 0.00073 	 m..s
   46 	    25 	 0.04970 	 0.00073 	 m..s
   56 	    26 	 0.05025 	 0.00074 	 m..s
   69 	    27 	 0.05113 	 0.00077 	 m..s
   60 	    28 	 0.05045 	 0.00078 	 m..s
    2 	    29 	 0.04453 	 0.00080 	 m..s
   36 	    30 	 0.04905 	 0.00081 	 m..s
    1 	    31 	 0.04446 	 0.00082 	 m..s
   64 	    32 	 0.05079 	 0.00084 	 m..s
   32 	    33 	 0.04877 	 0.00087 	 m..s
   10 	    34 	 0.04687 	 0.00089 	 m..s
    4 	    35 	 0.04545 	 0.00091 	 m..s
   57 	    36 	 0.05028 	 0.00092 	 m..s
   44 	    37 	 0.04961 	 0.00106 	 m..s
   65 	    38 	 0.05094 	 0.00112 	 m..s
    7 	    39 	 0.04599 	 0.00116 	 m..s
    9 	    40 	 0.04673 	 0.00129 	 m..s
    3 	    41 	 0.04511 	 0.00447 	 m..s
   74 	    42 	 0.05202 	 0.00611 	 m..s
   82 	    43 	 0.06374 	 0.00866 	 m..s
   16 	    44 	 0.04737 	 0.04093 	 ~...
   20 	    45 	 0.04763 	 0.04147 	 ~...
   26 	    46 	 0.04809 	 0.04825 	 ~...
   75 	    47 	 0.05204 	 0.04831 	 ~...
   18 	    48 	 0.04744 	 0.04898 	 ~...
   13 	    49 	 0.04725 	 0.04933 	 ~...
   71 	    50 	 0.05136 	 0.05072 	 ~...
   48 	    51 	 0.04978 	 0.05260 	 ~...
   24 	    52 	 0.04787 	 0.05266 	 ~...
   61 	    53 	 0.05064 	 0.05373 	 ~...
   72 	    54 	 0.05142 	 0.05378 	 ~...
   17 	    55 	 0.04741 	 0.05445 	 ~...
   15 	    56 	 0.04736 	 0.05457 	 ~...
   77 	    57 	 0.05219 	 0.05605 	 ~...
   25 	    58 	 0.04793 	 0.05653 	 ~...
   29 	    59 	 0.04842 	 0.05816 	 ~...
   54 	    60 	 0.05016 	 0.05836 	 ~...
   40 	    61 	 0.04925 	 0.05906 	 ~...
   22 	    62 	 0.04768 	 0.05969 	 ~...
   23 	    63 	 0.04770 	 0.06111 	 ~...
   59 	    64 	 0.05037 	 0.06181 	 ~...
   35 	    65 	 0.04890 	 0.06248 	 ~...
   63 	    66 	 0.05068 	 0.06257 	 ~...
   38 	    67 	 0.04918 	 0.06258 	 ~...
   79 	    68 	 0.05303 	 0.06293 	 ~...
   53 	    69 	 0.05011 	 0.06457 	 ~...
   50 	    70 	 0.04996 	 0.06494 	 ~...
   37 	    71 	 0.04913 	 0.06615 	 ~...
   68 	    72 	 0.05104 	 0.06711 	 ~...
   11 	    73 	 0.04715 	 0.06947 	 ~...
   78 	    74 	 0.05256 	 0.07174 	 ~...
   33 	    75 	 0.04884 	 0.07421 	 ~...
   31 	    76 	 0.04868 	 0.07431 	 ~...
   49 	    77 	 0.04985 	 0.07431 	 ~...
   43 	    78 	 0.04948 	 0.07637 	 ~...
   47 	    79 	 0.04976 	 0.07720 	 ~...
   80 	    80 	 0.05520 	 0.08161 	 ~...
   55 	    81 	 0.05022 	 0.08255 	 m..s
   76 	    82 	 0.05211 	 0.08711 	 m..s
   84 	    83 	 0.15373 	 0.13417 	 ~...
   85 	    84 	 0.15437 	 0.13631 	 ~...
   86 	    85 	 0.15438 	 0.13955 	 ~...
   83 	    86 	 0.13186 	 0.14372 	 ~...
   94 	    87 	 0.18831 	 0.14548 	 m..s
   96 	    88 	 0.19025 	 0.15883 	 m..s
   90 	    89 	 0.17815 	 0.15925 	 ~...
  102 	    90 	 0.19724 	 0.16162 	 m..s
  103 	    91 	 0.19738 	 0.16256 	 m..s
   89 	    92 	 0.17793 	 0.16607 	 ~...
   87 	    93 	 0.16649 	 0.17141 	 ~...
   97 	    94 	 0.19174 	 0.17639 	 ~...
   88 	    95 	 0.17181 	 0.17806 	 ~...
  101 	    96 	 0.19498 	 0.18435 	 ~...
   98 	    97 	 0.19255 	 0.18721 	 ~...
  105 	    98 	 0.20064 	 0.19365 	 ~...
  100 	    99 	 0.19437 	 0.20050 	 ~...
   91 	   100 	 0.18614 	 0.21003 	 ~...
   99 	   101 	 0.19433 	 0.21861 	 ~...
   93 	   102 	 0.18810 	 0.22261 	 m..s
   92 	   103 	 0.18715 	 0.22389 	 m..s
  106 	   104 	 0.23184 	 0.22448 	 ~...
  104 	   105 	 0.19738 	 0.23521 	 m..s
   95 	   106 	 0.18912 	 0.23716 	 m..s
  109 	   107 	 0.26579 	 0.24736 	 ~...
  108 	   108 	 0.26388 	 0.24895 	 ~...
  107 	   109 	 0.25760 	 0.25400 	 ~...
  110 	   110 	 0.27247 	 0.25821 	 ~...
  112 	   111 	 0.27473 	 0.26576 	 ~...
  118 	   112 	 0.30360 	 0.27326 	 m..s
  111 	   113 	 0.27361 	 0.27434 	 ~...
  114 	   114 	 0.29424 	 0.27439 	 ~...
  115 	   115 	 0.29599 	 0.27640 	 ~...
  116 	   116 	 0.29613 	 0.27903 	 ~...
  113 	   117 	 0.28360 	 0.27939 	 ~...
  117 	   118 	 0.29871 	 0.28573 	 ~...
  119 	   119 	 0.30464 	 0.30750 	 ~...
  120 	   120 	 0.32106 	 0.32380 	 ~...
==========================================
r_mrr = 0.9544792771339417
r2_mrr = 0.8822076320648193
spearmanr_mrr@5 = 0.9225701093673706
spearmanr_mrr@10 = 0.8532155752182007
spearmanr_mrr@50 = 0.9822080135345459
spearmanr_mrr@100 = 0.9660243988037109
spearmanr_mrr@All = 0.9631388187408447
==========================================
test time: 0.547
Done Testing dataset OpenEA
total time taken: 200.2158133983612
training time taken: 184.04851126670837
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9545)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8822)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9226)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.8532)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9822)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9660)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9631)}}, 'test_loss': {'DistMult': {'OpenEA': 0.20162317499853089}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 1019067770769621
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [160, 350, 301, 1082, 833, 1075, 773, 1163, 679, 71, 1099, 1001, 886, 355, 934, 827, 478, 457, 85, 980, 405, 1199, 1045, 1016, 745, 343, 634, 183, 1151, 241, 490, 139, 432, 881, 1116, 628, 492, 427, 800, 593, 1019, 522, 671, 565, 33, 320, 844, 1043, 166, 832, 345, 871, 186, 597, 6, 37, 15, 931, 283, 31, 36, 1214, 809, 613, 209, 333, 863, 5, 1169, 92, 596, 911, 625, 292, 686, 505, 1106, 1192, 1112, 471, 440, 504, 996, 841, 976, 280, 539, 586, 563, 247, 542, 375, 133, 1000, 1046, 2, 421, 117, 402, 211, 867, 100, 38, 795, 746, 188, 822, 319, 772, 232, 922, 304, 1061, 1160, 954, 511, 792, 296, 313, 973, 972]
valid_ids (0): []
train_ids (1094): [153, 129, 668, 59, 837, 239, 875, 155, 1170, 1053, 1107, 210, 857, 681, 299, 459, 644, 159, 851, 770, 606, 664, 226, 620, 967, 474, 62, 847, 1135, 873, 790, 1113, 1104, 363, 461, 879, 534, 760, 1042, 912, 13, 852, 122, 936, 1130, 932, 588, 1052, 380, 271, 739, 47, 791, 257, 683, 592, 1209, 14, 164, 723, 710, 57, 1057, 450, 484, 455, 660, 674, 755, 263, 937, 463, 659, 214, 16, 479, 887, 506, 696, 639, 849, 1054, 466, 584, 317, 332, 680, 602, 717, 741, 112, 882, 147, 344, 958, 477, 916, 158, 802, 530, 1109, 60, 284, 44, 884, 915, 751, 142, 486, 1050, 90, 1136, 354, 180, 148, 818, 276, 720, 1080, 1012, 558, 295, 688, 1133, 538, 338, 961, 356, 322, 665, 369, 1028, 771, 902, 1097, 305, 1068, 785, 1171, 1069, 248, 1131, 149, 892, 234, 677, 1184, 870, 562, 581, 782, 693, 26, 673, 655, 497, 1188, 987, 56, 279, 262, 874, 437, 22, 105, 803, 494, 716, 124, 174, 964, 499, 1147, 616, 136, 1100, 401, 1206, 204, 704, 945, 327, 220, 1128, 788, 985, 1180, 362, 1140, 315, 1166, 860, 167, 889, 191, 430, 896, 443, 955, 1093, 982, 981, 288, 780, 104, 1005, 168, 289, 811, 373, 1181, 152, 1033, 1048, 151, 53, 446, 491, 109, 742, 311, 781, 1123, 1146, 144, 1179, 907, 715, 743, 68, 260, 1105, 607, 970, 971, 8, 685, 1134, 901, 465, 551, 767, 118, 424, 274, 1201, 485, 829, 618, 605, 670, 258, 520, 1111, 583, 73, 146, 27, 225, 456, 98, 571, 794, 173, 946, 603, 267, 859, 687, 1035, 483, 331, 748, 691, 163, 959, 406, 30, 4, 357, 1013, 556, 784, 652, 589, 502, 787, 302, 177, 552, 1009, 1145, 645, 595, 546, 582, 796, 1132, 1120, 400, 736, 370, 196, 1154, 66, 776, 10, 452, 950, 1143, 49, 389, 858, 72, 481, 222, 1090, 540, 713, 1190, 1194, 268, 730, 75, 281, 391, 1176, 866, 1153, 992, 666, 434, 507, 270, 734, 995, 1167, 470, 761, 451, 162, 84, 642, 557, 87, 115, 65, 678, 205, 238, 880, 106, 726, 850, 498, 1186, 952, 1138, 891, 960, 190, 839, 918, 744, 690, 469, 138, 573, 116, 1081, 903, 19, 610, 909, 899, 58, 994, 7, 1011, 309, 777, 633, 468, 990, 1165, 462, 291, 1007, 612, 853, 207, 926, 622, 64, 108, 855, 448, 1018, 399, 1193, 414, 699, 436, 169, 667, 12, 1095, 1115, 500, 754, 388, 1110, 826, 41, 366, 88, 1213, 614, 553, 1020, 272, 949, 1041, 1197, 778, 1114, 298, 18, 365, 1049, 817, 352, 273, 890, 398, 371, 286, 29, 544, 1051, 250, 442, 569, 783, 404, 55, 384, 185, 988, 905, 348, 176, 962, 1155, 445, 1141, 351, 646, 547, 480, 413, 856, 753, 578, 335, 213, 626, 577, 925, 1148, 545, 339, 1059, 719, 711, 775, 113, 476, 1039, 227, 923, 762, 42, 135, 733, 1210, 1078, 805, 103, 181, 361, 554, 750, 658, 527, 1094, 82, 983, 623, 801, 300, 906, 812, 130, 526, 1108, 132, 641, 816, 842, 813, 1067, 287, 587, 728, 245, 640, 221, 194, 254, 897, 187, 1092, 215, 23, 835, 819, 525, 383, 1070, 1152, 150, 1064, 297, 314, 757, 920, 650, 458, 1119, 991, 797, 409, 269, 1162, 143, 316, 473, 1044, 516, 449, 460, 727, 1198, 255, 426, 156, 1077, 1062, 501, 865, 694, 868, 1, 428, 224, 714, 1177, 233, 904, 1004, 1126, 611, 524, 1172, 1030, 422, 823, 814, 97, 307, 1122, 845, 475, 360, 382, 464, 948, 1200, 165, 24, 749, 1182, 81, 561, 28, 854, 637, 528, 1003, 374, 769, 917, 549, 120, 1014, 878, 1204, 940, 698, 1002, 89, 649, 944, 86, 705, 953, 718, 764, 885, 123, 285, 708, 838, 95, 206, 1010, 512, 564, 1183, 377, 341, 80, 77, 386, 729, 393, 1084, 864, 1058, 536, 121, 198, 576, 199, 256, 1158, 1079, 417, 924, 806, 358, 786, 643, 579, 251, 326, 966, 703, 34, 560, 1060, 862, 707, 676, 69, 1159, 941, 763, 631, 1164, 487, 555, 178, 692, 193, 259, 913, 653, 189, 1187, 325, 243, 514, 48, 1023, 395, 342, 131, 101, 372, 725, 532, 495, 240, 977, 482, 519, 630, 935, 947, 700, 444, 963, 489, 110, 282, 175, 454, 94, 965, 294, 624, 1125, 535, 1137, 140, 550, 201, 598, 548, 779, 1096, 735, 789, 933, 1073, 1161, 1025, 46, 410, 793, 594, 1017, 721, 376, 340, 1142, 392, 747, 1174, 337, 419, 346, 919, 689, 647, 617, 651, 52, 575, 831, 1040, 321, 78, 230, 848, 537, 1006, 208, 1091, 111, 888, 367, 893, 407, 1211, 252, 910, 657, 425, 394, 894, 431, 91, 408, 615, 218, 1196, 834, 799, 323, 161, 43, 1089, 1034, 810, 11, 423, 1212, 228, 1118, 975, 35, 695, 861, 998, 574, 197, 1029, 1008, 1031, 590, 654, 277, 1063, 508, 334, 938, 202, 1207, 264, 1027, 70, 956, 608, 128, 1083, 808, 599, 137, 1117, 979, 876, 621, 3, 828, 1121, 521, 722, 93, 1032, 518, 39, 927, 541, 349, 566, 182, 1074, 840, 877, 737, 821, 636, 672, 1098, 438, 732, 435, 830, 968, 1127, 1157, 114, 1102, 1072, 766, 872, 107, 51, 1066, 330, 1024, 984, 195, 266, 324, 364, 9, 824, 869, 1071, 702, 381, 74, 219, 125, 669, 1056, 416, 242, 154, 1205, 756, 290, 663, 411, 591, 278, 758, 986, 768, 1055, 900, 433, 1202, 914, 820, 568, 804, 40, 930, 418, 1185, 368, 303, 533, 701, 509, 712, 898, 632, 157, 635, 928, 403, 1021, 1087, 619, 825, 385, 223, 629, 738, 570, 1036, 503, 308, 1065, 774, 353, 724, 1175, 310, 908, 706, 1037, 1088, 61, 172, 96, 1173, 974, 329, 453, 604, 192, 1101, 1208, 212, 648, 387, 656, 265, 1149, 543, 531, 993, 969, 63, 429, 336, 883, 249, 510, 179, 347, 1085, 127, 559, 1015, 1178, 517, 929, 759, 21, 32, 1195, 99, 25, 378, 217, 921, 79, 76, 203, 390, 50, 1129, 493, 661, 798, 45, 415, 1124, 740, 638, 957, 1026, 601, 752, 1189, 978, 1139, 1076, 229, 496, 807, 293, 585, 1144, 0, 126, 609, 765, 200, 600, 328, 441, 627, 662, 379, 1047, 216, 306, 261, 1203, 709, 989, 467, 846, 119, 244, 1086, 231, 529, 682, 939, 253, 237, 236, 312, 1168, 397, 141, 447, 580, 235, 675, 951, 1191, 246, 697, 943, 942, 184, 572, 83, 170, 420, 997, 134, 1156, 815, 513, 567, 515, 67, 275, 684, 1022, 20, 102, 396, 54, 412, 836, 1150, 359, 171, 999, 472, 318, 1038, 731, 17, 843, 439, 488, 895, 523, 1103, 145]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  6496665747151938
the save name prefix for this run is:  chkpt-ID_6496665747151938_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 664
rank avg (pred): 0.412 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002717132

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 696
rank avg (pred): 0.521 +- 0.005
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001242417

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1115
rank avg (pred): 0.453 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001567247

Epoch over!
epoch time: 13.082

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 779
rank avg (pred): 0.561 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001340839

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 110
rank avg (pred): 0.421 +- 0.000
mrr vals (pred, true): 0.000, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001985615

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 930
rank avg (pred): 0.561 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0, 0.0023674886

Epoch over!
epoch time: 12.36

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 997
rank avg (pred): 0.112 +- 0.000
mrr vals (pred, true): 0.001, 0.258
batch losses (mrrl, rdl): 0.0, 4.43991e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1137
rank avg (pred): 0.198 +- 0.000
mrr vals (pred, true): 0.000, 0.270
batch losses (mrrl, rdl): 0.0, 0.0001315507

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 685
rank avg (pred): 0.435 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002075903

Epoch over!
epoch time: 11.9

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 721
rank avg (pred): 0.425 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002449992

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 728
rank avg (pred): 0.445 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001722751

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 627
rank avg (pred): 0.424 +- 0.000
mrr vals (pred, true): 0.000, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001960541

Epoch over!
epoch time: 11.857

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1097
rank avg (pred): 0.431 +- 0.000
mrr vals (pred, true): 0.000, 0.054
batch losses (mrrl, rdl): 0.0, 0.0002238835

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 262
rank avg (pred): 0.056 +- 0.000
mrr vals (pred, true): 0.001, 0.302
batch losses (mrrl, rdl): 0.0, 1.06672e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 726
rank avg (pred): 0.457 +- 0.000
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001430673

Epoch over!
epoch time: 11.73

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1207
rank avg (pred): 0.422 +- 0.000
mrr vals (pred, true): 0.000, 0.000
batch losses (mrrl, rdl): 0.0248424001, 0.0002413817

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 567
rank avg (pred): 0.454 +- 0.208
mrr vals (pred, true): 0.069, 0.059
batch losses (mrrl, rdl): 0.0037253697, 0.0002532388

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 667
rank avg (pred): 0.398 +- 0.181
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.000187596, 0.000199803

Epoch over!
epoch time: 11.877

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 442
rank avg (pred): 0.343 +- 0.166
mrr vals (pred, true): 0.064, 0.001
batch losses (mrrl, rdl): 0.0020691096, 0.0004931401

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 526
rank avg (pred): 0.020 +- 0.014
mrr vals (pred, true): 0.182, 0.173
batch losses (mrrl, rdl): 0.0007278362, 0.0010668151

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 846
rank avg (pred): 0.510 +- 0.232
mrr vals (pred, true): 0.050, 0.006
batch losses (mrrl, rdl): 1.546e-07, 0.0001328486

Epoch over!
epoch time: 12.073

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 469
rank avg (pred): 0.408 +- 0.210
mrr vals (pred, true): 0.059, 0.001
batch losses (mrrl, rdl): 0.0007965393, 0.0001532306

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1010
rank avg (pred): 0.317 +- 0.167
mrr vals (pred, true): 0.068, 0.045
batch losses (mrrl, rdl): 0.0031490219, 0.0001231189

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 265
rank avg (pred): 0.002 +- 0.003
mrr vals (pred, true): 0.295, 0.300
batch losses (mrrl, rdl): 0.000272633, 5.98821e-05

Epoch over!
epoch time: 11.914

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 884
rank avg (pred): 0.470 +- 0.197
mrr vals (pred, true): 0.043, 0.000
batch losses (mrrl, rdl): 0.0004910879, 4.85137e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 282
rank avg (pred): 0.082 +- 0.075
mrr vals (pred, true): 0.157, 0.200
batch losses (mrrl, rdl): 0.0182882287, 3.77713e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1181
rank avg (pred): 0.337 +- 0.159
mrr vals (pred, true): 0.059, 0.050
batch losses (mrrl, rdl): 0.0008230122, 0.0001002041

Epoch over!
epoch time: 12.031

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1036
rank avg (pred): 0.331 +- 0.162
mrr vals (pred, true): 0.063, 0.001
batch losses (mrrl, rdl): 0.0016676962, 0.000540178

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 502
rank avg (pred): 0.097 +- 0.142
mrr vals (pred, true): 0.242, 0.266
batch losses (mrrl, rdl): 0.0056746546, 0.0002396695

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 167
rank avg (pred): 0.426 +- 0.206
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002226745, 0.0001435747

Epoch over!
epoch time: 11.996

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 212
rank avg (pred): 0.432 +- 0.204
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 9.7419e-05, 0.0001052397

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 716
rank avg (pred): 0.464 +- 0.226
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.1398e-06, 4.48781e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.348 +- 0.161
mrr vals (pred, true): 0.060, 0.075
batch losses (mrrl, rdl): 0.001048267, 9.39565e-05

Epoch over!
epoch time: 12.208

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 938
rank avg (pred): 0.516 +- 0.255
mrr vals (pred, true): 0.043, 0.005
batch losses (mrrl, rdl): 0.0004606051, 4.1461e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 154
rank avg (pred): 0.433 +- 0.202
mrr vals (pred, true): 0.051, 0.061
batch losses (mrrl, rdl): 2.19081e-05, 0.000184761

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 382
rank avg (pred): 0.451 +- 0.211
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 5.3051e-06, 0.000179704

Epoch over!
epoch time: 12.18

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1153
rank avg (pred): 0.230 +- 0.184
mrr vals (pred, true): 0.196, 0.187
batch losses (mrrl, rdl): 0.000861258, 5.29818e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 109
rank avg (pred): 0.447 +- 0.206
mrr vals (pred, true): 0.050, 0.054
batch losses (mrrl, rdl): 1.45e-08, 0.0002020553

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 975
rank avg (pred): 0.050 +- 0.070
mrr vals (pred, true): 0.283, 0.306
batch losses (mrrl, rdl): 0.0054106354, 2.49216e-05

Epoch over!
epoch time: 11.75

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 125
rank avg (pred): 0.426 +- 0.198
mrr vals (pred, true): 0.054, 0.066
batch losses (mrrl, rdl): 0.0001498073, 0.0002082576

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 839
rank avg (pred): 0.559 +- 0.273
mrr vals (pred, true): 0.039, 0.002
batch losses (mrrl, rdl): 0.0013199729, 3.13463e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 715
rank avg (pred): 0.459 +- 0.209
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 1.4127e-06, 5.87506e-05

Epoch over!
epoch time: 12.088

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1133
rank avg (pred): 0.337 +- 0.162
mrr vals (pred, true): 0.063, 0.001
batch losses (mrrl, rdl): 0.0016565328, 0.0004651959

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 919
rank avg (pred): 0.573 +- 0.282
mrr vals (pred, true): 0.037, 0.003
batch losses (mrrl, rdl): 0.0017991039, 5.30571e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 604
rank avg (pred): 0.460 +- 0.212
mrr vals (pred, true): 0.050, 0.053
batch losses (mrrl, rdl): 6.465e-07, 0.0002157457

Epoch over!
epoch time: 12.361

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.399 +- 0.159
mrr vals (pred, true): 0.049, 0.056

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    5 	     0 	 0.03675 	 8e-0500 	 m..s
    6 	     1 	 0.03830 	 0.00011 	 m..s
   15 	     2 	 0.04173 	 0.00027 	 m..s
   20 	     3 	 0.04190 	 0.00028 	 m..s
    0 	     4 	 0.02614 	 0.00038 	 ~...
   23 	     5 	 0.04206 	 0.00041 	 m..s
   39 	     6 	 0.04407 	 0.00044 	 m..s
   73 	     7 	 0.05085 	 0.00049 	 m..s
   30 	     8 	 0.04306 	 0.00050 	 m..s
    1 	     9 	 0.02759 	 0.00050 	 ~...
   51 	    10 	 0.04707 	 0.00050 	 m..s
   10 	    11 	 0.04125 	 0.00050 	 m..s
   43 	    12 	 0.04487 	 0.00050 	 m..s
   60 	    13 	 0.04928 	 0.00051 	 m..s
   52 	    14 	 0.04721 	 0.00051 	 m..s
   11 	    15 	 0.04125 	 0.00051 	 m..s
   29 	    16 	 0.04303 	 0.00052 	 m..s
   16 	    17 	 0.04184 	 0.00052 	 m..s
   49 	    18 	 0.04671 	 0.00055 	 m..s
   13 	    19 	 0.04154 	 0.00056 	 m..s
   36 	    20 	 0.04378 	 0.00056 	 m..s
   69 	    21 	 0.04988 	 0.00058 	 m..s
   44 	    22 	 0.04523 	 0.00059 	 m..s
   60 	    23 	 0.04928 	 0.00059 	 m..s
   54 	    24 	 0.04813 	 0.00059 	 m..s
   56 	    25 	 0.04861 	 0.00060 	 m..s
   60 	    26 	 0.04928 	 0.00060 	 m..s
   21 	    27 	 0.04201 	 0.00060 	 m..s
    4 	    28 	 0.03140 	 0.00061 	 m..s
   46 	    29 	 0.04644 	 0.00061 	 m..s
   26 	    30 	 0.04268 	 0.00064 	 m..s
    2 	    31 	 0.02763 	 0.00065 	 ~...
   28 	    32 	 0.04297 	 0.00068 	 m..s
   55 	    33 	 0.04847 	 0.00068 	 m..s
   60 	    34 	 0.04928 	 0.00069 	 m..s
    7 	    35 	 0.03836 	 0.00073 	 m..s
   40 	    36 	 0.04442 	 0.00075 	 m..s
   68 	    37 	 0.04931 	 0.00077 	 m..s
   34 	    38 	 0.04353 	 0.00078 	 m..s
    3 	    39 	 0.03112 	 0.00080 	 m..s
   60 	    40 	 0.04928 	 0.00085 	 m..s
   42 	    41 	 0.04463 	 0.00090 	 m..s
   41 	    42 	 0.04458 	 0.00098 	 m..s
   17 	    43 	 0.04185 	 0.00119 	 m..s
   74 	    44 	 0.05220 	 0.01722 	 m..s
   60 	    45 	 0.04928 	 0.05020 	 ~...
    8 	    46 	 0.04097 	 0.05026 	 ~...
   19 	    47 	 0.04186 	 0.05064 	 ~...
   25 	    48 	 0.04261 	 0.05266 	 ~...
   12 	    49 	 0.04154 	 0.05326 	 ~...
   57 	    50 	 0.04884 	 0.05564 	 ~...
   47 	    51 	 0.04645 	 0.05643 	 ~...
   18 	    52 	 0.04186 	 0.05653 	 ~...
   60 	    53 	 0.04928 	 0.05658 	 ~...
   35 	    54 	 0.04376 	 0.05685 	 ~...
   14 	    55 	 0.04168 	 0.05781 	 ~...
    9 	    56 	 0.04115 	 0.05976 	 ~...
   31 	    57 	 0.04323 	 0.06036 	 ~...
   33 	    58 	 0.04350 	 0.06050 	 ~...
   27 	    59 	 0.04279 	 0.06181 	 ~...
   72 	    60 	 0.05079 	 0.06192 	 ~...
   22 	    61 	 0.04204 	 0.06297 	 ~...
   75 	    62 	 0.05256 	 0.06302 	 ~...
   24 	    63 	 0.04246 	 0.06321 	 ~...
   32 	    64 	 0.04327 	 0.06329 	 ~...
   71 	    65 	 0.05070 	 0.06428 	 ~...
   59 	    66 	 0.04886 	 0.06472 	 ~...
   58 	    67 	 0.04885 	 0.06650 	 ~...
   38 	    68 	 0.04396 	 0.06711 	 ~...
   70 	    69 	 0.05070 	 0.07299 	 ~...
   53 	    70 	 0.04754 	 0.07450 	 ~...
   60 	    71 	 0.04928 	 0.07587 	 ~...
   48 	    72 	 0.04648 	 0.07606 	 ~...
   37 	    73 	 0.04393 	 0.07617 	 m..s
   45 	    74 	 0.04584 	 0.08255 	 m..s
   50 	    75 	 0.04701 	 0.08454 	 m..s
   81 	    76 	 0.14350 	 0.13828 	 ~...
   86 	    77 	 0.16022 	 0.14065 	 ~...
   84 	    78 	 0.15817 	 0.14126 	 ~...
   77 	    79 	 0.13151 	 0.14153 	 ~...
   80 	    80 	 0.14305 	 0.14674 	 ~...
   87 	    81 	 0.16411 	 0.15093 	 ~...
   85 	    82 	 0.15848 	 0.15219 	 ~...
   82 	    83 	 0.14999 	 0.16009 	 ~...
   78 	    84 	 0.13229 	 0.16565 	 m..s
   79 	    85 	 0.14169 	 0.17635 	 m..s
   83 	    86 	 0.15476 	 0.17806 	 ~...
  102 	    87 	 0.23915 	 0.17952 	 m..s
  101 	    88 	 0.23687 	 0.18082 	 m..s
   97 	    89 	 0.19518 	 0.18290 	 ~...
   76 	    90 	 0.12871 	 0.19016 	 m..s
   95 	    91 	 0.18639 	 0.19365 	 ~...
   88 	    92 	 0.16526 	 0.19954 	 m..s
   96 	    93 	 0.19470 	 0.20099 	 ~...
  106 	    94 	 0.24536 	 0.20438 	 m..s
  104 	    95 	 0.24015 	 0.21247 	 ~...
   92 	    96 	 0.17699 	 0.21352 	 m..s
   93 	    97 	 0.17817 	 0.22229 	 m..s
   94 	    98 	 0.18338 	 0.22261 	 m..s
   89 	    99 	 0.16956 	 0.22454 	 m..s
   99 	   100 	 0.22013 	 0.23004 	 ~...
  100 	   101 	 0.22090 	 0.23016 	 ~...
   90 	   102 	 0.17227 	 0.23436 	 m..s
   98 	   103 	 0.20999 	 0.23545 	 ~...
   91 	   104 	 0.17365 	 0.23553 	 m..s
  107 	   105 	 0.24781 	 0.25176 	 ~...
  103 	   106 	 0.23974 	 0.25727 	 ~...
  105 	   107 	 0.24314 	 0.25821 	 ~...
  108 	   108 	 0.25889 	 0.26974 	 ~...
  114 	   109 	 0.26877 	 0.27465 	 ~...
  110 	   110 	 0.26261 	 0.27749 	 ~...
  115 	   111 	 0.26921 	 0.27889 	 ~...
  112 	   112 	 0.26461 	 0.27927 	 ~...
  111 	   113 	 0.26368 	 0.27939 	 ~...
  109 	   114 	 0.26058 	 0.28142 	 ~...
  116 	   115 	 0.27062 	 0.28170 	 ~...
  117 	   116 	 0.27084 	 0.28408 	 ~...
  118 	   117 	 0.27241 	 0.29490 	 ~...
  113 	   118 	 0.26781 	 0.30750 	 m..s
  119 	   119 	 0.28867 	 0.31837 	 ~...
  120 	   120 	 0.29454 	 0.32201 	 ~...
==========================================
r_mrr = 0.9565576910972595
r2_mrr = 0.8955255746841431
spearmanr_mrr@5 = 0.8782842755317688
spearmanr_mrr@10 = 0.9208751916885376
spearmanr_mrr@50 = 0.9892160296440125
spearmanr_mrr@100 = 0.9762082099914551
spearmanr_mrr@All = 0.9761569499969482
==========================================
test time: 0.397
Done Testing dataset OpenEA
total time taken: 197.53017568588257
training time taken: 181.88594317436218
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9566)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8955)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.8783)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9209)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9892)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9762)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9762)}}, 'test_loss': {'DistMult': {'OpenEA': 0.4807328920433065}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 9756890460457694
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [118, 727, 639, 169, 259, 247, 650, 657, 72, 562, 1195, 527, 606, 755, 310, 897, 277, 773, 518, 55, 1174, 204, 389, 203, 322, 824, 1160, 156, 136, 278, 315, 430, 820, 1101, 514, 29, 1097, 90, 930, 1142, 705, 818, 48, 1041, 423, 669, 565, 242, 616, 808, 1007, 126, 1130, 387, 1086, 580, 291, 954, 386, 519, 442, 356, 956, 28, 747, 999, 678, 624, 468, 813, 1020, 655, 859, 778, 597, 1052, 757, 328, 195, 1107, 849, 871, 450, 838, 276, 429, 879, 321, 659, 708, 338, 487, 101, 1154, 147, 1067, 103, 544, 1050, 654, 378, 970, 721, 109, 236, 789, 68, 913, 979, 684, 553, 438, 69, 269, 548, 782, 666, 963, 500, 695, 807]
valid_ids (0): []
train_ids (1094): [1005, 941, 350, 369, 191, 225, 449, 841, 905, 903, 302, 100, 508, 1186, 167, 318, 1116, 288, 306, 298, 874, 792, 981, 368, 688, 626, 816, 482, 420, 216, 185, 31, 556, 969, 1188, 454, 448, 1037, 32, 73, 345, 647, 393, 637, 696, 98, 102, 407, 633, 81, 57, 947, 1162, 1057, 1197, 590, 681, 592, 750, 854, 480, 946, 17, 887, 717, 1091, 806, 1151, 991, 1025, 989, 348, 215, 467, 1208, 643, 890, 484, 940, 220, 533, 283, 1093, 382, 823, 319, 282, 1148, 111, 46, 35, 343, 326, 543, 926, 844, 300, 992, 962, 598, 998, 155, 119, 117, 235, 1083, 65, 993, 1158, 501, 836, 271, 437, 1213, 43, 1143, 1015, 296, 359, 309, 144, 62, 402, 3, 50, 812, 1136, 1017, 1121, 1126, 329, 741, 233, 686, 744, 183, 713, 948, 754, 724, 6, 244, 826, 769, 796, 740, 625, 748, 88, 324, 596, 292, 984, 738, 405, 392, 1056, 953, 172, 1040, 404, 106, 1094, 358, 868, 59, 486, 371, 312, 390, 40, 465, 495, 1103, 1210, 492, 896, 1157, 672, 1112, 629, 509, 21, 950, 157, 847, 139, 559, 478, 133, 327, 16, 75, 619, 209, 1042, 765, 253, 563, 1194, 339, 1178, 80, 477, 1019, 113, 837, 1198, 1135, 1193, 523, 84, 1085, 251, 410, 772, 479, 1184, 538, 1129, 289, 604, 572, 770, 603, 673, 682, 435, 884, 461, 421, 140, 973, 964, 829, 581, 781, 898, 928, 266, 1055, 174, 894, 614, 1137, 560, 463, 313, 93, 357, 452, 267, 293, 944, 9, 680, 228, 1082, 71, 424, 376, 489, 561, 189, 797, 192, 1024, 907, 960, 336, 609, 1045, 631, 380, 122, 4, 190, 995, 394, 317, 337, 888, 99, 63, 349, 95, 408, 436, 971, 586, 308, 763, 985, 911, 728, 1051, 1014, 952, 1204, 273, 86, 11, 187, 1199, 447, 720, 471, 163, 1000, 123, 112, 51, 297, 904, 397, 749, 360, 77, 497, 431, 74, 845, 257, 652, 311, 1190, 703, 1095, 158, 146, 1008, 882, 900, 1175, 131, 1073, 23, 8, 166, 937, 746, 188, 777, 355, 370, 272, 587, 700, 1089, 367, 1203, 822, 779, 648, 843, 965, 211, 199, 1072, 10, 531, 712, 908, 400, 613, 1100, 164, 493, 466, 212, 511, 1119, 197, 615, 1038, 210, 444, 857, 1039, 1013, 1044, 383, 858, 706, 364, 224, 873, 1070, 529, 285, 915, 1187, 105, 475, 301, 130, 249, 114, 958, 446, 260, 656, 184, 331, 153, 7, 230, 891, 1110, 583, 351, 665, 1043, 34, 1201, 256, 229, 840, 726, 830, 760, 469, 1063, 756, 481, 1167, 545, 664, 1062, 795, 284, 280, 1205, 558, 588, 935, 886, 906, 551, 675, 499, 638, 827, 1122, 921, 1109, 316, 472, 1068, 555, 617, 132, 361, 374, 120, 799, 968, 129, 207, 1035, 365, 237, 1115, 142, 1152, 577, 1023, 776, 243, 264, 60, 549, 507, 175, 222, 918, 414, 1108, 1202, 354, 662, 938, 1032, 608, 434, 165, 645, 115, 290, 82, 951, 697, 372, 1081, 398, 742, 1077, 121, 42, 453, 221, 505, 22, 379, 766, 49, 1114, 725, 340, 774, 138, 671, 182, 1117, 1146, 1034, 108, 70, 893, 30, 988, 1002, 94, 522, 238, 160, 731, 737, 1047, 1102, 694, 37, 729, 263, 936, 618, 649, 710, 1159, 245, 885, 286, 530, 455, 517, 539, 801, 1026, 409, 1132, 929, 1182, 1149, 205, 912, 1124, 149, 683, 957, 512, 584, 254, 252, 804, 1133, 783, 150, 1211, 794, 470, 574, 793, 265, 537, 262, 942, 1145, 335, 832, 601, 241, 743, 788, 460, 1001, 219, 1036, 540, 1214, 864, 417, 589, 1131, 176, 767, 931, 735, 1076, 498, 64, 388, 761, 107, 1079, 186, 1031, 1206, 1169, 734, 303, 18, 1156, 506, 855, 934, 1171, 395, 1078, 320, 524, 1030, 628, 270, 861, 208, 787, 711, 1049, 1054, 1098, 52, 526, 701, 181, 1058, 687, 814, 67, 860, 217, 171, 240, 1092, 866, 294, 780, 223, 892, 239, 983, 699, 899, 798, 862, 47, 927, 564, 817, 585, 1029, 571, 620, 853, 702, 715, 917, 723, 594, 552, 1165, 125, 104, 803, 1183, 883, 753, 570, 491, 299, 218, 159, 152, 232, 976, 1087, 784, 600, 825, 58, 810, 819, 575, 314, 1180, 137, 110, 206, 363, 611, 1113, 45, 373, 719, 987, 674, 535, 762, 325, 36, 426, 1141, 975, 154, 226, 850, 704, 972, 1191, 670, 933, 602, 464, 96, 15, 143, 261, 366, 248, 54, 751, 881, 768, 775, 441, 341, 925, 1176, 916, 168, 640, 676, 352, 20, 1128, 307, 634, 97, 406, 610, 1099, 815, 432, 986, 800, 66, 595, 736, 396, 231, 644, 1147, 457, 811, 1144, 733, 630, 196, 661, 542, 1064, 250, 573, 990, 1118, 722, 173, 679, 1209, 433, 677, 790, 623, 213, 1090, 605, 914, 274, 385, 764, 867, 334, 1016, 834, 692, 901, 578, 831, 287, 716, 473, 1066, 939, 1074, 848, 25, 412, 591, 1125, 377, 1006, 690, 547, 346, 179, 966, 56, 1033, 342, 89, 872, 1046, 919, 332, 835, 520, 621, 1106, 967, 2, 490, 323, 974, 949, 961, 170, 116, 923, 1170, 1140, 91, 279, 1120, 162, 642, 128, 151, 1164, 1163, 305, 627, 791, 576, 24, 200, 27, 875, 691, 1069, 83, 636, 730, 1060, 641, 33, 646, 1153, 909, 1123, 503, 502, 268, 347, 483, 660, 439, 413, 698, 87, 458, 13, 76, 714, 488, 0, 344, 1096, 994, 504, 536, 234, 1155, 1177, 61, 943, 1179, 550, 1111, 476, 1075, 515, 579, 635, 295, 977, 541, 569, 910, 902, 759, 1012, 689, 945, 14, 607, 663, 425, 304, 1071, 281, 428, 440, 582, 1018, 38, 1021, 932, 255, 534, 1134, 653, 1084, 178, 1212, 375, 1189, 865, 399, 718, 496, 1053, 214, 878, 978, 474, 668, 148, 1, 246, 846, 401, 821, 456, 920, 193, 79, 828, 1161, 513, 445, 135, 557, 180, 833, 889, 877, 593, 459, 384, 1196, 528, 997, 124, 1059, 612, 959, 1028, 809, 568, 19, 41, 732, 924, 554, 275, 658, 693, 1105, 516, 1088, 599, 707, 1027, 418, 53, 1150, 1065, 1010, 198, 12, 771, 44, 416, 39, 922, 805, 996, 419, 739, 863, 566, 876, 1207, 1181, 1004, 852, 895, 856, 1104, 1185, 1080, 745, 870, 1011, 1172, 709, 1022, 415, 26, 1173, 1003, 202, 78, 510, 1048, 485, 141, 521, 494, 786, 955, 1127, 443, 632, 422, 451, 1192, 785, 145, 651, 177, 411, 227, 353, 758, 258, 980, 330, 1168, 1139, 391, 667, 880, 685, 567, 194, 546, 92, 532, 381, 851, 1061, 842, 85, 427, 1166, 127, 403, 1009, 5, 161, 982, 462, 752, 333, 839, 802, 1200, 525, 622, 869, 134, 362, 201, 1138]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1728454002594199
the save name prefix for this run is:  chkpt-ID_1728454002594199_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 394
rank avg (pred): 0.467 +- 0.001
mrr vals (pred, true): 0.000, 0.069
batch losses (mrrl, rdl): 0.0, 0.0004916847

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1168
rank avg (pred): 0.471 +- 0.312
mrr vals (pred, true): 0.014, 0.080
batch losses (mrrl, rdl): 0.0, 0.0003999802

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 595
rank avg (pred): 0.362 +- 0.290
mrr vals (pred, true): 0.058, 0.059
batch losses (mrrl, rdl): 0.0, 2.29833e-05

Epoch over!
epoch time: 11.768

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 411
rank avg (pred): 0.369 +- 0.288
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001715489

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 510
rank avg (pred): 0.216 +- 0.190
mrr vals (pred, true): 0.076, 0.276
batch losses (mrrl, rdl): 0.0, 7.01316e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 933
rank avg (pred): 0.530 +- 0.341
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0, 0.0022322217

Epoch over!
epoch time: 11.742

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 317
rank avg (pred): 0.091 +- 0.079
mrr vals (pred, true): 0.101, 0.218
batch losses (mrrl, rdl): 0.0, 3.79279e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 295
rank avg (pred): 0.091 +- 0.080
mrr vals (pred, true): 0.086, 0.223
batch losses (mrrl, rdl): 0.0, 2.248e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 89
rank avg (pred): 0.427 +- 0.301
mrr vals (pred, true): 0.039, 0.054
batch losses (mrrl, rdl): 0.0, 0.0001651804

Epoch over!
epoch time: 12.038

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 513
rank avg (pred): 0.214 +- 0.187
mrr vals (pred, true): 0.075, 0.131
batch losses (mrrl, rdl): 0.0, 4.07939e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1019
rank avg (pred): 0.378 +- 0.303
mrr vals (pred, true): 0.058, 0.050
batch losses (mrrl, rdl): 0.0, 4.15963e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1084
rank avg (pred): 0.395 +- 0.311
mrr vals (pred, true): 0.048, 0.078
batch losses (mrrl, rdl): 0.0, 0.0001365863

Epoch over!
epoch time: 11.647

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 373
rank avg (pred): 0.387 +- 0.292
mrr vals (pred, true): 0.044, 0.074
batch losses (mrrl, rdl): 0.0, 9.59307e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 731
rank avg (pred): 0.195 +- 0.169
mrr vals (pred, true): 0.083, 0.081
batch losses (mrrl, rdl): 0.0, 0.0001913078

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 846
rank avg (pred): 0.449 +- 0.301
mrr vals (pred, true): 0.047, 0.006
batch losses (mrrl, rdl): 0.0, 2.62119e-05

Epoch over!
epoch time: 11.84

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 529
rank avg (pred): 0.197 +- 0.168
mrr vals (pred, true): 0.078, 0.169
batch losses (mrrl, rdl): 0.0823997855, 7.67787e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 475
rank avg (pred): 0.419 +- 0.177
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 4.5584e-06, 0.0001340754

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 993
rank avg (pred): 0.015 +- 0.014
mrr vals (pred, true): 0.232, 0.224
batch losses (mrrl, rdl): 0.0005897712, 0.0002983267

Epoch over!
epoch time: 12.252

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 51
rank avg (pred): 0.012 +- 0.011
mrr vals (pred, true): 0.231, 0.254
batch losses (mrrl, rdl): 0.0053565884, 0.0002619583

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 686
rank avg (pred): 0.432 +- 0.174
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 5.22237e-05, 0.0001278155

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 134
rank avg (pred): 0.439 +- 0.176
mrr vals (pred, true): 0.050, 0.064
batch losses (mrrl, rdl): 1.0322e-06, 0.0002990703

Epoch over!
epoch time: 12.378

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1120
rank avg (pred): 0.446 +- 0.190
mrr vals (pred, true): 0.051, 0.000
batch losses (mrrl, rdl): 1.23817e-05, 7.10721e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 113
rank avg (pred): 0.446 +- 0.175
mrr vals (pred, true): 0.049, 0.062
batch losses (mrrl, rdl): 1.64116e-05, 0.0002459916

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1109
rank avg (pred): 0.433 +- 0.183
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.461e-07, 0.0001081944

Epoch over!
epoch time: 12.497

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 908
rank avg (pred): 0.423 +- 0.308
mrr vals (pred, true): 0.102, 0.003
batch losses (mrrl, rdl): 0.0275453459, 0.0007142529

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 302
rank avg (pred): 0.177 +- 0.161
mrr vals (pred, true): 0.203, 0.158
batch losses (mrrl, rdl): 0.019640591, 0.0001289561

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 629
rank avg (pred): 0.419 +- 0.164
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 1.41e-08, 0.0001294886

Epoch over!
epoch time: 11.826

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 770
rank avg (pred): 0.475 +- 0.181
mrr vals (pred, true): 0.048, 0.000
batch losses (mrrl, rdl): 3.74335e-05, 0.0001605978

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 441
rank avg (pred): 0.418 +- 0.166
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.59006e-05, 0.000144921

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 350
rank avg (pred): 0.394 +- 0.148
mrr vals (pred, true): 0.050, 0.063
batch losses (mrrl, rdl): 4.475e-07, 0.000168057

Epoch over!
epoch time: 12.142

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 921
rank avg (pred): 0.697 +- 0.304
mrr vals (pred, true): 0.036, 0.001
batch losses (mrrl, rdl): 0.0019187656, 0.0003022112

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 230
rank avg (pred): 0.453 +- 0.174
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 2.6683e-06, 7.02923e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 738
rank avg (pred): 0.242 +- 0.181
mrr vals (pred, true): 0.140, 0.122
batch losses (mrrl, rdl): 0.0034831217, 0.0003441304

Epoch over!
epoch time: 11.952

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 685
rank avg (pred): 0.406 +- 0.153
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 7.87989e-05, 0.0002097731

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 130
rank avg (pred): 0.403 +- 0.141
mrr vals (pred, true): 0.052, 0.065
batch losses (mrrl, rdl): 5.38732e-05, 0.0001681301

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 873
rank avg (pred): 0.466 +- 0.181
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.11095e-05, 4.90792e-05

Epoch over!
epoch time: 12.111

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 875
rank avg (pred): 0.461 +- 0.179
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.53711e-05, 5.89224e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 100
rank avg (pred): 0.418 +- 0.136
mrr vals (pred, true): 0.048, 0.083
batch losses (mrrl, rdl): 5.35319e-05, 0.0002185303

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 506
rank avg (pred): 0.022 +- 0.018
mrr vals (pred, true): 0.310, 0.274
batch losses (mrrl, rdl): 0.0127366409, 0.0007440398

Epoch over!
epoch time: 12.039

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 18
rank avg (pred): 0.028 +- 0.022
mrr vals (pred, true): 0.289, 0.300
batch losses (mrrl, rdl): 0.0013761702, 6.36143e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 108
rank avg (pred): 0.434 +- 0.150
mrr vals (pred, true): 0.050, 0.052
batch losses (mrrl, rdl): 1.8833e-06, 0.0002091033

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 208
rank avg (pred): 0.436 +- 0.150
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.74784e-05, 0.0001356003

Epoch over!
epoch time: 11.847

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 603
rank avg (pred): 0.394 +- 0.142
mrr vals (pred, true): 0.052, 0.063
batch losses (mrrl, rdl): 6.14528e-05, 0.0001372308

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1063
rank avg (pred): 0.196 +- 0.150
mrr vals (pred, true): 0.229, 0.215
batch losses (mrrl, rdl): 0.0018447696, 0.0002257177

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1069
rank avg (pred): 0.044 +- 0.034
mrr vals (pred, true): 0.260, 0.263
batch losses (mrrl, rdl): 8.61418e-05, 0.0001284838

Epoch over!
epoch time: 12.133

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.449 +- 0.176
mrr vals (pred, true): 0.050, 0.058

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    9 	     0 	 0.04818 	 8e-0500 	 m..s
   67 	     1 	 0.05129 	 0.00010 	 m..s
   40 	     2 	 0.04968 	 0.00028 	 m..s
   46 	     3 	 0.04973 	 0.00030 	 m..s
   34 	     4 	 0.04960 	 0.00048 	 m..s
   15 	     5 	 0.04859 	 0.00048 	 m..s
    7 	     6 	 0.04809 	 0.00049 	 m..s
    2 	     7 	 0.04629 	 0.00050 	 m..s
   16 	     8 	 0.04861 	 0.00051 	 m..s
   20 	     9 	 0.04888 	 0.00053 	 m..s
    8 	    10 	 0.04816 	 0.00055 	 m..s
   18 	    11 	 0.04876 	 0.00055 	 m..s
   71 	    12 	 0.05142 	 0.00055 	 m..s
   64 	    13 	 0.05111 	 0.00055 	 m..s
   43 	    14 	 0.04972 	 0.00056 	 m..s
   55 	    15 	 0.05020 	 0.00058 	 m..s
   24 	    16 	 0.04910 	 0.00059 	 m..s
   33 	    17 	 0.04954 	 0.00060 	 m..s
   48 	    18 	 0.04978 	 0.00060 	 m..s
   29 	    19 	 0.04940 	 0.00061 	 m..s
   30 	    20 	 0.04941 	 0.00063 	 m..s
    4 	    21 	 0.04675 	 0.00064 	 m..s
   11 	    22 	 0.04826 	 0.00064 	 m..s
   50 	    23 	 0.04985 	 0.00065 	 m..s
   57 	    24 	 0.05063 	 0.00065 	 m..s
    0 	    25 	 0.04562 	 0.00068 	 m..s
    6 	    26 	 0.04779 	 0.00070 	 m..s
   32 	    27 	 0.04949 	 0.00071 	 m..s
   51 	    28 	 0.04991 	 0.00071 	 m..s
   31 	    29 	 0.04945 	 0.00072 	 m..s
   10 	    30 	 0.04822 	 0.00073 	 m..s
   44 	    31 	 0.04973 	 0.00073 	 m..s
   66 	    32 	 0.05124 	 0.00073 	 m..s
   65 	    33 	 0.05117 	 0.00074 	 m..s
   26 	    34 	 0.04920 	 0.00076 	 m..s
   13 	    35 	 0.04845 	 0.00076 	 m..s
   12 	    36 	 0.04828 	 0.00077 	 m..s
   23 	    37 	 0.04904 	 0.00078 	 m..s
   61 	    38 	 0.05087 	 0.00083 	 m..s
   38 	    39 	 0.04963 	 0.00084 	 m..s
   14 	    40 	 0.04845 	 0.00087 	 m..s
   37 	    41 	 0.04962 	 0.00089 	 m..s
   59 	    42 	 0.05076 	 0.00089 	 m..s
   56 	    43 	 0.05020 	 0.00092 	 m..s
    5 	    44 	 0.04770 	 0.00107 	 m..s
   25 	    45 	 0.04912 	 0.00112 	 m..s
    1 	    46 	 0.04576 	 0.00150 	 m..s
   79 	    47 	 0.05264 	 0.00209 	 m..s
   76 	    48 	 0.05161 	 0.00255 	 m..s
   78 	    49 	 0.05234 	 0.00406 	 m..s
   82 	    50 	 0.06966 	 0.00438 	 m..s
    3 	    51 	 0.04640 	 0.00510 	 m..s
   81 	    52 	 0.06898 	 0.00866 	 m..s
   80 	    53 	 0.06721 	 0.01348 	 m..s
   41 	    54 	 0.04969 	 0.03568 	 ~...
   19 	    55 	 0.04886 	 0.04732 	 ~...
   70 	    56 	 0.05137 	 0.04796 	 ~...
   42 	    57 	 0.04969 	 0.04807 	 ~...
   21 	    58 	 0.04900 	 0.04887 	 ~...
   49 	    59 	 0.04982 	 0.04898 	 ~...
   74 	    60 	 0.05158 	 0.05064 	 ~...
   27 	    61 	 0.04936 	 0.05118 	 ~...
   39 	    62 	 0.04964 	 0.05199 	 ~...
   75 	    63 	 0.05160 	 0.05221 	 ~...
   73 	    64 	 0.05152 	 0.05240 	 ~...
   54 	    65 	 0.05003 	 0.05355 	 ~...
   45 	    66 	 0.04973 	 0.05403 	 ~...
   47 	    67 	 0.04973 	 0.05493 	 ~...
   72 	    68 	 0.05149 	 0.05537 	 ~...
   17 	    69 	 0.04869 	 0.05564 	 ~...
   62 	    70 	 0.05097 	 0.05805 	 ~...
   22 	    71 	 0.04902 	 0.05828 	 ~...
   35 	    72 	 0.04961 	 0.05833 	 ~...
   53 	    73 	 0.05001 	 0.06054 	 ~...
   68 	    74 	 0.05133 	 0.06187 	 ~...
   28 	    75 	 0.04938 	 0.06298 	 ~...
   36 	    76 	 0.04962 	 0.06373 	 ~...
   60 	    77 	 0.05084 	 0.06625 	 ~...
   69 	    78 	 0.05135 	 0.06895 	 ~...
   63 	    79 	 0.05103 	 0.07227 	 ~...
   58 	    80 	 0.05071 	 0.07485 	 ~...
   52 	    81 	 0.04993 	 0.07943 	 ~...
   77 	    82 	 0.05192 	 0.08738 	 m..s
   83 	    83 	 0.13899 	 0.11561 	 ~...
   85 	    84 	 0.15321 	 0.12986 	 ~...
   85 	    85 	 0.15321 	 0.13270 	 ~...
   85 	    86 	 0.15321 	 0.13631 	 ~...
   92 	    87 	 0.15933 	 0.13870 	 ~...
   95 	    88 	 0.17330 	 0.13902 	 m..s
   98 	    89 	 0.18032 	 0.14548 	 m..s
   84 	    90 	 0.14288 	 0.14670 	 ~...
   85 	    91 	 0.15321 	 0.14677 	 ~...
   94 	    92 	 0.17238 	 0.15026 	 ~...
   85 	    93 	 0.15321 	 0.15221 	 ~...
  100 	    94 	 0.18108 	 0.15339 	 ~...
   90 	    95 	 0.15527 	 0.16226 	 ~...
   96 	    96 	 0.17472 	 0.16607 	 ~...
  101 	    97 	 0.18260 	 0.17081 	 ~...
  104 	    98 	 0.18808 	 0.17309 	 ~...
   91 	    99 	 0.15716 	 0.17822 	 ~...
  105 	   100 	 0.18883 	 0.18290 	 ~...
  102 	   101 	 0.18579 	 0.18435 	 ~...
   93 	   102 	 0.16658 	 0.19277 	 ~...
   97 	   103 	 0.17951 	 0.20232 	 ~...
  103 	   104 	 0.18677 	 0.22141 	 m..s
   99 	   105 	 0.18106 	 0.22688 	 m..s
  113 	   106 	 0.23683 	 0.23004 	 ~...
  106 	   107 	 0.21903 	 0.23220 	 ~...
  108 	   108 	 0.22774 	 0.23827 	 ~...
  112 	   109 	 0.23652 	 0.24008 	 ~...
  114 	   110 	 0.25524 	 0.24930 	 ~...
  109 	   111 	 0.22891 	 0.25221 	 ~...
  111 	   112 	 0.22985 	 0.25600 	 ~...
  118 	   113 	 0.29733 	 0.26176 	 m..s
  107 	   114 	 0.22053 	 0.26619 	 m..s
  115 	   115 	 0.26519 	 0.26721 	 ~...
  110 	   116 	 0.22927 	 0.26783 	 m..s
  116 	   117 	 0.28639 	 0.28408 	 ~...
  117 	   118 	 0.28922 	 0.29696 	 ~...
  119 	   119 	 0.30715 	 0.30605 	 ~...
  120 	   120 	 0.31611 	 0.31609 	 ~...
==========================================
r_mrr = 0.9591280221939087
r2_mrr = 0.8537336587905884
spearmanr_mrr@5 = 0.9582180976867676
spearmanr_mrr@10 = 0.9275005459785461
spearmanr_mrr@50 = 0.9822633266448975
spearmanr_mrr@100 = 0.9707599878311157
spearmanr_mrr@All = 0.9706712365150452
==========================================
test time: 0.394
Done Testing dataset OpenEA
total time taken: 196.4664604663849
training time taken: 180.67569255828857
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9591)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8537)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9582)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9275)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9823)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9708)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9707)}}, 'test_loss': {'DistMult': {'OpenEA': 0.2112318703711935}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 9518718177570708
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1210, 194, 641, 313, 70, 619, 870, 399, 114, 747, 814, 1074, 299, 990, 408, 533, 1184, 788, 652, 445, 510, 298, 134, 936, 761, 373, 631, 950, 242, 757, 552, 559, 806, 215, 1140, 630, 364, 1066, 851, 113, 361, 317, 336, 1123, 664, 58, 1051, 265, 1141, 961, 784, 211, 468, 830, 549, 876, 824, 369, 296, 970, 1002, 1046, 725, 706, 1087, 946, 78, 654, 987, 891, 1107, 774, 1015, 1007, 1177, 727, 1095, 1164, 429, 161, 1014, 1139, 338, 66, 43, 907, 1100, 940, 135, 496, 268, 1120, 30, 1134, 126, 288, 769, 127, 287, 882, 512, 417, 65, 1006, 63, 332, 1076, 1093, 246, 48, 266, 231, 170, 99, 1191, 286, 820, 310, 1031, 1034, 1200]
valid_ids (0): []
train_ids (1094): [1001, 1106, 795, 182, 39, 580, 267, 89, 1017, 1154, 230, 574, 251, 188, 274, 312, 1128, 34, 219, 729, 1010, 668, 270, 646, 359, 992, 185, 1181, 318, 923, 410, 781, 1113, 307, 837, 577, 61, 1117, 557, 1108, 543, 50, 1194, 181, 759, 1047, 472, 1144, 400, 488, 110, 446, 335, 18, 1075, 667, 958, 213, 731, 603, 611, 62, 910, 174, 467, 195, 901, 1187, 59, 570, 939, 1195, 469, 1138, 191, 197, 10, 671, 464, 379, 662, 168, 573, 1092, 300, 1028, 849, 615, 499, 120, 698, 8, 1061, 3, 539, 985, 815, 407, 843, 693, 133, 236, 736, 276, 495, 812, 819, 550, 1077, 282, 956, 362, 913, 81, 854, 690, 492, 1085, 721, 579, 884, 278, 422, 542, 835, 793, 609, 535, 404, 79, 1190, 460, 1212, 218, 13, 315, 331, 732, 1068, 778, 1207, 953, 431, 800, 1173, 229, 622, 897, 389, 890, 345, 685, 618, 886, 975, 1202, 889, 105, 162, 214, 606, 75, 728, 344, 1097, 386, 121, 1158, 909, 302, 694, 660, 643, 128, 46, 520, 1201, 171, 762, 1122, 115, 439, 1180, 1142, 1011, 714, 771, 97, 275, 165, 665, 93, 466, 692, 928, 764, 945, 462, 629, 143, 677, 1118, 49, 358, 443, 932, 56, 963, 55, 334, 986, 6, 144, 568, 395, 484, 816, 476, 929, 661, 22, 280, 83, 21, 106, 9, 799, 968, 776, 486, 900, 1040, 962, 597, 919, 1130, 482, 333, 937, 471, 453, 31, 921, 637, 387, 420, 506, 1088, 766, 823, 1125, 645, 147, 254, 786, 263, 966, 595, 647, 767, 636, 983, 521, 454, 621, 628, 750, 765, 922, 982, 365, 1090, 138, 186, 1161, 610, 723, 357, 1114, 192, 250, 223, 354, 1168, 887, 73, 12, 497, 247, 942, 297, 1151, 598, 734, 749, 881, 872, 118, 426, 1099, 697, 423, 1105, 927, 259, 941, 902, 1170, 591, 57, 976, 457, 1179, 136, 911, 51, 1155, 674, 680, 841, 123, 850, 411, 1132, 663, 713, 374, 614, 451, 415, 959, 479, 166, 157, 979, 391, 80, 1145, 478, 208, 522, 366, 739, 632, 256, 253, 1044, 1196, 1033, 756, 742, 707, 514, 828, 33, 689, 281, 380, 515, 717, 633, 206, 883, 316, 183, 225, 69, 679, 1124, 895, 430, 1185, 1056, 403, 791, 150, 805, 1193, 1069, 783, 1104, 1023, 967, 1082, 624, 915, 808, 592, 593, 237, 401, 527, 832, 1127, 817, 903, 447, 224, 355, 437, 813, 561, 234, 878, 260, 108, 743, 390, 501, 1050, 669, 130, 103, 481, 563, 1183, 483, 1166, 311, 1018, 1121, 695, 172, 980, 1027, 1041, 1131, 602, 160, 279, 772, 938, 860, 227, 1133, 862, 523, 1004, 1080, 673, 72, 5, 914, 682, 261, 1143, 565, 394, 1174, 1029, 1149, 998, 32, 715, 988, 341, 537, 617, 807, 353, 232, 947, 1214, 779, 811, 865, 129, 705, 294, 1081, 290, 413, 888, 683, 16, 834, 656, 1101, 1111, 777, 1037, 612, 973, 175, 432, 474, 754, 578, 948, 567, 90, 271, 119, 1103, 733, 735, 691, 45, 60, 301, 789, 1160, 531, 240, 124, 1156, 53, 37, 24, 1186, 583, 326, 760, 676, 726, 327, 498, 875, 880, 504, 566, 584, 613, 763, 562, 560, 320, 758, 517, 485, 972, 587, 965, 1175, 26, 92, 822, 588, 351, 17, 861, 519, 226, 551, 319, 507, 600, 122, 1162, 155, 221, 569, 1009, 125, 590, 217, 235, 809, 651, 427, 1137, 249, 38, 370, 530, 285, 1119, 601, 203, 222, 383, 393, 245, 684, 330, 686, 867, 864, 424, 272, 1035, 797, 1102, 23, 1091, 594, 933, 871, 94, 85, 193, 154, 1203, 1163, 1135, 382, 201, 951, 513, 82, 198, 398, 428, 1208, 949, 825, 720, 199, 712, 1063, 1059, 737, 36, 169, 918, 196, 845, 414, 996, 894, 1152, 833, 352, 14, 586, 792, 220, 164, 112, 141, 1204, 156, 371, 752, 1182, 1003, 555, 456, 931, 19, 1096, 1112, 339, 649, 392, 323, 88, 27, 96, 1021, 1072, 991, 255, 152, 347, 869, 76, 1053, 91, 704, 406, 678, 836, 703, 233, 289, 178, 892, 377, 29, 435, 925, 745, 978, 239, 605, 11, 238, 116, 384, 623, 20, 1205, 490, 1169, 785, 216, 548, 879, 204, 1054, 190, 984, 441, 367, 269, 346, 145, 421, 450, 1079, 151, 572, 974, 644, 1153, 844, 205, 1062, 350, 360, 173, 412, 1115, 780, 1084, 1005, 314, 1116, 396, 989, 409, 896, 545, 1070, 740, 688, 639, 1165, 455, 264, 840, 1055, 252, 1110, 701, 1094, 452, 526, 885, 305, 955, 957, 340, 810, 184, 708, 971, 718, 343, 648, 1126, 262, 804, 1012, 102, 1020, 787, 1043, 342, 167, 1052, 589, 1206, 1086, 473, 475, 981, 101, 363, 505, 1032, 434, 782, 935, 855, 283, 368, 842, 658, 487, 348, 977, 829, 375, 388, 177, 874, 244, 709, 738, 1048, 711, 596, 744, 207, 827, 857, 626, 25, 681, 440, 444, 848, 803, 906, 416, 107, 2, 321, 1065, 328, 202, 877, 98, 200, 753, 372, 163, 148, 1057, 449, 821, 719, 67, 995, 1049, 1022, 746, 655, 529, 180, 659, 1030, 153, 916, 137, 558, 1199, 634, 322, 494, 770, 638, 1167, 42, 1171, 571, 385, 607, 491, 775, 710, 650, 1147, 1197, 68, 863, 378, 818, 502, 1038, 748, 158, 964, 1064, 768, 773, 436, 1036, 500, 64, 179, 926, 7, 858, 1178, 1071, 459, 576, 994, 856, 74, 554, 672, 303, 801, 15, 1078, 142, 4, 556, 640, 581, 904, 405, 349, 553, 920, 325, 866, 899, 912, 1060, 189, 509, 470, 461, 1039, 309, 924, 293, 159, 826, 397, 541, 52, 442, 1209, 503, 304, 243, 859, 716, 722, 846, 620, 1129, 402, 508, 585, 796, 873, 839, 176, 139, 997, 1188, 852, 1098, 356, 893, 131, 1024, 564, 699, 657, 209, 696, 730, 687, 306, 532, 241, 1042, 627, 146, 1016, 575, 117, 798, 625, 675, 1013, 480, 376, 969, 1189, 547, 1159, 1008, 905, 40, 257, 954, 111, 538, 943, 751, 1148, 794, 582, 228, 329, 425, 518, 86, 544, 790, 666, 1089, 831, 516, 292, 635, 1211, 277, 493, 642, 1058, 433, 1213, 44, 741, 132, 960, 1150, 700, 511, 84, 1073, 1198, 477, 87, 868, 525, 210, 616, 1067, 534, 1083, 109, 0, 755, 381, 419, 291, 1192, 934, 930, 465, 448, 847, 54, 999, 1025, 1176, 528, 524, 28, 438, 35, 77, 100, 308, 284, 149, 838, 944, 724, 546, 853, 47, 212, 608, 489, 908, 1045, 258, 295, 248, 1146, 95, 1, 898, 599, 1157, 917, 273, 1136, 187, 540, 463, 702, 1000, 604, 140, 993, 653, 324, 458, 952, 418, 104, 71, 1109, 1026, 1172, 802, 670, 536, 337, 1019, 41]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7897406682794256
the save name prefix for this run is:  chkpt-ID_7897406682794256_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 153
rank avg (pred): 0.482 +- 0.007
mrr vals (pred, true): 0.000, 0.060
batch losses (mrrl, rdl): 0.0, 0.0004359414

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 224
rank avg (pred): 0.450 +- 0.248
mrr vals (pred, true): 0.098, 0.001
batch losses (mrrl, rdl): 0.0, 2.41776e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 802
rank avg (pred): 0.466 +- 0.318
mrr vals (pred, true): 0.191, 0.001
batch losses (mrrl, rdl): 0.0, 1.57937e-05

Epoch over!
epoch time: 11.931

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1009
rank avg (pred): 0.399 +- 0.284
mrr vals (pred, true): 0.195, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001194201

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1132
rank avg (pred): 0.372 +- 0.284
mrr vals (pred, true): 0.217, 0.000
batch losses (mrrl, rdl): 0.0, 0.0001166374

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 739
rank avg (pred): 0.119 +- 0.098
mrr vals (pred, true): 0.255, 0.128
batch losses (mrrl, rdl): 0.0, 2.66649e-05

Epoch over!
epoch time: 11.844

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 102
rank avg (pred): 0.410 +- 0.300
mrr vals (pred, true): 0.192, 0.070
batch losses (mrrl, rdl): 0.0, 0.00016999

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 598
rank avg (pred): 0.401 +- 0.294
mrr vals (pred, true): 0.182, 0.046
batch losses (mrrl, rdl): 0.0, 5.54239e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 628
rank avg (pred): 0.414 +- 0.305
mrr vals (pred, true): 0.185, 0.058
batch losses (mrrl, rdl): 0.0, 0.0001507903

Epoch over!
epoch time: 11.814

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 626
rank avg (pred): 0.401 +- 0.301
mrr vals (pred, true): 0.186, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001086131

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 792
rank avg (pred): 0.463 +- 0.320
mrr vals (pred, true): 0.143, 0.001
batch losses (mrrl, rdl): 0.0, 1.5269e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1055
rank avg (pred): 0.069 +- 0.058
mrr vals (pred, true): 0.225, 0.275
batch losses (mrrl, rdl): 0.0, 1.37215e-05

Epoch over!
epoch time: 11.753

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 920
rank avg (pred): 0.562 +- 0.354
mrr vals (pred, true): 0.137, 0.004
batch losses (mrrl, rdl): 0.0, 0.0001510106

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 46
rank avg (pred): 0.093 +- 0.081
mrr vals (pred, true): 0.166, 0.234
batch losses (mrrl, rdl): 0.0, 3.89699e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 229
rank avg (pred): 0.390 +- 0.317
mrr vals (pred, true): 0.143, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001132449

Epoch over!
epoch time: 11.986

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 986
rank avg (pred): 0.100 +- 0.170
mrr vals (pred, true): 0.215, 0.231
batch losses (mrrl, rdl): 0.0027560396, 6.0871e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1111
rank avg (pred): 0.488 +- 0.202
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 7.77562e-05, 3.58698e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 776
rank avg (pred): 0.526 +- 0.172
mrr vals (pred, true): 0.040, 0.006
batch losses (mrrl, rdl): 0.0009178649, 0.0002552524

Epoch over!
epoch time: 12.173

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 511
rank avg (pred): 0.119 +- 0.220
mrr vals (pred, true): 0.288, 0.275
batch losses (mrrl, rdl): 0.0018913017, 7.2476e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1170
rank avg (pred): 0.454 +- 0.204
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 0.0001162541, 0.0002310087

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 901
rank avg (pred): 0.395 +- 0.234
mrr vals (pred, true): 0.076, 0.015
batch losses (mrrl, rdl): 0.0066096606, 7.58447e-05

Epoch over!
epoch time: 11.995

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 702
rank avg (pred): 0.473 +- 0.175
mrr vals (pred, true): 0.045, 0.001
batch losses (mrrl, rdl): 0.0002602612, 5.23672e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1023
rank avg (pred): 0.433 +- 0.203
mrr vals (pred, true): 0.062, 0.075
batch losses (mrrl, rdl): 0.0014467676, 0.0003006583

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 567
rank avg (pred): 0.438 +- 0.190
mrr vals (pred, true): 0.053, 0.059
batch losses (mrrl, rdl): 8.91363e-05, 0.000201418

Epoch over!
epoch time: 12.061

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 339
rank avg (pred): 0.463 +- 0.172
mrr vals (pred, true): 0.046, 0.068
batch losses (mrrl, rdl): 0.0001410313, 0.0004078391

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 651
rank avg (pred): 0.435 +- 0.186
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002262903, 8.53581e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 948
rank avg (pred): 0.759 +- 0.233
mrr vals (pred, true): 0.030, 0.001
batch losses (mrrl, rdl): 0.0038197995, 0.0011549997

Epoch over!
epoch time: 11.957

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 818
rank avg (pred): 0.376 +- 0.217
mrr vals (pred, true): 0.086, 0.004
batch losses (mrrl, rdl): 0.0132564362, 7.51744e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 720
rank avg (pred): 0.443 +- 0.165
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 3.2183e-05, 9.74601e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 224
rank avg (pred): 0.431 +- 0.171
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001098706, 0.0001076624

Epoch over!
epoch time: 12.285

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 539
rank avg (pred): 0.156 +- 0.188
mrr vals (pred, true): 0.203, 0.201
batch losses (mrrl, rdl): 2.8566e-05, 0.0001205559

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 571
rank avg (pred): 0.447 +- 0.150
mrr vals (pred, true): 0.044, 0.065
batch losses (mrrl, rdl): 0.0003348471, 0.0002888863

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 867
rank avg (pred): 0.591 +- 0.241
mrr vals (pred, true): 0.037, 0.001
batch losses (mrrl, rdl): 0.0016643602, 0.0001462651

Epoch over!
epoch time: 12.075

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 572
rank avg (pred): 0.421 +- 0.169
mrr vals (pred, true): 0.055, 0.054
batch losses (mrrl, rdl): 0.0002608785, 0.0001633341

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 279
rank avg (pred): 0.159 +- 0.184
mrr vals (pred, true): 0.187, 0.201
batch losses (mrrl, rdl): 0.0016926037, 9.17647e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 355
rank avg (pred): 0.414 +- 0.167
mrr vals (pred, true): 0.057, 0.063
batch losses (mrrl, rdl): 0.0005191264, 0.0001817552

Epoch over!
epoch time: 12.199

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 670
rank avg (pred): 0.486 +- 0.223
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 8.114e-07, 2.13022e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 285
rank avg (pred): 0.159 +- 0.185
mrr vals (pred, true): 0.179, 0.187
batch losses (mrrl, rdl): 0.0006314673, 9.05999e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 562
rank avg (pred): 0.133 +- 0.182
mrr vals (pred, true): 0.230, 0.173
batch losses (mrrl, rdl): 0.03237243, 0.0001742691

Epoch over!
epoch time: 12.048

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 205
rank avg (pred): 0.461 +- 0.199
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 9.01441e-05, 5.67816e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 166
rank avg (pred): 0.457 +- 0.264
mrr vals (pred, true): 0.053, 0.001
batch losses (mrrl, rdl): 0.0001056627, 4.51587e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 277
rank avg (pred): 0.154 +- 0.196
mrr vals (pred, true): 0.154, 0.153
batch losses (mrrl, rdl): 5.142e-07, 5.00382e-05

Epoch over!
epoch time: 12.174

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1204
rank avg (pred): 0.499 +- 0.273
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 4.78119e-05, 5.7161e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 125
rank avg (pred): 0.455 +- 0.237
mrr vals (pred, true): 0.048, 0.066
batch losses (mrrl, rdl): 2.58351e-05, 0.0002655613

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1106
rank avg (pred): 0.411 +- 0.269
mrr vals (pred, true): 0.052, 0.062
batch losses (mrrl, rdl): 2.86491e-05, 7.89009e-05

Epoch over!
epoch time: 12.002

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.470 +- 0.257
mrr vals (pred, true): 0.049, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   10 	     0 	 0.04401 	 0.00013 	 m..s
   73 	     1 	 0.05882 	 0.00048 	 m..s
   42 	     2 	 0.05131 	 0.00048 	 m..s
    5 	     3 	 0.04032 	 0.00049 	 m..s
   56 	     4 	 0.05374 	 0.00052 	 m..s
    8 	     5 	 0.04354 	 0.00053 	 m..s
   35 	     6 	 0.04995 	 0.00055 	 m..s
    4 	     7 	 0.03961 	 0.00056 	 m..s
   25 	     8 	 0.04862 	 0.00058 	 m..s
   63 	     9 	 0.05464 	 0.00058 	 m..s
   19 	    10 	 0.04721 	 0.00060 	 m..s
   22 	    11 	 0.04807 	 0.00061 	 m..s
   58 	    12 	 0.05398 	 0.00061 	 m..s
   13 	    13 	 0.04673 	 0.00062 	 m..s
   43 	    14 	 0.05158 	 0.00062 	 m..s
   18 	    15 	 0.04703 	 0.00063 	 m..s
   55 	    16 	 0.05363 	 0.00063 	 m..s
    9 	    17 	 0.04361 	 0.00064 	 m..s
   34 	    18 	 0.04989 	 0.00065 	 m..s
    6 	    19 	 0.04043 	 0.00068 	 m..s
   39 	    20 	 0.05067 	 0.00068 	 m..s
   40 	    21 	 0.05091 	 0.00069 	 m..s
   72 	    22 	 0.05864 	 0.00072 	 m..s
   70 	    23 	 0.05665 	 0.00073 	 m..s
   12 	    24 	 0.04604 	 0.00075 	 m..s
   45 	    25 	 0.05222 	 0.00077 	 m..s
   29 	    26 	 0.04925 	 0.00077 	 m..s
   49 	    27 	 0.05280 	 0.00078 	 m..s
   51 	    28 	 0.05304 	 0.00080 	 m..s
   66 	    29 	 0.05524 	 0.00081 	 m..s
    1 	    30 	 0.03775 	 0.00083 	 m..s
   14 	    31 	 0.04674 	 0.00085 	 m..s
   15 	    32 	 0.04676 	 0.00086 	 m..s
   24 	    33 	 0.04858 	 0.00086 	 m..s
   27 	    34 	 0.04920 	 0.00087 	 m..s
    0 	    35 	 0.03665 	 0.00088 	 m..s
    2 	    36 	 0.03928 	 0.00088 	 m..s
   53 	    37 	 0.05331 	 0.00092 	 m..s
   68 	    38 	 0.05547 	 0.00099 	 m..s
   44 	    39 	 0.05210 	 0.00131 	 m..s
    3 	    40 	 0.03939 	 0.00150 	 m..s
   23 	    41 	 0.04826 	 0.00294 	 m..s
   46 	    42 	 0.05231 	 0.00344 	 m..s
   77 	    43 	 0.06158 	 0.00359 	 m..s
    7 	    44 	 0.04297 	 0.00500 	 m..s
   11 	    45 	 0.04580 	 0.00684 	 m..s
   78 	    46 	 0.07918 	 0.01025 	 m..s
   31 	    47 	 0.04934 	 0.04001 	 ~...
   64 	    48 	 0.05490 	 0.05079 	 ~...
   38 	    49 	 0.05061 	 0.05118 	 ~...
   71 	    50 	 0.05680 	 0.05140 	 ~...
   61 	    51 	 0.05444 	 0.05187 	 ~...
   28 	    52 	 0.04924 	 0.05226 	 ~...
   17 	    53 	 0.04702 	 0.05230 	 ~...
   21 	    54 	 0.04752 	 0.05927 	 ~...
   33 	    55 	 0.04949 	 0.06023 	 ~...
   50 	    56 	 0.05290 	 0.06158 	 ~...
   26 	    57 	 0.04916 	 0.06171 	 ~...
   37 	    58 	 0.05016 	 0.06203 	 ~...
   36 	    59 	 0.05016 	 0.06340 	 ~...
   48 	    60 	 0.05264 	 0.06412 	 ~...
   32 	    61 	 0.04944 	 0.06446 	 ~...
   74 	    62 	 0.05918 	 0.06508 	 ~...
   75 	    63 	 0.05969 	 0.06524 	 ~...
   62 	    64 	 0.05445 	 0.06524 	 ~...
   30 	    65 	 0.04932 	 0.06717 	 ~...
   20 	    66 	 0.04749 	 0.06745 	 ~...
   59 	    67 	 0.05414 	 0.06842 	 ~...
   57 	    68 	 0.05385 	 0.06895 	 ~...
   52 	    69 	 0.05328 	 0.06962 	 ~...
   65 	    70 	 0.05504 	 0.07014 	 ~...
   41 	    71 	 0.05094 	 0.07170 	 ~...
   54 	    72 	 0.05342 	 0.07190 	 ~...
   67 	    73 	 0.05525 	 0.07227 	 ~...
   69 	    74 	 0.05597 	 0.07228 	 ~...
   16 	    75 	 0.04688 	 0.07304 	 ~...
   47 	    76 	 0.05247 	 0.07421 	 ~...
   60 	    77 	 0.05419 	 0.07694 	 ~...
   76 	    78 	 0.05986 	 0.08425 	 ~...
   79 	    79 	 0.12173 	 0.11561 	 ~...
   85 	    80 	 0.16333 	 0.13444 	 ~...
   83 	    81 	 0.14587 	 0.13668 	 ~...
   82 	    82 	 0.14582 	 0.13870 	 ~...
   84 	    83 	 0.15568 	 0.14198 	 ~...
   90 	    84 	 0.18547 	 0.14811 	 m..s
   81 	    85 	 0.14543 	 0.15094 	 ~...
   80 	    86 	 0.14497 	 0.15680 	 ~...
   96 	    87 	 0.20613 	 0.16047 	 m..s
   88 	    88 	 0.17528 	 0.17924 	 ~...
   87 	    89 	 0.17423 	 0.18394 	 ~...
   89 	    90 	 0.17991 	 0.18566 	 ~...
   91 	    91 	 0.18737 	 0.19177 	 ~...
   86 	    92 	 0.16387 	 0.19277 	 ~...
   94 	    93 	 0.19793 	 0.19528 	 ~...
  108 	    94 	 0.25189 	 0.20438 	 m..s
   92 	    95 	 0.18832 	 0.21003 	 ~...
   98 	    96 	 0.21375 	 0.21352 	 ~...
  106 	    97 	 0.24573 	 0.21798 	 ~...
   95 	    98 	 0.20445 	 0.22069 	 ~...
   97 	    99 	 0.21024 	 0.22141 	 ~...
   93 	   100 	 0.18996 	 0.22389 	 m..s
  101 	   101 	 0.22411 	 0.22805 	 ~...
   99 	   102 	 0.21545 	 0.23608 	 ~...
  104 	   103 	 0.24330 	 0.23944 	 ~...
  105 	   104 	 0.24538 	 0.23996 	 ~...
  102 	   105 	 0.23416 	 0.24004 	 ~...
  107 	   106 	 0.24781 	 0.24123 	 ~...
  103 	   107 	 0.23919 	 0.25762 	 ~...
  110 	   108 	 0.25403 	 0.26014 	 ~...
  109 	   109 	 0.25388 	 0.26160 	 ~...
  100 	   110 	 0.22059 	 0.26619 	 m..s
  112 	   111 	 0.25577 	 0.26915 	 ~...
  116 	   112 	 0.26988 	 0.27022 	 ~...
  111 	   113 	 0.25414 	 0.27105 	 ~...
  115 	   114 	 0.26924 	 0.27105 	 ~...
  114 	   115 	 0.26684 	 0.27224 	 ~...
  113 	   116 	 0.26281 	 0.27569 	 ~...
  117 	   117 	 0.29380 	 0.27903 	 ~...
  118 	   118 	 0.31748 	 0.30018 	 ~...
  120 	   119 	 0.32183 	 0.30429 	 ~...
  119 	   120 	 0.32007 	 0.30614 	 ~...
==========================================
r_mrr = 0.961415708065033
r2_mrr = 0.889180064201355
spearmanr_mrr@5 = 0.9514528512954712
spearmanr_mrr@10 = 0.9710845351219177
spearmanr_mrr@50 = 0.9926254153251648
spearmanr_mrr@100 = 0.9745572805404663
spearmanr_mrr@All = 0.9749307632446289
==========================================
test time: 0.408
Done Testing dataset OpenEA
total time taken: 195.98395133018494
training time taken: 180.76867604255676
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9614)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8892)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9515)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9711)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9926)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9746)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9749)}}, 'test_loss': {'DistMult': {'OpenEA': 0.20049012695199053}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 3462737707435580
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [348, 587, 690, 422, 1087, 1113, 1102, 1070, 718, 1007, 818, 283, 543, 788, 354, 466, 450, 974, 11, 581, 1058, 1153, 1176, 731, 429, 1027, 252, 617, 1049, 10, 656, 2, 1096, 414, 1040, 472, 349, 203, 573, 574, 179, 223, 1045, 195, 343, 126, 138, 971, 1152, 534, 924, 779, 630, 28, 449, 793, 680, 186, 318, 471, 438, 668, 1095, 1012, 127, 250, 1154, 567, 726, 739, 143, 684, 1061, 1100, 536, 397, 334, 856, 1130, 29, 813, 180, 377, 838, 1026, 965, 358, 26, 106, 91, 57, 1125, 744, 707, 613, 82, 745, 5, 524, 647, 237, 828, 798, 385, 817, 1201, 485, 141, 895, 125, 1075, 976, 132, 1134, 792, 981, 352, 593, 622, 37, 1167]
valid_ids (0): []
train_ids (1094): [662, 113, 72, 735, 599, 746, 420, 41, 124, 1029, 807, 13, 987, 251, 521, 430, 275, 749, 269, 474, 929, 810, 761, 315, 796, 845, 277, 1000, 140, 952, 879, 620, 665, 253, 435, 486, 356, 776, 1107, 46, 948, 339, 116, 1207, 270, 1002, 852, 464, 220, 691, 1074, 1023, 1052, 395, 708, 706, 728, 689, 338, 766, 1137, 679, 403, 234, 335, 468, 479, 58, 565, 1047, 850, 1011, 607, 591, 263, 712, 325, 985, 546, 932, 207, 511, 115, 1043, 871, 644, 1164, 145, 173, 1031, 612, 866, 1132, 463, 922, 1206, 271, 60, 304, 507, 713, 477, 226, 980, 1156, 1051, 864, 205, 1163, 715, 440, 942, 289, 1073, 246, 475, 88, 762, 357, 15, 646, 111, 104, 437, 164, 258, 421, 905, 369, 160, 146, 936, 148, 944, 73, 518, 215, 1035, 632, 1092, 447, 50, 875, 200, 383, 688, 957, 753, 1099, 98, 1112, 213, 280, 1090, 506, 272, 137, 884, 1006, 488, 1148, 1146, 703, 777, 19, 681, 1055, 337, 306, 1143, 156, 915, 109, 541, 1003, 917, 392, 1105, 405, 309, 25, 659, 448, 526, 557, 729, 963, 227, 1205, 151, 281, 1120, 727, 451, 56, 1180, 881, 692, 638, 578, 1089, 470, 925, 624, 804, 1067, 797, 682, 87, 457, 547, 501, 598, 854, 674, 1122, 769, 240, 569, 803, 938, 1, 121, 428, 224, 1159, 673, 1157, 190, 800, 108, 1078, 34, 1028, 621, 552, 120, 157, 697, 278, 669, 637, 577, 633, 1128, 542, 161, 778, 1084, 1165, 836, 3, 874, 594, 103, 433, 855, 815, 738, 782, 1129, 678, 0, 896, 685, 123, 310, 1133, 68, 666, 892, 867, 409, 331, 602, 634, 1119, 719, 1021, 711, 802, 248, 1138, 4, 527, 386, 606, 1179, 1140, 554, 105, 490, 494, 610, 1004, 49, 130, 259, 355, 698, 912, 76, 496, 314, 1068, 805, 1053, 1033, 336, 24, 163, 1145, 222, 1185, 1196, 1010, 840, 27, 467, 441, 580, 84, 1189, 321, 588, 973, 517, 1014, 85, 366, 69, 919, 902, 904, 520, 1039, 273, 282, 305, 63, 891, 1093, 102, 516, 954, 562, 893, 575, 332, 460, 1079, 975, 945, 1136, 8, 242, 795, 823, 966, 481, 1178, 1135, 509, 185, 529, 316, 1103, 604, 1057, 424, 625, 843, 302, 298, 461, 772, 211, 771, 265, 899, 671, 616, 1104, 476, 780, 510, 301, 675, 1115, 709, 950, 653, 131, 523, 255, 327, 375, 1085, 900, 1169, 172, 279, 364, 216, 1204, 903, 379, 993, 655, 531, 672, 81, 1117, 576, 89, 806, 737, 1081, 1032, 841, 353, 20, 889, 225, 670, 1025, 221, 906, 754, 747, 142, 821, 152, 725, 897, 308, 695, 911, 947, 1036, 787, 834, 781, 1160, 489, 398, 401, 287, 652, 373, 1094, 916, 419, 44, 333, 544, 826, 601, 733, 360, 590, 631, 714, 851, 1155, 410, 1088, 734, 824, 515, 989, 540, 372, 293, 36, 898, 1202, 362, 848, 908, 859, 1059, 799, 537, 928, 495, 14, 59, 636, 999, 960, 1181, 230, 1060, 940, 110, 654, 721, 442, 595, 561, 1101, 784, 1038, 808, 344, 538, 35, 786, 1149, 701, 342, 951, 872, 31, 645, 52, 93, 499, 346, 558, 505, 564, 21, 70, 720, 370, 231, 23, 1212, 857, 432, 687, 865, 415, 789, 1191, 1083, 615, 64, 700, 380, 43, 175, 1192, 243, 1172, 459, 1211, 870, 1168, 994, 1147, 443, 758, 514, 849, 1210, 677, 371, 641, 862, 1080, 1194, 791, 750, 394, 74, 1126, 907, 444, 560, 427, 991, 133, 820, 704, 159, 266, 453, 436, 400, 365, 785, 241, 502, 969, 997, 291, 949, 1144, 664, 54, 100, 986, 260, 1158, 320, 307, 760, 1097, 295, 38, 229, 1187, 946, 608, 18, 303, 261, 535, 7, 53, 877, 292, 86, 964, 445, 847, 732, 1008, 118, 579, 387, 267, 559, 30, 12, 1173, 1009, 1062, 9, 40, 51, 742, 473, 519, 497, 801, 17, 770, 378, 978, 1141, 219, 883, 1013, 1124, 1108, 584, 193, 1109, 183, 1034, 1024, 551, 264, 326, 650, 553, 247, 830, 767, 995, 284, 723, 702, 484, 286, 117, 811, 33, 533, 341, 1042, 239, 45, 1213, 351, 210, 773, 941, 97, 933, 456, 218, 605, 206, 254, 194, 563, 1063, 837, 139, 136, 176, 667, 724, 1200, 550, 894, 431, 831, 888, 626, 736, 743, 972, 683, 640, 609, 1199, 1175, 775, 408, 890, 262, 794, 530, 763, 1183, 934, 513, 412, 833, 937, 465, 149, 959, 827, 571, 589, 319, 860, 1195, 618, 66, 730, 1098, 740, 382, 256, 1030, 492, 62, 885, 328, 1170, 1069, 1188, 909, 6, 212, 67, 487, 882, 78, 858, 299, 135, 174, 1182, 196, 1005, 1131, 312, 1116, 452, 710, 619, 480, 955, 918, 592, 1022, 1150, 99, 313, 979, 661, 1076, 1151, 92, 868, 233, 77, 404, 192, 1214, 235, 71, 764, 658, 829, 390, 114, 570, 413, 198, 819, 290, 388, 887, 330, 144, 417, 178, 853, 628, 55, 759, 1127, 503, 112, 783, 300, 95, 921, 170, 648, 583, 340, 80, 90, 238, 825, 705, 322, 1066, 717, 1114, 676, 1086, 1121, 627, 878, 166, 699, 285, 512, 439, 696, 426, 482, 158, 268, 914, 32, 276, 177, 167, 107, 1041, 317, 693, 182, 1190, 236, 1177, 1015, 968, 469, 1001, 943, 751, 491, 359, 1017, 1050, 1209, 1016, 367, 1174, 545, 329, 774, 150, 154, 741, 660, 1110, 1171, 752, 585, 901, 643, 931, 381, 582, 168, 835, 376, 1203, 572, 1019, 982, 839, 930, 249, 47, 165, 504, 294, 549, 923, 956, 1077, 716, 454, 809, 996, 181, 926, 399, 402, 478, 147, 539, 311, 48, 939, 844, 217, 962, 756, 548, 556, 927, 83, 406, 1071, 984, 425, 155, 209, 347, 1056, 913, 61, 184, 187, 297, 1106, 350, 389, 1048, 768, 323, 920, 863, 635, 816, 384, 345, 1193, 197, 169, 416, 1046, 748, 649, 992, 22, 876, 1082, 274, 79, 757, 411, 861, 961, 953, 983, 1020, 603, 458, 1118, 204, 597, 765, 1166, 16, 361, 967, 566, 935, 288, 1037, 418, 1111, 232, 988, 391, 122, 191, 958, 188, 880, 1184, 1091, 998, 910, 555, 722, 245, 94, 201, 39, 686, 639, 694, 600, 832, 842, 483, 508, 990, 101, 1054, 257, 651, 434, 202, 629, 886, 657, 423, 1123, 462, 873, 119, 790, 1072, 96, 244, 65, 1018, 393, 663, 128, 296, 977, 1161, 396, 596, 134, 363, 522, 525, 611, 1197, 1064, 446, 42, 208, 498, 532, 75, 846, 623, 324, 162, 214, 970, 528, 1186, 1139, 1044, 1142, 228, 129, 199, 869, 614, 755, 812, 171, 1208, 642, 493, 368, 455, 568, 500, 1065, 407, 586, 1198, 814, 189, 374, 153, 822, 1162]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7560007509420446
the save name prefix for this run is:  chkpt-ID_7560007509420446_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1118
rank avg (pred): 0.555 +- 0.003
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001757975

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 893
rank avg (pred): 0.423 +- 0.244
mrr vals (pred, true): 0.004, 0.003
batch losses (mrrl, rdl): 0.0, 4.08456e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 655
rank avg (pred): 0.424 +- 0.274
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0, 4.5873e-05

Epoch over!
epoch time: 11.93

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1073
rank avg (pred): 0.062 +- 0.046
mrr vals (pred, true): 0.211, 0.195
batch losses (mrrl, rdl): 0.0, 8.11784e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 67
rank avg (pred): 0.134 +- 0.104
mrr vals (pred, true): 0.130, 0.207
batch losses (mrrl, rdl): 0.0, 3.61809e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 145
rank avg (pred): 0.382 +- 0.302
mrr vals (pred, true): 0.101, 0.060
batch losses (mrrl, rdl): 0.0, 0.0001000583

Epoch over!
epoch time: 11.886

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 812
rank avg (pred): 0.130 +- 0.121
mrr vals (pred, true): 0.192, 0.056
batch losses (mrrl, rdl): 0.0, 2.7295e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 458
rank avg (pred): 0.372 +- 0.309
mrr vals (pred, true): 0.143, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001623305

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 510
rank avg (pred): 0.220 +- 0.190
mrr vals (pred, true): 0.168, 0.276
batch losses (mrrl, rdl): 0.0, 7.83729e-05

Epoch over!
epoch time: 11.963

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 767
rank avg (pred): 0.442 +- 0.318
mrr vals (pred, true): 0.110, 0.006
batch losses (mrrl, rdl): 0.0, 2.67324e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 823
rank avg (pred): 0.119 +- 0.103
mrr vals (pred, true): 0.229, 0.180
batch losses (mrrl, rdl): 0.0, 3.04587e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 907
rank avg (pred): 0.518 +- 0.382
mrr vals (pred, true): 0.117, 0.004
batch losses (mrrl, rdl): 0.0, 0.0002807379

Epoch over!
epoch time: 11.909

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 60
rank avg (pred): 0.102 +- 0.080
mrr vals (pred, true): 0.152, 0.133
batch losses (mrrl, rdl): 0.0, 6.21671e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 920
rank avg (pred): 0.528 +- 0.341
mrr vals (pred, true): 0.087, 0.004
batch losses (mrrl, rdl): 0.0, 6.23939e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 365
rank avg (pred): 0.353 +- 0.302
mrr vals (pred, true): 0.155, 0.057
batch losses (mrrl, rdl): 0.0, 2.44677e-05

Epoch over!
epoch time: 11.853

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1029
rank avg (pred): 0.404 +- 0.297
mrr vals (pred, true): 0.097, 0.001
batch losses (mrrl, rdl): 0.0221863054, 7.52802e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 611
rank avg (pred): 0.558 +- 0.177
mrr vals (pred, true): 0.053, 0.060
batch losses (mrrl, rdl): 6.4366e-05, 0.0009184115

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 418
rank avg (pred): 0.520 +- 0.176
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0002020083, 7.28225e-05

Epoch over!
epoch time: 12.238

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 55
rank avg (pred): 0.022 +- 0.016
mrr vals (pred, true): 0.189, 0.150
batch losses (mrrl, rdl): 0.0146789066, 0.0002660651

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 21
rank avg (pred): 0.001 +- 0.001
mrr vals (pred, true): 0.341, 0.299
batch losses (mrrl, rdl): 0.017012544, 0.0001179438

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 961
rank avg (pred): 0.516 +- 0.122
mrr vals (pred, true): 0.040, 0.001
batch losses (mrrl, rdl): 0.0009494386, 8.84501e-05

Epoch over!
epoch time: 12.058

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 156
rank avg (pred): 0.469 +- 0.170
mrr vals (pred, true): 0.054, 0.063
batch losses (mrrl, rdl): 0.0001313016, 0.0004565686

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 999
rank avg (pred): 0.463 +- 0.158
mrr vals (pred, true): 0.054, 0.055
batch losses (mrrl, rdl): 0.0001508203, 0.0003175919

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 894
rank avg (pred): 0.451 +- 0.161
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002552372, 0.0023450768

Epoch over!
epoch time: 12.099

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 207
rank avg (pred): 0.456 +- 0.156
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.77506e-05, 8.36469e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 364
rank avg (pred): 0.456 +- 0.149
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 1.39069e-05, 0.0003011545

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 927
rank avg (pred): 0.476 +- 0.115
mrr vals (pred, true): 0.045, 0.005
batch losses (mrrl, rdl): 0.0002191346, 0.0001939955

Epoch over!
epoch time: 12.329

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 528
rank avg (pred): 0.370 +- 0.240
mrr vals (pred, true): 0.120, 0.170
batch losses (mrrl, rdl): 0.0252332501, 0.0005383514

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 899
rank avg (pred): 0.437 +- 0.148
mrr vals (pred, true): 0.055, 0.000
batch losses (mrrl, rdl): 0.0002095851, 0.0005526483

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 996
rank avg (pred): 0.088 +- 0.073
mrr vals (pred, true): 0.268, 0.258
batch losses (mrrl, rdl): 0.001057228, 4.76722e-05

Epoch over!
epoch time: 12.159

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1111
rank avg (pred): 0.450 +- 0.142
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.92572e-05, 9.84386e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 84
rank avg (pred): 0.455 +- 0.135
mrr vals (pred, true): 0.051, 0.049
batch losses (mrrl, rdl): 1.96145e-05, 0.0003024854

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1018
rank avg (pred): 0.437 +- 0.148
mrr vals (pred, true): 0.053, 0.057
batch losses (mrrl, rdl): 6.36164e-05, 0.0002932777

Epoch over!
epoch time: 12.34

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 980
rank avg (pred): 0.006 +- 0.006
mrr vals (pred, true): 0.332, 0.318
batch losses (mrrl, rdl): 0.0019800947, 0.0001645513

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1035
rank avg (pred): 0.438 +- 0.132
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 1.11212e-05, 0.0001337476

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 394
rank avg (pred): 0.439 +- 0.123
mrr vals (pred, true): 0.050, 0.069
batch losses (mrrl, rdl): 3.889e-07, 0.0003500901

Epoch over!
epoch time: 12.091

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 747
rank avg (pred): 0.340 +- 0.205
mrr vals (pred, true): 0.147, 0.116
batch losses (mrrl, rdl): 0.0100123342, 0.0010971355

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 899
rank avg (pred): 0.480 +- 0.144
mrr vals (pred, true): 0.054, 0.000
batch losses (mrrl, rdl): 0.0001880548, 0.0003187026

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 420
rank avg (pred): 0.431 +- 0.110
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 5.5179e-06, 0.0001489768

Epoch over!
epoch time: 12.106

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 152
rank avg (pred): 0.432 +- 0.112
mrr vals (pred, true): 0.050, 0.055
batch losses (mrrl, rdl): 2.3229e-06, 0.0002248709

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 217
rank avg (pred): 0.432 +- 0.106
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.0001258431, 0.0001818578

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 274
rank avg (pred): 0.080 +- 0.059
mrr vals (pred, true): 0.190, 0.151
batch losses (mrrl, rdl): 0.0155457985, 5.94045e-05

Epoch over!
epoch time: 12.092

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 957
rank avg (pred): 0.636 +- 0.160
mrr vals (pred, true): 0.051, 0.001
batch losses (mrrl, rdl): 2.18987e-05, 0.0004950765

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1117
rank avg (pred): 0.437 +- 0.113
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 8.3518e-06, 0.0001581724

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 939
rank avg (pred): 0.607 +- 0.159
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 2.466e-07, 0.0016162033

Epoch over!
epoch time: 11.92

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.431 +- 0.111
mrr vals (pred, true): 0.049, 0.059

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.03547 	 9e-0500 	 m..s
   83 	     1 	 0.05141 	 0.00010 	 m..s
    8 	     2 	 0.04684 	 0.00048 	 m..s
    1 	     3 	 0.04506 	 0.00049 	 m..s
   25 	     4 	 0.04796 	 0.00050 	 m..s
   32 	     5 	 0.04817 	 0.00051 	 m..s
   41 	     6 	 0.04833 	 0.00051 	 m..s
   14 	     7 	 0.04761 	 0.00051 	 m..s
   39 	     8 	 0.04830 	 0.00052 	 m..s
    6 	     9 	 0.04655 	 0.00052 	 m..s
   63 	    10 	 0.04889 	 0.00052 	 m..s
   47 	    11 	 0.04837 	 0.00053 	 m..s
    7 	    12 	 0.04671 	 0.00053 	 m..s
   65 	    13 	 0.04897 	 0.00056 	 m..s
   29 	    14 	 0.04811 	 0.00059 	 m..s
    5 	    15 	 0.04655 	 0.00060 	 m..s
   33 	    16 	 0.04817 	 0.00061 	 m..s
   66 	    17 	 0.04899 	 0.00061 	 m..s
   13 	    18 	 0.04754 	 0.00063 	 m..s
   18 	    19 	 0.04778 	 0.00063 	 m..s
   70 	    20 	 0.04930 	 0.00065 	 m..s
   38 	    21 	 0.04830 	 0.00066 	 m..s
   17 	    22 	 0.04778 	 0.00068 	 m..s
   59 	    23 	 0.04878 	 0.00068 	 m..s
   40 	    24 	 0.04833 	 0.00068 	 m..s
   61 	    25 	 0.04878 	 0.00071 	 m..s
   15 	    26 	 0.04769 	 0.00072 	 m..s
   24 	    27 	 0.04792 	 0.00073 	 m..s
    4 	    28 	 0.04649 	 0.00073 	 m..s
   81 	    29 	 0.05083 	 0.00076 	 m..s
   46 	    30 	 0.04836 	 0.00078 	 m..s
   11 	    31 	 0.04731 	 0.00080 	 m..s
   27 	    32 	 0.04805 	 0.00081 	 m..s
   79 	    33 	 0.05048 	 0.00081 	 m..s
   36 	    34 	 0.04825 	 0.00083 	 m..s
   75 	    35 	 0.04957 	 0.00087 	 m..s
   23 	    36 	 0.04792 	 0.00087 	 m..s
   21 	    37 	 0.04788 	 0.00088 	 m..s
   35 	    38 	 0.04822 	 0.00089 	 m..s
    3 	    39 	 0.04621 	 0.00091 	 m..s
   37 	    40 	 0.04827 	 0.00092 	 m..s
   62 	    41 	 0.04878 	 0.00092 	 m..s
    9 	    42 	 0.04704 	 0.00125 	 m..s
   88 	    43 	 0.06312 	 0.00438 	 m..s
   87 	    44 	 0.06018 	 0.00439 	 m..s
    2 	    45 	 0.04612 	 0.00510 	 m..s
   73 	    46 	 0.04936 	 0.00635 	 m..s
   85 	    47 	 0.05807 	 0.00866 	 m..s
   67 	    48 	 0.04903 	 0.04044 	 ~...
   28 	    49 	 0.04805 	 0.04381 	 ~...
   54 	    50 	 0.04849 	 0.04383 	 ~...
   69 	    51 	 0.04912 	 0.04733 	 ~...
   31 	    52 	 0.04813 	 0.04752 	 ~...
   43 	    53 	 0.04834 	 0.04881 	 ~...
   16 	    54 	 0.04773 	 0.05084 	 ~...
   71 	    55 	 0.04931 	 0.05118 	 ~...
   19 	    56 	 0.04780 	 0.05140 	 ~...
   53 	    57 	 0.04847 	 0.05230 	 ~...
   64 	    58 	 0.04895 	 0.05231 	 ~...
   45 	    59 	 0.04836 	 0.05328 	 ~...
   77 	    60 	 0.04969 	 0.05376 	 ~...
   26 	    61 	 0.04803 	 0.05533 	 ~...
   74 	    62 	 0.04942 	 0.05560 	 ~...
   50 	    63 	 0.04844 	 0.05587 	 ~...
   72 	    64 	 0.04935 	 0.05701 	 ~...
   44 	    65 	 0.04835 	 0.05860 	 ~...
   68 	    66 	 0.04911 	 0.05885 	 ~...
   78 	    67 	 0.04996 	 0.05893 	 ~...
   34 	    68 	 0.04819 	 0.05974 	 ~...
   10 	    69 	 0.04727 	 0.05987 	 ~...
   56 	    70 	 0.04857 	 0.06050 	 ~...
   58 	    71 	 0.04877 	 0.06340 	 ~...
   51 	    72 	 0.04844 	 0.06378 	 ~...
   84 	    73 	 0.05165 	 0.06472 	 ~...
   48 	    74 	 0.04840 	 0.06508 	 ~...
   42 	    75 	 0.04834 	 0.06517 	 ~...
   20 	    76 	 0.04782 	 0.06559 	 ~...
   52 	    77 	 0.04846 	 0.06585 	 ~...
   82 	    78 	 0.05088 	 0.06605 	 ~...
   22 	    79 	 0.04790 	 0.06858 	 ~...
   49 	    80 	 0.04843 	 0.07128 	 ~...
   30 	    81 	 0.04812 	 0.07227 	 ~...
   60 	    82 	 0.04878 	 0.07431 	 ~...
   80 	    83 	 0.05076 	 0.07673 	 ~...
   55 	    84 	 0.04852 	 0.07680 	 ~...
   76 	    85 	 0.04968 	 0.07978 	 m..s
   86 	    86 	 0.05889 	 0.08079 	 ~...
   12 	    87 	 0.04738 	 0.08425 	 m..s
   57 	    88 	 0.04869 	 0.08454 	 m..s
   89 	    89 	 0.10734 	 0.12784 	 ~...
   92 	    90 	 0.12688 	 0.12934 	 ~...
   96 	    91 	 0.17113 	 0.13420 	 m..s
   93 	    92 	 0.13544 	 0.13572 	 ~...
   90 	    93 	 0.10882 	 0.13825 	 ~...
   97 	    94 	 0.17188 	 0.13902 	 m..s
   91 	    95 	 0.11328 	 0.14153 	 ~...
   98 	    96 	 0.17243 	 0.16607 	 ~...
   94 	    97 	 0.15481 	 0.17280 	 ~...
  103 	    98 	 0.18072 	 0.17639 	 ~...
   95 	    99 	 0.16254 	 0.17666 	 ~...
  100 	   100 	 0.17928 	 0.18435 	 ~...
  102 	   101 	 0.18037 	 0.18721 	 ~...
  105 	   102 	 0.19514 	 0.19856 	 ~...
  104 	   103 	 0.19320 	 0.20425 	 ~...
   99 	   104 	 0.17673 	 0.22454 	 m..s
  106 	   105 	 0.20470 	 0.23545 	 m..s
  101 	   106 	 0.17975 	 0.23553 	 m..s
  107 	   107 	 0.21034 	 0.25281 	 m..s
  108 	   108 	 0.24319 	 0.26887 	 ~...
  109 	   109 	 0.24704 	 0.27105 	 ~...
  116 	   110 	 0.28447 	 0.27439 	 ~...
  110 	   111 	 0.26990 	 0.27889 	 ~...
  111 	   112 	 0.27033 	 0.28170 	 ~...
  112 	   113 	 0.27488 	 0.28326 	 ~...
  115 	   114 	 0.28069 	 0.29425 	 ~...
  113 	   115 	 0.27662 	 0.30201 	 ~...
  120 	   116 	 0.29491 	 0.30589 	 ~...
  114 	   117 	 0.27768 	 0.30677 	 ~...
  118 	   118 	 0.28705 	 0.30750 	 ~...
  119 	   119 	 0.28768 	 0.31170 	 ~...
  117 	   120 	 0.28472 	 0.32201 	 m..s
==========================================
r_mrr = 0.9546124339103699
r2_mrr = 0.8761509656906128
spearmanr_mrr@5 = 0.9817042946815491
spearmanr_mrr@10 = 0.970816969871521
spearmanr_mrr@50 = 0.9913114905357361
spearmanr_mrr@100 = 0.9665360450744629
spearmanr_mrr@All = 0.9625868201255798
==========================================
test time: 0.451
Done Testing dataset OpenEA
total time taken: 196.15912055969238
training time taken: 181.50097727775574
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9546)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8762)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9817)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9708)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9913)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9665)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9626)}}, 'test_loss': {'DistMult': {'OpenEA': 0.2350487210569554}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 5801956104574769
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [653, 860, 612, 314, 240, 198, 819, 1207, 303, 538, 39, 886, 118, 844, 1138, 812, 65, 1020, 654, 823, 813, 7, 1146, 940, 918, 722, 1071, 1101, 1127, 943, 545, 24, 779, 488, 321, 710, 412, 241, 824, 977, 143, 1171, 432, 145, 738, 1130, 1191, 349, 727, 376, 973, 329, 517, 778, 1210, 360, 846, 553, 929, 1068, 705, 1095, 1148, 262, 319, 324, 556, 19, 286, 298, 641, 433, 856, 110, 1211, 224, 231, 78, 10, 476, 362, 68, 96, 58, 658, 739, 116, 648, 993, 661, 481, 15, 1116, 593, 1155, 273, 884, 948, 1136, 1160, 22, 1159, 1150, 1175, 135, 250, 112, 177, 358, 740, 1198, 875, 1126, 104, 798, 913, 81, 1031, 88, 1084, 868]
valid_ids (0): []
train_ids (1094): [902, 1063, 499, 1214, 1162, 14, 881, 622, 313, 745, 200, 185, 749, 1205, 683, 951, 939, 228, 797, 946, 893, 459, 652, 861, 270, 513, 1038, 1061, 930, 239, 344, 138, 410, 1104, 1118, 53, 698, 46, 578, 1196, 417, 777, 74, 463, 729, 1152, 614, 113, 20, 995, 1081, 1112, 328, 1115, 223, 703, 184, 796, 560, 63, 3, 910, 1135, 139, 934, 368, 528, 34, 597, 841, 1074, 310, 1145, 464, 1089, 157, 1111, 781, 204, 478, 715, 170, 1097, 446, 985, 1092, 523, 912, 221, 408, 697, 857, 1174, 941, 374, 889, 950, 832, 132, 592, 4, 984, 471, 1088, 381, 491, 390, 126, 516, 642, 305, 1064, 1085, 706, 936, 510, 456, 987, 965, 916, 404, 1093, 1076, 140, 816, 772, 770, 594, 531, 547, 67, 655, 477, 980, 549, 551, 1161, 76, 274, 758, 820, 422, 976, 308, 264, 101, 169, 1086, 828, 233, 611, 1042, 188, 807, 561, 982, 855, 1046, 931, 259, 426, 159, 769, 77, 685, 1208, 581, 66, 1029, 427, 1098, 890, 1033, 1035, 1187, 786, 1166, 75, 290, 175, 263, 60, 445, 507, 789, 768, 215, 567, 398, 450, 334, 955, 49, 315, 1034, 602, 373, 487, 41, 1040, 171, 862, 492, 983, 699, 386, 580, 880, 5, 585, 95, 800, 133, 539, 1186, 1066, 247, 219, 746, 1180, 48, 668, 1021, 162, 227, 1010, 1028, 13, 178, 93, 402, 356, 307, 391, 497, 586, 1128, 651, 428, 461, 482, 708, 0, 434, 106, 974, 158, 1080, 448, 914, 346, 971, 266, 355, 625, 701, 563, 990, 129, 922, 692, 92, 1, 335, 1057, 667, 662, 211, 474, 333, 256, 1007, 97, 557, 366, 532, 136, 1050, 351, 562, 850, 766, 265, 297, 283, 309, 94, 848, 811, 559, 453, 70, 921, 1099, 1056, 899, 1137, 436, 963, 127, 599, 1024, 757, 760, 629, 30, 666, 1058, 1179, 638, 217, 439, 89, 1140, 944, 1003, 485, 1121, 573, 679, 575, 782, 73, 623, 750, 595, 851, 111, 806, 898, 440, 268, 44, 42, 416, 484, 199, 361, 301, 479, 784, 245, 767, 1082, 588, 189, 71, 988, 103, 731, 790, 1001, 1009, 690, 1000, 659, 1107, 359, 455, 826, 393, 1078, 1192, 515, 252, 1144, 343, 700, 718, 1172, 647, 151, 258, 1185, 124, 1072, 325, 1168, 222, 1206, 957, 128, 107, 892, 192, 609, 424, 645, 236, 814, 1036, 480, 534, 163, 1132, 707, 1164, 568, 392, 338, 509, 882, 603, 379, 1125, 146, 799, 591, 649, 958, 587, 1193, 1197, 1123, 792, 1065, 260, 206, 1070, 780, 318, 415, 395, 1043, 815, 454, 646, 1190, 878, 529, 883, 458, 37, 730, 733, 656, 870, 421, 267, 933, 1077, 570, 414, 747, 1067, 947, 923, 762, 524, 743, 967, 134, 377, 1044, 583, 238, 845, 1079, 340, 430, 1204, 1049, 952, 1200, 681, 279, 311, 992, 871, 207, 1142, 300, 119, 613, 33, 339, 1014, 837, 213, 617, 407, 1060, 558, 776, 680, 122, 387, 153, 1165, 907, 804, 342, 1102, 840, 399, 716, 801, 1018, 775, 388, 214, 1139, 888, 571, 350, 576, 537, 341, 803, 550, 858, 664, 981, 326, 879, 1114, 773, 90, 425, 149, 520, 131, 1188, 1026, 903, 624, 29, 853, 353, 809, 1117, 953, 1131, 689, 755, 911, 438, 540, 925, 28, 367, 1189, 272, 1016, 1203, 917, 242, 320, 8, 754, 117, 842, 1090, 835, 927, 596, 928, 100, 304, 915, 831, 548, 744, 244, 942, 442, 62, 966, 1032, 369, 32, 839, 1048, 1041, 12, 142, 997, 634, 536, 821, 905, 35, 1108, 210, 1017, 682, 9, 1153, 843, 187, 678, 579, 87, 1199, 150, 633, 695, 429, 1170, 232, 23, 726, 765, 357, 1201, 409, 261, 774, 628, 237, 643, 1213, 80, 449, 873, 723, 891, 444, 759, 732, 1052, 383, 31, 1184, 64, 55, 630, 1122, 56, 156, 1030, 371, 486, 709, 589, 869, 504, 1141, 872, 51, 605, 1091, 637, 1012, 27, 1096, 544, 737, 500, 1022, 1051, 670, 618, 166, 527, 793, 574, 1015, 191, 1013, 394, 543, 401, 1173, 50, 179, 616, 287, 45, 1109, 834, 639, 864, 694, 431, 312, 938, 1045, 909, 203, 181, 1129, 631, 569, 11, 564, 229, 54, 1133, 601, 288, 1154, 867, 234, 1124, 714, 959, 209, 174, 230, 1027, 382, 818, 771, 610, 194, 475, 669, 202, 226, 535, 437, 721, 640, 734, 155, 1195, 251, 173, 460, 742, 6, 753, 822, 673, 384, 172, 193, 671, 626, 322, 720, 180, 121, 735, 787, 176, 908, 59, 494, 1176, 761, 291, 937, 1181, 473, 829, 403, 691, 863, 25, 866, 375, 420, 498, 675, 141, 489, 483, 712, 584, 152, 741, 830, 490, 43, 82, 1113, 400, 525, 1177, 496, 600, 1059, 462, 900, 541, 752, 493, 1073, 852, 364, 827, 472, 397, 802, 975, 518, 1134, 1039, 1156, 91, 216, 69, 165, 302, 277, 465, 469, 190, 197, 495, 748, 632, 865, 249, 330, 713, 269, 503, 336, 552, 650, 299, 725, 61, 347, 808, 1006, 317, 764, 389, 1143, 137, 756, 1110, 332, 998, 621, 457, 447, 316, 423, 1103, 904, 751, 130, 989, 606, 345, 275, 144, 1158, 590, 294, 289, 443, 1047, 411, 530, 969, 526, 235, 212, 859, 1025, 105, 26, 196, 788, 608, 1209, 704, 86, 546, 501, 183, 719, 84, 1183, 847, 791, 1194, 566, 354, 108, 254, 182, 348, 665, 413, 962, 895, 785, 1169, 278, 164, 452, 1157, 1106, 817, 1037, 406, 1202, 1100, 38, 419, 378, 810, 99, 577, 924, 280, 996, 40, 293, 468, 160, 964, 627, 565, 218, 887, 805, 1094, 257, 21, 147, 763, 370, 702, 385, 148, 1151, 876, 120, 285, 968, 16, 125, 663, 674, 514, 994, 208, 572, 380, 874, 636, 243, 961, 932, 1120, 795, 838, 396, 906, 1055, 337, 79, 687, 711, 1005, 919, 644, 72, 1147, 186, 693, 1182, 620, 123, 615, 978, 896, 522, 1119, 1053, 306, 195, 672, 435, 972, 1019, 619, 295, 225, 418, 83, 502, 466, 1087, 470, 877, 783, 1075, 1163, 255, 154, 677, 970, 323, 935, 467, 47, 949, 598, 686, 102, 885, 926, 441, 542, 954, 246, 728, 635, 511, 201, 161, 114, 115, 284, 1054, 1212, 901, 331, 849, 248, 999, 18, 897, 292, 555, 991, 1178, 220, 854, 945, 894, 98, 363, 521, 372, 688, 281, 508, 205, 282, 1083, 109, 451, 533, 1004, 405, 296, 1149, 607, 512, 684, 825, 2, 1011, 582, 271, 505, 657, 519, 736, 960, 979, 836, 676, 554, 1105, 85, 167, 352, 327, 794, 1062, 1069, 1002, 1167, 506, 696, 604, 920, 365, 986, 17, 168, 1008, 57, 833, 52, 717, 1023, 724, 956, 276, 660, 36, 253]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2172405729290142
the save name prefix for this run is:  chkpt-ID_2172405729290142_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 791
rank avg (pred): 0.587 +- 0.007
mrr vals (pred, true): 0.000, 0.001
batch losses (mrrl, rdl): 0.0, 0.0002770998

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 184
rank avg (pred): 0.418 +- 0.254
mrr vals (pred, true): 0.120, 0.001
batch losses (mrrl, rdl): 0.0, 6.74521e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 608
rank avg (pred): 0.379 +- 0.290
mrr vals (pred, true): 0.203, 0.056
batch losses (mrrl, rdl): 0.0, 4.23004e-05

Epoch over!
epoch time: 11.939

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1028
rank avg (pred): 0.374 +- 0.280
mrr vals (pred, true): 0.195, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001475997

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 815
rank avg (pred): 0.220 +- 0.183
mrr vals (pred, true): 0.235, 0.010
batch losses (mrrl, rdl): 0.0, 2.20008e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 192
rank avg (pred): 0.399 +- 0.298
mrr vals (pred, true): 0.197, 0.001
batch losses (mrrl, rdl): 0.0, 7.35194e-05

Epoch over!
epoch time: 11.869

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 332
rank avg (pred): 0.368 +- 0.297
mrr vals (pred, true): 0.222, 0.051
batch losses (mrrl, rdl): 0.0, 2.41066e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 557
rank avg (pred): 0.198 +- 0.164
mrr vals (pred, true): 0.249, 0.175
batch losses (mrrl, rdl): 0.0, 6.57318e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 3
rank avg (pred): 0.079 +- 0.064
mrr vals (pred, true): 0.229, 0.285
batch losses (mrrl, rdl): 0.0, 1.74534e-05

Epoch over!
epoch time: 11.954

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 573
rank avg (pred): 0.418 +- 0.280
mrr vals (pred, true): 0.160, 0.052
batch losses (mrrl, rdl): 0.0, 0.0001132799

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 117
rank avg (pred): 0.370 +- 0.295
mrr vals (pred, true): 0.215, 0.060
batch losses (mrrl, rdl): 0.0, 6.04582e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1212
rank avg (pred): 0.401 +- 0.294
mrr vals (pred, true): 0.220, 0.001
batch losses (mrrl, rdl): 0.0, 7.47711e-05

Epoch over!
epoch time: 11.914

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 383
rank avg (pred): 0.399 +- 0.306
mrr vals (pred, true): 0.214, 0.055
batch losses (mrrl, rdl): 0.0, 0.0001077955

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 49
rank avg (pred): 0.090 +- 0.075
mrr vals (pred, true): 0.239, 0.220
batch losses (mrrl, rdl): 0.0, 6.27195e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1099
rank avg (pred): 0.385 +- 0.304
mrr vals (pred, true): 0.216, 0.057
batch losses (mrrl, rdl): 0.0, 7.80227e-05

Epoch over!
epoch time: 11.975

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 807
rank avg (pred): 0.539 +- 0.347
mrr vals (pred, true): 0.164, 0.001
batch losses (mrrl, rdl): 0.130787015, 0.0001287048

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1214
rank avg (pred): 0.480 +- 0.252
mrr vals (pred, true): 0.078, 0.001
batch losses (mrrl, rdl): 0.0076028267, 1.56631e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 430
rank avg (pred): 0.479 +- 0.223
mrr vals (pred, true): 0.058, 0.001
batch losses (mrrl, rdl): 0.0006968722, 2.71194e-05

Epoch over!
epoch time: 12.265

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 508
rank avg (pred): 0.067 +- 0.074
mrr vals (pred, true): 0.255, 0.274
batch losses (mrrl, rdl): 0.003579237, 0.000438075

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 350
rank avg (pred): 0.474 +- 0.211
mrr vals (pred, true): 0.056, 0.063
batch losses (mrrl, rdl): 0.0004150324, 0.0004911835

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 186
rank avg (pred): 0.484 +- 0.196
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 5.91118e-05, 3.71481e-05

Epoch over!
epoch time: 12.378

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 455
rank avg (pred): 0.476 +- 0.201
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.67712e-05, 3.28697e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 770
rank avg (pred): 0.547 +- 0.200
mrr vals (pred, true): 0.040, 0.000
batch losses (mrrl, rdl): 0.0009209504, 4.22458e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 557
rank avg (pred): 0.250 +- 0.176
mrr vals (pred, true): 0.153, 0.175
batch losses (mrrl, rdl): 0.0048544351, 8.12819e-05

Epoch over!
epoch time: 12.175

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 267
rank avg (pred): 0.033 +- 0.027
mrr vals (pred, true): 0.295, 0.303
batch losses (mrrl, rdl): 0.0005993191, 1.86137e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 220
rank avg (pred): 0.433 +- 0.191
mrr vals (pred, true): 0.054, 0.001
batch losses (mrrl, rdl): 0.0001934187, 8.31562e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 852
rank avg (pred): 0.528 +- 0.200
mrr vals (pred, true): 0.044, 0.000
batch losses (mrrl, rdl): 0.0004066114, 0.0001496318

Epoch over!
epoch time: 12.33

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1076
rank avg (pred): 0.093 +- 0.067
mrr vals (pred, true): 0.222, 0.241
batch losses (mrrl, rdl): 0.0037669544, 2.89712e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 721
rank avg (pred): 0.438 +- 0.179
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002072494, 0.0001009663

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 207
rank avg (pred): 0.432 +- 0.182
mrr vals (pred, true): 0.055, 0.001
batch losses (mrrl, rdl): 0.0002996314, 0.0001086558

Epoch over!
epoch time: 12.466

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 151
rank avg (pred): 0.448 +- 0.180
mrr vals (pred, true): 0.048, 0.054
batch losses (mrrl, rdl): 3.13074e-05, 0.0002896895

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 591
rank avg (pred): 0.488 +- 0.197
mrr vals (pred, true): 0.046, 0.067
batch losses (mrrl, rdl): 0.0001617773, 0.0004313176

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1152
rank avg (pred): 0.240 +- 0.166
mrr vals (pred, true): 0.179, 0.176
batch losses (mrrl, rdl): 6.40992e-05, 7.45845e-05

Epoch over!
epoch time: 12.21

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 524
rank avg (pred): 0.230 +- 0.153
mrr vals (pred, true): 0.167, 0.177
batch losses (mrrl, rdl): 0.0008408554, 7.74277e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 589
rank avg (pred): 0.484 +- 0.208
mrr vals (pred, true): 0.048, 0.073
batch losses (mrrl, rdl): 6.16911e-05, 0.0003792348

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1158
rank avg (pred): 0.088 +- 0.063
mrr vals (pred, true): 0.250, 0.226
batch losses (mrrl, rdl): 0.0054056034, 0.0004337306

Epoch over!
epoch time: 12.235

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 86
rank avg (pred): 0.457 +- 0.159
mrr vals (pred, true): 0.046, 0.048
batch losses (mrrl, rdl): 0.0001538475, 0.0002556888

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 692
rank avg (pred): 0.454 +- 0.170
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 8.0691e-05, 7.25071e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 493
rank avg (pred): 0.244 +- 0.176
mrr vals (pred, true): 0.251, 0.249
batch losses (mrrl, rdl): 3.28338e-05, 0.000121362

Epoch over!
epoch time: 12.281

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 456
rank avg (pred): 0.451 +- 0.182
mrr vals (pred, true): 0.049, 0.001
batch losses (mrrl, rdl): 1.00152e-05, 8.25867e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 222
rank avg (pred): 0.456 +- 0.191
mrr vals (pred, true): 0.048, 0.001
batch losses (mrrl, rdl): 2.91798e-05, 7.52332e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 983
rank avg (pred): 0.178 +- 0.136
mrr vals (pred, true): 0.204, 0.192
batch losses (mrrl, rdl): 0.0012384319, 0.0001052568

Epoch over!
epoch time: 12.354

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 438
rank avg (pred): 0.453 +- 0.210
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 2.3367e-06, 7.49454e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 664
rank avg (pred): 0.428 +- 0.179
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 3.128e-07, 0.0001329725

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 232
rank avg (pred): 0.449 +- 0.212
mrr vals (pred, true): 0.050, 0.000
batch losses (mrrl, rdl): 9.19e-07, 8.78501e-05

Epoch over!
epoch time: 12.198

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.406 +- 0.165
mrr vals (pred, true): 0.051, 0.001

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    4 	     0 	 0.04243 	 8e-0500 	 m..s
    5 	     1 	 0.04507 	 0.00013 	 m..s
   44 	     2 	 0.04955 	 0.00048 	 m..s
   71 	     3 	 0.05352 	 0.00048 	 m..s
   13 	     4 	 0.04595 	 0.00049 	 m..s
   41 	     5 	 0.04939 	 0.00052 	 m..s
    1 	     6 	 0.03601 	 0.00054 	 m..s
   61 	     7 	 0.05040 	 0.00055 	 m..s
   62 	     8 	 0.05042 	 0.00055 	 m..s
   65 	     9 	 0.05094 	 0.00056 	 m..s
    0 	    10 	 0.03543 	 0.00057 	 m..s
   40 	    11 	 0.04938 	 0.00058 	 m..s
   34 	    12 	 0.04924 	 0.00058 	 m..s
   28 	    13 	 0.04905 	 0.00059 	 m..s
   26 	    14 	 0.04888 	 0.00059 	 m..s
   37 	    15 	 0.04932 	 0.00060 	 m..s
   25 	    16 	 0.04882 	 0.00064 	 m..s
    2 	    17 	 0.03627 	 0.00065 	 m..s
   60 	    18 	 0.05031 	 0.00066 	 m..s
   64 	    19 	 0.05083 	 0.00066 	 m..s
   56 	    20 	 0.04985 	 0.00067 	 m..s
   15 	    21 	 0.04634 	 0.00068 	 m..s
   42 	    22 	 0.04946 	 0.00068 	 m..s
   50 	    23 	 0.04967 	 0.00068 	 m..s
   66 	    24 	 0.05107 	 0.00070 	 m..s
   51 	    25 	 0.04967 	 0.00072 	 m..s
   57 	    26 	 0.04993 	 0.00072 	 m..s
   23 	    27 	 0.04875 	 0.00075 	 m..s
   22 	    28 	 0.04871 	 0.00075 	 m..s
   11 	    29 	 0.04591 	 0.00078 	 m..s
   43 	    30 	 0.04950 	 0.00079 	 m..s
    6 	    31 	 0.04530 	 0.00080 	 m..s
   18 	    32 	 0.04858 	 0.00081 	 m..s
    3 	    33 	 0.03906 	 0.00086 	 m..s
   35 	    34 	 0.04925 	 0.00090 	 m..s
    7 	    35 	 0.04564 	 0.00090 	 m..s
   46 	    36 	 0.04957 	 0.00094 	 m..s
   55 	    37 	 0.04985 	 0.00098 	 m..s
   19 	    38 	 0.04863 	 0.00099 	 m..s
   68 	    39 	 0.05145 	 0.00106 	 m..s
   21 	    40 	 0.04870 	 0.00116 	 m..s
   14 	    41 	 0.04617 	 0.00255 	 m..s
   12 	    42 	 0.04592 	 0.00416 	 m..s
    9 	    43 	 0.04584 	 0.00540 	 m..s
    8 	    44 	 0.04577 	 0.00570 	 m..s
   10 	    45 	 0.04587 	 0.00635 	 m..s
   74 	    46 	 0.06177 	 0.00866 	 m..s
   73 	    47 	 0.05576 	 0.01348 	 m..s
   47 	    48 	 0.04959 	 0.03871 	 ~...
   48 	    49 	 0.04965 	 0.04001 	 ~...
   33 	    50 	 0.04923 	 0.04191 	 ~...
   31 	    51 	 0.04918 	 0.04381 	 ~...
   24 	    52 	 0.04877 	 0.05080 	 ~...
   49 	    53 	 0.04965 	 0.05084 	 ~...
   52 	    54 	 0.04968 	 0.05199 	 ~...
   53 	    55 	 0.04975 	 0.05231 	 ~...
   63 	    56 	 0.05081 	 0.05438 	 ~...
   72 	    57 	 0.05466 	 0.05560 	 ~...
   16 	    58 	 0.04845 	 0.05574 	 ~...
   75 	    59 	 0.06796 	 0.05583 	 ~...
   20 	    60 	 0.04866 	 0.05605 	 ~...
   45 	    61 	 0.04956 	 0.05653 	 ~...
   30 	    62 	 0.04917 	 0.05693 	 ~...
   54 	    63 	 0.04985 	 0.05721 	 ~...
   36 	    64 	 0.04927 	 0.05833 	 ~...
   32 	    65 	 0.04921 	 0.05889 	 ~...
   38 	    66 	 0.04933 	 0.06035 	 ~...
   39 	    67 	 0.04934 	 0.06472 	 ~...
   70 	    68 	 0.05342 	 0.06508 	 ~...
   59 	    69 	 0.05025 	 0.06840 	 ~...
   58 	    70 	 0.05005 	 0.07174 	 ~...
   26 	    71 	 0.04888 	 0.07304 	 ~...
   67 	    72 	 0.05122 	 0.07577 	 ~...
   17 	    73 	 0.04854 	 0.07837 	 ~...
   29 	    74 	 0.04912 	 0.07943 	 m..s
   69 	    75 	 0.05172 	 0.08195 	 m..s
   77 	    76 	 0.13665 	 0.12182 	 ~...
   79 	    77 	 0.13827 	 0.12640 	 ~...
   78 	    78 	 0.13706 	 0.12784 	 ~...
   85 	    79 	 0.16604 	 0.13444 	 m..s
   83 	    80 	 0.15438 	 0.13967 	 ~...
   76 	    81 	 0.13170 	 0.14021 	 ~...
   90 	    82 	 0.18022 	 0.14364 	 m..s
   82 	    83 	 0.14508 	 0.14379 	 ~...
   88 	    84 	 0.17911 	 0.14511 	 m..s
   91 	    85 	 0.18327 	 0.14811 	 m..s
   81 	    86 	 0.14300 	 0.15855 	 ~...
   80 	    87 	 0.14042 	 0.16226 	 ~...
  105 	    88 	 0.24532 	 0.17952 	 m..s
   84 	    89 	 0.16529 	 0.17952 	 ~...
   87 	    90 	 0.17389 	 0.18313 	 ~...
   86 	    91 	 0.16650 	 0.19277 	 ~...
   89 	    92 	 0.17958 	 0.19335 	 ~...
   94 	    93 	 0.19208 	 0.20773 	 ~...
   97 	    94 	 0.19584 	 0.21193 	 ~...
   95 	    95 	 0.19296 	 0.21217 	 ~...
  100 	    96 	 0.23347 	 0.21385 	 ~...
   92 	    97 	 0.18471 	 0.22069 	 m..s
   99 	    98 	 0.22674 	 0.22448 	 ~...
  101 	    99 	 0.23422 	 0.22507 	 ~...
   93 	   100 	 0.18500 	 0.22688 	 m..s
  104 	   101 	 0.24251 	 0.22805 	 ~...
  102 	   102 	 0.23472 	 0.22966 	 ~...
  103 	   103 	 0.23805 	 0.23004 	 ~...
   96 	   104 	 0.19493 	 0.23608 	 m..s
   98 	   105 	 0.20135 	 0.24271 	 m..s
  107 	   106 	 0.26513 	 0.25200 	 ~...
  106 	   107 	 0.25364 	 0.25221 	 ~...
  108 	   108 	 0.28172 	 0.25365 	 ~...
  109 	   109 	 0.28407 	 0.27010 	 ~...
  110 	   110 	 0.28429 	 0.27124 	 ~...
  111 	   111 	 0.30093 	 0.27640 	 ~...
  113 	   112 	 0.30334 	 0.28142 	 ~...
  115 	   113 	 0.30548 	 0.28326 	 ~...
  114 	   114 	 0.30475 	 0.29490 	 ~...
  112 	   115 	 0.30175 	 0.30201 	 ~...
  119 	   116 	 0.31667 	 0.30219 	 ~...
  120 	   117 	 0.31956 	 0.30244 	 ~...
  118 	   118 	 0.31499 	 0.30439 	 ~...
  117 	   119 	 0.31354 	 0.30463 	 ~...
  116 	   120 	 0.30981 	 0.30519 	 ~...
==========================================
r_mrr = 0.9654810428619385
r2_mrr = 0.8956489562988281
spearmanr_mrr@5 = 0.9140110015869141
spearmanr_mrr@10 = 0.8457918763160706
spearmanr_mrr@50 = 0.9863940477371216
spearmanr_mrr@100 = 0.9750159978866577
spearmanr_mrr@All = 0.9750338196754456
==========================================
test time: 0.472
Done Testing dataset OpenEA
total time taken: 198.81310033798218
training time taken: 183.08869242668152
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9655)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8956)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9140)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.8458)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9864)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9750)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9750)}}, 'test_loss': {'DistMult': {'OpenEA': 0.27363674155640183}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 2867934705572319
Starting TWIG!
Loading datasets
Loading DistMult...
Loading OpenEA...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [273, 1069, 167, 1062, 1148, 829, 845, 259, 648, 962, 198, 1091, 768, 465, 229, 71, 928, 125, 1133, 500, 1060, 258, 540, 481, 240, 402, 1097, 954, 1038, 596, 750, 268, 618, 433, 541, 1008, 159, 634, 670, 808, 621, 424, 788, 1178, 515, 435, 440, 960, 185, 989, 306, 580, 869, 1096, 1162, 278, 628, 676, 1093, 523, 727, 778, 573, 1183, 1186, 54, 480, 1005, 362, 723, 30, 1037, 0, 272, 737, 1023, 158, 665, 529, 572, 299, 138, 162, 333, 751, 155, 642, 130, 1111, 80, 524, 1003, 1120, 86, 1179, 686, 936, 193, 886, 752, 1079, 669, 581, 985, 445, 422, 320, 630, 196, 485, 836, 406, 50, 766, 1212, 121, 399, 318, 224, 862, 970]
valid_ids (0): []
train_ids (1094): [549, 986, 96, 969, 965, 8, 1174, 99, 827, 1116, 1207, 95, 823, 351, 565, 415, 213, 458, 476, 378, 918, 108, 764, 660, 493, 291, 254, 854, 508, 7, 620, 917, 41, 872, 847, 797, 644, 559, 852, 382, 721, 404, 56, 753, 395, 1129, 900, 667, 1044, 1134, 1034, 76, 1042, 1196, 1015, 757, 932, 899, 856, 168, 428, 60, 156, 576, 106, 256, 895, 470, 153, 958, 334, 939, 1172, 715, 189, 225, 1100, 1142, 521, 1105, 477, 762, 871, 979, 688, 612, 833, 1201, 771, 232, 1039, 649, 934, 709, 898, 52, 641, 503, 188, 817, 342, 1081, 956, 288, 756, 165, 1077, 1110, 301, 1199, 449, 777, 1108, 1, 821, 251, 443, 5, 365, 774, 711, 1094, 453, 53, 851, 81, 510, 1040, 608, 116, 358, 674, 1175, 681, 38, 384, 587, 197, 993, 905, 244, 740, 853, 1157, 180, 902, 890, 484, 151, 706, 386, 287, 1188, 200, 906, 730, 1022, 83, 974, 369, 437, 326, 680, 84, 646, 945, 135, 1019, 471, 911, 638, 846, 784, 325, 13, 266, 296, 606, 139, 1128, 271, 653, 277, 230, 340, 889, 1000, 617, 1187, 640, 514, 867, 380, 1112, 792, 594, 673, 302, 1018, 354, 913, 113, 1011, 1151, 915, 131, 261, 550, 805, 62, 564, 448, 136, 909, 262, 554, 769, 316, 314, 35, 461, 544, 699, 988, 603, 1125, 749, 298, 849, 822, 147, 1047, 242, 370, 964, 300, 607, 1055, 943, 473, 166, 1084, 772, 619, 357, 692, 190, 210, 187, 701, 528, 922, 234, 442, 178, 1029, 379, 941, 828, 700, 839, 388, 1090, 1159, 401, 546, 951, 1198, 629, 492, 475, 1088, 819, 359, 102, 947, 1043, 601, 1113, 1024, 579, 114, 560, 1017, 809, 746, 679, 313, 450, 352, 894, 537, 513, 177, 434, 1049, 1036, 781, 840, 48, 51, 431, 720, 85, 545, 661, 775, 1074, 91, 982, 657, 134, 604, 233, 491, 4, 1184, 1181, 1004, 593, 743, 356, 1013, 861, 1169, 149, 331, 536, 425, 441, 639, 360, 439, 810, 1130, 1126, 322, 1053, 1086, 1149, 1204, 971, 726, 487, 668, 78, 3, 403, 1076, 966, 1177, 870, 297, 804, 563, 901, 882, 803, 1046, 526, 1213, 409, 307, 908, 144, 164, 1085, 677, 488, 247, 710, 937, 1058, 1176, 760, 1194, 411, 570, 509, 345, 1132, 94, 289, 1164, 1109, 28, 1059, 636, 977, 651, 685, 24, 1098, 145, 92, 548, 115, 120, 184, 684, 530, 235, 430, 119, 160, 1123, 381, 744, 623, 332, 1061, 348, 782, 260, 65, 209, 578, 383, 157, 400, 252, 930, 542, 285, 31, 516, 324, 191, 474, 655, 1200, 935, 49, 864, 891, 408, 100, 44, 281, 192, 598, 1027, 372, 495, 691, 798, 447, 812, 972, 506, 174, 645, 19, 517, 218, 724, 245, 837, 204, 172, 1156, 843, 57, 678, 610, 779, 831, 1057, 1095, 309, 920, 1206, 417, 1078, 1001, 944, 703, 525, 926, 888, 72, 66, 1168, 1021, 532, 534, 647, 759, 697, 583, 21, 783, 616, 223, 995, 222, 444, 1064, 104, 967, 738, 531, 1124, 269, 1160, 1182, 276, 733, 787, 754, 339, 558, 844, 820, 780, 707, 363, 103, 719, 9, 1103, 838, 903, 1115, 652, 1192, 925, 734, 816, 1009, 146, 1016, 90, 722, 690, 877, 295, 466, 410, 884, 976, 1092, 343, 940, 555, 23, 875, 248, 148, 518, 605, 270, 58, 6, 1209, 519, 814, 539, 328, 214, 929, 127, 512, 10, 1006, 1185, 868, 459, 698, 330, 758, 42, 212, 98, 567, 385, 566, 61, 815, 141, 373, 413, 118, 991, 143, 597, 226, 535, 1150, 132, 946, 1155, 88, 834, 338, 129, 973, 483, 632, 1070, 446, 527, 712, 1173, 835, 396, 635, 75, 405, 883, 239, 1170, 1138, 274, 1203, 571, 137, 574, 859, 228, 1161, 765, 350, 1014, 631, 586, 317, 414, 975, 490, 702, 1214, 552, 662, 1211, 950, 507, 1137, 1141, 217, 142, 745, 264, 714, 310, 865, 874, 1054, 938, 39, 455, 556, 390, 561, 1121, 171, 377, 824, 551, 418, 1165, 241, 472, 795, 801, 216, 992, 1056, 613, 436, 1205, 429, 695, 452, 921, 663, 650, 767, 366, 68, 93, 286, 112, 105, 1143, 305, 857, 280, 785, 876, 624, 367, 451, 391, 879, 195, 694, 1102, 207, 1208, 394, 1050, 708, 625, 376, 1045, 1107, 17, 959, 265, 186, 654, 283, 599, 194, 791, 1135, 1114, 64, 832, 591, 1025, 220, 32, 375, 907, 253, 687, 637, 562, 237, 543, 881, 133, 150, 1041, 454, 152, 1158, 392, 1166, 553, 109, 59, 825, 892, 124, 412, 77, 1106, 615, 948, 501, 371, 1026, 1020, 931, 312, 996, 557, 353, 584, 689, 855, 89, 63, 199, 349, 742, 1190, 1118, 547, 463, 910, 1193, 347, 761, 999, 206, 981, 456, 1153, 215, 208, 643, 1117, 811, 73, 1197, 957, 850, 998, 980, 329, 614, 683, 468, 1051, 656, 1145, 464, 74, 387, 729, 997, 739, 2, 374, 671, 896, 842, 609, 904, 666, 675, 111, 1089, 290, 292, 978, 1007, 522, 589, 1202, 323, 494, 588, 716, 728, 70, 294, 497, 1033, 696, 161, 1104, 1012, 40, 961, 955, 421, 592, 478, 863, 154, 987, 55, 600, 236, 914, 1163, 924, 101, 97, 736, 860, 858, 1136, 953, 1147, 255, 368, 438, 622, 1146, 337, 43, 46, 467, 117, 173, 311, 990, 346, 489, 123, 246, 140, 182, 1140, 813, 848, 963, 718, 15, 419, 802, 18, 1191, 704, 968, 87, 994, 1075, 29, 201, 179, 426, 407, 1073, 279, 933, 12, 1083, 1065, 1063, 122, 505, 175, 1082, 82, 912, 585, 397, 682, 773, 47, 885, 763, 1032, 243, 664, 611, 169, 45, 748, 511, 126, 893, 282, 569, 79, 460, 1010, 69, 1071, 202, 984, 327, 176, 308, 504, 211, 227, 732, 1144, 658, 389, 1031, 927, 1066, 1171, 238, 1127, 293, 1099, 275, 267, 486, 878, 582, 713, 1072, 20, 577, 361, 16, 336, 1028, 806, 183, 747, 741, 533, 1048, 335, 923, 538, 344, 416, 1189, 36, 659, 11, 303, 107, 284, 799, 250, 818, 1139, 693, 315, 873, 1180, 887, 786, 34, 830, 1101, 163, 27, 457, 776, 26, 800, 203, 1122, 1087, 1002, 181, 897, 219, 432, 393, 983, 602, 790, 1035, 469, 257, 22, 916, 626, 355, 423, 502, 420, 633, 1052, 1154, 482, 919, 462, 731, 590, 826, 725, 1152, 841, 321, 952, 568, 949, 364, 942, 627, 880, 595, 37, 575, 398, 1167, 249, 1119, 304, 1067, 427, 796, 479, 231, 1030, 496, 25, 170, 807, 789, 1068, 1195, 794, 33, 14, 1131, 205, 498, 755, 499, 705, 341, 319, 128, 793, 672, 263, 735, 67, 866, 1210, 520, 110, 770, 1080, 221, 717]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1812625387827192
the save name prefix for this run is:  chkpt-ID_1812625387827192_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'DistMult': {'OpenEA': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 564
rank avg (pred): 0.518 +- 0.010
mrr vals (pred, true): 0.000, 0.173
batch losses (mrrl, rdl): 0.0, 0.0017721915

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1054
rank avg (pred): 0.092 +- 0.061
mrr vals (pred, true): 0.080, 0.269
batch losses (mrrl, rdl): 0.0, 3.33202e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1089
rank avg (pred): 0.410 +- 0.289
mrr vals (pred, true): 0.153, 0.048
batch losses (mrrl, rdl): 0.0, 0.0001483361

Epoch over!
epoch time: 12.186

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 96
rank avg (pred): 0.388 +- 0.282
mrr vals (pred, true): 0.160, 0.076
batch losses (mrrl, rdl): 0.0, 0.0001141491

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 513
rank avg (pred): 0.181 +- 0.142
mrr vals (pred, true): 0.240, 0.131
batch losses (mrrl, rdl): 0.0, 9.83353e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 792
rank avg (pred): 0.458 +- 0.317
mrr vals (pred, true): 0.122, 0.001
batch losses (mrrl, rdl): 0.0, 1.58104e-05

Epoch over!
epoch time: 11.992

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1146
rank avg (pred): 0.177 +- 0.140
mrr vals (pred, true): 0.235, 0.212
batch losses (mrrl, rdl): 0.0, 0.0001132215

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 129
rank avg (pred): 0.368 +- 0.289
mrr vals (pred, true): 0.207, 0.066
batch losses (mrrl, rdl): 0.0, 6.07406e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 168
rank avg (pred): 0.377 +- 0.289
mrr vals (pred, true): 0.131, 0.001
batch losses (mrrl, rdl): 0.0, 0.0001571598

Epoch over!
epoch time: 11.941

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 557
rank avg (pred): 0.206 +- 0.168
mrr vals (pred, true): 0.209, 0.175
batch losses (mrrl, rdl): 0.0, 5.94668e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1166
rank avg (pred): 0.409 +- 0.294
mrr vals (pred, true): 0.023, 0.067
batch losses (mrrl, rdl): 0.0, 0.0001337335

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 17
rank avg (pred): 0.085 +- 0.072
mrr vals (pred, true): 0.123, 0.305
batch losses (mrrl, rdl): 0.0, 2.05372e-05

Epoch over!
epoch time: 11.751

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 754
rank avg (pred): 0.262 +- 0.220
mrr vals (pred, true): 0.063, 0.144
batch losses (mrrl, rdl): 0.0, 0.0001821828

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 898
rank avg (pred): 0.576 +- 0.388
mrr vals (pred, true): 0.035, 0.000
batch losses (mrrl, rdl): 0.0, 0.0011646509

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 290
rank avg (pred): 0.093 +- 0.088
mrr vals (pred, true): 0.089, 0.218
batch losses (mrrl, rdl): 0.0, 2.93462e-05

Epoch over!
epoch time: 12.014

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 951
rank avg (pred): 0.526 +- 0.357
mrr vals (pred, true): 0.026, 0.001
batch losses (mrrl, rdl): 0.0058308998, 9.94898e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1182
rank avg (pred): 0.417 +- 0.173
mrr vals (pred, true): 0.065, 0.058
batch losses (mrrl, rdl): 0.002181143, 0.0001505983

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1049
rank avg (pred): 0.428 +- 0.160
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 3.39499e-05, 0.000121106

Epoch over!
epoch time: 12.366

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 661
rank avg (pred): 0.437 +- 0.152
mrr vals (pred, true): 0.046, 0.001
batch losses (mrrl, rdl): 0.000162431, 0.0001229902

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 852
rank avg (pred): 0.512 +- 0.176
mrr vals (pred, true): 0.046, 0.000
batch losses (mrrl, rdl): 0.0001855713, 0.0001829589

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1087
rank avg (pred): 0.415 +- 0.163
mrr vals (pred, true): 0.055, 0.084
batch losses (mrrl, rdl): 0.0002028507, 0.0002408688

Epoch over!
epoch time: 12.405

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 103
rank avg (pred): 0.415 +- 0.160
mrr vals (pred, true): 0.055, 0.066
batch losses (mrrl, rdl): 0.0002301668, 0.000260297

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 156
rank avg (pred): 0.415 +- 0.154
mrr vals (pred, true): 0.055, 0.063
batch losses (mrrl, rdl): 0.0002346227, 0.0002264281

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 27
rank avg (pred): 0.131 +- 0.102
mrr vals (pred, true): 0.176, 0.141
batch losses (mrrl, rdl): 0.0124734547, 3.50259e-05

Epoch over!
epoch time: 12.252

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 34
rank avg (pred): 0.126 +- 0.095
mrr vals (pred, true): 0.190, 0.161
batch losses (mrrl, rdl): 0.0085610449, 3.55862e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 139
rank avg (pred): 0.425 +- 0.144
mrr vals (pred, true): 0.048, 0.053
batch losses (mrrl, rdl): 4.29369e-05, 0.0002260844

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1191
rank avg (pred): 0.428 +- 0.137
mrr vals (pred, true): 0.047, 0.001
batch losses (mrrl, rdl): 6.99152e-05, 0.0001515624

Epoch over!
epoch time: 12.191

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 68
rank avg (pred): 0.120 +- 0.083
mrr vals (pred, true): 0.200, 0.227
batch losses (mrrl, rdl): 0.0072730933, 3.17746e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1155
rank avg (pred): 0.347 +- 0.246
mrr vals (pred, true): 0.181, 0.208
batch losses (mrrl, rdl): 0.0071379687, 0.0004447488

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 935
rank avg (pred): 0.522 +- 0.169
mrr vals (pred, true): 0.045, 0.000
batch losses (mrrl, rdl): 0.0002864807, 0.0028827204

Epoch over!
epoch time: 12.094

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 429
rank avg (pred): 0.420 +- 0.149
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.7349e-06, 0.0001464253

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 123
rank avg (pred): 0.422 +- 0.146
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 2.81809e-05, 0.0001586355

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 396
rank avg (pred): 0.430 +- 0.139
mrr vals (pred, true): 0.045, 0.063
batch losses (mrrl, rdl): 0.0002431255, 0.000299375

Epoch over!
epoch time: 12.206

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 180
rank avg (pred): 0.419 +- 0.148
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 7.89e-08, 0.0001746903

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 104
rank avg (pred): 0.401 +- 0.163
mrr vals (pred, true): 0.059, 0.082
batch losses (mrrl, rdl): 0.0008971203, 0.0001964877

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 732
rank avg (pred): 0.367 +- 0.184
mrr vals (pred, true): 0.073, 0.007
batch losses (mrrl, rdl): 0.005268679, 0.0002915561

Epoch over!
epoch time: 12.133

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 340
rank avg (pred): 0.416 +- 0.144
mrr vals (pred, true): 0.051, 0.076
batch losses (mrrl, rdl): 1.27702e-05, 0.0002548725

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 331
rank avg (pred): 0.421 +- 0.138
mrr vals (pred, true): 0.050, 0.059
batch losses (mrrl, rdl): 1.2783e-06, 0.0002108882

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1113
rank avg (pred): 0.419 +- 0.138
mrr vals (pred, true): 0.050, 0.001
batch losses (mrrl, rdl): 1.3376e-06, 0.0001809798

Epoch over!
epoch time: 12.164

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 744
rank avg (pred): 0.116 +- 0.072
mrr vals (pred, true): 0.158, 0.138
batch losses (mrrl, rdl): 0.0039169635, 0.0002572668

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 377
rank avg (pred): 0.432 +- 0.127
mrr vals (pred, true): 0.050, 0.060
batch losses (mrrl, rdl): 2.3491e-06, 0.0002803187

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 987
rank avg (pred): 0.008 +- 0.005
mrr vals (pred, true): 0.274, 0.260
batch losses (mrrl, rdl): 0.0019468288, 0.000288998

Epoch over!
epoch time: 12.073

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 731
rank avg (pred): 0.388 +- 0.165
mrr vals (pred, true): 0.056, 0.081
batch losses (mrrl, rdl): 0.0003379161, 0.0016697148

running batch: 500 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 184
rank avg (pred): 0.413 +- 0.139
mrr vals (pred, true): 0.052, 0.001
batch losses (mrrl, rdl): 2.31337e-05, 0.0001949872

running batch: 1000 / 1094 and superbatch(1); data from DistMult, OpenEA, run 2.1, exp 1180
rank avg (pred): 0.420 +- 0.127
mrr vals (pred, true): 0.050, 0.056
batch losses (mrrl, rdl): 8.837e-07, 0.0001931146

Epoch over!
epoch time: 11.897

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG OpenEA
Running eval on the test set
running batch: 0
rank avg (pred): 0.176 +- 0.143
mrr vals (pred, true): 0.182, 0.145

Evaluation for OpenEA on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   19 	     0 	 0.04983 	 0.00047 	 m..s
   20 	     1 	 0.04983 	 0.00048 	 m..s
   16 	     2 	 0.04977 	 0.00048 	 m..s
    1 	     3 	 0.04762 	 0.00049 	 m..s
   46 	     4 	 0.05046 	 0.00050 	 m..s
   45 	     5 	 0.05045 	 0.00051 	 m..s
   65 	     6 	 0.05095 	 0.00055 	 m..s
    0 	     7 	 0.04568 	 0.00056 	 m..s
   74 	     8 	 0.05135 	 0.00056 	 m..s
    2 	     9 	 0.04767 	 0.00056 	 m..s
   61 	    10 	 0.05083 	 0.00057 	 m..s
   32 	    11 	 0.05002 	 0.00058 	 m..s
   57 	    12 	 0.05068 	 0.00060 	 m..s
   10 	    13 	 0.04951 	 0.00061 	 m..s
   49 	    14 	 0.05057 	 0.00062 	 m..s
   13 	    15 	 0.04960 	 0.00063 	 m..s
    5 	    16 	 0.04910 	 0.00064 	 m..s
   27 	    17 	 0.04994 	 0.00064 	 m..s
   44 	    18 	 0.05044 	 0.00065 	 m..s
   17 	    19 	 0.04977 	 0.00066 	 m..s
   85 	    20 	 0.05639 	 0.00067 	 m..s
   28 	    21 	 0.04996 	 0.00068 	 m..s
   22 	    22 	 0.04984 	 0.00068 	 m..s
   58 	    23 	 0.05069 	 0.00068 	 m..s
   24 	    24 	 0.04985 	 0.00069 	 m..s
   38 	    25 	 0.05013 	 0.00070 	 m..s
   11 	    26 	 0.04953 	 0.00070 	 m..s
   56 	    27 	 0.05068 	 0.00070 	 m..s
   14 	    28 	 0.04969 	 0.00071 	 m..s
   23 	    29 	 0.04984 	 0.00072 	 m..s
   79 	    30 	 0.05162 	 0.00073 	 m..s
   48 	    31 	 0.05048 	 0.00073 	 m..s
   75 	    32 	 0.05139 	 0.00074 	 m..s
   12 	    33 	 0.04955 	 0.00075 	 m..s
    3 	    34 	 0.04902 	 0.00078 	 m..s
    8 	    35 	 0.04941 	 0.00079 	 m..s
   21 	    36 	 0.04983 	 0.00084 	 m..s
   36 	    37 	 0.05010 	 0.00090 	 m..s
   82 	    38 	 0.05182 	 0.00090 	 m..s
    4 	    39 	 0.04903 	 0.00097 	 m..s
   43 	    40 	 0.05044 	 0.00098 	 m..s
   67 	    41 	 0.05096 	 0.00104 	 m..s
   42 	    42 	 0.05043 	 0.00112 	 m..s
   84 	    43 	 0.05232 	 0.00126 	 m..s
    6 	    44 	 0.04912 	 0.00134 	 m..s
   69 	    45 	 0.05109 	 0.00181 	 m..s
   81 	    46 	 0.05176 	 0.00255 	 m..s
   53 	    47 	 0.05061 	 0.00475 	 m..s
   55 	    48 	 0.05064 	 0.00500 	 m..s
   83 	    49 	 0.05206 	 0.00644 	 m..s
   35 	    50 	 0.05009 	 0.04733 	 ~...
   54 	    51 	 0.05063 	 0.04751 	 ~...
   52 	    52 	 0.05059 	 0.04831 	 ~...
   29 	    53 	 0.04996 	 0.04960 	 ~...
   78 	    54 	 0.05149 	 0.05021 	 ~...
   51 	    55 	 0.05059 	 0.05026 	 ~...
   72 	    56 	 0.05126 	 0.05072 	 ~...
   77 	    57 	 0.05148 	 0.05230 	 ~...
   41 	    58 	 0.05035 	 0.05231 	 ~...
   62 	    59 	 0.05086 	 0.05355 	 ~...
   80 	    60 	 0.05174 	 0.05384 	 ~...
   71 	    61 	 0.05119 	 0.05417 	 ~...
   15 	    62 	 0.04974 	 0.05563 	 ~...
    7 	    63 	 0.04927 	 0.05574 	 ~...
   60 	    64 	 0.05078 	 0.05587 	 ~...
   18 	    65 	 0.04980 	 0.05652 	 ~...
   68 	    66 	 0.05105 	 0.05653 	 ~...
   63 	    67 	 0.05091 	 0.05781 	 ~...
   47 	    68 	 0.05046 	 0.05828 	 ~...
   34 	    69 	 0.05003 	 0.06158 	 ~...
   31 	    70 	 0.04999 	 0.06177 	 ~...
   30 	    71 	 0.04997 	 0.06246 	 ~...
   50 	    72 	 0.05058 	 0.06293 	 ~...
   59 	    73 	 0.05077 	 0.06447 	 ~...
   37 	    74 	 0.05011 	 0.06494 	 ~...
   25 	    75 	 0.04985 	 0.06524 	 ~...
   70 	    76 	 0.05111 	 0.06554 	 ~...
    9 	    77 	 0.04951 	 0.06559 	 ~...
   39 	    78 	 0.05020 	 0.06622 	 ~...
   26 	    79 	 0.04993 	 0.07046 	 ~...
   33 	    80 	 0.05003 	 0.07299 	 ~...
   64 	    81 	 0.05092 	 0.07507 	 ~...
   66 	    82 	 0.05096 	 0.07617 	 ~...
   76 	    83 	 0.05147 	 0.07673 	 ~...
   40 	    84 	 0.05023 	 0.07720 	 ~...
   73 	    85 	 0.05131 	 0.07722 	 ~...
   87 	    86 	 0.14732 	 0.13046 	 ~...
   93 	    87 	 0.15862 	 0.13178 	 ~...
   88 	    88 	 0.14891 	 0.13221 	 ~...
   94 	    89 	 0.16017 	 0.13583 	 ~...
   90 	    90 	 0.15251 	 0.13955 	 ~...
   96 	    91 	 0.16367 	 0.14198 	 ~...
   89 	    92 	 0.15096 	 0.14372 	 ~...
   86 	    93 	 0.14538 	 0.14374 	 ~...
   97 	    94 	 0.16843 	 0.14394 	 ~...
   98 	    95 	 0.18188 	 0.14511 	 m..s
  101 	    96 	 0.20245 	 0.16047 	 m..s
  102 	    97 	 0.20301 	 0.16133 	 m..s
   95 	    98 	 0.16284 	 0.16887 	 ~...
   99 	    99 	 0.18460 	 0.17047 	 ~...
  108 	   100 	 0.21992 	 0.17081 	 m..s
   91 	   101 	 0.15552 	 0.17126 	 ~...
   92 	   102 	 0.15845 	 0.17666 	 ~...
  106 	   103 	 0.21860 	 0.18082 	 m..s
  105 	   104 	 0.21446 	 0.20655 	 ~...
  103 	   105 	 0.20438 	 0.21193 	 ~...
  109 	   106 	 0.22412 	 0.21315 	 ~...
  100 	   107 	 0.19654 	 0.21400 	 ~...
  110 	   108 	 0.22552 	 0.22049 	 ~...
  104 	   109 	 0.21354 	 0.22261 	 ~...
  107 	   110 	 0.21889 	 0.22936 	 ~...
  111 	   111 	 0.23812 	 0.25281 	 ~...
  113 	   112 	 0.24738 	 0.26103 	 ~...
  114 	   113 	 0.25280 	 0.26145 	 ~...
  112 	   114 	 0.24701 	 0.26253 	 ~...
  115 	   115 	 0.25870 	 0.26721 	 ~...
  116 	   116 	 0.27499 	 0.27913 	 ~...
  118 	   117 	 0.28852 	 0.29696 	 ~...
  117 	   118 	 0.28785 	 0.30026 	 ~...
  120 	   119 	 0.29525 	 0.30614 	 ~...
  119 	   120 	 0.29000 	 0.32380 	 m..s
==========================================
r_mrr = 0.9532657265663147
r2_mrr = 0.8520361185073853
spearmanr_mrr@5 = 0.9554729461669922
spearmanr_mrr@10 = 0.9650322198867798
spearmanr_mrr@50 = 0.9806687831878662
spearmanr_mrr@100 = 0.9624068140983582
spearmanr_mrr@All = 0.9602653384208679
==========================================
test time: 0.396
Done Testing dataset OpenEA
total time taken: 197.2978072166443
training time taken: 182.14603352546692
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'OpenEA': tensor(0.9533)}}, 'r2_mrr': {'DistMult': {'OpenEA': tensor(0.8520)}}, 'spearmanr_mrr@5': {'DistMult': {'OpenEA': tensor(0.9555)}}, 'spearmanr_mrr@10': {'DistMult': {'OpenEA': tensor(0.9650)}}, 'spearmanr_mrr@50': {'DistMult': {'OpenEA': tensor(0.9807)}}, 'spearmanr_mrr@100': {'DistMult': {'OpenEA': tensor(0.9624)}}, 'spearmanr_mrr@All': {'DistMult': {'OpenEA': tensor(0.9603)}}, 'test_loss': {'DistMult': {'OpenEA': 0.18008606307557784}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

===============================================================
---------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-loss
---------------------------------------------------------------
===============================================================
Running on a grid of size 1
Using random seed: 6356540888285267
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [769, 87, 592, 230, 665, 1117, 224, 1005, 924, 791, 1012, 25, 774, 741, 640, 1177, 676, 1175, 765, 469, 528, 39, 597, 1022, 115, 326, 440, 1089, 529, 1183, 4, 312, 1196, 576, 796, 546, 1108, 667, 1209, 617, 797, 746, 1140, 684, 995, 79, 188, 879, 27, 954, 1098, 1194, 623, 400, 424, 549, 764, 443, 779, 457, 490, 52, 955, 1066, 75, 985, 1006, 466, 1199, 289, 514, 706, 1195, 536, 92, 286, 887, 273, 480, 107, 788, 1128, 1214, 1122, 383, 1141, 385, 866, 541, 63, 532, 988, 239, 705, 1114, 896, 288, 1028, 1021, 975, 904, 609, 979, 453, 1146, 23, 1189, 1192, 262, 1008, 1020, 365, 714, 743, 504, 1084, 956, 118, 1061, 409, 448]
valid_ids (0): []
train_ids (1094): [276, 434, 1001, 538, 176, 957, 1166, 876, 53, 812, 168, 712, 161, 477, 451, 50, 733, 1212, 817, 936, 732, 611, 895, 1124, 1045, 635, 557, 1015, 859, 49, 583, 654, 467, 475, 923, 547, 297, 938, 252, 153, 622, 513, 946, 629, 739, 468, 15, 870, 185, 311, 16, 67, 343, 405, 548, 917, 878, 348, 74, 329, 174, 442, 666, 159, 108, 352, 331, 318, 726, 539, 1031, 959, 54, 47, 994, 1011, 540, 175, 1184, 1135, 89, 869, 757, 1046, 431, 200, 403, 197, 621, 523, 134, 64, 411, 822, 221, 700, 679, 905, 38, 939, 99, 35, 1118, 2, 776, 287, 1126, 958, 626, 551, 1, 1208, 398, 624, 800, 1093, 1151, 120, 1064, 196, 152, 685, 57, 178, 659, 429, 246, 897, 270, 966, 642, 997, 987, 1203, 267, 31, 494, 132, 638, 608, 1013, 484, 1123, 688, 497, 760, 110, 802, 669, 692, 1065, 643, 1047, 610, 521, 853, 444, 641, 96, 963, 1129, 360, 682, 1127, 750, 708, 1072, 186, 275, 821, 207, 281, 543, 1018, 804, 890, 1168, 898, 686, 689, 696, 672, 116, 1039, 1063, 1137, 1092, 157, 124, 103, 1100, 845, 1102, 1073, 423, 601, 272, 7, 978, 908, 603, 208, 104, 562, 412, 170, 1054, 1088, 839, 71, 1144, 868, 880, 893, 1167, 217, 264, 1062, 903, 965, 314, 1139, 40, 41, 293, 485, 282, 785, 742, 888, 234, 202, 1048, 449, 699, 1033, 1101, 581, 921, 430, 478, 225, 509, 974, 1105, 187, 674, 852, 336, 154, 740, 783, 930, 832, 1017, 810, 70, 1174, 268, 618, 1076, 518, 274, 1049, 369, 86, 941, 968, 932, 427, 826, 1121, 407, 775, 515, 1052, 1103, 929, 32, 292, 399, 1023, 579, 697, 66, 198, 359, 1142, 687, 269, 58, 373, 259, 1016, 702, 149, 323, 1145, 1120, 990, 195, 761, 462, 588, 19, 980, 1010, 219, 823, 1171, 942, 169, 203, 872, 1160, 432, 371, 856, 633, 349, 607, 565, 1155, 193, 1024, 1032, 848, 998, 829, 127, 752, 94, 1068, 945, 871, 1115, 310, 280, 709, 144, 332, 820, 652, 28, 495, 46, 439, 340, 486, 194, 177, 338, 33, 793, 719, 837, 366, 704, 222, 111, 693, 544, 350, 61, 698, 450, 465, 260, 136, 1173, 566, 671, 662, 1202, 782, 503, 807, 648, 1119, 631, 1161, 483, 656, 1110, 805, 59, 1143, 73, 780, 1069, 236, 1111, 1050, 999, 106, 1083, 561, 126, 925, 510, 842, 660, 794, 150, 381, 456, 716, 237, 316, 1136, 747, 458, 62, 482, 211, 394, 408, 657, 506, 137, 934, 192, 1152, 858, 818, 755, 645, 902, 651, 795, 964, 996, 410, 605, 1037, 786, 390, 1132, 1188, 567, 113, 637, 906, 296, 119, 675, 128, 320, 319, 644, 498, 860, 512, 231, 291, 758, 309, 838, 1026, 1204, 83, 44, 962, 68, 537, 8, 210, 604, 1014, 885, 1138, 1116, 1200, 1112, 78, 1190, 1198, 613, 285, 492, 1162, 971, 900, 522, 1034, 806, 1091, 1193, 294, 668, 139, 572, 1164, 241, 322, 460, 731, 146, 48, 1019, 1109, 815, 591, 505, 80, 745, 1056, 1071, 171, 535, 76, 101, 554, 673, 1150, 22, 375, 18, 650, 803, 166, 341, 681, 690, 91, 813, 232, 756, 727, 213, 553, 725, 940, 29, 147, 777, 1169, 722, 874, 694, 163, 1213, 26, 970, 1125, 701, 416, 358, 121, 1082, 125, 388, 370, 393, 1029, 1148, 661, 531, 599, 972, 335, 1009, 9, 798, 827, 949, 162, 730, 738, 206, 754, 784, 265, 14, 455, 873, 778, 1070, 952, 11, 209, 695, 406, 81, 301, 13, 257, 151, 441, 768, 459, 1149, 620, 658, 1179, 397, 1085, 347, 43, 402, 736, 333, 525, 315, 428, 167, 391, 636, 911, 916, 1003, 418, 619, 243, 436, 1186, 317, 6, 1051, 865, 248, 1187, 474, 36, 364, 1159, 17, 568, 10, 596, 723, 830, 993, 573, 950, 1131, 834, 395, 960, 363, 122, 1079, 586, 500, 533, 1035, 435, 901, 711, 182, 133, 487, 117, 1055, 244, 1201, 463, 1163, 508, 984, 1004, 922, 598, 284, 550, 563, 88, 307, 184, 1205, 417, 678, 261, 1007, 446, 367, 142, 479, 472, 24, 615, 892, 824, 251, 828, 308, 843, 361, 910, 1206, 772, 238, 799, 851, 437, 520, 664, 471, 863, 720, 977, 909, 1030, 634, 1067, 787, 218, 156, 299, 1095, 992, 967, 1025, 578, 1059, 158, 809, 321, 425, 100, 191, 933, 919, 683, 969, 511, 496, 889, 1096, 254, 339, 594, 875, 519, 862, 630, 374, 721, 1157, 45, 707, 42, 216, 1113, 491, 789, 499, 1147, 21, 526, 1087, 190, 951, 65, 1133, 600, 1000, 105, 90, 362, 585, 891, 713, 849, 833, 212, 372, 214, 98, 12, 593, 556, 324, 376, 734, 907, 438, 899, 691, 915, 173, 0, 220, 447, 476, 344, 247, 131, 180, 844, 249, 229, 918, 846, 790, 1053, 72, 302, 84, 155, 392, 728, 781, 886, 1042, 534, 189, 138, 290, 1038, 612, 1058, 857, 751, 748, 953, 337, 1107, 753, 976, 710, 85, 882, 847, 183, 1043, 1078, 555, 1158, 773, 34, 1207, 545, 914, 627, 564, 368, 422, 3, 488, 181, 628, 715, 836, 1002, 819, 384, 574, 461, 452, 677, 172, 334, 112, 811, 1074, 1156, 517, 655, 855, 766, 1027, 1106, 1060, 670, 935, 396, 379, 145, 353, 841, 552, 271, 912, 129, 524, 135, 1077, 413, 481, 215, 380, 1170, 542, 864, 123, 983, 30, 737, 1176, 831, 445, 95, 881, 1134, 1075, 279, 840, 1090, 767, 724, 258, 861, 278, 386, 305, 808, 1094, 926, 357, 303, 947, 404, 298, 1036, 614, 1081, 735, 931, 93, 328, 342, 382, 606, 351, 235, 419, 266, 60, 1181, 82, 346, 1130, 420, 470, 516, 894, 304, 632, 749, 825, 454, 927, 102, 587, 421, 164, 1172, 356, 5, 464, 595, 165, 1154, 616, 944, 507, 256, 109, 639, 414, 1191, 255, 729, 1080, 228, 703, 245, 816, 1211, 354, 501, 570, 1099, 663, 493, 199, 489, 130, 647, 577, 401, 883, 204, 961, 771, 1104, 530, 575, 560, 1057, 283, 625, 226, 233, 989, 884, 377, 1182, 680, 143, 559, 1153, 973, 242, 649, 502, 814, 646, 928, 1097, 51, 355, 913, 1178, 602, 295, 69, 55, 584, 97, 300, 205, 948, 867, 763, 569, 1165, 378, 433, 1210, 415, 991, 1040, 1197, 982, 943, 327, 240, 387, 1180, 56, 571, 877, 77, 770, 762, 141, 179, 37, 792, 201, 1185, 1041, 114, 835, 20, 653, 313, 580, 850, 250, 854, 426, 330, 717, 590, 473, 160, 263, 718, 759, 148, 253, 1044, 801, 277, 345, 527, 558, 389, 589, 920, 582, 1086, 981, 937, 306, 227, 140, 223, 744, 325, 986]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9065006184699382
the save name prefix for this run is:  chkpt-ID_9065006184699382_tag_Ablation-job-blacklist-loss
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 944
rank avg (pred): 0.552 +- 0.007
mrr vals (pred, true): 0.013, 0.016
batch losses (mrrl, rdl): 0.0, 0.0008254817

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1056
rank avg (pred): 0.074 +- 0.050
mrr vals (pred, true): 0.265, 0.557
batch losses (mrrl, rdl): 0.0, 3.23712e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 630
rank avg (pred): 0.476 +- 0.284
mrr vals (pred, true): 0.139, 0.040
batch losses (mrrl, rdl): 0.0, 6.1708e-06

Epoch over!
epoch time: 11.995

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 137
rank avg (pred): 0.349 +- 0.265
mrr vals (pred, true): 0.232, 0.090
batch losses (mrrl, rdl): 0.0, 4.98903e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 849
rank avg (pred): 0.377 +- 0.275
mrr vals (pred, true): 0.210, 0.029
batch losses (mrrl, rdl): 0.0, 0.0002675456

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 264
rank avg (pred): 0.041 +- 0.035
mrr vals (pred, true): 0.414, 0.562
batch losses (mrrl, rdl): 0.0, 1.5606e-06

Epoch over!
epoch time: 11.906

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 151
rank avg (pred): 0.354 +- 0.271
mrr vals (pred, true): 0.230, 0.104
batch losses (mrrl, rdl): 0.0, 8.93029e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 277
rank avg (pred): 0.031 +- 0.027
mrr vals (pred, true): 0.474, 0.554
batch losses (mrrl, rdl): 0.0, 7.998e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 374
rank avg (pred): 0.363 +- 0.266
mrr vals (pred, true): 0.205, 0.108
batch losses (mrrl, rdl): 0.0, 8.92657e-05

Epoch over!
epoch time: 11.909

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 442
rank avg (pred): 0.339 +- 0.274
mrr vals (pred, true): 0.281, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001389869

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1083
rank avg (pred): 0.390 +- 0.280
mrr vals (pred, true): 0.226, 0.074
batch losses (mrrl, rdl): 0.0, 4.87149e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 675
rank avg (pred): 0.485 +- 0.276
mrr vals (pred, true): 0.147, 0.046
batch losses (mrrl, rdl): 0.0, 9.49688e-05

Epoch over!
epoch time: 11.944

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1127
rank avg (pred): 0.369 +- 0.279
mrr vals (pred, true): 0.250, 0.050
batch losses (mrrl, rdl): 0.0, 4.05041e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 864
rank avg (pred): 0.388 +- 0.279
mrr vals (pred, true): 0.233, 0.052
batch losses (mrrl, rdl): 0.0, 9.6229e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 615
rank avg (pred): 0.446 +- 0.286
mrr vals (pred, true): 0.201, 0.044
batch losses (mrrl, rdl): 0.0, 1.2846e-05

Epoch over!
epoch time: 11.973

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 296
rank avg (pred): 0.032 +- 0.027
mrr vals (pred, true): 0.438, 0.544
batch losses (mrrl, rdl): 0.1116244048, 4.722e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 914
rank avg (pred): 0.366 +- 0.229
mrr vals (pred, true): 0.125, 0.057
batch losses (mrrl, rdl): 0.056675557, 8.73394e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1109
rank avg (pred): 0.426 +- 0.201
mrr vals (pred, true): 0.053, 0.049
batch losses (mrrl, rdl): 0.0001080752, 1.32941e-05

Epoch over!
epoch time: 12.22

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 189
rank avg (pred): 0.415 +- 0.197
mrr vals (pred, true): 0.063, 0.046
batch losses (mrrl, rdl): 0.0015929765, 3.47083e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1034
rank avg (pred): 0.405 +- 0.242
mrr vals (pred, true): 0.095, 0.056
batch losses (mrrl, rdl): 0.0204132777, 1.03345e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1171
rank avg (pred): 0.432 +- 0.183
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 9.93429e-05, 3.83104e-05

Epoch over!
epoch time: 12.088

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 712
rank avg (pred): 0.448 +- 0.163
mrr vals (pred, true): 0.041, 0.049
batch losses (mrrl, rdl): 0.0008609372, 4.03161e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 305
rank avg (pred): 0.022 +- 0.017
mrr vals (pred, true): 0.426, 0.544
batch losses (mrrl, rdl): 0.1389040202, 2.5957e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 192
rank avg (pred): 0.393 +- 0.202
mrr vals (pred, true): 0.061, 0.051
batch losses (mrrl, rdl): 0.0011941526, 2.95865e-05

Epoch over!
epoch time: 12.182

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 773
rank avg (pred): 0.415 +- 0.234
mrr vals (pred, true): 0.068, 0.055
batch losses (mrrl, rdl): 0.0032829561, 0.0002011063

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 280
rank avg (pred): 0.012 +- 0.011
mrr vals (pred, true): 0.552, 0.551
batch losses (mrrl, rdl): 2.86245e-05, 1.19428e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 474
rank avg (pred): 0.399 +- 0.194
mrr vals (pred, true): 0.053, 0.042
batch losses (mrrl, rdl): 0.0001186971, 4.52037e-05

Epoch over!
epoch time: 12.213

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1
rank avg (pred): 0.014 +- 0.013
mrr vals (pred, true): 0.550, 0.541
batch losses (mrrl, rdl): 0.0008652203, 1.23143e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 384
rank avg (pred): 0.371 +- 0.209
mrr vals (pred, true): 0.074, 0.122
batch losses (mrrl, rdl): 0.0229410361, 0.0002112674

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1206
rank avg (pred): 0.451 +- 0.150
mrr vals (pred, true): 0.036, 0.049
batch losses (mrrl, rdl): 0.0019051277, 4.51237e-05

Epoch over!
epoch time: 12.161

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 728
rank avg (pred): 0.452 +- 0.140
mrr vals (pred, true): 0.033, 0.046
batch losses (mrrl, rdl): 0.0029356042, 4.06596e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1207
rank avg (pred): 0.432 +- 0.156
mrr vals (pred, true): 0.041, 0.044
batch losses (mrrl, rdl): 0.000740473, 3.56918e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 307
rank avg (pred): 0.031 +- 0.031
mrr vals (pred, true): 0.538, 0.547
batch losses (mrrl, rdl): 0.0008771927, 5.639e-07

Epoch over!
epoch time: 12.122

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 826
rank avg (pred): 0.097 +- 0.097
mrr vals (pred, true): 0.424, 0.238
batch losses (mrrl, rdl): 0.3471736312, 0.000250791

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 339
rank avg (pred): 0.424 +- 0.249
mrr vals (pred, true): 0.059, 0.131
batch losses (mrrl, rdl): 0.0518963151, 0.0006421631

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 237
rank avg (pred): 0.372 +- 0.211
mrr vals (pred, true): 0.071, 0.042
batch losses (mrrl, rdl): 0.0042967885, 0.0001006017

Epoch over!
epoch time: 12.099

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 250
rank avg (pred): 0.034 +- 0.035
mrr vals (pred, true): 0.540, 0.566
batch losses (mrrl, rdl): 0.006742388, 1.356e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 133
rank avg (pred): 0.384 +- 0.189
mrr vals (pred, true): 0.054, 0.125
batch losses (mrrl, rdl): 0.0506575853, 0.0002112861

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1198
rank avg (pred): 0.423 +- 0.158
mrr vals (pred, true): 0.043, 0.047
batch losses (mrrl, rdl): 0.0004523252, 4.02624e-05

Epoch over!
epoch time: 12.196

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 235
rank avg (pred): 0.358 +- 0.198
mrr vals (pred, true): 0.068, 0.046
batch losses (mrrl, rdl): 0.0033932135, 0.000123437

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 844
rank avg (pred): 0.427 +- 0.238
mrr vals (pred, true): 0.064, 0.045
batch losses (mrrl, rdl): 0.0018530719, 6.59545e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 394
rank avg (pred): 0.378 +- 0.182
mrr vals (pred, true): 0.055, 0.124
batch losses (mrrl, rdl): 0.0486539267, 0.0002327981

Epoch over!
epoch time: 11.942

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 844
rank avg (pred): 0.452 +- 0.213
mrr vals (pred, true): 0.047, 0.045
batch losses (mrrl, rdl): 9.52195e-05, 4.61099e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 342
rank avg (pred): 0.390 +- 0.223
mrr vals (pred, true): 0.066, 0.143
batch losses (mrrl, rdl): 0.0593270063, 0.0003459491

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 909
rank avg (pred): 0.329 +- 0.203
mrr vals (pred, true): 0.085, 0.074
batch losses (mrrl, rdl): 0.0124811372, 8.50531e-05

Epoch over!
epoch time: 12.124

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.384 +- 0.173
mrr vals (pred, true): 0.051, 0.042

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   26 	     0 	 0.04166 	 0.01602 	 ~...
   93 	     1 	 0.10277 	 0.02127 	 m..s
   30 	     2 	 0.05004 	 0.02690 	 ~...
   54 	     3 	 0.05530 	 0.03457 	 ~...
   21 	     4 	 0.04012 	 0.03710 	 ~...
    8 	     5 	 0.03663 	 0.03791 	 ~...
   34 	     6 	 0.05061 	 0.03793 	 ~...
   18 	     7 	 0.03920 	 0.03798 	 ~...
    6 	     8 	 0.03624 	 0.03917 	 ~...
   36 	     9 	 0.05079 	 0.04183 	 ~...
   10 	    10 	 0.03668 	 0.04187 	 ~...
   12 	    11 	 0.03690 	 0.04224 	 ~...
   50 	    12 	 0.05498 	 0.04228 	 ~...
   16 	    13 	 0.03722 	 0.04248 	 ~...
   19 	    14 	 0.03950 	 0.04250 	 ~...
   17 	    15 	 0.03830 	 0.04279 	 ~...
   57 	    16 	 0.05677 	 0.04283 	 ~...
    3 	    17 	 0.03604 	 0.04302 	 ~...
   76 	    18 	 0.07063 	 0.04308 	 ~...
   39 	    19 	 0.05093 	 0.04349 	 ~...
   61 	    20 	 0.05730 	 0.04440 	 ~...
   27 	    21 	 0.04217 	 0.04459 	 ~...
   53 	    22 	 0.05527 	 0.04461 	 ~...
    0 	    23 	 0.03590 	 0.04470 	 ~...
    4 	    24 	 0.03616 	 0.04513 	 ~...
   62 	    25 	 0.05816 	 0.04542 	 ~...
   22 	    26 	 0.04140 	 0.04547 	 ~...
   68 	    27 	 0.05940 	 0.04610 	 ~...
   60 	    28 	 0.05729 	 0.04613 	 ~...
   11 	    29 	 0.03674 	 0.04625 	 ~...
   50 	    30 	 0.05498 	 0.04633 	 ~...
   72 	    31 	 0.06049 	 0.04637 	 ~...
   23 	    32 	 0.04146 	 0.04656 	 ~...
   32 	    33 	 0.05030 	 0.04659 	 ~...
   15 	    34 	 0.03716 	 0.04665 	 ~...
   65 	    35 	 0.05834 	 0.04704 	 ~...
   83 	    36 	 0.07568 	 0.04730 	 ~...
   75 	    37 	 0.06360 	 0.04736 	 ~...
   87 	    38 	 0.08266 	 0.04738 	 m..s
    2 	    39 	 0.03600 	 0.04741 	 ~...
   63 	    40 	 0.05823 	 0.04750 	 ~...
   29 	    41 	 0.04987 	 0.04827 	 ~...
   24 	    42 	 0.04147 	 0.04828 	 ~...
    9 	    43 	 0.03668 	 0.04844 	 ~...
   67 	    44 	 0.05908 	 0.04851 	 ~...
   13 	    45 	 0.03699 	 0.04922 	 ~...
   14 	    46 	 0.03710 	 0.04932 	 ~...
   33 	    47 	 0.05047 	 0.04951 	 ~...
   25 	    48 	 0.04149 	 0.04966 	 ~...
   71 	    49 	 0.06048 	 0.04986 	 ~...
   46 	    50 	 0.05421 	 0.04994 	 ~...
   52 	    51 	 0.05518 	 0.05009 	 ~...
   28 	    52 	 0.04221 	 0.05049 	 ~...
   55 	    53 	 0.05604 	 0.05059 	 ~...
    7 	    54 	 0.03651 	 0.05069 	 ~...
    0 	    55 	 0.03590 	 0.05073 	 ~...
   20 	    56 	 0.03952 	 0.05099 	 ~...
   77 	    57 	 0.07207 	 0.05143 	 ~...
   70 	    58 	 0.06007 	 0.05268 	 ~...
   44 	    59 	 0.05168 	 0.05331 	 ~...
   38 	    60 	 0.05093 	 0.05352 	 ~...
   47 	    61 	 0.05439 	 0.05412 	 ~...
    5 	    62 	 0.03618 	 0.05446 	 ~...
   85 	    63 	 0.07936 	 0.05540 	 ~...
   49 	    64 	 0.05496 	 0.05591 	 ~...
   89 	    65 	 0.09055 	 0.05632 	 m..s
   86 	    66 	 0.08071 	 0.06365 	 ~...
   88 	    67 	 0.08791 	 0.06948 	 ~...
   91 	    68 	 0.09151 	 0.07225 	 ~...
   36 	    69 	 0.05079 	 0.07471 	 ~...
   90 	    70 	 0.09110 	 0.08295 	 ~...
   58 	    71 	 0.05697 	 0.09158 	 m..s
   45 	    72 	 0.05373 	 0.09437 	 m..s
   48 	    73 	 0.05495 	 0.09517 	 m..s
   66 	    74 	 0.05849 	 0.09532 	 m..s
   42 	    75 	 0.05164 	 0.09772 	 m..s
   92 	    76 	 0.10014 	 0.09801 	 ~...
   39 	    77 	 0.05093 	 0.10464 	 m..s
   64 	    78 	 0.05829 	 0.10626 	 m..s
   73 	    79 	 0.06141 	 0.10861 	 m..s
   80 	    80 	 0.07321 	 0.11343 	 m..s
   30 	    81 	 0.05004 	 0.11551 	 m..s
   84 	    82 	 0.07738 	 0.11564 	 m..s
   74 	    83 	 0.06178 	 0.12158 	 m..s
   41 	    84 	 0.05146 	 0.12795 	 m..s
   43 	    85 	 0.05164 	 0.12896 	 m..s
   79 	    86 	 0.07299 	 0.13801 	 m..s
   35 	    87 	 0.05078 	 0.14090 	 m..s
   59 	    88 	 0.05700 	 0.14543 	 m..s
   78 	    89 	 0.07265 	 0.14609 	 m..s
   56 	    90 	 0.05655 	 0.15380 	 m..s
   69 	    91 	 0.05949 	 0.15690 	 m..s
   98 	    92 	 0.44812 	 0.16959 	 MISS
   94 	    93 	 0.10888 	 0.20213 	 m..s
   95 	    94 	 0.23778 	 0.21489 	 ~...
   81 	    95 	 0.07391 	 0.22480 	 MISS
   82 	    96 	 0.07512 	 0.24196 	 MISS
  109 	    97 	 0.52964 	 0.37516 	 MISS
  102 	    98 	 0.51790 	 0.41169 	 MISS
  116 	    99 	 0.53813 	 0.50782 	 m..s
  117 	   100 	 0.53828 	 0.51127 	 ~...
  110 	   101 	 0.53398 	 0.51128 	 ~...
  107 	   102 	 0.52600 	 0.51394 	 ~...
  114 	   103 	 0.53740 	 0.51710 	 ~...
  120 	   104 	 0.54406 	 0.52673 	 ~...
  112 	   105 	 0.53413 	 0.52699 	 ~...
  104 	   106 	 0.51933 	 0.53145 	 ~...
  106 	   107 	 0.52262 	 0.53601 	 ~...
  108 	   108 	 0.52845 	 0.53657 	 ~...
  101 	   109 	 0.51198 	 0.53971 	 ~...
  119 	   110 	 0.54146 	 0.54080 	 ~...
  111 	   111 	 0.53408 	 0.54154 	 ~...
  115 	   112 	 0.53790 	 0.54536 	 ~...
  118 	   113 	 0.53987 	 0.54696 	 ~...
   96 	   114 	 0.26460 	 0.54935 	 MISS
  113 	   115 	 0.53649 	 0.55007 	 ~...
   97 	   116 	 0.36796 	 0.55105 	 MISS
  103 	   117 	 0.51848 	 0.55203 	 m..s
  100 	   118 	 0.48025 	 0.55746 	 m..s
   99 	   119 	 0.47327 	 0.56065 	 m..s
  105 	   120 	 0.52197 	 0.56332 	 m..s
==========================================
r_mrr = 0.9527112245559692
r2_mrr = 0.9025241732597351
spearmanr_mrr@5 = 0.9641218781471252
spearmanr_mrr@10 = 0.9765772223472595
spearmanr_mrr@50 = 0.9948244094848633
spearmanr_mrr@100 = 0.9921723008155823
spearmanr_mrr@All = 0.9924461841583252
==========================================
test time: 0.403
Done Testing dataset UMLS
total time taken: 187.9000585079193
training time taken: 181.53747844696045
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9527)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9025)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9641)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9766)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9948)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9922)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9924)}}, 'test_loss': {'DistMult': {'UMLS': 4.073581964634286}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'loss'}

===================================================================
-------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-neg_samp
-------------------------------------------------------------------
===================================================================
Running on a grid of size 1
Using random seed: 5651539382329707
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [135, 1122, 1123, 336, 751, 733, 1099, 1015, 780, 15, 862, 909, 652, 296, 533, 396, 910, 86, 235, 1167, 831, 418, 468, 995, 547, 606, 69, 259, 35, 822, 1109, 654, 55, 101, 471, 271, 40, 596, 644, 840, 496, 9, 450, 1064, 994, 999, 949, 142, 324, 1039, 545, 122, 922, 874, 292, 210, 12, 317, 150, 81, 1210, 1211, 19, 456, 578, 811, 158, 1208, 532, 669, 841, 955, 729, 309, 356, 284, 881, 1028, 118, 600, 62, 427, 80, 60, 1044, 1108, 1200, 358, 1106, 382, 113, 244, 180, 375, 707, 104, 894, 583, 480, 693, 1130, 633, 1153, 944, 63, 683, 576, 624, 1095, 254, 160, 232, 936, 992, 925, 1023, 134, 307, 1119, 53, 98]
valid_ids (0): []
train_ids (1094): [732, 1129, 31, 898, 569, 1073, 331, 1075, 723, 68, 318, 447, 574, 7, 902, 1185, 764, 1021, 812, 207, 247, 77, 1030, 166, 655, 917, 686, 384, 139, 828, 455, 887, 861, 623, 304, 884, 1067, 1076, 835, 608, 719, 478, 827, 1152, 437, 1184, 137, 131, 406, 993, 90, 1175, 517, 904, 1118, 119, 85, 908, 1048, 1027, 1040, 738, 224, 800, 842, 241, 627, 511, 220, 954, 818, 38, 152, 1083, 873, 613, 523, 105, 335, 103, 757, 73, 796, 258, 1071, 813, 629, 591, 411, 233, 133, 273, 609, 168, 938, 385, 963, 626, 50, 327, 605, 374, 226, 1178, 529, 1002, 1029, 399, 33, 88, 978, 240, 1057, 865, 801, 515, 27, 1156, 986, 173, 449, 28, 880, 866, 1045, 689, 293, 93, 1018, 1173, 572, 1006, 863, 299, 489, 253, 1059, 465, 269, 748, 984, 741, 282, 48, 663, 414, 1159, 1125, 959, 246, 1144, 303, 473, 342, 37, 877, 516, 1035, 320, 1084, 567, 408, 407, 653, 1098, 99, 313, 188, 724, 937, 194, 16, 830, 262, 1046, 580, 337, 46, 321, 581, 488, 777, 616, 183, 193, 466, 551, 1212, 363, 648, 1198, 590, 250, 83, 675, 270, 537, 784, 893, 255, 369, 96, 394, 678, 283, 54, 1205, 1213, 798, 1072, 1056, 349, 839, 546, 965, 381, 360, 820, 1069, 117, 853, 837, 493, 467, 251, 620, 1082, 597, 25, 225, 1014, 632, 556, 367, 111, 575, 799, 1036, 343, 522, 485, 915, 430, 772, 577, 951, 859, 833, 649, 805, 17, 285, 639, 91, 461, 615, 933, 964, 768, 1116, 976, 1020, 962, 900, 746, 940, 990, 205, 1145, 229, 682, 409, 735, 291, 147, 997, 975, 75, 191, 804, 202, 870, 395, 787, 988, 792, 921, 212, 524, 1017, 743, 110, 501, 416, 958, 1147, 51, 67, 1160, 1024, 445, 508, 1157, 1139, 770, 345, 797, 680, 1050, 479, 647, 1037, 236, 1186, 643, 895, 330, 704, 595, 339, 1062, 4, 305, 346, 1007, 939, 361, 961, 112, 916, 94, 433, 593, 377, 472, 390, 1187, 29, 278, 889, 788, 114, 239, 1126, 1060, 1140, 1022, 1052, 1194, 991, 286, 222, 340, 720, 891, 469, 503, 539, 5, 3, 507, 477, 231, 1100, 121, 243, 209, 622, 351, 177, 747, 989, 957, 1000, 1199, 412, 857, 20, 1032, 423, 475, 1077, 594, 298, 116, 178, 540, 514, 758, 357, 1170, 228, 1009, 165, 359, 404, 932, 58, 876, 370, 727, 948, 341, 1074, 696, 132, 711, 66, 985, 364, 76, 1120, 846, 635, 1128, 1121, 1103, 734, 1143, 699, 599, 1171, 474, 332, 167, 334, 392, 753, 1, 120, 371, 448, 268, 181, 982, 1102, 1068, 301, 308, 72, 338, 603, 124, 943, 400, 878, 927, 500, 476, 1192, 26, 934, 1104, 294, 829, 314, 659, 497, 520, 728, 176, 1065, 171, 352, 1162, 143, 1207, 457, 190, 843, 1070, 435, 692, 519, 261, 879, 1081, 218, 1079, 710, 601, 149, 509, 434, 771, 151, 697, 996, 95, 534, 1190, 602, 754, 157, 1058, 163, 319, 774, 598, 950, 263, 806, 536, 24, 582, 946, 192, 326, 276, 204, 1114, 875, 868, 836, 6, 482, 755, 1203, 641, 779, 1165, 953, 1150, 36, 242, 460, 211, 184, 107, 1078, 92, 494, 323, 290, 1195, 353, 730, 1010, 667, 745, 1133, 295, 535, 1117, 1137, 199, 267, 589, 281, 570, 664, 454, 544, 931, 1209, 762, 79, 826, 808, 1026, 911, 981, 824, 956, 718, 1005, 1097, 417, 215, 297, 1166, 513, 817, 726, 967, 791, 694, 1158, 59, 484, 175, 402, 850, 636, 431, 1138, 775, 531, 558, 527, 1016, 39, 23, 89, 776, 851, 347, 860, 154, 49, 855, 1063, 803, 854, 737, 362, 703, 621, 453, 1182, 918, 715, 1176, 781, 760, 712, 156, 554, 844, 376, 1183, 773, 230, 125, 617, 492, 277, 1164, 530, 216, 538, 1149, 186, 750, 144, 568, 138, 611, 56, 1174, 969, 832, 896, 966, 420, 782, 637, 410, 196, 1001, 1169, 383, 483, 279, 213, 930, 252, 128, 1088, 783, 306, 161, 312, 84, 897, 1188, 552, 288, 713, 127, 555, 785, 354, 821, 487, 610, 1142, 333, 172, 634, 890, 740, 847, 1093, 426, 238, 973, 22, 765, 725, 1094, 260, 1003, 106, 1080, 695, 189, 825, 200, 1124, 1196, 565, 1177, 906, 656, 491, 650, 61, 266, 1148, 795, 329, 109, 245, 607, 506, 257, 1180, 662, 1089, 325, 563, 668, 618, 419, 155, 322, 365, 289, 71, 924, 838, 810, 794, 115, 405, 1112, 566, 219, 858, 1206, 1042, 767, 604, 510, 1115, 1155, 642, 78, 871, 214, 300, 0, 1091, 1054, 174, 438, 651, 1197, 1189, 793, 739, 612, 386, 550, 987, 1204, 677, 690, 901, 665, 972, 100, 1090, 945, 1151, 681, 145, 1013, 920, 548, 864, 311, 439, 398, 368, 1038, 481, 1055, 979, 108, 170, 1132, 350, 778, 882, 197, 1136, 1172, 907, 403, 32, 328, 815, 684, 21, 462, 18, 913, 802, 935, 1011, 1111, 470, 549, 823, 672, 561, 1201, 814, 912, 687, 153, 661, 1101, 717, 164, 942, 502, 899, 749, 499, 716, 44, 415, 658, 223, 756, 892, 670, 87, 185, 721, 968, 74, 960, 237, 441, 393, 130, 1066, 701, 452, 631, 187, 425, 42, 287, 126, 941, 1134, 1214, 769, 809, 742, 1043, 1191, 645, 424, 458, 97, 47, 444, 521, 8, 169, 34, 348, 10, 586, 217, 974, 736, 464, 705, 630, 1161, 43, 206, 234, 182, 459, 265, 807, 440, 1146, 355, 1163, 201, 162, 1047, 1096, 486, 790, 208, 413, 869, 141, 274, 1051, 302, 428, 14, 674, 422, 1110, 221, 679, 1033, 379, 1061, 366, 671, 971, 542, 528, 638, 573, 316, 280, 998, 387, 512, 1127, 136, 1019, 559, 272, 436, 660, 518, 429, 848, 947, 614, 1193, 905, 421, 700, 977, 983, 849, 1008, 446, 819, 1053, 625, 903, 786, 140, 845, 709, 706, 553, 65, 691, 564, 926, 432, 744, 179, 525, 557, 543, 1049, 761, 41, 45, 560, 463, 195, 1092, 1087, 685, 585, 579, 443, 588, 198, 584, 310, 722, 852, 13, 264, 1034, 248, 752, 708, 70, 391, 397, 1179, 203, 82, 698, 587, 541, 1141, 102, 619, 1113, 57, 759, 1168, 952, 628, 1012, 495, 249, 592, 872, 834, 1154, 646, 789, 378, 1086, 919, 816, 702, 315, 498, 1085, 30, 571, 1025, 344, 442, 1031, 867, 766, 372, 389, 123, 451, 562, 970, 64, 676, 888, 666, 52, 856, 980, 2, 688, 886, 256, 714, 148, 159, 380, 129, 401, 11, 640, 1131, 1105, 1135, 227, 388, 1181, 914, 929, 505, 885, 504, 1202, 146, 883, 731, 1107, 490, 373, 923, 673, 1004, 526, 1041, 928, 275, 657, 763]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2586741028841227
the save name prefix for this run is:  chkpt-ID_2586741028841227_tag_Ablation-job-blacklist-neg_samp
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=7, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 816
rank avg (pred): 0.454 +- 0.003
mrr vals (pred, true): 0.016, 0.037
batch losses (mrrl, rdl): 0.0, 4.2409e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1030
rank avg (pred): 0.440 +- 0.015
mrr vals (pred, true): 0.017, 0.045
batch losses (mrrl, rdl): 0.0, 7.93527e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 415
rank avg (pred): 0.393 +- 0.150
mrr vals (pred, true): 0.071, 0.052
batch losses (mrrl, rdl): 0.0, 7.57903e-05

Epoch over!
epoch time: 12.028

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 448
rank avg (pred): 0.342 +- 0.191
mrr vals (pred, true): 0.175, 0.056
batch losses (mrrl, rdl): 0.0, 0.0001550337

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1103
rank avg (pred): 0.389 +- 0.245
mrr vals (pred, true): 0.235, 0.088
batch losses (mrrl, rdl): 0.0, 0.0001189681

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 531
rank avg (pred): 0.193 +- 0.130
mrr vals (pred, true): 0.290, 0.052
batch losses (mrrl, rdl): 0.0, 0.0012620571

Epoch over!
epoch time: 11.997

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 746
rank avg (pred): 0.276 +- 0.191
mrr vals (pred, true): 0.290, 0.170
batch losses (mrrl, rdl): 0.0, 4.29248e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 935
rank avg (pred): 0.455 +- 0.316
mrr vals (pred, true): 0.287, 0.015
batch losses (mrrl, rdl): 0.0, 0.0014507405

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 199
rank avg (pred): 0.378 +- 0.263
mrr vals (pred, true): 0.293, 0.052
batch losses (mrrl, rdl): 0.0, 2.06592e-05

Epoch over!
epoch time: 11.736

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 253
rank avg (pred): 0.094 +- 0.066
mrr vals (pred, true): 0.337, 0.561
batch losses (mrrl, rdl): 0.0, 8.44068e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 318
rank avg (pred): 0.185 +- 0.131
mrr vals (pred, true): 0.315, 0.537
batch losses (mrrl, rdl): 0.0, 0.0004882147

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1082
rank avg (pred): 0.387 +- 0.270
mrr vals (pred, true): 0.295, 0.138
batch losses (mrrl, rdl): 0.0, 0.0004469134

Epoch over!
epoch time: 11.85

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 828
rank avg (pred): 0.197 +- 0.140
mrr vals (pred, true): 0.320, 0.520
batch losses (mrrl, rdl): 0.0, 0.0006042055

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 684
rank avg (pred): 0.365 +- 0.260
mrr vals (pred, true): 0.310, 0.046
batch losses (mrrl, rdl): 0.0, 2.10164e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 11
rank avg (pred): 0.102 +- 0.073
mrr vals (pred, true): 0.353, 0.547
batch losses (mrrl, rdl): 0.0, 9.47823e-05

Epoch over!
epoch time: 11.872

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 537
rank avg (pred): 0.149 +- 0.108
mrr vals (pred, true): 0.342, 0.049
batch losses (mrrl, rdl): 0.8526898623, 0.001868478

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1136
rank avg (pred): 0.084 +- 0.055
mrr vals (pred, true): 0.310, 0.227
batch losses (mrrl, rdl): 0.0699092597, 0.0002724851

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1141
rank avg (pred): 0.057 +- 0.043
mrr vals (pred, true): 0.377, 0.225
batch losses (mrrl, rdl): 0.23099944, 0.0006194108

Epoch over!
epoch time: 12.145

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 627
rank avg (pred): 0.880 +- 0.267
mrr vals (pred, true): 0.068, 0.042
batch losses (mrrl, rdl): 0.0033000985, 0.0029566437

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 117
rank avg (pred): 0.866 +- 0.258
mrr vals (pred, true): 0.064, 0.112
batch losses (mrrl, rdl): 0.0232213363, 0.0058210795

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 30
rank avg (pred): 0.103 +- 0.081
mrr vals (pred, true): 0.374, 0.535
batch losses (mrrl, rdl): 0.2580986321, 8.67718e-05

Epoch over!
epoch time: 12.071

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 588
rank avg (pred): 0.906 +- 0.254
mrr vals (pred, true): 0.061, 0.049
batch losses (mrrl, rdl): 0.0011804092, 0.003319147

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 990
rank avg (pred): 0.066 +- 0.053
mrr vals (pred, true): 0.409, 0.539
batch losses (mrrl, rdl): 0.1706466079, 2.34737e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 803
rank avg (pred): 0.944 +- 0.199
mrr vals (pred, true): 0.033, 0.055
batch losses (mrrl, rdl): 0.002736164, 0.0046578897

Epoch over!
epoch time: 11.979

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 267
rank avg (pred): 0.166 +- 0.127
mrr vals (pred, true): 0.345, 0.549
batch losses (mrrl, rdl): 0.4167512357, 0.0004276378

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 623
rank avg (pred): 0.760 +- 0.194
mrr vals (pred, true): 0.041, 0.042
batch losses (mrrl, rdl): 0.0008087083, 0.0015527903

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 488
rank avg (pred): 0.042 +- 0.037
mrr vals (pred, true): 0.464, 0.241
batch losses (mrrl, rdl): 0.4995406866, 0.0005635389

Epoch over!
epoch time: 12.14

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 957
rank avg (pred): 0.906 +- 0.227
mrr vals (pred, true): 0.053, 0.044
batch losses (mrrl, rdl): 6.36828e-05, 0.0039429972

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1056
rank avg (pred): 0.133 +- 0.101
mrr vals (pred, true): 0.342, 0.557
batch losses (mrrl, rdl): 0.4616520107, 0.0002260635

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1049
rank avg (pred): 0.477 +- 0.184
mrr vals (pred, true): 0.068, 0.059
batch losses (mrrl, rdl): 0.0031601749, 9.42546e-05

Epoch over!
epoch time: 12.03

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 294
rank avg (pred): 0.180 +- 0.148
mrr vals (pred, true): 0.380, 0.540
batch losses (mrrl, rdl): 0.2566630244, 0.0004711145

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 250
rank avg (pred): 0.135 +- 0.108
mrr vals (pred, true): 0.386, 0.566
batch losses (mrrl, rdl): 0.3246056437, 0.0002495813

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1144
rank avg (pred): 0.082 +- 0.069
mrr vals (pred, true): 0.416, 0.140
batch losses (mrrl, rdl): 0.7647656798, 0.0008124143

Epoch over!
epoch time: 12.302

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 49
rank avg (pred): 0.182 +- 0.147
mrr vals (pred, true): 0.365, 0.532
batch losses (mrrl, rdl): 0.2779961228, 0.0004901343

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 886
rank avg (pred): 0.900 +- 0.184
mrr vals (pred, true): 0.032, 0.054
batch losses (mrrl, rdl): 0.003217292, 0.0042717247

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 819
rank avg (pred): 0.243 +- 0.200
mrr vals (pred, true): 0.384, 0.533
batch losses (mrrl, rdl): 0.2213551104, 0.0010559862

Epoch over!
epoch time: 12.283

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 25
rank avg (pred): 0.135 +- 0.111
mrr vals (pred, true): 0.397, 0.531
batch losses (mrrl, rdl): 0.1810681671, 0.000227105

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1189
rank avg (pred): 0.392 +- 0.107
mrr vals (pred, true): 0.072, 0.038
batch losses (mrrl, rdl): 0.0047029289, 0.0001324117

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 481
rank avg (pred): 0.374 +- 0.115
mrr vals (pred, true): 0.080, 0.051
batch losses (mrrl, rdl): 0.0090385834, 0.0001885198

Epoch over!
epoch time: 12.074

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1204
rank avg (pred): 0.393 +- 0.099
mrr vals (pred, true): 0.070, 0.050
batch losses (mrrl, rdl): 0.0038420267, 0.000101013

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 331
rank avg (pred): 0.393 +- 0.104
mrr vals (pred, true): 0.073, 0.129
batch losses (mrrl, rdl): 0.031206198, 0.0004139307

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 239
rank avg (pred): 0.381 +- 0.104
mrr vals (pred, true): 0.075, 0.046
batch losses (mrrl, rdl): 0.0061193709, 0.0001901517

Epoch over!
epoch time: 12.084

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 618
rank avg (pred): 0.373 +- 0.108
mrr vals (pred, true): 0.078, 0.048
batch losses (mrrl, rdl): 0.0076185097, 0.0002501587

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 217
rank avg (pred): 0.406 +- 0.094
mrr vals (pred, true): 0.060, 0.048
batch losses (mrrl, rdl): 0.0009816901, 0.0001027832

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 108
rank avg (pred): 0.402 +- 0.098
mrr vals (pred, true): 0.069, 0.132
batch losses (mrrl, rdl): 0.03895244, 0.0003757119

Epoch over!
epoch time: 12.322

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.397 +- 0.097
mrr vals (pred, true): 0.070, 0.087

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    3 	     0 	 0.05241 	 0.01536 	 m..s
    0 	     1 	 0.04427 	 0.01569 	 ~...
    8 	     2 	 0.05770 	 0.01638 	 m..s
   24 	     3 	 0.06816 	 0.01822 	 m..s
   80 	     4 	 0.23072 	 0.02063 	 MISS
    2 	     5 	 0.05235 	 0.03469 	 ~...
   66 	     6 	 0.07405 	 0.03521 	 m..s
   41 	     7 	 0.07091 	 0.03570 	 m..s
    3 	     8 	 0.05241 	 0.03702 	 ~...
   47 	     9 	 0.07107 	 0.03710 	 m..s
   14 	    10 	 0.06608 	 0.03971 	 ~...
   11 	    11 	 0.06425 	 0.04074 	 ~...
   22 	    12 	 0.06808 	 0.04132 	 ~...
   61 	    13 	 0.07364 	 0.04191 	 m..s
   59 	    14 	 0.07338 	 0.04228 	 m..s
   33 	    15 	 0.06986 	 0.04271 	 ~...
    5 	    16 	 0.05652 	 0.04274 	 ~...
   18 	    17 	 0.06727 	 0.04316 	 ~...
   42 	    18 	 0.07095 	 0.04324 	 ~...
   38 	    19 	 0.07047 	 0.04345 	 ~...
   25 	    20 	 0.06851 	 0.04402 	 ~...
   23 	    21 	 0.06812 	 0.04447 	 ~...
    1 	    22 	 0.05122 	 0.04453 	 ~...
   20 	    23 	 0.06797 	 0.04460 	 ~...
   16 	    24 	 0.06680 	 0.04469 	 ~...
   45 	    25 	 0.07106 	 0.04480 	 ~...
   54 	    26 	 0.07181 	 0.04508 	 ~...
   56 	    27 	 0.07190 	 0.04508 	 ~...
   39 	    28 	 0.07064 	 0.04527 	 ~...
   34 	    29 	 0.07007 	 0.04533 	 ~...
   77 	    30 	 0.07567 	 0.04557 	 m..s
   37 	    31 	 0.07040 	 0.04581 	 ~...
   63 	    32 	 0.07372 	 0.04582 	 ~...
   58 	    33 	 0.07258 	 0.04598 	 ~...
   64 	    34 	 0.07377 	 0.04613 	 ~...
   27 	    35 	 0.06864 	 0.04631 	 ~...
   40 	    36 	 0.07069 	 0.04637 	 ~...
   18 	    37 	 0.06727 	 0.04741 	 ~...
   60 	    38 	 0.07346 	 0.04745 	 ~...
   10 	    39 	 0.06409 	 0.04750 	 ~...
   55 	    40 	 0.07183 	 0.04787 	 ~...
    6 	    41 	 0.05747 	 0.04801 	 ~...
   26 	    42 	 0.06859 	 0.04845 	 ~...
   43 	    43 	 0.07096 	 0.04856 	 ~...
   53 	    44 	 0.07151 	 0.05001 	 ~...
   20 	    45 	 0.06797 	 0.05161 	 ~...
   49 	    46 	 0.07120 	 0.05249 	 ~...
   43 	    47 	 0.07096 	 0.05268 	 ~...
   61 	    48 	 0.07364 	 0.05284 	 ~...
   65 	    49 	 0.07398 	 0.05366 	 ~...
   94 	    50 	 0.38719 	 0.05632 	 MISS
    9 	    51 	 0.06336 	 0.05645 	 ~...
   27 	    52 	 0.06864 	 0.05890 	 ~...
   91 	    53 	 0.38605 	 0.07057 	 MISS
   87 	    54 	 0.37224 	 0.07363 	 MISS
   86 	    55 	 0.37110 	 0.07836 	 MISS
   74 	    56 	 0.07464 	 0.07881 	 ~...
   78 	    57 	 0.07793 	 0.08228 	 ~...
   30 	    58 	 0.06924 	 0.08345 	 ~...
   32 	    59 	 0.06951 	 0.08456 	 ~...
   36 	    60 	 0.07032 	 0.08659 	 ~...
   68 	    61 	 0.07421 	 0.09012 	 ~...
   31 	    62 	 0.06928 	 0.09187 	 ~...
   51 	    63 	 0.07137 	 0.09788 	 ~...
   12 	    64 	 0.06443 	 0.10125 	 m..s
  110 	    65 	 0.39419 	 0.10262 	 MISS
   17 	    66 	 0.06714 	 0.10333 	 m..s
    7 	    67 	 0.05753 	 0.10574 	 m..s
   69 	    68 	 0.07427 	 0.10771 	 m..s
   15 	    69 	 0.06648 	 0.11350 	 m..s
   71 	    70 	 0.07460 	 0.11494 	 m..s
   66 	    71 	 0.07405 	 0.11517 	 m..s
   73 	    72 	 0.07461 	 0.11783 	 m..s
   57 	    73 	 0.07220 	 0.11794 	 m..s
   51 	    74 	 0.07137 	 0.12025 	 m..s
  108 	    75 	 0.39359 	 0.12299 	 MISS
   29 	    76 	 0.06872 	 0.12327 	 m..s
   76 	    77 	 0.07497 	 0.12428 	 m..s
   12 	    78 	 0.06443 	 0.12660 	 m..s
   75 	    79 	 0.07469 	 0.13098 	 m..s
   70 	    80 	 0.07431 	 0.13102 	 m..s
   50 	    81 	 0.07128 	 0.13160 	 m..s
   48 	    82 	 0.07115 	 0.13168 	 m..s
  113 	    83 	 0.39497 	 0.14525 	 MISS
   35 	    84 	 0.07018 	 0.14543 	 m..s
   72 	    85 	 0.07461 	 0.14714 	 m..s
   46 	    86 	 0.07107 	 0.14995 	 m..s
  118 	    87 	 0.40637 	 0.19363 	 MISS
   81 	    88 	 0.25539 	 0.26525 	 ~...
   79 	    89 	 0.22908 	 0.27181 	 m..s
   82 	    90 	 0.25679 	 0.36843 	 MISS
   84 	    91 	 0.35641 	 0.39802 	 m..s
   83 	    92 	 0.35618 	 0.46155 	 MISS
   85 	    93 	 0.35709 	 0.46774 	 MISS
   88 	    94 	 0.38374 	 0.51017 	 MISS
   89 	    95 	 0.38416 	 0.51097 	 MISS
   93 	    96 	 0.38671 	 0.51127 	 MISS
  100 	    97 	 0.38925 	 0.51740 	 MISS
   97 	    98 	 0.38813 	 0.53020 	 MISS
  102 	    99 	 0.39063 	 0.53090 	 MISS
   95 	   100 	 0.38799 	 0.53224 	 MISS
  105 	   101 	 0.39176 	 0.53266 	 MISS
  115 	   102 	 0.39919 	 0.53966 	 MISS
  103 	   103 	 0.39075 	 0.54120 	 MISS
   90 	   104 	 0.38596 	 0.54255 	 MISS
   97 	   105 	 0.38813 	 0.54412 	 MISS
   92 	   106 	 0.38609 	 0.54428 	 MISS
   99 	   107 	 0.38871 	 0.54725 	 MISS
  111 	   108 	 0.39444 	 0.54835 	 MISS
   96 	   109 	 0.38800 	 0.54875 	 MISS
  109 	   110 	 0.39400 	 0.54879 	 MISS
  116 	   111 	 0.40115 	 0.54894 	 MISS
  119 	   112 	 0.40648 	 0.54914 	 MISS
  101 	   113 	 0.38936 	 0.54951 	 MISS
  120 	   114 	 0.41938 	 0.54995 	 MISS
  114 	   115 	 0.39810 	 0.55097 	 MISS
  104 	   116 	 0.39117 	 0.55203 	 MISS
  106 	   117 	 0.39188 	 0.55245 	 MISS
  117 	   118 	 0.40628 	 0.55256 	 MISS
  112 	   119 	 0.39466 	 0.55497 	 MISS
  107 	   120 	 0.39308 	 0.55778 	 MISS
==========================================
r_mrr = 0.8566666841506958
r2_mrr = 0.7128897905349731
spearmanr_mrr@5 = 0.9111660718917847
spearmanr_mrr@10 = 0.9680973291397095
spearmanr_mrr@50 = 0.7768722772598267
spearmanr_mrr@100 = 0.9122620820999146
spearmanr_mrr@All = 0.9232257604598999
==========================================
test time: 0.418
Done Testing dataset UMLS
total time taken: 187.0611171722412
training time taken: 181.39692568778992
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.8567)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.7129)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9112)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9681)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.7769)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9123)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9232)}}, 'test_loss': {'DistMult': {'UMLS': 14.514535256414092}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'neg_samp'}

=============================================================
-------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-lr
-------------------------------------------------------------
=============================================================
Running on a grid of size 1
Using random seed: 4164796326378518
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [560, 1031, 995, 253, 759, 167, 782, 810, 1053, 331, 419, 1145, 887, 465, 20, 1062, 515, 1109, 406, 870, 394, 433, 837, 163, 755, 1127, 747, 613, 868, 961, 719, 423, 1138, 1097, 429, 495, 1155, 100, 997, 1195, 344, 835, 464, 590, 342, 641, 78, 674, 350, 592, 784, 930, 1077, 974, 373, 752, 449, 1197, 411, 488, 150, 191, 443, 1039, 343, 296, 1014, 379, 775, 620, 552, 608, 1105, 1079, 70, 991, 366, 935, 452, 2, 10, 307, 538, 249, 1169, 534, 603, 81, 1040, 990, 516, 300, 1111, 655, 566, 883, 110, 1206, 918, 606, 1007, 934, 518, 268, 1135, 374, 178, 37, 664, 677, 562, 754, 98, 1005, 557, 867, 710, 509, 13, 1084, 506]
valid_ids (0): []
train_ids (1094): [22, 60, 229, 232, 849, 134, 352, 902, 258, 1018, 476, 499, 335, 1027, 236, 625, 574, 186, 976, 55, 200, 1148, 1203, 800, 1081, 539, 1093, 401, 803, 547, 987, 309, 933, 1199, 1037, 391, 585, 50, 851, 571, 283, 950, 497, 123, 393, 458, 916, 369, 1041, 211, 758, 648, 251, 891, 1198, 430, 231, 273, 266, 83, 153, 1193, 226, 705, 1209, 267, 928, 1150, 372, 390, 767, 512, 896, 580, 706, 422, 1189, 1194, 593, 578, 334, 400, 543, 56, 1087, 901, 884, 546, 44, 118, 508, 228, 256, 227, 834, 619, 280, 937, 84, 607, 278, 3, 505, 1136, 47, 721, 527, 777, 424, 771, 487, 993, 722, 324, 358, 862, 744, 371, 193, 1012, 806, 561, 72, 627, 567, 327, 616, 704, 612, 760, 126, 564, 426, 1151, 772, 982, 1142, 120, 1057, 795, 913, 1046, 829, 878, 763, 609, 355, 716, 39, 953, 656, 1200, 329, 1187, 214, 520, 676, 692, 428, 846, 111, 182, 213, 1114, 1033, 942, 1035, 733, 869, 1143, 1066, 271, 463, 962, 146, 439, 412, 977, 454, 569, 336, 48, 1118, 776, 940, 1202, 90, 669, 302, 748, 573, 712, 955, 462, 645, 1179, 340, 646, 1165, 315, 801, 483, 220, 575, 364, 166, 368, 230, 49, 172, 42, 840, 377, 351, 717, 614, 91, 694, 1064, 672, 605, 1073, 637, 764, 1167, 970, 144, 769, 684, 973, 688, 1069, 409, 699, 639, 128, 685, 839, 570, 187, 818, 600, 504, 709, 734, 510, 147, 906, 670, 250, 531, 151, 965, 968, 1147, 565, 132, 1049, 633, 1044, 1130, 384, 602, 77, 943, 295, 1100, 856, 808, 700, 94, 853, 496, 316, 1140, 479, 260, 859, 347, 532, 349, 514, 957, 1117, 666, 421, 594, 778, 1170, 86, 756, 293, 208, 1, 338, 819, 702, 148, 204, 723, 773, 678, 568, 630, 217, 682, 598, 751, 116, 434, 1085, 353, 450, 407, 345, 323, 720, 1186, 790, 362, 203, 536, 498, 781, 107, 58, 34, 67, 701, 171, 32, 736, 1211, 233, 1107, 413, 774, 164, 958, 542, 860, 507, 291, 241, 1090, 981, 1160, 888, 1098, 629, 742, 1113, 885, 820, 375, 880, 442, 92, 1052, 794, 1086, 1099, 255, 821, 33, 844, 1180, 152, 524, 1110, 38, 1096, 816, 1009, 112, 817, 1208, 749, 681, 319, 671, 197, 73, 12, 944, 183, 828, 554, 396, 822, 445, 718, 673, 1002, 513, 188, 555, 1204, 88, 272, 221, 523, 456, 1092, 915, 640, 611, 397, 1201, 461, 892, 61, 201, 1154, 1068, 738, 1029, 952, 740, 960, 339, 866, 783, 130, 304, 1146, 89, 798, 165, 1063, 313, 850, 427, 235, 852, 979, 951, 95, 395, 1070, 383, 321, 796, 224, 1125, 572, 757, 954, 858, 1123, 921, 437, 971, 1175, 1188, 785, 949, 337, 1191, 779, 525, 836, 1008, 544, 15, 1101, 1132, 1015, 948, 1156, 727, 926, 730, 26, 41, 761, 938, 446, 924, 745, 199, 1185, 467, 889, 871, 244, 176, 596, 399, 802, 825, 865, 459, 621, 804, 1152, 984, 502, 588, 945, 234, 4, 281, 247, 1072, 841, 586, 357, 583, 890, 689, 1054, 257, 533, 886, 170, 481, 469, 989, 207, 1034, 998, 62, 482, 190, 63, 1112, 141, 1128, 117, 1119, 854, 581, 154, 1042, 1023, 1047, 653, 695, 1016, 1176, 985, 925, 654, 136, 491, 1164, 180, 963, 1174, 435, 624, 667, 305, 486, 563, 75, 1153, 108, 365, 158, 725, 815, 1076, 24, 558, 1059, 872, 301, 826, 405, 897, 662, 46, 1074, 320, 1051, 770, 996, 947, 1134, 277, 259, 743, 348, 876, 40, 380, 282, 941, 189, 276, 1032, 832, 441, 1022, 587, 101, 169, 184, 929, 1122, 739, 1080, 381, 292, 219, 1192, 807, 96, 80, 121, 1129, 1020, 553, 959, 238, 679, 792, 274, 1006, 848, 519, 517, 480, 932, 332, 905, 660, 863, 983, 715, 711, 1106, 104, 927, 1182, 54, 601, 138, 162, 298, 31, 873, 425, 216, 25, 908, 591, 1048, 980, 71, 668, 696, 500, 29, 746, 1050, 1214, 0, 972, 793, 766, 537, 159, 857, 978, 827, 661, 1157, 209, 417, 106, 814, 577, 1095, 521, 436, 875, 306, 787, 831, 1133, 1061, 899, 444, 1038, 1104, 245, 328, 912, 838, 579, 881, 448, 707, 907, 1011, 105, 447, 697, 113, 1166, 354, 535, 728, 370, 762, 303, 285, 248, 914, 1159, 489, 404, 644, 133, 286, 994, 657, 194, 526, 830, 389, 341, 1163, 222, 634, 917, 1141, 330, 93, 698, 988, 548, 855, 269, 103, 545, 904, 97, 797, 177, 652, 911, 1004, 842, 893, 102, 768, 1089, 789, 1071, 1030, 1003, 1115, 530, 432, 638, 1000, 408, 210, 894, 137, 946, 1013, 17, 1082, 845, 809, 824, 922, 173, 460, 683, 149, 551, 378, 478, 402, 909, 470, 474, 59, 582, 690, 416, 576, 431, 724, 731, 529, 23, 843, 82, 36, 753, 264, 99, 556, 385, 967, 311, 440, 765, 490, 610, 79, 185, 484, 312, 1144, 874, 920, 1168, 468, 1213, 45, 453, 541, 246, 658, 1124, 1196, 986, 472, 1149, 969, 279, 492, 387, 51, 703, 931, 114, 52, 750, 522, 847, 270, 729, 599, 359, 5, 813, 9, 265, 196, 1083, 903, 618, 218, 687, 1043, 1210, 1103, 617, 503, 1065, 414, 168, 1137, 691, 791, 1028, 181, 494, 1184, 595, 1067, 741, 43, 1177, 882, 923, 966, 382, 628, 636, 635, 290, 737, 7, 589, 1091, 1056, 360, 786, 726, 549, 642, 318, 939, 74, 174, 326, 115, 262, 615, 473, 287, 900, 1205, 289, 261, 477, 418, 1055, 294, 1158, 501, 192, 322, 1139, 919, 788, 131, 663, 28, 225, 299, 511, 1162, 212, 1078, 237, 1088, 363, 317, 650, 392, 263, 6, 205, 1120, 895, 124, 202, 780, 451, 713, 861, 1131, 455, 175, 438, 1025, 386, 156, 157, 87, 1126, 68, 66, 139, 1001, 85, 223, 999, 179, 65, 1207, 805, 356, 275, 127, 799, 649, 64, 475, 16, 1094, 936, 665, 140, 735, 680, 57, 877, 415, 1121, 69, 626, 242, 1173, 240, 125, 1190, 1021, 18, 686, 485, 457, 956, 584, 145, 1172, 143, 346, 622, 631, 812, 651, 647, 252, 333, 528, 898, 1045, 8, 1183, 604, 471, 1060, 308, 19, 35, 864, 811, 14, 1024, 53, 1075, 11, 879, 992, 1108, 493, 398, 325, 76, 1181, 975, 109, 1102, 1161, 643, 1058, 708, 714, 195, 1036, 254, 675, 239, 659, 540, 376, 559, 693, 129, 160, 1019, 964, 310, 388, 1026, 420, 823, 1116, 1171, 1212, 142, 632, 1178, 623, 297, 314, 466, 367, 243, 1017, 833, 135, 155, 597, 732, 910, 30, 1010, 403, 198, 215, 119, 161, 410, 206, 361, 27, 284, 122, 550, 21, 288]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8378122620358232
the save name prefix for this run is:  chkpt-ID_8378122620358232_tag_Ablation-job-blacklist-lr
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 237
rank avg (pred): 0.581 +- 0.005
mrr vals (pred, true): 0.013, 0.042
batch losses (mrrl, rdl): 0.0, 0.0003957945

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 816
rank avg (pred): 0.335 +- 0.229
mrr vals (pred, true): 0.174, 0.037
batch losses (mrrl, rdl): 0.0, 0.0001583097

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1059
rank avg (pred): 0.261 +- 0.213
mrr vals (pred, true): 0.278, 0.548
batch losses (mrrl, rdl): 0.0, 0.0012721241

Epoch over!
epoch time: 11.943

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 311
rank avg (pred): 0.266 +- 0.205
mrr vals (pred, true): 0.238, 0.559
batch losses (mrrl, rdl): 0.0, 0.0013289017

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1187
rank avg (pred): 0.379 +- 0.303
mrr vals (pred, true): 0.268, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001269691

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 862
rank avg (pred): 0.335 +- 0.272
mrr vals (pred, true): 0.271, 0.106
batch losses (mrrl, rdl): 0.0, 0.0002278005

Epoch over!
epoch time: 11.904

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 382
rank avg (pred): 0.236 +- 0.210
mrr vals (pred, true): 0.336, 0.123
batch losses (mrrl, rdl): 0.0, 3.36563e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 757
rank avg (pred): 0.315 +- 0.277
mrr vals (pred, true): 0.332, 0.055
batch losses (mrrl, rdl): 0.0, 0.0002200599

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 447
rank avg (pred): 0.237 +- 0.218
mrr vals (pred, true): 0.368, 0.056
batch losses (mrrl, rdl): 0.0, 0.0005547933

Epoch over!
epoch time: 12.041

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 857
rank avg (pred): 0.325 +- 0.275
mrr vals (pred, true): 0.296, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001330646

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 489
rank avg (pred): 0.349 +- 0.300
mrr vals (pred, true): 0.337, 0.185
batch losses (mrrl, rdl): 0.0, 0.0003591468

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 980
rank avg (pred): 0.268 +- 0.237
mrr vals (pred, true): 0.341, 0.552
batch losses (mrrl, rdl): 0.0, 0.0013773881

Epoch over!
epoch time: 11.873

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 417
rank avg (pred): 0.249 +- 0.222
mrr vals (pred, true): 0.346, 0.047
batch losses (mrrl, rdl): 0.0, 0.0005881074

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 468
rank avg (pred): 0.212 +- 0.210
mrr vals (pred, true): 0.406, 0.045
batch losses (mrrl, rdl): 0.0, 0.0009310085

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1185
rank avg (pred): 0.337 +- 0.273
mrr vals (pred, true): 0.311, 0.045
batch losses (mrrl, rdl): 0.0, 0.0002282071

Epoch over!
epoch time: 11.899

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 679
rank avg (pred): 0.343 +- 0.302
mrr vals (pred, true): 0.354, 0.060
batch losses (mrrl, rdl): 0.9221980572, 7.11809e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 166
rank avg (pred): 0.100 +- 0.060
mrr vals (pred, true): 0.247, 0.046
batch losses (mrrl, rdl): 0.3883396089, 0.0023516994

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 834
rank avg (pred): 0.701 +- 0.296
mrr vals (pred, true): 0.095, 0.220
batch losses (mrrl, rdl): 0.1577734947, 0.0054633361

Epoch over!
epoch time: 13.094

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 225
rank avg (pred): 0.157 +- 0.091
mrr vals (pred, true): 0.226, 0.050
batch losses (mrrl, rdl): 0.3110024631, 0.0016523759

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 450
rank avg (pred): 0.142 +- 0.083
mrr vals (pred, true): 0.237, 0.053
batch losses (mrrl, rdl): 0.3492741585, 0.001776559

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 634
rank avg (pred): 0.886 +- 0.301
mrr vals (pred, true): 0.076, 0.043
batch losses (mrrl, rdl): 0.0065343641, 0.0030532496

Epoch over!
epoch time: 12.373

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1052
rank avg (pred): 0.189 +- 0.106
mrr vals (pred, true): 0.218, 0.043
batch losses (mrrl, rdl): 0.2822073102, 0.0012798243

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 262
rank avg (pred): 0.120 +- 0.070
mrr vals (pred, true): 0.248, 0.563
batch losses (mrrl, rdl): 0.9930586219, 0.0001753397

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 483
rank avg (pred): 0.181 +- 0.100
mrr vals (pred, true): 0.225, 0.055
batch losses (mrrl, rdl): 0.3048983812, 0.0013249334

Epoch over!
epoch time: 12.5

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 104
rank avg (pred): 0.174 +- 0.096
mrr vals (pred, true): 0.226, 0.118
batch losses (mrrl, rdl): 0.1164400876, 0.0002671508

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1130
rank avg (pred): 0.232 +- 0.125
mrr vals (pred, true): 0.215, 0.046
batch losses (mrrl, rdl): 0.2718602717, 0.000810696

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 42
rank avg (pred): 0.244 +- 0.127
mrr vals (pred, true): 0.194, 0.531
batch losses (mrrl, rdl): 1.1353493929, 0.0008964672

Epoch over!
epoch time: 13.114

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1174
rank avg (pred): 0.871 +- 0.312
mrr vals (pred, true): 0.083, 0.036
batch losses (mrrl, rdl): 0.0110200793, 0.0026593558

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 5
rank avg (pred): 0.131 +- 0.079
mrr vals (pred, true): 0.263, 0.522
batch losses (mrrl, rdl): 0.673183918, 0.0001929189

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 354
rank avg (pred): 0.218 +- 0.121
mrr vals (pred, true): 0.225, 0.101
batch losses (mrrl, rdl): 0.1535449624, 0.0003326019

Epoch over!
epoch time: 13.326

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 187
rank avg (pred): 0.201 +- 0.107
mrr vals (pred, true): 0.214, 0.052
batch losses (mrrl, rdl): 0.2677357495, 0.0009391656

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 658
rank avg (pred): 0.836 +- 0.345
mrr vals (pred, true): 0.106, 0.046
batch losses (mrrl, rdl): 0.0308538508, 0.0028414319

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 399
rank avg (pred): 0.242 +- 0.128
mrr vals (pred, true): 0.217, 0.129
batch losses (mrrl, rdl): 0.0765204802, 3.65836e-05

Epoch over!
epoch time: 12.343

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 353
rank avg (pred): 0.161 +- 0.098
mrr vals (pred, true): 0.260, 0.096
batch losses (mrrl, rdl): 0.440538466, 0.0005267819

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 120
rank avg (pred): 0.259 +- 0.144
mrr vals (pred, true): 0.223, 0.100
batch losses (mrrl, rdl): 0.1491490155, 5.70027e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 355
rank avg (pred): 0.181 +- 0.103
mrr vals (pred, true): 0.245, 0.086
batch losses (mrrl, rdl): 0.3799223602, 0.0004224494

Epoch over!
epoch time: 14.674

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1020
rank avg (pred): 0.273 +- 0.152
mrr vals (pred, true): 0.227, 0.128
batch losses (mrrl, rdl): 0.0984629542, 3.14668e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1075
rank avg (pred): 0.293 +- 0.157
mrr vals (pred, true): 0.218, 0.547
batch losses (mrrl, rdl): 1.0769877434, 0.0015301688

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 430
rank avg (pred): 0.164 +- 0.090
mrr vals (pred, true): 0.236, 0.053
batch losses (mrrl, rdl): 0.3451115191, 0.0015777708

Epoch over!
epoch time: 16.252

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 628
rank avg (pred): 0.915 +- 0.265
mrr vals (pred, true): 0.069, 0.047
batch losses (mrrl, rdl): 0.0037035118, 0.0037770069

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 138
rank avg (pred): 0.308 +- 0.164
mrr vals (pred, true): 0.217, 0.106
batch losses (mrrl, rdl): 0.1219438836, 3.2033e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 30
rank avg (pred): 0.259 +- 0.142
mrr vals (pred, true): 0.229, 0.535
batch losses (mrrl, rdl): 0.9369796515, 0.0010796809

Epoch over!
epoch time: 16.048

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 654
rank avg (pred): 0.835 +- 0.355
mrr vals (pred, true): 0.117, 0.045
batch losses (mrrl, rdl): 0.0446810238, 0.0026222668

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 729
rank avg (pred): 0.797 +- 0.387
mrr vals (pred, true): 0.162, 0.368
batch losses (mrrl, rdl): 0.4273892939, 0.0087848641

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 858
rank avg (pred): 0.743 +- 0.373
mrr vals (pred, true): 0.183, 0.036
batch losses (mrrl, rdl): 0.1779600382, 0.0010183431

Epoch over!
epoch time: 13.868

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.923 +- 0.258
mrr vals (pred, true): 0.070, 0.100

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04610 	 0.01527 	 m..s
    3 	     1 	 0.04707 	 0.01573 	 m..s
    0 	     2 	 0.04525 	 0.01638 	 ~...
    4 	     3 	 0.05470 	 0.01764 	 m..s
   46 	     4 	 0.16455 	 0.03392 	 MISS
   16 	     5 	 0.06726 	 0.03661 	 m..s
   53 	     6 	 0.20109 	 0.03793 	 MISS
   30 	     7 	 0.08962 	 0.03921 	 m..s
  104 	     8 	 0.23458 	 0.03938 	 MISS
   69 	     9 	 0.22775 	 0.04091 	 MISS
   18 	    10 	 0.06968 	 0.04099 	 ~...
   20 	    11 	 0.06976 	 0.04115 	 ~...
   10 	    12 	 0.06344 	 0.04161 	 ~...
   93 	    13 	 0.23260 	 0.04229 	 MISS
    0 	    14 	 0.04525 	 0.04366 	 ~...
   33 	    15 	 0.09366 	 0.04440 	 m..s
   64 	    16 	 0.22738 	 0.04447 	 MISS
   65 	    17 	 0.22738 	 0.04450 	 MISS
   23 	    18 	 0.07180 	 0.04468 	 ~...
   48 	    19 	 0.16705 	 0.04475 	 MISS
   17 	    20 	 0.06768 	 0.04499 	 ~...
   82 	    21 	 0.23037 	 0.04506 	 MISS
   25 	    22 	 0.07317 	 0.04507 	 ~...
   96 	    23 	 0.23286 	 0.04542 	 MISS
  100 	    24 	 0.23349 	 0.04619 	 MISS
    6 	    25 	 0.06114 	 0.04626 	 ~...
  112 	    26 	 0.23584 	 0.04649 	 MISS
   29 	    27 	 0.08760 	 0.04656 	 m..s
   75 	    28 	 0.22894 	 0.04695 	 MISS
  101 	    29 	 0.23366 	 0.04696 	 MISS
   89 	    30 	 0.23187 	 0.04759 	 MISS
   99 	    31 	 0.23312 	 0.04771 	 MISS
   12 	    32 	 0.06445 	 0.04787 	 ~...
  108 	    33 	 0.23515 	 0.04790 	 MISS
  118 	    34 	 0.23763 	 0.04832 	 MISS
   81 	    35 	 0.23013 	 0.04856 	 MISS
   41 	    36 	 0.10942 	 0.04860 	 m..s
   56 	    37 	 0.21060 	 0.04881 	 MISS
   40 	    38 	 0.10570 	 0.04909 	 m..s
   22 	    39 	 0.07154 	 0.04921 	 ~...
   32 	    40 	 0.09183 	 0.04951 	 m..s
   87 	    41 	 0.23162 	 0.04960 	 MISS
    5 	    42 	 0.05660 	 0.05017 	 ~...
   59 	    43 	 0.22723 	 0.05022 	 MISS
    8 	    44 	 0.06339 	 0.05040 	 ~...
   31 	    45 	 0.09085 	 0.05049 	 m..s
   50 	    46 	 0.17997 	 0.05151 	 MISS
   47 	    47 	 0.16624 	 0.05175 	 MISS
   11 	    48 	 0.06428 	 0.05275 	 ~...
  114 	    49 	 0.23618 	 0.05348 	 MISS
   91 	    50 	 0.23217 	 0.05362 	 MISS
   37 	    51 	 0.09835 	 0.05364 	 m..s
   54 	    52 	 0.20908 	 0.05499 	 MISS
   49 	    53 	 0.17723 	 0.05523 	 MISS
    7 	    54 	 0.06216 	 0.05604 	 ~...
  107 	    55 	 0.23513 	 0.05812 	 MISS
   15 	    56 	 0.06580 	 0.05990 	 ~...
   14 	    57 	 0.06540 	 0.06022 	 ~...
   42 	    58 	 0.13985 	 0.07415 	 m..s
   75 	    59 	 0.22894 	 0.09096 	 MISS
   57 	    60 	 0.22383 	 0.09158 	 MISS
   24 	    61 	 0.07300 	 0.09994 	 ~...
   18 	    62 	 0.06968 	 0.10034 	 m..s
   67 	    63 	 0.22764 	 0.10109 	 MISS
   74 	    64 	 0.22885 	 0.10123 	 MISS
  120 	    65 	 0.24015 	 0.10545 	 MISS
   71 	    66 	 0.22856 	 0.10756 	 MISS
  110 	    67 	 0.23536 	 0.10777 	 MISS
   20 	    68 	 0.06976 	 0.10979 	 m..s
   86 	    69 	 0.23158 	 0.11191 	 MISS
  109 	    70 	 0.23534 	 0.11469 	 MISS
   77 	    71 	 0.22921 	 0.11794 	 MISS
   84 	    72 	 0.23061 	 0.11926 	 MISS
   88 	    73 	 0.23185 	 0.12025 	 MISS
   95 	    74 	 0.23276 	 0.12426 	 MISS
   13 	    75 	 0.06468 	 0.12739 	 m..s
   26 	    76 	 0.07378 	 0.12843 	 m..s
  117 	    77 	 0.23667 	 0.12861 	 MISS
    8 	    78 	 0.06339 	 0.13010 	 m..s
   92 	    79 	 0.23236 	 0.13014 	 MISS
  102 	    80 	 0.23417 	 0.13168 	 MISS
   27 	    81 	 0.07536 	 0.13415 	 m..s
   58 	    82 	 0.22407 	 0.14134 	 m..s
  104 	    83 	 0.23458 	 0.14266 	 m..s
  103 	    84 	 0.23439 	 0.15188 	 m..s
  119 	    85 	 0.23994 	 0.15403 	 m..s
   82 	    86 	 0.23037 	 0.15690 	 m..s
   28 	    87 	 0.08064 	 0.16639 	 m..s
   44 	    88 	 0.14223 	 0.17970 	 m..s
   45 	    89 	 0.14471 	 0.21216 	 m..s
   39 	    90 	 0.10327 	 0.24069 	 MISS
   35 	    91 	 0.09610 	 0.24174 	 MISS
   34 	    92 	 0.09477 	 0.24467 	 MISS
   42 	    93 	 0.13985 	 0.24502 	 MISS
   36 	    94 	 0.09698 	 0.25029 	 MISS
   37 	    95 	 0.09835 	 0.28010 	 MISS
   50 	    96 	 0.17997 	 0.36005 	 MISS
   52 	    97 	 0.19971 	 0.42624 	 MISS
   78 	    98 	 0.22932 	 0.50619 	 MISS
   55 	    99 	 0.20957 	 0.51622 	 MISS
   72 	   100 	 0.22876 	 0.52851 	 MISS
   98 	   101 	 0.23293 	 0.53150 	 MISS
   79 	   102 	 0.22958 	 0.53485 	 MISS
   60 	   103 	 0.22724 	 0.53587 	 MISS
   62 	   104 	 0.22733 	 0.53628 	 MISS
   68 	   105 	 0.22765 	 0.53728 	 MISS
   85 	   106 	 0.23137 	 0.53917 	 MISS
   61 	   107 	 0.22733 	 0.53922 	 MISS
   70 	   108 	 0.22848 	 0.54053 	 MISS
   80 	   109 	 0.22977 	 0.54280 	 MISS
  113 	   110 	 0.23598 	 0.54412 	 MISS
  106 	   111 	 0.23504 	 0.54463 	 MISS
   94 	   112 	 0.23270 	 0.54524 	 MISS
   66 	   113 	 0.22764 	 0.54712 	 MISS
   90 	   114 	 0.23198 	 0.54725 	 MISS
   97 	   115 	 0.23290 	 0.55148 	 MISS
   63 	   116 	 0.22738 	 0.55203 	 MISS
   73 	   117 	 0.22881 	 0.55441 	 MISS
  111 	   118 	 0.23583 	 0.55447 	 MISS
  116 	   119 	 0.23635 	 0.56083 	 MISS
  114 	   120 	 0.23618 	 0.56362 	 MISS
==========================================
r_mrr = 0.393289178609848
r2_mrr = 0.15341049432754517
spearmanr_mrr@5 = 0.9702462553977966
spearmanr_mrr@10 = 0.932304859161377
spearmanr_mrr@50 = 0.8972582817077637
spearmanr_mrr@100 = 0.566269040107727
spearmanr_mrr@All = 0.621451199054718
==========================================
test time: 0.612
Done Testing dataset UMLS
total time taken: 203.58588004112244
training time taken: 197.92853784561157
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.3933)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.1534)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9702)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9323)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.8973)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.5663)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.6215)}}, 'test_loss': {'DistMult': {'UMLS': 36.70579229912255}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'lr'}

====================================================================
--------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-reg_coeff
--------------------------------------------------------------------
====================================================================
Running on a grid of size 1
Using random seed: 4158082564001064
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [851, 970, 941, 1181, 290, 1055, 223, 456, 1111, 1041, 168, 1016, 643, 513, 1191, 806, 286, 844, 723, 654, 455, 543, 812, 35, 790, 8, 1206, 1013, 763, 1066, 783, 32, 746, 101, 647, 0, 762, 589, 811, 885, 438, 557, 72, 1054, 1019, 903, 190, 569, 1214, 693, 779, 606, 300, 856, 733, 1141, 755, 909, 553, 306, 91, 817, 335, 356, 98, 244, 936, 1211, 720, 821, 1040, 886, 107, 722, 445, 770, 1075, 656, 246, 708, 1083, 302, 1014, 894, 69, 124, 996, 12, 1188, 685, 673, 1138, 1133, 460, 995, 1020, 753, 860, 854, 577, 1032, 110, 55, 1001, 1144, 534, 331, 614, 1050, 556, 324, 441, 814, 646, 748, 1137, 925, 914, 237, 17, 843]
valid_ids (0): []
train_ids (1094): [1183, 409, 993, 254, 129, 348, 354, 11, 1025, 778, 572, 102, 954, 910, 873, 1168, 578, 1061, 121, 248, 205, 1116, 615, 1189, 346, 325, 135, 786, 645, 29, 443, 946, 1154, 252, 588, 655, 1199, 282, 703, 43, 231, 394, 780, 220, 1148, 242, 974, 709, 1186, 846, 351, 801, 973, 85, 336, 175, 239, 681, 119, 966, 566, 530, 463, 565, 943, 210, 206, 263, 568, 1142, 1165, 50, 279, 905, 798, 19, 564, 256, 932, 309, 1200, 899, 418, 117, 944, 519, 277, 808, 729, 384, 667, 694, 24, 116, 953, 196, 1121, 809, 37, 1120, 1092, 355, 591, 251, 824, 400, 159, 581, 737, 209, 764, 634, 776, 850, 1126, 486, 311, 847, 1036, 1108, 296, 934, 266, 784, 982, 317, 392, 493, 405, 579, 1089, 829, 459, 1064, 616, 187, 662, 1063, 1067, 668, 980, 1115, 623, 92, 96, 1174, 484, 573, 482, 114, 388, 483, 613, 1021, 1150, 9, 489, 619, 350, 61, 153, 532, 485, 1155, 601, 716, 13, 174, 930, 358, 293, 415, 1039, 488, 1078, 963, 548, 14, 916, 1079, 434, 431, 374, 838, 1034, 857, 1159, 120, 490, 807, 751, 901, 452, 151, 1091, 1197, 215, 22, 956, 592, 81, 700, 1022, 1024, 314, 44, 841, 945, 620, 961, 1038, 988, 171, 872, 136, 449, 1146, 79, 867, 393, 782, 524, 679, 510, 1049, 386, 428, 525, 474, 211, 118, 272, 113, 672, 659, 717, 881, 462, 962, 426, 836, 514, 791, 981, 695, 712, 390, 688, 347, 1017, 420, 837, 39, 469, 1134, 774, 179, 464, 247, 416, 36, 476, 342, 567, 407, 499, 152, 896, 537, 128, 738, 660, 697, 203, 343, 454, 163, 305, 298, 1031, 106, 874, 931, 732, 502, 627, 882, 34, 1045, 1163, 1070, 200, 366, 1009, 923, 965, 926, 235, 76, 180, 719, 299, 487, 1167, 1190, 38, 978, 466, 1060, 1210, 198, 516, 768, 823, 41, 1011, 835, 517, 714, 461, 749, 657, 705, 212, 570, 991, 397, 253, 58, 536, 138, 871, 370, 147, 1000, 505, 391, 262, 772, 792, 741, 352, 225, 559, 707, 329, 423, 1099, 491, 78, 598, 410, 64, 1005, 1008, 216, 261, 169, 5, 590, 757, 547, 633, 275, 142, 60, 731, 1145, 866, 947, 1104, 382, 1112, 1028, 436, 228, 642, 940, 994, 86, 93, 1114, 88, 696, 1147, 383, 535, 1131, 740, 249, 270, 134, 1195, 189, 1012, 1002, 442, 398, 637, 429, 666, 84, 53, 349, 357, 967, 494, 735, 1042, 640, 972, 820, 202, 937, 361, 337, 194, 378, 952, 754, 711, 998, 1110, 268, 876, 255, 795, 521, 1100, 1158, 170, 1130, 1082, 1095, 868, 204, 140, 1101, 105, 968, 864, 853, 907, 976, 321, 173, 1088, 904, 130, 935, 31, 1156, 1173, 511, 1058, 260, 1015, 148, 1207, 523, 793, 481, 267, 1179, 222, 736, 859, 1003, 23, 161, 928, 341, 312, 345, 542, 766, 1162, 594, 810, 691, 167, 777, 307, 758, 471, 1212, 1084, 411, 715, 531, 677, 922, 964, 546, 137, 218, 702, 865, 665, 1194, 376, 593, 319, 89, 622, 1, 880, 586, 879, 726, 146, 1023, 412, 710, 457, 315, 805, 425, 51, 447, 333, 773, 1029, 408, 734, 177, 550, 214, 295, 401, 1073, 18, 115, 574, 1132, 477, 861, 765, 1157, 977, 969, 512, 427, 451, 54, 834, 1201, 75, 605, 1007, 771, 869, 1187, 472, 2, 796, 372, 815, 90, 626, 1103, 375, 689, 156, 385, 52, 653, 1026, 503, 554, 674, 127, 500, 756, 890, 323, 1097, 1059, 164, 924, 612, 403, 1068, 775, 387, 430, 960, 1004, 243, 301, 920, 957, 1202, 1193, 833, 424, 379, 234, 987, 219, 625, 241, 1065, 334, 744, 467, 1123, 951, 419, 1081, 603, 877, 1175, 638, 74, 549, 364, 721, 139, 558, 699, 100, 229, 652, 435, 406, 958, 617, 504, 1170, 663, 813, 208, 389, 56, 979, 166, 7, 1184, 328, 761, 373, 71, 433, 316, 340, 221, 274, 1052, 607, 111, 1209, 1171, 258, 585, 313, 680, 639, 395, 1178, 870, 671, 73, 1151, 618, 676, 551, 10, 1125, 1136, 1169, 624, 132, 911, 1135, 898, 197, 144, 948, 59, 125, 1037, 292, 875, 818, 600, 42, 294, 289, 718, 57, 949, 848, 172, 849, 584, 902, 4, 479, 1072, 402, 583, 1048, 1077, 30, 1117, 320, 828, 70, 917, 377, 830, 1047, 632, 83, 938, 360, 562, 658, 602, 192, 288, 413, 497, 25, 832, 1203, 528, 297, 199, 915, 1185, 109, 785, 478, 651, 629, 399, 155, 108, 21, 178, 892, 381, 77, 1149, 819, 992, 97, 912, 154, 803, 285, 112, 322, 839, 713, 575, 188, 670, 919, 444, 1198, 826, 541, 103, 636, 522, 1129, 975, 825, 40, 888, 701, 628, 468, 538, 362, 1152, 788, 104, 509, 822, 893, 184, 769, 587, 599, 730, 123, 887, 236, 747, 1090, 45, 1122, 939, 929, 6, 669, 889, 1053, 95, 986, 94, 781, 595, 1128, 230, 326, 631, 692, 1033, 900, 984, 563, 80, 304, 257, 1124, 496, 47, 518, 145, 66, 143, 728, 67, 165, 831, 515, 816, 1196, 1119, 470, 1205, 126, 999, 706, 264, 475, 310, 396, 227, 745, 276, 863, 99, 149, 157, 895, 291, 799, 1035, 927, 664, 158, 725, 122, 540, 1076, 989, 609, 498, 271, 308, 1109, 959, 608, 527, 245, 1213, 28, 724, 611, 884, 1118, 27, 1176, 480, 742, 1098, 1094, 68, 201, 597, 743, 1027, 1057, 529, 683, 1071, 641, 48, 1018, 465, 1127, 365, 1030, 648, 840, 678, 422, 339, 1204, 359, 787, 273, 186, 265, 767, 698, 238, 507, 727, 560, 49, 1074, 883, 439, 582, 942, 1085, 862, 533, 950, 1139, 3, 368, 908, 259, 858, 181, 287, 162, 684, 526, 690, 539, 552, 440, 417, 635, 1113, 269, 913, 508, 802, 353, 1044, 501, 176, 630, 15, 224, 369, 1096, 555, 571, 1161, 1172, 363, 404, 1140, 344, 897, 1164, 16, 794, 1056, 46, 327, 332, 1069, 131, 580, 675, 1192, 150, 450, 1051, 182, 852, 217, 281, 226, 997, 918, 278, 797, 1086, 878, 891, 1208, 827, 621, 193, 62, 446, 185, 371, 1043, 800, 432, 506, 983, 644, 492, 63, 421, 933, 380, 804, 160, 1006, 1080, 971, 1102, 473, 789, 195, 1180, 1182, 752, 661, 686, 561, 759, 545, 1087, 207, 284, 855, 250, 191, 280, 604, 330, 414, 303, 842, 183, 1010, 990, 750, 596, 649, 87, 495, 687, 650, 682, 82, 1093, 213, 704, 845, 739, 26, 1160, 448, 520, 232, 985, 576, 65, 1062, 1143, 1166, 1105, 1153, 233, 1046, 437, 610, 20, 1106, 283, 921, 133, 318, 367, 338, 955, 240, 544, 760, 906, 453, 458, 141, 33, 1107, 1177]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5244356936068407
the save name prefix for this run is:  chkpt-ID_5244356936068407_tag_Ablation-job-blacklist-reg_coeff
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 47
rank avg (pred): 0.530 +- 0.005
mrr vals (pred, true): 0.014, 0.532
batch losses (mrrl, rdl): 0.0, 0.0051609511

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 145
rank avg (pred): 0.385 +- 0.231
mrr vals (pred, true): 0.140, 0.085
batch losses (mrrl, rdl): 0.0, 0.0001016749

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 487
rank avg (pred): 0.315 +- 0.254
mrr vals (pred, true): 0.208, 0.213
batch losses (mrrl, rdl): 0.0, 0.0002255462

Epoch over!
epoch time: 13.679

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 764
rank avg (pred): 0.420 +- 0.279
mrr vals (pred, true): 0.157, 0.035
batch losses (mrrl, rdl): 0.0, 0.0002764759

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 366
rank avg (pred): 0.339 +- 0.265
mrr vals (pred, true): 0.201, 0.130
batch losses (mrrl, rdl): 0.0, 0.0001013157

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 275
rank avg (pred): 0.020 +- 0.018
mrr vals (pred, true): 0.475, 0.541
batch losses (mrrl, rdl): 0.0, 4.2883e-06

Epoch over!
epoch time: 13.942

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 536
rank avg (pred): 0.316 +- 0.250
mrr vals (pred, true): 0.193, 0.098
batch losses (mrrl, rdl): 0.0, 2.95376e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 548
rank avg (pred): 0.330 +- 0.262
mrr vals (pred, true): 0.192, 0.132
batch losses (mrrl, rdl): 0.0, 2.98133e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 451
rank avg (pred): 0.372 +- 0.271
mrr vals (pred, true): 0.149, 0.047
batch losses (mrrl, rdl): 0.0, 5.27407e-05

Epoch over!
epoch time: 14.442

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 132
rank avg (pred): 0.362 +- 0.269
mrr vals (pred, true): 0.166, 0.128
batch losses (mrrl, rdl): 0.0, 0.0002125248

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1029
rank avg (pred): 0.318 +- 0.267
mrr vals (pred, true): 0.214, 0.054
batch losses (mrrl, rdl): 0.0, 0.0002431716

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 177
rank avg (pred): 0.370 +- 0.267
mrr vals (pred, true): 0.159, 0.043
batch losses (mrrl, rdl): 0.0, 5.93618e-05

Epoch over!
epoch time: 13.076

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 419
rank avg (pred): 0.372 +- 0.277
mrr vals (pred, true): 0.180, 0.048
batch losses (mrrl, rdl): 0.0, 3.14104e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 478
rank avg (pred): 0.339 +- 0.263
mrr vals (pred, true): 0.173, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001956786

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 764
rank avg (pred): 0.448 +- 0.287
mrr vals (pred, true): 0.123, 0.035
batch losses (mrrl, rdl): 0.0, 0.0001782062

Epoch over!
epoch time: 13.094

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 633
rank avg (pred): 0.465 +- 0.275
mrr vals (pred, true): 0.112, 0.045
batch losses (mrrl, rdl): 0.0386169888, 8.817e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 417
rank avg (pred): 0.428 +- 0.146
mrr vals (pred, true): 0.060, 0.047
batch losses (mrrl, rdl): 0.0010372367, 4.67708e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 699
rank avg (pred): 0.312 +- 0.166
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0005955391, 0.0004011651

Epoch over!
epoch time: 13.031

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 134
rank avg (pred): 0.374 +- 0.141
mrr vals (pred, true): 0.069, 0.115
batch losses (mrrl, rdl): 0.0208640266, 0.0001210881

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 504
rank avg (pred): 0.329 +- 0.155
mrr vals (pred, true): 0.103, 0.202
batch losses (mrrl, rdl): 0.0979793742, 0.0002759089

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 690
rank avg (pred): 0.330 +- 0.151
mrr vals (pred, true): 0.057, 0.044
batch losses (mrrl, rdl): 0.0004801242, 0.0002432667

Epoch over!
epoch time: 12.828

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 454
rank avg (pred): 0.375 +- 0.145
mrr vals (pred, true): 0.085, 0.047
batch losses (mrrl, rdl): 0.0120018693, 0.0001148393

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 535
rank avg (pred): 0.356 +- 0.192
mrr vals (pred, true): 0.128, 0.062
batch losses (mrrl, rdl): 0.0604485795, 9.94176e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 154
rank avg (pred): 0.359 +- 0.151
mrr vals (pred, true): 0.091, 0.119
batch losses (mrrl, rdl): 0.0077305464, 0.0001621669

Epoch over!
epoch time: 12.988

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 493
rank avg (pred): 0.359 +- 0.198
mrr vals (pred, true): 0.121, 0.218
batch losses (mrrl, rdl): 0.094070971, 0.0005864516

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 233
rank avg (pred): 0.388 +- 0.144
mrr vals (pred, true): 0.084, 0.047
batch losses (mrrl, rdl): 0.0117416168, 9.97968e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 530
rank avg (pred): 0.363 +- 0.193
mrr vals (pred, true): 0.121, 0.101
batch losses (mrrl, rdl): 0.0041161585, 6.03244e-05

Epoch over!
epoch time: 13.056

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1092
rank avg (pred): 0.409 +- 0.143
mrr vals (pred, true): 0.073, 0.111
batch losses (mrrl, rdl): 0.014480982, 0.0003209052

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 522
rank avg (pred): 0.404 +- 0.159
mrr vals (pred, true): 0.088, 0.050
batch losses (mrrl, rdl): 0.0146056106, 9.54787e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 374
rank avg (pred): 0.403 +- 0.162
mrr vals (pred, true): 0.089, 0.108
batch losses (mrrl, rdl): 0.0034057982, 0.0001992539

Epoch over!
epoch time: 12.927

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 132
rank avg (pred): 0.404 +- 0.130
mrr vals (pred, true): 0.065, 0.128
batch losses (mrrl, rdl): 0.0384839512, 0.0003916564

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 578
rank avg (pred): 0.364 +- 0.130
mrr vals (pred, true): 0.055, 0.036
batch losses (mrrl, rdl): 0.0002088984, 0.0004210385

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 800
rank avg (pred): 0.306 +- 0.169
mrr vals (pred, true): 0.058, 0.039
batch losses (mrrl, rdl): 0.0005843429, 0.0003390896

Epoch over!
epoch time: 13.124

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 149
rank avg (pred): 0.397 +- 0.126
mrr vals (pred, true): 0.063, 0.132
batch losses (mrrl, rdl): 0.0469005629, 0.0003296614

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 701
rank avg (pred): 0.339 +- 0.150
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 9.5385e-06, 0.000218686

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 507
rank avg (pred): 0.388 +- 0.189
mrr vals (pred, true): 0.123, 0.182
batch losses (mrrl, rdl): 0.0343117937, 0.0004903495

Epoch over!
epoch time: 12.851

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 228
rank avg (pred): 0.364 +- 0.132
mrr vals (pred, true): 0.070, 0.044
batch losses (mrrl, rdl): 0.0041307267, 0.0001368274

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 940
rank avg (pred): 0.316 +- 0.202
mrr vals (pred, true): 0.045, 0.018
batch losses (mrrl, rdl): 0.0002031834, 0.0033875918

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 301
rank avg (pred): 0.013 +- 0.012
mrr vals (pred, true): 0.555, 0.547
batch losses (mrrl, rdl): 0.0006858668, 1.19391e-05

Epoch over!
epoch time: 13.004

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 343
rank avg (pred): 0.388 +- 0.131
mrr vals (pred, true): 0.068, 0.115
batch losses (mrrl, rdl): 0.0215043239, 0.0002341226

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 287
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.582, 0.557
batch losses (mrrl, rdl): 0.0061820457, 1.16492e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 558
rank avg (pred): 0.405 +- 0.168
mrr vals (pred, true): 0.103, 0.050
batch losses (mrrl, rdl): 0.0283255968, 8.28783e-05

Epoch over!
epoch time: 12.99

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 236
rank avg (pred): 0.362 +- 0.132
mrr vals (pred, true): 0.069, 0.045
batch losses (mrrl, rdl): 0.0037162406, 0.0001812344

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 955
rank avg (pred): 0.321 +- 0.176
mrr vals (pred, true): 0.036, 0.047
batch losses (mrrl, rdl): 0.0018946374, 0.0003050906

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1053
rank avg (pred): 0.012 +- 0.011
mrr vals (pred, true): 0.581, 0.543
batch losses (mrrl, rdl): 0.014553098, 1.28555e-05

Epoch over!
epoch time: 12.955

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.352 +- 0.142
mrr vals (pred, true): 0.043, 0.042

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   37 	     0 	 0.04757 	 0.01569 	 m..s
   30 	     1 	 0.04423 	 0.01574 	 ~...
    2 	     2 	 0.03862 	 0.01822 	 ~...
   47 	     3 	 0.05459 	 0.02063 	 m..s
   13 	     4 	 0.04073 	 0.02690 	 ~...
    4 	     5 	 0.03886 	 0.02942 	 ~...
   19 	     6 	 0.04126 	 0.03231 	 ~...
   13 	     7 	 0.04073 	 0.03255 	 ~...
   16 	     8 	 0.04097 	 0.03391 	 ~...
   20 	     9 	 0.04151 	 0.03686 	 ~...
   11 	    10 	 0.04000 	 0.03747 	 ~...
   68 	    11 	 0.06463 	 0.03872 	 ~...
   34 	    12 	 0.04634 	 0.03999 	 ~...
   27 	    13 	 0.04323 	 0.04155 	 ~...
    6 	    14 	 0.03921 	 0.04191 	 ~...
   66 	    15 	 0.06339 	 0.04204 	 ~...
   75 	    16 	 0.07411 	 0.04242 	 m..s
   36 	    17 	 0.04710 	 0.04275 	 ~...
   26 	    18 	 0.04255 	 0.04292 	 ~...
    9 	    19 	 0.03968 	 0.04339 	 ~...
    5 	    20 	 0.03894 	 0.04361 	 ~...
   53 	    21 	 0.05811 	 0.04416 	 ~...
   61 	    22 	 0.06141 	 0.04450 	 ~...
   25 	    23 	 0.04208 	 0.04463 	 ~...
   22 	    24 	 0.04164 	 0.04467 	 ~...
   23 	    25 	 0.04192 	 0.04475 	 ~...
    6 	    26 	 0.03921 	 0.04493 	 ~...
   73 	    27 	 0.06900 	 0.04506 	 ~...
    0 	    28 	 0.03806 	 0.04527 	 ~...
   27 	    29 	 0.04323 	 0.04534 	 ~...
    0 	    30 	 0.03806 	 0.04536 	 ~...
   18 	    31 	 0.04116 	 0.04559 	 ~...
   38 	    32 	 0.04832 	 0.04573 	 ~...
   65 	    33 	 0.06295 	 0.04604 	 ~...
   75 	    34 	 0.07411 	 0.04608 	 ~...
   12 	    35 	 0.04050 	 0.04665 	 ~...
    9 	    36 	 0.03968 	 0.04667 	 ~...
   50 	    37 	 0.05564 	 0.04682 	 ~...
    3 	    38 	 0.03875 	 0.04716 	 ~...
   21 	    39 	 0.04153 	 0.04744 	 ~...
   56 	    40 	 0.06074 	 0.04750 	 ~...
    8 	    41 	 0.03960 	 0.04787 	 ~...
   15 	    42 	 0.04090 	 0.04804 	 ~...
   71 	    43 	 0.06852 	 0.04828 	 ~...
   75 	    44 	 0.07411 	 0.04832 	 ~...
   41 	    45 	 0.05246 	 0.04837 	 ~...
   41 	    46 	 0.05246 	 0.04921 	 ~...
   24 	    47 	 0.04199 	 0.04922 	 ~...
   80 	    48 	 0.08034 	 0.05017 	 m..s
   35 	    49 	 0.04681 	 0.05102 	 ~...
   40 	    50 	 0.05147 	 0.05108 	 ~...
   58 	    51 	 0.06088 	 0.05133 	 ~...
   33 	    52 	 0.04593 	 0.05144 	 ~...
   43 	    53 	 0.05313 	 0.05197 	 ~...
   29 	    54 	 0.04343 	 0.05219 	 ~...
   45 	    55 	 0.05353 	 0.05246 	 ~...
   79 	    56 	 0.07966 	 0.05340 	 ~...
   67 	    57 	 0.06405 	 0.05366 	 ~...
   32 	    58 	 0.04563 	 0.05419 	 ~...
   47 	    59 	 0.05459 	 0.05467 	 ~...
   39 	    60 	 0.05123 	 0.05719 	 ~...
   31 	    61 	 0.04533 	 0.05890 	 ~...
   82 	    62 	 0.08765 	 0.06245 	 ~...
   83 	    63 	 0.09486 	 0.06286 	 m..s
   81 	    64 	 0.08161 	 0.06455 	 ~...
   92 	    65 	 0.16916 	 0.06463 	 MISS
   44 	    66 	 0.05347 	 0.07363 	 ~...
   74 	    67 	 0.07238 	 0.07365 	 ~...
   69 	    68 	 0.06548 	 0.07881 	 ~...
   49 	    69 	 0.05544 	 0.08768 	 m..s
   52 	    70 	 0.05811 	 0.09096 	 m..s
   62 	    71 	 0.06247 	 0.09788 	 m..s
   59 	    72 	 0.06088 	 0.10018 	 m..s
   78 	    73 	 0.07512 	 0.10109 	 ~...
   70 	    74 	 0.06761 	 0.10197 	 m..s
   57 	    75 	 0.06077 	 0.10255 	 m..s
   86 	    76 	 0.11934 	 0.10979 	 ~...
   17 	    77 	 0.04103 	 0.11373 	 m..s
   55 	    78 	 0.05942 	 0.11564 	 m..s
   59 	    79 	 0.06088 	 0.12400 	 m..s
   54 	    80 	 0.05940 	 0.12660 	 m..s
   72 	    81 	 0.06870 	 0.12795 	 m..s
   51 	    82 	 0.05617 	 0.12861 	 m..s
   46 	    83 	 0.05438 	 0.13168 	 m..s
   64 	    84 	 0.06255 	 0.13246 	 m..s
   88 	    85 	 0.13032 	 0.13990 	 ~...
   63 	    86 	 0.06248 	 0.14243 	 m..s
   97 	    87 	 0.42192 	 0.14495 	 MISS
   84 	    88 	 0.10592 	 0.16959 	 m..s
   87 	    89 	 0.12148 	 0.22480 	 MISS
   91 	    90 	 0.14095 	 0.22949 	 m..s
   89 	    91 	 0.13639 	 0.24467 	 MISS
   84 	    92 	 0.10592 	 0.24502 	 MISS
   98 	    93 	 0.42310 	 0.26525 	 MISS
   93 	    94 	 0.35024 	 0.27181 	 m..s
   90 	    95 	 0.13963 	 0.31456 	 MISS
   95 	    96 	 0.41900 	 0.32168 	 m..s
   94 	    97 	 0.39013 	 0.50977 	 MISS
  100 	    98 	 0.49636 	 0.51017 	 ~...
  102 	    99 	 0.49898 	 0.51240 	 ~...
  108 	   100 	 0.50280 	 0.51740 	 ~...
   95 	   101 	 0.41900 	 0.52229 	 MISS
  109 	   102 	 0.50728 	 0.52273 	 ~...
  107 	   103 	 0.50179 	 0.52563 	 ~...
  117 	   104 	 0.51901 	 0.53233 	 ~...
  105 	   105 	 0.50157 	 0.53399 	 m..s
  111 	   106 	 0.51197 	 0.53662 	 ~...
  115 	   107 	 0.51749 	 0.53917 	 ~...
  103 	   108 	 0.49998 	 0.53966 	 m..s
  112 	   109 	 0.51220 	 0.53971 	 ~...
  114 	   110 	 0.51579 	 0.54080 	 ~...
  105 	   111 	 0.50157 	 0.54120 	 m..s
  112 	   112 	 0.51220 	 0.54663 	 m..s
   99 	   113 	 0.49120 	 0.55203 	 m..s
  118 	   114 	 0.52077 	 0.55256 	 m..s
  104 	   115 	 0.50096 	 0.55334 	 m..s
  119 	   116 	 0.52346 	 0.55473 	 m..s
  101 	   117 	 0.49710 	 0.55731 	 m..s
  110 	   118 	 0.50818 	 0.56546 	 m..s
  120 	   119 	 0.52613 	 0.56693 	 m..s
  115 	   120 	 0.51749 	 0.57101 	 m..s
==========================================
r_mrr = 0.9640913605690002
r2_mrr = 0.9257113337516785
spearmanr_mrr@5 = 0.955365777015686
spearmanr_mrr@10 = 0.9485984444618225
spearmanr_mrr@50 = 0.9755133390426636
spearmanr_mrr@100 = 0.9856700897216797
spearmanr_mrr@All = 0.9866034388542175
==========================================
test time: 0.435
Done Testing dataset UMLS
total time taken: 204.77367877960205
training time taken: 198.49239325523376
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9641)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9257)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9554)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9486)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9755)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9857)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9866)}}, 'test_loss': {'DistMult': {'UMLS': 3.355208444641903}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'reg_coeff'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-npp
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 2946777720149246
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1191, 1198, 844, 764, 967, 1108, 163, 997, 475, 406, 1032, 438, 10, 301, 598, 114, 766, 94, 1189, 1073, 239, 221, 1106, 459, 299, 1094, 825, 1059, 188, 603, 718, 285, 729, 1153, 461, 228, 982, 211, 328, 638, 747, 251, 914, 183, 119, 890, 78, 23, 678, 270, 1070, 112, 387, 149, 16, 518, 369, 871, 317, 477, 271, 423, 582, 1083, 1197, 207, 1145, 541, 605, 971, 195, 796, 452, 556, 1107, 485, 1125, 595, 1130, 504, 64, 93, 927, 643, 1181, 466, 142, 912, 201, 933, 490, 560, 763, 618, 1014, 783, 876, 327, 1148, 234, 401, 362, 963, 469, 591, 516, 744, 889, 36, 86, 934, 510, 500, 283, 891, 378, 960, 1211, 430, 937, 751]
valid_ids (0): []
train_ids (1094): [250, 448, 872, 657, 139, 802, 1116, 118, 403, 232, 690, 315, 263, 680, 677, 205, 588, 414, 198, 335, 1007, 1017, 1087, 361, 351, 1000, 51, 101, 1149, 993, 219, 364, 190, 642, 529, 308, 374, 561, 865, 800, 75, 812, 12, 174, 257, 822, 458, 27, 587, 1018, 537, 377, 359, 859, 658, 321, 1174, 1127, 870, 907, 853, 622, 137, 676, 866, 241, 615, 1185, 838, 383, 606, 509, 50, 204, 837, 750, 863, 433, 136, 84, 609, 372, 238, 206, 334, 1011, 147, 661, 972, 542, 793, 1203, 757, 132, 584, 696, 453, 717, 1028, 412, 88, 1023, 399, 245, 808, 512, 92, 897, 165, 65, 1192, 1112, 1204, 619, 557, 942, 956, 172, 887, 881, 987, 592, 602, 1043, 69, 919, 616, 784, 1140, 330, 1027, 77, 995, 670, 1114, 140, 666, 1133, 878, 181, 858, 1054, 624, 631, 760, 1210, 544, 291, 262, 267, 386, 1057, 148, 1119, 70, 1035, 604, 1172, 809, 737, 26, 146, 325, 868, 303, 265, 687, 1209, 785, 277, 1177, 476, 21, 54, 811, 445, 502, 305, 958, 788, 1193, 947, 425, 1040, 135, 553, 1051, 978, 884, 765, 166, 924, 363, 572, 778, 813, 786, 220, 1080, 869, 820, 679, 667, 520, 311, 79, 635, 662, 1199, 429, 877, 422, 906, 53, 713, 704, 324, 600, 979, 523, 768, 545, 155, 1179, 431, 154, 959, 489, 388, 437, 495, 1086, 629, 353, 1121, 90, 157, 72, 814, 875, 216, 105, 19, 748, 1006, 885, 227, 1205, 496, 945, 860, 835, 96, 306, 319, 625, 1030, 352, 948, 381, 1202, 1024, 462, 25, 1074, 307, 883, 152, 1136, 240, 761, 827, 652, 1100, 217, 513, 1056, 411, 464, 955, 707, 1003, 1176, 338, 597, 224, 432, 1009, 428, 138, 1019, 98, 733, 444, 856, 867, 612, 436, 567, 1013, 797, 607, 536, 4, 3, 1103, 734, 375, 177, 688, 357, 492, 1122, 486, 894, 392, 834, 1010, 903, 918, 845, 1058, 76, 274, 134, 235, 282, 817, 628, 904, 0, 532, 850, 49, 1081, 1002, 1161, 692, 725, 930, 196, 585, 81, 530, 921, 465, 647, 596, 39, 929, 479, 37, 184, 31, 511, 1085, 935, 278, 649, 575, 446, 824, 771, 656, 439, 720, 538, 117, 840, 1111, 297, 161, 113, 164, 525, 1012, 176, 440, 331, 427, 836, 191, 329, 1093, 1004, 830, 1170, 455, 1077, 210, 749, 1186, 1098, 549, 173, 233, 1123, 818, 1079, 34, 823, 1214, 347, 555, 491, 370, 129, 281, 1190, 807, 468, 715, 539, 832, 1066, 290, 1092, 418, 703, 527, 1160, 655, 449, 1132, 781, 879, 989, 266, 124, 222, 613, 312, 791, 805, 6, 974, 627, 1049, 546, 554, 954, 861, 373, 57, 395, 68, 899, 531, 304, 1041, 158, 1072, 130, 1104, 1061, 60, 360, 279, 497, 258, 478, 719, 48, 675, 908, 82, 964, 1162, 1105, 208, 41, 1053, 1147, 970, 202, 169, 379, 1099, 52, 1151, 1163, 400, 1206, 524, 116, 1015, 941, 775, 548, 1029, 13, 517, 1052, 237, 225, 571, 922, 1, 43, 849, 528, 326, 623, 759, 731, 63, 66, 313, 474, 882, 900, 193, 650, 754, 1137, 203, 159, 626, 724, 833, 981, 1101, 382, 333, 242, 673, 801, 709, 648, 253, 926, 566, 1064, 794, 1022, 18, 986, 99, 482, 695, 1152, 855, 199, 85, 776, 244, 633, 804, 236, 269, 292, 1129, 246, 699, 435, 151, 209, 186, 799, 975, 131, 1158, 746, 192, 711, 640, 366, 38, 102, 574, 484, 310, 1126, 772, 89, 33, 1165, 819, 1141, 1048, 1071, 637, 790, 255, 434, 153, 722, 35, 671, 773, 1078, 1055, 969, 1039, 864, 442, 632, 2, 29, 547, 231, 1175, 562, 590, 276, 910, 880, 782, 473, 215, 1180, 965, 940, 58, 1062, 726, 332, 994, 74, 946, 44, 294, 417, 59, 471, 145, 32, 573, 973, 415, 913, 636, 936, 91, 55, 569, 714, 180, 223, 443, 932, 498, 1021, 212, 314, 777, 447, 295, 1168, 515, 1008, 456, 911, 17, 1171, 712, 630, 621, 533, 200, 309, 80, 745, 886, 341, 350, 951, 405, 394, 391, 691, 1045, 742, 915, 1155, 20, 700, 1088, 1031, 732, 293, 487, 938, 249, 694, 551, 182, 284, 980, 342, 888, 874, 318, 344, 62, 179, 110, 730, 356, 769, 654, 185, 552, 701, 339, 843, 302, 952, 558, 559, 384, 968, 1201, 122, 286, 1090, 8, 170, 862, 1076, 1115, 9, 289, 961, 108, 998, 376, 962, 774, 390, 905, 581, 1159, 852, 1084, 1033, 1025, 1124, 1036, 1113, 839, 568, 420, 1005, 343, 398, 95, 463, 144, 736, 727, 803, 1046, 610, 893, 348, 999, 419, 787, 992, 1135, 322, 218, 426, 254, 107, 349, 816, 1139, 421, 653, 71, 296, 460, 735, 355, 24, 577, 336, 917, 841, 1089, 540, 522, 815, 753, 563, 273, 645, 741, 873, 128, 990, 1120, 1038, 1144, 681, 697, 1020, 583, 614, 189, 1167, 895, 898, 944, 1016, 288, 1195, 925, 248, 11, 789, 1134, 143, 1047, 684, 826, 550, 413, 586, 708, 916, 22, 404, 187, 261, 716, 127, 28, 1068, 663, 123, 739, 1143, 847, 1001, 646, 1082, 721, 949, 396, 214, 389, 901, 56, 1146, 519, 535, 617, 47, 494, 109, 167, 402, 1102, 1208, 508, 977, 685, 393, 320, 1050, 171, 795, 689, 408, 1091, 424, 892, 1128, 450, 454, 792, 639, 300, 1067, 665, 762, 457, 247, 30, 260, 280, 682, 705, 641, 472, 42, 1138, 7, 743, 779, 104, 470, 14, 40, 410, 1178, 565, 505, 1118, 1164, 1187, 570, 710, 175, 115, 256, 1063, 1075, 156, 416, 1183, 358, 848, 259, 996, 966, 593, 1034, 752, 1156, 168, 160, 578, 976, 1109, 943, 1110, 842, 230, 87, 272, 337, 564, 659, 162, 488, 738, 106, 243, 821, 931, 923, 493, 61, 1182, 126, 756, 483, 985, 1212, 521, 920, 580, 1207, 831, 5, 83, 668, 45, 514, 345, 594, 503, 767, 674, 758, 506, 298, 599, 501, 1060, 1131, 601, 854, 67, 371, 957, 397, 409, 73, 770, 611, 723, 953, 620, 316, 229, 991, 579, 103, 507, 1196, 634, 150, 1065, 576, 851, 1173, 197, 1117, 706, 1154, 683, 828, 672, 1194, 909, 1096, 950, 1097, 1142, 346, 983, 121, 664, 340, 120, 939, 467, 213, 141, 323, 100, 287, 275, 226, 1200, 589, 846, 698, 1213, 441, 385, 252, 928, 1157, 984, 693, 857, 1095, 499, 702, 480, 133, 97, 368, 669, 1169, 1042, 902, 810, 1150, 686, 651, 125, 1044, 526, 1188, 798, 268, 988, 481, 896, 780, 354, 660, 194, 1184, 46, 178, 806, 740, 534, 608, 380, 1166, 111, 644, 755, 543, 829, 1037, 264, 451, 407, 15, 1069, 365, 1026, 728, 367]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1663036519262332
the save name prefix for this run is:  chkpt-ID_1663036519262332_tag_Ablation-job-blacklist-npp
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 773
rank avg (pred): 0.569 +- 0.003
mrr vals (pred, true): 0.013, 0.055
batch losses (mrrl, rdl): 0.0, 0.0002003012

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 616
rank avg (pred): 0.435 +- 0.253
mrr vals (pred, true): 0.154, 0.046
batch losses (mrrl, rdl): 0.0, 1.11048e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 962
rank avg (pred): 0.430 +- 0.295
mrr vals (pred, true): 0.213, 0.047
batch losses (mrrl, rdl): 0.0, 1.82636e-05

Epoch over!
epoch time: 13.216

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1021
rank avg (pred): 0.317 +- 0.234
mrr vals (pred, true): 0.234, 0.141
batch losses (mrrl, rdl): 0.0, 9.33406e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 616
rank avg (pred): 0.454 +- 0.278
mrr vals (pred, true): 0.156, 0.046
batch losses (mrrl, rdl): 0.0, 4.7629e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 273
rank avg (pred): 0.062 +- 0.098
mrr vals (pred, true): 0.383, 0.547
batch losses (mrrl, rdl): 0.0, 1.59649e-05

Epoch over!
epoch time: 12.377

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 499
rank avg (pred): 0.218 +- 0.208
mrr vals (pred, true): 0.236, 0.212
batch losses (mrrl, rdl): 0.0, 9.7332e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 292
rank avg (pred): 0.036 +- 0.061
mrr vals (pred, true): 0.435, 0.544
batch losses (mrrl, rdl): 0.0, 2.35e-08

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 505
rank avg (pred): 0.254 +- 0.253
mrr vals (pred, true): 0.227, 0.203
batch losses (mrrl, rdl): 0.0, 1.94507e-05

Epoch over!
epoch time: 12.329

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1143
rank avg (pred): 0.216 +- 0.212
mrr vals (pred, true): 0.226, 0.139
batch losses (mrrl, rdl): 0.0, 3.80365e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 809
rank avg (pred): 0.473 +- 0.299
mrr vals (pred, true): 0.137, 0.051
batch losses (mrrl, rdl): 0.0, 1.79514e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 830
rank avg (pred): 0.114 +- 0.162
mrr vals (pred, true): 0.316, 0.513
batch losses (mrrl, rdl): 0.0, 0.0001456066

Epoch over!
epoch time: 12.241

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 742
rank avg (pred): 0.108 +- 0.162
mrr vals (pred, true): 0.318, 0.359
batch losses (mrrl, rdl): 0.0, 1.39557e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1005
rank avg (pred): 0.364 +- 0.281
mrr vals (pred, true): 0.181, 0.092
batch losses (mrrl, rdl): 0.0, 5.02882e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1140
rank avg (pred): 0.154 +- 0.200
mrr vals (pred, true): 0.252, 0.242
batch losses (mrrl, rdl): 0.0, 9.0512e-06

Epoch over!
epoch time: 12.152

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 941
rank avg (pred): 0.492 +- 0.289
mrr vals (pred, true): 0.134, 0.016
batch losses (mrrl, rdl): 0.070417814, 0.0011954517

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1100
rank avg (pred): 0.360 +- 0.197
mrr vals (pred, true): 0.088, 0.102
batch losses (mrrl, rdl): 0.0021352025, 8.45177e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 73
rank avg (pred): 0.174 +- 0.221
mrr vals (pred, true): 0.537, 0.521
batch losses (mrrl, rdl): 0.0024773751, 0.000524304

Epoch over!
epoch time: 12.412

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1117
rank avg (pred): 0.398 +- 0.191
mrr vals (pred, true): 0.068, 0.050
batch losses (mrrl, rdl): 0.0033051569, 2.27918e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1045
rank avg (pred): 0.374 +- 0.185
mrr vals (pred, true): 0.076, 0.049
batch losses (mrrl, rdl): 0.0068043349, 8.66818e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 289
rank avg (pred): 0.187 +- 0.224
mrr vals (pred, true): 0.544, 0.550
batch losses (mrrl, rdl): 0.0004037829, 0.0006476812

Epoch over!
epoch time: 12.425

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 780
rank avg (pred): 0.404 +- 0.167
mrr vals (pred, true): 0.072, 0.048
batch losses (mrrl, rdl): 0.0046313796, 0.0003273204

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 899
rank avg (pred): 0.399 +- 0.148
mrr vals (pred, true): 0.059, 0.038
batch losses (mrrl, rdl): 0.0007699663, 7.58447e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 976
rank avg (pred): 0.178 +- 0.219
mrr vals (pred, true): 0.567, 0.545
batch losses (mrrl, rdl): 0.0045717862, 0.0006032445

Epoch over!
epoch time: 12.241

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 575
rank avg (pred): 0.426 +- 0.163
mrr vals (pred, true): 0.059, 0.037
batch losses (mrrl, rdl): 0.000867978, 0.0001386873

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 514
rank avg (pred): 0.365 +- 0.165
mrr vals (pred, true): 0.082, 0.072
batch losses (mrrl, rdl): 0.0099548055, 7.76646e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 948
rank avg (pred): 0.484 +- 0.123
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.0001980009, 9.91205e-05

Epoch over!
epoch time: 13.558

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 277
rank avg (pred): 0.204 +- 0.227
mrr vals (pred, true): 0.529, 0.554
batch losses (mrrl, rdl): 0.0060158605, 0.0007909597

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 537
rank avg (pred): 0.370 +- 0.158
mrr vals (pred, true): 0.080, 0.049
batch losses (mrrl, rdl): 0.009247846, 0.0001647562

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1120
rank avg (pred): 0.410 +- 0.175
mrr vals (pred, true): 0.066, 0.048
batch losses (mrrl, rdl): 0.0025253161, 4.14675e-05

Epoch over!
epoch time: 13.257

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 257
rank avg (pred): 0.202 +- 0.225
mrr vals (pred, true): 0.538, 0.561
batch losses (mrrl, rdl): 0.0055494667, 0.0008076607

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 817
rank avg (pred): 0.269 +- 0.198
mrr vals (pred, true): 0.158, 0.065
batch losses (mrrl, rdl): 0.1165359169, 3.76374e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 952
rank avg (pred): 0.664 +- 0.173
mrr vals (pred, true): 0.024, 0.045
batch losses (mrrl, rdl): 0.0065030269, 0.0008692923

Epoch over!
epoch time: 12.295

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1049
rank avg (pred): 0.338 +- 0.177
mrr vals (pred, true): 0.101, 0.059
batch losses (mrrl, rdl): 0.0258427635, 0.000114466

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 932
rank avg (pred): 0.453 +- 0.096
mrr vals (pred, true): 0.039, 0.015
batch losses (mrrl, rdl): 0.0012169245, 0.0020395927

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 57
rank avg (pred): 0.204 +- 0.220
mrr vals (pred, true): 0.519, 0.508
batch losses (mrrl, rdl): 0.0012083041, 0.0006857708

Epoch over!
epoch time: 12.321

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 872
rank avg (pred): 0.569 +- 0.188
mrr vals (pred, true): 0.050, 0.046
batch losses (mrrl, rdl): 8.886e-07, 0.000335744

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 609
rank avg (pred): 0.489 +- 0.163
mrr vals (pred, true): 0.056, 0.042
batch losses (mrrl, rdl): 0.000315604, 4.3008e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 909
rank avg (pred): 0.418 +- 0.119
mrr vals (pred, true): 0.057, 0.074
batch losses (mrrl, rdl): 0.0004334611, 0.0001117515

Epoch over!
epoch time: 12.411

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 807
rank avg (pred): 0.450 +- 0.140
mrr vals (pred, true): 0.054, 0.044
batch losses (mrrl, rdl): 0.0001765199, 3.99182e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 859
rank avg (pred): 0.395 +- 0.156
mrr vals (pred, true): 0.070, 0.038
batch losses (mrrl, rdl): 0.0041081617, 0.0001834703

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 950
rank avg (pred): 0.612 +- 0.168
mrr vals (pred, true): 0.043, 0.047
batch losses (mrrl, rdl): 0.0005397516, 0.0007179116

Epoch over!
epoch time: 12.386

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 227
rank avg (pred): 0.420 +- 0.164
mrr vals (pred, true): 0.063, 0.053
batch losses (mrrl, rdl): 0.0018096385, 3.15743e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1033
rank avg (pred): 0.370 +- 0.140
mrr vals (pred, true): 0.072, 0.050
batch losses (mrrl, rdl): 0.0048758732, 8.69259e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 431
rank avg (pred): 0.353 +- 0.167
mrr vals (pred, true): 0.091, 0.042
batch losses (mrrl, rdl): 0.0167471766, 0.0002035571

Epoch over!
epoch time: 12.255

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.422 +- 0.162
mrr vals (pred, true): 0.072, 0.051

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.04761 	 0.01547 	 m..s
    5 	     1 	 0.04760 	 0.01638 	 m..s
    6 	     2 	 0.04761 	 0.01759 	 m..s
    4 	     3 	 0.04755 	 0.01975 	 ~...
   27 	     4 	 0.05893 	 0.03391 	 ~...
   25 	     5 	 0.05865 	 0.03408 	 ~...
   28 	     6 	 0.05971 	 0.03457 	 ~...
   14 	     7 	 0.05461 	 0.03707 	 ~...
   59 	     8 	 0.07174 	 0.03798 	 m..s
   18 	     9 	 0.05575 	 0.03855 	 ~...
   69 	    10 	 0.07438 	 0.03938 	 m..s
   41 	    11 	 0.06520 	 0.04036 	 ~...
    0 	    12 	 0.04744 	 0.04100 	 ~...
   11 	    13 	 0.05405 	 0.04161 	 ~...
   25 	    14 	 0.05865 	 0.04200 	 ~...
   20 	    15 	 0.05617 	 0.04226 	 ~...
   33 	    16 	 0.06245 	 0.04275 	 ~...
    3 	    17 	 0.04752 	 0.04377 	 ~...
   63 	    18 	 0.07272 	 0.04377 	 ~...
   21 	    19 	 0.05707 	 0.04411 	 ~...
   17 	    20 	 0.05522 	 0.04463 	 ~...
   35 	    21 	 0.06323 	 0.04468 	 ~...
    9 	    22 	 0.05391 	 0.04479 	 ~...
   49 	    23 	 0.06809 	 0.04483 	 ~...
   16 	    24 	 0.05516 	 0.04498 	 ~...
   15 	    25 	 0.05512 	 0.04554 	 ~...
   54 	    26 	 0.06963 	 0.04587 	 ~...
   66 	    27 	 0.07337 	 0.04610 	 ~...
   34 	    28 	 0.06263 	 0.04631 	 ~...
   49 	    29 	 0.06809 	 0.04635 	 ~...
   44 	    30 	 0.06599 	 0.04637 	 ~...
   77 	    31 	 0.07645 	 0.04649 	 ~...
   30 	    32 	 0.06098 	 0.04659 	 ~...
   10 	    33 	 0.05401 	 0.04665 	 ~...
   12 	    34 	 0.05439 	 0.04667 	 ~...
    1 	    35 	 0.04746 	 0.04676 	 ~...
   57 	    36 	 0.07149 	 0.04682 	 ~...
   36 	    37 	 0.06325 	 0.04683 	 ~...
   48 	    38 	 0.06791 	 0.04718 	 ~...
   89 	    39 	 0.12875 	 0.04730 	 m..s
   66 	    40 	 0.07337 	 0.04735 	 ~...
   47 	    41 	 0.06671 	 0.04750 	 ~...
    8 	    42 	 0.05357 	 0.04787 	 ~...
   40 	    43 	 0.06502 	 0.04790 	 ~...
   79 	    44 	 0.07770 	 0.04832 	 ~...
   83 	    45 	 0.09036 	 0.04862 	 m..s
   65 	    46 	 0.07318 	 0.04898 	 ~...
   19 	    47 	 0.05589 	 0.04949 	 ~...
   39 	    48 	 0.06486 	 0.04972 	 ~...
   29 	    49 	 0.06059 	 0.05025 	 ~...
   31 	    50 	 0.06113 	 0.05033 	 ~...
   62 	    51 	 0.07259 	 0.05082 	 ~...
    2 	    52 	 0.04747 	 0.05096 	 ~...
   60 	    53 	 0.07175 	 0.05108 	 ~...
   24 	    54 	 0.05859 	 0.05219 	 ~...
   81 	    55 	 0.08213 	 0.05271 	 ~...
   86 	    56 	 0.09716 	 0.05362 	 m..s
   42 	    57 	 0.06545 	 0.05365 	 ~...
   54 	    58 	 0.06963 	 0.05412 	 ~...
   13 	    59 	 0.05449 	 0.05439 	 ~...
   72 	    60 	 0.07502 	 0.05593 	 ~...
   61 	    61 	 0.07215 	 0.05631 	 ~...
   22 	    62 	 0.05765 	 0.05681 	 ~...
   23 	    63 	 0.05788 	 0.05719 	 ~...
   64 	    64 	 0.07277 	 0.05734 	 ~...
   32 	    65 	 0.06242 	 0.05890 	 ~...
   56 	    66 	 0.07123 	 0.06022 	 ~...
   73 	    67 	 0.07535 	 0.06245 	 ~...
   76 	    68 	 0.07630 	 0.07261 	 ~...
   45 	    69 	 0.06607 	 0.07365 	 ~...
   51 	    70 	 0.06901 	 0.08288 	 ~...
   74 	    71 	 0.07579 	 0.08295 	 ~...
   75 	    72 	 0.07605 	 0.08345 	 ~...
   88 	    73 	 0.11945 	 0.08456 	 m..s
   80 	    74 	 0.07928 	 0.09529 	 ~...
   68 	    75 	 0.07410 	 0.09859 	 ~...
   82 	    76 	 0.08964 	 0.10034 	 ~...
   85 	    77 	 0.09222 	 0.10042 	 ~...
   58 	    78 	 0.07152 	 0.10109 	 ~...
   43 	    79 	 0.06590 	 0.10817 	 m..s
   38 	    80 	 0.06351 	 0.11028 	 m..s
   70 	    81 	 0.07452 	 0.11590 	 m..s
   51 	    82 	 0.06901 	 0.12139 	 m..s
   87 	    83 	 0.10257 	 0.12203 	 ~...
   78 	    84 	 0.07707 	 0.12319 	 m..s
   46 	    85 	 0.06648 	 0.12609 	 m..s
   84 	    86 	 0.09038 	 0.12739 	 m..s
   37 	    87 	 0.06337 	 0.13098 	 m..s
   71 	    88 	 0.07482 	 0.13195 	 m..s
   91 	    89 	 0.13858 	 0.13415 	 ~...
   91 	    90 	 0.13858 	 0.14093 	 ~...
   53 	    91 	 0.06920 	 0.14099 	 m..s
   90 	    92 	 0.13808 	 0.14525 	 ~...
   95 	    93 	 0.18021 	 0.16880 	 ~...
   93 	    94 	 0.17281 	 0.20213 	 ~...
   96 	    95 	 0.20781 	 0.21489 	 ~...
   97 	    96 	 0.24328 	 0.23905 	 ~...
   99 	    97 	 0.31117 	 0.24983 	 m..s
   98 	    98 	 0.25848 	 0.30827 	 m..s
   94 	    99 	 0.17561 	 0.36843 	 MISS
  101 	   100 	 0.31872 	 0.39802 	 m..s
  102 	   101 	 0.48944 	 0.50619 	 ~...
  103 	   102 	 0.49150 	 0.51200 	 ~...
  100 	   103 	 0.31241 	 0.51622 	 MISS
  119 	   104 	 0.53004 	 0.52673 	 ~...
  114 	   105 	 0.50319 	 0.52858 	 ~...
  120 	   106 	 0.53094 	 0.53525 	 ~...
  113 	   107 	 0.50315 	 0.54355 	 m..s
  116 	   108 	 0.51006 	 0.54506 	 m..s
  117 	   109 	 0.52118 	 0.54524 	 ~...
  109 	   110 	 0.49975 	 0.54676 	 m..s
  115 	   111 	 0.50979 	 0.54712 	 m..s
  104 	   112 	 0.49474 	 0.54784 	 m..s
  107 	   113 	 0.49603 	 0.54813 	 m..s
  111 	   114 	 0.50258 	 0.54879 	 m..s
  118 	   115 	 0.52392 	 0.54910 	 ~...
  108 	   116 	 0.49821 	 0.54951 	 m..s
  105 	   117 	 0.49570 	 0.55053 	 m..s
  106 	   118 	 0.49575 	 0.55283 	 m..s
  110 	   119 	 0.50007 	 0.55519 	 m..s
  111 	   120 	 0.50258 	 0.55544 	 m..s
==========================================
r_mrr = 0.9809723496437073
r2_mrr = 0.9524543881416321
spearmanr_mrr@5 = 0.9389747977256775
spearmanr_mrr@10 = 0.9649216532707214
spearmanr_mrr@50 = 0.9913312792778015
spearmanr_mrr@100 = 0.9920362830162048
spearmanr_mrr@All = 0.9923819303512573
==========================================
test time: 0.409
Done Testing dataset UMLS
total time taken: 194.10541105270386
training time taken: 188.3459985256195
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9810)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9525)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9390)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9649)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9913)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9920)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9924)}}, 'test_loss': {'DistMult': {'UMLS': 1.9093722559628077}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'npp'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-margin
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 188013678276971
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [448, 1140, 706, 938, 920, 713, 512, 955, 343, 247, 251, 650, 624, 1167, 824, 73, 1072, 1052, 315, 31, 1143, 362, 291, 398, 881, 516, 941, 1064, 265, 503, 604, 1028, 993, 359, 583, 1191, 616, 862, 451, 437, 1026, 373, 552, 1208, 934, 301, 915, 555, 827, 102, 111, 490, 456, 261, 755, 727, 271, 211, 921, 811, 59, 1007, 136, 319, 159, 1061, 171, 1042, 815, 368, 700, 514, 417, 699, 903, 104, 355, 491, 878, 345, 679, 924, 865, 684, 848, 1090, 1154, 1111, 868, 329, 1119, 246, 668, 1151, 462, 565, 1011, 392, 1130, 507, 654, 595, 113, 310, 496, 334, 1144, 284, 859, 909, 985, 26, 596, 901, 972, 116, 834, 347, 639, 1006, 64]
valid_ids (0): []
train_ids (1094): [734, 923, 820, 61, 479, 888, 991, 1192, 1084, 775, 959, 344, 433, 60, 976, 761, 337, 214, 85, 235, 103, 78, 790, 300, 1093, 649, 593, 370, 831, 286, 193, 431, 253, 1060, 48, 954, 527, 133, 248, 1166, 323, 629, 348, 690, 902, 95, 515, 1160, 488, 80, 270, 866, 67, 203, 772, 774, 965, 1134, 240, 139, 1145, 622, 471, 958, 267, 852, 750, 625, 1066, 850, 269, 742, 469, 1127, 30, 293, 13, 145, 1124, 427, 1185, 784, 91, 208, 1059, 541, 27, 961, 661, 819, 243, 832, 298, 452, 146, 292, 502, 580, 435, 1152, 495, 169, 33, 705, 289, 328, 696, 330, 672, 776, 358, 646, 52, 242, 1147, 173, 928, 641, 945, 275, 758, 425, 288, 135, 125, 741, 238, 188, 245, 636, 314, 632, 1087, 140, 1039, 974, 953, 789, 485, 4, 1057, 230, 695, 989, 258, 688, 818, 539, 1016, 423, 562, 964, 383, 588, 1073, 1126, 760, 408, 781, 434, 1103, 224, 465, 950, 1034, 1139, 841, 42, 600, 735, 528, 1138, 204, 978, 25, 709, 382, 152, 18, 1010, 1069, 296, 1000, 87, 380, 88, 1070, 256, 883, 162, 384, 1027, 262, 454, 131, 914, 297, 336, 614, 376, 333, 342, 560, 1190, 1189, 49, 36, 659, 160, 788, 179, 510, 890, 1032, 470, 353, 405, 917, 35, 1, 257, 759, 43, 1180, 57, 17, 123, 803, 341, 1047, 872, 1079, 83, 157, 24, 6, 797, 704, 611, 712, 178, 1009, 1213, 281, 693, 236, 806, 810, 143, 0, 129, 1101, 692, 112, 108, 906, 586, 263, 79, 1102, 655, 436, 896, 1062, 278, 364, 187, 498, 1086, 874, 1125, 899, 780, 725, 466, 678, 927, 299, 1179, 174, 844, 324, 77, 1038, 482, 508, 190, 519, 846, 607, 545, 69, 394, 82, 717, 168, 1211, 99, 305, 295, 898, 971, 241, 1053, 492, 949, 664, 254, 418, 463, 520, 1163, 833, 402, 1077, 894, 880, 737, 363, 182, 1020, 1155, 445, 326, 547, 1022, 432, 575, 1033, 360, 925, 1193, 396, 533, 406, 266, 645, 592, 707, 480, 656, 130, 1178, 1108, 674, 826, 1168, 536, 598, 980, 773, 889, 745, 449, 115, 651, 316, 748, 724, 209, 1097, 1099, 986, 711, 200, 121, 181, 340, 829, 54, 155, 177, 1162, 642, 792, 893, 46, 733, 926, 671, 122, 335, 1063, 919, 802, 609, 682, 1199, 1159, 1201, 777, 1049, 1068, 578, 809, 1023, 442, 93, 1210, 500, 723, 569, 1091, 369, 677, 1196, 887, 814, 239, 106, 877, 752, 601, 97, 387, 791, 1209, 1116, 1114, 793, 997, 1165, 1177, 1109, 252, 105, 984, 150, 1129, 154, 134, 1078, 1107, 447, 746, 215, 443, 56, 937, 72, 1149, 1133, 557, 356, 892, 415, 44, 1045, 666, 863, 107, 414, 631, 786, 51, 977, 1182, 563, 210, 956, 232, 703, 313, 932, 566, 764, 633, 1141, 823, 697, 783, 626, 897, 474, 172, 517, 627, 1195, 378, 464, 982, 47, 948, 2, 184, 762, 216, 1002, 572, 352, 1054, 778, 548, 207, 571, 873, 610, 1089, 1104, 603, 581, 264, 75, 272, 867, 1025, 732, 426, 176, 553, 990, 65, 992, 701, 1004, 579, 768, 605, 1136, 1171, 5, 259, 165, 274, 838, 148, 416, 501, 412, 973, 331, 311, 694, 837, 618, 744, 141, 255, 233, 619, 1008, 879, 870, 404, 225, 635, 975, 8, 658, 568, 615, 506, 119, 804, 419, 166, 1001, 808, 205, 836, 497, 101, 570, 918, 1187, 484, 96, 62, 967, 1065, 726, 722, 461, 933, 41, 429, 1153, 637, 845, 856, 98, 529, 321, 673, 411, 339, 1029, 683, 486, 1169, 90, 186, 161, 1015, 936, 453, 1115, 317, 1112, 390, 1118, 280, 708, 1174, 822, 294, 439, 621, 1113, 1173, 460, 499, 74, 351, 638, 11, 1048, 306, 620, 1212, 189, 231, 170, 1003, 1202, 63, 195, 250, 202, 285, 374, 10, 222, 21, 652, 851, 58, 312, 244, 676, 930, 180, 81, 308, 84, 935, 276, 350, 1088, 37, 675, 559, 191, 957, 891, 441, 194, 736, 1204, 346, 494, 1146, 669, 908, 1206, 1161, 1095, 458, 1021, 381, 869, 361, 142, 389, 302, 132, 522, 393, 1150, 147, 525, 151, 770, 1110, 540, 640, 28, 662, 318, 840, 537, 385, 686, 564, 630, 531, 1158, 785, 756, 857, 943, 354, 218, 357, 794, 535, 660, 719, 32, 1198, 981, 1157, 422, 825, 446, 50, 283, 807, 511, 468, 237, 249, 749, 798, 963, 183, 1050, 307, 648, 1031, 766, 7, 667, 574, 573, 864, 1067, 279, 89, 1131, 407, 118, 994, 738, 1044, 1075, 542, 1186, 110, 634, 303, 1071, 613, 702, 413, 591, 226, 504, 623, 397, 403, 916, 39, 710, 117, 1024, 830, 680, 922, 799, 821, 1098, 728, 584, 729, 521, 1156, 260, 332, 475, 1184, 558, 175, 1203, 327, 1043, 290, 585, 375, 716, 409, 1017, 787, 951, 1056, 721, 444, 489, 472, 45, 577, 450, 907, 66, 731, 1018, 1005, 886, 1122, 718, 1040, 969, 882, 1121, 29, 1058, 944, 860, 372, 582, 968, 1092, 526, 843, 282, 338, 400, 1170, 939, 952, 1120, 40, 1105, 556, 1012, 228, 68, 534, 156, 192, 1080, 754, 608, 1200, 153, 587, 518, 430, 128, 467, 219, 395, 979, 23, 849, 998, 1106, 277, 962, 1037, 3, 1181, 1013, 532, 871, 483, 670, 858, 1148, 628, 320, 144, 388, 590, 199, 1081, 367, 795, 94, 842, 647, 1100, 1137, 1096, 322, 1172, 1051, 455, 365, 796, 1117, 853, 546, 38, 15, 86, 714, 904, 206, 905, 910, 687, 549, 19, 550, 1183, 987, 782, 771, 665, 1197, 597, 911, 401, 1135, 213, 1175, 988, 653, 900, 1142, 109, 481, 767, 715, 421, 551, 440, 828, 942, 966, 371, 124, 420, 201, 14, 554, 854, 685, 1164, 876, 234, 839, 273, 995, 1035, 304, 544, 1094, 92, 120, 386, 509, 126, 895, 543, 763, 524, 602, 999, 720, 220, 1207, 55, 366, 138, 801, 813, 1019, 114, 1214, 1014, 391, 309, 164, 473, 730, 1128, 855, 1194, 127, 612, 599, 197, 561, 1055, 698, 1082, 399, 487, 1188, 1041, 478, 817, 983, 76, 884, 223, 753, 1205, 816, 137, 765, 530, 589, 428, 457, 523, 960, 71, 513, 812, 185, 1083, 739, 377, 349, 221, 158, 912, 198, 663, 1176, 740, 149, 594, 1036, 1076, 644, 459, 847, 379, 16, 70, 913, 1123, 212, 217, 227, 196, 410, 163, 885, 996, 606, 493, 1132, 779, 576, 691, 9, 970, 20, 538, 757, 34, 931, 946, 229, 438, 1085, 167, 929, 805, 940, 617, 800, 53, 769, 643, 22, 751, 477, 875, 743, 1074, 747, 681, 424, 689, 268, 947, 1046, 505, 861, 657, 100, 12, 1030, 476, 287, 567, 325, 835]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7414060921254504
the save name prefix for this run is:  chkpt-ID_7414060921254504_tag_Ablation-job-blacklist-margin
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 401
rank avg (pred): 0.558 +- 0.004
mrr vals (pred, true): 0.013, 0.126
batch losses (mrrl, rdl): 0.0, 0.0016581899

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 200
rank avg (pred): 0.357 +- 0.033
mrr vals (pred, true): 0.021, 0.051
batch losses (mrrl, rdl): 0.0, 0.0002827948

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1038
rank avg (pred): 0.370 +- 0.235
mrr vals (pred, true): 0.149, 0.042
batch losses (mrrl, rdl): 0.0, 8.02942e-05

Epoch over!
epoch time: 12.277

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1053
rank avg (pred): 0.035 +- 0.025
mrr vals (pred, true): 0.361, 0.543
batch losses (mrrl, rdl): 0.0, 2.14e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 463
rank avg (pred): 0.360 +- 0.251
mrr vals (pred, true): 0.188, 0.042
batch losses (mrrl, rdl): 0.0, 5.42981e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 137
rank avg (pred): 0.357 +- 0.260
mrr vals (pred, true): 0.221, 0.090
batch losses (mrrl, rdl): 0.0, 6.55212e-05

Epoch over!
epoch time: 12.254

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 181
rank avg (pred): 0.385 +- 0.261
mrr vals (pred, true): 0.183, 0.051
batch losses (mrrl, rdl): 0.0, 4.49312e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 617
rank avg (pred): 0.415 +- 0.285
mrr vals (pred, true): 0.212, 0.043
batch losses (mrrl, rdl): 0.0, 3.14732e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 435
rank avg (pred): 0.356 +- 0.265
mrr vals (pred, true): 0.241, 0.054
batch losses (mrrl, rdl): 0.0, 4.22406e-05

Epoch over!
epoch time: 12.228

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 671
rank avg (pred): 0.420 +- 0.279
mrr vals (pred, true): 0.187, 0.045
batch losses (mrrl, rdl): 0.0, 7.576e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 476
rank avg (pred): 0.363 +- 0.260
mrr vals (pred, true): 0.213, 0.048
batch losses (mrrl, rdl): 0.0, 0.000103988

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1197
rank avg (pred): 0.432 +- 0.280
mrr vals (pred, true): 0.200, 0.045
batch losses (mrrl, rdl): 0.0, 1.68922e-05

Epoch over!
epoch time: 12.39

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 761
rank avg (pred): 0.451 +- 0.284
mrr vals (pred, true): 0.178, 0.036
batch losses (mrrl, rdl): 0.0, 5.08639e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 923
rank avg (pred): 0.518 +- 0.292
mrr vals (pred, true): 0.132, 0.018
batch losses (mrrl, rdl): 0.0, 0.0009423083

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 340
rank avg (pred): 0.356 +- 0.261
mrr vals (pred, true): 0.241, 0.134
batch losses (mrrl, rdl): 0.0, 0.0002475682

Epoch over!
epoch time: 12.199

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 210
rank avg (pred): 0.351 +- 0.267
mrr vals (pred, true): 0.259, 0.046
batch losses (mrrl, rdl): 0.4360133708, 7.4962e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1043
rank avg (pred): 0.293 +- 0.125
mrr vals (pred, true): 0.066, 0.051
batch losses (mrrl, rdl): 0.0027081799, 0.0003831392

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1156
rank avg (pred): 0.354 +- 0.203
mrr vals (pred, true): 0.115, 0.132
batch losses (mrrl, rdl): 0.003114268, 0.000198146

Epoch over!
epoch time: 12.516

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 553
rank avg (pred): 0.350 +- 0.164
mrr vals (pred, true): 0.088, 0.063
batch losses (mrrl, rdl): 0.0142077953, 0.0001483232

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 542
rank avg (pred): 0.355 +- 0.160
mrr vals (pred, true): 0.080, 0.124
batch losses (mrrl, rdl): 0.0193097703, 9.072e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 991
rank avg (pred): 0.012 +- 0.011
mrr vals (pred, true): 0.554, 0.536
batch losses (mrrl, rdl): 0.0029863901, 1.07052e-05

Epoch over!
epoch time: 12.489

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 82
rank avg (pred): 0.353 +- 0.164
mrr vals (pred, true): 0.089, 0.080
batch losses (mrrl, rdl): 0.0151884826, 2.91504e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 870
rank avg (pred): 0.316 +- 0.146
mrr vals (pred, true): 0.042, 0.049
batch losses (mrrl, rdl): 0.0006970035, 0.0003353382

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 524
rank avg (pred): 0.359 +- 0.172
mrr vals (pred, true): 0.095, 0.105
batch losses (mrrl, rdl): 0.0010741103, 7.16507e-05

Epoch over!
epoch time: 12.388

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1062
rank avg (pred): 0.011 +- 0.010
mrr vals (pred, true): 0.566, 0.536
batch losses (mrrl, rdl): 0.0089065991, 1.36958e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 773
rank avg (pred): 0.291 +- 0.173
mrr vals (pred, true): 0.049, 0.055
batch losses (mrrl, rdl): 1.80886e-05, 0.0010688745

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 289
rank avg (pred): 0.014 +- 0.013
mrr vals (pred, true): 0.543, 0.550
batch losses (mrrl, rdl): 0.000521581, 1.19881e-05

Epoch over!
epoch time: 12.698

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 847
rank avg (pred): 0.294 +- 0.177
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 2.10159e-05, 0.0004938116

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 550
rank avg (pred): 0.381 +- 0.180
mrr vals (pred, true): 0.087, 0.062
batch losses (mrrl, rdl): 0.013550967, 6.90495e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 617
rank avg (pred): 0.333 +- 0.251
mrr vals (pred, true): 0.053, 0.043
batch losses (mrrl, rdl): 8.69814e-05, 0.000320369

Epoch over!
epoch time: 12.859

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 854
rank avg (pred): 0.304 +- 0.255
mrr vals (pred, true): 0.074, 0.114
batch losses (mrrl, rdl): 0.0160130076, 0.0004456133

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 809
rank avg (pred): 0.342 +- 0.261
mrr vals (pred, true): 0.059, 0.051
batch losses (mrrl, rdl): 0.0007905375, 0.0002123793

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 91
rank avg (pred): 0.353 +- 0.138
mrr vals (pred, true): 0.065, 0.103
batch losses (mrrl, rdl): 0.0139953578, 8.17269e-05

Epoch over!
epoch time: 12.997

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 505
rank avg (pred): 0.299 +- 0.220
mrr vals (pred, true): 0.209, 0.203
batch losses (mrrl, rdl): 0.0003387421, 0.0001565047

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 504
rank avg (pred): 0.283 +- 0.222
mrr vals (pred, true): 0.226, 0.202
batch losses (mrrl, rdl): 0.0055600232, 0.0001279076

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 69
rank avg (pred): 0.016 +- 0.015
mrr vals (pred, true): 0.548, 0.510
batch losses (mrrl, rdl): 0.014094498, 1.94252e-05

Epoch over!
epoch time: 12.731

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 996
rank avg (pred): 0.018 +- 0.017
mrr vals (pred, true): 0.539, 0.557
batch losses (mrrl, rdl): 0.0034966562, 5.1866e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 694
rank avg (pred): 0.415 +- 0.276
mrr vals (pred, true): 0.044, 0.052
batch losses (mrrl, rdl): 0.0003573189, 3.0331e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1067
rank avg (pred): 0.020 +- 0.020
mrr vals (pred, true): 0.537, 0.545
batch losses (mrrl, rdl): 0.0005985534, 6.1425e-06

Epoch over!
epoch time: 14.174

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 187
rank avg (pred): 0.312 +- 0.171
mrr vals (pred, true): 0.060, 0.052
batch losses (mrrl, rdl): 0.0010639179, 0.0001980399

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1050
rank avg (pred): 0.257 +- 0.166
mrr vals (pred, true): 0.099, 0.042
batch losses (mrrl, rdl): 0.0242036674, 0.0007870367

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 676
rank avg (pred): 0.364 +- 0.235
mrr vals (pred, true): 0.050, 0.047
batch losses (mrrl, rdl): 6.73e-07, 6.50788e-05

Epoch over!
epoch time: 12.405

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1195
rank avg (pred): 0.417 +- 0.285
mrr vals (pred, true): 0.045, 0.047
batch losses (mrrl, rdl): 0.0002612173, 7.052e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 206
rank avg (pred): 0.321 +- 0.150
mrr vals (pred, true): 0.071, 0.049
batch losses (mrrl, rdl): 0.0042325729, 0.0002787759

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 306
rank avg (pred): 0.025 +- 0.025
mrr vals (pred, true): 0.531, 0.532
batch losses (mrrl, rdl): 2.17802e-05, 7.5489e-06

Epoch over!
epoch time: 13.468

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.299 +- 0.159
mrr vals (pred, true): 0.075, 0.056

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   29 	     0 	 0.05365 	 0.01574 	 m..s
   33 	     1 	 0.06010 	 0.01602 	 m..s
   34 	     2 	 0.06186 	 0.01638 	 m..s
    6 	     3 	 0.05034 	 0.01662 	 m..s
   12 	     4 	 0.05065 	 0.01923 	 m..s
   28 	     5 	 0.05353 	 0.02085 	 m..s
   24 	     6 	 0.05200 	 0.03707 	 ~...
    7 	     7 	 0.05035 	 0.03787 	 ~...
   27 	     8 	 0.05337 	 0.03841 	 ~...
   21 	     9 	 0.05176 	 0.03907 	 ~...
    5 	    10 	 0.05023 	 0.04008 	 ~...
   24 	    11 	 0.05200 	 0.04074 	 ~...
   35 	    12 	 0.06523 	 0.04135 	 ~...
    1 	    13 	 0.04718 	 0.04271 	 ~...
   32 	    14 	 0.05853 	 0.04274 	 ~...
   77 	    15 	 0.08390 	 0.04299 	 m..s
   26 	    16 	 0.05207 	 0.04304 	 ~...
   19 	    17 	 0.05160 	 0.04345 	 ~...
   13 	    18 	 0.05074 	 0.04357 	 ~...
    0 	    19 	 0.04579 	 0.04402 	 ~...
   15 	    20 	 0.05139 	 0.04469 	 ~...
   68 	    21 	 0.07647 	 0.04506 	 m..s
   82 	    22 	 0.10293 	 0.04520 	 m..s
    4 	    23 	 0.04912 	 0.04527 	 ~...
   16 	    24 	 0.05158 	 0.04581 	 ~...
   11 	    25 	 0.05051 	 0.04583 	 ~...
    2 	    26 	 0.04750 	 0.04598 	 ~...
    9 	    27 	 0.05049 	 0.04625 	 ~...
   66 	    28 	 0.07535 	 0.04631 	 ~...
   18 	    29 	 0.05160 	 0.04648 	 ~...
   72 	    30 	 0.07847 	 0.04666 	 m..s
   36 	    31 	 0.06544 	 0.04698 	 ~...
   43 	    32 	 0.06871 	 0.04700 	 ~...
   30 	    33 	 0.05412 	 0.04741 	 ~...
   78 	    34 	 0.08755 	 0.04779 	 m..s
   57 	    35 	 0.07294 	 0.04794 	 ~...
   70 	    36 	 0.07752 	 0.04870 	 ~...
   16 	    37 	 0.05158 	 0.04877 	 ~...
   23 	    38 	 0.05180 	 0.04932 	 ~...
   10 	    39 	 0.05049 	 0.04983 	 ~...
    8 	    40 	 0.05048 	 0.05108 	 ~...
   54 	    41 	 0.07206 	 0.05161 	 ~...
   14 	    42 	 0.05129 	 0.05175 	 ~...
   80 	    43 	 0.09636 	 0.05268 	 m..s
   69 	    44 	 0.07694 	 0.05275 	 ~...
   62 	    45 	 0.07456 	 0.05366 	 ~...
   60 	    46 	 0.07370 	 0.05467 	 ~...
   20 	    47 	 0.05172 	 0.05551 	 ~...
   63 	    48 	 0.07457 	 0.05591 	 ~...
   44 	    49 	 0.06885 	 0.05732 	 ~...
    3 	    50 	 0.04912 	 0.05734 	 ~...
   74 	    51 	 0.07928 	 0.05734 	 ~...
   22 	    52 	 0.05177 	 0.05983 	 ~...
   71 	    53 	 0.07843 	 0.06022 	 ~...
   75 	    54 	 0.07992 	 0.07225 	 ~...
   59 	    55 	 0.07301 	 0.07363 	 ~...
   42 	    56 	 0.06869 	 0.07486 	 ~...
   53 	    57 	 0.07204 	 0.07725 	 ~...
   37 	    58 	 0.06563 	 0.07982 	 ~...
   65 	    59 	 0.07507 	 0.08319 	 ~...
   48 	    60 	 0.06967 	 0.08490 	 ~...
   38 	    61 	 0.06642 	 0.08573 	 ~...
   41 	    62 	 0.06683 	 0.08894 	 ~...
   45 	    63 	 0.06889 	 0.09329 	 ~...
   39 	    64 	 0.06660 	 0.09729 	 m..s
   39 	    65 	 0.06660 	 0.10125 	 m..s
   76 	    66 	 0.08019 	 0.10155 	 ~...
   31 	    67 	 0.05794 	 0.10574 	 m..s
   55 	    68 	 0.07229 	 0.10817 	 m..s
   46 	    69 	 0.06896 	 0.11408 	 m..s
   50 	    70 	 0.06977 	 0.11469 	 m..s
   46 	    71 	 0.06896 	 0.11741 	 m..s
   48 	    72 	 0.06967 	 0.11783 	 m..s
   67 	    73 	 0.07578 	 0.11955 	 m..s
   61 	    74 	 0.07424 	 0.13476 	 m..s
   84 	    75 	 0.10710 	 0.13948 	 m..s
   83 	    76 	 0.10465 	 0.13990 	 m..s
   52 	    77 	 0.07193 	 0.14134 	 m..s
   79 	    78 	 0.09034 	 0.14821 	 m..s
   73 	    79 	 0.07912 	 0.15188 	 m..s
   64 	    80 	 0.07489 	 0.15380 	 m..s
   51 	    81 	 0.07113 	 0.15406 	 m..s
   81 	    82 	 0.09696 	 0.16048 	 m..s
   58 	    83 	 0.07296 	 0.16228 	 m..s
   94 	    84 	 0.23185 	 0.16603 	 m..s
   56 	    85 	 0.07288 	 0.16630 	 m..s
   87 	    86 	 0.17762 	 0.18150 	 ~...
   90 	    87 	 0.19387 	 0.19363 	 ~...
   88 	    88 	 0.19187 	 0.21489 	 ~...
   92 	    89 	 0.22459 	 0.22027 	 ~...
   86 	    90 	 0.17648 	 0.23488 	 m..s
   95 	    91 	 0.24245 	 0.24196 	 ~...
   91 	    92 	 0.19493 	 0.24278 	 m..s
   93 	    93 	 0.22717 	 0.24502 	 ~...
   88 	    94 	 0.19187 	 0.24936 	 m..s
   85 	    95 	 0.15533 	 0.26153 	 MISS
   96 	    96 	 0.30813 	 0.26525 	 m..s
   97 	    97 	 0.49030 	 0.45875 	 m..s
  117 	    98 	 0.56382 	 0.51440 	 m..s
  104 	    99 	 0.54990 	 0.51598 	 m..s
  100 	   100 	 0.53715 	 0.52041 	 ~...
  101 	   101 	 0.53915 	 0.52120 	 ~...
   98 	   102 	 0.53706 	 0.52512 	 ~...
   99 	   103 	 0.53715 	 0.52858 	 ~...
  107 	   104 	 0.55172 	 0.53161 	 ~...
  114 	   105 	 0.56055 	 0.53525 	 ~...
  109 	   106 	 0.55214 	 0.53566 	 ~...
  111 	   107 	 0.55450 	 0.53657 	 ~...
  118 	   108 	 0.56740 	 0.54162 	 ~...
  103 	   109 	 0.54959 	 0.54676 	 ~...
  120 	   110 	 0.58168 	 0.54935 	 m..s
  102 	   111 	 0.54937 	 0.54951 	 ~...
  105 	   112 	 0.55000 	 0.55075 	 ~...
  119 	   113 	 0.56855 	 0.55245 	 ~...
  108 	   114 	 0.55204 	 0.55251 	 ~...
  110 	   115 	 0.55407 	 0.55712 	 ~...
  106 	   116 	 0.55010 	 0.55778 	 ~...
  115 	   117 	 0.56168 	 0.56101 	 ~...
  116 	   118 	 0.56201 	 0.56147 	 ~...
  112 	   119 	 0.55954 	 0.56442 	 ~...
  112 	   120 	 0.55954 	 0.57101 	 ~...
==========================================
r_mrr = 0.9846556782722473
r2_mrr = 0.9682948589324951
spearmanr_mrr@5 = 0.9750729203224182
spearmanr_mrr@10 = 0.9436140060424805
spearmanr_mrr@50 = 0.9968228340148926
spearmanr_mrr@100 = 0.9922155141830444
spearmanr_mrr@All = 0.9915236234664917
==========================================
test time: 0.444
Done Testing dataset UMLS
total time taken: 196.52231669425964
training time taken: 190.58225893974304
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9847)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9683)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9751)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9436)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9968)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9922)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9915)}}, 'test_loss': {'DistMult': {'UMLS': 1.3029024105399003}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'margin'}

==============================================================
--------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-dim
--------------------------------------------------------------
==============================================================
Running on a grid of size 1
Using random seed: 8444590309522739
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [76, 165, 1189, 1037, 297, 477, 961, 298, 419, 1020, 662, 283, 252, 1198, 217, 107, 625, 53, 718, 309, 1026, 719, 1126, 695, 276, 336, 63, 775, 966, 513, 69, 9, 698, 450, 933, 1138, 476, 796, 1068, 1158, 833, 468, 246, 1109, 764, 1072, 1167, 720, 1173, 866, 249, 488, 219, 347, 663, 949, 1036, 12, 870, 948, 378, 1103, 478, 624, 729, 105, 1120, 587, 1062, 907, 205, 597, 694, 969, 575, 245, 728, 794, 186, 1063, 1153, 1134, 1028, 810, 855, 380, 51, 777, 655, 84, 376, 548, 388, 512, 1106, 418, 1172, 860, 287, 807, 1097, 492, 1016, 788, 1193, 199, 881, 830, 52, 772, 838, 504, 940, 1168, 67, 1092, 639, 410, 349, 306, 740]
valid_ids (0): []
train_ids (1094): [316, 883, 960, 590, 745, 806, 113, 333, 6, 634, 282, 224, 391, 100, 657, 366, 1075, 439, 344, 24, 676, 85, 467, 771, 364, 1034, 699, 536, 129, 227, 1123, 198, 483, 289, 799, 1061, 218, 882, 660, 352, 626, 659, 299, 592, 399, 516, 572, 1080, 386, 646, 820, 501, 171, 371, 260, 102, 1005, 327, 528, 704, 827, 871, 1132, 658, 184, 1119, 836, 974, 1145, 610, 200, 640, 884, 317, 363, 954, 464, 1130, 578, 278, 749, 593, 632, 161, 737, 191, 222, 1013, 466, 1127, 338, 277, 1212, 1083, 1104, 19, 786, 446, 232, 251, 300, 511, 579, 25, 918, 742, 1024, 325, 425, 550, 911, 226, 598, 411, 677, 782, 343, 586, 842, 875, 1115, 485, 1003, 326, 221, 817, 731, 0, 955, 549, 557, 608, 864, 945, 1078, 95, 18, 1162, 194, 906, 475, 137, 1043, 725, 576, 38, 1164, 987, 151, 43, 116, 1211, 1131, 708, 407, 714, 1204, 319, 872, 233, 329, 210, 917, 484, 717, 862, 398, 45, 1121, 547, 944, 808, 878, 994, 766, 1214, 438, 902, 41, 1160, 888, 111, 818, 259, 1091, 652, 1105, 874, 225, 1066, 527, 192, 971, 859, 992, 507, 611, 1152, 334, 594, 1067, 583, 112, 675, 1056, 1151, 175, 840, 891, 591, 561, 998, 465, 361, 519, 1004, 551, 254, 824, 821, 1009, 279, 440, 953, 1179, 815, 869, 1018, 978, 294, 152, 1015, 189, 795, 1012, 1213, 372, 641, 1038, 711, 423, 1156, 750, 401, 500, 243, 709, 479, 865, 999, 964, 1190, 1186, 1055, 975, 1117, 1114, 724, 1176, 895, 90, 257, 631, 781, 916, 451, 16, 293, 901, 1049, 56, 377, 1031, 1157, 877, 931, 497, 89, 558, 828, 995, 207, 1079, 912, 448, 495, 846, 546, 285, 145, 650, 403, 427, 216, 605, 211, 397, 885, 253, 1202, 898, 685, 1163, 702, 793, 235, 360, 845, 314, 305, 1140, 915, 973, 564, 230, 302, 1101, 890, 664, 447, 988, 80, 453, 751, 413, 730, 124, 1144, 1074, 496, 613, 762, 980, 1141, 288, 643, 726, 1082, 369, 942, 989, 754, 819, 62, 803, 900, 585, 792, 78, 1011, 311, 61, 160, 455, 1139, 541, 435, 963, 441, 48, 4, 710, 854, 537, 125, 602, 164, 588, 979, 991, 458, 573, 193, 798, 722, 134, 935, 7, 957, 1154, 1006, 17, 237, 426, 560, 983, 236, 837, 930, 581, 33, 228, 248, 802, 787, 733, 149, 168, 545, 339, 679, 1107, 402, 665, 540, 672, 746, 559, 263, 647, 926, 35, 101, 616, 150, 568, 91, 542, 385, 32, 959, 269, 522, 517, 370, 11, 582, 622, 144, 462, 1094, 1064, 563, 1051, 108, 82, 571, 206, 58, 908, 744, 445, 104, 812, 554, 437, 518, 400, 460, 456, 143, 1057, 753, 894, 1108, 505, 654, 690, 972, 123, 1021, 805, 684, 159, 47, 538, 977, 280, 472, 923, 158, 167, 574, 223, 118, 135, 39, 619, 10, 743, 521, 14, 270, 858, 117, 1116, 213, 1030, 1178, 984, 1122, 436, 509, 748, 444, 406, 1027, 22, 967, 1175, 706, 1085, 229, 741, 284, 1185, 290, 375, 1194, 274, 180, 739, 539, 351, 498, 968, 928, 331, 785, 404, 889, 629, 201, 530, 1102, 31, 23, 244, 183, 952, 146, 520, 122, 59, 1195, 927, 75, 1135, 357, 36, 1088, 181, 394, 153, 1183, 320, 767, 196, 667, 947, 166, 454, 13, 569, 555, 422, 275, 1201, 524, 127, 487, 301, 345, 212, 98, 723, 81, 374, 382, 162, 1184, 332, 147, 801, 489, 847, 886, 321, 261, 128, 656, 929, 997, 922, 609, 1040, 126, 876, 156, 532, 544, 44, 919, 896, 1071, 379, 1090, 409, 661, 1008, 1035, 1019, 1147, 308, 508, 474, 635, 323, 670, 490, 5, 88, 1205, 307, 687, 1001, 770, 328, 946, 420, 1033, 809, 531, 666, 66, 633, 1060, 615, 982, 1089, 442, 303, 179, 1118, 1196, 50, 494, 849, 29, 1069, 1133, 214, 202, 703, 603, 457, 178, 1045, 242, 873, 1039, 271, 797, 73, 716, 140, 950, 1150, 1041, 177, 848, 1046, 1148, 843, 1146, 256, 491, 757, 553, 1029, 1136, 627, 342, 250, 682, 239, 829, 231, 1014, 638, 700, 296, 503, 1042, 408, 674, 139, 486, 774, 20, 705, 238, 1054, 607, 1199, 783, 822, 72, 1182, 258, 1174, 1099, 241, 1207, 1169, 359, 904, 1050, 697, 292, 965, 350, 3, 30, 70, 390, 46, 892, 94, 1170, 312, 692, 99, 768, 234, 932, 459, 120, 264, 119, 1076, 42, 387, 49, 392, 396, 1208, 804, 693, 905, 247, 1161, 920, 480, 92, 188, 1191, 71, 620, 1053, 368, 880, 909, 141, 93, 636, 132, 1000, 1192, 861, 358, 623, 473, 1166, 169, 1113, 267, 913, 310, 956, 776, 595, 738, 867, 434, 1044, 707, 565, 1111, 903, 506, 841, 962, 514, 163, 823, 157, 384, 1022, 1149, 715, 381, 1181, 77, 780, 649, 1187, 1081, 197, 863, 567, 57, 1047, 470, 562, 1200, 1032, 1155, 130, 64, 552, 760, 173, 1010, 834, 1007, 1077, 8, 255, 1143, 15, 266, 596, 1098, 800, 172, 732, 1058, 668, 1084, 354, 535, 533, 170, 897, 893, 938, 761, 240, 621, 826, 735, 713, 1002, 773, 529, 60, 421, 27, 87, 993, 996, 618, 601, 315, 934, 758, 121, 452, 272, 681, 68, 1197, 790, 482, 2, 1129, 645, 789, 103, 1112, 493, 37, 778, 182, 752, 148, 510, 471, 925, 1203, 1159, 499, 1142, 853, 612, 268, 577, 281, 1070, 1171, 174, 154, 96, 405, 689, 1165, 97, 651, 1025, 40, 262, 831, 449, 110, 734, 109, 1125, 1209, 628, 79, 642, 1128, 1, 791, 1023, 970, 1073, 921, 373, 736, 653, 644, 857, 114, 958, 851, 155, 669, 337, 543, 424, 136, 1110, 106, 696, 365, 187, 678, 839, 131, 1124, 1188, 526, 763, 415, 769, 941, 176, 712, 525, 566, 813, 324, 1086, 1095, 1052, 630, 431, 584, 556, 220, 295, 747, 1100, 1096, 825, 142, 1210, 356, 291, 195, 852, 887, 463, 515, 721, 850, 606, 688, 937, 215, 910, 637, 614, 976, 835, 814, 856, 346, 1087, 1180, 534, 26, 648, 580, 1017, 924, 765, 727, 185, 432, 570, 686, 86, 265, 589, 74, 939, 318, 416, 65, 383, 832, 443, 55, 523, 417, 502, 1206, 412, 868, 208, 429, 691, 353, 341, 599, 83, 680, 811, 286, 304, 393, 335, 469, 330, 844, 115, 1093, 203, 617, 951, 362, 395, 701, 367, 755, 981, 1065, 914, 355, 816, 1059, 28, 34, 190, 600, 943, 759, 673, 671, 21, 985, 348, 879, 683, 1048, 756, 209, 204, 340, 461, 138, 1137, 433, 430, 936, 1177, 779, 784, 481, 54, 899, 322, 414, 604, 273, 313, 389, 986, 428, 133, 990]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  4876231479988150
the save name prefix for this run is:  chkpt-ID_4876231479988150_tag_Ablation-job-blacklist-dim
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=23, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=8, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 975
rank avg (pred): 0.511 +- 0.008
mrr vals (pred, true): 0.014, 0.557
batch losses (mrrl, rdl): 0.0, 0.0049031028

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 571
rank avg (pred): 0.456 +- 0.274
mrr vals (pred, true): 0.189, 0.030
batch losses (mrrl, rdl): 0.0, 2.72378e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 32
rank avg (pred): 0.039 +- 0.028
mrr vals (pred, true): 0.379, 0.523
batch losses (mrrl, rdl): 0.0, 3.607e-07

Epoch over!
epoch time: 12.553

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 976
rank avg (pred): 0.026 +- 0.018
mrr vals (pred, true): 0.419, 0.545
batch losses (mrrl, rdl): 0.0, 1.7021e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 308
rank avg (pred): 0.042 +- 0.031
mrr vals (pred, true): 0.389, 0.546
batch losses (mrrl, rdl): 0.0, 1.4552e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1175
rank avg (pred): 0.435 +- 0.287
mrr vals (pred, true): 0.228, 0.045
batch losses (mrrl, rdl): 0.0, 1.07368e-05

Epoch over!
epoch time: 12.66

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 257
rank avg (pred): 0.023 +- 0.018
mrr vals (pred, true): 0.449, 0.561
batch losses (mrrl, rdl): 0.0, 1.6488e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 293
rank avg (pred): 0.036 +- 0.027
mrr vals (pred, true): 0.398, 0.554
batch losses (mrrl, rdl): 0.0, 2.568e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 896
rank avg (pred): 0.357 +- 0.240
mrr vals (pred, true): 0.236, 0.021
batch losses (mrrl, rdl): 0.0, 0.0001246649

Epoch over!
epoch time: 12.207

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 912
rank avg (pred): 0.451 +- 0.271
mrr vals (pred, true): 0.193, 0.057
batch losses (mrrl, rdl): 0.0, 5.61175e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 136
rank avg (pred): 0.341 +- 0.269
mrr vals (pred, true): 0.287, 0.080
batch losses (mrrl, rdl): 0.0, 4.80582e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 398
rank avg (pred): 0.350 +- 0.253
mrr vals (pred, true): 0.245, 0.102
batch losses (mrrl, rdl): 0.0, 4.08269e-05

Epoch over!
epoch time: 12.333

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 975
rank avg (pred): 0.017 +- 0.013
mrr vals (pred, true): 0.497, 0.557
batch losses (mrrl, rdl): 0.0, 6.2226e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 353
rank avg (pred): 0.349 +- 0.255
mrr vals (pred, true): 0.238, 0.096
batch losses (mrrl, rdl): 0.0, 7.15079e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 970
rank avg (pred): 0.554 +- 0.301
mrr vals (pred, true): 0.158, 0.046
batch losses (mrrl, rdl): 0.0, 0.0002910112

Epoch over!
epoch time: 12.1

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 825
rank avg (pred): 0.199 +- 0.152
mrr vals (pred, true): 0.283, 0.308
batch losses (mrrl, rdl): 0.0064782947, 0.0001033244

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 985
rank avg (pred): 0.017 +- 0.009
mrr vals (pred, true): 0.381, 0.537
batch losses (mrrl, rdl): 0.24290362, 7.1585e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 661
rank avg (pred): 0.482 +- 0.146
mrr vals (pred, true): 0.051, 0.050
batch losses (mrrl, rdl): 1.70152e-05, 6.72288e-05

Epoch over!
epoch time: 12.348

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 185
rank avg (pred): 0.426 +- 0.166
mrr vals (pred, true): 0.071, 0.047
batch losses (mrrl, rdl): 0.0042283619, 3.95524e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 786
rank avg (pred): 0.472 +- 0.135
mrr vals (pred, true): 0.047, 0.053
batch losses (mrrl, rdl): 0.0001111352, 6.72271e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 931
rank avg (pred): 0.427 +- 0.104
mrr vals (pred, true): 0.041, 0.016
batch losses (mrrl, rdl): 0.0007531125, 0.002037677

Epoch over!
epoch time: 12.399

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1144
rank avg (pred): 0.277 +- 0.149
mrr vals (pred, true): 0.101, 0.140
batch losses (mrrl, rdl): 0.0148890279, 5.41901e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1171
rank avg (pred): 0.478 +- 0.188
mrr vals (pred, true): 0.060, 0.050
batch losses (mrrl, rdl): 0.0010937811, 4.27413e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1002
rank avg (pred): 0.272 +- 0.164
mrr vals (pred, true): 0.126, 0.141
batch losses (mrrl, rdl): 0.0023909407, 2.90871e-05

Epoch over!
epoch time: 12.393

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 140
rank avg (pred): 0.469 +- 0.228
mrr vals (pred, true): 0.070, 0.102
batch losses (mrrl, rdl): 0.0100381756, 0.0005409291

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 666
rank avg (pred): 0.415 +- 0.111
mrr vals (pred, true): 0.051, 0.047
batch losses (mrrl, rdl): 2.17954e-05, 5.39084e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 148
rank avg (pred): 0.448 +- 0.234
mrr vals (pred, true): 0.079, 0.111
batch losses (mrrl, rdl): 0.0103519252, 0.0005080382

Epoch over!
epoch time: 12.452

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 319
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.548, 0.553
batch losses (mrrl, rdl): 0.0001961167, 1.55857e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 146
rank avg (pred): 0.505 +- 0.227
mrr vals (pred, true): 0.058, 0.098
batch losses (mrrl, rdl): 0.0006538598, 0.000755213

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1152
rank avg (pred): 0.144 +- 0.091
mrr vals (pred, true): 0.156, 0.135
batch losses (mrrl, rdl): 0.0043173209, 0.0003251737

Epoch over!
epoch time: 12.67

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 531
rank avg (pred): 0.538 +- 0.261
mrr vals (pred, true): 0.060, 0.052
batch losses (mrrl, rdl): 0.0010124327, 0.000199569

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 285
rank avg (pred): 0.014 +- 0.010
mrr vals (pred, true): 0.492, 0.544
batch losses (mrrl, rdl): 0.026438972, 1.48699e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 541
rank avg (pred): 0.549 +- 0.277
mrr vals (pred, true): 0.061, 0.083
batch losses (mrrl, rdl): 0.001158466, 0.0004399675

Epoch over!
epoch time: 12.489

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 456
rank avg (pred): 0.349 +- 0.170
mrr vals (pred, true): 0.076, 0.054
batch losses (mrrl, rdl): 0.0069878595, 0.0001293083

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 955
rank avg (pred): 0.530 +- 0.163
mrr vals (pred, true): 0.035, 0.047
batch losses (mrrl, rdl): 0.0023765007, 0.000165003

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 251
rank avg (pred): 0.014 +- 0.011
mrr vals (pred, true): 0.511, 0.535
batch losses (mrrl, rdl): 0.0058920067, 8.2749e-06

Epoch over!
epoch time: 12.432

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 930
rank avg (pred): 0.552 +- 0.188
mrr vals (pred, true): 0.047, 0.016
batch losses (mrrl, rdl): 9.57394e-05, 0.0008385929

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 415
rank avg (pred): 0.413 +- 0.176
mrr vals (pred, true): 0.060, 0.052
batch losses (mrrl, rdl): 0.0009259786, 3.62948e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 150
rank avg (pred): 0.427 +- 0.217
mrr vals (pred, true): 0.071, 0.118
batch losses (mrrl, rdl): 0.0222205408, 0.0003304708

Epoch over!
epoch time: 12.435

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 773
rank avg (pred): 0.503 +- 0.162
mrr vals (pred, true): 0.046, 0.055
batch losses (mrrl, rdl): 0.0001759591, 9.77231e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 471
rank avg (pred): 0.375 +- 0.179
mrr vals (pred, true): 0.069, 0.052
batch losses (mrrl, rdl): 0.0036466226, 8.10839e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 47
rank avg (pred): 0.016 +- 0.013
mrr vals (pred, true): 0.500, 0.532
batch losses (mrrl, rdl): 0.0102619873, 1.26677e-05

Epoch over!
epoch time: 12.53

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 555
rank avg (pred): 0.583 +- 0.273
mrr vals (pred, true): 0.056, 0.048
batch losses (mrrl, rdl): 0.0004104683, 0.0003561271

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 688
rank avg (pred): 0.462 +- 0.205
mrr vals (pred, true): 0.065, 0.046
batch losses (mrrl, rdl): 0.0023223201, 2.62183e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 206
rank avg (pred): 0.482 +- 0.218
mrr vals (pred, true): 0.060, 0.049
batch losses (mrrl, rdl): 0.0010621296, 7.66103e-05

Epoch over!
epoch time: 12.373

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.013 +- 0.011
mrr vals (pred, true): 0.554, 0.528

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    2 	     0 	 0.04771 	 0.01547 	 m..s
    8 	     1 	 0.05279 	 0.01767 	 m..s
    6 	     2 	 0.05063 	 0.03457 	 ~...
   36 	     3 	 0.07042 	 0.03746 	 m..s
   38 	     4 	 0.07160 	 0.03791 	 m..s
   29 	     5 	 0.06454 	 0.03798 	 ~...
   24 	     6 	 0.06197 	 0.04008 	 ~...
   37 	     7 	 0.07081 	 0.04041 	 m..s
   43 	     8 	 0.07290 	 0.04115 	 m..s
   42 	     9 	 0.07215 	 0.04155 	 m..s
   14 	    10 	 0.05484 	 0.04271 	 ~...
    9 	    11 	 0.05305 	 0.04274 	 ~...
   39 	    12 	 0.07167 	 0.04345 	 ~...
   63 	    13 	 0.08686 	 0.04365 	 m..s
    0 	    14 	 0.04762 	 0.04366 	 ~...
   12 	    15 	 0.05458 	 0.04394 	 ~...
   31 	    16 	 0.06612 	 0.04440 	 ~...
    3 	    17 	 0.04790 	 0.04453 	 ~...
    1 	    18 	 0.04767 	 0.04460 	 ~...
   60 	    19 	 0.08631 	 0.04474 	 m..s
   75 	    20 	 0.09306 	 0.04480 	 m..s
   78 	    21 	 0.09662 	 0.04483 	 m..s
   23 	    22 	 0.06183 	 0.04493 	 ~...
   50 	    23 	 0.08186 	 0.04508 	 m..s
   79 	    24 	 0.09839 	 0.04520 	 m..s
   14 	    25 	 0.05484 	 0.04531 	 ~...
   18 	    26 	 0.06033 	 0.04534 	 ~...
   32 	    27 	 0.06836 	 0.04578 	 ~...
   34 	    28 	 0.06846 	 0.04585 	 ~...
   35 	    29 	 0.06889 	 0.04624 	 ~...
   43 	    30 	 0.07290 	 0.04634 	 ~...
   19 	    31 	 0.06048 	 0.04659 	 ~...
   40 	    32 	 0.07188 	 0.04665 	 ~...
   41 	    33 	 0.07201 	 0.04683 	 ~...
   52 	    34 	 0.08320 	 0.04688 	 m..s
    7 	    35 	 0.05270 	 0.04707 	 ~...
   82 	    36 	 0.09961 	 0.04733 	 m..s
   22 	    37 	 0.06164 	 0.04741 	 ~...
   60 	    38 	 0.08631 	 0.04754 	 m..s
   67 	    39 	 0.08865 	 0.04754 	 m..s
   68 	    40 	 0.08903 	 0.04755 	 m..s
   55 	    41 	 0.08501 	 0.04818 	 m..s
   32 	    42 	 0.06836 	 0.04818 	 ~...
   54 	    43 	 0.08475 	 0.04832 	 m..s
   51 	    44 	 0.08235 	 0.04836 	 m..s
   59 	    45 	 0.08622 	 0.04845 	 m..s
   10 	    46 	 0.05393 	 0.04851 	 ~...
   76 	    47 	 0.09384 	 0.04856 	 m..s
    5 	    48 	 0.04825 	 0.04860 	 ~...
   25 	    49 	 0.06220 	 0.04871 	 ~...
    3 	    50 	 0.04790 	 0.04966 	 ~...
   16 	    51 	 0.05492 	 0.04986 	 ~...
   21 	    52 	 0.06141 	 0.05095 	 ~...
   20 	    53 	 0.06126 	 0.05115 	 ~...
   71 	    54 	 0.09105 	 0.05146 	 m..s
   47 	    55 	 0.07840 	 0.05147 	 ~...
   63 	    56 	 0.08686 	 0.05189 	 m..s
   28 	    57 	 0.06383 	 0.05240 	 ~...
   79 	    58 	 0.09839 	 0.05268 	 m..s
   77 	    59 	 0.09624 	 0.05284 	 m..s
   27 	    60 	 0.06281 	 0.05344 	 ~...
   30 	    61 	 0.06552 	 0.05362 	 ~...
   17 	    62 	 0.06014 	 0.05484 	 ~...
   26 	    63 	 0.06240 	 0.05499 	 ~...
   13 	    64 	 0.05483 	 0.05749 	 ~...
   11 	    65 	 0.05414 	 0.06067 	 ~...
   45 	    66 	 0.07608 	 0.06455 	 ~...
   66 	    67 	 0.08782 	 0.08200 	 ~...
   73 	    68 	 0.09175 	 0.08288 	 ~...
   70 	    69 	 0.09099 	 0.08797 	 ~...
   46 	    70 	 0.07754 	 0.08802 	 ~...
   48 	    71 	 0.07914 	 0.09739 	 ~...
   83 	    72 	 0.10164 	 0.10064 	 ~...
   57 	    73 	 0.08589 	 0.10123 	 ~...
   65 	    74 	 0.08781 	 0.10197 	 ~...
   74 	    75 	 0.09297 	 0.10981 	 ~...
   69 	    76 	 0.09036 	 0.11140 	 ~...
   56 	    77 	 0.08580 	 0.11408 	 ~...
   72 	    78 	 0.09173 	 0.11564 	 ~...
   84 	    79 	 0.15924 	 0.12681 	 m..s
   62 	    80 	 0.08684 	 0.12795 	 m..s
   58 	    81 	 0.08620 	 0.13098 	 m..s
   53 	    82 	 0.08391 	 0.13182 	 m..s
   81 	    83 	 0.09956 	 0.13238 	 m..s
   86 	    84 	 0.16442 	 0.14525 	 ~...
   49 	    85 	 0.07996 	 0.14995 	 m..s
   87 	    86 	 0.18867 	 0.18507 	 ~...
   85 	    87 	 0.16413 	 0.20213 	 m..s
   88 	    88 	 0.24018 	 0.24069 	 ~...
   92 	    89 	 0.28140 	 0.24207 	 m..s
   90 	    90 	 0.28033 	 0.24467 	 m..s
   89 	    91 	 0.26914 	 0.26153 	 ~...
   91 	    92 	 0.28067 	 0.36005 	 m..s
   93 	    93 	 0.32048 	 0.36843 	 m..s
   94 	    94 	 0.44280 	 0.46923 	 ~...
  101 	    95 	 0.54621 	 0.51017 	 m..s
  101 	    96 	 0.54621 	 0.51127 	 m..s
  114 	    97 	 0.55433 	 0.51128 	 m..s
   96 	    98 	 0.50199 	 0.51250 	 ~...
  116 	    99 	 0.55681 	 0.52033 	 m..s
  115 	   100 	 0.55436 	 0.52778 	 ~...
  111 	   101 	 0.55172 	 0.52928 	 ~...
  107 	   102 	 0.54933 	 0.53020 	 ~...
  120 	   103 	 0.58925 	 0.53200 	 m..s
  112 	   104 	 0.55385 	 0.53233 	 ~...
  100 	   105 	 0.54380 	 0.53464 	 ~...
   97 	   106 	 0.54012 	 0.53484 	 ~...
  117 	   107 	 0.57583 	 0.53587 	 m..s
   95 	   108 	 0.49818 	 0.53600 	 m..s
   98 	   109 	 0.54227 	 0.53966 	 ~...
  119 	   110 	 0.57588 	 0.54162 	 m..s
  112 	   111 	 0.55385 	 0.54255 	 ~...
  117 	   112 	 0.57583 	 0.54522 	 m..s
   98 	   113 	 0.54227 	 0.54894 	 ~...
  110 	   114 	 0.55168 	 0.55272 	 ~...
  109 	   115 	 0.55134 	 0.55519 	 ~...
  108 	   116 	 0.54978 	 0.55644 	 ~...
  103 	   117 	 0.54638 	 0.55718 	 ~...
  106 	   118 	 0.54927 	 0.55879 	 ~...
  104 	   119 	 0.54768 	 0.56362 	 ~...
  104 	   120 	 0.54768 	 0.57101 	 ~...
==========================================
r_mrr = 0.991352915763855
r2_mrr = 0.979032039642334
spearmanr_mrr@5 = 0.7975955009460449
spearmanr_mrr@10 = 0.8726173043251038
spearmanr_mrr@50 = 0.995376706123352
spearmanr_mrr@100 = 0.9950318932533264
spearmanr_mrr@All = 0.9954790472984314
==========================================
test time: 0.401
Done Testing dataset UMLS
total time taken: 192.99761509895325
training time taken: 186.8414192199707
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9914)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9790)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.7976)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8726)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9954)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9950)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9955)}}, 'test_loss': {'DistMult': {'UMLS': 0.9680192826654093}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'dim'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 5978971835762898
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [369, 120, 795, 94, 37, 1130, 45, 753, 684, 674, 696, 166, 659, 281, 115, 1064, 167, 584, 435, 230, 151, 103, 1155, 440, 305, 551, 735, 609, 228, 393, 700, 145, 149, 979, 1079, 504, 234, 314, 960, 56, 1180, 245, 448, 755, 642, 152, 127, 450, 1044, 349, 293, 79, 3, 1066, 630, 1060, 987, 612, 477, 489, 126, 456, 134, 377, 338, 1020, 1012, 886, 1095, 318, 853, 449, 434, 452, 692, 1055, 643, 699, 2, 889, 421, 874, 763, 367, 767, 806, 591, 399, 28, 430, 774, 657, 292, 352, 77, 733, 303, 278, 284, 271, 731, 32, 563, 727, 388, 695, 796, 1062, 29, 108, 70, 770, 593, 326, 507, 673, 137, 202, 896, 36, 60]
valid_ids (0): []
train_ids (1094): [836, 219, 906, 1108, 1028, 835, 778, 285, 1188, 701, 539, 88, 231, 82, 344, 1103, 244, 559, 408, 819, 373, 962, 1045, 1074, 636, 924, 365, 378, 572, 30, 663, 1073, 233, 375, 1153, 676, 57, 794, 777, 485, 342, 1000, 904, 486, 1, 150, 279, 73, 1134, 358, 282, 834, 158, 518, 46, 928, 1024, 764, 900, 501, 533, 42, 788, 930, 585, 850, 1147, 514, 320, 1082, 62, 8, 997, 33, 481, 811, 797, 535, 865, 851, 468, 394, 1131, 1009, 210, 100, 154, 1029, 945, 333, 597, 856, 43, 959, 1139, 536, 1206, 474, 807, 1112, 398, 1152, 467, 1038, 144, 54, 756, 649, 466, 654, 236, 901, 1101, 383, 1199, 269, 634, 1173, 520, 578, 723, 1201, 39, 1089, 718, 111, 340, 446, 689, 624, 1058, 482, 223, 698, 222, 498, 1156, 725, 800, 266, 976, 620, 571, 991, 368, 295, 346, 427, 569, 929, 783, 263, 799, 53, 948, 982, 445, 552, 191, 74, 1176, 1174, 980, 509, 419, 250, 64, 1068, 59, 632, 220, 923, 112, 709, 1136, 854, 537, 458, 1181, 490, 878, 55, 354, 156, 1026, 370, 1075, 740, 357, 561, 376, 1013, 613, 267, 10, 617, 493, 195, 883, 229, 1123, 603, 970, 1034, 265, 893, 1115, 206, 50, 1110, 704, 720, 784, 26, 499, 1076, 844, 791, 61, 863, 337, 892, 86, 1172, 290, 1063, 608, 650, 429, 1177, 1190, 93, 644, 1143, 255, 885, 629, 1141, 760, 451, 779, 817, 1184, 528, 175, 538, 633, 803, 25, 882, 18, 666, 589, 312, 992, 905, 833, 23, 1135, 240, 251, 204, 707, 356, 869, 1167, 647, 761, 1121, 1118, 862, 994, 1178, 1146, 384, 1168, 128, 990, 706, 768, 981, 1039, 575, 616, 185, 690, 464, 570, 940, 469, 502, 76, 1050, 762, 957, 1124, 343, 436, 1200, 136, 1195, 804, 1054, 545, 899, 306, 416, 147, 789, 826, 14, 621, 180, 574, 247, 871, 51, 280, 652, 34, 1086, 159, 553, 773, 606, 867, 1003, 1014, 69, 1059, 177, 380, 140, 668, 955, 168, 268, 703, 525, 1070, 772, 734, 876, 179, 732, 550, 442, 1017, 476, 232, 1088, 736, 861, 505, 825, 98, 497, 1032, 832, 323, 996, 1021, 249, 1071, 995, 967, 40, 386, 11, 463, 963, 728, 974, 1025, 347, 403, 895, 1209, 171, 390, 917, 1056, 371, 218, 461, 745, 364, 860, 645, 207, 192, 96, 1030, 106, 586, 546, 161, 888, 837, 679, 238, 1036, 565, 381, 661, 1037, 66, 38, 129, 299, 1212, 1149, 361, 838, 330, 1207, 1057, 1004, 527, 488, 260, 410, 363, 1041, 12, 694, 1120, 24, 397, 710, 1114, 109, 557, 224, 939, 261, 592, 677, 664, 444, 843, 27, 739, 573, 366, 857, 242, 1191, 387, 576, 500, 170, 712, 362, 877, 226, 359, 396, 5, 614, 301, 107, 812, 153, 172, 431, 447, 688, 872, 717, 1154, 201, 726, 311, 691, 71, 830, 534, 667, 0, 1011, 355, 300, 908, 125, 771, 286, 521, 680, 287, 164, 360, 414, 954, 903, 217, 933, 610, 855, 273, 927, 142, 309, 316, 22, 187, 818, 407, 462, 1091, 852, 291, 7, 600, 298, 480, 1019, 105, 952, 588, 155, 1043, 1104, 328, 189, 315, 1016, 622, 988, 926, 160, 915, 880, 969, 313, 749, 814, 200, 526, 524, 846, 656, 548, 1122, 1077, 675, 1093, 582, 289, 389, 67, 637, 457, 41, 351, 759, 1162, 382, 971, 277, 935, 625, 1150, 769, 133, 859, 215, 124, 141, 822, 670, 1144, 197, 47, 567, 516, 748, 583, 983, 1008, 121, 44, 1015, 1163, 925, 214, 1052, 919, 907, 15, 58, 441, 581, 638, 198, 1080, 1182, 671, 1048, 4, 619, 602, 181, 90, 385, 203, 813, 114, 1145, 765, 1100, 78, 868, 472, 437, 824, 6, 395, 35, 747, 785, 491, 635, 782, 989, 21, 640, 1208, 790, 345, 465, 938, 681, 248, 1194, 555, 601, 823, 708, 662, 512, 68, 746, 870, 1179, 506, 1159, 1196, 296, 798, 1203, 805, 669, 473, 958, 544, 423, 1072, 1046, 65, 225, 847, 246, 890, 1160, 227, 986, 1157, 235, 63, 757, 319, 627, 16, 169, 1061, 831, 401, 237, 912, 484, 122, 1113, 1127, 542, 1125, 460, 212, 751, 193, 19, 110, 148, 475, 272, 288, 307, 1211, 325, 993, 417, 426, 116, 776, 1069, 1165, 594, 787, 875, 1205, 420, 839, 335, 239, 372, 693, 322, 91, 99, 428, 302, 257, 579, 174, 540, 658, 1001, 199, 1098, 182, 631, 781, 913, 178, 615, 1002, 711, 20, 503, 1053, 117, 123, 660, 866, 80, 1099, 827, 921, 937, 750, 580, 842, 392, 492, 946, 968, 184, 1106, 391, 157, 879, 519, 941, 332, 510, 132, 495, 998, 1105, 1027, 453, 966, 1119, 262, 840, 716, 702, 118, 902, 1051, 415, 1092, 741, 1006, 729, 549, 808, 329, 183, 598, 81, 918, 1133, 1085, 1161, 264, 494, 83, 909, 422, 1187, 683, 1204, 241, 276, 1164, 1166, 715, 884, 459, 119, 374, 400, 596, 139, 530, 1067, 845, 705, 49, 1040, 1111, 984, 1007, 243, 1158, 1137, 438, 327, 1102, 858, 1065, 775, 1169, 639, 496, 651, 522, 252, 841, 413, 881, 911, 547, 934, 944, 953, 554, 810, 604, 104, 873, 529, 1151, 1186, 424, 999, 914, 1083, 478, 1116, 646, 1214, 221, 211, 849, 1078, 1128, 1109, 411, 1189, 531, 758, 665, 730, 283, 898, 714, 936, 682, 517, 947, 256, 87, 1084, 1018, 931, 949, 821, 253, 321, 942, 13, 828, 829, 17, 891, 802, 102, 961, 1097, 331, 766, 72, 532, 687, 1140, 194, 623, 176, 1132, 820, 412, 560, 590, 1138, 566, 1033, 595, 1183, 568, 894, 562, 965, 1193, 916, 173, 95, 208, 1087, 483, 956, 1192, 425, 190, 738, 628, 641, 697, 165, 722, 724, 186, 310, 487, 686, 848, 213, 815, 897, 1094, 932, 618, 1031, 743, 471, 339, 439, 1210, 443, 607, 1185, 1005, 887, 943, 801, 977, 75, 648, 1049, 341, 577, 1081, 455, 146, 143, 985, 1170, 508, 52, 754, 270, 1142, 744, 742, 1042, 1202, 1010, 543, 479, 135, 209, 1175, 304, 324, 1117, 163, 317, 922, 672, 786, 1035, 433, 752, 97, 541, 864, 1096, 1090, 274, 611, 964, 85, 780, 406, 587, 972, 48, 721, 336, 920, 511, 1023, 1213, 259, 1129, 162, 405, 816, 432, 353, 404, 713, 950, 719, 685, 308, 84, 605, 951, 258, 1171, 350, 564, 793, 1047, 975, 409, 294, 101, 9, 275, 1198, 626, 205, 454, 978, 653, 348, 402, 910, 196, 113, 1197, 1022, 334, 138, 1107, 513, 678, 1126, 130, 973, 556, 558, 418, 216, 379, 792, 131, 737, 655, 297, 188, 599, 89, 809, 31, 254, 1148, 470, 523, 92, 515]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1443753691825442
the save name prefix for this run is:  chkpt-ID_1443753691825442_tag_Ablation-job-blacklist-s_deg
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 829
rank avg (pred): 0.505 +- 0.007
mrr vals (pred, true): 0.015, 0.532
batch losses (mrrl, rdl): 0.0, 0.004639593

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 315
rank avg (pred): 0.151 +- 0.207
mrr vals (pred, true): 0.193, 0.532
batch losses (mrrl, rdl): 0.0, 0.000277806

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 553
rank avg (pred): 0.433 +- 0.273
mrr vals (pred, true): 0.094, 0.063
batch losses (mrrl, rdl): 0.0, 5.3083e-06

Epoch over!
epoch time: 12.213

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 966
rank avg (pred): 0.515 +- 0.275
mrr vals (pred, true): 0.085, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001580556

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1018
rank avg (pred): 0.319 +- 0.282
mrr vals (pred, true): 0.154, 0.073
batch losses (mrrl, rdl): 0.0, 2.42564e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 487
rank avg (pred): 0.251 +- 0.241
mrr vals (pred, true): 0.166, 0.213
batch losses (mrrl, rdl): 0.0, 2.15108e-05

Epoch over!
epoch time: 12.209

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 304
rank avg (pred): 0.019 +- 0.037
mrr vals (pred, true): 0.488, 0.536
batch losses (mrrl, rdl): 0.0, 6.7059e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1191
rank avg (pred): 0.455 +- 0.280
mrr vals (pred, true): 0.107, 0.051
batch losses (mrrl, rdl): 0.0, 2.39727e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 648
rank avg (pred): 0.461 +- 0.279
mrr vals (pred, true): 0.112, 0.053
batch losses (mrrl, rdl): 0.0, 2.97007e-05

Epoch over!
epoch time: 12.113

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1034
rank avg (pred): 0.310 +- 0.269
mrr vals (pred, true): 0.159, 0.056
batch losses (mrrl, rdl): 0.0, 0.000266402

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1069
rank avg (pred): 0.060 +- 0.104
mrr vals (pred, true): 0.339, 0.548
batch losses (mrrl, rdl): 0.0, 1.42486e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1084
rank avg (pred): 0.388 +- 0.284
mrr vals (pred, true): 0.131, 0.157
batch losses (mrrl, rdl): 0.0, 0.0005466663

Epoch over!
epoch time: 12.237

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 730
rank avg (pred): 0.116 +- 0.171
mrr vals (pred, true): 0.277, 0.370
batch losses (mrrl, rdl): 0.0, 4.64675e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1014
rank avg (pred): 0.387 +- 0.275
mrr vals (pred, true): 0.123, 0.101
batch losses (mrrl, rdl): 0.0, 8.76206e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1137
rank avg (pred): 0.186 +- 0.237
mrr vals (pred, true): 0.212, 0.229
batch losses (mrrl, rdl): 0.0, 3.6558e-06

Epoch over!
epoch time: 12.149

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 20
rank avg (pred): 0.029 +- 0.083
mrr vals (pred, true): 0.528, 0.545
batch losses (mrrl, rdl): 0.0027130656, 1.1305e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 368
rank avg (pred): 0.387 +- 0.244
mrr vals (pred, true): 0.082, 0.120
batch losses (mrrl, rdl): 0.0142177399, 0.0002159245

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 811
rank avg (pred): 0.216 +- 0.255
mrr vals (pred, true): 0.257, 0.265
batch losses (mrrl, rdl): 0.0006851957, 0.0004774947

Epoch over!
epoch time: 12.459

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 192
rank avg (pred): 0.390 +- 0.225
mrr vals (pred, true): 0.068, 0.051
batch losses (mrrl, rdl): 0.0031333449, 3.06785e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 324
rank avg (pred): 0.388 +- 0.195
mrr vals (pred, true): 0.076, 0.098
batch losses (mrrl, rdl): 0.0067184521, 0.0001271635

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1198
rank avg (pred): 0.456 +- 0.200
mrr vals (pred, true): 0.060, 0.047
batch losses (mrrl, rdl): 0.0010532618, 1.74811e-05

Epoch over!
epoch time: 12.605

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 199
rank avg (pred): 0.381 +- 0.205
mrr vals (pred, true): 0.076, 0.052
batch losses (mrrl, rdl): 0.0066732345, 5.38943e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1174
rank avg (pred): 0.451 +- 0.191
mrr vals (pred, true): 0.062, 0.036
batch losses (mrrl, rdl): 0.0015494097, 3.51042e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 622
rank avg (pred): 0.469 +- 0.186
mrr vals (pred, true): 0.061, 0.042
batch losses (mrrl, rdl): 0.0012972669, 2.45959e-05

Epoch over!
epoch time: 12.601

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 219
rank avg (pred): 0.369 +- 0.215
mrr vals (pred, true): 0.071, 0.048
batch losses (mrrl, rdl): 0.0042554089, 7.13346e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 898
rank avg (pred): 0.455 +- 0.136
mrr vals (pred, true): 0.053, 0.019
batch losses (mrrl, rdl): 0.0001223902, 1.4204e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 193
rank avg (pred): 0.419 +- 0.212
mrr vals (pred, true): 0.063, 0.041
batch losses (mrrl, rdl): 0.0016762801, 2.69751e-05

Epoch over!
epoch time: 12.796

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 557
rank avg (pred): 0.336 +- 0.204
mrr vals (pred, true): 0.083, 0.110
batch losses (mrrl, rdl): 0.0073755858, 2.87804e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 797
rank avg (pred): 0.461 +- 0.153
mrr vals (pred, true): 0.061, 0.048
batch losses (mrrl, rdl): 0.0011377583, 5.10919e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 518
rank avg (pred): 0.362 +- 0.184
mrr vals (pred, true): 0.074, 0.127
batch losses (mrrl, rdl): 0.0288694277, 9.49105e-05

Epoch over!
epoch time: 12.978

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 832
rank avg (pred): 0.219 +- 0.243
mrr vals (pred, true): 0.484, 0.409
batch losses (mrrl, rdl): 0.0554876626, 0.0006495484

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 784
rank avg (pred): 0.473 +- 0.185
mrr vals (pred, true): 0.060, 0.055
batch losses (mrrl, rdl): 0.000926632, 5.08837e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 493
rank avg (pred): 0.251 +- 0.248
mrr vals (pred, true): 0.202, 0.218
batch losses (mrrl, rdl): 0.002687746, 0.0001002657

Epoch over!
epoch time: 12.45

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 877
rank avg (pred): 0.462 +- 0.168
mrr vals (pred, true): 0.063, 0.046
batch losses (mrrl, rdl): 0.0016375467, 3.15795e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1201
rank avg (pred): 0.537 +- 0.209
mrr vals (pred, true): 0.052, 0.049
batch losses (mrrl, rdl): 2.49785e-05, 0.0002340235

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 532
rank avg (pred): 0.360 +- 0.226
mrr vals (pred, true): 0.075, 0.056
batch losses (mrrl, rdl): 0.006349701, 9.27731e-05

Epoch over!
epoch time: 12.534

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 769
rank avg (pred): 0.484 +- 0.164
mrr vals (pred, true): 0.057, 0.042
batch losses (mrrl, rdl): 0.0005233646, 7.98635e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 952
rank avg (pred): 0.474 +- 0.151
mrr vals (pred, true): 0.056, 0.045
batch losses (mrrl, rdl): 0.000325997, 4.32665e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 926
rank avg (pred): 0.488 +- 0.162
mrr vals (pred, true): 0.053, 0.015
batch losses (mrrl, rdl): 7.65854e-05, 0.0016352304

Epoch over!
epoch time: 13.61

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 927
rank avg (pred): 0.515 +- 0.192
mrr vals (pred, true): 0.061, 0.018
batch losses (mrrl, rdl): 0.0012846163, 0.0006891857

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1067
rank avg (pred): 0.165 +- 0.209
mrr vals (pred, true): 0.561, 0.545
batch losses (mrrl, rdl): 0.002726434, 0.000484314

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 550
rank avg (pred): 0.400 +- 0.224
mrr vals (pred, true): 0.069, 0.062
batch losses (mrrl, rdl): 0.0035041319, 2.98855e-05

Epoch over!
epoch time: 12.488

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 178
rank avg (pred): 0.372 +- 0.215
mrr vals (pred, true): 0.071, 0.046
batch losses (mrrl, rdl): 0.0042480528, 8.74748e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 336
rank avg (pred): 0.335 +- 0.225
mrr vals (pred, true): 0.083, 0.150
batch losses (mrrl, rdl): 0.0452980623, 0.0001307227

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 698
rank avg (pred): 0.551 +- 0.231
mrr vals (pred, true): 0.051, 0.046
batch losses (mrrl, rdl): 1.46158e-05, 0.0001979309

Epoch over!
epoch time: 12.371

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.388 +- 0.202
mrr vals (pred, true): 0.068, 0.141

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   70 	     0 	 0.08069 	 0.01876 	 m..s
   30 	     1 	 0.06082 	 0.02127 	 m..s
   16 	     2 	 0.05683 	 0.03255 	 ~...
    1 	     3 	 0.05236 	 0.03391 	 ~...
   57 	     4 	 0.07245 	 0.03971 	 m..s
   21 	     5 	 0.05812 	 0.04000 	 ~...
   26 	     6 	 0.05818 	 0.04184 	 ~...
   21 	     7 	 0.05812 	 0.04187 	 ~...
   33 	     8 	 0.06466 	 0.04197 	 ~...
   18 	     9 	 0.05762 	 0.04200 	 ~...
    2 	    10 	 0.05264 	 0.04226 	 ~...
   35 	    11 	 0.06582 	 0.04283 	 ~...
   20 	    12 	 0.05767 	 0.04316 	 ~...
    0 	    13 	 0.05182 	 0.04377 	 ~...
   51 	    14 	 0.06885 	 0.04377 	 ~...
   29 	    15 	 0.05970 	 0.04423 	 ~...
    6 	    16 	 0.05544 	 0.04465 	 ~...
   44 	    17 	 0.06775 	 0.04483 	 ~...
    7 	    18 	 0.05550 	 0.04483 	 ~...
   21 	    19 	 0.05812 	 0.04487 	 ~...
    8 	    20 	 0.05560 	 0.04581 	 ~...
   21 	    21 	 0.05812 	 0.04625 	 ~...
   56 	    22 	 0.07213 	 0.04631 	 ~...
   58 	    23 	 0.07260 	 0.04631 	 ~...
    9 	    24 	 0.05561 	 0.04648 	 ~...
   12 	    25 	 0.05634 	 0.04659 	 ~...
   15 	    26 	 0.05683 	 0.04665 	 ~...
   10 	    27 	 0.05585 	 0.04667 	 ~...
   64 	    28 	 0.07414 	 0.04696 	 ~...
   27 	    29 	 0.05919 	 0.04716 	 ~...
   19 	    30 	 0.05764 	 0.04721 	 ~...
   71 	    31 	 0.08127 	 0.04771 	 m..s
   74 	    32 	 0.08434 	 0.04790 	 m..s
   14 	    33 	 0.05679 	 0.04818 	 ~...
    4 	    34 	 0.05519 	 0.04877 	 ~...
   11 	    35 	 0.05623 	 0.04899 	 ~...
   32 	    36 	 0.06431 	 0.04909 	 ~...
   43 	    37 	 0.06751 	 0.04957 	 ~...
   76 	    38 	 0.08583 	 0.05026 	 m..s
   69 	    39 	 0.07658 	 0.05059 	 ~...
   21 	    40 	 0.05812 	 0.05122 	 ~...
   72 	    41 	 0.08337 	 0.05271 	 m..s
   28 	    42 	 0.05951 	 0.05274 	 ~...
   45 	    43 	 0.06782 	 0.05284 	 ~...
   17 	    44 	 0.05759 	 0.05352 	 ~...
   37 	    45 	 0.06597 	 0.05366 	 ~...
   68 	    46 	 0.07532 	 0.05370 	 ~...
   13 	    47 	 0.05651 	 0.05419 	 ~...
    5 	    48 	 0.05542 	 0.05535 	 ~...
   61 	    49 	 0.07307 	 0.05591 	 ~...
   34 	    50 	 0.06576 	 0.05631 	 ~...
   60 	    51 	 0.07297 	 0.06010 	 ~...
   54 	    52 	 0.06954 	 0.07471 	 ~...
   62 	    53 	 0.07344 	 0.08059 	 ~...
    3 	    54 	 0.05264 	 0.08119 	 ~...
   47 	    55 	 0.06804 	 0.08475 	 ~...
   75 	    56 	 0.08474 	 0.08802 	 ~...
   31 	    57 	 0.06428 	 0.09012 	 ~...
   48 	    58 	 0.06826 	 0.09027 	 ~...
   65 	    59 	 0.07424 	 0.09253 	 ~...
   53 	    60 	 0.06919 	 0.09437 	 ~...
   36 	    61 	 0.06589 	 0.09972 	 m..s
   52 	    62 	 0.06900 	 0.10048 	 m..s
   41 	    63 	 0.06732 	 0.10361 	 m..s
   42 	    64 	 0.06750 	 0.10395 	 m..s
   79 	    65 	 0.10425 	 0.10451 	 ~...
   78 	    66 	 0.10135 	 0.10828 	 ~...
   63 	    67 	 0.07404 	 0.10981 	 m..s
   80 	    68 	 0.10771 	 0.11170 	 ~...
   66 	    69 	 0.07484 	 0.11339 	 m..s
   77 	    70 	 0.09909 	 0.11343 	 ~...
   40 	    71 	 0.06703 	 0.11494 	 m..s
   73 	    72 	 0.08345 	 0.11595 	 m..s
   67 	    73 	 0.07505 	 0.12203 	 m..s
   50 	    74 	 0.06885 	 0.12795 	 m..s
   83 	    75 	 0.14547 	 0.12843 	 ~...
   46 	    76 	 0.06790 	 0.12923 	 m..s
   55 	    77 	 0.07057 	 0.13169 	 m..s
   38 	    78 	 0.06610 	 0.13195 	 m..s
   49 	    79 	 0.06846 	 0.14099 	 m..s
   39 	    80 	 0.06662 	 0.14238 	 m..s
   59 	    81 	 0.07295 	 0.14586 	 m..s
   85 	    82 	 0.20146 	 0.18150 	 ~...
   87 	    83 	 0.22539 	 0.18459 	 m..s
   86 	    84 	 0.20356 	 0.20213 	 ~...
   82 	    85 	 0.13196 	 0.24502 	 MISS
   84 	    86 	 0.18554 	 0.27181 	 m..s
   81 	    87 	 0.12764 	 0.31456 	 MISS
   88 	    88 	 0.24177 	 0.43149 	 MISS
   91 	    89 	 0.52294 	 0.51097 	 ~...
   90 	    90 	 0.51711 	 0.51200 	 ~...
   89 	    91 	 0.51173 	 0.52142 	 ~...
  105 	    92 	 0.53969 	 0.52273 	 ~...
  104 	    93 	 0.53901 	 0.52274 	 ~...
  103 	    94 	 0.53894 	 0.52288 	 ~...
   93 	    95 	 0.52599 	 0.52699 	 ~...
   92 	    96 	 0.52590 	 0.52851 	 ~...
   94 	    97 	 0.52670 	 0.53137 	 ~...
  111 	    98 	 0.54575 	 0.53150 	 ~...
  119 	    99 	 0.55226 	 0.53484 	 ~...
   96 	   100 	 0.52979 	 0.53485 	 ~...
  109 	   101 	 0.54470 	 0.53587 	 ~...
   95 	   102 	 0.52878 	 0.53719 	 ~...
  100 	   103 	 0.53627 	 0.53799 	 ~...
  110 	   104 	 0.54479 	 0.53971 	 ~...
   99 	   105 	 0.53530 	 0.53995 	 ~...
  117 	   106 	 0.54853 	 0.54390 	 ~...
   98 	   107 	 0.53511 	 0.54428 	 ~...
  102 	   108 	 0.53793 	 0.54931 	 ~...
  101 	   109 	 0.53782 	 0.54951 	 ~...
  106 	   110 	 0.54126 	 0.55105 	 ~...
   97 	   111 	 0.53243 	 0.55206 	 ~...
  118 	   112 	 0.54859 	 0.55235 	 ~...
  114 	   113 	 0.54689 	 0.55245 	 ~...
  108 	   114 	 0.54443 	 0.55385 	 ~...
  107 	   115 	 0.54408 	 0.55441 	 ~...
  113 	   116 	 0.54662 	 0.55778 	 ~...
  115 	   117 	 0.54705 	 0.55836 	 ~...
  112 	   118 	 0.54610 	 0.56120 	 ~...
  116 	   119 	 0.54810 	 0.56317 	 ~...
  120 	   120 	 0.55276 	 0.56546 	 ~...
==========================================
r_mrr = 0.984324038028717
r2_mrr = 0.9682279825210571
spearmanr_mrr@5 = 0.9191417098045349
spearmanr_mrr@10 = 0.9532827734947205
spearmanr_mrr@50 = 0.9899709224700928
spearmanr_mrr@100 = 0.9913895726203918
spearmanr_mrr@All = 0.9913846850395203
==========================================
test time: 0.425
Done Testing dataset UMLS
total time taken: 194.2877640724182
training time taken: 188.3011507987976
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9843)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9682)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9191)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9533)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9900)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9914)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9914)}}, 'test_loss': {'DistMult': {'UMLS': 1.6217073556326795}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_deg'}

================================================================
----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_deg
----------------------------------------------------------------
================================================================
Running on a grid of size 1
Using random seed: 8999291551734174
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [675, 1190, 754, 547, 1124, 1044, 865, 872, 435, 22, 930, 1154, 538, 829, 187, 904, 31, 264, 137, 798, 57, 1199, 308, 14, 657, 239, 635, 1148, 464, 407, 3, 34, 608, 1193, 874, 286, 81, 452, 1084, 108, 1092, 684, 1030, 498, 380, 195, 790, 1058, 431, 58, 739, 492, 460, 230, 429, 32, 152, 356, 679, 724, 1150, 189, 994, 976, 662, 83, 554, 301, 839, 766, 1013, 225, 394, 869, 200, 854, 203, 1028, 1158, 1036, 475, 402, 622, 915, 215, 636, 700, 740, 1063, 691, 950, 964, 54, 275, 841, 836, 53, 176, 655, 282, 1061, 514, 1115, 197, 614, 974, 512, 738, 806, 773, 353, 1011, 405, 449, 1054, 870, 999, 126, 925, 1128, 887]
valid_ids (0): []
train_ids (1094): [287, 646, 261, 150, 1007, 96, 260, 975, 418, 1138, 690, 21, 558, 420, 314, 807, 556, 256, 1187, 24, 1067, 342, 666, 626, 39, 155, 576, 1031, 348, 1096, 470, 1163, 408, 191, 501, 320, 664, 891, 143, 1095, 699, 654, 693, 885, 535, 843, 159, 338, 30, 871, 568, 75, 811, 1198, 183, 1145, 471, 289, 127, 827, 447, 933, 998, 712, 210, 713, 673, 91, 945, 584, 45, 844, 572, 349, 719, 727, 764, 440, 528, 645, 1039, 139, 786, 436, 333, 559, 344, 583, 232, 414, 1023, 606, 172, 178, 952, 332, 1125, 551, 1099, 109, 746, 495, 1186, 1055, 1008, 480, 1175, 965, 1118, 1104, 561, 658, 1110, 876, 650, 787, 609, 883, 617, 224, 94, 20, 161, 1083, 817, 51, 723, 41, 847, 595, 43, 1172, 849, 92, 520, 866, 973, 598, 451, 273, 294, 784, 346, 979, 400, 422, 89, 620, 7, 732, 1079, 428, 624, 1053, 672, 879, 237, 124, 831, 388, 1000, 526, 749, 771, 1068, 1091, 252, 1041, 509, 1171, 506, 316, 565, 6, 816, 863, 992, 1214, 988, 95, 823, 206, 969, 483, 669, 819, 758, 902, 317, 369, 184, 539, 567, 981, 467, 511, 634, 1042, 959, 395, 1144, 809, 789, 780, 36, 623, 505, 726, 777, 86, 1184, 540, 1161, 355, 234, 1093, 413, 752, 1022, 110, 1126, 404, 966, 111, 423, 165, 741, 985, 1064, 419, 1006, 121, 28, 493, 962, 141, 382, 1166, 337, 949, 469, 778, 1, 456, 227, 97, 284, 1089, 1177, 698, 579, 199, 890, 995, 345, 951, 2, 160, 446, 209, 37, 74, 882, 11, 603, 785, 115, 715, 755, 298, 1017, 151, 534, 769, 707, 38, 250, 877, 211, 1035, 360, 430, 262, 607, 229, 441, 895, 1155, 990, 1162, 641, 631, 987, 834, 1076, 508, 359, 205, 958, 272, 399, 795, 295, 1066, 833, 848, 69, 398, 1108, 1117, 582, 709, 309, 1057, 873, 1038, 1159, 352, 15, 926, 1212, 1050, 590, 403, 26, 1120, 642, 67, 424, 437, 174, 135, 737, 112, 198, 372, 922, 481, 310, 941, 1073, 996, 462, 543, 1195, 611, 425, 899, 1121, 47, 1014, 213, 307, 146, 1209, 868, 957, 894, 814, 136, 326, 1116, 934, 82, 689, 168, 410, 65, 961, 524, 1183, 1101, 571, 772, 628, 937, 763, 46, 19, 1181, 397, 148, 648, 49, 181, 156, 238, 490, 193, 68, 1080, 782, 253, 731, 909, 363, 536, 72, 936, 293, 1143, 842, 970, 574, 861, 730, 542, 940, 171, 835, 248, 103, 1088, 244, 531, 850, 742, 1133, 494, 670, 1129, 663, 61, 1179, 668, 466, 602, 682, 312, 240, 828, 101, 963, 276, 12, 661, 625, 1109, 1137, 1078, 529, 247, 792, 305, 245, 725, 781, 1025, 718, 274, 1009, 1146, 1056, 120, 443, 545, 881, 503, 1202, 302, 1003, 1060, 893, 900, 604, 99, 1152, 593, 564, 13, 533, 1130, 640, 1071, 299, 802, 599, 521, 325, 824, 60, 947, 916, 800, 736, 406, 55, 913, 853, 695, 118, 569, 911, 1192, 889, 163, 759, 570, 219, 341, 370, 711, 799, 804, 627, 105, 175, 458, 285, 830, 903, 504, 1142, 686, 600, 455, 217, 257, 993, 812, 1047, 167, 519, 960, 288, 463, 884, 208, 1107, 1139, 376, 207, 591, 594, 216, 703, 179, 476, 619, 415, 927, 427, 246, 465, 1059, 350, 910, 222, 361, 201, 618, 653, 1100, 489, 1168, 632, 194, 867, 813, 1164, 1156, 914, 665, 1103, 1037, 845, 704, 837, 750, 767, 1111, 1019, 235, 1134, 401, 4, 17, 647, 532, 552, 177, 459, 548, 362, 808, 140, 880, 76, 263, 324, 319, 1106, 1094, 696, 1204, 633, 474, 496, 444, 296, 375, 649, 279, 897, 1102, 577, 5, 856, 613, 530, 450, 851, 1208, 129, 596, 541, 616, 892, 838, 581, 846, 630, 610, 1051, 687, 1026, 760, 1140, 585, 1176, 939, 328, 776, 468, 281, 986, 339, 18, 643, 484, 365, 327, 29, 720, 1211, 182, 1112, 1002, 85, 1153, 79, 1151, 421, 70, 580, 122, 852, 233, 434, 1081, 921, 439, 390, 1165, 605, 144, 637, 416, 783, 445, 557, 347, 271, 292, 1147, 87, 477, 1205, 917, 343, 212, 329, 685, 300, 131, 1034, 659, 832, 1070, 1075, 932, 566, 223, 396, 1122, 549, 1210, 793, 803, 676, 1123, 857, 1131, 1033, 442, 523, 1191, 923, 639, 157, 983, 16, 1136, 678, 461, 384, 331, 1090, 259, 721, 323, 123, 747, 714, 1197, 912, 929, 745, 956, 1105, 88, 93, 133, 204, 1024, 1200, 128, 389, 214, 66, 753, 202, 864, 818, 955, 1020, 688, 1004, 190, 946, 774, 334, 928, 537, 770, 472, 729, 280, 9, 107, 656, 48, 1213, 290, 1032, 638, 255, 1049, 322, 943, 791, 671, 562, 392, 515, 63, 516, 1169, 134, 935, 756, 44, 218, 378, 249, 706, 354, 762, 651, 1097, 417, 1207, 391, 454, 313, 100, 251, 908, 196, 297, 265, 1074, 267, 587, 1119, 1157, 801, 1132, 1043, 710, 1021, 954, 652, 73, 254, 1065, 166, 944, 231, 681, 1189, 8, 491, 705, 330, 905, 386, 387, 486, 448, 25, 589, 433, 924, 701, 728, 886, 318, 862, 517, 1082, 907, 488, 980, 106, 513, 743, 525, 735, 59, 164, 80, 42, 982, 487, 967, 660, 482, 518, 919, 186, 1077, 578, 192, 810, 270, 775, 716, 479, 1160, 500, 351, 826, 102, 1040, 40, 1052, 972, 236, 268, 381, 147, 1206, 154, 119, 371, 117, 383, 291, 888, 145, 779, 278, 366, 563, 1027, 615, 825, 522, 1029, 377, 340, 357, 629, 304, 717, 412, 898, 52, 989, 241, 373, 1141, 258, 303, 510, 185, 978, 677, 597, 125, 502, 393, 71, 822, 1086, 1098, 1185, 796, 113, 269, 1196, 226, 560, 918, 667, 306, 188, 336, 1001, 586, 840, 901, 149, 942, 553, 1046, 1174, 173, 84, 612, 457, 78, 860, 744, 1173, 1085, 507, 0, 478, 1180, 920, 138, 385, 761, 374, 815, 243, 748, 132, 158, 768, 485, 1182, 1016, 77, 575, 1005, 90, 697, 1113, 1018, 931, 315, 311, 473, 1170, 453, 797, 266, 938, 1203, 1135, 708, 991, 734, 426, 644, 98, 1048, 592, 1010, 875, 694, 27, 180, 1045, 1072, 497, 116, 321, 1167, 1194, 722, 805, 953, 153, 283, 114, 1087, 1188, 335, 896, 997, 499, 573, 358, 765, 242, 1114, 794, 1012, 555, 821, 601, 368, 438, 64, 977, 33, 62, 169, 674, 757, 788, 702, 1201, 550, 142, 948, 680, 878, 968, 859, 971, 432, 544, 10, 104, 56, 1127, 221, 546, 1178, 820, 277, 1062, 364, 855, 1069, 621, 1149, 733, 228, 588, 858, 35, 220, 50, 692, 984, 1015, 379, 367, 409, 751, 162, 683, 906, 130, 170, 23, 411, 527]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8248558604546647
the save name prefix for this run is:  chkpt-ID_8248558604546647_tag_Ablation-job-blacklist-o_deg
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 780
rank avg (pred): 0.462 +- 0.004
mrr vals (pred, true): 0.016, 0.048
batch losses (mrrl, rdl): 0.0, 0.0002642403

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 428
rank avg (pred): 0.386 +- 0.018
mrr vals (pred, true): 0.019, 0.052
batch losses (mrrl, rdl): 0.0, 0.0001700034

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 112
rank avg (pred): 0.354 +- 0.182
mrr vals (pred, true): 0.102, 0.099
batch losses (mrrl, rdl): 0.0, 4.82152e-05

Epoch over!
epoch time: 12.226

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1033
rank avg (pred): 0.367 +- 0.235
mrr vals (pred, true): 0.193, 0.050
batch losses (mrrl, rdl): 0.0, 2.82578e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 105
rank avg (pred): 0.353 +- 0.247
mrr vals (pred, true): 0.230, 0.097
batch losses (mrrl, rdl): 0.0, 3.57648e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 756
rank avg (pred): 0.469 +- 0.294
mrr vals (pred, true): 0.178, 0.056
batch losses (mrrl, rdl): 0.0, 4.70186e-05

Epoch over!
epoch time: 12.096

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 907
rank avg (pred): 0.472 +- 0.303
mrr vals (pred, true): 0.180, 0.054
batch losses (mrrl, rdl): 0.0, 4.27365e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 954
rank avg (pred): 0.536 +- 0.318
mrr vals (pred, true): 0.157, 0.043
batch losses (mrrl, rdl): 0.0, 0.0002315904

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 86
rank avg (pred): 0.385 +- 0.287
mrr vals (pred, true): 0.204, 0.085
batch losses (mrrl, rdl): 0.0, 2.12265e-05

Epoch over!
epoch time: 12.115

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 134
rank avg (pred): 0.360 +- 0.264
mrr vals (pred, true): 0.205, 0.115
batch losses (mrrl, rdl): 0.0, 6.45195e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 226
rank avg (pred): 0.352 +- 0.270
mrr vals (pred, true): 0.221, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001325535

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 220
rank avg (pred): 0.367 +- 0.282
mrr vals (pred, true): 0.209, 0.043
batch losses (mrrl, rdl): 0.0, 7.94403e-05

Epoch over!
epoch time: 12.065

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 886
rank avg (pred): 0.443 +- 0.274
mrr vals (pred, true): 0.149, 0.054
batch losses (mrrl, rdl): 0.0, 1.72379e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 361
rank avg (pred): 0.318 +- 0.263
mrr vals (pred, true): 0.252, 0.153
batch losses (mrrl, rdl): 0.0, 9.14581e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 400
rank avg (pred): 0.388 +- 0.275
mrr vals (pred, true): 0.162, 0.122
batch losses (mrrl, rdl): 0.0, 0.0003264663

Epoch over!
epoch time: 11.956

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 979
rank avg (pred): 0.031 +- 0.071
mrr vals (pred, true): 0.475, 0.551
batch losses (mrrl, rdl): 0.0582644269, 2.271e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1214
rank avg (pred): 0.436 +- 0.133
mrr vals (pred, true): 0.057, 0.049
batch losses (mrrl, rdl): 0.0005578349, 4.24805e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 325
rank avg (pred): 0.319 +- 0.131
mrr vals (pred, true): 0.088, 0.087
batch losses (mrrl, rdl): 0.0142231211, 6.24468e-05

Epoch over!
epoch time: 12.411

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 932
rank avg (pred): 0.436 +- 0.071
mrr vals (pred, true): 0.024, 0.015
batch losses (mrrl, rdl): 0.0067615346, 0.0023115044

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 422
rank avg (pred): 0.346 +- 0.118
mrr vals (pred, true): 0.073, 0.053
batch losses (mrrl, rdl): 0.0053170547, 0.000203951

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 722
rank avg (pred): 0.526 +- 0.198
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 2.28686e-05, 0.000149895

Epoch over!
epoch time: 12.387

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 602
rank avg (pred): 0.478 +- 0.200
mrr vals (pred, true): 0.053, 0.046
batch losses (mrrl, rdl): 9.68243e-05, 2.65641e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 784
rank avg (pred): 0.374 +- 0.118
mrr vals (pred, true): 0.060, 0.055
batch losses (mrrl, rdl): 0.0010244714, 0.0001371667

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 80
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.513, 0.549
batch losses (mrrl, rdl): 0.012547031, 1.48511e-05

Epoch over!
epoch time: 12.199

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 639
rank avg (pred): 0.519 +- 0.264
mrr vals (pred, true): 0.046, 0.040
batch losses (mrrl, rdl): 0.0001584935, 7.20377e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 345
rank avg (pred): 0.317 +- 0.102
mrr vals (pred, true): 0.075, 0.117
batch losses (mrrl, rdl): 0.0181195009, 6.15252e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 515
rank avg (pred): 0.432 +- 0.216
mrr vals (pred, true): 0.081, 0.130
batch losses (mrrl, rdl): 0.0243253112, 0.0003231692

Epoch over!
epoch time: 12.23

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 445
rank avg (pred): 0.406 +- 0.163
mrr vals (pred, true): 0.072, 0.051
batch losses (mrrl, rdl): 0.0047261659, 5.89597e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 257
rank avg (pred): 0.007 +- 0.005
mrr vals (pred, true): 0.589, 0.561
batch losses (mrrl, rdl): 0.0079738125, 1.29138e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 794
rank avg (pred): 0.500 +- 0.212
mrr vals (pred, true): 0.051, 0.051
batch losses (mrrl, rdl): 1.45546e-05, 0.0001152963

Epoch over!
epoch time: 12.32

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 540
rank avg (pred): 0.441 +- 0.191
mrr vals (pred, true): 0.071, 0.068
batch losses (mrrl, rdl): 0.0045840307, 6.49082e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1165
rank avg (pred): 0.457 +- 0.170
mrr vals (pred, true): 0.059, 0.042
batch losses (mrrl, rdl): 0.0007814709, 3.21281e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 847
rank avg (pred): 0.502 +- 0.225
mrr vals (pred, true): 0.052, 0.043
batch losses (mrrl, rdl): 3.78394e-05, 5.42519e-05

Epoch over!
epoch time: 12.271

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 399
rank avg (pred): 0.304 +- 0.096
mrr vals (pred, true): 0.070, 0.129
batch losses (mrrl, rdl): 0.0354684182, 9.40425e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 443
rank avg (pred): 0.374 +- 0.183
mrr vals (pred, true): 0.088, 0.045
batch losses (mrrl, rdl): 0.0143679045, 8.38702e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 461
rank avg (pred): 0.408 +- 0.188
mrr vals (pred, true): 0.077, 0.050
batch losses (mrrl, rdl): 0.0074398983, 2.73881e-05

Epoch over!
epoch time: 12.282

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 139
rank avg (pred): 0.416 +- 0.184
mrr vals (pred, true): 0.075, 0.141
batch losses (mrrl, rdl): 0.0437988304, 0.0007416902

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 835
rank avg (pred): 0.090 +- 0.064
mrr vals (pred, true): 0.254, 0.212
batch losses (mrrl, rdl): 0.0176179111, 0.0004481194

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 630
rank avg (pred): 0.538 +- 0.244
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 1.43955e-05, 4.13043e-05

Epoch over!
epoch time: 12.233

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 103
rank avg (pred): 0.209 +- 0.088
mrr vals (pred, true): 0.087, 0.146
batch losses (mrrl, rdl): 0.0350081623, 9.14988e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 919
rank avg (pred): 0.580 +- 0.271
mrr vals (pred, true): 0.046, 0.020
batch losses (mrrl, rdl): 0.0001844394, 0.0001040305

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 487
rank avg (pred): 0.112 +- 0.079
mrr vals (pred, true): 0.215, 0.213
batch losses (mrrl, rdl): 3.32195e-05, 0.0003337488

Epoch over!
epoch time: 12.402

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 617
rank avg (pred): 0.495 +- 0.219
mrr vals (pred, true): 0.050, 0.043
batch losses (mrrl, rdl): 2.2111e-06, 1.78367e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 370
rank avg (pred): 0.263 +- 0.100
mrr vals (pred, true): 0.082, 0.117
batch losses (mrrl, rdl): 0.0126135778, 4.57339e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 623
rank avg (pred): 0.461 +- 0.199
mrr vals (pred, true): 0.059, 0.042
batch losses (mrrl, rdl): 0.0007775345, 2.22134e-05

Epoch over!
epoch time: 12.167

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.520 +- 0.234
mrr vals (pred, true): 0.049, 0.046

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   35 	     0 	 0.05672 	 0.01569 	 m..s
   25 	     1 	 0.05386 	 0.01573 	 m..s
   15 	     2 	 0.05140 	 0.03661 	 ~...
   27 	     3 	 0.05445 	 0.03702 	 ~...
   14 	     4 	 0.05137 	 0.03773 	 ~...
    5 	     5 	 0.04854 	 0.03793 	 ~...
   61 	     6 	 0.07506 	 0.03858 	 m..s
   72 	     7 	 0.07733 	 0.03971 	 m..s
    3 	     8 	 0.04838 	 0.04101 	 ~...
   75 	     9 	 0.07824 	 0.04174 	 m..s
   10 	    10 	 0.04994 	 0.04194 	 ~...
   53 	    11 	 0.07092 	 0.04205 	 ~...
   81 	    12 	 0.08135 	 0.04229 	 m..s
   68 	    13 	 0.07692 	 0.04283 	 m..s
    1 	    14 	 0.04638 	 0.04316 	 ~...
   20 	    15 	 0.05293 	 0.04402 	 ~...
   23 	    16 	 0.05342 	 0.04440 	 ~...
   21 	    17 	 0.05324 	 0.04470 	 ~...
   18 	    18 	 0.05181 	 0.04475 	 ~...
    7 	    19 	 0.04898 	 0.04519 	 ~...
   45 	    20 	 0.06741 	 0.04531 	 ~...
   22 	    21 	 0.05337 	 0.04555 	 ~...
   33 	    22 	 0.05551 	 0.04562 	 ~...
   29 	    23 	 0.05465 	 0.04580 	 ~...
   11 	    24 	 0.05015 	 0.04581 	 ~...
   67 	    25 	 0.07644 	 0.04587 	 m..s
   39 	    26 	 0.06463 	 0.04594 	 ~...
    6 	    27 	 0.04867 	 0.04595 	 ~...
   62 	    28 	 0.07516 	 0.04610 	 ~...
   30 	    29 	 0.05471 	 0.04624 	 ~...
    4 	    30 	 0.04839 	 0.04625 	 ~...
    2 	    31 	 0.04767 	 0.04665 	 ~...
   76 	    32 	 0.07901 	 0.04696 	 m..s
   24 	    33 	 0.05342 	 0.04712 	 ~...
   19 	    34 	 0.05215 	 0.04721 	 ~...
   16 	    35 	 0.05152 	 0.04738 	 ~...
   78 	    36 	 0.08010 	 0.04750 	 m..s
   70 	    37 	 0.07728 	 0.04754 	 ~...
   56 	    38 	 0.07252 	 0.04771 	 ~...
   36 	    39 	 0.05772 	 0.04783 	 ~...
   58 	    40 	 0.07450 	 0.04790 	 ~...
   26 	    41 	 0.05436 	 0.04804 	 ~...
   32 	    42 	 0.05522 	 0.04860 	 ~...
   65 	    43 	 0.07566 	 0.04909 	 ~...
   69 	    44 	 0.07722 	 0.04955 	 ~...
   46 	    45 	 0.06835 	 0.04995 	 ~...
   37 	    46 	 0.06319 	 0.05029 	 ~...
    0 	    47 	 0.04399 	 0.05033 	 ~...
   77 	    48 	 0.07903 	 0.05036 	 ~...
   49 	    49 	 0.06928 	 0.05054 	 ~...
   71 	    50 	 0.07729 	 0.05073 	 ~...
    8 	    51 	 0.04958 	 0.05143 	 ~...
   63 	    52 	 0.07530 	 0.05168 	 ~...
   12 	    53 	 0.05031 	 0.05207 	 ~...
   48 	    54 	 0.06900 	 0.05268 	 ~...
   51 	    55 	 0.07015 	 0.05331 	 ~...
   34 	    56 	 0.05670 	 0.05344 	 ~...
   31 	    57 	 0.05509 	 0.05344 	 ~...
   57 	    58 	 0.07399 	 0.05370 	 ~...
   13 	    59 	 0.05120 	 0.05485 	 ~...
   28 	    60 	 0.05452 	 0.05551 	 ~...
   38 	    61 	 0.06449 	 0.05593 	 ~...
   59 	    62 	 0.07478 	 0.05812 	 ~...
    9 	    63 	 0.04974 	 0.05983 	 ~...
   50 	    64 	 0.06931 	 0.05990 	 ~...
   40 	    65 	 0.06520 	 0.07057 	 ~...
   42 	    66 	 0.06579 	 0.07225 	 ~...
   83 	    67 	 0.08243 	 0.08200 	 ~...
   79 	    68 	 0.08036 	 0.09027 	 ~...
   82 	    69 	 0.08218 	 0.09597 	 ~...
   43 	    70 	 0.06594 	 0.09972 	 m..s
   54 	    71 	 0.07107 	 0.10026 	 ~...
   55 	    72 	 0.07161 	 0.10286 	 m..s
   74 	    73 	 0.07812 	 0.10361 	 ~...
   80 	    74 	 0.08070 	 0.11105 	 m..s
   52 	    75 	 0.07084 	 0.11140 	 m..s
   17 	    76 	 0.05170 	 0.11373 	 m..s
   66 	    77 	 0.07570 	 0.12025 	 m..s
   88 	    78 	 0.11858 	 0.12212 	 ~...
   73 	    79 	 0.07794 	 0.12426 	 m..s
   84 	    80 	 0.08269 	 0.12660 	 m..s
   87 	    81 	 0.11759 	 0.12681 	 ~...
   47 	    82 	 0.06879 	 0.13160 	 m..s
   41 	    83 	 0.06534 	 0.13169 	 m..s
   60 	    84 	 0.07479 	 0.13476 	 m..s
   86 	    85 	 0.11452 	 0.14093 	 ~...
   64 	    86 	 0.07555 	 0.14243 	 m..s
   44 	    87 	 0.06702 	 0.15690 	 m..s
   85 	    88 	 0.11338 	 0.16048 	 m..s
   93 	    89 	 0.25851 	 0.17970 	 m..s
   89 	    90 	 0.14092 	 0.18507 	 m..s
   90 	    91 	 0.14186 	 0.19106 	 m..s
   91 	    92 	 0.24096 	 0.20395 	 m..s
   92 	    93 	 0.24267 	 0.26153 	 ~...
  103 	    94 	 0.50357 	 0.50804 	 ~...
   95 	    95 	 0.43469 	 0.51881 	 m..s
   94 	    96 	 0.43146 	 0.51956 	 m..s
  102 	    97 	 0.50249 	 0.52041 	 ~...
  106 	    98 	 0.51111 	 0.52260 	 ~...
   99 	    99 	 0.50011 	 0.52273 	 ~...
  100 	   100 	 0.50071 	 0.52810 	 ~...
  101 	   101 	 0.50238 	 0.52908 	 ~...
  111 	   102 	 0.51911 	 0.53020 	 ~...
   97 	   103 	 0.45285 	 0.53246 	 m..s
  105 	   104 	 0.51073 	 0.53566 	 ~...
   96 	   105 	 0.44268 	 0.53600 	 m..s
  108 	   106 	 0.51759 	 0.53652 	 ~...
  117 	   107 	 0.53304 	 0.53662 	 ~...
  114 	   108 	 0.52129 	 0.53728 	 ~...
  110 	   109 	 0.51823 	 0.54080 	 ~...
  112 	   110 	 0.52005 	 0.54126 	 ~...
  109 	   111 	 0.51778 	 0.54517 	 ~...
  113 	   112 	 0.52053 	 0.54522 	 ~...
  104 	   113 	 0.50952 	 0.54638 	 m..s
  115 	   114 	 0.52237 	 0.54676 	 ~...
   98 	   115 	 0.49358 	 0.54835 	 m..s
  107 	   116 	 0.51495 	 0.54931 	 m..s
  120 	   117 	 0.56416 	 0.54935 	 ~...
  118 	   118 	 0.54423 	 0.54936 	 ~...
  116 	   119 	 0.53065 	 0.55089 	 ~...
  119 	   120 	 0.54705 	 0.56198 	 ~...
==========================================
r_mrr = 0.9888067245483398
r2_mrr = 0.9735592007637024
spearmanr_mrr@5 = 0.8891263008117676
spearmanr_mrr@10 = 0.9125010371208191
spearmanr_mrr@50 = 0.9934613108634949
spearmanr_mrr@100 = 0.9929046034812927
spearmanr_mrr@All = 0.993350625038147
==========================================
test time: 0.402
Done Testing dataset UMLS
total time taken: 189.87374567985535
training time taken: 183.8274974822998
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9888)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9736)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8891)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9125)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9935)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9929)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9934)}}, 'test_loss': {'DistMult': {'UMLS': 1.2060919708510482}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_deg'}

=================================================================
-----------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-p_freq
-----------------------------------------------------------------
=================================================================
Running on a grid of size 1
Using random seed: 7060350119780399
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [5, 805, 286, 1143, 435, 934, 1022, 133, 95, 58, 880, 1051, 504, 154, 553, 563, 717, 115, 110, 634, 65, 73, 198, 1196, 673, 712, 1004, 277, 584, 87, 278, 774, 854, 1016, 543, 411, 1060, 691, 929, 643, 1171, 246, 454, 758, 671, 452, 1081, 931, 9, 517, 849, 713, 838, 353, 1009, 937, 121, 153, 464, 692, 965, 417, 61, 847, 1191, 718, 275, 397, 1203, 247, 702, 366, 619, 788, 190, 684, 1121, 651, 344, 412, 979, 1189, 626, 163, 430, 70, 336, 525, 611, 865, 193, 169, 885, 1063, 1109, 54, 1002, 75, 436, 943, 891, 1206, 839, 3, 1182, 125, 372, 935, 475, 941, 697, 94, 162, 157, 349, 215, 164, 745, 382, 920, 882]
valid_ids (0): []
train_ids (1094): [704, 977, 1027, 195, 593, 1011, 723, 374, 1153, 1170, 476, 1043, 203, 676, 829, 1197, 602, 24, 342, 566, 730, 981, 22, 565, 354, 1093, 279, 334, 1086, 1126, 680, 338, 884, 40, 251, 777, 12, 1024, 42, 472, 996, 401, 227, 1095, 570, 390, 487, 310, 952, 524, 714, 963, 883, 1032, 118, 288, 742, 479, 808, 313, 600, 1018, 922, 1067, 518, 1089, 1144, 638, 314, 721, 596, 1124, 26, 1213, 796, 761, 62, 188, 519, 261, 1173, 262, 1155, 1106, 33, 670, 273, 533, 243, 940, 999, 72, 498, 375, 878, 710, 819, 1005, 863, 373, 564, 751, 968, 509, 844, 358, 705, 347, 630, 370, 269, 659, 298, 421, 511, 155, 1042, 1167, 597, 821, 1154, 441, 1122, 181, 287, 816, 31, 822, 914, 581, 424, 232, 1158, 495, 1147, 8, 689, 780, 257, 1015, 781, 749, 936, 677, 368, 44, 962, 924, 141, 926, 221, 96, 648, 1090, 457, 541, 100, 520, 143, 384, 842, 471, 0, 523, 453, 1113, 1099, 1142, 136, 1070, 901, 84, 127, 728, 848, 202, 802, 423, 501, 398, 477, 1161, 1199, 268, 657, 151, 776, 506, 944, 337, 798, 522, 173, 515, 365, 402, 210, 348, 381, 199, 99, 325, 285, 507, 655, 1119, 894, 703, 1098, 1052, 947, 988, 1148, 893, 661, 679, 1149, 399, 625, 39, 473, 272, 1177, 1118, 306, 624, 224, 492, 152, 116, 632, 732, 574, 289, 172, 910, 1207, 209, 254, 429, 332, 711, 793, 1053, 991, 956, 1120, 606, 330, 682, 818, 779, 434, 1047, 258, 367, 1136, 888, 311, 204, 1078, 836, 55, 736, 200, 415, 183, 890, 1160, 244, 201, 1056, 989, 1187, 1, 174, 845, 177, 823, 803, 315, 985, 864, 902, 1037, 752, 1025, 862, 1039, 953, 343, 361, 1192, 571, 57, 416, 1146, 391, 160, 456, 165, 63, 535, 403, 521, 800, 396, 117, 276, 431, 407, 496, 628, 1048, 1163, 499, 585, 556, 539, 250, 1017, 66, 978, 139, 967, 30, 304, 29, 771, 966, 120, 971, 785, 753, 147, 1133, 983, 1097, 1083, 420, 91, 846, 355, 872, 1080, 861, 239, 1190, 265, 877, 644, 948, 292, 783, 598, 159, 178, 345, 629, 1059, 1102, 681, 744, 414, 589, 573, 876, 1034, 986, 896, 184, 1050, 1058, 807, 175, 167, 132, 329, 1145, 674, 488, 824, 294, 898, 83, 10, 911, 995, 50, 378, 1201, 1175, 171, 545, 256, 303, 46, 129, 510, 913, 961, 757, 1210, 897, 90, 992, 908, 56, 623, 119, 892, 792, 128, 1033, 765, 59, 568, 791, 1020, 25, 49, 124, 1044, 969, 383, 960, 413, 1214, 1074, 470, 86, 512, 724, 380, 97, 137, 98, 357, 806, 104, 312, 1031, 1007, 690, 52, 35, 1092, 907, 32, 820, 85, 516, 706, 255, 814, 871, 906, 1066, 938, 1021, 609, 588, 219, 812, 1195, 1029, 1019, 895, 997, 536, 642, 1165, 494, 222, 558, 1030, 784, 1172, 1091, 1069, 662, 592, 738, 833, 333, 69, 653, 260, 899, 238, 449, 616, 231, 1138, 851, 850, 404, 701, 1073, 1100, 4, 928, 595, 346, 664, 466, 603, 1188, 933, 1115, 437, 1134, 675, 528, 392, 426, 686, 142, 249, 958, 750, 950, 841, 830, 582, 694, 319, 668, 107, 480, 252, 537, 317, 647, 503, 340, 130, 811, 15, 957, 557, 1023, 81, 1176, 916, 295, 734, 23, 170, 264, 284, 1151, 185, 725, 612, 410, 508, 881, 707, 970, 562, 405, 708, 267, 280, 546, 551, 731, 442, 263, 804, 1041, 205, 78, 932, 485, 283, 461, 554, 18, 608, 1127, 1162, 666, 775, 1064, 234, 719, 356, 422, 220, 229, 406, 605, 591, 866, 1204, 586, 236, 610, 1085, 909, 534, 235, 741, 318, 613, 60, 856, 870, 645, 633, 481, 601, 538, 951, 74, 326, 631, 1159, 835, 327, 700, 460, 1075, 514, 206, 650, 77, 458, 868, 179, 857, 927, 555, 1026, 832, 307, 561, 212, 1084, 2, 76, 770, 134, 1082, 196, 377, 1096, 1036, 646, 641, 813, 620, 1065, 530, 1186, 14, 359, 639, 1168, 576, 754, 635, 148, 1200, 855, 575, 604, 1179, 843, 1012, 735, 579, 443, 1152, 1107, 580, 695, 858, 917, 739, 131, 1157, 722, 385, 1088, 305, 772, 1208, 875, 1185, 51, 213, 683, 696, 955, 905, 799, 187, 240, 113, 439, 1003, 446, 297, 879, 89, 308, 678, 293, 976, 1150, 331, 994, 6, 138, 740, 726, 1045, 590, 889, 316, 320, 1116, 1049, 400, 621, 699, 497, 1117, 1001, 21, 194, 489, 778, 904, 688, 1132, 409, 502, 20, 161, 433, 617, 105, 270, 149, 1014, 1141, 242, 321, 787, 903, 782, 371, 408, 544, 1178, 853, 873, 1181, 919, 448, 290, 7, 972, 607, 687, 451, 109, 376, 114, 225, 11, 1040, 930, 53, 786, 918, 186, 826, 746, 82, 150, 166, 716, 693, 615, 500, 1166, 795, 360, 432, 1130, 767, 428, 867, 445, 1035, 363, 925, 1104, 37, 387, 1131, 1076, 28, 68, 351, 145, 36, 815, 984, 685, 1174, 302, 447, 228, 663, 658, 324, 300, 17, 218, 825, 1125, 1055, 483, 1209, 747, 323, 762, 474, 140, 1008, 38, 831, 395, 923, 103, 946, 810, 80, 743, 531, 362, 168, 259, 230, 146, 637, 484, 886, 817, 764, 773, 869, 945, 13, 982, 1183, 720, 733, 266, 660, 939, 462, 189, 282, 364, 1028, 766, 1110, 418, 809, 559, 350, 135, 1068, 1071, 223, 440, 438, 1103, 990, 527, 64, 301, 614, 993, 101, 1180, 245, 964, 915, 1205, 1062, 794, 123, 191, 1013, 912, 1094, 1038, 790, 532, 542, 1000, 122, 217, 698, 1202, 1105, 1072, 328, 837, 1193, 1198, 102, 636, 1077, 709, 1061, 973, 106, 197, 47, 335, 214, 797, 1079, 840, 526, 71, 468, 126, 216, 748, 455, 552, 1114, 763, 388, 1156, 560, 207, 954, 550, 789, 393, 467, 656, 1135, 860, 425, 1054, 112, 975, 769, 649, 1046, 192, 801, 41, 1164, 1101, 834, 34, 1128, 88, 980, 921, 296, 942, 998, 465, 291, 322, 341, 180, 1010, 427, 727, 529, 493, 852, 271, 1123, 665, 622, 450, 486, 19, 233, 578, 309, 547, 572, 1006, 1129, 389, 92, 1140, 1194, 79, 1108, 729, 874, 419, 182, 491, 241, 93, 226, 672, 379, 828, 43, 654, 1137, 339, 859, 211, 482, 1184, 594, 490, 469, 652, 111, 759, 768, 760, 987, 577, 827, 176, 459, 549, 478, 394, 16, 756, 386, 281, 583, 1169, 1211, 248, 45, 156, 444, 108, 1057, 67, 887, 715, 587, 755, 48, 567, 144, 640, 737, 1112, 208, 1087, 959, 1139, 352, 569, 618, 253, 299, 974, 548, 599, 1212, 669, 369, 900, 667, 274, 540, 27, 463, 1111, 627, 949, 237, 513, 158, 505]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  2593247042893800
the save name prefix for this run is:  chkpt-ID_2593247042893800_tag_Ablation-job-blacklist-p_freq
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1091
rank avg (pred): 0.468 +- 0.003
mrr vals (pred, true): 0.016, 0.096
batch losses (mrrl, rdl): 0.0, 0.0003937513

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 570
rank avg (pred): 0.456 +- 0.007
mrr vals (pred, true): 0.016, 0.046
batch losses (mrrl, rdl): 0.0, 9.93244e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 537
rank avg (pred): 0.412 +- 0.009
mrr vals (pred, true): 0.018, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001481861

Epoch over!
epoch time: 12.298

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 228
rank avg (pred): 0.419 +- 0.011
mrr vals (pred, true): 0.018, 0.044
batch losses (mrrl, rdl): 0.0, 8.63301e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 910
rank avg (pred): 0.463 +- 0.011
mrr vals (pred, true): 0.016, 0.078
batch losses (mrrl, rdl): 0.0, 0.0002287045

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 212
rank avg (pred): 0.363 +- 0.192
mrr vals (pred, true): 0.132, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001049501

Epoch over!
epoch time: 11.992

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 446
rank avg (pred): 0.340 +- 0.208
mrr vals (pred, true): 0.196, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001238053

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 823
rank avg (pred): 0.088 +- 0.057
mrr vals (pred, true): 0.293, 0.426
batch losses (mrrl, rdl): 0.0, 1.04934e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 925
rank avg (pred): 0.523 +- 0.328
mrr vals (pred, true): 0.220, 0.016
batch losses (mrrl, rdl): 0.0, 0.0008253168

Epoch over!
epoch time: 12.037

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1102
rank avg (pred): 0.359 +- 0.239
mrr vals (pred, true): 0.247, 0.081
batch losses (mrrl, rdl): 0.0, 2.53874e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 200
rank avg (pred): 0.351 +- 0.240
mrr vals (pred, true): 0.277, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001101077

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 629
rank avg (pred): 0.410 +- 0.278
mrr vals (pred, true): 0.274, 0.039
batch losses (mrrl, rdl): 0.0, 3.87112e-05

Epoch over!
epoch time: 12.069

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 470
rank avg (pred): 0.351 +- 0.244
mrr vals (pred, true): 0.284, 0.053
batch losses (mrrl, rdl): 0.0, 0.0001074369

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 223
rank avg (pred): 0.348 +- 0.245
mrr vals (pred, true): 0.290, 0.052
batch losses (mrrl, rdl): 0.0, 8.73156e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 740
rank avg (pred): 0.075 +- 0.055
mrr vals (pred, true): 0.362, 0.536
batch losses (mrrl, rdl): 0.0, 3.18516e-05

Epoch over!
epoch time: 11.958

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 548
rank avg (pred): 0.308 +- 0.216
mrr vals (pred, true): 0.294, 0.132
batch losses (mrrl, rdl): 0.2603569329, 4.165e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1148
rank avg (pred): 0.185 +- 0.121
mrr vals (pred, true): 0.147, 0.141
batch losses (mrrl, rdl): 0.0003289478, 0.0001785102

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 642
rank avg (pred): 0.619 +- 0.179
mrr vals (pred, true): 0.044, 0.045
batch losses (mrrl, rdl): 0.0004206465, 0.0004450023

Epoch over!
epoch time: 12.304

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 174
rank avg (pred): 0.462 +- 0.144
mrr vals (pred, true): 0.064, 0.048
batch losses (mrrl, rdl): 0.0018350021, 4.96934e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1069
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.568, 0.548
batch losses (mrrl, rdl): 0.0040773242, 1.83154e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 84
rank avg (pred): 0.342 +- 0.150
mrr vals (pred, true): 0.068, 0.132
batch losses (mrrl, rdl): 0.040140003, 0.0001127442

Epoch over!
epoch time: 12.232

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 300
rank avg (pred): 0.013 +- 0.012
mrr vals (pred, true): 0.546, 0.539
batch losses (mrrl, rdl): 0.0004643632, 1.53267e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 913
rank avg (pred): 0.484 +- 0.143
mrr vals (pred, true): 0.059, 0.057
batch losses (mrrl, rdl): 0.0008552646, 9.62503e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 763
rank avg (pred): 0.412 +- 0.121
mrr vals (pred, true): 0.057, 0.034
batch losses (mrrl, rdl): 0.0004376594, 0.0003827356

Epoch over!
epoch time: 12.217

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 359
rank avg (pred): 0.369 +- 0.174
mrr vals (pred, true): 0.087, 0.075
batch losses (mrrl, rdl): 0.0136395525, 3.47259e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 556
rank avg (pred): 0.268 +- 0.142
mrr vals (pred, true): 0.081, 0.062
batch losses (mrrl, rdl): 0.0096876826, 0.0005470795

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1123
rank avg (pred): 0.362 +- 0.136
mrr vals (pred, true): 0.068, 0.047
batch losses (mrrl, rdl): 0.0033740238, 0.0002050047

Epoch over!
epoch time: 12.068

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 533
rank avg (pred): 0.370 +- 0.123
mrr vals (pred, true): 0.069, 0.103
batch losses (mrrl, rdl): 0.0110313529, 9.26212e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 431
rank avg (pred): 0.318 +- 0.166
mrr vals (pred, true): 0.073, 0.042
batch losses (mrrl, rdl): 0.0053679566, 0.0003649449

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 928
rank avg (pred): 0.579 +- 0.219
mrr vals (pred, true): 0.050, 0.021
batch losses (mrrl, rdl): 7.045e-07, 0.0002678335

Epoch over!
epoch time: 12.058

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 310
rank avg (pred): 0.017 +- 0.016
mrr vals (pred, true): 0.544, 0.551
batch losses (mrrl, rdl): 0.000497459, 9.4836e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 476
rank avg (pred): 0.338 +- 0.155
mrr vals (pred, true): 0.072, 0.048
batch losses (mrrl, rdl): 0.0047894428, 0.0003189566

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 51
rank avg (pred): 0.039 +- 0.039
mrr vals (pred, true): 0.484, 0.520
batch losses (mrrl, rdl): 0.0128435213, 1.8387e-06

Epoch over!
epoch time: 12.277

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 116
rank avg (pred): 0.561 +- 0.155
mrr vals (pred, true): 0.056, 0.093
batch losses (mrrl, rdl): 0.0003810197, 0.001246298

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1042
rank avg (pred): 0.356 +- 0.142
mrr vals (pred, true): 0.080, 0.048
batch losses (mrrl, rdl): 0.0091254413, 0.0001842924

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 924
rank avg (pred): 0.504 +- 0.165
mrr vals (pred, true): 0.051, 0.016
batch losses (mrrl, rdl): 3.4789e-06, 0.0012600586

Epoch over!
epoch time: 12.158

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 82
rank avg (pred): 0.351 +- 0.142
mrr vals (pred, true): 0.069, 0.080
batch losses (mrrl, rdl): 0.0034701922, 3.73671e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 138
rank avg (pred): 0.385 +- 0.171
mrr vals (pred, true): 0.068, 0.106
batch losses (mrrl, rdl): 0.0144028682, 0.0001871025

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 777
rank avg (pred): 0.579 +- 0.237
mrr vals (pred, true): 0.055, 0.051
batch losses (mrrl, rdl): 0.0002550912, 0.0001328769

Epoch over!
epoch time: 12.251

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 260
rank avg (pred): 0.029 +- 0.029
mrr vals (pred, true): 0.521, 0.562
batch losses (mrrl, rdl): 0.0170521736, 2.757e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 370
rank avg (pred): 0.413 +- 0.128
mrr vals (pred, true): 0.066, 0.117
batch losses (mrrl, rdl): 0.0256923698, 0.0004197399

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 722
rank avg (pred): 0.600 +- 0.209
mrr vals (pred, true): 0.052, 0.045
batch losses (mrrl, rdl): 3.56705e-05, 0.0004504485

Epoch over!
epoch time: 12.229

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 816
rank avg (pred): 0.321 +- 0.159
mrr vals (pred, true): 0.079, 0.037
batch losses (mrrl, rdl): 0.0082571693, 0.0002474353

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 432
rank avg (pred): 0.429 +- 0.148
mrr vals (pred, true): 0.064, 0.039
batch losses (mrrl, rdl): 0.0018687586, 3.09005e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 752
rank avg (pred): 0.095 +- 0.095
mrr vals (pred, true): 0.440, 0.426
batch losses (mrrl, rdl): 0.0018980458, 1.82826e-05

Epoch over!
epoch time: 12.071

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.018 +- 0.019
mrr vals (pred, true): 0.568, 0.522

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05204 	 0.01527 	 m..s
    4 	     1 	 0.05511 	 0.01574 	 m..s
    5 	     2 	 0.05532 	 0.01609 	 m..s
    1 	     3 	 0.05221 	 0.01638 	 m..s
    2 	     4 	 0.05223 	 0.01667 	 m..s
    8 	     5 	 0.05567 	 0.01697 	 m..s
    3 	     6 	 0.05348 	 0.01923 	 m..s
   10 	     7 	 0.05590 	 0.01975 	 m..s
   20 	     8 	 0.05880 	 0.02851 	 m..s
   31 	     9 	 0.06192 	 0.03798 	 ~...
   48 	    10 	 0.07996 	 0.03972 	 m..s
   40 	    11 	 0.06916 	 0.04018 	 ~...
   44 	    12 	 0.07292 	 0.04022 	 m..s
   70 	    13 	 0.09518 	 0.04080 	 m..s
   17 	    14 	 0.05853 	 0.04167 	 ~...
   78 	    15 	 0.09669 	 0.04229 	 m..s
   71 	    16 	 0.09524 	 0.04294 	 m..s
   22 	    17 	 0.05967 	 0.04302 	 ~...
   37 	    18 	 0.06790 	 0.04318 	 ~...
   72 	    19 	 0.09525 	 0.04416 	 m..s
   15 	    20 	 0.05810 	 0.04418 	 ~...
   35 	    21 	 0.06639 	 0.04423 	 ~...
   91 	    22 	 0.10474 	 0.04427 	 m..s
   24 	    23 	 0.05981 	 0.04444 	 ~...
   13 	    24 	 0.05714 	 0.04472 	 ~...
   36 	    25 	 0.06639 	 0.04519 	 ~...
    6 	    26 	 0.05545 	 0.04547 	 ~...
   62 	    27 	 0.09447 	 0.04587 	 m..s
   33 	    28 	 0.06570 	 0.04625 	 ~...
   34 	    29 	 0.06638 	 0.04665 	 ~...
   18 	    30 	 0.05861 	 0.04667 	 ~...
   97 	    31 	 0.11986 	 0.04683 	 m..s
   81 	    32 	 0.09788 	 0.04692 	 m..s
   77 	    33 	 0.09646 	 0.04700 	 m..s
    9 	    34 	 0.05588 	 0.04716 	 ~...
   93 	    35 	 0.10628 	 0.04758 	 m..s
   23 	    36 	 0.05979 	 0.04768 	 ~...
   16 	    37 	 0.05846 	 0.04783 	 ~...
   80 	    38 	 0.09771 	 0.04790 	 m..s
   38 	    39 	 0.06818 	 0.04819 	 ~...
   57 	    40 	 0.08939 	 0.04856 	 m..s
    7 	    41 	 0.05565 	 0.04866 	 ~...
   87 	    42 	 0.10050 	 0.04909 	 m..s
   39 	    43 	 0.06840 	 0.04916 	 ~...
   42 	    44 	 0.07239 	 0.04921 	 ~...
   41 	    45 	 0.07175 	 0.04981 	 ~...
   45 	    46 	 0.07611 	 0.04983 	 ~...
   21 	    47 	 0.05881 	 0.04986 	 ~...
   86 	    48 	 0.09958 	 0.05054 	 m..s
   19 	    49 	 0.05878 	 0.05102 	 ~...
   26 	    50 	 0.05993 	 0.05108 	 ~...
   62 	    51 	 0.09447 	 0.05110 	 m..s
   14 	    52 	 0.05765 	 0.05111 	 ~...
   43 	    53 	 0.07288 	 0.05122 	 ~...
   30 	    54 	 0.06161 	 0.05139 	 ~...
   67 	    55 	 0.09500 	 0.05158 	 m..s
   12 	    56 	 0.05608 	 0.05178 	 ~...
   50 	    57 	 0.08342 	 0.05271 	 m..s
   52 	    58 	 0.08408 	 0.05340 	 m..s
   27 	    59 	 0.06042 	 0.05344 	 ~...
   83 	    60 	 0.09880 	 0.05348 	 m..s
   25 	    61 	 0.05981 	 0.05352 	 ~...
   95 	    62 	 0.10891 	 0.05362 	 m..s
   46 	    63 	 0.07714 	 0.05365 	 ~...
   69 	    64 	 0.09515 	 0.05370 	 m..s
   49 	    65 	 0.08053 	 0.05468 	 ~...
   32 	    66 	 0.06407 	 0.05504 	 ~...
   28 	    67 	 0.06070 	 0.05551 	 ~...
   51 	    68 	 0.08367 	 0.05681 	 ~...
   29 	    69 	 0.06092 	 0.06067 	 ~...
   55 	    70 	 0.08684 	 0.06286 	 ~...
   75 	    71 	 0.09551 	 0.07545 	 ~...
   64 	    72 	 0.09448 	 0.08597 	 ~...
   53 	    73 	 0.08470 	 0.08802 	 ~...
   74 	    74 	 0.09549 	 0.09096 	 ~...
   61 	    75 	 0.09330 	 0.09399 	 ~...
   68 	    76 	 0.09506 	 0.09437 	 ~...
   76 	    77 	 0.09635 	 0.09597 	 ~...
   88 	    78 	 0.10086 	 0.10126 	 ~...
   47 	    79 	 0.07995 	 0.10197 	 ~...
   60 	    80 	 0.09283 	 0.10238 	 ~...
   98 	    81 	 0.12099 	 0.10249 	 ~...
   82 	    82 	 0.09847 	 0.10275 	 ~...
   54 	    83 	 0.08678 	 0.10391 	 ~...
   59 	    84 	 0.09131 	 0.10451 	 ~...
   94 	    85 	 0.10724 	 0.10861 	 ~...
   79 	    86 	 0.09687 	 0.11170 	 ~...
   11 	    87 	 0.05592 	 0.11373 	 m..s
   84 	    88 	 0.09906 	 0.11551 	 ~...
   66 	    89 	 0.09465 	 0.11882 	 ~...
   96 	    90 	 0.10947 	 0.12203 	 ~...
   73 	    91 	 0.09532 	 0.12327 	 ~...
   56 	    92 	 0.08797 	 0.12523 	 m..s
   92 	    93 	 0.10495 	 0.13014 	 ~...
   65 	    94 	 0.09448 	 0.13138 	 m..s
   99 	    95 	 0.13267 	 0.13948 	 ~...
   90 	    96 	 0.10312 	 0.14118 	 m..s
   85 	    97 	 0.09913 	 0.14995 	 m..s
   58 	    98 	 0.08980 	 0.15403 	 m..s
   89 	    99 	 0.10121 	 0.17231 	 m..s
  100 	   100 	 0.17295 	 0.20213 	 ~...
  101 	   101 	 0.20008 	 0.30766 	 MISS
  102 	   102 	 0.50922 	 0.51394 	 ~...
  108 	   103 	 0.54320 	 0.52120 	 ~...
  114 	   104 	 0.56765 	 0.52247 	 m..s
  105 	   105 	 0.53665 	 0.52807 	 ~...
  106 	   106 	 0.53736 	 0.52810 	 ~...
  104 	   107 	 0.53591 	 0.52908 	 ~...
  103 	   108 	 0.53352 	 0.53116 	 ~...
  107 	   109 	 0.53789 	 0.53485 	 ~...
  112 	   110 	 0.55566 	 0.54080 	 ~...
  109 	   111 	 0.54993 	 0.54126 	 ~...
  113 	   112 	 0.56011 	 0.54522 	 ~...
  117 	   113 	 0.57461 	 0.54894 	 ~...
  116 	   114 	 0.57346 	 0.54931 	 ~...
  120 	   115 	 0.61549 	 0.55105 	 m..s
  110 	   116 	 0.55071 	 0.55235 	 ~...
  111 	   117 	 0.55433 	 0.55372 	 ~...
  115 	   118 	 0.57339 	 0.55836 	 ~...
  118 	   119 	 0.59139 	 0.56442 	 ~...
  119 	   120 	 0.59349 	 0.57101 	 ~...
==========================================
r_mrr = 0.9865151047706604
r2_mrr = 0.9668611288070679
spearmanr_mrr@5 = 0.9710996747016907
spearmanr_mrr@10 = 0.9789754748344421
spearmanr_mrr@50 = 0.9956897497177124
spearmanr_mrr@100 = 0.9919626712799072
spearmanr_mrr@All = 0.9919938445091248
==========================================
test time: 0.43
Done Testing dataset UMLS
total time taken: 189.13159251213074
training time taken: 182.7132215499878
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9865)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9669)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9711)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9790)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9957)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9920)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9920)}}, 'test_loss': {'DistMult': {'UMLS': 1.2299834852165077}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'p_freq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4052553802049859
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [494, 819, 48, 119, 926, 21, 941, 853, 149, 918, 559, 235, 745, 1159, 160, 998, 481, 101, 981, 438, 87, 691, 311, 199, 154, 736, 404, 840, 731, 1074, 526, 362, 631, 1057, 1144, 1069, 181, 930, 642, 1111, 1059, 1191, 500, 905, 892, 988, 281, 598, 657, 250, 194, 761, 929, 846, 239, 4, 592, 407, 1193, 118, 528, 628, 345, 12, 995, 847, 370, 303, 405, 381, 331, 83, 660, 114, 447, 771, 174, 275, 430, 532, 137, 184, 151, 1179, 820, 955, 506, 1183, 227, 896, 999, 480, 389, 1134, 578, 670, 335, 204, 386, 312, 1004, 1046, 666, 1136, 925, 1151, 256, 553, 314, 875, 426, 1025, 457, 1042, 423, 196, 128, 726, 183, 837, 675]
valid_ids (0): []
train_ids (1094): [167, 801, 635, 1009, 938, 43, 513, 658, 724, 483, 984, 1044, 569, 88, 246, 367, 1122, 329, 462, 954, 649, 377, 313, 681, 877, 78, 1107, 208, 816, 602, 283, 551, 225, 398, 232, 202, 629, 1024, 186, 104, 1080, 942, 983, 835, 58, 556, 959, 169, 826, 659, 343, 916, 730, 529, 1195, 778, 318, 809, 932, 568, 1153, 350, 1188, 1211, 226, 1181, 346, 936, 347, 760, 26, 203, 969, 961, 717, 200, 779, 467, 460, 251, 238, 357, 1203, 1073, 1089, 365, 915, 1139, 600, 616, 135, 1021, 28, 706, 1124, 813, 650, 355, 328, 1053, 361, 651, 69, 782, 790, 1119, 945, 597, 1058, 1065, 108, 962, 1170, 366, 47, 150, 290, 525, 822, 815, 319, 887, 599, 1112, 116, 161, 738, 258, 842, 7, 299, 744, 767, 453, 109, 534, 451, 1196, 935, 53, 713, 757, 197, 1060, 968, 637, 212, 1110, 2, 41, 76, 475, 445, 1189, 673, 978, 469, 39, 780, 672, 223, 27, 839, 173, 818, 538, 1036, 478, 913, 185, 843, 84, 1047, 701, 255, 8, 163, 297, 80, 201, 752, 363, 982, 394, 55, 278, 1066, 110, 425, 966, 254, 291, 1062, 482, 580, 1209, 705, 606, 1143, 1091, 624, 523, 100, 269, 604, 567, 172, 718, 63, 920, 464, 348, 266, 1051, 805, 1104, 975, 287, 52, 68, 751, 1027, 1050, 1167, 123, 552, 1147, 931, 140, 284, 872, 686, 791, 924, 393, 618, 155, 699, 1101, 878, 817, 472, 1052, 899, 1205, 384, 158, 97, 832, 1037, 428, 808, 92, 737, 353, 585, 170, 764, 264, 126, 418, 667, 766, 54, 987, 861, 911, 1114, 294, 530, 298, 369, 385, 320, 862, 1076, 956, 632, 112, 122, 698, 844, 1141, 450, 215, 106, 17, 358, 406, 121, 684, 749, 507, 669, 973, 317, 136, 1063, 683, 933, 742, 1100, 1199, 399, 890, 166, 1173, 537, 171, 213, 415, 566, 865, 216, 646, 1210, 806, 620, 950, 410, 1149, 708, 895, 1000, 1155, 96, 309, 1045, 1001, 117, 879, 882, 514, 644, 536, 833, 509, 289, 391, 1093, 746, 241, 836, 416, 417, 187, 485, 1096, 231, 517, 612, 619, 591, 1108, 609, 753, 1115, 282, 1010, 980, 923, 831, 424, 162, 1088, 505, 851, 1182, 723, 678, 458, 263, 339, 1082, 1163, 1032, 664, 1017, 1034, 1165, 653, 133, 587, 687, 1169, 636, 383, 1022, 177, 876, 857, 220, 120, 1213, 1142, 73, 419, 456, 46, 946, 1038, 1056, 610, 855, 560, 125, 1092, 323, 704, 316, 883, 849, 70, 596, 633, 1113, 572, 222, 952, 38, 614, 265, 268, 807, 712, 34, 325, 31, 739, 957, 182, 648, 221, 189, 692, 61, 91, 468, 471, 1040, 322, 621, 75, 668, 561, 531, 965, 180, 991, 334, 812, 130, 240, 337, 850, 1098, 1099, 1105, 493, 914, 214, 1200, 288, 465, 548, 589, 466, 810, 582, 153, 868, 488, 588, 793, 401, 1028, 729, 565, 944, 1008, 178, 545, 829, 277, 1041, 728, 206, 190, 1118, 781, 754, 889, 1106, 1176, 308, 1148, 721, 584, 814, 971, 16, 727, 1020, 86, 558, 1048, 1164, 429, 62, 1067, 891, 1013, 295, 1130, 997, 512, 1078, 515, 127, 486, 501, 533, 919, 378, 433, 315, 72, 830, 510, 696, 823, 750, 30, 770, 586, 327, 245, 1121, 888, 341, 702, 662, 1083, 703, 402, 711, 260, 490, 205, 679, 976, 640, 421, 301, 741, 827, 338, 427, 784, 1177, 943, 852, 411, 564, 1156, 762, 863, 797, 800, 302, 958, 422, 716, 293, 138, 1054, 547, 60, 1150, 1186, 884, 102, 917, 522, 387, 789, 714, 113, 951, 804, 1180, 627, 593, 81, 19, 1003, 590, 652, 1202, 37, 732, 330, 157, 774, 95, 233, 502, 211, 700, 463, 625, 270, 605, 9, 147, 1135, 615, 6, 986, 848, 1187, 324, 379, 336, 249, 440, 1031, 626, 375, 351, 663, 449, 276, 98, 11, 307, 376, 40, 1095, 272, 803, 539, 168, 601, 496, 775, 993, 247, 927, 722, 477, 689, 934, 229, 1126, 111, 1162, 747, 107, 420, 979, 859, 874, 574, 1019, 45, 434, 693, 970, 413, 368, 1109, 1174, 1064, 132, 431, 783, 25, 1131, 1129, 885, 99, 179, 543, 563, 65, 305, 880, 1157, 546, 654, 1016, 74, 661, 967, 397, 455, 300, 725, 886, 437, 395, 51, 603, 611, 454, 29, 1125, 380, 907, 382, 734, 1166, 93, 459, 195, 148, 1084, 685, 193, 146, 124, 444, 1094, 484, 64, 1127, 720, 479, 364, 985, 321, 23, 165, 1071, 1152, 499, 436, 49, 912, 1175, 50, 535, 1197, 1123, 688, 949, 1006, 446, 655, 1208, 676, 207, 396, 939, 710, 948, 412, 131, 544, 1072, 1061, 217, 356, 198, 191, 243, 841, 115, 1012, 474, 10, 613, 252, 359, 432, 1079, 66, 921, 448, 267, 228, 1033, 352, 541, 1, 1023, 792, 285, 1161, 641, 373, 134, 715, 645, 763, 787, 825, 583, 210, 1140, 142, 443, 1132, 860, 24, 400, 188, 230, 571, 497, 897, 639, 409, 1086, 103, 594, 85, 1102, 1178, 82, 292, 595, 575, 273, 498, 94, 867, 296, 740, 516, 845, 79, 898, 1068, 244, 963, 607, 1207, 105, 773, 175, 1030, 665, 176, 218, 286, 695, 802, 1116, 452, 489, 1201, 145, 557, 1145, 893, 234, 156, 504, 77, 579, 1097, 542, 977, 152, 866, 236, 439, 519, 326, 71, 237, 304, 838, 1206, 1026, 647, 992, 570, 694, 758, 1185, 937, 13, 22, 748, 390, 796, 1039, 1137, 333, 392, 261, 690, 1018, 1146, 871, 881, 634, 511, 1049, 521, 1171, 129, 3, 491, 549, 765, 697, 864, 371, 994, 550, 209, 59, 503, 372, 909, 508, 257, 403, 870, 461, 253, 1055, 57, 1011, 682, 143, 1070, 974, 262, 928, 798, 36, 360, 719, 608, 1168, 902, 90, 576, 527, 540, 996, 990, 577, 858, 1002, 67, 414, 349, 44, 1090, 733, 1133, 1190, 811, 442, 671, 18, 755, 788, 89, 972, 0, 518, 344, 735, 159, 960, 1007, 139, 495, 1103, 340, 901, 1212, 408, 772, 35, 940, 487, 1154, 164, 854, 1085, 476, 1029, 922, 869, 821, 279, 1117, 989, 617, 470, 374, 900, 834, 904, 242, 15, 756, 5, 1160, 873, 1194, 776, 828, 20, 573, 56, 656, 824, 680, 581, 524, 1128, 562, 777, 1081, 769, 1077, 492, 630, 42, 759, 310, 622, 388, 274, 1005, 259, 1184, 1015, 271, 33, 1214, 1035, 280, 192, 964, 441, 555, 623, 342, 794, 677, 906, 638, 473, 709, 707, 908, 856, 14, 894, 1158, 799, 743, 520, 219, 306, 910, 1192, 1087, 795, 32, 332, 141, 1120, 786, 554, 785, 1198, 643, 1172, 1138, 1014, 248, 903, 947, 354, 224, 1043, 674, 1204, 144, 1075, 435, 768, 953]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  7495428340862845
the save name prefix for this run is:  chkpt-ID_7495428340862845_tag_Ablation-job-blacklist-s_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 18
rank avg (pred): 0.473 +- 0.005
mrr vals (pred, true): 0.016, 0.559
batch losses (mrrl, rdl): 0.0, 0.0041288417

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1200
rank avg (pred): 0.425 +- 0.235
mrr vals (pred, true): 0.182, 0.045
batch losses (mrrl, rdl): 0.0, 1.1002e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 298
rank avg (pred): 0.043 +- 0.030
mrr vals (pred, true): 0.385, 0.559
batch losses (mrrl, rdl): 0.0, 1.0548e-06

Epoch over!
epoch time: 12.102

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 212
rank avg (pred): 0.378 +- 0.249
mrr vals (pred, true): 0.265, 0.049
batch losses (mrrl, rdl): 0.0, 3.43422e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 911
rank avg (pred): 0.393 +- 0.269
mrr vals (pred, true): 0.286, 0.090
batch losses (mrrl, rdl): 0.0, 6.91064e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 346
rank avg (pred): 0.411 +- 0.262
mrr vals (pred, true): 0.260, 0.108
batch losses (mrrl, rdl): 0.0, 0.0003037515

Epoch over!
epoch time: 12.052

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1131
rank avg (pred): 0.370 +- 0.252
mrr vals (pred, true): 0.289, 0.049
batch losses (mrrl, rdl): 0.0, 5.82233e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1062
rank avg (pred): 0.025 +- 0.018
mrr vals (pred, true): 0.454, 0.536
batch losses (mrrl, rdl): 0.0, 3.2463e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 822
rank avg (pred): 0.059 +- 0.044
mrr vals (pred, true): 0.388, 0.462
batch losses (mrrl, rdl): 0.0, 2.0561e-06

Epoch over!
epoch time: 12.343

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 491
rank avg (pred): 0.241 +- 0.177
mrr vals (pred, true): 0.322, 0.249
batch losses (mrrl, rdl): 0.0, 0.0001127988

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1084
rank avg (pred): 0.358 +- 0.252
mrr vals (pred, true): 0.300, 0.157
batch losses (mrrl, rdl): 0.0, 0.000484975

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 997
rank avg (pred): 0.058 +- 0.043
mrr vals (pred, true): 0.388, 0.547
batch losses (mrrl, rdl): 0.0, 1.32371e-05

Epoch over!
epoch time: 11.871

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 172
rank avg (pred): 0.327 +- 0.242
mrr vals (pred, true): 0.320, 0.053
batch losses (mrrl, rdl): 0.0, 0.0001185258

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 817
rank avg (pred): 0.342 +- 0.255
mrr vals (pred, true): 0.342, 0.065
batch losses (mrrl, rdl): 0.0, 5.4546e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 279
rank avg (pred): 0.034 +- 0.026
mrr vals (pred, true): 0.427, 0.541
batch losses (mrrl, rdl): 0.0, 8.681e-07

Epoch over!
epoch time: 12.099

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 435
rank avg (pred): 0.340 +- 0.248
mrr vals (pred, true): 0.298, 0.054
batch losses (mrrl, rdl): 0.6127577424, 7.56676e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 262
rank avg (pred): 0.015 +- 0.014
mrr vals (pred, true): 0.547, 0.563
batch losses (mrrl, rdl): 0.0028191719, 5.7219e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 646
rank avg (pred): 0.356 +- 0.108
mrr vals (pred, true): 0.042, 0.029
batch losses (mrrl, rdl): 0.0005985096, 0.0004092326

Epoch over!
epoch time: 12.229

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 863
rank avg (pred): 0.301 +- 0.144
mrr vals (pred, true): 0.045, 0.100
batch losses (mrrl, rdl): 0.0300389305, 0.0007685289

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 859
rank avg (pred): 0.306 +- 0.145
mrr vals (pred, true): 0.049, 0.038
batch losses (mrrl, rdl): 2.8869e-06, 0.0006613997

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1141
rank avg (pred): 0.267 +- 0.202
mrr vals (pred, true): 0.208, 0.225
batch losses (mrrl, rdl): 0.0027937149, 9.82384e-05

Epoch over!
epoch time: 12.079

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 935
rank avg (pred): 0.279 +- 0.181
mrr vals (pred, true): 0.043, 0.015
batch losses (mrrl, rdl): 0.0004482795, 0.0045203026

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 238
rank avg (pred): 0.336 +- 0.125
mrr vals (pred, true): 0.066, 0.046
batch losses (mrrl, rdl): 0.0024607147, 0.000256638

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 476
rank avg (pred): 0.338 +- 0.133
mrr vals (pred, true): 0.078, 0.048
batch losses (mrrl, rdl): 0.0075681764, 0.0003451156

Epoch over!
epoch time: 12.149

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 593
rank avg (pred): 0.320 +- 0.149
mrr vals (pred, true): 0.055, 0.042
batch losses (mrrl, rdl): 0.0002246219, 0.0005581158

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 580
rank avg (pred): 0.334 +- 0.148
mrr vals (pred, true): 0.051, 0.044
batch losses (mrrl, rdl): 7.2478e-06, 0.0004072699

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 450
rank avg (pred): 0.343 +- 0.134
mrr vals (pred, true): 0.076, 0.053
batch losses (mrrl, rdl): 0.0065229228, 0.0002079435

Epoch over!
epoch time: 12.251

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1073
rank avg (pred): 0.033 +- 0.032
mrr vals (pred, true): 0.511, 0.551
batch losses (mrrl, rdl): 0.0157082006, 1.221e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 248
rank avg (pred): 0.024 +- 0.024
mrr vals (pred, true): 0.531, 0.547
batch losses (mrrl, rdl): 0.0027917973, 1.6211e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1013
rank avg (pred): 0.366 +- 0.137
mrr vals (pred, true): 0.073, 0.142
batch losses (mrrl, rdl): 0.0481068306, 0.0001536581

Epoch over!
epoch time: 12.205

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1072
rank avg (pred): 0.019 +- 0.020
mrr vals (pred, true): 0.570, 0.542
batch losses (mrrl, rdl): 0.007776408, 6.17e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1125
rank avg (pred): 0.346 +- 0.149
mrr vals (pred, true): 0.092, 0.050
batch losses (mrrl, rdl): 0.0173124075, 0.000182612

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 33
rank avg (pred): 0.062 +- 0.064
mrr vals (pred, true): 0.503, 0.513
batch losses (mrrl, rdl): 0.0009235205, 9.1214e-06

Epoch over!
epoch time: 12.307

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 616
rank avg (pred): 0.325 +- 0.187
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 9.02901e-05, 0.0004010402

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 308
rank avg (pred): 0.020 +- 0.021
mrr vals (pred, true): 0.591, 0.546
batch losses (mrrl, rdl): 0.0199752226, 4.2281e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 479
rank avg (pred): 0.383 +- 0.146
mrr vals (pred, true): 0.081, 0.042
batch losses (mrrl, rdl): 0.0094055645, 8.57569e-05

Epoch over!
epoch time: 12.13

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 579
rank avg (pred): 0.378 +- 0.146
mrr vals (pred, true): 0.046, 0.040
batch losses (mrrl, rdl): 0.0002023362, 0.0002621211

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 795
rank avg (pred): 0.354 +- 0.171
mrr vals (pred, true): 0.046, 0.049
batch losses (mrrl, rdl): 0.0001903031, 0.0001130263

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1106
rank avg (pred): 0.357 +- 0.152
mrr vals (pred, true): 0.080, 0.131
batch losses (mrrl, rdl): 0.0264038686, 0.0001860087

Epoch over!
epoch time: 12.235

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 935
rank avg (pred): 0.334 +- 0.203
mrr vals (pred, true): 0.038, 0.015
batch losses (mrrl, rdl): 0.0013952232, 0.0034971577

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1100
rank avg (pred): 0.388 +- 0.159
mrr vals (pred, true): 0.089, 0.102
batch losses (mrrl, rdl): 0.0016682822, 0.0001705592

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 147
rank avg (pred): 0.385 +- 0.149
mrr vals (pred, true): 0.083, 0.117
batch losses (mrrl, rdl): 0.0116229001, 0.0002307688

Epoch over!
epoch time: 12.215

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 473
rank avg (pred): 0.402 +- 0.137
mrr vals (pred, true): 0.064, 0.046
batch losses (mrrl, rdl): 0.0019195398, 7.68455e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 502
rank avg (pred): 0.345 +- 0.236
mrr vals (pred, true): 0.233, 0.185
batch losses (mrrl, rdl): 0.0234818012, 0.0002955511

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 868
rank avg (pred): 0.353 +- 0.184
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 1.67666e-05, 0.0001662095

Epoch over!
epoch time: 12.266

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.318 +- 0.246
mrr vals (pred, true): 0.293, 0.233

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    6 	     0 	 0.03843 	 0.01524 	 ~...
    4 	     1 	 0.03822 	 0.01569 	 ~...
    0 	     2 	 0.03428 	 0.01573 	 ~...
    2 	     3 	 0.03479 	 0.01574 	 ~...
    3 	     4 	 0.03494 	 0.01697 	 ~...
    5 	     5 	 0.03831 	 0.01764 	 ~...
   90 	     6 	 0.15216 	 0.01876 	 MISS
   86 	     7 	 0.10950 	 0.02127 	 m..s
   26 	     8 	 0.05410 	 0.03285 	 ~...
   28 	     9 	 0.05500 	 0.03469 	 ~...
   21 	    10 	 0.05069 	 0.03570 	 ~...
   30 	    11 	 0.05661 	 0.03646 	 ~...
   25 	    12 	 0.05232 	 0.03917 	 ~...
   52 	    13 	 0.07674 	 0.03938 	 m..s
   17 	    14 	 0.04710 	 0.04140 	 ~...
   12 	    15 	 0.04464 	 0.04176 	 ~...
   24 	    16 	 0.05186 	 0.04302 	 ~...
   23 	    17 	 0.05182 	 0.04357 	 ~...
   19 	    18 	 0.04721 	 0.04385 	 ~...
    8 	    19 	 0.04032 	 0.04465 	 ~...
   13 	    20 	 0.04468 	 0.04465 	 ~...
   72 	    21 	 0.08386 	 0.04506 	 m..s
    7 	    22 	 0.04031 	 0.04509 	 ~...
   11 	    23 	 0.04446 	 0.04519 	 ~...
   48 	    24 	 0.07538 	 0.04549 	 ~...
   59 	    25 	 0.07893 	 0.04557 	 m..s
   10 	    26 	 0.04397 	 0.04595 	 ~...
   53 	    27 	 0.07684 	 0.04610 	 m..s
   61 	    28 	 0.07950 	 0.04613 	 m..s
   22 	    29 	 0.05173 	 0.04628 	 ~...
   45 	    30 	 0.07428 	 0.04682 	 ~...
   71 	    31 	 0.08345 	 0.04704 	 m..s
   16 	    32 	 0.04607 	 0.04707 	 ~...
    9 	    33 	 0.04392 	 0.04709 	 ~...
   18 	    34 	 0.04711 	 0.04721 	 ~...
   78 	    35 	 0.08644 	 0.04739 	 m..s
    1 	    36 	 0.03434 	 0.04741 	 ~...
   73 	    37 	 0.08397 	 0.04771 	 m..s
   82 	    38 	 0.10122 	 0.04779 	 m..s
   79 	    39 	 0.08763 	 0.04812 	 m..s
   83 	    40 	 0.10349 	 0.04816 	 m..s
   69 	    41 	 0.08283 	 0.04862 	 m..s
   33 	    42 	 0.06876 	 0.04990 	 ~...
   58 	    43 	 0.07852 	 0.04995 	 ~...
   51 	    44 	 0.07631 	 0.05004 	 ~...
   14 	    45 	 0.04535 	 0.05049 	 ~...
   77 	    46 	 0.08603 	 0.05074 	 m..s
   63 	    47 	 0.08008 	 0.05080 	 ~...
   27 	    48 	 0.05480 	 0.05108 	 ~...
   31 	    49 	 0.05759 	 0.05151 	 ~...
   42 	    50 	 0.07281 	 0.05168 	 ~...
   41 	    51 	 0.07192 	 0.05189 	 ~...
   76 	    52 	 0.08497 	 0.05193 	 m..s
   50 	    53 	 0.07628 	 0.05271 	 ~...
   35 	    54 	 0.06940 	 0.05341 	 ~...
   29 	    55 	 0.05570 	 0.05344 	 ~...
   15 	    56 	 0.04591 	 0.05439 	 ~...
   60 	    57 	 0.07944 	 0.05540 	 ~...
   49 	    58 	 0.07574 	 0.05562 	 ~...
   32 	    59 	 0.05837 	 0.05594 	 ~...
   56 	    60 	 0.07730 	 0.05632 	 ~...
   54 	    61 	 0.07691 	 0.05917 	 ~...
   74 	    62 	 0.08424 	 0.06286 	 ~...
   75 	    63 	 0.08488 	 0.06424 	 ~...
   47 	    64 	 0.07514 	 0.07261 	 ~...
   44 	    65 	 0.07321 	 0.07620 	 ~...
   70 	    66 	 0.08292 	 0.07881 	 ~...
   20 	    67 	 0.04825 	 0.08119 	 m..s
   68 	    68 	 0.08276 	 0.08228 	 ~...
   34 	    69 	 0.06916 	 0.09027 	 ~...
   57 	    70 	 0.07750 	 0.10013 	 ~...
   62 	    71 	 0.07989 	 0.10026 	 ~...
   40 	    72 	 0.07138 	 0.10112 	 ~...
   84 	    73 	 0.10739 	 0.10126 	 ~...
   46 	    74 	 0.07512 	 0.10395 	 ~...
   36 	    75 	 0.07016 	 0.10630 	 m..s
   66 	    76 	 0.08107 	 0.10773 	 ~...
   37 	    77 	 0.07026 	 0.10817 	 m..s
   81 	    78 	 0.09121 	 0.10861 	 ~...
   38 	    79 	 0.07054 	 0.11590 	 m..s
   65 	    80 	 0.08078 	 0.11705 	 m..s
   55 	    81 	 0.07711 	 0.11741 	 m..s
   64 	    82 	 0.08032 	 0.11882 	 m..s
   87 	    83 	 0.11321 	 0.12331 	 ~...
   88 	    84 	 0.12418 	 0.12660 	 ~...
   80 	    85 	 0.08968 	 0.12861 	 m..s
   85 	    86 	 0.10766 	 0.13160 	 ~...
   39 	    87 	 0.07099 	 0.13195 	 m..s
   67 	    88 	 0.08197 	 0.13246 	 m..s
   91 	    89 	 0.15378 	 0.13990 	 ~...
   43 	    90 	 0.07300 	 0.14543 	 m..s
   89 	    91 	 0.13281 	 0.14821 	 ~...
   97 	    92 	 0.34202 	 0.22655 	 MISS
   95 	    93 	 0.29282 	 0.23331 	 m..s
   94 	    94 	 0.28135 	 0.23905 	 m..s
   92 	    95 	 0.25885 	 0.24174 	 ~...
   96 	    96 	 0.32643 	 0.24207 	 m..s
   93 	    97 	 0.26309 	 0.30766 	 m..s
   98 	    98 	 0.36481 	 0.43149 	 m..s
  102 	    99 	 0.52699 	 0.50916 	 ~...
   99 	   100 	 0.47999 	 0.51763 	 m..s
  100 	   101 	 0.48205 	 0.53267 	 m..s
  113 	   102 	 0.54749 	 0.53856 	 ~...
  104 	   103 	 0.52785 	 0.53966 	 ~...
  108 	   104 	 0.53961 	 0.53995 	 ~...
  106 	   105 	 0.53684 	 0.54126 	 ~...
  109 	   106 	 0.54029 	 0.54154 	 ~...
  114 	   107 	 0.54817 	 0.54319 	 ~...
  103 	   108 	 0.52744 	 0.54536 	 ~...
  117 	   109 	 0.55350 	 0.54767 	 ~...
  116 	   110 	 0.55191 	 0.54784 	 ~...
  101 	   111 	 0.52100 	 0.54921 	 ~...
  119 	   112 	 0.55931 	 0.55203 	 ~...
  120 	   113 	 0.56389 	 0.55474 	 ~...
  107 	   114 	 0.53747 	 0.55855 	 ~...
  118 	   115 	 0.55566 	 0.56065 	 ~...
  105 	   116 	 0.53629 	 0.56120 	 ~...
  110 	   117 	 0.54125 	 0.56317 	 ~...
  115 	   118 	 0.54928 	 0.56354 	 ~...
  112 	   119 	 0.54485 	 0.56643 	 ~...
  111 	   120 	 0.54162 	 0.56927 	 ~...
==========================================
r_mrr = 0.9856783747673035
r2_mrr = 0.9702554941177368
spearmanr_mrr@5 = 0.9915266036987305
spearmanr_mrr@10 = 0.9635378122329712
spearmanr_mrr@50 = 0.9932500720024109
spearmanr_mrr@100 = 0.9945661425590515
spearmanr_mrr@All = 0.9950049519538879
==========================================
test time: 0.437
Done Testing dataset UMLS
total time taken: 189.16644883155823
training time taken: 183.03359150886536
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9857)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9703)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9915)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9635)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9933)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9946)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9950)}}, 'test_loss': {'DistMult': {'UMLS': 1.2147797940042437}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-o_p_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 4414916609848972
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [829, 74, 155, 1121, 1086, 191, 344, 1171, 896, 614, 153, 449, 857, 619, 1059, 434, 719, 594, 76, 261, 1036, 980, 731, 419, 134, 212, 190, 1123, 353, 211, 327, 560, 443, 730, 704, 75, 737, 404, 1106, 301, 563, 1136, 1102, 1093, 1074, 14, 591, 986, 467, 1170, 766, 627, 348, 971, 218, 420, 341, 297, 595, 753, 411, 1185, 93, 1176, 21, 120, 406, 983, 665, 396, 450, 934, 73, 51, 1031, 1048, 796, 909, 194, 706, 335, 1205, 1165, 587, 552, 247, 1113, 238, 649, 834, 132, 256, 260, 855, 1, 905, 1070, 1005, 528, 41, 334, 982, 492, 1000, 455, 604, 39, 990, 1053, 184, 641, 844, 89, 973, 500, 452, 209, 965, 498, 997, 920]
valid_ids (0): []
train_ids (1094): [1033, 551, 150, 9, 200, 948, 44, 15, 1180, 571, 930, 741, 299, 270, 784, 136, 374, 114, 732, 532, 929, 817, 622, 117, 554, 354, 795, 277, 129, 103, 1193, 557, 638, 1064, 1041, 520, 978, 987, 1016, 502, 293, 488, 540, 61, 1181, 280, 265, 616, 881, 830, 803, 565, 17, 1058, 818, 887, 513, 85, 154, 534, 388, 842, 287, 442, 879, 257, 911, 698, 972, 776, 1083, 1076, 1152, 224, 139, 462, 700, 10, 904, 1139, 1066, 788, 690, 50, 1198, 446, 979, 692, 1046, 273, 339, 1084, 309, 146, 439, 26, 564, 473, 138, 593, 580, 415, 456, 494, 168, 635, 727, 1056, 674, 860, 1047, 944, 1111, 1129, 64, 285, 54, 135, 651, 286, 820, 611, 445, 384, 232, 728, 505, 1196, 996, 765, 600, 414, 937, 1199, 116, 409, 1004, 679, 395, 538, 548, 831, 172, 1208, 1120, 440, 175, 791, 780, 561, 459, 963, 480, 48, 63, 1044, 824, 271, 1142, 382, 49, 599, 774, 142, 694, 639, 52, 244, 152, 330, 1168, 615, 304, 957, 512, 800, 642, 268, 609, 799, 787, 1039, 1037, 1001, 1130, 461, 838, 399, 1188, 72, 691, 969, 206, 36, 710, 606, 387, 797, 882, 137, 1020, 1071, 763, 547, 629, 94, 230, 722, 424, 597, 106, 441, 880, 681, 507, 32, 883, 417, 1122, 959, 715, 141, 598, 158, 1124, 740, 759, 1161, 504, 27, 99, 1211, 1002, 65, 68, 754, 274, 847, 901, 392, 756, 523, 8, 912, 977, 5, 368, 1184, 18, 1051, 655, 542, 203, 644, 251, 664, 954, 733, 482, 917, 861, 358, 1197, 1203, 1015, 1081, 111, 497, 670, 97, 290, 123, 777, 695, 886, 1105, 808, 966, 1169, 479, 485, 778, 592, 267, 789, 603, 573, 453, 975, 1023, 248, 839, 1024, 159, 284, 148, 1101, 677, 903, 663, 1183, 632, 363, 854, 914, 43, 687, 678, 661, 984, 913, 900, 867, 1144, 558, 28, 510, 174, 643, 899, 360, 1077, 131, 416, 12, 1092, 993, 1206, 515, 846, 188, 553, 768, 947, 840, 658, 669, 264, 617, 1209, 107, 1114, 1116, 1189, 127, 1146, 310, 870, 810, 724, 813, 47, 918, 331, 620, 31, 88, 156, 457, 356, 648, 1025, 288, 118, 180, 83, 657, 113, 725, 760, 877, 1008, 586, 3, 1212, 197, 974, 1140, 0, 832, 352, 570, 1164, 805, 888, 13, 916, 323, 885, 34, 1104, 367, 1028, 991, 222, 377, 544, 1191, 1095, 216, 567, 42, 329, 584, 794, 460, 472, 1009, 165, 219, 263, 1043, 1125, 1022, 852, 1158, 637, 748, 764, 961, 130, 898, 828, 183, 1112, 254, 864, 1182, 432, 253, 365, 549, 204, 1207, 522, 489, 448, 1190, 124, 454, 202, 444, 407, 1091, 233, 906, 541, 856, 226, 702, 652, 343, 699, 381, 466, 40, 246, 1119, 667, 291, 1145, 585, 259, 755, 1110, 734, 543, 953, 696, 516, 676, 24, 943, 964, 941, 385, 682, 827, 1089, 1160, 231, 56, 735, 179, 207, 486, 1153, 100, 581, 921, 187, 751, 804, 823, 793, 1061, 508, 1055, 305, 853, 144, 30, 785, 321, 346, 84, 869, 1078, 618, 902, 295, 509, 371, 79, 720, 347, 173, 758, 316, 925, 82, 1054, 147, 78, 370, 357, 315, 59, 60, 1162, 1010, 430, 866, 333, 895, 897, 272, 110, 87, 1137, 506, 413, 579, 401, 1202, 96, 556, 1163, 716, 198, 1067, 1131, 1103, 1073, 868, 390, 647, 19, 1032, 1049, 950, 939, 1052, 189, 6, 688, 1150, 940, 640, 653, 524, 81, 1143, 45, 711, 359, 283, 355, 1157, 275, 626, 956, 307, 386, 1147, 182, 90, 214, 311, 1118, 662, 607, 393, 306, 806, 529, 518, 527, 66, 421, 140, 1087, 671, 1149, 514, 892, 942, 757, 968, 628, 1148, 398, 583, 62, 340, 525, 252, 533, 790, 569, 709, 164, 20, 1214, 469, 625, 517, 1156, 496, 336, 229, 503, 1108, 729, 468, 77, 484, 1179, 383, 463, 405, 845, 816, 318, 1141, 962, 910, 672, 697, 239, 511, 574, 889, 490, 1079, 621, 163, 1035, 919, 437, 874, 366, 976, 361, 33, 255, 798, 848, 1201, 825, 499, 320, 1019, 822, 701, 372, 458, 807, 133, 1213, 612, 435, 631, 1097, 145, 91, 478, 1109, 927, 217, 960, 199, 495, 213, 705, 588, 526, 177, 1175, 71, 119, 412, 952, 37, 559, 782, 809, 746, 302, 750, 22, 1011, 470, 429, 1017, 313, 907, 531, 235, 521, 1115, 668, 769, 225, 128, 908, 545, 410, 4, 105, 596, 493, 933, 708, 1187, 1133, 1030, 935, 477, 575, 380, 364, 240, 319, 471, 258, 186, 1127, 221, 167, 772, 865, 634, 210, 1100, 1014, 427, 143, 195, 814, 636, 241, 428, 296, 1085, 476, 276, 1042, 1090, 322, 659, 815, 125, 481, 1134, 1080, 325, 605, 938, 601, 802, 836, 878, 786, 151, 1060, 101, 1021, 46, 703, 324, 1068, 876, 104, 1007, 1195, 161, 893, 1003, 922, 723, 841, 298, 958, 1050, 590, 872, 999, 176, 562, 1151, 602, 871, 126, 185, 781, 819, 86, 249, 337, 38, 689, 80, 530, 242, 623, 1154, 369, 474, 890, 29, 749, 693, 994, 1126, 536, 955, 752, 122, 650, 464, 1167, 767, 171, 537, 718, 539, 736, 215, 770, 1128, 717, 109, 884, 379, 779, 58, 436, 418, 350, 985, 576, 1012, 645, 1138, 835, 108, 673, 608, 1082, 11, 236, 431, 192, 572, 928, 308, 932, 1096, 568, 170, 833, 314, 851, 519, 1132, 926, 1038, 837, 761, 685, 747, 1062, 773, 294, 223, 112, 378, 53, 712, 397, 891, 610, 1204, 228, 578, 1045, 646, 282, 894, 25, 1178, 1099, 550, 1192, 70, 491, 1172, 328, 69, 394, 745, 1027, 949, 589, 1135, 812, 95, 738, 115, 266, 660, 98, 713, 169, 1029, 92, 1040, 1210, 220, 821, 451, 402, 7, 403, 555, 624, 317, 546, 1194, 1065, 995, 423, 196, 945, 426, 686, 1075, 389, 245, 281, 946, 289, 55, 160, 577, 465, 633, 475, 923, 57, 35, 1006, 862, 936, 1057, 102, 873, 762, 408, 501, 967, 859, 863, 707, 630, 375, 988, 205, 1034, 1013, 234, 227, 1098, 680, 262, 771, 1200, 743, 237, 23, 422, 181, 989, 67, 656, 1063, 721, 345, 269, 1072, 970, 300, 801, 850, 312, 487, 157, 775, 362, 915, 193, 1174, 998, 875, 201, 858, 684, 162, 121, 433, 391, 373, 981, 1159, 683, 1173, 666, 483, 726, 811, 376, 566, 2, 1155, 739, 1088, 1094, 438, 744, 425, 292, 1117, 178, 843, 278, 1107, 826, 992, 166, 742, 349, 16, 1026, 208, 849, 338, 447, 1186, 783, 332, 1069, 1018, 582, 250, 400, 326, 535, 303, 1166, 924, 951, 351, 654, 149, 279, 792, 613, 243, 342, 1177, 931, 675, 714]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9259870135941546
the save name prefix for this run is:  chkpt-ID_9259870135941546_tag_Ablation-job-blacklist-o_p_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 959
rank avg (pred): 0.455 +- 0.006
mrr vals (pred, true): 0.016, 0.050
batch losses (mrrl, rdl): 0.0, 7.54841e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 691
rank avg (pred): 0.470 +- 0.027
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 8.36067e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 310
rank avg (pred): 0.057 +- 0.039
mrr vals (pred, true): 0.305, 0.551
batch losses (mrrl, rdl): 0.0, 8.3026e-06

Epoch over!
epoch time: 12.201

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 793
rank avg (pred): 0.425 +- 0.277
mrr vals (pred, true): 0.201, 0.048
batch losses (mrrl, rdl): 0.0, 1.21738e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 160
rank avg (pred): 0.364 +- 0.263
mrr vals (pred, true): 0.240, 0.082
batch losses (mrrl, rdl): 0.0, 3.21428e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 791
rank avg (pred): 0.482 +- 0.299
mrr vals (pred, true): 0.180, 0.050
batch losses (mrrl, rdl): 0.0, 8.55158e-05

Epoch over!
epoch time: 11.942

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 535
rank avg (pred): 0.395 +- 0.281
mrr vals (pred, true): 0.214, 0.062
batch losses (mrrl, rdl): 0.0, 7.1806e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 392
rank avg (pred): 0.304 +- 0.245
mrr vals (pred, true): 0.279, 0.166
batch losses (mrrl, rdl): 0.0, 0.0002207997

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 771
rank avg (pred): 0.471 +- 0.291
mrr vals (pred, true): 0.165, 0.041
batch losses (mrrl, rdl): 0.0, 4.23203e-05

Epoch over!
epoch time: 11.898

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 724
rank avg (pred): 0.402 +- 0.281
mrr vals (pred, true): 0.215, 0.052
batch losses (mrrl, rdl): 0.0, 2.44463e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 179
rank avg (pred): 0.373 +- 0.275
mrr vals (pred, true): 0.207, 0.051
batch losses (mrrl, rdl): 0.0, 3.4594e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 221
rank avg (pred): 0.343 +- 0.268
mrr vals (pred, true): 0.211, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001670234

Epoch over!
epoch time: 12.121

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1146
rank avg (pred): 0.251 +- 0.243
mrr vals (pred, true): 0.310, 0.146
batch losses (mrrl, rdl): 0.0, 6.3789e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 279
rank avg (pred): 0.016 +- 0.044
mrr vals (pred, true): 0.600, 0.541
batch losses (mrrl, rdl): 0.0, 1.09091e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 651
rank avg (pred): 0.456 +- 0.287
mrr vals (pred, true): 0.160, 0.051
batch losses (mrrl, rdl): 0.0, 1.74992e-05

Epoch over!
epoch time: 11.97

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 127
rank avg (pred): 0.360 +- 0.283
mrr vals (pred, true): 0.222, 0.142
batch losses (mrrl, rdl): 0.0635671914, 0.0003475207

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 549
rank avg (pred): 0.503 +- 0.371
mrr vals (pred, true): 0.093, 0.047
batch losses (mrrl, rdl): 0.0186705105, 5.12225e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 418
rank avg (pred): 0.505 +- 0.377
mrr vals (pred, true): 0.090, 0.045
batch losses (mrrl, rdl): 0.0156162828, 8.22388e-05

Epoch over!
epoch time: 12.284

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 484
rank avg (pred): 0.536 +- 0.358
mrr vals (pred, true): 0.078, 0.048
batch losses (mrrl, rdl): 0.0078693088, 0.0001831671

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 519
rank avg (pred): 0.571 +- 0.369
mrr vals (pred, true): 0.067, 0.065
batch losses (mrrl, rdl): 0.0030540733, 0.0002760194

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 7
rank avg (pred): 0.364 +- 0.458
mrr vals (pred, true): 0.553, 0.547
batch losses (mrrl, rdl): 0.0004807605, 0.0020666169

Epoch over!
epoch time: 12.212

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1137
rank avg (pred): 0.443 +- 0.450
mrr vals (pred, true): 0.183, 0.229
batch losses (mrrl, rdl): 0.0217346568, 0.0010977837

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 864
rank avg (pred): 0.492 +- 0.329
mrr vals (pred, true): 0.051, 0.052
batch losses (mrrl, rdl): 5.9271e-06, 5.80209e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1196
rank avg (pred): 0.515 +- 0.313
mrr vals (pred, true): 0.053, 0.045
batch losses (mrrl, rdl): 6.56479e-05, 6.0488e-05

Epoch over!
epoch time: 12.149

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1064
rank avg (pred): 0.346 +- 0.443
mrr vals (pred, true): 0.548, 0.552
batch losses (mrrl, rdl): 0.0001662742, 0.0019439855

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1194
rank avg (pred): 0.514 +- 0.315
mrr vals (pred, true): 0.057, 0.050
batch losses (mrrl, rdl): 0.0004665739, 6.12977e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1142
rank avg (pred): 0.412 +- 0.441
mrr vals (pred, true): 0.260, 0.227
batch losses (mrrl, rdl): 0.0109382486, 0.00082204

Epoch over!
epoch time: 12.342

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 621
rank avg (pred): 0.402 +- 0.277
mrr vals (pred, true): 0.058, 0.037
batch losses (mrrl, rdl): 0.0005901529, 0.0001885519

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 922
rank avg (pred): 0.627 +- 0.365
mrr vals (pred, true): 0.044, 0.015
batch losses (mrrl, rdl): 0.0003638248, 0.0004875211

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 100
rank avg (pred): 0.475 +- 0.312
mrr vals (pred, true): 0.076, 0.119
batch losses (mrrl, rdl): 0.0185712837, 0.000444493

Epoch over!
epoch time: 12.307

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 930
rank avg (pred): 0.492 +- 0.372
mrr vals (pred, true): 0.046, 0.016
batch losses (mrrl, rdl): 0.0001710122, 0.0014487873

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1174
rank avg (pred): 0.456 +- 0.275
mrr vals (pred, true): 0.043, 0.036
batch losses (mrrl, rdl): 0.0004825268, 3.66077e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1003
rank avg (pred): 0.431 +- 0.266
mrr vals (pred, true): 0.075, 0.151
batch losses (mrrl, rdl): 0.0577016212, 0.0006077173

Epoch over!
epoch time: 12.321

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 54
rank avg (pred): 0.411 +- 0.461
mrr vals (pred, true): 0.514, 0.528
batch losses (mrrl, rdl): 0.0019214882, 0.0026330855

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 487
rank avg (pred): 0.381 +- 0.293
mrr vals (pred, true): 0.104, 0.213
batch losses (mrrl, rdl): 0.119381614, 0.0005027861

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 293
rank avg (pred): 0.352 +- 0.422
mrr vals (pred, true): 0.543, 0.554
batch losses (mrrl, rdl): 0.0012694007, 0.002155246

Epoch over!
epoch time: 11.942

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 790
rank avg (pred): 0.580 +- 0.340
mrr vals (pred, true): 0.043, 0.048
batch losses (mrrl, rdl): 0.0005532462, 0.0003787667

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 895
rank avg (pred): 0.445 +- 0.202
mrr vals (pred, true): 0.057, 0.023
batch losses (mrrl, rdl): 0.0005566046, 1.91147e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 540
rank avg (pred): 0.642 +- 0.322
mrr vals (pred, true): 0.064, 0.068
batch losses (mrrl, rdl): 0.0018459447, 0.0007443597

Epoch over!
epoch time: 12.09

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 144
rank avg (pred): 0.567 +- 0.324
mrr vals (pred, true): 0.061, 0.130
batch losses (mrrl, rdl): 0.0471214429, 0.00121911

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 811
rank avg (pred): 0.378 +- 0.411
mrr vals (pred, true): 0.362, 0.265
batch losses (mrrl, rdl): 0.0928175151, 0.0017906962

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 410
rank avg (pred): 0.425 +- 0.281
mrr vals (pred, true): 0.081, 0.048
batch losses (mrrl, rdl): 0.0098635256, 6.6658e-06

Epoch over!
epoch time: 11.987

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 198
rank avg (pred): 0.441 +- 0.252
mrr vals (pred, true): 0.060, 0.051
batch losses (mrrl, rdl): 0.0009306735, 6.3777e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 357
rank avg (pred): 0.430 +- 0.283
mrr vals (pred, true): 0.090, 0.112
batch losses (mrrl, rdl): 0.0048020664, 0.0003084329

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 215
rank avg (pred): 0.519 +- 0.345
mrr vals (pred, true): 0.078, 0.049
batch losses (mrrl, rdl): 0.0079488521, 0.0001085989

Epoch over!
epoch time: 12.225

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.283 +- 0.359
mrr vals (pred, true): 0.512, 0.532

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   27 	     0 	 0.05920 	 0.01638 	 m..s
   82 	     1 	 0.08298 	 0.01876 	 m..s
    8 	     2 	 0.05024 	 0.01923 	 m..s
   29 	     3 	 0.05996 	 0.02127 	 m..s
   18 	     4 	 0.05172 	 0.03568 	 ~...
    3 	     5 	 0.04966 	 0.03679 	 ~...
    5 	     6 	 0.04990 	 0.03707 	 ~...
    6 	     7 	 0.04992 	 0.03787 	 ~...
   80 	     8 	 0.08115 	 0.03872 	 m..s
   20 	     9 	 0.05235 	 0.03921 	 ~...
   56 	    10 	 0.07421 	 0.04091 	 m..s
   13 	    11 	 0.05101 	 0.04099 	 ~...
   28 	    12 	 0.05996 	 0.04100 	 ~...
   12 	    13 	 0.05082 	 0.04115 	 ~...
    2 	    14 	 0.04966 	 0.04185 	 ~...
    0 	    15 	 0.04943 	 0.04200 	 ~...
   31 	    16 	 0.06252 	 0.04242 	 ~...
   15 	    17 	 0.05120 	 0.04304 	 ~...
   36 	    18 	 0.06465 	 0.04416 	 ~...
    7 	    19 	 0.04999 	 0.04418 	 ~...
   59 	    20 	 0.07441 	 0.04438 	 m..s
    9 	    21 	 0.05036 	 0.04463 	 ~...
   14 	    22 	 0.05107 	 0.04475 	 ~...
   17 	    23 	 0.05171 	 0.04501 	 ~...
   63 	    24 	 0.07556 	 0.04542 	 m..s
   44 	    25 	 0.06895 	 0.04618 	 ~...
   70 	    26 	 0.07774 	 0.04649 	 m..s
   19 	    27 	 0.05206 	 0.04659 	 ~...
   26 	    28 	 0.05777 	 0.04660 	 ~...
   50 	    29 	 0.07263 	 0.04695 	 ~...
   55 	    30 	 0.07402 	 0.04696 	 ~...
   24 	    31 	 0.05559 	 0.04741 	 ~...
   62 	    32 	 0.07518 	 0.04745 	 ~...
   54 	    33 	 0.07387 	 0.04754 	 ~...
   81 	    34 	 0.08138 	 0.04790 	 m..s
   30 	    35 	 0.06169 	 0.04828 	 ~...
   74 	    36 	 0.07869 	 0.04832 	 m..s
   16 	    37 	 0.05165 	 0.04866 	 ~...
    4 	    38 	 0.04986 	 0.04932 	 ~...
   65 	    39 	 0.07597 	 0.04934 	 ~...
   64 	    40 	 0.07578 	 0.04941 	 ~...
   21 	    41 	 0.05236 	 0.04981 	 ~...
   38 	    42 	 0.06595 	 0.04989 	 ~...
   49 	    43 	 0.07225 	 0.04990 	 ~...
   69 	    44 	 0.07732 	 0.05004 	 ~...
   23 	    45 	 0.05497 	 0.05005 	 ~...
   22 	    46 	 0.05353 	 0.05033 	 ~...
   48 	    47 	 0.07224 	 0.05081 	 ~...
   68 	    48 	 0.07685 	 0.05158 	 ~...
   41 	    49 	 0.06732 	 0.05193 	 ~...
   11 	    50 	 0.05078 	 0.05267 	 ~...
   35 	    51 	 0.06362 	 0.05275 	 ~...
   43 	    52 	 0.06850 	 0.05282 	 ~...
   52 	    53 	 0.07286 	 0.05284 	 ~...
   46 	    54 	 0.07053 	 0.05348 	 ~...
   25 	    55 	 0.05704 	 0.05484 	 ~...
   32 	    56 	 0.06260 	 0.05540 	 ~...
   10 	    57 	 0.05059 	 0.05594 	 ~...
   45 	    58 	 0.06922 	 0.05734 	 ~...
   76 	    59 	 0.07924 	 0.06010 	 ~...
    1 	    60 	 0.04955 	 0.07363 	 ~...
   71 	    61 	 0.07797 	 0.08073 	 ~...
   39 	    62 	 0.06647 	 0.08283 	 ~...
   75 	    63 	 0.07885 	 0.08578 	 ~...
   72 	    64 	 0.07802 	 0.09081 	 ~...
   42 	    65 	 0.06785 	 0.09158 	 ~...
   78 	    66 	 0.08033 	 0.09597 	 ~...
   84 	    67 	 0.08353 	 0.10034 	 ~...
   57 	    68 	 0.07422 	 0.10042 	 ~...
   33 	    69 	 0.06267 	 0.10048 	 m..s
   40 	    70 	 0.06701 	 0.10391 	 m..s
   79 	    71 	 0.08095 	 0.10773 	 ~...
   34 	    72 	 0.06304 	 0.11006 	 m..s
   83 	    73 	 0.08308 	 0.11170 	 ~...
   66 	    74 	 0.07599 	 0.11494 	 m..s
   58 	    75 	 0.07424 	 0.11954 	 m..s
   51 	    76 	 0.07267 	 0.12319 	 m..s
   37 	    77 	 0.06569 	 0.12750 	 m..s
   60 	    78 	 0.07444 	 0.13098 	 m..s
   77 	    79 	 0.07980 	 0.13246 	 m..s
   67 	    80 	 0.07634 	 0.13310 	 m..s
   53 	    81 	 0.07363 	 0.14714 	 m..s
   47 	    82 	 0.07158 	 0.15403 	 m..s
   61 	    83 	 0.07511 	 0.15406 	 m..s
   73 	    84 	 0.07858 	 0.17769 	 m..s
   85 	    85 	 0.18075 	 0.18507 	 ~...
   86 	    86 	 0.18391 	 0.19106 	 ~...
   89 	    87 	 0.23640 	 0.22027 	 ~...
   88 	    88 	 0.23419 	 0.22655 	 ~...
   87 	    89 	 0.22255 	 0.23905 	 ~...
   90 	    90 	 0.28962 	 0.31456 	 ~...
   92 	    91 	 0.39638 	 0.36971 	 ~...
   91 	    92 	 0.36996 	 0.43149 	 m..s
  101 	    93 	 0.53974 	 0.50782 	 m..s
   97 	    94 	 0.53408 	 0.51394 	 ~...
   94 	    95 	 0.53111 	 0.52033 	 ~...
   99 	    96 	 0.53650 	 0.52120 	 ~...
  100 	    97 	 0.53910 	 0.52744 	 ~...
   98 	    98 	 0.53564 	 0.52778 	 ~...
  110 	    99 	 0.54548 	 0.52928 	 ~...
  104 	   100 	 0.54255 	 0.53036 	 ~...
   93 	   101 	 0.51241 	 0.53246 	 ~...
  117 	   102 	 0.54681 	 0.53487 	 ~...
  106 	   103 	 0.54301 	 0.53652 	 ~...
  109 	   104 	 0.54425 	 0.53797 	 ~...
  119 	   105 	 0.55039 	 0.53856 	 ~...
  113 	   106 	 0.54622 	 0.53922 	 ~...
  103 	   107 	 0.54235 	 0.54081 	 ~...
  120 	   108 	 0.55263 	 0.54280 	 ~...
  111 	   109 	 0.54573 	 0.54506 	 ~...
  112 	   110 	 0.54588 	 0.54676 	 ~...
  105 	   111 	 0.54267 	 0.54712 	 ~...
  108 	   112 	 0.54422 	 0.54784 	 ~...
  118 	   113 	 0.54775 	 0.54813 	 ~...
  107 	   114 	 0.54356 	 0.54829 	 ~...
   95 	   115 	 0.53352 	 0.54921 	 ~...
  102 	   116 	 0.54183 	 0.55166 	 ~...
   96 	   117 	 0.53364 	 0.56101 	 ~...
  115 	   118 	 0.54651 	 0.56198 	 ~...
  116 	   119 	 0.54680 	 0.56442 	 ~...
  114 	   120 	 0.54647 	 0.56927 	 ~...
==========================================
r_mrr = 0.9902411699295044
r2_mrr = 0.9805768728256226
spearmanr_mrr@5 = 0.8256029486656189
spearmanr_mrr@10 = 0.8758296966552734
spearmanr_mrr@50 = 0.9963172078132629
spearmanr_mrr@100 = 0.9932794570922852
spearmanr_mrr@All = 0.9934207201004028
==========================================
test time: 0.422
Done Testing dataset UMLS
total time taken: 188.8542947769165
training time taken: 182.48066544532776
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9902)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9806)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8256)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8758)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9963)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9933)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9934)}}, 'test_loss': {'DistMult': {'UMLS': 0.9081833901182108}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o_p_cofreq'}

=====================================================================
---------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s_o_cofreq
---------------------------------------------------------------------
=====================================================================
Running on a grid of size 1
Using random seed: 9167631414731028
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [183, 368, 378, 29, 355, 608, 1044, 159, 937, 632, 452, 787, 1139, 1083, 1028, 232, 225, 857, 127, 1128, 642, 269, 718, 546, 208, 258, 1058, 645, 474, 224, 994, 156, 149, 1181, 412, 247, 451, 860, 963, 864, 1169, 26, 1189, 1023, 607, 590, 1084, 1203, 273, 335, 792, 1007, 326, 321, 46, 701, 919, 867, 688, 5, 628, 50, 985, 868, 391, 1052, 785, 1062, 1094, 119, 1002, 881, 264, 779, 1099, 463, 626, 419, 835, 908, 397, 732, 1177, 896, 746, 749, 255, 724, 1073, 577, 244, 565, 1048, 76, 400, 930, 409, 467, 630, 965, 674, 744, 114, 756, 14, 89, 1059, 4, 856, 767, 1145, 662, 466, 346, 19, 635, 912, 976, 726, 621, 131]
valid_ids (0): []
train_ids (1094): [791, 560, 20, 592, 43, 885, 151, 1102, 816, 352, 1187, 152, 1089, 629, 1130, 148, 680, 877, 393, 1026, 521, 643, 349, 300, 212, 354, 528, 44, 1012, 941, 10, 572, 563, 612, 299, 568, 973, 248, 814, 24, 890, 761, 784, 178, 458, 457, 671, 865, 365, 218, 486, 259, 694, 307, 364, 175, 793, 717, 48, 55, 1109, 327, 1172, 506, 1167, 1024, 317, 1027, 1198, 519, 822, 58, 145, 333, 1070, 1166, 338, 1066, 265, 465, 1199, 1036, 808, 1034, 293, 880, 613, 1141, 197, 888, 611, 1212, 53, 324, 682, 494, 382, 473, 54, 838, 777, 1118, 186, 525, 476, 206, 489, 492, 339, 1085, 990, 619, 286, 66, 435, 947, 1133, 698, 1180, 319, 13, 192, 130, 485, 562, 1110, 190, 1041, 128, 641, 686, 508, 848, 1086, 52, 418, 1079, 32, 692, 405, 254, 1096, 712, 989, 115, 341, 647, 834, 998, 56, 549, 770, 510, 797, 1069, 617, 844, 1098, 236, 579, 551, 772, 266, 667, 1132, 1082, 984, 1021, 573, 217, 1, 179, 1148, 786, 1164, 943, 1195, 314, 668, 491, 700, 207, 841, 81, 533, 322, 432, 453, 72, 1088, 1121, 445, 427, 1123, 847, 481, 921, 803, 819, 51, 297, 184, 650, 11, 271, 1191, 539, 1106, 1095, 395, 422, 95, 390, 850, 1149, 980, 1107, 821, 714, 1018, 876, 227, 1116, 366, 854, 403, 443, 972, 411, 15, 514, 578, 535, 64, 977, 1157, 558, 1140, 182, 813, 203, 362, 361, 374, 1093, 234, 1020, 869, 603, 878, 261, 901, 1035, 952, 518, 1202, 421, 721, 407, 669, 678, 728, 389, 738, 129, 79, 110, 472, 195, 820, 922, 1193, 360, 436, 530, 1075, 1192, 308, 773, 291, 589, 933, 120, 863, 1153, 342, 511, 1103, 725, 1174, 1184, 739, 135, 537, 371, 656, 804, 1186, 543, 1003, 30, 69, 139, 213, 294, 719, 133, 383, 1124, 239, 210, 637, 460, 1068, 547, 597, 1163, 150, 141, 999, 801, 798, 1105, 570, 523, 556, 991, 929, 468, 737, 1127, 758, 983, 97, 1178, 379, 1113, 915, 618, 196, 123, 639, 882, 113, 303, 709, 279, 1160, 1016, 544, 41, 843, 82, 211, 425, 100, 855, 978, 1134, 502, 62, 237, 99, 962, 1131, 161, 766, 522, 931, 274, 344, 1090, 504, 968, 742, 1108, 388, 776, 318, 788, 108, 644, 1074, 493, 909, 1072, 516, 1143, 907, 552, 1051, 245, 455, 126, 103, 1115, 858, 554, 795, 974, 903, 38, 685, 174, 424, 722, 104, 969, 125, 666, 730, 61, 600, 497, 1117, 323, 580, 1104, 187, 157, 1154, 337, 328, 906, 1014, 587, 954, 997, 950, 194, 311, 757, 495, 640, 40, 1129, 831, 594, 910, 567, 741, 70, 47, 924, 440, 825, 134, 859, 697, 242, 413, 1214, 193, 1097, 117, 137, 80, 331, 569, 509, 1037, 914, 689, 702, 658, 625, 1147, 45, 653, 301, 233, 221, 396, 616, 655, 837, 584, 849, 755, 272, 800, 462, 461, 817, 471, 270, 484, 1001, 57, 598, 789, 898, 296, 111, 745, 0, 1033, 1207, 928, 960, 691, 155, 751, 1179, 480, 280, 765, 892, 988, 143, 760, 27, 257, 623, 415, 571, 759, 512, 1137, 260, 85, 845, 332, 1013, 290, 967, 936, 434, 92, 73, 163, 981, 209, 874, 200, 169, 633, 1060, 381, 1005, 870, 1125, 112, 1170, 575, 353, 267, 1019, 966, 581, 1071, 601, 1208, 394, 91, 284, 235, 564, 566, 109, 615, 517, 59, 935, 945, 67, 553, 241, 144, 86, 399, 891, 951, 794, 631, 446, 665, 747, 1136, 1008, 214, 96, 879, 531, 1176, 65, 861, 343, 1063, 705, 651, 470, 199, 807, 1204, 648, 703, 735, 604, 1142, 832, 1182, 557, 1009, 281, 240, 423, 1040, 75, 586, 146, 806, 1171, 1077, 942, 1206, 138, 226, 526, 659, 369, 513, 74, 191, 829, 1087, 275, 310, 288, 33, 450, 923, 140, 764, 172, 68, 713, 905, 1100, 204, 1165, 345, 740, 1168, 1043, 283, 315, 654, 768, 1022, 610, 636, 98, 408, 356, 704, 1111, 263, 176, 731, 305, 1138, 1185, 118, 250, 180, 282, 993, 231, 414, 602, 1112, 363, 527, 1025, 417, 1173, 136, 707, 536, 7, 574, 77, 828, 763, 727, 664, 173, 524, 548, 1030, 588, 627, 359, 500, 420, 708, 812, 1076, 529, 672, 609, 351, 826, 313, 1101, 501, 3, 94, 1080, 622, 624, 370, 555, 833, 1054, 559, 1120, 743, 538, 325, 979, 503, 673, 734, 679, 720, 416, 818, 1197, 1049, 306, 278, 900, 1031, 105, 1196, 12, 723, 1045, 1067, 147, 1135, 456, 1144, 576, 154, 925, 1194, 449, 1029, 699, 444, 439, 836, 285, 88, 475, 895, 358, 312, 1047, 532, 448, 165, 695, 437, 28, 1081, 167, 1175, 926, 302, 614, 373, 309, 944, 663, 406, 1057, 769, 1159, 1119, 158, 1061, 596, 357, 606, 84, 101, 889, 189, 902, 330, 887, 550, 783, 469, 238, 866, 478, 591, 1053, 781, 715, 1188, 488, 824, 545, 367, 505, 124, 1011, 805, 431, 953, 957, 36, 334, 934, 401, 31, 918, 304, 22, 60, 911, 428, 706, 220, 752, 799, 652, 410, 675, 780, 477, 634, 684, 1210, 499, 277, 677, 386, 223, 116, 599, 1056, 71, 398, 595, 292, 681, 459, 729, 987, 1211, 438, 298, 16, 593, 649, 690, 774, 375, 295, 251, 683, 1183, 336, 790, 661, 541, 823, 932, 9, 168, 871, 660, 830, 657, 992, 166, 995, 894, 585, 1152, 202, 961, 736, 1078, 583, 1038, 496, 253, 1213, 778, 1114, 842, 958, 540, 185, 920, 733, 853, 716, 430, 377, 927, 498, 1158, 229, 638, 1039, 262, 205, 872, 25, 646, 605, 904, 122, 1017, 1015, 1126, 479, 87, 268, 996, 1190, 940, 753, 1065, 17, 181, 1150, 162, 827, 8, 956, 487, 955, 846, 975, 1055, 507, 775, 852, 873, 875, 380, 1091, 483, 748, 676, 316, 170, 107, 490, 320, 347, 49, 970, 106, 1201, 939, 693, 198, 201, 102, 153, 986, 1122, 35, 938, 946, 442, 132, 520, 447, 482, 160, 6, 1161, 1050, 1205, 256, 2, 620, 1146, 1032, 710, 392, 219, 899, 188, 1162, 1155, 372, 329, 289, 782, 1156, 230, 851, 948, 350, 810, 249, 982, 276, 78, 287, 1092, 711, 177, 1151, 802, 90, 949, 897, 534, 39, 815, 754, 222, 142, 964, 913, 542, 1010, 809, 404, 1046, 34, 464, 582, 959, 441, 696, 37, 384, 670, 840, 215, 1042, 83, 454, 1004, 884, 402, 93, 1064, 243, 1209, 164, 1006, 893, 771, 839, 1200, 687, 376, 18, 796, 228, 917, 561, 426, 216, 886, 21, 121, 246, 1000, 385, 387, 750, 429, 916, 762, 252, 883, 23, 63, 340, 515, 171, 433, 971, 862, 42, 348, 811]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5633655587527450
the save name prefix for this run is:  chkpt-ID_5633655587527450_tag_Ablation-job-blacklist-s_o_cofreq
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 854
rank avg (pred): 0.513 +- 0.005
mrr vals (pred, true): 0.014, 0.114
batch losses (mrrl, rdl): 0.0, 0.0002176829

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1095
rank avg (pred): 0.418 +- 0.254
mrr vals (pred, true): 0.115, 0.090
batch losses (mrrl, rdl): 0.0, 0.0002016358

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 958
rank avg (pred): 0.505 +- 0.303
mrr vals (pred, true): 0.151, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001493633

Epoch over!
epoch time: 12.046

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 431
rank avg (pred): 0.348 +- 0.266
mrr vals (pred, true): 0.246, 0.042
batch losses (mrrl, rdl): 0.0, 0.0001240686

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 789
rank avg (pred): 0.470 +- 0.277
mrr vals (pred, true): 0.161, 0.044
batch losses (mrrl, rdl): 0.0, 5.39348e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 813
rank avg (pred): 0.190 +- 0.161
mrr vals (pred, true): 0.335, 0.232
batch losses (mrrl, rdl): 0.0, 4.83063e-05

Epoch over!
epoch time: 11.916

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 521
rank avg (pred): 0.346 +- 0.274
mrr vals (pred, true): 0.265, 0.135
batch losses (mrrl, rdl): 0.0, 6.02081e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 207
rank avg (pred): 0.353 +- 0.275
mrr vals (pred, true): 0.258, 0.051
batch losses (mrrl, rdl): 0.0, 4.29874e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 826
rank avg (pred): 0.150 +- 0.124
mrr vals (pred, true): 0.336, 0.238
batch losses (mrrl, rdl): 0.0, 7.95409e-05

Epoch over!
epoch time: 11.93

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1072
rank avg (pred): 0.021 +- 0.017
mrr vals (pred, true): 0.490, 0.542
batch losses (mrrl, rdl): 0.0, 5.3596e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1066
rank avg (pred): 0.023 +- 0.020
mrr vals (pred, true): 0.487, 0.540
batch losses (mrrl, rdl): 0.0, 3.6226e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1045
rank avg (pred): 0.333 +- 0.254
mrr vals (pred, true): 0.255, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001357108

Epoch over!
epoch time: 11.799

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 599
rank avg (pred): 0.439 +- 0.286
mrr vals (pred, true): 0.188, 0.039
batch losses (mrrl, rdl): 0.0, 2.07774e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 725
rank avg (pred): 0.425 +- 0.294
mrr vals (pred, true): 0.212, 0.046
batch losses (mrrl, rdl): 0.0, 1.17662e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 803
rank avg (pred): 0.426 +- 0.294
mrr vals (pred, true): 0.214, 0.055
batch losses (mrrl, rdl): 0.0, 1.08153e-05

Epoch over!
epoch time: 12.01

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 567
rank avg (pred): 0.444 +- 0.281
mrr vals (pred, true): 0.168, 0.050
batch losses (mrrl, rdl): 0.1402587444, 4.4527e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 152
rank avg (pred): 0.307 +- 0.127
mrr vals (pred, true): 0.065, 0.104
batch losses (mrrl, rdl): 0.0149648674, 4.48031e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 535
rank avg (pred): 0.283 +- 0.153
mrr vals (pred, true): 0.106, 0.062
batch losses (mrrl, rdl): 0.0319090299, 0.0004210065

Epoch over!
epoch time: 12.268

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 587
rank avg (pred): 0.280 +- 0.137
mrr vals (pred, true): 0.052, 0.055
batch losses (mrrl, rdl): 5.36416e-05, 0.0007743135

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 623
rank avg (pred): 0.305 +- 0.137
mrr vals (pred, true): 0.044, 0.042
batch losses (mrrl, rdl): 0.0003046815, 0.0006678745

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 636
rank avg (pred): 0.341 +- 0.119
mrr vals (pred, true): 0.040, 0.041
batch losses (mrrl, rdl): 0.0010360078, 0.0004002447

Epoch over!
epoch time: 12.147

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 523
rank avg (pred): 0.327 +- 0.170
mrr vals (pred, true): 0.095, 0.072
batch losses (mrrl, rdl): 0.0200920366, 0.0002126681

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 745
rank avg (pred): 0.189 +- 0.143
mrr vals (pred, true): 0.197, 0.308
batch losses (mrrl, rdl): 0.121656999, 0.00015292

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 417
rank avg (pred): 0.297 +- 0.134
mrr vals (pred, true): 0.070, 0.047
batch losses (mrrl, rdl): 0.0040386091, 0.0004767267

Epoch over!
epoch time: 12.143

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 263
rank avg (pred): 0.009 +- 0.008
mrr vals (pred, true): 0.582, 0.542
batch losses (mrrl, rdl): 0.0155979311, 1.20162e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 503
rank avg (pred): 0.176 +- 0.144
mrr vals (pred, true): 0.246, 0.235
batch losses (mrrl, rdl): 0.0012050434, 3.57213e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 0
rank avg (pred): 0.298 +- 0.188
mrr vals (pred, true): 0.127, 0.553
batch losses (mrrl, rdl): 1.8156023026, 0.0016291924

Epoch over!
epoch time: 12.131

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 137
rank avg (pred): 0.363 +- 0.131
mrr vals (pred, true): 0.055, 0.090
batch losses (mrrl, rdl): 0.0002956293, 7.44479e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 576
rank avg (pred): 0.367 +- 0.121
mrr vals (pred, true): 0.041, 0.037
batch losses (mrrl, rdl): 0.0008393571, 0.0003168221

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 641
rank avg (pred): 0.335 +- 0.135
mrr vals (pred, true): 0.055, 0.041
batch losses (mrrl, rdl): 0.0002591077, 0.0004408618

Epoch over!
epoch time: 12.137

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1009
rank avg (pred): 0.317 +- 0.143
mrr vals (pred, true): 0.071, 0.103
batch losses (mrrl, rdl): 0.0098861922, 4.39003e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 190
rank avg (pred): 0.362 +- 0.128
mrr vals (pred, true): 0.052, 0.044
batch losses (mrrl, rdl): 3.97293e-05, 0.000175117

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1104
rank avg (pred): 0.330 +- 0.156
mrr vals (pred, true): 0.078, 0.094
batch losses (mrrl, rdl): 0.0080452403, 2.80951e-05

Epoch over!
epoch time: 12.292

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 312
rank avg (pred): 0.013 +- 0.012
mrr vals (pred, true): 0.559, 0.542
batch losses (mrrl, rdl): 0.0030298913, 1.83021e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 809
rank avg (pred): 0.318 +- 0.178
mrr vals (pred, true): 0.045, 0.051
batch losses (mrrl, rdl): 0.0002343921, 0.0004135674

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 483
rank avg (pred): 0.337 +- 0.160
mrr vals (pred, true): 0.078, 0.055
batch losses (mrrl, rdl): 0.007926547, 0.0001946298

Epoch over!
epoch time: 12.04

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 31
rank avg (pred): 0.020 +- 0.018
mrr vals (pred, true): 0.512, 0.520
batch losses (mrrl, rdl): 0.0007736082, 1.08228e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1057
rank avg (pred): 0.013 +- 0.013
mrr vals (pred, true): 0.573, 0.564
batch losses (mrrl, rdl): 0.0008292322, 9.4554e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 134
rank avg (pred): 0.326 +- 0.152
mrr vals (pred, true): 0.075, 0.115
batch losses (mrrl, rdl): 0.0160585307, 4.24502e-05

Epoch over!
epoch time: 12.295

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 561
rank avg (pred): 0.367 +- 0.156
mrr vals (pred, true): 0.073, 0.045
batch losses (mrrl, rdl): 0.005422737, 0.0001838443

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 236
rank avg (pred): 0.344 +- 0.156
mrr vals (pred, true): 0.076, 0.045
batch losses (mrrl, rdl): 0.0066064242, 0.0002228772

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1202
rank avg (pred): 0.368 +- 0.151
mrr vals (pred, true): 0.049, 0.043
batch losses (mrrl, rdl): 3.0037e-06, 0.0001351145

Epoch over!
epoch time: 12.047

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1030
rank avg (pred): 0.321 +- 0.158
mrr vals (pred, true): 0.067, 0.045
batch losses (mrrl, rdl): 0.0029503207, 0.0002822685

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 168
rank avg (pred): 0.276 +- 0.160
mrr vals (pred, true): 0.105, 0.052
batch losses (mrrl, rdl): 0.0304109901, 0.0005575054

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 923
rank avg (pred): 0.397 +- 0.151
mrr vals (pred, true): 0.032, 0.018
batch losses (mrrl, rdl): 0.0031138551, 0.0026943376

Epoch over!
epoch time: 12.154

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.311 +- 0.166
mrr vals (pred, true): 0.077, 0.049

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03647 	 0.01573 	 ~...
    2 	     1 	 0.03654 	 0.01975 	 ~...
    7 	     2 	 0.03971 	 0.02025 	 ~...
   81 	     3 	 0.07462 	 0.02127 	 m..s
   28 	     4 	 0.04658 	 0.02690 	 ~...
    5 	     5 	 0.03839 	 0.03421 	 ~...
   15 	     6 	 0.04178 	 0.03661 	 ~...
   37 	     7 	 0.05217 	 0.03686 	 ~...
   10 	     8 	 0.04119 	 0.03716 	 ~...
   13 	     9 	 0.04167 	 0.03773 	 ~...
   45 	    10 	 0.05827 	 0.03798 	 ~...
   42 	    11 	 0.05598 	 0.03921 	 ~...
   59 	    12 	 0.06366 	 0.03971 	 ~...
    9 	    13 	 0.04097 	 0.03972 	 ~...
   19 	    14 	 0.04226 	 0.04000 	 ~...
   14 	    15 	 0.04171 	 0.04075 	 ~...
   33 	    16 	 0.04721 	 0.04184 	 ~...
   75 	    17 	 0.07189 	 0.04214 	 ~...
   66 	    18 	 0.06712 	 0.04225 	 ~...
   77 	    19 	 0.07282 	 0.04229 	 m..s
   22 	    20 	 0.04503 	 0.04248 	 ~...
   30 	    21 	 0.04686 	 0.04274 	 ~...
   23 	    22 	 0.04602 	 0.04275 	 ~...
   84 	    23 	 0.08210 	 0.04299 	 m..s
   18 	    24 	 0.04216 	 0.04430 	 ~...
   67 	    25 	 0.06768 	 0.04438 	 ~...
   47 	    26 	 0.05935 	 0.04461 	 ~...
    6 	    27 	 0.03846 	 0.04465 	 ~...
   25 	    28 	 0.04626 	 0.04475 	 ~...
   12 	    29 	 0.04164 	 0.04487 	 ~...
    4 	    30 	 0.03824 	 0.04509 	 ~...
   53 	    31 	 0.06207 	 0.04533 	 ~...
   35 	    32 	 0.04974 	 0.04534 	 ~...
   27 	    33 	 0.04645 	 0.04575 	 ~...
   16 	    34 	 0.04179 	 0.04612 	 ~...
   41 	    35 	 0.05393 	 0.04624 	 ~...
   24 	    36 	 0.04610 	 0.04625 	 ~...
   32 	    37 	 0.04715 	 0.04653 	 ~...
   17 	    38 	 0.04188 	 0.04665 	 ~...
   86 	    39 	 0.08684 	 0.04666 	 m..s
   11 	    40 	 0.04122 	 0.04707 	 ~...
   69 	    41 	 0.06859 	 0.04736 	 ~...
   89 	    42 	 0.09383 	 0.04790 	 m..s
   62 	    43 	 0.06590 	 0.04832 	 ~...
   82 	    44 	 0.07657 	 0.04862 	 ~...
    0 	    45 	 0.03635 	 0.04866 	 ~...
   38 	    46 	 0.05266 	 0.04909 	 ~...
   20 	    47 	 0.04298 	 0.04973 	 ~...
   60 	    48 	 0.06466 	 0.05004 	 ~...
   36 	    49 	 0.05035 	 0.05005 	 ~...
   52 	    50 	 0.06166 	 0.05029 	 ~...
   73 	    51 	 0.07013 	 0.05054 	 ~...
    3 	    52 	 0.03683 	 0.05096 	 ~...
   21 	    53 	 0.04427 	 0.05111 	 ~...
   34 	    54 	 0.04952 	 0.05144 	 ~...
   26 	    55 	 0.04630 	 0.05175 	 ~...
    8 	    56 	 0.04038 	 0.05207 	 ~...
   29 	    57 	 0.04664 	 0.05218 	 ~...
   48 	    58 	 0.06100 	 0.05268 	 ~...
   58 	    59 	 0.06303 	 0.05331 	 ~...
   39 	    60 	 0.05295 	 0.05364 	 ~...
   68 	    61 	 0.06775 	 0.05412 	 ~...
   31 	    62 	 0.04686 	 0.05622 	 ~...
   40 	    63 	 0.05302 	 0.05681 	 ~...
   61 	    64 	 0.06505 	 0.05732 	 ~...
   56 	    65 	 0.06281 	 0.06365 	 ~...
   50 	    66 	 0.06114 	 0.07261 	 ~...
   43 	    67 	 0.05654 	 0.07365 	 ~...
   72 	    68 	 0.06966 	 0.08288 	 ~...
   79 	    69 	 0.07333 	 0.08319 	 ~...
   70 	    70 	 0.06903 	 0.08573 	 ~...
   76 	    71 	 0.07249 	 0.09130 	 ~...
   51 	    72 	 0.06150 	 0.10333 	 m..s
   74 	    73 	 0.07076 	 0.10401 	 m..s
   78 	    74 	 0.07289 	 0.10750 	 m..s
   83 	    75 	 0.07844 	 0.10771 	 ~...
   63 	    76 	 0.06635 	 0.11028 	 m..s
   57 	    77 	 0.06291 	 0.11343 	 m..s
   55 	    78 	 0.06217 	 0.11590 	 m..s
   65 	    79 	 0.06688 	 0.11954 	 m..s
   71 	    80 	 0.06917 	 0.11955 	 m..s
   88 	    81 	 0.09120 	 0.12158 	 m..s
   85 	    82 	 0.08457 	 0.13106 	 m..s
   54 	    83 	 0.06212 	 0.13195 	 m..s
   64 	    84 	 0.06646 	 0.13246 	 m..s
   90 	    85 	 0.12676 	 0.13415 	 ~...
   46 	    86 	 0.05849 	 0.14118 	 m..s
   49 	    87 	 0.06103 	 0.14134 	 m..s
   80 	    88 	 0.07422 	 0.14238 	 m..s
   44 	    89 	 0.05694 	 0.15690 	 m..s
   92 	    90 	 0.28462 	 0.16959 	 MISS
   87 	    91 	 0.09034 	 0.17231 	 m..s
   93 	    92 	 0.28493 	 0.21216 	 m..s
   95 	    93 	 0.31095 	 0.23045 	 m..s
   91 	    94 	 0.28084 	 0.24983 	 m..s
   94 	    95 	 0.29668 	 0.28799 	 ~...
  103 	    96 	 0.54375 	 0.51598 	 ~...
   96 	    97 	 0.49974 	 0.51730 	 ~...
  104 	    98 	 0.54380 	 0.52247 	 ~...
  106 	    99 	 0.54518 	 0.52274 	 ~...
  109 	   100 	 0.54752 	 0.52543 	 ~...
  111 	   101 	 0.55073 	 0.52720 	 ~...
  108 	   102 	 0.54744 	 0.52778 	 ~...
  114 	   103 	 0.55165 	 0.53079 	 ~...
  116 	   104 	 0.55235 	 0.53587 	 ~...
  107 	   105 	 0.54540 	 0.53652 	 ~...
  100 	   106 	 0.53991 	 0.53657 	 ~...
   97 	   107 	 0.52938 	 0.54517 	 ~...
   99 	   108 	 0.53971 	 0.54536 	 ~...
  110 	   109 	 0.54759 	 0.54696 	 ~...
  115 	   110 	 0.55212 	 0.54784 	 ~...
  101 	   111 	 0.54039 	 0.54835 	 ~...
  117 	   112 	 0.55262 	 0.54914 	 ~...
   98 	   113 	 0.53142 	 0.54936 	 ~...
  105 	   114 	 0.54455 	 0.55053 	 ~...
  118 	   115 	 0.55488 	 0.55256 	 ~...
  112 	   116 	 0.55109 	 0.55651 	 ~...
  119 	   117 	 0.55576 	 0.55859 	 ~...
  102 	   118 	 0.54137 	 0.55884 	 ~...
  120 	   119 	 0.55940 	 0.56198 	 ~...
  113 	   120 	 0.55156 	 0.56442 	 ~...
==========================================
r_mrr = 0.9877529740333557
r2_mrr = 0.974876344203949
spearmanr_mrr@5 = 0.9448213577270508
spearmanr_mrr@10 = 0.8988681435585022
spearmanr_mrr@50 = 0.9901977181434631
spearmanr_mrr@100 = 0.9921112656593323
spearmanr_mrr@All = 0.9925495982170105
==========================================
test time: 0.398
Done Testing dataset UMLS
total time taken: 187.378009557724
training time taken: 181.81863045692444
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9878)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9749)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.9448)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8989)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9902)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9921)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9925)}}, 'test_loss': {'DistMult': {'UMLS': 1.1272383946743503}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s_o_cofreq'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 7647586144446678
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [319, 684, 458, 1028, 95, 1030, 1005, 22, 1040, 1147, 16, 308, 1107, 423, 126, 622, 554, 252, 232, 1045, 69, 711, 1070, 912, 142, 107, 973, 8, 607, 598, 330, 1162, 256, 1037, 457, 546, 400, 902, 1150, 1098, 564, 93, 415, 112, 395, 533, 119, 923, 933, 736, 505, 679, 692, 1009, 310, 41, 705, 236, 231, 167, 1092, 634, 191, 511, 47, 893, 435, 671, 766, 914, 71, 1058, 83, 1158, 78, 490, 515, 99, 619, 212, 396, 344, 296, 991, 335, 154, 321, 331, 111, 324, 816, 827, 203, 524, 224, 984, 73, 101, 237, 1197, 626, 757, 136, 1206, 25, 1117, 482, 275, 980, 307, 1011, 161, 820, 683, 102, 386, 559, 206, 1075, 710, 541]
valid_ids (0): []
train_ids (1094): [558, 333, 76, 149, 156, 105, 834, 693, 18, 433, 160, 447, 108, 585, 517, 936, 573, 1026, 556, 943, 650, 158, 449, 181, 843, 1074, 407, 441, 771, 70, 399, 787, 743, 15, 942, 814, 786, 1202, 502, 891, 632, 989, 1211, 828, 775, 680, 378, 38, 34, 141, 312, 687, 143, 548, 470, 450, 273, 1101, 1160, 1193, 175, 864, 261, 796, 493, 589, 593, 96, 553, 561, 724, 235, 1120, 139, 819, 313, 244, 993, 213, 185, 479, 188, 210, 1081, 487, 475, 536, 1064, 354, 516, 1049, 216, 389, 337, 653, 91, 674, 401, 217, 588, 369, 1130, 600, 468, 255, 1133, 483, 165, 956, 1114, 1212, 825, 714, 940, 60, 663, 254, 1099, 418, 532, 609, 972, 403, 350, 295, 186, 159, 978, 822, 1207, 1125, 57, 197, 253, 114, 329, 730, 380, 950, 865, 596, 1127, 886, 790, 769, 464, 897, 514, 473, 1199, 494, 570, 1189, 788, 123, 264, 540, 268, 484, 701, 1059, 961, 1149, 1001, 1029, 753, 385, 336, 677, 397, 987, 339, 1181, 463, 910, 768, 669, 260, 737, 641, 767, 1104, 826, 416, 909, 813, 522, 859, 187, 1177, 945, 409, 408, 1185, 249, 1140, 151, 537, 219, 200, 1085, 388, 660, 127, 162, 193, 86, 633, 98, 1078, 997, 215, 202, 1032, 963, 722, 1024, 690, 341, 81, 300, 667, 462, 839, 207, 610, 440, 670, 853, 452, 53, 841, 976, 420, 673, 563, 751, 1126, 240, 872, 284, 625, 120, 469, 225, 259, 1128, 686, 172, 949, 64, 643, 856, 205, 754, 325, 14, 1010, 930, 873, 177, 1000, 527, 958, 583, 580, 779, 1144, 921, 1068, 1113, 1034, 12, 960, 1191, 829, 17, 509, 990, 497, 1111, 1093, 662, 520, 474, 620, 798, 461, 1018, 1088, 824, 1151, 343, 994, 602, 1129, 655, 118, 306, 547, 274, 176, 1155, 729, 1036, 979, 866, 691, 831, 1091, 752, 265, 241, 145, 568, 749, 735, 276, 289, 40, 1015, 529, 1062, 196, 1124, 1006, 982, 731, 353, 180, 364, 964, 326, 62, 672, 640, 292, 1135, 271, 879, 748, 209, 135, 682, 318, 806, 1143, 944, 624, 234, 122, 981, 565, 460, 444, 1119, 830, 821, 1132, 305, 900, 1056, 179, 361, 1080, 850, 1198, 572, 780, 718, 642, 807, 290, 1072, 345, 1071, 1063, 1201, 616, 1131, 746, 1066, 72, 323, 1173, 639, 1067, 715, 957, 405, 346, 815, 838, 507, 519, 1213, 594, 974, 478, 192, 293, 489, 613, 500, 46, 608, 906, 774, 28, 849, 1031, 442, 518, 379, 723, 499, 611, 184, 538, 1115, 720, 257, 1214, 1174, 525, 229, 294, 419, 1209, 629, 1204, 471, 654, 157, 999, 392, 592, 628, 742, 542, 847, 621, 694, 531, 0, 685, 155, 125, 869, 835, 799, 501, 1152, 7, 727, 576, 656, 51, 472, 975, 131, 837, 85, 201, 148, 510, 63, 851, 584, 248, 368, 1178, 917, 696, 373, 436, 246, 54, 706, 812, 486, 1087, 476, 760, 1208, 699, 808, 223, 377, 934, 1044, 163, 817, 803, 492, 281, 645, 1042, 48, 84, 567, 56, 1021, 169, 466, 668, 21, 648, 44, 1002, 427, 777, 347, 429, 1008, 394, 94, 174, 852, 1137, 283, 506, 431, 681, 1182, 55, 311, 1105, 907, 384, 1176, 2, 858, 1016, 50, 1136, 1186, 659, 716, 285, 922, 1096, 560, 366, 762, 646, 320, 863, 27, 908, 352, 291, 1188, 6, 954, 376, 132, 586, 267, 1153, 398, 459, 413, 1073, 709, 758, 1027, 183, 481, 355, 279, 10, 59, 810, 818, 414, 846, 359, 243, 637, 387, 890, 801, 348, 1138, 417, 1053, 445, 892, 1203, 959, 92, 889, 230, 712, 666, 759, 1134, 534, 1014, 1159, 363, 434, 1055, 342, 913, 896, 977, 166, 147, 1090, 103, 390, 623, 566, 877, 1079, 1004, 925, 178, 371, 194, 82, 948, 675, 597, 985, 535, 1164, 581, 951, 1175, 222, 1025, 1118, 928, 1020, 996, 1154, 446, 1, 65, 823, 190, 133, 947, 938, 443, 233, 832, 875, 734, 728, 198, 406, 881, 916, 932, 1168, 258, 905, 657, 772, 1065, 303, 49, 739, 130, 732, 848, 713, 1035, 338, 704, 761, 884, 37, 995, 842, 860, 871, 513, 455, 612, 770, 1196, 432, 1041, 695, 883, 1165, 356, 404, 496, 1192, 1112, 1157, 1169, 569, 1180, 1033, 631, 488, 1046, 375, 526, 778, 150, 39, 508, 880, 530, 1103, 327, 349, 117, 854, 789, 170, 903, 270, 870, 523, 763, 658, 430, 491, 317, 575, 204, 664, 740, 1003, 700, 134, 1048, 747, 811, 1060, 374, 726, 451, 595, 89, 1051, 1210, 689, 391, 577, 765, 968, 795, 1183, 1187, 967, 110, 334, 247, 652, 782, 887, 926, 599, 1146, 571, 986, 485, 314, 635, 227, 426, 358, 717, 1116, 868, 935, 512, 545, 124, 802, 164, 195, 745, 833, 1122, 1095, 189, 1200, 80, 638, 800, 1023, 1022, 61, 617, 661, 467, 792, 214, 1145, 878, 58, 360, 1148, 939, 857, 402, 1156, 263, 1057, 885, 220, 773, 9, 970, 721, 272, 836, 702, 946, 480, 1076, 182, 603, 410, 888, 703, 750, 199, 1061, 895, 797, 1190, 100, 168, 297, 738, 42, 121, 931, 755, 1007, 1094, 579, 1121, 362, 67, 1054, 678, 1179, 302, 971, 4, 1194, 282, 146, 676, 30, 152, 19, 109, 615, 1038, 1017, 315, 1109, 744, 924, 1097, 601, 74, 874, 919, 250, 340, 647, 1106, 299, 733, 955, 791, 1139, 113, 920, 1163, 862, 381, 707, 756, 1195, 77, 636, 1013, 221, 899, 741, 228, 277, 90, 578, 266, 173, 614, 36, 644, 465, 75, 962, 861, 918, 1052, 1161, 421, 698, 88, 901, 504, 697, 137, 911, 555, 298, 651, 262, 1166, 218, 544, 528, 665, 370, 411, 590, 367, 280, 840, 992, 1110, 618, 129, 211, 915, 688, 781, 1123, 288, 33, 138, 498, 153, 562, 587, 453, 725, 550, 238, 785, 966, 793, 591, 425, 630, 383, 13, 68, 844, 927, 5, 23, 549, 97, 26, 539, 316, 965, 328, 32, 867, 1089, 357, 301, 438, 29, 454, 552, 304, 941, 1043, 87, 3, 24, 439, 437, 627, 422, 606, 1083, 983, 31, 845, 1184, 226, 1141, 1019, 882, 937, 809, 1172, 495, 128, 898, 708, 322, 1084, 557, 1039, 428, 245, 1167, 988, 1012, 242, 365, 805, 605, 1171, 719, 20, 351, 424, 43, 604, 521, 309, 794, 239, 649, 1050, 106, 764, 894, 783, 45, 116, 52, 477, 551, 412, 35, 11, 208, 144, 503, 1077, 574, 287, 1086, 1069, 140, 448, 998, 904, 855, 1047, 1108, 79, 776, 876, 1205, 969, 382, 1100, 66, 332, 1142, 582, 1082, 1170, 953, 804, 115, 543, 104, 952, 286, 171, 251, 929, 1102, 278, 456, 269, 372, 393, 784]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  3661426538552693
the save name prefix for this run is:  chkpt-ID_3661426538552693_tag_Ablation-job-blacklist-s min deg neighbnour_o min deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 18
rank avg (pred): 0.549 +- 0.004
mrr vals (pred, true): 0.013, 0.559
batch losses (mrrl, rdl): 0.0, 0.0056683156

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 604
rank avg (pred): 0.460 +- 0.227
mrr vals (pred, true): 0.112, 0.038
batch losses (mrrl, rdl): 0.0, 1.42977e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 737
rank avg (pred): 0.105 +- 0.069
mrr vals (pred, true): 0.284, 0.019
batch losses (mrrl, rdl): 0.0, 0.0022871136

Epoch over!
epoch time: 12.12

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1210
rank avg (pred): 0.447 +- 0.281
mrr vals (pred, true): 0.208, 0.041
batch losses (mrrl, rdl): 0.0, 2.61555e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 226
rank avg (pred): 0.357 +- 0.239
mrr vals (pred, true): 0.243, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001032087

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 847
rank avg (pred): 0.435 +- 0.283
mrr vals (pred, true): 0.214, 0.043
batch losses (mrrl, rdl): 0.0, 9.2921e-06

Epoch over!
epoch time: 11.951

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 627
rank avg (pred): 0.470 +- 0.294
mrr vals (pred, true): 0.201, 0.042
batch losses (mrrl, rdl): 0.0, 1.8244e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 289
rank avg (pred): 0.036 +- 0.026
mrr vals (pred, true): 0.399, 0.550
batch losses (mrrl, rdl): 0.0, 3.686e-07

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 815
rank avg (pred): 0.099 +- 0.074
mrr vals (pred, true): 0.321, 0.166
batch losses (mrrl, rdl): 0.0, 0.0011686241

Epoch over!
epoch time: 11.891

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 785
rank avg (pred): 0.405 +- 0.285
mrr vals (pred, true): 0.241, 0.047
batch losses (mrrl, rdl): 0.0, 1.29131e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 488
rank avg (pred): 0.158 +- 0.117
mrr vals (pred, true): 0.301, 0.241
batch losses (mrrl, rdl): 0.0, 6.81698e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 596
rank avg (pred): 0.431 +- 0.270
mrr vals (pred, true): 0.199, 0.041
batch losses (mrrl, rdl): 0.0, 6.8699e-06

Epoch over!
epoch time: 11.873

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 889
rank avg (pred): 0.443 +- 0.292
mrr vals (pred, true): 0.212, 0.042
batch losses (mrrl, rdl): 0.0, 2.53558e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1080
rank avg (pred): 0.320 +- 0.246
mrr vals (pred, true): 0.281, 0.080
batch losses (mrrl, rdl): 0.0, 1.7978e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1213
rank avg (pred): 0.430 +- 0.272
mrr vals (pred, true): 0.202, 0.056
batch losses (mrrl, rdl): 0.0, 8.1504e-06

Epoch over!
epoch time: 11.892

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 531
rank avg (pred): 0.387 +- 0.290
mrr vals (pred, true): 0.287, 0.052
batch losses (mrrl, rdl): 0.5596097112, 2.19179e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 403
rank avg (pred): 0.437 +- 0.214
mrr vals (pred, true): 0.069, 0.095
batch losses (mrrl, rdl): 0.0035211372, 0.0003791091

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 81
rank avg (pred): 0.395 +- 0.228
mrr vals (pred, true): 0.078, 0.120
batch losses (mrrl, rdl): 0.0174726248, 0.0002004735

Epoch over!
epoch time: 12.11

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 756
rank avg (pred): 0.396 +- 0.235
mrr vals (pred, true): 0.093, 0.056
batch losses (mrrl, rdl): 0.0188243501, 2.67434e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 263
rank avg (pred): 0.016 +- 0.011
mrr vals (pred, true): 0.464, 0.542
batch losses (mrrl, rdl): 0.0618094765, 6.6823e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1209
rank avg (pred): 0.456 +- 0.195
mrr vals (pred, true): 0.057, 0.054
batch losses (mrrl, rdl): 0.0004662228, 4.14769e-05

Epoch over!
epoch time: 12.384

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 811
rank avg (pred): 0.025 +- 0.018
mrr vals (pred, true): 0.404, 0.265
batch losses (mrrl, rdl): 0.1936675906, 0.0001007959

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1026
rank avg (pred): 0.391 +- 0.229
mrr vals (pred, true): 0.076, 0.045
batch losses (mrrl, rdl): 0.006798855, 3.54278e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 91
rank avg (pred): 0.403 +- 0.217
mrr vals (pred, true): 0.067, 0.103
batch losses (mrrl, rdl): 0.0129774548, 0.0002345637

Epoch over!
epoch time: 12.198

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 286
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.548, 0.541
batch losses (mrrl, rdl): 0.0005138614, 1.63884e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 169
rank avg (pred): 0.388 +- 0.228
mrr vals (pred, true): 0.083, 0.048
batch losses (mrrl, rdl): 0.0107619921, 2.43964e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 578
rank avg (pred): 0.405 +- 0.216
mrr vals (pred, true): 0.072, 0.036
batch losses (mrrl, rdl): 0.0049108565, 0.0001420719

Epoch over!
epoch time: 12.022

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1208
rank avg (pred): 0.456 +- 0.188
mrr vals (pred, true): 0.057, 0.045
batch losses (mrrl, rdl): 0.0005617437, 3.61713e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 82
rank avg (pred): 0.393 +- 0.214
mrr vals (pred, true): 0.064, 0.080
batch losses (mrrl, rdl): 0.0019890973, 5.27814e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 390
rank avg (pred): 0.382 +- 0.216
mrr vals (pred, true): 0.078, 0.134
batch losses (mrrl, rdl): 0.0310705379, 0.0003287239

Epoch over!
epoch time: 12.133

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1194
rank avg (pred): 0.419 +- 0.200
mrr vals (pred, true): 0.060, 0.050
batch losses (mrrl, rdl): 0.000913167, 1.76109e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 403
rank avg (pred): 0.388 +- 0.223
mrr vals (pred, true): 0.075, 0.095
batch losses (mrrl, rdl): 0.0061764433, 0.0001682542

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 176
rank avg (pred): 0.325 +- 0.191
mrr vals (pred, true): 0.084, 0.051
batch losses (mrrl, rdl): 0.011667626, 0.0001955917

Epoch over!
epoch time: 12.285

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 72
rank avg (pred): 0.018 +- 0.013
mrr vals (pred, true): 0.439, 0.512
batch losses (mrrl, rdl): 0.0543754883, 2.22831e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 669
rank avg (pred): 0.473 +- 0.171
mrr vals (pred, true): 0.059, 0.048
batch losses (mrrl, rdl): 0.0007589085, 5.93998e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 248
rank avg (pred): 0.007 +- 0.006
mrr vals (pred, true): 0.601, 0.547
batch losses (mrrl, rdl): 0.0292163752, 1.37191e-05

Epoch over!
epoch time: 12.224

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 367
rank avg (pred): 0.401 +- 0.205
mrr vals (pred, true): 0.061, 0.081
batch losses (mrrl, rdl): 0.0012153855, 0.0001304517

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 377
rank avg (pred): 0.319 +- 0.197
mrr vals (pred, true): 0.088, 0.116
batch losses (mrrl, rdl): 0.0077349511, 4.80878e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 587
rank avg (pred): 0.404 +- 0.209
mrr vals (pred, true): 0.063, 0.055
batch losses (mrrl, rdl): 0.0016701294, 7.60071e-05

Epoch over!
epoch time: 11.988

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 627
rank avg (pred): 0.511 +- 0.137
mrr vals (pred, true): 0.052, 0.042
batch losses (mrrl, rdl): 2.62596e-05, 7.89481e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 664
rank avg (pred): 0.447 +- 0.178
mrr vals (pred, true): 0.050, 0.050
batch losses (mrrl, rdl): 3.71e-08, 3.73075e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 927
rank avg (pred): 0.531 +- 0.113
mrr vals (pred, true): 0.040, 0.018
batch losses (mrrl, rdl): 0.0010707385, 0.0005960656

Epoch over!
epoch time: 12.1

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1006
rank avg (pred): 0.337 +- 0.172
mrr vals (pred, true): 0.068, 0.154
batch losses (mrrl, rdl): 0.0742095783, 0.0002676487

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 745
rank avg (pred): 0.085 +- 0.063
mrr vals (pred, true): 0.260, 0.308
batch losses (mrrl, rdl): 0.0229881965, 2.60512e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 146
rank avg (pred): 0.348 +- 0.232
mrr vals (pred, true): 0.105, 0.098
batch losses (mrrl, rdl): 0.0299684927, 5.17296e-05

Epoch over!
epoch time: 12.371

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.542, 0.553

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04126 	 0.01547 	 ~...
   17 	     1 	 0.05109 	 0.01758 	 m..s
   89 	     2 	 0.14970 	 0.01876 	 MISS
   87 	     3 	 0.12589 	 0.03724 	 m..s
   27 	     4 	 0.06766 	 0.03785 	 ~...
   26 	     5 	 0.06762 	 0.03938 	 ~...
   16 	     6 	 0.05095 	 0.03972 	 ~...
   39 	     7 	 0.07205 	 0.04036 	 m..s
   75 	     8 	 0.08536 	 0.04132 	 m..s
   12 	     9 	 0.04801 	 0.04194 	 ~...
   58 	    10 	 0.07937 	 0.04204 	 m..s
    9 	    11 	 0.04756 	 0.04318 	 ~...
    4 	    12 	 0.04660 	 0.04401 	 ~...
    1 	    13 	 0.04563 	 0.04418 	 ~...
   10 	    14 	 0.04763 	 0.04430 	 ~...
   38 	    15 	 0.07155 	 0.04450 	 ~...
   52 	    16 	 0.07829 	 0.04461 	 m..s
   80 	    17 	 0.09345 	 0.04464 	 m..s
   19 	    18 	 0.05335 	 0.04468 	 ~...
   22 	    19 	 0.06264 	 0.04472 	 ~...
   50 	    20 	 0.07765 	 0.04474 	 m..s
   14 	    21 	 0.04919 	 0.04499 	 ~...
   45 	    22 	 0.07605 	 0.04531 	 m..s
   36 	    23 	 0.07126 	 0.04533 	 ~...
   25 	    24 	 0.06658 	 0.04540 	 ~...
    5 	    25 	 0.04667 	 0.04625 	 ~...
   37 	    26 	 0.07143 	 0.04671 	 ~...
   55 	    27 	 0.07895 	 0.04695 	 m..s
   60 	    28 	 0.07967 	 0.04704 	 m..s
   15 	    29 	 0.04929 	 0.04750 	 ~...
   68 	    30 	 0.08410 	 0.04771 	 m..s
   43 	    31 	 0.07522 	 0.04860 	 ~...
   18 	    32 	 0.05324 	 0.04921 	 ~...
   53 	    33 	 0.07864 	 0.04935 	 ~...
   79 	    34 	 0.09125 	 0.04941 	 m..s
   44 	    35 	 0.07548 	 0.04951 	 ~...
   57 	    36 	 0.07915 	 0.04955 	 ~...
   21 	    37 	 0.06137 	 0.04962 	 ~...
   77 	    38 	 0.08808 	 0.04967 	 m..s
   20 	    39 	 0.06075 	 0.05033 	 ~...
    3 	    40 	 0.04624 	 0.05069 	 ~...
   13 	    41 	 0.04917 	 0.05122 	 ~...
   33 	    42 	 0.06968 	 0.05240 	 ~...
   73 	    43 	 0.08464 	 0.05268 	 m..s
   66 	    44 	 0.08210 	 0.05370 	 ~...
   11 	    45 	 0.04765 	 0.05439 	 ~...
   27 	    46 	 0.06766 	 0.05461 	 ~...
    7 	    47 	 0.04721 	 0.05681 	 ~...
    2 	    48 	 0.04608 	 0.05719 	 ~...
   41 	    49 	 0.07368 	 0.05917 	 ~...
    8 	    50 	 0.04754 	 0.05983 	 ~...
   32 	    51 	 0.06961 	 0.06365 	 ~...
    6 	    52 	 0.04674 	 0.07007 	 ~...
   78 	    53 	 0.08854 	 0.07881 	 ~...
   47 	    54 	 0.07670 	 0.07982 	 ~...
   71 	    55 	 0.08450 	 0.08295 	 ~...
   59 	    56 	 0.07956 	 0.08345 	 ~...
   29 	    57 	 0.06816 	 0.08490 	 ~...
   42 	    58 	 0.07469 	 0.09158 	 ~...
   61 	    59 	 0.07972 	 0.09729 	 ~...
   23 	    60 	 0.06560 	 0.09753 	 m..s
   46 	    61 	 0.07668 	 0.09772 	 ~...
   24 	    62 	 0.06647 	 0.09788 	 m..s
   49 	    63 	 0.07706 	 0.09859 	 ~...
   63 	    64 	 0.08077 	 0.09972 	 ~...
   74 	    65 	 0.08526 	 0.10026 	 ~...
   34 	    66 	 0.06979 	 0.10042 	 m..s
   54 	    67 	 0.07882 	 0.10112 	 ~...
   70 	    68 	 0.08438 	 0.10249 	 ~...
   81 	    69 	 0.09557 	 0.10262 	 ~...
   48 	    70 	 0.07681 	 0.10275 	 ~...
   83 	    71 	 0.10664 	 0.10496 	 ~...
   76 	    72 	 0.08649 	 0.10907 	 ~...
   82 	    73 	 0.10218 	 0.11105 	 ~...
   31 	    74 	 0.06940 	 0.11140 	 m..s
   67 	    75 	 0.08301 	 0.11564 	 m..s
   62 	    76 	 0.08034 	 0.11590 	 m..s
   72 	    77 	 0.08450 	 0.11882 	 m..s
   51 	    78 	 0.07794 	 0.12080 	 m..s
   69 	    79 	 0.08437 	 0.12158 	 m..s
   85 	    80 	 0.11592 	 0.12212 	 ~...
   86 	    81 	 0.11875 	 0.12681 	 ~...
   35 	    82 	 0.06999 	 0.12861 	 m..s
   88 	    83 	 0.13232 	 0.12900 	 ~...
   30 	    84 	 0.06839 	 0.12951 	 m..s
   84 	    85 	 0.11178 	 0.13010 	 ~...
   64 	    86 	 0.08117 	 0.13246 	 m..s
   40 	    87 	 0.07218 	 0.13476 	 m..s
   65 	    88 	 0.08183 	 0.14714 	 m..s
   56 	    89 	 0.07915 	 0.15403 	 m..s
   91 	    90 	 0.18547 	 0.18563 	 ~...
   92 	    91 	 0.21184 	 0.20347 	 ~...
   93 	    92 	 0.21397 	 0.21489 	 ~...
   90 	    93 	 0.17872 	 0.24278 	 m..s
   96 	    94 	 0.52237 	 0.50619 	 ~...
   95 	    95 	 0.52214 	 0.51017 	 ~...
   94 	    96 	 0.52171 	 0.51763 	 ~...
   97 	    97 	 0.53074 	 0.52120 	 ~...
   99 	    98 	 0.53442 	 0.52543 	 ~...
  102 	    99 	 0.53843 	 0.53036 	 ~...
  108 	   100 	 0.54330 	 0.53145 	 ~...
  103 	   101 	 0.53974 	 0.53235 	 ~...
  117 	   102 	 0.55449 	 0.53399 	 ~...
  116 	   103 	 0.55253 	 0.53487 	 ~...
   98 	   104 	 0.53404 	 0.53628 	 ~...
  114 	   105 	 0.55103 	 0.54126 	 ~...
  101 	   106 	 0.53520 	 0.54405 	 ~...
  113 	   107 	 0.54817 	 0.54412 	 ~...
  115 	   108 	 0.55193 	 0.54638 	 ~...
  110 	   109 	 0.54590 	 0.54663 	 ~...
  106 	   110 	 0.54298 	 0.54725 	 ~...
  107 	   111 	 0.54304 	 0.54813 	 ~...
  109 	   112 	 0.54542 	 0.54910 	 ~...
  120 	   113 	 0.56152 	 0.54936 	 ~...
  104 	   114 	 0.54196 	 0.55075 	 ~...
  111 	   115 	 0.54680 	 0.55089 	 ~...
  112 	   116 	 0.54723 	 0.55166 	 ~...
  105 	   117 	 0.54204 	 0.55251 	 ~...
  100 	   118 	 0.53453 	 0.55581 	 ~...
  118 	   119 	 0.55704 	 0.55644 	 ~...
  119 	   120 	 0.56078 	 0.56927 	 ~...
==========================================
r_mrr = 0.9890244007110596
r2_mrr = 0.9780956506729126
spearmanr_mrr@5 = 0.7993326783180237
spearmanr_mrr@10 = 0.8563695549964905
spearmanr_mrr@50 = 0.9994596838951111
spearmanr_mrr@100 = 0.9963356256484985
spearmanr_mrr@All = 0.9964277148246765
==========================================
test time: 0.409
Done Testing dataset UMLS
total time taken: 188.00578474998474
training time taken: 182.0174117088318
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9890)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9781)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.7993)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8564)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9995)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9963)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9964)}}, 'test_loss': {'DistMult': {'UMLS': 0.9173755831925519}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o min deg neighbnour', 's min deg neighbnour'}

====================================================================================================
----------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
----------------------------------------------------------------------------------------------------
====================================================================================================
Running on a grid of size 1
Using random seed: 7561796508551420
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1107, 97, 110, 296, 593, 1061, 884, 584, 723, 814, 275, 1158, 585, 236, 873, 794, 316, 120, 257, 524, 454, 644, 994, 251, 209, 439, 143, 552, 34, 786, 102, 195, 437, 1059, 659, 648, 535, 916, 743, 860, 1041, 456, 39, 344, 410, 136, 833, 772, 630, 656, 368, 1081, 742, 1170, 734, 286, 1183, 527, 592, 731, 699, 858, 2, 272, 243, 608, 427, 1130, 1091, 478, 46, 800, 510, 1123, 1079, 1117, 558, 639, 303, 930, 1040, 329, 1032, 354, 747, 531, 963, 632, 1180, 202, 388, 25, 101, 754, 453, 1151, 849, 670, 782, 911, 855, 685, 827, 781, 149, 152, 1199, 1136, 341, 784, 904, 1034, 161, 79, 571, 857, 253, 588, 1038, 809, 520]
valid_ids (0): []
train_ids (1094): [231, 1200, 1194, 627, 53, 1067, 996, 999, 466, 561, 1046, 540, 701, 666, 763, 1145, 788, 828, 966, 18, 324, 162, 591, 738, 505, 856, 297, 562, 1208, 1143, 450, 726, 1088, 951, 898, 1075, 408, 929, 706, 509, 1150, 720, 1212, 59, 93, 949, 1149, 314, 151, 711, 843, 635, 522, 62, 595, 1074, 971, 247, 1184, 918, 596, 917, 96, 266, 113, 302, 704, 586, 471, 768, 629, 312, 1166, 616, 125, 186, 981, 1021, 625, 852, 1213, 157, 845, 882, 155, 997, 219, 838, 894, 214, 75, 360, 896, 52, 1089, 1129, 146, 1182, 137, 823, 705, 812, 933, 108, 24, 836, 326, 1152, 555, 413, 1169, 693, 578, 69, 990, 100, 1196, 181, 669, 60, 696, 462, 441, 495, 196, 1192, 1163, 1049, 163, 173, 807, 61, 425, 765, 1175, 1050, 539, 551, 513, 1177, 311, 459, 573, 802, 475, 816, 435, 1179, 570, 1039, 279, 291, 1173, 641, 1132, 132, 660, 987, 872, 259, 67, 716, 910, 364, 198, 1037, 847, 725, 708, 806, 640, 939, 899, 366, 68, 1124, 1156, 1148, 977, 897, 690, 657, 901, 915, 867, 376, 416, 1206, 1154, 107, 988, 863, 776, 564, 150, 839, 301, 942, 862, 662, 804, 688, 727, 99, 1095, 835, 650, 773, 17, 1030, 321, 790, 523, 735, 905, 874, 759, 677, 1099, 164, 109, 883, 1178, 1109, 634, 741, 504, 131, 1197, 605, 117, 227, 822, 45, 534, 1168, 610, 574, 941, 499, 791, 945, 438, 1140, 292, 651, 166, 94, 1137, 671, 760, 82, 33, 517, 755, 955, 665, 547, 653, 41, 545, 140, 1120, 853, 43, 153, 1119, 1209, 861, 1204, 496, 598, 1090, 433, 159, 418, 697, 736, 1103, 992, 587, 372, 572, 374, 141, 1157, 749, 216, 753, 282, 1104, 834, 909, 514, 11, 642, 167, 124, 448, 906, 568, 430, 1098, 380, 255, 234, 1052, 832, 947, 950, 174, 232, 133, 698, 398, 1096, 1, 1214, 28, 948, 479, 85, 356, 484, 193, 178, 925, 1073, 976, 1051, 449, 550, 1024, 361, 819, 1131, 938, 780, 746, 879, 77, 944, 490, 404, 799, 128, 254, 1188, 820, 1016, 530, 500, 721, 207, 502, 581, 943, 261, 549, 240, 226, 924, 1181, 728, 464, 1133, 875, 419, 371, 895, 565, 707, 1004, 348, 222, 919, 74, 378, 968, 402, 560, 1057, 31, 414, 1118, 1029, 813, 777, 104, 215, 365, 307, 870, 970, 633, 91, 421, 1086, 702, 262, 145, 623, 1036, 638, 937, 691, 353, 446, 637, 779, 362, 258, 556, 491, 205, 335, 188, 423, 912, 27, 1055, 118, 399, 373, 333, 935, 846, 512, 160, 1189, 190, 271, 907, 972, 305, 946, 165, 339, 1071, 864, 48, 1082, 264, 563, 506, 1110, 516, 1033, 44, 934, 732, 63, 485, 3, 470, 293, 194, 1072, 220, 81, 959, 793, 184, 310, 488, 603, 826, 199, 375, 1043, 306, 64, 844, 168, 675, 260, 1122, 775, 521, 962, 319, 1153, 111, 4, 767, 544, 428, 1164, 796, 1015, 1159, 76, 526, 476, 903, 626, 49, 758, 112, 1017, 985, 619, 224, 83, 290, 1193, 406, 511, 142, 468, 575, 1187, 877, 805, 245, 367, 686, 400, 931, 238, 14, 761, 1108, 472, 869, 147, 922, 559, 242, 1093, 762, 515, 1048, 330, 729, 778, 1080, 553, 55, 350, 1142, 612, 1106, 72, 957, 722, 998, 129, 211, 463, 602, 267, 12, 315, 1026, 225, 116, 473, 983, 1077, 984, 927, 868, 617, 606, 288, 345, 713, 582, 287, 609, 771, 590, 751, 105, 825, 172, 1141, 733, 975, 280, 489, 993, 1031, 213, 655, 1065, 649, 1000, 498, 718, 923, 15, 98, 1085, 447, 1101, 37, 217, 1205, 210, 1054, 338, 1028, 458, 477, 170, 1147, 405, 1068, 1014, 230, 19, 90, 383, 1211, 385, 953, 246, 276, 1138, 229, 566, 886, 431, 770, 542, 304, 795, 32, 218, 821, 134, 1027, 600, 180, 1172, 355, 42, 1198, 115, 589, 235, 268, 393, 135, 1139, 370, 5, 407, 652, 681, 474, 601, 661, 541, 349, 359, 871, 465, 831, 611, 667, 532, 1105, 1210, 452, 241, 119, 792, 840, 557, 1176, 533, 538, 127, 810, 284, 1007, 442, 1003, 389, 979, 123, 1047, 548, 1005, 390, 1146, 392, 1025, 961, 57, 859, 1083, 13, 274, 646, 1011, 183, 1066, 336, 739, 817, 177, 579, 1135, 850, 1060, 269, 1171, 1207, 709, 342, 801, 295, 798, 467, 546, 1202, 973, 613, 967, 952, 543, 1069, 672, 1092, 185, 594, 501, 386, 913, 429, 694, 926, 851, 175, 347, 518, 684, 604, 830, 583, 841, 249, 1195, 692, 273, 325, 346, 285, 340, 171, 618, 1162, 233, 281, 1010, 432, 426, 461, 678, 1012, 964, 144, 622, 921, 434, 503, 189, 1191, 1165, 958, 674, 803, 854, 481, 680, 51, 480, 409, 451, 878, 318, 1203, 577, 599, 621, 757, 748, 536, 58, 201, 80, 0, 1062, 769, 1013, 332, 84, 250, 569, 337, 8, 223, 837, 645, 244, 936, 842, 537, 270, 334, 92, 1134, 1186, 299, 774, 787, 554, 424, 352, 865, 744, 208, 928, 308, 986, 1190, 156, 797, 766, 1115, 412, 715, 668, 712, 212, 1161, 908, 703, 197, 507, 436, 679, 440, 457, 1018, 65, 1076, 1053, 20, 35, 737, 176, 1008, 891, 395, 1009, 126, 417, 1125, 890, 687, 1112, 700, 519, 382, 182, 88, 1185, 1114, 256, 415, 169, 1121, 228, 313, 1144, 673, 978, 394, 265, 508, 263, 443, 866, 239, 10, 607, 636, 676, 1084, 730, 1002, 138, 719, 1078, 756, 1035, 940, 486, 1174, 358, 283, 114, 885, 497, 237, 1111, 932, 1126, 38, 663, 397, 29, 887, 529, 750, 492, 1045, 615, 1023, 824, 331, 139, 1113, 95, 469, 717, 73, 848, 36, 298, 483, 1155, 78, 888, 1160, 40, 70, 158, 1116, 960, 252, 829, 710, 148, 1063, 289, 322, 154, 203, 16, 695, 323, 21, 122, 444, 278, 482, 320, 221, 351, 317, 54, 764, 789, 989, 752, 422, 363, 22, 328, 624, 1100, 880, 893, 26, 1042, 980, 689, 294, 920, 66, 580, 494, 56, 576, 493, 965, 991, 309, 1019, 1056, 420, 785, 300, 377, 455, 191, 384, 956, 683, 1167, 525, 187, 1102, 597, 47, 1006, 87, 647, 277, 881, 1001, 89, 815, 343, 387, 808, 396, 1128, 1087, 1094, 892, 179, 740, 1097, 248, 628, 200, 889, 682, 103, 643, 206, 614, 379, 654, 1201, 954, 369, 130, 357, 811, 658, 664, 528, 30, 620, 995, 106, 391, 487, 23, 631, 1058, 969, 876, 445, 7, 121, 403, 71, 900, 9, 714, 1127, 567, 745, 783, 460, 982, 914, 902, 1020, 401, 86, 1044, 204, 50, 327, 192, 724, 1022, 1070, 818, 381, 411, 974, 1064, 6]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9297594232673226
the save name prefix for this run is:  chkpt-ID_9297594232673226_tag_Ablation-job-blacklist-s max deg neighbnour_o max deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 919
rank avg (pred): 0.442 +- 0.003
mrr vals (pred, true): 0.017, 0.020
batch losses (mrrl, rdl): 0.0, 0.0010383229

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 825
rank avg (pred): 0.119 +- 0.089
mrr vals (pred, true): 0.238, 0.308
batch losses (mrrl, rdl): 0.0, 2.49366e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 254
rank avg (pred): 0.039 +- 0.031
mrr vals (pred, true): 0.385, 0.550
batch losses (mrrl, rdl): 0.0, 8.555e-07

Epoch over!
epoch time: 12.119

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 245
rank avg (pred): 0.039 +- 0.032
mrr vals (pred, true): 0.406, 0.535
batch losses (mrrl, rdl): 0.0, 8.805e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1018
rank avg (pred): 0.335 +- 0.261
mrr vals (pred, true): 0.231, 0.073
batch losses (mrrl, rdl): 0.0, 4.6472e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 422
rank avg (pred): 0.379 +- 0.262
mrr vals (pred, true): 0.153, 0.053
batch losses (mrrl, rdl): 0.0, 2.11068e-05

Epoch over!
epoch time: 11.861

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 579
rank avg (pred): 0.430 +- 0.278
mrr vals (pred, true): 0.150, 0.040
batch losses (mrrl, rdl): 0.0, 2.19136e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1209
rank avg (pred): 0.437 +- 0.272
mrr vals (pred, true): 0.126, 0.054
batch losses (mrrl, rdl): 0.0, 1.53402e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 313
rank avg (pred): 0.035 +- 0.029
mrr vals (pred, true): 0.425, 0.555
batch losses (mrrl, rdl): 0.0, 3.371e-07

Epoch over!
epoch time: 12.097

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 465
rank avg (pred): 0.356 +- 0.264
mrr vals (pred, true): 0.193, 0.048
batch losses (mrrl, rdl): 0.0, 7.53164e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 886
rank avg (pred): 0.406 +- 0.273
mrr vals (pred, true): 0.142, 0.054
batch losses (mrrl, rdl): 0.0, 4.6422e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 160
rank avg (pred): 0.374 +- 0.261
mrr vals (pred, true): 0.161, 0.082
batch losses (mrrl, rdl): 0.0, 4.27435e-05

Epoch over!
epoch time: 11.99

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 103
rank avg (pred): 0.371 +- 0.263
mrr vals (pred, true): 0.159, 0.146
batch losses (mrrl, rdl): 0.0, 0.000384226

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 910
rank avg (pred): 0.424 +- 0.256
mrr vals (pred, true): 0.095, 0.078
batch losses (mrrl, rdl): 0.0, 7.42736e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1097
rank avg (pred): 0.332 +- 0.252
mrr vals (pred, true): 0.193, 0.101
batch losses (mrrl, rdl): 0.0, 5.84704e-05

Epoch over!
epoch time: 12.108

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 837
rank avg (pred): 0.467 +- 0.286
mrr vals (pred, true): 0.114, 0.052
batch losses (mrrl, rdl): 0.04077911, 1.82735e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 132
rank avg (pred): 0.312 +- 0.149
mrr vals (pred, true): 0.079, 0.128
batch losses (mrrl, rdl): 0.0234339461, 6.58775e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 21
rank avg (pred): 0.012 +- 0.009
mrr vals (pred, true): 0.510, 0.549
batch losses (mrrl, rdl): 0.0154795134, 1.65635e-05

Epoch over!
epoch time: 12.281

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 989
rank avg (pred): 0.008 +- 0.006
mrr vals (pred, true): 0.576, 0.559
batch losses (mrrl, rdl): 0.0030198148, 1.47158e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 599
rank avg (pred): 0.455 +- 0.176
mrr vals (pred, true): 0.042, 0.039
batch losses (mrrl, rdl): 0.0006034815, 5.2583e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 169
rank avg (pred): 0.430 +- 0.202
mrr vals (pred, true): 0.057, 0.048
batch losses (mrrl, rdl): 0.0004384276, 1.2179e-05

Epoch over!
epoch time: 12.113

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 24
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.576, 0.547
batch losses (mrrl, rdl): 0.0086370073, 1.60093e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 10
rank avg (pred): 0.011 +- 0.008
mrr vals (pred, true): 0.548, 0.545
batch losses (mrrl, rdl): 5.25781e-05, 1.453e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 377
rank avg (pred): 0.387 +- 0.215
mrr vals (pred, true): 0.082, 0.116
batch losses (mrrl, rdl): 0.0116095636, 0.0002532303

Epoch over!
epoch time: 12.182

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 228
rank avg (pred): 0.404 +- 0.200
mrr vals (pred, true): 0.064, 0.044
batch losses (mrrl, rdl): 0.0019018815, 2.08897e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 131
rank avg (pred): 0.380 +- 0.202
mrr vals (pred, true): 0.079, 0.131
batch losses (mrrl, rdl): 0.0266286843, 0.0004053951

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 233
rank avg (pred): 0.408 +- 0.192
mrr vals (pred, true): 0.059, 0.047
batch losses (mrrl, rdl): 0.0007554892, 3.53736e-05

Epoch over!
epoch time: 12.241

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 205
rank avg (pred): 0.377 +- 0.183
mrr vals (pred, true): 0.065, 0.044
batch losses (mrrl, rdl): 0.0021578693, 0.0001141278

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1114
rank avg (pred): 0.343 +- 0.158
mrr vals (pred, true): 0.069, 0.050
batch losses (mrrl, rdl): 0.0035157707, 0.0001430291

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 877
rank avg (pred): 0.414 +- 0.172
mrr vals (pred, true): 0.059, 0.046
batch losses (mrrl, rdl): 0.0008286, 3.63511e-05

Epoch over!
epoch time: 12.29

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 19
rank avg (pred): 0.009 +- 0.007
mrr vals (pred, true): 0.579, 0.549
batch losses (mrrl, rdl): 0.0090651885, 1.74273e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 566
rank avg (pred): 0.497 +- 0.296
mrr vals (pred, true): 0.107, 0.100
batch losses (mrrl, rdl): 0.0326885432, 0.0004253174

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1021
rank avg (pred): 0.344 +- 0.187
mrr vals (pred, true): 0.083, 0.141
batch losses (mrrl, rdl): 0.033649195, 0.0001600906

Epoch over!
epoch time: 12.259

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1002
rank avg (pred): 0.381 +- 0.192
mrr vals (pred, true): 0.068, 0.141
batch losses (mrrl, rdl): 0.0535437986, 0.0002495979

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 78
rank avg (pred): 0.016 +- 0.013
mrr vals (pred, true): 0.510, 0.506
batch losses (mrrl, rdl): 0.0001430378, 2.50502e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 70
rank avg (pred): 0.012 +- 0.010
mrr vals (pred, true): 0.557, 0.535
batch losses (mrrl, rdl): 0.004716726, 2.17391e-05

Epoch over!
epoch time: 12.097

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 808
rank avg (pred): 0.443 +- 0.155
mrr vals (pred, true): 0.053, 0.047
batch losses (mrrl, rdl): 9.26556e-05, 2.7896e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 440
rank avg (pred): 0.448 +- 0.203
mrr vals (pred, true): 0.058, 0.051
batch losses (mrrl, rdl): 0.0007202552, 2.05661e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1169
rank avg (pred): 0.501 +- 0.229
mrr vals (pred, true): 0.057, 0.039
batch losses (mrrl, rdl): 0.000535833, 2.82119e-05

Epoch over!
epoch time: 12.087

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 26
rank avg (pred): 0.017 +- 0.015
mrr vals (pred, true): 0.515, 0.516
batch losses (mrrl, rdl): 5.1869e-06, 1.33801e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 91
rank avg (pred): 0.391 +- 0.211
mrr vals (pred, true): 0.079, 0.103
batch losses (mrrl, rdl): 0.005557782, 0.0001850178

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1201
rank avg (pred): 0.432 +- 0.168
mrr vals (pred, true): 0.060, 0.049
batch losses (mrrl, rdl): 0.0009652327, 2.99856e-05

Epoch over!
epoch time: 12.422

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 887
rank avg (pred): 0.459 +- 0.170
mrr vals (pred, true): 0.054, 0.038
batch losses (mrrl, rdl): 0.0001557482, 2.91713e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 927
rank avg (pred): 0.540 +- 0.202
mrr vals (pred, true): 0.045, 0.018
batch losses (mrrl, rdl): 0.000226608, 0.0005068536

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 603
rank avg (pred): 0.486 +- 0.170
mrr vals (pred, true): 0.053, 0.042
batch losses (mrrl, rdl): 7.75997e-05, 3.03231e-05

Epoch over!
epoch time: 12.312

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.336 +- 0.152
mrr vals (pred, true): 0.073, 0.040

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.04688 	 0.01573 	 m..s
   43 	     1 	 0.06026 	 0.02851 	 m..s
   36 	     2 	 0.05682 	 0.02964 	 ~...
   31 	     3 	 0.05571 	 0.03521 	 ~...
   44 	     4 	 0.06033 	 0.03595 	 ~...
   20 	     5 	 0.05464 	 0.03661 	 ~...
   12 	     6 	 0.05399 	 0.03872 	 ~...
   30 	     7 	 0.05565 	 0.03917 	 ~...
   26 	     8 	 0.05536 	 0.03921 	 ~...
    9 	     9 	 0.05249 	 0.04000 	 ~...
    5 	    10 	 0.05037 	 0.04008 	 ~...
   70 	    11 	 0.07286 	 0.04036 	 m..s
   38 	    12 	 0.05762 	 0.04197 	 ~...
   57 	    13 	 0.06802 	 0.04216 	 ~...
   27 	    14 	 0.05540 	 0.04337 	 ~...
    6 	    15 	 0.05043 	 0.04361 	 ~...
   60 	    16 	 0.06893 	 0.04374 	 ~...
   37 	    17 	 0.05731 	 0.04423 	 ~...
   80 	    18 	 0.08175 	 0.04440 	 m..s
   54 	    19 	 0.06545 	 0.04450 	 ~...
   85 	    20 	 0.09228 	 0.04464 	 m..s
   32 	    21 	 0.05581 	 0.04465 	 ~...
   14 	    22 	 0.05414 	 0.04470 	 ~...
   40 	    23 	 0.05775 	 0.04476 	 ~...
   25 	    24 	 0.05508 	 0.04487 	 ~...
   22 	    25 	 0.05482 	 0.04487 	 ~...
   39 	    26 	 0.05765 	 0.04534 	 ~...
   10 	    27 	 0.05359 	 0.04559 	 ~...
   79 	    28 	 0.08007 	 0.04581 	 m..s
   66 	    29 	 0.07013 	 0.04608 	 ~...
   52 	    30 	 0.06494 	 0.04631 	 ~...
   82 	    31 	 0.08420 	 0.04692 	 m..s
   56 	    32 	 0.06787 	 0.04698 	 ~...
   83 	    33 	 0.08680 	 0.04733 	 m..s
   46 	    34 	 0.06112 	 0.04741 	 ~...
   33 	    35 	 0.05606 	 0.04744 	 ~...
   59 	    36 	 0.06875 	 0.04745 	 ~...
   45 	    37 	 0.06045 	 0.04775 	 ~...
   65 	    38 	 0.06999 	 0.04832 	 ~...
   71 	    39 	 0.07346 	 0.04836 	 ~...
   24 	    40 	 0.05494 	 0.04856 	 ~...
    4 	    41 	 0.05020 	 0.04877 	 ~...
   88 	    42 	 0.09302 	 0.04934 	 m..s
   51 	    43 	 0.06480 	 0.04951 	 ~...
   63 	    44 	 0.06950 	 0.04957 	 ~...
   41 	    45 	 0.05814 	 0.05005 	 ~...
    2 	    46 	 0.04978 	 0.05028 	 ~...
   21 	    47 	 0.05478 	 0.05049 	 ~...
   11 	    48 	 0.05399 	 0.05069 	 ~...
   34 	    49 	 0.05648 	 0.05095 	 ~...
    1 	    50 	 0.04701 	 0.05096 	 ~...
   28 	    51 	 0.05540 	 0.05139 	 ~...
   23 	    52 	 0.05490 	 0.05143 	 ~...
    3 	    53 	 0.04995 	 0.05244 	 ~...
   17 	    54 	 0.05429 	 0.05266 	 ~...
   42 	    55 	 0.05862 	 0.05274 	 ~...
    7 	    56 	 0.05070 	 0.05275 	 ~...
   35 	    57 	 0.05649 	 0.05281 	 ~...
   75 	    58 	 0.07636 	 0.05366 	 ~...
   15 	    59 	 0.05424 	 0.05523 	 ~...
   53 	    60 	 0.06509 	 0.05570 	 ~...
   50 	    61 	 0.06479 	 0.05593 	 ~...
   29 	    62 	 0.05541 	 0.05749 	 ~...
    8 	    63 	 0.05247 	 0.06182 	 ~...
   13 	    64 	 0.05404 	 0.06967 	 ~...
   16 	    65 	 0.05426 	 0.07415 	 ~...
   87 	    66 	 0.09268 	 0.07881 	 ~...
   58 	    67 	 0.06850 	 0.07982 	 ~...
   18 	    68 	 0.05441 	 0.08147 	 ~...
   78 	    69 	 0.07923 	 0.08490 	 ~...
   19 	    70 	 0.05448 	 0.08955 	 m..s
   67 	    71 	 0.07013 	 0.09096 	 ~...
   69 	    72 	 0.07285 	 0.09399 	 ~...
   47 	    73 	 0.06373 	 0.09621 	 m..s
   64 	    74 	 0.06960 	 0.10048 	 m..s
   73 	    75 	 0.07464 	 0.10147 	 ~...
   62 	    76 	 0.06950 	 0.10361 	 m..s
   48 	    77 	 0.06447 	 0.10496 	 m..s
   81 	    78 	 0.08360 	 0.10907 	 ~...
   76 	    79 	 0.07674 	 0.10981 	 m..s
   55 	    80 	 0.06705 	 0.11789 	 m..s
   49 	    81 	 0.06476 	 0.11912 	 m..s
   61 	    82 	 0.06893 	 0.11955 	 m..s
   84 	    83 	 0.09046 	 0.12681 	 m..s
   68 	    84 	 0.07209 	 0.13195 	 m..s
   77 	    85 	 0.07826 	 0.14243 	 m..s
   95 	    86 	 0.31180 	 0.14495 	 MISS
   89 	    87 	 0.09497 	 0.14821 	 m..s
   86 	    88 	 0.09234 	 0.15403 	 m..s
   74 	    89 	 0.07471 	 0.16228 	 m..s
   90 	    90 	 0.14829 	 0.16880 	 ~...
   72 	    91 	 0.07394 	 0.17769 	 MISS
   92 	    92 	 0.22606 	 0.17970 	 m..s
   94 	    93 	 0.28831 	 0.22655 	 m..s
   91 	    94 	 0.22333 	 0.24278 	 ~...
   93 	    95 	 0.27324 	 0.25994 	 ~...
   99 	    96 	 0.43648 	 0.35869 	 m..s
   97 	    97 	 0.41925 	 0.41169 	 ~...
   96 	    98 	 0.31610 	 0.43149 	 MISS
   98 	    99 	 0.43452 	 0.46923 	 m..s
  102 	   100 	 0.54303 	 0.50782 	 m..s
  100 	   101 	 0.50358 	 0.51622 	 ~...
  101 	   102 	 0.54236 	 0.52260 	 ~...
  104 	   103 	 0.54638 	 0.52699 	 ~...
  103 	   104 	 0.54627 	 0.52720 	 ~...
  113 	   105 	 0.56716 	 0.53145 	 m..s
  106 	   106 	 0.56187 	 0.53150 	 m..s
  115 	   107 	 0.58426 	 0.53525 	 m..s
  109 	   108 	 0.56293 	 0.53995 	 ~...
  110 	   109 	 0.56368 	 0.54080 	 ~...
  108 	   110 	 0.56226 	 0.54126 	 ~...
  112 	   111 	 0.56704 	 0.54412 	 ~...
  107 	   112 	 0.56218 	 0.54487 	 ~...
  120 	   113 	 0.59525 	 0.54784 	 m..s
  105 	   114 	 0.54781 	 0.54835 	 ~...
  118 	   115 	 0.58482 	 0.54935 	 m..s
  111 	   116 	 0.56692 	 0.55300 	 ~...
  114 	   117 	 0.57290 	 0.55441 	 ~...
  116 	   118 	 0.58459 	 0.56058 	 ~...
  119 	   119 	 0.58517 	 0.56083 	 ~...
  117 	   120 	 0.58480 	 0.56122 	 ~...
==========================================
r_mrr = 0.9841842651367188
r2_mrr = 0.9651935696601868
spearmanr_mrr@5 = 0.48293089866638184
spearmanr_mrr@10 = 0.8624276518821716
spearmanr_mrr@50 = 0.9933508038520813
spearmanr_mrr@100 = 0.9933205246925354
spearmanr_mrr@All = 0.9933546781539917
==========================================
test time: 0.477
Done Testing dataset UMLS
total time taken: 189.12519311904907
training time taken: 183.0134847164154
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9842)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9652)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.4829)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8624)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9934)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9933)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9934)}}, 'test_loss': {'DistMult': {'UMLS': 1.411887641517751}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max deg neighbnour', 's max deg neighbnour'}

======================================================================================================
------------------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
------------------------------------------------------------------------------------------------------
======================================================================================================
Running on a grid of size 1
Using random seed: 4595585880310682
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [388, 540, 865, 1067, 219, 452, 128, 520, 82, 1194, 812, 1145, 796, 451, 595, 1184, 501, 269, 820, 75, 725, 346, 317, 1133, 999, 937, 907, 677, 49, 1135, 1077, 1192, 376, 1018, 1183, 234, 697, 408, 781, 961, 596, 475, 723, 1102, 551, 766, 934, 915, 787, 261, 1016, 87, 561, 514, 951, 849, 375, 629, 558, 424, 41, 614, 762, 1083, 944, 120, 410, 1031, 1076, 1042, 136, 378, 608, 1011, 179, 827, 557, 0, 125, 1171, 928, 609, 1052, 946, 436, 981, 1108, 839, 444, 74, 1149, 1071, 230, 461, 326, 488, 980, 38, 1087, 768, 736, 496, 936, 1091, 1061, 441, 312, 301, 503, 751, 902, 840, 309, 960, 917, 689, 547, 354, 888, 583, 1169]
valid_ids (0): []
train_ids (1094): [214, 860, 743, 535, 474, 1072, 783, 339, 536, 1144, 458, 704, 1147, 1006, 977, 447, 712, 460, 1004, 328, 351, 299, 925, 331, 735, 133, 20, 1179, 683, 330, 678, 1103, 590, 493, 858, 306, 1025, 693, 806, 575, 355, 1134, 1180, 985, 127, 39, 563, 94, 793, 538, 1195, 1120, 797, 969, 726, 11, 40, 43, 421, 1211, 641, 566, 308, 177, 727, 236, 1054, 976, 525, 202, 788, 28, 967, 276, 1117, 546, 1146, 1062, 739, 701, 1079, 950, 430, 871, 549, 155, 186, 630, 957, 192, 758, 366, 764, 51, 635, 978, 18, 286, 502, 809, 1056, 1206, 1074, 504, 524, 35, 84, 416, 1188, 497, 1037, 882, 19, 457, 37, 938, 1170, 989, 495, 382, 91, 541, 1167, 322, 478, 500, 114, 1209, 34, 918, 259, 325, 464, 792, 876, 449, 879, 1047, 920, 134, 1043, 229, 73, 1093, 794, 836, 819, 627, 1201, 151, 422, 974, 318, 67, 613, 526, 107, 209, 838, 13, 141, 42, 32, 365, 336, 529, 89, 949, 878, 696, 657, 1051, 801, 210, 1007, 1106, 775, 687, 645, 1115, 958, 906, 383, 581, 706, 675, 837, 1138, 168, 356, 599, 682, 972, 1199, 1023, 1033, 767, 1212, 598, 1125, 624, 92, 1173, 756, 579, 1040, 1082, 110, 498, 224, 333, 130, 188, 1113, 637, 446, 785, 287, 971, 901, 982, 208, 909, 869, 477, 810, 1126, 303, 667, 390, 250, 1143, 691, 803, 1039, 1168, 752, 1129, 165, 257, 203, 513, 244, 572, 198, 361, 543, 634, 1196, 98, 965, 533, 206, 160, 968, 720, 845, 872, 491, 834, 8, 1160, 272, 892, 252, 518, 745, 509, 459, 176, 404, 988, 400, 190, 335, 567, 830, 44, 143, 811, 1048, 53, 1109, 63, 144, 1068, 352, 1104, 211, 419, 164, 698, 610, 121, 782, 732, 774, 294, 398, 861, 1015, 101, 653, 896, 226, 601, 759, 1154, 104, 199, 1022, 207, 146, 991, 997, 508, 126, 48, 649, 555, 170, 631, 277, 138, 377, 505, 432, 275, 485, 552, 194, 1034, 183, 738, 402, 147, 266, 467, 1110, 1012, 883, 1114, 577, 1009, 232, 406, 1041, 843, 476, 510, 342, 197, 620, 469, 973, 1142, 1096, 887, 770, 671, 562, 1176, 1127, 1159, 640, 201, 674, 799, 1075, 426, 233, 707, 757, 407, 1090, 626, 282, 486, 17, 670, 135, 59, 519, 494, 145, 189, 633, 445, 1029, 531, 271, 1122, 1044, 340, 619, 862, 72, 580, 578, 76, 1081, 874, 948, 718, 857, 688, 360, 399, 821, 995, 1128, 1151, 24, 945, 156, 223, 154, 332, 1116, 215, 795, 933, 60, 929, 908, 260, 492, 647, 471, 714, 484, 185, 337, 55, 293, 288, 983, 600, 329, 656, 1163, 1038, 530, 1130, 109, 1028, 427, 913, 487, 1137, 900, 798, 643, 172, 69, 975, 602, 713, 852, 853, 1165, 623, 1045, 456, 220, 36, 550, 831, 964, 894, 943, 870, 440, 1198, 1046, 175, 415, 264, 1050, 216, 813, 822, 26, 606, 30, 196, 658, 923, 746, 428, 1002, 393, 664, 826, 695, 709, 1024, 638, 187, 576, 1136, 521, 771, 1182, 313, 780, 80, 523, 490, 1013, 279, 846, 374, 280, 411, 132, 153, 243, 676, 717, 1161, 240, 62, 668, 397, 438, 1099, 47, 511, 1186, 574, 784, 573, 439, 711, 1197, 195, 654, 21, 1119, 644, 1140, 603, 632, 1191, 180, 1060, 911, 966, 113, 2, 570, 680, 81, 405, 754, 534, 516, 434, 265, 1124, 499, 642, 947, 1153, 86, 615, 777, 1185, 1, 730, 661, 23, 320, 765, 939, 1204, 802, 64, 646, 418, 897, 465, 1010, 300, 169, 481, 1080, 88, 564, 979, 297, 1019, 545, 112, 1088, 1214, 554, 1095, 686, 228, 50, 517, 1105, 263, 618, 1084, 790, 855, 984, 970, 448, 1139, 1059, 385, 921, 314, 565, 662, 345, 150, 305, 992, 122, 591, 854, 221, 560, 85, 817, 225, 1020, 1073, 379, 367, 1014, 14, 942, 1187, 129, 1049, 281, 710, 825, 222, 914, 859, 205, 990, 721, 159, 96, 54, 744, 111, 741, 249, 124, 715, 585, 68, 708, 1027, 6, 395, 241, 589, 1157, 506, 1026, 479, 255, 181, 903, 289, 731, 472, 4, 716, 1057, 1175, 804, 173, 284, 79, 359, 231, 895, 387, 941, 256, 384, 319, 1150, 414, 663, 423, 470, 703, 954, 1030, 334, 184, 639, 648, 152, 381, 12, 1202, 528, 728, 679, 344, 905, 962, 953, 848, 267, 694, 1152, 149, 315, 749, 1035, 994, 636, 927, 401, 235, 115, 1055, 864, 1086, 1207, 25, 559, 108, 242, 396, 83, 556, 295, 171, 116, 327, 453, 1164, 450, 1111, 323, 700, 245, 926, 246, 1101, 886, 212, 893, 239, 412, 455, 1181, 57, 166, 597, 290, 829, 254, 856, 357, 515, 1089, 372, 586, 137, 916, 321, 553, 99, 705, 773, 607, 621, 274, 311, 77, 786, 1200, 778, 22, 217, 996, 835, 587, 659, 1069, 262, 368, 489, 924, 1210, 425, 588, 1193, 193, 462, 1000, 507, 763, 1053, 748, 1036, 729, 413, 594, 625, 370, 660, 998, 10, 304, 877, 97, 512, 386, 273, 347, 1003, 779, 52, 1141, 316, 733, 433, 1158, 1190, 956, 58, 932, 815, 409, 358, 268, 364, 218, 343, 291, 1166, 772, 747, 940, 702, 1065, 685, 571, 31, 544, 750, 139, 1132, 1100, 881, 690, 665, 963, 655, 103, 1174, 7, 1005, 33, 106, 666, 442, 776, 466, 842, 278, 734, 986, 1097, 161, 1118, 1078, 1094, 1008, 302, 394, 29, 722, 338, 100, 650, 1178, 740, 71, 417, 542, 371, 922, 955, 463, 119, 348, 828, 296, 885, 866, 78, 292, 527, 93, 1208, 429, 537, 363, 611, 15, 253, 118, 868, 285, 1189, 959, 307, 480, 1032, 889, 369, 616, 163, 420, 760, 45, 832, 341, 1107, 1205, 875, 873, 952, 403, 27, 65, 258, 380, 3, 807, 66, 298, 805, 684, 851, 568, 1021, 1063, 157, 1156, 651, 823, 816, 719, 935, 353, 890, 761, 247, 373, 158, 737, 123, 612, 56, 1098, 389, 841, 131, 863, 1121, 473, 673, 884, 1058, 1155, 174, 227, 681, 930, 1085, 102, 1064, 652, 362, 1162, 251, 850, 824, 891, 548, 617, 569, 753, 808, 5, 1131, 919, 604, 248, 454, 1203, 117, 904, 213, 993, 283, 742, 238, 789, 468, 910, 1092, 880, 270, 1148, 1172, 90, 105, 1177, 1112, 431, 61, 582, 9, 435, 1017, 912, 437, 669, 482, 349, 724, 692, 593, 178, 847, 898, 755, 899, 443, 791, 522, 70, 672, 532, 191, 140, 844, 867, 699, 622, 628, 392, 1066, 584, 204, 605, 1070, 833, 95, 310, 167, 350, 800, 1123, 1213, 142, 769, 987, 483, 182, 391, 16, 46, 1001, 148, 200, 818, 931, 814, 162, 237, 539, 592, 324]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  5611550623534398
the save name prefix for this run is:  chkpt-ID_5611550623534398_tag_Ablation-job-blacklist-s mean deg neighbnour_o mean deg neighbnour
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 879
rank avg (pred): 0.477 +- 0.006
mrr vals (pred, true): 0.015, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001083337

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 336
rank avg (pred): 0.351 +- 0.001
mrr vals (pred, true): 0.021, 0.150
batch losses (mrrl, rdl): 0.0, 0.0002233793

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 633
rank avg (pred): 0.448 +- 0.000
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 8.68631e-05

Epoch over!
epoch time: 12.209

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 221
rank avg (pred): 0.377 +- 0.001
mrr vals (pred, true): 0.019, 0.047
batch losses (mrrl, rdl): 0.0, 0.0002096713

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 578
rank avg (pred): 0.421 +- 0.000
mrr vals (pred, true): 0.017, 0.036
batch losses (mrrl, rdl): 0.0, 0.0002360816

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 643
rank avg (pred): 0.445 +- 0.000
mrr vals (pred, true): 0.017, 0.047
batch losses (mrrl, rdl): 0.0, 9.8492e-05

Epoch over!
epoch time: 11.901

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 642
rank avg (pred): 0.463 +- 0.000
mrr vals (pred, true): 0.016, 0.045
batch losses (mrrl, rdl): 0.0, 9.45243e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1201
rank avg (pred): 0.459 +- 0.000
mrr vals (pred, true): 0.016, 0.049
batch losses (mrrl, rdl): 0.0, 9.30265e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1033
rank avg (pred): 0.378 +- 0.000
mrr vals (pred, true): 0.019, 0.050
batch losses (mrrl, rdl): 0.0, 0.0001306765

Epoch over!
epoch time: 11.913

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1112
rank avg (pred): 0.362 +- 0.000
mrr vals (pred, true): 0.020, 0.049
batch losses (mrrl, rdl): 0.0, 0.0001672588

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 947
rank avg (pred): 0.574 +- 0.000
mrr vals (pred, true): 0.013, 0.060
batch losses (mrrl, rdl): 0.0, 0.0004559477

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 481
rank avg (pred): 0.371 +- 0.000
mrr vals (pred, true): 0.020, 0.051
batch losses (mrrl, rdl): 0.0, 0.0002553861

Epoch over!
epoch time: 11.998

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 393
rank avg (pred): 0.372 +- 0.000
mrr vals (pred, true): 0.020, 0.113
batch losses (mrrl, rdl): 0.0, 0.0001648446

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1203
rank avg (pred): 0.422 +- 0.000
mrr vals (pred, true): 0.017, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001003246

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 174
rank avg (pred): 0.387 +- 0.000
mrr vals (pred, true): 0.019, 0.048
batch losses (mrrl, rdl): 0.0, 0.0001651401

Epoch over!
epoch time: 11.833

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 563
rank avg (pred): 0.333 +- 0.000
mrr vals (pred, true): 0.022, 0.112
batch losses (mrrl, rdl): 0.0806171745, 0.0001167799

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 27
rank avg (pred): 0.011 +- 0.000
mrr vals (pred, true): 0.410, 0.517
batch losses (mrrl, rdl): 0.1153178364, 2.495e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 48
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.594, 0.509
batch losses (mrrl, rdl): 0.0715695396, 3.64602e-05

Epoch over!
epoch time: 12.381

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 334
rank avg (pred): 0.098 +- 0.000
mrr vals (pred, true): 0.071, 0.154
batch losses (mrrl, rdl): 0.0696455017, 0.0006580703

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 972
rank avg (pred): 0.005 +- 0.000
mrr vals (pred, true): 0.606, 0.514
batch losses (mrrl, rdl): 0.0838689134, 4.09575e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 162
rank avg (pred): 0.093 +- 0.000
mrr vals (pred, true): 0.074, 0.044
batch losses (mrrl, rdl): 0.0057999766, 0.0026254114

Epoch over!
epoch time: 12.098

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 113
rank avg (pred): 0.086 +- 0.000
mrr vals (pred, true): 0.080, 0.101
batch losses (mrrl, rdl): 0.004561014, 0.0010583024

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 59
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.507, 0.525
batch losses (mrrl, rdl): 0.0033495752, 2.11169e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 466
rank avg (pred): 0.095 +- 0.000
mrr vals (pred, true): 0.073, 0.054
batch losses (mrrl, rdl): 0.0053359037, 0.0023832987

Epoch over!
epoch time: 12.014

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 564
rank avg (pred): 0.097 +- 0.000
mrr vals (pred, true): 0.071, 0.050
batch losses (mrrl, rdl): 0.0045034257, 0.0026892307

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1044
rank avg (pred): 0.087 +- 0.000
mrr vals (pred, true): 0.079, 0.040
batch losses (mrrl, rdl): 0.0086085629, 0.0027017687

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 734
rank avg (pred): 0.017 +- 0.000
mrr vals (pred, true): 0.306, 0.260
batch losses (mrrl, rdl): 0.0214197878, 0.0004333924

Epoch over!
epoch time: 12.152

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 446
rank avg (pred): 0.094 +- 0.000
mrr vals (pred, true): 0.073, 0.047
batch losses (mrrl, rdl): 0.0054800874, 0.0024675282

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 664
rank avg (pred): 0.137 +- 0.000
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 2.31655e-05, 0.001855933

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 757
rank avg (pred): 0.127 +- 0.000
mrr vals (pred, true): 0.056, 0.055
batch losses (mrrl, rdl): 0.0003239689, 0.0023221986

Epoch over!
epoch time: 12.052

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 31
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.499, 0.520
batch losses (mrrl, rdl): 0.0044847392, 2.55257e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 442
rank avg (pred): 0.086 +- 0.000
mrr vals (pred, true): 0.080, 0.043
batch losses (mrrl, rdl): 0.0089991316, 0.0028597983

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1050
rank avg (pred): 0.102 +- 0.000
mrr vals (pred, true): 0.068, 0.042
batch losses (mrrl, rdl): 0.0033718483, 0.0027308306

Epoch over!
epoch time: 12.295

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 825
rank avg (pred): 0.034 +- 0.000
mrr vals (pred, true): 0.182, 0.308
batch losses (mrrl, rdl): 0.1592171341, 0.0002691152

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 97
rank avg (pred): 0.115 +- 0.000
mrr vals (pred, true): 0.061, 0.142
batch losses (mrrl, rdl): 0.0664957166, 0.0006188949

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 950
rank avg (pred): 0.145 +- 0.000
mrr vals (pred, true): 0.049, 0.047
batch losses (mrrl, rdl): 1.08783e-05, 0.0017786832

Epoch over!
epoch time: 12.148

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 54
rank avg (pred): 0.006 +- 0.000
mrr vals (pred, true): 0.563, 0.528
batch losses (mrrl, rdl): 0.0121216811, 3.05521e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 65
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.530, 0.531
batch losses (mrrl, rdl): 9.7016e-06, 1.90952e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1030
rank avg (pred): 0.098 +- 0.000
mrr vals (pred, true): 0.071, 0.045
batch losses (mrrl, rdl): 0.0042954758, 0.0024515402

Epoch over!
epoch time: 12.095

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1198
rank avg (pred): 0.152 +- 0.000
mrr vals (pred, true): 0.047, 0.047
batch losses (mrrl, rdl): 9.34953e-05, 0.0019488042

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 993
rank avg (pred): 0.006 +- 0.000
mrr vals (pred, true): 0.555, 0.557
batch losses (mrrl, rdl): 2.65382e-05, 1.87614e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 612
rank avg (pred): 0.158 +- 0.000
mrr vals (pred, true): 0.045, 0.045
batch losses (mrrl, rdl): 0.0002459662, 0.0022104799

Epoch over!
epoch time: 12.271

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 750
rank avg (pred): 0.007 +- 0.000
mrr vals (pred, true): 0.503, 0.408
batch losses (mrrl, rdl): 0.0891146958, 0.0001095422

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 755
rank avg (pred): 0.028 +- 0.000
mrr vals (pred, true): 0.209, 0.245
batch losses (mrrl, rdl): 0.01324597, 0.0006212997

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 964
rank avg (pred): 0.151 +- 0.000
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 8.11371e-05, 0.0020512317

Epoch over!
epoch time: 12.196

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.084 +- 0.000
mrr vals (pred, true): 0.081, 0.110

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   32 	     0 	 0.05384 	 0.01638 	 m..s
   34 	     1 	 0.05385 	 0.01638 	 m..s
    8 	     2 	 0.04839 	 0.01822 	 m..s
   63 	     3 	 0.07258 	 0.01876 	 m..s
    7 	     4 	 0.04838 	 0.01975 	 ~...
    6 	     5 	 0.04836 	 0.02125 	 ~...
   36 	     6 	 0.05426 	 0.02851 	 ~...
   30 	     7 	 0.05333 	 0.03204 	 ~...
   37 	     8 	 0.05429 	 0.03231 	 ~...
   38 	     9 	 0.05559 	 0.03469 	 ~...
   11 	    10 	 0.04856 	 0.03661 	 ~...
    9 	    11 	 0.04845 	 0.03707 	 ~...
   17 	    12 	 0.04918 	 0.03917 	 ~...
   46 	    13 	 0.05882 	 0.03921 	 ~...
    4 	    14 	 0.04832 	 0.03932 	 ~...
   13 	    15 	 0.04871 	 0.04074 	 ~...
   56 	    16 	 0.07045 	 0.04091 	 ~...
   26 	    17 	 0.05205 	 0.04145 	 ~...
   22 	    18 	 0.04957 	 0.04187 	 ~...
   69 	    19 	 0.07540 	 0.04283 	 m..s
   77 	    20 	 0.07963 	 0.04294 	 m..s
   64 	    21 	 0.07308 	 0.04299 	 m..s
   51 	    22 	 0.06808 	 0.04308 	 ~...
   16 	    23 	 0.04905 	 0.04361 	 ~...
   31 	    24 	 0.05384 	 0.04366 	 ~...
   33 	    25 	 0.05384 	 0.04377 	 ~...
   10 	    26 	 0.04854 	 0.04401 	 ~...
   21 	    27 	 0.04955 	 0.04402 	 ~...
    5 	    28 	 0.04834 	 0.04475 	 ~...
    0 	    29 	 0.04303 	 0.04514 	 ~...
   39 	    30 	 0.05561 	 0.04575 	 ~...
   74 	    31 	 0.07857 	 0.04587 	 m..s
    3 	    32 	 0.04820 	 0.04595 	 ~...
   27 	    33 	 0.05218 	 0.04598 	 ~...
   78 	    34 	 0.08035 	 0.04604 	 m..s
   49 	    35 	 0.06707 	 0.04637 	 ~...
   14 	    36 	 0.04890 	 0.04652 	 ~...
   29 	    37 	 0.05333 	 0.04659 	 ~...
   89 	    38 	 0.08824 	 0.04666 	 m..s
   58 	    39 	 0.07105 	 0.04735 	 ~...
   76 	    40 	 0.07961 	 0.04763 	 m..s
   71 	    41 	 0.07595 	 0.04779 	 ~...
   15 	    42 	 0.04901 	 0.04783 	 ~...
   86 	    43 	 0.08524 	 0.04790 	 m..s
   60 	    44 	 0.07191 	 0.04828 	 ~...
   75 	    45 	 0.07937 	 0.04836 	 m..s
   24 	    46 	 0.05136 	 0.04840 	 ~...
   57 	    47 	 0.07074 	 0.04845 	 ~...
   28 	    48 	 0.05259 	 0.04905 	 ~...
   45 	    49 	 0.05881 	 0.04966 	 ~...
   84 	    50 	 0.08218 	 0.04972 	 m..s
   18 	    51 	 0.04927 	 0.04981 	 ~...
    0 	    52 	 0.04303 	 0.05028 	 ~...
   35 	    53 	 0.05421 	 0.05033 	 ~...
   12 	    54 	 0.04869 	 0.05040 	 ~...
   44 	    55 	 0.05848 	 0.05099 	 ~...
   62 	    56 	 0.07233 	 0.05129 	 ~...
   25 	    57 	 0.05203 	 0.05139 	 ~...
   41 	    58 	 0.05657 	 0.05344 	 ~...
   20 	    59 	 0.04949 	 0.05362 	 ~...
   40 	    60 	 0.05594 	 0.05551 	 ~...
   59 	    61 	 0.07187 	 0.05631 	 ~...
   23 	    62 	 0.05057 	 0.06789 	 ~...
   19 	    63 	 0.04941 	 0.06807 	 ~...
    2 	    64 	 0.04416 	 0.07007 	 ~...
   42 	    65 	 0.05693 	 0.07057 	 ~...
   47 	    66 	 0.06449 	 0.07225 	 ~...
   61 	    67 	 0.07194 	 0.07317 	 ~...
   50 	    68 	 0.06732 	 0.07365 	 ~...
   70 	    69 	 0.07555 	 0.07982 	 ~...
   73 	    70 	 0.07654 	 0.08039 	 ~...
   68 	    71 	 0.07513 	 0.08073 	 ~...
   43 	    72 	 0.05762 	 0.08147 	 ~...
   80 	    73 	 0.08065 	 0.08288 	 ~...
   67 	    74 	 0.07460 	 0.09621 	 ~...
   53 	    75 	 0.06927 	 0.09786 	 ~...
   83 	    76 	 0.08206 	 0.10013 	 ~...
   65 	    77 	 0.07317 	 0.10048 	 ~...
   87 	    78 	 0.08618 	 0.10064 	 ~...
   79 	    79 	 0.08035 	 0.10147 	 ~...
   66 	    80 	 0.07432 	 0.10197 	 ~...
   91 	    81 	 0.09868 	 0.10451 	 ~...
   52 	    82 	 0.06870 	 0.10750 	 m..s
   54 	    83 	 0.06936 	 0.10861 	 m..s
   88 	    84 	 0.08806 	 0.10979 	 ~...
   82 	    85 	 0.08150 	 0.10981 	 ~...
   81 	    86 	 0.08149 	 0.11343 	 m..s
   85 	    87 	 0.08318 	 0.12428 	 m..s
   90 	    88 	 0.09836 	 0.12983 	 m..s
   72 	    89 	 0.07633 	 0.13138 	 m..s
   48 	    90 	 0.06668 	 0.13160 	 m..s
   92 	    91 	 0.10503 	 0.13415 	 ~...
   55 	    92 	 0.07015 	 0.13476 	 m..s
   93 	    93 	 0.15429 	 0.19126 	 m..s
   97 	    94 	 0.22193 	 0.19363 	 ~...
   99 	    95 	 0.24329 	 0.23488 	 ~...
   98 	    96 	 0.23842 	 0.24069 	 ~...
   94 	    97 	 0.20670 	 0.24278 	 m..s
   96 	    98 	 0.21845 	 0.25029 	 m..s
   95 	    99 	 0.20930 	 0.32168 	 MISS
  100 	   100 	 0.45644 	 0.39802 	 m..s
  106 	   101 	 0.51457 	 0.51394 	 ~...
  101 	   102 	 0.48099 	 0.51763 	 m..s
  111 	   103 	 0.53129 	 0.52744 	 ~...
  115 	   104 	 0.54384 	 0.53036 	 ~...
  109 	   105 	 0.52072 	 0.53184 	 ~...
  114 	   106 	 0.54160 	 0.53387 	 ~...
  110 	   107 	 0.52997 	 0.53957 	 ~...
  108 	   108 	 0.51735 	 0.54053 	 ~...
  119 	   109 	 0.55444 	 0.54154 	 ~...
  117 	   110 	 0.54774 	 0.54255 	 ~...
  103 	   111 	 0.50706 	 0.54319 	 m..s
  113 	   112 	 0.53686 	 0.54425 	 ~...
  112 	   113 	 0.53668 	 0.54489 	 ~...
  120 	   114 	 0.55610 	 0.54676 	 ~...
  118 	   115 	 0.55390 	 0.54879 	 ~...
  105 	   116 	 0.50850 	 0.54935 	 m..s
  102 	   117 	 0.48575 	 0.55166 	 m..s
  104 	   118 	 0.50707 	 0.55334 	 m..s
  116 	   119 	 0.54513 	 0.55859 	 ~...
  107 	   120 	 0.51718 	 0.56101 	 m..s
==========================================
r_mrr = 0.9891917109489441
r2_mrr = 0.9775205850601196
spearmanr_mrr@5 = 0.8950187563896179
spearmanr_mrr@10 = 0.9514389038085938
spearmanr_mrr@50 = 0.9964542984962463
spearmanr_mrr@100 = 0.9949958324432373
spearmanr_mrr@All = 0.9947395920753479
==========================================
test time: 0.4
Done Testing dataset UMLS
total time taken: 187.7315957546234
training time taken: 182.021910905838
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9892)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9775)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8950)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9514)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9965)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9950)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9947)}}, 'test_loss': {'DistMult': {'UMLS': 0.9929392613485106}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean deg neighbnour', 'o mean deg neighbnour'}

==============================================================================================
----------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num neighbnours_o num neighbnours
----------------------------------------------------------------------------------------------
==============================================================================================
Running on a grid of size 1
Using random seed: 1612160193359283
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [938, 972, 594, 1062, 707, 268, 149, 492, 498, 164, 205, 169, 1059, 64, 410, 467, 857, 506, 168, 516, 141, 587, 982, 778, 995, 450, 153, 815, 52, 727, 462, 876, 1071, 1160, 7, 1065, 341, 533, 642, 614, 850, 156, 101, 195, 932, 828, 175, 38, 214, 1066, 69, 899, 956, 49, 239, 259, 50, 114, 1034, 210, 1052, 703, 1150, 352, 790, 829, 15, 436, 860, 574, 655, 877, 942, 912, 1077, 332, 641, 461, 1172, 821, 891, 402, 40, 1070, 741, 914, 864, 1035, 846, 830, 495, 204, 931, 93, 54, 1000, 883, 1105, 398, 274, 1085, 137, 194, 424, 490, 142, 80, 167, 980, 468, 759, 186, 1199, 249, 1146, 333, 228, 465, 109, 319, 913]
valid_ids (0): []
train_ids (1094): [643, 1054, 390, 459, 170, 1109, 1159, 423, 1076, 216, 1198, 796, 885, 666, 154, 904, 965, 890, 709, 702, 281, 1082, 543, 359, 95, 362, 305, 292, 1157, 378, 715, 211, 1009, 338, 173, 380, 57, 1184, 718, 1001, 408, 925, 1064, 859, 672, 652, 696, 845, 517, 162, 1058, 266, 792, 900, 496, 1080, 1135, 621, 855, 116, 512, 847, 620, 1132, 523, 1056, 923, 514, 470, 1144, 172, 474, 697, 43, 560, 147, 945, 1063, 767, 566, 1121, 48, 190, 1137, 227, 111, 488, 483, 356, 487, 978, 657, 96, 1049, 371, 671, 662, 577, 105, 250, 593, 629, 803, 501, 313, 497, 504, 957, 277, 330, 562, 276, 191, 695, 464, 705, 376, 152, 763, 988, 893, 412, 303, 690, 183, 133, 670, 664, 1033, 813, 881, 793, 768, 234, 393, 837, 589, 640, 532, 819, 404, 728, 1100, 983, 254, 608, 750, 2, 610, 1007, 576, 304, 388, 955, 355, 260, 689, 112, 311, 279, 820, 218, 324, 529, 1048, 946, 521, 622, 905, 789, 280, 325, 432, 746, 1020, 335, 1204, 618, 1142, 273, 761, 1191, 299, 23, 916, 421, 475, 350, 647, 201, 1189, 706, 947, 638, 903, 1188, 486, 1015, 688, 1086, 711, 700, 22, 235, 633, 535, 312, 18, 106, 811, 994, 122, 694, 775, 285, 1158, 26, 799, 525, 209, 977, 315, 624, 1061, 527, 76, 185, 45, 975, 723, 554, 1036, 328, 708, 872, 880, 744, 882, 480, 921, 970, 749, 1180, 635, 809, 1155, 399, 55, 91, 1029, 561, 1002, 1094, 351, 951, 25, 935, 752, 1067, 934, 329, 1167, 320, 964, 403, 895, 1053, 839, 555, 748, 918, 973, 21, 384, 413, 1092, 62, 129, 924, 851, 135, 814, 863, 676, 1143, 1127, 32, 1182, 817, 547, 862, 354, 1173, 306, 314, 827, 519, 997, 406, 578, 673, 791, 1210, 217, 753, 908, 505, 1113, 1031, 1003, 684, 1125, 582, 233, 826, 1038, 781, 558, 747, 202, 823, 739, 1045, 1117, 286, 919, 686, 220, 886, 255, 263, 454, 774, 646, 783, 472, 785, 1164, 1186, 258, 853, 625, 818, 262, 1108, 223, 960, 1040, 367, 1026, 71, 968, 836, 79, 317, 630, 110, 230, 119, 226, 471, 644, 445, 457, 1055, 418, 1177, 801, 447, 734, 756, 1043, 685, 1019, 118, 1141, 868, 572, 1018, 28, 639, 508, 1179, 383, 448, 681, 834, 270, 1128, 927, 735, 1193, 120, 929, 326, 309, 933, 143, 225, 677, 392, 717, 564, 1106, 909, 342, 1098, 27, 166, 47, 966, 298, 580, 1116, 874, 442, 1200, 1044, 430, 1081, 939, 1010, 1, 1073, 381, 856, 779, 650, 787, 100, 1089, 617, 39, 1114, 1075, 745, 337, 867, 293, 449, 146, 780, 1197, 999, 573, 452, 1097, 247, 769, 557, 431, 275, 444, 331, 81, 1041, 634, 757, 503, 556, 336, 1008, 8, 363, 1087, 1027, 282, 1178, 401, 522, 387, 518, 660, 1162, 981, 489, 1112, 13, 873, 160, 824, 509, 540, 692, 656, 427, 733, 887, 41, 295, 611, 658, 1124, 51, 161, 591, 720, 372, 682, 30, 1147, 651, 417, 788, 632, 419, 16, 502, 236, 294, 165, 848, 451, 871, 986, 797, 565, 145, 484, 396, 1131, 838, 550, 714, 499, 31, 1017, 11, 971, 36, 974, 65, 1057, 949, 928, 626, 1187, 963, 157, 539, 536, 1039, 316, 231, 825, 922, 439, 180, 123, 699, 721, 70, 1025, 14, 126, 301, 113, 24, 575, 579, 212, 1051, 667, 1004, 1016, 1023, 206, 967, 34, 272, 1154, 961, 806, 1183, 493, 1068, 425, 89, 606, 1096, 368, 108, 150, 1005, 1069, 17, 1014, 866, 807, 1030, 264, 366, 244, 736, 798, 713, 159, 422, 102, 99, 1169, 414, 1088, 346, 87, 510, 800, 669, 278, 976, 725, 340, 370, 598, 77, 842, 251, 858, 121, 0, 1176, 772, 852, 892, 615, 590, 1047, 1151, 198, 930, 546, 1111, 1161, 950, 958, 835, 906, 740, 151, 46, 265, 1032, 894, 491, 944, 178, 107, 537, 911, 548, 426, 737, 901, 473, 1060, 6, 365, 1122, 879, 203, 869, 1202, 758, 888, 1134, 810, 1208, 948, 229, 391, 453, 307, 1211, 870, 1195, 1022, 224, 680, 37, 520, 1168, 917, 1213, 177, 940, 989, 581, 171, 405, 455, 382, 72, 415, 1129, 463, 83, 310, 1207, 1093, 348, 1136, 513, 732, 954, 524, 637, 659, 252, 1138, 534, 66, 571, 369, 103, 645, 68, 795, 920, 10, 605, 138, 61, 738, 544, 477, 377, 90, 179, 124, 878, 551, 466, 131, 679, 321, 300, 1103, 693, 411, 1209, 649, 1149, 802, 751, 1050, 42, 567, 104, 1013, 654, 1175, 74, 400, 754, 29, 437, 1139, 188, 271, 619, 59, 538, 196, 33, 60, 1078, 623, 221, 588, 822, 1126, 985, 482, 428, 500, 5, 861, 322, 148, 898, 764, 1046, 1133, 443, 726, 139, 1115, 993, 1072, 596, 990, 559, 339, 1120, 35, 794, 242, 12, 1212, 782, 1192, 1024, 568, 193, 585, 353, 952, 1201, 78, 962, 144, 1166, 469, 553, 816, 584, 722, 832, 115, 192, 902, 831, 507, 88, 347, 597, 1101, 1079, 1084, 698, 616, 94, 586, 284, 786, 343, 843, 334, 607, 456, 1110, 476, 125, 875, 385, 215, 601, 261, 784, 528, 327, 82, 1148, 140, 840, 176, 84, 222, 805, 600, 675, 290, 361, 1104, 75, 1091, 53, 602, 238, 627, 663, 1006, 926, 479, 1196, 665, 1206, 397, 155, 189, 1021, 127, 460, 1095, 612, 1099, 770, 163, 1140, 1205, 854, 1012, 563, 760, 134, 743, 1203, 648, 3, 174, 996, 1118, 766, 1119, 130, 1130, 979, 628, 849, 440, 941, 357, 661, 73, 288, 1028, 604, 549, 776, 246, 943, 199, 1102, 485, 257, 4, 897, 613, 833, 729, 701, 386, 1214, 394, 674, 287, 429, 953, 937, 991, 511, 691, 592, 889, 1011, 478, 267, 58, 240, 291, 253, 712, 1037, 132, 213, 128, 599, 197, 207, 187, 297, 433, 884, 364, 1163, 716, 896, 344, 283, 1190, 910, 907, 136, 308, 1123, 1107, 86, 603, 1185, 812, 777, 1194, 583, 609, 765, 243, 19, 407, 959, 545, 44, 771, 998, 435, 724, 9, 992, 67, 1170, 97, 379, 289, 1074, 755, 318, 184, 1090, 804, 241, 182, 208, 389, 20, 984, 936, 730, 323, 416, 653, 569, 446, 541, 530, 481, 117, 595, 844, 678, 631, 92, 219, 1083, 1153, 1042, 841, 375, 762, 773, 256, 438, 269, 302, 969, 683, 742, 515, 1165, 808, 409, 374, 531, 1171, 494, 1174, 1145, 704, 865, 245, 248, 731, 1156, 1152, 200, 63, 542, 158, 719, 434, 441, 296, 1181, 360, 915, 85, 358, 987, 668, 56, 232, 373, 552, 710, 237, 570, 181, 420, 349, 458, 636, 345, 395, 98, 687, 526]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  751859937684772
the save name prefix for this run is:  chkpt-ID_751859937684772_tag_Ablation-job-blacklist-s num neighbnours_o num neighbnours
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 933
rank avg (pred): 0.425 +- 0.007
mrr vals (pred, true): 0.017, 0.015
batch losses (mrrl, rdl): 0.0, 0.0022213515

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 29
rank avg (pred): 0.054 +- 0.005
mrr vals (pred, true): 0.123, 0.523
batch losses (mrrl, rdl): 0.0, 4.2509e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 861
rank avg (pred): 0.441 +- 0.269
mrr vals (pred, true): 0.054, 0.104
batch losses (mrrl, rdl): 0.0, 2.50235e-05

Epoch over!
epoch time: 12.184

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 673
rank avg (pred): 0.491 +- 0.277
mrr vals (pred, true): 0.046, 0.047
batch losses (mrrl, rdl): 0.0, 3.34618e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1193
rank avg (pred): 0.469 +- 0.278
mrr vals (pred, true): 0.060, 0.053
batch losses (mrrl, rdl): 0.0, 1.74458e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 841
rank avg (pred): 0.406 +- 0.259
mrr vals (pred, true): 0.072, 0.037
batch losses (mrrl, rdl): 0.0, 0.0002013353

Epoch over!
epoch time: 11.961

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 227
rank avg (pred): 0.336 +- 0.268
mrr vals (pred, true): 0.123, 0.053
batch losses (mrrl, rdl): 0.0, 0.0001081556

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 732
rank avg (pred): 0.258 +- 0.237
mrr vals (pred, true): 0.114, 0.288
batch losses (mrrl, rdl): 0.0, 0.0002924755

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 691
rank avg (pred): 0.425 +- 0.286
mrr vals (pred, true): 0.097, 0.045
batch losses (mrrl, rdl): 0.0, 4.8218e-06

Epoch over!
epoch time: 12.022

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1159
rank avg (pred): 0.345 +- 0.295
mrr vals (pred, true): 0.123, 0.127
batch losses (mrrl, rdl): 0.0, 0.0001343591

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 242
rank avg (pred): 0.373 +- 0.274
mrr vals (pred, true): 0.082, 0.052
batch losses (mrrl, rdl): 0.0, 0.0001192272

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 631
rank avg (pred): 0.430 +- 0.279
mrr vals (pred, true): 0.084, 0.042
batch losses (mrrl, rdl): 0.0, 2.86733e-05

Epoch over!
epoch time: 12.21

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 711
rank avg (pred): 0.416 +- 0.281
mrr vals (pred, true): 0.101, 0.044
batch losses (mrrl, rdl): 0.0, 7.0366e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 363
rank avg (pred): 0.405 +- 0.281
mrr vals (pred, true): 0.063, 0.164
batch losses (mrrl, rdl): 0.0, 0.0005258094

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 637
rank avg (pred): 0.455 +- 0.294
mrr vals (pred, true): 0.077, 0.048
batch losses (mrrl, rdl): 0.0, 5.4896e-06

Epoch over!
epoch time: 11.979

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 408
rank avg (pred): 0.338 +- 0.263
mrr vals (pred, true): 0.134, 0.047
batch losses (mrrl, rdl): 0.0700220838, 0.0001869495

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 947
rank avg (pred): 0.582 +- 0.348
mrr vals (pred, true): 0.039, 0.060
batch losses (mrrl, rdl): 0.0011966731, 0.0002536558

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 833
rank avg (pred): 0.405 +- 0.474
mrr vals (pred, true): 0.477, 0.469
batch losses (mrrl, rdl): 0.0006677875, 0.0023289931

Epoch over!
epoch time: 12.246

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 582
rank avg (pred): 0.536 +- 0.370
mrr vals (pred, true): 0.043, 0.034
batch losses (mrrl, rdl): 0.0005382961, 7.14625e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 233
rank avg (pred): 0.460 +- 0.374
mrr vals (pred, true): 0.099, 0.047
batch losses (mrrl, rdl): 0.0244314447, 6.90185e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 334
rank avg (pred): 0.491 +- 0.381
mrr vals (pred, true): 0.068, 0.154
batch losses (mrrl, rdl): 0.0741827339, 0.0007898585

Epoch over!
epoch time: 12.124

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1047
rank avg (pred): 0.483 +- 0.374
mrr vals (pred, true): 0.073, 0.039
batch losses (mrrl, rdl): 0.0052492386, 7.00352e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 541
rank avg (pred): 0.484 +- 0.399
mrr vals (pred, true): 0.081, 0.083
batch losses (mrrl, rdl): 0.0093663819, 0.0001409267

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1208
rank avg (pred): 0.462 +- 0.321
mrr vals (pred, true): 0.077, 0.045
batch losses (mrrl, rdl): 0.0071221823, 1.8883e-05

Epoch over!
epoch time: 12.174

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 275
rank avg (pred): 0.320 +- 0.409
mrr vals (pred, true): 0.540, 0.541
batch losses (mrrl, rdl): 1.6401e-05, 0.0017806432

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1102
rank avg (pred): 0.389 +- 0.296
mrr vals (pred, true): 0.090, 0.081
batch losses (mrrl, rdl): 0.0163830705, 3.40492e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 724
rank avg (pred): 0.523 +- 0.337
mrr vals (pred, true): 0.036, 0.052
batch losses (mrrl, rdl): 0.0018289746, 5.39208e-05

Epoch over!
epoch time: 12.326

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 551
rank avg (pred): 0.344 +- 0.330
mrr vals (pred, true): 0.126, 0.105
batch losses (mrrl, rdl): 0.0045066378, 9.7685e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 569
rank avg (pred): 0.487 +- 0.332
mrr vals (pred, true): 0.056, 0.040
batch losses (mrrl, rdl): 0.0003696216, 2.28671e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 584
rank avg (pred): 0.500 +- 0.346
mrr vals (pred, true): 0.055, 0.044
batch losses (mrrl, rdl): 0.0002500174, 3.79011e-05

Epoch over!
epoch time: 12.276

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 969
rank avg (pred): 0.456 +- 0.354
mrr vals (pred, true): 0.039, 0.045
batch losses (mrrl, rdl): 0.0011111298, 5.17982e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1193
rank avg (pred): 0.447 +- 0.327
mrr vals (pred, true): 0.061, 0.053
batch losses (mrrl, rdl): 0.0012637641, 1.76951e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 961
rank avg (pred): 0.447 +- 0.331
mrr vals (pred, true): 0.040, 0.044
batch losses (mrrl, rdl): 0.0009469871, 3.43084e-05

Epoch over!
epoch time: 12.331

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 317
rank avg (pred): 0.101 +- 0.138
mrr vals (pred, true): 0.566, 0.549
batch losses (mrrl, rdl): 0.0028573906, 0.0001164588

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 600
rank avg (pred): 0.497 +- 0.333
mrr vals (pred, true): 0.047, 0.043
batch losses (mrrl, rdl): 9.65104e-05, 2.7367e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 985
rank avg (pred): 0.153 +- 0.196
mrr vals (pred, true): 0.546, 0.537
batch losses (mrrl, rdl): 0.000872699, 0.0003700494

Epoch over!
epoch time: 12.135

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 229
rank avg (pred): 0.518 +- 0.362
mrr vals (pred, true): 0.055, 0.041
batch losses (mrrl, rdl): 0.0003008308, 0.0001017779

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 433
rank avg (pred): 0.460 +- 0.345
mrr vals (pred, true): 0.072, 0.050
batch losses (mrrl, rdl): 0.0046779085, 3.28002e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 589
rank avg (pred): 0.475 +- 0.337
mrr vals (pred, true): 0.055, 0.043
batch losses (mrrl, rdl): 0.0002481455, 4.37895e-05

Epoch over!
epoch time: 12.181

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 139
rank avg (pred): 0.480 +- 0.341
mrr vals (pred, true): 0.067, 0.141
batch losses (mrrl, rdl): 0.0553868413, 0.0009954146

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 244
rank avg (pred): 0.121 +- 0.140
mrr vals (pred, true): 0.545, 0.553
batch losses (mrrl, rdl): 0.0005733692, 0.000214336

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 911
rank avg (pred): 0.441 +- 0.282
mrr vals (pred, true): 0.041, 0.090
batch losses (mrrl, rdl): 0.0007442193, 5.43917e-05

Epoch over!
epoch time: 12.369

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 488
rank avg (pred): 0.316 +- 0.356
mrr vals (pred, true): 0.204, 0.241
batch losses (mrrl, rdl): 0.0136516429, 0.0002863951

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 55
rank avg (pred): 0.132 +- 0.163
mrr vals (pred, true): 0.535, 0.517
batch losses (mrrl, rdl): 0.0031486028, 0.000214562

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 549
rank avg (pred): 0.455 +- 0.359
mrr vals (pred, true): 0.067, 0.047
batch losses (mrrl, rdl): 0.0027827478, 4.44437e-05

Epoch over!
epoch time: 12.31

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.398 +- 0.308
mrr vals (pred, true): 0.054, 0.021

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    0 	     0 	 0.05331 	 0.01514 	 m..s
    1 	     1 	 0.05367 	 0.01609 	 m..s
    4 	     2 	 0.05390 	 0.01614 	 m..s
    2 	     3 	 0.05371 	 0.02085 	 m..s
   21 	     4 	 0.06800 	 0.03392 	 m..s
   20 	     5 	 0.06787 	 0.03491 	 m..s
   13 	     6 	 0.06231 	 0.03679 	 ~...
   27 	     7 	 0.07012 	 0.03739 	 m..s
    5 	     8 	 0.05497 	 0.03758 	 ~...
   11 	     9 	 0.06150 	 0.04099 	 ~...
   46 	    10 	 0.09286 	 0.04135 	 m..s
   17 	    11 	 0.06516 	 0.04155 	 ~...
   48 	    12 	 0.09387 	 0.04159 	 m..s
   19 	    13 	 0.06748 	 0.04203 	 ~...
   62 	    14 	 0.09818 	 0.04294 	 m..s
   39 	    15 	 0.09167 	 0.04299 	 m..s
   74 	    16 	 0.10784 	 0.04308 	 m..s
   50 	    17 	 0.09418 	 0.04365 	 m..s
   57 	    18 	 0.09684 	 0.04377 	 m..s
   18 	    19 	 0.06743 	 0.04440 	 ~...
    6 	    20 	 0.05573 	 0.04465 	 ~...
   16 	    21 	 0.06512 	 0.04470 	 ~...
   12 	    22 	 0.06153 	 0.04475 	 ~...
   53 	    23 	 0.09565 	 0.04480 	 m..s
   23 	    24 	 0.06895 	 0.04534 	 ~...
   52 	    25 	 0.09480 	 0.04540 	 m..s
   54 	    26 	 0.09581 	 0.04549 	 m..s
   49 	    27 	 0.09402 	 0.04582 	 m..s
   70 	    28 	 0.10454 	 0.04610 	 m..s
   25 	    29 	 0.07002 	 0.04613 	 ~...
   31 	    30 	 0.07261 	 0.04628 	 ~...
    7 	    31 	 0.05658 	 0.04648 	 ~...
   82 	    32 	 0.14167 	 0.04683 	 m..s
   56 	    33 	 0.09649 	 0.04758 	 m..s
   35 	    34 	 0.08909 	 0.04759 	 m..s
   81 	    35 	 0.14109 	 0.04771 	 m..s
   26 	    36 	 0.07007 	 0.04804 	 ~...
   77 	    37 	 0.11173 	 0.04836 	 m..s
   30 	    38 	 0.07205 	 0.04881 	 ~...
   28 	    39 	 0.07041 	 0.04949 	 ~...
   67 	    40 	 0.10201 	 0.04972 	 m..s
   33 	    41 	 0.08864 	 0.04990 	 m..s
   15 	    42 	 0.06458 	 0.05000 	 ~...
   64 	    43 	 0.09942 	 0.05004 	 m..s
   29 	    44 	 0.07085 	 0.05005 	 ~...
    3 	    45 	 0.05371 	 0.05073 	 ~...
   73 	    46 	 0.10741 	 0.05147 	 m..s
   22 	    47 	 0.06826 	 0.05218 	 ~...
   55 	    48 	 0.09583 	 0.05246 	 m..s
   71 	    49 	 0.10455 	 0.05284 	 m..s
   43 	    50 	 0.09252 	 0.05365 	 m..s
   60 	    51 	 0.09797 	 0.05456 	 m..s
   14 	    52 	 0.06407 	 0.05484 	 ~...
   65 	    53 	 0.10075 	 0.05570 	 m..s
   37 	    54 	 0.09146 	 0.05593 	 m..s
   24 	    55 	 0.06966 	 0.05645 	 ~...
    9 	    56 	 0.05893 	 0.05668 	 ~...
    8 	    57 	 0.05869 	 0.05681 	 ~...
   10 	    58 	 0.05978 	 0.05719 	 ~...
   32 	    59 	 0.08248 	 0.06022 	 ~...
   42 	    60 	 0.09223 	 0.07261 	 ~...
   75 	    61 	 0.10798 	 0.07881 	 ~...
   44 	    62 	 0.09254 	 0.08324 	 ~...
   36 	    63 	 0.09053 	 0.08345 	 ~...
   47 	    64 	 0.09346 	 0.08578 	 ~...
   34 	    65 	 0.08907 	 0.09027 	 ~...
   78 	    66 	 0.11491 	 0.09087 	 ~...
   41 	    67 	 0.09197 	 0.09091 	 ~...
   66 	    68 	 0.10088 	 0.09253 	 ~...
   63 	    69 	 0.09840 	 0.10042 	 ~...
   69 	    70 	 0.10348 	 0.10155 	 ~...
   79 	    71 	 0.11543 	 0.10262 	 ~...
   61 	    72 	 0.09799 	 0.10286 	 ~...
   58 	    73 	 0.09707 	 0.10391 	 ~...
   51 	    74 	 0.09478 	 0.10401 	 ~...
   45 	    75 	 0.09257 	 0.10756 	 ~...
   59 	    76 	 0.09772 	 0.11164 	 ~...
   76 	    77 	 0.10989 	 0.12212 	 ~...
   38 	    78 	 0.09151 	 0.13188 	 m..s
   40 	    79 	 0.09191 	 0.13195 	 m..s
   72 	    80 	 0.10644 	 0.13903 	 m..s
   80 	    81 	 0.12798 	 0.14609 	 ~...
   88 	    82 	 0.33870 	 0.16603 	 MISS
   85 	    83 	 0.19466 	 0.16639 	 ~...
   68 	    84 	 0.10317 	 0.17769 	 m..s
   83 	    85 	 0.18857 	 0.18507 	 ~...
   84 	    86 	 0.19344 	 0.19106 	 ~...
   86 	    87 	 0.21085 	 0.21489 	 ~...
   87 	    88 	 0.26138 	 0.24174 	 ~...
   89 	    89 	 0.49315 	 0.37516 	 MISS
   94 	    90 	 0.53353 	 0.51017 	 ~...
   98 	    91 	 0.53623 	 0.51128 	 ~...
   91 	    92 	 0.50491 	 0.51250 	 ~...
  118 	    93 	 0.54871 	 0.51440 	 m..s
   93 	    94 	 0.50616 	 0.52023 	 ~...
   90 	    95 	 0.50428 	 0.52229 	 ~...
   95 	    96 	 0.53449 	 0.52810 	 ~...
   97 	    97 	 0.53591 	 0.52858 	 ~...
  103 	    98 	 0.53962 	 0.53079 	 ~...
   99 	    99 	 0.53764 	 0.53184 	 ~...
   96 	   100 	 0.53574 	 0.53224 	 ~...
   92 	   101 	 0.50582 	 0.53246 	 ~...
  100 	   102 	 0.53818 	 0.53387 	 ~...
  109 	   103 	 0.54483 	 0.53587 	 ~...
  107 	   104 	 0.54475 	 0.53957 	 ~...
  108 	   105 	 0.54480 	 0.53971 	 ~...
  116 	   106 	 0.54724 	 0.54053 	 ~...
  104 	   107 	 0.53972 	 0.54506 	 ~...
  112 	   108 	 0.54535 	 0.54652 	 ~...
  105 	   109 	 0.54190 	 0.54664 	 ~...
  106 	   110 	 0.54382 	 0.54784 	 ~...
  114 	   111 	 0.54559 	 0.54813 	 ~...
  110 	   112 	 0.54506 	 0.54857 	 ~...
  101 	   113 	 0.53823 	 0.54875 	 ~...
  120 	   114 	 0.55048 	 0.55097 	 ~...
  115 	   115 	 0.54606 	 0.55166 	 ~...
  102 	   116 	 0.53913 	 0.55203 	 ~...
  113 	   117 	 0.54546 	 0.55251 	 ~...
  117 	   118 	 0.54740 	 0.55447 	 ~...
  111 	   119 	 0.54517 	 0.55497 	 ~...
  119 	   120 	 0.54960 	 0.56362 	 ~...
==========================================
r_mrr = 0.9893919229507446
r2_mrr = 0.9691874980926514
spearmanr_mrr@5 = 0.8736060857772827
spearmanr_mrr@10 = 0.9110597372055054
spearmanr_mrr@50 = 0.9920463562011719
spearmanr_mrr@100 = 0.9952881932258606
spearmanr_mrr@All = 0.9957605004310608
==========================================
test time: 0.399
Done Testing dataset UMLS
total time taken: 189.0769522190094
training time taken: 183.2943766117096
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9894)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9692)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8736)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9111)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9920)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9953)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9958)}}, 'test_loss': {'DistMult': {'UMLS': 1.6621250627940753}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num neighbnours', 'o num neighbnours'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s min freq rel_o min freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 1044488866231566
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1045, 996, 752, 477, 1061, 127, 145, 324, 868, 1004, 980, 282, 737, 1054, 670, 596, 1204, 555, 1034, 807, 566, 1105, 161, 574, 901, 428, 172, 8, 887, 260, 736, 358, 496, 768, 421, 657, 1002, 668, 157, 1026, 683, 564, 252, 875, 588, 397, 415, 95, 21, 794, 1152, 375, 822, 296, 18, 32, 389, 528, 4, 1166, 1062, 678, 224, 616, 539, 398, 171, 193, 823, 101, 480, 602, 898, 207, 427, 1114, 653, 303, 766, 1146, 1209, 640, 939, 56, 895, 677, 94, 99, 442, 1214, 1, 867, 618, 824, 1174, 356, 937, 318, 917, 9, 724, 628, 655, 1092, 904, 634, 612, 785, 133, 658, 239, 1135, 847, 940, 832, 580, 543, 636, 842, 723, 601]
valid_ids (0): []
train_ids (1094): [347, 363, 294, 877, 33, 355, 1097, 827, 242, 423, 1190, 290, 212, 110, 385, 926, 744, 31, 135, 142, 43, 1048, 283, 26, 447, 335, 492, 75, 179, 124, 441, 302, 816, 746, 1158, 188, 341, 617, 1159, 745, 406, 367, 988, 629, 1212, 525, 701, 764, 35, 295, 1084, 976, 568, 696, 1155, 586, 931, 989, 160, 413, 440, 198, 219, 1180, 865, 792, 644, 565, 394, 638, 719, 675, 818, 153, 422, 776, 372, 1094, 647, 469, 512, 377, 154, 278, 29, 460, 246, 905, 569, 292, 424, 689, 864, 797, 250, 273, 203, 1178, 165, 945, 47, 221, 495, 1169, 206, 825, 1070, 790, 16, 1038, 1142, 84, 177, 620, 15, 238, 520, 393, 609, 591, 134, 891, 614, 882, 479, 92, 382, 1203, 481, 370, 1027, 1138, 1102, 626, 829, 914, 30, 1015, 1132, 264, 1069, 557, 80, 1201, 848, 1085, 409, 130, 400, 274, 879, 83, 763, 269, 184, 881, 187, 468, 606, 354, 772, 1023, 12, 942, 115, 3, 656, 838, 1012, 299, 116, 201, 1091, 787, 774, 1161, 987, 364, 28, 232, 540, 930, 906, 654, 690, 458, 464, 851, 146, 831, 1068, 953, 225, 777, 288, 353, 78, 652, 418, 811, 575, 208, 907, 50, 77, 1148, 245, 861, 622, 54, 156, 51, 25, 671, 333, 1103, 513, 563, 809, 1000, 1183, 1195, 204, 1107, 36, 584, 1167, 799, 974, 401, 682, 371, 781, 497, 680, 106, 476, 1003, 1189, 840, 552, 708, 39, 532, 1181, 137, 1087, 465, 44, 1207, 470, 681, 843, 272, 1039, 268, 973, 812, 326, 784, 503, 1050, 1080, 852, 969, 359, 1041, 151, 241, 317, 615, 711, 649, 494, 585, 804, 265, 873, 510, 722, 605, 700, 6, 404, 1144, 1006, 795, 839, 337, 491, 801, 1131, 1020, 573, 925, 14, 780, 1013, 841, 1210, 360, 1205, 600, 408, 506, 863, 651, 966, 1150, 1188, 769, 985, 498, 410, 381, 1018, 211, 734, 862, 633, 1024, 941, 709, 79, 163, 920, 718, 185, 61, 886, 261, 1196, 248, 523, 1047, 558, 10, 1119, 1074, 548, 453, 791, 511, 924, 170, 1170, 577, 1078, 820, 998, 439, 277, 527, 489, 22, 287, 1162, 1049, 1184, 280, 817, 448, 553, 169, 88, 1096, 338, 733, 1056, 771, 305, 1077, 1194, 844, 182, 915, 125, 1008, 281, 541, 738, 392, 673, 872, 289, 55, 155, 762, 68, 627, 461, 478, 661, 484, 900, 902, 805, 1187, 229, 107, 666, 1206, 630, 183, 175, 200, 674, 226, 316, 1057, 405, 1083, 1089, 70, 1106, 1186, 603, 703, 1125, 1136, 632, 102, 452, 454, 304, 432, 459, 425, 921, 343, 908, 991, 625, 725, 685, 810, 395, 71, 1176, 971, 223, 387, 166, 149, 132, 645, 164, 910, 430, 128, 978, 414, 173, 1192, 1033, 800, 378, 159, 451, 582, 1028, 730, 209, 63, 195, 1075, 443, 1177, 147, 830, 803, 234, 814, 118, 947, 715, 748, 1011, 150, 1073, 399, 899, 779, 849, 581, 19, 275, 467, 213, 714, 913, 312, 589, 379, 554, 485, 117, 1182, 687, 1055, 426, 1072, 802, 643, 202, 300, 707, 216, 950, 948, 648, 97, 136, 667, 366, 598, 191, 883, 89, 753, 619, 858, 694, 73, 328, 49, 1154, 435, 693, 1172, 501, 309, 1160, 48, 108, 631, 610, 728, 1139, 58, 487, 60, 253, 1213, 1115, 438, 1140, 949, 1052, 521, 716, 1137, 897, 1151, 433, 702, 332, 981, 144, 650, 7, 933, 1199, 514, 505, 1021, 977, 1036, 217, 0, 1198, 1208, 967, 912, 621, 306, 123, 104, 1014, 559, 999, 1164, 1168, 502, 594, 982, 607, 1126, 662, 551, 320, 826, 599, 853, 37, 416, 313, 257, 168, 798, 271, 876, 919, 932, 972, 293, 383, 1043, 1143, 240, 178, 420, 1095, 517, 53, 534, 1082, 1079, 436, 1202, 1127, 1040, 330, 1029, 608, 138, 340, 105, 986, 1122, 466, 391, 1112, 970, 993, 922, 373, 754, 199, 263, 390, 402, 819, 344, 374, 361, 41, 778, 595, 1076, 870, 297, 1156, 994, 957, 279, 325, 562, 833, 960, 255, 936, 793, 522, 815, 903, 1053, 749, 888, 775, 943, 231, 916, 500, 1117, 1134, 958, 93, 944, 96, 315, 531, 515, 1120, 592, 992, 65, 834, 646, 866, 46, 604, 751, 59, 66, 1153, 2, 42, 230, 82, 270, 90, 721, 894, 664, 856, 859, 549, 729, 334, 176, 896, 91, 788, 742, 783, 327, 1065, 663, 561, 686, 13, 869, 141, 597, 975, 342, 1108, 113, 542, 1086, 533, 109, 419, 688, 57, 961, 11, 1060, 1130, 412, 1171, 720, 1116, 345, 483, 194, 556, 1200, 472, 590, 880, 307, 417, 997, 923, 1110, 139, 1059, 119, 362, 570, 482, 946, 86, 535, 301, 488, 965, 298, 938, 699, 210, 1175, 786, 45, 1191, 267, 773, 676, 52, 1064, 34, 835, 504, 72, 756, 310, 1007, 254, 747, 291, 111, 1104, 544, 1147, 437, 695, 1017, 5, 205, 403, 1037, 350, 81, 131, 152, 1001, 143, 368, 782, 929, 739, 457, 836, 259, 1128, 486, 1051, 761, 235, 1044, 576, 311, 1113, 1099, 321, 509, 243, 962, 1179, 471, 20, 1118, 572, 963, 64, 190, 351, 74, 526, 329, 1046, 679, 38, 1123, 704, 984, 705, 1025, 1185, 613, 623, 755, 684, 323, 434, 806, 1032, 537, 918, 956, 357, 909, 507, 1173, 1081, 174, 1009, 692, 979, 186, 384, 571, 431, 770, 928, 284, 1163, 697, 256, 1031, 854, 456, 710, 624, 251, 1035, 1121, 247, 98, 473, 122, 346, 1005, 837, 499, 121, 611, 796, 855, 935, 158, 757, 740, 1124, 1109, 369, 536, 884, 927, 964, 735, 579, 322, 518, 743, 713, 197, 845, 545, 24, 336, 103, 237, 1090, 878, 717, 114, 23, 990, 821, 1098, 893, 760, 758, 712, 167, 560, 181, 641, 1157, 860, 892, 983, 808, 429, 1016, 706, 1030, 493, 550, 215, 529, 1100, 1042, 727, 214, 228, 446, 635, 463, 857, 349, 189, 750, 593, 959, 233, 954, 1145, 1022, 1071, 17, 1129, 126, 192, 995, 871, 1101, 911, 27, 885, 444, 474, 388, 1093, 352, 726, 266, 731, 87, 530, 951, 741, 286, 1193, 218, 450, 1063, 516, 850, 789, 220, 376, 129, 660, 462, 319, 828, 546, 583, 698, 547, 538, 180, 767, 1067, 40, 952, 1088, 85, 227, 258, 249, 339, 196, 759, 455, 691, 813, 519, 162, 1058, 244, 732, 968, 148, 1133, 407, 411, 934, 380, 765, 672, 639, 314, 490, 348, 236, 120, 112, 140, 1149, 396, 1019, 331, 308, 874, 222, 1211, 642, 846, 100, 445, 1111, 955, 67, 69, 524, 76, 669, 665, 889, 365, 1197, 578, 637, 1141, 890, 62, 386, 1165, 587, 449, 475, 262, 1066, 285, 276, 1010, 508, 567, 659]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1104067080804520
the save name prefix for this run is:  chkpt-ID_1104067080804520_tag_Ablation-job-blacklist-s min freq rel_o min freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 443
rank avg (pred): 0.631 +- 0.004
mrr vals (pred, true): 0.012, 0.045
batch losses (mrrl, rdl): 0.0, 0.000828375

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 689
rank avg (pred): 0.429 +- 0.252
mrr vals (pred, true): 0.120, 0.044
batch losses (mrrl, rdl): 0.0, 3.9535e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 370
rank avg (pred): 0.324 +- 0.239
mrr vals (pred, true): 0.211, 0.117
batch losses (mrrl, rdl): 0.0, 9.21925e-05

Epoch over!
epoch time: 12.212

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 108
rank avg (pred): 0.359 +- 0.257
mrr vals (pred, true): 0.192, 0.132
batch losses (mrrl, rdl): 0.0, 0.0002171412

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 504
rank avg (pred): 0.333 +- 0.259
mrr vals (pred, true): 0.222, 0.202
batch losses (mrrl, rdl): 0.0, 0.0003381335

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 763
rank avg (pred): 0.429 +- 0.298
mrr vals (pred, true): 0.183, 0.034
batch losses (mrrl, rdl): 0.0, 0.0001106424

Epoch over!
epoch time: 12.003

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 199
rank avg (pred): 0.357 +- 0.259
mrr vals (pred, true): 0.185, 0.052
batch losses (mrrl, rdl): 0.0, 6.05486e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 873
rank avg (pred): 0.443 +- 0.277
mrr vals (pred, true): 0.121, 0.048
batch losses (mrrl, rdl): 0.0, 2.46624e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 294
rank avg (pred): 0.044 +- 0.063
mrr vals (pred, true): 0.421, 0.540
batch losses (mrrl, rdl): 0.0, 3.268e-07

Epoch over!
epoch time: 12.057

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1181
rank avg (pred): 0.454 +- 0.278
mrr vals (pred, true): 0.122, 0.043
batch losses (mrrl, rdl): 0.0, 4.3941e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 327
rank avg (pred): 0.382 +- 0.271
mrr vals (pred, true): 0.155, 0.123
batch losses (mrrl, rdl): 0.0, 0.0002314463

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 967
rank avg (pred): 0.535 +- 0.273
mrr vals (pred, true): 0.090, 0.047
batch losses (mrrl, rdl): 0.0, 0.0002042152

Epoch over!
epoch time: 11.939

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1058
rank avg (pred): 0.020 +- 0.042
mrr vals (pred, true): 0.543, 0.549
batch losses (mrrl, rdl): 0.0, 4.5368e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 108
rank avg (pred): 0.379 +- 0.268
mrr vals (pred, true): 0.178, 0.132
batch losses (mrrl, rdl): 0.0, 0.0002947179

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 374
rank avg (pred): 0.353 +- 0.275
mrr vals (pred, true): 0.232, 0.108
batch losses (mrrl, rdl): 0.0, 6.42801e-05

Epoch over!
epoch time: 12.049

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 120
rank avg (pred): 0.335 +- 0.267
mrr vals (pred, true): 0.236, 0.100
batch losses (mrrl, rdl): 0.1845478415, 6.74302e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 339
rank avg (pred): 0.271 +- 0.146
mrr vals (pred, true): 0.114, 0.131
batch losses (mrrl, rdl): 0.002895514, 3.51757e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 380
rank avg (pred): 0.358 +- 0.185
mrr vals (pred, true): 0.090, 0.082
batch losses (mrrl, rdl): 0.0159451589, 2.86371e-05

Epoch over!
epoch time: 12.232

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 789
rank avg (pred): 0.421 +- 0.165
mrr vals (pred, true): 0.049, 0.044
batch losses (mrrl, rdl): 1.13255e-05, 3.25918e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 256
rank avg (pred): 0.007 +- 0.004
mrr vals (pred, true): 0.591, 0.569
batch losses (mrrl, rdl): 0.0048578568, 1.41965e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 208
rank avg (pred): 0.379 +- 0.187
mrr vals (pred, true): 0.073, 0.042
batch losses (mrrl, rdl): 0.0052543199, 7.58948e-05

Epoch over!
epoch time: 12.193

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 429
rank avg (pred): 0.339 +- 0.185
mrr vals (pred, true): 0.081, 0.058
batch losses (mrrl, rdl): 0.0099153342, 8.83351e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 514
rank avg (pred): 0.245 +- 0.137
mrr vals (pred, true): 0.096, 0.072
batch losses (mrrl, rdl): 0.0208460502, 0.0005036396

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1044
rank avg (pred): 0.353 +- 0.191
mrr vals (pred, true): 0.073, 0.040
batch losses (mrrl, rdl): 0.0051243505, 0.0001365128

Epoch over!
epoch time: 12.177

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 19
rank avg (pred): 0.008 +- 0.005
mrr vals (pred, true): 0.573, 0.549
batch losses (mrrl, rdl): 0.005533359, 1.91096e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 510
rank avg (pred): 0.224 +- 0.129
mrr vals (pred, true): 0.102, 0.169
batch losses (mrrl, rdl): 0.0452790596, 8.8459e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 220
rank avg (pred): 0.361 +- 0.186
mrr vals (pred, true): 0.069, 0.043
batch losses (mrrl, rdl): 0.0036841433, 0.0001302642

Epoch over!
epoch time: 12.153

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 130
rank avg (pred): 0.353 +- 0.185
mrr vals (pred, true): 0.068, 0.107
batch losses (mrrl, rdl): 0.0150789022, 0.0001191693

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1124
rank avg (pred): 0.387 +- 0.252
mrr vals (pred, true): 0.111, 0.042
batch losses (mrrl, rdl): 0.0373722389, 2.62064e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 466
rank avg (pred): 0.342 +- 0.192
mrr vals (pred, true): 0.077, 0.054
batch losses (mrrl, rdl): 0.0073071653, 0.0001247741

Epoch over!
epoch time: 12.157

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 262
rank avg (pred): 0.009 +- 0.006
mrr vals (pred, true): 0.555, 0.563
batch losses (mrrl, rdl): 0.0007711671, 1.13215e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1127
rank avg (pred): 0.391 +- 0.244
mrr vals (pred, true): 0.079, 0.050
batch losses (mrrl, rdl): 0.0081339292, 2.24794e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 899
rank avg (pred): 0.384 +- 0.176
mrr vals (pred, true): 0.055, 0.038
batch losses (mrrl, rdl): 0.0002551111, 9.16141e-05

Epoch over!
epoch time: 12.223

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 611
rank avg (pred): 0.410 +- 0.131
mrr vals (pred, true): 0.041, 0.040
batch losses (mrrl, rdl): 0.0008154277, 0.0001791462

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 872
rank avg (pred): 0.416 +- 0.138
mrr vals (pred, true): 0.043, 0.046
batch losses (mrrl, rdl): 0.0004740491, 6.11782e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 48
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.532, 0.509
batch losses (mrrl, rdl): 0.0054423483, 2.89135e-05

Epoch over!
epoch time: 12.269

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 770
rank avg (pred): 0.408 +- 0.168
mrr vals (pred, true): 0.047, 0.033
batch losses (mrrl, rdl): 7.2812e-05, 0.0002205456

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1097
rank avg (pred): 0.388 +- 0.236
mrr vals (pred, true): 0.076, 0.101
batch losses (mrrl, rdl): 0.0065597589, 0.0002079325

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 659
rank avg (pred): 0.401 +- 0.203
mrr vals (pred, true): 0.056, 0.053
batch losses (mrrl, rdl): 0.0004075356, 1.54669e-05

Epoch over!
epoch time: 12.276

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1182
rank avg (pred): 0.387 +- 0.138
mrr vals (pred, true): 0.044, 0.040
batch losses (mrrl, rdl): 0.0003129442, 0.0001825027

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 820
rank avg (pred): 0.013 +- 0.009
mrr vals (pred, true): 0.487, 0.518
batch losses (mrrl, rdl): 0.0095109148, 1.47967e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 667
rank avg (pred): 0.408 +- 0.111
mrr vals (pred, true): 0.039, 0.045
batch losses (mrrl, rdl): 0.0011607485, 0.0001045977

Epoch over!
epoch time: 12.199

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1050
rank avg (pred): 0.398 +- 0.238
mrr vals (pred, true): 0.071, 0.042
batch losses (mrrl, rdl): 0.004433834, 4.26737e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 672
rank avg (pred): 0.419 +- 0.097
mrr vals (pred, true): 0.036, 0.058
batch losses (mrrl, rdl): 0.001847268, 8.16765e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 426
rank avg (pred): 0.364 +- 0.148
mrr vals (pred, true): 0.052, 0.050
batch losses (mrrl, rdl): 3.10077e-05, 0.0002314198

Epoch over!
epoch time: 12.262

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.388 +- 0.262
mrr vals (pred, true): 0.103, 0.049

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.03436 	 0.01573 	 ~...
    0 	     1 	 0.03434 	 0.01767 	 ~...
   94 	     2 	 0.16958 	 0.01876 	 MISS
   93 	     3 	 0.16827 	 0.01876 	 MISS
   29 	     4 	 0.05116 	 0.01876 	 m..s
    2 	     5 	 0.03473 	 0.01975 	 ~...
   50 	     6 	 0.07785 	 0.02280 	 m..s
   39 	     7 	 0.06312 	 0.03204 	 m..s
   31 	     8 	 0.05389 	 0.03634 	 ~...
   32 	     9 	 0.05701 	 0.03793 	 ~...
   44 	    10 	 0.06825 	 0.03825 	 ~...
   13 	    11 	 0.04144 	 0.03905 	 ~...
   25 	    12 	 0.04605 	 0.04074 	 ~...
   55 	    13 	 0.08392 	 0.04080 	 m..s
   16 	    14 	 0.04195 	 0.04097 	 ~...
   10 	    15 	 0.04117 	 0.04101 	 ~...
   37 	    16 	 0.05936 	 0.04203 	 ~...
    9 	    17 	 0.03961 	 0.04279 	 ~...
   40 	    18 	 0.06370 	 0.04302 	 ~...
   67 	    19 	 0.09676 	 0.04316 	 m..s
   19 	    20 	 0.04323 	 0.04318 	 ~...
   38 	    21 	 0.06241 	 0.04357 	 ~...
    4 	    22 	 0.03650 	 0.04361 	 ~...
   18 	    23 	 0.04248 	 0.04394 	 ~...
   36 	    24 	 0.05894 	 0.04440 	 ~...
   34 	    25 	 0.05751 	 0.04441 	 ~...
   56 	    26 	 0.08508 	 0.04461 	 m..s
   17 	    27 	 0.04242 	 0.04465 	 ~...
   12 	    28 	 0.04121 	 0.04475 	 ~...
   14 	    29 	 0.04147 	 0.04479 	 ~...
   60 	    30 	 0.08682 	 0.04483 	 m..s
    5 	    31 	 0.03662 	 0.04483 	 ~...
   49 	    32 	 0.07757 	 0.04505 	 m..s
   82 	    33 	 0.10661 	 0.04520 	 m..s
   47 	    34 	 0.07290 	 0.04581 	 ~...
    8 	    35 	 0.03957 	 0.04583 	 ~...
   33 	    36 	 0.05737 	 0.04590 	 ~...
   73 	    37 	 0.10012 	 0.04610 	 m..s
   58 	    38 	 0.08612 	 0.04613 	 m..s
   23 	    39 	 0.04468 	 0.04615 	 ~...
   26 	    40 	 0.04669 	 0.04653 	 ~...
   15 	    41 	 0.04181 	 0.04707 	 ~...
   27 	    42 	 0.04812 	 0.04721 	 ~...
   22 	    43 	 0.04460 	 0.04750 	 ~...
    3 	    44 	 0.03648 	 0.04787 	 ~...
   53 	    45 	 0.08320 	 0.04794 	 m..s
    6 	    46 	 0.03839 	 0.04856 	 ~...
   77 	    47 	 0.10304 	 0.04860 	 m..s
   76 	    48 	 0.10156 	 0.04870 	 m..s
   20 	    49 	 0.04380 	 0.04922 	 ~...
   66 	    50 	 0.09622 	 0.04962 	 m..s
   48 	    51 	 0.07438 	 0.04994 	 ~...
   21 	    52 	 0.04412 	 0.05000 	 ~...
   83 	    53 	 0.10674 	 0.05026 	 m..s
   45 	    54 	 0.06861 	 0.05033 	 ~...
   24 	    55 	 0.04597 	 0.05040 	 ~...
   61 	    56 	 0.08722 	 0.05082 	 m..s
   42 	    57 	 0.06660 	 0.05095 	 ~...
   41 	    58 	 0.06509 	 0.05143 	 ~...
   11 	    59 	 0.04119 	 0.05175 	 ~...
    7 	    60 	 0.03942 	 0.05207 	 ~...
   85 	    61 	 0.10935 	 0.05234 	 m..s
   78 	    62 	 0.10417 	 0.05240 	 m..s
   72 	    63 	 0.10011 	 0.05305 	 m..s
   52 	    64 	 0.08141 	 0.05340 	 ~...
   30 	    65 	 0.05366 	 0.05446 	 ~...
   54 	    66 	 0.08384 	 0.05540 	 ~...
   51 	    67 	 0.08053 	 0.05570 	 ~...
   35 	    68 	 0.05834 	 0.05734 	 ~...
   28 	    69 	 0.04812 	 0.06807 	 ~...
   46 	    70 	 0.06990 	 0.07725 	 ~...
   87 	    71 	 0.11067 	 0.07881 	 m..s
   59 	    72 	 0.08671 	 0.08475 	 ~...
   63 	    73 	 0.08971 	 0.09187 	 ~...
   81 	    74 	 0.10586 	 0.09788 	 ~...
   89 	    75 	 0.13736 	 0.09994 	 m..s
   90 	    76 	 0.13873 	 0.10029 	 m..s
   88 	    77 	 0.11506 	 0.10126 	 ~...
   80 	    78 	 0.10505 	 0.10155 	 ~...
   64 	    79 	 0.09105 	 0.10238 	 ~...
   68 	    80 	 0.09830 	 0.10249 	 ~...
   75 	    81 	 0.10080 	 0.10630 	 ~...
   84 	    82 	 0.10888 	 0.10756 	 ~...
   70 	    83 	 0.09926 	 0.10907 	 ~...
   79 	    84 	 0.10438 	 0.11140 	 ~...
   74 	    85 	 0.10058 	 0.12203 	 ~...
   57 	    86 	 0.08577 	 0.12428 	 m..s
   62 	    87 	 0.08848 	 0.12523 	 m..s
   69 	    88 	 0.09859 	 0.12660 	 ~...
   43 	    89 	 0.06702 	 0.12951 	 m..s
   91 	    90 	 0.14199 	 0.13495 	 ~...
   86 	    91 	 0.11031 	 0.14118 	 m..s
   65 	    92 	 0.09224 	 0.14238 	 m..s
   92 	    93 	 0.14874 	 0.14609 	 ~...
   71 	    94 	 0.10010 	 0.17231 	 m..s
   95 	    95 	 0.23662 	 0.19363 	 m..s
   96 	    96 	 0.28047 	 0.25029 	 m..s
  100 	    97 	 0.51796 	 0.40925 	 MISS
   99 	    98 	 0.51516 	 0.42619 	 m..s
   97 	    99 	 0.50339 	 0.42624 	 m..s
  101 	   100 	 0.51936 	 0.45875 	 m..s
   98 	   101 	 0.51377 	 0.46155 	 m..s
  102 	   102 	 0.53356 	 0.52273 	 ~...
  103 	   103 	 0.53725 	 0.52288 	 ~...
  104 	   104 	 0.53964 	 0.53399 	 ~...
  114 	   105 	 0.58046 	 0.53566 	 m..s
  118 	   106 	 0.59865 	 0.53587 	 m..s
  119 	   107 	 0.60078 	 0.53662 	 m..s
  115 	   108 	 0.58412 	 0.53719 	 m..s
  116 	   109 	 0.58581 	 0.53995 	 m..s
  105 	   110 	 0.54960 	 0.54081 	 ~...
  113 	   111 	 0.57907 	 0.54412 	 m..s
  106 	   112 	 0.55160 	 0.54536 	 ~...
  107 	   113 	 0.55398 	 0.54894 	 ~...
  109 	   114 	 0.55959 	 0.54921 	 ~...
  120 	   115 	 0.61131 	 0.54935 	 m..s
  117 	   116 	 0.58647 	 0.55166 	 m..s
  112 	   117 	 0.57853 	 0.55644 	 ~...
  111 	   118 	 0.57826 	 0.55731 	 ~...
  108 	   119 	 0.55759 	 0.55948 	 ~...
  110 	   120 	 0.57551 	 0.56198 	 ~...
==========================================
r_mrr = 0.9855442047119141
r2_mrr = 0.9577744603157043
spearmanr_mrr@5 = 0.896860659122467
spearmanr_mrr@10 = 0.9260489344596863
spearmanr_mrr@50 = 0.9955475330352783
spearmanr_mrr@100 = 0.9957605004310608
spearmanr_mrr@All = 0.9959068894386292
==========================================
test time: 0.711
Done Testing dataset UMLS
total time taken: 189.31857204437256
training time taken: 183.17658853530884
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9855)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9578)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8969)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9260)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9955)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9958)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9959)}}, 'test_loss': {'DistMult': {'UMLS': 1.7031177831086097}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s min freq rel', 'o min freq rel'}

========================================================================================
----------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s max freq rel_o max freq rel
----------------------------------------------------------------------------------------
========================================================================================
Running on a grid of size 1
Using random seed: 5515221797571992
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1178, 1195, 820, 194, 683, 655, 831, 348, 356, 508, 1133, 582, 1169, 622, 120, 1101, 896, 1082, 430, 772, 454, 293, 521, 956, 231, 766, 647, 28, 44, 424, 845, 522, 730, 800, 516, 916, 892, 681, 964, 108, 218, 1172, 871, 737, 426, 4, 302, 375, 1043, 250, 135, 621, 403, 668, 109, 489, 84, 178, 885, 1120, 285, 1129, 275, 714, 188, 307, 803, 79, 753, 506, 274, 207, 855, 542, 1033, 336, 951, 485, 1025, 312, 994, 172, 1023, 1180, 43, 477, 280, 71, 331, 480, 571, 286, 321, 527, 840, 707, 317, 507, 875, 821, 182, 1022, 5, 1107, 134, 578, 924, 487, 1051, 532, 911, 860, 967, 1058, 1056, 351, 806, 7, 333, 96, 284]
valid_ids (0): []
train_ids (1094): [1171, 446, 973, 689, 166, 648, 999, 889, 882, 1071, 510, 1158, 1007, 619, 54, 858, 193, 808, 775, 539, 591, 301, 1048, 963, 722, 920, 809, 852, 756, 596, 229, 1065, 258, 85, 1103, 1034, 607, 369, 742, 1000, 904, 138, 149, 1087, 927, 1038, 1057, 401, 651, 568, 798, 835, 501, 1191, 928, 22, 63, 287, 1003, 221, 1140, 853, 1092, 626, 179, 884, 1168, 711, 376, 1132, 303, 136, 822, 765, 863, 550, 678, 814, 311, 437, 304, 575, 1064, 799, 23, 1165, 1040, 502, 1046, 244, 982, 364, 197, 189, 415, 656, 530, 747, 124, 614, 104, 1181, 584, 491, 83, 78, 617, 366, 91, 757, 33, 776, 484, 1096, 763, 606, 988, 206, 434, 248, 278, 208, 387, 212, 338, 987, 1111, 670, 1167, 225, 93, 75, 975, 1066, 211, 719, 349, 443, 129, 1045, 167, 14, 627, 624, 433, 40, 946, 630, 205, 910, 932, 583, 77, 29, 613, 923, 886, 322, 25, 837, 386, 515, 700, 786, 830, 754, 482, 903, 725, 297, 748, 1123, 488, 517, 448, 1104, 620, 17, 92, 930, 483, 833, 352, 906, 439, 611, 1061, 127, 601, 745, 665, 922, 368, 761, 807, 750, 374, 51, 1115, 631, 1077, 520, 984, 940, 815, 240, 98, 458, 277, 131, 457, 848, 378, 1203, 524, 384, 460, 383, 65, 868, 123, 751, 654, 1173, 267, 1069, 818, 636, 535, 47, 423, 1073, 396, 1024, 417, 393, 173, 61, 196, 1075, 995, 657, 644, 1002, 664, 661, 350, 381, 1189, 525, 156, 459, 685, 435, 970, 771, 245, 645, 1012, 849, 890, 1183, 1067, 699, 891, 1068, 1137, 597, 1124, 866, 59, 537, 721, 263, 48, 152, 346, 1141, 812, 492, 153, 341, 314, 228, 603, 1032, 879, 315, 473, 1078, 168, 122, 695, 767, 1105, 888, 313, 1106, 810, 1028, 704, 11, 316, 1197, 816, 279, 395, 1146, 824, 1150, 6, 696, 623, 504, 1006, 309, 1080, 150, 604, 1042, 998, 743, 706, 1153, 1193, 705, 87, 406, 388, 839, 672, 580, 158, 431, 690, 133, 909, 478, 468, 877, 1142, 405, 300, 217, 804, 119, 382, 1109, 1052, 1049, 101, 496, 900, 455, 13, 358, 541, 1015, 659, 559, 58, 677, 861, 81, 702, 394, 829, 200, 662, 99, 214, 371, 1102, 213, 834, 26, 555, 529, 760, 540, 817, 407, 740, 534, 788, 1113, 1093, 2, 334, 996, 1059, 404, 1116, 1044, 1214, 842, 880, 602, 37, 838, 409, 979, 513, 294, 981, 345, 836, 19, 320, 1089, 52, 986, 254, 936, 941, 989, 1019, 634, 715, 653, 872, 219, 512, 997, 347, 789, 180, 183, 117, 640, 794, 397, 1074, 1047, 398, 115, 1206, 377, 1147, 961, 447, 605, 1054, 857, 86, 62, 174, 847, 107, 3, 42, 1005, 974, 232, 795, 697, 471, 633, 587, 268, 944, 1016, 558, 768, 971, 1010, 1182, 41, 676, 226, 980, 292, 171, 116, 778, 1151, 867, 373, 256, 486, 780, 1017, 733, 805, 526, 444, 306, 185, 577, 324, 908, 342, 1186, 10, 1163, 1081, 1205, 1127, 1185, 408, 164, 1070, 841, 339, 203, 576, 402, 1152, 694, 1076, 445, 1085, 56, 901, 440, 1201, 146, 637, 247, 972, 533, 1014, 581, 728, 893, 1091, 1121, 545, 118, 1021, 94, 514, 15, 660, 658, 693, 1190, 450, 9, 939, 145, 242, 1198, 111, 220, 210, 566, 1118, 227, 246, 1041, 421, 686, 625, 692, 1031, 1122, 355, 441, 27, 764, 1013, 243, 1139, 241, 49, 475, 472, 21, 142, 370, 429, 671, 1160, 1149, 716, 959, 1039, 30, 414, 960, 72, 698, 432, 16, 1148, 412, 914, 574, 1098, 418, 106, 67, 774, 912, 53, 8, 565, 599, 155, 462, 1063, 717, 1095, 36, 1145, 723, 466, 1188, 953, 420, 259, 389, 328, 380, 519, 64, 865, 162, 1174, 76, 428, 175, 663, 105, 562, 1020, 873, 144, 523, 792, 554, 870, 992, 463, 112, 103, 801, 948, 1192, 323, 593, 609, 669, 755, 641, 271, 400, 399, 549, 257, 813, 934, 856, 216, 736, 703, 628, 954, 1204, 456, 390, 379, 308, 639, 915, 202, 673, 195, 1060, 12, 360, 283, 586, 365, 585, 361, 925, 147, 758, 55, 262, 413, 80, 24, 467, 1001, 139, 913, 497, 735, 330, 598, 594, 187, 1187, 340, 427, 438, 125, 790, 887, 864, 531, 160, 1035, 362, 326, 260, 712, 600, 335, 1143, 165, 553, 281, 746, 894, 121, 1164, 966, 720, 1210, 359, 773, 159, 500, 926, 186, 230, 222, 547, 643, 588, 1166, 569, 744, 255, 132, 354, 827, 895, 236, 325, 151, 770, 363, 319, 688, 176, 949, 68, 1196, 223, 666, 1170, 419, 32, 169, 235, 933, 968, 201, 291, 1179, 1084, 947, 1126, 1062, 781, 797, 851, 785, 897, 557, 727, 969, 181, 552, 1117, 950, 990, 782, 465, 1159, 1008, 1125, 787, 272, 385, 1131, 114, 215, 110, 1086, 876, 907, 113, 1004, 650, 983, 929, 252, 793, 918, 976, 1055, 615, 69, 157, 1176, 18, 629, 137, 1112, 1128, 209, 874, 57, 1100, 1097, 82, 461, 1154, 945, 823, 470, 449, 1029, 1110, 343, 238, 687, 1094, 411, 97, 573, 701, 1099, 1030, 570, 495, 731, 416, 955, 1155, 921, 616, 1083, 60, 859, 679, 161, 1079, 684, 919, 73, 234, 344, 551, 50, 762, 266, 590, 608, 1018, 498, 962, 952, 902, 958, 713, 957, 779, 204, 253, 1156, 811, 724, 708, 154, 937, 675, 938, 935, 31, 734, 632, 46, 1200, 265, 680, 474, 869, 464, 567, 141, 1088, 1175, 563, 518, 493, 784, 442, 452, 1199, 729, 469, 276, 1114, 579, 546, 1211, 1209, 1119, 392, 20, 572, 367, 353, 646, 783, 592, 548, 1161, 436, 140, 251, 1177, 494, 589, 1194, 34, 726, 1208, 479, 391, 1108, 100, 828, 357, 95, 638, 652, 505, 564, 1130, 148, 741, 825, 1212, 1213, 425, 282, 1135, 1184, 718, 337, 70, 38, 422, 769, 1009, 826, 878, 273, 819, 691, 844, 881, 1138, 1162, 270, 556, 1053, 862, 832, 511, 642, 796, 66, 1157, 310, 674, 991, 290, 682, 905, 264, 1134, 528, 289, 749, 846, 1090, 192, 237, 536, 45, 372, 130, 177, 1036, 239, 224, 883, 198, 977, 791, 184, 732, 1202, 595, 965, 978, 163, 752, 1144, 143, 709, 90, 74, 1207, 649, 476, 503, 190, 543, 499, 298, 261, 233, 1037, 759, 170, 917, 1, 993, 1027, 738, 329, 451, 318, 0, 269, 199, 410, 538, 943, 191, 88, 1072, 544, 509, 490, 89, 1050, 739, 850, 35, 288, 102, 560, 332, 1026, 612, 128, 854, 327, 985, 931, 295, 777, 942, 635, 710, 610, 126, 39, 249, 305, 618, 899, 1011, 802, 898, 843, 561, 667, 1136, 296, 453, 481, 299]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  9483891844270800
the save name prefix for this run is:  chkpt-ID_9483891844270800_tag_Ablation-job-blacklist-s max freq rel_o max freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1187
rank avg (pred): 0.484 +- 0.005
mrr vals (pred, true): 0.015, 0.047
batch losses (mrrl, rdl): 0.0, 7.84874e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 776
rank avg (pred): 0.411 +- 0.231
mrr vals (pred, true): 0.164, 0.046
batch losses (mrrl, rdl): 0.0, 2.39829e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 379
rank avg (pred): 0.351 +- 0.240
mrr vals (pred, true): 0.245, 0.112
batch losses (mrrl, rdl): 0.0, 0.0001039227

Epoch over!
epoch time: 12.203

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1136
rank avg (pred): 0.233 +- 0.168
mrr vals (pred, true): 0.289, 0.227
batch losses (mrrl, rdl): 0.0, 8.38605e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1110
rank avg (pred): 0.376 +- 0.261
mrr vals (pred, true): 0.261, 0.047
batch losses (mrrl, rdl): 0.0, 3.26018e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 763
rank avg (pred): 0.506 +- 0.294
mrr vals (pred, true): 0.190, 0.034
batch losses (mrrl, rdl): 0.0, 1.19743e-05

Epoch over!
epoch time: 12.111

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 195
rank avg (pred): 0.345 +- 0.246
mrr vals (pred, true): 0.271, 0.056
batch losses (mrrl, rdl): 0.0, 8.89587e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 929
rank avg (pred): 0.517 +- 0.327
mrr vals (pred, true): 0.235, 0.017
batch losses (mrrl, rdl): 0.0, 0.0004497198

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 442
rank avg (pred): 0.344 +- 0.251
mrr vals (pred, true): 0.290, 0.043
batch losses (mrrl, rdl): 0.0, 0.0001348528

Epoch over!
epoch time: 11.991

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1148
rank avg (pred): 0.275 +- 0.209
mrr vals (pred, true): 0.329, 0.141
batch losses (mrrl, rdl): 0.0, 4.06958e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 389
rank avg (pred): 0.357 +- 0.255
mrr vals (pred, true): 0.270, 0.106
batch losses (mrrl, rdl): 0.0, 0.0001824297

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 87
rank avg (pred): 0.336 +- 0.244
mrr vals (pred, true): 0.299, 0.109
batch losses (mrrl, rdl): 0.0, 3.0269e-05

Epoch over!
epoch time: 12.074

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1068
rank avg (pred): 0.029 +- 0.023
mrr vals (pred, true): 0.445, 0.532
batch losses (mrrl, rdl): 0.0, 1.7078e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 580
rank avg (pred): 0.444 +- 0.293
mrr vals (pred, true): 0.251, 0.044
batch losses (mrrl, rdl): 0.0, 9.0087e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 340
rank avg (pred): 0.351 +- 0.243
mrr vals (pred, true): 0.274, 0.134
batch losses (mrrl, rdl): 0.0, 0.0002250719

Epoch over!
epoch time: 12.064

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 334
rank avg (pred): 0.354 +- 0.236
mrr vals (pred, true): 0.260, 0.154
batch losses (mrrl, rdl): 0.11158593, 0.0002609551

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 329
rank avg (pred): 0.370 +- 0.185
mrr vals (pred, true): 0.066, 0.162
batch losses (mrrl, rdl): 0.0922606513, 0.0003388795

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 928
rank avg (pred): 0.586 +- 0.172
mrr vals (pred, true): 0.036, 0.021
batch losses (mrrl, rdl): 0.0020526072, 0.0002229311

Epoch over!
epoch time: 12.628

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 266
rank avg (pred): 0.011 +- 0.009
mrr vals (pred, true): 0.549, 0.566
batch losses (mrrl, rdl): 0.0029691516, 9.9344e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 166
rank avg (pred): 0.354 +- 0.199
mrr vals (pred, true): 0.092, 0.046
batch losses (mrrl, rdl): 0.0176288802, 0.0001063506

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1063
rank avg (pred): 0.016 +- 0.013
mrr vals (pred, true): 0.499, 0.545
batch losses (mrrl, rdl): 0.0213032309, 8.6765e-06

Epoch over!
epoch time: 12.247

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 673
rank avg (pred): 0.472 +- 0.144
mrr vals (pred, true): 0.055, 0.047
batch losses (mrrl, rdl): 0.0002391497, 5.52396e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 524
rank avg (pred): 0.346 +- 0.166
mrr vals (pred, true): 0.071, 0.105
batch losses (mrrl, rdl): 0.0117100496, 6.00331e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 326
rank avg (pred): 0.394 +- 0.210
mrr vals (pred, true): 0.078, 0.113
batch losses (mrrl, rdl): 0.0127185602, 0.0001099199

Epoch over!
epoch time: 12.218

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1138
rank avg (pred): 0.123 +- 0.111
mrr vals (pred, true): 0.274, 0.245
batch losses (mrrl, rdl): 0.0083486279, 9.32927e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 186
rank avg (pred): 0.413 +- 0.157
mrr vals (pred, true): 0.063, 0.051
batch losses (mrrl, rdl): 0.0015862994, 5.02154e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1198
rank avg (pred): 0.630 +- 0.195
mrr vals (pred, true): 0.056, 0.047
batch losses (mrrl, rdl): 0.0003659995, 0.0007100379

Epoch over!
epoch time: 12.285

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 374
rank avg (pred): 0.325 +- 0.199
mrr vals (pred, true): 0.091, 0.108
batch losses (mrrl, rdl): 0.0028553405, 2.0391e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 761
rank avg (pred): 0.473 +- 0.107
mrr vals (pred, true): 0.047, 0.036
batch losses (mrrl, rdl): 0.0001010883, 0.0001169252

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 111
rank avg (pred): 0.463 +- 0.236
mrr vals (pred, true): 0.077, 0.097
batch losses (mrrl, rdl): 0.0071868352, 0.000450748

Epoch over!
epoch time: 12.101

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1057
rank avg (pred): 0.020 +- 0.019
mrr vals (pred, true): 0.511, 0.564
batch losses (mrrl, rdl): 0.0271264054, 4.147e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1085
rank avg (pred): 0.444 +- 0.141
mrr vals (pred, true): 0.065, 0.132
batch losses (mrrl, rdl): 0.0444247425, 0.0007157835

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1188
rank avg (pred): 0.533 +- 0.175
mrr vals (pred, true): 0.053, 0.048
batch losses (mrrl, rdl): 0.0001139353, 0.0002215815

Epoch over!
epoch time: 12.454

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 354
rank avg (pred): 0.550 +- 0.203
mrr vals (pred, true): 0.060, 0.101
batch losses (mrrl, rdl): 0.0173822101, 0.000944684

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 886
rank avg (pred): 0.443 +- 0.109
mrr vals (pred, true): 0.054, 0.054
batch losses (mrrl, rdl): 0.0001672262, 5.94484e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 815
rank avg (pred): 0.209 +- 0.194
mrr vals (pred, true): 0.332, 0.166
batch losses (mrrl, rdl): 0.2745988965, 0.0002426413

Epoch over!
epoch time: 12.327

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 653
rank avg (pred): 0.551 +- 0.191
mrr vals (pred, true): 0.063, 0.045
batch losses (mrrl, rdl): 0.0015894878, 0.0003114624

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 397
rank avg (pred): 0.412 +- 0.154
mrr vals (pred, true): 0.087, 0.172
batch losses (mrrl, rdl): 0.0725932494, 0.0008689384

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 738
rank avg (pred): 0.041 +- 0.040
mrr vals (pred, true): 0.474, 0.520
batch losses (mrrl, rdl): 0.0208744258, 2.374e-07

Epoch over!
epoch time: 12.169

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 991
rank avg (pred): 0.027 +- 0.027
mrr vals (pred, true): 0.517, 0.536
batch losses (mrrl, rdl): 0.0038875453, 1.4935e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 693
rank avg (pred): 0.564 +- 0.222
mrr vals (pred, true): 0.044, 0.042
batch losses (mrrl, rdl): 0.0003490985, 0.0002521413

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 233
rank avg (pred): 0.460 +- 0.204
mrr vals (pred, true): 0.067, 0.047
batch losses (mrrl, rdl): 0.0030278438, 2.22454e-05

Epoch over!
epoch time: 12.099

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1061
rank avg (pred): 0.024 +- 0.024
mrr vals (pred, true): 0.535, 0.549
batch losses (mrrl, rdl): 0.0019417674, 3.67e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1202
rank avg (pred): 0.677 +- 0.240
mrr vals (pred, true): 0.045, 0.043
batch losses (mrrl, rdl): 0.0002692025, 0.0009311431

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 726
rank avg (pred): 0.547 +- 0.238
mrr vals (pred, true): 0.048, 0.045
batch losses (mrrl, rdl): 2.30218e-05, 0.0001405048

Epoch over!
epoch time: 12.187

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.532 +- 0.238
mrr vals (pred, true): 0.058, 0.040

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.04237 	 0.01602 	 ~...
   83 	     1 	 0.10773 	 0.01876 	 m..s
   37 	     2 	 0.07224 	 0.02127 	 m..s
   32 	     3 	 0.06869 	 0.02964 	 m..s
   19 	     4 	 0.06226 	 0.03408 	 ~...
   11 	     5 	 0.05935 	 0.03469 	 ~...
   39 	     6 	 0.07402 	 0.03570 	 m..s
   21 	     7 	 0.06245 	 0.03716 	 ~...
   27 	     8 	 0.06413 	 0.03747 	 ~...
    7 	     9 	 0.05410 	 0.03872 	 ~...
   10 	    10 	 0.05933 	 0.03921 	 ~...
    9 	    11 	 0.05828 	 0.04025 	 ~...
   77 	    12 	 0.09613 	 0.04036 	 m..s
   22 	    13 	 0.06284 	 0.04155 	 ~...
   23 	    14 	 0.06327 	 0.04194 	 ~...
   65 	    15 	 0.08775 	 0.04308 	 m..s
   25 	    16 	 0.06339 	 0.04357 	 ~...
   31 	    17 	 0.06692 	 0.04440 	 ~...
   79 	    18 	 0.09745 	 0.04483 	 m..s
   24 	    19 	 0.06332 	 0.04487 	 ~...
   18 	    20 	 0.06219 	 0.04534 	 ~...
   61 	    21 	 0.08546 	 0.04541 	 m..s
    5 	    22 	 0.05210 	 0.04554 	 ~...
    6 	    23 	 0.05258 	 0.04555 	 ~...
   78 	    24 	 0.09645 	 0.04613 	 m..s
   53 	    25 	 0.08269 	 0.04619 	 m..s
   16 	    26 	 0.06190 	 0.04629 	 ~...
   14 	    27 	 0.06015 	 0.04656 	 ~...
   13 	    28 	 0.05998 	 0.04665 	 ~...
   42 	    29 	 0.07899 	 0.04671 	 m..s
    3 	    30 	 0.05191 	 0.04676 	 ~...
   82 	    31 	 0.10741 	 0.04692 	 m..s
   80 	    32 	 0.09802 	 0.04718 	 m..s
   73 	    33 	 0.09184 	 0.04730 	 m..s
   30 	    34 	 0.06518 	 0.04741 	 ~...
   28 	    35 	 0.06447 	 0.04750 	 ~...
   60 	    36 	 0.08527 	 0.04754 	 m..s
   45 	    37 	 0.08108 	 0.04816 	 m..s
   72 	    38 	 0.09094 	 0.04828 	 m..s
    0 	    39 	 0.04221 	 0.04840 	 ~...
   20 	    40 	 0.06226 	 0.04844 	 ~...
   35 	    41 	 0.07032 	 0.04979 	 ~...
   47 	    42 	 0.08127 	 0.04990 	 m..s
   41 	    43 	 0.07741 	 0.05004 	 ~...
   38 	    44 	 0.07364 	 0.05005 	 ~...
   17 	    45 	 0.06214 	 0.05033 	 ~...
   76 	    46 	 0.09550 	 0.05053 	 m..s
    4 	    47 	 0.05194 	 0.05073 	 ~...
   47 	    48 	 0.08127 	 0.05081 	 m..s
   67 	    49 	 0.08815 	 0.05082 	 m..s
   26 	    50 	 0.06363 	 0.05102 	 ~...
   56 	    51 	 0.08433 	 0.05117 	 m..s
   55 	    52 	 0.08398 	 0.05271 	 m..s
   59 	    53 	 0.08524 	 0.05305 	 m..s
   63 	    54 	 0.08652 	 0.05468 	 m..s
   15 	    55 	 0.06094 	 0.05484 	 ~...
   57 	    56 	 0.08455 	 0.05632 	 ~...
   29 	    57 	 0.06511 	 0.05645 	 ~...
   34 	    58 	 0.07021 	 0.05734 	 ~...
    8 	    59 	 0.05534 	 0.05749 	 ~...
   36 	    60 	 0.07110 	 0.06022 	 ~...
   12 	    61 	 0.05990 	 0.06967 	 ~...
    2 	    62 	 0.05166 	 0.07199 	 ~...
   47 	    63 	 0.08127 	 0.08324 	 ~...
   64 	    64 	 0.08702 	 0.08357 	 ~...
   43 	    65 	 0.08094 	 0.08659 	 ~...
   33 	    66 	 0.07013 	 0.08955 	 ~...
   81 	    67 	 0.10505 	 0.09547 	 ~...
   54 	    68 	 0.08282 	 0.10048 	 ~...
   66 	    69 	 0.08801 	 0.10771 	 ~...
   40 	    70 	 0.07606 	 0.11006 	 m..s
   71 	    71 	 0.09020 	 0.11164 	 ~...
   69 	    72 	 0.08998 	 0.11494 	 ~...
   47 	    73 	 0.08127 	 0.11551 	 m..s
   85 	    74 	 0.12248 	 0.11912 	 ~...
   44 	    75 	 0.08099 	 0.12052 	 m..s
   58 	    76 	 0.08507 	 0.12331 	 m..s
   86 	    77 	 0.13007 	 0.12363 	 ~...
   75 	    78 	 0.09421 	 0.12428 	 m..s
   51 	    79 	 0.08178 	 0.12660 	 m..s
   70 	    80 	 0.09004 	 0.12861 	 m..s
   46 	    81 	 0.08112 	 0.13169 	 m..s
   52 	    82 	 0.08223 	 0.13182 	 m..s
   84 	    83 	 0.12098 	 0.13486 	 ~...
   74 	    84 	 0.09262 	 0.13780 	 m..s
   62 	    85 	 0.08642 	 0.14749 	 m..s
   68 	    86 	 0.08939 	 0.14995 	 m..s
   88 	    87 	 0.18783 	 0.18150 	 ~...
   87 	    88 	 0.17489 	 0.18459 	 ~...
   90 	    89 	 0.24800 	 0.20778 	 m..s
   89 	    90 	 0.22862 	 0.21282 	 ~...
   92 	    91 	 0.32010 	 0.24174 	 m..s
   91 	    92 	 0.26428 	 0.31456 	 m..s
   93 	    93 	 0.32795 	 0.36971 	 m..s
   94 	    94 	 0.47552 	 0.46774 	 ~...
   96 	    95 	 0.51145 	 0.51763 	 ~...
   95 	    96 	 0.49120 	 0.52229 	 m..s
  108 	    97 	 0.55900 	 0.52247 	 m..s
  110 	    98 	 0.55935 	 0.52543 	 m..s
  100 	    99 	 0.54319 	 0.52699 	 ~...
   99 	   100 	 0.54213 	 0.52722 	 ~...
  101 	   101 	 0.54360 	 0.53137 	 ~...
   97 	   102 	 0.54061 	 0.53186 	 ~...
  107 	   103 	 0.55708 	 0.54080 	 ~...
  112 	   104 	 0.56012 	 0.54126 	 ~...
  105 	   105 	 0.55645 	 0.54154 	 ~...
  106 	   106 	 0.55652 	 0.54355 	 ~...
   98 	   107 	 0.54208 	 0.54405 	 ~...
  104 	   108 	 0.55411 	 0.54536 	 ~...
  102 	   109 	 0.55074 	 0.54652 	 ~...
  109 	   110 	 0.55923 	 0.54664 	 ~...
  113 	   111 	 0.56065 	 0.54725 	 ~...
  103 	   112 	 0.55176 	 0.54835 	 ~...
  117 	   113 	 0.56468 	 0.54879 	 ~...
  119 	   114 	 0.58335 	 0.54936 	 m..s
  115 	   115 	 0.56072 	 0.55070 	 ~...
  116 	   116 	 0.56381 	 0.55385 	 ~...
  111 	   117 	 0.56006 	 0.55473 	 ~...
  120 	   118 	 0.58730 	 0.55699 	 m..s
  114 	   119 	 0.56068 	 0.55778 	 ~...
  118 	   120 	 0.56506 	 0.56643 	 ~...
==========================================
r_mrr = 0.990036129951477
r2_mrr = 0.9771376252174377
spearmanr_mrr@5 = 0.8337268829345703
spearmanr_mrr@10 = 0.8881300091743469
spearmanr_mrr@50 = 0.9977383017539978
spearmanr_mrr@100 = 0.9953880310058594
spearmanr_mrr@All = 0.9956915974617004
==========================================
test time: 0.392
Done Testing dataset UMLS
total time taken: 189.16145491600037
training time taken: 183.61476492881775
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9900)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9771)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8337)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8881)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9977)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9954)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9957)}}, 'test_loss': {'DistMult': {'UMLS': 1.00802703334557}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'o max freq rel', 's max freq rel'}

==========================================================================================
------------------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s mean freq rel_o mean freq rel
------------------------------------------------------------------------------------------
==========================================================================================
Running on a grid of size 1
Using random seed: 8244399546254000
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1060, 801, 1208, 828, 877, 1212, 542, 701, 605, 169, 258, 1214, 364, 958, 342, 1132, 1151, 447, 388, 712, 1067, 987, 848, 718, 946, 514, 474, 1045, 574, 1186, 573, 68, 1119, 1001, 643, 179, 1005, 418, 476, 998, 765, 1191, 802, 819, 743, 73, 1093, 55, 311, 370, 850, 85, 500, 945, 270, 22, 1113, 1101, 281, 437, 881, 793, 110, 1107, 140, 385, 781, 563, 313, 259, 64, 888, 465, 1162, 623, 528, 580, 147, 163, 1163, 1129, 575, 80, 72, 473, 45, 874, 1199, 1180, 914, 74, 428, 666, 586, 102, 1206, 422, 18, 703, 224, 876, 1207, 1193, 1125, 862, 676, 243, 367, 842, 1116, 31, 717, 1131, 111, 649, 589, 84, 98, 546, 549, 578]
valid_ids (0): []
train_ids (1094): [343, 745, 937, 290, 933, 971, 512, 762, 190, 996, 257, 395, 795, 999, 867, 269, 1139, 6, 1104, 941, 543, 530, 411, 240, 13, 628, 777, 836, 77, 1144, 408, 53, 787, 796, 627, 1011, 602, 1188, 317, 323, 731, 617, 369, 857, 1148, 960, 495, 678, 423, 1053, 978, 750, 596, 986, 1146, 1213, 957, 276, 684, 620, 641, 1173, 780, 1020, 3, 44, 824, 568, 1178, 1039, 394, 969, 1044, 119, 1031, 37, 136, 1106, 1077, 1043, 964, 417, 362, 1081, 1130, 901, 135, 360, 839, 1003, 63, 640, 485, 608, 966, 203, 336, 65, 351, 95, 544, 738, 331, 501, 1012, 358, 157, 558, 403, 146, 1156, 296, 122, 306, 696, 779, 928, 220, 1085, 735, 400, 917, 809, 48, 484, 1052, 175, 421, 847, 920, 774, 739, 814, 798, 107, 896, 1059, 446, 886, 916, 1205, 366, 532, 831, 1021, 639, 727, 379, 803, 973, 794, 16, 834, 659, 27, 315, 1124, 116, 148, 382, 271, 1098, 349, 491, 143, 97, 521, 621, 87, 137, 195, 912, 807, 690, 478, 940, 266, 759, 1210, 10, 565, 1142, 883, 99, 1176, 167, 609, 209, 984, 249, 595, 1120, 723, 371, 1200, 754, 210, 94, 247, 644, 715, 786, 572, 763, 518, 237, 131, 672, 679, 769, 742, 1016, 1022, 1165, 354, 553, 406, 845, 926, 611, 193, 552, 631, 215, 126, 356, 1051, 165, 441, 1166, 846, 455, 898, 681, 142, 600, 128, 895, 646, 184, 348, 1126, 699, 1203, 91, 668, 365, 466, 730, 78, 930, 768, 982, 338, 316, 705, 218, 448, 171, 350, 504, 661, 934, 670, 947, 330, 944, 652, 935, 1075, 1155, 439, 52, 62, 583, 186, 704, 541, 413, 961, 288, 129, 101, 693, 721, 33, 398, 967, 866, 785, 687, 166, 412, 938, 855, 231, 92, 1080, 757, 792, 1088, 272, 638, 189, 261, 219, 393, 223, 334, 861, 954, 811, 321, 274, 650, 576, 70, 1, 75, 1114, 674, 487, 327, 716, 1108, 1152, 904, 138, 725, 496, 921, 997, 459, 654, 103, 235, 81, 604, 980, 925, 1159, 1128, 525, 335, 882, 783, 481, 929, 17, 616, 314, 775, 192, 337, 613, 188, 303, 469, 806, 594, 979, 851, 199, 1024, 1076, 410, 226, 244, 767, 1046, 637, 238, 284, 1175, 430, 658, 776, 943, 818, 634, 225, 59, 897, 172, 8, 50, 720, 279, 517, 840, 7, 368, 93, 632, 962, 753, 139, 664, 429, 629, 560, 708, 100, 1103, 234, 907, 507, 347, 1000, 1068, 1167, 1150, 216, 655, 452, 434, 79, 1030, 325, 663, 860, 823, 965, 1069, 1154, 38, 387, 622, 1035, 438, 653, 426, 1038, 510, 592, 158, 1185, 480, 714, 46, 183, 145, 1019, 339, 955, 733, 1177, 173, 729, 265, 11, 443, 582, 346, 1023, 185, 76, 206, 380, 196, 956, 180, 432, 458, 150, 267, 407, 1058, 1004, 378, 737, 373, 988, 669, 397, 642, 21, 490, 472, 1079, 112, 177, 756, 0, 1057, 509, 1095, 660, 564, 133, 740, 656, 635, 372, 409, 263, 1141, 260, 713, 1140, 1184, 830, 497, 837, 821, 939, 1195, 436, 1096, 985, 462, 1174, 1121, 1158, 1008, 1072, 555, 377, 598, 547, 537, 1050, 47, 275, 607, 707, 1157, 1149, 149, 645, 242, 1190, 88, 844, 86, 612, 719, 667, 648, 141, 894, 289, 698, 268, 320, 651, 771, 675, 599, 829, 503, 359, 755, 550, 1028, 153, 233, 307, 151, 591, 711, 1187, 121, 770, 948, 1168, 1183, 993, 435, 697, 300, 352, 14, 799, 1074, 477, 20, 557, 869, 43, 1169, 1123, 1048, 505, 1209, 854, 995, 590, 1172, 1034, 250, 1164, 174, 1100, 245, 301, 181, 1118, 647, 39, 927, 162, 1115, 1078, 890, 15, 752, 841, 470, 618, 766, 567, 545, 236, 130, 710, 902, 515, 187, 424, 892, 326, 483, 533, 1027, 1007, 310, 923, 453, 256, 593, 1026, 885, 386, 601, 264, 606, 294, 1002, 588, 908, 662, 416, 363, 118, 843, 832, 114, 58, 451, 764, 1117, 304, 1192, 502, 734, 1086, 357, 950, 125, 772, 345, 633, 527, 863, 891, 332, 1135, 390, 859, 871, 695, 893, 1083, 60, 1091, 56, 873, 461, 983, 19, 536, 51, 619, 878, 1202, 431, 880, 872, 922, 1041, 198, 952, 383, 1036, 569, 168, 1211, 35, 963, 161, 889, 826, 252, 784, 1062, 584, 324, 117, 1089, 746, 254, 722, 36, 182, 905, 1014, 511, 1197, 24, 106, 164, 865, 680, 402, 127, 556, 686, 626, 688, 749, 630, 1092, 990, 391, 1160, 40, 253, 1055, 683, 442, 513, 906, 468, 810, 977, 789, 489, 1013, 949, 516, 475, 603, 457, 534, 308, 875, 23, 1204, 706, 492, 747, 1127, 389, 227, 54, 773, 790, 344, 788, 176, 942, 726, 778, 109, 1009, 486, 178, 1110, 816, 34, 741, 152, 838, 559, 856, 144, 1161, 585, 229, 571, 539, 587, 579, 636, 376, 1070, 858, 392, 761, 1102, 1097, 493, 1063, 333, 1112, 425, 82, 526, 624, 531, 381, 911, 454, 420, 991, 241, 989, 915, 538, 1029, 736, 1094, 482, 760, 1137, 1182, 1181, 813, 124, 728, 1133, 282, 523, 519, 204, 554, 919, 909, 278, 1105, 4, 804, 551, 105, 852, 1006, 1090, 12, 820, 903, 671, 1138, 213, 69, 924, 160, 471, 968, 797, 899, 932, 207, 1109, 32, 522, 159, 211, 1015, 868, 700, 812, 953, 170, 1040, 1179, 577, 535, 202, 1073, 433, 67, 1065, 566, 318, 49, 581, 155, 450, 42, 808, 191, 57, 665, 156, 1017, 1025, 910, 197, 286, 239, 744, 355, 1189, 1087, 835, 709, 913, 154, 419, 570, 1054, 508, 682, 918, 123, 488, 329, 214, 405, 614, 1056, 83, 104, 115, 399, 959, 758, 900, 230, 822, 232, 689, 479, 1084, 71, 677, 322, 89, 994, 506, 1170, 751, 440, 981, 833, 685, 291, 248, 732, 1201, 384, 297, 610, 972, 277, 361, 800, 951, 970, 5, 305, 396, 374, 1033, 1099, 1018, 815, 1136, 61, 1134, 66, 625, 974, 134, 975, 201, 246, 1010, 931, 499, 1082, 319, 691, 936, 205, 887, 1145, 1049, 673, 1061, 228, 445, 222, 540, 415, 1198, 30, 217, 328, 1147, 524, 1071, 791, 29, 298, 25, 827, 2, 1042, 375, 26, 1032, 1066, 1037, 108, 194, 28, 340, 9, 597, 120, 817, 657, 251, 849, 561, 1047, 992, 692, 1143, 309, 444, 221, 302, 280, 805, 1122, 498, 293, 262, 884, 113, 312, 782, 529, 90, 1171, 460, 520, 132, 562, 467, 1194, 283, 1153, 96, 702, 414, 825, 464, 427, 879, 548, 456, 449, 615, 353, 1111, 976, 200, 864, 299, 404, 1064, 463, 694, 285, 273, 724, 287, 748, 292, 401, 295, 853, 212, 870, 494, 208, 1196, 255, 341, 41]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  8667048587025227
the save name prefix for this run is:  chkpt-ID_8667048587025227_tag_Ablation-job-blacklist-s mean freq rel_o mean freq rel
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 997
rank avg (pred): 0.424 +- 0.003
mrr vals (pred, true): 0.017, 0.547
batch losses (mrrl, rdl): 0.0, 0.0032620428

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 345
rank avg (pred): 0.393 +- 0.117
mrr vals (pred, true): 0.027, 0.117
batch losses (mrrl, rdl): 0.0, 0.0002568148

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 191
rank avg (pred): 0.353 +- 0.233
mrr vals (pred, true): 0.251, 0.047
batch losses (mrrl, rdl): 0.0, 0.0001028488

Epoch over!
epoch time: 12.072

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 256
rank avg (pred): 0.041 +- 0.029
mrr vals (pred, true): 0.376, 0.569
batch losses (mrrl, rdl): 0.0, 1.9884e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 35
rank avg (pred): 0.046 +- 0.034
mrr vals (pred, true): 0.384, 0.541
batch losses (mrrl, rdl): 0.0, 2.3569e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 456
rank avg (pred): 0.375 +- 0.258
mrr vals (pred, true): 0.269, 0.054
batch losses (mrrl, rdl): 0.0, 1.79355e-05

Epoch over!
epoch time: 11.884

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 779
rank avg (pred): 0.415 +- 0.279
mrr vals (pred, true): 0.258, 0.027
batch losses (mrrl, rdl): 0.0, 0.0002207326

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 709
rank avg (pred): 0.441 +- 0.276
mrr vals (pred, true): 0.227, 0.044
batch losses (mrrl, rdl): 0.0, 1.68522e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 461
rank avg (pred): 0.369 +- 0.255
mrr vals (pred, true): 0.266, 0.050
batch losses (mrrl, rdl): 0.0, 3.40847e-05

Epoch over!
epoch time: 11.969

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1192
rank avg (pred): 0.407 +- 0.271
mrr vals (pred, true): 0.248, 0.051
batch losses (mrrl, rdl): 0.0, 8.5923e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 840
rank avg (pred): 0.460 +- 0.295
mrr vals (pred, true): 0.226, 0.035
batch losses (mrrl, rdl): 0.0, 1.76904e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 680
rank avg (pred): 0.431 +- 0.258
mrr vals (pred, true): 0.212, 0.047
batch losses (mrrl, rdl): 0.0, 1.02614e-05

Epoch over!
epoch time: 11.902

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 355
rank avg (pred): 0.334 +- 0.243
mrr vals (pred, true): 0.290, 0.086
batch losses (mrrl, rdl): 0.0, 3.66375e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 780
rank avg (pred): 0.462 +- 0.292
mrr vals (pred, true): 0.225, 0.048
batch losses (mrrl, rdl): 0.0, 3.09254e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 406
rank avg (pred): 0.328 +- 0.240
mrr vals (pred, true): 0.289, 0.046
batch losses (mrrl, rdl): 0.0, 0.0001201501

Epoch over!
epoch time: 11.983

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 558
rank avg (pred): 0.400 +- 0.292
mrr vals (pred, true): 0.271, 0.050
batch losses (mrrl, rdl): 0.4877507985, 2.01441e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 251
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.553, 0.535
batch losses (mrrl, rdl): 0.003195524, 1.17686e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 159
rank avg (pred): 0.335 +- 0.136
mrr vals (pred, true): 0.084, 0.083
batch losses (mrrl, rdl): 0.0114429481, 5.02283e-05

Epoch over!
epoch time: 12.422

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 330
rank avg (pred): 0.356 +- 0.128
mrr vals (pred, true): 0.065, 0.098
batch losses (mrrl, rdl): 0.0022829694, 7.03054e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 431
rank avg (pred): 0.352 +- 0.141
mrr vals (pred, true): 0.073, 0.042
batch losses (mrrl, rdl): 0.0053565111, 0.0002333505

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 247
rank avg (pred): 0.010 +- 0.008
mrr vals (pred, true): 0.569, 0.564
batch losses (mrrl, rdl): 0.0002206204, 1.25136e-05

Epoch over!
epoch time: 12.222

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 496
rank avg (pred): 0.297 +- 0.210
mrr vals (pred, true): 0.204, 0.194
batch losses (mrrl, rdl): 0.0010819869, 0.0001806952

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 420
rank avg (pred): 0.341 +- 0.158
mrr vals (pred, true): 0.089, 0.053
batch losses (mrrl, rdl): 0.0149113489, 0.0001655782

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 999
rank avg (pred): 0.368 +- 0.146
mrr vals (pred, true): 0.066, 0.132
batch losses (mrrl, rdl): 0.0425058641, 0.0002068351

Epoch over!
epoch time: 12.27

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 788
rank avg (pred): 0.407 +- 0.101
mrr vals (pred, true): 0.046, 0.050
batch losses (mrrl, rdl): 0.0001740053, 0.0001170471

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1190
rank avg (pred): 0.370 +- 0.151
mrr vals (pred, true): 0.073, 0.048
batch losses (mrrl, rdl): 0.0053349258, 0.0001200294

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 296
rank avg (pred): 0.013 +- 0.011
mrr vals (pred, true): 0.547, 0.544
batch losses (mrrl, rdl): 0.0001091763, 1.10897e-05

Epoch over!
epoch time: 12.318

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 356
rank avg (pred): 0.377 +- 0.148
mrr vals (pred, true): 0.069, 0.127
batch losses (mrrl, rdl): 0.0333747789, 0.0002248958

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1090
rank avg (pred): 0.377 +- 0.152
mrr vals (pred, true): 0.070, 0.089
batch losses (mrrl, rdl): 0.0041331891, 8.98384e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 404
rank avg (pred): 0.337 +- 0.162
mrr vals (pred, true): 0.093, 0.108
batch losses (mrrl, rdl): 0.0021780028, 0.0001186517

Epoch over!
epoch time: 12.257

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 277
rank avg (pred): 0.016 +- 0.014
mrr vals (pred, true): 0.539, 0.554
batch losses (mrrl, rdl): 0.0021758899, 9.1519e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 936
rank avg (pred): 0.415 +- 0.076
mrr vals (pred, true): 0.031, 0.018
batch losses (mrrl, rdl): 0.0035391038, 0.0017684011

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1081
rank avg (pred): 0.457 +- 0.176
mrr vals (pred, true): 0.059, 0.094
batch losses (mrrl, rdl): 0.000731715, 0.0003559316

Epoch over!
epoch time: 12.496

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 156
rank avg (pred): 0.359 +- 0.135
mrr vals (pred, true): 0.069, 0.104
batch losses (mrrl, rdl): 0.0119104199, 9.57126e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 349
rank avg (pred): 0.357 +- 0.151
mrr vals (pred, true): 0.076, 0.088
batch losses (mrrl, rdl): 0.0067021595, 4.68771e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 544
rank avg (pred): 0.405 +- 0.185
mrr vals (pred, true): 0.079, 0.080
batch losses (mrrl, rdl): 0.0086748004, 5.46376e-05

Epoch over!
epoch time: 12.115

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 503
rank avg (pred): 0.311 +- 0.240
mrr vals (pred, true): 0.266, 0.235
batch losses (mrrl, rdl): 0.0098016439, 0.0003601191

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 344
rank avg (pred): 0.349 +- 0.141
mrr vals (pred, true): 0.074, 0.154
batch losses (mrrl, rdl): 0.0634637922, 0.0002742996

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1043
rank avg (pred): 0.353 +- 0.153
mrr vals (pred, true): 0.083, 0.051
batch losses (mrrl, rdl): 0.0109920502, 0.0001135397

Epoch over!
epoch time: 12.326

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1159
rank avg (pred): 0.379 +- 0.242
mrr vals (pred, true): 0.174, 0.127
batch losses (mrrl, rdl): 0.0229053088, 0.0003492758

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 556
rank avg (pred): 0.371 +- 0.160
mrr vals (pred, true): 0.081, 0.062
batch losses (mrrl, rdl): 0.0096500311, 0.0001077332

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 276
rank avg (pred): 0.021 +- 0.020
mrr vals (pred, true): 0.558, 0.553
batch losses (mrrl, rdl): 0.000266198, 8.3696e-06

Epoch over!
epoch time: 12.135

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 78
rank avg (pred): 0.028 +- 0.028
mrr vals (pred, true): 0.531, 0.506
batch losses (mrrl, rdl): 0.005912859, 1.04334e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 626
rank avg (pred): 0.369 +- 0.084
mrr vals (pred, true): 0.049, 0.040
batch losses (mrrl, rdl): 1.04091e-05, 0.0003082951

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 42
rank avg (pred): 0.026 +- 0.026
mrr vals (pred, true): 0.541, 0.531
batch losses (mrrl, rdl): 0.0009534182, 8.4102e-06

Epoch over!
epoch time: 12.264

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.030 +- 0.030
mrr vals (pred, true): 0.533, 0.558

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
   25 	     0 	 0.06328 	 0.03399 	 ~...
   40 	     1 	 0.06759 	 0.03570 	 m..s
   18 	     2 	 0.06142 	 0.03713 	 ~...
   43 	     3 	 0.06837 	 0.03739 	 m..s
   37 	     4 	 0.06673 	 0.03746 	 ~...
   51 	     5 	 0.07552 	 0.03785 	 m..s
   27 	     6 	 0.06342 	 0.03808 	 ~...
   10 	     7 	 0.05679 	 0.03855 	 ~...
    6 	     8 	 0.05368 	 0.03905 	 ~...
   38 	     9 	 0.06699 	 0.03907 	 ~...
   56 	    10 	 0.07962 	 0.04036 	 m..s
    2 	    11 	 0.04414 	 0.04145 	 ~...
   28 	    12 	 0.06372 	 0.04203 	 ~...
    9 	    13 	 0.05570 	 0.04224 	 ~...
   83 	    14 	 0.09036 	 0.04229 	 m..s
   19 	    15 	 0.06182 	 0.04274 	 ~...
   32 	    16 	 0.06447 	 0.04294 	 ~...
   41 	    17 	 0.06785 	 0.04316 	 ~...
   26 	    18 	 0.06340 	 0.04339 	 ~...
   36 	    19 	 0.06539 	 0.04349 	 ~...
   14 	    20 	 0.05898 	 0.04379 	 ~...
   33 	    21 	 0.06461 	 0.04441 	 ~...
   89 	    22 	 0.10675 	 0.04441 	 m..s
   44 	    23 	 0.07034 	 0.04461 	 ~...
   16 	    24 	 0.05960 	 0.04469 	 ~...
   17 	    25 	 0.05970 	 0.04470 	 ~...
   15 	    26 	 0.05956 	 0.04487 	 ~...
   73 	    27 	 0.08700 	 0.04508 	 m..s
   74 	    28 	 0.08851 	 0.04541 	 m..s
   30 	    29 	 0.06425 	 0.04561 	 ~...
   39 	    30 	 0.06749 	 0.04613 	 ~...
   65 	    31 	 0.08389 	 0.04630 	 m..s
   20 	    32 	 0.06219 	 0.04660 	 ~...
    8 	    33 	 0.05448 	 0.04665 	 ~...
    4 	    34 	 0.05290 	 0.04665 	 ~...
   11 	    35 	 0.05781 	 0.04667 	 ~...
    1 	    36 	 0.04406 	 0.04678 	 ~...
   55 	    37 	 0.07952 	 0.04698 	 m..s
   12 	    38 	 0.05871 	 0.04709 	 ~...
   58 	    39 	 0.08086 	 0.04738 	 m..s
   59 	    40 	 0.08137 	 0.04755 	 m..s
   61 	    41 	 0.08158 	 0.04758 	 m..s
   68 	    42 	 0.08475 	 0.04759 	 m..s
   31 	    43 	 0.06434 	 0.04837 	 ~...
   90 	    44 	 0.10995 	 0.04852 	 m..s
    0 	    45 	 0.04269 	 0.04852 	 ~...
   72 	    46 	 0.08574 	 0.04860 	 m..s
   22 	    47 	 0.06268 	 0.04905 	 ~...
    7 	    48 	 0.05375 	 0.04916 	 ~...
   13 	    49 	 0.05878 	 0.04921 	 ~...
   23 	    50 	 0.06296 	 0.04922 	 ~...
   42 	    51 	 0.06805 	 0.04949 	 ~...
   29 	    52 	 0.06393 	 0.04973 	 ~...
   50 	    53 	 0.07537 	 0.04989 	 ~...
    3 	    54 	 0.05283 	 0.05000 	 ~...
   67 	    55 	 0.08410 	 0.05025 	 m..s
   66 	    56 	 0.08404 	 0.05032 	 m..s
   47 	    57 	 0.07195 	 0.05108 	 ~...
   60 	    58 	 0.08147 	 0.05129 	 m..s
   34 	    59 	 0.06472 	 0.05139 	 ~...
   78 	    60 	 0.08950 	 0.05161 	 m..s
   88 	    61 	 0.10062 	 0.05234 	 m..s
   54 	    62 	 0.07943 	 0.05270 	 ~...
   48 	    63 	 0.07223 	 0.05344 	 ~...
   69 	    64 	 0.08479 	 0.05362 	 m..s
   52 	    65 	 0.07568 	 0.05480 	 ~...
   21 	    66 	 0.06255 	 0.05501 	 ~...
    5 	    67 	 0.05334 	 0.05504 	 ~...
   79 	    68 	 0.08980 	 0.05540 	 m..s
   81 	    69 	 0.09028 	 0.05562 	 m..s
   35 	    70 	 0.06512 	 0.05719 	 ~...
   80 	    71 	 0.08999 	 0.06365 	 ~...
   84 	    72 	 0.09046 	 0.07225 	 ~...
   76 	    73 	 0.08869 	 0.08059 	 ~...
   77 	    74 	 0.08920 	 0.08490 	 ~...
   82 	    75 	 0.09035 	 0.09081 	 ~...
   46 	    76 	 0.07165 	 0.09096 	 ~...
   70 	    77 	 0.08518 	 0.09158 	 ~...
   64 	    78 	 0.08345 	 0.09517 	 ~...
   49 	    79 	 0.07531 	 0.09729 	 ~...
   71 	    80 	 0.08549 	 0.09758 	 ~...
   45 	    81 	 0.07135 	 0.10153 	 m..s
   24 	    82 	 0.06317 	 0.10574 	 m..s
   86 	    83 	 0.09216 	 0.10651 	 ~...
   87 	    84 	 0.09357 	 0.10981 	 ~...
   91 	    85 	 0.11005 	 0.11170 	 ~...
   53 	    86 	 0.07905 	 0.11661 	 m..s
   93 	    87 	 0.14141 	 0.11705 	 ~...
   92 	    88 	 0.12401 	 0.12363 	 ~...
   57 	    89 	 0.08029 	 0.12400 	 m..s
   62 	    90 	 0.08279 	 0.13168 	 m..s
   75 	    91 	 0.08863 	 0.13182 	 m..s
   63 	    92 	 0.08334 	 0.14266 	 m..s
   85 	    93 	 0.09162 	 0.14749 	 m..s
   94 	    94 	 0.18481 	 0.14821 	 m..s
   95 	    95 	 0.28446 	 0.23905 	 m..s
   96 	    96 	 0.42267 	 0.41169 	 ~...
  109 	    97 	 0.54810 	 0.51240 	 m..s
  103 	    98 	 0.53959 	 0.51740 	 ~...
   98 	    99 	 0.48100 	 0.52023 	 m..s
  102 	   100 	 0.53926 	 0.52041 	 ~...
  108 	   101 	 0.54742 	 0.52120 	 ~...
  110 	   102 	 0.54824 	 0.52142 	 ~...
  104 	   103 	 0.54606 	 0.52744 	 ~...
  101 	   104 	 0.53832 	 0.52858 	 ~...
   97 	   105 	 0.47948 	 0.53267 	 m..s
  100 	   106 	 0.53798 	 0.54072 	 ~...
  115 	   107 	 0.55570 	 0.54489 	 ~...
  107 	   108 	 0.54670 	 0.54875 	 ~...
  117 	   109 	 0.56193 	 0.55089 	 ~...
  119 	   110 	 0.56275 	 0.55097 	 ~...
  113 	   111 	 0.54974 	 0.55206 	 ~...
  111 	   112 	 0.54824 	 0.55283 	 ~...
  114 	   113 	 0.55027 	 0.55474 	 ~...
  112 	   114 	 0.54828 	 0.55536 	 ~...
   99 	   115 	 0.53311 	 0.55836 	 ~...
  106 	   116 	 0.54661 	 0.55855 	 ~...
  120 	   117 	 0.56352 	 0.55884 	 ~...
  118 	   118 	 0.56251 	 0.55948 	 ~...
  116 	   119 	 0.55996 	 0.56058 	 ~...
  105 	   120 	 0.54651 	 0.56120 	 ~...
==========================================
r_mrr = 0.9925811290740967
r2_mrr = 0.9812002182006836
spearmanr_mrr@5 = 0.8552651405334473
spearmanr_mrr@10 = 0.9578896164894104
spearmanr_mrr@50 = 0.9973820447921753
spearmanr_mrr@100 = 0.9966641068458557
spearmanr_mrr@All = 0.9969124794006348
==========================================
test time: 0.429
Done Testing dataset UMLS
total time taken: 188.98697519302368
training time taken: 183.13424277305603
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9926)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9812)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.8553)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.9579)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9974)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9967)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9969)}}, 'test_loss': {'DistMult': {'UMLS': 0.8730262753961142}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s mean freq rel', 'o mean freq rel'}

================================================================================
--------------------------------------------------------------------------------
Running a TWIG experiment with tag: Ablation-job-blacklist-s num rels_o num rels
--------------------------------------------------------------------------------
================================================================================
Running on a grid of size 1
Using random seed: 7035051392482141
Starting TWIG!
Loading datasets
Loading DistMult...
Loading UMLS...
- loading run 2.1...
Creating the train-test split
using splits:
test_ids (121): [1020, 771, 661, 1038, 432, 544, 94, 402, 344, 1213, 703, 964, 1141, 1161, 179, 756, 1034, 842, 271, 380, 189, 439, 896, 176, 1184, 63, 644, 469, 423, 500, 301, 908, 636, 815, 801, 556, 51, 751, 631, 679, 673, 280, 972, 881, 437, 913, 989, 361, 884, 466, 140, 763, 678, 543, 474, 1187, 906, 443, 755, 351, 349, 522, 1202, 617, 42, 398, 783, 700, 975, 795, 91, 255, 625, 102, 64, 701, 804, 648, 101, 774, 711, 582, 650, 944, 163, 1013, 853, 546, 482, 653, 519, 607, 251, 1178, 710, 405, 1052, 1, 341, 905, 234, 600, 1182, 515, 86, 39, 30, 666, 8, 169, 1169, 514, 747, 969, 410, 890, 198, 332, 986, 342, 148]
valid_ids (0): []
train_ids (1094): [37, 529, 698, 817, 588, 605, 214, 173, 545, 172, 1201, 203, 420, 974, 143, 118, 586, 295, 524, 288, 922, 359, 1078, 707, 990, 481, 619, 206, 246, 321, 278, 732, 465, 312, 1066, 209, 1006, 537, 260, 871, 257, 592, 379, 300, 865, 868, 134, 553, 159, 493, 982, 940, 264, 632, 733, 71, 621, 608, 195, 1157, 525, 317, 126, 557, 1071, 657, 594, 44, 850, 40, 1059, 576, 542, 365, 429, 794, 115, 128, 117, 244, 646, 1051, 866, 776, 23, 389, 89, 767, 717, 1193, 430, 1212, 252, 424, 746, 139, 171, 394, 523, 916, 156, 170, 1144, 699, 1117, 1042, 814, 1080, 491, 348, 1063, 1167, 1145, 616, 475, 655, 535, 454, 910, 955, 885, 360, 721, 427, 259, 505, 254, 1073, 434, 689, 520, 1067, 683, 452, 263, 324, 26, 144, 66, 723, 180, 483, 207, 199, 162, 387, 480, 346, 399, 215, 762, 65, 343, 937, 970, 953, 1105, 108, 1174, 1021, 496, 994, 1019, 851, 859, 193, 502, 338, 618, 538, 995, 530, 1092, 55, 1011, 408, 183, 382, 153, 981, 114, 383, 936, 928, 1082, 213, 1069, 790, 392, 897, 1154, 297, 510, 1166, 310, 941, 925, 821, 414, 381, 1135, 722, 460, 663, 779, 1110, 370, 791, 580, 561, 99, 421, 1137, 555, 58, 1035, 1168, 1127, 495, 436, 1057, 838, 568, 28, 220, 934, 415, 745, 235, 891, 456, 285, 240, 942, 1043, 385, 654, 355, 323, 141, 1143, 258, 799, 660, 548, 373, 959, 83, 759, 521, 879, 957, 468, 107, 658, 643, 630, 1008, 0, 231, 2, 1016, 286, 591, 966, 787, 656, 847, 681, 290, 73, 917, 499, 82, 1090, 1097, 367, 1060, 1208, 854, 219, 31, 1017, 81, 478, 824, 888, 760, 21, 1004, 1048, 1185, 448, 637, 882, 41, 503, 1196, 517, 662, 714, 1120, 813, 1087, 1054, 708, 739, 413, 145, 1115, 536, 315, 575, 682, 59, 190, 729, 1147, 291, 407, 674, 695, 444, 340, 404, 1203, 740, 60, 489, 563, 100, 914, 416, 949, 135, 980, 36, 731, 769, 803, 585, 22, 287, 438, 362, 999, 61, 222, 840, 1152, 276, 1014, 668, 1083, 810, 877, 789, 541, 1068, 898, 1150, 13, 299, 425, 154, 777, 702, 48, 742, 820, 54, 844, 292, 599, 236, 119, 131, 138, 827, 441, 649, 933, 1195, 802, 1176, 1119, 1146, 160, 479, 1050, 684, 996, 793, 841, 418, 1031, 178, 67, 694, 24, 924, 142, 87, 487, 1018, 640, 572, 719, 157, 567, 511, 307, 1181, 528, 1151, 211, 302, 902, 224, 217, 741, 16, 1088, 303, 70, 624, 62, 68, 1200, 1102, 758, 845, 304, 744, 1037, 1121, 333, 554, 1026, 319, 455, 593, 1138, 88, 186, 1130, 692, 946, 216, 98, 184, 363, 129, 889, 377, 880, 947, 272, 768, 440, 136, 50, 196, 457, 1077, 628, 550, 1192, 17, 904, 106, 1056, 52, 1036, 704, 870, 1126, 15, 326, 1053, 1197, 828, 1189, 861, 35, 839, 531, 943, 57, 686, 831, 150, 856, 132, 1015, 920, 1070, 893, 921, 356, 256, 248, 664, 1091, 225, 597, 38, 788, 1045, 1075, 635, 212, 562, 1160, 998, 785, 1081, 19, 1003, 34, 459, 133, 886, 1095, 716, 1153, 895, 32, 1109, 453, 33, 1159, 1047, 289, 780, 1118, 1139, 816, 876, 188, 146, 266, 976, 147, 926, 110, 1122, 95, 464, 1030, 1162, 396, 1142, 602, 867, 761, 167, 347, 612, 911, 93, 449, 743, 12, 269, 105, 488, 473, 670, 596, 948, 993, 516, 1194, 932, 1116, 490, 725, 353, 6, 352, 450, 645, 372, 875, 1061, 798, 375, 935, 293, 862, 610, 1103, 241, 371, 279, 579, 14, 598, 296, 570, 477, 952, 1210, 1177, 1106, 152, 497, 208, 900, 90, 1046, 9, 29, 846, 691, 281, 426, 805, 237, 298, 915, 1096, 313, 1134, 238, 1112, 200, 676, 166, 175, 325, 1100, 565, 931, 792, 967, 174, 938, 311, 122, 80, 1032, 485, 706, 1124, 182, 1072, 647, 498, 903, 819, 963, 873, 614, 1132, 335, 829, 374, 1128, 1188, 1179, 1089, 539, 693, 274, 390, 1111, 641, 331, 634, 626, 328, 1064, 1114, 202, 1131, 391, 601, 595, 267, 675, 187, 233, 354, 27, 161, 53, 494, 1007, 560, 10, 1094, 262, 97, 532, 318, 111, 345, 56, 164, 368, 507, 445, 401, 518, 907, 47, 754, 718, 513, 228, 1033, 467, 566, 210, 18, 104, 927, 807, 1002, 834, 569, 181, 417, 736, 1086, 403, 388, 1163, 1204, 734, 221, 872, 284, 249, 526, 357, 687, 484, 458, 1190, 75, 765, 860, 1183, 74, 918, 406, 1074, 901, 177, 393, 43, 735, 337, 615, 712, 973, 168, 149, 322, 1108, 1155, 1027, 84, 316, 137, 463, 571, 165, 314, 151, 232, 728, 894, 77, 606, 709, 930, 306, 121, 1098, 672, 589, 965, 46, 85, 1180, 835, 247, 431, 832, 961, 864, 800, 113, 985, 1136, 49, 185, 909, 978, 447, 945, 69, 727, 613, 826, 581, 533, 369, 358, 808, 609, 887, 275, 977, 336, 627, 504, 1164, 45, 577, 962, 629, 283, 578, 96, 811, 836, 1198, 486, 1171, 76, 1041, 1158, 1076, 749, 848, 1207, 855, 992, 782, 651, 451, 786, 837, 757, 752, 770, 277, 573, 1023, 833, 812, 1170, 818, 378, 282, 112, 669, 883, 1165, 892, 3, 671, 192, 1009, 1214, 1065, 1000, 1025, 386, 1085, 508, 971, 857, 843, 103, 419, 991, 11, 400, 652, 1205, 509, 724, 574, 622, 784, 191, 205, 988, 334, 1058, 327, 534, 830, 899, 158, 1123, 584, 633, 7, 397, 878, 250, 750, 984, 223, 125, 968, 737, 696, 923, 201, 1084, 1125, 680, 753, 435, 825, 229, 726, 1191, 1173, 130, 713, 796, 1024, 665, 869, 690, 273, 620, 1028, 772, 547, 919, 912, 677, 950, 1099, 852, 72, 748, 320, 1039, 204, 261, 1029, 242, 4, 705, 1209, 411, 587, 116, 461, 720, 78, 590, 364, 1001, 611, 623, 715, 552, 1093, 512, 778, 1022, 849, 639, 446, 1156, 956, 997, 527, 194, 1040, 939, 858, 1148, 1129, 476, 329, 309, 5, 688, 270, 124, 979, 1206, 1079, 603, 1140, 604, 79, 781, 1186, 1104, 551, 412, 558, 730, 350, 1055, 471, 472, 384, 127, 25, 123, 109, 642, 697, 1133, 376, 239, 766, 958, 366, 409, 806, 1113, 197, 305, 422, 227, 667, 1062, 1044, 1012, 983, 1175, 428, 1101, 308, 1010, 1172, 339, 1211, 549, 265, 775, 559, 797, 243, 395, 442, 462, 155, 863, 268, 1049, 659, 1107, 987, 218, 230, 929, 294, 822, 330, 564, 492, 506, 1005, 685, 764, 823, 809, 253, 638, 20, 1199, 874, 954, 433, 226, 738, 501, 583, 951, 773, 245, 1149, 120, 540, 470, 92, 960]
Converting data to tensors
Normalising data
Finalising data preprocessing
the checkpoint ID for this run is:  1845299487710782
the save name prefix for this run is:  chkpt-ID_1845299487710782_tag_Ablation-job-blacklist-s num rels_o num rels
running TWIG with settings:
data_to_load: {'DistMult': {'UMLS': ['2.1']}}
test_ratio: 0.1
valid_ratio: 0.0
normalisation: zscore
n_bins: 30
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
optimizer_args: {'lr': 0.005}
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}
RUnning with TWIG model:
TWIG_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (linear_hps_1): Linear(in_features=9, out_features=6, bias=True)
  (relu_3): ReLU()
  (linear_integrate_1): Linear(in_features=16, out_features=8, bias=True)
  (relu_4): ReLU()
  (linear_final): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
Training with epochs in stages 1: 5 and 2: 10
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 827
rank avg (pred): 0.543 +- 0.005
mrr vals (pred, true): 0.014, 0.243
batch losses (mrrl, rdl): 0.0, 0.0031593186

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 10
rank avg (pred): 0.071 +- 0.042
mrr vals (pred, true): 0.257, 0.545
batch losses (mrrl, rdl): 0.0, 2.69038e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1126
rank avg (pred): 0.355 +- 0.237
mrr vals (pred, true): 0.234, 0.051
batch losses (mrrl, rdl): 0.0, 0.0001264167

Epoch over!
epoch time: 12.241

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 573
rank avg (pred): 0.474 +- 0.287
mrr vals (pred, true): 0.202, 0.037
batch losses (mrrl, rdl): 0.0, 9.5357e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 142
rank avg (pred): 0.356 +- 0.253
mrr vals (pred, true): 0.247, 0.083
batch losses (mrrl, rdl): 0.0, 5.30191e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 782
rank avg (pred): 0.445 +- 0.287
mrr vals (pred, true): 0.224, 0.074
batch losses (mrrl, rdl): 0.0, 4.77076e-05

Epoch over!
epoch time: 12.057

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 642
rank avg (pred): 0.434 +- 0.286
mrr vals (pred, true): 0.224, 0.045
batch losses (mrrl, rdl): 0.0, 1.37481e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1147
rank avg (pred): 0.207 +- 0.158
mrr vals (pred, true): 0.301, 0.129
batch losses (mrrl, rdl): 0.0, 8.50717e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1154
rank avg (pred): 0.226 +- 0.186
mrr vals (pred, true): 0.340, 0.160
batch losses (mrrl, rdl): 0.0, 4.12427e-05

Epoch over!
epoch time: 12.014

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 562
rank avg (pred): 0.362 +- 0.282
mrr vals (pred, true): 0.295, 0.056
batch losses (mrrl, rdl): 0.0, 3.15957e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 96
rank avg (pred): 0.323 +- 0.252
mrr vals (pred, true): 0.298, 0.121
batch losses (mrrl, rdl): 0.0, 0.0001141414

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 173
rank avg (pred): 0.337 +- 0.254
mrr vals (pred, true): 0.278, 0.052
batch losses (mrrl, rdl): 0.0, 0.0001056244

Epoch over!
epoch time: 12.006

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 981
rank avg (pred): 0.033 +- 0.025
mrr vals (pred, true): 0.419, 0.543
batch losses (mrrl, rdl): 0.0, 2.004e-07

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 347
rank avg (pred): 0.350 +- 0.260
mrr vals (pred, true): 0.278, 0.114
batch losses (mrrl, rdl): 0.0, 0.000144814

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 128
rank avg (pred): 0.355 +- 0.252
mrr vals (pred, true): 0.257, 0.100
batch losses (mrrl, rdl): 0.0, 8.27211e-05

Epoch over!
epoch time: 12.014

Saving checkpoint at [1] epoch 5
Done training phase:  0
Epoch 1 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 727
rank avg (pred): 0.447 +- 0.296
mrr vals (pred, true): 0.243, 0.046
batch losses (mrrl, rdl): 0.3712731302, 2.00947e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 295
rank avg (pred): 0.010 +- 0.007
mrr vals (pred, true): 0.538, 0.546
batch losses (mrrl, rdl): 0.0007051934, 1.63239e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1167
rank avg (pred): 0.486 +- 0.123
mrr vals (pred, true): 0.051, 0.043
batch losses (mrrl, rdl): 1.42785e-05, 6.03803e-05

Epoch over!
epoch time: 12.562

Epoch 2 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 854
rank avg (pred): 0.494 +- 0.105
mrr vals (pred, true): 0.040, 0.114
batch losses (mrrl, rdl): 0.0544175357, 0.0001698571

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 823
rank avg (pred): 0.017 +- 0.013
mrr vals (pred, true): 0.483, 0.426
batch losses (mrrl, rdl): 0.0323971584, 6.56266e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1080
rank avg (pred): 0.434 +- 0.145
mrr vals (pred, true): 0.069, 0.080
batch losses (mrrl, rdl): 0.0037122692, 0.0001253702

Epoch over!
epoch time: 12.245

Epoch 3 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 739
rank avg (pred): 0.018 +- 0.015
mrr vals (pred, true): 0.471, 0.519
batch losses (mrrl, rdl): 0.0228903051, 9.9849e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 452
rank avg (pred): 0.418 +- 0.159
mrr vals (pred, true): 0.073, 0.048
batch losses (mrrl, rdl): 0.0052571828, 5.06098e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 869
rank avg (pred): 0.471 +- 0.103
mrr vals (pred, true): 0.047, 0.046
batch losses (mrrl, rdl): 7.69822e-05, 7.77081e-05

Epoch over!
epoch time: 12.36

Epoch 4 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 429
rank avg (pred): 0.469 +- 0.111
mrr vals (pred, true): 0.052, 0.058
batch losses (mrrl, rdl): 4.29026e-05, 0.0001424809

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1117
rank avg (pred): 0.404 +- 0.155
mrr vals (pred, true): 0.078, 0.050
batch losses (mrrl, rdl): 0.0077798385, 3.81907e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 630
rank avg (pred): 0.537 +- 0.159
mrr vals (pred, true): 0.052, 0.040
batch losses (mrrl, rdl): 4.7523e-05, 7.18231e-05

Epoch over!
epoch time: 12.214

Epoch 5 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 447
rank avg (pred): 0.408 +- 0.154
mrr vals (pred, true): 0.076, 0.056
batch losses (mrrl, rdl): 0.0066343551, 5.09443e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 672
rank avg (pred): 0.463 +- 0.109
mrr vals (pred, true): 0.055, 0.058
batch losses (mrrl, rdl): 0.0002335258, 7.2589e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 12
rank avg (pred): 0.014 +- 0.013
mrr vals (pred, true): 0.564, 0.540
batch losses (mrrl, rdl): 0.0059113801, 1.07843e-05

Epoch over!
epoch time: 12.103

Saving checkpoint at [1] epoch 5
Epoch 6 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 656
rank avg (pred): 0.514 +- 0.146
mrr vals (pred, true): 0.054, 0.047
batch losses (mrrl, rdl): 0.0001479167, 0.0001691692

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 766
rank avg (pred): 0.457 +- 0.103
mrr vals (pred, true): 0.053, 0.050
batch losses (mrrl, rdl): 7.34093e-05, 6.98018e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 594
rank avg (pred): 0.508 +- 0.152
mrr vals (pred, true): 0.056, 0.037
batch losses (mrrl, rdl): 0.0004024243, 6.21935e-05

Epoch over!
epoch time: 12.351

Epoch 7 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 662
rank avg (pred): 0.478 +- 0.126
mrr vals (pred, true): 0.056, 0.046
batch losses (mrrl, rdl): 0.0003780518, 7.0112e-05

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 105
rank avg (pred): 0.416 +- 0.131
mrr vals (pred, true): 0.071, 0.097
batch losses (mrrl, rdl): 0.0042298445, 0.0001873665

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 980
rank avg (pred): 0.032 +- 0.029
mrr vals (pred, true): 0.495, 0.552
batch losses (mrrl, rdl): 0.0319291539, 5.331e-07

Epoch over!
epoch time: 12.22

Epoch 8 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 749
rank avg (pred): 0.028 +- 0.026
mrr vals (pred, true): 0.511, 0.517
batch losses (mrrl, rdl): 0.0004316074, 2.7813e-06

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 550
rank avg (pred): 0.412 +- 0.173
mrr vals (pred, true): 0.079, 0.062
batch losses (mrrl, rdl): 0.0082338285, 5.20033e-05

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 1017
rank avg (pred): 0.457 +- 0.190
mrr vals (pred, true): 0.075, 0.067
batch losses (mrrl, rdl): 0.0063898624, 0.0001903939

Epoch over!
epoch time: 12.285

Epoch 9 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 937
rank avg (pred): 0.440 +- 0.087
mrr vals (pred, true): 0.043, 0.020
batch losses (mrrl, rdl): 0.0004885558, 0.0012576608

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 103
rank avg (pred): 0.375 +- 0.145
mrr vals (pred, true): 0.078, 0.146
batch losses (mrrl, rdl): 0.0457899049, 0.0003630348

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 588
rank avg (pred): 0.427 +- 0.095
mrr vals (pred, true): 0.055, 0.049
batch losses (mrrl, rdl): 0.0002980344, 0.0001195918

Epoch over!
epoch time: 12.021

Epoch 10 -- 
running batch: 0 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 222
rank avg (pred): 0.519 +- 0.179
mrr vals (pred, true): 0.062, 0.060
batch losses (mrrl, rdl): 0.001425207, 0.0001973383

running batch: 500 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 15
rank avg (pred): 0.023 +- 0.022
mrr vals (pred, true): 0.533, 0.555
batch losses (mrrl, rdl): 0.0046177818, 5.1507e-06

running batch: 1000 / 1094 and superbatch(1); data from DistMult, UMLS, run 2.1, exp 18
rank avg (pred): 0.019 +- 0.018
mrr vals (pred, true): 0.553, 0.559
batch losses (mrrl, rdl): 0.0004438637, 5.6167e-06

Epoch over!
epoch time: 12.245

Saving checkpoint at [1] epoch 10
Done training phase:  1
Testing model with KGEM DistMult and KG UMLS
Running eval on the test set
running batch: 0
rank avg (pred): 0.511 +- 0.168
mrr vals (pred, true): 0.055, 0.128

Evaluation for UMLS on the test set
==========================================
(Sorted by True MRR values)
i_pred 	 i_true 	 Pred MRR 	 True MRR 	 Change Flag
    1 	     0 	 0.02670 	 0.01638 	 ~...
   84 	     1 	 0.06530 	 0.02127 	 m..s
   12 	     2 	 0.04205 	 0.03391 	 ~...
   40 	     3 	 0.05012 	 0.03408 	 ~...
   37 	     4 	 0.04944 	 0.03521 	 ~...
   71 	     5 	 0.06236 	 0.03861 	 ~...
   34 	     6 	 0.04816 	 0.03863 	 ~...
    7 	     7 	 0.03890 	 0.03905 	 ~...
   44 	     8 	 0.05160 	 0.03921 	 ~...
   55 	     9 	 0.05851 	 0.03938 	 ~...
   18 	    10 	 0.04367 	 0.04018 	 ~...
   36 	    11 	 0.04841 	 0.04025 	 ~...
   47 	    12 	 0.05462 	 0.04075 	 ~...
   28 	    13 	 0.04649 	 0.04101 	 ~...
    8 	    14 	 0.03947 	 0.04140 	 ~...
   19 	    15 	 0.04445 	 0.04176 	 ~...
   48 	    16 	 0.05466 	 0.04216 	 ~...
   93 	    17 	 0.06805 	 0.04229 	 ~...
   39 	    18 	 0.04948 	 0.04250 	 ~...
   30 	    19 	 0.04696 	 0.04274 	 ~...
    3 	    20 	 0.03755 	 0.04294 	 ~...
   70 	    21 	 0.06231 	 0.04299 	 ~...
   20 	    22 	 0.04455 	 0.04324 	 ~...
   25 	    23 	 0.04604 	 0.04332 	 ~...
   42 	    24 	 0.05086 	 0.04357 	 ~...
   76 	    25 	 0.06424 	 0.04374 	 ~...
   17 	    26 	 0.04362 	 0.04401 	 ~...
   31 	    27 	 0.04705 	 0.04411 	 ~...
   22 	    28 	 0.04522 	 0.04430 	 ~...
    0 	    29 	 0.02581 	 0.04460 	 ~...
   26 	    30 	 0.04633 	 0.04476 	 ~...
   13 	    31 	 0.04208 	 0.04479 	 ~...
    4 	    32 	 0.03773 	 0.04484 	 ~...
   21 	    33 	 0.04522 	 0.04499 	 ~...
   43 	    34 	 0.05130 	 0.04505 	 ~...
   74 	    35 	 0.06346 	 0.04542 	 ~...
    2 	    36 	 0.02760 	 0.04555 	 ~...
   23 	    37 	 0.04528 	 0.04581 	 ~...
   45 	    38 	 0.05286 	 0.04594 	 ~...
   16 	    39 	 0.04285 	 0.04634 	 ~...
   27 	    40 	 0.04634 	 0.04652 	 ~...
   65 	    41 	 0.06109 	 0.04698 	 ~...
   15 	    42 	 0.04269 	 0.04709 	 ~...
   33 	    43 	 0.04717 	 0.04716 	 ~...
   35 	    44 	 0.04836 	 0.04717 	 ~...
   87 	    45 	 0.06586 	 0.04750 	 ~...
   79 	    46 	 0.06494 	 0.04758 	 ~...
   56 	    47 	 0.05894 	 0.04771 	 ~...
   60 	    48 	 0.05991 	 0.04836 	 ~...
    5 	    49 	 0.03773 	 0.04899 	 ~...
   38 	    50 	 0.04946 	 0.04954 	 ~...
   95 	    51 	 0.06970 	 0.04967 	 ~...
   41 	    52 	 0.05051 	 0.04973 	 ~...
   66 	    53 	 0.06114 	 0.04979 	 ~...
   11 	    54 	 0.04169 	 0.05000 	 ~...
   86 	    55 	 0.06569 	 0.05054 	 ~...
   49 	    56 	 0.05473 	 0.05110 	 ~...
   88 	    57 	 0.06593 	 0.05129 	 ~...
    9 	    58 	 0.03949 	 0.05219 	 ~...
   46 	    59 	 0.05313 	 0.05253 	 ~...
   32 	    60 	 0.04708 	 0.05281 	 ~...
   68 	    61 	 0.06212 	 0.05340 	 ~...
    6 	    62 	 0.03815 	 0.05352 	 ~...
   72 	    63 	 0.06281 	 0.05362 	 ~...
   75 	    64 	 0.06420 	 0.05412 	 ~...
   24 	    65 	 0.04595 	 0.05560 	 ~...
   53 	    66 	 0.05663 	 0.05570 	 ~...
   69 	    67 	 0.06231 	 0.05594 	 ~...
   10 	    68 	 0.04012 	 0.05622 	 ~...
   58 	    69 	 0.05915 	 0.05631 	 ~...
   62 	    70 	 0.06038 	 0.05668 	 ~...
   14 	    71 	 0.04251 	 0.05983 	 ~...
   92 	    72 	 0.06802 	 0.06245 	 ~...
   81 	    73 	 0.06507 	 0.06365 	 ~...
   83 	    74 	 0.06526 	 0.06487 	 ~...
   77 	    75 	 0.06427 	 0.07225 	 ~...
   91 	    76 	 0.06662 	 0.07881 	 ~...
   82 	    77 	 0.06508 	 0.08048 	 ~...
   29 	    78 	 0.04675 	 0.08119 	 m..s
   67 	    79 	 0.06135 	 0.08200 	 ~...
   73 	    80 	 0.06321 	 0.08357 	 ~...
   89 	    81 	 0.06647 	 0.08456 	 ~...
   80 	    82 	 0.06494 	 0.08490 	 ~...
   59 	    83 	 0.05963 	 0.08802 	 ~...
   63 	    84 	 0.06079 	 0.09087 	 m..s
   50 	    85 	 0.05483 	 0.10153 	 m..s
   97 	    86 	 0.07072 	 0.10155 	 m..s
   78 	    87 	 0.06458 	 0.10255 	 m..s
   94 	    88 	 0.06938 	 0.10286 	 m..s
   52 	    89 	 0.05608 	 0.11112 	 m..s
   85 	    90 	 0.06534 	 0.12203 	 m..s
   51 	    91 	 0.05494 	 0.12795 	 m..s
   96 	    92 	 0.07046 	 0.13010 	 m..s
   54 	    93 	 0.05768 	 0.14243 	 m..s
   57 	    94 	 0.05912 	 0.14266 	 m..s
   90 	    95 	 0.06660 	 0.15339 	 m..s
   61 	    96 	 0.06032 	 0.15403 	 m..s
   98 	    97 	 0.16922 	 0.16603 	 ~...
   64 	    98 	 0.06088 	 0.17769 	 MISS
   99 	    99 	 0.19322 	 0.22480 	 m..s
  100 	   100 	 0.19431 	 0.23905 	 m..s
  101 	   101 	 0.19473 	 0.24502 	 m..s
  103 	   102 	 0.42723 	 0.39802 	 ~...
  105 	   103 	 0.49770 	 0.50782 	 ~...
  104 	   104 	 0.49709 	 0.51127 	 ~...
  114 	   105 	 0.51559 	 0.51440 	 ~...
  102 	   106 	 0.40955 	 0.51622 	 MISS
  110 	   107 	 0.50592 	 0.52033 	 ~...
  108 	   108 	 0.49949 	 0.52858 	 ~...
  107 	   109 	 0.49918 	 0.53079 	 m..s
  115 	   110 	 0.52045 	 0.53399 	 ~...
  106 	   111 	 0.49806 	 0.53497 	 m..s
  120 	   112 	 0.55011 	 0.53525 	 ~...
  111 	   113 	 0.51299 	 0.54081 	 ~...
  118 	   114 	 0.52698 	 0.54676 	 ~...
  109 	   115 	 0.50406 	 0.54829 	 m..s
  117 	   116 	 0.52657 	 0.54951 	 ~...
  116 	   117 	 0.52617 	 0.55070 	 ~...
  119 	   118 	 0.53869 	 0.55651 	 ~...
  113 	   119 	 0.51520 	 0.55746 	 m..s
  112 	   120 	 0.51443 	 0.55872 	 m..s
==========================================
r_mrr = 0.9871441125869751
r2_mrr = 0.9694113731384277
spearmanr_mrr@5 = 0.7648488283157349
spearmanr_mrr@10 = 0.8534094095230103
spearmanr_mrr@50 = 0.993388831615448
spearmanr_mrr@100 = 0.9914810657501221
spearmanr_mrr@All = 0.9918385148048401
==========================================
test time: 0.422
Done Testing dataset UMLS
total time taken: 189.3662850856781
training time taken: 183.42693614959717
TWIG out ;))
Ablation done!
The best results were: {'r_mrr': {'DistMult': {'UMLS': tensor(0.9871)}}, 'r2_mrr': {'DistMult': {'UMLS': tensor(0.9694)}}, 'spearmanr_mrr@5': {'DistMult': {'UMLS': tensor(0.7648)}}, 'spearmanr_mrr@10': {'DistMult': {'UMLS': tensor(0.8534)}}, 'spearmanr_mrr@50': {'DistMult': {'UMLS': tensor(0.9934)}}, 'spearmanr_mrr@100': {'DistMult': {'UMLS': tensor(0.9915)}}, 'spearmanr_mrr@All': {'DistMult': {'UMLS': tensor(0.9918)}}, 'test_loss': {'DistMult': {'UMLS': 1.0728922350481298}}}
The best settings found were:
normalisation: zscore
n_bins: 30
model_or_version: base
model_kwargs: None
optimizer: adam
optimizer_args: {'lr': 0.005}
epochs: [5, 10]
mrr_loss_coeffs: [0, 10]
rank_dist_loss_coeffs: [1, 1]
rescale_mrr_loss: False
rescale_rank_dist_loss: False
ft_blacklist: {'s num rels', 'o num rels'}

